<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Speech">
    <meta name="description" content="Speech 方向最新论文已更新，请持续关注 Update in 2025-04-08  MultiMed-ST Large-scale Many-to-many Multilingual Medical Speech   Translation">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Speech | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loader页面消失采用渐隐的方式*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logo出现动画 */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//使用渐隐的方法淡出loading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//强制显示loading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">嘘~  正在从服务器偷取页面 . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>首页</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>标签</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>分类</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>归档</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="搜索" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="深色/浅色模式" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			首页
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			标签
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			分类
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			归档
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-8e88eed302b9cc9c906e63d23f686c27.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Speech</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- 文章内容详情 -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Speech/">
                                <span class="chip bg-color">Speech</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Speech/" class="post-category">
                                Speech
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp;
                    2025-04-08
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>更新日期:&nbsp;&nbsp;
                    2025-04-21
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>文章字数:&nbsp;&nbsp;
                    5k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>阅读时长:&nbsp;&nbsp;
                    20 分
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>阅读次数:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>⚠️ 以下所有内容总结都来自于 大语言模型的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p>
</blockquote>
<h1 id="2025-04-08-更新"><a href="#2025-04-08-更新" class="headerlink" title="2025-04-08 更新"></a>2025-04-08 更新</h1><h2 id="MultiMed-ST-Large-scale-Many-to-many-Multilingual-Medical-Speech-Translation"><a href="#MultiMed-ST-Large-scale-Many-to-many-Multilingual-Medical-Speech-Translation" class="headerlink" title="MultiMed-ST: Large-scale Many-to-many Multilingual Medical Speech   Translation"></a>MultiMed-ST: Large-scale Many-to-many Multilingual Medical Speech   Translation</h2><p><strong>Authors:Khai Le-Duc, Tuyen Tran, Bach Phan Tat, Nguyen Kim Hai Bui, Quan Dang, Hung-Phong Tran, Thanh-Thuy Nguyen, Ly Nguyen, Tuan-Minh Phan, Thi Thu Phuong Tran, Chris Ngo, Nguyen X. Khanh, Thanh Nguyen-Tang</strong></p>
<p>Multilingual speech translation (ST) in the medical domain enhances patient care by enabling efficient communication across language barriers, alleviating specialized workforce shortages, and facilitating improved diagnosis and treatment, particularly during pandemics. In this work, we present the first systematic study on medical ST, to our best knowledge, by releasing MultiMed-ST, a large-scale ST dataset for the medical domain, spanning all translation directions in five languages: Vietnamese, English, German, French, Traditional Chinese and Simplified Chinese, together with the models. With 290,000 samples, our dataset is the largest medical machine translation (MT) dataset and the largest many-to-many multilingual ST among all domains. Secondly, we present the most extensive analysis study in ST research to date, including: empirical baselines, bilingual-multilingual comparative study, end-to-end vs. cascaded comparative study, task-specific vs. multi-task sequence-to-sequence (seq2seq) comparative study, code-switch analysis, and quantitative-qualitative error analysis. All code, data, and models are available online: <a target="_blank" rel="noopener" href="https://github.com/leduckhai/MultiMed-ST">https://github.com/leduckhai/MultiMed-ST</a>. </p>
<blockquote>
<p>在医疗领域，多语言语音识别（ST）通过突破语言障碍实现高效沟通、缓解专业劳动力短缺、促进诊断和治疗的改进，特别是在疫情期间，提升了患者护理体验。在这项工作中，我们据我们所知，首次对医疗ST进行了系统研究，并发布了MultiMed-ST大规模医疗领域ST数据集。该数据集涵盖五个语种的全部翻译方向：越南语、英语、德语、法语、繁体中文和简体中文，并配备了模型。我们的数据集包含29万个样本，是迄今为止最大的医疗机器翻译（MT）数据集以及所有领域最大的多语种ST数据集。其次，我们目前进行了ST研究中最为广泛的分析研究，包括：实证基准线研究、双语-多语比较研究、端到端与级联比较研究、任务特定与多任务序列到序列（seq2seq）比较研究、代码切换分析和定量-定性误差分析。所有代码、数据和模型均可在网上找到：<a target="_blank" rel="noopener" href="https://github.com/leduckhai/MultiMed-ST%E3%80%82">https://github.com/leduckhai/MultiMed-ST。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.03546v1">PDF</a> Preprint, 122 pages</p>
<p><strong>Summary</strong></p>
<p>本文介绍了医疗领域中的多语言语音识别翻译（ST）技术，该技术通过消除语言障碍、缓解专业劳动力短缺以及促进诊断和治疗的改进，提高了患者护理的效率。文章首次系统性地研究了医疗ST，并发布了MultiMed-ST数据集，包含五种语言的大规模ST数据集，以及相应的模型。该数据集包含29万个样本，是医疗机器翻译（MT）领域最大的数据集，也是所有领域中最大的多对多语言ST数据集。此外，文章还进行了迄今为止最全面的ST研究分析，包括实证基准、双语-多语对比研究、端到端与级联对比研究、任务特定与多任务序列到序列（seq2seq）对比研究、代码切换分析和定量定性错误分析。所有代码、数据和模型均可在网上获取。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>多语言语音识别翻译（ST）在医疗领域对提高患者护理效率至关重要，它能消除语言障碍、缓解专业劳动力短缺并促进诊断和治疗的改进。</li>
<li>首次系统性地研究了医疗ST，并发布了MultiMed-ST数据集，包含五种语言的大规模翻译数据集和模型。</li>
<li>MultiMed-ST数据集包含29万个样本，是医疗机器翻译（MT）领域最大的数据集。</li>
<li>该研究进行了全面的ST分析，包括实证基准、双语-多语对比、端到端与级联对比等。</li>
<li>文章进行了任务特定与多任务序列到序列（seq2seq）对比研究，对于了解ST技术的发展和进步具有重要意义。</li>
<li>代码切换分析是本研究的一大亮点，有助于理解不同语言间的转换复杂性。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.03546">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-c55ee3a0905f4135c1183b78e5ca9d34.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d596fb3f47f79b737c0aab489dfe0b50.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-839a3cbc57a18e0472b587b206286f0e.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-b9f24086e8db3cbcbf122d8ece5754b0.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Ichigo-Mixed-Modal-Early-Fusion-Realtime-Voice-Assistant"><a href="#Ichigo-Mixed-Modal-Early-Fusion-Realtime-Voice-Assistant" class="headerlink" title="Ichigo: Mixed-Modal Early-Fusion Realtime Voice Assistant"></a>Ichigo: Mixed-Modal Early-Fusion Realtime Voice Assistant</h2><p><strong>Authors:Alan Dao, Dinh Bach Vu, Huy Hoang Ha</strong></p>
<p>Large Language Models (LLMs) have revolutionized natural language processing, but their application to speech-based tasks remains challenging due to the complexities of integrating audio and text modalities. This paper introduces Ichigo, a mixed-modal model that seamlessly processes interleaved sequences of speech and text. Utilizing a tokenized early-fusion approach, Ichigo quantizes speech into discrete tokens and employs a uniform transformer-based architecture for both speech and text modalities. This method enables joint reasoning and generation across modalities without the need for separate adapters. We present a comprehensive training methodology, including pre-training on multilingual speech recognition datasets and fine-tuning on a curated instruction dataset. Ichigo demonstrates state-of-the-art performance on speech question-answering benchmarks, outperforming existing open-source speech language models and achieving comparable results to cascaded systems. Notably, Ichigo exhibits a latency of just 111 ms to first token generation, significantly lower than current models. Our approach not only advances the field of multimodal AI but also provides a framework for smaller research teams to contribute effectively to open-source speech-language models. </p>
<blockquote>
<p>大型语言模型（LLMs）已经彻底改变了自然语言处理的格局，但在基于语音的任务上应用这些模型仍然具有挑战性，原因是整合音频和文本模态的复杂性。本文介绍了一种名为Ichigo的混合模态模型，它能够无缝处理语音和文本交织的序列。Ichigo采用了一种早期融合令牌化的方法，将语音量化为离散令牌，并采用统一的基于转换器的架构来处理语音和文本两种模态。这种方法能够在无需单独适配器的情况下实现跨模态的联合推理和生成。我们提出了一种全面的训练方法，包括在多语言语音识别数据集上进行预训练以及在精选指令数据集上进行微调。Ichigo在语音问答基准测试中表现出了最先进的性能，超越了现有的开源语音语言模型，并取得了与级联系统相当的结果。值得注意的是，Ichigo的首个令牌生成的延迟时间仅为111毫秒，显著低于当前模型。我们的方法不仅推动了多模态人工智能领域的发展，而且为小型研究团队有效地为开源语音语言模型做出贡献提供了框架。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2410.15316v3">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>大语言模型在语音任务的应用中存在挑战，涉及语音和文字两种模态的集成。本文介绍了一种混合模态模型Ichigo，它能无缝处理语音和文字交织的序列。Ichigo采用早期融合方法，将语音量化成离散令牌，并使用统一的基于转换器的架构处理语音和文字模态。这种方法无需单独适配器即可实现跨模态推理和生成。Ichigo在多语种语音识别数据集上进行预训练，并在精选指令数据集上进行微调。在语音问答基准测试中，Ichigo表现出卓越性能，不仅优于现有开源语音语言模型，而且与级联系统结果相当。此外，Ichigo的首个令牌生成延迟仅为111毫秒，显著优于现有模型。这不仅推动了多模态人工智能领域的发展，也为小型研究团队有效参与开源语音语言模型提供了框架。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Ichigo是一个混合模态模型，能够处理语音和文字交织的序列。</li>
<li>采用早期融合方法和令牌化语音处理。</li>
<li>使用统一的基于转换器的架构进行多模态处理。</li>
<li>通过预训练和微调，Ichigo在语音问答方面表现出卓越性能。</li>
<li>Ichigo的生成延迟低于现有模型。</li>
<li>Ichigo的框架有助于小型研究团队有效参与开源语音语言模型。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2410.15316">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-33a211584c4efa8ba77c4eeb6da95ac4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7ba50be4668afd58dec998a56ee79734.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b344511c9ca9b4bd4889fa5fbbfba854.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Real-time-Speech-Summarization-for-Medical-Conversations"><a href="#Real-time-Speech-Summarization-for-Medical-Conversations" class="headerlink" title="Real-time Speech Summarization for Medical Conversations"></a>Real-time Speech Summarization for Medical Conversations</h2><p><strong>Authors:Khai Le-Duc, Khai-Nguyen Nguyen, Long Vo-Dang, Truong-Son Hy</strong></p>
<p>In doctor-patient conversations, identifying medically relevant information is crucial, posing the need for conversation summarization. In this work, we propose the first deployable real-time speech summarization system for real-world applications in industry, which generates a local summary after every N speech utterances within a conversation and a global summary after the end of a conversation. Our system could enhance user experience from a business standpoint, while also reducing computational costs from a technical perspective. Secondly, we present VietMed-Sum which, to our knowledge, is the first speech summarization dataset for medical conversations. Thirdly, we are the first to utilize LLM and human annotators collaboratively to create gold standard and synthetic summaries for medical conversation summarization. Finally, we present baseline results of state-of-the-art models on VietMed-Sum. All code, data (English-translated and Vietnamese) and models are available online: <a target="_blank" rel="noopener" href="https://github.com/leduckhai/MultiMed/tree/master/VietMed-Sum">https://github.com/leduckhai/MultiMed/tree/master/VietMed-Sum</a> </p>
<blockquote>
<p>在医患对话中，识别医学相关信息至关重要，这提出了对话摘要的需求。在这项工作中，我们提出了首个可用于行业实际应用部署的实时语音摘要系统，该系统会在对话中的每N个语音片段后生成局部摘要，并在对话结束时生成全局摘要。从商业角度来看，我们的系统可以提升用户体验，从技术角度来看，还能降低计算成本。其次，我们推出了VietMed-Sum数据集，据我们所知，这是首个用于医疗对话的语音摘要数据集。此外，我们还是首次利用大型语言模型（LLM）和人类注释器进行合作，为医疗对话摘要创建金标准和合成摘要。最后，我们在VietMed-Sum上展示了最新模型的基线结果。所有代码、数据（英语翻译和越南语）和模型均可在网上找到：<a target="_blank" rel="noopener" href="https://github.com/leduckhai/MultiMed/tree/master/VietMed-Sum">https://github.com/leduckhai/MultiMed/tree/master/VietMed-Sum</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2406.15888v2">PDF</a> Interspeech 2024 (Oral)</p>
<p><strong>Summary</strong></p>
<p>针对医患对话中的医学相关信息识别，提出首个可部署的实时语音摘要系统。该系统可在对话中的每个N个语音片段后生成局部摘要，并在对话结束时生成全局摘要。此系统从商业角度提升了用户体验，从技术角度降低了计算成本。此外，还推出了越南语医疗对话摘要数据集VietMed-Sum，并首次利用大型语言模型与人工标注师合作创建医疗对话摘要的金标准和合成摘要。最后，展示了先进模型在VietMed-Sum上的基线结果。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>提出首个可部署的实时语音摘要系统，适用于医疗行业中的对话内容。</li>
<li>系统能够生成局部和全局两种摘要，提高用户体验并降低计算成本。</li>
<li>推出越南语医疗对话摘要数据集VietMed-Sum，为医疗行业提供数据支持。</li>
<li>首次结合大型语言模型与人工标注师，共同创建医疗对话摘要的标准数据。</li>
<li>利用协同工作的方式提高了数据集的多样性和准确性。</li>
<li>系统化的方法有助于解决医疗对话中的信息过载问题，提高沟通效率。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2406.15888">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-1fdf2152b3e78634cfd776d5761a5682.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-af21283e676a0f9d7294933bb0b18979.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-31791eec831069cba63d1af50266b45c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-43f6f182535649eb344121093d026168.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-21846a2e24d3d89fe476d17a37d46479.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-1b79d3b27d0b89110b96d25783a7a351.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-6329684abc38718e9135eb3b41252923.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-56cbcc27a54e3701979c1ba2f660ef7a.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-df855d1714ead468576ad788261cbabe.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="VietMed-A-Dataset-and-Benchmark-for-Automatic-Speech-Recognition-of-Vietnamese-in-the-Medical-Domain"><a href="#VietMed-A-Dataset-and-Benchmark-for-Automatic-Speech-Recognition-of-Vietnamese-in-the-Medical-Domain" class="headerlink" title="VietMed: A Dataset and Benchmark for Automatic Speech Recognition of   Vietnamese in the Medical Domain"></a>VietMed: A Dataset and Benchmark for Automatic Speech Recognition of   Vietnamese in the Medical Domain</h2><p><strong>Authors:Khai Le-Duc</strong></p>
<p>Due to privacy restrictions, there’s a shortage of publicly available speech recognition datasets in the medical domain. In this work, we present VietMed - a Vietnamese speech recognition dataset in the medical domain comprising 16h of labeled medical speech, 1000h of unlabeled medical speech and 1200h of unlabeled general-domain speech. To our best knowledge, VietMed is by far the world’s largest public medical speech recognition dataset in 7 aspects: total duration, number of speakers, diseases, recording conditions, speaker roles, unique medical terms and accents. VietMed is also by far the largest public Vietnamese speech dataset in terms of total duration. Additionally, we are the first to present a medical ASR dataset covering all ICD-10 disease groups and all accents within a country. Moreover, we release the first public large-scale pre-trained models for Vietnamese ASR, w2v2-Viet and XLSR-53-Viet, along with the first public large-scale fine-tuned models for medical ASR. Even without any medical data in unsupervised pre-training, our best pre-trained model XLSR-53-Viet generalizes very well to the medical domain by outperforming state-of-the-art XLSR-53, from 51.8% to 29.6% WER on test set (a relative reduction of more than 40%). All code, data and models are made publicly available: <a target="_blank" rel="noopener" href="https://github.com/leduckhai/MultiMed/tree/master/VietMed">https://github.com/leduckhai/MultiMed/tree/master/VietMed</a>. </p>
<blockquote>
<p>由于隐私限制，医疗领域的公开语音识别数据集十分匮乏。在这项工作中，我们推出了VietMed——越南医疗领域的语音识别数据集，包含16小时的标注医疗语音、1000小时的无标签医疗语音和1200小时的无标签通用领域语音。据我们所知，VietMed迄今为止是世界上规模最大、涵盖7个方面的公开医疗语音识别数据集：总时长、发言人数量、疾病种类、录制条件、发言人角色、独特的医疗术语和口音。VietMed也是迄今为止公开越南语音数据集中总时长最长的。此外，我们是首次推出覆盖所有ICD-10疾病群体和一个国家内所有口音的医疗ASR数据集。而且，我们发布了首个公开的越南ASR大规模预训练模型w2v2-Viet和XLSR-53-Viet，以及首个公开的医疗ASR大规模微调模型。即使在没有医疗数据的无监督预训练中，我们最佳的预训练模型XLSR-53-Viet通过适应医疗领域，在测试集上的词错误率从XLSR-53的51.8%降低到29.6%（相对减少了40%以上）。所有代码、数据和模型均已公开提供：<a target="_blank" rel="noopener" href="https://github.com/leduckhai/MultiMed/tree/master/VietMed%E3%80%82">https://github.com/leduckhai/MultiMed/tree/master/VietMed。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2404.05659v3">PDF</a> LREC-COLING 2024 (Oral), 24 pages</p>
<p><strong>Summary</strong></p>
<p>本文介绍了一个名为VietMed的越南语医疗领域语音识别数据集，包含16小时的标注医疗语音、1000小时的无标签医疗语音和1200小时的无标签通用领域语音。VietMed是目前世界上最大的公开医疗领域语音识别数据集，涵盖了总时长、说话人数、疾病种类、录制条件、说话人角色、独特医疗术语和口音等7个方面。此外，还首次发布了覆盖所有ICD-10疾病分组和越南境内所有口音的医疗ASR数据集。同时，本文还介绍了针对越南语ASR的预训练模型和首次公开发布的大型预训练模型w2v2-Viet和XLSR-53-Viet，以及针对医疗ASR的首次公开发布的大型微调模型。其中，XLSR-53-Viet模型在医疗领域的性能表现尤为出色，相较于当前最先进的XLSR-53模型，其在测试集上的词错误率降低了超过40%。</p>
<p><strong>Key Takeaways</strong></p>
<p>1.VietMed是首个大规模的公开医疗领域语音识别数据集，涵盖了16小时的标注医疗语音等数据。<br>2.VietMed在总时长、说话人数、疾病种类等方面是目前世界上最大的公开医疗语音识别数据集。<br>3.VietMed是首个覆盖所有ICD-10疾病分组和越南境内所有口音的医疗ASR数据集。<br>4.研究团队发布了针对越南语ASR的预训练模型w2v2-Viet和XLSR-53-Viet。<br>5.XLSR-53-Viet模型在医疗领域的性能显著，相较于当前最先进的XLSR-53模型，词错误率降低了超过40%。<br>6.该研究的代码、数据和模型均已公开提供，便于公众访问和使用。</p>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2404.05659">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-961863716b42145ea56461ee62e75c4c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8e88eed302b9cc9c906e63d23f686c27.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-6697046c701b76d6f50fbb631f11a33a.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-cdddb4c7b4610cf4df1487af481af098.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c475be2a5fdb45ca092f96dba693849e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-da44be16cba10bffaa08f17d3f6640d3.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Spacewalk-18-A-Benchmark-for-Multimodal-and-Long-form-Procedural-Video-Understanding-in-Novel-Domains"><a href="#Spacewalk-18-A-Benchmark-for-Multimodal-and-Long-form-Procedural-Video-Understanding-in-Novel-Domains" class="headerlink" title="Spacewalk-18: A Benchmark for Multimodal and Long-form Procedural Video   Understanding in Novel Domains"></a>Spacewalk-18: A Benchmark for Multimodal and Long-form Procedural Video   Understanding in Novel Domains</h2><p><strong>Authors:Zitian Tang, Rohan Myer Krishnan, Zhiqiu Yu, Chen Sun</strong></p>
<p>Learning from (procedural) videos has increasingly served as a pathway for embodied agents to acquire skills from human demonstrations. To do this, video understanding models must be able to obtain structured understandings, such as the temporal segmentation of a demonstration into sequences of actions and skills, and to generalize the understandings to novel environments, tasks, and problem domains. In pursuit of this goal, we introduce Spacewalk-18, a benchmark containing two tasks: (1) step recognition and (2) video question answering, over a dataset of temporally segmented and labeled tasks in International Space Station spacewalk recordings. In tandem, the two tasks quantify a model’s ability to: (1) generalize to novel domains; (2) utilize long temporal context and multimodal (e.g. visual and speech) information. Our extensive experimental analysis highlights the challenges of Spacewalk-18, but also suggests best practices for domain generalization and long-form understanding. Notably, we discover a promising adaptation via summarization technique that leads to significant performance improvement without model fine-tuning. The Spacewalk-18 benchmark is released at <a target="_blank" rel="noopener" href="https://brown-palm.github.io/Spacewalk-18/">https://brown-palm.github.io/Spacewalk-18/</a>. </p>
<blockquote>
<p>从（程序性）视频中学习已成为实体代理从人类演示中获取知识的一种途径。为此，视频理解模型必须能够获得结构化理解，例如将演示暂时分割成动作和技能的序列，并将理解推广至新环境、任务和领域。为实现这一目标，我们引入了Spacewalk-18基准测试，其中包含两个任务：（1）步骤识别；（2）视频问答，涉及国际空间站太空行走记录中暂时分割和标记的任务数据集。同时，这两个任务衡量模型的能力： （1）推广到全新领域；（2）利用长期的临时背景和多媒体（例如视觉和语音）信息。我们广泛的实验分析突出了Spacewalk-18的挑战，但也提出了领域推广和长形式理解的最佳实践。值得注意的是，我们发现了一种有前景的通过摘要技术进行适应的方法，该方法在无需对模型进行微调的情况下实现了显著的性能提升。Spacewalk-18基准测试发布在<a target="_blank" rel="noopener" href="https://brown-palm.github.io/Spacewalk-18/%E4%B8%8A%E3%80%82">https://brown-palm.github.io/Spacewalk-18/上。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.18773v3">PDF</a> Under submission. Code and models will be released at   <a target="_blank" rel="noopener" href="https://brown-palm.github.io/Spacewalk-18/">https://brown-palm.github.io/Spacewalk-18/</a></p>
<p><strong>Summary</strong></p>
<p>从（程序性）视频中学习已成为实体代理人从人类演示中获取技能的重要途径。为此，视频理解模型必须获得结构化理解，如将演示内容在时间上分割成动作和技能序列，并能够将这些理解推广至新的环境、任务和领域。为实现这一目标，我们引入了Spacewalk-18基准测试，其中包含两项任务：（1）步骤识别；（2）视频问答。该数据集是对国际空间站太空行走记录中的任务进行时间分割和标注。同时进行的两项任务衡量模型的能力包括：（1）推广到新的领域；（2）利用长期时间背景和多媒体（如视觉和语音）信息。我们的实验分析突出了Spacewalk-18的挑战性，同时也提出了针对领域推广和长期理解的最佳实践。值得注意的是，我们发现了一种有前景的摘要技术改编方法，它在不微调模型的情况下实现了显著的性能提升。Spacewalk-18基准测试发布在<a target="_blank" rel="noopener" href="https://brown-palm.github.io/Spacewalk-18/%E3%80%82">https://brown-palm.github.io/Spacewalk-18/。</a></p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>学习从视频中获得技能已成为实体代理人的重要途径，要求视频理解模型具备结构化理解能力。</li>
<li>Spacewalk-18基准测试包含步骤识别和视频问答两项任务，衡量模型在推广至新环境、任务和领域的表现。</li>
<li>数据集涵盖国际空间站太空行走记录中的任务，并进行时间分割和标注。</li>
<li>模型需利用长期时间背景和多媒体信息，如视觉和语音。</li>
<li>实验分析突显了Spacewalk-18的挑战性，包括领域推广和长期理解方面的难题。</li>
<li>发现了一种通过摘要技术改编的适应方法，显著提高模型性能。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2311.18773">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-47939559852fbdc161b933ccf6b6c928.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-af9057ef0e2b29c88d76c2e190688865.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-f6911e18b496be41ee6eeddf8b7c7013.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        文章作者:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        文章链接:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-04-08/Speech/">https://kedreamix.github.io/Talk2Paper/Paper/2025-04-08/Speech/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        版权声明:
                    </i>
                </span>
                <span class="reprint-info">
                    本博客所有文章除特別声明外，均采用
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    许可协议。转载请注明来源
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Speech/">
                                    <span class="chip bg-color">Speech</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;上一篇</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-04-08/%E5%85%83%E5%AE%87%E5%AE%99_%E8%99%9A%E6%8B%9F%E4%BA%BA/">
                    <div class="card-image">
                        
                        <img src="https://pica.zhimg.com/v2-6066ef3cae096b3d394b3597fb7604b9.jpg" class="responsive-img" alt="元宇宙/虚拟人">
                        
                        <span class="card-title">元宇宙/虚拟人</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            元宇宙/虚拟人 方向最新论文已更新，请持续关注 Update in 2025-04-08  FRESA Feedforward Reconstruction of Personalized Skinned Avatars from   Few Images
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-04-08
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E5%85%83%E5%AE%87%E5%AE%99-%E8%99%9A%E6%8B%9F%E4%BA%BA/" class="post-category">
                                    元宇宙/虚拟人
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E5%85%83%E5%AE%87%E5%AE%99-%E8%99%9A%E6%8B%9F%E4%BA%BA/">
                        <span class="chip bg-color">元宇宙/虚拟人</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                下一篇&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-04-08/%E6%A3%80%E6%B5%8B_%E5%88%86%E5%89%B2_%E8%B7%9F%E8%B8%AA/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-3c48c3dcd5d51644233e51cb6cbe8f9a.jpg" class="responsive-img" alt="检测/分割/跟踪">
                        
                        <span class="card-title">检测/分割/跟踪</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            检测/分割/跟踪 方向最新论文已更新，请持续关注 Update in 2025-04-08  Mamba as a Bridge Where Vision Foundation Models Meet Vision Language   Models for Domain-Generalized Semantic Segmentation
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-04-08
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E6%A3%80%E6%B5%8B-%E5%88%86%E5%89%B2-%E8%B7%9F%E8%B8%AA/" class="post-category">
                                    检测/分割/跟踪
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E6%A3%80%E6%B5%8B-%E5%88%86%E5%89%B2-%E8%B7%9F%E8%B8%AA/">
                        <span class="chip bg-color">检测/分割/跟踪</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- 代码块功能依赖 -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- 代码语言 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- 代码块复制 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- 代码块收缩 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC 悬浮按钮. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* 修复文章卡片 div 的宽度. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // 切换TOC目录展开收缩的相关操作.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;站点总字数:&nbsp;<span
                        class="white-color">17548.7k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- 运行天数提醒. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // 区分是否有年份.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = '本站已运行 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                daysTip = '本站已運行 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = '本站已运行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = '本站已運行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="关注我的知乎: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">知</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- 搜索遮罩框 -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span>
            <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- 白天和黑夜主题 -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- 回到顶部按钮 -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // 只在桌面版网页启用特效
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- 雪花特效 -->
    

    <!-- 鼠标星星特效 -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--腾讯兔小巢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- 动态标签 -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Σ(っ °Д °;)っ诶，页面崩溃了嘛？", clearTimeout(st)) : (document.title = "φ(゜▽゜*)♪咦，又好了！", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
