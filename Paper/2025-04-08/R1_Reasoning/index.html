<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="R1_Reasoning">
    <meta name="description" content="R1_Reasoning æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-04-08  MME-Unify A Comprehensive Benchmark for Unified Multimodal   Understanding and Generation Models">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>R1_Reasoning | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-00d616db923791adfaaebfa4006122cc.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">R1_Reasoning</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/R1-Reasoning/">
                                <span class="chip bg-color">R1_Reasoning</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/R1-Reasoning/" class="post-category">
                                R1_Reasoning
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-04-08
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-05-14
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    21.6k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    88 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-04-08-æ›´æ–°"><a href="#2025-04-08-æ›´æ–°" class="headerlink" title="2025-04-08 æ›´æ–°"></a>2025-04-08 æ›´æ–°</h1><h2 id="MME-Unify-A-Comprehensive-Benchmark-for-Unified-Multimodal-Understanding-and-Generation-Models"><a href="#MME-Unify-A-Comprehensive-Benchmark-for-Unified-Multimodal-Understanding-and-Generation-Models" class="headerlink" title="MME-Unify: A Comprehensive Benchmark for Unified Multimodal   Understanding and Generation Models"></a>MME-Unify: A Comprehensive Benchmark for Unified Multimodal   Understanding and Generation Models</h2><p><strong>Authors:Wulin Xie, Yi-Fan Zhang, Chaoyou Fu, Yang Shi, Bingyan Nie, Hongkai Chen, Zhang Zhang, Liang Wang, Tieniu Tan</strong></p>
<p>Existing MLLM benchmarks face significant challenges in evaluating Unified MLLMs (U-MLLMs) due to: 1) lack of standardized benchmarks for traditional tasks, leading to inconsistent comparisons; 2) absence of benchmarks for mixed-modality generation, which fails to assess multimodal reasoning capabilities. We present a comprehensive evaluation framework designed to systematically assess U-MLLMs. Our benchmark includes: Standardized Traditional Task Evaluation. We sample from 12 datasets, covering 10 tasks with 30 subtasks, ensuring consistent and fair comparisons across studies.â€ 2. Unified Task Assessment. We introduce five novel tasks testing multimodal reasoning, including image editing, commonsense QA with image generation, and geometric reasoning. 3. Comprehensive Model Benchmarking. We evaluate 12 leading U-MLLMs, such as Janus-Pro, EMU3, VILA-U, and Gemini2-flash, alongside specialized understanding (e.g., Claude-3.5-Sonnet) and generation models (e.g., DALL-E-3). Our findings reveal substantial performance gaps in existing U-MLLMs, highlighting the need for more robust models capable of handling mixed-modality tasks effectively. The code and evaluation data can be found in <a target="_blank" rel="noopener" href="https://mme-unify.github.io/">https://mme-unify.github.io/</a>. </p>
<blockquote>
<p>ç°æœ‰çš„å¤šè¯­è¨€ä½èµ„æºæœºå™¨å­¦ä¹ ï¼ˆMLLMï¼‰åŸºå‡†æµ‹è¯•åœ¨è¯„ä¼°ç»Ÿä¸€MLLMï¼ˆU-MLLMsï¼‰æ—¶é¢ä¸´é‡å¤§æŒ‘æˆ˜ï¼ŒåŸå› å¦‚ä¸‹ï¼š1ï¼‰ç¼ºä¹ä¼ ç»Ÿä»»åŠ¡çš„æ ‡å‡†åŒ–åŸºå‡†æµ‹è¯•ï¼Œå¯¼è‡´æ¯”è¾ƒç»“æœä¸ä¸€è‡´ï¼›2ï¼‰ç¼ºå°‘æ··åˆæ¨¡å¼ç”Ÿæˆçš„åŸºå‡†æµ‹è¯•ï¼Œæ— æ³•è¯„ä¼°å¤šæ¨¡å¼æ¨ç†èƒ½åŠ›ã€‚æˆ‘ä»¬æå‡ºä¸€ä¸ªå…¨é¢çš„è¯„ä¼°æ¡†æ¶ï¼Œæ—¨åœ¨ç³»ç»Ÿåœ°è¯„ä¼°U-MLLMsã€‚æˆ‘ä»¬çš„åŸºå‡†æµ‹è¯•åŒ…æ‹¬ï¼šæ ‡å‡†åŒ–çš„ä¼ ç»Ÿä»»åŠ¡è¯„ä¼°ã€‚æˆ‘ä»¬ä»12ä¸ªæ•°æ®é›†ä¸­æŠ½æ ·ï¼Œæ¶µç›–10ä¸ªä»»åŠ¡åŠ30ä¸ªå­ä»»åŠ¡ï¼Œç¡®ä¿å„é¡¹ç ”ç©¶ä¹‹é—´çš„ä¸€è‡´æ€§å’Œå…¬å¹³æ¯”è¾ƒã€‚â€ 2. ç»Ÿä¸€ä»»åŠ¡è¯„ä¼°ã€‚æˆ‘ä»¬å¼•å…¥äº†äº”é¡¹æµ‹è¯•å¤šæ¨¡å¼æ¨ç†çš„æ–°ä»»åŠ¡ï¼ŒåŒ…æ‹¬å›¾åƒç¼–è¾‘ã€å¸¦å›¾åƒç”Ÿæˆå¸¸è¯†é—®ç­”å’Œå‡ ä½•æ¨ç†ç­‰ã€‚3. ç»¼åˆæ¨¡å‹åŸºå‡†æµ‹è¯•ã€‚æˆ‘ä»¬è¯„ä¼°äº†12æ¬¾é¢†å…ˆU-MLLMsï¼Œå¦‚Janus-Proã€EMU3ã€VILA-Uå’ŒGemini2-flashç­‰ï¼ŒåŒæ—¶é’ˆå¯¹ä¸“ä¸šç†è§£ï¼ˆå¦‚Claude-3.5-Sonnetï¼‰å’Œç”Ÿæˆæ¨¡å‹ï¼ˆå¦‚DALL-E-3ï¼‰ã€‚æˆ‘ä»¬çš„ç ”ç©¶å‘ç°äº†ç°æœ‰U-MLLMså­˜åœ¨çš„æ€§èƒ½å·®è·ï¼Œå¼ºè°ƒéœ€è¦æ›´ç¨³å¥çš„æ¨¡å‹ï¼Œèƒ½å¤Ÿæ›´æœ‰æ•ˆåœ°å¤„ç†æ··åˆæ¨¡å¼ä»»åŠ¡ã€‚ç›¸å…³ä»£ç å’Œè¯„ä¼°æ•°æ®å¯åœ¨<a target="_blank" rel="noopener" href="https://mme-unify.github.io/">https://mme-unify.github.io/</a>ä¸­æ‰¾åˆ°ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.03641v1">PDF</a> Project page: <a target="_blank" rel="noopener" href="https://mme-unify.github.io/">https://mme-unify.github.io/</a></p>
<p><strong>Summary</strong><br>éšç€å¤šä»»åŠ¡å­¦ä¹ æ¨¡å‹çš„å‘å±•ï¼Œç°æœ‰çš„åŸºå‡†æµ‹è¯•é¢ä¸´ç€è¯„ä»·ç»Ÿä¸€å¤šä»»åŠ¡å­¦ä¹ æ¨¡å‹ï¼ˆUnified MLLMsï¼Œç®€ç§°U-MLLMsï¼‰çš„æŒ‘æˆ˜ã€‚ç”±äºç¼ºä¹æ ‡å‡†åŒ–çš„ä¼ ç»Ÿä»»åŠ¡åŸºå‡†æµ‹è¯•å’Œæ··åˆæ¨¡æ€ç”ŸæˆåŸºå‡†æµ‹è¯•ï¼Œæ— æ³•å…¨é¢è¯„ä¼°U-MLLMsçš„èƒ½åŠ›ã€‚æœ¬ç ”ç©¶æå‡ºä¸€ä¸ªå…¨é¢çš„è¯„ä¼°æ¡†æ¶ï¼ŒåŒ…æ‹¬æ ‡å‡†åŒ–ä¼ ç»Ÿä»»åŠ¡è¯„ä¼°ã€ç»Ÿä¸€ä»»åŠ¡è¯„ä¼°å’Œç»¼åˆæ¨¡å‹è¯„ä¼°ã€‚è¯„ä¼°ç»“æœæ˜¾ç¤ºç°æœ‰U-MLLMså­˜åœ¨æ˜¾è‘—æ€§èƒ½å·®è·ï¼Œå¼ºè°ƒéœ€è¦æ›´ç¨³å¥çš„æ¨¡å‹ä»¥æœ‰æ•ˆå¤„ç†æ··åˆæ¨¡æ€ä»»åŠ¡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç¼ºä¹æ ‡å‡†åŒ–çš„ä¼ ç»Ÿä»»åŠ¡åŸºå‡†æµ‹è¯•å’Œæ··åˆæ¨¡æ€ç”ŸæˆåŸºå‡†æµ‹è¯•ï¼Œæ— æ³•å…¨é¢è¯„ä¼°Unified MLLMsï¼ˆU-MLLMsï¼‰ã€‚</li>
<li>æå‡ºçš„è¯„ä¼°æ¡†æ¶åŒ…æ‹¬æ ‡å‡†åŒ–ä¼ ç»Ÿä»»åŠ¡è¯„ä¼°ï¼Œæ¶µç›–12ä¸ªæ•°æ®é›†ï¼Œå…±10ä¸ªä»»åŠ¡ï¼Œ30ä¸ªå­ä»»åŠ¡ï¼Œä»¥ç¡®ä¿è·¨ç ”ç©¶çš„å…¬å¹³å’Œä¸€è‡´æ¯”è¾ƒã€‚</li>
<li>å¼•å…¥ç»Ÿä¸€ä»»åŠ¡è¯„ä¼°ï¼ŒåŒ…æ‹¬æµ‹è¯•å¤šæ¨¡æ€æ¨ç†çš„äº”ä¸ªæ–°ä»»åŠ¡ï¼Œå¦‚å›¾åƒç¼–è¾‘ã€å¸¸è¯†é—®ç­”ä¸å›¾åƒç”Ÿæˆã€å‡ ä½•æ¨ç†ç­‰ã€‚</li>
<li>ç»¼åˆæ¨¡å‹è¯„ä¼°åŒ…æ‹¬12æ¬¾é¢†å…ˆçš„U-MLLMsï¼Œå¦‚Janus-Proã€EMU3ã€VILA-Uã€Gemini2-flashç­‰ã€‚</li>
<li>ç ”ç©¶å‘ç°ç°æœ‰U-MLLMså­˜åœ¨æ˜¾è‘—æ€§èƒ½å·®è·ã€‚</li>
<li>éœ€è¦æ›´ç¨³å¥çš„æ¨¡å‹ä»¥å¤„ç†æ··åˆæ¨¡æ€ä»»åŠ¡ã€‚</li>
<li>è¯„ä¼°æ¡†æ¶çš„ä»£ç å’Œæ•°æ®å¯åœ¨<a target="_blank" rel="noopener" href="https://mme-unify.github.io/%E6%89%BE%E5%88%B0%E3%80%82">https://mme-unify.github.io/æ‰¾åˆ°ã€‚</a></li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.03641">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-72852e24914e99cd9aa5adf039b52a75.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-ee1dec5b8bc859f9e5517aec28d1c860.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-12345c79f3db49add1e20333b8dd867f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-59ce306fc08ce3b32797d8f13a0638a5.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Optimizing-Quantum-Circuits-via-ZX-Diagrams-using-Reinforcement-Learning-and-Graph-Neural-Networks"><a href="#Optimizing-Quantum-Circuits-via-ZX-Diagrams-using-Reinforcement-Learning-and-Graph-Neural-Networks" class="headerlink" title="Optimizing Quantum Circuits via ZX Diagrams using Reinforcement Learning   and Graph Neural Networks"></a>Optimizing Quantum Circuits via ZX Diagrams using Reinforcement Learning   and Graph Neural Networks</h2><p><strong>Authors:Alexander Mattick, Maniraman Periyasamy, Christian Ufrecht, Abhishek Y. Dubey, Christopher Mutschler, Axel Plinge, Daniel D. Scherer</strong></p>
<p>Quantum computing is currently strongly limited by the impact of noise, in particular introduced by the application of two-qubit gates. For this reason, reducing the number of two-qubit gates is of paramount importance on noisy intermediate-scale quantum hardware. To advance towards more reliable quantum computing, we introduce a framework based on ZX calculus, graph-neural networks and reinforcement learning for quantum circuit optimization. By combining reinforcement learning and tree search, our method addresses the challenge of selecting optimal sequences of ZX calculus rewrite rules. Instead of relying on existing heuristic rules for minimizing circuits, our method trains a novel reinforcement learning policy that directly operates on ZX-graphs, therefore allowing us to search through the space of all possible circuit transformations to find a circuit significantly minimizing the number of CNOT gates. This way we can scale beyond hard-coded rules towards discovering arbitrary optimization rules. We demonstrate our methodâ€™s competetiveness with state-of-the-art circuit optimizers and generalization capabilities on large sets of diverse random circuits. </p>
<blockquote>
<p>é‡å­è®¡ç®—ç›®å‰å—åˆ°å™ªå£°çš„ä¸¥é‡é™åˆ¶ï¼Œç‰¹åˆ«æ˜¯ä¸¤é‡å­ä½é—¨çš„åº”ç”¨æ‰€å¸¦æ¥çš„å™ªå£°ã€‚å› æ­¤ï¼Œåœ¨å¸¦æœ‰å™ªå£°çš„ä¸­é—´è§„æ¨¡é‡å­ç¡¬ä»¶ä¸Šå‡å°‘ä¸¤é‡å­ä½é—¨çš„ä½¿ç”¨è‡³å…³é‡è¦ã€‚ä¸ºäº†æœç€æ›´å¯é çš„é‡å­è®¡ç®—å‘å±•ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ªåŸºäºZXè®¡ç®—ã€å›¾ç¥ç»ç½‘ç»œå’Œå¼ºåŒ–å­¦ä¹ çš„é‡å­ç”µè·¯ä¼˜åŒ–æ¡†æ¶ã€‚é€šè¿‡ç»“åˆå¼ºåŒ–å­¦ä¹ å’Œæ ‘æœç´¢ï¼Œæˆ‘ä»¬çš„æ–¹æ³•è§£å†³äº†é€‰æ‹©ZXè®¡ç®—é‡å†™è§„åˆ™æœ€ä¼˜åºåˆ—çš„æŒ‘æˆ˜ã€‚æˆ‘ä»¬å¹¶ä¸ä¾èµ–ç°æœ‰çš„å¯å‘å¼è§„åˆ™æ¥æœ€å°åŒ–ç”µè·¯ï¼Œè€Œæ˜¯è®­ç»ƒä¸€ç§æ–°å‹å¼ºåŒ–å­¦ä¹ ç­–ç•¥ï¼Œç›´æ¥åœ¨ZXå›¾ä¸Šæ“ä½œï¼Œä»è€Œèƒ½å¤Ÿæœç´¢æ‰€æœ‰å¯èƒ½ç”µè·¯å˜æ¢çš„ç©ºé—´ï¼Œæ‰¾åˆ°èƒ½å¤Ÿæ˜¾è‘—å‡å°‘CNOTé—¨æ•°é‡çš„ç”µè·¯ã€‚é€šè¿‡è¿™ç§æ–¹å¼ï¼Œæˆ‘ä»¬å¯ä»¥è¶…è¶Šç¡¬ç¼–ç è§„åˆ™ï¼Œå‘ç°ä»»æ„ä¼˜åŒ–è§„åˆ™ã€‚æˆ‘ä»¬åœ¨å¤§å‹å¤šæ ·éšæœºç”µè·¯é›†ä¸Šå±•ç¤ºäº†è¯¥æ–¹æ³•ä¸æœ€æ–°ç”µè·¯ä¼˜åŒ–å™¨çš„ç«äº‰èƒ½åŠ›ä»¥åŠæ³›åŒ–èƒ½åŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.03429v1">PDF</a> </p>
<p><strong>Summary</strong>ï¼šé‡å­è®¡ç®—å—é™äºå™ªå£°å½±å“ï¼Œç‰¹åˆ«æ˜¯ä¸¤é‡å­æ¯”ç‰¹é—¨çš„åº”ç”¨æ‰€å¸¦æ¥çš„å™ªå£°ã€‚å› æ­¤ï¼Œåœ¨å™ªå£°ä¸­é—´æ€é‡å­ç¡¬ä»¶ä¸Šå‡å°‘ä¸¤é‡å­æ¯”ç‰¹é—¨æ•°é‡è‡³å…³é‡è¦ã€‚ä¸ºæ¨è¿›æ›´å¯é çš„é‡å­è®¡ç®—ï¼Œæœ¬æ–‡å¼•å…¥åŸºäºZXè®¡ç®—ã€å›¾ç¥ç»ç½‘ç»œå’Œå¼ºåŒ–å­¦ä¹ çš„é‡å­ç”µè·¯ä¼˜åŒ–æ¡†æ¶ã€‚é€šè¿‡ç»“åˆå¼ºåŒ–å­¦ä¹ å’Œæ ‘æœç´¢ï¼Œè¯¥æ–¹æ³•è§£å†³äº†é€‰æ‹©ZXè®¡ç®—é‡å†™è§„åˆ™åºåˆ—çš„æŒ‘æˆ˜æ€§é—®é¢˜ã€‚æœ¬æ–‡ä¸ä¾èµ–ç°æœ‰å¯å‘å¼è§„åˆ™æ¥æœ€å°åŒ–ç”µè·¯ï¼Œè€Œæ˜¯è®­ç»ƒä¸€ç§æ–°å‹å¼ºåŒ–å­¦ä¹ æ”¿ç­–ï¼Œç›´æ¥åœ¨ZXå›¾ä¸Šæ“ä½œï¼Œä»è€Œèƒ½å¤Ÿæœç´¢æ‰€æœ‰å¯èƒ½çš„ç”µè·¯è½¬æ¢ç©ºé—´ï¼Œæ‰¾åˆ°èƒ½æ˜¾è‘—å‡å°‘CNOTé—¨æ•°é‡çš„ç”µè·¯ã€‚è¿™æ ·æˆ‘ä»¬å°±èƒ½è¶…è¶Šç¡¬ç¼–ç è§„åˆ™ï¼Œå‘ç°ä»»æ„ä¼˜åŒ–è§„åˆ™ã€‚æˆ‘ä»¬åœ¨å¤§å‹å¤šæ ·çš„éšæœºç”µè·¯ä¸ŠéªŒè¯äº†è¯¥æ–¹æ³•çš„ç«äº‰åŠ›å’Œæ³›åŒ–èƒ½åŠ›ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>é‡å­è®¡ç®—å—é™äºå™ªå£°å½±å“ï¼Œç‰¹åˆ«æ˜¯åœ¨åº”ç”¨ä¸¤é‡å­æ¯”ç‰¹é—¨æ—¶äº§ç”Ÿçš„å™ªå£°ã€‚</li>
<li>åœ¨å™ªå£°ä¸­é—´æ€é‡å­ç¡¬ä»¶ä¸Šå‡å°‘ä¸¤é‡å­æ¯”ç‰¹é—¨çš„æ•°é‡è‡³å…³é‡è¦ã€‚</li>
<li>æœ¬æ–‡æå‡ºåŸºäºZXè®¡ç®—ã€å›¾ç¥ç»ç½‘ç»œå’Œå¼ºåŒ–å­¦ä¹ çš„é‡å­ç”µè·¯ä¼˜åŒ–æ¡†æ¶ã€‚</li>
<li>è¯¥æ–¹æ³•é€šè¿‡ç»“åˆå¼ºåŒ–å­¦ä¹ å’Œæ ‘æœç´¢ï¼Œè§£å†³äº†é€‰æ‹©ZXè®¡ç®—é‡å†™è§„åˆ™åºåˆ—çš„é—®é¢˜ã€‚</li>
<li>ä¸ä¾èµ–ç°æœ‰å¯å‘å¼è§„åˆ™æ¥æœ€å°åŒ–ç”µè·¯ï¼Œè€Œæ˜¯è®­ç»ƒä¸€ç§æ–°å‹å¼ºåŒ–å­¦ä¹ æ”¿ç­–ç›´æ¥åœ¨ZXå›¾ä¸Šæ“ä½œã€‚</li>
<li>è¯¥æ–¹æ³•èƒ½å¤Ÿæœç´¢æ‰€æœ‰å¯èƒ½çš„ç”µè·¯è½¬æ¢ç©ºé—´ï¼Œæ‰¾åˆ°æ˜¾è‘—å‡å°‘CNOTé—¨æ•°é‡çš„ç”µè·¯ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.03429">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-b025838ed06d3930042ffde91d4bad6e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-106d87da367164358eb609f385e7c65a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-3f749db532c301dc32a702e0fbbdd6b1.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f3dbf64feb145d4c712ed437865ebc1b.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="From-ChatGPT-to-DeepSeek-AI-A-Comprehensive-Analysis-of-Evolution-Deviation-and-Future-Implications-in-AI-Language-Models"><a href="#From-ChatGPT-to-DeepSeek-AI-A-Comprehensive-Analysis-of-Evolution-Deviation-and-Future-Implications-in-AI-Language-Models" class="headerlink" title="From ChatGPT to DeepSeek AI: A Comprehensive Analysis of Evolution,   Deviation, and Future Implications in AI-Language Models"></a>From ChatGPT to DeepSeek AI: A Comprehensive Analysis of Evolution,   Deviation, and Future Implications in AI-Language Models</h2><p><strong>Authors:Simrandeep Singh, Shreya Bansal, Abdulmotaleb El Saddik, Mukesh Saini</strong></p>
<p>The rapid advancement of artificial intelligence (AI) has reshaped the field of natural language processing (NLP), with models like OpenAI ChatGPT and DeepSeek AI. Although ChatGPT established a strong foundation for conversational AI, DeepSeek AI introduces significant improvements in architecture, performance, and ethical considerations. This paper presents a detailed analysis of the evolution from ChatGPT to DeepSeek AI, highlighting their technical differences, practical applications, and broader implications for AI development. To assess their capabilities, we conducted a case study using a predefined set of multiple choice questions in various domains, evaluating the strengths and limitations of each model. By examining these aspects, we provide valuable insight into the future trajectory of AI, its potential to transform industries, and key research directions for improving AI-driven language models. </p>
<blockquote>
<p>äººå·¥æ™ºèƒ½ï¼ˆAIï¼‰çš„å¿«é€Ÿå‘å±•å·²ç»é‡å¡‘äº†è‡ªç„¶è¯­è¨€å¤„ç†ï¼ˆNLPï¼‰é¢†åŸŸï¼Œå¦‚OpenAIçš„ChatGPTå’ŒDeepSeek AIç­‰æ¨¡å‹ã€‚è™½ç„¶ChatGPTä¸ºå¯¹è¯å¼äººå·¥æ™ºèƒ½å»ºç«‹äº†åšå®çš„åŸºç¡€ï¼Œä½†DeepSeek AIåœ¨æ¶æ„ã€æ€§èƒ½å’Œé“å¾·è€ƒé‡æ–¹é¢å¼•å…¥äº†é‡å¤§æ”¹è¿›ã€‚æœ¬æ–‡è¯¦ç»†åˆ†æäº†ä»ChatGPTåˆ°DeepSeek AIçš„æ¼”å˜è¿‡ç¨‹ï¼Œé‡ç‚¹ä»‹ç»äº†å®ƒä»¬çš„æŠ€æœ¯å·®å¼‚ã€å®é™…åº”ç”¨ä»¥åŠå¯¹äººå·¥æ™ºèƒ½å‘å±•çš„æ›´å¹¿æ³›å½±å“ã€‚ä¸ºäº†è¯„ä¼°å®ƒä»¬çš„èƒ½åŠ›ï¼Œæˆ‘ä»¬è¿›è¡Œäº†ä¸€é¡¹æ¡ˆä¾‹ç ”ç©¶ï¼Œä½¿ç”¨é¢„å®šçš„å¤šä¸ªé¢†åŸŸä¸­çš„å¤šé¡¹é€‰æ‹©é¢˜è¿›è¡Œè¯„ä¼°ï¼Œè¯„ä¼°æ¯ä¸ªæ¨¡å‹çš„ä¼˜ç‚¹å’Œå±€é™æ€§ã€‚é€šè¿‡è€ƒå¯Ÿè¿™äº›æ–¹é¢ï¼Œæˆ‘ä»¬ä¸ºAIçš„æœªæ¥èµ°å‘ã€å…¶æ”¹å˜è¡Œä¸šçš„æ½œåŠ›ä»¥åŠæ”¹è¿›AIé©±åŠ¨çš„è¯­è¨€æ¨¡å‹çš„å…³é”®ç ”ç©¶æ–¹å‘æä¾›äº†æœ‰ä»·å€¼çš„è§è§£ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.03219v1">PDF</a> 10 pages, 1 figure, 4 tables</p>
<p><strong>Summary</strong><br>äººå·¥æ™ºèƒ½çš„å¿«é€Ÿè¿›æ­¥å·²ç»é‡å¡‘äº†è‡ªç„¶è¯­è¨€å¤„ç†é¢†åŸŸï¼Œä»ChatGPTåˆ°DeepSeek AIçš„æ¼”å˜å±•ç¤ºäº†æ˜¾è‘—çš„è¿›æ­¥ã€‚æœ¬æ–‡è¯¦ç»†åˆ†æäº†ä¸¤è€…åœ¨æŠ€æœ¯ã€åº”ç”¨å’Œå¯¹AIå‘å±•çš„æ›´å¹¿æ³›å½±å“æ–¹é¢çš„å·®å¼‚ã€‚é€šè¿‡æ¡ˆä¾‹ç ”ç©¶ï¼Œæˆ‘ä»¬è¯„ä¼°äº†è¿™ä¸¤ç§æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¹¶æä¾›äº†å¯¹AIæœªæ¥å‘å±•æ–¹å‘ã€è¡Œä¸šå˜é©æ½œåŠ›ä»¥åŠæ”¹è¿›è¯­è¨€æ¨¡å‹çš„å…³é”®ç ”ç©¶æ–¹å‘çš„å®è´µè§è§£ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>äººå·¥æ™ºèƒ½ï¼ˆAIï¼‰çš„å¿«é€Ÿè¿›æ­¥å·²é‡å¡‘è‡ªç„¶è¯­è¨€å¤„ç†ï¼ˆNLPï¼‰é¢†åŸŸã€‚</li>
<li>ä»ChatGPTåˆ°DeepSeek AIçš„æ¼”å˜å±•ç¤ºäº†æ˜¾è‘—çš„æŠ€æœ¯è¿›æ­¥ã€‚</li>
<li>DeepSeek AIåœ¨æ¶æ„ã€æ€§èƒ½å’Œä¼¦ç†è€ƒé‡æ–¹é¢è¿›è¡Œäº†é‡å¤§æ”¹è¿›ã€‚</li>
<li>é€šè¿‡æ¡ˆä¾‹ç ”ç©¶ï¼Œå¯¹ChatGPTå’ŒDeepSeek AIçš„èƒ½åŠ›è¿›è¡Œäº†è¯„ä¼°ã€‚</li>
<li>ä¸¤è€…åœ¨æŠ€æœ¯ã€åº”ç”¨å’Œå¯¹AIå‘å±•çš„æ›´å¹¿æ³›å½±å“æ–¹é¢å­˜åœ¨å·®å¼‚ã€‚</li>
<li>AIæœ‰æ½œåŠ›æ”¹å˜å„è¡Œå„ä¸šï¼Œå¹¶æä¾›äº†å…³äºæœªæ¥AIå‘å±•æ–¹å‘çš„å…³é”®è§è§£ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.03219">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-dabf5572feec648db1695c3533726b1e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6ba79011e47333e2288c9194feed5e53.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-0142568f8f7a3988d6f072a81b3240f3.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="NuScenes-SpatialQA-A-Spatial-Understanding-and-Reasoning-Benchmark-for-Vision-Language-Models-in-Autonomous-Driving"><a href="#NuScenes-SpatialQA-A-Spatial-Understanding-and-Reasoning-Benchmark-for-Vision-Language-Models-in-Autonomous-Driving" class="headerlink" title="NuScenes-SpatialQA: A Spatial Understanding and Reasoning Benchmark for   Vision-Language Models in Autonomous Driving"></a>NuScenes-SpatialQA: A Spatial Understanding and Reasoning Benchmark for   Vision-Language Models in Autonomous Driving</h2><p><strong>Authors:Kexin Tian, Jingrui Mao, Yunlong Zhang, Jiwan Jiang, Yang Zhou, Zhengzhong Tu</strong></p>
<p>Recent advancements in Vision-Language Models (VLMs) have demonstrated strong potential for autonomous driving tasks. However, their spatial understanding and reasoning-key capabilities for autonomous driving-still exhibit significant limitations. Notably, none of the existing benchmarks systematically evaluate VLMsâ€™ spatial reasoning capabilities in driving scenarios. To fill this gap, we propose NuScenes-SpatialQA, the first large-scale ground-truth-based Question-Answer (QA) benchmark specifically designed to evaluate the spatial understanding and reasoning capabilities of VLMs in autonomous driving. Built upon the NuScenes dataset, the benchmark is constructed through an automated 3D scene graph generation pipeline and a QA generation pipeline. The benchmark systematically evaluates VLMsâ€™ performance in both spatial understanding and reasoning across multiple dimensions. Using this benchmark, we conduct extensive experiments on diverse VLMs, including both general and spatial-enhanced models, providing the first comprehensive evaluation of their spatial capabilities in autonomous driving. Surprisingly, the experimental results show that the spatial-enhanced VLM outperforms in qualitative QA but does not demonstrate competitiveness in quantitative QA. In general, VLMs still face considerable challenges in spatial understanding and reasoning. </p>
<blockquote>
<p>è¿‘æœŸè§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰çš„è¿›å±•ä¸ºè‡ªåŠ¨é©¾é©¶ä»»åŠ¡å±•ç°äº†å¼ºå¤§çš„æ½œåŠ›ã€‚ç„¶è€Œï¼Œå®ƒä»¬å¯¹ç©ºé—´çš„ç†è§£å’Œæ¨ç†â€”â€”è‡ªåŠ¨é©¾é©¶çš„å…³é”®èƒ½åŠ›â€”â€”ä»ç„¶å­˜åœ¨ç€æ˜æ˜¾çš„å±€é™ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œç°æœ‰çš„åŸºå‡†æµ‹è¯•å¹¶æ²¡æœ‰ç³»ç»Ÿåœ°è¯„ä¼°VLMåœ¨é©¾é©¶åœºæ™¯ä¸­çš„ç©ºé—´æ¨ç†èƒ½åŠ›ã€‚ä¸ºäº†å¡«è¡¥è¿™ä¸€ç©ºç™½ï¼Œæˆ‘ä»¬æå‡ºäº†NuScenes-SpatialQAï¼Œè¿™æ˜¯ç¬¬ä¸€ä¸ªåŸºäºå¤§è§„æ¨¡çœŸå®æ•°æ®çš„é—®ç­”ï¼ˆQAï¼‰åŸºå‡†æµ‹è¯•ï¼Œä¸“é—¨ç”¨äºè¯„ä¼°VLMåœ¨è‡ªåŠ¨é©¾é©¶ä¸­çš„ç©ºé—´ç†è§£å’Œæ¨ç†èƒ½åŠ›ã€‚è¯¥åŸºå‡†æµ‹è¯•å»ºç«‹åœ¨NuScenesæ•°æ®é›†ä¹‹ä¸Šï¼Œé€šè¿‡è‡ªåŠ¨åŒ–çš„3Dåœºæ™¯å›¾ç”Ÿæˆæµç¨‹å’ŒQAç”Ÿæˆæµç¨‹æ„å»ºã€‚è¯¥åŸºå‡†æµ‹è¯•ç³»ç»Ÿåœ°è¯„ä¼°äº†VLMåœ¨å¤šä¸ªç»´åº¦ä¸Šçš„ç©ºé—´ç†è§£å’Œæ¨ç†èƒ½åŠ›ã€‚ä½¿ç”¨æ­¤åŸºå‡†æµ‹è¯•ï¼Œæˆ‘ä»¬å¯¹å¤šç§VLMè¿›è¡Œäº†å¹¿æ³›çš„å®éªŒï¼ŒåŒ…æ‹¬é€šç”¨æ¨¡å‹å’Œç©ºé—´å¢å¼ºæ¨¡å‹ï¼Œé¦–æ¬¡å…¨é¢è¯„ä¼°äº†å®ƒä»¬åœ¨è‡ªåŠ¨é©¾é©¶ä¸­çš„ç©ºé—´èƒ½åŠ›ã€‚ä»¤äººæƒŠè®¶çš„æ˜¯ï¼Œå®éªŒç»“æœè¡¨æ˜ï¼Œç©ºé—´å¢å¼ºVLMåœ¨å®šæ€§QAä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œä½†åœ¨å®šé‡QAä¸Šå¹¶æ²¡æœ‰è¡¨ç°å‡ºç«äº‰åŠ›ã€‚æ€»çš„æ¥è¯´ï¼ŒVLMåœ¨ç©ºé—´ç†è§£å’Œæ¨ç†æ–¹é¢ä»ç„¶é¢ä¸´ç€å·¨å¤§çš„æŒ‘æˆ˜ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.03164v1">PDF</a> </p>
<p><strong>Summary</strong><br>è‡ªåŠ¨åŒ–é©¾é©¶ä¸­çš„è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰è™½æœ‰æ‰€è¿›å±•ï¼Œä½†åœ¨ç©ºé—´ç†è§£å’Œæ¨ç†æ–¹é¢ä»å­˜åœ¨æ˜¾è‘—å±€é™ï¼Œç›®å‰å°šæ— ç³»ç»Ÿè¯„ä¼°å…¶åœ¨é©¾é©¶åœºæ™¯ä¸­çš„ç©ºé—´æ¨ç†èƒ½åŠ›çš„åŸºå‡†æµ‹è¯•ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†NuScenes-SpatialQAåŸºå‡†æµ‹è¯•ï¼Œå®ƒæ˜¯åŸºäºå¤§è§„æ¨¡çœŸå®åœºæ™¯çš„è‡ªåŠ¨é©¾é©¶é—®ç­”æ•°æ®é›†ï¼Œæ—¨åœ¨è¯„ä¼°VLMsåœ¨ç©ºé—´ç†è§£å’Œæ¨ç†æ–¹é¢çš„æ€§èƒ½ã€‚å®éªŒè¡¨æ˜ï¼Œç©ºé—´å¢å¼ºçš„VLMåœ¨å®šæ€§é—®ç­”ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œä½†åœ¨å®šé‡é—®ç­”ä¸Šå¹¶ä¸å…·å¤‡ç«äº‰åŠ›ã€‚æ€»ä½“è€Œè¨€ï¼ŒVLMsåœ¨è‡ªåŠ¨é©¾é©¶çš„ç©ºé—´ç†è§£å’Œæ¨ç†æ–¹é¢ä»é¢ä¸´å·¨å¤§æŒ‘æˆ˜ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>VLMsåœ¨è‡ªåŠ¨é©¾é©¶ä¸­çš„ç©ºé—´ç†è§£å’Œæ¨ç†èƒ½åŠ›å­˜åœ¨å±€é™ã€‚</li>
<li>ç›®å‰ç¼ºä¹ç³»ç»Ÿè¯„ä¼°VLMåœ¨é©¾é©¶åœºæ™¯ä¸­çš„ç©ºé—´æ¨ç†èƒ½åŠ›çš„åŸºå‡†æµ‹è¯•ã€‚</li>
<li>NuScenes-SpatialQAåŸºå‡†æµ‹è¯•æ—¨åœ¨è¯„ä¼°VLMsåœ¨ç©ºé—´ç†è§£å’Œæ¨ç†æ–¹é¢çš„æ€§èƒ½ã€‚</li>
<li>è¯¥åŸºå‡†æµ‹è¯•æ˜¯åŸºäºå¤§è§„æ¨¡çœŸå®åœºæ™¯çš„è‡ªåŠ¨é©¾é©¶é—®ç­”æ•°æ®é›†ã€‚</li>
<li>ç©ºé—´å¢å¼ºçš„VLMåœ¨å®šæ€§é—®ç­”ä¸Šè¡¨ç°è¾ƒå¥½ï¼Œä½†åœ¨å®šé‡é—®ç­”ä¸Šè¡¨ç°ä¸å¤Ÿç«äº‰åŠ›ã€‚</li>
<li>VLMsçš„è‡ªåŠ¨é©¾é©¶ç©ºé—´ç†è§£å’Œæ¨ç†èƒ½åŠ›ä»éœ€è¿›ä¸€æ­¥æé«˜ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.03164">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-a6efe1b670e25b67f1291e142816c3d4.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c4c916e76fb4f4757f21e676cc3545d3.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-8838cec395a93ab66ab3d258fbd498b3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-508822a848f20464e6a6c4edffe9e22a.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="DeepResearcher-Scaling-Deep-Research-via-Reinforcement-Learning-in-Real-world-Environments"><a href="#DeepResearcher-Scaling-Deep-Research-via-Reinforcement-Learning-in-Real-world-Environments" class="headerlink" title="DeepResearcher: Scaling Deep Research via Reinforcement Learning in   Real-world Environments"></a>DeepResearcher: Scaling Deep Research via Reinforcement Learning in   Real-world Environments</h2><p><strong>Authors:Yuxiang Zheng, Dayuan Fu, Xiangkun Hu, Xiaojie Cai, Lyumanshan Ye, Pengrui Lu, Pengfei Liu</strong></p>
<p>Large Language Models (LLMs) equipped with web search capabilities have demonstrated impressive potential for deep research tasks. However, current approaches predominantly rely on either manually engineered prompts (prompt engineering-based) with brittle performance or reinforcement learning within controlled Retrieval-Augmented Generation (RAG) environments (RAG-based) that fail to capture the complexities of real-world interaction. In this paper, we introduce DeepResearcher, the first comprehensive framework for end-to-end training of LLM-based deep research agents through scaling reinforcement learning (RL) in real-world environments with authentic web search interactions. Unlike RAG-based approaches that assume all necessary information exists within a fixed corpus, our method trains agents to navigate the noisy, unstructured, and dynamic nature of the open web. We implement a specialized multi-agent architecture where browsing agents extract relevant information from various webpage structures and overcoming significant technical challenges. Extensive experiments on open-domain research tasks demonstrate that DeepResearcher achieves substantial improvements of up to 28.9 points over prompt engineering-based baselines and up to 7.2 points over RAG-based RL agents. Our qualitative analysis reveals emergent cognitive behaviors from end-to-end RL training, including the ability to formulate plans, cross-validate information from multiple sources, engage in self-reflection to redirect research, and maintain honesty when unable to find definitive answers. Our results highlight that end-to-end training in real-world web environments is not merely an implementation detail but a fundamental requirement for developing robust research capabilities aligned with real-world applications. We release DeepResearcher at <a target="_blank" rel="noopener" href="https://github.com/GAIR-NLP/DeepResearcher">https://github.com/GAIR-NLP/DeepResearcher</a>. </p>
<blockquote>
<p>é…å¤‡ç½‘é¡µæœç´¢åŠŸèƒ½çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨æ·±åº¦ç ”ç©¶ä»»åŠ¡ä¸­è¡¨ç°å‡ºäº†ä»¤äººå°è±¡æ·±åˆ»çš„æ½œåŠ›ã€‚ç„¶è€Œï¼Œå½“å‰çš„æ–¹æ³•ä¸»è¦ä¾èµ–äºæ‰‹åŠ¨è®¾è®¡çš„æç¤ºï¼ˆåŸºäºæç¤ºçš„å·¥ç¨‹ï¼‰è¡¨ç°ä¸ç¨³å®šï¼Œæˆ–è€…åœ¨å—æ§çš„æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰ç¯å¢ƒä¸­ä½¿ç”¨å¼ºåŒ–å­¦ä¹ ï¼ˆåŸºäºRAGçš„æ–¹æ³•ï¼‰ï¼Œæ— æ³•æ•æ‰ç°å®ä¸–ç•Œä¸­äº’åŠ¨çš„å¤æ‚æ€§ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†DeepResearcherï¼Œè¿™æ˜¯ç¬¬ä¸€ä¸ªé€šè¿‡å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰åœ¨çœŸå®ä¸–ç•Œç¯å¢ƒä¸­è¿›è¡Œç«¯åˆ°ç«¯è®­ç»ƒLLMåŸºäºæ·±åº¦ç ”ç©¶äººå‘˜çš„å…¨é¢æ¡†æ¶ï¼Œé€šè¿‡çœŸå®çš„ç½‘ç»œæœç´¢äº’åŠ¨è¿›è¡Œæ‰©å±•ã€‚ä¸åŒäºå‡è®¾æ‰€æœ‰å¿…è¦ä¿¡æ¯éƒ½å­˜åœ¨äºå›ºå®šè¯­æ–™åº“ä¸­çš„RAGæ–¹æ³•ï¼Œæˆ‘ä»¬çš„æ–¹æ³•è®­ç»ƒç ”ç©¶äººå‘˜åœ¨å¼€æ”¾ç½‘ç»œçš„å˜ˆæ‚ã€éç»“æ„åŒ–å’ŒåŠ¨æ€ç‰¹æ€§ä¸­è¿›è¡Œå¯¼èˆªã€‚æˆ‘ä»¬å®ç°äº†ä¸€ç§ç‰¹æ®Šçš„å¤šæ™ºèƒ½ä½“æ¶æ„ï¼Œæµè§ˆæ™ºèƒ½ä½“ä»ä¸åŒçš„ç½‘é¡µç»“æ„ä¸­æå–ç›¸å…³ä¿¡æ¯ï¼Œå¹¶å…‹æœé‡å¤§æŠ€æœ¯æŒ‘æˆ˜ã€‚åœ¨å¼€æ”¾åŸŸç ”ç©¶ä»»åŠ¡ä¸Šçš„å¤§é‡å®éªŒè¡¨æ˜ï¼ŒDeepResearcherè¾ƒåŸºäºæç¤ºçš„å·¥ç¨‹åŸºçº¿å®ç°äº†é«˜è¾¾28.9ç‚¹çš„å®è´¨æ€§æ”¹è¿›ï¼Œè¾ƒåŸºäºRAGçš„RLæ™ºèƒ½ä½“æé«˜äº†é«˜è¾¾7.2ç‚¹ã€‚æˆ‘ä»¬çš„å®šæ€§åˆ†ææ­ç¤ºäº†æ¥è‡ªç«¯åˆ°ç«¯RLè®­ç»ƒçš„æ–°å…´è®¤çŸ¥è¡Œä¸ºï¼ŒåŒ…æ‹¬åˆ¶å®šè®¡åˆ’çš„èƒ½åŠ›ã€ä»å¤šä¸ªæ¥æºäº¤å‰éªŒè¯ä¿¡æ¯ã€è¿›è¡Œåæ€ä»¥è°ƒæ•´ç ”ç©¶æ–¹å‘ï¼Œä»¥åŠåœ¨æ— æ³•æ‰¾åˆ°æ˜ç¡®ç­”æ¡ˆæ—¶ä¿æŒè¯šå®ã€‚æˆ‘ä»¬çš„ç»“æœå¼ºè°ƒï¼Œåœ¨çœŸå®ä¸–ç•Œç½‘ç»œç¯å¢ƒä¸­è¿›è¡Œç«¯åˆ°ç«¯è®­ç»ƒä¸ä»…æ˜¯å®ç°ç»†èŠ‚ï¼Œè€Œä¸”æ˜¯å¼€å‘ä¸ç°å®ä¸–ç•Œåº”ç”¨ç›¸åŒ¹é…çš„ç¨³å¥ç ”ç©¶èƒ½åŠ›çš„æ ¹æœ¬è¦æ±‚ã€‚æˆ‘ä»¬åœ¨<a target="_blank" rel="noopener" href="https://github.com/GAIR-NLP/DeepResearcher%E4%B8%8A%E5%8F%91%E5%B8%83%E4%BA%86DeepResearcher%E3%80%82">https://github.com/GAIR-NLP/DeepResearcherä¸Šå‘å¸ƒäº†DeepResearcherã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.03160v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ç»“åˆç½‘é¡µæœç´¢èƒ½åŠ›åœ¨æ·±ç ”ä»»åŠ¡ä¸­å±•ç°å‡ºå·¨å¤§æ½œåŠ›ã€‚ç„¶è€Œï¼Œå½“å‰æ–¹æ³•ä¸»è¦ä¾èµ–æ‰‹åŠ¨å·¥ç¨‹æç¤ºæˆ–å¼ºåŒ–å­¦ä¹ åœ¨æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰ç¯å¢ƒä¸­çš„è®­ç»ƒï¼Œè¿™ä¸¤è€…å‡å­˜åœ¨å±€é™æ€§ã€‚æœ¬æ–‡æå‡ºDeepResearcheræ¡†æ¶ï¼Œé€šè¿‡å¼ºåŒ–å­¦ä¹ åœ¨çœŸå®ç¯å¢ƒä¸­çš„ç«¯åˆ°ç«¯è®­ç»ƒï¼Œå®ç°LLMåŸºç¡€ä¸Šçš„æ·±åº¦ç ”ç©¶ä»£ç†ã€‚ä¸åŸºäºRAGçš„æ–¹æ³•ä¸åŒï¼ŒDeepResearcherè®­ç»ƒä»£ç†ä»¥é€‚åº”å¼€æ”¾ç½‘é¡µçš„å™ªå£°ã€éç»“æ„åŒ–å’ŒåŠ¨æ€ç‰¹æ€§ã€‚é€šè¿‡ç‰¹æ®Šçš„å¤šä»£ç†æ¶æ„å’Œæµè§ˆä»£ç†ä»å„ç§ç½‘é¡µç»“æ„ä¸­æå–ç›¸å…³ä¿¡æ¯ï¼Œå…‹æœæŠ€æœ¯æŒ‘æˆ˜ã€‚åœ¨å¼€æ”¾åŸŸç ”ç©¶ä»»åŠ¡ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒDeepResearcherç›¸è¾ƒäºåŸºäºæç¤ºçš„å·¥ç¨‹æ–¹æ³•å’ŒåŸºäºRAGçš„RLä»£ç†ï¼Œåˆ†åˆ«å®ç°äº†é«˜è¾¾28.9ç‚¹å’Œ7.2ç‚¹çš„å®è´¨æ€§æ”¹è¿›ã€‚å®šæ€§åˆ†ææ˜¾ç¤ºï¼Œæ¥è‡ªç«¯åˆ°ç«¯RLè®­ç»ƒçš„è®¤çŸ¥è¡Œä¸ºæ­£åœ¨å½¢æˆï¼ŒåŒ…æ‹¬åˆ¶å®šè®¡åˆ’ã€è·¨æºéªŒè¯ä¿¡æ¯ã€è‡ªæˆ‘åæ€ä»¥è°ƒæ•´ç ”ç©¶æ–¹å‘ï¼Œä»¥åŠåœ¨æ— æ³•æ‰¾åˆ°æ˜ç¡®ç­”æ¡ˆæ—¶ä¿æŒè¯šå®ã€‚ç»“æœè¡¨æ˜ï¼Œåœ¨çœŸå®ç½‘é¡µç¯å¢ƒä¸­è¿›è¡Œç«¯åˆ°ç«¯è®­ç»ƒä¸ä»…æ˜¯å®ç°ç»†èŠ‚ï¼Œæ›´æ˜¯å¼€å‘ä¸ç°å®åº”ç”¨ç›¸åŒ¹é…çš„ç¨³å¥ç ”ç©¶èƒ½åŠ›çš„æ ¹æœ¬è¦æ±‚ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ç»“åˆç½‘é¡µæœç´¢èƒ½åŠ›åœ¨æ·±ç ”ä»»åŠ¡ä¸­å±•ç°æ½œåŠ›ã€‚</li>
<li>å½“å‰æ–¹æ³•ä¸»è¦ä¾èµ–æ‰‹åŠ¨å·¥ç¨‹æç¤ºæˆ–å¼ºåŒ–å­¦ä¹ åœ¨RAGç¯å¢ƒä¸­çš„è®­ç»ƒï¼Œå­˜åœ¨å±€é™æ€§ã€‚</li>
<li>DeepResearcheræ¡†æ¶é€šè¿‡å¼ºåŒ–å­¦ä¹ åœ¨çœŸå®ç¯å¢ƒä¸­çš„ç«¯åˆ°ç«¯è®­ç»ƒï¼Œå®ç°LLMåŸºç¡€ä¸Šçš„æ·±åº¦ç ”ç©¶ã€‚</li>
<li>DeepResearcheré€‚åº”å¼€æ”¾ç½‘é¡µçš„å™ªå£°ã€éç»“æ„åŒ–å’ŒåŠ¨æ€ç‰¹æ€§ã€‚</li>
<li>ç‰¹æ®Šçš„å¤šä»£ç†æ¶æ„å’Œæµè§ˆä»£ç†ä»å„ç§ç½‘é¡µç»“æ„ä¸­æå–ç›¸å…³ä¿¡æ¯ã€‚</li>
<li>åœ¨å¼€æ”¾åŸŸç ”ç©¶ä»»åŠ¡ä¸Šï¼ŒDeepResearcherå®ç°äº†å¯¹åŸºå‡†æ–¹æ³•å’ŒRAG-based RLä»£ç†çš„æ˜¾è‘—æ”¹è¿›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.03160">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-2ca0c8b5586771065a3b08a336ce5c3b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3a9cbea987dbce4d42078c13618172e0.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-be48e5abcc893510cef1f35226280b92.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="LightPROF-A-Lightweight-Reasoning-Framework-for-Large-Language-Model-on-Knowledge-Graph"><a href="#LightPROF-A-Lightweight-Reasoning-Framework-for-Large-Language-Model-on-Knowledge-Graph" class="headerlink" title="LightPROF: A Lightweight Reasoning Framework for Large Language Model on   Knowledge Graph"></a>LightPROF: A Lightweight Reasoning Framework for Large Language Model on   Knowledge Graph</h2><p><strong>Authors:Tu Ao, Yanhua Yu, Yuling Wang, Yang Deng, Zirui Guo, Liang Pang, Pinghui Wang, Tat-Seng Chua, Xiao Zhang, Zhen Cai</strong></p>
<p>Large Language Models (LLMs) have impressive capabilities in text understanding and zero-shot reasoning. However, delays in knowledge updates may cause them to reason incorrectly or produce harmful results. Knowledge Graphs (KGs) provide rich and reliable contextual information for the reasoning process of LLMs by structurally organizing and connecting a wide range of entities and relations. Existing KG-based LLM reasoning methods only inject KGsâ€™ knowledge into prompts in a textual form, ignoring its structural information. Moreover, they mostly rely on close-source models or open-source models with large parameters, which poses challenges to high resource consumption. To address this, we propose a novel Lightweight and efficient Prompt learning-ReasOning Framework for KGQA (LightPROF), which leverages the full potential of LLMs to tackle complex reasoning tasks in a parameter-efficient manner. Specifically, LightPROF follows a â€œRetrieve-Embed-Reason processâ€, first accurately, and stably retrieving the corresponding reasoning graph from the KG through retrieval module. Next, through a Transformer-based Knowledge Adapter, it finely extracts and integrates factual and structural information from the KG, then maps this information to the LLMâ€™s token embedding space, creating an LLM-friendly prompt to be used by the LLM for the final reasoning. Additionally, LightPROF only requires training Knowledge Adapter and can be compatible with any open-source LLM. Extensive experiments on two public KGQA benchmarks demonstrate that LightPROF achieves superior performance with small-scale LLMs. Furthermore, LightPROF shows significant advantages in terms of input token count and reasoning time. </p>
<blockquote>
<p>å¤§è§„æ¨¡è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨æ–‡æœ¬ç†è§£å’Œé›¶æ ·æœ¬æ¨ç†æ–¹é¢æ‹¥æœ‰ä»¤äººå°è±¡æ·±åˆ»çš„èƒ½åŠ›ã€‚ç„¶è€Œï¼ŒçŸ¥è¯†æ›´æ–°çš„å»¶è¿Ÿå¯èƒ½ä¼šå¯¼è‡´å®ƒä»¬æ¨ç†é”™è¯¯æˆ–äº§ç”Ÿæœ‰å®³çš„ç»“æœã€‚çŸ¥è¯†å›¾è°±ï¼ˆKGï¼‰é€šè¿‡ç»“æ„åŒ–åœ°ç»„ç»‡å’Œè¿æ¥å¹¿æ³›çš„å®ä½“å’Œå…³ç³»ï¼Œä¸ºLLMçš„æ¨ç†è¿‡ç¨‹æä¾›äº†ä¸°å¯Œå¯é çš„ä¸Šæ–‡ä¿¡æ¯ã€‚ç°æœ‰çš„åŸºäºçŸ¥è¯†å›¾è°±çš„LLMæ¨ç†æ–¹æ³•ä»…å°†çŸ¥è¯†å›¾è°±çš„çŸ¥è¯†ä»¥æ–‡æœ¬å½¢å¼æ³¨å…¥æç¤ºä¸­ï¼Œå¿½ç•¥äº†å…¶ç»“æ„ä¿¡æ¯ã€‚æ­¤å¤–ï¼Œå®ƒä»¬å¤§å¤šä¾èµ–äºå°é—­æºæ¨¡å‹æˆ–å‚æ•°åºå¤§çš„å¼€æºæ¨¡å‹ï¼Œè¿™å¸¦æ¥äº†é«˜èµ„æºæ¶ˆè€—çš„æŒ‘æˆ˜ã€‚é’ˆå¯¹è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°å‹çš„è½»é‡çº§ã€é«˜æ•ˆçš„çŸ¥è¯†é—®ç­”æç¤ºå­¦ä¹ æ¨ç†æ¡†æ¶ï¼ˆLightPROFï¼‰ã€‚LightPROFå……åˆ†å‘æŒ¥äº†LLMçš„æ½œåŠ›ï¼Œä»¥å‚æ•°é«˜æ•ˆçš„æ–¹å¼è§£å†³å¤æ‚çš„æ¨ç†ä»»åŠ¡ã€‚å…·ä½“æ¥è¯´ï¼ŒLightPROFéµå¾ªâ€œæ£€ç´¢-åµŒå…¥-æ¨ç†â€çš„æµç¨‹ï¼Œé¦–å…ˆå‡†ç¡®ç¨³å®šåœ°ä»çŸ¥è¯†å›¾è°±ä¸­æ£€ç´¢ç›¸åº”çš„æ¨ç†å›¾ã€‚æ¥ä¸‹æ¥ï¼Œé€šè¿‡ä¸€ä¸ªåŸºäºå˜å‹å™¨çš„çŸ¥è¯†é€‚é…å™¨ï¼Œå®ƒç²¾ç»†åœ°æå–å’Œæ•´åˆäº†çŸ¥è¯†å›¾è°±ä¸­çš„äº‹å®å’Œç»“æ„ä¿¡æ¯ï¼Œç„¶åå°†è¿™äº›ä¿¡æ¯æ˜ å°„åˆ°LLMçš„ä»¤ç‰ŒåµŒå…¥ç©ºé—´ï¼Œä¸ºLLMåˆ›å»ºå‹å¥½çš„æç¤ºä»¥è¿›è¡Œæœ€ç»ˆçš„æ¨ç†ã€‚æ­¤å¤–ï¼ŒLightPROFåªéœ€è¦è®­ç»ƒçŸ¥è¯†é€‚é…å™¨ï¼Œå¹¶èƒ½ä¸ä»»ä½•å¼€æºçš„LLMå…¼å®¹ã€‚åœ¨ä¸¤ä¸ªå…¬å…±çŸ¥è¯†å›¾è°±é—®ç­”åŸºå‡†æµ‹è¯•ä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼ŒLightPROFåœ¨å°è§„æ¨¡LLMä¸Šå®ç°äº†å“è¶Šçš„æ€§èƒ½ã€‚æ­¤å¤–ï¼ŒLightPROFåœ¨è¾“å…¥ä»¤ç‰Œè®¡æ•°å’Œæ¨ç†æ—¶é—´æ–¹é¢æ˜¾ç¤ºå‡ºæ˜¾è‘—çš„ä¼˜åŠ¿ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.03137v1">PDF</a> This paper has been accepted by AAAI 2025</p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨æ–‡æœ¬ç†è§£å’Œé›¶æ ·æœ¬æ¨ç†æ–¹é¢è¡¨ç°å‡ºå¼ºå¤§çš„èƒ½åŠ›ï¼Œä½†çŸ¥è¯†æ›´æ–°çš„å»¶è¿Ÿå¯èƒ½å¯¼è‡´å…¶æ¨ç†é”™è¯¯æˆ–äº§ç”Ÿæœ‰å®³ç»“æœã€‚çŸ¥è¯†å›¾è°±ï¼ˆKGï¼‰é€šè¿‡ç»“æ„åŒ–åœ°ç»„ç»‡å’Œè¿æ¥å¹¿æ³›çš„å®ä½“å’Œå…³ç³»ï¼Œä¸ºLLMçš„æ¨ç†è¿‡ç¨‹æä¾›äº†ä¸°å¯Œå¯é çš„ä¸Šæ–‡ä¿¡æ¯ã€‚ç°æœ‰çš„åŸºäºKGçš„LLMæ¨ç†æ–¹æ³•ä»…å°†KGçš„çŸ¥è¯†ä»¥æ–‡æœ¬å½¢å¼æ³¨å…¥æç¤ºä¸­ï¼Œå¿½ç•¥äº†å…¶ç»“æ„ä¿¡æ¯ï¼Œå¹¶ä¸”å®ƒä»¬å¤§å¤šä¾èµ–äºå°é—­æºæ¨¡å‹æˆ–å‚æ•°åºå¤§çš„å¼€æºæ¨¡å‹ï¼Œè¿™å¸¦æ¥äº†é«˜èµ„æºæ¶ˆè€—çš„æŒ‘æˆ˜ã€‚ä¸ºè§£å†³æ­¤é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªè½»é‡çº§ã€é«˜æ•ˆçš„çŸ¥è¯†é—®ç­”æ¨ç†æ¡†æ¶LightPROFï¼Œè¯¥æ¡†æ¶èƒ½å¤Ÿå……åˆ†åˆ©ç”¨LLMçš„ä¼˜åŠ¿æ¥è§£å†³å¤æ‚çš„æ¨ç†ä»»åŠ¡ï¼ŒåŒæ—¶ä»¥å‚æ•°é«˜æ•ˆçš„æ–¹å¼è¿›è¡Œã€‚LightPROFéµå¾ªâ€œæ£€ç´¢-åµŒå…¥-æ¨ç†â€çš„æµç¨‹ï¼Œé¦–å…ˆç¨³å®šåœ°ä»KGä¸­æ£€ç´¢å‡ºå¯¹åº”çš„æ¨ç†å›¾ï¼Œç„¶åé€šè¿‡åŸºäºTransformerçš„çŸ¥è¯†é€‚é…å™¨ç²¾ç»†æå–å’Œæ•´åˆäº‹å®å’Œç»“æ„ä¿¡æ¯ï¼Œå¹¶å°†å…¶æ˜ å°„åˆ°LLMçš„ä»¤ç‰ŒåµŒå…¥ç©ºé—´ï¼Œä¸ºLLMåˆ›å»ºå‹å¥½çš„æç¤ºä»¥è¿›è¡Œæœ€ç»ˆæ¨ç†ã€‚æ­¤å¤–ï¼ŒLightPROFåªéœ€è¦è®­ç»ƒçŸ¥è¯†é€‚é…å™¨ï¼Œå¹¶ä¸”å¯ä»¥ä¸ä»»ä½•å¼€æºLLMå…¼å®¹ã€‚åœ¨å…¬å…±KGQAåŸºå‡†æµ‹è¯•ä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼ŒLightPROFåœ¨å°è§„æ¨¡LLMä¸Šå®ç°äº†å“è¶Šçš„æ€§èƒ½ï¼Œå¹¶ä¸”åœ¨è¾“å…¥ä»¤ç‰Œè®¡æ•°å’Œæ¨ç†æ—¶é—´æ–¹é¢æ˜¾ç¤ºå‡ºæ˜¾è‘—ä¼˜åŠ¿ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLMsé¢ä¸´çŸ¥è¯†æ›´æ–°å»¶è¿Ÿå¯¼è‡´æ¨ç†é”™è¯¯çš„é—®é¢˜ã€‚</li>
<li>KGæä¾›ä¸°å¯Œçš„ä¸Šä¸‹æ–‡ä¿¡æ¯ä»¥å¢å¼ºLLMsçš„æ¨ç†èƒ½åŠ›ã€‚</li>
<li>ç°æœ‰æ–¹æ³•å¿½è§†KGçš„ç»“æ„ä¿¡æ¯ï¼Œå¹¶ä¸”ä¸»è¦ä¾èµ–å¤§å‚æ•°æ¨¡å‹å¯¼è‡´èµ„æºæ¶ˆè€—å¤§ã€‚</li>
<li>LightPROFæ¡†æ¶åˆ©ç”¨LLMsè§£å†³å¤æ‚æ¨ç†ä»»åŠ¡ï¼ŒåŒæ—¶ä»¥å‚æ•°é«˜æ•ˆçš„æ–¹å¼è¿›è¡Œã€‚</li>
<li>LightPROFé€šè¿‡â€œRetrieve-Embed-Reasonâ€æµç¨‹å¤„ç†KGæ•°æ®ï¼Œæé«˜æ¨ç†å‡†ç¡®æ€§ã€‚</li>
<li>LightPROFåªéœ€è®­ç»ƒçŸ¥è¯†é€‚é…å™¨ï¼Œä¸ä»»ä½•å¼€æºLLMå…¼å®¹ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.03137">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-45f4c6ba5a046a03d1da21075c6c0f40.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-86725eb30b752ec1870f183c6ba63460.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-1d85d4cd543e448b4617ec2bee3822dc.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-2d7e0759b943a2135f00c3e37ba5b12e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d9a81f757dc3b920154b0273fb3a5683.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="VARGPT-v1-1-Improve-Visual-Autoregressive-Large-Unified-Model-via-Iterative-Instruction-Tuning-and-Reinforcement-Learning"><a href="#VARGPT-v1-1-Improve-Visual-Autoregressive-Large-Unified-Model-via-Iterative-Instruction-Tuning-and-Reinforcement-Learning" class="headerlink" title="VARGPT-v1.1: Improve Visual Autoregressive Large Unified Model via   Iterative Instruction Tuning and Reinforcement Learning"></a>VARGPT-v1.1: Improve Visual Autoregressive Large Unified Model via   Iterative Instruction Tuning and Reinforcement Learning</h2><p><strong>Authors:Xianwei Zhuang, Yuxin Xie, Yufan Deng, Dongchao Yang, Liming Liang, Jinghan Ru, Yuguo Yin, Yuexian Zou</strong></p>
<p>In this work, we present VARGPT-v1.1, an advanced unified visual autoregressive model that builds upon our previous framework VARGPT. The model preserves the dual paradigm of next-token prediction for visual understanding and next-scale generation for image synthesis. Specifically, VARGPT-v1.1 integrates: (1) a novel training strategy combining iterative visual instruction tuning with reinforcement learning through Direct Preference Optimization (DPO), (2) an expanded training corpus containing 8.3M visual-generative instruction pairs, (3) an upgraded language model backbone using Qwen2, (4) enhanced image generation resolution, and (5) emergent image editing capabilities without architectural modifications. These advancements enable VARGPT-v1.1 to achieve state-of-the-art performance in multimodal understanding and text-to-image instruction-following tasks, demonstrating significant improvements in both comprehension and generation metrics. Notably, through visual instruction tuning, the model acquires image editing functionality while maintaining architectural consistency with its predecessor, revealing the potential for unified visual understanding, generation, and editing. Our findings suggest that well-designed unified visual autoregressive models can effectively adopt flexible training strategies from large language models (LLMs), exhibiting promising scalability. The codebase and model weights are publicly available at <a target="_blank" rel="noopener" href="https://github.com/VARGPT-family/VARGPT-v1.1">https://github.com/VARGPT-family/VARGPT-v1.1</a>. </p>
<blockquote>
<p>åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æ¨å‡ºäº†VARGPT-v1.1ï¼Œè¿™æ˜¯ä¸€æ¬¾åŸºäºæˆ‘ä»¬ä¹‹å‰æ¡†æ¶VARGPTçš„é«˜çº§ç»Ÿä¸€è§†è§‰è‡ªå›å½’æ¨¡å‹ã€‚è¯¥æ¨¡å‹ä¿ç•™äº†ç”¨äºè§†è§‰ç†è§£å’Œå›¾åƒåˆæˆçš„ä¸‹ä¸€ä¸ªä»¤ç‰Œé¢„æµ‹å’Œä¸‹ä¸€ä¸ªå°ºåº¦ç”Ÿæˆçš„åŒé‡èŒƒå¼ã€‚å…·ä½“æ¥è¯´ï¼ŒVARGPT-v1.1é›†æˆäº†ï¼šï¼ˆ1ï¼‰ä¸€ç§æ–°å‹è®­ç»ƒç­–ç•¥ï¼Œç»“åˆè¿­ä»£è§†è§‰æŒ‡ä»¤è°ƒæ•´ä¸é€šè¿‡ç›´æ¥åå¥½ä¼˜åŒ–ï¼ˆDPOï¼‰çš„å¼ºåŒ–å­¦ä¹ ï¼Œï¼ˆ2ï¼‰ä¸€ä¸ªåŒ…å«830ä¸‡è§†è§‰ç”ŸæˆæŒ‡ä»¤å¯¹çš„å¤§è§„æ¨¡è®­ç»ƒè¯­æ–™åº“ï¼Œï¼ˆ3ï¼‰ä½¿ç”¨Qwen2çš„å‡çº§è¯­è¨€æ¨¡å‹ä¸»å¹²ï¼Œï¼ˆ4ï¼‰å¢å¼ºçš„å›¾åƒç”Ÿæˆåˆ†è¾¨ç‡ï¼Œï¼ˆ5ï¼‰æ— éœ€æ¶æ„ä¿®æ”¹å³å¯å®ç°å›¾åƒç¼–è¾‘åŠŸèƒ½ã€‚è¿™äº›è¿›æ­¥ä½¿VARGPT-v1.1åœ¨å¤šæ¨¡æ€ç†è§£å’Œæ–‡æœ¬é©±åŠ¨å›¾åƒç”Ÿæˆä»»åŠ¡ä¸Šè¾¾åˆ°äº†æœ€æ–°æŠ€æœ¯æ°´å¹³ï¼Œåœ¨ç†è§£å’Œç”ŸæˆæŒ‡æ ‡ä¸Šéƒ½å–å¾—äº†æ˜¾è‘—æ”¹è¿›ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œé€šè¿‡è§†è§‰æŒ‡ä»¤è°ƒæ•´ï¼Œè¯¥æ¨¡å‹åœ¨ä¿æŒä¸å‰ä»£æ¨¡å‹æ¶æ„ä¸€è‡´çš„åŒæ—¶è·å¾—äº†å›¾åƒç¼–è¾‘åŠŸèƒ½ï¼Œå±•ç¤ºäº†ç»Ÿä¸€è§†è§‰ç†è§£ã€ç”Ÿæˆå’Œç¼–è¾‘çš„æ½œåŠ›ã€‚æˆ‘ä»¬çš„ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œç²¾å¿ƒè®¾è®¡çš„ä¸€ä½“åŒ–è§†è§‰è‡ªå›å½’æ¨¡å‹å¯ä»¥æœ‰æ•ˆåœ°é‡‡ç”¨å¤§å‹è¯­è¨€æ¨¡å‹çš„çµæ´»è®­ç»ƒç­–ç•¥ï¼Œæ˜¾ç¤ºå‡ºæœ‰å¸Œæœ›çš„å¯æ‰©å±•æ€§ã€‚ä»£ç åº“å’Œæ¨¡å‹æƒé‡å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/VARGPT-family/VARGPT-v1.1%E4%B8%8A%E5%85%AC%E5%BC%80%E8%8E%B7%E5%BE%97%E3%80%82">https://github.com/VARGPT-family/VARGPT-v1.1ä¸Šå…¬å¼€è·å¾—ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.02949v1">PDF</a> Code is available at: <a target="_blank" rel="noopener" href="https://github.com/VARGPT-family/VARGPT-v1.1">https://github.com/VARGPT-family/VARGPT-v1.1</a>.   arXiv admin note: text overlap with arXiv:2501.12327</p>
<p><strong>Summary</strong></p>
<p>åŸºäºä¹‹å‰æ¡†æ¶VARGPTçš„åŸºç¡€ä¸Šï¼Œæˆ‘ä»¬æ¨å‡ºäº†å…ˆè¿›çš„ä¸€ä½“åŒ–è§†è§‰è‡ªå›å½’æ¨¡å‹VARGPT-v1.1ã€‚æ­¤æ¨¡å‹æ—¢ä¿ç•™äº†è§†è§‰ç†è§£ä¸ä¸‹ä¸€ä»£å°ºåº¦ç”Ÿæˆçš„åŒé‡èŒƒå¼ï¼Œåˆé€šè¿‡æ•´åˆå¤šé¡¹æ–°æŠ€æœ¯å®ç°äº†å¤šæ–¹é¢çš„æå‡ã€‚åŒ…æ‹¬ç»“åˆè¿­ä»£è§†è§‰æŒ‡ä»¤è°ƒæ•´ä¸å¼ºåŒ–å­¦ä¹ çš„æ–°è®­ç»ƒç­–ç•¥ã€åŒ…å«830ä¸‡è§†è§‰ç”ŸæˆæŒ‡ä»¤å¯¹çš„å¤§å‹è®­ç»ƒè¯­æ–™åº“ã€å‡çº§çš„Qwen2è¯­è¨€æ¨¡å‹ä¸»å¹²ã€æå‡çš„å›¾åƒç”Ÿæˆåˆ†è¾¨ç‡ä»¥åŠæ— éœ€æ¶æ„ä¿®æ”¹å³å¯å®ç°çš„å›¾åƒç¼–è¾‘åŠŸèƒ½ç­‰ã€‚è¿™äº›è¿›æ­¥ä½¿å¾—VARGPT-v1.1åœ¨å¤šæ¨¡æ€ç†è§£å’Œæ–‡æœ¬é©±åŠ¨å›¾åƒæ“ä½œä»»åŠ¡ä¸Šè¾¾åˆ°äº†ä¸šç•Œé¢†å…ˆçš„æ°´å¹³ï¼Œå¹¶åœ¨ç†è§£å’Œç”ŸæˆæŒ‡æ ‡ä¸Šå®ç°äº†æ˜¾è‘—çš„æå‡ã€‚æ¨¡å‹é€šè¿‡è§†è§‰æŒ‡ä»¤è°ƒæ•´è·å¾—äº†å›¾åƒç¼–è¾‘åŠŸèƒ½ï¼ŒåŒæ—¶ä¿æŒäº†ä¸å‰ä»£æ¨¡å‹æ¶æ„çš„ä¸€è‡´æ€§ï¼Œå±•ç¤ºäº†å…¶åœ¨ç»Ÿä¸€è§†è§‰ç†è§£ã€ç”Ÿæˆå’Œç¼–è¾‘æ–¹é¢çš„æ½œåŠ›ã€‚æˆ‘ä»¬çš„ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œè®¾è®¡ç²¾è‰¯çš„ä¸€ä½“åŒ–è§†è§‰è‡ªå›å½’æ¨¡å‹å¯ä»¥æœ‰æ•ˆåœ°é‡‡ç”¨å¤§å‹è¯­è¨€æ¨¡å‹çš„çµæ´»è®­ç»ƒç­–ç•¥ï¼Œå¹¶å±•ç°å‡ºä»¤äººé¼“èˆçš„å¯æ‰©å±•æ€§ã€‚ç›¸å…³ä»£ç å’Œæ¨¡å‹æƒé‡å·²å…¬å¼€å‘å¸ƒåœ¨<a target="_blank" rel="noopener" href="https://github.com/VARGPT-family/VARGPT-v1.1%E3%80%82">https://github.com/VARGPT-family/VARGPT-v1.1ã€‚</a></p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>VARGPT-v1.1æ˜¯å…ˆè¿›çš„ä¸€ä½“åŒ–è§†è§‰è‡ªå›å½’æ¨¡å‹ï¼ŒåŸºäºVARGPTæ¡†æ¶å¼€å‘ã€‚</li>
<li>æ¨¡å‹é‡‡ç”¨ä¸‹ä¸€ä»£å°ºåº¦ç”Ÿæˆä¸è§†è§‰ç†è§£çš„åŒé‡èŒƒå¼ã€‚</li>
<li>æ¨¡å‹å…·å¤‡å¤šé¡¹æ–°æŠ€æœ¯æ•´åˆï¼ŒåŒ…æ‹¬æ–°çš„è®­ç»ƒç­–ç•¥ã€å¤§å‹è®­ç»ƒè¯­æ–™åº“ã€å‡çº§çš„è¯­è¨€æ¨¡å‹ä¸»å¹²ç­‰ã€‚</li>
<li>æ¨¡å‹å®ç°å›¾åƒç”Ÿæˆåˆ†è¾¨ç‡çš„æå‡ï¼Œå¹¶å…·å¤‡å›¾åƒç¼–è¾‘åŠŸèƒ½ï¼Œæ— éœ€è¿›è¡Œæ¶æ„ä¿®æ”¹ã€‚</li>
<li>VARGPT-v1.1åœ¨å¤šæ¨¡æ€ç†è§£å’Œæ–‡æœ¬é©±åŠ¨å›¾åƒæ“ä½œä»»åŠ¡ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œç†è§£å’Œç”ŸæˆæŒ‡æ ‡å‡æœ‰æ˜¾è‘—æå‡ã€‚</li>
<li>æ¨¡å‹é€šè¿‡è§†è§‰æŒ‡ä»¤è°ƒæ•´è·å¾—å›¾åƒç¼–è¾‘åŠŸèƒ½ï¼Œå±•ç¤ºç»Ÿä¸€è§†è§‰ç†è§£ã€ç”Ÿæˆå’Œç¼–è¾‘çš„æ½œåŠ›ã€‚</li>
<li>ç ”ç©¶æ˜¾ç¤ºï¼Œè¯¥æ¨¡å‹å¯ä»¥æœ‰æ•ˆåœ°é‡‡ç”¨å¤§å‹è¯­è¨€æ¨¡å‹çš„çµæ´»è®­ç»ƒç­–ç•¥ï¼Œå¹¶å±•ç°å‡ºè‰¯å¥½çš„å¯æ‰©å±•æ€§ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.02949">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-cd7c35d0e4cc95d17091a86603c17590.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a7053ab45848716e25d44944f6de50ed.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1e7bf662f14a655fc0866926a7abe4b7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a35dd5dd32cc2b2e96e2f6b2aeb4741f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-59dcc182bab0974665bc7129393aabb8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-73bcf67590fadabe21c9d1f391182a1d.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="OnRL-RAG-Real-Time-Personalized-Mental-Health-Dialogue-System"><a href="#OnRL-RAG-Real-Time-Personalized-Mental-Health-Dialogue-System" class="headerlink" title="OnRL-RAG: Real-Time Personalized Mental Health Dialogue System"></a>OnRL-RAG: Real-Time Personalized Mental Health Dialogue System</h2><p><strong>Authors:Ahsan Bilal, Beiyu Lin, Mehdi Zaeifi</strong></p>
<p>Large language models (LLMs) have been widely used for various tasks and applications. However, LLMs and fine-tuning are limited to the pre-trained data. For example, ChatGPTâ€™s world knowledge until 2021 can be outdated or inaccurate. To enhance the capabilities of LLMs, Retrieval-Augmented Generation (RAG), is proposed to augment LLMs with additional, new, latest details and information to LLMs. While RAG offers the correct information, it may not best present it, especially to different population groups with personalizations. Reinforcement Learning from Human Feedback (RLHF) adapts to user needs by aligning model responses with human preference through feedback loops. In real-life applications, such as mental health problems, a dynamic and feedback-based model would continuously adapt to new information and offer personalized assistance due to complex factors fluctuating in a daily environment. Thus, we propose an Online Reinforcement Learning-based Retrieval-Augmented Generation (OnRL-RAG) system to detect and personalize the responding systems to mental health problems, such as stress, anxiety, and depression. We use an open-source dataset collected from 2028 College Students with 28 survey questions for each student to demonstrate the performance of our proposed system with the existing systems. Our system achieves superior performance compared to standard RAG and simple LLM via GPT-4o, GPT-4o-mini, Gemini-1.5, and GPT-3.5. This work would open up the possibilities of real-life applications of LLMs for personalized services in the everyday environment. The results will also help researchers in the fields of sociology, psychology, and neuroscience to align their theories more closely with the actual human daily environment. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å·²è¢«å¹¿æ³›åº”ç”¨äºå„ç§ä»»åŠ¡å’Œåº”ç”¨ä¸­ã€‚ç„¶è€Œï¼ŒLLMå’Œå¾®è°ƒéƒ½å—é™äºé¢„è®­ç»ƒæ•°æ®ã€‚ä¾‹å¦‚ï¼ŒChatGPTæˆªè‡³2021å¹´çš„ä¸–ç•ŒçŸ¥è¯†å¯èƒ½å·²è¿‡æ—¶æˆ–ä¸å‡†ç¡®ã€‚ä¸ºäº†å¢å¼ºLLMçš„èƒ½åŠ›ï¼Œæå‡ºäº†æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰æ¥å‘LLMæ·»åŠ é¢å¤–ã€æœ€æ–°ã€æœ€æ–°çš„ç»†èŠ‚å’Œä¿¡æ¯ã€‚è™½ç„¶RAGæä¾›äº†æ­£ç¡®çš„ä¿¡æ¯ï¼Œä½†å®ƒå¯èƒ½æ— æ³•æœ€å¥½åœ°å‘ˆç°å®ƒï¼Œç‰¹åˆ«æ˜¯å¯¹äºå…·æœ‰ä¸ªæ€§åŒ–çš„ä¸åŒäººç¾¤ã€‚å¼ºåŒ–å­¦ä¹ ä»äººç±»åé¦ˆï¼ˆRLHFï¼‰é€šè¿‡åé¦ˆå¾ªç¯ä½¿æ¨¡å‹å“åº”ä¸äººç±»åå¥½ç›¸é€‚åº”ï¼Œä»è€Œé€‚åº”ç”¨æˆ·éœ€æ±‚ã€‚åœ¨ç°å®ç”Ÿæ´»åº”ç”¨ï¼Œå¦‚å¿ƒç†å¥åº·é—®é¢˜ä¸­ï¼Œä¸€ä¸ªåŠ¨æ€ä¸”åŸºäºåé¦ˆçš„æ¨¡å‹å°†æ ¹æ®æ—¥å¸¸ç¯å¢ƒä¸­çš„å¤æ‚å› ç´ ä¸æ–­é€‚åº”æ–°ä¿¡æ¯å¹¶æä¾›ä¸ªæ€§åŒ–å¸®åŠ©ã€‚å› æ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºåœ¨çº¿å¼ºåŒ–å­¦ä¹ çš„æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆOnRL-RAGï¼‰ç³»ç»Ÿï¼Œç”¨äºæ£€æµ‹å’Œåº”å¯¹å¿ƒç†å¥åº·é—®é¢˜ï¼Œå¦‚å‹åŠ›ã€ç„¦è™‘å’ŒæŠ‘éƒçš„å“åº”ç³»ç»Ÿã€‚æˆ‘ä»¬ä½¿ç”¨ä»2028åå¤§å­¦ç”Ÿæ”¶é›†çš„å¼€æºæ•°æ®é›†è¿›è¡Œæ¼”ç¤ºï¼Œæ¯ä¸ªå­¦ç”Ÿæ¥å—28ä¸ªè°ƒæŸ¥é—®é¢˜ä»¥å±•ç¤ºæˆ‘ä»¬æå‡ºçš„ç³»ç»Ÿä¸ç°æœ‰ç³»ç»Ÿçš„æ€§èƒ½ã€‚æˆ‘ä»¬çš„ç³»ç»Ÿç›¸è¾ƒäºæ ‡å‡†RAGå’Œç®€å•çš„LLMï¼ˆå¦‚GPT-4oã€GPT-4o-miniã€Gemini-1.5å’ŒGPT-3.5ï¼‰è¡¨ç°å‡ºå“è¶Šçš„æ€§èƒ½ã€‚è¿™é¡¹å·¥ä½œå°†ä¸ºLLMåœ¨ä¸ªæ€§åŒ–æœåŠ¡æ–¹é¢çš„ç°å®ç”Ÿæ´»åº”ç”¨å¼€è¾Ÿå¯èƒ½æ€§ï¼Œç‰¹åˆ«æ˜¯åœ¨æ—¥å¸¸ç¯å¢ƒä¸­ã€‚ç»“æœè¿˜å°†å¸®åŠ©ç¤¾ä¼šå­¦ã€å¿ƒç†å­¦å’Œç¥ç»ç§‘å­¦é¢†åŸŸçš„ç ”ç©¶äººå‘˜å°†å…¶ç†è®ºæ›´ç´§å¯†åœ°ä¸å®é™…çš„æ—¥å¸¸äººç±»ç¯å¢ƒç›¸ç»“åˆã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.02894v1">PDF</a> </p>
<p><strong>æ‘˜è¦</strong><br>å¤§å‹è¯­è¨€æ¨¡å‹å¹¿æ³›åº”ç”¨äºå„é¡¹ä»»åŠ¡å’Œåº”ç”¨ï¼Œä½†å…¶åŸºäºé¢„è®­ç»ƒæ•°æ®çš„å±€é™æ€§æ˜¾è‘—ã€‚ä¸ºå¢å¼ºå¤§å‹è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œæå‡ºæ£€ç´¢å¢å¼ºç”Ÿæˆæ³•ï¼Œå¯ä»¥ä¸ºå…¶å¢æ·»é¢å¤–æœ€æ–°ä¿¡æ¯ã€‚ç„¶è€Œï¼Œæ£€ç´¢å¢å¼ºç”Ÿæˆæ³•æœªå¿…èƒ½æœ€ä½³åœ°å‘ˆç°ä¿¡æ¯ï¼Œç‰¹åˆ«æ˜¯å¯¹ä¸åŒäººç¾¤ä¸ªæ€§åŒ–çš„éœ€æ±‚ã€‚å¼ºåŒ–å­¦ä¹ ä»äººç±»åé¦ˆé€‚åº”ç¬¦åˆç”¨æˆ·éœ€æ±‚ï¼Œé€šè¿‡å¯¹æ¨¡å‹å“åº”ä¸äººç±»åå¥½å¯¹é½çš„åé¦ˆå¾ªç¯å®ç°ã€‚é’ˆå¯¹å¿ƒç†å¥åº·é—®é¢˜çš„å®é™…åº”ç”¨åœºæ™¯ï¼Œæˆ‘ä»¬æå‡ºåŸºäºåœ¨çº¿å¼ºåŒ–å­¦ä¹ çš„æ£€ç´¢å¢å¼ºç”Ÿæˆç³»ç»Ÿï¼ˆOnRL-RAGï¼‰ï¼Œç”¨äºæ£€æµ‹å’Œä¸ªæ€§åŒ–åº”å¯¹å¦‚å‹åŠ›ã€ç„¦è™‘å’ŒæŠ‘éƒç­‰å¿ƒç†é—®é¢˜ã€‚ä½¿ç”¨æ”¶é›†çš„2028åå¤§å­¦ç”Ÿçš„å¼€æºæ•°æ®é›†å’Œæ¯é¡¹åŒ…å«28ä¸ªé—®é¢˜çš„é—®å·è°ƒæŸ¥å±•ç¤ºç³»ç»Ÿæ€§èƒ½ã€‚æœ¬ç³»ç»Ÿç›¸è¾ƒäºæ ‡å‡†æ£€ç´¢å¢å¼ºç”Ÿæˆã€ç®€å•çš„GPTå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆå¦‚GPT-4oï¼ŒGPT-4o miniå’ŒGPT-3.5ï¼‰ä»¥åŠGemini-1.5ç³»ç»Ÿè¡¨ç°ä¼˜è¶Šã€‚è¿™é¡¹å·¥ä½œå°†ä¸ºå¤§å‹è¯­è¨€æ¨¡å‹åœ¨ä¸ªæ€§åŒ–æœåŠ¡é¢†åŸŸçš„å®é™…åº”ç”¨å¼€è¾Ÿé“è·¯ï¼Œå¸®åŠ©ç¤¾ä¼šå­¦ã€å¿ƒç†å­¦å’Œç¥ç»ç§‘å­¦é¢†åŸŸçš„ç ”ç©¶è€…æ›´å¥½åœ°ç»“åˆç†è®ºå®è·µã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ul>
<li>å¤§å‹è¯­è¨€æ¨¡å‹å¹¿æ³›åº”ç”¨äºå„ç±»ä»»åŠ¡å’Œåº”ç”¨ï¼Œä½†å—é™äºé¢„è®­ç»ƒæ•°æ®ï¼Œå­˜åœ¨çŸ¥è¯†è¿‡æ—¶æˆ–ä¸å‡†ç¡®çš„é—®é¢˜ã€‚</li>
<li>æ£€ç´¢å¢å¼ºç”Ÿæˆæ³•ï¼ˆRAGï¼‰å¯å¼¥è¡¥å¤§å‹è¯­è¨€æ¨¡å‹çš„ä¸è¶³ï¼Œä¸ºå…¶å¢æ·»æœ€æ–°ä¿¡æ¯ã€‚</li>
<li>å¼ºåŒ–å­¦ä¹ ä»äººç±»åé¦ˆï¼ˆRLHFï¼‰é€‚åº”ç¬¦åˆç”¨æˆ·éœ€æ±‚ï¼Œé€šè¿‡åé¦ˆå¾ªç¯ä½¿æ¨¡å‹å“åº”ä¸äººç±»åå¥½å¯¹é½ã€‚</li>
<li>æå‡ºåœ¨çº¿å¼ºåŒ–å­¦ä¹ åŸºç¡€ä¸Šçš„æ£€ç´¢å¢å¼ºç”Ÿæˆç³»ç»Ÿï¼ˆOnRL-RAGï¼‰ï¼Œç”¨äºä¸ªæ€§åŒ–åº”å¯¹å¿ƒç†å¥åº·é—®é¢˜ï¼Œå¦‚å‹åŠ›ã€ç„¦è™‘å’ŒæŠ‘éƒç­‰ã€‚</li>
<li>ä½¿ç”¨å¤§å­¦ç”Ÿæ•°æ®é›†å±•ç¤ºç³»ç»Ÿæ€§èƒ½ï¼Œæœ¬ç³»ç»Ÿç›¸è¾ƒäºå…¶ä»–ç³»ç»Ÿå’Œå¤§å‹è¯­è¨€æ¨¡å‹è¡¨ç°ä¼˜è¶Šã€‚</li>
<li>æœ¬ç ”ç©¶å°†ä¸ºå¤§å‹è¯­è¨€æ¨¡å‹åœ¨ä¸ªæ€§åŒ–æœåŠ¡é¢†åŸŸæä¾›æ–°æ€è·¯ï¼Œæ¨è¿›ç¤¾ä¼šå­¦ã€å¿ƒç†å­¦å’Œç¥ç»ç§‘å­¦é¢†åŸŸç†è®ºä¸å®è·µçš„ç»“åˆã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.02894">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-b4a3da99b261fa5dac37fca7ad47284a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-7c7fee3d6574c7b44579867702d433e4.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-ec1ff3bb0a697938e71c8865e7f52fc9.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Scaling-Test-time-Compute-for-Low-resource-Languages-Multilingual-Reasoning-in-LLMs"><a href="#Scaling-Test-time-Compute-for-Low-resource-Languages-Multilingual-Reasoning-in-LLMs" class="headerlink" title="Scaling Test-time Compute for Low-resource Languages: Multilingual   Reasoning in LLMs"></a>Scaling Test-time Compute for Low-resource Languages: Multilingual   Reasoning in LLMs</h2><p><strong>Authors:Khanh-Tung Tran, Barry Oâ€™Sullivan, Hoang D. Nguyen</strong></p>
<p>Recent advances in test-time compute scaling have enabled Large Language Models (LLMs) to tackle deep reasoning tasks by generating a chain-of-thought (CoT) that includes trial and error, backtracking, and intermediate reasoning steps before producing the final answer. However, these techniques have been applied predominantly to popular languages, such as English, leaving reasoning in low-resource languages underexplored and misaligned. In this work, we investigate the multilingual mechanism by which LLMs internally operate in a latent space biased toward their inherently dominant language. To leverage this phenomenon for low-resource languages, we train models to generate the CoT in English while outputting the final response in the target language, given input in the low-resource language. Our experiments demonstrate that this approach, named English-Pivoted CoT Training, outperforms other baselines, including training to generate both the CoT and the final response solely in the target language, with up to 28.33% improvement. Further analysis provides novel insights into the relationships between reasoning and multilinguality of LLMs, prompting for better approaches in developing multilingual large reasoning models </p>
<blockquote>
<p>è¿‘æœŸæµ‹è¯•æ—¶é—´è®¡ç®—å°ºåº¦æ–¹é¢çš„è¿›å±•ä½¿å¾—å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰èƒ½å¤Ÿé€šè¿‡ç”ŸæˆåŒ…å«è¯•é”™ã€å›æº¯å’Œä¸­é—´æ¨ç†æ­¥éª¤çš„æ€ç»´é“¾ï¼ˆCoTï¼‰æ¥å®Œæˆæ·±åº¦æ¨ç†ä»»åŠ¡ï¼Œç„¶åç»™å‡ºæœ€ç»ˆç­”æ¡ˆã€‚ç„¶è€Œï¼Œè¿™äº›æŠ€æœ¯ä¸»è¦åº”ç”¨åœ¨äº†è‹±è¯­ç­‰æµè¡Œè¯­è¨€ä¸Šï¼Œå¯¼è‡´åœ¨ä½èµ„æºè¯­è¨€ä¸­çš„æ¨ç†æ¢ç´¢ä¸è¶³ä¸”å­˜åœ¨åå·®ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬ç ”ç©¶äº†LLMçš„å†…åœ¨å¤šå…ƒè¯­è¨€æœºåˆ¶ï¼Œå³å®ƒä»¬åœ¨æ½œåœ¨ç©ºé—´ä¸­çš„è¿è¡Œåå‘å…¶å›ºæœ‰ä¸»å¯¼è¯­è¨€çš„ç°è±¡ã€‚ä¸ºäº†åˆ©ç”¨è¿™ä¸€ä½èµ„æºè¯­è¨€ç°è±¡ï¼Œæˆ‘ä»¬è®­ç»ƒæ¨¡å‹åœ¨è‹±è¯­ä¸­ç”Ÿæˆæ€ç»´é“¾ï¼Œåœ¨ç›®æ ‡è¯­è¨€ä¸­è¾“å‡ºæœ€ç»ˆå›åº”ï¼Œä»¥ä½èµ„æºè¯­è¨€ä½œä¸ºè¾“å…¥ã€‚æˆ‘ä»¬çš„å®éªŒè¡¨æ˜ï¼Œåä¸ºè‹±è¯­ä¸ºè½´çš„æ€ç»´é“¾è®­ç»ƒæ–¹æ³•ä¼˜äºå…¶ä»–åŸºå‡†æµ‹è¯•æ–¹æ³•ï¼ŒåŒ…æ‹¬ä»…åœ¨ç›®æ ‡è¯­è¨€ä¸­ç”Ÿæˆæ€ç»´é“¾å’Œæœ€ç»ˆå›åº”çš„è®­ç»ƒæ–¹æ³•ï¼Œæœ€é«˜æå‡äº†28.33%ã€‚è¿›ä¸€æ­¥çš„åˆ†ææä¾›äº†å…³äºLLMæ¨ç†å’Œå¤šè¯­è¨€æ€§ä¹‹é—´å…³ç³»çš„å…¨æ–°è§è§£ï¼Œä¸ºå¼€å‘å¤šè¯­è¨€å¤§å‹æ¨ç†æ¨¡å‹æä¾›äº†æ›´å¥½çš„æ–¹æ³•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.02890v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨æµ‹è¯•æ—¶çš„è®¡ç®—æ‰©å±•èƒ½åŠ›è¿›æ­¥ï¼Œä½¿å…¶èƒ½å¤Ÿé€šè¿‡ç”ŸæˆåŒ…å«è¯•é”™ã€å›æº¯å’Œä¸­é—´æ¨ç†æ­¥éª¤çš„â€œæ€ç»´é“¾â€ï¼ˆCoTï¼‰æ¥åº”å¯¹æ·±åº¦æ¨ç†ä»»åŠ¡ã€‚ç„¶è€Œï¼Œè¿™äº›æŠ€æœ¯ä¸»è¦åº”ç”¨äºè‹±è¯­ç­‰æµè¡Œè¯­è¨€ï¼Œå¯¼è‡´ä½èµ„æºè¯­è¨€çš„æ¨ç†ç ”ç©¶ä¸è¶³å’Œå¯¹é½å›°éš¾ã€‚æœ¬ç ”ç©¶æ¢è®¨LLMåœ¨å¤šè¯­è¨€æœºåˆ¶ä¸‹å¦‚ä½•åœ¨å…¶åå‘æ¯è¯­çš„å†…éšç©ºé—´ä¸­è¿›è¡Œæ“ä½œã€‚ä¸ºäº†åˆ©ç”¨æ­¤ç°è±¡æœåŠ¡äºä½èµ„æºè¯­è¨€ï¼Œæˆ‘ä»¬è®­ç»ƒæ¨¡å‹åœ¨è‹±è¯­ç”Ÿæˆæ€ç»´é“¾ï¼Œå¹¶åœ¨ç›®æ ‡è¯­è¨€è¾“å‡ºæœ€ç»ˆå›åº”ã€‚å®éªŒæ˜¾ç¤ºï¼Œåä¸ºè‹±è¯­æ”¯ç‚¹æ€ç»´é“¾è®­ç»ƒçš„æ–¹æ³•ä¼˜äºå…¶ä»–åŸºå‡†æµ‹è¯•ï¼ŒåŒ…æ‹¬åœ¨ç›®æ ‡è¯­è¨€ä¸­ç”Ÿæˆæ€ç»´é“¾å’Œæœ€ç»ˆå›åº”çš„è®­ç»ƒæ–¹æ³•ï¼Œæé«˜äº†æœ€å¤šè¾¾28.33%ã€‚è¿›ä¸€æ­¥çš„åˆ†æä¸ºLLMçš„æ¨ç†å’Œå¤šè¯­è¨€èƒ½åŠ›ä¹‹é—´çš„å…³ç³»æä¾›äº†æ–°çš„è§è§£ï¼Œæç¤ºäº†å¼€å‘å¤šè¯­è¨€å¤§å‹æ¨ç†æ¨¡å‹çš„æ–°æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰é€šè¿‡ç”Ÿæˆæ€ç»´é“¾ï¼ˆCoTï¼‰å®Œæˆæ·±åº¦æ¨ç†ä»»åŠ¡ï¼ŒåŒ…æ‹¬è¯•é”™ã€å›æº¯å’Œä¸­é—´æ¨ç†æ­¥éª¤ã€‚</li>
<li>ç°æœ‰æŠ€æœ¯ä¸»è¦å…³æ³¨æµè¡Œè¯­è¨€ï¼Œå¯¼è‡´ä½èµ„æºè¯­è¨€çš„æ¨ç†ç ”ç©¶ä¸è¶³å’Œå¯¹é½å›°éš¾ã€‚</li>
<li>LLMåœ¨å¤šè¯­è¨€å¤„ç†æ—¶å­˜åœ¨å†…éšç©ºé—´åå‘æ¯è¯­çš„ç°è±¡ã€‚</li>
<li>é€šè¿‡åœ¨è‹±è¯­ç”Ÿæˆæ€ç»´é“¾å¹¶åœ¨ç›®æ ‡è¯­è¨€è¾“å‡ºæœ€ç»ˆå›åº”çš„è®­ç»ƒæ–¹å¼ï¼Œç§°ä¸ºè‹±è¯­æ”¯ç‚¹æ€ç»´é“¾è®­ç»ƒï¼Œè¡¨ç°ä¼˜äºå…¶ä»–è®­ç»ƒæ–¹æ³•ã€‚</li>
<li>è‹±è¯­æ”¯ç‚¹æ€ç»´é“¾è®­ç»ƒæ–¹æ³•æœ€å¤šå¯æé«˜28.33%çš„æ€§èƒ½ã€‚</li>
<li>åˆ†ææ­ç¤ºäº†LLMæ¨ç†å’Œå¤šè¯­è¨€èƒ½åŠ›ä¹‹é—´çš„æ–°å…³ç³»ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.02890">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-291425ffc003a4865bb6c62de6919ba5.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-40f3254bd205d04ec1b0acd87a5a26d9.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-9ecadb2222b6f245e2d09ab9ae8e0319.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-cb86bf151879d92a1052b22a510c73da.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-6cfa334968ebe5f37345924881fd8c88.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-000d87c8abb7bf2c3d14e170fab5f832.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-0907797d3e65420d6fe2f823e7230211.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="Rethinking-RL-Scaling-for-Vision-Language-Models-A-Transparent-From-Scratch-Framework-and-Comprehensive-Evaluation-Scheme"><a href="#Rethinking-RL-Scaling-for-Vision-Language-Models-A-Transparent-From-Scratch-Framework-and-Comprehensive-Evaluation-Scheme" class="headerlink" title="Rethinking RL Scaling for Vision Language Models: A Transparent,   From-Scratch Framework and Comprehensive Evaluation Scheme"></a>Rethinking RL Scaling for Vision Language Models: A Transparent,   From-Scratch Framework and Comprehensive Evaluation Scheme</h2><p><strong>Authors:Yan Ma, Steffi Chern, Xuyang Shen, Yiran Zhong, Pengfei Liu</strong></p>
<p>Reinforcement learning (RL) has recently shown strong potential in improving the reasoning capabilities of large language models and is now being actively extended to vision-language models (VLMs). However, existing RL applications in VLMs often rely on heavily engineered frameworks that hinder reproducibility and accessibility, while lacking standardized evaluation protocols, making it difficult to compare results or interpret training dynamics. This work introduces a transparent, from-scratch framework for RL in VLMs, offering a minimal yet functional four-step pipeline validated across multiple models and datasets. In addition, a standardized evaluation scheme is proposed to assess training dynamics and reflective behaviors. Extensive experiments on visual reasoning tasks uncover key empirical findings: response length is sensitive to random seeds, reflection correlates with output length, and RL consistently outperforms supervised fine-tuning (SFT) in generalization, even with high-quality data. These findings, together with the proposed framework, aim to establish a reproducible baseline and support broader engagement in RL-based VLM research. </p>
<blockquote>
<p>å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰åœ¨æå‡å¤§å‹è¯­è¨€æ¨¡å‹çš„æ¨ç†èƒ½åŠ›æ–¹é¢å·²æ˜¾ç¤ºå‡ºå¼ºå¤§çš„æ½œåŠ›ï¼Œç›®å‰æ­£åœ¨ç§¯ææ‹“å±•åˆ°è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰ã€‚ç„¶è€Œï¼Œç°æœ‰çš„è§†è§‰è¯­è¨€æ¨¡å‹ä¸­çš„å¼ºåŒ–å­¦ä¹ åº”ç”¨å¾€å¾€ä¾èµ–äºé‡åº¦å·¥ç¨‹çš„æ¡†æ¶ï¼Œè¿™äº›æ¡†æ¶é˜»ç¢äº†å¯é‡å¤æ€§å’Œå¯è®¿é—®æ€§ï¼ŒåŒæ—¶ç¼ºä¹æ ‡å‡†åŒ–çš„è¯„ä¼°åè®®ï¼Œä½¿å¾—éš¾ä»¥æ¯”è¾ƒç»“æœæˆ–è§£é‡Šè®­ç»ƒåŠ¨æ€ã€‚è¿™é¡¹å·¥ä½œå¼•å…¥äº†ä¸€ä¸ªé€æ˜ã€ä»å¤´å¼€å§‹çš„è§†è§‰è¯­è¨€æ¨¡å‹å¼ºåŒ–å­¦ä¹ æ¡†æ¶ï¼Œæä¾›äº†ä¸€ä¸ªç®€æ´è€Œå®ç”¨çš„å››æ­¥æµç¨‹ï¼Œå·²åœ¨å¤šä¸ªæ¨¡å‹å’Œæ•°æ®é›†ä¸Šå¾—åˆ°éªŒè¯ã€‚æ­¤å¤–ï¼Œè¿˜æå‡ºäº†ä¸€ç§æ ‡å‡†åŒ–çš„è¯„ä¼°æ–¹æ¡ˆï¼Œä»¥è¯„ä¼°è®­ç»ƒåŠ¨æ€å’Œåæ€è¡Œä¸ºã€‚åœ¨è§†è§‰æ¨ç†ä»»åŠ¡ä¸Šçš„å¤§é‡å®éªŒæ­ç¤ºäº†å…³é”®ç»éªŒå‘ç°ï¼šå“åº”é•¿åº¦å¯¹éšæœºç§å­æ•æ„Ÿï¼Œåæ€ä¸è¾“å‡ºé•¿åº¦ç›¸å…³ï¼Œå¼ºåŒ–å­¦ä¹ åœ¨æ³›åŒ–æ–¹é¢å§‹ç»ˆä¼˜äºæœ‰ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰ï¼Œå³ä½¿åœ¨é«˜è´¨é‡æ•°æ®çš„æƒ…å†µä¸‹ä¹Ÿæ˜¯å¦‚æ­¤ã€‚è¿™äº›å‘ç°ä¸æ‰€æå‡ºçš„æ¡†æ¶ä¸€èµ·ï¼Œæ—¨åœ¨å»ºç«‹ä¸€ä¸ªå¯é‡å¤çš„åŸºæœ¬çº¿ï¼Œå¹¶æ”¯æŒæ›´å¹¿æ³›åœ°å‚ä¸åŸºäºå¼ºåŒ–å­¦ä¹ çš„è§†è§‰è¯­è¨€æ¨¡å‹ç ”ç©¶ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.02587v2">PDF</a> Code is public and available at: <a target="_blank" rel="noopener" href="https://github.com/GAIR-NLP/MAYE">https://github.com/GAIR-NLP/MAYE</a></p>
<p><strong>Summary</strong></p>
<p>å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰åœ¨æå‡å¤§å‹è¯­è¨€æ¨¡å‹çš„æ¨ç†èƒ½åŠ›æ–¹é¢å±•ç°å‡ºå¼ºå¤§æ½œåŠ›ï¼Œç°åœ¨æ­£ç§¯ææ‰©å±•è‡³è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰ã€‚ç„¶è€Œï¼Œç°æœ‰çš„VLMsä¸­çš„RLåº”ç”¨å¸¸å¸¸ä¾èµ–äºé‡åº¦å·¥ç¨‹çš„æ¡†æ¶ï¼Œè¿™é˜»ç¢äº†å¯é‡å¤æ€§å’Œå¯è®¿é—®æ€§ï¼ŒåŒæ—¶ç¼ºä¹æ ‡å‡†åŒ–çš„è¯„ä¼°åè®®ï¼Œä½¿å¾—ç»“æœéš¾ä»¥æ¯”è¾ƒæˆ–è§£é‡Šè®­ç»ƒåŠ¨æ€ã€‚æœ¬ç ”ç©¶ä»‹ç»äº†ä¸€ä¸ªé€æ˜ã€ä»å¤´å¼€å§‹çš„RLåœ¨VLMsä¸­çš„æ¡†æ¶ï¼Œæä¾›äº†ä¸€ä¸ªç®€æ´è€Œå®ç”¨çš„å››æ­¥ç®¡é“ï¼Œå¹¶åœ¨å¤šä¸ªæ¨¡å‹å’Œæ•°æ®é›†ä¸Šè¿›è¡Œäº†éªŒè¯ã€‚æ­¤å¤–ï¼Œè¿˜æå‡ºäº†ä¸€ä¸ªæ ‡å‡†åŒ–çš„è¯„ä¼°æ–¹æ¡ˆï¼Œä»¥è¯„ä¼°è®­ç»ƒåŠ¨æ€å’Œåæ€è¡Œä¸ºã€‚åœ¨è§†è§‰æ¨ç†ä»»åŠ¡ä¸Šçš„å¤§é‡å®éªŒæ­ç¤ºäº†å…³é”®å®è¯å‘ç°ï¼šå“åº”é•¿åº¦å¯¹éšæœºç§å­æ•æ„Ÿï¼Œåæ€ä¸è¾“å‡ºé•¿åº¦ç›¸å…³ï¼ŒRLåœ¨æ³›åŒ–æ–¹é¢æŒç»­ä¼˜äºç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰ï¼Œå³ä½¿åœ¨é«˜è´¨é‡æ•°æ®çš„æƒ…å†µä¸‹ä¹Ÿæ˜¯å¦‚æ­¤ã€‚è¿™äº›å‘ç°ä¸æ‰€æå‡ºçš„æ¡†æ¶æ—¨åœ¨å»ºç«‹å¯é‡å¤æ€§çš„åŸºå‡†ï¼Œå¹¶æ”¯æŒæ›´å¹¿æ³›çš„RLåŸºäºVLMç ”ç©¶çš„å‚ä¸ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¼ºåŒ–å­¦ä¹ åœ¨æå‡è¯­è¨€æ¨¡å‹çš„æ¨ç†èƒ½åŠ›æ–¹é¢è¡¨ç°å‡ºå¼ºå¤§æ½œåŠ›ï¼Œæ­£åœ¨é€æ­¥åº”ç”¨äºè§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰ã€‚</li>
<li>ç°æœ‰çš„VLMsä¸­çš„RLåº”ç”¨æ¡†æ¶å¤æ‚ï¼Œç¼ºä¹é€æ˜åº¦å’Œå¯é‡å¤æ€§ã€‚</li>
<li>æœ¬ç ”ç©¶æä¾›äº†ä¸€ä¸ªç®€æ´çš„RLæ¡†æ¶ï¼Œé€‚ç”¨äºVLMsï¼Œå¹¶è¿›è¡Œäº†å¤šæ¨¡å‹å’Œè·¨æ•°æ®é›†çš„éªŒè¯ã€‚</li>
<li>æå‡ºäº†æ ‡å‡†åŒ–çš„è¯„ä¼°æ–¹æ¡ˆï¼Œä»¥æ›´å¥½åœ°è¯„ä¼°VLMsçš„è®­ç»ƒåŠ¨æ€å’Œåæ€è¡Œä¸ºã€‚</li>
<li>å®éªŒå‘ç°å“åº”é•¿åº¦å¯¹éšæœºç§å­æ•æ„Ÿï¼Œä¸”åæ€ä¸è¾“å‡ºé•¿åº¦æœ‰å…³è”ã€‚</li>
<li>åœ¨æ³›åŒ–æ–¹é¢ï¼Œå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰é€šå¸¸ä¼˜äºç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰ï¼Œå³ä½¿é¢å¯¹é«˜è´¨é‡æ•°æ®ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.02587">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-b471f75d1c60d78227503d97b3f2f630.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-7ec2419c2e740d43c240bf20d9e8ed45.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-14e9a423e86774d816c87aa01d144290.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f29cb0e278791f76c3eef943314e5e46.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="EPO-Explicit-Policy-Optimization-for-Strategic-Reasoning-in-LLMs-via-Reinforcement-Learning"><a href="#EPO-Explicit-Policy-Optimization-for-Strategic-Reasoning-in-LLMs-via-Reinforcement-Learning" class="headerlink" title="EPO: Explicit Policy Optimization for Strategic Reasoning in LLMs via   Reinforcement Learning"></a>EPO: Explicit Policy Optimization for Strategic Reasoning in LLMs via   Reinforcement Learning</h2><p><strong>Authors:Xiaoqian Liu, Ke Wang, Yongbin Li, Yuchuan Wu, Wentao Ma, Aobo Kong, Fei Huang, Jianbin Jiao, Junge Zhang</strong></p>
<p>Large Language Models (LLMs) have shown impressive reasoning capabilities in well-defined problems with clear solutions, such as mathematics and coding. However, they still struggle with complex real-world scenarios like business negotiations, which require strategic reasoning-an ability to navigate dynamic environments and align long-term goals amidst uncertainty. Existing methods for strategic reasoning face challenges in adaptability, scalability, and transferring strategies to new contexts. To address these issues, we propose explicit policy optimization (EPO) for strategic reasoning, featuring an LLM that provides strategies in open-ended action space and can be plugged into arbitrary LLM agents to motivate goal-directed behavior. To improve adaptability and policy transferability, we train the strategic reasoning model via multi-turn reinforcement learning (RL) using process rewards and iterative self-play, without supervised fine-tuning (SFT) as a preliminary step. Experiments across social and physical domains demonstrate EPOâ€™s ability of long-term goal alignment through enhanced strategic reasoning, achieving state-of-the-art performance on social dialogue and web navigation tasks. Our findings reveal various collaborative reasoning mechanisms emergent in EPO and its effectiveness in generating novel strategies, underscoring its potential for strategic reasoning in real-world applications. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨å®šä¹‰æ˜ç¡®ã€è§£å†³æ–¹æ¡ˆæ¸…æ™°çš„é—®é¢˜ä¸­å±•ç¤ºäº†ä»¤äººå°è±¡æ·±åˆ»çš„æ¨ç†èƒ½åŠ›ï¼Œä¾‹å¦‚åœ¨æ•°å­¦å’Œç¼–ç¨‹æ–¹é¢ã€‚ç„¶è€Œï¼Œå®ƒä»¬åœ¨å¤„ç†å¤æ‚çš„ç°å®ä¸–ç•Œåœºæ™¯ï¼Œå¦‚å•†åŠ¡è°ˆåˆ¤ç­‰ï¼Œä»ç„¶é¢ä¸´æŒ‘æˆ˜ã€‚è¿™äº›åœºæ™¯éœ€è¦æˆ˜ç•¥æ¨ç†èƒ½åŠ›ï¼Œå³åœ¨åŠ¨æ€ç¯å¢ƒä¸­å¯¼èˆªå¹¶åœ¨ä¸ç¡®å®šæ€§ä¸­å®ç°é•¿æœŸç›®æ ‡çš„èƒ½åŠ›ã€‚ç°æœ‰çš„æˆ˜ç•¥æ¨ç†æ–¹æ³•é¢ä¸´ç€é€‚åº”æ€§ã€å¯æ‰©å±•æ€§å’Œç­–ç•¥è½¬ç§»ç­‰æ–¹é¢çš„æŒ‘æˆ˜ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†é’ˆå¯¹æˆ˜ç•¥æ¨ç†çš„æ˜¾å¼ç­–ç•¥ä¼˜åŒ–ï¼ˆEPOï¼‰æ–¹æ³•ã€‚è¯¥æ–¹æ³•ä½¿ç”¨LLMåœ¨å¼€æ”¾åŠ¨ä½œç©ºé—´ä¸­æä¾›ç­–ç•¥ï¼Œå¹¶å¯æ’å…¥åˆ°ä»»æ„LLMä»£ç†ä¸­ä»¥æ¿€åŠ±ç›®æ ‡å¯¼å‘è¡Œä¸ºã€‚ä¸ºäº†æ”¹å–„é€‚åº”æ€§å’Œç­–ç•¥å¯è½¬ç§»æ€§ï¼Œæˆ‘ä»¬é€šè¿‡å¤šå›åˆå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰è®­ç»ƒæˆ˜ç•¥æ¨ç†æ¨¡å‹ï¼Œä½¿ç”¨è¿‡ç¨‹å¥–åŠ±å’Œè¿­ä»£è‡ªæˆ‘å¯¹æŠ—ï¼Œæ— éœ€é¢„å…ˆè¿›è¡Œæœ‰ç›‘ç£çš„å¾®è°ƒï¼ˆSFTï¼‰ã€‚åœ¨ç¤¾ä¼šå’Œç‰©ç†é¢†åŸŸçš„å®éªŒè¡¨æ˜ï¼ŒEPOé€šè¿‡å¢å¼ºçš„æˆ˜ç•¥æ¨ç†å®ç°äº†é•¿æœŸç›®æ ‡å¯¹é½çš„èƒ½åŠ›ï¼Œåœ¨ç¤¾ä¼šå¯¹è¯å’Œç½‘é¡µå¯¼èˆªä»»åŠ¡ä¸Šè¾¾åˆ°äº†æœ€æ–°æŠ€æœ¯æ°´å¹³ã€‚æˆ‘ä»¬çš„ç ”ç©¶æ­ç¤ºäº†EPOä¸­æ¶Œç°çš„å„ç§åä½œæ¨ç†æœºåˆ¶åŠå…¶ç”Ÿæˆæ–°ç­–ç•¥çš„æœ‰æ•ˆæ€§ï¼Œçªæ˜¾äº†å…¶åœ¨ç°å®ä¸–ç•Œåº”ç”¨ä¸­çš„æˆ˜ç•¥æ¨ç†æ½œåŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.12486v3">PDF</a> 22 pages, 4 figures</p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨å…·æœ‰æ˜ç¡®è§£å†³æ–¹æ¡ˆçš„æ˜ç¡®é—®é¢˜ä¸­å±•ç°å‡ºä»¤äººå°è±¡æ·±åˆ»çš„æ¨ç†èƒ½åŠ›ï¼Œå¦‚æ•°å­¦å’Œç¼–ç¨‹ã€‚ç„¶è€Œï¼Œåœ¨é¢å¯¹éœ€è¦æˆ˜ç•¥æ¨ç†çš„å¤æ‚ç°å®ä¸–ç•Œåœºæ™¯ï¼Œå¦‚å•†ä¸šè°ˆåˆ¤ç­‰ï¼ŒLLMsä»é¢ä¸´æŒ‘æˆ˜ã€‚æˆ˜ç•¥æ¨ç†è¦æ±‚æ¨¡å‹èƒ½å¤Ÿåœ¨åŠ¨æ€ç¯å¢ƒä¸­å¯¼èˆªï¼Œå¹¶åœ¨ä¸ç¡®å®šæ€§ä¸­è°ƒæ•´é•¿æœŸç›®æ ‡ã€‚ä¸ºè§£å†³ç°æœ‰æˆ˜ç•¥æ¨ç†æ–¹æ³•é¢ä¸´çš„å¯é€‚åº”æ€§ã€å¯æ‰©å±•æ€§å’Œç­–ç•¥è½¬ç§»ç­‰æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†æ˜¾å¼ç­–ç•¥ä¼˜åŒ–ï¼ˆEPOï¼‰æ–¹æ³•ã€‚è¯¥æ–¹æ³•ç»“åˆäº†LLMï¼Œèƒ½åœ¨å¼€æ”¾è¡ŒåŠ¨ç©ºé—´ä¸­æä¾›ç­–ç•¥ï¼Œå¹¶å¯æ’å…¥åˆ°ä»»æ„LLMä»£ç†ä¸­ä»¥é©±åŠ¨ç›®æ ‡å¯¼å‘è¡Œä¸ºã€‚é€šè¿‡å¤šå›åˆå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰è¿›è¡Œè®­ç»ƒï¼Œä»¥æé«˜æ¨¡å‹çš„é€‚åº”æ€§å’Œç­–ç•¥å¯è½¬ç§»æ€§ã€‚å®éªŒè¡¨æ˜ï¼ŒEPOåœ¨ç¤¾äº¤å’Œç‰©ç†é¢†åŸŸå…·æœ‰é•¿æœŸç›®æ ‡å¯¹é½èƒ½åŠ›ï¼Œå¹¶åœ¨ç¤¾ä¼šå¯¹è¯å’Œç½‘é¡µå¯¼èˆªä»»åŠ¡ä¸Šå®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚ç ”ç©¶ç»“æœæ­ç¤ºäº†EPOä¸­æ¶Œç°çš„å„ç§åä½œæ¨ç†æœºåˆ¶åŠå…¶ç”Ÿæˆæ–°ç­–ç•¥çš„æœ‰æ•ˆæ€§ï¼Œçªæ˜¾å…¶åœ¨ç°å®ä¸–ç•Œåº”ç”¨ä¸­çš„æˆ˜ç•¥æ¨ç†æ½œåŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨æ˜ç¡®é—®é¢˜ä¸­å±•ç°å‡ºå¼ºå¤§çš„æ¨ç†èƒ½åŠ›ï¼Œä½†åœ¨éœ€è¦æˆ˜ç•¥æ¨ç†çš„å¤æ‚ç°å®ä¸–ç•Œåœºæ™¯ï¼ˆå¦‚å•†ä¸šè°ˆåˆ¤ï¼‰ä¸­ä»é¢ä¸´æŒ‘æˆ˜ã€‚</li>
<li>æˆ˜ç•¥æ¨ç†è¦æ±‚æ¨¡å‹èƒ½å¤Ÿåœ¨åŠ¨æ€ç¯å¢ƒä¸­å¯¼èˆªï¼Œå¹¶åœ¨ä¸ç¡®å®šæ€§ä¸­è°ƒæ•´é•¿æœŸç›®æ ‡ã€‚</li>
<li>ç°æœ‰çš„æˆ˜ç•¥æ¨ç†æ–¹æ³•é¢ä¸´å¯é€‚åº”æ€§ã€å¯æ‰©å±•æ€§å’Œç­–ç•¥è½¬ç§»ç­‰æŒ‘æˆ˜ã€‚</li>
<li>æ˜¾å¼ç­–ç•¥ä¼˜åŒ–ï¼ˆEPOï¼‰æ–¹æ³•é€šè¿‡ç»“åˆLLMï¼Œèƒ½åœ¨å¼€æ”¾è¡ŒåŠ¨ç©ºé—´ä¸­æä¾›ç­–ç•¥ï¼Œå¹¶ä¿ƒè¿›ç›®æ ‡å¯¼å‘è¡Œä¸ºã€‚</li>
<li>EPOé€šè¿‡å¤šå›åˆå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰è¿›è¡Œè®­ç»ƒï¼Œä»¥æé«˜æ¨¡å‹çš„é€‚åº”æ€§å’Œç­–ç•¥å¯è½¬ç§»æ€§ã€‚</li>
<li>å®éªŒè¡¨æ˜ï¼ŒEPOåœ¨ç¤¾äº¤å’Œç‰©ç†é¢†åŸŸçš„é•¿æœŸç›®æ ‡å¯¹é½èƒ½åŠ›å‡ºè‰²ï¼Œç¤¾ä¼šå¯¹è¯å’Œç½‘é¡µå¯¼èˆªä»»åŠ¡æ€§èƒ½å…ˆè¿›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.12486">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-97fa27e0b488f8873ea961838a4c126a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-8b362a1bd20af45a1de73d92c66aa409.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-8f087bb632fb79e939e2ff04b4f3fcd0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-97694c8c8837f774c6734c675b5d24d4.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="Quantifying-the-Capability-Boundary-of-DeepSeek-Models-An-Application-Driven-Performance-Analysis"><a href="#Quantifying-the-Capability-Boundary-of-DeepSeek-Models-An-Application-Driven-Performance-Analysis" class="headerlink" title="Quantifying the Capability Boundary of DeepSeek Models: An   Application-Driven Performance Analysis"></a>Quantifying the Capability Boundary of DeepSeek Models: An   Application-Driven Performance Analysis</h2><p><strong>Authors:Kaikai Zhao, Zhaoxiang Liu, Xuejiao Lei, Jiaojiao Zhao, Zhenhong Long, Zipeng Wang, Ning Wang, Meijuan An, Qingliang Meng, Peijun Yang, Minjie Hua, Chaoyang Ma, Wen Liu, Kai Wang, Shiguo Lian</strong></p>
<p>DeepSeek-R1, known for its low training cost and exceptional reasoning capabilities, has achieved state-of-the-art performance on various benchmarks. However, detailed evaluations for DeepSeek Series models from the perspective of real-world applications are lacking, making it challenging for users to select the most suitable DeepSeek models for their specific needs. To address this gap, we conduct a systematic evaluation of the DeepSeek-V3, DeepSeek-R1, DeepSeek-R1-Distill-Qwen series, DeepSeek-R1-Distill-Llama series, their corresponding 4-bit quantized models, and the reasoning model QwQ-32B using the enhanced A-Eval benchmark, A-Eval-2.0. Through a comparative analysis of original instruction-tuned models and their distilled counterparts, we investigate how reasoning enhancements impact performance across diverse practical tasks. To assist users in model selection, we quantify the capability boundary of DeepSeek models through performance tier classifications. Based on the quantification results, we develop a model selection handbook that clearly illustrates the relation among models, their capabilities and practical applications. This handbook enables users to select the most cost-effective models without efforts, ensuring optimal performance and resource efficiency in real-world applications. It should be noted that, despite our efforts to establish a comprehensive, objective, and authoritative evaluation benchmark, the selection of test samples, characteristics of data distribution, and the setting of evaluation criteria may inevitably introduce certain biases into the evaluation results. We will continuously optimize the evaluation benchmarks and periodically update this paper to provide more comprehensive and accurate evaluation results. Please refer to the latest version of the paper for the most current results and conclusions. </p>
<blockquote>
<p>DeepSeek-R1ä»¥å…¶ä½è®­ç»ƒæˆæœ¬å’Œå‡ºè‰²çš„æ¨ç†èƒ½åŠ›è€Œé—»åï¼Œå·²åœ¨å„ç§åŸºå‡†æµ‹è¯•ä¸­å®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚ç„¶è€Œï¼Œä»å®é™…åº”ç”¨çš„è§’åº¦å¯¹DeepSeekç³»åˆ—æ¨¡å‹çš„è¯¦ç»†è¯„ä¼°ä»ç„¶ç¼ºä¹ï¼Œè¿™ä½¿å¾—ç”¨æˆ·éš¾ä»¥ä¸ºå…¶ç‰¹å®šéœ€æ±‚é€‰æ‹©æœ€åˆé€‚çš„DeepSeekæ¨¡å‹ã€‚ä¸ºäº†å¼¥è¡¥è¿™ä¸€ç©ºç™½ï¼Œæˆ‘ä»¬å¯¹DeepSeek-V3ã€DeepSeek-R1ã€DeepSeek-R1-Distill-Qwenç³»åˆ—ã€DeepSeek-R1-Distill-Llamaç³»åˆ—ã€å…¶ç›¸åº”çš„4ä½é‡åŒ–æ¨¡å‹ä»¥åŠæ¨ç†æ¨¡å‹QwQ-32Bï¼Œä½¿ç”¨å¢å¼ºçš„A-EvalåŸºå‡†æµ‹è¯•A-Eval-2.0è¿›è¡Œäº†ç³»ç»Ÿè¯„ä¼°ã€‚é€šè¿‡å¯¹æ¯”åŸå§‹æŒ‡ä»¤è°ƒæ•´æ¨¡å‹å’Œå®ƒä»¬è’¸é¦åçš„å¯¹åº”æ¨¡å‹ï¼Œæˆ‘ä»¬ç ”ç©¶äº†æ¨ç†å¢å¼ºå¦‚ä½•å½±å“å„ç§å®é™…ä»»åŠ¡çš„æ€§èƒ½ã€‚ä¸ºäº†å¸®åŠ©ç”¨æˆ·é€‰æ‹©æ¨¡å‹ï¼Œæˆ‘ä»¬é€šè¿‡æ€§èƒ½åˆ†çº§åˆ†ç±»æ¥é‡åŒ–DeepSeekæ¨¡å‹çš„èƒ½åŠ›è¾¹ç•Œã€‚åŸºäºé‡åŒ–ç»“æœï¼Œæˆ‘ä»¬å¼€å‘äº†æ¨¡å‹é€‰æ‹©æ‰‹å†Œï¼Œè¯¥æ‰‹å†Œæ¸…æ™°åœ°è¯´æ˜äº†æ¨¡å‹ä¹‹é—´çš„å…³ç³»ã€å®ƒä»¬çš„èƒ½åŠ›ä»¥åŠå®é™…åº”ç”¨ã€‚è¯¥æ‰‹å†Œä½¿ç”¨æˆ·èƒ½å¤Ÿè½»æ¾é€‰æ‹©æœ€å…·æˆæœ¬æ•ˆç›Šçš„æ¨¡å‹ï¼Œç¡®ä¿åœ¨çœŸå®ä¸–ç•Œåº”ç”¨ä¸­å®ç°æœ€ä½³æ€§èƒ½å’Œèµ„æºæ•ˆç‡ã€‚åº”å½“æ³¨æ„çš„æ˜¯ï¼Œå°½ç®¡æˆ‘ä»¬åŠªåŠ›å»ºç«‹å…¨é¢ã€å®¢è§‚ã€æƒå¨çš„è¯„ä¼°åŸºå‡†ï¼Œä½†æµ‹è¯•æ ·æœ¬çš„é€‰æ‹©ã€æ•°æ®åˆ†å¸ƒçš„ç‰¹å¾ä»¥åŠè¯„ä¼°æ ‡å‡†çš„è®¾å®šä¸å¯é¿å…åœ°ä¼šå¯¹è¯„ä¼°ç»“æœå¼•å…¥ä¸€å®šçš„åè§ã€‚æˆ‘ä»¬å°†ä¸æ–­ä¼˜åŒ–è¯„ä¼°åŸºå‡†å¹¶å®šæœŸæ›´æ–°æœ¬æ–‡ï¼Œä»¥æä¾›æ›´å…¨é¢å’Œå‡†ç¡®çš„è¯„ä¼°ç»“æœã€‚æœ‰å…³æœ€æ–°ç»“æœå’Œç»“è®ºï¼Œè¯·å‚é˜…æœ¬æ–‡çš„æœ€æ–°ç‰ˆæœ¬ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.11164v4">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>DeepSeekç³»åˆ—æ¨¡å‹å…·å¤‡ä½æˆæœ¬å’Œé«˜æ¨ç†èƒ½åŠ›ï¼Œå·²åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¾¾åˆ°é¢†å…ˆæ°´å¹³ã€‚ç„¶è€Œï¼Œé’ˆå¯¹è¿™äº›æ¨¡å‹åœ¨çœŸå®ä¸–ç•Œåº”ç”¨ä¸­çš„è¯¦ç»†è¯„ä¼°ä»æ˜¾ä¸è¶³ï¼Œå¯¼è‡´ç”¨æˆ·éš¾ä»¥é€‰æ‹©æœ€é€‚åˆå…¶ç‰¹å®šéœ€æ±‚çš„DeepSeekæ¨¡å‹ã€‚ä¸ºè§£å†³æ­¤é—®é¢˜ï¼Œç ”ç©¶å›¢é˜Ÿå¯¹DeepSeek-V3ã€DeepSeek-R1ã€DeepSeek-R1-Distill-Qwenç³»åˆ—ã€DeepSeek-R1-Distill-Llamaç³»åˆ—åŠå…¶å¯¹åº”çš„4ä½é‡åŒ–æ¨¡å‹ï¼Œä»¥åŠæ¨ç†æ¨¡å‹QwQ-32Bè¿›è¡Œäº†ç³»ç»Ÿæ€§çš„è¯„ä¼°ã€‚ç ”ç©¶é‡‡ç”¨å¢å¼ºçš„A-EvalåŸºå‡†æµ‹è¯•A-Eval-2.0è¿›è¡Œè¯„ä¼°ï¼Œå¹¶æ¯”è¾ƒäº†åŸå§‹æŒ‡ä»¤è°ƒæ•´æ¨¡å‹å’Œè’¸é¦åæ¨¡å‹çš„è¡¨ç°ï¼Œæ¢è®¨äº†æ¨ç†å¢å¼ºå¦‚ä½•å½±å“å„ç§å®é™…ä»»åŠ¡çš„æ€§èƒ½ã€‚æ­¤å¤–ï¼Œç ”ç©¶è¿˜é€šè¿‡æ€§èƒ½åˆ†çº§åˆ†ç±»ç¡®å®šäº†DeepSeekæ¨¡å‹çš„èƒ½åŠ›è¾¹ç•Œï¼Œå¹¶æ®æ­¤åˆ¶å®šäº†æ¨¡å‹é€‰æ‹©æ‰‹å†Œï¼Œä»¥å¸®åŠ©ç”¨æˆ·è½»æ¾é€‰æ‹©æœ€å…·æˆæœ¬æ•ˆç›Šçš„æ¨¡å‹ï¼Œç¡®ä¿åœ¨çœŸå®ä¸–ç•Œåº”ç”¨ä¸­å®ç°æœ€ä½³æ€§èƒ½å’Œèµ„æºæ•ˆç‡ã€‚ä½†è¯„ä¼°è¿‡ç¨‹ä¸­å¯èƒ½å­˜åœ¨æµ‹è¯•æ ·æœ¬é€‰æ‹©ã€æ•°æ®åˆ†å¸ƒç‰¹å¾å’Œè¯„ä¼°æ ‡å‡†è®¾ç½®ç­‰æ–¹é¢çš„åè§ï¼Œå›¢é˜Ÿå°†æŒç»­ä¼˜åŒ–è¯„ä¼°åŸºå‡†å¹¶å®šæœŸæ›´æ–°æ­¤è®ºæ–‡ä»¥æä¾›æ›´å…¨é¢å’Œå‡†ç¡®çš„è¯„ä¼°ç»“æœã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>DeepSeekç³»åˆ—æ¨¡å‹åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºå“è¶Šæ€§èƒ½ï¼Œä½†çœŸå®ä¸–ç•Œåº”ç”¨çš„è¯¦ç»†è¯„ä¼°ä»ä¸è¶³ã€‚</li>
<li>ç ”ç©¶å¯¹DeepSeekå¤šä¸ªç³»åˆ—æ¨¡å‹è¿›è¡Œç³»ç»Ÿæ€§è¯„ä¼°ï¼ŒåŒ…æ‹¬ä½¿ç”¨å¢å¼ºçš„A-EvalåŸºå‡†æµ‹è¯•ã€‚</li>
<li>æ¯”è¾ƒäº†åŸå§‹æŒ‡ä»¤è°ƒæ•´æ¨¡å‹å’Œè’¸é¦æ¨¡å‹çš„è¡¨ç°ï¼Œæ¢è®¨äº†æ¨ç†å¢å¼ºå¯¹å®é™…ä»»åŠ¡æ€§èƒ½çš„å½±å“ã€‚</li>
<li>é€šè¿‡æ€§èƒ½åˆ†çº§åˆ†ç±»ç¡®å®šDeepSeekæ¨¡å‹çš„èƒ½åŠ›è¾¹ç•Œã€‚</li>
<li>åˆ¶å®šäº†æ¨¡å‹é€‰æ‹©æ‰‹å†Œï¼Œä»¥æŒ‡å¯¼ç”¨æˆ·è½»æ¾é€‰æ‹©æœ€å…·æˆæœ¬æ•ˆç›Šçš„æ¨¡å‹ã€‚</li>
<li>è¯„ä¼°è¿‡ç¨‹ä¸­å¯èƒ½å­˜åœ¨åè§ï¼Œå›¢é˜Ÿå°†æŒç»­ä¼˜åŒ–è¯„ä¼°åŸºå‡†å¹¶å®šæœŸæ›´æ–°è®ºæ–‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.11164">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-c777be450e91965811e9692983405e15.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-610ca94921a22b69af95da15ac53b01e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ac7f7a774c0bcd5382ddb0ddce07ac34.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="Demystifying-Long-Chain-of-Thought-Reasoning-in-LLMs"><a href="#Demystifying-Long-Chain-of-Thought-Reasoning-in-LLMs" class="headerlink" title="Demystifying Long Chain-of-Thought Reasoning in LLMs"></a>Demystifying Long Chain-of-Thought Reasoning in LLMs</h2><p><strong>Authors:Edward Yeo, Yuxuan Tong, Morry Niu, Graham Neubig, Xiang Yue</strong></p>
<p>Scaling inference compute enhances reasoning in large language models (LLMs), with long chains-of-thought (CoTs) enabling strategies like backtracking and error correction. Reinforcement learning (RL) has emerged as a crucial method for developing these capabilities, yet the conditions under which long CoTs emerge remain unclear, and RL training requires careful design choices. In this study, we systematically investigate the mechanics of long CoT reasoning, identifying the key factors that enable models to generate long CoT trajectories. Through extensive supervised fine-tuning (SFT) and RL experiments, we present four main findings: (1) While SFT is not strictly necessary, it simplifies training and improves efficiency; (2) Reasoning capabilities tend to emerge with increased training compute, but their development is not guaranteed, making reward shaping crucial for stabilizing CoT length growth; (3) Scaling verifiable reward signals is critical for RL. We find that leveraging noisy, web-extracted solutions with filtering mechanisms shows strong potential, particularly for out-of-distribution (OOD) tasks such as STEM reasoning; and (4) Core abilities like error correction are inherently present in base models, but incentivizing these skills effectively for complex tasks via RL demands significant compute, and measuring their emergence requires a nuanced approach. These insights provide practical guidance for optimizing training strategies to enhance long CoT reasoning in LLMs. Our code is available at: <a target="_blank" rel="noopener" href="https://github.com/eddycmu/demystify-long-cot">https://github.com/eddycmu/demystify-long-cot</a>. </p>
<blockquote>
<p>æ‰©å±•æ¨ç†è®¡ç®—å¯ä»¥å¢å¼ºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ¨ç†èƒ½åŠ›ï¼Œé€šè¿‡é•¿é“¾æ¡æ€ç»´ï¼ˆCoTsï¼‰å®ç°å›æº¯å’Œé”™è¯¯çº æ­£ç­‰ç­–ç•¥ã€‚å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰å·²æˆä¸ºå¼€å‘è¿™äº›èƒ½åŠ›çš„é‡è¦æ–¹æ³•ï¼Œç„¶è€Œé•¿CoTså‡ºç°çš„æ¡ä»¶ä»ä¸æ¸…æ¥šï¼Œä¸”RLè®­ç»ƒéœ€è¦è°¨æ…çš„è®¾è®¡é€‰æ‹©ã€‚åœ¨è¿™é¡¹ç ”ç©¶ä¸­ï¼Œæˆ‘ä»¬ç³»ç»Ÿåœ°ç ”ç©¶äº†é•¿CoTæ¨ç†çš„æœºåˆ¶ï¼Œç¡®å®šäº†ä½¿æ¨¡å‹ç”Ÿæˆé•¿CoTè½¨è¿¹çš„å…³é”®å› ç´ ã€‚é€šè¿‡å¹¿æ³›çš„ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰å’ŒRLå®éªŒï¼Œæˆ‘ä»¬æå‡ºäº†å››ä¸ªä¸»è¦å‘ç°ï¼š</p>
</blockquote>
<p>ï¼ˆ1ï¼‰è™½ç„¶SFTå¹¶éä¸¥æ ¼å¿…è¦ï¼Œä½†å®ƒç®€åŒ–äº†è®­ç»ƒå¹¶æé«˜äº†æ•ˆç‡ï¼›<br>ï¼ˆ2ï¼‰æ¨ç†èƒ½åŠ›å¾€å¾€éšç€è®­ç»ƒè®¡ç®—çš„å¢åŠ è€Œæ¶Œç°ï¼Œä½†å…¶å‘å±•å¹¶éå¿…ç„¶ï¼Œè¿™ä½¿å¾—å¥–åŠ±å¡‘é€ å¯¹äºç¨³å®šCoTé•¿åº¦å¢é•¿è‡³å…³é‡è¦ï¼›<br>ï¼ˆ3ï¼‰æ‰©å¤§å¯éªŒè¯çš„å¥–åŠ±ä¿¡å·å¯¹RLè‡³å…³é‡è¦ã€‚æˆ‘ä»¬å‘ç°ï¼Œåˆ©ç”¨å¸¦æœ‰è¿‡æ»¤æœºåˆ¶çš„å˜ˆæ‚çš„ç½‘é¡µæå–è§£å†³æ–¹æ¡ˆå…·æœ‰å¾ˆå¼ºçš„æ½œåŠ›ï¼Œç‰¹åˆ«æ˜¯å¯¹äºç¦»ç¾¤åˆ†å¸ƒï¼ˆOODï¼‰ä»»åŠ¡ï¼Œå¦‚STEMæ¨ç†ï¼›</p>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.03373v1">PDF</a> Preprint, under review</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ç ”ç©¶äº†å¦‚ä½•æå‡å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„é•¿é“¾æ€ç»´ï¼ˆCoTï¼‰æ¨ç†èƒ½åŠ›ã€‚é€šè¿‡å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰å’Œç³»ç»Ÿå®éªŒï¼Œå‘ç°è®­ç»ƒè®¡ç®—é‡çš„å¢åŠ æœ‰åŠ©äºæ¨ç†èƒ½åŠ›çš„æå‡ï¼Œä½†ä¹Ÿéœ€è¦å¯¹å¥–åŠ±æœºåˆ¶è¿›è¡Œå¡‘å½¢ä»¥ç¡®ä¿æ€ç»´é“¾çš„ç¨³å®šå¢é•¿ã€‚åŒæ—¶ï¼Œåˆ©ç”¨è¿‡æ»¤æœºåˆ¶çš„å™ªå£°ç½‘ç»œè§£å†³æ–¹æ¡ˆå¯¹äºéå¸¸è§„ä»»åŠ¡å¦‚STEMæ¨ç†å…·æœ‰å¾ˆå¼ºçš„æ½œåŠ›ã€‚æ–‡ç« è¿˜æŒ‡å‡ºåŸºç¡€æ¨¡å‹æœ¬èº«å°±å…·å¤‡å¦‚é”™è¯¯ä¿®æ­£ç­‰æ ¸å¿ƒèƒ½åŠ›ï¼Œä½†è¦é€šè¿‡RLè¿›è¡Œæœ‰æ•ˆæ¿€åŠ±éœ€è¦æ›´å¤šçš„è®¡ç®—ï¼Œå¹¶éœ€è¦ç²¾ç»†çš„æµ‹é‡æ–¹æ³•æ¥è¯„ä¼°è¿™äº›èƒ½åŠ›çš„å‡ºç°ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¢åŠ è®­ç»ƒè®¡ç®—é‡æœ‰åŠ©äºæå‡å¤§å‹è¯­è¨€æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ã€‚</li>
<li>å¼ºåŒ–å­¦ä¹ æ˜¯å‘å±•é•¿é“¾æ€ç»´æ¨ç†èƒ½åŠ›çš„é‡è¦æ–¹æ³•ï¼Œä½†éœ€è¦å¯¹å¥–åŠ±æœºåˆ¶è¿›è¡Œå¡‘å½¢ä»¥ç¡®ä¿æ€ç»´é“¾çš„ç¨³å®šå¢é•¿ã€‚</li>
<li>ç³»ç»Ÿå®éªŒå‘ç°ï¼Œè™½ç„¶ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰ä¸æ˜¯å¿…é¡»çš„ï¼Œä½†å®ƒèƒ½ç®€åŒ–è®­ç»ƒå¹¶æé«˜æ•ˆç‡ã€‚</li>
<li>åˆ©ç”¨è¿‡æ»¤æœºåˆ¶çš„å™ªå£°ç½‘ç»œè§£å†³æ–¹æ¡ˆå¯¹äºéå¸¸è§„ä»»åŠ¡å…·æœ‰å¾ˆå¼ºçš„æ½œåŠ›ã€‚</li>
<li>åŸºç¡€æ¨¡å‹æœ¬èº«å°±å…·å¤‡æ ¸å¿ƒèƒ½åŠ›ï¼Œå¦‚é”™è¯¯ä¿®æ­£ï¼Œä½†è¦æœ‰æ•ˆæ¿€åŠ±è¿™äº›èƒ½åŠ›éœ€è¦æ›´å¤šçš„è®¡ç®—ã€‚</li>
<li>æµ‹é‡è¿™äº›èƒ½åŠ›çš„å‡ºç°éœ€è¦ç²¾ç»†çš„æ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.03373">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-af10f0a7b7cc78df96784fd158c86ecb.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-6360c7a588ae7009d3d3e7922862ef54.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-92e6895250d51fe4181ef19af287058c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-119ccae63ac71ad8b35ef892b14169a9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e44880da0561de8f301fe12e79d1323e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e7474b4beb9b100aeaa7cc3bbcf22453.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="LlamaRestTest-Effective-REST-API-Testing-with-Small-Language-Models"><a href="#LlamaRestTest-Effective-REST-API-Testing-with-Small-Language-Models" class="headerlink" title="LlamaRestTest: Effective REST API Testing with Small Language Models"></a>LlamaRestTest: Effective REST API Testing with Small Language Models</h2><p><strong>Authors:Myeongsoo Kim, Saurabh Sinha, Alessandro Orso</strong></p>
<p>Modern web services rely heavily on REST APIs, typically documented using the OpenAPI specification. The widespread adoption of this standard has resulted in the development of many black-box testing tools that generate tests based on OpenAPI specifications. Although Large Language Models (LLMs) have shown promising test-generation abilities, their application to REST API testing remains mostly unexplored. We present LlamaRestTest, a novel approach that employs two custom LLMs-created by fine-tuning and quantizing the Llama3-8B model using mined datasets of REST API example values and inter-parameter dependencies-to generate realistic test inputs and uncover inter-parameter dependencies during the testing process by analyzing server responses. We evaluated LlamaRestTest on 12 real-world services (including popular services such as Spotify), comparing it against RESTGPT, a GPT-powered specification-enhancement tool, as well as several state-of-the-art REST API testing tools, including RESTler, MoRest, EvoMaster, and ARAT-RL. Our results demonstrate that fine-tuning enables smaller models to outperform much larger models in detecting actionable parameter-dependency rules and generating valid inputs for REST API testing. We also evaluated different tool configurations, ranging from the base Llama3-8B model to fine-tuned versions, and explored multiple quantization techniques, including 2-bit, 4-bit, and 8-bit integer formats. Our study shows that small language models can perform as well as, or better than, large language models in REST API testing, balancing effectiveness and efficiency. Furthermore, LlamaRestTest outperforms state-of-the-art REST API testing tools in code coverage achieved and internal server errors identified, even when those tools use RESTGPT-enhanced specifications. </p>
<blockquote>
<p>ç°ä»£webæœåŠ¡ä¸¥é‡ä¾èµ–äºREST APIï¼Œé€šå¸¸ä½¿ç”¨OpenAPIè§„èŒƒè¿›è¡Œæ–‡æ¡£åŒ–ã€‚è¿™ä¸€æ ‡å‡†çš„å¹¿æ³›åº”ç”¨ä¿ƒä½¿äº†è®¸å¤šé»‘ç›’æµ‹è¯•å·¥å…·çš„å‘å±•ï¼Œè¿™äº›å·¥å…·æ ¹æ®OpenAPIè§„èŒƒç”Ÿæˆæµ‹è¯•ã€‚å°½ç®¡å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨æµ‹è¯•ç”Ÿæˆæ–¹é¢è¡¨ç°å‡ºäº†ä»¤äººç©ç›®çš„èƒ½åŠ›ï¼Œä½†å®ƒä»¬åœ¨REST APIæµ‹è¯•ä¸­çš„åº”ç”¨ä»ç„¶ä¸»è¦æœªè¢«æ¢ç´¢ã€‚æˆ‘ä»¬æå‡ºäº†LlamaRestTestï¼Œè¿™æ˜¯ä¸€ç§æ–°é¢–çš„æ–¹æ³•ï¼Œå®ƒé‡‡ç”¨ä¸¤ä¸ªè‡ªå®šä¹‰çš„LLMsï¼Œé€šè¿‡å¾®è°ƒå’Œä½¿ç”¨æŒ–æ˜çš„REST APIç¤ºä¾‹å€¼å’Œæ•°æ®é›†é‡åŒ–Llama3-8Bæ¨¡å‹ï¼Œç”Ÿæˆç°å®çš„æµ‹è¯•è¾“å…¥ï¼Œå¹¶é€šè¿‡åˆ†ææœåŠ¡å™¨å“åº”åœ¨æµ‹è¯•è¿‡ç¨‹ä¸­å‘ç°å‚æ•°ä¹‹é—´çš„ä¾èµ–å…³ç³»ã€‚æˆ‘ä»¬å¯¹LlamaRestTeståœ¨12ä¸ªçœŸå®ä¸–ç•ŒæœåŠ¡ï¼ˆåŒ…æ‹¬æµè¡Œçš„æœåŠ¡å¦‚Spotifyï¼‰ä¸Šè¿›è¡Œäº†è¯„ä¼°ï¼Œå°†å…¶ä¸ç”±GPTé©±åŠ¨çš„è§„æ ¼å¢å¼ºå·¥å…·RESTGPTä»¥åŠå‡ ç§æœ€å…ˆè¿›çš„REST APIæµ‹è¯•å·¥å…·ï¼ˆåŒ…æ‹¬RESTlerã€MoRestã€EvoMasterå’ŒARAT-RLï¼‰è¿›è¡Œäº†æ¯”è¾ƒã€‚æˆ‘ä»¬çš„ç»“æœè¡¨æ˜ï¼Œå¾®è°ƒä½¿è¾ƒå°çš„æ¨¡å‹åœ¨æ£€æµ‹å¯æ“ä½œçš„å‚æ•°ä¾èµ–è§„åˆ™å’Œä¸ºREST APIæµ‹è¯•ç”Ÿæˆæœ‰æ•ˆè¾“å…¥æ–¹é¢è¡¨ç°å‡ºè¶…è¶Šè®¸å¤šè¾ƒå¤§æ¨¡å‹çš„èƒ½åŠ›ã€‚æˆ‘ä»¬è¿˜è¯„ä¼°äº†ä»åŸºæœ¬çš„Llama3-8Bæ¨¡å‹åˆ°å¾®è°ƒç‰ˆæœ¬çš„å·¥å…·é…ç½®ï¼Œå¹¶æ¢ç´¢äº†å¤šç§é‡åŒ–æŠ€æœ¯ï¼ŒåŒ…æ‹¬2ä½ã€4ä½å’Œ8ä½æ•´æ•°æ ¼å¼ã€‚æˆ‘ä»¬çš„ç ”ç©¶è¡¨æ˜ï¼Œåœ¨REST APIæµ‹è¯•ä¸­ï¼Œå°å‹è¯­è¨€æ¨¡å‹å¯ä»¥è¡¨ç°å¾—ä¸å¤§å‹è¯­è¨€æ¨¡å‹ä¸€æ ·å¥½ç”šè‡³æ›´å¥½ï¼Œå¹³è¡¡äº†æœ‰æ•ˆæ€§å’Œæ•ˆç‡ã€‚æ­¤å¤–ï¼Œå³ä½¿åœ¨é‚£äº›å·¥å…·ä½¿ç”¨RESTGPTå¢å¼ºçš„è§„æ ¼æ—¶ï¼ŒLlamaRestTeståœ¨ä»£ç è¦†ç›–ç‡å’Œå†…éƒ¨æœåŠ¡å™¨é”™è¯¯è¯†åˆ«æ–¹é¢ä¹Ÿæ¯”æœ€å…ˆè¿›çš„REST APIæµ‹è¯•å·¥å…·è¡¨ç°æ›´å‡ºè‰²ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.08598v2">PDF</a> To be published in the ACM International Conference on the   Foundations of Software Engineering (FSE 2025)</p>
<p><strong>æ‘˜è¦</strong></p>
<p>ç°ä»£WebæœåŠ¡ä¾èµ–REST APIï¼Œé€šå¸¸é€šè¿‡OpenAPIè§„èŒƒè¿›è¡Œæ–‡æ¡£åŒ–ã€‚è®¸å¤šé»‘ç›’æµ‹è¯•å·¥å…·åŸºäºOpenAPIè§„èŒƒç”Ÿæˆæµ‹è¯•ã€‚å°½ç®¡å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨æµ‹è¯•ç”Ÿæˆæ–¹é¢å±•ç°å‡ºæ½œåŠ›ï¼Œä½†å®ƒä»¬åœ¨REST APIæµ‹è¯•ä¸­çš„åº”ç”¨ä»ç„¶ä¸»è¦æœªè¢«æ¢ç´¢ã€‚æœ¬æ–‡æå‡ºLlamaRestTestï¼Œä¸€ç§é‡‡ç”¨å®šåˆ¶LLMï¼ˆé€šè¿‡å¾®è°ƒå¹¶ä½¿ç”¨æŒ–æ˜çš„REST APIç¤ºä¾‹å€¼å’Œå‚æ•°é—´ä¾èµ–å…³ç³»æ•°æ®é›†å¯¹Llama3-8Bæ¨¡å‹è¿›è¡Œé‡åŒ–å¤„ç†ï¼‰æ¥ç”ŸæˆçœŸå®æµ‹è¯•è¾“å…¥å¹¶æ­ç¤ºå‚æ•°é—´ä¾èµ–å…³ç³»çš„æ–¹æ³•ã€‚é€šè¿‡å¯¹åŒ…æ‹¬Spotifyç­‰12ä¸ªçœŸå®æœåŠ¡è¿›è¡Œçš„è¯„ä¼°ï¼Œå¯¹æ¯”äº†RESTGPTé©±åŠ¨çš„è§„æ ¼å¢å¼ºå·¥å…·åŠå…¶ä»–å…ˆè¿›çš„REST APIæµ‹è¯•å·¥å…·ï¼ˆå¦‚RESTlerã€MoRestã€EvoMasterå’ŒARAT-RLï¼‰ã€‚ç»“æœè¡¨æ˜ï¼Œå¾®è°ƒä½¿å°å‹æ¨¡å‹åœ¨æ£€æµ‹å¯æ“ä½œçš„å‚æ•°ä¾èµ–è§„åˆ™ä»¥åŠä¸ºREST APIæµ‹è¯•ç”Ÿæˆæœ‰æ•ˆè¾“å…¥æ–¹é¢è¡¨ç°å‡ºè¶…è¶Šå¤§å‹æ¨¡å‹çš„èƒ½åŠ›ã€‚æœ¬ç ”ç©¶è¿˜å¯¹ä»åŸºç¡€Llama3-8Bæ¨¡å‹åˆ°å¾®è°ƒç‰ˆæœ¬çš„ä¸åŒå·¥å…·é…ç½®è¿›è¡Œäº†è¯„ä¼°ï¼Œå¹¶æ¢ç´¢äº†å¤šç§é‡åŒ–æŠ€æœ¯ã€‚ç»“æœæ˜¾ç¤ºï¼Œåœ¨REST APIæµ‹è¯•ä¸­ï¼Œå°å‹è¯­è¨€æ¨¡å‹å¯åœ¨æœ‰æ•ˆæ€§ä¸æ•ˆç‡ä¹‹é—´è¾¾åˆ°æˆ–è¶…è¶Šå¤§å‹è¯­è¨€æ¨¡å‹çš„æ€§èƒ½ã€‚æ­¤å¤–ï¼ŒLlamaRestTeståœ¨ä»£ç è¦†ç›–ç‡åŠå†…éƒ¨æœåŠ¡å™¨é”™è¯¯è¯†åˆ«æ–¹é¢ï¼Œå³ä½¿ä½¿ç”¨RESTGPTå¢å¼ºè§„æ ¼çš„å·¥å…·ï¼Œä¹Ÿä¼˜äºç°æœ‰å…ˆè¿›çš„REST APIæµ‹è¯•å·¥å…·ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>LlamaRestTestæ˜¯é¦–ä¸ªåˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è¿›è¡ŒREST APIæµ‹è¯•çš„æ–¹æ³•ã€‚</li>
<li>é€šè¿‡å¾®è°ƒä¸é‡åŒ–å¤„ç†Llama3-8Bæ¨¡å‹ï¼ŒLlamaRestTestèƒ½ç”ŸæˆçœŸå®çš„æµ‹è¯•è¾“å…¥ã€‚</li>
<li>ç›¸æ¯”å…¶ä»–å·¥å…·ï¼ŒLlamaRestTestæ›´æ“…é•¿æ£€æµ‹å‚æ•°é—´çš„ä¾èµ–å…³ç³»ã€‚</li>
<li>å°å‹è¯­è¨€æ¨¡å‹ï¼ˆç»å¾®è°ƒä¸é‡åŒ–å¤„ç†ï¼‰åœ¨REST APIæµ‹è¯•æ–¹é¢çš„è¡¨ç°å¯ä¸å¤§å‹è¯­è¨€æ¨¡å‹ç›¸å½“æˆ–æ›´ä¼˜ã€‚</li>
<li>LlamaRestTestçš„ä»£ç è¦†ç›–ç‡é«˜ï¼Œèƒ½è¯†åˆ«æ›´å¤šå†…éƒ¨æœåŠ¡å™¨é”™è¯¯ã€‚</li>
<li>RESTGPTåœ¨æŸäº›æƒ…å†µä¸‹èƒ½æé«˜æµ‹è¯•è§„æ ¼çš„è´¨é‡ï¼Œä½†ä¸LlamaRestTestç›¸æ¯”ä»æœ‰æ‰€ä¸è¶³ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.08598">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-a189e944592c0d7a81247aaa43540f57.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-eaf175705521c3ab4f398934f3512c26.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="REINFORCE-An-Efficient-RLHF-Algorithm-with-Robustness-to-Both-Prompt-and-Reward-Models"><a href="#REINFORCE-An-Efficient-RLHF-Algorithm-with-Robustness-to-Both-Prompt-and-Reward-Models" class="headerlink" title="REINFORCE++: An Efficient RLHF Algorithm with Robustness to Both Prompt   and Reward Models"></a>REINFORCE++: An Efficient RLHF Algorithm with Robustness to Both Prompt   and Reward Models</h2><p><strong>Authors:Jian Hu, Jason Klein Liu, Shen Wei</strong></p>
<p>Reinforcement Learning from Human Feedback (RLHF) plays a crucial role in aligning large language models (LLMs) with human values and preferences. While state-of-the-art applications like ChatGPT&#x2F;GPT-4 commonly employ Proximal Policy Optimization (PPO), the inclusion of a critic network introduces significant computational overhead. REINFORCE-based methods, such as REINFORCE Leave One-Out (RLOO), ReMax, and Group Relative Policy Optimization (GRPO), address this limitation by eliminating the critic network. However, these approaches face challenges in accurate advantage estimation. Specifically, they estimate advantages independently for responses to each prompt, which can lead to overfitting on simpler prompts and vulnerability to reward hacking. To address these challenges, we introduce REINFORCE++, a novel approach that removes the critic model while using the normalized reward of a batch as the baseline. Our empirical evaluation demonstrates that REINFORCE++ exhibits robust performance across various reward models without requiring prompt set truncation. Furthermore, it achieves superior generalization in both RLHF and long chain-of-thought (CoT) settings compared to existing REINFORCE-based methods. The implementation is available at <a target="_blank" rel="noopener" href="https://github.com/OpenRLHF/OpenRLHF">https://github.com/OpenRLHF/OpenRLHF</a>. </p>
<blockquote>
<p>å¼ºåŒ–å­¦ä¹ ä»äººç±»åé¦ˆï¼ˆRLHFï¼‰åœ¨å°†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¸äººç±»ä»·å€¼è§‚å’Œåå¥½å¯¹é½æ–¹é¢å‘æŒ¥ç€è‡³å…³é‡è¦çš„ä½œç”¨ã€‚è™½ç„¶æœ€å…ˆè¿›çš„åº”ç”¨å¦‚ChatGPT&#x2F;GPT-4é€šå¸¸é‡‡ç”¨è¿‘ç«¯ç­–ç•¥ä¼˜åŒ–ï¼ˆPPOï¼‰ï¼Œä½†åŠ å…¥è¯„è®ºå®¶ç½‘ç»œä¼šå¼•å…¥å¤§é‡çš„è®¡ç®—å¼€é”€ã€‚åŸºäºREINFORCEçš„æ–¹æ³•ï¼Œå¦‚REINFORCE Leave One-Outï¼ˆRLOOï¼‰ã€ReMaxå’Œé›†å›¢ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆGRPOï¼‰ï¼Œé€šè¿‡æ¶ˆé™¤è¯„è®ºå®¶ç½‘ç»œæ¥è§£å†³è¿™ä¸€é™åˆ¶ã€‚ç„¶è€Œï¼Œè¿™äº›æ–¹æ³•åœ¨å‡†ç¡®ä¼°ç®—ä¼˜åŠ¿æ–¹é¢é¢ä¸´æŒ‘æˆ˜ã€‚å…·ä½“æ¥è¯´ï¼Œå®ƒä»¬ç‹¬ç«‹åœ°ä¸ºæ¯ä¸ªæç¤ºçš„å›åº”ä¼°è®¡ä¼˜åŠ¿ï¼Œè¿™å¯èƒ½å¯¼è‡´åœ¨ç®€å•æç¤ºä¸Šè¿‡æ‹Ÿåˆï¼Œå¹¶å®¹æ˜“å—åˆ°å¥–åŠ±é»‘å®¢æ”»å‡»ã€‚ä¸ºäº†è§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†REINFORCE++ï¼Œè¿™æ˜¯ä¸€ç§æ–°å‹æ–¹æ³•ï¼Œå®ƒç§»é™¤äº†è¯„è®ºå®¶æ¨¡å‹ï¼ŒåŒæ—¶ä½¿ç”¨ä¸€æ‰¹çš„æ ‡å‡†åŒ–å¥–åŠ±ä½œä¸ºåŸºçº¿ã€‚æˆ‘ä»¬çš„ç»éªŒè¯„ä¼°è¡¨æ˜ï¼ŒREINFORCE++åœ¨å„ç§å¥–åŠ±æ¨¡å‹ä¸­å…·æœ‰ç¨³å¥çš„æ€§èƒ½è¡¨ç°ï¼Œæ— éœ€æˆªæ–­æç¤ºé›†ã€‚æ­¤å¤–ï¼Œä¸ç°æœ‰çš„åŸºäºREINFORCEçš„æ–¹æ³•ç›¸æ¯”ï¼Œå®ƒåœ¨RLHFå’Œé•¿é“¾æ€ç»´ï¼ˆCoTï¼‰è®¾ç½®ä¸­å®ç°äº†æ›´å¥½çš„æ³›åŒ–èƒ½åŠ›ã€‚å®ç°æ–¹æ³•è¯¦è§<a target="_blank" rel="noopener" href="https://github.com/OpenRLHF/OpenRLHF%E3%80%82">https://github.com/OpenRLHF/OpenRLHFã€‚</a></p>
</blockquote>
<p><strong>Translation</strong></p>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.03262v2">PDF</a> this is a tech report</p>
<p><strong>Summary</strong></p>
<p>å¼ºåŒ–å­¦ä¹ ä»äººç±»åé¦ˆï¼ˆRLHFï¼‰åœ¨å°†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¸äººç±»ä»·å€¼è§‚å’Œåå¥½å¯¹é½æ–¹é¢å‘æŒ¥ç€å…³é”®ä½œç”¨ã€‚æœ€æ–°çš„åº”ç”¨å¦‚ChatGPT&#x2F;GPT-4å¸¸é‡‡ç”¨è¿‘ç«¯ç­–ç•¥ä¼˜åŒ–ï¼ˆPPOï¼‰ï¼Œä½†åŠ å…¥è¯„è®ºå®¶ç½‘ç»œå¸¦æ¥äº†æ˜¾è‘—çš„è®¡ç®—å¼€é”€ã€‚REINFORCEç³»åˆ—æ–¹æ³•å¦‚REINFORCE Leave One-Out (RLOO)ã€ReMaxå’ŒGroup Relative Policy Optimization (GRPO)é€šè¿‡æ¶ˆé™¤è¯„è®ºå®¶ç½‘ç»œè§£å†³äº†è¿™ä¸€é—®é¢˜ï¼Œä½†åœ¨å‡†ç¡®çš„ä¼˜åŠ¿ä¼°è®¡æ–¹é¢é¢ä¸´æŒ‘æˆ˜ã€‚æˆ‘ä»¬æå‡ºäº†REINFORCE++è¿™ä¸€æ–°æ–¹æ³•ï¼Œå®ƒä¸ä½¿ç”¨è¯„è®ºå®¶æ¨¡å‹ï¼Œè€Œæ˜¯ä½¿ç”¨æ‰¹æ¬¡çš„æ ‡å‡†åŒ–å¥–åŠ±ä½œä¸ºåŸºçº¿ã€‚å®è¯è¯„ä¼°æ˜¾ç¤ºï¼ŒREINFORCE++åœ¨ä¸åŒå¥–åŠ±æ¨¡å‹ä¸‹è¡¨ç°ç¨³å¥ï¼Œæ— éœ€æˆªæ–­æç¤ºé›†ï¼Œå¹¶ä¸”åœ¨RLHFå’Œé•¿é“¾æ€ç»´ï¼ˆCoTï¼‰è®¾ç½®ä¸­ç›¸è¾ƒäºç°æœ‰çš„REINFORCEç³»åˆ—æ–¹æ³•è¡¨ç°æ›´ä¼˜è¶Šã€‚å…¶å®ç°ä»£ç å·²åœ¨OpenRLHF&#x2F;OpenRLHFå…¬å¼€ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¼ºåŒ–å­¦ä¹ ä»äººç±»åé¦ˆï¼ˆRLHFï¼‰åœ¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¸äººç±»ä»·å€¼è§‚å’Œåå¥½å¯¹é½ä¸­èµ·å…³é”®ä½œç”¨ã€‚</li>
<li>ç°æœ‰æ–¹æ³•å¦‚PPOè™½ç„¶æœ‰æ•ˆï¼Œä½†å­˜åœ¨è®¡ç®—å¼€é”€å¤§çš„é—®é¢˜ã€‚</li>
<li>REINFORCEç³»åˆ—æ–¹æ³•é€šè¿‡æ¶ˆé™¤è¯„è®ºå®¶ç½‘ç»œè§£å†³äº†è®¡ç®—å¼€é”€é—®é¢˜ï¼Œä½†åœ¨ä¼˜åŠ¿ä¼°è®¡æ–¹é¢å­˜åœ¨æŒ‘æˆ˜ã€‚</li>
<li>REINFORCE++æå‡ºäº†æ–°æ–¹æ³•ï¼Œä¸ä½¿ç”¨è¯„è®ºå®¶æ¨¡å‹ï¼Œè€Œæ˜¯é‡‡ç”¨æ‰¹æ¬¡çš„æ ‡å‡†åŒ–å¥–åŠ±ä½œä¸ºåŸºçº¿ã€‚</li>
<li>REINFORCE++åœ¨ä¸åŒå¥–åŠ±æ¨¡å‹ä¸‹è¡¨ç°ç¨³å¥ï¼Œæ— éœ€æˆªæ–­æç¤ºé›†ã€‚</li>
<li>REINFORCE++åœ¨RLHFå’Œé•¿é“¾æ€ç»´ï¼ˆCoTï¼‰è®¾ç½®ä¸­çš„è¡¨ç°ä¼˜äºç°æœ‰REINFORCEç³»åˆ—æ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.03262">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-3518badb9e930244122125447e88f757.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-05dcae92e9a10d0676177280aa65f9cd.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f9552f0fdcff1eddf26a95142951a3e0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1b5fbaae4b7bcd08d0f50a3159ae2f1e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7ea153908dae181c8930ae17f581e2fe.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-48f00b2102489e5552cab284b7a094b5.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="CALICO-Part-Focused-Semantic-Co-Segmentation-with-Large-Vision-Language-Models"><a href="#CALICO-Part-Focused-Semantic-Co-Segmentation-with-Large-Vision-Language-Models" class="headerlink" title="CALICO: Part-Focused Semantic Co-Segmentation with Large Vision-Language   Models"></a>CALICO: Part-Focused Semantic Co-Segmentation with Large Vision-Language   Models</h2><p><strong>Authors:Kiet A. Nguyen, Adheesh Juvekar, Tianjiao Yu, Muntasir Wahed, Ismini Lourentzou</strong></p>
<p>Recent advances in Large Vision-Language Models (LVLMs) have enabled general-purpose vision tasks through visual instruction tuning. While existing LVLMs can generate segmentation masks from text prompts for single images, they struggle with segmentation-grounded reasoning across images, especially at finer granularities such as object parts. In this paper, we introduce the new task of part-focused semantic co-segmentation, which involves identifying and segmenting common objects, as well as common and unique object parts across images. To address this task, we present CALICO, the first LVLM designed for multi-image part-level reasoning segmentation. CALICO features two key components, a novel Correspondence Extraction Module that identifies semantic part-level correspondences, and Correspondence Adaptation Modules that embed this information into the LVLM to facilitate multi-image understanding in a parameter-efficient manner. To support training and evaluation, we curate MixedParts, a large-scale multi-image segmentation dataset containing $\sim$2.4M samples across $\sim$44K images spanning diverse object and part categories. Experimental results demonstrate that CALICO, with just 0.3% of its parameters finetuned, achieves strong performance on this challenging task. </p>
<blockquote>
<p>è¿‘æœŸå¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆLVLMsï¼‰çš„è¿›æ­¥é€šè¿‡è§†è§‰æŒ‡ä»¤è°ƒæ•´å®ç°äº†é€šç”¨è§†è§‰ä»»åŠ¡ã€‚è™½ç„¶ç°æœ‰çš„LVLMså¯ä»¥é€šè¿‡æ–‡æœ¬æç¤ºå¯¹å•å›¾åƒç”Ÿæˆåˆ†å‰²æ©è†œï¼Œä½†å®ƒä»¬åœ¨è·¨å›¾åƒçš„åˆ†å‰²æ¨ç†æ–¹é¢é‡åˆ°å›°éš¾ï¼Œå°¤å…¶æ˜¯åœ¨æ›´ç²¾ç»†çš„ç²’åº¦ï¼ˆå¦‚ç‰©ä½“éƒ¨åˆ†ï¼‰ä¸Šã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬å¼•å…¥äº†æ–°çš„éƒ¨åˆ†èšç„¦è¯­ä¹‰ååŒåˆ†å‰²ä»»åŠ¡ï¼Œè¯¥ä»»åŠ¡æ¶‰åŠè¯†åˆ«å’Œåˆ†å‰²è·¨å›¾åƒä¸­çš„å¸¸è§å¯¹è±¡ä»¥åŠå¸¸è§å’Œç‹¬ç‰¹çš„å¯¹è±¡éƒ¨åˆ†ã€‚ä¸ºäº†è§£å†³æ­¤ä»»åŠ¡ï¼Œæˆ‘ä»¬æå‡ºäº†CALICOï¼Œè¿™æ˜¯ä¸“ä¸ºå¤šå›¾åƒéƒ¨åˆ†çº§æ¨ç†åˆ†å‰²è®¾è®¡çš„é¦–ä¸ªLVLMã€‚CALICOå…·æœ‰ä¸¤ä¸ªå…³é”®ç»„ä»¶ï¼šæ–°å‹å¯¹åº”æå–æ¨¡å—ï¼Œç”¨äºè¯†åˆ«è¯­ä¹‰éƒ¨åˆ†çº§å¯¹åº”å…³ç³»ï¼›ä»¥åŠå¯¹åº”é€‚åº”æ¨¡å—ï¼Œå°†æ­¤ä¿¡æ¯åµŒå…¥LVLMä¸­ï¼Œä»¥é«˜æ•ˆå‚æ•°çš„æ–¹å¼ä¿ƒè¿›å¤šå›¾åƒç†è§£ã€‚ä¸ºäº†æ”¯æŒå’Œè¯„ä¼°è®­ç»ƒï¼Œæˆ‘ä»¬æ•´ç†äº†MixedPartsï¼Œè¿™æ˜¯ä¸€ä¸ªå¤§è§„æ¨¡çš„å¤šå›¾åƒåˆ†å‰²æ•°æ®é›†ï¼ŒåŒ…å«çº¦4.4ä¸‡å¼ å›¾åƒä¸­çš„çº¦240ä¸‡æ ·æœ¬æ ·æœ¬ï¼Œæ¶µç›–å¤šæ ·åŒ–çš„å¯¹è±¡å’Œç±»åˆ«éƒ¨åˆ†ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒCALICOä»…å¾®è°ƒå…¶å‚æ•°çš„0.3%ï¼Œå°±èƒ½åœ¨æ­¤å…·æœ‰æŒ‘æˆ˜æ€§çš„ä»»åŠ¡ä¸Šå®ç°å‡ºè‰²çš„æ€§èƒ½ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.19331v2">PDF</a> Accepted to CVPR 2025. Project page:   <a target="_blank" rel="noopener" href="https://plan-lab.github.io/calico/">https://plan-lab.github.io/calico/</a></p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆLVLMsï¼‰çš„æœ€æ–°è¿›å±•å¦‚ä½•é€šè¿‡è§†è§‰æŒ‡ä»¤å¾®è°ƒå®ç°é€šç”¨è§†è§‰ä»»åŠ¡ã€‚é’ˆå¯¹ç°æœ‰LVLMsåœ¨è·¨å›¾åƒåˆ†å‰²æ¨ç†ï¼Œç‰¹åˆ«æ˜¯åœ¨æ›´ç²¾ç»†çš„ç²’åº¦å¦‚ç‰©ä½“éƒ¨åˆ†ä¸Šçš„å›°éš¾ï¼Œæœ¬æ–‡å¼•å…¥äº†éƒ¨åˆ†èšç„¦è¯­ä¹‰ååŒåˆ†å‰²çš„æ–°ä»»åŠ¡ã€‚ä¸ºåº”å¯¹æ­¤ä»»åŠ¡ï¼Œæå‡ºäº†CALICOï¼Œé¦–æ¬¾è®¾è®¡ç”¨äºå¤šå›¾åƒéƒ¨åˆ†çº§åˆ«æ¨ç†åˆ†å‰²çš„LVLMã€‚CALICOåŒ…å«ä¸¤ä¸ªå…³é”®ç»„ä»¶ï¼šå¯¹åº”æå–æ¨¡å—ï¼Œç”¨äºè¯†åˆ«è¯­ä¹‰éƒ¨åˆ†çº§åˆ«çš„å¯¹åº”å…³ç³»ï¼›å¯¹åº”é€‚åº”æ¨¡å—ï¼Œå°†æ­¤ä¿¡æ¯åµŒå…¥LVLMä¸­ä»¥ä»¥å‚æ•°æ•ˆç‡é«˜çš„æ–¹å¼ä¿ƒè¿›å¤šå›¾åƒç†è§£ã€‚ä¸ºæ”¯æŒè®­ç»ƒå’Œè¯„ä¼°ï¼Œæœ¬æ–‡æ•´ç†äº†MixedPartsï¼Œä¸€ä¸ªå¤§è§„æ¨¡å¤šå›¾åƒåˆ†å‰²æ•°æ®é›†ï¼ŒåŒ…å«çº¦240ä¸‡æ ·æœ¬ï¼Œè·¨è¶Šçº¦4ä¸‡å¼ å›¾åƒï¼Œæ¶µç›–å¤šæ ·åŒ–çš„å¯¹è±¡å’Œé›¶ä»¶ç±»åˆ«ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒCALICOä»…éœ€å¾®è°ƒå…¶å‚æ•°çš„0.3%ï¼Œå³å¯åœ¨æ­¤å…·æœ‰æŒ‘æˆ˜æ€§çš„ä»»åŠ¡ä¸Šå®ç°å¼ºåŠ²è¡¨ç°ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LVLMsèƒ½å¤Ÿé€šè¿‡è§†è§‰æŒ‡ä»¤å¾®è°ƒå®Œæˆé€šç”¨è§†è§‰ä»»åŠ¡ã€‚</li>
<li>ç°æœ‰LVLMsåœ¨è·¨å›¾åƒåˆ†å‰²æ¨ç†ä¸Šè¡¨ç°ä¸è¶³ï¼Œå°¤å…¶åœ¨ç‰©ä½“éƒ¨åˆ†çš„ç²¾ç»†ç²’åº¦ä¸Šã€‚</li>
<li>å¼•å…¥äº†éƒ¨åˆ†èšç„¦è¯­ä¹‰ååŒåˆ†å‰²çš„æ–°ä»»åŠ¡ï¼Œæ—¨åœ¨è§£å†³è·¨å›¾åƒçš„ç‰©ä½“å’Œéƒ¨åˆ†çº§åˆ«çš„åˆ†å‰²é—®é¢˜ã€‚</li>
<li>æå‡ºäº†CALICOæ¨¡å‹ï¼ŒåŒ…å«å¯¹åº”æå–æ¨¡å—å’Œå¯¹åº”é€‚åº”æ¨¡å—ï¼Œç”¨äºå¤„ç†å¤šå›¾åƒéƒ¨åˆ†çº§åˆ«çš„æ¨ç†åˆ†å‰²ã€‚</li>
<li>CALICOè®¾è®¡ç”¨äºå¤„ç†å¤§è§„æ¨¡å¤šå›¾åƒåˆ†å‰²æ•°æ®é›†MixedPartsã€‚</li>
<li>MixedPartsæ•°æ®é›†åŒ…å«å¤§é‡æ ·æœ¬å’Œå¤šæ ·åŒ–çš„å¯¹è±¡å’Œé›¶ä»¶ç±»åˆ«ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.19331">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-a6cc99700f1b1bfa75c2a451f860ecc3.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a21e00ccb1f8020e81751da33796b829.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-bf2079458df343e6a0e5678170128344.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-5f294b1649b43f0755d74680f498bb62.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-90b28495fc73d1cda7d98f8d14cf7263.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="MedHallBench-A-New-Benchmark-for-Assessing-Hallucination-in-Medical-Large-Language-Models"><a href="#MedHallBench-A-New-Benchmark-for-Assessing-Hallucination-in-Medical-Large-Language-Models" class="headerlink" title="MedHallBench: A New Benchmark for Assessing Hallucination in Medical   Large Language Models"></a>MedHallBench: A New Benchmark for Assessing Hallucination in Medical   Large Language Models</h2><p><strong>Authors:Kaiwen Zuo, Yirui Jiang</strong></p>
<p>Medical Large Language Models (MLLMs) have demonstrated potential in healthcare applications, yet their propensity for hallucinations â€“ generating medically implausible or inaccurate information â€“ presents substantial risks to patient care. This paper introduces MedHallBench, a comprehensive benchmark framework for evaluating and mitigating hallucinations in MLLMs. Our methodology integrates expert-validated medical case scenarios with established medical databases to create a robust evaluation dataset. The framework employs a sophisticated measurement system that combines automated ACHMI (Automatic Caption Hallucination Measurement in Medical Imaging) scoring with rigorous clinical expert evaluations and utilizes reinforcement learning methods to achieve automatic annotation. Through an optimized reinforcement learning from human feedback (RLHF) training pipeline specifically designed for medical applications, MedHallBench enables thorough evaluation of MLLMs across diverse clinical contexts while maintaining stringent accuracy standards. We conducted comparative experiments involving various models, utilizing the benchmark to establish a baseline for widely adopted large language models (LLMs). Our findings indicate that ACHMI provides a more nuanced understanding of the effects of hallucinations compared to traditional metrics, thereby highlighting its advantages in hallucination assessment. This research establishes a foundational framework for enhancing MLLMsâ€™ reliability in healthcare settings and presents actionable strategies for addressing the critical challenge of AI hallucinations in medical applications. </p>
<blockquote>
<p>åŒ»ç–—é¢†åŸŸçš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨åŒ»ç–—ä¿å¥åº”ç”¨ä¸­æ˜¾ç¤ºå‡ºå·¨å¤§æ½œåŠ›ï¼Œä½†å®ƒä»¬æ˜“äºäº§ç”Ÿå¹»è§‰ï¼Œå³ç”ŸæˆåŒ»å­¦ä¸Šä¸ç°å®æˆ–ä¸å‡†ç¡®çš„ä¿¡æ¯ï¼Œè¿™å¯¹æ‚£è€…æŠ¤ç†å¸¦æ¥äº†å®è´¨æ€§çš„é£é™©ã€‚æœ¬æ–‡ä»‹ç»äº†MedHallBenchï¼Œè¿™æ˜¯ä¸€ä¸ªè¯„ä¼°å’Œç¼“è§£MLLMsä¸­å¹»è§‰çš„å…¨é¢åŸºå‡†æ¡†æ¶ã€‚æˆ‘ä»¬çš„æ–¹æ³•ç»“åˆäº†ä¸“å®¶éªŒè¯çš„åŒ»ç–—æ¡ˆä¾‹åœºæ™¯å’Œç°æœ‰çš„åŒ»ç–—æ•°æ®åº“ï¼Œä»¥åˆ›å»ºç¨³å¥çš„è¯„ä¼°æ•°æ®é›†ã€‚è¯¥æ¡†æ¶é‡‡ç”¨å…ˆè¿›çš„æµ‹é‡ç³»ç»Ÿï¼Œç»“åˆè‡ªåŠ¨åŒ»å­¦æˆåƒå¹»è§‰è‡ªåŠ¨æµ‹é‡ï¼ˆACHMIï¼‰è¯„åˆ†ä¸ä¸¥æ ¼çš„ä¸´åºŠä¸“å®¶è¯„ä¼°ï¼Œå¹¶åˆ©ç”¨å¼ºåŒ–å­¦ä¹ æ–¹æ³•å®ç°è‡ªåŠ¨æ³¨é‡Šã€‚é€šè¿‡é’ˆå¯¹åŒ»ç–—åº”ç”¨è€Œä¼˜åŒ–çš„ã€ä»äººç±»åé¦ˆä¸­å¼ºåŒ–å­¦ä¹ ï¼ˆRLHFï¼‰çš„è®­ç»ƒç®¡é“ï¼ŒMedHallBenchèƒ½å¤Ÿåœ¨ç»´æŒä¸¥æ ¼å‡†ç¡®æ€§æ ‡å‡†çš„åŒæ—¶ï¼Œåœ¨å¤šç§ä¸´åºŠèƒŒæ™¯ä¸‹å¯¹MLLMsè¿›è¡Œå…¨é¢è¯„ä¼°ã€‚æˆ‘ä»¬è¿›è¡Œäº†æ¶‰åŠå¤šç§æ¨¡å‹çš„å¯¹æ¯”å®éªŒï¼Œåˆ©ç”¨è¯¥åŸºå‡†ä¸ºå¹¿æ³›é‡‡ç”¨çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å»ºç«‹åŸºçº¿ã€‚æˆ‘ä»¬çš„ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œä¸ä¼ ç»Ÿçš„åº¦é‡æŒ‡æ ‡ç›¸æ¯”ï¼ŒACHMIæä¾›äº†å¯¹å¹»è§‰å½±å“çš„æ›´ç»†å¾®ç†è§£ï¼Œä»è€Œçªå‡ºäº†å…¶åœ¨å¹»è§‰è¯„ä¼°ä¸­çš„ä¼˜åŠ¿ã€‚è¯¥ç ”ç©¶ä¸ºå¢å¼ºMLLMsåœ¨åŒ»ç–—ç¯å¢ƒä¸­çš„å¯é æ€§å¥ å®šäº†åŸºçŸ³ï¼Œå¹¶æä¾›äº†è§£å†³åŒ»ç–—åº”ç”¨ä¸­äººå·¥æ™ºèƒ½å¹»è§‰è¿™ä¸€å…³é”®æŒ‘æˆ˜çš„å¯è¡Œç­–ç•¥ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.18947v4">PDF</a> Published to AAAI-25 Bridge Program</p>
<p><strong>Summary</strong><br>åŒ»ç–—å¤§è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨åŒ»ç–—ä¿å¥åº”ç”¨ä¸­å…·æœ‰æ½œåŠ›ï¼Œä½†å…¶äº§ç”ŸåŒ»å­¦ä¸Šä¸åˆ‡å®é™…æˆ–ä¸å‡†ç¡®ä¿¡æ¯çš„å€¾å‘å¯¹æ‚£è€…æŠ¤ç†æ„æˆé‡å¤§é£é™©ã€‚æœ¬æ–‡ä»‹ç»MedHallBenchï¼Œä¸€ä¸ªå…¨é¢è¯„ä¼°å’Œæ”¹è¿›MLLMsä¸­äº§ç”Ÿå¹»è§‰çš„æ¡†æ¶ã€‚è¯¥æ¡†æ¶é€šè¿‡ç»“åˆä¸“å®¶éªŒè¯çš„åŒ»å­¦æ¡ˆä¾‹åœºæ™¯å’Œå»ºç«‹çš„åŒ»å­¦æ•°æ®åº“ï¼Œå»ºç«‹äº†ä¸€ä¸ªç¨³å¥çš„è¯„ä¼°æ•°æ®é›†ã€‚é€šè¿‡ç»“åˆè‡ªåŠ¨åŒ–è¯„åˆ†ä¸ä¸´åºŠä¸“å®¶è¯„ä¼°çš„å¤æ‚æµ‹é‡ç³»ç»Ÿï¼Œå¹¶åˆ©ç”¨å¼ºåŒ–å­¦ä¹ æ–¹æ³•å®ç°è‡ªåŠ¨æ ‡æ³¨ï¼Œèƒ½å¤Ÿåœ¨å¤šæ ·åŒ–çš„ä¸´åºŠç¯å¢ƒä¸­å…¨é¢è¯„ä¼°MLLMså¹¶ä¿æŒä¸¥æ ¼çš„æ ‡å‡†ã€‚é€šè¿‡å®éªŒå¯¹æ¯”ï¼Œæˆ‘ä»¬å‘ç°ACHMIåœ¨è¯„ä¼°å¹»è§‰æ–¹é¢ç›¸æ¯”ä¼ ç»ŸæŒ‡æ ‡å…·æœ‰æ›´å¾®å¦™çš„æ´å¯ŸåŠ›ï¼Œå‡¸æ˜¾å…¶åœ¨è¯„ä¼°å¹»è§‰æ–¹é¢çš„ä¼˜åŠ¿ã€‚è¯¥ç ”ç©¶ä¸ºå¢å¼ºMLLMsåœ¨åŒ»ç–—ä¿å¥ç¯å¢ƒä¸­çš„å¯é æ€§æä¾›äº†åŸºç¡€æ¡†æ¶ï¼Œå¹¶æå‡ºäº†è§£å†³åŒ»ç–—åº”ç”¨ä¸­AIå¹»è§‰è¿™ä¸€å…³é”®æŒ‘æˆ˜çš„è¡ŒåŠ¨ç­–ç•¥ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>MLLMsåœ¨åŒ»ç–—ä¿å¥åº”ç”¨ä¸­å…·æœ‰æ½œåŠ›ï¼Œä½†å­˜åœ¨äº§ç”ŸåŒ»å­¦ä¸Šä¸åˆ‡å®é™…ä¿¡æ¯çš„é£é™©ã€‚</li>
<li>MedHallBenchæ˜¯ä¸€ä¸ªè¯„ä¼°å’Œæ”¹è¿›MLLMsä¸­äº§ç”Ÿå¹»è§‰çš„æ¡†æ¶ã€‚</li>
<li>è¯¥æ¡†æ¶ç»“åˆä¸“å®¶éªŒè¯çš„åŒ»å­¦æ¡ˆä¾‹å’ŒåŒ»å­¦æ•°æ®åº“å»ºç«‹ç¨³å¥è¯„ä¼°æ•°æ®é›†ã€‚</li>
<li>MedHallBenchä½¿ç”¨è‡ªåŠ¨åŒ–è¯„åˆ†ä¸ä¸´åºŠä¸“å®¶è¯„ä¼°çš„å¤æ‚æµ‹é‡ç³»ç»Ÿã€‚</li>
<li>å¼ºåŒ–å­¦ä¹ ç”¨äºå®ç°è‡ªåŠ¨æ ‡æ³¨ï¼Œæé«˜è¯„ä¼°æ•ˆç‡ã€‚</li>
<li>ACHMIæŒ‡æ ‡ç›¸æ¯”ä¼ ç»Ÿè¯„ä¼°æ–¹æ³•æ›´èƒ½å¾®å¦™åœ°ç†è§£å¹»è§‰çš„å½±å“ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.18947">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-ccb90cb435765cc3a6a0bb653dc4de6c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1cba9133e99c5ee57d88ff716425e402.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-294d14263f597304d7828de8c279e897.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-24adf07b9a850a64ebf6125ef759c61b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-138aa38cd9bb218401ccc8eef5b60ca4.jpg" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="FlashRNN-I-O-Aware-Optimization-of-Traditional-RNNs-on-modern-hardware"><a href="#FlashRNN-I-O-Aware-Optimization-of-Traditional-RNNs-on-modern-hardware" class="headerlink" title="FlashRNN: I&#x2F;O-Aware Optimization of Traditional RNNs on modern hardware"></a>FlashRNN: I&#x2F;O-Aware Optimization of Traditional RNNs on modern hardware</h2><p><strong>Authors:Korbinian PÃ¶ppel, Maximilian Beck, Sepp Hochreiter</strong></p>
<p>While Transformers and other sequence-parallelizable neural network architectures seem like the current state of the art in sequence modeling, they specifically lack state-tracking capabilities. These are important for time-series tasks and logical reasoning. Traditional RNNs like LSTMs and GRUs, as well as modern variants like sLSTM do have these capabilities at the cost of strictly sequential processing. While this is often seen as a strong limitation, we show how fast these networks can get with our hardware-optimization FlashRNN in Triton and CUDA, optimizing kernels to the register level on modern GPUs. We extend traditional RNNs with a parallelization variant that processes multiple RNNs of smaller hidden state in parallel, similar to the head-wise processing in Transformers. To enable flexibility on different GPU variants, we introduce a new optimization framework for hardware-internal cache sizes, memory and compute handling. It models the hardware in a setting using polyhedral-like constraints, including the notion of divisibility. This speeds up the solution process in our ConstrINT library for general integer constraint satisfaction problems (integer CSPs). We show that our kernels can achieve 50x speed-ups over a vanilla PyTorch implementation and allow 40x larger hidden sizes compared to our Triton implementation. Our open-source kernels and the optimization library are released here to boost research in the direction of state-tracking enabled RNNs and sequence modeling: <a target="_blank" rel="noopener" href="https://github.com/NX-AI/flashrnn">https://github.com/NX-AI/flashrnn</a> </p>
<blockquote>
<p>è™½ç„¶Transformerå’Œå…¶ä»–å¯åºåˆ—å¹¶è¡ŒåŒ–çš„ç¥ç»ç½‘ç»œæ¶æ„çœ‹èµ·æ¥æ˜¯å½“å‰çš„åºåˆ—å»ºæ¨¡æŠ€æœ¯çš„å‰æ²¿ï¼Œä½†å®ƒä»¬ç‰¹åˆ«ç¼ºä¹çŠ¶æ€è·Ÿè¸ªèƒ½åŠ›ã€‚è¿™å¯¹äºæ—¶é—´åºåˆ—ä»»åŠ¡å’Œé€»è¾‘æ¨ç†å¾ˆé‡è¦ã€‚ä¼ ç»Ÿçš„RNNï¼Œå¦‚LSTMå’ŒGRUï¼Œä»¥åŠç°ä»£å˜ä½“ï¼Œå¦‚sLSTMï¼Œè™½ç„¶å…·æœ‰è¿™ç§èƒ½åŠ›ï¼Œä½†éœ€è¦ä¸¥æ ¼åºåˆ—å¤„ç†ã€‚è™½ç„¶è¿™é€šå¸¸è¢«è§†ä¸ºä¸€ä¸ªå¼ºå¤§é™åˆ¶ï¼Œä½†æˆ‘ä»¬é€šè¿‡ç¡¬ä»¶ä¼˜åŒ–çš„FlashRNNåœ¨Tritonå’ŒCUDAä¸­å±•ç¤ºäº†è¿™äº›ç½‘ç»œå¯ä»¥å¤šä¹ˆå¿«é€Ÿã€‚æˆ‘ä»¬å¯¹ä¼ ç»ŸRNNè¿›è¡Œäº†å¹¶è¡ŒåŒ–æ”¹è¿›ï¼Œå¯ä»¥åŒæ—¶å¤„ç†å¤šä¸ªå…·æœ‰è¾ƒå°éšè—çŠ¶æ€çš„å°å‹RNNï¼Œç±»ä¼¼äºTransformerä¸­çš„å¤šå¤´å¤„ç†ã€‚ä¸ºäº†åœ¨ä¸åŒGPUå˜ä½“ä¸Šå®ç°çµæ´»æ€§ï¼Œæˆ‘ä»¬ä¸ºç¡¬ä»¶å†…éƒ¨ç¼“å­˜å¤§å°ã€å†…å­˜å’Œè®¡ç®—å¤„ç†å¼•å…¥äº†ä¸€ä¸ªæ–°çš„ä¼˜åŒ–æ¡†æ¶ã€‚å®ƒä½¿ç”¨å¤šé¢ä½“ç±»ä¼¼çš„çº¦æŸåœ¨ä¸€ç§ç¯å¢ƒä¸­å»ºæ¨¡ç¡¬ä»¶ï¼ŒåŒ…æ‹¬å¯é™¤æ€§çš„æ¦‚å¿µã€‚è¿™åŠ é€Ÿäº†æˆ‘ä»¬ç”¨äºä¸€èˆ¬æ•´æ•°çº¦æŸæ»¡è¶³é—®é¢˜ï¼ˆæ•´æ•°CSPï¼‰çš„ConstrINTåº“ä¸­çš„è§£å†³æ–¹æ¡ˆè¿‡ç¨‹ã€‚æˆ‘ä»¬æ˜¾ç¤ºï¼Œæˆ‘ä»¬çš„å†…æ ¸å¯ä»¥å®ç°æ¯”PyTorchåŸç”Ÿå®ç°å¿«50å€çš„é€Ÿåº¦æå‡ï¼Œå¹¶ä¸”ä¸æˆ‘ä»¬Tritonå®ç°ç›¸æ¯”å…è®¸æ›´å¤§çš„éšè—å°ºå¯¸é«˜è¾¾40å€ã€‚æˆ‘ä»¬çš„å¼€æºå†…æ ¸å’Œä¼˜åŒ–åº“å·²åœ¨æ­¤å‘å¸ƒï¼Œä»¥ä¿ƒè¿›çŠ¶æ€è·Ÿè¸ªå¯ç”¨RNNå’Œåºåˆ—å»ºæ¨¡æ–¹å‘çš„ç ”ç©¶ï¼š<a target="_blank" rel="noopener" href="https://github.com/NX-AI/flashrnn%E3%80%82">https://github.com/NX-AI/flashrnnã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.07752v3">PDF</a> </p>
<p><strong>Summary</strong><br>     å˜æ¢å™¨å’Œå…¶ä»–å¯å¹¶è¡ŒåŒ–ç¥ç»ç½‘ç»œæ¶æ„è™½ç„¶çœ‹ä¼¼æ˜¯å½“å‰åºåˆ—å»ºæ¨¡çš„å°–ç«¯æŠ€æœ¯ï¼Œä½†å®ƒä»¬ç¼ºä¹çŠ¶æ€è·Ÿè¸ªèƒ½åŠ›ï¼Œè¿™å¯¹äºæ—¶é—´åºåˆ—ä»»åŠ¡å’Œé€»è¾‘æ¨ç†å¾ˆé‡è¦ã€‚ä¼ ç»ŸRNNsï¼ˆå¦‚LSTMå’ŒGRUï¼‰ä»¥åŠç°ä»£å˜ä½“ï¼ˆå¦‚sLSTMï¼‰ç¡®å®å…·æœ‰è¿™ç§èƒ½åŠ›ï¼Œä½†ä»£ä»·æ˜¯ä¸¥æ ¼åºåˆ—å¤„ç†ã€‚æˆ‘ä»¬é€šè¿‡ç¡¬ä»¶ä¼˜åŒ–çš„FlashRNNåœ¨Tritonå’ŒCUDAä¸­å±•ç¤ºäº†è¿™äº›ç½‘ç»œå¯ä»¥å¤šä¹ˆå¿«é€Ÿï¼Œä¼˜åŒ–å†…æ ¸è¾¾åˆ°ç°ä»£GPUçš„å¯„å­˜å™¨çº§åˆ«ã€‚æˆ‘ä»¬ä¸ºä¼ ç»ŸRNNså¼•å…¥äº†ä¸€ç§å¹¶è¡ŒåŒ–å˜ä½“ï¼Œå¹¶è¡Œå¤„ç†å¤šä¸ªå…·æœ‰è¾ƒå°éšè—çŠ¶æ€RNNï¼Œç±»ä¼¼äºTransformerä¸­çš„å¤´å¤„ç†ã€‚æˆ‘ä»¬è¿˜ä¸ºä¸åŒçš„GPUå˜ä½“å¼•å…¥äº†æ–°çš„ä¼˜åŒ–æ¡†æ¶ï¼Œå¯¹ç¡¬ä»¶å†…éƒ¨ç¼“å­˜å¤§å°ã€å†…å­˜å’Œè®¡ç®—å¤„ç†è¿›è¡Œå»ºæ¨¡ï¼Œä½¿ç”¨å¤šé¢ä½“çº¦æŸç­‰æ¦‚å¿µï¼ŒåŒ…æ‹¬å¯é™¤æ€§ã€‚è¿™åŠ å¿«äº†æˆ‘ä»¬ç”¨äºä¸€èˆ¬æ•´æ•°çº¦æŸæ»¡è¶³é—®é¢˜ï¼ˆinteger CSPsï¼‰çš„ConstrINTåº“çš„è§£å†³æ–¹æ¡ˆè¿‡ç¨‹ã€‚æˆ‘ä»¬å±•ç¤ºæˆ‘ä»¬çš„å†…æ ¸å¯ä»¥å®ç°æ¯”PyTorchåŸç”Ÿå®ç°å¿«50å€çš„é€Ÿåº¦ï¼Œå¹¶ä¸”ä¸æˆ‘ä»¬çš„Tritonå®ç°ç›¸æ¯”ï¼Œå…è®¸éšè—å±‚å¤§å°æ‰©å¤§40å€ã€‚æˆ‘ä»¬çš„å¼€æºå†…æ ¸å’Œä¼˜åŒ–åº“æ—¨åœ¨æ¨åŠ¨çŠ¶æ€è·Ÿè¸ªä½¿èƒ½çš„RNNå’Œåºåˆ—å»ºæ¨¡çš„ç ”ç©¶æ–¹å‘ï¼š<a target="_blank" rel="noopener" href="https://github.com/NX-AI/flashrnn">https://github.com/NX-AI/flashrnn</a></p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å˜æ¢å™¨å’Œå…¶ä»–ç¥ç»ç½‘ç»œæ¶æ„è™½ç„¶å…ˆè¿›ï¼Œä½†åœ¨çŠ¶æ€è·Ÿè¸ªæ–¹é¢å­˜åœ¨å±€é™ï¼Œè¿™å¯¹äºæ—¶é—´åºåˆ—ä»»åŠ¡å’Œé€»è¾‘æ¨ç†å¾ˆé‡è¦ã€‚</li>
<li>ä¼ ç»ŸRNNsï¼ˆå¦‚LSTMå’ŒGRUï¼‰å…·æœ‰çŠ¶æ€è·Ÿè¸ªèƒ½åŠ›ï¼Œä½†å¤„ç†é€Ÿåº¦å—é™åˆ¶ã€‚</li>
<li>FlashRNNé€šè¿‡ç¡¬ä»¶ä¼˜åŒ–æå‡äº†RNNçš„å¤„ç†é€Ÿåº¦ã€‚</li>
<li>å¼•å…¥äº†ä¸€ç§å¹¶è¡ŒåŒ–å˜ä½“ï¼Œèƒ½å¹¶è¡Œå¤„ç†å¤šä¸ªå…·æœ‰è¾ƒå°éšè—çŠ¶æ€çš„RNNã€‚</li>
<li>é’ˆå¯¹ä¸åŒGPUå˜ä½“ï¼Œæå‡ºäº†æ–°çš„ä¼˜åŒ–æ¡†æ¶ï¼Œè€ƒè™‘äº†ç¡¬ä»¶å†…éƒ¨ç¼“å­˜ã€å†…å­˜å’Œè®¡ç®—å¤„ç†ã€‚</li>
<li>é€šè¿‡ä½¿ç”¨å¤šé¢ä½“çº¦æŸç­‰æ¦‚å¿µï¼ŒåŒ…æ‹¬å¯é™¤æ€§ï¼ŒåŠ å¿«äº†æ•´æ•°çº¦æŸæ»¡è¶³é—®é¢˜çš„è§£å†³æ–¹æ¡ˆè¿‡ç¨‹ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.07752">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-e403497f69fdaf94fb671ccca61cb8c7.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-e41bb7743b3d9d3f6138c3a25e7f3e26.jpg" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="VERA-Explainable-Video-Anomaly-Detection-via-Verbalized-Learning-of-Vision-Language-Models"><a href="#VERA-Explainable-Video-Anomaly-Detection-via-Verbalized-Learning-of-Vision-Language-Models" class="headerlink" title="VERA: Explainable Video Anomaly Detection via Verbalized Learning of   Vision-Language Models"></a>VERA: Explainable Video Anomaly Detection via Verbalized Learning of   Vision-Language Models</h2><p><strong>Authors:Muchao Ye, Weiyang Liu, Pan He</strong></p>
<p>The rapid advancement of vision-language models (VLMs) has established a new paradigm in video anomaly detection (VAD): leveraging VLMs to simultaneously detect anomalies and provide comprehendible explanations for the decisions. Existing work in this direction often assumes the complex reasoning required for VAD exceeds the capabilities of pretrained VLMs. Consequently, these approaches either incorporate specialized reasoning modules during inference or rely on instruction tuning datasets through additional training to adapt VLMs for VAD. However, such strategies often incur substantial computational costs or data annotation overhead. To address these challenges in explainable VAD, we introduce a verbalized learning framework named VERA that enables VLMs to perform VAD without model parameter modifications. Specifically, VERA automatically decomposes the complex reasoning required for VAD into reflections on simpler, more focused guiding questions capturing distinct abnormal patterns. It treats these reflective questions as learnable parameters and optimizes them through data-driven verbal interactions between learner and optimizer VLMs, using coarsely labeled training data. During inference, VERA embeds the learned questions into model prompts to guide VLMs in generating segment-level anomaly scores, which are then refined into frame-level scores via the fusion of scene and temporal contexts. Experimental results on challenging benchmarks demonstrate that the learned questions of VERA are highly adaptable, significantly improving both detection performance and explainability of VLMs for VAD. </p>
<blockquote>
<p>è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰çš„å¿«é€Ÿå‘å±•ä¸ºè§†é¢‘å¼‚å¸¸æ£€æµ‹ï¼ˆVADï¼‰å»ºç«‹äº†ä¸€ç§æ–°çš„èŒƒå¼ï¼šåˆ©ç”¨VLMsåŒæ—¶æ£€æµ‹å¼‚å¸¸å¹¶ä¸ºå†³ç­–æä¾›å¯ç†è§£çš„è§£é‡Šã€‚ç°æœ‰å·¥ä½œé€šå¸¸è®¤ä¸ºVADæ‰€éœ€çš„å¤æ‚æ¨ç†è¶…å‡ºäº†é¢„è®­ç»ƒVLMsçš„èƒ½åŠ›èŒƒå›´ã€‚å› æ­¤ï¼Œè¿™äº›ç­–ç•¥è¦ä¹ˆåœ¨æ¨ç†è¿‡ç¨‹ä¸­èå…¥ä¸“é—¨çš„æ¨ç†æ¨¡å—ï¼Œè¦ä¹ˆé€šè¿‡é¢å¤–çš„è®­ç»ƒä¾èµ–æŒ‡ä»¤è°ƒæ•´æ•°æ®é›†æ¥é€‚åº”VADçš„VLMsã€‚ç„¶è€Œï¼Œè¿™æ ·çš„ç­–ç•¥å¾€å¾€å¸¦æ¥è¾ƒå¤§çš„è®¡ç®—æˆæœ¬æˆ–æ•°æ®æ ‡æ³¨å¼€é”€ã€‚ä¸ºäº†è§£å†³å¯è§£é‡ŠVADä¸­çš„è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§åä¸ºVERAçš„è¨€è¯­åŒ–å­¦ä¹ æ¡†æ¶ï¼Œè¯¥æ¡†æ¶èƒ½ä½¿VLMsæ‰§è¡ŒVADè€Œæ— éœ€ä¿®æ”¹æ¨¡å‹å‚æ•°ã€‚å…·ä½“æ¥è¯´ï¼ŒVERAä¼šè‡ªåŠ¨å°†VADæ‰€éœ€çš„å¤æ‚æ¨ç†åˆ†è§£æˆå¯¹æ•æ‰å„ç§å¼‚å¸¸æ¨¡å¼çš„æ›´ç®€å•ã€æ›´ä¸“æ³¨çš„å¼•å¯¼é—®é¢˜çš„åæ€ã€‚å®ƒå°†è¿™äº›åæ€é—®é¢˜è§†ä¸ºå¯å­¦ä¹ çš„å‚æ•°ï¼Œå¹¶é€šè¿‡å­¦ä¹ è€…ä¸ä¼˜åŒ–å™¨VLMsä¹‹é—´çš„æ•°æ®é©±åŠ¨è¨€è¯­äº¤äº’æ¥ä¼˜åŒ–è¿™äº›å‚æ•°ï¼Œä½¿ç”¨ç²—ç•¥æ ‡è®°çš„è®­ç»ƒæ•°æ®ã€‚åœ¨æ¨ç†è¿‡ç¨‹ä¸­ï¼ŒVERAå°†å­¦ä¹ åˆ°çš„é—®é¢˜åµŒå…¥åˆ°æ¨¡å‹æç¤ºä¸­ï¼Œå¼•å¯¼VLMsç”Ÿæˆåˆ†æ®µçº§åˆ«çš„å¼‚å¸¸åˆ†æ•°ï¼Œç„¶åé€šè¿‡åœºæ™¯å’Œæ—¶é—´ä¸Šä¸‹æ–‡çš„èåˆæ¥å®Œå–„å¸§çº§åˆ«çš„åˆ†æ•°ã€‚åœ¨å…·æœ‰æŒ‘æˆ˜æ€§çš„åŸºå‡†æµ‹è¯•ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼ŒVERAå­¦ä¹ çš„é—®é¢˜å…·æœ‰é«˜åº¦é€‚åº”æ€§ï¼Œæ˜¾è‘—æé«˜äº†VLMså¯¹VADçš„æ£€æµ‹æ€§èƒ½å’Œè§£é‡Šæ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.01095v3">PDF</a> Accepted in CVPR 2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†å¿«é€Ÿæ¨è¿›çš„è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰ä¸ºè§†é¢‘å¼‚å¸¸æ£€æµ‹ï¼ˆVADï¼‰æä¾›äº†æ–°çš„æ¨¡å¼ã€‚å€ŸåŠ©VLMsï¼Œè¯¥æ¨¡å¼å¯ä»¥æ£€æµ‹å¼‚å¸¸åŒæ—¶ä¸ºå†³ç­–æä¾›å¯ç†è§£çš„è§£é‡Šã€‚é’ˆå¯¹ç°æœ‰å·¥ä½œä¸­å¯¹é¢„è®­ç»ƒVLMsèƒ½åŠ›çš„å‡è®¾ï¼Œæˆ‘ä»¬å¼•å…¥äº†åä¸ºVERAçš„è¨€è¯­åŒ–å­¦ä¹ æ¡†æ¶ï¼Œä½¿VLMsèƒ½å¤Ÿåœ¨æ— éœ€ä¿®æ”¹æ¨¡å‹å‚æ•°çš„æƒ…å†µä¸‹è¿›è¡ŒVADã€‚VERAé€šè¿‡æ•°æ®é©±åŠ¨çš„è¨€è¯­äº¤äº’ä¼˜åŒ–æŒ‡å¯¼é—®é¢˜ï¼Œä»è€Œåˆ†è§£VADæ‰€éœ€çš„å¤æ‚æ¨ç†ã€‚åœ¨æ¨ç†è¿‡ç¨‹ä¸­ï¼ŒVERAå°†é—®é¢˜åµŒå…¥æ¨¡å‹æç¤ºä¸­ï¼ŒæŒ‡å¯¼VLMsç”Ÿæˆåˆ†æ®µçº§åˆ«çš„å¼‚å¸¸åˆ†æ•°ï¼Œå†é€šè¿‡åœºæ™¯å’Œä¸´æ—¶ä¸Šä¸‹æ–‡çš„èåˆï¼Œå½¢æˆå¸§çº§åˆ«çš„åˆ†æ•°ã€‚å®éªŒç»“æœè¯æ˜ï¼ŒVERAçš„é—®é¢˜å­¦ä¹ å…·æœ‰é«˜åº¦é€‚åº”æ€§ï¼Œæ˜¾è‘—æé«˜äº†VLMsåœ¨VADä¸­çš„æ£€æµ‹æ€§èƒ½å’Œè§£é‡Šæ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰åœ¨è§†é¢‘å¼‚å¸¸æ£€æµ‹ï¼ˆVADï¼‰ä¸­å±•ç°å‡ºæ–°çš„åº”ç”¨æ¨¡å¼ã€‚</li>
<li>VLMsèƒ½å¤ŸåŒæ—¶æ£€æµ‹å¼‚å¸¸å¹¶æä¾›è§£é‡Šï¼Œæ¨åŠ¨äº†VADçš„å‘å±•ã€‚</li>
<li>ç°æœ‰æ–¹æ³•å¸¸éœ€ä¸“ä¸šåŒ–çš„æ¨ç†æ¨¡å—æˆ–é¢å¤–çš„è®­ç»ƒæ•°æ®æ¥é€‚åº”VLMsè¿›è¡ŒVADï¼Œä½†ä¼šå¸¦æ¥è®¡ç®—æˆæœ¬æˆ–æ•°æ®æ ‡æ³¨è´Ÿæ‹…ã€‚</li>
<li>VERAæ¡†æ¶è§£å†³äº†è¿™äº›é—®é¢˜ï¼Œä½¿VLMsèƒ½åœ¨æ— éœ€ä¿®æ”¹å‚æ•°çš„æƒ…å†µä¸‹è¿›è¡ŒVADã€‚</li>
<li>VERAé€šè¿‡åˆ†è§£å¤æ‚æ¨ç†ä¸ºæ›´ç®€å•çš„é—®é¢˜æ¥æŒ‡å¯¼VADï¼Œè¿™äº›é—®é¢˜ä½œä¸ºå¯å­¦ä¹ çš„å‚æ•°è¿›è¡Œä¼˜åŒ–ã€‚</li>
<li>VERAåˆ©ç”¨ç²—æ ‡ç­¾çš„è®­ç»ƒæ•°æ®ï¼Œé€šè¿‡æ•°æ®é©±åŠ¨çš„è¨€è¯­äº¤äº’ä¼˜åŒ–é—®é¢˜å­¦ä¹ ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.01095">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-c85e22d8b5f135a92c0d541cbeb815c0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-908c2ac23a100ac670b04d06d85c32aa.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-08761fc33c0544f59ac03c00a0401b20.jpg" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1><h2 id="BALROG-Benchmarking-Agentic-LLM-and-VLM-Reasoning-On-Games"><a href="#BALROG-Benchmarking-Agentic-LLM-and-VLM-Reasoning-On-Games" class="headerlink" title="BALROG: Benchmarking Agentic LLM and VLM Reasoning On Games"></a>BALROG: Benchmarking Agentic LLM and VLM Reasoning On Games</h2><p><strong>Authors:Davide Paglieri, BartÅ‚omiej CupiaÅ‚, Samuel Coward, Ulyana Piterbarg, Maciej Wolczyk, Akbir Khan, Eduardo Pignatelli, Åukasz KuciÅ„ski, Lerrel Pinto, Rob Fergus, Jakob Nicolaus Foerster, Jack Parker-Holder, Tim RocktÃ¤schel</strong></p>
<p>Large Language Models (LLMs) and Vision Language Models (VLMs) possess extensive knowledge and exhibit promising reasoning abilities, however, they still struggle to perform well in complex, dynamic environments. Real-world tasks require handling intricate interactions, advanced spatial reasoning, long-term planning, and continuous exploration of new strategies-areas in which we lack effective methodologies for comprehensively evaluating these capabilities. To address this gap, we introduce BALROG, a novel benchmark designed to assess the agentic capabilities of LLMs and VLMs through a diverse set of challenging games. Our benchmark incorporates a range of existing reinforcement learning environments with varying levels of difficulty, including tasks that are solvable by non-expert humans in seconds to extremely challenging ones that may take years to master (e.g., the NetHack Learning Environment). We devise fine-grained metrics to measure performance and conduct an extensive evaluation of several popular open-source and closed-source LLMs and VLMs. Our findings indicate that while current models achieve partial success in the easier games, they struggle significantly with more challenging tasks. Notably, we observe severe deficiencies in vision-based decision-making, as several models perform worse when visual representations of the environments are provided. We release BALROG as an open and user-friendly benchmark to facilitate future research and development in the agentic community. Code and Leaderboard at balrogai.com. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å’Œè§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰æ‹¥æœ‰å¹¿æ³›çš„çŸ¥è¯†å¹¶å±•ç°å‡ºæœ‰å‰æ™¯çš„æ¨ç†èƒ½åŠ›ï¼Œç„¶è€Œï¼Œå®ƒä»¬åœ¨å¤æ‚ã€åŠ¨æ€çš„ç¯å¢ƒä¸­ä»ç„¶éš¾ä»¥è¡¨ç°å‡ºè‰¯å¥½çš„æ€§èƒ½ã€‚ç°å®ä¸–ç•Œä»»åŠ¡éœ€è¦å¤„ç†å¤æ‚çš„äº¤äº’ã€å…ˆè¿›çš„ç©ºé—´æ¨ç†ã€é•¿æœŸè§„åˆ’å’ŒæŒç»­æ¢ç´¢æ–°ç­–ç•¥â€”â€”æˆ‘ä»¬åœ¨è¿™äº›æ–¹é¢ç¼ºä¹å…¨é¢è¯„ä¼°è¿™äº›èƒ½åŠ›çš„æœ‰æ•ˆæ–¹æ³•è®ºã€‚ä¸ºäº†è§£å†³è¿™ä¸€å·®è·ï¼Œæˆ‘ä»¬å¼•å…¥äº†BALROGï¼Œè¿™æ˜¯ä¸€ä¸ªæ–°é¢–çš„åŸºå‡†æµ‹è¯•ï¼Œæ—¨åœ¨é€šè¿‡ä¸€ç³»åˆ—å…·æœ‰æŒ‘æˆ˜æ€§çš„æ¸¸æˆæ¥è¯„ä¼°LLMå’ŒVLMçš„è‡ªä¸»èƒ½åŠ›ã€‚æˆ‘ä»¬çš„åŸºå‡†æµ‹è¯•ç»“åˆäº†ä¸åŒéš¾åº¦çš„ç°æœ‰å¼ºåŒ–å­¦ä¹ ç¯å¢ƒï¼ŒåŒ…æ‹¬éä¸“å®¶äººç±»å¯ä»¥åœ¨å‡ ç§’é’Ÿå†…è§£å†³çš„ä»»åŠ¡åˆ°å¯èƒ½éœ€è¦å¤šå¹´æ‰èƒ½æŒæ¡çš„è¶…éš¾ä»»åŠ¡ï¼ˆä¾‹å¦‚NetHackå­¦ä¹ ç¯å¢ƒï¼‰ã€‚æˆ‘ä»¬åˆ¶å®šäº†ç²¾ç»†çš„æŒ‡æ ‡æ¥è¡¡é‡æ€§èƒ½ï¼Œå¹¶å¯¹å¤šä¸ªæµè¡Œçš„å¼€æºå’Œé—­æºLLMå’ŒVLMè¿›è¡Œäº†å¹¿æ³›è¯„ä¼°ã€‚æˆ‘ä»¬çš„ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œè™½ç„¶å½“å‰æ¨¡å‹åœ¨è¾ƒç®€å•çš„æ¸¸æˆä¸­å–å¾—äº†éƒ¨åˆ†æˆåŠŸï¼Œä½†åœ¨æ›´å…·æŒ‘æˆ˜æ€§çš„ä»»åŠ¡ä¸­å´é‡åˆ°äº†å¾ˆå¤§çš„å›°éš¾ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œæˆ‘ä»¬è§‚å¯Ÿåˆ°åœ¨åŸºäºè§†è§‰çš„å†³ç­–åˆ¶å®šæ–¹é¢å­˜åœ¨ä¸¥é‡ç¼ºé™·ï¼Œå› ä¸ºå½“æä¾›ç¯å¢ƒçš„è§†è§‰è¡¨ç¤ºæ—¶ï¼Œå‡ ä¸ªæ¨¡å‹çš„è¡¨ç°æ›´å·®ã€‚æˆ‘ä»¬å‘å¸ƒBALROGä½œä¸ºä¸€ä¸ªå¼€æ”¾å’Œç”¨æˆ·å‹å¥½çš„åŸºå‡†æµ‹è¯•ï¼Œä»¥ä¿ƒè¿›æœªæ¥åœ¨è‡ªä¸»ç¤¾åŒºä¸­çš„ç ”ç©¶å’Œå¼€å‘ã€‚ä»£ç å’Œæ’è¡Œæ¦œå¯åœ¨balrogai.comæŸ¥çœ‹ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2411.13543v2">PDF</a> Published as a conference paper at ICLR 2025</p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å’Œè§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰å…·å¤‡å¹¿æ³›çš„çŸ¥è¯†å’Œå‡ºè‰²çš„æ¨ç†èƒ½åŠ›ï¼Œä½†åœ¨å¤æ‚ã€åŠ¨æ€çš„ç¯å¢ƒä¸­è¡¨ç°æ¬ ä½³ã€‚ä¸ºè§£å†³ç°æœ‰è¯„ä¼°æ–¹æ³•æ— æ³•å…¨é¢è¯„ä¼°è¿™äº›æ¨¡å‹åœ¨ç°å®ä¸–ç•Œä»»åŠ¡ä¸­çš„ä»£ç†èƒ½åŠ›çš„é—®é¢˜ï¼Œæå‡ºäº†BALROGè¿™ä¸€æ–°åŸºå‡†æµ‹è¯•ï¼Œé€šè¿‡ä¸€ç³»åˆ—å…·æœ‰æŒ‘æˆ˜æ€§çš„æ¸¸æˆæ¥è¯„ä¼°LLMså’ŒVLMsçš„ä»£ç†èƒ½åŠ›ã€‚BALROGç»“åˆäº†ä¸åŒéš¾åº¦çš„ç°æœ‰å¼ºåŒ–å­¦ä¹ ç¯å¢ƒï¼ŒåŒ…æ‹¬éä¸“å®¶äººç±»å¯åœ¨å‡ ç§’é’Ÿå†…è§£å†³çš„ä»»åŠ¡åˆ°å¯èƒ½éœ€è¦æ•°å¹´æ‰èƒ½æŒæ¡çš„ä»»åŠ¡ã€‚è¯„ä¼°å‘ç°ï¼Œå½“å‰æ¨¡å‹åœ¨è¾ƒç®€å•çš„æ¸¸æˆä¸­å–å¾—äº†ä¸€å®šçš„æˆåŠŸï¼Œä½†åœ¨æ›´å…·æŒ‘æˆ˜æ€§çš„ä»»åŠ¡ä¸­å´é‡åˆ°äº†å›°éš¾ã€‚ç‰¹åˆ«æ˜¯å½“æä¾›ç¯å¢ƒçš„è§†è§‰è¡¨ç¤ºæ—¶ï¼Œè®¸å¤šæ¨¡å‹çš„è§†è§‰å†³ç­–èƒ½åŠ›å­˜åœ¨ä¸¥é‡ç¼ºé™·ã€‚BALROGä½œä¸ºä¸€ä¸ªå¼€æ”¾å’Œç”¨æˆ·å‹å¥½çš„åŸºå‡†æµ‹è¯•å‘å¸ƒï¼Œä»¥ä¿ƒè¿›æœªæ¥åœ¨ä»£ç†é¢†åŸŸçš„ç ”å‘ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLMså’ŒVLMsåœ¨å¤æ‚ã€åŠ¨æ€çš„ç¯å¢ƒä¸­è¡¨ç°ä¸è¶³ã€‚</li>
<li>BALROGæ˜¯ä¸€ä¸ªæ–°çš„åŸºå‡†æµ‹è¯•ï¼Œæ—¨åœ¨è¯„ä¼°LLMså’ŒVLMsåœ¨ç°å®ä¸–ç•Œä»»åŠ¡ä¸­çš„ä»£ç†èƒ½åŠ›ã€‚</li>
<li>BALROGç»“åˆäº†ä¸åŒéš¾åº¦çš„å¼ºåŒ–å­¦ä¹ ç¯å¢ƒï¼ŒåŒ…æ‹¬ä»ç®€å•åˆ°æéš¾çš„ä»»åŠ¡ã€‚</li>
<li>å½“å‰æ¨¡å‹åœ¨ç®€å•æ¸¸æˆä¸­éƒ¨åˆ†æˆåŠŸï¼Œä½†åœ¨æ›´å…·æŒ‘æˆ˜æ€§çš„ä»»åŠ¡ä¸­é‡åˆ°å›°éš¾ã€‚</li>
<li>æ¨¡å‹çš„è§†è§‰å†³ç­–èƒ½åŠ›å­˜åœ¨ä¸¥é‡ç¼ºé™·ã€‚</li>
<li>BALROGä½œä¸ºå¼€æ”¾å’Œç”¨æˆ·å‹å¥½çš„åŸºå‡†æµ‹è¯•å‘å¸ƒï¼Œæ–¹ä¾¿æœªæ¥ç ”ç©¶å’Œå‘å±•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2411.13543">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-50411dcff52719ca83a45a155a7eed32.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-9e2171643db4673e7a03258e5beafed7.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-7f9c10191c3227a54a2c21a25621f6a2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-00d616db923791adfaaebfa4006122cc.jpg" align="middle">
</details>


<h1 id="-19"><a href="#-19" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-04-08/R1_Reasoning/">https://kedreamix.github.io/Talk2Paper/Paper/2025-04-08/R1_Reasoning/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/R1-Reasoning/">
                                    <span class="chip bg-color">R1_Reasoning</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-04-08/LLM/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-8a86db5510bded256462018e6fce4ba3.jpg" class="responsive-img" alt="LLM">
                        
                        <span class="card-title">LLM</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            LLM æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-04-08  Align to Structure Aligning Large Language Models with Structural   Information
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-04-08
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/LLM/" class="post-category">
                                    LLM
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/LLM/">
                        <span class="chip bg-color">LLM</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-04-06/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-26613c19e31caa9022529215f236cdb2.jpg" class="responsive-img" alt="åŒ»å­¦å›¾åƒ">
                        
                        <span class="card-title">åŒ»å­¦å›¾åƒ</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            åŒ»å­¦å›¾åƒ æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-04-06  MedConv Convolutions Beat Transformers on Long-Tailed Bone Density   Prediction
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-04-06
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/" class="post-category">
                                    åŒ»å­¦å›¾åƒ
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                        <span class="chip bg-color">åŒ»å­¦å›¾åƒ</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">24231k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
