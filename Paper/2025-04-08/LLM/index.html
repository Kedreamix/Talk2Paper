<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="LLM">
    <meta name="description" content="LLM æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-04-08  Align to Structure Aligning Large Language Models with Structural   Information">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>LLM | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-8a86db5510bded256462018e6fce4ba3.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">LLM</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/LLM/">
                                <span class="chip bg-color">LLM</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/LLM/" class="post-category">
                                LLM
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-04-08
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-05-14
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    20.4k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    83 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-04-08-æ›´æ–°"><a href="#2025-04-08-æ›´æ–°" class="headerlink" title="2025-04-08 æ›´æ–°"></a>2025-04-08 æ›´æ–°</h1><h2 id="Align-to-Structure-Aligning-Large-Language-Models-with-Structural-Information"><a href="#Align-to-Structure-Aligning-Large-Language-Models-with-Structural-Information" class="headerlink" title="Align to Structure: Aligning Large Language Models with Structural   Information"></a>Align to Structure: Aligning Large Language Models with Structural   Information</h2><p><strong>Authors:Zae Myung Kim, Anand Ramachandran, Farideh Tavazoee, Joo-Kyung Kim, Oleg Rokhlenko, Dongyeop Kang</strong></p>
<p>Generating long, coherent text remains a challenge for large language models (LLMs), as they lack hierarchical planning and structured organization in discourse generation. We introduce Structural Alignment, a novel method that aligns LLMs with human-like discourse structures to enhance long-form text generation. By integrating linguistically grounded discourse frameworks into reinforcement learning, our approach guides models to produce coherent and well-organized outputs. We employ a dense reward scheme within a Proximal Policy Optimization framework, assigning fine-grained, token-level rewards based on the discourse distinctiveness relative to human writing. Two complementary reward models are evaluated: the first improves readability by scoring surface-level textual features to provide explicit structuring, while the second reinforces deeper coherence and rhetorical sophistication by analyzing global discourse patterns through hierarchical discourse motifs, outperforming both standard and RLHF-enhanced models in tasks such as essay generation and long-document summarization. All training data and code will be publicly shared at <a target="_blank" rel="noopener" href="https://github.com/minnesotanlp/struct_align">https://github.com/minnesotanlp/struct_align</a>. </p>
<blockquote>
<p>ç”Ÿæˆé•¿æ–‡å¹¶ä½¿å…¶ä¿æŒè¿è´¯æ€§å¯¹äºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ¥è¯´ä»ç„¶æ˜¯ä¸€ä¸ªæŒ‘æˆ˜ï¼Œå› ä¸ºå®ƒä»¬ç¼ºä¹å±‚æ¬¡æ€§çš„è§„åˆ’å’Œç»“æ„åŒ–ç»„ç»‡æ¥è¿›è¡Œæ–‡æœ¬ç”Ÿæˆã€‚æˆ‘ä»¬å¼•å…¥äº†ç»“æ„å¯¹é½è¿™ä¸€æ–°æ–¹æ³•ï¼Œé€šè¿‡ä½¿LLMä¸äººç±»ä¸€æ ·çš„æ–‡æœ¬ç»“æ„è¿›è¡Œå¯¹é½ï¼Œä»¥æå‡é•¿æ–‡æœ¬ç”Ÿæˆçš„èƒ½åŠ›ã€‚é€šè¿‡ç»“åˆè¯­è¨€å­¦çš„æ–‡æœ¬æ¡†æ¶å¹¶åº”ç”¨äºå¼ºåŒ–å­¦ä¹ ï¼Œæˆ‘ä»¬çš„æ–¹æ³•èƒ½å¼•å¯¼æ¨¡å‹äº§ç”Ÿè¿è´¯ã€ç»„ç»‡è‰¯å¥½çš„è¾“å‡ºã€‚æˆ‘ä»¬åœ¨æ¥è¿‘ç­–ç•¥ä¼˜åŒ–æ¡†æ¶å†…é‡‡ç”¨äº†ä¸€ç§å¯†é›†çš„å¥–åŠ±æ–¹æ¡ˆï¼Œæ ¹æ®ä¸äººç±»å†™ä½œç›¸æ¯”çš„æ–‡æœ¬ç‰¹å¾ç²¾ç»†åº¦æ¥åˆ†é…ä»¤ç‰Œçº§åˆ«çš„å¥–åŠ±ã€‚è¯„ä¼°äº†ä¸¤ç§äº’è¡¥çš„å¥–åŠ±æ¨¡å‹ï¼šç¬¬ä¸€ç§é€šè¿‡è¯„åˆ†è¡¨é¢çº§çš„æ–‡æœ¬ç‰¹å¾æ¥æé«˜å¯è¯»æ€§ï¼Œæä¾›æ˜ç¡®çš„ç»“æ„ï¼›ç¬¬äºŒç§é€šè¿‡å±‚æ¬¡åŒ–çš„æ–‡æœ¬æ¨¡å¼åˆ†ææ¥åŠ å¼ºæ›´æ·±å±‚æ¬¡çš„è¿è´¯æ€§å’Œä¿®è¾æŠ€å·§ï¼Œåœ¨è¯¸å¦‚ä½œæ–‡ç”Ÿæˆå’Œé•¿æ–‡æ¡£æ‘˜è¦ç­‰ä»»åŠ¡ä¸­è¡¨ç°å‡ºä¼˜äºæ ‡å‡†æ¨¡å‹å’Œå¢å¼ºå‹RLHFæ¨¡å‹çš„æ€§èƒ½ã€‚æ‰€æœ‰è®­ç»ƒæ•°æ®å’Œä»£ç å°†åœ¨<a target="_blank" rel="noopener" href="https://github.com/minnesotanlp/struct_align%E4%B8%8A%E5%85%AC%E5%BC%80%E5%88%86%E4%BA%AB%E3%80%82">https://github.com/minnesotanlp/struct_alignä¸Šå…¬å¼€åˆ†äº«ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.03622v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨ç”Ÿæˆé•¿ç¯‡è¿è´¯æ–‡æœ¬æ—¶å­˜åœ¨æŒ‘æˆ˜ï¼Œç¼ºä¹å±‚æ¬¡è§„åˆ’å’Œç»“æ„åŒ–ç»„ç»‡ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†ç»“æ„å¯¹é½æ³•ï¼Œé€šè¿‡æ•´åˆè¯­è¨€åŸºç¡€çš„è¯è¯­æ¡†æ¶å’Œå¼ºåŒ–å­¦ä¹ ï¼Œä½¿LLMsä¸äººç±»è¯è¯­ç»“æ„å¯¹é½ï¼Œæå‡é•¿ç¯‡æ–‡æœ¬ç”Ÿæˆèƒ½åŠ›ã€‚æˆ‘ä»¬é‡‡ç”¨å¯†é›†å¥–åŠ±æ–¹æ¡ˆï¼Œåœ¨è¿‘ç«¯ç­–ç•¥ä¼˜åŒ–æ¡†æ¶å†…ï¼Œæ ¹æ®è¯è¯­çš„é²œæ˜æ€§ç›¸å¯¹äºäººç±»å†™ä½œç»™äºˆç²¾ç»†æ ‡è®°å¥–åŠ±ã€‚è¯„ä¼°äº†ä¸¤ç§äº’è¡¥å¥–åŠ±æ¨¡å‹ï¼Œç¬¬ä¸€ç§é€šè¿‡è¯„åˆ†è¡¨é¢æ–‡æœ¬ç‰¹å¾æé«˜å¯è¯»æ€§ï¼Œæä¾›æ˜ç¡®çš„ç»“æ„æ€§ï¼›ç¬¬äºŒç§é€šè¿‡è¯†åˆ«å…¨å±€è¯è¯­æ¨¡å¼åŠ å¼ºæ·±å±‚æ¬¡è¿è´¯æ€§å’Œä¿®è¾æŠ€å·§ã€‚æ­¤æ–¹æ³•åœ¨ä½œæ–‡ç”Ÿæˆå’Œé•¿æ–‡æ¡£æ‘˜è¦ç­‰ä»»åŠ¡ä¸­è¡¨ç°ä¼˜äºæ ‡å‡†æ¨¡å‹å’ŒRLHFå¢å¼ºæ¨¡å‹ã€‚ç›¸å…³è®­ç»ƒæ•°æ®å’Œä»£ç å°†å…¬å¼€åˆ†äº«åœ¨<a target="_blank" rel="noopener" href="https://github.com/minnesotanlp/struct_align%E3%80%82">https://github.com/minnesotanlp/struct_alignã€‚</a></p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLMåœ¨ç”Ÿæˆé•¿ç¯‡è¿è´¯æ–‡æœ¬æ—¶å­˜åœ¨æŒ‘æˆ˜ï¼Œç¼ºä¹å±‚æ¬¡è§„åˆ’å’Œç»“æ„åŒ–ç»„ç»‡ã€‚</li>
<li>æå‡ºäº†ç»“æ„å¯¹é½æ³•ï¼Œé€šè¿‡æ•´åˆè¯­è¨€åŸºç¡€çš„è¯è¯­æ¡†æ¶å’Œå¼ºåŒ–å­¦ä¹ æ¥æå‡LLMçš„é•¿ç¯‡æ–‡æœ¬ç”Ÿæˆèƒ½åŠ›ã€‚</li>
<li>é‡‡ç”¨å¯†é›†å¥–åŠ±æ–¹æ¡ˆï¼Œæ ¹æ®è¯è¯­çš„é²œæ˜æ€§ç›¸å¯¹äºäººç±»å†™ä½œç»™äºˆç²¾ç»†æ ‡è®°å¥–åŠ±ã€‚</li>
<li>æœ‰ä¸¤ç§äº’è¡¥å¥–åŠ±æ¨¡å‹ï¼Œåˆ†åˆ«å…³æ³¨æé«˜å¯è¯»æ€§å’ŒåŠ å¼ºæ·±å±‚æ¬¡è¿è´¯æ€§åŠä¿®è¾æŠ€å·§ã€‚</li>
<li>è¯¥æ–¹æ³•é€šè¿‡æ˜ç¡®çš„ç»“æ„æ€§å’Œå¯¹å…¨å±€è¯è¯­æ¨¡å¼çš„è¯†åˆ«æ¥æ”¹è¿›LLMçš„è¯è¯­ç”Ÿæˆã€‚</li>
<li>æ­¤æ–¹æ³•åœ¨ä½œæ–‡ç”Ÿæˆå’Œé•¿æ–‡æ¡£æ‘˜è¦ç­‰ä»»åŠ¡ä¸­çš„è¡¨ç°ä¼˜äºæ ‡å‡†æ¨¡å‹å’ŒRLHFå¢å¼ºæ¨¡å‹ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.03622">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-0126efc095d4796c35626cc43fd7c6ac.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ddc3e07ddbaeb17a583e313bb1ef1178.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c9ce1e6bef0ade7fe07fb464ee231724.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-3a6eae9d05e6e010d934e0e7f0c00070.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="VISTA-OCR-Towards-generative-and-interactive-end-to-end-OCR-models"><a href="#VISTA-OCR-Towards-generative-and-interactive-end-to-end-OCR-models" class="headerlink" title="VISTA-OCR: Towards generative and interactive end to end OCR models"></a>VISTA-OCR: Towards generative and interactive end to end OCR models</h2><p><strong>Authors:Laziz Hamdi, Amine Tamasna, Pascal Boisson, Thierry Paquet</strong></p>
<p>We introduce \textbf{VISTA-OCR} (Vision and Spatially-aware Text Analysis OCR), a lightweight architecture that unifies text detection and recognition within a single generative model. Unlike conventional methods that require separate branches with dedicated parameters for text recognition and detection, our approach leverages a Transformer decoder to sequentially generate text transcriptions and their spatial coordinates in a unified branch. Built on an encoder-decoder architecture, VISTA-OCR is progressively trained, starting with the visual feature extraction phase, followed by multitask learning with multimodal token generation. To address the increasing demand for versatile OCR systems capable of advanced tasks, such as content-based text localization \ref{content_based_localization}, we introduce new prompt-controllable OCR tasks during pre-training.To enhance the modelâ€™s capabilities, we built a new dataset composed of real-world examples enriched with bounding box annotations and synthetic samples. Although recent Vision Large Language Models (VLLMs) can efficiently perform these tasks, their high computational cost remains a barrier for practical deployment. In contrast, our VISTA$_{\text{omni}}$ variant processes both handwritten and printed documents with only 150M parameters, interactively, by prompting. Extensive experiments on multiple datasets demonstrate that VISTA-OCR achieves better performance compared to state-of-the-art specialized models on standard OCR tasks while showing strong potential for more sophisticated OCR applications, addressing the growing need for interactive OCR systems. All code and annotations for VISTA-OCR will be made publicly available upon acceptance. </p>
<blockquote>
<p>æˆ‘ä»¬ä»‹ç»äº†<strong>VISTA-OCR</strong>ï¼ˆè§†è§‰å’Œç©ºé—´æ„ŸçŸ¥æ–‡æœ¬åˆ†æOCRï¼‰ï¼Œè¿™æ˜¯ä¸€ä¸ªè½»é‡çº§çš„æ¶æ„ï¼Œåœ¨ä¸€ä¸ªå•ä¸€çš„ç”Ÿæˆæ¨¡å‹ä¸­ç»Ÿä¸€äº†æ–‡æœ¬æ£€æµ‹å’Œè¯†åˆ«ã€‚ä¸åŒäºä¼ ç»Ÿæ–¹æ³•éœ€è¦ä¸ºæ–‡æœ¬è¯†åˆ«å’Œæ£€æµ‹è®¾ç½®å•ç‹¬çš„åˆ†æ”¯å’Œä¸“ç”¨å‚æ•°ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åˆ©ç”¨Transformerè§£ç å™¨æ¥é¡ºåºç”Ÿæˆæ–‡æœ¬è½¬å½•å’Œå…¶ç©ºé—´åæ ‡çš„ç»Ÿä¸€åˆ†æ”¯ã€‚VISTA-OCRåŸºäºç¼–ç å™¨-è§£ç å™¨æ¶æ„æ„å»ºï¼Œé‡‡ç”¨æ¸è¿›å¼è®­ç»ƒæ–¹æ³•ï¼Œé¦–å…ˆä»è§†è§‰ç‰¹å¾æå–é˜¶æ®µå¼€å§‹ï¼Œç„¶åæ˜¯å¤šä»»åŠ¡å­¦ä¹ æ¨¡æ€ä»¤ç‰Œç”Ÿæˆã€‚ä¸ºäº†æ»¡è¶³å¯¹èƒ½å¤Ÿè¿›è¡Œé«˜çº§ä»»åŠ¡ï¼ˆå¦‚åŸºäºå†…å®¹çš„æ–‡æœ¬å®šä½ï¼‰çš„é€šç”¨OCRç³»ç»Ÿçš„æ—¥ç›Šå¢é•¿éœ€æ±‚ï¼Œæˆ‘ä»¬åœ¨é¢„è®­ç»ƒæœŸé—´å¼•å…¥äº†æ–°çš„æç¤ºå¯æ§OCRä»»åŠ¡ã€‚ä¸ºäº†æé«˜æ¨¡å‹çš„èƒ½åŠ›ï¼Œæˆ‘ä»¬æ„å»ºäº†ä¸€ä¸ªæ–°çš„æ•°æ®é›†ï¼Œç”±ä¸°å¯Œçš„è¾¹ç•Œæ¡†æ³¨é‡Šçš„åˆæˆæ ·æœ¬å’ŒçœŸå®ä¸–ç•Œç¤ºä¾‹ç»„æˆã€‚å°½ç®¡æœ€è¿‘çš„è§†è§‰å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆVLLMï¼‰å¯ä»¥æœ‰æ•ˆåœ°æ‰§è¡Œè¿™äº›ä»»åŠ¡ï¼Œä½†å®ƒä»¬çš„é«˜è®¡ç®—æˆæœ¬ä»ç„¶æ˜¯å®é™…éƒ¨ç½²çš„éšœç¢ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼Œæˆ‘ä»¬çš„VISTA${\text{omni}}$å˜ä½“ä»…ä½¿ç”¨150Må‚æ•°å³å¯äº¤äº’å¼åœ°å¤„ç†æ‰‹å†™å’Œæ‰“å°æ–‡æ¡£ã€‚åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼ŒVISTA-OCRåœ¨æ ‡å‡†OCRä»»åŠ¡ä¸Šçš„æ€§èƒ½ä¼˜äºæœ€æ–°ä¸“ä¸šæ¨¡å‹ï¼ŒåŒæ—¶å¯¹äºæ›´å¤æ‚çš„OCRåº”ç”¨æ˜¾ç¤ºå‡ºå¼ºå¤§æ½œåŠ›ï¼Œæ»¡è¶³æ—¥ç›Šå¢é•¿çš„å¯¹äº¤äº’å¼OCRç³»ç»Ÿçš„éœ€æ±‚ã€‚VISTA-OCRçš„æ‰€æœ‰ä»£ç å’Œæ³¨é‡Šåœ¨éªŒæ”¶åå°†å…¬å¼€å‘å¸ƒã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.03621v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†VISTA-OCRï¼ˆå…·æœ‰è§†è§‰å’Œç©ºé—´æ„ŸçŸ¥èƒ½åŠ›çš„æ–‡æœ¬åˆ†æOCRï¼‰ï¼Œè¿™æ˜¯ä¸€ç§è½»å‹æ¶æ„ï¼Œå°†æ–‡æœ¬æ£€æµ‹å’Œè¯†åˆ«ç»Ÿä¸€åˆ°ä¸€ä¸ªå•ä¸€çš„ç”Ÿæˆæ¨¡å‹ä¸­ã€‚ä¸åŒäºä¼ ç»Ÿæ–¹æ³•ï¼Œå®ƒé‡‡ç”¨ä¸€ä¸ªTransformerè§£ç å™¨ï¼Œåœ¨ä¸€ä¸ªç»Ÿä¸€çš„åˆ†æ”¯ä¸­é¡ºåºç”Ÿæˆæ–‡æœ¬è½¬å½•å’Œå…¶ç©ºé—´åæ ‡ï¼Œæ— éœ€ä¸ºæ–‡æœ¬è¯†åˆ«å’Œæ£€æµ‹è®¾ç½®å•ç‹¬çš„åˆ†æ”¯å’Œä¸“ç”¨å‚æ•°ã€‚VISTA-OCRé€æ­¥è®­ç»ƒï¼Œèµ·å§‹äºè§†è§‰ç‰¹å¾æå–é˜¶æ®µï¼Œç„¶åæ˜¯ç”¨äºå¤šæ¨¡æ€æ ‡è®°ç”Ÿæˆçš„å¤šä»»åŠ¡å­¦ä¹ ã€‚ä¸ºæé«˜æ¨¡å‹çš„æ½œåŠ›å¹¶æ»¡è¶³æ—¥ç›Šå¢é•¿çš„å¯¹é«˜çº§OCRåº”ç”¨çš„éœ€æ±‚ï¼Œå¦‚åœ¨åŸºäºå†…å®¹çš„å†…å®¹å®šä½ç­‰åº”ç”¨ä¸­å¼•å…¥äº†æ–°çš„æç¤ºæ§åˆ¶OCRä»»åŠ¡è¿›è¡Œé¢„è®­ç»ƒã€‚ä¸ºäº†å¢å¼ºæ¨¡å‹çš„æ€§èƒ½ï¼Œä½¿ç”¨ç”±å¸¦è¾¹ç•Œæ¡†æ³¨è§£çš„çœŸå®ä¸–ç•Œç¤ºä¾‹å’Œåˆæˆæ ·æœ¬ç»„æˆçš„æ–°æ•°æ®é›†è¿›è¡Œè®­ç»ƒã€‚è™½ç„¶æœ€è¿‘çš„è§†è§‰å¤§å‹è¯­è¨€æ¨¡å‹å¯ä»¥æœ‰æ•ˆåœ°æ‰§è¡Œè¿™äº›ä»»åŠ¡ï¼Œä½†å®ƒä»¬çš„é«˜è®¡ç®—æˆæœ¬ä»æ˜¯å®é™…éƒ¨ç½²çš„éšœç¢ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼Œæˆ‘ä»¬çš„VISTAomniå˜ä½“ä»…ä½¿ç”¨1.5äº¿ä¸ªå‚æ•°å³å¯äº¤äº’å¼åœ°å¤„ç†æ‰‹å†™å’Œæ‰“å°æ–‡æ¡£ã€‚åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼ŒVISTA-OCRåœ¨æ ‡å‡†OCRä»»åŠ¡ä¸Šå–å¾—äº†æ¯”æœ€æ–°ä¸“ç”¨æ¨¡å‹æ›´å¥½çš„æ€§èƒ½ï¼Œå¹¶å…·æœ‰å¼ºå¤§çš„æ½œåŠ›æ»¡è¶³æ›´å¤æ‚çš„OCRåº”ç”¨éœ€æ±‚ã€‚æ»¡è¶³éœ€æ±‚ã€‚æä¾›æ‘˜è¦çš„å†…å®¹å°†åœ¨æ­£å¼é€šè¿‡åå…¬å¼€æ‰€æœ‰ä»£ç å’Œæ³¨é‡Šã€‚     â€‹â€‹â€‹â€‹    â€‹â€‹ï¼ˆä»¥ä¸‹æ˜¯ä»¥ç®€åŒ–æ±‰å­—ç”Ÿæˆçš„å…³é”®ä¿¡æ¯ï¼‰   â€‹â€‹ â€‹â€‹æ ¹æ®æä¾›çš„æŠ€æœ¯è®ºæ–‡æ‘˜è¦å¯çŸ¥ï¼Œæœ¬æ–‡ä»‹ç»äº†ä¸€ç§æ–°å‹çš„æ–‡æœ¬æ£€æµ‹å’Œè¯†åˆ«æ¨¡å‹VISTA-OCRã€‚è¯¥æ¨¡å‹é‡‡ç”¨å•ä¸€ç”Ÿæˆæ¨¡å‹å®ç°æ–‡æœ¬æ£€æµ‹å’Œè¯†åˆ«çš„ç»Ÿä¸€ï¼Œé€šè¿‡åˆ©ç”¨Transformerè§£ç å™¨ç”Ÿæˆæ–‡æœ¬è½¬å½•åŠå…¶ç©ºé—´åæ ‡ï¼Œç®€åŒ–äº†ä¼ ç»Ÿæ–¹æ³•ä¸­éœ€è¦ç‹¬ç«‹å¤„ç†æ–‡æœ¬è¯†åˆ«å’Œæ£€æµ‹çš„å¤æ‚æµç¨‹ã€‚è¯¥æ¨¡å‹å…·æœ‰ä¼˜ç§€çš„æ€§èƒ½è¡¨ç°ï¼Œèƒ½å¤Ÿåœ¨å¤šä¸ªæ•°æ®é›†ä¸Šå®ç°ä¼˜äºç°æœ‰æ¨¡å‹çš„æ€§èƒ½è¡¨ç°ï¼Œå¹¶ä¸”å…·æœ‰å¼ºå¤§çš„æ½œåŠ›åº”ç”¨äºæ›´å¤æ‚çš„OCRåº”ç”¨éœ€æ±‚ã€‚æ­¤å¤–ï¼Œä¸ºäº†æ»¡è¶³æ—¥ç›Šå¢é•¿çš„éœ€æ±‚ï¼Œè¯¥æ¨¡å‹è¿˜å¼•å…¥äº†æ–°çš„æç¤ºæ§åˆ¶OCRä»»åŠ¡è¿›è¡Œé¢„è®­ç»ƒï¼Œå¹¶é‡‡ç”¨æ–°æ•°æ®é›†è¿›è¡Œè®­ç»ƒä»¥å¢å¼ºæ€§èƒ½ã€‚å°½ç®¡ä¸€äº›ç°æœ‰æ¨¡å‹å…·å¤‡é«˜æ•ˆçš„æ€§èƒ½è¡¨ç°èƒ½åŠ›å¾ˆå¼ºå¤§ä½†å¯¹èµ„æºéœ€æ±‚æ¯”è¾ƒé«˜è€Œè¯¥æ¨¡å‹ä»¥è¾ƒä½çš„å‚æ•°é‡å®ç°äº†é«˜æ•ˆçš„æ€§èƒ½è¡¨ç°æ»¡è¶³äº†å®é™…åº”ç”¨çš„éœ€æ±‚ã€‚è¯¥æ¨¡å‹å°†åœ¨æ­£å¼é€šè¿‡åå…¬å¼€æ‰€æœ‰ä»£ç å’Œæ³¨é‡Šä»¥ä¾›å…¬ä¼—ä½¿ç”¨ã€‚â€‹â€‹<br>     â€‹â€‹<br>â€‹<br><strong>Key Takeaways</strong></p>
<p>ä»¥ä¸‹æ˜¯è®ºæ–‡çš„ä¸»è¦è§‚ç‚¹å’Œå…³é”®ä¿¡æ¯ç‚¹ï¼š</p>
<ol>
<li>VISTA-OCRæ˜¯ä¸€ä¸ªæ–°å‹çš„æ–‡æœ¬æ£€æµ‹å’Œè¯†åˆ«æ¨¡å‹ï¼Œå°†æ–‡æœ¬æ£€æµ‹å’Œè¯†åˆ«ç»Ÿä¸€åˆ°ä¸€ä¸ªå•ä¸€çš„ç”Ÿæˆæ¨¡å‹ä¸­ã€‚</li>
<li>è¯¥æ¨¡å‹é‡‡ç”¨Transformerè§£ç å™¨ç”Ÿæˆæ–‡æœ¬è½¬å½•åŠå…¶ç©ºé—´åæ ‡ï¼Œç®€åŒ–äº†ä¼ ç»Ÿæ–¹æ³•çš„å¤æ‚æµç¨‹ã€‚</li>
<li>VISTA-OCRé€šè¿‡å¼•å…¥æ–°çš„æç¤ºæ§åˆ¶OCRä»»åŠ¡æ¥æé«˜æ¨¡å‹çš„æ½œåŠ›å¹¶æ»¡è¶³å¤æ‚OCRåº”ç”¨çš„éœ€æ±‚ã€‚</li>
<li>è¯¥æ¨¡å‹ä½¿ç”¨ä¸€ä¸ªæ–°çš„æ•°æ®é›†è¿›è¡Œè®­ç»ƒï¼Œè¯¥æ•°æ®é›†ç”±çœŸå®ä¸–ç•Œç¤ºä¾‹å’Œåˆæˆæ ·æœ¬ç»„æˆï¼Œä»¥å¢å¼ºæ¨¡å‹çš„æ€§èƒ½ã€‚</li>
<li>ä¸ç°æœ‰æ¨¡å‹ç›¸æ¯”ï¼ŒVISTA-OCRåœ¨å¤šä¸ªæ•°æ®é›†ä¸Šå®ç°äº†æ›´å¥½çš„æ€§èƒ½è¡¨ç°ã€‚</li>
<li>VISTA-OCRå…·æœ‰è¾ƒä½çš„è®¡ç®—æˆæœ¬ï¼Œä»¥è¾ƒå°‘çš„å‚æ•°å®ç°äº†é«˜æ•ˆçš„æ€§èƒ½è¡¨ç°ï¼Œæ»¡è¶³äº†å®é™…åº”ç”¨çš„éœ€æ±‚ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.03621">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-8a86db5510bded256462018e6fce4ba3.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-a826726736243064cb471e4a74cfc613.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c51f4bc2923e733f8a3e268073cc49b5.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="EnrichIndex-Using-LLMs-to-Enrich-Retrieval-Indices-Offline"><a href="#EnrichIndex-Using-LLMs-to-Enrich-Retrieval-Indices-Offline" class="headerlink" title="EnrichIndex: Using LLMs to Enrich Retrieval Indices Offline"></a>EnrichIndex: Using LLMs to Enrich Retrieval Indices Offline</h2><p><strong>Authors:Peter Baile Chen, Tomer Wolfson, Michael Cafarella, Dan Roth</strong></p>
<p>Existing information retrieval systems excel in cases where the language of target documents closely matches that of the user query. However, real-world retrieval systems are often required to implicitly reason whether a document is relevant. For example, when retrieving technical texts or tables, their relevance to the user query may be implied through a particular jargon or structure, rather than explicitly expressed in their content. Large language models (LLMs) hold great potential in identifying such implied relevance by leveraging their reasoning skills. Nevertheless, current LLM-augmented retrieval is hindered by high latency and computation cost, as the LLM typically computes the query-document relevance online, for every query anew. To tackle this issue we introduce EnrichIndex, a retrieval approach which instead uses the LLM offline to build semantically-enriched retrieval indices, by performing a single pass over all documents in the retrieval corpus once during ingestion time. Furthermore, the semantically-enriched indices can complement existing online retrieval approaches, boosting the performance of LLM re-rankers. We evaluated EnrichIndex on five retrieval tasks, involving passages and tables, and found that it outperforms strong online LLM-based retrieval systems, with an average improvement of 11.7 points in recall @ 10 and 10.6 points in NDCG @ 10 compared to strong baselines. In terms of online calls to the LLM, it processes 293.3 times fewer tokens which greatly reduces the online latency and cost. Overall, EnrichIndex is an effective way to build better retrieval indices offline by leveraging the strong reasoning skills of LLMs. </p>
<blockquote>
<p>ç°æœ‰çš„ä¿¡æ¯æ£€ç´¢ç³»ç»Ÿåœ¨å¤„ç†ç›®æ ‡æ–‡æ¡£ä¸ç”¨æˆ·æŸ¥è¯¢è¯­è¨€ç›¸åŒ¹é…çš„æƒ…å†µä¸‹è¡¨ç°ä¼˜å¼‚ã€‚ç„¶è€Œï¼Œåœ¨å®é™…çš„ä¿¡æ¯æ£€ç´¢ç³»ç»Ÿä¸­ï¼Œé€šå¸¸éœ€è¦éšå¼åˆ¤æ–­æ–‡æ¡£çš„å…³è”æ€§ã€‚ä¾‹å¦‚ï¼Œåœ¨æ£€ç´¢æŠ€æœ¯æ–‡æœ¬æˆ–è¡¨æ ¼æ—¶ï¼Œå…¶ä¸ç”¨æˆ·æŸ¥è¯¢çš„ç›¸å…³æ€§å¯èƒ½é€šè¿‡ç‰¹å®šçš„æœ¯è¯­æˆ–ç»“æ„éšå«è¡¨è¾¾ï¼Œè€Œéåœ¨å†…å®¹ä¸­æ˜ç¡®è¡¨è¿°ã€‚å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨è¯†åˆ«è¿™ç§éšå«ç›¸å…³æ€§æ–¹é¢æ½œåŠ›å·¨å¤§ï¼Œé€šè¿‡åˆ©ç”¨å…¶æ¨ç†èƒ½åŠ›ã€‚ç„¶è€Œï¼Œå½“å‰ä½¿ç”¨LLMå¢å¼ºçš„æ£€ç´¢å—åˆ°é«˜å»¶è¿Ÿå’Œè®¡ç®—æˆæœ¬çš„é˜»ç¢ï¼Œå› ä¸ºLLMé€šå¸¸åœ¨çº¿è®¡ç®—æŸ¥è¯¢ä¸æ–‡æ¡£çš„ç›¸å…³æ€§ï¼Œå¹¶é’ˆå¯¹æ¯ä¸ªæŸ¥è¯¢é‡æ–°è¿›è¡Œè®¡ç®—ã€‚ä¸ºè§£å†³æ­¤é—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†EnrichIndexæ£€ç´¢æ–¹æ³•ï¼Œè¯¥æ–¹æ³•ä½¿ç”¨LLMç¦»çº¿æ„å»ºè¯­ä¹‰ä¸°å¯Œçš„æ£€ç´¢ç´¢å¼•ï¼Œé€šè¿‡å¯¹æ£€ç´¢è¯­æ–™åº“ä¸­çš„æ‰€æœ‰æ–‡æ¡£è¿›è¡Œä¸€æ¬¡æ€§éå†æ¥å®Œæˆã€‚æ­¤å¤–ï¼Œè¯­ä¹‰ä¸°å¯Œçš„ç´¢å¼•å¯ä»¥å¼¥è¡¥ç°æœ‰çš„åœ¨çº¿æ£€ç´¢æ–¹æ³•çš„ä¸è¶³ï¼Œæå‡LLMé‡æ–°æ’åºå™¨çš„æ€§èƒ½ã€‚æˆ‘ä»¬åœ¨äº”ä¸ªæ£€ç´¢ä»»åŠ¡ä¸Šè¯„ä¼°äº†EnrichIndexï¼Œæ¶‰åŠæ®µè½å’Œè¡¨æ ¼ï¼Œå‘ç°å®ƒä¼˜äºå¼ºå¤§çš„åœ¨çº¿LLMæ£€ç´¢ç³»ç»Ÿï¼Œåœ¨å¬å›ç‡@10å’ŒNDCG@10æ–¹é¢å¹³å‡æé«˜äº†11.7ç‚¹å’Œ10.6ç‚¹ï¼Œç›¸è¾ƒäºå¼ºåŸºçº¿æœ‰æ‰€æ”¹è¿›ã€‚å°±LLMçš„åœ¨çº¿è°ƒç”¨è€Œè¨€ï¼Œå®ƒå¤„ç†çš„ä»¤ç‰Œå‡å°‘äº†293.3å€ï¼Œæå¤§åœ°é™ä½äº†åœ¨çº¿å»¶è¿Ÿå’Œæˆæœ¬ã€‚æ€»ä½“è€Œè¨€ï¼ŒEnrichIndexæ˜¯ä¸€ç§åˆ©ç”¨LLMçš„å¼ºå¤§æ¨ç†èƒ½åŠ›ç¦»çº¿æ„å»ºæ›´å¥½çš„æ£€ç´¢ç´¢å¼•çš„æœ‰æ•ˆæ–¹æ³•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.03598v1">PDF</a> Dataset and code are available at   <a target="_blank" rel="noopener" href="https://peterbaile.github.io/enrichindex/">https://peterbaile.github.io/enrichindex/</a></p>
<p><strong>æ‘˜è¦</strong></p>
<p>ç°æœ‰ä¿¡æ¯æ£€ç´¢ç³»ç»Ÿæ“…é•¿å¤„ç†ç›®æ ‡æ–‡æ¡£ä¸ç”¨æˆ·æŸ¥è¯¢è¯­è¨€ç›¸åŒ¹é…çš„æƒ…å†µã€‚ç„¶è€Œï¼Œåœ¨å®é™…çš„ä¸–ç•Œä¸­ï¼Œæ£€ç´¢ç³»ç»Ÿéœ€è¦éšå¼åˆ¤æ–­æ–‡æ¡£çš„å…³è”æ€§ã€‚ä¾‹å¦‚ï¼Œåœ¨æ£€ç´¢æŠ€æœ¯æ–‡æœ¬æˆ–è¡¨æ ¼æ—¶ï¼Œå…¶ä¸ç”¨æˆ·æŸ¥è¯¢çš„å…³è”æ€§å¯èƒ½é€šè¿‡ç‰¹å®šçš„æœ¯è¯­æˆ–ç»“æ„éšå¼è¡¨è¾¾ï¼Œè€Œéåœ¨å†…å®¹ä¸­æ˜ç¡®è¡¨è¾¾ã€‚å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨è¯†åˆ«è¿™ç§éšå¼å…³è”æ€§æ–¹é¢å…·æœ‰å·¨å¤§æ½œåŠ›ï¼Œå¯é€šè¿‡æ¨ç†èƒ½åŠ›æ¥å®ç°ã€‚ç„¶è€Œï¼Œå½“å‰LLMå¢å¼ºçš„æ£€ç´¢å—åˆ°é«˜å»¶è¿Ÿå’Œè®¡ç®—æˆæœ¬çš„é˜»ç¢ï¼Œå› ä¸ºLLMé€šå¸¸åœ¨çº¿è®¡ç®—æŸ¥è¯¢æ–‡æ¡£çš„ç›¸å…³æ€§ï¼Œé’ˆå¯¹æ¯ä¸ªæŸ¥è¯¢éƒ½æ˜¯å…¨æ–°çš„è®¡ç®—ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†EnrichIndexï¼Œè¿™æ˜¯ä¸€ç§æ£€ç´¢æ–¹æ³•ï¼Œå®ƒåˆ©ç”¨LLMç¦»çº¿æ„å»ºè¯­ä¹‰ä¸°å¯Œçš„æ£€ç´¢ç´¢å¼•ï¼Œé€šè¿‡å¯¹æ£€ç´¢è¯­æ–™åº“ä¸­æ‰€æœ‰æ–‡æ¡£è¿›è¡Œä¸€æ¬¡å•ä¸€éå†æ¥å®ç°ã€‚æ­¤å¤–ï¼Œè¯­ä¹‰ä¸°å¯Œçš„ç´¢å¼•å¯ä»¥è¡¥å……ç°æœ‰çš„åœ¨çº¿æ£€ç´¢æ–¹æ³•ï¼Œæå‡LLMé‡æ–°æ’åºå™¨çš„æ€§èƒ½ã€‚æˆ‘ä»¬åœ¨äº”ä¸ªæ£€ç´¢ä»»åŠ¡ä¸Šè¯„ä¼°äº†EnrichIndexï¼Œæ¶‰åŠæ®µè½å’Œè¡¨æ ¼ï¼Œå‘ç°å®ƒä¼˜äºå¼ºå¤§çš„åœ¨çº¿LLMæ£€ç´¢ç³»ç»Ÿï¼Œåœ¨å¬å›ç‡@10å’ŒNDCG@10æ–¹é¢å¹³å‡æé«˜äº†11.7ç‚¹å’Œ10.6ç‚¹ï¼Œç›¸è¾ƒäºå¼ºåŸºçº¿ã€‚åœ¨åœ¨çº¿è°ƒç”¨LLMæ–¹é¢ï¼Œå®ƒå¤„ç†çš„ä»¤ç‰Œæ•°é‡å‡å°‘äº†293.3å€ï¼Œå¤§å¤§é™ä½äº†åœ¨çº¿å»¶è¿Ÿå’Œæˆæœ¬ã€‚æ€»ä½“è€Œè¨€ï¼ŒEnrichIndexæ˜¯ä¸€ç§æœ‰æ•ˆåˆ©ç”¨LLMå¼ºå¤§æ¨ç†èƒ½åŠ›æ„å»ºæ›´å¥½æ£€ç´¢ç´¢å¼•çš„ç¦»çº¿æ–¹æ³•ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>LLMå…·æœ‰è¯†åˆ«æ–‡æ¡£ä¸ç”¨æˆ·æŸ¥è¯¢ä¹‹é—´éšå¼å…³è”æ€§çš„æ½œåŠ›ã€‚</li>
<li>å½“å‰LLMå¢å¼ºçš„æ£€ç´¢é¢ä¸´é«˜å»¶è¿Ÿå’Œè®¡ç®—æˆæœ¬çš„é—®é¢˜ã€‚</li>
<li>EnrichIndexæ˜¯ä¸€ç§åˆ©ç”¨LLMç¦»çº¿æ„å»ºè¯­ä¹‰ä¸°å¯Œæ£€ç´¢ç´¢å¼•çš„æ–¹æ³•ã€‚</li>
<li>EnrichIndexé€šè¿‡ä¸€æ¬¡éå†æ‰€æœ‰æ–‡æ¡£ï¼Œåœ¨æ‘„å…¥æ—¶é—´å»ºç«‹è¯­ä¹‰ä¸°å¯Œçš„ç´¢å¼•ã€‚</li>
<li>EnrichIndexæé«˜äº†æ£€ç´¢æ€§èƒ½ï¼Œç‰¹åˆ«æ˜¯åœ¨å¤„ç†åŒ…å«æŠ€æœ¯æ–‡æœ¬å’Œè¡¨æ ¼çš„æ£€ç´¢ä»»åŠ¡æ—¶ã€‚</li>
<li>EnrichIndexç›¸è¾ƒäºå¼ºåŸºçº¿åœ¨å¬å›ç‡@10å’ŒNDCG@10æ–¹é¢è¡¨ç°å‡ºä¼˜å¼‚çš„æ€§èƒ½æå‡ã€‚</li>
<li>EnrichIndexé™ä½äº†åœ¨çº¿è°ƒç”¨LLMçš„é¢‘ç‡å’Œæˆæœ¬ï¼Œæé«˜äº†æ•ˆç‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.03598">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-12d1f1b402905e003bca263a9640cf1c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-19d147643aab28c9fdac63a0ef6cb588.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-5d9e2dbe6f68b33de234e79b99f62524.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-efd3c452ce35c3ec1dc8099353298b48.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Agentic-Knowledgeable-Self-awareness"><a href="#Agentic-Knowledgeable-Self-awareness" class="headerlink" title="Agentic Knowledgeable Self-awareness"></a>Agentic Knowledgeable Self-awareness</h2><p><strong>Authors:Shuofei Qiao, Zhisong Qiu, Baochang Ren, Xiaobin Wang, Xiangyuan Ru, Ningyu Zhang, Xiang Chen, Yong Jiang, Pengjun Xie, Fei Huang, Huajun Chen</strong></p>
<p>Large Language Models (LLMs) have achieved considerable performance across various agentic planning tasks. However, traditional agent planning approaches adopt a â€œflood irrigationâ€ methodology that indiscriminately injects gold trajectories, external feedback, and domain knowledge into agent models. This practice overlooks the fundamental human cognitive principle of situational self-awareness during decision-making-the ability to dynamically assess situational demands and strategically employ resources during decision-making. We propose agentic knowledgeable self-awareness to address this gap, a novel paradigm enabling LLM-based agents to autonomously regulate knowledge utilization. Specifically, we propose KnowSelf, a data-centric approach that applies agents with knowledgeable self-awareness like humans. Concretely, we devise a heuristic situation judgement criterion to mark special tokens on the agentâ€™s self-explored trajectories for collecting training data. Through a two-stage training process, the agent model can switch between different situations by generating specific special tokens, achieving optimal planning effects with minimal costs. Our experiments demonstrate that KnowSelf can outperform various strong baselines on different tasks and models with minimal use of external knowledge. Code is available at <a target="_blank" rel="noopener" href="https://github.com/zjunlp/KnowSelf">https://github.com/zjunlp/KnowSelf</a>. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨å„ç§è‡ªä¸»è§„åˆ’ä»»åŠ¡ä¸­å–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½ã€‚ç„¶è€Œï¼Œä¼ ç»Ÿçš„ä»£ç†è§„åˆ’æ–¹æ³•é‡‡ç”¨â€œæ´ªæ°´çŒæº‰â€çš„æ–¹æ³•ï¼Œä¸åŠ åŒºåˆ«åœ°å°†é»„é‡‘è½¨è¿¹ã€å¤–éƒ¨åé¦ˆå’Œé¢†åŸŸçŸ¥è¯†æ³¨å…¥ä»£ç†æ¨¡å‹ä¸­ã€‚è¿™ç§åšæ³•å¿½ç•¥äº†å†³ç­–è¿‡ç¨‹ä¸­æƒ…å¢ƒè‡ªæˆ‘æ„è¯†è¿™ä¸€åŸºæœ¬çš„äººç±»è®¤çŸ¥åŸåˆ™â€”â€”å³åœ¨å†³ç­–è¿‡ç¨‹ä¸­åŠ¨æ€è¯„ä¼°æƒ…å¢ƒéœ€æ±‚å¹¶æˆ˜ç•¥æ€§åœ°åˆ©ç”¨èµ„æºçš„èƒ½åŠ›ã€‚ä¸ºäº†å¼¥è¡¥è¿™ä¸€å·®è·ï¼Œæˆ‘ä»¬æå‡ºäº†ä»£ç†çŸ¥è¯†è‡ªæˆ‘æ„è¯†è¿™ä¸€æ–°èŒƒå¼ï¼Œä½¿åŸºäºLLMçš„ä»£ç†èƒ½å¤Ÿè‡ªä¸»åœ°è°ƒèŠ‚çŸ¥è¯†åˆ©ç”¨ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬æå‡ºäº†KnowSelfæ–¹æ³•ï¼Œè¿™æ˜¯ä¸€ç§ä»¥æ•°æ®ä¸ºä¸­å¿ƒçš„æ–¹æ³•ï¼Œå°†å…·æœ‰çŸ¥è¯†è‡ªæˆ‘æ„è¯†çš„ä»£ç†åº”ç”¨äºäººç±»ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸€ç§å¯å‘å¼æƒ…å¢ƒåˆ¤æ–­æ ‡å‡†ï¼Œåœ¨ä»£ç†è‡ªæˆ‘æ¢ç´¢çš„è½¨è¿¹ä¸Šæ ‡è®°ç‰¹æ®Šä»¤ç‰Œä»¥æ”¶é›†è®­ç»ƒæ•°æ®ã€‚é€šè¿‡ä¸¤é˜¶æ®µè®­ç»ƒè¿‡ç¨‹ï¼Œä»£ç†æ¨¡å‹å¯ä»¥é€šè¿‡ç”Ÿæˆç‰¹å®šçš„ç‰¹æ®Šä»¤ç‰Œåœ¨ä¸åŒæƒ…å¢ƒä¹‹é—´è¿›è¡Œåˆ‡æ¢ï¼Œä»¥æœ€ä½çš„æˆæœ¬å®ç°æœ€ä½³çš„è§„åˆ’æ•ˆæœã€‚æˆ‘ä»¬çš„å®éªŒè¡¨æ˜ï¼Œåœ¨ä¸åŒçš„ä»»åŠ¡å’Œæ¨¡å‹ä¸­ï¼ŒKnowSelfå¯ä»¥è¶…è¶Šå„ç§å¼ºå¤§çš„åŸºçº¿ï¼Œä¸”å¯¹å¤–éƒ¨çŸ¥è¯†çš„ä½¿ç”¨æœ€å°‘ã€‚ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/zjunlp/KnowSelf%E4%B8%8A%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/zjunlp/KnowSelfä¸Šæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.03553v1">PDF</a> Work in progress</p>
<p><strong>Summary</strong><br>å¤§è¯­è¨€æ¨¡å‹åœ¨ä»£ç†è®¡åˆ’ä»»åŠ¡ä¸Šå–å¾—äº†æ˜¾è‘—çš„æˆæ•ˆï¼Œä½†ä¼ ç»Ÿæ–¹æ³•å¿½ç•¥äº†äººç±»å†³ç­–æ—¶çš„æƒ…å¢ƒè‡ªæˆ‘æ„è¯†ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†åŸºäºçŸ¥è¯†è‡ªæˆ‘æ„è¯†çš„ä»£ç†æ¨¡å‹ï¼Œå¹¶ä»‹ç»äº†KnowSelfæ–¹æ³•ã€‚è¯¥æ–¹æ³•é€šè¿‡å¯å‘å¼æƒ…å¢ƒåˆ¤æ–­æ ‡å‡†ï¼Œä½¿ä»£ç†æ¨¡å‹åœ¨è‡ªæˆ‘æ¢ç´¢è½¨è¿¹ä¸Šç”Ÿæˆç‰¹å®šæ ‡è®°ï¼Œå®ç°çŸ¥è¯†åˆ©ç”¨çš„è‡ªé€‚åº”è°ƒèŠ‚ã€‚å®éªŒè¡¨æ˜ï¼ŒKnowSelfåœ¨ä¸åŒä»»åŠ¡ä¸Šçš„è¡¨ç°ä¼˜äºå¤šä¸ªå¼ºåŸºçº¿æ¨¡å‹ï¼Œä¸”å¯¹å¤–éƒ¨çŸ¥è¯†çš„éœ€æ±‚è¾ƒä½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLMåœ¨ä»£ç†è§„åˆ’ä»»åŠ¡ä¸­æœ‰å‡ºè‰²è¡¨ç°ã€‚</li>
<li>ä¼ ç»Ÿä»£ç†è§„åˆ’æ–¹æ³•å­˜åœ¨ç¼ºä¹æƒ…å¢ƒè‡ªæˆ‘æ„è¯†çš„é—®é¢˜ã€‚</li>
<li>çŸ¥è¯†è‡ªæˆ‘æ„è¯†çš„å¼•å…¥æ˜¯ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ã€‚</li>
<li>KnowSelfæ–¹æ³•é‡‡ç”¨å¯å‘å¼æƒ…å¢ƒåˆ¤æ–­æ ‡å‡†æ ‡è®°ç‰¹æ®Šä»¤ç‰Œã€‚</li>
<li>é€šè¿‡ä¸¤é˜¶æ®µè®­ç»ƒè¿‡ç¨‹ï¼Œä»£ç†æ¨¡å‹å¯åœ¨ä¸åŒæƒ…å¢ƒé—´åˆ‡æ¢ã€‚</li>
<li>KnowSelfåœ¨ä¸åŒä»»åŠ¡ä¸Šçš„è¡¨ç°ä¼˜äºå¤šä¸ªåŸºçº¿æ¨¡å‹ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.03553">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-efdadcf849b059b22c17261033bf8363.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3c7f3d535dedeefe4002c86ae6aad038.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-9d9fa3e5666d8826cd44bc5261123202.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d53e0ceed79d9be43c87c0f63fdc923f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4042d8d0dfca304eb6a60187782539e4.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="LLMSched-Uncertainty-Aware-Workload-Scheduling-for-Compound-LLM-Applications"><a href="#LLMSched-Uncertainty-Aware-Workload-Scheduling-for-Compound-LLM-Applications" class="headerlink" title="LLMSched: Uncertainty-Aware Workload Scheduling for Compound LLM   Applications"></a>LLMSched: Uncertainty-Aware Workload Scheduling for Compound LLM   Applications</h2><p><strong>Authors:Botao Zhu, Chen Chen, Xiaoyi Fan, Yifei Zhu</strong></p>
<p>Developing compound Large Language Model (LLM) applications is becoming an increasingly prevalent approach to solving real-world problems. In these applications, an LLM collaborates with various external modules, including APIs and even other LLMs, to realize complex intelligent services. However, we reveal that the intrinsic duration and structural uncertainty in compound LLM applications pose great challenges for LLM service providers in serving and scheduling them efficiently. In this paper, we propose LLMSched, an uncertainty-aware scheduling framework for emerging compound LLM applications. In LLMSched, we first design a novel DAG-based model to describe the uncertain compound LLM applications. Then, we adopt the Bayesian network to comprehensively profile compound LLM applications and identify uncertainty-reducing stages, along with an entropy-based mechanism to quantify their uncertainty reduction. Combining an uncertainty reduction strategy and a job completion time (JCT)-efficient scheme, we further propose an efficient scheduler to reduce the average JCT. Evaluation of both simulation and testbed experiments on various representative compound LLM applications shows that compared to existing state-of-the-art scheduling schemes, LLMSched can reduce the average JCT by 14~79%. </p>
<blockquote>
<p>å¼€å‘å¤åˆå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åº”ç”¨å·²æˆä¸ºè§£å†³ç°å®ä¸–ç•Œé—®é¢˜çš„ä¸€ç§è¶Šæ¥è¶Šæ™®éçš„æ–¹æ³•ã€‚åœ¨è¿™äº›åº”ç”¨ä¸­ï¼ŒLLMä¸å„ç§å¤–éƒ¨æ¨¡å—ï¼ˆåŒ…æ‹¬APIç”šè‡³å…¶ä»–LLMï¼‰åä½œï¼Œä»¥å®ç°å¤æ‚çš„æ™ºèƒ½æœåŠ¡ã€‚ç„¶è€Œï¼Œæˆ‘ä»¬æ­ç¤ºï¼Œå¤åˆLLMåº”ç”¨ä¸­çš„å†…åœ¨æŒç»­æ—¶é—´å’Œç»“æ„ä¸ç¡®å®šæ€§ç»™LLMæœåŠ¡æä¾›å•†åœ¨æœåŠ¡å’Œè°ƒåº¦æ–¹é¢å¸¦æ¥äº†å·¨å¤§æŒ‘æˆ˜ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†LLMSchedï¼Œä¸€ä¸ªç”¨äºæ–°å…´å¤åˆLLMåº”ç”¨çš„ä¸ç¡®å®šæ€§æ„ŸçŸ¥è°ƒåº¦æ¡†æ¶ã€‚åœ¨LLMSchedä¸­ï¼Œæˆ‘ä»¬é¦–å…ˆè®¾è®¡äº†ä¸€ä¸ªåŸºäºDAGçš„æ–°å‹æ¨¡å‹æ¥æè¿°ä¸ç¡®å®šçš„å¤åˆLLMåº”ç”¨ã€‚ç„¶åï¼Œæˆ‘ä»¬é‡‡ç”¨è´å¶æ–¯ç½‘ç»œå¯¹å¤åˆLLMåº”ç”¨è¿›è¡Œå…¨é¢åˆ†æï¼Œå¹¶è¯†åˆ«å‡ºå‡å°‘ä¸ç¡®å®šæ€§çš„é˜¶æ®µï¼Œä»¥åŠä¸€ç§åŸºäºç†µçš„æœºåˆ¶æ¥é‡åŒ–å…¶ä¸ç¡®å®šæ€§å‡å°‘ç¨‹åº¦ã€‚ç»“åˆä¸€ç§ä¸ç¡®å®šæ€§å‡å°‘ç­–ç•¥å’Œä¸€ç§ä½œä¸šå®Œæˆæ—¶é—´ï¼ˆJCTï¼‰é«˜æ•ˆæ–¹æ¡ˆï¼Œæˆ‘ä»¬è¿›ä¸€æ­¥æå‡ºäº†ä¸€ç§æœ‰æ•ˆçš„è°ƒåº¦å™¨æ¥å‡å°‘å¹³å‡JCTã€‚å¯¹å„ç§ä»£è¡¨æ€§å¤åˆLLMåº”ç”¨çš„ä»¿çœŸå’Œæµ‹è¯•åºŠå®éªŒè¯„ä¼°è¡¨æ˜ï¼Œä¸ç°æœ‰çš„æœ€å…ˆè¿›çš„è°ƒåº¦æ–¹æ¡ˆç›¸æ¯”ï¼ŒLLMSchedå¯ä»¥å°†å¹³å‡JCTå‡å°‘14~79%ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.03444v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>åŸºäºå¤åˆå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„ååŒåº”ç”¨ç¨‹åºåœ¨è§£å†³ç°å®ä¸–ç•Œä¸­å¤æ‚é—®é¢˜æ—¶è¶Šæ¥è¶Šå—åˆ°å…³æ³¨ã€‚ä½†æœåŠ¡æä¾›è€…é¢ä¸´ç€è¿è¡Œä¸è°ƒåº¦ä¸ç¡®å®šæ€§ä¸é«˜éš¾åº¦çš„æŒ‘æˆ˜ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†LLMSchedä¸ç¡®å®šæ€§æ„ŸçŸ¥è°ƒåº¦æ¡†æ¶ã€‚LLMSchedä½¿ç”¨åŸºäºDAGçš„æ–°æ¨¡å‹æè¿°ä¸ç¡®å®šçš„å¤åˆLLMåº”ç”¨ï¼Œé€šè¿‡è´å¶æ–¯ç½‘ç»œç»¼åˆè¯„ä¼°åº”ç”¨ï¼Œå¹¶æå‡ºç†µå€¼è¡¡é‡ä¸ç¡®å®šæ€§é™ä½ç¨‹åº¦ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œä¸ç°æœ‰å…ˆè¿›çš„è°ƒåº¦æ–¹æ¡ˆç›¸æ¯”ï¼ŒLLMSchedå¯é™ä½å¹³å‡ä»»åŠ¡å®Œæˆæ—¶é—´è¾¾çº¦æé«˜å·¥ä½œæˆæ•ˆåˆ°ä¸€å®šçš„èŒƒå›´å†…ã€‚ï¼ˆå¯å‡å°‘çš„å¹³å‡ä½œä¸šå®Œæˆæ—¶é—´å…·ä½“å–å†³äºåº”ç”¨æƒ…å†µã€‚ï¼‰ </p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLMååŒåº”ç”¨ç¨‹åºæ˜¯è§£å†³ç°å®é—®é¢˜çš„æµè¡Œæ–¹æ³•ã€‚</li>
<li>æœåŠ¡æä¾›è€…é¢ä¸´å¤åˆLLMåº”ç”¨çš„è°ƒåº¦ä¸ç¡®å®šæ€§æŒ‘æˆ˜ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.03444">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-bec739e46c8b2b798a504a6b62806627.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f6bc407ee350d35ea10c4565cb65baa7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-18afdc206aa7861324a25718ab14a131.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-f6a2a31472072757967c5cda008c0199.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-00e2c8b4a4720463e763099e88c51d31.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-62e10390e47e9e53b2115f7280ea5b88.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Know-What-You-do-Not-Know-Verbalized-Uncertainty-Estimation-Robustness-on-Corrupted-Images-in-Vision-Language-Models"><a href="#Know-What-You-do-Not-Know-Verbalized-Uncertainty-Estimation-Robustness-on-Corrupted-Images-in-Vision-Language-Models" class="headerlink" title="Know What You do Not Know: Verbalized Uncertainty Estimation Robustness   on Corrupted Images in Vision-Language Models"></a>Know What You do Not Know: Verbalized Uncertainty Estimation Robustness   on Corrupted Images in Vision-Language Models</h2><p><strong>Authors:Mirko Borszukovszki, Ivo Pascal de Jong, Matias Valdenegro-Toro</strong></p>
<p>To leverage the full potential of Large Language Models (LLMs) it is crucial to have some information on their answersâ€™ uncertainty. This means that the model has to be able to quantify how certain it is in the correctness of a given response. Bad uncertainty estimates can lead to overconfident wrong answers undermining trust in these models. Quite a lot of research has been done on language models that work with text inputs and provide text outputs. Still, since the visual capabilities have been added to these models recently, there has not been much progress on the uncertainty of Visual Language Models (VLMs). We tested three state-of-the-art VLMs on corrupted image data. We found that the severity of the corruption negatively impacted the modelsâ€™ ability to estimate their uncertainty and the models also showed overconfidence in most of the experiments. </p>
<blockquote>
<p>ä¸ºäº†å……åˆ†åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ½œåŠ›ï¼Œäº†è§£å®ƒä»¬ç­”æ¡ˆçš„ä¸ç¡®å®šæ€§è‡³å…³é‡è¦ã€‚è¿™æ„å‘³ç€æ¨¡å‹å¿…é¡»èƒ½å¤Ÿé‡åŒ–å®ƒå¯¹ç»™å®šç­”æ¡ˆæ­£ç¡®æ€§çš„ä¿¡å¿ƒç¨‹åº¦ã€‚ä¸è‰¯çš„ä¸ç¡®å®šæ€§ä¼°è®¡å¯èƒ½å¯¼è‡´è¿‡äºè‡ªä¿¡çš„é”™ç­”ï¼Œä»è€Œå‰Šå¼±å¯¹è¿™äº›æ¨¡å‹çš„ä¿¡ä»»ã€‚å…³äºå¤„ç†æ–‡æœ¬è¾“å…¥å¹¶æä¾›æ–‡æœ¬è¾“å‡ºçš„è¯­è¨€æ¨¡å‹å·²ç»è¿›è¡Œäº†å¤§é‡çš„ç ”ç©¶ã€‚ç„¶è€Œï¼Œç”±äºè¿™äº›æ¨¡å‹çš„è§†è§‰åŠŸèƒ½æœ€è¿‘æ‰è¢«æ·»åŠ è¿›æ¥ï¼Œå…³äºè§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰çš„ä¸ç¡®å®šæ€§æ–¹é¢çš„è¿›å±•å¹¶ä¸å¤§ã€‚æˆ‘ä»¬åœ¨è¢«è…èš€çš„å›¾åƒæ•°æ®ä¸Šæµ‹è¯•äº†ä¸‰ç§æœ€å…ˆè¿›çš„VLMã€‚æˆ‘ä»¬å‘ç°ï¼Œè…èš€çš„ä¸¥é‡æ€§å¯¹æ¨¡å‹ä¼°è®¡ä¸ç¡®å®šæ€§çš„èƒ½åŠ›äº§ç”Ÿäº†è´Ÿé¢å½±å“ï¼Œå¹¶ä¸”åœ¨å¤§å¤šæ•°å®éªŒä¸­ï¼Œè¿™äº›æ¨¡å‹æ˜¾ç¤ºå‡ºè¿‡äºè‡ªä¿¡çš„æƒ…å†µã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.03440v1">PDF</a> 10 pages, 11 figures, TrustNLP Workshop @ NAACL 2025 Camera ready</p>
<p><strong>Summary</strong>ï¼š<br>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ½œåŠ›å®Œå…¨å‘æŒ¥éœ€è¦å¯¹å…¶ç­”æ¡ˆçš„ä¸ç¡®å®šæ€§è¿›è¡Œé‡åŒ–è¯„ä¼°ã€‚æ¨¡å‹éœ€è¦èƒ½å¤Ÿè¡¡é‡å¯¹ç»™å®šç­”æ¡ˆæ­£ç¡®æ€§çš„ç½®ä¿¡åº¦ã€‚ä¸è‰¯çš„ä¸ç¡®å®šæ€§ä¼°è®¡å¯èƒ½å¯¼è‡´è¿‡åº¦è‡ªä¿¡çš„é”™è¯¯ç­”æ¡ˆï¼Œç ´åå¯¹è¿™äº›æ¨¡å‹çš„ä¿¡ä»»ã€‚ç›®å‰å¯¹å¤„ç†æ–‡æœ¬è¾“å…¥çš„è¯­è¨€æ¨¡å‹çš„ç ”ç©¶è¾ƒå¤šï¼Œä½†å¯¹åŒ…å«è§†è§‰åŠŸèƒ½çš„è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰çš„ä¸ç¡®å®šæ€§ç ”ç©¶å°šæœªå–å¾—å¤ªå¤šè¿›å±•ã€‚æˆ‘ä»¬å¯¹ä¸‰æ¬¾æœ€å…ˆè¿›çš„VLMè¿›è¡Œäº†å›¾åƒæ•°æ®æŸåæµ‹è¯•ï¼Œå‘ç°æ•°æ®æŸåçš„ä¸¥é‡ç¨‹åº¦å¯¹æ¨¡å‹ä¼°è®¡å…¶ä¸ç¡®å®šæ€§çš„å½±å“å‘ˆè´Ÿé¢è¶‹åŠ¿ï¼Œä¸”å¤§å¤šæ•°å®éªŒä¸­æ¨¡å‹è¡¨ç°å‡ºè¿‡åº¦è‡ªä¿¡ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è¦å®Œå…¨å‘æŒ¥æ½œåŠ›ï¼Œéœ€è¦è¯„ä¼°ç­”æ¡ˆçš„ä¸ç¡®å®šæ€§ã€‚</li>
<li>æ¨¡å‹åº”èƒ½è¡¡é‡å¯¹ç­”æ¡ˆæ­£ç¡®æ€§çš„ç½®ä¿¡åº¦ã€‚</li>
<li>ä¸è‰¯çš„ä¸ç¡®å®šæ€§ä¼°è®¡å¯èƒ½å¯¼è‡´è¿‡åº¦è‡ªä¿¡çš„é”™è¯¯ç­”æ¡ˆã€‚</li>
<li>ç›®å‰å¯¹è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰çš„ä¸ç¡®å®šæ€§ç ”ç©¶å°šæœªå–å¾—è¶³å¤Ÿè¿›å±•ã€‚</li>
<li>æµ‹è¯•äº†ä¸‰ç§æœ€å…ˆè¿›çš„VLMåœ¨æŸåå›¾åƒæ•°æ®ä¸Šçš„è¡¨ç°ã€‚</li>
<li>æ•°æ®æŸåçš„ä¸¥é‡ç¨‹åº¦å¯¹æ¨¡å‹ä¼°è®¡å…¶ä¸ç¡®å®šæ€§çš„å½±å“å‘ˆè´Ÿé¢è¶‹åŠ¿ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.03440">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-d24df4115207ad7248a3bd141d5198bb.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-157c666ac6ce118f050bc92697d2ce09.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-592298955863f68b3d3361acdc20374e.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-43311077fbd411ac2425e0da2201a86b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7efe3d5765a38dbd030338192a420705.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Locations-of-Characters-in-Narratives-Andersen-and-Persuasion-Datasets"><a href="#Locations-of-Characters-in-Narratives-Andersen-and-Persuasion-Datasets" class="headerlink" title="Locations of Characters in Narratives: Andersen and Persuasion Datasets"></a>Locations of Characters in Narratives: Andersen and Persuasion Datasets</h2><p><strong>Authors:Batuhan Ozyurt, Roya Arkhmammadova, Deniz Yuret</strong></p>
<p>The ability of machines to grasp spatial understanding within narrative contexts is an intriguing aspect of reading comprehension that continues to be studied. Motivated by the goal to test the AIâ€™s competence in understanding the relationship between characters and their respective locations in narratives, we introduce two new datasets: Andersen and Persuasion. For the Andersen dataset, we selected fifteen childrenâ€™s stories from â€œAndersenâ€™s Fairy Talesâ€ by Hans Christian Andersen and manually annotated the characters and their respective locations throughout each story. Similarly, for the Persuasion dataset, characters and their locations in the novel â€œPersuasionâ€ by Jane Austen were also manually annotated. We used these datasets to prompt Large Language Models (LLMs). The prompts are created by extracting excerpts from the stories or the novel and combining them with a question asking the location of a character mentioned in that excerpt. Out of the five LLMs we tested, the best-performing one for the Andersen dataset accurately identified the location in 61.85% of the examples, while for the Persuasion dataset, the best-performing one did so in 56.06% of the cases. </p>
<blockquote>
<p>æœºå™¨åœ¨å™äº‹è¯­å¢ƒä¸­æŠŠæ¡ç©ºé—´ç†è§£çš„èƒ½åŠ›æ˜¯é˜…è¯»ç†è§£ä¸­ä¸€ä¸ªå¼•äººå…¥èƒœçš„æ–¹é¢ï¼Œä»ç„¶æŒç»­è¢«ç ”ç©¶ã€‚ä¸ºäº†æµ‹è¯•äººå·¥æ™ºèƒ½ç†è§£å™äº‹ä¸­è§’è‰²åŠå…¶æ‰€åœ¨ä½ç½®çš„å…³ç³»çš„èƒ½åŠ›ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸¤ä¸ªæ–°æ•°æ®é›†ï¼šå®‰å¾’ç”Ÿæ•°æ®é›†å’ŒåŠè¯«æ•°æ®é›†ã€‚å¯¹äºå®‰å¾’ç”Ÿæ•°æ®é›†ï¼Œæˆ‘ä»¬ä»æ±‰æ–¯Â·å…‹é‡Œæ–¯è’‚å®‰Â·å®‰å¾’ç”Ÿçš„ã€Šå®‰å¾’ç”Ÿç«¥è¯ã€‹ä¸­é€‰å–äº†åäº”ä¸ªæ•…äº‹ï¼Œå¹¶æ‰‹åŠ¨æ ‡æ³¨äº†æ¯ä¸ªæ•…äº‹ä¸­çš„è§’è‰²åŠå…¶æ‰€åœ¨ä½ç½®ã€‚åŒæ ·ï¼Œå¯¹äºåŠè¯«æ•°æ®é›†ï¼Œæˆ‘ä»¬ä¹Ÿæ‰‹åŠ¨æ ‡æ³¨äº†ç®€Â·å¥¥æ–¯æ±€çš„å°è¯´ã€ŠåŠè¯«ã€‹ä¸­çš„è§’è‰²åŠå…¶ä½ç½®ã€‚æˆ‘ä»¬ä½¿ç”¨è¿™äº›æ•°æ®é›†æ¥æç¤ºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ã€‚æç¤ºæ˜¯é€šè¿‡æå–æ•…äº‹æˆ–å°è¯´çš„ç‰‡æ®µï¼Œå¹¶å°†å…¶ä¸ä¸€ä¸ªé—®é¢˜ç›¸ç»“åˆè€Œåˆ›å»ºçš„ï¼Œè¿™ä¸ªé—®é¢˜ä¼šè¯¢é—®è¯¥ç‰‡æ®µä¸­æåˆ°çš„è§’è‰²çš„ä½ç½®ã€‚åœ¨æˆ‘ä»¬æµ‹è¯•çš„äº”ä¸ªLLMä¸­ï¼Œå¯¹äºå®‰å¾’ç”Ÿæ•°æ®é›†ï¼Œè¡¨ç°æœ€å¥½çš„ä¸€ä¸ªå‡†ç¡®è¯†åˆ«äº†61.8dçš„ä½ç½®çš„å®ä¾‹ï¼Œè€Œå¯¹äºåŠè¯«æ•°æ®é›†ï¼Œè¡¨ç°æœ€å¥½çš„ä¸€ä¸ªå‡†ç¡®è¯†åˆ«äº†56.06%çš„å®ä¾‹ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.03434v1">PDF</a> 14 pages, 3 figures, 10 tables</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†æœºå™¨åœ¨å™äº‹è¯­å¢ƒä¸­æŠŠæ¡ç©ºé—´ç†è§£çš„èƒ½åŠ›ï¼Œå¹¶ä¸ºäº†æµ‹è¯•äººå·¥æ™ºèƒ½åœ¨ç†è§£å™äº‹ä¸­è§’è‰²ä¸åœ°ç‚¹å…³ç³»æ–¹é¢çš„èƒ½åŠ›ï¼Œæå‡ºäº†Andersenå’ŒPersuasionä¸¤ä¸ªæ–°æ•°æ®é›†ã€‚é€šè¿‡å¯¹Hans Christian Andersençš„ã€Šå®‰å¾’ç”Ÿç«¥è¯ã€‹å’ŒJane Austençš„ã€ŠåŠå¯¼ã€‹ä¸­çš„æ•…äº‹å’Œå°è¯´è¿›è¡Œæ‰‹åŠ¨æ ‡æ³¨è§’è‰²åŠå…¶æ‰€åœ¨ä½ç½®ï¼Œåˆ›å»ºæ•°æ®é›†å¹¶ç”¨äºæç¤ºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ã€‚æœ€å¥½çš„LLMåœ¨Andersenæ•°æ®é›†ä¸Šå‡†ç¡®è¯†åˆ«åœ°ç‚¹çš„ä¾‹å­å 61.85%ï¼Œåœ¨Persuasionæ•°æ®é›†ä¸Šå 56.06%ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æœºå™¨åœ¨å™äº‹è¯­å¢ƒä¸­çš„ç©ºé—´ç†è§£èƒ½åŠ›æ˜¯é˜…è¯»ç†è§£èƒ½åŠ›çš„ä¸€ä¸ªæœ‰è¶£æ–¹é¢ï¼Œä»æŒç»­è¢«ç ”ç©¶ã€‚</li>
<li>ä¸ºäº†æµ‹è¯•AIç†è§£è§’è‰²ä¸åœ°ç‚¹åœ¨å™äº‹ä¸­çš„å…³ç³»çš„èƒ½åŠ›ï¼Œæå‡ºäº†Andersenå’ŒPersuasionä¸¤ä¸ªæ–°æ•°æ®é›†ã€‚</li>
<li>Andersenæ•°æ®é›†æ˜¯ä»Hans Christian Andersençš„å„¿ç«¥æ•…äº‹ä¸­æ‰‹åŠ¨æ ‡æ³¨è§’è‰²å’Œåœ°ç‚¹ï¼Œè€ŒPersuasionæ•°æ®é›†åˆ™æ˜¯ä»Jane Austençš„å°è¯´ä¸­æ ‡æ³¨ã€‚</li>
<li>ä½¿ç”¨è¿™äº›æ•°æ®é›†æ¥æç¤ºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ã€‚</li>
<li>æœ€å¥½çš„LLMåœ¨Andersenæ•°æ®é›†ä¸­å‡†ç¡®è¯†åˆ«åœ°ç‚¹çš„æ¯”ä¾‹ä¸º61.85%ã€‚</li>
<li>åœ¨Persuasionæ•°æ®é›†ä¸­ï¼Œæœ€å¥½çš„LLMå‡†ç¡®è¯†åˆ«åœ°ç‚¹çš„æ¯”ä¾‹ä¸º56.06%ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.03434">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-fbd53f87c59cfaae14b9621d92cbc476.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-43d56ee84095533ecb1c4c01edb99666.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-fc6ac25afee9068248c05bd0146975cc.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-68cd3605f09032c02fe89bdb7c086c50.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-462f8a2c1d0f9d9294e7d6b01664f698.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-6926099eea4c04e3e9d80f275efb878c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b7cb14c7ab3238278d7f7be3b4778c09.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-055e2115a8666ba6f4c83246e29570d1.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Evaluating-Compact-LLMs-for-Zero-Shot-Iberian-Language-Tasks-on-End-User-Devices"><a href="#Evaluating-Compact-LLMs-for-Zero-Shot-Iberian-Language-Tasks-on-End-User-Devices" class="headerlink" title="Evaluating Compact LLMs for Zero-Shot Iberian Language Tasks on End-User   Devices"></a>Evaluating Compact LLMs for Zero-Shot Iberian Language Tasks on End-User   Devices</h2><p><strong>Authors:LuÃ­s Couto Seller, ÃÃ±igo Sanz Torres, AdriÃ¡n Vogel-FernÃ¡ndez, Carlos GonzÃ¡lez Carballo, Pedro Miguel SÃ¡nchez SÃ¡nchez, AdriÃ¡n Carruana MartÃ­n, Enrique de Miguel Ambite</strong></p>
<p>Large Language Models have significantly advanced natural language processing, achieving remarkable performance in tasks such as language generation, translation, and reasoning. However, their substantial computational requirements restrict deployment to high-end systems, limiting accessibility on consumer-grade devices. This challenge is especially pronounced for under-resourced languages like those spoken in the Iberian Peninsula, where relatively limited linguistic resources and benchmarks hinder effective evaluation. This work presents a comprehensive evaluation of compact state-of-the-art LLMs across several essential NLP tasks tailored for Iberian languages. The results reveal that while some models consistently excel in certain tasks, significant performance gaps remain, particularly for languages such as Basque. These findings highlight the need for further research on balancing model compactness with robust multilingual performance </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹åœ¨è‡ªç„¶è¯­è¨€å¤„ç†æ–¹é¢å–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œåœ¨è¯­è¨€ç”Ÿæˆã€ç¿»è¯‘å’Œæ¨ç†ç­‰ä»»åŠ¡ä¸­è¡¨ç°çªå‡ºã€‚ç„¶è€Œï¼Œå…¶å·¨å¤§çš„è®¡ç®—éœ€æ±‚é™åˆ¶äº†å…¶åœ¨é«˜ç«¯ç³»ç»Ÿä¸Šçš„éƒ¨ç½²ï¼Œä¹Ÿé™åˆ¶äº†å…¶åœ¨æ¶ˆè´¹è€…çº§è®¾å¤‡ä¸Šçš„å¯ç”¨æ€§ã€‚è¿™ä¸€æŒ‘æˆ˜å¯¹äºåƒä¼Šæ¯”åˆ©äºšåŠå²›æ‰€ä½¿ç”¨çš„é‚£äº›èµ„æºåŒ®ä¹çš„è¯­è¨€æ¥è¯´å°¤ä¸ºçªå‡ºï¼Œç›¸å¯¹æœ‰é™çš„è¯­è¨€èµ„æºå’ŒåŸºå‡†æµ‹è¯•é˜»ç¢äº†æœ‰æ•ˆçš„è¯„ä¼°ã€‚æœ¬ç ”ç©¶å¯¹é’ˆå¯¹ä¼Šæ¯”åˆ©äºšè¯­è¨€çš„å¤šä¸ªåŸºæœ¬NLPä»»åŠ¡ï¼Œå¯¹æœ€æ–°çš„ç´§å‡‘å‹LLMè¿›è¡Œäº†å…¨é¢è¯„ä¼°ã€‚ç»“æœè¡¨æ˜ï¼Œè™½ç„¶ä¸€äº›æ¨¡å‹åœ¨ç‰¹å®šä»»åŠ¡ä¸Šè¡¨ç°ä¸€ç›´å¾ˆå¥½ï¼Œä½†ä»ç„¶å­˜åœ¨æ˜¾è‘—çš„æ€§èƒ½å·®è·ï¼Œå°¤å…¶æ˜¯å·´æ–¯å…‹è¯­ç­‰è¯­è¨€ã€‚è¿™äº›å‘ç°å¼ºè°ƒäº†éœ€è¦åœ¨æ¨¡å‹ç´§å‡‘æ€§ä¸ç¨³å¥çš„å¤šè¯­è¨€æ€§èƒ½ä¹‹é—´å–å¾—å¹³è¡¡ï¼Œä»¥è¿›è¡Œè¿›ä¸€æ­¥çš„ç ”ç©¶ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.03312v1">PDF</a> Under Revision al SEPLN conference</p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹åœ¨è‡ªç„¶è¯­è¨€å¤„ç†æ–¹é¢å–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œå°¤å…¶åœ¨è¯­è¨€ç”Ÿæˆã€ç¿»è¯‘å’Œæ¨ç†ç­‰ä»»åŠ¡ä¸­è¡¨ç°çªå‡ºã€‚ç„¶è€Œï¼Œå…¶å·¨å¤§çš„è®¡ç®—éœ€æ±‚é™åˆ¶äº†å…¶åœ¨ä½ç«¯è®¾å¤‡ä¸Šçš„éƒ¨ç½²ã€‚å¯¹äºåƒä¼Šæ¯”åˆ©äºšåŠå²›ä¸Šä½¿ç”¨çš„é‚£äº›èµ„æºåŒ®ä¹çš„è¯­è¨€æ¥è¯´ï¼Œè¿™ä¸€æŒ‘æˆ˜æ›´ä¸ºçªå‡ºï¼Œå› ä¸ºç›¸å¯¹æœ‰é™çš„è¯­è¨€èµ„æºå’ŒåŸºå‡†æµ‹è¯•é˜»ç¢äº†æœ‰æ•ˆçš„è¯„ä¼°ã€‚æœ¬ç ”ç©¶å¯¹ç´§å‡‘çš„æœ€å…ˆè¿›çš„å¤§å‹è¯­è¨€æ¨¡å‹åœ¨é’ˆå¯¹ä¼Šæ¯”åˆ©äºšè¯­è¨€çš„å¤šä¸ªåŸºæœ¬NLPä»»åŠ¡ä¸Šè¿›è¡Œäº†å…¨é¢è¯„ä¼°ã€‚ç»“æœè¡¨æ˜ï¼Œè™½ç„¶ä¸€äº›æ¨¡å‹åœ¨ç‰¹å®šä»»åŠ¡ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œä½†ä»å­˜åœ¨æ˜¾è‘—çš„æ€§èƒ½å·®è·ï¼Œå°¤å…¶æ˜¯å·´æ–¯å…‹è¯­ã€‚è¿™å¼ºè°ƒäº†éœ€è¦åœ¨æ¨¡å‹ç´§å‡‘æ€§ä¸ç¨³å¥çš„å¤šè¯­ç§æ€§èƒ½ä¹‹é—´å–å¾—å¹³è¡¡çš„ç ”ç©¶éœ€æ±‚ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹åœ¨è‡ªç„¶è¯­è¨€å¤„ç†æ–¹é¢å–å¾—æ˜¾è‘—è¿›å±•ï¼Œå°¤å…¶åœ¨è¯­è¨€ç”Ÿæˆã€ç¿»è¯‘å’Œæ¨ç†ç­‰ä»»åŠ¡ä¸­ã€‚</li>
<li>å¤§å‹è¯­è¨€æ¨¡å‹çš„è®¡ç®—éœ€æ±‚å·¨å¤§ï¼Œé™åˆ¶äº†å…¶åœ¨ä½ç«¯è®¾å¤‡ä¸Šçš„éƒ¨ç½²ã€‚</li>
<li>å¯¹äºèµ„æºåŒ®ä¹çš„è¯­è¨€ï¼Œæœ‰æ•ˆçš„è¯„ä¼°å°¤ä¸ºé‡è¦ã€‚</li>
<li>ç´§å‡‘çš„å¤§å‹è¯­è¨€æ¨¡å‹åœ¨é’ˆå¯¹ä¼Šæ¯”åˆ©äºšè¯­è¨€çš„å¤šä¸ªNLPä»»åŠ¡ä¸Šè¿›è¡Œäº†è¯„ä¼°ã€‚</li>
<li>æŸäº›æ¨¡å‹åœ¨ç‰¹å®šä»»åŠ¡ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œä½†ä»ç„¶å­˜åœ¨æ˜¾è‘—çš„æ€§èƒ½å·®è·ã€‚</li>
<li>éœ€è¦è¿›ä¸€æ­¥ç ”ç©¶ä»¥å¹³è¡¡æ¨¡å‹ç´§å‡‘æ€§ä¸å¤šè¯­ç§æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.03312">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-c9e19101cb82336e7cac5adb33314c17.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-eac19201d989dce99445d16f0a4940ff.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8eb7b2147b287c64eb996e9aff72b8c3.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Stance-Driven-Multimodal-Controlled-Statement-Generation-New-Dataset-and-Task"><a href="#Stance-Driven-Multimodal-Controlled-Statement-Generation-New-Dataset-and-Task" class="headerlink" title="Stance-Driven Multimodal Controlled Statement Generation: New Dataset   and Task"></a>Stance-Driven Multimodal Controlled Statement Generation: New Dataset   and Task</h2><p><strong>Authors:Bingqian Wang, Quan Fang, Jiachen Sun, Xiaoxiao Ma</strong></p>
<p>Formulating statements that support diverse or controversial stances on specific topics is vital for platforms that enable user expression, reshape political discourse, and drive social critique and information dissemination. With the rise of Large Language Models (LLMs), controllable text generation towards specific stances has become a promising research area with applications in shaping public opinion and commercial marketing. However, current datasets often focus solely on pure texts, lacking multimodal content and effective context, particularly in the context of stance detection. In this paper, we formally define and study the new problem of stance-driven controllable content generation for tweets with text and images, where given a multimodal post (text and image&#x2F;video), a model generates a stance-controlled response. To this end, we create the Multimodal Stance Generation Dataset (StanceGen2024), the first resource explicitly designed for multimodal stance-controllable text generation in political discourse. It includes posts and user comments from the 2024 U.S. presidential election, featuring text, images, videos, and stance annotations to explore how multimodal political content shapes stance expression. Furthermore, we propose a Stance-Driven Multimodal Generation (SDMG) framework that integrates weighted fusion of multimodal features and stance guidance to improve semantic consistency and stance control. We release the dataset and code (<a target="_blank" rel="noopener" href="https://anonymous.4open.science/r/StanceGen-BE9D">https://anonymous.4open.science/r/StanceGen-BE9D</a>) for public use and further research. </p>
<blockquote>
<p>è¡¨è¿°æ”¯æŒç‰¹å®šè¯é¢˜çš„å¤šæ ·æˆ–äº‰è®®æ€§ç«‹åœºï¼Œå¯¹äºå…è®¸ç”¨æˆ·è¡¨è¾¾ã€é‡å¡‘æ”¿æ²»è¯è¯­å¹¶æ¨åŠ¨ç¤¾ä¼šæ‰¹è¯„å’Œä¿¡æ¯ä¼ æ’­çš„å¹³å°è‡³å…³é‡è¦ã€‚éšç€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„å…´èµ·ï¼Œé’ˆå¯¹ç‰¹å®šç«‹åœºçš„å¯æ§æ–‡æœ¬ç”Ÿæˆå·²æˆä¸ºä¸€ä¸ªå……æ»¡å¸Œæœ›çš„ç ”ç©¶é¢†åŸŸï¼Œå¹¶åº”ç”¨äºå¡‘é€ å…¬ä¼—èˆ†è®ºå’Œå•†ä¸šè¥é”€ã€‚ç„¶è€Œï¼Œå½“å‰çš„æ•°æ®é›†é€šå¸¸åªä¸“æ³¨äºçº¯æ–‡æœ¬ï¼Œç¼ºä¹å¤šæ¨¡å¼å†…å®¹å’Œæœ‰æ•ˆçš„ä¸Šä¸‹æ–‡ï¼Œç‰¹åˆ«æ˜¯åœ¨ç«‹åœºæ£€æµ‹æ–¹é¢ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æ­£å¼å®šä¹‰å¹¶ç ”ç©¶äº†æ¨ç‰¹æ–‡æœ¬å’Œå›¾åƒçš„æ–°é—®é¢˜ï¼Œå³ç«‹åœºé©±åŠ¨çš„å¯æ§å†…å®¹ç”Ÿæˆã€‚ç»™å®šå¤šæ¨¡å¼å¸–å­ï¼ˆæ–‡æœ¬å’Œå›¾åƒ&#x2F;è§†é¢‘ï¼‰ï¼Œæ¨¡å‹ç”Ÿæˆå—ç«‹åœºæ§åˆ¶çš„å“åº”ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬åˆ›å»ºäº†å¤šæ¨¡å¼ç«‹åœºç”Ÿæˆæ•°æ®é›†ï¼ˆStanceGen2024ï¼‰ï¼Œè¿™æ˜¯ç¬¬ä¸€ä¸ªä¸“ä¸ºæ”¿æ²»è¯è¯­ä¸­çš„å¤šæ¨¡å¼ç«‹åœºå¯æ§æ–‡æœ¬ç”Ÿæˆè®¾è®¡çš„èµ„æºã€‚å®ƒåŒ…å«æ¥è‡ª2024å¹´ç¾å›½æ€»ç»Ÿé€‰ä¸¾çš„å¸–å­å’Œç”¨æˆ·è¯„è®ºï¼ŒåŒ…æ‹¬æ–‡æœ¬ã€å›¾åƒã€è§†é¢‘å’Œç«‹åœºæ³¨é‡Šï¼Œä»¥æ¢ç´¢å¤šæ¨¡å¼æ”¿æ²»å†…å®¹å¦‚ä½•å½±å“ç«‹åœºè¡¨è¾¾ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æå‡ºäº†ç«‹åœºé©±åŠ¨çš„å¤šæ¨¡å¼ç”Ÿæˆï¼ˆSDMGï¼‰æ¡†æ¶ï¼Œè¯¥æ¡†æ¶èåˆäº†å¤šæ¨¡å¼ç‰¹å¾çš„åŠ æƒèåˆå’Œç«‹åœºæŒ‡å¯¼ï¼Œä»¥æé«˜è¯­ä¹‰ä¸€è‡´æ€§å’Œç«‹åœºæ§åˆ¶ã€‚æˆ‘ä»¬å…¬å¼€æ•°æ®é›†å’Œä»£ç ï¼ˆ<a target="_blank" rel="noopener" href="https://anonymous.4open.science/r/StanceGen-BE9D%EF%BC%89%EF%BC%8C%E4%BE%9B%E5%85%AC%E4%BC%97%E4%BD%BF%E7%94%A8%E5%92%8C%E8%BF%9B%E4%B8%80%E6%AD%A5%E7%A0%94%E7%A9%B6%E3%80%82">https://anonymous.4open.science/r/StanceGen-BE9Dï¼‰ï¼Œä¾›å…¬ä¼—ä½¿ç”¨å’Œè¿›ä¸€æ­¥ç ”ç©¶ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.03295v1">PDF</a> </p>
<p><strong>æ‘˜è¦</strong><br>åŸºäºç”¨æˆ·è¡¨è¾¾çš„ç«‹åœºé©±åŠ¨çš„å¯æ§æ–‡æœ¬ç”Ÿæˆå¯¹äºé‡å¡‘æ”¿æ²»è¯è¯­ã€æ¨åŠ¨ç¤¾ä¼šæ‰¹åˆ¤å’Œä¿¡æ¯ä¼ æ’­è‡³å…³é‡è¦ã€‚éšç€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„å…´èµ·ï¼Œå¯æ§æ–‡æœ¬ç”Ÿæˆåœ¨ç‰¹å®šç«‹åœºä¸Šçš„åº”ç”¨å·²æˆä¸ºå¡‘é€ å…¬ä¼—èˆ†è®ºå’Œå•†ä¸šè¥é”€çš„ç ”ç©¶çƒ­ç‚¹ã€‚ç„¶è€Œï¼Œå½“å‰æ•°æ®é›†å¾€å¾€ä»…å…³æ³¨çº¯æ–‡æœ¬ï¼Œç¼ºä¹å¤šæ¨¡æ€å†…å®¹å’Œæœ‰æ•ˆä¸Šä¸‹æ–‡ï¼Œç‰¹åˆ«æ˜¯åœ¨ç«‹åœºæ£€æµ‹æ–¹é¢ã€‚æœ¬æ–‡æ­£å¼å®šä¹‰å¹¶ç ”ç©¶é¢å‘æ¨ç‰¹æ–‡æœ¬å’Œå›¾åƒçš„å¤šæ¨¡æ€ç«‹åœºé©±åŠ¨å¯æ§å†…å®¹ç”Ÿæˆçš„æ–°é—®é¢˜ã€‚ç»™å®šå¤šæ¨¡æ€å¸–å­ï¼ˆæ–‡æœ¬å’Œå›¾åƒ&#x2F;è§†é¢‘ï¼‰ï¼Œæ¨¡å‹ç”Ÿæˆç«‹åœºæ§åˆ¶å“åº”ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬åˆ›å»ºäº†ä¸“ä¸ºæ”¿æ²»è¯è¯­ä¸­å¤šæ¨¡æ€ç«‹åœºå¯æ§æ–‡æœ¬ç”Ÿæˆè®¾è®¡çš„Multimodal Stance Generation Datasetï¼ˆStanceGen2024ï¼‰ã€‚å®ƒåŒ…å«æ¥è‡ªç¾å›½2024å¹´æ€»ç»Ÿé€‰ä¸¾çš„å¸–å­å’Œç”¨æˆ·è¯„è®ºï¼ŒåŒ…å«æ–‡æœ¬ã€å›¾åƒã€è§†é¢‘å’Œç«‹åœºæ³¨é‡Šï¼Œä»¥æ¢è®¨å¤šåª’ä½“æ”¿æ²»å†…å®¹å¦‚ä½•å½±å“ç«‹åœºè¡¨è¾¾ã€‚æ­¤å¤–ï¼Œæå‡ºäº†Stance-Driven Multimodal Generationï¼ˆSDMGï¼‰æ¡†æ¶ï¼Œé€šè¿‡å¤šæ¨¡æ€ç‰¹å¾çš„åŠ æƒèåˆå’Œç«‹åœºæŒ‡å¯¼æ¥æé«˜è¯­ä¹‰ä¸€è‡´æ€§å’Œç«‹åœºæ§åˆ¶åŠ›ã€‚æˆ‘ä»¬å…¬å¼€æ•°æ®é›†å’Œä»£ç ä»¥ä¾›å…¬ä¼—ä½¿ç”¨å’Œè¿›ä¸€æ­¥ç ”ç©¶ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨å¯æ§æ–‡æœ¬ç”Ÿæˆæ–¹é¢å±•ç°å‡ºå·¨å¤§æ½œåŠ›ï¼Œç‰¹åˆ«æ˜¯åœ¨æ”¯æŒç”¨æˆ·è¡¨è¾¾ç‰¹å®šç«‹åœºçš„åº”ç”¨ä¸­ã€‚</li>
<li>å½“å‰æ•°æ®é›†ç¼ºä¹å¤šæ¨¡æ€å†…å®¹å’Œæœ‰æ•ˆä¸Šä¸‹æ–‡ï¼Œè¿™å¯¹äºå‡†ç¡®æ•æ‰æ”¿æ²»è¯è¯­ä¸­çš„ç«‹åœºè‡³å…³é‡è¦ã€‚</li>
<li>æœ¬æ–‡ä»‹ç»äº†ä¸€ä¸ªæ–°çš„æ•°æ®é›†Multimodal Stance Generation Datasetï¼ˆStanceGen2024ï¼‰ï¼Œä¸“é—¨ç”¨äºæ”¿æ²»è¯è¯­ä¸­çš„å¤šæ¨¡æ€ç«‹åœºå¯æ§æ–‡æœ¬ç”Ÿæˆã€‚</li>
<li>StanGenæ•°æ®é›†åŒ…å«äº†å¤šç§åª’ä½“å½¢å¼ï¼ˆå¦‚æ–‡æœ¬ã€å›¾åƒå’Œè§†é¢‘ï¼‰ï¼Œå¹¶é™„æœ‰ç«‹åœºæ³¨é‡Šï¼Œæœ‰åŠ©äºç ”ç©¶å¤šåª’ä½“å†…å®¹å¦‚ä½•å½±å“ç«‹åœºè¡¨è¾¾ã€‚</li>
<li>æå‡ºäº†ä¸€ä¸ªæ–°çš„æ¡†æ¶Stance-Driven Multimodal Generationï¼ˆSDMGï¼‰ï¼Œè¯¥æ¡†æ¶é€šè¿‡æ•´åˆå¤šæ¨¡æ€ç‰¹å¾å’Œç«‹åœºæŒ‡å¯¼æ¥æé«˜æ–‡æœ¬ç”Ÿæˆçš„è¯­ä¹‰ä¸€è‡´æ€§å’Œç«‹åœºæ§åˆ¶åŠ›ã€‚</li>
<li>è¯¥æ¡†æ¶æœ‰æœ›ä¿ƒè¿›åœ¨æ”¿æ²»è¯è¯­åˆ†æã€ä¿¡æ¯è¡¨è¾¾å’Œç¤¾äº¤åª’ä½“è¥é”€ç­‰å¤šä¸ªé¢†åŸŸçš„æ·±å…¥ç ”ç©¶ä¸åº”ç”¨å¼€å‘ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.03295">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-1367128a2e82ba8b72e06214290a0a2f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-03f547d0178ae43100fe8ec496bc6793.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2a77402881e6e52577cde52ae4ab9f60.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-02a164fb9b3a2377bb679127464ae6ca.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="From-ChatGPT-to-DeepSeek-AI-A-Comprehensive-Analysis-of-Evolution-Deviation-and-Future-Implications-in-AI-Language-Models"><a href="#From-ChatGPT-to-DeepSeek-AI-A-Comprehensive-Analysis-of-Evolution-Deviation-and-Future-Implications-in-AI-Language-Models" class="headerlink" title="From ChatGPT to DeepSeek AI: A Comprehensive Analysis of Evolution,   Deviation, and Future Implications in AI-Language Models"></a>From ChatGPT to DeepSeek AI: A Comprehensive Analysis of Evolution,   Deviation, and Future Implications in AI-Language Models</h2><p><strong>Authors:Simrandeep Singh, Shreya Bansal, Abdulmotaleb El Saddik, Mukesh Saini</strong></p>
<p>The rapid advancement of artificial intelligence (AI) has reshaped the field of natural language processing (NLP), with models like OpenAI ChatGPT and DeepSeek AI. Although ChatGPT established a strong foundation for conversational AI, DeepSeek AI introduces significant improvements in architecture, performance, and ethical considerations. This paper presents a detailed analysis of the evolution from ChatGPT to DeepSeek AI, highlighting their technical differences, practical applications, and broader implications for AI development. To assess their capabilities, we conducted a case study using a predefined set of multiple choice questions in various domains, evaluating the strengths and limitations of each model. By examining these aspects, we provide valuable insight into the future trajectory of AI, its potential to transform industries, and key research directions for improving AI-driven language models. </p>
<blockquote>
<p>äººå·¥æ™ºèƒ½ï¼ˆAIï¼‰çš„å¿«é€Ÿå‘å±•å·²ç»é‡å¡‘äº†è‡ªç„¶è¯­è¨€å¤„ç†ï¼ˆNLPï¼‰é¢†åŸŸï¼Œå‡ºç°äº†OpenAI ChatGPTå’ŒDeepSeek AIç­‰æ¨¡å‹ã€‚è™½ç„¶ChatGPTä¸ºå¯¹è¯å¼AIå¥ å®šäº†åšå®åŸºç¡€ï¼Œä½†DeepSeek AIåœ¨æ¶æ„ã€æ€§èƒ½å’Œé“å¾·è€ƒé‡æ–¹é¢å¼•å…¥äº†é‡å¤§æ”¹è¿›ã€‚æœ¬æ–‡è¯¦ç»†åˆ†æäº†ä»ChatGPTåˆ°DeepSeek AIçš„æ¼”å˜è¿‡ç¨‹ï¼Œé‡ç‚¹ä»‹ç»äº†å®ƒä»¬çš„æŠ€æœ¯å·®å¼‚ã€å®é™…åº”ç”¨ä»¥åŠå¯¹AIå‘å±•çš„æ›´å¹¿æ³›å½±å“ã€‚ä¸ºäº†è¯„ä¼°å®ƒä»¬çš„èƒ½åŠ›ï¼Œæˆ‘ä»¬é‡‡ç”¨é¢„å…ˆè®¾å®šçš„å¤šä¸ªé¢†åŸŸçš„å¤šé¡¹é€‰æ‹©é¢˜è¿›è¡Œæ¡ˆä¾‹ç ”ç©¶ï¼Œè¯„ä¼°æ¯ä¸ªæ¨¡å‹çš„ä¼˜ç‚¹å’Œå±€é™æ€§ã€‚é€šè¿‡è€ƒå¯Ÿè¿™äº›æ–¹é¢ï¼Œæˆ‘ä»¬ä¸ºAIçš„æœªæ¥å‘å±•æ–¹å‘ã€å…¶æ”¹å˜è¡Œä¸šçš„æ½œåŠ›ä»¥åŠæ”¹è¿›AIé©±åŠ¨çš„è¯­è¨€æ¨¡å‹çš„å…³é”®ç ”ç©¶æ–¹å‘æä¾›äº†æœ‰ä»·å€¼çš„è§è§£ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.03219v1">PDF</a> 10 pages, 1 figure, 4 tables</p>
<p><strong>Summary</strong></p>
<p>éšç€äººå·¥æ™ºèƒ½ï¼ˆAIï¼‰çš„å¿«é€Ÿå‘å±•ï¼Œè‡ªç„¶è¯­è¨€å¤„ç†ï¼ˆNLPï¼‰é¢†åŸŸå·²ç»å‘ç”Ÿäº†å·¨å¤§å˜åŒ–ï¼Œå…¶ä¸­OpenAIçš„ChatGPTå’ŒDeepSeek AIç­‰æ¨¡å‹å¼•é¢†äº†å˜é©ã€‚è™½ç„¶ChatGPTä¸ºå¯¹è¯AIå¥ å®šäº†åšå®åŸºç¡€ï¼Œä½†DeepSeek AIåœ¨æ¶æ„ã€æ€§èƒ½å’Œä¼¦ç†è€ƒé‡æ–¹é¢å®ç°äº†æ˜¾è‘—æ”¹è¿›ã€‚æœ¬æ–‡è¯¦ç»†åˆ†æäº†ä»ChatGPTåˆ°DeepSeek AIçš„æ¼”è¿›è¿‡ç¨‹ï¼Œé‡ç‚¹ä»‹ç»äº†å®ƒä»¬çš„æŠ€æœ¯å·®å¼‚ã€å®é™…åº”ç”¨ä»¥åŠå¯¹AIå‘å±•çš„æ›´å¹¿æ³›å½±å“ã€‚é€šè¿‡å¤šé¡¹é€‰æ‹©é¢˜çš„æ¡ˆä¾‹ç ”ç©¶ï¼Œæˆ‘ä»¬è¯„ä¼°äº†è¿™ä¸¤ä¸ªæ¨¡å‹çš„èƒ½åŠ›ï¼Œæ¢è®¨äº†å®ƒä»¬çš„ä¼˜ç‚¹å’Œå±€é™æ€§ã€‚è¿™ä¸ºæ·±å…¥äº†è§£AIçš„æœªæ¥å‘å±•è¶‹åŠ¿ã€å…¶å¯¹è¡Œä¸šçš„æ½œåœ¨å˜é©ä»¥åŠæ”¹è¿›AIé©±åŠ¨çš„è¯­è¨€æ¨¡å‹çš„å…³é”®ç ”ç©¶æ–¹å‘æä¾›äº†æœ‰ä»·å€¼çš„ä¿¡æ¯ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>AIçš„å‘å±•å·²ç»æ·±åˆ»å½±å“äº†NLPé¢†åŸŸï¼Œä»¥ChatGPTå’ŒDeepSeek AIä¸ºä»£è¡¨çš„æ–°æ¨¡å‹æ¨åŠ¨äº†å˜é©ã€‚</li>
<li>ChatGPTä¸ºå¯¹è¯AIå¥ å®šäº†åŸºçŸ³ï¼Œè€ŒDeepSeek AIåœ¨æ¶æ„ã€æ€§èƒ½å’Œä¼¦ç†æ–¹é¢è¿›è¡Œäº†æ˜¾è‘—æ”¹è¿›ã€‚</li>
<li>æ–‡ç« é€šè¿‡æ¡ˆä¾‹åˆ†æè¯¦ç»†æ¯”è¾ƒäº†ChatGPTå’ŒDeepSeek AIçš„æŠ€æœ¯å·®å¼‚ã€å®é™…åº”ç”¨åŠå½±å“ã€‚</li>
<li>æ¡ˆä¾‹ç ”ç©¶åŒ…æ‹¬å¤šä¸ªé¢†åŸŸçš„å¤šé¡¹é€‰æ‹©é¢˜æµ‹è¯•ï¼Œä»¥è¯„ä¼°ä¸¤ä¸ªæ¨¡å‹çš„å®åŠ›å’Œå±€é™ã€‚</li>
<li>DeepSeek AIçš„æ”¹è¿›ä¸ä»…ä½“ç°åœ¨æŠ€æœ¯èƒ½åŠ›ä¸Šï¼Œè¿˜åŒ…æ‹¬å¯¹ä¼¦ç†é—®é¢˜çš„é‡è§†ã€‚</li>
<li>æ–‡ç« æä¾›äº†å…³äºAIæœªæ¥å‘å±•è¶‹åŠ¿ã€è¡Œä¸šå˜é©ä»¥åŠç ”ç©¶æ–¹å‘çš„æ·±åˆ»è§è§£ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.03219">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-dabf5572feec648db1695c3533726b1e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-6ba79011e47333e2288c9194feed5e53.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-0142568f8f7a3988d6f072a81b3240f3.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="VARGPT-v1-1-Improve-Visual-Autoregressive-Large-Unified-Model-via-Iterative-Instruction-Tuning-and-Reinforcement-Learning"><a href="#VARGPT-v1-1-Improve-Visual-Autoregressive-Large-Unified-Model-via-Iterative-Instruction-Tuning-and-Reinforcement-Learning" class="headerlink" title="VARGPT-v1.1: Improve Visual Autoregressive Large Unified Model via   Iterative Instruction Tuning and Reinforcement Learning"></a>VARGPT-v1.1: Improve Visual Autoregressive Large Unified Model via   Iterative Instruction Tuning and Reinforcement Learning</h2><p><strong>Authors:Xianwei Zhuang, Yuxin Xie, Yufan Deng, Dongchao Yang, Liming Liang, Jinghan Ru, Yuguo Yin, Yuexian Zou</strong></p>
<p>In this work, we present VARGPT-v1.1, an advanced unified visual autoregressive model that builds upon our previous framework VARGPT. The model preserves the dual paradigm of next-token prediction for visual understanding and next-scale generation for image synthesis. Specifically, VARGPT-v1.1 integrates: (1) a novel training strategy combining iterative visual instruction tuning with reinforcement learning through Direct Preference Optimization (DPO), (2) an expanded training corpus containing 8.3M visual-generative instruction pairs, (3) an upgraded language model backbone using Qwen2, (4) enhanced image generation resolution, and (5) emergent image editing capabilities without architectural modifications. These advancements enable VARGPT-v1.1 to achieve state-of-the-art performance in multimodal understanding and text-to-image instruction-following tasks, demonstrating significant improvements in both comprehension and generation metrics. Notably, through visual instruction tuning, the model acquires image editing functionality while maintaining architectural consistency with its predecessor, revealing the potential for unified visual understanding, generation, and editing. Our findings suggest that well-designed unified visual autoregressive models can effectively adopt flexible training strategies from large language models (LLMs), exhibiting promising scalability. The codebase and model weights are publicly available at <a target="_blank" rel="noopener" href="https://github.com/VARGPT-family/VARGPT-v1.1">https://github.com/VARGPT-family/VARGPT-v1.1</a>. </p>
<blockquote>
<p>åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æ¨å‡ºäº†VARGPT-v1.1ï¼Œè¿™æ˜¯ä¸€æ¬¾å…ˆè¿›çš„ç»Ÿä¸€è§†è§‰è‡ªå›å½’æ¨¡å‹ï¼ŒåŸºäºæˆ‘ä»¬ä¹‹å‰çš„æ¡†æ¶VARGPTæ„å»ºã€‚è¯¥æ¨¡å‹ä¿ç•™äº†è§†è§‰ç†è§£å’Œå›¾åƒåˆæˆä¸‹ä¸€æ¬¡ä»¤ç‰Œé¢„æµ‹å’Œä¸‹ä¸€ä»£å°ºåº¦çš„åŒé‡èŒƒå¼ã€‚å…·ä½“æ¥è¯´ï¼ŒVARGPT-v1.1é›†æˆäº†ä»¥ä¸‹åŠŸèƒ½ï¼šï¼ˆ1ï¼‰ä¸€ç§æ–°å‹è®­ç»ƒç­–ç•¥ï¼Œç»“åˆè¿­ä»£è§†è§‰æŒ‡ä»¤å¾®è°ƒä¸é€šè¿‡ç›´æ¥åå¥½ä¼˜åŒ–ï¼ˆDPOï¼‰çš„å¼ºåŒ–å­¦ä¹ ï¼Œï¼ˆ2ï¼‰åŒ…å«830ä¸‡è§†è§‰ç”ŸæˆæŒ‡ä»¤å¯¹çš„æ‰©å±•è®­ç»ƒè¯­æ–™åº“ï¼Œï¼ˆ3ï¼‰ä½¿ç”¨Qwen2çš„å‡çº§è¯­è¨€æ¨¡å‹ä¸»å¹²ï¼Œï¼ˆ4ï¼‰å¢å¼ºçš„å›¾åƒç”Ÿæˆåˆ†è¾¨ç‡ï¼Œï¼ˆ5ï¼‰æ— éœ€æ¶æ„ä¿®æ”¹å³å¯å®ç°å›¾åƒç¼–è¾‘åŠŸèƒ½ã€‚è¿™äº›è¿›æ­¥ä½¿VARGPT-v1.1åœ¨è·¨æ¨¡æ€ç†è§£å’Œæ–‡æœ¬åˆ°å›¾åƒçš„æŒ‡ä»¤è·Ÿè¸ªä»»åŠ¡ä¸Šè¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œç†è§£å’Œç”ŸæˆæŒ‡æ ‡å‡æ˜¾è‘—æ”¹è¿›ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œé€šè¿‡è§†è§‰æŒ‡ä»¤å¾®è°ƒï¼Œè¯¥æ¨¡å‹åœ¨ä¿æŒä¸å‰ä»£æ¨¡å‹æ¶æ„ä¸€è‡´çš„åŒæ—¶è·å¾—äº†å›¾åƒç¼–è¾‘åŠŸèƒ½ï¼Œå±•ç¤ºäº†ç»Ÿä¸€è§†è§‰ç†è§£ã€ç”Ÿæˆå’Œç¼–è¾‘çš„æ½œåŠ›ã€‚æˆ‘ä»¬çš„ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œç²¾å¿ƒè®¾è®¡çš„ä¸€ä½“åŒ–è§†è§‰è‡ªå›å½’æ¨¡å‹å¯ä»¥æœ‰æ•ˆåœ°é‡‡ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„çµæ´»è®­ç»ƒç­–ç•¥ï¼Œå±•ç°å‡ºæœ‰å‰æ™¯çš„å¯æ‰©å±•æ€§ã€‚ä»£ç åº“å’Œæ¨¡å‹æƒé‡å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/VARGPT-family/VARGPT-v1.1%E4%B8%8A%E5%85%AC%E5%BC%80%E8%8E%B7%E5%BE%97%E3%80%82">https://github.com/VARGPT-family/VARGPT-v1.1ä¸Šå…¬å¼€è·å–ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.02949v1">PDF</a> Code is available at: <a target="_blank" rel="noopener" href="https://github.com/VARGPT-family/VARGPT-v1.1">https://github.com/VARGPT-family/VARGPT-v1.1</a>.   arXiv admin note: text overlap with arXiv:2501.12327</p>
<p><strong>Summary</strong><br>VARGPT-v1.1æ˜¯ä¸€æ¬¾å…ˆè¿›çš„ç»Ÿä¸€è§†è§‰è‡ªå›å½’æ¨¡å‹ï¼ŒåŸºäºå…ˆå‰çš„VARGPTæ¡†æ¶è¿›è¡Œå¼€å‘ã€‚å®ƒä¿ç•™äº†è§†è§‰ç†è§£å’Œå›¾åƒåˆæˆçš„ä¸‹ä¸€ä»£æ ‡è®°é¢„æµ‹å’Œä¸‹ä¸€ä»£æ¯”ä¾‹ç”Ÿæˆçš„åŒé‡èŒƒå¼ï¼Œå¹¶å¢åŠ äº†å¤šé¡¹æ–°æŠ€æœ¯å’ŒåŠŸèƒ½ã€‚é€šè¿‡ç»“åˆè¿­ä»£è§†è§‰æŒ‡ä»¤è°ƒæ•´ä¸é€šè¿‡ç›´æ¥åå¥½ä¼˜åŒ–ï¼ˆDPOï¼‰çš„å¼ºåŒ–å­¦ä¹ çš„æ–°å‹è®­ç»ƒç­–ç•¥ã€åŒ…å«830ä¸‡è§†è§‰ç”ŸæˆæŒ‡ä»¤å¯¹çš„å¤§å‹è®­ç»ƒè¯­æ–™åº“ã€å‡çº§çš„Qwen2è¯­è¨€æ¨¡å‹ä¸»å¹²ã€æé«˜çš„å›¾åƒç”Ÿæˆåˆ†è¾¨ç‡ä»¥åŠæ–°å…´çš„å›¾åƒç¼–è¾‘åŠŸèƒ½ï¼ŒVARGPT-v1.1åœ¨å¤šæ¨¡æ€ç†è§£å’Œæ–‡æœ¬åˆ°å›¾åƒæŒ‡ä»¤éµå¾ªä»»åŠ¡ä¸Šå®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œæ˜¾è‘—æé«˜äº†ç†è§£å’Œç”ŸæˆæŒ‡æ ‡ã€‚è¯¥æ¨¡å‹é€šè¿‡è§†è§‰æŒ‡ä»¤è°ƒæ•´è·å¾—å›¾åƒç¼–è¾‘åŠŸèƒ½ï¼ŒåŒæ—¶ä¿æŒäº†ä¸å‰ä½“çš„æ¶æ„ä¸€è‡´æ€§ï¼Œæ˜¾ç¤ºå‡ºç»Ÿä¸€è§†è§‰ç†è§£ã€ç”Ÿæˆå’Œç¼–è¾‘çš„æ½œåŠ›ã€‚æˆ‘ä»¬çš„ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œç²¾å¿ƒè®¾è®¡çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„ç»Ÿä¸€è§†è§‰è‡ªå›å½’æ¨¡å‹å¯ä»¥æœ‰æ•ˆåœ°é‡‡ç”¨çµæ´»çš„è®­ç»ƒç­–ç•¥ï¼Œå¹¶å±•ç°å‡ºä»¤äººé¼“èˆçš„å¯æ‰©å±•æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>VARGPT-v1.1æ˜¯ä¸€ä¸ªå…ˆè¿›çš„ç»Ÿä¸€è§†è§‰è‡ªå›å½’æ¨¡å‹ï¼ŒåŸºäºVARGPTæ¡†æ¶å¼€å‘ã€‚</li>
<li>æ¨¡å‹é‡‡ç”¨æ–°å‹è®­ç»ƒç­–ç•¥ï¼Œç»“åˆè¿­ä»£è§†è§‰æŒ‡ä»¤è°ƒæ•´ä¸å¼ºåŒ–å­¦ä¹ ã€‚</li>
<li>VARGPT-v1.1æ‹¥æœ‰æ‰©å¤§çš„è®­ç»ƒè¯­æ–™åº“ï¼ŒåŒ…å«830ä¸‡è§†è§‰ç”ŸæˆæŒ‡ä»¤å¯¹ã€‚</li>
<li>å‡çº§ä¸ºQwen2è¯­è¨€æ¨¡å‹ä¸»å¹²ï¼Œå¢å¼ºå›¾åƒç”Ÿæˆåˆ†è¾¨ç‡å’Œå›¾åƒç¼–è¾‘åŠŸèƒ½ã€‚</li>
<li>æ¨¡å‹åœ¨å¤šæ¨¡æ€ç†è§£å’Œæ–‡æœ¬åˆ°å›¾åƒæŒ‡ä»¤éµå¾ªä»»åŠ¡ä¸Šè¡¨ç°å…ˆè¿›ï¼Œç†è§£å’Œç”ŸæˆæŒ‡æ ‡æ˜¾è‘—æé«˜ã€‚</li>
<li>é€šè¿‡è§†è§‰æŒ‡ä»¤è°ƒæ•´ï¼Œæ¨¡å‹å…·å¤‡å›¾åƒç¼–è¾‘åŠŸèƒ½ï¼Œå¹¶ä¿æŒä¸å‰ä½“çš„æ¶æ„ä¸€è‡´æ€§ã€‚</li>
<li>ç ”ç©¶è¡¨æ˜ï¼Œç»Ÿä¸€è§†è§‰è‡ªå›å½’æ¨¡å‹å¯çµæ´»é‡‡ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„è®­ç»ƒç­–ç•¥ï¼Œå…·æœ‰å¯æ‰©å±•æ€§ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.02949">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-cd7c35d0e4cc95d17091a86603c17590.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a7053ab45848716e25d44944f6de50ed.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1e7bf662f14a655fc0866926a7abe4b7.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a35dd5dd32cc2b2e96e2f6b2aeb4741f.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-59dcc182bab0974665bc7129393aabb8.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-73bcf67590fadabe21c9d1f391182a1d.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="Better-Bill-GPT-Comparing-Large-Language-Models-against-Legal-Invoice-Reviewers"><a href="#Better-Bill-GPT-Comparing-Large-Language-Models-against-Legal-Invoice-Reviewers" class="headerlink" title="Better Bill GPT: Comparing Large Language Models against Legal Invoice   Reviewers"></a>Better Bill GPT: Comparing Large Language Models against Legal Invoice   Reviewers</h2><p><strong>Authors:Nick Whitehouse, Nicole Lincoln, Stephanie Yiu, Lizzie Catterson, Rivindu Perera</strong></p>
<p>Legal invoice review is a costly, inconsistent, and time-consuming process, traditionally performed by Legal Operations, Lawyers or Billing Specialists who scrutinise billing compliance line by line. This study presents the first empirical comparison of Large Language Models (LLMs) against human invoice reviewers - Early-Career Lawyers, Experienced Lawyers, and Legal Operations Professionals-assessing their accuracy, speed, and cost-effectiveness. Benchmarking state-of-the-art LLMs against a ground truth set by expert legal professionals, our empirically substantiated findings reveal that LLMs decisively outperform humans across every metric. In invoice approval decisions, LLMs achieve up to 92% accuracy, surpassing the 72% ceiling set by experienced lawyers. On a granular level, LLMs dominate line-item classification, with top models reaching F-scores of 81%, compared to just 43% for the best-performing human group. Speed comparisons are even more striking - while lawyers take 194 to 316 seconds per invoice, LLMs are capable of completing reviews in as fast as 3.6 seconds. And cost? AI slashes review expenses by 99.97%, reducing invoice processing costs from an average of $4.27 per invoice for human invoice reviewers to mere cents. These results highlight the evolving role of AI in legal spend management. As law firms and corporate legal departments struggle with inefficiencies, this study signals a seismic shift: The era of LLM-powered legal spend management is not on the horizon, it has arrived. The challenge ahead is not whether AI can perform as well as human reviewers, but how legal teams will strategically incorporate it, balancing automation with human discretion. </p>
<blockquote>
<p>æ³•å¾‹å‘ç¥¨å®¡æ ¸æ˜¯ä¸€ä¸ªæˆæœ¬é«˜ã€ä¸ä¸€è‡´ä¸”è€—æ—¶çš„è¿‡ç¨‹ï¼Œä¼ ç»Ÿä¸Šç”±æ³•å¾‹è¿è¥ã€å¾‹å¸ˆæˆ–è®¡è´¹ä¸“å‘˜æ‰§è¡Œï¼Œä»–ä»¬é€è¡Œå®¡æŸ¥è®¡è´¹åˆè§„æ€§ã€‚æœ¬ç ”ç©¶é¦–æ¬¡å¯¹å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¸äººç±»å‘ç¥¨å®¡æ ¸å‘˜ï¼ˆåˆçº§å¾‹å¸ˆã€èµ„æ·±å¾‹å¸ˆå’Œæ³•å¾‹è¿è¥ä¸“ä¸šäººå£«ï¼‰è¿›è¡Œå®è¯ç ”ç©¶ï¼Œè¯„ä¼°å…¶å‡†ç¡®æ€§ã€é€Ÿåº¦å’Œæˆæœ¬æ•ˆç›Šã€‚ä»¥ä¸“å®¶æ³•å¾‹ä¸“ä¸šäººå£«è®¾å®šçš„çœŸå®æ•°æ®é›†ä¸ºåŸºå‡†ï¼Œå¯¹æ¯”æœ€å…ˆè¿›çš„LLMï¼Œæˆ‘ä»¬çš„å®è¯ç ”ç©¶å‘ç°ï¼ŒLLMåœ¨æ‰€æœ‰æŒ‡æ ‡ä¸Šå‡æ˜¾è‘—ä¼˜äºäººç±»ã€‚åœ¨å‘ç¥¨å®¡æ‰¹å†³ç­–ä¸­ï¼ŒLLMçš„å‡†ç¡®ç‡é«˜è¾¾92%ï¼Œè¶…è¶Šäº†èµ„æ·±å¾‹å¸ˆè®¾å®šçš„72%ä¸Šé™ã€‚åœ¨ç»†ç²’åº¦å±‚é¢ï¼ŒLLMåœ¨æ¡ç›®åˆ†ç±»ä¸Šå æ®ä¸»å¯¼åœ°ä½ï¼Œé¡¶çº§æ¨¡å‹çš„Fåˆ†æ•°è¾¾åˆ°81%ï¼Œè€Œè¡¨ç°æœ€ä½³çš„äººç±»å›¢é˜Ÿä»…ä¸º43%ã€‚é€Ÿåº¦å¯¹æ¯”æ›´åŠ ä»¤äººå°è±¡æ·±åˆ»â€”â€”è™½ç„¶å¾‹å¸ˆå®¡æ ¸æ¯å¼ å‘ç¥¨éœ€è¦194è‡³316ç§’ï¼Œä½†LLMå´èƒ½åœ¨çŸ­çŸ­3.6ç§’å†…å®Œæˆå®¡æ ¸ã€‚é‚£ä¹ˆæˆæœ¬å‘¢ï¼Ÿäººå·¥æ™ºèƒ½å°†å®¡æ ¸è´¹ç”¨å‡å°‘äº†99.97%ï¼Œå°†äººç±»å‘ç¥¨å®¡æ ¸å‘˜å¹³å‡æ¯å¼ å‘ç¥¨çš„å®¡æ ¸æˆæœ¬ä»4.27ç¾å…ƒé™è‡³ä»…å‡ ç¾åˆ†ã€‚è¿™äº›ç»“æœå‡¸æ˜¾äº†äººå·¥æ™ºèƒ½åœ¨æ³•å¾‹è´¹ç”¨ç®¡ç†ä¸­çš„ä¸æ–­æ¼”å˜çš„ä½œç”¨ã€‚éšç€å¾‹å¸ˆäº‹åŠ¡æ‰€å’Œä¼ä¸šæ³•åŠ¡éƒ¨é—¨é¢ä¸´æ•ˆç‡é—®é¢˜ï¼Œè¿™é¡¹ç ”ç©¶æ ‡å¿—ç€é‡å¤§å˜åŒ–ï¼šå¤§å‹è¯­è¨€æ¨¡å‹é©±åŠ¨çš„æ³•åŠ¡æ”¯å‡ºç®¡ç†æ—¶ä»£å·²ç»åˆ°æ¥ã€‚æœªæ¥çš„æŒ‘æˆ˜ä¸åœ¨äºäººå·¥æ™ºèƒ½èƒ½å¦åƒäººç±»å®¡æ ¸å‘˜ä¸€æ ·è¡¨ç°è‰¯å¥½ï¼Œè€Œåœ¨äºæ³•å¾‹å›¢é˜Ÿå¦‚ä½•å°†å…¶æˆ˜ç•¥æ€§åœ°çº³å…¥å…¶ä¸­ï¼Œå®ç°è‡ªåŠ¨åŒ–ä¸äººç±»åˆ¤æ–­ä¹‹é—´çš„å¹³è¡¡ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.02881v1">PDF</a> </p>
<p><strong>æ‘˜è¦</strong></p>
<p>ä¼ ç»Ÿæ³•å¾‹å‘ç¥¨å®¡æŸ¥æµç¨‹æˆæœ¬é«˜æ˜‚ã€ç»“æœä¸ä¸€è‡´ä¸”è€—æ—¶ï¼Œé€šå¸¸ç”±æ³•åŠ¡è¿è¥äººå‘˜ã€å¾‹å¸ˆæˆ–è®¡è´¹ä¸“å‘˜æ‰§è¡Œï¼Œé€è¡Œå®¡æŸ¥è®¡è´¹åˆè§„æ€§ã€‚æœ¬ç ”ç©¶é¦–æ¬¡å®è¯æ¯”è¾ƒå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¸äººç±»å‘ç¥¨å®¡æŸ¥å‘˜ï¼ˆåˆçº§å¾‹å¸ˆã€èµ„æ·±å¾‹å¸ˆåŠæ³•åŠ¡è¿è¥ä¸“ä¸šäººå£«ï¼‰åœ¨å‡†ç¡®æ€§ã€é€Ÿåº¦å’Œæˆæœ¬æ•ˆç›Šæ–¹é¢çš„è¡¨ç°ã€‚ä»¥ä¸“å®¶æ³•å¾‹ä¸“ä¸šäººå£«çš„åŸºå‡†çœŸå®æ•°æ®é›†ä¸ºæ ‡å‡†ï¼Œå¯¹æœ€å‰æ²¿LLMè¿›è¡ŒåŸºå‡†æµ‹è¯•ï¼Œç»“æœæ˜¾ç¤ºLLMåœ¨å„é¡¹æŒ‡æ ‡ä¸Šå‡æ˜¾è‘—è¶…è¶Šäººç±»ã€‚åœ¨å‘ç¥¨å®¡æ‰¹å†³ç­–ä¸­ï¼ŒLLMçš„å‡†ç¡®ç‡é«˜è¾¾92%ï¼Œçªç ´äº†èµ„æ·±å¾‹å¸ˆè®¾å®šçš„72%ä¸Šé™ã€‚åœ¨å…·ä½“å±‚é¢ï¼ŒLLMåœ¨é¡¹ç›®åˆ†ç±»ä¸Šè¡¨ç°å“è¶Šï¼Œé¡¶å°–æ¨¡å‹çš„Fåˆ†æ•°è¾¾81%ï¼Œè€Œè¡¨ç°æœ€ä½³çš„äººç±»å›¢é˜Ÿä»…ä¸º43%ã€‚é€Ÿåº¦æ–¹é¢çš„å¯¹æ¯”æ›´ä¸ºæ˜¾è‘—â€”â€”å¾‹å¸ˆå®¡æŸ¥æ¯å¼ å‘ç¥¨éœ€194è‡³316ç§’ï¼Œè€ŒLLMä»…éœ€3.6ç§’å³å¯å®Œæˆå®¡æŸ¥ã€‚è‡³äºæˆæœ¬ï¼Ÿäººå·¥æ™ºèƒ½å°†å®¡æŸ¥æˆæœ¬é™ä½äº†99.97%ï¼Œå°†äººç±»å‘ç¥¨å®¡æŸ¥è€…çš„å¹³å‡å‘ç¥¨å¤„ç†æˆæœ¬ä»4.27ç¾å…ƒé™è‡³å‡ ç¾åˆ†ã€‚è¯¥ç ”ç©¶çªæ˜¾äº†äººå·¥æ™ºèƒ½åœ¨æ³•å¾‹è´¹ç”¨ç®¡ç†ä¸­çš„ä¸æ–­è¿›åŒ–è§’è‰²ã€‚éšç€å¾‹å¸ˆäº‹åŠ¡æ‰€å’Œä¼ä¸šæ³•åŠ¡éƒ¨é—¨é¢ä¸´æ•ˆç‡é—®é¢˜ï¼Œè¿™é¡¹ç ”ç©¶é¢„ç¤ºç€ä¸€æ¬¡é‡å¤§å˜é©ï¼šLLMé©±åŠ¨çš„æ³•å¾‹è´¹ç”¨ç®¡ç†æ—¶ä»£å·²ç»åˆ°æ¥ã€‚æœªæ¥çš„æŒ‘æˆ˜ä¸åœ¨äºäººå·¥æ™ºèƒ½èƒ½å¦åƒäººç±»è¯„å®¡è€…é‚£æ ·è¡¨ç°ï¼Œè€Œåœ¨äºæ³•å¾‹å›¢é˜Ÿå¦‚ä½•å°†å…¶çº³å…¥æˆ˜ç•¥å¹³è¡¡è‡ªåŠ¨åŒ–ä¸äººç±»åˆ¤æ–­ä¹‹ä¸­ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>æ³•å¾‹å‘ç¥¨å®¡æŸ¥ä¼ ç»Ÿæµç¨‹æˆæœ¬é«˜ä¸”è€—æ—¶ã€‚</li>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰é¦–æ¬¡è¢«å®è¯æ¯”è¾ƒäºä¸åŒæ°´å¹³çš„æ³•å¾‹å‘ç¥¨å®¡æŸ¥å‘˜ï¼ˆäººç±»ï¼‰ã€‚</li>
<li>LLMåœ¨å‡†ç¡®æ€§æ–¹é¢æ˜¾è‘—è¶…è¶Šäººç±»å®¡æŸ¥å‘˜ï¼Œå‡†ç¡®ç‡é«˜è¾¾92%ã€‚</li>
<li>LLMåœ¨é¡¹ç›®åˆ†ç±»ä¸Šè¡¨ç°å“è¶Šï¼Œé¡¶å°–æ¨¡å‹Fåˆ†æ•°è¾¾81%ã€‚</li>
<li>LLMå®¡æŸ¥é€Ÿåº¦è¿œè¶…äººç±»ï¼Œä»…éœ€3.6ç§’å³å¯å®Œæˆå®¡æŸ¥ã€‚</li>
<li>äººå·¥æ™ºèƒ½å¤§å¹…é™ä½äº†æ³•å¾‹å‘ç¥¨å®¡æŸ¥çš„æˆæœ¬ï¼Œå°†æˆæœ¬é™ä½äº†99.97%ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.02881">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-a30face0b859041a25fbd486162e12bb.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="RAIDER-Tool-Equipped-Large-Language-Model-Agent-for-Robotic-Action-Issue-Detection-Explanation-and-Recovery"><a href="#RAIDER-Tool-Equipped-Large-Language-Model-Agent-for-Robotic-Action-Issue-Detection-Explanation-and-Recovery" class="headerlink" title="RAIDER: Tool-Equipped Large Language Model Agent for Robotic Action   Issue Detection, Explanation and Recovery"></a>RAIDER: Tool-Equipped Large Language Model Agent for Robotic Action   Issue Detection, Explanation and Recovery</h2><p><strong>Authors:Silvia Izquierdo-Badiola, Carlos Rizzo, Guillem AlenyÃ </strong></p>
<p>As robots increasingly operate in dynamic human-centric environments, improving their ability to detect, explain, and recover from action-related issues becomes crucial. Traditional model-based and data-driven techniques lack adaptability, while more flexible generative AI methods struggle with grounding extracted information to real-world constraints. We introduce RAIDER, a novel agent that integrates Large Language Models (LLMs) with grounded tools for adaptable and efficient issue detection and explanation. Using a unique â€œGround, Ask&amp;Answer, Issueâ€ procedure, RAIDER dynamically generates context-aware precondition questions and selects appropriate tools for resolution, achieving targeted information gathering. Our results within a simulated household environment surpass methods relying on predefined models, full scene descriptions, or standalone trained models. Additionally, RAIDERâ€™s explanations enhance recovery success, including cases requiring human interaction. Its modular architecture, featuring self-correction mechanisms, enables straightforward adaptation to diverse scenarios, as demonstrated in a real-world human-assistive task. This showcases RAIDERâ€™s potential as a versatile agentic AI solution for robotic issue detection and explanation, while addressing the problem of grounding generative AI for its effective application in embodied agents. Project website: <a target="_blank" rel="noopener" href="https://eurecat.github.io/raider-llmagent/">https://eurecat.github.io/raider-llmagent/</a> </p>
<blockquote>
<p>éšç€æœºå™¨äººè¶Šæ¥è¶Šå¤šåœ°åœ¨ä»¥äººç±»ä¸ºä¸­å¿ƒçš„ç¯å¢ƒä¸­è¿è¡Œï¼Œæé«˜å®ƒä»¬æ£€æµ‹ã€è§£é‡Šå’Œä»åŠ¨ä½œç›¸å…³é—®é¢˜ä¸­æ¢å¤çš„èƒ½åŠ›å˜å¾—è‡³å…³é‡è¦ã€‚åŸºäºæ¨¡å‹å’Œä¼ ç»Ÿæ•°æ®é©±åŠ¨çš„æŠ€æœ¯ç¼ºä¹é€‚åº”æ€§ï¼Œè€Œæ›´çµæ´»çš„ç”Ÿæˆå¼äººå·¥æ™ºèƒ½æ–¹æ³•åœ¨å°†æå–çš„ä¿¡æ¯ä¸ç°å®ä¸–ç•Œçš„çº¦æŸç›¸ç»“åˆæ–¹é¢å´è¡¨ç°æŒ£æ‰ã€‚æˆ‘ä»¬å¼•å…¥äº†RAIDERï¼Œè¿™æ˜¯ä¸€ä¸ªæ–°å‹æ™ºèƒ½ä½“ï¼Œå®ƒèåˆäº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å’ŒåŸºäºç°å®çš„å·¥å…·ï¼Œç”¨äºé€‚åº”æ€§å¼ºã€é«˜æ•ˆçš„é—®é¢˜æ£€æµ‹å’Œè§£é‡Šã€‚é€šè¿‡ç‹¬ç‰¹çš„â€œæ¥åœ°ã€é—®ç­”ã€é—®é¢˜â€æµç¨‹ï¼ŒRAIDERèƒ½å¤ŸåŠ¨æ€ç”Ÿæˆå…·æœ‰æƒ…å¢ƒæ„è¯†çš„å…ˆå†³æ¡ä»¶é—®é¢˜ï¼Œå¹¶é€‰æ‹©é€‚å½“çš„å·¥å…·è¿›è¡Œè§£å†³ï¼Œä»è€Œå®ç°æœ‰é’ˆå¯¹æ€§çš„ä¿¡æ¯æ”¶é›†ã€‚åœ¨æ¨¡æ‹Ÿå®¶åº­ç¯å¢ƒä¸­çš„ç»“æœè¶…è¿‡äº†ä¾èµ–äºé¢„å®šä¹‰æ¨¡å‹ã€å…¨æ™¯æè¿°æˆ–ç‹¬ç«‹è®­ç»ƒæ¨¡å‹çš„æ–¹æ³•ã€‚æ­¤å¤–ï¼ŒRAIDERçš„è§£é‡Šå¢å¼ºäº†æ¢å¤æˆåŠŸçš„å¯èƒ½æ€§ï¼ŒåŒ…æ‹¬éœ€è¦äººç±»äº¤äº’çš„æƒ…å†µã€‚å…¶æ¨¡å—åŒ–æ¶æ„é…å¤‡äº†è‡ªæˆ‘æ ¡æ­£æœºåˆ¶ï¼Œå¯è½»æ¾é€‚åº”å„ç§åœºæ™¯ï¼Œæ­£å¦‚åœ¨äººç±»è¾…åŠ©ä»»åŠ¡ä¸­æ‰€å±•ç¤ºçš„é‚£æ ·ã€‚è¿™å±•ç¤ºäº†RAIDERä½œä¸ºé€šç”¨æ™ºèƒ½ä½“è§£å†³æ–¹æ¡ˆåœ¨æœºå™¨äººé—®é¢˜æ£€æµ‹å’Œè§£é‡Šæ–¹é¢çš„æ½œåŠ›ï¼ŒåŒæ—¶è§£å†³äº†ç”Ÿæˆå¼äººå·¥æ™ºèƒ½åœ¨å®ä½“æœºå™¨äººä¸­çš„åº”ç”¨ä¸­çš„æ¥åœ°é—®é¢˜ã€‚é¡¹ç›®ç½‘ç«™ï¼š<a target="_blank" rel="noopener" href="https://eurecat.github.io/raider-llmagent/">https://eurecat.github.io/raider-llmagent/</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.17703v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœºå™¨äººæŠ€æœ¯åœ¨åŠ¨æ€ä»¥äººä¸ºä¸­å¿ƒçš„ç¯å¢ƒä¸­æ“ä½œæ—¥ç›Šæ™®éï¼Œæé«˜æœºå™¨äººæ£€æµ‹å’Œè§£é‡ŠåŠ¨ä½œç›¸å…³é—®é¢˜çš„èƒ½åŠ›ä»¥åŠä»æ•…éšœä¸­æ¢å¤çš„èƒ½åŠ›å˜å¾—è‡³å…³é‡è¦ã€‚ä¼ ç»Ÿçš„æ¨¡å‹é©±åŠ¨å’Œæ•°æ®é©±åŠ¨æ–¹æ³•ç¼ºä¹é€‚åº”æ€§ï¼Œè€Œçµæ´»çš„ç”Ÿæˆå¼AIæ–¹æ³•åˆ™éš¾ä»¥å°†æå–çš„ä¿¡æ¯ä¸ç°å®ä¸–ç•Œçš„çº¦æŸç›¸è”ç³»ã€‚æœ¬ç ”ç©¶å¼•å…¥äº†RAIDERï¼Œè¿™æ˜¯ä¸€ç§é›†æˆäº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å’Œæ¥åœ°å·¥å…·çš„æ–°å‹æ™ºèƒ½ä½“ï¼Œç”¨äºé€‚åº”æ€§å¼ºã€æ•ˆç‡é«˜çš„æ•…éšœæ£€æµ‹ä¸è§£é‡Šã€‚é€šè¿‡ç‹¬ç‰¹çš„â€œæ¥åœ°ã€æé—®ä¸å›ç­”ã€è§£å†³é—®é¢˜â€æµç¨‹ï¼ŒRAIDERèƒ½åŠ¨æ€ç”Ÿæˆæƒ…å¢ƒæ„ŸçŸ¥çš„å‰ç½®é—®é¢˜ï¼Œå¹¶é€‰æ‹©é€‚å½“çš„å·¥å…·è¿›è¡Œè§£å†³ï¼Œå®ç°æœ‰é’ˆå¯¹æ€§çš„ä¿¡æ¯æ”¶é›†ã€‚åœ¨æ¨¡æ‹Ÿå®¶åº­ç¯å¢ƒä¸­çš„è¡¨ç°ä¼˜äºä¾èµ–é¢„è®¾æ¨¡å‹ã€å®Œæ•´åœºæ™¯æè¿°æˆ–ç‹¬ç«‹è®­ç»ƒæ¨¡å‹çš„æ–¹æ³•ã€‚æ­¤å¤–ï¼ŒRAIDERçš„è§£é‡ŠåŠŸèƒ½æé«˜äº†æ¢å¤æˆåŠŸç‡ï¼ŒåŒ…æ‹¬éœ€è¦äººç±»äº¤äº’çš„æƒ…å†µã€‚å…¶æ¨¡å—åŒ–æ¶æ„å…·æœ‰è‡ªæˆ‘ä¿®æ­£æœºåˆ¶ï¼Œèƒ½è½»æ¾é€‚åº”å„ç§åœºæ™¯ï¼Œåœ¨çœŸå®ä¸–ç•Œçš„äººç±»è¾…åŠ©ä»»åŠ¡ä¸­å¾—åˆ°äº†éªŒè¯ã€‚è¿™ä¸ºRAIDERä½œä¸ºé€šç”¨æ™ºèƒ½ä½“åœ¨æœºå™¨äººé—®é¢˜æ£€æµ‹å’Œè§£é‡Šæ–¹é¢çš„æ½œåŠ›æä¾›äº†å±•ç¤ºï¼Œè§£å†³äº†ç”Ÿæˆå¼AIåœ¨ç°å®åº”ç”¨ä¸­çš„æ¥åœ°é—®é¢˜ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>éšç€æœºå™¨äººåœ¨ä»¥äººä¸ºä¸­å¿ƒçš„ç¯å¢ƒä¸­æ“ä½œæ—¥ç›Šå¢å¤šï¼Œæé«˜å…¶æ£€æµ‹å’Œè§£é‡ŠåŠ¨ä½œç›¸å…³é—®é¢˜çš„èƒ½åŠ›å˜å¾—è‡³å…³é‡è¦ã€‚</li>
<li>ä¼ ç»Ÿæ¨¡å‹é©±åŠ¨å’Œæ•°æ®é©±åŠ¨æ–¹æ³•ç¼ºä¹é€‚åº”æ€§ï¼Œéš¾ä»¥æ»¡è¶³åŠ¨æ€ç¯å¢ƒå˜åŒ–çš„éœ€æ±‚ã€‚</li>
<li>å¼•å…¥RAIDERæ™ºèƒ½ä½“ï¼Œé›†æˆäº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å’Œæ¥åœ°å·¥å…·ï¼Œç”¨äºå¢å¼ºæœºå™¨äººçš„é—®é¢˜æ£€æµ‹å’Œè§£é‡Šèƒ½åŠ›ã€‚</li>
<li>RAIDERé‡‡ç”¨ç‹¬ç‰¹çš„â€œæ¥åœ°ã€æé—®ä¸å›ç­”ã€è§£å†³é—®é¢˜â€æµç¨‹ï¼Œå®ç°åŠ¨æ€ç”Ÿæˆæƒ…å¢ƒæ„ŸçŸ¥çš„å‰ç½®é—®é¢˜å’Œæœ‰é’ˆå¯¹æ€§çš„ä¿¡æ¯æ”¶é›†ã€‚</li>
<li>åœ¨æ¨¡æ‹Ÿå®¶åº­ç¯å¢ƒä¸­ï¼ŒRAIDERè¡¨ç°ä¼˜äºä¼ ç»Ÿæ–¹æ³•ï¼Œå…·æœ‰å¼ºå¤§çš„é€‚åº”æ€§ã€‚</li>
<li>RAIDERçš„è§£é‡ŠåŠŸèƒ½å¢å¼ºäº†æ¢å¤æˆåŠŸç‡ï¼Œå¹¶åœ¨éœ€è¦äººç±»äº¤äº’çš„æƒ…å†µä¸‹è¡¨ç°å‡ºä¼˜åŠ¿ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.17703">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-c0691163bcfe7037087349c93d113000.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f638df604c7656fed145cb1930f02de0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2fc94a3bd4a293947d48c1196f61f692.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-5dd7d46e34d800866ffc1e22d1fd6360.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="Adaptive-Semantic-Prompt-Caching-with-VectorQ"><a href="#Adaptive-Semantic-Prompt-Caching-with-VectorQ" class="headerlink" title="Adaptive Semantic Prompt Caching with VectorQ"></a>Adaptive Semantic Prompt Caching with VectorQ</h2><p><strong>Authors:Luis Gaspar Schroeder, Shu Liu, Alejandro Cuadron, Mark Zhao, Stephan Krusche, Alfons Kemper, Matei Zaharia, Joseph E. Gonzalez</strong></p>
<p>Semantic prompt caches reduce the latency and cost of large language model (LLM) inference by reusing cached LLM-generated responses for semantically similar prompts. Vector similarity metrics assign a numerical score to quantify the similarity between an embedded prompt and its nearest neighbor in the cache. Existing systems rely on a static threshold to classify whether the similarity score is sufficiently high to result in a cache hit. We show that this one-size-fits-all threshold is insufficient across different embeddings. We propose VectorQ, an online framework with a threshold convergence guarantee to learn embedding-specific threshold regions that adapt to the uncertainty of an embedding. Through evaluations on a combination of three diverse datasets, we show that VectorQ consistently outperforms state-of-the-art systems across all static thresholds, achieving up to 26x increases in cache hit rate and error rate reductions up to 74%. </p>
<blockquote>
<p>è¯­ä¹‰æç¤ºç¼“å­˜é€šè¿‡é‡ç”¨ç¼“å­˜ä¸­LLMç”Ÿæˆçš„ä¸è¯­ä¹‰ç›¸ä¼¼çš„æç¤ºå“åº”ï¼Œå‡å°‘äº†å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ¨ç†çš„å»¶è¿Ÿå’Œæˆæœ¬ã€‚å‘é‡ç›¸ä¼¼åº¦åº¦é‡ä¼šç»™å‡ºä¸€ä¸ªæ•°å€¼åˆ†æ•°æ¥è¡¡é‡åµŒå…¥æç¤ºä¸å…¶åœ¨ç¼“å­˜ä¸­çš„æœ€è¿‘é‚»å±…ä¹‹é—´çš„ç›¸ä¼¼åº¦ã€‚ç°æœ‰ç³»ç»Ÿä¾èµ–äºé™æ€é˜ˆå€¼æ¥åˆ†ç±»ç›¸ä¼¼åº¦åˆ†æ•°æ˜¯å¦è¶³å¤Ÿé«˜ä»¥äº§ç”Ÿç¼“å­˜å‘½ä¸­ã€‚æˆ‘ä»¬è¡¨æ˜ï¼Œè¿™ç§ä¸€åˆ€åˆ‡é˜ˆå€¼åœ¨ä¸åŒåµŒå…¥ä¸­æ˜¯ä¸å¤Ÿç”¨çš„ã€‚æˆ‘ä»¬æå‡ºäº†VectorQï¼Œä¸€ä¸ªå¸¦æœ‰é˜ˆå€¼æ”¶æ•›ä¿è¯çš„åœ¨çº¿æ¡†æ¶ï¼Œå­¦ä¹ é’ˆå¯¹åµŒå…¥çš„ç‰¹å®šé˜ˆå€¼åŒºåŸŸï¼Œä»¥é€‚åº”åµŒå…¥çš„ä¸ç¡®å®šæ€§ã€‚é€šè¿‡å¯¹ä¸‰ä¸ªä¸åŒæ•°æ®é›†çš„è¯„ä¼°ï¼Œæˆ‘ä»¬è¯æ˜äº†VectorQåœ¨æ‰€æœ‰é™æ€é˜ˆå€¼ä¸Šå‡ä¼˜äºæœ€æ–°ç³»ç»Ÿï¼Œå®ç°äº†é«˜è¾¾26å€çš„ç¼“å­˜å‘½ä¸­ç‡æå‡ï¼Œè¯¯å·®ç‡é™ä½é«˜è¾¾74%ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.03771v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>è¯­ä¹‰æç¤ºç¼“å­˜é€šè¿‡é‡ç”¨ç¼“å­˜ä¸­LLMç”Ÿæˆçš„ä¸è¯­ä¹‰ç›¸ä¼¼æç¤ºç›¸å¯¹åº”çš„å›ç­”ï¼Œå‡å°‘äº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ¨ç†çš„å»¶è¿Ÿå’Œæˆæœ¬ã€‚å‘é‡ç›¸ä¼¼åº¦åº¦é‡æ ‡å‡†ç»™å‡ºä¸€æ•°å€¼åˆ†æ•°æ¥è¡¡é‡åµŒå…¥æç¤ºä¸å…¶åœ¨ç¼“å­˜ä¸­çš„æœ€è¿‘é‚»å±…ä¹‹é—´çš„ç›¸ä¼¼æ€§ã€‚ç°æœ‰ç³»ç»Ÿä¾èµ–äºé™æ€é˜ˆå€¼æ¥åˆ¤æ–­ç›¸ä¼¼åº¦åˆ†æ•°æ˜¯å¦è¶³å¤Ÿé«˜ä»¥äº§ç”Ÿç¼“å­˜å‘½ä¸­ã€‚æœ¬æ–‡æŒ‡å‡ºï¼Œè¿™ç§ä¸€åˆ€åˆ‡é˜ˆå€¼åœ¨ä¸åŒåµŒå…¥ä¸­çš„é€‚ç”¨æ€§ä¸è¶³ã€‚å› æ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†VectorQï¼Œä¸€ä¸ªåœ¨çº¿æ¡†æ¶ï¼Œå…·æœ‰é˜ˆå€¼æ”¶æ•›ä¿è¯ï¼Œèƒ½å¤Ÿå­¦ä¹ é€‚åº”åµŒå…¥ä¸ç¡®å®šæ€§çš„åµŒå…¥ç‰¹å®šé˜ˆå€¼åŒºåŸŸã€‚åœ¨ä¸‰ä¸ªä¸åŒæ•°æ®é›†ä¸Šçš„è¯„ä¼°è¡¨æ˜ï¼ŒVectorQåœ¨æ‰€æœ‰çš„é™æ€é˜ˆå€¼ä¸Šéƒ½è¡¨ç°å‡ºä¸€è‡´çš„ä¼˜åŠ¿ï¼Œå®ç°äº†é«˜è¾¾26å€çš„ç¼“å­˜å‘½ä¸­ç‡æå‡å’Œé«˜è¾¾74%çš„é”™è¯¯ç‡é™ä½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è¯­ä¹‰æç¤ºç¼“å­˜ç”¨äºå‡å°‘LLMæ¨ç†çš„å»¶è¿Ÿå’Œæˆæœ¬ã€‚</li>
<li>å‘é‡ç›¸ä¼¼åº¦åº¦é‡ç”¨äºè¯„ä¼°åµŒå…¥æç¤ºä¸å…¶ç¼“å­˜ä¸­æœ€è¿‘é‚»å±…çš„ç›¸ä¼¼æ€§ã€‚</li>
<li>ç°æœ‰ç³»ç»Ÿä¾èµ–é™æ€é˜ˆå€¼åˆ¤æ–­ç¼“å­˜å‘½ä¸­å­˜åœ¨ä¸è¶³ã€‚</li>
<li>VectorQæ¡†æ¶é€šè¿‡åœ¨çº¿å­¦ä¹ å’Œé˜ˆå€¼æ”¶æ•›ä¿è¯æ¥é€‚åº”ä¸åŒåµŒå…¥çš„ä¸ç¡®å®šæ€§ã€‚</li>
<li>VectorQåœ¨ä¸åŒæ•°æ®é›†ä¸Šçš„è¯„ä¼°è¡¨ç°ä¼˜äºç°æœ‰ç³»ç»Ÿã€‚</li>
<li>VectorQæé«˜äº†é«˜è¾¾26å€çš„ç¼“å­˜å‘½ä¸­ç‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.03771">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-045d5cd87e777160a2740b6d00deb79d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-019508769096abab5190d815dfb974bb.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ec82169e80010bb8440c9f5bff04d268.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-607044dc65508deb7beec50bb3f3da1b.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="CALICO-Part-Focused-Semantic-Co-Segmentation-with-Large-Vision-Language-Models"><a href="#CALICO-Part-Focused-Semantic-Co-Segmentation-with-Large-Vision-Language-Models" class="headerlink" title="CALICO: Part-Focused Semantic Co-Segmentation with Large Vision-Language   Models"></a>CALICO: Part-Focused Semantic Co-Segmentation with Large Vision-Language   Models</h2><p><strong>Authors:Kiet A. Nguyen, Adheesh Juvekar, Tianjiao Yu, Muntasir Wahed, Ismini Lourentzou</strong></p>
<p>Recent advances in Large Vision-Language Models (LVLMs) have enabled general-purpose vision tasks through visual instruction tuning. While existing LVLMs can generate segmentation masks from text prompts for single images, they struggle with segmentation-grounded reasoning across images, especially at finer granularities such as object parts. In this paper, we introduce the new task of part-focused semantic co-segmentation, which involves identifying and segmenting common objects, as well as common and unique object parts across images. To address this task, we present CALICO, the first LVLM designed for multi-image part-level reasoning segmentation. CALICO features two key components, a novel Correspondence Extraction Module that identifies semantic part-level correspondences, and Correspondence Adaptation Modules that embed this information into the LVLM to facilitate multi-image understanding in a parameter-efficient manner. To support training and evaluation, we curate MixedParts, a large-scale multi-image segmentation dataset containing $\sim$2.4M samples across $\sim$44K images spanning diverse object and part categories. Experimental results demonstrate that CALICO, with just 0.3% of its parameters finetuned, achieves strong performance on this challenging task. </p>
<blockquote>
<p>è¿‘æœŸå¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆLVLMsï¼‰çš„è¿›æ­¥é€šè¿‡è§†è§‰æŒ‡ä»¤è°ƒæ•´å®ç°äº†é€šç”¨è§†è§‰ä»»åŠ¡ã€‚è™½ç„¶ç°æœ‰çš„LVLMså¯ä»¥é€šè¿‡æ–‡æœ¬æç¤ºå¯¹å•å¼ å›¾åƒç”Ÿæˆåˆ†å‰²æ©è†œï¼Œä½†å®ƒä»¬åœ¨è¿›è¡Œè·¨å›¾åƒçš„åˆ†å‰²æ¨ç†æ—¶é‡åˆ°å›°éš¾ï¼Œå°¤å…¶æ˜¯åœ¨è¾ƒç»†çš„ç²’åº¦ï¼ˆå¦‚ç‰©ä½“éƒ¨åˆ†ï¼‰ä¸Šã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬å¼•å…¥äº†æ–°çš„éƒ¨åˆ†èšç„¦è¯­ä¹‰ååŒåˆ†å‰²ä»»åŠ¡ï¼Œè¯¥ä»»åŠ¡æ¶‰åŠè¯†åˆ«å¹¶åˆ†å‰²è·¨å›¾åƒä¸­çš„å¸¸è§å¯¹è±¡ä»¥åŠå¸¸è§å’Œç‹¬ç‰¹çš„å¯¹è±¡éƒ¨åˆ†ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†CALICOï¼Œè¿™æ˜¯é¦–æ¬¾é’ˆå¯¹å¤šå›¾åƒéƒ¨åˆ†çº§æ¨ç†åˆ†å‰²è®¾è®¡çš„LVLMã€‚CALICOæœ‰ä¸¤ä¸ªå…³é”®ç»„ä»¶ï¼Œä¸€ä¸ªæ˜¯æ–°å‹å¯¹åº”å…³ç³»æå–æ¨¡å—ï¼Œç”¨äºè¯†åˆ«è¯­ä¹‰éƒ¨åˆ†çº§å¯¹åº”å…³ç³»ï¼Œå¦ä¸€ä¸ªæ˜¯å¯¹åº”å…³ç³»é€‚åº”æ¨¡å—ï¼Œå°†è¿™äº›ä¿¡æ¯åµŒå…¥LVLMä¸­ï¼Œä»¥é«˜æ•ˆå‚æ•°çš„æ–¹å¼ä¿ƒè¿›å¤šå›¾åƒç†è§£ã€‚ä¸ºäº†æ”¯æŒè®­ç»ƒå’Œè¯„ä¼°ï¼Œæˆ‘ä»¬æ•´ç†äº†MixedPartsï¼Œè¿™æ˜¯ä¸€ä¸ªå¤§è§„æ¨¡çš„å¤šå›¾åƒåˆ†å‰²æ•°æ®é›†ï¼ŒåŒ…å«çº¦240ä¸‡æ ·æœ¬ï¼Œåˆ†å¸ƒåœ¨çº¦4ä¸‡å¼ å›¾åƒä¸Šï¼Œæ¶µç›–å„ç§å¯¹è±¡å’Œéƒ¨ä»¶ç±»åˆ«ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒCALICOä»…å¾®è°ƒå…¶å‚æ•°çš„0.3%ï¼Œå°±èƒ½åœ¨è¿™ä¸ªå…·æœ‰æŒ‘æˆ˜æ€§çš„ä»»åŠ¡ä¸Šå–å¾—å‡ºè‰²çš„è¡¨ç°ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.19331v2">PDF</a> Accepted to CVPR 2025. Project page:   <a target="_blank" rel="noopener" href="https://plan-lab.github.io/calico/">https://plan-lab.github.io/calico/</a></p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆLVLMsï¼‰çš„æœ€æ–°è¿›å±•ï¼Œé€šè¿‡è§†è§‰æŒ‡ä»¤å¾®è°ƒå®ç°äº†é€šç”¨è§†è§‰ä»»åŠ¡ã€‚ç°æœ‰LVLMså¯ä»¥ä»æ–‡æœ¬æç¤ºä¸­ç”Ÿæˆå•ä¸ªå›¾åƒçš„åˆ†æ®µæ©è†œï¼Œä½†åœ¨è·¨å›¾åƒçš„åˆ†æ®µåŸºç¡€æ¨ç†æ–¹é¢å­˜åœ¨å›°éš¾ï¼Œç‰¹åˆ«æ˜¯åœ¨å¯¹è±¡éƒ¨åˆ†çš„æ›´ç²¾ç»†ç²’åº¦ä¸Šã€‚ä¸ºæ­¤ï¼Œæœ¬æ–‡å¼•å…¥äº†éƒ¨åˆ†èšç„¦è¯­ä¹‰ååŒåˆ†æ®µçš„æ–°ä»»åŠ¡ï¼Œæ¶‰åŠè¯†åˆ«å¹¶åˆ†æ®µè·¨å›¾åƒçš„å¸¸è§å¯¹è±¡åŠå¯¹è±¡éƒ¨åˆ†ã€‚ä¸ºåº”å¯¹æ­¤ä»»åŠ¡ï¼Œæå‡ºäº†CALICOï¼Œé¦–ä¸ªè®¾è®¡ç”¨äºå¤šå›¾åƒéƒ¨åˆ†çº§åˆ«æ¨ç†åˆ†æ®µçš„LVLMã€‚CALICOå…·æœ‰ä¸¤ä¸ªå…³é”®ç»„ä»¶ï¼šå¯¹åº”æå–æ¨¡å—ï¼Œç”¨äºè¯†åˆ«è¯­ä¹‰éƒ¨åˆ†çº§åˆ«çš„å¯¹åº”æ€§ï¼›å¯¹åº”é€‚åº”æ¨¡å—ï¼Œå°†ä¿¡æ¯åµŒå…¥LVLMä¸­ï¼Œä»¥é«˜æ•ˆå‚æ•°çš„æ–¹å¼ä¿ƒè¿›å¤šå›¾åƒç†è§£ã€‚ä¸ºæ”¯æŒå’Œè¯„ä¼°è®­ç»ƒï¼Œæ•´ç†å‡ºMixedPartså¤§å‹å¤šå›¾åƒåˆ†æ®µæ•°æ®é›†ï¼ŒåŒ…å«çº¦44Kå¼ å›¾åƒã€æ¶µç›–ä¸åŒå¯¹è±¡å’Œé›¶ä»¶ç±»åˆ«çš„çº¦2.4Mæ ·æœ¬ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒCALICOä»…å¾®è°ƒå…¶å‚æ•°çš„0.3%ï¼Œå³å¯åœ¨æ­¤æŒ‘æˆ˜æ€§ä»»åŠ¡ä¸Šå–å¾—è‰¯å¥½è¡¨ç°ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LVLMsèƒ½å¤Ÿé€šè¿‡è§†è§‰æŒ‡ä»¤å¾®è°ƒå®Œæˆé€šç”¨è§†è§‰ä»»åŠ¡ã€‚</li>
<li>ç°æœ‰LVLMsåœ¨è·¨å›¾åƒåˆ†æ®µæ¨ç†æ–¹é¢å­˜åœ¨å›°éš¾ï¼Œç‰¹åˆ«æ˜¯åœ¨å¯¹è±¡éƒ¨åˆ†çš„ç²¾ç»†ç²’åº¦ä¸Šã€‚</li>
<li>å¼•å…¥æ–°çš„ä»»åŠ¡ï¼šéƒ¨åˆ†èšç„¦è¯­ä¹‰ååŒåˆ†æ®µï¼Œæ—¨åœ¨è¯†åˆ«å¹¶åˆ†æ®µè·¨å›¾åƒçš„å¸¸è§å¯¹è±¡åŠå…¶éƒ¨åˆ†ã€‚</li>
<li>æå‡ºCALICOæ¨¡å‹ï¼Œä¸“ä¸ºå¤šå›¾åƒéƒ¨åˆ†çº§åˆ«æ¨ç†åˆ†æ®µè®¾è®¡ã€‚</li>
<li>CALICOåŒ…å«ä¸¤ä¸ªå…³é”®ç»„ä»¶ï¼šå¯¹åº”æå–æ¨¡å—å’Œå¯¹åº”é€‚åº”æ¨¡å—ã€‚</li>
<li>ä¸ºäº†è®­ç»ƒå’Œè¯„ä¼°æ¨¡å‹ï¼Œåˆ›å»ºäº†MixedPartsæ•°æ®é›†ï¼ŒåŒ…å«å¤§é‡çš„å¤šå›¾åƒåˆ†æ®µæ ·æœ¬ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.19331">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-a6cc99700f1b1bfa75c2a451f860ecc3.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a21e00ccb1f8020e81751da33796b829.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-bf2079458df343e6a0e5678170128344.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-5f294b1649b43f0755d74680f498bb62.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-90b28495fc73d1cda7d98f8d14cf7263.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="AdaCM-2-On-Understanding-Extremely-Long-Term-Video-with-Adaptive-Cross-Modality-Memory-Reduction"><a href="#AdaCM-2-On-Understanding-Extremely-Long-Term-Video-with-Adaptive-Cross-Modality-Memory-Reduction" class="headerlink" title="AdaCM$^2$: On Understanding Extremely Long-Term Video with Adaptive   Cross-Modality Memory Reduction"></a>AdaCM$^2$: On Understanding Extremely Long-Term Video with Adaptive   Cross-Modality Memory Reduction</h2><p><strong>Authors:Yuanbin Man, Ying Huang, Chengming Zhang, Bingzhe Li, Wei Niu, Miao Yin</strong></p>
<p>The advancements in large language models (LLMs) have propelled the improvement of video understanding tasks by incorporating LLMs with visual models. However, most existing LLM-based models (e.g., VideoLLaMA, VideoChat) are constrained to processing short-duration videos. Recent attempts to understand long-term videos by extracting and compressing visual features into a fixed memory size. Nevertheless, those methods leverage only visual modality to merge video tokens and overlook the correlation between visual and textual queries, leading to difficulties in effectively handling complex question-answering tasks. To address the challenges of long videos and complex prompts, we propose AdaCM$^2$, which, for the first time, introduces an adaptive cross-modality memory reduction approach to video-text alignment in an auto-regressive manner on video streams. Our extensive experiments on various video understanding tasks, such as video captioning, video question answering, and video classification, demonstrate that AdaCM$^2$ achieves state-of-the-art performance across multiple datasets while significantly reducing memory usage. Notably, it achieves a 4.5% improvement across multiple tasks in the LVU dataset with a GPU memory consumption reduction of up to 65%. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„è¿›æ­¥é€šè¿‡å°†å…¶ä¸è§†è§‰æ¨¡å‹ç»“åˆï¼Œæ¨åŠ¨äº†è§†é¢‘ç†è§£ä»»åŠ¡çš„æ”¹è¿›ã€‚ç„¶è€Œï¼Œå¤§å¤šæ•°ç°æœ‰çš„åŸºäºLLMçš„æ¨¡å‹ï¼ˆä¾‹å¦‚VideoLLaMAã€VideoChatï¼‰ä»…é™äºå¤„ç†çŸ­æ—¶é•¿è§†é¢‘ã€‚æœ€è¿‘çš„å°è¯•é€šè¿‡æå–å’Œå‹ç¼©è§†é¢‘ç‰¹å¾åˆ°å›ºå®šå†…å­˜å¤§å°æ¥ç†è§£é•¿è§†é¢‘ã€‚ç„¶è€Œï¼Œè¿™äº›æ–¹æ³•ä»…åˆ©ç”¨è§†è§‰æ¨¡å¼æ¥åˆå¹¶è§†é¢‘ä»¤ç‰Œï¼Œå¹¶å¿½ç•¥äº†è§†è§‰å’Œæ–‡æœ¬æŸ¥è¯¢ä¹‹é—´çš„ç›¸å…³æ€§ï¼Œå¯¼è‡´åœ¨å¤„ç†å¤æ‚çš„é—®ç­”ä»»åŠ¡æ—¶é¢ä¸´å›°éš¾ã€‚ä¸ºäº†è§£å†³é•¿è§†é¢‘å’Œå¤æ‚æç¤ºçš„æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†AdaCM$^2$ï¼Œå®ƒé¦–æ¬¡åœ¨è§†é¢‘æµä¸Šä»¥ä¸€ç§è‡ªå›å½’çš„æ–¹å¼å¼•å…¥äº†è‡ªé€‚åº”è·¨æ¨¡æ€å†…å­˜ç¼©å‡æ–¹æ³•è¿›è¡Œè§†é¢‘æ–‡æœ¬å¯¹é½ã€‚æˆ‘ä»¬åœ¨å„ç§è§†é¢‘ç†è§£ä»»åŠ¡ä¸Šè¿›è¡Œäº†å¤§é‡å®éªŒï¼Œå¦‚è§†é¢‘æè¿°ã€è§†é¢‘é—®ç­”å’Œè§†é¢‘åˆ†ç±»ï¼Œç»“æœè¡¨æ˜AdaCM$^2$åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šå®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼ŒåŒæ—¶æ˜¾è‘—é™ä½äº†å†…å­˜ä½¿ç”¨ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œå®ƒåœ¨LVUæ•°æ®é›†çš„å¤šé¡¹ä»»åŠ¡ä¸Šå®ç°äº†4.5%çš„æ”¹è¿›ï¼ŒGPUå†…å­˜æ¶ˆè€—å‡å°‘äº†é«˜è¾¾65%ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2411.12593v3">PDF</a> CVPR 2025 Highlight</p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨è§†é¢‘ç†è§£ä»»åŠ¡ä¸­çš„åº”ç”¨å–å¾—äº†æ˜¾è‘—è¿›å±•ã€‚ç„¶è€Œï¼Œç°æœ‰LLMæ¨¡å‹ä¸»è¦å¤„ç†çŸ­è§†é¢‘ï¼Œéš¾ä»¥åº”å¯¹é•¿è§†é¢‘å’Œå¤æ‚æŸ¥è¯¢ä»»åŠ¡ã€‚ä¸ºæ­¤ï¼Œæœ¬æ–‡æå‡ºäº†AdaCM$^2$æ¨¡å‹ï¼Œé¦–æ¬¡é‡‡ç”¨è‡ªé€‚åº”è·¨æ¨¡æ€è®°å¿†ç¼©å‡æ–¹æ³•ï¼Œä»¥è‡ªé€‚åº”ã€è‡ªå›å½’çš„æ–¹å¼å¤„ç†è§†é¢‘æµä¸­çš„è§†é¢‘æ–‡æœ¬å¯¹é½é—®é¢˜ã€‚å®éªŒè¯æ˜ï¼ŒAdaCM$^2$åœ¨å¤šä¸ªè§†é¢‘ç†è§£ä»»åŠ¡ä¸Šè¾¾åˆ°äº†å…ˆè¿›æ€§èƒ½ï¼ŒåŒæ—¶åœ¨LVUæ•°æ®é›†ä¸Šå®ç°äº†é«˜è¾¾65%çš„GPUå†…å­˜æ¶ˆè€—å‡å°‘ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLMçš„è¿›æ­¥æ¨åŠ¨äº†è§†é¢‘ç†è§£ä»»åŠ¡çš„æ”¹å–„ã€‚</li>
<li>ç°æœ‰LLMæ¨¡å‹ä¸»è¦å¤„ç†çŸ­è§†é¢‘ï¼Œéš¾ä»¥åº”å¯¹é•¿è§†é¢‘å’Œå¤æ‚æŸ¥è¯¢ä»»åŠ¡ã€‚</li>
<li>AdaCM$^2$æ¨¡å‹é¦–æ¬¡é‡‡ç”¨è‡ªé€‚åº”è·¨æ¨¡æ€è®°å¿†ç¼©å‡æ–¹æ³•å¤„ç†è§†é¢‘æ–‡æœ¬å¯¹é½é—®é¢˜ã€‚</li>
<li>AdaCM$^2$æ¨¡å‹ä»¥è‡ªé€‚åº”ã€è‡ªå›å½’çš„æ–¹å¼å¤„ç†è§†é¢‘æµã€‚</li>
<li>AdaCM$^2$åœ¨å¤šä¸ªè§†é¢‘ç†è§£ä»»åŠ¡ä¸Šè¡¨ç°å‡ºå…ˆè¿›æ€§èƒ½ã€‚</li>
<li>AdaCM$^2$åœ¨LVUæ•°æ®é›†ä¸Šçš„æ€§èƒ½ä¼˜äºå…¶ä»–æ¨¡å‹ï¼Œå®ç°äº†é«˜è¾¾65%çš„GPUå†…å­˜æ¶ˆè€—å‡å°‘ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2411.12593">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-1ff05df0b5f749aed90f3f388b96d301.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-fc24960bf530eefc9336f3b0bcc74bd4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1a4596d8c20d674c631830971b7f2215.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-fd31b8751dc54027d2cc91c17fbe87f3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-212d066d1836dbd26fd50e84b5de62c1.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="RMB-Comprehensively-Benchmarking-Reward-Models-in-LLM-Alignment"><a href="#RMB-Comprehensively-Benchmarking-Reward-Models-in-LLM-Alignment" class="headerlink" title="RMB: Comprehensively Benchmarking Reward Models in LLM Alignment"></a>RMB: Comprehensively Benchmarking Reward Models in LLM Alignment</h2><p><strong>Authors:Enyu Zhou, Guodong Zheng, Binghai Wang, Zhiheng Xi, Shihan Dou, Rong Bao, Wei Shen, Limao Xiong, Jessica Fan, Yurong Mou, Rui Zheng, Tao Gui, Qi Zhang, Xuanjing Huang</strong></p>
<p>Reward models (RMs) guide the alignment of large language models (LLMs), steering them toward behaviors preferred by humans. Evaluating RMs is the key to better aligning LLMs. However, the current evaluation of RMs may not directly correspond to their alignment performance due to the limited distribution of evaluation data and evaluation methods that are not closely related to alignment objectives. To address these limitations, we propose RMB, a comprehensive RM benchmark that covers over 49 real-world scenarios and includes both pairwise and Best-of-N (BoN) evaluations to better reflect the effectiveness of RMs in guiding alignment optimization. We demonstrate a positive correlation between our benchmark and the downstream alignment task performance. Based on our benchmark, we conduct extensive analysis on the state-of-the-art RMs, revealing their generalization defects that were not discovered by previous benchmarks, and highlighting the potential of generative RMs. Furthermore, we delve into open questions in reward models, specifically examining the effectiveness of majority voting for the evaluation of reward models and analyzing the impact factors of generative RMs, including the influence of evaluation criteria and instructing methods. Our evaluation code and datasets are available at <a target="_blank" rel="noopener" href="https://github.com/Zhou-Zoey/RMB-Reward-Model-Benchmark">https://github.com/Zhou-Zoey/RMB-Reward-Model-Benchmark</a>. </p>
<blockquote>
<p>å¥–åŠ±æ¨¡å‹ï¼ˆRMï¼‰å¼•å¯¼å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„å¯¹é½ï¼Œä½¿å®ƒä»¬æœå‘äººç±»åå¥½çš„è¡Œä¸ºã€‚è¯„ä¼°RMæ˜¯æ›´å¥½åœ°å¯¹é½LLMçš„å…³é”®ã€‚ç„¶è€Œï¼Œå½“å‰çš„RMè¯„ä¼°å¯èƒ½æ— æ³•ç›´æ¥å¯¹åº”å…¶å¯¹é½æ€§èƒ½ï¼Œå› ä¸ºè¯„ä¼°æ•°æ®çš„åˆ†å¸ƒæœ‰é™ï¼Œä¸”è¯„ä¼°æ–¹æ³•ä¸å¯¹é½ç›®æ ‡ä¸ç´§å¯†ç›¸å…³ã€‚ä¸ºäº†è§£å†³è¿™äº›å±€é™æ€§ï¼Œæˆ‘ä»¬æå‡ºäº†RMBï¼ˆRMåŸºå‡†æµ‹è¯•ï¼‰ï¼Œè¿™æ˜¯ä¸€ä¸ªå…¨é¢çš„RMåŸºå‡†æµ‹è¯•ï¼Œæ¶µç›–è¶…è¿‡49ä¸ªçœŸå®åœºæ™¯ï¼ŒåŒ…æ‹¬æˆå¯¹å’ŒBest-of-Nï¼ˆBoNï¼‰è¯„ä¼°ï¼Œä»¥æ›´å¥½åœ°åæ˜ RMåœ¨å¼•å¯¼å¯¹é½ä¼˜åŒ–æ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚æˆ‘ä»¬è¯æ˜äº†æˆ‘ä»¬çš„åŸºå‡†æµ‹è¯•ä¸ä¸‹æ¸¸å¯¹é½ä»»åŠ¡æ€§èƒ½ä¹‹é—´çš„æ­£ç›¸å…³å…³ç³»ã€‚åŸºäºæˆ‘ä»¬çš„åŸºå‡†æµ‹è¯•ï¼Œæˆ‘ä»¬å¯¹æœ€å…ˆè¿›çš„RMè¿›è¡Œäº†å¹¿æ³›çš„åˆ†æï¼Œæ­ç¤ºäº†å®ƒä»¬æœªè¢«ä»¥å‰åŸºå‡†æµ‹è¯•å‘ç°çš„æ³›åŒ–ç¼ºé™·ï¼Œå¹¶çªå‡ºäº†ç”ŸæˆRMçš„æ½œåŠ›ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æ·±å…¥æ¢è®¨äº†å¥–åŠ±æ¨¡å‹ä¸­çš„å¼€æ”¾é—®é¢˜ï¼Œå…·ä½“æ£€æŸ¥äº†å¤šæ•°æŠ•ç¥¨åœ¨è¯„ä¼°å¥–åŠ±æ¨¡å‹ä¸­çš„æœ‰æ•ˆæ€§ï¼Œå¹¶åˆ†æäº†ç”ŸæˆRMçš„å½±å“å› ç´ ï¼ŒåŒ…æ‹¬è¯„ä¼°æ ‡å‡†å’ŒæŒ‡å¯¼æ–¹æ³•çš„å½±å“ã€‚æˆ‘ä»¬çš„è¯„ä¼°ä»£ç å’Œæ•°æ®é›†å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/Zhou-Zoey/RMB-Reward-Model-Benchmark%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/Zhou-Zoey/RMB-Reward-Model-Benchmarkæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2410.09893v2">PDF</a> Accepted by ICLR2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºä¸€ç§é’ˆå¯¹å¥–åŠ±æ¨¡å‹ï¼ˆRMï¼‰çš„ç»¼åˆåŸºå‡†æµ‹è¯•RMBï¼Œè¯¥åŸºå‡†æµ‹è¯•æ¶µç›–è¶…è¿‡49ä¸ªçœŸå®åœºæ™¯ï¼ŒåŒ…æ‹¬æˆå¯¹è¯„ä»·å’Œæœ€ä½³Nè¯„ä»·ï¼Œä»¥æ›´å¥½åœ°åæ˜ RMåœ¨æŒ‡å¯¼å¯¹é½ä¼˜åŒ–æ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚ç ”ç©¶å‘ç°ï¼Œè¯¥åŸºå‡†æµ‹è¯•ä¸ä¸‹æ¸¸å¯¹é½ä»»åŠ¡æ€§èƒ½ä¹‹é—´å­˜åœ¨æ­£ç›¸å…³ï¼Œå¯¹æœ€å…ˆè¿›çš„RMè¿›è¡Œäº†å¹¿æ³›çš„åˆ†æï¼Œæ­ç¤ºäº†å…¶ä»¥å¾€åŸºå‡†æµ‹è¯•ä¸­æœªå‘ç°çš„æ³›åŒ–ç¼ºé™·ï¼Œå¹¶å¼ºè°ƒäº†ç”Ÿæˆå¼RMçš„æ½œåŠ›ã€‚åŒæ—¶ï¼Œæ¢è®¨äº†å¥–åŠ±æ¨¡å‹ä¸­çš„å¼€æ”¾é—®é¢˜ï¼ŒåŒ…æ‹¬å¤šæ•°æŠ•ç¥¨çš„æœ‰æ•ˆæ€§è¯„ä¼°å’Œç”Ÿæˆå¼RMçš„å½±å“å› ç´ ç­‰ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¥–åŠ±æ¨¡å‹ï¼ˆRMï¼‰ç”¨äºæŒ‡å¯¼å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„å¯¹é½ï¼Œä½¿å…¶æœå‘äººç±»åå¥½çš„è¡Œä¸ºå‘å±•ã€‚</li>
<li>è¯„ä¼°RMæ˜¯ä¼˜åŒ–LLMå¯¹é½çš„å…³é”®ã€‚</li>
<li>å½“å‰RMè¯„ä¼°æ–¹æ³•å­˜åœ¨å±€é™æ€§ï¼Œæ— æ³•ç›´æ¥åæ˜ å…¶å¯¹é½æ€§èƒ½ï¼Œä¸»è¦ä½“ç°åœ¨è¯„ä»·æ•°æ®åˆ†å¸ƒæœ‰é™ä»¥åŠè¯„ä»·æ–¹æ³•æœªä¸å¯¹é½ç›®æ ‡ç´§å¯†ç›¸å…³ã€‚</li>
<li>ä¸ºè§£å†³è¿™äº›å±€é™æ€§ï¼Œæå‡ºäº†RMBåŸºå‡†æµ‹è¯•ï¼Œæ¶µç›–å¤šç§çœŸå®åœºæ™¯å¹¶åŒ…æ‹¬æˆå¯¹å’Œæœ€ä½³Nè¯„ä»·ï¼Œä»¥æ›´å‡†ç¡®åœ°åæ˜ RMåœ¨æŒ‡å¯¼å¯¹é½æ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚</li>
<li>RMBåŸºå‡†æµ‹è¯•ä¸ä¸‹æ¸¸å¯¹é½ä»»åŠ¡æ€§èƒ½ä¹‹é—´å­˜åœ¨æ­£ç›¸å…³ã€‚</li>
<li>åŸºäºRMBåŸºå‡†æµ‹è¯•ï¼Œå¯¹ç°æœ‰RMè¿›è¡Œäº†å¹¿æ³›åˆ†æï¼Œæ­ç¤ºäº†å…¶æ³›åŒ–ç¼ºé™·ï¼Œå¹¶å¼ºè°ƒäº†ç”Ÿæˆå¼RMçš„æ½œåŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2410.09893">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-3ac85868cfb658f0d48aadca0ad86a2d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4e46cf1254ec7b7699e124d343851061.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-9798766a6d4762ddb7098178f5d6f641.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3149b8e54d5c9314721ca6a9f4dba985.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1614c0a213e76819fae39577f5c9d994.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-cc90db5cb0643702db8335b95a9fa410.jpg" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="Assessing-SPARQL-capabilities-of-Large-Language-Models"><a href="#Assessing-SPARQL-capabilities-of-Large-Language-Models" class="headerlink" title="Assessing SPARQL capabilities of Large Language Models"></a>Assessing SPARQL capabilities of Large Language Models</h2><p><strong>Authors:Lars-Peter Meyer, Johannes Frey, Felix Brei, Natanael Arndt</strong></p>
<p>The integration of Large Language Models (LLMs) with Knowledge Graphs (KGs) offers significant synergistic potential for knowledge-driven applications. One possible integration is the interpretation and generation of formal languages, such as those used in the Semantic Web, with SPARQL being a core technology for accessing KGs. In this paper, we focus on measuring out-of-the box capabilities of LLMs to work with SPARQL and more specifically with SPARQL SELECT queries applying a quantitative approach.   We implemented various benchmarking tasks in the LLM-KG-Bench framework for automated execution and evaluation with several LLMs. The tasks assess capabilities along the dimensions of syntax, semantic read, semantic create, and the role of knowledge graph prompt inclusion.   With this new benchmarking tasks, we evaluated a selection of GPT, Gemini, and Claude models. Our findings indicate that working with SPARQL SELECT queries is still challenging for LLMs and heavily depends on the specific LLM as well as the complexity of the task. While fixing basic syntax errors seems to pose no problems for the best of the current LLMs evaluated, creating semantically correct SPARQL SELECT queries is difficult in several cases. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ä¸çŸ¥è¯†å›¾è°±ï¼ˆKGsï¼‰çš„èåˆä¸ºçŸ¥è¯†é©±åŠ¨çš„åº”ç”¨æä¾›äº†å·¨å¤§çš„ååŒæ½œåŠ›ã€‚ä¸€ç§å¯èƒ½çš„èåˆæ˜¯è§£é‡Šå’Œç”Ÿæˆå½¢å¼è¯­è¨€ï¼Œå¦‚è¯­ä¹‰ç½‘æ‰€ä½¿ç”¨çš„è¯­è¨€ï¼ŒSPARQLæ˜¯è®¿é—®çŸ¥è¯†å›¾è°±çš„æ ¸å¿ƒæŠ€æœ¯ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬ä¸“æ³¨äºæµ‹é‡LLMsä¸SPARQLçš„å³æ’å³ç”¨èƒ½åŠ›ï¼Œæ›´å…·ä½“åœ°è¯´ï¼Œæ˜¯åº”ç”¨å®šé‡æ–¹æ³•çš„SPARQL SELECTæŸ¥è¯¢ã€‚æˆ‘ä»¬åœ¨LLM-KG-Benchæ¡†æ¶ä¸­å®æ–½äº†å„ç§åŸºå‡†æµ‹è¯•ä»»åŠ¡ï¼Œç”¨äºè‡ªåŠ¨æ‰§è¡Œå’Œè¯„ä¼°å¤šä¸ªLLMsã€‚è¿™äº›ä»»åŠ¡è¯„ä¼°äº†è¯­æ³•ã€è¯­ä¹‰é˜…è¯»ã€è¯­ä¹‰åˆ›å»ºä»¥åŠçŸ¥è¯†å›¾è°±æç¤ºåŒ…å«çš„ä½œç”¨ç­‰æ–¹é¢çš„èƒ½åŠ›ã€‚é€šè¿‡è¿™é¡¹æ–°çš„åŸºå‡†æµ‹è¯•ä»»åŠ¡ï¼Œæˆ‘ä»¬è¯„ä¼°äº†ä¸€äº›GPTã€Geminiå’ŒClaudeæ¨¡å‹ã€‚æˆ‘ä»¬çš„ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œå¯¹äºLLMsæ¥è¯´ï¼Œå¤„ç†SPARQL SELECTæŸ¥è¯¢ä»ç„¶å…·æœ‰æŒ‘æˆ˜æ€§ï¼Œè¿™å¾ˆå¤§ç¨‹åº¦ä¸Šå–å†³äºç‰¹å®šçš„LLMä»¥åŠä»»åŠ¡çš„å¤æ‚æ€§ã€‚è™½ç„¶çº æ­£åŸºæœ¬è¯­æ³•é”™è¯¯ä¼¼ä¹å¯¹è¯„ä¼°çš„æœ€ä½³LLMsæ²¡æœ‰é—®é¢˜ï¼Œä½†åœ¨è®¸å¤šæƒ…å†µä¸‹ï¼Œç”Ÿæˆè¯­ä¹‰æ­£ç¡®çš„SPARQL SELECTæŸ¥è¯¢æ˜¯å¾ˆå›°éš¾çš„ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2409.05925v2">PDF</a> Peer reviewed and published at NLP4KGc @ Semantics 2024, see original   publication at <a target="_blank" rel="noopener" href="https://ceur-ws.org/Vol-3874/paper3.pdf">https://ceur-ws.org/Vol-3874/paper3.pdf</a> . Updated Metadata</p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¸çŸ¥è¯†å›¾è°±ï¼ˆKGï¼‰çš„èåˆä¸ºçŸ¥è¯†é©±åŠ¨çš„åº”ç”¨æä¾›äº†å·¨å¤§çš„ååŒæ½œåŠ›ã€‚æœ¬æ–‡å…³æ³¨LLMå¯¹SPARQLçš„å³å¸­èƒ½åŠ›è¿›è¡Œè¡¡é‡ï¼Œç‰¹åˆ«æ˜¯SPARQL SELECTæŸ¥è¯¢ã€‚é€šè¿‡å®šé‡æ–¹æ³•ï¼Œåœ¨LLM-KG-Benchæ¡†æ¶ä¸‹å®æ–½å¤šç§åŸºå‡†æµ‹è¯•ä»»åŠ¡ï¼Œå¯¹è¯­æ³•ã€è¯­ä¹‰é˜…è¯»ã€è¯­ä¹‰åˆ›å»ºä»¥åŠçŸ¥è¯†å›¾è°±æç¤ºçš„èå…¥è§’è‰²è¿›è¡Œè¯„ä¼°ã€‚è¯„ä¼°äº†GPTã€Geminiå’ŒClaudeç­‰æ¨¡å‹ã€‚ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œå¯¹äºSPARQL SELECTæŸ¥è¯¢ï¼ŒLLMä»é¢ä¸´æŒ‘æˆ˜ï¼Œå…¶è¡¨ç°å–å†³äºç‰¹å®šLLMåŠä»»åŠ¡çš„å¤æ‚æ€§ã€‚è™½ç„¶ä¿®æ­£åŸºæœ¬è¯­æ³•é”™è¯¯å¯¹æœ€ä½³LLMæ¥è¯´ä¼¼ä¹ä¸æ˜¯é—®é¢˜ï¼Œä½†åœ¨è®¸å¤šæƒ…å†µä¸‹ç”Ÿæˆè¯­ä¹‰æ­£ç¡®çš„SPARQL SELECTæŸ¥è¯¢ä»ç„¶å…·æœ‰æŒ‘æˆ˜æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLMä¸KGçš„é›†æˆå…·æœ‰ä¸ºçŸ¥è¯†é©±åŠ¨åº”ç”¨æä¾›é‡å¤§ååŒæ½œåŠ›çš„æ½œåŠ›ã€‚</li>
<li>åŸºå‡†æµ‹è¯•ä»»åŠ¡åœ¨è¯„ä¼°LLMä¸SPARQLäº’åŠ¨æ–¹é¢éå¸¸é‡è¦ã€‚</li>
<li>LLMåœ¨å¤„ç†SPARQL SELECTæŸ¥è¯¢æ—¶ä»é¢ä¸´æŒ‘æˆ˜ã€‚</li>
<li>LLMçš„è¡¨ç°å–å†³äºæ¨¡å‹çš„ç‰¹å®šå¤æ‚æ€§ä»¥åŠä»»åŠ¡çš„å¤æ‚æ€§ã€‚</li>
<li>ä¿®æ­£åŸºæœ¬è¯­æ³•é”™è¯¯å¯¹æœ€ä½³LLMæ¥è¯´ä¸æ˜¯ä¸»è¦é—®é¢˜ã€‚</li>
<li>ç”Ÿæˆè¯­ä¹‰æ­£ç¡®çš„SPARQL SELECTæŸ¥è¯¢åœ¨å¤šä¸ªæ¡ˆä¾‹ä¸­ä»å…·æœ‰æŒ‘æˆ˜æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2409.05925">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-91d142a9207023095c7fb9b509479872.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-29d3af07f3bd1c80ea7df227cb383fb9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d45b1ebb3822221d0ab146aa44b674fe.jpg" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="LLMs-Prompted-for-Graphs-Hallucinations-and-Generative-Capabilities"><a href="#LLMs-Prompted-for-Graphs-Hallucinations-and-Generative-Capabilities" class="headerlink" title="LLMs Prompted for Graphs: Hallucinations and Generative Capabilities"></a>LLMs Prompted for Graphs: Hallucinations and Generative Capabilities</h2><p><strong>Authors:Gurvan Richardeau, Samy Chali, Erwan Le Merrer, Camilla Penzo, Gilles Tredan</strong></p>
<p>Large Language Models (LLMs) are nowadays prompted for a wide variety of tasks. In this article, we investigate their ability in reciting and generating graphs. We first study the ability of LLMs to regurgitate well known graphs from the literature (e.g. Karate club or the graph atlas)4. Secondly, we question the generative capabilities of LLMs by asking for Erdos-Renyi random graphs. As opposed to the possibility that they could memorize some Erdos-Renyi graphs included in their scraped training set, this second investigation aims at studying a possible emergent property of LLMs. For both tasks, we propose a metric to assess their errors with the lens of hallucination (i.e. incorrect information returned as facts). We most notably find that the amplitude of graph hallucinations can characterize the superiority of some LLMs. Indeed, for the recitation task, we observe that graph hallucinations correlate with the Hallucination Leaderboard, a hallucination rank that leverages 10, 000 times more prompts to obtain its ranking. For the generation task, we find surprisingly good and reproducible results in most of LLMs. We believe this to constitute a starting point for more in-depth studies of this emergent capability and a challenging benchmark for their improvements. Altogether, these two aspects of LLMs capabilities bridge a gap between the network science and machine learning communities. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å¦‚ä»Šå·²è¢«åº”ç”¨äºå„ç§ä»»åŠ¡ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬è°ƒæŸ¥äº†å®ƒä»¬åœ¨èƒŒè¯µå’Œç”Ÿæˆå›¾å½¢æ–¹é¢çš„èƒ½åŠ›ã€‚æˆ‘ä»¬é¦–å…ˆç ”ç©¶LLMä»æ–‡çŒ®ä¸­å¤è¿°å·²çŸ¥å›¾å½¢çš„èƒ½åŠ›ï¼ˆä¾‹å¦‚ç©ºæ‰‹é“ä¿±ä¹éƒ¨æˆ–å›¾å½¢é›†ï¼‰ã€‚å…¶æ¬¡ï¼Œæˆ‘ä»¬é€šè¿‡è¦æ±‚ç”ŸæˆErdos-Renyiéšæœºå›¾å½¢æ¥è´¨ç–‘LLMçš„ç”Ÿæˆèƒ½åŠ›ã€‚ä¸å¯èƒ½è®°ä½ä¸€äº›Erdos-Renyiå›¾å½¢çš„è®­ç»ƒé›†ä¸åŒï¼Œç¬¬äºŒæ¬¡è°ƒæŸ¥æ—¨åœ¨ç ”ç©¶LLMçš„æ½œåœ¨æ–°å…´å±æ€§ã€‚å¯¹äºè¿™ä¸¤ä¸ªä»»åŠ¡ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªè¯„ä¼°é”™è¯¯çš„æŒ‡æ ‡ï¼Œé€šè¿‡å¹»è§‰çš„è§†è§’æ¥è¯„ä¼°ï¼ˆå³é”™è¯¯çš„ä¿¡æ¯è¢«å½“ä½œäº‹å®è¿”å›ï¼‰ã€‚æˆ‘ä»¬å°¤å…¶å‘ç°ï¼Œå›¾å½¢å¹»è§‰çš„å¹…åº¦å¯ä»¥åæ˜ æŸäº›LLMçš„ä¼˜åŠ¿ã€‚äº‹å®ä¸Šï¼Œåœ¨èƒŒè¯µä»»åŠ¡ä¸­ï¼Œæˆ‘ä»¬è§‚å¯Ÿåˆ°å›¾å½¢å¹»è§‰ä¸å¹»è§‰æ’è¡Œæ¦œç›¸å…³ï¼Œå¹»è§‰æ’è¡Œæ¦œåˆ©ç”¨æ›´å¤šçš„æç¤ºæ¥è·å¾—å…¶æ’åã€‚å¯¹äºç”Ÿæˆä»»åŠ¡ï¼Œæˆ‘ä»¬åœ¨å¤§å¤šæ•°LLMä¸­éƒ½å‘ç°äº†ä»¤äººæƒŠè®¶çš„è‰¯å¥½å’Œå¯é‡å¤çš„ç»“æœã€‚æˆ‘ä»¬ç›¸ä¿¡è¿™æ„æˆäº†å¯¹è¿™ä¸€æ–°å…´èƒ½åŠ›çš„æ·±å…¥ç ”ç©¶èµ·ç‚¹ï¼Œå¹¶ä¸ºæ”¹è¿›å®ƒä»¬æä¾›äº†å…·æœ‰æŒ‘æˆ˜æ€§çš„åŸºå‡†æµ‹è¯•ã€‚æ€»ä½“è€Œè¨€ï¼ŒLLMèƒ½åŠ›çš„è¿™ä¸¤ä¸ªæ–¹é¢åœ¨ç½‘ç»œç§‘å­¦å’Œæœºå™¨å­¦ä¹ ç¤¾åŒºä¹‹é—´æ¶èµ·äº†ä¸€åº§æ¡¥æ¢ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2409.00159v3">PDF</a> A preliminary version of this work appeared in the Complex Networks   2024 conference, under the title â€œLLMs hallucinate graphs too: a structural   perspectiveâ€</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ¢è®¨äº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨èƒŒè¯µå’Œç”Ÿæˆå›¾æ–¹é¢çš„èƒ½åŠ›ã€‚ç ”ç©¶å†…å®¹åŒ…æ‹¬å¯¹LLMså¤è¿°å·²çŸ¥å›¾çš„èƒ½åŠ›çš„è€ƒå¯Ÿï¼Œä»¥åŠå¯¹ç”ŸæˆErdos-Renyiéšæœºå›¾çš„èƒ½åŠ›çš„æ¢ç©¶ã€‚ç ”ç©¶å‘ç°ï¼Œå›¾å¹»è§‰çš„å¹…åº¦å¯ä»¥åæ˜ æŸäº›LLMsçš„ä¼˜è¶Šæ€§ï¼Œä¸”LLMsåœ¨ç”Ÿæˆä»»åŠ¡æ–¹é¢çš„è¡¨ç°è‰¯å¥½ä¸”å…·æœ‰å¯é‡å¤æ€§ã€‚è¿™äº›å‘ç°æœ‰åŠ©äºæ·±åŒ–å¯¹LLMsçš„ç†è§£ï¼Œä¸ºç½‘ç»œç§‘å­¦å’Œæœºå™¨å­¦ä¹ é¢†åŸŸçš„ç ”ç©¶äººå‘˜æä¾›äº†ä¸€ä¸ªå¯Œæœ‰æŒ‘æˆ˜æ€§çš„åŸºå‡†æµ‹è¯•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLMså…·å¤‡å¤è¿°å·²çŸ¥å›¾çš„èƒ½åŠ›ï¼Œå¦‚Karate clubæˆ–å›¾é›†ç­‰ã€‚</li>
<li>LLMsèƒ½å¤Ÿç”ŸæˆErdos-Renyiéšæœºå›¾ï¼Œè¿™æ˜¯å¯¹å…¶ç”Ÿæˆèƒ½åŠ›çš„æ¢ç©¶ã€‚</li>
<li>å›¾å¹»è§‰çš„å¹…åº¦å¯ä½œä¸ºè¯„ä¼°æŸäº›LLMsæ€§èƒ½çš„é‡è¦æŒ‡æ ‡ã€‚</li>
<li>åœ¨èƒŒè¯µä»»åŠ¡ä¸­ï¼Œå›¾å¹»è§‰ä¸å¹»è§‰é¢†å¯¼è€…æ¦œï¼ˆåˆ©ç”¨10,000æ¬¡æç¤ºè¿›è¡Œæ’åï¼‰å­˜åœ¨å…³è”ã€‚</li>
<li>åœ¨ç”Ÿæˆä»»åŠ¡æ–¹é¢ï¼ŒLLMsçš„è¡¨ç°å‡ºä¹æ„æ–™åœ°è‰¯å¥½ä¸”å…·æœ‰å¯é‡å¤æ€§ã€‚</li>
<li>LLMsçš„èƒ½åŠ›è¡¨ç°åœ¨èƒŒè¯µå’Œç”Ÿæˆä¸¤ä¸ªæ–¹é¢ï¼Œä¸ºç½‘ç»œç§‘å­¦å’Œæœºå™¨å­¦ä¹ é¢†åŸŸçš„ç ”ç©¶æä¾›äº†æ¡¥æ¢ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2409.00159">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-1b7869da115d81f7e68cb8531505acc0.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-f30b9d4dce65526ef1fdf486912a7cef.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9115677da52287590cb44b9e761d470e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-12c0aeb7cfe5cf05a6c6783305977dd2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-cc97eba67c6b632490e15fdfedadc68c.jpg" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1><h2 id="ORLM-A-Customizable-Framework-in-Training-Large-Models-for-Automated-Optimization-Modeling"><a href="#ORLM-A-Customizable-Framework-in-Training-Large-Models-for-Automated-Optimization-Modeling" class="headerlink" title="ORLM: A Customizable Framework in Training Large Models for Automated   Optimization Modeling"></a>ORLM: A Customizable Framework in Training Large Models for Automated   Optimization Modeling</h2><p><strong>Authors:Chenyu Huang, Zhengyang Tang, Shixi Hu, Ruoqing Jiang, Xin Zheng, Dongdong Ge, Benyou Wang, Zizhuo Wang</strong></p>
<p>Optimization modeling plays a critical role in the application of Operations Research (OR) tools to address real-world problems, yet they pose challenges and require extensive expertise from OR experts. With the advent of large language models (LLMs), new opportunities have emerged to streamline and automate such task. However, current research predominantly relies on closed-source LLMs such as GPT-4, along with extensive prompt engineering techniques. This reliance stems from the scarcity of high-quality training datasets for optimization modeling, resulting in elevated costs, prolonged processing times, and privacy concerns. To address these challenges, our work is the first to propose a viable path for training open-source LLMs that are capable of optimization modeling and developing solver codes, eventually leading to a superior ability for automating optimization modeling and solving. Particularly, we design the {\sc OR-Instruct}, a semi-automated data synthesis framework for optimization modeling that enables customizable enhancements for specific scenarios or model types. This work also introduces IndustryOR, the first industrial benchmark for evaluating LLMs in solving practical OR problems. We train several 7B-scale open-source LLMs using synthesized data (dubbed ORLMs{<a target="_blank" rel="noopener" href="https://github.com/Cardinal-Operations/ORLM%7D">https://github.com/Cardinal-Operations/ORLM}</a>), which exhibit significantly enhanced optimization modeling capabilities, achieving competitive performance across the NL4OPT, MAMO, and IndustryOR benchmarks. Additionally, our experiments highlight the potential of scaling law and reinforcement learning to further enhance the performance of ORLMs. The workflows and human-machine interaction paradigms of ORLMs in practical industrial applications are also discussed in the paper. </p>
<blockquote>
<p>ä¼˜åŒ–å»ºæ¨¡åœ¨è¿ç­¹å­¦å·¥å…·è§£å†³ç°å®ä¸–ç•Œé—®é¢˜ä¸­èµ·ç€å…³é”®ä½œç”¨ï¼Œä½†å®ƒä»¬å¸¦æ¥æŒ‘æˆ˜ï¼Œéœ€è¦è¿ç­¹å­¦ä¸“å®¶çš„æ·±åšä¸“ä¸šçŸ¥è¯†ã€‚éšç€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„å‡ºç°ï¼Œç®€åŒ–å¹¶è‡ªåŠ¨åŒ–è¿™ç±»ä»»åŠ¡çš„æ–°æœºä¼šå·²ç»äº§ç”Ÿã€‚ç„¶è€Œï¼Œç›®å‰çš„ç ”ç©¶ä¸»è¦ä¾èµ–äºè¯¸å¦‚GPT-4ç­‰å°é—­æºä»£ç çš„LLMï¼Œä»¥åŠå¹¿æ³›çš„æç¤ºå·¥ç¨‹æŠ€æœ¯ã€‚è¿™ç§ä¾èµ–æºäºä¼˜åŒ–å»ºæ¨¡é«˜è´¨é‡è®­ç»ƒæ•°æ®é›†çš„ç¨€ç¼ºæ€§ï¼Œå¯¼è‡´æˆæœ¬å¢åŠ ã€å¤„ç†æ—¶é—´å»¶é•¿å’Œéšç§æ‹…å¿§ã€‚</p>
</blockquote>
<p>æˆ‘ä»¬çš„å·¥ä½œæ˜¯ç¬¬ä¸€ä¸ªæå‡ºè®­ç»ƒèƒ½å¤Ÿè¿›è¡Œä¼˜åŒ–å»ºæ¨¡å’Œå¼€å‘æ±‚è§£å™¨ä»£ç çš„å¼€æºLLMçš„å¯è¡Œè·¯å¾„ï¼Œæœ€ç»ˆå®ç°å¯¹ä¼˜åŒ–å»ºæ¨¡å’Œæ±‚è§£çš„è‡ªåŠ¨åŒ–èƒ½åŠ›çš„å“è¶Šæå‡ã€‚ç‰¹åˆ«æ˜¯ï¼Œæˆ‘ä»¬è®¾è®¡äº†åŠè‡ªåŠ¨åŒ–æ•°æ®åˆæˆæ¡†æ¶{\sc OR-Instruct}ï¼Œç”¨äºä¼˜åŒ–å»ºæ¨¡ï¼Œå¯ä¸ºç‰¹å®šåœºæ™¯æˆ–æ¨¡å‹ç±»å‹æä¾›å¯å®šåˆ¶å¢å¼ºåŠŸèƒ½ã€‚è¿™é¡¹å·¥ä½œè¿˜å¼•å…¥äº†IndustryORï¼Œè¿™æ˜¯è¯„ä¼°LLMè§£å†³å®é™…è¿ç­¹é—®é¢˜çš„ç¬¬ä¸€ä¸ªå·¥ä¸šåŸºå‡†æµ‹è¯•ã€‚æˆ‘ä»¬ä½¿ç”¨åˆæˆæ•°æ®è®­ç»ƒäº†å‡ ä¸ª7Bçº§å¼€æºLLMï¼ˆç§°ä¸ºORLMï¼‰ï¼Œå±•ç°å‡ºæ˜¾è‘—å¢å¼ºçš„ä¼˜åŒ–å»ºæ¨¡èƒ½åŠ›ï¼Œåœ¨NL4OPTã€MAMOå’ŒIndustryORåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºç«äº‰åŠ›ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬çš„å®éªŒè¿˜çªå‡ºäº†æ‰©å±•å®šå¾‹å’Œå¼ºåŒ–å­¦ä¹ åœ¨è¿›ä¸€æ­¥æé«˜ORLMæ€§èƒ½æ–¹é¢çš„æ½œåŠ›ã€‚è®ºæ–‡è¿˜è®¨è®ºäº†ORLMåœ¨å®é™…å·¥ä¸šåº”ç”¨ä¸­çš„å·¥ä½œæµç¨‹å’Œäººæœºäº¤äº’èŒƒå¼ã€‚</p>
<p>æ³¨ï¼šæ–‡ä¸­ç½‘å€çš„ä¸­æ–‡ç¿»è¯‘å¯æŒ‰ç…§å®é™…æƒ…å†µè¿›è¡Œå¤„ç†ï¼Œæœ¬æ–‡æš‚ç¿»è¯‘ä¸ºâ€œæˆ‘ä»¬åœ¨GitHubä¸Šå¼€è®¾çš„é¡¹ç›®åº“â€˜ORLMâ€™â€ã€‚</p>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2405.17743v5">PDF</a> accepted by Operations Research</p>
<p><strong>Summary</strong><br>     éšç€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„å‡ºç°ï¼Œæ“ä½œç ”ç©¶ï¼ˆORï¼‰å·¥å…·çš„ä¼˜åŒ–å»ºæ¨¡é¢ä¸´æ–°çš„æŒ‘æˆ˜å’Œæœºé‡ã€‚å½“å‰ç ”ç©¶ä¸»è¦ä¾èµ–å°é—­æºä»£ç çš„LLMå’Œæç¤ºå·¥ç¨‹æŠ€æœ¯ï¼Œè¿™å¢åŠ äº†æˆæœ¬ã€å»¶é•¿äº†å¤„ç†æ—¶é—´å¹¶å¼•å‘äº†éšç§æ‹…å¿§ã€‚æœ¬ç ”ç©¶é¦–æ¬¡æå‡ºè®­ç»ƒèƒ½å¤Ÿä¼˜åŒ–å»ºæ¨¡å’Œç¼–å†™æ±‚è§£ä»£ç çš„å¼€æºLLMçš„è·¯å¾„ï¼Œå¹¶è®¾è®¡äº†ä¸€ä¸ªåŠè‡ªåŠ¨åŒ–çš„æ•°æ®åˆæˆæ¡†æ¶{\sc OR-Instruct}ï¼Œç”¨äºä¼˜åŒ–å»ºæ¨¡ï¼Œä¸ºç‰¹å®šåœºæ™¯æˆ–æ¨¡å‹ç±»å‹æä¾›å¯å®šåˆ¶çš„å¢å¼ºåŠŸèƒ½ã€‚æ­¤å¤–ï¼Œæœ¬ç ”ç©¶å¼•å…¥äº†IndustryORä½œä¸ºè¯„ä¼°LLMè§£å†³å®é™…ORé—®é¢˜çš„å·¥ä¸šåŸºå‡†ã€‚è®­ç»ƒå‡ºçš„å¼€æºLLMï¼ˆç§°ä¸ºORLMï¼‰å±•ç°å‡ºå¼ºå¤§çš„ä¼˜åŒ–å»ºæ¨¡èƒ½åŠ›ï¼Œåœ¨NL4OPTã€MAMOå’ŒIndustryORåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ä¼˜åŒ–å»ºæ¨¡åœ¨è§£å†³ç°å®ä¸–ç•Œé—®é¢˜ä¸­æ‰®æ¼”å…³é”®è§’è‰²ï¼Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¸ºæ­¤å¸¦æ¥äº†æ–°çš„æœºé‡ã€‚</li>
<li>å½“å‰ç ”ç©¶ä¸»è¦ä¾èµ–å°é—­æºä»£ç çš„LLMå’Œæç¤ºå·¥ç¨‹æŠ€æœ¯ï¼Œå­˜åœ¨æˆæœ¬é«˜ã€å¤„ç†æ—¶é—´é•¿å’Œéšç§æ‹…å¿§ç­‰é—®é¢˜ã€‚</li>
<li>æœ¬ç ”ç©¶é¦–æ¬¡æå‡ºè®­ç»ƒèƒ½å¤Ÿä¼˜åŒ–å»ºæ¨¡å’Œç¼–å†™æ±‚è§£ä»£ç çš„å¼€æºLLMçš„è·¯å¾„ã€‚</li>
<li>è®¾è®¡äº†åŠè‡ªåŠ¨åŒ–çš„æ•°æ®åˆæˆæ¡†æ¶{\sc OR-Instruct}ï¼Œç”¨äºä¼˜åŒ–å»ºæ¨¡ï¼Œæ”¯æŒç‰¹å®šåœºæ™¯æˆ–æ¨¡å‹ç±»å‹çš„å®šåˆ¶å¢å¼ºã€‚</li>
<li>å¼•å…¥äº†IndustryORä½œä¸ºè¯„ä¼°LLMè§£å†³å®é™…æ“ä½œç ”ç©¶é—®é¢˜çš„å·¥ä¸šåŸºå‡†ã€‚</li>
<li>è®­ç»ƒå‡ºçš„å¼€æºLLMï¼ˆORLMï¼‰å±•ç°å‡ºå¼ºå¤§çš„ä¼˜åŒ–å»ºæ¨¡èƒ½åŠ›ï¼Œåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2405.17743">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-af8c811599b66875385cb5d1380d44ca.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-cc74a1604d07e70e5f39fb3ecff807e4.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e3e0fc28585fcb7c069dbc9113ed9a1d.jpg" align="middle">
</details>


<h1 id="-19"><a href="#-19" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-04-08/LLM/">https://kedreamix.github.io/Talk2Paper/Paper/2025-04-08/LLM/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/LLM/">
                                    <span class="chip bg-color">LLM</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-04-08/Agent/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-bd176178c3c765d01f264ab8c390a604.jpg" class="responsive-img" alt="Agent">
                        
                        <span class="card-title">Agent</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Agent æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-04-08  APIGen-MT Agentic Pipeline for Multi-Turn Data Generation via Simulated   Agent-Human Interplay
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-04-08
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Agent/" class="post-category">
                                    Agent
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Agent/">
                        <span class="chip bg-color">Agent</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-04-08/R1_Reasoning/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-00d616db923791adfaaebfa4006122cc.jpg" class="responsive-img" alt="R1_Reasoning">
                        
                        <span class="card-title">R1_Reasoning</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            R1_Reasoning æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-04-08  MME-Unify A Comprehensive Benchmark for Unified Multimodal   Understanding and Generation Models
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-04-08
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/R1-Reasoning/" class="post-category">
                                    R1_Reasoning
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/R1-Reasoning/">
                        <span class="chip bg-color">R1_Reasoning</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">23542.1k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
