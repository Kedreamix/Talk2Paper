<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="R1_Reasoning">
    <meta name="description" content="R1_Reasoning æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-04-20  Memorization vs. Reasoning Updating LLMs with New Knowledge">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>R1_Reasoning | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-52b875081e85d964ca333fe9ab756775.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">R1_Reasoning</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/R1-Reasoning/">
                                <span class="chip bg-color">R1_Reasoning</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/R1-Reasoning/" class="post-category">
                                R1_Reasoning
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-04-20
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-05-14
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    11.8k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    48 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-04-20-æ›´æ–°"><a href="#2025-04-20-æ›´æ–°" class="headerlink" title="2025-04-20 æ›´æ–°"></a>2025-04-20 æ›´æ–°</h1><h2 id="Memorization-vs-Reasoning-Updating-LLMs-with-New-Knowledge"><a href="#Memorization-vs-Reasoning-Updating-LLMs-with-New-Knowledge" class="headerlink" title="Memorization vs. Reasoning: Updating LLMs with New Knowledge"></a>Memorization vs. Reasoning: Updating LLMs with New Knowledge</h2><p><strong>Authors:Aochong Oliver Li, Tanya Goyal</strong></p>
<p>Large language models (LLMs) encode vast amounts of pre-trained knowledge in their parameters, but updating them as real-world information evolves remains a challenge. Existing methodologies and benchmarks primarily target entity substitutions, failing to capture the full breadth of complex real-world dynamics. In this paper, we introduce Knowledge Update Playground (KUP), an automatic pipeline for simulating realistic knowledge updates reflected in an evidence corpora. KUPâ€™s evaluation framework includes direct and indirect probes to both test memorization of updated facts and reasoning over them, for any update learning methods. Next, we present a lightweight method called memory conditioned training (MCT), which conditions tokens in the update corpus on self-generated â€œmemoryâ€ tokens during training. Our strategy encourages LLMs to surface and reason over newly memorized knowledge at inference. Our results on two strong LLMs show that (1) KUP benchmark is highly challenging, with the best CPT models achieving $&lt;2%$ in indirect probing setting (reasoning) and (2) MCT training significantly outperforms prior continued pre-training (CPT) baselines, improving direct probing (memorization) results by up to $25.4%$. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨å…¶å‚æ•°ä¸­ç¼–ç äº†å¤§é‡çš„é¢„è®­ç»ƒçŸ¥è¯†ï¼Œä½†éšç€ç°å®ä¸–ç•Œä¿¡æ¯çš„æ¼”å˜ï¼Œå¦‚ä½•æ›´æ–°è¿™äº›çŸ¥è¯†ä»ç„¶æ˜¯ä¸€ä¸ªæŒ‘æˆ˜ã€‚ç°æœ‰çš„æ–¹æ³•å’ŒåŸºå‡†æµ‹è¯•ä¸»è¦å…³æ³¨å®ä½“æ›¿æ¢ï¼Œæ— æ³•æ•æ‰ç°å®ä¸–ç•ŒåŠ¨æ€çš„å…¨é¢å¹¿åº¦ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†çŸ¥è¯†æ›´æ–°æ¸¸ä¹åœºï¼ˆKUPï¼‰ï¼Œè¿™æ˜¯ä¸€ä¸ªè‡ªåŠ¨ç®¡é“ï¼Œç”¨äºæ¨¡æ‹Ÿåæ˜ åœ¨è¯æ®è¯­æ–™åº“ä¸­çš„ç°å®çŸ¥è¯†æ›´æ–°ã€‚KUPçš„è¯„ä¼°æ¡†æ¶åŒ…æ‹¬ç›´æ¥å’Œé—´æ¥æ¢é’ˆï¼Œç”¨äºæµ‹è¯•ä»»ä½•æ›´æ–°å­¦ä¹ æ–¹æ³•ä¸­å¯¹æ›´æ–°äº‹å®çš„è®°å¿†å’Œæ¨ç†ã€‚æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§è½»é‡çº§çš„æ–¹æ³•ï¼Œç§°ä¸ºè®°å¿†æ¡ä»¶è®­ç»ƒï¼ˆMCTï¼‰ï¼Œè¯¥æ–¹æ³•åœ¨è®­ç»ƒæœŸé—´å°†æ›´æ–°è¯­æ–™åº“ä¸­çš„ä»¤ç‰Œè®¾ç½®ä¸ºè‡ªæˆ‘ç”Ÿæˆçš„â€œè®°å¿†â€ä»¤ç‰Œçš„æ¡ä»¶ã€‚æˆ‘ä»¬çš„ç­–ç•¥é¼“åŠ±å¤§å‹è¯­è¨€æ¨¡å‹åœ¨æ¨ç†æ—¶å±•ç°å¹¶æ¨ç†æ–°è®°å¿†çš„çŸ¥è¯†ã€‚æˆ‘ä»¬åœ¨ä¸¤ä¸ªå¼ºå¤§çš„å¤§å‹è¯­è¨€æ¨¡å‹ä¸Šçš„ç»“æœè¡¨æ˜ï¼Œï¼ˆ1ï¼‰KUPåŸºå‡†æµ‹è¯•æå…·æŒ‘æˆ˜æ€§ï¼Œæœ€å¥½çš„CPTæ¨¡å‹åœ¨é—´æ¥æ¢æµ‹è®¾ç½®ï¼ˆæ¨ç†ï¼‰ä¸­è¾¾åˆ°&lt;2%ï¼›ï¼ˆ2ï¼‰MCTè®­ç»ƒæ˜¾è‘—ä¼˜äºå…ˆå‰çš„æŒç»­é¢„è®­ç»ƒï¼ˆCPTï¼‰åŸºçº¿ï¼Œç›´æ¥æ¢æµ‹ï¼ˆè®°å¿†ï¼‰ç»“æœæœ€å¤šæé«˜äº†25.4%ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.12523v1">PDF</a> 9 pages, 3 figures</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†çŸ¥è¯†æ›´æ–°æ¸¸ä¹åœºï¼ˆKUPï¼‰è¿™ä¸€è‡ªåŠ¨ç®¡é“ï¼Œç”¨äºæ¨¡æ‹Ÿåæ˜ äºè¯æ®è¯­æ–™åº“ä¸­çš„ç°å®çŸ¥è¯†æ›´æ–°ã€‚KUPè¯„ä¼°æ¡†æ¶åŒ…æ‹¬ç›´æ¥å’Œé—´æ¥æ¢é’ˆï¼Œä»¥æµ‹è¯•æ›´æ–°äº‹å®çš„è®°å¿†å’Œå¯¹å…¶çš„æ¨ç†ã€‚æ­¤å¤–ï¼Œæå‡ºäº†ä¸€ç§è½»é‡çº§æ–¹æ³•â€”â€”è®°å¿†æ¡ä»¶è®­ç»ƒï¼ˆMCTï¼‰ï¼Œåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­å°†æ›´æ–°è¯­æ–™åº“ä¸­çš„ä»¤ç‰Œç½®äºè‡ªæˆ‘ç”Ÿæˆçš„â€œè®°å¿†â€ä»¤ç‰Œä¸Šã€‚ç­–ç•¥é¼“åŠ±å¤§å‹è¯­è¨€æ¨¡å‹åœ¨æ¨ç†æ—¶å”¤èµ·å¹¶æ¨ç†æ–°è®°å¿†çš„çŸ¥è¯†ã€‚åœ¨å¼ºå¤§çš„å¤§å‹è¯­è¨€æ¨¡å‹ä¸Šçš„ç»“æœè¡¨æ˜ï¼ŒKUPåŸºå‡†æµ‹è¯•å…·æœ‰æŒ‘æˆ˜æ€§ï¼Œè€ŒMCTè®­ç»ƒæ˜¾è‘—ä¼˜äºå…ˆå‰çš„æŒç»­é¢„è®­ç»ƒï¼ˆCPTï¼‰åŸºå‡†æµ‹è¯•ï¼Œç›´æ¥æ¢æµ‹ï¼ˆè®°å¿†ï¼‰ç»“æœæœ€å¤šå¯æé«˜25.4%ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>çŸ¥è¯†æ›´æ–°æ¸¸ä¹åœºï¼ˆKUPï¼‰æ˜¯ä¸€ä¸ªè‡ªåŠ¨ç®¡é“ï¼Œæ—¨åœ¨æ¨¡æ‹Ÿç°å®çŸ¥è¯†æ›´æ–°å¹¶åœ¨è¯æ®è¯­æ–™åº“ä¸­åæ˜ è¿™äº›æ›´æ–°ã€‚</li>
<li>KUPè¯„ä¼°æ¡†æ¶é€šè¿‡ç›´æ¥å’Œé—´æ¥æ¢é’ˆæµ‹è¯•å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å¯¹æ›´æ–°çŸ¥è¯†çš„è®°å¿†å’Œæ¨ç†èƒ½åŠ›ã€‚</li>
<li>ç°æœ‰æ–¹æ³•ä¸»è¦å…³æ³¨å®ä½“æ›¿æ¢ï¼Œæ— æ³•æ•æ‰å¤æ‚ç°å®åŠ¨æ€çš„å…¨è²Œã€‚</li>
<li>å¼•å…¥äº†ä¸€ç§è½»é‡çº§æ–¹æ³•â€”â€”è®°å¿†æ¡ä»¶è®­ç»ƒï¼ˆMCTï¼‰ï¼Œåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ä½¿ç”¨è‡ªæˆ‘ç”Ÿæˆçš„â€œè®°å¿†â€ä»¤ç‰Œã€‚<br>5.MCTé¼“åŠ±LLMåœ¨æ¨ç†æ—¶åˆ©ç”¨æ–°è®°å¿†çš„çŸ¥è¯†ã€‚</li>
<li>åœ¨å¼ºå¤§çš„LLMä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼ŒKUPåŸºå‡†æµ‹è¯•å…·æœ‰æŒ‘æˆ˜æ€§ï¼Œé—´æ¥æ¢é’ˆè®¾ç½®ä¸­çš„æœ€ä½³CPTæ¨¡å‹ä»…è¾¾åˆ°&lt;2%ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.12523">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-7bb226a1ebc102c2eab31c9d4af6972b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e43aaad51b455d13ff8be1f38f70f37e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-591c8d532ba52a91aa5b4c5d14d5b1f6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2663941ad7ed579ad97bfd69153eb1fe.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Robust-Reinforcement-Learning-from-Human-Feedback-for-Large-Language-Models-Fine-Tuning"><a href="#Robust-Reinforcement-Learning-from-Human-Feedback-for-Large-Language-Models-Fine-Tuning" class="headerlink" title="Robust Reinforcement Learning from Human Feedback for Large Language   Models Fine-Tuning"></a>Robust Reinforcement Learning from Human Feedback for Large Language   Models Fine-Tuning</h2><p><strong>Authors:Kai Ye, Hongyi Zhou, Jin Zhu, Francesco Quinzan, Chengchung Shi</strong></p>
<p>Reinforcement learning from human feedback (RLHF) has emerged as a key technique for aligning the output of large language models (LLMs) with human preferences. To learn the reward function, most existing RLHF algorithms use the Bradley-Terry model, which relies on assumptions about human preferences that may not reflect the complexity and variability of real-world judgments. In this paper, we propose a robust algorithm to enhance the performance of existing approaches under such reward model misspecifications. Theoretically, our algorithm reduces the variance of reward and policy estimators, leading to improved regret bounds. Empirical evaluations on LLM benchmark datasets demonstrate that the proposed algorithm consistently outperforms existing methods, with 77-81% of responses being favored over baselines on the Anthropic Helpful and Harmless dataset. </p>
<blockquote>
<p>å¼ºåŒ–å­¦ä¹ ä»äººç±»åé¦ˆï¼ˆRLHFï¼‰å·²ç»æˆä¸ºå°†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„è¾“å‡ºä¸äººç±»åå¥½å¯¹é½çš„å…³é”®æŠ€æœ¯ã€‚ä¸ºäº†å­¦ä¹ å¥–åŠ±å‡½æ•°ï¼Œå¤§å¤šæ•°ç°æœ‰çš„RLHFç®—æ³•ä½¿ç”¨Bradley-Terryæ¨¡å‹ï¼Œè¯¥æ¨¡å‹ä¾èµ–äºå¯èƒ½æ— æ³•åæ˜ ç°å®ä¸–ç•Œåˆ¤æ–­å¤æ‚æ€§å’Œå¯å˜æ€§çš„äººç±»åå¥½å‡è®¾ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ç¨³å¥çš„ç®—æ³•ï¼Œä»¥æé«˜åœ¨è¿™ç§å¥–åŠ±æ¨¡å‹é”™è¯¯æŒ‡å®šæƒ…å†µä¸‹ç°æœ‰æ–¹æ³•çš„æ€§èƒ½ã€‚ä»ç†è®ºä¸Šè®²ï¼Œæˆ‘ä»¬çš„ç®—æ³•é™ä½äº†å¥–åŠ±å’Œæ”¿ç­–ä¼°ç®—å™¨çš„æ–¹å·®ï¼Œä»è€Œæé«˜äº†åæ‚”ç•Œã€‚åœ¨LLMåŸºå‡†æ•°æ®é›†ä¸Šçš„ç»éªŒè¯„ä¼°è¡¨æ˜ï¼Œæ‰€æç®—æ³•å§‹ç»ˆä¼˜äºç°æœ‰æ–¹æ³•ï¼Œåœ¨Anthropicæœ‰ç›Šå’Œæ— å®³æ•°æ®é›†ä¸Šï¼Œæœ‰7 ä¿®ä¸ç ”ç©¶çš„å…³æ³¨åº¦æŠ€æœ¯åˆ™è¶Šæ¥è¶Šå—åˆ°ä¸šç•Œçš„å…³æ³¨å’Œç ”ç©¶è€…çš„é‡è§†ã€‚è¿™ä¸€æŠ€æœ¯çš„å‡ºç°ä½¿å¾—æœºå™¨å­¦ä¹ æ¨¡å‹èƒ½å¤Ÿæ›´å¥½åœ°ç†è§£äººç±»ç”¨æˆ·çš„éœ€æ±‚å’Œæ„å›¾ï¼Œä»è€Œæ›´å¥½åœ°æ»¡è¶³ç”¨æˆ·çš„éœ€æ±‚å¹¶æä¾›æ›´ä¸ªæ€§åŒ–çš„æœåŠ¡ã€‚ä¾‹å¦‚ï¼Œéšç€è¿™ä¸€æŠ€æœ¯çš„å‘å±•ï¼Œè¯­éŸ³åŠ©æ‰‹èƒ½å¤Ÿæ›´å‡†ç¡®åœ°ç†è§£ç”¨æˆ·çš„è¯­éŸ³æŒ‡ä»¤å¹¶æ‰§è¡Œç›¸åº”çš„æ“ä½œï¼›æ™ºèƒ½å®¢æœæœºå™¨äººèƒ½å¤Ÿæ›´å¥½åœ°ç†è§£ç”¨æˆ·çš„é—®é¢˜å¹¶æä¾›æ›´ç²¾å‡†çš„è§£ç­”å’Œå¸®åŠ©ã€‚åœ¨è‡ªç„¶è¯­è¨€å¤„ç†é¢†åŸŸçš„åº”ç”¨ä¹‹å¤–ï¼Œè¿™ä¸€æŠ€æœ¯ä¹Ÿåœ¨å…¶ä»–é¢†åŸŸå¾—åˆ°äº†å¹¿æ³›çš„åº”ç”¨ã€‚ä¾‹å¦‚ï¼Œåœ¨é‡‘èé¢†åŸŸï¼ŒåŸºäºå¼ºåŒ–å­¦ä¹ çš„æ™ºèƒ½äº¤æ˜“ç³»ç»Ÿå¯ä»¥æ ¹æ®å¸‚åœºæ•°æ®å’Œç”¨æˆ·åå¥½è¿›è¡Œè‡ªåŠ¨äº¤æ˜“å†³ç­–ï¼›åœ¨åŒ»ç–—é¢†åŸŸï¼Œå¼ºåŒ–å­¦ä¹ ä¹Ÿè¢«åº”ç”¨äºè¯Šæ–­å’Œç–¾ç—…ç®¡ç†ç­‰æ–¹é¢ã€‚æ€»ä¹‹ï¼Œéšç€æœºå™¨å­¦ä¹ é¢†åŸŸä¸­å¯¹å¼ºåŒ–å­¦ä¹ ä¸äººç±»åé¦ˆç›¸ç»“åˆçš„æŠ€æœ¯åº”ç”¨çš„ä¸æ–­æ·±åŒ–å’Œæ‰©å±•ï¼Œè¿™å°†ä¸ºæœªæ¥å¸¦æ¥æ— é™å¯èƒ½çš„æ–°åœºæ™¯å’Œæ–°æŠ€æœ¯é©å‘½ã€‚ç›®å‰æœºå™¨å­¦ä¹ æ¨¡å‹å¦‚GPTç³»åˆ—è™½ç„¶å·²ç»æœ‰äº†å¾ˆå¥½çš„æˆç»©ä½†ä»æœ‰å¾ˆå¤šé—®é¢˜å€¼å¾—æ¢è®¨ã€‚é€šè¿‡æ·±å…¥ç ”ç©¶å¼ºåŒ–å­¦ä¹ ä»äººç±»åé¦ˆçš„æŠ€æœ¯å¹¶ä¸å…¶ä»–æŠ€æœ¯ç›¸ç»“åˆç›¸ä¿¡æœªæ¥ä¼šæ¶Œç°å‡ºæ›´å¤šæœ‰è¶£è€Œå®ç”¨çš„åº”ç”¨åœºæ™¯æ¨åŠ¨æŠ€æœ¯çš„è¿›æ­¥å’Œå‘å±•ã€‚æˆ‘ä»¬ç›¸ä¿¡æœªæ¥åŸºäºå¼ºåŒ–å­¦ä¹ çš„è‡ªé€‚åº”æ™ºèƒ½ç³»ç»Ÿå°†æˆä¸ºäººå·¥æ™ºèƒ½é¢†åŸŸçš„é‡è¦å‘å±•æ–¹å‘ä¹‹ä¸€ä¸ºäººç±»å¸¦æ¥æ›´åŠ æ™ºèƒ½ä¾¿æ·çš„ç”Ÿæ´»ä½“éªŒã€‚æˆ‘ä»¬ç›¸ä¿¡è¿™ä¸€é¢†åŸŸçš„ç ”ç©¶å°†ç»§ç»­æ·±å…¥å¹¶å–å¾—æ›´å¤šçš„çªç ´æ€§è¿›å±•ä»è€Œä¸æ–­æå‡äººç±»ç¤¾ä¼šçš„æ™ºèƒ½åŒ–æ°´å¹³å¹¶å®ç°ç§‘æŠ€çš„å¯æŒç»­å‘å±•ã€‚ï¼Œç›¸æ¯”åŸºçº¿æœ‰å…«ä¿®çš„æ”¹è¿›æˆæœæ˜¾ç¤ºå‡ºç®—æ³•çš„é«˜æ•ˆç‡ä¸ä¼˜è¶Šæ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.03784v3">PDF</a> </p>
<p><strong>Summary</strong>ï¼šå¼ºåŒ–å­¦ä¹ ä»äººç±»åé¦ˆï¼ˆRLHFï¼‰å·²æˆä¸ºå°†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„è¾“å‡ºä¸äººç±»åå¥½å¯¹é½çš„å…³é”®æŠ€æœ¯ã€‚å¤§å¤šæ•°ç°æœ‰çš„RLHFç®—æ³•ä½¿ç”¨å¸ƒæ‹‰å¾·åˆ©-ç‰¹é‡Œæ¨¡å‹æ¥å­¦ä¹ å¥–åŠ±å‡½æ•°ï¼Œè¿™ä¾èµ–äºå¯èƒ½æ— æ³•åæ˜ ç°å®ä¸–ç•Œä¸­åˆ¤æ–­å’Œå¤æ‚æ€§çš„å‡è®¾ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§ç¨³å¥çš„ç®—æ³•ï¼Œä»¥æé«˜åœ¨å¥–åŠ±æ¨¡å‹è¯¯å·®ä¸‹çš„ç°æœ‰æ–¹æ³•æ€§èƒ½ã€‚ç†è®ºä¸Šï¼Œè¯¥ç®—æ³•é™ä½äº†å¥–åŠ±å’Œæ”¿ç­–ä¼°è®¡é‡çš„æ–¹å·®ï¼Œæé«˜äº†åæ‚”ç•Œã€‚åœ¨LLMåŸºå‡†æ•°æ®é›†ä¸Šçš„ç»éªŒè¯„ä¼°è¡¨æ˜ï¼Œè¯¥ç®—æ³•å§‹ç»ˆä¼˜äºç°æœ‰æ–¹æ³•ï¼Œåœ¨Anthropicæœ‰ç›Šå’Œæ— å®³æ•°æ®é›†ä¸Šï¼Œ77-81%çš„å“åº”ä¼˜äºåŸºçº¿ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>å¼ºåŒ–å­¦ä¹ ä»äººç±»åé¦ˆï¼ˆRLHFï¼‰æŠ€æœ¯ç”¨äºå¯¹é½å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è¾“å‡ºä¸äººç±»åå¥½ã€‚</li>
<li>ç°æœ‰RLHFç®—æ³•ä¸»è¦ä½¿ç”¨å¸ƒæ‹‰å¾·åˆ©-ç‰¹é‡Œæ¨¡å‹ï¼Œå­˜åœ¨å¯¹äººç±»åå¥½å‡è®¾çš„å±€é™æ€§ã€‚</li>
<li>æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„ç¨³å¥ç®—æ³•ï¼Œæ—¨åœ¨æé«˜åœ¨å¥–åŠ±æ¨¡å‹è¯¯å·®ä¸‹çš„ç°æœ‰æ–¹æ³•æ€§èƒ½ã€‚</li>
<li>æ–°ç®—æ³•ç†è®ºä¸Šé™ä½äº†å¥–åŠ±å’Œæ”¿ç­–ä¼°è®¡é‡çš„æ–¹å·®ï¼Œæé«˜äº†åæ‚”ç•Œã€‚</li>
<li>å®è¯è¯„ä¼°è¡¨æ˜ï¼Œæ–°ç®—æ³•åœ¨LLMåŸºå‡†æ•°æ®é›†ä¸Šè¡¨ç°ä¼˜äºç°æœ‰æ–¹æ³•ã€‚</li>
<li>åœ¨Anthropicæœ‰ç›Šå’Œæ— å®³æ•°æ®é›†ä¸Šï¼Œæ–°ç®—æ³•çš„å“åº”ä¼˜äºåŸºçº¿ï¼Œè¾¾åˆ°77-81%ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.03784">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-8b003af875b42231da6af5a6acb615ca.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-7f9b51ef57f8058197bf40683aa875f1.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-cce2ff1466f9e978a0ab1daaf67a66a3.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="OnRL-RAG-Real-Time-Personalized-Mental-Health-Dialogue-System"><a href="#OnRL-RAG-Real-Time-Personalized-Mental-Health-Dialogue-System" class="headerlink" title="OnRL-RAG: Real-Time Personalized Mental Health Dialogue System"></a>OnRL-RAG: Real-Time Personalized Mental Health Dialogue System</h2><p><strong>Authors:Ahsan Bilal, Beiyu Lin</strong></p>
<p>Large language models (LLMs) have been widely used for various tasks and applications. However, LLMs and fine-tuning are limited to the pre-trained data. For example, ChatGPTâ€™s world knowledge until 2021 can be outdated or inaccurate. To enhance the capabilities of LLMs, Retrieval-Augmented Generation (RAG), is proposed to augment LLMs with additional, new, latest details and information to LLMs. While RAG offers the correct information, it may not best present it, especially to different population groups with personalizations. Reinforcement Learning from Human Feedback (RLHF) adapts to user needs by aligning model responses with human preference through feedback loops. In real-life applications, such as mental health problems, a dynamic and feedback-based model would continuously adapt to new information and offer personalized assistance due to complex factors fluctuating in a daily environment. Thus, we propose an Online Reinforcement Learning-based Retrieval-Augmented Generation (OnRL-RAG) system to detect and personalize the responding systems to mental health problems, such as stress, anxiety, and depression. We use an open-source dataset collected from 2028 College Students with 28 survey questions for each student to demonstrate the performance of our proposed system with the existing systems. Our system achieves superior performance compared to standard RAG and simple LLM via GPT-4o, GPT-4o-mini, Gemini-1.5, and GPT-3.5. This work would open up the possibilities of real-life applications of LLMs for personalized services in the everyday environment. The results will also help researchers in the fields of sociology, psychology, and neuroscience to align their theories more closely with the actual human daily environment. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å·²å¹¿æ³›åº”ç”¨äºå„ç§ä»»åŠ¡å’Œåº”ç”¨ã€‚ç„¶è€Œï¼ŒLLMå’Œå¾®è°ƒéƒ½å—é™äºé¢„è®­ç»ƒæ•°æ®ã€‚ä¾‹å¦‚ï¼ŒChatGPTæˆªè‡³2021å¹´çš„ä¸–ç•ŒçŸ¥è¯†å¯èƒ½ä¼šè¿‡æ—¶æˆ–ä¸å‡†ç¡®ã€‚ä¸ºäº†å¢å¼ºLLMçš„èƒ½åŠ›ï¼Œæå‡ºäº†æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰æ¥å‘LLMæ·»åŠ é¢å¤–ã€æœ€æ–°ã€æœ€æ–°çš„ç»†èŠ‚å’Œä¿¡æ¯ã€‚è™½ç„¶RAGæä¾›äº†æ­£ç¡®çš„ä¿¡æ¯ï¼Œä½†å®ƒå¯èƒ½æ— æ³•æœ€å¥½åœ°å‘ˆç°å®ƒï¼Œå°¤å…¶æ˜¯å¯¹äºå…·æœ‰ä¸ªæ€§åŒ–çš„ä¸åŒäººç¾¤ã€‚å¼ºåŒ–å­¦ä¹ ä»äººç±»åé¦ˆï¼ˆRLHFï¼‰é€šè¿‡åé¦ˆå¾ªç¯ä½¿æ¨¡å‹å“åº”ä¸äººç±»åå¥½ç›¸é€‚åº”ï¼Œä»è€Œé€‚åº”ç”¨æˆ·éœ€æ±‚ã€‚åœ¨ç°å®ç”Ÿæ´»åº”ç”¨ï¼Œå¦‚å¿ƒç†å¥åº·é—®é¢˜ä¸­ï¼Œä¸€ä¸ªåŠ¨æ€ä¸”åŸºäºåé¦ˆçš„æ¨¡å‹å°†ä¸æ–­é€‚åº”æ–°ä¿¡æ¯ï¼Œå¹¶æä¾›ä¸ªæ€§åŒ–çš„å¸®åŠ©ï¼Œè¿™æ˜¯ç”±äºæ—¥å¸¸ç¯å¢ƒä¸­å¤æ‚å› ç´ çš„æ³¢åŠ¨ã€‚å› æ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†åŸºäºåœ¨çº¿å¼ºåŒ–å­¦ä¹ çš„æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆOnRL-RAGï¼‰ç³»ç»Ÿï¼Œç”¨äºæ£€æµ‹å’Œä¸ªæ€§åŒ–åº”å¯¹å¿ƒç†å¥åº·é—®é¢˜ï¼Œå¦‚å‹åŠ›ã€ç„¦è™‘å’ŒæŠ‘éƒã€‚æˆ‘ä»¬ä½¿ç”¨ä»2028åå¤§å­¦ç”Ÿæ”¶é›†çš„å¼€æºæ•°æ®é›†è¿›è¡Œæ¼”ç¤ºï¼Œæ¯ä¸ªå­¦ç”Ÿå›ç­”äº†28ä¸ªé—®é¢˜ï¼Œä»¥å±•ç¤ºæˆ‘ä»¬æå‡ºçš„ç³»ç»Ÿä¸ç°æœ‰ç³»ç»Ÿçš„æ€§èƒ½ã€‚æˆ‘ä»¬çš„ç³»ç»Ÿç›¸è¾ƒäºæ ‡å‡†RAGå’Œç®€å•çš„LLMï¼ˆå¦‚GPT-4oã€GPT-4o-miniã€Gemini-1.5å’ŒGPT-3.5ï¼‰è¡¨ç°å‡ºå“è¶Šçš„æ€§èƒ½ã€‚è¿™é¡¹å·¥ä½œå°†ä¸ºLLMåœ¨ä¸ªæ€§åŒ–æœåŠ¡æ–¹é¢çš„ç°å®ç”Ÿæ´»åº”ç”¨å¼€è¾Ÿå¯èƒ½æ€§ï¼Œæ—¥å¸¸ç¯å¢ƒä¸­çš„åº”ç”¨ã€‚æ­¤å¤–ï¼Œè¯¥ç ”ç©¶è¿˜å°†å¸®åŠ©ç¤¾ä¼šå­¦ã€å¿ƒç†å­¦å’Œç¥ç»ç§‘å­¦é¢†åŸŸçš„ç ”ç©¶äººå‘˜å°†ä»–ä»¬çš„ç†è®ºä¸å®é™…äººç±»æ—¥å¸¸ç¯å¢ƒæ›´åŠ ç´§å¯†åœ°ç»“åˆèµ·æ¥ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.02894v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å¹¿æ³›åº”ç”¨äºå„ç§ä»»åŠ¡å’Œåº”ç”¨ï¼Œä½†å…¶çŸ¥è¯†å’Œèƒ½åŠ›å—é™äºé¢„è®­ç»ƒæ•°æ®ã€‚ä¸ºå¢å¼ºLLMsçš„èƒ½åŠ›ï¼Œæå‡ºäº†æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰æ–¹æ³•ï¼Œä½†RAGå¯èƒ½æ— æ³•æœ€ä½³åœ°å‘ˆç°ä¿¡æ¯ï¼Œå°¤å…¶å¯¹ä¸åŒäººç¾¤ä¸ªæ€§åŒ–çš„éœ€æ±‚ã€‚å› æ­¤ï¼Œç»“åˆå¼ºåŒ–å­¦ä¹ äººç±»åé¦ˆï¼ˆRLHFï¼‰çš„åœ¨çº¿å¼ºåŒ–å­¦ä¹ åŸºäºæ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆOnRL-RAGï¼‰ç³»ç»Ÿè¢«æå‡ºï¼Œä»¥æ£€æµ‹å¹¶ä¸ªæ€§åŒ–å“åº”ç²¾ç¥å¥åº·é—®é¢˜ï¼Œå¦‚å‹åŠ›ã€ç„¦è™‘å’ŒæŠ‘éƒã€‚ä½¿ç”¨ä»28æ‰€é«˜æ ¡æ”¶é›†çš„å¼€æºæ•°æ®é›†ï¼Œå±•ç¤ºäº†è¯¥ç³»ç»Ÿçš„ä¼˜è¶Šæ€§ã€‚è¿™ä¸€ç ”ç©¶ä¸ºLLMsåœ¨ä¸ªæ€§åŒ–æœåŠ¡æ–¹é¢çš„å®é™…åº”ç”¨æ‰“å¼€äº†å¯èƒ½æ€§ï¼Œå¹¶å°†å¸®åŠ©ç¤¾ä¼šå­¦ã€å¿ƒç†å­¦å’Œç¥ç»ç§‘å­¦é¢†åŸŸçš„ç ”ç©¶è€…æ›´å¥½åœ°å°†ç†è®ºåº”ç”¨äºå®é™…ç¯å¢ƒã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å—é™äºé¢„è®­ç»ƒæ•°æ®çš„çŸ¥è¯†å’Œèƒ½åŠ›ã€‚</li>
<li>æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰æ–¹æ³•ç”¨äºå¢å¼ºLLMsçš„èƒ½åŠ›ï¼Œæä¾›é¢å¤–çš„æœ€æ–°ä¿¡æ¯ã€‚</li>
<li>RAGå¯èƒ½æ— æ³•æœ€ä½³åœ°å‘ˆç°ä¿¡æ¯ï¼Œå°¤å…¶å¯¹ä¸åŒäººç¾¤çš„ä¸ªæ€§åŒ–éœ€æ±‚ã€‚</li>
<li>æå‡ºç»“åˆå¼ºåŒ–å­¦ä¹ äººç±»åé¦ˆï¼ˆRLHFï¼‰çš„åœ¨çº¿å¼ºåŒ–å­¦ä¹ åŸºäºæ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆOnRL-RAGï¼‰ç³»ç»Ÿã€‚</li>
<li>OnRL-RAGç³»ç»Ÿç”¨äºæ£€æµ‹å¹¶ä¸ªæ€§åŒ–å“åº”ç²¾ç¥å¥åº·é—®é¢˜ï¼Œå¦‚å‹åŠ›ã€ç„¦è™‘å’ŒæŠ‘éƒã€‚</li>
<li>ä½¿ç”¨ä»é«˜æ ¡æ”¶é›†çš„å¼€æºæ•°æ®é›†å±•ç¤ºäº†OnRL-RAGç³»ç»Ÿçš„ä¼˜è¶Šæ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.02894">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-6c8a98243aaff6eae1eeb5298102889b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-7c7fee3d6574c7b44579867702d433e4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ec1ff3bb0a697938e71c8865e7f52fc9.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="GPG-A-Simple-and-Strong-Reinforcement-Learning-Baseline-for-Model-Reasoning"><a href="#GPG-A-Simple-and-Strong-Reinforcement-Learning-Baseline-for-Model-Reasoning" class="headerlink" title="GPG: A Simple and Strong Reinforcement Learning Baseline for Model   Reasoning"></a>GPG: A Simple and Strong Reinforcement Learning Baseline for Model   Reasoning</h2><p><strong>Authors:Xiangxiang Chu, Hailang Huang, Xiao Zhang, Fei Wei, Yong Wang</strong></p>
<p>Reinforcement Learning (RL) can directly enhance the reasoning capabilities of large language models without extensive reliance on Supervised Fine-Tuning (SFT). In this work, we revisit the traditional Policy Gradient (PG) mechanism and propose a minimalist RL approach termed Group Policy Gradient (GPG). Unlike conventional methods, GPG directly optimize the original RL objective, thus obviating the need for surrogate loss functions. By eliminating the critic and reference models, avoiding KL divergence constraints, and addressing the advantage and gradient estimation bias, our approach significantly simplifies the training process compared to Group Relative Policy Optimization (GRPO). Our approach achieves superior performance without relying on auxiliary techniques or adjustments. As illustrated in Figure 1, extensive experiments demonstrate that our method not only reduces computational costs but also consistently outperforms GRPO across various unimodal and multimodal tasks. Our code is available at <a target="_blank" rel="noopener" href="https://github.com/AMAP-ML/GPG">https://github.com/AMAP-ML/GPG</a>. </p>
<blockquote>
<p>å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰å¯ä»¥ç›´æ¥æå‡å¤§è¯­è¨€æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ï¼Œè€Œæ— éœ€è¿‡å¤šä¾èµ–ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬é‡æ–°å®¡è§†äº†ä¼ ç»Ÿçš„ç­–ç•¥æ¢¯åº¦ï¼ˆPGï¼‰æœºåˆ¶ï¼Œå¹¶æå‡ºäº†ä¸€ç§æç®€çš„RLæ–¹æ³•ï¼Œç§°ä¸ºç»„ç­–ç•¥æ¢¯åº¦ï¼ˆGPGï¼‰ã€‚ä¸åŒäºä¼ ç»Ÿæ–¹æ³•ï¼ŒGPGç›´æ¥ä¼˜åŒ–åŸå§‹çš„RLç›®æ ‡ï¼Œä»è€Œæ— éœ€æ›¿ä»£æŸå¤±å‡½æ•°ã€‚é€šè¿‡æ¶ˆé™¤è¯„è®ºå®¶æ¨¡å‹å’Œå‚è€ƒæ¨¡å‹ï¼Œé¿å…KLæ•£åº¦çº¦æŸï¼Œå¹¶è§£å†³ä¼˜åŠ¿å’Œæ¢¯åº¦ä¼°è®¡åå·®çš„é—®é¢˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•å¤§å¤§ç®€åŒ–äº†ä¸ç»„ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆGRPOï¼‰ç›¸æ¯”çš„è®­ç»ƒè¿‡ç¨‹ã€‚æˆ‘ä»¬çš„æ–¹æ³•åœ¨ä¸ä¾èµ–è¾…åŠ©æŠ€æœ¯æˆ–è°ƒæ•´çš„æƒ…å†µä¸‹å®ç°äº†å“è¶Šçš„æ€§èƒ½ã€‚å¦‚å›¾1æ‰€ç¤ºï¼Œå¤§é‡å®éªŒè¯æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•ä¸ä»…é™ä½äº†è®¡ç®—æˆæœ¬ï¼Œè€Œä¸”åœ¨å„ç§å•æ¨¡æ€å’Œå¤šæ¨¡æ€ä»»åŠ¡ä¸Šå§‹ç»ˆä¼˜äºGRPOã€‚æˆ‘ä»¬çš„ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/AMAP-ML/GPG%E4%B8%8A%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/AMAP-ML/GPGä¸Šæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.02546v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰å¯ä»¥ç›´æ¥æå‡å¤§è¯­è¨€æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ï¼Œæ— éœ€è¿‡å¤šä¾èµ–ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰ã€‚æœ¬ç ”ç©¶é‡æ–°å®¡è§†äº†ä¼ ç»Ÿçš„ç­–ç•¥æ¢¯åº¦ï¼ˆPGï¼‰æœºåˆ¶ï¼Œå¹¶æå‡ºäº†ä¸€ç§æç®€çš„RLæ–¹æ³•â€”â€”ç¾¤ç»„ç­–ç•¥æ¢¯åº¦ï¼ˆGPGï¼‰ã€‚GPGç›´æ¥ä¼˜åŒ–åŸå§‹çš„RLç›®æ ‡ï¼Œä»è€Œæ— éœ€æ›¿ä»£æŸå¤±å‡½æ•°ã€‚é€šè¿‡æ¶ˆé™¤è¯„è®ºå®¶æ¨¡å‹å’Œå‚è€ƒæ¨¡å‹ï¼Œé¿å…KLæ•£åº¦çº¦æŸï¼Œå¹¶è§£å†³ä¼˜åŠ¿å’Œæ¢¯åº¦ä¼°è®¡åå·®é—®é¢˜ï¼ŒGPGç®€åŒ–äº†ä¸ç¾¤ç»„ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆGRPOï¼‰ç›¸æ¯”çš„è®­ç»ƒè¿‡ç¨‹ã€‚åœ¨ä¸ä¾èµ–è¾…åŠ©æŠ€æœ¯æˆ–è°ƒæ•´çš„æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬çš„æ–¹æ³•å®ç°äº†å“è¶Šçš„æ€§èƒ½ã€‚å¦‚å›¾1æ‰€ç¤ºï¼Œå¤§é‡å®éªŒè¯æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•ä¸ä»…é™ä½äº†è®¡ç®—æˆæœ¬ï¼Œè€Œä¸”åœ¨å„ç§å•æ¨¡æ€å’Œå¤šæ¨¡æ€ä»»åŠ¡ä¸Šå§‹ç»ˆä¼˜äºGRPOã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰èƒ½å¤Ÿå¢å¼ºå¤§è¯­è¨€æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ï¼Œä¸”ä¸éœ€è¦è¿‡åº¦ä¾èµ–ç›‘ç£å¾®è°ƒã€‚</li>
<li>æå‡ºäº†ä¸€ç§æ–°çš„RLæ–¹æ³•â€”â€”ç¾¤ç»„ç­–ç•¥æ¢¯åº¦ï¼ˆGPGï¼‰ï¼Œè¯¥æ–¹æ³•ç›´æ¥ä¼˜åŒ–åŸå§‹çš„RLç›®æ ‡ã€‚</li>
<li>GPGç®€åŒ–äº†è®­ç»ƒè¿‡ç¨‹ï¼Œä¸å…¶ä»–æ–¹æ³•ç›¸æ¯”ï¼Œå®ƒä¸éœ€è¦ä½¿ç”¨æ›¿ä»£æŸå¤±å‡½æ•°ã€è¯„è®ºå®¶æ¨¡å‹å’Œå‚è€ƒæ¨¡å‹ã€‚</li>
<li>GPGè§£å†³äº†ä¼˜åŠ¿å’Œæ¢¯åº¦ä¼°è®¡åå·®é—®é¢˜ã€‚</li>
<li>GPGåœ¨å¤šç§ä»»åŠ¡ä¸Šè¡¨ç°å‡ºå“è¶Šæ€§èƒ½ï¼ŒåŒ…æ‹¬å•æ¨¡æ€å’Œå¤šæ¨¡æ€ä»»åŠ¡ã€‚</li>
<li>GPGæ–¹æ³•é™ä½äº†è®¡ç®—æˆæœ¬ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.02546">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-96d443e6632ee50810fe008b9f05c935.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b54a2d1843508cb20fbb024aeea7c04b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-dd74a39c41367cbdd9c94aee2c56beae.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b303edd1c6c88e2bbd9d342c7c4fd646.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-7dc6170186382993f0f806dfaa2836eb.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Improved-Visual-Spatial-Reasoning-via-R1-Zero-Like-Training"><a href="#Improved-Visual-Spatial-Reasoning-via-R1-Zero-Like-Training" class="headerlink" title="Improved Visual-Spatial Reasoning via R1-Zero-Like Training"></a>Improved Visual-Spatial Reasoning via R1-Zero-Like Training</h2><p><strong>Authors:Zhenyi Liao, Qingsong Xie, Yanhao Zhang, Zijian Kong, Haonan Lu, Zhenyu Yang, Zhijie Deng</strong></p>
<p>Increasing attention has been placed on improving the reasoning capacities of multi-modal large language models (MLLMs). As the cornerstone for AI agents that function in the physical realm, video-based visual-spatial intelligence (VSI) emerges as one of the most pivotal reasoning capabilities of MLLMs. This work conducts a first, in-depth study on improving the visual-spatial reasoning of MLLMs via R1-Zero-like training. Technically, we first identify that the visual-spatial reasoning capacities of small- to medium-sized Qwen2-VL models cannot be activated via Chain of Thought (CoT) prompts. We then incorporate GRPO training for improved visual-spatial reasoning, using the carefully curated VSI-100k dataset, following DeepSeek-R1-Zero. During the investigation, we identify the necessity to keep the KL penalty (even with a small value) in GRPO. With just 120 GPU hours, our vsGRPO-2B model, fine-tuned from Qwen2-VL-2B, can outperform the base model by 12.1% and surpass GPT-4o. Moreover, our vsGRPO-7B model, fine-tuned from Qwen2-VL-7B, achieves performance comparable to that of the best open-source model LLaVA-NeXT-Video-72B. Additionally, we compare vsGRPO to supervised fine-tuning and direct preference optimization baselines and observe strong performance superiority. The code and dataset will be available soon. </p>
<blockquote>
<p>è¿‘å¹´æ¥ï¼Œæé«˜å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰çš„æ¨ç†èƒ½åŠ›è¶Šæ¥è¶Šå—åˆ°å…³æ³¨ã€‚ä½œä¸ºåœ¨ç‰©ç†é¢†åŸŸå‘æŒ¥ä½œç”¨çš„AIä»£ç†çš„åŸºçŸ³ï¼ŒåŸºäºè§†é¢‘çš„è§†è§‰ç©ºé—´æ™ºèƒ½ï¼ˆVSIï¼‰è¢«è®¤ä¸ºæ˜¯MLLMsä¸­æœ€å…³é”®çš„æ¨ç†èƒ½åŠ›ä¹‹ä¸€ã€‚æœ¬ç ”ç©¶é¦–æ¬¡æ·±å…¥æ¢è®¨äº†é€šè¿‡R1-Zeroç±»ä¼¼çš„è®­ç»ƒæé«˜MLLMsçš„è§†è§‰ç©ºé—´æ¨ç†èƒ½åŠ›ã€‚ä»æŠ€æœ¯ä¸Šè®²ï¼Œæˆ‘ä»¬é¦–å…ˆå‘ç°å°å‹åˆ°ä¸­å‹Qwen2-VLæ¨¡å‹çš„è§†è§‰ç©ºé—´æ¨ç†èƒ½åŠ›æ— æ³•é€šè¿‡æ€ç»´é“¾ï¼ˆCoTï¼‰æç¤ºæ¥æ¿€æ´»ã€‚ç„¶åï¼Œæˆ‘ä»¬é‡‡ç”¨GRPOè®­ç»ƒæ³•æ¥æé«˜è§†è§‰ç©ºé—´æ¨ç†èƒ½åŠ›ï¼Œä½¿ç”¨ç²¾å¿ƒåˆ¶ä½œçš„VSI-100kæ•°æ®é›†ï¼Œéµå¾ªDeepSeek-R1-Zeroã€‚åœ¨ç ”ç©¶è¿‡ç¨‹ä¸­ï¼Œæˆ‘ä»¬å‘ç°éœ€è¦åœ¨GRPOä¸­ä¿æŒKLæƒ©ç½šï¼ˆå³ä½¿å€¼å¾ˆå°ï¼‰ã€‚åªéœ€120ä¸ªGPUå°æ—¶ï¼Œæˆ‘ä»¬çš„vsGRPO-2Bæ¨¡å‹ï¼Œåœ¨Qwen2-VL-2Bçš„åŸºç¡€ä¸Šè¿›è¡Œäº†å¾®è°ƒï¼Œæ€§èƒ½æ¯”åŸºç¡€æ¨¡å‹æé«˜äº†12.1%ï¼Œå¹¶è¶…è¶Šäº†GPT-4oã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬çš„vsGRPO-7Bæ¨¡å‹ï¼Œåœ¨Qwen2-VL-7Bçš„åŸºç¡€ä¸Šè¿›è¡Œå¾®è°ƒï¼Œå…¶æ€§èƒ½ä¸æœ€ä½³å¼€æºæ¨¡å‹LLaVA-NeXT-Video-72Bç›¸å½“ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å°†vsGRPOä¸ç›‘ç£å¾®è°ƒæ³•å’Œç›´æ¥åå¥½ä¼˜åŒ–åŸºå‡†çº¿è¿›è¡Œäº†æ¯”è¾ƒï¼Œå¹¶è§‚å¯Ÿåˆ°å…¶æ€§èƒ½ä¼˜åŠ¿æ˜¾è‘—ã€‚ä»£ç å’Œæ•°æ®é›†å°†å¾ˆå¿«å¯ç”¨ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.00883v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<pre><code>éšç€å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰çš„æ¨ç†èƒ½åŠ›æ—¥ç›Šå—åˆ°å…³æ³¨ï¼ŒåŸºäºè§†é¢‘çš„ç©ºé—´è§†è§‰æ™ºèƒ½ï¼ˆVSIï¼‰ä½œä¸ºå®ä½“AIä»£ç†çš„æ ¸å¿ƒï¼Œæˆä¸ºäº†MLLMsçš„å…³é”®æ¨ç†èƒ½åŠ›ä¹‹ä¸€ã€‚æœ¬ç ”ç©¶é€šè¿‡R1-Zeroç±»ä¼¼çš„è®­ç»ƒé¦–æ¬¡æ·±å…¥æ¢è®¨äº†å¢å¼ºMLLMsçš„è§†è§‰ç©ºé—´æ¨ç†èƒ½åŠ›ã€‚ç ”ç©¶ä¸­å‘ç°ä¸­å°å‹Qwen2-VLæ¨¡å‹çš„è§†è§‰ç©ºé—´æ¨ç†èƒ½åŠ›æ— æ³•é€šè¿‡æ€ç»´é“¾ï¼ˆCoTï¼‰æç¤ºæ¿€æ´»ï¼Œäºæ˜¯é‡‡ç”¨äº†GRPOè®­ç»ƒç»“åˆç²¾å¿ƒç­›é€‰çš„VSI-100kæ•°æ®é›†ä»¥å¢å¼ºå…¶è§†è§‰ç©ºé—´æ¨ç†èƒ½åŠ›ï¼Œå¹¶è®¤åŒäº†åœ¨GRPOä¸­ä¿æŒKLæƒ©ç½šçš„å¿…è¦æ€§ã€‚ç ”ç©¶ç»“æœæ˜¾ç¤ºï¼Œä½¿ç”¨GPUä»…120å°æ—¶ï¼ŒvsGRPO-2Bæ¨¡å‹æ€§èƒ½ä¼˜äºåŸºç¡€æ¨¡å‹è¾¾12.1%ï¼Œè¶…è¶ŠGPT-4oï¼›è€ŒvsGRPO-7Bæ¨¡å‹åˆ™å®ç°äº†ä¸é¡¶å°–å¼€æºæ¨¡å‹LLaVA-NeXT-Video-72Bç›¸å½“çš„ç»©æ•ˆã€‚ç ”ç©¶è¿˜å°†vsGRPOä¸ç›‘ç£å¾®è°ƒåŠç›´æ¥åå¥½ä¼˜åŒ–åŸºçº¿è¿›è¡Œäº†æ¯”è¾ƒï¼Œæ˜¾ç¤ºå‡ºå…¶å“è¶Šæ€§èƒ½ã€‚ä»£ç å’Œæ•°æ®é›†å³å°†å…¬å¸ƒã€‚
 
**Key Takeaways**

ä¸€ã€è§†é¢‘å‹è§†è§‰ç©ºé—´æ™ºèƒ½æ˜¯AIå®ä½“ä»£ç†åœ¨ç‰©ç†é¢†åŸŸä¸­çš„æ ¸å¿ƒæŠ€èƒ½ä¹‹ä¸€ï¼Œå¯¹å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰çš„æ¨ç†èƒ½åŠ›è‡³å…³é‡è¦ã€‚
äºŒã€ç ”ç©¶ä½¿ç”¨GRPOè®­ç»ƒæ–¹æ³•æé«˜MLLMsçš„è§†è§‰ç©ºé—´æ¨ç†èƒ½åŠ›ï¼Œé€šè¿‡ç²¾å¿ƒç­›é€‰çš„VSI-100kæ•°æ®é›†è¿›è¡Œè®­ç»ƒã€‚
ä¸‰ã€ç ”ç©¶å‘ç°ä¸­å°å‹Qwen2-VLæ¨¡å‹çš„è§†è§‰ç©ºé—´æ¨ç†èƒ½åŠ›æ— æ³•é€šè¿‡æ€ç»´é“¾ï¼ˆCoTï¼‰æç¤ºæ¿€æ´»ã€‚
å››ã€KLæƒ©ç½šåœ¨GRPOè®­ç»ƒä¸­å¿…è¦ï¼Œå¯¹æ¨¡å‹çš„æ€§èƒ½å½±å“æ˜¾è‘—ã€‚
äº”ã€ä½¿ç”¨ä»…æœ‰é™çš„è®¡ç®—èµ„æºï¼ˆGPUä»…è¿è¡Œ120å°æ—¶ï¼‰ï¼ŒvsGRPOè®­ç»ƒæ¨¡å‹æ€§èƒ½æ˜¾è‘—æé«˜ï¼Œè¶…è¶Šäº†ä¸€äº›ç°æœ‰æ¨¡å‹ã€‚
å…­ã€ç ”ç©¶éªŒè¯äº†vsGRPOè®­ç»ƒæ–¹æ³•çš„ä¼˜è¶Šæ€§ï¼Œä¸ç›‘ç£å¾®è°ƒåŠç›´æ¥åå¥½ä¼˜åŒ–åŸºçº¿ç›¸æ¯”è¡¨ç°çªå‡ºã€‚
ä¸ƒã€ä»£ç å’Œæ•°æ®é›†å³å°†å…¬å¼€ä»¥ä¾›ç ”ç©¶ä½¿ç”¨ã€‚
</code></pre>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.00883">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-24187fd5878a2449cb5f38bf3d051919.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-52b875081e85d964ca333fe9ab756775.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-225103d2558ab9f0b87ad4f111004969.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-1b49972410f36410ae3a781010c0dff3.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f219457257c1f643f459b5654ef8d396.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-93d69a0939107ecec5ba50057a730b1a.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="UI-R1-Enhancing-Action-Prediction-of-GUI-Agents-by-Reinforcement-Learning"><a href="#UI-R1-Enhancing-Action-Prediction-of-GUI-Agents-by-Reinforcement-Learning" class="headerlink" title="UI-R1: Enhancing Action Prediction of GUI Agents by Reinforcement   Learning"></a>UI-R1: Enhancing Action Prediction of GUI Agents by Reinforcement   Learning</h2><p><strong>Authors:Zhengxi Lu, Yuxiang Chai, Yaxuan Guo, Xi Yin, Liang Liu, Hao Wang, Han Xiao, Shuai Ren, Guanjing Xiong, Hongsheng Li</strong></p>
<p>The recent DeepSeek-R1 has showcased the emergence of reasoning capabilities in LLMs through reinforcement learning (RL) with rule-based rewards. Despite its success in language models, its application in multi-modal domains, particularly in graphic user interface (GUI) agent tasks, remains under-explored. To address this issue, we propose UI-R1, the first framework to explore how rule-based RL can enhance the reasoning capabilities of multimodal large language models (MLLMs) for GUI action prediction tasks. Specifically, UI-R1 introduces a novel rule-based action reward, enabling model optimization via policy-based algorithms such as Group Relative Policy Optimization (GRPO). For efficient training, we curate a small yet high-quality dataset of 136 challenging tasks, encompassing five common action types on mobile devices. Experimental results demonstrate that our proposed UI-R1-3B achieves significant improvements over the base model (i.e. Qwen2.5-VL-3B) on both in-domain (ID) and out-of-domain (OOD) tasks, with average accuracy gains of 22.1% on ScreenSpot, 6.0% on ScreenSpot-Pro, and 12.7% on ANDROIDCONTROL. Furthermore, UI-R1-3B delivers competitive performance compared to larger models (e.g., OS-Atlas-7B) trained via supervised fine-tuning (SFT) on 76K samples. These results underscore the potential of rule-based reinforcement learning to advance GUI understanding and control, paving the way for future research in this domain. Code website: <a target="_blank" rel="noopener" href="https://github.com/lll6gg/UI-R1">https://github.com/lll6gg/UI-R1</a>. </p>
<blockquote>
<p>æœ€è¿‘ï¼ŒDeepSeek-R1å±•ç¤ºäº†é€šè¿‡åŸºäºè§„åˆ™çš„å¥–åŠ±å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰åœ¨å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¸­å‡ºç°çš„æ¨ç†èƒ½åŠ›ã€‚å°½ç®¡å…¶åœ¨è¯­è¨€æ¨¡å‹æ–¹é¢å–å¾—äº†æˆåŠŸï¼Œä½†åœ¨å¤šæ¨¡æ€é¢†åŸŸï¼Œç‰¹åˆ«æ˜¯åœ¨å›¾å½¢ç”¨æˆ·ç•Œé¢ï¼ˆGUIï¼‰ä»£ç†ä»»åŠ¡ä¸­çš„åº”ç”¨ä»ç„¶æœªè¢«å……åˆ†æ¢ç´¢ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†UI-R1ï¼Œè¿™æ˜¯ç¬¬ä¸€ä¸ªæ¢ç´¢åŸºäºè§„åˆ™çš„RLå¦‚ä½•å¢å¼ºå¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰çš„æ¨ç†èƒ½åŠ›ä»¥æ‰§è¡ŒGUIåŠ¨ä½œé¢„æµ‹ä»»åŠ¡çš„æ¡†æ¶ã€‚å…·ä½“æ¥è¯´ï¼ŒUI-R1å¼•å…¥äº†ä¸€ç§æ–°å‹çš„åŸºäºåŠ¨ä½œçš„è§„åˆ™å¥–åŠ±ï¼Œé€šè¿‡åŸºäºç­–ç•¥ç®—æ³•ï¼ˆå¦‚Group Relative Policy Optimizationï¼ˆGRPOï¼‰ï¼‰ä¼˜åŒ–æ¨¡å‹ã€‚ä¸ºäº†è¿›è¡Œæœ‰æ•ˆçš„è®­ç»ƒï¼Œæˆ‘ä»¬æ•´ç†äº†ä¸€ä¸ªåŒ…å«136ä¸ªå…·æœ‰æŒ‘æˆ˜æ€§çš„ä»»åŠ¡çš„å°å‹é«˜è´¨é‡æ•°æ®é›†ï¼Œæ¶µç›–ç§»åŠ¨è®¾å¤‡ä¸Šçš„äº”ç§å¸¸è§åŠ¨ä½œç±»å‹ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬æå‡ºçš„UI-R1-3Bç›¸è¾ƒäºåŸºå‡†æ¨¡å‹ï¼ˆå³Qwen2.5-VL-3Bï¼‰åœ¨é¢†åŸŸå†…ï¼ˆIDï¼‰å’Œé¢†åŸŸå¤–ï¼ˆOODï¼‰çš„ä»»åŠ¡ä¸Šéƒ½å–å¾—äº†æ˜¾è‘—çš„æå‡ï¼Œåœ¨ScreenSpotä¸Šçš„å¹³å‡ç²¾åº¦æé«˜äº†22.1%ï¼Œåœ¨ScreenSpot-Proä¸Šæé«˜äº†6.0%ï¼Œåœ¨ANDROIDCONTROLä¸Šæé«˜äº†12.7%ã€‚æ­¤å¤–ï¼Œä¸åœ¨7.6ä¸‡ä¸ªæ ·æœ¬ä¸Šé€šè¿‡ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰è®­ç»ƒçš„æ›´å¤§æ¨¡å‹ï¼ˆå¦‚OS-Atlas-7Bï¼‰ç›¸æ¯”ï¼ŒUI-R1-3Bè¡¨ç°å‡ºå…·æœ‰ç«äº‰åŠ›çš„æ€§èƒ½ã€‚è¿™äº›ç»“æœçªæ˜¾äº†åŸºäºè§„åˆ™çš„å¼ºåŒ–å­¦ä¹ åœ¨GUIç†è§£å’Œæ§åˆ¶æ–¹é¢çš„æ½œåŠ›ï¼Œä¸ºæœªæ¥çš„ç ”ç©¶é“ºå¹³äº†é“è·¯ã€‚ç›¸å…³ä»£ç ç½‘ç«™ä¸ºï¼š<a target="_blank" rel="noopener" href="https://github.com/lll6gg/UI-R1%E3%80%82">https://github.com/lll6gg/UI-R1ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.21620v3">PDF</a> </p>
<p><strong>Summary</strong><br>    è¿‘æœŸDeepSeek-R1å±•ç¤ºäº†å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰é€šè¿‡åŸºäºè§„åˆ™çš„å¥–åŠ±åœ¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¸­æ¨åŠ¨æ¨ç†èƒ½åŠ›çš„å‘å±•ã€‚ç„¶è€Œï¼Œå…¶åœ¨å¤šæ¨¡æ€é¢†åŸŸï¼Œç‰¹åˆ«æ˜¯åœ¨å›¾å½¢ç”¨æˆ·ç•Œé¢ï¼ˆGUIï¼‰ä»£ç†ä»»åŠ¡ä¸­çš„åº”ç”¨ä»å¾…æ¢ç´¢ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºUI-R1æ¡†æ¶ï¼Œé¦–æ¬¡æ¢ç´¢åŸºäºè§„åˆ™çš„å¼ºåŒ–å­¦ä¹ å¦‚ä½•å¢å¼ºå¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨GUIåŠ¨ä½œé¢„æµ‹ä»»åŠ¡ä¸­çš„æ¨ç†èƒ½åŠ›ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒUI-R1åœ¨ç‰¹å®šæ•°æ®é›†ä¸Šè¾ƒåŸºç¡€æ¨¡å‹æœ‰æ˜¾è‘—æ”¹è¿›ï¼Œå¹¶åœ¨ä¸åŒä»»åŠ¡ä¸Šå±•ç°å‡ºå¼ºå¤§çš„æ€§èƒ½ã€‚æ­¤å¤–ï¼ŒUI-R1æ¨¡å‹çš„æ€§èƒ½è¡¨ç°ç”šè‡³å¯ä¸é‡‡ç”¨ç›‘ç£å¾®è°ƒçš„å¤§å‹æ¨¡å‹ç›¸æ¯”ã€‚è¿™é¢„ç¤ºç€åŸºäºè§„åˆ™çš„å¼ºåŒ–å­¦ä¹ åœ¨GUIç†è§£å’Œæ§åˆ¶æ–¹é¢çš„å·¨å¤§æ½œåŠ›ã€‚ä»£ç å…¬å¼€äºGitHubç½‘ç«™ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>DeepSeek-R1å±•ç¤ºäº†å¼ºåŒ–å­¦ä¹ åœ¨å¤§å‹è¯­è¨€æ¨¡å‹ä¸­çš„æ¨ç†èƒ½åŠ›ã€‚</li>
<li>åŸºäºè§„åˆ™çš„å¼ºåŒ–å­¦ä¹ åœ¨å¤šæ¨¡æ€é¢†åŸŸçš„åº”ç”¨ï¼Œç‰¹åˆ«æ˜¯åœ¨GUIä»£ç†ä»»åŠ¡ä¸­ï¼Œä»ç„¶æ˜¯ä¸€ä¸ªæœªå……åˆ†æ¢ç´¢çš„é¢†åŸŸã€‚</li>
<li>UI-R1æ¡†æ¶è¢«æå‡ºç”¨äºå¢å¼ºå¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹åœ¨GUIåŠ¨ä½œé¢„æµ‹ä»»åŠ¡ä¸­çš„æ¨ç†èƒ½åŠ›ã€‚</li>
<li>UI-R1å¼•å…¥äº†ä¸€ç§æ–°çš„åŸºäºè§„åˆ™çš„è¡ŒåŠ¨å¥–åŠ±ï¼Œé€šè¿‡ç­–ç•¥ä¼˜åŒ–ç®—æ³•ï¼ˆå¦‚GRPOï¼‰è¿›è¡Œæ¨¡å‹ä¼˜åŒ–ã€‚</li>
<li>å®éªŒç»“æœæ˜¾ç¤ºï¼ŒUI-R1åœ¨ç‰¹å®šæ•°æ®é›†ä¸Šçš„æ€§èƒ½è¾ƒåŸºç¡€æ¨¡å‹æœ‰æ˜¾è‘—æ”¹å–„ï¼Œå¹¶åœ¨ä¸åŒä»»åŠ¡ä¸Šå±•ç°å‡ºæ˜¾è‘—ä¼˜åŠ¿ã€‚</li>
<li>UI-R1çš„æ€§èƒ½ä¸é‡‡ç”¨ç›‘ç£å¾®è°ƒçš„å¤§å‹æ¨¡å‹ç›¸æ¯”å…·æœ‰ç«äº‰åŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.21620">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-57572e7a13f08dc88cf8256224ce8c37.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-759ab689c47377f1d5f7ec8d41fab7e2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7888ec58d54be02625496730e97f7c0e.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="FastCuRL-Curriculum-Reinforcement-Learning-with-Progressive-Context-Extension-for-Efficient-Training-R1-like-Reasoning-Models"><a href="#FastCuRL-Curriculum-Reinforcement-Learning-with-Progressive-Context-Extension-for-Efficient-Training-R1-like-Reasoning-Models" class="headerlink" title="FastCuRL: Curriculum Reinforcement Learning with Progressive Context   Extension for Efficient Training R1-like Reasoning Models"></a>FastCuRL: Curriculum Reinforcement Learning with Progressive Context   Extension for Efficient Training R1-like Reasoning Models</h2><p><strong>Authors:Mingyang Song, Mao Zheng, Zheng Li, Wenjie Yang, Xuan Luo, Yue Pan, Feng Zhang</strong></p>
<p>Improving the training efficiency remains one of the most significant challenges in large-scale reinforcement learning. In this paper, we investigate how the modelâ€™s context length and the complexity of the training dataset influence the training process of R1-like models. Our experiments reveal three key insights: (1) adopting longer context lengths may not necessarily result in better performance; (2) selecting an appropriate context length helps mitigate entropy collapse; and (3) appropriately controlling the modelâ€™s context length and curating training data based on input prompt length can effectively improve RL training efficiency, achieving better performance with shorter thinking length. Inspired by these insights, we propose FastCuRL, a curriculum reinforcement learning framework with the progressive context extension strategy, and successfully accelerate the training process of RL models. Experimental results demonstrate that FastCuRL-1.5B-Preview surpasses DeepScaleR-1.5B-Preview across all five benchmarks while only utilizing 50% of training steps. Furthermore, all training stages for FastCuRL-1.5B-Preview are completed using a single node with 8 GPUs. </p>
<blockquote>
<p>åœ¨å¤§è§„æ¨¡å¼ºåŒ–å­¦ä¹ ä¸­ï¼Œæé«˜è®­ç»ƒæ•ˆç‡ä»ç„¶æ˜¯æœ€å¤§çš„æŒ‘æˆ˜ä¹‹ä¸€ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬ç ”ç©¶äº†æ¨¡å‹çš„ä¸Šä¸‹æ–‡é•¿åº¦å’Œè®­ç»ƒæ•°æ®é›†å¤æ‚åº¦å¯¹R1ç±»ä¼¼æ¨¡å‹è®­ç»ƒè¿‡ç¨‹çš„å½±å“ã€‚æˆ‘ä»¬çš„å®éªŒæ­ç¤ºäº†ä¸‰ä¸ªå…³é”®è§è§£ï¼šï¼ˆ1ï¼‰é‡‡ç”¨æ›´é•¿çš„ä¸Šä¸‹æ–‡é•¿åº¦å¹¶ä¸ä¸€å®šèƒ½å¤Ÿå¸¦æ¥æ›´å¥½çš„æ€§èƒ½ï¼›ï¼ˆ2ï¼‰é€‰æ‹©åˆé€‚çš„ä¸Šä¸‹æ–‡é•¿åº¦æœ‰åŠ©äºç¼“è§£ç†µå´©æºƒï¼›ï¼ˆ3ï¼‰é€‚å½“åœ°æ§åˆ¶æ¨¡å‹çš„ä¸Šä¸‹æ–‡é•¿åº¦ï¼Œå¹¶æ ¹æ®è¾“å…¥æç¤ºé•¿åº¦ç­›é€‰è®­ç»ƒæ•°æ®ï¼Œå¯ä»¥æœ‰æ•ˆåœ°æé«˜RLè®­ç»ƒæ•ˆç‡ï¼Œä»¥æ›´çŸ­çš„æ€è€ƒé•¿åº¦å®ç°æ›´å¥½çš„æ€§èƒ½ã€‚å—åˆ°è¿™äº›è§è§£çš„å¯å‘ï¼Œæˆ‘ä»¬æå‡ºäº†FastCuRLï¼Œè¿™æ˜¯ä¸€ä¸ªå¸¦æœ‰æ¸è¿›å¼ä¸Šä¸‹æ–‡æ‰©å±•ç­–ç•¥çš„è¯¾ç¨‹å¼ºåŒ–å­¦ä¹ æ¡†æ¶ï¼ŒæˆåŠŸåŠ é€Ÿäº†RLæ¨¡å‹çš„è®­ç»ƒè¿‡ç¨‹ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒFastCuRL-1.5B-Previewåœ¨æ‰€æœ‰äº”ä¸ªåŸºå‡†æµ‹è¯•ä¸­å‡è¶…è¶Šäº†DeepScaleR-1.5B-Previewï¼Œè€Œä¸”ä»…ä½¿ç”¨äº†50%çš„è®­ç»ƒæ­¥éª¤ã€‚æ­¤å¤–ï¼ŒFastCuRL-1.5B-Previewçš„æ‰€æœ‰è®­ç»ƒé˜¶æ®µå‡ä½¿ç”¨å•ä¸ªèŠ‚ç‚¹å’Œ8ä¸ªGPUå®Œæˆã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.17287v2">PDF</a> Ongoing Work</p>
<p><strong>Summary</strong></p>
<p>å¤§å‹å¼ºåŒ–å­¦ä¹ è®­ç»ƒæ•ˆç‡çš„æå‡æ˜¯ä¸€å¤§æŒ‘æˆ˜ã€‚æœ¬æ–‡é€šè¿‡å®éªŒæ¢è®¨äº†æ¨¡å‹ä¸Šä¸‹æ–‡é•¿åº¦ä¸è®­ç»ƒæ•°æ®é›†å¤æ‚åº¦å¯¹R1ç±»æ¨¡å‹è®­ç»ƒè¿‡ç¨‹çš„å½±å“ï¼Œå¾—åˆ°ä»¥ä¸‹ä¸‰ä¸ªå…³é”®è§è§£ï¼šä¸€æ˜¯å¢åŠ ä¸Šä¸‹æ–‡é•¿åº¦ä¸ä¸€å®šèƒ½æé«˜æ€§èƒ½ï¼›äºŒæ˜¯é€‰æ‹©é€‚å½“çš„ä¸Šä¸‹æ–‡é•¿åº¦æœ‰åŠ©äºç¼“è§£ç†µå´©æºƒï¼›ä¸‰æ˜¯é€šè¿‡æ§åˆ¶æ¨¡å‹ä¸Šä¸‹æ–‡é•¿åº¦å¹¶æ ¹æ®è¾“å…¥æç¤ºé•¿åº¦æ•´ç†è®­ç»ƒæ•°æ®ï¼Œå¯ä»¥æœ‰æ•ˆæé«˜å¼ºåŒ–å­¦ä¹ è®­ç»ƒæ•ˆç‡ï¼Œå®ç°æ›´çŸ­çš„æ€è€ƒé•¿åº¦ä¸‹æ›´å¥½çš„æ€§èƒ½ã€‚åŸºäºè¿™äº›è§è§£ï¼Œæœ¬æ–‡æå‡ºäº†å¸¦æœ‰æ¸è¿›å¼ä¸Šä¸‹æ–‡æ‰©å±•ç­–ç•¥çš„FastCuRLå¼ºåŒ–å­¦ä¹ æ¡†æ¶ï¼ŒæˆåŠŸåŠ é€Ÿäº†RLæ¨¡å‹çš„è®­ç»ƒè¿‡ç¨‹ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒFastCuRL-1.5B-Previewåœ¨æ‰€æœ‰äº”ä¸ªåŸºå‡†æµ‹è¯•ä¸­å‡è¶…è¿‡äº†DeepScaleR-1.5B-Previewï¼Œä¸”å…¶æ‰€æœ‰è®­ç»ƒé˜¶æ®µå‡ä½¿ç”¨å•ä¸ªèŠ‚ç‚¹8 GPUå®Œæˆã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ¨¡å‹ä¸Šä¸‹æ–‡é•¿åº¦ä¸è®­ç»ƒæ•°æ®é›†å¤æ‚åº¦å¯¹å¼ºåŒ–å­¦ä¹ æ¨¡å‹è®­ç»ƒæ•ˆç‡æœ‰å½±å“ã€‚</li>
<li>å¢åŠ ä¸Šä¸‹æ–‡é•¿åº¦å¹¶ä¸ä¸€å®šæå‡æ¨¡å‹æ€§èƒ½ã€‚</li>
<li>é€‰æ‹©åˆé€‚çš„ä¸Šä¸‹æ–‡é•¿åº¦æœ‰åŠ©äºç¼“è§£ç†µå´©æºƒé—®é¢˜ã€‚</li>
<li>æ§åˆ¶æ¨¡å‹ä¸Šä¸‹æ–‡é•¿åº¦å¹¶æ•´ç†è®­ç»ƒæ•°æ®èƒ½æœ‰æ•ˆæé«˜å¼ºåŒ–å­¦ä¹ è®­ç»ƒæ•ˆç‡ã€‚</li>
<li>FastCuRLæ¡†æ¶é€šè¿‡æ¸è¿›å¼ä¸Šä¸‹æ–‡æ‰©å±•ç­–ç•¥æˆåŠŸåŠ é€Ÿäº†RLæ¨¡å‹çš„è®­ç»ƒã€‚</li>
<li>FastCuRL-1.5B-Previewåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜äºDeepScaleR-1.5B-Previewã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.17287">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-b2bbd68ecb85518841fa7fdd786431cb.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-91befed409cf747ec90504448b6e5396.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8b213f235c82011051470824a2b64968.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-47e5bb6873fca799fd2a6e6b8832cf31.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ff46b00a5df7fa82d528d3936a7a9081.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-797efee412d68ebf065d1aa436b0af36.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-efb4ea27859c08e3503d5f8ac99735be.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-37b67cfd031de900c301d49016fe5675.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Think-or-Not-Think-A-Study-of-Explicit-Thinking-in-Rule-Based-Visual-Reinforcement-Fine-Tuning"><a href="#Think-or-Not-Think-A-Study-of-Explicit-Thinking-in-Rule-Based-Visual-Reinforcement-Fine-Tuning" class="headerlink" title="Think or Not Think: A Study of Explicit Thinking in Rule-Based Visual   Reinforcement Fine-Tuning"></a>Think or Not Think: A Study of Explicit Thinking in Rule-Based Visual   Reinforcement Fine-Tuning</h2><p><strong>Authors:Ming Li, Jike Zhong, Shitian Zhao, Yuxiang Lai, Kaipeng Zhang</strong></p>
<p>This paper investigates the thinking process in rule-based reinforcement learning fine-tuning (RFT) for multi-modal large language models (MLLMs). We first propose CLS-RL for classification, using verifiable rewards to encourage MLLM thinking. Experiments show CLS-RL significantly outperforms SFT and yields a â€˜free-lunchâ€™ generalization effect (improving performance on unseen datasets after training on one dataset). We then question if this explicit thinking is always necessary for RFT. Challenging convention that explicit thinking is crucial for RFT, we introduce No-Thinking-RL, minimizing thinking via a simple equality accuracy reward. Experiments show No-Thinking-RL surpasses CLS-RL in in-domain and generalization abilities, with significantly less fine-tuning time. This suggests reducing thinking can improve MLLM fine-tuning efficiency and effectiveness for certain visual tasks. We hypothesize explicit thinking negatively impacts reward convergence during RFT. To test this, we propose the Think-After-Answerwer method to let models first output the answer and then generate thinking process to alliviate the negative impact of thinking. We further test No-Thinking-RL on diverse tasks (including math, spatial, puzzles) with 2B and 7B models. For 2B models, No-Thinking-RL outperforms thinking-based RFT for all tasks, even on math, with Think-After-Answerwer performing intermediately. For 7B models, performance is comparable on simple visual tasks, but RFT with thinking excels on complex reasoning (math). This implies when dealing with complex math problems, smaller models struggle with generating effective reasoning, hurting performance on complex tasks. Conversely, for simple visual tasks, thinking is not indispensable, and its removal can boost performance and reduce training time. We hope our findings offer insights for better understanding the effect of the thinking process in RFT. </p>
<blockquote>
<p>æœ¬æ–‡æ¢è®¨äº†åŸºäºè§„åˆ™å¼ºåŒ–å­¦ä¹ å¾®è°ƒï¼ˆRFTï¼‰åœ¨å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰ä¸­çš„æ€è€ƒè¿‡ç¨‹ã€‚æˆ‘ä»¬é¦–å…ˆæå‡ºç”¨äºåˆ†ç±»çš„CLS-RLï¼Œä½¿ç”¨å¯éªŒè¯çš„å¥–åŠ±æ¥é¼“åŠ±MLLMæ€è€ƒã€‚å®éªŒè¡¨æ˜ï¼ŒCLS-RLæ˜¾è‘—ä¼˜äºSFTï¼Œå¹¶äº§ç”Ÿäº†â€œå…è´¹åˆé¤â€çš„æ³›åŒ–æ•ˆæœï¼ˆåœ¨å•ä¸ªæ•°æ®é›†ä¸Šè¿›è¡Œè®­ç»ƒåï¼Œåœ¨æœªè§æ•°æ®é›†ä¸Šæé«˜äº†æ€§èƒ½ï¼‰ã€‚ç„¶åï¼Œæˆ‘ä»¬è´¨ç–‘è¿™ç§æ˜ç¡®çš„æ€è€ƒå¯¹äºRFTæ˜¯å¦å§‹ç»ˆå¿…è¦ã€‚æˆ‘ä»¬æŒ‘æˆ˜è®¤ä¸ºæ˜ç¡®æ€è€ƒå¯¹äºRFTè‡³å…³é‡è¦çš„ä¼ ç»Ÿè§‚å¿µï¼Œå¹¶å¼•å…¥äº†æ— æ€è€ƒå¼ºåŒ–å­¦ä¹ ï¼ˆNo-Thinking-RLï¼‰ï¼Œé€šè¿‡ç®€å•çš„ç­‰å¼å‡†ç¡®æ€§å¥–åŠ±æ¥å‡å°‘æ€è€ƒã€‚å®éªŒè¡¨æ˜ï¼Œåœ¨é¢†åŸŸå†…éƒ¨å’Œæ³›åŒ–èƒ½åŠ›æ–¹é¢ï¼Œæ— æ€è€ƒå¼ºåŒ–å­¦ä¹ è¶…è¶Šäº†CLS-RLï¼Œå¹¶ä¸”å¾®è°ƒæ—¶é—´å¤§å¤§å‡å°‘ã€‚è¿™è¡¨æ˜åœ¨æŸäº›è§†è§‰ä»»åŠ¡ä¸­ï¼Œå‡å°‘æ€è€ƒå¯ä»¥æé«˜MLLMå¾®è°ƒçš„æœ‰æ•ˆæ€§å’Œæ•ˆç‡ã€‚æˆ‘ä»¬å‡è®¾æ˜ç¡®çš„æ€è€ƒä¼šå¯¹RFTæœŸé—´çš„å¥–åŠ±æ”¶æ•›äº§ç”Ÿè´Ÿé¢å½±å“ã€‚ä¸ºäº†æµ‹è¯•è¿™ä¸€ç‚¹ï¼Œæˆ‘ä»¬æå‡ºäº†Think-After-Answerweræ–¹æ³•ï¼Œè®©æ¨¡å‹å…ˆç»™å‡ºç­”æ¡ˆï¼Œç„¶åç”Ÿæˆæ€è€ƒè¿‡ç¨‹ï¼Œä»¥å‡è½»æ€è€ƒå¸¦æ¥çš„è´Ÿé¢å½±å“ã€‚æˆ‘ä»¬è¿˜è¿›ä¸€æ­¥åœ¨ä¸åŒçš„ä»»åŠ¡ï¼ˆåŒ…æ‹¬æ•°å­¦ã€ç©ºé—´ã€è°œé¢˜ï¼‰ä¸Šæµ‹è¯•äº†æ— æ€è€ƒå¼ºåŒ–å­¦ä¹ ï¼Œä½¿ç”¨äº†2Bå’Œ7Bæ¨¡å‹ã€‚å¯¹äº2Bæ¨¡å‹ï¼Œæ— æ€è€ƒå¼ºåŒ–å­¦ä¹ åœ¨æ‰€æœ‰ä»»åŠ¡ä¸Šçš„è¡¨ç°éƒ½ä¼˜äºåŸºäºæ€è€ƒçš„RFTï¼Œç”šè‡³åœ¨æ•°å­¦ä¸Šä¹Ÿå¦‚æ­¤ï¼Œè€ŒThink-After-Answerwerçš„è¡¨ç°å¤„äºä¸­é—´æ°´å¹³ã€‚å¯¹äº7Bæ¨¡å‹ï¼Œåœ¨ç®€å•çš„è§†è§‰ä»»åŠ¡ä¸Šè¡¨ç°ç›¸å½“ï¼Œä½†åœ¨å¤æ‚æ¨ç†ï¼ˆæ•°å­¦ï¼‰æ–¹é¢ï¼Œå¸¦æœ‰æ€è€ƒçš„RFTè¡¨ç°æ›´å¥½ã€‚è¿™æ„å‘³ç€åœ¨å¤„ç†å¤æ‚çš„æ•°å­¦é—®é¢˜æ—¶ï¼Œè¾ƒå°çš„æ¨¡å‹åœ¨ç”Ÿæˆæœ‰æ•ˆæ¨ç†æ–¹é¢é‡åˆ°å›°éš¾ï¼Œå½±å“äº†å¤æ‚ä»»åŠ¡çš„æ€§èƒ½ã€‚ç›¸åï¼Œå¯¹äºç®€å•çš„è§†è§‰ä»»åŠ¡ï¼Œæ€è€ƒå¹¶ä¸æ˜¯å¿…ä¸å¯å°‘çš„ï¼Œè€Œä¸”ç§»é™¤æ€è€ƒå¯ä»¥æé«˜æ€§èƒ½å¹¶å‡å°‘è®­ç»ƒæ—¶é—´ã€‚æˆ‘ä»¬å¸Œæœ›æˆ‘ä»¬çš„ç ”ç©¶èƒ½ä¸ºæ›´å¥½åœ°ç†è§£RFTä¸­æ€è€ƒè¿‡ç¨‹çš„å½±å“æä¾›å¯ç¤ºã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.16188v3">PDF</a> Preprint, work in progress. Add results on math, cvbench, and puzzle</p>
<p><strong>Summary</strong></p>
<p>è¿™ç¯‡è®ºæ–‡æ¢è®¨äº†åŸºäºè§„åˆ™çš„å¼ºåŒ–å­¦ä¹ å¾®è°ƒï¼ˆRFTï¼‰åœ¨å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰ä¸­çš„æ€è€ƒè¿‡ç¨‹ã€‚ç ”ç©¶é¦–å…ˆæå‡ºäº†ç”¨äºåˆ†ç±»çš„CLS-RLæ–¹æ³•ï¼Œä½¿ç”¨å¯éªŒè¯çš„å¥–åŠ±æ¥é¼“åŠ±MLLMæ€è€ƒã€‚å®éªŒè¡¨æ˜CLS-RLæ˜¾è‘—ä¼˜äºSFTï¼Œå¹¶äº§ç”Ÿäº†â€œå…è´¹åˆé¤â€å¼çš„æ³›åŒ–æ•ˆæœã€‚ç„¶è€Œï¼Œç ”ç©¶ä¹Ÿè´¨ç–‘RFTæ˜¯å¦æ€»æ˜¯éœ€è¦æ˜ç¡®çš„æ€è€ƒã€‚é€šè¿‡å¼•å…¥ä¸éœ€è¦æ€è€ƒçš„No-Thinking-RLæ–¹æ³•ï¼Œè¯¥æ–¹æ³•é€šè¿‡ç®€å•çš„å‡†ç¡®æ€§å¥–åŠ±æ¥æœ€å°åŒ–æ€è€ƒï¼Œå®éªŒè¡¨æ˜å…¶åœ¨åŸŸå†…å’Œæ³›åŒ–èƒ½åŠ›ä¸Šè¶…è¶Šäº†CLS-RLï¼Œä¸”å¾®è°ƒæ—¶é—´å¤§å¤§å‡å°‘ã€‚è¿™è¡¨æ˜åœ¨æŸäº›è§†è§‰ä»»åŠ¡ä¸­ï¼Œå‡å°‘æ€è€ƒå¯ä»¥æé«˜MLLMçš„å¾®è°ƒæ•ˆç‡å’Œæ•ˆæœã€‚åŒæ—¶æå‡ºå‡è®¾è®¤ä¸ºæ˜ç¡®æ€è€ƒä¼šå½±å“å¥–åŠ±æ”¶æ•›è¿‡ç¨‹ï¼Œå¹¶æå‡ºThink-After-Answerweræ–¹æ³•æ¥å‡è½»å…¶è´Ÿé¢å½±å“ã€‚é€šè¿‡å¯¹ä¸åŒä»»åŠ¡çš„æµ‹è¯•è¡¨æ˜ï¼Œå¯¹äºç®€å•è§†è§‰ä»»åŠ¡ï¼Œä¸éœ€è¦æ€è€ƒå¯ä»¥æé«˜æ€§èƒ½å’Œå‡å°‘è®­ç»ƒæ—¶é—´ï¼›è€Œå¯¹äºå¤æ‚ä»»åŠ¡å¦‚æ•°å­¦æ¨ç†ï¼Œæ€è€ƒåˆ™æ›´ä¸ºå…³é”®ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>CLS-RLæ–¹æ³•åˆ©ç”¨å¯éªŒè¯å¥–åŠ±é¼“åŠ±MLLMæ¨¡å‹æ€è€ƒï¼Œæ˜¾è‘—ä¼˜äºSFTï¼Œå¹¶äº§ç”Ÿæ³›åŒ–æ•ˆæœã€‚</li>
<li>No-Thinking-RLæ–¹æ³•é€šè¿‡æœ€å°åŒ–æ€è€ƒæ¥æé«˜MLLMçš„å¾®è°ƒæ•ˆç‡å’Œæ•ˆæœï¼Œç‰¹åˆ«æ˜¯å¯¹äºæŸäº›è§†è§‰ä»»åŠ¡ã€‚</li>
<li>æ˜ç¡®æ€è€ƒå¯¹å¥–åŠ±æ”¶æ•›è¿‡ç¨‹æœ‰è´Ÿé¢å½±å“ï¼Œå¯èƒ½å¯¼è‡´è®­ç»ƒæ•ˆç‡é™ä½ã€‚</li>
<li>Think-After-Answerweræ–¹æ³•æ—¨åœ¨å‡è½»æ€è€ƒå¯¹å¥–åŠ±æ”¶æ•›çš„è´Ÿé¢å½±å“ã€‚</li>
<li>å¯¹äºç®€å•è§†è§‰ä»»åŠ¡ï¼Œä¸éœ€è¦æ€è€ƒå¯ä»¥æé«˜æ€§èƒ½å’Œå‡å°‘è®­ç»ƒæ—¶é—´ï¼›è€Œå¯¹äºå¤æ‚ä»»åŠ¡å¦‚æ•°å­¦æ¨ç†ï¼Œåˆ™éœ€è¦æ€è€ƒè¿‡ç¨‹ä»¥äº§ç”Ÿæ›´å¥½çš„æ€§èƒ½ã€‚</li>
<li>å¯¹äºä¸åŒçš„ä»»åŠ¡è§„æ¨¡å’Œç±»å‹ï¼ˆå¦‚æ•°å­¦ã€ç©ºé—´ã€è°œé¢˜ç­‰ï¼‰ï¼ŒNo-Thinking-RLçš„è¡¨ç°æœ‰æ‰€ä¸åŒã€‚å¯¹äºè¾ƒå°çš„æ¨¡å‹ï¼Œå®ƒåœ¨ç®€å•ä»»åŠ¡ä¸Šçš„è¡¨ç°ä¼˜äºå¸¦æœ‰æ€è€ƒè¿‡ç¨‹çš„RFTï¼›è€Œå¯¹äºå¤æ‚çš„æ•°å­¦ä»»åŠ¡ï¼Œå¸¦æœ‰æ€è€ƒè¿‡ç¨‹çš„RFTè¡¨ç°æ›´å¥½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.16188">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-f709d4288fe7b2cce6ea793934090039.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8c1dd0ba59f7408388774e7a6972af04.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e216b6232d87137b9aeb65df05c71032.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="FNSE-SBGAN-Far-field-Speech-Enhancement-with-Schrodinger-Bridge-and-Generative-Adversarial-Networks"><a href="#FNSE-SBGAN-Far-field-Speech-Enhancement-with-Schrodinger-Bridge-and-Generative-Adversarial-Networks" class="headerlink" title="FNSE-SBGAN: Far-field Speech Enhancement with Schrodinger Bridge and   Generative Adversarial Networks"></a>FNSE-SBGAN: Far-field Speech Enhancement with Schrodinger Bridge and   Generative Adversarial Networks</h2><p><strong>Authors:Tong Lei, Qinwen Hu, Ziyao Lin, Andong Li, Rilin Chen, Meng Yu, Dong Yu, Jing Lu</strong></p>
<p>The prevailing method for neural speech enhancement predominantly utilizes fully-supervised deep learning with simulated pairs of far-field noisy-reverberant speech and clean speech. Nonetheless, these models frequently demonstrate restricted generalizability to mixtures recorded in real-world conditions. To address this issue, this study investigates training enhancement models directly on real mixtures. Specifically, we revisit the single-channel far-field to near-field speech enhancement (FNSE) task, focusing on real-world data characterized by low signal-to-noise ratio (SNR), high reverberation, and mid-to-high frequency attenuation. We propose FNSE-SBGAN, a framework that integrates a Schrodinger Bridge (SB)-based diffusion model with generative adversarial networks (GANs). Our approach achieves state-of-the-art performance across various metrics and subjective evaluations, significantly reducing the character error rate (CER) by up to 14.58% compared to far-field signals. Experimental results demonstrate that FNSE-SBGAN preserves superior subjective quality and establishes a new benchmark for real-world far-field speech enhancement. Additionally, we introduce an evaluation framework leveraging matrix rank analysis in the time-frequency domain, providing systematic insights into model performance and revealing the strengths and weaknesses of different generative methods. </p>
<blockquote>
<p>å½“å‰ç¥ç»ç½‘ç»œè¯­éŸ³å¢å¼ºçš„ä¸»æµæ–¹æ³•ä¸»è¦åˆ©ç”¨æ¨¡æ‹Ÿçš„è¿œåœºå¸¦å™ªå£°å’Œå›å“çš„è¯­éŸ³ä¸å¹²å‡€è¯­éŸ³é…å¯¹è¿›è¡Œå…¨ç›‘ç£æ·±åº¦å­¦ä¹ ã€‚ç„¶è€Œï¼Œè¿™äº›æ¨¡å‹åœ¨ç°å®æ¡ä»¶ä¸‹å½•åˆ¶çš„æ··åˆéŸ³ä¸Šé€šå¸¸è¡¨ç°å‡ºæœ‰é™çš„æ³›åŒ–èƒ½åŠ›ã€‚ä¸ºè§£å†³æ­¤é—®é¢˜ï¼Œæœ¬ç ”ç©¶æ—¨åœ¨ç›´æ¥åœ¨å®é™…æ··åˆéŸ³ä¸Šè®­ç»ƒå¢å¼ºæ¨¡å‹ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬é‡æ–°å®¡è§†å•é€šé“è¿œåœºåˆ°è¿‘åœºè¯­éŸ³å¢å¼ºï¼ˆFNSEï¼‰ä»»åŠ¡ï¼Œé‡ç‚¹å…³æ³¨ç°å®ä¸–ç•Œæ•°æ®çš„ç‰¹ç‚¹ï¼Œå¦‚ä½ä¿¡å™ªæ¯”ï¼ˆSNRï¼‰ã€é«˜å›å“ä»¥åŠä¸­åˆ°é«˜é¢‘è¡°å‡ã€‚æˆ‘ä»¬æå‡ºäº†FNSE-SBGANæ¡†æ¶ï¼Œå®ƒèåˆäº†åŸºäºSchrodinger Bridgeï¼ˆSBï¼‰çš„æ‰©æ•£æ¨¡å‹ä¸ç”Ÿæˆå¯¹æŠ—ç½‘ç»œï¼ˆGANsï¼‰ã€‚æˆ‘ä»¬çš„æ–¹æ³•åœ¨å„ç§æŒ‡æ ‡å’Œä¸»è§‚è¯„ä»·ä¸Šå‡è¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œä¸è¿œåœºä¿¡å·ç›¸æ¯”ï¼Œå­—ç¬¦é”™è¯¯ç‡ï¼ˆCERï¼‰é™ä½äº†é«˜è¾¾14.58%ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒFNSE-SBGANä¿æŒäº†å‡ºè‰²çš„ä¸»è§‚è´¨é‡ï¼Œä¸ºç°å®ä¸–ç•Œçš„è¿œåœºè¯­éŸ³å¢å¼ºå»ºç«‹äº†æ–°çš„åŸºå‡†ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜å¼•å…¥äº†ä¸€ä¸ªåˆ©ç”¨æ—¶é¢‘åŸŸçŸ©é˜µç§©åˆ†æçš„è¯„ä»·æ¡†æ¶ï¼Œä¸ºæ¨¡å‹æ€§èƒ½æä¾›äº†ç³»ç»Ÿçš„è§è§£ï¼Œæ­ç¤ºäº†ä¸åŒç”Ÿæˆæ–¹æ³•çš„ä¼˜åŠ¿å’ŒåŠ£åŠ¿ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.12936v3">PDF</a> 13 pages, 6 figures</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä¸»è¦ç ”ç©¶äº†ç¥ç»ç½‘ç»œè¯­éŸ³å¢å¼ºçš„ç°æœ‰æ–¹æ³•ï¼Œä¸»è¦é‡‡ç”¨åŸºäºæ¨¡æ‹Ÿè¿œåœºå™ªå£°å›è¡è¯­éŸ³å’Œå¹²å‡€è¯­éŸ³é…å¯¹çš„å…¨ç›‘ç£æ·±åº¦å­¦ä¹ ã€‚ç„¶è€Œï¼Œè¿™äº›æ¨¡å‹åœ¨çœŸå®æ¡ä»¶ä¸‹çš„æ··åˆå½•éŸ³ä¸­é€šå¸¸è¡¨ç°å‡ºä¸€èˆ¬åŒ–èƒ½åŠ›å—é™çš„é—®é¢˜ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œæœ¬ç ”ç©¶ç›´æ¥å¯¹çœŸå®æ··åˆè¿›è¡Œè®­ç»ƒå¢å¼ºæ¨¡å‹ã€‚é’ˆå¯¹ä½ä¿¡å™ªæ¯”ã€é«˜å›å£°å’Œä¸­ç­‰è‡³é«˜é¢‘è¡°å‡ç­‰çœŸå®ä¸–ç•Œæ•°æ®ç‰¹ç‚¹çš„å•é€šé“è¿œåœºåˆ°è¿‘åœºè¯­éŸ³å¢å¼ºï¼ˆFNSEï¼‰ä»»åŠ¡è¿›è¡Œæ·±å…¥ç ”ç©¶ã€‚æå‡ºä¸€ç§åŸºäºSchrodinger Bridgeï¼ˆSBï¼‰æ‰©æ•£æ¨¡å‹ä¸ç”Ÿæˆå¯¹æŠ—ç½‘ç»œï¼ˆGANsï¼‰ç»“åˆçš„FNSE-SBGANæ¡†æ¶ã€‚è¯¥æ–¹æ³•åœ¨å„ç§æŒ‡æ ‡å’Œä¸»è§‚è¯„ä¼°ä¸Šå–å¾—äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œä¸è¿œåœºä¿¡å·ç›¸æ¯”ï¼Œå­—ç¬¦é”™è¯¯ç‡ï¼ˆCERï¼‰é™ä½äº†é«˜è¾¾14.58%ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒFNSE-SBGANä¿ç•™äº†è¾ƒé«˜çš„ä¸»è§‚è´¨é‡ï¼Œä¸ºçœŸå®ä¸–ç•Œçš„è¿œåœºè¯­éŸ³å¢å¼ºå»ºç«‹äº†æ–°çš„åŸºå‡†ã€‚æ­¤å¤–ï¼Œæœ¬ç ”ç©¶è¿˜å¼•å…¥äº†ä¸€ç§åŸºäºçŸ©é˜µç§©åˆ†æçš„æ—¶é—´-é¢‘ç‡åŸŸè¯„ä¼°æ¡†æ¶ï¼Œä¸ºæ¨¡å‹æ€§èƒ½æä¾›äº†ç³»ç»Ÿçš„è§è§£ï¼Œæ­ç¤ºäº†ä¸åŒç”Ÿæˆæ–¹æ³•çš„ä¼˜åŠ¿å’ŒåŠ£åŠ¿ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç¥ç»ç½‘ç»œè¯­éŸ³å¢å¼ºä¸»è¦ä½¿ç”¨å…¨ç›‘ç£æ·±åº¦å­¦ä¹ æ¨¡æ‹Ÿé…å¯¹è¿œåœºå™ªå£°å›è¡è¯­éŸ³å’Œå¹²å‡€è¯­éŸ³ã€‚</li>
<li>å½“å‰æ¨¡å‹åœ¨çœŸå®æ¡ä»¶ä¸‹çš„æ··åˆå½•éŸ³ä¸­è¡¨ç°ä¸€èˆ¬åŒ–èƒ½åŠ›å—é™ã€‚</li>
<li>ç ”ç©¶æå‡ºä¸€ç§æ–°å‹çš„è¯­éŸ³å¢å¼ºæ¨¡å‹FNSE-SBGANï¼Œç»“åˆäº†Schrodinger Bridgeæ‰©æ•£æ¨¡å‹å’Œç”Ÿæˆå¯¹æŠ—ç½‘ç»œã€‚</li>
<li>FNSE-SBGANåœ¨å„é¡¹æŒ‡æ ‡å’Œä¸»è§‚è¯„ä¼°ä¸Šå–å¾—äº†å…ˆè¿›æ€§èƒ½ï¼Œæ˜¾è‘—é™ä½äº†å­—ç¬¦é”™è¯¯ç‡ã€‚</li>
<li>FNSE-SBGANä¿ç•™äº†è¾ƒé«˜çš„ä¸»è§‚è´¨é‡ï¼Œä¸ºçœŸå®ä¸–ç•Œçš„è¿œåœºè¯­éŸ³å¢å¼ºå»ºç«‹äº†æ–°çš„åŸºå‡†ã€‚</li>
<li>å¼•å…¥äº†ä¸€ç§åŸºäºçŸ©é˜µç§©åˆ†æçš„æ—¶é—´-é¢‘ç‡åŸŸè¯„ä¼°æ¡†æ¶ï¼Œæä¾›æ¨¡å‹æ€§èƒ½çš„ç³»ç»Ÿè§è§£ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.12936">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-459078161a16fb07004809fdc00d21c9.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-8853a8c3a6fd9c1ce121200b015bddfd.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="MM-Eureka-Exploring-the-Frontiers-of-Multimodal-Reasoning-with-Rule-based-Reinforcement-Learning"><a href="#MM-Eureka-Exploring-the-Frontiers-of-Multimodal-Reasoning-with-Rule-based-Reinforcement-Learning" class="headerlink" title="MM-Eureka: Exploring the Frontiers of Multimodal Reasoning with   Rule-based Reinforcement Learning"></a>MM-Eureka: Exploring the Frontiers of Multimodal Reasoning with   Rule-based Reinforcement Learning</h2><p><strong>Authors:Fanqing Meng, Lingxiao Du, Zongkai Liu, Zhixiang Zhou, Quanfeng Lu, Daocheng Fu, Tiancheng Han, Botian Shi, Wenhai Wang, Junjun He, Kaipeng Zhang, Ping Luo, Yu Qiao, Qiaosheng Zhang, Wenqi Shao</strong></p>
<p>DeepSeek R1, and o1 have demonstrated powerful reasoning capabilities in the text domain through stable large-scale reinforcement learning. To enable broader applications, some works have attempted to transfer these capabilities to multimodal reasoning. However, these efforts have been limited by the limited difficulty of selected tasks and relatively small training scales, making it challenging to demonstrate strong multimodal reasoning abilities. To address this gap, we introduce the MMK12 dataset and MM-EUREKA with 7B and 32B parameters. The former is a high-quality multimodal mathematics reasoning dataset featuring diverse knowledge domains with human-verified answers and solution processes. The latter is a multimodal model employing rule-based reinforcement learning on MMK12, utilizing online filtering and two-stage training strategy to enhance training stability. MM-EUREKA demonstrates remarkable performance gains in multimodal mathematical reasoning, outperforming previous powerful models like InternVL2.5-78B or InternVL2.5-38B-MPO. In particular, MM-EUREKA achieves competitive or superior performance compared to both open-source and closed-source models, and trails slightly behind o1 in multidisciplinary reasoning tasks. We open-source our complete pipeline to foster further research in this area. We release all our codes, models, data, etc. at <a target="_blank" rel="noopener" href="https://github.com/ModalMinds/MM-EUREKA">https://github.com/ModalMinds/MM-EUREKA</a> </p>
<blockquote>
<p>DeepSeek R1å’Œo1å·²é€šè¿‡ç¨³å®šçš„å¤§è§„æ¨¡å¼ºåŒ–å­¦ä¹ åœ¨æ–‡æœ¬é¢†åŸŸå±•ç¤ºäº†å¼ºå¤§çš„æ¨ç†èƒ½åŠ›ã€‚ä¸ºäº†æ”¯æŒæ›´å¹¿æ³›çš„åº”ç”¨ï¼Œä¸€äº›å·¥ä½œå·²å°è¯•å°†è¿™äº›èƒ½åŠ›è½¬ç§»åˆ°å¤šæ¨¡æ€æ¨ç†ã€‚ç„¶è€Œï¼Œç”±äºæ‰€é€‰ä»»åŠ¡çš„éš¾åº¦æœ‰é™å’Œç›¸å¯¹è¾ƒå°çš„è®­ç»ƒè§„æ¨¡ï¼Œè¿™äº›åŠªåŠ›åœ¨å±•ç¤ºå¼ºå¤§çš„å¤šæ¨¡æ€æ¨ç†èƒ½åŠ›æ–¹é¢é¢ä¸´æŒ‘æˆ˜ã€‚ä¸ºäº†è§£å†³è¿™ä¸€å·®è·ï¼Œæˆ‘ä»¬æ¨å‡ºäº†MMK12æ•°æ®é›†å’ŒMM-EUREKAï¼Œåˆ†åˆ«å…·æœ‰7Bå’Œ32Bçš„å‚æ•°ã€‚å‰è€…æ˜¯ä¸€ä¸ªé«˜è´¨é‡çš„å¤šæ¨¡æ€æ•°å­¦æ¨ç†æ•°æ®é›†ï¼Œå…·æœ‰å¤šæ ·åŒ–çŸ¥è¯†é¢†åŸŸçš„äººç±»éªŒè¯ç­”æ¡ˆå’Œè§£å†³æ–¹æ¡ˆè¿‡ç¨‹ã€‚åè€…æ˜¯ä¸€ä¸ªå¤šæ¨¡æ€æ¨¡å‹ï¼Œé‡‡ç”¨åŸºäºè§„åˆ™çš„å¼ºåŒ–å­¦ä¹ åœ¨MMK12ä¸Šè¿›è¡Œè®­ç»ƒï¼Œåˆ©ç”¨åœ¨çº¿è¿‡æ»¤å’Œä¸¤é˜¶æ®µè®­ç»ƒç­–ç•¥æ¥æé«˜è®­ç»ƒç¨³å®šæ€§ã€‚MM-EUREKAåœ¨å¤šæ¨¡æ€æ•°å­¦æ¨ç†æ–¹é¢è¡¨ç°å‡ºæ˜¾è‘—çš„æ€§èƒ½æå‡ï¼Œè¶…è¶Šäº†ä¹‹å‰å¼ºå¤§çš„æ¨¡å‹ï¼Œå¦‚InternVL2.5-78Bæˆ–InternVL2.5-38B-MPOã€‚ç‰¹åˆ«æ˜¯ï¼ŒMM-EUREKAåœ¨è·¨å­¦ç§‘çš„æ¨ç†ä»»åŠ¡ä¸­ï¼Œä¸å¼€æºå’Œé—­æºæ¨¡å‹ç›¸æ¯”å…·æœ‰ç«äº‰åŠ›æˆ–æ›´ä¼˜è¶Šçš„æ€§èƒ½ï¼Œç•¥å¾®è½åäºo1ã€‚æˆ‘ä»¬å…¬å¼€äº†å®Œæ•´çš„ç®¡é“ï¼Œä»¥ä¿ƒè¿›è¯¥é¢†åŸŸçš„ç ”ç©¶ã€‚æˆ‘ä»¬å°†åœ¨<a target="_blank" rel="noopener" href="https://github.com/ModalMinds/MM-EUREKA">https://github.com/ModalMinds/MM-EUREKA</a>ä¸Šå‘å¸ƒæ‰€æœ‰ä»£ç ã€æ¨¡å‹ã€æ•°æ®ç­‰ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.07365v2">PDF</a> </p>
<p><strong>Summary</strong><br>æ–‡æœ¬ä»‹ç»äº†DeepSeek R1å’Œo1åœ¨æ–‡æœ¬é¢†åŸŸå±•ç°çš„å¼ºå¤§æ¨ç†èƒ½åŠ›ï¼Œå¹¶é€šè¿‡å¤§è§„æ¨¡å¼ºåŒ–å­¦ä¹ è¿›è¡Œå¤šä»»åŠ¡æ¨ç†ã€‚ä¸ºæ‰©å¤§åº”ç”¨èŒƒå›´ï¼Œä¸€äº›å·¥ä½œå°è¯•å°†è¿™äº›èƒ½åŠ›è½¬ç§»åˆ°å¤šæ¨¡æ€æ¨ç†ã€‚ç„¶è€Œï¼Œç”±äºæ‰€é€‰ä»»åŠ¡çš„éš¾åº¦æœ‰é™å’Œè®­ç»ƒè§„æ¨¡ç›¸å¯¹è¾ƒå°ï¼Œå±•ç¤ºå¼ºå¤§çš„å¤šæ¨¡æ€æ¨ç†èƒ½åŠ›å…·æœ‰æŒ‘æˆ˜æ€§ã€‚ä¸ºè§£å†³æ­¤é—®é¢˜ï¼Œæˆ‘ä»¬æ¨å‡ºäº†MMK12æ•°æ®é›†å’ŒMM-EUREKAæ¨¡å‹ï¼ˆå…·æœ‰7Bå’Œ32Bå‚æ•°ï¼‰ã€‚MMK12æ˜¯ä¸€ä¸ªé«˜è´¨é‡çš„å¤šæ¨¡æ€æ•°å­¦æ¨ç†æ•°æ®é›†ï¼Œæ¶µç›–äº†ä¸åŒçŸ¥è¯†é¢†åŸŸå¹¶å…·æœ‰äººå·¥éªŒè¯çš„ç­”æ¡ˆå’Œè§£å†³æ–¹æ¡ˆè¿‡ç¨‹ã€‚MM-EUREKAæ˜¯ä¸€ä¸ªå¤šæ¨¡æ€æ¨¡å‹ï¼Œé‡‡ç”¨åŸºäºè§„åˆ™çš„å¼ºåŒ–å­¦ä¹ åœ¨MMK12ä¸Šè¿›è¡Œè®­ç»ƒï¼Œä½¿ç”¨åœ¨çº¿è¿‡æ»¤å’Œä¸¤é˜¶æ®µè®­ç»ƒç­–ç•¥æé«˜è®­ç»ƒç¨³å®šæ€§ã€‚MM-EUREKAåœ¨å¤šæ¨¡æ€æ•°å­¦æ¨ç†æ–¹é¢è¡¨ç°å‡ºæ˜¾è‘—çš„æ€§èƒ½æå‡ï¼Œè¶…è¶Šäº†ä¹‹å‰å¼ºå¤§çš„æ¨¡å‹ï¼Œå¦‚InternVL2.5-78Bæˆ–InternVL2.5-38B-MPOã€‚ç‰¹åˆ«æ˜¯åœ¨è·¨å­¦ç§‘æ¨ç†ä»»åŠ¡ä¸­ï¼ŒMM-EUREKAçš„æ€§èƒ½ç•¥é€Šäºo1ã€‚æˆ‘ä»¬å…¬å¼€äº†å®Œæ•´çš„ç®¡é“ï¼Œä»¥ä¿ƒè¿›è¯¥é¢†åŸŸçš„ç ”ç©¶ã€‚æˆ‘ä»¬æ‰€æœ‰çš„ä»£ç ã€æ¨¡å‹ã€æ•°æ®ç­‰éƒ½å¯ä»¥åœ¨<a target="_blank" rel="noopener" href="https://github.com/ModalMinds/MM-EUREKA">https://github.com/ModalMinds/MM-EUREKA</a>è·å–ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>DeepSeek R1å’Œo1å·²åœ¨æ–‡æœ¬é¢†åŸŸå±•ç°å‡ºå¼ºå¤§çš„æ¨ç†èƒ½åŠ›ï¼Œå¹¶é€šè¿‡å¤§è§„æ¨¡å¼ºåŒ–å­¦ä¹ è¿›è¡Œå¤šä»»åŠ¡æ¨ç†ã€‚</li>
<li>å°†è¿™äº›èƒ½åŠ›è½¬ç§»åˆ°å¤šæ¨¡æ€æ¨ç†é¢ä¸´æŒ‘æˆ˜ï¼Œä¸»è¦ç”±äºä»»åŠ¡éš¾åº¦æœ‰é™å’Œè®­ç»ƒè§„æ¨¡è¾ƒå°ã€‚</li>
<li>MMK12æ•°æ®é›†æ˜¯ä¸€ä¸ªå¤šæ¨¡æ€æ•°å­¦æ¨ç†æ•°æ®é›†ï¼Œå…·æœ‰å¤šæ ·çŸ¥è¯†é¢†åŸŸå’Œäººå·¥éªŒè¯çš„ç­”æ¡ˆã€‚</li>
<li>MM-EUREKAæ¨¡å‹é‡‡ç”¨åŸºäºè§„åˆ™çš„å¼ºåŒ–å­¦ä¹ åœ¨MMK12ä¸Šè®­ç»ƒï¼Œåˆ©ç”¨åœ¨çº¿è¿‡æ»¤å’Œä¸¤é˜¶æ®µç­–ç•¥æé«˜ç¨³å®šæ€§ã€‚</li>
<li>MM-EUREKAåœ¨å¤šæ¨¡æ€æ•°å­¦æ¨ç†ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œè¶…è¶Šäº†ä¸€äº›å…ˆå‰çš„å¼ºå¤§æ¨¡å‹ã€‚</li>
<li>MM-EUREKAåœ¨è·¨å­¦ç§‘çš„æ¨ç†ä»»åŠ¡ä¸­æ€§èƒ½ç•¥é€Šäºo1ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.07365">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-0e51ba6bc70959c7b953508b498c7bfd.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-723069b8845379f5301d8099dba9b891.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a8cc4b559448977d985cc38b37adf97d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7047cc945f08811fff4de224ee9c2fa1.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="BioMaze-Benchmarking-and-Enhancing-Large-Language-Models-for-Biological-Pathway-Reasoning"><a href="#BioMaze-Benchmarking-and-Enhancing-Large-Language-Models-for-Biological-Pathway-Reasoning" class="headerlink" title="BioMaze: Benchmarking and Enhancing Large Language Models for Biological   Pathway Reasoning"></a>BioMaze: Benchmarking and Enhancing Large Language Models for Biological   Pathway Reasoning</h2><p><strong>Authors:Haiteng Zhao, Chang Ma, Fangzhi Xu, Lingpeng Kong, Zhi-Hong Deng</strong></p>
<p>The applications of large language models (LLMs) in various biological domains have been explored recently, but their reasoning ability in complex biological systems, such as pathways, remains underexplored, which is crucial for predicting biological phenomena, formulating hypotheses, and designing experiments. This work explores the potential of LLMs in pathway reasoning. We introduce BioMaze, a dataset with 5.1K complex pathway problems derived from real research, covering various biological contexts including natural dynamic changes, disturbances, additional intervention conditions, and multi-scale research targets. Our evaluation of methods such as CoT and graph-augmented reasoning, shows that LLMs struggle with pathway reasoning, especially in perturbed systems. To address this, we propose PathSeeker, an LLM agent that enhances reasoning through interactive subgraph-based navigation, enabling a more effective approach to handling the complexities of biological systems in a scientifically aligned manner. The dataset and code are available at <a target="_blank" rel="noopener" href="https://github.com/zhao-ht/BioMaze">https://github.com/zhao-ht/BioMaze</a>. </p>
<blockquote>
<p>æœ€è¿‘å·²ç»æ¢ç´¢äº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨å„ç§ç”Ÿç‰©é¢†åŸŸçš„åº”ç”¨ï¼Œä½†å®ƒä»¬åœ¨å¤æ‚ç”Ÿç‰©ç³»ç»Ÿï¼ˆå¦‚é€”å¾„ï¼‰ä¸­çš„æ¨ç†èƒ½åŠ›ä»ç„¶è¢«æ¢ç´¢å¾—ä¸å¤Ÿå……åˆ†ï¼Œè¿™å¯¹äºé¢„æµ‹ç”Ÿç‰©ç°è±¡ã€åˆ¶å®šå‡è®¾å’Œè®¾è®¡å®éªŒè‡³å…³é‡è¦ã€‚è¿™é¡¹å·¥ä½œæ¢ç´¢äº†LLMåœ¨é€”å¾„æ¨ç†ä¸­çš„æ½œåŠ›ã€‚æˆ‘ä»¬ä»‹ç»äº†BioMazeï¼Œè¿™æ˜¯ä¸€ä¸ªåŒ…å«5.1Kä¸ªæ¥è‡ªçœŸå®ç ”ç©¶çš„å¤æ‚é€”å¾„é—®é¢˜çš„æ•°æ®é›†ï¼Œæ¶µç›–å„ç§ç”Ÿç‰©èƒŒæ™¯ï¼ŒåŒ…æ‹¬è‡ªç„¶åŠ¨æ€å˜åŒ–ã€å¹²æ‰°ã€é¢å¤–å¹²é¢„æ¡ä»¶å’Œå¤šå°ºåº¦ç ”ç©¶ç›®æ ‡ã€‚æˆ‘ä»¬å¯¹åŒ…æ‹¬CoTå’Œå¢å¼ºå›¾æ¨ç†ç­‰æ–¹æ³•è¿›è¡Œè¯„ä¼°çš„ç»“æœè¡¨æ˜ï¼ŒLLMåœ¨é€”å¾„æ¨ç†æ–¹é¢å­˜åœ¨å›°éš¾ï¼Œå°¤å…¶æ˜¯åœ¨å—å¹²æ‰°çš„ç³»ç»Ÿä¸­ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†PathSeekerï¼Œè¿™æ˜¯ä¸€ä¸ªLLMä»£ç†ï¼Œé€šè¿‡åŸºäºäº¤äº’å­å›¾çš„å¯¼èˆªå¢å¼ºæ¨ç†èƒ½åŠ›ï¼Œä»¥æ›´ç§‘å­¦çš„æ–¹å¼æ›´æœ‰æ•ˆåœ°åº”å¯¹ç”Ÿç‰©ç³»ç»Ÿçš„å¤æ‚æ€§ã€‚æ•°æ®é›†å’Œä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/zhao-ht/BioMaze">https://github.com/zhao-ht/BioMaze</a>è·å–ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.16660v4">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨ç”Ÿç‰©é¢†åŸŸçš„åº”ç”¨å·²å—åˆ°å…³æ³¨ï¼Œä½†åœ¨å¤æ‚ç”Ÿç‰©ç³»ç»Ÿï¼ˆå¦‚é€”å¾„ï¼‰ä¸­çš„æ¨ç†èƒ½åŠ›ä»è¢«ä½ä¼°ã€‚æœ¬ç ”ç©¶æ¢ç´¢äº†LLMsåœ¨é€”å¾„æ¨ç†ä¸­çš„æ½œåŠ›ï¼Œå¹¶å¼•å…¥BioMazeæ•°æ®é›†ï¼ŒåŒ…å«5.1Kä¸ªé€”å¾„é—®é¢˜å®ä¾‹ã€‚è¯„ä¼°æ˜¾ç¤ºLLMsåœ¨å—å¹²æ‰°ç³»ç»Ÿä¸Šçš„æ¨ç†èƒ½åŠ›å—é™ï¼Œå› æ­¤æå‡ºPathSeekerï¼Œä¸€ä¸ªé€šè¿‡äº¤äº’å¼å­å›¾å¯¼èˆªå¢å¼ºæ¨ç†çš„LLMä»£ç†ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨ç”Ÿç‰©é¢†åŸŸçš„åº”ç”¨é€æ¸å—åˆ°å…³æ³¨ï¼Œç‰¹åˆ«æ˜¯åœ¨é€”å¾„æ¨ç†æ–¹é¢ã€‚</li>
<li>BioMazeæ•°æ®é›†åŒ…å«çœŸå®ç ”ç©¶ä¸­çš„å¤æ‚é€”å¾„é—®é¢˜å®ä¾‹ï¼Œæœ‰åŠ©äºè¯„ä¼°æ¨¡å‹åœ¨é€”å¾„æ¨ç†æ–¹é¢çš„è¡¨ç°ã€‚</li>
<li>LLMsåœ¨å—å¹²æ‰°ç³»ç»Ÿä¸Šçš„æ¨ç†èƒ½åŠ›å—é™ï¼Œè¿™å½±å“äº†å…¶åœ¨ç”Ÿç‰©ç³»ç»Ÿå¤æ‚æ€§çš„å¤„ç†æ•ˆæœã€‚</li>
<li>PathSeekeræ˜¯ä¸€ä¸ªLLMä»£ç†ï¼Œé€šè¿‡äº¤äº’å¼å­å›¾å¯¼èˆªå¢å¼ºæ¨ç†èƒ½åŠ›ï¼Œæ›´ç§‘å­¦åœ°åº”å¯¹ç”Ÿç‰©ç³»ç»Ÿçš„å¤æ‚æ€§ã€‚</li>
<li>BioMazeæ•°æ®é›†å’Œä»£ç å¯é€šè¿‡GitHubå…±äº«ã€‚</li>
<li>LLMsåœ¨é¢„æµ‹ç”Ÿç‰©ç°è±¡ã€å‡è®¾åˆ¶å®šå’Œå®éªŒè®¾è®¡æ–¹é¢å­˜åœ¨å·¨å¤§æ½œåŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.16660">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-453f5d5b9afb253943720d557c693bf9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f50f71bd991c3c5db24d14612a62d5ec.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4509a357c681b5b1bc065ccd88e6c93d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e616561d2bc315a4bb324499caf77181.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0015393d9eb5a90f1979e22a304189cf.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-04-20/R1_Reasoning/">https://kedreamix.github.io/Talk2Paper/Paper/2025-04-20/R1_Reasoning/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/R1-Reasoning/">
                                    <span class="chip bg-color">R1_Reasoning</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-04-20/LLM/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-1799a5c57e34d17fa4a998e98eee9508.jpg" class="responsive-img" alt="LLM">
                        
                        <span class="card-title">LLM</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            LLM æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-04-20  Lightning IR Straightforward Fine-tuning and Inference of   Transformer-based Language Models for Information Retrieval
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-04-20
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/LLM/" class="post-category">
                                    LLM
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/LLM/">
                        <span class="chip bg-color">LLM</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-04-19/Talking%20Head%20Generation/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-0070c3045e6f05ce9991f09599f8f4b4.jpg" class="responsive-img" alt="Talking Head Generation">
                        
                        <span class="card-title">Talking Head Generation</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Talking Head Generation æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-04-19  FLAP Fully-controllable Audio-driven Portrait Video Generation through   3D head conditioned diffusion model
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-04-19
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Talking-Head-Generation/" class="post-category">
                                    Talking Head Generation
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Talking-Head-Generation/">
                        <span class="chip bg-color">Talking Head Generation</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">27685.3k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
