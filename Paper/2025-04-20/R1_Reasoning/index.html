<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="R1_Reasoning">
    <meta name="description" content="R1_Reasoning 方向最新论文已更新，请持续关注 Update in 2025-04-20  Memorization vs. Reasoning Updating LLMs with New Knowledge">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>R1_Reasoning | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loader页面消失采用渐隐的方式*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logo出现动画 */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//使用渐隐的方法淡出loading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//强制显示loading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">嘘~  正在从服务器偷取页面 . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>首页</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>标签</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>分类</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>归档</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="搜索" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="深色/浅色模式" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			首页
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			标签
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			分类
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			归档
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-52b875081e85d964ca333fe9ab756775.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">R1_Reasoning</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- 文章内容详情 -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/R1-Reasoning/">
                                <span class="chip bg-color">R1_Reasoning</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/R1-Reasoning/" class="post-category">
                                R1_Reasoning
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp;
                    2025-04-20
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>更新日期:&nbsp;&nbsp;
                    2025-05-14
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>文章字数:&nbsp;&nbsp;
                    11.8k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>阅读时长:&nbsp;&nbsp;
                    48 分
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>阅读次数:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>⚠️ 以下所有内容总结都来自于 大语言模型的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p>
</blockquote>
<h1 id="2025-04-20-更新"><a href="#2025-04-20-更新" class="headerlink" title="2025-04-20 更新"></a>2025-04-20 更新</h1><h2 id="Memorization-vs-Reasoning-Updating-LLMs-with-New-Knowledge"><a href="#Memorization-vs-Reasoning-Updating-LLMs-with-New-Knowledge" class="headerlink" title="Memorization vs. Reasoning: Updating LLMs with New Knowledge"></a>Memorization vs. Reasoning: Updating LLMs with New Knowledge</h2><p><strong>Authors:Aochong Oliver Li, Tanya Goyal</strong></p>
<p>Large language models (LLMs) encode vast amounts of pre-trained knowledge in their parameters, but updating them as real-world information evolves remains a challenge. Existing methodologies and benchmarks primarily target entity substitutions, failing to capture the full breadth of complex real-world dynamics. In this paper, we introduce Knowledge Update Playground (KUP), an automatic pipeline for simulating realistic knowledge updates reflected in an evidence corpora. KUP’s evaluation framework includes direct and indirect probes to both test memorization of updated facts and reasoning over them, for any update learning methods. Next, we present a lightweight method called memory conditioned training (MCT), which conditions tokens in the update corpus on self-generated “memory” tokens during training. Our strategy encourages LLMs to surface and reason over newly memorized knowledge at inference. Our results on two strong LLMs show that (1) KUP benchmark is highly challenging, with the best CPT models achieving $&lt;2%$ in indirect probing setting (reasoning) and (2) MCT training significantly outperforms prior continued pre-training (CPT) baselines, improving direct probing (memorization) results by up to $25.4%$. </p>
<blockquote>
<p>大型语言模型（LLM）在其参数中编码了大量的预训练知识，但随着现实世界信息的演变，如何更新这些知识仍然是一个挑战。现有的方法和基准测试主要关注实体替换，无法捕捉现实世界动态的全面广度。在本文中，我们介绍了知识更新游乐场（KUP），这是一个自动管道，用于模拟反映在证据语料库中的现实知识更新。KUP的评估框架包括直接和间接探针，用于测试任何更新学习方法中对更新事实的记忆和推理。接下来，我们提出了一种轻量级的方法，称为记忆条件训练（MCT），该方法在训练期间将更新语料库中的令牌设置为自我生成的“记忆”令牌的条件。我们的策略鼓励大型语言模型在推理时展现并推理新记忆的知识。我们在两个强大的大型语言模型上的结果表明，（1）KUP基准测试极具挑战性，最好的CPT模型在间接探测设置（推理）中达到&lt;2%；（2）MCT训练显著优于先前的持续预训练（CPT）基线，直接探测（记忆）结果最多提高了25.4%。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.12523v1">PDF</a> 9 pages, 3 figures</p>
<p><strong>Summary</strong></p>
<p>本文介绍了知识更新游乐场（KUP）这一自动管道，用于模拟反映于证据语料库中的现实知识更新。KUP评估框架包括直接和间接探针，以测试更新事实的记忆和对其的推理。此外，提出了一种轻量级方法——记忆条件训练（MCT），在训练过程中将更新语料库中的令牌置于自我生成的“记忆”令牌上。策略鼓励大型语言模型在推理时唤起并推理新记忆的知识。在强大的大型语言模型上的结果表明，KUP基准测试具有挑战性，而MCT训练显著优于先前的持续预训练（CPT）基准测试，直接探测（记忆）结果最多可提高25.4%。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>知识更新游乐场（KUP）是一个自动管道，旨在模拟现实知识更新并在证据语料库中反映这些更新。</li>
<li>KUP评估框架通过直接和间接探针测试大型语言模型（LLM）对更新知识的记忆和推理能力。</li>
<li>现有方法主要关注实体替换，无法捕捉复杂现实动态的全貌。</li>
<li>引入了一种轻量级方法——记忆条件训练（MCT），在训练过程中使用自我生成的“记忆”令牌。<br>5.MCT鼓励LLM在推理时利用新记忆的知识。</li>
<li>在强大的LLM上的实验结果表明，KUP基准测试具有挑战性，间接探针设置中的最佳CPT模型仅达到&lt;2%。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.12523">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-7bb226a1ebc102c2eab31c9d4af6972b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e43aaad51b455d13ff8be1f38f70f37e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-591c8d532ba52a91aa5b4c5d14d5b1f6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2663941ad7ed579ad97bfd69153eb1fe.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Robust-Reinforcement-Learning-from-Human-Feedback-for-Large-Language-Models-Fine-Tuning"><a href="#Robust-Reinforcement-Learning-from-Human-Feedback-for-Large-Language-Models-Fine-Tuning" class="headerlink" title="Robust Reinforcement Learning from Human Feedback for Large Language   Models Fine-Tuning"></a>Robust Reinforcement Learning from Human Feedback for Large Language   Models Fine-Tuning</h2><p><strong>Authors:Kai Ye, Hongyi Zhou, Jin Zhu, Francesco Quinzan, Chengchung Shi</strong></p>
<p>Reinforcement learning from human feedback (RLHF) has emerged as a key technique for aligning the output of large language models (LLMs) with human preferences. To learn the reward function, most existing RLHF algorithms use the Bradley-Terry model, which relies on assumptions about human preferences that may not reflect the complexity and variability of real-world judgments. In this paper, we propose a robust algorithm to enhance the performance of existing approaches under such reward model misspecifications. Theoretically, our algorithm reduces the variance of reward and policy estimators, leading to improved regret bounds. Empirical evaluations on LLM benchmark datasets demonstrate that the proposed algorithm consistently outperforms existing methods, with 77-81% of responses being favored over baselines on the Anthropic Helpful and Harmless dataset. </p>
<blockquote>
<p>强化学习从人类反馈（RLHF）已经成为将大型语言模型（LLM）的输出与人类偏好对齐的关键技术。为了学习奖励函数，大多数现有的RLHF算法使用Bradley-Terry模型，该模型依赖于可能无法反映现实世界判断复杂性和可变性的人类偏好假设。在本文中，我们提出了一种稳健的算法，以提高在这种奖励模型错误指定情况下现有方法的性能。从理论上讲，我们的算法降低了奖励和政策估算器的方差，从而提高了后悔界。在LLM基准数据集上的经验评估表明，所提算法始终优于现有方法，在Anthropic有益和无害数据集上，有7 修丞研究的关注度技术则越来越受到业界的关注和研究者的重视。这一技术的出现使得机器学习模型能够更好地理解人类用户的需求和意图，从而更好地满足用户的需求并提供更个性化的服务。例如，随着这一技术的发展，语音助手能够更准确地理解用户的语音指令并执行相应的操作；智能客服机器人能够更好地理解用户的问题并提供更精准的解答和帮助。在自然语言处理领域的应用之外，这一技术也在其他领域得到了广泛的应用。例如，在金融领域，基于强化学习的智能交易系统可以根据市场数据和用户偏好进行自动交易决策；在医疗领域，强化学习也被应用于诊断和疾病管理等方面。总之，随着机器学习领域中对强化学习与人类反馈相结合的技术应用的不断深化和扩展，这将为未来带来无限可能的新场景和新技术革命。目前机器学习模型如GPT系列虽然已经有了很好的成绩但仍有很多问题值得探讨。通过深入研究强化学习从人类反馈的技术并与其他技术相结合相信未来会涌现出更多有趣而实用的应用场景推动技术的进步和发展。我们相信未来基于强化学习的自适应智能系统将成为人工智能领域的重要发展方向之一为人类带来更加智能便捷的生活体验。我们相信这一领域的研究将继续深入并取得更多的突破性进展从而不断提升人类社会的智能化水平并实现科技的可持续发展。，相比基线有八修的改进成果显示出算法的高效率与优越性。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.03784v3">PDF</a> </p>
<p><strong>Summary</strong>：强化学习从人类反馈（RLHF）已成为将大型语言模型（LLM）的输出与人类偏好对齐的关键技术。大多数现有的RLHF算法使用布拉德利-特里模型来学习奖励函数，这依赖于可能无法反映现实世界中判断和复杂性的假设。本文提出了一种稳健的算法，以提高在奖励模型误差下的现有方法性能。理论上，该算法降低了奖励和政策估计量的方差，提高了后悔界。在LLM基准数据集上的经验评估表明，该算法始终优于现有方法，在Anthropic有益和无害数据集上，77-81%的响应优于基线。</p>
<p><strong>Key Takeaways</strong>：</p>
<ol>
<li>强化学习从人类反馈（RLHF）技术用于对齐大型语言模型（LLM）输出与人类偏好。</li>
<li>现有RLHF算法主要使用布拉德利-特里模型，存在对人类偏好假设的局限性。</li>
<li>本文提出了一种新的稳健算法，旨在提高在奖励模型误差下的现有方法性能。</li>
<li>新算法理论上降低了奖励和政策估计量的方差，提高了后悔界。</li>
<li>实证评估表明，新算法在LLM基准数据集上表现优于现有方法。</li>
<li>在Anthropic有益和无害数据集上，新算法的响应优于基线，达到77-81%。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.03784">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-8b003af875b42231da6af5a6acb615ca.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-7f9b51ef57f8058197bf40683aa875f1.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-cce2ff1466f9e978a0ab1daaf67a66a3.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="OnRL-RAG-Real-Time-Personalized-Mental-Health-Dialogue-System"><a href="#OnRL-RAG-Real-Time-Personalized-Mental-Health-Dialogue-System" class="headerlink" title="OnRL-RAG: Real-Time Personalized Mental Health Dialogue System"></a>OnRL-RAG: Real-Time Personalized Mental Health Dialogue System</h2><p><strong>Authors:Ahsan Bilal, Beiyu Lin</strong></p>
<p>Large language models (LLMs) have been widely used for various tasks and applications. However, LLMs and fine-tuning are limited to the pre-trained data. For example, ChatGPT’s world knowledge until 2021 can be outdated or inaccurate. To enhance the capabilities of LLMs, Retrieval-Augmented Generation (RAG), is proposed to augment LLMs with additional, new, latest details and information to LLMs. While RAG offers the correct information, it may not best present it, especially to different population groups with personalizations. Reinforcement Learning from Human Feedback (RLHF) adapts to user needs by aligning model responses with human preference through feedback loops. In real-life applications, such as mental health problems, a dynamic and feedback-based model would continuously adapt to new information and offer personalized assistance due to complex factors fluctuating in a daily environment. Thus, we propose an Online Reinforcement Learning-based Retrieval-Augmented Generation (OnRL-RAG) system to detect and personalize the responding systems to mental health problems, such as stress, anxiety, and depression. We use an open-source dataset collected from 2028 College Students with 28 survey questions for each student to demonstrate the performance of our proposed system with the existing systems. Our system achieves superior performance compared to standard RAG and simple LLM via GPT-4o, GPT-4o-mini, Gemini-1.5, and GPT-3.5. This work would open up the possibilities of real-life applications of LLMs for personalized services in the everyday environment. The results will also help researchers in the fields of sociology, psychology, and neuroscience to align their theories more closely with the actual human daily environment. </p>
<blockquote>
<p>大型语言模型（LLM）已广泛应用于各种任务和应用。然而，LLM和微调都受限于预训练数据。例如，ChatGPT截至2021年的世界知识可能会过时或不准确。为了增强LLM的能力，提出了检索增强生成（RAG）来向LLM添加额外、最新、最新的细节和信息。虽然RAG提供了正确的信息，但它可能无法最好地呈现它，尤其是对于具有个性化的不同人群。强化学习从人类反馈（RLHF）通过反馈循环使模型响应与人类偏好相适应，从而适应用户需求。在现实生活应用，如心理健康问题中，一个动态且基于反馈的模型将不断适应新信息，并提供个性化的帮助，这是由于日常环境中复杂因素的波动。因此，我们提出了基于在线强化学习的检索增强生成（OnRL-RAG）系统，用于检测和个性化应对心理健康问题，如压力、焦虑和抑郁。我们使用从2028名大学生收集的开源数据集进行演示，每个学生回答了28个问题，以展示我们提出的系统与现有系统的性能。我们的系统相较于标准RAG和简单的LLM（如GPT-4o、GPT-4o-mini、Gemini-1.5和GPT-3.5）表现出卓越的性能。这项工作将为LLM在个性化服务方面的现实生活应用开辟可能性，日常环境中的应用。此外，该研究还将帮助社会学、心理学和神经科学领域的研究人员将他们的理论与实际人类日常环境更加紧密地结合起来。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.02894v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>大型语言模型（LLMs）广泛应用于各种任务和应用，但其知识和能力受限于预训练数据。为增强LLMs的能力，提出了检索增强生成（RAG）方法，但RAG可能无法最佳地呈现信息，尤其对不同人群个性化的需求。因此，结合强化学习人类反馈（RLHF）的在线强化学习基于检索增强生成（OnRL-RAG）系统被提出，以检测并个性化响应精神健康问题，如压力、焦虑和抑郁。使用从28所高校收集的开源数据集，展示了该系统的优越性。这一研究为LLMs在个性化服务方面的实际应用打开了可能性，并将帮助社会学、心理学和神经科学领域的研究者更好地将理论应用于实际环境。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>大型语言模型（LLMs）受限于预训练数据的知识和能力。</li>
<li>检索增强生成（RAG）方法用于增强LLMs的能力，提供额外的最新信息。</li>
<li>RAG可能无法最佳地呈现信息，尤其对不同人群的个性化需求。</li>
<li>提出结合强化学习人类反馈（RLHF）的在线强化学习基于检索增强生成（OnRL-RAG）系统。</li>
<li>OnRL-RAG系统用于检测并个性化响应精神健康问题，如压力、焦虑和抑郁。</li>
<li>使用从高校收集的开源数据集展示了OnRL-RAG系统的优越性。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.02894">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-6c8a98243aaff6eae1eeb5298102889b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-7c7fee3d6574c7b44579867702d433e4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ec1ff3bb0a697938e71c8865e7f52fc9.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="GPG-A-Simple-and-Strong-Reinforcement-Learning-Baseline-for-Model-Reasoning"><a href="#GPG-A-Simple-and-Strong-Reinforcement-Learning-Baseline-for-Model-Reasoning" class="headerlink" title="GPG: A Simple and Strong Reinforcement Learning Baseline for Model   Reasoning"></a>GPG: A Simple and Strong Reinforcement Learning Baseline for Model   Reasoning</h2><p><strong>Authors:Xiangxiang Chu, Hailang Huang, Xiao Zhang, Fei Wei, Yong Wang</strong></p>
<p>Reinforcement Learning (RL) can directly enhance the reasoning capabilities of large language models without extensive reliance on Supervised Fine-Tuning (SFT). In this work, we revisit the traditional Policy Gradient (PG) mechanism and propose a minimalist RL approach termed Group Policy Gradient (GPG). Unlike conventional methods, GPG directly optimize the original RL objective, thus obviating the need for surrogate loss functions. By eliminating the critic and reference models, avoiding KL divergence constraints, and addressing the advantage and gradient estimation bias, our approach significantly simplifies the training process compared to Group Relative Policy Optimization (GRPO). Our approach achieves superior performance without relying on auxiliary techniques or adjustments. As illustrated in Figure 1, extensive experiments demonstrate that our method not only reduces computational costs but also consistently outperforms GRPO across various unimodal and multimodal tasks. Our code is available at <a target="_blank" rel="noopener" href="https://github.com/AMAP-ML/GPG">https://github.com/AMAP-ML/GPG</a>. </p>
<blockquote>
<p>强化学习（RL）可以直接提升大语言模型的推理能力，而无需过多依赖监督微调（SFT）。在这项工作中，我们重新审视了传统的策略梯度（PG）机制，并提出了一种极简的RL方法，称为组策略梯度（GPG）。不同于传统方法，GPG直接优化原始的RL目标，从而无需替代损失函数。通过消除评论家模型和参考模型，避免KL散度约束，并解决优势和梯度估计偏差的问题，我们的方法大大简化了与组相对策略优化（GRPO）相比的训练过程。我们的方法在不依赖辅助技术或调整的情况下实现了卓越的性能。如图1所示，大量实验证明，我们的方法不仅降低了计算成本，而且在各种单模态和多模态任务上始终优于GRPO。我们的代码可在<a target="_blank" rel="noopener" href="https://github.com/AMAP-ML/GPG%E4%B8%8A%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/AMAP-ML/GPG上找到。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.02546v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>强化学习（RL）可以直接提升大语言模型的推理能力，无需过多依赖监督微调（SFT）。本研究重新审视了传统的策略梯度（PG）机制，并提出了一种极简的RL方法——群组策略梯度（GPG）。GPG直接优化原始的RL目标，从而无需替代损失函数。通过消除评论家模型和参考模型，避免KL散度约束，并解决优势和梯度估计偏差问题，GPG简化了与群组相对策略优化（GRPO）相比的训练过程。在不依赖辅助技术或调整的情况下，我们的方法实现了卓越的性能。如图1所示，大量实验证明，我们的方法不仅降低了计算成本，而且在各种单模态和多模态任务上始终优于GRPO。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>强化学习（RL）能够增强大语言模型的推理能力，且不需要过度依赖监督微调。</li>
<li>提出了一种新的RL方法——群组策略梯度（GPG），该方法直接优化原始的RL目标。</li>
<li>GPG简化了训练过程，与其他方法相比，它不需要使用替代损失函数、评论家模型和参考模型。</li>
<li>GPG解决了优势和梯度估计偏差问题。</li>
<li>GPG在多种任务上表现出卓越性能，包括单模态和多模态任务。</li>
<li>GPG方法降低了计算成本。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.02546">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-96d443e6632ee50810fe008b9f05c935.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b54a2d1843508cb20fbb024aeea7c04b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-dd74a39c41367cbdd9c94aee2c56beae.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b303edd1c6c88e2bbd9d342c7c4fd646.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-7dc6170186382993f0f806dfaa2836eb.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Improved-Visual-Spatial-Reasoning-via-R1-Zero-Like-Training"><a href="#Improved-Visual-Spatial-Reasoning-via-R1-Zero-Like-Training" class="headerlink" title="Improved Visual-Spatial Reasoning via R1-Zero-Like Training"></a>Improved Visual-Spatial Reasoning via R1-Zero-Like Training</h2><p><strong>Authors:Zhenyi Liao, Qingsong Xie, Yanhao Zhang, Zijian Kong, Haonan Lu, Zhenyu Yang, Zhijie Deng</strong></p>
<p>Increasing attention has been placed on improving the reasoning capacities of multi-modal large language models (MLLMs). As the cornerstone for AI agents that function in the physical realm, video-based visual-spatial intelligence (VSI) emerges as one of the most pivotal reasoning capabilities of MLLMs. This work conducts a first, in-depth study on improving the visual-spatial reasoning of MLLMs via R1-Zero-like training. Technically, we first identify that the visual-spatial reasoning capacities of small- to medium-sized Qwen2-VL models cannot be activated via Chain of Thought (CoT) prompts. We then incorporate GRPO training for improved visual-spatial reasoning, using the carefully curated VSI-100k dataset, following DeepSeek-R1-Zero. During the investigation, we identify the necessity to keep the KL penalty (even with a small value) in GRPO. With just 120 GPU hours, our vsGRPO-2B model, fine-tuned from Qwen2-VL-2B, can outperform the base model by 12.1% and surpass GPT-4o. Moreover, our vsGRPO-7B model, fine-tuned from Qwen2-VL-7B, achieves performance comparable to that of the best open-source model LLaVA-NeXT-Video-72B. Additionally, we compare vsGRPO to supervised fine-tuning and direct preference optimization baselines and observe strong performance superiority. The code and dataset will be available soon. </p>
<blockquote>
<p>近年来，提高多模态大型语言模型（MLLMs）的推理能力越来越受到关注。作为在物理领域发挥作用的AI代理的基石，基于视频的视觉空间智能（VSI）被认为是MLLMs中最关键的推理能力之一。本研究首次深入探讨了通过R1-Zero类似的训练提高MLLMs的视觉空间推理能力。从技术上讲，我们首先发现小型到中型Qwen2-VL模型的视觉空间推理能力无法通过思维链（CoT）提示来激活。然后，我们采用GRPO训练法来提高视觉空间推理能力，使用精心制作的VSI-100k数据集，遵循DeepSeek-R1-Zero。在研究过程中，我们发现需要在GRPO中保持KL惩罚（即使值很小）。只需120个GPU小时，我们的vsGRPO-2B模型，在Qwen2-VL-2B的基础上进行了微调，性能比基础模型提高了12.1%，并超越了GPT-4o。此外，我们的vsGRPO-7B模型，在Qwen2-VL-7B的基础上进行微调，其性能与最佳开源模型LLaVA-NeXT-Video-72B相当。此外，我们将vsGRPO与监督微调法和直接偏好优化基准线进行了比较，并观察到其性能优势显著。代码和数据集将很快可用。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.00883v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<pre><code>随着多模态大型语言模型（MLLMs）的推理能力日益受到关注，基于视频的空间视觉智能（VSI）作为实体AI代理的核心，成为了MLLMs的关键推理能力之一。本研究通过R1-Zero类似的训练首次深入探讨了增强MLLMs的视觉空间推理能力。研究中发现中小型Qwen2-VL模型的视觉空间推理能力无法通过思维链（CoT）提示激活，于是采用了GRPO训练结合精心筛选的VSI-100k数据集以增强其视觉空间推理能力，并认同了在GRPO中保持KL惩罚的必要性。研究结果显示，使用GPU仅120小时，vsGRPO-2B模型性能优于基础模型达12.1%，超越GPT-4o；而vsGRPO-7B模型则实现了与顶尖开源模型LLaVA-NeXT-Video-72B相当的绩效。研究还将vsGRPO与监督微调及直接偏好优化基线进行了比较，显示出其卓越性能。代码和数据集即将公布。
 
**Key Takeaways**

一、视频型视觉空间智能是AI实体代理在物理领域中的核心技能之一，对多模态大型语言模型（MLLMs）的推理能力至关重要。
二、研究使用GRPO训练方法提高MLLMs的视觉空间推理能力，通过精心筛选的VSI-100k数据集进行训练。
三、研究发现中小型Qwen2-VL模型的视觉空间推理能力无法通过思维链（CoT）提示激活。
四、KL惩罚在GRPO训练中必要，对模型的性能影响显著。
五、使用仅有限的计算资源（GPU仅运行120小时），vsGRPO训练模型性能显著提高，超越了一些现有模型。
六、研究验证了vsGRPO训练方法的优越性，与监督微调及直接偏好优化基线相比表现突出。
七、代码和数据集即将公开以供研究使用。
</code></pre>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.00883">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-24187fd5878a2449cb5f38bf3d051919.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-52b875081e85d964ca333fe9ab756775.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-225103d2558ab9f0b87ad4f111004969.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-1b49972410f36410ae3a781010c0dff3.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f219457257c1f643f459b5654ef8d396.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-93d69a0939107ecec5ba50057a730b1a.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="UI-R1-Enhancing-Action-Prediction-of-GUI-Agents-by-Reinforcement-Learning"><a href="#UI-R1-Enhancing-Action-Prediction-of-GUI-Agents-by-Reinforcement-Learning" class="headerlink" title="UI-R1: Enhancing Action Prediction of GUI Agents by Reinforcement   Learning"></a>UI-R1: Enhancing Action Prediction of GUI Agents by Reinforcement   Learning</h2><p><strong>Authors:Zhengxi Lu, Yuxiang Chai, Yaxuan Guo, Xi Yin, Liang Liu, Hao Wang, Han Xiao, Shuai Ren, Guanjing Xiong, Hongsheng Li</strong></p>
<p>The recent DeepSeek-R1 has showcased the emergence of reasoning capabilities in LLMs through reinforcement learning (RL) with rule-based rewards. Despite its success in language models, its application in multi-modal domains, particularly in graphic user interface (GUI) agent tasks, remains under-explored. To address this issue, we propose UI-R1, the first framework to explore how rule-based RL can enhance the reasoning capabilities of multimodal large language models (MLLMs) for GUI action prediction tasks. Specifically, UI-R1 introduces a novel rule-based action reward, enabling model optimization via policy-based algorithms such as Group Relative Policy Optimization (GRPO). For efficient training, we curate a small yet high-quality dataset of 136 challenging tasks, encompassing five common action types on mobile devices. Experimental results demonstrate that our proposed UI-R1-3B achieves significant improvements over the base model (i.e. Qwen2.5-VL-3B) on both in-domain (ID) and out-of-domain (OOD) tasks, with average accuracy gains of 22.1% on ScreenSpot, 6.0% on ScreenSpot-Pro, and 12.7% on ANDROIDCONTROL. Furthermore, UI-R1-3B delivers competitive performance compared to larger models (e.g., OS-Atlas-7B) trained via supervised fine-tuning (SFT) on 76K samples. These results underscore the potential of rule-based reinforcement learning to advance GUI understanding and control, paving the way for future research in this domain. Code website: <a target="_blank" rel="noopener" href="https://github.com/lll6gg/UI-R1">https://github.com/lll6gg/UI-R1</a>. </p>
<blockquote>
<p>最近，DeepSeek-R1展示了通过基于规则的奖励强化学习（RL）在大语言模型（LLM）中出现的推理能力。尽管其在语言模型方面取得了成功，但在多模态领域，特别是在图形用户界面（GUI）代理任务中的应用仍然未被充分探索。为了解决这个问题，我们提出了UI-R1，这是第一个探索基于规则的RL如何增强多模态大型语言模型（MLLM）的推理能力以执行GUI动作预测任务的框架。具体来说，UI-R1引入了一种新型的基于动作的规则奖励，通过基于策略算法（如Group Relative Policy Optimization（GRPO））优化模型。为了进行有效的训练，我们整理了一个包含136个具有挑战性的任务的小型高质量数据集，涵盖移动设备上的五种常见动作类型。实验结果表明，我们提出的UI-R1-3B相较于基准模型（即Qwen2.5-VL-3B）在领域内（ID）和领域外（OOD）的任务上都取得了显著的提升，在ScreenSpot上的平均精度提高了22.1%，在ScreenSpot-Pro上提高了6.0%，在ANDROIDCONTROL上提高了12.7%。此外，与在7.6万个样本上通过监督微调（SFT）训练的更大模型（如OS-Atlas-7B）相比，UI-R1-3B表现出具有竞争力的性能。这些结果突显了基于规则的强化学习在GUI理解和控制方面的潜力，为未来的研究铺平了道路。相关代码网站为：<a target="_blank" rel="noopener" href="https://github.com/lll6gg/UI-R1%E3%80%82">https://github.com/lll6gg/UI-R1。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.21620v3">PDF</a> </p>
<p><strong>Summary</strong><br>    近期DeepSeek-R1展示了强化学习（RL）通过基于规则的奖励在大型语言模型（LLM）中推动推理能力的发展。然而，其在多模态领域，特别是在图形用户界面（GUI）代理任务中的应用仍待探索。为此，我们提出UI-R1框架，首次探索基于规则的强化学习如何增强多模态大型语言模型（MLLMs）在GUI动作预测任务中的推理能力。实验结果表明，UI-R1在特定数据集上较基础模型有显著改进，并在不同任务上展现出强大的性能。此外，UI-R1模型的性能表现甚至可与采用监督微调的大型模型相比。这预示着基于规则的强化学习在GUI理解和控制方面的巨大潜力。代码公开于GitHub网站。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>DeepSeek-R1展示了强化学习在大型语言模型中的推理能力。</li>
<li>基于规则的强化学习在多模态领域的应用，特别是在GUI代理任务中，仍然是一个未充分探索的领域。</li>
<li>UI-R1框架被提出用于增强多模态大型语言模型在GUI动作预测任务中的推理能力。</li>
<li>UI-R1引入了一种新的基于规则的行动奖励，通过策略优化算法（如GRPO）进行模型优化。</li>
<li>实验结果显示，UI-R1在特定数据集上的性能较基础模型有显著改善，并在不同任务上展现出显著优势。</li>
<li>UI-R1的性能与采用监督微调的大型模型相比具有竞争力。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.21620">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-57572e7a13f08dc88cf8256224ce8c37.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-759ab689c47377f1d5f7ec8d41fab7e2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7888ec58d54be02625496730e97f7c0e.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="FastCuRL-Curriculum-Reinforcement-Learning-with-Progressive-Context-Extension-for-Efficient-Training-R1-like-Reasoning-Models"><a href="#FastCuRL-Curriculum-Reinforcement-Learning-with-Progressive-Context-Extension-for-Efficient-Training-R1-like-Reasoning-Models" class="headerlink" title="FastCuRL: Curriculum Reinforcement Learning with Progressive Context   Extension for Efficient Training R1-like Reasoning Models"></a>FastCuRL: Curriculum Reinforcement Learning with Progressive Context   Extension for Efficient Training R1-like Reasoning Models</h2><p><strong>Authors:Mingyang Song, Mao Zheng, Zheng Li, Wenjie Yang, Xuan Luo, Yue Pan, Feng Zhang</strong></p>
<p>Improving the training efficiency remains one of the most significant challenges in large-scale reinforcement learning. In this paper, we investigate how the model’s context length and the complexity of the training dataset influence the training process of R1-like models. Our experiments reveal three key insights: (1) adopting longer context lengths may not necessarily result in better performance; (2) selecting an appropriate context length helps mitigate entropy collapse; and (3) appropriately controlling the model’s context length and curating training data based on input prompt length can effectively improve RL training efficiency, achieving better performance with shorter thinking length. Inspired by these insights, we propose FastCuRL, a curriculum reinforcement learning framework with the progressive context extension strategy, and successfully accelerate the training process of RL models. Experimental results demonstrate that FastCuRL-1.5B-Preview surpasses DeepScaleR-1.5B-Preview across all five benchmarks while only utilizing 50% of training steps. Furthermore, all training stages for FastCuRL-1.5B-Preview are completed using a single node with 8 GPUs. </p>
<blockquote>
<p>在大规模强化学习中，提高训练效率仍然是最大的挑战之一。在本文中，我们研究了模型的上下文长度和训练数据集复杂度对R1类似模型训练过程的影响。我们的实验揭示了三个关键见解：（1）采用更长的上下文长度并不一定能够带来更好的性能；（2）选择合适的上下文长度有助于缓解熵崩溃；（3）适当地控制模型的上下文长度，并根据输入提示长度筛选训练数据，可以有效地提高RL训练效率，以更短的思考长度实现更好的性能。受到这些见解的启发，我们提出了FastCuRL，这是一个带有渐进式上下文扩展策略的课程强化学习框架，成功加速了RL模型的训练过程。实验结果表明，FastCuRL-1.5B-Preview在所有五个基准测试中均超越了DeepScaleR-1.5B-Preview，而且仅使用了50%的训练步骤。此外，FastCuRL-1.5B-Preview的所有训练阶段均使用单个节点和8个GPU完成。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.17287v2">PDF</a> Ongoing Work</p>
<p><strong>Summary</strong></p>
<p>大型强化学习训练效率的提升是一大挑战。本文通过实验探讨了模型上下文长度与训练数据集复杂度对R1类模型训练过程的影响，得到以下三个关键见解：一是增加上下文长度不一定能提高性能；二是选择适当的上下文长度有助于缓解熵崩溃；三是通过控制模型上下文长度并根据输入提示长度整理训练数据，可以有效提高强化学习训练效率，实现更短的思考长度下更好的性能。基于这些见解，本文提出了带有渐进式上下文扩展策略的FastCuRL强化学习框架，成功加速了RL模型的训练过程。实验结果显示，FastCuRL-1.5B-Preview在所有五个基准测试中均超过了DeepScaleR-1.5B-Preview，且其所有训练阶段均使用单个节点8 GPU完成。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>模型上下文长度与训练数据集复杂度对强化学习模型训练效率有影响。</li>
<li>增加上下文长度并不一定提升模型性能。</li>
<li>选择合适的上下文长度有助于缓解熵崩溃问题。</li>
<li>控制模型上下文长度并整理训练数据能有效提高强化学习训练效率。</li>
<li>FastCuRL框架通过渐进式上下文扩展策略成功加速了RL模型的训练。</li>
<li>FastCuRL-1.5B-Preview在多个基准测试中表现优于DeepScaleR-1.5B-Preview。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.17287">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-b2bbd68ecb85518841fa7fdd786431cb.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-91befed409cf747ec90504448b6e5396.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8b213f235c82011051470824a2b64968.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-47e5bb6873fca799fd2a6e6b8832cf31.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ff46b00a5df7fa82d528d3936a7a9081.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-797efee412d68ebf065d1aa436b0af36.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-efb4ea27859c08e3503d5f8ac99735be.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-37b67cfd031de900c301d49016fe5675.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Think-or-Not-Think-A-Study-of-Explicit-Thinking-in-Rule-Based-Visual-Reinforcement-Fine-Tuning"><a href="#Think-or-Not-Think-A-Study-of-Explicit-Thinking-in-Rule-Based-Visual-Reinforcement-Fine-Tuning" class="headerlink" title="Think or Not Think: A Study of Explicit Thinking in Rule-Based Visual   Reinforcement Fine-Tuning"></a>Think or Not Think: A Study of Explicit Thinking in Rule-Based Visual   Reinforcement Fine-Tuning</h2><p><strong>Authors:Ming Li, Jike Zhong, Shitian Zhao, Yuxiang Lai, Kaipeng Zhang</strong></p>
<p>This paper investigates the thinking process in rule-based reinforcement learning fine-tuning (RFT) for multi-modal large language models (MLLMs). We first propose CLS-RL for classification, using verifiable rewards to encourage MLLM thinking. Experiments show CLS-RL significantly outperforms SFT and yields a ‘free-lunch’ generalization effect (improving performance on unseen datasets after training on one dataset). We then question if this explicit thinking is always necessary for RFT. Challenging convention that explicit thinking is crucial for RFT, we introduce No-Thinking-RL, minimizing thinking via a simple equality accuracy reward. Experiments show No-Thinking-RL surpasses CLS-RL in in-domain and generalization abilities, with significantly less fine-tuning time. This suggests reducing thinking can improve MLLM fine-tuning efficiency and effectiveness for certain visual tasks. We hypothesize explicit thinking negatively impacts reward convergence during RFT. To test this, we propose the Think-After-Answerwer method to let models first output the answer and then generate thinking process to alliviate the negative impact of thinking. We further test No-Thinking-RL on diverse tasks (including math, spatial, puzzles) with 2B and 7B models. For 2B models, No-Thinking-RL outperforms thinking-based RFT for all tasks, even on math, with Think-After-Answerwer performing intermediately. For 7B models, performance is comparable on simple visual tasks, but RFT with thinking excels on complex reasoning (math). This implies when dealing with complex math problems, smaller models struggle with generating effective reasoning, hurting performance on complex tasks. Conversely, for simple visual tasks, thinking is not indispensable, and its removal can boost performance and reduce training time. We hope our findings offer insights for better understanding the effect of the thinking process in RFT. </p>
<blockquote>
<p>本文探讨了基于规则强化学习微调（RFT）在多模态大型语言模型（MLLMs）中的思考过程。我们首先提出用于分类的CLS-RL，使用可验证的奖励来鼓励MLLM思考。实验表明，CLS-RL显著优于SFT，并产生了“免费午餐”的泛化效果（在单个数据集上进行训练后，在未见数据集上提高了性能）。然后，我们质疑这种明确的思考对于RFT是否始终必要。我们挑战认为明确思考对于RFT至关重要的传统观念，并引入了无思考强化学习（No-Thinking-RL），通过简单的等式准确性奖励来减少思考。实验表明，在领域内部和泛化能力方面，无思考强化学习超越了CLS-RL，并且微调时间大大减少。这表明在某些视觉任务中，减少思考可以提高MLLM微调的有效性和效率。我们假设明确的思考会对RFT期间的奖励收敛产生负面影响。为了测试这一点，我们提出了Think-After-Answerwer方法，让模型先给出答案，然后生成思考过程，以减轻思考带来的负面影响。我们还进一步在不同的任务（包括数学、空间、谜题）上测试了无思考强化学习，使用了2B和7B模型。对于2B模型，无思考强化学习在所有任务上的表现都优于基于思考的RFT，甚至在数学上也如此，而Think-After-Answerwer的表现处于中间水平。对于7B模型，在简单的视觉任务上表现相当，但在复杂推理（数学）方面，带有思考的RFT表现更好。这意味着在处理复杂的数学问题时，较小的模型在生成有效推理方面遇到困难，影响了复杂任务的性能。相反，对于简单的视觉任务，思考并不是必不可少的，而且移除思考可以提高性能并减少训练时间。我们希望我们的研究能为更好地理解RFT中思考过程的影响提供启示。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.16188v3">PDF</a> Preprint, work in progress. Add results on math, cvbench, and puzzle</p>
<p><strong>Summary</strong></p>
<p>这篇论文探讨了基于规则的强化学习微调（RFT）在多模态大型语言模型（MLLMs）中的思考过程。研究首先提出了用于分类的CLS-RL方法，使用可验证的奖励来鼓励MLLM思考。实验表明CLS-RL显著优于SFT，并产生了“免费午餐”式的泛化效果。然而，研究也质疑RFT是否总是需要明确的思考。通过引入不需要思考的No-Thinking-RL方法，该方法通过简单的准确性奖励来最小化思考，实验表明其在域内和泛化能力上超越了CLS-RL，且微调时间大大减少。这表明在某些视觉任务中，减少思考可以提高MLLM的微调效率和效果。同时提出假设认为明确思考会影响奖励收敛过程，并提出Think-After-Answerwer方法来减轻其负面影响。通过对不同任务的测试表明，对于简单视觉任务，不需要思考可以提高性能和减少训练时间；而对于复杂任务如数学推理，思考则更为关键。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>CLS-RL方法利用可验证奖励鼓励MLLM模型思考，显著优于SFT，并产生泛化效果。</li>
<li>No-Thinking-RL方法通过最小化思考来提高MLLM的微调效率和效果，特别是对于某些视觉任务。</li>
<li>明确思考对奖励收敛过程有负面影响，可能导致训练效率降低。</li>
<li>Think-After-Answerwer方法旨在减轻思考对奖励收敛的负面影响。</li>
<li>对于简单视觉任务，不需要思考可以提高性能和减少训练时间；而对于复杂任务如数学推理，则需要思考过程以产生更好的性能。</li>
<li>对于不同的任务规模和类型（如数学、空间、谜题等），No-Thinking-RL的表现有所不同。对于较小的模型，它在简单任务上的表现优于带有思考过程的RFT；而对于复杂的数学任务，带有思考过程的RFT表现更好。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.16188">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-f709d4288fe7b2cce6ea793934090039.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8c1dd0ba59f7408388774e7a6972af04.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e216b6232d87137b9aeb65df05c71032.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="FNSE-SBGAN-Far-field-Speech-Enhancement-with-Schrodinger-Bridge-and-Generative-Adversarial-Networks"><a href="#FNSE-SBGAN-Far-field-Speech-Enhancement-with-Schrodinger-Bridge-and-Generative-Adversarial-Networks" class="headerlink" title="FNSE-SBGAN: Far-field Speech Enhancement with Schrodinger Bridge and   Generative Adversarial Networks"></a>FNSE-SBGAN: Far-field Speech Enhancement with Schrodinger Bridge and   Generative Adversarial Networks</h2><p><strong>Authors:Tong Lei, Qinwen Hu, Ziyao Lin, Andong Li, Rilin Chen, Meng Yu, Dong Yu, Jing Lu</strong></p>
<p>The prevailing method for neural speech enhancement predominantly utilizes fully-supervised deep learning with simulated pairs of far-field noisy-reverberant speech and clean speech. Nonetheless, these models frequently demonstrate restricted generalizability to mixtures recorded in real-world conditions. To address this issue, this study investigates training enhancement models directly on real mixtures. Specifically, we revisit the single-channel far-field to near-field speech enhancement (FNSE) task, focusing on real-world data characterized by low signal-to-noise ratio (SNR), high reverberation, and mid-to-high frequency attenuation. We propose FNSE-SBGAN, a framework that integrates a Schrodinger Bridge (SB)-based diffusion model with generative adversarial networks (GANs). Our approach achieves state-of-the-art performance across various metrics and subjective evaluations, significantly reducing the character error rate (CER) by up to 14.58% compared to far-field signals. Experimental results demonstrate that FNSE-SBGAN preserves superior subjective quality and establishes a new benchmark for real-world far-field speech enhancement. Additionally, we introduce an evaluation framework leveraging matrix rank analysis in the time-frequency domain, providing systematic insights into model performance and revealing the strengths and weaknesses of different generative methods. </p>
<blockquote>
<p>当前神经网络语音增强的主流方法主要利用模拟的远场带噪声和回响的语音与干净语音配对进行全监督深度学习。然而，这些模型在现实条件下录制的混合音上通常表现出有限的泛化能力。为解决此问题，本研究旨在直接在实际混合音上训练增强模型。具体来说，我们重新审视单通道远场到近场语音增强（FNSE）任务，重点关注现实世界数据的特点，如低信噪比（SNR）、高回响以及中到高频衰减。我们提出了FNSE-SBGAN框架，它融合了基于Schrodinger Bridge（SB）的扩散模型与生成对抗网络（GANs）。我们的方法在各种指标和主观评价上均达到了最先进的性能，与远场信号相比，字符错误率（CER）降低了高达14.58%。实验结果表明，FNSE-SBGAN保持了出色的主观质量，为现实世界的远场语音增强建立了新的基准。此外，我们还引入了一个利用时频域矩阵秩分析的评价框架，为模型性能提供了系统的见解，揭示了不同生成方法的优势和劣势。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.12936v3">PDF</a> 13 pages, 6 figures</p>
<p><strong>Summary</strong></p>
<p>本文主要研究了神经网络语音增强的现有方法，主要采用基于模拟远场噪声回荡语音和干净语音配对的全监督深度学习。然而，这些模型在真实条件下的混合录音中通常表现出一般化能力受限的问题。为解决这一问题，本研究直接对真实混合进行训练增强模型。针对低信噪比、高回声和中等至高频衰减等真实世界数据特点的单通道远场到近场语音增强（FNSE）任务进行深入研究。提出一种基于Schrodinger Bridge（SB）扩散模型与生成对抗网络（GANs）结合的FNSE-SBGAN框架。该方法在各种指标和主观评估上取得了最先进的性能，与远场信号相比，字符错误率（CER）降低了高达14.58%。实验结果表明，FNSE-SBGAN保留了较高的主观质量，为真实世界的远场语音增强建立了新的基准。此外，本研究还引入了一种基于矩阵秩分析的时间-频率域评估框架，为模型性能提供了系统的见解，揭示了不同生成方法的优势和劣势。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>神经网络语音增强主要使用全监督深度学习模拟配对远场噪声回荡语音和干净语音。</li>
<li>当前模型在真实条件下的混合录音中表现一般化能力受限。</li>
<li>研究提出一种新型的语音增强模型FNSE-SBGAN，结合了Schrodinger Bridge扩散模型和生成对抗网络。</li>
<li>FNSE-SBGAN在各项指标和主观评估上取得了先进性能，显著降低了字符错误率。</li>
<li>FNSE-SBGAN保留了较高的主观质量，为真实世界的远场语音增强建立了新的基准。</li>
<li>引入了一种基于矩阵秩分析的时间-频率域评估框架，提供模型性能的系统见解。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.12936">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-459078161a16fb07004809fdc00d21c9.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-8853a8c3a6fd9c1ce121200b015bddfd.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="MM-Eureka-Exploring-the-Frontiers-of-Multimodal-Reasoning-with-Rule-based-Reinforcement-Learning"><a href="#MM-Eureka-Exploring-the-Frontiers-of-Multimodal-Reasoning-with-Rule-based-Reinforcement-Learning" class="headerlink" title="MM-Eureka: Exploring the Frontiers of Multimodal Reasoning with   Rule-based Reinforcement Learning"></a>MM-Eureka: Exploring the Frontiers of Multimodal Reasoning with   Rule-based Reinforcement Learning</h2><p><strong>Authors:Fanqing Meng, Lingxiao Du, Zongkai Liu, Zhixiang Zhou, Quanfeng Lu, Daocheng Fu, Tiancheng Han, Botian Shi, Wenhai Wang, Junjun He, Kaipeng Zhang, Ping Luo, Yu Qiao, Qiaosheng Zhang, Wenqi Shao</strong></p>
<p>DeepSeek R1, and o1 have demonstrated powerful reasoning capabilities in the text domain through stable large-scale reinforcement learning. To enable broader applications, some works have attempted to transfer these capabilities to multimodal reasoning. However, these efforts have been limited by the limited difficulty of selected tasks and relatively small training scales, making it challenging to demonstrate strong multimodal reasoning abilities. To address this gap, we introduce the MMK12 dataset and MM-EUREKA with 7B and 32B parameters. The former is a high-quality multimodal mathematics reasoning dataset featuring diverse knowledge domains with human-verified answers and solution processes. The latter is a multimodal model employing rule-based reinforcement learning on MMK12, utilizing online filtering and two-stage training strategy to enhance training stability. MM-EUREKA demonstrates remarkable performance gains in multimodal mathematical reasoning, outperforming previous powerful models like InternVL2.5-78B or InternVL2.5-38B-MPO. In particular, MM-EUREKA achieves competitive or superior performance compared to both open-source and closed-source models, and trails slightly behind o1 in multidisciplinary reasoning tasks. We open-source our complete pipeline to foster further research in this area. We release all our codes, models, data, etc. at <a target="_blank" rel="noopener" href="https://github.com/ModalMinds/MM-EUREKA">https://github.com/ModalMinds/MM-EUREKA</a> </p>
<blockquote>
<p>DeepSeek R1和o1已通过稳定的大规模强化学习在文本领域展示了强大的推理能力。为了支持更广泛的应用，一些工作已尝试将这些能力转移到多模态推理。然而，由于所选任务的难度有限和相对较小的训练规模，这些努力在展示强大的多模态推理能力方面面临挑战。为了解决这一差距，我们推出了MMK12数据集和MM-EUREKA，分别具有7B和32B的参数。前者是一个高质量的多模态数学推理数据集，具有多样化知识领域的人类验证答案和解决方案过程。后者是一个多模态模型，采用基于规则的强化学习在MMK12上进行训练，利用在线过滤和两阶段训练策略来提高训练稳定性。MM-EUREKA在多模态数学推理方面表现出显著的性能提升，超越了之前强大的模型，如InternVL2.5-78B或InternVL2.5-38B-MPO。特别是，MM-EUREKA在跨学科的推理任务中，与开源和闭源模型相比具有竞争力或更优越的性能，略微落后于o1。我们公开了完整的管道，以促进该领域的研究。我们将在<a target="_blank" rel="noopener" href="https://github.com/ModalMinds/MM-EUREKA">https://github.com/ModalMinds/MM-EUREKA</a>上发布所有代码、模型、数据等。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.07365v2">PDF</a> </p>
<p><strong>Summary</strong><br>文本介绍了DeepSeek R1和o1在文本领域展现的强大推理能力，并通过大规模强化学习进行多任务推理。为扩大应用范围，一些工作尝试将这些能力转移到多模态推理。然而，由于所选任务的难度有限和训练规模相对较小，展示强大的多模态推理能力具有挑战性。为解决此问题，我们推出了MMK12数据集和MM-EUREKA模型（具有7B和32B参数）。MMK12是一个高质量的多模态数学推理数据集，涵盖了不同知识领域并具有人工验证的答案和解决方案过程。MM-EUREKA是一个多模态模型，采用基于规则的强化学习在MMK12上进行训练，使用在线过滤和两阶段训练策略提高训练稳定性。MM-EUREKA在多模态数学推理方面表现出显著的性能提升，超越了之前强大的模型，如InternVL2.5-78B或InternVL2.5-38B-MPO。特别是在跨学科推理任务中，MM-EUREKA的性能略逊于o1。我们公开了完整的管道，以促进该领域的研究。我们所有的代码、模型、数据等都可以在<a target="_blank" rel="noopener" href="https://github.com/ModalMinds/MM-EUREKA">https://github.com/ModalMinds/MM-EUREKA</a>获取。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>DeepSeek R1和o1已在文本领域展现出强大的推理能力，并通过大规模强化学习进行多任务推理。</li>
<li>将这些能力转移到多模态推理面临挑战，主要由于任务难度有限和训练规模较小。</li>
<li>MMK12数据集是一个多模态数学推理数据集，具有多样知识领域和人工验证的答案。</li>
<li>MM-EUREKA模型采用基于规则的强化学习在MMK12上训练，利用在线过滤和两阶段策略提高稳定性。</li>
<li>MM-EUREKA在多模态数学推理上表现优异，超越了一些先前的强大模型。</li>
<li>MM-EUREKA在跨学科的推理任务中性能略逊于o1。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.07365">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-0e51ba6bc70959c7b953508b498c7bfd.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-723069b8845379f5301d8099dba9b891.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a8cc4b559448977d985cc38b37adf97d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7047cc945f08811fff4de224ee9c2fa1.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="BioMaze-Benchmarking-and-Enhancing-Large-Language-Models-for-Biological-Pathway-Reasoning"><a href="#BioMaze-Benchmarking-and-Enhancing-Large-Language-Models-for-Biological-Pathway-Reasoning" class="headerlink" title="BioMaze: Benchmarking and Enhancing Large Language Models for Biological   Pathway Reasoning"></a>BioMaze: Benchmarking and Enhancing Large Language Models for Biological   Pathway Reasoning</h2><p><strong>Authors:Haiteng Zhao, Chang Ma, Fangzhi Xu, Lingpeng Kong, Zhi-Hong Deng</strong></p>
<p>The applications of large language models (LLMs) in various biological domains have been explored recently, but their reasoning ability in complex biological systems, such as pathways, remains underexplored, which is crucial for predicting biological phenomena, formulating hypotheses, and designing experiments. This work explores the potential of LLMs in pathway reasoning. We introduce BioMaze, a dataset with 5.1K complex pathway problems derived from real research, covering various biological contexts including natural dynamic changes, disturbances, additional intervention conditions, and multi-scale research targets. Our evaluation of methods such as CoT and graph-augmented reasoning, shows that LLMs struggle with pathway reasoning, especially in perturbed systems. To address this, we propose PathSeeker, an LLM agent that enhances reasoning through interactive subgraph-based navigation, enabling a more effective approach to handling the complexities of biological systems in a scientifically aligned manner. The dataset and code are available at <a target="_blank" rel="noopener" href="https://github.com/zhao-ht/BioMaze">https://github.com/zhao-ht/BioMaze</a>. </p>
<blockquote>
<p>最近已经探索了大型语言模型（LLM）在各种生物领域的应用，但它们在复杂生物系统（如途径）中的推理能力仍然被探索得不够充分，这对于预测生物现象、制定假设和设计实验至关重要。这项工作探索了LLM在途径推理中的潜力。我们介绍了BioMaze，这是一个包含5.1K个来自真实研究的复杂途径问题的数据集，涵盖各种生物背景，包括自然动态变化、干扰、额外干预条件和多尺度研究目标。我们对包括CoT和增强图推理等方法进行评估的结果表明，LLM在途径推理方面存在困难，尤其是在受干扰的系统中。为了解决这一问题，我们提出了PathSeeker，这是一个LLM代理，通过基于交互子图的导航增强推理能力，以更科学的方式更有效地应对生物系统的复杂性。数据集和代码可在<a target="_blank" rel="noopener" href="https://github.com/zhao-ht/BioMaze">https://github.com/zhao-ht/BioMaze</a>获取。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.16660v4">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>大型语言模型（LLMs）在生物领域的应用已受到关注，但在复杂生物系统（如途径）中的推理能力仍被低估。本研究探索了LLMs在途径推理中的潜力，并引入BioMaze数据集，包含5.1K个途径问题实例。评估显示LLMs在受干扰系统上的推理能力受限，因此提出PathSeeker，一个通过交互式子图导航增强推理的LLM代理。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>大型语言模型（LLMs）在生物领域的应用逐渐受到关注，特别是在途径推理方面。</li>
<li>BioMaze数据集包含真实研究中的复杂途径问题实例，有助于评估模型在途径推理方面的表现。</li>
<li>LLMs在受干扰系统上的推理能力受限，这影响了其在生物系统复杂性的处理效果。</li>
<li>PathSeeker是一个LLM代理，通过交互式子图导航增强推理能力，更科学地应对生物系统的复杂性。</li>
<li>BioMaze数据集和代码可通过GitHub共享。</li>
<li>LLMs在预测生物现象、假设制定和实验设计方面存在巨大潜力。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.16660">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-453f5d5b9afb253943720d557c693bf9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f50f71bd991c3c5db24d14612a62d5ec.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4509a357c681b5b1bc065ccd88e6c93d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e616561d2bc315a4bb324499caf77181.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0015393d9eb5a90f1979e22a304189cf.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        文章作者:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        文章链接:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-04-20/R1_Reasoning/">https://kedreamix.github.io/Talk2Paper/Paper/2025-04-20/R1_Reasoning/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        版权声明:
                    </i>
                </span>
                <span class="reprint-info">
                    本博客所有文章除特別声明外，均采用
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    许可协议。转载请注明来源
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/R1-Reasoning/">
                                    <span class="chip bg-color">R1_Reasoning</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;上一篇</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-04-20/LLM/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-1799a5c57e34d17fa4a998e98eee9508.jpg" class="responsive-img" alt="LLM">
                        
                        <span class="card-title">LLM</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            LLM 方向最新论文已更新，请持续关注 Update in 2025-04-20  Lightning IR Straightforward Fine-tuning and Inference of   Transformer-based Language Models for Information Retrieval
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-04-20
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/LLM/" class="post-category">
                                    LLM
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/LLM/">
                        <span class="chip bg-color">LLM</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                下一篇&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-04-19/Talking%20Head%20Generation/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-0070c3045e6f05ce9991f09599f8f4b4.jpg" class="responsive-img" alt="Talking Head Generation">
                        
                        <span class="card-title">Talking Head Generation</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Talking Head Generation 方向最新论文已更新，请持续关注 Update in 2025-04-19  FLAP Fully-controllable Audio-driven Portrait Video Generation through   3D head conditioned diffusion model
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-04-19
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Talking-Head-Generation/" class="post-category">
                                    Talking Head Generation
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Talking-Head-Generation/">
                        <span class="chip bg-color">Talking Head Generation</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- 代码块功能依赖 -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- 代码语言 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- 代码块复制 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- 代码块收缩 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC 悬浮按钮. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* 修复文章卡片 div 的宽度. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // 切换TOC目录展开收缩的相关操作.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;站点总字数:&nbsp;<span
                        class="white-color">27685.3k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- 运行天数提醒. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // 区分是否有年份.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = '本站已运行 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                daysTip = '本站已運行 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = '本站已运行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = '本站已運行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="关注我的知乎: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">知</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- 搜索遮罩框 -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span>
            <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- 白天和黑夜主题 -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- 回到顶部按钮 -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // 只在桌面版网页启用特效
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- 雪花特效 -->
    

    <!-- 鼠标星星特效 -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--腾讯兔小巢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- 动态标签 -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Σ(っ °Д °;)っ诶，页面崩溃了嘛？", clearTimeout(st)) : (document.title = "φ(゜▽゜*)♪咦，又好了！", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
