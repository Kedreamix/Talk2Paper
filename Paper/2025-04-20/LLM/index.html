<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="LLM">
    <meta name="description" content="LLM 方向最新论文已更新，请持续关注 Update in 2025-04-20  Lightning IR Straightforward Fine-tuning and Inference of   Transformer-based Language Models for Information Retrieval">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>LLM | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loader页面消失采用渐隐的方式*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logo出现动画 */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//使用渐隐的方法淡出loading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//强制显示loading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">嘘~  正在从服务器偷取页面 . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>首页</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>标签</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>分类</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>归档</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="搜索" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="深色/浅色模式" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			首页
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			标签
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			分类
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			归档
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pic1.zhimg.com/v2-1799a5c57e34d17fa4a998e98eee9508.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">LLM</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- 文章内容详情 -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/LLM/">
                                <span class="chip bg-color">LLM</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/LLM/" class="post-category">
                                LLM
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp;
                    2025-04-20
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>更新日期:&nbsp;&nbsp;
                    2025-04-26
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>文章字数:&nbsp;&nbsp;
                    11.3k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>阅读时长:&nbsp;&nbsp;
                    45 分
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>阅读次数:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>⚠️ 以下所有内容总结都来自于 大语言模型的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p>
</blockquote>
<h1 id="2025-04-20-更新"><a href="#2025-04-20-更新" class="headerlink" title="2025-04-20 更新"></a>2025-04-20 更新</h1><h2 id="Lightning-IR-Straightforward-Fine-tuning-and-Inference-of-Transformer-based-Language-Models-for-Information-Retrieval"><a href="#Lightning-IR-Straightforward-Fine-tuning-and-Inference-of-Transformer-based-Language-Models-for-Information-Retrieval" class="headerlink" title="Lightning IR: Straightforward Fine-tuning and Inference of   Transformer-based Language Models for Information Retrieval"></a>Lightning IR: Straightforward Fine-tuning and Inference of   Transformer-based Language Models for Information Retrieval</h2><p><strong>Authors:Ferdinand Schlatt, Maik Fröbe, Matthias Hagen</strong></p>
<p>A wide range of transformer-based language models have been proposed for information retrieval tasks. However, including transformer-based models in retrieval pipelines is often complex and requires substantial engineering effort. In this paper, we introduce Lightning IR, an easy-to-use PyTorch Lightning-based framework for applying transformer-based language models in retrieval scenarios. Lightning IR provides a modular and extensible architecture that supports all stages of a retrieval pipeline: from fine-tuning and indexing to searching and re-ranking. Designed to be scalable and reproducible, Lightning IR is available as open-source: <a target="_blank" rel="noopener" href="https://github.com/webis-de/lightning-ir">https://github.com/webis-de/lightning-ir</a>. </p>
<blockquote>
<p>针对信息检索任务，已经提出了多种基于转换器的语言模型。然而，在检索管道中包含基于转换器的模型通常很复杂，需要大量的工程努力。在本文中，我们介绍了Lightning IR，这是一个易于使用的基于PyTorch Lightning的框架，可用于在检索场景中应用基于转换器的语言模型。Lightning IR提供了一个模块化且可扩展的架构，支持检索管道的所有阶段：从微调、索引到搜索和重新排名。设计可伸缩且可重复，Lightning IR作为开源可用：<a target="_blank" rel="noopener" href="https://github.com/webis-de/lightning-ir%E3%80%82">https://github.com/webis-de/lightning-ir。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2411.04677v5">PDF</a> Accepted as a demo at WSDM’25</p>
<p><strong>Summary</strong>：介绍了Lightning IR，一个基于PyTorch Lightning的易用框架，用于在检索场景中应用基于转换器的语言模型。该框架提供了一种模块化且可扩展的架构，支持检索管道的所有阶段：从微调、索引到搜索和重新排名。Lightning IR旨在实现可扩展性和可重复性，可作为开源获得。</p>
<p><strong>Key Takeaways</strong>：</p>
<ol>
<li>该论文提出了Lightning IR框架，旨在简化在检索任务中应用基于转换器的语言模型的复杂性。</li>
<li>Lightning IR是基于PyTorch Lightning构建的，易于使用。</li>
<li>该框架支持检索管道的所有阶段，包括微调、索引、搜索和重新排名。</li>
<li>Lightning IR具有模块化和可扩展性，能够适应不同的需求。</li>
<li>该框架旨在实现可扩展性和可重复性，有助于提高研究效率和成果质量。</li>
<li>Lightning IR作为开源框架，可方便研究人员和其他开发者使用和改进。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2411.04677">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-915690373e0e4adaee825b9058c9d60c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-afa40453b1512cdc0a614e9c4faa420f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5363ca9ab969b1bba343f8862b93fad8.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b873b5b4c07877a0ec8a8f4b2825df7b.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="ResiDual-Transformer-Alignment-with-Spectral-Decomposition"><a href="#ResiDual-Transformer-Alignment-with-Spectral-Decomposition" class="headerlink" title="ResiDual Transformer Alignment with Spectral Decomposition"></a>ResiDual Transformer Alignment with Spectral Decomposition</h2><p><strong>Authors:Lorenzo Basile, Valentino Maiorca, Luca Bortolussi, Emanuele Rodolà, Francesco Locatello</strong></p>
<p>When examined through the lens of their residual streams, a puzzling property emerges in transformer networks: residual contributions (e.g., attention heads) sometimes specialize in specific tasks or input attributes. In this paper, we analyze this phenomenon in vision transformers, focusing on the spectral geometry of residuals, and explore its implications for modality alignment in vision-language models. First, we link it to the intrinsically low-dimensional structure of visual head representations, zooming into their principal components and showing that they encode specialized roles across a wide variety of input data distributions. Then, we analyze the effect of head specialization in multimodal models, focusing on how improved alignment between text and specialized heads impacts zero-shot classification performance. This specialization-performance link consistently holds across diverse pre-training data, network sizes, and objectives, demonstrating a powerful new mechanism for boosting zero-shot classification through targeted alignment. Ultimately, we translate these insights into actionable terms by introducing ResiDual, a technique for spectral alignment of the residual stream. Much like panning for gold, it lets the noise from irrelevant unit principal components (i.e., attributes) wash away to amplify task-relevant ones. Remarkably, this dual perspective on modality alignment yields fine-tuning level performance on different data distributions while modelling an extremely interpretable and parameter-efficient transformation, as we extensively show on 70 pre-trained network-dataset combinations (7 models, 10 datasets). </p>
<blockquote>
<p>当从剩余流的角度审视变压器网络时，会出现一个令人困惑的特性：剩余贡献（例如，注意力头）有时会专门用于特定任务或输入属性。在本文中，我们分析了视觉变压器中的这种现象，重点关注剩余的谱几何，并探索了其对视觉语言模型中模态对齐的影响。首先，我们将其与视觉头表示的固有低维结构联系起来，深入探究其主要成分，并表明它们在各种输入数据分布中编码了专业角色。接着，我们分析了头专业化在多模态模型中的影响，重点关注文本和专业化头之间改进的对齐对零样本分类性能的影响。这种专业化和性能之间的联系在各种预训练数据、网络规模和目标中始终存在，证明了一种通过有针对性的对齐来提高零样本分类性能的有力新机制。最终，我们将这些见解转化为可行的术语，并引入了ResiDual技术，这是一种用于剩余流的谱对齐技术。就像淘金一样，它可以让无关紧要的单元主成分（即属性）的噪声消失，以放大与任务相关的成分。令人惊奇的是，这种对模态对齐的双重视角在不同数据分布上达到了微调级别的性能，同时建立了一个极其可解释和参数高效的转换模型，我们在70个预训练网络数据集组合（7个模型，10个数据集）上进行了广泛展示。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2411.00246v2">PDF</a> Published in Transactions on Machine Learning Research (TMLR)</p>
<p><strong>Summary</strong>：本文研究了通过残差流视角分析transformer网络中的谜题属性，发现残差贡献（如注意力头）有时会在特定任务或输入属性上专业化。文章重点关注视觉transformer的谱几何残差，并探讨了其在视觉语言模型的模态对齐中的影响。研究发现视觉头表示的固有低维结构编码了专业化的角色，并且头专业化在跨模态模型中提高了文本与专业化头部之间的对齐效果，进而提高零样本分类性能。研究还发现头专业化在跨不同预训练数据、网络规模和目标时，专业化与性能的关联始终存在。最终，文章将这些见解转化为实际应用，推出ResiDual技术，用于残差流的谱对齐。这一技术极大地提高了任务相关单位的振幅，同时在不同的数据分布上达到了微调级别的性能，展示出极为可解释和参数高效的转换。</p>
<p><strong>Key Takeaways</strong>：</p>
<ol>
<li>残差贡献在transformer网络中有时会在特定任务或输入属性上专业化。</li>
<li>视觉头表示的固有低维结构编码了专业化的角色。</li>
<li>头专业化能提高跨模态模型中文本与专业化头部之间的对齐效果。</li>
<li>专业化与性能的关联在跨不同的预训练数据、网络规模和目标时始终存在。</li>
<li>推出ResiDual技术，用于残差流的谱对齐，提高了任务相关单位的振幅。</li>
<li>ResiDual技术在不同的数据分布上达到了微调级别的性能。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2411.00246">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-1799a5c57e34d17fa4a998e98eee9508.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1d3bb102a48106f2afc1bc634c81d149.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-45bdd0f6c6ff406fcd29bdad356b86e0.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Improving-Instruction-Following-in-Language-Models-through-Activation-Steering"><a href="#Improving-Instruction-Following-in-Language-Models-through-Activation-Steering" class="headerlink" title="Improving Instruction-Following in Language Models through Activation   Steering"></a>Improving Instruction-Following in Language Models through Activation   Steering</h2><p><strong>Authors:Alessandro Stolfo, Vidhisha Balachandran, Safoora Yousefi, Eric Horvitz, Besmira Nushi</strong></p>
<p>The ability to follow instructions is crucial for numerous real-world applications of language models. In pursuit of deeper insights and more powerful capabilities, we derive instruction-specific vector representations from language models and use them to steer models accordingly. These vectors are computed as the difference in activations between inputs with and without instructions, enabling a modular approach to activation steering. We demonstrate how this method can enhance model adherence to constraints such as output format, length, and word inclusion, providing inference-time control over instruction following. Our experiments across four models demonstrate how we can use the activation vectors to guide models to follow constraints even without explicit instructions and to enhance performance when instructions are present. Additionally, we explore the compositionality of activation steering, successfully applying multiple instructions simultaneously. Finally, we demonstrate that steering vectors computed on instruction-tuned models can transfer to improve base models. Our findings demonstrate that activation steering offers a practical and scalable approach for fine-grained control in language generation. Our code and data are available at <a target="_blank" rel="noopener" href="https://github.com/microsoft/llm-steer-instruct">https://github.com/microsoft/llm-steer-instruct</a>. </p>
<blockquote>
<p>从语言模型在现实世界中的众多应用来看，遵循指令的能力至关重要。为了获得更深入的理解和更强大的能力，我们从语言模型中导出与指令相关的向量表示，并据此引导模型。这些向量是通过计算带指令与不带指令的输入之间的激活差异来得到的，这使得激活引导可以采用模块化方法。我们展示了这种方法如何增强模型对输出格式、长度和词汇包含等约束的遵循能力，为指令遵循提供推理时的控制。我们在四个模型上的实验展示了如何使用激活向量来指导模型遵循约束，即使在没有任何明确指令的情况下也能提高性能，并在有指令时增强表现。此外，我们还探索了激活引导的组成性，能够同时应用多个指令。最后，我们证明了在指令调整模型上计算的转向向量可以转移到基础模型中以进行改进。我们的研究结果表明，激活引导为语言生成中的精细粒度控制提供了一种实用且可扩展的方法。我们的代码和数据在<a target="_blank" rel="noopener" href="https://github.com/microsoft/llm-steer-instruct%E3%80%82">https://github.com/microsoft/llm-steer-instruct。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2410.12877v2">PDF</a> ICLR 2025</p>
<p><strong>Summary</strong></p>
<p>语言模型的指令遵循能力在现实应用中的重要性日益凸显。研究团队从语言模型中推导出指令特定的向量表示，用于引导模型遵循指令。这些向量通过计算带有和不带指令的输入激活差异得到，为激活引导提供了模块化方法。研究展示了该方法如何提升模型对输出格式、长度和词汇约束的遵循能力，实现推理时的指令遵循控制。实验表明，激活向量可用于引导模型遵循约束，甚至在无明确指令的情况下提升性能。此外，研究还探索了激活引导的组合性，成功同时应用多个指令。最终，研究证明在指令调整模型上计算的引导向量可转移至基础模型以提升性能。研究结果表明，激活引导为一种实用且可伸缩的方法，可实现语言生成中的精细控制。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>语言模型的指令遵循能力对于其现实应用非常重要。</li>
<li>通过推导指令特定的向量表示，可以引导语言模型遵循指令。</li>
<li>激活向量是通过计算带有和不带指令的输入激活差异得到的。</li>
<li>该方法可以提高模型对输出格式、长度和词汇约束的遵循能力。</li>
<li>激活向量可用于在无明确指令的情况下引导模型遵循约束，提升性能。</li>
<li>研究展示了激活引导的组合性，可以同时应用多个指令。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2410.12877">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-5aa9870ce353ea48f4fdce673c55420b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-11861ba2fdc95dd9073b6a186517c77f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1c41f716b265f94c815d10db24ea322b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-79cc63b0219a45f86347d15f37d10ced.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-d0135e0797280ddf08c7c7f67e825400.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Teaching-Transformers-Causal-Reasoning-through-Axiomatic-Training"><a href="#Teaching-Transformers-Causal-Reasoning-through-Axiomatic-Training" class="headerlink" title="Teaching Transformers Causal Reasoning through Axiomatic Training"></a>Teaching Transformers Causal Reasoning through Axiomatic Training</h2><p><strong>Authors:Aniket Vashishtha, Abhinav Kumar, Atharva Pandey, Abbavaram Gowtham Reddy, Kabir Ahuja, Vineeth N Balasubramanian, Amit Sharma</strong></p>
<p>For text-based AI systems to interact in the real world, causal reasoning is an essential skill. Since active interventions are costly, we study to what extent a system can learn causal reasoning from symbolic demonstrations of causal axioms. Specifically, we present an axiomatic training method where the system learns from multiple demonstrations of a causal axiom (or rule), rather than incorporating the axiom as an inductive bias or inferring it from data values. A key question is whether the system would learn to generalize from the axiom demonstrations to more complex scenarios. Our results, based on applying axiomatic training to learn the transitivity axiom and d-separation rule, indicate that such generalization is possible. To avoid data contamination issues, we start with a 67 million parameter transformer model and train it from scratch. On both tasks, we find that a model trained on linear causal chains (along with some noisy variations) can generalize well to complex graphs, including longer causal chains, causal chains with reversed order, and graphs with branching.To handle diverse text inputs, the same method is extended to finetune language models. Finetuning Llama-3.1 8B model on our axiomatic data leads to significant gains on causal benchmarks such as Corr2Cause and CLEAR, in some cases providing state-of-the-art performance surpassing GPT-4. </p>
<blockquote>
<p>对于基于文本的AI系统在现实世界中的交互来说，因果推理是一项基本技巧。由于主动干预成本高昂，我们研究系统能从因果公理的符号演示中学习到何种程度的因果推理。具体来说，我们提出了一种公理化训练方法，该系统从因果公理（或规则）的多个演示中学习，而不是将公理作为归纳偏见或从数据值中推断出来。一个关键的问题是，系统是否会从公理演示中学习并推广到更复杂的场景。我们的结果基于应用公理化训练来学习传递性公理和d-分离规则，表明这种推广是可能的。为了避免数据污染问题，我们从6700万个参数的变压器模型开始，并从零开始对其进行训练。在这两项任务中，我们发现训练有素的系统在面临线性因果链（以及带有噪声的一些变体）时，能够很好地适应复杂的图形结构，包括更长的因果链、逆向顺序的因果链和带有分支的图形。为了处理多样的文本输入，对语言模型进行了微调以应用相同的方法。在公理数据上微调Llama-3.1的8B模型后，在诸如Corr2Cause和CLEAR等因果基准测试中取得了显著的收益，在某些情况下超越了GPT-4的最先进表现。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2407.07612v2">PDF</a> </p>
<p><strong>Summary</strong>：基于符号因果公理演示的文本生成模型可以学习因果推理技能，并从简单的因果链推广到更复杂的场景。研究采用了一种公理化训练方法，并展示了这种训练对线性因果链模型的适用性。通过对大型语言模型进行微调，该方法可以显著提高因果基准测试的性能。</p>
<p><strong>Key Takeaways</strong>：</p>
<ol>
<li>因果推理对于文本生成的AI系统在现实世界中的交互至关重要。</li>
<li>公理化训练方法是AI系统学习因果推理的一种有效方式，通过从多个因果公理演示中学习，而不是作为归纳偏见或从数据值中推断出来。</li>
<li>系统可以从简单的因果链公理推广到更复杂的场景，如更长、顺序颠倒和分支的因果链。</li>
<li>采用这种训练方法的模型在因果基准测试中表现出显著的性能提升。</li>
<li>研究使用了包含约67百万参数的变压器模型，并从基础开始训练以避免数据污染问题。</li>
<li>对大型语言模型（如Llama-3.1 8B模型）进行微调可以进一步提高性能，在某些情况下达到或超越了GPT-4的表现。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2407.07612">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-08e9dc958659494830840d0ed6c59b58.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-2c10c9871ecdd2c2f2907f9dd8784749.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-7df41c3f0336f9513e238c78c0d391d7.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="F-LMM-Grounding-Frozen-Large-Multimodal-Models"><a href="#F-LMM-Grounding-Frozen-Large-Multimodal-Models" class="headerlink" title="F-LMM: Grounding Frozen Large Multimodal Models"></a>F-LMM: Grounding Frozen Large Multimodal Models</h2><p><strong>Authors:Size Wu, Sheng Jin, Wenwei Zhang, Lumin Xu, Wentao Liu, Wei Li, Chen Change Loy</strong></p>
<p>Endowing Large Multimodal Models (LMMs) with visual grounding capability can significantly enhance AIs’ understanding of the visual world and their interaction with humans. However, existing methods typically fine-tune the parameters of LMMs to learn additional segmentation tokens and overfit grounding and segmentation datasets. Such a design would inevitably cause a catastrophic diminution in the indispensable conversational capability of general AI assistants. In this paper, we comprehensively evaluate state-of-the-art grounding LMMs across a suite of multimodal question-answering benchmarks, observing drastic performance drops that indicate vanishing general knowledge comprehension and weakened instruction following ability. To address this issue, we present F-LMM – grounding frozen off-the-shelf LMMs in human-AI conversations – a straightforward yet effective design based on the fact that word-pixel correspondences conducive to visual grounding inherently exist in the attention mechanism of well-trained LMMs. Using only a few trainable CNN layers, we can translate word-pixel attention weights to mask logits, which a SAM-based mask refiner can further optimise. Our F-LMM neither learns special segmentation tokens nor utilises high-quality grounded instruction-tuning data, but achieves competitive performance on referring expression segmentation and panoptic narrative grounding benchmarks while completely preserving LMMs’ original conversational ability. Additionally, with instruction-following ability preserved and grounding ability obtained, F-LMM can be directly applied to complex tasks like reasoning segmentation, grounded conversation generation and visual chain-of-thought reasoning. Our code can be found at <a target="_blank" rel="noopener" href="https://github.com/wusize/F-LMM">https://github.com/wusize/F-LMM</a>. </p>
<blockquote>
<p>赋予大型多模态模型（LMMs）视觉接地能力可以显著增强AI对视觉世界的理解以及与人类的交互能力。然而，现有方法通常通过对LMM的参数进行微调来学习额外的分割令牌，并过度适应接地和分割数据集。这种设计不可避免地会导致通用AI助理必不可少的管理能力发生灾难性降低。在本文中，我们全面评估了最先进的地基LMM在一系列多模态问答基准测试上的表现，观察到性能急剧下降，表明一般知识理解消失，以及指令执行能力减弱。为了解决这个问题，我们提出了F-LMM——在人机对话中冻结现成的LMM进行接地，这是一个简单而有效的设计，基于这样一个事实：在训练良好的LMM的注意力机制中，有利于视觉接地的单词像素对应关系是固有的。通过使用只有少数可训练的CNN层，我们可以将单词像素注意力权重转换为掩码对数概率，SAM基掩码精炼器可以进一步优化它。我们的F-LMM既不需要学习特殊的分割令牌，也不需要利用高质量的地基指令调整数据，就能在指代表达式分割和全景叙事接地基准测试上取得有竞争力的表现，同时完全保留LMMs原有的对话能力。此外，在保留指令执行能力并获得接地能力的情况下，F-LMM可以直接应用于推理分割、基于接地的对话生成和视觉思维链推理等复杂任务。我们的代码可以在<a target="_blank" rel="noopener" href="https://github.com/wusize/F-LMM">https://github.com/wusize/F-LMM</a>找到。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2406.05821v3">PDF</a> Project Page: <a target="_blank" rel="noopener" href="https://github.com/wusize/F-LMM">https://github.com/wusize/F-LMM</a></p>
<p><strong>Summary</strong></p>
<p>本文探讨了大型多模态模型（LMMs）的视觉接地能力对增强AI对视觉世界的理解与人类交互的重要性。然而，现有方法通常通过微调LMM参数来学习额外的分割令牌，并过度拟合接地和分割数据集，这会损害AI助手不可或缺的对一般知识的理解和指令执行能力。为解决这一问题，本文评估了最先进的多模态问题回答基准测试中的接地LMM性能，并观察到性能急剧下降。为此，提出了一种基于现有预训练LMM的视觉接地方法——F-LMM。通过利用LMM内部的注意力机制，在保留原有对话能力的基础上，实现词像素对应关系可视化接地。F-LMM无需学习特殊分割令牌，也不依赖高质量接地指令微调数据，但仍能在指代表达式分割和全视叙事接地基准测试中实现具有竞争力的性能。此外，凭借其保留的指令执行能力和视觉接地能力，F-LMM可直接应用于复杂任务如推理分割、基于视觉的对话生成和视觉链推理等。更多详细信息请参见相关网站：<a target="_blank" rel="noopener" href="https://github.com/wusize/F-LMM">网址链接</a>。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LMMs的视觉接地能力对增强AI与人类的交互至关重要。</li>
<li>现有方法微调LMM参数以学习额外的分割令牌并过度拟合数据，导致一般知识理解和指令执行能力下降。</li>
<li>通过评估多种多模态问题回答基准测试，发现现有方法的性能急剧下降。</li>
<li>提出了一种基于预训练LMM的视觉接地方法——F-LMM。</li>
<li>F-LMM利用注意力机制实现词像素对应关系可视化接地，无需学习特殊分割令牌。</li>
<li>F-LMM在不损害原有对话能力的情况下，实现了良好的分割和接地性能。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2406.05821">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-d0422c362b470a7c5a84300ea461b71a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ce286065d678995919602d2935bdc9e3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-eb32e1ecd8ab3f2db67ed5b8e9b5b422.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-cd901d9e6f45c99a8f6414e115b94ecc.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-80c68afc596a746dfe973d23e2e5f6ca.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-fc8c5133b7af4d73f673a10658ff4ef1.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Towards-Translating-Real-World-Code-with-LLMs-A-Study-of-Translating-to-Rust"><a href="#Towards-Translating-Real-World-Code-with-LLMs-A-Study-of-Translating-to-Rust" class="headerlink" title="Towards Translating Real-World Code with LLMs: A Study of Translating to   Rust"></a>Towards Translating Real-World Code with LLMs: A Study of Translating to   Rust</h2><p><strong>Authors:Hasan Ferit Eniser, Hanliang Zhang, Cristina David, Meng Wang, Maria Christakis, Brandon Paulsen, Joey Dodds, Daniel Kroening</strong></p>
<p>Large language models (LLMs) show promise in code translation - the task of translating code written in one programming language to another language - due to their ability to write code in most programming languages. However, LLM’s effectiveness on translating real-world code remains largely unstudied. In this work, we perform the first substantial study on LLM-based translation to Rust by assessing the ability of five state-of-the-art LLMs, GPT4, Claude 3, Claude 2.1, Gemini Pro, and Mixtral. We conduct our study on code extracted from real-world open source projects. To enable our study, we develop FLOURINE, an end-to-end code translation tool that uses differential fuzzing to check if a Rust translation is I&#x2F;O equivalent to the original source program, eliminating the need for pre-existing test cases. As part of our investigation, we assess both the LLM’s ability to produce an initially successful translation, as well as their capacity to fix a previously generated buggy one. If the original and the translated programs are not I&#x2F;O equivalent, we apply a set of automated feedback strategies, including feedback to the LLM with counterexamples. Our results show that the most successful LLM can translate 47% of our benchmarks, and also provides insights into next steps for improvements. </p>
<blockquote>
<p>大型语言模型（LLM）在代码翻译任务中显示出巨大的潜力，即将一种编程语言编写的代码翻译成另一种语言的代码。这得益于它们能够使用大多数编程语言编写代码的能力。然而，LLM在现实世界的代码翻译方面的有效性在很大程度上尚未被研究。在这项工作中，我们对基于LLM的Rust翻译进行了首次实质性的研究，评估了五种最新LLM的能力，包括GPT4、Claude 3、Claude 2.1、Gemini Pro和Mixtral。我们的研究是在从现实世界中的开源项目中提取的代码上进行的。为了支持我们的研究，我们开发了FLOURINE，这是一款端到端的代码翻译工具，它使用差分模糊测试来检查Rust翻译是否与原始源程序在I&#x2F;O上等效，从而不再需要预先存在的测试用例。作为调查的一部分，我们评估了LLM能够产生初步成功的翻译的能力，以及它们修复先前生成的错误翻译的能力。如果原始程序和翻译后的程序在I&#x2F;O上不等效，我们会采用一系列的自动化反馈策略，包括对LLM进行反例反馈。我们的结果表明，最成功的LLM能够翻译我们基准测试的47%，并提供了关于下一步改进方向的见解。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2405.11514v3">PDF</a> 12 pages, 12 figures</p>
<p><strong>Summary</strong></p>
<p>本文研究了基于大型语言模型（LLM）的代码翻译能力，特别是对Rust语言的翻译。研究对五款先进的LLM进行了评估，包括GPT4、Claude 3、Claude 2.1、Gemini Pro和Mixtral。研究中开发了一个名为FLOURINE的代码翻译工具，采用差分模糊测试来检查翻译结果的输入输出等效性。研究发现，最成功的LLM能够翻译47%的基准测试代码，同时也提供了改进的方向。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>大型语言模型（LLM）在代码翻译领域具有潜力，特别是在将代码从一种编程语言翻译到另一种语言方面。</li>
<li>对五款先进的LLM在Rust翻译方面的能力进行了首次实质性研究。</li>
<li>开发了一个名为FLOURINE的代码翻译工具，采用差分模糊测试来验证翻译结果的准确性。</li>
<li>LLM能够初步成功翻译的代码比例最高达到47%。</li>
<li>LLM在修复先前生成的错误代码方面也有一定能力。</li>
<li>当原始程序和翻译程序在输入输出方面不等效时，采用了一系列自动化反馈策略，包括向LLM提供反例。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2405.11514">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-fdf3b623d61f78e00a5af216a3a47199.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-83ca1a83862f31d118cc61e9ccdb1a86.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-92eb9eef867cfd1641027556cdee921b.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-e95623593c8cf5c48502e2ad394a138b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-eabfa5c6e5f9aa10f2b186b196f17e0a.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="MemLLM-Finetuning-LLMs-to-Use-An-Explicit-Read-Write-Memory"><a href="#MemLLM-Finetuning-LLMs-to-Use-An-Explicit-Read-Write-Memory" class="headerlink" title="MemLLM: Finetuning LLMs to Use An Explicit Read-Write Memory"></a>MemLLM: Finetuning LLMs to Use An Explicit Read-Write Memory</h2><p><strong>Authors:Ali Modarressi, Abdullatif Köksal, Ayyoob Imani, Mohsen Fayyaz, Hinrich Schütze</strong></p>
<p>While current large language models (LLMs) perform well on many knowledge-related tasks, they are limited by relying on their parameters as an implicit storage mechanism. As a result, they struggle with memorizing rare events and with updating their memory as facts change over time. In addition, the uninterpretable nature of parametric memory makes it challenging to prevent hallucination. Model editing and augmenting LLMs with parameters specialized for memory are only partial solutions. In this paper, we introduce MemLLM, a novel method of enhancing LLMs by integrating a structured and explicit read-and-write memory module. MemLLM tackles the aforementioned challenges by enabling dynamic interaction with the memory and improving the LLM’s capabilities in using stored knowledge. Our experiments indicate that MemLLM enhances the LLM’s performance and interpretability, in language modeling in general and knowledge-intensive tasks in particular. We see MemLLM as an important step towards making LLMs more grounded and factual through memory augmentation. The project repository is publicly available at <a target="_blank" rel="noopener" href="https://github.com/amodaresi/MemLLM">https://github.com/amodaresi/MemLLM</a> </p>
<blockquote>
<p>当前的大型语言模型（LLM）在许多与知识相关的任务上表现良好，但它们依赖于参数作为隐式存储机制，从而存在局限性。因此，它们在记忆罕见事件和随时间更新事实方面遇到困难。此外，参数记忆的不可解释性使得防止幻觉具有挑战性。通过参数对LLM进行模型编辑和扩充，专门用于记忆只是部分解决方案。在本文中，我们介绍了MemLLM，这是一种通过集成结构化和显式读写内存模块增强LLM的新型方法。MemLLM通过实现与内存的动态交互，提高LLM利用存储知识的能力，解决了上述挑战。我们的实验表明，MemLLM提高了LLM的性能和可解释性，尤其是在一般语言建模和知识密集型任务中。我们认为MemLLM是朝着通过记忆增强使LLM更加扎实和基于事实的重要一步。项目仓库可在<a target="_blank" rel="noopener" href="https://github.com/amodaresi/MemLLM%E5%85%AC%E5%BC%80%E8%AE%BF%E9%97%AE%E3%80%82">https://github.com/amodaresi/MemLLM公开访问。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2404.11672v3">PDF</a> Published in Transactions on Machine Learning Research (TMLR)</p>
<p><strong>Summary</strong><br>大型语言模型（LLM）在知识相关任务上表现良好，但受限于隐式存储机制，难以记忆罕见事件和随时间变化的事实。本文提出MemLLM方法，通过集成结构化、显式读写内存模块增强LLM。MemLLM解决了上述问题，通过动态交互提高LLM使用存储知识的能力。实验表明，MemLLM增强了LLM的性能和可解释性，尤其是在语言建模和知识密集型任务中。MemLLM是朝着通过内存增强使LLM更真实和基于事实的重要一步。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLMs受限于隐式存储机制，难以处理罕见事件和更新随时间变化的事实。</li>
<li>MemLLM是一种通过集成结构化、显式的读写内存模块来增强LLMs的方法。</li>
<li>MemLLM解决了LLMs在动态交互和使用存储知识方面的挑战。</li>
<li>MemLLM提高了LLM的性能和可解释性，特别是在语言建模和知识密集型任务中。</li>
<li>MemLLM对于防止幻觉具有挑战性，因为它具有可解释的内存机制。</li>
<li>MemLLM是朝着使LLM更真实和基于事实的方向迈出的重要一步。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2404.11672">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-263ba0d433221960937477d084b9ea40.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-7e8002bb699281c79ddbf75ed7124f80.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-428d3b8bbce14d78e2b2b6bf5cbbf43b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-4411c332258b38302a8a083488a7be32.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="PhD-A-ChatGPT-Prompted-Visual-hallucination-Evaluation-Dataset"><a href="#PhD-A-ChatGPT-Prompted-Visual-hallucination-Evaluation-Dataset" class="headerlink" title="PhD: A ChatGPT-Prompted Visual hallucination Evaluation Dataset"></a>PhD: A ChatGPT-Prompted Visual hallucination Evaluation Dataset</h2><p><strong>Authors:Jiazhen Liu, Yuhan Fu, Ruobing Xie, Runquan Xie, Xingwu Sun, Fengzong Lian, Zhanhui Kang, Xirong Li</strong></p>
<p>Multimodal Large Language Models (MLLMs) hallucinate, resulting in an emerging topic of visual hallucination evaluation (VHE). This paper contributes a ChatGPT-Prompted visual hallucination evaluation Dataset (PhD) for objective VHE at a large scale. The essence of VHE is to ask an MLLM questions about specific images to assess its susceptibility to hallucination. Depending on what to ask (objects, attributes, sentiment, etc.) and how the questions are asked, we structure PhD along two dimensions, i.e. task and mode. Five visual recognition tasks, ranging from low-level (object &#x2F; attribute recognition) to middle-level (sentiment &#x2F; position recognition and counting), are considered. Besides a normal visual QA mode, which we term PhD-base, PhD also asks questions with specious context (PhD-sec) or with incorrect context ({PhD-icc), or with AI-generated counter common sense images (PhD-ccs). We construct PhD by a ChatGPT-assisted semi-automated pipeline, encompassing four pivotal modules: task-specific hallucinatory item (hitem) selection, hitem-embedded question generation, specious &#x2F; incorrect context generation, and counter-common-sense (CCS) image generation. With over 14k daily images, 750 CCS images and 102k VQA triplets in total, PhD reveals considerable variability in MLLMs’ performance across various modes and tasks, offering valuable insights into the nature of hallucination. As such, PhD stands as a potent tool not only for VHE but may also play a significant role in the refinement of MLLMs. </p>
<blockquote>
<p>多模态大型语言模型（MLLMs）会产生幻觉，从而引发视觉幻觉评估（VHE）这一新兴话题。本文贡献了一个基于ChatGPT的幻觉评估数据集（PhD），用于大规模客观VHE。VHE的本质是向MLLM提出关于特定图像的问题，以评估其是否容易受到幻觉的影响。根据要问的问题（对象、属性、情感等）和提问的方式，我们按任务和模式两个维度构建PhD。我们考虑了五个视觉识别任务，从低级的（对象&#x2F;属性识别）到中级的（情感&#x2F;位置识别和计数）。除了正常的视觉问答模式（我们称之为PhD-base），PhD还会在具有特殊上下文（PhD-sec）或错误上下文（PhD-icc）的情况下提问，或使用AI生成的反常识图像（PhD-ccs）。我们通过ChatGPT辅助的半自动化管道构建PhD，包括四个关键模块：任务特定幻觉项目（hitem）选择、hitem嵌入问题生成、特殊&#x2F;错误上下文生成和反常识（CCS）图像生成。PhD每天涵盖超过14k张图像、750张常识图像和总共102k个VQA三元组，揭示了MLLMs在不同模式和任务之间的性能存在相当大的差异，为幻觉的性质提供了宝贵的见解。因此，PhD不仅是一个强大的VHE工具，也可能在MLLM的改进中发挥重要作用。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2403.11116v4">PDF</a> Accepted by CVPR 2025, Highlight</p>
<p><strong>摘要</strong></p>
<p>多模态大型语言模型（MLLMs）会产生幻觉，从而产生了视觉幻觉评估（VHE）这一新兴主题。本文贡献了一个基于ChatGPT的视觉幻觉评估数据集（PhD），用于大规模客观VHE。VHE的本质是向MLLM询问特定图像的问题，以评估其是否容易产生幻觉。根据询问的内容和方式（例如对象、属性、情感等）以及问题的结构，我们将数据集沿着任务和模式两个维度构建。考虑五种视觉识别任务，从低级的对象&#x2F;属性识别到中级的情感&#x2F;位置识别以及计数。除了正常的视觉问答模式（我们称之为PhD-base）外，PhD还针对具有奇异上下文（PhD-sec）、错误上下文（PhD-icc）或AI生成的反常识图像（PhD-ccs）的问题进行提问。我们通过ChatGPT辅助的半自动化管道构建PhD，包括四个关键模块：特定任务的幻觉项目选择、嵌入问题的生成、奇异&#x2F;错误上下文的生成和反常识（CCS）图像的生成。PhD每天包含超过14k张图像、750张反常识图像和总共的10.2万张视觉问答三元组，揭示了MLLM在不同模式和任务中的性能存在显著的可变性，为幻觉的性质提供了宝贵的见解。因此，PhD不仅是VHE的强大工具，还可能对MLLM的改进起到重要作用。</p>
<p><strong>要点摘要</strong></p>
<ol>
<li>多模态大型语言模型（MLLMs）存在视觉幻觉评估（VHE）问题。</li>
<li>本文介绍了一个基于ChatGPT的视觉幻觉评估数据集（PhD）。</li>
<li>PhD数据集包含多种视觉识别任务，从对象&#x2F;属性识别到情感&#x2F;位置识别和计数。</li>
<li>除了正常视觉问答模式外，PhD还包括具有奇异、错误上下文或AI生成的反常识图像的问题。</li>
<li>通过半自动化管道构建PhD，包括任务特定的幻觉项目选择、问题生成、上下文生成和图像生成等模块。</li>
<li>PhD数据集揭示了MLLM在不同模式和任务中的性能差异，为评估和改进MLLM的幻觉问题提供了宝贵数据。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2403.11116">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-8b5d922959896d026734d86087677548.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c73ea8ce2d172b06a01a1ca0c0322f44.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-da75e4a48e84ea3dcb95ec57081904a7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-42dc7b09b9f78e790cfcc1d9ac5e2ebc.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-b551ec68ebc031d4e1986b58491a7d7f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-603a715be06a18e66d506c6aff79b69d.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Citation-Enhanced-Generation-for-LLM-based-Chatbots"><a href="#Citation-Enhanced-Generation-for-LLM-based-Chatbots" class="headerlink" title="Citation-Enhanced Generation for LLM-based Chatbots"></a>Citation-Enhanced Generation for LLM-based Chatbots</h2><p><strong>Authors:Weitao Li, Junkai Li, Weizhi Ma, Yang Liu</strong></p>
<p>Large language models (LLMs) exhibit powerful general intelligence across diverse scenarios, including their integration into chatbots. However, a vital challenge of LLM-based chatbots is that they may produce hallucinated content in responses, which significantly limits their applicability. Various efforts have been made to alleviate hallucination, such as retrieval augmented generation and reinforcement learning with human feedback, but most of them require additional training and data annotation. In this paper, we propose a novel post-hoc Citation-Enhanced Generation (CEG) approach combined with retrieval argumentation. Unlike previous studies that focus on preventing hallucinations during generation, our method addresses this issue in a post-hoc way. It incorporates a retrieval module to search for supporting documents relevant to the generated content, and employs a natural language inference-based citation generation module. Once the statements in the generated content lack of reference, our model can regenerate responses until all statements are supported by citations. Note that our method is a training-free plug-and-play plugin that is capable of various LLMs. Experiments on various hallucination-related datasets show our framework outperforms state-of-the-art methods in both hallucination detection and response regeneration on three benchmarks. Our codes and dataset will be publicly available. </p>
<blockquote>
<p>大型语言模型（LLM）在多种场景中展现出强大的通用智能，包括在聊天机器人中的集成。然而，LLM驱动的聊天机器人面临的一个关键挑战是它们可能会产生幻觉内容作为回应，这极大地限制了其适用性。尽管已经做出了各种努力来缓解幻觉问题，如增强检索生成和通过人类反馈进行强化学习，但大多数方法都需要额外的训练和数据标注。在本文中，我们提出了一种新型的后验引用增强生成（CEG）方法，该方法结合了检索论证。不同于之前侧重于在生成过程中防止幻觉的研究，我们的方法以一种后验方式解决这一问题。它结合了一个检索模块来搜索与生成内容相关的支持文档，并采用基于自然语言推理的引用生成模块。一旦生成的内容中的陈述缺乏参考，我们的模型可以重新生成回应，直到所有陈述都得到引用的支持。值得注意的是，我们的方法是一种无需训练的即插即用插件，可以与各种LLM配合使用。在各种幻觉相关的数据集上的实验表明，我们的框架在三个基准测试中优于最先进的方法，在幻觉检测和响应再生方面均表现出色。我们的代码和数据集将公开可用。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2402.16063v4">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>大型语言模型（LLM）在多种场景中展现出强大的通用智能，尤其是在集成到聊天机器人方面。然而，LLM-based聊天机器人的一个重大挑战是可能会产生虚构的回应内容，这极大地限制了其适用性。尽管已有许多缓解虚构性的尝试，如增强检索生成和强化学习人类反馈，但大多数方法都需要额外的训练和标注数据。本文提出了一种新颖的后验式Citation-Enhanced Generation（CEG）方法，结合了检索论证。与其他研究不同，我们的方法专注于在生成后解决虚构性问题。它采用检索模块来搜索与生成内容相关的支持文档，并采用基于自然语言推理的引用生成模块。当生成的回应内容缺乏引用时，我们的模型会重新生成回应，直至所有陈述都得到引用的支持。值得注意的是，我们的方法是一种无需训练的即插即用插件，适用于各种LLM。在多个与虚构性相关的数据集上的实验表明，我们的框架在三个基准测试上优于最新方法在虚构性检测和回应再生方面的表现。我们的代码和数据集将公开提供。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLMs展现出强大的通用智能，集成到聊天机器人具有应用价值。</li>
<li>LLM-based聊天机器人面临虚构性回应的挑战，限制了其适用性。</li>
<li>当前减轻虚构性的方法多需额外的训练和标注数据。</li>
<li>本文提出一种新颖的后验式Citation-Enhanced Generation（CEG）方法来解决虚构性问题。</li>
<li>CEG方法结合检索论证，通过搜索支持文档来增强回应的可靠性。</li>
<li>CEG模型能重新生成回应直至所有陈述都得到引用的支持。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2402.16063">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-436d75fa27d3bb9480ba2ec248262c13.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-49ab13cb4418eea249d79f1573e0e923.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-83eb6dd04097ab69fdbef18edc243ec7.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="Rethinking-Channel-Dimensions-to-Isolate-Outliers-for-Low-bit-Weight-Quantization-of-Large-Language-Models"><a href="#Rethinking-Channel-Dimensions-to-Isolate-Outliers-for-Low-bit-Weight-Quantization-of-Large-Language-Models" class="headerlink" title="Rethinking Channel Dimensions to Isolate Outliers for Low-bit Weight   Quantization of Large Language Models"></a>Rethinking Channel Dimensions to Isolate Outliers for Low-bit Weight   Quantization of Large Language Models</h2><p><strong>Authors:Jung Hwan Heo, Jeonghoon Kim, Beomseok Kwon, Byeongwook Kim, Se Jung Kwon, Dongsoo Lee</strong></p>
<p>Large Language Models (LLMs) have recently demonstrated remarkable success across various tasks. However, efficiently serving LLMs has been a challenge due to the large memory bottleneck, specifically in small batch inference settings (e.g. mobile devices). Weight-only quantization can be a promising approach, but sub-4 bit quantization remains a challenge due to large-magnitude activation outliers. To mitigate the undesirable outlier effect, we first propose per-IC quantization, a simple yet effective method that creates quantization groups within each input channel (IC) rather than the conventional per-output-channel (per-OC). Our method is motivated by the observation that activation outliers affect the input dimension of the weight matrix, so similarly grouping the weights in the IC direction can isolate outliers within a group. We also find that activation outliers do not dictate quantization difficulty, and inherent weight sensitivities also exist. With per-IC quantization as a new outlier-friendly scheme, we propose Adaptive Dimensions (AdaDim), a versatile quantization framework that can adapt to various weight sensitivity patterns. We demonstrate the effectiveness of AdaDim by augmenting prior methods such as Round-To-Nearest and GPTQ, showing significant improvements across various language modeling benchmarks for both base (up to +4.7% on MMLU) and instruction-tuned (up to +10% on HumanEval) LLMs. Code is available at <a target="_blank" rel="noopener" href="https://github.com/johnheo/adadim-llm">https://github.com/johnheo/adadim-llm</a> </p>
<blockquote>
<p>大型语言模型（LLM）在各种任务中取得了显著的成功。然而，由于内存瓶颈问题，特别是在小批量推理设置（如移动设备）中，有效地服务LLM一直是一个挑战。权重仅量化可能是一种有前途的方法，但低于4位的量化仍然是一个挑战，因为存在大量幅度激活异常值。为了减轻不良异常值的影响，我们首先提出了针对每个输入通道（IC）的量化方法，这是一种简单有效的方法，在每个输入通道内创建量化组，而不是传统的每个输出通道（OC）。我们的方法源于观察到激活异常值影响权重矩阵的输入维度，因此按IC方向对权重进行分组可以隔离异常值到一个组内。我们还发现激活异常值并不决定量化的难度，权重本身也存在固有的敏感性。作为一种新的异常值友好的方案，我们提出了自适应维度（AdaDim），这是一个灵活的量化框架，能够适应各种权重敏感性模式。我们通过增强先前的Round-To-Nearest和GPTQ等方法来展示AdaDim的有效性，在各种语言建模基准测试中取得了显著改进，无论是在基础模型（MMLU上最多提高+4.7%）还是在指令调优模型（HumanEval上最多提高+10%）上。代码在<a target="_blank" rel="noopener" href="https://github.com/johnheo/adadim-llm%E4%B8%8A%E6%8F%90%E4%BE%9B%E3%80%82">https://github.com/johnheo/adadim-llm上提供。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.15531v4">PDF</a> ICLR 2024</p>
<p><strong>Summary</strong></p>
<p>大型语言模型（LLM）在各任务中表现出卓越性能，但其内存需求巨大，在小批量推理设置（如移动设备）中提供服务具有挑战性。针对这一问题，本文提出了一种新的量化方法——AdaDim，它结合了每输入通道（IC）的量化方案和自适应维度量化框架，以提高模型的量化性能并减少内存瓶颈。AdaDim可有效改善现有量化方法如Round-To-Nearest和GPTQ的性能，在多种语言建模基准测试中表现显著，包括基础模型和指令微调模型。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLMs在多种任务中表现出卓越性能，但内存需求大，特别是在小批量推理环境中。</li>
<li>现有量化方法面临挑战，特别是低于4位的量化由于激活异常值的问题。</li>
<li>论文提出了一种新的量化策略——AdaDim，结合每输入通道（IC）的量化方法和自适应维度量化框架。</li>
<li>AdaDim旨在通过创建量化组来隔离激活异常值，并适应不同的权重敏感性模式。</li>
<li>AdaDim对先前的量化方法进行了改进，显著提高了基础模型和指令微调模型在多种语言建模基准测试中的性能。</li>
<li>AdaDim方法已在GitHub上公开可用。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2309.15531">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-f84a009b9cc1590d5a0be407de627c50.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7066d0041548994d53da8fd596feac07.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-27690c950804726e61eaba5e0557ec70.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8ec67a10efc039cba5fb7d33fa3000fb.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2a0ea5e9940ccec2b53dee9cdfa7177b.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-fbd4f7dc5e3f1b810d64076ddb6fbf07.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="UMLS-KGI-BERT-Data-Centric-Knowledge-Integration-in-Transformers-for-Biomedical-Entity-Recognition"><a href="#UMLS-KGI-BERT-Data-Centric-Knowledge-Integration-in-Transformers-for-Biomedical-Entity-Recognition" class="headerlink" title="UMLS-KGI-BERT: Data-Centric Knowledge Integration in Transformers for   Biomedical Entity Recognition"></a>UMLS-KGI-BERT: Data-Centric Knowledge Integration in Transformers for   Biomedical Entity Recognition</h2><p><strong>Authors:Aidan Mannion, Thierry Chevalier, Didier Schwab, Lorraine Geouriot</strong></p>
<p>Pre-trained transformer language models (LMs) have in recent years become the dominant paradigm in applied NLP. These models have achieved state-of-the-art performance on tasks such as information extraction, question answering, sentiment analysis, document classification and many others. In the biomedical domain, significant progress has been made in adapting this paradigm to NLP tasks that require the integration of domain-specific knowledge as well as statistical modelling of language. In particular, research in this area has focused on the question of how best to construct LMs that take into account not only the patterns of token distribution in medical text, but also the wealth of structured information contained in terminology resources such as the UMLS. This work contributes a data-centric paradigm for enriching the language representations of biomedical transformer-encoder LMs by extracting text sequences from the UMLS. This allows for graph-based learning objectives to be combined with masked-language pre-training. Preliminary results from experiments in the extension of pre-trained LMs as well as training from scratch show that this framework improves downstream performance on multiple biomedical and clinical Named Entity Recognition (NER) tasks. </p>
<blockquote>
<p>预训练转换语言模型（LMs）近年来已成为应用NLP中的主导范式。这些模型在信息提取、问答、情感分析、文档分类等任务上达到了最新技术水平的表现。在生物医学领域，将这种范式适应于需要整合特定领域知识和语言统计建模的NLP任务方面，已经取得了重大进展。特别是，该领域的研究重点是如何构建最佳的LMs，这些LMs不仅要考虑医疗文本中的标记分布模式，还要利用术语资源（如UMLS）中包含的大量结构化信息。本文贡献了一种以数据为中心的方法，通过从UMLS中提取文本序列来丰富生物医学转换编码器LMs的语言表示。这允许将基于图的学习目标与掩码语言预训练相结合。初步实验结果显示，通过扩展预训练LMs或从头开始训练的结果表明，该框架在多生物医学和临床命名实体识别（NER）任务上的下游性能有所提升。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11170v2">PDF</a> Addition of v2 experiments</p>
<p><strong>Summary</strong></p>
<p>预训练转换器语言模型（LMs）已成为应用自然语言处理（NLP）中的主要范式，特别是在生物医学领域，这些模型在任务上取得了最先进的性能，如信息提取、问答、情感分析、文档分类等。研究集中于如何构建考虑医学文本中的标记分布以及术语资源（如UMLS）中丰富结构信息的语言模型。本文提出了一种以数据为中心的方法，通过从UMLS中提取文本序列来丰富生物医学转换器编码器的语言表示，结合基于图的学习目标和掩码语言预训练。初步实验结果表明，该框架在多生物医学和临床命名实体识别（NER）任务上提高了下游性能。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>预训练转换器语言模型（LMs）在自然语言处理（NLP）领域占据主导地位，特别是在完成信息提取、问答、情感分析和文档分类等任务时表现出色。</li>
<li>在生物医学领域，结合领域特定知识和语言统计模型的LMs研究已取得显著进展。</li>
<li>研究焦点在于如何构建考虑医学文本中的标记分布和术语资源（如UMLS）中的结构化信息的语言模型。</li>
<li>本文提出了一种数据为中心的方法，通过从UMLS提取文本序列来丰富生物医学LMs的语言表示。</li>
<li>该方法结合了基于图的学习目标和掩码语言预训练。</li>
<li>初步实验表明，该框架在多个生物医学和临床命名实体识别（NER）任务上提高了性能。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2307.11170">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-69a253479d960bf364fe976e4bc6f9da.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e1908b87bcdfe7b6466d7e0b895db446.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7f7738084745254cd417fbd933018ea5.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        文章作者:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        文章链接:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-04-20/LLM/">https://kedreamix.github.io/Talk2Paper/Paper/2025-04-20/LLM/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        版权声明:
                    </i>
                </span>
                <span class="reprint-info">
                    本博客所有文章除特別声明外，均采用
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    许可协议。转载请注明来源
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/LLM/">
                                    <span class="chip bg-color">LLM</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;上一篇</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-04-22/R1_Reasoning/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-6a0cd3eae0718cf031ebf00fa2d2899c.jpg" class="responsive-img" alt="R1_Reasoning">
                        
                        <span class="card-title">R1_Reasoning</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            R1_Reasoning 方向最新论文已更新，请持续关注 Update in 2025-04-22  Does Reinforcement Learning Really Incentivize Reasoning Capacity in   LLMs Beyond the Base Model?
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-04-22
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/R1-Reasoning/" class="post-category">
                                    R1_Reasoning
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/R1-Reasoning/">
                        <span class="chip bg-color">R1_Reasoning</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                下一篇&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-04-20/R1_Reasoning/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-52b875081e85d964ca333fe9ab756775.jpg" class="responsive-img" alt="R1_Reasoning">
                        
                        <span class="card-title">R1_Reasoning</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            R1_Reasoning 方向最新论文已更新，请持续关注 Update in 2025-04-20  Memorization vs. Reasoning Updating LLMs with New Knowledge
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-04-20
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/R1-Reasoning/" class="post-category">
                                    R1_Reasoning
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/R1-Reasoning/">
                        <span class="chip bg-color">R1_Reasoning</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- 代码块功能依赖 -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- 代码语言 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- 代码块复制 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- 代码块收缩 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC 悬浮按钮. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* 修复文章卡片 div 的宽度. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // 切换TOC目录展开收缩的相关操作.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;站点总字数:&nbsp;<span
                        class="white-color">16663.4k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- 运行天数提醒. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // 区分是否有年份.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = '本站已运行 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                daysTip = '本站已運行 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = '本站已运行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = '本站已運行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="关注我的知乎: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">知</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- 搜索遮罩框 -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span>
            <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- 白天和黑夜主题 -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- 回到顶部按钮 -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // 只在桌面版网页启用特效
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- 雪花特效 -->
    

    <!-- 鼠标星星特效 -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--腾讯兔小巢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- 动态标签 -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Σ(っ °Д °;)っ诶，页面崩溃了嘛？", clearTimeout(st)) : (document.title = "φ(゜▽゜*)♪咦，又好了！", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
