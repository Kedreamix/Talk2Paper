<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="LLM">
    <meta name="description" content="LLM æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-04-20  Lightning IR Straightforward Fine-tuning and Inference of   Transformer-based Language Models for Information Retrieval">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>LLM | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pic1.zhimg.com/v2-1799a5c57e34d17fa4a998e98eee9508.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">LLM</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/LLM/">
                                <span class="chip bg-color">LLM</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/LLM/" class="post-category">
                                LLM
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-04-20
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-04-26
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    11.3k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    45 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-04-20-æ›´æ–°"><a href="#2025-04-20-æ›´æ–°" class="headerlink" title="2025-04-20 æ›´æ–°"></a>2025-04-20 æ›´æ–°</h1><h2 id="Lightning-IR-Straightforward-Fine-tuning-and-Inference-of-Transformer-based-Language-Models-for-Information-Retrieval"><a href="#Lightning-IR-Straightforward-Fine-tuning-and-Inference-of-Transformer-based-Language-Models-for-Information-Retrieval" class="headerlink" title="Lightning IR: Straightforward Fine-tuning and Inference of   Transformer-based Language Models for Information Retrieval"></a>Lightning IR: Straightforward Fine-tuning and Inference of   Transformer-based Language Models for Information Retrieval</h2><p><strong>Authors:Ferdinand Schlatt, Maik FrÃ¶be, Matthias Hagen</strong></p>
<p>A wide range of transformer-based language models have been proposed for information retrieval tasks. However, including transformer-based models in retrieval pipelines is often complex and requires substantial engineering effort. In this paper, we introduce Lightning IR, an easy-to-use PyTorch Lightning-based framework for applying transformer-based language models in retrieval scenarios. Lightning IR provides a modular and extensible architecture that supports all stages of a retrieval pipeline: from fine-tuning and indexing to searching and re-ranking. Designed to be scalable and reproducible, Lightning IR is available as open-source: <a target="_blank" rel="noopener" href="https://github.com/webis-de/lightning-ir">https://github.com/webis-de/lightning-ir</a>. </p>
<blockquote>
<p>é’ˆå¯¹ä¿¡æ¯æ£€ç´¢ä»»åŠ¡ï¼Œå·²ç»æå‡ºäº†å¤šç§åŸºäºè½¬æ¢å™¨çš„è¯­è¨€æ¨¡å‹ã€‚ç„¶è€Œï¼Œåœ¨æ£€ç´¢ç®¡é“ä¸­åŒ…å«åŸºäºè½¬æ¢å™¨çš„æ¨¡å‹é€šå¸¸å¾ˆå¤æ‚ï¼Œéœ€è¦å¤§é‡çš„å·¥ç¨‹åŠªåŠ›ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†Lightning IRï¼Œè¿™æ˜¯ä¸€ä¸ªæ˜“äºä½¿ç”¨çš„åŸºäºPyTorch Lightningçš„æ¡†æ¶ï¼Œå¯ç”¨äºåœ¨æ£€ç´¢åœºæ™¯ä¸­åº”ç”¨åŸºäºè½¬æ¢å™¨çš„è¯­è¨€æ¨¡å‹ã€‚Lightning IRæä¾›äº†ä¸€ä¸ªæ¨¡å—åŒ–ä¸”å¯æ‰©å±•çš„æ¶æ„ï¼Œæ”¯æŒæ£€ç´¢ç®¡é“çš„æ‰€æœ‰é˜¶æ®µï¼šä»å¾®è°ƒã€ç´¢å¼•åˆ°æœç´¢å’Œé‡æ–°æ’åã€‚è®¾è®¡å¯ä¼¸ç¼©ä¸”å¯é‡å¤ï¼ŒLightning IRä½œä¸ºå¼€æºå¯ç”¨ï¼š<a target="_blank" rel="noopener" href="https://github.com/webis-de/lightning-ir%E3%80%82">https://github.com/webis-de/lightning-irã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2411.04677v5">PDF</a> Accepted as a demo at WSDMâ€™25</p>
<p><strong>Summary</strong>ï¼šä»‹ç»äº†Lightning IRï¼Œä¸€ä¸ªåŸºäºPyTorch Lightningçš„æ˜“ç”¨æ¡†æ¶ï¼Œç”¨äºåœ¨æ£€ç´¢åœºæ™¯ä¸­åº”ç”¨åŸºäºè½¬æ¢å™¨çš„è¯­è¨€æ¨¡å‹ã€‚è¯¥æ¡†æ¶æä¾›äº†ä¸€ç§æ¨¡å—åŒ–ä¸”å¯æ‰©å±•çš„æ¶æ„ï¼Œæ”¯æŒæ£€ç´¢ç®¡é“çš„æ‰€æœ‰é˜¶æ®µï¼šä»å¾®è°ƒã€ç´¢å¼•åˆ°æœç´¢å’Œé‡æ–°æ’åã€‚Lightning IRæ—¨åœ¨å®ç°å¯æ‰©å±•æ€§å’Œå¯é‡å¤æ€§ï¼Œå¯ä½œä¸ºå¼€æºè·å¾—ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>è¯¥è®ºæ–‡æå‡ºäº†Lightning IRæ¡†æ¶ï¼Œæ—¨åœ¨ç®€åŒ–åœ¨æ£€ç´¢ä»»åŠ¡ä¸­åº”ç”¨åŸºäºè½¬æ¢å™¨çš„è¯­è¨€æ¨¡å‹çš„å¤æ‚æ€§ã€‚</li>
<li>Lightning IRæ˜¯åŸºäºPyTorch Lightningæ„å»ºçš„ï¼Œæ˜“äºä½¿ç”¨ã€‚</li>
<li>è¯¥æ¡†æ¶æ”¯æŒæ£€ç´¢ç®¡é“çš„æ‰€æœ‰é˜¶æ®µï¼ŒåŒ…æ‹¬å¾®è°ƒã€ç´¢å¼•ã€æœç´¢å’Œé‡æ–°æ’åã€‚</li>
<li>Lightning IRå…·æœ‰æ¨¡å—åŒ–å’Œå¯æ‰©å±•æ€§ï¼Œèƒ½å¤Ÿé€‚åº”ä¸åŒçš„éœ€æ±‚ã€‚</li>
<li>è¯¥æ¡†æ¶æ—¨åœ¨å®ç°å¯æ‰©å±•æ€§å’Œå¯é‡å¤æ€§ï¼Œæœ‰åŠ©äºæé«˜ç ”ç©¶æ•ˆç‡å’Œæˆæœè´¨é‡ã€‚</li>
<li>Lightning IRä½œä¸ºå¼€æºæ¡†æ¶ï¼Œå¯æ–¹ä¾¿ç ”ç©¶äººå‘˜å’Œå…¶ä»–å¼€å‘è€…ä½¿ç”¨å’Œæ”¹è¿›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2411.04677">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-915690373e0e4adaee825b9058c9d60c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-afa40453b1512cdc0a614e9c4faa420f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5363ca9ab969b1bba343f8862b93fad8.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b873b5b4c07877a0ec8a8f4b2825df7b.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="ResiDual-Transformer-Alignment-with-Spectral-Decomposition"><a href="#ResiDual-Transformer-Alignment-with-Spectral-Decomposition" class="headerlink" title="ResiDual Transformer Alignment with Spectral Decomposition"></a>ResiDual Transformer Alignment with Spectral Decomposition</h2><p><strong>Authors:Lorenzo Basile, Valentino Maiorca, Luca Bortolussi, Emanuele RodolÃ , Francesco Locatello</strong></p>
<p>When examined through the lens of their residual streams, a puzzling property emerges in transformer networks: residual contributions (e.g., attention heads) sometimes specialize in specific tasks or input attributes. In this paper, we analyze this phenomenon in vision transformers, focusing on the spectral geometry of residuals, and explore its implications for modality alignment in vision-language models. First, we link it to the intrinsically low-dimensional structure of visual head representations, zooming into their principal components and showing that they encode specialized roles across a wide variety of input data distributions. Then, we analyze the effect of head specialization in multimodal models, focusing on how improved alignment between text and specialized heads impacts zero-shot classification performance. This specialization-performance link consistently holds across diverse pre-training data, network sizes, and objectives, demonstrating a powerful new mechanism for boosting zero-shot classification through targeted alignment. Ultimately, we translate these insights into actionable terms by introducing ResiDual, a technique for spectral alignment of the residual stream. Much like panning for gold, it lets the noise from irrelevant unit principal components (i.e., attributes) wash away to amplify task-relevant ones. Remarkably, this dual perspective on modality alignment yields fine-tuning level performance on different data distributions while modelling an extremely interpretable and parameter-efficient transformation, as we extensively show on 70 pre-trained network-dataset combinations (7 models, 10 datasets). </p>
<blockquote>
<p>å½“ä»å‰©ä½™æµçš„è§’åº¦å®¡è§†å˜å‹å™¨ç½‘ç»œæ—¶ï¼Œä¼šå‡ºç°ä¸€ä¸ªä»¤äººå›°æƒ‘çš„ç‰¹æ€§ï¼šå‰©ä½™è´¡çŒ®ï¼ˆä¾‹å¦‚ï¼Œæ³¨æ„åŠ›å¤´ï¼‰æœ‰æ—¶ä¼šä¸“é—¨ç”¨äºç‰¹å®šä»»åŠ¡æˆ–è¾“å…¥å±æ€§ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬åˆ†æäº†è§†è§‰å˜å‹å™¨ä¸­çš„è¿™ç§ç°è±¡ï¼Œé‡ç‚¹å…³æ³¨å‰©ä½™çš„è°±å‡ ä½•ï¼Œå¹¶æ¢ç´¢äº†å…¶å¯¹è§†è§‰è¯­è¨€æ¨¡å‹ä¸­æ¨¡æ€å¯¹é½çš„å½±å“ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬å°†å…¶ä¸è§†è§‰å¤´è¡¨ç¤ºçš„å›ºæœ‰ä½ç»´ç»“æ„è”ç³»èµ·æ¥ï¼Œæ·±å…¥æ¢ç©¶å…¶ä¸»è¦æˆåˆ†ï¼Œå¹¶è¡¨æ˜å®ƒä»¬åœ¨å„ç§è¾“å…¥æ•°æ®åˆ†å¸ƒä¸­ç¼–ç äº†ä¸“ä¸šè§’è‰²ã€‚æ¥ç€ï¼Œæˆ‘ä»¬åˆ†æäº†å¤´ä¸“ä¸šåŒ–åœ¨å¤šæ¨¡æ€æ¨¡å‹ä¸­çš„å½±å“ï¼Œé‡ç‚¹å…³æ³¨æ–‡æœ¬å’Œä¸“ä¸šåŒ–å¤´ä¹‹é—´æ”¹è¿›çš„å¯¹é½å¯¹é›¶æ ·æœ¬åˆ†ç±»æ€§èƒ½çš„å½±å“ã€‚è¿™ç§ä¸“ä¸šåŒ–å’Œæ€§èƒ½ä¹‹é—´çš„è”ç³»åœ¨å„ç§é¢„è®­ç»ƒæ•°æ®ã€ç½‘ç»œè§„æ¨¡å’Œç›®æ ‡ä¸­å§‹ç»ˆå­˜åœ¨ï¼Œè¯æ˜äº†ä¸€ç§é€šè¿‡æœ‰é’ˆå¯¹æ€§çš„å¯¹é½æ¥æé«˜é›¶æ ·æœ¬åˆ†ç±»æ€§èƒ½çš„æœ‰åŠ›æ–°æœºåˆ¶ã€‚æœ€ç»ˆï¼Œæˆ‘ä»¬å°†è¿™äº›è§è§£è½¬åŒ–ä¸ºå¯è¡Œçš„æœ¯è¯­ï¼Œå¹¶å¼•å…¥äº†ResiDualæŠ€æœ¯ï¼Œè¿™æ˜¯ä¸€ç§ç”¨äºå‰©ä½™æµçš„è°±å¯¹é½æŠ€æœ¯ã€‚å°±åƒæ·˜é‡‘ä¸€æ ·ï¼Œå®ƒå¯ä»¥è®©æ— å…³ç´§è¦çš„å•å…ƒä¸»æˆåˆ†ï¼ˆå³å±æ€§ï¼‰çš„å™ªå£°æ¶ˆå¤±ï¼Œä»¥æ”¾å¤§ä¸ä»»åŠ¡ç›¸å…³çš„æˆåˆ†ã€‚ä»¤äººæƒŠå¥‡çš„æ˜¯ï¼Œè¿™ç§å¯¹æ¨¡æ€å¯¹é½çš„åŒé‡è§†è§’åœ¨ä¸åŒæ•°æ®åˆ†å¸ƒä¸Šè¾¾åˆ°äº†å¾®è°ƒçº§åˆ«çš„æ€§èƒ½ï¼ŒåŒæ—¶å»ºç«‹äº†ä¸€ä¸ªæå…¶å¯è§£é‡Šå’Œå‚æ•°é«˜æ•ˆçš„è½¬æ¢æ¨¡å‹ï¼Œæˆ‘ä»¬åœ¨70ä¸ªé¢„è®­ç»ƒç½‘ç»œæ•°æ®é›†ç»„åˆï¼ˆ7ä¸ªæ¨¡å‹ï¼Œ10ä¸ªæ•°æ®é›†ï¼‰ä¸Šè¿›è¡Œäº†å¹¿æ³›å±•ç¤ºã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2411.00246v2">PDF</a> Published in Transactions on Machine Learning Research (TMLR)</p>
<p><strong>Summary</strong>ï¼šæœ¬æ–‡ç ”ç©¶äº†é€šè¿‡æ®‹å·®æµè§†è§’åˆ†ætransformerç½‘ç»œä¸­çš„è°œé¢˜å±æ€§ï¼Œå‘ç°æ®‹å·®è´¡çŒ®ï¼ˆå¦‚æ³¨æ„åŠ›å¤´ï¼‰æœ‰æ—¶ä¼šåœ¨ç‰¹å®šä»»åŠ¡æˆ–è¾“å…¥å±æ€§ä¸Šä¸“ä¸šåŒ–ã€‚æ–‡ç« é‡ç‚¹å…³æ³¨è§†è§‰transformerçš„è°±å‡ ä½•æ®‹å·®ï¼Œå¹¶æ¢è®¨äº†å…¶åœ¨è§†è§‰è¯­è¨€æ¨¡å‹çš„æ¨¡æ€å¯¹é½ä¸­çš„å½±å“ã€‚ç ”ç©¶å‘ç°è§†è§‰å¤´è¡¨ç¤ºçš„å›ºæœ‰ä½ç»´ç»“æ„ç¼–ç äº†ä¸“ä¸šåŒ–çš„è§’è‰²ï¼Œå¹¶ä¸”å¤´ä¸“ä¸šåŒ–åœ¨è·¨æ¨¡æ€æ¨¡å‹ä¸­æé«˜äº†æ–‡æœ¬ä¸ä¸“ä¸šåŒ–å¤´éƒ¨ä¹‹é—´çš„å¯¹é½æ•ˆæœï¼Œè¿›è€Œæé«˜é›¶æ ·æœ¬åˆ†ç±»æ€§èƒ½ã€‚ç ”ç©¶è¿˜å‘ç°å¤´ä¸“ä¸šåŒ–åœ¨è·¨ä¸åŒé¢„è®­ç»ƒæ•°æ®ã€ç½‘ç»œè§„æ¨¡å’Œç›®æ ‡æ—¶ï¼Œä¸“ä¸šåŒ–ä¸æ€§èƒ½çš„å…³è”å§‹ç»ˆå­˜åœ¨ã€‚æœ€ç»ˆï¼Œæ–‡ç« å°†è¿™äº›è§è§£è½¬åŒ–ä¸ºå®é™…åº”ç”¨ï¼Œæ¨å‡ºResiDualæŠ€æœ¯ï¼Œç”¨äºæ®‹å·®æµçš„è°±å¯¹é½ã€‚è¿™ä¸€æŠ€æœ¯æå¤§åœ°æé«˜äº†ä»»åŠ¡ç›¸å…³å•ä½çš„æŒ¯å¹…ï¼ŒåŒæ—¶åœ¨ä¸åŒçš„æ•°æ®åˆ†å¸ƒä¸Šè¾¾åˆ°äº†å¾®è°ƒçº§åˆ«çš„æ€§èƒ½ï¼Œå±•ç¤ºå‡ºæä¸ºå¯è§£é‡Šå’Œå‚æ•°é«˜æ•ˆçš„è½¬æ¢ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>æ®‹å·®è´¡çŒ®åœ¨transformerç½‘ç»œä¸­æœ‰æ—¶ä¼šåœ¨ç‰¹å®šä»»åŠ¡æˆ–è¾“å…¥å±æ€§ä¸Šä¸“ä¸šåŒ–ã€‚</li>
<li>è§†è§‰å¤´è¡¨ç¤ºçš„å›ºæœ‰ä½ç»´ç»“æ„ç¼–ç äº†ä¸“ä¸šåŒ–çš„è§’è‰²ã€‚</li>
<li>å¤´ä¸“ä¸šåŒ–èƒ½æé«˜è·¨æ¨¡æ€æ¨¡å‹ä¸­æ–‡æœ¬ä¸ä¸“ä¸šåŒ–å¤´éƒ¨ä¹‹é—´çš„å¯¹é½æ•ˆæœã€‚</li>
<li>ä¸“ä¸šåŒ–ä¸æ€§èƒ½çš„å…³è”åœ¨è·¨ä¸åŒçš„é¢„è®­ç»ƒæ•°æ®ã€ç½‘ç»œè§„æ¨¡å’Œç›®æ ‡æ—¶å§‹ç»ˆå­˜åœ¨ã€‚</li>
<li>æ¨å‡ºResiDualæŠ€æœ¯ï¼Œç”¨äºæ®‹å·®æµçš„è°±å¯¹é½ï¼Œæé«˜äº†ä»»åŠ¡ç›¸å…³å•ä½çš„æŒ¯å¹…ã€‚</li>
<li>ResiDualæŠ€æœ¯åœ¨ä¸åŒçš„æ•°æ®åˆ†å¸ƒä¸Šè¾¾åˆ°äº†å¾®è°ƒçº§åˆ«çš„æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2411.00246">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-1799a5c57e34d17fa4a998e98eee9508.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1d3bb102a48106f2afc1bc634c81d149.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-45bdd0f6c6ff406fcd29bdad356b86e0.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Improving-Instruction-Following-in-Language-Models-through-Activation-Steering"><a href="#Improving-Instruction-Following-in-Language-Models-through-Activation-Steering" class="headerlink" title="Improving Instruction-Following in Language Models through Activation   Steering"></a>Improving Instruction-Following in Language Models through Activation   Steering</h2><p><strong>Authors:Alessandro Stolfo, Vidhisha Balachandran, Safoora Yousefi, Eric Horvitz, Besmira Nushi</strong></p>
<p>The ability to follow instructions is crucial for numerous real-world applications of language models. In pursuit of deeper insights and more powerful capabilities, we derive instruction-specific vector representations from language models and use them to steer models accordingly. These vectors are computed as the difference in activations between inputs with and without instructions, enabling a modular approach to activation steering. We demonstrate how this method can enhance model adherence to constraints such as output format, length, and word inclusion, providing inference-time control over instruction following. Our experiments across four models demonstrate how we can use the activation vectors to guide models to follow constraints even without explicit instructions and to enhance performance when instructions are present. Additionally, we explore the compositionality of activation steering, successfully applying multiple instructions simultaneously. Finally, we demonstrate that steering vectors computed on instruction-tuned models can transfer to improve base models. Our findings demonstrate that activation steering offers a practical and scalable approach for fine-grained control in language generation. Our code and data are available at <a target="_blank" rel="noopener" href="https://github.com/microsoft/llm-steer-instruct">https://github.com/microsoft/llm-steer-instruct</a>. </p>
<blockquote>
<p>ä»è¯­è¨€æ¨¡å‹åœ¨ç°å®ä¸–ç•Œä¸­çš„ä¼—å¤šåº”ç”¨æ¥çœ‹ï¼Œéµå¾ªæŒ‡ä»¤çš„èƒ½åŠ›è‡³å…³é‡è¦ã€‚ä¸ºäº†è·å¾—æ›´æ·±å…¥çš„ç†è§£å’Œæ›´å¼ºå¤§çš„èƒ½åŠ›ï¼Œæˆ‘ä»¬ä»è¯­è¨€æ¨¡å‹ä¸­å¯¼å‡ºä¸æŒ‡ä»¤ç›¸å…³çš„å‘é‡è¡¨ç¤ºï¼Œå¹¶æ®æ­¤å¼•å¯¼æ¨¡å‹ã€‚è¿™äº›å‘é‡æ˜¯é€šè¿‡è®¡ç®—å¸¦æŒ‡ä»¤ä¸ä¸å¸¦æŒ‡ä»¤çš„è¾“å…¥ä¹‹é—´çš„æ¿€æ´»å·®å¼‚æ¥å¾—åˆ°çš„ï¼Œè¿™ä½¿å¾—æ¿€æ´»å¼•å¯¼å¯ä»¥é‡‡ç”¨æ¨¡å—åŒ–æ–¹æ³•ã€‚æˆ‘ä»¬å±•ç¤ºäº†è¿™ç§æ–¹æ³•å¦‚ä½•å¢å¼ºæ¨¡å‹å¯¹è¾“å‡ºæ ¼å¼ã€é•¿åº¦å’Œè¯æ±‡åŒ…å«ç­‰çº¦æŸçš„éµå¾ªèƒ½åŠ›ï¼Œä¸ºæŒ‡ä»¤éµå¾ªæä¾›æ¨ç†æ—¶çš„æ§åˆ¶ã€‚æˆ‘ä»¬åœ¨å››ä¸ªæ¨¡å‹ä¸Šçš„å®éªŒå±•ç¤ºäº†å¦‚ä½•ä½¿ç”¨æ¿€æ´»å‘é‡æ¥æŒ‡å¯¼æ¨¡å‹éµå¾ªçº¦æŸï¼Œå³ä½¿åœ¨æ²¡æœ‰ä»»ä½•æ˜ç¡®æŒ‡ä»¤çš„æƒ…å†µä¸‹ä¹Ÿèƒ½æé«˜æ€§èƒ½ï¼Œå¹¶åœ¨æœ‰æŒ‡ä»¤æ—¶å¢å¼ºè¡¨ç°ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜æ¢ç´¢äº†æ¿€æ´»å¼•å¯¼çš„ç»„æˆæ€§ï¼Œèƒ½å¤ŸåŒæ—¶åº”ç”¨å¤šä¸ªæŒ‡ä»¤ã€‚æœ€åï¼Œæˆ‘ä»¬è¯æ˜äº†åœ¨æŒ‡ä»¤è°ƒæ•´æ¨¡å‹ä¸Šè®¡ç®—çš„è½¬å‘å‘é‡å¯ä»¥è½¬ç§»åˆ°åŸºç¡€æ¨¡å‹ä¸­ä»¥è¿›è¡Œæ”¹è¿›ã€‚æˆ‘ä»¬çš„ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œæ¿€æ´»å¼•å¯¼ä¸ºè¯­è¨€ç”Ÿæˆä¸­çš„ç²¾ç»†ç²’åº¦æ§åˆ¶æä¾›äº†ä¸€ç§å®ç”¨ä¸”å¯æ‰©å±•çš„æ–¹æ³•ã€‚æˆ‘ä»¬çš„ä»£ç å’Œæ•°æ®åœ¨<a target="_blank" rel="noopener" href="https://github.com/microsoft/llm-steer-instruct%E3%80%82">https://github.com/microsoft/llm-steer-instructã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2410.12877v2">PDF</a> ICLR 2025</p>
<p><strong>Summary</strong></p>
<p>è¯­è¨€æ¨¡å‹çš„æŒ‡ä»¤éµå¾ªèƒ½åŠ›åœ¨ç°å®åº”ç”¨ä¸­çš„é‡è¦æ€§æ—¥ç›Šå‡¸æ˜¾ã€‚ç ”ç©¶å›¢é˜Ÿä»è¯­è¨€æ¨¡å‹ä¸­æ¨å¯¼å‡ºæŒ‡ä»¤ç‰¹å®šçš„å‘é‡è¡¨ç¤ºï¼Œç”¨äºå¼•å¯¼æ¨¡å‹éµå¾ªæŒ‡ä»¤ã€‚è¿™äº›å‘é‡é€šè¿‡è®¡ç®—å¸¦æœ‰å’Œä¸å¸¦æŒ‡ä»¤çš„è¾“å…¥æ¿€æ´»å·®å¼‚å¾—åˆ°ï¼Œä¸ºæ¿€æ´»å¼•å¯¼æä¾›äº†æ¨¡å—åŒ–æ–¹æ³•ã€‚ç ”ç©¶å±•ç¤ºäº†è¯¥æ–¹æ³•å¦‚ä½•æå‡æ¨¡å‹å¯¹è¾“å‡ºæ ¼å¼ã€é•¿åº¦å’Œè¯æ±‡çº¦æŸçš„éµå¾ªèƒ½åŠ›ï¼Œå®ç°æ¨ç†æ—¶çš„æŒ‡ä»¤éµå¾ªæ§åˆ¶ã€‚å®éªŒè¡¨æ˜ï¼Œæ¿€æ´»å‘é‡å¯ç”¨äºå¼•å¯¼æ¨¡å‹éµå¾ªçº¦æŸï¼Œç”šè‡³åœ¨æ— æ˜ç¡®æŒ‡ä»¤çš„æƒ…å†µä¸‹æå‡æ€§èƒ½ã€‚æ­¤å¤–ï¼Œç ”ç©¶è¿˜æ¢ç´¢äº†æ¿€æ´»å¼•å¯¼çš„ç»„åˆæ€§ï¼ŒæˆåŠŸåŒæ—¶åº”ç”¨å¤šä¸ªæŒ‡ä»¤ã€‚æœ€ç»ˆï¼Œç ”ç©¶è¯æ˜åœ¨æŒ‡ä»¤è°ƒæ•´æ¨¡å‹ä¸Šè®¡ç®—çš„å¼•å¯¼å‘é‡å¯è½¬ç§»è‡³åŸºç¡€æ¨¡å‹ä»¥æå‡æ€§èƒ½ã€‚ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œæ¿€æ´»å¼•å¯¼ä¸ºä¸€ç§å®ç”¨ä¸”å¯ä¼¸ç¼©çš„æ–¹æ³•ï¼Œå¯å®ç°è¯­è¨€ç”Ÿæˆä¸­çš„ç²¾ç»†æ§åˆ¶ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è¯­è¨€æ¨¡å‹çš„æŒ‡ä»¤éµå¾ªèƒ½åŠ›å¯¹äºå…¶ç°å®åº”ç”¨éå¸¸é‡è¦ã€‚</li>
<li>é€šè¿‡æ¨å¯¼æŒ‡ä»¤ç‰¹å®šçš„å‘é‡è¡¨ç¤ºï¼Œå¯ä»¥å¼•å¯¼è¯­è¨€æ¨¡å‹éµå¾ªæŒ‡ä»¤ã€‚</li>
<li>æ¿€æ´»å‘é‡æ˜¯é€šè¿‡è®¡ç®—å¸¦æœ‰å’Œä¸å¸¦æŒ‡ä»¤çš„è¾“å…¥æ¿€æ´»å·®å¼‚å¾—åˆ°çš„ã€‚</li>
<li>è¯¥æ–¹æ³•å¯ä»¥æé«˜æ¨¡å‹å¯¹è¾“å‡ºæ ¼å¼ã€é•¿åº¦å’Œè¯æ±‡çº¦æŸçš„éµå¾ªèƒ½åŠ›ã€‚</li>
<li>æ¿€æ´»å‘é‡å¯ç”¨äºåœ¨æ— æ˜ç¡®æŒ‡ä»¤çš„æƒ…å†µä¸‹å¼•å¯¼æ¨¡å‹éµå¾ªçº¦æŸï¼Œæå‡æ€§èƒ½ã€‚</li>
<li>ç ”ç©¶å±•ç¤ºäº†æ¿€æ´»å¼•å¯¼çš„ç»„åˆæ€§ï¼Œå¯ä»¥åŒæ—¶åº”ç”¨å¤šä¸ªæŒ‡ä»¤ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2410.12877">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-5aa9870ce353ea48f4fdce673c55420b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-11861ba2fdc95dd9073b6a186517c77f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1c41f716b265f94c815d10db24ea322b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-79cc63b0219a45f86347d15f37d10ced.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-d0135e0797280ddf08c7c7f67e825400.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Teaching-Transformers-Causal-Reasoning-through-Axiomatic-Training"><a href="#Teaching-Transformers-Causal-Reasoning-through-Axiomatic-Training" class="headerlink" title="Teaching Transformers Causal Reasoning through Axiomatic Training"></a>Teaching Transformers Causal Reasoning through Axiomatic Training</h2><p><strong>Authors:Aniket Vashishtha, Abhinav Kumar, Atharva Pandey, Abbavaram Gowtham Reddy, Kabir Ahuja, Vineeth N Balasubramanian, Amit Sharma</strong></p>
<p>For text-based AI systems to interact in the real world, causal reasoning is an essential skill. Since active interventions are costly, we study to what extent a system can learn causal reasoning from symbolic demonstrations of causal axioms. Specifically, we present an axiomatic training method where the system learns from multiple demonstrations of a causal axiom (or rule), rather than incorporating the axiom as an inductive bias or inferring it from data values. A key question is whether the system would learn to generalize from the axiom demonstrations to more complex scenarios. Our results, based on applying axiomatic training to learn the transitivity axiom and d-separation rule, indicate that such generalization is possible. To avoid data contamination issues, we start with a 67 million parameter transformer model and train it from scratch. On both tasks, we find that a model trained on linear causal chains (along with some noisy variations) can generalize well to complex graphs, including longer causal chains, causal chains with reversed order, and graphs with branching.To handle diverse text inputs, the same method is extended to finetune language models. Finetuning Llama-3.1 8B model on our axiomatic data leads to significant gains on causal benchmarks such as Corr2Cause and CLEAR, in some cases providing state-of-the-art performance surpassing GPT-4. </p>
<blockquote>
<p>å¯¹äºåŸºäºæ–‡æœ¬çš„AIç³»ç»Ÿåœ¨ç°å®ä¸–ç•Œä¸­çš„äº¤äº’æ¥è¯´ï¼Œå› æœæ¨ç†æ˜¯ä¸€é¡¹åŸºæœ¬æŠ€å·§ã€‚ç”±äºä¸»åŠ¨å¹²é¢„æˆæœ¬é«˜æ˜‚ï¼Œæˆ‘ä»¬ç ”ç©¶ç³»ç»Ÿèƒ½ä»å› æœå…¬ç†çš„ç¬¦å·æ¼”ç¤ºä¸­å­¦ä¹ åˆ°ä½•ç§ç¨‹åº¦çš„å› æœæ¨ç†ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§å…¬ç†åŒ–è®­ç»ƒæ–¹æ³•ï¼Œè¯¥ç³»ç»Ÿä»å› æœå…¬ç†ï¼ˆæˆ–è§„åˆ™ï¼‰çš„å¤šä¸ªæ¼”ç¤ºä¸­å­¦ä¹ ï¼Œè€Œä¸æ˜¯å°†å…¬ç†ä½œä¸ºå½’çº³åè§æˆ–ä»æ•°æ®å€¼ä¸­æ¨æ–­å‡ºæ¥ã€‚ä¸€ä¸ªå…³é”®çš„é—®é¢˜æ˜¯ï¼Œç³»ç»Ÿæ˜¯å¦ä¼šä»å…¬ç†æ¼”ç¤ºä¸­å­¦ä¹ å¹¶æ¨å¹¿åˆ°æ›´å¤æ‚çš„åœºæ™¯ã€‚æˆ‘ä»¬çš„ç»“æœåŸºäºåº”ç”¨å…¬ç†åŒ–è®­ç»ƒæ¥å­¦ä¹ ä¼ é€’æ€§å…¬ç†å’Œd-åˆ†ç¦»è§„åˆ™ï¼Œè¡¨æ˜è¿™ç§æ¨å¹¿æ˜¯å¯èƒ½çš„ã€‚ä¸ºäº†é¿å…æ•°æ®æ±¡æŸ“é—®é¢˜ï¼Œæˆ‘ä»¬ä»6700ä¸‡ä¸ªå‚æ•°çš„å˜å‹å™¨æ¨¡å‹å¼€å§‹ï¼Œå¹¶ä»é›¶å¼€å§‹å¯¹å…¶è¿›è¡Œè®­ç»ƒã€‚åœ¨è¿™ä¸¤é¡¹ä»»åŠ¡ä¸­ï¼Œæˆ‘ä»¬å‘ç°è®­ç»ƒæœ‰ç´ çš„ç³»ç»Ÿåœ¨é¢ä¸´çº¿æ€§å› æœé“¾ï¼ˆä»¥åŠå¸¦æœ‰å™ªå£°çš„ä¸€äº›å˜ä½“ï¼‰æ—¶ï¼Œèƒ½å¤Ÿå¾ˆå¥½åœ°é€‚åº”å¤æ‚çš„å›¾å½¢ç»“æ„ï¼ŒåŒ…æ‹¬æ›´é•¿çš„å› æœé“¾ã€é€†å‘é¡ºåºçš„å› æœé“¾å’Œå¸¦æœ‰åˆ†æ”¯çš„å›¾å½¢ã€‚ä¸ºäº†å¤„ç†å¤šæ ·çš„æ–‡æœ¬è¾“å…¥ï¼Œå¯¹è¯­è¨€æ¨¡å‹è¿›è¡Œäº†å¾®è°ƒä»¥åº”ç”¨ç›¸åŒçš„æ–¹æ³•ã€‚åœ¨å…¬ç†æ•°æ®ä¸Šå¾®è°ƒLlama-3.1çš„8Bæ¨¡å‹åï¼Œåœ¨è¯¸å¦‚Corr2Causeå’ŒCLEARç­‰å› æœåŸºå‡†æµ‹è¯•ä¸­å–å¾—äº†æ˜¾è‘—çš„æ”¶ç›Šï¼Œåœ¨æŸäº›æƒ…å†µä¸‹è¶…è¶Šäº†GPT-4çš„æœ€å…ˆè¿›è¡¨ç°ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2407.07612v2">PDF</a> </p>
<p><strong>Summary</strong>ï¼šåŸºäºç¬¦å·å› æœå…¬ç†æ¼”ç¤ºçš„æ–‡æœ¬ç”Ÿæˆæ¨¡å‹å¯ä»¥å­¦ä¹ å› æœæ¨ç†æŠ€èƒ½ï¼Œå¹¶ä»ç®€å•çš„å› æœé“¾æ¨å¹¿åˆ°æ›´å¤æ‚çš„åœºæ™¯ã€‚ç ”ç©¶é‡‡ç”¨äº†ä¸€ç§å…¬ç†åŒ–è®­ç»ƒæ–¹æ³•ï¼Œå¹¶å±•ç¤ºäº†è¿™ç§è®­ç»ƒå¯¹çº¿æ€§å› æœé“¾æ¨¡å‹çš„é€‚ç”¨æ€§ã€‚é€šè¿‡å¯¹å¤§å‹è¯­è¨€æ¨¡å‹è¿›è¡Œå¾®è°ƒï¼Œè¯¥æ–¹æ³•å¯ä»¥æ˜¾è‘—æé«˜å› æœåŸºå‡†æµ‹è¯•çš„æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>å› æœæ¨ç†å¯¹äºæ–‡æœ¬ç”Ÿæˆçš„AIç³»ç»Ÿåœ¨ç°å®ä¸–ç•Œä¸­çš„äº¤äº’è‡³å…³é‡è¦ã€‚</li>
<li>å…¬ç†åŒ–è®­ç»ƒæ–¹æ³•æ˜¯AIç³»ç»Ÿå­¦ä¹ å› æœæ¨ç†çš„ä¸€ç§æœ‰æ•ˆæ–¹å¼ï¼Œé€šè¿‡ä»å¤šä¸ªå› æœå…¬ç†æ¼”ç¤ºä¸­å­¦ä¹ ï¼Œè€Œä¸æ˜¯ä½œä¸ºå½’çº³åè§æˆ–ä»æ•°æ®å€¼ä¸­æ¨æ–­å‡ºæ¥ã€‚</li>
<li>ç³»ç»Ÿå¯ä»¥ä»ç®€å•çš„å› æœé“¾å…¬ç†æ¨å¹¿åˆ°æ›´å¤æ‚çš„åœºæ™¯ï¼Œå¦‚æ›´é•¿ã€é¡ºåºé¢ å€’å’Œåˆ†æ”¯çš„å› æœé“¾ã€‚</li>
<li>é‡‡ç”¨è¿™ç§è®­ç»ƒæ–¹æ³•çš„æ¨¡å‹åœ¨å› æœåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºæ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚</li>
<li>ç ”ç©¶ä½¿ç”¨äº†åŒ…å«çº¦67ç™¾ä¸‡å‚æ•°çš„å˜å‹å™¨æ¨¡å‹ï¼Œå¹¶ä»åŸºç¡€å¼€å§‹è®­ç»ƒä»¥é¿å…æ•°æ®æ±¡æŸ“é—®é¢˜ã€‚</li>
<li>å¯¹å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆå¦‚Llama-3.1 8Bæ¨¡å‹ï¼‰è¿›è¡Œå¾®è°ƒå¯ä»¥è¿›ä¸€æ­¥æé«˜æ€§èƒ½ï¼Œåœ¨æŸäº›æƒ…å†µä¸‹è¾¾åˆ°æˆ–è¶…è¶Šäº†GPT-4çš„è¡¨ç°ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2407.07612">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-08e9dc958659494830840d0ed6c59b58.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-2c10c9871ecdd2c2f2907f9dd8784749.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-7df41c3f0336f9513e238c78c0d391d7.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="F-LMM-Grounding-Frozen-Large-Multimodal-Models"><a href="#F-LMM-Grounding-Frozen-Large-Multimodal-Models" class="headerlink" title="F-LMM: Grounding Frozen Large Multimodal Models"></a>F-LMM: Grounding Frozen Large Multimodal Models</h2><p><strong>Authors:Size Wu, Sheng Jin, Wenwei Zhang, Lumin Xu, Wentao Liu, Wei Li, Chen Change Loy</strong></p>
<p>Endowing Large Multimodal Models (LMMs) with visual grounding capability can significantly enhance AIsâ€™ understanding of the visual world and their interaction with humans. However, existing methods typically fine-tune the parameters of LMMs to learn additional segmentation tokens and overfit grounding and segmentation datasets. Such a design would inevitably cause a catastrophic diminution in the indispensable conversational capability of general AI assistants. In this paper, we comprehensively evaluate state-of-the-art grounding LMMs across a suite of multimodal question-answering benchmarks, observing drastic performance drops that indicate vanishing general knowledge comprehension and weakened instruction following ability. To address this issue, we present F-LMM â€“ grounding frozen off-the-shelf LMMs in human-AI conversations â€“ a straightforward yet effective design based on the fact that word-pixel correspondences conducive to visual grounding inherently exist in the attention mechanism of well-trained LMMs. Using only a few trainable CNN layers, we can translate word-pixel attention weights to mask logits, which a SAM-based mask refiner can further optimise. Our F-LMM neither learns special segmentation tokens nor utilises high-quality grounded instruction-tuning data, but achieves competitive performance on referring expression segmentation and panoptic narrative grounding benchmarks while completely preserving LMMsâ€™ original conversational ability. Additionally, with instruction-following ability preserved and grounding ability obtained, F-LMM can be directly applied to complex tasks like reasoning segmentation, grounded conversation generation and visual chain-of-thought reasoning. Our code can be found at <a target="_blank" rel="noopener" href="https://github.com/wusize/F-LMM">https://github.com/wusize/F-LMM</a>. </p>
<blockquote>
<p>èµ‹äºˆå¤§å‹å¤šæ¨¡æ€æ¨¡å‹ï¼ˆLMMsï¼‰è§†è§‰æ¥åœ°èƒ½åŠ›å¯ä»¥æ˜¾è‘—å¢å¼ºAIå¯¹è§†è§‰ä¸–ç•Œçš„ç†è§£ä»¥åŠä¸äººç±»çš„äº¤äº’èƒ½åŠ›ã€‚ç„¶è€Œï¼Œç°æœ‰æ–¹æ³•é€šå¸¸é€šè¿‡å¯¹LMMçš„å‚æ•°è¿›è¡Œå¾®è°ƒæ¥å­¦ä¹ é¢å¤–çš„åˆ†å‰²ä»¤ç‰Œï¼Œå¹¶è¿‡åº¦é€‚åº”æ¥åœ°å’Œåˆ†å‰²æ•°æ®é›†ã€‚è¿™ç§è®¾è®¡ä¸å¯é¿å…åœ°ä¼šå¯¼è‡´é€šç”¨AIåŠ©ç†å¿…ä¸å¯å°‘çš„ç®¡ç†èƒ½åŠ›å‘ç”Ÿç¾éš¾æ€§é™ä½ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬å…¨é¢è¯„ä¼°äº†æœ€å…ˆè¿›çš„åœ°åŸºLMMåœ¨ä¸€ç³»åˆ—å¤šæ¨¡æ€é—®ç­”åŸºå‡†æµ‹è¯•ä¸Šçš„è¡¨ç°ï¼Œè§‚å¯Ÿåˆ°æ€§èƒ½æ€¥å‰§ä¸‹é™ï¼Œè¡¨æ˜ä¸€èˆ¬çŸ¥è¯†ç†è§£æ¶ˆå¤±ï¼Œä»¥åŠæŒ‡ä»¤æ‰§è¡Œèƒ½åŠ›å‡å¼±ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†F-LMMâ€”â€”åœ¨äººæœºå¯¹è¯ä¸­å†»ç»“ç°æˆçš„LMMè¿›è¡Œæ¥åœ°ï¼Œè¿™æ˜¯ä¸€ä¸ªç®€å•è€Œæœ‰æ•ˆçš„è®¾è®¡ï¼ŒåŸºäºè¿™æ ·ä¸€ä¸ªäº‹å®ï¼šåœ¨è®­ç»ƒè‰¯å¥½çš„LMMçš„æ³¨æ„åŠ›æœºåˆ¶ä¸­ï¼Œæœ‰åˆ©äºè§†è§‰æ¥åœ°çš„å•è¯åƒç´ å¯¹åº”å…³ç³»æ˜¯å›ºæœ‰çš„ã€‚é€šè¿‡ä½¿ç”¨åªæœ‰å°‘æ•°å¯è®­ç»ƒçš„CNNå±‚ï¼Œæˆ‘ä»¬å¯ä»¥å°†å•è¯åƒç´ æ³¨æ„åŠ›æƒé‡è½¬æ¢ä¸ºæ©ç å¯¹æ•°æ¦‚ç‡ï¼ŒSAMåŸºæ©ç ç²¾ç‚¼å™¨å¯ä»¥è¿›ä¸€æ­¥ä¼˜åŒ–å®ƒã€‚æˆ‘ä»¬çš„F-LMMæ—¢ä¸éœ€è¦å­¦ä¹ ç‰¹æ®Šçš„åˆ†å‰²ä»¤ç‰Œï¼Œä¹Ÿä¸éœ€è¦åˆ©ç”¨é«˜è´¨é‡çš„åœ°åŸºæŒ‡ä»¤è°ƒæ•´æ•°æ®ï¼Œå°±èƒ½åœ¨æŒ‡ä»£è¡¨è¾¾å¼åˆ†å‰²å’Œå…¨æ™¯å™äº‹æ¥åœ°åŸºå‡†æµ‹è¯•ä¸Šå–å¾—æœ‰ç«äº‰åŠ›çš„è¡¨ç°ï¼ŒåŒæ—¶å®Œå…¨ä¿ç•™LMMsåŸæœ‰çš„å¯¹è¯èƒ½åŠ›ã€‚æ­¤å¤–ï¼Œåœ¨ä¿ç•™æŒ‡ä»¤æ‰§è¡Œèƒ½åŠ›å¹¶è·å¾—æ¥åœ°èƒ½åŠ›çš„æƒ…å†µä¸‹ï¼ŒF-LMMå¯ä»¥ç›´æ¥åº”ç”¨äºæ¨ç†åˆ†å‰²ã€åŸºäºæ¥åœ°çš„å¯¹è¯ç”Ÿæˆå’Œè§†è§‰æ€ç»´é“¾æ¨ç†ç­‰å¤æ‚ä»»åŠ¡ã€‚æˆ‘ä»¬çš„ä»£ç å¯ä»¥åœ¨<a target="_blank" rel="noopener" href="https://github.com/wusize/F-LMM">https://github.com/wusize/F-LMM</a>æ‰¾åˆ°ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2406.05821v3">PDF</a> Project Page: <a target="_blank" rel="noopener" href="https://github.com/wusize/F-LMM">https://github.com/wusize/F-LMM</a></p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ¢è®¨äº†å¤§å‹å¤šæ¨¡æ€æ¨¡å‹ï¼ˆLMMsï¼‰çš„è§†è§‰æ¥åœ°èƒ½åŠ›å¯¹å¢å¼ºAIå¯¹è§†è§‰ä¸–ç•Œçš„ç†è§£ä¸äººç±»äº¤äº’çš„é‡è¦æ€§ã€‚ç„¶è€Œï¼Œç°æœ‰æ–¹æ³•é€šå¸¸é€šè¿‡å¾®è°ƒLMMå‚æ•°æ¥å­¦ä¹ é¢å¤–çš„åˆ†å‰²ä»¤ç‰Œï¼Œå¹¶è¿‡åº¦æ‹Ÿåˆæ¥åœ°å’Œåˆ†å‰²æ•°æ®é›†ï¼Œè¿™ä¼šæŸå®³AIåŠ©æ‰‹ä¸å¯æˆ–ç¼ºçš„å¯¹ä¸€èˆ¬çŸ¥è¯†çš„ç†è§£å’ŒæŒ‡ä»¤æ‰§è¡Œèƒ½åŠ›ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œæœ¬æ–‡è¯„ä¼°äº†æœ€å…ˆè¿›çš„å¤šæ¨¡æ€é—®é¢˜å›ç­”åŸºå‡†æµ‹è¯•ä¸­çš„æ¥åœ°LMMæ€§èƒ½ï¼Œå¹¶è§‚å¯Ÿåˆ°æ€§èƒ½æ€¥å‰§ä¸‹é™ã€‚ä¸ºæ­¤ï¼Œæå‡ºäº†ä¸€ç§åŸºäºç°æœ‰é¢„è®­ç»ƒLMMçš„è§†è§‰æ¥åœ°æ–¹æ³•â€”â€”F-LMMã€‚é€šè¿‡åˆ©ç”¨LMMå†…éƒ¨çš„æ³¨æ„åŠ›æœºåˆ¶ï¼Œåœ¨ä¿ç•™åŸæœ‰å¯¹è¯èƒ½åŠ›çš„åŸºç¡€ä¸Šï¼Œå®ç°è¯åƒç´ å¯¹åº”å…³ç³»å¯è§†åŒ–æ¥åœ°ã€‚F-LMMæ— éœ€å­¦ä¹ ç‰¹æ®Šåˆ†å‰²ä»¤ç‰Œï¼Œä¹Ÿä¸ä¾èµ–é«˜è´¨é‡æ¥åœ°æŒ‡ä»¤å¾®è°ƒæ•°æ®ï¼Œä½†ä»èƒ½åœ¨æŒ‡ä»£è¡¨è¾¾å¼åˆ†å‰²å’Œå…¨è§†å™äº‹æ¥åœ°åŸºå‡†æµ‹è¯•ä¸­å®ç°å…·æœ‰ç«äº‰åŠ›çš„æ€§èƒ½ã€‚æ­¤å¤–ï¼Œå‡­å€Ÿå…¶ä¿ç•™çš„æŒ‡ä»¤æ‰§è¡Œèƒ½åŠ›å’Œè§†è§‰æ¥åœ°èƒ½åŠ›ï¼ŒF-LMMå¯ç›´æ¥åº”ç”¨äºå¤æ‚ä»»åŠ¡å¦‚æ¨ç†åˆ†å‰²ã€åŸºäºè§†è§‰çš„å¯¹è¯ç”Ÿæˆå’Œè§†è§‰é“¾æ¨ç†ç­‰ã€‚æ›´å¤šè¯¦ç»†ä¿¡æ¯è¯·å‚è§ç›¸å…³ç½‘ç«™ï¼š<a target="_blank" rel="noopener" href="https://github.com/wusize/F-LMM">ç½‘å€é“¾æ¥</a>ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LMMsçš„è§†è§‰æ¥åœ°èƒ½åŠ›å¯¹å¢å¼ºAIä¸äººç±»çš„äº¤äº’è‡³å…³é‡è¦ã€‚</li>
<li>ç°æœ‰æ–¹æ³•å¾®è°ƒLMMå‚æ•°ä»¥å­¦ä¹ é¢å¤–çš„åˆ†å‰²ä»¤ç‰Œå¹¶è¿‡åº¦æ‹Ÿåˆæ•°æ®ï¼Œå¯¼è‡´ä¸€èˆ¬çŸ¥è¯†ç†è§£å’ŒæŒ‡ä»¤æ‰§è¡Œèƒ½åŠ›ä¸‹é™ã€‚</li>
<li>é€šè¿‡è¯„ä¼°å¤šç§å¤šæ¨¡æ€é—®é¢˜å›ç­”åŸºå‡†æµ‹è¯•ï¼Œå‘ç°ç°æœ‰æ–¹æ³•çš„æ€§èƒ½æ€¥å‰§ä¸‹é™ã€‚</li>
<li>æå‡ºäº†ä¸€ç§åŸºäºé¢„è®­ç»ƒLMMçš„è§†è§‰æ¥åœ°æ–¹æ³•â€”â€”F-LMMã€‚</li>
<li>F-LMMåˆ©ç”¨æ³¨æ„åŠ›æœºåˆ¶å®ç°è¯åƒç´ å¯¹åº”å…³ç³»å¯è§†åŒ–æ¥åœ°ï¼Œæ— éœ€å­¦ä¹ ç‰¹æ®Šåˆ†å‰²ä»¤ç‰Œã€‚</li>
<li>F-LMMåœ¨ä¸æŸå®³åŸæœ‰å¯¹è¯èƒ½åŠ›çš„æƒ…å†µä¸‹ï¼Œå®ç°äº†è‰¯å¥½çš„åˆ†å‰²å’Œæ¥åœ°æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2406.05821">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-d0422c362b470a7c5a84300ea461b71a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ce286065d678995919602d2935bdc9e3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-eb32e1ecd8ab3f2db67ed5b8e9b5b422.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-cd901d9e6f45c99a8f6414e115b94ecc.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-80c68afc596a746dfe973d23e2e5f6ca.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-fc8c5133b7af4d73f673a10658ff4ef1.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Towards-Translating-Real-World-Code-with-LLMs-A-Study-of-Translating-to-Rust"><a href="#Towards-Translating-Real-World-Code-with-LLMs-A-Study-of-Translating-to-Rust" class="headerlink" title="Towards Translating Real-World Code with LLMs: A Study of Translating to   Rust"></a>Towards Translating Real-World Code with LLMs: A Study of Translating to   Rust</h2><p><strong>Authors:Hasan Ferit Eniser, Hanliang Zhang, Cristina David, Meng Wang, Maria Christakis, Brandon Paulsen, Joey Dodds, Daniel Kroening</strong></p>
<p>Large language models (LLMs) show promise in code translation - the task of translating code written in one programming language to another language - due to their ability to write code in most programming languages. However, LLMâ€™s effectiveness on translating real-world code remains largely unstudied. In this work, we perform the first substantial study on LLM-based translation to Rust by assessing the ability of five state-of-the-art LLMs, GPT4, Claude 3, Claude 2.1, Gemini Pro, and Mixtral. We conduct our study on code extracted from real-world open source projects. To enable our study, we develop FLOURINE, an end-to-end code translation tool that uses differential fuzzing to check if a Rust translation is I&#x2F;O equivalent to the original source program, eliminating the need for pre-existing test cases. As part of our investigation, we assess both the LLMâ€™s ability to produce an initially successful translation, as well as their capacity to fix a previously generated buggy one. If the original and the translated programs are not I&#x2F;O equivalent, we apply a set of automated feedback strategies, including feedback to the LLM with counterexamples. Our results show that the most successful LLM can translate 47% of our benchmarks, and also provides insights into next steps for improvements. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨ä»£ç ç¿»è¯‘ä»»åŠ¡ä¸­æ˜¾ç¤ºå‡ºå·¨å¤§çš„æ½œåŠ›ï¼Œå³å°†ä¸€ç§ç¼–ç¨‹è¯­è¨€ç¼–å†™çš„ä»£ç ç¿»è¯‘æˆå¦ä¸€ç§è¯­è¨€çš„ä»£ç ã€‚è¿™å¾—ç›Šäºå®ƒä»¬èƒ½å¤Ÿä½¿ç”¨å¤§å¤šæ•°ç¼–ç¨‹è¯­è¨€ç¼–å†™ä»£ç çš„èƒ½åŠ›ã€‚ç„¶è€Œï¼ŒLLMåœ¨ç°å®ä¸–ç•Œçš„ä»£ç ç¿»è¯‘æ–¹é¢çš„æœ‰æ•ˆæ€§åœ¨å¾ˆå¤§ç¨‹åº¦ä¸Šå°šæœªè¢«ç ”ç©¶ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬å¯¹åŸºäºLLMçš„Rustç¿»è¯‘è¿›è¡Œäº†é¦–æ¬¡å®è´¨æ€§çš„ç ”ç©¶ï¼Œè¯„ä¼°äº†äº”ç§æœ€æ–°LLMçš„èƒ½åŠ›ï¼ŒåŒ…æ‹¬GPT4ã€Claude 3ã€Claude 2.1ã€Gemini Proå’ŒMixtralã€‚æˆ‘ä»¬çš„ç ”ç©¶æ˜¯åœ¨ä»ç°å®ä¸–ç•Œä¸­çš„å¼€æºé¡¹ç›®ä¸­æå–çš„ä»£ç ä¸Šè¿›è¡Œçš„ã€‚ä¸ºäº†æ”¯æŒæˆ‘ä»¬çš„ç ”ç©¶ï¼Œæˆ‘ä»¬å¼€å‘äº†FLOURINEï¼Œè¿™æ˜¯ä¸€æ¬¾ç«¯åˆ°ç«¯çš„ä»£ç ç¿»è¯‘å·¥å…·ï¼Œå®ƒä½¿ç”¨å·®åˆ†æ¨¡ç³Šæµ‹è¯•æ¥æ£€æŸ¥Rustç¿»è¯‘æ˜¯å¦ä¸åŸå§‹æºç¨‹åºåœ¨I&#x2F;Oä¸Šç­‰æ•ˆï¼Œä»è€Œä¸å†éœ€è¦é¢„å…ˆå­˜åœ¨çš„æµ‹è¯•ç”¨ä¾‹ã€‚ä½œä¸ºè°ƒæŸ¥çš„ä¸€éƒ¨åˆ†ï¼Œæˆ‘ä»¬è¯„ä¼°äº†LLMèƒ½å¤Ÿäº§ç”Ÿåˆæ­¥æˆåŠŸçš„ç¿»è¯‘çš„èƒ½åŠ›ï¼Œä»¥åŠå®ƒä»¬ä¿®å¤å…ˆå‰ç”Ÿæˆçš„é”™è¯¯ç¿»è¯‘çš„èƒ½åŠ›ã€‚å¦‚æœåŸå§‹ç¨‹åºå’Œç¿»è¯‘åçš„ç¨‹åºåœ¨I&#x2F;Oä¸Šä¸ç­‰æ•ˆï¼Œæˆ‘ä»¬ä¼šé‡‡ç”¨ä¸€ç³»åˆ—çš„è‡ªåŠ¨åŒ–åé¦ˆç­–ç•¥ï¼ŒåŒ…æ‹¬å¯¹LLMè¿›è¡Œåä¾‹åé¦ˆã€‚æˆ‘ä»¬çš„ç»“æœè¡¨æ˜ï¼Œæœ€æˆåŠŸçš„LLMèƒ½å¤Ÿç¿»è¯‘æˆ‘ä»¬åŸºå‡†æµ‹è¯•çš„47%ï¼Œå¹¶æä¾›äº†å…³äºä¸‹ä¸€æ­¥æ”¹è¿›æ–¹å‘çš„è§è§£ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2405.11514v3">PDF</a> 12 pages, 12 figures</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ç ”ç©¶äº†åŸºäºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„ä»£ç ç¿»è¯‘èƒ½åŠ›ï¼Œç‰¹åˆ«æ˜¯å¯¹Rustè¯­è¨€çš„ç¿»è¯‘ã€‚ç ”ç©¶å¯¹äº”æ¬¾å…ˆè¿›çš„LLMè¿›è¡Œäº†è¯„ä¼°ï¼ŒåŒ…æ‹¬GPT4ã€Claude 3ã€Claude 2.1ã€Gemini Proå’ŒMixtralã€‚ç ”ç©¶ä¸­å¼€å‘äº†ä¸€ä¸ªåä¸ºFLOURINEçš„ä»£ç ç¿»è¯‘å·¥å…·ï¼Œé‡‡ç”¨å·®åˆ†æ¨¡ç³Šæµ‹è¯•æ¥æ£€æŸ¥ç¿»è¯‘ç»“æœçš„è¾“å…¥è¾“å‡ºç­‰æ•ˆæ€§ã€‚ç ”ç©¶å‘ç°ï¼Œæœ€æˆåŠŸçš„LLMèƒ½å¤Ÿç¿»è¯‘47%çš„åŸºå‡†æµ‹è¯•ä»£ç ï¼ŒåŒæ—¶ä¹Ÿæä¾›äº†æ”¹è¿›çš„æ–¹å‘ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨ä»£ç ç¿»è¯‘é¢†åŸŸå…·æœ‰æ½œåŠ›ï¼Œç‰¹åˆ«æ˜¯åœ¨å°†ä»£ç ä»ä¸€ç§ç¼–ç¨‹è¯­è¨€ç¿»è¯‘åˆ°å¦ä¸€ç§è¯­è¨€æ–¹é¢ã€‚</li>
<li>å¯¹äº”æ¬¾å…ˆè¿›çš„LLMåœ¨Rustç¿»è¯‘æ–¹é¢çš„èƒ½åŠ›è¿›è¡Œäº†é¦–æ¬¡å®è´¨æ€§ç ”ç©¶ã€‚</li>
<li>å¼€å‘äº†ä¸€ä¸ªåä¸ºFLOURINEçš„ä»£ç ç¿»è¯‘å·¥å…·ï¼Œé‡‡ç”¨å·®åˆ†æ¨¡ç³Šæµ‹è¯•æ¥éªŒè¯ç¿»è¯‘ç»“æœçš„å‡†ç¡®æ€§ã€‚</li>
<li>LLMèƒ½å¤Ÿåˆæ­¥æˆåŠŸç¿»è¯‘çš„ä»£ç æ¯”ä¾‹æœ€é«˜è¾¾åˆ°47%ã€‚</li>
<li>LLMåœ¨ä¿®å¤å…ˆå‰ç”Ÿæˆçš„é”™è¯¯ä»£ç æ–¹é¢ä¹Ÿæœ‰ä¸€å®šèƒ½åŠ›ã€‚</li>
<li>å½“åŸå§‹ç¨‹åºå’Œç¿»è¯‘ç¨‹åºåœ¨è¾“å…¥è¾“å‡ºæ–¹é¢ä¸ç­‰æ•ˆæ—¶ï¼Œé‡‡ç”¨äº†ä¸€ç³»åˆ—è‡ªåŠ¨åŒ–åé¦ˆç­–ç•¥ï¼ŒåŒ…æ‹¬å‘LLMæä¾›åä¾‹ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2405.11514">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-fdf3b623d61f78e00a5af216a3a47199.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-83ca1a83862f31d118cc61e9ccdb1a86.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-92eb9eef867cfd1641027556cdee921b.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-e95623593c8cf5c48502e2ad394a138b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-eabfa5c6e5f9aa10f2b186b196f17e0a.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="MemLLM-Finetuning-LLMs-to-Use-An-Explicit-Read-Write-Memory"><a href="#MemLLM-Finetuning-LLMs-to-Use-An-Explicit-Read-Write-Memory" class="headerlink" title="MemLLM: Finetuning LLMs to Use An Explicit Read-Write Memory"></a>MemLLM: Finetuning LLMs to Use An Explicit Read-Write Memory</h2><p><strong>Authors:Ali Modarressi, Abdullatif KÃ¶ksal, Ayyoob Imani, Mohsen Fayyaz, Hinrich SchÃ¼tze</strong></p>
<p>While current large language models (LLMs) perform well on many knowledge-related tasks, they are limited by relying on their parameters as an implicit storage mechanism. As a result, they struggle with memorizing rare events and with updating their memory as facts change over time. In addition, the uninterpretable nature of parametric memory makes it challenging to prevent hallucination. Model editing and augmenting LLMs with parameters specialized for memory are only partial solutions. In this paper, we introduce MemLLM, a novel method of enhancing LLMs by integrating a structured and explicit read-and-write memory module. MemLLM tackles the aforementioned challenges by enabling dynamic interaction with the memory and improving the LLMâ€™s capabilities in using stored knowledge. Our experiments indicate that MemLLM enhances the LLMâ€™s performance and interpretability, in language modeling in general and knowledge-intensive tasks in particular. We see MemLLM as an important step towards making LLMs more grounded and factual through memory augmentation. The project repository is publicly available at <a target="_blank" rel="noopener" href="https://github.com/amodaresi/MemLLM">https://github.com/amodaresi/MemLLM</a> </p>
<blockquote>
<p>å½“å‰çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨è®¸å¤šä¸çŸ¥è¯†ç›¸å…³çš„ä»»åŠ¡ä¸Šè¡¨ç°è‰¯å¥½ï¼Œä½†å®ƒä»¬ä¾èµ–äºå‚æ•°ä½œä¸ºéšå¼å­˜å‚¨æœºåˆ¶ï¼Œä»è€Œå­˜åœ¨å±€é™æ€§ã€‚å› æ­¤ï¼Œå®ƒä»¬åœ¨è®°å¿†ç½•è§äº‹ä»¶å’Œéšæ—¶é—´æ›´æ–°äº‹å®æ–¹é¢é‡åˆ°å›°éš¾ã€‚æ­¤å¤–ï¼Œå‚æ•°è®°å¿†çš„ä¸å¯è§£é‡Šæ€§ä½¿å¾—é˜²æ­¢å¹»è§‰å…·æœ‰æŒ‘æˆ˜æ€§ã€‚é€šè¿‡å‚æ•°å¯¹LLMè¿›è¡Œæ¨¡å‹ç¼–è¾‘å’Œæ‰©å……ï¼Œä¸“é—¨ç”¨äºè®°å¿†åªæ˜¯éƒ¨åˆ†è§£å†³æ–¹æ¡ˆã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†MemLLMï¼Œè¿™æ˜¯ä¸€ç§é€šè¿‡é›†æˆç»“æ„åŒ–å’Œæ˜¾å¼è¯»å†™å†…å­˜æ¨¡å—å¢å¼ºLLMçš„æ–°å‹æ–¹æ³•ã€‚MemLLMé€šè¿‡å®ç°ä¸å†…å­˜çš„åŠ¨æ€äº¤äº’ï¼Œæé«˜LLMåˆ©ç”¨å­˜å‚¨çŸ¥è¯†çš„èƒ½åŠ›ï¼Œè§£å†³äº†ä¸Šè¿°æŒ‘æˆ˜ã€‚æˆ‘ä»¬çš„å®éªŒè¡¨æ˜ï¼ŒMemLLMæé«˜äº†LLMçš„æ€§èƒ½å’Œå¯è§£é‡Šæ€§ï¼Œå°¤å…¶æ˜¯åœ¨ä¸€èˆ¬è¯­è¨€å»ºæ¨¡å’ŒçŸ¥è¯†å¯†é›†å‹ä»»åŠ¡ä¸­ã€‚æˆ‘ä»¬è®¤ä¸ºMemLLMæ˜¯æœç€é€šè¿‡è®°å¿†å¢å¼ºä½¿LLMæ›´åŠ æ‰å®å’ŒåŸºäºäº‹å®çš„é‡è¦ä¸€æ­¥ã€‚é¡¹ç›®ä»“åº“å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/amodaresi/MemLLM%E5%85%AC%E5%BC%80%E8%AE%BF%E9%97%AE%E3%80%82">https://github.com/amodaresi/MemLLMå…¬å¼€è®¿é—®ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2404.11672v3">PDF</a> Published in Transactions on Machine Learning Research (TMLR)</p>
<p><strong>Summary</strong><br>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨çŸ¥è¯†ç›¸å…³ä»»åŠ¡ä¸Šè¡¨ç°è‰¯å¥½ï¼Œä½†å—é™äºéšå¼å­˜å‚¨æœºåˆ¶ï¼Œéš¾ä»¥è®°å¿†ç½•è§äº‹ä»¶å’Œéšæ—¶é—´å˜åŒ–çš„äº‹å®ã€‚æœ¬æ–‡æå‡ºMemLLMæ–¹æ³•ï¼Œé€šè¿‡é›†æˆç»“æ„åŒ–ã€æ˜¾å¼è¯»å†™å†…å­˜æ¨¡å—å¢å¼ºLLMã€‚MemLLMè§£å†³äº†ä¸Šè¿°é—®é¢˜ï¼Œé€šè¿‡åŠ¨æ€äº¤äº’æé«˜LLMä½¿ç”¨å­˜å‚¨çŸ¥è¯†çš„èƒ½åŠ›ã€‚å®éªŒè¡¨æ˜ï¼ŒMemLLMå¢å¼ºäº†LLMçš„æ€§èƒ½å’Œå¯è§£é‡Šæ€§ï¼Œå°¤å…¶æ˜¯åœ¨è¯­è¨€å»ºæ¨¡å’ŒçŸ¥è¯†å¯†é›†å‹ä»»åŠ¡ä¸­ã€‚MemLLMæ˜¯æœç€é€šè¿‡å†…å­˜å¢å¼ºä½¿LLMæ›´çœŸå®å’ŒåŸºäºäº‹å®çš„é‡è¦ä¸€æ­¥ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLMså—é™äºéšå¼å­˜å‚¨æœºåˆ¶ï¼Œéš¾ä»¥å¤„ç†ç½•è§äº‹ä»¶å’Œæ›´æ–°éšæ—¶é—´å˜åŒ–çš„äº‹å®ã€‚</li>
<li>MemLLMæ˜¯ä¸€ç§é€šè¿‡é›†æˆç»“æ„åŒ–ã€æ˜¾å¼çš„è¯»å†™å†…å­˜æ¨¡å—æ¥å¢å¼ºLLMsçš„æ–¹æ³•ã€‚</li>
<li>MemLLMè§£å†³äº†LLMsåœ¨åŠ¨æ€äº¤äº’å’Œä½¿ç”¨å­˜å‚¨çŸ¥è¯†æ–¹é¢çš„æŒ‘æˆ˜ã€‚</li>
<li>MemLLMæé«˜äº†LLMçš„æ€§èƒ½å’Œå¯è§£é‡Šæ€§ï¼Œç‰¹åˆ«æ˜¯åœ¨è¯­è¨€å»ºæ¨¡å’ŒçŸ¥è¯†å¯†é›†å‹ä»»åŠ¡ä¸­ã€‚</li>
<li>MemLLMå¯¹äºé˜²æ­¢å¹»è§‰å…·æœ‰æŒ‘æˆ˜æ€§ï¼Œå› ä¸ºå®ƒå…·æœ‰å¯è§£é‡Šçš„å†…å­˜æœºåˆ¶ã€‚</li>
<li>MemLLMæ˜¯æœç€ä½¿LLMæ›´çœŸå®å’ŒåŸºäºäº‹å®çš„æ–¹å‘è¿ˆå‡ºçš„é‡è¦ä¸€æ­¥ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2404.11672">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-263ba0d433221960937477d084b9ea40.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-7e8002bb699281c79ddbf75ed7124f80.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-428d3b8bbce14d78e2b2b6bf5cbbf43b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-4411c332258b38302a8a083488a7be32.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="PhD-A-ChatGPT-Prompted-Visual-hallucination-Evaluation-Dataset"><a href="#PhD-A-ChatGPT-Prompted-Visual-hallucination-Evaluation-Dataset" class="headerlink" title="PhD: A ChatGPT-Prompted Visual hallucination Evaluation Dataset"></a>PhD: A ChatGPT-Prompted Visual hallucination Evaluation Dataset</h2><p><strong>Authors:Jiazhen Liu, Yuhan Fu, Ruobing Xie, Runquan Xie, Xingwu Sun, Fengzong Lian, Zhanhui Kang, Xirong Li</strong></p>
<p>Multimodal Large Language Models (MLLMs) hallucinate, resulting in an emerging topic of visual hallucination evaluation (VHE). This paper contributes a ChatGPT-Prompted visual hallucination evaluation Dataset (PhD) for objective VHE at a large scale. The essence of VHE is to ask an MLLM questions about specific images to assess its susceptibility to hallucination. Depending on what to ask (objects, attributes, sentiment, etc.) and how the questions are asked, we structure PhD along two dimensions, i.e. task and mode. Five visual recognition tasks, ranging from low-level (object &#x2F; attribute recognition) to middle-level (sentiment &#x2F; position recognition and counting), are considered. Besides a normal visual QA mode, which we term PhD-base, PhD also asks questions with specious context (PhD-sec) or with incorrect context ({PhD-icc), or with AI-generated counter common sense images (PhD-ccs). We construct PhD by a ChatGPT-assisted semi-automated pipeline, encompassing four pivotal modules: task-specific hallucinatory item (hitem) selection, hitem-embedded question generation, specious &#x2F; incorrect context generation, and counter-common-sense (CCS) image generation. With over 14k daily images, 750 CCS images and 102k VQA triplets in total, PhD reveals considerable variability in MLLMsâ€™ performance across various modes and tasks, offering valuable insights into the nature of hallucination. As such, PhD stands as a potent tool not only for VHE but may also play a significant role in the refinement of MLLMs. </p>
<blockquote>
<p>å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰ä¼šäº§ç”Ÿå¹»è§‰ï¼Œä»è€Œå¼•å‘è§†è§‰å¹»è§‰è¯„ä¼°ï¼ˆVHEï¼‰è¿™ä¸€æ–°å…´è¯é¢˜ã€‚æœ¬æ–‡è´¡çŒ®äº†ä¸€ä¸ªåŸºäºChatGPTçš„å¹»è§‰è¯„ä¼°æ•°æ®é›†ï¼ˆPhDï¼‰ï¼Œç”¨äºå¤§è§„æ¨¡å®¢è§‚VHEã€‚VHEçš„æœ¬è´¨æ˜¯å‘MLLMæå‡ºå…³äºç‰¹å®šå›¾åƒçš„é—®é¢˜ï¼Œä»¥è¯„ä¼°å…¶æ˜¯å¦å®¹æ˜“å—åˆ°å¹»è§‰çš„å½±å“ã€‚æ ¹æ®è¦é—®çš„é—®é¢˜ï¼ˆå¯¹è±¡ã€å±æ€§ã€æƒ…æ„Ÿç­‰ï¼‰å’Œæé—®çš„æ–¹å¼ï¼Œæˆ‘ä»¬æŒ‰ä»»åŠ¡å’Œæ¨¡å¼ä¸¤ä¸ªç»´åº¦æ„å»ºPhDã€‚æˆ‘ä»¬è€ƒè™‘äº†äº”ä¸ªè§†è§‰è¯†åˆ«ä»»åŠ¡ï¼Œä»ä½çº§çš„ï¼ˆå¯¹è±¡&#x2F;å±æ€§è¯†åˆ«ï¼‰åˆ°ä¸­çº§çš„ï¼ˆæƒ…æ„Ÿ&#x2F;ä½ç½®è¯†åˆ«å’Œè®¡æ•°ï¼‰ã€‚é™¤äº†æ­£å¸¸çš„è§†è§‰é—®ç­”æ¨¡å¼ï¼ˆæˆ‘ä»¬ç§°ä¹‹ä¸ºPhD-baseï¼‰ï¼ŒPhDè¿˜ä¼šåœ¨å…·æœ‰ç‰¹æ®Šä¸Šä¸‹æ–‡ï¼ˆPhD-secï¼‰æˆ–é”™è¯¯ä¸Šä¸‹æ–‡ï¼ˆPhD-iccï¼‰çš„æƒ…å†µä¸‹æé—®ï¼Œæˆ–ä½¿ç”¨AIç”Ÿæˆçš„åå¸¸è¯†å›¾åƒï¼ˆPhD-ccsï¼‰ã€‚æˆ‘ä»¬é€šè¿‡ChatGPTè¾…åŠ©çš„åŠè‡ªåŠ¨åŒ–ç®¡é“æ„å»ºPhDï¼ŒåŒ…æ‹¬å››ä¸ªå…³é”®æ¨¡å—ï¼šä»»åŠ¡ç‰¹å®šå¹»è§‰é¡¹ç›®ï¼ˆhitemï¼‰é€‰æ‹©ã€hitemåµŒå…¥é—®é¢˜ç”Ÿæˆã€ç‰¹æ®Š&#x2F;é”™è¯¯ä¸Šä¸‹æ–‡ç”Ÿæˆå’Œåå¸¸è¯†ï¼ˆCCSï¼‰å›¾åƒç”Ÿæˆã€‚PhDæ¯å¤©æ¶µç›–è¶…è¿‡14kå¼ å›¾åƒã€750å¼ å¸¸è¯†å›¾åƒå’Œæ€»å…±102kä¸ªVQAä¸‰å…ƒç»„ï¼Œæ­ç¤ºäº†MLLMsåœ¨ä¸åŒæ¨¡å¼å’Œä»»åŠ¡ä¹‹é—´çš„æ€§èƒ½å­˜åœ¨ç›¸å½“å¤§çš„å·®å¼‚ï¼Œä¸ºå¹»è§‰çš„æ€§è´¨æä¾›äº†å®è´µçš„è§è§£ã€‚å› æ­¤ï¼ŒPhDä¸ä»…æ˜¯ä¸€ä¸ªå¼ºå¤§çš„VHEå·¥å…·ï¼Œä¹Ÿå¯èƒ½åœ¨MLLMçš„æ”¹è¿›ä¸­å‘æŒ¥é‡è¦ä½œç”¨ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2403.11116v4">PDF</a> Accepted by CVPR 2025, Highlight</p>
<p><strong>æ‘˜è¦</strong></p>
<p>å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰ä¼šäº§ç”Ÿå¹»è§‰ï¼Œä»è€Œäº§ç”Ÿäº†è§†è§‰å¹»è§‰è¯„ä¼°ï¼ˆVHEï¼‰è¿™ä¸€æ–°å…´ä¸»é¢˜ã€‚æœ¬æ–‡è´¡çŒ®äº†ä¸€ä¸ªåŸºäºChatGPTçš„è§†è§‰å¹»è§‰è¯„ä¼°æ•°æ®é›†ï¼ˆPhDï¼‰ï¼Œç”¨äºå¤§è§„æ¨¡å®¢è§‚VHEã€‚VHEçš„æœ¬è´¨æ˜¯å‘MLLMè¯¢é—®ç‰¹å®šå›¾åƒçš„é—®é¢˜ï¼Œä»¥è¯„ä¼°å…¶æ˜¯å¦å®¹æ˜“äº§ç”Ÿå¹»è§‰ã€‚æ ¹æ®è¯¢é—®çš„å†…å®¹å’Œæ–¹å¼ï¼ˆä¾‹å¦‚å¯¹è±¡ã€å±æ€§ã€æƒ…æ„Ÿç­‰ï¼‰ä»¥åŠé—®é¢˜çš„ç»“æ„ï¼Œæˆ‘ä»¬å°†æ•°æ®é›†æ²¿ç€ä»»åŠ¡å’Œæ¨¡å¼ä¸¤ä¸ªç»´åº¦æ„å»ºã€‚è€ƒè™‘äº”ç§è§†è§‰è¯†åˆ«ä»»åŠ¡ï¼Œä»ä½çº§çš„å¯¹è±¡&#x2F;å±æ€§è¯†åˆ«åˆ°ä¸­çº§çš„æƒ…æ„Ÿ&#x2F;ä½ç½®è¯†åˆ«ä»¥åŠè®¡æ•°ã€‚é™¤äº†æ­£å¸¸çš„è§†è§‰é—®ç­”æ¨¡å¼ï¼ˆæˆ‘ä»¬ç§°ä¹‹ä¸ºPhD-baseï¼‰å¤–ï¼ŒPhDè¿˜é’ˆå¯¹å…·æœ‰å¥‡å¼‚ä¸Šä¸‹æ–‡ï¼ˆPhD-secï¼‰ã€é”™è¯¯ä¸Šä¸‹æ–‡ï¼ˆPhD-iccï¼‰æˆ–AIç”Ÿæˆçš„åå¸¸è¯†å›¾åƒï¼ˆPhD-ccsï¼‰çš„é—®é¢˜è¿›è¡Œæé—®ã€‚æˆ‘ä»¬é€šè¿‡ChatGPTè¾…åŠ©çš„åŠè‡ªåŠ¨åŒ–ç®¡é“æ„å»ºPhDï¼ŒåŒ…æ‹¬å››ä¸ªå…³é”®æ¨¡å—ï¼šç‰¹å®šä»»åŠ¡çš„å¹»è§‰é¡¹ç›®é€‰æ‹©ã€åµŒå…¥é—®é¢˜çš„ç”Ÿæˆã€å¥‡å¼‚&#x2F;é”™è¯¯ä¸Šä¸‹æ–‡çš„ç”Ÿæˆå’Œåå¸¸è¯†ï¼ˆCCSï¼‰å›¾åƒçš„ç”Ÿæˆã€‚PhDæ¯å¤©åŒ…å«è¶…è¿‡14kå¼ å›¾åƒã€750å¼ åå¸¸è¯†å›¾åƒå’Œæ€»å…±çš„10.2ä¸‡å¼ è§†è§‰é—®ç­”ä¸‰å…ƒç»„ï¼Œæ­ç¤ºäº†MLLMåœ¨ä¸åŒæ¨¡å¼å’Œä»»åŠ¡ä¸­çš„æ€§èƒ½å­˜åœ¨æ˜¾è‘—çš„å¯å˜æ€§ï¼Œä¸ºå¹»è§‰çš„æ€§è´¨æä¾›äº†å®è´µçš„è§è§£ã€‚å› æ­¤ï¼ŒPhDä¸ä»…æ˜¯VHEçš„å¼ºå¤§å·¥å…·ï¼Œè¿˜å¯èƒ½å¯¹MLLMçš„æ”¹è¿›èµ·åˆ°é‡è¦ä½œç”¨ã€‚</p>
<p><strong>è¦ç‚¹æ‘˜è¦</strong></p>
<ol>
<li>å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰å­˜åœ¨è§†è§‰å¹»è§‰è¯„ä¼°ï¼ˆVHEï¼‰é—®é¢˜ã€‚</li>
<li>æœ¬æ–‡ä»‹ç»äº†ä¸€ä¸ªåŸºäºChatGPTçš„è§†è§‰å¹»è§‰è¯„ä¼°æ•°æ®é›†ï¼ˆPhDï¼‰ã€‚</li>
<li>PhDæ•°æ®é›†åŒ…å«å¤šç§è§†è§‰è¯†åˆ«ä»»åŠ¡ï¼Œä»å¯¹è±¡&#x2F;å±æ€§è¯†åˆ«åˆ°æƒ…æ„Ÿ&#x2F;ä½ç½®è¯†åˆ«å’Œè®¡æ•°ã€‚</li>
<li>é™¤äº†æ­£å¸¸è§†è§‰é—®ç­”æ¨¡å¼å¤–ï¼ŒPhDè¿˜åŒ…æ‹¬å…·æœ‰å¥‡å¼‚ã€é”™è¯¯ä¸Šä¸‹æ–‡æˆ–AIç”Ÿæˆçš„åå¸¸è¯†å›¾åƒçš„é—®é¢˜ã€‚</li>
<li>é€šè¿‡åŠè‡ªåŠ¨åŒ–ç®¡é“æ„å»ºPhDï¼ŒåŒ…æ‹¬ä»»åŠ¡ç‰¹å®šçš„å¹»è§‰é¡¹ç›®é€‰æ‹©ã€é—®é¢˜ç”Ÿæˆã€ä¸Šä¸‹æ–‡ç”Ÿæˆå’Œå›¾åƒç”Ÿæˆç­‰æ¨¡å—ã€‚</li>
<li>PhDæ•°æ®é›†æ­ç¤ºäº†MLLMåœ¨ä¸åŒæ¨¡å¼å’Œä»»åŠ¡ä¸­çš„æ€§èƒ½å·®å¼‚ï¼Œä¸ºè¯„ä¼°å’Œæ”¹è¿›MLLMçš„å¹»è§‰é—®é¢˜æä¾›äº†å®è´µæ•°æ®ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2403.11116">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-8b5d922959896d026734d86087677548.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c73ea8ce2d172b06a01a1ca0c0322f44.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-da75e4a48e84ea3dcb95ec57081904a7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-42dc7b09b9f78e790cfcc1d9ac5e2ebc.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-b551ec68ebc031d4e1986b58491a7d7f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-603a715be06a18e66d506c6aff79b69d.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Citation-Enhanced-Generation-for-LLM-based-Chatbots"><a href="#Citation-Enhanced-Generation-for-LLM-based-Chatbots" class="headerlink" title="Citation-Enhanced Generation for LLM-based Chatbots"></a>Citation-Enhanced Generation for LLM-based Chatbots</h2><p><strong>Authors:Weitao Li, Junkai Li, Weizhi Ma, Yang Liu</strong></p>
<p>Large language models (LLMs) exhibit powerful general intelligence across diverse scenarios, including their integration into chatbots. However, a vital challenge of LLM-based chatbots is that they may produce hallucinated content in responses, which significantly limits their applicability. Various efforts have been made to alleviate hallucination, such as retrieval augmented generation and reinforcement learning with human feedback, but most of them require additional training and data annotation. In this paper, we propose a novel post-hoc Citation-Enhanced Generation (CEG) approach combined with retrieval argumentation. Unlike previous studies that focus on preventing hallucinations during generation, our method addresses this issue in a post-hoc way. It incorporates a retrieval module to search for supporting documents relevant to the generated content, and employs a natural language inference-based citation generation module. Once the statements in the generated content lack of reference, our model can regenerate responses until all statements are supported by citations. Note that our method is a training-free plug-and-play plugin that is capable of various LLMs. Experiments on various hallucination-related datasets show our framework outperforms state-of-the-art methods in both hallucination detection and response regeneration on three benchmarks. Our codes and dataset will be publicly available. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨å¤šç§åœºæ™¯ä¸­å±•ç°å‡ºå¼ºå¤§çš„é€šç”¨æ™ºèƒ½ï¼ŒåŒ…æ‹¬åœ¨èŠå¤©æœºå™¨äººä¸­çš„é›†æˆã€‚ç„¶è€Œï¼ŒLLMé©±åŠ¨çš„èŠå¤©æœºå™¨äººé¢ä¸´çš„ä¸€ä¸ªå…³é”®æŒ‘æˆ˜æ˜¯å®ƒä»¬å¯èƒ½ä¼šäº§ç”Ÿå¹»è§‰å†…å®¹ä½œä¸ºå›åº”ï¼Œè¿™æå¤§åœ°é™åˆ¶äº†å…¶é€‚ç”¨æ€§ã€‚å°½ç®¡å·²ç»åšå‡ºäº†å„ç§åŠªåŠ›æ¥ç¼“è§£å¹»è§‰é—®é¢˜ï¼Œå¦‚å¢å¼ºæ£€ç´¢ç”Ÿæˆå’Œé€šè¿‡äººç±»åé¦ˆè¿›è¡Œå¼ºåŒ–å­¦ä¹ ï¼Œä½†å¤§å¤šæ•°æ–¹æ³•éƒ½éœ€è¦é¢å¤–çš„è®­ç»ƒå’Œæ•°æ®æ ‡æ³¨ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°å‹çš„åéªŒå¼•ç”¨å¢å¼ºç”Ÿæˆï¼ˆCEGï¼‰æ–¹æ³•ï¼Œè¯¥æ–¹æ³•ç»“åˆäº†æ£€ç´¢è®ºè¯ã€‚ä¸åŒäºä¹‹å‰ä¾§é‡äºåœ¨ç”Ÿæˆè¿‡ç¨‹ä¸­é˜²æ­¢å¹»è§‰çš„ç ”ç©¶ï¼Œæˆ‘ä»¬çš„æ–¹æ³•ä»¥ä¸€ç§åéªŒæ–¹å¼è§£å†³è¿™ä¸€é—®é¢˜ã€‚å®ƒç»“åˆäº†ä¸€ä¸ªæ£€ç´¢æ¨¡å—æ¥æœç´¢ä¸ç”Ÿæˆå†…å®¹ç›¸å…³çš„æ”¯æŒæ–‡æ¡£ï¼Œå¹¶é‡‡ç”¨åŸºäºè‡ªç„¶è¯­è¨€æ¨ç†çš„å¼•ç”¨ç”Ÿæˆæ¨¡å—ã€‚ä¸€æ—¦ç”Ÿæˆçš„å†…å®¹ä¸­çš„é™ˆè¿°ç¼ºä¹å‚è€ƒï¼Œæˆ‘ä»¬çš„æ¨¡å‹å¯ä»¥é‡æ–°ç”Ÿæˆå›åº”ï¼Œç›´åˆ°æ‰€æœ‰é™ˆè¿°éƒ½å¾—åˆ°å¼•ç”¨çš„æ”¯æŒã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œæˆ‘ä»¬çš„æ–¹æ³•æ˜¯ä¸€ç§æ— éœ€è®­ç»ƒçš„å³æ’å³ç”¨æ’ä»¶ï¼Œå¯ä»¥ä¸å„ç§LLMé…åˆä½¿ç”¨ã€‚åœ¨å„ç§å¹»è§‰ç›¸å…³çš„æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ¡†æ¶åœ¨ä¸‰ä¸ªåŸºå‡†æµ‹è¯•ä¸­ä¼˜äºæœ€å…ˆè¿›çš„æ–¹æ³•ï¼Œåœ¨å¹»è§‰æ£€æµ‹å’Œå“åº”å†ç”Ÿæ–¹é¢å‡è¡¨ç°å‡ºè‰²ã€‚æˆ‘ä»¬çš„ä»£ç å’Œæ•°æ®é›†å°†å…¬å¼€å¯ç”¨ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2402.16063v4">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨å¤šç§åœºæ™¯ä¸­å±•ç°å‡ºå¼ºå¤§çš„é€šç”¨æ™ºèƒ½ï¼Œå°¤å…¶æ˜¯åœ¨é›†æˆåˆ°èŠå¤©æœºå™¨äººæ–¹é¢ã€‚ç„¶è€Œï¼ŒLLM-basedèŠå¤©æœºå™¨äººçš„ä¸€ä¸ªé‡å¤§æŒ‘æˆ˜æ˜¯å¯èƒ½ä¼šäº§ç”Ÿè™šæ„çš„å›åº”å†…å®¹ï¼Œè¿™æå¤§åœ°é™åˆ¶äº†å…¶é€‚ç”¨æ€§ã€‚å°½ç®¡å·²æœ‰è®¸å¤šç¼“è§£è™šæ„æ€§çš„å°è¯•ï¼Œå¦‚å¢å¼ºæ£€ç´¢ç”Ÿæˆå’Œå¼ºåŒ–å­¦ä¹ äººç±»åé¦ˆï¼Œä½†å¤§å¤šæ•°æ–¹æ³•éƒ½éœ€è¦é¢å¤–çš„è®­ç»ƒå’Œæ ‡æ³¨æ•°æ®ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°é¢–çš„åéªŒå¼Citation-Enhanced Generationï¼ˆCEGï¼‰æ–¹æ³•ï¼Œç»“åˆäº†æ£€ç´¢è®ºè¯ã€‚ä¸å…¶ä»–ç ”ç©¶ä¸åŒï¼Œæˆ‘ä»¬çš„æ–¹æ³•ä¸“æ³¨äºåœ¨ç”Ÿæˆåè§£å†³è™šæ„æ€§é—®é¢˜ã€‚å®ƒé‡‡ç”¨æ£€ç´¢æ¨¡å—æ¥æœç´¢ä¸ç”Ÿæˆå†…å®¹ç›¸å…³çš„æ”¯æŒæ–‡æ¡£ï¼Œå¹¶é‡‡ç”¨åŸºäºè‡ªç„¶è¯­è¨€æ¨ç†çš„å¼•ç”¨ç”Ÿæˆæ¨¡å—ã€‚å½“ç”Ÿæˆçš„å›åº”å†…å®¹ç¼ºä¹å¼•ç”¨æ—¶ï¼Œæˆ‘ä»¬çš„æ¨¡å‹ä¼šé‡æ–°ç”Ÿæˆå›åº”ï¼Œç›´è‡³æ‰€æœ‰é™ˆè¿°éƒ½å¾—åˆ°å¼•ç”¨çš„æ”¯æŒã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œæˆ‘ä»¬çš„æ–¹æ³•æ˜¯ä¸€ç§æ— éœ€è®­ç»ƒçš„å³æ’å³ç”¨æ’ä»¶ï¼Œé€‚ç”¨äºå„ç§LLMã€‚åœ¨å¤šä¸ªä¸è™šæ„æ€§ç›¸å…³çš„æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ¡†æ¶åœ¨ä¸‰ä¸ªåŸºå‡†æµ‹è¯•ä¸Šä¼˜äºæœ€æ–°æ–¹æ³•åœ¨è™šæ„æ€§æ£€æµ‹å’Œå›åº”å†ç”Ÿæ–¹é¢çš„è¡¨ç°ã€‚æˆ‘ä»¬çš„ä»£ç å’Œæ•°æ®é›†å°†å…¬å¼€æä¾›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLMså±•ç°å‡ºå¼ºå¤§çš„é€šç”¨æ™ºèƒ½ï¼Œé›†æˆåˆ°èŠå¤©æœºå™¨äººå…·æœ‰åº”ç”¨ä»·å€¼ã€‚</li>
<li>LLM-basedèŠå¤©æœºå™¨äººé¢ä¸´è™šæ„æ€§å›åº”çš„æŒ‘æˆ˜ï¼Œé™åˆ¶äº†å…¶é€‚ç”¨æ€§ã€‚</li>
<li>å½“å‰å‡è½»è™šæ„æ€§çš„æ–¹æ³•å¤šéœ€é¢å¤–çš„è®­ç»ƒå’Œæ ‡æ³¨æ•°æ®ã€‚</li>
<li>æœ¬æ–‡æå‡ºä¸€ç§æ–°é¢–çš„åéªŒå¼Citation-Enhanced Generationï¼ˆCEGï¼‰æ–¹æ³•æ¥è§£å†³è™šæ„æ€§é—®é¢˜ã€‚</li>
<li>CEGæ–¹æ³•ç»“åˆæ£€ç´¢è®ºè¯ï¼Œé€šè¿‡æœç´¢æ”¯æŒæ–‡æ¡£æ¥å¢å¼ºå›åº”çš„å¯é æ€§ã€‚</li>
<li>CEGæ¨¡å‹èƒ½é‡æ–°ç”Ÿæˆå›åº”ç›´è‡³æ‰€æœ‰é™ˆè¿°éƒ½å¾—åˆ°å¼•ç”¨çš„æ”¯æŒã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2402.16063">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-436d75fa27d3bb9480ba2ec248262c13.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-49ab13cb4418eea249d79f1573e0e923.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-83eb6dd04097ab69fdbef18edc243ec7.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="Rethinking-Channel-Dimensions-to-Isolate-Outliers-for-Low-bit-Weight-Quantization-of-Large-Language-Models"><a href="#Rethinking-Channel-Dimensions-to-Isolate-Outliers-for-Low-bit-Weight-Quantization-of-Large-Language-Models" class="headerlink" title="Rethinking Channel Dimensions to Isolate Outliers for Low-bit Weight   Quantization of Large Language Models"></a>Rethinking Channel Dimensions to Isolate Outliers for Low-bit Weight   Quantization of Large Language Models</h2><p><strong>Authors:Jung Hwan Heo, Jeonghoon Kim, Beomseok Kwon, Byeongwook Kim, Se Jung Kwon, Dongsoo Lee</strong></p>
<p>Large Language Models (LLMs) have recently demonstrated remarkable success across various tasks. However, efficiently serving LLMs has been a challenge due to the large memory bottleneck, specifically in small batch inference settings (e.g. mobile devices). Weight-only quantization can be a promising approach, but sub-4 bit quantization remains a challenge due to large-magnitude activation outliers. To mitigate the undesirable outlier effect, we first propose per-IC quantization, a simple yet effective method that creates quantization groups within each input channel (IC) rather than the conventional per-output-channel (per-OC). Our method is motivated by the observation that activation outliers affect the input dimension of the weight matrix, so similarly grouping the weights in the IC direction can isolate outliers within a group. We also find that activation outliers do not dictate quantization difficulty, and inherent weight sensitivities also exist. With per-IC quantization as a new outlier-friendly scheme, we propose Adaptive Dimensions (AdaDim), a versatile quantization framework that can adapt to various weight sensitivity patterns. We demonstrate the effectiveness of AdaDim by augmenting prior methods such as Round-To-Nearest and GPTQ, showing significant improvements across various language modeling benchmarks for both base (up to +4.7% on MMLU) and instruction-tuned (up to +10% on HumanEval) LLMs. Code is available at <a target="_blank" rel="noopener" href="https://github.com/johnheo/adadim-llm">https://github.com/johnheo/adadim-llm</a> </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨å„ç§ä»»åŠ¡ä¸­å–å¾—äº†æ˜¾è‘—çš„æˆåŠŸã€‚ç„¶è€Œï¼Œç”±äºå†…å­˜ç“¶é¢ˆé—®é¢˜ï¼Œç‰¹åˆ«æ˜¯åœ¨å°æ‰¹é‡æ¨ç†è®¾ç½®ï¼ˆå¦‚ç§»åŠ¨è®¾å¤‡ï¼‰ä¸­ï¼Œæœ‰æ•ˆåœ°æœåŠ¡LLMä¸€ç›´æ˜¯ä¸€ä¸ªæŒ‘æˆ˜ã€‚æƒé‡ä»…é‡åŒ–å¯èƒ½æ˜¯ä¸€ç§æœ‰å‰é€”çš„æ–¹æ³•ï¼Œä½†ä½äº4ä½çš„é‡åŒ–ä»ç„¶æ˜¯ä¸€ä¸ªæŒ‘æˆ˜ï¼Œå› ä¸ºå­˜åœ¨å¤§é‡å¹…åº¦æ¿€æ´»å¼‚å¸¸å€¼ã€‚ä¸ºäº†å‡è½»ä¸è‰¯å¼‚å¸¸å€¼çš„å½±å“ï¼Œæˆ‘ä»¬é¦–å…ˆæå‡ºäº†é’ˆå¯¹æ¯ä¸ªè¾“å…¥é€šé“ï¼ˆICï¼‰çš„é‡åŒ–æ–¹æ³•ï¼Œè¿™æ˜¯ä¸€ç§ç®€å•æœ‰æ•ˆçš„æ–¹æ³•ï¼Œåœ¨æ¯ä¸ªè¾“å…¥é€šé“å†…åˆ›å»ºé‡åŒ–ç»„ï¼Œè€Œä¸æ˜¯ä¼ ç»Ÿçš„æ¯ä¸ªè¾“å‡ºé€šé“ï¼ˆOCï¼‰ã€‚æˆ‘ä»¬çš„æ–¹æ³•æºäºè§‚å¯Ÿåˆ°æ¿€æ´»å¼‚å¸¸å€¼å½±å“æƒé‡çŸ©é˜µçš„è¾“å…¥ç»´åº¦ï¼Œå› æ­¤æŒ‰ICæ–¹å‘å¯¹æƒé‡è¿›è¡Œåˆ†ç»„å¯ä»¥éš”ç¦»å¼‚å¸¸å€¼åˆ°ä¸€ä¸ªç»„å†…ã€‚æˆ‘ä»¬è¿˜å‘ç°æ¿€æ´»å¼‚å¸¸å€¼å¹¶ä¸å†³å®šé‡åŒ–çš„éš¾åº¦ï¼Œæƒé‡æœ¬èº«ä¹Ÿå­˜åœ¨å›ºæœ‰çš„æ•æ„Ÿæ€§ã€‚ä½œä¸ºä¸€ç§æ–°çš„å¼‚å¸¸å€¼å‹å¥½çš„æ–¹æ¡ˆï¼Œæˆ‘ä»¬æå‡ºäº†è‡ªé€‚åº”ç»´åº¦ï¼ˆAdaDimï¼‰ï¼Œè¿™æ˜¯ä¸€ä¸ªçµæ´»çš„é‡åŒ–æ¡†æ¶ï¼Œèƒ½å¤Ÿé€‚åº”å„ç§æƒé‡æ•æ„Ÿæ€§æ¨¡å¼ã€‚æˆ‘ä»¬é€šè¿‡å¢å¼ºå…ˆå‰çš„Round-To-Nearestå’ŒGPTQç­‰æ–¹æ³•æ¥å±•ç¤ºAdaDimçš„æœ‰æ•ˆæ€§ï¼Œåœ¨å„ç§è¯­è¨€å»ºæ¨¡åŸºå‡†æµ‹è¯•ä¸­å–å¾—äº†æ˜¾è‘—æ”¹è¿›ï¼Œæ— è®ºæ˜¯åœ¨åŸºç¡€æ¨¡å‹ï¼ˆMMLUä¸Šæœ€å¤šæé«˜+4.7%ï¼‰è¿˜æ˜¯åœ¨æŒ‡ä»¤è°ƒä¼˜æ¨¡å‹ï¼ˆHumanEvalä¸Šæœ€å¤šæé«˜+10%ï¼‰ä¸Šã€‚ä»£ç åœ¨<a target="_blank" rel="noopener" href="https://github.com/johnheo/adadim-llm%E4%B8%8A%E6%8F%90%E4%BE%9B%E3%80%82">https://github.com/johnheo/adadim-llmä¸Šæä¾›ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.15531v4">PDF</a> ICLR 2024</p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨å„ä»»åŠ¡ä¸­è¡¨ç°å‡ºå“è¶Šæ€§èƒ½ï¼Œä½†å…¶å†…å­˜éœ€æ±‚å·¨å¤§ï¼Œåœ¨å°æ‰¹é‡æ¨ç†è®¾ç½®ï¼ˆå¦‚ç§»åŠ¨è®¾å¤‡ï¼‰ä¸­æä¾›æœåŠ¡å…·æœ‰æŒ‘æˆ˜æ€§ã€‚é’ˆå¯¹è¿™ä¸€é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„é‡åŒ–æ–¹æ³•â€”â€”AdaDimï¼Œå®ƒç»“åˆäº†æ¯è¾“å…¥é€šé“ï¼ˆICï¼‰çš„é‡åŒ–æ–¹æ¡ˆå’Œè‡ªé€‚åº”ç»´åº¦é‡åŒ–æ¡†æ¶ï¼Œä»¥æé«˜æ¨¡å‹çš„é‡åŒ–æ€§èƒ½å¹¶å‡å°‘å†…å­˜ç“¶é¢ˆã€‚AdaDimå¯æœ‰æ•ˆæ”¹å–„ç°æœ‰é‡åŒ–æ–¹æ³•å¦‚Round-To-Nearestå’ŒGPTQçš„æ€§èƒ½ï¼Œåœ¨å¤šç§è¯­è¨€å»ºæ¨¡åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°æ˜¾è‘—ï¼ŒåŒ…æ‹¬åŸºç¡€æ¨¡å‹å’ŒæŒ‡ä»¤å¾®è°ƒæ¨¡å‹ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLMsåœ¨å¤šç§ä»»åŠ¡ä¸­è¡¨ç°å‡ºå“è¶Šæ€§èƒ½ï¼Œä½†å†…å­˜éœ€æ±‚å¤§ï¼Œç‰¹åˆ«æ˜¯åœ¨å°æ‰¹é‡æ¨ç†ç¯å¢ƒä¸­ã€‚</li>
<li>ç°æœ‰é‡åŒ–æ–¹æ³•é¢ä¸´æŒ‘æˆ˜ï¼Œç‰¹åˆ«æ˜¯ä½äº4ä½çš„é‡åŒ–ç”±äºæ¿€æ´»å¼‚å¸¸å€¼çš„é—®é¢˜ã€‚</li>
<li>è®ºæ–‡æå‡ºäº†ä¸€ç§æ–°çš„é‡åŒ–ç­–ç•¥â€”â€”AdaDimï¼Œç»“åˆæ¯è¾“å…¥é€šé“ï¼ˆICï¼‰çš„é‡åŒ–æ–¹æ³•å’Œè‡ªé€‚åº”ç»´åº¦é‡åŒ–æ¡†æ¶ã€‚</li>
<li>AdaDimæ—¨åœ¨é€šè¿‡åˆ›å»ºé‡åŒ–ç»„æ¥éš”ç¦»æ¿€æ´»å¼‚å¸¸å€¼ï¼Œå¹¶é€‚åº”ä¸åŒçš„æƒé‡æ•æ„Ÿæ€§æ¨¡å¼ã€‚</li>
<li>AdaDimå¯¹å…ˆå‰çš„é‡åŒ–æ–¹æ³•è¿›è¡Œäº†æ”¹è¿›ï¼Œæ˜¾è‘—æé«˜äº†åŸºç¡€æ¨¡å‹å’ŒæŒ‡ä»¤å¾®è°ƒæ¨¡å‹åœ¨å¤šç§è¯­è¨€å»ºæ¨¡åŸºå‡†æµ‹è¯•ä¸­çš„æ€§èƒ½ã€‚</li>
<li>AdaDimæ–¹æ³•å·²åœ¨GitHubä¸Šå…¬å¼€å¯ç”¨ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2309.15531">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-f84a009b9cc1590d5a0be407de627c50.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7066d0041548994d53da8fd596feac07.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-27690c950804726e61eaba5e0557ec70.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8ec67a10efc039cba5fb7d33fa3000fb.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2a0ea5e9940ccec2b53dee9cdfa7177b.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-fbd4f7dc5e3f1b810d64076ddb6fbf07.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="UMLS-KGI-BERT-Data-Centric-Knowledge-Integration-in-Transformers-for-Biomedical-Entity-Recognition"><a href="#UMLS-KGI-BERT-Data-Centric-Knowledge-Integration-in-Transformers-for-Biomedical-Entity-Recognition" class="headerlink" title="UMLS-KGI-BERT: Data-Centric Knowledge Integration in Transformers for   Biomedical Entity Recognition"></a>UMLS-KGI-BERT: Data-Centric Knowledge Integration in Transformers for   Biomedical Entity Recognition</h2><p><strong>Authors:Aidan Mannion, Thierry Chevalier, Didier Schwab, Lorraine Geouriot</strong></p>
<p>Pre-trained transformer language models (LMs) have in recent years become the dominant paradigm in applied NLP. These models have achieved state-of-the-art performance on tasks such as information extraction, question answering, sentiment analysis, document classification and many others. In the biomedical domain, significant progress has been made in adapting this paradigm to NLP tasks that require the integration of domain-specific knowledge as well as statistical modelling of language. In particular, research in this area has focused on the question of how best to construct LMs that take into account not only the patterns of token distribution in medical text, but also the wealth of structured information contained in terminology resources such as the UMLS. This work contributes a data-centric paradigm for enriching the language representations of biomedical transformer-encoder LMs by extracting text sequences from the UMLS. This allows for graph-based learning objectives to be combined with masked-language pre-training. Preliminary results from experiments in the extension of pre-trained LMs as well as training from scratch show that this framework improves downstream performance on multiple biomedical and clinical Named Entity Recognition (NER) tasks. </p>
<blockquote>
<p>é¢„è®­ç»ƒè½¬æ¢è¯­è¨€æ¨¡å‹ï¼ˆLMsï¼‰è¿‘å¹´æ¥å·²æˆä¸ºåº”ç”¨NLPä¸­çš„ä¸»å¯¼èŒƒå¼ã€‚è¿™äº›æ¨¡å‹åœ¨ä¿¡æ¯æå–ã€é—®ç­”ã€æƒ…æ„Ÿåˆ†æã€æ–‡æ¡£åˆ†ç±»ç­‰ä»»åŠ¡ä¸Šè¾¾åˆ°äº†æœ€æ–°æŠ€æœ¯æ°´å¹³çš„è¡¨ç°ã€‚åœ¨ç”Ÿç‰©åŒ»å­¦é¢†åŸŸï¼Œå°†è¿™ç§èŒƒå¼é€‚åº”äºéœ€è¦æ•´åˆç‰¹å®šé¢†åŸŸçŸ¥è¯†å’Œè¯­è¨€ç»Ÿè®¡å»ºæ¨¡çš„NLPä»»åŠ¡æ–¹é¢ï¼Œå·²ç»å–å¾—äº†é‡å¤§è¿›å±•ã€‚ç‰¹åˆ«æ˜¯ï¼Œè¯¥é¢†åŸŸçš„ç ”ç©¶é‡ç‚¹æ˜¯å¦‚ä½•æ„å»ºæœ€ä½³çš„LMsï¼Œè¿™äº›LMsä¸ä»…è¦è€ƒè™‘åŒ»ç–—æ–‡æœ¬ä¸­çš„æ ‡è®°åˆ†å¸ƒæ¨¡å¼ï¼Œè¿˜è¦åˆ©ç”¨æœ¯è¯­èµ„æºï¼ˆå¦‚UMLSï¼‰ä¸­åŒ…å«çš„å¤§é‡ç»“æ„åŒ–ä¿¡æ¯ã€‚æœ¬æ–‡è´¡çŒ®äº†ä¸€ç§ä»¥æ•°æ®ä¸ºä¸­å¿ƒçš„æ–¹æ³•ï¼Œé€šè¿‡ä»UMLSä¸­æå–æ–‡æœ¬åºåˆ—æ¥ä¸°å¯Œç”Ÿç‰©åŒ»å­¦è½¬æ¢ç¼–ç å™¨LMsçš„è¯­è¨€è¡¨ç¤ºã€‚è¿™å…è®¸å°†åŸºäºå›¾çš„å­¦ä¹ ç›®æ ‡ä¸æ©ç è¯­è¨€é¢„è®­ç»ƒç›¸ç»“åˆã€‚åˆæ­¥å®éªŒç»“æœæ˜¾ç¤ºï¼Œé€šè¿‡æ‰©å±•é¢„è®­ç»ƒLMsæˆ–ä»å¤´å¼€å§‹è®­ç»ƒçš„ç»“æœè¡¨æ˜ï¼Œè¯¥æ¡†æ¶åœ¨å¤šç”Ÿç‰©åŒ»å­¦å’Œä¸´åºŠå‘½åå®ä½“è¯†åˆ«ï¼ˆNERï¼‰ä»»åŠ¡ä¸Šçš„ä¸‹æ¸¸æ€§èƒ½æœ‰æ‰€æå‡ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11170v2">PDF</a> Addition of v2 experiments</p>
<p><strong>Summary</strong></p>
<p>é¢„è®­ç»ƒè½¬æ¢å™¨è¯­è¨€æ¨¡å‹ï¼ˆLMsï¼‰å·²æˆä¸ºåº”ç”¨è‡ªç„¶è¯­è¨€å¤„ç†ï¼ˆNLPï¼‰ä¸­çš„ä¸»è¦èŒƒå¼ï¼Œç‰¹åˆ«æ˜¯åœ¨ç”Ÿç‰©åŒ»å­¦é¢†åŸŸï¼Œè¿™äº›æ¨¡å‹åœ¨ä»»åŠ¡ä¸Šå–å¾—äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œå¦‚ä¿¡æ¯æå–ã€é—®ç­”ã€æƒ…æ„Ÿåˆ†æã€æ–‡æ¡£åˆ†ç±»ç­‰ã€‚ç ”ç©¶é›†ä¸­äºå¦‚ä½•æ„å»ºè€ƒè™‘åŒ»å­¦æ–‡æœ¬ä¸­çš„æ ‡è®°åˆ†å¸ƒä»¥åŠæœ¯è¯­èµ„æºï¼ˆå¦‚UMLSï¼‰ä¸­ä¸°å¯Œç»“æ„ä¿¡æ¯çš„è¯­è¨€æ¨¡å‹ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§ä»¥æ•°æ®ä¸ºä¸­å¿ƒçš„æ–¹æ³•ï¼Œé€šè¿‡ä»UMLSä¸­æå–æ–‡æœ¬åºåˆ—æ¥ä¸°å¯Œç”Ÿç‰©åŒ»å­¦è½¬æ¢å™¨ç¼–ç å™¨çš„è¯­è¨€è¡¨ç¤ºï¼Œç»“åˆåŸºäºå›¾çš„å­¦ä¹ ç›®æ ‡å’Œæ©ç è¯­è¨€é¢„è®­ç»ƒã€‚åˆæ­¥å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ¡†æ¶åœ¨å¤šç”Ÿç‰©åŒ»å­¦å’Œä¸´åºŠå‘½åå®ä½“è¯†åˆ«ï¼ˆNERï¼‰ä»»åŠ¡ä¸Šæé«˜äº†ä¸‹æ¸¸æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>é¢„è®­ç»ƒè½¬æ¢å™¨è¯­è¨€æ¨¡å‹ï¼ˆLMsï¼‰åœ¨è‡ªç„¶è¯­è¨€å¤„ç†ï¼ˆNLPï¼‰é¢†åŸŸå æ®ä¸»å¯¼åœ°ä½ï¼Œç‰¹åˆ«æ˜¯åœ¨å®Œæˆä¿¡æ¯æå–ã€é—®ç­”ã€æƒ…æ„Ÿåˆ†æå’Œæ–‡æ¡£åˆ†ç±»ç­‰ä»»åŠ¡æ—¶è¡¨ç°å‡ºè‰²ã€‚</li>
<li>åœ¨ç”Ÿç‰©åŒ»å­¦é¢†åŸŸï¼Œç»“åˆé¢†åŸŸç‰¹å®šçŸ¥è¯†å’Œè¯­è¨€ç»Ÿè®¡æ¨¡å‹çš„LMsç ”ç©¶å·²å–å¾—æ˜¾è‘—è¿›å±•ã€‚</li>
<li>ç ”ç©¶ç„¦ç‚¹åœ¨äºå¦‚ä½•æ„å»ºè€ƒè™‘åŒ»å­¦æ–‡æœ¬ä¸­çš„æ ‡è®°åˆ†å¸ƒå’Œæœ¯è¯­èµ„æºï¼ˆå¦‚UMLSï¼‰ä¸­çš„ç»“æ„åŒ–ä¿¡æ¯çš„è¯­è¨€æ¨¡å‹ã€‚</li>
<li>æœ¬æ–‡æå‡ºäº†ä¸€ç§æ•°æ®ä¸ºä¸­å¿ƒçš„æ–¹æ³•ï¼Œé€šè¿‡ä»UMLSæå–æ–‡æœ¬åºåˆ—æ¥ä¸°å¯Œç”Ÿç‰©åŒ»å­¦LMsçš„è¯­è¨€è¡¨ç¤ºã€‚</li>
<li>è¯¥æ–¹æ³•ç»“åˆäº†åŸºäºå›¾çš„å­¦ä¹ ç›®æ ‡å’Œæ©ç è¯­è¨€é¢„è®­ç»ƒã€‚</li>
<li>åˆæ­¥å®éªŒè¡¨æ˜ï¼Œè¯¥æ¡†æ¶åœ¨å¤šä¸ªç”Ÿç‰©åŒ»å­¦å’Œä¸´åºŠå‘½åå®ä½“è¯†åˆ«ï¼ˆNERï¼‰ä»»åŠ¡ä¸Šæé«˜äº†æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2307.11170">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-69a253479d960bf364fe976e4bc6f9da.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e1908b87bcdfe7b6466d7e0b895db446.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7f7738084745254cd417fbd933018ea5.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-04-20/LLM/">https://kedreamix.github.io/Talk2Paper/Paper/2025-04-20/LLM/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/LLM/">
                                    <span class="chip bg-color">LLM</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-04-22/R1_Reasoning/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-6a0cd3eae0718cf031ebf00fa2d2899c.jpg" class="responsive-img" alt="R1_Reasoning">
                        
                        <span class="card-title">R1_Reasoning</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            R1_Reasoning æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-04-22  Does Reinforcement Learning Really Incentivize Reasoning Capacity in   LLMs Beyond the Base Model?
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-04-22
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/R1-Reasoning/" class="post-category">
                                    R1_Reasoning
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/R1-Reasoning/">
                        <span class="chip bg-color">R1_Reasoning</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-04-20/R1_Reasoning/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-52b875081e85d964ca333fe9ab756775.jpg" class="responsive-img" alt="R1_Reasoning">
                        
                        <span class="card-title">R1_Reasoning</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            R1_Reasoning æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-04-20  Memorization vs. Reasoning Updating LLMs with New Knowledge
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-04-20
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/R1-Reasoning/" class="post-category">
                                    R1_Reasoning
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/R1-Reasoning/">
                        <span class="chip bg-color">R1_Reasoning</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">16663.4k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
