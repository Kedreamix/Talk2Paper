<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="LLM">
    <meta name="description" content="LLM æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2024-12-22  Lightning IR Straightforward Fine-tuning and Inference of   Transformer-based Language Models for Information Retrieval">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>LLM | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-a89122439cd12cd8a282bce1aa710f09.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">LLM</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/LLM/">
                                <span class="chip bg-color">LLM</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/LLM/" class="post-category">
                                LLM
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2024-12-22
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2024-12-22
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    18.9k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    76 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2024-12-22-æ›´æ–°"><a href="#2024-12-22-æ›´æ–°" class="headerlink" title="2024-12-22 æ›´æ–°"></a>2024-12-22 æ›´æ–°</h1><h2 id="Lightning-IR-Straightforward-Fine-tuning-and-Inference-of-Transformer-based-Language-Models-for-Information-Retrieval"><a href="#Lightning-IR-Straightforward-Fine-tuning-and-Inference-of-Transformer-based-Language-Models-for-Information-Retrieval" class="headerlink" title="Lightning IR: Straightforward Fine-tuning and Inference of   Transformer-based Language Models for Information Retrieval"></a>Lightning IR: Straightforward Fine-tuning and Inference of   Transformer-based Language Models for Information Retrieval</h2><p><strong>Authors:Ferdinand Schlatt, Maik FrÃ¶be, Matthias Hagen</strong></p>
<p>A wide range of transformer-based language models have been proposed for information retrieval tasks. However, including transformer-based models in retrieval pipelines is often complex and requires substantial engineering effort. In this paper, we introduce Lightning IR, an easy-to-use PyTorch Lightning-based framework for applying transformer-based language models in retrieval scenarios. Lightning IR provides a modular and extensible architecture that supports all stages of a retrieval pipeline: from fine-tuning and indexing to searching and re-ranking. Designed to be scalable and reproducible, Lightning IR is available as open-source: <a target="_blank" rel="noopener" href="https://github.com/webis-de/lightning-ir">https://github.com/webis-de/lightning-ir</a>. </p>
<blockquote>
<p>é’ˆå¯¹ä¿¡æ¯æ£€ç´¢ä»»åŠ¡ï¼Œå·²ç»æå‡ºäº†å¤šç§åŸºäºTransformerçš„è¯­è¨€æ¨¡å‹ã€‚ç„¶è€Œï¼Œåœ¨æ£€ç´¢ç®¡é“ä¸­åŒ…å«åŸºäºTransformerçš„æ¨¡å‹é€šå¸¸å¾ˆå¤æ‚ï¼Œéœ€è¦å¤§é‡çš„å·¥ç¨‹åŠªåŠ›ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†Lightning IRï¼Œè¿™æ˜¯ä¸€ä¸ªæ˜“äºä½¿ç”¨çš„åŸºäºPyTorch Lightningçš„æ¡†æ¶ï¼Œå¯ç”¨äºåœ¨æ£€ç´¢åœºæ™¯ä¸­åº”ç”¨åŸºäºTransformerçš„è¯­è¨€æ¨¡å‹ã€‚Lightning IRæä¾›äº†ä¸€ç§æ¨¡å—åŒ–ä¸”å¯æ‰©å±•çš„æ¶æ„ï¼Œæ”¯æŒæ£€ç´¢ç®¡é“çš„æ‰€æœ‰é˜¶æ®µï¼šä»å¾®è°ƒã€ç´¢å¼•åˆ°æœç´¢å’Œé‡æ–°æ’åºã€‚è®¾è®¡ç”¨äºå¯æ‰©å±•æ€§å’Œå¯é‡å¤æ€§ï¼ŒLightning IRå¯ä½œä¸ºå¼€æºä½¿ç”¨ï¼š<a target="_blank" rel="noopener" href="https://github.com/webis-de/lightning-ir%E3%80%82">https://github.com/webis-de/lightning-irã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2411.04677v3">PDF</a> Accepted as a demo at WSDMâ€™25</p>
<p><strong>Summary</strong>ï¼š<br>é—ªç”µIRæ˜¯ä¸€ä¸ªåŸºäºPyTorch Lightningçš„æ˜“ç”¨æ¡†æ¶ï¼Œç”¨äºåœ¨æ£€ç´¢åœºæ™¯ä¸­åº”ç”¨åŸºäºè½¬æ¢å™¨çš„è¯­è¨€æ¨¡å‹ã€‚å®ƒæä¾›äº†ä¸€ä¸ªæ¨¡å—åŒ–ä¸”å¯æ‰©å±•çš„æ¶æ„ï¼Œæ”¯æŒæ£€ç´¢ç®¡é“çš„æ‰€æœ‰é˜¶æ®µï¼ŒåŒ…æ‹¬å¾®è°ƒã€ç´¢å¼•ã€æœç´¢å’Œé‡æ–°æ’åã€‚é—ªç”µIRæ—¨åœ¨å®ç°å¯æ‰©å±•æ€§å’Œå¯é‡å¤æ€§ï¼Œå¯ä½œä¸ºå¼€æºä½¿ç”¨ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ul>
<li>å¤šç§åŸºäºè½¬æ¢å™¨çš„è¯­è¨€æ¨¡å‹è¢«æå‡ºç”¨äºä¿¡æ¯æ£€ç´¢ä»»åŠ¡ã€‚</li>
<li>åœ¨æ£€ç´¢ç®¡é“ä¸­åŒ…å«åŸºäºè½¬æ¢å™¨çš„æ¨¡å‹é€šå¸¸éœ€è¦å¤æ‚çš„å·¥ç¨‹å·¥ä½œã€‚</li>
<li>é—ªç”µIRæ˜¯ä¸€ä¸ªåŸºäºPyTorch Lightningçš„æ¡†æ¶ï¼Œæ˜“äºåº”ç”¨åœ¨æ£€ç´¢åœºæ™¯ä¸­çš„åŸºäºè½¬æ¢å™¨çš„è¯­è¨€æ¨¡å‹ã€‚</li>
<li>é—ªç”µIRæä¾›äº†æ¨¡å—åŒ–ä¸”å¯æ‰©å±•çš„æ¶æ„ï¼Œæ”¯æŒæ£€ç´¢ç®¡é“çš„æ‰€æœ‰é˜¶æ®µã€‚</li>
<li>é—ªç”µIRæ—¨åœ¨å®ç°å¯æ‰©å±•æ€§å’Œå¯é‡å¤æ€§ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2411.04677">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-87f72f936b8aba8a709151a5fd9b6610.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-5b5dd2dad1bd298ba6a8e6704861d17b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-6d678d2835f16b3b9cda3e96f63c79bd.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-634ac9430bbdc2afa8852ad14b5e7eb7.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Analysing-Zero-Shot-Readability-Controlled-Sentence-Simplification"><a href="#Analysing-Zero-Shot-Readability-Controlled-Sentence-Simplification" class="headerlink" title="Analysing Zero-Shot Readability-Controlled Sentence Simplification"></a>Analysing Zero-Shot Readability-Controlled Sentence Simplification</h2><p><strong>Authors:Abdullah Barayan, Jose Camacho-Collados, Fernando Alva-Manchego</strong></p>
<p>Readability-controlled text simplification (RCTS) rewrites texts to lower readability levels while preserving their meaning. RCTS models often depend on parallel corpora with readability annotations on both source and target sides. Such datasets are scarce and difficult to curate, especially at the sentence level. To reduce reliance on parallel data, we explore using instruction-tuned large language models for zero-shot RCTS. Through automatic and manual evaluations, we examine: (1) how different types of contextual information affect a modelâ€™s ability to generate sentences with the desired readability, and (2) the trade-off between achieving target readability and preserving meaning. Results show that all tested models struggle to simplify sentences (especially to the lowest levels) due to modelsâ€™ limitations and characteristics of the source sentences that impede adequate rewriting. Our experiments also highlight the need for better automatic evaluation metrics tailored to RCTS, as standard ones often misinterpret common simplification operations, and inaccurately assess readability and meaning preservation. </p>
<blockquote>
<p>å¯è¯»æ€§æ§åˆ¶æ–‡æœ¬ç®€åŒ–ï¼ˆRCTSï¼‰é‡å†™æ–‡æœ¬ä»¥é™ä½å¯è¯»æ€§æ°´å¹³ï¼ŒåŒæ—¶ä¿ç•™å…¶å«ä¹‰ã€‚RCTSæ¨¡å‹é€šå¸¸ä¾èµ–äºå¸¦æœ‰æºå’Œç›®æ ‡ä¸¤ä¾§å¯è¯»æ€§æ³¨é‡Šçš„å¹³è¡Œè¯­æ–™åº“ã€‚è¿™æ ·çš„æ•°æ®é›†å¾ˆç¨€ç¼ºï¼Œä¸”éš¾ä»¥ç¼–çº‚ï¼Œå°¤å…¶æ˜¯åœ¨å¥å­å±‚é¢ã€‚ä¸ºäº†å‡å°‘å¹³è¡Œæ•°æ®çš„ä¾èµ–ï¼Œæˆ‘ä»¬æ¢ç´¢ä½¿ç”¨æŒ‡ä»¤è°ƒä¼˜çš„å¤§å‹è¯­è¨€æ¨¡å‹è¿›è¡Œé›¶æ ·æœ¬RCTSã€‚é€šè¿‡è‡ªåŠ¨å’Œæ‰‹åŠ¨è¯„ä¼°ï¼Œæˆ‘ä»¬ç ”ç©¶äº†ï¼šï¼ˆ1ï¼‰ä¸åŒç±»å‹çš„ä¸Šä¸‹æ–‡ä¿¡æ¯å¦‚ä½•å½±å“æ¨¡å‹ç”Ÿæˆå…·æœ‰æ‰€éœ€å¯è¯»æ€§çš„å¥å­çš„èƒ½åŠ›ï¼›ï¼ˆ2ï¼‰å®ç°ç›®æ ‡å¯è¯»æ€§å’Œä¿ç•™æ„ä¹‰ä¹‹é—´çš„æƒè¡¡ã€‚ç»“æœè¡¨æ˜ï¼Œæ‰€æœ‰æµ‹è¯•æ¨¡å‹åœ¨ç®€åŒ–å¥å­æ–¹é¢éƒ½å­˜åœ¨å›°éš¾ï¼ˆå°¤å…¶æ˜¯åœ¨æœ€ä½æ°´å¹³ï¼‰ï¼Œè¿™æ˜¯ç”±äºæ¨¡å‹çš„å±€é™æ€§ä»¥åŠæºå¥å­çš„ç‰¹æ€§é˜»ç¢äº†é€‚å½“çš„é‡å†™ã€‚æˆ‘ä»¬çš„å®éªŒè¿˜å¼ºè°ƒäº†éœ€è¦ä¸ºRCTSé‡èº«å®šåˆ¶æ›´å¥½çš„è‡ªåŠ¨è¯„ä¼°æŒ‡æ ‡ï¼Œå› ä¸ºæ ‡å‡†æŒ‡æ ‡é€šå¸¸è¯¯è§£å¸¸è§çš„ç®€åŒ–æ“ä½œï¼Œå¹¶é”™è¯¯åœ°è¯„ä¼°å¯è¯»æ€§å’Œæ„ä¹‰ä¿ç•™ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2409.20246v2">PDF</a> Accepted on COLING 2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ¢è®¨äº†å¯è¯»æ€§æ§åˆ¶æ–‡æœ¬ç®€åŒ–ï¼ˆRCTSï¼‰çš„æ–¹æ³•ï¼Œå³é€šè¿‡é‡å†™æ–‡æœ¬é™ä½å…¶å¯è¯»æ€§æ°´å¹³åŒæ—¶ä¿æŒå…¶åŸæ„ã€‚RCTSæ¨¡å‹é€šå¸¸ä¾èµ–äºå¸¦æœ‰å¯è¯»æ€§å’Œç›®æ ‡ä¾§æ³¨é‡Šçš„å¹³è¡Œè¯­æ–™åº“ã€‚ä¸ºäº†å‡å°‘å¯¹å¹³è¡Œæ•°æ®çš„ä¾èµ–ï¼Œæœ¬æ–‡å°è¯•ä½¿ç”¨ç»è¿‡æŒ‡ä»¤è®­ç»ƒçš„å¤§å‹è¯­è¨€æ¨¡å‹è¿›è¡Œé›¶æ ·æœ¬RCTSã€‚é€šè¿‡è‡ªåŠ¨å’Œæ‰‹åŠ¨è¯„ä¼°ï¼Œæœ¬æ–‡ç ”ç©¶äº†ä¸åŒç±»å‹ä¸Šä¸‹æ–‡ä¿¡æ¯å¯¹æ¨¡å‹ç”Ÿæˆå…·æœ‰æ‰€éœ€å¯è¯»æ€§çš„å¥å­çš„èƒ½åŠ›çš„å½±å“ï¼Œä»¥åŠå®ç°ç›®æ ‡å¯è¯»æ€§å’Œä¿æŒæ„ä¹‰ä¹‹é—´çš„æƒè¡¡ã€‚ç„¶è€Œï¼Œå®éªŒç»“æœæ­ç¤ºç”±äºæ¨¡å‹çš„å±€é™æ€§å’Œæºå¥å­çš„ç‰¹æ€§ï¼Œæ‰€æœ‰æµ‹è¯•çš„æ¨¡å‹åœ¨ç®€åŒ–å¥å­æ–¹é¢éƒ½å­˜åœ¨å›°éš¾ï¼ˆå°¤å…¶æ˜¯åœ¨è¾¾åˆ°æœ€ä½æ°´å¹³æ—¶ï¼‰ã€‚æ­¤å¤–ï¼Œå®éªŒè¿˜å¼ºè°ƒäº†éœ€è¦ä¸ºRCTSé‡èº«å®šåˆ¶æ›´å¥½çš„è‡ªåŠ¨è¯„ä¼°æŒ‡æ ‡ï¼Œå› ä¸ºæ ‡å‡†æŒ‡æ ‡å¸¸å¸¸è¯¯è§£å¸¸è§çš„ç®€åŒ–æ“ä½œï¼Œå¹¶å‡†ç¡®è¯„ä¼°å¯è¯»æ€§å’Œæ„ä¹‰ä¿ç•™æƒ…å†µã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>RCTSæ—¨åœ¨é€šè¿‡é‡å†™æ–‡æœ¬é™ä½å¯è¯»æ€§æ°´å¹³ï¼ŒåŒæ—¶ä¿æŒåŸæ„ã€‚</li>
<li>RCTSæ¨¡å‹é€šå¸¸ä¾èµ–äºå¹³è¡Œè¯­æ–™åº“ï¼Œä½†è¿™ç±»æ•°æ®ç¨€ç¼ºä¸”éš¾ä»¥æ”¶é›†ï¼Œç‰¹åˆ«æ˜¯åœ¨å¥å­çº§åˆ«ã€‚</li>
<li>ä¸ºäº†å‡å°‘å¹³è¡Œæ•°æ®çš„ä¾èµ–ï¼Œç ”ç©¶äº†ä½¿ç”¨æŒ‡ä»¤è®­ç»ƒçš„å¤§å‹è¯­è¨€æ¨¡å‹è¿›è¡Œé›¶æ ·æœ¬RCTSã€‚</li>
<li>ä¸åŒç±»å‹çš„ä¸Šä¸‹æ–‡ä¿¡æ¯å¯¹æ¨¡å‹ç”Ÿæˆå…·æœ‰æ‰€éœ€å¯è¯»æ€§çš„å¥å­çš„èƒ½åŠ›æœ‰é‡è¦å½±å“ã€‚</li>
<li>å®ç°ç›®æ ‡å¯è¯»æ€§å’Œä¿æŒæ„ä¹‰ä¹‹é—´å­˜åœ¨æƒè¡¡ã€‚</li>
<li>ç°æœ‰æ¨¡å‹åœ¨ç®€åŒ–å¥å­æ–¹é¢å­˜åœ¨å›°éš¾ï¼Œå°¤å…¶æ˜¯åœ¨è¾¾åˆ°æœ€ä½å¯è¯»æ€§æ°´å¹³æ—¶ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2409.20246">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-1ed12a6ffa18193eca1a676094480895.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-9073d6f5a71b18c55191740085e9b16f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8af15dd0cdaafa848890da63b3b0d779.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-733df78e8473592907eeefb08a46aaa3.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-cf30e76f2bb372852cf4c501794416c6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a192f4f0ee634160df18a01ab42943d7.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="KnowFormer-Revisiting-Transformers-for-Knowledge-Graph-Reasoning"><a href="#KnowFormer-Revisiting-Transformers-for-Knowledge-Graph-Reasoning" class="headerlink" title="KnowFormer: Revisiting Transformers for Knowledge Graph Reasoning"></a>KnowFormer: Revisiting Transformers for Knowledge Graph Reasoning</h2><p><strong>Authors:Junnan Liu, Qianren Mao, Weifeng Jiang, Jianxin Li</strong></p>
<p>Knowledge graph reasoning plays a vital role in various applications and has garnered considerable attention. Recently, path-based methods have achieved impressive performance. However, they may face limitations stemming from constraints in message-passing neural networks, such as missing paths and information over-squashing. In this paper, we revisit the application of transformers for knowledge graph reasoning to address the constraints faced by path-based methods and propose a novel method KnowFormer. KnowFormer utilizes a transformer architecture to perform reasoning on knowledge graphs from the message-passing perspective, rather than reasoning by textual information like previous pretrained language model based methods. Specifically, we define the attention computation based on the query prototype of knowledge graph reasoning, facilitating convenient construction and efficient optimization. To incorporate structural information into the self-attention mechanism, we introduce structure-aware modules to calculate query, key, and value respectively. Additionally, we present an efficient attention computation method for better scalability. Experimental results demonstrate the superior performance of KnowFormer compared to prominent baseline methods on both transductive and inductive benchmarks. </p>
<blockquote>
<p>çŸ¥è¯†å›¾è°±æ¨ç†åœ¨å„ç§åº”ç”¨ä¸­å‘æŒ¥ç€è‡³å…³é‡è¦çš„ä½œç”¨ï¼Œå¹¶å¼•èµ·äº†å¹¿æ³›çš„å…³æ³¨ã€‚æœ€è¿‘ï¼ŒåŸºäºè·¯å¾„çš„æ–¹æ³•å–å¾—äº†ä»¤äººå°è±¡æ·±åˆ»çš„æ€§èƒ½ã€‚ç„¶è€Œï¼Œå®ƒä»¬å¯èƒ½ä¼šé¢ä¸´æ¥è‡ªæ¶ˆæ¯ä¼ é€’ç¥ç»ç½‘ç»œçº¦æŸçš„é™åˆ¶ï¼Œä¾‹å¦‚è·¯å¾„ç¼ºå¤±å’Œä¿¡æ¯è¿‡åº¦å‹ç¼©ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬é‡æ–°å®¡è§†äº†å˜å‹å™¨åœ¨çŸ¥è¯†å›¾è°±æ¨ç†ä¸­çš„åº”ç”¨ï¼Œä»¥è§£å†³åŸºäºè·¯å¾„çš„æ–¹æ³•æ‰€é¢ä¸´çš„çº¦æŸï¼Œå¹¶æå‡ºäº†ä¸€ç§æ–°æ–¹æ³•KnowFormerã€‚KnowFormeråˆ©ç”¨å˜å‹å™¨æ¶æ„ä»æ¶ˆæ¯ä¼ é€’çš„è§’åº¦å¯¹çŸ¥è¯†å›¾è°±è¿›è¡Œæ¨ç†ï¼Œè€Œä¸æ˜¯åƒä»¥å‰åŸºäºé¢„è®­ç»ƒçš„è¯­è¨€æ¨¡å‹æ–¹æ³•é‚£æ ·é€šè¿‡æ–‡æœ¬ä¿¡æ¯è¿›è¡Œæ¨ç†ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬åŸºäºçŸ¥è¯†å›¾è°±æ¨ç†çš„æŸ¥è¯¢åŸå‹æ¥å®šä¹‰æ³¨æ„åŠ›è®¡ç®—ï¼Œä¾¿äºæ„å»ºå’Œé«˜æ•ˆä¼˜åŒ–ã€‚ä¸ºäº†å°†ç»“æ„ä¿¡æ¯èå…¥è‡ªæ³¨æ„åŠ›æœºåˆ¶ï¼Œæˆ‘ä»¬å¼•å…¥äº†ç»“æ„æ„ŸçŸ¥æ¨¡å—æ¥åˆ†åˆ«è®¡ç®—æŸ¥è¯¢ã€é”®å’Œå€¼ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜æå‡ºäº†ä¸€ç§é«˜æ•ˆçš„æ³¨æ„åŠ›è®¡ç®—æ–¹æ³•ï¼Œä»¥æé«˜å¯æ‰©å±•æ€§ã€‚å®éªŒç»“æœè¡¨è¡¨æ˜ï¼Œä¸è½¬å¯¼å’Œå½’çº³åŸºå‡†æµ‹è¯•ä¸Šçš„ä¸»æµæ–¹æ³•ç›¸æ¯”ï¼ŒKnowFormerçš„æ€§èƒ½æ›´ä¼˜è¶Šã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2409.12865v2">PDF</a> Accepted by ICML2024</p>
<p><strong>Summary</strong><br>çŸ¥è¯†å›¾è°±æ¨ç†åœ¨å„ç§åº”ç”¨ä¸­å‘æŒ¥ç€é‡è¦ä½œç”¨ï¼Œå¹¶å¼•èµ·äº†å¹¿æ³›å…³æ³¨ã€‚å°½ç®¡è·¯å¾„åŸºäºæ–¹æ³•å·²ç»å–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ï¼Œä½†å®ƒä»¬ä»é¢ä¸´ç€æ¶ˆæ¯ä¼ é€’ç¥ç»ç½‘ç»œä¸­çš„çº¦æŸï¼Œå¦‚è·¯å¾„ç¼ºå¤±å’Œä¿¡æ¯è¿‡åº¦å‹ç¼©ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°å‹çŸ¥è¯†å›¾è°±æ¨ç†æ–¹æ³•KnowFormerï¼Œé‡‡ç”¨å˜å‹å™¨æ¶æ„ä»æ¶ˆæ¯ä¼ é€’è§’åº¦è¿›è¡ŒçŸ¥è¯†å›¾è°±æ¨ç†ï¼Œè§£å†³äº†è·¯å¾„åŸºäºæ–¹æ³•æ‰€é¢ä¸´çš„çº¦æŸã€‚KnowFormerå®šä¹‰äº†åŸºäºçŸ¥è¯†å›¾è°±æ¨ç†æŸ¥è¯¢åŸå‹çš„æ³¨æ„åŠ›è®¡ç®—ï¼Œä¾¿äºæ„å»ºå’Œé«˜æ•ˆä¼˜åŒ–ã€‚åŒæ—¶ï¼Œä¸ºäº†å°†ç»“æ„ä¿¡æ¯èå…¥è‡ªæ³¨æ„åŠ›æœºåˆ¶ï¼Œå¼•å…¥äº†ç»“æ„æ„ŸçŸ¥æ¨¡å—åˆ†åˆ«è®¡ç®—æŸ¥è¯¢ã€é”®å’Œå€¼ã€‚æ­¤å¤–ï¼Œè¿˜æå‡ºäº†ä¸€ç§æœ‰æ•ˆçš„æ³¨æ„åŠ›è®¡ç®—æ–¹æ³•ä»¥å®ç°æ›´å¥½çš„å¯æ‰©å±•æ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œåœ¨è½¬å¯¼å’Œå½’çº³åŸºå‡†æµ‹è¯•ä¸­ï¼ŒKnowFormerç›¸è¾ƒäºä¸»æµåŸºçº¿æ–¹æ³•è¡¨ç°å‡ºå“è¶Šæ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>çŸ¥è¯†å›¾è°±æ¨ç†åœ¨è®¸å¤šåº”ç”¨ä¸­è‡³å…³é‡è¦ï¼Œå¹¶å—åˆ°å¹¿æ³›å…³æ³¨ã€‚</li>
<li>è·¯å¾„åŸºäºçš„æ–¹æ³•åœ¨çŸ¥è¯†å›¾è°±æ¨ç†ä¸­å–å¾—äº†æ˜¾è‘—æ€§èƒ½æå‡ï¼Œä½†ä»é¢ä¸´çº¦æŸã€‚</li>
<li>KnowFormeré‡‡ç”¨å˜å‹å™¨æ¶æ„è¿›è¡ŒçŸ¥è¯†å›¾è°±æ¨ç†ï¼Œè§£å†³äº†è·¯å¾„åŸºäºæ–¹æ³•çš„çº¦æŸã€‚</li>
<li>KnowFormeråˆ©ç”¨æŸ¥è¯¢åŸå‹å®šä¹‰æ³¨æ„åŠ›è®¡ç®—ï¼Œæ–¹ä¾¿æ„å»ºå’Œé«˜æ•ˆä¼˜åŒ–ã€‚</li>
<li>KnowFormerå¼•å…¥ç»“æ„æ„ŸçŸ¥æ¨¡å—èå…¥ç»“æ„ä¿¡æ¯åˆ°è‡ªæ³¨æ„åŠ›æœºåˆ¶ä¸­ã€‚</li>
<li>KnowFormeræå‡ºäº†ä¸€ç§æœ‰æ•ˆçš„æ³¨æ„åŠ›è®¡ç®—æ–¹æ³•ä»¥å®ç°æ›´å¥½çš„å¯æ‰©å±•æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2409.12865">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-7bfb173c68daaff98304e748ac1c1718.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-9f0a1d23a70507f52f71accff801dd3f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-6db571c78da734b2189f7c67ed42a3a6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0c8b4bb05e46a33d9f1c2013006430fd.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Can-GPT-O1-Kill-All-Bugs-An-Evaluation-of-GPT-Family-LLMs-on-QuixBugs"><a href="#Can-GPT-O1-Kill-All-Bugs-An-Evaluation-of-GPT-Family-LLMs-on-QuixBugs" class="headerlink" title="Can GPT-O1 Kill All Bugs? An Evaluation of GPT-Family LLMs on QuixBugs"></a>Can GPT-O1 Kill All Bugs? An Evaluation of GPT-Family LLMs on QuixBugs</h2><p><strong>Authors:Haichuan Hu, Ye Shang, Guolin Xu, Congqing He, Quanjun Zhang</strong></p>
<p>LLMs have long demonstrated remarkable effectiveness in automatic program repair (APR), with OpenAIâ€™s ChatGPT being one of the most widely used models in this domain. Through continuous iterations and upgrades of GPT-family models, their performance in fixing bugs has already reached state-of-the-art levels. However, there are few works comparing the effectiveness and variations of different versions of GPT-family models on APR. In this work, inspired by the recent public release of the GPT-o1 models, we conduct the first study to compare the effectiveness of different versions of the GPT-family models in APR. We evaluate the performance of the latest version of the GPT-family models (i.e., O1-preview and O1-mini), GPT-4o, and the historical version of ChatGPT on APR. We conduct an empirical study of the four GPT-family models against other LLMs and APR techniques on the QuixBugs benchmark from multiple evaluation perspectives, including repair success rate, repair cost, response length, and behavior patterns. The results demonstrate that O1â€™s repair capability exceeds that of prior GPT-family models, successfully fixing all 40 bugs in the benchmark. Our work can serve as a foundation for further in-depth exploration of the applications of GPT-family models in APR. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨è‡ªåŠ¨ç¨‹åºä¿®å¤ï¼ˆAPRï¼‰é¢†åŸŸå·²ç»å±•ç°å‡ºæ˜¾è‘—çš„æ•ˆæœã€‚OpenAIçš„ChatGPTæ˜¯è¯¥é¢†åŸŸä¸­æœ€å¹¿æ³›ä½¿ç”¨çš„æ¨¡å‹ä¹‹ä¸€ã€‚é€šè¿‡GPTç³»åˆ—æ¨¡å‹çš„æŒç»­è¿­ä»£å’Œå‡çº§ï¼Œå®ƒä»¬åœ¨ä¿®å¤é”™è¯¯æ–¹é¢çš„æ€§èƒ½å·²ç»è¾¾åˆ°äº†æœ€æ–°æ°´å¹³ã€‚ç„¶è€Œï¼Œå…³äºä¸åŒç‰ˆæœ¬çš„GPTç³»åˆ—æ¨¡å‹åœ¨APRä¸­çš„æ•ˆæœå’Œå·®å¼‚çš„ç ”ç©¶å·¥ä½œå¾ˆå°‘ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œå—åˆ°æœ€è¿‘GPT-o1æ¨¡å‹å…¬å¼€å‘å¸ƒçš„å¯å‘ï¼Œæˆ‘ä»¬è¿›è¡Œäº†ç¬¬ä¸€é¡¹æ¯”è¾ƒGPTç³»åˆ—ä¸åŒç‰ˆæœ¬åœ¨APRä¸­æœ‰æ•ˆæ€§çš„ç ”ç©¶ã€‚æˆ‘ä»¬è¯„ä¼°äº†GPTç³»åˆ—æ¨¡å‹çš„æœ€æ–°ç‰ˆæœ¬ï¼ˆå³O1-previewå’ŒO1-miniï¼‰ã€GPT-4oä»¥åŠChatGPTçš„å†å²ç‰ˆæœ¬åœ¨APRä¸­çš„æ€§èƒ½ã€‚æˆ‘ä»¬åœ¨QuixBugsåŸºå‡†æµ‹è¯•ä¸Šå¯¹è¿™å››ä¸ªGPTç³»åˆ—æ¨¡å‹ä¸å…¶ä»–å¤§å‹è¯­è¨€æ¨¡å‹å’ŒAPRæŠ€æœ¯è¿›è¡Œäº†å®è¯ç ”ç©¶ï¼Œä»å¤šä¸ªè¯„ä¼°è§’åº¦è¿›è¡Œäº†å¯¹æ¯”ï¼ŒåŒ…æ‹¬ä¿®å¤æˆåŠŸç‡ã€ä¿®å¤æˆæœ¬ã€å“åº”é•¿åº¦å’Œè¡Œä¸ºæ¨¡å¼ã€‚ç»“æœè¡¨æ˜ï¼ŒO1çš„ä¿®å¤èƒ½åŠ›è¶…è¿‡äº†ä¹‹å‰çš„GPTç³»åˆ—æ¨¡å‹ï¼ŒæˆåŠŸä¿®å¤äº†åŸºå‡†æµ‹è¯•ä¸­çš„æ‰€æœ‰40ä¸ªé”™è¯¯ã€‚æˆ‘ä»¬çš„å·¥ä½œå¯ä»¥ä¸ºè¿›ä¸€æ­¥æ·±å…¥ç ”ç©¶GPTç³»åˆ—æ¨¡å‹åœ¨APRä¸­çš„åº”ç”¨å¥ å®šåŸºç¡€ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2409.10033v3">PDF</a> Accepted to the 6th International Workshop on Automated Program   Repair (APR 2025)</p>
<p><strong>Summary</strong></p>
<p>LLMsåœ¨è‡ªåŠ¨ç¨‹åºä¿®å¤ï¼ˆAPRï¼‰ä¸­è¡¨ç°ä¼˜å¼‚ï¼ŒGPTç³»åˆ—æ¨¡å‹åœ¨APRé¢†åŸŸçš„è¡¨ç°å·²å¤„äºå‰æ²¿åœ°ä½ã€‚æœ¬ç ”ç©¶å¯¹æ¯”äº†ä¸åŒç‰ˆæœ¬GPTç³»åˆ—æ¨¡å‹åœ¨APRä¸­çš„æœ‰æ•ˆæ€§ï¼Œç»“æœæ˜¾ç¤ºO1çš„ä¿®å¤èƒ½åŠ›è¶…è¿‡äº†å…ˆå‰çš„GPTç³»åˆ—æ¨¡å‹ï¼ŒæˆåŠŸä¿®å¤äº†æ‰€æœ‰40ä¸ªåŸºå‡†æµ‹è¯•ä¸­çš„é”™è¯¯ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLMsåœ¨è‡ªåŠ¨ç¨‹åºä¿®å¤ï¼ˆAPRï¼‰ä¸­å±•ç°å‡ºæ˜¾è‘—çš„æœ‰æ•ˆæ€§ã€‚</li>
<li>GPTç³»åˆ—æ¨¡å‹æ˜¯APRé¢†åŸŸä¸­æœ€å¸¸ç”¨çš„æ¨¡å‹ä¹‹ä¸€ï¼Œå…¶æ€§èƒ½å·²è¾¾åˆ°å‰æ²¿æ°´å¹³ã€‚</li>
<li>ç›®å‰å¯¹äºä¸åŒç‰ˆæœ¬GPTç³»åˆ—æ¨¡å‹åœ¨APRä¸­çš„æœ‰æ•ˆæ€§å¯¹æ¯”ç ”ç©¶è¾ƒå°‘ã€‚</li>
<li>æœ¬ç ”ç©¶å¯¹æ¯”äº†GPTå®¶æ—ä¸åŒç‰ˆæœ¬ï¼ˆO1-previewã€O1-miniã€GPT-4oå’Œå†å²ç‰ˆChatGPTï¼‰åœ¨APRä¸­çš„è¡¨ç°ã€‚</li>
<li>é€šè¿‡å®è¯ç ”ç©¶å‘ç°ï¼ŒO1çš„ä¿®å¤èƒ½åŠ›è¶…è¿‡äº†å…ˆå‰çš„GPTç³»åˆ—æ¨¡å‹ã€‚</li>
<li>O1æˆåŠŸä¿®å¤äº†æ‰€æœ‰40ä¸ªåŸºå‡†æµ‹è¯•ä¸­çš„é”™è¯¯ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2409.10033">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-1d39095cf885a341c2366a859805519a.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-8628a52cda463ced199e51a4d414e9ad.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-60d6f0ed29c6dfba5a5a6e52af99c9fc.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-d031cc60f9cbbc7488ca7a9686307bba.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-d03d3d24fddfa70c83d86d84c6306011.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Knowledge-Tagging-with-Large-Language-Model-based-Multi-Agent-System"><a href="#Knowledge-Tagging-with-Large-Language-Model-based-Multi-Agent-System" class="headerlink" title="Knowledge Tagging with Large Language Model based Multi-Agent System"></a>Knowledge Tagging with Large Language Model based Multi-Agent System</h2><p><strong>Authors:Hang Li, Tianlong Xu, Ethan Chang, Qingsong Wen</strong></p>
<p>Knowledge tagging for questions is vital in modern intelligent educational applications, including learning progress diagnosis, practice question recommendations, and course content organization. Traditionally, these annotations have been performed by pedagogical experts, as the task demands not only a deep semantic understanding of question stems and knowledge definitions but also a strong ability to link problem-solving logic with relevant knowledge concepts. With the advent of advanced natural language processing (NLP) algorithms, such as pre-trained language models and large language models (LLMs), pioneering studies have explored automating the knowledge tagging process using various machine learning models. In this paper, we investigate the use of a multi-agent system to address the limitations of previous algorithms, particularly in handling complex cases involving intricate knowledge definitions and strict numerical constraints. By demonstrating its superior performance on the publicly available math question knowledge tagging dataset, MathKnowCT, we highlight the significant potential of an LLM-based multi-agent system in overcoming the challenges that previous methods have encountered. Finally, through an in-depth discussion of the implications of automating knowledge tagging, we underscore the promising results of deploying LLM-based algorithms in educational contexts. </p>
<blockquote>
<p>çŸ¥è¯†æ ‡æ³¨åœ¨ç°ä»£æ™ºèƒ½æ•™è‚²åº”ç”¨ä¸­è‡³å…³é‡è¦ï¼ŒåŒ…æ‹¬å­¦ä¹ è¿›åº¦è¯Šæ–­ã€ç»ƒä¹ é¢˜æ¨èå’Œè¯¾ç¨‹å†…å®¹ç»„ç»‡ã€‚ä¼ ç»Ÿä¸Šï¼Œè¿™äº›æ³¨é‡Šç”±æ•™è‚²ä¸“å®¶å®Œæˆï¼Œå› ä¸ºè¿™é¡¹å·¥ä½œä¸ä»…éœ€è¦æ·±åº¦ç†è§£é—®é¢˜é¢˜å¹²å’ŒçŸ¥è¯†å®šä¹‰ï¼Œè¿˜éœ€è¦å¼ºå¤§çš„å°†é—®é¢˜è§£å†³é€»è¾‘ä¸ç›¸å…³çŸ¥è¯†æ¦‚å¿µè”ç³»èµ·æ¥çš„èƒ½åŠ›ã€‚éšç€å…ˆè¿›çš„è‡ªç„¶è¯­è¨€å¤„ç†ï¼ˆNLPï¼‰ç®—æ³•çš„å…´èµ·ï¼Œå¦‚é¢„è®­ç»ƒè¯­è¨€æ¨¡å‹å’Œå¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ï¼Œå¼€åˆ›æ€§ç ”ç©¶å·²ç»æ¢ç´¢ä½¿ç”¨å„ç§æœºå™¨å­¦ä¹ æ¨¡å‹è‡ªåŠ¨è¿›è¡ŒçŸ¥è¯†æ ‡æ³¨è¿‡ç¨‹ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬ç ”ç©¶äº†å¤šæ™ºèƒ½ä½“ç³»ç»Ÿåœ¨è§£å†³ä»¥å‰ç®—æ³•å±€é™æ€§æ–¹é¢çš„åº”ç”¨ï¼Œç‰¹åˆ«æ˜¯åœ¨å¤„ç†æ¶‰åŠå¤æ‚çŸ¥è¯†å®šä¹‰å’Œä¸¥æ ¼æ•°å€¼çº¦æŸçš„å¤æ‚æ¡ˆä¾‹æ–¹é¢ã€‚æˆ‘ä»¬åœ¨å…¬å…±å¯ç”¨çš„æ•°å­¦é¢˜ç›®çŸ¥è¯†æ ‡æ³¨æ•°æ®é›†MathKnowCTä¸Šå±•ç¤ºäº†å…¶å“è¶Šçš„æ€§èƒ½ï¼Œçªæ˜¾äº†åŸºäºLLMçš„å¤šæ™ºèƒ½ä½“ç³»ç»Ÿåœ¨å…‹æœä»¥å‰æ–¹æ³•æ‰€é‡åˆ°çš„æŒ‘æˆ˜æ–¹é¢çš„å·¨å¤§æ½œåŠ›ã€‚æœ€åï¼Œé€šè¿‡å¯¹çŸ¥è¯†æ ‡æ³¨è‡ªåŠ¨åŒ–çš„æ·±å…¥è®¨è®ºï¼Œæˆ‘ä»¬å¼ºè°ƒäº†åŸºäºLLMçš„ç®—æ³•åœ¨æ•™è‚²ç¯å¢ƒä¸­çš„ä»¤äººé¼“èˆçš„ç»“æœã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2409.08406v2">PDF</a> Accepted by AAAI 2025 (AAAI&#x2F;IAAI 2025 Innovative Application Award)</p>
<p><strong>æ€»ç»“</strong><br>åŸºäºå¤šä»£ç†ç³»ç»Ÿçš„çŸ¥è¯†æ ‡æ³¨ï¼Œå¯¹äºå¤„ç†å¤æ‚çš„æ ‡æ³¨éœ€æ±‚è¡¨ç°å‡ºä¼˜è¶Šæ€§èƒ½ã€‚è¯¥ç ”ç©¶æ¢ç´¢ä½¿ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å®ç°çŸ¥è¯†æ ‡æ³¨è‡ªåŠ¨åŒ–ï¼Œåœ¨å…¬å¼€çš„æ•°å­¦çŸ¥è¯†æ ‡æ³¨æ•°æ®é›†MathKnowCTä¸Šè¡¨ç°ä¼˜å¼‚ã€‚è¯¥ç ”ç©¶å¼ºè°ƒäº†è‡ªåŠ¨åŒ–çŸ¥è¯†æ ‡æ³¨åœ¨æ•™è‚²ç¯å¢ƒä¸­çš„æ½œåŠ›ã€‚éšç€å…ˆè¿›è‡ªç„¶è¯­è¨€å¤„ç†ç®—æ³•çš„å‘å±•ï¼ŒçŸ¥è¯†æ ‡æ³¨åœ¨æ™ºèƒ½æ•™è‚²åº”ç”¨ä¸­çš„éœ€æ±‚ä¸æ–­å¢å¼ºï¼Œå¯å¸®åŠ©æé«˜æ•™å­¦æ•ˆæœä¸å­¦ç”Ÿçš„å­¦ä¹ æ•ˆç‡ã€‚ç›®å‰æ•™è‚²é¢†åŸŸæ™®éè®¤å¯çŸ¥è¯†çš„ç»†è‡´åˆ’åˆ†ä¸ºå­¦ä¹ æ•ˆæœçš„åŸºç¡€æ”¯æ’‘ä¹‹ä¸€ã€‚ç»“åˆä»»åŠ¡ä¸é¢˜å‹è¿›è¡ŒçŸ¥è¯†åˆ†ç±»æ ‡æ³¨ï¼Œæœ‰åˆ©äºæå‡æ•™è‚²æ™ºèƒ½åŒ–æ°´å¹³ã€‚è‡ªåŠ¨åŒ–çŸ¥è¯†æ ‡æ³¨å¯åŠ©åŠ›è¯¾ç¨‹å†…å®¹çš„ç»„ç»‡å’Œè¯¾ç¨‹çŸ¥è¯†çš„ä¼˜åŒ–åˆ†é…ã€‚ä»¥å¾€äººå·¥å®Œæˆè¯¾ç¨‹çŸ¥è¯†åˆ’åˆ†æ•ˆç‡è¾ƒä½ä¸”æ ‡å‡†éš¾ä»¥ç»Ÿä¸€ã€‚é‡‡ç”¨å¤šä»£ç†ç³»ç»Ÿçš„è‡ªåŠ¨åŒ–æ ‡æ³¨æŠ€æœ¯å°†æå¤§åœ°æé«˜æ•™è‚²å†…å®¹çš„ç»„ç»‡å’ŒçŸ¥è¯†åˆ†é…çš„æ•ˆç‡å’Œå‡†ç¡®æ€§ã€‚åˆ©ç”¨æœºå™¨å­¦ä¹ ç®—æ³•ç»“åˆè‡ªé€‚åº”æ¨èæŠ€æœ¯å½¢æˆç”¨æˆ·ä¸“å±çš„å­¦ä¹ ç³»ç»Ÿå¯å¤§å¤§æé«˜æ•™è‚²å·¥ä½œçš„æ•ˆç‡å’Œä¾¿æ·æ€§ï¼Œä¹Ÿæ˜¯æ™ºèƒ½æ•™è‚²çš„ä¸€ä¸ªé‡è¦æ–¹å‘ã€‚å½“å‰æ•™è‚²é¢†åŸŸæ­£é¢ä¸´æ•°å­—åŒ–æ”¹é©çš„é‡è¦é˜¶æ®µï¼Œè¯¥æŠ€æœ¯çš„å‡ºç°å°†æå¤§åœ°æ¨åŠ¨æ•™è‚²è¡Œä¸šçš„æ™ºèƒ½åŒ–å‘å±•è¿›ç¨‹ã€‚éšç€æœºå™¨å­¦ä¹ æŠ€æœ¯ä¸æ–­åœ°æ¨é™ˆå‡ºæ–°ä¸äººå·¥æ™ºèƒ½äº§å“çš„å¿«é€Ÿå‘å±•å‡çº§ç­‰å¸‚åœºéœ€æ±‚ä¸æ–­å¢é•¿å…±åŒä¿ƒè¿›äº†æˆ‘å›½æ™ºèƒ½åŒ–æ°´å¹³çš„ä¸æ–­æé«˜è¿›ç¨‹å°†ä¼šæŒç»­åŠ é€Ÿæ™ºèƒ½åŒ–å‡çº§æµªæ½®çš„æ¥ä¸´ï¼Œæœ€ç»ˆå®ç°ä»¥æœºå™¨ç®—æ³•ä¸»å¯¼è‡ªé€‚åº”æ•™å­¦èµ‹èƒ½æ•™è‚²çš„ä¼Ÿå¤§ç›®æ ‡ã€‚å¯¹äºæ™ºèƒ½æ•™è‚²ç³»ç»Ÿçš„ç ”å‘å’Œæ¨å¹¿å…·æœ‰é‡è¦æ„ä¹‰ã€‚è¿™ä¸€å‘ç°ä¸ä»…å°†ä¿ƒè¿›æœºå™¨å­¦ä¹ å’Œäººå·¥æ™ºèƒ½åœ¨æ•™è‚²é¢†åŸŸçš„åº”ç”¨ï¼ŒåŒæ—¶ä¹Ÿä¸ºæˆ‘å›½æ™ºèƒ½æ•™è‚²çš„æœªæ¥å‘å±•æä¾›äº†å¼ºæœ‰åŠ›çš„æŠ€æœ¯æ”¯æ’‘ã€‚æ­¤å¤–ï¼Œè¿™ä¸€ç ”ç©¶è¿˜å…·æœ‰å¹¿é˜”çš„åº”ç”¨å‰æ™¯å’Œé‡è¦çš„ç¤¾ä¼šä»·å€¼å’Œç»æµä»·å€¼ã€‚æˆ‘ä»¬ç›¸ä¿¡è¿™ä¸€æŠ€æœ¯å°†ä¸ºæ™ºèƒ½æ•™è‚²å¸¦æ¥æ›´åŠ å¹¿é˜”çš„å‰æ™¯å’Œæ›´å¤šçš„å¯èƒ½æ€§ã€‚æ­¤å¤–è¯¥æŠ€æœ¯å¯¹äºè‡ªç„¶è¯­è¨€å¤„ç†é¢†åŸŸä¹Ÿå…·æœ‰é‡è¦çš„å¯ç¤ºä½œç”¨å¯¹äºæ•´ä¸ªæŠ€æœ¯çš„å‘å±•å…·æœ‰é‡å¤§çš„æ¨åŠ¨ä½œç”¨æ¨åŠ¨çŸ¥è¯†æ ‡æ³¨è‡ªåŠ¨åŒ–æŠ€æœ¯çš„è¿›æ­¥å’Œå‘å±•å¯¹äºæé«˜äººå·¥æ™ºèƒ½é¢†åŸŸæ•´ä½“çš„è¿›æ­¥å…·æœ‰é‡è¦æ„ä¹‰å’Œä½œç”¨éšç€çŸ¥è¯†çš„ä¸æ–­æ·±åŒ–å¯¹äºä¸“ä¸šé¢†åŸŸçš„å­¦ä¹ éœ€æ±‚å’Œä¹ æƒ¯è½¬å˜æ— ç–‘éœ€è¦æ»¡è¶³å·®å¼‚åŒ–çš„ç”¨æˆ·éœ€æ±‚å¤æ‚çš„çŸ¥è¯†ä½“ç³»æ„å»ºå’ŒçŸ¥è¯†åˆ†ç±»éœ€è¦æ›´åŠ ç²¾ç»†åŒ–çš„å¤„ç†ä»¥åŠæ›´å¼ºå¤§çš„æŠ€æœ¯æ”¯æ’‘æ‰èƒ½æ»¡è¶³å½“å‰ç¤¾ä¼šçš„éœ€æ±‚ã€‚å› æ­¤è¯¥æŠ€æœ¯çš„å‡ºç°ä¹Ÿè¿›ä¸€æ­¥å‡¸æ˜¾äº†å…¶åœ¨æœªæ¥äººå·¥æ™ºèƒ½é¢†åŸŸå‘å±•ä¸­çš„é‡è¦åœ°ä½ã€‚é€šè¿‡å¯¹è¿™ä¸€ç ”ç©¶çš„æ·±å…¥æ¢è®¨å’Œä¸æ–­å®è·µæˆ‘ä»¬æœ‰æœ›æ¨åŠ¨äººå·¥æ™ºèƒ½æŠ€æœ¯åœ¨æ•™è‚²é¢†åŸŸçš„å¹¿æ³›åº”ç”¨å’Œæ·±åº¦å‘å±•è¿›è€Œå®ç°æ™ºèƒ½åŒ–æ•™è‚²çš„ç¾å¥½æ„¿æ™¯ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>çŸ¥è¯†æ ‡æ³¨åœ¨ç°ä»£æ™ºèƒ½æ•™è‚²åº”ç”¨ä¸­è‡³å…³é‡è¦ï¼ŒåŒ…æ‹¬å­¦ä¹ è¿›åº¦è¯Šæ–­ã€ç»ƒä¹ é¢˜æ¨èå’Œè¯¾ç¨‹å†…å®¹ç»„ç»‡ç­‰æ–¹é¢ã€‚</li>
<li>ä¼ ç»ŸçŸ¥è¯†æ ‡æ³¨ç”±æ•™è‚²ä¸“å®¶è¿›è¡Œï¼Œä»»åŠ¡éœ€æ±‚å¤æ‚ï¼ŒåŒ…æ‹¬æ·±åº¦è¯­ä¹‰ç†è§£å’ŒçŸ¥è¯†å®šä¹‰è”ç³»é—®é¢˜è§£å†³é€»è¾‘ç­‰èƒ½åŠ›ã€‚</li>
<li>éšç€NLPç®—æ³•çš„å‘å±•ï¼Œå°¤å…¶æ˜¯é¢„è®­ç»ƒè¯­è¨€æ¨¡å‹å’Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ï¼Œæœ‰è¶Šæ¥è¶Šå¤šçš„ç ”ç©¶å°è¯•è‡ªåŠ¨åŒ–çŸ¥è¯†æ ‡æ³¨è¿‡ç¨‹ã€‚</li>
<li>å¤šä»£ç†ç³»ç»Ÿåœ¨å¤„ç†å¤æ‚çŸ¥è¯†æ ‡æ³¨é—®é¢˜ä¸Šè¡¨ç°å‡ºä¼˜è¶Šæ€§èƒ½ï¼Œç‰¹åˆ«æ˜¯åœ¨å¤„ç†å¤æ‚æ¡ˆä¾‹å’Œä¸¥æ ¼æ•°å€¼çº¦æŸæ–¹é¢ã€‚</li>
<li>åœ¨å…¬å¼€çš„æ•°å­¦çŸ¥è¯†æ ‡æ³¨æ•°æ®é›†ä¸Šçš„è¡¨ç°éªŒè¯äº†LLMåŸºäºå¤šä»£ç†ç³»ç»Ÿçš„æ½œåŠ›ã€‚</li>
<li>è‡ªåŠ¨åŒ–çŸ¥è¯†æ ‡æ³¨èƒ½æé«˜æ•™è‚²å†…å®¹ç»„ç»‡å’ŒçŸ¥è¯†åˆ†é…çš„æ•ˆç‡å’Œå‡†ç¡®æ€§ï¼Œæœ‰åŠ©äºæ¨åŠ¨æ•™è‚²è¡Œä¸šçš„æ™ºèƒ½åŒ–å‘å±•è¿›ç¨‹ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2409.08406">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-5eab2143f156147757b3b70e7d058e8a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-dec7e2bef41052c1d6d24f7e02431b61.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-e52f299d172a0c3ea8966cf353feefcd.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-932a206f18cd6babf7270cf52c6086fc.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-34f28fb558b8b866ab6d14b7529929d7.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-935372e271cf710d2cee66f2f7cfaec9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ce7a54548c75bd2522dec9b935412922.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="LLMs-as-Zero-shot-Graph-Learners-Alignment-of-GNN-Representations-with-LLM-Token-Embeddings"><a href="#LLMs-as-Zero-shot-Graph-Learners-Alignment-of-GNN-Representations-with-LLM-Token-Embeddings" class="headerlink" title="LLMs as Zero-shot Graph Learners: Alignment of GNN Representations with   LLM Token Embeddings"></a>LLMs as Zero-shot Graph Learners: Alignment of GNN Representations with   LLM Token Embeddings</h2><p><strong>Authors:Duo Wang, Yuan Zuo, Fengzhi Li, Junjie Wu</strong></p>
<p>Zero-shot graph machine learning, especially with graph neural networks (GNNs), has garnered significant interest due to the challenge of scarce labeled data. While methods like self-supervised learning and graph prompt learning have been extensively explored, they often rely on fine-tuning with task-specific labels, limiting their effectiveness in zero-shot scenarios. Inspired by the zero-shot capabilities of instruction-fine-tuned large language models (LLMs), we introduce a novel framework named Token Embedding-Aligned Graph Language Model (TEA-GLM) that leverages LLMs as cross-dataset and cross-task zero-shot learners for graph machine learning. Concretely, we pretrain a GNN, aligning its representations with token embeddings of an LLM. We then train a linear projector that transforms the GNNâ€™s representations into a fixed number of graph token embeddings without tuning the LLM. A unified instruction is designed for various graph tasks at different levels, such as node classification (node-level) and link prediction (edge-level). These design choices collectively enhance our methodâ€™s effectiveness in zero-shot learning, setting it apart from existing methods. Experiments show that our graph token embeddings help the LLM predictor achieve state-of-the-art performance on unseen datasets and tasks compared to other methods using LLMs as predictors. </p>
<blockquote>
<p>é›¶æ ·æœ¬å›¾æœºå™¨å­¦ä¹ ï¼Œç‰¹åˆ«æ˜¯ä¸å›¾ç¥ç»ç½‘ç»œï¼ˆGNNï¼‰ç›¸ç»“åˆï¼Œç”±äºæ ‡æ³¨æ•°æ®ç¨€ç¼ºçš„æŒ‘æˆ˜è€Œå¤‡å—å…³æ³¨ã€‚è™½ç„¶è‡ªç›‘ç£å­¦ä¹ å’Œå›¾æç¤ºå­¦ä¹ ç­‰æ–¹æ³•å·²ç»å¾—åˆ°äº†å¹¿æ³›çš„ç ”ç©¶ï¼Œä½†å®ƒä»¬é€šå¸¸ä¾èµ–äºç‰¹å®šä»»åŠ¡çš„æ ‡ç­¾è¿›è¡Œå¾®è°ƒï¼Œä»è€Œåœ¨é›¶æ ·æœ¬åœºæ™¯ä¸­é™åˆ¶äº†å…¶æœ‰æ•ˆæ€§ã€‚å—æŒ‡ä»¤å¾®è°ƒå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰é›¶æ ·æœ¬èƒ½åŠ›çš„å¯å‘ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§æ–°çš„æ¡†æ¶ï¼Œåä¸ºToken Embedding-Aligned Graph Language Modelï¼ˆTEA-GLMï¼‰ï¼Œè¯¥æ¡†æ¶åˆ©ç”¨LLMä½œä¸ºè·¨æ•°æ®é›†å’Œè·¨ä»»åŠ¡çš„é›¶æ ·æœ¬å­¦ä¹ è€…è¿›è¡Œå›¾æœºå™¨å­¦ä¹ ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬å¯¹GNNè¿›è¡Œé¢„è®­ç»ƒï¼Œå°†å…¶è¡¨ç¤ºä¸LLMçš„ä»¤ç‰ŒåµŒå…¥è¿›è¡Œå¯¹é½ã€‚ç„¶åï¼Œæˆ‘ä»¬è®­ç»ƒä¸€ä¸ªçº¿æ€§æŠ•å½±ä»ªï¼Œå°†GNNçš„è¡¨ç¤ºè½¬æ¢ä¸ºå›ºå®šæ•°é‡çš„å›¾ä»¤ç‰ŒåµŒå…¥ï¼Œè€Œæ— éœ€è°ƒæ•´LLMã€‚ä¸ºä¸åŒå±‚æ¬¡çš„å„ç§å›¾å½¢ä»»åŠ¡è®¾è®¡äº†ä¸€ä¸ªç»Ÿä¸€çš„æŒ‡ä»¤ï¼Œå¦‚èŠ‚ç‚¹åˆ†ç±»ï¼ˆèŠ‚ç‚¹çº§ï¼‰å’Œé“¾æ¥é¢„æµ‹ï¼ˆè¾¹ç¼˜çº§ï¼‰ã€‚è¿™äº›è®¾è®¡é€‰æ‹©å…±åŒæé«˜äº†æˆ‘ä»¬çš„æ–¹æ³•åœ¨é›¶æ ·æœ¬å­¦ä¹ ä¸­çš„æœ‰æ•ˆæ€§ï¼Œä½¿å…¶ä¸ç°æœ‰æ–¹æ³•æœ‰æ‰€ä¸åŒã€‚å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„å›¾ä»¤ç‰ŒåµŒå…¥æœ‰åŠ©äºLLMé¢„æµ‹å™¨åœ¨æœªè§è¿‡çš„æ•°æ®é›†å’Œä»»åŠ¡ä¸Šå®ç°æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2408.14512v3">PDF</a> </p>
<p><strong>Summary</strong><br>     é›¶æ ·æœ¬å›¾æœºå™¨å­¦ä¹ ï¼Œç‰¹åˆ«æ˜¯åŸºäºå›¾ç¥ç»ç½‘ç»œï¼ˆGNNsï¼‰çš„æ–¹æ³•ï¼Œå› ç¼ºä¹æ ‡æ³¨æ•°æ®è€Œå¤‡å—å…³æ³¨ã€‚å°½ç®¡è‡ªç›‘ç£å­¦ä¹ å’Œå›¾æç¤ºå­¦ä¹ ç­‰æ–¹æ³•å·²è¢«å¹¿æ³›æ¢ç´¢ï¼Œä½†å®ƒä»¬é€šå¸¸ä¾èµ–äºç‰¹å®šä»»åŠ¡çš„æ ‡ç­¾è¿›è¡Œå¾®è°ƒï¼Œè¿™åœ¨é›¶æ ·æœ¬åœºæ™¯ä¸­é™åˆ¶äº†å…¶æœ‰æ•ˆæ€§ã€‚å—æŒ‡ä»¤å¾®è°ƒå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰é›¶æ ·æœ¬èƒ½åŠ›çš„å¯å‘ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åä¸ºToken Embedding-Aligned Graph Language Modelï¼ˆTEA-GLMï¼‰çš„æ–°å‹æ¡†æ¶ï¼Œè¯¥æ¡†æ¶åˆ©ç”¨LLMä½œä¸ºè·¨æ•°æ®é›†å’Œè·¨ä»»åŠ¡çš„é›¶æ ·æœ¬å­¦ä¹ è€…æ¥è¿›è¡Œå›¾æœºå™¨å­¦ä¹ ã€‚æˆ‘ä»¬é€šè¿‡å°†GNNçš„è¡¨ç¤ºä¸LLMçš„ä»¤ç‰ŒåµŒå…¥è¿›è¡Œå¯¹é½æ¥é¢„è®­ç»ƒå®ƒï¼Œç„¶åè®­ç»ƒä¸€ä¸ªçº¿æ€§æŠ•å½±ä»ªï¼Œå°†GNNçš„è¡¨ç¤ºè½¬æ¢ä¸ºå›ºå®šæ•°é‡çš„å›¾ä»¤ç‰ŒåµŒå…¥ï¼Œè€Œæ— éœ€è°ƒæ•´LLMã€‚ä¸ºå„çº§å›¾å½¢ä»»åŠ¡è®¾è®¡äº†ç»Ÿä¸€çš„æŒ‡ä»¤ï¼Œå¦‚èŠ‚ç‚¹åˆ†ç±»ï¼ˆèŠ‚ç‚¹çº§ï¼‰å’Œé“¾æ¥é¢„æµ‹ï¼ˆè¾¹ç¼˜çº§ï¼‰ã€‚è¿™äº›è®¾è®¡é€‰æ‹©å…±åŒæé«˜äº†è¯¥æ–¹æ³•åœ¨é›¶æ ·æœ¬å­¦ä¹ ä¸­çš„æœ‰æ•ˆæ€§ï¼Œä½¿å…¶æœ‰åˆ«äºç°æœ‰æ–¹æ³•ã€‚å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„å›¾ä»¤ç‰ŒåµŒå…¥æœ‰åŠ©äºLLMé¢„æµ‹å™¨åœ¨æœªè§è¿‡çš„æ•°æ®é›†å’Œä»»åŠ¡ä¸Šå®ç°æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>é›¶æ ·æœ¬å›¾æœºå™¨å­¦ä¹ é¢ä¸´ç¼ºä¹æ ‡æ³¨æ•°æ®çš„æŒ‘æˆ˜ã€‚</li>
<li>ç°æœ‰æ–¹æ³•å¦‚è‡ªç›‘ç£å­¦ä¹ å’Œå›¾æç¤ºå­¦ä¹ é€šå¸¸åœ¨é›¶æ ·æœ¬åœºæ™¯ä¸­æœ‰æ•ˆæ€§å—é™ã€‚</li>
<li>å—LLMé›¶æ ·æœ¬èƒ½åŠ›çš„å¯å‘ï¼Œæå‡ºTEA-GLMæ¡†æ¶ã€‚</li>
<li>TEA-GLMé€šè¿‡å°†GNNä¸LLMç»“åˆï¼Œåˆ©ç”¨LLMä½œä¸ºè·¨æ•°æ®é›†å’Œè·¨ä»»åŠ¡çš„é›¶æ ·æœ¬å­¦ä¹ è€…ã€‚</li>
<li>é€šè¿‡é¢„è®­ç»ƒGNNå¹¶ä¸LLMçš„ä»¤ç‰ŒåµŒå…¥å¯¹é½ï¼Œå†è®­ç»ƒçº¿æ€§æŠ•å½±ä»ªå°†GNNè¡¨ç¤ºè½¬æ¢ä¸ºå›¾ä»¤ç‰ŒåµŒå…¥ã€‚</li>
<li>è®¾è®¡äº†å„çº§å›¾å½¢ä»»åŠ¡çš„ç»Ÿä¸€æŒ‡ä»¤ï¼Œå¦‚èŠ‚ç‚¹åˆ†ç±»å’Œé“¾æ¥é¢„æµ‹ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2408.14512">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-4e43927c2468841971e19231d38dc2e4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-44c5fec94c8a4b995e8a7882c3088b8e.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="GARLIC-GPT-Augmented-Reinforcement-Learning-with-Intelligent-Control-for-Vehicle-Dispatching"><a href="#GARLIC-GPT-Augmented-Reinforcement-Learning-with-Intelligent-Control-for-Vehicle-Dispatching" class="headerlink" title="GARLIC: GPT-Augmented Reinforcement Learning with Intelligent Control   for Vehicle Dispatching"></a>GARLIC: GPT-Augmented Reinforcement Learning with Intelligent Control   for Vehicle Dispatching</h2><p><strong>Authors:Xiao Han, Zijian Zhang, Xiangyu Zhao, Yuanshao Zhu, Guojiang Shen, Xiangjie Kong, Xuetao Wei, Liqiang Nie, Jieping Ye</strong></p>
<p>As urban residents demand higher travel quality, vehicle dispatch has become a critical component of online ride-hailing services. However, current vehicle dispatch systems struggle to navigate the complexities of urban traffic dynamics, including unpredictable traffic conditions, diverse driver behaviors, and fluctuating supply and demand patterns. These challenges have resulted in travel difficulties for passengers in certain areas, while many drivers in other areas are unable to secure orders, leading to a decline in the overall quality of urban transportation services. To address these issues, this paper introduces GARLIC: a framework of GPT-Augmented Reinforcement Learning with Intelligent Control for vehicle dispatching. GARLIC utilizes multiview graphs to capture hierarchical traffic states, and learns a dynamic reward function that accounts for individual driving behaviors. The framework further integrates a GPT model trained with a custom loss function to enable high-precision predictions and optimize dispatching policies in real-world scenarios. Experiments conducted on two real-world datasets demonstrate that GARLIC effectively aligns with driver behaviors while reducing the empty load rate of vehicles. </p>
<blockquote>
<p>éšç€åŸå¸‚å±…æ°‘å¯¹æ—…è¡Œå“è´¨çš„è¦æ±‚è¶Šæ¥è¶Šé«˜ï¼Œè½¦è¾†è°ƒåº¦å·²æˆä¸ºåœ¨çº¿å«è½¦æœåŠ¡çš„é‡è¦ç»„æˆéƒ¨åˆ†ã€‚ç„¶è€Œï¼Œå½“å‰è½¦è¾†è°ƒåº¦ç³»ç»Ÿåœ¨åº”å¯¹åŸå¸‚äº¤é€šåŠ¨æ€çš„å¤æ‚æ€§æ–¹é¢å­˜åœ¨å›°éš¾ï¼ŒåŒ…æ‹¬ä¸å¯é¢„æµ‹çš„äº¤é€šçŠ¶å†µã€å¸æœºè¡Œä¸ºçš„å¤šæ ·æ€§ä»¥åŠä¾›éœ€æ¨¡å¼çš„æ³¢åŠ¨ã€‚è¿™äº›æŒ‘æˆ˜å¯¼è‡´æŸäº›åœ°åŒºçš„ä¹˜å®¢å‡ºè¡Œå›°éš¾ï¼Œè€Œå…¶ä»–åœ°åŒºçš„è®¸å¤šå¸æœºæ— æ³•æ¥åˆ°è®¢å•ï¼Œä»è€Œé™ä½äº†åŸå¸‚äº¤é€šè¿è¾“æœåŠ¡çš„æ•´ä½“è´¨é‡ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæœ¬æ–‡ä»‹ç»äº†GARLICï¼šä¸€ç§ç”¨äºè½¦è¾†è°ƒåº¦çš„GPTå¢å¼ºå‹å¼ºåŒ–å­¦ä¹ æ™ºèƒ½æ§åˆ¶æ¡†æ¶ã€‚GARLICåˆ©ç”¨å¤šè§†å›¾å›¾æ¥æ•è·åˆ†å±‚äº¤é€šçŠ¶æ€ï¼Œå¹¶å­¦ä¹ ä¸€ä¸ªè€ƒè™‘ä¸ªä½“é©¾é©¶è¡Œä¸ºçš„åŠ¨æ€å¥–åŠ±å‡½æ•°ã€‚è¯¥æ¡†æ¶è¿›ä¸€æ­¥é›†æˆäº†ä¸€ä¸ªä½¿ç”¨è‡ªå®šä¹‰æŸå¤±å‡½æ•°è®­ç»ƒçš„GPTæ¨¡å‹ï¼Œä»¥å®ç°é«˜ç²¾åº¦é¢„æµ‹ï¼Œå¹¶åœ¨ç°å®åœºæ™¯ä¸­ä¼˜åŒ–è°ƒåº¦ç­–ç•¥ã€‚åœ¨ä¸¤ä¸ªçœŸå®æ•°æ®é›†ä¸Šè¿›è¡Œçš„å®éªŒè¡¨æ˜ï¼ŒGARLICæœ‰æ•ˆåœ°é€‚åº”äº†é©¾é©¶å‘˜è¡Œä¸ºï¼Œå¹¶é™ä½äº†è½¦è¾†çš„ç©ºé©¶ç‡ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2408.10286v3">PDF</a> Accepted by AAAI 2025</p>
<p><strong>Summary</strong></p>
<p>éšç€åŸå¸‚å±…æ°‘å¯¹æ—…è¡Œå“è´¨è¦æ±‚çš„æé«˜ï¼Œè½¦è¾†è°ƒåº¦å·²æˆä¸ºåœ¨çº¿æ‹¼è½¦æœåŠ¡çš„é‡è¦ç»„æˆéƒ¨åˆ†ã€‚ç„¶è€Œï¼Œå½“å‰è½¦è¾†è°ƒåº¦ç³»ç»Ÿåœ¨åº”å¯¹åŸå¸‚äº¤é€šçš„å¤æ‚æ€§æ–¹é¢é¢ä¸´æŒ‘æˆ˜ï¼Œå¦‚ä¸å¯é¢„æµ‹çš„äº¤é€šçŠ¶å†µã€å¸æœºè¡Œä¸ºçš„å¤šæ ·æ€§ä»¥åŠä¾›éœ€æ¨¡å¼çš„æ³¢åŠ¨ã€‚ä¸ºè§£å†³è¿™äº›é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†GARLICæ¡†æ¶ï¼Œè¯¥æ¡†æ¶åˆ©ç”¨GPTå¢å¼ºå‹å¼ºåŒ–å­¦ä¹ æ™ºèƒ½æ§åˆ¶è½¦è¾†è°ƒåº¦ã€‚é€šè¿‡å¤šè§†å›¾å›¾æ•æ‰åˆ†å±‚äº¤é€šçŠ¶æ€ï¼Œå­¦ä¹ åŠ¨æ€å¥–åŠ±å‡½æ•°ä»¥è€ƒè™‘ä¸ªä½“é©¾é©¶è¡Œä¸ºï¼Œå¹¶é›†æˆGPTæ¨¡å‹è¿›è¡Œé«˜ç²¾åº¦é¢„æµ‹å’Œä¼˜åŒ–å®é™…åœºæ™¯ä¸­çš„è°ƒåº¦ç­–ç•¥ã€‚å®éªŒè¯æ˜ï¼ŒGARLICæ¡†æ¶èƒ½æœ‰æ•ˆç¬¦åˆå¸æœºè¡Œä¸ºå¹¶é™ä½è½¦è¾†ç©ºé©¶ç‡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>åŸå¸‚å±…æ°‘å¯¹æ—…è¡Œå“è´¨çš„è¦æ±‚æé«˜ï¼Œè½¦è¾†è°ƒåº¦æˆä¸ºåœ¨çº¿æ‹¼è½¦æœåŠ¡çš„æ ¸å¿ƒç»„æˆéƒ¨åˆ†ã€‚</li>
<li>å½“å‰è½¦è¾†è°ƒåº¦ç³»ç»Ÿé¢ä¸´åŸå¸‚äº¤é€šå¤æ‚æ€§çš„æŒ‘æˆ˜ï¼ŒåŒ…æ‹¬ä¸å¯é¢„æµ‹çš„äº¤é€šçŠ¶å†µã€å¸æœºè¡Œä¸ºçš„å¤šæ ·æ€§ä»¥åŠä¾›éœ€æ³¢åŠ¨ã€‚</li>
<li>GARLICæ¡†æ¶ç»“åˆäº†GPTå¢å¼ºå‹å¼ºåŒ–å­¦ä¹ æ™ºèƒ½æ§åˆ¶ï¼Œç”¨äºä¼˜åŒ–è½¦è¾†è°ƒåº¦ã€‚</li>
<li>å¤šè§†å›¾å›¾è¢«ç”¨æ¥æ•æ‰åˆ†å±‚äº¤é€šçŠ¶æ€ï¼Œå¹¶å­¦ä¹ è€ƒè™‘ä¸ªä½“é©¾é©¶è¡Œä¸ºçš„åŠ¨æ€å¥–åŠ±å‡½æ•°ã€‚</li>
<li>GPTæ¨¡å‹çš„é›†æˆä½¿å¾—é¢„æµ‹æ›´ä¸ºç²¾ç¡®ï¼Œä¼˜åŒ–äº†å®é™…åœºæ™¯ä¸­çš„è°ƒåº¦ç­–ç•¥ã€‚</li>
<li>å®éªŒè¯æ˜ï¼ŒGARLICæ¡†æ¶èƒ½æœ‰æ•ˆç¬¦åˆå¸æœºè¡Œä¸ºã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2408.10286">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-a9a8d3d728d9bde8f1fc025d2dca1969.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-8a67b07eff9e86cf7ad79e0515f87689.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9c916965f5fbd5309521e7ac64c92803.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-5de92de7b44e53d7f3e01889ae488bea.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="GPT-4V-Cannot-Generate-Radiology-Reports-Yet"><a href="#GPT-4V-Cannot-Generate-Radiology-Reports-Yet" class="headerlink" title="GPT-4V Cannot Generate Radiology Reports Yet"></a>GPT-4V Cannot Generate Radiology Reports Yet</h2><p><strong>Authors:Yuyang Jiang, Chacha Chen, Dang Nguyen, Benjamin M. Mervak, Chenhao Tan</strong></p>
<p>GPT-4Vâ€™s purported strong multimodal abilities raise interests in using it to automate radiology report writing, but there lacks thorough evaluations. In this work, we perform a systematic evaluation of GPT-4V in generating radiology reports on two chest X-ray report datasets: MIMIC-CXR and IU X-Ray. We attempt to directly generate reports using GPT-4V through different prompting strategies and find that it fails terribly in both lexical metrics and clinical efficacy metrics. To understand the low performance, we decompose the task into two steps: 1) the medical image reasoning step of predicting medical condition labels from images; and 2) the report synthesis step of generating reports from (groundtruth) conditions. We show that GPT-4Vâ€™s performance in image reasoning is consistently low across different prompts. In fact, the distributions of model-predicted labels remain constant regardless of which groundtruth conditions are present on the image, suggesting that the model is not interpreting chest X-rays meaningfully. Even when given groundtruth conditions in report synthesis, its generated reports are less correct and less natural-sounding than a finetuned LLaMA-2. Altogether, our findings cast doubt on the viability of using GPT-4V in a radiology workflow. </p>
<blockquote>
<p>GPT-4Vå£°ç§°çš„å¼ºå¤§å¤šæ¨¡æ€èƒ½åŠ›å¼•å‘äº†å¯¹å…¶ç”¨äºè‡ªåŠ¨åŒ–æ”¾å°„å­¦æŠ¥å‘Šå†™ä½œçš„å…´è¶£ï¼Œä½†ç›®å‰ç¼ºä¹å…¨é¢è¯„ä¼°ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬å¯¹GPT-4Våœ¨MIMIC-CXRå’ŒIU X-Rayä¸¤ä¸ªèƒ¸éƒ¨Xå…‰æŠ¥å‘Šæ•°æ®é›†ä¸Šç”Ÿæˆæ”¾å°„å­¦æŠ¥å‘Šçš„èƒ½åŠ›è¿›è¡Œäº†ç³»ç»Ÿè¯„ä¼°ã€‚æˆ‘ä»¬å°è¯•é€šè¿‡ä¸åŒçš„æç¤ºç­–ç•¥ç›´æ¥ä½¿ç”¨GPT-4Vç”ŸæˆæŠ¥å‘Šï¼Œå¹¶å‘ç°å®ƒåœ¨è¯æ±‡æŒ‡æ ‡å’Œä¸´åºŠæœ‰æ•ˆæ€§æŒ‡æ ‡æ–¹é¢éƒ½è¡¨ç°ç³Ÿç³•ã€‚ä¸ºäº†äº†è§£æ€§èƒ½ä¸ä½³çš„åŸå› ï¼Œæˆ‘ä»¬å°†ä»»åŠ¡åˆ†è§£ä¸ºä¸¤ä¸ªæ­¥éª¤ï¼š1ï¼‰åŒ»å­¦å›¾åƒæ¨ç†æ­¥éª¤ï¼Œå³ä»å›¾åƒä¸­é¢„æµ‹åŒ»å­¦çŠ¶å†µæ ‡ç­¾ï¼›2ï¼‰æŠ¥å‘Šåˆæˆæ­¥éª¤ï¼Œå³æ ¹æ®ï¼ˆçœŸå®ï¼‰çŠ¶å†µç”ŸæˆæŠ¥å‘Šã€‚æˆ‘ä»¬è¡¨æ˜ï¼ŒGPT-4Våœ¨ä¸åŒæç¤ºä¸­çš„å›¾åƒæ¨ç†æ€§èƒ½å§‹ç»ˆè¾ƒä½ã€‚äº‹å®ä¸Šï¼Œæ— è®ºå›¾åƒä¸Šå­˜åœ¨å“ªäº›çœŸå®çŠ¶å†µï¼Œæ¨¡å‹é¢„æµ‹çš„æ ‡ç­¾åˆ†å¸ƒéƒ½ä¿æŒä¸å˜ï¼Œè¿™è¡¨æ˜æ¨¡å‹å¹¶æ²¡æœ‰å¯¹èƒ¸éƒ¨Xå…‰è¿›è¡Œæœ‰æ„ä¹‰çš„è§£è¯»ã€‚å³ä½¿åœ¨æŠ¥å‘Šåˆæˆæ—¶ç»™å®šçœŸå®çŠ¶å†µï¼Œå…¶ç”Ÿæˆçš„æŠ¥å‘Šä¹Ÿæ¯”å¾®è°ƒåçš„LLaMA-2æ›´æ­£å’Œæ›´è‡ªç„¶æµç•…ã€‚æ€»ä½“è€Œè¨€ï¼Œæˆ‘ä»¬çš„ç ”ç©¶ç»“æœå¯¹åœ¨æ”¾å°„å­¦å·¥ä½œæµç¨‹ä¸­ä½¿ç”¨GPT-4Vçš„å¯è¡Œæ€§è¡¨ç¤ºæ€€ç–‘ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2407.12176v4">PDF</a> 24 pages, 3 figures, code:   <a target="_blank" rel="noopener" href="https://github.com/ChicagoHAI/cxr-eval-gpt-4v">https://github.com/ChicagoHAI/cxr-eval-gpt-4v</a> Findings paper presented at   Machine Learning for Health (ML4H) symposium 2024, December 15-16, 2024,   Vancouver, Canada, 26 pages</p>
<p><strong>Summary</strong></p>
<p>GPT-4Våœ¨å¤šæ¨¡æ€èƒ½åŠ›æ–¹é¢å¤‡å—å…³æ³¨ï¼Œå°¤å…¶åœ¨è‡ªåŠ¨åŒ–ç”Ÿæˆæ”¾å°„å­¦æŠ¥å‘Šæ–¹é¢çš„æ½œåŠ›ã€‚ç„¶è€Œï¼Œæœ¬ç ”ç©¶å¯¹GPT-4Våœ¨ç”Ÿæˆæ”¾å°„å­¦æŠ¥å‘Šæ–¹é¢çš„è¡¨ç°è¿›è¡Œäº†ç³»ç»Ÿè¯„ä¼°ï¼Œå‘ç°å…¶åœ¨è¯æ±‡å’Œä¸´åºŠæŒ‡æ ‡ä¸Šçš„è¡¨ç°å‡ä¸ä½³ã€‚GPT-4Våœ¨å›¾åƒæ¨ç†å’ŒæŠ¥å‘Šç”Ÿæˆä¸¤ä¸ªæ­¥éª¤ä¸­çš„è¡¨ç°å‡ä¸å°½å¦‚äººæ„ï¼Œæ— æ³•æœ‰æ•ˆè§£è¯»èƒ¸éƒ¨Xå…‰ç‰‡ä¿¡æ¯ã€‚å› æ­¤ï¼Œæœ¬ç ”ç©¶å¯¹GPT-4Våœ¨æ”¾å°„å­¦å·¥ä½œæµç¨‹ä¸­çš„ä½¿ç”¨æå‡ºäº†è´¨ç–‘ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>GPT-4Våœ¨å¤šæ¨¡æ€èƒ½åŠ›ä¸Šå…·æœ‰æ½œåœ¨ä»·å€¼ï¼Œå°¤å…¶æ˜¯åœ¨æ”¾å°„å­¦æŠ¥å‘Šè‡ªåŠ¨åŒ–æ–¹é¢ã€‚</li>
<li>åœ¨æœ¬ç ”ç©¶ä¸­ï¼ŒGPT-4Våœ¨ç”Ÿæˆæ”¾å°„å­¦æŠ¥å‘Šæ–¹é¢çš„è¡¨ç°ä¸ä½³ï¼Œæ— è®ºåœ¨è¯æ±‡æŒ‡æ ‡è¿˜æ˜¯ä¸´åºŠæŒ‡æ ‡ä¸Šå‡æœªè¾¾åˆ°é¢„æœŸæ•ˆæœã€‚</li>
<li>GPT-4Våœ¨å›¾åƒæ¨ç†æ–¹é¢çš„è¡¨ç°æŒç»­ä½ä¸‹ï¼Œæ— æ³•æœ‰æ•ˆè§£è¯»èƒ¸éƒ¨Xå…‰ç‰‡ä¿¡æ¯ã€‚</li>
<li>GPT-4Våœ¨æŠ¥å‘Šç”Ÿæˆæ­¥éª¤ä¸­çš„è¡¨ç°ä¸å¦‚ç»è¿‡å¾®è°ƒåçš„LLaMA-2æ¨¡å‹ç”Ÿæˆçš„æŠ¥å‘Šå‡†ç¡®å’Œè‡ªç„¶ã€‚</li>
<li>GPT-4Væ¨¡å‹é¢„æµ‹æ ‡ç­¾çš„åˆ†å¸ƒä¸å›¾åƒä¸Šçš„å®é™…ç—…æƒ…æ— å…³ï¼Œè¡¨æ˜æ¨¡å‹æœªæœ‰æ•ˆè§£è¯»èƒ¸éƒ¨Xå…‰ç‰‡å†…å®¹ã€‚</li>
<li>ä½¿ç”¨GPT-4Våœ¨æ”¾å°„å­¦å·¥ä½œæµç¨‹ä¸­çš„å¯è¡Œæ€§å€¼å¾—æ€€ç–‘ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2407.12176">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-77e468256fea0d986df0238b3f1e1225.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-98ced62d8a096760282aeea85785bdbd.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4f7497912d2885aa67f0a7cb5e1e58f9.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-79e6a04602839cad48ca4177281385d3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a594d2e7c4b67e8b9ed0b28724b0e870.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-373556bb560c33171aff455f0783c75c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-8639b2fba9555d219d28320ba878742d.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="TrackFormers-In-Search-of-Transformer-Based-Particle-Tracking-for-the-High-Luminosity-LHC-Era"><a href="#TrackFormers-In-Search-of-Transformer-Based-Particle-Tracking-for-the-High-Luminosity-LHC-Era" class="headerlink" title="TrackFormers: In Search of Transformer-Based Particle Tracking for the   High-Luminosity LHC Era"></a>TrackFormers: In Search of Transformer-Based Particle Tracking for the   High-Luminosity LHC Era</h2><p><strong>Authors:Sascha Caron, Nadezhda Dobreva, Antonio Ferrer SÃ¡nchez, JosÃ© D. MartÃ­n-Guerrero, Uraz Odyurt, Roberto Ruiz de Austri Bazan, Zef Wolffs, Yue Zhao</strong></p>
<p>High-Energy Physics experiments are facing a multi-fold data increase with every new iteration. This is certainly the case for the upcoming High-Luminosity LHC upgrade. Such increased data processing requirements forces revisions to almost every step of the data processing pipeline. One such step in need of an overhaul is the task of particle track reconstruction, a.k.a., tracking. A Machine Learning-assisted solution is expected to provide significant improvements, since the most time-consuming step in tracking is the assignment of hits to particles or track candidates. This is the topic of this paper.   We take inspiration from large language models. As such, we consider two approaches: the prediction of the next word in a sentence (next hit point in a track), as well as the one-shot prediction of all hits within an event. In an extensive design effort, we have experimented with three models based on the Transformer architecture and one model based on the U-Net architecture, performing track association predictions for collision event hit points. In our evaluation, we consider a spectrum of simple to complex representations of the problem, eliminating designs with lower metrics early on. We report extensive results, covering both prediction accuracy (score) and computational performance. We have made use of the REDVID simulation framework, as well as reductions applied to the TrackML data set, to compose five data sets from simple to complex, for our experiments. The results highlight distinct advantages among different designs in terms of prediction accuracy and computational performance, demonstrating the efficiency of our methodology. Most importantly, the results show the viability of a one-shot encoder-classifier based Transformer solution as a practical approach for the task of tracking. </p>
<blockquote>
<p>é«˜èƒ½ç‰©ç†å®éªŒé¢ä¸´ç€æ¯ä¸€æ¬¡è¿­ä»£éƒ½å‘ˆç°å¤šé‡æ•°æ®å¢é•¿çš„æƒ…å†µã€‚å³å°†åˆ°æ¥çš„é«˜äº®åº¦LHCå‡çº§ä¹Ÿæ˜¯å¦‚æ­¤ã€‚è¿™ç§å¢åŠ çš„æ•°æ®å¤„ç†è¦æ±‚å‡ ä¹éœ€è¦å¯¹æ•°æ®å¤„ç†çš„æ¯ä¸ªæ­¥éª¤è¿›è¡Œä¿®æ”¹ã€‚éœ€è¦è¿›è¡Œå…¨é¢æ£€ä¿®çš„ä¸€ä¸ªæ­¥éª¤æ˜¯ç²’å­è½¨è¿¹é‡å»ºçš„ä»»åŠ¡ï¼Œä¹Ÿè¢«ç§°ä¸ºè·Ÿè¸ªã€‚ç”±äºè·Ÿè¸ªè¿‡ç¨‹ä¸­æœ€è€—æ—¶çš„æ­¥éª¤æ˜¯å°†å‘½ä¸­ç‚¹åˆ†é…ç»™ç²’å­æˆ–è½¨è¿¹å€™é€‰è€…ï¼Œå› æ­¤é¢„æœŸæœºå™¨å­¦ä¹ è¾…åŠ©è§£å†³æ–¹æ¡ˆå°†å¸¦æ¥é‡å¤§æ”¹è¿›ã€‚è¿™æ˜¯æœ¬æ–‡çš„ä¸»é¢˜ã€‚æˆ‘ä»¬ä»å¤§å‹è¯­è¨€æ¨¡å‹ä¸­æ±²å–çµæ„Ÿã€‚å› æ­¤ï¼Œæˆ‘ä»¬è€ƒè™‘ä¸¤ç§æ–¹æ³•ï¼šé¢„æµ‹å¥å­ä¸­çš„ä¸‹ä¸€ä¸ªè¯ï¼ˆè½¨è¿¹ä¸­çš„ä¸‹ä¸€ä¸ªå‘½ä¸­ç‚¹ï¼‰ï¼Œä»¥åŠäº‹ä»¶å†…æ‰€æœ‰å‘½ä¸­ç‚¹çš„ä¸€æ¬¡æ€§é¢„æµ‹ã€‚åœ¨å¹¿æ³›çš„è®¾è®¡å·¥ä½œä¸­ï¼Œæˆ‘ä»¬å°è¯•äº†åŸºäºTransformeræ¶æ„çš„ä¸‰ç§æ¨¡å‹å’Œä¸€ç§åŸºäºU-Netæ¶æ„çš„æ¨¡å‹ï¼Œå¯¹ç¢°æ’äº‹ä»¶å‘½ä¸­ç‚¹è¿›è¡Œè½¨è¿¹å…³è”é¢„æµ‹ã€‚åœ¨æˆ‘ä»¬çš„è¯„ä¼°ä¸­ï¼Œæˆ‘ä»¬è€ƒè™‘äº†é—®é¢˜çš„ç®€å•åˆ°å¤æ‚è¡¨ç¤ºå½¢å¼ï¼Œå¹¶å°½æ—©æ’é™¤äº†æŒ‡æ ‡è¾ƒä½çš„æ—©æœŸè®¾è®¡ã€‚æˆ‘ä»¬æŠ¥å‘Šäº†å¤§é‡ç»“æœï¼ŒåŒ…æ‹¬é¢„æµ‹å‡†ç¡®åº¦ï¼ˆåˆ†æ•°ï¼‰å’Œè®¡ç®—æ€§èƒ½ã€‚æˆ‘ä»¬åˆ©ç”¨REDVIDæ¨¡æ‹Ÿæ¡†æ¶ä»¥åŠå¯¹TrackMLæ•°æ®é›†çš„ç¼©å‡ï¼Œä¸ºæˆ‘ä»¬çš„å®éªŒä»ç®€å•åˆ°å¤æ‚åˆ›å»ºäº†äº”ä¸ªæ•°æ®é›†ã€‚ç»“æœçªå‡ºäº†ä¸åŒè®¾è®¡åœ¨é¢„æµ‹ç²¾åº¦å’Œè®¡ç®—æ€§èƒ½æ–¹é¢çš„ä¼˜åŠ¿ï¼Œè¯æ˜äº†æˆ‘ä»¬çš„æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚æœ€é‡è¦çš„æ˜¯ï¼Œç»“æœè¡¨æ˜åŸºäºä¸€æ¬¡æ€§ç¼–ç å™¨åˆ†ç±»å™¨è§£å†³æ–¹æ¡ˆçš„Transformeræ˜¯ä¸€ç§ç”¨äºè·Ÿè¸ªä»»åŠ¡çš„å®ç”¨æ–¹æ³•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2407.07179v2">PDF</a> </p>
<p><strong>Summary</strong><br>é«˜èƒ½é‡ç‰©ç†å®éªŒé¢ä¸´æ•°æ®é‡çš„å¢é•¿ï¼Œç‰¹åˆ«æ˜¯åœ¨å³å°†å‡çº§çš„é«˜äº®åº¦LHCä¸­ã€‚æ•°æ®å¤„ç†çš„æ¯ä¸ªæ­¥éª¤éƒ½éœ€è¦è¿›è¡Œä¿®è®¢ï¼Œå…¶ä¸­åŒ…æ‹¬ç²’å­è½¨è¿¹é‡å»ºï¼ˆè·Ÿè¸ªï¼‰ã€‚æœ¬æ–‡é‡‡ç”¨æœºå™¨å­¦ä¹ è¾…åŠ©çš„è§£å†³æ–¹æ¡ˆï¼Œé’ˆå¯¹è·Ÿè¸ªä¸­æœ€è€—æ—¶çš„æ­¥éª¤â€”â€”å‘½ä¸­ç‚¹åˆ†é…è‡³ç²’å­æˆ–è½¨è¿¹å€™é€‰ï¼Œæå‡ºæœ‰æ•ˆçš„æ–¹æ³•ã€‚ä»å¤§å‹è¯­è¨€æ¨¡å‹ä¸­æ±²å–çµæ„Ÿï¼Œé‡‡ç”¨Transformeræ¶æ„çš„æ¨¡å‹å’ŒU-Netæ¶æ„çš„æ¨¡å‹è¿›è¡Œè½¨è¿¹å…³è”é¢„æµ‹ã€‚é€šè¿‡äº”ä¸ªä»ç®€å•åˆ°å¤æ‚çš„æ•°æ®é›†çš„å®éªŒç»“æœå±•ç¤ºé¢„æµ‹å‡†ç¡®æ€§å’Œè®¡ç®—æ€§èƒ½ï¼Œè¡¨æ˜ä¸€ç§åŸºäºTransformerçš„ä¸€æ¬¡æ€§ç¼–ç å™¨åˆ†ç±»å™¨çš„è§£å†³æ–¹æ¡ˆå…·æœ‰å®ç”¨æ€§å’Œå¯è¡Œæ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>é«˜èƒ½é‡ç‰©ç†å®éªŒé¢ä¸´æ•°æ®å¢é•¿é—®é¢˜ï¼Œç‰¹åˆ«æ˜¯åœ¨å³å°†å‡çº§çš„é«˜äº®åº¦LHCä¸­ã€‚</li>
<li>æ•°æ®å¤„ç†ç®¡é“å‡ ä¹æ¯ä¸€æ­¥éƒ½éœ€è¦ä¿®è®¢ï¼Œå…¶ä¸­åŒ…æ‹¬ç²’å­è½¨è¿¹é‡å»ºï¼ˆè·Ÿè¸ªï¼‰ã€‚</li>
<li>æœºå™¨å­¦ä¹ è¾…åŠ©çš„è§£å†³æ–¹æ¡ˆå¯¹äºè·Ÿè¸ªä»»åŠ¡ä¸­çš„å‘½ä¸­ç‚¹åˆ†é…è‡³å…³é‡è¦ã€‚</li>
<li>ä»å¤§å‹è¯­è¨€æ¨¡å‹ä¸­æ±²å–çµæ„Ÿï¼Œä½¿ç”¨Transformeræ¶æ„æ¨¡å‹å’ŒU-Netæ¶æ„æ¨¡å‹è¿›è¡Œè½¨è¿¹å…³è”é¢„æµ‹ã€‚</li>
<li>é‡‡ç”¨äº”ç§ä¸åŒå¤æ‚ç¨‹åº¦çš„æ•°æ®é›†è¿›è¡Œå®éªŒï¼Œä»¥è¯„ä¼°é¢„æµ‹å‡†ç¡®æ€§å’Œè®¡ç®—æ€§èƒ½ã€‚</li>
<li>å®éªŒç»“æœå±•ç¤ºäº†ä¸åŒè®¾è®¡çš„ä¼˜åŠ¿ï¼ŒåŒ…æ‹¬é¢„æµ‹å‡†ç¡®æ€§å’Œè®¡ç®—æ€§èƒ½æ–¹é¢çš„å·®å¼‚ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2407.07179">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-4d91132cb9e350b5bf9db4fda5ae37ad.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-17669adfa0ca8445d0907be9f76752ee.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="SOLO-A-Single-Transformer-for-Scalable-Vision-Language-Modeling"><a href="#SOLO-A-Single-Transformer-for-Scalable-Vision-Language-Modeling" class="headerlink" title="SOLO: A Single Transformer for Scalable Vision-Language Modeling"></a>SOLO: A Single Transformer for Scalable Vision-Language Modeling</h2><p><strong>Authors:Yangyi Chen, Xingyao Wang, Hao Peng, Heng Ji</strong></p>
<p>We present SOLO, a single transformer for Scalable visiOn-Language mOdeling. Current large vision-language models (LVLMs) such as LLaVA mostly employ heterogeneous architectures that connect pre-trained visual encoders with large language models (LLMs) to facilitate visual recognition and complex reasoning. Although achieving remarkable performance with relatively lightweight training, we identify four primary scalability limitations: (1) The visual capacity is constrained by pre-trained visual encoders, which are typically an order of magnitude smaller than LLMs. (2) The heterogeneous architecture complicates the use of established hardware and software infrastructure. (3) Study of scaling laws on such architecture must consider three separate components - visual encoder, connector, and LLMs, which complicates the analysis. (4) The use of existing visual encoders typically requires following a pre-defined specification of image inputs pre-processing, for example, by reshaping inputs to fixed-resolution square images, which presents difficulties in processing and training on high-resolution images or those with unusual aspect ratio. A unified single Transformer architecture, like SOLO, effectively addresses these scalability concerns in LVLMs; however, its limited adoption in the modern context likely stems from the absence of reliable training recipes that balance both modalities and ensure stable training for billion-scale models. In this paper, we introduce the first open-source training recipe for developing SOLO, an open-source 7B LVLM using moderate academic resources. The training recipe involves initializing from LLMs, sequential pre-training on ImageNet and web-scale data, and instruction fine-tuning on our curated high-quality datasets. On extensive evaluation, SOLO demonstrates performance comparable to LLaVA-v1.5-7B, particularly excelling in visual mathematical reasoning. </p>
<blockquote>
<p>æˆ‘ä»¬æå‡ºäº†SOLOï¼Œè¿™æ˜¯ä¸€ç§å¯æ‰©å±•çš„é¢å‘è§†è§‰è¯­è¨€å»ºæ¨¡çš„å•å˜å‹å™¨æ¨¡å‹ã€‚å½“å‰çš„å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆLVLMsï¼‰ï¼Œå¦‚LLaVAï¼Œå¤§å¤šé‡‡ç”¨å¼‚æ„æ¶æ„ï¼Œå°†é¢„è®­ç»ƒçš„è§†è§‰ç¼–ç å™¨ä¸å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰è¿æ¥èµ·æ¥ï¼Œä»¥ä¿ƒè¿›è§†è§‰è¯†åˆ«å’Œå¤æ‚æ¨ç†ã€‚å°½ç®¡åœ¨ç›¸å¯¹è¾ƒè½»çš„è®­ç»ƒä¸­å–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½ï¼Œä½†æˆ‘ä»¬å‘ç°äº†å››ä¸ªä¸»è¦çš„å¯æ‰©å±•æ€§é™åˆ¶ï¼šï¼ˆ1ï¼‰è§†è§‰å®¹é‡å—é™äºé¢„è®­ç»ƒçš„è§†è§‰ç¼–ç å™¨ï¼Œè¿™äº›ç¼–ç å™¨çš„è§„æ¨¡é€šå¸¸æ¯”LLMå°ä¸€ä¸ªæ•°é‡çº§ã€‚ï¼ˆ2ï¼‰å¼‚æ„æ¶æ„ä½¿å¾—ä½¿ç”¨ç°æœ‰çš„ç¡¬ä»¶å’Œè½¯ä»¶åŸºç¡€è®¾æ–½å˜å¾—å¤æ‚ã€‚ï¼ˆ3ï¼‰åœ¨è¿™ç§æ¶æ„ä¸Šç ”ç©¶æ‰©å±•å®šå¾‹éœ€è¦è€ƒè™‘ä¸‰ä¸ªç‹¬ç«‹çš„éƒ¨åˆ†â€”â€”è§†è§‰ç¼–ç å™¨ã€è¿æ¥å™¨å’ŒLLMsï¼Œè¿™å¢åŠ äº†åˆ†æçš„å¤æ‚æ€§ã€‚ï¼ˆ4ï¼‰ä½¿ç”¨ç°æœ‰çš„è§†è§‰ç¼–ç å™¨é€šå¸¸éœ€è¦éµå¾ªé¢„å®šä¹‰çš„å›¾åƒè¾“å…¥é¢„å¤„ç†è§„èŒƒï¼Œä¾‹å¦‚å°†è¾“å…¥é‡å¡‘ä¸ºå›ºå®šåˆ†è¾¨ç‡çš„æ–¹å½¢å›¾åƒï¼Œè¿™åœ¨å¤„ç†é«˜åˆ†è¾¨ç‡å›¾åƒæˆ–å…·æœ‰ä¸å¯»å¸¸çºµæ¨ªæ¯”çš„å›¾åƒæ—¶å¸¦æ¥äº†å›°éš¾ã€‚åƒSOLOè¿™æ ·çš„ç»Ÿä¸€å•ä¸€Transformeræ¶æ„æœ‰æ•ˆåœ°è§£å†³äº†LVLMçš„å¯æ‰©å±•æ€§é—®é¢˜ï¼›ç„¶è€Œï¼Œåœ¨ç°ä»£ç¯å¢ƒä¸­å…¶æœ‰é™çš„é‡‡ç”¨å¯èƒ½æ˜¯ç”±äºç¼ºä¹å¯é çš„è®­ç»ƒé…æ–¹ï¼Œè¿™äº›é…æ–¹éœ€è¦åœ¨ä¸¤ç§æ¨¡æ€ä¹‹é—´å–å¾—å¹³è¡¡ï¼Œå¹¶ç¡®ä¿å¤§è§„æ¨¡æ¨¡å‹çš„ç¨³å®šè®­ç»ƒã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†å¼€å‘SOLOçš„ç¬¬ä¸€ä¸ªå¼€æºè®­ç»ƒé…æ–¹ï¼Œè¿™æ˜¯ä¸€ä¸ªä½¿ç”¨é€‚åº¦å­¦æœ¯èµ„æºçš„7B LVLMã€‚è®­ç»ƒé…æ–¹åŒ…æ‹¬ä»LLMsè¿›è¡Œåˆå§‹åŒ–ï¼Œåœ¨ImageNetå’ŒWebè§„æ¨¡æ•°æ®ä¸Šè¿›è¡Œé¡ºåºé¢„è®­ç»ƒï¼Œä»¥åŠåœ¨æˆ‘ä»¬ç²¾é€‰çš„é«˜è´¨é‡æ•°æ®é›†ä¸Šè¿›è¡ŒæŒ‡ä»¤å¾®è°ƒã€‚ç»è¿‡å¹¿æ³›è¯„ä¼°ï¼ŒSOLOçš„æ€§èƒ½ä¸LLaVA-v1.5-7Bç›¸å½“ï¼Œç‰¹åˆ«æ˜¯åœ¨è§†è§‰æ•°å­¦æ¨ç†æ–¹é¢è¡¨ç°çªå‡ºã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2407.06438v3">PDF</a> Accepted to TMLR</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†SOLOæ¨¡å‹ï¼Œè¿™æ˜¯ä¸€ç§ç”¨äºå¯æ‰©å±•è§†è§‰è¯­è¨€å»ºæ¨¡çš„å•å˜å‹å™¨æ¶æ„ã€‚é’ˆå¯¹å½“å‰å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆLVLMsï¼‰åœ¨è§†è§‰å®¹é‡ã€æ¶æ„å¤æ‚æ€§ã€æ‰©å±•æ€§åˆ†æå’Œå›¾åƒè¾“å…¥å¤„ç†æ–¹é¢çš„å±€é™æ€§ï¼ŒSOLOæ¨¡å‹é‡‡ç”¨å•ä¸€å˜å‹å™¨æ¶æ„å®ç°è§†è§‰å’Œè¯­è¨€å»ºæ¨¡çš„æœ‰æ•ˆæ•´åˆã€‚æœ¬æ–‡è¿˜ä»‹ç»äº†SOLOæ¨¡å‹çš„å¼€æºè®­ç»ƒé…æ–¹ï¼Œè¯¥é…æ–¹é€šè¿‡åˆå§‹åŒ–å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ã€åœ¨ImageNetå’ŒWebè§„æ¨¡æ•°æ®ä¸Šè¿›è¡Œé¡ºåºé¢„è®­ç»ƒä»¥åŠåœ¨æˆ‘ä»¬ç²¾é€‰çš„é«˜è´¨é‡æ•°æ®é›†ä¸Šè¿›è¡ŒæŒ‡ä»¤å¾®è°ƒï¼Œå®ç°äº†åœ¨é€‚åº¦å­¦æœ¯èµ„æºä¸‹çš„7B LVLMå¼€å‘ã€‚SOLOåœ¨å¹¿æ³›è¯„ä¼°ä¸­è¡¨ç°å‡ºäº†ä¸LLaVAv1.5-7Bç›¸å½“çš„æ€§èƒ½ï¼Œå°¤å…¶åœ¨è§†è§‰æ•°å­¦æ¨ç†æ–¹é¢è¡¨ç°ä¼˜å¼‚ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>SOLOæ¨¡å‹æ˜¯ä¸€ç§ç”¨äºå¯æ‰©å±•è§†è§‰è¯­è¨€å»ºæ¨¡çš„å•å˜å‹å™¨æ¶æ„ã€‚</li>
<li>å½“å‰å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆLVLMsï¼‰å­˜åœ¨è§†è§‰å®¹é‡ã€æ¶æ„å¤æ‚æ€§ã€æ‰©å±•æ€§åˆ†æå’Œå›¾åƒè¾“å…¥å¤„ç†æ–¹é¢çš„å±€é™æ€§ã€‚</li>
<li>SOLOé€šè¿‡å•ä¸€å˜å‹å™¨æ¶æ„è§£å†³äº†LVLMsçš„æ‰©å±•æ€§é—®é¢˜ã€‚</li>
<li>SOLOæ¨¡å‹çš„å¼€æºè®­ç»ƒé…æ–¹åŒ…æ‹¬åˆå§‹åŒ–å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ã€åœ¨ImageNetå’ŒWebè§„æ¨¡æ•°æ®ä¸Šè¿›è¡Œé¡ºåºé¢„è®­ç»ƒï¼Œä»¥åŠåœ¨ç²¾é€‰çš„é«˜è´¨é‡æ•°æ®é›†ä¸Šè¿›è¡ŒæŒ‡ä»¤å¾®è°ƒã€‚</li>
<li>SOLOæ¨¡å‹æ€§èƒ½ä¸LLaVAv1.5-7Bç›¸å½“ï¼Œå°¤å…¶åœ¨è§†è§‰æ•°å­¦æ¨ç†æ–¹é¢è¡¨ç°ä¼˜å¼‚ã€‚</li>
<li>SOLOæ¨¡å‹é‡‡ç”¨é€‚åº¦å­¦æœ¯èµ„æºå³å¯è¿›è¡Œå¼€å‘ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2407.06438">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-0080a481a6cc9f8b0b47edbe704cc0da.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-bad8bc9ad6de1f8262e7285e7c21b3e0.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-19a0de5de4ff862b9218a4266261f5e2.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="Identifying-Query-Relevant-Neurons-in-Large-Language-Models-for-Long-Form-Texts"><a href="#Identifying-Query-Relevant-Neurons-in-Large-Language-Models-for-Long-Form-Texts" class="headerlink" title="Identifying Query-Relevant Neurons in Large Language Models for   Long-Form Texts"></a>Identifying Query-Relevant Neurons in Large Language Models for   Long-Form Texts</h2><p><strong>Authors:Lihu Chen, Adam Dejl, Francesca Toni</strong></p>
<p>Large Language Models (LLMs) possess vast amounts of knowledge within their parameters, prompting research into methods for locating and editing this knowledge. Previous work has largely focused on locating entity-related (often single-token) facts in smaller models. However, several key questions remain unanswered: (1) How can we effectively locate query-relevant neurons in decoder-only LLMs, such as Llama and Mistral? (2) How can we address the challenge of long-form (or free-form) text generation? (3) Are there localized knowledge regions in LLMs? In this study, we introduce Query-Relevant Neuron Cluster Attribution (QRNCA), a novel architecture-agnostic framework capable of identifying query-relevant neurons in LLMs. QRNCA allows for the examination of long-form answers beyond triplet facts by employing the proxy task of multi-choice question answering. To evaluate the effectiveness of our detected neurons, we build two multi-choice QA datasets spanning diverse domains and languages. Empirical evaluations demonstrate that our method outperforms baseline methods significantly. Further, analysis of neuron distributions reveals the presence of visible localized regions, particularly within different domains. Finally, we show potential applications of our detected neurons in knowledge editing and neuron-based prediction. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨å…¶å‚æ•°ä¸­æ‹¥æœ‰å¤§é‡çŸ¥è¯†ï¼Œè¿™ä¿ƒä½¿ç ”ç©¶äººå‘˜å¯»æ‰¾å®šä½å’Œç¼–è¾‘è¿™äº›çŸ¥è¯†çš„æ–¹æ³•ã€‚ä»¥å‰çš„å·¥ä½œä¸»è¦é›†ä¸­åœ¨åœ¨è¾ƒå°çš„æ¨¡å‹ä¸­å®šä½å®ä½“ç›¸å…³çš„ï¼ˆé€šå¸¸æ˜¯ä¸€ä¸ªæ ‡è®°ï¼‰äº‹å®ã€‚ç„¶è€Œï¼Œä»æœ‰å‡ ä¸ªå…³é”®é—®é¢˜æ‚¬è€Œæœªå†³ï¼šï¼ˆ1ï¼‰æˆ‘ä»¬å¦‚ä½•åœ¨ä»…è§£ç å™¨çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆå¦‚Llamaå’ŒMistralï¼‰ä¸­æœ‰æ•ˆåœ°å®šä½æŸ¥è¯¢ç›¸å…³ç¥ç»å…ƒï¼Ÿï¼ˆ2ï¼‰æˆ‘ä»¬å¦‚ä½•åº”å¯¹é•¿æ–‡æœ¬ï¼ˆæˆ–è‡ªç”±å½¢å¼ï¼‰ç”ŸæˆæŒ‘æˆ˜ï¼Ÿï¼ˆ3ï¼‰å¤§å‹è¯­è¨€æ¨¡å‹ä¸­æœ‰å±€éƒ¨çŸ¥è¯†åŒºåŸŸå—ï¼Ÿåœ¨æœ¬ç ”ç©¶ä¸­ï¼Œæˆ‘ä»¬å¼•å…¥äº†æŸ¥è¯¢ç›¸å…³ç¥ç»å…ƒé›†ç¾¤å½’å±ï¼ˆQRNCAï¼‰ï¼Œè¿™æ˜¯ä¸€ç§æ–°å‹ã€é€‚ç”¨äºæ‰€æœ‰æ¶æ„çš„æ¡†æ¶ï¼Œèƒ½å¤Ÿè¯†åˆ«å¤§å‹è¯­è¨€æ¨¡å‹ä¸­çš„æŸ¥è¯¢ç›¸å…³ç¥ç»å…ƒã€‚QRNCAé€šè¿‡é‡‡ç”¨å¤šé€‰é—®ç­”çš„ä»£ç†ä»»åŠ¡ï¼Œå¯ä»¥æ£€æŸ¥è¶…å‡ºä¸‰å…ƒäº‹å®çš„é•¿å½¢å¼ç­”æ¡ˆã€‚ä¸ºäº†è¯„ä¼°æˆ‘ä»¬æ£€æµ‹åˆ°çš„ç¥ç»å…ƒçš„æœ‰æ•ˆæ€§ï¼Œæˆ‘ä»¬æ„å»ºäº†ä¸¤ä¸ªæ¶µç›–ä¸åŒé¢†åŸŸå’Œè¯­è¨€çš„å¤šé€‰é—®ç­”æ•°æ®é›†ã€‚ç»éªŒè¯„ä¼°è¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•æ˜¾è‘—ä¼˜äºåŸºçº¿æ–¹æ³•ã€‚æ­¤å¤–ï¼Œå¯¹ç¥ç»å…ƒåˆ†å¸ƒçš„åˆ†ææ˜¾ç¤ºå­˜åœ¨æ˜æ˜¾çš„å±€éƒ¨åŒºåŸŸï¼Œç‰¹åˆ«æ˜¯åœ¨ä¸åŒçš„é¢†åŸŸå†…ã€‚æœ€åï¼Œæˆ‘ä»¬å±•ç¤ºäº†æ‰€æ£€æµ‹ç¥ç»å…ƒåœ¨çŸ¥è¯†ç¼–è¾‘å’ŒåŸºäºç¥ç»å…ƒçš„é¢„æµ‹ä¸­çš„æ½œåœ¨åº”ç”¨ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2406.10868v4">PDF</a> AAAI 2025 Main Track</p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å†…å«æœ‰å¤§é‡çŸ¥è¯†ï¼Œç ”ç©¶è€…æ­£åœ¨æ¢ç´¢å¦‚ä½•å®šä½ä¸ç¼–è¾‘è¿™äº›çŸ¥è¯†ã€‚å…ˆå‰çš„ç ”ç©¶ä¸»è¦å…³æ³¨åœ¨å°å‹æ¨¡å‹ä¸­å®šä½å®ä½“ç›¸å…³ï¼ˆé€šå¸¸æ˜¯å•ä»¤ç‰Œï¼‰äº‹å®ã€‚ç„¶è€Œï¼Œæœ¬ç ”ç©¶é¢ä¸´å‡ ä¸ªå…³é”®é—®é¢˜ï¼šå¦‚ä½•æœ‰æ•ˆå®šä½æŸ¥è¯¢ç›¸å…³ç¥ç»å…ƒã€å¦‚ä½•åº”å¯¹é•¿æ–‡æœ¬ç”ŸæˆæŒ‘æˆ˜ï¼Œä»¥åŠæ˜¯å¦å­˜åœ¨å±€éƒ¨çŸ¥è¯†åŒºåŸŸã€‚æœ¬ç ”ç©¶å¼•å…¥äº†ä¸€ç§æ–°å‹æ¶æ„æ— å…³æ¡†æ¶â€”â€”æŸ¥è¯¢ç›¸å…³ç¥ç»å…ƒé›†ç¾¤å½’å±ï¼ˆQRNCAï¼‰ï¼Œèƒ½å¤Ÿè¯†åˆ«LLMä¸­çš„æŸ¥è¯¢ç›¸å…³ç¥ç»å…ƒã€‚QRNCAé€šè¿‡å¤šé€‰é—®ç­”çš„ä»£ç†ä»»åŠ¡ï¼Œèƒ½å¤Ÿæ£€æŸ¥è¶…è¶Šä¸‰å…ƒäº‹å®çš„é•¿å½¢å¼ç­”æ¡ˆã€‚é€šè¿‡æ„å»ºæ¶µç›–ä¸åŒé¢†åŸŸå’Œè¯­è¨€çš„ä¸¤ä¸ªå¤šé€‰é—®ç­”æ•°æ®é›†æ¥è¯„ä¼°æ£€æµ‹åˆ°çš„ç¥ç»å…ƒçš„æœ‰æ•ˆæ€§ï¼Œå®è¯è¯„ä¼°æ˜¾ç¤ºï¼Œè¯¥æ–¹æ³•æ˜¾è‘—ä¼˜äºåŸºçº¿æ–¹æ³•ã€‚è¿›ä¸€æ­¥åˆ†æç¥ç»å…ƒåˆ†å¸ƒæ˜¾ç¤ºï¼Œä¸åŒé¢†åŸŸå†…å­˜åœ¨æ˜æ˜¾çš„å±€éƒ¨åŒºåŸŸã€‚æœ€åï¼Œæˆ‘ä»¬å±•ç¤ºäº†æ£€æµ‹åˆ°çš„ç¥ç»å…ƒåœ¨çŸ¥è¯†ç¼–è¾‘å’ŒåŸºäºç¥ç»å…ƒçš„é¢„æµ‹ä¸­çš„æ½œåœ¨åº”ç”¨ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åŒ…å«å¤§é‡çŸ¥è¯†ï¼Œç ”ç©¶æ­£æ¢ç´¢å¦‚ä½•å®šä½å’Œç¼–è¾‘è¿™äº›çŸ¥è¯†ã€‚</li>
<li>å…ˆå‰ç ”ç©¶ä¸»è¦å…³æ³¨å°å‹æ¨¡å‹çš„å®ä½“ç›¸å…³çŸ¥è¯†çš„å®šä½ã€‚</li>
<li>æŸ¥è¯¢ç›¸å…³ç¥ç»å…ƒå®šä½ã€é•¿æ–‡æœ¬ç”ŸæˆæŒ‘æˆ˜å’Œå±€éƒ¨çŸ¥è¯†åŒºåŸŸçš„å­˜åœ¨æ˜¯LLMç ”ç©¶çš„å…³é”®é—®é¢˜ã€‚</li>
<li>å¼•å…¥äº†ä¸€ç§æ–°çš„æ¶æ„æ— å…³æ¡†æ¶QRNCAï¼Œèƒ½æœ‰æ•ˆè¯†åˆ«LLMä¸­çš„æŸ¥è¯¢ç›¸å…³ç¥ç»å…ƒã€‚</li>
<li>QRNCAé€šè¿‡å¤šé€‰é—®ç­”çš„ä»£ç†ä»»åŠ¡æ¥æ£€æŸ¥é•¿å½¢å¼ç­”æ¡ˆï¼Œè¶…è¶Šäº†ä»¥å‰çš„ä¸‰å…ƒäº‹å®æ£€æŸ¥ã€‚</li>
<li>é€šè¿‡æ„å»ºä¸¤ä¸ªæ¶µç›–ä¸åŒé¢†åŸŸå’Œè¯­è¨€çš„é—®ç­”æ•°æ®é›†ï¼Œå®è¯è¯„ä¼°æ˜¾ç¤ºQRNCAä¼˜äºåŸºçº¿æ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2406.10868">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-b91bd2bb2d8aa730bc2dab19dc86bfbd.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-de74bd5cf9a08b697b246743455515c9.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-d602bd5c852c117dbabee44a94926c5d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e2ccc853b2c0e12375b6546e2e6c3728.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-7aa332d436ddc722b2bc0093d10bdc16.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="Diversifying-Query-Region-Guided-Transformer-for-Temporal-Sentence-Grounding"><a href="#Diversifying-Query-Region-Guided-Transformer-for-Temporal-Sentence-Grounding" class="headerlink" title="Diversifying Query: Region-Guided Transformer for Temporal Sentence   Grounding"></a>Diversifying Query: Region-Guided Transformer for Temporal Sentence   Grounding</h2><p><strong>Authors:Xiaolong Sun, Liushuai Shi, Le Wang, Sanping Zhou, Kun Xia, Yabing Wang, Gang Hua</strong></p>
<p>Temporal sentence grounding is a challenging task that aims to localize the moment spans relevant to a language description. Although recent DETR-based models have achieved notable progress by leveraging multiple learnable moment queries, they suffer from overlapped and redundant proposals, leading to inaccurate predictions. We attribute this limitation to the lack of task-related guidance for the learnable queries to serve a specific mode. Furthermore, the complex solution space generated by variable and open-vocabulary language descriptions complicates optimization, making it harder for learnable queries to distinguish each other adaptively. To tackle this limitation, we present a Region-Guided TRansformer (RGTR) for temporal sentence grounding, which diversifies moment queries to eliminate overlapped and redundant predictions. Instead of using learnable queries, RGTR adopts a set of anchor pairs as moment queries to introduce explicit regional guidance. Each anchor pair takes charge of moment prediction for a specific temporal region, which reduces the optimization difficulty and ensures the diversity of the final predictions. In addition, we design an IoU-aware scoring head to improve proposal quality. Extensive experiments demonstrate the effectiveness of RGTR, outperforming state-of-the-art methods on QVHighlights, Charades-STA and TACoS datasets. Codes are available at <a target="_blank" rel="noopener" href="https://github.com/TensorsSun/RGTR">https://github.com/TensorsSun/RGTR</a> </p>
<blockquote>
<p>æ—¶åºå¥å­å®šä½æ˜¯ä¸€é¡¹å…·æœ‰æŒ‘æˆ˜æ€§çš„ä»»åŠ¡ï¼Œæ—¨åœ¨å®šä½ä¸è¯­è¨€æè¿°ç›¸å…³çš„æ—¶åˆ»è·¨åº¦ã€‚å°½ç®¡åŸºäºDETRçš„è¿‘æœŸæ¨¡å‹é€šè¿‡åˆ©ç”¨å¤šä¸ªå¯å­¦ä¹ çš„æ—¶åˆ»æŸ¥è¯¢å–å¾—äº†æ˜¾è‘—çš„è¿›å±•ï¼Œä½†å®ƒä»¬ä»å­˜åœ¨é‡å å’Œå†—ä½™çš„ææ¡ˆï¼Œå¯¼è‡´é¢„æµ‹ä¸å‡†ç¡®ã€‚æˆ‘ä»¬å°†è¿™ä¸€å±€é™æ€§å½’å› äºç¼ºä¹é’ˆå¯¹ç‰¹å®šæ¨¡å¼æœåŠ¡çš„å¯å­¦ä¹ æŸ¥è¯¢çš„ä»»åŠ¡ç›¸å…³æŒ‡å¯¼ã€‚æ­¤å¤–ï¼Œç”±å¯å˜å’Œå¼€æ”¾è¯æ±‡è¯­è¨€æè¿°ç”Ÿæˆçš„å¤æ‚è§£ç©ºé—´ä½¿ä¼˜åŒ–å˜å¾—å¤æ‚ï¼Œä½¿å¯å­¦ä¹ æŸ¥è¯¢æ›´éš¾å½¼æ­¤é€‚åº”åŒºåˆ†ã€‚ä¸ºäº†å…‹æœè¿™ä¸€å±€é™æ€§ï¼Œæˆ‘ä»¬æå‡ºäº†ç”¨äºæ—¶åºå¥å­å®šä½çš„Region-Guided Transformerï¼ˆRGTRï¼‰ï¼Œå®ƒé€šè¿‡å¤šæ ·åŒ–æ—¶åˆ»æŸ¥è¯¢æ¥æ¶ˆé™¤é‡å å’Œå†—ä½™çš„é¢„æµ‹ã€‚ä¸åŒäºä½¿ç”¨å¯å­¦ä¹ æŸ¥è¯¢ï¼ŒRGTRé‡‡ç”¨ä¸€ç»„é”šç‚¹ä½œä¸ºæ—¶åˆ»æŸ¥è¯¢æ¥å¼•å…¥æ˜ç¡®çš„åŒºåŸŸæŒ‡å¯¼ã€‚æ¯ä¸ªé”šç‚¹è´Ÿè´£ç‰¹å®šæ—¶é—´åŒºåŸŸçš„æ—¶åˆ»é¢„æµ‹ï¼Œè¿™é™ä½äº†ä¼˜åŒ–éš¾åº¦ï¼Œç¡®ä¿äº†æœ€ç»ˆé¢„æµ‹çš„å¤šæ ·æ€§ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸€ä¸ªIoUæ„ŸçŸ¥è¯„åˆ†å¤´æ¥æé«˜ææ¡ˆè´¨é‡ã€‚å¤§é‡å®éªŒè¯æ˜äº†RGTRçš„æœ‰æ•ˆæ€§ï¼Œåœ¨QVHighlightsã€Charades-STAå’ŒTACoSæ•°æ®é›†ä¸Šå‡ä¼˜äºæœ€å…ˆè¿›çš„æ–¹æ³•ã€‚ä»£ç å¯ä»<a target="_blank" rel="noopener" href="https://github.com/TensorsSun/RGTR">https://github.com/TensorsSun/RGTR</a>è·å–ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2406.00143v2">PDF</a> Accepted by AAAI-25. Code is available at   <a target="_blank" rel="noopener" href="https://github.com/TensorsSun/RGTR">https://github.com/TensorsSun/RGTR</a></p>
<p><strong>Summary</strong></p>
<p>åŸºäºDETRçš„æ¨¡å‹åœ¨æ—¶åºå¥å­å®šä½ä»»åŠ¡ä¸­å–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œä½†ä»å­˜åœ¨é‡å å’Œå†—ä½™ææ¡ˆçš„é—®é¢˜ï¼Œå¯¼è‡´é¢„æµ‹ä¸å‡†ç¡®ã€‚ä¸ºè§£å†³æ­¤é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†Region-Guided TRansformerï¼ˆRGTRï¼‰ï¼Œé‡‡ç”¨é”šç‚¹å¯¹çš„å½¢å¼ä½œä¸ºæ—¶åˆ»æŸ¥è¯¢ï¼Œå¼•å…¥æ˜ç¡®çš„åŒºåŸŸæŒ‡å¯¼ï¼Œå‡å°‘ä¼˜åŒ–éš¾åº¦ï¼Œç¡®ä¿æœ€ç»ˆé¢„æµ‹çš„å¤šæ ·æ€§ã€‚åŒæ—¶è®¾è®¡IoUæ„ŸçŸ¥è¯„åˆ†å¤´ä»¥æé«˜ææ¡ˆè´¨é‡ã€‚å®éªŒè¯æ˜RGTRçš„æœ‰æ•ˆæ€§ï¼Œåœ¨QVHighlightsã€Charades-STAå’ŒTACoSæ•°æ®é›†ä¸Šä¼˜äºç°æœ‰æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>åŸºäºDETRçš„æ¨¡å‹åœ¨æ—¶åºå¥å­å®šä½ä»»åŠ¡ä¸­æœ‰æ˜¾è‘—è¿›å±•ã€‚</li>
<li>è¿™äº›æ¨¡å‹å­˜åœ¨é‡å å’Œå†—ä½™ææ¡ˆçš„é—®é¢˜ï¼Œå¯¼è‡´é¢„æµ‹ä¸å‡†ç¡®ã€‚</li>
<li>ç¼ºä¹é’ˆå¯¹ç‰¹å®šæ¨¡å¼çš„ä»»åŠ¡ç›¸å…³æŒ‡å¯¼æ˜¯é€ æˆè¿™ä¸€å±€é™çš„åŸå› ä¹‹ä¸€ã€‚</li>
<li>ä¸ºè§£å†³æ­¤é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†Region-Guided TRansformerï¼ˆRGTRï¼‰ã€‚</li>
<li>RGTRé‡‡ç”¨é”šç‚¹å¯¹çš„å½¢å¼ä½œä¸ºæ—¶åˆ»æŸ¥è¯¢ï¼Œå¼•å…¥åŒºåŸŸæŒ‡å¯¼ã€‚</li>
<li>æ¯ä¸ªé”šç‚¹å¯¹è´Ÿè´£ç‰¹å®šæ—¶åºåŒºåŸŸçš„æ—¶åˆ»é¢„æµ‹ï¼Œæé«˜äº†é¢„æµ‹çš„å¤šæ ·æ€§å’Œå‡†ç¡®æ€§ã€‚</li>
<li>RGTRè®¾è®¡IoUæ„ŸçŸ¥è¯„åˆ†å¤´ä»¥æé«˜ææ¡ˆè´¨é‡ï¼Œå¹¶åœ¨å®éªŒä¸­è¡¨ç°å‡ºå“è¶Šæ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2406.00143">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-a56f613c947ae8aa42f6d238aba4973f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-34e6d07e37b45b1193b29e90e1c0785a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a378a50b18c7fc158663130f426505e1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c9b011e0fcab5e64a085a39bac047e9e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-4af3ae92f226c76a502967fae62bbab4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d216b93cef7010066d93835a77ab07fb.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="Knowledge-Circuits-in-Pretrained-Transformers"><a href="#Knowledge-Circuits-in-Pretrained-Transformers" class="headerlink" title="Knowledge Circuits in Pretrained Transformers"></a>Knowledge Circuits in Pretrained Transformers</h2><p><strong>Authors:Yunzhi Yao, Ningyu Zhang, Zekun Xi, Mengru Wang, Ziwen Xu, Shumin Deng, Huajun Chen</strong></p>
<p>The remarkable capabilities of modern large language models are rooted in their vast repositories of knowledge encoded within their parameters, enabling them to perceive the world and engage in reasoning. The inner workings of how these models store knowledge have long been a subject of intense interest and investigation among researchers. To date, most studies have concentrated on isolated components within these models, such as the Multilayer Perceptrons and attention head. In this paper, we delve into the computation graph of the language model to uncover the knowledge circuits that are instrumental in articulating specific knowledge. The experiments, conducted with GPT2 and TinyLLAMA, have allowed us to observe how certain information heads, relation heads, and Multilayer Perceptrons collaboratively encode knowledge within the model. Moreover, we evaluate the impact of current knowledge editing techniques on these knowledge circuits, providing deeper insights into the functioning and constraints of these editing methodologies. Finally, we utilize knowledge circuits to analyze and interpret language model behaviors such as hallucinations and in-context learning. We believe the knowledge circuits hold potential for advancing our understanding of Transformers and guiding the improved design of knowledge editing. Code and data are available in <a target="_blank" rel="noopener" href="https://github.com/zjunlp/KnowledgeCircuits">https://github.com/zjunlp/KnowledgeCircuits</a>. </p>
<blockquote>
<p>ç°ä»£å¤§å‹è¯­è¨€æ¨¡å‹çš„å“è¶Šèƒ½åŠ›æ ¹æ¤äºå…¶å‚æ•°ä¸­ç¼–ç çš„åºå¤§çŸ¥è¯†åº“ï¼Œä½¿å®ƒä»¬èƒ½å¤Ÿæ„ŸçŸ¥ä¸–ç•Œå¹¶å‚ä¸æ¨ç†ã€‚è¿™äº›æ¨¡å‹å¦‚ä½•å†…éƒ¨å­˜å‚¨çŸ¥è¯†çš„è¿è¡Œæœºåˆ¶ä¸€ç›´æ˜¯ç ”ç©¶è€…ä»¬å…³æ³¨çš„ç„¦ç‚¹ã€‚è¿„ä»Šä¸ºæ­¢ï¼Œå¤§å¤šæ•°ç ”ç©¶éƒ½é›†ä¸­åœ¨è¿™äº›æ¨¡å‹çš„å­¤ç«‹ç»„ä»¶ä¸Šï¼Œå¦‚å¤šå±‚æ„ŸçŸ¥å™¨å’Œæ³¨æ„åŠ›å¤´ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æ·±å…¥æ¢è®¨äº†è¯­è¨€æ¨¡å‹çš„è®¡ç®—å›¾ï¼Œä»¥æ­ç¤ºå¯¹è¡¨è¾¾ç‰¹å®šçŸ¥è¯†è‡³å…³é‡è¦çš„çŸ¥è¯†å›è·¯ã€‚æˆ‘ä»¬åˆ©ç”¨GPT2å’ŒTinyLLAMAè¿›è¡Œçš„å®éªŒä½¿æˆ‘ä»¬èƒ½å¤Ÿè§‚å¯Ÿåˆ°æŸäº›ä¿¡æ¯å¤´ã€å…³ç³»å¤´å’Œå¤šå±‚æ„ŸçŸ¥å™¨å¦‚ä½•åœ¨æ¨¡å‹ä¸­ååŒç¼–ç çŸ¥è¯†ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜è¯„ä¼°äº†å½“å‰çŸ¥è¯†ç¼–è¾‘æŠ€æœ¯å¯¹è¿™äº›çŸ¥è¯†å›è·¯çš„å½±å“ï¼Œä¸ºè¿™äº›ç¼–è¾‘æ–¹æ³•çš„è¿ä½œå’Œå±€é™æ€§æä¾›äº†æ›´æ·±å…¥çš„è§è§£ã€‚æœ€åï¼Œæˆ‘ä»¬åˆ©ç”¨çŸ¥è¯†å›è·¯åˆ†æå’Œè§£é‡Šè¯­è¨€æ¨¡å‹çš„è¡Œä¸ºï¼Œå¦‚è™šæ„å’Œä¸Šä¸‹æ–‡å­¦ä¹ ã€‚æˆ‘ä»¬ç›¸ä¿¡çŸ¥è¯†å›è·¯åœ¨æ¨è¿›æˆ‘ä»¬å¯¹Transformerçš„ç†è§£ä»¥åŠæŒ‡å¯¼çŸ¥è¯†ç¼–è¾‘çš„æ”¹è¿›è®¾è®¡æ–¹é¢å…·æœ‰æ½œåŠ›ã€‚ä»£ç å’Œæ•°æ®å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/zjunlp/KnowledgeCircuits%E4%B8%AD%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/zjunlp/KnowledgeCircuitsä¸­æ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2405.17969v3">PDF</a> NeurIPS 2024, 26 pages</p>
<p><strong>Summary</strong><br>ç°ä»£å¤§å‹è¯­è¨€æ¨¡å‹çš„å¼ºå¤§èƒ½åŠ›æºäºå…¶å‚æ•°ä¸­ç¼–ç çš„ä¸°å¯ŒçŸ¥è¯†åº“ï¼Œèƒ½å¤Ÿæ„ŸçŸ¥ä¸–ç•Œå¹¶è¿›è¡Œæ¨ç†ã€‚æœ¬æ–‡æ·±å…¥æ¢ç©¶äº†è¯­è¨€æ¨¡å‹çš„è®¡ç®—å›¾ï¼Œæ­ç¤ºäº†è¡¨è¿°ç‰¹å®šçŸ¥è¯†çš„å…³é”®çŸ¥è¯†å›è·¯ã€‚é€šè¿‡å®éªŒè§‚å¯Ÿäº†ä¿¡æ¯å¤´ã€å…³ç³»å¤´å’Œå¤šå±‚æ„ŸçŸ¥å™¨ç­‰å¦‚ä½•ååŒåœ¨æ¨¡å‹ä¸­ç¼–ç çŸ¥è¯†ï¼Œå¹¶è¯„ä¼°äº†å½“å‰çŸ¥è¯†ç¼–è¾‘æŠ€æœ¯å¯¹çŸ¥è¯†å›è·¯çš„å½±å“ã€‚æ­¤å¤–ï¼Œè¿˜åˆ©ç”¨çŸ¥è¯†å›è·¯åˆ†æå’Œè§£è¯»äº†è¯­è¨€æ¨¡å‹çš„è¡Œä¸ºï¼Œå¦‚å¹»è§‰å’Œä¸Šä¸‹æ–‡å­¦ä¹ ã€‚çŸ¥è¯†å›è·¯æœ‰åŠ©äºåŠ æ·±å¯¹Transformerçš„ç†è§£å¹¶æ”¹è¿›çŸ¥è¯†ç¼–è¾‘è®¾è®¡ã€‚ç›¸å…³ä»£ç å’Œæ•°æ®å¯åœ¨GitHubä¸Šæ‰¾åˆ°ï¼š<a target="_blank" rel="noopener" href="https://github.com/zjunlp/KnowledgeCircuits">https://github.com/zjunlp/KnowledgeCircuits</a>ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç°ä»£å¤§å‹è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›æ¥æºäºå…¶å‚æ•°ä¸­çš„çŸ¥è¯†åº“ã€‚</li>
<li>è¯­è¨€æ¨¡å‹çš„è®¡ç®—å›¾æ­ç¤ºäº†è¡¨è¿°ç‰¹å®šçŸ¥è¯†çš„å…³é”®çŸ¥è¯†å›è·¯ã€‚</li>
<li>ä¿¡æ¯å¤´ã€å…³ç³»å¤´å’Œå¤šå±‚æ„ŸçŸ¥å™¨ç­‰ååŒåœ¨æ¨¡å‹ä¸­ç¼–ç çŸ¥è¯†ã€‚</li>
<li>å½“å‰çŸ¥è¯†ç¼–è¾‘æŠ€æœ¯å¯¹çŸ¥è¯†å›è·¯æœ‰å½±å“ã€‚</li>
<li>çŸ¥è¯†å›è·¯å¯ç”¨äºåˆ†æå’Œè§£è¯»è¯­è¨€æ¨¡å‹çš„è¡Œä¸ºï¼Œå¦‚å¹»è§‰å’Œä¸Šä¸‹æ–‡å­¦ä¹ ã€‚</li>
<li>çŸ¥è¯†å›è·¯æœ‰åŠ©äºåŠ æ·±å¯¹Transformerçš„ç†è§£ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2405.17969">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-af730609e7fc22b4dee875bf26238928.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-8d3cb131479da7b37c780048cc82c152.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8734983426fdd7a6d09d51c8e813eb10.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4262ffa2ba0634752220b6817b9d40ab.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="Evolutionary-Large-Language-Model-for-Automated-Feature-Transformation"><a href="#Evolutionary-Large-Language-Model-for-Automated-Feature-Transformation" class="headerlink" title="Evolutionary Large Language Model for Automated Feature Transformation"></a>Evolutionary Large Language Model for Automated Feature Transformation</h2><p><strong>Authors:Nanxu Gong, Chandan K. Reddy, Wangyang Ying, Haifeng Chen, Yanjie Fu</strong></p>
<p>Feature transformation aims to reconstruct the feature space of raw features to enhance the performance of downstream models. However, the exponential growth in the combinations of features and operations poses a challenge, making it difficult for existing methods to efficiently explore a wide space. Additionally, their optimization is solely driven by the accuracy of downstream models in specific domains, neglecting the acquisition of general feature knowledge. To fill this research gap, we propose an evolutionary LLM framework for automated feature transformation. This framework consists of two parts: 1) constructing a multi-population database through an RL data collector while utilizing evolutionary algorithm strategies for database maintenance, and 2) utilizing the ability of Large Language Model (LLM) in sequence understanding, we employ few-shot prompts to guide LLM in generating superior samples based on feature transformation sequence distinction. Leveraging the multi-population database initially provides a wide search scope to discover excellent populations. Through culling and evolution, the high-quality populations are afforded greater opportunities, thereby furthering the pursuit of optimal individuals. Through the integration of LLMs with evolutionary algorithms, we achieve efficient exploration within a vast space, while harnessing feature knowledge to propel optimization, thus realizing a more adaptable search paradigm. Finally, we empirically demonstrate the effectiveness and generality of our proposed method. </p>
<blockquote>
<p>ç‰¹å¾è½¬æ¢æ—¨åœ¨é‡æ„åŸå§‹ç‰¹å¾çš„ç‰¹å¾ç©ºé—´ï¼Œä»¥æé«˜ä¸‹æ¸¸æ¨¡å‹çš„æ€§èƒ½ã€‚ç„¶è€Œï¼Œç‰¹å¾å’Œæ“ä½œçš„ç»„åˆå‘ˆæŒ‡æ•°çº§å¢é•¿ï¼Œç»™ç°æœ‰æ–¹æ³•å¸¦æ¥äº†æŒ‘æˆ˜ï¼Œä½¿å…¶éš¾ä»¥æœ‰æ•ˆåœ°æ¢ç´¢å¹¿é˜”çš„ç©ºé—´ã€‚æ­¤å¤–ï¼Œå®ƒä»¬çš„ä¼˜åŒ–ä»…ç”±ç‰¹å®šé¢†åŸŸçš„ä¸‹æ¸¸æ¨¡å‹çš„å‡†ç¡®æ€§é©±åŠ¨ï¼Œå¿½è§†äº†é€šç”¨ç‰¹å¾çŸ¥è¯†çš„è·å–ã€‚ä¸ºäº†å¡«è¡¥è¿™ä¸€ç ”ç©¶ç©ºç™½ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªç”¨äºè‡ªåŠ¨åŒ–ç‰¹å¾è½¬æ¢çš„è¿›åŒ–å¼LLMæ¡†æ¶ã€‚è¯¥æ¡†æ¶ç”±ä¸¤éƒ¨åˆ†ç»„æˆï¼š1ï¼‰é€šè¿‡RLæ•°æ®æ”¶é›†å™¨æ„å»ºå¤šäººå£æ•°æ®åº“ï¼Œå¹¶åˆ©ç”¨è¿›åŒ–ç®—æ³•ç­–ç•¥è¿›è¡Œæ•°æ®åº“ç»´æŠ¤ï¼›2ï¼‰åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨åºåˆ—ç†è§£æ–¹é¢çš„èƒ½åŠ›ï¼Œæˆ‘ä»¬é€šè¿‡å°‘æ•°æç¤ºæ¥æŒ‡å¯¼LLMæ ¹æ®ç‰¹å¾è½¬æ¢åºåˆ—çš„åŒºåˆ«ç”Ÿæˆä¼˜è´¨æ ·æœ¬ã€‚åˆ©ç”¨å¤šäººå£æ•°æ®åº“æœ€åˆæä¾›äº†å¹¿æ³›çš„æœç´¢èŒƒå›´æ¥å‘ç°ä¼˜ç§€çš„äººå£ã€‚é€šè¿‡ç­›é€‰å’Œè¿›åŒ–ï¼Œé«˜è´¨é‡çš„äººç¾¤è·å¾—äº†æ›´å¤šçš„æœºä¼šï¼Œä»è€Œè¿›ä¸€æ­¥è¿½æ±‚ä¼˜ç§€ä¸ªä½“ã€‚é€šè¿‡å°†LLMä¸è¿›åŒ–ç®—æ³•ç›¸ç»“åˆï¼Œæˆ‘ä»¬åœ¨å¹¿é˜”çš„ç©ºé—´å†…å®ç°äº†æœ‰æ•ˆçš„æ¢ç´¢ï¼Œå¹¶åˆ©ç”¨ç‰¹å¾çŸ¥è¯†æ¨åŠ¨ä¼˜åŒ–ï¼Œä»è€Œå®ç°äº†ä¸€ä¸ªæ›´çµæ´»çš„æœç´¢èŒƒå¼ã€‚æœ€åï¼Œæˆ‘ä»¬é€šè¿‡å®è¯è¯æ˜äº†æ‰€æå‡ºæ–¹æ³•çš„æœ‰æ•ˆæ€§å’Œæ™®éæ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2405.16203v2">PDF</a> Accepted to AAAI 2025</p>
<p><strong>Summary</strong><br>ç‰¹å¾è½¬æ¢æ—¨åœ¨é‡å»ºåŸå§‹ç‰¹å¾çš„ç‰¹å¾ç©ºé—´ä»¥æé«˜ä¸‹æ¸¸æ¨¡å‹çš„æ€§èƒ½ã€‚ç„¶è€Œï¼Œç‰¹å¾å’Œæ“ä½œçš„ç»„åˆå‘ˆæŒ‡æ•°çº§å¢é•¿ï¼Œç°æœ‰æ–¹æ³•éš¾ä»¥æœ‰æ•ˆæ¢ç´¢å¹¿é˜”çš„ç©ºé—´ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºè¿›åŒ–ç®—æ³•çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è‡ªåŠ¨åŒ–ç‰¹å¾è½¬æ¢æ¡†æ¶ã€‚è¯¥æ¡†æ¶åŒ…æ‹¬ä¸¤éƒ¨åˆ†ï¼š1ï¼‰é€šè¿‡RLæ•°æ®æ”¶é›†å™¨æ„å»ºå¤šäººå£æ•°æ®åº“ï¼Œå¹¶åˆ©ç”¨è¿›åŒ–ç®—æ³•ç­–ç•¥è¿›è¡Œæ•°æ®åº“ç»´æŠ¤ï¼›2ï¼‰åˆ©ç”¨LLMåœ¨åºåˆ—ç†è§£æ–¹é¢çš„èƒ½åŠ›ï¼Œé€šè¿‡å°‘é‡æç¤ºæ¥æŒ‡å¯¼LLMç”ŸæˆåŸºäºç‰¹å¾è½¬æ¢åºåˆ—åŒºåˆ†çš„ä¼˜è´¨æ ·æœ¬ã€‚åˆ©ç”¨å¤šäººå£æ•°æ®åº“å¯ä»¥æä¾›ä¸€ä¸ªå¹¿é˜”çš„æœç´¢èŒƒå›´æ¥å‘ç°ä¼˜ç§€ç§ç¾¤ã€‚é€šè¿‡ç­›é€‰å’Œè¿›åŒ–ï¼Œä¼˜è´¨ç§ç¾¤è·å¾—æ›´å¤šæœºä¼šï¼Œä»è€Œè¿½æ±‚æ›´ä¼˜ç§€çš„ä¸ªä½“ã€‚é€šè¿‡ç»“åˆLLMå’Œè¿›åŒ–ç®—æ³•ï¼Œæˆ‘ä»¬å®ç°äº†åœ¨å¹¿é˜”ç©ºé—´å†…çš„æœ‰æ•ˆæ¢ç´¢ï¼Œå¹¶åˆ©ç”¨ç‰¹å¾çŸ¥è¯†æ¨åŠ¨ä¼˜åŒ–ï¼Œä»è€Œå®ç°æ›´çµæ´»æœç´¢èŒƒå¼ã€‚æœ€åï¼Œæˆ‘ä»¬é€šè¿‡å®éªŒè¯æ˜äº†æ‰€æå‡ºæ–¹æ³•çš„æœ‰æ•ˆæ€§å’Œé€šç”¨æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç‰¹å¾è½¬æ¢æ—¨åœ¨æ”¹å–„ä¸‹æ¸¸æ¨¡å‹çš„æ€§èƒ½ï¼Œé€šè¿‡é‡å»ºåŸå§‹ç‰¹å¾çš„ç‰¹å¾ç©ºé—´æ¥å®ç°ã€‚</li>
<li>ç°æœ‰æ–¹æ³•é¢ä¸´ç‰¹å¾å’Œæ“ä½œç»„åˆçˆ†ç‚¸çš„é—®é¢˜ï¼Œéš¾ä»¥æœ‰æ•ˆæ¢ç´¢å¹¿é˜”çš„ç‰¹å¾ç©ºé—´ã€‚</li>
<li>æå‡ºçš„åŸºäºè¿›åŒ–ç®—æ³•çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è‡ªåŠ¨åŒ–ç‰¹å¾è½¬æ¢æ¡†æ¶æ—¨åœ¨è§£å†³è¿™ä¸€é—®é¢˜ã€‚</li>
<li>æ¡†æ¶åŒ…æ‹¬æ„å»ºå¤šäººå£æ•°æ®åº“å’Œåˆ©ç”¨LLMåœ¨åºåˆ—ç†è§£æ–¹é¢çš„èƒ½åŠ›æ¥æŒ‡å¯¼ç‰¹å¾è½¬æ¢ã€‚</li>
<li>åˆ©ç”¨å¤šäººå£æ•°æ®åº“å®ç°å¹¿æ³›æœç´¢ï¼Œå¹¶é€šè¿‡ç­›é€‰å’Œè¿›åŒ–æ‰¾åˆ°ä¼˜è´¨ç§ç¾¤ã€‚</li>
<li>ç»“åˆLLMå’Œè¿›åŒ–ç®—æ³•ï¼Œå®ç°åœ¨å¹¿é˜”ç©ºé—´å†…çš„æœ‰æ•ˆæ¢ç´¢ï¼Œæ¨åŠ¨ä¼˜åŒ–ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2405.16203">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-e04d87cf6a04a3bcb38db07a5f7f2638.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a89122439cd12cd8a282bce1aa710f09.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7b0ce361e38add2a239bc20a9af6bac0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2d398949587d47b549cd807490bd7c17.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-019f5afef5d08f081143c0a99845ff7f.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-1a75e097261534a0405725031a940eac.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="CEM-A-Data-Efficient-Method-for-Large-Language-Models-to-Continue-Evolving-From-Mistakes"><a href="#CEM-A-Data-Efficient-Method-for-Large-Language-Models-to-Continue-Evolving-From-Mistakes" class="headerlink" title="CEM: A Data-Efficient Method for Large Language Models to Continue   Evolving From Mistakes"></a>CEM: A Data-Efficient Method for Large Language Models to Continue   Evolving From Mistakes</h2><p><strong>Authors:Haokun Zhao, Haixia Han, Jie Shi, Chengyu Du, Jiaqing Liang, Yanghua Xiao</strong></p>
<p>As world knowledge advances and new task schemas emerge, Continual Learning (CL) becomes essential for keeping Large Language Models (LLMs) current and addressing their shortcomings. This process typically involves continual instruction tuning (CIT) and continual pre-training (CPT) to enable these models to adapt to novel tasks and acquire critical knowledge. However, collecting sufficient CPT data and efficiently bridging knowledge gaps remain significant challenges. Inspired by the â€˜summarizing mistakesâ€™ strategy, we propose the Continue Evolving from Mistakes (CEM) method, a data-efficient approach aiming to collect CPT data and continually improve LLMsâ€™ performance through iterative evaluation and supplementation with mistake-relevant knowledge. To further optimize data usage and mitigate forgetting, we introduce a novel training paradigm that combines CIT and CPT. Experiments show that CEM substantially enhances multiple modelsâ€™ performance on both in-domain and out-of-domain QA tasks, achieving gains of up to 29.63%. Code and datasets are available on <a target="_blank" rel="noopener" href="https://anonymous.4open.science/r/cem-BB25">https://anonymous.4open.science/r/cem-BB25</a>. </p>
<blockquote>
<p>éšç€å…¨çƒçŸ¥è¯†çš„è¿›æ­¥å’Œæ–°ä»»åŠ¡æ¨¡å¼çš„å‡ºç°ï¼ŒæŒç»­å­¦ä¹ ï¼ˆCLï¼‰å¯¹äºä¿æŒå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ—¶æ•ˆæ€§å’Œè§£å†³å…¶ç¼ºç‚¹å˜å¾—è‡³å…³é‡è¦ã€‚è¿™ä¸€è¿‡ç¨‹é€šå¸¸æ¶‰åŠæŒç»­æŒ‡ä»¤å¾®è°ƒï¼ˆCITï¼‰å’ŒæŒç»­é¢„è®­ç»ƒï¼ˆCPTï¼‰ï¼Œä»¥ä½¿è¿™äº›æ¨¡å‹èƒ½å¤Ÿé€‚åº”æ–°ä»»åŠ¡å¹¶è·å–å…³é”®çŸ¥è¯†ã€‚ç„¶è€Œï¼Œæ”¶é›†è¶³å¤Ÿçš„CPTæ•°æ®å’Œæœ‰æ•ˆåœ°å¡«è¡¥çŸ¥è¯†ç©ºç™½ä»ç„¶é¢ä¸´é‡å¤§æŒ‘æˆ˜ã€‚å—â€œæ€»ç»“é”™è¯¯â€ç­–ç•¥çš„å¯å‘ï¼Œæˆ‘ä»¬æå‡ºäº†â€œä»é”™è¯¯ä¸­æŒç»­è¿›åŒ–â€ï¼ˆCEMï¼‰æ–¹æ³•ï¼Œè¿™æ˜¯ä¸€ç§æ•°æ®é«˜æ•ˆçš„æ–¹æ³•ï¼Œæ—¨åœ¨æ”¶é›†CPTæ•°æ®å¹¶é€šè¿‡è¿­ä»£è¯„ä¼°å’Œä½¿ç”¨ä¸é”™è¯¯ç›¸å…³çš„çŸ¥è¯†æ¥æŒç»­æ”¹è¿›LLMçš„æ€§èƒ½ã€‚ä¸ºäº†è¿›ä¸€æ­¥ä¼˜åŒ–æ•°æ®ä½¿ç”¨å¹¶ç¼“è§£é—å¿˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§ç»“åˆCITå’ŒCPTçš„æ–°å‹è®­ç»ƒèŒƒå¼ã€‚å®éªŒè¡¨æ˜ï¼ŒCEMåœ¨åŸŸå†…å’ŒåŸŸå¤–é—®ç­”ä»»åŠ¡ä¸Šå¤§å¹…åº¦æé«˜äº†å¤šä¸ªæ¨¡å‹çš„è¡¨ç°ï¼Œæœ€é«˜æå‡äº†29.63%ã€‚ç›¸å…³ä»£ç å’Œæ•°æ®é›†å¯é€šè¿‡<a target="_blank" rel="noopener" href="https://anonymous.4open.science/r/cem-BB25%E8%8E%B7%E5%8F%96%E3%80%82">https://anonymous.4open.science/r/cem-BB25è·å–ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2404.08707v7">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>éšç€å…¨çƒçŸ¥è¯†çš„è¿›æ­¥å’Œæ–°ä»»åŠ¡æ¨¡å¼çš„å‡ºç°ï¼ŒæŒç»­å­¦ä¹ ï¼ˆCLï¼‰å¯¹äºä¿æŒå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ›´æ–°å’Œå…‹æœå…¶å±€é™æ€§å˜å¾—è‡³å…³é‡è¦ã€‚è¿™æ¶‰åŠæŒç»­æŒ‡ä»¤è°ƒæ•´ï¼ˆCITï¼‰å’ŒæŒç»­é¢„è®­ç»ƒï¼ˆCPTï¼‰çš„è¿‡ç¨‹ï¼Œä½¿è¿™äº›æ¨¡å‹èƒ½å¤Ÿé€‚åº”æ–°ä»»åŠ¡å¹¶è·å–å…³é”®çŸ¥è¯†ã€‚ç„¶è€Œï¼Œæ”¶é›†è¶³å¤Ÿçš„CPTæ•°æ®å’Œæœ‰æ•ˆåœ°å¡«è¡¥çŸ¥è¯†ç©ºç™½ä»ç„¶æ˜¯å·¨å¤§çš„æŒ‘æˆ˜ã€‚å—â€œæ€»ç»“é”™è¯¯â€ç­–ç•¥çš„å¯å‘ï¼Œæˆ‘ä»¬æå‡ºäº†â€œä»é”™è¯¯ä¸­æŒç»­è¿›åŒ–â€ï¼ˆCEMï¼‰æ–¹æ³•ï¼Œè¿™æ˜¯ä¸€ç§æ—¨åœ¨æ”¶é›†CPTæ•°æ®å¹¶æŒç»­æé«˜LLMæ€§èƒ½çš„æ•°æ®é«˜æ•ˆæ–¹æ³•ï¼Œé€šè¿‡è¿­ä»£è¯„ä¼°å’Œä½¿ç”¨é”™è¯¯ç›¸å…³çŸ¥è¯†è¿›è¡Œè¡¥å……ã€‚ä¸ºäº†è¿›ä¸€æ­¥ä¼˜åŒ–æ•°æ®ä½¿ç”¨å¹¶ç¼“è§£é—å¿˜é—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§ç»“åˆCITå’ŒCPTçš„æ–°å‹è®­ç»ƒèŒƒå¼ã€‚å®éªŒè¡¨æ˜ï¼ŒCEMæ˜¾è‘—æé«˜äº†å¤šä¸ªæ¨¡å‹åœ¨é¢†åŸŸå†…å¤–é—®ç­”ä»»åŠ¡ä¸Šçš„æ€§èƒ½ï¼Œæœ€é«˜æå‡è¾¾29.63%ã€‚ç›¸å…³ä»£ç å’Œæ•°æ®é›†å¯åœ¨åŒ¿åå¹³å°ä¸Šè·å–ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æŒç»­å­¦ä¹ ï¼ˆCLï¼‰å¯¹äºä¿æŒå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ›´æ–°å’Œé€‚åº”æ–°ä»»åŠ¡è‡³å…³é‡è¦ã€‚</li>
<li>æŒç»­æŒ‡ä»¤è°ƒæ•´ï¼ˆCITï¼‰å’ŒæŒç»­é¢„è®­ç»ƒï¼ˆCPTï¼‰æ˜¯ä½¿LLMé€‚åº”æ–°ä»»åŠ¡å’Œè·å–å…³é”®çŸ¥è¯†çš„å…³é”®è¿‡ç¨‹ã€‚</li>
<li>æ”¶é›†è¶³å¤Ÿçš„CPTæ•°æ®å’Œå¡«è¡¥çŸ¥è¯†ç©ºç™½æ˜¯LLMé¢ä¸´çš„ä¸»è¦æŒ‘æˆ˜ä¹‹ä¸€ã€‚</li>
<li>å—â€œæ€»ç»“é”™è¯¯â€ç­–ç•¥çš„å¯å‘ï¼Œæå‡ºäº†â€œä»é”™è¯¯ä¸­æŒç»­è¿›åŒ–â€ï¼ˆCEMï¼‰æ–¹æ³•ï¼Œæ—¨åœ¨é€šè¿‡è¿­ä»£è¯„ä¼°å’Œä½¿ç”¨é”™è¯¯ç›¸å…³çŸ¥è¯†æ¥æé«˜LLMçš„æ€§èƒ½ã€‚</li>
<li>CEMæ–¹æ³•ç»“åˆäº†CITå’ŒCPTï¼Œæ—¨åœ¨ä¼˜åŒ–æ•°æ®ä½¿ç”¨å¹¶ç¼“è§£é—å¿˜é—®é¢˜ã€‚</li>
<li>å®éªŒè¡¨æ˜ï¼ŒCEMæ–¹æ³•èƒ½æ˜¾è‘—æé«˜LLMåœ¨å¤šä¸ªä»»åŠ¡ä¸Šçš„æ€§èƒ½ï¼ŒåŒ…æ‹¬é¢†åŸŸå†…å¤–é—®ç­”ä»»åŠ¡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2404.08707">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-d32e4c1fbdd6f2a3d7dca22c4e40ab56.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0c9538b474c94168d61fa37c18e2fccc.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b1de9330afb2146c73f97390da94fba5.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="Red-Teaming-GPT-4V-Are-GPT-4V-Safe-Against-Uni-Multi-Modal-Jailbreak-Attacks"><a href="#Red-Teaming-GPT-4V-Are-GPT-4V-Safe-Against-Uni-Multi-Modal-Jailbreak-Attacks" class="headerlink" title="Red Teaming GPT-4V: Are GPT-4V Safe Against Uni&#x2F;Multi-Modal Jailbreak   Attacks?"></a>Red Teaming GPT-4V: Are GPT-4V Safe Against Uni&#x2F;Multi-Modal Jailbreak   Attacks?</h2><p><strong>Authors:Shuo Chen, Zhen Han, Bailan He, Zifeng Ding, Wenqian Yu, Philip Torr, Volker Tresp, Jindong Gu</strong></p>
<p>Various jailbreak attacks have been proposed to red-team Large Language Models (LLMs) and revealed the vulnerable safeguards of LLMs. Besides, some methods are not limited to the textual modality and extend the jailbreak attack to Multimodal Large Language Models (MLLMs) by perturbing the visual input. However, the absence of a universal evaluation benchmark complicates the performance reproduction and fair comparison. Besides, there is a lack of comprehensive evaluation of closed-source state-of-the-art (SOTA) models, especially MLLMs, such as GPT-4V. To address these issues, this work first builds a comprehensive jailbreak evaluation dataset with 1445 harmful questions covering 11 different safety policies. Based on this dataset, extensive red-teaming experiments are conducted on 11 different LLMs and MLLMs, including both SOTA proprietary models and open-source models. We then conduct a deep analysis of the evaluated results and find that (1) GPT4 and GPT-4V demonstrate better robustness against jailbreak attacks compared to open-source LLMs and MLLMs. (2) Llama2 and Qwen-VL-Chat are more robust compared to other open-source models. (3) The transferability of visual jailbreak methods is relatively limited compared to textual jailbreak methods. The dataset and code can be found <a target="_blank" rel="noopener" href="https://github.com/chenxshuo/RedTeamingGPT4V">https://github.com/chenxshuo/RedTeamingGPT4V</a> </p>
<blockquote>
<p>é’ˆå¯¹å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„çº¢é˜Ÿæ”»å‡»å·²ç»æå‡ºï¼Œå¹¶æ­ç¤ºäº†LLMçš„è„†å¼±ä¿æŠ¤æªæ–½ã€‚é™¤æ­¤ä¹‹å¤–ï¼Œä¸€äº›æ–¹æ³•ä¸ä»…é™äºæ–‡æœ¬æ¨¡å¼ï¼Œè€Œæ˜¯é€šè¿‡å¹²æ‰°è§†è§‰è¾“å…¥å°†è¶Šç‹±æ”»å‡»æ‰©å±•åˆ°å¤šæ¨¡å¼å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰ã€‚ç„¶è€Œï¼Œç”±äºç¼ºä¹é€šç”¨çš„è¯„ä¼°åŸºå‡†ï¼Œä½¿å¾—æ€§èƒ½å†ç”Ÿå’Œå…¬å¹³æ¯”è¾ƒå˜å¾—å¤æ‚ã€‚æ­¤å¤–ï¼Œå¯¹äºé—­æºæœ€å…ˆè¿›çš„æ¨¡å‹ï¼ˆå°¤å…¶æ˜¯MLLMsï¼‰ï¼Œå¦‚GPT-4Vç­‰ç¼ºä¹å…¨é¢çš„è¯„ä¼°ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œè¿™é¡¹å·¥ä½œé¦–å…ˆå»ºç«‹äº†ä¸€ä¸ªåŒ…å«1445ä¸ªæœ‰å®³é—®é¢˜çš„å…¨é¢è¶Šç‹±è¯„ä¼°æ•°æ®é›†ï¼Œæ¶µç›–11é¡¹ä¸åŒçš„å®‰å…¨æ”¿ç­–ã€‚åŸºäºæ­¤æ•°æ®é›†ï¼Œå¯¹åŒ…æ‹¬æœ€å…ˆè¿›ç§æœ‰æ¨¡å‹å’Œå¼€æºæ¨¡å‹åœ¨å†…çš„11ç§ä¸åŒçš„LLMå’ŒMLLMè¿›è¡Œäº†å¹¿æ³›çš„çº¢è‰²å›¢é˜Ÿå®éªŒã€‚ç„¶åæˆ‘ä»¬å¯¹è¯„ä¼°ç»“æœè¿›è¡Œäº†æ·±å…¥åˆ†æï¼Œå‘ç°ï¼ˆ1ï¼‰GPT4å’ŒGPT-4Vç›¸è¾ƒäºå¼€æºLLMå’ŒMLLMè¡¨ç°å‡ºæ›´å¥½çš„å¯¹æŠ—è¶Šç‹±æ”»å‡»çš„ç¨³å¥æ€§ã€‚ï¼ˆ2ï¼‰ç›¸è¾ƒäºå…¶ä»–å¼€æºæ¨¡å‹ï¼ŒLlama2å’ŒQwen-VL-Chatæ›´åŠ ç¨³å¥ã€‚ï¼ˆ3ï¼‰ç›¸è¾ƒäºæ–‡æœ¬è¶Šç‹±æ–¹æ³•ï¼Œè§†è§‰è¶Šç‹±æ–¹æ³•çš„å¯è¿ç§»æ€§ç›¸å¯¹æœ‰é™ã€‚æ•°æ®é›†å’Œä»£ç å¯ä»¥åœ¨<a target="_blank" rel="noopener" href="https://github.com/chenxshuo/RedTeamingGPT4V">https://github.com/chenxshuo/RedTeamingGPT4V</a>æ‰¾åˆ°ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2404.03411v2">PDF</a> technical report; update code repo link</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†é’ˆå¯¹å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„çº¢é˜Ÿæ”»å‡»ï¼ˆjailbreak attacksï¼‰åŠå…¶æ‰©å±•åˆ°å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰çš„æƒ…å†µã€‚æ–‡ç« æŒ‡å‡ºå½“å‰ç¼ºä¹é€šç”¨è¯„ä¼°åŸºå‡†çš„é—®é¢˜ï¼Œå¹¶æ„å»ºäº†ä¸€ä¸ªåŒ…å«1445ä¸ªæœ‰å®³é—®é¢˜çš„ç»¼åˆè¯„ä¼°æ•°æ®é›†ï¼Œæ¶µç›–11é¡¹ä¸åŒçš„å®‰å…¨ç­–ç•¥ã€‚é€šè¿‡å¯¹åŒ…æ‹¬å…ˆè¿›ç§æœ‰æ¨¡å‹å’Œå¼€æºæ¨¡å‹åœ¨å†…çš„11ç§LLMå’ŒMLLMè¿›è¡Œçº¢é˜Ÿå®éªŒï¼Œå‘ç°GPT4å’ŒGPT-4Våœ¨æŠµå¾¡æ”»å‡»æ–¹é¢è¡¨ç°å‡ºè¾ƒå¼ºçš„ç¨³å¥æ€§ã€‚åŒæ—¶ï¼Œæ–‡ç« è¿˜åˆ†æäº†è§†è§‰è¶Šç‹±æ–¹æ³•çš„å¯è½¬ç§»æ€§ç›¸å¯¹æœ‰é™ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ–‡ç« ä»‹ç»äº†é’ˆå¯¹å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å’Œå¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰çš„jailbreakæ”»å‡»ï¼Œæ­ç¤ºäº†LLMçš„è„†å¼±æ€§ä¿éšœã€‚</li>
<li>ç¼ºä¹é€šç”¨è¯„ä¼°åŸºå‡†ç»™æ€§èƒ½å¤ç°å’Œå…¬å¹³æ¯”è¾ƒå¸¦æ¥äº†å›°éš¾ã€‚</li>
<li>æ„å»ºäº†åŒ…å«1445ä¸ªæœ‰å®³é—®é¢˜çš„ç»¼åˆè¯„ä¼°æ•°æ®é›†ï¼Œæ¶µç›–11é¡¹ä¸åŒçš„å®‰å…¨ç­–ç•¥ã€‚</li>
<li>GPT4å’ŒGPT-4Våœ¨æŠµå¾¡jailbreakæ”»å‡»æ–¹é¢è¡¨ç°å‡ºè¾ƒå¼ºçš„ç¨³å¥æ€§ã€‚</li>
<li>Llama2å’ŒQwen-VL-Chatåœ¨ä¸å…¶ä»–å¼€æºæ¨¡å‹çš„æ¯”è¾ƒä¸­è¡¨ç°å‡ºæ›´é«˜çš„ç¨³å¥æ€§ã€‚</li>
<li>è§†è§‰è¶Šç‹±æ–¹æ³•çš„å¯è½¬ç§»æ€§ç›¸å¯¹æœ‰é™ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2404.03411">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-042e91b317d2b72e36eeeb2a98112282.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-32e6fdea9e7f36da2149fcff88bd5055.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0ab3645ace9983ad4865462d622d7a4b.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="Merging-Text-Transformer-Models-from-Different-Initializations"><a href="#Merging-Text-Transformer-Models-from-Different-Initializations" class="headerlink" title="Merging Text Transformer Models from Different Initializations"></a>Merging Text Transformer Models from Different Initializations</h2><p><strong>Authors:Neha Verma, Maha Elbayad</strong></p>
<p>Recent work on permutation-based model merging has shown impressive low- or zero-barrier mode connectivity between models from completely different initializations. However, this line of work has not yet extended to the Transformer architecture, despite its dominant popularity in the language domain. Therefore, in this work, we investigate the extent to which separate Transformer minima learn similar features, and propose a model merging technique to investigate the relationship between these minima in the loss landscape. The specifics of the architecture, like its residual connections, multi-headed attention, and discrete, sequential input, require specific interventions in order to compute model permutations that remain within the same functional equivalence class. In merging these models with our method, we consistently find lower loss barriers between minima compared to model averaging, across models trained on a masked-language modeling task or fine-tuned on a language understanding benchmark. Our results show that the minima of these models are less sharp and isolated than previously understood, and provide a basis for future work on merging separately trained Transformer models. </p>
<blockquote>
<p>è¿‘æœŸå…³äºåŸºäºæ’åˆ—æ¨¡å‹åˆå¹¶çš„ç ”ç©¶å·¥ä½œæ˜¾ç¤ºï¼Œä»ä¸åŒåˆå§‹åŒ–æ¨¡å‹ä¹‹é—´å…·æœ‰ä»¤äººå°è±¡æ·±åˆ»çš„ä½éšœç¢æˆ–é›¶éšœç¢æ¨¡å¼è¿æ¥ã€‚ç„¶è€Œï¼Œå°½ç®¡å…¶åœ¨è¯­è¨€é¢†åŸŸçš„æ™®åŠå æ®ä¸»å¯¼åœ°ä½ï¼Œè¿™ä¸€ç ”ç©¶æ–¹å‘å°šæœªæ‰©å±•åˆ°Transformeræ¶æ„ã€‚å› æ­¤ï¼Œåœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬ç ”ç©¶äº†å•ç‹¬è®­ç»ƒçš„Transformeræ¨¡å‹å­¦ä¹ ç›¸ä¼¼ç‰¹å¾çš„ç¨‹åº¦ï¼Œå¹¶æå‡ºäº†ä¸€ç§æ¨¡å‹åˆå¹¶æŠ€æœ¯ï¼Œä»¥ç ”ç©¶æŸå¤±æ™¯è§‚ä¸­è¿™äº›å±€éƒ¨æœ€å°å€¼ä¹‹é—´çš„å…³ç³»ã€‚è¯¥æ¶æ„çš„ç‰¹å®šç»†èŠ‚ï¼Œå¦‚æ®‹å·®è¿æ¥ã€å¤šå¤´æ³¨æ„åŠ›ä»¥åŠç¦»æ•£åºåˆ—è¾“å…¥ç­‰ï¼Œéœ€è¦è¿›è¡Œç‰¹å®šçš„å¹²é¢„æ“ä½œï¼Œä»¥ä¾¿è®¡ç®—ä¿æŒåœ¨åŒä¸€åŠŸèƒ½ç­‰ä»·ç±»å†…çš„æ¨¡å‹æ’åˆ—ã€‚ä½¿ç”¨æˆ‘ä»¬çš„æ–¹æ³•è¿›è¡Œæ¨¡å‹åˆå¹¶æ—¶ï¼Œæˆ‘ä»¬å‘ç°ä¸æ¨¡å‹å¹³å‡ç›¸æ¯”ï¼Œåœ¨é’ˆå¯¹æ©ç è¯­è¨€å»ºæ¨¡ä»»åŠ¡è¿›è¡Œè®­ç»ƒæˆ–åœ¨è¯­è¨€ç†è§£åŸºå‡†æµ‹è¯•ä¸Šè¿›è¡Œå¾®è°ƒåçš„æ¨¡å‹ä¹‹é—´çš„æŸå¤±éšœç¢å§‹ç»ˆè¾ƒä½ã€‚æˆ‘ä»¬çš„ç»“æœè¡¨æ˜ï¼Œè¿™äº›æ¨¡å‹çš„å±€éƒ¨æœ€å°å€¼æ¯”ä¹‹å‰æ‰€ç†è§£çš„æ›´ä¸ºå¹³ç¼“ä¸”ä¸é‚£ä¹ˆå­¤ç«‹ï¼Œå¹¶ä¸ºæœªæ¥åˆå¹¶å•ç‹¬è®­ç»ƒçš„Transformeræ¨¡å‹çš„å·¥ä½œå¥ å®šäº†åŸºç¡€ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2403.00986v3">PDF</a> TMLR, November 2024</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ¢è®¨äº†å°†ä¸åŒåˆå§‹åŒ–çš„Transformeræ¨¡å‹è¿›è¡Œåˆå¹¶çš„æ½œåŠ›ã€‚ç ”ç©¶å‘ç°ï¼Œé€šè¿‡ç‰¹å®šçš„æ¨¡å‹åˆå¹¶æŠ€æœ¯ï¼Œå¯ä»¥åœ¨Transformeræ¶æ„çš„æŸå¤±æ™¯è§‚ä¸­æ¢ç´¢ä¸åŒæœ€å°å€¼ç‚¹ä¹‹é—´çš„å…³ç³»ã€‚è¯¥æ–¹æ³•èƒ½å¤Ÿè®¡ç®—æ¨¡å‹æ’åˆ—çš„ç»„åˆï¼Œä½¿å®ƒä»¬ä¿æŒåŠŸèƒ½ç­‰ä»·æ€§ã€‚ç›¸è¾ƒäºæ¨¡å‹å¹³å‡æ–¹æ³•ï¼Œæœ¬æ–‡æå‡ºçš„æ¨¡å‹åˆå¹¶æ–¹æ³•èƒ½å¤Ÿåœ¨è¯­è¨€å»ºæ¨¡ä»»åŠ¡æˆ–è¯­è¨€ç†è§£åŸºå‡†æµ‹è¯•ä¸­å®ç°æ›´ä½çš„æŸå¤±å±éšœã€‚ç ”ç©¶ç»“æœæ­ç¤ºäº†è¿™äº›æ¨¡å‹çš„æœ€ä½ç‚¹å¹¶ä¸åƒå…ˆå‰è®¤ä¸ºçš„é‚£æ ·å°–é”å’Œå­¤ç«‹ï¼Œä¸ºæœªæ¥ç ”ç©¶åˆå¹¶å•ç‹¬è®­ç»ƒçš„Transformeræ¨¡å‹å¥ å®šäº†åŸºç¡€ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç ”ç©¶æ¢è®¨äº†å°†ä¸åŒåˆå§‹åŒ–çš„Transformeræ¨¡å‹åˆå¹¶çš„æ½œåŠ›ã€‚</li>
<li>é€šè¿‡ç‰¹å®šçš„æ¨¡å‹åˆå¹¶æŠ€æœ¯ï¼Œèƒ½å¤Ÿåœ¨Transformeræ¶æ„ä¸­æ¢ç´¢ä¸åŒæœ€å°å€¼ç‚¹ä¹‹é—´çš„å…³ç³»ã€‚</li>
<li>æ¨¡å‹åˆå¹¶æ–¹æ³•æ¶‰åŠåˆ°è®¡ç®—æ¨¡å‹æ’åˆ—ç»„åˆï¼Œä¿æŒåŠŸèƒ½ç­‰ä»·æ€§ã€‚</li>
<li>ä¸æ¨¡å‹å¹³å‡æ–¹æ³•ç›¸æ¯”ï¼Œæå‡ºçš„æ¨¡å‹åˆå¹¶æ–¹æ³•åœ¨ç‰¹å®šä»»åŠ¡ä¸Šå®ç°äº†æ›´ä½çš„æŸå¤±å±éšœã€‚</li>
<li>ç ”ç©¶æ­ç¤ºäº†Transformeræ¨¡å‹çš„æœ€ä½ç‚¹å¹¶ä¸åƒå…ˆå‰è®¤ä¸ºçš„é‚£æ ·å°–é”å’Œå­¤ç«‹ã€‚</li>
<li>æœ¬æ–‡ä¸ºæœªæ¥ç ”ç©¶åˆå¹¶å•ç‹¬è®­ç»ƒçš„Transformeræ¨¡å‹æä¾›äº†æ–°çš„è§†è§’å’Œæ½œåœ¨æ–¹å‘ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2403.00986">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-de4d1b2082fb1ff64a03e938308ccbf1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3f8a9eed26f2b3fc223c5a7873110271.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c0f52dce093c2804f3bda5ac1c0a02dc.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-77ae6820091a308bd25936edffe6a0f8.jpg" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="SLEB-Streamlining-LLMs-through-Redundancy-Verification-and-Elimination-of-Transformer-Blocks"><a href="#SLEB-Streamlining-LLMs-through-Redundancy-Verification-and-Elimination-of-Transformer-Blocks" class="headerlink" title="SLEB: Streamlining LLMs through Redundancy Verification and Elimination   of Transformer Blocks"></a>SLEB: Streamlining LLMs through Redundancy Verification and Elimination   of Transformer Blocks</h2><p><strong>Authors:Jiwon Song, Kyungseok Oh, Taesu Kim, Hyungjun Kim, Yulhwa Kim, Jae-Joon Kim</strong></p>
<p>Large language models (LLMs) have proven to be highly effective across various natural language processing tasks. However, their large number of parameters poses significant challenges for practical deployment. Pruning, a technique aimed at reducing the size and complexity of LLMs, offers a potential solution by removing redundant components from the network. Despite the promise of pruning, existing methods often struggle to achieve substantial end-to-end LLM inference speedup. In this paper, we introduce SLEB, a novel approach designed to streamline LLMs by eliminating redundant transformer blocks. We choose the transformer block as the fundamental unit for pruning, because LLMs exhibit block-level redundancy with high similarity between the outputs of neighboring blocks. This choice allows us to effectively enhance the processing speed of LLMs. Our experimental results demonstrate that SLEB outperforms previous LLM pruning methods in accelerating LLM inference while also maintaining superior perplexity and accuracy, making SLEB as a promising technique for enhancing the efficiency of LLMs. The code is available at: <a target="_blank" rel="noopener" href="https://github.com/jiwonsong-dev/SLEB">https://github.com/jiwonsong-dev/SLEB</a>. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨å„ç§è‡ªç„¶è¯­è¨€å¤„ç†ä»»åŠ¡ä¸­è¡¨ç°å‡ºäº†é«˜åº¦çš„æœ‰æ•ˆæ€§ã€‚ç„¶è€Œï¼Œå®ƒä»¬çš„å¤§é‡å‚æ•°ç»™å®é™…éƒ¨ç½²å¸¦æ¥äº†é‡å¤§æŒ‘æˆ˜ã€‚å‰ªææ˜¯ä¸€ç§æ—¨åœ¨å‡å°‘LLMå¤§å°å’Œå¤æ‚æ€§çš„æŠ€æœ¯ï¼Œé€šè¿‡æ¶ˆé™¤ç½‘ç»œä¸­çš„å†—ä½™ç»„ä»¶æ¥æä¾›æ½œåœ¨çš„è§£å†³æ–¹æ¡ˆã€‚å°½ç®¡å‰ªæå¾ˆæœ‰å‰æ™¯ï¼Œä½†ç°æœ‰æ–¹æ³•å¾€å¾€éš¾ä»¥å®ç°ç«¯åˆ°ç«¯çš„LLMæ¨ç†é€Ÿåº¦çš„å¤§å¹…æå‡ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†SLEBï¼Œè¿™æ˜¯ä¸€ç§é€šè¿‡æ¶ˆé™¤å†—ä½™transformerå—æ¥ä¼˜åŒ–LLMçš„æ–°æ–¹æ³•ã€‚æˆ‘ä»¬é€‰æ‹©transformerå—ä½œä¸ºå‰ªæçš„åŸºæœ¬å•ä½ï¼Œå› ä¸ºLLMè¡¨ç°å‡ºå—çº§å†—ä½™ï¼Œç›¸é‚»å—ä¹‹é—´çš„è¾“å‡ºé«˜åº¦ç›¸ä¼¼ã€‚è¿™ä¸€é€‰æ‹©ä½¿æˆ‘ä»¬èƒ½å¤Ÿæœ‰æ•ˆåœ°æé«˜LLMçš„å¤„ç†é€Ÿåº¦ã€‚æˆ‘ä»¬çš„å®éªŒç»“æœè¡¨æ˜ï¼ŒSLEBåœ¨åŠ é€ŸLLMæ¨ç†æ–¹é¢ä¼˜äºä»¥å‰çš„LLMå‰ªææ–¹æ³•ï¼ŒåŒæ—¶ä¿æŒäº†ä¼˜å¼‚çš„å›°æƒ‘åº¦å’Œå‡†ç¡®æ€§ï¼Œä½¿SLEBæˆä¸ºæé«˜LLMæ•ˆç‡çš„æœ‰å‰é€”çš„æŠ€æœ¯ã€‚ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/jiwonsong-dev/SLEB">https://github.com/jiwonsong-dev/SLEB</a>è·å¾—ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2402.09025v6">PDF</a> ICML 2024</p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨è‡ªç„¶è¯­è¨€å¤„ç†ä»»åŠ¡ä¸­è¡¨ç°å‡ºé«˜æ•ˆæ€§ï¼Œä½†å…¶å‚æ•°ä¼—å¤šç»™å®é™…åº”ç”¨å¸¦æ¥äº†æŒ‘æˆ˜ã€‚ä¿®å‰ªæŠ€æœ¯æ—¨åœ¨å‡å°‘LLMçš„å¤§å°å’Œå¤æ‚æ€§ï¼Œé€šè¿‡ç§»é™¤ç½‘ç»œä¸­çš„å†—ä½™ç»„ä»¶æ¥è§£å†³é—®é¢˜ã€‚å°½ç®¡ä¿®å‰ªå…·æœ‰æ½œåŠ›ï¼Œä½†ç°æœ‰æ–¹æ³•å¾€å¾€éš¾ä»¥å®ç°LLMç«¯åˆ°ç«¯çš„æ¨ç†é€Ÿåº¦æå‡ã€‚æœ¬æ–‡ä»‹ç»äº†ä¸€ç§æ–°å‹æ–¹æ³•SLEBï¼Œé€šè¿‡æ¶ˆé™¤å†—ä½™çš„è½¬æ¢å™¨å—æ¥ä¼˜åŒ–LLMã€‚æˆ‘ä»¬é€‰æ‹©è½¬æ¢å™¨å—ä½œä¸ºä¿®å‰ªçš„åŸºæœ¬å•å…ƒï¼Œå› ä¸ºLLMåœ¨ç›¸é‚»å—è¾“å‡ºä¹‹é—´è¡¨ç°å‡ºå—çº§å†—ä½™ã€‚è¿™ç§é€‰æ‹©å¯ä»¥æœ‰æ•ˆåœ°æé«˜LLMçš„å¤„ç†é€Ÿåº¦ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒSLEBåœ¨åŠ é€ŸLLMæ¨ç†æ–¹é¢ä¼˜äºå…ˆå‰çš„LLMä¿®å‰ªæ–¹æ³•ï¼ŒåŒæ—¶ä¿æŒè¾ƒä½çš„å›°æƒ‘åº¦å’Œå‡†ç¡®æ€§ï¼Œæ˜¯ä¸€ç§æé«˜LLMæ•ˆç‡çš„æœ‰å‰é€”çš„æŠ€æœ¯ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLMåœ¨è‡ªç„¶è¯­è¨€å¤„ç†ä»»åŠ¡ä¸­è¡¨ç°å‡ºé«˜æ•ˆæ€§ï¼Œä½†å‚æ•°ä¼—å¤šç»™å®é™…åº”ç”¨å¸¦æ¥æŒ‘æˆ˜ã€‚</li>
<li>ä¿®å‰ªæŠ€æœ¯æ˜¯å‡å°‘LLMå¤§å°å’Œå¤æ‚æ€§çš„æœ‰æ•ˆæ–¹æ³•ã€‚</li>
<li>ç°æœ‰LLMä¿®å‰ªæ–¹æ³•éš¾ä»¥å®ç°ç«¯åˆ°ç«¯çš„æ¨ç†é€Ÿåº¦æå‡ã€‚</li>
<li>SLEBæ˜¯ä¸€ç§æ–°å‹LLMä¼˜åŒ–æ–¹æ³•ï¼Œé€šè¿‡æ¶ˆé™¤å†—ä½™çš„è½¬æ¢å™¨å—æ¥åŠ é€Ÿæ¨ç†ã€‚</li>
<li>é€‰æ‹©è½¬æ¢å™¨å—ä½œä¸ºä¿®å‰ªçš„åŸºæœ¬å•å…ƒï¼Œå› ä¸ºLLMä¸­å­˜åœ¨å—çº§å†—ä½™ã€‚</li>
<li>SLEBåœ¨åŠ é€ŸLLMæ¨ç†çš„åŒæ—¶ä¿æŒè¾ƒä½çš„å›°æƒ‘åº¦å’Œå‡†ç¡®æ€§ã€‚</li>
<li>SLEBä»£ç å·²å…¬å¼€åœ¨GitHubä¸Šã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2402.09025">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-04c54814472a1b6e8cf6ee38a09b55fe.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e0c590a33c418fdffedb6648c557c0a8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4486ef41a44a2fdab26fb22e5179bcb2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d93e5a09a42b6929a978045df9c71479.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-60808ce7f2af23717a88088f99fdb99a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4c214dfe2dfb3e56a6ed6e95b87c660a.jpg" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="Lying-Blindly-Bypassing-ChatGPTâ€™s-Safeguards-to-Generate-Hard-to-Detect-Disinformation-Claims"><a href="#Lying-Blindly-Bypassing-ChatGPTâ€™s-Safeguards-to-Generate-Hard-to-Detect-Disinformation-Claims" class="headerlink" title="Lying Blindly: Bypassing ChatGPTâ€™s Safeguards to Generate Hard-to-Detect   Disinformation Claims"></a>Lying Blindly: Bypassing ChatGPTâ€™s Safeguards to Generate Hard-to-Detect   Disinformation Claims</h2><p><strong>Authors:Freddy Heppell, Mehmet E. Bakir, Kalina Bontcheva</strong></p>
<p>As Large Language Models become more proficient, their misuse in coordinated disinformation campaigns is a growing concern. This study explores the capability of ChatGPT with GPT-3.5 to generate short-form disinformation claims about the war in Ukraine, both in general and on a specific event, which is beyond the GPT-3.5 knowledge cutoff. Unlike prior work, we do not provide the model with human-written disinformation narratives by including them in the prompt. Thus the generated short claims are hallucinations based on prior world knowledge and inference from the minimal prompt. With a straightforward prompting technique, we are able to bypass model safeguards and generate numerous short claims. We compare those against human-authored false claims on the war in Ukraine from ClaimReview, specifically with respect to differences in their linguistic properties. We also evaluate whether AI authorship can be differentiated by human readers or state-of-the-art authorship detection tools. Thus, we demonstrate that ChatGPT can produce realistic, target-specific disinformation claims, even on a specific post-cutoff event, and that they cannot be reliably distinguished by humans or existing automated tools. </p>
<blockquote>
<p>éšç€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLarge Language Modelsï¼ŒLLMï¼‰è¶Šæ¥è¶Šç†Ÿç»ƒï¼Œå®ƒä»¬åœ¨ååŒåˆ¶é€ è™šå‡ä¿¡æ¯è¿åŠ¨ä¸­çš„è¯¯ç”¨æˆä¸ºä¸€ä¸ªæ—¥ç›Šä¸¥é‡çš„é—®é¢˜ã€‚æœ¬ç ”ç©¶æ¢è®¨äº†ChatGPTä¸GPT-3.5ç”Ÿæˆå…³äºä¹Œå…‹å…°æˆ˜äº‰ç®€çŸ­è™šå‡ä¿¡æ¯å£°æ˜çš„èƒ½åŠ›ï¼Œæ—¢åŒ…æ‹¬ä¸€èˆ¬æ€§å†…å®¹ä¹ŸåŒ…æ‹¬ç‰¹å®šäº‹ä»¶ï¼Œè¿™äº›äº‹ä»¶è¶…å‡ºäº†GPT-3.5çš„çŸ¥è¯†èŒƒå›´ã€‚ä¸ä¹‹å‰çš„å·¥ä½œä¸åŒï¼Œæˆ‘ä»¬æ²¡æœ‰é€šè¿‡æç¤ºå‘æ¨¡å‹æä¾›äººä¸ºç¼–å†™çš„è™šå‡ä¿¡æ¯å™äº‹ã€‚å› æ­¤ï¼Œäº§ç”Ÿçš„ç®€çŸ­å£°æ˜æ˜¯åŸºäºå…ˆå‰å¯¹ä¸–ç•Œçš„äº†è§£ä»¥åŠä»æœ€å°æç¤ºæ¨æ–­å‡ºæ¥çš„å¹»è§‰ã€‚é€šè¿‡ç®€å•çš„æç¤ºæŠ€æœ¯ï¼Œæˆ‘ä»¬èƒ½å¤Ÿç»•è¿‡æ¨¡å‹ä¿æŠ¤æªæ–½å¹¶ç”Ÿæˆå¤§é‡ç®€çŸ­çš„å£°æ˜ã€‚æˆ‘ä»¬å°†è¿™äº›å£°æ˜ä¸ClaimReviewä¸­å…³äºä¹Œå…‹å…°æˆ˜äº‰çš„äººä¸ºç¼–é€ è™šå‡å£°æ˜è¿›è¡Œæ¯”è¾ƒï¼Œå°¤å…¶å…³æ³¨å®ƒä»¬åœ¨è¯­è¨€ç‰¹æ€§ä¸Šçš„å·®å¼‚ã€‚æˆ‘ä»¬è¿˜è¯„ä¼°äº†äººç±»è¯»è€…æˆ–æœ€å…ˆè¿›çš„ä½œè€…èº«ä»½æ£€æµ‹å·¥å…·æ˜¯å¦èƒ½åŒºåˆ†å‡ºAIä½œè€…çš„èº«ä»½ã€‚å› æ­¤ï¼Œæˆ‘ä»¬è¯æ˜äº†ChatGPTèƒ½å¤Ÿäº§ç”Ÿé’ˆå¯¹ç‰¹å®šç›®æ ‡çš„çœŸå®ã€æœ‰é’ˆå¯¹æ€§çš„è™šå‡ä¿¡æ¯å£°æ˜ï¼Œå³ä½¿æ˜¯å¯¹ç‰¹å®šæˆªæ–­ç‚¹ä¹‹åçš„äº‹ä»¶ä¹Ÿæ˜¯å¦‚æ­¤ï¼Œè€Œä¸”å®ƒä»¬æ— æ³•è¢«äººç±»æˆ–ç°æœ‰çš„è‡ªåŠ¨åŒ–å·¥å…·å¯é åœ°åŒºåˆ†å‡ºæ¥ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2402.08467v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆå¦‚ChatGPTï¼‰åœ¨ç”Ÿæˆå…³äºä¹Œå…‹å…°æˆ˜äº‰ç­‰ç‰¹å®šäº‹ä»¶çš„è™šå‡ä¿¡æ¯æ–¹é¢å­˜åœ¨æ½œåœ¨é£é™©ã€‚æœ¬ç ”ç©¶é€šè¿‡GPT-3.5æ¨¡å‹ç”Ÿæˆäº†åŸºäºå…ˆå‰çŸ¥è¯†å’Œæœ€å°æç¤ºæ¨æ–­çš„è™šå‡å£°æ˜ï¼Œå¹¶ä¸äººç±»åˆ›ä½œçš„ä¹Œå…‹å…°æˆ˜äº‰è™šå‡å£°æ˜è¿›è¡Œäº†æ¯”è¾ƒã€‚ç ”ç©¶ç»“æœæ˜¾ç¤ºï¼ŒChatGPTèƒ½å¤Ÿç”Ÿæˆé’ˆå¯¹ç‰¹å®šäº‹ä»¶çš„ç°å®ã€ç›®æ ‡æ˜ç¡®çš„è™šå‡ä¿¡æ¯ï¼Œä¸”éš¾ä»¥è¢«äººç±»æˆ–ç°æœ‰è‡ªåŠ¨åŒ–å·¥å…·è¯†åˆ«ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹å¦‚ChatGPTæœ‰æ½œåŠ›è¢«ç”¨äºç”Ÿæˆå…³äºç‰¹å®šäº‹ä»¶çš„è™šå‡ä¿¡æ¯ã€‚</li>
<li>GPT-3.5èƒ½å¤Ÿæ ¹æ®å…ˆå‰çš„çŸ¥è¯†å’Œä»æœ€å°æç¤ºä¸­çš„æ¨æ–­æ¥ç”Ÿæˆè™šå‡å£°æ˜ã€‚</li>
<li>ChatGPTç”Ÿæˆçš„è™šå‡å£°æ˜åœ¨è¯­è¨€å­¦ç‰¹æ€§ä¸Šä¸äººç±»åˆ›ä½œçš„è™šå‡å£°æ˜å­˜åœ¨å·®å¼‚ã€‚</li>
<li>ChatGPTç”Ÿæˆçš„è™šå‡å£°æ˜èƒ½å¤Ÿé’ˆå¯¹ç‰¹å®šäº‹ä»¶ï¼Œå³ä½¿è¿™äº›äº‹ä»¶è¶…å‡ºäº†æ¨¡å‹çš„å·²çŸ¥çŸ¥è¯†èŒƒå›´ã€‚</li>
<li>äººç±»å’Œç°æœ‰çš„è‡ªåŠ¨åŒ–å·¥å…·éš¾ä»¥åŒºåˆ†ChatGPTç”Ÿæˆçš„è™šå‡ä¿¡æ¯ã€‚</li>
<li>åœ¨ä¸å‘æ¨¡å‹æä¾›äººç±»ç¼–å†™çš„è™šå‡å™äº‹æç¤ºçš„æƒ…å†µä¸‹ï¼ŒChatGPTä¾ç„¶èƒ½å¤Ÿç”Ÿæˆè™šå‡å£°æ˜ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2402.08467">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-cae00808fca5a97447ed11f4fe5f6319.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-66432187594b444680b5902b2d671d5d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-99e9cc4da85348df3d9812cb68210f1b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f3c34eed92f90545921cdbd91d5562d2.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-340efaa2fcc0cc79259b8c8779b04a44.jpg" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2024-12-22/LLM/">https://kedreamix.github.io/Talk2Paper/Paper/2024-12-22/LLM/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/LLM/">
                                    <span class="chip bg-color">LLM</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2024-12-22/Diffusion%20Models/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-f39e47747c5ff00a9b63854b8a67416f.jpg" class="responsive-img" alt="Diffusion Models">
                        
                        <span class="card-title">Diffusion Models</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Diffusion Models æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2024-12-22  DepthFM Fast Monocular Depth Estimation with Flow Matching
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2024-12-22
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Diffusion-Models/" class="post-category">
                                    Diffusion Models
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Diffusion-Models/">
                        <span class="chip bg-color">Diffusion Models</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2024-12-21/Text-to-Motion/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-0862562aa429ce8e2434392110d13aa3.jpg" class="responsive-img" alt="Text-to-Motion">
                        
                        <span class="card-title">Text-to-Motion</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Text-to-Motion æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2024-12-21  EnergyMoGen Compositional Human Motion Generation with Energy-Based   Diffusion Model in Latent Space
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2024-12-21
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Text-to-Motion/" class="post-category">
                                    Text-to-Motion
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Text-to-Motion/">
                        <span class="chip bg-color">Text-to-Motion</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">17665k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
