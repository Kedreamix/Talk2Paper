<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="R1_Reasoning">
    <meta name="description" content="R1_Reasoning æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-04-09  InteractVLM 3D Interaction Reasoning from 2D Foundational Models">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>R1_Reasoning | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-1d6f3f8c195b241546a73c58d51f2e92.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">R1_Reasoning</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/R1-Reasoning/">
                                <span class="chip bg-color">R1_Reasoning</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/R1-Reasoning/" class="post-category">
                                R1_Reasoning
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-04-09
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-05-14
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    20.4k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    83 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-04-09-æ›´æ–°"><a href="#2025-04-09-æ›´æ–°" class="headerlink" title="2025-04-09 æ›´æ–°"></a>2025-04-09 æ›´æ–°</h1><h2 id="InteractVLM-3D-Interaction-Reasoning-from-2D-Foundational-Models"><a href="#InteractVLM-3D-Interaction-Reasoning-from-2D-Foundational-Models" class="headerlink" title="InteractVLM: 3D Interaction Reasoning from 2D Foundational Models"></a>InteractVLM: 3D Interaction Reasoning from 2D Foundational Models</h2><p><strong>Authors:Sai Kumar Dwivedi, Dimitrije AntiÄ‡, Shashank Tripathi, Omid Taheri, Cordelia Schmid, Michael J. Black, Dimitrios Tzionas</strong></p>
<p>We introduce InteractVLM, a novel method to estimate 3D contact points on human bodies and objects from single in-the-wild images, enabling accurate human-object joint reconstruction in 3D. This is challenging due to occlusions, depth ambiguities, and widely varying object shapes. Existing methods rely on 3D contact annotations collected via expensive motion-capture systems or tedious manual labeling, limiting scalability and generalization. To overcome this, InteractVLM harnesses the broad visual knowledge of large Vision-Language Models (VLMs), fine-tuned with limited 3D contact data. However, directly applying these models is non-trivial, as they reason only in 2D, while human-object contact is inherently 3D. Thus we introduce a novel Render-Localize-Lift module that: (1) embeds 3D body and object surfaces in 2D space via multi-view rendering, (2) trains a novel multi-view localization model (MV-Loc) to infer contacts in 2D, and (3) lifts these to 3D. Additionally, we propose a new task called Semantic Human Contact estimation, where human contact predictions are conditioned explicitly on object semantics, enabling richer interaction modeling. InteractVLM outperforms existing work on contact estimation and also facilitates 3D reconstruction from an in-the wild image. Code and models are available at <a target="_blank" rel="noopener" href="https://interactvlm.is.tue.mpg.de/">https://interactvlm.is.tue.mpg.de</a>. </p>
<blockquote>
<p>æˆ‘ä»¬ä»‹ç»äº†InteractVLMï¼Œè¿™æ˜¯ä¸€ç§ä»å•å¼ é‡å¤–å›¾åƒä¼°è®¡äººä½“å’Œç‰©ä½“ä¸Š3Dæ¥è§¦ç‚¹çš„æ–°å‹æ–¹æ³•ï¼Œèƒ½å¤Ÿå®ç°3Dä¸­å‡†ç¡®çš„äººä½“-ç‰©ä½“å…³èŠ‚é‡å»ºã€‚ç”±äºé®æŒ¡ã€æ·±åº¦æ­§ä¹‰å’Œç‰©ä½“å½¢çŠ¶å„å¼‚ï¼Œè¿™é¡¹å·¥ä½œå…·æœ‰æŒ‘æˆ˜æ€§ã€‚ç°æœ‰æ–¹æ³•ä¾èµ–äºé€šè¿‡æ˜‚è´µçš„åŠ¨ä½œæ•æ‰ç³»ç»Ÿæˆ–ç¹ççš„æ‰‹åŠ¨æ ‡æ³¨æ”¶é›†åˆ°çš„3Dæ¥è§¦ç‚¹æ³¨é‡Šï¼Œè¿™é™åˆ¶äº†å…¶å¯æ‰©å±•æ€§å’Œé€šç”¨æ€§ã€‚ä¸ºäº†å…‹æœè¿™ä¸€éš¾é¢˜ï¼ŒInteractVLMåˆ©ç”¨å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰çš„å¹¿æ³›è§†è§‰çŸ¥è¯†ï¼Œè¿™äº›æ¨¡å‹é€šè¿‡æœ‰é™çš„3Dæ¥è§¦æ•°æ®è¿›è¡Œå¾®è°ƒã€‚ç„¶è€Œï¼Œç›´æ¥åº”ç”¨è¿™äº›æ¨¡å‹å¹¶ä¸ç®€å•ï¼Œå› ä¸ºå®ƒä»¬åªåœ¨2Dä¸­è¿›è¡Œæ¨ç†ï¼Œè€Œäººä½“ä¸ç‰©ä½“çš„æ¥è§¦æœ¬è´¨ä¸Šæ˜¯3Dçš„ã€‚å› æ­¤ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§æ–°é¢–çš„Render-Localize-Liftæ¨¡å—ï¼Œè¯¥æ¨¡å—é€šè¿‡ä»¥ä¸‹æ–¹å¼å·¥ä½œï¼šï¼ˆ1ï¼‰é€šè¿‡å¤šè§†è§’æ¸²æŸ“å°†3Däººä½“å’Œç‰©ä½“è¡¨é¢åµŒå…¥åˆ°äºŒç»´ç©ºé—´ä¸­ï¼›ï¼ˆ2ï¼‰è®­ç»ƒä¸€ç§æ–°å‹çš„å¤šè§†è§’å®šä½æ¨¡å‹ï¼ˆMV-Locï¼‰ï¼Œä»¥åœ¨äºŒç»´ç©ºé—´ä¸­æ¨æ–­æ¥è§¦ç‚¹ï¼›ï¼ˆ3ï¼‰å°†è¿™äº›æ¥è§¦ç‚¹æå‡åˆ°ä¸‰ç»´ç©ºé—´ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªæ–°çš„ä»»åŠ¡ï¼Œç§°ä¸ºè¯­ä¹‰äººç±»æ¥è§¦ä¼°è®¡ï¼Œå…¶ä¸­äººç±»æ¥è§¦é¢„æµ‹æ˜¾å¼åœ°ä»¥å¯¹è±¡è¯­ä¹‰ä¸ºæ¡ä»¶ï¼Œä»è€Œå®ç°æ›´ä¸°å¯Œçš„äº¤äº’å»ºæ¨¡ã€‚InteractVLMåœ¨æ¥è§¦ç‚¹ä¼°è®¡æ–¹é¢è¡¨ç°ä¼˜äºç°æœ‰å·¥ä½œï¼Œå¹¶ä¿ƒè¿›äº†ä»é‡å¤–å›¾åƒè¿›è¡Œä¸‰ç»´é‡å»ºã€‚ç›¸å…³ä»£ç å’Œæ¨¡å‹å¯åœ¨<a target="_blank" rel="noopener" href="https://interactvlm.is.tue.mpg.deè®¿é—®./">https://interactvlm.is.tue.mpg.deè®¿é—®ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.05303v1">PDF</a> CVPR 2025</p>
<p><strong>Summary</strong>ï¼šæˆ‘ä»¬æå‡ºäº†ä¸€ç§åä¸ºInteractVLMçš„æ–°æ–¹æ³•ï¼Œå¯ä»¥ä»å•å¼ é‡å¤–å›¾åƒä¸­ä¼°è®¡äººä½“ä¸ç‰©ä½“çš„3Dæ¥è§¦ç‚¹ï¼Œä»è€Œå®ç°å‡†ç¡®çš„äºº-ç‰©ä½“å…³èŠ‚é‡å»ºã€‚è¯¥æ–¹æ³•é€šè¿‡åˆ©ç”¨å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰çš„å¹¿æ³›è§†è§‰çŸ¥è¯†ï¼Œå¹¶å€ŸåŠ©æœ‰é™çš„3Dæ¥è§¦æ•°æ®è¿›è¡Œå¾®è°ƒï¼Œè§£å†³äº†å› é®æŒ¡ã€æ·±åº¦æ¨¡ç³Šå’Œç‰©ä½“å½¢çŠ¶å¤šæ ·è€Œå¸¦æ¥çš„æŒ‘æˆ˜ã€‚ä¸ºå°†æ¨¡å‹åº”ç”¨äºäºº-ç‰©ä½“æ¥è§¦è¿™ä¸€å›ºæœ‰3Dé—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†Render-Localize-Liftæ¨¡å—ï¼Œé€šè¿‡å¤šè§†å›¾æ¸²æŸ“å°†3Däººä½“å’Œç‰©ä½“è¡¨é¢åµŒå…¥2Dç©ºé—´ï¼Œè®­ç»ƒæ–°å‹å¤šè§†å›¾å®šä½æ¨¡å‹ï¼ˆMV-Locï¼‰ä»¥åœ¨2Dç©ºé—´ä¸­æ¨æ–­æ¥è§¦ï¼Œå¹¶å°†å…¶æå‡åˆ°3Dç©ºé—´ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜æå‡ºäº†åŸºäºå¯¹è±¡è¯­ä¹‰çš„è¯­ä¹‰äººç±»æ¥è§¦ä¼°è®¡æ–°ä»»åŠ¡ï¼Œä½¿äººæœºäº¤äº’å»ºæ¨¡æ›´åŠ ä¸°å¯Œã€‚InteractVLMåœ¨æ¥è§¦ä¼°è®¡æ–¹é¢è¡¨ç°å‡ºè¶…è¶Šç°æœ‰å·¥ä½œçš„æ€§èƒ½ï¼Œå¹¶èƒ½å¤Ÿä»é‡å¤–å›¾åƒè¿›è¡Œ3Dé‡å»ºã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>InteractVLMæ˜¯ä¸€ç§ä»å•å¼ é‡å¤–å›¾åƒä¼°è®¡äºº-ç‰©ä½“3Dæ¥è§¦ç‚¹çš„æ–°æ–¹æ³•ã€‚</li>
<li>è¯¥æ–¹æ³•è§£å†³äº†é®æŒ¡ã€æ·±åº¦æ¨¡ç³Šå’Œç‰©ä½“å½¢çŠ¶å¤šæ ·å¸¦æ¥çš„æŒ‘æˆ˜ã€‚</li>
<li>InteractVLMåˆ©ç”¨å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹çš„å¹¿æ³›è§†è§‰çŸ¥è¯†ï¼Œå¹¶é€šè¿‡æœ‰é™çš„3Dæ¥è§¦æ•°æ®è¿›è¡Œå¾®è°ƒã€‚</li>
<li>å¼•å…¥Render-Localize-Liftæ¨¡å—è§£å†³ä»2Dåˆ°3Dçš„æ¨ç†é—®é¢˜ã€‚</li>
<li>æå‡ºæ–°å‹å¤šè§†å›¾å®šä½æ¨¡å‹ï¼ˆMV-Locï¼‰ä»¥åœ¨2Dç©ºé—´ä¸­æ¨æ–­æ¥è§¦ã€‚</li>
<li>æå‡ºåŸºäºå¯¹è±¡è¯­ä¹‰çš„è¯­ä¹‰äººç±»æ¥è§¦ä¼°è®¡æ–°ä»»åŠ¡ã€‚</li>
<li>InteractVLMåœ¨æ¥è§¦ä¼°è®¡å’Œ3Dé‡å»ºæ–¹é¢è¡¨ç°å‡ºå“è¶Šæ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.05303">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-161c23ca6cd337e073b42f800e346980.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-634bf076d43e7444a4411545fff5bfc0.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e1c4a4ca6df93569085385191d44a64e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1203d91b3ec8b7e345b5c7f20f87b8da.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Learning-to-Reason-Over-Time-Timeline-Self-Reflection-for-Improved-Temporal-Reasoning-in-Language-Models"><a href="#Learning-to-Reason-Over-Time-Timeline-Self-Reflection-for-Improved-Temporal-Reasoning-in-Language-Models" class="headerlink" title="Learning to Reason Over Time: Timeline Self-Reflection for Improved   Temporal Reasoning in Language Models"></a>Learning to Reason Over Time: Timeline Self-Reflection for Improved   Temporal Reasoning in Language Models</h2><p><strong>Authors:AdriÃ¡n Bazaga, Rexhina Blloshmi, Bill Byrne, AdriÃ  de Gispert</strong></p>
<p>Large Language Models (LLMs) have emerged as powerful tools for generating coherent text, understanding context, and performing reasoning tasks. However, they struggle with temporal reasoning, which requires processing time-related information such as event sequencing, durations, and inter-temporal relationships. These capabilities are critical for applications including question answering, scheduling, and historical analysis. In this paper, we introduce TISER, a novel framework that enhances the temporal reasoning abilities of LLMs through a multi-stage process that combines timeline construction with iterative self-reflection. Our approach leverages test-time scaling to extend the length of reasoning traces, enabling models to capture complex temporal dependencies more effectively. This strategy not only boosts reasoning accuracy but also improves the traceability of the inference process. Experimental results demonstrate state-of-the-art performance across multiple benchmarks, including out-of-distribution test sets, and reveal that TISER enables smaller open-source models to surpass larger closed-weight models on challenging temporal reasoning tasks. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å·²ç»æˆä¸ºç”Ÿæˆè¿è´¯æ–‡æœ¬ã€ç†è§£ä¸Šä¸‹æ–‡å’Œè¿›è¡Œæ¨ç†ä»»åŠ¡çš„å¼ºå¤§å·¥å…·ã€‚ç„¶è€Œï¼Œå®ƒä»¬åœ¨æ—¶é—´æ¨ç†æ–¹é¢é‡åˆ°å›°éš¾ï¼Œæ—¶é—´æ¨ç†éœ€è¦å¤„ç†ä¸æ—¶é—´ç›¸å…³çš„ä¿¡æ¯ï¼Œå¦‚äº‹ä»¶åºåˆ—ã€æŒç»­æ—¶é—´å’Œæ—¶é—´é—´å…³ç³»ã€‚è¿™äº›èƒ½åŠ›å¯¹äºé—®ç­”ã€æ—¥ç¨‹å®‰æ’å’Œå†å²åˆ†æç­‰åº”ç”¨è‡³å…³é‡è¦ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†TISERï¼Œè¿™æ˜¯ä¸€ä¸ªé€šè¿‡ç»“åˆæ—¶é—´çº¿æ„å»ºå’Œè¿­ä»£è‡ªæˆ‘åæ€çš„å¤šé˜¶æ®µè¿‡ç¨‹ï¼Œæé«˜LLMçš„æ—¶é—´æ¨ç†èƒ½åŠ›çš„æ–°å‹æ¡†æ¶ã€‚æˆ‘ä»¬çš„æ–¹æ³•åˆ©ç”¨æµ‹è¯•æ—¶ç¼©æ”¾æ¥å»¶é•¿æ¨ç†ç—•è¿¹çš„é•¿åº¦ï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿæ›´æœ‰æ•ˆåœ°æ•æ‰å¤æ‚çš„æ—¶åºä¾èµ–å…³ç³»ã€‚è¿™ç§ç­–ç•¥ä¸ä»…æé«˜äº†æ¨ç†å‡†ç¡®æ€§ï¼Œè¿˜æé«˜äº†æ¨ç†è¿‡ç¨‹çš„å¯è¿½æº¯æ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•é›†ä¸Šè¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼ŒåŒ…æ‹¬è¶…å‡ºåˆ†å¸ƒå¤–çš„æµ‹è¯•é›†ï¼Œå¹¶ä¸”æ­ç¤ºäº†TISERä½¿è¾ƒå°çš„å¼€æºæ¨¡å‹èƒ½å¤Ÿåœ¨å…·æœ‰æŒ‘æˆ˜æ€§çš„æ—¶é—´æ¨ç†ä»»åŠ¡ä¸Šè¶…è¶Šè¾ƒå¤§çš„å°é—­æƒé‡æ¨¡å‹ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.05258v1">PDF</a> </p>
<p><strong>Summary</strong><br>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨ç”Ÿæˆè¿è´¯æ–‡æœ¬ã€ç†è§£ä¸Šä¸‹æ–‡å’Œè¿›è¡Œæ¨ç†ä»»åŠ¡æ–¹é¢å±•ç°å‡ºå¼ºå¤§èƒ½åŠ›ï¼Œä½†åœ¨éœ€è¦å¤„ç†ä¸æ—¶é—´ç›¸å…³ä¿¡æ¯çš„ä¸´æ—¶æ¨ç†æ–¹é¢å­˜åœ¨å›°éš¾ï¼Œå¦‚äº‹ä»¶æ’åºã€æŒç»­æ—¶é—´å’Œæ—¶é—´é—´å…³ç³»ã€‚å¯¹äºé—®ç­”ã€æ—¥ç¨‹å®‰æ’å’Œå†å²åˆ†æç­‰åº”ç”¨ï¼Œè¿™äº›èƒ½åŠ›è‡³å…³é‡è¦ã€‚æœ¬æ–‡ä»‹ç»äº†ä¸€ä¸ªå¢å¼ºLLMä¸´æ—¶æ¨ç†èƒ½åŠ›çš„æ–°æ¡†æ¶TISERï¼Œå®ƒé‡‡ç”¨åˆ†é˜¶æ®µçš„è¿‡ç¨‹ï¼Œç»“åˆæ—¶é—´çº¿æ„å»ºå’Œè¿­ä»£è‡ªæˆ‘åæ€ã€‚è¯¥æ–¹æ³•åˆ©ç”¨æµ‹è¯•æ—¶ç¼©æ”¾ï¼Œå»¶é•¿æ¨ç†è½¨è¿¹çš„é•¿åº¦ï¼Œä½¿æ¨¡å‹æ›´æœ‰æ•ˆåœ°æ•æ‰å¤æ‚çš„ä¸´æ—¶ä¾èµ–å…³ç³»ã€‚æ­¤ç­–ç•¥ä¸ä»…æé«˜äº†æ¨ç†çš„å‡†ç¡®æ€§ï¼Œè¿˜æé«˜äº†æ¨ç†è¿‡ç¨‹çš„å¯è¿½æº¯æ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸Šè¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½æ°´å¹³ï¼ŒåŒ…æ‹¬è¶…å‡ºåˆ†å¸ƒå¼æµ‹è¯•é›†çš„æ€§èƒ½ï¼Œå¹¶ä¸”TISERè¿˜ä½¿è¾ƒå°çš„å¼€æºæ¨¡å‹èƒ½å¤Ÿåœ¨å…·æœ‰æŒ‘æˆ˜æ€§çš„ä¸´æ—¶æ¨ç†ä»»åŠ¡ä¸Šè¶…è¶Šè¾ƒå¤§çš„å°é—­æƒé‡æ¨¡å‹ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨æ–‡æœ¬ç”Ÿæˆã€ä¸Šä¸‹æ–‡ç†è§£å’Œæ¨ç†ä»»åŠ¡æ–¹é¢è¡¨ç°å‡ºå¼ºå¤§çš„èƒ½åŠ›ã€‚</li>
<li>LLMåœ¨ä¸´æ—¶æ¨ç†æ–¹é¢å­˜åœ¨æŒ‘æˆ˜ï¼Œéœ€è¦å¤„ç†æ—¶é—´ç›¸å…³ä¿¡æ¯ï¼Œå¦‚äº‹ä»¶æ’åºã€æŒç»­æ—¶é—´å’Œæ—¶é—´é—´å…³ç³»ã€‚</li>
<li>TISERæ˜¯ä¸€ä¸ªæ–°æ¡†æ¶ï¼Œæ—¨åœ¨å¢å¼ºLLMçš„ä¸´æ—¶æ¨ç†èƒ½åŠ›ã€‚</li>
<li>TISERé‡‡ç”¨åˆ†é˜¶æ®µè¿‡ç¨‹ï¼Œç»“åˆæ—¶é—´çº¿æ„å»ºå’Œè¿­ä»£è‡ªæˆ‘åæ€ã€‚</li>
<li>TISERåˆ©ç”¨æµ‹è¯•æ—¶ç¼©æ”¾æ¥å»¶é•¿æ¨ç†è½¨è¿¹é•¿åº¦ï¼Œæé«˜æ•æ‰å¤æ‚ä¸´æ—¶ä¾èµ–å…³ç³»çš„èƒ½åŠ›ã€‚</li>
<li>TISERæé«˜äº†æ¨ç†å‡†ç¡®æ€§å’Œæ¨ç†è¿‡ç¨‹çš„å¯è¿½æº¯æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.05258">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-6a4610db5cfa1b93993903eaeee03337.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1d6f3f8c195b241546a73c58d51f2e92.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a187b7435ca923e1c73adbbc9aa89a76.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-1daea595de2bcda898c173f53c8d1f36.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-2601bb03ffecc21e904b67cf5ac1b23e.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="FinGrAct-A-Framework-for-FINe-GRrained-Evaluation-of-ACTionability-in-Explainable-Automatic-Fact-Checking"><a href="#FinGrAct-A-Framework-for-FINe-GRrained-Evaluation-of-ACTionability-in-Explainable-Automatic-Fact-Checking" class="headerlink" title="FinGrAct: A Framework for FINe-GRrained Evaluation of ACTionability in   Explainable Automatic Fact-Checking"></a>FinGrAct: A Framework for FINe-GRrained Evaluation of ACTionability in   Explainable Automatic Fact-Checking</h2><p><strong>Authors:Islam Eldifrawi, Shengrui Wang, Amine Trabelsi</strong></p>
<p>The field of explainable Automatic Fact-Checking (AFC) aims to enhance the transparency and trustworthiness of automated fact-verification systems by providing clear and comprehensible explanations. However, the effectiveness of these explanations depends on their actionability â€“their ability to empower users to make informed decisions and mitigate misinformation. Despite actionability being a critical property of high-quality explanations, no prior research has proposed a dedicated method to evaluate it. This paper introduces FinGrAct, a fine-grained evaluation framework that can access the web, and it is designed to assess actionability in AFC explanations through well-defined criteria and an evaluation dataset. FinGrAct surpasses state-of-the-art (SOTA) evaluators, achieving the highest Pearson and Kendall correlation with human judgments while demonstrating the lowest ego-centric bias, making it a more robust evaluation approach for actionability evaluation in AFC. </p>
<blockquote>
<p>å¯è§£é‡Šæ€§è‡ªåŠ¨äº‹å®æ ¸æŸ¥ï¼ˆAFCï¼‰é¢†åŸŸæ—¨åœ¨é€šè¿‡æä¾›æ¸…æ™°ä¸”æ˜“äºç†è§£çš„è§£é‡Šæ¥æé«˜è‡ªåŠ¨äº‹å®æ ¸æŸ¥ç³»ç»Ÿçš„é€æ˜åº¦å’Œå¯ä¿¡åº¦ã€‚ç„¶è€Œï¼Œè¿™äº›è§£é‡Šçš„æœ‰æ•ˆæ€§å–å†³äºå®ƒä»¬çš„å¯æ“ä½œæ€§â€”â€”å³å®ƒä»¬å¸®åŠ©ç”¨æˆ·åšå‡ºæ˜æ™ºå†³ç­–å’Œç¼“è§£å‡ä¿¡æ¯çš„èƒ½åŠ›ã€‚å°½ç®¡å¯æ“ä½œæ€§æ˜¯é«˜è´¨é‡è§£é‡Šçš„å…³é”®å±æ€§ï¼Œä½†ä¹‹å‰çš„ç ”ç©¶å¹¶æ²¡æœ‰æå‡ºä¸“é—¨çš„æ–¹æ³•æ¥è¯„ä¼°å®ƒã€‚æœ¬æ–‡ä»‹ç»äº†FineGrActï¼Œè¿™æ˜¯ä¸€ä¸ªå¯ä»¥è®¿é—®ç½‘é¡µçš„ç²¾ç»†è¯„ä¼°æ¡†æ¶ï¼Œæ—¨åœ¨é€šè¿‡æ˜ç¡®å®šä¹‰çš„æ ‡å‡†å’Œè¯„ä¼°æ•°æ®é›†æ¥è¯„ä¼°AFCè§£é‡Šä¸­çš„å¯æ“ä½œæ€§ã€‚FineGrActè¶…è¶Šäº†æœ€å…ˆè¿›çš„è¯„ä¼°è€…ï¼Œå®ç°äº†ä¸äººå·¥åˆ¤æ–­çš„æœ€é«˜Pearsonå’ŒKendallç›¸å…³æ€§ï¼ŒåŒæ—¶è¡¨ç°å‡ºäº†æœ€ä½çš„ä»¥è‡ªæˆ‘ä¸ºä¸­å¿ƒåè§ï¼Œä½¿å…¶æˆä¸ºAFCä¸­å¯æ“ä½œæ€§è¯„ä¼°çš„æ›´ç¨³å¥çš„è¯„ä¼°æ–¹æ³•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.05229v1">PDF</a> </p>
<p><strong>Summary</strong><br>æœ¬æ–‡ä»‹ç»äº†è‡ªåŠ¨äº‹å®æ ¸æŸ¥ï¼ˆAFCï¼‰é¢†åŸŸçš„ä¸€ä¸ªç ”ç©¶ç›®æ ‡ï¼Œå³æé«˜è‡ªåŠ¨åŒ–äº‹å®æ ¸æŸ¥ç³»ç»Ÿçš„é€æ˜åº¦å’Œå¯ä¿¡åº¦ã€‚ä¸ºæ­¤ï¼Œç ”ç©¶è€…ä»¬éœ€è¦å¼€å‘æ¸…æ™°æ˜“æ‡‚ä¸”æ˜“äºè¡ŒåŠ¨çš„è§£é‡Šã€‚ç„¶è€Œï¼Œç°æœ‰çš„ç ”ç©¶å°šæœªæå‡ºä¸“é—¨çš„æ–¹æ³•æ¥è¯„ä¼°è¿™äº›è§£é‡Šçš„å¯è¡Œæ€§ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§ç²¾ç»†çš„è¯„ä¼°æ¡†æ¶FinGrActï¼Œæ—¨åœ¨é€šè¿‡æ˜ç¡®çš„æ ‡å‡†å’Œè¯„ä¼°æ•°æ®é›†æ¥è¯„ä¼°AFCè§£é‡Šçš„å¯è¡Œæ€§ã€‚è¯¥æ¡†æ¶èƒ½å¤Ÿè®¿é—®ç½‘ç»œï¼Œå¹¶ä¸”ç›¸è¾ƒäºç°æœ‰çš„è¯„ä¼°å™¨ï¼Œå®ƒå®ç°äº†æ›´é«˜çš„Pearsonå’ŒKendallç›¸å…³æ€§ï¼ŒåŒæ—¶æ˜¾ç¤ºå‡ºæœ€ä½çš„è‡ªæˆ‘ä¸­å¿ƒåè§ï¼Œä½¿å…¶æˆä¸ºAFCä¸­è¯„ä¼°å¯è¡Œæ€§çš„æ›´ç¨³å¥çš„æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è‡ªåŠ¨äº‹å®æ ¸æŸ¥ï¼ˆAFCï¼‰çš„ç›®æ ‡æ˜¯å¢å¼ºè‡ªåŠ¨åŒ–äº‹å®éªŒè¯ç³»ç»Ÿçš„é€æ˜åº¦å’Œå¯ä¿¡åº¦ã€‚</li>
<li>è¯„ä¼°AFCè§£é‡Šçš„æœ‰æ•ˆæ€§å…³é”®åœ¨äºå…¶å¯è¡Œæ€§ï¼Œå³è§£é‡Šèƒ½å¦å¸®åŠ©ç”¨æˆ·åšå‡ºæ˜æ™ºçš„å†³ç­–å¹¶å‡å°‘è¯¯å¯¼ä¿¡æ¯ã€‚</li>
<li>å½“å‰ç¼ºä¹ä¸“é—¨çš„æ–¹æ³•æ¥è¯„ä¼°AFCè§£é‡Šçš„å¯è¡Œæ€§ã€‚</li>
<li>FinGrActæ˜¯ä¸€ä¸ªç”¨äºè¯„ä¼°AFCè§£é‡Šå¯è¡Œæ€§çš„ç²¾ç»†è¯„ä¼°æ¡†æ¶ã€‚</li>
<li>FinGrActå¯ä»¥è®¿é—®ç½‘ç»œå¹¶ä½¿ç”¨æ˜ç¡®çš„æ ‡å‡†å’Œè¯„ä¼°æ•°æ®é›†æ¥è¯„ä¼°è§£é‡Šçš„å¯è¡Œæ€§ã€‚</li>
<li>FinGrActå®ç°äº†é«˜ç›¸å…³æ€§å¹¶ä¸äººç±»åˆ¤æ–­ç›¸æ¯”æ˜¾ç¤ºå‡ºè¾ƒä½çš„è‡ªæˆ‘ä¸­å¿ƒåè§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.05229">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-8e738dba03d4102a7faa06c06a23cf2c.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-1745af89f91cd51d97568829da8d4600.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-74e84e321a9cb5d79c24167060f57996.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f8044cd0f595c03953cb5564bae2f6ab.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-30ad782f62194a06f6f6b6b09be8c543.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="VAPO-Efficient-and-Reliable-Reinforcement-Learning-for-Advanced-Reasoning-Tasks"><a href="#VAPO-Efficient-and-Reliable-Reinforcement-Learning-for-Advanced-Reasoning-Tasks" class="headerlink" title="VAPO: Efficient and Reliable Reinforcement Learning for Advanced   Reasoning Tasks"></a>VAPO: Efficient and Reliable Reinforcement Learning for Advanced   Reasoning Tasks</h2><p><strong>Authors: YuYue, Yufeng Yuan, Qiying Yu, Xiaochen Zuo, Ruofei Zhu, Wenyuan Xu, Jiaze Chen, Chengyi Wang, TianTian Fan, Zhengyin Du, Xiangpeng Wei, Gaohong Liu, Juncai Liu, Lingjun Liu, Haibin Lin, Zhiqi Lin, Bole Ma, Chi Zhang, Mofan Zhang, Wang Zhang, Hang Zhu, Ru Zhang, Xin Liu, Mingxuan Wang, Yonghui Wu, Lin Yan</strong></p>
<p>We present VAPO, Value-based Augmented Proximal Policy Optimization framework for reasoning models., a novel framework tailored for reasoning models within the value-based paradigm. Benchmarked the AIME 2024 dataset, VAPO, built on the Qwen 32B pre-trained model, attains a state-of-the-art score of $\mathbf{60.4}$. In direct comparison under identical experimental settings, VAPO outperforms the previously reported results of DeepSeek-R1-Zero-Qwen-32B and DAPO by more than 10 points. The training process of VAPO stands out for its stability and efficiency. It reaches state-of-the-art performance within a mere 5,000 steps. Moreover, across multiple independent runs, no training crashes occur, underscoring its reliability. This research delves into long chain-of-thought (long-CoT) reasoning using a value-based reinforcement learning framework. We pinpoint three key challenges that plague value-based methods: value model bias, the presence of heterogeneous sequence lengths, and the sparsity of reward signals. Through systematic design, VAPO offers an integrated solution that effectively alleviates these challenges, enabling enhanced performance in long-CoT reasoning tasks. </p>
<blockquote>
<p>æˆ‘ä»¬æå‡ºäº†VAPOï¼Œå³åŸºäºä»·å€¼çš„å¢å¼ºè¿‘ç«¯ç­–ç•¥ä¼˜åŒ–æ¡†æ¶ï¼ˆValue-based Augmented Proximal Policy Optimization frameworkï¼‰ï¼Œè¿™æ˜¯ä¸€ä¸ªé’ˆå¯¹åŸºäºä»·å€¼çš„èŒƒå¼ä¸­çš„æ¨ç†æ¨¡å‹çš„æ–°å‹æ¡†æ¶ã€‚ä½¿ç”¨AIME 2024æ•°æ®é›†ä½œä¸ºåŸºå‡†æµ‹è¯•ï¼ŒVAPOå»ºç«‹åœ¨Qwen 32Bé¢„è®­ç»ƒæ¨¡å‹ä¹‹ä¸Šï¼Œè¾¾åˆ°äº†æœ€å…ˆè¿›çš„60.4åˆ†ã€‚åœ¨ç›¸åŒçš„å®éªŒè®¾ç½®ä¸‹ç›´æ¥æ¯”è¾ƒï¼ŒVAPOæ¯”å…ˆå‰æŠ¥é“çš„DeepSeek-R1-Zero-Qwen-32Bå’ŒDAPOçš„ç»“æœé«˜å‡ºè¶…è¿‡10åˆ†ã€‚VAPOçš„è®­ç»ƒè¿‡ç¨‹ä»¥ç¨³å®šæ€§å’Œæ•ˆç‡è€Œçªå‡ºã€‚å®ƒä»…åœ¨5000æ­¥å†…å°±è¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚æ­¤å¤–ï¼Œåœ¨å¤šæ¬¡ç‹¬ç«‹è¿è¡Œä¸­ï¼Œæ²¡æœ‰å‘ç”Ÿè®­ç»ƒå´©æºƒï¼Œè¿™è¯æ˜äº†å…¶å¯é æ€§ã€‚æœ¬ç ”ç©¶æ·±å…¥æ¢è®¨äº†åŸºäºä»·å€¼çš„å¼ºåŒ–å­¦ä¹ æ¡†æ¶ä¸­çš„é•¿é“¾æ€ç»´ï¼ˆlong-CoTï¼‰æ¨ç†ã€‚æˆ‘ä»¬ç¡®å®šäº†å›°æ‰°åŸºäºä»·å€¼çš„æ–¹æ³•çš„ä¸‰ä¸ªå…³é”®æŒ‘æˆ˜ï¼šä»·å€¼æ¨¡å‹åè§ã€åºåˆ—é•¿åº¦å¼‚è´¨æ€§çš„å­˜åœ¨å’Œå¥–åŠ±ä¿¡å·ç¨€ç–ã€‚é€šè¿‡ç³»ç»Ÿè®¾è®¡ï¼ŒVAPOæä¾›äº†ä¸€ä¸ªç»¼åˆè§£å†³æ–¹æ¡ˆï¼Œæœ‰æ•ˆåœ°ç¼“è§£äº†è¿™äº›æŒ‘æˆ˜ï¼Œä»è€Œåœ¨é•¿é“¾æ€ç»´æ¨ç†ä»»åŠ¡ä¸­å®ç°äº†å¢å¼ºçš„æ€§èƒ½ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.05118v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>VAPOï¼Œä¸€ç§é’ˆå¯¹ä»·å€¼åŸºå‡†æ¨¡å‹æ¨ç†çš„æ–°å‹æ¡†æ¶ï¼Œå®ç°äº†AIME 2024æ•°æ®é›†ä¸Šçš„å“è¶Šè¡¨ç°ã€‚åŸºäºQwen 32Bé¢„è®­ç»ƒæ¨¡å‹ï¼ŒVAPOè·å¾—å‰æ‰€æœªæœ‰çš„é«˜åˆ†60.4ã€‚åœ¨ç›¸åŒå®éªŒç¯å¢ƒä¸‹ï¼ŒVAPOç›¸è¾ƒäºDeepSeek-R1-Zero-Qwen-32Bå’ŒDAPOæœ‰è¶…å‡º10åˆ†çš„æ˜¾è‘—ä¼˜åŠ¿ã€‚å…¶è®­ç»ƒè¿‡ç¨‹ç¨³å®šé«˜æ•ˆï¼Œ5000æ­¥å†…è¾¾æˆé¡¶å°–æ€§èƒ½ï¼Œä¸”å¤šæ¬¡ç‹¬ç«‹è¿è¡Œæ— å´©æºƒï¼Œè¡¨ç°å‡ºå¯é æ€§ã€‚æœ¬ç ”ç©¶æ·±å…¥æ¢è®¨äº†ä»·å€¼åŸºå‡†å¼ºåŒ–å­¦ä¹ æ¡†æ¶ä¸‹çš„é•¿é“¾æ€ç»´æ¨ç†ï¼Œå¹¶æŒ‡å‡ºäº†ä»·å€¼æ¨¡å‹åå·®ã€åºåˆ—é•¿åº¦å¼‚è´¨æ€§å’Œå¥–åŠ±ä¿¡å·ç¨€ç–æ€§ä¸‰å¤§æŒ‘æˆ˜ã€‚VAPOé€šè¿‡ç³»ç»Ÿè®¾è®¡æä¾›äº†ç»¼åˆè§£å†³æ–¹æ¡ˆï¼Œæœ‰æ•ˆåº”å¯¹è¿™äº›æŒ‘æˆ˜ï¼Œæå‡é•¿é“¾æ€ç»´æ¨ç†ä»»åŠ¡æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>VAPOæ˜¯ä¸“é—¨ä¸ºä»·å€¼åŸºå‡†æ¨¡å‹æ¨ç†è®¾è®¡çš„æ–°å‹æ¡†æ¶ã€‚</li>
<li>åœ¨AIME 2024æ•°æ®é›†ä¸Šï¼ŒVAPOå–å¾—äº†çªç ´æ€§çš„æˆç»©ï¼Œè¾¾åˆ°60.4åˆ†ã€‚</li>
<li>åœ¨ç›¸åŒå®éªŒç¯å¢ƒä¸‹ï¼ŒVAPOæ˜¾è‘—ä¼˜äºå…¶ä»–æ¨¡å‹ï¼Œå¦‚DeepSeek-R1-Zero-Qwen-32Bå’ŒDAPOã€‚</li>
<li>VAPOè®­ç»ƒè¿‡ç¨‹ç¨³å®šé«˜æ•ˆï¼Œèƒ½åœ¨çŸ­æ—¶é—´å†…è¾¾åˆ°é¡¶å°–æ€§èƒ½ã€‚</li>
<li>VAPOå¤šæ¬¡ç‹¬ç«‹è¿è¡Œæ— å´©æºƒï¼Œè¡¨ç°å‡ºé«˜åº¦å¯é æ€§ã€‚</li>
<li>ç ”ç©¶æŒ‡å‡ºäº†ä»·å€¼åŸºå‡†æ¨¡å‹é¢ä¸´çš„ä¸‰å¤§æŒ‘æˆ˜ï¼šä»·å€¼æ¨¡å‹åå·®ã€åºåˆ—é•¿åº¦å¼‚è´¨æ€§å’Œå¥–åŠ±ä¿¡å·ç¨€ç–æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.05118">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-df3454ba96bd4fe79a92f1cb97185785.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Algorithm-Discovery-With-LLMs-Evolutionary-Search-Meets-Reinforcement-Learning"><a href="#Algorithm-Discovery-With-LLMs-Evolutionary-Search-Meets-Reinforcement-Learning" class="headerlink" title="Algorithm Discovery With LLMs: Evolutionary Search Meets Reinforcement   Learning"></a>Algorithm Discovery With LLMs: Evolutionary Search Meets Reinforcement   Learning</h2><p><strong>Authors:Anja Surina, Amin Mansouri, Lars Quaedvlieg, Amal Seddas, Maryna Viazovska, Emmanuel Abbe, Caglar Gulcehre</strong></p>
<p>Discovering efficient algorithms for solving complex problems has been an outstanding challenge in mathematics and computer science, requiring substantial human expertise over the years. Recent advancements in evolutionary search with large language models (LLMs) have shown promise in accelerating the discovery of algorithms across various domains, particularly in mathematics and optimization. However, existing approaches treat the LLM as a static generator, missing the opportunity to update the model with the signal obtained from evolutionary exploration. In this work, we propose to augment LLM-based evolutionary search by continuously refining the search operator - the LLM - through reinforcement learning (RL) fine-tuning. Our method leverages evolutionary search as an exploration strategy to discover improved algorithms, while RL optimizes the LLM policy based on these discoveries. Our experiments on three combinatorial optimization tasks - bin packing, traveling salesman, and the flatpack problem - show that combining RL and evolutionary search improves discovery efficiency of improved algorithms, showcasing the potential of RL-enhanced evolutionary strategies to assist computer scientists and mathematicians for more efficient algorithm design. </p>
<blockquote>
<p>å‘ç°è§£å†³å¤æ‚é—®é¢˜çš„æœ‰æ•ˆç®—æ³•ä¸€ç›´æ˜¯æ•°å­¦å’Œè®¡ç®—æœºç§‘å­¦é¢†åŸŸçš„ä¸€é¡¹é‡å¤§æŒ‘æˆ˜ï¼Œéœ€è¦å¤šå¹´çš„äººç±»ä¸“ä¸šçŸ¥è¯†ã€‚æœ€è¿‘ä½¿ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„è¿›åŒ–æœç´¢çš„è¿›å±•è¡¨æ˜ï¼Œåœ¨å„ä¸ªé¢†åŸŸåŠ é€Ÿç®—æ³•å‘ç°æ–¹é¢å­˜åœ¨å¸Œæœ›ï¼Œç‰¹åˆ«æ˜¯åœ¨æ•°å­¦å’Œä¼˜åŒ–æ–¹é¢ã€‚ç„¶è€Œï¼Œç°æœ‰æ–¹æ³•å°†LLMè§†ä¸ºé™æ€ç”Ÿæˆå™¨ï¼Œé”™è¿‡äº†é€šè¿‡è¿›åŒ–æ¢ç´¢è·å¾—çš„ä¿¡å·æ›´æ–°æ¨¡å‹çš„æœºä¼šã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æè®®é€šè¿‡å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰å¾®è°ƒæ¥ä¸æ–­ç²¾ç‚¼æœç´¢è¿ç®—ç¬¦â€”â€”LLMï¼Œä»¥å¢å¼ºåŸºäºLLMçš„è¿›åŒ–æœç´¢ã€‚æˆ‘ä»¬çš„æ–¹æ³•åˆ©ç”¨è¿›åŒ–æœç´¢ä½œä¸ºæ¢ç´¢ç­–ç•¥æ¥å‘ç°æ”¹è¿›åçš„ç®—æ³•ï¼Œè€Œå¼ºåŒ–å­¦ä¹ åˆ™æ ¹æ®è¿™äº›å‘ç°ä¼˜åŒ–LLMç­–ç•¥ã€‚æˆ‘ä»¬åœ¨ä¸‰é¡¹ç»„åˆä¼˜åŒ–ä»»åŠ¡â€”â€”è£…ç®±é—®é¢˜ã€æ—…è¡Œæ¨é”€å‘˜é—®é¢˜å’Œå¹³æ¿åŒ…è£…é—®é¢˜ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œå°†å¼ºåŒ–å­¦ä¹ ä¸è¿›åŒ–æœç´¢ç›¸ç»“åˆï¼Œæé«˜äº†å‘ç°æ”¹è¿›ç®—æ³•çš„æ•ˆç‡ï¼Œå±•ç¤ºäº†å¼ºåŒ–å­¦ä¹ å¢å¼ºå‹è¿›åŒ–ç­–ç•¥åœ¨å¸®åŠ©è®¡ç®—æœºç§‘å­¦å®¶å’Œæ•°å­¦å®¶è¿›è¡Œæ›´æœ‰æ•ˆç‡åœ°ç®—æ³•è®¾è®¡æ–¹é¢çš„æ½œåŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.05108v1">PDF</a> 30 pages</p>
<p><strong>Summary</strong>ï¼šè¿‘æœŸï¼Œè¿›åŒ–æœç´¢ä¸å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„èåˆå±•ç°å‡ºå¯¹åŠ é€Ÿç®—æ³•å‘ç°çš„æ½œåŠ›ï¼Œå°¤å…¶åœ¨æ•°å­¦å’Œä¼˜åŒ–é¢†åŸŸã€‚ç„¶è€Œï¼Œç°æœ‰æ–¹æ³•å°†LLMè§†ä¸ºé™æ€ç”Ÿæˆå™¨ï¼Œå¿½ç•¥äº†é€šè¿‡è¿›åŒ–æ¢ç´¢è·å¾—çš„ä¿¡å·æ¥æ›´æ–°æ¨¡å‹çš„æœºä¼šã€‚æœ¬ç ”ç©¶æå‡ºé€šè¿‡å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰å¾®è°ƒæ¥æŒç»­å®Œå–„æœç´¢è¿ç®—ç¬¦LLMï¼Œä»è€Œå¢å¼ºLLMåŸºç¡€çš„è¿›åŒ–æœç´¢ã€‚å®éªŒè¡¨æ˜ï¼ŒRLå’Œè¿›åŒ–æœç´¢çš„ç»“åˆæé«˜äº†å‘ç°æ”¹è¿›ç®—æ³•çš„æ•ˆç‡ï¼Œå±•ç¤ºäº†RLå¢å¼ºè¿›åŒ–ç­–ç•¥åœ¨ååŠ©è®¡ç®—æœºç§‘å­¦å®¶å’Œæ•°å­¦å®¶è¿›è¡Œæ›´é«˜æ•ˆç®—æ³•è®¾è®¡æ–¹é¢çš„æ½œåŠ›ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>è¿›åŒ–æœç´¢ä¸å¤§å‹è¯­è¨€æ¨¡å‹çš„èåˆä¸ºç®—æ³•å‘ç°æä¾›äº†æ–°çš„æ½œåŠ›ï¼Œç‰¹åˆ«æ˜¯åœ¨æ•°å­¦å’Œä¼˜åŒ–é¢†åŸŸã€‚</li>
<li>ç°æœ‰æ–¹æ³•ä¸»è¦å°†å¤§å‹è¯­è¨€æ¨¡å‹è§†ä¸ºé™æ€ç”Ÿæˆå™¨ï¼Œæœªèƒ½å……åˆ†åˆ©ç”¨è¿›åŒ–æ¢ç´¢çš„ä¿¡å·æ¥æ›´æ–°æ¨¡å‹ã€‚</li>
<li>å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰å¯ä»¥ç”¨äºå¾®è°ƒå¤§å‹è¯­è¨€æ¨¡å‹ï¼Œä»¥æŒç»­æå‡æœç´¢æ€§èƒ½ã€‚</li>
<li>é€šè¿‡ç»“åˆRLå’Œè¿›åŒ–æœç´¢ï¼Œå‘ç°æ”¹è¿›ç®—æ³•çš„æ•ˆç‡å¾—åˆ°äº†æé«˜ã€‚</li>
<li>å®éªŒåœ¨ç»„åˆä¼˜åŒ–ä»»åŠ¡ä¸ŠéªŒè¯äº†æ‰€æå‡ºæ–¹æ³•çš„æœ‰æ•ˆæ€§ï¼ŒåŒ…æ‹¬è£…ç®±é—®é¢˜ã€æ—…è¡Œæ¨é”€å‘˜é—®é¢˜å’Œflatpacké—®é¢˜ã€‚</li>
<li>RLå¢å¼ºè¿›åŒ–ç­–ç•¥åœ¨ååŠ©è®¡ç®—æœºç§‘å­¦å®¶å’Œæ•°å­¦å®¶è¿›è¡Œæ›´é«˜æ•ˆç®—æ³•è®¾è®¡æ–¹é¢å±•ç°å‡ºæ½œåŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.05108">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-a69965aa04b780f9c758c3fe98890887.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5c5cdaad98384c81391c81f53987a5ba.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c4c088176936a53f67b25fe0509fd8ba.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="AI-for-Climate-Finance-Agentic-Retrieval-and-Multi-Step-Reasoning-for-Early-Warning-System-Investments"><a href="#AI-for-Climate-Finance-Agentic-Retrieval-and-Multi-Step-Reasoning-for-Early-Warning-System-Investments" class="headerlink" title="AI for Climate Finance: Agentic Retrieval and Multi-Step Reasoning for   Early Warning System Investments"></a>AI for Climate Finance: Agentic Retrieval and Multi-Step Reasoning for   Early Warning System Investments</h2><p><strong>Authors:Saeid Ario Vaghefi, Aymane Hachcham, Veronica Grasso, Jiska Manicus, Nakiete Msemo, Chiara Colesanti Senni, Markus Leippold</strong></p>
<p>Tracking financial investments in climate adaptation is a complex and expertise-intensive task, particularly for Early Warning Systems (EWS), which lack standardized financial reporting across multilateral development banks (MDBs) and funds. To address this challenge, we introduce an LLM-based agentic AI system that integrates contextual retrieval, fine-tuning, and multi-step reasoning to extract relevant financial data, classify investments, and ensure compliance with funding guidelines. Our study focuses on a real-world application: tracking EWS investments in the Climate Risk and Early Warning Systems (CREWS) Fund. We analyze 25 MDB project documents and evaluate multiple AI-driven classification methods, including zero-shot and few-shot learning, fine-tuned transformer-based classifiers, chain-of-thought (CoT) prompting, and an agent-based retrieval-augmented generation (RAG) approach. Our results show that the agent-based RAG approach significantly outperforms other methods, achieving 87% accuracy, 89% precision, and 83% recall. Additionally, we contribute a benchmark dataset and expert-annotated corpus, providing a valuable resource for future research in AI-driven financial tracking and climate finance transparency. </p>
<blockquote>
<p>è¿½è¸ªæ°”å€™é€‚åº”é¢†åŸŸçš„é‡‘èæŠ•èµ„æ˜¯ä¸€é¡¹å¤æ‚ä¸”éœ€è¦ä¸“ä¸šæŠ€èƒ½çš„ä»»åŠ¡ï¼Œç‰¹åˆ«æ˜¯å¯¹äºç¼ºä¹å¤šè¾¹å‘å±•é“¶è¡Œå’ŒåŸºé‡‘æ ‡å‡†åŒ–è´¢åŠ¡æŠ¥å‘Šçš„æ—©æœŸé¢„è­¦ç³»ç»Ÿï¼ˆEWSï¼‰è€Œè¨€ã€‚ä¸ºäº†åº”å¯¹è¿™ä¸€æŒ‘æˆ˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ªåŸºäºå¤§å‹è¯­è¨€æ¨¡å‹çš„æ™ºèƒ½AIç³»ç»Ÿï¼Œè¯¥ç³»ç»Ÿç»“åˆäº†ä¸Šä¸‹æ–‡æ£€ç´¢ã€å¾®è°ƒä»¥åŠå¤šæ­¥éª¤æ¨ç†ï¼Œä»¥æå–ç›¸å…³è´¢åŠ¡æ•°æ®ã€åˆ†ç±»æŠ•èµ„å¹¶ç¡®ä¿ç¬¦åˆèµ„é‡‘æŒ‡å¯¼æ–¹é’ˆã€‚æˆ‘ä»¬çš„ç ”ç©¶å…³æ³¨ç°å®åº”ç”¨ï¼šè¿½è¸ªæ°”å€™é£é™©ä¸æ—©æœŸé¢„è­¦ç³»ç»Ÿï¼ˆCREWSï¼‰åŸºé‡‘ä¸­çš„EWSæŠ•èµ„ã€‚æˆ‘ä»¬åˆ†æäº†25ä¸ªå¤šè¾¹å‘å±•é“¶è¡Œçš„é¡¹ç›®æ–‡ä»¶ï¼Œå¹¶è¯„ä¼°äº†å¤šç§AIé©±åŠ¨çš„åˆ†ç±»æ–¹æ³•ï¼ŒåŒ…æ‹¬é›¶æ ·æœ¬å’Œå°‘æ ·æœ¬å­¦ä¹ ã€å¾®è°ƒåŸºäºè½¬æ¢å™¨çš„åˆ†ç±»å™¨ã€æ€ç»´é“¾æç¤ºä»¥åŠåŸºäºä»£ç†çš„æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰æ–¹æ³•ã€‚æˆ‘ä»¬çš„ç»“æœè¡¨æ˜ï¼ŒåŸºäºä»£ç†çš„RAGæ–¹æ³•æ˜¾è‘—ä¼˜äºå…¶ä»–æ–¹æ³•ï¼Œè¾¾åˆ°äº†87%çš„å‡†ç¡®ç‡ã€89%çš„ç²¾ç¡®ç‡å’Œ80%çš„å¬å›ç‡ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜æä¾›äº†åŸºå‡†æ•°æ®é›†å’Œä¸“å®¶æ³¨é‡Šè¯­æ–™åº“ï¼Œä¸ºAIé©±åŠ¨çš„é‡‘èè¿½è¸ªå’Œæ°”å€™é‡‘èé€æ˜åº¦çš„æœªæ¥ç ”ç©¶æä¾›äº†å®è´µèµ„æºã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.05104v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸€ä¸ªåŸºäºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ™ºèƒ½åŒ–AIç³»ç»Ÿï¼Œç”¨äºè¿½è¸ªæ°”å€™é€‚åº”æŠ•èµ„ã€‚è¯¥ç³»ç»Ÿèåˆäº†è¯­å¢ƒæ£€ç´¢ã€å¾®è°ƒæŠ€æœ¯å’Œå¤šæ­¥éª¤æ¨ç†ï¼Œèƒ½å¤Ÿæå–ç›¸å…³è´¢åŠ¡æ•°æ®ã€åˆ†ç±»æŠ•èµ„å¹¶ç¡®ä¿ç¬¦åˆèµ„é‡‘æŒ‡å¯¼æ–¹é’ˆã€‚ç ”ç©¶ä»¥æ°”å€™é£é™©å’Œé¢„è­¦ç³»ç»Ÿï¼ˆCREWSï¼‰åŸºé‡‘ä¸­çš„æ—©æœŸé¢„è­¦ç³»ç»Ÿï¼ˆEWSï¼‰æŠ•èµ„è¿½è¸ªä¸ºä¾‹ï¼Œåˆ†æäº†25ä¸ªå¤šè¾¹å‘å±•é“¶è¡Œé¡¹ç›®æ–‡ä»¶ï¼Œå¹¶è¯„ä¼°äº†å¤šç§AIåˆ†ç±»æ–¹æ³•ã€‚ç»“æœè¡¨æ˜ï¼ŒåŸºäºä»£ç†çš„RAGæ–¹æ³•è¡¨ç°æœ€ä½³ï¼Œå‡†ç¡®ç‡ã€ç²¾ç¡®åº¦å’Œå¬å›ç‡åˆ†åˆ«è¾¾åˆ°äº†87%ã€89%å’Œ83%ã€‚åŒæ—¶ï¼Œæ–‡ç« è¿˜è´¡çŒ®äº†ä¸€ä¸ªåŸºå‡†æ•°æ®é›†å’Œä¸“å®¶æ³¨é‡Šè¯­æ–™åº“ï¼Œä¸ºæœªæ¥AIé©±åŠ¨è´¢åŠ¡è¿½è¸ªå’Œæ°”å€™é‡‘èé€æ˜åº¦ç ”ç©¶æä¾›äº†å®è´µèµ„æºã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLM-based AIç³»ç»Ÿè¢«ç”¨äºè¿½è¸ªæ°”å€™é€‚åº”æŠ•èµ„ï¼Œè¯¥ç³»ç»Ÿé›†æˆäº†è¯­å¢ƒæ£€ç´¢ã€å¾®è°ƒæŠ€æœ¯å’Œå¤šæ­¥éª¤æ¨ç†ã€‚</li>
<li>ç³»ç»Ÿèƒ½å¤Ÿæå–ç›¸å…³è´¢åŠ¡æ•°æ®ã€åˆ†ç±»æŠ•èµ„å¹¶ç¡®ä¿ä¸èµ„é‡‘æŒ‡å¯¼æ–¹é’ˆç›¸ç¬¦ã€‚</li>
<li>ä»¥æ°”å€™é£é™©å’Œé¢„è­¦ç³»ç»Ÿï¼ˆCREWSï¼‰åŸºé‡‘ä¸­çš„EWSæŠ•èµ„è¿½è¸ªä¸ºä¾‹ï¼Œè¿›è¡Œäº†å®è¯ç ”ç©¶ã€‚</li>
<li>åˆ†ææ¶‰åŠ25ä¸ªå¤šè¾¹å‘å±•é“¶è¡Œé¡¹ç›®æ–‡ä»¶ï¼Œå¹¶è¯„ä¼°äº†å¤šç§AIåˆ†ç±»æ–¹æ³•ã€‚</li>
<li>åŸºäºä»£ç†çš„RAGæ–¹æ³•åœ¨è¿½è¸ªEWSæŠ•èµ„æ–¹é¢è¡¨ç°æœ€ä½³ï¼Œå‡†ç¡®ç‡ã€ç²¾ç¡®åº¦å’Œå¬å›ç‡å‡è¶…è¿‡å…¶ä»–æ–¹æ³•ã€‚</li>
<li>æ–‡ç« è´¡çŒ®äº†ä¸€ä¸ªåŸºå‡†æ•°æ®é›†å’Œä¸“å®¶æ³¨é‡Šè¯­æ–™åº“ï¼Œä¸ºæœªæ¥ç ”ç©¶æä¾›èµ„æºã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.05104">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-b14b2138cc9d33b6f61ec87ff52b6ea6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8dbecbfc97e68a765663f7bc7204c7df.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-cb925f8fd08ff465ea3ab621c914fc96.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="A-Unified-Pairwise-Framework-for-RLHF-Bridging-Generative-Reward-Modeling-and-Policy-Optimization"><a href="#A-Unified-Pairwise-Framework-for-RLHF-Bridging-Generative-Reward-Modeling-and-Policy-Optimization" class="headerlink" title="A Unified Pairwise Framework for RLHF: Bridging Generative Reward   Modeling and Policy Optimization"></a>A Unified Pairwise Framework for RLHF: Bridging Generative Reward   Modeling and Policy Optimization</h2><p><strong>Authors:Wenyuan Xu, Xiaochen Zuo, Chao Xin, Yu Yue, Lin Yan, Yonghui Wu</strong></p>
<p>Reinforcement Learning from Human Feedback (RLHF) has emerged as a important paradigm for aligning large language models (LLMs) with human preferences during post-training. This framework typically involves two stages: first, training a reward model on human preference data, followed by optimizing the language model using reinforcement learning algorithms. However, current RLHF approaches may constrained by two limitations. First, existing RLHF frameworks often rely on Bradley-Terry models to assign scalar rewards based on pairwise comparisons of individual responses. However, this approach imposes significant challenges on reward model (RM), as the inherent variability in prompt-response pairs across different contexts demands robust calibration capabilities from the RM. Second, reward models are typically initialized from generative foundation models, such as pre-trained or supervised fine-tuned models, despite the fact that reward models perform discriminative tasks, creating a mismatch. This paper introduces Pairwise-RL, a RLHF framework that addresses these challenges through a combination of generative reward modeling and a pairwise proximal policy optimization (PPO) algorithm. Pairwise-RL unifies reward model training and its application during reinforcement learning within a consistent pairwise paradigm, leveraging generative modeling techniques to enhance reward model performance and score calibration. Experimental evaluations demonstrate that Pairwise-RL outperforms traditional RLHF frameworks across both internal evaluation datasets and standard public benchmarks, underscoring its effectiveness in improving alignment and model behavior. </p>
<blockquote>
<p>å¼ºåŒ–å­¦ä¹ ä»äººç±»åé¦ˆï¼ˆRLHFï¼‰å·²ç»æˆä¸ºä¸€ç§é‡è¦çš„èŒƒå¼ï¼Œç”¨äºåœ¨è®­ç»ƒåæœŸå°†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¸äººç±»åå¥½å¯¹é½ã€‚è¯¥æ¡†æ¶é€šå¸¸æ¶‰åŠä¸¤ä¸ªé˜¶æ®µï¼šé¦–å…ˆï¼ŒåŸºäºäººç±»åå¥½æ•°æ®è®­ç»ƒå¥–åŠ±æ¨¡å‹ï¼Œç„¶åä½¿ç”¨å¼ºåŒ–å­¦ä¹ ç®—æ³•ä¼˜åŒ–è¯­è¨€æ¨¡å‹ã€‚ç„¶è€Œï¼Œå½“å‰çš„RLHFæ–¹æ³•å¯èƒ½å—åˆ°ä¸¤ä¸ªé™åˆ¶ã€‚é¦–å…ˆï¼Œç°æœ‰çš„RLHFæ¡†æ¶é€šå¸¸ä¾èµ–äºBradley-Terryæ¨¡å‹ï¼Œæ ¹æ®ä¸ªåˆ«å“åº”çš„é…å¯¹æ¯”è¾ƒåˆ†é…æ ‡é‡å¥–åŠ±ã€‚ç„¶è€Œï¼Œè¿™ç§æ–¹æ³•ç»™å¥–åŠ±æ¨¡å‹ï¼ˆRMï¼‰å¸¦æ¥äº†é‡å¤§æŒ‘æˆ˜ï¼Œå› ä¸ºä¸åŒä¸Šä¸‹æ–‡ä¸­çš„æç¤º-å“åº”å¯¹å›ºæœ‰çš„å˜åŒ–è¦æ±‚RMå…·å¤‡å¼ºå¤§çš„æ ¡å‡†èƒ½åŠ›ã€‚å…¶æ¬¡ï¼Œå°½ç®¡å¥–åŠ±æ¨¡å‹æ‰§è¡Œçš„æ˜¯åˆ¤åˆ«ä»»åŠ¡ï¼Œä½†å¥–åŠ±æ¨¡å‹é€šå¸¸æ˜¯ä»ç”ŸæˆåŸºç¡€æ¨¡å‹ï¼ˆå¦‚é¢„è®­ç»ƒæˆ–ç›‘ç£å¾®è°ƒæ¨¡å‹ï¼‰åˆå§‹åŒ–çš„ï¼Œè¿™é€ æˆäº†ä¸åŒ¹é…ã€‚æœ¬æ–‡ä»‹ç»äº†Pairwise-RLï¼Œè¿™æ˜¯ä¸€ä¸ªRLHFæ¡†æ¶ï¼Œå®ƒé€šè¿‡ç»“åˆç”Ÿæˆå¥–åŠ±æ¨¡å‹å’Œæˆå¯¹è¿‘ç«¯ç­–ç•¥ä¼˜åŒ–ï¼ˆPPOï¼‰ç®—æ³•æ¥è§£å†³è¿™äº›æŒ‘æˆ˜ã€‚Pairwise-RLåœ¨ä¸€è‡´çš„å¯¹å¶èŒƒå¼å†…ç»Ÿä¸€äº†å¥–åŠ±æ¨¡å‹çš„è®­ç»ƒåŠå…¶åœ¨å¼ºåŒ–å­¦ä¹ ä¸­çš„åº”ç”¨ï¼Œåˆ©ç”¨ç”Ÿæˆå»ºæ¨¡æŠ€æœ¯æé«˜å¥–åŠ±æ¨¡å‹çš„æ€§èƒ½å’Œåˆ†æ•°æ ¡å‡†ã€‚å®éªŒè¯„ä¼°è¡¨æ˜ï¼ŒPairwise-RLåœ¨å†…éƒ¨è¯„ä¼°æ•°æ®é›†å’Œæ ‡å‡†å…¬å…±åŸºå‡†æµ‹è¯•ä¸Šçš„è¡¨ç°éƒ½ä¼˜äºä¼ ç»ŸRLHFæ¡†æ¶ï¼Œè¿™çªå‡ºäº†å…¶åœ¨æé«˜å¯¹é½å’Œæ¨¡å‹è¡Œä¸ºæ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.04950v1">PDF</a> 11oages,2 figures</p>
<p><strong>Summary</strong>ï¼š<br>å¼ºåŒ–å­¦ä¹ äººç±»åé¦ˆï¼ˆRLHFï¼‰å·²æˆä¸ºè®­ç»ƒå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¸äººç±»åå¥½å¯¹é½çš„é‡è¦èŒƒå¼ã€‚æœ¬æ–‡ä»‹ç»äº†Pairwise-RLï¼Œä¸€ä¸ªè§£å†³ç°æœ‰RLHFæ¡†æ¶æŒ‘æˆ˜çš„æ–°æ¡†æ¶ã€‚å®ƒé€šè¿‡ç»“åˆç”Ÿæˆå¥–åŠ±æ¨¡å‹å’Œæˆå¯¹è¿‘ç«¯ç­–ç•¥ä¼˜åŒ–ï¼ˆPPOï¼‰ç®—æ³•ï¼Œåœ¨ä¸€è‡´çš„ä¸¤ä¸¤æ¯”è¾ƒèŒƒå¼ä¸­ç»Ÿä¸€å¥–åŠ±æ¨¡å‹è®­ç»ƒå’Œå¼ºåŒ–å­¦ä¹ åº”ç”¨ã€‚å®éªŒè¯„ä¼°è¡¨æ˜ï¼ŒPairwise-RLåœ¨å†…éƒ¨è¯„ä¼°æ•°æ®é›†å’Œå…¬å…±æ ‡å‡†åŸºå‡†æµ‹è¯•ä¸­å‡ä¼˜äºä¼ ç»ŸRLHFæ¡†æ¶ï¼Œæé«˜äº†æ¨¡å‹å¯¹é½å’Œè¡Œä¸ºçš„æ•ˆæœã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>RLHFå·²æˆä¸ºè®­ç»ƒLLMä¸äººç±»åå¥½å¯¹é½çš„é‡è¦æ–¹æ³•ï¼Œæ¶‰åŠä¸¤é˜¶æ®µï¼šè®­ç»ƒå¥–åŠ±æ¨¡å‹å’Œä½¿ç”¨å¼ºåŒ–å­¦ä¹ ç®—æ³•ä¼˜åŒ–è¯­è¨€æ¨¡å‹ã€‚</li>
<li>å½“å‰RLHFæ–¹æ³•å­˜åœ¨ä¸¤ä¸ªå±€é™æ€§ï¼šä¾èµ–Bradley-Terryæ¨¡å‹è¿›è¡ŒåŸºäºä¸¤ä¸¤æ¯”è¾ƒçš„ä¸ªäººå“åº”çš„æ ‡é‡å¥–åŠ±åˆ†é…ï¼Œä»¥åŠå¥–åŠ±æ¨¡å‹çš„åˆå§‹åŒ–å’Œåº”ç”¨å­˜åœ¨ä¸åŒ¹é…é—®é¢˜ã€‚</li>
<li>Pairwise-RLé€šè¿‡ç»“åˆç”Ÿæˆå¥–åŠ±æ¨¡å‹å’ŒPPOç®—æ³•è§£å†³è¿™äº›æŒ‘æˆ˜ã€‚</li>
<li>Pairwise-RLåœ¨å¥–åŠ±æ¨¡å‹è®­ç»ƒå’Œå¼ºåŒ–å­¦ä¹ åº”ç”¨ä¸­é‡‡ç”¨ä¸€è‡´çš„ä¸¤ä¸¤æ¯”è¾ƒèŒƒå¼ã€‚</li>
<li>ç”Ÿæˆå»ºæ¨¡æŠ€æœ¯è¢«ç”¨æ¥æé«˜å¥–åŠ±æ¨¡å‹æ€§èƒ½å’Œåˆ†æ•°æ ¡å‡†ã€‚</li>
<li>å®éªŒè¯„ä¼°è¡¨æ˜ï¼ŒPairwise-Rlåœ¨å†…éƒ¨å’Œå…¬å…±åŸºå‡†æµ‹è¯•ä¸­å‡è¡¨ç°å‡ºä¼˜äºä¼ ç»ŸRLHFæ¡†æ¶çš„æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.04950">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-af042a15ed75ad0d9f19163e9cc042f5.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Quantization-Hurts-Reasoning-An-Empirical-Study-on-Quantized-Reasoning-Models"><a href="#Quantization-Hurts-Reasoning-An-Empirical-Study-on-Quantized-Reasoning-Models" class="headerlink" title="Quantization Hurts Reasoning? An Empirical Study on Quantized Reasoning   Models"></a>Quantization Hurts Reasoning? An Empirical Study on Quantized Reasoning   Models</h2><p><strong>Authors:Ruikang Liu, Yuxuan Sun, Manyi Zhang, Haoli Bai, Xianzhi Yu, Tiezheng Yu, Chun Yuan, Lu Hou</strong></p>
<p>Recent advancements in reasoning language models have demonstrated remarkable performance in complex tasks, but their extended chain-of-thought reasoning process increases inference overhead. While quantization has been widely adopted to reduce the inference cost of large language models, its impact on reasoning models remains understudied. In this study, we conduct the first systematic study on quantized reasoning models, evaluating the open-sourced DeepSeek-R1-Distilled Qwen and LLaMA families ranging from 1.5B to 70B parameters, and QwQ-32B. Our investigation covers weight, KV cache, and activation quantization using state-of-the-art algorithms at varying bit-widths, with extensive evaluation across mathematical (AIME, MATH-500), scientific (GPQA), and programming (LiveCodeBench) reasoning benchmarks. Our findings reveal that while lossless quantization can be achieved with W8A8 or W4A16 quantization, lower bit-widths introduce significant accuracy risks. We further identify model size, model origin, and task difficulty as critical determinants of performance. Contrary to expectations, quantized models do not exhibit increased output lengths. In addition, strategically scaling the model sizes or reasoning steps can effectively enhance the performance. All quantized models and codes will be open-sourced in <a target="_blank" rel="noopener" href="https://github.com/ruikangliu/Quantized-Reasoning-Models">https://github.com/ruikangliu/Quantized-Reasoning-Models</a>. </p>
<blockquote>
<p>è¿‘æœŸæ¨ç†è¯­è¨€æ¨¡å‹çš„å‘å±•åœ¨å¤æ‚ä»»åŠ¡ä¸­è¡¨ç°å‡ºå“è¶Šçš„æ€§èƒ½ï¼Œä½†å…¶æ‰©å±•çš„é“¾å¼æ¨ç†è¿‡ç¨‹å¢åŠ äº†æ¨ç†å¼€é”€ã€‚è™½ç„¶é‡åŒ–å·²å¹¿æ³›åº”ç”¨äºé™ä½å¤§å‹è¯­è¨€æ¨¡å‹çš„æ¨ç†æˆæœ¬ï¼Œä½†å¯¹æ¨ç†æ¨¡å‹çš„å½±å“ä»ç ”ç©¶ä¸è¶³ã€‚æœ¬ç ”ç©¶é¦–æ¬¡å¯¹é‡åŒ–æ¨ç†æ¨¡å‹è¿›è¡Œç³»ç»Ÿç ”ç©¶ï¼Œè¯„ä¼°å¼€æºçš„DeepSeek-R1-Distilled Qwenå’ŒLLaMAç³»åˆ—ï¼Œå‚æ•°èŒƒå›´ä»1.5Båˆ°70Bï¼Œä»¥åŠQwQ-32Bã€‚æˆ‘ä»¬çš„ç ”ç©¶æ¶µç›–äº†ä½¿ç”¨æœ€æ–°ç®—æ³•çš„æƒé‡ã€KVç¼“å­˜å’Œæ¿€æ´»é‡åŒ–ï¼Œå¹¶åœ¨æ•°å­¦ï¼ˆAIMEï¼ŒMATH-500ï¼‰ã€ç§‘å­¦ï¼ˆGPQAï¼‰å’Œç¼–ç¨‹ï¼ˆLiveCodeBenchï¼‰æ¨ç†åŸºå‡†æµ‹è¯•è¿›è¡Œäº†å¹¿æ³›è¯„ä¼°ã€‚æˆ‘ä»¬çš„ç ”ç©¶å‘ç°ï¼Œä½¿ç”¨W8A8æˆ–W4A16é‡åŒ–å¯ä»¥å®ç°æ— æŸé‡åŒ–ï¼Œä½†è¾ƒä½ä½å®½ä¼šå¼•å…¥æ˜¾è‘—å‡†ç¡®æ€§é£é™©ã€‚æˆ‘ä»¬è¿›ä¸€æ­¥ç¡®å®šäº†æ¨¡å‹å¤§å°ã€æ¨¡å‹æ¥æºå’Œä»»åŠ¡éš¾åº¦æ˜¯æ€§èƒ½çš„å…³é”®å†³å®šå› ç´ ã€‚å‡ºä¹æ„æ–™çš„æ˜¯ï¼Œé‡åŒ–æ¨¡å‹å¹¶æ²¡æœ‰è¡¨ç°å‡ºè¾“å‡ºé•¿åº¦å¢åŠ çš„æƒ…å†µã€‚æ­¤å¤–ï¼Œæœ‰é’ˆå¯¹æ€§åœ°è°ƒæ•´æ¨¡å‹å¤§å°æˆ–æ¨ç†æ­¥éª¤å¯ä»¥æœ‰æ•ˆæé«˜æ€§èƒ½ã€‚æ‰€æœ‰é‡åŒ–æ¨¡å‹å’Œä»£ç å°†åœ¨<a target="_blank" rel="noopener" href="https://github.com/ruikangliu/Quantized-Reasoning-Models">https://github.com/ruikangliu/Quantized-Reasoning-Models</a>ä¸Šå¼€æºã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.04823v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ€è¿‘çš„è¯­è¨€æ¨¡å‹æ¨ç†æŠ€æœ¯å–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œåœ¨å¤æ‚ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ï¼Œä½†å…¶æ‰©å±•çš„æ¨ç†è¿‡ç¨‹å¢åŠ äº†æ¨ç†å¼€é”€ã€‚æœ¬ç ”ç©¶é¦–æ¬¡å¯¹é‡åŒ–æ¨ç†æ¨¡å‹è¿›è¡Œç³»ç»Ÿç ”ç©¶ï¼Œè¯„ä¼°äº†DeepSeek-R1-Distilled Qwenå’ŒLLaMAç³»åˆ—ä»¥åŠQwQ-32Bç­‰å¼€æºé‡åŒ–æ¨ç†æ¨¡å‹ï¼Œæ¶‰åŠæƒé‡ã€KVç¼“å­˜å’Œæ¿€æ´»é‡åŒ–çš„ç ”ç©¶ã€‚ç ”ç©¶å‘ç°ï¼Œåœ¨ç‰¹å®šé‡åŒ–æ–¹æ³•ä¸‹å¯ä»¥å®ç°æ— æŸé‡åŒ–ï¼Œä½†ä½æ¯”ç‰¹å®½åº¦ä¼šå¼•å…¥ç²¾åº¦é£é™©ã€‚æ¨¡å‹å¤§å°ã€æ¥æºå’Œä»»åŠ¡éš¾åº¦å¯¹æ€§èƒ½æœ‰é‡è¦å½±å“ã€‚é‡åŒ–æ¨¡å‹çš„è¾“å‡ºé•¿åº¦å¹¶æœªå¢åŠ ï¼Œä¸”é€šè¿‡æˆ˜ç•¥æ€§åœ°è°ƒæ•´æ¨¡å‹å¤§å°æˆ–æ¨ç†æ­¥éª¤å¯ä»¥æœ‰æ•ˆæé«˜æ€§èƒ½ã€‚æ‰€æœ‰é‡åŒ–æ¨¡å‹å’Œä»£ç å°†å¼€æºåœ¨ruikangliuçš„Quantized-Reasoning-Modelsä»“åº“ä¸­ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>é‡åŒ–æ¨ç†æ¨¡å‹çš„ç³»ç»Ÿç ”ç©¶æ˜¯å¿…è¦çš„ï¼Œå› ä¸ºè™½ç„¶é‡åŒ–å¹¿æ³›åº”ç”¨äºå‡å°‘å¤§å‹è¯­è¨€æ¨¡å‹çš„æ¨ç†æˆæœ¬ï¼Œä½†å¯¹æ¨ç†æ¨¡å‹çš„å½±å“å°šæœªå¾—åˆ°å……åˆ†ç ”ç©¶ã€‚</li>
<li>ç ”ç©¶æ¶µç›–äº†å¤šç§å¼€æºé‡åŒ–æ¨ç†æ¨¡å‹ï¼ŒåŒ…æ‹¬DeepSeek-R1-Distilled Qwenå’ŒLLaMAç³»åˆ—ç­‰ã€‚</li>
<li>ç ”ç©¶æ¶‰åŠæƒé‡ã€KVç¼“å­˜å’Œæ¿€æ´»é‡åŒ–çš„è¯„ä¼°ï¼Œå‘ç°ç‰¹å®šé‡åŒ–æ–¹æ³•å¯ä»¥å®ç°æ— æŸé‡åŒ–ã€‚</li>
<li>ä½æ¯”ç‰¹å®½åº¦é‡åŒ–å¯èƒ½å¯¼è‡´ç²¾åº¦é£é™©ã€‚</li>
<li>æ¨¡å‹å¤§å°ã€æ¥æºå’Œä»»åŠ¡éš¾åº¦å¯¹é‡åŒ–æ¨ç†æ¨¡å‹çš„æ€§èƒ½æœ‰é‡è¦å½±å“ã€‚</li>
<li>é‡åŒ–æ¨¡å‹çš„è¾“å‡ºé•¿åº¦å¹¶æœªå› é‡åŒ–è€Œå¢åŠ ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.04823">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-3dd66df461e59462afc37ded26b30362.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-014295f29b3625d37f786df1561bad50.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Enhancing-Compositional-Reasoning-in-Vision-Language-Models-with-Synthetic-Preference-Data"><a href="#Enhancing-Compositional-Reasoning-in-Vision-Language-Models-with-Synthetic-Preference-Data" class="headerlink" title="Enhancing Compositional Reasoning in Vision-Language Models with   Synthetic Preference Data"></a>Enhancing Compositional Reasoning in Vision-Language Models with   Synthetic Preference Data</h2><p><strong>Authors:Samarth Mishra, Kate Saenko, Venkatesh Saligrama</strong></p>
<p>Compositionality, or correctly recognizing scenes as compositions of atomic visual concepts, remains difficult for multimodal large language models (MLLMs). Even state of the art MLLMs such as GPT-4o can make mistakes in distinguishing compositions like â€œdog chasing catâ€ vs â€œcat chasing dogâ€. While on Winoground, a benchmark for measuring such reasoning, MLLMs have made significant progress, they are still far from a humanâ€™s performance. We show that compositional reasoning in these models can be improved by elucidating such concepts via data, where a model is trained to prefer the correct caption for an image over a close but incorrect one. We introduce SCRAMBLe: Synthetic Compositional Reasoning Augmentation of MLLMs with Binary preference Learning, an approach for preference tuning open-weight MLLMs on synthetic preference data generated in a fully automated manner from existing image-caption data. SCRAMBLe holistically improves these MLLMsâ€™ compositional reasoning capabilities which we can see through significant improvements across multiple vision language compositionality benchmarks, as well as smaller but significant improvements on general question answering tasks. As a sneak peek, SCRAMBLe tuned Molmo-7B model improves on Winoground from 49.5% to 54.8% (best reported to date), while improving by ~1% on more general visual question answering tasks. Code for SCRAMBLe along with tuned models and our synthetic training dataset is available at <a target="_blank" rel="noopener" href="https://github.com/samarth4149/SCRAMBLe">https://github.com/samarth4149/SCRAMBLe</a>. </p>
<blockquote>
<p>å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨è¯†åˆ«åœºæ™¯ä½œä¸ºåŸå­è§†è§‰æ¦‚å¿µç»„åˆæ–¹é¢ä»ç„¶é¢ä¸´å›°éš¾ï¼Œå³ç»„åˆæ€§ï¼ˆCompositionalityï¼‰ã€‚å³ä½¿æ˜¯ç›®å‰æœ€å…ˆè¿›çš„MLLMsï¼Œå¦‚GPT-4oï¼Œåœ¨åŒºåˆ†â€œç‹—è¿½çŒ«â€å’Œâ€œçŒ«è¿½ç‹—â€ç­‰ç»„åˆæ—¶ä¹Ÿä¼šå‡ºé”™ã€‚è™½ç„¶åœ¨è¡¡é‡æ­¤ç±»æ¨ç†èƒ½åŠ›çš„WinogroundåŸºå‡†æµ‹è¯•ä¸­ï¼ŒMLLMså·²ç»å–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œä½†å®ƒä»¬ä¸äººç±»çš„è¡¨ç°ç›¸æ¯”ä»æœ‰å¾ˆå¤§å·®è·ã€‚æˆ‘ä»¬å±•ç¤ºå¯ä»¥é€šè¿‡æ•°æ®é˜æ˜è¿™äº›æ¦‚å¿µæ¥æ”¹å–„è¿™äº›æ¨¡å‹ä¸­çš„ç»„åˆæ¨ç†èƒ½åŠ›ï¼Œæ¨¡å‹ç»è¿‡è®­ç»ƒï¼Œæ›´å€¾å‘äºä¸ºå›¾åƒé€‰æ‹©æ­£ç¡®çš„æ ‡é¢˜ï¼Œè€Œéæ¥è¿‘ä½†é”™è¯¯çš„æ ‡é¢˜ã€‚æˆ‘ä»¬æ¨å‡ºäº†SCRAMBLeï¼šåŸºäºäºŒè¿›åˆ¶åå¥½å­¦ä¹ çš„å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹åˆæˆæ¨ç†å¢å¼ºæ–¹æ³•ã€‚è¿™æ˜¯ä¸€ç§åå¥½è°ƒæ•´æ–¹æ³•ï¼Œç”¨äºåœ¨å®Œå…¨è‡ªåŠ¨åŒ–çš„æ–¹å¼ä¸‹ä»ç°æœ‰çš„å›¾åƒæ ‡é¢˜æ•°æ®ä¸­ç”Ÿæˆåˆæˆåå¥½æ•°æ®ï¼Œå¯¹å¼€æ”¾æƒé‡MLLMsè¿›è¡Œåå¥½è°ƒæ•´ã€‚SCRAMBLeå…¨é¢æé«˜äº†è¿™äº›MLLMsçš„ç»„åˆæ¨ç†èƒ½åŠ›ï¼Œè¿™å¯ä»¥ä»å¤šä¸ªè§†è§‰è¯­è¨€ç»„åˆåŸºå‡†æµ‹è¯•ä¸­æ˜¾è‘—çš„æ”¹è¿›ä¸­çœ‹å‡ºï¼Œä»¥åŠåœ¨ä¸€èˆ¬é—®ç­”ä»»åŠ¡ä¸­çš„å°å¹…ä½†æ˜¾è‘—çš„æ”¹è¿›ã€‚ä½œä¸ºé¢„è§ˆï¼ŒSCRAMBLeè°ƒæ•´äº†Molmo-7Bæ¨¡å‹ï¼Œä½¿å…¶åœ¨Winogroundä¸Šçš„è¡¨ç°ä»49.5%æé«˜åˆ°54.8%ï¼ˆè‡³ä»Šæœ€ä½³æŠ¥å‘Šç»“æœï¼‰ï¼ŒåŒæ—¶åœ¨æ›´ä¸€èˆ¬çš„è§†è§‰é—®ç­”ä»»åŠ¡ä¸Šæé«˜äº†çº¦1%ã€‚SCRAMBLeçš„ä»£ç ä»¥åŠè°ƒæ•´åçš„æ¨¡å‹å’Œæˆ‘ä»¬çš„åˆæˆè®­ç»ƒæ•°æ®é›†å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/samarth4149/SCRAMBLe">https://github.com/samarth4149/SCRAMBLe</a>è·å¾—ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.04740v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æ–‡æœ¬è®¨è®ºäº†å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨è¯†åˆ«åœºæ™¯ç»„åˆæ–¹é¢å­˜åœ¨çš„å›°éš¾ï¼Œå³ä½¿æ˜¯æœ€å…ˆè¿›çš„æ¨¡å‹å¦‚GPT-4oä¹Ÿéš¾ä»¥åŒºåˆ†ç±»ä¼¼çš„åœºæ™¯ç»„åˆã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºSCRAMBLeçš„æ–¹æ³•ï¼Œé€šè¿‡åå¥½å­¦ä¹ è®­ç»ƒæ¨¡å‹ä»¥åœ¨åˆæˆåå¥½æ•°æ®ä¸Šé€‰æ‹©æ­£ç¡®çš„å›¾åƒæè¿°ã€‚SCRAMBLeæ–¹æ³•å…¨é¢æé«˜äº†MLLMsçš„ç»„åˆæ¨ç†èƒ½åŠ›ï¼Œå¹¶åœ¨å¤šä¸ªè§†è§‰è¯­è¨€ç»„åˆåŸºå‡†æµ‹è¯•ä¸­å–å¾—äº†æ˜¾è‘—æ”¹è¿›ã€‚æ­¤å¤–ï¼ŒSCRAMBLeå¯¹Molmo-7Bæ¨¡å‹çš„è°ƒæ•´åœ¨Winogroundä¸Šçš„è¡¨ç°ä»49.5%æé«˜åˆ°äº†54.8%ï¼ŒåŒæ—¶åœ¨æ›´ä¸€èˆ¬çš„è§†è§‰é—®ç­”ä»»åŠ¡ä¸Šä¹Ÿæé«˜äº†çº¦1%ã€‚ç›¸å…³ä»£ç å’Œè®­ç»ƒæ¨¡å‹å¯åœ¨ç›¸å…³ç½‘ç«™ä¸Šæ‰¾åˆ°ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨è¯†åˆ«åœºæ™¯ç»„åˆæ–¹é¢å­˜åœ¨å›°éš¾ï¼Œéš¾ä»¥åŒºåˆ†ç±»ä¼¼çš„åœºæ™¯ç»„åˆã€‚</li>
<li>SCRAMBLeæ–¹æ³•é€šè¿‡åå¥½å­¦ä¹ è®­ç»ƒæ¨¡å‹ï¼Œä»¥æé«˜å…¶åœ¨åˆæˆåå¥½æ•°æ®ä¸Šé€‰æ‹©æ­£ç¡®å›¾åƒæè¿°çš„èƒ½åŠ›ã€‚</li>
<li>SCRAMBLeæ–¹æ³•æ˜¾è‘—æé«˜äº†MLLMsçš„ç»„åˆæ¨ç†èƒ½åŠ›ï¼Œå¹¶åœ¨å¤šä¸ªè§†è§‰è¯­è¨€ç»„åˆåŸºå‡†æµ‹è¯•ä¸­å–å¾—äº†è‰¯å¥½è¡¨ç°ã€‚</li>
<li>SCRAMBLeå¯¹Molmo-7Bæ¨¡å‹çš„è°ƒæ•´åœ¨Winogroundä¸Šçš„è¡¨ç°æœ‰æ‰€æå‡ï¼ŒåŒæ—¶ä¹Ÿæé«˜äº†è§†è§‰é—®ç­”ä»»åŠ¡çš„æ€§èƒ½ã€‚</li>
<li>åˆæˆæ•°æ®åœ¨è®­ç»ƒæ¨¡å‹å’Œæé«˜æ¨¡å‹æ€§èƒ½ä¸Šèµ·åˆ°äº†å…³é”®ä½œç”¨ã€‚</li>
<li>SCRAMBLeæ–¹æ³•é€‚ç”¨äºå¼€æºå¤§å‹è¯­è¨€æ¨¡å‹çš„åå¥½è°ƒæ•´ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.04740">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-2ae2de9621feea1ce73cefa4bf405dd7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-99b6e4fd98212a920b2749c28a4d2d1c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1c5ad3b7d08f56fcd5f7277e48334366.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-769182b0460a8e18357060d3ab8bf982.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="Synthetic-Data-Generation-Multi-Step-RL-for-Reasoning-Tool-Use"><a href="#Synthetic-Data-Generation-Multi-Step-RL-for-Reasoning-Tool-Use" class="headerlink" title="Synthetic Data Generation &amp; Multi-Step RL for Reasoning &amp; Tool Use"></a>Synthetic Data Generation &amp; Multi-Step RL for Reasoning &amp; Tool Use</h2><p><strong>Authors:Anna Goldie, Azalia Mirhoseini, Hao Zhou, Irene Cai, Christopher D. Manning</strong></p>
<p>Reinforcement learning has been shown to improve the performance of large language models. However, traditional approaches like RLHF or RLAIF treat the problem as single-step. As focus shifts toward more complex reasoning and agentic tasks, language models must take multiple steps of text generation, reasoning and environment interaction before generating a solution. We propose a synthetic data generation and RL methodology targeting multi-step optimization scenarios. This approach, called Step-Wise Reinforcement Learning (SWiRL), iteratively generates multi-step reasoning and tool use data, and then learns from that data. It employs a simple step-wise decomposition that breaks each multi-step trajectory into multiple sub-trajectories corresponding to each action by the original model. It then applies synthetic data filtering and RL optimization on these sub-trajectories. We evaluated SWiRL on a number of multi-step tool use, question answering, and mathematical reasoning tasks. Our experiments show that SWiRL outperforms baseline approaches by 21.5%, 12.3%, 14.8%, 11.1%, and 15.3% in relative accuracy on GSM8K, HotPotQA, CofCA, MuSiQue, and BeerQA, respectively. Excitingly, the approach exhibits generalization across tasks: for example, training only on HotPotQA (text question-answering) improves zero-shot performance on GSM8K (a math dataset) by a relative 16.9%. </p>
<blockquote>
<p>å¼ºåŒ–å­¦ä¹ å·²è¢«è¯æ˜å¯ä»¥æé«˜å¤§å‹è¯­è¨€æ¨¡å‹çš„æ€§èƒ½ã€‚ç„¶è€Œï¼ŒåƒRLHFæˆ–RLAIFè¿™æ ·çš„ä¼ ç»Ÿæ–¹æ³•å°†é—®é¢˜è§†ä¸ºå•æ­¥éª¤çš„ã€‚éšç€ç„¦ç‚¹è½¬å‘æ›´å¤æ‚çš„æ¨ç†å’Œä»£ç†ä»»åŠ¡ï¼Œè¯­è¨€æ¨¡å‹å¿…é¡»åœ¨ç”Ÿæˆè§£å†³æ–¹æ¡ˆä¹‹å‰è¿›è¡Œå¤šæ­¥éª¤çš„æ–‡æœ¬ç”Ÿæˆã€æ¨ç†å’Œç¯å¢ƒäº¤äº’ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§é’ˆå¯¹å¤šæ­¥éª¤ä¼˜åŒ–åœºæ™¯çš„äººå·¥æ•°æ®ç”Ÿæˆå’Œå¼ºåŒ–å­¦ä¹ æ–¹æ³•ã€‚è¿™ç§æ–¹æ³•è¢«ç§°ä¸ºStep-Wise Reinforcement Learningï¼ˆSWiRLï¼‰ï¼Œå®ƒè¿­ä»£åœ°ç”Ÿæˆå¤šæ­¥éª¤æ¨ç†å’Œå·¥å…·ä½¿ç”¨æ•°æ®ï¼Œç„¶åä»ä¸­å­¦ä¹ ã€‚å®ƒé‡‡ç”¨ç®€å•çš„é€æ­¥åˆ†è§£æ–¹æ³•ï¼Œå°†æ¯ä¸ªå¤šæ­¥éª¤è½¨è¿¹åˆ†è§£ä¸ºä¸åŸå§‹æ¨¡å‹çš„æ¯ä¸ªåŠ¨ä½œç›¸å¯¹åº”çš„å¤šä¸ªå­è½¨è¿¹ã€‚ç„¶åï¼Œå®ƒå¯¹è¿™äº›å­è½¨è¿¹è¿›è¡Œäººå·¥æ•°æ®è¿‡æ»¤å’Œå¼ºåŒ–å­¦ä¹ ä¼˜åŒ–ã€‚æˆ‘ä»¬åœ¨å¤šä¸ªå¤šæ­¥éª¤å·¥å…·ä½¿ç”¨ã€é—®ç­”å’Œæ•°å­¦æ¨ç†ä»»åŠ¡ä¸Šè¯„ä¼°äº†SWiRLã€‚å®éªŒè¡¨æ˜ï¼Œåœ¨GSM8Kã€HotPotQAã€CofCAã€MuSiQueå’ŒBeerQAä¸Šï¼ŒSWiRLçš„ç›¸å¯¹å‡†ç¡®ç‡åˆ†åˆ«æ¯”åŸºçº¿æ–¹æ³•é«˜å‡º21.5%ã€12.3%ã€14.8%ã€11.1%å’Œ15.3%ã€‚ä»¤äººå…´å¥‹çš„æ˜¯ï¼Œè¯¥æ–¹æ³•åœ¨ä»»åŠ¡ä¹‹é—´è¡¨ç°å‡ºæ³›åŒ–èƒ½åŠ›ï¼šä¾‹å¦‚ï¼Œä»…åœ¨HotPotQAï¼ˆæ–‡æœ¬é—®ç­”ï¼‰ä¸Šè¿›è¡Œè®­ç»ƒï¼Œå¯¹GSM8Kï¼ˆæ•°å­¦æ•°æ®é›†ï¼‰çš„é›¶æ ·æœ¬æ€§èƒ½ç›¸å¯¹æé«˜äº†1 ç»“ç‚¹è®¡ç®—æ•ˆç‡æ˜¯æŒ‡å®ŒæˆæŸé¡¹ä»»åŠ¡æˆ–æ“ä½œæ‰€éœ€çš„æ—¶é—´è¶Šå°‘ã€‚è¿™åœ¨å¤šä»»åŠ¡å¤„ç†æˆ–åœ¨çŸ­æ—¶é—´å†…å®Œæˆä»»åŠ¡çš„æƒ…å†µä¸‹ç‰¹åˆ«é‡è¦ã€‚åœ¨ä¸Šè¿°åœºæ™¯ä¸­ï¼Œé€šè¿‡ä½¿ç”¨SWiRLç­‰æ–¹æ³•æé«˜æ¨¡å‹çš„æ•ˆç‡å’Œå‡†ç¡®æ€§ï¼Œå¯ä»¥è¿›ä¸€æ­¥æ¨åŠ¨è‡ªç„¶è¯­è¨€å¤„ç†é¢†åŸŸçš„å‘å±•å’Œåº”ç”¨ã€‚è¿™ä¸ä»…æœ‰åŠ©äºæé«˜è¯­è¨€æ¨¡å‹çš„æ€§èƒ½ï¼Œè¿˜æœ‰åŠ©äºå¼€å‘æ›´æ™ºèƒ½ã€æ›´é«˜æ•ˆçš„æ™ºèƒ½ç³»ç»Ÿï¼Œä»è€Œæ›´å¥½åœ°æœåŠ¡äºäººç±»ç¤¾ä¼šã€‚æˆ‘ä»¬çš„ç ”ç©¶åªæ˜¯å†°å±±ä¸€è§’ï¼Œæˆ‘ä»¬ç›¸ä¿¡æœªæ¥ä¼šæœ‰æ›´å¤šçš„åˆ›æ–°æ–¹æ³•å’ŒæŠ€æœ¯ä¸æ–­æ¶Œç°ï¼Œæ¨åŠ¨äººå·¥æ™ºèƒ½é¢†åŸŸçš„è¿›æ­¥å’Œå‘å±•ã€‚%ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.04736v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¼ºåŒ–å­¦ä¹ å·²è¯æ˜å¯æå‡å¤§å‹è¯­è¨€æ¨¡å‹çš„æ€§èƒ½ã€‚ç„¶è€Œï¼Œä¼ ç»Ÿæ–¹æ³•å¦‚RLHFæˆ–RLAIFå°†é—®é¢˜è§†ä¸ºå•æ­¥éª¤çš„ã€‚éšç€ç„¦ç‚¹è½¬å‘æ›´å¤æ‚çš„æ¨ç†å’Œä»£ç†ä»»åŠ¡ï¼Œè¯­è¨€æ¨¡å‹å¿…é¡»åœ¨ç”Ÿæˆè§£å†³æ–¹æ¡ˆä¹‹å‰è¿›è¡Œå¤šæ­¥éª¤çš„æ–‡æœ¬ç”Ÿæˆã€æ¨ç†å’Œç¯å¢ƒäº¤äº’ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§é’ˆå¯¹å¤šæ­¥éª¤ä¼˜åŒ–åœºæ™¯åˆæˆæ•°æ®ç”Ÿæˆå’Œå¼ºåŒ–å­¦ä¹ æ–¹æ³•â€”â€”Step-Wise Reinforcement Learningï¼ˆSWiRLï¼‰ã€‚SWiRLé€šè¿‡è¿­ä»£ç”Ÿæˆå¤šæ­¥éª¤æ¨ç†å’Œå·¥å…·ä½¿ç”¨æ•°æ®ï¼Œå¹¶å­¦ä¹ è¿™äº›æ•°æ®ã€‚å®ƒé‡‡ç”¨ç®€å•çš„é€æ­¥åˆ†è§£æ–¹æ³•ï¼Œå°†æ¯ä¸ªå¤šæ­¥éª¤è½¨è¿¹åˆ†è§£ä¸ºå¤šä¸ªå¯¹åº”äºåŸå§‹æ¨¡å‹æ¯ä¸ªæ“ä½œçš„å­è½¨è¿¹ï¼Œç„¶åå¯¹è¿™äº›å­è½¨è¿¹è¿›è¡Œåˆæˆæ•°æ®è¿‡æ»¤å’Œå¼ºåŒ–å­¦ä¹ ä¼˜åŒ–ã€‚åœ¨å¤šé¡¹å¤šæ­¥éª¤å·¥å…·ä½¿ç”¨ã€é—®ç­”å’Œæ•°å­¦æ¨ç†ä»»åŠ¡ä¸Šè¯„ä¼°SWiRLï¼Œç›¸å¯¹äºåŸºçº¿æ–¹æ³•ï¼Œå…¶åœ¨GSM8Kã€HotPotQAã€CofCAã€MuSiQueå’ŒBeerQAä¸Šçš„ç›¸å¯¹å‡†ç¡®æ€§åˆ†åˆ«æé«˜äº†21.5%ã€12.3%ã€14.8%ã€11.1%å’Œ15.3%ã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ³•å±•ç°å‡ºè·¨ä»»åŠ¡çš„æ³›åŒ–èƒ½åŠ›ï¼Œä¾‹å¦‚åœ¨HotPotQAï¼ˆæ–‡æœ¬é—®ç­”ï¼‰ä¸Šè®­ç»ƒåï¼Œåœ¨GSM8Kï¼ˆæ•°å­¦æ•°æ®é›†ï¼‰ä¸Šçš„é›¶æ ·æœ¬æ€§èƒ½ç›¸å¯¹æé«˜äº†16.9%ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¼ºåŒ–å­¦ä¹ èƒ½æé«˜å¤§å‹è¯­è¨€æ¨¡å‹çš„æ€§èƒ½ã€‚</li>
<li>ä¼ ç»Ÿæ–¹æ³•ä¸»è¦å¤„ç†å•æ­¥éª¤é—®é¢˜ï¼Œä½†å¤æ‚ä»»åŠ¡éœ€å¤šæ­¥éª¤è§£å†³ã€‚</li>
<li>æå‡ºçš„Step-Wise Reinforcement Learning (SWiRL) æ–¹æ³•èƒ½å¤„ç†å¤šæ­¥éª¤ä¼˜åŒ–åœºæ™¯ã€‚</li>
<li>SWiRLé€šè¿‡ç”Ÿæˆå¤šæ­¥éª¤æ¨ç†å’Œå·¥å…·ä½¿ç”¨æ•°æ®ï¼Œå¹¶å­¦ä¹ è¿™äº›æ•°æ®æ¥ä¼˜åŒ–æ¨¡å‹ã€‚</li>
<li>SWiRLé‡‡ç”¨é€æ­¥åˆ†è§£æ³•å¤„ç†å¤šæ­¥éª¤è½¨è¿¹ï¼Œæé«˜æ¨¡å‹å­¦ä¹ æ•ˆæœã€‚</li>
<li>SWiRLåœ¨å¤šé¡¹ä»»åŠ¡ä¸Šçš„æ€§èƒ½ä¼˜äºåŸºçº¿æ–¹æ³•ï¼Œæ˜¾ç¤ºå‡ºæ˜æ˜¾çš„æ€§èƒ½æå‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.04736">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-514ca0dbb047623326979c3d57038e80.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5d76740b642fa93aeedaa6e804581f82.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d20fe2fa2fff07c27922f10bab9709a7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b08f1710e9ecece21a37fef41ddec6a9.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="ACE-RLHF-Automated-Code-Evaluation-and-Socratic-Feedback-Generation-Tool-using-Large-Language-Models-and-Reinforcement-Learning-with-Human-Feedback"><a href="#ACE-RLHF-Automated-Code-Evaluation-and-Socratic-Feedback-Generation-Tool-using-Large-Language-Models-and-Reinforcement-Learning-with-Human-Feedback" class="headerlink" title="ACE-RLHF: Automated Code Evaluation and Socratic Feedback Generation   Tool using Large Language Models and Reinforcement Learning with Human   Feedback"></a>ACE-RLHF: Automated Code Evaluation and Socratic Feedback Generation   Tool using Large Language Models and Reinforcement Learning with Human   Feedback</h2><p><strong>Authors:Tasnia Rahman, Sathish A. P. Kumar, Sumit Jha, Arvind Ramanathan</strong></p>
<p>Automated Program Repair tools are developed for generating feedback and suggesting a repair method for erroneous code. State of the art (SOTA) code repair methods rely on data-driven approaches and often fail to deliver solution for complicated programming questions. To interpret the natural language of unprecedented programming problems, using Large Language Models (LLMs) for code-feedback generation is crucial. LLMs generate more comprehensible feedback than compiler-generated error messages, and Reinforcement Learning with Human Feedback (RLHF) further enhances quality by integrating human-in-the-loop which helps novice students to lean programming from scratch interactively. We are applying RLHF fine-tuning technique for an expected Socratic response such as a question with hint to solve the programming issue. We are proposing code feedback generation tool by fine-tuning LLM with RLHF, Automated Code Evaluation with RLHF (ACE-RLHF), combining two open-source LLM models with two different SOTA optimization techniques. The quality of feedback is evaluated on two benchmark datasets containing basic and competition-level programming questions where the later is proposed by us. We achieved 2-5% higher accuracy than RL-free SOTA techniques using Llama-3-7B-Proximal-policy optimization in automated evaluation and similar or slightly higher accuracy compared to reward model-free RL with AI Feedback (RLAIF). We achieved almost 40% higher accuracy with GPT-3.5 Best-of-n optimization while performing manual evaluation. </p>
<blockquote>
<p>è‡ªåŠ¨åŒ–ç¨‹åºä¿®å¤å·¥å…·æ—¨åœ¨é’ˆå¯¹é”™è¯¯ä»£ç æä¾›åé¦ˆå¹¶å»ºè®®ä¿®å¤æ–¹æ³•ã€‚æœ€å…ˆè¿›çš„ä»£ç ä¿®å¤æ–¹æ³•ä¾èµ–äºæ•°æ®é©±åŠ¨çš„æ–¹æ³•ï¼Œå¹¶ä¸”é€šå¸¸éš¾ä»¥è§£å†³å¤æ‚çš„ç¼–ç¨‹é—®é¢˜ã€‚ä¸ºäº†è§£é‡Šå‰æ‰€æœªæœ‰çš„ç¼–ç¨‹é—®é¢˜çš„è‡ªç„¶è¯­è¨€ï¼Œä½¿ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ¥ç”Ÿæˆä»£ç åé¦ˆè‡³å…³é‡è¦ã€‚LLMç”Ÿæˆçš„åé¦ˆæ¯”ç¼–è¯‘å™¨ç”Ÿæˆçš„é”™è¯¯æ¶ˆæ¯æ›´æ˜“äºç†è§£ï¼Œè€Œå€ŸåŠ©äººç±»åé¦ˆçš„å¼ºåŒ–å­¦ä¹ ï¼ˆRLHFï¼‰é€šè¿‡èå…¥äººç±»å‚ä¸å¾ªç¯è¿›ä¸€æ­¥æé«˜äº†åé¦ˆè´¨é‡ï¼Œæœ‰åŠ©äºåˆå­¦è€…ä»é›¶å¼€å§‹äº¤äº’å¼åœ°å­¦ä¹ ç¼–ç¨‹ã€‚æˆ‘ä»¬æ­£åœ¨åº”ç”¨RLHFå¾®è°ƒæŠ€æœ¯ï¼Œä»¥æœŸæœ›è·å¾—è‹æ ¼æ‹‰åº•å¼çš„å›åº”ï¼Œå¦‚å¸¦æœ‰æç¤ºçš„é—®é¢˜ä»¥è§£å†³ç¼–ç¨‹é—®é¢˜ã€‚æˆ‘ä»¬æå‡ºé€šè¿‡RLHFå¾®è°ƒLLMçš„ä»£ç åé¦ˆç”Ÿæˆå·¥å…·ï¼Œå³ç»“åˆä¸¤ç§å¼€æºLLMæ¨¡å‹å’Œä¸¤ç§ä¸åŒæœ€å…ˆè¿›ä¼˜åŒ–æŠ€æœ¯çš„è‡ªåŠ¨åŒ–ä»£ç è¯„ä¼°ä¸RLHFï¼ˆACE-RLHFï¼‰ã€‚åé¦ˆçš„è´¨é‡æ˜¯åœ¨åŒ…å«åŸºæœ¬ç¼–ç¨‹é—®é¢˜å’Œç«èµ›çº§åˆ«ç¼–ç¨‹é—®é¢˜çš„ä¸¤ä¸ªåŸºå‡†æ•°æ®é›†ä¸Šè¿›è¡Œè¯„ä»·çš„ï¼Œå…¶ä¸­åè€…æ˜¯ç”±æˆ‘ä»¬æå‡ºçš„ã€‚åœ¨è‡ªåŠ¨è¯„ä¼°ä¸­ä½¿ç”¨Llama-3-7B-è¿‘ç«¯ç­–ç•¥ä¼˜åŒ–ï¼Œæˆ‘ä»¬å®ç°äº†æ¯”æ— RLçš„æœ€å…ˆè¿›æŠ€æœ¯é«˜2-5%çš„å‡†ç¡®ç‡ï¼Œä¸æ— å¥–åŠ±æ¨¡å‹çš„AIåé¦ˆçš„å¼ºåŒ–å­¦ä¹ ï¼ˆRLAIFï¼‰ç›¸æ¯”ï¼Œå‡†ç¡®ç‡ç›¸ä¼¼æˆ–ç•¥é«˜ã€‚åœ¨è¿›è¡Œæ‰‹åŠ¨è¯„ä¼°æ—¶ï¼Œä½¿ç”¨GPT-3.5çš„æœ€ä½³nä¼˜åŒ–æ–¹æ¡ˆï¼Œæˆ‘ä»¬å®ç°äº†è¿‘40%çš„é«˜å‡†ç¡®ç‡ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.04657v1">PDF</a> 9 pages, 3 figures</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†è‡ªåŠ¨åŒ–ç¨‹åºä¿®å¤å·¥å…·çš„å‘å±•æƒ…å†µï¼ŒæŒ‡å‡ºå½“å‰æœ€å…ˆè¿›çš„ä»£ç ä¿®å¤æ–¹æ³•ä¸»è¦ä¾èµ–äºæ•°æ®é©±åŠ¨çš„æ–¹æ³•ï¼Œå¯¹äºå¤æ‚çš„ç¼–ç¨‹é—®é¢˜å¾€å¾€æ— æ³•æä¾›æœ‰æ•ˆçš„è§£å†³æ–¹æ¡ˆã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œå¼•å…¥äº†å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ç”¨äºä»£ç åé¦ˆç”Ÿæˆçš„æ–¹æ³•ã€‚æ–‡ç« è¿›ä¸€æ­¥é˜è¿°äº†å¼ºåŒ–å­¦ä¹ ä¸äººç±»åé¦ˆï¼ˆRLHFï¼‰æŠ€æœ¯å¯¹äºæé«˜ä»£ç åé¦ˆè´¨é‡çš„é‡è¦æ€§ï¼Œå¹¶é€šè¿‡ç²¾ç»†è°ƒæ•´LLMä¸RLHFæŠ€æœ¯çš„ç»“åˆï¼Œæå‡ºäº†ä¸€ç§æ–°çš„ä»£ç åé¦ˆç”Ÿæˆå·¥å…·â€”â€”ACE-RLHFã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨åŸºå‡†æ•°æ®é›†ä¸Šçš„è¡¨ç°ä¼˜äºæ— å¼ºåŒ–å­¦ä¹ çš„æŠ€æœ¯ï¼Œå¹¶å®ç°äº†è¾ƒé«˜çš„å‡†ç¡®æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è‡ªåŠ¨åŒ–ç¨‹åºä¿®å¤å·¥å…·éœ€è¦èƒ½å¤Ÿç”Ÿæˆåé¦ˆå¹¶å»ºè®®ä¿®å¤é”™è¯¯ä»£ç çš„æ–¹æ³•ã€‚</li>
<li>å½“å‰æœ€å…ˆè¿›çš„ä»£ç ä¿®å¤æ–¹æ³•ä¸»è¦ä¾èµ–äºæ•°æ®é©±åŠ¨çš„æ–¹æ³•ï¼Œä½†å¯¹äºå¤æ‚çš„ç¼–ç¨‹é—®é¢˜å¸¸å¸¸å¤±æ•ˆã€‚</li>
<li>å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è¢«ç”¨äºç”Ÿæˆæ›´æ˜“äºç†è§£çš„ä»£ç åé¦ˆï¼Œç›¸è¾ƒäºç¼–è¯‘å™¨ç”Ÿæˆçš„é”™è¯¯æ¶ˆæ¯æ›´å…·ä¼˜åŠ¿ã€‚</li>
<li>å¼ºåŒ–å­¦ä¹ ä¸äººç±»åé¦ˆï¼ˆRLHFï¼‰æŠ€æœ¯èƒ½æé«˜ä»£ç åé¦ˆçš„è´¨é‡ï¼Œå¸®åŠ©æ–°æ‰‹ç¨‹åºå‘˜äº’åŠ¨å­¦ä¹ ç¼–ç¨‹ã€‚</li>
<li>ACE-RLHFæ˜¯ç»“åˆäº†LLMå’ŒRLHFæŠ€æœ¯çš„ä»£ç åé¦ˆç”Ÿæˆå·¥å…·ï¼Œé€šè¿‡ç²¾ç»†è°ƒæ•´å®ç°é¢„æœŸçš„ç¤¾ä¼šæ€§é—®ç­”ååº”ï¼Œå¦‚æä¾›é—®é¢˜å’Œæç¤ºä»¥è§£å†³ç¼–ç¨‹é—®é¢˜ã€‚</li>
<li>å®éªŒç»“æœè¡¨æ˜ï¼ŒACE-RLHFåœ¨åŸºå‡†æ•°æ®é›†ä¸Šçš„è¡¨ç°ä¼˜äºæ— å¼ºåŒ–å­¦ä¹ çš„æŠ€æœ¯ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.04657">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-0c6752dafe19712223b4786b0fd77b84.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b9dbafc176ab8f2066d489c9d458900a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-751a74ca17a61e4009abde22c8970965.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-902b350107b081368a21ec92eed8cf31.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4a8acf13871f1fef3126a1e5238e292d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e2edd1b2b893d0024dfa5c81c39e3d59.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-7d48a79e94056bf80d6c96f50b2b87de.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-974e33e1e3219bdf8db5ecbb28811a88.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="Splits-A-Flexible-Dataset-for-Evaluating-a-Modelâ€™s-Demographic-Social-Inference"><a href="#Splits-A-Flexible-Dataset-for-Evaluating-a-Modelâ€™s-Demographic-Social-Inference" class="headerlink" title="Splits! A Flexible Dataset for Evaluating a Modelâ€™s Demographic Social   Inference"></a>Splits! A Flexible Dataset for Evaluating a Modelâ€™s Demographic Social   Inference</h2><p><strong>Authors:Eylon Caplan, Tania Chakraborty, Dan Goldwasser</strong></p>
<p>Understanding how people of various demographics think, feel, and express themselves (collectively called group expression) is essential for social science and underlies the assessment of bias in Large Language Models (LLMs). While LLMs can effectively summarize group expression when provided with empirical examples, coming up with generalizable theories of how a groupâ€™s expression manifests in real-world text is challenging. In this paper, we define a new task called Group Theorization, in which a system must write theories that differentiate expression across demographic groups. We make available a large dataset on this task, Splits!, constructed by splitting Reddit posts by neutral topics (e.g. sports, cooking, and movies) and by demographics (e.g. occupation, religion, and race). Finally, we suggest a simple evaluation framework for assessing how effectively a method can generate â€˜betterâ€™ theories about group expression, backed by human validation. We publicly release the raw corpora and evaluation scripts for Splits! to help researchers assess how methods inferâ€“and potentially misrepresentâ€“group differences in expression. We make Splits! and our evaluation module available at <a target="_blank" rel="noopener" href="https://github.com/eyloncaplan/splits">https://github.com/eyloncaplan/splits</a>. </p>
<blockquote>
<p>ç†è§£ä¸åŒäººå£ç»Ÿè®¡ç¾¤ä½“å¦‚ä½•æ€è€ƒã€æ„Ÿå—å’Œè¡¨è¾¾è‡ªèº«ï¼ˆç»Ÿç§°ä¸ºç¾¤ä½“è¡¨è¾¾ï¼‰å¯¹ç¤¾ä¼šç§‘å­¦è‡³å…³é‡è¦ï¼Œä¹Ÿæ˜¯è¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åè§çš„åŸºç¡€ã€‚è™½ç„¶LLMåœ¨æä¾›å®è¯ä¾‹å­æ—¶å¯ä»¥æœ‰æ•ˆåœ°æ€»ç»“ç¾¤ä½“è¡¨è¾¾ï¼Œä½†è¦æƒ³æå‡ºä¸€ç§å¯æ¨å¹¿çš„ç†è®ºï¼Œé˜è¿°ç¾¤ä½“è¡¨è¾¾å¦‚ä½•åœ¨ç°å®æ–‡æœ¬ä¸­å‘ˆç°ï¼Œä»å…·æœ‰æŒ‘æˆ˜æ€§ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬å®šä¹‰äº†ä¸€ä¸ªæ–°ä»»åŠ¡ï¼Œç§°ä¸ºâ€œç¾¤ä½“ç†è®ºåŒ–â€ï¼Œç³»ç»Ÿå¿…é¡»åœ¨è¯¥ä»»åŠ¡ä¸­æ’°å†™åŒºåˆ†ä¸åŒäººå£ç»Ÿè®¡ç¾¤ä½“è¡¨è¾¾çš„ç†è®ºã€‚æˆ‘ä»¬ä¸ºæ­¤ä»»åŠ¡æ„å»ºäº†ä¸€ä¸ªå¤§å‹æ•°æ®é›†â€œSplits!â€ï¼Œé€šè¿‡Redditå¸–å­æŒ‰ä¸­æ€§ä¸»é¢˜ï¼ˆå¦‚ä½“è‚²ã€çƒ¹é¥ªå’Œç”µå½±ï¼‰å’Œäººå£ç»Ÿè®¡ï¼ˆå¦‚èŒä¸šã€å®—æ•™å’Œç§æ—ï¼‰è¿›è¡Œåˆ’åˆ†ã€‚æœ€åï¼Œæˆ‘ä»¬ä¸ºè¯„ä¼°æ–¹æ³•ç”Ÿæˆå…³äºç¾¤ä½“è¡¨è¾¾â€œæ›´å¥½â€ç†è®ºçš„æœ‰æ•ˆæ€§æä¾›äº†ä¸€ä¸ªç®€å•çš„è¯„ä¼°æ¡†æ¶ï¼Œè¯¥æ¡†æ¶ä»¥äººå·¥éªŒè¯ä¸ºæ”¯æŒã€‚æˆ‘ä»¬å…¬å¼€å‘å¸ƒåŸå§‹è¯­æ–™åº“å’Œè¯„ä¼°è„šæœ¬æ¥å¸®åŠ©ç ”ç©¶äººå‘˜è¯„ä¼°æ–¹æ³•å¦‚ä½•æ¨æ–­â€”â€”ä»¥åŠå¯èƒ½è¯¯ä»£è¡¨â€”â€”ç¾¤ä½“åœ¨è¡¨è¾¾ä¸Šçš„å·®å¼‚ã€‚æˆ‘ä»¬åœ¨<a target="_blank" rel="noopener" href="https://github.com/eyloncaplan/splits%E5%A4%84%E6%8F%90%E4%BE%9BSplits!%E5%92%8C%E6%88%91%E4%BB%AC%E7%9A%84%E8%AF%84%E4%BC%B0%E6%A8%A1%E5%9D%97%E3%80%82">https://github.com/eyloncaplan/splitså¤„æä¾›Splits!å’Œæˆ‘ä»¬çš„è¯„ä¼°æ¨¡å—ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.04640v1">PDF</a> Under review for COLM 2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡å®šä¹‰äº†ä¸€é¡¹æ–°ä»»åŠ¡â€”â€”ç¾¤ä½“ç†è®ºåŒ–ï¼Œæ—¨åœ¨è®©ç³»ç»Ÿèƒ½å¤Ÿæ’°å†™åŒºåˆ†ä¸åŒç¾¤ä½“è¡¨è¾¾çš„ç†è®ºã€‚ä¸ºå®Œæˆæ­¤ä»»åŠ¡ï¼Œæ„å»ºäº†åä¸ºSplits!çš„å¤§å‹æ•°æ®é›†ï¼Œé€šè¿‡Redditä¸Šçš„ä¸­æ€§è¯é¢˜å’Œäººå£ç»Ÿè®¡ä¿¡æ¯æ¥åˆ†ç±»å¸–å­ã€‚åŒæ—¶ï¼Œæå‡ºäº†ä¸€ç§ç®€å•çš„è¯„ä¼°æ¡†æ¶ï¼Œä»¥è¯„ä¼°æ–¹æ³•ç”Ÿæˆå…³äºç¾¤ä½“è¡¨è¾¾â€œæ›´å¥½â€ç†è®ºçš„æœ‰æ•ˆæ€§ï¼Œå¹¶é€šè¿‡äººå·¥éªŒè¯è¿›è¡Œæ”¯æŒã€‚å…¬å¼€å‘å¸ƒäº†åŸå§‹æ•°æ®é›†å’Œè¯„ä¼°è„šæœ¬ï¼Œä»¥å¸®åŠ©ç ”ç©¶äººå‘˜è¯„ä¼°æ–¹æ³•å¦‚ä½•æ¨æ–­â€”â€”ä»¥åŠå¯èƒ½è¯¯ä»£è¡¨â€”â€”ç¾¤ä½“åœ¨è¡¨è¾¾ä¸Šçš„å·®å¼‚ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è®ºæ–‡å¼ºè°ƒäº†ç†è§£ä¸åŒç¾¤ä½“çš„æ€è€ƒã€æ„Ÿå—å’Œè¡¨è¾¾æ–¹å¼å¯¹ç¤¾ä¼šç§‘å­¦çš„é‡è¦æ€§ï¼Œå¯¹äºè¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹çš„åè§ä¹Ÿè‡³å…³é‡è¦ã€‚</li>
<li>æå‡ºäº†ä¸€ä¸ªæ–°ä»»åŠ¡â€”â€”ç¾¤ä½“ç†è®ºåŒ–ï¼Œè¦æ±‚ç³»ç»Ÿèƒ½å¤Ÿæ’°å†™åŒºåˆ†ä¸åŒç¾¤ä½“è¡¨è¾¾çš„ç†è®ºã€‚</li>
<li>æ„å»ºäº†ä¸€ä¸ªåä¸ºSplits!çš„å¤§å‹æ•°æ®é›†ï¼Œé€šè¿‡Redditå¸–å­æŒ‰ä¸­æ€§è¯é¢˜å’Œäººå£ç»Ÿè®¡ä¿¡æ¯è¿›è¡Œåˆ†ç±»ã€‚</li>
<li>æä¾›äº†ä¸€ä¸ªç®€å•çš„è¯„ä¼°æ¡†æ¶æ¥è¯„ä¼°æ–¹æ³•ç”Ÿæˆå…³äºç¾¤ä½“è¡¨è¾¾ç†è®ºçš„æœ‰æ•ˆæ€§ã€‚</li>
<li>è¯¥æ¡†æ¶é€šè¿‡äººå·¥éªŒè¯è¿›è¡Œæ”¯æŒï¼Œä»¥ç¡®ä¿è¯„ä¼°çš„å‡†ç¡®æ€§ã€‚</li>
<li>è®ºæ–‡å…¬å¼€æä¾›äº†åŸå§‹æ•°æ®é›†å’Œè¯„ä¼°è„šæœ¬ï¼Œä¾¿äºç ”ç©¶äººå‘˜ä½¿ç”¨ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.04640">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-823ca7efd90456db32b13e28bf4c3522.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-6e17e6bb2b266da18bc5b191380c3f7c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-64865bf99f471d06508663a2e5275efb.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-196ec1dcdb01e29975f554ee1b4990b9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5b421623971dc8edcffe740b5f5edb4e.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="The-Point-the-Vision-and-the-Text-Does-Point-Cloud-Boost-Spatial-Reasoning-of-Large-Language-Models"><a href="#The-Point-the-Vision-and-the-Text-Does-Point-Cloud-Boost-Spatial-Reasoning-of-Large-Language-Models" class="headerlink" title="The Point, the Vision and the Text: Does Point Cloud Boost Spatial   Reasoning of Large Language Models?"></a>The Point, the Vision and the Text: Does Point Cloud Boost Spatial   Reasoning of Large Language Models?</h2><p><strong>Authors:Weichen Zhang, Ruiying Peng, Chen Gao, Jianjie Fang, Xin Zeng, Kaiyuan Li, Ziyou Wang, Jinqiang Cui, Xin Wang, Xinlei Chen, Yong Li</strong></p>
<p>3D Large Language Models (LLMs) leveraging spatial information in point clouds for 3D spatial reasoning attract great attention. Despite some promising results, the role of point clouds in 3D spatial reasoning remains under-explored. In this work, we comprehensively evaluate and analyze these models to answer the research question: \textit{Does point cloud truly boost the spatial reasoning capacities of 3D LLMs?} We first evaluate the spatial reasoning capacity of LLMs with different input modalities by replacing the point cloud with the visual and text counterparts. We then propose a novel 3D QA (Question-answering) benchmark, ScanReQA, that comprehensively evaluates modelsâ€™ understanding of binary spatial relationships. Our findings reveal several critical insights: 1) LLMs without point input could even achieve competitive performance even in a zero-shot manner; 2) existing 3D LLMs struggle to comprehend the binary spatial relationships; 3) 3D LLMs exhibit limitations in exploiting the structural coordinates in point clouds for fine-grained spatial reasoning. We think these conclusions can help the next step of 3D LLMs and also offer insights for foundation models in other modalities. We release datasets and reproducible codes in the anonymous project page: <a target="_blank" rel="noopener" href="https://3d-llm.xyz/">https://3d-llm.xyz</a>. </p>
<blockquote>
<p>åœ¨ç‚¹äº‘çš„ä¸‰ç»´ç©ºé—´æ¨ç†ä¸­ï¼Œåˆ©ç”¨ä¸‰ç»´å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„ç©ºé—´ä¿¡æ¯å·²ç»å¼•èµ·äº†å¹¿æ³›å…³æ³¨ã€‚å°½ç®¡å·²ç»å–å¾—äº†ä¸€äº›ä»¤äººé¼“èˆçš„ç»“æœï¼Œä½†ç‚¹äº‘åœ¨ä¸‰ç»´ç©ºé—´æ¨ç†ä¸­çš„ä½œç”¨ä»ç„¶æœ‰å¾…æ¢ç´¢ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬å¯¹è¿™äº›æ¨¡å‹è¿›è¡Œäº†å…¨é¢çš„è¯„ä¼°å’Œåˆ†æï¼Œä»¥å›ç­”ç ”ç©¶é—®é¢˜ï¼šç‚¹äº‘æ˜¯å¦çœŸçš„æå‡äº†ä¸‰ç»´LLMçš„ç©ºé—´æ¨ç†èƒ½åŠ›ï¼Ÿæˆ‘ä»¬é¦–å…ˆé€šè¿‡ç”¨è§†è§‰å’Œæ–‡æœ¬ä»£æ›¿ç‚¹äº‘æ¥è¯„ä¼°ä¸åŒè¾“å…¥æ¨¡å¼çš„ä¸‰ç»´LLMçš„ç©ºé—´æ¨ç†èƒ½åŠ›ã€‚ç„¶åï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªæ–°çš„ä¸‰ç»´é—®ç­”ï¼ˆQAï¼‰åŸºå‡†æµ‹è¯•ScanReQAï¼Œå®ƒå…¨é¢è¯„ä¼°äº†æ¨¡å‹å¯¹äºŒå…ƒç©ºé—´å…³ç³»çš„ç†è§£ã€‚æˆ‘ä»¬çš„ç ”ç©¶å‘ç°äº†ä¸€äº›å…³é”®è§è§£ï¼š1ï¼‰å³ä½¿æ²¡æœ‰ç‚¹è¾“å…¥çš„LLMç”šè‡³å¯ä»¥åœ¨é›¶æ ·æœ¬æ–¹å¼ä¸‹å®ç°å…·æœ‰ç«äº‰åŠ›çš„æ€§èƒ½ï¼›2ï¼‰ç°æœ‰çš„ä¸‰ç»´LLMåœ¨ç†è§£äºŒå…ƒç©ºé—´å…³ç³»æ–¹é¢å­˜åœ¨å›°éš¾ï¼›3ï¼‰ä¸‰ç»´LLMåœ¨åˆ©ç”¨ç‚¹äº‘çš„ç»“æ„åæ ‡è¿›è¡Œç²¾ç»†ç©ºé—´æ¨ç†æ–¹é¢å­˜åœ¨å±€é™æ€§ã€‚æˆ‘ä»¬è®¤ä¸ºè¿™äº›ç»“è®ºå¯ä»¥å¸®åŠ©ä¸‰ç»´LLMçš„ä¸‹ä¸€æ­¥å‘å±•ï¼Œå¹¶ä¸ºå…¶ä»–æ¨¡æ€çš„åŸºç¡€æ¨¡å‹æä¾›è§è§£ã€‚æˆ‘ä»¬åœ¨åŒ¿åé¡¹ç›®é¡µé¢å‘å¸ƒäº†æ•°æ®é›†å’Œå¯å¤åˆ¶çš„ä»£ç ï¼š<a target="_blank" rel="noopener" href="https://3d-llm.xyz./">https://3d-llm.xyzã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.04540v1">PDF</a> </p>
<p><strong>Summary</strong><br>ç‚¹äº‘åœ¨3Dç©ºé—´æ¨ç†ä¸­çš„é‡è¦ä½œç”¨å¸å¼•äº†å¾ˆå¤šå…³æ³¨ï¼Œä½†å¯¹äºå…¶åœ¨3Då¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¸­çš„è§’è‰²ä»å­˜åœ¨äº‰è®®ã€‚æœ¬ç ”ç©¶é€šè¿‡è¯„ä¼°å’Œåˆ†æLLMåœ¨ä¸åŒè¾“å…¥æ¨¡å¼ä¸‹çš„ç©ºé—´æ¨ç†èƒ½åŠ›ï¼Œä»¥åŠæå‡ºæ–°çš„3Dé—®ç­”åŸºå‡†æµ‹è¯•ScanReQAï¼Œæ­ç¤ºäº†ä¸€äº›å…³é”®è§è§£ã€‚ç ”ç©¶å‘ç°ï¼Œå³ä½¿æ²¡æœ‰ç‚¹äº‘è¾“å…¥ï¼ŒLLMä¹Ÿèƒ½å®ç°å‡ºè‰²çš„æ€§èƒ½ï¼›ç°æœ‰çš„3D LLMåœ¨ç†è§£äºŒå…ƒç©ºé—´å…³ç³»æ–¹é¢å­˜åœ¨å›°éš¾ï¼›å¹¶ä¸”åœ¨åˆ©ç”¨ç‚¹äº‘çš„ç»“æ„åæ ‡è¿›è¡Œç²¾ç»†ç©ºé—´æ¨ç†æ–¹é¢è¡¨ç°å‡ºå±€é™æ€§ã€‚è¿™äº›ç»“è®ºæœ‰åŠ©äºæŒ‡å¯¼æœªæ¥3D LLMçš„ç ”ç©¶ï¼Œå¹¶ä¸ºå…¶ä»–æ¨¡æ€çš„åŸºç¡€æ¨¡å‹æä¾›å¯ç¤ºã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>ç‚¹äº‘åœ¨å¢å¼ºLLMçš„ç©ºé—´æ¨ç†èƒ½åŠ›ä¸­æ‰®æ¼”é‡è¦è§’è‰²ï¼Œä½†å…¶ä½œç”¨å°šæœªå®Œå…¨æ˜ç¡®ã€‚</li>
<li>ä¸åŒè¾“å…¥æ¨¡æ€ä¸‹çš„LLMç©ºé—´æ¨ç†èƒ½åŠ›è¯„ä¼°æ˜¾ç¤ºï¼Œå³ä½¿æ²¡æœ‰ç‚¹äº‘è¾“å…¥ï¼ŒLLMä¹Ÿèƒ½å®ç°è‰¯å¥½æ€§èƒ½ã€‚</li>
<li>ç°æœ‰çš„3D LLMåœ¨ç†è§£äºŒå…ƒç©ºé—´å…³ç³»æ–¹é¢å­˜åœ¨æŒ‘æˆ˜ã€‚</li>
<li>3D LLMåœ¨åˆ©ç”¨ç‚¹äº‘çš„ç»“æ„åæ ‡è¿›è¡Œç²¾ç»†ç©ºé—´æ¨ç†æ–¹é¢å­˜åœ¨å±€é™æ€§ã€‚</li>
<li>æœ¬ç ”ç©¶æå‡ºä¸€ä¸ªæ–°çš„åŸºå‡†æµ‹è¯•ScanReQAï¼Œç”¨äºå…¨é¢è¯„ä¼°æ¨¡å‹å¯¹äºŒå…ƒç©ºé—´å…³ç³»çš„ç†è§£ã€‚</li>
<li>ç ”ç©¶ç»“è®ºæœ‰åŠ©äºæŒ‡å¯¼æœªæ¥å¯¹3D LLMçš„ç ”ç©¶ï¼Œå¹¶ä¸ºå…¶ä»–æ¨¡æ€çš„åŸºç¡€æ¨¡å‹æä¾›å¯ç¤ºã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.04540">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-3f5e42d323fbda6e751ef1ca9a07db0b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-72d05a9e3a6e506dbba78490341f4d10.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a5af4979e57aec10c6b93d04790bd9ac.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a45e0011ebf43aa33d02c6c19e9ad4ea.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-7e838922802b0afb806704fbef717aad.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b1c824358848e452d2a471d4be2714a4.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="Trust-Region-Preference-Approximation-A-simple-and-stable-reinforcement-learning-algorithm-for-LLM-reasoning"><a href="#Trust-Region-Preference-Approximation-A-simple-and-stable-reinforcement-learning-algorithm-for-LLM-reasoning" class="headerlink" title="Trust Region Preference Approximation: A simple and stable reinforcement   learning algorithm for LLM reasoning"></a>Trust Region Preference Approximation: A simple and stable reinforcement   learning algorithm for LLM reasoning</h2><p><strong>Authors:Xuerui Su, Shufang Xie, Guoqing Liu, Yingce Xia, Renqian Luo, Peiran Jin, Zhiming Ma, Yue Wang, Zun Wang, Yuting Liu</strong></p>
<p>Recently, Large Language Models (LLMs) have rapidly evolved, approaching Artificial General Intelligence (AGI) while benefiting from large-scale reinforcement learning to enhance Human Alignment (HA) and Reasoning. Recent reward-based optimization algorithms, such as Proximal Policy Optimization (PPO) and Group Relative Policy Optimization (GRPO) have achieved significant performance on reasoning tasks, whereas preference-based optimization algorithms such as Direct Preference Optimization (DPO) significantly improve the performance of LLMs on human alignment. However, despite the strong performance of reward-based optimization methods in alignment tasks , they remain vulnerable to reward hacking. Furthermore, preference-based algorithms (such as Online DPO) havenâ€™t yet matched the performance of reward-based optimization algorithms (like PPO) on reasoning tasks, making their exploration in this specific area still a worthwhile pursuit. Motivated by these challenges, we propose the Trust Region Preference Approximation (TRPA) algorithm, which integrates rule-based optimization with preference-based optimization for reasoning tasks. As a preference-based algorithm, TRPA naturally eliminates the reward hacking issue. TRPA constructs preference levels using predefined rules, forms corresponding preference pairs, and leverages a novel optimization algorithm for RL training with a theoretical monotonic improvement guarantee. Experimental results demonstrate that TRPA not only achieves competitive performance on reasoning tasks but also exhibits robust stability. The code of this paper are released and updating on <a target="_blank" rel="noopener" href="https://github.com/XueruiSu/Trust-Region-Preference-Approximation.git">https://github.com/XueruiSu/Trust-Region-Preference-Approximation.git</a>. </p>
<blockquote>
<p>æœ€è¿‘ï¼Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è¿…é€Ÿè¿›åŒ–ï¼Œåœ¨å—ç›Šäºå¤§è§„æ¨¡å¼ºåŒ–å­¦ä¹ æé«˜äººç±»å¯¹é½ï¼ˆHAï¼‰å’Œæ¨ç†çš„åŒæ—¶ï¼Œé€æ¸æ¥è¿‘äººå·¥æ™ºèƒ½é€šç”¨æ™ºèƒ½ï¼ˆAGIï¼‰ã€‚åŸºäºå¥–åŠ±çš„ä¼˜åŒ–ç®—æ³•ï¼Œå¦‚è¿‘ç«¯ç­–ç•¥ä¼˜åŒ–ï¼ˆPPOï¼‰å’Œç¾¤ä½“ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆGRPOï¼‰ï¼Œåœ¨æ¨ç†ä»»åŠ¡ä¸Šå–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½æˆå°±ã€‚è€ŒåŸºäºåå¥½çš„ä¼˜åŒ–ç®—æ³•ï¼Œå¦‚ç›´æ¥åå¥½ä¼˜åŒ–ï¼ˆDPOï¼‰ï¼Œåˆ™æ˜¾è‘—æé«˜äº†è¯­è¨€æ¨¡å‹åœ¨äººæœºå¯¹é½æ–¹é¢çš„æ€§èƒ½ã€‚ç„¶è€Œï¼Œå°½ç®¡åŸºäºå¥–åŠ±çš„ä¼˜åŒ–æ–¹æ³•åœ¨äººæœºå¯¹é½ä»»åŠ¡ä¸­è¡¨ç°å‡ºå¼ºå¤§çš„æ€§èƒ½ï¼Œä½†å®ƒä»¬ä»ç„¶å®¹æ˜“å—åˆ°å¥–åŠ±æ“çºµçš„å½±å“ã€‚æ­¤å¤–ï¼ŒåŸºäºåå¥½çš„ç®—æ³•ï¼ˆå¦‚åœ¨çº¿DPOï¼‰åœ¨æ¨ç†ä»»åŠ¡ä¸Šçš„æ€§èƒ½å°šæœªè¾¾åˆ°åŸºäºå¥–åŠ±çš„ä¼˜åŒ–ç®—æ³•ï¼ˆå¦‚PPOï¼‰çš„æ°´å¹³ï¼Œè¿™ä½¿å¾—åœ¨è¿™ä¸€ç‰¹å®šé¢†åŸŸçš„æ¢ç´¢ä»ç„¶å…·æœ‰ä»·å€¼ã€‚é’ˆå¯¹è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¿¡ä»»åŒºåŸŸåå¥½é€¼è¿‘ï¼ˆTRPAï¼‰ç®—æ³•ï¼Œè¯¥ç®—æ³•å°†åŸºäºè§„åˆ™çš„ä¼˜åŒ–ä¸åŸºäºåå¥½çš„ä¼˜åŒ–ç›¸ç»“åˆï¼Œç”¨äºæ¨ç†ä»»åŠ¡ã€‚ä½œä¸ºä¸€ç§åŸºäºåå¥½çš„ç®—æ³•ï¼ŒTRPAè‡ªç„¶åœ°è§£å†³äº†å¥–åŠ±æ“çºµçš„é—®é¢˜ã€‚TRPAä½¿ç”¨é¢„å®šä¹‰çš„è§„åˆ™æ„å»ºåå¥½å±‚æ¬¡ï¼Œå½¢æˆç›¸åº”çš„åå¥½å¯¹ï¼Œå¹¶åˆ©ç”¨ä¸€ç§æ–°çš„ä¼˜åŒ–ç®—æ³•è¿›è¡ŒRLè®­ç»ƒï¼Œè¯¥ç®—æ³•å…·æœ‰ç†è®ºä¸Šçš„å•è°ƒæ”¹è¿›ä¿è¯ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒTRPAä¸ä»…åœ¨æ¨ç†ä»»åŠ¡ä¸Šå…·æœ‰è‰¯å¥½çš„æ€§èƒ½ï¼Œè€Œä¸”è¡¨ç°å‡ºç¨³å¥çš„ç¨³å®šæ€§ã€‚æœ¬æ–‡çš„ä»£ç å·²å‘å¸ƒå¹¶åœ¨<a target="_blank" rel="noopener" href="https://github.com/XueruiSu/Trust-Region-Preference-Approximation.git%E4%B8%8A%E8%BF%9B%E8%A1%8C%E6%9B%B4%E6%96%B0%E3%80%82">https://github.com/XueruiSu/Trust-Region-Preference-Approximation.gitä¸Šè¿›è¡Œæ›´æ–°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.04524v1">PDF</a> 10pages</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ¢è®¨äº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨äººå·¥é€šç”¨æ™ºèƒ½ï¼ˆAGIï¼‰æ–¹é¢çš„æœ€æ–°è¿›å±•ã€‚é€šè¿‡å¤§è§„æ¨¡å¼ºåŒ–å­¦ä¹ æå‡äººç±»å¯¹é½ï¼ˆHAï¼‰å’Œæ¨ç†èƒ½åŠ›ã€‚æ–‡ç« ä»‹ç»äº†åŸºäºå¥–åŠ±çš„ä¼˜åŒ–ç®—æ³•ï¼Œå¦‚è¿‘ç«¯ç­–ç•¥ä¼˜åŒ–ï¼ˆPPOï¼‰å’Œç»„ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆGRPOï¼‰ï¼Œåœ¨æ¨ç†ä»»åŠ¡ä¸Šçš„ä¼˜å¼‚è¡¨ç°ï¼›åŒæ—¶ä¹Ÿæåˆ°äº†åŸºäºåå¥½çš„ä¼˜åŒ–ç®—æ³•ï¼Œå¦‚ç›´æ¥åå¥½ä¼˜åŒ–ï¼ˆDPOï¼‰ï¼Œåœ¨LLMçš„äººç±»å¯¹é½æ–¹é¢è¡¨ç°å‡ºè‰²ã€‚ç„¶è€Œï¼Œå°½ç®¡åŸºäºå¥–åŠ±çš„ä¼˜åŒ–æ–¹æ³•åœ¨å¯¹é½ä»»åŠ¡ä¸Šè¡¨ç°å‡ºè‰²ï¼Œä½†å®ƒä»¬ä»ç„¶å®¹æ˜“å—å¥–åŠ±ä½œå¼Šçš„å½±å“ã€‚æ–‡ç« è¿˜æå‡ºäº†ä¸€ç§æ–°çš„ç®—æ³•â€”â€”ä¿¡ä»»åŒºåŸŸåå¥½è¿‘ä¼¼ï¼ˆTRPAï¼‰ï¼Œè¯¥ç®—æ³•ç»“åˆäº†åŸºäºè§„åˆ™çš„ä¼˜åŒ–å’ŒåŸºäºåå¥½çš„ä¼˜åŒ–ï¼Œæ—¨åœ¨è§£å†³æ¨ç†ä»»åŠ¡ä¸­çš„æŒ‘æˆ˜ã€‚TRPAè§£å†³äº†å¥–åŠ±ä½œå¼Šé—®é¢˜ï¼Œå¹¶é€šè¿‡é¢„è®¾è§„åˆ™æ„å»ºåå¥½å±‚æ¬¡ï¼Œå½¢æˆç›¸åº”çš„åå¥½å¯¹ï¼Œç„¶åä½¿ç”¨æ–°çš„ä¼˜åŒ–ç®—æ³•è¿›è¡ŒRLè®­ç»ƒï¼Œå…·æœ‰ç†è®ºä¸Šçš„å•è°ƒæ”¹è¿›ä¿è¯ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒTRPAåœ¨æ¨ç†ä»»åŠ¡ä¸Šå…·æœ‰è‰¯å¥½çš„ç«äº‰åŠ›å’Œç¨³å¥æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰é€šè¿‡å¼ºåŒ–å­¦ä¹ æå‡äººç±»å¯¹é½ï¼ˆHAï¼‰å’Œæ¨ç†èƒ½åŠ›ã€‚</li>
<li>åŸºäºå¥–åŠ±çš„ä¼˜åŒ–ç®—æ³•å¦‚PPOå’ŒGRPOåœ¨æ¨ç†ä»»åŠ¡ä¸Šè¡¨ç°è‰¯å¥½ï¼Œä½†å­˜åœ¨å¥–åŠ±ä½œå¼Šçš„è„†å¼±æ€§ã€‚</li>
<li>åŸºäºåå¥½çš„ä¼˜åŒ–ç®—æ³•å¦‚DPOåœ¨LLMçš„äººç±»å¯¹é½æ–¹é¢æœ‰æ‰€çªç ´ï¼Œä½†ä»æœªåŒ¹é…åŸºäºå¥–åŠ±çš„ä¼˜åŒ–ç®—æ³•åœ¨æ¨ç†ä»»åŠ¡ä¸Šçš„æ€§èƒ½ã€‚</li>
<li>TRPAç®—æ³•ç»“åˆäº†åŸºäºè§„åˆ™çš„ä¼˜åŒ–å’ŒåŸºäºåå¥½çš„ä¼˜åŒ–ï¼Œè§£å†³äº†å¥–åŠ±ä½œå¼Šé—®é¢˜ï¼Œå¹¶åœ¨æ¨ç†ä»»åŠ¡ä¸Šè¡¨ç°å‡ºç«äº‰åŠ›å’Œç¨³å¥æ€§ã€‚</li>
<li>TRPAé€šè¿‡æ„å»ºåå¥½å±‚æ¬¡å’Œå½¢æˆåå¥½å¯¹æ¥è¿›è¡Œä¼˜åŒ–ï¼Œå…·æœ‰ç†è®ºä¸Šçš„å•è°ƒæ”¹è¿›ä¿è¯ã€‚</li>
<li>å‘å¸ƒçš„ä»£ç å¯ä»¥åœ¨æŒ‡å®šç½‘ç«™æ‰¾åˆ°å¹¶ä¸æ–­æ›´æ–°å®Œå–„ã€‚ç½‘ç«™é“¾æ¥ä¸ºï¼š<a target="_blank" rel="noopener" href="https://github.com/XueruiSu/Trust-Region-Preference-Approximation.git%E3%80%82">https://github.com/XueruiSu/Trust-Region-Preference-Approximation.gitã€‚</a></li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.04524">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-c2f926718cab342ce4b3f260bca0f544.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-49869cfdae1ddecc3ac4e22d19b764f9.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="VideoAgent2-Enhancing-the-LLM-Based-Agent-System-for-Long-Form-Video-Understanding-by-Uncertainty-Aware-CoT"><a href="#VideoAgent2-Enhancing-the-LLM-Based-Agent-System-for-Long-Form-Video-Understanding-by-Uncertainty-Aware-CoT" class="headerlink" title="VideoAgent2: Enhancing the LLM-Based Agent System for Long-Form Video   Understanding by Uncertainty-Aware CoT"></a>VideoAgent2: Enhancing the LLM-Based Agent System for Long-Form Video   Understanding by Uncertainty-Aware CoT</h2><p><strong>Authors:Zhuo Zhi, Qiangqiang Wu, Minghe shen, Wenbo Li, Yinchuan Li, Kun Shao, Kaiwen Zhou</strong></p>
<p>Long video understanding has emerged as an increasingly important yet challenging task in computer vision. Agent-based approaches are gaining popularity for processing long videos, as they can handle extended sequences and integrate various tools to capture fine-grained information. However, existing methods still face several challenges: (1) they often rely solely on the reasoning ability of large language models (LLMs) without dedicated mechanisms to enhance reasoning in long video scenarios; and (2) they remain vulnerable to errors or noise from external tools. To address these issues, we propose a specialized chain-of-thought (CoT) process tailored for long video analysis. Our proposed CoT with plan-adjust mode enables the LLM to incrementally plan and adapt its information-gathering strategy. We further incorporate heuristic uncertainty estimation of both the LLM and external tools to guide the CoT process. This allows the LLM to assess the reliability of newly collected information, refine its collection strategy, and make more robust decisions when synthesizing final answers. Empirical experiments show that our uncertainty-aware CoT effectively mitigates noise from external tools, leading to more reliable outputs. We implement our approach in a system called VideoAgent2, which also includes additional modules such as general context acquisition and specialized tool design. Evaluation on three dedicated long video benchmarks (and their subsets) demonstrates that VideoAgent2 outperforms the previous state-of-the-art agent-based method, VideoAgent, by an average of 13.1% and achieves leading performance among all zero-shot approaches </p>
<blockquote>
<p>é•¿è§†é¢‘ç†è§£æ˜¯è®¡ç®—æœºè§†è§‰ä¸­è¶Šæ¥è¶Šé‡è¦ä¸”å…·æœ‰æŒ‘æˆ˜æ€§çš„ä»»åŠ¡ã€‚åŸºäºä»£ç†çš„æ–¹æ³•åœ¨å¤„ç†é•¿è§†é¢‘æ—¶è¶Šæ¥è¶Šå—æ¬¢è¿ï¼Œå› ä¸ºå®ƒä»¬å¯ä»¥å¤„ç†æ‰©å±•åºåˆ—å¹¶é›†æˆå„ç§å·¥å…·æ¥æ•è·ç²¾ç»†ä¿¡æ¯ã€‚ç„¶è€Œï¼Œç°æœ‰æ–¹æ³•ä»ç„¶é¢ä¸´ä¸€äº›æŒ‘æˆ˜ï¼šï¼ˆ1ï¼‰å®ƒä»¬é€šå¸¸ä»…ä¾èµ–äºå¤§å‹è¯­è¨€æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ï¼Œè€Œæ²¡æœ‰ä¸“é—¨çš„æœºåˆ¶æ¥å¢å¼ºé•¿è§†é¢‘åœºæ™¯ä¸­çš„æ¨ç†ï¼›ï¼ˆ2ï¼‰å®ƒä»¬ä»ç„¶å®¹æ˜“å—åˆ°å¤–éƒ¨å·¥å…·çš„é”™è¯¯æˆ–å™ªå£°çš„å½±å“ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ä¸“ä¸ºé•¿è§†é¢‘åˆ†æé‡èº«å®šåˆ¶çš„ä¸“é¡¹æ€ç»´é“¾ï¼ˆCoTï¼‰æµç¨‹ã€‚æˆ‘ä»¬æå‡ºçš„å…·æœ‰è®¡åˆ’è°ƒæ•´æ¨¡å¼çš„CoTä½¿LLMèƒ½å¤Ÿå¢é‡åœ°è§„åˆ’å¹¶é€‚åº”å…¶ä¿¡æ¯æ”¶é›†ç­–ç•¥ã€‚æˆ‘ä»¬è¿›ä¸€æ­¥ç»“åˆäº†LLMå’Œå¤–éƒ¨å·¥å…·å¯å‘å¼çš„ä¸ç¡®å®šæ€§ä¼°è®¡æ¥æŒ‡å¯¼CoTè¿‡ç¨‹ã€‚è¿™ä½¿å¾—LLMèƒ½å¤Ÿè¯„ä¼°æ–°æ”¶é›†ä¿¡æ¯çš„å¯é æ€§ï¼Œæ”¹è¿›å…¶æ”¶é›†ç­–ç•¥ï¼Œå¹¶åœ¨åˆæˆæœ€ç»ˆç­”æ¡ˆæ—¶åšå‡ºæ›´ç¨³å¥çš„å†³ç­–ã€‚ç»éªŒå®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ„ŸçŸ¥ä¸ç¡®å®šæ€§çš„CoTæœ‰æ•ˆåœ°å‡è½»äº†æ¥è‡ªå¤–éƒ¨å·¥å…·çš„å™ªå£°ï¼Œä»è€Œäº§ç”Ÿäº†æ›´å¯é çš„è¾“å‡ºã€‚æˆ‘ä»¬åœ¨åä¸ºVideoAgent2çš„ç³»ç»Ÿä¸­å®ç°äº†æˆ‘ä»¬çš„æ–¹æ³•ï¼Œè¯¥ç³»ç»Ÿè¿˜åŒ…æ‹¬é€šç”¨ä¸Šä¸‹æ–‡è·å–å’Œä¸“ç”¨å·¥å…·è®¾è®¡ç­‰å…¶ä»–æ¨¡å—ã€‚åœ¨ä¸‰ä¸ªä¸“ç”¨çš„é•¿è§†é¢‘åŸºå‡†æµ‹è¯•ï¼ˆåŠå…¶å­é›†ï¼‰ä¸Šçš„è¯„ä¼°è¡¨æ˜ï¼ŒVideoAgent2è¾ƒä¹‹å‰çš„å…ˆè¿›ä»£ç†æ–¹æ³•VideoAgentå¹³å‡æé«˜äº†13.1%ï¼Œåœ¨æ‰€æœ‰é›¶æ ·æœ¬æ–¹æ³•ä¸­è¡¨ç°é¢†å…ˆã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.04471v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>é•¿è§†é¢‘ç†è§£åœ¨è®¡ç®—æœºè§†è§‰ä¸­æ˜¯ä¸€é¡¹é‡è¦ä¸”å…·æœ‰æŒ‘æˆ˜æ€§çš„ä»»åŠ¡ã€‚åŸºäºä»£ç†çš„æ–¹æ³•å¤„ç†é•¿è§†é¢‘æ—¥ç›Šæµè¡Œï¼Œèƒ½å¤Ÿå¤„ç†æ‰©å±•åºåˆ—å¹¶æ•´åˆå„ç§å·¥å…·æ¥æ•æ‰ç²¾ç»†ä¿¡æ¯ã€‚ç„¶è€Œï¼Œç°æœ‰æ–¹æ³•ä»é¢ä¸´æŒ‘æˆ˜ï¼Œå¦‚è¿‡åº¦ä¾èµ–å¤§å‹è¯­è¨€æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ï¼Œä»¥åŠæ˜“å—å¤–éƒ¨å·¥å…·é”™è¯¯æˆ–å™ªå£°çš„å½±å“ã€‚ä¸ºè§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºä¸“ä¸ºé•¿è§†é¢‘åˆ†æå®šåˆ¶çš„é“¾å¼æ€ç»´è¿‡ç¨‹ï¼Œå¹¶å¼•å…¥è®¡åˆ’è°ƒæ•´æ¨¡å¼ï¼Œä½¿å¤§å‹è¯­è¨€æ¨¡å‹èƒ½å¤Ÿé€æ­¥è§„åˆ’å¹¶è°ƒæ•´å…¶ä¿¡æ¯æ”¶é›†ç­–ç•¥ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬ç»“åˆäº†å¯å‘å¼ä¸ç¡®å®šæ€§ä¼°è®¡ï¼ŒæŒ‡å¯¼é“¾å¼æ€ç»´è¿‡ç¨‹ã€‚å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„ä¸ç¡®å®šæ€§æ„ŸçŸ¥é“¾å¼æ€ç»´æœ‰æ•ˆå‡è½»äº†å¤–éƒ¨å·¥å…·å™ªå£°çš„å½±å“ï¼Œäº§ç”Ÿäº†æ›´å¯é çš„ç»“æœã€‚æˆ‘ä»¬å®ç°äº†åä¸ºVideoAgent2çš„ç³»ç»Ÿï¼ŒåŒ…å«é€šç”¨ä¸Šä¸‹æ–‡è·å–å’Œä¸“ç”¨å·¥å…·è®¾è®¡ç­‰é¢å¤–æ¨¡å—ã€‚åœ¨ä¸‰ä¸ªä¸“ç”¨çš„é•¿è§†é¢‘åŸºå‡†æµ‹è¯•ï¼ˆåŠå…¶å­é›†ï¼‰ä¸Šçš„è¯„ä¼°æ˜¾ç¤ºï¼ŒVideoAgent2è¾ƒä¹‹å‰çš„ä»£ç†æ–¹æ³•VideoAgentå¹³å‡æé«˜äº†13.1%çš„æ€§èƒ½ï¼Œå¹¶åœ¨é›¶æ ·æœ¬æ–¹æ³•ä¸­è¡¨ç°é¢†å…ˆã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>é•¿è§†é¢‘ç†è§£æ˜¯è®¡ç®—æœºè§†è§‰ä¸­çš„é‡è¦ä¸”å…·æœ‰æŒ‘æˆ˜æ€§çš„ä»»åŠ¡ã€‚</li>
<li>åŸºäºä»£ç†çš„æ–¹æ³•å¤„ç†é•¿è§†é¢‘æ—¥ç›Šæµè¡Œï¼Œä½†éœ€è¦è§£å†³ä¾èµ–å¤§å‹è¯­è¨€æ¨¡å‹çš„æ¨ç†èƒ½åŠ›å’Œåº”å¯¹å¤–éƒ¨å·¥å…·é”™è¯¯æˆ–å™ªå£°çš„é—®é¢˜ã€‚</li>
<li>æå‡ºäº†ä¸€ä¸ªä¸“é—¨ä¸ºé•¿è§†é¢‘åˆ†æå®šåˆ¶çš„é“¾å¼æ€ç»´è¿‡ç¨‹ï¼ŒåŒ…æ‹¬è®¡åˆ’è°ƒæ•´æ¨¡å¼å’Œä¸ç¡®å®šæ€§ä¼°è®¡ã€‚</li>
<li>é€šè¿‡å®éªŒéªŒè¯äº†ä¸ç¡®å®šæ€§æ„ŸçŸ¥é“¾å¼æ€ç»´çš„æœ‰æ•ˆæ€§ï¼Œå‡è½»äº†å¤–éƒ¨å·¥å…·å™ªå£°çš„å½±å“ã€‚</li>
<li>å®ç°äº†ä¸€ä¸ªåä¸ºVideoAgent2çš„ç³»ç»Ÿï¼Œè¯¥ç³»ç»Ÿç»“åˆäº†é€šç”¨ä¸Šä¸‹æ–‡è·å–å’Œä¸“ç”¨å·¥å…·è®¾è®¡ç­‰åŠŸèƒ½ã€‚</li>
<li>VideoAgent2åœ¨å¤šä¸ªé•¿è§†é¢‘åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜äºä¹‹å‰çš„ä»£ç†æ–¹æ³•VideoAgentã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.04471">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-8f2f916368ef209252a0e43e1326965a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d3e7f5b73d7cb6430c4ac6fe588fd4e6.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-04301a2abbe8f7cf257d92bb09b665b6.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="OmniDrive-A-Holistic-Vision-Language-Dataset-for-Autonomous-Driving-with-Counterfactual-Reasoning"><a href="#OmniDrive-A-Holistic-Vision-Language-Dataset-for-Autonomous-Driving-with-Counterfactual-Reasoning" class="headerlink" title="OmniDrive: A Holistic Vision-Language Dataset for Autonomous Driving   with Counterfactual Reasoning"></a>OmniDrive: A Holistic Vision-Language Dataset for Autonomous Driving   with Counterfactual Reasoning</h2><p><strong>Authors:Shihao Wang, Zhiding Yu, Xiaohui Jiang, Shiyi Lan, Min Shi, Nadine Chang, Jan Kautz, Ying Li, Jose M. Alvarez</strong></p>
<p>The advances in vision-language models (VLMs) have led to a growing interest in autonomous driving to leverage their strong reasoning capabilities. However, extending these capabilities from 2D to full 3D understanding is crucial for real-world applications. To address this challenge, we propose OmniDrive, a holistic vision-language dataset that aligns agent models with 3D driving tasks through counterfactual reasoning. This approach enhances decision-making by evaluating potential scenarios and their outcomes, similar to human drivers considering alternative actions. Our counterfactual-based synthetic data annotation process generates large-scale, high-quality datasets, providing denser supervision signals that bridge planning trajectories and language-based reasoning. Futher, we explore two advanced OmniDrive-Agent frameworks, namely Omni-L and Omni-Q, to assess the importance of vision-language alignment versus 3D perception, revealing critical insights into designing effective LLM-agents. Significant improvements on the DriveLM Q&amp;A benchmark and nuScenes open-loop planning demonstrate the effectiveness of our dataset and methods. </p>
<blockquote>
<p>è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰çš„è¿›æ­¥å¼•å‘äº†äººä»¬å¯¹è‡ªåŠ¨é©¾é©¶åˆ©ç”¨å…¶å¼ºå¤§æ¨ç†èƒ½åŠ›çš„æ—¥ç›Šå¢é•¿çš„å…´è¶£ã€‚ç„¶è€Œï¼Œä»äºŒç»´æ‰©å±•åˆ°å…¨é¢çš„ä¸‰ç»´ç†è§£å¯¹äºå®é™…åº”ç”¨è‡³å…³é‡è¦ã€‚ä¸ºäº†åº”å¯¹è¿™ä¸€æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†OmniDriveï¼Œè¿™æ˜¯ä¸€ä¸ªå…¨é¢çš„è§†è§‰è¯­è¨€æ•°æ®é›†ï¼Œå®ƒé€šè¿‡åäº‹å®æ¨ç†å°†ä»£ç†æ¨¡å‹ä¸ä¸‰ç»´é©¾é©¶ä»»åŠ¡å¯¹é½ã€‚è¿™ç§æ–¹æ³•é€šè¿‡è¯„ä¼°æ½œåœ¨åœºæ™¯åŠå…¶ç»“æœæ¥å¢å¼ºå†³ç­–åˆ¶å®šï¼Œä¸äººç±»é©¾é©¶å‘˜è€ƒè™‘æ›¿ä»£è¡ŒåŠ¨çš„æ–¹å¼ç›¸ä¼¼ã€‚æˆ‘ä»¬çš„åŸºäºåäº‹å®çš„åˆæˆæ•°æ®æ ‡æ³¨è¿‡ç¨‹ç”Ÿæˆå¤§è§„æ¨¡é«˜è´¨é‡æ•°æ®é›†ï¼Œæä¾›æ›´å¯†é›†çš„ç›‘ç£ä¿¡å·ï¼Œä»è€Œå»ºç«‹è§„åˆ’è½¨è¿¹å’Œè¯­è¨€åŸºç¡€æ¨ç†ä¹‹é—´çš„æ¡¥æ¢ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æ¢ç´¢äº†ä¸¤ä¸ªå…ˆè¿›çš„OmniDrive-Agentæ¡†æ¶ï¼Œå³Omni-Lå’ŒOmni-Qï¼Œä»¥è¯„ä¼°è§†è§‰è¯­è¨€å¯¹é½ä¸ä¸‰ç»´æ„ŸçŸ¥çš„é‡è¦æ€§ï¼Œä¸ºè®¾è®¡æœ‰æ•ˆçš„LLM-agentsæä¾›å…³é”®è§è§£ã€‚åœ¨DriveLMé—®ç­”åŸºå‡†æµ‹è¯•å’ŒnuSceneså¼€æ”¾å¾ªç¯è§„åˆ’ä¸Šçš„æ˜¾è‘—æ”¹è¿›è¯æ˜äº†æˆ‘ä»¬çš„æ•°æ®é›†å’Œæ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.04348v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>åœ¨è‡ªåŠ¨é©¾é©¶é¢†åŸŸï¼Œè§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰çš„è¿›æ­¥å¼•å‘äº†å¯¹å…¶å¼ºå¤§æ¨ç†èƒ½åŠ›çš„å…´è¶£ã€‚ç„¶è€Œï¼Œä»äºŒç»´æ‰©å±•åˆ°ä¸‰ç»´ç†è§£å¯¹äºå®é™…åº”ç”¨è‡³å…³é‡è¦ã€‚ä¸ºäº†åº”å¯¹è¿™ä¸€æŒ‘æˆ˜ï¼Œæœ¬æ–‡æå‡ºäº†OmniDriveï¼Œè¿™æ˜¯ä¸€ä¸ªå…¨é¢çš„è§†è§‰è¯­è¨€æ•°æ®é›†ï¼Œå®ƒé€šè¿‡åäº‹å®æ¨ç†å°†æ™ºèƒ½ä½“æ¨¡å‹ä¸ä¸‰ç»´é©¾é©¶ä»»åŠ¡å¯¹é½ã€‚æ­¤æ–¹æ³•é€šè¿‡è¯„ä¼°æ½œåœ¨åœºæ™¯åŠå…¶ç»“æœæ¥å¢å¼ºå†³ç­–åˆ¶å®šï¼Œç±»ä¼¼äºäººç±»é©¾é©¶å‘˜è€ƒè™‘æ›¿ä»£è¡ŒåŠ¨ã€‚æ­¤å¤–ï¼Œæœ¬æ–‡è¿˜æ¢ç´¢äº†ä¸¤ä¸ªå…ˆè¿›çš„OmniDrive-Agentæ¡†æ¶ï¼Œå³Omni-Lå’ŒOmni-Qï¼Œä»¥è¯„ä¼°è§†è§‰è¯­è¨€å¯¹é½ä¸ä¸‰ç»´æ„ŸçŸ¥çš„é‡è¦æ€§ï¼Œä¸ºè®¾è®¡æœ‰æ•ˆçš„LLMä»£ç†æä¾›å…³é”®è§è§£ã€‚åœ¨DriveLMé—®ç­”åŸºå‡†æµ‹è¯•å’ŒnuSceneså¼€æ”¾å¾ªç¯è§„åˆ’ä¸Šçš„æ˜¾è‘—æ”¹å–„è¯æ˜äº†æœ¬æ–‡æ•°æ®é›†å’Œæ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰çš„è¿›æ­¥æ¿€å‘äº†è‡ªåŠ¨é©¾é©¶é¢†åŸŸçš„å…´è¶£ï¼Œä½†å…¶ä»äºŒç»´åˆ°ä¸‰ç»´ç†è§£çš„æ‰©å±•è‡³å…³é‡è¦ã€‚</li>
<li>OmniDriveæ˜¯ä¸€ä¸ªå…¨é¢çš„è§†è§‰è¯­è¨€æ•°æ®é›†ï¼Œé€šè¿‡åäº‹å®æ¨ç†å°†æ™ºèƒ½ä½“æ¨¡å‹ä¸ä¸‰ç»´é©¾é©¶ä»»åŠ¡ç›¸ç»“åˆï¼Œä»¥å¢å¼ºå†³ç­–è¿‡ç¨‹ã€‚</li>
<li>åäº‹å®æ¨ç†æ–¹æ³•è¯„ä¼°æ½œåœ¨åœºæ™¯å’Œç»“æœï¼Œç±»ä¼¼äºäººç±»é©¾é©¶å‘˜è€ƒè™‘æ›¿ä»£è¡ŒåŠ¨ã€‚</li>
<li>OmniDrive-Agentæ¡†æ¶åŒ…æ‹¬Omni-Lå’ŒOmni-Qï¼Œç”¨äºè¯„ä¼°è§†è§‰è¯­è¨€å¯¹é½ä¸ä¸‰ç»´æ„ŸçŸ¥çš„é‡è¦æ€§ã€‚</li>
<li>OmniDriveçš„æ•°æ®é›†å’Œæ–¹æ³•åœ¨DriveLMé—®ç­”åŸºå‡†æµ‹è¯•å’ŒnuSceneså¼€æ”¾å¾ªç¯è§„åˆ’ä»»åŠ¡ä¸Šè¡¨ç°å‡ºæ˜¾è‘—æ•ˆæœã€‚</li>
<li>é€šè¿‡å¤§è§„æ¨¡é«˜è´¨é‡æ•°æ®é›†çš„ç”Ÿæˆï¼ŒOmniDriveæä¾›äº†æ›´å¯†é›†çš„ç›‘ç£ä¿¡å·ï¼Œæ¡¥æ¥äº†è§„åˆ’è½¨è¿¹å’Œè¯­è¨€åŸºç¡€æ¨ç†ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.04348">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-de34c97803e176264adc8ecd09b26302.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c9d3cd432d9a353759c4a1f2be3cb554.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1cd63c1e07f7dd7d87cd66ea00e7e761.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-726b4fdbaf74b22d672fd04d82d56f6f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-eb736707752bae3c6ade1932d07092d8.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-ad65fa14d061b2a3a37450110cf6ff45.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="Distillation-and-Refinement-of-Reasoning-in-Small-Language-Models-for-Document-Re-ranking"><a href="#Distillation-and-Refinement-of-Reasoning-in-Small-Language-Models-for-Document-Re-ranking" class="headerlink" title="Distillation and Refinement of Reasoning in Small Language Models for   Document Re-ranking"></a>Distillation and Refinement of Reasoning in Small Language Models for   Document Re-ranking</h2><p><strong>Authors:Chris Samarinas, Hamed Zamani</strong></p>
<p>We present a novel approach for training small language models for reasoning-intensive document ranking that combines knowledge distillation with reinforcement learning optimization. While existing methods often rely on expensive human annotations or large black-box language models, our methodology leverages web data and a teacher LLM to automatically generate high-quality training examples with relevance explanations. By framing document ranking as a reinforcement learning problem and incentivizing explicit reasoning capabilities, we train a compact 3B parameter language model that achieves state-of-the-art performance on the BRIGHT benchmark. Our model ranks third on the leaderboard while using substantially fewer parameters than other approaches, outperforming models that are over 20 times larger. Through extensive experiments, we demonstrate that generating explanations during inference, rather than directly predicting relevance scores, enables more effective reasoning with smaller language models. The self-supervised nature of our method offers a scalable and interpretable solution for modern information retrieval systems. </p>
<blockquote>
<p>æˆ‘ä»¬æå‡ºäº†ä¸€ç§ç»“åˆçŸ¥è¯†è’¸é¦å’Œå¼ºåŒ–å­¦ä¹ ä¼˜åŒ–è®­ç»ƒå°å‹è¯­è¨€æ¨¡å‹çš„æ–°æ–¹æ³•ï¼Œç”¨äºæ¨ç†å¯†é›†å‹æ–‡æ¡£æ’åºã€‚è™½ç„¶ç°æœ‰æ–¹æ³•å¸¸å¸¸ä¾èµ–äºæ˜‚è´µçš„äººåŠ›æ ‡æ³¨æˆ–å¤§å‹é»‘ç›’è¯­è¨€æ¨¡å‹ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åˆ™åˆ©ç”¨ç½‘é¡µæ•°æ®å’Œæ•™å¸ˆå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è‡ªåŠ¨ç”Ÿæˆé«˜è´¨é‡çš„è®­ç»ƒæ ·æœ¬åŠå…¶ç›¸å…³æ€§è§£é‡Šã€‚é€šè¿‡å°†æ–‡æ¡£æ’åºè®¾å®šä¸ºå¼ºåŒ–å­¦ä¹ é—®é¢˜å¹¶æ¿€åŠ±æ˜ç¡®çš„æ¨ç†èƒ½åŠ›ï¼Œæˆ‘ä»¬è®­ç»ƒäº†ä¸€ä¸ªç´§å‡‘çš„3Bå‚æ•°è¯­è¨€æ¨¡å‹ï¼Œåœ¨BRIGHTåŸºå‡†æµ‹è¯•ä¸Šå–å¾—äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚æˆ‘ä»¬çš„æ¨¡å‹åœ¨æ’è¡Œæ¦œä¸Šæ’åç¬¬ä¸‰ï¼ŒåŒæ—¶ä½¿ç”¨çš„å‚æ•°æ¯”å…¶ä»–æ–¹æ³•å°‘å¾—å¤šï¼Œè¶…è¶Šäº†è¶…è¿‡20å€çš„å¤§å‹æ¨¡å‹ã€‚é€šè¿‡å¤§é‡å®éªŒï¼Œæˆ‘ä»¬è¯æ˜åœ¨æ¨ç†è¿‡ç¨‹ä¸­ç”Ÿæˆè§£é‡Šï¼Œè€Œä¸æ˜¯ç›´æ¥é¢„æµ‹ç›¸å…³æ€§åˆ†æ•°ï¼Œèƒ½å¤Ÿä½¿å°å‹è¯­è¨€æ¨¡å‹æ›´æœ‰æ•ˆåœ°è¿›è¡Œæ¨ç†ã€‚æˆ‘ä»¬çš„æ–¹æ³•çš„è‡ªæˆ‘ç›‘ç£æ€§è´¨ä¸ºç°ä»£ä¿¡æ¯æ£€ç´¢ç³»ç»Ÿæä¾›äº†ä¸€ç§å¯æ‰©å±•å’Œå¯è§£é‡Šçš„è§£å†³æ–¹æ¡ˆã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.03947v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§ç»“åˆçŸ¥è¯†è’¸é¦å’Œå¼ºåŒ–å­¦ä¹ ä¼˜åŒ–çš„å°å‹è¯­è¨€æ¨¡å‹è®­ç»ƒæ–¹æ³•ï¼Œç”¨äºå¯†é›†æ¨ç†æ–‡æ¡£æ’åã€‚ä¸åŒäºä¾èµ–æ˜‚è´µçš„äººåŠ›æ ‡æ³¨æˆ–å¤§å‹é»‘ç›’è¯­è¨€æ¨¡å‹çš„æ–¹æ³•ï¼Œæœ¬æ–‡åˆ©ç”¨Webæ•°æ®å’Œæ•™å¸ˆå¤§å‹è¯­è¨€æ¨¡å‹è‡ªåŠ¨ç”Ÿæˆé«˜è´¨é‡çš„è®­ç»ƒæ ·æœ¬åŠå…¶ç›¸å…³æ€§è§£é‡Šã€‚é€šè¿‡å°†æ–‡æ¡£æ’åè§†ä¸ºå¼ºåŒ–å­¦ä¹ é—®é¢˜å¹¶æ¿€åŠ±æ˜ç¡®çš„æ¨ç†èƒ½åŠ›ï¼Œæœ¬æ–‡è®­ç»ƒäº†ä¸€ä¸ªç´§å‡‘çš„3äº¿å‚æ•°è¯­è¨€æ¨¡å‹ï¼Œåœ¨BRIGHTåŸºå‡†æµ‹è¯•ä¸­å–å¾—äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚åœ¨å‚æ•°è¿œå°‘äºå…¶ä»–æ–¹æ³•çš„æƒ…å†µä¸‹ï¼Œæœ¬æ¨¡å‹åœ¨æ’è¡Œæ¦œä¸Šæ’åç¬¬ä¸‰ï¼ŒåŒæ—¶è¯æ˜äº†å¯¹è¶…å‡ºåŒç±»æ¨¡å‹äºŒåå€çš„å¤§å‹æ¨¡å‹çš„è¶…è¶Šèƒ½åŠ›ã€‚æ­¤å¤–ï¼Œæœ¬æ–‡é€šè¿‡å®éªŒè¯æ˜ï¼Œåœ¨æ¨ç†è¿‡ç¨‹ä¸­ç”Ÿæˆè§£é‡Šè€Œéç›´æ¥é¢„æµ‹ç›¸å…³æ€§åˆ†æ•°ï¼Œèƒ½å¤Ÿä½¿å°å‹è¯­è¨€æ¨¡å‹çš„æ¨ç†æ›´åŠ æœ‰æ•ˆã€‚æœ¬æ–¹æ³•å…·æœ‰è‡ªæˆ‘ç›‘ç£çš„ç‰¹æ€§ï¼Œä¸ºç°ä»£ä¿¡æ¯æ£€ç´¢ç³»ç»Ÿæä¾›äº†å¯æ‰©å±•å’Œå¯è§£é‡Šçš„è§£å†³æ–¹æ¡ˆã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æå‡ºäº†ä¸€ç§ç»“åˆçŸ¥è¯†è’¸é¦å’Œå¼ºåŒ–å­¦ä¹ çš„å°å‹è¯­è¨€æ¨¡å‹è®­ç»ƒæ–¹æ³•ç”¨äºæ¨ç†å¯†é›†å‹æ–‡æ¡£æ’åã€‚</li>
<li>åˆ©ç”¨Webæ•°æ®å’Œæ•™å¸ˆå¤§å‹è¯­è¨€æ¨¡å‹è‡ªåŠ¨ç”Ÿæˆé«˜è´¨é‡çš„è®­ç»ƒæ ·æœ¬åŠå…¶ç›¸å…³æ€§è§£é‡Šã€‚</li>
<li>å°†æ–‡æ¡£æ’åè§†ä¸ºå¼ºåŒ–å­¦ä¹ é—®é¢˜ï¼Œè®­ç»ƒå‡ºå‚æ•°è¾ƒå°‘çš„ç´§å‡‘è¯­è¨€æ¨¡å‹ã€‚</li>
<li>æ¨¡å‹åœ¨æ’è¡Œæ¦œä¸Šæ’åç¬¬ä¸‰ï¼Œä¸”æ€§èƒ½ä¼˜äºå…¶ä»–å¤§å‹æ¨¡å‹ã€‚</li>
<li>é€šè¿‡å®éªŒè¯æ˜ï¼Œæ¨ç†è¿‡ç¨‹ä¸­ç”Ÿæˆè§£é‡Šèƒ½æé«˜å°å‹è¯­è¨€æ¨¡å‹çš„æ¨ç†æœ‰æ•ˆæ€§ã€‚</li>
<li>æœ¬æ–¹æ³•å…·æœ‰è‡ªæˆ‘ç›‘ç£çš„ç‰¹æ€§ï¼Œä¸ºç°ä»£ä¿¡æ¯æ£€ç´¢ç³»ç»Ÿæä¾›äº†è§£å†³æ–¹æ¡ˆã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.03947">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-ae525515739e598142660720e47c9d19.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ece68f76eef1dd24e28245a10ae96751.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-33da7404cbde1e6889a1fcc499cd20fb.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-f0ae507063a85b8adcef8cbe3945ce82.jpg" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="Have-Large-Language-Models-Learned-to-Reason-A-Characterization-via-3-SAT-Phase-Transition"><a href="#Have-Large-Language-Models-Learned-to-Reason-A-Characterization-via-3-SAT-Phase-Transition" class="headerlink" title="Have Large Language Models Learned to Reason? A Characterization via   3-SAT Phase Transition"></a>Have Large Language Models Learned to Reason? A Characterization via   3-SAT Phase Transition</h2><p><strong>Authors:Rishi Hazra, Gabriele Venturato, Pedro Zuidberg Dos Martires, Luc De Raedt</strong></p>
<p>Large Language Models (LLMs) have been touted as AI models possessing advanced reasoning abilities. In theory, autoregressive LLMs with Chain-of-Thought (CoT) can perform more serial computations to solve complex reasoning tasks. However, recent studies suggest that, despite this capacity, LLMs do not truly learn to reason but instead fit on statistical features. To study the reasoning capabilities in a principled fashion, we adopt a computational theory perspective and propose an experimental protocol centered on 3-SAT â€“ the prototypical NP-complete problem lying at the core of logical reasoning and constraint satisfaction tasks. Specifically, we examine the phase transitions in random 3-SAT and characterize the reasoning abilities of state-of-the-art LLMs by varying the inherent hardness of the problem instances. By comparing DeepSeek R1 with other LLMs, our findings reveal two key insights (1) LLM accuracy drops significantly on harder instances, suggesting all current models struggle when statistical shortcuts are unavailable (2) Unlike other LLMs, R1 shows signs of having learned the underlying reasoning. Following a principled experimental protocol, our study moves beyond the benchmark-driven evidence often found in LLM reasoning research. Our findings highlight important gaps and suggest clear directions for future research. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è¢«èª‰ä¸ºæ‹¥æœ‰å…ˆè¿›æ¨ç†èƒ½åŠ›çš„AIæ¨¡å‹ã€‚ç†è®ºä¸Šï¼Œé‡‡ç”¨æ€ç»´é“¾ï¼ˆCoTï¼‰çš„è‡ªå›å½’LLMå¯ä»¥æ‰§è¡Œæ›´å¤šçš„ä¸²è¡Œè®¡ç®—æ¥è§£å†³å¤æ‚çš„æ¨ç†ä»»åŠ¡ã€‚ç„¶è€Œï¼Œæœ€è¿‘çš„ç ”ç©¶è¡¨æ˜ï¼Œå°½ç®¡å…·å¤‡è¿™ç§èƒ½åŠ›ï¼ŒLLMå¹¶æ²¡æœ‰çœŸæ­£å­¦ä¼šæ¨ç†ï¼Œè€Œæ˜¯é€‚åº”äºç»Ÿè®¡ç‰¹å¾ã€‚ä¸ºäº†ä»¥åŸåˆ™æ€§çš„æ–¹å¼ç ”ç©¶æ¨ç†èƒ½åŠ›ï¼Œæˆ‘ä»¬ä»è®¡ç®—ç†è®ºçš„è§’åº¦å‡ºå‘ï¼Œæå‡ºäº†ä»¥3-SATä¸ºä¸­å¿ƒçš„å®éªŒåè®®â€”â€”3-SATæ˜¯é€»è¾‘æ¨ç†å’Œçº¦æŸæ»¡è¶³ä»»åŠ¡æ ¸å¿ƒçš„å…¸å‹NPå®Œå…¨é—®é¢˜ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬ç ”ç©¶äº†éšæœº3-SATä¸­çš„ç›¸å˜ï¼Œå¹¶é€šè¿‡æ”¹å˜é—®é¢˜å®ä¾‹çš„å›ºæœ‰éš¾åº¦æ¥è¡¨å¾æœ€å…ˆè¿›çš„LLMçš„æ¨ç†èƒ½åŠ›ã€‚é€šè¿‡æ¯”è¾ƒDeepSeek R1ä¸å…¶ä»–LLMï¼Œæˆ‘ä»¬çš„ç ”ç©¶å‘ç°ä¸¤ä¸ªå…³é”®è§è§£ï¼šï¼ˆ1ï¼‰åœ¨æ›´éš¾çš„å®ä¾‹ä¸Šï¼ŒLLMçš„å‡†ç¡®ç‡æ˜¾è‘—ä¸‹é™ï¼Œè¿™è¡¨æ˜å½“æ²¡æœ‰ç»Ÿè®¡æ·å¾„å¯ç”¨æ—¶ï¼Œå½“å‰æ‰€æœ‰æ¨¡å‹éƒ½ä¼šé‡åˆ°å›°éš¾ï¼›ï¼ˆ2ï¼‰ä¸å…¶ä»–LLMä¸åŒï¼ŒR1æ˜¾ç¤ºå‡ºå·²æŒæ¡åŸºæœ¬æ¨ç†çš„è¿¹è±¡ã€‚éµå¾ªåŸåˆ™æ€§çš„å®éªŒåè®®ï¼Œæˆ‘ä»¬çš„ç ”ç©¶è¶…è¶Šäº†LLMæ¨ç†ç ”ç©¶ä¸­é€šå¸¸å‡ºç°çš„åŸºå‡†æµ‹è¯•é©±åŠ¨çš„è¯æ®ã€‚æˆ‘ä»¬çš„ç ”ç©¶å‘ç°äº†é‡è¦çš„å·®è·ï¼Œå¹¶ä¸ºæœªæ¥çš„ç ”ç©¶æå‡ºäº†æ˜ç¡®çš„æ–¹å‘ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.03930v1">PDF</a> An updated version of arXiv:2408.07215v2, featuring: (1) inclusion of   recent LRMs and recent LLMs, (2) revised conclusions reflecting recent   developments, and (3) updated analysis</p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å…·å¤‡ç†è®ºä¸Šçš„æ¨ç†èƒ½åŠ›ï¼Œå°¤å…¶åœ¨é‡‡ç”¨é“¾å¼æ€ç»´ï¼ˆCoTï¼‰çš„è‡ªå›å½’LLMsä¸­è¡¨ç°æ›´çªå‡ºã€‚ç„¶è€Œï¼Œæœ€è¿‘çš„ç ”ç©¶è¡¨æ˜ï¼ŒLLMså¹¶æ²¡æœ‰çœŸæ­£å­¦ä¼šæ¨ç†ï¼Œè€Œæ˜¯ä¾èµ–äºç»Ÿè®¡ç‰¹å¾ã€‚ä¸ºäº†æ›´ç³»ç»Ÿåœ°ç ”ç©¶å…¶æ¨ç†èƒ½åŠ›ï¼Œæœ¬ç ”ç©¶ä»è®¡ç®—ç†è®ºè§’åº¦å…¥æ‰‹ï¼Œå›´ç»•3-SATè¿™ä¸€é€»è¾‘å’Œçº¦æŸæ»¡è¶³ä»»åŠ¡çš„æ ¸å¿ƒNPå®Œå…¨é—®é¢˜è®¾è®¡å®éªŒã€‚é€šè¿‡å¯¹æ¯”DeepSeek R1å’Œå…¶ä»–LLMsåœ¨éšæœº3-SATä¸­çš„é˜¶æ®µè½¬å˜ï¼Œå‘ç°ä»¥ä¸‹ä¸¤ç‚¹å…³é”®è§è§£ï¼šï¼ˆ1ï¼‰åœ¨æ›´å›°éš¾çš„å®ä¾‹ä¸­ï¼ŒLLMå‡†ç¡®ç‡å¤§å¹…ä¸‹é™ï¼Œè¡¨æ˜åœ¨æ²¡æœ‰ç»Ÿè®¡æ·å¾„å¯ç”¨æ—¶ï¼Œæ‰€æœ‰å½“å‰æ¨¡å‹éƒ½å­˜åœ¨é—®é¢˜ï¼›ï¼ˆ2ï¼‰ä¸å…¶ä»–LLMsç›¸æ¯”ï¼ŒR1æ˜¾ç¤ºå‡ºå·²æŒæ¡åŸºç¡€æ¨ç†çš„è¿¹è±¡ã€‚æœ¬ç ”ç©¶éµå¾ªç³»ç»Ÿçš„å®éªŒåè®®ï¼Œä¸ºç†è§£LLMçš„æ¨ç†èƒ½åŠ›æä¾›äº†è¶…è¶ŠåŸºå‡†æµ‹è¯•çš„æ–°è¯æ®ã€‚å‘ç°å½“å‰æŠ€æœ¯çš„ä¸è¶³ä¹‹å¤„å¹¶ä¸ºæœªæ¥çš„ç ”ç©¶æŒ‡æ˜äº†æ–¹å‘ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰è™½ç„¶ç†è®ºä¸Šå…·å¤‡æ¨ç†èƒ½åŠ›ï¼Œä½†ä¾èµ–ç»Ÿè®¡ç‰¹å¾è€ŒéçœŸæ­£çš„é€»è¾‘æ¨ç†ã€‚</li>
<li>é‡‡ç”¨è®¡ç®—ç†è®ºè§†è§’ç ”ç©¶LLMsçš„æ¨ç†èƒ½åŠ›æ˜¯ä¸€ç§æ–°çš„æ–¹æ³•ã€‚</li>
<li>å›´ç»•3-SATé—®é¢˜è®¾è®¡çš„å®éªŒæ­ç¤ºäº†LLMsåœ¨å›°éš¾å®ä¾‹ä¸­çš„å‡†ç¡®ç‡ä¸‹é™ã€‚</li>
<li>DeepSeek R1ä¸å…¶ä»–LLMsç›¸æ¯”ï¼Œåœ¨æ¨ç†èƒ½åŠ›æ–¹é¢è¡¨ç°å‡ºç‹¬ç‰¹æ€§ã€‚</li>
<li>LLMsé¢ä¸´åœ¨æ²¡æœ‰ç»Ÿè®¡æ·å¾„æ—¶çš„æŒ‘æˆ˜ã€‚</li>
<li>å½“å‰ç ”ç©¶ä¸ºç†è§£LLMçš„æ¨ç†èƒ½åŠ›æä¾›äº†è¶…è¶ŠåŸºå‡†æµ‹è¯•çš„æ–°è¯æ®ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.03930">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-5b0e44556655bc72a47bc4577c39e595.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ed4f8d9c5be93a12eeae2182838a3520.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-14743308bc2e3a19928b45e1cba22cc8.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e924c445338a595d925a02087aa97a29.jpg" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="MME-Unify-A-Comprehensive-Benchmark-for-Unified-Multimodal-Understanding-and-Generation-Models"><a href="#MME-Unify-A-Comprehensive-Benchmark-for-Unified-Multimodal-Understanding-and-Generation-Models" class="headerlink" title="MME-Unify: A Comprehensive Benchmark for Unified Multimodal   Understanding and Generation Models"></a>MME-Unify: A Comprehensive Benchmark for Unified Multimodal   Understanding and Generation Models</h2><p><strong>Authors:Wulin Xie, Yi-Fan Zhang, Chaoyou Fu, Yang Shi, Bingyan Nie, Hongkai Chen, Zhang Zhang, Liang Wang, Tieniu Tan</strong></p>
<p>Existing MLLM benchmarks face significant challenges in evaluating Unified MLLMs (U-MLLMs) due to: 1) lack of standardized benchmarks for traditional tasks, leading to inconsistent comparisons; 2) absence of benchmarks for mixed-modality generation, which fails to assess multimodal reasoning capabilities. We present a comprehensive evaluation framework designed to systematically assess U-MLLMs. Our benchmark includes: Standardized Traditional Task Evaluation. We sample from 12 datasets, covering 10 tasks with 30 subtasks, ensuring consistent and fair comparisons across studies.â€ 2. Unified Task Assessment. We introduce five novel tasks testing multimodal reasoning, including image editing, commonsense QA with image generation, and geometric reasoning. 3. Comprehensive Model Benchmarking. We evaluate 12 leading U-MLLMs, such as Janus-Pro, EMU3, VILA-U, and Gemini2-flash, alongside specialized understanding (e.g., Claude-3.5-Sonnet) and generation models (e.g., DALL-E-3). Our findings reveal substantial performance gaps in existing U-MLLMs, highlighting the need for more robust models capable of handling mixed-modality tasks effectively. The code and evaluation data can be found in <a target="_blank" rel="noopener" href="https://mme-unify.github.io/">https://mme-unify.github.io/</a>. </p>
<blockquote>
<p>ç°æœ‰çš„å¤šè¯­è¨€ä½èµ„æºæœºå™¨å­¦ä¹ ï¼ˆMLLMï¼‰åŸºå‡†æµ‹è¯•é¢ä¸´è¯„ä¼°ç»Ÿä¸€MLLMï¼ˆUnified MLLMsï¼Œç®€ç§°U-MLLMsï¼‰çš„é‡å¤§æŒ‘æˆ˜ï¼Œä¸»è¦ç”±äºä»¥ä¸‹å‡ ç‚¹åŸå› ï¼š1ï¼‰ç¼ºä¹ä¼ ç»Ÿä»»åŠ¡çš„æ ‡å‡†åŒ–åŸºå‡†æµ‹è¯•ï¼Œå¯¼è‡´æ¯”è¾ƒç»“æœä¸ä¸€è‡´ï¼›2ï¼‰ç¼ºå°‘æ··åˆæ¨¡æ€ç”Ÿæˆçš„åŸºå‡†æµ‹è¯•ï¼Œæ— æ³•è¯„ä¼°å¤šæ¨¡æ€æ¨ç†èƒ½åŠ›ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªå…¨é¢çš„è¯„ä¼°æ¡†æ¶ï¼Œæ—¨åœ¨ç³»ç»Ÿåœ°è¯„ä¼°U-MLLMsã€‚æˆ‘ä»¬çš„åŸºå‡†æµ‹è¯•åŒ…æ‹¬ï¼šæ ‡å‡†åŒ–çš„ä¼ ç»Ÿä»»åŠ¡è¯„ä¼°ã€‚æˆ‘ä»¬ä»12ä¸ªæ•°æ®é›†ä¸­æŠ½æ ·ï¼Œæ¶µç›–10ä¸ªä»»åŠ¡åŠ30ä¸ªå­ä»»åŠ¡ï¼Œç¡®ä¿è·¨ç ”ç©¶çš„æ¯”è¾ƒç»“æœä¸€è‡´ä¸”å…¬æ­£ã€‚â€ 2. ç»Ÿä¸€ä»»åŠ¡è¯„ä¼°ã€‚æˆ‘ä»¬å¼•å…¥äº†äº”é¡¹æ–°çš„ä»»åŠ¡æ¥æµ‹è¯•å¤šæ¨¡æ€æ¨ç†ï¼ŒåŒ…æ‹¬å›¾åƒç¼–è¾‘ã€å¸¦æœ‰å›¾åƒç”Ÿæˆå¸¸è¯†é—®ç­”å’Œå‡ ä½•æ¨ç†ã€‚3. ç»¼åˆæ¨¡å‹åŸºå‡†æµ‹è¯•ã€‚æˆ‘ä»¬è¯„ä¼°äº†é¢†å…ˆçš„12ä¸ªU-MLLMsï¼Œå¦‚Janus-Proã€EMU3ã€VILA-Uå’ŒGemini2-flashç­‰ï¼ŒåŒæ—¶è¯„ä¼°ä¸“ä¸šç†è§£ï¼ˆå¦‚Claude-3.5-Sonnetï¼‰å’Œç”Ÿæˆæ¨¡å‹ï¼ˆå¦‚DALL-E-3ï¼‰ã€‚æˆ‘ä»¬çš„ç ”ç©¶å‘ç°ï¼Œç°æœ‰çš„U-MLLMså­˜åœ¨æ˜¾è‘—çš„æ€§èƒ½å·®è·ï¼Œè¿™çªæ˜¾äº†éœ€è¦æ›´ç¨³å¥çš„æ¨¡å‹æ¥æœ‰æ•ˆå¤„ç†æ··åˆæ¨¡æ€ä»»åŠ¡ã€‚ç›¸å…³ä»£ç å’Œè¯„ä¼°æ•°æ®å¯åœ¨<a target="_blank" rel="noopener" href="https://mme-unify.github.io/%E6%89%BE%E5%88%B0%E3%80%82">https://mme-unify.github.io/æ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.03641v2">PDF</a> Project page: <a target="_blank" rel="noopener" href="https://mme-unify.github.io/">https://mme-unify.github.io/</a></p>
<p><strong>Summary</strong><br>     ç°å­˜çš„MLLMåŸºå‡†åœ¨è¯„ä¼°ç»Ÿä¸€MLLMsæ—¶é¢ä¸´é‡å¤§æŒ‘æˆ˜ï¼Œå¦‚ç¼ºä¹æ ‡å‡†åŒ–åŸºå‡†ä¸ä¼ ç»Ÿä»»åŠ¡çš„æ··åˆæ¨¡æ€ç”ŸæˆåŸºå‡†ç¼ºå¤±ï¼Œæ— æ³•è¯„ä¼°å¤šæ¨¡æ€æ¨ç†èƒ½åŠ›ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ä¸ªå…¨é¢è¯„ä¼°æ¡†æ¶ä»¥ç³»ç»ŸåŒ–è¯„ä¼°U-MLLMsã€‚è¯¥åŸºå‡†åŒ…æ‹¬æ ‡å‡†åŒ–ä¼ ç»Ÿä»»åŠ¡è¯„ä¼°ã€ç»Ÿä¸€ä»»åŠ¡è¯„ä¼°å’Œå…¨é¢æ¨¡å‹è¯„ä¼°ã€‚è¯„ä¼°äº†å¤šç§é¢†å…ˆU-MLLMsæ¨¡å‹ï¼Œå‘ç°ç°æœ‰U-MLLMså­˜åœ¨æ˜¾è‘—æ€§èƒ½å·®è·ï¼Œçªæ˜¾å‡ºéœ€è¦æ›´ç¨³å¥çš„æ¨¡å‹ä»¥æœ‰æ•ˆå¤„ç†æ··åˆæ¨¡æ€ä»»åŠ¡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>U-MLLMsç°æœ‰è¯„ä¼°é¢ä¸´çš„æŒ‘æˆ˜åŒ…æ‹¬ç¼ºä¹æ ‡å‡†åŒ–åŸºå‡†å’Œä¼ ç»Ÿä»»åŠ¡æ··åˆæ¨¡æ€ç”ŸæˆåŸºå‡†çš„ç¼ºå¤±ã€‚</li>
<li>æå‡ºä¸€ä¸ªå…¨é¢è¯„ä¼°æ¡†æ¶æ¥ç³»ç»ŸåŒ–è¯„ä¼°U-MLLMsï¼ŒåŒ…æ‹¬æ ‡å‡†åŒ–ä¼ ç»Ÿä»»åŠ¡è¯„ä¼°ã€ç»Ÿä¸€ä»»åŠ¡è¯„ä¼°å’Œå…¨é¢æ¨¡å‹è¯„ä¼°ã€‚</li>
<li>æ ‡å‡†åŒ–ä¼ ç»Ÿä»»åŠ¡è¯„ä¼°åŒ…æ‹¬ä»12ä¸ªæ•°æ®é›†ä¸­é‡‡æ ·ï¼Œæ¶µç›–10ä¸ªä»»åŠ¡å’Œ30ä¸ªå­ä»»åŠ¡ï¼Œä»¥ç¡®ä¿è·¨ç ”ç©¶çš„å…¬å¹³å’Œä¸€è‡´æ¯”è¾ƒã€‚</li>
<li>ç»Ÿä¸€ä»»åŠ¡è¯„ä¼°å¼•å…¥äº”ä¸ªæµ‹è¯•å¤šæ¨¡æ€æ¨ç†çš„æ–°ä»»åŠ¡ï¼ŒåŒ…æ‹¬å›¾åƒç¼–è¾‘ã€å¸¦æœ‰å›¾åƒç”Ÿæˆå¸¸è¯†é—®ç­”å’Œå‡ ä½•æ¨ç†ç­‰ã€‚</li>
<li>ç»¼åˆæ¨¡å‹è¯„ä¼°æ¶µç›–äº†å¤šç§é¢†å…ˆçš„U-MLLMsæ¨¡å‹ï¼Œå¦‚Janus-Proã€EMU3ã€VILA-Uã€Gemini2-flashç­‰ã€‚åŒæ—¶ä¹Ÿè¯„ä¼°äº†ä¸“é¡¹ç†è§£å’Œç”Ÿæˆæ¨¡å‹ã€‚</li>
<li>ç ”ç©¶å‘ç°ç°æœ‰U-MLLMså­˜åœ¨æ˜¾è‘—æ€§èƒ½å·®è·ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.03641">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-a572127986231d3505de48157b27fe46.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-24da48f59f642d87d1ae423b647e3515.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-238b081f50f116a00ce7c71fff79e66f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-bd72bc35ccfac22941c7f3bd3a649dc2.jpg" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1><h2 id="NuScenes-SpatialQA-A-Spatial-Understanding-and-Reasoning-Benchmark-for-Vision-Language-Models-in-Autonomous-Driving"><a href="#NuScenes-SpatialQA-A-Spatial-Understanding-and-Reasoning-Benchmark-for-Vision-Language-Models-in-Autonomous-Driving" class="headerlink" title="NuScenes-SpatialQA: A Spatial Understanding and Reasoning Benchmark for   Vision-Language Models in Autonomous Driving"></a>NuScenes-SpatialQA: A Spatial Understanding and Reasoning Benchmark for   Vision-Language Models in Autonomous Driving</h2><p><strong>Authors:Kexin Tian, Jingrui Mao, Yunlong Zhang, Jiwan Jiang, Yang Zhou, Zhengzhong Tu</strong></p>
<p>Recent advancements in Vision-Language Models (VLMs) have demonstrated strong potential for autonomous driving tasks. However, their spatial understanding and reasoning-key capabilities for autonomous driving-still exhibit significant limitations. Notably, none of the existing benchmarks systematically evaluate VLMsâ€™ spatial reasoning capabilities in driving scenarios. To fill this gap, we propose NuScenes-SpatialQA, the first large-scale ground-truth-based Question-Answer (QA) benchmark specifically designed to evaluate the spatial understanding and reasoning capabilities of VLMs in autonomous driving. Built upon the NuScenes dataset, the benchmark is constructed through an automated 3D scene graph generation pipeline and a QA generation pipeline. The benchmark systematically evaluates VLMsâ€™ performance in both spatial understanding and reasoning across multiple dimensions. Using this benchmark, we conduct extensive experiments on diverse VLMs, including both general and spatial-enhanced models, providing the first comprehensive evaluation of their spatial capabilities in autonomous driving. Surprisingly, the experimental results show that the spatial-enhanced VLM outperforms in qualitative QA but does not demonstrate competitiveness in quantitative QA. In general, VLMs still face considerable challenges in spatial understanding and reasoning. </p>
<blockquote>
<p>æœ€è¿‘ï¼Œè§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰çš„è¿›å±•ä¸ºè‡ªåŠ¨é©¾é©¶ä»»åŠ¡å±•ç¤ºäº†å¼ºå¤§çš„æ½œåŠ›ã€‚ç„¶è€Œï¼Œå®ƒä»¬åœ¨ç©ºé—´ç†è§£å’Œæ¨ç†æ–¹é¢çš„å…³é”®èƒ½åŠ›å¯¹äºè‡ªåŠ¨é©¾é©¶æ¥è¯´ä»æ˜¾ç¤ºå‡ºæ˜¾è‘—çš„å±€é™æ€§ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œç°æœ‰çš„åŸºå‡†æµ‹è¯•å¹¶æœªç³»ç»Ÿåœ°è¯„ä¼°é©¾é©¶åœºæ™¯ä¸­VLMsçš„ç©ºé—´æ¨ç†èƒ½åŠ›ã€‚ä¸ºäº†å¡«è¡¥è¿™ä¸€ç©ºç™½ï¼Œæˆ‘ä»¬æå‡ºäº†NuScenes-SpatialQAï¼Œè¿™æ˜¯ç¬¬ä¸€ä¸ªåŸºäºå¤§è§„æ¨¡çœŸå®æ•°æ®çš„å¤§å‹é—®ç­”ï¼ˆQAï¼‰åŸºå‡†æµ‹è¯•ï¼Œä¸“é—¨è®¾è®¡ç”¨äºè¯„ä¼°VLMsåœ¨è‡ªåŠ¨é©¾é©¶ä¸­çš„ç©ºé—´ç†è§£å’Œæ¨ç†èƒ½åŠ›ã€‚è¯¥åŸºå‡†æµ‹è¯•å»ºç«‹åœ¨NuScenesæ•°æ®é›†ä¹‹ä¸Šï¼Œé€šè¿‡è‡ªåŠ¨åŒ–çš„3Dåœºæ™¯å›¾ç”Ÿæˆæµç¨‹å’ŒQAç”Ÿæˆæµç¨‹æ„å»ºã€‚è¯¥åŸºå‡†æµ‹è¯•ç³»ç»Ÿåœ°è¯„ä¼°äº†VLMsåœ¨å¤šä¸ªç»´åº¦ä¸Šçš„ç©ºé—´ç†è§£å’Œæ¨ç†æ€§èƒ½ã€‚ä½¿ç”¨è¿™ä¸ªåŸºå‡†æµ‹è¯•ï¼Œæˆ‘ä»¬å¯¹å¤šç§VLMsè¿›è¡Œäº†å¹¿æ³›çš„å®éªŒï¼ŒåŒ…æ‹¬é€šç”¨æ¨¡å‹å’Œç©ºé—´å¢å¼ºæ¨¡å‹ï¼Œé¦–æ¬¡å…¨é¢è¯„ä¼°äº†å®ƒä»¬åœ¨è‡ªåŠ¨é©¾é©¶ä¸­çš„ç©ºé—´èƒ½åŠ›ã€‚ä»¤äººæƒŠè®¶çš„æ˜¯ï¼Œå®éªŒç»“æœè¡¨æ˜ï¼Œç©ºé—´å¢å¼ºVLMåœ¨å®šæ€§é—®ç­”ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œä½†åœ¨å®šé‡é—®ç­”ä¸­å¹¶æœªå±•ç°å‡ºç«äº‰åŠ›ã€‚æ€»ä½“è€Œè¨€ï¼ŒVLMsåœ¨ç©ºé—´ç†è§£å’Œæ¨ç†æ–¹é¢ä»é¢ä¸´å·¨å¤§çš„æŒ‘æˆ˜ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.03164v2">PDF</a> </p>
<p><strong>Summary</strong><br>è‡ªåŠ¨åŒ–é©¾é©¶é¢†åŸŸä¸­ï¼Œè§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰åœ¨è‡ªåŠ¨é©¾é©¶ä»»åŠ¡ä¸Šçš„æ½œåŠ›å’Œè¡¨ç°å¾—åˆ°äº†å±•ç¤ºã€‚ä½†å…¶ç©ºé—´ç†è§£å’Œæ¨ç†èƒ½åŠ›çš„æ ¸å¿ƒä»æœ‰è¯¸å¤šå±€é™ï¼Œè€Œç°æœ‰åŸºå‡†æµ‹è¯•å¹¶æœªç³»ç»Ÿåœ°è¯„ä¼°å…¶åœ¨é©¾é©¶åœºæ™¯ä¸­çš„ç©ºé—´æ¨ç†èƒ½åŠ›ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†NuScenes-SpatialQAåŸºå‡†æµ‹è¯•ï¼Œè¯¥æµ‹è¯•ä»¥NuScenesæ•°æ®é›†ä¸ºåŸºç¡€ï¼Œé€šè¿‡è‡ªåŠ¨åŒ–çš„ä¸‰ç»´åœºæ™¯å›¾ç”Ÿæˆç®¡é“å’Œé—®ç­”ç”Ÿæˆç®¡é“æ„å»ºè€Œæˆã€‚æ­¤åŸºå‡†æµ‹è¯•ç³»ç»Ÿåœ°ä»å¤šä¸ªç»´åº¦è¯„ä¼°äº†VLMsåœ¨ç©ºé—´ç†è§£å’Œæ¨ç†ä¸Šçš„è¡¨ç°ã€‚æˆ‘ä»¬å¯¹ä¸€ç³»åˆ—å¤šæ ·åŒ–çš„VLMsè¿›è¡Œäº†åŸºå‡†æµ‹è¯•çš„å®éªŒç ”ç©¶ï¼Œå…¶ä¸­åŒ…æ‹¬ä¸€èˆ¬æ¨¡å‹å’Œå¢å¼ºç©ºé—´ç†è§£çš„æ¨¡å‹ï¼Œç ”ç©¶ç»“æœè¡¨æ˜ç©ºé—´ç†è§£çš„å¢å¼ºæ¨¡å‹åœ¨å®šæ€§é—®ç­”ä¸Šçš„è¡¨ç°è¾ƒä¸ºä¼˜ç§€ä½†åœ¨å®šé‡é—®ç­”ä¸Šå¹¶æ— ç«äº‰åŠ›ã€‚æ€»ä½“æ¥è¯´ï¼ŒVLMsåœ¨ç©ºé—´ç†è§£å’Œæ¨ç†æ–¹é¢ä»å­˜åœ¨è¾ƒå¤§æŒ‘æˆ˜ã€‚è¯¥ç ”ç©¶çš„ç»“è®ºåæ˜ äº†æˆ‘ä»¬éœ€è¦é’ˆå¯¹æ­¤é¢†åŸŸçš„æ·±å…¥ç†è§£å’ŒæŒç»­çš„ç ”å‘è¿›æ­¥ä»¥ç¡®ä¿æ›´é«˜æ•ˆå¯é çš„è‡ªåŠ¨åŒ–é©¾é©¶æŠ€æœ¯çš„å‘å±•å’Œåº”ç”¨è½åœ°ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>VLMsåœ¨è‡ªåŠ¨é©¾é©¶ä»»åŠ¡ä¸­å±•ç°å‡ºæ½œåŠ›ï¼Œä½†åœ¨ç©ºé—´ç†è§£å’Œæ¨ç†æ–¹é¢å­˜åœ¨å±€é™ã€‚</li>
<li>ç›®å‰ç¼ºä¹ç³»ç»Ÿè¯„ä¼°VLMsåœ¨é©¾é©¶åœºæ™¯ä¸­ç©ºé—´æ¨ç†èƒ½åŠ›çš„åŸºå‡†æµ‹è¯•ã€‚</li>
<li>NuScenes-SpatialQAåŸºå‡†æµ‹è¯•æ˜¯é¦–ä¸ªä¸“ä¸ºè¯„ä¼°VLMsåœ¨è‡ªåŠ¨é©¾é©¶ä¸­çš„ç©ºé—´ç†è§£å’Œæ¨ç†èƒ½åŠ›è€Œè®¾è®¡çš„å¤§å‹åŸºäºçœŸå®åœºæ™¯çš„åŸºå‡†æµ‹è¯•ã€‚</li>
<li>è¯¥åŸºå‡†æµ‹è¯•é€šè¿‡è‡ªåŠ¨åŒ–çš„ä¸‰ç»´åœºæ™¯å›¾ç”Ÿæˆç®¡é“å’Œé—®ç­”ç”Ÿæˆç®¡é“æ„å»ºè€Œæˆã€‚</li>
<li>å®éªŒç ”ç©¶äº†å¤šç§VLMsçš„è¡¨ç°ï¼Œå‘ç°å¢å¼ºç©ºé—´ç†è§£çš„æ¨¡å‹åœ¨å®šæ€§é—®ç­”ä¸Šè¡¨ç°è‰¯å¥½ä½†åœ¨å®šé‡é—®ç­”ä¸Šä»æœ‰æŒ‘æˆ˜ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.03164">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-211c7c4f8e5a45374dd44e0bcf9e1a87.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c4c916e76fb4f4757f21e676cc3545d3.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-8838cec395a93ab66ab3d258fbd498b3.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-508822a848f20464e6a6c4edffe9e22a.jpg" align="middle">
</details>


<h1 id="-19"><a href="#-19" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-04-09/R1_Reasoning/">https://kedreamix.github.io/Talk2Paper/Paper/2025-04-09/R1_Reasoning/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/R1-Reasoning/">
                                    <span class="chip bg-color">R1_Reasoning</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-04-09/LLM/">
                    <div class="card-image">
                        
                        <img src="https://pica.zhimg.com/v2-4bf1f6e626846a87d9d4abeeabb6cb96.jpg" class="responsive-img" alt="LLM">
                        
                        <span class="card-title">LLM</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            LLM æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-04-09  URECA Unique Region Caption Anything
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-04-09
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/LLM/" class="post-category">
                                    LLM
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/LLM/">
                        <span class="chip bg-color">LLM</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-04-08/Text-to-Motion/">
                    <div class="card-image">
                        
                        <img src="https://pica.zhimg.com/v2-c0dcc5dd37f7105814558d562309410b.jpg" class="responsive-img" alt="Text-to-Motion">
                        
                        <span class="card-title">Text-to-Motion</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Text-to-Motion æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-04-08  Shape My Moves Text-Driven Shape-Aware Synthesis of Human Motions
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-04-08
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Text-to-Motion/" class="post-category">
                                    Text-to-Motion
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Text-to-Motion/">
                        <span class="chip bg-color">Text-to-Motion</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">25243.5k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
