<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="3DGS">
    <meta name="description" content="3DGS 方向最新论文已更新，请持续关注 Update in 2025-04-09  Let it Snow! Animating Static Gaussian Scenes With Dynamic Weather   Effects">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>3DGS | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loader页面消失采用渐隐的方式*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logo出现动画 */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//使用渐隐的方法淡出loading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//强制显示loading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">嘘~  正在从服务器偷取页面 . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>首页</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>标签</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>分类</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>归档</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="搜索" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="深色/浅色模式" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			首页
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			标签
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			分类
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			归档
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pic1.zhimg.com/v2-96820a96652e1b4e81f4a21511556126.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">3DGS</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- 文章内容详情 -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/3DGS/">
                                <span class="chip bg-color">3DGS</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/3DGS/" class="post-category">
                                3DGS
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp;
                    2025-04-09
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>更新日期:&nbsp;&nbsp;
                    2025-04-21
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>文章字数:&nbsp;&nbsp;
                    11.7k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>阅读时长:&nbsp;&nbsp;
                    47 分
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>阅读次数:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>⚠️ 以下所有内容总结都来自于 大语言模型的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p>
</blockquote>
<h1 id="2025-04-09-更新"><a href="#2025-04-09-更新" class="headerlink" title="2025-04-09 更新"></a>2025-04-09 更新</h1><h2 id="Let-it-Snow-Animating-Static-Gaussian-Scenes-With-Dynamic-Weather-Effects"><a href="#Let-it-Snow-Animating-Static-Gaussian-Scenes-With-Dynamic-Weather-Effects" class="headerlink" title="Let it Snow! Animating Static Gaussian Scenes With Dynamic Weather   Effects"></a>Let it Snow! Animating Static Gaussian Scenes With Dynamic Weather   Effects</h2><p><strong>Authors:Gal Fiebelman, Hadar Averbuch-Elor, Sagie Benaim</strong></p>
<p>3D Gaussian Splatting has recently enabled fast and photorealistic reconstruction of static 3D scenes. However, introducing dynamic elements that interact naturally with such static scenes remains challenging. Accordingly, we present a novel hybrid framework that combines Gaussian-particle representations for incorporating physically-based global weather effects into static 3D Gaussian Splatting scenes, correctly handling the interactions of dynamic elements with the static scene. We follow a three-stage process: we first map static 3D Gaussians to a particle-based representation. We then introduce dynamic particles and simulate their motion using the Material Point Method (MPM). Finally, we map the simulated particles back to the Gaussian domain while introducing appearance parameters tailored for specific effects. To correctly handle the interactions of dynamic elements with the static scene, we introduce specialized collision handling techniques. Our approach supports a variety of weather effects, including snowfall, rainfall, fog, and sandstorms, and can also support falling objects, all with physically plausible motion and appearance. Experiments demonstrate that our method significantly outperforms existing approaches in both visual quality and physical realism. </p>
<blockquote>
<p>3D高斯摊铺技术最近已经实现了静态3D场景的快照和逼真的重建。然而，引入能与这些静态场景自然交互的动态元素仍然是一个挑战。因此，我们提出了一种新型混合框架，它结合了高斯粒子表示法，将基于物理的全球天气效果融入到静态的3D高斯摊铺场景中，正确处理动态元素与静态场景的交互。我们遵循一个三阶段的过程：首先，我们将静态的3D高斯映射到基于粒子的表示法上。然后引入动态粒子，并使用物质点法（MPM）模拟其运动。最后，我们将模拟的粒子映射回高斯域，同时引入针对特定效果的外观参数。为了正确处理动态元素与静态场景的交互，我们引入了专门的碰撞处理技术。我们的方法支持多种天气效果，包括下雪、下雨、雾和沙尘暴，还支持下落物体，所有这些都具有物理上合理的运动和外观。实验表明，我们的方法在视觉质量和物理现实性方面都显著优于现有方法。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.05296v1">PDF</a> Project webpage: <a target="_blank" rel="noopener" href="https://galfiebelman.github.io/let-it-snow/">https://galfiebelman.github.io/let-it-snow/</a></p>
<p><strong>摘要</strong><br>    本文提出了一种结合高斯粒子表示法的新型混合框架，将基于物理的全球天气效应引入静态三维高斯贴图场景，并正确处理动态元素与静态场景的交互。该研究采用三阶段流程，首先，将静态三维高斯映射到基于粒子的表示法；接着，引入动态粒子并使用物质点法模拟其运动；最后，将模拟粒子映射回高斯域，同时引入针对特定效果的外观参数。为处理动态元素与静态场景的交互，研究引入了专业碰撞处理技术。该方法支持多种天气效应，包括降雪、降雨、雾和沙尘暴，还能支持落体物体，具有物理上合理的运动和外观。实验表明，该方法在视觉质量和物理真实性方面显著优于现有方法。</p>
<p><strong>要点</strong></p>
<ol>
<li>3D Gaussian Splatting技术能快速重建静态3D场景，但引入自然交互的动态元素仍是挑战。</li>
<li>新型混合框架结合高斯粒子表示法，引入物理全球天气效应到静态3D场景中。</li>
<li>采用三阶段流程：映射静态高斯到粒子表示，引入动态粒子并使用物质点法模拟，再将粒子映射回高斯域并引入外观参数。</li>
<li>引入专业碰撞处理技术，正确处理动态元素与静态场景的交互。</li>
<li>支持多种天气效应和落体物体，具有物理上合理的运动和外观。</li>
<li>实验显示，该方法在视觉质量和物理真实性上优于现有方法。</li>
<li>该研究为动态元素与静态3D场景的交互提供了一种新的解决方案。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.05296">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-dc267450c297496160af692870765a04.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-bfc5edda6a7f06d43a064922a0808078.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ecae4370886d63bc92b3eace5ec8201c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c5b03a1bac2afab607aca593858e6ea7.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="PanoDreamer-Consistent-Text-to-360-Degree-Scene-Generation"><a href="#PanoDreamer-Consistent-Text-to-360-Degree-Scene-Generation" class="headerlink" title="PanoDreamer: Consistent Text to 360-Degree Scene Generation"></a>PanoDreamer: Consistent Text to 360-Degree Scene Generation</h2><p><strong>Authors:Zhexiao Xiong, Zhang Chen, Zhong Li, Yi Xu, Nathan Jacobs</strong></p>
<p>Automatically generating a complete 3D scene from a text description, a reference image, or both has significant applications in fields like virtual reality and gaming. However, current methods often generate low-quality textures and inconsistent 3D structures. This is especially true when extrapolating significantly beyond the field of view of the reference image. To address these challenges, we propose PanoDreamer, a novel framework for consistent, 3D scene generation with flexible text and image control. Our approach employs a large language model and a warp-refine pipeline, first generating an initial set of images and then compositing them into a 360-degree panorama. This panorama is then lifted into 3D to form an initial point cloud. We then use several approaches to generate additional images, from different viewpoints, that are consistent with the initial point cloud and expand&#x2F;refine the initial point cloud. Given the resulting set of images, we utilize 3D Gaussian Splatting to create the final 3D scene, which can then be rendered from different viewpoints. Experiments demonstrate the effectiveness of PanoDreamer in generating high-quality, geometrically consistent 3D scenes. </p>
<blockquote>
<p>通过文本描述、参考图像或两者结合自动生成完整的3D场景，在虚拟现实和游戏等领域具有显著的应用价值。然而，当前的方法往往生成低质量的纹理和不一致的3D结构。当超出参考图像的视野范围进行推断时，尤其如此。为了解决这些挑战，我们提出了PanoDreamer，这是一种具有灵活文本和图像控制的一致3D场景生成新框架。我们的方法采用大型语言模型和warp-refine管道，首先生成一组初始图像，然后将它们合成一个360度的全景图。接着将这个全景图提升为3D，形成初始点云。然后，我们使用几种方法从不同的视点生成与初始点云一致并扩展&#x2F;细化初始点云的附加图像。根据所得的图像集，我们利用3D高斯摊铺技术创建最终的3D场景，然后可以从不同的视点进行渲染。实验表明，PanoDreamer在生成高质量、几何一致的3D场景方面非常有效。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.05152v1">PDF</a> Accepted by CVPR 2025 Workshop on Computer Vision for Metaverse</p>
<p><strong>Summary</strong><br>新一代PanoDreamer框架通过结合文本和图像控制实现了高质量、一致的3D场景生成。利用大型语言模型和warp-refine管道，PanoDreamer通过构建初始全景图并结合不同视角的图像来创建3D场景，并采用多种方法来完善点云数据并生成最终的3D场景。该框架在虚拟现实和游戏等领域具有广泛的应用前景。</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>PanoDreamer是一个新颖的框架，用于从文本描述或参考图像（或两者结合）自动生成完整的3D场景。</li>
<li>当前方法生成纹理质量低下且三维结构不一致的问题得到重视。尤其是在视野之外的场景外推上尤为突出。</li>
<li>PanoDreamer采用大型语言模型和warp-refine管道，先生成初始图像集并组合成全景图，再转化为三维场景。</li>
<li>利用多种方法从不同视角生成与初始点云一致的图像，并对其进行扩展和优化。</li>
<li>采用3D高斯拼贴技术创建最终的3D场景，可从不同角度进行渲染。</li>
<li>实验证明PanoDreamer在生成高质量、几何一致的3D场景方面的有效性。</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.05152">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-96820a96652e1b4e81f4a21511556126.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-edb66d36146d23caf2e0bb7d1f0c8570.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-30894dfa7d6b9a7a98332d9c642b46f4.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Embracing-Dynamics-Dynamics-aware-4D-Gaussian-Splatting-SLAM"><a href="#Embracing-Dynamics-Dynamics-aware-4D-Gaussian-Splatting-SLAM" class="headerlink" title="Embracing Dynamics: Dynamics-aware 4D Gaussian Splatting SLAM"></a>Embracing Dynamics: Dynamics-aware 4D Gaussian Splatting SLAM</h2><p><strong>Authors:Zhicong Sun, Jacqueline Lo, Jinxing Hu</strong></p>
<p>Simultaneous localization and mapping (SLAM) technology now has photorealistic mapping capabilities thanks to the real-time high-fidelity rendering capability of 3D Gaussian splatting (3DGS). However, due to the static representation of scenes, current 3DGS-based SLAM encounters issues with pose drift and failure to reconstruct accurate maps in dynamic environments. To address this problem, we present D4DGS-SLAM, the first SLAM method based on 4DGS map representation for dynamic environments. By incorporating the temporal dimension into scene representation, D4DGS-SLAM enables high-quality reconstruction of dynamic scenes. Utilizing the dynamics-aware InfoModule, we can obtain the dynamics, visibility, and reliability of scene points, and filter stable static points for tracking accordingly. When optimizing Gaussian points, we apply different isotropic regularization terms to Gaussians with varying dynamic characteristics. Experimental results on real-world dynamic scene datasets demonstrate that our method outperforms state-of-the-art approaches in both camera pose tracking and map quality. </p>
<blockquote>
<p>由于3D高斯混合（3DGS）的实时高保真渲染能力，现在的同时定位与地图构建（SLAM）技术已经具备逼真的映射能力。然而，由于场景的静态表示，当前的基于3DGS的SLAM在动态环境中遇到了姿态漂移和无法重建准确地图的问题。为了解决这个问题，我们提出了基于四维高斯混合地图表示的动态环境SLAM方法——D4DGS-SLAM。通过将时间维度融入场景表示，D4DGS-SLAM能够实现动态场景的高质量重建。通过使用动态感知的InfoModule，我们可以获得场景点的动态性、可见性和可靠性，并据此过滤出稳定的静态点进行跟踪。在优化高斯点时，我们对具有不同动态特性的高斯应用不同的等距正则化项。在真实世界动态场景数据集上的实验结果证明了我们的方法在相机姿态跟踪和地图质量方面均优于现有先进技术。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.04844v1">PDF</a> This paper is currently under reviewed for IROS 2025</p>
<p><strong>Summary</strong><br>     基于三维高斯点云技术（3DGS）的实时渲染能力，现在的同步定位与地图构建（SLAM）技术可以实现逼真的地图构建。然而，在动态环境下，现有的基于3DGS的SLAM技术存在姿态漂移和无法准确重建地图的问题。为解决这些问题，我们提出了基于四维高斯点云表示（D4DGS）的SLAM方法。通过引入时间维度来表示场景，D4DGS-SLAM能够高质量地重建动态场景。采用动力学感知的InfoModule模块，可以获取场景点的动力学、可见性和可靠性，并据此过滤稳定的静态点进行跟踪。在优化高斯点时，我们对具有不同动态特性的高斯点应用不同的同构正则化项。在真实世界的动态场景数据集上的实验表明，我们的方法在主摄像的姿态追踪和地图质量上优于当前的主流方法。</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>SLAM技术结合3DGS实现了逼真的地图构建。</li>
<li>现有基于3DGS的SLAM技术在动态环境下存在姿态漂移和地图重建不准确的问题。</li>
<li>D4DGS-SLAM是首个基于四维高斯点云表示的SLAM方法，能够高质量重建动态场景。</li>
<li>D4DGS-SLAM利用动力学感知模块获取场景点的动力学、可见性和可靠性信息。</li>
<li>D4DGS-SLAM通过不同的同构正则化项优化具有不同动态特性的高斯点。</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.04844">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-cf467c1e0d7f83f8d0e8cb7b099761cd.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f5260a4b222e275ef727f0e47b9b523b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-cd40af6eb6fb4e04aad062d51917a625.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-9dd41171c18b82dea38726f902fcafc2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9393c380b8efa86e1b9da816e7101159.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-dc73a7acedd74949192e4ae2ace9c62a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c316b769c271853e08a719d3a2e1b782.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-714bb4641eaf52d09589bb14e117839b.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="DeclutterNeRF-Generative-Free-3D-Scene-Recovery-for-Occlusion-Removal"><a href="#DeclutterNeRF-Generative-Free-3D-Scene-Recovery-for-Occlusion-Removal" class="headerlink" title="DeclutterNeRF: Generative-Free 3D Scene Recovery for Occlusion Removal"></a>DeclutterNeRF: Generative-Free 3D Scene Recovery for Occlusion Removal</h2><p><strong>Authors:Wanzhou Liu, Zhexiao Xiong, Xinyu Li, Nathan Jacobs</strong></p>
<p>Recent novel view synthesis (NVS) techniques, including Neural Radiance Fields (NeRF) and 3D Gaussian Splatting (3DGS) have greatly advanced 3D scene reconstruction with high-quality rendering and realistic detail recovery. Effectively removing occlusions while preserving scene details can further enhance the robustness and applicability of these techniques. However, existing approaches for object and occlusion removal predominantly rely on generative priors, which, despite filling the resulting holes, introduce new artifacts and blurriness. Moreover, existing benchmark datasets for evaluating occlusion removal methods lack realistic complexity and viewpoint variations. To address these issues, we introduce DeclutterSet, a novel dataset featuring diverse scenes with pronounced occlusions distributed across foreground, midground, and background, exhibiting substantial relative motion across viewpoints. We further introduce DeclutterNeRF, an occlusion removal method free from generative priors. DeclutterNeRF introduces joint multi-view optimization of learnable camera parameters, occlusion annealing regularization, and employs an explainable stochastic structural similarity loss, ensuring high-quality, artifact-free reconstructions from incomplete images. Experiments demonstrate that DeclutterNeRF significantly outperforms state-of-the-art methods on our proposed DeclutterSet, establishing a strong baseline for future research. </p>
<blockquote>
<p>近期的新型视图合成（NVS）技术，包括神经辐射场（NeRF）和3D高斯拼贴（3DGS），在高质量渲染和真实细节恢复方面极大地推动了3D场景重建的发展。在保留场景细节的同时有效地去除遮挡物可以进一步增强这些技术的稳健性和适用性。然而，现有的物体和遮挡物去除方法主要依赖于生成先验，尽管可以填补空洞，但会引入新的伪影和模糊。此外，用于评估遮挡物去除方法的现有基准数据集缺乏现实的复杂性和视角变化。为了解决这些问题，我们引入了DeclutterSet，这是一个具有显著遮挡物的多样化场景的新数据集，这些遮挡物分布在前景、中景和背景中，并在不同视角之间表现出大量的相对运动。我们还介绍了DeclutterNeRF，这是一种无需生成先验的遮挡物去除方法。DeclutterNeRF引入了可学习相机参数的联合多视角优化、遮挡物退火正则化，并采用了可解释的随机结构相似性损失，确保从不完整图像中进行高质量、无伪影的重建。实验表明，在我们的提议的DeclutterSet上，DeclutterNeRF显著优于最先进的方法，为未来研究建立了强有力的基准。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.04679v1">PDF</a> Accepted by CVPR 2025 4th CV4Metaverse Workshop. 15 pages, 10   figures. Code and data at: <a target="_blank" rel="noopener" href="https://github.com/wanzhouliu/declutter-nerf">https://github.com/wanzhouliu/declutter-nerf</a></p>
<p><strong>Summary</strong></p>
<p>NeRF和3DGS等新型视图合成技术已在3D场景重建方面取得显著进展，但在去除遮挡物时仍面临生成先验导致的模糊和人工制品问题。为解决此问题，研究者引入了DeclutterSet数据集以及无需生成先验的DeclutterNeRF方法。DeclutterNeRF通过联合多视角优化学习相机参数、实施遮挡退火正则化并采用可解释的随机结构相似性损失，确保了从不完整图像进行高质量、无人工制品的重建。此方法在DeclutterSet上的表现显著优于现有方法，为未来研究确立了坚实的基准。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>NVS技术如NeRF和3DGS推动了高质量渲染的3D场景重建发展。</li>
<li>遮挡移除技术在NVS中仍面临挑战，尤其是引入生成先验导致的新问题。</li>
<li>提出了一个新的数据集DeclutterSet，涵盖了具有显著遮挡的多样化场景，并展现出视点变化的复杂性。</li>
<li>介绍了无需生成先验的遮挡移除方法DeclutterNeRF。</li>
<li>DeclutterNeRF通过联合多视角优化相机参数，提高了遮挡处理效果。</li>
<li>DeclutterNeRF采用遮挡退火正则化和可解释的随机结构相似性损失，确保高质量的重建结果。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.04679">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-19fbf9cae2da7d297367328e01541b38.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-fff8fbfe0808cf428889777054f40288.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-af0ebf63427d642319122994e7412687.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e0ca639c588edbdfd830c6caa834440e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f16bd9d5b8361feb1489b81af6cb5c59.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Thermoxels-a-voxel-based-method-to-generate-simulation-ready-3D-thermal-models"><a href="#Thermoxels-a-voxel-based-method-to-generate-simulation-ready-3D-thermal-models" class="headerlink" title="Thermoxels: a voxel-based method to generate simulation-ready 3D thermal   models"></a>Thermoxels: a voxel-based method to generate simulation-ready 3D thermal   models</h2><p><strong>Authors:Etienne Chassaing, Florent Forest, Olga Fink, Malcolm Mielle</strong></p>
<p>In the European Union, buildings account for 42% of energy use and 35% of greenhouse gas emissions. Since most existing buildings will still be in use by 2050, retrofitting is crucial for emissions reduction. However, current building assessment methods rely mainly on qualitative thermal imaging, which limits data-driven decisions for energy savings. On the other hand, quantitative assessments using finite element analysis (FEA) offer precise insights but require manual CAD design, which is tedious and error-prone. Recent advances in 3D reconstruction, such as Neural Radiance Fields (NeRF) and Gaussian Splatting, enable precise 3D modeling from sparse images but lack clearly defined volumes and the interfaces between them needed for FEA. We propose Thermoxels, a novel voxel-based method able to generate FEA-compatible models, including both geometry and temperature, from a sparse set of RGB and thermal images. Using pairs of RGB and thermal images as input, Thermoxels represents a scene’s geometry as a set of voxels comprising color and temperature information. After optimization, a simple process is used to transform Thermoxels’ models into tetrahedral meshes compatible with FEA. We demonstrate Thermoxels’ capability to generate RGB+Thermal meshes of 3D scenes, surpassing other state-of-the-art methods. To showcase the practical applications of Thermoxels’ models, we conduct a simple heat conduction simulation using FEA, achieving convergence from an initial state defined by Thermoxels’ thermal reconstruction. Additionally, we compare Thermoxels’ image synthesis abilities with current state-of-the-art methods, showing competitive results, and discuss the limitations of existing metrics in assessing mesh quality. </p>
<blockquote>
<p>在欧洲联盟，建筑物占能源使用的42%和温室气体排放的35%。由于大多数现有建筑在2050年之前仍在使用，因此改造对于减少排放至关重要。然而，当前建筑评估方法主要依赖于定性热成像，这限制了基于数据节能的决策。另一方面，使用有限元分析（FEA）的定量评估提供了精确见解，但需要手动CAD设计，这既繁琐又容易出错。最近的三维重建技术进展，如神经辐射场（NeRF）和高斯溅射（Gaussian Splatting），能够从稀疏图像进行精确的三维建模，但缺乏明确定义的体积和进行FEA所需的界面。我们提出Thermoxels，这是一种新型基于体素的方法，能够从稀疏的RGB和热图像生成与FEA兼容的模型，包括几何和温度信息。使用成对的RGB和热图像作为输入，Thermoxels将场景的几何表示为包含颜色和温度信息的体素集合。经过优化后，使用简单的过程将Thermoxels模型转换为与FEA兼容的四面体网格。我们展示了Thermoxels生成RGB+热网格的三维场景的能力，超越了其他最先进的方法。为了展示Thermoxels模型的实际应用，我们使用FEA进行简单的热传导模拟，从Thermoxels热重建定义的初始状态实现收敛。此外，我们将Thermoxels的图像合成能力与当前最先进的方法进行比较，显示了具有竞争力的结果，并讨论了现有指标在评估网格质量方面的局限性。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.04448v1">PDF</a> 7 pages, 2 figures</p>
<p><strong>Summary</strong><br>    提出了一种名为Thermoxels的新方法，通过稀疏的RGB和热图像生成有限元分析（FEA）兼容的模型，用于建筑的热性能评估。该方法能生成包含几何和温度信息的voxel模型，并转化为FEA兼容的四面体网格，实现精确的热传导模拟。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>建筑在欧洲联盟中占据大量的能源使用和温室气体排放。</li>
<li>现有建筑到2050年仍将占据大部分，因此改造对于减少排放至关重要。</li>
<li>当前建筑评估方法主要依赖定性热成像，限制了数据驱动的节能决策。</li>
<li>定量评估使用有限元分析（FEA）提供精确见解，但需要手动CAD设计，过程繁琐且易出错。</li>
<li>Thermoxels是一种基于voxel的新方法，能从稀疏的RGB和热图像生成FEA兼容的模型，包括几何和温度信息。</li>
<li>Thermoxels模型可转化为四面体网格，用于精确热传导模拟。</li>
<li>通过与现有先进方法的比较，展示了Thermoxels在图像合成能力方面的竞争力，并讨论了评估网格质量现有指标的局限性。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.04448">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-69902a83801dc06d1b1d278a30309244.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-35596417805808688f6c69093fc60c5c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-cc885b0ca20339af953135051aa399e5.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-627ce00fd37dba50a2ff20159f8b3639.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="3R-GS-Best-Practice-in-Optimizing-Camera-Poses-Along-with-3DGS"><a href="#3R-GS-Best-Practice-in-Optimizing-Camera-Poses-Along-with-3DGS" class="headerlink" title="3R-GS: Best Practice in Optimizing Camera Poses Along with 3DGS"></a>3R-GS: Best Practice in Optimizing Camera Poses Along with 3DGS</h2><p><strong>Authors:Zhisheng Huang, Peng Wang, Jingdong Zhang, Yuan Liu, Xin Li, Wenping Wang</strong></p>
<p>3D Gaussian Splatting (3DGS) has revolutionized neural rendering with its efficiency and quality, but like many novel view synthesis methods, it heavily depends on accurate camera poses from Structure-from-Motion (SfM) systems. Although recent SfM pipelines have made impressive progress, questions remain about how to further improve both their robust performance in challenging conditions (e.g., textureless scenes) and the precision of camera parameter estimation simultaneously. We present 3R-GS, a 3D Gaussian Splatting framework that bridges this gap by jointly optimizing 3D Gaussians and camera parameters from large reconstruction priors MASt3R-SfM. We note that naively performing joint 3D Gaussian and camera optimization faces two challenges: the sensitivity to the quality of SfM initialization, and its limited capacity for global optimization, leading to suboptimal reconstruction results. Our 3R-GS, overcomes these issues by incorporating optimized practices, enabling robust scene reconstruction even with imperfect camera registration. Extensive experiments demonstrate that 3R-GS delivers high-quality novel view synthesis and precise camera pose estimation while remaining computationally efficient. Project page: <a target="_blank" rel="noopener" href="https://zsh523.github.io/3R-GS/">https://zsh523.github.io/3R-GS/</a> </p>
<blockquote>
<p>3D高斯融合（3DGS）以其高效性和质量彻底改变了神经渲染，但与其他许多新颖的视角合成方法一样，它严重依赖于结构从运动（SfM）系统的准确相机姿态。尽管最近的SfM管道已经取得了令人印象深刻的进展，但仍存在如何进一步提高其在具有挑战性的条件下（例如无纹理场景）的稳健性能以及与相机参数估计精度同时提高的问题。我们提出了3R-GS，这是一个三维高斯融合框架，它通过优化三维高斯和来自大规模重建先验MASt3R-SfM的相机参数来弥合这一鸿沟。我们注意到，盲目进行三维高斯和相机的联合优化面临两个挑战：对SfM初始化的质量敏感，以及全局优化的能力有限，导致重建结果不理想。我们的3R-GS通过融入优化实践克服了这些问题，即使在相机注册不完美的情况下也能实现稳健的场景重建。大量实验表明，3R-GS能够实现高质量的新视角合成和精确的相机姿态估计，同时保持计算效率。项目页面：<a target="_blank" rel="noopener" href="https://zsh523.github.io/3R-GS/">https://zsh523.github.io/3R-GS/</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.04294v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>基于神经渲染的高效性和质量，三维高斯映射（3DGS）在视景合成方法中引发了革命。然而，其依赖于结构从运动（SfM）系统的精确相机姿态。针对SfM管道在具有挑战性的条件下（如纹理缺失的场景）如何进一步提高其稳健性能和同时提升相机参数估计精度的问题，本文提出了3R-GS方法。它通过联合优化三维高斯和来自大规模重建先验MASt3R-SfM的相机参数来解决上述问题。通过采用优化实践，即使在不完美的相机注册下也能实现稳健的场景重建。实验表明，与传统方法相比，这种方法不仅能进行高质量的新型视景合成，还能实现精确的相机姿态估计，且计算效率高。项目页面：<a target="_blank" rel="noopener" href="https://zsh523.github.io/3R-GS/">链接</a>。</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>介绍了基于神经渲染的三维高斯映射（3DGS）技术及其在视景合成中的重要作用。</li>
<li>指出了现有的结构从运动（SfM）系统在特定条件下如纹理缺失场景的局限性，如性能稳健性和相机参数估计的准确性。</li>
<li>介绍了通过联合优化三维高斯和来自大规模重建先验MASt3R-SfM的相机参数的解决方案——即新的方法“3R-GS”。</li>
<li>分析了新方法面临的挑战，包括初始化的质量和全局优化的能力限制，并解释了如何通过优化实践克服这些问题。</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.04294">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-6040f694522d5ba4612f8f27aebfbe29.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-62a3850f27f62f62a4548067a9f2d484.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d0c06cd68225718eb8f76f7b7afdce43.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="WildGS-SLAM-Monocular-Gaussian-Splatting-SLAM-in-Dynamic-Environments"><a href="#WildGS-SLAM-Monocular-Gaussian-Splatting-SLAM-in-Dynamic-Environments" class="headerlink" title="WildGS-SLAM: Monocular Gaussian Splatting SLAM in Dynamic Environments"></a>WildGS-SLAM: Monocular Gaussian Splatting SLAM in Dynamic Environments</h2><p><strong>Authors:Jianhao Zheng, Zihan Zhu, Valentin Bieri, Marc Pollefeys, Songyou Peng, Iro Armeni</strong></p>
<p>We present WildGS-SLAM, a robust and efficient monocular RGB SLAM system designed to handle dynamic environments by leveraging uncertainty-aware geometric mapping. Unlike traditional SLAM systems, which assume static scenes, our approach integrates depth and uncertainty information to enhance tracking, mapping, and rendering performance in the presence of moving objects. We introduce an uncertainty map, predicted by a shallow multi-layer perceptron and DINOv2 features, to guide dynamic object removal during both tracking and mapping. This uncertainty map enhances dense bundle adjustment and Gaussian map optimization, improving reconstruction accuracy. Our system is evaluated on multiple datasets and demonstrates artifact-free view synthesis. Results showcase WildGS-SLAM’s superior performance in dynamic environments compared to state-of-the-art methods. </p>
<blockquote>
<p>我们提出了WildGS-SLAM，这是一个稳健且高效的单目RGB SLAM系统，通过利用不确定性感知几何映射来处理动态环境。与传统的假设场景静态的SLAM系统不同，我们的方法结合了深度和不确定性信息，以提高移动物体存在时的跟踪、映射和渲染性能。我们引入了一个由浅层多层感知器和DINOv2特征预测的不确定性地图，以指导跟踪和映射过程中的动态对象移除。这个不确定性地图增强了密集捆绑调整和高斯地图优化，提高了重建精度。我们的系统在多个数据集上进行了评估，并展示了无伪影的视图合成。结果证明了WildGS-SLAM在动态环境中相较于最先进的方法具有卓越的性能。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.03886v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文介绍了WildGS-SLAM系统，这是一种用于处理动态环境的稳健高效的单目RGB SLAM系统。它通过利用具有感知不确定性的几何映射来增强跟踪、映射和渲染性能。该系统引入了一个由浅层多层感知器和DINOv2特征预测的不确定性地图，用于在跟踪和映射过程中引导动态对象移除。该不确定性地图增强了密集捆调整和高斯地图优化，提高了重建精度。在多个数据集上的评估结果表明，WildGS-SLAM在动态环境中的性能优于现有方法。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>WildGS-SLAM是一种用于处理动态环境的RGB SLAM系统。</li>
<li>系统利用不确定性感知的几何映射进行设计。</li>
<li>引入不确定性地图以增强跟踪、映射和渲染性能。</li>
<li>浅层多层感知器和DINOv2特征用于预测不确定性地图。</li>
<li>不确定性地图有助于提高密集捆调整和高斯地图优化的效果。</li>
<li>WildGS-SLAM在多个数据集上的性能优于现有方法。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.03886">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-80aa034fb50bbbda46f09c88b5dc3b60.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-45ec72a679946cb983b7c1d7e197bcf2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5fb8afd7c85b65bea22fe3c58f1cd44e.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="SelfSplat-Pose-Free-and-3D-Prior-Free-Generalizable-3D-Gaussian-Splatting"><a href="#SelfSplat-Pose-Free-and-3D-Prior-Free-Generalizable-3D-Gaussian-Splatting" class="headerlink" title="SelfSplat: Pose-Free and 3D Prior-Free Generalizable 3D Gaussian   Splatting"></a>SelfSplat: Pose-Free and 3D Prior-Free Generalizable 3D Gaussian   Splatting</h2><p><strong>Authors:Gyeongjin Kang, Jisang Yoo, Jihyeon Park, Seungtae Nam, Hyeonsoo Im, Sangheon Shin, Sangpil Kim, Eunbyung Park</strong></p>
<p>We propose SelfSplat, a novel 3D Gaussian Splatting model designed to perform pose-free and 3D prior-free generalizable 3D reconstruction from unposed multi-view images. These settings are inherently ill-posed due to the lack of ground-truth data, learned geometric information, and the need to achieve accurate 3D reconstruction without finetuning, making it difficult for conventional methods to achieve high-quality results. Our model addresses these challenges by effectively integrating explicit 3D representations with self-supervised depth and pose estimation techniques, resulting in reciprocal improvements in both pose accuracy and 3D reconstruction quality. Furthermore, we incorporate a matching-aware pose estimation network and a depth refinement module to enhance geometry consistency across views, ensuring more accurate and stable 3D reconstructions. To present the performance of our method, we evaluated it on large-scale real-world datasets, including RealEstate10K, ACID, and DL3DV. SelfSplat achieves superior results over previous state-of-the-art methods in both appearance and geometry quality, also demonstrates strong cross-dataset generalization capabilities. Extensive ablation studies and analysis also validate the effectiveness of our proposed methods. Code and pretrained models are available at <a target="_blank" rel="noopener" href="https://gynjn.github.io/selfsplat/">https://gynjn.github.io/selfsplat/</a> </p>
<blockquote>
<p>我们提出了SelfSplat，这是一种新型的三维高斯扩展模型，旨在从无姿势的多视角图像进行无姿态和无三维先验的可泛化的三维重建。由于缺少真实数据、学习到的几何信息，以及需要在不进行微调的情况下实现精确的三维重建，这些设置本质上是病态的，使得传统方法难以实现高质量的结果。我们的模型通过有效地将显式三维表示与自监督的深度和姿态估计技术相结合，解决了这些挑战，实现了姿态准确性和三维重建质量的相互提高。此外，我们引入了一个感知匹配的姿态估计网络和深度细化模块，以提高跨视图的几何一致性，确保更准确和稳定的三维重建。为了展示我们方法的表现，我们在大规模真实世界数据集上对其进行了评估，包括RealEstate10K、ACID和DL3DV。SelfSplat在外观和几何质量方面均达到了先前最先进的水平，并展示了强大的跨数据集泛化能力。广泛的消融研究和分析也验证了我们所提出方法的有效性。代码和预训练模型可在[<a target="_blank" rel="noopener" href="https://gynjn.github.io/selfsplat/]%E6%89%BE%E5%88%B0%E3%80%82">https://gynjn.github.io/selfsplat/]找到。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2411.17190v5">PDF</a> Project page: <a target="_blank" rel="noopener" href="https://gynjn.github.io/selfsplat/">https://gynjn.github.io/selfsplat/</a></p>
<p><strong>Summary</strong></p>
<p>SelfSplat是一种新型3D高斯混合模型，无需姿势和先验信息即可进行多视角图像的通用化3D重建。它通过整合显式3D表示与自监督的深度和姿态估计技术来解决缺乏真实数据等挑战，提高了姿态准确性和重建质量。此外，还加入了匹配感知姿态估计网络和深度优化模块，确保跨视图几何一致性，实现更准确稳定的重建。在大型真实世界数据集上的评估显示，SelfSplat在外观和几何质量上均优于先前的方法，并具有强大的跨数据集泛化能力。</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>SelfSplat是一种新的3D重建模型，可以在无姿势先验的情况下进行多视角图像的可泛化重建。</li>
<li>它通过整合显式三维表示与自监督的深度和姿态估计技术解决了无真实数据的问题。</li>
<li>模型包括匹配感知姿态估计网络和深度优化模块来确保跨视图的几何一致性。</li>
<li>在大规模真实世界数据集上的测试显示，SelfSplat在外观和几何质量方面表现出超越最新技术的优势。同时展示强大的跨数据集泛化能力。 </li>
<li>该模型还实现了姿态准确性和重建质量的相互提升。</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2411.17190">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-acfa17a83a5be5554ba358fc4c6cb766.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-010093d9b72f6230f5992418e15cd1c6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-60abd01a82e63e279de79c28c1e12a9f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c288f90d43e8f383e302879a6aef74ae.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-00980a56c3cad7fef54ca3a989e662a7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c5a301387b3715b0169537bd7aec78fd.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-fa8785ec118c6a612c676cc71fc564a2.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Gaussian-Scenes-Pose-Free-Sparse-View-Scene-Reconstruction-using-Depth-Enhanced-Diffusion-Priors"><a href="#Gaussian-Scenes-Pose-Free-Sparse-View-Scene-Reconstruction-using-Depth-Enhanced-Diffusion-Priors" class="headerlink" title="Gaussian Scenes: Pose-Free Sparse-View Scene Reconstruction using   Depth-Enhanced Diffusion Priors"></a>Gaussian Scenes: Pose-Free Sparse-View Scene Reconstruction using   Depth-Enhanced Diffusion Priors</h2><p><strong>Authors:Soumava Paul, Prakhar Kaushik, Alan Yuille</strong></p>
<p>In this work, we introduce a generative approach for pose-free (without camera parameters) reconstruction of 360 scenes from a sparse set of 2D images. Pose-free scene reconstruction from incomplete, pose-free observations is usually regularized with depth estimation or 3D foundational priors. While recent advances have enabled sparse-view reconstruction of large complex scenes (with high degree of foreground and background detail) with known camera poses using view-conditioned generative priors, these methods cannot be directly adapted for the pose-free setting when ground-truth poses are not available during evaluation. To address this, we propose an image-to-image generative model designed to inpaint missing details and remove artifacts in novel view renders and depth maps of a 3D scene. We introduce context and geometry conditioning using Feature-wise Linear Modulation (FiLM) modulation layers as a lightweight alternative to cross-attention and also propose a novel confidence measure for 3D Gaussian splat representations to allow for better detection of these artifacts. By progressively integrating these novel views in a Gaussian-SLAM-inspired process, we achieve a multi-view-consistent 3D representation. Evaluations on the MipNeRF360 and DL3DV-10K benchmark datasets demonstrate that our method surpasses existing pose-free techniques and performs competitively with state-of-the-art posed (precomputed camera parameters are given) reconstruction methods in complex 360 scenes. </p>
<blockquote>
<p>在这项工作中，我们提出了一种无姿态（无需相机参数）的生成方法，用于从稀疏的二维图像集中重建360度场景。从不完整、无姿态的观察中进行场景重建通常是通过深度估计或三维基础先验进行正则化的。虽然最近的进展已经实现了具有已知相机姿态的大型复杂场景（前景和背景细节丰富）的稀疏视图重建，使用视图条件生成先验，但这些方法无法直接适应无姿态设置，即在评估期间无法使用真实姿态。为了解决这一问题，我们提出了一种图像到图像的生成模型，旨在填充缺失的细节并消除新型视图渲染和三维场景深度图中的伪影。我们通过特征线性调制（FiLM）调制层引入上下文和几何条件，作为交叉注意力的轻量级替代方案，并提出了一种新的三维高斯斑点表示置信度度量，以更好地检测这些伪影。通过以高斯SLAM为灵感的过程逐步集成这些新型视图，我们实现了多视图一致的三维表示。在MipNeRF360和DL3DV-10K基准数据集上的评估表明，我们的方法超越了现有的无姿态技术，并在复杂360度场景的重建方法与最先进的定位（预先计算相机参数已给出）重建方法中表现相当。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2411.15966v2">PDF</a> Project page is available at <a target="_blank" rel="noopener" href="https://gaussianscenes.github.io/">https://gaussianscenes.github.io/</a></p>
<p><strong>摘要</strong><br>     本文介绍了一种无需相机参数的360度场景重建方法，能够从稀疏的2D图像集合中进行场景重建。该方法通过深度估计或3D基础先验进行规则化。虽然最近的进展已经实现了在已知相机姿态下从稀疏视角重建大型复杂场景，但这些方法无法直接适应于没有地面真实姿态的评估时的姿态自由设置。为解决此问题，我们提出了一种图像到图像的生成模型，旨在填补缺失的细节并消除新视图渲染和深度图中的伪影。我们使用特征线性调制（FiLM）调制层引入上下文和几何条件作为跨注意力的轻量级替代方案，并提出了一种针对高斯splat表示的置信度度量来更好地检测这些伪影。通过渐进地整合这些新视图在高斯SLAM启发的过程中，我们获得了一个多视角一致的3D表示。在MipNeRF360和DL3DV-10K基准数据集上的评估表明，我们的方法超越了现有的姿态自由技术，并在复杂360度场景中与最先进的定位重建方法具有竞争力。</p>
<p><strong>要点</strong></p>
<ol>
<li>本文介绍了一种无需相机参数的360度场景重建方法。</li>
<li>该方法通过深度估计或3D基础先验进行规则化。</li>
<li>提出了一种图像到图像的生成模型，用于填充缺失的细节并消除新视图渲染和深度图中的伪影。</li>
<li>引入特征线性调制（FiLM）调制层来处理上下文和几何条件。</li>
<li>提出一种针对高斯splat表示的置信度度量来检测伪影。</li>
<li>通过渐进地整合新视图，实现多视角一致的3D表示。</li>
<li>在基准数据集上的评估表明，该方法在姿态自由技术方面表现出色，与最先进的定位重建方法具有竞争力。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2411.15966">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-0d16c638b724e302202e9ac54eee4407.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-6a42c326b9dab99129e68f0a717d16da.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-43f42cac0388a7780ab0772208c421bf.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0b4ec731f275d2e00dd765110da70fca.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-53b3ae10a8e38eed2786e5048f9bff82.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="RNG-Relightable-Neural-Gaussians"><a href="#RNG-Relightable-Neural-Gaussians" class="headerlink" title="RNG: Relightable Neural Gaussians"></a>RNG: Relightable Neural Gaussians</h2><p><strong>Authors:Jiahui Fan, Fujun Luan, Jian Yang, Miloš Hašan, Beibei Wang</strong></p>
<p>3D Gaussian Splatting (3DGS) has shown impressive results for the novel view synthesis task, where lighting is assumed to be fixed. However, creating relightable 3D assets, especially for objects with ill-defined shapes (fur, fabric, etc.), remains a challenging task. The decomposition between light, geometry, and material is ambiguous, especially if either smooth surface assumptions or surfacebased analytical shading models do not apply. We propose Relightable Neural Gaussians (RNG), a novel 3DGS-based framework that enables the relighting of objects with both hard surfaces or soft boundaries, while avoiding assumptions on the shading model. We condition the radiance at each point on both view and light directions. We also introduce a shadow cue, as well as a depth refinement network to improve shadow accuracy. Finally, we propose a hybrid forward-deferred fitting strategy to balance geometry and appearance quality. Our method achieves significantly faster training (1.3 hours) and rendering (60 frames per second) compared to a prior method based on neural radiance fields and produces higher-quality shadows than a concurrent 3DGS-based method. Project page: <a target="_blank" rel="noopener" href="https://www.whois-jiahui.fun/project_pages/RNG">https://www.whois-jiahui.fun/project_pages/RNG</a>. </p>
<blockquote>
<p>三维高斯绘制技术（3DGS）对于新的视角合成任务已经取得了令人印象深刻的结果，该任务假设光照是固定的。然而，创建可重新照明的三维资产仍然是一个具有挑战性的任务，特别是对于形状不明确（如毛发、织物等）的对象。光线、几何形状和材料之间的分解关系不明确，特别是如果光滑表面假设或基于表面的分析着色模型不适用的话。我们提出了可重新照明的神经高斯（RNG），这是一种基于三维高斯绘制技术（3DGS）的新框架，可以实现对具有硬表面或软边界的物体的重新照明，同时避免了对着色模型的假设。我们将每一点的辐射量设置为视点和光照方向的条件。我们还引入了一个阴影提示和一个深度细化网络来提高阴影的准确性。最后，我们提出了一种混合的前向延迟拟合策略来平衡几何形状和外观质量。我们的方法与基于神经辐射场的先前方法相比，实现了更快的训练（1.3小时）和渲染（每秒60帧），并且与同期的基于三维高斯绘制技术的方法相比，产生了更高质量的阴影。项目页面：<a target="_blank" rel="noopener" href="https://www.whois-jiahui.fun/project_pages/RNG%E3%80%82">https://www.whois-jiahui.fun/project_pages&#x2F;RNG。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2409.19702v5">PDF</a> Camera-ready version. Proceedings of CVPR 2025</p>
<p><strong>Summary</strong></p>
<p>在光照固定假设下，基于神经网络辐射场的观点合成方法虽展现出惊人效果，但对于照明可调整的三维资产创建，尤其是形状不明确物体（如毛发、布料等）仍是一大挑战。本文对基于高斯插值的几何渲染（Geometry Splatting）提出一种改进方法——Relightable Neural Gaussians（RNG）。该方法能够针对具有硬表面或软边界的对象进行照明调整，无需对阴影模型进行假设。此外，我们还引入了一种阴影提示和一个深度修正网络来提高阴影的准确性。最终，通过一种混合的正向延迟拟合策略平衡几何和外观质量。我们的方法相较于基于神经网络辐射场的先前方法，训练时间显著缩短（仅1.3小时），渲染速度更快（每秒60帧），并且产生的阴影质量更高。相关项目页面可访问：<a target="_blank" rel="noopener" href="https://www.whois-jiahui.fun/project_pages/RNG">链接</a>。</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>3DGS在高光固定的假设下对于视点合成效果良好，但在照明可调整的三维资产创建上仍面临挑战。</li>
<li>RNG框架解决照明可调整性问题，适用于硬表面和软边界物体。</li>
<li>RNG不需要对阴影模型进行假设，通过引入阴影提示和深度修正网络提高阴影准确性。</li>
<li>混合正向延迟拟合策略实现几何和外观质量的平衡。</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2409.19702">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-5623541c6f6e44caa9f179fc060a805e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2f7a2a2bb6183ffea7599ff23098f6d0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c20d546954b3a9e1bf33f2c1c1149206.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3cf94761c46975cf6949ae4dc8f22102.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="Enhancing-Temporal-Consistency-in-Video-Editing-by-Reconstructing-Videos-with-3D-Gaussian-Splatting"><a href="#Enhancing-Temporal-Consistency-in-Video-Editing-by-Reconstructing-Videos-with-3D-Gaussian-Splatting" class="headerlink" title="Enhancing Temporal Consistency in Video Editing by Reconstructing Videos   with 3D Gaussian Splatting"></a>Enhancing Temporal Consistency in Video Editing by Reconstructing Videos   with 3D Gaussian Splatting</h2><p><strong>Authors:Inkyu Shin, Qihang Yu, Xiaohui Shen, In So Kweon, Kuk-Jin Yoon, Liang-Chieh Chen</strong></p>
<p>Recent advancements in zero-shot video diffusion models have shown promise for text-driven video editing, but challenges remain in achieving high temporal consistency. To address this, we introduce Video-3DGS, a 3D Gaussian Splatting (3DGS)-based video refiner designed to enhance temporal consistency in zero-shot video editors. Our approach utilizes a two-stage 3D Gaussian optimizing process tailored for editing dynamic monocular videos. In the first stage, Video-3DGS employs an improved version of COLMAP, referred to as MC-COLMAP, which processes original videos using a Masked and Clipped approach. For each video clip, MC-COLMAP generates the point clouds for dynamic foreground objects and complex backgrounds. These point clouds are utilized to initialize two sets of 3D Gaussians (Frg-3DGS and Bkg-3DGS) aiming to represent foreground and background views. Both foreground and background views are then merged with a 2D learnable parameter map to reconstruct full views. In the second stage, we leverage the reconstruction ability developed in the first stage to impose the temporal constraints on the video diffusion model. To demonstrate the efficacy of Video-3DGS on both stages, we conduct extensive experiments across two related tasks: Video Reconstruction and Video Editing. Video-3DGS trained with 3k iterations significantly improves video reconstruction quality (+3 PSNR, +7 PSNR increase) and training efficiency (x1.9, x4.5 times faster) over NeRF-based and 3DGS-based state-of-art methods on DAVIS dataset, respectively. Moreover, it enhances video editing by ensuring temporal consistency across 58 dynamic monocular videos. </p>
<blockquote>
<p>最近的零样本视频扩散模型进展对文本驱动的视频编辑显示出巨大潜力，但在实现高时间一致性方面仍存在挑战。为了解决这一问题，我们引入了Video-3DGS，这是一种基于3D高斯拼贴（3DGS）的视频细化器，旨在提高零样本视频编辑器的时间一致性。我们的方法利用两阶段3D高斯优化过程，针对动态单目视频编辑而定制。在第一阶段，Video-3DGS采用COLMAP的改进版本，称为MC-COLMAP，它通过遮罩和裁剪的方法处理原始视频。对于每个视频片段，MC-COLMAP生成动态前景对象和复杂背景的点云。这些点云用于初始化两组3D高斯（Frg-3DGS和Bkg-3DGS），旨在表示前景和背景视图。然后将前景和背景视图与2D可学习参数图合并，以重建全景。在第二阶段，我们利用第一阶段开发的重建能力对视频扩散模型施加时间约束。为了证明Video-3DGS在两个阶段的有效性，我们在两个相关任务上进行了大量实验：视频重建和视频编辑。Video-3DGS在DAVIS数据集上与基于NeRF和基于3DGS的最先进方法相比，经过3k次迭代训练，显著提高了视频重建质量（+3 PSNR，+7 PSNR增加），并提高了训练效率（分别加快了1.9倍和4.5倍）。此外，它通过确保58个动态单目视频的时间一致性，增强了视频编辑效果。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2406.02541v4">PDF</a> Accepted to TMLR 2025. Project page at   <a target="_blank" rel="noopener" href="https://video-3dgs-project.github.io/">https://video-3dgs-project.github.io/</a></p>
<p><strong>摘要</strong></p>
<p>近期零样本视频扩散模型在文本驱动的视频编辑领域展现出巨大潜力，但在实现高时间一致性方面仍面临挑战。为解决这一问题，我们推出Video-3DGS，一种基于3D高斯拼贴（3DGS）的视频精炼器，旨在提升零样本视频编辑器的时间一致性。该方法采用两阶段3D高斯优化流程，专门针对动态单目视频编辑。第一阶段，Video-3DGS采用改进版COLMAP（称为MC-COLMAP），通过遮罩和裁剪方法处理原始视频。MC-COLMAP为每段视频生成动态前景和复杂背景的点云，用于初始化两组3D高斯（Frg-3DGS和Bkg-3DGS），分别代表前景和背景视图。然后，前景和背景视图与2D可学习参数图合并，以重建全视图。第二阶段，我们利用第一阶段的重建能力，对视频扩散模型施加时间约束。为证明Video-3DGS在两个阶段的有效性，我们在两个相关任务（视频重建和视频编辑）上进行了广泛实验。在DAVIS数据集上，与最新基于NeRF和基于3DGS的方法相比，Video-3DGS经过3k次迭代训练，显著提高了视频重建质量（PSNR提高3点，提高7点），并提高了训练效率（分别提高了1.9倍和4.5倍）。此外，它还能确保在58个动态单目视频上的时间一致性，从而提高视频编辑效果。</p>
<p><strong>关键见解</strong></p>
<ol>
<li>Video-3DGS被引入以解决零样本视频编辑中的时间一致性问题。</li>
<li>引入两阶段3D高斯优化流程，适用于动态单目视频编辑。</li>
<li>MC-COLMAP方法用于生成前景和背景的点云，初始化3D高斯。</li>
<li>Video-3DGS在DAVIS数据集上的实验表明，相较于其他方法，它能显著提高视频重建的质量和训练效率。</li>
<li>Video-3DGS通过确保时间一致性，提升了视频编辑的效果。</li>
<li>该方法在两个相关任务（视频重建和视频编辑）上均表现出优异性能。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2406.02541">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-db1aa88f168a9dc813b77e27508dbcdb.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-32f0c93b2bde571af9c5b7f26846f95a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-1a14b4762fcc70b5d04b3be32f030a61.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c135640fbe09b755272b0ea0d6f15254.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a37a69fc232575f75630c7c0a178d297.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="LeanGaussian-Breaking-Pixel-or-Point-Cloud-Correspondence-in-Modeling-3D-Gaussians"><a href="#LeanGaussian-Breaking-Pixel-or-Point-Cloud-Correspondence-in-Modeling-3D-Gaussians" class="headerlink" title="LeanGaussian: Breaking Pixel or Point Cloud Correspondence in Modeling   3D Gaussians"></a>LeanGaussian: Breaking Pixel or Point Cloud Correspondence in Modeling   3D Gaussians</h2><p><strong>Authors:Jiamin Wu, Kenkun Liu, Han Gao, Xiaoke Jiang, Lei Zhang</strong></p>
<p>Recently, Gaussian splatting has demonstrated significant success in novel view synthesis. Current methods often regress Gaussians with pixel or point cloud correspondence, linking each Gaussian with a pixel or a 3D point. This leads to the redundancy of Gaussians being used to overfit the correspondence rather than the objects represented by the 3D Gaussians themselves, consequently wasting resources and lacking accurate geometries or textures. In this paper, we introduce LeanGaussian, a novel approach that treats each query in deformable Transformer as one 3D Gaussian ellipsoid, breaking the pixel or point cloud correspondence constraints. We leverage deformable decoder to iteratively refine the Gaussians layer-by-layer with the image features as keys and values. Notably, the center of each 3D Gaussian is defined as 3D reference points, which are then projected onto the image for deformable attention in 2D space. On both the ShapeNet SRN dataset (category level) and the Google Scanned Objects dataset (open-category level, trained with the Objaverse dataset), our approach, outperforms prior methods by approximately 6.1%, achieving a PSNR of 25.44 and 22.36, respectively. Additionally, our method achieves a 3D reconstruction speed of 7.2 FPS and rendering speed 500 FPS. Codes are available at <a target="_blank" rel="noopener" href="https://github.com/jwubz123/LeanGaussian">https://github.com/jwubz123/LeanGaussian</a>. </p>
<blockquote>
<p>近期，高斯喷溅技术在新型视图合成方面取得了显著的成功。当前的方法通常通过像素或点云对应关系回归高斯，将每个高斯与像素或3D点相关联。这导致使用大量冗余高斯来过度拟合对应关系，而非由3D高斯本身所代表的物体，从而浪费资源，并且缺乏精确几何或纹理。在本文中，我们介绍了LeanGaussian，这是一种新颖的方法，它将可变形Transformer中的每个查询视为一个3D高斯椭圆体，打破了像素或点云对应关系的约束。我们利用可变形解码器逐层迭代地细化高斯，以图像特征作为键和值。值得注意的是，每个3D高斯的中心被定义为3D参考点，然后投影到图像上进行2D空间的可变形注意力。在ShapeNet SRN数据集（类别级别）和Google扫描对象数据集（开放类别级别，使用Objaverse数据集进行训练）上，我们的方法较之前的方法高出约6.1%，分别实现了PSNR值为25.44和22.36。此外，我们的方法实现了3D重建速度为7.2 FPS和渲染速度为500 FPS。代码可通过<a target="_blank" rel="noopener" href="https://github.com/jwubz123/LeanGaussian%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/jwubz123/LeanGaussian获取。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2404.16323v3">PDF</a> </p>
<p><strong>摘要</strong><br>    本文提出LeanGaussian方法，以3D高斯椭圆体处理变形Transformer中的查询，打破像素或点云对应的约束。利用可变形解码器逐层细化高斯，以图像特征为键和值。定义每个3D高斯的中心为3D参考点，投影到图像上进行二维空间的变形注意。在ShapeNet SRN数据集和Google扫描对象数据集上，该方法优于现有方法约6.1%，PSNR分别达到25.44和22.36。此外，该方法达到3D重建速度7.2 FPS和渲染速度500 FPS。</p>
<p><strong>关键要点</strong></p>
<ul>
<li>LeanGaussian方法引入3D高斯椭圆体处理变形Transformer中的查询。</li>
<li>方法打破像素或点云对应的约束，利用可变形解码器逐层细化高斯。</li>
<li>3D高斯中心定义为3D参考点，用于二维空间的变形注意。</li>
<li>在ShapeNet SRN和Google扫描对象数据集上，该方法性能优于现有方法约6.1%。</li>
<li>LeanGaussian方法实现3D重建速度7.2 FPS和渲染速度500 FPS。</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2404.16323">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-0b97721984d85ff4e81ef74c3dbb780a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b3ed68eade2d1cc550451ee132d9d6f4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-598a7e528836754e5c461a22002b1454.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-edb95b68d707cb8e8a96b0032017cdc1.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-4e0e0a035c452a7943320afb9c5a9727.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-85f7de78ff2be66d0c334d8d078911fb.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        文章作者:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        文章链接:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-04-09/3DGS/">https://kedreamix.github.io/Talk2Paper/Paper/2025-04-09/3DGS/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        版权声明:
                    </i>
                </span>
                <span class="reprint-info">
                    本博客所有文章除特別声明外，均采用
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    许可协议。转载请注明来源
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/3DGS/">
                                    <span class="chip bg-color">3DGS</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;上一篇</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-04-09/NeRF/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-f57da3615c8eb9d74925d80c987ef4f2.jpg" class="responsive-img" alt="NeRF">
                        
                        <span class="card-title">NeRF</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            NeRF 方向最新论文已更新，请持续关注 Update in 2025-04-09  DeclutterNeRF Generative-Free 3D Scene Recovery for Occlusion Removal
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-04-09
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/NeRF/" class="post-category">
                                    NeRF
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/NeRF/">
                        <span class="chip bg-color">NeRF</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                下一篇&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-04-09/%E5%85%83%E5%AE%87%E5%AE%99_%E8%99%9A%E6%8B%9F%E4%BA%BA/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-41d76e454de5b601d43b812addd8ee3c.jpg" class="responsive-img" alt="元宇宙/虚拟人">
                        
                        <span class="card-title">元宇宙/虚拟人</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            元宇宙/虚拟人 方向最新论文已更新，请持续关注 Update in 2025-04-09  A multidimensional measurement of photorealistic avatar quality of   experience
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-04-09
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E5%85%83%E5%AE%87%E5%AE%99-%E8%99%9A%E6%8B%9F%E4%BA%BA/" class="post-category">
                                    元宇宙/虚拟人
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E5%85%83%E5%AE%87%E5%AE%99-%E8%99%9A%E6%8B%9F%E4%BA%BA/">
                        <span class="chip bg-color">元宇宙/虚拟人</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- 代码块功能依赖 -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- 代码语言 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- 代码块复制 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- 代码块收缩 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC 悬浮按钮. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* 修复文章卡片 div 的宽度. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // 切换TOC目录展开收缩的相关操作.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;站点总字数:&nbsp;<span
                        class="white-color">17204.1k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- 运行天数提醒. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // 区分是否有年份.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = '本站已运行 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                daysTip = '本站已運行 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = '本站已运行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = '本站已運行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="关注我的知乎: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">知</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- 搜索遮罩框 -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span>
            <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- 白天和黑夜主题 -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- 回到顶部按钮 -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // 只在桌面版网页启用特效
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- 雪花特效 -->
    

    <!-- 鼠标星星特效 -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--腾讯兔小巢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- 动态标签 -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Σ(っ °Д °;)っ诶，页面崩溃了嘛？", clearTimeout(st)) : (document.title = "φ(゜▽゜*)♪咦，又好了！", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
