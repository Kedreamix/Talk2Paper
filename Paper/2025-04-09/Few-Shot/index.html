<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Few-Shot">
    <meta name="description" content="Few-Shot æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-04-09  AI for Climate Finance Agentic Retrieval and Multi-Step Reasoning for   Early Warning System Investments">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Few-Shot | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pic1.zhimg.com/v2-1b45b57645b7d18c5b5fcfd91210e2b3.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Few-Shot</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Few-Shot/">
                                <span class="chip bg-color">Few-Shot</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Few-Shot/" class="post-category">
                                Few-Shot
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-04-09
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-05-14
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    20k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    82 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-04-09-æ›´æ–°"><a href="#2025-04-09-æ›´æ–°" class="headerlink" title="2025-04-09 æ›´æ–°"></a>2025-04-09 æ›´æ–°</h1><h2 id="AI-for-Climate-Finance-Agentic-Retrieval-and-Multi-Step-Reasoning-for-Early-Warning-System-Investments"><a href="#AI-for-Climate-Finance-Agentic-Retrieval-and-Multi-Step-Reasoning-for-Early-Warning-System-Investments" class="headerlink" title="AI for Climate Finance: Agentic Retrieval and Multi-Step Reasoning for   Early Warning System Investments"></a>AI for Climate Finance: Agentic Retrieval and Multi-Step Reasoning for   Early Warning System Investments</h2><p><strong>Authors:Saeid Ario Vaghefi, Aymane Hachcham, Veronica Grasso, Jiska Manicus, Nakiete Msemo, Chiara Colesanti Senni, Markus Leippold</strong></p>
<p>Tracking financial investments in climate adaptation is a complex and expertise-intensive task, particularly for Early Warning Systems (EWS), which lack standardized financial reporting across multilateral development banks (MDBs) and funds. To address this challenge, we introduce an LLM-based agentic AI system that integrates contextual retrieval, fine-tuning, and multi-step reasoning to extract relevant financial data, classify investments, and ensure compliance with funding guidelines. Our study focuses on a real-world application: tracking EWS investments in the Climate Risk and Early Warning Systems (CREWS) Fund. We analyze 25 MDB project documents and evaluate multiple AI-driven classification methods, including zero-shot and few-shot learning, fine-tuned transformer-based classifiers, chain-of-thought (CoT) prompting, and an agent-based retrieval-augmented generation (RAG) approach. Our results show that the agent-based RAG approach significantly outperforms other methods, achieving 87% accuracy, 89% precision, and 83% recall. Additionally, we contribute a benchmark dataset and expert-annotated corpus, providing a valuable resource for future research in AI-driven financial tracking and climate finance transparency. </p>
<blockquote>
<p>è·Ÿè¸ªæ°”å€™é€‚åº”ä¸­çš„é‡‘èæŠ•èµ„æ˜¯ä¸€é¡¹å¤æ‚ä¸”éœ€è¦ä¸“ä¸šçŸ¥è¯†çš„å·¥ä½œï¼Œç‰¹åˆ«æ˜¯å¯¹äºç¼ºä¹å¤šè¾¹å‘å±•é“¶è¡Œå’ŒåŸºé‡‘æ ‡å‡†åŒ–è´¢åŠ¡æŠ¥å‘Šçš„æ—©æœŸé¢„è­¦ç³»ç»Ÿï¼ˆEWSï¼‰è€Œè¨€æ›´æ˜¯å¦‚æ­¤ã€‚ä¸ºäº†åº”å¯¹è¿™ä¸€æŒ‘æˆ˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ªåŸºäºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„ä¸»ä½“äººå·¥æ™ºèƒ½ç³»ç»Ÿï¼Œè¯¥ç³»ç»Ÿèåˆäº†æƒ…å¢ƒæ£€ç´¢ã€å¾®è°ƒä»¥åŠå¤šæ­¥æ¨ç†ï¼Œä»¥æå–ç›¸å…³çš„è´¢åŠ¡æ•°æ®ï¼Œå¯¹æŠ•èµ„è¿›è¡Œåˆ†ç±»ï¼Œå¹¶ç¡®ä¿ç¬¦åˆèµ„é‡‘æŒ‡å¯¼æ–¹é’ˆã€‚æˆ‘ä»¬çš„ç ”ç©¶å…³æ³¨ä¸€ä¸ªç°å®ä¸–ç•Œçš„è¿ç”¨ï¼šè·Ÿè¸ªæ°”å€™é£é™©å’Œæ—©æœŸé¢„è­¦ç³»ç»Ÿï¼ˆCREWSï¼‰åŸºé‡‘ä¸­çš„EWSæŠ•èµ„ã€‚æˆ‘ä»¬åˆ†æäº†2.pdfä¸­åŒ…å«äº†æœ‰å…³æ°”å€™é£é™©ç®¡ç†å’Œé¢„è­¦ç³»ç»Ÿçš„æ–‡çŒ®å’Œé¡¹ç›®æ–‡æ¡£ç­‰ç›¸å…³ä¿¡æ¯å¤šä¸ªè‡ªç„¶æ ‡æ³¨çš„æ°”å€™é‡‘èé£é™©ç›¸å…³é£é™©éšæ‚£çš„æ—©æœŸé¢„è­¦æ¨¡å‹å…¬å¼€æ•°æ®ã€‚é€šè¿‡å¯¹åŒ…æ‹¬é›¶æ¬¡å­¦ä¹ å’Œå°‘æ•°å­¦ä¹ åœ¨å†…çš„å¤šç§äººå·¥æ™ºèƒ½é©±åŠ¨çš„åˆ†ç±»æ–¹æ³•è¿›è¡Œåˆ†æå’Œè¯„ä¼°ï¼Œä»¥åŠå¾®è°ƒåŸºäºè½¬æ¢å™¨çš„åˆ†ç±»å™¨ã€é“¾å¼æ€ç»´ï¼ˆCoTï¼‰æç¤ºå’ŒåŸºäºä»£ç†çš„æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰æ–¹æ³•ç­‰ï¼Œæˆ‘ä»¬å‘ç°åŸºäºä»£ç†çš„RAGæ–¹æ³•æ˜¾è‘—ä¼˜äºå…¶ä»–æ–¹æ³•ï¼Œè¾¾åˆ°äº†87%çš„å‡†ç¡®æ€§ã€89%çš„ç²¾ç¡®åº¦å’Œ83%çš„å¬å›ç‡ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜è´¡çŒ®äº†ä¸€ä¸ªåŸºå‡†æ•°æ®é›†å’Œä¸“å®¶æ³¨é‡Šè¯­æ–™åº“ï¼Œä¸ºäººå·¥æ™ºèƒ½é©±åŠ¨çš„è´¢åŠ¡è·Ÿè¸ªå’Œæ°”å€™é‡‘èé€æ˜åº¦çš„æœªæ¥ç ”ç©¶æä¾›äº†å®è´µçš„èµ„æºã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.05104v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸€ä¸ªåŸºäºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„ä»£ç†äººå·¥æ™ºèƒ½ç³»ç»Ÿï¼Œç”¨äºè·Ÿè¸ªæ°”å€™é€‚åº”æŠ•èµ„ã€‚è¯¥ç³»ç»Ÿç»“åˆäº†æƒ…å¢ƒæ£€ç´¢ã€å¾®è°ƒå’Œå¤šæ­¥æ¨ç†æŠ€æœ¯ï¼Œèƒ½å¤Ÿä»å¤šè¾¹å‘å±•é“¶è¡Œå’ŒåŸºé‡‘çš„é¡¹ç›®æ–‡æ¡£ä¸­æŠ½å–ç›¸å…³è´¢åŠ¡æ•°æ®ï¼Œå¯¹æŠ•èµ„è¿›è¡Œåˆ†ç±»ï¼Œå¹¶ç¡®ä¿ç¬¦åˆèµ„åŠ©æŒ‡å—çš„è¦æ±‚ã€‚åœ¨æ°”å€™é£é™©å’Œé¢„è­¦ç³»ç»ŸåŸºé‡‘ï¼ˆCREWSï¼‰çš„æ—©æœŸé¢„è­¦ç³»ç»Ÿï¼ˆEWSï¼‰æŠ•èµ„è·Ÿè¸ªçš„å®ä¾‹åº”ç”¨ä¸­ï¼Œè¯¥ç³»ç»Ÿæ˜¾è‘—ä¼˜äºå…¶ä»–æ–¹æ³•ï¼Œå®ç°äº†è¾ƒé«˜çš„å‡†ç¡®æ€§å’Œç²¾ç¡®åº¦ã€‚æ­¤å¤–ï¼Œæœ¬æ–‡è¿˜æä¾›äº†ä¸€ä¸ªåŸºå‡†æ•°æ®é›†å’Œä¸“å®¶æ³¨é‡Šè¯­æ–™åº“ï¼Œä¸ºæœªæ¥åœ¨é‡‘èè·Ÿè¸ªå’Œæ°”å€™é‡‘èé€æ˜åº¦æ–¹é¢çš„AIç ”ç©¶æä¾›äº†å®è´µèµ„æºã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¼•å…¥åŸºäºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„ä»£ç†äººå·¥æ™ºèƒ½ç³»ç»Ÿæ¥åº”å¯¹æ°”å€™é€‚åº”æŠ•èµ„è·Ÿè¸ªçš„æŒ‘æˆ˜ã€‚</li>
<li>è¯¥ç³»ç»Ÿé›†æˆäº†æƒ…å¢ƒæ£€ç´¢ã€å¾®è°ƒåŠå¤šæ­¥æ¨ç†æŠ€æœ¯ä»¥æŠ½å–å’Œåˆ†ç±»é‡‘èæ•°æ®ã€‚</li>
<li>ç ”ç©¶ä¾§é‡äºå®é™…åº”ç”¨ç¨‹åºï¼šè·Ÿè¸ªæ°”å€™é£é™©ä¸æ—©æœŸé¢„è­¦ç³»ç»ŸåŸºé‡‘ï¼ˆCREWSï¼‰çš„æ—©æœŸé¢„è­¦ç³»ç»Ÿï¼ˆEWSï¼‰æŠ•èµ„ã€‚</li>
<li>é€šè¿‡åˆ†æ25ä¸ªMDBé¡¹ç›®æ–‡æ¡£ï¼Œè¯„ä¼°äº†å¤šç§AIé©±åŠ¨çš„åˆ†ç±»æ–¹æ³•ã€‚</li>
<li>ä»£ç†åŸºäºæ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰çš„æ–¹æ³•åœ¨æ€§èƒ½ä¸Šæ˜¾è‘—ä¼˜äºå…¶ä»–æ–¹æ³•ï¼Œè¾¾åˆ°é«˜å‡†ç¡®æ€§ã€ç²¾ç¡®æ€§å’Œå¬å›ç‡ã€‚</li>
<li>è´¡çŒ®äº†ä¸€ä¸ªåŸºå‡†æ•°æ®é›†å’Œä¸“å®¶æ³¨é‡Šè¯­æ–™åº“ï¼Œä¸ºæœªæ¥ç ”ç©¶æä¾›èµ„æºã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.05104">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-b14b2138cc9d33b6f61ec87ff52b6ea6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8dbecbfc97e68a765663f7bc7204c7df.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-cb925f8fd08ff465ea3ab621c914fc96.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="CMaP-SAM-Contraction-Mapping-Prior-for-SAM-driven-Few-shot-Segmentation"><a href="#CMaP-SAM-Contraction-Mapping-Prior-for-SAM-driven-Few-shot-Segmentation" class="headerlink" title="CMaP-SAM: Contraction Mapping Prior for SAM-driven Few-shot Segmentation"></a>CMaP-SAM: Contraction Mapping Prior for SAM-driven Few-shot Segmentation</h2><p><strong>Authors:Shuai Chen, Fanman Meng, Haoran Wei, Chenhao Wu, Qingbo Wu, Linfeng Xu, Hongliang Li</strong></p>
<p>Few-shot segmentation (FSS) aims to segment new classes using few annotated images. While recent FSS methods have shown considerable improvements by leveraging Segment Anything Model (SAM), they face two critical limitations: insufficient utilization of structural correlations in query images, and significant information loss when converting continuous position priors to discrete point prompts. To address these challenges, we propose CMaP-SAM, a novel framework that introduces contraction mapping theory to optimize position priors for SAM-driven few-shot segmentation. CMaP-SAM consists of three key components: (1) a contraction mapping module that formulates position prior optimization as a Banach contraction mapping with convergence guarantees. This module iteratively refines position priors through pixel-wise structural similarity, generating a converged prior that preserves both semantic guidance from reference images and structural correlations in query images; (2) an adaptive distribution alignment module bridging continuous priors with SAMâ€™s binary mask prompt encoder; and (3) a foreground-background decoupled refinement architecture producing accurate final segmentation masks. Extensive experiments demonstrate CMaP-SAMâ€™s effectiveness, achieving state-of-the-art performance with 71.1 mIoU on PASCAL-$5^i$ and 56.1 on COCO-$20^i$ datasets. </p>
<blockquote>
<p>å°‘é‡æ ·æœ¬åˆ†å‰²ï¼ˆFSSï¼‰æ—¨åœ¨ä½¿ç”¨å°‘é‡æ ‡æ³¨å›¾åƒå¯¹æ–°ç±»åˆ«è¿›è¡Œåˆ†å‰²ã€‚è™½ç„¶æœ€è¿‘çš„FSSæ–¹æ³•é€šè¿‡åˆ©ç”¨åˆ†å‰²ä»»ä½•äº‹ç‰©æ¨¡å‹ï¼ˆSAMï¼‰å–å¾—äº†æ˜¾è‘—çš„æ”¹è¿›ï¼Œä½†å®ƒä»¬é¢ä¸´ä¸¤ä¸ªå…³é”®å±€é™ï¼šæœªå……åˆ†åˆ©ç”¨æŸ¥è¯¢å›¾åƒä¸­çš„ç»“æ„ç›¸å…³æ€§ï¼Œä»¥åŠåœ¨å°†è¿ç»­ä½ç½®å…ˆéªŒè½¬æ¢ä¸ºç¦»æ•£ç‚¹æç¤ºæ—¶ä¸¢å¤±äº†å¤§é‡ä¿¡æ¯ã€‚ä¸ºäº†è§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†CMaP-SAMï¼Œè¿™æ˜¯ä¸€ä¸ªå¼•å…¥æ”¶ç¼©æ˜ å°„ç†è®ºæ¥ä¼˜åŒ–SAMé©±åŠ¨å°‘é‡æ ·æœ¬åˆ†å‰²çš„ä½ç½®å…ˆéªŒå€¼çš„æ–°æ¡†æ¶ã€‚CMaP-SAMç”±ä¸‰ä¸ªå…³é”®ç»„ä»¶æ„æˆï¼šï¼ˆ1ï¼‰æ”¶ç¼©æ˜ å°„æ¨¡å—ï¼Œå®ƒå°†ä½ç½®å…ˆéªŒä¼˜åŒ–å…¬å¼åŒ–ä¸ºå…·æœ‰æ”¶æ•›ä¿è¯çš„å·´æ‹¿èµ«æ”¶ç¼©æ˜ å°„ã€‚è¯¥æ¨¡å—é€šè¿‡åƒç´ çº§ç»“æ„ç›¸ä¼¼æ€§è¿­ä»£åœ°ä¼˜åŒ–ä½ç½®å…ˆéªŒï¼Œç”Ÿæˆä¸€ä¸ªæ”¶æ•›çš„å…ˆéªŒå€¼ï¼Œè¯¥å…ˆéªŒå€¼æ—¢ä¿ç•™äº†å‚è€ƒå›¾åƒçš„è¯­ä¹‰æŒ‡å¯¼ï¼Œåˆä¿ç•™äº†æŸ¥è¯¢å›¾åƒä¸­çš„ç»“æ„ç›¸å…³æ€§ï¼›ï¼ˆ2ï¼‰è‡ªé€‚åº”åˆ†å¸ƒå¯¹é½æ¨¡å—ï¼Œè¯¥æ¨¡å—è¿æ¥è¿ç»­å…ˆéªŒä¸SAMçš„äºŒè¿›åˆ¶æ©è†œæç¤ºç¼–ç å™¨ï¼›ï¼ˆ3ï¼‰å‰æ™¯èƒŒæ™¯è§£è€¦ç»†åŒ–æ¶æ„ï¼Œç”Ÿæˆç²¾ç¡®çš„æœ€ç»ˆåˆ†å‰²æ©è†œã€‚å¤§é‡å®éªŒè¯æ˜äº†CMaP-SAMçš„æœ‰æ•ˆæ€§ï¼Œåœ¨PASCAL-$5^i$æ•°æ®é›†ä¸Šè¾¾åˆ°äº†71.1 mIoUçš„æœ€æ–°æ€§èƒ½ï¼Œåœ¨COCO-$20^i$æ•°æ®é›†ä¸Šè¾¾åˆ°äº†56.1çš„æ€§èƒ½ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.05049v1">PDF</a> 7 figures</p>
<p><strong>Summary</strong></p>
<p>åŸºäºå°‘é‡æ ‡æ³¨å›¾åƒè¿›è¡Œæ–°ç±»åˆ«åˆ†å‰²çš„Few-shot segmentationï¼ˆFSSï¼‰æ–¹æ³•è™½ç„¶å·²ç»å–å¾—äº†æ˜¾è‘—è¿›æ­¥ï¼Œä½†å®ƒä»¬ä»ç„¶é¢ä¸´ä¸¤ä¸ªå…³é”®é—®é¢˜ï¼šæœªèƒ½å……åˆ†åˆ©ç”¨æŸ¥è¯¢å›¾åƒä¸­çš„ç»“æ„ç›¸å…³æ€§ï¼Œä»¥åŠåœ¨å°†è¿ç»­ä½ç½®å…ˆéªŒè½¬æ¢ä¸ºç¦»æ•£ç‚¹æç¤ºæ—¶ä¸¢å¤±äº†å¤§é‡ä¿¡æ¯ã€‚ä¸ºè§£å†³è¿™äº›é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†CMaP-SAMæ¡†æ¶ï¼Œå¼•å…¥æ”¶ç¼©æ˜ å°„ç†è®ºä¼˜åŒ–SAMçš„ä½ç½®å…ˆéªŒä¿¡æ¯ã€‚CMaP-SAMåŒ…æ‹¬ä¸‰ä¸ªå…³é”®ç»„ä»¶ï¼šæ”¶ç¼©æ˜ å°„æ¨¡å—ã€è‡ªé€‚åº”åˆ†å¸ƒå¯¹é½æ¨¡å—å’Œå‰æ™¯èƒŒæ™¯åˆ†ç¦»ç»†åŒ–æ¶æ„ã€‚å®éªŒè¯æ˜ï¼ŒCMaP-SAMåœ¨PASCAL-$5^i$å’ŒCOCO-$20^i$æ•°æ®é›†ä¸Šè¾¾åˆ°äº†æœ€ä½³æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Few-shot segmentation (FSS)æ—¨åœ¨ä½¿ç”¨å°‘é‡æ ‡æ³¨å›¾åƒè¿›è¡Œæ–°ç±»åˆ«åˆ†å‰²ã€‚</li>
<li>ç°æœ‰FSSæ–¹æ³•å­˜åœ¨ä¸¤ä¸ªå…³é”®é™åˆ¶ï¼šæœªå……åˆ†åˆ©ç”¨æŸ¥è¯¢å›¾åƒçš„ç»“æ„ç›¸å…³æ€§ï¼Œä»¥åŠåœ¨è½¬æ¢ä½ç½®å…ˆéªŒæ—¶ä¸¢å¤±ä¿¡æ¯ã€‚</li>
<li>CMaP-SAMæ¡†æ¶è¢«æå‡ºä»¥è§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œå¼•å…¥æ”¶ç¼©æ˜ å°„ç†è®ºä¼˜åŒ–ä½ç½®å…ˆéªŒä¿¡æ¯ã€‚</li>
<li>CMaP-SAMåŒ…æ‹¬ä¸‰ä¸ªå…³é”®ç»„ä»¶ï¼šæ”¶ç¼©æ˜ å°„æ¨¡å—ã€è‡ªé€‚åº”åˆ†å¸ƒå¯¹é½æ¨¡å—å’Œç»†åŒ–æ¶æ„ã€‚</li>
<li>æ”¶ç¼©æ˜ å°„æ¨¡å—é€šè¿‡å…¬å¼åŒ–ä½ç½®å…ˆéªŒä¼˜åŒ–ä¸ºBanachæ”¶ç¼©æ˜ å°„ï¼Œä¿è¯æ”¶æ•›æ€§ï¼Œå¹¶ç”Ÿæˆä¿ç•™è¯­ä¹‰æŒ‡å¯¼å’Œç»“æ„å…³è”çš„æ”¶æ•›å…ˆéªŒã€‚</li>
<li>è‡ªé€‚åº”åˆ†å¸ƒå¯¹é½æ¨¡å—è¿æ¥è¿ç»­å…ˆéªŒå’ŒSAMçš„äºŒå…ƒæ©è†œæç¤ºç¼–ç å™¨ã€‚</li>
<li>CMaP-SAMåœ¨PASCAL-$5^i$å’ŒCOCO-$20^i$æ•°æ®é›†ä¸Šå–å¾—äº†æœ€ä½³æ€§èƒ½ï¼Œè¾¾åˆ°71.1 mIoUå’Œ56.1çš„å‡†ç¡®ç‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.05049">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-462ee0da444badece2e314f29f5f541e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-7884afc5fb84d816b2ed0a269561f845.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-4277868fe11494a73ff8611d52e61d3c.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="GOTHAM-Graph-Class-Incremental-Learning-Framework-under-Weak-Supervision"><a href="#GOTHAM-Graph-Class-Incremental-Learning-Framework-under-Weak-Supervision" class="headerlink" title="GOTHAM: Graph Class Incremental Learning Framework under Weak   Supervision"></a>GOTHAM: Graph Class Incremental Learning Framework under Weak   Supervision</h2><p><strong>Authors:Aditya Hemant Shahane, Prathosh A. P, Sandeep Kumar</strong></p>
<p>Graphs are growing rapidly, along with the number of distinct label categories associated with them. Applications like e-commerce, healthcare, recommendation systems, and various social media platforms are rapidly moving towards graph representation of data due to their ability to capture both structural and attribute information. One crucial task in graph analysis is node classification, where unlabeled nodes are categorized into predefined classes. In practice, novel classes appear incrementally sometimes with just a few labels (seen classes) or even without any labels (unseen classes), either because they are new or havenâ€™t been explored much. Traditional methods assume abundant labeled data for training, which isnâ€™t always feasible. We investigate a broader objective: \emph{Graph Class Incremental Learning under Weak Supervision (GCL)}, addressing this challenge by meta-training on base classes with limited labeled instances. During the incremental streams, novel classes can have few-shot or zero-shot representation. Our proposed framework GOTHAM efficiently accommodates these unlabeled nodes by finding the closest prototype representation, serving as class representatives in the attribute space. For Text-Attributed Graphs (TAGs), our framework additionally incorporates semantic information to enhance the representation. By employing teacher-student knowledge distillation to mitigate forgetting, GOTHAM achieves promising results across various tasks. Experiments on datasets such as Cora-ML, Amazon, and OBGN-Arxiv showcase the effectiveness of our approach in handling evolving graph data under limited supervision. The repository is available here: \href{<a target="_blank" rel="noopener" href="https://github.com/adityashahane10/GOTHAM--Graph-based-Class-Incremental-Learning-Framework-under-Weak-Supervision%7D%7B/small">https://github.com/adityashahane10/GOTHAM--Graph-based-Class-Incremental-Learning-Framework-under-Weak-Supervision}{\small</a> \textcolor{blue}{Code}} </p>
<blockquote>
<p>å›¾è®ºæ­£éšç€ä¸å…¶ç›¸å…³çš„ä¸åŒæ ‡ç­¾ç±»åˆ«çš„æ•°é‡è€Œå¿«é€Ÿå‘å±•ã€‚ç”µå­å•†åŠ¡ã€åŒ»ç–—ä¿å¥ã€æ¨èç³»ç»Ÿä»¥åŠå„ç§ç¤¾äº¤åª’ä½“å¹³å°ç­‰åº”ç”¨ç¨‹åºï¼Œç”±äºèƒ½å¤Ÿæ•è·ç»“æ„å’Œå±æ€§ä¿¡æ¯ï¼Œæ­£è¿…é€Ÿå°†æ•°æ®è¡¨ç¤ºä¸ºå›¾ã€‚å›¾åˆ†æä¸­çš„ä¸€ä¸ªå…³é”®ä»»åŠ¡æ˜¯å¯¹æœªæ ‡è®°èŠ‚ç‚¹è¿›è¡Œåˆ†ç±»ï¼Œå°†å®ƒä»¬åˆ†ç±»åˆ°é¢„å®šä¹‰çš„ç±»åˆ«ä¸­ã€‚åœ¨å®è·µä¸­ï¼Œæ–°ç±»åˆ«æœ‰æ—¶ä¼šä»¥ä»…åŒ…å«å°‘é‡æ ‡ç­¾ï¼ˆå¯è§ç±»åˆ«ï¼‰ç”šè‡³æ²¡æœ‰ä»»ä½•æ ‡ç­¾ï¼ˆæœªè§ç±»åˆ«ï¼‰çš„å½¢å¼å‡ºç°ï¼Œè¿™å¯èƒ½æ˜¯å› ä¸ºå®ƒä»¬æ˜¯æ–°å‡ºç°çš„æˆ–å°šæœªè¢«å……åˆ†æ¢ç´¢ã€‚ä¼ ç»Ÿæ–¹æ³•å‡è®¾æœ‰å¤§é‡çš„æ ‡è®°æ•°æ®ç”¨äºè®­ç»ƒï¼Œä½†è¿™å¹¶ä¸æ€»æ˜¯å¯è¡Œçš„ã€‚æˆ‘ä»¬ç ”ç©¶äº†ä¸€ä¸ªæ›´å¹¿æ³›çš„ç›®æ ‡ï¼šåœ¨åŸºç¡€ç±»åˆ«æœ‰é™æ ‡è®°å®ä¾‹çš„å…ƒè®­ç»ƒä¸‹ï¼Œè§£å†³è¿™ä¸€æŒ‘æˆ˜çš„ã€Šå›¾ç±»å¢é‡å­¦ä¹ åœ¨å¼±ç›‘ç£ä¸‹çš„æŒ‘æˆ˜ï¼ˆGCLï¼‰ã€‹ã€‚åœ¨å¢é‡æµæœŸé—´ï¼Œæ–°ç±»åˆ«å¯ä»¥å…·æœ‰å°æ ·æœ¬æˆ–é›¶æ ·æœ¬è¡¨ç¤ºã€‚æˆ‘ä»¬æå‡ºçš„GOTHAMæ¡†æ¶é€šè¿‡æ‰¾åˆ°æœ€æ¥è¿‘çš„åŸå‹è¡¨ç¤ºæ¥æœ‰æ•ˆåœ°å®¹çº³è¿™äº›æœªæ ‡è®°çš„èŠ‚ç‚¹ï¼Œè¯¥åŸå‹è¡¨ç¤ºå¯ä½œä¸ºå±æ€§ç©ºé—´ä¸­çš„ç±»åˆ«ä»£è¡¨ã€‚å¯¹äºæ–‡æœ¬å±æ€§å›¾ï¼ˆTAGï¼‰ï¼Œæˆ‘ä»¬çš„æ¡†æ¶è¿˜ç»“åˆäº†è¯­ä¹‰ä¿¡æ¯ä»¥å¢å¼ºè¡¨ç¤ºã€‚é€šè¿‡é‡‡ç”¨æ•™å¸ˆ-å­¦ç”ŸçŸ¥è¯†è’¸é¦æ¥ç¼“è§£é—å¿˜ï¼ŒGOTHAMåœ¨å„ç§ä»»åŠ¡ä¸Šå–å¾—äº†æœ‰å‰æ™¯çš„ç»“æœã€‚åœ¨Cora-MLã€Amazonå’ŒOBGN-Arxivç­‰æ•°æ®é›†ä¸Šçš„å®éªŒå±•ç¤ºäº†æˆ‘ä»¬çš„æ–¹æ³•åœ¨æœ‰é™ç›‘ç£ä¸‹å¤„ç†ä¸æ–­å‘å±•çš„å›¾å½¢æ•°æ®çš„æœ‰æ•ˆæ€§ã€‚ç›¸å…³ä»“åº“è¯·ç‚¹å‡»è¿™é‡ŒæŸ¥çœ‹ï¼šã€ä»£ç ã€‘(<a target="_blank" rel="noopener" href="https://github.com/adityashahane10/GOTHAM--Graph-based-Class-Incremental-Learning-Framework-under-Weak-Supervision)/textcolor%7Bblue%7D%7B%F0%9F%94%97%7D%E3%80%82%EF%BC%88%E7%BD%91%E5%9D%80%E4%B8%AD%E7%9A%84%E4%B8%AD%E6%96%87%E5%B7%B2%E5%81%9A%E7%9B%B8%E5%BA%94%E8%B0%83%E6%95%B4%EF%BC%89">https://github.com/adityashahane10/GOTHAM--Graph-based-Class-Incremental-Learning-Framework-under-Weak-Supervision)\textcolor{blue}{ğŸ”—}ã€‚ï¼ˆç½‘å€ä¸­çš„ä¸­æ–‡å·²åšç›¸åº”è°ƒæ•´ï¼‰</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.04954v1">PDF</a> </p>
<p><strong>Summary</strong><br>     å›¾è®ºæ­£å¿«é€Ÿå‘å±•ï¼Œä¸åŒæ ‡ç­¾ç±»åˆ«çš„æ•°é‡ä¹Ÿæ€¥å‰§å¢é•¿ã€‚å› å›¾èƒ½æ•æ‰ç»“æ„æ€§å’Œå±æ€§ä¿¡æ¯ï¼Œç”µå­å•†åŠ¡ã€åŒ»ç–—ã€æ¨èç³»ç»Ÿä»¥åŠå„ç§ç¤¾äº¤åª’ä½“å¹³å°ç­‰åº”ç”¨çº·çº·é‡‡ç”¨æ•°æ®å›¾çš„è¡¨ç¤ºå½¢å¼ã€‚å›¾åˆ†æçš„ä¸€ä¸ªé‡è¦ä»»åŠ¡æ˜¯èŠ‚ç‚¹åˆ†ç±»ï¼Œå³æŠŠæœªæ ‡è®°çš„èŠ‚ç‚¹å½’ç±»åˆ°é¢„å®šä¹‰çš„ç±»åˆ«ä¸­ã€‚ç°å®åœºæ™¯ä¸­ï¼Œæ–°ç±»åˆ«å¾€å¾€ä»…æœ‰å‡ å¼ æ ‡ç­¾æˆ–è€…å®Œå…¨æ²¡æœ‰æ ‡ç­¾ï¼Œä¼ ç»Ÿæ–¹æ³•éœ€è¦å¤§é‡æ ‡è®°æ•°æ®è¿›è¡Œè®­ç»ƒï¼Œè¿™å¹¶ä¸æ€»æ˜¯å¯è¡Œã€‚æœ¬ç ”ç©¶æ¢è®¨äº†ä¸€ä¸ªæ›´å¹¿æ³›çš„ç›®æ ‡ï¼šå¼±ç›‘ç£ä¸‹çš„å›¾ç±»å¢é‡å­¦ä¹ ï¼ˆGCLï¼‰ï¼Œé€šè¿‡åœ¨åŸºç¡€ç±»åˆ«ä¸Šè¿›è¡Œå…ƒè®­ç»ƒæ¥è§£å†³è¿™ä¸€æŒ‘æˆ˜ï¼Œä¸”è®­ç»ƒæ ·æœ¬æœ‰é™ã€‚åœ¨å¢é‡æµä¸­ï¼Œæ–°ç±»åˆ«å¯å…·æœ‰å°æ ·æœ¬æˆ–é›¶æ ·æœ¬è¡¨ç¤ºã€‚æå‡ºçš„GOTHAMæ¡†æ¶èƒ½é«˜æ•ˆå®¹çº³è¿™äº›æœªæ ‡è®°èŠ‚ç‚¹ï¼Œé€šè¿‡å¯»æ‰¾æœ€æ¥è¿‘çš„åŸå‹è¡¨ç¤ºä½œä¸ºå±æ€§ç©ºé—´ä¸­çš„ç±»åˆ«ä»£è¡¨ã€‚å¯¹äºæ–‡æœ¬å±æ€§å›¾ï¼ˆTAGï¼‰ï¼Œè¯¥æ¡†æ¶è¿˜èå…¥äº†è¯­ä¹‰ä¿¡æ¯ä»¥å¢å¼ºè¡¨ç¤ºã€‚é€šè¿‡é‡‡ç”¨æ•™å¸ˆ-å­¦ç”ŸçŸ¥è¯†è’¸é¦æ¥ç¼“è§£é—å¿˜é—®é¢˜ï¼ŒGOTHAMåœ¨å„ç§ä»»åŠ¡ä¸Šå–å¾—äº†æœ‰å‰æ™¯çš„ç»“æœã€‚åœ¨Cora-MLã€Amazonå’ŒOBGN-Arxivç­‰æ•°æ®é›†ä¸Šçš„å®éªŒå±•ç¤ºäº†è¯¥æ–¹æ³•åœ¨å¤„ç†æœ‰é™ç›‘ç£ä¸‹çš„åŠ¨æ€å›¾æ•°æ®çš„æœ‰æ•ˆæ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å›¾è®ºåŠå…¶ç›¸å…³åº”ç”¨æ­£å¿«é€Ÿå‘å±•ï¼Œæ¶µç›–å¤šç§æ•°æ®ç±»å‹å’Œæ ‡ç­¾ç±»åˆ«ã€‚</li>
<li>ç°å®åœºæ™¯ä¸­ï¼Œæ–°çš„æ•°æ®ç±»åˆ«å¯èƒ½ä»…å¸¦æœ‰å°‘é‡æ ‡ç­¾æˆ–æ— æ ‡ç­¾ï¼Œä¼ ç»Ÿæ–¹æ³•å—é™ã€‚</li>
<li>ç ”ç©¶æå‡ºäº†å¼±ç›‘ç£ä¸‹çš„å›¾ç±»å¢é‡å­¦ä¹ ï¼ˆGCLï¼‰ç›®æ ‡æ¥åº”å¯¹è¿™ä¸€æŒ‘æˆ˜ã€‚</li>
<li>GOTHAMæ¡†æ¶é€šè¿‡å¯»æ‰¾åŸå‹è¡¨ç¤ºæ¥å®¹çº³æœªæ ‡è®°èŠ‚ç‚¹ï¼Œå¹¶åœ¨å±æ€§ç©ºé—´ä¸­ä½œä¸ºç±»åˆ«ä»£è¡¨ã€‚</li>
<li>GOTHAMæ¡†æ¶å¯¹äºæ–‡æœ¬å±æ€§å›¾èå…¥äº†è¯­ä¹‰ä¿¡æ¯ä»¥å¢å¼ºè¡¨ç¤ºã€‚</li>
<li>é‡‡ç”¨æ•™å¸ˆ-å­¦ç”ŸçŸ¥è¯†è’¸é¦æ¥å‡è½»é—å¿˜é—®é¢˜ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.04954">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-f29019baf5cd118ac2f83e81d562b7a9.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-dfa7195cb4a8a62814319c55d136842c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-96eef469525894e60ac5a09dc02daacc.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Video-Bench-Human-Aligned-Video-Generation-Benchmark"><a href="#Video-Bench-Human-Aligned-Video-Generation-Benchmark" class="headerlink" title="Video-Bench: Human-Aligned Video Generation Benchmark"></a>Video-Bench: Human-Aligned Video Generation Benchmark</h2><p><strong>Authors:Hui Han, Siyuan Li, Jiaqi Chen, Yiwen Yuan, Yuling Wu, Chak Tou Leong, Hanwen Du, Junchen Fu, Youhua Li, Jie Zhang, Chi Zhang, Li-jia Li, Yongxin Ni</strong></p>
<p>Video generation assessment is essential for ensuring that generative models produce visually realistic, high-quality videos while aligning with human expectations. Current video generation benchmarks fall into two main categories: traditional benchmarks, which use metrics and embeddings to evaluate generated video quality across multiple dimensions but often lack alignment with human judgments; and large language model (LLM)-based benchmarks, though capable of human-like reasoning, are constrained by a limited understanding of video quality metrics and cross-modal consistency. To address these challenges and establish a benchmark that better aligns with human preferences, this paper introduces Video-Bench, a comprehensive benchmark featuring a rich prompt suite and extensive evaluation dimensions. This benchmark represents the first attempt to systematically leverage MLLMs across all dimensions relevant to video generation assessment in generative models. By incorporating few-shot scoring and chain-of-query techniques, Video-Bench provides a structured, scalable approach to generated video evaluation. Experiments on advanced models including Sora demonstrate that Video-Bench achieves superior alignment with human preferences across all dimensions. Moreover, in instances where our frameworkâ€™s assessments diverge from human evaluations, it consistently offers more objective and accurate insights, suggesting an even greater potential advantage over traditional human judgment. </p>
<blockquote>
<p>è§†é¢‘ç”Ÿæˆè¯„ä¼°å¯¹äºç¡®ä¿ç”Ÿæˆæ¨¡å‹äº§ç”Ÿè§†è§‰çœŸå®ã€é«˜è´¨é‡çš„è§†é¢‘å¹¶ä¸”ç¬¦åˆäººç±»æœŸæœ›è‡³å…³é‡è¦ã€‚å½“å‰çš„è§†é¢‘ç”ŸæˆåŸºå‡†æµ‹è¯•ä¸»è¦åˆ†ä¸ºä¸¤å¤§ç±»ï¼šä¼ ç»ŸåŸºå‡†æµ‹è¯•ä½¿ç”¨æŒ‡æ ‡å’ŒåµŒå…¥æ¥è¯„ä¼°ç”Ÿæˆè§†é¢‘è´¨é‡å¤šä¸ªç»´åº¦ï¼Œä½†å¾€å¾€ä¸äººç±»åˆ¤æ–­ç¼ºä¹ä¸€è‡´æ€§ï¼›è€ŒåŸºäºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„åŸºå‡†æµ‹è¯•è™½ç„¶å…·å¤‡äººç±»æ¨ç†èƒ½åŠ›ï¼Œä½†å¯¹è§†é¢‘è´¨é‡æŒ‡æ ‡çš„è·¨æ¨¡æ€ä¸€è‡´æ€§ç†è§£æœ‰é™ã€‚ä¸ºäº†è§£å†³è¿™äº›æŒ‘æˆ˜å¹¶å»ºç«‹ä¸€ä¸ªæ›´å¥½åœ°ç¬¦åˆäººç±»åå¥½çš„åŸºå‡†æµ‹è¯•ï¼Œæœ¬æ–‡ä»‹ç»äº†Video-Benchï¼Œè¿™æ˜¯ä¸€ä¸ªæ‹¥æœ‰ä¸°å¯Œæç¤ºå¥—ä»¶å’Œå¹¿æ³›è¯„ä¼°ç»´åº¦çš„ç»¼åˆåŸºå‡†æµ‹è¯•ã€‚è¯¥åŸºå‡†æµ‹è¯•æ˜¯é¦–æ¬¡å°è¯•åœ¨ç”Ÿæˆæ¨¡å‹ä¸­æ¶‰åŠè§†é¢‘ç”Ÿæˆè¯„ä¼°çš„æ‰€æœ‰ç»´åº¦ä¸Šç³»ç»Ÿåœ°åˆ©ç”¨å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰ã€‚é€šè¿‡ç»“åˆå°‘æ ·æœ¬è¯„åˆ†å’ŒæŸ¥è¯¢é“¾æŠ€æœ¯ï¼ŒVideo-Benchä¸ºç”Ÿæˆçš„è§†é¢‘è¯„ä¼°æä¾›äº†ä¸€ç§ç»“æ„åŒ–ã€å¯æ‰©å±•çš„æ–¹æ³•ã€‚åœ¨åŒ…æ‹¬Soraçš„é«˜çº§æ¨¡å‹ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒVideo-Benchåœ¨æ‰€æœ‰ç»´åº¦ä¸Šå®ç°äº†ä¸äººç±»åå¥½æ›´ä¼˜è¶Šçš„å¯¹é½ã€‚æ­¤å¤–ï¼Œåœ¨æˆ‘ä»¬çš„æ¡†æ¶çš„è¯„ä¼°ä¸äººç±»è¯„ä¼°å‡ºç°åˆ†æ­§çš„æƒ…å†µä¸‹ï¼Œå®ƒå§‹ç»ˆæä¾›æ›´å®¢è§‚å’Œå‡†ç¡®çš„è§è§£ï¼Œè¿™è¡¨æ˜å®ƒæ¯”ä¼ ç»Ÿçš„åŸºäºäººç±»çš„åˆ¤æ–­å…·æœ‰æ›´å¤§çš„æ½œåœ¨ä¼˜åŠ¿ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.04907v1">PDF</a> Accepted by CVPRâ€™25</p>
<p><strong>Summary</strong><br>è§†é¢‘ç”Ÿæˆè¯„ä¼°å¯¹äºç¡®ä¿ç”Ÿæˆæ¨¡å‹ç”Ÿæˆè§†è§‰çœŸå®ã€é«˜è´¨é‡çš„è§†é¢‘è‡³å…³é‡è¦ï¼ŒåŒæ—¶ç¬¦åˆäººç±»æœŸæœ›ã€‚å½“å‰è§†é¢‘ç”ŸæˆåŸºå‡†æµ‹è¯•ä¸»è¦åˆ†ä¸ºä¸¤ç±»ï¼šä¼ ç»ŸåŸºå‡†æµ‹è¯•ä½¿ç”¨æŒ‡æ ‡å’ŒåµŒå…¥æ¥è¯„ä¼°ç”Ÿæˆè§†é¢‘è´¨é‡ï¼Œä½†å¾€å¾€ç¼ºä¹ä¸äººç±»åˆ¤æ–­çš„å¯¹é½ï¼›è€ŒåŸºäºå¤§å‹è¯­è¨€æ¨¡å‹çš„åŸºå‡†æµ‹è¯•è™½ç„¶èƒ½å¤Ÿè¿›è¡Œäººç±»æ¨ç†ï¼Œä½†å¯¹è§†é¢‘è´¨é‡æŒ‡æ ‡å’Œè·¨æ¨¡æ€ä¸€è‡´æ€§çš„ç†è§£æœ‰é™ã€‚ä¸ºè§£å†³è¿™äº›æŒ‘æˆ˜å¹¶å»ºç«‹ä¸€ä¸ªæ›´ç¬¦åˆäººç±»åå¥½çš„åŸºå‡†æµ‹è¯•ï¼Œæœ¬æ–‡ä»‹ç»äº†Video-Benchï¼Œè¿™æ˜¯ä¸€ä¸ªåŒ…å«ä¸°å¯Œæç¤ºå¥—ä»¶å’Œå¹¿æ³›è¯„ä¼°ç»´åº¦çš„ç»¼åˆåŸºå‡†æµ‹è¯•ã€‚å®ƒé¦–æ¬¡å°è¯•åœ¨è§†é¢‘ç”Ÿæˆè¯„ä¼°çš„æ‰€æœ‰ç›¸å…³ç»´åº¦ä¸Šç³»ç»Ÿåœ°åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ã€‚é€šè¿‡ç»“åˆå°‘æ ·æœ¬è¯„åˆ†å’Œé“¾æŸ¥è¯¢æŠ€æœ¯ï¼ŒVideo-Benchä¸ºç”Ÿæˆçš„è§†é¢‘æä¾›äº†ä¸€ç§ç»“æ„åŒ–ã€å¯æ‰©å±•çš„è¯„ä¼°æ–¹æ³•ã€‚åœ¨é«˜çº§æ¨¡å‹ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒVideo-Benchåœ¨æ‰€æœ‰ç»´åº¦ä¸Šå®ç°äº†ä¸äººç±»åå¥½çš„ä¼˜è¶Šå¯¹é½ã€‚æ­¤å¤–ï¼Œåœ¨æˆ‘ä»¬æ¡†æ¶çš„è¯„ä¼°ä¸äººç±»è¯„ä¼°å­˜åœ¨åˆ†æ­§çš„æƒ…å†µä¸‹ï¼Œå®ƒå§‹ç»ˆæä¾›æ›´å®¢è§‚å’Œå‡†ç¡®çš„è§è§£ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li><p>Video generation assessment is essential for generative models to produce visually realistic, high-quality videos.<br>è§†é¢‘ç”Ÿæˆè¯„ä¼°å¯¹äºç”Ÿæˆæ¨¡å‹äº§ç”Ÿè§†è§‰çœŸå®ã€é«˜è´¨é‡çš„è§†é¢‘è‡³å…³é‡è¦ã€‚</p>
</li>
<li><p>Current video generation benchmarks have limitations, lacking alignment with human judgments.<br>å½“å‰è§†é¢‘ç”Ÿæˆçš„åŸºå‡†æµ‹è¯•å­˜åœ¨å±€é™æ€§ï¼Œä¸äººç±»åˆ¤æ–­çš„å¯¹é½ç¨‹åº¦ä¸å¤Ÿã€‚</p>
</li>
<li><p>Video-Bench is introduced as a comprehensive benchmark for video generation assessment, featuring a rich prompt suite and extensive evaluation dimensions.<br>Video-Benchè¢«å¼•å…¥ä½œä¸ºè§†é¢‘ç”Ÿæˆè¯„ä¼°çš„ç»¼åˆåŸºå‡†æµ‹è¯•ï¼Œå…·æœ‰ä¸°å¯Œçš„æç¤ºå¥—ä»¶å’Œå¹¿æ³›çš„è¯„ä¼°ç»´åº¦ã€‚</p>
</li>
<li><p>Video-Bench leverages MLLMs (å¤§å‹è¯­è¨€æ¨¡å‹) across all dimensions of video generation assessment, marking the first attempt of this kind.<br>Video-Benchåœ¨è§†é¢‘ç”Ÿæˆè¯„ä¼°çš„æ‰€æœ‰ç»´åº¦ä¸Šéƒ½åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼Œè¿™æ˜¯ç¬¬ä¸€æ¬¡å°è¯•è¿™æ ·åšã€‚</p>
</li>
<li><p>Video-Bench incorporates few-shot scoring and chain-of-query techniques, providing a structured and scalable approach to generated video evaluation.<br>Video-Benchç»“åˆäº†å°‘æ ·æœ¬è¯„åˆ†å’Œé“¾æŸ¥è¯¢æŠ€æœ¯ï¼Œä¸ºç”Ÿæˆçš„è§†é¢‘è¯„ä»·æä¾›äº†ä¸€ç§ç»“æ„åŒ–å’Œå¯æ‰©å±•çš„æ–¹æ³•ã€‚</p>
</li>
<li><p>Experiments demonstrate that Video-Bench achieves superior alignment with human preferences across all dimensions compared to traditional benchmarks.<br>å®éªŒè¡¨æ˜ï¼Œä¸ä¼ ç»Ÿçš„åŸºå‡†æµ‹è¯•ç›¸æ¯”ï¼ŒVideo-Benchåœ¨æ‰€æœ‰ç»´åº¦ä¸Šéƒ½å®ç°äº†ä¸äººç±»åå¥½çš„ä¼˜è¶Šå¯¹é½ã€‚</p>
</li>
<li><p>When there are divergences between Video-Bench assessments and human evaluations, Video-Bench offers more objective and accurate insights.</p>
</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.04907">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-3f90c1d988fed8e8c35d9ce3b0154cfb.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-3e300daea1caa72404c324611dd0a5c8.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-82f277eb502f125db24353b45ec8ab5e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-aa4297b55e458a34d2c5858076f2106f.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Leveraging-Large-Language-Models-for-Cost-Effective-Multilingual-Depression-Detection-and-Severity-Assessment"><a href="#Leveraging-Large-Language-Models-for-Cost-Effective-Multilingual-Depression-Detection-and-Severity-Assessment" class="headerlink" title="Leveraging Large Language Models for Cost-Effective, Multilingual   Depression Detection and Severity Assessment"></a>Leveraging Large Language Models for Cost-Effective, Multilingual   Depression Detection and Severity Assessment</h2><p><strong>Authors:Longdi Xian, Jianzhang Ni, Mingzhu Wang</strong></p>
<p>Depression is a prevalent mental health disorder that is difficult to detect early due to subjective symptom assessments. Recent advancements in large language models have offered efficient and cost-effective approaches for this objective. In this study, we evaluated the performance of four LLMs in depression detection using clinical interview data. We selected the best performing model and further tested it in the severity evaluation scenario and knowledge enhanced scenario. The robustness was evaluated in complex diagnostic scenarios using a dataset comprising 51074 statements from six different mental disorders. We found that DeepSeek V3 is the most reliable and cost-effective model for depression detection, performing well in both zero-shot and few-shot scenarios, with zero-shot being the most efficient choice. The evaluation of severity showed low agreement with the human evaluator, particularly for mild depression. The model maintains stably high AUCs for detecting depression in complex diagnostic scenarios. These findings highlight DeepSeek V3s strong potential for text-based depression detection in real-world clinical applications. However, they also underscore the need for further refinement in severity assessment and the mitigation of potential biases to enhance clinical reliability. </p>
<blockquote>
<p>æŠ‘éƒç—‡æ˜¯ä¸€ç§å¸¸è§çš„å¿ƒç†å¥åº·ç–¾ç—…ï¼Œç”±äºä¸»è§‚ç—‡çŠ¶è¯„ä¼°ï¼Œæ—©æœŸå¾ˆéš¾å‘ç°ã€‚æœ€è¿‘å¤§å‹è¯­è¨€æ¨¡å‹çš„è¿›æ­¥ä¸ºè¿™ä¸€ç›®æ ‡æä¾›äº†é«˜æ•ˆä¸”ä½æˆæœ¬çš„æ–¹æ³•ã€‚åœ¨è¿™é¡¹ç ”ç©¶ä¸­ï¼Œæˆ‘ä»¬ä½¿ç”¨ä¸´åºŠè®¿è°ˆæ•°æ®è¯„ä¼°äº†å››ç§å¤§å‹è¯­è¨€æ¨¡å‹åœ¨æŠ‘éƒç—‡æ£€æµ‹æ–¹é¢çš„æ€§èƒ½ã€‚æˆ‘ä»¬é€‰æ‹©äº†è¡¨ç°æœ€ä½³çš„æ¨¡å‹ï¼Œå¹¶åœ¨ä¸¥é‡ç¨‹åº¦è¯„ä¼°å’ŒçŸ¥è¯†å¢å¼ºåœºæ™¯ä¸­è¿›è¡Œäº†è¿›ä¸€æ­¥æµ‹è¯•ã€‚é€šè¿‡åŒ…å«æ¥è‡ªå…­ç§ä¸åŒç²¾ç¥ç–¾ç—…çš„51074ä¸ªé™ˆè¿°çš„æ•°æ®é›†ï¼Œå¯¹å¤æ‚è¯Šæ–­åœºæ™¯ä¸­çš„ç¨³å¥æ€§è¿›è¡Œäº†è¯„ä¼°ã€‚æˆ‘ä»¬å‘ç°DeepSeek V3åœ¨æŠ‘éƒç—‡æ£€æµ‹æ–¹é¢æ˜¯æœ€å¯é ä¸”æˆæœ¬æ•ˆç›Šæœ€é«˜çš„æ¨¡å‹ï¼Œåœ¨é›¶æ ·æœ¬å’Œå°‘æ ·æœ¬åœºæ™¯ä¸­è¡¨ç°è‰¯å¥½ï¼Œå…¶ä¸­é›¶æ ·æœ¬æ˜¯æœ€æœ‰æ•ˆç‡çš„é€‰æ‹©ã€‚ä¸¥é‡ç¨‹åº¦çš„è¯„ä¼°æ˜¾ç¤ºä¸äººç±»è¯„ä¼°è€…çš„æ„è§ä¸€è‡´æ€§è¾ƒä½ï¼Œå°¤å…¶æ˜¯è½»åº¦æŠ‘éƒã€‚è¯¥æ¨¡å‹åœ¨å¤æ‚è¯Šæ–­åœºæ™¯ä¸­ä¿æŒè¾ƒé«˜çš„AUCå€¼ï¼Œç”¨äºæ£€æµ‹æŠ‘éƒç—‡ã€‚è¿™äº›å‘ç°çªå‡ºäº†DeepSeek V3åœ¨ç°å®ä¸–ç•Œä¸´åºŠåº”ç”¨ä¸­çš„å¼ºå¤§æ½œåŠ›ï¼Œç”¨äºåŸºäºæ–‡æœ¬çš„æŠ‘éƒç—‡æ£€æµ‹ã€‚ç„¶è€Œï¼Œå®ƒä»¬ä¹Ÿå¼ºè°ƒäº†æœ‰å¿…è¦æ”¹è¿›ä¸¥é‡ç¨‹åº¦çš„è¯„ä¼°ï¼Œå¹¶å‡è½»æ½œåœ¨åè§ï¼Œä»¥æé«˜ä¸´åºŠå¯é æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.04891v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹åœ¨æŠ‘éƒç—‡æ£€æµ‹æ–¹é¢çš„åº”ç”¨å–å¾—äº†æ˜¾è‘—è¿›å±•ã€‚æœ¬ç ”ç©¶è¯„ä¼°äº†å››ç§LLMsåœ¨æŠ‘éƒç—‡æ£€æµ‹ä¸­çš„æ€§èƒ½ï¼Œå¹¶å‘ç°DeepSeek V3æ¨¡å‹åœ¨é›¶æ ·æœ¬å’Œå°‘æ ·æœ¬åœºæ™¯ä¸‹è¡¨ç°æœ€å¯é ä¸”æˆæœ¬æ•ˆç›Šæœ€é«˜ã€‚ç„¶è€Œï¼Œåœ¨è¯„ä¼°ä¸¥é‡ç¨‹åº¦æ–¹é¢ä»éœ€è¿›ä¸€æ­¥æ”¹è¿›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹å·²ç”¨äºæŠ‘éƒç—‡æ£€æµ‹ï¼Œæä¾›æœ‰æ•ˆä¸”ç»æµçš„è§£å†³æ–¹æ¡ˆã€‚</li>
<li>æœ¬ç ”ç©¶è¯„ä¼°äº†å››ç§LLMsåœ¨æŠ‘éƒç—‡æ£€æµ‹ä¸­çš„æ€§èƒ½ã€‚</li>
<li>DeepSeek V3æ¨¡å‹åœ¨æŠ‘éƒç—‡æ£€æµ‹æ–¹é¢è¡¨ç°æœ€ä½³ï¼Œé€‚ç”¨äºé›¶æ ·æœ¬å’Œå°‘æ ·æœ¬åœºæ™¯ã€‚</li>
<li>DeepSeek V3åœ¨å¤æ‚è¯Šæ–­æƒ…å¢ƒä¸­çš„è¡¨ç°ç¨³å®šä¸”å¯é ã€‚</li>
<li>æŠ‘éƒç—‡çš„ä¸¥é‡ç¨‹åº¦è¯„ä¼°ä¸äººå·¥è¯„ä»·è€…ä¹‹é—´å­˜åœ¨ä½ä¸€è‡´æ€§ï¼Œå°¤å…¶æ˜¯è½»åº¦æŠ‘éƒç—‡ã€‚</li>
<li>éœ€è¦è¿›ä¸€æ­¥æ”¹è¿›ä¸¥é‡ç¨‹åº¦è¯„ä¼°ï¼Œä»¥æé«˜ä¸´åºŠå¯é æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.04891">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-912d75a8d6f7ac5f14ac22d79d6f8c21.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-21ce1bc4150a3b5dd7ba0a778bb1dba6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4eaa78cc64350cd972397ff08404e9e8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a6ae5b1cb51bbf30bcbc136e003147d1.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Planning-Safety-Trajectories-with-Dual-Phase-Physics-Informed-and-Transportation-Knowledge-Driven-Large-Language-Models"><a href="#Planning-Safety-Trajectories-with-Dual-Phase-Physics-Informed-and-Transportation-Knowledge-Driven-Large-Language-Models" class="headerlink" title="Planning Safety Trajectories with Dual-Phase, Physics-Informed, and   Transportation Knowledge-Driven Large Language Models"></a>Planning Safety Trajectories with Dual-Phase, Physics-Informed, and   Transportation Knowledge-Driven Large Language Models</h2><p><strong>Authors:Rui Gan, Pei Li, Keke Long, Bocheng An, Junwei You, Keshu Wu, Bin Ran</strong></p>
<p>Foundation models have demonstrated strong reasoning and generalization capabilities in driving-related tasks, including scene understanding, planning, and control. However, they still face challenges in hallucinations, uncertainty, and long inference latency. While existing foundation models have general knowledge of avoiding collisions, they often lack transportation-specific safety knowledge. To overcome these limitations, we introduce LetsPi, a physics-informed, dual-phase, knowledge-driven framework for safe, human-like trajectory planning. To prevent hallucinations and minimize uncertainty, this hybrid framework integrates Large Language Model (LLM) reasoning with physics-informed social force dynamics. LetsPi leverages the LLM to analyze driving scenes and historical information, providing appropriate parameters and target destinations (goals) for the social force model, which then generates the future trajectory. Moreover, the dual-phase architecture balances reasoning and computational efficiency through its Memory Collection phase and Fast Inference phase. The Memory Collection phase leverages the physics-informed LLM to process and refine planning results through reasoning, reflection, and memory modules, storing safe, high-quality driving experiences in a memory bank. Surrogate safety measures and physics-informed prompt techniques are introduced to enhance the LLMâ€™s knowledge of transportation safety and physical force, respectively. The Fast Inference phase extracts similar driving experiences as few-shot examples for new scenarios, while simplifying input-output requirements to enable rapid trajectory planning without compromising safety. Extensive experiments using the HighD dataset demonstrate that LetsPi outperforms baseline models across five safety metrics.See PDF for project Github link. </p>
<blockquote>
<p>é¢„è®­ç»ƒæ¨¡å‹åœ¨é©¾é©¶ç›¸å…³ä»»åŠ¡ä¸­è¡¨ç°å‡ºäº†å¼ºå¤§çš„æ¨ç†å’Œæ³›åŒ–èƒ½åŠ›ï¼ŒåŒ…æ‹¬åœºæ™¯ç†è§£ã€è§„åˆ’å’Œæ§åˆ¶ã€‚ç„¶è€Œï¼Œå®ƒä»¬ä»é¢ä¸´ç€å¹»è§‰ã€ä¸ç¡®å®šæ€§å’Œé•¿æ¨ç†å»¶è¿Ÿç­‰æŒ‘æˆ˜ã€‚å°½ç®¡ç°æœ‰çš„é¢„è®­ç»ƒæ¨¡å‹å…·å¤‡é¿å…ç¢°æ’çš„ä¸€èˆ¬çŸ¥è¯†ï¼Œä½†å®ƒä»¬å¾€å¾€ç¼ºä¹ç‰¹å®šçš„äº¤é€šå®‰å…¨çŸ¥è¯†ã€‚ä¸ºäº†å…‹æœè¿™äº›å±€é™æ€§ï¼Œæˆ‘ä»¬å¼•å…¥äº†LetsPiï¼Œè¿™æ˜¯ä¸€ä¸ªç‰©ç†ä¿¡æ¯åŒé˜¶æ®µçŸ¥è¯†é©±åŠ¨çš„å®‰å…¨ã€äººæ€§åŒ–çš„è½¨è¿¹è§„åˆ’æ¡†æ¶ã€‚è¯¥æ··åˆæ¡†æ¶æ—¨åœ¨é˜²æ­¢å¹»è§‰å¹¶æœ€å°åŒ–ä¸ç¡®å®šæ€§ï¼Œé›†æˆäº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ¨ç†å’Œç‰©ç†ä¿¡æ¯ç¤¾ä¼šåŠ¨åŠ›å­¦ã€‚LetsPiåˆ©ç”¨LLMåˆ†æé©¾é©¶åœºæ™¯å’Œå†å²ä¿¡æ¯ï¼Œä¸ºç¤¾ä¼šåŠ›æ¨¡å‹æä¾›é€‚å½“çš„å‚æ•°å’Œç›®æ ‡ç›®çš„åœ°ï¼ˆç›®æ ‡ï¼‰ï¼Œç„¶åç”Ÿæˆæœªæ¥è½¨è¿¹ã€‚æ­¤å¤–ï¼ŒåŒé˜¶æ®µæ¶æ„é€šè¿‡å…¶è®°å¿†æ”¶é›†é˜¶æ®µå’Œå¿«é€Ÿæ¨ç†é˜¶æ®µæ¥å¹³è¡¡æ¨ç†å’Œè®¡ç®—æ•ˆç‡ã€‚è®°å¿†æ”¶é›†é˜¶æ®µåˆ©ç”¨ç‰©ç†ä¿¡æ¯LLMé€šè¿‡æ¨ç†ã€åæ€å’Œè®°å¿†æ¨¡å—å¤„ç†å’Œä¼˜åŒ–è§„åˆ’ç»“æœï¼Œå°†å®‰å…¨ã€é«˜è´¨é‡çš„é©¾é©¶ç»éªŒå­˜å‚¨åœ¨è®°å¿†åº“ä¸­ã€‚å¼•å…¥äº†ä»£ç†å®‰å…¨æªæ–½å’Œç‰©ç†ä¿¡æ¯æç¤ºæŠ€æœ¯ï¼Œä»¥å¢å¼ºLLMå¯¹äº¤é€šè¿è¾“å®‰å…¨å’Œç‰©ç†åŠ›çš„äº†è§£ã€‚å¿«é€Ÿæ¨ç†é˜¶æ®µæå–ç±»ä¼¼é©¾é©¶ç»éªŒä½œä¸ºæ–°åœºæ™¯çš„å°‘é‡ç¤ºä¾‹ï¼Œç®€åŒ–è¾“å…¥è¾“å‡ºè¦æ±‚ï¼Œä»¥å®ç°å¿«é€Ÿè½¨è¿¹è§„åˆ’è€Œä¸å½±å“å®‰å…¨ã€‚ä½¿ç”¨HighDæ•°æ®é›†è¿›è¡Œçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼ŒLetsPiåœ¨äº”é¡¹å®‰å…¨æŒ‡æ ‡ä¸Šçš„è¡¨ç°å‡ä¼˜äºåŸºå‡†æ¨¡å‹ã€‚æœ‰å…³é¡¹ç›®Githubé“¾æ¥ï¼Œè¯·å‚é˜…PDFæ–‡ä»¶ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.04562v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>åŸºäºç°æœ‰æ¨¡å‹åœ¨é©¾é©¶ç›¸å…³ä»»åŠ¡ä¸­çš„å¼ºå¤§æ¨ç†å’Œæ³›åŒ–èƒ½åŠ›ï¼Œä½†ä»å­˜åœ¨å¹»è§‰ã€ä¸ç¡®å®šæ€§å’Œé•¿æ¨ç†å»¶è¿Ÿç­‰æŒ‘æˆ˜ã€‚ä¸ºè§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†LetsPiï¼Œä¸€ä¸ªç»“åˆç‰©ç†ä¿¡æ¯å’Œç¤¾ä¼šåŠ›é‡åŠ¨æ€çŸ¥è¯†çš„åŒé˜¶æ®µæ¡†æ¶ï¼Œç”¨äºå®‰å…¨ã€äººæ€§åŒ–çš„è½¨è¿¹è§„åˆ’ã€‚è¯¥æ¡†æ¶ç»“åˆäº†å¤§å‹è¯­è¨€æ¨¡å‹çš„æ¨ç†èƒ½åŠ›å’Œç‰©ç†ä¿¡æ¯çš„ç¤¾ä¼šåŠ›é‡åŠ¨æ€ï¼Œä»¥é¢„é˜²å¹»è§‰å¹¶å‡å°‘ä¸ç¡®å®šæ€§ã€‚å®ƒé€šè¿‡å­˜å‚¨å®‰å…¨é«˜è´¨é‡çš„é©¾é©¶ç»éªŒæ¥å¹³è¡¡æ¨ç†å’Œè®¡ç®—æ•ˆç‡ã€‚é€šè¿‡å¤§é‡çš„å®éªŒéªŒè¯ï¼ŒLetsPiåœ¨äº”ä¸ªå®‰å…¨æŒ‡æ ‡ä¸Šå‡ä¼˜äºåŸºå‡†æ¨¡å‹ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç°æœ‰æ¨¡å‹è™½åœ¨é©¾é©¶ä»»åŠ¡ä¸­æœ‰å¼ºæ¨ç†å’Œæ³›åŒ–èƒ½åŠ›ï¼Œä½†ä»å­˜åœ¨å¹»è§‰ã€ä¸ç¡®å®šæ€§å’Œæ¨ç†å»¶è¿Ÿé—®é¢˜ã€‚</li>
<li>LetsPiæ¡†æ¶ç»“åˆäº†å¤§å‹è¯­è¨€æ¨¡å‹å’Œç‰©ç†ä¿¡æ¯çš„ç¤¾ä¼šåŠ›é‡åŠ¨æ€ï¼Œé¢„é˜²å¹»è§‰å¹¶å‡å°‘ä¸ç¡®å®šæ€§ã€‚</li>
<li>LetsPié€šè¿‡åŒé˜¶æ®µæ¶æ„å¹³è¡¡æ¨ç†å’Œè®¡ç®—æ•ˆç‡ï¼ŒåŒ…æ‹¬å­˜å‚¨å®‰å…¨é©¾é©¶ç»éªŒçš„è®°å¿†æ”¶é›†é˜¶æ®µå’Œå¿«é€Ÿæ¨ç†é˜¶æ®µã€‚</li>
<li>è®°å¿†æ”¶é›†é˜¶æ®µåˆ©ç”¨ç‰©ç†ä¿¡æ¯çš„å¤§å‹è¯­è¨€æ¨¡å‹å¤„ç†è§„åˆ’ç»“æœï¼Œé€šè¿‡æ¨ç†ã€åæ€å’Œè®°å¿†æ¨¡å—è¿›è¡Œç²¾ç»†åŒ–ã€‚</li>
<li>å¼•å…¥æ›¿èº«å®‰å…¨æªæ–½å’Œç‰©ç†ä¿¡æ¯æç¤ºæŠ€æœ¯ï¼Œå¢å¼ºå¤§å‹è¯­è¨€æ¨¡å‹å¯¹è¿è¾“å®‰å…¨å’Œç‰©ç†åŠ›çš„äº†è§£ã€‚</li>
<li>å¿«é€Ÿæ¨ç†é˜¶æ®µæå–ç±»ä¼¼é©¾é©¶ç»éªŒä½œä¸ºå°‘æ•°æ¡ˆä¾‹ï¼Œç®€åŒ–è¾“å…¥-è¾“å‡ºè¦æ±‚ï¼Œå®ç°å¿«é€Ÿè½¨è¿¹è§„åˆ’è€Œä¸å½±å“å®‰å…¨æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.04562">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-0085073033d8d0c520ee785c1f53fc6c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e4a25d2e9612a6110961bd51cb9e57c2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e70d66a9e650be4138e9611fc8f971f6.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-af944dbf25588af74b6174ec1253bbc1.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Enhance-Then-Search-An-Augmentation-Search-Strategy-with-Foundation-Models-for-Cross-Domain-Few-Shot-Object-Detection"><a href="#Enhance-Then-Search-An-Augmentation-Search-Strategy-with-Foundation-Models-for-Cross-Domain-Few-Shot-Object-Detection" class="headerlink" title="Enhance Then Search: An Augmentation-Search Strategy with Foundation   Models for Cross-Domain Few-Shot Object Detection"></a>Enhance Then Search: An Augmentation-Search Strategy with Foundation   Models for Cross-Domain Few-Shot Object Detection</h2><p><strong>Authors:Jiancheng Pan, Yanxing Liu, Xiao He, Long Peng, Jiahao Li, Yuze Sun, Xiaomeng Huang</strong></p>
<p>Foundation models pretrained on extensive datasets, such as GroundingDINO and LAE-DINO, have performed remarkably in the cross-domain few-shot object detection (CD-FSOD) task. Through rigorous few-shot training, we found that the integration of image-based data augmentation techniques and grid-based sub-domain search strategy significantly enhances the performance of these foundation models. Building upon GroundingDINO, we employed several widely used image augmentation methods and established optimization objectives to effectively navigate the expansive domain space in search of optimal sub-domains. This approach facilitates efficient few-shot object detection and introduces an approach to solving the CD-FSOD problem by efficiently searching for the optimal parameter configuration from the foundation model. Our findings substantially advance the practical deployment of vision-language models in data-scarce environments, offering critical insights into optimizing their cross-domain generalization capabilities without labor-intensive retraining. Code is available at <a target="_blank" rel="noopener" href="https://github.com/jaychempan/ETS">https://github.com/jaychempan/ETS</a>. </p>
<blockquote>
<p>åœ¨å¤§é‡æ•°æ®é›†ä¸Šé¢„è®­ç»ƒçš„æ¨¡å‹ï¼Œå¦‚GroundingDINOå’ŒLAE-DINOï¼Œåœ¨è·¨åŸŸå°‘æ ·æœ¬ç›®æ ‡æ£€æµ‹ï¼ˆCD-FSODï¼‰ä»»åŠ¡ä¸­è¡¨ç°çªå‡ºã€‚é€šè¿‡ä¸¥æ ¼çš„å°‘æ ·æœ¬è®­ç»ƒï¼Œæˆ‘ä»¬å‘ç°ç»“åˆåŸºäºå›¾åƒçš„æ•°æ®å¢å¼ºæŠ€æœ¯å’ŒåŸºäºç½‘æ ¼çš„å­åŸŸæœç´¢ç­–ç•¥ï¼Œå¯ä»¥æ˜¾è‘—æé«˜è¿™äº›åŸºç¡€æ¨¡å‹çš„æ€§èƒ½ã€‚åœ¨GroundingDINOçš„åŸºç¡€ä¸Šï¼Œæˆ‘ä»¬é‡‡ç”¨äº†å‡ ç§å¹¿æ³›ä½¿ç”¨çš„å›¾åƒå¢å¼ºæ–¹æ³•ï¼Œå¹¶å»ºç«‹äº†ä¼˜åŒ–ç›®æ ‡ï¼Œä»¥æœ‰æ•ˆåœ°åœ¨åºå¤§çš„åŸŸç©ºé—´ä¸­æœç´¢æœ€ä¼˜å­åŸŸã€‚è¿™ç§æ–¹æ³•ä¿ƒè¿›äº†é«˜æ•ˆå°‘æ ·æœ¬ç›®æ ‡æ£€æµ‹ï¼Œå¹¶é€šè¿‡ä»åŸºç¡€æ¨¡å‹ä¸­æœ‰æ•ˆæœç´¢æœ€ä¼˜å‚æ•°é…ç½®æ¥è§£å†³CD-FSODé—®é¢˜ã€‚æˆ‘ä»¬çš„ç ”ç©¶ä¸ºåœ¨æ•°æ®ç¨€ç¼ºç¯å¢ƒä¸­å®é™…éƒ¨ç½²è§†è§‰è¯­è¨€æ¨¡å‹æä¾›äº†å®è´¨æ€§è¿›å±•ï¼Œå¹¶ä¸ºä¼˜åŒ–å…¶è·¨åŸŸæ³›åŒ–èƒ½åŠ›æä¾›äº†å…³é”®è§è§£ï¼Œæ— éœ€åŠ³åŠ¨å¯†é›†å‹çš„é‡æ–°è®­ç»ƒã€‚ä»£ç å¯é€šè¿‡<a target="_blank" rel="noopener" href="https://github.com/jaychempan/ETS%E8%AE%BF%E9%97%AE%E3%80%82">https://github.com/jaychempan/ETSè®¿é—®ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.04517v1">PDF</a> 9 pages, 6 figures</p>
<p><strong>Summary</strong></p>
<p>åŸºäºGroundingDINOå’ŒLAE-DINOç­‰é¢„è®­ç»ƒæ•°æ®é›†ï¼Œé€šè¿‡å›¾åƒæ•°æ®å¢å¼ºæŠ€æœ¯å’Œç½‘æ ¼å­åŸŸæœç´¢ç­–ç•¥çš„ç»“åˆï¼Œæ˜¾è‘—æå‡äº†è·¨åŸŸå°æ ·æœ¬ç›®æ ‡æ£€æµ‹ä»»åŠ¡çš„æ€§èƒ½ã€‚è¯¥ç ”ç©¶ä¸ºä¼˜åŒ–è·¨åŸŸé€šç”¨åŒ–èƒ½åŠ›æä¾›äº†é‡è¦è§è§£ï¼Œé€‚ç”¨äºæ•°æ®ç¨€ç¼ºç¯å¢ƒä¸­çš„è§†è§‰è¯­è¨€æ¨¡å‹å®é™…åº”ç”¨ã€‚ä»£ç å¯åœ¨æŒ‡å®šé“¾æ¥ä¸‹è½½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>é¢„è®­ç»ƒæ•°æ®é›†å¦‚GroundingDINOå’ŒLAE-DINOåœ¨è·¨åŸŸå°æ ·æœ¬ç›®æ ‡æ£€æµ‹ä»»åŠ¡ä¸­è¡¨ç°å“è¶Šã€‚</li>
<li>å›¾åƒæ•°æ®å¢å¼ºæŠ€æœ¯æ˜¾è‘—æå‡æ¨¡å‹æ€§èƒ½ã€‚</li>
<li>ç»“åˆç½‘æ ¼å­åŸŸæœç´¢ç­–ç•¥å®ç°æ›´æœ‰æ•ˆçš„æ¨¡å‹ä¼˜åŒ–ã€‚</li>
<li>ä¼˜åŒ–çš„æ¨¡å‹å…·å¤‡æ›´ä½³çš„è·¨åŸŸé€šç”¨åŒ–èƒ½åŠ›ã€‚</li>
<li>æ¨¡å‹æ— éœ€å¤§è§„æ¨¡é‡æ–°è®­ç»ƒï¼Œæé«˜äº†å®ç”¨æ€§ã€‚</li>
<li>è¯¥ç ”ç©¶ä¸ºæ•°æ®ç¨€ç¼ºç¯å¢ƒä¸‹çš„è§†è§‰è¯­è¨€æ¨¡å‹åº”ç”¨æä¾›äº†é‡è¦æŒ‡å¯¼ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.04517">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-8c564e6d6460ed39ed6dc4ae1d09760e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-abb71328397a8a6629b09d0f5deca41d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2b97486cbb59d8ed4ff67480f97f9709.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-b6681ab63f960004e9c81babead169da.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-448d085c3070c8f578c770982352a618.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6652620b7293f64921df0d7bc485518e.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Pre-trained-Language-Models-and-Few-shot-Learning-for-Medical-Entity-Extraction"><a href="#Pre-trained-Language-Models-and-Few-shot-Learning-for-Medical-Entity-Extraction" class="headerlink" title="Pre-trained Language Models and Few-shot Learning for Medical Entity   Extraction"></a>Pre-trained Language Models and Few-shot Learning for Medical Entity   Extraction</h2><p><strong>Authors:Xiaokai Wang, Guiran Liu, Binrong Zhu, Jacky He, Hongye Zheng, Hanlu Zhang</strong></p>
<p>This study proposes a medical entity extraction method based on Transformer to enhance the information extraction capability of medical literature. Considering the professionalism and complexity of medical texts, we compare the performance of different pre-trained language models (BERT, BioBERT, PubMedBERT, ClinicalBERT) in medical entity extraction tasks. Experimental results show that PubMedBERT achieves the best performance (F1-score &#x3D; 88.8%), indicating that a language model pre-trained on biomedical literature is more effective in the medical domain. In addition, we analyze the impact of different entity extraction methods (CRF, Span-based, Seq2Seq) and find that the Span-based approach performs best in medical entity extraction tasks (F1-score &#x3D; 88.6%). It demonstrates superior accuracy in identifying entity boundaries. In low-resource scenarios, we further explore the application of Few-shot Learning in medical entity extraction. Experimental results show that even with only 10-shot training samples, the model achieves an F1-score of 79.1%, verifying the effectiveness of Few-shot Learning under limited data conditions. This study confirms that the combination of pre-trained language models and Few-shot Learning can enhance the accuracy of medical entity extraction. Future research can integrate knowledge graphs and active learning strategies to improve the modelâ€™s generalization and stability, providing a more effective solution for medical NLP research. Keywords- Natural Language Processing, medical named entity recognition, pre-trained language model, Few-shot Learning, information extraction, deep learning </p>
<blockquote>
<p>æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§åŸºäºTransformerçš„åŒ»ç–—å®ä½“æå–æ–¹æ³•ï¼Œæ—¨åœ¨æé«˜åŒ»ç–—æ–‡çŒ®çš„ä¿¡æ¯æå–èƒ½åŠ›ã€‚è€ƒè™‘åˆ°åŒ»ç–—æ–‡æœ¬çš„ä¸“ä¸šæ€§å’Œå¤æ‚æ€§ï¼Œæˆ‘ä»¬æ¯”è¾ƒäº†ä¸åŒé¢„è®­ç»ƒè¯­è¨€æ¨¡å‹ï¼ˆBERTã€BioBERTã€PubMedBERTã€ClinicalBERTï¼‰åœ¨åŒ»ç–—å®ä½“æå–ä»»åŠ¡ä¸­çš„æ€§èƒ½ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒPubMedBERTçš„è¡¨ç°æœ€ä½³ï¼ˆF1åˆ†æ•°ä¸º88.8%ï¼‰ï¼Œè¿™è¡¨æ˜åœ¨ç”Ÿç‰©åŒ»å­¦æ–‡çŒ®ä¸Šé¢„è®­ç»ƒçš„è¯­è¨€æ¨¡å‹åœ¨åŒ»å­¦é¢†åŸŸæ›´ä¸ºæœ‰æ•ˆã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬åˆ†æäº†ä¸åŒçš„å®ä½“æå–æ–¹æ³•ï¼ˆCRFã€åŸºäºSpançš„ã€Seq2Seqï¼‰çš„å½±å“ï¼Œå¹¶å‘ç°åŸºäºSpançš„æ–¹æ³•åœ¨åŒ»ç–—å®ä½“æå–ä»»åŠ¡ä¸­è¡¨ç°æœ€ä½³ï¼ˆF1åˆ†æ•°ä¸º88.6%ï¼‰ã€‚å®ƒåœ¨è¯†åˆ«å®ä½“è¾¹ç•Œæ–¹é¢è¡¨ç°å‡ºè¾ƒé«˜çš„å‡†ç¡®æ€§ã€‚åœ¨èµ„æºæœ‰é™çš„æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬è¿›ä¸€æ­¥æ¢ç´¢äº†Few-shot Learningåœ¨åŒ»ç–—å®ä½“æå–ä¸­çš„åº”ç”¨ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œå³ä½¿åªæœ‰10ä¸ªè®­ç»ƒæ ·æœ¬ï¼Œæ¨¡å‹ä¹Ÿèƒ½è¾¾åˆ°79.1%çš„F1åˆ†æ•°ï¼ŒéªŒè¯äº†æœ‰é™æ•°æ®æ¡ä»¶ä¸‹Few-shot Learningçš„æœ‰æ•ˆæ€§ã€‚æœ¬ç ”ç©¶è¯å®äº†é¢„è®­ç»ƒè¯­è¨€æ¨¡å‹å’ŒFew-shot Learningçš„ç»“åˆå¯ä»¥æé«˜åŒ»ç–—å®ä½“æå–çš„å‡†ç¡®æ€§ã€‚æœªæ¥çš„ç ”ç©¶å¯ä»¥æ•´åˆçŸ¥è¯†å›¾è°±å’Œä¸»åŠ¨å­¦ä¹ ç­–ç•¥ï¼Œä»¥æé«˜æ¨¡å‹çš„é€šç”¨æ€§å’Œç¨³å®šæ€§ï¼Œä¸ºåŒ»ç–—NLPç ”ç©¶æä¾›æ›´æœ‰æ•ˆçš„è§£å†³æ–¹æ¡ˆã€‚å…³é”®è¯ï¼šè‡ªç„¶è¯­è¨€å¤„ç†ã€åŒ»ç–—å‘½åå®ä½“è¯†åˆ«ã€é¢„è®­ç»ƒè¯­è¨€æ¨¡å‹ã€Few-shot Learningã€ä¿¡æ¯æå–ã€æ·±åº¦å­¦ä¹ ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.04385v1">PDF</a> </p>
<p><strong>Summary</strong>ï¼šè¯¥ç ”ç©¶æå‡ºä¸€ç§åŸºäºTransformerçš„åŒ»ç–—å®ä½“æå–æ–¹æ³•ï¼Œä»¥æé«˜åŒ»ç–—æ–‡çŒ®çš„ä¿¡æ¯æå–èƒ½åŠ›ã€‚ç ”ç©¶æ¯”è¾ƒäº†ä¸åŒé¢„è®­ç»ƒè¯­è¨€æ¨¡å‹ï¼ˆBERTã€BioBERTã€PubMedBERTã€ClinicalBERTï¼‰åœ¨åŒ»ç–—å®ä½“æå–ä»»åŠ¡ä¸Šçš„è¡¨ç°ï¼Œå‘ç°PubMedBERTè¡¨ç°æœ€ä½³ï¼ˆF1åˆ†æ•°ä¸º88.8%ï¼‰ã€‚åŒæ—¶ï¼ŒSpan-basedæ–¹æ³•åœ¨å®ä½“æå–ä¸­è¡¨ç°å‡ºæœ€ä½³æ€§èƒ½ï¼ˆF1åˆ†æ•°ä¸º88.6%ï¼‰ï¼Œå¹¶ä¸”åœ¨ä½èµ„æºåœºæ™¯ä¸‹åº”ç”¨Few-shot Learningï¼Œä»…ä½¿ç”¨10ä¸ªè®­ç»ƒæ ·æœ¬å³å¯è¾¾åˆ°79.1%çš„F1åˆ†æ•°ã€‚è¯¥ç ”ç©¶è¡¨æ˜ï¼Œé¢„è®­ç»ƒè¯­è¨€æ¨¡å‹å’ŒFew-shot Learningçš„ç»“åˆå¯ä»¥æé«˜åŒ»ç–—å®ä½“æå–çš„å‡†ç¡®æ€§ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>ç ”ç©¶æå‡ºåŸºäºTransformerçš„åŒ»ç–—å®ä½“æå–æ–¹æ³•ï¼Œæ—¨åœ¨æé«˜åŒ»ç–—æ–‡çŒ®çš„ä¿¡æ¯æå–èƒ½åŠ›ã€‚</li>
<li>ä¸åŒé¢„è®­ç»ƒè¯­è¨€æ¨¡å‹åœ¨åŒ»ç–—å®ä½“æå–ä»»åŠ¡ä¸­çš„æ€§èƒ½è¿›è¡Œæ¯”è¾ƒï¼Œå‘ç°PubMedBERTè¡¨ç°æœ€ä½³ã€‚</li>
<li>Span-basedæ–¹æ³•åœ¨åŒ»ç–—å®ä½“æå–ä»»åŠ¡ä¸­è¡¨ç°å‡ºæœ€ä½³æ€§èƒ½ï¼Œèƒ½å¤Ÿå‡†ç¡®è¯†åˆ«å®ä½“è¾¹ç•Œã€‚</li>
<li>åœ¨ä½èµ„æºåœºæ™¯ä¸‹åº”ç”¨Few-shot Learningï¼Œä»…ä½¿ç”¨å°‘é‡è®­ç»ƒæ ·æœ¬å³å¯å®ç°è¾ƒé«˜çš„æ€§èƒ½ã€‚</li>
<li>é¢„è®­ç»ƒè¯­è¨€æ¨¡å‹å’ŒFew-shot Learningçš„ç»“åˆå¯ä»¥æé«˜åŒ»ç–—å®ä½“æå–çš„å‡†ç¡®æ€§ã€‚</li>
<li>æœªæ¥ç ”ç©¶å¯ç»“åˆçŸ¥è¯†å›¾è°±å’Œä¸»åŠ¨å­¦ä¹ ç­–ç•¥ï¼Œæé«˜æ¨¡å‹çš„é€šç”¨æ€§å’Œç¨³å®šæ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.04385">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-fa715612c07ac1fbbef60f4b681ccb18.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-9a27f71b3e1de892de7fe15a7f37dc02.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-72ccbc52127a3a4519c2dc32f6e7a975.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2ba2d2dc0a71f13d52930537dd8e39a2.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="StyleRec-A-Benchmark-Dataset-for-Prompt-Recovery-in-Writing-Style-Transformation"><a href="#StyleRec-A-Benchmark-Dataset-for-Prompt-Recovery-in-Writing-Style-Transformation" class="headerlink" title="StyleRec: A Benchmark Dataset for Prompt Recovery in Writing Style   Transformation"></a>StyleRec: A Benchmark Dataset for Prompt Recovery in Writing Style   Transformation</h2><p><strong>Authors:Shenyang Liu, Yang Gao, Shaoyan Zhai, Liqiang Wang</strong></p>
<p>Prompt Recovery, reconstructing prompts from the outputs of large language models (LLMs), has grown in importance as LLMs become ubiquitous. Most users access LLMs through APIs without internal model weights, relying only on outputs and logits, which complicates recovery. This paper explores a unique prompt recovery task focused on reconstructing prompts for style transfer and rephrasing, rather than typical question-answering. We introduce a dataset created with LLM assistance, ensuring quality through multiple techniques, and test methods like zero-shot, few-shot, jailbreak, chain-of-thought, fine-tuning, and a novel canonical-prompt fallback for poor-performing cases. Our results show that one-shot and fine-tuning yield the best outcomes but highlight flaws in traditional sentence similarity metrics for evaluating prompt recovery. Contributions include (1) a benchmark dataset, (2) comprehensive experiments on prompt recovery strategies, and (3) identification of limitations in current evaluation metrics, all of which advance general prompt recovery research, where the structure of the input prompt is unrestricted. </p>
<blockquote>
<p>æç¤ºæ¢å¤ï¼ˆPrompt Recoveryï¼‰æ˜¯ä»å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„è¾“å‡ºä¸­é‡å»ºæç¤ºçš„é‡è¦ä»»åŠ¡ï¼Œéšç€LLMçš„æ™®åŠï¼Œå…¶é‡è¦æ€§æ—¥ç›Šå‡¸æ˜¾ã€‚å¤§å¤šæ•°ç”¨æˆ·é€šè¿‡APIè®¿é—®LLMï¼Œè€Œä¸æ¶‰åŠå†…éƒ¨æ¨¡å‹æƒé‡ï¼Œä»…ä¾èµ–è¾“å‡ºå’Œé€»è¾‘ï¼Œè¿™å¢åŠ äº†æ¢å¤çš„å¤æ‚æ€§ã€‚æœ¬æ–‡ä¸“æ³¨äºç ”ç©¶ä¸€ç§ç‹¬ç‰¹çš„æç¤ºæ¢å¤ä»»åŠ¡ï¼Œå³é’ˆå¯¹é£æ ¼è½¬æ¢å’Œé‡æ–°è¡¨è¿°çš„æç¤ºé‡å»ºï¼Œè€Œéå…¸å‹çš„é—®é¢˜å›ç­”ã€‚æˆ‘ä»¬åˆ©ç”¨LLMè¾…åŠ©åˆ›å»ºäº†ä¸€ä¸ªæ•°æ®é›†ï¼Œé€šè¿‡å¤šç§æŠ€æœ¯ç¡®ä¿æ•°æ®è´¨é‡ï¼Œå¹¶æµ‹è¯•äº†é›¶æ ·æœ¬ã€å°‘æ ·æœ¬ã€çªç ´å¼ã€é“¾å¼æ€ç»´ã€å¾®è°ƒä»¥åŠé’ˆå¯¹è¡¨ç°ä¸ä½³æƒ…å†µçš„å…¨æ–°è§„èŒƒæç¤ºå›é€€ç­‰æ–¹æ³•ã€‚ç»“æœè¡¨æ˜ï¼Œå•æ¬¡è®­ç»ƒï¼ˆone-shotï¼‰å’Œå¾®è°ƒçš„æ•ˆæœæœ€ä½³ï¼Œä½†ä¹Ÿçªæ˜¾äº†ç”¨äºè¯„ä¼°æç¤ºæ¢å¤çš„ä¼ ç»Ÿçš„å¥å­ç›¸ä¼¼åº¦æŒ‡æ ‡çš„ä¸è¶³ã€‚æœ¬ç ”ç©¶çš„è´¡çŒ®åŒ…æ‹¬ï¼šï¼ˆ1ï¼‰ä¸€ä¸ªåŸºå‡†æ•°æ®é›†ï¼Œï¼ˆ2ï¼‰å…³äºæç¤ºæ¢å¤ç­–ç•¥çš„ç»¼åˆæ€§å®éªŒï¼Œï¼ˆ3ï¼‰å¯¹å½“å‰è¯„ä¼°æŒ‡æ ‡çš„å±€é™æ€§è¿›è¡Œè¯†åˆ«ï¼Œæ‰€æœ‰è¿™äº›è´¡çŒ®éƒ½æ¨åŠ¨äº†è¾“å…¥æç¤ºç»“æ„ä¸å—é™åˆ¶çš„ä¸€èˆ¬æç¤ºæ¢å¤ç ”ç©¶ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.04373v1">PDF</a> 2024 IEEE International Conference on Big Data (BigData)</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†åŸºäºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æç¤ºæ¢å¤æŠ€æœ¯çš„é‡è¦æ€§åŠå…¶æŒ‘æˆ˜ã€‚é’ˆå¯¹é£æ ¼è½¬æ¢å’Œé‡è¿°çš„ç‰¹å®šåœºæ™¯ï¼Œæå‡ºäº†ä¸€ç§æ–°çš„æç¤ºæ¢å¤ä»»åŠ¡ã€‚é€šè¿‡åˆ›å»ºLLMè¾…åŠ©æ•°æ®é›†ï¼Œé‡‡ç”¨å¤šç§æŠ€æœ¯ä¿è¯æ•°æ®è´¨é‡ã€‚å®éªŒæµ‹è¯•äº†å¤šç§æ–¹æ³•ï¼ŒåŒ…æ‹¬é›¶æ ·æœ¬ã€ä¸€ä¸ªæ ·ä¾‹ã€çªç ´é“¾æ€ç»´ã€å¾®è°ƒä»¥åŠé’ˆå¯¹è¡¨ç°ä¸ä½³æ¡ˆä¾‹çš„æ–°å‹è§„èŒƒæç¤ºå›é€€ç­–ç•¥ã€‚ç»“æœæ˜¾ç¤ºï¼Œä¸€ä¸ªæ ·ä¾‹å’Œå¾®è°ƒæ•ˆæœæœ€ä½³ï¼Œå¹¶æŒ‡å‡ºäº†ä¼ ç»Ÿå¥å­ç›¸ä¼¼åº¦åº¦é‡åœ¨è¯„ä¼°æç¤ºæ¢å¤æ–¹é¢çš„ä¸è¶³ã€‚è´¡çŒ®åŒ…æ‹¬ï¼šæä¾›åŸºå‡†æ•°æ®é›†ã€å…¨é¢çš„æç¤ºæ¢å¤ç­–ç•¥å®éªŒä»¥åŠå½“å‰è¯„ä¼°æŒ‡æ ‡çš„å±€é™æ€§è¯†åˆ«ï¼Œä¸ºæ— é™åˆ¶è¾“å…¥æç¤ºç»“æ„çš„é€šç”¨æç¤ºæ¢å¤ç ”ç©¶æä¾›äº†è¿›å±•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ™®åŠä½¿å¾—åŸºäºLLMçš„æç¤ºæ¢å¤æŠ€æœ¯æ„ˆå‘é‡è¦ã€‚</li>
<li>æœ¬æ–‡ä¸“æ³¨äºé£æ ¼è½¬æ¢å’Œé‡è¿°çš„æç¤ºæ¢å¤ä»»åŠ¡ã€‚</li>
<li>åˆ©ç”¨LLMè¾…åŠ©åˆ›å»ºæ•°æ®é›†ï¼Œå¹¶é€šè¿‡å¤šç§æŠ€æœ¯ä¿è¯æ•°æ®è´¨é‡ã€‚</li>
<li>å®éªŒæµ‹è¯•äº†å¤šç§æç¤ºæ¢å¤æ–¹æ³•ï¼ŒåŒ…æ‹¬é›¶æ ·æœ¬ã€ä¸€ä¸ªæ ·ä¾‹ã€çªç ´é“¾æ€ç»´ã€å¾®è°ƒç­‰ã€‚</li>
<li>ä¸€ä¸ªæ ·ä¾‹å’Œå¾®è°ƒåœ¨å®éªŒä¸­è¡¨ç°æœ€ä½³ã€‚</li>
<li>æŒ‡å‡ºä¼ ç»Ÿå¥å­ç›¸ä¼¼åº¦åº¦é‡åœ¨è¯„ä¼°æç¤ºæ¢å¤æ–¹é¢çš„å±€é™æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.04373">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-92a4e90520b8300778019cfc5b99a1d2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-42de292c54cb7690f4060c33fa40e20e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-5f3a3209707dfa26a9f44a638374406d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c37a7c3d42a1a33f8c8013ca80da1c40.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-eef4afa161299bf1091314d627d61cd5.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-1b45b57645b7d18c5b5fcfd91210e2b3.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="Generative-Large-Language-Models-Trained-for-Detecting-Errors-in-Radiology-Reports"><a href="#Generative-Large-Language-Models-Trained-for-Detecting-Errors-in-Radiology-Reports" class="headerlink" title="Generative Large Language Models Trained for Detecting Errors in   Radiology Reports"></a>Generative Large Language Models Trained for Detecting Errors in   Radiology Reports</h2><p><strong>Authors:Cong Sun, Kurt Teichman, Yiliang Zhou, Brian Critelli, David Nauheim, Graham Keir, Xindi Wang, Judy Zhong, Adam E Flanders, George Shih, Yifan Peng</strong></p>
<p>In this retrospective study, a dataset was constructed with two parts. The first part included 1,656 synthetic chest radiology reports generated by GPT-4 using specified prompts, with 828 being error-free synthetic reports and 828 containing errors. The second part included 614 reports: 307 error-free reports between 2011 and 2016 from the MIMIC-CXR database and 307 corresponding synthetic reports with errors generated by GPT-4 on the basis of these MIMIC-CXR reports and specified prompts. All errors were categorized into four types: negation, left&#x2F;right, interval change, and transcription errors. Then, several models, including Llama-3, GPT-4, and BiomedBERT, were refined using zero-shot prompting, few-shot prompting, or fine-tuning strategies. Finally, the performance of these models was evaluated using the F1 score, 95% confidence interval (CI) and paired-sample t-tests on our constructed dataset, with the prediction results further assessed by radiologists. Using zero-shot prompting, the fine-tuned Llama-3-70B-Instruct model achieved the best performance with the following F1 scores: 0.769 for negation errors, 0.772 for left&#x2F;right errors, 0.750 for interval change errors, 0.828 for transcription errors, and 0.780 overall. In the real-world evaluation phase, two radiologists reviewed 200 randomly selected reports output by the model. Of these, 99 were confirmed to contain errors detected by the models by both radiologists, and 163 were confirmed to contain model-detected errors by at least one radiologist. Generative LLMs, fine-tuned on synthetic and MIMIC-CXR radiology reports, greatly enhanced error detection in radiology reports. </p>
<blockquote>
<p>åœ¨æœ¬æ¬¡å›é¡¾æ€§ç ”ç©¶ä¸­ï¼Œæ„å»ºäº†åŒ…å«ä¸¤éƒ¨åˆ†çš„æ•°æ®é›†ã€‚ç¬¬ä¸€éƒ¨åˆ†åŒ…æ‹¬ç”±GPT-4æ ¹æ®ç‰¹å®šæç¤ºç”Ÿæˆçš„1656ä»½åˆæˆèƒ¸éƒ¨æ”¾å°„å­¦æŠ¥å‘Šï¼Œå…¶ä¸­828ä»½æ˜¯æ— é”™è¯¯çš„åˆæˆæŠ¥å‘Šï¼Œå¦å¤–828ä»½åŒ…å«é”™è¯¯ã€‚ç¬¬äºŒéƒ¨åˆ†åŒ…å«614ä»½æŠ¥å‘Šï¼šå…¶ä¸­307ä»½æ˜¯åœ¨2011å¹´è‡³2016å¹´æœŸé—´æ¥è‡ªMIMIC-CXRæ•°æ®åº“çš„æ— é”™è¯¯æŠ¥å‘Šï¼Œå¦å¤–307ä»½æ˜¯åŸºäºè¿™äº›MIMIC-CXRæŠ¥å‘Šå’Œç‰¹å®šæç¤ºç”±GPT-4ç”Ÿæˆçš„ç›¸åº”åˆæˆæŠ¥å‘Šï¼Œå…¶ä¸­åŒ…å«äº†é”™è¯¯ã€‚æ‰€æœ‰é”™è¯¯éƒ½è¢«åˆ†ä¸ºå››ç§ç±»å‹ï¼šå¦å®šã€å·¦å³ã€é—´éš”å˜åŒ–å’Œè½¬å½•é”™è¯¯ã€‚ç„¶åï¼Œä½¿ç”¨é›¶æ ·æœ¬æç¤ºã€å°‘æ ·æœ¬æç¤ºæˆ–å¾®è°ƒç­–ç•¥å¯¹åŒ…æ‹¬Llama-3ã€GPT-4å’ŒBiomedBERTç­‰å¤šä¸ªæ¨¡å‹è¿›è¡Œäº†æ”¹è¿›ã€‚æœ€åï¼Œä½¿ç”¨F1åˆ†æ•°ã€95%ç½®ä¿¡åŒºé—´ï¼ˆCIï¼‰å’Œé…å¯¹æ ·æœ¬tæ£€éªŒå¯¹æˆ‘ä»¬æ„å»ºçš„æ•°æ®é›†ä¸Šè¿™äº›æ¨¡å‹çš„è¡¨ç°è¿›è¡Œäº†è¯„ä¼°ï¼Œé¢„æµ‹ç»“æœè¿˜å¾—åˆ°äº†æ”¾å°„ç§‘çš„è¿›ä¸€æ­¥è¯„ä¼°ã€‚é‡‡ç”¨é›¶æ ·æœ¬æç¤ºæ–¹æ³•ï¼Œç»è¿‡å¾®è°ƒçš„Llama-3-70B-Instructæ¨¡å‹åœ¨ä»¥ä¸‹æ–¹é¢çš„F1åˆ†æ•°è¾¾åˆ°æœ€ä½³ï¼šå¦å®šé”™è¯¯ä¸º0.769ï¼Œå·¦å³é”™è¯¯ä¸º0.772ï¼Œé—´éš”å˜åŒ–é”™è¯¯ä¸º0.750ï¼Œè½¬å½•é”™è¯¯ä¸º0.828ï¼Œæ€»ä½“ä¸º0.780ã€‚åœ¨ç°å®ä¸–ç•Œè¯„ä¼°é˜¶æ®µï¼Œä¸¤åæ”¾å°„ç§‘åŒ»ç”Ÿå®¡æŸ¥äº†æ¨¡å‹è¾“å‡ºçš„200ä»½éšæœºé€‰æ‹©çš„æŠ¥å‘Šã€‚å…¶ä¸­ï¼Œæœ‰99ä»½æŠ¥å‘Šä¸­çš„é”™è¯¯è¢«æ¨¡å‹æ£€æµ‹å‡ºæ¥å¹¶å¾—åˆ°ä¸¤åæ”¾å°„ç§‘åŒ»ç”Ÿçš„ç¡®è®¤ï¼Œæœ‰163ä»½æŠ¥å‘Šä¸­çš„æ¨¡å‹æ£€æµ‹åˆ°çš„é”™è¯¯è‡³å°‘å¾—åˆ°ä¸€åæ”¾å°„ç§‘åŒ»ç”Ÿçš„ç¡®è®¤ã€‚åœ¨åˆæˆå’ŒMIMIC-CXRæ”¾å°„å­¦æŠ¥å‘Šä¸Šç»è¿‡å¾®è°ƒçš„ç”Ÿæˆå¼å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰æå¤§åœ°æé«˜äº†æ”¾å°„å­¦æŠ¥å‘Šä¸­çš„é”™è¯¯æ£€æµ‹èƒ½åŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.04336v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡å›é¡¾æ€§ç ”ç©¶æ„å»ºäº†åŒ…å«ä¸¤éƒ¨åˆ†çš„æ•°æ®é›†ï¼Œç¬¬ä¸€éƒ¨åˆ†ä¸ºGPT-4æ ¹æ®ç‰¹å®šæç¤ºç”Ÿæˆçš„åˆæˆèƒ¸éƒ¨æ”¾å°„å­¦æŠ¥å‘Šï¼Œç¬¬äºŒéƒ¨åˆ†ä¸ºæ¥è‡ªMIMIC-CXRæ•°æ®åº“çš„çœŸå®æŠ¥å‘Šå’Œå¯¹åº”çš„åˆæˆæŠ¥å‘Šã€‚ç ”ç©¶è®­ç»ƒäº†å¤šä¸ªæ¨¡å‹ï¼ŒåŒ…æ‹¬Llama-3ã€GPT-4å’ŒBiomedBERTç­‰ï¼Œç”¨äºæ£€æµ‹å››ç±»é”™è¯¯ã€‚è¯„ä¼°ç»“æœæ˜¾ç¤ºï¼Œé‡‡ç”¨é›¶æ ·æœ¬æç¤ºçš„Llama-3æ¨¡å‹è¡¨ç°æœ€ä½³ã€‚åœ¨çœŸå®ä¸–ç•Œè¯„ä¼°é˜¶æ®µï¼Œä¸¤ä½æ”¾å°„ç§‘åŒ»ç”Ÿå®¡æ ¸äº†æ¨¡å‹è¾“å‡ºçš„éšæœºé€‰å–çš„200ä»½æŠ¥å‘Šï¼Œè¯å®æ¨¡å‹å¯æœ‰æ•ˆæ£€æµ‹é”™è¯¯ã€‚æ€»çš„æ¥è¯´ï¼Œåœ¨åˆæˆå’ŒMIMIC-CXRèƒ¸éƒ¨æ”¾å°„æŠ¥å‘Šçš„å¾®è°ƒåŸºç¡€ä¸Šï¼Œç”Ÿæˆå¼å¤§å‹è¯­è¨€æ¨¡å‹å¯æ˜¾è‘—æé«˜æ”¾å°„å­¦æŠ¥å‘Šçš„è¯¯å·®æ£€æµ‹èƒ½åŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç ”ç©¶æ„å»ºäº†åŒ…å«åˆæˆå’ŒçœŸå®èƒ¸éƒ¨æ”¾å°„å­¦æŠ¥å‘Šçš„æ•°æ®é›†ï¼Œç”¨äºæ¨¡å‹è®­ç»ƒå’Œè¯„ä¼°ã€‚</li>
<li>è®­ç»ƒäº†å¤šä¸ªæ¨¡å‹ï¼ŒåŒ…æ‹¬Llama-3ã€GPT-4å’ŒBiomedBERTç­‰ï¼Œç”¨äºæ£€æµ‹æ”¾å°„å­¦æŠ¥å‘Šä¸­çš„å››ç±»é”™è¯¯ã€‚</li>
<li>é‡‡ç”¨é›¶æ ·æœ¬æç¤ºçš„Llama-3æ¨¡å‹åœ¨é”™è¯¯æ£€æµ‹æ–¹é¢è¡¨ç°æœ€ä½³ã€‚</li>
<li>åœ¨çœŸå®ä¸–ç•Œè¯„ä¼°é˜¶æ®µï¼Œæ¨¡å‹èƒ½å¤Ÿæœ‰æ•ˆæ£€æµ‹å‡ºæ”¾å°„å­¦æŠ¥å‘Šä¸­çš„é”™è¯¯ï¼Œå¾—åˆ°æ”¾å°„ç§‘åŒ»ç”Ÿçš„ç¡®è®¤ã€‚</li>
<li>æ¨¡å‹å¯¹ä¸åŒç±»å‹çš„é”™è¯¯æ£€æµ‹å…·æœ‰ä¸åŒçš„F1å¾—åˆ†ï¼Œå…¶ä¸­å¯¹è½¬å½•é”™è¯¯çš„æ£€æµ‹è¡¨ç°æœ€ä½³ã€‚</li>
<li>é€šè¿‡å¯¹åˆæˆå’ŒMIMIC-CXRèƒ¸éƒ¨æ”¾å°„æŠ¥å‘Šçš„å¾®è°ƒï¼Œç”Ÿæˆå¼å¤§å‹è¯­è¨€æ¨¡å‹æé«˜äº†è¯¯å·®æ£€æµ‹èƒ½åŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.04336">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-f0cc4cb416b94e2569db037861d86b72.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-eba34acfe32e4813e1292af49bc46720.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-18d6d9efc97f5765c8f64506d7039f6a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-dde40e079c4d03743c8bc4fec0275ee2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a771de8fde7e12ddcc623004e214a16d.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="Tratto-A-Neuro-Symbolic-Approach-to-Deriving-Axiomatic-Test-Oracles"><a href="#Tratto-A-Neuro-Symbolic-Approach-to-Deriving-Axiomatic-Test-Oracles" class="headerlink" title="Tratto: A Neuro-Symbolic Approach to Deriving Axiomatic Test Oracles"></a>Tratto: A Neuro-Symbolic Approach to Deriving Axiomatic Test Oracles</h2><p><strong>Authors:Davide Molinelli, Alberto Martin-Lopez, Elliott Zackrone, Beyza Eken, Michael D. Ernst, Mauro PezzÃ¨</strong></p>
<p>This paper presents Tratto, a neuro-symbolic approach that generates assertions (boolean expressions) that can serve as axiomatic oracles, from source code and documentation. The symbolic module of Tratto takes advantage of the grammar of the programming language, the unit under test, and the context of the unit (its class and available APIs) to restrict the search space of the tokens that can be successfully used to generate valid oracles. The neural module of Tratto uses transformers fine-tuned for both deciding whether to output an oracle or not and selecting the next lexical token to incrementally build the oracle from the set of tokens returned by the symbolic module. Our experiments show that Tratto outperforms the state-of-the-art axiomatic oracle generation approaches, with 73% accuracy, 72% precision, and 61% F1-score, largely higher than the best results of the symbolic and neural approaches considered in our study (61%, 62%, and 37%, respectively). Tratto can generate three times more axiomatic oracles than current symbolic approaches, while generating 10 times less false positives than GPT4 complemented with few-shot learning and Chain-of-Thought prompting. </p>
<blockquote>
<p>æœ¬æ–‡ä»‹ç»äº†Trattoï¼Œè¿™æ˜¯ä¸€ç§ç¥ç»ç¬¦å·æ–¹æ³•ï¼Œèƒ½å¤Ÿä»æºä»£ç å’Œæ–‡æ¡£ç”Ÿæˆæ–­è¨€ï¼ˆå¸ƒå°”è¡¨è¾¾å¼ï¼‰ï¼Œè¿™äº›æ–­è¨€å¯ä»¥ä½œä¸ºå…¬ç†é¢„è¨€ã€‚Trattoçš„ç¬¦å·æ¨¡å—åˆ©ç”¨ç¼–ç¨‹è¯­è¨€çš„è¯­æ³•ã€æµ‹è¯•å•å…ƒä»¥åŠå•å…ƒä¸Šä¸‹æ–‡ï¼ˆå…¶ç±»å’Œå¯ç”¨çš„APIï¼‰æ¥é™åˆ¶æˆåŠŸç”¨äºç”Ÿæˆæœ‰æ•ˆé¢„è¨€çš„æ ‡è®°æœç´¢ç©ºé—´ã€‚Trattoçš„ç¥ç»æ¨¡å—ä½¿ç”¨ç»è¿‡å¾®è°ƒçš„è¯­è¨€æ¨¡å‹æ¥å†³å®šæ˜¯å¦è¾“å‡ºé¢„è¨€ä»¥åŠé€‰æ‹©ä¸‹ä¸€ä¸ªè¯ç´ æ¥é€æ­¥æ„å»ºé¢„è¨€ï¼Œè¿™äº›è¯ç´ ç”±ç¬¦å·æ¨¡å—è¿”å›çš„æ ‡è®°é›†æä¾›ã€‚æˆ‘ä»¬çš„å®éªŒè¡¨æ˜ï¼ŒTrattoåœ¨å‡†ç¡®æ€§ã€ç²¾ç¡®æ€§å’ŒF1åˆ†æ•°æ–¹é¢è¶…è¿‡äº†æœ€æ–°çš„å…¬ç†é¢„è¨€ç”Ÿæˆæ–¹æ³•ï¼Œåˆ†åˆ«ä¸º73%ã€72%å’Œ61%ï¼Œå¤§å¤§é«˜äºæˆ‘ä»¬ç ”ç©¶ä¸­è€ƒè™‘çš„æœ€ä½³ç¬¦å·æ–¹æ³•å’Œç¥ç»æ–¹æ³•çš„ç»“æœï¼ˆåˆ†åˆ«ä¸º61%ã€62%å’Œ37%ï¼‰ã€‚Trattoèƒ½å¤Ÿç”Ÿæˆæ¯”å½“å‰ç¬¦å·æ–¹æ³•é«˜ä¸‰å€çš„å…¬ç†é¢„è¨€ï¼ŒåŒæ—¶ç”Ÿæˆçš„æ¯”GPT4é…åˆå°‘æ ·æœ¬å­¦ä¹ å’Œæ€ç»´é“¾æç¤ºç”Ÿæˆçš„å‡é˜³æ€§ç»“æœå°‘åå€ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.04251v1">PDF</a> Accepted for publication at ISSTA 2025</p>
<p><strong>Summary</strong></p>
<p>Trattoæ˜¯ä¸€ç§ç¥ç»ç¬¦å·æ–¹æ³•ï¼Œå¯ä»¥ä»æºä»£ç å’Œæ–‡æ¡£ç”Ÿæˆæ–­è¨€ï¼ˆå¸ƒå°”è¡¨è¾¾å¼ï¼‰ï¼Œè¿™äº›æ–­è¨€å¯ä»¥ä½œä¸ºå…¬ç†è°•æ—¨ã€‚Trattoçš„ç¬¦å·æ¨¡å—åˆ©ç”¨ç¼–ç¨‹è¯­è¨€çš„è¯­æ³•ã€æµ‹è¯•å•å…ƒä»¥åŠå•å…ƒä¸Šä¸‹æ–‡ï¼ˆå…¶ç±»å’Œå¯ç”¨çš„APIï¼‰æ¥é™åˆ¶å¯ä»¥æˆåŠŸç”¨äºç”Ÿæˆæœ‰æ•ˆè°•æ—¨çš„æ ‡è®°æœç´¢ç©ºé—´ã€‚Trattoçš„ç¥ç»ç½‘ç»œæ¨¡å—ä½¿ç”¨ç»è¿‡å¾®è°ƒå˜å‹å™¨ï¼Œç”¨äºå†³å®šæ˜¯å¦è¾“å‡ºè°•æ—¨ä»¥åŠä»ç¬¦å·æ¨¡å—è¿”å›çš„æ ‡è®°é›†ä¸­é€æ­¥æ„å»ºè°•æ—¨æ—¶é€‰æ‹©ä¸‹ä¸€ä¸ªè¯æ±‡æ ‡è®°ã€‚å®éªŒè¡¨æ˜ï¼ŒTrattoåœ¨ç”Ÿæˆå…¬ç†è°•æ—¨æ–¹é¢ä¼˜äºæœ€æ–°æŠ€æœ¯ï¼Œå…·æœ‰73%çš„å‡†ç¡®ç‡ã€72%çš„ç²¾ç¡®åº¦å’Œ61%çš„F1åˆ†æ•°ï¼Œå¤§å¤§é«˜äºæˆ‘ä»¬ç ”ç©¶ä¸­è€ƒè™‘çš„è±¡å¾æ€§å’Œç¥ç»æ–¹æ³•çš„æœ€ä½³ç»“æœã€‚Trattoå¯ä»¥ç”Ÿæˆæ¯”å½“å‰è±¡å¾æ€§æ–¹æ³•ä¸‰å€å¤šçš„å…¬ç†è°•æ—¨ï¼ŒåŒæ—¶äº§ç”Ÿçš„å‡é˜³æ€§æ¯”GPT4ä½¿ç”¨å°‘é‡é•œå¤´å­¦ä¹ å’Œæ€ç»´é“¾æç¤ºå°‘åå€ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Trattoæ˜¯ä¸€ç§ç¥ç»ç¬¦å·æ–¹æ³•ï¼Œèƒ½ç»“åˆæºä»£ç å’Œæ–‡æ¡£ç”Ÿæˆæ–­è¨€ï¼ˆå¸ƒå°”è¡¨è¾¾å¼ï¼‰ã€‚</li>
<li>Trattoçš„ç¬¦å·æ¨¡å—åˆ©ç”¨ç¼–ç¨‹è¯­è¨€çš„è¯­æ³•ã€æµ‹è¯•å•å…ƒåŠå…¶ä¸Šä¸‹æ–‡æ¥é™åˆ¶æœ‰æ•ˆæ ‡è®°çš„æœç´¢ç©ºé—´ã€‚</li>
<li>Trattoçš„ç¥ç»ç½‘ç»œæ¨¡å—ä½¿ç”¨å¾®è°ƒåçš„å˜å‹å™¨æ¥å†³ç­–å¹¶æ„å»ºè°•æ—¨ã€‚</li>
<li>Trattoåœ¨ç”Ÿæˆå…¬ç†è°•æ—¨æ–¹é¢è¡¨ç°å‡ºä¼˜å¼‚çš„æ€§èƒ½ï¼Œå…·æœ‰è¾ƒé«˜çš„å‡†ç¡®ç‡ã€ç²¾ç¡®åº¦å’ŒF1åˆ†æ•°ã€‚</li>
<li>Trattoç›¸æ¯”å½“å‰è±¡å¾æ€§æ–¹æ³•èƒ½ç”Ÿæˆæ›´å¤šçš„å…¬ç†è°•æ—¨ã€‚</li>
<li>Trattoç”Ÿæˆçš„å‡é˜³æ€§ç›¸æ¯”GPT4å’Œå…¶ä»–æ–¹æ³•å¤§å¤§å‡å°‘ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.04251">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-1ce9f862477e3f6d9f4d196f52642d2f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-679b682307a57cd61b81cf620dc85abc.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="YaleNLP-PerAnsSumm-2025-Multi-Perspective-Integration-via-Mixture-of-Agents-for-Enhanced-Healthcare-QA-Summarization"><a href="#YaleNLP-PerAnsSumm-2025-Multi-Perspective-Integration-via-Mixture-of-Agents-for-Enhanced-Healthcare-QA-Summarization" class="headerlink" title="YaleNLP @ PerAnsSumm 2025: Multi-Perspective Integration via   Mixture-of-Agents for Enhanced Healthcare QA Summarization"></a>YaleNLP @ PerAnsSumm 2025: Multi-Perspective Integration via   Mixture-of-Agents for Enhanced Healthcare QA Summarization</h2><p><strong>Authors:Dongsuk Jang, Alan Li, Arman Cohan</strong></p>
<p>Automated summarization of healthcare community question-answering forums is challenging due to diverse perspectives presented across multiple user responses to each question. The PerAnsSumm Shared Task was therefore proposed to tackle this challenge by identifying perspectives from different answers and then generating a comprehensive answer to the question. In this study, we address the PerAnsSumm Shared Task using two complementary paradigms: (i) a training-based approach through QLoRA fine-tuning of LLaMA-3.3-70B-Instruct, and (ii) agentic approaches including zero- and few-shot prompting with frontier LLMs (LLaMA-3.3-70B-Instruct and GPT-4o) and a Mixture-of-Agents (MoA) framework that leverages a diverse set of LLMs by combining outputs from multi-layer feedback aggregation. For perspective span identification&#x2F;classification, GPT-4o zero-shot achieves an overall score of 0.57, substantially outperforming the 0.40 score of the LLaMA baseline. With a 2-layer MoA configuration, we were able to improve LLaMA performance up by 28 percent to 0.51. For perspective-based summarization, GPT-4o zero-shot attains an overall score of 0.42 compared to 0.28 for the best LLaMA zero-shot, and our 2-layer MoA approach boosts LLaMA performance by 32 percent to 0.37. Furthermore, in few-shot setting, our results show that the sentence-transformer embedding-based exemplar selection provides more gain than manually selected exemplars on LLaMA models, although the few-shot prompting is not always helpful for GPT-4o. The YaleNLP teamâ€™s approach ranked the overall second place in the shared task. </p>
<blockquote>
<p>å¯¹åŒ»ç–—å¥åº·ç¤¾åŒºé—®ç­”è®ºå›è¿›è¡Œè‡ªåŠ¨æ‘˜è¦æ˜¯ä¸€é¡¹å…·æœ‰æŒ‘æˆ˜æ€§çš„ä»»åŠ¡ï¼Œå› ä¸ºæ¯ä¸ªé—®é¢˜éƒ½æœ‰æ¥è‡ªå¤šä¸ªç”¨æˆ·çš„å¤šæ ·åŒ–è§‚ç‚¹ã€‚å› æ­¤ï¼Œæå‡ºäº†PerAnsSummå…±äº«ä»»åŠ¡ï¼Œé€šè¿‡è¯†åˆ«ä¸åŒç­”æ¡ˆä¸­çš„è§‚ç‚¹ï¼Œç„¶åç”Ÿæˆå¯¹é—®é¢˜çš„å…¨é¢ç­”æ¡ˆæ¥è§£å†³è¿™ä¸€æŒ‘æˆ˜ã€‚åœ¨è¿™é¡¹ç ”ç©¶ä¸­ï¼Œæˆ‘ä»¬ä½¿ç”¨ä¸¤ç§äº’è¡¥çš„æ–¹æ³•æ¥è§£å†³PerAnsSummå…±äº«ä»»åŠ¡ï¼šï¼ˆi)åŸºäºè®­ç»ƒçš„æ–¹æ³•ï¼Œé€šè¿‡QLoRAå¾®è°ƒLLaMA-3.3-70B-Instructï¼›ï¼ˆii)ä»£ç†æ–¹æ³•ï¼ŒåŒ…æ‹¬å‰æ²¿LLMï¼ˆLLaMA-3.3-70B-Instructå’ŒGPT-4oï¼‰çš„é›¶æ¬¡å’Œå°‘æ¬¡æç¤ºï¼Œä»¥åŠåˆ©ç”¨å¤šå±‚åé¦ˆèšåˆè¾“å‡ºçš„æ··åˆä»£ç†ï¼ˆMoAï¼‰æ¡†æ¶ã€‚å¯¹äºè§‚ç‚¹è·¨åº¦è¯†åˆ«&#x2F;åˆ†ç±»ï¼ŒGPT-4oé›¶æ ·æœ¬çš„æ€»ä½“å¾—åˆ†ä¸º0.57ï¼Œæ˜¾è‘—ä¼˜äºLLaMAåŸºå‡†çš„0.40ã€‚ä½¿ç”¨ä¸¤å±‚MoAé…ç½®åï¼Œæˆ‘ä»¬èƒ½å¤Ÿæé«˜LLaMAçš„æ€§èƒ½è‡³åŸæ¥çš„ç™¾åˆ†ä¹‹äºŒåå…«è‡³0.51ã€‚å¯¹äºåŸºäºè§‚ç‚¹æ‘˜è¦çš„ä»»åŠ¡ï¼ŒGPT-4oé›¶æ ·æœ¬çš„å¾—åˆ†è¾¾åˆ°ä¸ºæ•´ä½“çš„å¾—åˆ†ç‡ä¸ºé›¶æ¯”äºŒçš„LLaMAæ¨¡å‹0.4æ¯”æ›´é«˜çš„ç‚¹ä¸‰ä¸ƒç›¸æ¯”ä¹‹ä¸‹ä¸ƒåä¹å¯¹è‡ªåŠ¨è°ƒè°æˆ‘ä»¬åœ¨åœ¨æ¡†æ¶ä¸­è·å¾—è¾ƒé«˜çš„é›¶ç»©æ•ˆå¢åŠ åŒå±‚æœ«åŒºä¸€äº›è·å¾—å…¶æ¬¡æ˜¯æœ¬æ–‡é‡‡ç”¨çš„è¿™ä¸ªæ–¹æ³•å¯ä»¥å€Ÿæ­¤èƒ½å¤ŸæŒ‰ç…§è‡ªæˆ‘é¼“åŠ±åœ¨å…¨å±åºå°±æ˜¯è·³å‡ºå¼ºè°ƒé‡ä¹˜ä¸ªä½“æ¿€å‘åæ˜ çš„å¥å­å›åº”æ€¥ä¿ç•™å˜å¾—æ¨ªè·¨è°±äººçš„å·¦å³æ—¶çš„å¤–è§‚è½¨é“è¾ƒå¤šå¢å¤§è¿œè¿œçš„å‡ ä¸ªä¾‹å­å½“ä¸­æˆ‘ä»¬çš„ç»“æœä¹Ÿæ˜¾ç¤ºåŸºäºå¥å­å˜æ¢å™¨åµŒå…¥çš„èŒƒä¾‹é€‰æ‹©æ¯”æ‰‹åŠ¨é€‰æ‹©çš„èŒƒä¾‹å¯¹LLaMAæ¨¡å‹æ›´æœ‰ç›Šè™½ç„¶å°‘æç¤ºå¯¹GPT-4oå¹¶ä¸æ€»æ˜¯æœ‰å¸®åŠ©ã€‚è€¶é²å¤§å­¦è‡ªç„¶è¯­è¨€å¤„ç†å›¢é˜Ÿçš„æ–¹æ³•åœ¨å…±äº«ä»»åŠ¡ä¸­å–å¾—äº†ç¬¬äºŒåã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.03932v1">PDF</a> Paper accepted at CL4HEALTH @ NAACL 2025: Annual Conference of the   Nations of the Americas Chapter of the Association for Computational   Linguistics</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†é’ˆå¯¹åŒ»ç–—å¥åº·ç¤¾åŒºé—®ç­”è®ºå›çš„è‡ªåŠ¨åŒ–æ‘˜è¦æ‰€é¢ä¸´çš„æŒ‘æˆ˜ï¼Œå¹¶æå‡ºäº†PerAnsSummå…±äº«ä»»åŠ¡ä»¥åº”å¯¹è¿™ä¸€æŒ‘æˆ˜ã€‚è¯¥ç ”ç©¶é‡‡ç”¨ä¸¤ç§äº’è¡¥çš„æ–¹æ³•æ¥å®Œæˆæ­¤ä»»åŠ¡ï¼šåŸºäºè®­ç»ƒçš„æ–¹æ³•å’ŒåŸºäºä»£ç†çš„æ–¹æ³•ã€‚åœ¨è§†è§’è¯†åˆ«æ–¹é¢ï¼ŒGPT-4oçš„é›¶æ‹è¡¨ç°ä¼˜äºLLaMAåŸºçº¿ã€‚ä½¿ç”¨ä¸¤å±‚MoAé…ç½®ï¼ŒLLaMAçš„æ€§èƒ½æé«˜äº†28%ã€‚åœ¨åŸºäºè§†è§’çš„æ‘˜è¦æ–¹é¢ï¼ŒGPT-4oçš„é›¶æ‹è¡¨ç°ä¹Ÿæ¯”LLaMAçš„é›¶æ‹è¦å¥½ã€‚æ­¤å¤–ï¼Œè¯¥ç ”ç©¶è¿˜è¡¨æ˜ï¼Œåœ¨å°‘é‡æ ·æœ¬çš„æƒ…å†µä¸‹ï¼ŒåŸºäºå¥å­è½¬æ¢å™¨çš„åµŒå…¥ç¤ºä¾‹é€‰æ‹©å¯¹LLaMAæ¨¡å‹æ›´æœ‰ç›Šï¼Œä½†å¯¹GPT-4oçš„å¸®åŠ©ä¸å¤§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>PerAnsSummå…±äº«ä»»åŠ¡æ—¨åœ¨é€šè¿‡è¯†åˆ«ä¸åŒç­”æ¡ˆçš„è§†è§’æ¥ç”Ÿæˆä¸€ä¸ªå…¨é¢çš„é—®ç­”æ‘˜è¦ï¼Œä»¥åº”å¯¹åŒ»ç–—å¥åº·ç¤¾åŒºé—®ç­”è®ºå›çš„è‡ªåŠ¨åŒ–æ‘˜è¦æŒ‘æˆ˜ã€‚</li>
<li>ç ”ç©¶é‡‡ç”¨åŸºäºè®­ç»ƒå’ŒåŸºäºä»£ç†ä¸¤ç§äº’è¡¥æ–¹æ³•æ¥å®Œæˆæ­¤ä»»åŠ¡ã€‚</li>
<li>GPT-4oåœ¨è§†è§’è¯†åˆ«å’ŒåŸºäºè§†è§’çš„æ‘˜è¦æ–¹é¢çš„è¡¨ç°ä¼˜äºLLaMAåŸºçº¿ã€‚</li>
<li>ä½¿ç”¨ä¸¤å±‚MoAé…ç½®å¯ä»¥æé«˜LLaMAçš„æ€§èƒ½ã€‚</li>
<li>åœ¨å°‘é‡æ ·æœ¬çš„æƒ…å†µä¸‹ï¼ŒåŸºäºå¥å­è½¬æ¢å™¨çš„åµŒå…¥ç¤ºä¾‹é€‰æ‹©å¯¹LLaMAæ¨¡å‹æœ‰æ›´å¤§çš„å¸®åŠ©ã€‚</li>
<li>GPT-4oåœ¨å°‘é‡æ ·æœ¬æç¤ºä¸‹å¹¶ä¸æ€»æ˜¯æœ‰å¸®åŠ©ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.03932">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-c2bd1616bfeaf34c01b3ef460b306e98.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-0a1368d97e863c1a741bcbc63dc43dc3.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a854cb78f30bb30b853adb85136b9337.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f5e1d3a6c2160276fae73b709f2cff52.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="Hierarchical-Local-Global-Feature-Learning-for-Few-shot-Malicious-Traffic-Detection"><a href="#Hierarchical-Local-Global-Feature-Learning-for-Few-shot-Malicious-Traffic-Detection" class="headerlink" title="Hierarchical Local-Global Feature Learning for Few-shot Malicious   Traffic Detection"></a>Hierarchical Local-Global Feature Learning for Few-shot Malicious   Traffic Detection</h2><p><strong>Authors:Songtao Peng, Lei Wang, Wu Shuai, Hao Song, Jiajun Zhou, Shanqing Yu, Qi Xuan</strong></p>
<p>With the rapid growth of internet traffic, malicious network attacks have become increasingly frequent and sophisticated, posing significant threats to global cybersecurity. Traditional detection methods, including rule-based and machine learning-based approaches, struggle to accurately identify emerging threats, particularly in scenarios with limited samples. While recent advances in few-shot learning have partially addressed the data scarcity issue, existing methods still exhibit high false positive rates and lack the capability to effectively capture crucial local traffic patterns. In this paper, we propose HLoG, a novel hierarchical few-shot malicious traffic detection framework that leverages both local and global features extracted from network sessions. HLoG employs a sliding-window approach to segment sessions into phases, capturing fine-grained local interaction patterns through hierarchical bidirectional GRU encoding, while simultaneously modeling global contextual dependencies. We further design a session similarity assessment module that integrates local similarity with global self-attention-enhanced representations, achieving accurate and robust few-shot traffic classification. Comprehensive experiments on three meticulously reconstructed datasets demonstrate that HLoG significantly outperforms existing state-of-the-art methods. Particularly, HLoG achieves superior recall rates while substantially reducing false positives, highlighting its effectiveness and practical value in real-world cybersecurity applications. </p>
<blockquote>
<p>éšç€ç½‘ç»œæµé‡çš„å¿«é€Ÿå¢é•¿ï¼Œæ¶æ„ç½‘ç»œæ”»å‡»ä¹Ÿå˜å¾—è¶Šæ¥è¶Šé¢‘ç¹å’Œé«˜çº§ï¼Œå¯¹å…¨çƒç½‘ç»œå®‰å…¨æ„æˆäº†é‡å¤§å¨èƒã€‚ä¼ ç»Ÿçš„æ£€æµ‹æ–¹æ³•ï¼ŒåŒ…æ‹¬åŸºäºè§„åˆ™å’ŒåŸºäºæœºå™¨å­¦ä¹ çš„æ–¹æ³•ï¼Œåœ¨å‡†ç¡®è¯†åˆ«æ–°å…´å¨èƒæ–¹é¢é‡åˆ°äº†å›°éš¾ï¼Œç‰¹åˆ«æ˜¯åœ¨æ ·æœ¬æœ‰é™çš„æƒ…å†µä¸‹ã€‚å°½ç®¡æœ€è¿‘çš„å°æ ·æœ¬å­¦ä¹ è¿›å±•éƒ¨åˆ†è§£å†³äº†æ•°æ®ç¨€ç¼ºçš„é—®é¢˜ï¼Œä½†ç°æœ‰æ–¹æ³•ä»è¡¨ç°å‡ºè¾ƒé«˜çš„è¯¯æŠ¥ç‡ï¼Œå¹¶ä¸”ç¼ºä¹æœ‰æ•ˆæ•æ‰å…³é”®æœ¬åœ°æµé‡æ¨¡å¼çš„èƒ½åŠ›ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†HLoGï¼Œè¿™æ˜¯ä¸€ç§æ–°å‹åˆ†å±‚å°æ ·æœ¬æ¶æ„æµé‡æ£€æµ‹æ¡†æ¶ï¼Œå®ƒåˆ©ç”¨ä»ç½‘ç»œä¼šè¯ä¸­æå–çš„å±€éƒ¨å’Œå…¨å±€ç‰¹å¾ã€‚HLoGé‡‡ç”¨æ»‘åŠ¨çª—å£æ–¹æ³•å°†ä¼šè¯åˆ†æ®µæˆé˜¶æ®µï¼Œé€šè¿‡åˆ†å±‚åŒå‘GRUç¼–ç æ•è·ç²¾ç»†çš„å±€éƒ¨äº¤äº’æ¨¡å¼ï¼ŒåŒæ—¶å»ºæ¨¡å…¨å±€ä¸Šä¸‹æ–‡ä¾èµ–å…³ç³»ã€‚æˆ‘ä»¬è¿›ä¸€æ­¥è®¾è®¡äº†ä¸€ä¸ªä¼šè¯ç›¸ä¼¼æ€§è¯„ä¼°æ¨¡å—ï¼Œè¯¥æ¨¡å—å°†å±€éƒ¨ç›¸ä¼¼æ€§ä¸å…¨å±€è‡ªæ³¨æ„åŠ›å¢å¼ºè¡¨ç¤ºç›¸ç»“åˆï¼Œå®ç°äº†å‡†ç¡®ä¸”ç¨³å¥çš„å°æ ·æœ¬æµé‡åˆ†ç±»ã€‚åœ¨ä¸‰ä¸ªç²¾å¿ƒé‡å»ºçš„æ•°æ®é›†ä¸Šçš„ç»¼åˆå®éªŒè¡¨æ˜ï¼ŒHLoGæ˜¾è‘—ä¼˜äºç°æœ‰çš„æœ€å…ˆè¿›æ–¹æ³•ã€‚ç‰¹åˆ«çš„æ˜¯ï¼ŒHLoGåœ¨è¾¾åˆ°è¾ƒé«˜çš„å¬å›ç‡çš„åŒæ—¶å¤§å¤§é™ä½äº†è¯¯æŠ¥ç‡ï¼Œå‡¸æ˜¾äº†å…¶åœ¨ç°å®ç½‘ç»œå®‰å…¨åº”ç”¨ä¸­çš„æœ‰æ•ˆæ€§å’Œå®ç”¨ä»·å€¼ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.03742v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>éšç€äº’è”ç½‘æµé‡çš„å¿«é€Ÿå¢é•¿ï¼Œæ¶æ„ç½‘ç»œæ”»å‡»æ—¥ç›Šé¢‘ç¹ä¸”å¤æ‚ï¼Œå¯¹å…¨çƒç½‘ç»œå®‰å…¨æ„æˆé‡å¤§å¨èƒã€‚ä¼ ç»Ÿçš„æ£€æµ‹æ–¹æ³•ï¼Œå¦‚åŸºäºè§„åˆ™å’Œæœºå™¨å­¦ä¹ çš„æ–¹æ³•ï¼Œéš¾ä»¥å‡†ç¡®è¯†åˆ«æ–°å…´å¨èƒï¼Œç‰¹åˆ«æ˜¯åœ¨æ ·æœ¬æœ‰é™çš„æƒ…å†µä¸‹ã€‚æœ¬æ–‡æå‡ºä¸€ç§æ–°å‹çš„åˆ†å±‚å°æ ·æœ¬æ¶æ„æµé‡æ£€æµ‹æ¡†æ¶HLoGï¼Œå®ƒç»“åˆäº†ç½‘ç»œä¼šè¯çš„å±€éƒ¨å’Œå…¨å±€ç‰¹å¾ã€‚HLoGé‡‡ç”¨æ»‘åŠ¨çª—å£æŠ€æœ¯å°†ä¼šè¯åˆ†æ®µï¼Œé€šè¿‡åˆ†å±‚åŒå‘GRUç¼–ç æ•è·ç²¾ç»†çš„å±€éƒ¨äº¤äº’æ¨¡å¼ï¼ŒåŒæ—¶å»ºç«‹å…¨å±€ä¸Šä¸‹æ–‡ä¾èµ–å…³ç³»ã€‚æ­¤å¤–ï¼Œè®¾è®¡äº†ä¸€ä¸ªä¼šè¯ç›¸ä¼¼æ€§è¯„ä¼°æ¨¡å—ï¼Œå°†å±€éƒ¨ç›¸ä¼¼æ€§ä¸å…¨å±€è‡ªæ³¨æ„åŠ›å¢å¼ºè¡¨ç¤ºç›¸ç»“åˆï¼Œå®ç°å‡†ç¡®ä¸”ç¨³å¥çš„å°æ ·æœ¬æµé‡åˆ†ç±»ã€‚åœ¨ä¸‰ä¸ªç²¾å¿ƒé‡å»ºçš„æ•°æ®é›†ä¸Šçš„ç»¼åˆå®éªŒè¡¨æ˜ï¼ŒHLoGæ˜¾è‘—ä¼˜äºç°æœ‰æœ€æ–°æ–¹æ³•ï¼Œç‰¹åˆ«æ˜¯åœ¨å¬å›ç‡å’Œè¯¯æŠ¥ç‡æ–¹é¢è¡¨ç°ä¼˜å¼‚ï¼Œå‡¸æ˜¾å…¶åœ¨ç°å®ç½‘ç»œå®‰å…¨åº”ç”¨ä¸­çš„å®ç”¨æ€§å’Œä»·å€¼ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>äº’è”ç½‘æµé‡çš„å¢é•¿å¯¼è‡´æ¶æ„ç½‘ç»œæ”»å‡»é¢‘ç¹ä¸”å¤æ‚ï¼Œå¯¹å…¨çƒç½‘ç»œå®‰å…¨æ„æˆå¨èƒã€‚</li>
<li>ä¼ ç»Ÿæ£€æµ‹æ–¹æ³•åœ¨è¯†åˆ«æ–°å…´å¨èƒæ—¶é¢ä¸´å›°éš¾ï¼Œå°¤å…¶åœ¨æ ·æœ¬æœ‰é™çš„æƒ…å†µä¸‹ã€‚</li>
<li>HLoGæ¡†æ¶ç»“åˆäº†ç½‘ç»œä¼šè¯çš„å±€éƒ¨å’Œå…¨å±€ç‰¹å¾ï¼Œæé«˜æ¶æ„æµé‡æ£€æµ‹çš„å‡†ç¡®æ€§ã€‚</li>
<li>HLoGé‡‡ç”¨æ»‘åŠ¨çª—å£æŠ€æœ¯å°†ä¼šè¯åˆ†æ®µï¼Œå¹¶é€šè¿‡åˆ†å±‚åŒå‘GRUç¼–ç æ•è·å±€éƒ¨å’Œå…¨å±€ç‰¹å¾ã€‚</li>
<li>HLoGè®¾è®¡äº†ä¼šè¯ç›¸ä¼¼æ€§è¯„ä¼°æ¨¡å—ï¼Œç»“åˆå±€éƒ¨å’Œå…¨å±€ç‰¹å¾ï¼Œå®ç°å‡†ç¡®ä¸”ç¨³å¥çš„æµé‡åˆ†ç±»ã€‚</li>
<li>HLoGåœ¨å¤šä¸ªæ•°æ®é›†ä¸Šçš„å®éªŒè¡¨ç°ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œç‰¹åˆ«æ˜¯åœ¨å¬å›ç‡å’Œè¯¯æŠ¥ç‡æ–¹é¢ã€‚</li>
<li>HLoGå…·æœ‰å®ç”¨æ€§å’Œä»·å€¼ï¼Œé€‚ç”¨äºç°å®ç½‘ç»œå®‰å…¨åº”ç”¨ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.03742">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-fa74f6f04412f1c8eb2f3e2006d77c29.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-aeca7089cfbf045863bdcab49469e26f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4be6f66e3d1b209d943b68d4f7db7c42.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-f56560ac790d71765431446fce71ec77.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="TransNet-Transfer-Knowledge-for-Few-shot-Knowledge-Graph-Completion"><a href="#TransNet-Transfer-Knowledge-for-Few-shot-Knowledge-Graph-Completion" class="headerlink" title="TransNet: Transfer Knowledge for Few-shot Knowledge Graph Completion"></a>TransNet: Transfer Knowledge for Few-shot Knowledge Graph Completion</h2><p><strong>Authors:Lihui Liu, Zihao Wang, Dawei Zhou, Ruijie Wang, Yuchen Yan, Bo Xiong, Sihong He, Kai Shu, Hanghang Tong</strong></p>
<p>Knowledge graphs (KGs) are ubiquitous and widely used in various applications. However, most real-world knowledge graphs are incomplete, which significantly degrades their performance on downstream tasks. Additionally, the relationships in real-world knowledge graphs often follow a long-tail distribution, meaning that most relations are represented by only a few training triplets. To address these challenges, few-shot learning has been introduced. Few-shot KG completion aims to make accurate predictions for triplets involving novel relations when only a limited number of training triplets are available. Although many methods have been proposed, they typically learn each relation individually, overlooking the correlations between different tasks and the relevant information in previously trained tasks. In this paper, we propose a transfer learning-based few-shot KG completion method (TransNet). By learning the relationships between different tasks, TransNet effectively transfers knowledge from similar tasks to improve the current taskâ€™s performance. Furthermore, by employing meta-learning, TransNet can generalize effectively to new, unseen relations. Extensive experiments on benchmark datasets demonstrate the superiority of TransNet over state-of-the-art methods. Code can be found at <a target="_blank" rel="noopener" href="https://github.com/lihuiliullh/TransNet/tree/main">https://github.com/lihuiliullh/TransNet/tree/main</a> </p>
<blockquote>
<p>çŸ¥è¯†å›¾è°±ï¼ˆKGsï¼‰åœ¨å„ç§åº”ç”¨ä¸­æ— å¤„ä¸åœ¨ä¸”åº”ç”¨å¹¿æ³›ã€‚ç„¶è€Œï¼Œå¤§å¤šæ•°ç°å®ä¸–ç•Œçš„çŸ¥è¯†å›¾è°±éƒ½æ˜¯ä¸å®Œæ•´ï¼Œè¿™æå¤§åœ°é™ä½äº†å®ƒä»¬åœ¨ä¸‹æ¸¸ä»»åŠ¡ä¸Šçš„æ€§èƒ½ã€‚æ­¤å¤–ï¼Œç°å®ä¸–ç•Œä¸­çŸ¥è¯†å›¾è°±ä¸­çš„å…³ç³»é€šå¸¸éµå¾ªé•¿å°¾åˆ†å¸ƒï¼Œè¿™æ„å‘³ç€å¤§å¤šæ•°å…³ç³»ä»…ç”±å°‘æ•°è®­ç»ƒä¸‰å…ƒç»„è¡¨ç¤ºã€‚ä¸ºäº†è§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œå·²ç»å¼•å…¥äº†å°æ ·å­¦ä¹ ã€‚å°æ ·çŸ¥è¯†å›¾è°±è¡¥å…¨æ—¨åœ¨å½“åªæœ‰æœ‰é™æ•°é‡çš„è®­ç»ƒä¸‰å…ƒç»„å¯ç”¨æ—¶ï¼Œå¯¹æ¶‰åŠæ–°å…³ç³»çš„ä¸‰å…ƒç»„è¿›è¡Œå‡†ç¡®é¢„æµ‹ã€‚å°½ç®¡å·²ç»æå‡ºäº†è®¸å¤šæ–¹æ³•ï¼Œä½†å®ƒä»¬é€šå¸¸å•ç‹¬å­¦ä¹ æ¯ä¸ªå…³ç³»ï¼Œè€Œå¿½ç•¥äº†ä¸åŒä»»åŠ¡ä¹‹é—´çš„ç›¸å…³æ€§ä»¥åŠå…ˆå‰è®­ç»ƒä»»åŠ¡ä¸­çš„ç›¸å…³ä¿¡æ¯ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºè¿ç§»å­¦ä¹ çš„å°æ ·çŸ¥è¯†å›¾è°±è¡¥å…¨æ–¹æ³•ï¼ˆTransNetï¼‰ã€‚é€šè¿‡å­¦ä¹ ä¸åŒä»»åŠ¡ä¹‹é—´çš„å…³ç³»ï¼ŒTransNetæœ‰æ•ˆåœ°å°†ä»ç›¸ä¼¼ä»»åŠ¡ä¸­å­¦åˆ°çš„çŸ¥è¯†è½¬ç§»åˆ°å½“å‰ä»»åŠ¡ä¸­ä»¥æé«˜æ€§èƒ½ã€‚æ­¤å¤–ï¼Œé€šè¿‡é‡‡ç”¨å…ƒå­¦ä¹ ï¼ŒTransNetå¯ä»¥æœ‰æ•ˆåœ°æ¨å¹¿åˆ°æ–°çš„ã€æœªè§è¿‡çš„å…³ç³»ã€‚åœ¨åŸºå‡†æ•°æ®é›†ä¸Šçš„å¤§é‡å®éªŒè¡¨æ˜ï¼ŒTransNetä¼˜äºæœ€å…ˆè¿›çš„æ–¹æ³•ã€‚ä»£ç å¯åœ¨ <a target="_blank" rel="noopener" href="https://github.com/lihuiliullh/TransNet/tree/main">https://github.com/lihuiliullh/TransNet/tree/main</a> æ‰¾åˆ°ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.03720v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†çŸ¥è¯†å›¾è°±ï¼ˆKGsï¼‰åœ¨ç°å®ä¸–ç•Œåº”ç”¨ä¸­çš„ä¸¤ä¸ªä¸»è¦é—®é¢˜ï¼šä¸å®Œæ•´æ€§å’Œé•¿å°¾åˆ†å¸ƒå…³ç³»ã€‚é’ˆå¯¹è¿™äº›é—®é¢˜ï¼Œå¼•å…¥äº†å°‘æ ·æœ¬å­¦ä¹ ã€‚å°‘æ ·æœ¬KGè¡¥å…¨æ—¨åœ¨ä»…ä½¿ç”¨æœ‰é™æ•°é‡çš„è®­ç»ƒä¸‰å…ƒç»„å¯¹æ¶‰åŠæ–°å…³ç³»çš„ä¸‰å…ƒç»„è¿›è¡Œå‡†ç¡®é¢„æµ‹ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºè¿ç§»å­¦ä¹ çš„å°‘æ ·æœ¬KGè¡¥å…¨æ–¹æ³•ï¼ˆTransNetï¼‰ã€‚TransNeté€šè¿‡å­¦ä¹ ä»»åŠ¡ä¹‹é—´çš„å…³ç³»ï¼Œæœ‰æ•ˆåœ°å°†ç›¸ä¼¼ä»»åŠ¡çš„çŸ¥è¯†è½¬ç§»åˆ°å½“å‰ä»»åŠ¡ä¸­ä»¥æé«˜æ€§èƒ½ã€‚æ­¤å¤–ï¼Œé€šè¿‡é‡‡ç”¨å…ƒå­¦ä¹ ï¼ŒTransNetå¯ä»¥æœ‰æ•ˆåœ°æ¨å¹¿åˆ°æ–°çš„ã€æœªè§è¿‡çš„å…³ç³»ä¸Šã€‚åœ¨åŸºå‡†æ•°æ®é›†ä¸Šçš„å¹¿æ³›å®éªŒè¯æ˜äº†TransNetä¼˜äºæœ€å…ˆè¿›çš„æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>çŸ¥è¯†å›¾è°±ï¼ˆKGsï¼‰åœ¨å¤šç§åº”ç”¨ä¸­å¹¿æ³›ä½¿ç”¨ï¼Œä½†å­˜åœ¨ä¸å®Œæ•´æ€§å’Œé•¿å°¾åˆ†å¸ƒå…³ç³»çš„é—®é¢˜ã€‚</li>
<li>å°‘æ ·æœ¬å­¦ä¹ è¢«å¼•å…¥è§£å†³è¿™äº›é—®é¢˜ï¼Œå°‘æ ·æœ¬KGè¡¥å…¨æ—¨åœ¨ç”¨æœ‰é™æ•°æ®é¢„æµ‹æ–°å…³ç³»ã€‚</li>
<li>TransNetæ˜¯ä¸€ç§åŸºäºè¿ç§»å­¦ä¹ çš„å°‘æ ·æœ¬KGè¡¥å…¨æ–¹æ³•ï¼Œèƒ½è¿ç§»ç›¸ä¼¼ä»»åŠ¡çŸ¥è¯†ä»¥æé«˜æ€§èƒ½ã€‚</li>
<li>TransNeté€šè¿‡å­¦ä¹ ä»»åŠ¡é—´çš„å…³ç³»æ¥å·¥ä½œï¼Œè¿™æœ‰åŠ©äºçŸ¥è¯†çš„è¿ç§»ã€‚</li>
<li>TransNeté‡‡ç”¨å…ƒå­¦ä¹ ï¼Œèƒ½æœ‰æ•ˆæ¨å¹¿åˆ°æ–°çš„ã€æœªè§è¿‡çš„å…³ç³»ä¸Šã€‚</li>
<li>åœ¨åŸºå‡†æ•°æ®é›†ä¸Šçš„å®éªŒè¯æ˜TransNetä¼˜äºç°æœ‰æ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.03720">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-4bcfb23bab8c4350d63616cc18ed863e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-faea2830a0eabdf5b13d695e63e2100e.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-ef0573f9da2be96a5c4a536a2d9442ae.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="Is-Temporal-Prompting-All-We-Need-For-Limited-Labeled-Action-Recognition"><a href="#Is-Temporal-Prompting-All-We-Need-For-Limited-Labeled-Action-Recognition" class="headerlink" title="Is Temporal Prompting All We Need For Limited Labeled Action   Recognition?"></a>Is Temporal Prompting All We Need For Limited Labeled Action   Recognition?</h2><p><strong>Authors:Shreyank N Gowda, Boyan Gao, Xiao Gu, Xiaobo Jin</strong></p>
<p>Video understanding has shown remarkable improvements in recent years, largely dependent on the availability of large scaled labeled datasets. Recent advancements in visual-language models, especially based on contrastive pretraining, have shown remarkable generalization in zero-shot tasks, helping to overcome this dependence on labeled datasets. Adaptations of such models for videos, typically involve modifying the architecture of vision-language models to cater to video data. However, this is not trivial, since such adaptations are mostly computationally intensive and struggle with temporal modeling. We present TP-CLIP, an adaptation of CLIP that leverages temporal visual prompting for temporal adaptation without modifying the core CLIP architecture. This preserves its generalization abilities. TP-CLIP efficiently integrates into the CLIP architecture, leveraging its pre-trained capabilities for video data. Extensive experiments across various datasets demonstrate its efficacy in zero-shot and few-shot learning, outperforming existing approaches with fewer parameters and computational efficiency. In particular, we use just 1&#x2F;3 the GFLOPs and 1&#x2F;28 the number of tuneable parameters in comparison to recent state-of-the-art and still outperform it by up to 15.8% depending on the task and dataset. </p>
<blockquote>
<p>è§†é¢‘ç†è§£åœ¨æœ€è¿‘å‡ å¹´å–å¾—äº†æ˜¾è‘—çš„è¿›æ­¥ï¼Œè¿™ä¸»è¦ä¾èµ–äºå¤§è§„æ¨¡æ ‡è®°æ•°æ®é›†çš„å¯ç”¨æ€§ã€‚åŸºäºå¯¹æ¯”é¢„è®­ç»ƒçš„è§†è§‰è¯­è¨€æ¨¡å‹çš„æœ€æ–°è¿›å±•åœ¨é›¶æ ·æœ¬ä»»åŠ¡ä¸­è¡¨ç°å‡ºäº†å‡ºè‰²çš„æ³›åŒ–èƒ½åŠ›ï¼Œæœ‰åŠ©äºå…‹æœå¯¹æ ‡è®°æ•°æ®é›†çš„ä¾èµ–ã€‚æ­¤ç±»è§†é¢‘æ¨¡å‹çš„æ”¹ç¼–é€šå¸¸éœ€è¦å¯¹è§†è§‰è¯­è¨€æ¨¡å‹çš„æ¶æ„è¿›è¡Œä¿®æ”¹ï¼Œä»¥é€‚åº”è§†é¢‘æ•°æ®ã€‚ç„¶è€Œï¼Œè¿™å¹¶ä¸ç®€å•ï¼Œå› ä¸ºè¿™æ ·çš„æ”¹ç¼–å¤§å¤šè®¡ç®—å¯†é›†ä¸”é¢ä¸´æ—¶åºå»ºæ¨¡çš„æŒ‘æˆ˜ã€‚æˆ‘ä»¬æå‡ºäº†TP-CLIPï¼Œè¿™æ˜¯CLIPçš„ä¸€ç§æ”¹ç¼–ï¼Œå®ƒåˆ©ç”¨æ—¶åºè§†è§‰æç¤ºè¿›è¡Œæ—¶åºé€‚åº”ï¼Œè€Œæ— éœ€ä¿®æ”¹CLIPæ¶æ„çš„æ ¸å¿ƒéƒ¨åˆ†ï¼Œä»è€Œä¿ç•™äº†å…¶æ³›åŒ–èƒ½åŠ›ã€‚TP-CLIPèƒ½å¤Ÿé«˜æ•ˆåœ°é›†æˆåˆ°CLIPæ¶æ„ä¸­ï¼Œåˆ©ç”¨å…¶å¯¹è§†é¢‘æ•°æ®çš„é¢„è®­ç»ƒèƒ½åŠ›ã€‚åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šè¿›è¡Œçš„å¤§é‡å®éªŒè¯æ˜äº†å…¶åœ¨é›¶æ ·æœ¬å’Œå°‘æ ·æœ¬å­¦ä¹ ä¸­çš„æœ‰æ•ˆæ€§ï¼Œä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼Œä½¿ç”¨æ›´å°‘çš„å‚æ•°å’Œæ›´é«˜çš„è®¡ç®—æ•ˆç‡ã€‚ç‰¹åˆ«æ˜¯ï¼Œæˆ‘ä»¬ä½¿ç”¨çš„GFLOPsä»…ä¸ºæœ€è¿‘æœ€å…ˆè¿›çš„æŠ€æœ¯çš„ä¸‰åˆ†ä¹‹ä¸€ï¼Œå¯è°ƒå‚æ•°æ•°é‡æ˜¯äºŒåå…«åˆ†ä¹‹ä¸€ï¼Œå¹¶ä¸”åœ¨æŸäº›ä»»åŠ¡å’Œæ•°æ®é›†ä¸Šä»ç„¶è¶…å‡ºå…¶æ€§èƒ½é«˜è¾¾15.8%ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.01890v2">PDF</a> Accepted in CVPR-W 2025</p>
<p><strong>Summary</strong></p>
<p>è¿‘æœŸè§†é¢‘ç†è§£é¢†åŸŸå–å¾—æ˜¾è‘—è¿›å±•ï¼Œè¿™ä¸»è¦ä¾èµ–äºå¤§è§„æ¨¡æ ‡æ³¨æ•°æ®é›†çš„å¯ç”¨æ€§ã€‚åŸºäºå¯¹æ¯”é¢„è®­ç»ƒçš„è§†è§‰è¯­è¨€æ¨¡å‹åœ¨é›¶æ ·æœ¬ä»»åŠ¡ä¸­å±•ç°å‡ºå¼ºå¤§çš„æ³›åŒ–èƒ½åŠ›ï¼Œæœ‰åŠ©äºå‡å°‘å¯¹æ ‡æ³¨æ•°æ®é›†çš„ä¾èµ–ã€‚ä¸ºè§†é¢‘æ•°æ®é€‚åº”æ­¤ç±»æ¨¡å‹é€šå¸¸éœ€è¦ä¿®æ”¹è§†è§‰è¯­è¨€æ¨¡å‹çš„æ¶æ„ï¼Œä½†è¿™å¹¶ä¸ç®€å•ï¼Œå› ä¸ºè¿™æ ·çš„é€‚åº”è®¡ç®—å¯†é›†ä¸”é¢ä¸´æ—¶åºå»ºæ¨¡çš„æŒ‘æˆ˜ã€‚æœ¬æ–‡æå‡ºTP-CLIPï¼Œè¿™æ˜¯CLIPçš„ä¸€ç§é€‚åº”æ–¹æ³•ï¼Œåˆ©ç”¨æ—¶åºè§†è§‰æç¤ºè¿›è¡Œæ—¶åºé€‚åº”ï¼Œè€Œæ— éœ€ä¿®æ”¹æ ¸å¿ƒCLIPæ¶æ„ï¼Œä¿æŒäº†å…¶æ³›åŒ–èƒ½åŠ›ã€‚TP-CLIPæœ‰æ•ˆåœ°é›†æˆåˆ°CLIPæ¶æ„ä¸­ï¼Œåˆ©ç”¨å…¶é¢„è®­ç»ƒèƒ½åŠ›å¤„ç†è§†é¢‘æ•°æ®ã€‚åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šçš„å¤§é‡å®éªŒè¯æ˜äº†å…¶åœ¨é›¶æ ·æœ¬å’Œå°‘æ ·æœ¬å­¦ä¹ ä¸­çš„æœ‰æ•ˆæ€§ï¼Œä¸ä½¿ç”¨è¾ƒå°‘å‚æ•°å’Œè®¡ç®—èµ„æºç›¸æ¯”ï¼Œè¡¨ç°å‡ºå“è¶Šæ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è§†é¢‘ç†è§£çš„è¿›æ­¥ä¾èµ–äºå¤§è§„æ¨¡æ ‡æ³¨æ•°æ®é›†çš„å¯ç”¨æ€§ã€‚</li>
<li>å¯¹æ¯”é¢„è®­ç»ƒçš„è§†è§‰è¯­è¨€æ¨¡å‹åœ¨é›¶æ ·æœ¬ä»»åŠ¡ä¸­å±•ç°å‡ºå¼ºå¤§çš„æ³›åŒ–èƒ½åŠ›ã€‚</li>
<li>é€‚åº”è§†é¢‘æ•°æ®çš„è§†è§‰è¯­è¨€æ¨¡å‹é€šå¸¸éœ€è¦ä¿®æ”¹æ¶æ„ï¼Œä½†é¢ä¸´è®¡ç®—å¯†é›†å’Œæ—¶åºå»ºæ¨¡çš„æŒ‘æˆ˜ã€‚</li>
<li>TP-CLIPæ˜¯CLIPçš„ä¸€ç§é€‚åº”æ–¹æ³•ï¼Œåˆ©ç”¨æ—¶åºè§†è§‰æç¤ºè¿›è¡Œæ—¶åºé€‚åº”ã€‚</li>
<li>TP-CLIPä¿ç•™äº†CLIPçš„æ³›åŒ–èƒ½åŠ›ï¼Œä¸”é›†æˆåˆ°CLIPæ¶æ„ä¸­æ•ˆç‡é«˜ã€‚</li>
<li>TP-CLIPåœ¨å¤šä¸ªæ•°æ®é›†ä¸Šçš„å®éªŒè¯æ˜äº†å…¶åœ¨é›¶æ ·æœ¬å’Œå°‘æ ·æœ¬å­¦ä¹ ä¸­çš„æœ‰æ•ˆæ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.01890">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-beb7ba9a1cf5f12fc1ba151dfaf296d8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-02e29c3abc2dc3f700139657e496800e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-37ad0ac4892cac9c2d1669e88ea9db36.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-55ff0d601c9148fd8a391434cfac0ce7.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="MSCPT-Few-shot-Whole-Slide-Image-Classification-with-Multi-scale-and-Context-focused-Prompt-Tuning"><a href="#MSCPT-Few-shot-Whole-Slide-Image-Classification-with-Multi-scale-and-Context-focused-Prompt-Tuning" class="headerlink" title="MSCPT: Few-shot Whole Slide Image Classification with Multi-scale and   Context-focused Prompt Tuning"></a>MSCPT: Few-shot Whole Slide Image Classification with Multi-scale and   Context-focused Prompt Tuning</h2><p><strong>Authors:Minghao Han, Linhao Qu, Dingkang Yang, Xukun Zhang, Xiaoying Wang, Lihua Zhang</strong></p>
<p>Multiple instance learning (MIL) has become a standard paradigm for the weakly supervised classification of whole slide images (WSIs). However, this paradigm relies on using a large number of labeled WSIs for training. The lack of training data and the presence of rare diseases pose significant challenges for these methods. Prompt tuning combined with pre-trained Vision-Language models (VLMs) is an effective solution to the Few-shot Weakly Supervised WSI Classification (FSWC) task. Nevertheless, applying prompt tuning methods designed for natural images to WSIs presents three significant challenges: 1) These methods fail to fully leverage the prior knowledge from the VLMâ€™s text modality; 2) They overlook the essential multi-scale and contextual information in WSIs, leading to suboptimal results; and 3) They lack exploration of instance aggregation methods. To address these problems, we propose a Multi-Scale and Context-focused Prompt Tuning (MSCPT) method for FSWC task. Specifically, MSCPT employs the frozen large language model to generate pathological visual language prior knowledge at multiple scales, guiding hierarchical prompt tuning. Additionally, we design a graph prompt tuning module to learn essential contextual information within WSI, and finally, a non-parametric cross-guided instance aggregation module has been introduced to derive the WSI-level features. Extensive experiments, visualizations, and interpretability analyses were conducted on five datasets and three downstream tasks using three VLMs, demonstrating the strong performance of our MSCPT. All codes have been made publicly accessible at <a target="_blank" rel="noopener" href="https://github.com/Hanminghao/MSCPT">https://github.com/Hanminghao/MSCPT</a>. </p>
<blockquote>
<p>å¤šå®ä¾‹å­¦ä¹ ï¼ˆMILï¼‰å·²æˆä¸ºå…¨å¹»ç¯ç‰‡å›¾åƒï¼ˆWSIï¼‰å¼±ç›‘ç£åˆ†ç±»çš„æ ‡å‡†èŒƒå¼ã€‚ç„¶è€Œï¼Œè¿™ç§èŒƒå¼ä¾èµ–äºå¤§é‡æœ‰æ ‡ç­¾çš„WSIè¿›è¡Œè®­ç»ƒã€‚ç¼ºä¹è®­ç»ƒæ•°æ®å’Œç½•è§ç–¾ç—…çš„å­˜åœ¨ç»™è¿™äº›æ–¹æ³•å¸¦æ¥äº†é‡å¤§æŒ‘æˆ˜ã€‚ç»“åˆé¢„è®­ç»ƒçš„è¯­è¨€è§†è§‰æ¨¡å‹ï¼ˆVLMsï¼‰çš„æç¤ºå¾®è°ƒæ˜¯è§£å†³å°æ ·æœ¬å¼±ç›‘ç£WSIåˆ†ç±»ï¼ˆFSWCï¼‰ä»»åŠ¡çš„æœ‰æ•ˆæ–¹æ³•ã€‚ç„¶è€Œï¼Œå°†é’ˆå¯¹è‡ªç„¶å›¾åƒè®¾è®¡çš„æç¤ºå¾®è°ƒæ–¹æ³•åº”ç”¨äºWSIé¢ä¸´ä¸‰å¤§æŒ‘æˆ˜ï¼š1ï¼‰è¿™äº›æ–¹æ³•æœªèƒ½å……åˆ†åˆ©ç”¨æ¥è‡ªVLMæ–‡æœ¬æ¨¡æ€çš„å…ˆéªŒçŸ¥è¯†ï¼›2ï¼‰å®ƒä»¬å¿½ç•¥äº†WSIä¸­çš„å¤šå°ºåº¦å’Œä¸Šä¸‹æ–‡ä¿¡æ¯ï¼Œå¯¼è‡´ç»“æœä¸ç†æƒ³ï¼›å¹¶ä¸”3ï¼‰ç¼ºä¹å¯¹å®ä¾‹èšåˆæ–¹æ³•çš„æ¢ç´¢ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†é¢å‘FSWCä»»åŠ¡çš„å¤šå°ºåº¦ä¸Šä¸‹æ–‡èšç„¦æç¤ºå¾®è°ƒï¼ˆMSCPTï¼‰æ–¹æ³•ã€‚å…·ä½“è€Œè¨€ï¼ŒMSCPTä½¿ç”¨å†»ç»“çš„å¤§å‹è¯­è¨€æ¨¡å‹åœ¨å¤šå°ºåº¦ä¸Šç”Ÿæˆç—…ç†å­¦è§†è§‰è¯­è¨€å…ˆéªŒçŸ¥è¯†ï¼Œå¼•å¯¼åˆ†å±‚æç¤ºå¾®è°ƒã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸€ä¸ªå›¾æç¤ºå¾®è°ƒæ¨¡å—æ¥å­¦ä¹ WSIä¸­çš„å…³é”®ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼Œå¹¶æœ€ç»ˆå¼•å…¥äº†ä¸€ä¸ªéå‚æ•°åŒ–äº¤å‰å¼•å¯¼å®ä¾‹èšåˆæ¨¡å—æ¥æå–WSIçº§ç‰¹å¾ã€‚åœ¨äº”ä¸ªæ•°æ®é›†å’Œä¸‰ä¸ªä¸‹æ¸¸ä»»åŠ¡ä¸Šä½¿ç”¨äº†ä¸‰ç§VLMsè¿›è¡Œäº†å¹¿æ³›çš„å®éªŒã€å¯è§†åŒ–å’Œè§£é‡Šæ€§åˆ†æï¼Œå±•ç¤ºäº†æˆ‘ä»¬çš„MSCPTçš„å¼ºå¤§æ€§èƒ½ã€‚æ‰€æœ‰ä»£ç å·²å…¬å¼€è®¿é—®äº<a target="_blank" rel="noopener" href="https://github.com/Hanminghao/MSCPT%E3%80%82">https://github.com/Hanminghao/MSCPTã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2408.11505v2">PDF</a> This work has been submitted to the IEEE TMI for possible publication</p>
<p><strong>Summary</strong></p>
<p>åŸºäºå¤šå®ä¾‹å­¦ä¹ ï¼ˆMILï¼‰çš„å¼±ç›‘ç£å…¨å¹»ç¯ç‰‡å›¾åƒï¼ˆWSIï¼‰åˆ†ç±»å·²æˆä¸ºæ ‡å‡†èŒƒå¼ï¼Œä½†é¢ä¸´ç¼ºä¹è®­ç»ƒæ•°æ®å’Œç½•è§ç–¾ç—…çš„æŒ‘æˆ˜ã€‚æç¤ºè°ƒæ•´ä¸é¢„è®­ç»ƒè§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰çš„ç»“åˆæ˜¯è§£å†³å°‘æ•°å¼±ç›‘ç£WSIåˆ†ç±»ï¼ˆFSWCï¼‰ä»»åŠ¡çš„æœ‰æ•ˆæ–¹æ³•ã€‚ç„¶è€Œï¼Œå°†é’ˆå¯¹è‡ªç„¶å›¾åƒè®¾è®¡çš„æç¤ºè°ƒæ•´æ–¹æ³•åº”ç”¨äºWSIå­˜åœ¨ä¸‰ä¸ªä¸»è¦æŒ‘æˆ˜ã€‚ä¸ºè§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†å¤šå°ºåº¦ä¸ä¸Šä¸‹æ–‡èšç„¦çš„æç¤ºè°ƒæ•´ï¼ˆMSCPTï¼‰æ–¹æ³•ã€‚MSCPTåˆ©ç”¨å†»ç»“çš„å¤§å‹è¯­è¨€æ¨¡å‹ç”Ÿæˆç—…ç†è§†è§‰è¯­è¨€å…ˆéªŒçŸ¥è¯†ï¼ŒæŒ‡å¯¼åˆ†å±‚æç¤ºè°ƒæ•´ï¼Œå¹¶è®¾è®¡å›¾æç¤ºè°ƒæ•´æ¨¡å—æ¥å­¦ä¹ WSIä¸­çš„å…³é”®ä¸Šä¸‹æ–‡ä¿¡æ¯ã€‚æ­¤å¤–ï¼Œå¼•å…¥äº†éå‚æ•°åŒ–äº¤å‰å¼•å¯¼å®ä¾‹èšåˆæ¨¡å—æ¥æå–WSIçº§åˆ«çš„ç‰¹å¾ã€‚åœ¨äº”ä¸ªæ•°æ®é›†å’Œä¸‰ä¸ªä¸‹æ¸¸ä»»åŠ¡ä¸Šä½¿ç”¨ä¸‰ç§VLMè¿›è¡Œçš„å¹¿æ³›å®éªŒã€å¯è§†åŒ–å’Œå¯è§£é‡Šæ€§åˆ†æè¯æ˜äº†MSCPTçš„å¼ºå¤§æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤šå®ä¾‹å­¦ä¹ ï¼ˆMILï¼‰æ˜¯å¼±ç›‘ç£å…¨å¹»ç¯ç‰‡å›¾åƒï¼ˆWSIï¼‰åˆ†ç±»çš„æ ‡å‡†èŒƒå¼ï¼Œä½†ç¼ºä¹è®­ç»ƒæ•°æ®å’Œç½•è§ç–¾ç—…å¸¦æ¥æŒ‘æˆ˜ã€‚</li>
<li>æç¤ºè°ƒæ•´ä¸é¢„è®­ç»ƒè§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰çš„ç»“åˆä¸ºå°‘æ•°å¼±ç›‘ç£WSIåˆ†ç±»ï¼ˆFSWCï¼‰ä»»åŠ¡æä¾›äº†æœ‰æ•ˆè§£å†³æ–¹æ¡ˆã€‚</li>
<li>å°†é’ˆå¯¹è‡ªç„¶å›¾åƒçš„æç¤ºè°ƒæ•´æ–¹æ³•åº”ç”¨äºWSIå­˜åœ¨æ˜¾è‘—æŒ‘æˆ˜ï¼ŒåŒ…æ‹¬æœªèƒ½å……åˆ†åˆ©ç”¨VLMçš„æ–‡æœ¬æ¨¡æ€å…ˆéªŒçŸ¥è¯†ã€å¿½è§†å¤šå°ºåº¦å’Œä¸Šä¸‹æ–‡ä¿¡æ¯ï¼Œä»¥åŠç¼ºä¹å®ä¾‹èšåˆæ–¹æ³•çš„æ¢ç´¢ã€‚</li>
<li>æå‡ºçš„Multi-Scale and Context-focused Prompt Tuningï¼ˆMSCPTï¼‰æ–¹æ³•åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ç”Ÿæˆç—…ç†è§†è§‰è¯­è¨€å…ˆéªŒçŸ¥è¯†ï¼Œå¹¶è®¾è®¡å›¾æç¤ºè°ƒæ•´æ¨¡å—æ¥å­¦ä¹ WSIçš„ä¸Šä¸‹æ–‡ä¿¡æ¯ã€‚</li>
<li>MSCPTå¼•å…¥äº†éå‚æ•°åŒ–äº¤å‰å¼•å¯¼å®ä¾‹èšåˆæ¨¡å—ï¼Œä»¥æå–WSIçº§åˆ«çš„ç‰¹å¾ã€‚</li>
<li>åœ¨å¤šä¸ªæ•°æ®é›†å’Œä¸‹æ¸¸ä»»åŠ¡ä¸Šçš„å®éªŒè¯æ˜äº†MSCPTçš„å¼ºå¤§æ€§èƒ½ã€‚</li>
<li>æ‰€æœ‰ä»£ç å·²å…¬å¼€å¯è®¿é—®ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2408.11505">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-f4724a53eacdccd365aeebc366316bc3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d8bf6a1a9768792668d85745c8fde8ad.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-988c4c0047bbbcbb97c28564f9fa8423.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="No-Re-Train-More-Gain-Upgrading-Backbones-with-Diffusion-model-for-Pixel-Wise-and-Weakly-Supervised-Few-Shot-Segmentation"><a href="#No-Re-Train-More-Gain-Upgrading-Backbones-with-Diffusion-model-for-Pixel-Wise-and-Weakly-Supervised-Few-Shot-Segmentation" class="headerlink" title="No Re-Train, More Gain: Upgrading Backbones with Diffusion model for   Pixel-Wise and Weakly-Supervised Few-Shot Segmentation"></a>No Re-Train, More Gain: Upgrading Backbones with Diffusion model for   Pixel-Wise and Weakly-Supervised Few-Shot Segmentation</h2><p><strong>Authors:Shuai Chen, Fanman Meng, Chenhao Wu, Haoran Wei, Runtong Zhang, Qingbo Wu, Linfeng Xu, Hongliang Li</strong></p>
<p>Few-Shot Segmentation (FSS) aims to segment novel classes using only a few annotated images. Despite considerable progress under pixel-wise support annotation, current FSS methods still face three issues: the inflexibility of backbone upgrade without re-training, the inability to uniformly handle various types of annotations (e.g., scribble, bounding box, mask, and text), and the difficulty in accommodating different annotation quantity. To address these issues simultaneously, we propose DiffUp, a novel framework that conceptualizes the FSS task as a conditional generative problem using a diffusion process. For the first issue, we introduce a backbone-agnostic feature transformation module that converts different segmentation cues into unified coarse priors, facilitating seamless backbone upgrade without re-training. For the second issue, due to the varying granularity of transformed priors from diverse annotation types (scribble, bounding box, mask, and text), we conceptualize these multi-granular transformed priors as analogous to noisy intermediates at different steps of a diffusion model. This is implemented via a self-conditioned modulation block coupled with a dual-level quality modulation branch. For the third issue, we incorporate an uncertainty-aware information fusion module to harmonize the variability across zero-shot, one-shot, and many-shot scenarios. Evaluated through rigorous benchmarks, DiffUp significantly outperforms existing FSS models in terms of flexibility and accuracy. </p>
<blockquote>
<p>Few-Shot Segmentationï¼ˆFSSï¼‰æ—¨åœ¨ä»…ä½¿ç”¨å°‘é‡æ ‡æ³¨å›¾åƒå¯¹æ–°å‹ç±»åˆ«è¿›è¡Œåˆ†å‰²ã€‚å°½ç®¡åœ¨åƒç´ çº§æ”¯æŒæ ‡æ³¨æ–¹é¢å–å¾—äº†ç›¸å½“å¤§çš„è¿›å±•ï¼Œä½†å½“å‰çš„FSSæ–¹æ³•ä»ç„¶é¢ä¸´ä¸‰ä¸ªé—®é¢˜ï¼šä¸»å¹²å‡çº§ä¸çµæ´»éœ€è¦é‡æ–°è®­ç»ƒï¼Œæ— æ³•ç»Ÿä¸€å¤„ç†å„ç§ç±»å‹æ ‡æ³¨ï¼ˆä¾‹å¦‚æ¶‚é¸¦ã€è¾¹ç•Œæ¡†ã€è’™ç‰ˆå’Œæ–‡æœ¬ï¼‰ï¼Œä»¥åŠéš¾ä»¥é€‚åº”ä¸åŒçš„æ ‡æ³¨æ•°é‡ã€‚ä¸ºäº†è§£å†³è¿™ä¸‰ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†DiffUpè¿™ä¸€æ–°å‹æ¡†æ¶ï¼Œå®ƒå°†FSSä»»åŠ¡æ¦‚å¿µåŒ–ä¸ºä¸€ä¸ªæ¡ä»¶ç”Ÿæˆé—®é¢˜ï¼Œå¹¶ä½¿ç”¨æ‰©æ•£è¿‡ç¨‹æ¥è§£å†³ã€‚é’ˆå¯¹ç¬¬ä¸€ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ªé€šç”¨çš„ç‰¹å¾è½¬æ¢æ¨¡å—ï¼Œè¯¥æ¨¡å—èƒ½å¤Ÿå°†ä¸åŒçš„åˆ†å‰²çº¿ç´¢è½¬æ¢ä¸ºç»Ÿä¸€çš„ç²—ç•¥å…ˆéªŒçŸ¥è¯†ï¼Œä»è€Œå®ç°æ— ç¼ä¸»å¹²å‡çº§è€Œæ— éœ€é‡æ–°è®­ç»ƒã€‚å¯¹äºç¬¬äºŒä¸ªé—®é¢˜ï¼Œç”±äºä¸åŒæ ‡æ³¨ç±»å‹ï¼ˆæ¶‚é¸¦ã€è¾¹ç•Œæ¡†ã€è’™ç‰ˆå’Œæ–‡æœ¬ï¼‰è½¬æ¢åçš„å…ˆéªŒçŸ¥è¯†ç²’åº¦ä¸åŒï¼Œæˆ‘ä»¬å°†è¿™äº›å¤šç²’åº¦è½¬æ¢åçš„å…ˆéªŒçŸ¥è¯†æ¦‚å¿µåŒ–ä¸ºæ‰©æ•£æ¨¡å‹ä¸åŒæ­¥éª¤ä¸­çš„å™ªå£°ä¸­é—´äº§ç‰©ã€‚è¿™æ˜¯é€šè¿‡ä¸€ä¸ªè‡ªè°ƒèŠ‚è°ƒåˆ¶å—ä¸åŒçº§è´¨é‡è°ƒåˆ¶åˆ†æ”¯çš„ç»“åˆå®ç°çš„ã€‚å¯¹äºç¬¬ä¸‰ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸ç¡®å®šä¿¡æ¯èåˆæ¨¡å—ï¼Œä»¥åè°ƒé›¶æ ·æœ¬ã€å•æ ·æœ¬å’Œå¤šæ ·æœ¬åœºæ™¯ä¸­çš„å˜é‡ã€‚é€šè¿‡ä¸¥æ ¼çš„åŸºå‡†æµ‹è¯•ï¼ŒDiffUpåœ¨çµæ´»æ€§å’Œå‡†ç¡®æ€§æ–¹é¢æ˜¾è‘—ä¼˜äºç°æœ‰çš„FSSæ¨¡å‹ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2407.16182v2">PDF</a> 9 figures</p>
<p><strong>Summary</strong></p>
<p>åŸºäºå°‘é‡æ ‡æ³¨å›¾åƒè¿›è¡Œæ–°å‹ç±»åˆ«åˆ†å‰²çš„Few-Shot Segmentationï¼ˆFSSï¼‰ä»é¢ä¸´ä¸‰å¤§é—®é¢˜ï¼šéª¨å¹²ç½‘å‡çº§ä¸çµæ´»ã€éš¾ä»¥å¤„ç†å¤šç§ç±»å‹æ³¨è§£ä»¥åŠä¸åŒæ³¨è§£æ•°é‡çš„é€‚é…å›°éš¾ã€‚ä¸ºåŒæ—¶è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºDiffUpæ¡†æ¶ï¼Œå°†FSSä»»åŠ¡æ¦‚å¿µåŒ–ä¸ºä¸€ä¸ªæ¡ä»¶ç”Ÿæˆé—®é¢˜å¹¶ä½¿ç”¨æ‰©æ•£è¿‡ç¨‹è§£å†³ã€‚å¼•å…¥éª¨å¹²ç½‘æ— å…³çš„ç‰¹å¾è½¬æ¢æ¨¡å—ï¼Œç»Ÿä¸€å¤„ç†å„ç§åˆ†å‰²çº¿ç´¢ï¼Œå®ç°éª¨å¹²ç½‘æ— ç¼å‡çº§ã€‚å€ŸåŠ©è‡ªæ¡ä»¶è°ƒåˆ¶å—å’ŒåŒçº§è´¨é‡è°ƒåˆ¶åˆ†æ”¯ï¼Œå°†å¤šç²’åº¦è½¬æ¢çº¿ç´¢è§†ä¸ºæ‰©æ•£æ¨¡å‹ä¸åŒæ­¥éª¤çš„å™ªå£°ä¸­é—´äº§ç‰©ã€‚èå…¥ä¸ç¡®å®šæ€§æ„ŸçŸ¥ä¿¡æ¯èåˆæ¨¡å—ï¼Œåè°ƒé›¶æ ·æœ¬ã€å•æ ·æœ¬å’Œå¤šæ ·æœ¬åœºæ™¯ä¸­çš„å˜åŒ–ã€‚ç»ä¸¥æ ¼åŸºå‡†æµ‹è¯•ï¼ŒDiffUpåœ¨çµæ´»æ€§å’Œå‡†ç¡®æ€§ä¸Šæ˜¾è‘—ä¼˜äºç°æœ‰FSSæ¨¡å‹ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Few-Shot Segmentation (FSS)é¢ä¸´ä¸‰å¤§æŒ‘æˆ˜ï¼šéª¨å¹²ç½‘å‡çº§çš„çµæ´»æ€§ã€å¤„ç†å¤šç§ç±»å‹æ³¨è§£çš„èƒ½åŠ›ï¼Œä»¥åŠé€‚é…ä¸åŒæ³¨è§£æ•°é‡çš„éš¾åº¦ã€‚</li>
<li>DiffUpæ¡†æ¶å°†FSSä»»åŠ¡è§†ä¸ºä¸€ä¸ªæ¡ä»¶ç”Ÿæˆé—®é¢˜ï¼Œå¹¶é‡‡ç”¨æ‰©æ•£è¿‡ç¨‹æ¥è§£å†³ã€‚</li>
<li>å¼•å…¥çš„ç‰¹å¾è½¬æ¢æ¨¡å—ä½¿éª¨å¹²ç½‘å‡çº§æ›´ä¸ºçµæ´»ï¼Œæ— éœ€é‡æ–°è®­ç»ƒã€‚</li>
<li>å¤šç²’åº¦è½¬æ¢çº¿ç´¢è¢«è§†ä¸ºæ‰©æ•£æ¨¡å‹ä¸­ä¸åŒæ­¥éª¤çš„å™ªå£°ä¸­é—´äº§ç‰©ï¼Œé€šè¿‡è‡ªæ¡ä»¶è°ƒåˆ¶å—å’ŒåŒçº§è´¨é‡è°ƒåˆ¶åˆ†æ”¯å¤„ç†ã€‚</li>
<li>åˆ©ç”¨ä¸ç¡®å®šæ€§æ„ŸçŸ¥ä¿¡æ¯èåˆæ¨¡å—æ¥åè°ƒä¸åŒåœºæ™¯ä¸­çš„å˜åŒ–ã€‚</li>
<li>DiffUpåœ¨çµæ´»æ€§å’Œå‡†ç¡®æ€§ä¸Šæ˜¾è‘—ä¼˜äºç°æœ‰çš„FSSæ¨¡å‹ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2407.16182">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-d52f86b8a3a182fe409f5fe93fe2a0a9.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-bec40d5f377ed20b44e5dc02faa73a15.jpg" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="6Img-to-3D-Few-Image-Large-Scale-Outdoor-Driving-Scene-Reconstruction"><a href="#6Img-to-3D-Few-Image-Large-Scale-Outdoor-Driving-Scene-Reconstruction" class="headerlink" title="6Img-to-3D: Few-Image Large-Scale Outdoor Driving Scene Reconstruction"></a>6Img-to-3D: Few-Image Large-Scale Outdoor Driving Scene Reconstruction</h2><p><strong>Authors:ThÃ©o Gieruc, Marius KÃ¤stingschÃ¤fer, Sebastian Bernhard, Mathieu Salzmann</strong></p>
<p>Current 3D reconstruction techniques struggle to infer unbounded scenes from a few images faithfully. Specifically, existing methods have high computational demands, require detailed pose information, and cannot reconstruct occluded regions reliably. We introduce 6Img-to-3D, an efficient, scalable transformer-based encoder-renderer method for single-shot image to 3D reconstruction. Our method outputs a 3D-consistent parameterized triplane from only six outward-facing input images for large-scale, unbounded outdoor driving scenarios. We take a step towards resolving existing shortcomings by combining contracted custom cross- and self-attention mechanisms for triplane parameterization, differentiable volume rendering, scene contraction, and image feature projection. We showcase that six surround-view vehicle images from a single timestamp without global pose information are enough to reconstruct 360$^{\circ}$ scenes during inference time, taking 395 ms. Our method allows, for example, rendering third-person images and birds-eye views. Our code is available at <a target="_blank" rel="noopener" href="https://github.com/continental/6Img-to-3D">https://github.com/continental/6Img-to-3D</a>, and more examples can be found at our website here <a target="_blank" rel="noopener" href="https://6img-to-3d.github.io/">https://6Img-to-3D.GitHub.io/</a>. </p>
<blockquote>
<p>å½“å‰çš„ä¸‰ç»´é‡å»ºæŠ€æœ¯å¾ˆéš¾ä»å°‘æ•°å›¾åƒä¸­å¿ å®åœ°æ¨æ¼”å‡ºæ— è¾¹ç•Œçš„åœºæ™¯ã€‚å…·ä½“æ¥è¯´ï¼Œç°æœ‰æ–¹æ³•è®¡ç®—éœ€æ±‚é«˜ï¼Œéœ€è¦è¯¦ç»†çš„å§¿æ€ä¿¡æ¯ï¼Œå¹¶ä¸”æ— æ³•å¯é åœ°é‡å»ºé®æŒ¡åŒºåŸŸã€‚æˆ‘ä»¬æ¨å‡ºäº†6Img-to-3Dï¼Œè¿™æ˜¯ä¸€ç§é«˜æ•ˆã€å¯æ‰©å±•çš„åŸºäºtransformerçš„ç¼–ç å™¨-æ¸²æŸ“å™¨æ–¹æ³•ï¼Œç”¨äºå•å¼ å›¾åƒåˆ°ä¸‰ç»´é‡å»ºã€‚æˆ‘ä»¬çš„æ–¹æ³•ä»…ä»å…­å¼ å¤–å‘è¾“å…¥å›¾åƒä¸­è¾“å‡ºä¸€è‡´çš„ä¸‰ç»´å‚æ•°åŒ–triplaneï¼Œé€‚ç”¨äºå¤§è§„æ¨¡ã€æ— è¾¹ç•Œçš„æˆ·å¤–é©¾é©¶åœºæ™¯ã€‚æˆ‘ä»¬é€šè¿‡ç»“åˆå®šåˆ¶çš„äº¤å‰å’Œè‡ªæ³¨æ„åŠ›æœºåˆ¶è¿›è¡Œtriplaneå‚æ•°åŒ–ã€å¯å¾®åˆ†ä½“ç§¯æ¸²æŸ“ã€åœºæ™¯æ”¶ç¼©å’Œå›¾åƒç‰¹å¾æŠ•å½±ï¼Œæ¥è§£å†³ç°æœ‰ç¼ºé™·ã€‚æˆ‘ä»¬å±•ç¤ºï¼Œåœ¨æ¨ç†æ—¶é—´å†…ï¼Œæ²¡æœ‰å…¨å±€å§¿æ€ä¿¡æ¯çš„å•ä¸ªæ—¶é—´æˆ³çš„å…­å¼ ç¯ç»•è§†å›¾è½¦è¾†å›¾åƒè¶³ä»¥é‡å»º360Â°åœºæ™¯ï¼Œåªéœ€395æ¯«ç§’ã€‚æˆ‘ä»¬çš„æ–¹æ³•å…è®¸ä¾‹å¦‚æ¸²æŸ“ç¬¬ä¸‰äººç§°å›¾åƒå’Œé¸Ÿç°å›¾ã€‚æˆ‘ä»¬çš„ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/continental/6Img-to-3D%E6%89%BE%E5%88%B0%EF%BC%8C%E6%9B%B4%E5%A4%9A%E7%A4%BA%E4%BE%8B%E8%AF%B7%E8%AE%BF%E9%97%AE%E6%88%91%E4%BB%AC%E7%9A%84%E7%BD%91%E7%AB%99https://6Img-to-3D.GitHub.io/">https://github.com/continental/6Img-to-3Dæ‰¾åˆ°ï¼Œæ›´å¤šç¤ºä¾‹è¯·è®¿é—®æˆ‘ä»¬çš„ç½‘ç«™https://6Img-to-3D.GitHub.io/</a>.</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2404.12378v2">PDF</a> IV 2025. Joint first authorship. Project page:   <a target="_blank" rel="noopener" href="https://6img-to-3d.github.io/">https://6Img-to-3D.GitHub.io/</a> Code <a target="_blank" rel="noopener" href="https://github.com/continental/6Img-to-3D">https://github.com/continental/6Img-to-3D</a></p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ç°æœ‰çš„ä¸‰ç»´é‡å»ºæŠ€æœ¯é¢ä¸´çš„æŒ‘æˆ˜ï¼Œå¦‚è®¡ç®—éœ€æ±‚é«˜ã€éœ€è¦è¯¦ç»†çš„å§¿æ€ä¿¡æ¯ä»¥åŠæ— æ³•å¯é åœ°é‡å»ºé®æŒ¡åŒºåŸŸç­‰ã€‚ä¸ºè§£å†³è¿™äº›é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§åŸºäºTransformerçš„é«˜æ•ˆã€å¯æ‰©å±•çš„ç¼–ç å™¨æ¸²æŸ“å™¨æ–¹æ³•â€”â€”6Img-to-3Dï¼Œå¯ä»å…­å¼ é¢å‘å¤–éƒ¨çš„è¾“å…¥å›¾åƒè¿›è¡Œå•å¹…å›¾åƒåˆ°ä¸‰ç»´é‡å»ºã€‚è¯¥æ–¹æ³•é€‚ç”¨äºå¤§è§„æ¨¡ã€æ— ç•Œçš„å¤–æ™¯é©¾é©¶åœºæ™¯ï¼Œè¾“å‡ºå‚æ•°åŒ–çš„ä¸‰å¹³é¢ï¼Œå¹¶åœ¨æ¨ç†æ—¶é—´å†…ä»å•ä¸€æ—¶é—´æˆ³çš„å…­ä¸ªç¯ç»•è§†å›¾è½¦è¾†å›¾åƒé‡å»ºå‡ºå®Œæ•´çš„åœºæ™¯ã€‚é€šè¿‡è‡ªå®šä¹‰çš„äº¤å‰å’Œè‡ªæ³¨æ„åŠ›æœºåˆ¶ã€å¯å¾®ä½“ç§¯æ¸²æŸ“ã€åœºæ™¯æ”¶ç¼©å’Œå›¾åƒç‰¹å¾æŠ•å½±ç­‰æŠ€æœ¯è§£å†³äº†ç°æœ‰é—®é¢˜ã€‚è¯¥æ–¹æ³•å…è®¸æ¸²æŸ“ç¬¬ä¸‰äººç§°å›¾åƒå’Œé¸Ÿç°å›¾ç­‰ã€‚ä»£ç å¯åœ¨æŒ‡å®šçš„GitHubç½‘å€æ‰¾åˆ°ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>å½“å‰ä¸‰ç»´é‡å»ºæŠ€æœ¯é¢ä¸´çš„æŒ‘æˆ˜åŒ…æ‹¬é«˜è®¡ç®—éœ€æ±‚ã€ä¾èµ–è¯¦ç»†å§¿æ€ä¿¡æ¯å’Œæ— æ³•å¯é é‡å»ºé®æŒ¡åŒºåŸŸç­‰ã€‚</li>
<li>ä»‹ç»äº†ä¸€ç§åŸºäºTransformerçš„æ–°æ–¹æ³•â€”â€”6Img-to-3Dï¼Œå¯ä»å°‘é‡å›¾åƒè¿›è¡Œé«˜æ•ˆçš„ä¸‰ç»´é‡å»ºã€‚</li>
<li>è¯¥æ–¹æ³•é€‚ç”¨äºå¤§è§„æ¨¡ã€æ— ç•Œçš„å¤–æ™¯é©¾é©¶åœºæ™¯çš„é‡å»ºï¼Œè¾“å‡ºå‚æ•°åŒ–çš„ä¸‰å¹³é¢ã€‚</li>
<li>6Img-to-3Dèƒ½å¤Ÿåœ¨æ²¡æœ‰å…¨å±€å§¿æ€ä¿¡æ¯çš„æƒ…å†µä¸‹ï¼Œä»å•ä¸€æ—¶é—´ç‚¹çš„å…­ä¸ªç¯ç»•è§†å›¾è½¦è¾†å›¾åƒé‡å»ºå‡ºå®Œæ•´çš„åœºæ™¯ã€‚</li>
<li>æ–¹æ³•ç»“åˆäº†è‡ªå®šä¹‰çš„äº¤å‰å’Œè‡ªæ³¨æ„åŠ›æœºåˆ¶ã€å¯å¾®ä½“ç§¯æ¸²æŸ“ç­‰æŠ€æœ¯æ¥è§£å†³ç°æœ‰é—®é¢˜ã€‚</li>
<li>è¯¥æ–¹æ³•å…è®¸æ¸²æŸ“å¤šç§è§†å›¾ï¼Œå¦‚ç¬¬ä¸‰äººç§°å›¾åƒå’Œé¸Ÿç°å›¾ç­‰ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2404.12378">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-3a87c52f9dc79cacea1c83f38d6542fd.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-5f7f481f38b0b909e7588008966be7ec.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-66c48f63ff7d18f28e711954020c15a0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c018142052df070b13f444b4e57b4a21.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-3a6c0aa57e3766a53d333e78a27787d3.jpg" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="ARC-NeRF-Area-Ray-Casting-for-Broader-Unseen-View-Coverage-in-Few-shot-Object-Rendering"><a href="#ARC-NeRF-Area-Ray-Casting-for-Broader-Unseen-View-Coverage-in-Few-shot-Object-Rendering" class="headerlink" title="ARC-NeRF: Area Ray Casting for Broader Unseen View Coverage in Few-shot   Object Rendering"></a>ARC-NeRF: Area Ray Casting for Broader Unseen View Coverage in Few-shot   Object Rendering</h2><p><strong>Authors:Seunghyeon Seo, Yeonjin Chang, Jayeon Yoo, Seungwoo Lee, Hojun Lee, Nojun Kwak</strong></p>
<p>Recent advancements in the Neural Radiance Field (NeRF) have enhanced its capabilities for novel view synthesis, yet its reliance on dense multi-view training images poses a practical challenge, often leading to artifacts and a lack of fine object details. Addressing this, we propose ARC-NeRF, an effective regularization-based approach with a novel Area Ray Casting strategy. While the previous ray augmentation methods are limited to covering only a single unseen view per extra ray, our proposed Area Ray covers a broader range of unseen views with just a single ray and enables an adaptive high-frequency regularization based on target pixel photo-consistency. Moreover, we propose luminance consistency regularization, which enhances the consistency of relative luminance between the original and Area Ray, leading to more accurate object textures. The relative luminance, as a free lunch extra data easily derived from RGB images, can be effectively utilized in few-shot scenarios where available training data is limited. Our ARC-NeRF outperforms its baseline and achieves competitive results on multiple benchmarks with sharply rendered fine details. </p>
<blockquote>
<p>æœ€è¿‘ç¥ç»è¾å°„åœºï¼ˆNeRFï¼‰çš„è¿›å±•å¢å¼ºäº†å…¶ç”¨äºæ–°å‹è§†å›¾åˆæˆçš„èƒ½åŠ›ï¼Œç„¶è€Œï¼Œå®ƒå¯¹å¯†é›†å¤šè§†å›¾è®­ç»ƒå›¾åƒçš„ä¾èµ–æ„æˆäº†ä¸€ä¸ªå®é™…æŒ‘æˆ˜ï¼Œå¾€å¾€å¯¼è‡´å‡ºç°ä¼ªå½±å’Œç¼ºä¹ç²¾ç»†çš„å¯¹è±¡ç»†èŠ‚ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºARC-NeRFï¼Œè¿™æ˜¯ä¸€ç§åŸºäºæœ‰æ•ˆæ­£åˆ™åŒ–çš„æ–¹æ³•ï¼Œé‡‡ç”¨äº†æ–°å‹çš„åŒºåŸŸå…‰çº¿æŠ•å°„ç­–ç•¥ã€‚è™½ç„¶ä¹‹å‰çš„å…‰çº¿å¢å¼ºæ–¹æ³•ä»…é™äºæ¯æ¡é¢å¤–å…‰çº¿åªè¦†ç›–ä¸€ä¸ªæœªè§è§†å›¾ï¼Œä½†æˆ‘ä»¬æå‡ºçš„åŒºåŸŸå…‰çº¿å¯ä»¥ä»…ä½¿ç”¨ä¸€æ¡å…‰çº¿è¦†ç›–æ›´å¹¿æ³›çš„æœªè§è§†å›¾èŒƒå›´ï¼Œå¹¶å®ç°åŸºäºç›®æ ‡åƒç´ ç…§ç‰‡ä¸€è‡´æ€§çš„è‡ªé€‚åº”é«˜é¢‘æ­£åˆ™åŒ–ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜æå‡ºäº†äº®åº¦ä¸€è‡´æ€§æ­£åˆ™åŒ–ï¼Œå¢å¼ºäº†åŸå§‹å›¾åƒå’ŒåŒºåŸŸå…‰çº¿ä¹‹é—´ç›¸å¯¹äº®åº¦çš„ä¸€è‡´æ€§ï¼Œä»è€Œå¾—åˆ°æ›´ç²¾ç¡®çš„å¯¹è±¡çº¹ç†ã€‚ç›¸å¯¹äº®åº¦ä½œä¸ºå®¹æ˜“ä»RGBå›¾åƒä¸­è·å¾—çš„é¢å¤–æ•°æ®ï¼Œå¯ä»¥åœ¨è®­ç»ƒæ•°æ®æœ‰é™çš„å°‘æ•°åœºæ™¯ä¸­æœ‰æ•ˆåˆ©ç”¨ã€‚æˆ‘ä»¬çš„ARC-NeRFè¶…è¶Šäº†åŸºçº¿æ–¹æ³•ï¼Œå¹¶åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­å®ç°äº†å‡ºè‰²çš„è¡¨ç°ï¼Œèƒ½å¤Ÿæ¸…æ™°åœ°å‘ˆç°ç²¾ç»†ç»†èŠ‚ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2403.10906v2">PDF</a> CVPR 2025 Workshop: 4th Computer Vision for Metaverse Workshop</p>
<p><strong>Summary</strong></p>
<p>ç¥ç»ç½‘ç»œè¾å°„åœºï¼ˆNeRFï¼‰æŠ€æœ¯çš„æœ€æ–°è¿›å±•å¢å¼ºäº†å…¶æ–°é¢–çš„è§†å›¾åˆæˆèƒ½åŠ›ï¼Œä½†å®ƒå¯¹å¯†é›†çš„å¤šè§†å›¾è®­ç»ƒå›¾åƒçš„ä¾èµ–å¸¦æ¥äº†å®é™…æ“ä½œä¸­çš„æŒ‘æˆ˜ï¼Œå¯èƒ½ä¼šå¯¼è‡´ä¼ªå½±å’Œç¼ºä¹ç²¾ç»†çš„å¯¹è±¡ç»†èŠ‚ã€‚ä¸ºè§£å†³æ­¤é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ARC-NeRFï¼Œè¿™æ˜¯ä¸€ç§åŸºäºæ­£åˆ™åŒ–çš„æœ‰æ•ˆæ–¹æ³•ï¼Œå¹¶é‡‡ç”¨äº†æ–°å‹çš„åŒºåŸŸå…‰çº¿æŠ•å°„ç­–ç•¥ã€‚ä¸ä¹‹å‰çš„å…‰çº¿å¢å¼ºæ–¹æ³•ä¸åŒï¼Œæˆ‘ä»¬çš„åŒºåŸŸå…‰çº¿èƒ½å¤Ÿåœ¨å•æ¬¡æŠ•å°„ä¸­è¦†ç›–æ›´å¹¿æ³›çš„æœªè§è§†å›¾ï¼Œå¹¶å®ç°åŸºäºç›®æ ‡åƒç´ ç…§ç‰‡ä¸€è‡´æ€§çš„è‡ªé€‚åº”é«˜é¢‘æ­£åˆ™åŒ–ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜æå‡ºäº†äº®åº¦ä¸€è‡´æ€§æ­£åˆ™åŒ–ï¼Œå¢å¼ºäº†åŸå§‹å›¾åƒå’ŒåŒºåŸŸå…‰çº¿ä¹‹é—´çš„ç›¸å¯¹äº®åº¦ä¸€è‡´æ€§ï¼Œä»è€Œå¾—åˆ°æ›´ç²¾ç¡®çš„å¯¹è±¡çº¹ç†ã€‚ç›¸å¯¹äº®åº¦ä½œä¸ºæ¥è‡ªRGBå›¾åƒçš„é¢å¤–æ•°æ®ï¼Œå¯ä»¥åœ¨å°‘é‡è®­ç»ƒæ•°æ®çš„æƒ…å†µä¸‹å¾—åˆ°æœ‰æ•ˆåˆ©ç”¨ã€‚ARC-NeRFè¶…è¶Šäº†åŸºçº¿æ–¹æ³•ï¼Œå¹¶åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­å®ç°äº†ç²¾ç»†çš„æ¸²æŸ“æ•ˆæœã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>NeRFæŠ€æœ¯åœ¨æ–°å‹è§†å›¾åˆæˆæ–¹é¢å±•ç°å‡ºè¿›å±•ï¼Œä½†ä¾èµ–å¯†é›†å¤šè§†å›¾è®­ç»ƒå›¾åƒå¯¼è‡´å®é™…åº”ç”¨ä¸­çš„æŒ‘æˆ˜ã€‚</li>
<li>ARC-NeRFé€šè¿‡æœ‰æ•ˆçš„æ­£åˆ™åŒ–æ–¹æ³•å’Œæ–°å‹åŒºåŸŸå…‰çº¿æŠ•å°„ç­–ç•¥è§£å†³äº†è¿™ä¸€é—®é¢˜ã€‚</li>
<li>åŒºåŸŸå…‰çº¿èƒ½å¤Ÿåœ¨å•æ¬¡æŠ•å°„ä¸­è¦†ç›–æ›´å¹¿æ³›çš„æœªè§è§†å›¾ã€‚</li>
<li>ARC-NeRFå®ç°äº†åŸºäºç›®æ ‡åƒç´ ç…§ç‰‡ä¸€è‡´æ€§çš„è‡ªé€‚åº”é«˜é¢‘æ­£åˆ™åŒ–ã€‚</li>
<li>äº®åº¦ä¸€è‡´æ€§æ­£åˆ™åŒ–å¢å¼ºäº†åŸå§‹å›¾åƒå’ŒåŒºåŸŸå…‰çº¿ä¹‹é—´çš„ç›¸å¯¹äº®åº¦ä¸€è‡´æ€§ã€‚</li>
<li>ç›¸å¯¹äº®åº¦ä¿¡æ¯å¯ä½œä¸ºæ¥è‡ªRGBå›¾åƒçš„é¢å¤–æ•°æ®ï¼Œåœ¨è®­ç»ƒæ•°æ®æœ‰é™çš„æƒ…å†µä¸‹æä¾›æœ‰æ•ˆå¸®åŠ©ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2403.10906">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-29ae4f85a7b28fd50d9d2762409032f4.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f57da3615c8eb9d74925d80c987ef4f2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d8a15ff01bb329f42513133df2ad9003.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-31310efbe9b4a478693df17b94ac64fb.jpg" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-04-09/Few-Shot/">https://kedreamix.github.io/Talk2Paper/Paper/2025-04-09/Few-Shot/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Few-Shot/">
                                    <span class="chip bg-color">Few-Shot</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-04-09/I2I%20Translation/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-77c2e0afe5ba79c7c7378ff2be818586.jpg" class="responsive-img" alt="I2I Translation">
                        
                        <span class="card-title">I2I Translation</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            I2I Translation æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-04-09  GAMBAS Generalised-Hilbert Mamba for Super-resolution of Paediatric   Ultra-Low-Field MRI
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-04-09
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/I2I-Translation/" class="post-category">
                                    I2I Translation
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/I2I-Translation/">
                        <span class="chip bg-color">I2I Translation</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-04-09/Agent/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-ca7525f9b77e52545650c04458c2cc6c.jpg" class="responsive-img" alt="Agent">
                        
                        <span class="card-title">Agent</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Agent æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-04-09  CREA A Collaborative Multi-Agent Framework for Creative Content   Generation with Diffusion Models
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-04-09
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Agent/" class="post-category">
                                    Agent
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Agent/">
                        <span class="chip bg-color">Agent</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">19710k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
