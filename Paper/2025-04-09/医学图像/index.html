<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="åŒ»å­¦å›¾åƒ">
    <meta name="description" content="åŒ»å­¦å›¾åƒ æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-04-09  Federated Learning for Medical Image Classification A Comprehensive   Benchmark">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>åŒ»å­¦å›¾åƒ | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-d9964f7ed391a1d68c160081be409351.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">åŒ»å­¦å›¾åƒ</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                                <span class="chip bg-color">åŒ»å­¦å›¾åƒ</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/" class="post-category">
                                åŒ»å­¦å›¾åƒ
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-04-09
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-05-14
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    17.5k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    72 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-04-09-æ›´æ–°"><a href="#2025-04-09-æ›´æ–°" class="headerlink" title="2025-04-09 æ›´æ–°"></a>2025-04-09 æ›´æ–°</h1><h2 id="Federated-Learning-for-Medical-Image-Classification-A-Comprehensive-Benchmark"><a href="#Federated-Learning-for-Medical-Image-Classification-A-Comprehensive-Benchmark" class="headerlink" title="Federated Learning for Medical Image Classification: A Comprehensive   Benchmark"></a>Federated Learning for Medical Image Classification: A Comprehensive   Benchmark</h2><p><strong>Authors:Zhekai Zhou, Guibo Luo, Mingzhi Chen, Zhenyu Weng, Yuesheng Zhu</strong></p>
<p>The federated learning paradigm is wellsuited for the field of medical image analysis, as it can effectively cope with machine learning on isolated multicenter data while protecting the privacy of participating parties. However, current research on optimization algorithms in federated learning often focuses on limited datasets and scenarios, primarily centered around natural images, with insufficient comparative experiments in medical contexts. In this work, we conduct a comprehensive evaluation of several state-of-the-art federated learning algorithms in the context of medical imaging. We conduct a fair comparison of classification models trained using various federated learning algorithms across multiple medical imaging datasets. Additionally, we evaluate system performance metrics, such as communication cost and computational efficiency, while considering different federated learning architectures. Our findings show that medical imaging datasets pose substantial challenges for current federated learning optimization algorithms. No single algorithm consistently delivers optimal performance across all medical federated learning scenarios, and many optimization algorithms may underperform when applied to these datasets. Our experiments provide a benchmark and guidance for future research and application of federated learning in medical imaging contexts. Furthermore, we propose an efficient and robust method that combines generative techniques using denoising diffusion probabilistic models with label smoothing to augment datasets, widely enhancing the performance of federated learning on classification tasks across various medical imaging datasets. Our code will be released on GitHub, offering a reliable and comprehensive benchmark for future federated learning studies in medical imaging. </p>
<blockquote>
<p>è”é‚¦å­¦ä¹ èŒƒå¼éå¸¸é€‚åˆåŒ»å­¦å›¾åƒåˆ†æé¢†åŸŸï¼Œå› ä¸ºå®ƒèƒ½å¤Ÿåœ¨ä¿æŠ¤å‚ä¸æ–¹éšç§çš„åŒæ—¶ï¼Œæœ‰æ•ˆåº”å¯¹å­¤ç«‹çš„å¤šä¸­å¿ƒæ•°æ®è¿›è¡Œæœºå™¨å­¦ä¹ ã€‚ç„¶è€Œï¼Œç›®å‰è”é‚¦å­¦ä¹ ä¸­çš„ä¼˜åŒ–ç®—æ³•ç ”ç©¶ä¸»è¦é›†ä¸­åœ¨æœ‰é™çš„æ•°æ®é›†å’Œåœºæ™¯ä¸Šï¼Œä¸»è¦å›´ç»•è‡ªç„¶å›¾åƒå±•å¼€ï¼Œåœ¨åŒ»ç–—ç¯å¢ƒä¸‹çš„å¯¹æ¯”å®éªŒä¸è¶³ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬å¯¹åŒ»ç–—æˆåƒèƒŒæ™¯ä¸‹çš„å‡ ç§æœ€æ–°è”é‚¦å­¦ä¹ ç®—æ³•è¿›è¡Œäº†å…¨é¢è¯„ä¼°ã€‚æˆ‘ä»¬å…¬å¹³åœ°æ¯”è¾ƒäº†ä½¿ç”¨å„ç§è”é‚¦å­¦ä¹ ç®—æ³•åœ¨å¤šä¸ªåŒ»å­¦æˆåƒæ•°æ®é›†ä¸Šè®­ç»ƒçš„åˆ†ç±»æ¨¡å‹ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜è¯„ä¼°äº†ç³»ç»Ÿæ€§èƒ½æŒ‡æ ‡ï¼Œå¦‚é€šä¿¡æˆæœ¬å’Œè®¡ç®—æ•ˆç‡ï¼ŒåŒæ—¶è€ƒè™‘äº†ä¸åŒçš„è”é‚¦å­¦ä¹ æ¶æ„ã€‚æˆ‘ä»¬çš„ç ”ç©¶å‘ç°ï¼ŒåŒ»å­¦æˆåƒæ•°æ®é›†å¯¹å½“å‰è”é‚¦å­¦ä¹ ä¼˜åŒ–ç®—æ³•æ„æˆäº†é‡å¤§æŒ‘æˆ˜ã€‚æ²¡æœ‰ä¸€ç§ç®—æ³•èƒ½åœ¨æ‰€æœ‰åŒ»å­¦è”é‚¦å­¦ä¹ åœºæ™¯ä¸­å§‹ç»ˆæä¾›æœ€ä½³æ€§èƒ½ï¼Œè®¸å¤šä¼˜åŒ–ç®—æ³•åœ¨è¿™äº›æ•°æ®é›†ä¸Šçš„è¡¨ç°å¯èƒ½ä¼šä¸ä½³ã€‚æˆ‘ä»¬çš„å®éªŒä¸ºè”é‚¦å­¦ä¹ åœ¨åŒ»å­¦æˆåƒé¢†åŸŸæœªæ¥çš„ç ”ç©¶ä¸åº”ç”¨æä¾›äº†åŸºå‡†å’ŒæŒ‡å¯¼ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§é«˜æ•ˆä¸”ç¨³å¥çš„æ–¹æ³•ï¼Œç»“åˆå»å™ªæ‰©æ•£æ¦‚ç‡æ¨¡å‹çš„ç”ŸæˆæŠ€æœ¯ä¸æ ‡ç­¾å¹³æ»‘æ¥å¢å¼ºæ•°æ®é›†ï¼Œä»è€Œå¹¿æ³›æé«˜äº†è”é‚¦å­¦ä¹ åœ¨è·¨å„ç§åŒ»å­¦æˆåƒæ•°æ®é›†çš„åˆ†ç±»ä»»åŠ¡ä¸Šçš„æ€§èƒ½ã€‚æˆ‘ä»¬çš„ä»£ç å°†åœ¨GitHubä¸Šå‘å¸ƒï¼Œä¸ºåŒ»å­¦æˆåƒä¸­çš„è”é‚¦å­¦ä¹ ç ”ç©¶æä¾›å¯é å’Œå…¨é¢çš„åŸºå‡†ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.05238v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ç ”ç©¶äº†è”é‚¦å­¦ä¹ ç®—æ³•åœ¨åŒ»å­¦å›¾åƒåˆ†æé¢†åŸŸçš„è¡¨ç°ï¼Œå‘ç°å½“å‰è”é‚¦å­¦ä¹ ä¼˜åŒ–ç®—æ³•åœ¨åŒ»å­¦å›¾åƒæ•°æ®é›†ä¸ŠæŒ‘æˆ˜è¾ƒå¤§ï¼Œæ— å•ä¸€ç®—æ³•èƒ½åœ¨æ‰€æœ‰åœºæ™¯ä¸­éƒ½è¡¨ç°æœ€ä¼˜ã€‚åŒæ—¶ï¼Œæå‡ºäº†ç»“åˆç”ŸæˆæŠ€æœ¯ä¸æ ‡ç­¾å¹³æ»‘çš„æ–¹æ³•ï¼Œèƒ½æœ‰æ•ˆå¢å¼ºæ•°æ®é›†ï¼Œæé«˜è”é‚¦å­¦ä¹ åœ¨åŒ»å­¦å›¾åƒåˆ†ç±»ä»»åŠ¡ä¸Šçš„æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è”é‚¦å­¦ä¹ èŒƒå¼é€‚åˆåŒ»å­¦å›¾åƒåˆ†æé¢†åŸŸï¼Œèƒ½æœ‰æ•ˆå¤„ç†å¤šä¸­å¿ƒæ•°æ®å¹¶ä¿æŠ¤éšç§ã€‚</li>
<li>å½“å‰è”é‚¦å­¦ä¹ ä¼˜åŒ–ç®—æ³•åœ¨åŒ»å­¦å›¾åƒæ•°æ®é›†ä¸Šçš„è¡¨ç°é¢ä¸´æŒ‘æˆ˜ã€‚</li>
<li>ä¸åŒçš„è”é‚¦å­¦ä¹ ç®—æ³•åœ¨ä¸åŒåŒ»å­¦å›¾åƒæ•°æ®é›†ä¸Šçš„è¡¨ç°å­˜åœ¨å·®å¼‚ï¼Œæ— å•ä¸€æœ€ä¼˜ç®—æ³•ã€‚</li>
<li>åŒ»å­¦å›¾åƒæ•°æ®é›†çš„åº”ç”¨ç»™ç°æœ‰è”é‚¦å­¦ä¹ ä¼˜åŒ–ç®—æ³•å¸¦æ¥å®è´¨æŒ‘æˆ˜ã€‚</li>
<li>ç»“åˆç”ŸæˆæŠ€æœ¯ä¸æ ‡ç­¾å¹³æ»‘çš„æ–¹æ³•èƒ½å¹¿æ³›å¢å¼ºæ•°æ®é›†ï¼Œæé«˜è”é‚¦å­¦ä¹ åœ¨åŒ»å­¦å›¾åƒåˆ†ç±»ä»»åŠ¡ä¸Šçš„æ€§èƒ½ã€‚</li>
<li>æœ¬æ–‡æä¾›çš„å®éªŒä¸ºæœªæ¥è”é‚¦å­¦ä¹ åœ¨åŒ»å­¦å›¾åƒé¢†åŸŸçš„ç ”ç©¶ä¸åº”ç”¨æä¾›äº†åŸºå‡†å’ŒæŒ‡å¼•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.05238">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-14a8094bd9a6f0dc381941f4fa5ca8ca.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e9bad073ca71121fda9deb96d80b6dd8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-cc765ba8abf7eab4a720c23b90e01a3d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7517b4af62f68a27b990561b44e5fbcd.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-08aeef87367ea136cb9a0a9be8aefe59.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Influence-of-pore-confined-water-on-the-thermal-expansion-of-a-zinc-based-metal-organic-framework"><a href="#Influence-of-pore-confined-water-on-the-thermal-expansion-of-a-zinc-based-metal-organic-framework" class="headerlink" title="Influence of pore-confined water on the thermal expansion of a   zinc-based metal-organic framework"></a>Influence of pore-confined water on the thermal expansion of a   zinc-based metal-organic framework</h2><p><strong>Authors:Nina Strasser, Benedikt Schrode, Ana Torvisco, Sanjay John, Birgit Kunert, Brigitte Bitschnau, Florian Patrick Lindner, Christian Slugovc, Egbert Zojer, Roland Resel</strong></p>
<p>Understanding the reversible intercalation of guest molecules into metal-organic frameworks is crucial for advancing their design for practical applications. In this work, we explore the impact of H$<em>{\mathrm{2}}!$O as a guest molecule on the thermal expansion of the zinc-based metal-organic framework GUT-2. Dehydration is achieved by thermal treatment of hydrated GUT-2. Rietveld refinement performed on temperature-dependent X-ray powder diffraction data confirms the reversible structural transformation. Additionally, it allows the determination of anisotropic thermal expansion coefficients for both phases. The hydrated form exhibits near-zero thermal expansion along the polymer chain direction, moderate expansion In the direction of predominantly hydrogen bonds, and the highest expansion in the direction with only Van der Waals bonding. Upon activation, the removal of H$</em>{\mathrm{2}}!$O molecules triggers a doubling of the thermal expansion coefficient in the direction, where the hydrogen bonds have been removed. Regarding the dynamics of the process, thermal activation in air occurs within 6 hours at a temperature of 50{\deg}C and takes only 30 minutes when heating to 90{\deg}C. In contrast, full rehydration under standard lab conditions (30 % relative humidity) requires two days. During the activation&#x2F;dehydration processes no change of the widths of the X-ray diffraction peaks is observed, which shows that the underlying crystal structures remains fully intact during the transition processes. Fitting the transformations by the Avrami equation reveals a quasi one-dimensional evolution of the dehydrated areas for the activation process and a more intricate, predominantly two-dimensional mechanism for the rehydration. </p>
<blockquote>
<p>äº†è§£å®¢ä½“åˆ†å­å¯é€†æ’å…¥é‡‘å±æœ‰æœºæ¡†æ¶å¯¹äºæ¨åŠ¨å…¶åœ¨å®é™…åº”ç”¨ä¸­çš„è®¾è®¡è‡³å…³é‡è¦ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æ¢ç´¢äº†æ°´ä½œä¸ºå®¢ä½“åˆ†å­å¯¹é”ŒåŸºé‡‘å±æœ‰æœºæ¡†æ¶GUT-2çƒ­è†¨èƒ€çš„å½±å“ã€‚é€šè¿‡çƒ­å¤„ç†æ°´åˆGUT-2æ¥å®ç°è„±æ°´ã€‚é‡Œç‰¹ç»´å°”å¾·ï¼ˆRietveldï¼‰å¯¹æ¸©åº¦ä¾èµ–çš„Xå°„çº¿ç²‰æœ«è¡å°„æ•°æ®è¿›è¡Œäº†ä¿®æ­£ï¼Œè¯å®äº†å¯é€†ç»“æ„è½¬å˜ã€‚æ­¤å¤–ï¼Œå®ƒè¿˜å…è®¸ç¡®å®šä¸¤ä¸ªé˜¶æ®µçš„çƒ­è†¨èƒ€ç³»æ•°ã€‚æ°´åˆå½¢å¼åœ¨èšåˆç‰©é“¾æ–¹å‘ä¸Šè¡¨ç°å‡ºè¿‘é›¶çƒ­è†¨èƒ€ï¼Œåœ¨ä¸»è¦ç”±æ°¢é”®æ–¹å‘ä¸Šæœ‰ä¸­ç­‰è†¨èƒ€ï¼Œä»¥åŠåœ¨ä»…é€šè¿‡èŒƒå¾·ååŠ›ç»“åˆçš„æ–¹å‘ä¸Šæœ‰æœ€é«˜è†¨èƒ€ã€‚æ´»åŒ–åï¼ŒHâ‚‚Oåˆ†å­çš„å»é™¤å¯¼è‡´åœ¨æ°¢é”®è¢«ç§»é™¤çš„æ–¹å‘ä¸Šçƒ­è†¨èƒ€ç³»æ•°åŠ å€ã€‚å…³äºè¿‡ç¨‹çš„åŠ¨æ€ï¼Œåœ¨ç©ºæ°”ä¸­çš„çƒ­æ´»åŒ–åœ¨50æ‘„æ°åº¦çš„æ¸©åº¦ä¸‹åœ¨6å°æ—¶å†…å‘ç”Ÿï¼Œå½“åŠ çƒ­åˆ°90æ‘„æ°åº¦æ—¶ä»…éœ€30åˆ†é’Ÿã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼Œåœ¨æ ‡å‡†å®éªŒå®¤æ¡ä»¶ä¸‹ï¼ˆ30%ç›¸å¯¹æ¹¿åº¦ï¼‰å®Œå…¨é‡æ–°æ°´åˆéœ€è¦ä¸¤å¤©ã€‚åœ¨æ´»åŒ–&#x2F;è„±æ°´è¿‡ç¨‹ä¸­ï¼Œè§‚å¯Ÿåˆ°Xå°„çº¿è¡å°„å³°å®½åº¦æ²¡æœ‰å˜åŒ–ï¼Œè¿™è¡¨æ˜åœ¨è¿‡æ¸¡è¿‡ç¨‹ä¸­åŸºç¡€æ™¶ä½“ç»“æ„ä¿æŒå®Œæ•´ã€‚é€šè¿‡é˜¿å¤«æ‹‰ç±³æ–¹ç¨‹ï¼ˆAvrami equationï¼‰æ‹Ÿåˆå˜å½¢æ­ç¤ºï¼Œè„±æ°´åŒºåŸŸçš„æ´»åŒ–è¿‡ç¨‹å‘ˆç°å‡†ä¸€ç»´æ¼”å˜ï¼Œè€Œå†æ°´åˆè¿‡ç¨‹åˆ™æ›´ä¸ºå¤æ‚ï¼Œä¸»è¦æ˜¯äºŒç»´æœºåˆ¶ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.05189v1">PDF</a> </p>
<p><strong>æ‘˜è¦</strong><br>    æ°´åˆ†å­ä½œä¸ºå®¢ä½“åˆ†å­å¯¹é”ŒåŸºé‡‘å±æœ‰æœºæ¡†æ¶GUT-2çš„çƒ­è†¨èƒ€å…·æœ‰é‡è¦å½±å“ã€‚é€šè¿‡çƒ­å¤„ç†è„±æ°´ï¼Œå€ŸåŠ©ç‘ç‰¹éŸ¦å°”å¾·ä¿®æ­£æ³•å¯¹æ¸©åº¦ä¾èµ–çš„Xå°„çº¿ç²‰æœ«è¡å°„æ•°æ®è¿›è¡Œåˆ†æï¼Œè¯å®äº†å¯é€†ç»“æ„è½¬å˜çš„å­˜åœ¨ã€‚å„æ–¹å‘çš„çƒ­è†¨èƒ€ç³»æ•°å¯ç”±æ­¤ç¡®å®šã€‚æ°´åˆå½¢å¼åœ¨æŸäº›æ–¹å‘è¡¨ç°å‡ºè¿‘é›¶çƒ­è†¨èƒ€ï¼Œåœ¨æ°¢é”®ä¸»å¯¼çš„æ–¹å‘ä¸Šè¡¨ç°ä¸­ç­‰è†¨èƒ€ï¼Œè€Œåœ¨ä»…å­˜åœ¨èŒƒå¾·åé”®çš„æ–¹å‘ä¸Šè†¨èƒ€æœ€é«˜ã€‚æ¿€æ´»åå»é™¤æ°´åˆ†å­ä½¿å¾—çƒ­è†¨èƒ€ç³»æ•°åŠ å€ã€‚æ¿€æ´»è¿‡ç¨‹åœ¨ç©ºæ°”åŠ çƒ­è‡³50â„ƒæ—¶å¯åœ¨6å°æ—¶å†…å®Œæˆï¼ŒåŠ çƒ­è‡³90â„ƒæ—¶ä»…éœ€30åˆ†é’Ÿã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼Œåœ¨æ ‡å‡†å®éªŒå®¤æ¡ä»¶ä¸‹ï¼ˆç›¸å¯¹æ¹¿åº¦30%ï¼‰å®Œå…¨å¤æ°´éœ€è¦ä¸¤å¤©æ—¶é—´ã€‚æ´»åŒ–è„±æ°´è¿‡ç¨‹ä¸­è§‚å¯Ÿåˆ°Xå°„çº¿è¡å°„å³°å®½æ²¡æœ‰å˜åŒ–ï¼Œè¡¨æ˜æ™¶ä½“ç»“æ„åœ¨è½¬å˜è¿‡ç¨‹ä¸­ä¿æŒå®Œæ•´ã€‚é€šè¿‡é˜¿å¤«æ‹‰ç±³æ–¹ç¨‹æ‹Ÿåˆæ­ç¤ºäº†è„±æ°´åŒºåŸŸçš„å‡†ä¸€ç»´æ¼”åŒ–è¿‡ç¨‹å’Œå¤æ°´è¿‡ç¨‹çš„å¤æ‚äºŒç»´æœºåˆ¶ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>ç†è§£å®¢ä½“åˆ†å­ï¼ˆå¦‚æ°´ï¼‰åœ¨é‡‘å±æœ‰æœºæ¡†æ¶ä¸­çš„å¯é€†æ’å±‚å¯¹äºæ¨åŠ¨å…¶å®é™…åº”ç”¨è®¾è®¡è‡³å…³é‡è¦ã€‚</li>
<li>é€šè¿‡çƒ­å¤„ç†å®ç°è„±æ°´çŠ¶æ€ï¼Œå¹¶åˆ©ç”¨ç‘ç‰¹éŸ¦å°”å¾·ä¿®æ­£æ³•åˆ†ææ¸©åº¦ä¾èµ–çš„Xå°„çº¿ç²‰æœ«è¡å°„æ•°æ®ï¼Œæ­ç¤ºäº†å¯é€†ç»“æ„è½¬å˜ã€‚</li>
<li>æ°´åˆå½¢å¼çš„GUT-2åœ¨ä¸åŒæ–¹å‘ä¸Šå±•ç°å‡ºä¸åŒçš„çƒ­è†¨èƒ€ç‰¹æ€§ï¼Œå…¶ä¸­æŸäº›æ–¹å‘çš„çƒ­è†¨èƒ€å—æ°´åˆ†å­å½±å“æ˜¾è‘—ã€‚</li>
<li>æ¿€æ´»è¿‡ç¨‹ä¸­å»é™¤æ°´åˆ†å­å¯¼è‡´çƒ­è†¨èƒ€ç³»æ•°å¢åŠ ã€‚</li>
<li>åŠ çƒ­è‡³è¾ƒé«˜æ¸©åº¦ï¼ˆå¦‚90â„ƒï¼‰å¯æ˜¾è‘—ç¼©çŸ­æ¿€æ´»è¿‡ç¨‹çš„æ—¶é—´ã€‚</li>
<li>å¤æ°´è¿‡ç¨‹éœ€è¦è¾ƒé•¿æ—¶é—´ï¼Œè¡¨æ˜å…¶ä¸æ´»åŒ–è¿‡ç¨‹åŠ¨åŠ›å­¦æœºåˆ¶ä¸åŒã€‚</li>
<li>æ™¶ä½“ç»“æ„åœ¨æ´»åŒ–è„±æ°´è¿‡ç¨‹ä¸­ä¿æŒå®Œæ•´ï¼Œæœªè§‚å¯Ÿåˆ°Xå°„çº¿è¡å°„å³°å®½çš„å˜åŒ–ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.05189">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-255bfbaea0084a66f32542d8b38ff8eb.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-28a34b00cfe8799df178569bf959fef4.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a252bcb508fa6f07545ca28c8d1878d1.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-0690478438e9800b772332a8165452c5.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="MSA-UNet3-Multi-Scale-Attention-UNet3-with-New-Supervised-Prototypical-Contrastive-Loss-for-Coronary-DSA-Image-Segmentation"><a href="#MSA-UNet3-Multi-Scale-Attention-UNet3-with-New-Supervised-Prototypical-Contrastive-Loss-for-Coronary-DSA-Image-Segmentation" class="headerlink" title="MSA-UNet3+: Multi-Scale Attention UNet3+ with New Supervised   Prototypical Contrastive Loss for Coronary DSA Image Segmentation"></a>MSA-UNet3+: Multi-Scale Attention UNet3+ with New Supervised   Prototypical Contrastive Loss for Coronary DSA Image Segmentation</h2><p><strong>Authors:Rayan Merghani Ahmed, Adnan Iltaf, Bin Li, Shoujun Zhou</strong></p>
<p>The accurate segmentation of coronary Digital Subtraction Angiography (DSA) images is essential for diagnosing and treating coronary artery diseases. Despite advances in deep learning-based segmentation, challenges such as low contrast, noise, overlapping structures, high intra-class variance, and class imbalance limit precise vessel delineation. To overcome these limitations, we propose the MSA-UNet3+: a Multi-Scale Attention enhanced UNet3+ architecture for coronary DSA image segmentation. The framework combined Multi-Scale Dilated Bottleneck (MSD-Bottleneck) with Contextual Attention Fusion Module (CAFM), which not only enhances multi-scale feature extraction but also preserve fine-grained details, and improve contextual understanding. Furthermore, we propose a new Supervised Prototypical Contrastive Loss (SPCL), which combines supervised and prototypical contrastive learning to minimize class imbalance and high intra-class variance by focusing on hard-to-classified background samples. Experiments carried out on a private coronary DSA dataset demonstrate that MSA-UNet3+ outperforms state-of-the-art methods, achieving a Dice coefficient of 87.73%, an F1-score of 87.78%, and significantly reduced Average Surface Distance (ASD) and Average Contour Distance (ACD). The developed framework provides clinicians with precise vessel segmentation, enabling accurate identification of coronary stenosis and supporting informed diagnostic and therapeutic decisions. The code will be released at the following GitHub profile link <a target="_blank" rel="noopener" href="https://github.com/rayanmerghani/MSA-UNet3plus">https://github.com/rayanmerghani/MSA-UNet3plus</a>. </p>
<blockquote>
<p>å† çŠ¶åŠ¨è„‰æ•°å­—å‡å½±è¡€ç®¡é€ å½±ï¼ˆDSAï¼‰å›¾åƒçš„å‡†ç¡®åˆ†å‰²å¯¹äºå† çŠ¶åŠ¨è„‰ç–¾ç—…çš„è¯Šæ–­å’Œæ²»ç–—è‡³å…³é‡è¦ã€‚å°½ç®¡åŸºäºæ·±åº¦å­¦ä¹ çš„åˆ†å‰²æŠ€æœ¯æœ‰æ‰€è¿›å±•ï¼Œä½†ä½å¯¹æ¯”åº¦ã€å™ªå£°ã€ç»“æ„é‡å ã€é«˜ç±»å†…æ–¹å·®å’Œç±»ä¸å¹³è¡¡ç­‰æŒ‘æˆ˜ä»ç„¶é™åˆ¶äº†è¡€ç®¡ç²¾ç¡®å‹¾å‹’ã€‚ä¸ºäº†å…‹æœè¿™äº›å±€é™æ€§ï¼Œæˆ‘ä»¬æå‡ºäº†MSA-UNet3+ï¼šä¸€ç§ç”¨äºå† çŠ¶åŠ¨è„‰DSAå›¾åƒåˆ†å‰²çš„å¤šå°ºåº¦æ³¨æ„åŠ›å¢å¼ºUNet3+æ¶æ„ã€‚è¯¥æ¡†æ¶ç»“åˆäº†å¤šå°ºåº¦è†¨èƒ€ç“¶é¢ˆï¼ˆMSD-Bottleneckï¼‰ä¸ä¸Šä¸‹æ–‡æ³¨æ„åŠ›èåˆæ¨¡å—ï¼ˆCAFMï¼‰ï¼Œè¿™ä¸ä»…å¯ä»¥å¢å¼ºå¤šå°ºåº¦ç‰¹å¾æå–ï¼Œè¿˜å¯ä»¥ä¿ç•™ç»†èŠ‚å¹¶æ”¹è¿›ä¸Šä¸‹æ–‡ç†è§£ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„ç›‘ç£åŸå‹å¯¹æ¯”æŸå¤±ï¼ˆSPCLï¼‰ï¼Œå®ƒå°†ç›‘ç£å­¦ä¹ ä¸åŸå‹å¯¹æ¯”å­¦ä¹ ç›¸ç»“åˆï¼Œé€šè¿‡å…³æ³¨éš¾ä»¥åˆ†ç±»çš„èƒŒæ™¯æ ·æœ¬ï¼Œæœ€å°åŒ–ç±»ä¸å¹³è¡¡å’Œé«˜ç±»å†…æ–¹å·®é—®é¢˜ã€‚åœ¨ç§æœ‰å† çŠ¶åŠ¨è„‰DSAæ•°æ®é›†ä¸Šè¿›è¡Œçš„å®éªŒè¡¨æ˜ï¼ŒMSA-UNet3+ä¼˜äºæœ€å…ˆè¿›çš„æ–¹æ³•ï¼Œè¾¾åˆ°87.73%çš„Diceç³»æ•°ï¼Œ87.78%çš„F1åˆ†æ•°ï¼Œä»¥åŠæ˜¾è‘—é™ä½çš„å¹³å‡è¡¨é¢è·ç¦»ï¼ˆASDï¼‰å’Œå¹³å‡è½®å»“è·ç¦»ï¼ˆACDï¼‰ã€‚æ‰€å¼€å‘çš„æ¡†æ¶ä¸ºä¸´åºŠåŒ»ç”Ÿæä¾›äº†ç²¾ç¡®çš„è¡€ç®¡åˆ†å‰²ï¼Œèƒ½å¤Ÿå‡†ç¡®è¯†åˆ«å† çŠ¶åŠ¨è„‰ç‹­çª„ï¼Œä¸ºåŒ»ç”Ÿæä¾›æœ‰æ ¹æ®çš„è¯Šæ–­å’Œæ²»ç–—å†³ç­–æ”¯æŒã€‚ç›¸å…³ä»£ç å°†å‘å¸ƒåœ¨ä»¥ä¸‹GitHubä¸ªäººä¸»é¡µé“¾æ¥ï¼š<a target="_blank" rel="noopener" href="https://github.com/rayanmerghani/MSA-UNet3plus">é“¾æ¥åœ°å€</a>ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.05184v1">PDF</a> Work in progress</p>
<p><strong>Summary</strong><br>å† çŠ¶åŠ¨è„‰æ•°å­—å‡å½±è¡€ç®¡é€ å½±ï¼ˆDSAï¼‰å›¾åƒçš„å‡†ç¡®åˆ†å‰²å¯¹äºè¯Šæ–­å’Œæ²»ç–—å† çŠ¶åŠ¨è„‰ç–¾ç—…è‡³å…³é‡è¦ã€‚é¢å¯¹ä½å¯¹æ¯”åº¦ã€å™ªå£°ã€ç»“æ„é‡å ã€é«˜ç±»å†…æ–¹å·®å’Œç±»åˆ«ä¸å¹³è¡¡ç­‰æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†MSA-UNet3+æ¶æ„ï¼Œç»“åˆå¤šå°ºåº¦æ‰©å¼ ç“¶é¢ˆå’Œä¸Šä¸‹æ–‡æ³¨æ„åŠ›èåˆæ¨¡å—ï¼Œä»¥æé«˜å¤šå°ºåº¦ç‰¹å¾æå–å’Œç»†èŠ‚ä¿ç•™ï¼Œå¹¶æ”¹å–„ä¸Šä¸‹æ–‡ç†è§£ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜æå‡ºäº†ä¸€ç§æ–°çš„ç›‘ç£åŸå‹å¯¹æ¯”æŸå¤±ï¼ˆSPCLï¼‰ï¼Œé€šè¿‡å…³æ³¨éš¾ä»¥åˆ†ç±»çš„èƒŒæ™¯æ ·æœ¬ï¼Œæœ€å°åŒ–ç±»åˆ«ä¸å¹³è¡¡å’Œé«˜ç±»å†…æ–¹å·®ã€‚å®éªŒè¡¨æ˜ï¼ŒMSA-UNet3+ä¼˜äºç°æœ‰æ–¹æ³•ï¼ŒDiceç³»æ•°ä¸º87.73%ï¼ŒF1åˆ†æ•°ä¸º87.78%ï¼Œå¹³å‡è¡¨é¢è·ç¦»ï¼ˆASDï¼‰å’Œå¹³å‡è½®å»“è·ç¦»ï¼ˆACDï¼‰æ˜¾è‘—é™ä½ã€‚è¯¥æ¡†æ¶ä¸ºä¸´åºŠåŒ»ç”Ÿæä¾›ç²¾ç¡®çš„è¡€ç®¡åˆ†å‰²ï¼Œæœ‰åŠ©äºå‡†ç¡®è¯†åˆ«å† çŠ¶åŠ¨è„‰ç‹­çª„ï¼Œæ”¯æŒè¯Šæ–­å’Œæ²»ç–—çš„å†³ç­–åˆ¶å®šã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å† çŠ¶åŠ¨è„‰DSAå›¾åƒåˆ†å‰²å¯¹è¯Šæ–­å’Œæ²»ç–—å† çŠ¶åŠ¨è„‰ç–¾ç—…è‡³å…³é‡è¦ã€‚</li>
<li>å­˜åœ¨ä½å¯¹æ¯”åº¦ã€å™ªå£°ã€ç»“æ„é‡å ç­‰æŒ‘æˆ˜ï¼Œå½±å“ç²¾ç¡®è¡€ç®¡æç»˜ã€‚</li>
<li>æå‡ºäº†MSA-UNet3+æ¶æ„ï¼Œç»“åˆå¤šå°ºåº¦æ‰©å¼ ç“¶é¢ˆå’Œä¸Šä¸‹æ–‡æ³¨æ„åŠ›èåˆæ¨¡å—ï¼Œæé«˜åˆ†å‰²æ€§èƒ½ã€‚</li>
<li>å¼•å…¥æ–°çš„ç›‘ç£åŸå‹å¯¹æ¯”æŸå¤±ï¼ˆSPCLï¼‰ï¼Œä»¥å¤„ç†ç±»åˆ«ä¸å¹³è¡¡å’Œé«˜ç±»å†…æ–¹å·®é—®é¢˜ã€‚</li>
<li>å®éªŒè¯æ˜MSA-UNet3+åœ¨å† çŠ¶åŠ¨è„‰DSAå›¾åƒåˆ†å‰²ä¸Šè¡¨ç°ä¼˜å¼‚ã€‚</li>
<li>è¯¥æ¡†æ¶æä¾›ç²¾ç¡®çš„è¡€ç®¡åˆ†å‰²ï¼Œæœ‰åŠ©äºè¯†åˆ«å† çŠ¶åŠ¨è„‰ç‹­çª„ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.05184">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-c8239d66f951865a4fc68f9b49d80ef4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4b4af3bcf41e92eda5fe6fbadac12078.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ef4bb09527de970e7e286c1ae5349233.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a3e5a979c6a6e08adfce48cd945944db.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-60f12ae738da6a0f338679fe0130c7cc.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="X-class-flare-on-Dec-31-2023-observed-by-the-Solar-Ultraviolet-Imaging-Telescope-on-board-Aditya-L1"><a href="#X-class-flare-on-Dec-31-2023-observed-by-the-Solar-Ultraviolet-Imaging-Telescope-on-board-Aditya-L1" class="headerlink" title="X-class flare on Dec 31, 2023, observed by the Solar Ultraviolet Imaging   Telescope on board Aditya-L1"></a>X-class flare on Dec 31, 2023, observed by the Solar Ultraviolet Imaging   Telescope on board Aditya-L1</h2><p><strong>Authors:Soumya Roy, Durgesh Tripathi, Vishal Upendran, Sreejith Padinhatteeri, A. N. Ramaprakash, Nived V. N., K. Sankarasubramanian, Sami K. Solanki, Janmejoy Sarkar, Rahul Gopalakrishnan, Rushikesh Deogaonkar, Dibyendu Nandy, Dipankar Banerjee</strong></p>
<p>We present the multi-wavelength study of the ejection of a plasma blob from the limb flare SOL2023-12-31T21:36:00 from NOAA 13536 observed by the Solar Ultraviolet Imaging Telescope (SUIT) on board Aditya-L1. We use SUIT observations along with those from Atmospheric Imaging Assembly (AIA) on board SDO and Spectrometer&#x2F;Telescope for Imaging X-rays (STIX) on board Solar Orbiter to infer the kinematics and thermal nature of the ejected blob and its connection to the associated flare. The observations show that the flare was comprised of two eruptions. The blob was ejected during the first eruption and later accelerated to velocities over 1500 km&#x2F;s measured at a maximum projected height of ~ 178 Mm from the Sunâ€™s surface. The acceleration of the ejected plasma blob is co-temporal with the bursty appearance of the hard X-ray light curve recorded by STIX. Radio spectrogram observations from STEREO-A&#x2F;WAVES and RSTN reveal type III bursts at the same time, indicative of magnetic reconnection. DEM analysis using AIA observations suggests the plasma blob is comprised of cooler and denser plasma in comparison to the ambient corona. To the best of our knowledge, this is the first observation of such a plasma blob in the NUV, providing crucial measurements for eruption thermodynamics. </p>
<blockquote>
<p>æˆ‘ä»¬å¯¹æ¥è‡ªNOAA 13536çš„ç­‰ç¦»å­ä½“å›¢å–·å°„è¿›è¡Œäº†å¤šæ³¢é•¿ç ”ç©¶ï¼Œè¯¥å–·å°„ç”±Aditya-L1ä¸Šçš„å¤ªé˜³ç´«å¤–çº¿æˆåƒæœ›è¿œé•œï¼ˆSUITï¼‰è§‚æµ‹åˆ°çš„SOL2023-12-31T21:36:00çš„è¾¹ç•Œè€€æ–‘å¼•èµ·ã€‚æˆ‘ä»¬ä½¿ç”¨SUITè§‚æµ‹ç»“æœä»¥åŠä¸SDOä¸Šçš„å¤§æ°”æˆåƒä»ªï¼ˆAIAï¼‰å’ŒSolar Orbiterä¸Šçš„Xå°„çº¿æˆåƒå…‰è°±ä»ª&#x2F;æœ›è¿œé•œï¼ˆSTIXï¼‰çš„è§‚æµ‹ç»“æœï¼Œæ¨æ–­å‡ºå–·å°„å‡ºçš„ç­‰ç¦»å­ä½“å›¢çš„åŠ¨æ€ç‰¹æ€§å’Œçƒ­ç‰¹æ€§åŠå…¶ä¸ç›¸å…³è€€æ–‘çš„è”ç³»ã€‚è§‚æµ‹ç»“æœæ˜¾ç¤ºï¼Œè€€æ–‘ç”±ä¸¤æ¬¡çˆ†å‘ç»„æˆã€‚ç­‰ç¦»å­ä½“å›¢åœ¨ç¬¬ä¸€æ¬¡çˆ†å‘æ—¶è¢«å–·å°„å‡ºå»ï¼Œä¹‹ååœ¨è·ç¦»å¤ªé˜³è¡¨é¢æœ€å¤§æŠ•å½±é«˜åº¦çº¦178 Mmå¤„åŠ é€Ÿè‡³è¶…è¿‡1500 km&#x2F;sçš„é€Ÿåº¦ã€‚å–·å°„å‡ºçš„ç­‰ç¦»å­ä½“å›¢åŠ é€Ÿçš„åŒæ—¶ä¼´éšç€STIXè®°å½•çš„ç¡¬Xå°„çº¿å…‰æ›²çº¿çš„çªå‘å¤–è§‚ã€‚æ¥è‡ªSTEREO-A&#x2F;WAVESå’ŒRSTNçš„æ— çº¿ç”µé¢‘è°±è§‚æµ‹æ˜¾ç¤ºåŒæ—¶å‡ºç°äº†â…¢å‹çˆ†å‘ï¼Œè¿™æ ‡å¿—ç€ç£åœºé‡è”ã€‚ä½¿ç”¨AIAè§‚æµ‹ç»“æœè¿›è¡ŒDEMåˆ†æè¡¨æ˜ï¼Œè¯¥ç­‰ç¦»å­ä½“å›¢ä¸å‘¨å›´æ—¥å†•ç›¸æ¯”ï¼Œæ˜¯ç”±è¾ƒå†·ä¸”è¾ƒå¯†é›†çš„ç­‰ç¦»å­ä½“ç»„æˆã€‚æ®æˆ‘ä»¬æ‰€çŸ¥ï¼Œè¿™æ˜¯é¦–æ¬¡åœ¨è¿‘ç´«å¤–æ³¢æ®µè§‚å¯Ÿåˆ°è¿™æ ·çš„ç­‰ç¦»å­ä½“å›¢ï¼Œä¸ºçˆ†å‘çƒ­åŠ›å­¦æä¾›äº†å…³é”®æµ‹é‡æ•°æ®ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.04793v1">PDF</a> 13 pages, 8 figures</p>
<p><strong>Summary</strong><br>     é‡‡ç”¨å¤šæ³¢é•¿è§‚æµ‹ç ”ç©¶å¤ªé˜³ç­‰ç¦»å­ä½“å›¢ä»è¾¹ç¼˜è€€æ–‘SOL2023-12-31T21:36:00çš„å–·å°„ç°è±¡ï¼Œé€šè¿‡Aditya-L1ä¸Šçš„SUITæœ›è¿œé•œã€SDOä¸Šçš„AIAä»¥åŠSolar Orbiterä¸Šçš„STIXä»ªå™¨è§‚æµ‹æ•°æ®æ¨æ–­å–·å°„ä½“çš„è¿åŠ¨å­¦å’Œçƒ­ç‰¹æ€§åŠå…¶ä¸è€€æ–‘çš„å…³è”ã€‚è§‚å¯Ÿåˆ°è¯¥è€€æ–‘ç”±ä¸¤æ¬¡çˆ†å‘ç»„æˆï¼Œç­‰ç¦»å­ä½“å›¢åœ¨ç¬¬ä¸€æ¬¡çˆ†å‘ä¸­è¢«å–·å°„å‡ºï¼Œä¹‹ååœ¨çº¦ç¦»å¤ªé˜³è¡¨é¢æœ€å¤§é«˜åº¦ä¸º~ 178 Mmå¤„åŠ é€Ÿè‡³è¶…è¿‡1500 km&#x2F;sçš„é€Ÿåº¦ã€‚ç­‰ç¦»å­ä½“å›¢çš„åŠ é€Ÿä¸STIXè®°å½•çš„ç¡¬Xå°„çº¿å…‰æ›²çº¿çš„çªå‘å¤–è§‚ç›¸å»åˆã€‚åŒæ—¶ï¼Œä»STEREO-A&#x2F;WAVESå’ŒRSTNçš„æ— çº¿ç”µé¢‘è°±è§‚æµ‹æ˜¾ç¤ºå‡ºäº†â…¢å‹çˆ†å‘ï¼ŒæŒ‡ç¤ºäº†ç£é‡è”çš„å‘ç”Ÿã€‚ä½¿ç”¨AIAè§‚æµ‹æ•°æ®çš„DEMåˆ†æè¡¨æ˜ï¼Œç­‰ç¦»å­ä½“å›¢ç›¸å¯¹äºå‘¨å›´æ—¥å†•å…·æœ‰è¾ƒå†·å’Œè¾ƒå¯†çš„ç­‰ç¦»å­ä½“ç‰¹æ€§ã€‚è¿™æ˜¯é¦–æ¬¡åœ¨ç´«å¤–æ³¢æ®µè§‚å¯Ÿåˆ°æ­¤ç±»ç­‰ç¦»å­ä½“å›¢ï¼Œä¸ºçˆ†å‘çƒ­åŠ›å­¦æä¾›äº†å…³é”®æµ‹é‡æ•°æ®ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ–‡ä¸­ç ”ç©¶äº†ä¸€æ¬¡å¤ªé˜³è¾¹ç¼˜è€€æ–‘å–·å°„ç­‰ç¦»å­ä½“å›¢çš„å¤šæ³¢é•¿è§‚æµ‹ã€‚</li>
<li>ç­‰ç¦»å­ä½“å›¢åœ¨ç¬¬ä¸€æ¬¡çˆ†å‘ä¸­è¢«å–·å°„ï¼Œä¹‹ååŠ é€Ÿè‡³è¶…é«˜é€Ÿã€‚</li>
<li>åŠ é€Ÿçš„ç­‰ç¦»å­ä½“å›¢ä¸ç¡¬Xå°„çº¿å…‰æ›²çº¿çš„çªå‘æ´»åŠ¨åŒæ­¥ã€‚</li>
<li>æ— çº¿ç”µé¢‘è°±è§‚æµ‹æ˜¾ç¤ºç£é‡è”çš„è¿¹è±¡ã€‚</li>
<li>é€šè¿‡DEMåˆ†æï¼Œå‘ç°ç­‰ç¦»å­ä½“å›¢å…·æœ‰è¾ƒå†·å’Œè¾ƒå¯†çš„ç‰¹æ€§ï¼Œä¸å‘¨å›´æ—¥å†•ä¸åŒã€‚</li>
<li>è¿™æ˜¯é¦–æ¬¡åœ¨ç´«å¤–æ³¢æ®µè§‚å¯Ÿåˆ°æ­¤ç±»ç­‰ç¦»å­ä½“å›¢ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.04793">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-0f6bb7c8afb99ae630328146abebfba6.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-cc7ddf76747b172c48e0797a9dbdd1f4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2231df72ab6509b4699aaea3e9d7f46c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3bf21a49ec8ac559c7e50225e91d208e.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Low-Count-X-ray-Polarimetry-using-the-Bayesian-Approach-Reveals-Fast-Polarization-Angle-Variations"><a href="#Low-Count-X-ray-Polarimetry-using-the-Bayesian-Approach-Reveals-Fast-Polarization-Angle-Variations" class="headerlink" title="Low-Count X-ray Polarimetry using the Bayesian Approach Reveals Fast   Polarization Angle Variations"></a>Low-Count X-ray Polarimetry using the Bayesian Approach Reveals Fast   Polarization Angle Variations</h2><p><strong>Authors:Hong Li, Qing-Chang Zhao, Hua Feng, Lian Tao, Sergey S. Tsygankov</strong></p>
<p>X-ray polarimetry of accreting compact object has revealed fast time variations in the polarization angle (PA), suggesting that the geometry and&#x2F;or optical depth of the corona is changing rapidly. This prompts investigations into how fast such variability can be. Conventionally, the data are often binned to examine the time variability such that the measurement in each bin is above the minimum detectable polarization (MDP). Here we demonstrate that this is unnecessary, and even below the MDP, one can infer the posterior distribution of PA reliably using the Bayesian approach and still be able to place useful constraints on the physics in many cases. With this approach, we discovered that the PA variation in one of the Imaging X-ray Polarimetry Explorer (IXPE) observations of GX 13+1 is not following a linear rotation mode as suggested previously. Instead, the PA swings between two discrete angles, suggesting that there are two emitting components, e.g., the boundary layer and the spreading layer, competing with each other. Also in one of the observations of GX 13+1 and Sco X-1, the PA is found to vary in correlation with the source count rate, indicating that the mass accretion rate is shaping the corona properties. Also, during the IXPE observation of Sco X-1, the PA in highest flux level seems to deviate from the averaged value and appear to be consistent with previous measurement results with PolarLight and OSO-8. </p>
<blockquote>
<p>Xå°„çº¿åæŒ¯æµ‹é‡æ˜¾ç¤ºï¼Œå¸ç§¯è‡´å¯†å¤©ä½“ï¼ˆaccreting compact objectï¼‰çš„åæŒ¯è§’ï¼ˆPAï¼‰å­˜åœ¨å¿«é€Ÿæ—¶é—´å˜åŒ–ï¼Œæš—ç¤ºç€å†•å±‚çš„å‡ ä½•ç»“æ„æˆ–å…‰å­¦æ·±åº¦æ­£åœ¨å¿«é€Ÿå˜åŒ–ã€‚è¿™ä¿ƒä½¿ç ”ç©¶è€…ä»¬ç ”ç©¶è¿™ç§å˜åŒ–èƒ½æœ‰å¤šå¿«ã€‚ä¼ ç»Ÿä¸Šï¼Œæ•°æ®é€šå¸¸ä¼šè¢«åˆ†ç»„ä»¥æ£€æŸ¥æ—¶é—´å˜åŒ–ï¼Œä»¥ç¡®ä¿æ¯ä¸ªåˆ†ç»„ä¸­çš„æµ‹é‡å€¼é«˜äºæœ€å°å¯æ£€æµ‹åæŒ¯åº¦ï¼ˆMDPï¼‰ã€‚åœ¨è¿™é‡Œæˆ‘ä»¬è¯æ˜è¿™æ˜¯ä¸å¿…è¦çš„ï¼Œå³ä½¿ä½äºMDPï¼Œæˆ‘ä»¬ä¹Ÿå¯ä»¥åˆ©ç”¨è´å¶æ–¯æ–¹æ³•å¯é åœ°æ¨æ–­å‡ºåæŒ¯è§’çš„åéªŒåˆ†å¸ƒï¼Œå¹¶åœ¨è®¸å¤šæƒ…å†µä¸‹å¯¹ç‰©ç†è¿‡ç¨‹æ–½åŠ æœ‰ç”¨çš„çº¦æŸã€‚ä½¿ç”¨è¿™ç§æ–¹æ³•ï¼Œæˆ‘ä»¬å‘ç°GX 13+1çš„ä¸€æ¬¡æˆåƒXå°„çº¿åæŒ¯ä»ªè§‚æµ‹ä¸­çš„åæŒ¯è§’å˜åŒ–å¹¶ä¸åƒå…ˆå‰æ‰€å»ºè®®çš„é‚£æ ·éµå¾ªçº¿æ€§æ—‹è½¬æ¨¡å¼ã€‚ç›¸åï¼ŒåæŒ¯è§’åœ¨ä¸¤ä¸ªç¦»æ•£è§’åº¦ä¹‹é—´æ‘†åŠ¨ï¼Œè¿™è¡¨æ˜å­˜åœ¨ä¸¤ä¸ªå‘å°„åˆ†é‡ï¼ˆä¾‹å¦‚è¾¹ç•Œå±‚å’Œæ‰©æ•£å±‚ï¼‰ç›¸äº’ç«äº‰ã€‚åœ¨GX 13+1å’ŒSco X-1çš„ä¸€æ¬¡è§‚æµ‹ä¸­ï¼Œè¿˜å‘ç°åæŒ¯è§’ä¸æºè®¡æ•°ç‡ç›¸å…³å˜åŒ–ï¼Œè¿™è¡¨æ˜ç‰©è´¨å¸ç§¯ç‡æ­£åœ¨å¡‘é€ å†•å±‚ç‰¹æ€§ã€‚æ­¤å¤–ï¼Œåœ¨Sco X-1çš„ä¸€æ¬¡IXPEè§‚æµ‹ä¸­ï¼Œæœ€é«˜æµé‡æ°´å¹³çš„åæŒ¯è§’ä¼¼ä¹åç¦»å¹³å‡å€¼ï¼Œä¸ä¹‹å‰çš„PolarLightå’ŒOSO-8çš„æµ‹é‡ç»“æœä¸€è‡´ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.04775v1">PDF</a> </p>
<p><strong>Summary</strong><br>     Xå°„çº¿åæŒ¯æˆåƒè§‚æµ‹æ˜¾ç¤ºï¼Œé«˜å…‰åº¦æºå¿«é€Ÿå˜åŒ–çš„å…‰åº¦å˜åŒ–å¯ä»¥é€šè¿‡è´å¶æ–¯åˆ†ææ­ç¤ºå‡ºå…¶æåŒ–è§’åº¦ï¼ˆPAï¼‰çš„åæŒ¯ç‰¹æ€§å˜åŒ–æ¥æ¢æµ‹å‡ºæ¥ï¼Œæ˜¾ç¤ºå‡ºå†…åŒºç£å±‚å’Œå¢ç›ŠåŒºçš„äº¤æ›¿åˆ†å¸ƒå¯¹æåŒ–ç‰¹å¾æœ‰å½±å“ï¼Œå…¶ä¸­è§‚æµ‹åˆ°çš„GX 13+1å’ŒSco X-1çš„è§‚æµ‹ç»“æœè¿›ä¸€æ­¥è¯å®äº†è¿™ä¸€ç‚¹ã€‚è¿™äº›å‘ç°æ­ç¤ºäº†Xå°„çº¿å…‰å­çš„é«˜åº¦ç¦»æ•£è¾å°„æ€§è´¨å’Œç­‰ç¦»å­ç»“æ„æ•ˆåº”çš„å˜åŒ–å…³ç³»ï¼Œä¹Ÿå¯¹å¸ç§¯ç‡å’Œå…¶ä»–å‡ ä½•ç‰¹å¾å‚æ•°å˜åŒ–çš„å»ºæ¨¡æä¾›äº†æ–°çš„çº¿ç´¢ã€‚é€šè¿‡å¯¹è¿™äº›å¤æ‚æºçš„åæŒ¯ç ”ç©¶ï¼Œå¯ä»¥æ›´å¥½åœ°ç†è§£è¿™äº›å…³é”®è¿‡ç¨‹ï¼Œå¯¹Xå°„çº¿å¤©æ–‡å­¦é¢†åŸŸçš„ç ”ç©¶å…·æœ‰é‡è¦å½±å“ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>é«˜å…‰åº¦æºçš„å¿«é€Ÿå˜åŒ–èƒ½å¤Ÿé€šè¿‡ç ”ç©¶æåŒ–è§’åº¦å˜åŒ–è¿›è¡Œæ¢æµ‹å’Œåˆ†æã€‚ä¼ ç»Ÿçš„åˆ†ææ–¹æ³•é€šè¿‡å°†æ•°æ®åˆ†ç»„ä»¥è¶…è¿‡æœ€å°å¯æ£€æµ‹æåŒ–åº¦ï¼ˆMDPï¼‰æ¥æ£€æµ‹æ—¶é—´å˜åŒ–ï¼Œä½†æ–°æ–¹æ³•æ˜¾ç¤ºå³ä½¿ä½äºMDPï¼Œä¹Ÿèƒ½é€šè¿‡è´å¶æ–¯åˆ†æå¯é åœ°æ¨æ–­å‡ºPAçš„åéªŒåˆ†å¸ƒã€‚</li>
<li>åœ¨GX 13+1çš„è§‚æµ‹ä¸­ï¼Œå‘ç°æåŒ–è§’åº¦å˜åŒ–å¹¶ä¸éµå¾ªå…ˆå‰è®¤ä¸ºçš„çº¿æ€§æ—‹è½¬æ¨¡å¼ï¼Œè€Œæ˜¯è¡¨ç°å‡ºåœ¨ä¸¤ä¸ªç¦»æ•£è§’åº¦ä¹‹é—´çš„æ‘†åŠ¨ï¼Œæš—ç¤ºå­˜åœ¨ä¸¤ä¸ªç«äº‰å‘å°„ç»„åˆ†ï¼Œå¯èƒ½æ˜¯è¾¹ç•Œå±‚å’Œæ‰©å±•å±‚ã€‚è¿™ä¸ºå¸ç§¯ç›˜ä¸­ç£åœºæ•ˆåº”çš„ç©ºé—´åˆ†å¸ƒæ¨¡å‹æä¾›äº†çº¿ç´¢ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.04775">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-6a00938eeeaff5ad89c171c1aacc3596.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e1119a396adfe9454e6fbe6111d758d9.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-68b9d219e5187b6e44fbb57dd19619c0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-185cd177dd8603205fe5279a26e34219.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b89d0243bd53ba1ed20eb1c3a4198d9a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-29ac4c21e24a64719d4a29cef5c458fa.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-bf5b339c97cf9d8bd018cab3a6a2e1e5.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-fa8162a1d15b109bb1a1e61813bcbb6f.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Here-Comes-the-Explanation-A-Shapley-Perspective-on-Multi-contrast-Medical-Image-Segmentation"><a href="#Here-Comes-the-Explanation-A-Shapley-Perspective-on-Multi-contrast-Medical-Image-Segmentation" class="headerlink" title="Here Comes the Explanation: A Shapley Perspective on Multi-contrast   Medical Image Segmentation"></a>Here Comes the Explanation: A Shapley Perspective on Multi-contrast   Medical Image Segmentation</h2><p><strong>Authors:Tianyi Ren, Juampablo Heras Rivera, Hitender Oswal, Yutong Pan, Agamdeep Chopra, Jacob Ruzevick, Mehmet Kurt</strong></p>
<p>Deep learning has been successfully applied to medical image segmentation, enabling accurate identification of regions of interest such as organs and lesions. This approach works effectively across diverse datasets, including those with single-image contrast, multi-contrast, and multimodal imaging data. To improve human understanding of these black-box models, there is a growing need for Explainable AI (XAI) techniques for model transparency and accountability. Previous research has primarily focused on post hoc pixel-level explanations, using methods gradient-based and perturbation-based apporaches. These methods rely on gradients or perturbations to explain model predictions. However, these pixel-level explanations often struggle with the complexity inherent in multi-contrast magnetic resonance imaging (MRI) segmentation tasks, and the sparsely distributed explanations have limited clinical relevance. In this study, we propose using contrast-level Shapley values to explain state-of-the-art models trained on standard metrics used in brain tumor segmentation. Our results demonstrate that Shapley analysis provides valuable insights into different modelsâ€™ behavior used for tumor segmentation. We demonstrated a bias for U-Net towards over-weighing T1-contrast and FLAIR, while Swin-UNETR provided a cross-contrast understanding with balanced Shapley distribution. </p>
<blockquote>
<p>æ·±åº¦å­¦ä¹ å·²æˆåŠŸåº”ç”¨äºåŒ»å­¦å›¾åƒåˆ†å‰²ï¼Œèƒ½å¤Ÿå‡†ç¡®è¯†åˆ«æ„Ÿå…´è¶£åŒºåŸŸï¼Œå¦‚å™¨å®˜å’Œç—…å˜ã€‚è¯¥æ–¹æ³•åœ¨å¤šç§æ•°æ®é›†ä¸Šå‡æœ‰æ•ˆï¼ŒåŒ…æ‹¬å•å›¾åƒå¯¹æ¯”åº¦ã€å¤šå¯¹æ¯”åº¦å’Œå¤šæ¨¡æ€æˆåƒæ•°æ®ã€‚ä¸ºäº†æé«˜äººä»¬å¯¹è¿™äº›é»‘ç®±æ¨¡å‹çš„ç†è§£ï¼Œå¯¹æ¨¡å‹é€æ˜åº¦å’Œå¯è§£é‡Šæ€§çš„å¯è§£é‡Šäººå·¥æ™ºèƒ½ï¼ˆXAIï¼‰æŠ€æœ¯çš„éœ€æ±‚æ—¥ç›Šå¢é•¿ã€‚ä»¥å¾€çš„ç ”ç©¶ä¸»è¦é›†ä¸­åœ¨äº‹ååƒç´ çº§è§£é‡Šä¸Šï¼Œä½¿ç”¨åŸºäºæ¢¯åº¦å’ŒåŸºäºæ‰°åŠ¨çš„æ–¹æ³•ã€‚è¿™äº›æ–¹æ³•ä¾èµ–äºæ¢¯åº¦æˆ–æ‰°åŠ¨æ¥è§£é‡Šæ¨¡å‹é¢„æµ‹ã€‚ç„¶è€Œï¼Œè¿™äº›åƒç´ çº§è§£é‡Šå¾€å¾€éš¾ä»¥åº”å¯¹å¤šå¯¹æ¯”åº¦ç£å…±æŒ¯æˆåƒï¼ˆMRIï¼‰åˆ†å‰²ä»»åŠ¡çš„å›ºæœ‰å¤æ‚æ€§ï¼Œç¨€ç–åˆ†å¸ƒçš„è§£é‡Šä¸´åºŠç›¸å…³æ€§æœ‰é™ã€‚æœ¬ç ”ç©¶ä¸­ï¼Œæˆ‘ä»¬æå‡ºä½¿ç”¨å¯¹æ¯”åº¦çš„Shapleyå€¼æ¥è§£é‡Šåœ¨è„‘è‚¿ç˜¤åˆ†å‰²ä¸­ä½¿ç”¨çš„æ ‡å‡†æŒ‡æ ‡è®­ç»ƒçš„æœ€æ–°æ¨¡å‹ã€‚ç»“æœè¡¨æ˜ï¼ŒShapleyåˆ†æä¸ºä¸åŒæ¨¡å‹åœ¨è‚¿ç˜¤åˆ†å‰²ä¸­çš„è¡Œä¸ºæä¾›äº†æœ‰ä»·å€¼çš„è§è§£ã€‚æˆ‘ä»¬å±•ç¤ºäº†U-Netåå‘äºè¿‡åº¦é‡è§†T1å¯¹æ¯”åº¦å’ŒFLAIRï¼Œè€ŒSwin-UNETRæä¾›äº†è·¨å¯¹æ¯”åº¦çš„ç†è§£ï¼ŒShapleyåˆ†å¸ƒå‡è¡¡ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.04645v1">PDF</a> </p>
<p><strong>Summary</strong><br>åŒ»å­¦å›¾åƒåˆ†å‰²ä¸­æ·±åº¦å­¦ä¹ åº”ç”¨å¹¿æ³›ï¼Œèƒ½å¤Ÿé€šè¿‡ç²¾å‡†è¯†åˆ«æ„Ÿå…´è¶£åŒºåŸŸå¦‚å™¨å®˜å’Œç—…ç¶ç­‰ï¼Œå®ç°è·¨å¤šç§æ•°æ®é›†çš„æœ‰æ•ˆåˆ†æã€‚ä¸ºæé«˜å¯¹é»‘ç®±æ¨¡å‹çš„ç†è§£ï¼Œéœ€è¦é‡‡ç”¨å¯è§£é‡Šäººå·¥æ™ºèƒ½ï¼ˆXAIï¼‰æŠ€æœ¯æ¥æå‡æ¨¡å‹çš„é€æ˜åº¦å’Œé—®è´£æ€§ã€‚æœ¬ç ”ç©¶é‡‡ç”¨åŸºäºå¯¹æ¯”åº¦æ°´å¹³çš„æ²™æ™®åˆ©å€¼ï¼ˆShapley valuesï¼‰åˆ†ææ–¹æ³•ï¼Œé’ˆå¯¹è„‘éƒ¨è‚¿ç˜¤åˆ†å‰²çš„å…ˆè¿›æ¨¡å‹è¿›è¡Œè§£é‡Šã€‚ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œæ²™æ™®åˆ©åˆ†æä¸ºä¸åŒæ¨¡å‹çš„è‚¿ç˜¤åˆ†å‰²è¡Œä¸ºæä¾›äº†å®è´µè§è§£ã€‚U-Netå€¾å‘äºè¿‡åº¦é‡è§†T1å¯¹æ¯”åº¦å’ŒFLAIRï¼Œè€ŒSwin-UNETRåˆ™å®ç°äº†è·¨å¯¹æ¯”åº¦çš„å‡è¡¡ç†è§£ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ·±åº¦å­¦ä¹ åœ¨åŒ»å­¦å›¾åƒåˆ†å‰²ä¸­æœ‰å¹¿æ³›åº”ç”¨ï¼Œèƒ½ç²¾å‡†è¯†åˆ«å™¨å®˜å’Œç—…ç¶ç­‰æ„Ÿå…´è¶£åŒºåŸŸã€‚</li>
<li>æ·±åº¦å­¦ä¹ æ¨¡å‹å¯è·¨å¤šç§æ•°æ®é›†è¿›è¡Œæœ‰æ•ˆåˆ†æï¼ŒåŒ…æ‹¬å•å›¾åƒå¯¹æ¯”ã€å¤šå¯¹æ¯”åº¦å’Œå¤šæ¨¡æ€æˆåƒæ•°æ®ã€‚</li>
<li>ä¸ºæé«˜æ¨¡å‹é€æ˜åº¦ä¸ç†è§£åº¦ï¼Œéœ€è¦é‡‡ç”¨å¯è§£é‡Šäººå·¥æ™ºèƒ½ï¼ˆXAIï¼‰æŠ€æœ¯ã€‚</li>
<li>ç°æœ‰è§£é‡Šæ–¹æ³•ä¸»è¦ä¾§é‡äºäº‹ååƒç´ çº§è§£é‡Šï¼Œä½†å®ƒä»¬åœ¨å¤šå¯¹æ¯”åº¦ç£å…±æŒ¯æˆåƒï¼ˆMRIï¼‰åˆ†å‰²ä»»åŠ¡ä¸­é¢ä¸´æŒ‘æˆ˜ã€‚</li>
<li>æ²™æ™®åˆ©å€¼åˆ†ææ–¹æ³•èƒ½å¤Ÿæä¾›ä¸åŒæ¨¡å‹åœ¨è‚¿ç˜¤åˆ†å‰²ä¸­çš„è¡Œä¸ºæ´å¯Ÿã€‚</li>
<li>U-Netæ¨¡å‹åœ¨T1å¯¹æ¯”åº¦å’ŒFLAIRä¸Šå­˜åœ¨è¿‡åº¦é‡è§†çš„åå‘ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.04645">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-2a5cf858dd3874a7333a7ce93b3a6754.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b4075a9e59523b4e38e30c748926fa8f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-392b1a739a1e66a5dbfce99a9841c850.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="DyCON-Dynamic-Uncertainty-aware-Consistency-and-Contrastive-Learning-for-Semi-supervised-Medical-Image-Segmentation"><a href="#DyCON-Dynamic-Uncertainty-aware-Consistency-and-Contrastive-Learning-for-Semi-supervised-Medical-Image-Segmentation" class="headerlink" title="DyCON: Dynamic Uncertainty-aware Consistency and Contrastive Learning   for Semi-supervised Medical Image Segmentation"></a>DyCON: Dynamic Uncertainty-aware Consistency and Contrastive Learning   for Semi-supervised Medical Image Segmentation</h2><p><strong>Authors:Maregu Assefa, Muzammal Naseer, Iyyakutti Iyappan Ganapathi, Syed Sadaf Ali, Mohamed L Seghier, Naoufel Werghi</strong></p>
<p>Semi-supervised learning in medical image segmentation leverages unlabeled data to reduce annotation burdens through consistency learning. However, current methods struggle with class imbalance and high uncertainty from pathology variations, leading to inaccurate segmentation in 3D medical images. To address these challenges, we present DyCON, a Dynamic Uncertainty-aware Consistency and Contrastive Learning framework that enhances the generalization of consistency methods with two complementary losses: Uncertainty-aware Consistency Loss (UnCL) and Focal Entropy-aware Contrastive Loss (FeCL). UnCL enforces global consistency by dynamically weighting the contribution of each voxel to the consistency loss based on its uncertainty, preserving high-uncertainty regions instead of filtering them out. Initially, UnCL prioritizes learning from uncertain voxels with lower penalties, encouraging the model to explore challenging regions. As training progress, the penalty shift towards confident voxels to refine predictions and ensure global consistency. Meanwhile, FeCL enhances local feature discrimination in imbalanced regions by introducing dual focal mechanisms and adaptive confidence adjustments into the contrastive principle. These mechanisms jointly prioritizes hard positives and negatives while focusing on uncertain sample pairs, effectively capturing subtle lesion variations under class imbalance. Extensive evaluations on four diverse medical image segmentation datasets (ISLESâ€™22, BraTSâ€™19, LA, Pancreas) show DyCONâ€™s superior performance against SOTA methods. </p>
<blockquote>
<p>åœ¨åŒ»å­¦å›¾åƒåˆ†å‰²ä¸­ï¼ŒåŠç›‘ç£å­¦ä¹ åˆ©ç”¨æ— æ ‡ç­¾æ•°æ®é€šè¿‡ä¸€è‡´æ€§å­¦ä¹ æ¥å‡è½»æ ‡æ³¨è´Ÿæ‹…ã€‚ç„¶è€Œï¼Œå½“å‰çš„æ–¹æ³•åœ¨é¢ä¸´ç±»åˆ«ä¸å¹³è¡¡å’Œé«˜ç—…ç†å˜åŒ–ä¸ç¡®å®šæ€§æ—¶ï¼Œä¼šå‡ºç°å¯¹ä¸‰ç»´åŒ»å­¦å›¾åƒåˆ†å‰²ä¸å‡†ç¡®çš„é—®é¢˜ã€‚ä¸ºäº†åº”å¯¹è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†DyCONï¼Œä¸€ä¸ªåŠ¨æ€æ„ŸçŸ¥ä¸ç¡®å®šæ€§å’Œå¯¹æ¯”å­¦ä¹ æ¡†æ¶ï¼Œå®ƒé€šè¿‡ä¸¤ç§äº’è¡¥çš„æŸå¤±æ¥å¢å¼ºä¸€è‡´æ€§æ–¹æ³•çš„æ³›åŒ–èƒ½åŠ›ï¼šæ„ŸçŸ¥ä¸ç¡®å®šæ€§ä¸€è‡´æ€§æŸå¤±ï¼ˆUnCLï¼‰å’Œç„¦ç‚¹ç†µæ„ŸçŸ¥å¯¹æ¯”æŸå¤±ï¼ˆFeCLï¼‰ã€‚UnCLé€šè¿‡åŠ¨æ€æƒè¡¡æ¯ä¸ªä½“ç´ å¯¹ä¸€è‡´æ€§æŸå¤±çš„è´¡çŒ®æ¥å®ç°å…¨å±€ä¸€è‡´æ€§ï¼Œè¿™åŸºäºå…¶ä¸ç¡®å®šæ€§ï¼Œä¿ç•™é«˜ä¸ç¡®å®šæ€§åŒºåŸŸè€Œä¸æ˜¯è¿‡æ»¤æ‰å®ƒä»¬ã€‚æœ€åˆï¼ŒUnCLä¼˜å…ˆä»ä¸ç¡®å®šæ€§è¾ƒé«˜çš„ä½“ç´ ä¸­å­¦ä¹ ï¼Œé‡‡ç”¨è¾ƒä½çš„æƒ©ç½šåŠ›åº¦ï¼Œé¼“åŠ±æ¨¡å‹æ¢ç´¢å…·æœ‰æŒ‘æˆ˜æ€§çš„åŒºåŸŸã€‚éšç€è®­ç»ƒçš„è¿›è¡Œï¼Œæƒ©ç½šåŠ›åº¦è½¬å‘ç¡®å®šæ€§è¾ƒé«˜çš„ä½“ç´ ï¼Œä»¥å®Œå–„é¢„æµ‹å¹¶ç¡®ä¿å…¨å±€ä¸€è‡´æ€§ã€‚åŒæ—¶ï¼ŒFeCLé€šè¿‡åœ¨å¯¹æ¯”åŸåˆ™ä¸­å¼•å…¥åŒé‡ç„¦ç‚¹æœºåˆ¶å’Œè‡ªé€‚åº”ç½®ä¿¡è°ƒæ•´æ¥å¢å¼ºä¸å¹³è¡¡åŒºåŸŸçš„å±€éƒ¨ç‰¹å¾è¾¨åˆ«èƒ½åŠ›ã€‚è¿™äº›æœºåˆ¶å…±åŒä¼˜å…ˆå¤„ç†ç¡¬æ­£æ ·æœ¬å’Œè´Ÿæ ·æœ¬ï¼ŒåŒæ—¶å…³æ³¨ä¸ç¡®å®šçš„æ ·æœ¬å¯¹ï¼Œæœ‰æ•ˆæ•æ‰ç±»åˆ«ä¸å¹³è¡¡ä¸‹çš„ç»†å¾®ç—…å˜å˜åŒ–ã€‚åœ¨å››ä¸ªä¸åŒçš„åŒ»å­¦å›¾åƒåˆ†å‰²æ•°æ®é›†ï¼ˆISLESâ€™22ã€BraTSâ€™19ã€LAã€èƒ°è…ºï¼‰ä¸Šçš„å¹¿æ³›è¯„ä¼°è¡¨æ˜ï¼ŒDyCONçš„æ€§èƒ½ä¼˜äºæœ€æ–°æ–¹æ³•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.04566v1">PDF</a> Accepted to CVPR 2025</p>
<p><strong>Summary</strong><br>åŒ»å­¦å›¾åƒåˆ†å‰²ä¸­çš„åŠç›‘ç£å­¦ä¹ é€šè¿‡åˆ©ç”¨æ— æ ‡ç­¾æ•°æ®å‡å°‘æ ‡æ³¨è´Ÿæ‹…ï¼Œä½†é¢ä¸´ç±»åˆ«ä¸å¹³è¡¡å’Œé«˜ä¸ç¡®å®šæ€§ç­‰é—®é¢˜ã€‚ä¸ºæ­¤ï¼Œæå‡ºDyCONæ¡†æ¶ï¼Œç»“åˆåŠ¨æ€ä¸ç¡®å®šæ€§æ„ŸçŸ¥çš„ä¸€è‡´æ€§æŸå¤±å’Œç„¦ç‚¹ç†µæ„ŸçŸ¥å¯¹æ¯”æŸå¤±ï¼Œæé«˜æ¨¡å‹æ³›åŒ–èƒ½åŠ›ã€‚DyCONèƒ½åŠ¨æ€æƒè¡¡æ¯ä¸ªä½“ç´ å¯¹ä¸€è‡´æ€§æŸå¤±çš„ä¸ç¡®å®šæ€§è´¡çŒ®ï¼ŒåŒæ—¶å¼ºåŒ–å±€éƒ¨ç‰¹å¾åˆ¤åˆ«èƒ½åŠ›ï¼Œæœ‰æ•ˆåº”å¯¹ç±»åˆ«ä¸å¹³è¡¡é—®é¢˜ã€‚åœ¨å››ä¸ªåŒ»å­¦å›¾åƒåˆ†å‰²æ•°æ®é›†ä¸Šçš„è¯„ä¼°æ˜¾ç¤ºï¼ŒDyCONæ€§èƒ½ä¼˜äºç°æœ‰æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>åŠç›‘ç£å­¦ä¹ åœ¨åŒ»å­¦å›¾åƒåˆ†å‰²ä¸­åˆ©ç”¨æ— æ ‡ç­¾æ•°æ®å‡å°‘æ ‡æ³¨è´Ÿæ‹…ï¼Œä½†é¢ä¸´ç±»åˆ«ä¸å¹³è¡¡å’Œé«˜ä¸ç¡®å®šæ€§æŒ‘æˆ˜ã€‚</li>
<li>DyCONæ¡†æ¶é€šè¿‡ç»“åˆä¸ç¡®å®šæ€§æ„ŸçŸ¥çš„ä¸€è‡´æ€§æŸå¤±å’Œç„¦ç‚¹ç†µæ„ŸçŸ¥å¯¹æ¯”æŸå¤±ï¼Œæé«˜æ¨¡å‹æ³›åŒ–èƒ½åŠ›ã€‚</li>
<li>DyCONèƒ½åŠ¨æ€å¤„ç†ä¸ç¡®å®šæ€§ï¼Œåœ¨è®­ç»ƒåˆæœŸé¼“åŠ±æ¨¡å‹æ¢ç´¢é«˜ä¸ç¡®å®šæ€§åŒºåŸŸï¼ŒåæœŸç²¾ç»†é¢„æµ‹å¹¶ç¡®ä¿å…¨å±€ä¸€è‡´æ€§ã€‚</li>
<li>FeCLæŸå¤±é€šè¿‡å¼•å…¥åŒé‡ç„¦ç‚¹æœºåˆ¶å’Œè‡ªé€‚åº”ä¿¡å¿ƒè°ƒæ•´ï¼Œå¢å¼ºå±€éƒ¨ç‰¹å¾åˆ¤åˆ«èƒ½åŠ›ï¼Œåº”å¯¹ç±»åˆ«ä¸å¹³è¡¡é—®é¢˜ã€‚</li>
<li>DyCONæ¡†æ¶åœ¨å››ä¸ªåŒ»å­¦å›¾åƒåˆ†å‰²æ•°æ®é›†ä¸Šçš„æ€§èƒ½è¯„ä¼°è¡¨ç°ä¼˜å¼‚ã€‚</li>
<li>UnCLæŸå¤±é€šè¿‡åŠ¨æ€æƒè¡¡æ¯ä¸ªä½“ç´ çš„ä¸ç¡®å®šæ€§è´¡çŒ®ï¼Œæé«˜æ¨¡å‹å¯¹é«˜ä¸ç¡®å®šæ€§åŒºåŸŸçš„ä¿ç•™èƒ½åŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.04566">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-12b9ff4fd27e00e876f1a90b76db44ff.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-cda5db769f90c5b9d5a5b8d98284a50d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9752790c3375ee30a5eaf60a682e0cae.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5c4bb5e65064ef93a4ed517121fccd3f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-466404243fa2757f962c3c89d4735a9d.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="GAMBAS-Generalised-Hilbert-Mamba-for-Super-resolution-of-Paediatric-Ultra-Low-Field-MRI"><a href="#GAMBAS-Generalised-Hilbert-Mamba-for-Super-resolution-of-Paediatric-Ultra-Low-Field-MRI" class="headerlink" title="GAMBAS: Generalised-Hilbert Mamba for Super-resolution of Paediatric   Ultra-Low-Field MRI"></a>GAMBAS: Generalised-Hilbert Mamba for Super-resolution of Paediatric   Ultra-Low-Field MRI</h2><p><strong>Authors:Levente Baljer, Ula Briski, Robert Leech, Niall J. Bourke, Kirsten A. Donald, Layla E. Bradford, Simone R. Williams, Sadia Parkar, Sidra Kaleem, Salman Osmani, Sean C. L. Deoni, Steven C. R. Williams, Rosalyn J. Moran, Emma C. Robinson, Frantisek Vasa</strong></p>
<p>Magnetic resonance imaging (MRI) is critical for neurodevelopmental research, however access to high-field (HF) systems in low- and middle-income countries is severely hindered by their cost. Ultra-low-field (ULF) systems mitigate such issues of access inequality, however their diminished signal-to-noise ratio limits their applicability for research and clinical use. Deep-learning approaches can enhance the quality of scans acquired at lower field strengths at no additional cost. For example, Convolutional neural networks (CNNs) fused with transformer modules have demonstrated a remarkable ability to capture both local information and long-range context. Unfortunately, the quadratic complexity of transformers leads to an undesirable trade-off between long-range sensitivity and local precision. We propose a hybrid CNN and state-space model (SSM) architecture featuring a novel 3D to 1D serialisation (GAMBAS), which learns long-range context without sacrificing spatial precision. We exhibit improved performance compared to other state-of-the-art medical image-to-image translation models. </p>
<blockquote>
<p>ç£å…±æŒ¯æˆåƒï¼ˆMRIï¼‰åœ¨ç¥ç»å‘è‚²ç ”ç©¶ä¸­å…·æœ‰é‡è¦æ„ä¹‰ï¼Œä½†åœ¨ä½æ”¶å…¥å’Œä¸­ç­‰æ”¶å…¥å›½å®¶ï¼Œç”±äºæˆæœ¬é—®é¢˜ï¼Œè·å¾—é«˜åœºï¼ˆHFï¼‰ç³»ç»Ÿçš„æœºä¼šå—åˆ°ä¸¥é‡é˜»ç¢ã€‚è¶…ä½åœºï¼ˆULFï¼‰ç³»ç»Ÿç¼“è§£äº†è·å–ä¸å¹³ç­‰çš„é—®é¢˜ï¼Œä½†å…¶ä¿¡å™ªæ¯”é™ä½é™åˆ¶äº†å…¶åœ¨ç ”ç©¶å’Œä¸´åºŠä½¿ç”¨ä¸­çš„åº”ç”¨ã€‚æ·±åº¦å­¦ä¹ çš„æ–¹æ³•å¯ä»¥åœ¨ä¸å¢åŠ æˆæœ¬çš„æƒ…å†µä¸‹æé«˜ä½åœºå¼ºæ‰«æçš„è´¨é‡ã€‚ä¾‹å¦‚ï¼Œä¸å˜å‹å™¨æ¨¡å—èåˆçš„å·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNsï¼‰è¡¨ç°å‡ºæ•æ‰å±€éƒ¨ä¿¡æ¯å’Œè¿œè·ç¦»ä¸Šä¸‹æ–‡çš„èƒ½åŠ›ã€‚ç„¶è€Œï¼Œå˜å‹å™¨äºŒæ¬¡å¤æ‚æ€§å¯¼è‡´è¿œç¨‹æ•æ„Ÿæ€§å’Œå±€éƒ¨ç²¾åº¦ä¹‹é—´å‡ºç°äº†ä¸ç†æƒ³çš„æƒè¡¡ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ··åˆCNNå’ŒçŠ¶æ€ç©ºé—´æ¨¡å‹ï¼ˆSSMï¼‰æ¶æ„ï¼Œå…·æœ‰æ–°å‹3Dåˆ°1Dåºåˆ—åŒ–ï¼ˆGAMBASï¼‰ï¼Œèƒ½å¤Ÿåœ¨ä¸ç‰ºç‰²ç©ºé—´ç²¾åº¦çš„æƒ…å†µä¸‹å­¦ä¹ è¿œè·ç¦»ä¸Šä¸‹æ–‡ã€‚ä¸å…¶ä»–æœ€å…ˆè¿›çš„åŒ»å­¦å›¾åƒåˆ°å›¾åƒç¿»è¯‘æ¨¡å‹ç›¸æ¯”ï¼Œæˆ‘ä»¬å±•ç¤ºäº†æ›´å¥½çš„æ€§èƒ½ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.04523v1">PDF</a> Accepted for publication at MIDL 2025, 21 pages, 8 figures</p>
<p><strong>Summary</strong><br>     æ·±åº¦å­¦ä¹ æŠ€æœ¯èƒ½å¤Ÿæé«˜ä½ç£åœºå¼ºåº¦MRIæ‰«æå›¾åƒçš„è´¨é‡ï¼Œä»è€Œå®ç°ç¥ç»å‘è‚²ç ”ç©¶çš„éœ€æ±‚ã€‚æå‡ºä¸€ç§èåˆå·ç§¯ç¥ç»ç½‘ç»œä¸çŠ¶æ€ç©ºé—´æ¨¡å‹çš„æ¶æ„ï¼Œå…·æœ‰åˆ›æ–°æ€§çš„3Dè½¬1Dåºåˆ—åŒ–æ–¹æ³•ï¼ˆGAMBASï¼‰ï¼Œèƒ½å¤Ÿå…¼é¡¾è¿œç¨‹ä¸Šä¸‹æ–‡å­¦ä¹ å’Œç©ºé—´ç²¾åº¦ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>ç£å…±æŒ¯æˆåƒåœ¨ç¥ç»å‘è‚²ç ”ç©¶ä¸­éå¸¸é‡è¦ï¼Œä½†é«˜åœºç³»ç»Ÿåœ¨ä¸­ä½æ”¶å…¥å›½å®¶çš„æ™®åŠå—åˆ°æˆæœ¬é™åˆ¶ã€‚</li>
<li>è¶…ä½åœºç³»ç»Ÿç¼“è§£äº†è®¿é—®ä¸å¹³ç­‰é—®é¢˜ï¼Œä½†å…¶ä¿¡å™ªæ¯”é™ä½é™åˆ¶äº†å…¶åœ¨ç ”ç©¶å’Œä¸´åºŠä½¿ç”¨ä¸­çš„åº”ç”¨ã€‚</li>
<li>æ·±åº¦å­¦ä¹ æŠ€æœ¯å¯ä»¥æ— é¢å¤–è´¹ç”¨åœ°æé«˜ä½ç£åœºå¼ºåº¦MRIæ‰«æå›¾åƒçš„è´¨é‡ã€‚</li>
<li>å·ç§¯ç¥ç»ç½‘ç»œä¸è½¬æ¢å™¨æ¨¡å—çš„èåˆå±•ç¤ºäº†æ•æ‰å±€éƒ¨ä¿¡æ¯å’Œè¿œç¨‹ä¸Šä¸‹æ–‡çš„èƒ½åŠ›ã€‚</li>
<li>è½¬æ¢å™¨æ¨¡å—å…·æœ‰äºŒæ¬¡å¤æ‚æ€§ï¼Œåœ¨è¿œç¨‹æ•æ„Ÿæ€§å’Œå±€éƒ¨ç²¾ç¡®æ€§ä¹‹é—´å­˜åœ¨æƒè¡¡ã€‚</li>
<li>æå‡ºäº†ä¸€ç§æ–°å‹çš„æ··åˆCNNå’ŒSSMæ¶æ„ï¼Œç»“åˆäº†åˆ›æ–°çš„GAMBASæ–¹æ³•ï¼Œèƒ½å¤Ÿåœ¨ä¸ç‰ºç‰²ç©ºé—´ç²¾åº¦çš„æƒ…å†µä¸‹å­¦ä¹ è¿œç¨‹ä¸Šä¸‹æ–‡ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.04523">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-8ceec35dc68852974cd554c72d70ea7c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1a76effc938ffa1beb3957b8adb0d5c5.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b319942fd2673b7174371d6b24e37545.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Unveiling-the-intensity-dependent-wake-structure-of-Vela-X-1-using-MAXI-GSC"><a href="#Unveiling-the-intensity-dependent-wake-structure-of-Vela-X-1-using-MAXI-GSC" class="headerlink" title="Unveiling the intensity-dependent wake structure of Vela X-1 using   MAXI&#x2F;GSC"></a>Unveiling the intensity-dependent wake structure of Vela X-1 using   MAXI&#x2F;GSC</h2><p><strong>Authors:Abhisek Tamang, Kinjal Roy, Hemanth Manikantan, Ajith Balu, Biswajit Paul</strong></p>
<p>Context. Vela X-1 is among the earliest discovered high-mass X-ray binary (HMXB) pulsars. In such systems, the companionâ€™s stellar wind is strongly affected by ionisation from X-rays emitted by the compact object. A smooth, isotropic stellar wind model cannot account for the observed orbital variation in absorption column density. A stream-like photoionisation wake trailing the neutron star has been proposed to explain this variation. Aims. We investigated the variability of the circumbinary environment at different intensity levels of the Vela X-1 and used a model similar to the above-mentioned stream-like photoionisation wake to explain the asymmetric absorption column density present in the source. Methods. The 2.0-20.0 keV MAXI&#x2F;GSC spectrum was well modelled with a Comptonised continuum absorbed by local and interstellar material. Nearly 13 years of MAXI&#x2F;GSC data was used to constrain the variations in absorption column density in Vela X-1 from orbital-phase and intensity-and-orbital-phase resolved spectroscopy. Results. The long-term light curve of Vela X-1 shows orbit-to-orbit intensity level variations without any apparent super-orbital periodicity. The orbital-phase resolved spectroscopy in multiple intensity levels reveals asymmetric variation in absorption column density changes across the intensity levels. Conclusions. We confirm that the orbital variation in absorption column density in Vela X-1 cannot be explained by a smooth stellar wind alone using long-term MAXI&#x2F;GSC data. An additional component, such as a photoionisation or accretion wake, is required. The wake structure is present across intensity levels, with geometry varying by intensity. The long-term MAXI&#x2F;GSC data enabled us to vary wake parameters and derive best-fit stellar wind parameters for the time-averaged intensity, reproducing observed absorption column density across intensity levels. (Abbreviated.) </p>
<blockquote>
<p>èƒŒæ™¯ã€‚Vela X-1æ˜¯æœ€æ—©å‘ç°çš„é«˜è´¨é‡Xå°„çº¿åŒæ˜Ÿï¼ˆHMXBï¼‰è„‰å†²æ˜Ÿä¹‹ä¸€ã€‚åœ¨è¿™ç§ç³»ç»Ÿä¸­ï¼Œä¼´æ˜Ÿçš„æ’æ˜Ÿé£å—åˆ°æ¥è‡ªç´§å‡‘ç‰©ä½“å‘å‡ºçš„Xå°„çº¿çš„ç”µç¦»çš„å¼ºçƒˆå½±å“ã€‚å¹³æ»‘çš„ã€å„å‘åŒæ€§çš„æ’æ˜Ÿé£æ¨¡å‹æ— æ³•è§£é‡Šè§‚å¯Ÿåˆ°çš„è½¨é“å˜åŒ–ä¸­çš„å¸æ”¶æŸ±å¯†åº¦ã€‚å·²ç»æå‡ºäº†ç±»ä¼¼äºä¸­å­æ˜Ÿåé¢çš„æµçŠ¶å…‰ç¦»å­åŒ–å°¾è¿¹æ¥è§£é‡Šè¿™ç§å˜åŒ–ã€‚ç›®æ ‡ã€‚æˆ‘ä»¬ç ”ç©¶äº†Vela X-1åœ¨ä¸åŒå¼ºåº¦æ°´å¹³çš„äºŒå…ƒç¯å¢ƒå‘¨å›´çš„å˜åŒ–ï¼Œå¹¶ä½¿ç”¨ç±»ä¼¼äºä¸Šè¿°æµçŠ¶å…‰ç¦»å­åŒ–å°¾è¿¹çš„æ¨¡å‹æ¥è§£é‡Šæºä¸­å­˜åœ¨çš„ä¸å¯¹ç§°å¸æ”¶æŸ±å¯†åº¦ã€‚æ–¹æ³•ã€‚2.0-20.0 keVçš„MAXI&#x2F;GSCå…‰è°±è¢«æœ¬åœ°å’Œæ˜Ÿé™…ç‰©è´¨å¸æ”¶çš„åº·æ™®é¡¿è¿ç»­è°±å¾ˆå¥½åœ°å»ºæ¨¡ã€‚ä½¿ç”¨è¿‘13å¹´çš„MAXI&#x2F;GSCæ•°æ®ï¼Œé€šè¿‡è½¨é“ç›¸ä½å’Œå¼ºåº¦ä¸è½¨é“ç›¸ä½è§£æå…‰è°±æ¥é™åˆ¶Vela X-1ä¸­å¸æ”¶æŸ±å¯†åº¦çš„å˜åŒ–ã€‚ç»“æœã€‚Vela X-1çš„é•¿æœŸå…‰å˜æ›²çº¿æ˜¾ç¤ºå‡ºè½¨é“åˆ°è½¨é“çš„å¼ºåº¦æ°´å¹³å˜åŒ–ï¼Œæ²¡æœ‰æ˜æ˜¾çš„è¶…è½¨é“å‘¨æœŸæ€§ã€‚å¤šå¼ºåº¦æ°´å¹³çš„è½¨é“ç›¸ä½è§£æå…‰è°±æ˜¾ç¤ºå‡ºå¸æ”¶æŸ±å¯†åº¦å˜åŒ–çš„ä¸å¯¹ç§°å˜åŒ–ã€‚ç»“è®ºã€‚æˆ‘ä»¬ä½¿ç”¨é•¿æœŸçš„MAXI&#x2F;GSCæ•°æ®ç¡®è®¤ï¼Œä»…å‡­å¹³æ»‘çš„æ’æ˜Ÿé£æ— æ³•è§£é‡ŠVela X-1ä¸­å¸æ”¶æŸ±å¯†åº¦çš„è½¨é“å˜åŒ–ã€‚éœ€è¦é¢å¤–çš„æˆåˆ†ï¼Œä¾‹å¦‚å…‰ç¦»å­åŒ–æˆ–å¸ç§¯å°¾è¿¹ã€‚å°¾è¿¹ç»“æ„å­˜åœ¨äºå„ä¸ªå¼ºåº¦æ°´å¹³ï¼Œå¹¶ä¸”å…¶å‡ ä½•å½¢çŠ¶éšå¼ºåº¦è€Œå˜åŒ–ã€‚é•¿æœŸçš„MAXI&#x2F;GSCæ•°æ®ä½¿æˆ‘ä»¬èƒ½å¤Ÿæ”¹å˜å°¾è¿¹å‚æ•°ï¼Œå¹¶æ ¹æ®æ—¶é—´å¹³å‡å¼ºåº¦å¾—å‡ºæœ€ä½³çš„æ’æ˜Ÿé£å‚æ•°ï¼Œä»è€Œå†ç°äº†è§‚å¯Ÿåˆ°çš„å¸æ”¶æŸ±å¯†åº¦å˜åŒ–ã€‚ï¼ˆæ‘˜è¦ï¼‰ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.04504v1">PDF</a> 10 pages, 15 figures, 3 tables, Accepted for publication in Astronomy   &amp; Astrophysics</p>
<p><strong>æ‘˜è¦</strong></p>
<p>Vela X-1æ˜¯æ—©æœŸå‘ç°çš„é«˜è´¨é‡Xå°„çº¿åŒæ˜Ÿï¼ˆHMXBï¼‰è„‰å†²æ˜Ÿä¹‹ä¸€ã€‚åœ¨è¿™ç§ç³»ç»Ÿä¸­ï¼Œä¼´æ˜Ÿçš„æ’æ˜Ÿé£å—åˆ°æ¥è‡ªç´§å‡‘ç‰©ä½“å‘å‡ºçš„Xå°„çº¿çš„ç”µç¦»å½±å“ã€‚å¹³æ»‘çš„ã€å„å‘åŒæ€§çš„æ’æ˜Ÿé£æ¨¡å‹æ— æ³•è§£é‡Šè§‚å¯Ÿåˆ°çš„è½¨é“å˜åŒ–ä¸­çš„å¸æ”¶æŸ±å¯†åº¦ã€‚å·²ç»æå‡ºç±»ä¼¼æµçŠ¶çš„å…‰ç”µç¦»å°¾è¿¹æ¥è§£é‡Šè¿™ç§å˜åŒ–ã€‚é€šè¿‡è°ƒæŸ¥ä¸åŒå¼ºåº¦æ°´å¹³çš„Vela X-1å‘¨å›´çš„äºŒè¿›åˆ¶ç¯å¢ƒï¼Œå¹¶ä½¿ç”¨ç±»ä¼¼äºä¸Šè¿°æµçŠ¶å…‰ç”µç¦»å°¾è¿¹çš„æ¨¡å‹æ¥è§£é‡Šæºä¸­å­˜åœ¨çš„ä¸å¯¹ç§°å¸æ”¶æŸ±å¯†åº¦ï¼Œæˆ‘ä»¬å‘ç°é•¿æœŸå…‰æ›²çº¿æ˜¾ç¤ºè½¨é“åˆ°è½¨é“çš„å¼ºåº¦æ°´å¹³å˜åŒ–ï¼Œæ²¡æœ‰æ˜æ˜¾çš„è¶…è½¨é“å‘¨æœŸæ€§ã€‚åœ¨å¤šä¸ªå¼ºåº¦æ°´å¹³ä¸Šçš„è½¨é“ç›¸ä½è§£æå…‰è°±æ˜¾ç¤ºä¸å¯¹ç§°çš„å¸æ”¶æŸ±å¯†åº¦å˜åŒ–ã€‚æˆ‘ä»¬ç¡®è®¤ï¼Œä»…ä½¿ç”¨å¹³æ»‘çš„æ’æ˜Ÿé£æ— æ³•è§£é‡ŠVela X-1ä¸­çš„è½¨é“å¸æ”¶æŸ±å¯†åº¦å˜åŒ–ã€‚éœ€è¦é¢å¤–çš„ç»„ä»¶ï¼Œä¾‹å¦‚å…‰ç”µç¦»æˆ–ç§¯å¢å°¾è¿¹ã€‚å°¾è¿¹ç»“æ„å­˜åœ¨äºå„ä¸ªå¼ºåº¦æ°´å¹³ï¼Œå…¶å‡ ä½•å½¢çŠ¶éšå¼ºåº¦è€Œå˜åŒ–ã€‚é•¿æœŸMAXI&#x2F;GSCæ•°æ®ä½¿æˆ‘ä»¬èƒ½å¤Ÿæ”¹å˜å°¾è¿¹å‚æ•°ï¼Œå¹¶å¾—å‡ºæ—¶é—´å¹³å‡å¼ºåº¦çš„æœ€ä½³æ’æ˜Ÿé£å‚æ•°ï¼Œä»¥å†ç°è§‚å¯Ÿåˆ°çš„å¸æ”¶æŸ±å¯†åº¦å˜åŒ–ã€‚</p>
<p><strong>è¦ç‚¹</strong></p>
<ol>
<li>Vela X-1æ˜¯é«˜è´¨é‡Xå°„çº¿åŒæ˜Ÿï¼ˆHMXBï¼‰è„‰å†²æ˜Ÿï¼Œå…¶ä¼´éšçš„æ’æ˜Ÿé£å—åˆ°Xå°„çº¿ç”µç¦»çš„å½±å“ã€‚</li>
<li>è½¨é“å˜åŒ–çš„å¸æ”¶æŸ±å¯†åº¦æ— æ³•ç”¨å¹³æ»‘çš„æ’æ˜Ÿé£æ¨¡å‹è§£é‡Šã€‚</li>
<li>æå‡ºæµçŠ¶å…‰ç”µç¦»å°¾è¿¹æ¥è§£é‡Šè¿™ç§å˜åŒ–ã€‚</li>
<li>é€šè¿‡åˆ†æé•¿æœŸMAXI&#x2F;GSCæ•°æ®ï¼Œå‘ç°Vela X-1çš„è½¨é“åˆ°è½¨é“å¼ºåº¦æ°´å¹³å˜åŒ–æ²¡æœ‰è¶…è½¨é“å‘¨æœŸæ€§ã€‚</li>
<li>åœ¨å¤šä¸ªå¼ºåº¦æ°´å¹³çš„è½¨é“ç›¸ä½è§£æå…‰è°±æ˜¾ç¤ºä¸å¯¹ç§°çš„å¸æ”¶æŸ±å¯†åº¦å˜åŒ–ã€‚</li>
<li>éœ€è¦é¢å¤–çš„ç»„ä»¶ï¼ˆå¦‚å…‰ç”µç¦»å°¾è¿¹ï¼‰æ¥è§£é‡Šè§‚å¯Ÿåˆ°çš„å¸æ”¶æŸ±å¯†åº¦å˜åŒ–ï¼Œè¯¥å°¾è¿¹ç»“æ„å­˜åœ¨äºä¸åŒçš„å¼ºåº¦æ°´å¹³ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.04504">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-30ef6782332217f39feae5585022d3aa.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b35d554acbeaced5e7cf44a636c02a9e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-edc7bfdb28d140aa18a3d6d1e41a9038.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d88589abf3a6346c65cde0e73f610657.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-196b63f4cf5f814179981a801d31edeb.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0245c411dd9e77d8ab36a2327e6c0b1f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-10207634a51bcccf5fc7be09b255dc51.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4c90b886119299e8a86ef4ef583512eb.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="Generative-Large-Language-Models-Trained-for-Detecting-Errors-in-Radiology-Reports"><a href="#Generative-Large-Language-Models-Trained-for-Detecting-Errors-in-Radiology-Reports" class="headerlink" title="Generative Large Language Models Trained for Detecting Errors in   Radiology Reports"></a>Generative Large Language Models Trained for Detecting Errors in   Radiology Reports</h2><p><strong>Authors:Cong Sun, Kurt Teichman, Yiliang Zhou, Brian Critelli, David Nauheim, Graham Keir, Xindi Wang, Judy Zhong, Adam E Flanders, George Shih, Yifan Peng</strong></p>
<p>In this retrospective study, a dataset was constructed with two parts. The first part included 1,656 synthetic chest radiology reports generated by GPT-4 using specified prompts, with 828 being error-free synthetic reports and 828 containing errors. The second part included 614 reports: 307 error-free reports between 2011 and 2016 from the MIMIC-CXR database and 307 corresponding synthetic reports with errors generated by GPT-4 on the basis of these MIMIC-CXR reports and specified prompts. All errors were categorized into four types: negation, left&#x2F;right, interval change, and transcription errors. Then, several models, including Llama-3, GPT-4, and BiomedBERT, were refined using zero-shot prompting, few-shot prompting, or fine-tuning strategies. Finally, the performance of these models was evaluated using the F1 score, 95% confidence interval (CI) and paired-sample t-tests on our constructed dataset, with the prediction results further assessed by radiologists. Using zero-shot prompting, the fine-tuned Llama-3-70B-Instruct model achieved the best performance with the following F1 scores: 0.769 for negation errors, 0.772 for left&#x2F;right errors, 0.750 for interval change errors, 0.828 for transcription errors, and 0.780 overall. In the real-world evaluation phase, two radiologists reviewed 200 randomly selected reports output by the model. Of these, 99 were confirmed to contain errors detected by the models by both radiologists, and 163 were confirmed to contain model-detected errors by at least one radiologist. Generative LLMs, fine-tuned on synthetic and MIMIC-CXR radiology reports, greatly enhanced error detection in radiology reports. </p>
<blockquote>
<p>åœ¨æœ¬æ¬¡å›é¡¾æ€§ç ”ç©¶ä¸­ï¼Œæ„å»ºäº†åŒ…å«ä¸¤éƒ¨åˆ†çš„æ•°æ®é›†ã€‚ç¬¬ä¸€éƒ¨åˆ†åŒ…å«ç”±GPT-4æ ¹æ®æŒ‡å®šæç¤ºç”Ÿæˆçš„1656ä»½åˆæˆèƒ¸éƒ¨æ”¾å°„å­¦æŠ¥å‘Šï¼Œå…¶ä¸­828ä»½æ˜¯æ— é”™è¯¯çš„åˆæˆæŠ¥å‘Šï¼Œå¦å¤–828ä»½å«æœ‰é”™è¯¯ã€‚ç¬¬äºŒéƒ¨åˆ†åŒ…å«614ä»½æŠ¥å‘Šï¼Œå…¶ä¸­åŒ…æ‹¬æ¥è‡ªMIMIC-CXRæ•°æ®åº“çš„2011å¹´è‡³2016å¹´é—´çš„307ä»½æ— é”™è¯¯æŠ¥å‘Šï¼Œä»¥åŠåŸºäºè¿™äº›MIMIC-CXRæŠ¥å‘Šå’Œç‰¹å®šæç¤ºç”±GPT-4ç”Ÿæˆçš„307ä»½ç›¸åº”çš„åˆæˆæŠ¥å‘Šï¼Œå…¶ä¸­å«æœ‰é”™è¯¯ã€‚æ‰€æœ‰é”™è¯¯éƒ½è¢«åˆ†ä¸ºå››ç§ç±»å‹ï¼šå¦å®šã€å·¦å³ã€é—´éš”å˜åŒ–å’Œè½¬å½•é”™è¯¯ã€‚ç„¶åï¼Œä½¿ç”¨é›¶æ ·æœ¬æç¤ºã€å°‘æ ·æœ¬æç¤ºæˆ–å¾®è°ƒç­–ç•¥å¯¹åŒ…æ‹¬Llama-3ã€GPT-4å’ŒBiomedBERTåœ¨å†…çš„å‡ ä¸ªæ¨¡å‹è¿›è¡Œäº†æ”¹è¿›ã€‚æœ€åï¼Œä½¿ç”¨F1å¾—åˆ†ã€95%ç½®ä¿¡åŒºé—´ï¼ˆCIï¼‰å’Œé…å¯¹æ ·æœ¬tæ£€éªŒå¯¹æˆ‘ä»¬æ„å»ºçš„æ•°æ®é›†ä¸Šè¿™äº›æ¨¡å‹çš„è¡¨ç°è¿›è¡Œäº†è¯„ä¼°ï¼Œé¢„æµ‹ç»“æœè¿˜å¾—åˆ°äº†æ”¾å°„ç§‘çš„è¿›ä¸€æ­¥è¯„ä¼°ã€‚åœ¨é›¶æ ·æœ¬æç¤ºä¸‹ï¼Œç»è¿‡å¾®è°ƒçš„Llama-3-70B-Instructæ¨¡å‹åœ¨ä»¥ä¸‹F1å¾—åˆ†ä¸Šè¡¨ç°æœ€ä½³ï¼šå¦å®šé”™è¯¯ä¸º0.769ï¼Œå·¦å³é”™è¯¯ä¸º0.772ï¼Œé—´éš”å˜åŒ–é”™è¯¯ä¸º0.750ï¼Œè½¬å½•é”™è¯¯ä¸º0.828ï¼Œæ€»ä½“ä¸º0.780ã€‚åœ¨ç°å®ä¸–ç•Œè¯„ä¼°é˜¶æ®µï¼Œä¸¤åæ”¾å°„ç§‘åŒ»ç”Ÿå®¡æŸ¥äº†æ¨¡å‹è¾“å‡ºçš„200ä»½éšæœºé€‰æ‹©çš„æŠ¥å‘Šã€‚å…¶ä¸­ï¼Œ99ä»½è¢«æ¨¡å‹å’Œä¸¤ä½æ”¾å°„ç§‘åŒ»ç”Ÿéƒ½ç¡®è®¤å«æœ‰é”™è¯¯ï¼Œ163ä»½è¢«æ¨¡å‹æˆ–è‡³å°‘ä¸€ä½æ”¾å°„ç§‘åŒ»ç”Ÿç¡®è®¤å«æœ‰é”™è¯¯ã€‚åœ¨åˆæˆå’ŒMIMIC-CXRæ”¾å°„å­¦æŠ¥å‘Šä¸Šç»è¿‡å¾®è°ƒçš„ç”Ÿæˆå¼LLMæå¤§åœ°æé«˜äº†æ”¾å°„å­¦æŠ¥å‘Šä¸­çš„é”™è¯¯æ£€æµ‹èƒ½åŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.04336v1">PDF</a> </p>
<p><strong>Summary</strong><br>     æœ¬ç ”ç©¶åˆ©ç”¨åˆæˆæŠ¥å‘Šå’ŒçœŸå®æŠ¥å‘Šæ„å»ºäº†æ•°æ®é›†ï¼Œå¯¹LLama-3ã€GPT-4å’ŒBiomedBERTç­‰æ¨¡å‹è¿›è¡Œè®­ç»ƒï¼Œå¹¶è¯„ä¼°å…¶åœ¨æ£€æµ‹èƒ¸éƒ¨æ”¾å°„å­¦æŠ¥å‘Šé”™è¯¯æ–¹é¢çš„æ€§èƒ½ã€‚ç»“æœæ˜¾ç¤ºï¼Œç»è¿‡å¾®è°ƒçš„Llama-3æ¨¡å‹åœ¨é›¶æ ·æœ¬æç¤ºä¸‹è¡¨ç°æœ€ä½³ï¼Œç”Ÿæˆå¼LLMæ¨¡å‹åœ¨åˆæˆæŠ¥å‘Šå’ŒMIMIC-CXRæ•°æ®åº“æŠ¥å‘Šä¸Šçš„å¾®è°ƒæœ‰åŠ©äºå¢å¼ºæŠ¥å‘Šé”™è¯¯çš„æ£€æµ‹èƒ½åŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç ”ç©¶æ„å»ºäº†ä¸€ä¸ªåŒ…å«åˆæˆå’ŒçœŸå®èƒ¸éƒ¨æ”¾å°„å­¦æŠ¥å‘Šçš„æ•°æ®é›†ï¼Œç”¨äºæ¨¡å‹è®­ç»ƒã€‚</li>
<li>æ•°æ®é›†ä¸­çš„æŠ¥å‘ŠåŒ…å«ä¸åŒç±»å‹çš„é”™è¯¯ï¼Œå¦‚å¦å®šã€å·¦å³ã€é—´éš”å˜åŒ–å’Œè½¬å½•é”™è¯¯ã€‚</li>
<li>ç ”ç©¶è¯„ä¼°äº†Llama-3ã€GPT-4å’ŒBiomedBERTç­‰æ¨¡å‹åœ¨æŠ¥å‘Šé”™è¯¯æ£€æµ‹æ–¹é¢çš„æ€§èƒ½ã€‚</li>
<li>ç»è¿‡å¾®è°ƒå’Œé›¶æ ·æœ¬æç¤ºï¼ŒLlama-3æ¨¡å‹è¡¨ç°å‡ºæœ€ä½³æ€§èƒ½ã€‚</li>
<li>åœ¨çœŸå®ä¸–ç•Œè¯„ä¼°é˜¶æ®µï¼Œæ¨¡å‹æ£€æµ‹åˆ°çš„é”™è¯¯å¾—åˆ°äº†æ”¾å°„ç§‘åŒ»å¸ˆçš„ç¡®è®¤ã€‚</li>
<li>ç”Ÿæˆçš„LLMæ¨¡å‹åœ¨åˆæˆå’ŒMIMIC-CXRæ”¾å°„å­¦æŠ¥å‘Šä¸Šçš„å¾®è°ƒæœ‰åŠ©äºå¢å¼ºæŠ¥å‘Šé”™è¯¯çš„æ£€æµ‹èƒ½åŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.04336">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-f0cc4cb416b94e2569db037861d86b72.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-eba34acfe32e4813e1292af49bc46720.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-18d6d9efc97f5765c8f64506d7039f6a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-dde40e079c4d03743c8bc4fec0275ee2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a771de8fde7e12ddcc623004e214a16d.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="Improving-Chronic-Kidney-Disease-Detection-Efficiency-Fine-Tuned-CatBoost-and-Nature-Inspired-Algorithms-with-Explainable-AI"><a href="#Improving-Chronic-Kidney-Disease-Detection-Efficiency-Fine-Tuned-CatBoost-and-Nature-Inspired-Algorithms-with-Explainable-AI" class="headerlink" title="Improving Chronic Kidney Disease Detection Efficiency: Fine Tuned   CatBoost and Nature-Inspired Algorithms with Explainable AI"></a>Improving Chronic Kidney Disease Detection Efficiency: Fine Tuned   CatBoost and Nature-Inspired Algorithms with Explainable AI</h2><p><strong>Authors:Md. Ehsanul Haque, S. M. Jahidul Islam, Jeba Maliha, Md. Shakhauat Hossan Sumon, Rumana Sharmin, Sakib Rokoni</strong></p>
<p>Chronic Kidney Disease (CKD) is a major global health issue which is affecting million people around the world and with increasing rate of mortality. Mitigation of progression of CKD and better patient outcomes requires early detection. Nevertheless, limitations lie in traditional diagnostic methods, especially in resource constrained settings. This study proposes an advanced machine learning approach to enhance CKD detection by evaluating four models: Random Forest (RF), Multi-Layer Perceptron (MLP), Logistic Regression (LR), and a fine-tuned CatBoost algorithm. Specifically, among these, the fine-tuned CatBoost model demonstrated the best overall performance having an accuracy of 98.75%, an AUC of 0.9993 and a Kappa score of 97.35% of the studies. The proposed CatBoost model has used a nature inspired algorithm such as Simulated Annealing to select the most important features, Cuckoo Search to adjust outliers and grid search to fine tune its settings in such a way to achieve improved prediction accuracy. Features significance is explained by SHAP-a well-known XAI technique-for gaining transparency in the decision-making process of proposed model and bring up trust in diagnostic systems. Using SHAP, the significant clinical features were identified as specific gravity, serum creatinine, albumin, hemoglobin, and diabetes mellitus. The potential of advanced machine learning techniques in CKD detection is shown in this research, particularly for low income and middle-income healthcare settings where prompt and correct diagnoses are vital. This study seeks to provide a highly accurate, interpretable, and efficient diagnostic tool to add to efforts for early intervention and improved healthcare outcomes for all CKD patients. </p>
<blockquote>
<p>æ…¢æ€§è‚¾è„ç—…ï¼ˆCKDï¼‰æ˜¯ä¸€ä¸ªå…¨çƒæ€§çš„é‡å¤§å¥åº·é—®é¢˜ï¼Œå½±å“å…¨çƒæ•°åƒä¸‡äººï¼Œä¸”æ­»äº¡ç‡å‘ˆä¸Šå‡è¶‹åŠ¿ã€‚å‡ç¼“CKDçš„è¿›å±•å¹¶æ”¹å–„æ‚£è€…é¢„åéœ€è¦æ—©æœŸå‘ç°ã€‚ç„¶è€Œï¼Œä¼ ç»Ÿè¯Šæ–­æ–¹æ³•å­˜åœ¨å±€é™æ€§ï¼Œç‰¹åˆ«æ˜¯åœ¨èµ„æºå—é™çš„ç¯å¢ƒä¸­ã€‚æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§å…ˆè¿›çš„æœºå™¨å­¦ä¹ æ–¹æ³•æ¥æé«˜CKDçš„æ£€æµ‹èƒ½åŠ›ï¼Œè¯„ä¼°äº†å››ç§æ¨¡å‹ï¼šéšæœºæ£®æ—ï¼ˆRFï¼‰ã€å¤šå±‚æ„ŸçŸ¥å™¨ï¼ˆMLPï¼‰ã€é€»è¾‘å›å½’ï¼ˆLRï¼‰å’Œç»è¿‡è°ƒä¼˜çš„CatBoostç®—æ³•ã€‚ç‰¹åˆ«æ˜¯å…¶ä¸­ï¼Œç»è¿‡è°ƒä¼˜çš„CatBoostæ¨¡å‹è¡¨ç°å‡ºäº†æœ€ä½³çš„æ€»ä½“æ€§èƒ½ï¼Œå…¶å‡†ç¡®åº¦ä¸º98.75%ï¼ŒAUCä¸º0.9993ï¼ŒKappaå¾—åˆ†ä¸ºç ”ç©¶çš„97.35%ã€‚æ‰€æå‡ºçš„CatBoostæ¨¡å‹ä½¿ç”¨äº†ä¸€ç§å—è‡ªç„¶å¯å‘çš„ç®—æ³•ï¼Œå¦‚æ¨¡æ‹Ÿé€€ç«ç®—æ³•æ¥é€‰æ‹©æœ€é‡è¦çš„ç‰¹å¾ï¼Œåˆ©ç”¨å¸ƒè°·é¸Ÿæœç´¢è°ƒæ•´å¼‚å¸¸å€¼ï¼Œå¹¶ä½¿ç”¨ç½‘æ ¼æœç´¢å¾®è°ƒå…¶è®¾ç½®ï¼Œä»è€Œå®ç°æé«˜é¢„æµ‹ç²¾åº¦çš„ç›®çš„ã€‚é€šè¿‡SHAPï¼ˆä¸€ç§ä¼—æ‰€å‘¨çŸ¥çš„XAIæŠ€æœ¯ï¼‰è§£é‡Šç‰¹å¾çš„é‡è¦æ€§ï¼Œä»¥è·å¾—å†³ç­–è¿‡ç¨‹çš„é€æ˜åº¦å¹¶å¯¹è¯Šæ–­ç³»ç»Ÿå»ºç«‹ä¿¡ä»»ã€‚ä½¿ç”¨SHAPï¼Œç¡®å®šçš„å…³é”®ä¸´åºŠç‰¹å¾ä¸ºç‰¹å®šæ¯”é‡ã€è¡€æ¸…è‚Œé…ã€ç™½è›‹ç™½ã€è¡€çº¢è›‹ç™½å’Œç³–å°¿ç—…ã€‚è¿™é¡¹ç ”ç©¶è¡¨æ˜å…ˆè¿›æœºå™¨å­¦ä¹ æŠ€æœ¯åœ¨CKDæ£€æµ‹ä¸­çš„æ½œåŠ›ï¼Œç‰¹åˆ«æ˜¯åœ¨ä½æ”¶å…¥å’Œä¸­æ”¶å…¥åŒ»ç–—ä¿å¥ç¯å¢ƒä¸­ï¼Œå¿«é€Ÿè€Œæ­£ç¡®çš„è¯Šæ–­è‡³å…³é‡è¦ã€‚æœ¬ç ”ç©¶æ—¨åœ¨ä¸ºæ‰€æœ‰CKDæ‚£è€…æä¾›é«˜åº¦å‡†ç¡®ã€å¯è§£é‡Šå’Œé«˜æ•ˆçš„è¯Šæ–­å·¥å…·ï¼Œä»¥æ”¯æŒæ—©æœŸå¹²é¢„å’Œæ”¹å–„åŒ»ç–—ä¿å¥ç»“æœã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.04262v1">PDF</a> 8 page, 8 figures , conference : 14th IEEE International Conference   on Communication Systems and Network Technologies (CSNT2025)</p>
<p><strong>æ‘˜è¦</strong><br>    æœ¬ç ”ç©¶é‡‡ç”¨å…ˆè¿›çš„æœºå™¨å­¦ä¹ æ–¹æ³•ï¼Œå¯¹æ…¢æ€§è‚¾ç—…ï¼ˆCKDï¼‰è¿›è¡Œæ—©æœŸæ£€æµ‹ã€‚ç»è¿‡å¯¹æ¯”å››ç§æ¨¡å‹ï¼Œå‘ç°ç»è¿‡ç²¾ç»†è°ƒæ•´çš„CatBoostæ¨¡å‹è¡¨ç°æœ€ä½³ï¼Œå‡†ç¡®åº¦é«˜è¾¾98.75%ï¼ŒAUCå€¼ä¸º0.9993ï¼ŒKappaå¾—åˆ†ä¸º97.35%ã€‚è¯¥æ¨¡å‹é‡‡ç”¨æ¨¡æ‹Ÿé€€ç«ç­‰è‡ªç„¶å¯å‘ç®—æ³•é€‰æ‹©é‡è¦ç‰¹å¾ï¼Œé‡‡ç”¨å¸ƒè°·é¸Ÿæœç´¢è°ƒæ•´å¼‚å¸¸å€¼ï¼Œå¹¶é‡‡ç”¨ç½‘æ ¼æœç´¢è¿›è¡Œç²¾ç»†è°ƒæ•´ï¼Œä»¥æé«˜é¢„æµ‹å‡†ç¡®æ€§ã€‚åŒæ—¶é‡‡ç”¨SHAPè§£é‡Šç‰¹å¾é‡è¦æ€§ï¼Œæœ‰åŠ©äºå»ºç«‹è¯Šæ–­å†³ç­–è¿‡ç¨‹çš„é€æ˜åº¦ï¼Œå¹¶ä¸ºè¯Šæ–­ç³»ç»Ÿå»ºç«‹ä¿¡ä»»ã€‚æœ¬ç ”ç©¶æ­ç¤ºäº†æœºå™¨å­¦ä¹ æ–¹æ³•åœ¨æ…¢æ€§è‚¾ç—…æ£€æµ‹ä¸­çš„æ½œåŠ›ï¼Œç‰¹åˆ«æ˜¯å¯¹æ”¶å…¥è¾ƒä½å’Œä¸­ç­‰åŒ»ç–—ç¯å¢ƒçš„å›½å®¶å’Œåœ°åŒºå…·æœ‰é‡è¦å½±å“ã€‚å®ƒä¸ºæ—©æœŸå¹²é¢„å’Œæ‰€æœ‰æ…¢æ€§è‚¾ç—…æ‚£è€…çš„å¥åº·ç»“æœæ”¹å–„æä¾›äº†ä¸€ä¸ªå‡†ç¡®ã€å¯è§£é‡Šå’Œé«˜æ•ˆçš„è¯Šæ–­å·¥å…·ã€‚<br>    <strong>å…³é”®è§è§£</strong><br>    1. æ…¢æ€§è‚¾ç—…ï¼ˆCKDï¼‰æ˜¯ä¸€ä¸ªå…¨çƒæ€§çš„å¥åº·é—®é¢˜ï¼Œå½±å“ç€æ•°ç™¾ä¸‡äººï¼Œä¸”æ­»äº¡ç‡ä¸æ–­ä¸Šå‡ã€‚<br>    2. ä¼ ç»Ÿè¯Šæ–­æ–¹æ³•å­˜åœ¨å±€é™æ€§ï¼Œå°¤å…¶æ˜¯åœ¨èµ„æºå—é™çš„ç¯å¢ƒä¸­ã€‚<br>    3. ç ”ç©¶æå‡ºäº†ä¸€ç§åŸºäºæœºå™¨å­¦ä¹ çš„æ”¹è¿›CKDæ£€æµ‹æ–¹æ³•ï¼Œå…¶ä¸­CatBoostæ¨¡å‹è¡¨ç°æœ€ä½³ï¼Œå‡†ç¡®åº¦é«˜ã€‚<br>    4. CatBoostæ¨¡å‹ä½¿ç”¨æ¨¡æ‹Ÿé€€ç«ç­‰è‡ªç„¶å¯å‘ç®—æ³•é€‰æ‹©é‡è¦ç‰¹å¾ï¼Œå¹¶é‡‡ç”¨å¸ƒè°·é¸Ÿæœç´¢å’Œç½‘æ ¼æœç´¢è¿›è¡Œä¼˜åŒ–ã€‚<br>    5. SHAPè¢«ç”¨æ¥è§£é‡Šç‰¹å¾çš„é‡è¦æ€§ï¼Œä»¥å¢åŠ è¯Šæ–­è¿‡ç¨‹çš„é€æ˜åº¦å¹¶å»ºç«‹ä¿¡ä»»ã€‚<br>    6. ç ”ç©¶è¯†åˆ«äº†å…³é”®çš„ä¸´åºŠç‰¹å¾ï¼Œå¦‚ç‰¹å®šæ¯”é‡ã€è¡€æ¸…è‚Œï¿½ï¿½.ç™½è›‹ç™½ã€è¡€çº¢è›‹ç™½å’Œç³–å°¿ç—…ç­‰ã€‚</p>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.04262">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-1076c9217a4fd398d5e1d085289057bc.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e61beb98f3820d26f07a887a0e13de49.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-45836e225b7a2c2f2f7af5cee07c9881.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b9a99cf56fe7cd76169ef41c343dadae.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ba2a7aafb3869f3de9a0c075f1f0dc44.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3a9a28ba2da3b70c303fa2d2b7f78964.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-dd3616cea05a628248d618a82e1f0967.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d85d4a62b11e99e81df53020934c9e05.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-3198b1295d948236b5750dfc11cadf4a.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-baaac898c40625b3aa43f8a0d6604e64.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c4c1aa70996abe4b117767e589e50def.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="DocSAM-Unified-Document-Image-Segmentation-via-Query-Decomposition-and-Heterogeneous-Mixed-Learning"><a href="#DocSAM-Unified-Document-Image-Segmentation-via-Query-Decomposition-and-Heterogeneous-Mixed-Learning" class="headerlink" title="DocSAM: Unified Document Image Segmentation via Query Decomposition and   Heterogeneous Mixed Learning"></a>DocSAM: Unified Document Image Segmentation via Query Decomposition and   Heterogeneous Mixed Learning</h2><p><strong>Authors:Xiao-Hui Li, Fei Yin, Cheng-Lin Liu</strong></p>
<p>Document image segmentation is crucial for document analysis and recognition but remains challenging due to the diversity of document formats and segmentation tasks. Existing methods often address these tasks separately, resulting in limited generalization and resource wastage. This paper introduces DocSAM, a transformer-based unified framework designed for various document image segmentation tasks, such as document layout analysis, multi-granularity text segmentation, and table structure recognition, by modelling these tasks as a combination of instance and semantic segmentation. Specifically, DocSAM employs Sentence-BERT to map category names from each dataset into semantic queries that match the dimensionality of instance queries. These two sets of queries interact through an attention mechanism and are cross-attended with image features to predict instance and semantic segmentation masks. Instance categories are predicted by computing the dot product between instance and semantic queries, followed by softmax normalization of scores. Consequently, DocSAM can be jointly trained on heterogeneous datasets, enhancing robustness and generalization while reducing computational and storage resources. Comprehensive evaluations show that DocSAM surpasses existing methods in accuracy, efficiency, and adaptability, highlighting its potential for advancing document image understanding and segmentation across various applications. Codes are available at <a target="_blank" rel="noopener" href="https://github.com/xhli-git/DocSAM">https://github.com/xhli-git/DocSAM</a>. </p>
<blockquote>
<p>æ–‡æ¡£å›¾åƒåˆ†å‰²å¯¹äºæ–‡æ¡£åˆ†æå’Œè¯†åˆ«è‡³å…³é‡è¦ï¼Œä½†ç”±äºæ–‡æ¡£æ ¼å¼å’Œåˆ†å‰²ä»»åŠ¡çš„å¤šæ ·æ€§ï¼Œå®ƒä»ç„¶æ˜¯ä¸€ä¸ªæŒ‘æˆ˜ã€‚ç°æœ‰æ–¹æ³•é€šå¸¸å°†è¿™äº›ä»»åŠ¡åˆ†åˆ«å¤„ç†ï¼Œå¯¼è‡´æ³›åŒ–èƒ½åŠ›æœ‰é™å’Œèµ„æºæµªè´¹ã€‚æœ¬æ–‡ä»‹ç»äº†DocSAMï¼Œè¿™æ˜¯ä¸€ä¸ªåŸºäºå˜å‹å™¨çš„ç»Ÿä¸€æ¡†æ¶ï¼Œæ—¨åœ¨ä¸ºå„ç§æ–‡æ¡£å›¾åƒåˆ†å‰²ä»»åŠ¡ï¼ˆå¦‚æ–‡æ¡£å¸ƒå±€åˆ†æã€å¤šç²’åº¦æ–‡æœ¬åˆ†å‰²å’Œè¡¨æ ¼ç»“æ„è¯†åˆ«ï¼‰è€Œè®¾è®¡ã€‚å®ƒé€šè¿‡å°†è¿™äº›ä»»åŠ¡å»ºæ¨¡ä¸ºå®ä¾‹åˆ†å‰²å’Œè¯­ä¹‰åˆ†å‰²çš„ç»„åˆæ¥å®ç°è¿™ä¸€ç›®æ ‡ã€‚å…·ä½“æ¥è¯´ï¼ŒDocSAMé‡‡ç”¨Sentence-BERTå°†æ¯ä¸ªæ•°æ®é›†ä¸­çš„ç±»åˆ«åç§°æ˜ å°„åˆ°è¯­ä¹‰æŸ¥è¯¢ä¸Šï¼Œè¿™äº›æŸ¥è¯¢ä¸å®ä¾‹æŸ¥è¯¢çš„ç»´åº¦ç›¸åŒ¹é…ã€‚è¿™ä¸¤ç»„æŸ¥è¯¢é€šè¿‡æ³¨æ„åŠ›æœºåˆ¶è¿›è¡Œäº¤äº’ï¼Œå¹¶é€šè¿‡ä¸å›¾åƒç‰¹å¾çš„äº¤å‰æ³¨æ„åŠ›æ¥é¢„æµ‹å®ä¾‹å’Œè¯­ä¹‰åˆ†å‰²æ©ç ã€‚é€šè¿‡è®¡ç®—å®ä¾‹æŸ¥è¯¢å’Œè¯­ä¹‰æŸ¥è¯¢ä¹‹é—´çš„ç‚¹ç§¯æ¥é¢„æµ‹å®ä¾‹ç±»åˆ«ï¼Œç„¶åå¯¹åˆ†æ•°è¿›è¡Œsoftmaxå½’ä¸€åŒ–ã€‚å› æ­¤ï¼ŒDocSAMå¯ä»¥åœ¨å¼‚è´¨æ•°æ®é›†ä¸Šè¿›è¡Œè”åˆè®­ç»ƒï¼Œå¢å¼ºäº†ç¨³å¥æ€§å’Œæ³›åŒ–èƒ½åŠ›ï¼ŒåŒæ—¶å‡å°‘äº†è®¡ç®—å’Œå­˜å‚¨èµ„æºã€‚å…¨é¢è¯„ä¼°è¡¨æ˜ï¼ŒDocSAMåœ¨å‡†ç¡®æ€§ã€æ•ˆç‡å’Œé€‚åº”æ€§æ–¹é¢è¶…è¶Šäº†ç°æœ‰æ–¹æ³•ï¼Œçªæ˜¾äº†å…¶åœ¨æ¨è¿›å„ç§åº”ç”¨ä¸­çš„æ–‡æ¡£å›¾åƒç†è§£å’Œåˆ†å‰²çš„æ½œåŠ›ã€‚ä»£ç å¯ç”¨åœ¨<a target="_blank" rel="noopener" href="https://github.com/xhli-git/DocSAM%E3%80%82">https://github.com/xhli-git/DocSAMã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.04085v1">PDF</a> This paper has been accepted by CVPR 2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†DocSAMï¼Œè¿™æ˜¯ä¸€ä¸ªåŸºäºè½¬æ¢å™¨çš„ç»Ÿä¸€æ¡†æ¶ï¼Œç”¨äºå„ç§æ–‡æ¡£å›¾åƒåˆ†å‰²ä»»åŠ¡ï¼Œå¦‚æ–‡æ¡£å¸ƒå±€åˆ†æã€å¤šç²’åº¦æ–‡æœ¬åˆ†å‰²å’Œè¡¨æ ¼ç»“æ„è¯†åˆ«ã€‚å®ƒé€šè¿‡å®ä¾‹åˆ†å‰²å’Œè¯­ä¹‰åˆ†å‰²çš„ç»“åˆæ¥å»ºæ¨¡è¿™äº›ä»»åŠ¡ï¼Œå¯ä»¥è”åˆè®­ç»ƒåœ¨å¼‚è´¨æ•°æ®é›†ä¸Šï¼Œæé«˜ç¨³å¥æ€§å’Œæ³›åŒ–èƒ½åŠ›ï¼ŒåŒæ—¶å‡å°‘è®¡ç®—å’Œå­˜å‚¨èµ„æºã€‚DocSAMåœ¨å‡†ç¡®æ€§ã€æ•ˆç‡å’Œé€‚åº”æ€§æ–¹é¢è¶…è¶Šäº†ç°æœ‰æ–¹æ³•ï¼Œæœ‰æœ›æ¨åŠ¨æ–‡æ¡£å›¾åƒç†è§£å’Œåˆ†å‰²çš„å¹¿æ³›åº”ç”¨ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>DocSAMæ˜¯ä¸€ä¸ªç»Ÿä¸€æ¡†æ¶ï¼Œç”¨äºæ–‡æ¡£å›¾åƒåˆ†å‰²ä»»åŠ¡ï¼ŒåŒ…æ‹¬æ–‡æ¡£å¸ƒå±€åˆ†æã€å¤šç²’åº¦æ–‡æœ¬åˆ†å‰²å’Œè¡¨æ ¼ç»“æ„è¯†åˆ«ã€‚</li>
<li>DocSAMç»“åˆå®ä¾‹åˆ†å‰²å’Œè¯­ä¹‰åˆ†å‰²æ¥å»ºæ¨¡ä»»åŠ¡ã€‚</li>
<li>DocSAMä½¿ç”¨Sentence-BERTå°†ç±»åˆ«åç§°æ˜ å°„åˆ°è¯­ä¹‰æŸ¥è¯¢ï¼Œä¸å®ä¾‹æŸ¥è¯¢åŒ¹é…ç»´åº¦ã€‚</li>
<li>é€šè¿‡æ³¨æ„åŠ›æœºåˆ¶ï¼Œå®ä¾‹æŸ¥è¯¢å’Œè¯­ä¹‰æŸ¥è¯¢ç›¸äº’äº¤äº’ï¼Œå¹¶ä¸å›¾åƒç‰¹å¾è¿›è¡Œäº¤å‰æ³¨æ„åŠ›ï¼Œä»¥é¢„æµ‹å®ä¾‹å’Œè¯­ä¹‰åˆ†å‰²æ©è†œã€‚</li>
<li>DocSAMå¯ä»¥é€šè¿‡è®¡ç®—å®ä¾‹å’Œè¯­ä¹‰æŸ¥è¯¢çš„ç‚¹ç§¯æ¥é¢„æµ‹å®ä¾‹ç±»åˆ«ï¼Œå¹¶é€šè¿‡softmaxå½’ä¸€åŒ–åˆ†æ•°ã€‚</li>
<li>DocSAMå¯ä»¥åœ¨å¼‚è´¨æ•°æ®é›†ä¸Šè¿›è¡Œè”åˆè®­ç»ƒï¼Œå¢å¼ºç¨³å¥æ€§å’Œæ³›åŒ–èƒ½åŠ›ï¼ŒåŒæ—¶å‡å°‘è®¡ç®—å’Œå­˜å‚¨èµ„æºã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.04085">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-36b7e096efd41fb4b400e5ca2bd3dd81.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4b4e439db8f49f0ed287c47a17e2570c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-32b81f0aa33cee65a3b747c23cc9239c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c36568faa0bf45281838c4768f42f9c6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ad5d09b7d3c6fc11499cf8dc23c15c45.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="Performance-Analysis-of-Deep-Learning-Models-for-Femur-Segmentation-in-MRI-Scan"><a href="#Performance-Analysis-of-Deep-Learning-Models-for-Femur-Segmentation-in-MRI-Scan" class="headerlink" title="Performance Analysis of Deep Learning Models for Femur Segmentation in   MRI Scan"></a>Performance Analysis of Deep Learning Models for Femur Segmentation in   MRI Scan</h2><p><strong>Authors:Mengyuan Liu, Yixiao Chen, Anning Tian, Xinmeng Wu, Mozhi Shen, Tianchou Gong, Jeongkyu Lee</strong></p>
<p>Convolutional neural networks like U-Net excel in medical image segmentation, while attention mechanisms and KAN enhance feature extraction. Metaâ€™s SAM 2 uses Vision Transformers for prompt-based segmentation without fine-tuning. However, biases in these models impact generalization with limited data. In this study, we systematically evaluate and compare the performance of three CNN-based models, i.e., U-Net, Attention U-Net, and U-KAN, and one transformer-based model, i.e., SAM 2 for segmenting femur bone structures in MRI scan. The dataset comprises 11,164 MRI scans with detailed annotations of femoral regions. Performance is assessed using the Dice Similarity Coefficient, which ranges from 0.932 to 0.954. Attention U-Net achieves the highest overall scores, while U-KAN demonstrated superior performance in anatomical regions with a smaller region of interest, leveraging its enhanced learning capacity to improve segmentation accuracy. </p>
<blockquote>
<p>å·ç§¯ç¥ç»ç½‘ç»œï¼ˆå¦‚U-Netï¼‰åœ¨åŒ»å­¦å›¾åƒåˆ†å‰²æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œè€Œæ³¨æ„åŠ›æœºåˆ¶å’ŒKANåˆ™å¢å¼ºäº†ç‰¹å¾æå–çš„èƒ½åŠ›ã€‚Metaçš„SAM 2ä½¿ç”¨Vision Transformersè¿›è¡ŒåŸºäºæç¤ºçš„åˆ†å‰²ï¼Œæ— éœ€å¾®è°ƒã€‚ç„¶è€Œï¼Œè¿™äº›æ¨¡å‹ä¸­çš„åè§å½±å“åœ¨æœ‰é™æ•°æ®ä¸‹çš„æ³›åŒ–èƒ½åŠ›ã€‚åœ¨æœ¬ç ”ç©¶ä¸­ï¼Œæˆ‘ä»¬ç³»ç»Ÿåœ°è¯„ä¼°å’Œæ¯”è¾ƒäº†ä¸‰ç§åŸºäºCNNçš„æ¨¡å‹ï¼ˆå³U-Netã€Attention U-Netå’ŒU-KANï¼‰å’Œä¸€ç§åŸºäºå˜å‹å™¨çš„æ¨¡å‹ï¼ˆå³SAM 2ï¼‰åœ¨MRIæ‰«æä¸­åˆ†å‰²è‚¡éª¨ç»“æ„çš„è¡¨ç°ã€‚æ•°æ®é›†åŒ…å«11,164ä»½MRIæ‰«æï¼Œè¯¦ç»†æ ‡æ³¨äº†è‚¡éª¨åŒºåŸŸã€‚æ€§èƒ½è¯„ä¼°é‡‡ç”¨Diceç›¸ä¼¼ç³»æ•°ï¼ŒèŒƒå›´ä»0.932åˆ°0.954ã€‚Attention U-Netè·å¾—æœ€é«˜æ€»åˆ†ï¼Œè€ŒU-KANåœ¨å…·æœ‰è¾ƒå°å…´è¶£åŒºåŸŸçš„è§£å‰–åŒºåŸŸè¡¨ç°å‡ºå“è¶Šæ€§èƒ½ï¼Œåˆ©ç”¨å…¶å¢å¼ºçš„å­¦ä¹ èƒ½åŠ›æé«˜åˆ†å‰²ç²¾åº¦ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.04066v1">PDF</a> </p>
<p><strong>Summary</strong><br>     å·ç§¯ç¥ç»ç½‘ç»œå¦‚U-Netåœ¨åŒ»å­¦å›¾åƒåˆ†å‰²ä¸­è¡¨ç°å‡ºè‰²ï¼Œè€Œæ³¨æ„åŠ›æœºåˆ¶å’ŒKANå¯å¢å¼ºç‰¹å¾æå–åŠŸèƒ½ã€‚æœ¬ç ”ç©¶å¯¹ä¸‰ç§åŸºäºCNNçš„æ¨¡å‹ï¼ˆU-Netã€Attention U-Netå’ŒU-KANï¼‰å’Œä¸€ç§åŸºäºå˜å‹å™¨çš„æ¨¡å‹ï¼ˆMetaçš„SAM 2ï¼‰è¿›è¡Œäº†ç³»ç»Ÿè¯„ä»·æ¯”è¾ƒï¼Œç”¨äºMRIæ‰«æä¸­çš„è‚¡éª¨ç»“æ„åˆ†å‰²ã€‚æ€§èƒ½è¯„ä¼°ä½¿ç”¨Diceç›¸ä¼¼ç³»æ•°ï¼ŒèŒƒå›´ä»0.932åˆ°0.954ã€‚Attention U-Netè·å¾—æœ€é«˜æ€»ä½“å¾—åˆ†ï¼ŒU-KANåœ¨è¾ƒå°çš„æ„Ÿå…´è¶£åŒºåŸŸå±•ç°å‡ºå“è¶Šæ€§èƒ½ï¼Œåˆ©ç”¨å…¶å¢å¼ºå­¦ä¹ èƒ½åŠ›æé«˜åˆ†å‰²å‡†ç¡®æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNï¼‰å¦‚U-Netåœ¨åŒ»å­¦å›¾åƒåˆ†å‰²ä¸­è¡¨ç°ä¼˜ç§€ã€‚</li>
<li>æ³¨æ„åŠ›æœºåˆ¶å’ŒKANæœºåˆ¶èƒ½å¢å¼ºç‰¹å¾æå–èƒ½åŠ›ã€‚</li>
<li>Metaçš„SAM 2ä½¿ç”¨Vision Transformersè¿›è¡ŒåŸºäºæç¤ºçš„åˆ†å‰²ï¼Œæ— éœ€å¾®è°ƒã€‚</li>
<li>æ¨¡å‹çš„åè§ä¼šå½±å“åœ¨æœ‰é™æ•°æ®ä¸Šçš„æ³›åŒ–èƒ½åŠ›ã€‚</li>
<li>ç ”ç©¶ä¸­å¯¹å››ç§æ¨¡å‹è¿›è¡Œäº†ç³»ç»Ÿè¯„ä»·ï¼ŒåŒ…æ‹¬ä¸‰ç§CNNæ¨¡å‹ï¼ˆU-Netã€Attention U-Netå’ŒU-KANï¼‰å’Œä¸€ç§åŸºäºå˜å‹å™¨çš„æ¨¡å‹ï¼ˆSAM 2ï¼‰ã€‚</li>
<li>åœ¨MRIæ‰«æè‚¡éª¨ç»“æ„åˆ†å‰²çš„ä»»åŠ¡ä¸­ï¼ŒAttention U-Netè·å¾—æœ€é«˜æ€»ä½“å¾—åˆ†ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.04066">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-726ae4620e9644849b3d96a889cf98f8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0a3120eb21888c69e21d43958178705b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7700f89352ad6f91f0a1b1243a674ce7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-86a872ea9182ffd5189ea2ded1dde0d2.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="Decentralized-Personalization-for-Federated-Medical-Image-Segmentation-via-Gossip-Contrastive-Mutual-Learning"><a href="#Decentralized-Personalization-for-Federated-Medical-Image-Segmentation-via-Gossip-Contrastive-Mutual-Learning" class="headerlink" title="Decentralized Personalization for Federated Medical Image Segmentation   via Gossip Contrastive Mutual Learning"></a>Decentralized Personalization for Federated Medical Image Segmentation   via Gossip Contrastive Mutual Learning</h2><p><strong>Authors:Jingyun Chen, Yading Yuan</strong></p>
<p>Federated Learning (FL) presents a promising avenue for collaborative model training among medical centers, facilitating knowledge exchange without compromising data privacy. However, vanilla FL is prone to server failures and rarely achieves optimal performance on all participating sites due to heterogeneous data distributions among them. To overcome these challenges, we propose Gossip Contrastive Mutual Learning (GCML), a unified framework to optimize personalized models in a decentralized environment, where Gossip Protocol is employed for flexible and robust peer-to-peer communication. To make efficient and reliable knowledge exchange in each communication without the global knowledge across all the sites, we introduce deep contrast mutual learning (DCML), a simple yet effective scheme to encourage knowledge transfer between the incoming and local models through collaborative training on local data. By integrating DCML with other efforts to optimize site-specific models by leveraging useful information from peers, we evaluated the performance and efficiency of the proposed method on three publicly available datasets with different segmentation tasks. Our extensive experimental results show that the proposed GCML framework outperformed both centralized and decentralized FL methods with significantly reduced communication overhead, indicating its potential for real-world deployment. Upon the acceptance of manuscript, the code will be available at: <a target="_blank" rel="noopener" href="https://github.com/CUMC-Yuan-Lab/GCML">https://github.com/CUMC-Yuan-Lab/GCML</a>. </p>
<blockquote>
<p>è”é‚¦å­¦ä¹ ï¼ˆFLï¼‰ä¸ºåŒ»ç–—ä¸­å¿ƒä¹‹é—´çš„åä½œæ¨¡å‹è®­ç»ƒæä¾›äº†ä¸€ä¸ªæœ‰å‰æ™¯çš„é€”å¾„ï¼Œä¿ƒè¿›äº†çŸ¥è¯†äº¤æ¢è€Œä¸ä¾µçŠ¯æ•°æ®éšç§ã€‚ç„¶è€Œï¼ŒåŸç”Ÿçš„è”é‚¦å­¦ä¹ å®¹æ˜“å—åˆ°æœåŠ¡å™¨æ•…éšœçš„å½±å“ï¼Œå¹¶ä¸”ç”±äºå„å‚ä¸æ–¹ä¹‹é—´æ•°æ®åˆ†å¸ƒçš„å¼‚è´¨æ€§ï¼Œå¾ˆå°‘åœ¨æ‰€æœ‰å‚ä¸æ–¹ä¸Šå®ç°æœ€ä½³æ€§èƒ½ã€‚ä¸ºäº†å…‹æœè¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†Gossipå¯¹æ¯”äº’å­¦ä¹ ï¼ˆGCMLï¼‰æ¡†æ¶ï¼Œè¿™æ˜¯ä¸€ä¸ªåœ¨åˆ†å¸ƒå¼ç¯å¢ƒä¸­ä¼˜åŒ–ä¸ªæ€§åŒ–æ¨¡å‹çš„ç»Ÿä¸€æ¡†æ¶ï¼Œé‡‡ç”¨Gossipåè®®è¿›è¡Œçµæ´»å’Œç¨³å¥çš„ç‚¹å¯¹ç‚¹é€šä¿¡ã€‚ä¸ºäº†åœ¨æ¯æ¬¡é€šä¿¡ä¸­è¿›è¡Œé«˜æ•ˆå¯é çš„çŸ¥è¯†äº¤æ¢ï¼Œè€Œä¸æ¶‰åŠæ‰€æœ‰ç«™ç‚¹çš„å…¨å±€çŸ¥è¯†ï¼Œæˆ‘ä»¬å¼•å…¥äº†æ·±åº¦å¯¹æ¯”äº’å­¦ä¹ ï¼ˆDCMLï¼‰æ–¹æ¡ˆï¼Œè¿™æ˜¯ä¸€ç§ç®€å•æœ‰æ•ˆçš„æ–¹æ¡ˆï¼Œé€šè¿‡æœ¬åœ°æ•°æ®çš„åä½œè®­ç»ƒæ¥é¼“åŠ±ä¼ å…¥æ¨¡å‹å’Œæœ¬åœ°æ¨¡å‹ä¹‹é—´çš„çŸ¥è¯†è½¬ç§»ã€‚é€šè¿‡å°†DCMLä¸å…¶ä»–åŠªåŠ›ç›¸ç»“åˆï¼Œä»¥åˆ©ç”¨åŒè¡Œä¸­çš„æœ‰ç”¨ä¿¡æ¯æ¥ä¼˜åŒ–ç‰¹å®šç«™ç‚¹çš„æ¨¡å‹ï¼Œæˆ‘ä»¬åœ¨ä¸‰ä¸ªå…¬å¼€æ•°æ®é›†ä¸Šå¯¹æ‰€æå‡ºçš„æ–¹æ³•è¿›è¡Œäº†ä¸åŒåˆ†å‰²ä»»åŠ¡çš„æ€§èƒ½å’Œæ•ˆç‡è¯„ä¼°ã€‚æˆ‘ä»¬çš„å¹¿æ³›å®éªŒç»“æœè¡¨æ˜ï¼Œæ‰€æå‡ºçš„GCMLæ¡†æ¶åœ¨é€šä¿¡å¼€é”€æ–¹é¢æ˜¾è‘—ä¼˜äºé›†ä¸­å¼å’Œåˆ†å¸ƒå¼è”é‚¦å­¦ä¹ æ–¹æ³•ï¼Œæ˜¾ç¤ºå‡ºå…¶åœ¨ç°å®ä¸–ç•Œéƒ¨ç½²çš„æ½œåŠ›ã€‚æ‰‹ç¨¿ä¸€æ—¦è¢«æ¥å—ï¼Œä»£ç å°†å¯é€šè¿‡ä»¥ä¸‹é“¾æ¥è·å–ï¼š<a target="_blank" rel="noopener" href="https://github.com/CUMC-Yuan-Lab/GCML">https://github.com/CUMC-Yuan-Lab/GCML</a>ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.03883v2">PDF</a> Accepted by IEEE Transactions on Medical Imaging, Open-source code   at: <a target="_blank" rel="noopener" href="https://github.com/CUMC-Yuan-Lab/GCML">https://github.com/CUMC-Yuan-Lab/GCML</a></p>
<p><strong>Summary</strong><br>    è”é‚¦å­¦ä¹ ï¼ˆFLï¼‰ä¸ºåŒ»ç–—ä¸­å¿ƒé—´çš„åä½œæ¨¡å‹è®­ç»ƒæä¾›äº†æœ‰å‰æ™¯çš„é€”å¾„ï¼Œä¿ƒè¿›çŸ¥è¯†äº¤æ¢è€Œä¸æ³„éœ²æ•°æ®éšç§ã€‚ç„¶è€Œï¼ŒåŸå§‹FLæ˜“å—æœåŠ¡å™¨æ•…éšœå½±å“ï¼Œä¸”ç”±äºæ•°æ®åˆ†å¸ƒä¸å‡å¾ˆéš¾åœ¨æ‰€æœ‰å‚ä¸ç«™ç‚¹ä¸Šå®ç°æœ€ä½³æ€§èƒ½ã€‚ä¸ºå…‹æœè¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºGossipå¯¹æ¯”äº’å­¦ä¹ ï¼ˆGCMLï¼‰ç»Ÿä¸€æ¡†æ¶ï¼Œç”¨äºåœ¨åˆ†å¸ƒå¼ç¯å¢ƒä¸­ä¼˜åŒ–ä¸ªæ€§åŒ–æ¨¡å‹ã€‚é‡‡ç”¨Gossipåè®®å®ç°çµæ´»ç¨³å¥çš„ç‚¹å¯¹ç‚¹é€šä¿¡ã€‚é€šè¿‡å¼•å…¥æ·±åº¦å¯¹æ¯”äº’å­¦ä¹ ï¼ˆDCMLï¼‰ï¼Œåœ¨æ¯æ¬¡é€šä¿¡ä¸­è¿›è¡Œé«˜æ•ˆå¯é çš„çŸ¥è¯†äº¤æ¢ï¼Œé¼“åŠ±æœ¬åœ°ä¸ä¼ å…¥æ¨¡å‹é—´çš„çŸ¥è¯†è½¬ç§»ï¼Œé€šè¿‡æœ¬åœ°æ•°æ®åä½œè®­ç»ƒã€‚å°†DCMLä¸å…¶ä»–ä¼˜åŒ–åŒè¡Œç‰¹å®šæ¨¡å‹çš„ç­–ç•¥ç›¸ç»“åˆï¼Œæˆ‘ä»¬åœ¨ä¸‰ä¸ªå…¬å¼€æ•°æ®é›†ä¸Šè¿›è¡Œä¸åŒåˆ†å‰²ä»»åŠ¡çš„è¯„ä¼°ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒGCMLæ¡†æ¶åœ¨é€šä¿¡å¼€é”€æ˜¾è‘—é™ä½çš„æƒ…å†µä¸‹ï¼Œè¡¨ç°ä¼˜äºé›†ä¸­å¼å’Œåˆ†å¸ƒå¼è”é‚¦å­¦ä¹ æ–¹æ³•ï¼Œå…·æœ‰ç°å®éƒ¨ç½²æ½œåŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è”é‚¦å­¦ä¹ ï¼ˆFLï¼‰æœ‰åŠ©äºåŒ»ç–—ä¸­å¿ƒé—´çš„æ¨¡å‹åä½œè®­ç»ƒï¼Œä¿ƒè¿›çŸ¥è¯†äº¤æ¢è€Œä¸æ³„éœ²æ•°æ®éšç§ã€‚</li>
<li>åŸå§‹è”é‚¦å­¦ä¹ å­˜åœ¨æœåŠ¡å™¨æ•…éšœå’Œæ€§èƒ½ä¼˜åŒ–é—®é¢˜ï¼Œç‰¹åˆ«æ˜¯åœ¨æ•°æ®åˆ†å¸ƒä¸å‡çš„ç¯å¢ƒä¸‹ã€‚</li>
<li>æå‡ºGossipå¯¹æ¯”äº’å­¦ä¹ ï¼ˆGCMLï¼‰æ¡†æ¶ï¼Œä»¥ä¼˜åŒ–åˆ†å¸ƒå¼ç¯å¢ƒä¸­çš„ä¸ªæ€§åŒ–æ¨¡å‹ã€‚</li>
<li>é‡‡ç”¨Gossipåè®®å®ç°çµæ´»ç¨³å¥çš„ç‚¹å¯¹ç‚¹é€šä¿¡ï¼Œæé«˜çŸ¥è¯†äº¤æ¢æ•ˆç‡ã€‚</li>
<li>å¼•å…¥æ·±åº¦å¯¹æ¯”äº’å­¦ä¹ ï¼ˆDCMLï¼‰ï¼Œé¼“åŠ±æœ¬åœ°ä¸ä¼ å…¥æ¨¡å‹é—´çš„çŸ¥è¯†è½¬ç§»ã€‚</li>
<li>GCMLæ¡†æ¶é€šè¿‡ç»“åˆå¤šç§ç­–ç•¥ï¼Œåœ¨å¤šä¸ªå…¬å¼€æ•°æ®é›†ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œä¼˜äºä¼ ç»Ÿè”é‚¦å­¦ä¹ æ–¹æ³•ã€‚</li>
<li>GCMLæ¡†æ¶é™ä½äº†é€šä¿¡å¼€é”€ï¼Œå…·æœ‰ç°å®éƒ¨ç½²çš„æ½œåŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.03883">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-06534c82a9b619b33dd9b9bfd33332b4.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-075b6ae62f61fda912b429f4a6c5a580.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-91c855727cd2903863a1532c9958e839.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="INRetouch-Context-Aware-Implicit-Neural-Representation-for-Photography-Retouching"><a href="#INRetouch-Context-Aware-Implicit-Neural-Representation-for-Photography-Retouching" class="headerlink" title="INRetouch: Context Aware Implicit Neural Representation for Photography   Retouching"></a>INRetouch: Context Aware Implicit Neural Representation for Photography   Retouching</h2><p><strong>Authors:Omar Elezabi, Marcos V. Conde, Zongwei Wu, Radu Timofte</strong></p>
<p>Professional photo editing remains challenging, requiring extensive knowledge of imaging pipelines and significant expertise. While recent deep learning approaches, particularly style transfer methods, have attempted to automate this process, they often struggle with output fidelity, editing control, and complex retouching capabilities. We propose a novel retouch transfer approach that learns from professional edits through before-after image pairs, enabling precise replication of complex editing operations. We develop a context-aware Implicit Neural Representation that learns to apply edits adaptively based on image content and context, and is capable of learning from a single example. Our method extracts implicit transformations from reference edits and adaptively applies them to new images. To facilitate this research direction, we introduce a comprehensive Photo Retouching Dataset comprising 100,000 high-quality images edited using over 170 professional Adobe Lightroom presets. Through extensive evaluation, we demonstrate that our approach not only surpasses existing methods in photo retouching but also enhances performance in related image reconstruction tasks like Gamut Mapping and Raw Reconstruction. By bridging the gap between professional editing capabilities and automated solutions, our work presents a significant step toward making sophisticated photo editing more accessible while maintaining high-fidelity results. Check the Project Page at <a target="_blank" rel="noopener" href="https://omaralezaby.github.io/inretouch">https://omaralezaby.github.io/inretouch</a> for more Results and information about Code and Dataset availability. </p>
<blockquote>
<p>ä¸“ä¸šç…§ç‰‡ç¼–è¾‘ä»ç„¶æ˜¯ä¸€ä¸ªæŒ‘æˆ˜ï¼Œéœ€è¦æ·±å…¥äº†è§£æˆåƒç®¡é“å’Œå¤§é‡çš„ä¸“ä¸šçŸ¥è¯†ã€‚å°½ç®¡æœ€è¿‘çš„æ·±åº¦å­¦ä¹ æŠ€æœ¯ï¼Œå°¤å…¶æ˜¯é£æ ¼è½¬æ¢æ–¹æ³•ï¼Œå·²ç»å°è¯•è‡ªåŠ¨å®Œæˆæ­¤è¿‡ç¨‹ï¼Œä½†å®ƒä»¬åœ¨è¾“å‡ºä¿çœŸåº¦ã€ç¼–è¾‘æ§åˆ¶å’Œå¤æ‚æ¶¦é¥°åŠŸèƒ½æ–¹é¢å¾€å¾€é¢ä¸´å›°éš¾ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°é¢–çš„æ¶¦é¥°è½¬æ¢æ–¹æ³•ï¼Œå®ƒé€šè¿‡ä¸“ä¸šç¼–è¾‘å‰åçš„å›¾åƒå¯¹è¿›è¡Œå­¦ä¹ ï¼Œèƒ½å¤Ÿç²¾ç¡®å¤åˆ¶å¤æ‚çš„ç¼–è¾‘æ“ä½œã€‚æˆ‘ä»¬å¼€å‘äº†ä¸€ç§ä¸Šä¸‹æ–‡æ„ŸçŸ¥çš„éšå¼ç¥ç»è¡¨ç¤ºï¼Œå®ƒå¯ä»¥æ ¹æ®å›¾åƒå†…å®¹å’Œä¸Šä¸‹æ–‡å­¦ä¹ è‡ªé€‚åº”åº”ç”¨ç¼–è¾‘ï¼Œå¹¶ä¸”èƒ½å¤Ÿä»å•ä¸ªç¤ºä¾‹ä¸­å­¦ä¹ ã€‚æˆ‘ä»¬çš„æ–¹æ³•ä»å‚è€ƒç¼–è¾‘ä¸­æå–éšå¼è½¬æ¢ï¼Œå¹¶è‡ªé€‚åº”åœ°åº”ç”¨äºæ–°å›¾åƒã€‚ä¸ºäº†ä¿ƒè¿›è¿™ä¸€ç ”ç©¶æ–¹å‘ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ªå…¨é¢çš„ç…§ç‰‡æ¶¦é¥°æ•°æ®é›†ï¼ŒåŒ…å«ä½¿ç”¨è¶…è¿‡170ç§ä¸“ä¸šAdobe Lightroomé¢„è®¾ç¼–è¾‘çš„10ä¸‡å¼ é«˜è´¨é‡å›¾åƒã€‚é€šè¿‡å¹¿æ³›è¯„ä¼°ï¼Œæˆ‘ä»¬è¯æ˜æˆ‘ä»¬çš„æ–¹æ³•ä¸ä»…åœ¨ç…§ç‰‡æ¶¦é¥°æ–¹é¢è¶…è¶Šäº†ç°æœ‰æ–¹æ³•ï¼Œè€Œä¸”åœ¨ç›¸å…³å›¾åƒé‡å»ºä»»åŠ¡ï¼ˆå¦‚è‰²åŸŸæ˜ å°„å’ŒåŸå§‹é‡å»ºï¼‰æ–¹é¢ä¹Ÿæé«˜äº†æ€§èƒ½ã€‚é€šè¿‡ç¼©å°ä¸“ä¸šç¼–è¾‘èƒ½åŠ›å’Œè‡ªåŠ¨åŒ–è§£å†³æ–¹æ¡ˆä¹‹é—´çš„å·®è·ï¼Œæˆ‘ä»¬çš„å·¥ä½œåœ¨ä½¿å¤æ‚ç…§ç‰‡ç¼–è¾‘æ›´åŠ æ˜“äºè®¿é—®çš„åŒæ—¶ä¿æŒé«˜ä¿çœŸç»“æœæ–¹é¢è¿ˆå‡ºäº†é‡è¦ä¸€æ­¥ã€‚æœ‰å…³æ›´å¤šç»“æœå’Œå…³äºä»£ç å’Œæ•°æ®é›†å¯ç”¨æ€§çš„ä¿¡æ¯ï¼Œè¯·è®¿é—®é¡¹ç›®é¡µé¢ï¼š<a target="_blank" rel="noopener" href="https://omaralezaby.github.io/inretouch">ç½‘ç«™é“¾æ¥</a>ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.03848v3">PDF</a> </p>
<p><strong>Summary</strong><br>     è¯¥ç ”ç©¶æå‡ºä¸€ç§æ–°é¢–çš„ç…§ç‰‡ä¿®é¥°è½¬æ¢æ–¹æ³•ï¼Œé€šè¿‡ä¸“ä¸šä¿®é¥°å‰åçš„å›¾åƒå¯¹è¿›è¡Œå­¦ä¹ ï¼Œèƒ½å¤Ÿç²¾ç¡®å¤åˆ¶å¤æ‚çš„ç¼–è¾‘æ“ä½œã€‚å¼€å‘äº†ä¸€ç§åŸºäºä¸Šä¸‹æ–‡å†…å®¹çš„éšå¼ç¥ç»è¡¨ç¤ºæ–¹æ³•ï¼Œèƒ½æ ¹æ®å›¾åƒå†…å®¹å’Œä¸Šä¸‹æ–‡è‡ªé€‚åº”åœ°åº”ç”¨ç¼–è¾‘ï¼Œå¹¶ä»å•ä¸ªç¤ºä¾‹ä¸­å­¦ä¹ ã€‚ä¸ºæ­¤ç ”ç©¶æ–¹å‘å¼•å…¥åŒ…å«10ä¸‡å¼ é«˜è´¨é‡å›¾ç‰‡çš„ç»¼åˆç…§ç‰‡ä¿®é¥°æ•°æ®é›†ï¼Œæ¶µç›–ä½¿ç”¨è¶…è¿‡170ç§ä¸“ä¸šAdobe Lightroomé¢„è®¾çš„ç¼–è¾‘ã€‚ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•ä¸ä»…åœ¨ç…§ç‰‡ä¿®é¥°æ–¹é¢è¶…è¶Šç°æœ‰æ–¹æ³•ï¼Œè¿˜åœ¨ç›¸å…³å›¾åƒé‡å»ºä»»åŠ¡å¦‚è‰²åŸŸæ˜ å°„å’ŒåŸå§‹é‡å»ºä¸­æé«˜æ€§èƒ½ã€‚è¯¥ç ”ç©¶åœ¨ç¼©çŸ­ä¸“ä¸šç¼–è¾‘ä¸è‡ªåŠ¨åŒ–è§£å†³æ–¹æ¡ˆä¹‹é—´çš„å·®è·æ–¹é¢è¿ˆå‡ºé‡è¦ä¸€æ­¥ï¼Œä½¿é«˜çº§ç…§ç‰‡ç¼–è¾‘æ›´åŠ æ˜“äºå®ç°å¹¶ä¿æŒé«˜è´¨é‡ç»“æœã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è¯¥ç ”ç©¶è§£å†³äº†ä¸“ä¸šç…§ç‰‡ç¼–è¾‘çš„æŒ‘æˆ˜ï¼ŒåŒ…æ‹¬æˆåƒç®¡é“çŸ¥è¯†å’Œä¸“ä¸šæŠ€èƒ½çš„å¹¿æ³›éœ€æ±‚ã€‚</li>
<li>é€šè¿‡å¯¹ä¸“ä¸šä¿®é¥°å‰åçš„å›¾åƒå¯¹è¿›è¡Œå­¦ä¹ ï¼Œèƒ½å¤Ÿç²¾ç¡®å¤åˆ¶å¤æ‚çš„ç¼–è¾‘æ“ä½œã€‚</li>
<li>å¼€å‘äº†ä¸€ç§åŸºäºä¸Šä¸‹æ–‡å†…å®¹çš„éšå¼ç¥ç»è¡¨ç¤ºæ–¹æ³•ï¼Œè‡ªé€‚åº”åœ°åº”ç”¨ç¼–è¾‘ã€‚</li>
<li>æ–¹æ³•èƒ½å¤Ÿä»å•ä¸ªç¤ºä¾‹ä¸­å­¦ä¹ å¹¶æå–éšå¼è½¬æ¢ã€‚</li>
<li>ç ”ç©¶å¼•å…¥äº†åŒ…å«å¤§é‡é«˜è´¨é‡å›¾ç‰‡çš„ç»¼åˆç…§ç‰‡ä¿®é¥°æ•°æ®é›†ã€‚</li>
<li>ç»“æœæ˜¾ç¤ºï¼Œè¯¥æ–¹æ³•åœ¨ç…§ç‰‡ä¿®é¥°å’Œå›¾åƒé‡å»ºä»»åŠ¡ä¸Šè¶…è¶Šäº†ç°æœ‰æ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.03848">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-0d3898e6123d57f4e7e4f25f59924b93.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-bd682ec343c13fb94f6a12deb212c148.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6f3fd067d2c458c64e5ce65fce70a7b0.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-2116a6d68048e66c2418cf9435003eea.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-29c2a2d8dcfe9a0d8fa0d0cc3bd75a50.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="A-Large-Model-for-Non-invasive-and-Personalized-Management-of-Breast-Cancer-from-Multiparametric-MRI"><a href="#A-Large-Model-for-Non-invasive-and-Personalized-Management-of-Breast-Cancer-from-Multiparametric-MRI" class="headerlink" title="A Large Model for Non-invasive and Personalized Management of Breast   Cancer from Multiparametric MRI"></a>A Large Model for Non-invasive and Personalized Management of Breast   Cancer from Multiparametric MRI</h2><p><strong>Authors:Luyang Luo, Mingxiang Wu, Mei Li, Yi Xin, Qiong Wang, Varut Vardhanabhuti, Winnie CW Chu, Zhenhui Li, Juan Zhou, Pranav Rajpurkar, Hao Chen</strong></p>
<p>Breast Magnetic Resonance Imaging (MRI) demonstrates the highest sensitivity for breast cancer detection among imaging modalities and is standard practice for high-risk women. Interpreting the multi-sequence MRI is time-consuming and prone to subjective variation. We develop a large mixture-of-modality-experts model (MOME) that integrates multiparametric MRI information within a unified structure, leveraging breast MRI scans from 5,205 female patients in China for model development and validation. MOME matches four senior radiologistsâ€™ performance in identifying breast cancer and outperforms a junior radiologist. The model is able to reduce unnecessary biopsies in Breast Imaging-Reporting and Data System (BI-RADS) 4 patients, classify triple-negative breast cancer, and predict pathological complete response to neoadjuvant chemotherapy. MOME further supports inference with missing modalities and provides decision explanations by highlighting lesions and measuring modality contributions. To summarize, MOME exemplifies an accurate and robust multimodal model for noninvasive, personalized management of breast cancer patients via multiparametric MRI. Code is available at <a target="_blank" rel="noopener" href="https://github.com/LLYXC/MOME/tree/main">https://github.com/LLYXC/MOME/tree/main</a>. </p>
<blockquote>
<p>ä¹³è…ºç£å…±æŒ¯æˆåƒï¼ˆMRIï¼‰åœ¨æˆåƒæ¨¡å¼ä¸­å¯¹ä¹³è…ºç™Œæ£€æµ‹å…·æœ‰æœ€é«˜çš„çµæ•åº¦ï¼Œæ˜¯é«˜é£é™©å¥³æ€§çš„æ ‡å‡†å®è·µã€‚è§£é‡Šå¤šåºåˆ—MRIè€—æ—¶ä¸”å®¹æ˜“å‡ºç°ä¸»è§‚å·®å¼‚ã€‚æˆ‘ä»¬å¼€å‘äº†ä¸€ç§å¤§å‹æ··åˆæ¨¡å¼ä¸“å®¶æ¨¡å‹ï¼ˆMOMEï¼‰ï¼Œè¯¥æ¨¡å‹åœ¨ç»Ÿä¸€ç»“æ„ä¸­æ•´åˆå¤šå‚æ•°MRIä¿¡æ¯ï¼Œåˆ©ç”¨ä¸­å›½5205åå¥³æ€§æ‚£è€…çš„ä¹³è…ºMRIæ‰«æè¿›è¡Œæ¨¡å‹å¼€å‘å’ŒéªŒè¯ã€‚MOMEçš„ä¹³è…ºç™Œè¯†åˆ«æ€§èƒ½ä¸å››åèµ„æ·±æ”¾å°„ç§‘åŒ»ç”Ÿç›¸åŒ¹é…ï¼Œå¹¶ä¼˜äºåˆçº§æ”¾å°„ç§‘åŒ»ç”Ÿã€‚è¯¥æ¨¡å‹èƒ½å¤Ÿå‡å°‘ä¹³è…ºå½±åƒæŠ¥å‘Šå’Œæ•°æ®ç³»ç»Ÿï¼ˆBI-RADSï¼‰4çº§æ‚£è€…çš„ä¸å¿…è¦æ´»æ£€ï¼Œå¯¹ä¸‰é˜´æ€§ä¹³è…ºç™Œè¿›è¡Œåˆ†ç±»ï¼Œå¹¶é¢„æµ‹æ–°è¾…åŠ©åŒ–ç–—çš„ç—…ç†å®Œå…¨ååº”ã€‚MOMEè¿˜æ”¯æŒæ¨æ–­ç¼ºå¤±çš„æ¨¡å¼ï¼Œå¹¶é€šè¿‡çªå‡ºç—…å˜å’Œæµ‹é‡æ¨¡å¼è´¡çŒ®æ¥æä¾›å†³ç­–è§£é‡Šã€‚æ€»ä¹‹ï¼ŒMOMEé€šè¿‡å¤šå‚æ•°MRIå±•ç¤ºäº†å‡†ç¡®ä¸”ç¨³å¥çš„å¤šæ¨¡å¼æ¨¡å‹ï¼Œå¯ç”¨äºéä¾µå…¥å¼ã€ä¸ªæ€§åŒ–çš„ä¹³è…ºç™Œæ‚£è€…ç®¡ç†ã€‚ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/LLYXC/MOME/tree/main%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/LLYXC/MOME/tree/mainè·å–ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2408.12606v3">PDF</a> Nature Communications 2025</p>
<p><strong>Summary</strong><br>     å‘å±•äº†ä¸€ç§å¤§å‹æ··åˆæ¨¡æ€ä¸“å®¶æ¨¡å‹ï¼ˆMOMEï¼‰ï¼Œæ•´åˆå¤šå‚æ•°MRIä¿¡æ¯äºç»Ÿä¸€ç»“æ„å†…ï¼Œç”¨äºä¹³è…ºç™Œæ£€æµ‹ã€‚è¯¥æ¨¡å‹æ€§èƒ½ä¸å››ä½èµ„æ·±æ”¾å°„ç§‘åŒ»ç”Ÿç›¸åŒ¹é…ï¼Œåœ¨è¯†åˆ«ä¹³è…ºç™Œæ–¹é¢ä¼˜äºåˆçº§æ”¾å°„ç§‘åŒ»ç”Ÿã€‚MOMEèƒ½å‡å°‘ä¸å¿…è¦çš„æ´»æ£€ï¼Œåˆ†ç±»ä¸‰é˜´æ€§ä¹³è…ºç™Œï¼Œé¢„æµ‹æ–°è¾…åŠ©åŒ–ç–—çš„ç—…ç†å®Œå…¨ååº”ã€‚è¯¥æ¨¡å‹æ”¯æŒæ¨ç†ç¼ºå¤±æ¨¡æ€å¹¶æä¾›å†³ç­–è§£é‡Šï¼Œé€šè¿‡çªå‡ºç—…å˜å’Œæµ‹é‡æ¨¡æ€è´¡çŒ®æ¥æ”¯æŒã€‚æ€»ä¹‹ï¼ŒMOMEæ˜¯ä¸€ä¸ªå‡†ç¡®ã€ç¨³å¥çš„å¤šæ¨¡æ€æ¨¡å‹ï¼Œå¯é€šè¿‡å¤šå‚æ•°MRIå®ç°ä¹³è…ºç™Œæ‚£è€…çš„æ— åˆ›ã€ä¸ªæ€§åŒ–ç®¡ç†ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>MOMEæ¨¡å‹åˆ©ç”¨å¤šå‚æ•°MRIä¿¡æ¯ï¼Œå±•ç¤ºäº†åœ¨ä¹³è…ºç™Œæ£€æµ‹æ–¹é¢çš„é«˜æ•æ„Ÿæ€§ã€‚</li>
<li>æ¨¡å‹å¼€å‘ä½¿ç”¨äº†æ¥è‡ªä¸­å›½5,205åå¥³æ€§çš„ä¹³è…ºMRIæ‰«ææ•°æ®ã€‚</li>
<li>MOMEä¸èµ„æ·±æ”¾å°„å­¦å®¶åœ¨è¯†åˆ«ä¹³è…ºç™Œæ–¹é¢çš„æ€§èƒ½ç›¸åŒ¹é…ï¼Œå¹¶ä¼˜äºåˆçº§æ”¾å°„å­¦å®¶ã€‚</li>
<li>MOMEèƒ½å¤Ÿå‡å°‘ä¸å¿…è¦çš„æ´»æ£€ï¼Œç‰¹åˆ«æ˜¯åœ¨BI-RADS 4æ‚£è€…ä¸­ã€‚</li>
<li>è¯¥æ¨¡å‹èƒ½å¤Ÿåˆ†ç±»ä¸‰é˜´æ€§ä¹³è…ºç™Œï¼Œå¹¶é¢„æµ‹æ–°è¾…åŠ©åŒ–ç–—çš„ç—…ç†å®Œå…¨ååº”ã€‚</li>
<li>MOMEæ”¯æŒåœ¨ç¼ºå¤±æ¨¡æ€çš„æƒ…å†µä¸‹è¿›è¡Œæ¨ç†ï¼Œå¹¶æä¾›å†³ç­–è§£é‡Šã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2408.12606">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-37e30f0294ff23752258c6f513ad6291.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7b9da00395a5bcd16119f27045387639.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-174a88664f82d951898e1c3b739fff48.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d9964f7ed391a1d68c160081be409351.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="CFPFormer-Feature-pyramid-like-Transformer-Decoder-for-Segmentation-and-Detection"><a href="#CFPFormer-Feature-pyramid-like-Transformer-Decoder-for-Segmentation-and-Detection" class="headerlink" title="CFPFormer: Feature-pyramid like Transformer Decoder for Segmentation and   Detection"></a>CFPFormer: Feature-pyramid like Transformer Decoder for Segmentation and   Detection</h2><p><strong>Authors:Hongyi Cai, Mohammad Mahdinur Rahman, Wenzhen Dong, Jingyu Wu</strong></p>
<p>Feature pyramids have been widely adopted in convolutional neural networks and transformers for tasks in medical image segmentation. However, existing models generally focus on the Encoder-side Transformer for feature extraction. We further explore the potential in improving the feature decoder with a well-designed architecture. We propose Cross Feature Pyramid Transformer decoder (CFPFormer), a novel decoder block that integrates feature pyramids and transformers. Even though transformer-like architecture impress with outstanding performance in segmentation, the concerns to reduce the redundancy and training costs still exist. Specifically, by leveraging patch embedding, cross-layer feature concatenation mechanisms, CFPFormer enhances feature extraction capabilities while complexity issue is mitigated by our Gaussian Attention. Benefiting from Transformer structure and U-shaped connections, our work is capable of capturing long-range dependencies and effectively up-sample feature maps. Experimental results are provided to evaluate CFPFormer on medical image segmentation datasets, demonstrating the efficacy and effectiveness. With a ResNet50 backbone, our method achieves 92.02% Dice Score, highlighting the efficacy of our methods. Notably, our VGG-based model outperformed baselines with more complex ViT and Swin Transformer backbone. </p>
<blockquote>
<p>ç‰¹å¾é‡‘å­—å¡”å·²å¹¿æ³›åº”ç”¨äºå·ç§¯ç¥ç»ç½‘ç»œå’Œå˜å‹å™¨ä¸­ï¼Œç”¨äºåŒ»å­¦å›¾åƒåˆ†å‰²ä»»åŠ¡ã€‚ç„¶è€Œï¼Œç°æœ‰æ¨¡å‹é€šå¸¸ä¸“æ³¨äºç‰¹å¾æå–çš„ç¼–ç å™¨ä¾§å˜å‹å™¨ã€‚æˆ‘ä»¬è¿›ä¸€æ­¥æ¢ç´¢äº†é€šè¿‡ç²¾å¿ƒè®¾è®¡çš„æ¶æ„æ”¹è¿›ç‰¹å¾è§£ç å™¨çš„æ½œåŠ›ã€‚æˆ‘ä»¬æå‡ºäº†è·¨ç‰¹å¾é‡‘å­—å¡”å˜å‹å™¨è§£ç å™¨ï¼ˆCFPFormerï¼‰ï¼Œè¿™æ˜¯ä¸€ç§æ–°çš„è§£ç å™¨å—ï¼Œå®ƒé›†æˆäº†ç‰¹å¾é‡‘å­—å¡”å’Œå˜å‹å™¨ã€‚å°½ç®¡å˜å‹å™¨å¼çš„æ¶æ„åœ¨åˆ†å‰²æ–¹é¢è¡¨ç°å‡ºå“è¶Šçš„æ€§èƒ½ï¼Œä½†å‡å°‘å†—ä½™å’Œè®­ç»ƒæˆæœ¬çš„æ‹…å¿§ä»ç„¶å­˜åœ¨ã€‚å…·ä½“æ¥è¯´ï¼Œé€šè¿‡åˆ©ç”¨è¡¥ä¸åµŒå…¥ã€è·¨å±‚ç‰¹å¾æ‹¼æ¥æœºåˆ¶ï¼ŒCFPFormerå¢å¼ºäº†ç‰¹å¾æå–èƒ½åŠ›ï¼Œè€Œå¤æ‚æ€§é—®é¢˜åˆ™é€šè¿‡æˆ‘ä»¬çš„é«˜æ–¯æ³¨æ„åŠ›å¾—åˆ°äº†ç¼“è§£ã€‚å¾—ç›Šäºå˜å‹å™¨ç»“æ„å’ŒUå½¢è¿æ¥ï¼Œæˆ‘ä»¬çš„å·¥ä½œèƒ½å¤Ÿæ•æ‰é•¿è·ç¦»ä¾èµ–å…³ç³»å¹¶æœ‰æ•ˆåœ°ä¸Šé‡‡æ ·ç‰¹å¾å›¾ã€‚å®éªŒç»“æœä¸ºCFPFormeråœ¨åŒ»å­¦å›¾åƒåˆ†å‰²æ•°æ®é›†ä¸Šçš„è¯„ä¼°æä¾›äº†ä¾æ®ï¼Œè¯æ˜äº†å…¶æœ‰æ•ˆæ€§å’Œæœ‰æ•ˆæ€§ã€‚ä½¿ç”¨ResNet50éª¨å¹²ç½‘ï¼Œæˆ‘ä»¬çš„æ–¹æ³•å®ç°äº†92.02%çš„Diceå¾—åˆ†ï¼Œå‡¸æ˜¾äº†æˆ‘ä»¬æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œæˆ‘ä»¬åŸºäºVGGçš„æ¨¡å‹è¶…è¶Šäº†å…·æœ‰æ›´å¤æ‚ViTå’ŒSwin Transformeréª¨å¹²ç½‘çš„åŸºçº¿ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2404.15451v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ¢ç´¢äº†åœ¨åŒ»å­¦å›¾åƒåˆ†å‰²ä»»åŠ¡ä¸­ï¼Œåˆ©ç”¨ç‰¹å¾é‡‘å­—å¡”å’Œå˜å‹å™¨ç»“æ„æ”¹è¿›ç‰¹å¾è§£ç å™¨çš„æ½œåŠ›ã€‚æå‡ºäº†ä¸€ç§æ–°å‹çš„è§£ç å™¨å—â€”â€”Cross Feature Pyramid Transformerï¼ˆCFPFormerï¼‰ï¼Œè¯¥è§£ç å™¨ç»“åˆäº†ç‰¹å¾é‡‘å­—å¡”å’Œå˜å‹å™¨çš„ä¼˜ç‚¹ï¼Œèƒ½æœ‰æ•ˆæå‡ç‰¹å¾æå–èƒ½åŠ›ï¼Œå¹¶é™ä½å†—ä½™å’Œè®­ç»ƒæˆæœ¬ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒCFPFormeråœ¨åŒ»å­¦å›¾åƒåˆ†å‰²æ•°æ®é›†ä¸Šè¡¨ç°å‡ºä¼˜å¼‚çš„æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æœ¬æ–‡æå‡ºäº†Cross Feature Pyramid Transformerï¼ˆCFPFormerï¼‰è§£ç å™¨ï¼Œè¿™æ˜¯ä¸€ç§æ–°å‹çš„è§£ç å™¨å—ï¼Œç»“åˆäº†ç‰¹å¾é‡‘å­—å¡”å’Œå˜å‹å™¨çš„ä¼˜ç‚¹ã€‚</li>
<li>CFPFormeré€šè¿‡åˆ©ç”¨è¡¥ä¸åµŒå…¥å’Œè·¨å±‚ç‰¹å¾æ‹¼æ¥æœºåˆ¶ï¼Œå¢å¼ºäº†ç‰¹å¾æå–èƒ½åŠ›ã€‚</li>
<li>ä¸ç°æœ‰çš„æ¨¡å‹ç›¸æ¯”ï¼ŒCFPFormeré™ä½äº†æ¨¡å‹çš„å†—ä½™å’Œè®­ç»ƒæˆæœ¬ã€‚</li>
<li>åˆ©ç”¨é«˜æ–¯æ³¨æ„åŠ›æœºåˆ¶ï¼ŒCFPFormerç¼“è§£äº†å¤æ‚æ€§é—®é¢˜ã€‚</li>
<li>CFPFormerèƒ½å¤Ÿæ•æ‰é•¿è·ç¦»ä¾èµ–å…³ç³»å¹¶æœ‰æ•ˆåœ°ä¸Šé‡‡æ ·ç‰¹å¾å›¾ã€‚</li>
<li>å®éªŒç»“æœè¡¨æ˜ï¼ŒCFPFormeråœ¨åŒ»å­¦å›¾åƒåˆ†å‰²ä»»åŠ¡ä¸Šå…·æœ‰è‰¯å¥½çš„æ€§èƒ½å’Œæ•ˆæœã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2404.15451">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-5773c7a05e0a6ee5893e1685bcfd7008.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7cf52c207b1d182127384714c7305f7a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-40247c64d0eafcc5ed8185ac508d35dd.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8eedcae206f7fd25bb9c5321f2292d92.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2fb1cb856537fb0eb4412f8416c58583.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-86192fe31cfca47a1847e2914aec69a2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5f5e61aa088f876558984c311ce48b84.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d53e3197cfe7315df34e47175026c6c1.jpg" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-04-09/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">https://kedreamix.github.io/Talk2Paper/Paper/2025-04-09/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                                    <span class="chip bg-color">åŒ»å­¦å›¾åƒ</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-04-09/Interactive/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-e960cc4afc6fc4f81a1f7d8d42289545.jpg" class="responsive-img" alt="Interactive">
                        
                        <span class="card-title">Interactive</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Interactive æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-04-09  Anisotropic space-time goal-oriented error control and mesh adaptivity   for convection-diffusion-reaction equations
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-04-09
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Interactive/" class="post-category">
                                    Interactive
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Interactive/">
                        <span class="chip bg-color">Interactive</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-04-09/Diffusion%20Models/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-1d874aa872307bde9e8a53820468e24f.jpg" class="responsive-img" alt="Diffusion Models">
                        
                        <span class="card-title">Diffusion Models</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Diffusion Models æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-04-09  Gaussian Mixture Flow Matching Models
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-04-09
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Diffusion-Models/" class="post-category">
                                    Diffusion Models
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Diffusion-Models/">
                        <span class="chip bg-color">Diffusion Models</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">19380.7k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
