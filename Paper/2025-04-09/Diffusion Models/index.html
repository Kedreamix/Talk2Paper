<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Diffusion Models">
    <meta name="description" content="Diffusion Models æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-04-09  Gaussian Mixture Flow Matching Models">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Diffusion Models | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pic1.zhimg.com/v2-1d874aa872307bde9e8a53820468e24f.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Diffusion Models</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Diffusion-Models/">
                                <span class="chip bg-color">Diffusion Models</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Diffusion-Models/" class="post-category">
                                Diffusion Models
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-04-09
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-05-14
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    16.6k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    67 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-04-09-æ›´æ–°"><a href="#2025-04-09-æ›´æ–°" class="headerlink" title="2025-04-09 æ›´æ–°"></a>2025-04-09 æ›´æ–°</h1><h2 id="Gaussian-Mixture-Flow-Matching-Models"><a href="#Gaussian-Mixture-Flow-Matching-Models" class="headerlink" title="Gaussian Mixture Flow Matching Models"></a>Gaussian Mixture Flow Matching Models</h2><p><strong>Authors:Hansheng Chen, Kai Zhang, Hao Tan, Zexiang Xu, Fujun Luan, Leonidas Guibas, Gordon Wetzstein, Sai Bi</strong></p>
<p>Diffusion models approximate the denoising distribution as a Gaussian and predict its mean, whereas flow matching models reparameterize the Gaussian mean as flow velocity. However, they underperform in few-step sampling due to discretization error and tend to produce over-saturated colors under classifier-free guidance (CFG). To address these limitations, we propose a novel Gaussian mixture flow matching (GMFlow) model: instead of predicting the mean, GMFlow predicts dynamic Gaussian mixture (GM) parameters to capture a multi-modal flow velocity distribution, which can be learned with a KL divergence loss. We demonstrate that GMFlow generalizes previous diffusion and flow matching models where a single Gaussian is learned with an $L_2$ denoising loss. For inference, we derive GM-SDE&#x2F;ODE solvers that leverage analytic denoising distributions and velocity fields for precise few-step sampling. Furthermore, we introduce a novel probabilistic guidance scheme that mitigates the over-saturation issues of CFG and improves image generation quality. Extensive experiments demonstrate that GMFlow consistently outperforms flow matching baselines in generation quality, achieving a Precision of 0.942 with only 6 sampling steps on ImageNet 256$\times$256. </p>
<blockquote>
<p>æ‰©æ•£æ¨¡å‹é€šè¿‡è¿‘ä¼¼å»å™ªåˆ†å¸ƒä¸ºé«˜æ–¯åˆ†å¸ƒå¹¶é¢„æµ‹å…¶å‡å€¼ï¼Œè€ŒæµåŒ¹é…æ¨¡å‹åˆ™å°†é«˜æ–¯å‡å€¼é‡æ–°å‚æ•°åŒ–ä¸ºæµé€Ÿã€‚ç„¶è€Œï¼Œç”±äºç¦»æ•£åŒ–è¯¯å·®ï¼Œå®ƒä»¬åœ¨å‡ æ­¥é‡‡æ ·ä¸Šè¡¨ç°ä¸ä½³ï¼Œå¹¶ä¸”åœ¨æ— åˆ†ç±»å™¨å¼•å¯¼ï¼ˆCFGï¼‰ä¸‹å®¹æ˜“äº§ç”Ÿè¿‡é¥±å’Œé¢œè‰²ã€‚ä¸ºäº†è§£å†³è¿™äº›å±€é™æ€§ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°å‹çš„é«˜æ–¯æ··åˆæµåŒ¹é…ï¼ˆGMFlowï¼‰æ¨¡å‹ï¼šGMFlowä¸é¢„æµ‹å‡å€¼ï¼Œè€Œæ˜¯é¢„æµ‹åŠ¨æ€é«˜æ–¯æ··åˆï¼ˆGMï¼‰å‚æ•°ï¼Œä»¥æ•æ‰å¤šæ¨¡å¼æµé€Ÿåˆ†å¸ƒï¼Œè¿™å¯ä»¥é€šè¿‡KLæ•£åº¦æŸå¤±æ¥å­¦ä¹ ã€‚æˆ‘ä»¬è¯æ˜äº†GMFlowèƒ½å¤Ÿæ¦‚æ‹¬ä¹‹å‰çš„æ‰©æ•£æ¨¡å‹å’ŒæµåŒ¹é…æ¨¡å‹ï¼Œå…¶ä¸­å•ä¸ªé«˜æ–¯æ˜¯é€šè¿‡L2å»å™ªæŸå¤±å­¦ä¹ çš„ã€‚å¯¹äºæ¨ç†ï¼Œæˆ‘ä»¬æ¨å‡ºäº†åˆ©ç”¨è§£æå»å™ªåˆ†å¸ƒå’Œé€Ÿåº¦åœºçš„GM-SDE&#x2F;ODEæ±‚è§£å™¨ï¼Œç”¨äºç²¾ç¡®å‡ æ­¥é‡‡æ ·ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§æ–°çš„æ¦‚ç‡å¼•å¯¼æ–¹æ¡ˆï¼Œè¯¥æ–¹æ¡ˆç¼“è§£äº†CFGçš„è¿‡é¥±å’Œé—®é¢˜ï¼Œæé«˜äº†å›¾åƒç”Ÿæˆè´¨é‡ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼ŒGMFlowåœ¨ç”Ÿæˆè´¨é‡æ–¹é¢å§‹ç»ˆä¼˜äºæµåŒ¹é…åŸºçº¿ï¼Œåœ¨ImageNet 256Ã—256ä¸Šä»…6æ­¥é‡‡æ ·å°±è¾¾åˆ°äº†0.942çš„ç²¾åº¦ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.05304v1">PDF</a> Code: <a target="_blank" rel="noopener" href="https://github.com/Lakonik/GMFlow">https://github.com/Lakonik/GMFlow</a></p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºä¸€ç§æ–°å‹çš„é«˜æ–¯æ··åˆæµåŒ¹é…ï¼ˆGMFlowï¼‰æ¨¡å‹ï¼Œç”¨äºè§£å†³æ‰©æ•£æ¨¡å‹å’ŒæµåŒ¹é…æ¨¡å‹åœ¨å°‘æ•°æ­¥éª¤é‡‡æ ·å’Œåˆ†ç±»å™¨å…è´¹æŒ‡å¯¼ä¸‹çš„å±€é™æ€§ã€‚GMFlowé€šè¿‡é¢„æµ‹åŠ¨æ€é«˜æ–¯æ··åˆå‚æ•°æ¥æ•æ‰å¤šæ¨¡æ€æµé€Ÿåº¦åˆ†å¸ƒï¼Œå¹¶é‡‡ç”¨KLæ•£åº¦æŸå¤±è¿›è¡Œå­¦ä¹ ã€‚GMFlowèƒ½å¤Ÿæ³›åŒ–ä¹‹å‰çš„æ‰©æ•£æ¨¡å‹å’ŒæµåŒ¹é…æ¨¡å‹ï¼Œå¹¶å¼•å…¥æ–°çš„æ¦‚ç‡æŒ‡å¯¼æ–¹æ¡ˆæ¥æé«˜å›¾åƒç”Ÿæˆè´¨é‡ã€‚å®éªŒè¡¨æ˜ï¼ŒGMFlowåœ¨ç”Ÿæˆè´¨é‡æ–¹é¢ä¸€ç›´ä¼˜äºæµåŒ¹é…åŸºçº¿ï¼Œåœ¨ImageNet 256x256ä¸Šä»…6æ­¥é‡‡æ ·å°±è¾¾åˆ°äº†0.942çš„ç²¾åº¦ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ‰©æ•£æ¨¡å‹å’ŒæµåŒ¹é…æ¨¡å‹åœ¨å°‘æ•°æ­¥éª¤é‡‡æ ·ä¸­å­˜åœ¨ç¦»æ•£åŒ–è¯¯å·®ã€‚</li>
<li>åˆ†ç±»å™¨å…è´¹æŒ‡å¯¼ï¼ˆCFGï¼‰ä¸‹ï¼Œæ¨¡å‹ä¼šäº§ç”Ÿè¿‡é¥±å’Œè‰²å½©ã€‚</li>
<li>æå‡ºçš„GMFlowæ¨¡å‹é€šè¿‡é¢„æµ‹åŠ¨æ€é«˜æ–¯æ··åˆå‚æ•°æ¥æ•æ‰å¤šæ¨¡æ€æµé€Ÿåº¦åˆ†å¸ƒã€‚</li>
<li>GMFlowé‡‡ç”¨KLæ•£åº¦æŸå¤±è¿›è¡Œå­¦ä¹ ï¼Œèƒ½å¤Ÿæ³›åŒ–ä¹‹å‰çš„æ‰©æ•£æ¨¡å‹å’ŒæµåŒ¹é…æ¨¡å‹ã€‚</li>
<li>GMFlowå¼•å…¥äº†æ–°çš„æ¦‚ç‡æŒ‡å¯¼æ–¹æ¡ˆï¼Œç¼“è§£äº†CFGçš„è¿‡é¥±å’Œé—®é¢˜ï¼Œæé«˜äº†å›¾åƒç”Ÿæˆè´¨é‡ã€‚</li>
<li>å®éªŒæ˜¾ç¤ºï¼ŒGMFlowåœ¨ç”Ÿæˆè´¨é‡æ–¹é¢ä¼˜äºæµåŒ¹é…åŸºçº¿ï¼Œåœ¨ImageNet 256x256ä¸Šä»…éœ€è¦6æ­¥é‡‡æ ·å°±è¾¾åˆ°äº†é«˜ç²¾åº¦ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.05304">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-7eea28d205cfffb76de0c3b7f4b8f6c5.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-0b4c7e2ef43a9ac8649c96d9249e7520.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-07f7b1050b07c546a1397d6f4d142896.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="DA2Diff-Exploring-Degradation-aware-Adaptive-Diffusion-Priors-for-All-in-One-Weather-Restoration"><a href="#DA2Diff-Exploring-Degradation-aware-Adaptive-Diffusion-Priors-for-All-in-One-Weather-Restoration" class="headerlink" title="DA2Diff: Exploring Degradation-aware Adaptive Diffusion Priors for   All-in-One Weather Restoration"></a>DA2Diff: Exploring Degradation-aware Adaptive Diffusion Priors for   All-in-One Weather Restoration</h2><p><strong>Authors:Jiamei Xiong, Xuefeng Yan, Yongzhen Wang, Wei Zhao, Xiao-Ping Zhang, Mingqiang Wei</strong></p>
<p>Image restoration under adverse weather conditions is a critical task for many vision-based applications. Recent all-in-one frameworks that handle multiple weather degradations within a unified model have shown potential. However, the diversity of degradation patterns across different weather conditions, as well as the complex and varied nature of real-world degradations, pose significant challenges for multiple weather removal. To address these challenges, we propose an innovative diffusion paradigm with degradation-aware adaptive priors for all-in-one weather restoration, termed DA2Diff. It is a new exploration that applies CLIP to perceive degradation-aware properties for better multi-weather restoration. Specifically, we deploy a set of learnable prompts to capture degradation-aware representations by the prompt-image similarity constraints in the CLIP space. By aligning the snowy&#x2F;hazy&#x2F;rainy images with snow&#x2F;haze&#x2F;rain prompts, each prompt contributes to different weather degradation characteristics. The learned prompts are then integrated into the diffusion model via the designed weather specific prompt guidance module, making it possible to restore multiple weather types. To further improve the adaptiveness to complex weather degradations, we propose a dynamic expert selection modulator that employs a dynamic weather-aware router to flexibly dispatch varying numbers of restoration experts for each weather-distorted image, allowing the diffusion model to restore diverse degradations adaptively. Experimental results substantiate the favorable performance of DA2Diff over state-of-the-arts in quantitative and qualitative evaluation. Source code will be available after acceptance. </p>
<blockquote>
<p>æ¶åŠ£å¤©æ°”æ¡ä»¶ä¸‹çš„å›¾åƒæ¢å¤æ˜¯è®¸å¤šè§†è§‰åº”ç”¨ä¸­çš„ä¸€é¡¹å…³é”®ä»»åŠ¡ã€‚è¿‘æœŸçš„ä¸€ç«™å¼æ¡†æ¶èƒ½å¤Ÿåœ¨ç»Ÿä¸€æ¨¡å‹ä¸­å¤„ç†å¤šç§å¤©æ°”é€€åŒ–ï¼Œæ˜¾ç¤ºå‡ºå…¶æ½œåŠ›ã€‚ç„¶è€Œï¼Œä¸åŒå¤©æ°”æ¡ä»¶ä¸‹é€€åŒ–æ¨¡å¼çš„å¤šæ ·æ€§ä»¥åŠç°å®ä¸–ç•Œé€€åŒ–çš„å¤æ‚æ€§å’Œå¤šå˜æ€§è´¨ï¼Œç»™å¤šç§å¤©æ°”ç§»é™¤å¸¦æ¥äº†é‡å¤§æŒ‘æˆ˜ã€‚ä¸ºäº†åº”å¯¹è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åˆ›æ–°çš„æ‰©æ•£èŒƒå¼ï¼Œé‡‡ç”¨é€€åŒ–æ„ŸçŸ¥è‡ªé€‚åº”å…ˆéªŒè¿›è¡Œä¸€ç«™å¼å¤©æ°”æ¢å¤ï¼Œç§°ä¸ºDA2Diffã€‚è¿™æ˜¯ä¸€æ¬¡æ–°çš„æ¢ç´¢ï¼Œå°†CLIPåº”ç”¨äºæ„ŸçŸ¥é€€åŒ–æ„ŸçŸ¥å±æ€§ï¼Œä»¥æ›´å¥½åœ°è¿›è¡Œå¤šå¤©æ°”æ¢å¤ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬éƒ¨ç½²äº†ä¸€ç³»åˆ—å¯å­¦ä¹ çš„æç¤ºæ¥é€šè¿‡CLIPç©ºé—´ä¸­çš„æç¤ºå›¾åƒç›¸ä¼¼æ€§çº¦æŸæ¥æ•è·é€€åŒ–æ„ŸçŸ¥è¡¨ç¤ºã€‚é€šè¿‡å°†ä¸‹é›ª&#x2F;é›¾éœ¾&#x2F;é›¨å¤©å›¾åƒä¸é›ª&#x2F;é›¾éœ¾&#x2F;é›¨æç¤ºå¯¹é½ï¼Œæ¯ä¸ªæç¤ºæœ‰åŠ©äºä¸åŒçš„å¤©æ°”é€€åŒ–ç‰¹å¾ã€‚ç„¶åå°†å­¦ä¹ åˆ°çš„æç¤ºé›†æˆåˆ°æ‰©æ•£æ¨¡å‹ä¸­ï¼Œé€šè¿‡è®¾è®¡çš„å¤©æ°”ç‰¹å®šæç¤ºæŒ‡å¯¼æ¨¡å—ï¼Œä½¿å¾—æ¢å¤å¤šç§å¤©æ°”ç±»å‹æˆä¸ºå¯èƒ½ã€‚ä¸ºäº†è¿›ä¸€æ­¥æé«˜å¯¹å¤æ‚å¤©æ°”é€€åŒ–çš„é€‚åº”æ€§ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åŠ¨æ€ä¸“å®¶é€‰æ‹©è°ƒåˆ¶å™¨ï¼Œå®ƒé‡‡ç”¨åŠ¨æ€å¤©æ°”æ„ŸçŸ¥è·¯ç”±å™¨ï¼Œçµæ´»åœ°ä¸ºæ¯ç§å¤©æ°”å¤±çœŸå›¾åƒåˆ†é…ä¸åŒæ•°é‡çš„æ¢å¤ä¸“å®¶ï¼Œä½¿æ‰©æ•£æ¨¡å‹èƒ½å¤Ÿè‡ªé€‚åº”åœ°æ¢å¤å„ç§é€€åŒ–ã€‚å®éªŒç»“æœè¯å®ï¼ŒDA2Diffåœ¨å®šé‡å’Œå®šæ€§è¯„ä¼°ä¸Šçš„æ€§èƒ½ä¼˜äºç°æœ‰æŠ€æœ¯ã€‚æºä»£ç å°†åœ¨æ¥å—åæä¾›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.05135v1">PDF</a> </p>
<p><strong>Summary</strong><br>     æœ¬æ–‡æå‡ºä¸€ç§åä¸ºDA2Diffçš„æ‰©æ•£æ¨¡å‹ï¼Œç”¨äºä¸€æ¬¡åº”å¯¹å¤šç§å¤©æ°”çŠ¶å†µçš„å›¾åƒä¿®å¤ã€‚è¯¥æ¨¡å‹é€šè¿‡å¼•å…¥é™è§£æ„ŸçŸ¥è‡ªé€‚åº”å…ˆéªŒå’ŒCLIPæŠ€æœ¯ï¼Œèƒ½æ›´å‡†ç¡®åœ°æ„ŸçŸ¥ä¸åŒå¤©æ°”æ¡ä»¶ä¸‹çš„å›¾åƒé€€åŒ–ç‰¹å¾ã€‚é€šè¿‡è®¾è®¡å¤©æ°”ç‰¹å®šçš„æç¤ºå¼•å¯¼æ¨¡å—å’ŒåŠ¨æ€ä¸“å®¶é€‰æ‹©è°ƒåˆ¶å™¨ï¼Œæ¨¡å‹èƒ½æ›´çµæ´»åœ°é€‚åº”å„ç§å¤æ‚å¤©æ°”é€€åŒ–æƒ…å†µï¼Œæé«˜äº†å›¾åƒä¿®å¤çš„æ•ˆæœã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>DA2Diffæ¨¡å‹æ˜¯ä¸€ç§é’ˆå¯¹å¤šç§å¤©æ°”çŠ¶å†µçš„å›¾åƒä¿®å¤æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³ä¸åŒå¤©æ°”å¼•èµ·çš„å›¾åƒé€€åŒ–é—®é¢˜ã€‚</li>
<li>æ¨¡å‹é‡‡ç”¨é™è§£æ„ŸçŸ¥è‡ªé€‚åº”å…ˆéªŒï¼Œèƒ½æ›´å‡†ç¡®åœ°è¯†åˆ«ä¸åŒå¤©æ°”æ¡ä»¶ä¸‹çš„å›¾åƒç‰¹å¾ã€‚</li>
<li>CLIPæŠ€æœ¯è¢«ç”¨äºæ„ŸçŸ¥é™è§£æ„ŸçŸ¥å±æ€§ï¼Œæœ‰åŠ©äºæé«˜å¤šå¤©æ°”ä¿®å¤æ•ˆæœã€‚</li>
<li>é€šè¿‡è®¾è®¡å¤©æ°”ç‰¹å®šçš„æç¤ºå¼•å¯¼æ¨¡å—ï¼Œå°†å­¦ä¹ åˆ°çš„æç¤ºé›†æˆåˆ°æ‰©æ•£æ¨¡å‹ä¸­ï¼Œå®ç°å¤šç§å¤©æ°”ç±»å‹çš„å›¾åƒä¿®å¤ã€‚</li>
<li>åŠ¨æ€ä¸“å®¶é€‰æ‹©è°ƒåˆ¶å™¨çš„å¼•å…¥ï¼Œä½¿å¾—æ¨¡å‹èƒ½æ›´çµæ´»åœ°é€‚åº”å¤æ‚çš„å¤©æ°”é€€åŒ–æƒ…å†µã€‚</li>
<li>æ¨¡å‹é€šè¿‡åŠ¨æ€å¤©æ°”æ„ŸçŸ¥è·¯ç”±å™¨è°ƒåº¦ä¸åŒæ•°é‡çš„ä¿®å¤ä¸“å®¶ï¼Œä½¿æ‰©æ•£æ¨¡å‹èƒ½è‡ªé€‚åº”åœ°ä¿®å¤å„ç§é€€åŒ–æƒ…å†µã€‚</li>
<li>å®éªŒç»“æœè¡¨æ˜ï¼ŒDA2Diffåœ¨å®šé‡å’Œå®šæ€§è¯„ä¼°ä¸Šå‡è¡¨ç°å‡ºä¼˜äºç°æœ‰æŠ€æœ¯çš„æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.05135">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-401573e68ed138e4a813a56548ec89fe.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-2518ff788ef52a54e35d7b88c915484d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ebf1e6e2239ab6871ba1bb18c534b56e.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="REWIND-Real-Time-Egocentric-Whole-Body-Motion-Diffusion-with-Exemplar-Based-Identity-Conditioning"><a href="#REWIND-Real-Time-Egocentric-Whole-Body-Motion-Diffusion-with-Exemplar-Based-Identity-Conditioning" class="headerlink" title="REWIND: Real-Time Egocentric Whole-Body Motion Diffusion with   Exemplar-Based Identity Conditioning"></a>REWIND: Real-Time Egocentric Whole-Body Motion Diffusion with   Exemplar-Based Identity Conditioning</h2><p><strong>Authors:Jihyun Lee, Weipeng Xu, Alexander Richard, Shih-En Wei, Shunsuke Saito, Shaojie Bai, Te-Li Wang, Minhyuk Sung,  Tae-Kyun,  Kim, Jason Saragih</strong></p>
<p>We present REWIND (Real-Time Egocentric Whole-Body Motion Diffusion), a one-step diffusion model for real-time, high-fidelity human motion estimation from egocentric image inputs. While an existing method for egocentric whole-body (i.e., body and hands) motion estimation is non-real-time and acausal due to diffusion-based iterative motion refinement to capture correlations between body and hand poses, REWIND operates in a fully causal and real-time manner. To enable real-time inference, we introduce (1) cascaded body-hand denoising diffusion, which effectively models the correlation between egocentric body and hand motions in a fast, feed-forward manner, and (2) diffusion distillation, which enables high-quality motion estimation with a single denoising step. Our denoising diffusion model is based on a modified Transformer architecture, designed to causally model output motions while enhancing generalizability to unseen motion lengths. Additionally, REWIND optionally supports identity-conditioned motion estimation when identity prior is available. To this end, we propose a novel identity conditioning method based on a small set of pose exemplars of the target identity, which further enhances motion estimation quality. Through extensive experiments, we demonstrate that REWIND significantly outperforms the existing baselines both with and without exemplar-based identity conditioning. </p>
<blockquote>
<p>æˆ‘ä»¬æå‡ºäº†REWINDï¼ˆå®æ—¶ä»¥è‡ªæˆ‘ä¸ºä¸­å¿ƒçš„å…¨èº«è¿åŠ¨æ‰©æ•£ï¼‰æ¨¡å‹ï¼Œè¿™æ˜¯ä¸€ä¸ªä¸€æ­¥æ‰©æ•£æ¨¡å‹ï¼Œå¯ä»¥ä»ä»¥è‡ªæˆ‘ä¸ºä¸­å¿ƒçš„å›¾åƒè¾“å…¥ä¸­è¿›è¡Œå®æ—¶ã€é«˜ä¿çœŸçš„äººç±»è¿åŠ¨ä¼°è®¡ã€‚ç°æœ‰çš„ä»¥è‡ªæˆ‘ä¸ºä¸­å¿ƒçš„å…¨èº«ï¼ˆå³èº«ä½“å’Œæ‰‹ï¼‰è¿åŠ¨ä¼°è®¡æ–¹æ³•ç”±äºåŸºäºæ‰©æ•£çš„è¿­ä»£è¿åŠ¨ç»†åŒ–æ¥æ•æ‰èº«ä½“å’Œæ‰‹éƒ¨å§¿åŠ¿ä¹‹é—´çš„ç›¸å…³æ€§ï¼Œå› æ­¤æ˜¯éå®æ—¶çš„å’Œéå› æœçš„ã€‚ç„¶è€Œï¼ŒREWINDä»¥å®Œå…¨å› æœå’Œå®æ—¶çš„æ–¹å¼è¿›è¡Œæ“ä½œã€‚ä¸ºäº†å®ç°å®æ—¶æ¨ç†ï¼Œæˆ‘ä»¬å¼•å…¥äº†ï¼ˆ1ï¼‰çº§è”çš„èº«ä½“æ‰‹éƒ¨å»å™ªæ‰©æ•£ï¼Œå®ƒä»¥å¿«é€Ÿå‰é¦ˆçš„æ–¹å¼æœ‰æ•ˆåœ°å»ºæ¨¡äº†ä»¥è‡ªæˆ‘ä¸ºä¸­å¿ƒçš„èº«ä½“å’Œæ‰‹éƒ¨è¿åŠ¨ä¹‹é—´çš„ç›¸å…³æ€§ï¼›ï¼ˆ2ï¼‰æ‰©æ•£è’¸é¦ï¼Œå®ƒå¯ä»¥åœ¨å•ä¸ªå»å™ªæ­¥éª¤ä¸­å®ç°é«˜è´¨é‡çš„è¿åŠ¨ä¼°è®¡ã€‚æˆ‘ä»¬çš„å»å™ªæ‰©æ•£æ¨¡å‹åŸºäºæ”¹è¿›çš„Transformeræ¶æ„ï¼Œæ—¨åœ¨å› æœåœ°å»ºæ¨¡è¾“å‡ºè¿åŠ¨ï¼ŒåŒæ—¶æé«˜æœªè§è¿åŠ¨é•¿åº¦çš„æ³›åŒ–èƒ½åŠ›ã€‚å¦å¤–ï¼Œå½“å­˜åœ¨èº«ä»½å…ˆéªŒæ—¶ï¼ŒREWINDè¿˜æ”¯æŒå¸¦èº«ä»½æ¡ä»¶è¿åŠ¨ä¼°è®¡ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºç›®æ ‡èº«ä»½çš„å°å§¿æ€æ ·æœ¬é›†çš„æ–°å‹èº«ä»½æ¡ä»¶æ–¹æ³•ï¼Œè¿™è¿›ä¸€æ­¥æé«˜äº†è¿åŠ¨ä¼°è®¡çš„è´¨é‡ã€‚é€šè¿‡å¹¿æ³›çš„å®éªŒï¼Œæˆ‘ä»¬è¯æ˜äº†REWINDåœ¨æœ‰æˆ–æ— åŸºäºèŒƒä¾‹çš„èº«ä»½æ¡ä»¶ä¸‹éƒ½æ˜¾è‘—ä¼˜äºç°æœ‰åŸºçº¿ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.04956v1">PDF</a> Accepted to CVPR 2025, project page:   <a target="_blank" rel="noopener" href="https://jyunlee.github.io/projects/rewind/">https://jyunlee.github.io/projects/rewind/</a></p>
<p><strong>Summary</strong></p>
<p>REWINDæ˜¯ä¸€ä¸ªä¸€æ­¥åˆ°ä½çš„æ‰©æ•£æ¨¡å‹ï¼Œå¯ä»ç¬¬ä¸€äººç§°å›¾åƒè¾“å…¥ä¸­è¿›è¡Œå®æ—¶ã€é«˜ä¿çœŸçš„äººç±»è¿åŠ¨ä¼°è®¡ã€‚ä¸ç°æœ‰çš„å…¨èº«è¿åŠ¨ä¼°è®¡æ–¹æ³•ä¸åŒï¼ŒREWINDå®ç°äº†å…¨å› æœå’Œå®æ—¶æ“ä½œã€‚é€šè¿‡å¼•å…¥çº§è”çš„ä½“æ‰‹å»å™ªæ‰©æ•£å’Œæ‰©æ•£è’¸é¦æŠ€æœ¯ï¼Œè¯¥æ¨¡å‹å®ç°äº†å¿«é€Ÿå‰é¦ˆçš„å…¨èº«è¿åŠ¨ä¸æ‰‹éƒ¨è¿åŠ¨å…³è”å»ºæ¨¡ï¼Œå¹¶èƒ½åœ¨å•æ¬¡å»å™ªæ­¥éª¤ä¸­å®ç°é«˜è´¨é‡çš„è¿åŠ¨ä¼°è®¡ã€‚æ­¤å¤–ï¼ŒREWINDè¿˜æ”¯æŒåœ¨æœ‰èº«ä»½å…ˆéªŒçš„æƒ…å†µä¸‹è¿›è¡Œèº«ä»½æ¡ä»¶è¿åŠ¨ä¼°è®¡ï¼Œè¿›ä¸€æ­¥æé«˜äº†è¿åŠ¨ä¼°è®¡çš„è´¨é‡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>REWINDæ˜¯ä¸€ä¸ªç”¨äºå®æ—¶é«˜ä¿çœŸäººç±»è¿åŠ¨ä¼°è®¡çš„ä¸€æ­¥æ‰©æ•£æ¨¡å‹ï¼Œä»ç¬¬ä¸€äººç§°å›¾åƒè¾“å…¥ã€‚</li>
<li>ä¸ç°æœ‰æ–¹æ³•ä¸åŒï¼ŒREWINDå®ç°äº†å…¨å› æœå’Œå®æ—¶çš„æ“ä½œæ–¹å¼ã€‚</li>
<li>é€šè¿‡å¼•å…¥çº§è”çš„ä½“æ‰‹å»å™ªæ‰©æ•£ï¼ŒREWINDæœ‰æ•ˆåœ°å»ºæ¨¡äº†èº«ä½“ä¸æ‰‹éƒ¨è¿åŠ¨çš„å¿«é€Ÿå‰é¦ˆå…³è”ã€‚</li>
<li>æ‰©æ•£è’¸é¦æŠ€æœ¯ä½¿REWINDåœ¨å•æ¬¡å»å™ªæ­¥éª¤ä¸­å°±èƒ½å®ç°é«˜è´¨é‡çš„è¿åŠ¨ä¼°è®¡ã€‚</li>
<li>REWINDæ”¯æŒåœ¨æœ‰èº«ä»½å…ˆéªŒçš„æƒ…å†µä¸‹è¿›è¡Œèº«ä»½æ¡ä»¶è¿åŠ¨ä¼°è®¡ã€‚</li>
<li>æå‡ºäº†ä¸€ç§åŸºäºå°‘é‡ç›®æ ‡èº«ä»½å§¿åŠ¿èŒƒä¾‹çš„æ–°å‹èº«ä»½æ¡ä»¶æ–¹æ³•ï¼Œè¿›ä¸€æ­¥æé«˜è¿åŠ¨ä¼°è®¡è´¨é‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.04956">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-399087734d3342ae8d536dc61034e09c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5e81f98747d86474e3f1cbd299db9509.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Continuous-Locomotive-Crowd-Behavior-Generation"><a href="#Continuous-Locomotive-Crowd-Behavior-Generation" class="headerlink" title="Continuous Locomotive Crowd Behavior Generation"></a>Continuous Locomotive Crowd Behavior Generation</h2><p><strong>Authors:Inhwan Bae, Junoh Lee, Hae-Gon Jeon</strong></p>
<p>Modeling and reproducing crowd behaviors are important in various domains including psychology, robotics, transport engineering and virtual environments. Conventional methods have focused on synthesizing momentary scenes, which have difficulty in replicating the continuous nature of real-world crowds. In this paper, we introduce a novel method for automatically generating continuous, realistic crowd trajectories with heterogeneous behaviors and interactions among individuals. We first design a crowd emitter model. To do this, we obtain spatial layouts from single input images, including a segmentation map, appearance map, population density map and population probability, prior to crowd generation. The emitter then continually places individuals on the timeline by assigning independent behavior characteristics such as agentsâ€™ type, pace, and start&#x2F;end positions using diffusion models. Next, our crowd simulator produces their long-term locomotions. To simulate diverse actions, it can augment their behaviors based on a Markov chain. As a result, our overall framework populates the scenes with heterogeneous crowd behaviors by alternating between the proposed emitter and simulator. Note that all the components in the proposed framework are user-controllable. Lastly, we propose a benchmark protocol to evaluate the realism and quality of the generated crowds in terms of the scene-level population dynamics and the individual-level trajectory accuracy. We demonstrate that our approach effectively models diverse crowd behavior patterns and generalizes well across different geographical environments. Code is publicly available at <a target="_blank" rel="noopener" href="https://github.com/InhwanBae/CrowdES">https://github.com/InhwanBae/CrowdES</a> . </p>
<blockquote>
<p>å»ºæ¨¡å’Œå†ç°äººç¾¤è¡Œä¸ºå¯¹äºå¿ƒç†å­¦ã€æœºå™¨äººæŠ€æœ¯ã€äº¤é€šå·¥ç¨‹å’Œè™šæ‹Ÿç¯å¢ƒç­‰å¤šä¸ªé¢†åŸŸéƒ½éå¸¸é‡è¦ã€‚ä¼ ç»Ÿæ–¹æ³•ä¸»è¦å…³æ³¨å³æ—¶åœºæ™¯çš„åˆæˆï¼Œå¾ˆéš¾å¤åˆ¶ç°å®ä¸–ç•Œä¸­äººç¾¤è¡Œä¸ºçš„è¿ç»­æ€§ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†ä¸€ç§è‡ªåŠ¨ç”Ÿæˆè¿ç»­ã€é€¼çœŸçš„äººç¾¤è½¨è¿¹çš„æ–°æ–¹æ³•ï¼Œè¯¥æ–¹æ³•èƒ½å¤Ÿå±•ç°å‡ºäººç¾¤ä¸­çš„ä¸åŒè¡Œä¸ºå’Œä¸ªä½“é—´çš„äº¤äº’ã€‚æˆ‘ä»¬é¦–å…ˆè®¾è®¡äº†ä¸€ä¸ªäººç¾¤å‘å°„å™¨æ¨¡å‹ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬ä»å•ä¸ªè¾“å…¥å›¾åƒä¸­è·å–ç©ºé—´å¸ƒå±€ï¼ŒåŒ…æ‹¬åˆ†å‰²å›¾ã€å¤–è§‚å›¾ã€äººå£å¯†åº¦å›¾å’Œäººå£æ¦‚ç‡å›¾ï¼Œç„¶åè¿›è¡Œäººç¾¤ç”Ÿæˆã€‚å‘å°„å™¨é€šè¿‡åˆ†é…ç‹¬ç«‹çš„è¡Œä¸ºç‰¹å¾ï¼ˆå¦‚ä»£ç†çš„ç±»å‹ã€é€Ÿåº¦ä»¥åŠèµ·å§‹&#x2F;ç»“æŸä½ç½®ï¼‰æ¥åœ¨æ—¶é—´ä¸Šä¸æ–­æ”¾ç½®ä¸ªä½“ï¼Œå¹¶åˆ©ç”¨æ‰©æ•£æ¨¡å‹æ¥æ¨¡æ‹Ÿè¿™äº›è¡Œä¸ºç‰¹å¾ã€‚æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬çš„äººç¾¤æ¨¡æ‹Ÿå™¨äº§ç”Ÿä»–ä»¬çš„é•¿æœŸè¿åŠ¨ã€‚ä¸ºäº†æ¨¡æ‹Ÿå„ç§åŠ¨ä½œï¼Œæ¨¡æ‹Ÿå™¨å¯ä»¥æ ¹æ®é©¬å°”å¯å¤«é“¾å¢å¼ºä»–ä»¬çš„è¡Œä¸ºã€‚å› æ­¤ï¼Œé€šè¿‡äº¤æ›¿ä½¿ç”¨æ‰€æå‡ºå‘å°„å™¨å’Œæ¨¡æ‹Ÿå™¨ï¼Œæˆ‘ä»¬çš„æ•´ä½“æ¡†æ¶èƒ½å¤Ÿåœ¨åœºæ™¯ä¸­å¡«å……å…·æœ‰ä¸åŒè¡Œä¸ºçš„äººç¾¤ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œè¯¥æ¡†æ¶çš„æ‰€æœ‰ç»„ä»¶éƒ½æ˜¯ç”¨æˆ·å¯æ§åˆ¶çš„ã€‚æœ€åï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºå‡†åè®®æ¥è¯„ä¼°ç”Ÿæˆäººç¾¤çš„é€¼çœŸåº¦å’Œè´¨é‡ï¼ŒåŒ…æ‹¬åœºæ™¯çº§åˆ«çš„äººå£åŠ¨æ€å’Œä¸ªäººçº§åˆ«çš„è½¨è¿¹å‡†ç¡®æ€§ã€‚æˆ‘ä»¬è¯æ˜äº†æˆ‘ä»¬çš„æ–¹æ³•èƒ½å¤Ÿæœ‰æ•ˆåœ°æ¨¡æ‹Ÿä¸åŒçš„äººç¾¤è¡Œä¸ºæ¨¡å¼ï¼Œå¹¶åœ¨ä¸åŒçš„åœ°ç†ç¯å¢ƒä¸­å…·æœ‰è‰¯å¥½çš„æ³›åŒ–èƒ½åŠ›ã€‚ä»£ç å…¬å¼€åœ¨ <a target="_blank" rel="noopener" href="https://github.com/InhwanBae/CrowdES%E3%80%82">https://github.com/InhwanBae/CrowdESã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.04756v1">PDF</a> Accepted at CVPR 2025. Project page:   <a target="_blank" rel="noopener" href="https://ihbae.com/publication/crowdes/">https://ihbae.com/publication/crowdes/</a></p>
<p><strong>æ‘˜è¦</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸€ç§è‡ªåŠ¨ç”Ÿæˆè¿ç»­ã€çœŸå®äººç¾¤è½¨è¿¹çš„æ–°æ–¹æ³•ï¼Œè¯¥æ–¹æ³•èƒ½å¤Ÿæ¨¡æ‹Ÿå‡ºäººç¾¤ä¸­çš„ä¸åŒè¡Œä¸ºå’Œä¸ªä½“é—´çš„äº¤äº’ã€‚é€šè¿‡è®¾è®¡äººç¾¤å‘å°„å™¨æ¨¡å‹ï¼Œä»å•ä¸€è¾“å…¥å›¾åƒä¸­è·å–ç©ºé—´å¸ƒå±€ï¼ŒåŒ…æ‹¬åˆ†å‰²å›¾ã€å¤–è§‚å›¾ã€äººå£å¯†åº¦å›¾å’Œäººå£æ¦‚ç‡ç­‰ä¿¡æ¯ï¼Œä¸ºåç»­çš„äººç¾¤ç”Ÿæˆæä¾›ä¾æ®ã€‚ä½¿ç”¨æ‰©æ•£æ¨¡å‹ä¸ºä¸ªä½“åˆ†é…ç‹¬ç«‹çš„è¡Œä¸ºç‰¹å¾ï¼Œå¦‚ç±»å‹ã€é€Ÿåº¦ã€èµ·å§‹&#x2F;ç»ˆæ­¢ä½ç½®ç­‰ï¼Œç”Ÿæˆé•¿æœŸè¿åŠ¨æ¨¡å¼ã€‚æ­¤å¤–ï¼Œå¯é€šè¿‡é©¬å°”å¯å¤«é“¾å¢å¼ºè¡Œä¸ºæ¨¡æ‹Ÿå¤šæ ·åŒ–çš„åŠ¨ä½œã€‚æ•´ä½“æ¡†æ¶é€šè¿‡äº¤æ›¿ä½¿ç”¨å‘å°„å™¨å’Œæ¨¡æ‹Ÿå™¨æ¥å¡«å……åœºæ™¯ï¼Œå±•ç°å‡ºäººç¾¤è¡Œä¸ºçš„å¼‚è´¨æ€§ã€‚æ‰€æœ‰ç»„ä»¶å‡å¯ç”±ç”¨æˆ·æ§åˆ¶ã€‚æœ€åï¼Œæå‡ºäº†ä¸€ä¸ªåŸºå‡†åè®®æ¥è¯„ä¼°ç”Ÿæˆäººç¾¤çš„é€¼çœŸåº¦å’Œè´¨é‡ï¼Œå®éªŒè¯æ˜è¯¥æ–¹æ³•èƒ½æœ‰æ•ˆæ¨¡æ‹Ÿå¤šç§äººç¾¤è¡Œä¸ºæ¨¡å¼ï¼Œå¹¶åœ¨ä¸åŒçš„åœ°ç†ç¯å¢ƒä¸­å…·æœ‰è‰¯å¥½çš„æ³›åŒ–èƒ½åŠ›ã€‚ç›¸å…³ä»£ç å·²å…¬å¼€ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>ä»‹ç»äº†ä¸€ç§è‡ªåŠ¨ç”Ÿæˆè¿ç»­ã€çœŸå®äººç¾¤è½¨è¿¹çš„æ–°æ–¹æ³•ï¼Œæ¨¡æ‹Ÿäººç¾¤ä¸­çš„ä¸åŒè¡Œä¸ºå’Œä¸ªä½“äº¤äº’ã€‚</li>
<li>é€šè¿‡è®¾è®¡äººç¾¤å‘å°„å™¨æ¨¡å‹ï¼Œä»å•ä¸€è¾“å…¥å›¾åƒè·å–ç©ºé—´å¸ƒå±€ä¿¡æ¯ã€‚</li>
<li>ä½¿ç”¨æ‰©æ•£æ¨¡å‹ä¸ºä¸ªä½“åˆ†é…ç‹¬ç«‹çš„è¡Œä¸ºç‰¹å¾ã€‚</li>
<li>é€šè¿‡é©¬å°”å¯å¤«é“¾å¢å¼ºè¡Œä¸ºæ¨¡æ‹Ÿä»¥äº§ç”Ÿå¤šæ ·åŒ–çš„åŠ¨ä½œã€‚</li>
<li>æ•´ä½“æ¡†æ¶é€šè¿‡äº¤æ›¿ä½¿ç”¨å‘å°„å™¨å’Œæ¨¡æ‹Ÿå™¨å¡«å……åœºæ™¯ï¼Œå±•ç°å‡ºäººç¾¤è¡Œä¸ºçš„å¼‚è´¨æ€§ã€‚</li>
<li>æå‡ºçš„åŸºå‡†åè®®å¯è¯„ä¼°ç”Ÿæˆäººç¾¤çš„é€¼çœŸåº¦å’Œè´¨é‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.04756">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-1d22f94141b8b497221bf81f1623548a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-eee7678fdb6e2eadbe594b82cbbb3633.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-91b1739072f7ff162c7f933fe1dcb6f8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f1223007f2bd2d9dd518c4f183a03518.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="DiTaiListener-Controllable-High-Fidelity-Listener-Video-Generation-with-Diffusion"><a href="#DiTaiListener-Controllable-High-Fidelity-Listener-Video-Generation-with-Diffusion" class="headerlink" title="DiTaiListener: Controllable High Fidelity Listener Video Generation with   Diffusion"></a>DiTaiListener: Controllable High Fidelity Listener Video Generation with   Diffusion</h2><p><strong>Authors:Maksim Siniukov, Di Chang, Minh Tran, Hongkun Gong, Ashutosh Chaubey, Mohammad Soleymani</strong></p>
<p>Generating naturalistic and nuanced listener motions for extended interactions remains an open problem. Existing methods often rely on low-dimensional motion codes for facial behavior generation followed by photorealistic rendering, limiting both visual fidelity and expressive richness. To address these challenges, we introduce DiTaiListener, powered by a video diffusion model with multimodal conditions. Our approach first generates short segments of listener responses conditioned on the speakerâ€™s speech and facial motions with DiTaiListener-Gen. It then refines the transitional frames via DiTaiListener-Edit for a seamless transition. Specifically, DiTaiListener-Gen adapts a Diffusion Transformer (DiT) for the task of listener head portrait generation by introducing a Causal Temporal Multimodal Adapter (CTM-Adapter) to process speakersâ€™ auditory and visual cues. CTM-Adapter integrates speakersâ€™ input in a causal manner into the video generation process to ensure temporally coherent listener responses. For long-form video generation, we introduce DiTaiListener-Edit, a transition refinement video-to-video diffusion model. The model fuses video segments into smooth and continuous videos, ensuring temporal consistency in facial expressions and image quality when merging short video segments produced by DiTaiListener-Gen. Quantitatively, DiTaiListener achieves the state-of-the-art performance on benchmark datasets in both photorealism (+73.8% in FID on RealTalk) and motion representation (+6.1% in FD metric on VICO) spaces. User studies confirm the superior performance of DiTaiListener, with the model being the clear preference in terms of feedback, diversity, and smoothness, outperforming competitors by a significant margin. </p>
<blockquote>
<p>ç”Ÿæˆè‡ªç„¶ä¸”ç»†è…»çš„åœ¨è¿ç»­äº’åŠ¨ä¸­çš„å¬ä¼—åŠ¨ä½œä»ç„¶æ˜¯ä¸€ä¸ªæ‚¬è€Œæœªå†³çš„é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•é€šå¸¸ä¾èµ–äºä½ç»´è¿åŠ¨ä»£ç è¿›è¡Œé¢éƒ¨è¡Œä¸ºç”Ÿæˆï¼Œéšåè¿›è¡Œé€¼çœŸçš„æ¸²æŸ“ï¼Œè¿™é™åˆ¶äº†è§†è§‰ä¿çœŸåº¦å’Œè¡¨è¾¾ä¸°å¯Œæ€§ã€‚ä¸ºäº†åº”å¯¹è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†DiTaiListenerï¼Œå®ƒç”±ä¸€ä¸ªå…·æœ‰å¤šæ¨¡æ€æ¡ä»¶é™åˆ¶çš„è§†é¢‘æ‰©æ•£æ¨¡å‹é©±åŠ¨ã€‚æˆ‘ä»¬çš„æ–¹æ³•é¦–å…ˆä½¿ç”¨DiTaiListener-Genæ ¹æ®è¯´è¯è€…çš„è¯­éŸ³å’Œé¢éƒ¨åŠ¨ä½œç”Ÿæˆå¬ä¼—çš„ç®€çŸ­å›åº”ç‰‡æ®µã€‚ç„¶åï¼Œå®ƒä½¿ç”¨DiTaiListener-Editå¯¹è¿‡æ¸¡å¸§è¿›è¡Œç»†åŒ–ï¼Œä»¥å®ç°æ— ç¼è¿‡æ¸¡ã€‚å…·ä½“æ¥è¯´ï¼ŒDiTaiListener-Gené‡‡ç”¨æ‰©æ•£å˜å‹å™¨ï¼ˆDiTï¼‰æ¥å®Œæˆå¬ä¼—å¤´éƒ¨è‚–åƒç”Ÿæˆçš„ä»»åŠ¡ï¼Œé€šè¿‡å¼•å…¥å› æœæ—¶åºå¤šæ¨¡æ€é€‚é…å™¨ï¼ˆCTM-Adapterï¼‰æ¥å¤„ç†è¯´è¯è€…çš„å¬è§‰å’Œè§†è§‰çº¿ç´¢ã€‚CTM-Adapterä»¥å› æœçš„æ–¹å¼å°†è¯´è¯è€…çš„è¾“å…¥æ•´åˆåˆ°è§†é¢‘ç”Ÿæˆè¿‡ç¨‹ä¸­ï¼Œä»¥ç¡®ä¿å¬ä¼—çš„å›åº”åœ¨æ—¶é—´ä¸Šè¿è´¯ã€‚å¯¹äºé•¿æ ¼å¼è§†é¢‘ç”Ÿæˆï¼Œæˆ‘ä»¬å¼•å…¥äº†DiTaiListener-Editï¼Œè¿™æ˜¯ä¸€ç§è¿‡æ¸¡ç»†åŒ–è§†é¢‘åˆ°è§†é¢‘æ‰©æ•£æ¨¡å‹ã€‚è¯¥æ¨¡å‹å°†è§†é¢‘ç‰‡æ®µèåˆæˆå¹³æ»‘ä¸”è¿ç»­çš„è§†é¢‘ï¼Œç¡®ä¿åœ¨åˆå¹¶ç”±DiTaiListener-Genäº§ç”Ÿçš„çŸ­ç‰‡æ—¶ï¼Œé¢éƒ¨è¡¨æƒ…å’Œå›¾åƒè´¨é‡çš„æ—¶åºä¸€è‡´æ€§ã€‚åœ¨åŸºå‡†æ•°æ®é›†ä¸Šï¼ŒDiTaiListeneråœ¨çœŸå®ä¸»ä¹‰å’Œè¿åŠ¨è¡¨ç¤ºæ–¹é¢éƒ½è¾¾åˆ°äº†æœ€æ–°æŠ€æœ¯æ°´å¹³ï¼Œå…¶ä¸­åœ¨RealTalkä¸Šçš„FIDå¢åŠ äº†73.8%ï¼Œåœ¨VICOä¸Šçš„FDæŒ‡æ ‡å¢åŠ äº†6.1%ã€‚ç”¨æˆ·ç ”ç©¶è¯å®äº†DiTaiListenerçš„å“è¶Šæ€§èƒ½ï¼Œåœ¨åé¦ˆã€å¤šæ ·æ€§å’Œå¹³æ»‘åº¦æ–¹é¢ï¼Œè¯¥æ¨¡å‹æ˜æ˜¾ä¼˜äºç«äº‰å¯¹æ‰‹ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.04010v1">PDF</a> Project page: <a target="_blank" rel="noopener" href="https://havent-invented.github.io/DiTaiListener">https://havent-invented.github.io/DiTaiListener</a></p>
<p><strong>Summary</strong><br>     è¿ªå¡”ä¼Šå€¾å¬è€…ï¼ˆDiTaiListenerï¼‰é€šè¿‡è§†é¢‘æ‰©æ•£æ¨¡å‹å’Œå¤šæ¨¡æ€æ¡ä»¶ï¼Œè§£å†³äº†ç”Ÿæˆè‡ªç„¶ä¸”å¯Œæœ‰ç»†å¾®å·®åˆ«çš„å¬è€…åŠ¨ä½œçš„é—®é¢˜ã€‚å®ƒé¦–å…ˆæ ¹æ®è¯´è¯è€…çš„è¯­éŸ³å’Œé¢éƒ¨åŠ¨ä½œç”Ÿæˆå¬è€…å“åº”çš„çŸ­ç‰‡æ®µï¼Œç„¶åé€šè¿‡è¿ªå¡”ä¼Šå€¾å¬è€…ç¼–è¾‘ï¼ˆDiTaiListener-Editï¼‰å¯¹è¿‡æ¸¡å¸§è¿›è¡Œç»†åŒ–ï¼Œä»¥å®ç°æ— ç¼è¿‡æ¸¡ã€‚è¯¥æ–¹æ³•é€šè¿‡å¼•å…¥æ‰©æ•£å˜å‹å™¨ï¼ˆDiTï¼‰å’Œå› æœæ—¶é—´å¤šæ¨¡æ€é€‚é…å™¨ï¼ˆCTM-Adapterï¼‰æ¥é€‚åº”å¬è€…å¤´éƒ¨è‚–åƒç”Ÿæˆçš„ä»»åŠ¡ã€‚CTM-Adapterä»¥å› æœæ–¹å¼å°†è¯´è¯è€…çš„å¬è§‰å’Œè§†è§‰çº¿ç´¢é›†æˆåˆ°è§†é¢‘ç”Ÿæˆè¿‡ç¨‹ä¸­ï¼Œç¡®ä¿å¬è€…å“åº”çš„æ—¶é—´è¿è´¯æ€§ã€‚å¯¹äºé•¿æ ¼å¼è§†é¢‘ç”Ÿæˆï¼Œå¼•å…¥äº†è¿ªå¡”ä¼Šå€¾å¬è€…ç¼–è¾‘ï¼ˆDiTaiListener-Editï¼‰çš„è¿‡æ¸¡ä¼˜åŒ–è§†é¢‘å¯¹è§†é¢‘æ‰©æ•£æ¨¡å‹ï¼Œè¯¥æ¨¡å‹å¯å°†è§†é¢‘ç‰‡æ®µèåˆæˆæµç•…è¿ç»­çš„è§†é¢‘ï¼Œç¡®ä¿é¢éƒ¨è¡¨è¾¾å’Œå›¾åƒè´¨é‡åœ¨åˆå¹¶ç”±DiTaiListener-Genäº§ç”Ÿçš„çŸ­è§†é¢‘ç‰‡æ®µæ—¶çš„æ—¶é—´ä¸€è‡´æ€§ã€‚å®šé‡è¯„ä¼°æ˜¾ç¤ºï¼ŒDiTaiListeneråœ¨åŸºå‡†æ•°æ®é›†ä¸Šå®ç°äº†æœ€ä½³æ€§èƒ½ï¼Œå¹¶åœ¨çœŸå®è°ˆè¯ï¼ˆRealTalkï¼‰ä¸Šçš„FIDå¢åŠ äº†73.8%ï¼Œåœ¨VICOä¸Šçš„FDæŒ‡æ ‡å¢åŠ äº†6.1%ã€‚ç”¨æˆ·ç ”ç©¶è¯å®äº†DiTaiListenerçš„å“è¶Šæ€§èƒ½ï¼Œåœ¨åé¦ˆã€å¤šæ ·æ€§å’Œå¹³æ»‘åº¦æ–¹é¢æ˜æ˜¾ä¼˜äºç«äº‰å¯¹æ‰‹ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>DiTaiListeneråˆ©ç”¨è§†é¢‘æ‰©æ•£æ¨¡å‹å’Œå¤šæ¨¡æ€æ¡ä»¶è§£å†³äº†ç”Ÿæˆè‡ªç„¶ä¸”å¯Œæœ‰ç»†å¾®å·®åˆ«çš„å¬è€…åŠ¨ä½œçš„é—®é¢˜ã€‚</li>
<li>DiTaiListeneré€šè¿‡ç”ŸæˆçŸ­ç‰‡æ®µçš„å¬è€…å“åº”ï¼Œå†å¯¹è¿‡æ¸¡å¸§è¿›è¡Œç»†åŒ–ï¼Œå®ç°æ— ç¼è¿‡æ¸¡ã€‚</li>
<li>å¼•å…¥äº†æ‰©æ•£å˜å‹å™¨ï¼ˆDiTï¼‰å’Œå› æœæ—¶é—´å¤šæ¨¡æ€é€‚é…å™¨ï¼ˆCTM-Adapterï¼‰æ¥é€‚åº”å¬è€…å¤´éƒ¨è‚–åƒç”Ÿæˆçš„ä»»åŠ¡ã€‚</li>
<li>CTM-Adapterä»¥å› æœæ–¹å¼å¤„ç†è¯´è¯è€…çš„å¬è§‰å’Œè§†è§‰çº¿ç´¢ï¼Œç¡®ä¿å¬è€…å“åº”çš„æ—¶é—´è¿è´¯æ€§ã€‚</li>
<li>DiTaiListenerç¼–è¾‘æ¨¡å‹èƒ½å¤Ÿèåˆè§†é¢‘ç‰‡æ®µï¼Œç”Ÿæˆæµç•…ä¸”è¿ç»­çš„è§†é¢‘ã€‚</li>
<li>DiTaiListeneråœ¨åŸºå‡†æ•°æ®é›†ä¸Šçš„æ€§èƒ½è¾¾åˆ°äº†å…ˆè¿›æ°´å¹³ï¼Œå¹¶åœ¨FIDå’ŒFDæŒ‡æ ‡ä¸Šå®ç°äº†æ˜¾è‘—çš„æå‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.04010">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-47a164ddf5c03f04a056d17b2634297f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e960cc4afc6fc4f81a1f7d8d42289545.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5bfe5c201c65518cc6258b67991c5491.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-afc434d373c73904ce3f6d1677d059f4.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-2ef6234af408608f33cc003c7769bf41.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-9d79ff31514dfddecc8c64712a51b06a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-60181c9c360a3a89ff5349aa9fd5844f.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="A-Hybrid-Wavelet-Fourier-Method-for-Next-Generation-Conditional-Diffusion-Models"><a href="#A-Hybrid-Wavelet-Fourier-Method-for-Next-Generation-Conditional-Diffusion-Models" class="headerlink" title="A Hybrid Wavelet-Fourier Method for Next-Generation Conditional   Diffusion Models"></a>A Hybrid Wavelet-Fourier Method for Next-Generation Conditional   Diffusion Models</h2><p><strong>Authors:Andrew Kiruluta, Andreas Lemos</strong></p>
<p>We present a novel generative modeling framework,Wavelet-Fourier-Diffusion, which adapts the diffusion paradigm to hybrid frequency representations in order to synthesize high-quality, high-fidelity images with improved spatial localization. In contrast to conventional diffusion models that rely exclusively on additive noise in pixel space, our approach leverages a multi-transform that combines wavelet sub-band decomposition with partial Fourier steps. This strategy progressively degrades and then reconstructs images in a hybrid spectral domain during the forward and reverse diffusion processes. By supplementing traditional Fourier-based analysis with the spatial localization capabilities of wavelets, our model can capture both global structures and fine-grained features more effectively. We further extend the approach to conditional image generation by integrating embeddings or conditional features via cross-attention. Experimental evaluations on CIFAR-10, CelebA-HQ, and a conditional ImageNet subset illustrate that our method achieves competitive or superior performance relative to baseline diffusion models and state-of-the-art GANs, as measured by Fr&#39;echet Inception Distance (FID) and Inception Score (IS). We also show how the hybrid frequency-based representation improves control over global coherence and fine texture synthesis, paving the way for new directions in multi-scale generative modeling. </p>
<blockquote>
<p>æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„ç”Ÿæˆæ¨¡å‹æ¡†æ¶â€”â€”Wavelet-Fourier-Diffusionï¼Œè¯¥æ¡†æ¶å°†æ‰©æ•£èŒƒå¼é€‚åº”äºæ··åˆé¢‘ç‡è¡¨ç¤ºï¼Œä»¥åˆæˆé«˜è´¨é‡ã€é«˜ä¿çœŸåº¦çš„å›¾åƒï¼Œå¹¶æ”¹è¿›ç©ºé—´å®šä½ã€‚ä¸ä¼ ç»Ÿçš„ä»…ä¾èµ–äºåƒç´ ç©ºé—´æ·»åŠ å™ªå£°çš„æ‰©æ•£æ¨¡å‹ä¸åŒï¼Œæˆ‘ä»¬çš„æ–¹æ³•åˆ©ç”¨äº†ä¸€ç§å¤šè½¬æ¢æŠ€æœ¯ï¼Œç»“åˆäº†å°æ³¢å­å¸¦åˆ†è§£å’Œéƒ¨åˆ†å‚…é‡Œå¶æ­¥éª¤ã€‚è¯¥ç­–ç•¥åœ¨æ­£å‘å’Œåå‘æ‰©æ•£è¿‡ç¨‹ä¸­ï¼Œåœ¨æ··åˆå…‰è°±åŸŸä¸­é€æ­¥é™è§£ç„¶åé‡å»ºå›¾åƒã€‚é€šè¿‡ç»“åˆä¼ ç»ŸåŸºäºå‚…é‡Œå¶çš„åˆ†æä¸å°æ³¢çš„ç©ºé—´å®šä½èƒ½åŠ›ï¼Œæˆ‘ä»¬çš„æ¨¡å‹èƒ½å¤Ÿæ›´æœ‰æ•ˆåœ°æ•æ‰å…¨å±€ç»“æ„å’Œç²¾ç»†ç‰¹å¾ã€‚æˆ‘ä»¬é€šè¿‡é›†æˆåµŒå…¥æˆ–é€šè¿‡äº¤å‰æ³¨æ„åŠ›æ·»åŠ æ¡ä»¶ç‰¹å¾ï¼Œå°†è¯¥æ–¹æ³•è¿›ä¸€æ­¥æ‰©å±•åˆ°æ¡ä»¶å›¾åƒç”Ÿæˆã€‚åœ¨CIFAR-10ã€CelebA-HQå’Œæ¡ä»¶ImageNetå­é›†ä¸Šçš„å®éªŒè¯„ä¼°è¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨FrÃ©chet Inception Distanceï¼ˆFIDï¼‰å’ŒInception Scoreï¼ˆISï¼‰çš„è¡¡é‡ä¸‹ï¼Œç›¸è¾ƒäºåŸºå‡†æ‰©æ•£æ¨¡å‹å’Œæœ€å…ˆè¿›çš„GANsï¼Œè¾¾åˆ°äº†ç«äº‰æˆ–æ›´ä¼˜çš„æ€§èƒ½ã€‚æˆ‘ä»¬è¿˜å±•ç¤ºäº†åŸºäºæ··åˆé¢‘ç‡çš„è¡¨ç¤ºå¦‚ä½•æ”¹å–„å…¨å±€ä¸€è‡´æ€§å’Œç²¾ç»†çº¹ç†åˆæˆçš„æ§åˆ¶ï¼Œä¸ºå¤šå°ºåº¦ç”Ÿæˆæ¨¡å‹çš„æ–°æ–¹å‘é“ºå¹³äº†é“è·¯ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.03821v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°å‹ç”Ÿæˆå»ºæ¨¡æ¡†æ¶â€”â€”Wavelet-Fourier-Diffusionï¼Œè¯¥æ¡†æ¶ç»“åˆäº†å°æ³¢å’Œå‚…é‡Œå¶å˜æ¢çš„å¤šé‡è½¬æ¢ï¼Œå¯¹æ‰©æ•£æ¨¡å‹è¿›è¡Œäº†æ”¹è¿›ï¼Œä»¥åœ¨æ··åˆé¢‘ç‡è¡¨ç¤ºä¸­ç”Ÿæˆé«˜è´¨é‡ã€é«˜ä¿çœŸåº¦çš„å›¾åƒï¼Œå¹¶æ”¹å–„äº†ç©ºé—´å®šä½èƒ½åŠ›ã€‚è¯¥æ–¹æ³•åœ¨æ‰©æ•£çš„æ­£åè¿‡ç¨‹ä¸­ï¼Œåœ¨æ··åˆå…‰è°±åŸŸä¸­é€æ­¥é™è§£å¹¶é‡å»ºå›¾åƒï¼ŒåŒæ—¶é€šè¿‡è¡¥å……åŸºäºå‚…ç«‹å¶çš„ä¼ ç»Ÿåˆ†æä¸å°æ³¢çš„ç©ºé—´å®šä½èƒ½åŠ›ï¼Œæ›´æœ‰æ•ˆåœ°æ•æ‰å…¨å±€ç»“æ„å’Œç²¾ç»†ç‰¹å¾ã€‚å®éªŒè¯„ä¼°è¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨CIFAR-10ã€CelebA-HQå’Œæ¡ä»¶ImageNetå­é›†ä¸Šçš„æ€§èƒ½ä¸åŸºå‡†æ‰©æ•£æ¨¡å‹å’Œæœ€å…ˆè¿›çš„GANå…·æœ‰ç«äº‰åŠ›æˆ–æ›´ä¼˜è¶Šï¼Œå¹¶å±•ç¤ºäº†æ··åˆé¢‘ç‡è¡¨ç¤ºæ³•å¯¹æé«˜å…¨å±€ä¸€è‡´æ€§å’Œç²¾ç»†çº¹ç†åˆæˆçš„æ§åˆ¶åŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ä»‹ç»äº†Wavelet-Fourier-Diffusionç”Ÿæˆå»ºæ¨¡æ¡†æ¶ï¼Œç»“åˆäº†æ‰©æ•£æ¨¡å‹å’Œå°æ³¢å‚…ç«‹å¶å˜æ¢çš„å¤šé‡è½¬æ¢ã€‚</li>
<li>è¯¥æ–¹æ³•æ”¹è¿›äº†æ‰©æ•£æ¨¡å‹ï¼Œä»¥åœ¨æ··åˆé¢‘ç‡è¡¨ç¤ºä¸­ç”Ÿæˆé«˜è´¨é‡ã€é«˜ä¿çœŸåº¦çš„å›¾åƒã€‚</li>
<li>æ–¹æ³•ç»“åˆäº†å‚…ç«‹å¶å˜æ¢çš„ä¼ ç»Ÿåˆ†æä¸å°æ³¢çš„ç©ºé—´å®šä½èƒ½åŠ›ï¼Œæé«˜å›¾åƒåˆæˆè´¨é‡ã€‚</li>
<li>é€šè¿‡é€æ­¥é™è§£å¹¶é‡å»ºå›¾åƒçš„è¿‡ç¨‹ï¼Œå®ç°äº†åœ¨æ··åˆå…‰è°±åŸŸä¸­çš„å›¾åƒåˆæˆã€‚</li>
<li>æ–¹æ³•å¯æ‰©å±•åˆ°æ¡ä»¶å›¾åƒç”Ÿæˆï¼Œé€šè¿‡é›†æˆåµŒå…¥æˆ–æ¡ä»¶ç‰¹å¾è¿›è¡Œäº¤å‰æ³¨æ„åŠ›ã€‚</li>
<li>å®éªŒè¯„ä¼°è¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šçš„æ€§èƒ½ä¸æœ€æ–°æŠ€æœ¯ç›¸æ¯”å…·æœ‰ç«äº‰åŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.03821">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-89bf01c8ff44b5321a00faf9fb68df0c.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-09738a779e6bc3acd2122b351152c8c7.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Safeguarding-Vision-Language-Models-Mitigating-Vulnerabilities-to-Gaussian-Noise-in-Perturbation-based-Attacks"><a href="#Safeguarding-Vision-Language-Models-Mitigating-Vulnerabilities-to-Gaussian-Noise-in-Perturbation-based-Attacks" class="headerlink" title="Safeguarding Vision-Language Models: Mitigating Vulnerabilities to   Gaussian Noise in Perturbation-based Attacks"></a>Safeguarding Vision-Language Models: Mitigating Vulnerabilities to   Gaussian Noise in Perturbation-based Attacks</h2><p><strong>Authors:Jiawei Wang, Yushen Zuo, Yuanjun Chai, Zhendong Liu, Yicheng Fu, Yichun Feng, Kin-Man Lam</strong></p>
<p>Vision-Language Models (VLMs) extend the capabilities of Large Language Models (LLMs) by incorporating visual information, yet they remain vulnerable to jailbreak attacks, especially when processing noisy or corrupted images. Although existing VLMs adopt security measures during training to mitigate such attacks, vulnerabilities associated with noise-augmented visual inputs are overlooked. In this work, we identify that missing noise-augmented training causes critical security gaps: many VLMs are susceptible to even simple perturbations such as Gaussian noise. To address this challenge, we propose Robust-VLGuard, a multimodal safety dataset with aligned &#x2F; misaligned image-text pairs, combined with noise-augmented fine-tuning that reduces attack success rates while preserving functionality of VLM. For stronger optimization-based visual perturbation attacks, we propose DiffPure-VLM, leveraging diffusion models to convert adversarial perturbations into Gaussian-like noise, which can be defended by VLMs with noise-augmented safety fine-tuning. Experimental results demonstrate that the distribution-shifting property of diffusion model aligns well with our fine-tuned VLMs, significantly mitigating adversarial perturbations across varying intensities. The dataset and code are available at <a target="_blank" rel="noopener" href="https://github.com/JarvisUSTC/DiffPure-RobustVLM">https://github.com/JarvisUSTC/DiffPure-RobustVLM</a>. </p>
<blockquote>
<p>è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰é€šè¿‡èå…¥è§†è§‰ä¿¡æ¯æ‰©å±•äº†å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„åŠŸèƒ½ï¼Œä½†å®ƒä»¬åœ¨å¤„ç†å™ªå£°æˆ–æŸåçš„å›¾åƒæ—¶ä»ç„¶å®¹æ˜“å—åˆ°æ–­ç‚¹æ”»å‡»ã€‚å°½ç®¡ç°æœ‰çš„VLMsåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­é‡‡å–äº†å®‰å…¨æªæ–½æ¥å‡è½»è¿™ç±»æ”»å‡»ï¼Œä½†ä¸å¢å¼ºå™ªå£°è§†è§‰è¾“å…¥ç›¸å…³çš„æ¼æ´å´è¢«å¿½è§†äº†ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬å‘ç°ç¼ºå°‘å™ªå£°å¢å¼ºè®­ç»ƒä¼šå¯¼è‡´å…³é”®çš„å®‰å…¨æ¼æ´ï¼šè®¸å¤šVLMsç”šè‡³å®¹æ˜“å—åˆ°é«˜æ–¯å™ªå£°ç­‰ç®€å•å¹²æ‰°çš„å½±å“ã€‚ä¸ºäº†åº”å¯¹è¿™ä¸€æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†Robust-VLGuardï¼Œè¿™æ˜¯ä¸€ä¸ªå¤šæ¨¡å¼å®‰å…¨æ•°æ®é›†ï¼ŒåŒ…å«å¯¹é½&#x2F;æœªå¯¹é½çš„å›¾åƒæ–‡æœ¬å¯¹ï¼Œç»“åˆå™ªå£°å¢å¼ºå¾®è°ƒï¼Œé™ä½äº†æ”»å‡»æˆåŠŸç‡ï¼ŒåŒæ—¶ä¿æŒäº†VLMçš„åŠŸèƒ½æ€§ã€‚å¯¹äºåŸºäºæ›´å¼ºä¼˜åŒ–çš„è§†è§‰æ‰°åŠ¨æ”»å‡»ï¼Œæˆ‘ä»¬æå‡ºäº†DiffPure-VLMï¼Œåˆ©ç”¨æ‰©æ•£æ¨¡å‹å°†å¯¹æŠ—æ€§æ‰°åŠ¨è½¬åŒ–ä¸ºé«˜æ–¯å‹å™ªå£°ï¼Œé€šè¿‡å¸¦æœ‰å™ªå£°å¢å¼ºå®‰å…¨å¾®è°ƒçš„VLMè¿›è¡Œé˜²å¾¡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæ‰©æ•£æ¨¡å‹çš„åˆ†å¸ƒè½¬ç§»å±æ€§ä¸æˆ‘ä»¬çš„å¾®è°ƒVLMséå¸¸å¥‘åˆï¼Œèƒ½åœ¨ä¸åŒå¼ºåº¦ä¸‹æ˜¾è‘—å‡è½»å¯¹æŠ—æ€§æ‰°åŠ¨ã€‚æ•°æ®é›†å’Œä»£ç å¯é€šè¿‡<a target="_blank" rel="noopener" href="https://github.com/JarvisUSTC/DiffPure-RobustVLM%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/JarvisUSTC/DiffPure-RobustVLMè·å–ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.01308v2">PDF</a> </p>
<p><strong>Summary</strong><br>     è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰é€šè¿‡ç»“åˆè§†è§‰ä¿¡æ¯æ‰©å±•äº†å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„åŠŸèƒ½ï¼Œä½†å®ƒä»¬ä»æ˜“å—æ”»å‡»ï¼Œç‰¹åˆ«æ˜¯åœ¨å¤„ç†å™ªå£°æˆ–æŸåçš„å›¾åƒæ—¶ã€‚å°½ç®¡ç°æœ‰VLMåœ¨è®­ç»ƒæœŸé—´é‡‡å–å®‰å…¨æªæ–½ä»¥å‡è½»è¿™äº›æ”»å‡»ï¼Œä½†é’ˆå¯¹å¢å¼ºå™ªå£°çš„è§†è§‰è¾“å…¥çš„æ¼æ´å´è¢«å¿½è§†ã€‚æœ¬æ–‡æŒ‡å‡ºï¼Œç¼ºå°‘å™ªå£°å¢å¼ºè®­ç»ƒä¼šå¯¼è‡´å…³é”®çš„å®‰å…¨æ¼æ´ï¼Œè®¸å¤šVLMå®¹æ˜“å—åˆ°å¦‚é«˜æ–¯å™ªå£°ç­‰ç®€å•æ‰°åŠ¨çš„å½±å“ã€‚ä¸ºè§£å†³æ­¤é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†Robust-VLGuardï¼Œè¿™æ˜¯ä¸€ä¸ªå¤šæ¨¡å¼å®‰å…¨æ•°æ®é›†ï¼ŒåŒ…å«å¯¹é½&#x2F;æœªå¯¹é½çš„å›¾åƒæ–‡æœ¬å¯¹ï¼Œç»“åˆå™ªå£°å¢å¼ºå¾®è°ƒï¼Œé™ä½äº†æ”»å‡»æˆåŠŸç‡ï¼ŒåŒæ—¶ä¿æŒäº†VLMçš„åŠŸèƒ½æ€§ã€‚é’ˆå¯¹æ›´ä¼˜åŒ–çš„è§†è§‰æ‰°åŠ¨æ”»å‡»ï¼Œæˆ‘ä»¬æå‡ºäº†DiffPure-VLMï¼Œåˆ©ç”¨æ‰©æ•£æ¨¡å‹å°†å¯¹æŠ—æ€§æ‰°åŠ¨è½¬åŒ–ä¸ºç±»ä¼¼é«˜æ–¯å™ªå£°çš„å½¢å¼ï¼Œå¯é€šè¿‡å…·æœ‰å™ªå£°å¢å¼ºå®‰å…¨å¾®è°ƒçš„VLMè¿›è¡Œé˜²å¾¡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæ‰©æ•£æ¨¡å‹çš„åˆ†å¸ƒè¿ç§»å±æ€§ä¸æˆ‘ä»¬çš„å¾®è°ƒVLMsç›¸å»åˆï¼Œå¯åœ¨ä¸åŒå¼ºåº¦ä¸‹æ˜¾è‘—å‡è½»å¯¹æŠ—æ€§æ‰°åŠ¨ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>VLMsè™½ç„¶åœ¨å¤§è¯­è¨€æ¨¡å‹çš„åŸºç¡€ä¸Šç»“åˆäº†è§†è§‰ä¿¡æ¯ï¼Œæå‡äº†åŠŸèƒ½ï¼Œä½†åœ¨å¤„ç†å™ªå£°æˆ–æŸåå›¾åƒæ—¶æ˜“å—æ”»å‡»ã€‚</li>
<li>ç°æœ‰VLMè®­ç»ƒä¸­çš„å®‰å…¨æªæ–½ä¸»è¦å…³æ³¨å¯¹æŠ—æ€§æ”»å‡»ï¼Œä½†å¿½è§†äº†å¯¹å¢å¼ºå™ªå£°çš„è§†è§‰è¾“å…¥çš„é˜²å¾¡ã€‚</li>
<li>ç¼ºä¹å™ªå£°å¢å¼ºè®­ç»ƒä¼šå¯¼è‡´VLMçš„å®‰å…¨æ¼æ´ï¼Œä½¿å…¶å®¹æ˜“å—åˆ°ç®€å•æ‰°åŠ¨ï¼ˆå¦‚é«˜æ–¯å™ªå£°ï¼‰çš„å½±å“ã€‚</li>
<li>æå‡ºäº†ä¸€ç§æ–°çš„å¤šæ¨¡å¼å®‰å…¨æ•°æ®é›†Robust-VLGuardï¼ŒåŒ…å«å¯¹é½&#x2F;æœªå¯¹é½çš„å›¾åƒæ–‡æœ¬å¯¹ï¼Œä»¥æé«˜VLMå¯¹å™ªå£°çš„é²æ£’æ€§ã€‚</li>
<li>é’ˆå¯¹ä¼˜åŒ–åçš„è§†è§‰æ‰°åŠ¨æ”»å‡»ï¼Œåˆ©ç”¨æ‰©æ•£æ¨¡å‹æå‡ºäº†DiffPure-VLMæ–¹æ³•ï¼Œå°†å¯¹æŠ—æ€§æ‰°åŠ¨è½¬åŒ–ä¸ºç±»ä¼¼é«˜æ–¯å™ªå£°çš„å½¢å¼ã€‚</li>
<li>é€šè¿‡å™ªå£°å¢å¼ºå®‰å…¨å¾®è°ƒçš„VLMå¯ä»¥æœ‰æ•ˆé˜²å¾¡è¿™ç§è½¬åŒ–åçš„å™ªå£°ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.01308">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-6f05557e711e78bd2af38971d48d2369.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a559cc1bf6190fd25d58077a4bb91799.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-977ee4c475d9e8074b607f8e6ca2d4b9.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-81a090bf71f2060ad43fda7c490b5e5a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-49923dfda67b907568219914f9e733cb.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="FairDiffusion-Enhancing-Equity-in-Latent-Diffusion-Models-via-Fair-Bayesian-Perturbation"><a href="#FairDiffusion-Enhancing-Equity-in-Latent-Diffusion-Models-via-Fair-Bayesian-Perturbation" class="headerlink" title="FairDiffusion: Enhancing Equity in Latent Diffusion Models via Fair   Bayesian Perturbation"></a>FairDiffusion: Enhancing Equity in Latent Diffusion Models via Fair   Bayesian Perturbation</h2><p><strong>Authors:Yan Luo, Muhammad Osama Khan, Congcong Wen, Muhammad Muneeb Afzal, Titus Fidelis Wuermeling, Min Shi, Yu Tian, Yi Fang, Mengyu Wang</strong></p>
<p>Recent progress in generative AI, especially diffusion models, has demonstrated significant utility in text-to-image synthesis. Particularly in healthcare, these models offer immense potential in generating synthetic datasets and training medical students. However, despite these strong performances, it remains uncertain if the image generation quality is consistent across different demographic subgroups. To address this critical concern, we present the first comprehensive study on the fairness of medical text-to-image diffusion models. Our extensive evaluations of the popular Stable Diffusion model reveal significant disparities across gender, race, and ethnicity. To mitigate these biases, we introduce FairDiffusion, an equity-aware latent diffusion model that enhances fairness in both image generation quality as well as the semantic correlation of clinical features. In addition, we also design and curate FairGenMed, the first dataset for studying the fairness of medical generative models. Complementing this effort, we further evaluate FairDiffusion on two widely-used external medical datasets: HAM10000 (dermatoscopic images) and CheXpert (chest X-rays) to demonstrate FairDiffusionâ€™s effectiveness in addressing fairness concerns across diverse medical imaging modalities. Together, FairDiffusion and FairGenMed significantly advance research in fair generative learning, promoting equitable benefits of generative AI in healthcare. </p>
<blockquote>
<p>è¿‘å¹´æ¥ï¼Œç”Ÿæˆå¼äººå·¥æ™ºèƒ½ï¼Œå°¤å…¶æ˜¯æ‰©æ•£æ¨¡å‹ï¼Œåœ¨æ–‡æœ¬åˆ°å›¾åƒåˆæˆæ–¹é¢å–å¾—äº†æ˜¾è‘—è¿›å±•ã€‚ç‰¹åˆ«æ˜¯åœ¨åŒ»ç–—é¢†åŸŸï¼Œè¿™äº›æ¨¡å‹åœ¨ç”Ÿæˆåˆæˆæ•°æ®é›†å’ŒåŸ¹è®­åŒ»å­¦ç”Ÿæ–¹é¢æ˜¾ç¤ºå‡ºå·¨å¤§çš„æ½œåŠ›ã€‚ç„¶è€Œï¼Œå°½ç®¡è¿™äº›è¡¨ç°å¾ˆå‡ºè‰²ï¼Œä½†ä¸åŒäººå£äºšç»„çš„å›¾åƒç”Ÿæˆè´¨é‡æ˜¯å¦ä¸€è‡´ä»ç„¶ä¸ç¡®å®šã€‚ä¸ºäº†è§£å†³è¿™ä¸€å…³é”®å…³åˆ‡é—®é¢˜ï¼Œæˆ‘ä»¬é¦–æ¬¡å¯¹åŒ»ç–—æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹çš„å…¬å¹³æ€§è¿›è¡Œäº†å…¨é¢ç ”ç©¶ã€‚æˆ‘ä»¬å¯¹æµè¡Œçš„Stable Diffusionæ¨¡å‹çš„å¹¿æ³›è¯„ä¼°æ­ç¤ºäº†æ€§åˆ«ã€ç§æ—å’Œæ°‘æ—ä¹‹é—´çš„æ˜¾è‘—å·®å¼‚ã€‚ä¸ºäº†ç¼“è§£è¿™äº›åè§ï¼Œæˆ‘ä»¬å¼•å…¥äº†FairDiffusionï¼Œè¿™æ˜¯ä¸€ä¸ªæ³¨é‡å…¬å¹³æ€§çš„æ½œåœ¨æ‰©æ•£æ¨¡å‹ï¼Œæ—¨åœ¨æé«˜å›¾åƒç”Ÿæˆè´¨é‡å’Œä¸´åºŠç‰¹å¾çš„è¯­ä¹‰ç›¸å…³æ€§æ–¹é¢çš„å…¬å¹³æ€§ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜è®¾è®¡å’Œæ•´ç†äº†FairGenMedï¼Œè¿™æ˜¯é¦–ä¸ªç”¨äºç ”ç©¶åŒ»ç–—ç”Ÿæˆæ¨¡å‹å…¬å¹³æ€§çš„æ•°æ®é›†ã€‚ä½œä¸ºè¡¥å……ï¼Œæˆ‘ä»¬è¿›ä¸€æ­¥åœ¨ä¸¤ä¸ªå¹¿æ³›ä½¿ç”¨çš„å¤–éƒ¨åŒ»ç–—æ•°æ®é›†ä¸Šå¯¹FairDiffusionè¿›è¡Œäº†è¯„ä¼°ï¼šHAM10000ï¼ˆçš®è‚¤ç§‘å›¾åƒï¼‰å’ŒCheXpertï¼ˆèƒ¸éƒ¨Xå°„çº¿ï¼‰ï¼Œä»¥è¯æ˜FairDiffusionåœ¨ä¸åŒåŒ»ç–—æˆåƒæ¨¡æ€ä¸Šè§£å†³å…¬å¹³æ€§é—®é¢˜çš„æœ‰æ•ˆæ€§ã€‚æ€»ä¹‹ï¼ŒFairDiffusionå’ŒFairGenMedçš„æ¨å‡ºï¼Œæ˜¾è‘—æ¨åŠ¨äº†å…¬å¹³ç”Ÿæˆå­¦ä¹ é¢†åŸŸçš„ç ”ç©¶ï¼Œä¿ƒè¿›äº†ç”Ÿæˆäººå·¥æ™ºèƒ½åœ¨åŒ»ç–—é¢†åŸŸçš„å…¬å¹³å—ç›Šã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.20374v2">PDF</a> Published in Science Advances   (<a target="_blank" rel="noopener" href="https://www.science.org/doi/full/10.1126/sciadv.ads4593">https://www.science.org/doi/full/10.1126/sciadv.ads4593</a>). The data and code   are made publicly available at   <a target="_blank" rel="noopener" href="https://github.com/Harvard-Ophthalmology-AI-Lab/FairDiffusion">https://github.com/Harvard-Ophthalmology-AI-Lab/FairDiffusion</a></p>
<p><strong>Summary</strong></p>
<p>æ‰©æ•£æ¨¡å‹åœ¨ç”ŸæˆåŒ»ç–—å›¾åƒæ–¹é¢æ˜¾ç¤ºå‡ºå·¨å¤§çš„æ½œåŠ›ï¼Œä½†åœ¨ä¸åŒäººç¾¤ä¸­ç”Ÿæˆå›¾åƒè´¨é‡çš„ä¸€è‡´æ€§æ–¹é¢ä»å­˜åœ¨ä¸ç¡®å®šæ€§ã€‚ä¸€é¡¹æ–°ç ”ç©¶é¦–æ¬¡å…¨é¢ç ”ç©¶äº†åŒ»ç–—æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹çš„å…¬å¹³æ€§ï¼Œå‘ç°æµè¡Œçš„Stable Diffusionæ¨¡å‹åœ¨æ€§åˆ«ã€ç§æ—å’Œæ°‘æ—æ–¹é¢å­˜åœ¨æ˜¾è‘—å·®å¼‚ã€‚ä¸ºè§£å†³è¿™äº›é—®é¢˜ï¼Œç ”ç©¶å›¢é˜Ÿæ¨å‡ºäº†FairDiffusionæ¨¡å‹å’ŒFairGenMedæ•°æ®é›†ï¼Œæ—¨åœ¨æé«˜å›¾åƒç”Ÿæˆè´¨é‡å’Œä¸´åºŠç‰¹å¾è¯­ä¹‰ç›¸å…³æ€§æ–¹é¢çš„å…¬å¹³æ€§ã€‚åœ¨å¤–éƒ¨åŒ»å­¦æ•°æ®é›†HAM10000å’ŒCheXpertä¸Šçš„è¯„ä¼°ç»“æœè¡¨æ˜ï¼ŒFairDiffusionèƒ½å¤Ÿæœ‰æ•ˆè§£å†³è·¨ä¸åŒåŒ»å­¦æˆåƒæ¨¡æ€çš„å…¬å¹³æ€§é—®é¢˜ã€‚FairDiffusionå’ŒFairGenMedçš„æ¨å‡ºå¯¹äºå…¬å¹³ç”Ÿæˆå­¦ä¹ é¢†åŸŸçš„ç ”ç©¶å…·æœ‰é‡å¤§æ„ä¹‰ï¼Œä¿ƒè¿›äº†äººå·¥æ™ºèƒ½åœ¨åŒ»ç–—ä¿å¥é¢†åŸŸçš„å…¬å¹³å—ç›Šã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ‰©æ•£æ¨¡å‹åœ¨æ–‡æœ¬åˆ°å›¾åƒåˆæˆä¸­å±•ç°å‡ºæ˜¾è‘—ä¼˜åŠ¿ï¼Œç‰¹åˆ«æ˜¯åœ¨åŒ»ç–—é¢†åŸŸã€‚</li>
<li>ç°æœ‰æ‰©æ•£æ¨¡å‹åœ¨ä¸åŒäººç¾¤ä¸­ç”Ÿæˆå›¾åƒè´¨é‡çš„ä¸€è‡´æ€§æ–¹é¢å­˜åœ¨é—®é¢˜ã€‚</li>
<li>ç ”ç©¶é¦–æ¬¡å…¨é¢ç ”ç©¶äº†åŒ»ç–—æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹çš„å…¬å¹³æ€§ã€‚</li>
<li>å‘ç°æµè¡Œçš„Stable Diffusionæ¨¡å‹åœ¨æ€§åˆ«ã€ç§æ—å’Œæ°‘æ—æ–¹é¢å­˜åœ¨æ˜¾è‘—å·®å¼‚ã€‚</li>
<li>ä¸ºè§£å†³å…¬å¹³æ€§é—®é¢˜ï¼Œæ¨å‡ºäº†FairDiffusionæ¨¡å‹å’ŒFairGenMedæ•°æ®é›†ã€‚</li>
<li>FairDiffusionæ¨¡å‹æ—¨åœ¨æé«˜å›¾åƒç”Ÿæˆè´¨é‡å’Œä¸´åºŠç‰¹å¾è¯­ä¹‰ç›¸å…³æ€§æ–¹é¢çš„å…¬å¹³æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.20374">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-be714ef7f4afc6aebe6679d0bc18471f.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Pattern-Analogies-Learning-to-Perform-Programmatic-Image-Edits-by-Analogy"><a href="#Pattern-Analogies-Learning-to-Perform-Programmatic-Image-Edits-by-Analogy" class="headerlink" title="Pattern Analogies: Learning to Perform Programmatic Image Edits by   Analogy"></a>Pattern Analogies: Learning to Perform Programmatic Image Edits by   Analogy</h2><p><strong>Authors:Aditya Ganeshan, Thibault Groueix, Paul Guerrero, RadomÃ­r MÄ›ch, Matthew Fisher, Daniel Ritchie</strong></p>
<p>Pattern images are everywhere in the digital and physical worlds, and tools to edit them are valuable. But editing pattern images is tricky: desired edits are often programmatic: structure-aware edits that alter the underlying program which generates the pattern. One could attempt to infer this underlying program, but current methods for doing so struggle with complex images and produce unorganized programs that make editing tedious. In this work, we introduce a novel approach to perform programmatic edits on pattern images. By using a pattern analogy â€“ a pair of simple patterns to demonstrate the intended edit â€“ and a learning-based generative model to execute these edits, our method allows users to intuitively edit patterns. To enable this paradigm, we introduce SplitWeave, a domain-specific language that, combined with a framework for sampling synthetic pattern analogies, enables the creation of a large, high-quality synthetic training dataset. We also present TriFuser, a Latent Diffusion Model (LDM) designed to overcome critical issues that arise when naively deploying LDMs to this task. Extensive experiments on real-world, artist-sourced patterns reveals that our method faithfully performs the demonstrated edit while also generalizing to related pattern styles beyond its training distribution. </p>
<blockquote>
<p>åœ¨æ•°å­—ä¸–ç•Œå’Œç‰©ç†ä¸–ç•Œä¸­ï¼Œå›¾æ¡ˆå›¾åƒæ— å¤„ä¸åœ¨ï¼Œç¼–è¾‘å®ƒä»¬çš„å·¥å…·ä¹Ÿæå…·ä»·å€¼ã€‚ä½†ç¼–è¾‘å›¾æ¡ˆå›¾åƒæ˜¯æœ‰æŠ€å·§çš„ï¼šæœŸæœ›çš„ç¼–è¾‘é€šå¸¸æ˜¯ç¨‹åºåŒ–çš„ï¼šæ”¹å˜ç”Ÿæˆå›¾æ¡ˆçš„åŸºç¡€ç¨‹åºçš„ç»“æ„æ„ŸçŸ¥ç¼–è¾‘ã€‚äººä»¬å¯ä»¥å°è¯•æ¨æ–­è¿™ä¸ªåŸºç¡€ç¨‹åºï¼Œä½†å½“å‰çš„æ–¹æ³•åœ¨å¤„ç†å¤æ‚å›¾åƒæ—¶é‡åˆ°äº†å›°éš¾ï¼Œäº§ç”Ÿçš„ç¨‹åºç»„ç»‡æ— åºï¼Œä½¿å¾—ç¼–è¾‘å˜å¾—ä¹å‘³ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†ä¸€ç§å¯¹å›¾æ¡ˆå›¾åƒè¿›è¡Œç¨‹åºåŒ–ç¼–è¾‘çš„æ–°æ–¹æ³•ã€‚é€šè¿‡ä½¿ç”¨å›¾æ¡ˆç±»æ¯”ï¼ˆä¸€å¯¹ç®€å•çš„å›¾æ¡ˆæ¥å±•ç¤ºæ‰€éœ€çš„ç¼–è¾‘ï¼‰å’ŒåŸºäºå­¦ä¹ çš„ç”Ÿæˆæ¨¡å‹æ¥æ‰§è¡Œè¿™äº›ç¼–è¾‘ï¼Œæˆ‘ä»¬çš„æ–¹æ³•å…è®¸ç”¨æˆ·ç›´è§‚åœ°ç¼–è¾‘å›¾æ¡ˆã€‚ä¸ºäº†å®ç°è¿™ä¸€èŒƒå¼ï¼Œæˆ‘ä»¬å¼•å…¥äº†SplitWeaveï¼Œä¸€ç§ç‰¹å®šé¢†åŸŸçš„è¯­è¨€ï¼Œç»“åˆé‡‡æ ·åˆæˆå›¾æ¡ˆç±»æ¯”çš„æ¡†æ¶ï¼Œå¯ä»¥åˆ›å»ºå¤§è§„æ¨¡ã€é«˜è´¨é‡åˆæˆè®­ç»ƒæ•°æ®é›†ã€‚æˆ‘ä»¬è¿˜æå‡ºäº†TriFuserï¼Œå®ƒæ˜¯ä¸€ç§æ½œåœ¨æ‰©æ•£æ¨¡å‹ï¼ˆLatent Diffusion Modelï¼Œç®€ç§°LDMï¼‰ï¼Œæ—¨åœ¨å…‹æœåœ¨å°†æ­¤ä»»åŠ¡ç›´æ¥éƒ¨ç½²åˆ°LDMæ—¶å‡ºç°çš„å…³é”®é—®é¢˜ã€‚åœ¨çœŸå®ä¸–ç•Œå’Œè‰ºæœ¯å®¶æ¥æºçš„å›¾æ¡ˆä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•èƒ½å¤Ÿå¿ å®æ‰§è¡Œå±•ç¤ºçš„ç¼–è¾‘ï¼Œå¹¶æ¨å¹¿åˆ°å…¶è®­ç»ƒåˆ†å¸ƒä¹‹å¤–çš„ç›¸å…³å›¾æ¡ˆé£æ ¼ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.12463v2">PDF</a> CVPR 2024 - Website: <a target="_blank" rel="noopener" href="https://bardofcodes.github.io/patterns/">https://bardofcodes.github.io/patterns/</a></p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åŸºäºæ¨¡å¼ç±»æ¯”å’Œç”Ÿæˆæ¨¡å‹çš„æ–¹æ³•ï¼Œç”¨äºå¯¹å›¾æ¡ˆå›¾åƒè¿›è¡Œç¨‹å¼åŒ–ç¼–è¾‘ã€‚é€šè¿‡åˆ©ç”¨ç®€å•çš„æ¨¡å¼ç±»æ¯”æ¥å±•ç¤ºé¢„æœŸçš„ç¼–è¾‘ï¼Œå¹¶ç»“åˆå­¦ä¹ ç”Ÿæˆæ¨¡å‹æ¥æ‰§è¡Œè¿™äº›ç¼–è¾‘ï¼Œè¯¥æ–¹æ³•ä½¿ç”¨æˆ·èƒ½å¤Ÿç›´è§‚åœ°ç¼–è¾‘å›¾æ¡ˆã€‚ä¸ºè§£å†³è¿™ä¸€ä»»åŠ¡ï¼Œç ”ç©¶å›¢é˜Ÿå¼•å…¥äº†SplitWeaveè¿™ä¸€é¢†åŸŸç‰¹å®šè¯­è¨€å’Œæ¡†æ¶æ¥é‡‡æ ·åˆæˆæ¨¡å¼ç±»æ¯”ï¼Œå¹¶è®¾è®¡äº†TriFuseræ½œæ‰©æ•£æ¨¡å‹ï¼ˆLatent Diffusion Modelï¼ŒLDMï¼‰ä»¥å…‹æœåœ¨éƒ¨ç½²è¿‡ç¨‹ä¸­å‡ºç°çš„é—®é¢˜ã€‚ç»è¿‡å¯¹çœŸå®ä¸–ç•Œè‰ºæœ¯å®¶æ¥æºå›¾æ¡ˆçš„å¹¿æ³›å®éªŒï¼Œè¯¥æ–¹æ³•ä¸ä»…èƒ½å¤Ÿå¿ å®æ‰§è¡Œå±•ç¤ºçš„ç¼–è¾‘æ“ä½œï¼Œè¿˜èƒ½æ¨å¹¿åˆ°è®­ç»ƒåˆ†å¸ƒä¹‹å¤–çš„ç›¸ä¼¼å›¾æ¡ˆé£æ ¼ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å›¾æ¡ˆå›¾åƒåœ¨æ•°å­—ä¸ç‰©ç†ä¸–ç•Œä¸­æ™®éå­˜åœ¨ï¼Œç¼–è¾‘è¿™äº›å›¾åƒéœ€è¦ç‰¹æ®Šå·¥å…·å’ŒæŠ€æœ¯ã€‚</li>
<li>å½“å‰æ–¹æ³•åœ¨å¤„ç†å¤æ‚å›¾åƒæ—¶å­˜åœ¨å›°éš¾ï¼Œäº§ç”Ÿçš„ç¨‹åºç»„ç»‡æ€§ä¸è¶³ï¼Œä½¿å¾—ç¼–è¾‘è¿‡ç¨‹ç¹çã€‚</li>
<li>ç ”ç©¶æå‡ºäº†ä¸€ç§åŸºäºæ¨¡å¼ç±»æ¯”å’Œç”Ÿæˆæ¨¡å‹çš„æ–°æ–¹æ³•ï¼Œç”¨äºç›´è§‚ç¼–è¾‘å›¾æ¡ˆå›¾åƒã€‚</li>
<li>å¼•å…¥äº†SplitWeaveè¿™ä¸€é¢†åŸŸç‰¹å®šè¯­è¨€å’Œæ¡†æ¶é‡‡æ ·åˆæˆæ¨¡å¼ç±»æ¯”ï¼Œä»¥æ”¯æŒè¿™ä¸€æ–°æ–¹æ³•ã€‚</li>
<li>è®¾è®¡äº†TriFuseræ½œæ‰©æ•£æ¨¡å‹ï¼ˆLatent Diffusion Model, LDMï¼‰æ¥è§£å†³åœ¨éƒ¨ç½²è¿‡ç¨‹ä¸­é‡åˆ°çš„å…³é”®é—®é¢˜ã€‚</li>
<li>è¯¥æ–¹æ³•èƒ½åœ¨çœŸå®ä¸–ç•Œå®éªŒä¸­å¯¹è‰ºæœ¯å®¶æ¥æºçš„å›¾æ¡ˆè¿›è¡Œå¿ å®ç¼–è¾‘ï¼Œå¹¶å±•ç¤ºè‰¯å¥½çš„æ³›åŒ–èƒ½åŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.12463">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-1cf9cd030512c8230f0cdfde6aaaec03.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-accaecbf63f05436e3b3ed083131af0d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4015bb2b48fb86a1d4041273f90a5be7.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-6d1026dda6a61f91d227c88cd26e51ef.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e4b93b8eb5f49d82569ed9924170a7b7.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="Efficient-Diversity-Preserving-Diffusion-Alignment-via-Gradient-Informed-GFlowNets"><a href="#Efficient-Diversity-Preserving-Diffusion-Alignment-via-Gradient-Informed-GFlowNets" class="headerlink" title="Efficient Diversity-Preserving Diffusion Alignment via Gradient-Informed   GFlowNets"></a>Efficient Diversity-Preserving Diffusion Alignment via Gradient-Informed   GFlowNets</h2><p><strong>Authors:Zhen Liu, Tim Z. Xiao, Weiyang Liu, Yoshua Bengio, Dinghuai Zhang</strong></p>
<p>While one commonly trains large diffusion models by collecting datasets on target downstream tasks, it is often desired to align and finetune pretrained diffusion models with some reward functions that are either designed by experts or learned from small-scale datasets. Existing post-training methods for reward finetuning of diffusion models typically suffer from lack of diversity in generated samples, lack of prior preservation, and&#x2F;or slow convergence in finetuning. Inspired by recent successes in generative flow networks (GFlowNets), a class of probabilistic models that sample with the unnormalized density of a reward function, we propose a novel GFlowNet method dubbed Nabla-GFlowNet (abbreviated as $\nabla$-GFlowNet), the first GFlowNet method that leverages the rich signal in reward gradients, together with an objective called $\nabla$-DB plus its variant residual $\nabla$-DB designed for prior-preserving diffusion finetuning. We show that our proposed method achieves fast yet diversity- and prior-preserving finetuning of Stable Diffusion, a large-scale text-conditioned image diffusion model, on different realistic reward functions. </p>
<blockquote>
<p>åœ¨è®­ç»ƒå¤§å‹æ‰©æ•£æ¨¡å‹æ—¶ï¼Œé€šå¸¸é€šè¿‡æ”¶é›†ç›®æ ‡ä¸‹æ¸¸ä»»åŠ¡çš„æ•°æ®é›†æ¥è¿›è¡Œã€‚ç„¶è€Œï¼Œäººä»¬å¾€å¾€å¸Œæœ›å°†é¢„è®­ç»ƒçš„æ‰©æ•£æ¨¡å‹ä¸æŸäº›å¥–åŠ±å‡½æ•°è¿›è¡Œå¯¹é½å’Œå¾®è°ƒï¼Œè¿™äº›å¥–åŠ±å‡½æ•°è¦ä¹ˆæ˜¯ä¸“å®¶è®¾è®¡çš„ï¼Œè¦ä¹ˆæ˜¯ä»å°è§„æ¨¡æ•°æ®é›†ä¸­å­¦ä¹ å¾—åˆ°çš„ã€‚ç°æœ‰çš„æ‰©æ•£æ¨¡å‹å¥–åŠ±å¾®è°ƒçš„åè®­ç»ƒæ–¹æ³•é€šå¸¸å­˜åœ¨ç”Ÿæˆæ ·æœ¬ç¼ºä¹å¤šæ ·æ€§ã€ç¼ºä¹å…ˆéªŒçŸ¥è¯†ä¿ç•™ä»¥åŠå¾®è°ƒæ”¶æ•›ç¼“æ…¢ç­‰é—®é¢˜ã€‚å—æœ€è¿‘ç”Ÿæˆæµç½‘ç»œï¼ˆGFlowNetsï¼‰æˆåŠŸçš„å¯å‘ï¼Œä¸€ç±»ä»¥å¥–åŠ±å‡½æ•°çš„æœªæ ‡å‡†åŒ–å¯†åº¦è¿›è¡Œé‡‡æ ·çš„æ¦‚ç‡æ¨¡å‹ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„GFlowNetæ–¹æ³•ï¼Œç§°ä¸ºNabla-GFlowNetï¼ˆç®€ç§°$\nabla$-GFlowNetï¼‰ï¼Œè¿™æ˜¯ç¬¬ä¸€ä¸ªåˆ©ç”¨ä¸°å¯Œçš„å¥–åŠ±æ¢¯åº¦ä¿¡å·çš„GFlowNetæ–¹æ³•ï¼Œä»¥åŠä¸€ä¸ªç§°ä¸º$\nabla$-DBçš„ç›®æ ‡åŠå…¶ç”¨äºå…ˆéªŒä¿ç•™æ‰©æ•£å¾®è°ƒçš„å˜ä½“æ®‹å·®$\nabla$-DBã€‚æˆ‘ä»¬å±•ç¤ºäº†æ‰€æå‡ºçš„æ–¹æ³•åœ¨ä¸åŒç°å®çš„å¥–åŠ±å‡½æ•°ä¸Šï¼Œå®ç°äº†å¿«é€Ÿã€å¤šæ ·åŒ–å’Œä¿ç•™å…ˆéªŒçš„ç¨³å®šæ‰©æ•£ï¼Œè¿™æ˜¯ä¸€ç§å¤§è§„æ¨¡çš„æ–‡æœ¬æ¡ä»¶å›¾åƒæ‰©æ•£æ¨¡å‹ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.07775v3">PDF</a> Technical Report (35 pages, 31 figures), Accepted at ICLR 2025</p>
<p><strong>æ‘˜è¦</strong></p>
<p>åŸºäºæ–‡æœ¬æå‡ºçš„GFlowNetæ–°æ–¹æ³•è¢«ç§°ä¸ºNabla-GFlowNetï¼ˆç®€ç§°$\nabla$-GFlowNetï¼‰ï¼Œå®ƒæ˜¯é¦–ä¸ªåˆ©ç”¨å¥–åŠ±æ¢¯åº¦ä¸°å¯Œä¿¡å·çš„GFlowNetæ–¹æ³•ï¼Œç»“åˆä¸€ä¸ªåä¸º$\nabla$-DBçš„ç›®æ ‡åŠå…¶ç”¨äºä¿ç•™å…ˆéªŒçš„å˜ä½“æ®‹å·®$\nabla$-DBï¼Œç”¨äºå®ç°å¿«é€Ÿä¸”å¤šæ ·åŒ–çš„æ‰©æ•£å¾®è°ƒã€‚æ­¤æ–¹æ³•æˆåŠŸåº”ç”¨äºå¤§å‹æ–‡æœ¬æ¡ä»¶å›¾åƒæ‰©æ•£æ¨¡å‹Stable Diffusionçš„ä¸åŒçœŸå®å¥–åŠ±å‡½æ•°ä¸Šã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>æå‡ºäº†ä¸€ç§æ–°çš„GFlowNetæ–¹æ³•â€”â€”Nabla-GFlowNetï¼Œé€‚ç”¨äºæ‰©æ•£æ¨¡å‹çš„å¥–åŠ±å¾®è°ƒã€‚</li>
<li>Nabla-GFlowNeté¦–æ¬¡åˆ©ç”¨å¥–åŠ±æ¢¯åº¦ä¸­çš„ä¸°å¯Œä¿¡å·ã€‚</li>
<li>$\nabla$-DBåŠå…¶å˜ä½“æ®‹å·®$\nabla$-DBçš„ç›®æ ‡è¢«è®¾è®¡ä¸ºç”¨äºä¿ç•™å…ˆéªŒçš„æ‰©æ•£å¾®è°ƒã€‚</li>
<li>è¯¥æ–¹æ³•å®ç°äº†å¿«é€Ÿä¸”å¤šæ ·åŒ–çš„å¾®è°ƒè¿‡ç¨‹ã€‚</li>
<li>è¯¥æ–¹æ³•æˆåŠŸåº”ç”¨äºå¤§å‹æ–‡æœ¬æ¡ä»¶å›¾åƒæ‰©æ•£æ¨¡å‹Stable Diffusionçš„ä¸åŒå¥–åŠ±å‡½æ•°ä¸Šã€‚</li>
<li>æé«˜äº†ç°æœ‰æ‰©æ•£æ¨¡å‹çš„æ€§èƒ½è¡¨ç°ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.07775">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-1bf14807914fa67af019de96e8c2b5a6.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-1d874aa872307bde9e8a53820468e24f.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="DiffPatch-Generating-Customizable-Adversarial-Patches-using-Diffusion-Models"><a href="#DiffPatch-Generating-Customizable-Adversarial-Patches-using-Diffusion-Models" class="headerlink" title="DiffPatch: Generating Customizable Adversarial Patches using Diffusion   Models"></a>DiffPatch: Generating Customizable Adversarial Patches using Diffusion   Models</h2><p><strong>Authors:Zhixiang Wang, Xiaosen Wang, Bo Wang, Siheng Chen, Zhibo Wang, Xingjun Ma, Yu-Gang Jiang</strong></p>
<p>Physical adversarial patches printed on clothing can enable individuals to evade person detectors, but most existing methods prioritize attack effectiveness over stealthiness, resulting in aesthetically unpleasing patches. While generative adversarial networks and diffusion models can produce more natural-looking patches, they often fail to balance stealthiness with attack effectiveness and lack flexibility for user customization. To address these limitations, we propose DiffPatch, a novel diffusion-based framework for generating customizable and naturalistic adversarial patches. Our approach allows users to start from a reference image (rather than random noise) and incorporates masks to create patches of various shapes, not limited to squares. To preserve the original semantics during the diffusion process, we employ Null-text inversion to map random noise samples to a single input image and generate patches through Incomplete Diffusion Optimization (IDO). Our method achieves attack performance comparable to state-of-the-art non-naturalistic patches while maintaining a natural appearance. Using DiffPatch, we construct AdvT-shirt-1K, the first physical adversarial T-shirt dataset comprising over a thousand images captured in diverse scenarios. AdvT-shirt-1K can serve as a useful dataset for training or testing future defense methods. </p>
<blockquote>
<p>ç‰©ç†å¯¹æŠ—è¡¥ä¸æ‰“å°åœ¨è¡£ç‰©ä¸Šå¯ä»¥ä½¿ä¸ªäººèº²é¿äººå‘˜æ£€æµ‹å™¨ï¼Œä½†ç°æœ‰çš„å¤§å¤šæ•°æ–¹æ³•ä¼˜å…ˆæ”»å‡»æ•ˆæœè€Œééšè”½æ€§ï¼Œå¯¼è‡´ç¾å­¦ä¸Šçœ‹èµ·æ¥å¹¶ä¸ç†æƒ³ã€‚è™½ç„¶ç”Ÿæˆå¯¹æŠ—ç½‘ç»œï¼ˆGANï¼‰å’Œæ‰©æ•£æ¨¡å‹å¯ä»¥äº§ç”Ÿæ›´è‡ªç„¶çš„è¡¥ä¸ï¼Œä½†å®ƒä»¬å¾€å¾€æ— æ³•å¹³è¡¡éšè”½æ€§å’Œæ”»å‡»æ•ˆæœï¼Œå¹¶ä¸”ç¼ºä¹ç”¨æˆ·å®šåˆ¶çµæ´»æ€§ã€‚ä¸ºäº†è§£å†³è¿™äº›é™åˆ¶ï¼Œæˆ‘ä»¬æå‡ºäº†DiffPatchï¼Œè¿™æ˜¯ä¸€ç§åŸºäºæ‰©æ•£çš„æ–°å‹ç”Ÿæˆå¯¹æŠ—è¡¥ä¸æ¡†æ¶ï¼Œå…è®¸ç”¨æˆ·ä»å‚è€ƒå›¾åƒå¼€å§‹ï¼ˆè€Œä¸æ˜¯éšæœºå™ªå£°ï¼‰ï¼Œå¹¶ç»“åˆæ©ç åˆ›å»ºå„ç§å½¢çŠ¶çš„è¡¥ä¸ï¼Œä¸é™äºæ­£æ–¹å½¢ã€‚ä¸ºäº†åœ¨æ‰©æ•£è¿‡ç¨‹ä¸­ä¿ç•™åŸå§‹è¯­ä¹‰ï¼Œæˆ‘ä»¬é‡‡ç”¨Nullæ–‡æœ¬åè½¬å°†éšæœºå™ªå£°æ ·æœ¬æ˜ å°„åˆ°å•ä¸ªè¾“å…¥å›¾åƒï¼Œå¹¶é€šè¿‡ä¸å®Œå…¨æ‰©æ•£ä¼˜åŒ–ï¼ˆIDOï¼‰ç”Ÿæˆè¡¥ä¸ã€‚æˆ‘ä»¬çš„æ–¹æ³•è¾¾åˆ°äº†ä¸è‡ªç„¶éç°å®è¡¥ä¸ç›¸å½“çš„æ”»å‡»æ€§èƒ½ï¼ŒåŒæ—¶ä¿æŒäº†è‡ªç„¶å¤–è§‚ã€‚ä½¿ç”¨DiffPatchï¼Œæˆ‘ä»¬æ„å»ºäº†AdvT-shirt-1Kæ•°æ®é›†ï¼Œè¿™æ˜¯ç¬¬ä¸€ä¸ªåŒ…å«åœ¨ä¸€åƒå¤šä¸ªä¸åŒåœºæ™¯ä¸­æ•è·çš„å›¾åƒçš„ç‰©ç†å¯¹æŠ—æ€§Tæ¤æ•°æ®é›†ã€‚AdvT-shirt-1Kå¯ä»¥ä½œä¸ºæœªæ¥è®­ç»ƒæˆ–æµ‹è¯•é˜²å¾¡æ–¹æ³•çš„å®è´µæ•°æ®é›†ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.01440v3">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>ç‰©ç†å¯¹æŠ—æ€§è¡¥ä¸å¯ä»¥æ‰“å°åœ¨è¡£ç‰©ä¸Šä»¥èº²é¿äººå‘˜æ£€æµ‹ï¼Œä½†ç°æœ‰æ–¹æ³•å¾€å¾€é‡è§†æ”»å‡»æ•ˆæœè€Œå¿½è§†éšè”½æ€§ï¼Œå¯¼è‡´è¡¥ä¸å¤–è§‚ä¸ç¾è§‚ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºDiffPatchï¼Œä¸€ç§åŸºäºæ‰©æ•£æ¨¡å‹çš„æ–°å‹æ¡†æ¶ï¼Œå¯ç”Ÿæˆå¯å®šåˆ¶å’Œè‡ªç„¶å¯¹æŠ—æ€§è¡¥ä¸ã€‚è¯¥æ–¹æ³•å…è®¸ç”¨æˆ·ä»å‚è€ƒå›¾åƒå¼€å§‹ï¼ˆè€Œééšæœºå™ªå£°ï¼‰ï¼Œå¹¶ç»“åˆæ©è†œåˆ›å»ºå„ç§å½¢çŠ¶çš„è¡¥ä¸ï¼Œä¸é™äºæ–¹å½¢ã€‚é€šè¿‡é‡‡ç”¨Nullæ–‡æœ¬åè½¬å’Œä¸å®Œå…¨æ‰©æ•£ä¼˜åŒ–ï¼ˆIDOï¼‰ï¼Œåœ¨æ‰©æ•£è¿‡ç¨‹ä¸­ä¿æŒåŸå§‹è¯­ä¹‰ã€‚è¯¥æ–¹æ³•åœ¨ä¿æŒè‡ªç„¶å¤–è§‚çš„åŒæ—¶ï¼Œæ”»å‡»æ€§èƒ½ä¸æœ€å…ˆè¿›çš„éè‡ªç„¶è¡¥ä¸ç›¸å½“ã€‚æ­¤å¤–ï¼Œåˆ©ç”¨DiffPatchæ„å»ºé¦–ä¸ªç‰©ç†å¯¹æŠ—æ€§Tæ¤æ•°æ®é›†AdvT-shirt-1Kï¼ŒåŒ…å«åœ¨ä¸åŒåœºæ™¯æ•è·çš„åƒä½™å›¾åƒï¼Œå¯ä½œä¸ºæœªæ¥é˜²å¾¡æ–¹æ³•çš„è®­ç»ƒæˆ–æµ‹è¯•æ•°æ®é›†ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¯¹æŠ—æ€§è¡¥ä¸å¯æ‰“å°åœ¨è¡£ç‰©ä¸Šä»¥èº²é¿äººå‘˜æ£€æµ‹ã€‚</li>
<li>ç°æœ‰æ–¹æ³•å¾€å¾€é‡è§†æ”»å‡»æ•ˆæœè€Œå¿½è§†éšè”½æ€§å’Œç¾è§‚æ€§ã€‚</li>
<li>DiffPatchæ˜¯ä¸€ç§åŸºäºæ‰©æ•£æ¨¡å‹çš„å¯¹æŠ—æ€§è¡¥ä¸ç”Ÿæˆæ¡†æ¶ï¼Œå¯ç”Ÿæˆè‡ªç„¶ä¸”å¯å®šåˆ¶çš„è¡¥ä¸ã€‚</li>
<li>DiffPatchå…è®¸ä»å‚è€ƒå›¾åƒå¼€å§‹ï¼Œå¹¶ç»“åˆæ©è†œåˆ›å»ºå„ç§å½¢çŠ¶çš„è¡¥ä¸ã€‚</li>
<li>é‡‡ç”¨Nullæ–‡æœ¬åè½¬å’Œä¸å®Œå…¨æ‰©æ•£ä¼˜åŒ–ï¼ˆIDOï¼‰ä»¥ä¿æŒåŸå§‹è¯­ä¹‰ã€‚</li>
<li>DiffPatchåœ¨ä¿æŒè‡ªç„¶å¤–è§‚çš„åŒæ—¶ï¼Œæ”»å‡»æ€§èƒ½ä¸æœ€å…ˆè¿›çš„éè‡ªç„¶è¡¥ä¸ç›¸å½“ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.01440">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-028bf4cc9ac29edaf607856b8052c9a7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b9c31033a6e2821a7e45a074f03292cd.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-fa066d645263a5bb40abb2829d2cc14b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a958a70c7df74c11d7f3cf6a5501e014.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7c792925fb93ff0c0aa5a1d71caf458f.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="DreamRelation-Bridging-Customization-and-Relation-Generation"><a href="#DreamRelation-Bridging-Customization-and-Relation-Generation" class="headerlink" title="DreamRelation: Bridging Customization and Relation Generation"></a>DreamRelation: Bridging Customization and Relation Generation</h2><p><strong>Authors:Qingyu Shi, Lu Qi, Jianzong Wu, Jinbin Bai, Jingbo Wang, Yunhai Tong, Xiangtai Li</strong></p>
<p>Customized image generation is essential for creating personalized content based on user prompts, allowing large-scale text-to-image diffusion models to more effectively meet individual needs. However, existing models often neglect the relationships between customized objects in generated images. In contrast, this work addresses this gap by focusing on relation-aware customized image generation, which seeks to preserve the identities from image prompts while maintaining the relationship specified in text prompts. Specifically, we introduce DreamRelation, a framework that disentangles identity and relation learning using a carefully curated dataset. Our training data consists of relation-specific images, independent object images containing identity information, and text prompts to guide relation generation. Then, we propose two key modules to tackle the two main challenges: generating accurate and natural relationships, especially when significant pose adjustments are required, and avoiding object confusion in cases of overlap. First, we introduce a keypoint matching loss that effectively guides the model in adjusting object poses closely tied to their relationships. Second, we incorporate local features of the image prompts to better distinguish between objects, preventing confusion in overlapping cases. Extensive results on our proposed benchmarks demonstrate the superiority of DreamRelation in generating precise relations while preserving object identities across a diverse set of objects and relationships. </p>
<blockquote>
<p>ä¸ªæ€§åŒ–å›¾åƒç”Ÿæˆå¯¹äºæ ¹æ®ç”¨æˆ·æç¤ºåˆ›å»ºä¸ªæ€§åŒ–å†…å®¹è‡³å…³é‡è¦ï¼Œä½¿å¾—å¤§è§„æ¨¡æ–‡æœ¬åˆ°å›¾åƒçš„æ‰©æ•£æ¨¡å‹èƒ½å¤Ÿæ›´æœ‰æ•ˆåœ°æ»¡è¶³ä¸ªäººéœ€æ±‚ã€‚ç„¶è€Œï¼Œç°æœ‰æ¨¡å‹å¾€å¾€å¿½ç•¥äº†ç”Ÿæˆå›¾åƒä¸­è‡ªå®šä¹‰å¯¹è±¡ä¹‹é—´çš„å…³ç³»ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼Œè¿™é¡¹å·¥ä½œé€šè¿‡å…³æ³¨å…³ç³»æ„ŸçŸ¥çš„å®šåˆ¶å›¾åƒç”Ÿæˆæ¥è§£å†³è¿™ä¸€ç©ºç™½ï¼Œæ—¨åœ¨ä¿ç•™å›¾åƒæç¤ºä¸­çš„èº«ä»½ï¼ŒåŒæ—¶ä¿æŒæ–‡æœ¬æç¤ºä¸­æŒ‡å®šçš„å…³ç³»ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬å¼•å…¥äº†DreamRelationæ¡†æ¶ï¼Œè¯¥æ¡†æ¶ä½¿ç”¨ç²¾å¿ƒåˆ¶ä½œçš„æ•°æ®é›†æ¥åˆ†ç¦»èº«ä»½å’Œå…³ç³»å­¦ä¹ ã€‚æˆ‘ä»¬çš„è®­ç»ƒæ•°æ®ç”±å…³ç³»ç‰¹å®šçš„å›¾åƒã€åŒ…å«èº«ä»½ä¿¡æ¯çš„ç‹¬ç«‹å¯¹è±¡å›¾åƒå’Œç”¨äºæŒ‡å¯¼å…³ç³»ç”Ÿæˆçš„æ–‡æœ¬æç¤ºç»„æˆã€‚ç„¶åï¼Œæˆ‘ä»¬æå‡ºäº†ä¸¤ä¸ªå…³é”®æ¨¡å—æ¥è§£å†³ä¸¤ä¸ªä¸»è¦æŒ‘æˆ˜ï¼šç”Ÿæˆå‡†ç¡®å’Œè‡ªç„¶çš„å…³ç³»ï¼Œå°¤å…¶æ˜¯åœ¨éœ€è¦è¿›è¡Œé‡å¤§å§¿åŠ¿è°ƒæ•´æ—¶ï¼›ä»¥åŠåœ¨é‡å æƒ…å†µä¸‹é¿å…å¯¹è±¡æ··æ·†ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ªå…³é”®ç‚¹åŒ¹é…æŸå¤±ï¼Œæœ‰æ•ˆåœ°æŒ‡å¯¼æ¨¡å‹è°ƒæ•´ä¸å…³ç³»ç´§å¯†ç›¸å…³çš„å¯¹è±¡å§¿åŠ¿ã€‚å…¶æ¬¡ï¼Œæˆ‘ä»¬ç»“åˆäº†å›¾åƒæç¤ºçš„å±€éƒ¨ç‰¹å¾æ¥æ›´å¥½åœ°åŒºåˆ†å¯¹è±¡ï¼Œä»è€Œåœ¨é‡å æƒ…å†µä¸‹é¿å…æ··æ·†ã€‚åœ¨æˆ‘ä»¬æå‡ºçš„åŸºå‡†æµ‹è¯•ä¸Šçš„å¤§é‡ç»“æœè¡¨æ˜ï¼ŒDreamRelationåœ¨ç”Ÿæˆç²¾ç¡®å…³ç³»çš„åŒæ—¶ï¼Œåœ¨å¤šç§å¯¹è±¡å’Œå…³ç³»ä¸Šä¿ç•™äº†å¯¹è±¡èº«ä»½çš„ä¼˜åŠ¿ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2410.23280v4">PDF</a> CVPR 2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡å…³æ³¨ä¸ªæ€§åŒ–å›¾åƒç”Ÿæˆä¸­çš„å…³ç³»æ„ŸçŸ¥æŠ€æœ¯ï¼Œé’ˆå¯¹ç°æœ‰æ¨¡å‹å¿½ç•¥å›¾åƒä¸­è‡ªå®šä¹‰å¯¹è±¡é—´å…³ç³»çš„é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§æ–°çš„æ–¹æ³•DreamRelationã€‚è¯¥æ–¹æ³•é€šè¿‡åˆ†ç¦»èº«ä»½ä¸å…³ç³»å­¦ä¹ ï¼Œä½¿ç”¨ç‰¹å®šçš„æ•°æ®é›†å’Œæ–‡æœ¬æç¤ºæ¥ç”Ÿæˆæ—¢ä¿ç•™å›¾åƒæç¤ºä¸­çš„èº«ä»½åˆä¿æŒæ–‡æœ¬æç¤ºä¸­æŒ‡å®šçš„å…³ç³»çš„å›¾åƒã€‚è¯¥æ–¹æ³•çš„å…³é”®ç‚¹åŒ¹é…æŸå¤±å¯ä»¥æœ‰æ•ˆåœ°è°ƒæ•´å¯¹è±¡çš„å§¿åŠ¿ä»¥é€‚åº”å®ƒä»¬çš„å…³ç³»ï¼Œè€Œèå…¥å›¾åƒæç¤ºçš„å±€éƒ¨ç‰¹å¾åˆ™æœ‰åŠ©äºåŒºåˆ†é‡å å¯¹è±¡ï¼Œé¿å…æ··æ·†ã€‚å®éªŒç»“æœè¯æ˜ï¼ŒDreamRelationåœ¨å¤„ç†å¤šæ ·åŒ–çš„å¯¹è±¡å’Œå…³ç³»æ—¶èƒ½å¤Ÿç”Ÿæˆç²¾ç¡®çš„å…³ç³»å¹¶ä¿ç•™å¯¹è±¡èº«ä»½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>å®šåˆ¶å›¾åƒç”Ÿæˆåœ¨åŸºäºç”¨æˆ·æç¤ºåˆ›å»ºä¸ªæ€§åŒ–å†…å®¹æ–¹é¢è‡³å…³é‡è¦ï¼Œç°æœ‰æ¨¡å‹åœ¨ç”Ÿæˆå›¾åƒæ—¶å¿½ç•¥äº†å¯¹è±¡é—´çš„å…³ç³»ã€‚</li>
<li>DreamRelationæ¡†æ¶è§£å†³äº†è¿™ä¸€é—®é¢˜ï¼Œé€šè¿‡åˆ†ç¦»èº«ä»½ä¸å…³ç³»å­¦ä¹ ï¼Œåœ¨ç”Ÿæˆå›¾åƒæ—¶æ—¢ä¿ç•™å›¾åƒæç¤ºä¸­çš„èº«ä»½ï¼Œåˆä¿æŒæ–‡æœ¬æç¤ºä¸­çš„å…³ç³»ã€‚</li>
<li>ä½¿ç”¨å…³é”®ç‚¹åŒ¹é…æŸå¤±æ¥è°ƒæ•´å¯¹è±¡çš„å§¿åŠ¿ä»¥é€‚åº”å®ƒä»¬çš„å…³ç³»ï¼Œè¿™æ˜¯DreamRelationçš„ä¸¤ä¸ªå…³é”®æ¨¡å—ä¹‹ä¸€ã€‚</li>
<li>ä¸ºäº†åŒºåˆ†é‡å å¯¹è±¡å¹¶é¿å…æ··æ·†ï¼ŒDreamRelationèå…¥äº†å›¾åƒæç¤ºçš„å±€éƒ¨ç‰¹å¾ã€‚</li>
<li>åœ¨æå‡ºçš„åŸºå‡†æµ‹è¯•ä¸­ï¼ŒDreamRelationåœ¨å¤„ç†å¤šæ ·åŒ–çš„å¯¹è±¡å’Œå…³ç³»æ—¶è¡¨ç°å‡ºä¼˜è¶Šæ€§ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2410.23280">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-8a1d7f138f072d06cee00333f1f46e48.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-0adb043a5b8fc93ca3d1e783ec3d9213.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b908cef38a6f73ca23aa533abc416ee8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ed8e7431a09f80e4464be0df46dab630.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="FRAP-Faithful-and-Realistic-Text-to-Image-Generation-with-Adaptive-Prompt-Weighting"><a href="#FRAP-Faithful-and-Realistic-Text-to-Image-Generation-with-Adaptive-Prompt-Weighting" class="headerlink" title="FRAP: Faithful and Realistic Text-to-Image Generation with Adaptive   Prompt Weighting"></a>FRAP: Faithful and Realistic Text-to-Image Generation with Adaptive   Prompt Weighting</h2><p><strong>Authors:Liyao Jiang, Negar Hassanpour, Mohammad Salameh, Mohan Sai Singamsetti, Fengyu Sun, Wei Lu, Di Niu</strong></p>
<p>Text-to-image (T2I) diffusion models have demonstrated impressive capabilities in generating high-quality images given a text prompt. However, ensuring the prompt-image alignment remains a considerable challenge, i.e., generating images that faithfully align with the promptâ€™s semantics. Recent works attempt to improve the faithfulness by optimizing the latent code, which potentially could cause the latent code to go out-of-distribution and thus produce unrealistic images. In this paper, we propose FRAP, a simple, yet effective approach based on adaptively adjusting the per-token prompt weights to improve prompt-image alignment and authenticity of the generated images. We design an online algorithm to adaptively update each tokenâ€™s weight coefficient, which is achieved by minimizing a unified objective function that encourages object presence and the binding of object-modifier pairs. Through extensive evaluations, we show FRAP generates images with significantly higher prompt-image alignment to prompts from complex datasets, while having a lower average latency compared to recent latent code optimization methods, e.g., 4 seconds faster than D&amp;B on the COCO-Subject dataset. Furthermore, through visual comparisons and evaluation of the CLIP-IQA-Real metric, we show that FRAP not only improves prompt-image alignment but also generates more authentic images with realistic appearances. We also explore combining FRAP with prompt rewriting LLM to recover their degraded prompt-image alignment, where we observe improvements in both prompt-image alignment and image quality. We release the code at the following link: <a target="_blank" rel="noopener" href="https://github.com/LiyaoJiang1998/FRAP/">https://github.com/LiyaoJiang1998/FRAP/</a>. </p>
<blockquote>
<p>æ–‡æœ¬åˆ°å›¾åƒï¼ˆT2Iï¼‰çš„æ‰©æ•£æ¨¡å‹åœ¨ç»™å®šæ–‡æœ¬æç¤ºçš„æƒ…å†µä¸‹ï¼Œå·²ç»æ˜¾ç¤ºå‡ºç”Ÿæˆé«˜è´¨é‡å›¾åƒçš„å¼ºå¤§èƒ½åŠ›ã€‚ç„¶è€Œï¼Œç¡®ä¿æç¤ºä¸å›¾åƒçš„å¯¹åº”ä»ç„¶æ˜¯ä¸€ä¸ªå·¨å¤§çš„æŒ‘æˆ˜ï¼Œå³ç”Ÿæˆå¿ å®äºæç¤ºè¯­ä¹‰çš„å›¾åƒã€‚è¿‘æœŸçš„ç ”ç©¶å·¥ä½œè¯•å›¾é€šè¿‡ä¼˜åŒ–æ½œåœ¨ä»£ç æ¥æé«˜å¿ å®åº¦ï¼Œè¿™å¯èƒ½ä¼šå¯¼è‡´æ½œåœ¨ä»£ç åç¦»åˆ†å¸ƒï¼Œä»è€Œç”Ÿæˆä¸ç°å®çš„å›¾åƒã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†FRAPï¼Œè¿™æ˜¯ä¸€ç§ç®€å•è€Œæœ‰æ•ˆçš„æ–¹æ³•ï¼ŒåŸºäºè‡ªé€‚åº”è°ƒæ•´æ¯ä¸ªæ ‡è®°çš„æç¤ºæƒé‡ï¼Œä»¥æé«˜æç¤ºä¸å›¾åƒçš„å¯¹åº”ä»¥åŠç”Ÿæˆå›¾åƒçš„çœŸå®æ€§ã€‚æˆ‘ä»¬è®¾è®¡äº†ä¸€ç§åœ¨çº¿ç®—æ³•ï¼Œè‡ªé€‚åº”åœ°æ›´æ–°æ¯ä¸ªæ ‡è®°çš„æƒé‡ç³»æ•°ï¼Œè¿™æ˜¯é€šè¿‡æœ€å°åŒ–ä¸€ä¸ªç»Ÿä¸€çš„ç›®æ ‡å‡½æ•°æ¥å®ç°çš„ï¼Œè¯¥å‡½æ•°é¼“åŠ±å¯¹è±¡çš„å­˜åœ¨å’Œå¯¹è±¡ä¿®é¥°ç¬¦å¯¹çš„ç»‘å®šã€‚é€šè¿‡å¹¿æ³›çš„è¯„ä¼°ï¼Œæˆ‘ä»¬å±•ç¤ºäº†FRAPåœ¨å¤æ‚æ•°æ®é›†ä¸Šçš„æç¤ºä¸å›¾åƒå¯¹åº”èƒ½åŠ›æ›´å¼ºï¼ŒåŒæ—¶ä¸æœ€è¿‘çš„æ½œåœ¨ä»£ç ä¼˜åŒ–æ–¹æ³•ç›¸æ¯”å…·æœ‰æ›´ä½çš„å¹³å‡å»¶è¿Ÿï¼Œä¾‹å¦‚åœ¨COCO-Subjectæ•°æ®é›†ä¸Šæ¯”D&amp;Bå¿«4ç§’ã€‚æ­¤å¤–ï¼Œé€šè¿‡è§†è§‰æ¯”è¾ƒå’Œå¯¹CLIP-IQA-RealæŒ‡æ ‡çš„è¯„ä¼°ï¼Œæˆ‘ä»¬è¯æ˜FRAPä¸ä»…æé«˜äº†æç¤ºä¸å›¾åƒçš„å¯¹åº”èƒ½åŠ›ï¼Œè€Œä¸”ç”Ÿæˆäº†æ›´çœŸå®çš„å›¾åƒï¼Œå…·æœ‰é€¼çœŸçš„å¤–è§‚ã€‚æˆ‘ä»¬è¿˜æ¢ç´¢äº†å°†FRAPä¸æç¤ºé‡å†™çš„å¤§å‹è¯­è¨€æ¨¡å‹ç›¸ç»“åˆï¼Œä»¥æ¢å¤å…¶é€€åŒ–çš„æç¤ºä¸å›¾åƒå¯¹åº”èƒ½åŠ›ï¼Œè§‚å¯Ÿåˆ°æç¤ºä¸å›¾åƒå¯¹åº”èƒ½åŠ›å’Œå›¾åƒè´¨é‡çš„æ”¹å–„ã€‚æˆ‘ä»¬å·²åœ¨ä»¥ä¸‹é“¾æ¥å‘å¸ƒä»£ç ï¼š<a target="_blank" rel="noopener" href="https://github.com/LiyaoJiang1998/FRAP/%E3%80%82">https://github.com/LiyaoJiang1998/FRAP/ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2408.11706v2">PDF</a> TMLR 2025</p>
<p><strong>æ‘˜è¦</strong></p>
<p>æ–‡æœ¬åˆ°å›¾åƒï¼ˆT2Iï¼‰æ‰©æ•£æ¨¡å‹åœ¨ç»™å®šæ–‡æœ¬æç¤ºç”Ÿæˆé«˜è´¨é‡å›¾åƒæ–¹é¢è¡¨ç°å‡ºä»¤äººå°è±¡æ·±åˆ»çš„èƒ½åŠ›ã€‚ç„¶è€Œï¼Œç¡®ä¿æç¤ºä¸å›¾åƒçš„å¯¹é½ä»ç„¶æ˜¯ä¸€ä¸ªå·¨å¤§çš„æŒ‘æˆ˜ï¼Œå³ç”Ÿæˆèƒ½å¤Ÿå¿ å®å¯¹åº”æç¤ºè¯­ä¹‰çš„å›¾åƒã€‚æœ€è¿‘çš„ç ”ç©¶å·¥ä½œè¯•å›¾é€šè¿‡ä¼˜åŒ–æ½œåœ¨ä»£ç æ¥æé«˜å¿ å®æ€§ï¼Œä½†è¿™å¯èƒ½ä¼šå¯¼è‡´æ½œåœ¨ä»£ç è„±ç¦»åˆ†å¸ƒï¼Œä»è€Œç”Ÿæˆä¸ç°å®çš„å›¾åƒã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§ç®€å•è€Œæœ‰æ•ˆçš„æ–¹æ³•FRAPï¼Œé€šè¿‡è‡ªé€‚åº”è°ƒæ•´æ¯ä¸ªä»¤ç‰Œæç¤ºæƒé‡æ¥æ”¹å–„æç¤ºä¸å›¾åƒçš„å¯¹é½ä»¥åŠç”Ÿæˆå›¾åƒçš„çœŸå®æ€§ã€‚æˆ‘ä»¬è®¾è®¡äº†ä¸€ç§åœ¨çº¿ç®—æ³•ï¼Œé€šè¿‡æœ€å°åŒ–ä¸€ä¸ªç»Ÿä¸€çš„ç›®æ ‡å‡½æ•°æ¥åŠ¨æ€æ›´æ–°æ¯ä¸ªä»¤ç‰Œçš„æƒé‡ç³»æ•°ï¼Œè¯¥å‡½æ•°é¼“åŠ±ç‰©ä½“å­˜åœ¨å’Œç‰©ä½“ä¿®é¥°ç¬¦å¯¹çš„ç»‘å®šã€‚é€šè¿‡å¹¿æ³›è¯„ä¼°ï¼Œæˆ‘ä»¬å±•ç¤ºäº†FRAPç”Ÿæˆçš„å›¾åƒä¸æ¥è‡ªå¤æ‚æ•°æ®é›†çš„æç¤ºå…·æœ‰æ›´é«˜çš„æç¤ºå¯¹é½åº¦ï¼ŒåŒæ—¶ä¸æœ€è¿‘çš„æ½œåœ¨ä»£ç ä¼˜åŒ–æ–¹æ³•ç›¸æ¯”å…·æœ‰æ›´ä½çš„å¹³å‡å»¶è¿Ÿï¼Œä¾‹å¦‚åœ¨COCO-Subjectæ•°æ®é›†ä¸Šæ¯”D&amp;Bå¿«4ç§’ã€‚æ­¤å¤–ï¼Œé€šè¿‡è§†è§‰æ¯”è¾ƒå’ŒCLIP-IQA-Realè¯„ä»·æŒ‡æ ‡çš„è¯„ä¼°ï¼Œæˆ‘ä»¬è¯æ˜FRAPä¸ä»…æé«˜äº†æç¤ºä¸å›¾åƒçš„å¯¹é½åº¦ï¼Œè€Œä¸”ç”Ÿæˆäº†å…·æœ‰æ›´çœŸå®å¤–è§‚çš„å›¾åƒã€‚æˆ‘ä»¬è¿˜æ¢ç´¢äº†å°†FRAPä¸æç¤ºé‡å†™LLMç›¸ç»“åˆï¼Œä»¥æ¢å¤å…¶é€€åŒ–çš„æç¤ºä¸å›¾åƒå¯¹é½åº¦ï¼Œè§‚å¯Ÿåˆ°æç¤ºä¸å›¾åƒå¯¹é½åº¦å’Œå›¾åƒè´¨é‡éƒ½æœ‰æ‰€æé«˜ã€‚æˆ‘ä»¬å·²å°†ä»£ç å‘å¸ƒåœ¨ä»¥ä¸‹é“¾æ¥ï¼š<a target="_blank" rel="noopener" href="https://github.com/LiyaoJiang1998/FRAP/">https://github.com/LiyaoJiang1998/FRAP/</a>ã€‚</p>
<p><strong>è¦ç‚¹å½’çº³</strong></p>
<ol>
<li>T2Iæ‰©æ•£æ¨¡å‹èƒ½ç”Ÿæˆé«˜è´¨é‡å›¾åƒï¼Œä½†ä¿è¯æç¤ºä¸å›¾åƒå¯¹é½ä»æ˜¯æŒ‘æˆ˜ã€‚</li>
<li>ç°æœ‰æ–¹æ³•é€šè¿‡ä¼˜åŒ–æ½œåœ¨ä»£ç æ¥æå‡å¿ å®æ€§ï¼Œå¯èƒ½äº§ç”Ÿä¸ç°å®å›¾åƒã€‚</li>
<li>FRAPæ–¹æ³•é€šè¿‡è‡ªé€‚åº”è°ƒæ•´ä»¤ç‰Œæç¤ºæƒé‡æ¥æ”¹å–„å¯¹é½å’Œå›¾åƒçœŸå®æ€§ã€‚</li>
<li>FRAPé‡‡ç”¨åœ¨çº¿ç®—æ³•å’Œç»Ÿä¸€ç›®æ ‡å‡½æ•°æ¥åŠ¨æ€æ›´æ–°ä»¤ç‰Œæƒé‡ã€‚</li>
<li>FRAPåœ¨å¤æ‚æ•°æ®é›†ä¸Šå®ç°é«˜æç¤ºå¯¹é½åº¦ï¼Œå¹¶æœ‰è¾ƒä½å¹³å‡å»¶è¿Ÿã€‚</li>
<li>FRAPä¸ä»…èƒ½æé«˜å¯¹é½åº¦ï¼Œè¿˜èƒ½ç”Ÿæˆæ›´çœŸå®çš„å›¾åƒï¼Œå¾—åˆ°è§†è§‰å’Œè¯„ä»·æŒ‡æ ‡çš„éªŒè¯ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2408.11706">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-946696754ca97073fa9102456b75d061.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d9c551b186c94dd2454b162c6d5d9866.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="Latent-Feature-and-Attention-Dual-Erasure-Attack-against-Multi-View-Diffusion-Models-for-3D-Assets-Protection"><a href="#Latent-Feature-and-Attention-Dual-Erasure-Attack-against-Multi-View-Diffusion-Models-for-3D-Assets-Protection" class="headerlink" title="Latent Feature and Attention Dual Erasure Attack against Multi-View   Diffusion Models for 3D Assets Protection"></a>Latent Feature and Attention Dual Erasure Attack against Multi-View   Diffusion Models for 3D Assets Protection</h2><p><strong>Authors:Jingwei Sun, Xuchong Zhang, Changfeng Sun, Qicheng Bai, Hongbin Sun</strong></p>
<p>Multi-View Diffusion Models (MVDMs) enable remarkable improvements in the field of 3D geometric reconstruction, but the issue regarding intellectual property has received increasing attention due to unauthorized imitation. Recently, some works have utilized adversarial attacks to protect copyright. However, all these works focus on single-image generation tasks which only need to consider the inner feature of images. Previous methods are inefficient in attacking MVDMs because they lack the consideration of disrupting the geometric and visual consistency among the generated multi-view images. This paper is the first to address the intellectual property infringement issue arising from MVDMs. Accordingly, we propose a novel latent feature and attention dual erasure attack to disrupt the distribution of latent feature and the consistency across the generated images from multi-view and multi-domain simultaneously. The experiments conducted on SOTA MVDMs indicate that our approach achieves superior performances in terms of attack effectiveness, transferability, and robustness against defense methods. Therefore, this paper provides an efficient solution to protect 3D assets from MVDMs-based 3D geometry reconstruction. </p>
<blockquote>
<p>å¤šè§†è§’æ‰©æ•£æ¨¡å‹ï¼ˆMVDMsï¼‰åœ¨3Då‡ ä½•é‡å»ºé¢†åŸŸå®ç°äº†æ˜¾è‘—çš„æ”¹è¿›ï¼Œä½†ç”±äºæœªç»æˆæƒçš„æ¨¡ä»¿ï¼ŒçŸ¥è¯†äº§æƒé—®é¢˜æ—¥ç›Šå—åˆ°å…³æ³¨ã€‚æœ€è¿‘ï¼Œä¸€äº›å·¥ä½œåˆ©ç”¨å¯¹æŠ—æ€§æ”»å‡»æ¥ä¿æŠ¤ç‰ˆæƒã€‚ç„¶è€Œï¼Œæ‰€æœ‰è¿™äº›å·¥ä½œéƒ½é›†ä¸­åœ¨å•å›¾åƒç”Ÿæˆä»»åŠ¡ä¸Šï¼Œåªéœ€è¦è€ƒè™‘å›¾åƒçš„å†…éƒ¨ç‰¹å¾ã€‚ä»¥å‰çš„æ–¹æ³•åœ¨æ”»å‡»MVDMsæ—¶æ•ˆç‡ä¸é«˜ï¼Œå› ä¸ºå®ƒä»¬æ²¡æœ‰è€ƒè™‘åˆ°ç ´åç”Ÿæˆçš„å¤šè§†è§’å›¾åƒä¹‹é—´çš„å‡ ä½•å’Œè§†è§‰ä¸€è‡´æ€§ã€‚æœ¬æ–‡é¦–æ¬¡è§£å†³ç”±MVDMså¼•èµ·çš„çŸ¥è¯†äº§æƒä¾µæƒé—®é¢˜ã€‚æ®æ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°å‹çš„æ½œåœ¨ç‰¹å¾å’Œæ³¨æ„åŠ›åŒé‡æ“¦é™¤æ”»å‡»ï¼Œä»¥ç ´åæ½œåœ¨ç‰¹å¾åˆ†å¸ƒå’Œæ¥è‡ªå¤šè§†è§’å’Œå¤šé¢†åŸŸçš„ç”Ÿæˆå›¾åƒä¹‹é—´çš„ä¸€è‡´æ€§ã€‚åœ¨SOTA MVDMsä¸Šè¿›è¡Œçš„å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨æ”»å‡»æœ‰æ•ˆæ€§ã€å¯è½¬ç§»æ€§å’Œå¯¹é˜²å¾¡æ–¹æ³•çš„ç¨³å¥æ€§æ–¹é¢è¡¨ç°å‡ºå“è¶Šæ€§èƒ½ã€‚å› æ­¤ï¼Œæœ¬æ–‡æä¾›äº†ä¸€ç§æœ‰æ•ˆçš„è§£å†³æ–¹æ¡ˆï¼Œä¿æŠ¤3Dèµ„äº§å…å—åŸºäºMVDMsçš„3Då‡ ä½•é‡å»ºçš„ä¾µå®³ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2408.11408v2">PDF</a> This paper has been accepted by ICME 2025</p>
<p><strong>Summary</strong></p>
<p>å¤šè§†å›¾æ‰©æ•£æ¨¡å‹ï¼ˆMVDMï¼‰åœ¨3Då‡ ä½•é‡å»ºé¢†åŸŸå–å¾—äº†æ˜¾è‘—è¿›æ­¥ï¼Œä½†çŸ¥è¯†äº§æƒé—®é¢˜å› æœªç»æˆæƒçš„æ¨¡ä»¿è€Œå—åˆ°è¶Šæ¥è¶Šå¤šçš„å…³æ³¨ã€‚é’ˆå¯¹è¿™ä¸€é—®é¢˜ï¼Œæœ¬æ–‡é¦–æ¬¡æå‡ºä¸€ç§é’ˆå¯¹MVDMçš„æ½œåœ¨ç‰¹å¾å’Œæ³¨æ„åŠ›åŒé‡æ“¦é™¤æ”»å‡»æ–¹æ³•ï¼Œè¯¥æ–¹æ³•èƒ½å¤ŸåŒæ—¶ç ´åç”Ÿæˆçš„å¤šè§†è§’å’Œå¤šé¢†åŸŸå›¾åƒçš„æ½œåœ¨ç‰¹å¾åˆ†å¸ƒå’Œä¸€è‡´æ€§ã€‚å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨æ”»å‡»æ•ˆæœã€è¿ç§»æ€§å’Œé˜²å¾¡æ–¹æ³•ç¨³å¥æ€§æ–¹é¢è¡¨ç°ä¼˜è¶Šï¼Œä¸º3Dèµ„äº§ä¿æŠ¤æä¾›äº†ä¸€ç§æœ‰æ•ˆçš„è§£å†³æ–¹æ¡ˆã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤šè§†å›¾æ‰©æ•£æ¨¡å‹ï¼ˆMVDMï¼‰åœ¨3Då‡ ä½•é‡å»ºä¸­çš„åº”ç”¨å–å¾—æ˜¾è‘—è¿›æ­¥ï¼Œä½†çŸ¥è¯†äº§æƒé—®é¢˜æ—¥ç›Šçªå‡ºã€‚</li>
<li>ç›®å‰å­˜åœ¨åˆ©ç”¨å¯¹æŠ—æ€§æ”»å‡»ä¿æŠ¤ç‰ˆæƒçš„æ–¹æ³•ï¼Œä½†é’ˆå¯¹MVDMçš„æ”»å‡»æ–¹æ³•æ•ˆç‡è¾ƒä½ã€‚</li>
<li>æœ¬æ–‡é¦–æ¬¡æå‡ºé’ˆå¯¹MVDMçš„æ½œåœ¨ç‰¹å¾å’Œæ³¨æ„åŠ›åŒé‡æ“¦é™¤æ”»å‡»æ–¹æ³•ã€‚</li>
<li>è¯¥æ–¹æ³•èƒ½å¤ŸåŒæ—¶ç ´åç”Ÿæˆçš„å¤šè§†è§’å’Œå¤šé¢†åŸŸå›¾åƒçš„æ½œåœ¨ç‰¹å¾åˆ†å¸ƒå’Œä¸€è‡´æ€§ã€‚</li>
<li>å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨æ”»å‡»æ•ˆæœã€è¿ç§»æ€§å’Œé˜²å¾¡æ–¹æ³•ç¨³å¥æ€§æ–¹é¢è¡¨ç°ä¼˜è¶Šã€‚</li>
<li>è¯¥æ–¹æ³•ä¸ºè§£å†³åŸºäºMVDMçš„3Då‡ ä½•é‡å»ºä¸­çš„çŸ¥è¯†äº§æƒä¿æŠ¤é—®é¢˜æä¾›äº†æœ‰æ•ˆè§£å†³æ–¹æ¡ˆã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2408.11408">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-75d944f44c05eb36da5ab060772ec6fb.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4710ad42d5b63a86a35f635a21e0961a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ef3a2664285de322fb8c549c7187ab6b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-225ce787398f40f01312a246b986dbd5.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-87ddd6c8462dfc3c268a4172ef306953.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d2616b69c729ca215c4f7f545e71d5e9.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-4f7cf7547fc47905385b384c70657d3f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-34dec09fffcf43bfa59c1c280414ea19.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="No-Re-Train-More-Gain-Upgrading-Backbones-with-Diffusion-model-for-Pixel-Wise-and-Weakly-Supervised-Few-Shot-Segmentation"><a href="#No-Re-Train-More-Gain-Upgrading-Backbones-with-Diffusion-model-for-Pixel-Wise-and-Weakly-Supervised-Few-Shot-Segmentation" class="headerlink" title="No Re-Train, More Gain: Upgrading Backbones with Diffusion model for   Pixel-Wise and Weakly-Supervised Few-Shot Segmentation"></a>No Re-Train, More Gain: Upgrading Backbones with Diffusion model for   Pixel-Wise and Weakly-Supervised Few-Shot Segmentation</h2><p><strong>Authors:Shuai Chen, Fanman Meng, Chenhao Wu, Haoran Wei, Runtong Zhang, Qingbo Wu, Linfeng Xu, Hongliang Li</strong></p>
<p>Few-Shot Segmentation (FSS) aims to segment novel classes using only a few annotated images. Despite considerable progress under pixel-wise support annotation, current FSS methods still face three issues: the inflexibility of backbone upgrade without re-training, the inability to uniformly handle various types of annotations (e.g., scribble, bounding box, mask, and text), and the difficulty in accommodating different annotation quantity. To address these issues simultaneously, we propose DiffUp, a novel framework that conceptualizes the FSS task as a conditional generative problem using a diffusion process. For the first issue, we introduce a backbone-agnostic feature transformation module that converts different segmentation cues into unified coarse priors, facilitating seamless backbone upgrade without re-training. For the second issue, due to the varying granularity of transformed priors from diverse annotation types (scribble, bounding box, mask, and text), we conceptualize these multi-granular transformed priors as analogous to noisy intermediates at different steps of a diffusion model. This is implemented via a self-conditioned modulation block coupled with a dual-level quality modulation branch. For the third issue, we incorporate an uncertainty-aware information fusion module to harmonize the variability across zero-shot, one-shot, and many-shot scenarios. Evaluated through rigorous benchmarks, DiffUp significantly outperforms existing FSS models in terms of flexibility and accuracy. </p>
<blockquote>
<p>å°‘æ•°æ ·æœ¬åˆ†å‰²ï¼ˆFSSï¼‰æ—¨åœ¨ä»…ä½¿ç”¨å°‘é‡æ ‡æ³¨å›¾åƒå¯¹æ–°å‹ç±»åˆ«è¿›è¡Œåˆ†å‰²ã€‚å°½ç®¡åœ¨åƒç´ çº§æ”¯æŒæ ‡æ³¨æ–¹é¢å–å¾—äº†ç›¸å½“å¤§çš„è¿›å±•ï¼Œä½†å½“å‰FSSæ–¹æ³•ä»ç„¶é¢ä¸´ä¸‰ä¸ªé—®é¢˜ï¼šéª¨å¹²ç½‘å‡çº§çš„çµæ´»æ€§ä¸è¶³éœ€è¦é‡æ–°è®­ç»ƒï¼Œæ— æ³•ç»Ÿä¸€å¤„ç†å„ç§ç±»å‹æ ‡æ³¨ï¼ˆå¦‚æ¶‚é¸¦ã€è¾¹ç•Œæ¡†ã€è’™ç‰ˆå’Œæ–‡æœ¬ï¼‰ï¼Œä»¥åŠéš¾ä»¥é€‚åº”ä¸åŒçš„æ ‡æ³¨æ•°é‡ã€‚ä¸ºäº†è§£å†³è¿™ä¸‰ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†DiffUpè¿™ä¸€æ–°å‹æ¡†æ¶ï¼Œå®ƒå°†FSSä»»åŠ¡æ¦‚å¿µåŒ–ä¸ºä¸€ä¸ªä½¿ç”¨æ‰©æ•£è¿‡ç¨‹çš„æ¡ä»¶ç”Ÿæˆé—®é¢˜ã€‚å¯¹äºç¬¬ä¸€ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ªé€šç”¨çš„ç‰¹å¾è½¬æ¢æ¨¡å—ï¼Œè¯¥æ¨¡å—èƒ½å¤Ÿå°†ä¸åŒçš„åˆ†å‰²çº¿ç´¢è½¬æ¢ä¸ºç»Ÿä¸€çš„ç²—ç•¥å…ˆéªŒçŸ¥è¯†ï¼Œä»è€Œå®ç°æ— éœ€é‡æ–°è®­ç»ƒçš„éª¨å¹²ç½‘æ— ç¼å‡çº§ã€‚å¯¹äºç¬¬äºŒä¸ªé—®é¢˜ï¼Œç”±äºä¸åŒæ ‡æ³¨ç±»å‹ï¼ˆæ¶‚é¸¦ã€è¾¹ç•Œæ¡†ã€è’™ç‰ˆå’Œæ–‡æœ¬ï¼‰äº§ç”Ÿçš„è½¬æ¢å…ˆéªŒç²’åº¦ä¸åŒï¼Œæˆ‘ä»¬å°†è¿™äº›å¤šç²’åº¦è½¬æ¢å…ˆéªŒæ¦‚å¿µåŒ–ä¸ºæ‰©æ•£æ¨¡å‹ä¸åŒæ­¥éª¤ä¸­çš„å™ªå£°ä¸­é—´äº§ç‰©ã€‚è¿™æ˜¯é€šè¿‡ä¸€ä¸ªè‡ªæ¡ä»¶è°ƒåˆ¶æ¨¡å—ä¸ä¸€ä¸ªåŒçº§è´¨é‡è°ƒåˆ¶åˆ†æ”¯ç›¸ç»“åˆæ¥å®ç°çš„ã€‚å¯¹äºç¬¬ä¸‰ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬èå…¥äº†ä¸€ä¸ªä¸ç¡®å®šæ€§æ„ŸçŸ¥çš„ä¿¡æ¯èåˆæ¨¡å—ï¼Œä»¥åè°ƒé›¶æ ·æœ¬ã€å•æ ·æœ¬å’Œå¤šæ ·æœ¬åœºæ™¯ä¹‹é—´çš„å˜åŒ–ã€‚é€šè¿‡ä¸¥æ ¼çš„åŸºå‡†æµ‹è¯•ï¼ŒDiffUpåœ¨çµæ´»æ€§å’Œå‡†ç¡®æ€§æ–¹é¢æ˜¾è‘—ä¼˜äºç°æœ‰çš„FSSæ¨¡å‹ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2407.16182v2">PDF</a> 9 figures</p>
<p><strong>Summary</strong></p>
<p>åŸºäºå°‘æ ·æœ¬åˆ†å‰²ï¼ˆFSSï¼‰ä»»åŠ¡çš„ç›®æ ‡æ˜¯å¯¹æ–°ç±»åˆ«è¿›è¡Œåˆ†å‰²ï¼Œä»…ä½¿ç”¨å°‘é‡æ ‡æ³¨å›¾åƒã€‚ä¸ºè§£å†³å½“å‰FSSæ–¹æ³•åœ¨éª¨å¹²å‡çº§ã€å¤„ç†ä¸åŒç±»å‹æ ‡æ³¨å’Œé€‚åº”ä¸åŒæ ‡æ³¨æ•°é‡ä¸Šçš„éš¾é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†DiffUpæ¡†æ¶ã€‚è¯¥æ¡†æ¶å°†FSSä»»åŠ¡æ¦‚å¿µåŒ–ä¸ºä¸€ä¸ªåŸºäºæ‰©æ•£è¿‡ç¨‹çš„æ¡ä»¶ç”Ÿæˆé—®é¢˜ã€‚é€šè¿‡å¼•å…¥ç‰¹å¾è½¬æ¢æ¨¡å—ã€è‡ªæ¡ä»¶è°ƒåˆ¶å—å’Œä¿¡æ¯èåˆæ¨¡å—ï¼ŒDiffUpæ¡†æ¶æ˜¾è‘—æé«˜äº†æ¨¡å‹çš„çµæ´»æ€§ã€å‡†ç¡®æ€§ä¸æ³›åŒ–èƒ½åŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>DiffUpæ¡†æ¶æ—¨åœ¨è§£å†³å°‘æ ·æœ¬åˆ†å‰²ï¼ˆFSSï¼‰ä¸­çš„ä¸‰ä¸ªä¸»è¦é—®é¢˜ï¼šéª¨å¹²å‡çº§çš„çµæ´»æ€§ã€å¤„ç†å¤šç§ç±»å‹æ ‡æ³¨çš„èƒ½åŠ›ä»¥åŠé€‚åº”ä¸åŒæ ‡æ³¨æ•°é‡çš„éš¾åº¦ã€‚</li>
<li>å¼•å…¥ç‰¹å¾è½¬æ¢æ¨¡å—ï¼Œå®ç°æ— ç¼éª¨å¹²å‡çº§è€Œæ— éœ€é‡æ–°è®­ç»ƒã€‚</li>
<li>ç”±äºä¸åŒæ ‡æ³¨ç±»å‹äº§ç”Ÿçš„å…ˆéªŒçŸ¥è¯†ç²’åº¦ä¸åŒï¼Œç±»æ¯”äºæ‰©æ•£æ¨¡å‹çš„ä¸åŒæ­¥éª¤ä¸­çš„å™ªå£°ä¸­é—´ä½“ï¼Œåˆ©ç”¨è‡ªæ¡ä»¶è°ƒåˆ¶å—ä¸åŒçº§è´¨é‡è°ƒåˆ¶åˆ†æ”¯è¿›è¡Œå¤„ç†ã€‚</li>
<li>çº³å…¥ä¸ç¡®å®šæ€§æ„ŸçŸ¥ä¿¡æ¯èåˆæ¨¡å—ï¼Œåè°ƒé›¶æ ·æœ¬ã€å•æ ·æœ¬å’Œå¤šæ ·æœ¬åœºæ™¯ä¸‹çš„å˜é‡ã€‚</li>
<li>DiffUpæ¡†æ¶å°†FSSä»»åŠ¡æ¦‚å¿µåŒ–ä¸ºæ¡ä»¶ç”Ÿæˆé—®é¢˜ï¼Œä½¿ç”¨æ‰©æ•£è¿‡ç¨‹æ¥å¤„ç†ã€‚</li>
<li>é€šè¿‡ä¸¥æ ¼çš„åŸºå‡†æµ‹è¯•ï¼ŒDiffUpåœ¨çµæ´»æ€§å’Œå‡†ç¡®æ€§æ–¹é¢æ˜¾è‘—ä¼˜äºç°æœ‰FSSæ¨¡å‹ã€‚</li>
<li>è¯¥æ¡†æ¶ä¸ºå¤„ç†å°‘æ ·æœ¬å­¦ä¹ ä¸­çš„åˆ†å‰²ä»»åŠ¡æä¾›äº†æ–°çš„è§†è§’å’Œè§£å†³æ–¹æ¡ˆã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2407.16182">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-d52f86b8a3a182fe409f5fe93fe2a0a9.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-bec40d5f377ed20b44e5dc02faa73a15.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="TerDiT-Ternary-Diffusion-Models-with-Transformers"><a href="#TerDiT-Ternary-Diffusion-Models-with-Transformers" class="headerlink" title="TerDiT: Ternary Diffusion Models with Transformers"></a>TerDiT: Ternary Diffusion Models with Transformers</h2><p><strong>Authors:Xudong Lu, Aojun Zhou, Ziyi Lin, Qi Liu, Yuhui Xu, Renrui Zhang, Xue Yang, Junchi Yan, Peng Gao, Hongsheng Li</strong></p>
<p>Recent developments in large-scale pre-trained text-to-image diffusion models have significantly improved the generation of high-fidelity images, particularly with the emergence of diffusion transformer models (DiTs). Among diffusion models, diffusion transformers have demonstrated superior image-generation capabilities, boosting lower FID scores and higher scalability. However, deploying large-scale DiT models can be expensive due to their excessive parameter numbers. Although existing research has explored efficient deployment techniques for diffusion models, such as model quantization, there is still little work concerning DiT-based models. To tackle this research gap, we propose TerDiT, the first quantization-aware training (QAT) and efficient deployment scheme for extremely low-bit diffusion transformer models. We focus on the ternarization of DiT networks, with model sizes ranging from 600M to 4.2B, and image resolution from 256$\times$256 to 512$\times$512. Our work contributes to the exploration of efficient deployment of large-scale DiT models, demonstrating the feasibility of training extremely low-bit DiT models from scratch while maintaining competitive image generation capacities compared to full-precision models. Our code and pre-trained TerDiT checkpoints have been released at <a target="_blank" rel="noopener" href="https://github.com/Lucky-Lance/TerDiT">https://github.com/Lucky-Lance/TerDiT</a>. </p>
<blockquote>
<p>è¿‘æœŸï¼Œå¤§å‹é¢„è®­ç»ƒæ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹çš„æœ€æ–°å‘å±•æå¤§åœ°æé«˜äº†é«˜ä¿çœŸå›¾åƒçš„ç”Ÿæˆèƒ½åŠ›ï¼Œç‰¹åˆ«æ˜¯éšç€æ‰©æ•£è½¬æ¢å™¨æ¨¡å‹ï¼ˆDiTsï¼‰çš„å‡ºç°ã€‚åœ¨æ‰©æ•£æ¨¡å‹ä¸­ï¼Œæ‰©æ•£è½¬æ¢å™¨è¡¨ç°å‡ºäº†å“è¶Šçš„å›¾è±¡ç”Ÿæˆèƒ½åŠ›ï¼Œæé«˜äº†è¾ƒä½çš„FIDåˆ†æ•°å’Œæ›´é«˜çš„å¯æ‰©å±•æ€§ã€‚ç„¶è€Œï¼Œç”±äºå‚æ•°æ•°é‡è¿‡å¤šï¼Œéƒ¨ç½²å¤§è§„æ¨¡çš„DiTæ¨¡å‹å¯èƒ½ä¼šå¾ˆæ˜‚è´µã€‚å°½ç®¡ç°æœ‰ç ”ç©¶å·²ç»æ¢ç´¢äº†æ‰©æ•£æ¨¡å‹çš„é«˜æ•ˆéƒ¨ç½²æŠ€æœ¯ï¼Œå¦‚æ¨¡å‹é‡åŒ–ï¼Œä½†å…³äºåŸºäºDiTçš„æ¨¡å‹çš„ç ”ç©¶å·¥ä½œä»ç„¶å¾ˆå°‘ã€‚ä¸ºäº†å¡«è¡¥è¿™ä¸€ç ”ç©¶ç©ºç™½ï¼Œæˆ‘ä»¬æå‡ºäº†TerDiTï¼Œè¿™æ˜¯ç¬¬ä¸€ä¸ªé’ˆå¯¹æä½ä½æ‰©æ•£è½¬æ¢å™¨æ¨¡å‹çš„æ•°é‡åŒ–æ„ŸçŸ¥è®­ç»ƒï¼ˆQATï¼‰å’Œé«˜æ•ˆéƒ¨ç½²æ–¹æ¡ˆã€‚æˆ‘ä»¬ä¸“æ³¨äºDiTç½‘ç»œçš„ä¸‰å…ƒåŒ–ï¼Œæ¶µç›–ä»6äº¿åˆ°4.2äº¿ä¸åŒå¤§å°çš„æ¨¡å‹å’Œä»256Ã—256åˆ°512Ã—512çš„å›¾åƒåˆ†è¾¨ç‡ã€‚æˆ‘ä»¬çš„ç ”ç©¶ä¸ºå¤§è§„æ¨¡DiTæ¨¡å‹çš„é«˜æ•ˆéƒ¨ç½²æ¢ç´¢åšå‡ºäº†è´¡çŒ®ï¼Œè¯æ˜äº†ä»å¤´å¼€å§‹è®­ç»ƒæä½ä½çš„DiTæ¨¡å‹çš„åŒæ—¶ä¿æŒä¸å…¨ç²¾åº¦æ¨¡å‹ç«äº‰çš„å›¾è±¡ç”Ÿæˆèƒ½åŠ›ã€‚æˆ‘ä»¬çš„ä»£ç å’Œé¢„å…ˆè®­ç»ƒçš„TerDiTæ£€æŸ¥ç‚¹å·²åœ¨<a target="_blank" rel="noopener" href="https://github.com/Lucky-Lance/TerDiT%E5%8F%91%E5%B8%83%E3%80%82">https://github.com/Lucky-Lance/TerDiTå‘å¸ƒã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2405.14854v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤§è§„æ¨¡é¢„è®­ç»ƒæ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹çš„æ–°å‘å±•ï¼Œå°¤å…¶æ˜¯æ‰©æ•£å˜å‹å™¨æ¨¡å‹ï¼ˆDiTsï¼‰çš„å‡ºç°ï¼Œæ˜¾è‘—æé«˜äº†é«˜ä¿çœŸå›¾åƒçš„ç”Ÿæˆèƒ½åŠ›ã€‚ä½†éƒ¨ç½²å¤§å‹DiTæ¨¡å‹æˆæœ¬é«˜æ˜‚ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºTerDiTï¼Œé¦–ä¸ªé’ˆå¯¹æä½ä½æ‰©æ•£å˜å‹å™¨æ¨¡å‹çš„é‡åŒ–æ„ŸçŸ¥è®­ç»ƒï¼ˆQATï¼‰å’Œé«˜æ•ˆéƒ¨ç½²æ–¹æ¡ˆã€‚æˆ‘ä»¬ä¸“æ³¨äºDiTç½‘ç»œçš„ä¸‰æ€åŒ–ï¼Œæ¨¡å‹è§„æ¨¡ä»6äº¿åˆ°42äº¿ä¸ç­‰ï¼Œå›¾åƒåˆ†è¾¨ç‡ä»256x256åˆ°512x512ã€‚TerDiTåœ¨ç»´æŒä¸å…¨ç²¾åº¦æ¨¡å‹ç›¸å½“çš„å›¾åƒç”Ÿæˆèƒ½åŠ›çš„åŒæ—¶ï¼Œå±•ç¤ºäº†ä»å¤´å¼€å§‹è®­ç»ƒæä½ä½DiTæ¨¡å‹çš„å¯è¡Œæ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ‰©æ•£æ¨¡å‹ä¸­çš„æ‰©æ•£å˜å‹å™¨ï¼ˆDiTï¼‰åœ¨å›¾åƒç”Ÿæˆæ–¹é¢è¡¨ç°å‡ºå“è¶Šæ€§èƒ½ï¼Œå…·æœ‰æ›´ä½çš„FIDåˆ†æ•°å’Œæ›´é«˜çš„å¯æ‰©å±•æ€§ã€‚</li>
<li>å¤§å‹DiTæ¨¡å‹çš„éƒ¨ç½²æˆæœ¬é«˜æ˜‚ï¼Œéœ€è¦æ¢ç´¢é«˜æ•ˆéƒ¨ç½²æŠ€æœ¯ã€‚</li>
<li>TerDiTæ˜¯é¦–ä¸ªé’ˆå¯¹æä½ä½æ‰©æ•£å˜å‹å™¨æ¨¡å‹çš„é‡åŒ–æ„ŸçŸ¥è®­ç»ƒï¼ˆQATï¼‰å’Œé«˜æ•ˆéƒ¨ç½²æ–¹æ¡ˆã€‚</li>
<li>TerDiTä¸“æ³¨äºDiTç½‘ç»œçš„ä¸‰æ€åŒ–ï¼Œæ¶‰åŠä¸åŒè§„æ¨¡æ¨¡å‹å’Œå›¾åƒåˆ†è¾¨ç‡ã€‚</li>
<li>TerDiTåœ¨ç»´æŒä¸å…¨ç²¾åº¦æ¨¡å‹ç›¸å½“çš„å›¾åƒç”Ÿæˆèƒ½åŠ›çš„åŒæ—¶ï¼Œå®ç°äº†ä»æºå¤´è®­ç»ƒæä½ä½DiTæ¨¡å‹çš„å¯è¡Œæ€§ã€‚</li>
<li>TerDiTçš„æºä»£ç å’Œé¢„å…ˆè®­ç»ƒå¥½çš„æ¨¡å‹æ£€æŸ¥ç‚¹å·²ç»å…¬å¼€å‘å¸ƒåœ¨<a target="_blank" rel="noopener" href="https://github.com/Lucky-Lance/TerDiT%E3%80%82">https://github.com/Lucky-Lance/TerDiTã€‚</a></li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2405.14854">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-15982d980298f159f7e2ec0a0081f5c4.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-da46ddc1539470d2d977a06d6fdfeacd.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-37fd028a6a60208de666fecc09af647e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ce3fc508bd173ebbd93a9edfa5d3b1a1.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-2d64d7ae83cae0235916cd048c84c3f4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c3c6341931063abb54c47debc0495323.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-04-09/Diffusion%20Models/">https://kedreamix.github.io/Talk2Paper/Paper/2025-04-09/Diffusion%20Models/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Diffusion-Models/">
                                    <span class="chip bg-color">Diffusion Models</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-04-09/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-d9964f7ed391a1d68c160081be409351.jpg" class="responsive-img" alt="åŒ»å­¦å›¾åƒ">
                        
                        <span class="card-title">åŒ»å­¦å›¾åƒ</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            åŒ»å­¦å›¾åƒ æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-04-09  Federated Learning for Medical Image Classification A Comprehensive   Benchmark
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-04-09
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/" class="post-category">
                                    åŒ»å­¦å›¾åƒ
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                        <span class="chip bg-color">åŒ»å­¦å›¾åƒ</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-04-09/NeRF/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-f57da3615c8eb9d74925d80c987ef4f2.jpg" class="responsive-img" alt="NeRF">
                        
                        <span class="card-title">NeRF</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            NeRF æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-04-09  DeclutterNeRF Generative-Free 3D Scene Recovery for Occlusion Removal
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-04-09
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/NeRF/" class="post-category">
                                    NeRF
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/NeRF/">
                        <span class="chip bg-color">NeRF</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">23667.4k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
