<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="检测/分割/跟踪">
    <meta name="description" content="检测/分割/跟踪 方向最新论文已更新，请持续关注 Update in 2025-04-09  SSLFusion Scale &amp; Space Aligned Latent Fusion Model for Multimodal 3D   Object Detection">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>检测/分割/跟踪 | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loader页面消失采用渐隐的方式*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logo出现动画 */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//使用渐隐的方法淡出loading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//强制显示loading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">嘘~  正在从服务器偷取页面 . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>首页</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>标签</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>分类</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>归档</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="搜索" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="深色/浅色模式" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			首页
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			标签
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			分类
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			归档
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pic1.zhimg.com/v2-b0bdf034384f8da4962c37811fdb7ccf.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">检测/分割/跟踪</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- 文章内容详情 -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/%E6%A3%80%E6%B5%8B-%E5%88%86%E5%89%B2-%E8%B7%9F%E8%B8%AA/">
                                <span class="chip bg-color">检测/分割/跟踪</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/%E6%A3%80%E6%B5%8B-%E5%88%86%E5%89%B2-%E8%B7%9F%E8%B8%AA/" class="post-category">
                                检测/分割/跟踪
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp;
                    2025-04-09
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>更新日期:&nbsp;&nbsp;
                    2025-05-14
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>文章字数:&nbsp;&nbsp;
                    7k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>阅读时长:&nbsp;&nbsp;
                    28 分
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>阅读次数:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>⚠️ 以下所有内容总结都来自于 大语言模型的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p>
</blockquote>
<h1 id="2025-04-09-更新"><a href="#2025-04-09-更新" class="headerlink" title="2025-04-09 更新"></a>2025-04-09 更新</h1><h2 id="SSLFusion-Scale-Space-Aligned-Latent-Fusion-Model-for-Multimodal-3D-Object-Detection"><a href="#SSLFusion-Scale-Space-Aligned-Latent-Fusion-Model-for-Multimodal-3D-Object-Detection" class="headerlink" title="SSLFusion: Scale &amp; Space Aligned Latent Fusion Model for Multimodal 3D   Object Detection"></a>SSLFusion: Scale &amp; Space Aligned Latent Fusion Model for Multimodal 3D   Object Detection</h2><p><strong>Authors:Bonan Ding, Jin Xie, Jing Nie, Jiale Cao</strong></p>
<p>Multimodal 3D object detection based on deep neural networks has indeed made significant progress. However, it still faces challenges due to the misalignment of scale and spatial information between features extracted from 2D images and those derived from 3D point clouds. Existing methods usually aggregate multimodal features at a single stage. However, leveraging multi-stage cross-modal features is crucial for detecting objects of various scales. Therefore, these methods often struggle to integrate features across different scales and modalities effectively, thereby restricting the accuracy of detection. Additionally, the time-consuming Query-Key-Value-based (QKV-based) cross-attention operations often utilized in existing methods aid in reasoning the location and existence of objects by capturing non-local contexts. However, this approach tends to increase computational complexity. To address these challenges, we present SSLFusion, a novel Scale &amp; Space Aligned Latent Fusion Model, consisting of a scale-aligned fusion strategy (SAF), a 3D-to-2D space alignment module (SAM), and a latent cross-modal fusion module (LFM). SAF mitigates scale misalignment between modalities by aggregating features from both images and point clouds across multiple levels. SAM is designed to reduce the inter-modal gap between features from images and point clouds by incorporating 3D coordinate information into 2D image features. Additionally, LFM captures cross-modal non-local contexts in the latent space without utilizing the QKV-based attention operations, thus mitigating computational complexity. Experiments on the KITTI and DENSE datasets demonstrate that our SSLFusion outperforms state-of-the-art methods. Our approach obtains an absolute gain of 2.15% in 3D AP, compared with the state-of-art method GraphAlign on the moderate level of the KITTI test set. </p>
<blockquote>
<p>基于深度神经网络的多模态三维物体检测确实已经取得了显著的进展。然而，由于从二维图像提取的特征与从三维点云衍生的特征在规模和空间信息上的不匹配，它仍然面临挑战。现有方法通常在单一阶段聚合多模态特征。然而，利用多阶段的跨模态特征对于检测各种规模的目标至关重要。因此，这些方法在整合不同规模和模态的特征时往往面临困难，从而限制了检测精度。此外，现有方法中常用的基于查询-键-值（QKV）的跨注意力操作虽然有助于通过捕捉非局部上下文来推理物体的位置和存在，但这种方法往往会增加计算复杂性。为了应对这些挑战，我们提出了SSLFusion，这是一种新的规模和空间对齐潜在融合模型，包括规模对齐融合策略（SAF）、3D到2D空间对齐模块（SAM）和潜在跨模态融合模块（LFM）。SAF通过在不同层级上聚合图像和点云的特征，缓解模态之间的规模不匹配问题。SAM通过融入3D坐标信息到2D图像特征中，旨在缩小图像和点云特征之间的跨模态差距。此外，LFM在潜在空间中捕捉跨模态的非局部上下文，而不使用QKV基于注意力的操作，从而缓解计算复杂性。在KITTI和DENSE数据集上的实验表明，我们的SSLFusion优于最新方法。我们的方法在KITTI测试集的适度水平上，与最新方法GraphAlign相比，3D AP绝对提升了2.15%。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.05170v1">PDF</a> Accepted by AAAI 2025</p>
<p><strong>Summary</strong><br>基于深度神经网络的多模态三维物体检测技术已经取得显著进展，但仍面临二维图像和三维点云特征尺度与空间信息不对齐的挑战。现有方法通常在单一阶段进行多模态特征聚合，难以有效整合不同尺度和模态的特征，影响检测准确性。针对这些问题，我们提出了SSLFusion模型，包括尺度对齐融合策略（SAF）、三维到二维空间对齐模块（SAM）和潜在跨模态融合模块（LFM）。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>多模态三维物体检测基于深度神经网络有显著的进步，但存在特征和尺度不对齐的问题。</li>
<li>现有方法通常在单一阶段进行多模态特征聚合，难以检测不同尺度的物体。</li>
<li>SSLFusion模型包括尺度对齐融合策略（SAF），用于解决跨模态的尺度不对齐问题。</li>
<li>三维到二维空间对齐模块（SAM）通过融入三维坐标信息来缩小跨模态特征之间的差距。</li>
<li>潜在跨模态融合模块（LFM）能够在潜在空间中捕获跨模态的非局部上下文，同时不增加计算复杂性。</li>
<li>SSLFusion模型在KITTI和DENSE数据集上的表现优于现有最先进的检测方法。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.05170">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-35b7ed9d4ec41b7cfee1d53a24f4a90a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e2e10343a1a3c7299709152b1bc11d77.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-bcac2332f9cd07ae12e69adde284b0a3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-642b151a4e155f8a43f0327fdfc06c1f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6e3d18f077a48b7b1a76d7e72bde0f28.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-727abfa8d0044d6c63863893e0cccb88.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-7350f0ddf8cfe0675c059d476861db3a.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="DFormerv2-Geometry-Self-Attention-for-RGBD-Semantic-Segmentation"><a href="#DFormerv2-Geometry-Self-Attention-for-RGBD-Semantic-Segmentation" class="headerlink" title="DFormerv2: Geometry Self-Attention for RGBD Semantic Segmentation"></a>DFormerv2: Geometry Self-Attention for RGBD Semantic Segmentation</h2><p><strong>Authors:Bo-Wen Yin, Jiao-Long Cao, Ming-Ming Cheng, Qibin Hou</strong></p>
<p>Recent advances in scene understanding benefit a lot from depth maps because of the 3D geometry information, especially in complex conditions (e.g., low light and overexposed). Existing approaches encode depth maps along with RGB images and perform feature fusion between them to enable more robust predictions. Taking into account that depth can be regarded as a geometry supplement for RGB images, a straightforward question arises: Do we really need to explicitly encode depth information with neural networks as done for RGB images? Based on this insight, in this paper, we investigate a new way to learn RGBD feature representations and present DFormerv2, a strong RGBD encoder that explicitly uses depth maps as geometry priors rather than encoding depth information with neural networks. Our goal is to extract the geometry clues from the depth and spatial distances among all the image patch tokens, which will then be used as geometry priors to allocate attention weights in self-attention. Extensive experiments demonstrate that DFormerv2 exhibits exceptional performance in various RGBD semantic segmentation benchmarks. Code is available at: <a target="_blank" rel="noopener" href="https://github.com/VCIP-RGBD/DFormer">https://github.com/VCIP-RGBD/DFormer</a>. </p>
<blockquote>
<p>最近，场景理解的进步在很大程度上得益于深度图所包含的3D几何信息，特别是在复杂条件下（例如低光和过度曝光）。现有方法将深度图与RGB图像进行编码，并在两者之间执行特征融合，以实现更稳健的预测。考虑到深度可被视为RGB图像的几何补充，一个直接的问题出现了：我们是否真的需要将深度信息像处理RGB图像那样显式地编码到神经网络中？基于这一见解，本文研究了一种新的RGBD特征表示学习方法，并推出了DFormerv2，这是一种强大的RGBD编码器，它显式地使用深度图作为几何先验，而不是通过神经网络编码深度信息。我们的目标是从深度和所有图像补丁标记之间的空间距离中提取几何线索，然后将这些线索用作几何先验来分配自注意力中的注意力权重。大量实验表明，DFormerv2在各种RGBD语义分割基准测试中表现出卓越的性能。代码可在以下网址找到：<a target="_blank" rel="noopener" href="https://github.com/VCIP-RGBD/DFormer%E3%80%82">https://github.com/VCIP-RGBD/DFormer。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.04701v1">PDF</a> Accepted by CVPR 2025</p>
<p><strong>Summary</strong></p>
<p>本文探讨了RGBD特征表示的新学习方法，提出了一种名为DFormerv2的强RGBD编码器，它利用深度图作为几何先验信息，而不是通过神经网络对深度信息进行编码。该研究的目标是从深度图和图像块令牌间的空间距离中提取几何线索，将其作为几何先验信息用于分配自注意力中的注意力权重。实验表明，DFormerv2在多种RGBD语义分割基准测试中表现出卓越性能。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>DFormerv2利用深度图作为几何先验信息，提出了新的RGBD特征表示学习方法。</li>
<li>现有方法通常将深度图与RGB图像一起编码，并进行特征融合，而DFormerv2则将深度信息直接用于分配自注意力中的权重。</li>
<li>该方法从深度图和空间距离中提取几何线索，以增强图像理解。</li>
<li>DFormerv2在多种RGBD语义分割基准测试中表现出卓越性能。</li>
<li>该研究强调了深度信息在复杂条件下的重要性，如低光和过曝光环境。</li>
<li>DFormerv2的目标是通过利用深度图的几何信息来改进RGB图像的表示能力。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.04701">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-71d0ee06075620376b456cef8464cf22.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-6ea1145438cafaf3eb6e4e124dcf955e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c295887bec639d85a9a63f4e9b1f058f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-2343e404f7acb1ab7324f17fef6f3c81.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8f3ad0e8f3d76a1c86042ce78f904690.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c1314d817d2dc531996637685d5e37b6.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Enhance-Then-Search-An-Augmentation-Search-Strategy-with-Foundation-Models-for-Cross-Domain-Few-Shot-Object-Detection"><a href="#Enhance-Then-Search-An-Augmentation-Search-Strategy-with-Foundation-Models-for-Cross-Domain-Few-Shot-Object-Detection" class="headerlink" title="Enhance Then Search: An Augmentation-Search Strategy with Foundation   Models for Cross-Domain Few-Shot Object Detection"></a>Enhance Then Search: An Augmentation-Search Strategy with Foundation   Models for Cross-Domain Few-Shot Object Detection</h2><p><strong>Authors:Jiancheng Pan, Yanxing Liu, Xiao He, Long Peng, Jiahao Li, Yuze Sun, Xiaomeng Huang</strong></p>
<p>Foundation models pretrained on extensive datasets, such as GroundingDINO and LAE-DINO, have performed remarkably in the cross-domain few-shot object detection (CD-FSOD) task. Through rigorous few-shot training, we found that the integration of image-based data augmentation techniques and grid-based sub-domain search strategy significantly enhances the performance of these foundation models. Building upon GroundingDINO, we employed several widely used image augmentation methods and established optimization objectives to effectively navigate the expansive domain space in search of optimal sub-domains. This approach facilitates efficient few-shot object detection and introduces an approach to solving the CD-FSOD problem by efficiently searching for the optimal parameter configuration from the foundation model. Our findings substantially advance the practical deployment of vision-language models in data-scarce environments, offering critical insights into optimizing their cross-domain generalization capabilities without labor-intensive retraining. Code is available at <a target="_blank" rel="noopener" href="https://github.com/jaychempan/ETS">https://github.com/jaychempan/ETS</a>. </p>
<blockquote>
<p>基于大规模数据集预训练的模型，如GroundingDINO和LAE-DINO，在跨域少样本目标检测（CD-FSOD）任务中表现突出。通过严格的少样本训练，我们发现结合基于图像的数据增强技术和基于网格的子域搜索策略，可以显著提高这些基础模型的性能。在GroundingDINO的基础上，我们采用了几种常用的图像增强方法，并建立了优化目标，以有效地遍历庞大的域空间，寻找最佳子域。这种方法促进了高效少样本目标检测，并通过从基础模型中有效搜索最佳参数配置来解决CD-FSOD问题。我们的研究为在数据稀缺环境中部署视觉语言模型提供了重大进展，并为在不进行劳动密集型再训练的情况下优化其跨域泛化能力提供了关键见解。代码可在<a target="_blank" rel="noopener" href="https://github.com/jaychempan/ETS%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/jaychempan/ETS找到。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.04517v1">PDF</a> 9 pages, 6 figures</p>
<p><strong>Summary</strong></p>
<p>基于大规模数据集预训练的模型，如GroundingDINO和LAE-DINO，在跨域少样本目标检测任务中表现出卓越性能。研究通过严格少样本训练发现，图像数据增强技术与基于网格的子域搜索策略的结合，显著提升了这些预训练模型的性能。在GroundingDINO基础上，研究采用多种常用图像增强方法并建立优化目标，以有效遍历广阔域空间并寻找最佳子域，为解决CD-FSOD问题提供了一种方法。该研究有助于高效少样本目标检测，并为优化视觉语言模型在数据稀缺环境中的跨域泛化能力提供了关键见解。代码已公开于GitHub上。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>预训练的模型，如GroundingDINO和LAE-DINO，在跨域少样本目标检测中表现出优异性能。</li>
<li>图像数据增强技术结合基于网格的子域搜索策略显著提升了模型性能。</li>
<li>在GroundingDINO基础上，采用多种图像增强方法和优化目标，有效搜索最佳子域。</li>
<li>该方法为解决CD-FSOD问题提供了一种有效途径。</li>
<li>研究有助于提高少样本目标检测的效率和准确性。</li>
<li>研究为优化视觉语言模型在数据稀缺环境中的跨域泛化能力提供了关键见解。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.04517">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-8c564e6d6460ed39ed6dc4ae1d09760e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-abb71328397a8a6629b09d0f5deca41d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2b97486cbb59d8ed4ff67480f97f9709.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-b6681ab63f960004e9c81babead169da.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-448d085c3070c8f578c770982352a618.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6652620b7293f64921df0d7bc485518e.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Prompt-Categories-Cluster-for-Weakly-Supervised-Semantic-Segmentation"><a href="#Prompt-Categories-Cluster-for-Weakly-Supervised-Semantic-Segmentation" class="headerlink" title="Prompt Categories Cluster for Weakly Supervised Semantic Segmentation"></a>Prompt Categories Cluster for Weakly Supervised Semantic Segmentation</h2><p><strong>Authors:Wangyu Wu, Xianglin Qiu, Siqi Song, Xiaowei Huang, Fei Ma, Jimin Xiao</strong></p>
<p>Weakly Supervised Semantic Segmentation (WSSS), which leverages image-level labels, has garnered significant attention due to its cost-effectiveness. The previous methods mainly strengthen the inter-class differences to avoid class semantic ambiguity which may lead to erroneous activation. However, they overlook the positive function of some shared information between similar classes. Categories within the same cluster share some similar features. Allowing the model to recognize these features can further relieve the semantic ambiguity between these classes. To effectively identify and utilize this shared information, in this paper, we introduce a novel WSSS framework called Prompt Categories Clustering (PCC). Specifically, we explore the ability of Large Language Models (LLMs) to derive category clusters through prompts. These clusters effectively represent the intrinsic relationships between categories. By integrating this relational information into the training network, our model is able to better learn the hidden connections between categories. Experimental results demonstrate the effectiveness of our approach, showing its ability to enhance performance on the PASCAL VOC 2012 dataset and surpass existing state-of-the-art methods in WSSS. </p>
<blockquote>
<p>弱监督语义分割（WSSS）利用图像级别的标签，因其成本效益而备受关注。之前的方法主要强调类间差异，以避免可能导致错误激活的类语义模糊。然而，他们忽视了类似类别之间共享信息的积极作用。同一聚类中的类别共享一些相似特征。允许模型识别这些特征可以进一步缓解这些类别之间的语义模糊。为了有效识别和利用这些共享信息，本文引入了一种新型的WSSS框架，称为提示类别聚类（PCC）。具体来说，我们探索了大型语言模型（LLM）通过提示推导类别聚类的能力。这些聚类有效地代表了类别之间的内在关系。通过将这种关系信息集成到训练网络中，我们的模型能够更好地学习类别之间的隐藏连接。实验结果证明了我们的方法的有效性，表明其在PASCAL VOC 2012数据集上的性能有所提升，并超越了现有的WSSS先进方法。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.13823v2">PDF</a> Accepted at CVPR 2025 ELVM</p>
<p><strong>摘要</strong></p>
<p>基于图像级标签的弱监督语义分割（WSSS）因其成本效益而受到广泛关注。以往的方法主要强化类间差异以避免语义模糊导致的错误激活，但忽略了相似类别间共享信息的积极作用。本文引入了一种名为Prompt Categories Clustering（PCC）的新型WSSS框架，能够识别并利用类别间的共享信息。通过大型语言模型（LLM）的提示能力，我们探索了类别聚类的形成，这些聚类有效地代表了类别之间的内在关系。通过将这种关系信息整合到训练网络中，我们的模型能够更好地学习类别之间的隐藏联系。实验结果表明，我们的方法在PASCAL VOC 2012数据集上的性能有所提升，并超越了现有的WSSS领域最先进的方法。</p>
<p><strong>要点速览</strong></p>
<ul>
<li>WSSS方法利用图像级标签，旨在解决语义模糊问题。</li>
<li>以往方法主要强化类间差异，但忽略了相似类别间共享信息的积极作用。</li>
<li>本研究引入了Prompt Categories Clustering（PCC）框架，利用大型语言模型识别类别间的共享信息。</li>
<li>PCC通过形成类别聚类来代表类别间的内在关系。</li>
<li>将关系信息整合到训练网络中，提升模型学习类别间隐藏联系的能力。</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.13823">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-db08b845ae6f3b0bc4429182ec8cb64d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7fc0d8b5c152b0b199031e611bca1f22.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-78a3be64951c4c0690c870c14da38a1a.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="LUIEO-A-Lightweight-Model-for-Integrating-Underwater-Image-Enhancement-and-Object-Detection"><a href="#LUIEO-A-Lightweight-Model-for-Integrating-Underwater-Image-Enhancement-and-Object-Detection" class="headerlink" title="LUIEO: A Lightweight Model for Integrating Underwater Image Enhancement   and Object Detection"></a>LUIEO: A Lightweight Model for Integrating Underwater Image Enhancement   and Object Detection</h2><p><strong>Authors:Bin Li, Li Li, Zhenwei Zhang, Yuping Duan</strong></p>
<p>Underwater optical images inevitably suffer from various degradation factors such as blurring, low contrast, and color distortion, which hinder the accuracy of object detection tasks. Due to the lack of paired underwater&#x2F;clean images, most research methods adopt a strategy of first enhancing and then detecting, resulting in a lack of feature communication between the two learning tasks. On the other hand, due to the contradiction between the diverse degradation factors of underwater images and the limited number of samples, existing underwater enhancement methods are difficult to effectively enhance degraded images of unknown water bodies, thereby limiting the improvement of object detection accuracy. Therefore, most underwater target detection results are still displayed on degraded images, making it difficult to visually judge the correctness of the detection results. To address the above issues, this paper proposes a multi-task learning method that simultaneously enhances underwater images and improves detection accuracy. Compared with single-task learning, the integrated model allows for the dynamic adjustment of information communication and sharing between different tasks. Due to the fact that real underwater images can only provide annotated object labels, this paper introduces physical constraints to ensure that object detection tasks do not interfere with image enhancement tasks. Therefore, this article introduces a physical module to decompose underwater images into clean images, background light, and transmission images and uses a physical model to calculate underwater images for self-supervision. Numerical experiments demonstrate that the proposed model achieves satisfactory results in visual performance, object detection accuracy, and detection efficiency compared to state-of-the-art comparative methods. </p>
<blockquote>
<p>水下光学图像不可避免地受到模糊、低对比度和颜色失真等多种降质因素的影响，从而阻碍了目标检测任务的准确性。由于缺少配对的水下&#x2F;清洁图像，大多数研究方法采用先增强后检测的策略，导致两个学习任务之间缺乏特征交流。另一方面，由于水下图像的各种降质因素与样本数量有限之间的矛盾，现有的水下增强方法难以有效增强未知水体的退化图像，从而限制了目标检测准确度的提高。因此，大多数水下目标检测结果仍显示在退化图像上，很难直观判断检测结果的正确性。为了解决上述问题，本文提出了一种多任务学习方法，同时增强水下图像并提高检测准确性。与单任务学习相比，集成模型允许不同任务之间动态调整信息通信和共享。由于真实水下图像只能提供注释的对象标签，本文引入物理约束以确保目标检测任务不会干扰图像增强任务。因此，本文引入了一个物理模块来将水下图像分解为清洁图像、背景光和传输图像，并使用物理模型计算水下图像进行自我监督。数值实验表明，与最先进的比较方法相比，所提模型在视觉性能、目标检测准确性和检测效率方面都取得了令人满意的结果。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.07009v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文提出一种多任务学习方法，旨在同时提升水下图像质量并提高其检测精度。面对水下图像模糊、对比度低、色彩失真等退化问题，以及缺乏配对的水下&#x2F;清晰图像样本，该研究通过引入物理约束模块分解水下图像为清洁图像、背景光和透射图像，并利用物理模型计算水下图像进行自我监督，实现动态调整不同任务间的信息共享与交流。实验表明，该模型在视觉性能、目标检测精度和检测效率方面均取得满意结果。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>水下图像退化问题如模糊、低对比度和色彩失真等，影响目标检测的准确性。</li>
<li>缺乏配对的水下&#x2F;清晰图像样本，使得现有水下增强方法在未知水体中的图像增强效果有限。</li>
<li>提出一种多任务学习方法，同时增强水下图像并提升检测精度。</li>
<li>引入物理约束模块分解水下图像，便于自我监督和学习。</li>
<li>模型通过动态调整不同任务间的信息共享与交流，优化检测效果。</li>
<li>实验证明该模型在视觉性能、目标检测精度和检测效率上表现优异。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.07009">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-9b6214872956c977f479a0f7d339ddfe.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-384e68499488a0c885e7e2bd1e85533b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d4da24f58bd8a02392303291144d0fa4.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a5b64145dbd3e42a6714aac945c01e61.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-2765fe9af753475265324ca57f60103f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-8a52cdf41c73bdc2a2676f5e2ec8a8d8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-fac922265dc3184831cd349fccf0d438.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Distilling-Spectral-Graph-for-Object-Context-Aware-Open-Vocabulary-Semantic-Segmentation"><a href="#Distilling-Spectral-Graph-for-Object-Context-Aware-Open-Vocabulary-Semantic-Segmentation" class="headerlink" title="Distilling Spectral Graph for Object-Context Aware Open-Vocabulary   Semantic Segmentation"></a>Distilling Spectral Graph for Object-Context Aware Open-Vocabulary   Semantic Segmentation</h2><p><strong>Authors:Chanyoung Kim, Dayun Ju, Woojung Han, Ming-Hsuan Yang, Seong Jae Hwang</strong></p>
<p>Open-Vocabulary Semantic Segmentation (OVSS) has advanced with recent vision-language models (VLMs), enabling segmentation beyond predefined categories through various learning schemes. Notably, training-free methods offer scalable, easily deployable solutions for handling unseen data, a key goal of OVSS. Yet, a critical issue persists: lack of object-level context consideration when segmenting complex objects in the challenging environment of OVSS based on arbitrary query prompts. This oversight limits models’ ability to group semantically consistent elements within object and map them precisely to user-defined arbitrary classes. In this work, we introduce a novel approach that overcomes this limitation by incorporating object-level contextual knowledge within images. Specifically, our model enhances intra-object consistency by distilling spectral-driven features from vision foundation models into the attention mechanism of the visual encoder, enabling semantically coherent components to form a single object mask. Additionally, we refine the text embeddings with zero-shot object presence likelihood to ensure accurate alignment with the specific objects represented in the images. By leveraging object-level contextual knowledge, our proposed approach achieves state-of-the-art performance with strong generalizability across diverse datasets. </p>
<blockquote>
<p>开放词汇语义分割（OVSS）随着最近的视觉语言模型（VLMs）的发展而进步，通过各种学习方案实现了超出预定类别的分割。值得注意的是，无训练方法为处理未见数据提供了可扩展、易于部署的解决方案，这是OVSS的关键目标。然而，一个关键问题依然存在：在基于任意查询提示的OVSS的复杂环境中，对复杂对象进行分割时缺乏对象级别的上下文考虑。这种疏忽限制了模型在对象内组合语义一致元素的能力，并准确地将它们映射到用户定义的任意类别。在这项工作中，我们介绍了一种克服这一限制的新方法，该方法通过在图像中融入对象级别的上下文知识。具体来说，我们的模型通过从视觉基础模型中提炼光谱驱动特征并将其蒸馏到视觉编码器的注意力机制中，增强了对象内部的连贯性，使得语义一致的组件能够形成单个对象掩码。此外，我们还通过零样本对象存在概率对文本嵌入进行了精炼，以确保与图像中表示的特定对象的准确对齐。通过利用对象级别的上下文知识，我们提出的方法在多个数据集上实现了最先进的性能，并具有较强的泛化能力。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2411.17150v3">PDF</a> </p>
<p><strong>Summary</strong><br>本文主要介绍了开放词汇语义分割（OVSS）的最新进展，特别是如何利用视觉语言模型（VLMs）的先进技术实现超越预定类别的分割。文章重点介绍了一种新的方法，通过融入对象级别的上下文知识来解决在开放词汇语义分割中因任意查询提示而导致的复杂对象分割问题。该方法提高了模型的性能，使模型能够在各种数据集上实现最先进的性能并具有强大的泛化能力。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>开放词汇语义分割（OVSS）借助视觉语言模型（VLMs）的先进技术，实现了超越预定类别的分割。</li>
<li>训练免费的方法为处理未见数据提供了可伸缩和易于部署的解决方案，这是OVSS的关键目标。</li>
<li>当前存在的问题是，在基于任意查询提示的开放词汇语义分割的复杂环境中，缺乏对象级别的上下文考虑。</li>
<li>新方法通过融入对象级别的上下文知识来解决此问题，提高了模型对对象内部一致性的表现。</li>
<li>该方法通过提炼视觉基础模型的频谱驱动特征并注入视觉编码器的注意力机制来实现。</li>
<li>对文本嵌入进行改进，使用零样本对象存在可能性来确保与图像中表示的特定对象的准确对齐。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2411.17150">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-38ce5f2d9890b4959a80e23e4bff8955.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-5b17542025b8df1633d8f8c54cbb60b5.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b0bdf034384f8da4962c37811fdb7ccf.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-70e98ebfbdecc978c059d2cb079af8da.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e5470087ecfe696acad0206281682d35.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="A-Survey-and-Evaluation-of-Adversarial-Attacks-for-Object-Detection"><a href="#A-Survey-and-Evaluation-of-Adversarial-Attacks-for-Object-Detection" class="headerlink" title="A Survey and Evaluation of Adversarial Attacks for Object Detection"></a>A Survey and Evaluation of Adversarial Attacks for Object Detection</h2><p><strong>Authors:Khoi Nguyen Tiet Nguyen, Wenyu Zhang, Kangkang Lu, Yuhuan Wu, Xingjian Zheng, Hui Li Tan, Liangli Zhen</strong></p>
<p>Deep learning models achieve remarkable accuracy in computer vision tasks, yet remain vulnerable to adversarial examples–carefully crafted perturbations to input images that can deceive these models into making confident but incorrect predictions. This vulnerability pose significant risks in high-stakes applications such as autonomous vehicles, security surveillance, and safety-critical inspection systems. While the existing literature extensively covers adversarial attacks in image classification, comprehensive analyses of such attacks on object detection systems remain limited. This paper presents a novel taxonomic framework for categorizing adversarial attacks specific to object detection architectures, synthesizes existing robustness metrics, and provides a comprehensive empirical evaluation of state-of-the-art attack methodologies on popular object detection models, including both traditional detectors and modern detectors with vision-language pretraining. Through rigorous analysis of open-source attack implementations and their effectiveness across diverse detection architectures, we derive key insights into attack characteristics. Furthermore, we delineate critical research gaps and emerging challenges to guide future investigations in securing object detection systems against adversarial threats. Our findings establish a foundation for developing more robust detection models while highlighting the urgent need for standardized evaluation protocols in this rapidly evolving domain. </p>
<blockquote>
<p>深度学习模型在计算机视觉任务中取得了令人瞩目的准确性，但仍然容易受到对抗样本的威胁。对抗样本是对输入图像进行精心制作的扰动，可以欺骗这些模型做出自信但错误的预测。这种脆弱性在高风险应用（如自动驾驶、安全监控和关键安全检测系统）中构成了重大风险。尽管现有文献广泛涵盖了图像分类中的对抗性攻击，但对目标检测系统中此类攻击的综合分析仍然有限。本文提出了一个针对目标检测架构的对抗性攻击的新型分类框架，对现有的稳健性指标进行了综合，对流行的目标检测模型上的最新攻击方法进行了全面的经验评估，包括传统检测器和使用视觉语言预训练的现代检测器。通过对开源攻击实现及其在不同检测架构中的有效性进行严谨分析，我们获得了关于攻击特性的关键见解。此外，我们指出了关键的研究空白和新兴挑战，为未来的研究提供了指导，以保护目标检测系统免受对抗性威胁。我们的研究为开发更稳健的检测模型奠定了基础，同时强调了在这一快速发展领域制定标准化评估协议的紧迫需求。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2408.01934v4">PDF</a> Accepted for publication in the IEEE Transactions on Neural Networks   and Learning Systems (TNNLS)</p>
<p><strong>Summary</strong></p>
<p>深度学习模型在计算机视觉任务中取得了惊人的准确性，但在面临对抗样本时仍显得脆弱。对抗样本是精心制作的输入图像的扰动，可以欺骗模型做出自信但错误的预测。这一漏洞在高风险应用（如自动驾驶、安全监控和关键安全检查系统）中构成重大风险。本文提出了针对对象检测架构的对抗攻击的新型分类框架，综合了现有的稳健性指标，并对流行对象检测模型上的最新攻击方法进行了全面的实证评估，包括传统检测器和具有视觉语言预训练的现代检测器。通过对开源攻击实现的严格分析以及它们在各种检测架构中的有效性，我们获得了关于攻击特性的关键见解。我们的发现为开发更稳健的检测模型奠定了基础，同时强调了在这一快速发展领域中对标准化评估协议的迫切需求。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>深度学习模型在计算机视觉任务中表现出色，但在面对对抗样本时仍容易出错。</li>
<li>对抗样本可以欺骗模型做出自信但错误的预测，这在高风险应用中构成重大风险。</li>
<li>当前文献对图像分类中的对抗攻击进行了广泛覆盖，但对对象检测系统中的对抗攻击的综合分析仍然有限。</li>
<li>本文提出了针对对象检测架构的对抗攻击的新型分类框架。</li>
<li>综合了现有的稳健性指标，并对最新攻击方法在流行对象检测模型上的表现进行了全面评估。</li>
<li>通过对开源攻击实施的严格分析，获得了关于攻击特性的关键见解。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2408.01934">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-67ab0cf66053c977b94a36e3e0975279.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-cee0fb9ffa038544d3d77616647ca7f4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-737ba1841bf835060cb5f0aa6d336ee5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7c530ba2881a7bf706d2fdcb528cd247.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ee6fec97784057899f8d30b2277adfd7.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e436f56f2fb30525a668208ad651dac4.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        文章作者:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        文章链接:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-04-09/%E6%A3%80%E6%B5%8B_%E5%88%86%E5%89%B2_%E8%B7%9F%E8%B8%AA/">https://kedreamix.github.io/Talk2Paper/Paper/2025-04-09/%E6%A3%80%E6%B5%8B_%E5%88%86%E5%89%B2_%E8%B7%9F%E8%B8%AA/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        版权声明:
                    </i>
                </span>
                <span class="reprint-info">
                    本博客所有文章除特別声明外，均采用
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    许可协议。转载请注明来源
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/%E6%A3%80%E6%B5%8B-%E5%88%86%E5%89%B2-%E8%B7%9F%E8%B8%AA/">
                                    <span class="chip bg-color">检测/分割/跟踪</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;上一篇</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-04-09/%E4%BA%BA%E8%84%B8%E7%9B%B8%E5%85%B3/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-d76c24baec6d4a4fe77761b9e60f2809.jpg" class="responsive-img" alt="人脸相关">
                        
                        <span class="card-title">人脸相关</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            人脸相关 方向最新论文已更新，请持续关注 Update in 2025-04-09  Semantic Contextualization of Face Forgery A New Definition, Dataset,   and Detection Method
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-04-09
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E4%BA%BA%E8%84%B8%E7%9B%B8%E5%85%B3/" class="post-category">
                                    人脸相关
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E4%BA%BA%E8%84%B8%E7%9B%B8%E5%85%B3/">
                        <span class="chip bg-color">人脸相关</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                下一篇&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-04-09/Vision%20Transformer/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-5773c7a05e0a6ee5893e1685bcfd7008.jpg" class="responsive-img" alt="Vision Transformer">
                        
                        <span class="card-title">Vision Transformer</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Vision Transformer 方向最新论文已更新，请持续关注 Update in 2025-04-09  DA2Diff Exploring Degradation-aware Adaptive Diffusion Priors for   All-in-One Weather Restoration
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-04-09
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Vision-Transformer/" class="post-category">
                                    Vision Transformer
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Vision-Transformer/">
                        <span class="chip bg-color">Vision Transformer</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- 代码块功能依赖 -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- 代码语言 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- 代码块复制 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- 代码块收缩 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC 悬浮按钮. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* 修复文章卡片 div 的宽度. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // 切换TOC目录展开收缩的相关操作.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;站点总字数:&nbsp;<span
                        class="white-color">28051.5k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- 运行天数提醒. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // 区分是否有年份.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = '本站已运行 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                daysTip = '本站已運行 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = '本站已运行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = '本站已運行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="关注我的知乎: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">知</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- 搜索遮罩框 -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span>
            <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- 白天和黑夜主题 -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- 回到顶部按钮 -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // 只在桌面版网页启用特效
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- 雪花特效 -->
    

    <!-- 鼠标星星特效 -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--腾讯兔小巢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- 动态标签 -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Σ(っ °Д °;)っ诶，页面崩溃了嘛？", clearTimeout(st)) : (document.title = "φ(゜▽゜*)♪咦，又好了！", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
