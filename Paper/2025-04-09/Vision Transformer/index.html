<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Vision Transformer">
    <meta name="description" content="Vision Transformer æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-04-09  DA2Diff Exploring Degradation-aware Adaptive Diffusion Priors for   All-in-One Weather Restoration">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Vision Transformer | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-5773c7a05e0a6ee5893e1685bcfd7008.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Vision Transformer</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Vision-Transformer/">
                                <span class="chip bg-color">Vision Transformer</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Vision-Transformer/" class="post-category">
                                Vision Transformer
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-04-09
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-05-14
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    8k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    32 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-04-09-æ›´æ–°"><a href="#2025-04-09-æ›´æ–°" class="headerlink" title="2025-04-09 æ›´æ–°"></a>2025-04-09 æ›´æ–°</h1><h2 id="DA2Diff-Exploring-Degradation-aware-Adaptive-Diffusion-Priors-for-All-in-One-Weather-Restoration"><a href="#DA2Diff-Exploring-Degradation-aware-Adaptive-Diffusion-Priors-for-All-in-One-Weather-Restoration" class="headerlink" title="DA2Diff: Exploring Degradation-aware Adaptive Diffusion Priors for   All-in-One Weather Restoration"></a>DA2Diff: Exploring Degradation-aware Adaptive Diffusion Priors for   All-in-One Weather Restoration</h2><p><strong>Authors:Jiamei Xiong, Xuefeng Yan, Yongzhen Wang, Wei Zhao, Xiao-Ping Zhang, Mingqiang Wei</strong></p>
<p>Image restoration under adverse weather conditions is a critical task for many vision-based applications. Recent all-in-one frameworks that handle multiple weather degradations within a unified model have shown potential. However, the diversity of degradation patterns across different weather conditions, as well as the complex and varied nature of real-world degradations, pose significant challenges for multiple weather removal. To address these challenges, we propose an innovative diffusion paradigm with degradation-aware adaptive priors for all-in-one weather restoration, termed DA2Diff. It is a new exploration that applies CLIP to perceive degradation-aware properties for better multi-weather restoration. Specifically, we deploy a set of learnable prompts to capture degradation-aware representations by the prompt-image similarity constraints in the CLIP space. By aligning the snowy&#x2F;hazy&#x2F;rainy images with snow&#x2F;haze&#x2F;rain prompts, each prompt contributes to different weather degradation characteristics. The learned prompts are then integrated into the diffusion model via the designed weather specific prompt guidance module, making it possible to restore multiple weather types. To further improve the adaptiveness to complex weather degradations, we propose a dynamic expert selection modulator that employs a dynamic weather-aware router to flexibly dispatch varying numbers of restoration experts for each weather-distorted image, allowing the diffusion model to restore diverse degradations adaptively. Experimental results substantiate the favorable performance of DA2Diff over state-of-the-arts in quantitative and qualitative evaluation. Source code will be available after acceptance. </p>
<blockquote>
<p>æ¶åŠ£å¤©æ°”æ¡ä»¶ä¸‹çš„å›¾åƒæ¢å¤æ˜¯è®¸å¤šåŸºäºè§†è§‰çš„åº”ç”¨ä¸­çš„å…³é”®ä»»åŠ¡ã€‚æœ€è¿‘çš„å…¨èƒ½æ¡†æ¶èƒ½å¤Ÿåœ¨ç»Ÿä¸€æ¨¡å‹ä¸­å¤„ç†å¤šç§å¤©æ°”é€€åŒ–ï¼Œå·²æ˜¾ç¤ºå‡ºå…¶æ½œåŠ›ã€‚ç„¶è€Œï¼Œä¸åŒå¤©æ°”æ¡ä»¶ä¸‹é€€åŒ–æ¨¡å¼çš„å¤šæ ·æ€§ä»¥åŠçœŸå®ä¸–ç•Œé€€åŒ–çš„å¤æ‚æ€§å’Œå¤šå˜æ€§è´¨ï¼Œç»™å¤šç§å¤©æ°”å»é™¤å¸¦æ¥äº†é‡å¤§æŒ‘æˆ˜ã€‚ä¸ºäº†åº”å¯¹è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åˆ›æ–°çš„æ‰©æ•£èŒƒå¼ï¼Œå¸¦æœ‰é€€åŒ–æ„ŸçŸ¥è‡ªé€‚åº”å…ˆéªŒçš„å…¨èƒ½å¤©æ°”æ¢å¤ï¼Œç§°ä¸ºDA2Diffã€‚è¿™æ˜¯ä¸€ç§æ–°çš„æ¢ç´¢ï¼Œåº”ç”¨CLIPæ¥æ„ŸçŸ¥é€€åŒ–æ„ŸçŸ¥å±æ€§ä»¥æ›´å¥½åœ°è¿›è¡Œå¤šå¤©æ°”æ¢å¤ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬éƒ¨ç½²äº†ä¸€ç»„å¯å­¦ä¹ çš„æç¤ºï¼Œé€šè¿‡CLIPç©ºé—´ä¸­çš„æç¤ºå›¾åƒç›¸ä¼¼æ€§çº¦æŸæ¥æ•è·é€€åŒ–æ„ŸçŸ¥è¡¨ç¤ºã€‚é€šè¿‡å¯¹é›ª&#x2F;é›¾&#x2F;é›¨å›¾åƒä¸é›ª&#x2F;é›¾&#x2F;é›¨æç¤ºè¿›è¡Œå¯¹é½ï¼Œæ¯ä¸ªæç¤ºéƒ½æœ‰åŠ©äºå‘ˆç°ä¸åŒçš„å¤©æ°”é€€åŒ–ç‰¹å¾ã€‚ç„¶åå°†å­¦åˆ°çš„æç¤ºé›†æˆåˆ°æ‰©æ•£æ¨¡å‹ä¸­ï¼Œé€šè¿‡è®¾è®¡çš„å¤©æ°”ç‰¹å®šæç¤ºæŒ‡å¯¼æ¨¡å—ï¼Œä½¿å¾—æ¢å¤å¤šç§å¤©æ°”ç±»å‹æˆä¸ºå¯èƒ½ã€‚ä¸ºäº†è¿›ä¸€æ­¥æ”¹è¿›å¯¹å¤æ‚å¤©æ°”é€€åŒ–çš„é€‚åº”æ€§ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åŠ¨æ€ä¸“å®¶é€‰æ‹©è°ƒåˆ¶å™¨ï¼Œå®ƒé‡‡ç”¨åŠ¨æ€å¤©æ°”æ„ŸçŸ¥è·¯ç”±å™¨ï¼Œçµæ´»åœ°ä¸ºæ¯ç§å¤©æ°”å¤±çœŸå›¾åƒè°ƒåº¦ä¸åŒæ•°é‡çš„æ¢å¤ä¸“å®¶ï¼Œä½¿æ‰©æ•£æ¨¡å‹èƒ½å¤Ÿè‡ªé€‚åº”åœ°æ¢å¤å„ç§é€€åŒ–ã€‚å®éªŒç»“æœè¯å®ï¼ŒDA2Diffåœ¨å®šé‡å’Œå®šæ€§è¯„ä¼°ä¸Šçš„æ€§èƒ½ä¼˜äºæœ€æ–°æŠ€æœ¯ã€‚æºä»£ç å°†åœ¨æ¥å—åæä¾›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.05135v1">PDF</a> </p>
<p><strong>Summary</strong><br>å¤©æ°”æ¶åŠ£çŠ¶å†µä¸‹çš„å›¾åƒä¿®å¤æ˜¯è®¸å¤šè§†è§‰åº”ç”¨ä¸­çš„å…³é”®ä»»åŠ¡ã€‚è¿‘æœŸç»Ÿä¸€æ¡†æ¶åœ¨å¤„ç†å¤šç§å¤©æ°”é€€åŒ–æ–¹é¢å±•ç°å‡ºæ½œåŠ›ï¼Œä½†ä»é¢ä¸´ä¸åŒå¤©æ°”æ¡ä»¶ä¸‹é€€åŒ–æ¨¡å¼çš„å¤šæ ·æ€§å’ŒçœŸå®ä¸–ç•Œé€€åŒ–çš„å¤æ‚æ€§çš„æŒ‘æˆ˜ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªåä¸ºDA2Diffçš„åˆ›æ–°æ‰©æ•£èŒƒå¼ï¼Œå®ƒå…·å¤‡æ„ŸçŸ¥é€€åŒ–çš„è‡ªé€‚åº”å…ˆéªŒã€‚è¯¥èŒƒå¼æ¢ç´¢äº†å°†CLIPåº”ç”¨äºæ„ŸçŸ¥é€€åŒ–æ„ŸçŸ¥å±æ€§ä»¥è¿›è¡Œæ›´å¥½çš„å¤šå¤©æ°”ä¿®å¤ã€‚é€šè¿‡éƒ¨ç½²ä¸€ç»„å¯å­¦ä¹ çš„æç¤ºæ¥æ•è·é€€åŒ–æ„ŸçŸ¥è¡¨ç¤ºï¼Œè¿™äº›æç¤ºé€šè¿‡CLIPç©ºé—´ä¸­çš„æç¤ºå›¾åƒç›¸ä¼¼æ€§çº¦æŸæ¥å®ç°ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒDA2Diffåœ¨å®šé‡å’Œå®šæ€§è¯„ä¼°ä¸Šçš„è¡¨ç°ä¼˜äºç°æœ‰æŠ€æœ¯ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å›¾åƒä¿®å¤åœ¨æ¶åŠ£å¤©æ°”ä¸‹æ˜¯è§†è§‰åº”ç”¨çš„å…³é”®ä»»åŠ¡ã€‚</li>
<li>ç°æœ‰ç»Ÿä¸€æ¡†æ¶åœ¨å¤„ç†å¤šç§å¤©æ°”é€€åŒ–æ—¶é¢ä¸´æŒ‘æˆ˜ï¼Œå¦‚é€€åŒ–æ¨¡å¼çš„å¤šæ ·æ€§å’ŒçœŸå®ä¸–ç•Œé€€åŒ–çš„å¤æ‚æ€§ã€‚</li>
<li>DA2DiffèŒƒå¼é€šè¿‡å¼•å…¥æ„ŸçŸ¥é€€åŒ–çš„è‡ªé€‚åº”å…ˆéªŒæ¥è§£å†³è¿™äº›æŒ‘æˆ˜ã€‚</li>
<li>DA2Diffåˆ©ç”¨CLIPæŠ€æœ¯æ¥æ„ŸçŸ¥é€€åŒ–å±æ€§ï¼Œå®ç°æ›´å¥½çš„å¤šå¤©æ°”ä¿®å¤ã€‚</li>
<li>é€šè¿‡éƒ¨ç½²å¯å­¦ä¹ æç¤ºæ¥æ•è·é€€åŒ–æ„ŸçŸ¥è¡¨ç¤ºï¼Œè¿™äº›æç¤ºé€šè¿‡æç¤ºå›¾åƒç›¸ä¼¼æ€§çº¦æŸåœ¨CLIPç©ºé—´ä¸­å®ç°ã€‚</li>
<li>åŠ¨æ€ä¸“å®¶é€‰æ‹©è°ƒåˆ¶å™¨é€šè¿‡åŠ¨æ€å¤©æ°”æ„ŸçŸ¥è·¯ç”±å™¨çµæ´»è°ƒåº¦ä¸åŒæ•°é‡çš„ä¿®å¤ä¸“å®¶ï¼Œä½¿æ‰©æ•£æ¨¡å‹èƒ½å¤Ÿè‡ªé€‚åº”åœ°ä¿®å¤å„ç§é€€åŒ–ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.05135">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-401573e68ed138e4a813a56548ec89fe.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-2518ff788ef52a54e35d7b88c915484d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ebf1e6e2239ab6871ba1bb18c534b56e.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Performance-Analysis-of-Deep-Learning-Models-for-Femur-Segmentation-in-MRI-Scan"><a href="#Performance-Analysis-of-Deep-Learning-Models-for-Femur-Segmentation-in-MRI-Scan" class="headerlink" title="Performance Analysis of Deep Learning Models for Femur Segmentation in   MRI Scan"></a>Performance Analysis of Deep Learning Models for Femur Segmentation in   MRI Scan</h2><p><strong>Authors:Mengyuan Liu, Yixiao Chen, Anning Tian, Xinmeng Wu, Mozhi Shen, Tianchou Gong, Jeongkyu Lee</strong></p>
<p>Convolutional neural networks like U-Net excel in medical image segmentation, while attention mechanisms and KAN enhance feature extraction. Metaâ€™s SAM 2 uses Vision Transformers for prompt-based segmentation without fine-tuning. However, biases in these models impact generalization with limited data. In this study, we systematically evaluate and compare the performance of three CNN-based models, i.e., U-Net, Attention U-Net, and U-KAN, and one transformer-based model, i.e., SAM 2 for segmenting femur bone structures in MRI scan. The dataset comprises 11,164 MRI scans with detailed annotations of femoral regions. Performance is assessed using the Dice Similarity Coefficient, which ranges from 0.932 to 0.954. Attention U-Net achieves the highest overall scores, while U-KAN demonstrated superior performance in anatomical regions with a smaller region of interest, leveraging its enhanced learning capacity to improve segmentation accuracy. </p>
<blockquote>
<p>å·ç§¯ç¥ç»ç½‘ç»œï¼ˆå¦‚U-Netï¼‰åœ¨åŒ»å­¦å›¾åƒåˆ†å‰²æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œè€Œæ³¨æ„åŠ›æœºåˆ¶å’ŒKANåˆ™å¢å¼ºäº†ç‰¹å¾æå–ã€‚Metaçš„SAM 2ä½¿ç”¨Vision Transformersè¿›è¡ŒåŸºäºæç¤ºçš„åˆ†å‰²ï¼Œæ— éœ€å¾®è°ƒã€‚ç„¶è€Œï¼Œè¿™äº›æ¨¡å‹ä¸­çš„åè§å½±å“åœ¨æœ‰é™æ•°æ®ä¸‹çš„æ³›åŒ–èƒ½åŠ›ã€‚åœ¨è¿™é¡¹ç ”ç©¶ä¸­ï¼Œæˆ‘ä»¬ç³»ç»Ÿåœ°è¯„ä¼°å’Œæ¯”è¾ƒäº†ä¸‰ç§åŸºäºCNNçš„æ¨¡å‹ï¼ˆå³U-Netã€Attention U-Netå’ŒU-KANï¼‰å’Œä¸€ç§åŸºäºå˜å‹å™¨çš„æ¨¡å‹ï¼ˆå³SAM 2ï¼‰åœ¨MRIæ‰«æä¸­åˆ†å‰²è‚¡éª¨ç»“æ„æ€§èƒ½çš„è¡¨ç°ã€‚æ•°æ®é›†åŒ…å«11,164ä»½MRIæ‰«æï¼Œè¯¦ç»†æ ‡æ³¨äº†è‚¡éª¨åŒºåŸŸã€‚æ€§èƒ½è¯„ä¼°é‡‡ç”¨Diceç›¸ä¼¼ç³»æ•°ï¼ŒèŒƒå›´ä»0.932åˆ°0.954ã€‚Attention U-Netè·å¾—æœ€é«˜æ€»ä½“å¾—åˆ†ï¼Œè€ŒU-KANåœ¨è¾ƒå°æ„Ÿå…´è¶£åŒºåŸŸçš„è§£å‰–ç»“æ„ä¸­è¡¨ç°å‡ºå“è¶Šæ€§èƒ½ï¼Œåˆ©ç”¨å…¶å¢å¼ºçš„å­¦ä¹ èƒ½åŠ›æé«˜åˆ†å‰²ç²¾åº¦ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.04066v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ç ”ç©¶äº†å·ç§¯ç¥ç»ç½‘ç»œï¼ˆå¦‚U-Netã€Attention U-Netå’ŒU-KANï¼‰ä¸åŸºäºVision Transformerçš„SAM 2æ¨¡å‹åœ¨MRIæ‰«æä¸­çš„è‚¡éª¨ç»“æ„åˆ†å‰²æ€§èƒ½ã€‚é€šè¿‡å¯¹å¤§é‡MRIæ‰«ææ•°æ®é›†çš„å®éªŒè¯„ä¼°ï¼Œå‘ç°Attention U-Netæ€»ä½“è¡¨ç°æœ€ä½³ï¼Œç‰¹åˆ«æ˜¯åœ¨è¾ƒå°æ„Ÿå…´è¶£åŒºåŸŸä¸­ï¼ŒU-KANè¡¨ç°å‡ºè¾ƒé«˜çš„åˆ†å‰²å‡†ç¡®æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNï¼‰å¦‚U-Netåœ¨åŒ»å­¦å›¾åƒåˆ†å‰²ä¸­è¡¨ç°ä¼˜å¼‚ã€‚</li>
<li>æ³¨æ„åŠ›æœºåˆ¶å’ŒKANå¢å¼ºç‰¹å¾æå–èƒ½åŠ›ã€‚</li>
<li>Metaçš„SAM 2ä½¿ç”¨Vision Transformerè¿›è¡ŒåŸºäºæç¤ºçš„åˆ†å‰²ï¼Œæ— éœ€å¾®è°ƒã€‚</li>
<li>æ¨¡å‹ä¸­çš„åè§å½±å“æœ‰é™æ•°æ®çš„æ³›åŒ–èƒ½åŠ›ã€‚</li>
<li>å®éªŒä¸­è¯„ä¼°äº†U-Netã€Attention U-Netã€U-KANå’ŒSAM 2å››ä¸ªæ¨¡å‹çš„æ€§èƒ½ã€‚</li>
<li>Attention U-Netåœ¨æ€»ä½“è¯„ä¼°ä¸­è¡¨ç°æœ€ä½³ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.04066">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-726ae4620e9644849b3d96a889cf98f8.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-0a3120eb21888c69e21d43958178705b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7700f89352ad6f91f0a1b1243a674ce7.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-86a872ea9182ffd5189ea2ded1dde0d2.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Filter-Images-First-Generate-Instructions-Later-Pre-Instruction-Data-Selection-for-Visual-Instruction-Tuning"><a href="#Filter-Images-First-Generate-Instructions-Later-Pre-Instruction-Data-Selection-for-Visual-Instruction-Tuning" class="headerlink" title="Filter Images First, Generate Instructions Later: Pre-Instruction Data   Selection for Visual Instruction Tuning"></a>Filter Images First, Generate Instructions Later: Pre-Instruction Data   Selection for Visual Instruction Tuning</h2><p><strong>Authors:Bardia Safaei, Faizan Siddiqui, Jiacong Xu, Vishal M. Patel, Shao-Yuan Lo</strong></p>
<p>Visual instruction tuning (VIT) for large vision-language models (LVLMs) requires training on expansive datasets of image-instruction pairs, which can be costly. Recent efforts in VIT data selection aim to select a small subset of high-quality image-instruction pairs, reducing VIT runtime while maintaining performance comparable to full-scale training. However, a major challenge often overlooked is that generating instructions from unlabeled images for VIT is highly expensive. Most existing VIT datasets rely heavily on human annotations or paid services like the GPT API, which limits users with constrained resources from creating VIT datasets for custom applications. To address this, we introduce Pre-Instruction Data Selection (PreSel), a more practical data selection paradigm that directly selects the most beneficial unlabeled images and generates instructions only for the selected images. PreSel first estimates the relative importance of each vision task within VIT datasets to derive task-wise sampling budgets. It then clusters image features within each task, selecting the most representative images with the budget. This approach reduces computational overhead for both instruction generation during VIT data formation and LVLM fine-tuning. By generating instructions for only 15% of the images, PreSel achieves performance comparable to full-data VIT on the LLaVA-1.5 and Vision-Flan datasets. The link to our project page: <a target="_blank" rel="noopener" href="https://bardisafa.github.io/PreSel">https://bardisafa.github.io/PreSel</a> </p>
<blockquote>
<p>è§†è§‰æŒ‡ä»¤è°ƒæ•´ï¼ˆVITï¼‰é’ˆå¯¹å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆLVLMsï¼‰éœ€è¦åœ¨å¤§é‡çš„å›¾åƒæŒ‡ä»¤å¯¹æ•°æ®é›†ä¸Šè¿›è¡Œè®­ç»ƒï¼Œè¿™å¯èƒ½ä¼šå¾ˆæ˜‚è´µã€‚æœ€è¿‘åœ¨VITæ•°æ®é€‰æ‹©æ–¹é¢çš„åŠªåŠ›æ—¨åœ¨é€‰æ‹©ä¸€å°éƒ¨åˆ†é«˜è´¨é‡çš„å›¾åƒæŒ‡ä»¤å¯¹ï¼Œåœ¨å‡å°‘VITè¿è¡Œæ—¶é—´çš„åŒæ—¶ä¿æŒä¸å…¨è§„æ¨¡è®­ç»ƒç›¸å½“çš„æ€§èƒ½ã€‚ç„¶è€Œï¼Œç»å¸¸è¢«å¿½è§†çš„ä¸€ä¸ªä¸»è¦æŒ‘æˆ˜æ˜¯ï¼Œä»æ— æ ‡ç­¾å›¾åƒä¸­ç”ŸæˆVITæŒ‡ä»¤çš„æˆæœ¬éå¸¸é«˜ã€‚å¤§å¤šæ•°ç°æœ‰çš„VITæ•°æ®é›†ä¸¥é‡ä¾èµ–äºäººå·¥æ³¨é‡Šæˆ–å¦‚GPT APIç­‰ä»˜è´¹æœåŠ¡ï¼Œè¿™é™åˆ¶äº†èµ„æºæœ‰é™çš„ç”¨æˆ·åˆ›å»ºç”¨äºè‡ªå®šä¹‰åº”ç”¨ç¨‹åºçš„VITæ•°æ®é›†ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†é¢„æŒ‡ä»¤æ•°æ®é€‰æ‹©ï¼ˆPreSelï¼‰ï¼Œè¿™æ˜¯ä¸€ç§æ›´å®ç”¨çš„æ•°æ®é€‰æ‹©èŒƒå¼ï¼Œå®ƒç›´æ¥é€‰æ‹©æœ€æœ‰ç›Šçš„æ— æ ‡ç­¾å›¾åƒï¼Œåªä¸ºæ‰€é€‰å›¾åƒç”ŸæˆæŒ‡ä»¤ã€‚PreSelé¦–å…ˆä¼°è®¡VITæ•°æ®é›†ä¸­æ¯ä¸ªè§†è§‰ä»»åŠ¡çš„ç›¸å¯¹é‡è¦æ€§ï¼Œä»¥å¾—å‡ºä»»åŠ¡çº§é‡‡æ ·é¢„ç®—ã€‚ç„¶åï¼Œå®ƒåœ¨æ¯ä¸ªä»»åŠ¡å†…å¯¹å›¾åƒç‰¹å¾è¿›è¡Œèšç±»ï¼Œé€‰æ‹©æœ€å…·ä»£è¡¨æ€§çš„å›¾åƒæ¥ä½¿ç”¨é¢„ç®—ã€‚è¿™ç§æ–¹æ³•å‡å°‘äº†VITæ•°æ®å½¢æˆå’ŒLVLMå¾®è°ƒè¿‡ç¨‹ä¸­æŒ‡ä»¤ç”Ÿæˆçš„è®¡ç®—å¼€é”€ã€‚é€šè¿‡åªä¸º15%çš„å›¾åƒç”ŸæˆæŒ‡ä»¤ï¼ŒPreSelåœ¨LLaVA-1.5å’ŒVision-Flanæ•°æ®é›†ä¸Šå®ç°äº†ä¸å…¨æ•°æ®VITç›¸å½“çš„æ€§èƒ½ã€‚æˆ‘ä»¬çš„é¡¹ç›®é¡µé¢é“¾æ¥ä¸ºï¼š<a target="_blank" rel="noopener" href="https://bardisafa.github.io/PreSel%E3%80%82">https://bardisafa.github.io/PreSel</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.07591v2">PDF</a> Accepted at CVPR 2025 (Highlight)</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†é’ˆå¯¹å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆLVLMsï¼‰çš„è§†è§‰æŒ‡ä»¤è°ƒæ•´ï¼ˆVITï¼‰é¢ä¸´çš„æŒ‘æˆ˜ï¼Œå°¤å…¶æ˜¯é«˜æˆæœ¬çš„æ•°æ®é›†éœ€æ±‚ã€‚ä¸ºåº”å¯¹è¿™ä¸€é—®é¢˜ï¼Œç ”ç©¶äº†ä¸€ç§æ›´ä¸ºå®ç”¨çš„æ•°æ®é€‰æ‹©æ¨¡å¼â€”â€”Pre-Instruction Data Selectionï¼ˆPreSelï¼‰ã€‚PreSelèƒ½å¤Ÿç›´æ¥é€‰å–æœ€æœ‰ç›Šçš„æ— æ ‡ç­¾å›¾åƒï¼Œå¹¶ä¸ºé€‰ä¸­çš„å›¾åƒç”ŸæˆæŒ‡ä»¤ã€‚å®ƒé€šè¿‡ä¼°è®¡æ¯ä¸ªè§†è§‰ä»»åŠ¡çš„é‡è¦æ€§æ¥æ¨å¯¼ä»»åŠ¡çº§åˆ«çš„é‡‡æ ·é¢„ç®—ï¼Œå¹¶åœ¨æ¯ä¸ªä»»åŠ¡å†…å¯¹å›¾åƒç‰¹å¾è¿›è¡Œèšç±»ï¼Œé€‰æ‹©æœ€å…·ä»£è¡¨æ€§çš„å›¾åƒã€‚è¿™ç§ç­–ç•¥å‡å°‘äº†åœ¨æ„å»ºVITæ•°æ®é›†å’Œå¾®è°ƒLVLMæœŸé—´çš„è®¡ç®—å¼€é”€ã€‚é€šè¿‡å¯¹ä»…15%çš„å›¾åƒç”ŸæˆæŒ‡ä»¤ï¼ŒPreSelåœ¨LLaVA-1.5å’ŒVision-Flanæ•°æ®é›†ä¸Šçš„æ€§èƒ½ä¸å…¨æ•°æ®VITç›¸å½“ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>PreSelè§£å†³äº†å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆLVLMsï¼‰çš„è§†è§‰æŒ‡ä»¤è°ƒæ•´ï¼ˆVITï¼‰ä¸­æ•°æ®é€‰æ‹©çš„é—®é¢˜ï¼Œå°¤å…¶å…³æ³¨ä»æ— æ ‡ç­¾å›¾åƒç”ŸæˆæŒ‡ä»¤çš„é«˜æˆæœ¬é—®é¢˜ã€‚</li>
<li>PreSelé€šè¿‡ä¼°è®¡æ¯ä¸ªè§†è§‰ä»»åŠ¡çš„é‡è¦æ€§æ¥æ¨å¯¼ä»»åŠ¡çº§åˆ«çš„é‡‡æ ·é¢„ç®—ï¼Œä»¥å®ç°æ›´é«˜æ•ˆçš„æ•°æ®é€‰æ‹©ã€‚</li>
<li>PreSelé‡‡ç”¨èšç±»æ–¹æ³•ï¼Œé€‰æ‹©æ¯ä¸ªä»»åŠ¡å†…æœ€å…·ä»£è¡¨æ€§çš„å›¾åƒï¼Œé™ä½è®¡ç®—å¼€é”€ã€‚</li>
<li>PreSelä»…å¯¹éƒ¨åˆ†å›¾åƒç”ŸæˆæŒ‡ä»¤ï¼Œå³å¯è¾¾åˆ°ä¸å…¨æ•°æ®VITç›¸å½“çš„æ€§èƒ½ã€‚</li>
<li>PreSelæ–¹æ³•æ˜¾è‘—å‡å°‘äº†æ•°æ®æ”¶é›†å’Œå¤„ç†çš„æˆæœ¬ï¼Œä½¿å¾—æ›´å¤šç”¨æˆ·èƒ½å¤Ÿåœ¨æœ‰é™èµ„æºä¸‹åˆ›å»ºè‡ªå®šä¹‰çš„VITæ•°æ®é›†ã€‚</li>
<li>PreSelåœ¨LLaVA-1.5å’ŒVision-Flanæ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¡¨æ˜å…¶æœ‰æ•ˆæ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.07591">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-c9377d2c0291d7db3c1e387fc718d8cd.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-93ec5db65b826b27d696e3a99ffda4de.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e265abccdd703087bfb59e5a7564ce27.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f68ee10db0da8de33097a4f4f36d45f7.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="FSFM-A-Generalizable-Face-Security-Foundation-Model-via-Self-Supervised-Facial-Representation-Learning"><a href="#FSFM-A-Generalizable-Face-Security-Foundation-Model-via-Self-Supervised-Facial-Representation-Learning" class="headerlink" title="FSFM: A Generalizable Face Security Foundation Model via Self-Supervised   Facial Representation Learning"></a>FSFM: A Generalizable Face Security Foundation Model via Self-Supervised   Facial Representation Learning</h2><p><strong>Authors:Gaojian Wang, Feng Lin, Tong Wu, Zhenguang Liu, Zhongjie Ba, Kui Ren</strong></p>
<p>This work asks: with abundant, unlabeled real faces, how to learn a robust and transferable facial representation that boosts various face security tasks with respect to generalization performance? We make the first attempt and propose a self-supervised pretraining framework to learn fundamental representations of real face images, FSFM, that leverages the synergy between masked image modeling (MIM) and instance discrimination (ID). We explore various facial masking strategies for MIM and present a simple yet powerful CRFR-P masking, which explicitly forces the model to capture meaningful intra-region consistency and challenging inter-region coherency. Furthermore, we devise the ID network that naturally couples with MIM to establish underlying local-to-global correspondence via tailored self-distillation. These three learning objectives, namely 3C, empower encoding both local features and global semantics of real faces. After pretraining, a vanilla ViT serves as a universal vision foundation model for downstream face security tasks: cross-dataset deepfake detection, cross-domain face anti-spoofing, and unseen diffusion facial forgery detection. Extensive experiments on 10 public datasets demonstrate that our model transfers better than supervised pretraining, visual and facial self-supervised learning arts, and even outperforms task-specialized SOTA methods. </p>
<blockquote>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ä¸ªé—®é¢˜ï¼šåœ¨å¤§é‡æ— æ ‡ç­¾çš„çœŸå®äººè„¸æƒ…å†µä¸‹ï¼Œå¦‚ä½•å­¦ä¹ ä¸€ç§ç¨³å¥ä¸”å¯è¿ç§»çš„äººè„¸è¡¨ç¤ºï¼Œä»¥æé«˜å…³äºæ³›åŒ–æ€§èƒ½çš„å„ç§äººè„¸å®‰å…¨ä»»åŠ¡çš„æ•ˆæœï¼Ÿæˆ‘ä»¬é¦–æ¬¡å°è¯•å¹¶æå‡ºä¸€ç§è‡ªç›‘ç£é¢„è®­ç»ƒæ¡†æ¶ï¼Œç”¨äºå­¦ä¹ çœŸå®äººè„¸å›¾åƒçš„åŸºæœ¬è¡¨ç¤ºï¼ŒFSFMï¼Œè¯¥æ¡†æ¶åˆ©ç”¨æ©è†œå›¾åƒå»ºæ¨¡ï¼ˆMIMï¼‰å’Œå®ä¾‹é‰´åˆ«ï¼ˆIDï¼‰ä¹‹é—´çš„ååŒä½œç”¨ã€‚æˆ‘ä»¬æ¢ç´¢äº†MIMçš„å„ç§é¢éƒ¨æ©è†œç­–ç•¥ï¼Œå¹¶æå‡ºäº†ä¸€ç§ç®€å•è€Œå¼ºå¤§çš„CRFR-Pæ©è†œï¼Œå®ƒæ˜ç¡®åœ°è¿«ä½¿æ¨¡å‹æ•æ‰åŒºåŸŸå†…æœ‰æ„ä¹‰çš„ä¸€è‡´æ€§ä»¥åŠåŒºåŸŸé—´çš„æŒ‘æˆ˜æ€§è¿è´¯æ€§ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸€ä¸ªä¸MIMè‡ªç„¶ç»“åˆçš„IDç½‘ç»œï¼Œé€šè¿‡å®šåˆ¶çš„è‡ªè’¸é¦å»ºç«‹åŸºæœ¬çš„å±€éƒ¨åˆ°å…¨å±€å¯¹åº”å…³ç³»ã€‚è¿™ä¸‰ä¸ªå­¦ä¹ ç›®æ ‡ï¼Œå³3Cï¼Œä½¿ç¼–ç çœŸå®äººè„¸çš„å±€éƒ¨ç‰¹å¾å’Œå…¨å±€è¯­ä¹‰æˆä¸ºå¯èƒ½ã€‚é¢„è®­ç»ƒåï¼Œä¸€ä¸ªç®€å•çš„ViTä½œä¸ºé€šç”¨è§†è§‰åŸºç¡€æ¨¡å‹ï¼Œå¯ç”¨äºä¸‹æ¸¸äººè„¸å®‰å…¨ä»»åŠ¡ï¼šè·¨æ•°æ®é›†æ·±åº¦ä¼ªé€ æ£€æµ‹ã€è·¨åŸŸé¢éƒ¨é˜²ä¼ªã€æœªè§æ‰©æ•£é¢éƒ¨ä¼ªé€ æ£€æµ‹ã€‚åœ¨1tä¸ªå…¬å¼€æ•°æ®é›†ä¸Šçš„å¤§é‡å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ¨¡å‹è¿ç§»æ•ˆæœä¼˜äºç›‘ç£é¢„è®­ç»ƒã€è§†è§‰å’Œé¢éƒ¨è‡ªç›‘ç£å­¦ä¹ æŠ€æœ¯ï¼Œç”šè‡³è¶…è¶Šäº†ä»»åŠ¡ä¸“ä¸šåŒ–çš„SOTAæ–¹æ³•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.12032v3">PDF</a> 21 pages, 11 figures, project page: <a target="_blank" rel="noopener" href="https://fsfm-3c.github.io/">https://fsfm-3c.github.io</a></p>
<p><strong>Summary</strong><br>åœ¨å¤§é‡çš„æ— æ ‡ç­¾çœŸå®äººè„¸å›¾åƒé¢å‰ï¼Œå¦‚ä½•å­¦ä¹ ä¸€ç§ç¨³å¥ä¸”å…·æœ‰è¿ç§»æ€§çš„é¢éƒ¨è¡¨ç¤ºä»¥æå‡å„ç§é¢éƒ¨å®‰å…¨ä»»åŠ¡çš„æ³›åŒ–æ€§èƒ½ï¼Ÿæˆ‘ä»¬é¦–æ¬¡å°è¯•å¹¶æå‡ºäº†ä¸€ç§è‡ªç›‘ç£é¢„è®­ç»ƒæ¡†æ¶FSFMï¼Œç”¨äºå­¦ä¹ çœŸå®äººè„¸å›¾åƒçš„åŸºæœ¬è¡¨ç¤ºã€‚è¯¥æ¡†æ¶ç»“åˆäº†æ©è†œå›¾åƒå»ºæ¨¡ï¼ˆMIMï¼‰å’Œå®ä¾‹é‰´åˆ«ï¼ˆIDï¼‰çš„ååŒä½œç”¨ã€‚æˆ‘ä»¬æ¢ç´¢äº†å„ç§é¢éƒ¨æ©è†œç­–ç•¥ï¼Œå¹¶æå‡ºäº†ä¸€ç§ç®€å•è€Œå¼ºå¤§çš„CRFR-Pæ©è†œï¼Œå®ƒæ˜ç¡®åœ°è¿«ä½¿æ¨¡å‹æ•æ‰åŒºåŸŸå†…æœ‰æ„ä¹‰çš„ä¸€è‡´æ€§ä»¥åŠåŒºåŸŸé—´çš„è¿è´¯æ€§ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜è®¾è®¡äº†ä¸MIMè‡ªç„¶ç»“åˆçš„IDç½‘ç»œï¼Œé€šè¿‡å®šåˆ¶çš„è‡ªæˆ‘è’¸é¦å»ºç«‹å±€éƒ¨åˆ°å…¨å±€çš„åŸºç¡€å¯¹åº”ã€‚è¿™ä¸‰ä¸ªå­¦ä¹ ç›®æ ‡ï¼Œå³3Cï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿç¼–ç çœŸå®äººè„¸çš„å±€éƒ¨ç‰¹å¾å’Œå…¨å±€è¯­ä¹‰ã€‚é¢„è®­ç»ƒåï¼Œæ™®é€šçš„ViTå¯ä½œä¸ºä¸‹æ¸¸é¢éƒ¨å®‰å…¨ä»»åŠ¡çš„é€šç”¨è§†è§‰åŸºç¡€æ¨¡å‹ï¼Œå¦‚è·¨æ•°æ®é›†æ·±åº¦ä¼ªé€ æ£€æµ‹ã€è·¨åŸŸé¢éƒ¨é˜²ä¼ªã€æœªè§æ‰©æ•£é¢éƒ¨ä¼ªé€ æ£€æµ‹ç­‰ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è¯¥ç ”ç©¶å…³æ³¨åœ¨å¤§é‡æ— æ ‡ç­¾çœŸå®äººè„¸å›¾åƒä¸Šï¼Œå¦‚ä½•å­¦ä¹ ä¸€ç§æ›´ç¨³å¥å’Œå¯è¿ç§»çš„é¢éƒ¨è¡¨ç¤ºã€‚</li>
<li>æå‡ºäº†ä¸€ä¸ªè‡ªç›‘ç£é¢„è®­ç»ƒæ¡†æ¶FSFMï¼Œç»“åˆäº†æ©è†œå›¾åƒå»ºæ¨¡ï¼ˆMIMï¼‰å’Œå®ä¾‹é‰´åˆ«ï¼ˆIDï¼‰ã€‚</li>
<li>ä»‹ç»äº†CRFR-Pæ©è†œç­–ç•¥ï¼Œå¼ºè°ƒæ¨¡å‹éœ€è¦æ•æ‰åŒºåŸŸå†…å¤–çš„è¿è´¯æ€§ã€‚</li>
<li>è®¾è®¡äº†ä¸MIMç»“åˆçš„IDç½‘ç»œï¼Œé€šè¿‡è‡ªæˆ‘è’¸é¦å»ºç«‹å±€éƒ¨åˆ°å…¨å±€çš„å¯¹åº”ã€‚</li>
<li>ä¸‰ä¸ªå­¦ä¹ ç›®æ ‡ï¼ˆ3Cï¼‰ä½¿æ¨¡å‹èƒ½å¤Ÿç¼–ç çœŸå®äººè„¸çš„å±€éƒ¨å’Œå…¨å±€ç‰¹å¾ã€‚</li>
<li>é¢„è®­ç»ƒæ¨¡å‹é€‚ç”¨äºå¤šç§é¢éƒ¨å®‰å…¨ä»»åŠ¡ï¼ŒåŒ…æ‹¬è·¨æ•°æ®é›†æ·±åº¦ä¼ªé€ æ£€æµ‹ã€è·¨åŸŸé¢éƒ¨é˜²ä¼ªç­‰ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.12032">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-ce435ae88aaf09ba1ea2dfeb1eaecd8f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b1f95bf9579c3f03a317f7556fcb2634.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-2b99273b9d5be84b7551e50985c49d3f.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-ef162aba0ba575b2789970c0451e1274.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-5b53bbc8fa7991471d27ba8d9de624dd.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Distilling-Spectral-Graph-for-Object-Context-Aware-Open-Vocabulary-Semantic-Segmentation"><a href="#Distilling-Spectral-Graph-for-Object-Context-Aware-Open-Vocabulary-Semantic-Segmentation" class="headerlink" title="Distilling Spectral Graph for Object-Context Aware Open-Vocabulary   Semantic Segmentation"></a>Distilling Spectral Graph for Object-Context Aware Open-Vocabulary   Semantic Segmentation</h2><p><strong>Authors:Chanyoung Kim, Dayun Ju, Woojung Han, Ming-Hsuan Yang, Seong Jae Hwang</strong></p>
<p>Open-Vocabulary Semantic Segmentation (OVSS) has advanced with recent vision-language models (VLMs), enabling segmentation beyond predefined categories through various learning schemes. Notably, training-free methods offer scalable, easily deployable solutions for handling unseen data, a key goal of OVSS. Yet, a critical issue persists: lack of object-level context consideration when segmenting complex objects in the challenging environment of OVSS based on arbitrary query prompts. This oversight limits modelsâ€™ ability to group semantically consistent elements within object and map them precisely to user-defined arbitrary classes. In this work, we introduce a novel approach that overcomes this limitation by incorporating object-level contextual knowledge within images. Specifically, our model enhances intra-object consistency by distilling spectral-driven features from vision foundation models into the attention mechanism of the visual encoder, enabling semantically coherent components to form a single object mask. Additionally, we refine the text embeddings with zero-shot object presence likelihood to ensure accurate alignment with the specific objects represented in the images. By leveraging object-level contextual knowledge, our proposed approach achieves state-of-the-art performance with strong generalizability across diverse datasets. </p>
<blockquote>
<p>å¼€æ”¾è¯æ±‡è¯­ä¹‰åˆ†å‰²ï¼ˆOVSSï¼‰éšç€æœ€è¿‘çš„è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰çš„å‘å±•è€Œè¿›æ­¥ï¼Œé€šè¿‡å„ç§å­¦ä¹ æ–¹æ¡ˆå®ç°äº†è¶…å‡ºé¢„å®šç±»åˆ«çš„åˆ†å‰²ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œæ— è®­ç»ƒæ–¹æ³•ä¸ºå¤„ç†æœªè§æ•°æ®æä¾›äº†å¯æ‰©å±•ã€æ˜“äºéƒ¨ç½²çš„è§£å†³æ–¹æ¡ˆï¼Œè¿™æ˜¯OVSSçš„å…³é”®ç›®æ ‡ã€‚ç„¶è€Œï¼Œä¸€ä¸ªå…³é”®é—®é¢˜ä¾ç„¶å­˜åœ¨ï¼šåœ¨OVSSçš„æŒ‘æˆ˜ç¯å¢ƒä¸­ï¼ŒåŸºäºä»»æ„æŸ¥è¯¢æç¤ºå¯¹å¤æ‚å¯¹è±¡è¿›è¡Œåˆ†å‰²æ—¶ï¼Œç¼ºä¹å¯¹è±¡çº§åˆ«çš„ä¸Šä¸‹æ–‡è€ƒè™‘ã€‚è¿™ç§ç–å¿½é™åˆ¶äº†æ¨¡å‹åœ¨å¯¹è±¡å†…éƒ¨åˆ†ç»„è¯­ä¹‰ä¸€è‡´å…ƒç´ çš„èƒ½åŠ›ï¼Œå¹¶ç²¾ç¡®åœ°å°†å…¶æ˜ å°„åˆ°ç”¨æˆ·å®šä¹‰çš„ä»»æ„ç±»åˆ«ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†ä¸€ç§å…‹æœè¿™ä¸€å±€é™çš„æ–°æ–¹æ³•ï¼Œè¯¥æ–¹æ³•é€šè¿‡åœ¨å›¾åƒä¸­èå…¥å¯¹è±¡çº§åˆ«çš„ä¸Šä¸‹æ–‡çŸ¥è¯†ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬çš„æ¨¡å‹é€šè¿‡å°†ä»è§†è§‰åŸºç¡€æ¨¡å‹è’¸é¦å‡ºçš„å…‰è°±é©±åŠ¨ç‰¹å¾èå…¥åˆ°è§†è§‰ç¼–ç å™¨çš„æ³¨æ„æœºåˆ¶ä¸­ï¼Œå¢å¼ºäº†å¯¹è±¡å†…éƒ¨çš„ä¸€è‡´æ€§ï¼Œä½¿è¯­ä¹‰ä¸€è‡´çš„ç»„ä»¶èƒ½å¤Ÿå½¢æˆå•ä¸ªå¯¹è±¡æ©ç ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜é€šè¿‡é›¶å°„ç‰©ä½“å­˜åœ¨å¯èƒ½æ€§æ¥å®Œå–„æ–‡æœ¬åµŒå…¥ï¼Œä»¥ç¡®ä¿ä¸å›¾åƒä¸­è¡¨ç¤ºçš„ç‰¹å®šå¯¹è±¡çš„å‡†ç¡®å¯¹é½ã€‚é€šè¿‡åˆ©ç”¨å¯¹è±¡çº§åˆ«çš„ä¸Šä¸‹æ–‡çŸ¥è¯†ï¼Œæˆ‘ä»¬æå‡ºçš„æ–¹æ³•åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šå®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œå¹¶å…·æœ‰è¾ƒå¼ºçš„æ³›åŒ–èƒ½åŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2411.17150v3">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>åŸºäºæœ€æ–°è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰çš„å¼€æ”¾è¯æ±‡è¯­ä¹‰åˆ†å‰²ï¼ˆOVSSï¼‰å–å¾—äº†è¿›å±•ï¼Œå®ƒé€šè¿‡ä¸åŒçš„å­¦ä¹ æ–¹æ¡ˆå®ç°äº†è¶…è¶Šé¢„å®šä¹‰ç±»åˆ«çš„åˆ†å‰²ã€‚æœ¬æ–‡ä¸»è¦è§£å†³äº†ä¸€ä¸ªå…³é”®é—®é¢˜ï¼šåœ¨OVSSçš„å¤æ‚ç¯å¢ƒä¸­ï¼ŒåŸºäºä»»æ„æŸ¥è¯¢æç¤ºè¿›è¡Œå¤æ‚å¯¹è±¡åˆ†å‰²æ—¶ç¼ºä¹å¯¹è±¡çº§åˆ«çš„ä¸Šä¸‹æ–‡è€ƒè™‘ã€‚æœ¬æ–‡ä»‹ç»äº†ä¸€ç§æ–°æ–¹æ³•ï¼Œé€šè¿‡èå…¥å›¾åƒå†…çš„å¯¹è±¡çº§åˆ«ä¸Šä¸‹æ–‡çŸ¥è¯†æ¥è§£å†³è¿™ä¸€é—®é¢˜ã€‚è¯¥æ–¹æ³•æé«˜äº†å¯¹è±¡å†…éƒ¨çš„ä¸€è‡´æ€§ï¼Œå¹¶é€šè¿‡ç²¾ç‚¼æ–‡æœ¬åµŒå…¥ä¸é›¶æ ·æœ¬å¯¹è±¡å­˜åœ¨æ¦‚ç‡ï¼Œç¡®ä¿ä¸å›¾åƒä¸­ç‰¹å®šå¯¹è±¡çš„å‡†ç¡®å¯¹é½ã€‚åˆ©ç”¨å¯¹è±¡çº§åˆ«çš„ä¸Šä¸‹æ–‡çŸ¥è¯†ï¼Œæ‰€æå‡ºçš„æ–¹æ³•åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šå®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œå¹¶å…·æœ‰è¾ƒå¼ºçš„æ³›åŒ–èƒ½åŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¼€æ”¾è¯æ±‡è¯­ä¹‰åˆ†å‰²ï¼ˆOVSSï¼‰èƒ½å¤Ÿå¤„ç†é¢„å®šä¹‰ç±»åˆ«ä¹‹å¤–çš„åˆ†å‰²ã€‚</li>
<li>è®­ç»ƒå…è´¹çš„æ–¹æ³•ä¸ºå¤„ç†æœªè§è¿‡çš„æ•°æ®æä¾›äº†å¯ä¼¸ç¼©ã€æ˜“äºéƒ¨ç½²çš„è§£å†³æ–¹æ¡ˆã€‚</li>
<li>å½“å‰æ–¹æ³•ç¼ºä¹åœ¨å¤æ‚ç¯å¢ƒä¸­å¯¹å¯¹è±¡çº§åˆ«ä¸Šä¸‹æ–‡çš„è€ƒè™‘ï¼Œè¿™é™åˆ¶äº†æ¨¡å‹çš„æ€§èƒ½ã€‚</li>
<li>æœ¬æ–‡ä»‹ç»äº†ä¸€ç§æ–°æ–¹æ³•ï¼Œé€šè¿‡èå…¥å¯¹è±¡çº§åˆ«ä¸Šä¸‹æ–‡çŸ¥è¯†æ¥æé«˜æ¨¡å‹æ€§èƒ½ã€‚</li>
<li>è¯¥æ–¹æ³•æé«˜äº†å¯¹è±¡å†…éƒ¨çš„ä¸€è‡´æ€§ï¼Œé€šè¿‡ç²¾ç‚¼æ–‡æœ¬åµŒå…¥ç¡®ä¿ä¸å›¾åƒä¸­å¯¹è±¡çš„å‡†ç¡®å¯¹é½ã€‚</li>
<li>åˆ©ç”¨å¯¹è±¡çº§åˆ«çš„ä¸Šä¸‹æ–‡çŸ¥è¯†ï¼Œæ‰€æå‡ºçš„æ–¹æ³•åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šå®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2411.17150">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-38ce5f2d9890b4959a80e23e4bff8955.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5b17542025b8df1633d8f8c54cbb60b5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b0bdf034384f8da4962c37811fdb7ccf.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-70e98ebfbdecc978c059d2cb079af8da.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-e5470087ecfe696acad0206281682d35.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Open-Vocabulary-Action-Localization-with-Iterative-Visual-Prompting"><a href="#Open-Vocabulary-Action-Localization-with-Iterative-Visual-Prompting" class="headerlink" title="Open-Vocabulary Action Localization with Iterative Visual Prompting"></a>Open-Vocabulary Action Localization with Iterative Visual Prompting</h2><p><strong>Authors:Naoki Wake, Atsushi Kanehira, Kazuhiro Sasabuchi, Jun Takamatsu, Katsushi Ikeuchi</strong></p>
<p>Video action localization aims to find the timings of specific actions from a long video. Although existing learning-based approaches have been successful, they require annotating videos, which comes with a considerable labor cost. This paper proposes a training-free, open-vocabulary approach based on emerging off-the-shelf vision-language models (VLMs). The challenge stems from the fact that VLMs are neither designed to process long videos nor tailored for finding actions. We overcome these problems by extending an iterative visual prompting technique. Specifically, we sample video frames and create a concatenated image with frame index labels, allowing a VLM to identify the frames that most likely correspond to the start and end of the action. By iteratively narrowing the sampling window around the selected frames, the estimation gradually converges to more precise temporal boundaries. We demonstrate that this technique yields reasonable performance, achieving results comparable to state-of-the-art zero-shot action localization. These results support the use of VLMs as a practical tool for understanding videos. Sample code is available at <a target="_blank" rel="noopener" href="https://microsoft.github.io/VLM-Video-Action-Localization/">https://microsoft.github.io/VLM-Video-Action-Localization/</a> </p>
<blockquote>
<p>è§†é¢‘åŠ¨ä½œå®šä½æ—¨åœ¨ä»é•¿è§†é¢‘ä¸­æ‰¾å‡ºç‰¹å®šåŠ¨ä½œçš„æ—¶é—´ç‚¹ã€‚å°½ç®¡ç°æœ‰çš„åŸºäºå­¦ä¹ çš„æ–¹æ³•å·²ç»å–å¾—äº†æˆåŠŸï¼Œä½†å®ƒä»¬éœ€è¦ç»™è§†é¢‘åšæ ‡æ³¨ï¼Œè¿™éœ€è¦ç›¸å½“å¤§çš„åŠ³åŠ›æˆæœ¬ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºç°æˆçš„è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰çš„æ— è®­ç»ƒã€å¼€æ”¾è¯æ±‡çš„æ–¹æ³•ã€‚æŒ‘æˆ˜åœ¨äºVLMsæ—¢æ²¡æœ‰è¢«è®¾è®¡æ¥å¤„ç†é•¿è§†é¢‘ï¼Œä¹Ÿæ²¡æœ‰é’ˆå¯¹å¯»æ‰¾åŠ¨ä½œè¿›è¡Œä¼˜åŒ–ã€‚æˆ‘ä»¬é€šè¿‡æ‰©å±•è¿­ä»£è§†è§‰æç¤ºæŠ€æœ¯æ¥å…‹æœè¿™äº›é—®é¢˜ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬é‡‡æ ·è§†é¢‘å¸§ï¼Œå¹¶ä¸å¸§ç´¢å¼•æ ‡ç­¾ä¸€èµ·åˆ›å»ºæ‹¼æ¥å›¾åƒï¼Œå…è®¸VLMè¯†åˆ«æœ€æœ‰å¯èƒ½å¯¹åº”äºåŠ¨ä½œå¼€å§‹å’Œç»“æŸçš„å¸§ã€‚é€šè¿‡å›´ç»•æ‰€é€‰å¸§è¿­ä»£ç¼©å°é‡‡æ ·çª—å£ï¼Œä¼°è®¡ä¼šé€æ¸æ”¶æ•›åˆ°æ›´ç²¾ç¡®çš„æ—¶é—´è¾¹ç•Œã€‚æˆ‘ä»¬è¯æ˜ï¼Œè¿™ç§æŠ€æœ¯å…·æœ‰è‰¯å¥½çš„æ€§èƒ½ï¼Œå®ç°äº†ä¸æœ€å…ˆè¿›çš„é›¶æ ·æœ¬åŠ¨ä½œå®šä½ç›¸å½“çš„ç»“æœã€‚è¿™äº›ç»“æœæ”¯æŒå°†VLMsä½œä¸ºç†è§£è§†é¢‘çš„å®é™…å·¥å…·ä½¿ç”¨ã€‚ç¤ºä¾‹ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://microsoft.github.io/VLM-Video-Action-Localization/">https://microsoft.github.io/VLM-Video-Action-Localization/</a>æ‰¾åˆ°ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2408.17422v5">PDF</a> 9 pages, 5 figures, 6 tables. Published in IEEE Access. Last updated   on April 7th, 2025</p>
<p><strong>Summary</strong><br>è§†é¢‘åŠ¨ä½œå®šä½æ—¨åœ¨ä»é•¿è§†é¢‘ä¸­æ‰¾å‡ºç‰¹å®šåŠ¨ä½œçš„æ—¶é—´ç‚¹ã€‚ç°æœ‰å­¦ä¹ ç±»æ–¹æ³•è™½ç„¶æˆåŠŸï¼Œä½†éœ€è¦æ ‡æ³¨è§†é¢‘ï¼ŒåŠ³åŠ¨æˆæœ¬è¾ƒé«˜ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºç°æˆçš„è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰çš„æ— è®­ç»ƒã€å¼€æ”¾è¯æ±‡æ–¹æ³•ã€‚é¢ä¸´çš„æŒ‘æˆ˜æºäºVLMsæ—¢ä¸é€‚ç”¨äºå¤„ç†é•¿è§†é¢‘ï¼Œä¹Ÿä¸é€‚ç”¨äºæŸ¥æ‰¾åŠ¨ä½œã€‚æˆ‘ä»¬é€šè¿‡é‡‡ç”¨è¿­ä»£è§†è§‰æç¤ºæŠ€æœ¯æ¥å…‹æœè¿™äº›é—®é¢˜ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬å¯¹è§†é¢‘å¸§è¿›è¡Œé‡‡æ ·ï¼Œå¹¶åˆ›å»ºå¸¦æœ‰å¸§ç´¢å¼•æ ‡ç­¾çš„æ‹¼æ¥å›¾åƒï¼Œä½¿VLMèƒ½å¤Ÿè¯†åˆ«æœ€å¯èƒ½å¯¹åº”äºåŠ¨ä½œå¼€å§‹å’Œç»“æŸçš„å¸§ã€‚é€šè¿‡è¿­ä»£ç¼©å°æ‰€é€‰å¸§å‘¨å›´çš„é‡‡æ ·çª—å£ï¼Œä¼°è®¡é€æ¸æ”¶æ•›åˆ°æ›´ç²¾ç¡®çš„æ—¶é—´è¾¹ç•Œã€‚æˆ‘ä»¬è¯æ˜ï¼Œè¿™ç§æ–¹æ³•å–å¾—äº†åˆç†çš„æ•ˆæœï¼Œå®ç°äº†ä¸æœ€å…ˆè¿›çš„é›¶æ ·æœ¬åŠ¨ä½œå®šä½ç›¸å½“çš„ç»“æœã€‚è¿™äº›ç»“æœæ”¯æŒä½¿ç”¨VLMsä½œä¸ºç†è§£è§†é¢‘çš„å®é™…å·¥å…·ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è§†é¢‘åŠ¨ä½œå®šä½æ—¨åœ¨ä»é•¿è§†é¢‘ä¸­æ‰¾å‡ºç‰¹å®šåŠ¨ä½œçš„æ—¶é—´ç‚¹ã€‚</li>
<li>ç°æœ‰æ–¹æ³•éœ€è¦æ ‡æ³¨è§†é¢‘ï¼ŒåŠ³åŠ¨æˆæœ¬è¾ƒé«˜ã€‚</li>
<li>æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºè§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰çš„æ— è®­ç»ƒã€å¼€æ”¾è¯æ±‡æ–¹æ³•ã€‚</li>
<li>VLMsé¢ä¸´å¤„ç†é•¿è§†é¢‘å’ŒæŸ¥æ‰¾åŠ¨ä½œçš„æŒ‘æˆ˜ã€‚</li>
<li>é€šè¿‡è¿­ä»£è§†è§‰æç¤ºæŠ€æœ¯å…‹æœè¿™äº›é—®é¢˜ã€‚</li>
<li>é€šè¿‡é‡‡æ ·è§†é¢‘å¸§å¹¶åˆ›å»ºå¸¦æœ‰å¸§ç´¢å¼•æ ‡ç­¾çš„æ‹¼æ¥å›¾åƒæ¥è¯†åˆ«åŠ¨ä½œè¾¹ç•Œã€‚</li>
<li>è¿™ç§æ–¹æ³•å–å¾—äº†åˆç†çš„æ•ˆæœï¼Œå®ç°äº†ä¸æœ€å…ˆè¿›çš„é›¶æ ·æœ¬åŠ¨ä½œå®šä½ç›¸å½“çš„æ€§èƒ½ï¼Œæ”¯æŒä½¿ç”¨VLMsä½œä¸ºç†è§£è§†é¢‘çš„å·¥å…·ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2408.17422">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-064b2531467c228c67d636ddca255c7d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0889f39b9f1678609fbd41f93db4dea7.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-c740446d9ea7ce94d29ed666d380e8ad.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-67637ab9a4f1cafeba4890813185e78e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f18a867870dace7544d08d15ad409885.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f89abb9c2871da85f04a9f9f076b3ce4.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="MSCPT-Few-shot-Whole-Slide-Image-Classification-with-Multi-scale-and-Context-focused-Prompt-Tuning"><a href="#MSCPT-Few-shot-Whole-Slide-Image-Classification-with-Multi-scale-and-Context-focused-Prompt-Tuning" class="headerlink" title="MSCPT: Few-shot Whole Slide Image Classification with Multi-scale and   Context-focused Prompt Tuning"></a>MSCPT: Few-shot Whole Slide Image Classification with Multi-scale and   Context-focused Prompt Tuning</h2><p><strong>Authors:Minghao Han, Linhao Qu, Dingkang Yang, Xukun Zhang, Xiaoying Wang, Lihua Zhang</strong></p>
<p>Multiple instance learning (MIL) has become a standard paradigm for the weakly supervised classification of whole slide images (WSIs). However, this paradigm relies on using a large number of labeled WSIs for training. The lack of training data and the presence of rare diseases pose significant challenges for these methods. Prompt tuning combined with pre-trained Vision-Language models (VLMs) is an effective solution to the Few-shot Weakly Supervised WSI Classification (FSWC) task. Nevertheless, applying prompt tuning methods designed for natural images to WSIs presents three significant challenges: 1) These methods fail to fully leverage the prior knowledge from the VLMâ€™s text modality; 2) They overlook the essential multi-scale and contextual information in WSIs, leading to suboptimal results; and 3) They lack exploration of instance aggregation methods. To address these problems, we propose a Multi-Scale and Context-focused Prompt Tuning (MSCPT) method for FSWC task. Specifically, MSCPT employs the frozen large language model to generate pathological visual language prior knowledge at multiple scales, guiding hierarchical prompt tuning. Additionally, we design a graph prompt tuning module to learn essential contextual information within WSI, and finally, a non-parametric cross-guided instance aggregation module has been introduced to derive the WSI-level features. Extensive experiments, visualizations, and interpretability analyses were conducted on five datasets and three downstream tasks using three VLMs, demonstrating the strong performance of our MSCPT. All codes have been made publicly accessible at <a target="_blank" rel="noopener" href="https://github.com/Hanminghao/MSCPT">https://github.com/Hanminghao/MSCPT</a>. </p>
<blockquote>
<p>å¤šä»»åŠ¡å­¦ä¹ ï¼ˆMILï¼‰å·²æˆä¸ºå…¨å¹»ç¯ç‰‡å›¾åƒï¼ˆWSIï¼‰å¼±ç›‘ç£åˆ†ç±»çš„æ ‡å‡†èŒƒå¼ã€‚ç„¶è€Œï¼Œè¯¥èŒƒå¼ä¾èµ–äºå¤§é‡æœ‰æ ‡ç­¾çš„WSIè¿›è¡Œè®­ç»ƒã€‚ç¼ºä¹è®­ç»ƒæ•°æ®å’Œç½•è§ç–¾ç—…çš„å­˜åœ¨ç»™è¿™äº›æ–¹æ³•å¸¦æ¥äº†é‡å¤§æŒ‘æˆ˜ã€‚ç»“åˆé¢„è®­ç»ƒçš„è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰çš„æç¤ºå¾®è°ƒæ˜¯è§£å†³å°æ ·æœ¬å¼±ç›‘ç£WSIåˆ†ç±»ï¼ˆFSWCï¼‰ä»»åŠ¡çš„æœ‰æ•ˆæ–¹æ³•ã€‚ç„¶è€Œï¼Œå°†é’ˆå¯¹è‡ªç„¶å›¾åƒè®¾è®¡çš„æç¤ºå¾®è°ƒæ–¹æ³•åº”ç”¨äºWSIé¢ä¸´ä¸‰å¤§æŒ‘æˆ˜ï¼š1ï¼‰è¿™äº›æ–¹æ³•æœªèƒ½å……åˆ†åˆ©ç”¨æ¥è‡ªVLMæ–‡æœ¬æ¨¡æ€çš„å…ˆéªŒçŸ¥è¯†ï¼›2ï¼‰å®ƒä»¬å¿½ç•¥äº†WSIä¸­çš„å¤šå°ºåº¦å’Œä¸Šä¸‹æ–‡ä¿¡æ¯ï¼Œå¯¼è‡´ç»“æœä¸ä½³ï¼›3ï¼‰å®ƒä»¬ç¼ºä¹å®ä¾‹èšåˆæ–¹æ³•çš„æ¢ç´¢ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ç”¨äºFSWCä»»åŠ¡çš„å¤šå°ºåº¦ã€å…³æ³¨ä¸Šä¸‹æ–‡çš„æç¤ºå¾®è°ƒï¼ˆMSCPTï¼‰æ–¹æ³•ã€‚å…·ä½“æ¥è¯´ï¼ŒMSCPTé‡‡ç”¨å†»ç»“çš„å¤§å‹è¯­è¨€æ¨¡å‹æ¥ç”Ÿæˆå¤šå°ºåº¦çš„ç—…ç†è§†è§‰è¯­è¨€å…ˆéªŒçŸ¥è¯†ï¼Œå¼•å¯¼åˆ†å±‚æç¤ºå¾®è°ƒã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸€ä¸ªå›¾æç¤ºå¾®è°ƒæ¨¡å—æ¥å­¦ä¹ WSIå†…çš„å…³é”®ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼Œå¹¶æœ€ç»ˆå¼•å…¥äº†ä¸€ä¸ªéå‚æ•°åŒ–äº¤å‰å¼•å¯¼å®ä¾‹èšåˆæ¨¡å—æ¥æå–WSIçº§åˆ«çš„ç‰¹å¾ã€‚åœ¨äº”ä¸ªæ•°æ®é›†å’Œä¸‰ä¸ªä¸‹æ¸¸ä»»åŠ¡ä¸Šï¼Œä½¿ç”¨ä¸‰ç§VLMè¿›è¡Œäº†å¹¿æ³›å®éªŒã€å¯è§†åŒ–å’Œå¯è§£é‡Šæ€§åˆ†æï¼Œè¯æ˜äº†æˆ‘ä»¬çš„MSCPTçš„å¼ºå¤§æ€§èƒ½ã€‚æ‰€æœ‰ä»£ç å·²å…¬å¼€å‘å¸ƒåœ¨<a target="_blank" rel="noopener" href="https://github.com/Hanminghao/MSCPT%E3%80%82">https://github.com/Hanminghao/MSCPTã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2408.11505v2">PDF</a> This work has been submitted to the IEEE TMI for possible publication</p>
<p><strong>Summary</strong><br>     é’ˆå¯¹å¼±ç›‘ç£å…¨å¹»ç¯ç‰‡å›¾åƒåˆ†ç±»ä»»åŠ¡ä¸­è®­ç»ƒæ•°æ®ä¸è¶³å’Œç½•è§ç–¾ç—…çš„é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§å¤šå°ºåº¦ä¸ä¸Šä¸‹æ–‡èšç„¦çš„æç¤ºè°ƒæ•´ï¼ˆMSCPTï¼‰æ–¹æ³•ã€‚è¯¥æ–¹æ³•åˆ©ç”¨å†»ç»“çš„å¤§å‹è¯­è¨€æ¨¡å‹ç”Ÿæˆç—…ç†è§†è§‰è¯­è¨€å…ˆéªŒçŸ¥è¯†ï¼Œè®¾è®¡äº†ä¸€ä¸ªå›¾æç¤ºè°ƒæ•´æ¨¡å—æ¥å­¦ä¹ å¹»ç¯ç‰‡å›¾åƒå†…çš„å…³é”®ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼Œå¹¶å¼•å…¥äº†ä¸€ä¸ªéå‚æ•°åŒ–äº¤å‰å¼•å¯¼å®ä¾‹èšåˆæ¨¡å—æ¥æå–å¹»ç¯ç‰‡å›¾åƒçº§åˆ«çš„ç‰¹å¾ã€‚åœ¨äº”ä¸ªæ•°æ®é›†å’Œä¸‰ä¸ªä¸‹æ¸¸ä»»åŠ¡ä¸Šè¿›è¡Œçš„å¹¿æ³›å®éªŒéªŒè¯äº†MSCPTæ–¹æ³•çš„å¼ºå¤§æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤šå®ä¾‹å­¦ä¹ ï¼ˆMILï¼‰å·²æˆä¸ºå…¨å¹»ç¯ç‰‡å›¾åƒï¼ˆWSIï¼‰å¼±ç›‘ç£åˆ†ç±»çš„æ ‡å‡†èŒƒå¼ï¼Œä½†ä¾èµ–äºå¤§é‡æ ‡è®°æ•°æ®è¿›è¡Œè®­ç»ƒã€‚</li>
<li>é’ˆå¯¹ç¼ºä¹è®­ç»ƒæ•°æ®å’Œç½•è§ç–¾ç—…çš„é—®é¢˜ï¼Œæå‡ºäº†ç»“åˆé¢„è®­ç»ƒè§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰çš„æç¤ºè°ƒæ•´æ–¹æ³•ã€‚</li>
<li>åº”ç”¨äºè‡ªç„¶å›¾åƒçš„æç¤ºè°ƒæ•´æ–¹æ³•åœ¨å…¨å¹»ç¯ç‰‡å›¾åƒä¸Šé‡åˆ°ä¸‰å¤§æŒ‘æˆ˜ï¼šæœªèƒ½å……åˆ†åˆ©ç”¨VLMçš„æ–‡æœ¬æ¨¡æ€å…ˆéªŒçŸ¥è¯†ã€å¿½ç•¥å¤šå°ºåº¦å’Œä¸Šä¸‹æ–‡ä¿¡æ¯ã€ç¼ºä¹å®ä¾‹èšåˆæ–¹æ³•çš„æ¢ç´¢ã€‚</li>
<li>æå‡ºçš„å¤šå°ºåº¦ä¸ä¸Šä¸‹æ–‡èšç„¦çš„æç¤ºè°ƒæ•´ï¼ˆMSCPTï¼‰æ–¹æ³•é’ˆå¯¹è¿™äº›æŒ‘æˆ˜è¿›è¡Œæ”¹è¿›ã€‚</li>
<li>MSCPTåˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ç”Ÿæˆç—…ç†è§†è§‰è¯­è¨€å…ˆéªŒçŸ¥è¯†ï¼Œå¹¶è®¾è®¡å›¾æç¤ºè°ƒæ•´æ¨¡å—å­¦ä¹ WSIå†…çš„å…³é”®ä¸Šä¸‹æ–‡ä¿¡æ¯ã€‚</li>
<li>å¼•å…¥éå‚æ•°åŒ–äº¤å‰å¼•å¯¼å®ä¾‹èšåˆæ¨¡å—ï¼Œä»¥æå–WSIçº§åˆ«çš„ç‰¹å¾ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2408.11505">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-f4724a53eacdccd365aeebc366316bc3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d8bf6a1a9768792668d85745c8fde8ad.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-988c4c0047bbbcbb97c28564f9fa8423.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="CFPFormer-Feature-pyramid-like-Transformer-Decoder-for-Segmentation-and-Detection"><a href="#CFPFormer-Feature-pyramid-like-Transformer-Decoder-for-Segmentation-and-Detection" class="headerlink" title="CFPFormer: Feature-pyramid like Transformer Decoder for Segmentation and   Detection"></a>CFPFormer: Feature-pyramid like Transformer Decoder for Segmentation and   Detection</h2><p><strong>Authors:Hongyi Cai, Mohammad Mahdinur Rahman, Wenzhen Dong, Jingyu Wu</strong></p>
<p>Feature pyramids have been widely adopted in convolutional neural networks and transformers for tasks in medical image segmentation. However, existing models generally focus on the Encoder-side Transformer for feature extraction. We further explore the potential in improving the feature decoder with a well-designed architecture. We propose Cross Feature Pyramid Transformer decoder (CFPFormer), a novel decoder block that integrates feature pyramids and transformers. Even though transformer-like architecture impress with outstanding performance in segmentation, the concerns to reduce the redundancy and training costs still exist. Specifically, by leveraging patch embedding, cross-layer feature concatenation mechanisms, CFPFormer enhances feature extraction capabilities while complexity issue is mitigated by our Gaussian Attention. Benefiting from Transformer structure and U-shaped connections, our work is capable of capturing long-range dependencies and effectively up-sample feature maps. Experimental results are provided to evaluate CFPFormer on medical image segmentation datasets, demonstrating the efficacy and effectiveness. With a ResNet50 backbone, our method achieves 92.02% Dice Score, highlighting the efficacy of our methods. Notably, our VGG-based model outperformed baselines with more complex ViT and Swin Transformer backbone. </p>
<blockquote>
<p>ç‰¹å¾é‡‘å­—å¡”åœ¨å·ç§¯ç¥ç»ç½‘ç»œå’Œè½¬æ¢å™¨ä¸­å¹¿æ³›åº”ç”¨äºåŒ»å­¦å›¾åƒåˆ†å‰²ä»»åŠ¡ã€‚ç„¶è€Œï¼Œç°æœ‰æ¨¡å‹é€šå¸¸å…³æ³¨äºç¼–ç å™¨ä¾§çš„è½¬æ¢å™¨è¿›è¡Œç‰¹å¾æå–ã€‚æˆ‘ä»¬è¿›ä¸€æ­¥æ¢ç´¢äº†é€šè¿‡ç²¾å¿ƒè®¾è®¡æ¶æ„æ¥æ”¹è¿›ç‰¹å¾è§£ç å™¨çš„æ½œåŠ›ã€‚æˆ‘ä»¬æå‡ºäº†è·¨ç‰¹å¾é‡‘å­—å¡”è½¬æ¢å™¨è§£ç å™¨ï¼ˆCFPFormerï¼‰ï¼Œè¿™æ˜¯ä¸€ç§æ–°çš„è§£ç å™¨å—ï¼Œå®ƒå°†ç‰¹å¾é‡‘å­—å¡”å’Œè½¬æ¢å™¨é›†æˆåœ¨ä¸€èµ·ã€‚å°½ç®¡å˜å‹å™¨å¼æ¶æ„åœ¨åˆ†å‰²æ–¹é¢è¡¨ç°å‡ºä»¤äººå°è±¡æ·±åˆ»çš„æ€§èƒ½ï¼Œä½†å‡å°‘å†—ä½™å’Œè®­ç»ƒæˆæœ¬çš„æ‹…å¿§ä»ç„¶å­˜åœ¨ã€‚å…·ä½“æ¥è¯´ï¼Œé€šè¿‡åˆ©ç”¨è¡¥ä¸åµŒå…¥ã€è·¨å±‚ç‰¹å¾æ‹¼æ¥æœºåˆ¶ï¼ŒCFPFormerå¢å¼ºäº†ç‰¹å¾æå–èƒ½åŠ›ï¼Œè€Œå¤æ‚æ€§é—®é¢˜åˆ™é€šè¿‡æˆ‘ä»¬çš„é«˜æ–¯æ³¨æ„åŠ›å¾—åˆ°äº†ç¼“è§£ã€‚å¾—ç›ŠäºTransformerç»“æ„å’ŒUå½¢è¿æ¥ï¼Œæˆ‘ä»¬çš„å·¥ä½œèƒ½å¤Ÿæ•æ‰è¿œç¨‹ä¾èµ–å…³ç³»å¹¶æœ‰æ•ˆåœ°ä¸Šé‡‡æ ·ç‰¹å¾å›¾ã€‚å®éªŒç»“æœä¸ºCFPFormeråœ¨åŒ»å­¦å›¾åƒåˆ†å‰²æ•°æ®é›†ä¸Šçš„è¡¨ç°æä¾›äº†è¯„ä¼°ï¼Œè¯æ˜äº†å…¶æœ‰æ•ˆæ€§å’Œæœ‰æ•ˆæ€§ã€‚ä½¿ç”¨ResNet50ä¸»å¹²ç½‘ï¼Œæˆ‘ä»¬çš„æ–¹æ³•å®ç°äº†92.02%çš„Diceå¾—åˆ†ï¼Œçªæ˜¾äº†æˆ‘ä»¬çš„æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œæˆ‘ä»¬åŸºäºVGGçš„æ¨¡å‹åœ¨å…·æœ‰æ›´å¤æ‚çš„ViTå’ŒSwin Transformerä¸»å¹²ç½‘çš„åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°æ›´å¥½ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2404.15451v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ¢ç´¢äº†åœ¨åŒ»ç–—å›¾åƒåˆ†å‰²ä»»åŠ¡ä¸­æ”¹è¿›ç‰¹å¾è§£ç å™¨çš„æ½œåŠ›ï¼Œæå‡ºäº†ä¸€ç§æ–°çš„è§£ç å™¨å—â€”â€”Cross Feature Pyramid Transformerï¼ˆCFPFormerï¼‰ã€‚CFPFormerç»“åˆäº†ç‰¹å¾é‡‘å­—å¡”å’Œå˜å‹å™¨ï¼Œé€šè¿‡åˆ©ç”¨è¡¥ä¸åµŒå…¥å’Œè·¨å±‚ç‰¹å¾æ‹¼æ¥æœºåˆ¶ï¼Œæé«˜äº†ç‰¹å¾æå–èƒ½åŠ›ã€‚åŒæ—¶ï¼Œé€šè¿‡é«˜æ–¯æ³¨æ„åŠ›æœºåˆ¶é™ä½äº†å¤æ‚æ€§å’Œè®­ç»ƒæˆæœ¬ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒCFPFormeråœ¨åŒ»ç–—å›¾åƒåˆ†å‰²æ•°æ®é›†ä¸Šè¡¨ç°å‡ºè‰²ï¼Œä½¿ç”¨ResNet50éª¨å¹²ç½‘çš„æ–¹æ³•è¾¾åˆ°92.02%çš„Diceå¾—åˆ†ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼ŒåŸºäºVGGçš„æ¨¡å‹è¶…è¶Šäº†å…·æœ‰æ›´å¤æ‚ViTå’ŒSwin Transformeréª¨å¹²ç½‘çš„åŸºçº¿æ¨¡å‹ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ–‡ç« æ¢ç´¢äº†æ”¹è¿›åŒ»ç–—å›¾åƒåˆ†å‰²ä»»åŠ¡ä¸­çš„ç‰¹å¾è§£ç å™¨çš„æ½œåŠ›ã€‚</li>
<li>æå‡ºäº†ä¸€ç§æ–°çš„è§£ç å™¨å—â€”â€”Cross Feature Pyramid Transformerï¼ˆCFPFormerï¼‰ã€‚</li>
<li>CFPFormerç»“åˆäº†ç‰¹å¾é‡‘å­—å¡”å’Œå˜å‹å™¨ï¼Œä»¥æé«˜ç‰¹å¾æå–èƒ½åŠ›ã€‚</li>
<li>é€šè¿‡åˆ©ç”¨è¡¥ä¸åµŒå…¥å’Œè·¨å±‚ç‰¹å¾æ‹¼æ¥æœºåˆ¶ï¼ŒCFPFormerèƒ½å¤Ÿå¢å¼ºç‰¹å¾æå–æ•ˆæœã€‚</li>
<li>é«˜æ–¯æ³¨æ„åŠ›æœºåˆ¶ç”¨äºé™ä½å¤æ‚æ€§å’Œè®­ç»ƒæˆæœ¬ã€‚</li>
<li>å®éªŒç»“æœè¡¨æ˜ï¼ŒCFPFormeråœ¨åŒ»ç–—å›¾åƒåˆ†å‰²ä»»åŠ¡ä¸Šè¡¨ç°ä¼˜å¼‚ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2404.15451">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-5773c7a05e0a6ee5893e1685bcfd7008.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7cf52c207b1d182127384714c7305f7a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-40247c64d0eafcc5ed8185ac508d35dd.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-8eedcae206f7fd25bb9c5321f2292d92.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2fb1cb856537fb0eb4412f8416c58583.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-86192fe31cfca47a1847e2914aec69a2.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-5f5e61aa088f876558984c311ce48b84.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d53e3197cfe7315df34e47175026c6c1.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-04-09/Vision%20Transformer/">https://kedreamix.github.io/Talk2Paper/Paper/2025-04-09/Vision%20Transformer/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Vision-Transformer/">
                                    <span class="chip bg-color">Vision Transformer</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-04-09/%E6%A3%80%E6%B5%8B_%E5%88%86%E5%89%B2_%E8%B7%9F%E8%B8%AA/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-b0bdf034384f8da4962c37811fdb7ccf.jpg" class="responsive-img" alt="æ£€æµ‹/åˆ†å‰²/è·Ÿè¸ª">
                        
                        <span class="card-title">æ£€æµ‹/åˆ†å‰²/è·Ÿè¸ª</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            æ£€æµ‹/åˆ†å‰²/è·Ÿè¸ª æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-04-09  SSLFusion Scale & Space Aligned Latent Fusion Model for Multimodal 3D   Object Detection
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-04-09
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E6%A3%80%E6%B5%8B-%E5%88%86%E5%89%B2-%E8%B7%9F%E8%B8%AA/" class="post-category">
                                    æ£€æµ‹/åˆ†å‰²/è·Ÿè¸ª
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E6%A3%80%E6%B5%8B-%E5%88%86%E5%89%B2-%E8%B7%9F%E8%B8%AA/">
                        <span class="chip bg-color">æ£€æµ‹/åˆ†å‰²/è·Ÿè¸ª</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-04-09/%E8%A7%86%E9%A2%91%E7%90%86%E8%A7%A3/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-072ab81ec0286d7e04e42a21a13fbfe5.jpg" class="responsive-img" alt="è§†é¢‘ç†è§£">
                        
                        <span class="card-title">è§†é¢‘ç†è§£</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            è§†é¢‘ç†è§£ æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-04-09  InstructionBench An Instructional Video Understanding Benchmark
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-04-09
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E8%A7%86%E9%A2%91%E7%90%86%E8%A7%A3/" class="post-category">
                                    è§†é¢‘ç†è§£
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E8%A7%86%E9%A2%91%E7%90%86%E8%A7%A3/">
                        <span class="chip bg-color">è§†é¢‘ç†è§£</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">29301k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
