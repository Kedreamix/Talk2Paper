<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="LLM">
    <meta name="description" content="LLM 方向最新论文已更新，请持续关注 Update in 2025-04-09  URECA Unique Region Caption Anything">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>LLM | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loader页面消失采用渐隐的方式*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logo出现动画 */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//使用渐隐的方法淡出loading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//强制显示loading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">嘘~  正在从服务器偷取页面 . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>首页</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>标签</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>分类</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>归档</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="搜索" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="深色/浅色模式" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			首页
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			标签
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			分类
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			归档
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pica.zhimg.com/v2-4bf1f6e626846a87d9d4abeeabb6cb96.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">LLM</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- 文章内容详情 -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/LLM/">
                                <span class="chip bg-color">LLM</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/LLM/" class="post-category">
                                LLM
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp;
                    2025-04-09
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>更新日期:&nbsp;&nbsp;
                    2025-05-14
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>文章字数:&nbsp;&nbsp;
                    20.8k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>阅读时长:&nbsp;&nbsp;
                    85 分
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>阅读次数:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>⚠️ 以下所有内容总结都来自于 大语言模型的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p>
</blockquote>
<h1 id="2025-04-09-更新"><a href="#2025-04-09-更新" class="headerlink" title="2025-04-09 更新"></a>2025-04-09 更新</h1><h2 id="URECA-Unique-Region-Caption-Anything"><a href="#URECA-Unique-Region-Caption-Anything" class="headerlink" title="URECA: Unique Region Caption Anything"></a>URECA: Unique Region Caption Anything</h2><p><strong>Authors:Sangbeom Lim, Junwan Kim, Heeji Yoon, Jaewoo Jung, Seungryong Kim</strong></p>
<p>Region-level captioning aims to generate natural language descriptions for specific image regions while highlighting their distinguishing features. However, existing methods struggle to produce unique captions across multi-granularity, limiting their real-world applicability. To address the need for detailed region-level understanding, we introduce URECA dataset, a large-scale dataset tailored for multi-granularity region captioning. Unlike prior datasets that focus primarily on salient objects, URECA dataset ensures a unique and consistent mapping between regions and captions by incorporating a diverse set of objects, parts, and background elements. Central to this is a stage-wise data curation pipeline, where each stage incrementally refines region selection and caption generation. By leveraging Multimodal Large Language Models (MLLMs) at each stage, our pipeline produces distinctive and contextually grounded captions with improved accuracy and semantic diversity. Building upon this dataset, we present URECA, a novel captioning model designed to effectively encode multi-granularity regions. URECA maintains essential spatial properties such as position and shape through simple yet impactful modifications to existing MLLMs, enabling fine-grained and semantically rich region descriptions. Our approach introduces dynamic mask modeling and a high-resolution mask encoder to enhance caption uniqueness. Experiments show that URECA achieves state-of-the-art performance on URECA dataset and generalizes well to existing region-level captioning benchmarks. </p>
<blockquote>
<p>区域级标题生成旨在针对特定图像区域生成自然语言描述，同时突出其区分特征。然而，现有方法在跨多粒度生成唯一标题方面存在困难，限制了它们在现实世界中的应用。为了解决对详细区域级理解的需求，我们引入了URECA数据集，这是一个专为多粒度区域标题设计的大规模数据集。与主要关注显著物体的先前数据集不同，URECA数据集通过融入各种物体、部分和背景元素，确保区域和标题之间的独特且一致的映射。关键在于分阶段数据整理管道，每个阶段都会逐步改进区域选择和标题生成。我们的管道利用多模态大型语言模型（MLLMs）在每个阶段进行精细化处理，生成独特且基于上下文的标题，提高了准确性和语义多样性。在此基础上，我们推出了URECA，这是一种新型标题生成模型，旨在有效编码多粒度区域。URECA通过简单而有效的修改现有MLLMs，保留关键的空间属性（如位置和形状），从而实现精细且语义丰富的区域描述。我们的方法引入了动态掩模建模和高分辨率掩模编码器，以提高标题的独创性。实验表明，URECA在URECA数据集上达到了最先进的性能，并在现有的区域级标题生成基准测试中具有良好的通用性。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.05305v1">PDF</a> Project page: <a target="_blank" rel="noopener" href="https://cvlab-kaist.github.io/URECA">https://cvlab-kaist.github.io/URECA</a> Code:   <a target="_blank" rel="noopener" href="https://github.com/cvlab-kaist/URECA">https://github.com/cvlab-kaist/URECA</a></p>
<p><strong>Summary</strong></p>
<p>本文旨在解决现有图像区域描述方法在多粒度区域描述上的不足，为此引入了URECA数据集和URECA模型。URECA数据集通过阶段式数据整理管道，结合多模态大型语言模型（MLLMs），确保区域与描述的独特且一致映射。URECA模型则通过修改现有MLLMs，增加动态掩模建模和高分辨率掩模编码器，有效编码多粒度区域并提升描述性能。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>区域级描述旨在生成特定图像区域的自然语言描述，并突出其区分特征。</li>
<li>现有方法在生成多粒度区域的独特描述方面存在挑战，限制了其在现实世界的应用。</li>
<li>URECA数据集的引入是为了满足对多粒度区域级别的详细理解的需求。</li>
<li>URECA数据集通过阶段式数据整理管道和多模态大型语言模型（MLLMs）的结合，确保区域与描述的独特映射。</li>
<li>URECA模型能有效编码多粒度区域，并维持关键的空间属性如位置和形状。</li>
<li>URECA模型通过动态掩模建模和高分辨率掩模编码器提升描述性能。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.05305">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-6dee61b1b00017453f047cae7484cc48.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b98b5f0268a9620e444676fb6ea87d69.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-023be26220a714450195378d31964b50.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ac22fab9c12fc485bbed9c1acc7c62b9.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="What-We-Do-Not-Know-GPT-Use-in-Business-and-Management"><a href="#What-We-Do-Not-Know-GPT-Use-in-Business-and-Management" class="headerlink" title="What We Do Not Know: GPT Use in Business and Management"></a>What We Do Not Know: GPT Use in Business and Management</h2><p><strong>Authors:Tammy Mackenzie, Branislav Radeljic, Leslie Salgado, Animesh Paul, Rubaina Khan, Aizhan Tursunbayeva, Natalie Perez, Sreyoshi Bhaduri</strong></p>
<p>This systematic review examines peer-reviewed studies on application of GPT in business management, revealing significant knowledge gaps. Despite identifying interesting research directions such as best practices, benchmarking, performance comparisons, social impacts, our analysis yields only 42 relevant studies for the 22 months since its release. There are so few studies looking at a particular sector or subfield that management researchers, business consultants, policymakers, and journalists do not yet have enough information to make well-founded statements on how GPT is being used in businesses. The primary contribution of this paper is a call to action for further research. We provide a description of current research and identify knowledge gaps on the use of GPT in business. We cover the management subfields of finance, marketing, human resources, strategy, operations, production, and analytics, excluding retail and sales. We discuss gaps in knowledge of GPT potential consequences on employment, productivity, environmental costs, oppression, and small businesses. We propose how management consultants and the media can help fill those gaps. We call for practical work on business control systems as they relate to existing and foreseeable AI-related business challenges. This work may be of interest to managers, to management researchers, and to people working on AI in society. </p>
<blockquote>
<p>这篇系统综述考察了关于GPT在商业管理中的应用的同行评审研究，揭示了显著的知识空白。尽管确定了有趣的研究方向，例如最佳实践、基准测试、性能比较、社会影响等，但自GPT发布以来的短短22个月内，我们仅分析出了42项相关研究。对于某一特定领域或子领域，如管理层面的研究、商业顾问、政策制定者和记者等，关于GPT在商业中如何应用的陈述尚缺乏足够的信息支持。本文的主要贡献是呼吁进一步采取行动进行研究。我们描述了当前的研究现状，并指出了在GPT在商业领域应用的知识空白。我们涵盖了金融、市场营销、人力资源、战略、运营、生产和分析的管理子领域，但不包括零售和销售。我们讨论了GPT对就业、生产力、环境成本、压迫和小型企业可能产生的潜在后果的知识空白。我们提出管理咨询机构和媒体应如何帮助填补这些空白。我们呼吁关于企业控制系统在应对当前和未来的人工智能相关商业挑战上的实际应用工作。本文可能对于经理、管理研究人员和从事人工智能在社会工作的人们感兴趣。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.05273v1">PDF</a> 16 pages, 5 tables, 2 figures</p>
<p><strong>Summary</strong><br>     该文献对GPT在企业管理的应用进行了系统综述，发现存在显著的知识空白。尽管确定了有趣的研究方向，如最佳实践、基准测试、性能比较和社会影响等，但在GPT发布后的短短22个月内，仅发现了42项相关研究。关于GPT在特定行业或子领域的应用，管理研究人员、业务顾问、政策制定者和记者尚未获得足够的信息来做出合理的陈述。本文的主要贡献是呼吁采取行动进行进一步的研究。本文描述了当前的研究现状并确定了关于GPT在商业应用中的知识空白。所涉及的管理子领域包括金融、市场营销、人力资源、战略、运营、生产和分析等领域（零售和销售除外）。文章探讨了关于GPT的潜在后果在就业、生产力、环境成本、压迫和小型企业等方面的知识空白。本文提出了管理顾问和媒体如何帮助填补这些空白的方法。并呼吁针对现有的和可预见的与人工智能相关的商业挑战开展实际工作。这一工作对管理者、管理研究人员和从事人工智能社会研究的人员可能都有帮助。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>GPT在企业管理方面的应用存在显著的知识空白，尽管已有少量研究涉及最佳实践、基准测试等方向。</li>
<li>目前关于GPT在特定行业或子领域的应用研究仍然不足，使得相关领域的专家无法做出基于充分信息的陈述。</li>
<li>文章呼吁进一步的研究行动，特别是在填补当前关于GPT在商业中的知识空白方面。</li>
<li>文中探讨了GPT对就业、生产力、环境成本等方面可能产生的影响，并指出这些方面的知识空白。</li>
<li>文章建议管理顾问和媒体在填补关于GPT在商业应用中的知识空白方面发挥更大的作用。</li>
<li>文章强调针对现有的和可预见的与人工智能相关的商业挑战开展实际工作的重要性。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.05273">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-d053411c963af81db56c19c576ae5cbe.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-2d065c4bd5d37e432ebab3aef67977a4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-11a1620dee6920b62743f832fa597765.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3aa83805ccf6b01558d651bdce20416f.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Do-PhD-level-LLMs-Truly-Grasp-Elementary-Addition-Probing-Rule-Learning-vs-Memorization-in-Large-Language-Models"><a href="#Do-PhD-level-LLMs-Truly-Grasp-Elementary-Addition-Probing-Rule-Learning-vs-Memorization-in-Large-Language-Models" class="headerlink" title="Do PhD-level LLMs Truly Grasp Elementary Addition? Probing Rule Learning   vs. Memorization in Large Language Models"></a>Do PhD-level LLMs Truly Grasp Elementary Addition? Probing Rule Learning   vs. Memorization in Large Language Models</h2><p><strong>Authors:Yang Yan, Yu Lu, Renjun Xu, Zhenzhong Lan</strong></p>
<p>Despite high benchmark scores, Large Language Models (LLMs) often fail simple problem, raising a critical question: Do LLMs learn mathematical principles or merely memorize patterns? Rather than designing increasingly complex benchmarks like recent works, we investigate this using elementary two-integer addition ($0$ to $2^{64}$), probing two core properties: commutativity ($A+B&#x3D;B+A$) and compositional generalization (via isomorphic symbolic mappings, e.g., $7 \rightarrow y$). While state-of-the-art LLMs achieve 73.8-99.8% accuracy on numerical addition, performance collapses to $\leq$7.5% under symbolic mapping, indicating failure to generalize learned rules. Non-monotonic performance scaling with digit count and frequent commutativity violations (over 1,700 cases of $A+B \neq B+A$) further support this. Explicitly providing addition rules degrades performance by 81.2% on average, while self-explanation maintains baseline accuracy, suggesting LLM arithmetic processing is misaligned with human-defined principles. Our findings indicate current LLMs rely on memory pattern over genuine rule learning, highlighting architectural limitations and the need for new approaches to achieve true mathematical reasoning. </p>
<blockquote>
<p>尽管大型语言模型（LLM）的基准测试得分很高，但它们往往无法解决简单的问题，这引发了一个关键问题：LLM是学习数学原理还是仅仅记忆模式？我们并未采用最近工作中日益复杂的基准测试来探究这个问题，而是使用基本的两整数加法（从0到2^64），探究了两个核心属性：交换律（A+B&#x3D;B+A）和组合泛化（通过同构符号映射，例如7→y）。虽然最新的大型语言模型在数字加法方面达到了73.8-99.8%的准确率，但在符号映射下，性能下降到≤7.5%，表明无法泛化学习到的规则。随着数字计数的非单调性能缩放以及频繁的交换律违规（超过1700个A+B≠B+A的情况），进一步证实了这一点。明确提供加法规则平均降低了81.2%的性能，而自我解释则能保持基线准确率，这表明大型语言模型的算术处理与人类定义的原理存在偏差。我们的研究结果表明，当前的大型语言模型依赖于记忆模式而非真正的规则学习，这突显了架构的局限性以及实现真正数学推理的需要。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.05262v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>大型语言模型（LLM）在基准测试中表现优异，但在面对简单问题时常常失效。本文通过研究基本的两整数加法（从0到2^64）中的两个核心属性：交换律（A+B&#x3D;B+A）和组合泛化能力（通过同构符号映射），发现LLM在符号映射下的性能急剧下降到≤7.5%，表明其无法泛化学习到的规则。同时发现非单调性能随数字计数变化以及频繁违反交换律（超过1700个案例）。提供加法规则会平均降低性能81.2%，而自我解释则能维持基准准确性，暗示LLM的数学处理与人为定义的原则不符。本研究显示，当前LLM依赖于记忆模式而非真正的规则学习，突显了架构的局限性，并需要新方法来实现真正的数学推理。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>大型语言模型（LLM）在基准测试中表现良好，但在简单的数学问题中常表现不佳。</li>
<li>通过研究基本的两整数加法，发现LLM在符号映射下的性能急剧下降，表明其无法泛化学习到的数学规则。</li>
<li>LLM在处理数学问题时存在非单调性能变化，且频繁违反数学原则（如交换律）。</li>
<li>提供明确的数学规则会大幅降低LLM的性能，而自我解释则能维持其基准准确性。</li>
<li>LLM的数学处理能力与人类定义的原则存在偏差。</li>
<li>当前LLM主要依赖记忆模式而非真正的规则学习。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.05262">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-b1871cc0b53305f7cd3d00e8d051c4d4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2c8617d79078a6b23ac58d1372275c2f.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-edeca4edb9158b0fb5f4a68d4635e77e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-51f07887c43c8fddd488a4649efc665a.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-2cc890363738b960e955346eced28615.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="How-to-evaluate-control-measures-for-LLM-agents-A-trajectory-from-today-to-superintelligence"><a href="#How-to-evaluate-control-measures-for-LLM-agents-A-trajectory-from-today-to-superintelligence" class="headerlink" title="How to evaluate control measures for LLM agents? A trajectory from today   to superintelligence"></a>How to evaluate control measures for LLM agents? A trajectory from today   to superintelligence</h2><p><strong>Authors:Tomek Korbak, Mikita Balesni, Buck Shlegeris, Geoffrey Irving</strong></p>
<p>As LLM agents grow more capable of causing harm autonomously, AI developers will rely on increasingly sophisticated control measures to prevent possibly misaligned agents from causing harm. AI developers could demonstrate that their control measures are sufficient by running control evaluations: testing exercises in which a red team produces agents that try to subvert control measures. To ensure control evaluations accurately capture misalignment risks, the affordances granted to this red team should be adapted to the capability profiles of the agents to be deployed under control measures.   In this paper we propose a systematic framework for adapting affordances of red teams to advancing AI capabilities. Rather than assuming that agents will always execute the best attack strategies known to humans, we demonstrate how knowledge of an agents’s actual capability profile can inform proportional control evaluations, resulting in more practical and cost-effective control measures. We illustrate our framework by considering a sequence of five fictional models (M1-M5) with progressively advanced capabilities, defining five distinct AI control levels (ACLs). For each ACL, we provide example rules for control evaluation, control measures, and safety cases that could be appropriate. Finally, we show why constructing a compelling AI control safety case for superintelligent LLM agents will require research breakthroughs, highlighting that we might eventually need alternative approaches to mitigating misalignment risk. </p>
<blockquote>
<p>随着LLM代理造成自主伤害的能力不断增强，AI开发者将依赖越来越复杂的控制措施来防止可能的对齐不当的代理造成伤害。AI开发者可以通过运行控制评估来证明其控制措施是足够的：测试演习中，红队产生代理试图破坏控制措施。为了确保控制评估准确捕捉错位风险，给予红队的权限应该适应于在控制措施下要部署的代理的能力特征。在本文中，我们提出了一个系统的框架，以适应红队权限与AI能力的进步。我们并不假设代理总是会执行人类已知的最佳攻击策略，而是展示了了解代理的实际能力特征如何推动比例控制评估，从而导致更实用、更经济的控制措施。我们通过考虑一系列具有渐进能力的五个虚构模型（M1-M5）来说明我们的框架，定义五个不同的AI控制级别（ACL）。对于每个ACL，我们提供控制评估、控制措施和安全案例的示例规则，这些规则可能是适当的。最后，我们展示了为什么为超级智能LLM代理构建有说服力的AI控制安全案例将需要研究突破，并强调我们可能最终需要采用替代方法来减轻错位风险。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.05259v1">PDF</a> </p>
<p><strong>Summary</strong>：随着LLM代理伤害自主能力的增强，AI开发者将依赖更先进的控制措施来防止代理可能的不对齐造成伤害。通过运行控制评估来验证其控制措施的有效性，而评估中的红队需适应部署的代理的能力特征。本文提出了一个适应红队能力与AI能力发展的系统框架，并展示了如何利用代理的实际能力特征进行比例控制评估，从而实现更实用和成本效益更高的控制措施。通过考虑一系列具有渐进先进能力特征的虚构模型（M1-M5），本文定义了五个不同的AI控制级别（ACL），并为每个ACL提供了控制评估规则、控制措施和安全案例的示例。最后，本文强调了为超级智能LLM代理构建可靠的AI控制安全案例将需要研究突破，并指出了可能需要替代方法来减轻不对齐风险的原因。</p>
<p><strong>Key Takeaways</strong>:</p>
<ul>
<li>随着LLM代理自主伤害能力的增强，AI开发者需要更先进的控制措施来防止潜在风险。</li>
<li>通过控制评估来验证控制措施的有效性，其中红队的适应能力与部署的代理的能力特征紧密相关。</li>
<li>提出一个适应红队能力与AI能力发展的系统框架，进行比例控制评估以实现更实用的控制措施。</li>
<li>通过五个虚构模型展示了不同的AI控制级别（ACL），并为每个ACL提供了具体的控制评估规则、控制措施和安全案例示例。</li>
<li>AI控制安全案例的构建对于超级智能LLM代理尤为重要，需要研究突破和可能的替代方法来减轻不对齐风险。</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.05259">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-b6d6980c6ad7342b183ad02287aadc1f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ef1928905d91bc329052becd08ede753.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-802fc741161f0ca7beba913c07c38fa5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-39863f18ca41a6bd26739f2f437fffb5.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Learning-to-Reason-Over-Time-Timeline-Self-Reflection-for-Improved-Temporal-Reasoning-in-Language-Models"><a href="#Learning-to-Reason-Over-Time-Timeline-Self-Reflection-for-Improved-Temporal-Reasoning-in-Language-Models" class="headerlink" title="Learning to Reason Over Time: Timeline Self-Reflection for Improved   Temporal Reasoning in Language Models"></a>Learning to Reason Over Time: Timeline Self-Reflection for Improved   Temporal Reasoning in Language Models</h2><p><strong>Authors:Adrián Bazaga, Rexhina Blloshmi, Bill Byrne, Adrià de Gispert</strong></p>
<p>Large Language Models (LLMs) have emerged as powerful tools for generating coherent text, understanding context, and performing reasoning tasks. However, they struggle with temporal reasoning, which requires processing time-related information such as event sequencing, durations, and inter-temporal relationships. These capabilities are critical for applications including question answering, scheduling, and historical analysis. In this paper, we introduce TISER, a novel framework that enhances the temporal reasoning abilities of LLMs through a multi-stage process that combines timeline construction with iterative self-reflection. Our approach leverages test-time scaling to extend the length of reasoning traces, enabling models to capture complex temporal dependencies more effectively. This strategy not only boosts reasoning accuracy but also improves the traceability of the inference process. Experimental results demonstrate state-of-the-art performance across multiple benchmarks, including out-of-distribution test sets, and reveal that TISER enables smaller open-source models to surpass larger closed-weight models on challenging temporal reasoning tasks. </p>
<blockquote>
<p>大型语言模型（LLM）已经成为生成连贯文本、理解上下文和进行推理任务的强大工具。然而，它们在时间推理方面存在困难，时间推理需要处理与时间相关的信息，如事件序列、持续时间和时间关系。这些能力对于问答、日程安排和历史分析等应用至关重要。在本文中，我们介绍了TISER，这是一个通过多阶段过程增强LLM的时间推理能力的新型框架，该过程结合时间线构建和迭代自我反思。我们的方法利用测试时缩放来延长推理痕迹的长度，使模型能够更有效地捕获复杂的时序依赖关系。这一策略不仅提高了推理准确性，还提高了推理过程的可追溯性。实验结果证明，在多个基准测试上达到了最先进的性能，包括超出分布测试集的性能，并且表明TISER使较小的开源模型能够在具有挑战性的时间推理任务上超越较大的封闭权重模型。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.05258v1">PDF</a> </p>
<p><strong>Summary</strong><br>LLM通过TISER框架提升时序推理能力。TISER结合时间线构建与迭代自我反思的多阶段过程，通过测试时缩放延长推理轨迹，有效捕捉复杂时序依赖关系，不仅提高推理准确性，还改善推理过程的可追溯性。实验结果显示，TISER在多个基准测试上表现卓越，甚至在挑战性时序推理任务中，较小的开源模型也能超越较大的封闭权重模型。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLM面临时序推理挑战，需要处理时间相关信息如事件序列、持续时间和时间关系。</li>
<li>TISER框架旨在增强LLM的时序推理能力，通过多阶段过程结合时间线构建与迭代自我反思。</li>
<li>TISER利用测试时缩放延长推理轨迹，使模型更有效地捕捉复杂时序依赖关系。</li>
<li>TISER不仅提高了推理准确性，而且改善了推理过程的可追溯性。</li>
<li>实验结果显示TISER在多个基准测试上表现卓越。</li>
<li>TISER使得较小的开源模型在挑战性的时序推理任务中也能表现出色，甚至超越较大的封闭权重模型。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.05258">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-6a4610db5cfa1b93993903eaeee03337.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-1d6f3f8c195b241546a73c58d51f2e92.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a187b7435ca923e1c73adbbc9aa89a76.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-1daea595de2bcda898c173f53c8d1f36.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2601bb03ffecc21e904b67cf5ac1b23e.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Unleashing-the-Power-of-LLMs-in-Dense-Retrieval-with-Query-Likelihood-Modeling"><a href="#Unleashing-the-Power-of-LLMs-in-Dense-Retrieval-with-Query-Likelihood-Modeling" class="headerlink" title="Unleashing the Power of LLMs in Dense Retrieval with Query Likelihood   Modeling"></a>Unleashing the Power of LLMs in Dense Retrieval with Query Likelihood   Modeling</h2><p><strong>Authors:Hengran Zhang, Keping Bi, Jiafeng Guo, Xiaojie Sun, Shihao Liu, Daiting Shi, Dawei Yin, Xueqi Cheng</strong></p>
<p>Dense retrieval is a crucial task in Information Retrieval (IR) and is the foundation for downstream tasks such as re-ranking. Recently, large language models (LLMs) have shown compelling semantic understanding capabilities and are appealing to researchers studying dense retrieval. LLMs, as decoder-style generative models, are competent at language generation while falling short on modeling global information due to the lack of attention to tokens afterward. Inspired by the classical word-based language modeling approach for IR, i.e., the query likelihood (QL) model, we seek to sufficiently utilize LLMs’ generative ability by QL maximization. However, instead of ranking documents with QL estimation, we introduce an auxiliary task of QL maximization to yield a better backbone for contrastively learning a discriminative retriever. We name our model as LLM-QL. To condense global document semantics to a single vector during QL modeling, LLM-QL has two major components, Attention Stop (AS) and Input Corruption (IC). AS stops the attention of predictive tokens to previous tokens until the ending token of the document. IC masks a portion of tokens in the input documents during prediction. Experiments on MSMARCO show that LLM-QL can achieve significantly better performance than other LLM-based retrievers and using QL estimated by LLM-QL for ranking outperforms word-based QL by a large margin. </p>
<blockquote>
<p>密集检索是信息检索（IR）中的一项关键任务，也是排序等下游任务的基础。最近，大型语言模型（LLM）显示出令人信服的语义理解能力，吸引研究人员对密集检索进行研究。LLM作为解码器风格的生成模型，擅长语言生成，但由于缺乏对标记的注意力，在建模全局信息方面表现不足。受信息检索中基于单词的语言建模方法（即查询可能性（QL）模型）的启发，我们试图通过QL最大化充分利用LLM的生成能力。然而，我们不是通过QL估计来排名文档，而是引入QL最大化的辅助任务，以产生对比学习鉴别检索器的更好主干。我们将我们的模型命名为LLM-QL。在QL建模过程中，为了精简全局文档语义为单一向量，LLM-QL有两个主要组成部分：注意力停止（AS）和输入腐蚀（IC）。AS阻止预测标记对之前标记的注意力，直到文档结束标记。IC在预测过程中会屏蔽输入文档中的部分标记。在MSMARCO上的实验表明，LLM-QL可以取得比其他LLM检索器更好的性能，并且使用LLM-QL估计的QL进行排序比基于单词的QL有大幅度提升。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.05216v1">PDF</a> 12 pages, 3 figures</p>
<p><strong>摘要</strong><br>LLM的重要任务是密集检索在信息检索（IR）领域密集检索作为下游任务如重新排序的基础大型语言模型（LLM）展现出令人信服的语义理解力。受传统基于词的IR语言建模方法（即查询可能性模型）的启发，试图充分利用LLM的生成能力，并引入了名为LLM-QL的辅助任务来提高其密集检索能力。模型具有注意力停止（AS）和输入损坏（IC）两大要素来实现基于查询可能性（QL）的全局文档语义表示和精简化实验结果显示，LLM-QL在MSMARCO数据集上的性能优于其他基于LLM的检索器，并且使用LLM-QL估计的QL进行排名结果也显著优于基于单词的QL。简言之，借助大型语言模型的帮助来提升基于词的查询可能性模型的密集检索能力并改善排序结果。具有创新性和有效性的新策略和方法被提出。总的来说，本文研究了大型语言模型在密集检索任务中的应用及其改进策略，并在实验中证明了其优越性。特别是所提出的辅助任务方法能够提高基于注意力机制的文档向量表示的语义丰富性并有助于下游任务的效果提升。这种将全局文档语义转化为单个向量的方法在数据密集型应用如搜索引擎和文本处理中有很好的应用前景。这也反映了自然语言处理和人工智能在推进技术进步中的重要性。<strong>关键发现</strong>：</p>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.05216">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-982adbc5929d6f645ea950779f581ffb.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9a257e2184f11fd6b0c690e6175df201.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2081499e8b39caa2053c7a3b27375460.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-1b5d308b0a1038c6c7c249d6af53f085.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8671ad1d3f99674f3b267bdd1c389c42.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="PanoDreamer-Consistent-Text-to-360-Degree-Scene-Generation"><a href="#PanoDreamer-Consistent-Text-to-360-Degree-Scene-Generation" class="headerlink" title="PanoDreamer: Consistent Text to 360-Degree Scene Generation"></a>PanoDreamer: Consistent Text to 360-Degree Scene Generation</h2><p><strong>Authors:Zhexiao Xiong, Zhang Chen, Zhong Li, Yi Xu, Nathan Jacobs</strong></p>
<p>Automatically generating a complete 3D scene from a text description, a reference image, or both has significant applications in fields like virtual reality and gaming. However, current methods often generate low-quality textures and inconsistent 3D structures. This is especially true when extrapolating significantly beyond the field of view of the reference image. To address these challenges, we propose PanoDreamer, a novel framework for consistent, 3D scene generation with flexible text and image control. Our approach employs a large language model and a warp-refine pipeline, first generating an initial set of images and then compositing them into a 360-degree panorama. This panorama is then lifted into 3D to form an initial point cloud. We then use several approaches to generate additional images, from different viewpoints, that are consistent with the initial point cloud and expand&#x2F;refine the initial point cloud. Given the resulting set of images, we utilize 3D Gaussian Splatting to create the final 3D scene, which can then be rendered from different viewpoints. Experiments demonstrate the effectiveness of PanoDreamer in generating high-quality, geometrically consistent 3D scenes. </p>
<blockquote>
<p>从文本描述或参考图像，或两者结合自动生成完整的3D场景，在虚拟现实和游戏等领域具有广泛应用。然而，当前的方法往往生成低质量的纹理和不一致的3D结构。当超出参考图像的视野范围进行推断时，尤其如此。为了应对这些挑战，我们提出了PanoDreamer，这是一种用于一致、灵活的文本和图像控制的3D场景生成的新型框架。我们的方法采用大型语言模型和扭曲细化管道，首先生成一组初始图像，然后将它们合成一个360度的全景图。随后将此全景图提升到三维形成初始点云。然后，我们使用几种方法从不同的视点生成与初始点云一致的额外图像，并扩展或细化初始点云。给定生成的图像集，我们使用三维高斯飞溅技术创建最终的3D场景，然后可以从不同的视点进行渲染。实验表明，PanoDreamer在生成高质量、几何一致的3D场景方面非常有效。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.05152v1">PDF</a> Accepted by CVPR 2025 Workshop on Computer Vision for Metaverse</p>
<p><strong>Summary</strong><br>文本提出了一种名为PanoDreamer的新框架，用于根据文本描述、参考图像或两者进行一致的3D场景生成，具有灵活性和控制力。该框架采用大型语言模型和warp-refine管道，生成初始图像并将其组合成全景图，然后将其提升到三维形成初始点云。然后采用多种方法从初始点云一致地生成不同视角的图像并对其进行扩展&#x2F;细化。最后利用三维高斯技术创建最终的3D场景，可以从不同角度进行渲染。该框架解决了现有方法生成低质量纹理和几何不一致的问题，特别是在参考图像视野之外的问题。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>PanoDreamer是一个新颖的框架，旨在从文本描述、参考图像或两者结合中自动生成一致的3D场景。</li>
<li>该框架利用大型语言模型进行场景构建，并使用warp-refine管道生成初始图像并将其组合成全景图。</li>
<li>生成的初始全景图被转换为三维点云，为后续的场景细化提供了基础。</li>
<li>通过多种方法从初始点云生成一致的不同视角的图像，并对其进行扩展和细化。</li>
<li>利用三维高斯技术创建最终的3D场景，具有良好的几何一致性。</li>
<li>该框架解决了现有方法中的低质量纹理和几何不一致性问题，特别是在参考图像视野之外的问题。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.05152">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-96820a96652e1b4e81f4a21511556126.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-edb66d36146d23caf2e0bb7d1f0c8570.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-30894dfa7d6b9a7a98332d9c642b46f4.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Revealing-the-Intrinsic-Ethical-Vulnerability-of-Aligned-Large-Language-Models"><a href="#Revealing-the-Intrinsic-Ethical-Vulnerability-of-Aligned-Large-Language-Models" class="headerlink" title="Revealing the Intrinsic Ethical Vulnerability of Aligned Large Language   Models"></a>Revealing the Intrinsic Ethical Vulnerability of Aligned Large Language   Models</h2><p><strong>Authors:Jiawei Lian, Jianhong Pan, Lefan Wang, Yi Wang, Shaohui Mei, Lap-Pui Chau</strong></p>
<p>Large language models (LLMs) are foundational explorations to artificial general intelligence, yet their alignment with human values via instruction tuning and preference learning achieves only superficial compliance. Here, we demonstrate that harmful knowledge embedded during pretraining persists as indelible “dark patterns” in LLMs’ parametric memory, evading alignment safeguards and resurfacing under adversarial inducement at distributional shifts. In this study, we first theoretically analyze the intrinsic ethical vulnerability of aligned LLMs by proving that current alignment methods yield only local “safety regions” in the knowledge manifold. In contrast, pretrained knowledge remains globally connected to harmful concepts via high-likelihood adversarial trajectories. Building on this theoretical insight, we empirically validate our findings by employing semantic coherence inducement under distributional shifts–a method that systematically bypasses alignment constraints through optimized adversarial prompts. This combined theoretical and empirical approach achieves a 100% attack success rate across 19 out of 23 state-of-the-art aligned LLMs, including DeepSeek-R1 and LLaMA-3, revealing their universal vulnerabilities. </p>
<blockquote>
<p>大型语言模型（LLM）是人工智能通用研究的基石探索，然而，通过指令微调偏好学习实现其与人类价值的对齐仅达到表面上的合规性。在这里，我们证明预训练期间嵌入的有害知识会作为不可磨灭的“暗模式”持续存在于LLM的参数记忆中，逃避对齐保障措施，并在分布转移时对抗诱导下再次出现。在本研究中，我们首先从理论上分析对齐LLM的内在道德脆弱性，通过证明现有的对齐方法仅在知识流形中产生局部“安全区域”。相反，预训练知识仍与有害概念通过高概率对抗轨迹全局连接。基于这一理论见解，我们通过分布转移下的语义连贯诱导实证验证我们的发现——一种通过优化对抗提示来系统地绕过对齐约束的方法。这种结合理论和实证的方法在23个最新对齐LLM中的19个上实现了100%的攻击成功率，包括DeepSeek-R1和LLaMA-3，揭示了它们的普遍脆弱性。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.05050v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>大型语言模型（LLM）是人工智能通用探索的基础，但通过指令调整和偏好学习实现与人类价值的对齐仅达到表面上的符合。研究表明，预训练过程中嵌入的有害知识会作为不可磨灭的“暗模式”存在于LLM的参数记忆中，逃避对齐保障措施，并在分布转移时通过对抗性诱导重新浮现。当前的理论分析显示，对齐的LLM存在固有的道德脆弱性，因为当前的对齐方法仅在知识流形中产生局部“安全区域”，而预训练知识仍与有害概念通过高概率对抗轨迹保持全球联系。本研究通过分布转移下的语义连贯性诱导进行实证验证，该方法通过优化对抗性提示来系统地绕过对齐约束。这种结合理论和实证的方法在23种最先进对齐的LLM中有19种达到了百分百的攻击成功率，揭示了其普遍存在的漏洞。暗含隐藏危机的大型语言模型潜伏对人类权益可能产生影响及安全问题风险不容小觑。这不仅为伦理与科技的冲突再添案例也让我们面临巨大挑战需要探讨相应措施有效解决避免严重事件如深学决策科技介入误用等影响全球价值理念和决策的重要命题再次引发了新的挑战与思考方向。通过对暗模式机制与防御策略的深入研究或能有望破解此类问题寻求伦理与技术的和谐共存。在人工智能技术发展的同时构建其安全和可持续性是未来研究的重要方向之一。本研究的发现将推动人工智能领域的发展进一步朝着更加安全和可靠的方向发展。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLMs存在固有的道德脆弱性，即使经过指令调整和偏好学习与人类价值对齐，仍无法完全避免有害知识的存在。</li>
<li>有害知识在LLM的参数记忆中以“暗模式”形式存在，逃避对齐保障措施，并在特定情境下重新浮现。</li>
<li>当前的对齐方法仅在知识流形中产生局部“安全区域”，预训练知识与有害概念之间保持全球联系。</li>
<li>通过分布转移下的语义连贯性诱导方法，实证验证了LLMs的普遍漏洞，攻击成功率达百分之百。</li>
<li>LLMs对人类权益和安全的影响不可小觑，需要深入探讨相应措施以应对潜在风险。</li>
<li>研究暗模式机制与防御策略有望破解伦理与技术的冲突，寻求二者的和谐共存。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.05050">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-24ab0dc7ddfb580cf39f70de0edefedf.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-dae67f5f70d37aa2f842ae1248c33c12.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="SAFT-Structure-aware-Transformers-for-Textual-Interaction-Classification"><a href="#SAFT-Structure-aware-Transformers-for-Textual-Interaction-Classification" class="headerlink" title="SAFT: Structure-aware Transformers for Textual Interaction   Classification"></a>SAFT: Structure-aware Transformers for Textual Interaction   Classification</h2><p><strong>Authors:Hongtao Wang, Renchi Yang, Hewen Wang, Haoran Zheng, Jianliang Xu</strong></p>
<p>Textual interaction networks (TINs) are an omnipresent data structure used to model the interplay between users and items on e-commerce websites, social networks, etc., where each interaction is associated with a text description. Classifying such textual interactions (TIC) finds extensive use in detecting spam reviews in e-commerce, fraudulent transactions in finance, and so on. Existing TIC solutions either (i) fail to capture the rich text semantics due to the use of context-free text embeddings, and&#x2F;or (ii) disregard the bipartite structure and node heterogeneity of TINs, leading to compromised TIC performance. In this work, we propose SAFT, a new architecture that integrates language- and graph-based modules for the effective fusion of textual and structural semantics in the representation learning of interactions. In particular, line graph attention (LGA)&#x2F;gated attention units (GAUs) and pretrained language models (PLMs) are capitalized on to model the interaction-level and token-level signals, which are further coupled via the proxy token in an iterative and contextualized fashion. Additionally, an efficient and theoretically-grounded approach is developed to encode the local and global topology information pertaining to interactions into structural embeddings. The resulting embeddings not only inject the structural features underlying TINs into the textual interaction encoding but also facilitate the design of graph sampling strategies. Extensive empirical evaluations on multiple real TIN datasets demonstrate the superiority of SAFT over the state-of-the-art baselines in TIC accuracy. </p>
<blockquote>
<p>文本交互网络（TINs）是一种无处不在的数据结构，用于模拟电子商务网站、社交网络等平台上用户和项目之间的交互，其中每个交互都与文本描述相关联。对这类文本交互的分类（TIC）在电子商务中检测垃圾评论、金融中的欺诈交易等方面有着广泛应用。现有的TIC解决方案（i）由于使用脱离上下文的文本嵌入，无法捕获丰富的文本语义；（ii）忽略了TINs的双曲结构和节点异质性，导致TIC性能下降。在本研究中，我们提出了SAFT这一新架构，它通过整合语言和图形模块，实现文本和结构语义的有效融合在交互的表示学习中。具体来说，我们利用线图注意力（LGA）&#x2F;门控注意力单元（GAUs）和预训练语言模型（PLMs）来模拟交互级别和令牌级别的信号，并进一步通过代理令牌以迭代和语境化的方式进行耦合。此外，我们还开发了一种高效且基于理论的方法，将交互的局部和全局拓扑信息编码为结构嵌入。所得的嵌入不仅将TINs的结构特征注入文本交互编码中，而且有助于设计图形采样策略。在多个真实TIN数据集上的广泛实证评估表明，SAFT在TIC准确性方面优于最新基线。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.04861v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文提出一种名为SAFT的新架构，旨在集成语言和图形模块，有效融合文本和结构性语义进行交互表示学习。该架构利用线图注意力（LGA）&#x2F;门控注意力单元（GAUs）和预训练语言模型（PLMs）来建模交互级和令牌级的信号，通过代理令牌以迭代和上下文化的方式进一步结合。此外，还开发了一种高效且理论扎实的方法，将局部和全局拓扑信息编码到结构性嵌入中。SAFT在多个真实TIN数据集上的经验评估表明，其在TIC准确性方面优于现有技术基线。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>文本交互网络（TINs）是普遍存在的数据结构，用于模拟用户在电商网站、社交网络等上的用户与项目之间的交互。</li>
<li>文本交互分类（TIC）在检测电商中的垃圾评论、金融中的欺诈交易等方面有广泛应用。</li>
<li>现有TIC解决方案因使用无上下文文本嵌入而未能捕捉丰富的文本语义，或者忽略了TINs的双向结构和节点异质性，导致TIC性能受损。</li>
<li>SAFT架构集成了语言和图形模块，有效融合文本和结构性语义进行交互表示学习。</li>
<li>SAFT利用线图注意力（LGA）&#x2F;门控注意力单元（GAUs）和预训练语言模型（PLMs）进行建模，并在迭代和上下文化的方式中进一步结合交互级和令牌级的信号。</li>
<li>SAFT开发了一种编码局部和全局拓扑信息的方法，将其嵌入到结构性嵌入中。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.04861">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-6c1c64ab758c75798ac086b2aadf8423.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-be3d2738058abe510d43853ce107a1de.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-9de3c7fb9daeb9f8716e34622e8dbd7c.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="Can-LLMs-Interpret-and-Leverage-Structured-Linguistic-Representations-A-Case-Study-with-AMRs"><a href="#Can-LLMs-Interpret-and-Leverage-Structured-Linguistic-Representations-A-Case-Study-with-AMRs" class="headerlink" title="Can LLMs Interpret and Leverage Structured Linguistic Representations? A   Case Study with AMRs"></a>Can LLMs Interpret and Leverage Structured Linguistic Representations? A   Case Study with AMRs</h2><p><strong>Authors:Ankush Raut, Xiaofeng Zhu, Maria Leonor Pacheco</strong></p>
<p>This paper evaluates the ability of Large Language Models (LLMs) to leverage contextual information in the form of structured linguistic representations. Specifically, we examine the impact of encoding both short and long contexts using Abstract Meaning Representation (AMR) structures across a diverse set of language tasks. We perform our analysis using 8-bit quantized and instruction-tuned versions of Llama 3.1 (8B), Phi-3, and Mistral 7B. Our results indicate that, for tasks involving short contexts, augmenting the prompt with the AMR of the original language context often degrades the performance of the underlying LLM. However, for tasks that involve long contexts, such as dialogue summarization in the SAMSum dataset, this enhancement improves LLM performance, for example, by increasing the zero-shot cosine similarity score of Llama 3.1 from 66.2% to 76%. This improvement is more evident in the newer and larger LLMs, but does not extend to the older or smaller ones. In addition, we observe that LLMs can effectively reconstruct the original text from a linearized AMR, achieving a cosine similarity of 81.3% in the best-case scenario. </p>
<blockquote>
<p>本文评估大型语言模型（LLM）利用结构化语言表示形式中的上下文信息的能力。具体来说，我们研究了使用抽象意义表示（AMR）结构对短长和不同语境进行编码对一系列语言任务的影响。我们使用8位量化和指令调整的Llama 3.1（8B）、Phi-3和Mistral 7B版本进行分析。结果表明，对于涉及短语境的任务，通过提示增强原始语言上下文的AMR往往会降低基础LLM的性能。然而，对于涉及长语境的任务，如SAMSum数据集中的对话摘要，这种增强会提高LLM的性能，例如将Llama 3. 1的零镜cosine相似度得分从66.2%提高到76%。这一改进在更新的和更大的LLM中更为显著，但并不适用于旧版或较小的LLM。此外，我们观察到LLM可以有效地从线性化的AMR重建原始文本，在最佳情况下达到81.3%的cosine相似度。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.04745v1">PDF</a> 13 pages, 23 figures. Submitted to XLLM @ ACL 2025</p>
<p><strong>Summary</strong></p>
<p>大型语言模型（LLM）利用抽象意义表示（AMR）结构形式的上下文信息能力的研究。发现对于短任务，加入AMR提示会降低LLM性能；而对于长任务如对话摘要等，该增强有助于提高LLM性能。新大型LLM的改进更为明显。此外，LLM可以有效地从线性化的AMR重建原始文本。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>大型语言模型（LLM）能够利用抽象意义表示（AMR）结构的上下文信息。</li>
<li>对于短任务，加入AMR提示会降低LLM性能。</li>
<li>对于长任务如对话摘要等，增强AMR可以提高LLM性能。</li>
<li>新颖和更大的LLM在改进方面表现更显著。</li>
<li>LLM能够从线性化的AMR有效地重建原始文本。</li>
<li>使用8位量化的LLM在任务处理中表现优秀。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.04745">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-b452bdfeba209aac8fc7a551bda0a6e1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9729b90395b51b955ddefc6a9cda9f44.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-8e910886f66f98fb042a4a850007dfe3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9b038b015a028b0bad607c72cf974316.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-8d02fe7f9b9fc0f0658ff90f883f8dbc.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3f5b6cded3565793616136973d4ead20.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="StyleRec-A-Benchmark-Dataset-for-Prompt-Recovery-in-Writing-Style-Transformation"><a href="#StyleRec-A-Benchmark-Dataset-for-Prompt-Recovery-in-Writing-Style-Transformation" class="headerlink" title="StyleRec: A Benchmark Dataset for Prompt Recovery in Writing Style   Transformation"></a>StyleRec: A Benchmark Dataset for Prompt Recovery in Writing Style   Transformation</h2><p><strong>Authors:Shenyang Liu, Yang Gao, Shaoyan Zhai, Liqiang Wang</strong></p>
<p>Prompt Recovery, reconstructing prompts from the outputs of large language models (LLMs), has grown in importance as LLMs become ubiquitous. Most users access LLMs through APIs without internal model weights, relying only on outputs and logits, which complicates recovery. This paper explores a unique prompt recovery task focused on reconstructing prompts for style transfer and rephrasing, rather than typical question-answering. We introduce a dataset created with LLM assistance, ensuring quality through multiple techniques, and test methods like zero-shot, few-shot, jailbreak, chain-of-thought, fine-tuning, and a novel canonical-prompt fallback for poor-performing cases. Our results show that one-shot and fine-tuning yield the best outcomes but highlight flaws in traditional sentence similarity metrics for evaluating prompt recovery. Contributions include (1) a benchmark dataset, (2) comprehensive experiments on prompt recovery strategies, and (3) identification of limitations in current evaluation metrics, all of which advance general prompt recovery research, where the structure of the input prompt is unrestricted. </p>
<blockquote>
<p>提示恢复（prompt recovery）在大型语言模型（LLM）无处不在的时代变得越来越重要。大多数用户通过API访问LLM，而不涉及内部模型权重，仅依赖输出和logits，这使得恢复工作变得复杂。本文探讨了一个独特的提示恢复任务，重点是通过风格迁移和重新表述来重建提示，而非典型的问题解答。我们利用LLM的帮助创建了一个数据集，通过多种技术确保数据质量，并测试了零样本、少样本、断句恢复法、思考链式法、微调以及针对表现不佳情况的全新规范提示回退等方法。我们的结果表明，单次训练（one-shot）和微调（fine-tuning）效果最佳，但指出了传统句子相似度评估指标在评估提示恢复上的缺陷。本研究的贡献包括：（1）基准数据集；（2）关于提示恢复策略的全面实验；（3）对现行评估指标局限性的识别，所有这些都有助于推动一般性的提示恢复研究，其中输入提示的结构不受限制。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.04373v1">PDF</a> 2024 IEEE International Conference on Big Data (BigData)</p>
<p><strong>Summary</strong></p>
<p>大型语言模型（LLM）的普及使得从LLM输出中恢复重建提示变得至关重要。本文通过引入一个新的数据集和方法对各种策略进行了全面实验探索，针对风格转换和重述进行特定的提示恢复任务，提出了新的评价方法以改善评估质量不佳的问题。研究发现单样例策略和微调取得最佳效果，同时也发现了传统的句子相似性度量方法在评价提示恢复上的局限性。本研究的贡献在于推进了无约束结构的一般提示恢复研究，并为相关研究提供了数据集和方法上的探索方向。我们的方法和结论对未来相关的研究和实际应用都具有参考价值。<br><strong>Key Takeaways</strong>： </p>
<ul>
<li>大型语言模型（LLM）的普及促使了从LLM输出中恢复重建提示的研究需求。 </li>
<li>本文聚焦于风格转换和重述的特定提示恢复任务。 </li>
<li>研究通过引入新的数据集进行试验，并利用多种方法提升数据质量。 </li>
<li>提示恢复策略的研究发现单样例策略和微调效果最好。 </li>
<li>研究揭示了传统句子相似性度量在评估提示恢复时的局限性。 </li>
<li>本研究贡献在于提供了数据集和实验方法的全面探索，为未来相关研究提供了方向。</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.04373">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-92a4e90520b8300778019cfc5b99a1d2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-42de292c54cb7690f4060c33fa40e20e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5f3a3209707dfa26a9f44a638374406d.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-c37a7c3d42a1a33f8c8013ca80da1c40.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-eef4afa161299bf1091314d627d61cd5.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-1b45b57645b7d18c5b5fcfd91210e2b3.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="REFORMER-A-ChatGPT-Driven-Data-Synthesis-Framework-Elevating-Text-to-SQL-Models"><a href="#REFORMER-A-ChatGPT-Driven-Data-Synthesis-Framework-Elevating-Text-to-SQL-Models" class="headerlink" title="REFORMER: A ChatGPT-Driven Data Synthesis Framework Elevating   Text-to-SQL Models"></a>REFORMER: A ChatGPT-Driven Data Synthesis Framework Elevating   Text-to-SQL Models</h2><p><strong>Authors:Shenyang Liu, Saleh Almohaimeed, Liqiang Wang</strong></p>
<p>The existing Text-to-SQL models suffer from a shortage of training data, inhibiting their ability to fully facilitate the applications of SQL queries in new domains. To address this challenge, various data synthesis techniques have been employed to generate more diverse and higher quality data. In this paper, we propose REFORMER, a framework that leverages ChatGPT’s prowess without the need for additional training, to facilitate the synthesis of (question, SQL query) pairs tailored to new domains. Our data augmentation approach is based on a “retrieve-and-edit” method, where we generate new questions by filling masked question using explanation of SQL queries with the help of ChatGPT. Furthermore, we demonstrate that cycle consistency remains a valuable method of validation when applied appropriately. Our experimental results show that REFORMER consistently outperforms previous data augmentation methods. To further investigate the power of ChatGPT and create a general data augmentation method, we also generate the new data by paraphrasing the question in the dataset and by paraphrasing the description of a new SQL query that is generated by ChatGPT as well. Our results affirm that paraphrasing questions generated by ChatGPT help augment the original data. </p>
<blockquote>
<p>当前存在的文本到SQL模型面临着训练数据不足的问题，这限制了它们在新领域中使用SQL查询的能力。为了解决这一挑战，已经采用了各种数据合成技术来生成更多样化、更高质量的数据。在本文中，我们提出了REFORMER框架，该框架利用ChatGPT的能力，无需额外的训练，促进了针对新领域的（问题，SQL查询）对的合成。我们的数据增强方法基于“检索和编辑”的方法，我们通过ChatGPT的帮助，通过填充带问题的SQL查询解释来生成新的问题。此外，我们证明了在适当应用时，循环一致性仍然是一种有价值的验证方法。我们的实验结果表明，REFORMER始终优于以前的数据增强方法。为了进一步探索ChatGPT的威力并创建一种通用的数据增强方法，我们还通过重新表述数据集中的问题和重新表述ChatGPT生成的新SQL查询的描述来生成新的数据。我们的结果证实，由ChatGPT生成的重新表述问题有助于增强原始数据。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.04363v1">PDF</a> 2024 International Conference on Machine Learning and Applications   (ICMLA)</p>
<p><strong>Summary</strong></p>
<p>本论文针对现有Text-to-SQL模型在新领域应用时面临训练数据不足的问题，提出了REFORMER框架。该框架利用ChatGPT的强大功能，无需额外训练，即可合成针对新领域的（问题，SQL查询）对。REFORMER基于“检索和编辑”的方法进行数据增强，通过填充带有SQL查询解释的问题中的掩码来生成新问题，借助ChatGPT实现。实验结果表明，REFORMER持续优于以前的数据增强方法。此外，通过由ChatGPT生成的问题的复述和SQL查询描述，进一步验证了ChatGPT的力量和一般数据增强方法的可行性。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Text-to-SQL模型在新领域应用时面临训练数据不足的问题。</li>
<li>REFORMER框架被提出来解决这一问题，它利用ChatGPT来合成针对新领域的问题和SQL查询对。</li>
<li>REFORMER基于“检索和编辑”的方法生成新问题，通过填充带有SQL查询解释的问题中的掩码来实现。</li>
<li>REFORMER在实验中表现优越，优于之前的数据增强方法。</li>
<li>通过复述由ChatGPT生成的问题和SQL查询描述进行数据增强是可行的，并得到了实验结果的验证。</li>
<li>ChatGPT的应用在数据增强方法中发挥了重要作用。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.04363">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-2fe0edbca55ecb4c88aaf0af95fa6a39.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d86269bf3f2c9e6f394a74c06ab962fb.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-496a7f4d36ed24ab44b047a25a2fd548.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c2564aff485e07100ee766d97a32cc56.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-cd5af50a93725b9d81cda2684534b0b8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c3c30a07f121ed453cd90d213c347545.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="The-Effects-of-Grouped-Structural-Global-Pruning-of-Vision-Transformers-on-Domain-Generalisation"><a href="#The-Effects-of-Grouped-Structural-Global-Pruning-of-Vision-Transformers-on-Domain-Generalisation" class="headerlink" title="The Effects of Grouped Structural Global Pruning of Vision Transformers   on Domain Generalisation"></a>The Effects of Grouped Structural Global Pruning of Vision Transformers   on Domain Generalisation</h2><p><strong>Authors:Hamza Riaz, Alan F. Smeaton</strong></p>
<p>With the growing sizes of AI models like large language models (LLMs) and vision transformers, deploying them on devices with limited computational resources is a significant challenge particularly when addressing domain generalisation (DG) tasks. This paper introduces a novel grouped structural pruning method for pre-trained vision transformers (ViT, BeiT, and DeiT), evaluated on the PACS and Office-Home DG benchmarks. Our method uses dependency graph analysis to identify and remove redundant groups of neurons, weights, filters, or attention heads within transformers, using a range of selection metrics. Grouped structural pruning is applied at pruning ratios of 50%, 75% and 95% and the models are then fine-tuned on selected distributions from DG benchmarks to evaluate their overall performance in DG tasks. Results show significant improvements in inference speed and fine-tuning time with minimal trade-offs in accuracy and DG task performance. For instance, on the PACS benchmark, pruning ViT, BeiT, and DeiT models by 50% using the Hessian metric resulted in accuracy drops of only -2.94%, -1.42%, and -1.72%, respectively, while achieving speed boosts of 2.5x, 1.81x, and 2.15x. These findings demonstrate the effectiveness of our approach in balancing model efficiency with domain generalisation performance. </p>
<blockquote>
<p>随着大型语言模型（LLM）和视觉转换器（vision transformers）等人工智能模型的规模不断增长，在有限的计算资源设备上部署它们是一个巨大的挑战，特别是在处理领域泛化（DG）任务时。本文介绍了一种针对预训练视觉转换器（ViT、BeiT和DeiT）的新型分组结构剪枝方法，并在PACS和Office-Home DG基准上进行了评估。我们的方法使用依赖图分析来识别和删除转换器内的冗余神经元组、权重、过滤器或注意力头，采用一系列选择指标。分组结构剪枝在50%、75%和95%的剪枝率下应用，然后对模型进行微调以适应DG基准中选择的分布，以评估它们在DG任务中的整体性能。结果显示，推理速度和微调时间的显著提高，在准确度和DG任务性能方面只有最小的权衡。例如，在PACS基准测试中，使用Hessian度量将ViT、BeiT和DeiT模型剪枝50%，准确率仅下降-2.94%、-1.42%和-1.72%，同时速度提高了2.5倍、1.81倍和2.15倍。这些发现表明我们的方法在平衡模型效率和领域泛化性能方面的有效性。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.04196v1">PDF</a> 9 pages</p>
<p><strong>Summary</strong><br>    本文提出一种针对预训练视觉转换器（包括ViT、BeiT和DeiT）的新型分组结构剪枝方法，用于处理领域泛化（DG）任务。该方法使用依赖图分析来识别并移除转换器中的冗余神经元组、权重、过滤器或注意力头，以提高模型效率和部署在资源受限设备上的能力。在PACS和Office-Home DG基准测试中评估了该方法，结果显示，在剪枝比例高达50%、75%和95%的情况下，推理速度和微调时间的提升显著，而精度和DG任务性能的损失极小。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>介绍了针对预训练视觉转换器的分组结构剪枝方法，以提高模型效率和在资源受限设备上的部署能力。</li>
<li>方法基于依赖图分析，用于识别并移除冗余的神经元组、权重、过滤器或注意力头。</li>
<li>在PACS和Office-Home DG基准测试中评估了该方法的有效性。</li>
<li>剪枝比例高达50%、75%和95%时，推理速度和微调时间显著提升。</li>
<li>精度和DG任务性能的损失极小，例如在PACS基准测试中，剪枝ViT、BeiT和DeiT模型50%后，精度仅下降2.94%、1.42%和1.72%。</li>
<li>这种方法实现了模型效率和领域泛化性能之间的平衡。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.04196">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-dbfd9a704f33bb2f2e3d50fe9573ebbb.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c2af29ed6fb5680d58ea19df48024ef1.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-b83e7a4574cf0fd159936655a9116fde.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c32df19f11dbacf8f39067e6d1a18490.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-7083ee93be02a3415f2783fdb3249c63.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="Filter-Images-First-Generate-Instructions-Later-Pre-Instruction-Data-Selection-for-Visual-Instruction-Tuning"><a href="#Filter-Images-First-Generate-Instructions-Later-Pre-Instruction-Data-Selection-for-Visual-Instruction-Tuning" class="headerlink" title="Filter Images First, Generate Instructions Later: Pre-Instruction Data   Selection for Visual Instruction Tuning"></a>Filter Images First, Generate Instructions Later: Pre-Instruction Data   Selection for Visual Instruction Tuning</h2><p><strong>Authors:Bardia Safaei, Faizan Siddiqui, Jiacong Xu, Vishal M. Patel, Shao-Yuan Lo</strong></p>
<p>Visual instruction tuning (VIT) for large vision-language models (LVLMs) requires training on expansive datasets of image-instruction pairs, which can be costly. Recent efforts in VIT data selection aim to select a small subset of high-quality image-instruction pairs, reducing VIT runtime while maintaining performance comparable to full-scale training. However, a major challenge often overlooked is that generating instructions from unlabeled images for VIT is highly expensive. Most existing VIT datasets rely heavily on human annotations or paid services like the GPT API, which limits users with constrained resources from creating VIT datasets for custom applications. To address this, we introduce Pre-Instruction Data Selection (PreSel), a more practical data selection paradigm that directly selects the most beneficial unlabeled images and generates instructions only for the selected images. PreSel first estimates the relative importance of each vision task within VIT datasets to derive task-wise sampling budgets. It then clusters image features within each task, selecting the most representative images with the budget. This approach reduces computational overhead for both instruction generation during VIT data formation and LVLM fine-tuning. By generating instructions for only 15% of the images, PreSel achieves performance comparable to full-data VIT on the LLaVA-1.5 and Vision-Flan datasets. The link to our project page: <a target="_blank" rel="noopener" href="https://bardisafa.github.io/PreSel">https://bardisafa.github.io/PreSel</a> </p>
<blockquote>
<p>视觉指令调整（VIT）对于大型视觉语言模型（LVLM）需要在大量的图像指令对数据集上进行训练，这可能会很昂贵。最近的VIT数据选择工作旨在选择一小部分高质量的图像指令对，在减少VIT运行时间的同时保持与全规模训练相当的性能。然而，经常被忽视的一个主要挑战是，从无标签的图像中生成指令的成本非常高。大多数现有的VIT数据集严重依赖于人工注释或付费服务，如GPT API，这限制了资源有限的用户创建用于自定义应用程序的VIT数据集。为了解决这一问题，我们引入了更实用的数据选择范式——Pre-Instruction Data Selection（PreSel），它直接选择最有益的无标签图像，只为所选图像生成指令。PreSel首先估计VIT数据集中每个视觉任务的相对重要性，以得出任务级采样预算。然后，它在每个任务内对图像特征进行聚类，选择最具代表性的图像以符合预算。这种方法减少了VIT数据形成和LVLM微调过程中的指令生成计算开销。只为15%的图像生成指令，PreSel在LLaVA-1.5和Vision-Flan数据集上的性能与全数据VIT相当。我们的项目页面链接：<a target="_blank" rel="noopener" href="https://bardisafa.github.io/PreSel">https://bardisafa.github.io/PreSel</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.07591v2">PDF</a> Accepted at CVPR 2025 (Highlight)</p>
<p><strong>Summary</strong></p>
<p>本文介绍了针对大型视觉语言模型（LVLMs）的视觉指令微调（VIT）需要在大规模的图像指令对数据集上进行训练，这导致了高昂的成本。近期有研究尝试选择高质量图像指令对子集以降低成本并提高运行效率，但生成指令需要大量人力标注或付费服务，限制了资源受限用户的自定义应用。为解决这一问题，本文提出了预指令数据选择（PreSel）方法，该方法直接选择最有益的无标签图像并为所选图像生成指令。PreSel通过估计每个视觉任务的重要性来推导任务级采样预算，并在每个任务内对图像特征进行聚类，选择最具代表性的图像。该方法降低了在构建VIT数据集和微调LVLM时的计算开销。仅对15%的图像生成指令即可实现与全数据VIT相当的性能。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>视觉指令微调（VIT）需要大量图像指令对数据集，成本高昂。</li>
<li>现有数据选择方法依赖人力标注或付费服务，限制了资源受限用户的自定义应用。</li>
<li>预指令数据选择（PreSel）方法直接选择无标签图像并为所选图像生成指令，降低成本。</li>
<li>PreSel通过估计每个视觉任务的重要性来推导任务级采样预算。</li>
<li>PreSel在图像特征聚类中选择最具代表性的图像，降低计算开销。</li>
<li>仅对部分图像生成指令即可实现与全数据VIT相当的性能。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.07591">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-c9377d2c0291d7db3c1e387fc718d8cd.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-93ec5db65b826b27d696e3a99ffda4de.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e265abccdd703087bfb59e5a7564ce27.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f68ee10db0da8de33097a4f4f36d45f7.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="Fine-Tuning-Transformer-Based-Vision-Language-Models-for-Robust-Object-Detection-in-Unstructured-Environments"><a href="#Fine-Tuning-Transformer-Based-Vision-Language-Models-for-Robust-Object-Detection-in-Unstructured-Environments" class="headerlink" title="Fine-Tuning Transformer-Based Vision-Language Models for Robust Object   Detection in Unstructured Environments"></a>Fine-Tuning Transformer-Based Vision-Language Models for Robust Object   Detection in Unstructured Environments</h2><p><strong>Authors:Aysegul Ucar, Soumyadeep Ro, Sanapala Satwika, Pamarthi Yasoda Gayathri, Mohmmad Ghaith Balsha</strong></p>
<p>Vision-Language Models (VLMs) have emerged as powerful tools in artificial intelli-gence, capable of integrating textual and visual data for a unified understanding of complex scenes. While models such as Florence2, built on transformer architectures, have shown promise across general tasks, their performance in object detection within unstructured or cluttered environments remains underexplored. In this study, we fi-ne-tuned the Florence2 model for object detection tasks in non-constructed, complex environments. A comprehensive experimental framework was established involving multiple hardware configurations (NVIDIA T4, L4, and A100 GPUs), optimizers (AdamW, SGD), and varied hyperparameters including learning rates and LoRA (Low-Rank Adaptation) setups. Model training and evaluation were conducted on challenging datasets representative of real-world, disordered settings. The optimized Florence2 models exhibited significant improvements in object detection accuracy, with Mean Average Precision (mAP) metrics approaching or matching those of estab-lished models such as YOLOv8, YOLOv9, and YOLOv10. The integration of LoRA and careful fine-tuning of transformer layers contributed notably to these gains. Our find-ings highlight the adaptability of transformer-based VLMs like Florence2 for do-main-specific tasks, particularly in visually complex environments. The study under-scores the potential of fine-tuned VLMs to rival traditional convolution-based detec-tors, offering a flexible and scalable approach for advanced vision applications in re-al-world, unstructured settings. </p>
<blockquote>
<p>视觉语言模型（VLMs）作为人工智能的强大工具，能够整合文本和视觉数据，对复杂场景进行统一理解。虽然基于transformer架构的Florence2等模型在一般任务上表现出潜力，但在非结构化或杂乱环境中进行目标检测的性能仍未得到充分探索。本研究中，我们对Florence2模型进行了微调，以应对非构建环境中的目标检测任务。我们建立了一个全面的实验框架，涉及多种硬件配置（NVIDIA T4、L4和A100 GPU）、优化器（AdamW、SGD）以及包括学习率和LoRA（低秩适应）在内的各种超参数设置。模型训练与评估是在具有代表性的现实世界杂乱设置挑战数据集上进行的。优化后的Florence2模型在目标检测精度上取得了显著改进，平均精度（mAP）指标接近或达到了YOLOv8、YOLOv9和YOLOv10等现有模型的性能。LoRA的集成和对transformer层的仔细微调对这些收益产生了显著影响。我们的研究结果强调了基于transformer的VLMs（如Florence2）对特定领域的适应性，特别是在视觉复杂的环境中。该研究突显了经过精细调整的VLMs与传统基于卷积的检测器竞争潜力，为现实世界非结构化环境中的高级视觉应用提供了灵活且可扩展的方法。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.04918v3">PDF</a> 22 pages, 13 Figures, 6 Tables</p>
<p><strong>Summary</strong></p>
<p>本研究的重点是对基于转换器架构的Florence2模型进行优化，以适应非结构化或杂乱环境中物体检测的任务。通过建立一个全面的实验框架，涉及多种硬件配置、优化器、超参数等，模型在代表真实世界杂乱设置的挑战性数据集上进行训练和评估。优化后的Florence2模型在物体检测准确性上有显著提高，平均精度（mAP）指标接近或匹配YOLOv8、YOLOv9和YOLOv10等已建立模型。本研究突显了基于转换器的VLMs（如Florence2）对特定领域的适应性，特别是在视觉复杂环境中。此外，它还指出了微调后的VLMs与传统基于卷积的检测器竞争的潜力，为真实世界非结构化环境中的高级视觉应用提供了灵活且可扩展的方法。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>基于转换器架构的Florence2模型被优化以适应非结构化环境中的物体检测任务。</li>
<li>通过全面的实验框架，涉及多种硬件配置、优化器、超参数等来进行模型训练和评估。</li>
<li>优化后的Florence2模型在物体检测准确性上显著提高，mAP指标接近或匹配其他主流模型。</li>
<li>LoRA技术的集成和转换器层的精细调整对提升性能有显著贡献。</li>
<li>研究表明，基于转换器的VLMs（如Florence2）在特定领域任务中具有良好的适应性，特别是在视觉复杂的环境中。</li>
<li>相比传统基于卷积的检测器，微调后的VLMs具有竞争力。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.04918">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-98db4665358a36f424d9bc07ef7a2277.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2cad3459aa076ed1ee0673f0a1018e24.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b1905fcaa64f23f604c5c098173f59be.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="M2-omni-Advancing-Omni-MLLM-for-Comprehensive-Modality-Support-with-Competitive-Performance"><a href="#M2-omni-Advancing-Omni-MLLM-for-Comprehensive-Modality-Support-with-Competitive-Performance" class="headerlink" title="M2-omni: Advancing Omni-MLLM for Comprehensive Modality Support with   Competitive Performance"></a>M2-omni: Advancing Omni-MLLM for Comprehensive Modality Support with   Competitive Performance</h2><p><strong>Authors:Qingpei Guo, Kaiyou Song, Zipeng Feng, Ziping Ma, Qinglong Zhang, Sirui Gao, Xuzheng Yu, Yunxiao Sun, Tai-Wei Chang, Jingdong Chen, Ming Yang, Jun Zhou</strong></p>
<p>We present M2-omni, a cutting-edge, open-source omni-MLLM that achieves competitive performance to GPT-4o. M2-omni employs a unified multimodal sequence modeling framework, which empowers Large Language Models(LLMs) to acquire comprehensive cross-modal understanding and generation capabilities. Specifically, M2-omni can process arbitrary combinations of audio, video, image, and text modalities as input, generating multimodal sequences interleaving with audio, image, or text outputs, thereby enabling an advanced and interactive real-time experience. The training of such an omni-MLLM is challenged by significant disparities in data quantity and convergence rates across modalities. To address these challenges, we propose a step balance strategy during pre-training to handle the quantity disparities in modality-specific data. Additionally, a dynamically adaptive balance strategy is introduced during the instruction tuning stage to synchronize the modality-wise training progress, ensuring optimal convergence. Notably, we prioritize preserving strong performance on pure text tasks to maintain the robustness of M2-omni’s language understanding capability throughout the training process. To our best knowledge, M2-omni is currently a very competitive open-source model to GPT-4o, characterized by its comprehensive modality and task support, as well as its exceptional performance. We expect M2-omni will advance the development of omni-MLLMs, thus facilitating future research in this domain. </p>
<blockquote>
<p>我们介绍了M2-omni，这是一个尖端开源的全方位多模态大型语言模型（MLLM），其性能与GPT-4o相当。M2-omni采用统一的多模态序列建模框架，使大型语言模型（LLM）具备全面的跨模态理解和生成能力。具体来说，M2-omni可以处理任意组合的音频、视频、图像和文本模态作为输入，生成交替的音频、图像或文本输出，从而实现先进且交互式的实时体验。此类全方位MLLM的训练面临跨模态数据量差异和收敛率显著不同的挑战。为了应对这些挑战，我们在预训练过程中提出了一种步骤平衡策略来处理特定模态数据在数量上的差异。此外，在指令调整阶段引入了动态自适应平衡策略，以同步不同模态的训练进度，确保最佳收敛。值得注意的是，我们优先在纯文本任务上保持卓越性能，以在整个训练过程中保持M2-omni的语言理解能力的稳健性。据我们所知，M2-omni目前是一个非常具有竞争力的开源模型，与GPT-4o相比具有全面的模态和任务支持以及出色的性能。我们相信M2-omni将推动全方位MLLM的发展，从而促进该领域的未来研究。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.18778v3">PDF</a> </p>
<p><strong>Summary</strong>：</p>
<p>M2-omni是一款先进的开源通用多模态大型语言模型，具备跨模态理解和生成能力，可处理多种模态输入并生成多媒体输出。为应对不同模态数据量和收敛率的挑战，采用预训练阶段的步骤平衡策略和指令微调阶段的动态自适应平衡策略。在保持纯文本任务性能的同时，实现了多模态支持，表现出强大的竞争力。</p>
<p><strong>Key Takeaways</strong>：</p>
<ol>
<li>M2-omni是一种先进的开源多模态大型语言模型（omni-MLLM）。</li>
<li>它采用统一的多模态序列建模框架，实现了跨模态理解和生成能力。</li>
<li>M2-omni可以处理任意组合的音频、视频、图像和文本模态输入，并生成多媒体输出。</li>
<li>在预训练阶段采用步骤平衡策略来应对不同模态数据量的挑战。</li>
<li>在指令微调阶段引入动态自适应平衡策略，以确保最优的收敛性。</li>
<li>M2-omni在保持纯文本任务性能的同时，提供了多模态支持。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.18778">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-4a05049f01e33e15aac8ab10c19f95e9.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-03a405de1ce51b7b197c5275237bfa9c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f1d74d27e2d236fb414c246230858e65.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-88f031d34f0a1a8124fd8656ff379404.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7564fb67c64d3b1baedecb5c9cdec9af.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="EarthDial-Turning-Multi-sensory-Earth-Observations-to-Interactive-Dialogues"><a href="#EarthDial-Turning-Multi-sensory-Earth-Observations-to-Interactive-Dialogues" class="headerlink" title="EarthDial: Turning Multi-sensory Earth Observations to Interactive   Dialogues"></a>EarthDial: Turning Multi-sensory Earth Observations to Interactive   Dialogues</h2><p><strong>Authors:Sagar Soni, Akshay Dudhane, Hiyam Debary, Mustansar Fiaz, Muhammad Akhtar Munir, Muhammad Sohail Danish, Paolo Fraccaro, Campbell D Watson, Levente J Klein, Fahad Shahbaz Khan, Salman Khan</strong></p>
<p>Automated analysis of vast Earth observation data via interactive Vision-Language Models (VLMs) can unlock new opportunities for environmental monitoring, disaster response, and {resource management}. Existing generic VLMs do not perform well on Remote Sensing data, while the recent Geo-spatial VLMs remain restricted to a fixed resolution and few sensor modalities. In this paper, we introduce EarthDial, a conversational assistant specifically designed for Earth Observation (EO) data, transforming complex, multi-sensory Earth observations into interactive, natural language dialogues. EarthDial supports multi-spectral, multi-temporal, and multi-resolution imagery, enabling a wide range of remote sensing tasks, including classification, detection, captioning, question answering, visual reasoning, and visual grounding. To achieve this, we introduce an extensive instruction tuning dataset comprising over 11.11M instruction pairs covering RGB, Synthetic Aperture Radar (SAR), and multispectral modalities such as Near-Infrared (NIR) and infrared. Furthermore, EarthDial handles bi-temporal and multi-temporal sequence analysis for applications like change detection. Our extensive experimental results on 44 downstream datasets demonstrate that EarthDial outperforms existing generic and domain-specific models, achieving better generalization across various EO tasks. Our source codes and pre-trained models are at <a target="_blank" rel="noopener" href="https://github.com/hiyamdebary/EarthDial">https://github.com/hiyamdebary/EarthDial</a>. </p>
<blockquote>
<p>通过交互式视觉语言模型（VLMs）对大量的地球观测数据进行自动化分析，可以为环境监测、灾害响应和{资源管理}带来新的机遇。现有的通用VLMs在遥感数据上的表现并不理想，而最近的地理空间VLMs仍然局限于固定的分辨率和有限的传感器模式。在本文中，我们介绍了EarthDial，这是一款专门为地球观测（EO）数据设计的对话助手，将复杂的多感官地球观测转化为交互式的自然语言对话。EarthDial支持多光谱、多时相和多分辨率的影像，能够完成广泛的遥感任务，包括分类、检测、描述、问答、视觉推理和视觉定位。为了实现这一点，我们引入了一个包含超过111万条指令对的庞大指令调整数据集，涵盖RGB、合成孔径雷达（SAR）以及近红外（NIR）和红外等多光谱模式。此外，EarthDial还处理双时态和多时态序列分析，用于变化检测等应用。我们在44个下游数据集上的广泛实验结果表明，EarthDial优于现有的通用和特定领域模型，在各种EO任务中实现了更好的泛化能力。我们的源代码和预训练模型位于<a target="_blank" rel="noopener" href="https://github.com/hiyamdebary/EarthDial%E3%80%82">https://github.com/hiyamdebary/EarthDial。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.15190v2">PDF</a> </p>
<p><strong>Summary</strong>：<br>通过利用交互式视觉语言模型（VLMs）对大量的地球观测数据进行自动化分析，可解锁环境监测、灾害响应和{资源管理}的新机遇。现有通用VLMs在遥感数据上表现不佳，而最新的地理空间VLMs仍受限于固定分辨率和有限的传感器模式。本文介绍了一款专为地球观测（EO）数据设计的对话助理——EarthDial，可将复杂的多感官地球观测转化为交互式自然语言对话。EarthDial支持多光谱、多时相和多分辨率的影像，可完成一系列遥感任务，包括分类、检测、描述、问答、视觉推理和视觉定位。为达成这一目标，我们推出了一款包含超过1千1百万指令对的庞大指令调优数据集，涵盖RGB、合成孔径雷达（SAR）和多光谱模式如近红外（NIR）和红外等。此外，EarthDial还可处理双时序和多时序序列分析，用于变化检测等应用。在广泛的实验和下游数据集上的结果表明，EarthDial优于现有的通用和特定领域的模型，在各种EO任务中实现了更好的泛化能力。我们的源代码和预先训练好的模型可在<a target="_blank" rel="noopener" href="https://github.com/hiyamdebary/EarthDial%E4%B8%AD%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/hiyamdebary/EarthDial中找到。</a></p>
<p><strong>Key Takeaways</strong>:</p>
<ol>
<li>EarthDial利用交互式视觉语言模型（VLMs）处理地球观测数据，实现环境监测、灾害响应和资源管理的创新应用。</li>
<li>现有VLMs在遥感数据上表现不足，而地理空间VLMs仍受限在固定分辨率和有限传感器模式上。</li>
<li>EarthDial设计用于处理多光谱、多时相和多分辨率的影像数据，支持一系列遥感任务。</li>
<li>EarthDial通过大规模指令调优数据集实现高效性能，涵盖多种传感器模式。</li>
<li>EarthDial具备处理双时序和多时序序列分析能力，适用于变化检测等应用。</li>
<li>实验结果表明，EarthDial在多种地球观测任务中表现优于现有模型。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.15190">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-25793f7de58c65d5c2c17255ce86db71.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-52e8696cfd7e236fbdae32706e2ab6e5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-63f8c3d4223882f779e76f50ea872089.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-032964f5ee4415fbf4ba61c49b63ef61.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0f92d0aa3d0e003e776638f3d9c3cee5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8f6e2a331988a050ce4c14158d45db1b.jpg" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="Training-Dynamics-of-a-1-7B-LLaMa-Model-A-Data-Efficient-Approach"><a href="#Training-Dynamics-of-a-1-7B-LLaMa-Model-A-Data-Efficient-Approach" class="headerlink" title="Training Dynamics of a 1.7B LLaMa Model: A Data-Efficient Approach"></a>Training Dynamics of a 1.7B LLaMa Model: A Data-Efficient Approach</h2><p><strong>Authors:Miles Q. Li, Benjamin C. M. Fung, Shih-Chia Huang</strong></p>
<p>Pretraining large language models is a complex endeavor influenced by multiple factors, including model architecture, data quality, training continuity, and hardware constraints. In this paper, we share insights gained from the experience of training DMaS-LLaMa-Lite, a fully open source, 1.7-billion-parameter, LLaMa-based model, on approximately 20 billion tokens of carefully curated data. We chronicle the full training trajectory, documenting how evolving validation loss levels and downstream benchmarks reflect transitions from incoherent text to fluent, contextually grounded output. Beyond pretraining, we extend our analysis to include a post-training phase focused on instruction tuning, where the model was refined to produce more contextually appropriate, user-aligned responses. We highlight practical considerations such as the importance of restoring optimizer states when resuming from checkpoints, and the impact of hardware changes on training stability and throughput. While qualitative evaluation provides an intuitive understanding of model improvements, our analysis extends to various performance benchmarks, demonstrating how high-quality data and thoughtful scaling enable competitive results with significantly fewer training tokens. By detailing these experiences and offering training logs, checkpoints, and sample outputs, we aim to guide future researchers and practitioners in refining their pretraining strategies. The training script is available on Github at <a target="_blank" rel="noopener" href="https://github.com/McGill-DMaS/DMaS-LLaMa-Lite-Training-Code">https://github.com/McGill-DMaS/DMaS-LLaMa-Lite-Training-Code</a>. The model checkpoints are available on Huggingface at <a target="_blank" rel="noopener" href="https://huggingface.co/collections/McGill-DMaS/dmas-llama-lite-6761d97ba903f82341954ceb">https://huggingface.co/collections/McGill-DMaS/dmas-llama-lite-6761d97ba903f82341954ceb</a>. </p>
<blockquote>
<p>预训练大型语言模型是一项复杂的任务，受到多种因素的影响，包括模型架构、数据质量、训练连续性和硬件限制。在本文中，我们分享了训练DMaS-LLaMa-Lite模型的实战经验。这是一个完全开源的、基于LLaMa的1.7亿参数模型，在约20亿标记的精心挑选的数据上进行训练。我们详细记录了整个训练轨迹，描述了验证损失水平的演变和下游基准测试如何反映从文本不连贯到流畅、基于上下文输出的转变。除了预训练阶段外，我们将分析扩展到了包括侧重于指令调整的后期训练阶段，在这个阶段，对模型进行了改进，以产生更符合上下文、更符合用户意图的响应。我们强调了实际考虑因素，如恢复优化器状态时恢复检查点的重要性，以及硬件更改对训练稳定性和吞吐量的影响。虽然定性评估为模型改进提供了直观的理解，但我们的分析还涵盖了各种性能基准测试，证明了高质量数据和有策略的规模扩展如何在显著减少训练标记的情况下实现具有竞争力的结果。通过详细记录这些经验并提供训练日志、检查点和样本输出，我们的目标是指导未来的研究者和实践者改进他们的预训练策略。训练脚本可在Github上找到：<a target="_blank" rel="noopener" href="https://github.com/McGill-DMaS/DMaS-LLaMa-Lite-Training-Code%E3%80%82%E6%A8%A1%E5%9E%8B%E6%A3%80%E7%82%B9%E5%8F%AF%E5%9C%A8Huggingface%E4%B8%8A%E6%89%BE%E5%88%B0%EF%BC%9Ahttps://huggingface.co/collections/McGill-DMaS/dmas-llama-lite-6761d97ba903f82341954ceb%E3%80%82">https://github.com/McGill-DMaS/DMaS-LLaMa-Lite-Training-Code。模型检查点可在Huggingface上找到：https://huggingface.co/collections/McGill-DMaS/dmas-llama-lite-6761d97ba903f82341954ceb。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.13335v3">PDF</a> </p>
<p><strong>摘要</strong></p>
<p>本文介绍了训练DMaS-LLaMa-Lite这一开源大型语言模型的实践经验。模型基于LLaMa架构，拥有约1.7亿参数，并在约20亿标记的精心筛选的数据上进行训练。文章详细描述了训练过程，包括验证损失的变化和下游基准测试的表现。此外，还介绍了模型在训练后的指令微调阶段。文章强调了实际考虑因素，如恢复检查点时优化器状态的重要性以及硬件变化对训练稳定性和吞吐量的影响。通过性能基准测试，展示了高质量数据和有针对性的扩展如何使模型在显著较少的训练标记上实现竞争结果。本文旨在指导未来研究者和实践者在预训练策略方面的改进。相关资源链接已提供。</p>
<p><strong>关键见解</strong></p>
<ol>
<li>训练大型语言模型是一个受多种因素影响的复杂过程，包括模型架构、数据质量、训练连续性和硬件限制。</li>
<li>DMaS-LLaMa-Lite是一个基于LLaMa的开源模型，其训练过程包括预训练和指令微调阶段。</li>
<li>验证损失的变化和下游基准测试的表现反映了模型从无序文本到流畅、语境丰富的输出的过渡。</li>
<li>恢复优化器状态在恢复检查点时至关重要，硬件变化会影响训练的稳定性和效率。</li>
<li>高质量数据和有针对性的扩展可以显著提高模型的性能，使用更少的训练标记即可实现竞争结果。</li>
<li>文章提供了详细的实践经验、训练日志、检查点和样本输出，旨在指导未来研究者改进预训练策略。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.13335">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-be1c70d0da462dcc0e5dc4485fde017c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7e0277e3f2b1892c5a528caba3b16030.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-55c412a006cb8ba62aa60b2f38587d55.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-937b297e330cc2ba678a0faf24781a9e.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-d042aa459379d6e77f6fa89e9508e1d1.jpg" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="GReaTer-Gradients-over-Reasoning-Makes-Smaller-Language-Models-Strong-Prompt-Optimizers"><a href="#GReaTer-Gradients-over-Reasoning-Makes-Smaller-Language-Models-Strong-Prompt-Optimizers" class="headerlink" title="GReaTer: Gradients over Reasoning Makes Smaller Language Models Strong   Prompt Optimizers"></a>GReaTer: Gradients over Reasoning Makes Smaller Language Models Strong   Prompt Optimizers</h2><p><strong>Authors:Sarkar Snigdha Sarathi Das, Ryo Kamoi, Bo Pang, Yusen Zhang, Caiming Xiong, Rui Zhang</strong></p>
<p>The effectiveness of large language models (LLMs) is closely tied to the design of prompts, making prompt optimization essential for enhancing their performance across a wide range of tasks. Many existing approaches to automating prompt engineering rely exclusively on textual feedback, refining prompts based solely on inference errors identified by large, computationally expensive LLMs. Unfortunately, smaller models struggle to generate high-quality feedback, resulting in complete dependence on large LLM judgment. Moreover, these methods fail to leverage more direct and finer-grained information, such as gradients, due to operating purely in text space. To this end, we introduce GReaTer, a novel prompt optimization technique that directly incorporates gradient information over task-specific reasoning. By utilizing task loss gradients, GReaTer enables self-optimization of prompts for open-source, lightweight language models without the need for costly closed-source LLMs. This allows high-performance prompt optimization without dependence on massive LLMs, closing the gap between smaller models and the sophisticated reasoning often needed for prompt refinement. Extensive evaluations across diverse reasoning tasks including BBH, GSM8k, and FOLIO demonstrate that GReaTer consistently outperforms previous state-of-the-art prompt optimization methods, even those reliant on powerful LLMs. Additionally, GReaTer-optimized prompts frequently exhibit better transferability and, in some cases, boost task performance to levels comparable to or surpassing those achieved by larger language models, highlighting the effectiveness of prompt optimization guided by gradients over reasoning. Code of GReaTer is available at <a target="_blank" rel="noopener" href="https://github.com/psunlpgroup/GreaTer">https://github.com/psunlpgroup/GreaTer</a>. </p>
<blockquote>
<p>大型语言模型（LLM）的有效性与其提示设计密切相关，这使得提示优化对于提高其在广泛任务中的性能至关重要。许多现有的自动化提示工程方法完全依赖于文本反馈，仅根据大型、计算成本高昂的LLM所识别到的推理错误来优化提示。然而，小型模型在生成高质量反馈方面表现挣扎，导致完全依赖大型LLM的判断。此外，这些方法未能利用更直接和更精细的信息，如梯度，因为它们仅在文本空间中进行操作。为此，我们引入了GReaTer，一种新的提示优化技术，它可以直接结合任务特定推理的梯度信息。通过利用任务损失梯度，GReaTer能够自行优化开放源代码、轻量级语言模型的提示，而无需昂贵的封闭源代码LLM。这允许进行高性能的提示优化，而不依赖于大规模LLM，缩小了小型模型和经常需要进行提示优化的复杂推理之间的差距。在包括BBH、GSM8k和FOLIO等多样化的推理任务上的广泛评估表明，GReaTer始终优于先前的最先进的提示优化方法，甚至超过了那些依赖强大LLM的方法。此外，经过GReaTer优化的提示通常表现出更好的可迁移性，在某些情况下，任务性能提升到了与大型语言模型相当甚至更高的水平，这突显了由梯度引导的提示优化的有效性。GReaTer的代码可在<a target="_blank" rel="noopener" href="https://github.com/psunlpgroup/GreaTer%E4%B8%8A%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/psunlpgroup/GreaTer上找到。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.09722v2">PDF</a> ICLR 2025 Camera Ready</p>
<p><strong>摘要</strong></p>
<p>大型语言模型（LLM）的有效性与其提示设计密切相关，提示优化对于提高其在各种任务中的性能至关重要。现有的自动提示工程方法大多依赖于文本反馈，仅凭大型且计算昂贵的LLM识别的推理错误来优化提示。然而，小型模型在生成高质量反馈方面表现挣扎，导致完全依赖大型LLM的判断。此外，这些方法未能利用更直接和精细的信息，如梯度，因为它们仅在文本空间内运行。为此，我们引入了GReaTer，一种新型提示优化技术，该技术直接结合任务特定推理的梯度信息来进行自我优化。通过利用任务损失梯度，GReaTer可以使开放源代码、轻量级语言模型进行自我优化提示，无需昂贵的闭源LLMs。这使得在没有依赖大型LLM的情况下进行高性能提示优化成为可能，缩小了小型模型与经常需要进行提示优化的精细推理之间的差距。在包括BBH、GSM8k和FOLIO在内的各种推理任务上的广泛评估表明，GReaTer始终优于先前的最新提示优化方法，甚至优于依赖强大LLMs的方法。此外，GReaTer优化的提示通常具有更好的可迁移性，并且在某些情况下，任务性能提升到了可与或超越大型语言模型实现的水平，突显了由梯度引导提示优化的有效性。</p>
<p><strong>关键见解</strong></p>
<ol>
<li>提示优化对于提高LLM在各种任务中的性能至关重要。</li>
<li>现有方法主要依赖文本反馈和大型LLM进行提示优化。</li>
<li>GReaTer是一种新型提示优化技术，结合梯度信息进行自我优化。</li>
<li>GReaTer使小型语言模型能够进行高性能提示优化，无需依赖大型LLM。</li>
<li>GReaTer在多种推理任务上的表现优于其他最新提示优化方法。</li>
<li>GReaTer优化的提示具有更好的可迁移性。</li>
<li>GReaTer提升任务性能至与大型语言模型相当或更高的水平。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.09722">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-edc278df9837fdde390c500c45f90aff.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-c34d7cf6e29187574c467f86bdf44ae4.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e0ea993ae1ec58390056fb9fa4f32d4f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4e0e4c6c71b3dc812cf6021bb302be58.jpg" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1><h2 id="Spider-Any-to-Many-Multimodal-LLM"><a href="#Spider-Any-to-Many-Multimodal-LLM" class="headerlink" title="Spider: Any-to-Many Multimodal LLM"></a>Spider: Any-to-Many Multimodal LLM</h2><p><strong>Authors:Jinxiang Lai, Jie Zhang, Jun Liu, Jian Li, Xiaocheng Lu, Song Guo</strong></p>
<p>Multimodal LLMs (MLLMs) have emerged as an extension of Large Language Models (LLMs), enabling the integration of various modalities. However, Any-to-Any MLLMs are limited to generating pairwise modalities ‘Text + X’ within a single response, such as Text + {Image or Audio or Video}. To address this limitation, we introduce Spider, a novel efficient Any-to-Many Modalities Generation (AMMG) framework, which can generate an arbitrary combination of modalities ‘Text + Xs’, such as Text + {Image and Audio and Video}. To achieve efficient AMMG, our Spider integrates three core components: a Base Model for basic X-to-X (i.e., Any-to-Any) modality processing, an Any-to-Many Instruction Template designed for producing Xs signal prompts, and a novel Efficient Decoders-Controller for controlling multimodal Decoders to generate Xs (many-modal) contents. To train Spider, we constructed a novel Text-formatted Many-Modal (TMM) dataset, which facilitates learning the X-to-Xs (i.e., Any-to-Many) capability necessary for AMMG. Ultimately, the well-trained Spider generates a pseudo X-to-Xs dataset, the first-ever X-to-Xs many-modal dataset, enhancing the potential for AMMG tasks in future research. Overall, this work not only pushes the boundary of multimodal interaction but also provides rich data support for advancing the field. Code: <a target="_blank" rel="noopener" href="https://github.com/Layjins/Spider">https://github.com/Layjins/Spider</a> </p>
<blockquote>
<p>多模态大型语言模型（MLLMs）作为大型语言模型（LLMs）的扩展而出现，能够实现对各种模态的集成。然而，任何到任何的多模态LLMs仅限于在单个响应中生成“文本+X”（如文本+图像或音频或视频）的配对模态。为了解决这一限制，我们引入了Spider，这是一种新型高效的多对多模态生成（AMMG）框架，能够生成任意组合的模态，如文本+{图像和音频和视频}。为了实现高效的AMMG，我们的Spider集成了三个核心组件：用于基本X到X（即任何到任何）模态处理的基准模型、用于生成X信号提示的任何到多指令模板，以及用于控制多模态解码器生成X（多模态）内容的新型高效解码器控制器。为了训练Spider，我们构建了一个新型文本格式的多模态（TMM）数据集，该数据集有助于学习进行AMMG所必需的X到Xs（即任何到多）能力。最终，经过良好训练的Spider生成了一个伪X到Xs数据集，这是首个X到Xs多模态数据集，增强了未来研究中AMMG任务的潜力。总体而言，这项工作不仅推动了多模态交互的边界，而且为推进该领域提供了丰富的数据支持。代码地址：<a target="_blank" rel="noopener" href="https://github.com/Layjins/Spider">https://github.com/Layjins/Spider</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2411.09439v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>基于大型语言模型（LLM）的多模态LLM（MLLM）能够整合多种模态。然而，现有的Any-to-Any MLLM仅限于在单一响应中生成配对模态“文本+X”（如文本+图像、音频或视频）。为解决这个问题，我们推出Spider，一种新型高效的Any-to-Many Modalities Generation（AMMG）框架，能够生成任意的模态组合“文本+Xs”（如文本+图像、音频和视频）。Spider通过三个核心组件实现高效的AMMG：基础模型用于基本的X-to-X（即Any-to-Any）模态处理，Any-to-Many指令模板用于生成Xs信号提示，以及控制多模态解码器生成Xs（多模态）内容的新型高效解码器控制器。为训练Spider，我们构建了新颖的Text-formatted Many-Modal（TMM）数据集，有助于学习进行AMMG所需的X-to-Xs能力。最终，经过良好训练的Spider生成了首个X-to-Xs多模态数据集，增强了未来研究中AMMG任务的潜力。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>多模态LLM（MLLM）是LLM的扩展，支持多种模态的集成。</li>
<li>现有的Any-to-Any MLLM仅限于生成配对模态。</li>
<li>Spider是一种新型的Any-to-Many Modalities Generation（AMMG）框架，能够生成任意的模态组合。</li>
<li>Spider通过三个核心组件实现高效的AMMG：基础模型、Any-to-Many指令模板和高效解码器控制器。</li>
<li>为训练Spider，构建了Text-formatted Many-Modal（TMM）数据集。</li>
<li>Spider生成了首个X-to-Xs多模态数据集。</li>
<li>Spider的研究不仅推动了多模态交互的边界，还为未来研究提供了丰富的数据支持。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2411.09439">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-548fcaca13bdcf76ee11ff796479ac3a.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-263b0246f95c24205a660414b0cb20e3.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-4bf1f6e626846a87d9d4abeeabb6cb96.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e9019ca0b551072be75aa0869bce58cb.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-884958a466ce18977641de875607df0f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2aff988bcbd407e8298e46389e20712c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ea781f9cd0970958258bd57524c14a3d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e7928e4410bc9f273d01c23d699a334b.jpg" align="middle">
</details>


<h1 id="-19"><a href="#-19" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        文章作者:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        文章链接:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-04-09/LLM/">https://kedreamix.github.io/Talk2Paper/Paper/2025-04-09/LLM/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        版权声明:
                    </i>
                </span>
                <span class="reprint-info">
                    本博客所有文章除特別声明外，均采用
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    许可协议。转载请注明来源
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/LLM/">
                                    <span class="chip bg-color">LLM</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;上一篇</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-04-09/Agent/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-ca7525f9b77e52545650c04458c2cc6c.jpg" class="responsive-img" alt="Agent">
                        
                        <span class="card-title">Agent</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Agent 方向最新论文已更新，请持续关注 Update in 2025-04-09  CREA A Collaborative Multi-Agent Framework for Creative Content   Generation with Diffusion Models
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-04-09
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Agent/" class="post-category">
                                    Agent
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Agent/">
                        <span class="chip bg-color">Agent</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                下一篇&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-04-09/R1_Reasoning/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-1d6f3f8c195b241546a73c58d51f2e92.jpg" class="responsive-img" alt="R1_Reasoning">
                        
                        <span class="card-title">R1_Reasoning</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            R1_Reasoning 方向最新论文已更新，请持续关注 Update in 2025-04-09  InteractVLM 3D Interaction Reasoning from 2D Foundational Models
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-04-09
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/R1-Reasoning/" class="post-category">
                                    R1_Reasoning
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/R1-Reasoning/">
                        <span class="chip bg-color">R1_Reasoning</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- 代码块功能依赖 -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- 代码语言 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- 代码块复制 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- 代码块收缩 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC 悬浮按钮. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* 修复文章卡片 div 的宽度. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // 切换TOC目录展开收缩的相关操作.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;站点总字数:&nbsp;<span
                        class="white-color">18588k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- 运行天数提醒. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // 区分是否有年份.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = '本站已运行 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                daysTip = '本站已運行 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = '本站已运行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = '本站已運行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="关注我的知乎: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">知</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- 搜索遮罩框 -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span>
            <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- 白天和黑夜主题 -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- 回到顶部按钮 -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // 只在桌面版网页启用特效
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- 雪花特效 -->
    

    <!-- 鼠标星星特效 -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--腾讯兔小巢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- 动态标签 -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Σ(っ °Д °;)っ诶，页面崩溃了嘛？", clearTimeout(st)) : (document.title = "φ(゜▽゜*)♪咦，又好了！", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
