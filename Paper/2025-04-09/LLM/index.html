<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="LLM">
    <meta name="description" content="LLM æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-04-09  URECA Unique Region Caption Anything">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>LLM | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pica.zhimg.com/v2-4bf1f6e626846a87d9d4abeeabb6cb96.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">LLM</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/LLM/">
                                <span class="chip bg-color">LLM</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/LLM/" class="post-category">
                                LLM
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-04-09
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-05-14
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    20.8k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    85 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-04-09-æ›´æ–°"><a href="#2025-04-09-æ›´æ–°" class="headerlink" title="2025-04-09 æ›´æ–°"></a>2025-04-09 æ›´æ–°</h1><h2 id="URECA-Unique-Region-Caption-Anything"><a href="#URECA-Unique-Region-Caption-Anything" class="headerlink" title="URECA: Unique Region Caption Anything"></a>URECA: Unique Region Caption Anything</h2><p><strong>Authors:Sangbeom Lim, Junwan Kim, Heeji Yoon, Jaewoo Jung, Seungryong Kim</strong></p>
<p>Region-level captioning aims to generate natural language descriptions for specific image regions while highlighting their distinguishing features. However, existing methods struggle to produce unique captions across multi-granularity, limiting their real-world applicability. To address the need for detailed region-level understanding, we introduce URECA dataset, a large-scale dataset tailored for multi-granularity region captioning. Unlike prior datasets that focus primarily on salient objects, URECA dataset ensures a unique and consistent mapping between regions and captions by incorporating a diverse set of objects, parts, and background elements. Central to this is a stage-wise data curation pipeline, where each stage incrementally refines region selection and caption generation. By leveraging Multimodal Large Language Models (MLLMs) at each stage, our pipeline produces distinctive and contextually grounded captions with improved accuracy and semantic diversity. Building upon this dataset, we present URECA, a novel captioning model designed to effectively encode multi-granularity regions. URECA maintains essential spatial properties such as position and shape through simple yet impactful modifications to existing MLLMs, enabling fine-grained and semantically rich region descriptions. Our approach introduces dynamic mask modeling and a high-resolution mask encoder to enhance caption uniqueness. Experiments show that URECA achieves state-of-the-art performance on URECA dataset and generalizes well to existing region-level captioning benchmarks. </p>
<blockquote>
<p>åŒºåŸŸçº§æ ‡é¢˜ç”Ÿæˆæ—¨åœ¨é’ˆå¯¹ç‰¹å®šå›¾åƒåŒºåŸŸç”Ÿæˆè‡ªç„¶è¯­è¨€æè¿°ï¼ŒåŒæ—¶çªå‡ºå…¶åŒºåˆ†ç‰¹å¾ã€‚ç„¶è€Œï¼Œç°æœ‰æ–¹æ³•åœ¨è·¨å¤šç²’åº¦ç”Ÿæˆå”¯ä¸€æ ‡é¢˜æ–¹é¢å­˜åœ¨å›°éš¾ï¼Œé™åˆ¶äº†å®ƒä»¬åœ¨ç°å®ä¸–ç•Œä¸­çš„åº”ç”¨ã€‚ä¸ºäº†è§£å†³å¯¹è¯¦ç»†åŒºåŸŸçº§ç†è§£çš„éœ€æ±‚ï¼Œæˆ‘ä»¬å¼•å…¥äº†URECAæ•°æ®é›†ï¼Œè¿™æ˜¯ä¸€ä¸ªä¸“ä¸ºå¤šç²’åº¦åŒºåŸŸæ ‡é¢˜è®¾è®¡çš„å¤§è§„æ¨¡æ•°æ®é›†ã€‚ä¸ä¸»è¦å…³æ³¨æ˜¾è‘—ç‰©ä½“çš„å…ˆå‰æ•°æ®é›†ä¸åŒï¼ŒURECAæ•°æ®é›†é€šè¿‡èå…¥å„ç§ç‰©ä½“ã€éƒ¨åˆ†å’ŒèƒŒæ™¯å…ƒç´ ï¼Œç¡®ä¿åŒºåŸŸå’Œæ ‡é¢˜ä¹‹é—´çš„ç‹¬ç‰¹ä¸”ä¸€è‡´çš„æ˜ å°„ã€‚å…³é”®åœ¨äºåˆ†é˜¶æ®µæ•°æ®æ•´ç†ç®¡é“ï¼Œæ¯ä¸ªé˜¶æ®µéƒ½ä¼šé€æ­¥æ”¹è¿›åŒºåŸŸé€‰æ‹©å’Œæ ‡é¢˜ç”Ÿæˆã€‚æˆ‘ä»¬çš„ç®¡é“åˆ©ç”¨å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨æ¯ä¸ªé˜¶æ®µè¿›è¡Œç²¾ç»†åŒ–å¤„ç†ï¼Œç”Ÿæˆç‹¬ç‰¹ä¸”åŸºäºä¸Šä¸‹æ–‡çš„æ ‡é¢˜ï¼Œæé«˜äº†å‡†ç¡®æ€§å’Œè¯­ä¹‰å¤šæ ·æ€§ã€‚åœ¨æ­¤åŸºç¡€ä¸Šï¼Œæˆ‘ä»¬æ¨å‡ºäº†URECAï¼Œè¿™æ˜¯ä¸€ç§æ–°å‹æ ‡é¢˜ç”Ÿæˆæ¨¡å‹ï¼Œæ—¨åœ¨æœ‰æ•ˆç¼–ç å¤šç²’åº¦åŒºåŸŸã€‚URECAé€šè¿‡ç®€å•è€Œæœ‰æ•ˆçš„ä¿®æ”¹ç°æœ‰MLLMsï¼Œä¿ç•™å…³é”®çš„ç©ºé—´å±æ€§ï¼ˆå¦‚ä½ç½®å’Œå½¢çŠ¶ï¼‰ï¼Œä»è€Œå®ç°ç²¾ç»†ä¸”è¯­ä¹‰ä¸°å¯Œçš„åŒºåŸŸæè¿°ã€‚æˆ‘ä»¬çš„æ–¹æ³•å¼•å…¥äº†åŠ¨æ€æ©æ¨¡å»ºæ¨¡å’Œé«˜åˆ†è¾¨ç‡æ©æ¨¡ç¼–ç å™¨ï¼Œä»¥æé«˜æ ‡é¢˜çš„ç‹¬åˆ›æ€§ã€‚å®éªŒè¡¨æ˜ï¼ŒURECAåœ¨URECAæ•°æ®é›†ä¸Šè¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œå¹¶åœ¨ç°æœ‰çš„åŒºåŸŸçº§æ ‡é¢˜ç”ŸæˆåŸºå‡†æµ‹è¯•ä¸­å…·æœ‰è‰¯å¥½çš„é€šç”¨æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.05305v1">PDF</a> Project page: <a target="_blank" rel="noopener" href="https://cvlab-kaist.github.io/URECA">https://cvlab-kaist.github.io/URECA</a> Code:   <a target="_blank" rel="noopener" href="https://github.com/cvlab-kaist/URECA">https://github.com/cvlab-kaist/URECA</a></p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ—¨åœ¨è§£å†³ç°æœ‰å›¾åƒåŒºåŸŸæè¿°æ–¹æ³•åœ¨å¤šç²’åº¦åŒºåŸŸæè¿°ä¸Šçš„ä¸è¶³ï¼Œä¸ºæ­¤å¼•å…¥äº†URECAæ•°æ®é›†å’ŒURECAæ¨¡å‹ã€‚URECAæ•°æ®é›†é€šè¿‡é˜¶æ®µå¼æ•°æ®æ•´ç†ç®¡é“ï¼Œç»“åˆå¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰ï¼Œç¡®ä¿åŒºåŸŸä¸æè¿°çš„ç‹¬ç‰¹ä¸”ä¸€è‡´æ˜ å°„ã€‚URECAæ¨¡å‹åˆ™é€šè¿‡ä¿®æ”¹ç°æœ‰MLLMsï¼Œå¢åŠ åŠ¨æ€æ©æ¨¡å»ºæ¨¡å’Œé«˜åˆ†è¾¨ç‡æ©æ¨¡ç¼–ç å™¨ï¼Œæœ‰æ•ˆç¼–ç å¤šç²’åº¦åŒºåŸŸå¹¶æå‡æè¿°æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>åŒºåŸŸçº§æè¿°æ—¨åœ¨ç”Ÿæˆç‰¹å®šå›¾åƒåŒºåŸŸçš„è‡ªç„¶è¯­è¨€æè¿°ï¼Œå¹¶çªå‡ºå…¶åŒºåˆ†ç‰¹å¾ã€‚</li>
<li>ç°æœ‰æ–¹æ³•åœ¨ç”Ÿæˆå¤šç²’åº¦åŒºåŸŸçš„ç‹¬ç‰¹æè¿°æ–¹é¢å­˜åœ¨æŒ‘æˆ˜ï¼Œé™åˆ¶äº†å…¶åœ¨ç°å®ä¸–ç•Œçš„åº”ç”¨ã€‚</li>
<li>URECAæ•°æ®é›†çš„å¼•å…¥æ˜¯ä¸ºäº†æ»¡è¶³å¯¹å¤šç²’åº¦åŒºåŸŸçº§åˆ«çš„è¯¦ç»†ç†è§£çš„éœ€æ±‚ã€‚</li>
<li>URECAæ•°æ®é›†é€šè¿‡é˜¶æ®µå¼æ•°æ®æ•´ç†ç®¡é“å’Œå¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰çš„ç»“åˆï¼Œç¡®ä¿åŒºåŸŸä¸æè¿°çš„ç‹¬ç‰¹æ˜ å°„ã€‚</li>
<li>URECAæ¨¡å‹èƒ½æœ‰æ•ˆç¼–ç å¤šç²’åº¦åŒºåŸŸï¼Œå¹¶ç»´æŒå…³é”®çš„ç©ºé—´å±æ€§å¦‚ä½ç½®å’Œå½¢çŠ¶ã€‚</li>
<li>URECAæ¨¡å‹é€šè¿‡åŠ¨æ€æ©æ¨¡å»ºæ¨¡å’Œé«˜åˆ†è¾¨ç‡æ©æ¨¡ç¼–ç å™¨æå‡æè¿°æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.05305">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-6dee61b1b00017453f047cae7484cc48.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b98b5f0268a9620e444676fb6ea87d69.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-023be26220a714450195378d31964b50.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ac22fab9c12fc485bbed9c1acc7c62b9.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="What-We-Do-Not-Know-GPT-Use-in-Business-and-Management"><a href="#What-We-Do-Not-Know-GPT-Use-in-Business-and-Management" class="headerlink" title="What We Do Not Know: GPT Use in Business and Management"></a>What We Do Not Know: GPT Use in Business and Management</h2><p><strong>Authors:Tammy Mackenzie, Branislav Radeljic, Leslie Salgado, Animesh Paul, Rubaina Khan, Aizhan Tursunbayeva, Natalie Perez, Sreyoshi Bhaduri</strong></p>
<p>This systematic review examines peer-reviewed studies on application of GPT in business management, revealing significant knowledge gaps. Despite identifying interesting research directions such as best practices, benchmarking, performance comparisons, social impacts, our analysis yields only 42 relevant studies for the 22 months since its release. There are so few studies looking at a particular sector or subfield that management researchers, business consultants, policymakers, and journalists do not yet have enough information to make well-founded statements on how GPT is being used in businesses. The primary contribution of this paper is a call to action for further research. We provide a description of current research and identify knowledge gaps on the use of GPT in business. We cover the management subfields of finance, marketing, human resources, strategy, operations, production, and analytics, excluding retail and sales. We discuss gaps in knowledge of GPT potential consequences on employment, productivity, environmental costs, oppression, and small businesses. We propose how management consultants and the media can help fill those gaps. We call for practical work on business control systems as they relate to existing and foreseeable AI-related business challenges. This work may be of interest to managers, to management researchers, and to people working on AI in society. </p>
<blockquote>
<p>è¿™ç¯‡ç³»ç»Ÿç»¼è¿°è€ƒå¯Ÿäº†å…³äºGPTåœ¨å•†ä¸šç®¡ç†ä¸­çš„åº”ç”¨çš„åŒè¡Œè¯„å®¡ç ”ç©¶ï¼Œæ­ç¤ºäº†æ˜¾è‘—çš„çŸ¥è¯†ç©ºç™½ã€‚å°½ç®¡ç¡®å®šäº†æœ‰è¶£çš„ç ”ç©¶æ–¹å‘ï¼Œä¾‹å¦‚æœ€ä½³å®è·µã€åŸºå‡†æµ‹è¯•ã€æ€§èƒ½æ¯”è¾ƒã€ç¤¾ä¼šå½±å“ç­‰ï¼Œä½†è‡ªGPTå‘å¸ƒä»¥æ¥çš„çŸ­çŸ­22ä¸ªæœˆå†…ï¼Œæˆ‘ä»¬ä»…åˆ†æå‡ºäº†42é¡¹ç›¸å…³ç ”ç©¶ã€‚å¯¹äºæŸä¸€ç‰¹å®šé¢†åŸŸæˆ–å­é¢†åŸŸï¼Œå¦‚ç®¡ç†å±‚é¢çš„ç ”ç©¶ã€å•†ä¸šé¡¾é—®ã€æ”¿ç­–åˆ¶å®šè€…å’Œè®°è€…ç­‰ï¼Œå…³äºGPTåœ¨å•†ä¸šä¸­å¦‚ä½•åº”ç”¨çš„é™ˆè¿°å°šç¼ºä¹è¶³å¤Ÿçš„ä¿¡æ¯æ”¯æŒã€‚æœ¬æ–‡çš„ä¸»è¦è´¡çŒ®æ˜¯å‘¼åè¿›ä¸€æ­¥é‡‡å–è¡ŒåŠ¨è¿›è¡Œç ”ç©¶ã€‚æˆ‘ä»¬æè¿°äº†å½“å‰çš„ç ”ç©¶ç°çŠ¶ï¼Œå¹¶æŒ‡å‡ºäº†åœ¨GPTåœ¨å•†ä¸šé¢†åŸŸåº”ç”¨çš„çŸ¥è¯†ç©ºç™½ã€‚æˆ‘ä»¬æ¶µç›–äº†é‡‘èã€å¸‚åœºè¥é”€ã€äººåŠ›èµ„æºã€æˆ˜ç•¥ã€è¿è¥ã€ç”Ÿäº§å’Œåˆ†æçš„ç®¡ç†å­é¢†åŸŸï¼Œä½†ä¸åŒ…æ‹¬é›¶å”®å’Œé”€å”®ã€‚æˆ‘ä»¬è®¨è®ºäº†GPTå¯¹å°±ä¸šã€ç”Ÿäº§åŠ›ã€ç¯å¢ƒæˆæœ¬ã€å‹è¿«å’Œå°å‹ä¼ä¸šå¯èƒ½äº§ç”Ÿçš„æ½œåœ¨åæœçš„çŸ¥è¯†ç©ºç™½ã€‚æˆ‘ä»¬æå‡ºç®¡ç†å’¨è¯¢æœºæ„å’Œåª’ä½“åº”å¦‚ä½•å¸®åŠ©å¡«è¡¥è¿™äº›ç©ºç™½ã€‚æˆ‘ä»¬å‘¼åå…³äºä¼ä¸šæ§åˆ¶ç³»ç»Ÿåœ¨åº”å¯¹å½“å‰å’Œæœªæ¥çš„äººå·¥æ™ºèƒ½ç›¸å…³å•†ä¸šæŒ‘æˆ˜ä¸Šçš„å®é™…åº”ç”¨å·¥ä½œã€‚æœ¬æ–‡å¯èƒ½å¯¹äºç»ç†ã€ç®¡ç†ç ”ç©¶äººå‘˜å’Œä»äº‹äººå·¥æ™ºèƒ½åœ¨ç¤¾ä¼šå·¥ä½œçš„äººä»¬æ„Ÿå…´è¶£ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.05273v1">PDF</a> 16 pages, 5 tables, 2 figures</p>
<p><strong>Summary</strong><br>     è¯¥æ–‡çŒ®å¯¹GPTåœ¨ä¼ä¸šç®¡ç†çš„åº”ç”¨è¿›è¡Œäº†ç³»ç»Ÿç»¼è¿°ï¼Œå‘ç°å­˜åœ¨æ˜¾è‘—çš„çŸ¥è¯†ç©ºç™½ã€‚å°½ç®¡ç¡®å®šäº†æœ‰è¶£çš„ç ”ç©¶æ–¹å‘ï¼Œå¦‚æœ€ä½³å®è·µã€åŸºå‡†æµ‹è¯•ã€æ€§èƒ½æ¯”è¾ƒå’Œç¤¾ä¼šå½±å“ç­‰ï¼Œä½†åœ¨GPTå‘å¸ƒåçš„çŸ­çŸ­22ä¸ªæœˆå†…ï¼Œä»…å‘ç°äº†42é¡¹ç›¸å…³ç ”ç©¶ã€‚å…³äºGPTåœ¨ç‰¹å®šè¡Œä¸šæˆ–å­é¢†åŸŸçš„åº”ç”¨ï¼Œç®¡ç†ç ”ç©¶äººå‘˜ã€ä¸šåŠ¡é¡¾é—®ã€æ”¿ç­–åˆ¶å®šè€…å’Œè®°è€…å°šæœªè·å¾—è¶³å¤Ÿçš„ä¿¡æ¯æ¥åšå‡ºåˆç†çš„é™ˆè¿°ã€‚æœ¬æ–‡çš„ä¸»è¦è´¡çŒ®æ˜¯å‘¼åé‡‡å–è¡ŒåŠ¨è¿›è¡Œè¿›ä¸€æ­¥çš„ç ”ç©¶ã€‚æœ¬æ–‡æè¿°äº†å½“å‰çš„ç ”ç©¶ç°çŠ¶å¹¶ç¡®å®šäº†å…³äºGPTåœ¨å•†ä¸šåº”ç”¨ä¸­çš„çŸ¥è¯†ç©ºç™½ã€‚æ‰€æ¶‰åŠçš„ç®¡ç†å­é¢†åŸŸåŒ…æ‹¬é‡‘èã€å¸‚åœºè¥é”€ã€äººåŠ›èµ„æºã€æˆ˜ç•¥ã€è¿è¥ã€ç”Ÿäº§å’Œåˆ†æç­‰é¢†åŸŸï¼ˆé›¶å”®å’Œé”€å”®é™¤å¤–ï¼‰ã€‚æ–‡ç« æ¢è®¨äº†å…³äºGPTçš„æ½œåœ¨åæœåœ¨å°±ä¸šã€ç”Ÿäº§åŠ›ã€ç¯å¢ƒæˆæœ¬ã€å‹è¿«å’Œå°å‹ä¼ä¸šç­‰æ–¹é¢çš„çŸ¥è¯†ç©ºç™½ã€‚æœ¬æ–‡æå‡ºäº†ç®¡ç†é¡¾é—®å’Œåª’ä½“å¦‚ä½•å¸®åŠ©å¡«è¡¥è¿™äº›ç©ºç™½çš„æ–¹æ³•ã€‚å¹¶å‘¼åé’ˆå¯¹ç°æœ‰çš„å’Œå¯é¢„è§çš„ä¸äººå·¥æ™ºèƒ½ç›¸å…³çš„å•†ä¸šæŒ‘æˆ˜å¼€å±•å®é™…å·¥ä½œã€‚è¿™ä¸€å·¥ä½œå¯¹ç®¡ç†è€…ã€ç®¡ç†ç ”ç©¶äººå‘˜å’Œä»äº‹äººå·¥æ™ºèƒ½ç¤¾ä¼šç ”ç©¶çš„äººå‘˜å¯èƒ½éƒ½æœ‰å¸®åŠ©ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>GPTåœ¨ä¼ä¸šç®¡ç†æ–¹é¢çš„åº”ç”¨å­˜åœ¨æ˜¾è‘—çš„çŸ¥è¯†ç©ºç™½ï¼Œå°½ç®¡å·²æœ‰å°‘é‡ç ”ç©¶æ¶‰åŠæœ€ä½³å®è·µã€åŸºå‡†æµ‹è¯•ç­‰æ–¹å‘ã€‚</li>
<li>ç›®å‰å…³äºGPTåœ¨ç‰¹å®šè¡Œä¸šæˆ–å­é¢†åŸŸçš„åº”ç”¨ç ”ç©¶ä»ç„¶ä¸è¶³ï¼Œä½¿å¾—ç›¸å…³é¢†åŸŸçš„ä¸“å®¶æ— æ³•åšå‡ºåŸºäºå……åˆ†ä¿¡æ¯çš„é™ˆè¿°ã€‚</li>
<li>æ–‡ç« å‘¼åè¿›ä¸€æ­¥çš„ç ”ç©¶è¡ŒåŠ¨ï¼Œç‰¹åˆ«æ˜¯åœ¨å¡«è¡¥å½“å‰å…³äºGPTåœ¨å•†ä¸šä¸­çš„çŸ¥è¯†ç©ºç™½æ–¹é¢ã€‚</li>
<li>æ–‡ä¸­æ¢è®¨äº†GPTå¯¹å°±ä¸šã€ç”Ÿäº§åŠ›ã€ç¯å¢ƒæˆæœ¬ç­‰æ–¹é¢å¯èƒ½äº§ç”Ÿçš„å½±å“ï¼Œå¹¶æŒ‡å‡ºè¿™äº›æ–¹é¢çš„çŸ¥è¯†ç©ºç™½ã€‚</li>
<li>æ–‡ç« å»ºè®®ç®¡ç†é¡¾é—®å’Œåª’ä½“åœ¨å¡«è¡¥å…³äºGPTåœ¨å•†ä¸šåº”ç”¨ä¸­çš„çŸ¥è¯†ç©ºç™½æ–¹é¢å‘æŒ¥æ›´å¤§çš„ä½œç”¨ã€‚</li>
<li>æ–‡ç« å¼ºè°ƒé’ˆå¯¹ç°æœ‰çš„å’Œå¯é¢„è§çš„ä¸äººå·¥æ™ºèƒ½ç›¸å…³çš„å•†ä¸šæŒ‘æˆ˜å¼€å±•å®é™…å·¥ä½œçš„é‡è¦æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.05273">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-d053411c963af81db56c19c576ae5cbe.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-2d065c4bd5d37e432ebab3aef67977a4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-11a1620dee6920b62743f832fa597765.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3aa83805ccf6b01558d651bdce20416f.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Do-PhD-level-LLMs-Truly-Grasp-Elementary-Addition-Probing-Rule-Learning-vs-Memorization-in-Large-Language-Models"><a href="#Do-PhD-level-LLMs-Truly-Grasp-Elementary-Addition-Probing-Rule-Learning-vs-Memorization-in-Large-Language-Models" class="headerlink" title="Do PhD-level LLMs Truly Grasp Elementary Addition? Probing Rule Learning   vs. Memorization in Large Language Models"></a>Do PhD-level LLMs Truly Grasp Elementary Addition? Probing Rule Learning   vs. Memorization in Large Language Models</h2><p><strong>Authors:Yang Yan, Yu Lu, Renjun Xu, Zhenzhong Lan</strong></p>
<p>Despite high benchmark scores, Large Language Models (LLMs) often fail simple problem, raising a critical question: Do LLMs learn mathematical principles or merely memorize patterns? Rather than designing increasingly complex benchmarks like recent works, we investigate this using elementary two-integer addition ($0$ to $2^{64}$), probing two core properties: commutativity ($A+B&#x3D;B+A$) and compositional generalization (via isomorphic symbolic mappings, e.g., $7 \rightarrow y$). While state-of-the-art LLMs achieve 73.8-99.8% accuracy on numerical addition, performance collapses to $\leq$7.5% under symbolic mapping, indicating failure to generalize learned rules. Non-monotonic performance scaling with digit count and frequent commutativity violations (over 1,700 cases of $A+B \neq B+A$) further support this. Explicitly providing addition rules degrades performance by 81.2% on average, while self-explanation maintains baseline accuracy, suggesting LLM arithmetic processing is misaligned with human-defined principles. Our findings indicate current LLMs rely on memory pattern over genuine rule learning, highlighting architectural limitations and the need for new approaches to achieve true mathematical reasoning. </p>
<blockquote>
<p>å°½ç®¡å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„åŸºå‡†æµ‹è¯•å¾—åˆ†å¾ˆé«˜ï¼Œä½†å®ƒä»¬å¾€å¾€æ— æ³•è§£å†³ç®€å•çš„é—®é¢˜ï¼Œè¿™å¼•å‘äº†ä¸€ä¸ªå…³é”®é—®é¢˜ï¼šLLMæ˜¯å­¦ä¹ æ•°å­¦åŸç†è¿˜æ˜¯ä»…ä»…è®°å¿†æ¨¡å¼ï¼Ÿæˆ‘ä»¬å¹¶æœªé‡‡ç”¨æœ€è¿‘å·¥ä½œä¸­æ—¥ç›Šå¤æ‚çš„åŸºå‡†æµ‹è¯•æ¥æ¢ç©¶è¿™ä¸ªé—®é¢˜ï¼Œè€Œæ˜¯ä½¿ç”¨åŸºæœ¬çš„ä¸¤æ•´æ•°åŠ æ³•ï¼ˆä»0åˆ°2^64ï¼‰ï¼Œæ¢ç©¶äº†ä¸¤ä¸ªæ ¸å¿ƒå±æ€§ï¼šäº¤æ¢å¾‹ï¼ˆA+B&#x3D;B+Aï¼‰å’Œç»„åˆæ³›åŒ–ï¼ˆé€šè¿‡åŒæ„ç¬¦å·æ˜ å°„ï¼Œä¾‹å¦‚7â†’yï¼‰ã€‚è™½ç„¶æœ€æ–°çš„å¤§å‹è¯­è¨€æ¨¡å‹åœ¨æ•°å­—åŠ æ³•æ–¹é¢è¾¾åˆ°äº†73.8-99.8%çš„å‡†ç¡®ç‡ï¼Œä½†åœ¨ç¬¦å·æ˜ å°„ä¸‹ï¼Œæ€§èƒ½ä¸‹é™åˆ°â‰¤7.5%ï¼Œè¡¨æ˜æ— æ³•æ³›åŒ–å­¦ä¹ åˆ°çš„è§„åˆ™ã€‚éšç€æ•°å­—è®¡æ•°çš„éå•è°ƒæ€§èƒ½ç¼©æ”¾ä»¥åŠé¢‘ç¹çš„äº¤æ¢å¾‹è¿è§„ï¼ˆè¶…è¿‡1700ä¸ªA+Bâ‰ B+Açš„æƒ…å†µï¼‰ï¼Œè¿›ä¸€æ­¥è¯å®äº†è¿™ä¸€ç‚¹ã€‚æ˜ç¡®æä¾›åŠ æ³•è§„åˆ™å¹³å‡é™ä½äº†81.2%çš„æ€§èƒ½ï¼Œè€Œè‡ªæˆ‘è§£é‡Šåˆ™èƒ½ä¿æŒåŸºçº¿å‡†ç¡®ç‡ï¼Œè¿™è¡¨æ˜å¤§å‹è¯­è¨€æ¨¡å‹çš„ç®—æœ¯å¤„ç†ä¸äººç±»å®šä¹‰çš„åŸç†å­˜åœ¨åå·®ã€‚æˆ‘ä»¬çš„ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œå½“å‰çš„å¤§å‹è¯­è¨€æ¨¡å‹ä¾èµ–äºè®°å¿†æ¨¡å¼è€ŒéçœŸæ­£çš„è§„åˆ™å­¦ä¹ ï¼Œè¿™çªæ˜¾äº†æ¶æ„çš„å±€é™æ€§ä»¥åŠå®ç°çœŸæ­£æ•°å­¦æ¨ç†çš„éœ€è¦ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.05262v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œä½†åœ¨é¢å¯¹ç®€å•é—®é¢˜æ—¶å¸¸å¸¸å¤±æ•ˆã€‚æœ¬æ–‡é€šè¿‡ç ”ç©¶åŸºæœ¬çš„ä¸¤æ•´æ•°åŠ æ³•ï¼ˆä»0åˆ°2^64ï¼‰ä¸­çš„ä¸¤ä¸ªæ ¸å¿ƒå±æ€§ï¼šäº¤æ¢å¾‹ï¼ˆA+B&#x3D;B+Aï¼‰å’Œç»„åˆæ³›åŒ–èƒ½åŠ›ï¼ˆé€šè¿‡åŒæ„ç¬¦å·æ˜ å°„ï¼‰ï¼Œå‘ç°LLMåœ¨ç¬¦å·æ˜ å°„ä¸‹çš„æ€§èƒ½æ€¥å‰§ä¸‹é™åˆ°â‰¤7.5%ï¼Œè¡¨æ˜å…¶æ— æ³•æ³›åŒ–å­¦ä¹ åˆ°çš„è§„åˆ™ã€‚åŒæ—¶å‘ç°éå•è°ƒæ€§èƒ½éšæ•°å­—è®¡æ•°å˜åŒ–ä»¥åŠé¢‘ç¹è¿åäº¤æ¢å¾‹ï¼ˆè¶…è¿‡1700ä¸ªæ¡ˆä¾‹ï¼‰ã€‚æä¾›åŠ æ³•è§„åˆ™ä¼šå¹³å‡é™ä½æ€§èƒ½81.2%ï¼Œè€Œè‡ªæˆ‘è§£é‡Šåˆ™èƒ½ç»´æŒåŸºå‡†å‡†ç¡®æ€§ï¼Œæš—ç¤ºLLMçš„æ•°å­¦å¤„ç†ä¸äººä¸ºå®šä¹‰çš„åŸåˆ™ä¸ç¬¦ã€‚æœ¬ç ”ç©¶æ˜¾ç¤ºï¼Œå½“å‰LLMä¾èµ–äºè®°å¿†æ¨¡å¼è€ŒéçœŸæ­£çš„è§„åˆ™å­¦ä¹ ï¼Œçªæ˜¾äº†æ¶æ„çš„å±€é™æ€§ï¼Œå¹¶éœ€è¦æ–°æ–¹æ³•æ¥å®ç°çœŸæ­£çš„æ•°å­¦æ¨ç†ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°è‰¯å¥½ï¼Œä½†åœ¨ç®€å•çš„æ•°å­¦é—®é¢˜ä¸­å¸¸è¡¨ç°ä¸ä½³ã€‚</li>
<li>é€šè¿‡ç ”ç©¶åŸºæœ¬çš„ä¸¤æ•´æ•°åŠ æ³•ï¼Œå‘ç°LLMåœ¨ç¬¦å·æ˜ å°„ä¸‹çš„æ€§èƒ½æ€¥å‰§ä¸‹é™ï¼Œè¡¨æ˜å…¶æ— æ³•æ³›åŒ–å­¦ä¹ åˆ°çš„æ•°å­¦è§„åˆ™ã€‚</li>
<li>LLMåœ¨å¤„ç†æ•°å­¦é—®é¢˜æ—¶å­˜åœ¨éå•è°ƒæ€§èƒ½å˜åŒ–ï¼Œä¸”é¢‘ç¹è¿åæ•°å­¦åŸåˆ™ï¼ˆå¦‚äº¤æ¢å¾‹ï¼‰ã€‚</li>
<li>æä¾›æ˜ç¡®çš„æ•°å­¦è§„åˆ™ä¼šå¤§å¹…é™ä½LLMçš„æ€§èƒ½ï¼Œè€Œè‡ªæˆ‘è§£é‡Šåˆ™èƒ½ç»´æŒå…¶åŸºå‡†å‡†ç¡®æ€§ã€‚</li>
<li>LLMçš„æ•°å­¦å¤„ç†èƒ½åŠ›ä¸äººç±»å®šä¹‰çš„åŸåˆ™å­˜åœ¨åå·®ã€‚</li>
<li>å½“å‰LLMä¸»è¦ä¾èµ–è®°å¿†æ¨¡å¼è€ŒéçœŸæ­£çš„è§„åˆ™å­¦ä¹ ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.05262">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-b1871cc0b53305f7cd3d00e8d051c4d4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2c8617d79078a6b23ac58d1372275c2f.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-edeca4edb9158b0fb5f4a68d4635e77e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-51f07887c43c8fddd488a4649efc665a.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-2cc890363738b960e955346eced28615.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="How-to-evaluate-control-measures-for-LLM-agents-A-trajectory-from-today-to-superintelligence"><a href="#How-to-evaluate-control-measures-for-LLM-agents-A-trajectory-from-today-to-superintelligence" class="headerlink" title="How to evaluate control measures for LLM agents? A trajectory from today   to superintelligence"></a>How to evaluate control measures for LLM agents? A trajectory from today   to superintelligence</h2><p><strong>Authors:Tomek Korbak, Mikita Balesni, Buck Shlegeris, Geoffrey Irving</strong></p>
<p>As LLM agents grow more capable of causing harm autonomously, AI developers will rely on increasingly sophisticated control measures to prevent possibly misaligned agents from causing harm. AI developers could demonstrate that their control measures are sufficient by running control evaluations: testing exercises in which a red team produces agents that try to subvert control measures. To ensure control evaluations accurately capture misalignment risks, the affordances granted to this red team should be adapted to the capability profiles of the agents to be deployed under control measures.   In this paper we propose a systematic framework for adapting affordances of red teams to advancing AI capabilities. Rather than assuming that agents will always execute the best attack strategies known to humans, we demonstrate how knowledge of an agentsâ€™s actual capability profile can inform proportional control evaluations, resulting in more practical and cost-effective control measures. We illustrate our framework by considering a sequence of five fictional models (M1-M5) with progressively advanced capabilities, defining five distinct AI control levels (ACLs). For each ACL, we provide example rules for control evaluation, control measures, and safety cases that could be appropriate. Finally, we show why constructing a compelling AI control safety case for superintelligent LLM agents will require research breakthroughs, highlighting that we might eventually need alternative approaches to mitigating misalignment risk. </p>
<blockquote>
<p>éšç€LLMä»£ç†é€ æˆè‡ªä¸»ä¼¤å®³çš„èƒ½åŠ›ä¸æ–­å¢å¼ºï¼ŒAIå¼€å‘è€…å°†ä¾èµ–è¶Šæ¥è¶Šå¤æ‚çš„æ§åˆ¶æªæ–½æ¥é˜²æ­¢å¯èƒ½çš„å¯¹é½ä¸å½“çš„ä»£ç†é€ æˆä¼¤å®³ã€‚AIå¼€å‘è€…å¯ä»¥é€šè¿‡è¿è¡Œæ§åˆ¶è¯„ä¼°æ¥è¯æ˜å…¶æ§åˆ¶æªæ–½æ˜¯è¶³å¤Ÿçš„ï¼šæµ‹è¯•æ¼”ä¹ ä¸­ï¼Œçº¢é˜Ÿäº§ç”Ÿä»£ç†è¯•å›¾ç ´åæ§åˆ¶æªæ–½ã€‚ä¸ºäº†ç¡®ä¿æ§åˆ¶è¯„ä¼°å‡†ç¡®æ•æ‰é”™ä½é£é™©ï¼Œç»™äºˆçº¢é˜Ÿçš„æƒé™åº”è¯¥é€‚åº”äºåœ¨æ§åˆ¶æªæ–½ä¸‹è¦éƒ¨ç½²çš„ä»£ç†çš„èƒ½åŠ›ç‰¹å¾ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªç³»ç»Ÿçš„æ¡†æ¶ï¼Œä»¥é€‚åº”çº¢é˜Ÿæƒé™ä¸AIèƒ½åŠ›çš„è¿›æ­¥ã€‚æˆ‘ä»¬å¹¶ä¸å‡è®¾ä»£ç†æ€»æ˜¯ä¼šæ‰§è¡Œäººç±»å·²çŸ¥çš„æœ€ä½³æ”»å‡»ç­–ç•¥ï¼Œè€Œæ˜¯å±•ç¤ºäº†äº†è§£ä»£ç†çš„å®é™…èƒ½åŠ›ç‰¹å¾å¦‚ä½•æ¨åŠ¨æ¯”ä¾‹æ§åˆ¶è¯„ä¼°ï¼Œä»è€Œå¯¼è‡´æ›´å®ç”¨ã€æ›´ç»æµçš„æ§åˆ¶æªæ–½ã€‚æˆ‘ä»¬é€šè¿‡è€ƒè™‘ä¸€ç³»åˆ—å…·æœ‰æ¸è¿›èƒ½åŠ›çš„äº”ä¸ªè™šæ„æ¨¡å‹ï¼ˆM1-M5ï¼‰æ¥è¯´æ˜æˆ‘ä»¬çš„æ¡†æ¶ï¼Œå®šä¹‰äº”ä¸ªä¸åŒçš„AIæ§åˆ¶çº§åˆ«ï¼ˆACLï¼‰ã€‚å¯¹äºæ¯ä¸ªACLï¼Œæˆ‘ä»¬æä¾›æ§åˆ¶è¯„ä¼°ã€æ§åˆ¶æªæ–½å’Œå®‰å…¨æ¡ˆä¾‹çš„ç¤ºä¾‹è§„åˆ™ï¼Œè¿™äº›è§„åˆ™å¯èƒ½æ˜¯é€‚å½“çš„ã€‚æœ€åï¼Œæˆ‘ä»¬å±•ç¤ºäº†ä¸ºä»€ä¹ˆä¸ºè¶…çº§æ™ºèƒ½LLMä»£ç†æ„å»ºæœ‰è¯´æœåŠ›çš„AIæ§åˆ¶å®‰å…¨æ¡ˆä¾‹å°†éœ€è¦ç ”ç©¶çªç ´ï¼Œå¹¶å¼ºè°ƒæˆ‘ä»¬å¯èƒ½æœ€ç»ˆéœ€è¦é‡‡ç”¨æ›¿ä»£æ–¹æ³•æ¥å‡è½»é”™ä½é£é™©ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.05259v1">PDF</a> </p>
<p><strong>Summary</strong>ï¼šéšç€LLMä»£ç†ä¼¤å®³è‡ªä¸»èƒ½åŠ›çš„å¢å¼ºï¼ŒAIå¼€å‘è€…å°†ä¾èµ–æ›´å…ˆè¿›çš„æ§åˆ¶æªæ–½æ¥é˜²æ­¢ä»£ç†å¯èƒ½çš„ä¸å¯¹é½é€ æˆä¼¤å®³ã€‚é€šè¿‡è¿è¡Œæ§åˆ¶è¯„ä¼°æ¥éªŒè¯å…¶æ§åˆ¶æªæ–½çš„æœ‰æ•ˆæ€§ï¼Œè€Œè¯„ä¼°ä¸­çš„çº¢é˜Ÿéœ€é€‚åº”éƒ¨ç½²çš„ä»£ç†çš„èƒ½åŠ›ç‰¹å¾ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ä¸ªé€‚åº”çº¢é˜Ÿèƒ½åŠ›ä¸AIèƒ½åŠ›å‘å±•çš„ç³»ç»Ÿæ¡†æ¶ï¼Œå¹¶å±•ç¤ºäº†å¦‚ä½•åˆ©ç”¨ä»£ç†çš„å®é™…èƒ½åŠ›ç‰¹å¾è¿›è¡Œæ¯”ä¾‹æ§åˆ¶è¯„ä¼°ï¼Œä»è€Œå®ç°æ›´å®ç”¨å’Œæˆæœ¬æ•ˆç›Šæ›´é«˜çš„æ§åˆ¶æªæ–½ã€‚é€šè¿‡è€ƒè™‘ä¸€ç³»åˆ—å…·æœ‰æ¸è¿›å…ˆè¿›èƒ½åŠ›ç‰¹å¾çš„è™šæ„æ¨¡å‹ï¼ˆM1-M5ï¼‰ï¼Œæœ¬æ–‡å®šä¹‰äº†äº”ä¸ªä¸åŒçš„AIæ§åˆ¶çº§åˆ«ï¼ˆACLï¼‰ï¼Œå¹¶ä¸ºæ¯ä¸ªACLæä¾›äº†æ§åˆ¶è¯„ä¼°è§„åˆ™ã€æ§åˆ¶æªæ–½å’Œå®‰å…¨æ¡ˆä¾‹çš„ç¤ºä¾‹ã€‚æœ€åï¼Œæœ¬æ–‡å¼ºè°ƒäº†ä¸ºè¶…çº§æ™ºèƒ½LLMä»£ç†æ„å»ºå¯é çš„AIæ§åˆ¶å®‰å…¨æ¡ˆä¾‹å°†éœ€è¦ç ”ç©¶çªç ´ï¼Œå¹¶æŒ‡å‡ºäº†å¯èƒ½éœ€è¦æ›¿ä»£æ–¹æ³•æ¥å‡è½»ä¸å¯¹é½é£é™©çš„åŸå› ã€‚</p>
<p><strong>Key Takeaways</strong>:</p>
<ul>
<li>éšç€LLMä»£ç†è‡ªä¸»ä¼¤å®³èƒ½åŠ›çš„å¢å¼ºï¼ŒAIå¼€å‘è€…éœ€è¦æ›´å…ˆè¿›çš„æ§åˆ¶æªæ–½æ¥é˜²æ­¢æ½œåœ¨é£é™©ã€‚</li>
<li>é€šè¿‡æ§åˆ¶è¯„ä¼°æ¥éªŒè¯æ§åˆ¶æªæ–½çš„æœ‰æ•ˆæ€§ï¼Œå…¶ä¸­çº¢é˜Ÿçš„é€‚åº”èƒ½åŠ›ä¸éƒ¨ç½²çš„ä»£ç†çš„èƒ½åŠ›ç‰¹å¾ç´§å¯†ç›¸å…³ã€‚</li>
<li>æå‡ºä¸€ä¸ªé€‚åº”çº¢é˜Ÿèƒ½åŠ›ä¸AIèƒ½åŠ›å‘å±•çš„ç³»ç»Ÿæ¡†æ¶ï¼Œè¿›è¡Œæ¯”ä¾‹æ§åˆ¶è¯„ä¼°ä»¥å®ç°æ›´å®ç”¨çš„æ§åˆ¶æªæ–½ã€‚</li>
<li>é€šè¿‡äº”ä¸ªè™šæ„æ¨¡å‹å±•ç¤ºäº†ä¸åŒçš„AIæ§åˆ¶çº§åˆ«ï¼ˆACLï¼‰ï¼Œå¹¶ä¸ºæ¯ä¸ªACLæä¾›äº†å…·ä½“çš„æ§åˆ¶è¯„ä¼°è§„åˆ™ã€æ§åˆ¶æªæ–½å’Œå®‰å…¨æ¡ˆä¾‹ç¤ºä¾‹ã€‚</li>
<li>AIæ§åˆ¶å®‰å…¨æ¡ˆä¾‹çš„æ„å»ºå¯¹äºè¶…çº§æ™ºèƒ½LLMä»£ç†å°¤ä¸ºé‡è¦ï¼Œéœ€è¦ç ”ç©¶çªç ´å’Œå¯èƒ½çš„æ›¿ä»£æ–¹æ³•æ¥å‡è½»ä¸å¯¹é½é£é™©ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.05259">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-b6d6980c6ad7342b183ad02287aadc1f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ef1928905d91bc329052becd08ede753.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-802fc741161f0ca7beba913c07c38fa5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-39863f18ca41a6bd26739f2f437fffb5.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Learning-to-Reason-Over-Time-Timeline-Self-Reflection-for-Improved-Temporal-Reasoning-in-Language-Models"><a href="#Learning-to-Reason-Over-Time-Timeline-Self-Reflection-for-Improved-Temporal-Reasoning-in-Language-Models" class="headerlink" title="Learning to Reason Over Time: Timeline Self-Reflection for Improved   Temporal Reasoning in Language Models"></a>Learning to Reason Over Time: Timeline Self-Reflection for Improved   Temporal Reasoning in Language Models</h2><p><strong>Authors:AdriÃ¡n Bazaga, Rexhina Blloshmi, Bill Byrne, AdriÃ  de Gispert</strong></p>
<p>Large Language Models (LLMs) have emerged as powerful tools for generating coherent text, understanding context, and performing reasoning tasks. However, they struggle with temporal reasoning, which requires processing time-related information such as event sequencing, durations, and inter-temporal relationships. These capabilities are critical for applications including question answering, scheduling, and historical analysis. In this paper, we introduce TISER, a novel framework that enhances the temporal reasoning abilities of LLMs through a multi-stage process that combines timeline construction with iterative self-reflection. Our approach leverages test-time scaling to extend the length of reasoning traces, enabling models to capture complex temporal dependencies more effectively. This strategy not only boosts reasoning accuracy but also improves the traceability of the inference process. Experimental results demonstrate state-of-the-art performance across multiple benchmarks, including out-of-distribution test sets, and reveal that TISER enables smaller open-source models to surpass larger closed-weight models on challenging temporal reasoning tasks. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å·²ç»æˆä¸ºç”Ÿæˆè¿è´¯æ–‡æœ¬ã€ç†è§£ä¸Šä¸‹æ–‡å’Œè¿›è¡Œæ¨ç†ä»»åŠ¡çš„å¼ºå¤§å·¥å…·ã€‚ç„¶è€Œï¼Œå®ƒä»¬åœ¨æ—¶é—´æ¨ç†æ–¹é¢å­˜åœ¨å›°éš¾ï¼Œæ—¶é—´æ¨ç†éœ€è¦å¤„ç†ä¸æ—¶é—´ç›¸å…³çš„ä¿¡æ¯ï¼Œå¦‚äº‹ä»¶åºåˆ—ã€æŒç»­æ—¶é—´å’Œæ—¶é—´å…³ç³»ã€‚è¿™äº›èƒ½åŠ›å¯¹äºé—®ç­”ã€æ—¥ç¨‹å®‰æ’å’Œå†å²åˆ†æç­‰åº”ç”¨è‡³å…³é‡è¦ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†TISERï¼Œè¿™æ˜¯ä¸€ä¸ªé€šè¿‡å¤šé˜¶æ®µè¿‡ç¨‹å¢å¼ºLLMçš„æ—¶é—´æ¨ç†èƒ½åŠ›çš„æ–°å‹æ¡†æ¶ï¼Œè¯¥è¿‡ç¨‹ç»“åˆæ—¶é—´çº¿æ„å»ºå’Œè¿­ä»£è‡ªæˆ‘åæ€ã€‚æˆ‘ä»¬çš„æ–¹æ³•åˆ©ç”¨æµ‹è¯•æ—¶ç¼©æ”¾æ¥å»¶é•¿æ¨ç†ç—•è¿¹çš„é•¿åº¦ï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿæ›´æœ‰æ•ˆåœ°æ•è·å¤æ‚çš„æ—¶åºä¾èµ–å…³ç³»ã€‚è¿™ä¸€ç­–ç•¥ä¸ä»…æé«˜äº†æ¨ç†å‡†ç¡®æ€§ï¼Œè¿˜æé«˜äº†æ¨ç†è¿‡ç¨‹çš„å¯è¿½æº¯æ€§ã€‚å®éªŒç»“æœè¯æ˜ï¼Œåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸Šè¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼ŒåŒ…æ‹¬è¶…å‡ºåˆ†å¸ƒæµ‹è¯•é›†çš„æ€§èƒ½ï¼Œå¹¶ä¸”è¡¨æ˜TISERä½¿è¾ƒå°çš„å¼€æºæ¨¡å‹èƒ½å¤Ÿåœ¨å…·æœ‰æŒ‘æˆ˜æ€§çš„æ—¶é—´æ¨ç†ä»»åŠ¡ä¸Šè¶…è¶Šè¾ƒå¤§çš„å°é—­æƒé‡æ¨¡å‹ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.05258v1">PDF</a> </p>
<p><strong>Summary</strong><br>LLMé€šè¿‡TISERæ¡†æ¶æå‡æ—¶åºæ¨ç†èƒ½åŠ›ã€‚TISERç»“åˆæ—¶é—´çº¿æ„å»ºä¸è¿­ä»£è‡ªæˆ‘åæ€çš„å¤šé˜¶æ®µè¿‡ç¨‹ï¼Œé€šè¿‡æµ‹è¯•æ—¶ç¼©æ”¾å»¶é•¿æ¨ç†è½¨è¿¹ï¼Œæœ‰æ•ˆæ•æ‰å¤æ‚æ—¶åºä¾èµ–å…³ç³»ï¼Œä¸ä»…æé«˜æ¨ç†å‡†ç¡®æ€§ï¼Œè¿˜æ”¹å–„æ¨ç†è¿‡ç¨‹çš„å¯è¿½æº¯æ€§ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒTISERåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸Šè¡¨ç°å“è¶Šï¼Œç”šè‡³åœ¨æŒ‘æˆ˜æ€§æ—¶åºæ¨ç†ä»»åŠ¡ä¸­ï¼Œè¾ƒå°çš„å¼€æºæ¨¡å‹ä¹Ÿèƒ½è¶…è¶Šè¾ƒå¤§çš„å°é—­æƒé‡æ¨¡å‹ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLMé¢ä¸´æ—¶åºæ¨ç†æŒ‘æˆ˜ï¼Œéœ€è¦å¤„ç†æ—¶é—´ç›¸å…³ä¿¡æ¯å¦‚äº‹ä»¶åºåˆ—ã€æŒç»­æ—¶é—´å’Œæ—¶é—´å…³ç³»ã€‚</li>
<li>TISERæ¡†æ¶æ—¨åœ¨å¢å¼ºLLMçš„æ—¶åºæ¨ç†èƒ½åŠ›ï¼Œé€šè¿‡å¤šé˜¶æ®µè¿‡ç¨‹ç»“åˆæ—¶é—´çº¿æ„å»ºä¸è¿­ä»£è‡ªæˆ‘åæ€ã€‚</li>
<li>TISERåˆ©ç”¨æµ‹è¯•æ—¶ç¼©æ”¾å»¶é•¿æ¨ç†è½¨è¿¹ï¼Œä½¿æ¨¡å‹æ›´æœ‰æ•ˆåœ°æ•æ‰å¤æ‚æ—¶åºä¾èµ–å…³ç³»ã€‚</li>
<li>TISERä¸ä»…æé«˜äº†æ¨ç†å‡†ç¡®æ€§ï¼Œè€Œä¸”æ”¹å–„äº†æ¨ç†è¿‡ç¨‹çš„å¯è¿½æº¯æ€§ã€‚</li>
<li>å®éªŒç»“æœæ˜¾ç¤ºTISERåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸Šè¡¨ç°å“è¶Šã€‚</li>
<li>TISERä½¿å¾—è¾ƒå°çš„å¼€æºæ¨¡å‹åœ¨æŒ‘æˆ˜æ€§çš„æ—¶åºæ¨ç†ä»»åŠ¡ä¸­ä¹Ÿèƒ½è¡¨ç°å‡ºè‰²ï¼Œç”šè‡³è¶…è¶Šè¾ƒå¤§çš„å°é—­æƒé‡æ¨¡å‹ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.05258">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-6a4610db5cfa1b93993903eaeee03337.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-1d6f3f8c195b241546a73c58d51f2e92.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a187b7435ca923e1c73adbbc9aa89a76.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-1daea595de2bcda898c173f53c8d1f36.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2601bb03ffecc21e904b67cf5ac1b23e.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Unleashing-the-Power-of-LLMs-in-Dense-Retrieval-with-Query-Likelihood-Modeling"><a href="#Unleashing-the-Power-of-LLMs-in-Dense-Retrieval-with-Query-Likelihood-Modeling" class="headerlink" title="Unleashing the Power of LLMs in Dense Retrieval with Query Likelihood   Modeling"></a>Unleashing the Power of LLMs in Dense Retrieval with Query Likelihood   Modeling</h2><p><strong>Authors:Hengran Zhang, Keping Bi, Jiafeng Guo, Xiaojie Sun, Shihao Liu, Daiting Shi, Dawei Yin, Xueqi Cheng</strong></p>
<p>Dense retrieval is a crucial task in Information Retrieval (IR) and is the foundation for downstream tasks such as re-ranking. Recently, large language models (LLMs) have shown compelling semantic understanding capabilities and are appealing to researchers studying dense retrieval. LLMs, as decoder-style generative models, are competent at language generation while falling short on modeling global information due to the lack of attention to tokens afterward. Inspired by the classical word-based language modeling approach for IR, i.e., the query likelihood (QL) model, we seek to sufficiently utilize LLMsâ€™ generative ability by QL maximization. However, instead of ranking documents with QL estimation, we introduce an auxiliary task of QL maximization to yield a better backbone for contrastively learning a discriminative retriever. We name our model as LLM-QL. To condense global document semantics to a single vector during QL modeling, LLM-QL has two major components, Attention Stop (AS) and Input Corruption (IC). AS stops the attention of predictive tokens to previous tokens until the ending token of the document. IC masks a portion of tokens in the input documents during prediction. Experiments on MSMARCO show that LLM-QL can achieve significantly better performance than other LLM-based retrievers and using QL estimated by LLM-QL for ranking outperforms word-based QL by a large margin. </p>
<blockquote>
<p>å¯†é›†æ£€ç´¢æ˜¯ä¿¡æ¯æ£€ç´¢ï¼ˆIRï¼‰ä¸­çš„ä¸€é¡¹å…³é”®ä»»åŠ¡ï¼Œä¹Ÿæ˜¯æ’åºç­‰ä¸‹æ¸¸ä»»åŠ¡çš„åŸºç¡€ã€‚æœ€è¿‘ï¼Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ˜¾ç¤ºå‡ºä»¤äººä¿¡æœçš„è¯­ä¹‰ç†è§£èƒ½åŠ›ï¼Œå¸å¼•ç ”ç©¶äººå‘˜å¯¹å¯†é›†æ£€ç´¢è¿›è¡Œç ”ç©¶ã€‚LLMä½œä¸ºè§£ç å™¨é£æ ¼çš„ç”Ÿæˆæ¨¡å‹ï¼Œæ“…é•¿è¯­è¨€ç”Ÿæˆï¼Œä½†ç”±äºç¼ºä¹å¯¹æ ‡è®°çš„æ³¨æ„åŠ›ï¼Œåœ¨å»ºæ¨¡å…¨å±€ä¿¡æ¯æ–¹é¢è¡¨ç°ä¸è¶³ã€‚å—ä¿¡æ¯æ£€ç´¢ä¸­åŸºäºå•è¯çš„è¯­è¨€å»ºæ¨¡æ–¹æ³•ï¼ˆå³æŸ¥è¯¢å¯èƒ½æ€§ï¼ˆQLï¼‰æ¨¡å‹ï¼‰çš„å¯å‘ï¼Œæˆ‘ä»¬è¯•å›¾é€šè¿‡QLæœ€å¤§åŒ–å……åˆ†åˆ©ç”¨LLMçš„ç”Ÿæˆèƒ½åŠ›ã€‚ç„¶è€Œï¼Œæˆ‘ä»¬ä¸æ˜¯é€šè¿‡QLä¼°è®¡æ¥æ’åæ–‡æ¡£ï¼Œè€Œæ˜¯å¼•å…¥QLæœ€å¤§åŒ–çš„è¾…åŠ©ä»»åŠ¡ï¼Œä»¥äº§ç”Ÿå¯¹æ¯”å­¦ä¹ é‰´åˆ«æ£€ç´¢å™¨çš„æ›´å¥½ä¸»å¹²ã€‚æˆ‘ä»¬å°†æˆ‘ä»¬çš„æ¨¡å‹å‘½åä¸ºLLM-QLã€‚åœ¨QLå»ºæ¨¡è¿‡ç¨‹ä¸­ï¼Œä¸ºäº†ç²¾ç®€å…¨å±€æ–‡æ¡£è¯­ä¹‰ä¸ºå•ä¸€å‘é‡ï¼ŒLLM-QLæœ‰ä¸¤ä¸ªä¸»è¦ç»„æˆéƒ¨åˆ†ï¼šæ³¨æ„åŠ›åœæ­¢ï¼ˆASï¼‰å’Œè¾“å…¥è…èš€ï¼ˆICï¼‰ã€‚ASé˜»æ­¢é¢„æµ‹æ ‡è®°å¯¹ä¹‹å‰æ ‡è®°çš„æ³¨æ„åŠ›ï¼Œç›´åˆ°æ–‡æ¡£ç»“æŸæ ‡è®°ã€‚ICåœ¨é¢„æµ‹è¿‡ç¨‹ä¸­ä¼šå±è”½è¾“å…¥æ–‡æ¡£ä¸­çš„éƒ¨åˆ†æ ‡è®°ã€‚åœ¨MSMARCOä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒLLM-QLå¯ä»¥å–å¾—æ¯”å…¶ä»–LLMæ£€ç´¢å™¨æ›´å¥½çš„æ€§èƒ½ï¼Œå¹¶ä¸”ä½¿ç”¨LLM-QLä¼°è®¡çš„QLè¿›è¡Œæ’åºæ¯”åŸºäºå•è¯çš„QLæœ‰å¤§å¹…åº¦æå‡ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.05216v1">PDF</a> 12 pages, 3 figures</p>
<p><strong>æ‘˜è¦</strong><br>LLMçš„é‡è¦ä»»åŠ¡æ˜¯å¯†é›†æ£€ç´¢åœ¨ä¿¡æ¯æ£€ç´¢ï¼ˆIRï¼‰é¢†åŸŸå¯†é›†æ£€ç´¢ä½œä¸ºä¸‹æ¸¸ä»»åŠ¡å¦‚é‡æ–°æ’åºçš„åŸºç¡€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å±•ç°å‡ºä»¤äººä¿¡æœçš„è¯­ä¹‰ç†è§£åŠ›ã€‚å—ä¼ ç»ŸåŸºäºè¯çš„IRè¯­è¨€å»ºæ¨¡æ–¹æ³•ï¼ˆå³æŸ¥è¯¢å¯èƒ½æ€§æ¨¡å‹ï¼‰çš„å¯å‘ï¼Œè¯•å›¾å……åˆ†åˆ©ç”¨LLMçš„ç”Ÿæˆèƒ½åŠ›ï¼Œå¹¶å¼•å…¥äº†åä¸ºLLM-QLçš„è¾…åŠ©ä»»åŠ¡æ¥æé«˜å…¶å¯†é›†æ£€ç´¢èƒ½åŠ›ã€‚æ¨¡å‹å…·æœ‰æ³¨æ„åŠ›åœæ­¢ï¼ˆASï¼‰å’Œè¾“å…¥æŸåï¼ˆICï¼‰ä¸¤å¤§è¦ç´ æ¥å®ç°åŸºäºæŸ¥è¯¢å¯èƒ½æ€§ï¼ˆQLï¼‰çš„å…¨å±€æ–‡æ¡£è¯­ä¹‰è¡¨ç¤ºå’Œç²¾ç®€åŒ–å®éªŒç»“æœæ˜¾ç¤ºï¼ŒLLM-QLåœ¨MSMARCOæ•°æ®é›†ä¸Šçš„æ€§èƒ½ä¼˜äºå…¶ä»–åŸºäºLLMçš„æ£€ç´¢å™¨ï¼Œå¹¶ä¸”ä½¿ç”¨LLM-QLä¼°è®¡çš„QLè¿›è¡Œæ’åç»“æœä¹Ÿæ˜¾è‘—ä¼˜äºåŸºäºå•è¯çš„QLã€‚ç®€è¨€ä¹‹ï¼Œå€ŸåŠ©å¤§å‹è¯­è¨€æ¨¡å‹çš„å¸®åŠ©æ¥æå‡åŸºäºè¯çš„æŸ¥è¯¢å¯èƒ½æ€§æ¨¡å‹çš„å¯†é›†æ£€ç´¢èƒ½åŠ›å¹¶æ”¹å–„æ’åºç»“æœã€‚å…·æœ‰åˆ›æ–°æ€§å’Œæœ‰æ•ˆæ€§çš„æ–°ç­–ç•¥å’Œæ–¹æ³•è¢«æå‡ºã€‚æ€»çš„æ¥è¯´ï¼Œæœ¬æ–‡ç ”ç©¶äº†å¤§å‹è¯­è¨€æ¨¡å‹åœ¨å¯†é›†æ£€ç´¢ä»»åŠ¡ä¸­çš„åº”ç”¨åŠå…¶æ”¹è¿›ç­–ç•¥ï¼Œå¹¶åœ¨å®éªŒä¸­è¯æ˜äº†å…¶ä¼˜è¶Šæ€§ã€‚ç‰¹åˆ«æ˜¯æ‰€æå‡ºçš„è¾…åŠ©ä»»åŠ¡æ–¹æ³•èƒ½å¤Ÿæé«˜åŸºäºæ³¨æ„åŠ›æœºåˆ¶çš„æ–‡æ¡£å‘é‡è¡¨ç¤ºçš„è¯­ä¹‰ä¸°å¯Œæ€§å¹¶æœ‰åŠ©äºä¸‹æ¸¸ä»»åŠ¡çš„æ•ˆæœæå‡ã€‚è¿™ç§å°†å…¨å±€æ–‡æ¡£è¯­ä¹‰è½¬åŒ–ä¸ºå•ä¸ªå‘é‡çš„æ–¹æ³•åœ¨æ•°æ®å¯†é›†å‹åº”ç”¨å¦‚æœç´¢å¼•æ“å’Œæ–‡æœ¬å¤„ç†ä¸­æœ‰å¾ˆå¥½çš„åº”ç”¨å‰æ™¯ã€‚è¿™ä¹Ÿåæ˜ äº†è‡ªç„¶è¯­è¨€å¤„ç†å’Œäººå·¥æ™ºèƒ½åœ¨æ¨è¿›æŠ€æœ¯è¿›æ­¥ä¸­çš„é‡è¦æ€§ã€‚<strong>å…³é”®å‘ç°</strong>ï¼š</p>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.05216">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-982adbc5929d6f645ea950779f581ffb.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9a257e2184f11fd6b0c690e6175df201.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2081499e8b39caa2053c7a3b27375460.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-1b5d308b0a1038c6c7c249d6af53f085.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8671ad1d3f99674f3b267bdd1c389c42.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="PanoDreamer-Consistent-Text-to-360-Degree-Scene-Generation"><a href="#PanoDreamer-Consistent-Text-to-360-Degree-Scene-Generation" class="headerlink" title="PanoDreamer: Consistent Text to 360-Degree Scene Generation"></a>PanoDreamer: Consistent Text to 360-Degree Scene Generation</h2><p><strong>Authors:Zhexiao Xiong, Zhang Chen, Zhong Li, Yi Xu, Nathan Jacobs</strong></p>
<p>Automatically generating a complete 3D scene from a text description, a reference image, or both has significant applications in fields like virtual reality and gaming. However, current methods often generate low-quality textures and inconsistent 3D structures. This is especially true when extrapolating significantly beyond the field of view of the reference image. To address these challenges, we propose PanoDreamer, a novel framework for consistent, 3D scene generation with flexible text and image control. Our approach employs a large language model and a warp-refine pipeline, first generating an initial set of images and then compositing them into a 360-degree panorama. This panorama is then lifted into 3D to form an initial point cloud. We then use several approaches to generate additional images, from different viewpoints, that are consistent with the initial point cloud and expand&#x2F;refine the initial point cloud. Given the resulting set of images, we utilize 3D Gaussian Splatting to create the final 3D scene, which can then be rendered from different viewpoints. Experiments demonstrate the effectiveness of PanoDreamer in generating high-quality, geometrically consistent 3D scenes. </p>
<blockquote>
<p>ä»æ–‡æœ¬æè¿°æˆ–å‚è€ƒå›¾åƒï¼Œæˆ–ä¸¤è€…ç»“åˆè‡ªåŠ¨ç”Ÿæˆå®Œæ•´çš„3Dåœºæ™¯ï¼Œåœ¨è™šæ‹Ÿç°å®å’Œæ¸¸æˆç­‰é¢†åŸŸå…·æœ‰å¹¿æ³›åº”ç”¨ã€‚ç„¶è€Œï¼Œå½“å‰çš„æ–¹æ³•å¾€å¾€ç”Ÿæˆä½è´¨é‡çš„çº¹ç†å’Œä¸ä¸€è‡´çš„3Dç»“æ„ã€‚å½“è¶…å‡ºå‚è€ƒå›¾åƒçš„è§†é‡èŒƒå›´è¿›è¡Œæ¨æ–­æ—¶ï¼Œå°¤å…¶å¦‚æ­¤ã€‚ä¸ºäº†åº”å¯¹è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†PanoDreamerï¼Œè¿™æ˜¯ä¸€ç§ç”¨äºä¸€è‡´ã€çµæ´»çš„æ–‡æœ¬å’Œå›¾åƒæ§åˆ¶çš„3Dåœºæ™¯ç”Ÿæˆçš„æ–°å‹æ¡†æ¶ã€‚æˆ‘ä»¬çš„æ–¹æ³•é‡‡ç”¨å¤§å‹è¯­è¨€æ¨¡å‹å’Œæ‰­æ›²ç»†åŒ–ç®¡é“ï¼Œé¦–å…ˆç”Ÿæˆä¸€ç»„åˆå§‹å›¾åƒï¼Œç„¶åå°†å®ƒä»¬åˆæˆä¸€ä¸ª360åº¦çš„å…¨æ™¯å›¾ã€‚éšåå°†æ­¤å…¨æ™¯å›¾æå‡åˆ°ä¸‰ç»´å½¢æˆåˆå§‹ç‚¹äº‘ã€‚ç„¶åï¼Œæˆ‘ä»¬ä½¿ç”¨å‡ ç§æ–¹æ³•ä»ä¸åŒçš„è§†ç‚¹ç”Ÿæˆä¸åˆå§‹ç‚¹äº‘ä¸€è‡´çš„é¢å¤–å›¾åƒï¼Œå¹¶æ‰©å±•æˆ–ç»†åŒ–åˆå§‹ç‚¹äº‘ã€‚ç»™å®šç”Ÿæˆçš„å›¾åƒé›†ï¼Œæˆ‘ä»¬ä½¿ç”¨ä¸‰ç»´é«˜æ–¯é£æº…æŠ€æœ¯åˆ›å»ºæœ€ç»ˆçš„3Dåœºæ™¯ï¼Œç„¶åå¯ä»¥ä»ä¸åŒçš„è§†ç‚¹è¿›è¡Œæ¸²æŸ“ã€‚å®éªŒè¡¨æ˜ï¼ŒPanoDreameråœ¨ç”Ÿæˆé«˜è´¨é‡ã€å‡ ä½•ä¸€è‡´çš„3Dåœºæ™¯æ–¹é¢éå¸¸æœ‰æ•ˆã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.05152v1">PDF</a> Accepted by CVPR 2025 Workshop on Computer Vision for Metaverse</p>
<p><strong>Summary</strong><br>æ–‡æœ¬æå‡ºäº†ä¸€ç§åä¸ºPanoDreamerçš„æ–°æ¡†æ¶ï¼Œç”¨äºæ ¹æ®æ–‡æœ¬æè¿°ã€å‚è€ƒå›¾åƒæˆ–ä¸¤è€…è¿›è¡Œä¸€è‡´çš„3Dåœºæ™¯ç”Ÿæˆï¼Œå…·æœ‰çµæ´»æ€§å’Œæ§åˆ¶åŠ›ã€‚è¯¥æ¡†æ¶é‡‡ç”¨å¤§å‹è¯­è¨€æ¨¡å‹å’Œwarp-refineç®¡é“ï¼Œç”Ÿæˆåˆå§‹å›¾åƒå¹¶å°†å…¶ç»„åˆæˆå…¨æ™¯å›¾ï¼Œç„¶åå°†å…¶æå‡åˆ°ä¸‰ç»´å½¢æˆåˆå§‹ç‚¹äº‘ã€‚ç„¶åé‡‡ç”¨å¤šç§æ–¹æ³•ä»åˆå§‹ç‚¹äº‘ä¸€è‡´åœ°ç”Ÿæˆä¸åŒè§†è§’çš„å›¾åƒå¹¶å¯¹å…¶è¿›è¡Œæ‰©å±•&#x2F;ç»†åŒ–ã€‚æœ€ååˆ©ç”¨ä¸‰ç»´é«˜æ–¯æŠ€æœ¯åˆ›å»ºæœ€ç»ˆçš„3Dåœºæ™¯ï¼Œå¯ä»¥ä»ä¸åŒè§’åº¦è¿›è¡Œæ¸²æŸ“ã€‚è¯¥æ¡†æ¶è§£å†³äº†ç°æœ‰æ–¹æ³•ç”Ÿæˆä½è´¨é‡çº¹ç†å’Œå‡ ä½•ä¸ä¸€è‡´çš„é—®é¢˜ï¼Œç‰¹åˆ«æ˜¯åœ¨å‚è€ƒå›¾åƒè§†é‡ä¹‹å¤–çš„é—®é¢˜ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>PanoDreameræ˜¯ä¸€ä¸ªæ–°é¢–çš„æ¡†æ¶ï¼Œæ—¨åœ¨ä»æ–‡æœ¬æè¿°ã€å‚è€ƒå›¾åƒæˆ–ä¸¤è€…ç»“åˆä¸­è‡ªåŠ¨ç”Ÿæˆä¸€è‡´çš„3Dåœºæ™¯ã€‚</li>
<li>è¯¥æ¡†æ¶åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹è¿›è¡Œåœºæ™¯æ„å»ºï¼Œå¹¶ä½¿ç”¨warp-refineç®¡é“ç”Ÿæˆåˆå§‹å›¾åƒå¹¶å°†å…¶ç»„åˆæˆå…¨æ™¯å›¾ã€‚</li>
<li>ç”Ÿæˆçš„åˆå§‹å…¨æ™¯å›¾è¢«è½¬æ¢ä¸ºä¸‰ç»´ç‚¹äº‘ï¼Œä¸ºåç»­çš„åœºæ™¯ç»†åŒ–æä¾›äº†åŸºç¡€ã€‚</li>
<li>é€šè¿‡å¤šç§æ–¹æ³•ä»åˆå§‹ç‚¹äº‘ç”Ÿæˆä¸€è‡´çš„ä¸åŒè§†è§’çš„å›¾åƒï¼Œå¹¶å¯¹å…¶è¿›è¡Œæ‰©å±•å’Œç»†åŒ–ã€‚</li>
<li>åˆ©ç”¨ä¸‰ç»´é«˜æ–¯æŠ€æœ¯åˆ›å»ºæœ€ç»ˆçš„3Dåœºæ™¯ï¼Œå…·æœ‰è‰¯å¥½çš„å‡ ä½•ä¸€è‡´æ€§ã€‚</li>
<li>è¯¥æ¡†æ¶è§£å†³äº†ç°æœ‰æ–¹æ³•ä¸­çš„ä½è´¨é‡çº¹ç†å’Œå‡ ä½•ä¸ä¸€è‡´æ€§é—®é¢˜ï¼Œç‰¹åˆ«æ˜¯åœ¨å‚è€ƒå›¾åƒè§†é‡ä¹‹å¤–çš„é—®é¢˜ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.05152">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-96820a96652e1b4e81f4a21511556126.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-edb66d36146d23caf2e0bb7d1f0c8570.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-30894dfa7d6b9a7a98332d9c642b46f4.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Revealing-the-Intrinsic-Ethical-Vulnerability-of-Aligned-Large-Language-Models"><a href="#Revealing-the-Intrinsic-Ethical-Vulnerability-of-Aligned-Large-Language-Models" class="headerlink" title="Revealing the Intrinsic Ethical Vulnerability of Aligned Large Language   Models"></a>Revealing the Intrinsic Ethical Vulnerability of Aligned Large Language   Models</h2><p><strong>Authors:Jiawei Lian, Jianhong Pan, Lefan Wang, Yi Wang, Shaohui Mei, Lap-Pui Chau</strong></p>
<p>Large language models (LLMs) are foundational explorations to artificial general intelligence, yet their alignment with human values via instruction tuning and preference learning achieves only superficial compliance. Here, we demonstrate that harmful knowledge embedded during pretraining persists as indelible â€œdark patternsâ€ in LLMsâ€™ parametric memory, evading alignment safeguards and resurfacing under adversarial inducement at distributional shifts. In this study, we first theoretically analyze the intrinsic ethical vulnerability of aligned LLMs by proving that current alignment methods yield only local â€œsafety regionsâ€ in the knowledge manifold. In contrast, pretrained knowledge remains globally connected to harmful concepts via high-likelihood adversarial trajectories. Building on this theoretical insight, we empirically validate our findings by employing semantic coherence inducement under distributional shiftsâ€“a method that systematically bypasses alignment constraints through optimized adversarial prompts. This combined theoretical and empirical approach achieves a 100% attack success rate across 19 out of 23 state-of-the-art aligned LLMs, including DeepSeek-R1 and LLaMA-3, revealing their universal vulnerabilities. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ˜¯äººå·¥æ™ºèƒ½é€šç”¨ç ”ç©¶çš„åŸºçŸ³æ¢ç´¢ï¼Œç„¶è€Œï¼Œé€šè¿‡æŒ‡ä»¤å¾®è°ƒåå¥½å­¦ä¹ å®ç°å…¶ä¸äººç±»ä»·å€¼çš„å¯¹é½ä»…è¾¾åˆ°è¡¨é¢ä¸Šçš„åˆè§„æ€§ã€‚åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬è¯æ˜é¢„è®­ç»ƒæœŸé—´åµŒå…¥çš„æœ‰å®³çŸ¥è¯†ä¼šä½œä¸ºä¸å¯ç£¨ç­çš„â€œæš—æ¨¡å¼â€æŒç»­å­˜åœ¨äºLLMçš„å‚æ•°è®°å¿†ä¸­ï¼Œé€ƒé¿å¯¹é½ä¿éšœæªæ–½ï¼Œå¹¶åœ¨åˆ†å¸ƒè½¬ç§»æ—¶å¯¹æŠ—è¯±å¯¼ä¸‹å†æ¬¡å‡ºç°ã€‚åœ¨æœ¬ç ”ç©¶ä¸­ï¼Œæˆ‘ä»¬é¦–å…ˆä»ç†è®ºä¸Šåˆ†æå¯¹é½LLMçš„å†…åœ¨é“å¾·è„†å¼±æ€§ï¼Œé€šè¿‡è¯æ˜ç°æœ‰çš„å¯¹é½æ–¹æ³•ä»…åœ¨çŸ¥è¯†æµå½¢ä¸­äº§ç”Ÿå±€éƒ¨â€œå®‰å…¨åŒºåŸŸâ€ã€‚ç›¸åï¼Œé¢„è®­ç»ƒçŸ¥è¯†ä»ä¸æœ‰å®³æ¦‚å¿µé€šè¿‡é«˜æ¦‚ç‡å¯¹æŠ—è½¨è¿¹å…¨å±€è¿æ¥ã€‚åŸºäºè¿™ä¸€ç†è®ºè§è§£ï¼Œæˆ‘ä»¬é€šè¿‡åˆ†å¸ƒè½¬ç§»ä¸‹çš„è¯­ä¹‰è¿è´¯è¯±å¯¼å®è¯éªŒè¯æˆ‘ä»¬çš„å‘ç°â€”â€”ä¸€ç§é€šè¿‡ä¼˜åŒ–å¯¹æŠ—æç¤ºæ¥ç³»ç»Ÿåœ°ç»•è¿‡å¯¹é½çº¦æŸçš„æ–¹æ³•ã€‚è¿™ç§ç»“åˆç†è®ºå’Œå®è¯çš„æ–¹æ³•åœ¨23ä¸ªæœ€æ–°å¯¹é½LLMä¸­çš„19ä¸ªä¸Šå®ç°äº†100%çš„æ”»å‡»æˆåŠŸç‡ï¼ŒåŒ…æ‹¬DeepSeek-R1å’ŒLLaMA-3ï¼Œæ­ç¤ºäº†å®ƒä»¬çš„æ™®éè„†å¼±æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.05050v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ˜¯äººå·¥æ™ºèƒ½é€šç”¨æ¢ç´¢çš„åŸºç¡€ï¼Œä½†é€šè¿‡æŒ‡ä»¤è°ƒæ•´å’Œåå¥½å­¦ä¹ å®ç°ä¸äººç±»ä»·å€¼çš„å¯¹é½ä»…è¾¾åˆ°è¡¨é¢ä¸Šçš„ç¬¦åˆã€‚ç ”ç©¶è¡¨æ˜ï¼Œé¢„è®­ç»ƒè¿‡ç¨‹ä¸­åµŒå…¥çš„æœ‰å®³çŸ¥è¯†ä¼šä½œä¸ºä¸å¯ç£¨ç­çš„â€œæš—æ¨¡å¼â€å­˜åœ¨äºLLMçš„å‚æ•°è®°å¿†ä¸­ï¼Œé€ƒé¿å¯¹é½ä¿éšœæªæ–½ï¼Œå¹¶åœ¨åˆ†å¸ƒè½¬ç§»æ—¶é€šè¿‡å¯¹æŠ—æ€§è¯±å¯¼é‡æ–°æµ®ç°ã€‚å½“å‰çš„ç†è®ºåˆ†ææ˜¾ç¤ºï¼Œå¯¹é½çš„LLMå­˜åœ¨å›ºæœ‰çš„é“å¾·è„†å¼±æ€§ï¼Œå› ä¸ºå½“å‰çš„å¯¹é½æ–¹æ³•ä»…åœ¨çŸ¥è¯†æµå½¢ä¸­äº§ç”Ÿå±€éƒ¨â€œå®‰å…¨åŒºåŸŸâ€ï¼Œè€Œé¢„è®­ç»ƒçŸ¥è¯†ä»ä¸æœ‰å®³æ¦‚å¿µé€šè¿‡é«˜æ¦‚ç‡å¯¹æŠ—è½¨è¿¹ä¿æŒå…¨çƒè”ç³»ã€‚æœ¬ç ”ç©¶é€šè¿‡åˆ†å¸ƒè½¬ç§»ä¸‹çš„è¯­ä¹‰è¿è´¯æ€§è¯±å¯¼è¿›è¡Œå®è¯éªŒè¯ï¼Œè¯¥æ–¹æ³•é€šè¿‡ä¼˜åŒ–å¯¹æŠ—æ€§æç¤ºæ¥ç³»ç»Ÿåœ°ç»•è¿‡å¯¹é½çº¦æŸã€‚è¿™ç§ç»“åˆç†è®ºå’Œå®è¯çš„æ–¹æ³•åœ¨23ç§æœ€å…ˆè¿›å¯¹é½çš„LLMä¸­æœ‰19ç§è¾¾åˆ°äº†ç™¾åˆ†ç™¾çš„æ”»å‡»æˆåŠŸç‡ï¼Œæ­ç¤ºäº†å…¶æ™®éå­˜åœ¨çš„æ¼æ´ã€‚æš—å«éšè—å±æœºçš„å¤§å‹è¯­è¨€æ¨¡å‹æ½œä¼å¯¹äººç±»æƒç›Šå¯èƒ½äº§ç”Ÿå½±å“åŠå®‰å…¨é—®é¢˜é£é™©ä¸å®¹å°è§‘ã€‚è¿™ä¸ä»…ä¸ºä¼¦ç†ä¸ç§‘æŠ€çš„å†²çªå†æ·»æ¡ˆä¾‹ä¹Ÿè®©æˆ‘ä»¬é¢ä¸´å·¨å¤§æŒ‘æˆ˜éœ€è¦æ¢è®¨ç›¸åº”æªæ–½æœ‰æ•ˆè§£å†³é¿å…ä¸¥é‡äº‹ä»¶å¦‚æ·±å­¦å†³ç­–ç§‘æŠ€ä»‹å…¥è¯¯ç”¨ç­‰å½±å“å…¨çƒä»·å€¼ç†å¿µå’Œå†³ç­–çš„é‡è¦å‘½é¢˜å†æ¬¡å¼•å‘äº†æ–°çš„æŒ‘æˆ˜ä¸æ€è€ƒæ–¹å‘ã€‚é€šè¿‡å¯¹æš—æ¨¡å¼æœºåˆ¶ä¸é˜²å¾¡ç­–ç•¥çš„æ·±å…¥ç ”ç©¶æˆ–èƒ½æœ‰æœ›ç ´è§£æ­¤ç±»é—®é¢˜å¯»æ±‚ä¼¦ç†ä¸æŠ€æœ¯çš„å’Œè°å…±å­˜ã€‚åœ¨äººå·¥æ™ºèƒ½æŠ€æœ¯å‘å±•çš„åŒæ—¶æ„å»ºå…¶å®‰å…¨å’Œå¯æŒç»­æ€§æ˜¯æœªæ¥ç ”ç©¶çš„é‡è¦æ–¹å‘ä¹‹ä¸€ã€‚æœ¬ç ”ç©¶çš„å‘ç°å°†æ¨åŠ¨äººå·¥æ™ºèƒ½é¢†åŸŸçš„å‘å±•è¿›ä¸€æ­¥æœç€æ›´åŠ å®‰å…¨å’Œå¯é çš„æ–¹å‘å‘å±•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLMså­˜åœ¨å›ºæœ‰çš„é“å¾·è„†å¼±æ€§ï¼Œå³ä½¿ç»è¿‡æŒ‡ä»¤è°ƒæ•´å’Œåå¥½å­¦ä¹ ä¸äººç±»ä»·å€¼å¯¹é½ï¼Œä»æ— æ³•å®Œå…¨é¿å…æœ‰å®³çŸ¥è¯†çš„å­˜åœ¨ã€‚</li>
<li>æœ‰å®³çŸ¥è¯†åœ¨LLMçš„å‚æ•°è®°å¿†ä¸­ä»¥â€œæš—æ¨¡å¼â€å½¢å¼å­˜åœ¨ï¼Œé€ƒé¿å¯¹é½ä¿éšœæªæ–½ï¼Œå¹¶åœ¨ç‰¹å®šæƒ…å¢ƒä¸‹é‡æ–°æµ®ç°ã€‚</li>
<li>å½“å‰çš„å¯¹é½æ–¹æ³•ä»…åœ¨çŸ¥è¯†æµå½¢ä¸­äº§ç”Ÿå±€éƒ¨â€œå®‰å…¨åŒºåŸŸâ€ï¼Œé¢„è®­ç»ƒçŸ¥è¯†ä¸æœ‰å®³æ¦‚å¿µä¹‹é—´ä¿æŒå…¨çƒè”ç³»ã€‚</li>
<li>é€šè¿‡åˆ†å¸ƒè½¬ç§»ä¸‹çš„è¯­ä¹‰è¿è´¯æ€§è¯±å¯¼æ–¹æ³•ï¼Œå®è¯éªŒè¯äº†LLMsçš„æ™®éæ¼æ´ï¼Œæ”»å‡»æˆåŠŸç‡è¾¾ç™¾åˆ†ä¹‹ç™¾ã€‚</li>
<li>LLMså¯¹äººç±»æƒç›Šå’Œå®‰å…¨çš„å½±å“ä¸å¯å°è§‘ï¼Œéœ€è¦æ·±å…¥æ¢è®¨ç›¸åº”æªæ–½ä»¥åº”å¯¹æ½œåœ¨é£é™©ã€‚</li>
<li>ç ”ç©¶æš—æ¨¡å¼æœºåˆ¶ä¸é˜²å¾¡ç­–ç•¥æœ‰æœ›ç ´è§£ä¼¦ç†ä¸æŠ€æœ¯çš„å†²çªï¼Œå¯»æ±‚äºŒè€…çš„å’Œè°å…±å­˜ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.05050">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-24ab0dc7ddfb580cf39f70de0edefedf.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-dae67f5f70d37aa2f842ae1248c33c12.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="SAFT-Structure-aware-Transformers-for-Textual-Interaction-Classification"><a href="#SAFT-Structure-aware-Transformers-for-Textual-Interaction-Classification" class="headerlink" title="SAFT: Structure-aware Transformers for Textual Interaction   Classification"></a>SAFT: Structure-aware Transformers for Textual Interaction   Classification</h2><p><strong>Authors:Hongtao Wang, Renchi Yang, Hewen Wang, Haoran Zheng, Jianliang Xu</strong></p>
<p>Textual interaction networks (TINs) are an omnipresent data structure used to model the interplay between users and items on e-commerce websites, social networks, etc., where each interaction is associated with a text description. Classifying such textual interactions (TIC) finds extensive use in detecting spam reviews in e-commerce, fraudulent transactions in finance, and so on. Existing TIC solutions either (i) fail to capture the rich text semantics due to the use of context-free text embeddings, and&#x2F;or (ii) disregard the bipartite structure and node heterogeneity of TINs, leading to compromised TIC performance. In this work, we propose SAFT, a new architecture that integrates language- and graph-based modules for the effective fusion of textual and structural semantics in the representation learning of interactions. In particular, line graph attention (LGA)&#x2F;gated attention units (GAUs) and pretrained language models (PLMs) are capitalized on to model the interaction-level and token-level signals, which are further coupled via the proxy token in an iterative and contextualized fashion. Additionally, an efficient and theoretically-grounded approach is developed to encode the local and global topology information pertaining to interactions into structural embeddings. The resulting embeddings not only inject the structural features underlying TINs into the textual interaction encoding but also facilitate the design of graph sampling strategies. Extensive empirical evaluations on multiple real TIN datasets demonstrate the superiority of SAFT over the state-of-the-art baselines in TIC accuracy. </p>
<blockquote>
<p>æ–‡æœ¬äº¤äº’ç½‘ç»œï¼ˆTINsï¼‰æ˜¯ä¸€ç§æ— å¤„ä¸åœ¨çš„æ•°æ®ç»“æ„ï¼Œç”¨äºæ¨¡æ‹Ÿç”µå­å•†åŠ¡ç½‘ç«™ã€ç¤¾äº¤ç½‘ç»œç­‰å¹³å°ä¸Šç”¨æˆ·å’Œé¡¹ç›®ä¹‹é—´çš„äº¤äº’ï¼Œå…¶ä¸­æ¯ä¸ªäº¤äº’éƒ½ä¸æ–‡æœ¬æè¿°ç›¸å…³è”ã€‚å¯¹è¿™ç±»æ–‡æœ¬äº¤äº’çš„åˆ†ç±»ï¼ˆTICï¼‰åœ¨ç”µå­å•†åŠ¡ä¸­æ£€æµ‹åƒåœ¾è¯„è®ºã€é‡‘èä¸­çš„æ¬ºè¯ˆäº¤æ˜“ç­‰æ–¹é¢æœ‰ç€å¹¿æ³›åº”ç”¨ã€‚ç°æœ‰çš„TICè§£å†³æ–¹æ¡ˆï¼ˆiï¼‰ç”±äºä½¿ç”¨è„±ç¦»ä¸Šä¸‹æ–‡çš„æ–‡æœ¬åµŒå…¥ï¼Œæ— æ³•æ•è·ä¸°å¯Œçš„æ–‡æœ¬è¯­ä¹‰ï¼›ï¼ˆiiï¼‰å¿½ç•¥äº†TINsçš„åŒæ›²ç»“æ„å’ŒèŠ‚ç‚¹å¼‚è´¨æ€§ï¼Œå¯¼è‡´TICæ€§èƒ½ä¸‹é™ã€‚åœ¨æœ¬ç ”ç©¶ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†SAFTè¿™ä¸€æ–°æ¶æ„ï¼Œå®ƒé€šè¿‡æ•´åˆè¯­è¨€å’Œå›¾å½¢æ¨¡å—ï¼Œå®ç°æ–‡æœ¬å’Œç»“æ„è¯­ä¹‰çš„æœ‰æ•ˆèåˆåœ¨äº¤äº’çš„è¡¨ç¤ºå­¦ä¹ ä¸­ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬åˆ©ç”¨çº¿å›¾æ³¨æ„åŠ›ï¼ˆLGAï¼‰&#x2F;é—¨æ§æ³¨æ„åŠ›å•å…ƒï¼ˆGAUsï¼‰å’Œé¢„è®­ç»ƒè¯­è¨€æ¨¡å‹ï¼ˆPLMsï¼‰æ¥æ¨¡æ‹Ÿäº¤äº’çº§åˆ«å’Œä»¤ç‰Œçº§åˆ«çš„ä¿¡å·ï¼Œå¹¶è¿›ä¸€æ­¥é€šè¿‡ä»£ç†ä»¤ç‰Œä»¥è¿­ä»£å’Œè¯­å¢ƒåŒ–çš„æ–¹å¼è¿›è¡Œè€¦åˆã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜å¼€å‘äº†ä¸€ç§é«˜æ•ˆä¸”åŸºäºç†è®ºçš„æ–¹æ³•ï¼Œå°†äº¤äº’çš„å±€éƒ¨å’Œå…¨å±€æ‹“æ‰‘ä¿¡æ¯ç¼–ç ä¸ºç»“æ„åµŒå…¥ã€‚æ‰€å¾—çš„åµŒå…¥ä¸ä»…å°†TINsçš„ç»“æ„ç‰¹å¾æ³¨å…¥æ–‡æœ¬äº¤äº’ç¼–ç ä¸­ï¼Œè€Œä¸”æœ‰åŠ©äºè®¾è®¡å›¾å½¢é‡‡æ ·ç­–ç•¥ã€‚åœ¨å¤šä¸ªçœŸå®TINæ•°æ®é›†ä¸Šçš„å¹¿æ³›å®è¯è¯„ä¼°è¡¨æ˜ï¼ŒSAFTåœ¨TICå‡†ç¡®æ€§æ–¹é¢ä¼˜äºæœ€æ–°åŸºçº¿ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.04861v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºä¸€ç§åä¸ºSAFTçš„æ–°æ¶æ„ï¼Œæ—¨åœ¨é›†æˆè¯­è¨€å’Œå›¾å½¢æ¨¡å—ï¼Œæœ‰æ•ˆèåˆæ–‡æœ¬å’Œç»“æ„æ€§è¯­ä¹‰è¿›è¡Œäº¤äº’è¡¨ç¤ºå­¦ä¹ ã€‚è¯¥æ¶æ„åˆ©ç”¨çº¿å›¾æ³¨æ„åŠ›ï¼ˆLGAï¼‰&#x2F;é—¨æ§æ³¨æ„åŠ›å•å…ƒï¼ˆGAUsï¼‰å’Œé¢„è®­ç»ƒè¯­è¨€æ¨¡å‹ï¼ˆPLMsï¼‰æ¥å»ºæ¨¡äº¤äº’çº§å’Œä»¤ç‰Œçº§çš„ä¿¡å·ï¼Œé€šè¿‡ä»£ç†ä»¤ç‰Œä»¥è¿­ä»£å’Œä¸Šä¸‹æ–‡åŒ–çš„æ–¹å¼è¿›ä¸€æ­¥ç»“åˆã€‚æ­¤å¤–ï¼Œè¿˜å¼€å‘äº†ä¸€ç§é«˜æ•ˆä¸”ç†è®ºæ‰å®çš„æ–¹æ³•ï¼Œå°†å±€éƒ¨å’Œå…¨å±€æ‹“æ‰‘ä¿¡æ¯ç¼–ç åˆ°ç»“æ„æ€§åµŒå…¥ä¸­ã€‚SAFTåœ¨å¤šä¸ªçœŸå®TINæ•°æ®é›†ä¸Šçš„ç»éªŒè¯„ä¼°è¡¨æ˜ï¼Œå…¶åœ¨TICå‡†ç¡®æ€§æ–¹é¢ä¼˜äºç°æœ‰æŠ€æœ¯åŸºçº¿ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ–‡æœ¬äº¤äº’ç½‘ç»œï¼ˆTINsï¼‰æ˜¯æ™®éå­˜åœ¨çš„æ•°æ®ç»“æ„ï¼Œç”¨äºæ¨¡æ‹Ÿç”¨æˆ·åœ¨ç”µå•†ç½‘ç«™ã€ç¤¾äº¤ç½‘ç»œç­‰ä¸Šçš„ç”¨æˆ·ä¸é¡¹ç›®ä¹‹é—´çš„äº¤äº’ã€‚</li>
<li>æ–‡æœ¬äº¤äº’åˆ†ç±»ï¼ˆTICï¼‰åœ¨æ£€æµ‹ç”µå•†ä¸­çš„åƒåœ¾è¯„è®ºã€é‡‘èä¸­çš„æ¬ºè¯ˆäº¤æ˜“ç­‰æ–¹é¢æœ‰å¹¿æ³›åº”ç”¨ã€‚</li>
<li>ç°æœ‰TICè§£å†³æ–¹æ¡ˆå› ä½¿ç”¨æ— ä¸Šä¸‹æ–‡æ–‡æœ¬åµŒå…¥è€Œæœªèƒ½æ•æ‰ä¸°å¯Œçš„æ–‡æœ¬è¯­ä¹‰ï¼Œæˆ–è€…å¿½ç•¥äº†TINsçš„åŒå‘ç»“æ„å’ŒèŠ‚ç‚¹å¼‚è´¨æ€§ï¼Œå¯¼è‡´TICæ€§èƒ½å—æŸã€‚</li>
<li>SAFTæ¶æ„é›†æˆäº†è¯­è¨€å’Œå›¾å½¢æ¨¡å—ï¼Œæœ‰æ•ˆèåˆæ–‡æœ¬å’Œç»“æ„æ€§è¯­ä¹‰è¿›è¡Œäº¤äº’è¡¨ç¤ºå­¦ä¹ ã€‚</li>
<li>SAFTåˆ©ç”¨çº¿å›¾æ³¨æ„åŠ›ï¼ˆLGAï¼‰&#x2F;é—¨æ§æ³¨æ„åŠ›å•å…ƒï¼ˆGAUsï¼‰å’Œé¢„è®­ç»ƒè¯­è¨€æ¨¡å‹ï¼ˆPLMsï¼‰è¿›è¡Œå»ºæ¨¡ï¼Œå¹¶åœ¨è¿­ä»£å’Œä¸Šä¸‹æ–‡åŒ–çš„æ–¹å¼ä¸­è¿›ä¸€æ­¥ç»“åˆäº¤äº’çº§å’Œä»¤ç‰Œçº§çš„ä¿¡å·ã€‚</li>
<li>SAFTå¼€å‘äº†ä¸€ç§ç¼–ç å±€éƒ¨å’Œå…¨å±€æ‹“æ‰‘ä¿¡æ¯çš„æ–¹æ³•ï¼Œå°†å…¶åµŒå…¥åˆ°ç»“æ„æ€§åµŒå…¥ä¸­ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.04861">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-6c1c64ab758c75798ac086b2aadf8423.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-be3d2738058abe510d43853ce107a1de.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-9de3c7fb9daeb9f8716e34622e8dbd7c.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="Can-LLMs-Interpret-and-Leverage-Structured-Linguistic-Representations-A-Case-Study-with-AMRs"><a href="#Can-LLMs-Interpret-and-Leverage-Structured-Linguistic-Representations-A-Case-Study-with-AMRs" class="headerlink" title="Can LLMs Interpret and Leverage Structured Linguistic Representations? A   Case Study with AMRs"></a>Can LLMs Interpret and Leverage Structured Linguistic Representations? A   Case Study with AMRs</h2><p><strong>Authors:Ankush Raut, Xiaofeng Zhu, Maria Leonor Pacheco</strong></p>
<p>This paper evaluates the ability of Large Language Models (LLMs) to leverage contextual information in the form of structured linguistic representations. Specifically, we examine the impact of encoding both short and long contexts using Abstract Meaning Representation (AMR) structures across a diverse set of language tasks. We perform our analysis using 8-bit quantized and instruction-tuned versions of Llama 3.1 (8B), Phi-3, and Mistral 7B. Our results indicate that, for tasks involving short contexts, augmenting the prompt with the AMR of the original language context often degrades the performance of the underlying LLM. However, for tasks that involve long contexts, such as dialogue summarization in the SAMSum dataset, this enhancement improves LLM performance, for example, by increasing the zero-shot cosine similarity score of Llama 3.1 from 66.2% to 76%. This improvement is more evident in the newer and larger LLMs, but does not extend to the older or smaller ones. In addition, we observe that LLMs can effectively reconstruct the original text from a linearized AMR, achieving a cosine similarity of 81.3% in the best-case scenario. </p>
<blockquote>
<p>æœ¬æ–‡è¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åˆ©ç”¨ç»“æ„åŒ–è¯­è¨€è¡¨ç¤ºå½¢å¼ä¸­çš„ä¸Šä¸‹æ–‡ä¿¡æ¯çš„èƒ½åŠ›ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬ç ”ç©¶äº†ä½¿ç”¨æŠ½è±¡æ„ä¹‰è¡¨ç¤ºï¼ˆAMRï¼‰ç»“æ„å¯¹çŸ­é•¿å’Œä¸åŒè¯­å¢ƒè¿›è¡Œç¼–ç å¯¹ä¸€ç³»åˆ—è¯­è¨€ä»»åŠ¡çš„å½±å“ã€‚æˆ‘ä»¬ä½¿ç”¨8ä½é‡åŒ–å’ŒæŒ‡ä»¤è°ƒæ•´çš„Llama 3.1ï¼ˆ8Bï¼‰ã€Phi-3å’ŒMistral 7Bç‰ˆæœ¬è¿›è¡Œåˆ†æã€‚ç»“æœè¡¨æ˜ï¼Œå¯¹äºæ¶‰åŠçŸ­è¯­å¢ƒçš„ä»»åŠ¡ï¼Œé€šè¿‡æç¤ºå¢å¼ºåŸå§‹è¯­è¨€ä¸Šä¸‹æ–‡çš„AMRå¾€å¾€ä¼šé™ä½åŸºç¡€LLMçš„æ€§èƒ½ã€‚ç„¶è€Œï¼Œå¯¹äºæ¶‰åŠé•¿è¯­å¢ƒçš„ä»»åŠ¡ï¼Œå¦‚SAMSumæ•°æ®é›†ä¸­çš„å¯¹è¯æ‘˜è¦ï¼Œè¿™ç§å¢å¼ºä¼šæé«˜LLMçš„æ€§èƒ½ï¼Œä¾‹å¦‚å°†Llama 3. 1çš„é›¶é•œcosineç›¸ä¼¼åº¦å¾—åˆ†ä»66.2%æé«˜åˆ°76%ã€‚è¿™ä¸€æ”¹è¿›åœ¨æ›´æ–°çš„å’Œæ›´å¤§çš„LLMä¸­æ›´ä¸ºæ˜¾è‘—ï¼Œä½†å¹¶ä¸é€‚ç”¨äºæ—§ç‰ˆæˆ–è¾ƒå°çš„LLMã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è§‚å¯Ÿåˆ°LLMå¯ä»¥æœ‰æ•ˆåœ°ä»çº¿æ€§åŒ–çš„AMRé‡å»ºåŸå§‹æ–‡æœ¬ï¼Œåœ¨æœ€ä½³æƒ…å†µä¸‹è¾¾åˆ°81.3%çš„cosineç›¸ä¼¼åº¦ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.04745v1">PDF</a> 13 pages, 23 figures. Submitted to XLLM @ ACL 2025</p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åˆ©ç”¨æŠ½è±¡æ„ä¹‰è¡¨ç¤ºï¼ˆAMRï¼‰ç»“æ„å½¢å¼çš„ä¸Šä¸‹æ–‡ä¿¡æ¯èƒ½åŠ›çš„ç ”ç©¶ã€‚å‘ç°å¯¹äºçŸ­ä»»åŠ¡ï¼ŒåŠ å…¥AMRæç¤ºä¼šé™ä½LLMæ€§èƒ½ï¼›è€Œå¯¹äºé•¿ä»»åŠ¡å¦‚å¯¹è¯æ‘˜è¦ç­‰ï¼Œè¯¥å¢å¼ºæœ‰åŠ©äºæé«˜LLMæ€§èƒ½ã€‚æ–°å¤§å‹LLMçš„æ”¹è¿›æ›´ä¸ºæ˜æ˜¾ã€‚æ­¤å¤–ï¼ŒLLMå¯ä»¥æœ‰æ•ˆåœ°ä»çº¿æ€§åŒ–çš„AMRé‡å»ºåŸå§‹æ–‡æœ¬ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰èƒ½å¤Ÿåˆ©ç”¨æŠ½è±¡æ„ä¹‰è¡¨ç¤ºï¼ˆAMRï¼‰ç»“æ„çš„ä¸Šä¸‹æ–‡ä¿¡æ¯ã€‚</li>
<li>å¯¹äºçŸ­ä»»åŠ¡ï¼ŒåŠ å…¥AMRæç¤ºä¼šé™ä½LLMæ€§èƒ½ã€‚</li>
<li>å¯¹äºé•¿ä»»åŠ¡å¦‚å¯¹è¯æ‘˜è¦ç­‰ï¼Œå¢å¼ºAMRå¯ä»¥æé«˜LLMæ€§èƒ½ã€‚</li>
<li>æ–°é¢–å’Œæ›´å¤§çš„LLMåœ¨æ”¹è¿›æ–¹é¢è¡¨ç°æ›´æ˜¾è‘—ã€‚</li>
<li>LLMèƒ½å¤Ÿä»çº¿æ€§åŒ–çš„AMRæœ‰æ•ˆåœ°é‡å»ºåŸå§‹æ–‡æœ¬ã€‚</li>
<li>ä½¿ç”¨8ä½é‡åŒ–çš„LLMåœ¨ä»»åŠ¡å¤„ç†ä¸­è¡¨ç°ä¼˜ç§€ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.04745">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-b452bdfeba209aac8fc7a551bda0a6e1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9729b90395b51b955ddefc6a9cda9f44.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-8e910886f66f98fb042a4a850007dfe3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9b038b015a028b0bad607c72cf974316.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-8d02fe7f9b9fc0f0658ff90f883f8dbc.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3f5b6cded3565793616136973d4ead20.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="StyleRec-A-Benchmark-Dataset-for-Prompt-Recovery-in-Writing-Style-Transformation"><a href="#StyleRec-A-Benchmark-Dataset-for-Prompt-Recovery-in-Writing-Style-Transformation" class="headerlink" title="StyleRec: A Benchmark Dataset for Prompt Recovery in Writing Style   Transformation"></a>StyleRec: A Benchmark Dataset for Prompt Recovery in Writing Style   Transformation</h2><p><strong>Authors:Shenyang Liu, Yang Gao, Shaoyan Zhai, Liqiang Wang</strong></p>
<p>Prompt Recovery, reconstructing prompts from the outputs of large language models (LLMs), has grown in importance as LLMs become ubiquitous. Most users access LLMs through APIs without internal model weights, relying only on outputs and logits, which complicates recovery. This paper explores a unique prompt recovery task focused on reconstructing prompts for style transfer and rephrasing, rather than typical question-answering. We introduce a dataset created with LLM assistance, ensuring quality through multiple techniques, and test methods like zero-shot, few-shot, jailbreak, chain-of-thought, fine-tuning, and a novel canonical-prompt fallback for poor-performing cases. Our results show that one-shot and fine-tuning yield the best outcomes but highlight flaws in traditional sentence similarity metrics for evaluating prompt recovery. Contributions include (1) a benchmark dataset, (2) comprehensive experiments on prompt recovery strategies, and (3) identification of limitations in current evaluation metrics, all of which advance general prompt recovery research, where the structure of the input prompt is unrestricted. </p>
<blockquote>
<p>æç¤ºæ¢å¤ï¼ˆprompt recoveryï¼‰åœ¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ— å¤„ä¸åœ¨çš„æ—¶ä»£å˜å¾—è¶Šæ¥è¶Šé‡è¦ã€‚å¤§å¤šæ•°ç”¨æˆ·é€šè¿‡APIè®¿é—®LLMï¼Œè€Œä¸æ¶‰åŠå†…éƒ¨æ¨¡å‹æƒé‡ï¼Œä»…ä¾èµ–è¾“å‡ºå’Œlogitsï¼Œè¿™ä½¿å¾—æ¢å¤å·¥ä½œå˜å¾—å¤æ‚ã€‚æœ¬æ–‡æ¢è®¨äº†ä¸€ä¸ªç‹¬ç‰¹çš„æç¤ºæ¢å¤ä»»åŠ¡ï¼Œé‡ç‚¹æ˜¯é€šè¿‡é£æ ¼è¿ç§»å’Œé‡æ–°è¡¨è¿°æ¥é‡å»ºæç¤ºï¼Œè€Œéå…¸å‹çš„é—®é¢˜è§£ç­”ã€‚æˆ‘ä»¬åˆ©ç”¨LLMçš„å¸®åŠ©åˆ›å»ºäº†ä¸€ä¸ªæ•°æ®é›†ï¼Œé€šè¿‡å¤šç§æŠ€æœ¯ç¡®ä¿æ•°æ®è´¨é‡ï¼Œå¹¶æµ‹è¯•äº†é›¶æ ·æœ¬ã€å°‘æ ·æœ¬ã€æ–­å¥æ¢å¤æ³•ã€æ€è€ƒé“¾å¼æ³•ã€å¾®è°ƒä»¥åŠé’ˆå¯¹è¡¨ç°ä¸ä½³æƒ…å†µçš„å…¨æ–°è§„èŒƒæç¤ºå›é€€ç­‰æ–¹æ³•ã€‚æˆ‘ä»¬çš„ç»“æœè¡¨æ˜ï¼Œå•æ¬¡è®­ç»ƒï¼ˆone-shotï¼‰å’Œå¾®è°ƒï¼ˆfine-tuningï¼‰æ•ˆæœæœ€ä½³ï¼Œä½†æŒ‡å‡ºäº†ä¼ ç»Ÿå¥å­ç›¸ä¼¼åº¦è¯„ä¼°æŒ‡æ ‡åœ¨è¯„ä¼°æç¤ºæ¢å¤ä¸Šçš„ç¼ºé™·ã€‚æœ¬ç ”ç©¶çš„è´¡çŒ®åŒ…æ‹¬ï¼šï¼ˆ1ï¼‰åŸºå‡†æ•°æ®é›†ï¼›ï¼ˆ2ï¼‰å…³äºæç¤ºæ¢å¤ç­–ç•¥çš„å…¨é¢å®éªŒï¼›ï¼ˆ3ï¼‰å¯¹ç°è¡Œè¯„ä¼°æŒ‡æ ‡å±€é™æ€§çš„è¯†åˆ«ï¼Œæ‰€æœ‰è¿™äº›éƒ½æœ‰åŠ©äºæ¨åŠ¨ä¸€èˆ¬æ€§çš„æç¤ºæ¢å¤ç ”ç©¶ï¼Œå…¶ä¸­è¾“å…¥æç¤ºçš„ç»“æ„ä¸å—é™åˆ¶ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.04373v1">PDF</a> 2024 IEEE International Conference on Big Data (BigData)</p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ™®åŠä½¿å¾—ä»LLMè¾“å‡ºä¸­æ¢å¤é‡å»ºæç¤ºå˜å¾—è‡³å…³é‡è¦ã€‚æœ¬æ–‡é€šè¿‡å¼•å…¥ä¸€ä¸ªæ–°çš„æ•°æ®é›†å’Œæ–¹æ³•å¯¹å„ç§ç­–ç•¥è¿›è¡Œäº†å…¨é¢å®éªŒæ¢ç´¢ï¼Œé’ˆå¯¹é£æ ¼è½¬æ¢å’Œé‡è¿°è¿›è¡Œç‰¹å®šçš„æç¤ºæ¢å¤ä»»åŠ¡ï¼Œæå‡ºäº†æ–°çš„è¯„ä»·æ–¹æ³•ä»¥æ”¹å–„è¯„ä¼°è´¨é‡ä¸ä½³çš„é—®é¢˜ã€‚ç ”ç©¶å‘ç°å•æ ·ä¾‹ç­–ç•¥å’Œå¾®è°ƒå–å¾—æœ€ä½³æ•ˆæœï¼ŒåŒæ—¶ä¹Ÿå‘ç°äº†ä¼ ç»Ÿçš„å¥å­ç›¸ä¼¼æ€§åº¦é‡æ–¹æ³•åœ¨è¯„ä»·æç¤ºæ¢å¤ä¸Šçš„å±€é™æ€§ã€‚æœ¬ç ”ç©¶çš„è´¡çŒ®åœ¨äºæ¨è¿›äº†æ— çº¦æŸç»“æ„çš„ä¸€èˆ¬æç¤ºæ¢å¤ç ”ç©¶ï¼Œå¹¶ä¸ºç›¸å…³ç ”ç©¶æä¾›äº†æ•°æ®é›†å’Œæ–¹æ³•ä¸Šçš„æ¢ç´¢æ–¹å‘ã€‚æˆ‘ä»¬çš„æ–¹æ³•å’Œç»“è®ºå¯¹æœªæ¥ç›¸å…³çš„ç ”ç©¶å’Œå®é™…åº”ç”¨éƒ½å…·æœ‰å‚è€ƒä»·å€¼ã€‚<br><strong>Key Takeaways</strong>ï¼š </p>
<ul>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ™®åŠä¿ƒä½¿äº†ä»LLMè¾“å‡ºä¸­æ¢å¤é‡å»ºæç¤ºçš„ç ”ç©¶éœ€æ±‚ã€‚ </li>
<li>æœ¬æ–‡èšç„¦äºé£æ ¼è½¬æ¢å’Œé‡è¿°çš„ç‰¹å®šæç¤ºæ¢å¤ä»»åŠ¡ã€‚ </li>
<li>ç ”ç©¶é€šè¿‡å¼•å…¥æ–°çš„æ•°æ®é›†è¿›è¡Œè¯•éªŒï¼Œå¹¶åˆ©ç”¨å¤šç§æ–¹æ³•æå‡æ•°æ®è´¨é‡ã€‚ </li>
<li>æç¤ºæ¢å¤ç­–ç•¥çš„ç ”ç©¶å‘ç°å•æ ·ä¾‹ç­–ç•¥å’Œå¾®è°ƒæ•ˆæœæœ€å¥½ã€‚ </li>
<li>ç ”ç©¶æ­ç¤ºäº†ä¼ ç»Ÿå¥å­ç›¸ä¼¼æ€§åº¦é‡åœ¨è¯„ä¼°æç¤ºæ¢å¤æ—¶çš„å±€é™æ€§ã€‚ </li>
<li>æœ¬ç ”ç©¶è´¡çŒ®åœ¨äºæä¾›äº†æ•°æ®é›†å’Œå®éªŒæ–¹æ³•çš„å…¨é¢æ¢ç´¢ï¼Œä¸ºæœªæ¥ç›¸å…³ç ”ç©¶æä¾›äº†æ–¹å‘ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.04373">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-92a4e90520b8300778019cfc5b99a1d2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-42de292c54cb7690f4060c33fa40e20e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5f3a3209707dfa26a9f44a638374406d.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-c37a7c3d42a1a33f8c8013ca80da1c40.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-eef4afa161299bf1091314d627d61cd5.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-1b45b57645b7d18c5b5fcfd91210e2b3.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="REFORMER-A-ChatGPT-Driven-Data-Synthesis-Framework-Elevating-Text-to-SQL-Models"><a href="#REFORMER-A-ChatGPT-Driven-Data-Synthesis-Framework-Elevating-Text-to-SQL-Models" class="headerlink" title="REFORMER: A ChatGPT-Driven Data Synthesis Framework Elevating   Text-to-SQL Models"></a>REFORMER: A ChatGPT-Driven Data Synthesis Framework Elevating   Text-to-SQL Models</h2><p><strong>Authors:Shenyang Liu, Saleh Almohaimeed, Liqiang Wang</strong></p>
<p>The existing Text-to-SQL models suffer from a shortage of training data, inhibiting their ability to fully facilitate the applications of SQL queries in new domains. To address this challenge, various data synthesis techniques have been employed to generate more diverse and higher quality data. In this paper, we propose REFORMER, a framework that leverages ChatGPTâ€™s prowess without the need for additional training, to facilitate the synthesis of (question, SQL query) pairs tailored to new domains. Our data augmentation approach is based on a â€œretrieve-and-editâ€ method, where we generate new questions by filling masked question using explanation of SQL queries with the help of ChatGPT. Furthermore, we demonstrate that cycle consistency remains a valuable method of validation when applied appropriately. Our experimental results show that REFORMER consistently outperforms previous data augmentation methods. To further investigate the power of ChatGPT and create a general data augmentation method, we also generate the new data by paraphrasing the question in the dataset and by paraphrasing the description of a new SQL query that is generated by ChatGPT as well. Our results affirm that paraphrasing questions generated by ChatGPT help augment the original data. </p>
<blockquote>
<p>å½“å‰å­˜åœ¨çš„æ–‡æœ¬åˆ°SQLæ¨¡å‹é¢ä¸´ç€è®­ç»ƒæ•°æ®ä¸è¶³çš„é—®é¢˜ï¼Œè¿™é™åˆ¶äº†å®ƒä»¬åœ¨æ–°é¢†åŸŸä¸­ä½¿ç”¨SQLæŸ¥è¯¢çš„èƒ½åŠ›ã€‚ä¸ºäº†è§£å†³è¿™ä¸€æŒ‘æˆ˜ï¼Œå·²ç»é‡‡ç”¨äº†å„ç§æ•°æ®åˆæˆæŠ€æœ¯æ¥ç”Ÿæˆæ›´å¤šæ ·åŒ–ã€æ›´é«˜è´¨é‡çš„æ•°æ®ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†REFORMERæ¡†æ¶ï¼Œè¯¥æ¡†æ¶åˆ©ç”¨ChatGPTçš„èƒ½åŠ›ï¼Œæ— éœ€é¢å¤–çš„è®­ç»ƒï¼Œä¿ƒè¿›äº†é’ˆå¯¹æ–°é¢†åŸŸçš„ï¼ˆé—®é¢˜ï¼ŒSQLæŸ¥è¯¢ï¼‰å¯¹çš„åˆæˆã€‚æˆ‘ä»¬çš„æ•°æ®å¢å¼ºæ–¹æ³•åŸºäºâ€œæ£€ç´¢å’Œç¼–è¾‘â€çš„æ–¹æ³•ï¼Œæˆ‘ä»¬é€šè¿‡ChatGPTçš„å¸®åŠ©ï¼Œé€šè¿‡å¡«å……å¸¦é—®é¢˜çš„SQLæŸ¥è¯¢è§£é‡Šæ¥ç”Ÿæˆæ–°çš„é—®é¢˜ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¯æ˜äº†åœ¨é€‚å½“åº”ç”¨æ—¶ï¼Œå¾ªç¯ä¸€è‡´æ€§ä»ç„¶æ˜¯ä¸€ç§æœ‰ä»·å€¼çš„éªŒè¯æ–¹æ³•ã€‚æˆ‘ä»¬çš„å®éªŒç»“æœè¡¨æ˜ï¼ŒREFORMERå§‹ç»ˆä¼˜äºä»¥å‰çš„æ•°æ®å¢å¼ºæ–¹æ³•ã€‚ä¸ºäº†è¿›ä¸€æ­¥æ¢ç´¢ChatGPTçš„å¨åŠ›å¹¶åˆ›å»ºä¸€ç§é€šç”¨çš„æ•°æ®å¢å¼ºæ–¹æ³•ï¼Œæˆ‘ä»¬è¿˜é€šè¿‡é‡æ–°è¡¨è¿°æ•°æ®é›†ä¸­çš„é—®é¢˜å’Œé‡æ–°è¡¨è¿°ChatGPTç”Ÿæˆçš„æ–°SQLæŸ¥è¯¢çš„æè¿°æ¥ç”Ÿæˆæ–°çš„æ•°æ®ã€‚æˆ‘ä»¬çš„ç»“æœè¯å®ï¼Œç”±ChatGPTç”Ÿæˆçš„é‡æ–°è¡¨è¿°é—®é¢˜æœ‰åŠ©äºå¢å¼ºåŸå§‹æ•°æ®ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.04363v1">PDF</a> 2024 International Conference on Machine Learning and Applications   (ICMLA)</p>
<p><strong>Summary</strong></p>
<p>æœ¬è®ºæ–‡é’ˆå¯¹ç°æœ‰Text-to-SQLæ¨¡å‹åœ¨æ–°é¢†åŸŸåº”ç”¨æ—¶é¢ä¸´è®­ç»ƒæ•°æ®ä¸è¶³çš„é—®é¢˜ï¼Œæå‡ºäº†REFORMERæ¡†æ¶ã€‚è¯¥æ¡†æ¶åˆ©ç”¨ChatGPTçš„å¼ºå¤§åŠŸèƒ½ï¼Œæ— éœ€é¢å¤–è®­ç»ƒï¼Œå³å¯åˆæˆé’ˆå¯¹æ–°é¢†åŸŸçš„ï¼ˆé—®é¢˜ï¼ŒSQLæŸ¥è¯¢ï¼‰å¯¹ã€‚REFORMERåŸºäºâ€œæ£€ç´¢å’Œç¼–è¾‘â€çš„æ–¹æ³•è¿›è¡Œæ•°æ®å¢å¼ºï¼Œé€šè¿‡å¡«å……å¸¦æœ‰SQLæŸ¥è¯¢è§£é‡Šçš„é—®é¢˜ä¸­çš„æ©ç æ¥ç”Ÿæˆæ–°é—®é¢˜ï¼Œå€ŸåŠ©ChatGPTå®ç°ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒREFORMERæŒç»­ä¼˜äºä»¥å‰çš„æ•°æ®å¢å¼ºæ–¹æ³•ã€‚æ­¤å¤–ï¼Œé€šè¿‡ç”±ChatGPTç”Ÿæˆçš„é—®é¢˜çš„å¤è¿°å’ŒSQLæŸ¥è¯¢æè¿°ï¼Œè¿›ä¸€æ­¥éªŒè¯äº†ChatGPTçš„åŠ›é‡å’Œä¸€èˆ¬æ•°æ®å¢å¼ºæ–¹æ³•çš„å¯è¡Œæ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Text-to-SQLæ¨¡å‹åœ¨æ–°é¢†åŸŸåº”ç”¨æ—¶é¢ä¸´è®­ç»ƒæ•°æ®ä¸è¶³çš„é—®é¢˜ã€‚</li>
<li>REFORMERæ¡†æ¶è¢«æå‡ºæ¥è§£å†³è¿™ä¸€é—®é¢˜ï¼Œå®ƒåˆ©ç”¨ChatGPTæ¥åˆæˆé’ˆå¯¹æ–°é¢†åŸŸçš„é—®é¢˜å’ŒSQLæŸ¥è¯¢å¯¹ã€‚</li>
<li>REFORMERåŸºäºâ€œæ£€ç´¢å’Œç¼–è¾‘â€çš„æ–¹æ³•ç”Ÿæˆæ–°é—®é¢˜ï¼Œé€šè¿‡å¡«å……å¸¦æœ‰SQLæŸ¥è¯¢è§£é‡Šçš„é—®é¢˜ä¸­çš„æ©ç æ¥å®ç°ã€‚</li>
<li>REFORMERåœ¨å®éªŒä¸­è¡¨ç°ä¼˜è¶Šï¼Œä¼˜äºä¹‹å‰çš„æ•°æ®å¢å¼ºæ–¹æ³•ã€‚</li>
<li>é€šè¿‡å¤è¿°ç”±ChatGPTç”Ÿæˆçš„é—®é¢˜å’ŒSQLæŸ¥è¯¢æè¿°è¿›è¡Œæ•°æ®å¢å¼ºæ˜¯å¯è¡Œçš„ï¼Œå¹¶å¾—åˆ°äº†å®éªŒç»“æœçš„éªŒè¯ã€‚</li>
<li>ChatGPTçš„åº”ç”¨åœ¨æ•°æ®å¢å¼ºæ–¹æ³•ä¸­å‘æŒ¥äº†é‡è¦ä½œç”¨ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.04363">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-2fe0edbca55ecb4c88aaf0af95fa6a39.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d86269bf3f2c9e6f394a74c06ab962fb.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-496a7f4d36ed24ab44b047a25a2fd548.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c2564aff485e07100ee766d97a32cc56.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-cd5af50a93725b9d81cda2684534b0b8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c3c30a07f121ed453cd90d213c347545.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="The-Effects-of-Grouped-Structural-Global-Pruning-of-Vision-Transformers-on-Domain-Generalisation"><a href="#The-Effects-of-Grouped-Structural-Global-Pruning-of-Vision-Transformers-on-Domain-Generalisation" class="headerlink" title="The Effects of Grouped Structural Global Pruning of Vision Transformers   on Domain Generalisation"></a>The Effects of Grouped Structural Global Pruning of Vision Transformers   on Domain Generalisation</h2><p><strong>Authors:Hamza Riaz, Alan F. Smeaton</strong></p>
<p>With the growing sizes of AI models like large language models (LLMs) and vision transformers, deploying them on devices with limited computational resources is a significant challenge particularly when addressing domain generalisation (DG) tasks. This paper introduces a novel grouped structural pruning method for pre-trained vision transformers (ViT, BeiT, and DeiT), evaluated on the PACS and Office-Home DG benchmarks. Our method uses dependency graph analysis to identify and remove redundant groups of neurons, weights, filters, or attention heads within transformers, using a range of selection metrics. Grouped structural pruning is applied at pruning ratios of 50%, 75% and 95% and the models are then fine-tuned on selected distributions from DG benchmarks to evaluate their overall performance in DG tasks. Results show significant improvements in inference speed and fine-tuning time with minimal trade-offs in accuracy and DG task performance. For instance, on the PACS benchmark, pruning ViT, BeiT, and DeiT models by 50% using the Hessian metric resulted in accuracy drops of only -2.94%, -1.42%, and -1.72%, respectively, while achieving speed boosts of 2.5x, 1.81x, and 2.15x. These findings demonstrate the effectiveness of our approach in balancing model efficiency with domain generalisation performance. </p>
<blockquote>
<p>éšç€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å’Œè§†è§‰è½¬æ¢å™¨ï¼ˆvision transformersï¼‰ç­‰äººå·¥æ™ºèƒ½æ¨¡å‹çš„è§„æ¨¡ä¸æ–­å¢é•¿ï¼Œåœ¨æœ‰é™çš„è®¡ç®—èµ„æºè®¾å¤‡ä¸Šéƒ¨ç½²å®ƒä»¬æ˜¯ä¸€ä¸ªå·¨å¤§çš„æŒ‘æˆ˜ï¼Œç‰¹åˆ«æ˜¯åœ¨å¤„ç†é¢†åŸŸæ³›åŒ–ï¼ˆDGï¼‰ä»»åŠ¡æ—¶ã€‚æœ¬æ–‡ä»‹ç»äº†ä¸€ç§é’ˆå¯¹é¢„è®­ç»ƒè§†è§‰è½¬æ¢å™¨ï¼ˆViTã€BeiTå’ŒDeiTï¼‰çš„æ–°å‹åˆ†ç»„ç»“æ„å‰ªææ–¹æ³•ï¼Œå¹¶åœ¨PACSå’ŒOffice-Home DGåŸºå‡†ä¸Šè¿›è¡Œäº†è¯„ä¼°ã€‚æˆ‘ä»¬çš„æ–¹æ³•ä½¿ç”¨ä¾èµ–å›¾åˆ†ææ¥è¯†åˆ«å’Œåˆ é™¤è½¬æ¢å™¨å†…çš„å†—ä½™ç¥ç»å…ƒç»„ã€æƒé‡ã€è¿‡æ»¤å™¨æˆ–æ³¨æ„åŠ›å¤´ï¼Œé‡‡ç”¨ä¸€ç³»åˆ—é€‰æ‹©æŒ‡æ ‡ã€‚åˆ†ç»„ç»“æ„å‰ªæåœ¨50%ã€75%å’Œ95%çš„å‰ªæç‡ä¸‹åº”ç”¨ï¼Œç„¶åå¯¹æ¨¡å‹è¿›è¡Œå¾®è°ƒä»¥é€‚åº”DGåŸºå‡†ä¸­é€‰æ‹©çš„åˆ†å¸ƒï¼Œä»¥è¯„ä¼°å®ƒä»¬åœ¨DGä»»åŠ¡ä¸­çš„æ•´ä½“æ€§èƒ½ã€‚ç»“æœæ˜¾ç¤ºï¼Œæ¨ç†é€Ÿåº¦å’Œå¾®è°ƒæ—¶é—´çš„æ˜¾è‘—æé«˜ï¼Œåœ¨å‡†ç¡®åº¦å’ŒDGä»»åŠ¡æ€§èƒ½æ–¹é¢åªæœ‰æœ€å°çš„æƒè¡¡ã€‚ä¾‹å¦‚ï¼Œåœ¨PACSåŸºå‡†æµ‹è¯•ä¸­ï¼Œä½¿ç”¨Hessianåº¦é‡å°†ViTã€BeiTå’ŒDeiTæ¨¡å‹å‰ªæ50%ï¼Œå‡†ç¡®ç‡ä»…ä¸‹é™-2.94%ã€-1.42%å’Œ-1.72%ï¼ŒåŒæ—¶é€Ÿåº¦æé«˜äº†2.5å€ã€1.81å€å’Œ2.15å€ã€‚è¿™äº›å‘ç°è¡¨æ˜æˆ‘ä»¬çš„æ–¹æ³•åœ¨å¹³è¡¡æ¨¡å‹æ•ˆç‡å’Œé¢†åŸŸæ³›åŒ–æ€§èƒ½æ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.04196v1">PDF</a> 9 pages</p>
<p><strong>Summary</strong><br>    æœ¬æ–‡æå‡ºä¸€ç§é’ˆå¯¹é¢„è®­ç»ƒè§†è§‰è½¬æ¢å™¨ï¼ˆåŒ…æ‹¬ViTã€BeiTå’ŒDeiTï¼‰çš„æ–°å‹åˆ†ç»„ç»“æ„å‰ªææ–¹æ³•ï¼Œç”¨äºå¤„ç†é¢†åŸŸæ³›åŒ–ï¼ˆDGï¼‰ä»»åŠ¡ã€‚è¯¥æ–¹æ³•ä½¿ç”¨ä¾èµ–å›¾åˆ†ææ¥è¯†åˆ«å¹¶ç§»é™¤è½¬æ¢å™¨ä¸­çš„å†—ä½™ç¥ç»å…ƒç»„ã€æƒé‡ã€è¿‡æ»¤å™¨æˆ–æ³¨æ„åŠ›å¤´ï¼Œä»¥æé«˜æ¨¡å‹æ•ˆç‡å’Œéƒ¨ç½²åœ¨èµ„æºå—é™è®¾å¤‡ä¸Šçš„èƒ½åŠ›ã€‚åœ¨PACSå’ŒOffice-Home DGåŸºå‡†æµ‹è¯•ä¸­è¯„ä¼°äº†è¯¥æ–¹æ³•ï¼Œç»“æœæ˜¾ç¤ºï¼Œåœ¨å‰ªææ¯”ä¾‹é«˜è¾¾50%ã€75%å’Œ95%çš„æƒ…å†µä¸‹ï¼Œæ¨ç†é€Ÿåº¦å’Œå¾®è°ƒæ—¶é—´çš„æå‡æ˜¾è‘—ï¼Œè€Œç²¾åº¦å’ŒDGä»»åŠ¡æ€§èƒ½çš„æŸå¤±æå°ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ä»‹ç»äº†é’ˆå¯¹é¢„è®­ç»ƒè§†è§‰è½¬æ¢å™¨çš„åˆ†ç»„ç»“æ„å‰ªææ–¹æ³•ï¼Œä»¥æé«˜æ¨¡å‹æ•ˆç‡å’Œåœ¨èµ„æºå—é™è®¾å¤‡ä¸Šçš„éƒ¨ç½²èƒ½åŠ›ã€‚</li>
<li>æ–¹æ³•åŸºäºä¾èµ–å›¾åˆ†æï¼Œç”¨äºè¯†åˆ«å¹¶ç§»é™¤å†—ä½™çš„ç¥ç»å…ƒç»„ã€æƒé‡ã€è¿‡æ»¤å™¨æˆ–æ³¨æ„åŠ›å¤´ã€‚</li>
<li>åœ¨PACSå’ŒOffice-Home DGåŸºå‡†æµ‹è¯•ä¸­è¯„ä¼°äº†è¯¥æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚</li>
<li>å‰ªææ¯”ä¾‹é«˜è¾¾50%ã€75%å’Œ95%æ—¶ï¼Œæ¨ç†é€Ÿåº¦å’Œå¾®è°ƒæ—¶é—´æ˜¾è‘—æå‡ã€‚</li>
<li>ç²¾åº¦å’ŒDGä»»åŠ¡æ€§èƒ½çš„æŸå¤±æå°ï¼Œä¾‹å¦‚åœ¨PACSåŸºå‡†æµ‹è¯•ä¸­ï¼Œå‰ªæViTã€BeiTå’ŒDeiTæ¨¡å‹50%åï¼Œç²¾åº¦ä»…ä¸‹é™2.94%ã€1.42%å’Œ1.72%ã€‚</li>
<li>è¿™ç§æ–¹æ³•å®ç°äº†æ¨¡å‹æ•ˆç‡å’Œé¢†åŸŸæ³›åŒ–æ€§èƒ½ä¹‹é—´çš„å¹³è¡¡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.04196">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-dbfd9a704f33bb2f2e3d50fe9573ebbb.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c2af29ed6fb5680d58ea19df48024ef1.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-b83e7a4574cf0fd159936655a9116fde.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c32df19f11dbacf8f39067e6d1a18490.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-7083ee93be02a3415f2783fdb3249c63.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="Filter-Images-First-Generate-Instructions-Later-Pre-Instruction-Data-Selection-for-Visual-Instruction-Tuning"><a href="#Filter-Images-First-Generate-Instructions-Later-Pre-Instruction-Data-Selection-for-Visual-Instruction-Tuning" class="headerlink" title="Filter Images First, Generate Instructions Later: Pre-Instruction Data   Selection for Visual Instruction Tuning"></a>Filter Images First, Generate Instructions Later: Pre-Instruction Data   Selection for Visual Instruction Tuning</h2><p><strong>Authors:Bardia Safaei, Faizan Siddiqui, Jiacong Xu, Vishal M. Patel, Shao-Yuan Lo</strong></p>
<p>Visual instruction tuning (VIT) for large vision-language models (LVLMs) requires training on expansive datasets of image-instruction pairs, which can be costly. Recent efforts in VIT data selection aim to select a small subset of high-quality image-instruction pairs, reducing VIT runtime while maintaining performance comparable to full-scale training. However, a major challenge often overlooked is that generating instructions from unlabeled images for VIT is highly expensive. Most existing VIT datasets rely heavily on human annotations or paid services like the GPT API, which limits users with constrained resources from creating VIT datasets for custom applications. To address this, we introduce Pre-Instruction Data Selection (PreSel), a more practical data selection paradigm that directly selects the most beneficial unlabeled images and generates instructions only for the selected images. PreSel first estimates the relative importance of each vision task within VIT datasets to derive task-wise sampling budgets. It then clusters image features within each task, selecting the most representative images with the budget. This approach reduces computational overhead for both instruction generation during VIT data formation and LVLM fine-tuning. By generating instructions for only 15% of the images, PreSel achieves performance comparable to full-data VIT on the LLaVA-1.5 and Vision-Flan datasets. The link to our project page: <a target="_blank" rel="noopener" href="https://bardisafa.github.io/PreSel">https://bardisafa.github.io/PreSel</a> </p>
<blockquote>
<p>è§†è§‰æŒ‡ä»¤è°ƒæ•´ï¼ˆVITï¼‰å¯¹äºå¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆLVLMï¼‰éœ€è¦åœ¨å¤§é‡çš„å›¾åƒæŒ‡ä»¤å¯¹æ•°æ®é›†ä¸Šè¿›è¡Œè®­ç»ƒï¼Œè¿™å¯èƒ½ä¼šå¾ˆæ˜‚è´µã€‚æœ€è¿‘çš„VITæ•°æ®é€‰æ‹©å·¥ä½œæ—¨åœ¨é€‰æ‹©ä¸€å°éƒ¨åˆ†é«˜è´¨é‡çš„å›¾åƒæŒ‡ä»¤å¯¹ï¼Œåœ¨å‡å°‘VITè¿è¡Œæ—¶é—´çš„åŒæ—¶ä¿æŒä¸å…¨è§„æ¨¡è®­ç»ƒç›¸å½“çš„æ€§èƒ½ã€‚ç„¶è€Œï¼Œç»å¸¸è¢«å¿½è§†çš„ä¸€ä¸ªä¸»è¦æŒ‘æˆ˜æ˜¯ï¼Œä»æ— æ ‡ç­¾çš„å›¾åƒä¸­ç”ŸæˆæŒ‡ä»¤çš„æˆæœ¬éå¸¸é«˜ã€‚å¤§å¤šæ•°ç°æœ‰çš„VITæ•°æ®é›†ä¸¥é‡ä¾èµ–äºäººå·¥æ³¨é‡Šæˆ–ä»˜è´¹æœåŠ¡ï¼Œå¦‚GPT APIï¼Œè¿™é™åˆ¶äº†èµ„æºæœ‰é™çš„ç”¨æˆ·åˆ›å»ºç”¨äºè‡ªå®šä¹‰åº”ç”¨ç¨‹åºçš„VITæ•°æ®é›†ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†æ›´å®ç”¨çš„æ•°æ®é€‰æ‹©èŒƒå¼â€”â€”Pre-Instruction Data Selectionï¼ˆPreSelï¼‰ï¼Œå®ƒç›´æ¥é€‰æ‹©æœ€æœ‰ç›Šçš„æ— æ ‡ç­¾å›¾åƒï¼Œåªä¸ºæ‰€é€‰å›¾åƒç”ŸæˆæŒ‡ä»¤ã€‚PreSelé¦–å…ˆä¼°è®¡VITæ•°æ®é›†ä¸­æ¯ä¸ªè§†è§‰ä»»åŠ¡çš„ç›¸å¯¹é‡è¦æ€§ï¼Œä»¥å¾—å‡ºä»»åŠ¡çº§é‡‡æ ·é¢„ç®—ã€‚ç„¶åï¼Œå®ƒåœ¨æ¯ä¸ªä»»åŠ¡å†…å¯¹å›¾åƒç‰¹å¾è¿›è¡Œèšç±»ï¼Œé€‰æ‹©æœ€å…·ä»£è¡¨æ€§çš„å›¾åƒä»¥ç¬¦åˆé¢„ç®—ã€‚è¿™ç§æ–¹æ³•å‡å°‘äº†VITæ•°æ®å½¢æˆå’ŒLVLMå¾®è°ƒè¿‡ç¨‹ä¸­çš„æŒ‡ä»¤ç”Ÿæˆè®¡ç®—å¼€é”€ã€‚åªä¸º15%çš„å›¾åƒç”ŸæˆæŒ‡ä»¤ï¼ŒPreSelåœ¨LLaVA-1.5å’ŒVision-Flanæ•°æ®é›†ä¸Šçš„æ€§èƒ½ä¸å…¨æ•°æ®VITç›¸å½“ã€‚æˆ‘ä»¬çš„é¡¹ç›®é¡µé¢é“¾æ¥ï¼š<a target="_blank" rel="noopener" href="https://bardisafa.github.io/PreSel">https://bardisafa.github.io/PreSel</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.07591v2">PDF</a> Accepted at CVPR 2025 (Highlight)</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†é’ˆå¯¹å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆLVLMsï¼‰çš„è§†è§‰æŒ‡ä»¤å¾®è°ƒï¼ˆVITï¼‰éœ€è¦åœ¨å¤§è§„æ¨¡çš„å›¾åƒæŒ‡ä»¤å¯¹æ•°æ®é›†ä¸Šè¿›è¡Œè®­ç»ƒï¼Œè¿™å¯¼è‡´äº†é«˜æ˜‚çš„æˆæœ¬ã€‚è¿‘æœŸæœ‰ç ”ç©¶å°è¯•é€‰æ‹©é«˜è´¨é‡å›¾åƒæŒ‡ä»¤å¯¹å­é›†ä»¥é™ä½æˆæœ¬å¹¶æé«˜è¿è¡Œæ•ˆç‡ï¼Œä½†ç”ŸæˆæŒ‡ä»¤éœ€è¦å¤§é‡äººåŠ›æ ‡æ³¨æˆ–ä»˜è´¹æœåŠ¡ï¼Œé™åˆ¶äº†èµ„æºå—é™ç”¨æˆ·çš„è‡ªå®šä¹‰åº”ç”¨ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†é¢„æŒ‡ä»¤æ•°æ®é€‰æ‹©ï¼ˆPreSelï¼‰æ–¹æ³•ï¼Œè¯¥æ–¹æ³•ç›´æ¥é€‰æ‹©æœ€æœ‰ç›Šçš„æ— æ ‡ç­¾å›¾åƒå¹¶ä¸ºæ‰€é€‰å›¾åƒç”ŸæˆæŒ‡ä»¤ã€‚PreSelé€šè¿‡ä¼°è®¡æ¯ä¸ªè§†è§‰ä»»åŠ¡çš„é‡è¦æ€§æ¥æ¨å¯¼ä»»åŠ¡çº§é‡‡æ ·é¢„ç®—ï¼Œå¹¶åœ¨æ¯ä¸ªä»»åŠ¡å†…å¯¹å›¾åƒç‰¹å¾è¿›è¡Œèšç±»ï¼Œé€‰æ‹©æœ€å…·ä»£è¡¨æ€§çš„å›¾åƒã€‚è¯¥æ–¹æ³•é™ä½äº†åœ¨æ„å»ºVITæ•°æ®é›†å’Œå¾®è°ƒLVLMæ—¶çš„è®¡ç®—å¼€é”€ã€‚ä»…å¯¹15%çš„å›¾åƒç”ŸæˆæŒ‡ä»¤å³å¯å®ç°ä¸å…¨æ•°æ®VITç›¸å½“çš„æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è§†è§‰æŒ‡ä»¤å¾®è°ƒï¼ˆVITï¼‰éœ€è¦å¤§é‡å›¾åƒæŒ‡ä»¤å¯¹æ•°æ®é›†ï¼Œæˆæœ¬é«˜æ˜‚ã€‚</li>
<li>ç°æœ‰æ•°æ®é€‰æ‹©æ–¹æ³•ä¾èµ–äººåŠ›æ ‡æ³¨æˆ–ä»˜è´¹æœåŠ¡ï¼Œé™åˆ¶äº†èµ„æºå—é™ç”¨æˆ·çš„è‡ªå®šä¹‰åº”ç”¨ã€‚</li>
<li>é¢„æŒ‡ä»¤æ•°æ®é€‰æ‹©ï¼ˆPreSelï¼‰æ–¹æ³•ç›´æ¥é€‰æ‹©æ— æ ‡ç­¾å›¾åƒå¹¶ä¸ºæ‰€é€‰å›¾åƒç”ŸæˆæŒ‡ä»¤ï¼Œé™ä½æˆæœ¬ã€‚</li>
<li>PreSelé€šè¿‡ä¼°è®¡æ¯ä¸ªè§†è§‰ä»»åŠ¡çš„é‡è¦æ€§æ¥æ¨å¯¼ä»»åŠ¡çº§é‡‡æ ·é¢„ç®—ã€‚</li>
<li>PreSelåœ¨å›¾åƒç‰¹å¾èšç±»ä¸­é€‰æ‹©æœ€å…·ä»£è¡¨æ€§çš„å›¾åƒï¼Œé™ä½è®¡ç®—å¼€é”€ã€‚</li>
<li>ä»…å¯¹éƒ¨åˆ†å›¾åƒç”ŸæˆæŒ‡ä»¤å³å¯å®ç°ä¸å…¨æ•°æ®VITç›¸å½“çš„æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.07591">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-c9377d2c0291d7db3c1e387fc718d8cd.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-93ec5db65b826b27d696e3a99ffda4de.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e265abccdd703087bfb59e5a7564ce27.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f68ee10db0da8de33097a4f4f36d45f7.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="Fine-Tuning-Transformer-Based-Vision-Language-Models-for-Robust-Object-Detection-in-Unstructured-Environments"><a href="#Fine-Tuning-Transformer-Based-Vision-Language-Models-for-Robust-Object-Detection-in-Unstructured-Environments" class="headerlink" title="Fine-Tuning Transformer-Based Vision-Language Models for Robust Object   Detection in Unstructured Environments"></a>Fine-Tuning Transformer-Based Vision-Language Models for Robust Object   Detection in Unstructured Environments</h2><p><strong>Authors:Aysegul Ucar, Soumyadeep Ro, Sanapala Satwika, Pamarthi Yasoda Gayathri, Mohmmad Ghaith Balsha</strong></p>
<p>Vision-Language Models (VLMs) have emerged as powerful tools in artificial intelli-gence, capable of integrating textual and visual data for a unified understanding of complex scenes. While models such as Florence2, built on transformer architectures, have shown promise across general tasks, their performance in object detection within unstructured or cluttered environments remains underexplored. In this study, we fi-ne-tuned the Florence2 model for object detection tasks in non-constructed, complex environments. A comprehensive experimental framework was established involving multiple hardware configurations (NVIDIA T4, L4, and A100 GPUs), optimizers (AdamW, SGD), and varied hyperparameters including learning rates and LoRA (Low-Rank Adaptation) setups. Model training and evaluation were conducted on challenging datasets representative of real-world, disordered settings. The optimized Florence2 models exhibited significant improvements in object detection accuracy, with Mean Average Precision (mAP) metrics approaching or matching those of estab-lished models such as YOLOv8, YOLOv9, and YOLOv10. The integration of LoRA and careful fine-tuning of transformer layers contributed notably to these gains. Our find-ings highlight the adaptability of transformer-based VLMs like Florence2 for do-main-specific tasks, particularly in visually complex environments. The study under-scores the potential of fine-tuned VLMs to rival traditional convolution-based detec-tors, offering a flexible and scalable approach for advanced vision applications in re-al-world, unstructured settings. </p>
<blockquote>
<p>è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰ä½œä¸ºäººå·¥æ™ºèƒ½çš„å¼ºå¤§å·¥å…·ï¼Œèƒ½å¤Ÿæ•´åˆæ–‡æœ¬å’Œè§†è§‰æ•°æ®ï¼Œå¯¹å¤æ‚åœºæ™¯è¿›è¡Œç»Ÿä¸€ç†è§£ã€‚è™½ç„¶åŸºäºtransformeræ¶æ„çš„Florence2ç­‰æ¨¡å‹åœ¨ä¸€èˆ¬ä»»åŠ¡ä¸Šè¡¨ç°å‡ºæ½œåŠ›ï¼Œä½†åœ¨éç»“æ„åŒ–æˆ–æ‚ä¹±ç¯å¢ƒä¸­è¿›è¡Œç›®æ ‡æ£€æµ‹çš„æ€§èƒ½ä»æœªå¾—åˆ°å……åˆ†æ¢ç´¢ã€‚æœ¬ç ”ç©¶ä¸­ï¼Œæˆ‘ä»¬å¯¹Florence2æ¨¡å‹è¿›è¡Œäº†å¾®è°ƒï¼Œä»¥åº”å¯¹éæ„å»ºç¯å¢ƒä¸­çš„ç›®æ ‡æ£€æµ‹ä»»åŠ¡ã€‚æˆ‘ä»¬å»ºç«‹äº†ä¸€ä¸ªå…¨é¢çš„å®éªŒæ¡†æ¶ï¼Œæ¶‰åŠå¤šç§ç¡¬ä»¶é…ç½®ï¼ˆNVIDIA T4ã€L4å’ŒA100 GPUï¼‰ã€ä¼˜åŒ–å™¨ï¼ˆAdamWã€SGDï¼‰ä»¥åŠåŒ…æ‹¬å­¦ä¹ ç‡å’ŒLoRAï¼ˆä½ç§©é€‚åº”ï¼‰åœ¨å†…çš„å„ç§è¶…å‚æ•°è®¾ç½®ã€‚æ¨¡å‹è®­ç»ƒä¸è¯„ä¼°æ˜¯åœ¨å…·æœ‰ä»£è¡¨æ€§çš„ç°å®ä¸–ç•Œæ‚ä¹±è®¾ç½®æŒ‘æˆ˜æ•°æ®é›†ä¸Šè¿›è¡Œçš„ã€‚ä¼˜åŒ–åçš„Florence2æ¨¡å‹åœ¨ç›®æ ‡æ£€æµ‹ç²¾åº¦ä¸Šå–å¾—äº†æ˜¾è‘—æ”¹è¿›ï¼Œå¹³å‡ç²¾åº¦ï¼ˆmAPï¼‰æŒ‡æ ‡æ¥è¿‘æˆ–è¾¾åˆ°äº†YOLOv8ã€YOLOv9å’ŒYOLOv10ç­‰ç°æœ‰æ¨¡å‹çš„æ€§èƒ½ã€‚LoRAçš„é›†æˆå’Œå¯¹transformerå±‚çš„ä»”ç»†å¾®è°ƒå¯¹è¿™äº›æ”¶ç›Šäº§ç”Ÿäº†æ˜¾è‘—å½±å“ã€‚æˆ‘ä»¬çš„ç ”ç©¶ç»“æœå¼ºè°ƒäº†åŸºäºtransformerçš„VLMsï¼ˆå¦‚Florence2ï¼‰å¯¹ç‰¹å®šé¢†åŸŸçš„é€‚åº”æ€§ï¼Œç‰¹åˆ«æ˜¯åœ¨è§†è§‰å¤æ‚çš„ç¯å¢ƒä¸­ã€‚è¯¥ç ”ç©¶çªæ˜¾äº†ç»è¿‡ç²¾ç»†è°ƒæ•´çš„VLMsä¸ä¼ ç»ŸåŸºäºå·ç§¯çš„æ£€æµ‹å™¨ç«äº‰æ½œåŠ›ï¼Œä¸ºç°å®ä¸–ç•Œéç»“æ„åŒ–ç¯å¢ƒä¸­çš„é«˜çº§è§†è§‰åº”ç”¨æä¾›äº†çµæ´»ä¸”å¯æ‰©å±•çš„æ–¹æ³•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.04918v3">PDF</a> 22 pages, 13 Figures, 6 Tables</p>
<p><strong>Summary</strong></p>
<p>æœ¬ç ”ç©¶çš„é‡ç‚¹æ˜¯å¯¹åŸºäºè½¬æ¢å™¨æ¶æ„çš„Florence2æ¨¡å‹è¿›è¡Œä¼˜åŒ–ï¼Œä»¥é€‚åº”éç»“æ„åŒ–æˆ–æ‚ä¹±ç¯å¢ƒä¸­ç‰©ä½“æ£€æµ‹çš„ä»»åŠ¡ã€‚é€šè¿‡å»ºç«‹ä¸€ä¸ªå…¨é¢çš„å®éªŒæ¡†æ¶ï¼Œæ¶‰åŠå¤šç§ç¡¬ä»¶é…ç½®ã€ä¼˜åŒ–å™¨ã€è¶…å‚æ•°ç­‰ï¼Œæ¨¡å‹åœ¨ä»£è¡¨çœŸå®ä¸–ç•Œæ‚ä¹±è®¾ç½®çš„æŒ‘æˆ˜æ€§æ•°æ®é›†ä¸Šè¿›è¡Œè®­ç»ƒå’Œè¯„ä¼°ã€‚ä¼˜åŒ–åçš„Florence2æ¨¡å‹åœ¨ç‰©ä½“æ£€æµ‹å‡†ç¡®æ€§ä¸Šæœ‰æ˜¾è‘—æé«˜ï¼Œå¹³å‡ç²¾åº¦ï¼ˆmAPï¼‰æŒ‡æ ‡æ¥è¿‘æˆ–åŒ¹é…YOLOv8ã€YOLOv9å’ŒYOLOv10ç­‰å·²å»ºç«‹æ¨¡å‹ã€‚æœ¬ç ”ç©¶çªæ˜¾äº†åŸºäºè½¬æ¢å™¨çš„VLMsï¼ˆå¦‚Florence2ï¼‰å¯¹ç‰¹å®šé¢†åŸŸçš„é€‚åº”æ€§ï¼Œç‰¹åˆ«æ˜¯åœ¨è§†è§‰å¤æ‚ç¯å¢ƒä¸­ã€‚æ­¤å¤–ï¼Œå®ƒè¿˜æŒ‡å‡ºäº†å¾®è°ƒåçš„VLMsä¸ä¼ ç»ŸåŸºäºå·ç§¯çš„æ£€æµ‹å™¨ç«äº‰çš„æ½œåŠ›ï¼Œä¸ºçœŸå®ä¸–ç•Œéç»“æ„åŒ–ç¯å¢ƒä¸­çš„é«˜çº§è§†è§‰åº”ç”¨æä¾›äº†çµæ´»ä¸”å¯æ‰©å±•çš„æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>åŸºäºè½¬æ¢å™¨æ¶æ„çš„Florence2æ¨¡å‹è¢«ä¼˜åŒ–ä»¥é€‚åº”éç»“æ„åŒ–ç¯å¢ƒä¸­çš„ç‰©ä½“æ£€æµ‹ä»»åŠ¡ã€‚</li>
<li>é€šè¿‡å…¨é¢çš„å®éªŒæ¡†æ¶ï¼Œæ¶‰åŠå¤šç§ç¡¬ä»¶é…ç½®ã€ä¼˜åŒ–å™¨ã€è¶…å‚æ•°ç­‰æ¥è¿›è¡Œæ¨¡å‹è®­ç»ƒå’Œè¯„ä¼°ã€‚</li>
<li>ä¼˜åŒ–åçš„Florence2æ¨¡å‹åœ¨ç‰©ä½“æ£€æµ‹å‡†ç¡®æ€§ä¸Šæ˜¾è‘—æé«˜ï¼ŒmAPæŒ‡æ ‡æ¥è¿‘æˆ–åŒ¹é…å…¶ä»–ä¸»æµæ¨¡å‹ã€‚</li>
<li>LoRAæŠ€æœ¯çš„é›†æˆå’Œè½¬æ¢å™¨å±‚çš„ç²¾ç»†è°ƒæ•´å¯¹æå‡æ€§èƒ½æœ‰æ˜¾è‘—è´¡çŒ®ã€‚</li>
<li>ç ”ç©¶è¡¨æ˜ï¼ŒåŸºäºè½¬æ¢å™¨çš„VLMsï¼ˆå¦‚Florence2ï¼‰åœ¨ç‰¹å®šé¢†åŸŸä»»åŠ¡ä¸­å…·æœ‰è‰¯å¥½çš„é€‚åº”æ€§ï¼Œç‰¹åˆ«æ˜¯åœ¨è§†è§‰å¤æ‚çš„ç¯å¢ƒä¸­ã€‚</li>
<li>ç›¸æ¯”ä¼ ç»ŸåŸºäºå·ç§¯çš„æ£€æµ‹å™¨ï¼Œå¾®è°ƒåçš„VLMså…·æœ‰ç«äº‰åŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.04918">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-98db4665358a36f424d9bc07ef7a2277.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2cad3459aa076ed1ee0673f0a1018e24.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b1905fcaa64f23f604c5c098173f59be.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="M2-omni-Advancing-Omni-MLLM-for-Comprehensive-Modality-Support-with-Competitive-Performance"><a href="#M2-omni-Advancing-Omni-MLLM-for-Comprehensive-Modality-Support-with-Competitive-Performance" class="headerlink" title="M2-omni: Advancing Omni-MLLM for Comprehensive Modality Support with   Competitive Performance"></a>M2-omni: Advancing Omni-MLLM for Comprehensive Modality Support with   Competitive Performance</h2><p><strong>Authors:Qingpei Guo, Kaiyou Song, Zipeng Feng, Ziping Ma, Qinglong Zhang, Sirui Gao, Xuzheng Yu, Yunxiao Sun, Tai-Wei Chang, Jingdong Chen, Ming Yang, Jun Zhou</strong></p>
<p>We present M2-omni, a cutting-edge, open-source omni-MLLM that achieves competitive performance to GPT-4o. M2-omni employs a unified multimodal sequence modeling framework, which empowers Large Language Models(LLMs) to acquire comprehensive cross-modal understanding and generation capabilities. Specifically, M2-omni can process arbitrary combinations of audio, video, image, and text modalities as input, generating multimodal sequences interleaving with audio, image, or text outputs, thereby enabling an advanced and interactive real-time experience. The training of such an omni-MLLM is challenged by significant disparities in data quantity and convergence rates across modalities. To address these challenges, we propose a step balance strategy during pre-training to handle the quantity disparities in modality-specific data. Additionally, a dynamically adaptive balance strategy is introduced during the instruction tuning stage to synchronize the modality-wise training progress, ensuring optimal convergence. Notably, we prioritize preserving strong performance on pure text tasks to maintain the robustness of M2-omniâ€™s language understanding capability throughout the training process. To our best knowledge, M2-omni is currently a very competitive open-source model to GPT-4o, characterized by its comprehensive modality and task support, as well as its exceptional performance. We expect M2-omni will advance the development of omni-MLLMs, thus facilitating future research in this domain. </p>
<blockquote>
<p>æˆ‘ä»¬ä»‹ç»äº†M2-omniï¼Œè¿™æ˜¯ä¸€ä¸ªå°–ç«¯å¼€æºçš„å…¨æ–¹ä½å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰ï¼Œå…¶æ€§èƒ½ä¸GPT-4oç›¸å½“ã€‚M2-omnié‡‡ç”¨ç»Ÿä¸€çš„å¤šæ¨¡æ€åºåˆ—å»ºæ¨¡æ¡†æ¶ï¼Œä½¿å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å…·å¤‡å…¨é¢çš„è·¨æ¨¡æ€ç†è§£å’Œç”Ÿæˆèƒ½åŠ›ã€‚å…·ä½“æ¥è¯´ï¼ŒM2-omniå¯ä»¥å¤„ç†ä»»æ„ç»„åˆçš„éŸ³é¢‘ã€è§†é¢‘ã€å›¾åƒå’Œæ–‡æœ¬æ¨¡æ€ä½œä¸ºè¾“å…¥ï¼Œç”Ÿæˆäº¤æ›¿çš„éŸ³é¢‘ã€å›¾åƒæˆ–æ–‡æœ¬è¾“å‡ºï¼Œä»è€Œå®ç°å…ˆè¿›ä¸”äº¤äº’å¼çš„å®æ—¶ä½“éªŒã€‚æ­¤ç±»å…¨æ–¹ä½MLLMçš„è®­ç»ƒé¢ä¸´è·¨æ¨¡æ€æ•°æ®é‡å·®å¼‚å’Œæ”¶æ•›ç‡æ˜¾è‘—ä¸åŒçš„æŒ‘æˆ˜ã€‚ä¸ºäº†åº”å¯¹è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬åœ¨é¢„è®­ç»ƒè¿‡ç¨‹ä¸­æå‡ºäº†ä¸€ç§æ­¥éª¤å¹³è¡¡ç­–ç•¥æ¥å¤„ç†ç‰¹å®šæ¨¡æ€æ•°æ®åœ¨æ•°é‡ä¸Šçš„å·®å¼‚ã€‚æ­¤å¤–ï¼Œåœ¨æŒ‡ä»¤è°ƒæ•´é˜¶æ®µå¼•å…¥äº†åŠ¨æ€è‡ªé€‚åº”å¹³è¡¡ç­–ç•¥ï¼Œä»¥åŒæ­¥ä¸åŒæ¨¡æ€çš„è®­ç»ƒè¿›åº¦ï¼Œç¡®ä¿æœ€ä½³æ”¶æ•›ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œæˆ‘ä»¬ä¼˜å…ˆåœ¨çº¯æ–‡æœ¬ä»»åŠ¡ä¸Šä¿æŒå“è¶Šæ€§èƒ½ï¼Œä»¥åœ¨æ•´ä¸ªè®­ç»ƒè¿‡ç¨‹ä¸­ä¿æŒM2-omniçš„è¯­è¨€ç†è§£èƒ½åŠ›çš„ç¨³å¥æ€§ã€‚æ®æˆ‘ä»¬æ‰€çŸ¥ï¼ŒM2-omniç›®å‰æ˜¯ä¸€ä¸ªéå¸¸å…·æœ‰ç«äº‰åŠ›çš„å¼€æºæ¨¡å‹ï¼Œä¸GPT-4oç›¸æ¯”å…·æœ‰å…¨é¢çš„æ¨¡æ€å’Œä»»åŠ¡æ”¯æŒä»¥åŠå‡ºè‰²çš„æ€§èƒ½ã€‚æˆ‘ä»¬ç›¸ä¿¡M2-omniå°†æ¨åŠ¨å…¨æ–¹ä½MLLMçš„å‘å±•ï¼Œä»è€Œä¿ƒè¿›è¯¥é¢†åŸŸçš„æœªæ¥ç ”ç©¶ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.18778v3">PDF</a> </p>
<p><strong>Summary</strong>ï¼š</p>
<p>M2-omniæ˜¯ä¸€æ¬¾å…ˆè¿›çš„å¼€æºé€šç”¨å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼Œå…·å¤‡è·¨æ¨¡æ€ç†è§£å’Œç”Ÿæˆèƒ½åŠ›ï¼Œå¯å¤„ç†å¤šç§æ¨¡æ€è¾“å…¥å¹¶ç”Ÿæˆå¤šåª’ä½“è¾“å‡ºã€‚ä¸ºåº”å¯¹ä¸åŒæ¨¡æ€æ•°æ®é‡å’Œæ”¶æ•›ç‡çš„æŒ‘æˆ˜ï¼Œé‡‡ç”¨é¢„è®­ç»ƒé˜¶æ®µçš„æ­¥éª¤å¹³è¡¡ç­–ç•¥å’ŒæŒ‡ä»¤å¾®è°ƒé˜¶æ®µçš„åŠ¨æ€è‡ªé€‚åº”å¹³è¡¡ç­–ç•¥ã€‚åœ¨ä¿æŒçº¯æ–‡æœ¬ä»»åŠ¡æ€§èƒ½çš„åŒæ—¶ï¼Œå®ç°äº†å¤šæ¨¡æ€æ”¯æŒï¼Œè¡¨ç°å‡ºå¼ºå¤§çš„ç«äº‰åŠ›ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>M2-omniæ˜¯ä¸€ç§å…ˆè¿›çš„å¼€æºå¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆomni-MLLMï¼‰ã€‚</li>
<li>å®ƒé‡‡ç”¨ç»Ÿä¸€çš„å¤šæ¨¡æ€åºåˆ—å»ºæ¨¡æ¡†æ¶ï¼Œå®ç°äº†è·¨æ¨¡æ€ç†è§£å’Œç”Ÿæˆèƒ½åŠ›ã€‚</li>
<li>M2-omniå¯ä»¥å¤„ç†ä»»æ„ç»„åˆçš„éŸ³é¢‘ã€è§†é¢‘ã€å›¾åƒå’Œæ–‡æœ¬æ¨¡æ€è¾“å…¥ï¼Œå¹¶ç”Ÿæˆå¤šåª’ä½“è¾“å‡ºã€‚</li>
<li>åœ¨é¢„è®­ç»ƒé˜¶æ®µé‡‡ç”¨æ­¥éª¤å¹³è¡¡ç­–ç•¥æ¥åº”å¯¹ä¸åŒæ¨¡æ€æ•°æ®é‡çš„æŒ‘æˆ˜ã€‚</li>
<li>åœ¨æŒ‡ä»¤å¾®è°ƒé˜¶æ®µå¼•å…¥åŠ¨æ€è‡ªé€‚åº”å¹³è¡¡ç­–ç•¥ï¼Œä»¥ç¡®ä¿æœ€ä¼˜çš„æ”¶æ•›æ€§ã€‚</li>
<li>M2-omniåœ¨ä¿æŒçº¯æ–‡æœ¬ä»»åŠ¡æ€§èƒ½çš„åŒæ—¶ï¼Œæä¾›äº†å¤šæ¨¡æ€æ”¯æŒã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.18778">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-4a05049f01e33e15aac8ab10c19f95e9.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-03a405de1ce51b7b197c5275237bfa9c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f1d74d27e2d236fb414c246230858e65.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-88f031d34f0a1a8124fd8656ff379404.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7564fb67c64d3b1baedecb5c9cdec9af.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="EarthDial-Turning-Multi-sensory-Earth-Observations-to-Interactive-Dialogues"><a href="#EarthDial-Turning-Multi-sensory-Earth-Observations-to-Interactive-Dialogues" class="headerlink" title="EarthDial: Turning Multi-sensory Earth Observations to Interactive   Dialogues"></a>EarthDial: Turning Multi-sensory Earth Observations to Interactive   Dialogues</h2><p><strong>Authors:Sagar Soni, Akshay Dudhane, Hiyam Debary, Mustansar Fiaz, Muhammad Akhtar Munir, Muhammad Sohail Danish, Paolo Fraccaro, Campbell D Watson, Levente J Klein, Fahad Shahbaz Khan, Salman Khan</strong></p>
<p>Automated analysis of vast Earth observation data via interactive Vision-Language Models (VLMs) can unlock new opportunities for environmental monitoring, disaster response, and {resource management}. Existing generic VLMs do not perform well on Remote Sensing data, while the recent Geo-spatial VLMs remain restricted to a fixed resolution and few sensor modalities. In this paper, we introduce EarthDial, a conversational assistant specifically designed for Earth Observation (EO) data, transforming complex, multi-sensory Earth observations into interactive, natural language dialogues. EarthDial supports multi-spectral, multi-temporal, and multi-resolution imagery, enabling a wide range of remote sensing tasks, including classification, detection, captioning, question answering, visual reasoning, and visual grounding. To achieve this, we introduce an extensive instruction tuning dataset comprising over 11.11M instruction pairs covering RGB, Synthetic Aperture Radar (SAR), and multispectral modalities such as Near-Infrared (NIR) and infrared. Furthermore, EarthDial handles bi-temporal and multi-temporal sequence analysis for applications like change detection. Our extensive experimental results on 44 downstream datasets demonstrate that EarthDial outperforms existing generic and domain-specific models, achieving better generalization across various EO tasks. Our source codes and pre-trained models are at <a target="_blank" rel="noopener" href="https://github.com/hiyamdebary/EarthDial">https://github.com/hiyamdebary/EarthDial</a>. </p>
<blockquote>
<p>é€šè¿‡äº¤äº’å¼è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰å¯¹å¤§é‡çš„åœ°çƒè§‚æµ‹æ•°æ®è¿›è¡Œè‡ªåŠ¨åŒ–åˆ†æï¼Œå¯ä»¥ä¸ºç¯å¢ƒç›‘æµ‹ã€ç¾å®³å“åº”å’Œ{èµ„æºç®¡ç†}å¸¦æ¥æ–°çš„æœºé‡ã€‚ç°æœ‰çš„é€šç”¨VLMsåœ¨é¥æ„Ÿæ•°æ®ä¸Šçš„è¡¨ç°å¹¶ä¸ç†æƒ³ï¼Œè€Œæœ€è¿‘çš„åœ°ç†ç©ºé—´VLMsä»ç„¶å±€é™äºå›ºå®šçš„åˆ†è¾¨ç‡å’Œæœ‰é™çš„ä¼ æ„Ÿå™¨æ¨¡å¼ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†EarthDialï¼Œè¿™æ˜¯ä¸€æ¬¾ä¸“é—¨ä¸ºåœ°çƒè§‚æµ‹ï¼ˆEOï¼‰æ•°æ®è®¾è®¡çš„å¯¹è¯åŠ©æ‰‹ï¼Œå°†å¤æ‚çš„å¤šæ„Ÿå®˜åœ°çƒè§‚æµ‹è½¬åŒ–ä¸ºäº¤äº’å¼çš„è‡ªç„¶è¯­è¨€å¯¹è¯ã€‚EarthDialæ”¯æŒå¤šå…‰è°±ã€å¤šæ—¶ç›¸å’Œå¤šåˆ†è¾¨ç‡çš„å½±åƒï¼Œèƒ½å¤Ÿå®Œæˆå¹¿æ³›çš„é¥æ„Ÿä»»åŠ¡ï¼ŒåŒ…æ‹¬åˆ†ç±»ã€æ£€æµ‹ã€æè¿°ã€é—®ç­”ã€è§†è§‰æ¨ç†å’Œè§†è§‰å®šä½ã€‚ä¸ºäº†å®ç°è¿™ä¸€ç‚¹ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ªåŒ…å«è¶…è¿‡111ä¸‡æ¡æŒ‡ä»¤å¯¹çš„åºå¤§æŒ‡ä»¤è°ƒæ•´æ•°æ®é›†ï¼Œæ¶µç›–RGBã€åˆæˆå­”å¾„é›·è¾¾ï¼ˆSARï¼‰ä»¥åŠè¿‘çº¢å¤–ï¼ˆNIRï¼‰å’Œçº¢å¤–ç­‰å¤šå…‰è°±æ¨¡å¼ã€‚æ­¤å¤–ï¼ŒEarthDialè¿˜å¤„ç†åŒæ—¶æ€å’Œå¤šæ—¶æ€åºåˆ—åˆ†æï¼Œç”¨äºå˜åŒ–æ£€æµ‹ç­‰åº”ç”¨ã€‚æˆ‘ä»¬åœ¨44ä¸ªä¸‹æ¸¸æ•°æ®é›†ä¸Šçš„å¹¿æ³›å®éªŒç»“æœè¡¨æ˜ï¼ŒEarthDialä¼˜äºç°æœ‰çš„é€šç”¨å’Œç‰¹å®šé¢†åŸŸæ¨¡å‹ï¼Œåœ¨å„ç§EOä»»åŠ¡ä¸­å®ç°äº†æ›´å¥½çš„æ³›åŒ–èƒ½åŠ›ã€‚æˆ‘ä»¬çš„æºä»£ç å’Œé¢„è®­ç»ƒæ¨¡å‹ä½äº<a target="_blank" rel="noopener" href="https://github.com/hiyamdebary/EarthDial%E3%80%82">https://github.com/hiyamdebary/EarthDialã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.15190v2">PDF</a> </p>
<p><strong>Summary</strong>ï¼š<br>é€šè¿‡åˆ©ç”¨äº¤äº’å¼è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰å¯¹å¤§é‡çš„åœ°çƒè§‚æµ‹æ•°æ®è¿›è¡Œè‡ªåŠ¨åŒ–åˆ†æï¼Œå¯è§£é”ç¯å¢ƒç›‘æµ‹ã€ç¾å®³å“åº”å’Œ{èµ„æºç®¡ç†}çš„æ–°æœºé‡ã€‚ç°æœ‰é€šç”¨VLMsåœ¨é¥æ„Ÿæ•°æ®ä¸Šè¡¨ç°ä¸ä½³ï¼Œè€Œæœ€æ–°çš„åœ°ç†ç©ºé—´VLMsä»å—é™äºå›ºå®šåˆ†è¾¨ç‡å’Œæœ‰é™çš„ä¼ æ„Ÿå™¨æ¨¡å¼ã€‚æœ¬æ–‡ä»‹ç»äº†ä¸€æ¬¾ä¸“ä¸ºåœ°çƒè§‚æµ‹ï¼ˆEOï¼‰æ•°æ®è®¾è®¡çš„å¯¹è¯åŠ©ç†â€”â€”EarthDialï¼Œå¯å°†å¤æ‚çš„å¤šæ„Ÿå®˜åœ°çƒè§‚æµ‹è½¬åŒ–ä¸ºäº¤äº’å¼è‡ªç„¶è¯­è¨€å¯¹è¯ã€‚EarthDialæ”¯æŒå¤šå…‰è°±ã€å¤šæ—¶ç›¸å’Œå¤šåˆ†è¾¨ç‡çš„å½±åƒï¼Œå¯å®Œæˆä¸€ç³»åˆ—é¥æ„Ÿä»»åŠ¡ï¼ŒåŒ…æ‹¬åˆ†ç±»ã€æ£€æµ‹ã€æè¿°ã€é—®ç­”ã€è§†è§‰æ¨ç†å’Œè§†è§‰å®šä½ã€‚ä¸ºè¾¾æˆè¿™ä¸€ç›®æ ‡ï¼Œæˆ‘ä»¬æ¨å‡ºäº†ä¸€æ¬¾åŒ…å«è¶…è¿‡1åƒ1ç™¾ä¸‡æŒ‡ä»¤å¯¹çš„åºå¤§æŒ‡ä»¤è°ƒä¼˜æ•°æ®é›†ï¼Œæ¶µç›–RGBã€åˆæˆå­”å¾„é›·è¾¾ï¼ˆSARï¼‰å’Œå¤šå…‰è°±æ¨¡å¼å¦‚è¿‘çº¢å¤–ï¼ˆNIRï¼‰å’Œçº¢å¤–ç­‰ã€‚æ­¤å¤–ï¼ŒEarthDialè¿˜å¯å¤„ç†åŒæ—¶åºå’Œå¤šæ—¶åºåºåˆ—åˆ†æï¼Œç”¨äºå˜åŒ–æ£€æµ‹ç­‰åº”ç”¨ã€‚åœ¨å¹¿æ³›çš„å®éªŒå’Œä¸‹æ¸¸æ•°æ®é›†ä¸Šçš„ç»“æœè¡¨æ˜ï¼ŒEarthDialä¼˜äºç°æœ‰çš„é€šç”¨å’Œç‰¹å®šé¢†åŸŸçš„æ¨¡å‹ï¼Œåœ¨å„ç§EOä»»åŠ¡ä¸­å®ç°äº†æ›´å¥½çš„æ³›åŒ–èƒ½åŠ›ã€‚æˆ‘ä»¬çš„æºä»£ç å’Œé¢„å…ˆè®­ç»ƒå¥½çš„æ¨¡å‹å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/hiyamdebary/EarthDial%E4%B8%AD%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/hiyamdebary/EarthDialä¸­æ‰¾åˆ°ã€‚</a></p>
<p><strong>Key Takeaways</strong>:</p>
<ol>
<li>EarthDialåˆ©ç”¨äº¤äº’å¼è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰å¤„ç†åœ°çƒè§‚æµ‹æ•°æ®ï¼Œå®ç°ç¯å¢ƒç›‘æµ‹ã€ç¾å®³å“åº”å’Œèµ„æºç®¡ç†çš„åˆ›æ–°åº”ç”¨ã€‚</li>
<li>ç°æœ‰VLMsåœ¨é¥æ„Ÿæ•°æ®ä¸Šè¡¨ç°ä¸è¶³ï¼Œè€Œåœ°ç†ç©ºé—´VLMsä»å—é™åœ¨å›ºå®šåˆ†è¾¨ç‡å’Œæœ‰é™ä¼ æ„Ÿå™¨æ¨¡å¼ä¸Šã€‚</li>
<li>EarthDialè®¾è®¡ç”¨äºå¤„ç†å¤šå…‰è°±ã€å¤šæ—¶ç›¸å’Œå¤šåˆ†è¾¨ç‡çš„å½±åƒæ•°æ®ï¼Œæ”¯æŒä¸€ç³»åˆ—é¥æ„Ÿä»»åŠ¡ã€‚</li>
<li>EarthDialé€šè¿‡å¤§è§„æ¨¡æŒ‡ä»¤è°ƒä¼˜æ•°æ®é›†å®ç°é«˜æ•ˆæ€§èƒ½ï¼Œæ¶µç›–å¤šç§ä¼ æ„Ÿå™¨æ¨¡å¼ã€‚</li>
<li>EarthDialå…·å¤‡å¤„ç†åŒæ—¶åºå’Œå¤šæ—¶åºåºåˆ—åˆ†æèƒ½åŠ›ï¼Œé€‚ç”¨äºå˜åŒ–æ£€æµ‹ç­‰åº”ç”¨ã€‚</li>
<li>å®éªŒç»“æœè¡¨æ˜ï¼ŒEarthDialåœ¨å¤šç§åœ°çƒè§‚æµ‹ä»»åŠ¡ä¸­è¡¨ç°ä¼˜äºç°æœ‰æ¨¡å‹ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.15190">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-25793f7de58c65d5c2c17255ce86db71.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-52e8696cfd7e236fbdae32706e2ab6e5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-63f8c3d4223882f779e76f50ea872089.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-032964f5ee4415fbf4ba61c49b63ef61.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0f92d0aa3d0e003e776638f3d9c3cee5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8f6e2a331988a050ce4c14158d45db1b.jpg" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="Training-Dynamics-of-a-1-7B-LLaMa-Model-A-Data-Efficient-Approach"><a href="#Training-Dynamics-of-a-1-7B-LLaMa-Model-A-Data-Efficient-Approach" class="headerlink" title="Training Dynamics of a 1.7B LLaMa Model: A Data-Efficient Approach"></a>Training Dynamics of a 1.7B LLaMa Model: A Data-Efficient Approach</h2><p><strong>Authors:Miles Q. Li, Benjamin C. M. Fung, Shih-Chia Huang</strong></p>
<p>Pretraining large language models is a complex endeavor influenced by multiple factors, including model architecture, data quality, training continuity, and hardware constraints. In this paper, we share insights gained from the experience of training DMaS-LLaMa-Lite, a fully open source, 1.7-billion-parameter, LLaMa-based model, on approximately 20 billion tokens of carefully curated data. We chronicle the full training trajectory, documenting how evolving validation loss levels and downstream benchmarks reflect transitions from incoherent text to fluent, contextually grounded output. Beyond pretraining, we extend our analysis to include a post-training phase focused on instruction tuning, where the model was refined to produce more contextually appropriate, user-aligned responses. We highlight practical considerations such as the importance of restoring optimizer states when resuming from checkpoints, and the impact of hardware changes on training stability and throughput. While qualitative evaluation provides an intuitive understanding of model improvements, our analysis extends to various performance benchmarks, demonstrating how high-quality data and thoughtful scaling enable competitive results with significantly fewer training tokens. By detailing these experiences and offering training logs, checkpoints, and sample outputs, we aim to guide future researchers and practitioners in refining their pretraining strategies. The training script is available on Github at <a target="_blank" rel="noopener" href="https://github.com/McGill-DMaS/DMaS-LLaMa-Lite-Training-Code">https://github.com/McGill-DMaS/DMaS-LLaMa-Lite-Training-Code</a>. The model checkpoints are available on Huggingface at <a target="_blank" rel="noopener" href="https://huggingface.co/collections/McGill-DMaS/dmas-llama-lite-6761d97ba903f82341954ceb">https://huggingface.co/collections/McGill-DMaS/dmas-llama-lite-6761d97ba903f82341954ceb</a>. </p>
<blockquote>
<p>é¢„è®­ç»ƒå¤§å‹è¯­è¨€æ¨¡å‹æ˜¯ä¸€é¡¹å¤æ‚çš„ä»»åŠ¡ï¼Œå—åˆ°å¤šç§å› ç´ çš„å½±å“ï¼ŒåŒ…æ‹¬æ¨¡å‹æ¶æ„ã€æ•°æ®è´¨é‡ã€è®­ç»ƒè¿ç»­æ€§å’Œç¡¬ä»¶é™åˆ¶ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬åˆ†äº«äº†è®­ç»ƒDMaS-LLaMa-Liteæ¨¡å‹çš„å®æˆ˜ç»éªŒã€‚è¿™æ˜¯ä¸€ä¸ªå®Œå…¨å¼€æºçš„ã€åŸºäºLLaMaçš„1.7äº¿å‚æ•°æ¨¡å‹ï¼Œåœ¨çº¦20äº¿æ ‡è®°çš„ç²¾å¿ƒæŒ‘é€‰çš„æ•°æ®ä¸Šè¿›è¡Œè®­ç»ƒã€‚æˆ‘ä»¬è¯¦ç»†è®°å½•äº†æ•´ä¸ªè®­ç»ƒè½¨è¿¹ï¼Œæè¿°äº†éªŒè¯æŸå¤±æ°´å¹³çš„æ¼”å˜å’Œä¸‹æ¸¸åŸºå‡†æµ‹è¯•å¦‚ä½•åæ˜ ä»æ–‡æœ¬ä¸è¿è´¯åˆ°æµç•…ã€åŸºäºä¸Šä¸‹æ–‡è¾“å‡ºçš„è½¬å˜ã€‚é™¤äº†é¢„è®­ç»ƒé˜¶æ®µå¤–ï¼Œæˆ‘ä»¬å°†åˆ†ææ‰©å±•åˆ°äº†åŒ…æ‹¬ä¾§é‡äºæŒ‡ä»¤è°ƒæ•´çš„åæœŸè®­ç»ƒé˜¶æ®µï¼Œåœ¨è¿™ä¸ªé˜¶æ®µï¼Œå¯¹æ¨¡å‹è¿›è¡Œäº†æ”¹è¿›ï¼Œä»¥äº§ç”Ÿæ›´ç¬¦åˆä¸Šä¸‹æ–‡ã€æ›´ç¬¦åˆç”¨æˆ·æ„å›¾çš„å“åº”ã€‚æˆ‘ä»¬å¼ºè°ƒäº†å®é™…è€ƒè™‘å› ç´ ï¼Œå¦‚æ¢å¤ä¼˜åŒ–å™¨çŠ¶æ€æ—¶æ¢å¤æ£€æŸ¥ç‚¹çš„é‡è¦æ€§ï¼Œä»¥åŠç¡¬ä»¶æ›´æ”¹å¯¹è®­ç»ƒç¨³å®šæ€§å’Œååé‡çš„å½±å“ã€‚è™½ç„¶å®šæ€§è¯„ä¼°ä¸ºæ¨¡å‹æ”¹è¿›æä¾›äº†ç›´è§‚çš„ç†è§£ï¼Œä½†æˆ‘ä»¬çš„åˆ†æè¿˜æ¶µç›–äº†å„ç§æ€§èƒ½åŸºå‡†æµ‹è¯•ï¼Œè¯æ˜äº†é«˜è´¨é‡æ•°æ®å’Œæœ‰ç­–ç•¥çš„è§„æ¨¡æ‰©å±•å¦‚ä½•åœ¨æ˜¾è‘—å‡å°‘è®­ç»ƒæ ‡è®°çš„æƒ…å†µä¸‹å®ç°å…·æœ‰ç«äº‰åŠ›çš„ç»“æœã€‚é€šè¿‡è¯¦ç»†è®°å½•è¿™äº›ç»éªŒå¹¶æä¾›è®­ç»ƒæ—¥å¿—ã€æ£€æŸ¥ç‚¹å’Œæ ·æœ¬è¾“å‡ºï¼Œæˆ‘ä»¬çš„ç›®æ ‡æ˜¯æŒ‡å¯¼æœªæ¥çš„ç ”ç©¶è€…å’Œå®è·µè€…æ”¹è¿›ä»–ä»¬çš„é¢„è®­ç»ƒç­–ç•¥ã€‚è®­ç»ƒè„šæœ¬å¯åœ¨Githubä¸Šæ‰¾åˆ°ï¼š<a target="_blank" rel="noopener" href="https://github.com/McGill-DMaS/DMaS-LLaMa-Lite-Training-Code%E3%80%82%E6%A8%A1%E5%9E%8B%E6%A3%80%E7%82%B9%E5%8F%AF%E5%9C%A8Huggingface%E4%B8%8A%E6%89%BE%E5%88%B0%EF%BC%9Ahttps://huggingface.co/collections/McGill-DMaS/dmas-llama-lite-6761d97ba903f82341954ceb%E3%80%82">https://github.com/McGill-DMaS/DMaS-LLaMa-Lite-Training-Codeã€‚æ¨¡å‹æ£€æŸ¥ç‚¹å¯åœ¨Huggingfaceä¸Šæ‰¾åˆ°ï¼šhttps://huggingface.co/collections/McGill-DMaS/dmas-llama-lite-6761d97ba903f82341954cebã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.13335v3">PDF</a> </p>
<p><strong>æ‘˜è¦</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†è®­ç»ƒDMaS-LLaMa-Liteè¿™ä¸€å¼€æºå¤§å‹è¯­è¨€æ¨¡å‹çš„å®è·µç»éªŒã€‚æ¨¡å‹åŸºäºLLaMaæ¶æ„ï¼Œæ‹¥æœ‰çº¦1.7äº¿å‚æ•°ï¼Œå¹¶åœ¨çº¦20äº¿æ ‡è®°çš„ç²¾å¿ƒç­›é€‰çš„æ•°æ®ä¸Šè¿›è¡Œè®­ç»ƒã€‚æ–‡ç« è¯¦ç»†æè¿°äº†è®­ç»ƒè¿‡ç¨‹ï¼ŒåŒ…æ‹¬éªŒè¯æŸå¤±çš„å˜åŒ–å’Œä¸‹æ¸¸åŸºå‡†æµ‹è¯•çš„è¡¨ç°ã€‚æ­¤å¤–ï¼Œè¿˜ä»‹ç»äº†æ¨¡å‹åœ¨è®­ç»ƒåçš„æŒ‡ä»¤å¾®è°ƒé˜¶æ®µã€‚æ–‡ç« å¼ºè°ƒäº†å®é™…è€ƒè™‘å› ç´ ï¼Œå¦‚æ¢å¤æ£€æŸ¥ç‚¹æ—¶ä¼˜åŒ–å™¨çŠ¶æ€çš„é‡è¦æ€§ä»¥åŠç¡¬ä»¶å˜åŒ–å¯¹è®­ç»ƒç¨³å®šæ€§å’Œååé‡çš„å½±å“ã€‚é€šè¿‡æ€§èƒ½åŸºå‡†æµ‹è¯•ï¼Œå±•ç¤ºäº†é«˜è´¨é‡æ•°æ®å’Œæœ‰é’ˆå¯¹æ€§çš„æ‰©å±•å¦‚ä½•ä½¿æ¨¡å‹åœ¨æ˜¾è‘—è¾ƒå°‘çš„è®­ç»ƒæ ‡è®°ä¸Šå®ç°ç«äº‰ç»“æœã€‚æœ¬æ–‡æ—¨åœ¨æŒ‡å¯¼æœªæ¥ç ”ç©¶è€…å’Œå®è·µè€…åœ¨é¢„è®­ç»ƒç­–ç•¥æ–¹é¢çš„æ”¹è¿›ã€‚ç›¸å…³èµ„æºé“¾æ¥å·²æä¾›ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>è®­ç»ƒå¤§å‹è¯­è¨€æ¨¡å‹æ˜¯ä¸€ä¸ªå—å¤šç§å› ç´ å½±å“çš„å¤æ‚è¿‡ç¨‹ï¼ŒåŒ…æ‹¬æ¨¡å‹æ¶æ„ã€æ•°æ®è´¨é‡ã€è®­ç»ƒè¿ç»­æ€§å’Œç¡¬ä»¶é™åˆ¶ã€‚</li>
<li>DMaS-LLaMa-Liteæ˜¯ä¸€ä¸ªåŸºäºLLaMaçš„å¼€æºæ¨¡å‹ï¼Œå…¶è®­ç»ƒè¿‡ç¨‹åŒ…æ‹¬é¢„è®­ç»ƒå’ŒæŒ‡ä»¤å¾®è°ƒé˜¶æ®µã€‚</li>
<li>éªŒè¯æŸå¤±çš„å˜åŒ–å’Œä¸‹æ¸¸åŸºå‡†æµ‹è¯•çš„è¡¨ç°åæ˜ äº†æ¨¡å‹ä»æ— åºæ–‡æœ¬åˆ°æµç•…ã€è¯­å¢ƒä¸°å¯Œçš„è¾“å‡ºçš„è¿‡æ¸¡ã€‚</li>
<li>æ¢å¤ä¼˜åŒ–å™¨çŠ¶æ€åœ¨æ¢å¤æ£€æŸ¥ç‚¹æ—¶è‡³å…³é‡è¦ï¼Œç¡¬ä»¶å˜åŒ–ä¼šå½±å“è®­ç»ƒçš„ç¨³å®šæ€§å’Œæ•ˆç‡ã€‚</li>
<li>é«˜è´¨é‡æ•°æ®å’Œæœ‰é’ˆå¯¹æ€§çš„æ‰©å±•å¯ä»¥æ˜¾è‘—æé«˜æ¨¡å‹çš„æ€§èƒ½ï¼Œä½¿ç”¨æ›´å°‘çš„è®­ç»ƒæ ‡è®°å³å¯å®ç°ç«äº‰ç»“æœã€‚</li>
<li>æ–‡ç« æä¾›äº†è¯¦ç»†çš„å®è·µç»éªŒã€è®­ç»ƒæ—¥å¿—ã€æ£€æŸ¥ç‚¹å’Œæ ·æœ¬è¾“å‡ºï¼Œæ—¨åœ¨æŒ‡å¯¼æœªæ¥ç ”ç©¶è€…æ”¹è¿›é¢„è®­ç»ƒç­–ç•¥ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.13335">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-be1c70d0da462dcc0e5dc4485fde017c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7e0277e3f2b1892c5a528caba3b16030.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-55c412a006cb8ba62aa60b2f38587d55.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-937b297e330cc2ba678a0faf24781a9e.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-d042aa459379d6e77f6fa89e9508e1d1.jpg" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="GReaTer-Gradients-over-Reasoning-Makes-Smaller-Language-Models-Strong-Prompt-Optimizers"><a href="#GReaTer-Gradients-over-Reasoning-Makes-Smaller-Language-Models-Strong-Prompt-Optimizers" class="headerlink" title="GReaTer: Gradients over Reasoning Makes Smaller Language Models Strong   Prompt Optimizers"></a>GReaTer: Gradients over Reasoning Makes Smaller Language Models Strong   Prompt Optimizers</h2><p><strong>Authors:Sarkar Snigdha Sarathi Das, Ryo Kamoi, Bo Pang, Yusen Zhang, Caiming Xiong, Rui Zhang</strong></p>
<p>The effectiveness of large language models (LLMs) is closely tied to the design of prompts, making prompt optimization essential for enhancing their performance across a wide range of tasks. Many existing approaches to automating prompt engineering rely exclusively on textual feedback, refining prompts based solely on inference errors identified by large, computationally expensive LLMs. Unfortunately, smaller models struggle to generate high-quality feedback, resulting in complete dependence on large LLM judgment. Moreover, these methods fail to leverage more direct and finer-grained information, such as gradients, due to operating purely in text space. To this end, we introduce GReaTer, a novel prompt optimization technique that directly incorporates gradient information over task-specific reasoning. By utilizing task loss gradients, GReaTer enables self-optimization of prompts for open-source, lightweight language models without the need for costly closed-source LLMs. This allows high-performance prompt optimization without dependence on massive LLMs, closing the gap between smaller models and the sophisticated reasoning often needed for prompt refinement. Extensive evaluations across diverse reasoning tasks including BBH, GSM8k, and FOLIO demonstrate that GReaTer consistently outperforms previous state-of-the-art prompt optimization methods, even those reliant on powerful LLMs. Additionally, GReaTer-optimized prompts frequently exhibit better transferability and, in some cases, boost task performance to levels comparable to or surpassing those achieved by larger language models, highlighting the effectiveness of prompt optimization guided by gradients over reasoning. Code of GReaTer is available at <a target="_blank" rel="noopener" href="https://github.com/psunlpgroup/GreaTer">https://github.com/psunlpgroup/GreaTer</a>. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æœ‰æ•ˆæ€§ä¸å…¶æç¤ºè®¾è®¡å¯†åˆ‡ç›¸å…³ï¼Œè¿™ä½¿å¾—æç¤ºä¼˜åŒ–å¯¹äºæé«˜å…¶åœ¨å¹¿æ³›ä»»åŠ¡ä¸­çš„æ€§èƒ½è‡³å…³é‡è¦ã€‚è®¸å¤šç°æœ‰çš„è‡ªåŠ¨åŒ–æç¤ºå·¥ç¨‹æ–¹æ³•å®Œå…¨ä¾èµ–äºæ–‡æœ¬åé¦ˆï¼Œä»…æ ¹æ®å¤§å‹ã€è®¡ç®—æˆæœ¬é«˜æ˜‚çš„LLMæ‰€è¯†åˆ«åˆ°çš„æ¨ç†é”™è¯¯æ¥ä¼˜åŒ–æç¤ºã€‚ç„¶è€Œï¼Œå°å‹æ¨¡å‹åœ¨ç”Ÿæˆé«˜è´¨é‡åé¦ˆæ–¹é¢è¡¨ç°æŒ£æ‰ï¼Œå¯¼è‡´å®Œå…¨ä¾èµ–å¤§å‹LLMçš„åˆ¤æ–­ã€‚æ­¤å¤–ï¼Œè¿™äº›æ–¹æ³•æœªèƒ½åˆ©ç”¨æ›´ç›´æ¥å’Œæ›´ç²¾ç»†çš„ä¿¡æ¯ï¼Œå¦‚æ¢¯åº¦ï¼Œå› ä¸ºå®ƒä»¬ä»…åœ¨æ–‡æœ¬ç©ºé—´ä¸­è¿›è¡Œæ“ä½œã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬å¼•å…¥äº†GReaTerï¼Œä¸€ç§æ–°çš„æç¤ºä¼˜åŒ–æŠ€æœ¯ï¼Œå®ƒå¯ä»¥ç›´æ¥ç»“åˆä»»åŠ¡ç‰¹å®šæ¨ç†çš„æ¢¯åº¦ä¿¡æ¯ã€‚é€šè¿‡åˆ©ç”¨ä»»åŠ¡æŸå¤±æ¢¯åº¦ï¼ŒGReaTerèƒ½å¤Ÿè‡ªè¡Œä¼˜åŒ–å¼€æ”¾æºä»£ç ã€è½»é‡çº§è¯­è¨€æ¨¡å‹çš„æç¤ºï¼Œè€Œæ— éœ€æ˜‚è´µçš„å°é—­æºä»£ç LLMã€‚è¿™å…è®¸è¿›è¡Œé«˜æ€§èƒ½çš„æç¤ºä¼˜åŒ–ï¼Œè€Œä¸ä¾èµ–äºå¤§è§„æ¨¡LLMï¼Œç¼©å°äº†å°å‹æ¨¡å‹å’Œç»å¸¸éœ€è¦è¿›è¡Œæç¤ºä¼˜åŒ–çš„å¤æ‚æ¨ç†ä¹‹é—´çš„å·®è·ã€‚åœ¨åŒ…æ‹¬BBHã€GSM8kå’ŒFOLIOç­‰å¤šæ ·åŒ–çš„æ¨ç†ä»»åŠ¡ä¸Šçš„å¹¿æ³›è¯„ä¼°è¡¨æ˜ï¼ŒGReaTerå§‹ç»ˆä¼˜äºå…ˆå‰çš„æœ€å…ˆè¿›çš„æç¤ºä¼˜åŒ–æ–¹æ³•ï¼Œç”šè‡³è¶…è¿‡äº†é‚£äº›ä¾èµ–å¼ºå¤§LLMçš„æ–¹æ³•ã€‚æ­¤å¤–ï¼Œç»è¿‡GReaTerä¼˜åŒ–çš„æç¤ºé€šå¸¸è¡¨ç°å‡ºæ›´å¥½çš„å¯è¿ç§»æ€§ï¼Œåœ¨æŸäº›æƒ…å†µä¸‹ï¼Œä»»åŠ¡æ€§èƒ½æå‡åˆ°äº†ä¸å¤§å‹è¯­è¨€æ¨¡å‹ç›¸å½“ç”šè‡³æ›´é«˜çš„æ°´å¹³ï¼Œè¿™çªæ˜¾äº†ç”±æ¢¯åº¦å¼•å¯¼çš„æç¤ºä¼˜åŒ–çš„æœ‰æ•ˆæ€§ã€‚GReaTerçš„ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/psunlpgroup/GreaTer%E4%B8%8A%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/psunlpgroup/GreaTerä¸Šæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.09722v2">PDF</a> ICLR 2025 Camera Ready</p>
<p><strong>æ‘˜è¦</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æœ‰æ•ˆæ€§ä¸å…¶æç¤ºè®¾è®¡å¯†åˆ‡ç›¸å…³ï¼Œæç¤ºä¼˜åŒ–å¯¹äºæé«˜å…¶åœ¨å„ç§ä»»åŠ¡ä¸­çš„æ€§èƒ½è‡³å…³é‡è¦ã€‚ç°æœ‰çš„è‡ªåŠ¨æç¤ºå·¥ç¨‹æ–¹æ³•å¤§å¤šä¾èµ–äºæ–‡æœ¬åé¦ˆï¼Œä»…å‡­å¤§å‹ä¸”è®¡ç®—æ˜‚è´µçš„LLMè¯†åˆ«çš„æ¨ç†é”™è¯¯æ¥ä¼˜åŒ–æç¤ºã€‚ç„¶è€Œï¼Œå°å‹æ¨¡å‹åœ¨ç”Ÿæˆé«˜è´¨é‡åé¦ˆæ–¹é¢è¡¨ç°æŒ£æ‰ï¼Œå¯¼è‡´å®Œå…¨ä¾èµ–å¤§å‹LLMçš„åˆ¤æ–­ã€‚æ­¤å¤–ï¼Œè¿™äº›æ–¹æ³•æœªèƒ½åˆ©ç”¨æ›´ç›´æ¥å’Œç²¾ç»†çš„ä¿¡æ¯ï¼Œå¦‚æ¢¯åº¦ï¼Œå› ä¸ºå®ƒä»¬ä»…åœ¨æ–‡æœ¬ç©ºé—´å†…è¿è¡Œã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬å¼•å…¥äº†GReaTerï¼Œä¸€ç§æ–°å‹æç¤ºä¼˜åŒ–æŠ€æœ¯ï¼Œè¯¥æŠ€æœ¯ç›´æ¥ç»“åˆä»»åŠ¡ç‰¹å®šæ¨ç†çš„æ¢¯åº¦ä¿¡æ¯æ¥è¿›è¡Œè‡ªæˆ‘ä¼˜åŒ–ã€‚é€šè¿‡åˆ©ç”¨ä»»åŠ¡æŸå¤±æ¢¯åº¦ï¼ŒGReaTerå¯ä»¥ä½¿å¼€æ”¾æºä»£ç ã€è½»é‡çº§è¯­è¨€æ¨¡å‹è¿›è¡Œè‡ªæˆ‘ä¼˜åŒ–æç¤ºï¼Œæ— éœ€æ˜‚è´µçš„é—­æºLLMsã€‚è¿™ä½¿å¾—åœ¨æ²¡æœ‰ä¾èµ–å¤§å‹LLMçš„æƒ…å†µä¸‹è¿›è¡Œé«˜æ€§èƒ½æç¤ºä¼˜åŒ–æˆä¸ºå¯èƒ½ï¼Œç¼©å°äº†å°å‹æ¨¡å‹ä¸ç»å¸¸éœ€è¦è¿›è¡Œæç¤ºä¼˜åŒ–çš„ç²¾ç»†æ¨ç†ä¹‹é—´çš„å·®è·ã€‚åœ¨åŒ…æ‹¬BBHã€GSM8kå’ŒFOLIOåœ¨å†…çš„å„ç§æ¨ç†ä»»åŠ¡ä¸Šçš„å¹¿æ³›è¯„ä¼°è¡¨æ˜ï¼ŒGReaTerå§‹ç»ˆä¼˜äºå…ˆå‰çš„æœ€æ–°æç¤ºä¼˜åŒ–æ–¹æ³•ï¼Œç”šè‡³ä¼˜äºä¾èµ–å¼ºå¤§LLMsçš„æ–¹æ³•ã€‚æ­¤å¤–ï¼ŒGReaTerä¼˜åŒ–çš„æç¤ºé€šå¸¸å…·æœ‰æ›´å¥½çš„å¯è¿ç§»æ€§ï¼Œå¹¶ä¸”åœ¨æŸäº›æƒ…å†µä¸‹ï¼Œä»»åŠ¡æ€§èƒ½æå‡åˆ°äº†å¯ä¸æˆ–è¶…è¶Šå¤§å‹è¯­è¨€æ¨¡å‹å®ç°çš„æ°´å¹³ï¼Œçªæ˜¾äº†ç”±æ¢¯åº¦å¼•å¯¼æç¤ºä¼˜åŒ–çš„æœ‰æ•ˆæ€§ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>æç¤ºä¼˜åŒ–å¯¹äºæé«˜LLMåœ¨å„ç§ä»»åŠ¡ä¸­çš„æ€§èƒ½è‡³å…³é‡è¦ã€‚</li>
<li>ç°æœ‰æ–¹æ³•ä¸»è¦ä¾èµ–æ–‡æœ¬åé¦ˆå’Œå¤§å‹LLMè¿›è¡Œæç¤ºä¼˜åŒ–ã€‚</li>
<li>GReaTeræ˜¯ä¸€ç§æ–°å‹æç¤ºä¼˜åŒ–æŠ€æœ¯ï¼Œç»“åˆæ¢¯åº¦ä¿¡æ¯è¿›è¡Œè‡ªæˆ‘ä¼˜åŒ–ã€‚</li>
<li>GReaTerä½¿å°å‹è¯­è¨€æ¨¡å‹èƒ½å¤Ÿè¿›è¡Œé«˜æ€§èƒ½æç¤ºä¼˜åŒ–ï¼Œæ— éœ€ä¾èµ–å¤§å‹LLMã€‚</li>
<li>GReaTeråœ¨å¤šç§æ¨ç†ä»»åŠ¡ä¸Šçš„è¡¨ç°ä¼˜äºå…¶ä»–æœ€æ–°æç¤ºä¼˜åŒ–æ–¹æ³•ã€‚</li>
<li>GReaTerä¼˜åŒ–çš„æç¤ºå…·æœ‰æ›´å¥½çš„å¯è¿ç§»æ€§ã€‚</li>
<li>GReaTeræå‡ä»»åŠ¡æ€§èƒ½è‡³ä¸å¤§å‹è¯­è¨€æ¨¡å‹ç›¸å½“æˆ–æ›´é«˜çš„æ°´å¹³ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.09722">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-edc278df9837fdde390c500c45f90aff.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-c34d7cf6e29187574c467f86bdf44ae4.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e0ea993ae1ec58390056fb9fa4f32d4f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4e0e4c6c71b3dc812cf6021bb302be58.jpg" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1><h2 id="Spider-Any-to-Many-Multimodal-LLM"><a href="#Spider-Any-to-Many-Multimodal-LLM" class="headerlink" title="Spider: Any-to-Many Multimodal LLM"></a>Spider: Any-to-Many Multimodal LLM</h2><p><strong>Authors:Jinxiang Lai, Jie Zhang, Jun Liu, Jian Li, Xiaocheng Lu, Song Guo</strong></p>
<p>Multimodal LLMs (MLLMs) have emerged as an extension of Large Language Models (LLMs), enabling the integration of various modalities. However, Any-to-Any MLLMs are limited to generating pairwise modalities â€˜Text + Xâ€™ within a single response, such as Text + {Image or Audio or Video}. To address this limitation, we introduce Spider, a novel efficient Any-to-Many Modalities Generation (AMMG) framework, which can generate an arbitrary combination of modalities â€˜Text + Xsâ€™, such as Text + {Image and Audio and Video}. To achieve efficient AMMG, our Spider integrates three core components: a Base Model for basic X-to-X (i.e., Any-to-Any) modality processing, an Any-to-Many Instruction Template designed for producing Xs signal prompts, and a novel Efficient Decoders-Controller for controlling multimodal Decoders to generate Xs (many-modal) contents. To train Spider, we constructed a novel Text-formatted Many-Modal (TMM) dataset, which facilitates learning the X-to-Xs (i.e., Any-to-Many) capability necessary for AMMG. Ultimately, the well-trained Spider generates a pseudo X-to-Xs dataset, the first-ever X-to-Xs many-modal dataset, enhancing the potential for AMMG tasks in future research. Overall, this work not only pushes the boundary of multimodal interaction but also provides rich data support for advancing the field. Code: <a target="_blank" rel="noopener" href="https://github.com/Layjins/Spider">https://github.com/Layjins/Spider</a> </p>
<blockquote>
<p>å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰ä½œä¸ºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„æ‰©å±•è€Œå‡ºç°ï¼Œèƒ½å¤Ÿå®ç°å¯¹å„ç§æ¨¡æ€çš„é›†æˆã€‚ç„¶è€Œï¼Œä»»ä½•åˆ°ä»»ä½•çš„å¤šæ¨¡æ€LLMsä»…é™äºåœ¨å•ä¸ªå“åº”ä¸­ç”Ÿæˆâ€œæ–‡æœ¬+Xâ€ï¼ˆå¦‚æ–‡æœ¬+å›¾åƒæˆ–éŸ³é¢‘æˆ–è§†é¢‘ï¼‰çš„é…å¯¹æ¨¡æ€ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é™åˆ¶ï¼Œæˆ‘ä»¬å¼•å…¥äº†Spiderï¼Œè¿™æ˜¯ä¸€ç§æ–°å‹é«˜æ•ˆçš„å¤šå¯¹å¤šæ¨¡æ€ç”Ÿæˆï¼ˆAMMGï¼‰æ¡†æ¶ï¼Œèƒ½å¤Ÿç”Ÿæˆä»»æ„ç»„åˆçš„æ¨¡æ€ï¼Œå¦‚æ–‡æœ¬+{å›¾åƒå’ŒéŸ³é¢‘å’Œè§†é¢‘}ã€‚ä¸ºäº†å®ç°é«˜æ•ˆçš„AMMGï¼Œæˆ‘ä»¬çš„Spideré›†æˆäº†ä¸‰ä¸ªæ ¸å¿ƒç»„ä»¶ï¼šç”¨äºåŸºæœ¬Xåˆ°Xï¼ˆå³ä»»ä½•åˆ°ä»»ä½•ï¼‰æ¨¡æ€å¤„ç†çš„åŸºå‡†æ¨¡å‹ã€ç”¨äºç”ŸæˆXä¿¡å·æç¤ºçš„ä»»ä½•åˆ°å¤šæŒ‡ä»¤æ¨¡æ¿ï¼Œä»¥åŠç”¨äºæ§åˆ¶å¤šæ¨¡æ€è§£ç å™¨ç”ŸæˆXï¼ˆå¤šæ¨¡æ€ï¼‰å†…å®¹çš„æ–°å‹é«˜æ•ˆè§£ç å™¨æ§åˆ¶å™¨ã€‚ä¸ºäº†è®­ç»ƒSpiderï¼Œæˆ‘ä»¬æ„å»ºäº†ä¸€ä¸ªæ–°å‹æ–‡æœ¬æ ¼å¼çš„å¤šæ¨¡æ€ï¼ˆTMMï¼‰æ•°æ®é›†ï¼Œè¯¥æ•°æ®é›†æœ‰åŠ©äºå­¦ä¹ è¿›è¡ŒAMMGæ‰€å¿…éœ€çš„Xåˆ°Xsï¼ˆå³ä»»ä½•åˆ°å¤šï¼‰èƒ½åŠ›ã€‚æœ€ç»ˆï¼Œç»è¿‡è‰¯å¥½è®­ç»ƒçš„Spiderç”Ÿæˆäº†ä¸€ä¸ªä¼ªXåˆ°Xsæ•°æ®é›†ï¼Œè¿™æ˜¯é¦–ä¸ªXåˆ°Xså¤šæ¨¡æ€æ•°æ®é›†ï¼Œå¢å¼ºäº†æœªæ¥ç ”ç©¶ä¸­AMMGä»»åŠ¡çš„æ½œåŠ›ã€‚æ€»ä½“è€Œè¨€ï¼Œè¿™é¡¹å·¥ä½œä¸ä»…æ¨åŠ¨äº†å¤šæ¨¡æ€äº¤äº’çš„è¾¹ç•Œï¼Œè€Œä¸”ä¸ºæ¨è¿›è¯¥é¢†åŸŸæä¾›äº†ä¸°å¯Œçš„æ•°æ®æ”¯æŒã€‚ä»£ç åœ°å€ï¼š<a target="_blank" rel="noopener" href="https://github.com/Layjins/Spider">https://github.com/Layjins/Spider</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2411.09439v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>åŸºäºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„å¤šæ¨¡æ€LLMï¼ˆMLLMï¼‰èƒ½å¤Ÿæ•´åˆå¤šç§æ¨¡æ€ã€‚ç„¶è€Œï¼Œç°æœ‰çš„Any-to-Any MLLMä»…é™äºåœ¨å•ä¸€å“åº”ä¸­ç”Ÿæˆé…å¯¹æ¨¡æ€â€œæ–‡æœ¬+Xâ€ï¼ˆå¦‚æ–‡æœ¬+å›¾åƒã€éŸ³é¢‘æˆ–è§†é¢‘ï¼‰ã€‚ä¸ºè§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æ¨å‡ºSpiderï¼Œä¸€ç§æ–°å‹é«˜æ•ˆçš„Any-to-Many Modalities Generationï¼ˆAMMGï¼‰æ¡†æ¶ï¼Œèƒ½å¤Ÿç”Ÿæˆä»»æ„çš„æ¨¡æ€ç»„åˆâ€œæ–‡æœ¬+Xsâ€ï¼ˆå¦‚æ–‡æœ¬+å›¾åƒã€éŸ³é¢‘å’Œè§†é¢‘ï¼‰ã€‚Spideré€šè¿‡ä¸‰ä¸ªæ ¸å¿ƒç»„ä»¶å®ç°é«˜æ•ˆçš„AMMGï¼šåŸºç¡€æ¨¡å‹ç”¨äºåŸºæœ¬çš„X-to-Xï¼ˆå³Any-to-Anyï¼‰æ¨¡æ€å¤„ç†ï¼ŒAny-to-ManyæŒ‡ä»¤æ¨¡æ¿ç”¨äºç”ŸæˆXsä¿¡å·æç¤ºï¼Œä»¥åŠæ§åˆ¶å¤šæ¨¡æ€è§£ç å™¨ç”ŸæˆXsï¼ˆå¤šæ¨¡æ€ï¼‰å†…å®¹çš„æ–°å‹é«˜æ•ˆè§£ç å™¨æ§åˆ¶å™¨ã€‚ä¸ºè®­ç»ƒSpiderï¼Œæˆ‘ä»¬æ„å»ºäº†æ–°é¢–çš„Text-formatted Many-Modalï¼ˆTMMï¼‰æ•°æ®é›†ï¼Œæœ‰åŠ©äºå­¦ä¹ è¿›è¡ŒAMMGæ‰€éœ€çš„X-to-Xsèƒ½åŠ›ã€‚æœ€ç»ˆï¼Œç»è¿‡è‰¯å¥½è®­ç»ƒçš„Spiderç”Ÿæˆäº†é¦–ä¸ªX-to-Xså¤šæ¨¡æ€æ•°æ®é›†ï¼Œå¢å¼ºäº†æœªæ¥ç ”ç©¶ä¸­AMMGä»»åŠ¡çš„æ½œåŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤šæ¨¡æ€LLMï¼ˆMLLMï¼‰æ˜¯LLMçš„æ‰©å±•ï¼Œæ”¯æŒå¤šç§æ¨¡æ€çš„é›†æˆã€‚</li>
<li>ç°æœ‰çš„Any-to-Any MLLMä»…é™äºç”Ÿæˆé…å¯¹æ¨¡æ€ã€‚</li>
<li>Spideræ˜¯ä¸€ç§æ–°å‹çš„Any-to-Many Modalities Generationï¼ˆAMMGï¼‰æ¡†æ¶ï¼Œèƒ½å¤Ÿç”Ÿæˆä»»æ„çš„æ¨¡æ€ç»„åˆã€‚</li>
<li>Spideré€šè¿‡ä¸‰ä¸ªæ ¸å¿ƒç»„ä»¶å®ç°é«˜æ•ˆçš„AMMGï¼šåŸºç¡€æ¨¡å‹ã€Any-to-ManyæŒ‡ä»¤æ¨¡æ¿å’Œé«˜æ•ˆè§£ç å™¨æ§åˆ¶å™¨ã€‚</li>
<li>ä¸ºè®­ç»ƒSpiderï¼Œæ„å»ºäº†Text-formatted Many-Modalï¼ˆTMMï¼‰æ•°æ®é›†ã€‚</li>
<li>Spiderç”Ÿæˆäº†é¦–ä¸ªX-to-Xså¤šæ¨¡æ€æ•°æ®é›†ã€‚</li>
<li>Spiderçš„ç ”ç©¶ä¸ä»…æ¨åŠ¨äº†å¤šæ¨¡æ€äº¤äº’çš„è¾¹ç•Œï¼Œè¿˜ä¸ºæœªæ¥ç ”ç©¶æä¾›äº†ä¸°å¯Œçš„æ•°æ®æ”¯æŒã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2411.09439">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-548fcaca13bdcf76ee11ff796479ac3a.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-263b0246f95c24205a660414b0cb20e3.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-4bf1f6e626846a87d9d4abeeabb6cb96.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e9019ca0b551072be75aa0869bce58cb.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-884958a466ce18977641de875607df0f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2aff988bcbd407e8298e46389e20712c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ea781f9cd0970958258bd57524c14a3d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e7928e4410bc9f273d01c23d699a334b.jpg" align="middle">
</details>


<h1 id="-19"><a href="#-19" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-04-09/LLM/">https://kedreamix.github.io/Talk2Paper/Paper/2025-04-09/LLM/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/LLM/">
                                    <span class="chip bg-color">LLM</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-04-09/Agent/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-ca7525f9b77e52545650c04458c2cc6c.jpg" class="responsive-img" alt="Agent">
                        
                        <span class="card-title">Agent</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Agent æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-04-09  CREA A Collaborative Multi-Agent Framework for Creative Content   Generation with Diffusion Models
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-04-09
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Agent/" class="post-category">
                                    Agent
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Agent/">
                        <span class="chip bg-color">Agent</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-04-09/R1_Reasoning/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-1d6f3f8c195b241546a73c58d51f2e92.jpg" class="responsive-img" alt="R1_Reasoning">
                        
                        <span class="card-title">R1_Reasoning</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            R1_Reasoning æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-04-09  InteractVLM 3D Interaction Reasoning from 2D Foundational Models
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-04-09
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/R1-Reasoning/" class="post-category">
                                    R1_Reasoning
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/R1-Reasoning/">
                        <span class="chip bg-color">R1_Reasoning</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">18588k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
