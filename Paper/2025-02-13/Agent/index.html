<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Agent">
    <meta name="description" content="Agent æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-02-13  Learning in Markets with Heterogeneous Agents Dynamics and Survival of   Bayesian vs. No-Regret Learners">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Agent | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-04fc13813f6549fb5d8d50b7fb684543.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Agent</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Agent/">
                                <span class="chip bg-color">Agent</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Agent/" class="post-category">
                                Agent
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-02-13
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-05-14
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    18.1k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    73 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-02-13-æ›´æ–°"><a href="#2025-02-13-æ›´æ–°" class="headerlink" title="2025-02-13 æ›´æ–°"></a>2025-02-13 æ›´æ–°</h1><h2 id="Learning-in-Markets-with-Heterogeneous-Agents-Dynamics-and-Survival-of-Bayesian-vs-No-Regret-Learners"><a href="#Learning-in-Markets-with-Heterogeneous-Agents-Dynamics-and-Survival-of-Bayesian-vs-No-Regret-Learners" class="headerlink" title="Learning in Markets with Heterogeneous Agents: Dynamics and Survival of   Bayesian vs. No-Regret Learners"></a>Learning in Markets with Heterogeneous Agents: Dynamics and Survival of   Bayesian vs. No-Regret Learners</h2><p><strong>Authors:David Easley, Yoav Kolumbus, Eva Tardos</strong></p>
<p>We analyze the performance of heterogeneous learning agents in asset markets with stochastic payoffs. Our agents aim to maximize the expected growth rate of their wealth but have different theories on how to learn this best. We focus on comparing Bayesian and no-regret learners in market dynamics. Bayesian learners with a prior over a finite set of models that assign positive prior probability to the correct model have posterior probabilities that converge exponentially to the correct model. Consequently, they survive even in the presence of agents who invest according to the correct model of the stochastic process. Bayesians with a continuum prior converge to the correct model at a rate of $O((\log T)&#x2F;T)$. Online learning theory provides no-regret algorithms for maximizing the log of wealth in this setting, achieving a worst-case regret bound of $O(\log T)$ without assuming a steady underlying stochastic process but comparing to the best fixed investment rule. This regret, as we observe, is of the same order of magnitude as that of a Bayesian learner with a continuum prior. However, we show that even such low regret may not be sufficient for survival in asset markets: an agent can have regret as low as $O(\log T)$, but still vanish in market dynamics when competing against agents who invest according to the correct model or even against a perfect Bayesian with a finite prior. On the other hand, we show that Bayesian learning is fragile, while no-regret learning requires less knowledge of the environment and is therefore more robust. Any no-regret learner will drive out of the market an imperfect Bayesian whose finite prior or update rule has even small errors. We formally establish the relationship between notions of survival, vanishing, and market domination studied in economics and the framework of regret minimization, thus bridging these theories. </p>
<blockquote>
<p>æˆ‘ä»¬åˆ†æäº†å¼‚æ„å­¦ä¹ ä»£ç†äººåœ¨å…·æœ‰éšæœºæ”¶ç›Šçš„èµ„äº§å¸‚åœºä¸­çš„è¡¨ç°ã€‚æˆ‘ä»¬çš„ä»£ç†äººæ—¨åœ¨æœ€å¤§åŒ–å…¶è´¢å¯Œçš„é¢„æœŸå¢é•¿ç‡ï¼Œä½†åœ¨å¦‚ä½•æœ€å¥½åœ°å®ç°è¿™ä¸€ç›®æ ‡æ–¹é¢å­˜åœ¨ä¸åŒçš„ç†è®ºã€‚æˆ‘ä»¬é‡ç‚¹å…³æ³¨æ¯”è¾ƒå¸‚åœºåŠ¨æ€çš„è´å¶æ–¯å­¦ä¹ å’Œæ— åæ‚”å­¦ä¹ è€…ã€‚å¸¦æœ‰æœ‰é™æ¨¡å‹é›†çš„å…ˆéªŒè´å¶æ–¯å­¦ä¹ è€…ä¼šå¯¹æ­£ç¡®çš„æ¨¡å‹åˆ†é…æ­£å…ˆéªŒæ¦‚ç‡ï¼Œå…¶åéªŒæ¦‚ç‡ä¼šä»¥æŒ‡æ•°å½¢å¼æ”¶æ•›åˆ°æ­£ç¡®çš„æ¨¡å‹ã€‚å› æ­¤ï¼Œå³ä½¿åœ¨å­˜åœ¨æ ¹æ®éšæœºè¿‡ç¨‹çš„æ­£ç¡®æ¨¡å‹è¿›è¡ŒæŠ•èµ„çš„ä»£ç†äººçš„æƒ…å†µä¸‹ï¼Œä»–ä»¬ä¹Ÿèƒ½ç”Ÿå­˜ä¸‹æ¥ã€‚å…·æœ‰è¿ç»­å…ˆéªŒçš„è´å¶æ–¯ä»¥O((\log T)&#x2F;T)çš„é€Ÿç‡æ”¶æ•›åˆ°æ­£ç¡®çš„æ¨¡å‹ã€‚åœ¨çº¿å­¦ä¹ ç†è®ºæä¾›äº†åœ¨è¿™ç§æƒ…å†µä¸‹æœ€å¤§åŒ–è´¢å¯Œå¯¹æ•°çš„æ— åæ‚”ç®—æ³•ï¼Œå³ä½¿æ²¡æœ‰å‡è®¾ç¨³å®šçš„æ½œåœ¨éšæœºè¿‡ç¨‹ï¼Œä¸æœ€ä½³å›ºå®šæŠ•èµ„è§„åˆ™ç›¸æ¯”ï¼Œä¹Ÿèƒ½è¾¾åˆ°æœ€åæƒ…å†µä¸‹çš„åæ‚”ç•Œé™ä¸ºO(\log T)ã€‚æ­£å¦‚æˆ‘ä»¬æ‰€è§‚å¯Ÿåˆ°çš„é‚£æ ·ï¼Œè¿™ç§åæ‚”ä¸å…·æœ‰è¿ç»­å…ˆéªŒçš„è´å¶æ–¯å­¦ä¹ è€…çš„åæ‚”ç¨‹åº¦ç›¸åŒã€‚ä½†æ˜¯ï¼Œæˆ‘ä»¬è¡¨æ˜ï¼Œå³ä½¿åœ¨èµ„äº§å¸‚åœºä¸­ï¼Œå¦‚æ­¤ä½çš„åæ‚”ä¹Ÿå¯èƒ½ä¸è¶³ä»¥ç”Ÿå­˜ï¼šä¸€ä¸ªä»£ç†äººçš„åæ‚”å¯èƒ½ä½è‡³O(\log T)ï¼Œä½†å½“ä¸æ ¹æ®æ­£ç¡®æ¨¡å‹è¿›è¡ŒæŠ•èµ„çš„ä»£ç†äººç«äº‰æ—¶ï¼Œç”šè‡³åœ¨é¢å¯¹å…·æœ‰æœ‰é™å…ˆéªŒæˆ–æ›´æ–°è§„åˆ™å­˜åœ¨å¾®å°é”™è¯¯çš„å®Œç¾è´å¶æ–¯æ—¶ï¼Œä»ä¼šåœ¨å¸‚åœºåŠ¨æ€ä¸­æ¶ˆå¤±ã€‚å¦ä¸€æ–¹é¢ï¼Œæˆ‘ä»¬è¯æ˜äº†è´å¶æ–¯å­¦ä¹ æ˜¯è„†å¼±çš„ï¼Œè€Œæ— åæ‚”çš„å­¦ä¹ éœ€è¦è¾ƒå°‘çš„ç¯å¢ƒçŸ¥è¯†ï¼Œå› æ­¤æ›´åŠ ç¨³å¥ã€‚ä»»ä½•æ— åæ‚”çš„å­¦ä¹ è€…éƒ½ä¼šå°†å­˜åœ¨å¾®å°é”™è¯¯çš„éå®Œç¾è´å¶æ–¯é€å‡ºå¸‚åœºã€‚æˆ‘ä»¬æ­£å¼å»ºç«‹äº†ç»æµå­¦ä¸­ç ”ç©¶çš„ç”Ÿå­˜ã€æ¶ˆå¤±ã€å¸‚åœºæ”¯é…åŠ›ä¸åæ‚”æœ€å°åŒ–çš„æ¡†æ¶ä¹‹é—´çš„å…³ç³»ï¼Œä»è€Œæ¡¥æ¥äº†è¿™äº›ç†è®ºã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.08597v1">PDF</a> Learning in Markets, Heterogeneous Agents, Regret and Survival</p>
<p><strong>æ‘˜è¦</strong></p>
<p>åœ¨éšæœºæ”¶ç›Šæ¡ä»¶ä¸‹åˆ†æå¼‚è´¨å­¦ä¹ ä¸»ä½“åœ¨èµ„äº§å¸‚åœºçš„è¡¨ç°ã€‚ä¸»ä½“ç›®æ ‡æ˜¯æœ€å¤§åŒ–è´¢å¯Œé¢„æœŸå¢é•¿ç‡ï¼Œä½†é‡‡ç”¨ä¸åŒçš„å­¦ä¹ æ–¹æ³•ã€‚æ–‡ç« èšç„¦äºæ¯”è¾ƒè´å¶æ–¯ä¸»ä½“å’Œæ— æ‚”å­¦ä¹ è€…åœ¨å¸‚åœºåŠ¨æ€ä¸­çš„è¡¨ç°ã€‚å¸¦æœ‰æœ‰é™æ¨¡å‹é›†çš„è´å¶æ–¯ä¸»ä½“åœ¨æ­£ç¡®çš„æ¨¡å‹ä¸Šå…·æœ‰ç§¯æçš„å…ˆéªŒæ¦‚ç‡ï¼Œå…¶åéªŒæ¦‚ç‡ä»¥æŒ‡æ•°æ–¹å¼æ”¶æ•›åˆ°æ­£ç¡®çš„æ¨¡å‹ï¼Œå³ä¾¿åœ¨å­˜åœ¨ä¾æ®éšæœºè¿‡ç¨‹æ­£ç¡®æ¨¡å‹è¿›è¡ŒæŠ•èµ„çš„ä¸»ä½“ä¸­ä¹Ÿèƒ½ç”Ÿå­˜ä¸‹æ¥ã€‚å…·æœ‰è¿ç»­å…ˆéªŒçš„è´å¶æ–¯ä¸»ä½“ä»¥O(logâ¡T)&#x2F;TO(\log T)&#x2F;TOï¼ˆlogTï¼‰&#x2F;T çš„é€Ÿåº¦æ”¶æ•›åˆ°æ­£ç¡®çš„æ¨¡å‹ã€‚åœ¨çº¿å­¦ä¹ ç†è®ºä¸ºæ­¤ç±»ç¯å¢ƒä¸­çš„æœ€å¤§åŒ–è´¢å¯Œå¯¹æ•°æä¾›äº†æ— æ‚”ç®—æ³•ï¼Œåœ¨æ²¡æœ‰å‡è®¾ç¨³å®šçš„éšæœºè¿‡ç¨‹çš„æƒ…å†µä¸‹ï¼Œå…¶æœ€åæƒ…å†µä¸‹çš„é—æ†¾ç•Œé™ä¸ºO(logâ¡T)O(\log T)Oï¼ˆlogTï¼‰ã€‚ç„¶è€Œï¼Œå³ä½¿é—æ†¾ç¨‹åº¦å¦‚æ­¤ä¹‹ä½ï¼Œä¹Ÿå¯èƒ½æ— æ³•ä¿è¯åœ¨èµ„äº§å¸‚åœºçš„ç”Ÿå­˜èƒ½åŠ›ï¼šä¸»ä½“çš„é—æ†¾å¯èƒ½ä½è‡³O(logâ¡T)O(\log T)Oï¼ˆlogTï¼‰ï¼Œä½†åœ¨ä¸ä¾æ®æ­£ç¡®æ¨¡å‹è¿›è¡ŒæŠ•èµ„çš„ä¸»ä½“ç”šè‡³ä¸æ‹¥æœ‰æœ‰é™å…ˆéªŒçš„å®Œç¾è´å¶æ–¯ä¸»ä½“ç«äº‰æ—¶ï¼Œä»å¯èƒ½ä¸§å¤±å¸‚åœºåœ°ä½ã€‚å¦ä¸€æ–¹é¢ï¼Œè´å¶æ–¯å­¦ä¹ æ˜¯è„†å¼±çš„ï¼Œè€Œæ— æ‚”å­¦ä¹ å¯¹ç¯å¢ƒçŸ¥è¯†çš„è¦æ±‚æ›´å°‘ï¼Œå› æ­¤æ›´ç¨³å¥ã€‚ä»»ä½•æ— æ‚”å­¦ä¹ è€…éƒ½å°†æŠŠæœ‰å¾®å°è¯¯å·®çš„ä¸å®Œç¾è´å¶æ–¯ä¸»ä½“æ’é™¤å‡ºå¸‚åœºã€‚æ–‡ç« æ­£å¼ç¡®ç«‹äº†ç»æµå­¦ä¸­çš„ç”Ÿå­˜ã€æ¶ˆå¤±ã€å¸‚åœºæ”¯é…ç­‰æ¦‚å¿µä¸åæ‚”æœ€å°åŒ–çš„æ¡†æ¶ä¹‹é—´çš„å…³ç³»ï¼Œä»è€Œæ¡¥æ¥äº†è¿™äº›ç†è®ºã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>åœ¨éšæœºæ”¶ç›Šæ¡ä»¶ä¸‹ï¼Œåˆ†æäº†å¼‚è´¨å­¦ä¹ ä¸»ä½“åœ¨èµ„äº§å¸‚åœºçš„è¡¨ç°ï¼Œä¸»ä½“é‡‡ç”¨ä¸åŒçš„å­¦ä¹ æ–¹æ³•æœ€å¤§åŒ–è´¢å¯Œé¢„æœŸå¢é•¿ç‡ã€‚</li>
<li>æ¯”è¾ƒäº†è´å¶æ–¯ä¸»ä½“å’Œæ— æ‚”å­¦ä¹ è€…åœ¨å¸‚åœºåŠ¨æ€ä¸­çš„è¡¨ç°ï¼Œå‘ç°è´å¶æ–¯å­¦ä¹ å…·æœ‰æ”¶æ•›é€Ÿåº¦å¿«çš„ç‰¹ç‚¹ï¼Œä½†ä¹Ÿå­˜åœ¨è„†å¼±æ€§ã€‚</li>
<li>æ— æ‚”å­¦ä¹ å¯¹ç¯å¢ƒçš„ä¾èµ–æ›´å°‘ï¼Œæ›´å…·ç¨³å¥æ€§ï¼Œå¯ä»¥æ’é™¤å¸‚åœºä¸­å­˜åœ¨å¾®å°è¯¯å·®çš„ä¸»ä½“ã€‚</li>
<li>åœ¨ç»æµå­¦ä¸­çš„ç”Ÿå­˜ã€æ¶ˆå¤±ã€å¸‚åœºæ”¯é…æ¦‚å¿µä¸åæ‚”æœ€å°åŒ–ç†è®ºä¹‹é—´å­˜åœ¨å¯†åˆ‡å…³ç³»ã€‚</li>
<li>ä¸»ä½“å³ä¾¿å…·æœ‰ä½åæ‚”åº¦ï¼Œä¹Ÿå¯èƒ½åœ¨ç«äº‰æ¿€çƒˆçš„èµ„äº§å¸‚åœºä¸­ä¸§å¤±ç”Ÿå­˜èƒ½åŠ›ã€‚</li>
<li>å®Œç¾è´å¶æ–¯ä¸»ä½“å…·æœ‰å¼ºå¤§çš„å¸‚åœºç«äº‰åŠ›ï¼Œå³ä½¿åœ¨é¢å¯¹æ— æ‚”å­¦ä¹ è€…æ—¶ä¹Ÿèƒ½ä¿æŒä¼˜åŠ¿ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.08597">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-a47ba33b97f296adf5f67c246522722a.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Human-Centric-Foundation-Models-Perception-Generation-and-Agentic-Modeling"><a href="#Human-Centric-Foundation-Models-Perception-Generation-and-Agentic-Modeling" class="headerlink" title="Human-Centric Foundation Models: Perception, Generation and Agentic   Modeling"></a>Human-Centric Foundation Models: Perception, Generation and Agentic   Modeling</h2><p><strong>Authors:Shixiang Tang, Yizhou Wang, Lu Chen, Yuan Wang, Sida Peng, Dan Xu, Wanli Ouyang</strong></p>
<p>Human understanding and generation are critical for modeling digital humans and humanoid embodiments. Recently, Human-centric Foundation Models (HcFMs) inspired by the success of generalist models, such as large language and vision models, have emerged to unify diverse human-centric tasks into a single framework, surpassing traditional task-specific approaches. In this survey, we present a comprehensive overview of HcFMs by proposing a taxonomy that categorizes current approaches into four groups: (1) Human-centric Perception Foundation Models that capture fine-grained features for multi-modal 2D and 3D understanding. (2) Human-centric AIGC Foundation Models that generate high-fidelity, diverse human-related content. (3) Unified Perception and Generation Models that integrate these capabilities to enhance both human understanding and synthesis. (4) Human-centric Agentic Foundation Models that extend beyond perception and generation to learn human-like intelligence and interactive behaviors for humanoid embodied tasks. We review state-of-the-art techniques, discuss emerging challenges and future research directions. This survey aims to serve as a roadmap for researchers and practitioners working towards more robust, versatile, and intelligent digital human and embodiments modeling. </p>
<blockquote>
<p>äººç±»å¯¹æ•°å­—äººç±»å’Œäººç±»å®ä½“å»ºæ¨¡çš„ç†è§£å’Œç”Ÿæˆè‡³å…³é‡è¦ã€‚æœ€è¿‘ï¼Œå—é€šç”¨æ¨¡å‹ï¼ˆå¦‚å¤§å‹è¯­è¨€å’Œè§†è§‰æ¨¡å‹ï¼‰æˆåŠŸçš„å¯å‘ï¼Œä»¥äººä¸ºæœ¬çš„åŸºç¡€æ¨¡å‹ï¼ˆHcFMsï¼‰åº”è¿è€Œç”Ÿï¼Œå°†å¤šæ ·åŒ–çš„äººç±»ä»»åŠ¡ç»Ÿä¸€åˆ°ä¸€ä¸ªæ¡†æ¶å†…ï¼Œè¶…è¶Šäº†ä¼ ç»Ÿçš„é’ˆå¯¹ç‰¹å®šä»»åŠ¡çš„æ–¹æ³•ã€‚åœ¨è¿™ç¯‡ç»¼è¿°ä¸­ï¼Œæˆ‘ä»¬é€šè¿‡æå‡ºä¸€ä¸ªåˆ†ç±»æ³•ï¼Œå¯¹å½“å‰æ–¹æ³•è¿›è¡Œäº†å…¨é¢æ¦‚è¿°ï¼Œå°†å…¶åˆ†ä¸ºå››ç±»ï¼š1ï¼‰ä»¥äººä¸ºæœ¬çš„æ„ŸçŸ¥åŸºç¡€æ¨¡å‹ï¼Œç”¨äºæ•è·å¤šæ¨¡æ€2Då’Œ3Dç†è§£çš„ç²¾ç»†ç‰¹å¾ã€‚2ï¼‰ä»¥äººä¸ºæœ¬çš„AIGCåŸºç¡€æ¨¡å‹ï¼Œç”Ÿæˆé«˜ä¿çœŸã€å¤šæ ·åŒ–çš„äººç±»ç›¸å…³å†…å®¹ã€‚3ï¼‰ç»Ÿä¸€æ„ŸçŸ¥å’Œç”Ÿæˆæ¨¡å‹ï¼Œå°†è¿™äº›èƒ½åŠ›ç»“åˆèµ·æ¥ï¼Œå¢å¼ºäººç±»å¯¹ç†è§£å’Œåˆæˆçš„èƒ½åŠ›ã€‚4ï¼‰ä»¥äººä¸ºæœ¬çš„ä»£ç†åŸºç¡€æ¨¡å‹ï¼Œè¶…è¶Šäº†æ„ŸçŸ¥å’Œç”Ÿæˆï¼Œå­¦ä¹ äººç±»æ™ºèƒ½å’Œäº¤äº’å¼è¡Œä¸ºï¼Œç”¨äºäººç±»å®ä½“ä»»åŠ¡ã€‚æˆ‘ä»¬å›é¡¾äº†æœ€æ–°æŠ€æœ¯ï¼Œè®¨è®ºäº†æ–°å…´æŒ‘æˆ˜å’Œæœªæ¥ç ”ç©¶æ–¹å‘ã€‚è¿™ç¯‡ç»¼è¿°æ—¨åœ¨ä¸ºæœç€æ›´ç¨³å¥ã€å¤šåŠŸèƒ½å’Œæ™ºèƒ½çš„æ•°å­—äººç±»å’Œå®ä½“å»ºæ¨¡å·¥ä½œçš„ç ”ç©¶äººå‘˜å’Œå®è·µè€…æä¾›æŒ‡å¯¼ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.08556v1">PDF</a> 9 pages</p>
<p><strong>Summary</strong>ï¼š<br>äººç±»ç†è§£å’Œç”Ÿæˆèƒ½åŠ›æ˜¯æ„å»ºæ•°å­—äººç±»å’Œäººç±»ä½“ç°æ¨¡å‹çš„å…³é”®ã€‚è¿‘æœŸå‡ºç°çš„ä»¥äººç±»ä¸ºä¸­å¿ƒçš„åŸºçŸ³æ¨¡å‹ï¼ˆHcFMsï¼‰ï¼Œå€Ÿé‰´äº†é€šç”¨æ¨¡å‹ï¼ˆå¦‚å¤§å‹è¯­è¨€å’Œè§†è§‰æ¨¡å‹ï¼‰çš„æˆåŠŸç»éªŒï¼Œæ—¨åœ¨å°†å¤šç§ä»¥äººç±»ä¸ºä¸­å¿ƒçš„ä»»åŠ¡ç»Ÿä¸€åˆ°ä¸€ä¸ªæ¡†æ¶ä¸­ï¼Œè¶…è¶Šä¼ ç»Ÿçš„é’ˆå¯¹ç‰¹å®šä»»åŠ¡çš„æ–¹æ³•ã€‚æœ¬æ–‡ç»¼è¿°äº†HcFMsçš„å…¨é¢å‘å±•ï¼Œå¹¶æå‡ºäº†ä¸€ä¸ªåˆ†ç±»æ³•ï¼Œå°†å½“å‰çš„æ–¹æ³•åˆ†ä¸ºå››ç±»ã€‚æœ¬æ–‡è¿˜å›é¡¾äº†æœ€æ–°æŠ€æœ¯ï¼Œè®¨è®ºäº†æ–°å…´æŒ‘æˆ˜å’Œæœªæ¥ç ”ç©¶æ–¹å‘ã€‚æœ¬æ–‡æ—¨åœ¨æˆä¸ºç ”ç©¶è€…å’Œå®è·µè€…çš„è·¯çº¿å›¾ï¼Œè‡´åŠ›äºæ„å»ºæ›´ç¨³å¥ã€å¤šåŠŸèƒ½å’Œæ™ºèƒ½çš„æ•°å­—äººç±»å’Œä½“ç°æ¨¡å‹ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>HcFMså°†å¤šç§äººç±»ä¸ºä¸­å¿ƒçš„ä»»åŠ¡ç»Ÿä¸€åˆ°ä¸€ä¸ªæ¡†æ¶ä¸­ï¼Œè¶…è¶Šäº†ä¼ ç»Ÿä»»åŠ¡ç‰¹å®šæ–¹æ³•ã€‚</li>
<li>HcFMsåˆ†ä¸ºå››å¤§ç±»ï¼šäººç±»ä¸ºä¸­å¿ƒçš„æ„ŸçŸ¥åŸºçŸ³æ¨¡å‹ã€äººç±»ä¸ºä¸­å¿ƒçš„AIGCåŸºçŸ³æ¨¡å‹ã€ç»Ÿä¸€æ„ŸçŸ¥å’Œç”Ÿæˆæ¨¡å‹ã€ä»¥åŠä»¥äººç±»ä¸ºä¸­å¿ƒçš„ä¸»ä½“åŸºçŸ³æ¨¡å‹ã€‚</li>
<li>äººç±»ä¸ºä¸­å¿ƒçš„æ„ŸçŸ¥åŸºçŸ³æ¨¡å‹ç”¨äºæ•æ‰å¤šæ¨¡æ€çš„ç²¾ç»†ç‰¹å¾ï¼Œç”¨äºäºŒç»´å’Œä¸‰ç»´ç†è§£ã€‚</li>
<li>äººç±»ä¸ºä¸­å¿ƒçš„AIGCåŸºçŸ³æ¨¡å‹ç”Ÿæˆé«˜ä¿çœŸã€å¤šæ ·åŒ–çš„äººç±»ç›¸å…³å†…å®¹ã€‚</li>
<li>ç»Ÿä¸€æ„ŸçŸ¥å’Œç”Ÿæˆæ¨¡å‹é›†æˆäº†è¿™äº›èƒ½åŠ›ï¼Œæé«˜äº†äººç±»ç†è§£å’Œåˆæˆçš„èƒ½åŠ›ã€‚</li>
<li>ä»¥äººç±»ä¸ºä¸­å¿ƒçš„ä¸»ä½“åŸºçŸ³æ¨¡å‹è¶…è¶Šäº†æ„ŸçŸ¥å’Œç”Ÿæˆï¼Œå­¦ä¹ äººç±»æ™ºèƒ½å’Œäº¤äº’è¡Œä¸ºï¼Œç”¨äºäººç±»ä½“ç°ä»»åŠ¡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.08556">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-585f1f1c079fb4c98b7275b9e7b410cf.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-acafdc0dfe25a52f6bdbadf41492aba1.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ed36878303dfed8f114f8baaecc4a61f.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-3f928de4a5dac56c070926c520e83001.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4752d7557483ce57381608d5cb128174.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Decentralised-multi-agent-coordination-for-real-time-railway-traffic-management"><a href="#Decentralised-multi-agent-coordination-for-real-time-railway-traffic-management" class="headerlink" title="Decentralised multi-agent coordination for real-time railway traffic   management"></a>Decentralised multi-agent coordination for real-time railway traffic   management</h2><p><strong>Authors:Leo Dâ€™Amato, Paola Pellegrini, Vito Trianni</strong></p>
<p>The real-time Railway Traffic Management Problem (rtRTMP) is a challenging optimisation problem in railway transportation. It involves the efficient management of train movements while minimising delay propagation caused by unforeseen perturbations due to, e.g, temporary speed limitations or signal failures. This paper re-frames the rtRTMP as a multi-agent coordination problem and formalises it as a Distributed Constraint Optimisation Problem (DCOP) to explore its potential for decentralised solutions. We propose a novel coordination algorithm that extends the widely known Distributed Stochastic Algorithm (DSA), allowing trains to self-organise and resolve scheduling conflicts. The performance of our algorithm is compared to a classical DSA through extensive simulations on a synthetic dataset reproducing diverse problem configurations. Results show that our approach achieves significant improvements in solution quality and convergence speed, demonstrating its effectiveness and scalability in managing large-scale railway networks. Beyond the railway domain, this framework can have broader applicability in autonomous systems, such as self-driving vehicles or inter-satellite coordination. </p>
<blockquote>
<p>å®æ—¶é“è·¯äº¤é€šç®¡ç†é—®é¢˜ï¼ˆrtRTMPï¼‰æ˜¯é“è·¯è¿è¾“ä¸­ä¸€é¡¹å…·æœ‰æŒ‘æˆ˜æ€§çš„ä¼˜åŒ–é—®é¢˜ã€‚å®ƒæ¶‰åŠç«è½¦è¿åŠ¨çš„æ•ˆç‡ç®¡ç†ï¼ŒåŒæ—¶æœ€å°åŒ–ç”±äºä¸´æ—¶é€Ÿåº¦é™åˆ¶æˆ–ä¿¡å·æ•…éšœç­‰ä¸å¯é¢„è§çš„æ‰°åŠ¨é€ æˆçš„å»¶è¿Ÿä¼ æ’­ã€‚æœ¬æ–‡å°†rtRTMPé‡æ–°æ„å»ºä¸ºä¸€ä¸ªå¤šæ™ºèƒ½ä½“åè°ƒé—®é¢˜ï¼Œå¹¶å°†å…¶å½¢å¼åŒ–ä¸ºåˆ†å¸ƒå¼çº¦æŸä¼˜åŒ–é—®é¢˜ï¼ˆDCOPï¼‰ï¼Œä»¥æ¢ç´¢å…¶åˆ†æ•£å¼è§£å†³æ–¹æ¡ˆçš„æ½œåŠ›ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°å‹çš„åè°ƒç®—æ³•ï¼Œè¯¥ç®—æ³•æ‰©å±•äº†å¹¿ä¸ºäººçŸ¥çš„åˆ†å¸ƒå¼éšæœºç®—æ³•ï¼ˆDSAï¼‰ï¼Œä½¿ç«è½¦èƒ½å¤Ÿè‡ªæˆ‘ç»„ç»‡å’Œè§£å†³è°ƒåº¦å†²çªã€‚æˆ‘ä»¬é€šè¿‡åœ¨ä¸€ä¸ªåˆæˆæ•°æ®é›†ä¸Šè¿›è¡Œçš„å¤§é‡æ¨¡æ‹Ÿï¼Œå°†ç®—æ³•çš„æ€§èƒ½ä¸ä¸€ä¸ªç»å…¸çš„DSAè¿›è¡Œäº†æ¯”è¾ƒï¼Œè¯¥æ•°æ®é›†é‡ç°äº†å¤šç§é—®é¢˜é…ç½®ã€‚ç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨è§£å†³æ–¹æ¡ˆè´¨é‡å’Œæ”¶æ•›é€Ÿåº¦ä¸Šå–å¾—äº†æ˜¾è‘—çš„æ”¹è¿›ï¼Œè¯æ˜äº†å…¶åœ¨ç®¡ç†å¤§è§„æ¨¡é“è·¯ç½‘ç»œä¸­çš„æœ‰æ•ˆæ€§å’Œå¯æ‰©å±•æ€§ã€‚é™¤äº†é“è·¯é¢†åŸŸï¼Œè¯¥æ¡†æ¶è¿˜å¯ä»¥æ›´å¹¿æ³›åœ°åº”ç”¨äºè‡ªä¸»ç³»ç»Ÿï¼Œå¦‚è‡ªåŠ¨é©¾é©¶æ±½è½¦æˆ–å«æ˜Ÿé—´åè°ƒã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.08324v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å®æ—¶é“è·¯äº¤é€šç®¡ç†é—®é¢˜ï¼ˆrtRTMPï¼‰æ˜¯é“è·¯è¿è¾“ä¸­çš„ä¸€é¡¹å…·æœ‰æŒ‘æˆ˜æ€§çš„ä¼˜åŒ–é—®é¢˜ã€‚æœ¬æ–‡å°†å…¶é‡æ–°æ„å»ºä¸ºå¤šæ™ºèƒ½ä½“åè°ƒé—®é¢˜ï¼Œå¹¶å½¢å¼åŒ–ä¸ºåˆ†å¸ƒå¼çº¦æŸä¼˜åŒ–é—®é¢˜ï¼ˆDCOPï¼‰ï¼Œä»¥æ¢ç´¢åˆ†æ•£å¼è§£å†³æ–¹æ¡ˆçš„æ½œåŠ›ã€‚æå‡ºä¸€ç§æ–°å‹çš„åè°ƒç®—æ³•ï¼Œè¯¥ç®—æ³•æ‰©å±•äº†ä¼—æ‰€å‘¨çŸ¥çš„åˆ†å¸ƒå¼éšæœºç®—æ³•ï¼ˆDSAï¼‰ï¼Œä½¿åˆ—è½¦èƒ½å¤Ÿè‡ªæˆ‘ç»„ç»‡å’Œè§£å†³è°ƒåº¦å†²çªã€‚é€šè¿‡æ¨¡æ‹Ÿåˆæˆæ•°æ®é›†çš„å¤§é‡ä»¿çœŸä¸ç»å…¸DSAè¿›è¡Œæ¯”è¾ƒï¼Œç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨è§£å†³æ–¹æ¡ˆè´¨é‡å’Œæ”¶æ•›é€Ÿåº¦æ–¹é¢å–å¾—äº†æ˜¾è‘—æ”¹è¿›ï¼Œåœ¨ç®¡ç†å¤§è§„æ¨¡é“è·¯ç½‘ç»œä¸­è¡¨ç°å‡ºæœ‰æ•ˆæ€§å’Œå¯æ‰©å±•æ€§ã€‚æ­¤å¤–ï¼Œè¯¥æ¡†æ¶è¿˜å¯åº”ç”¨äºè‡ªåŠ¨é©¾é©¶è½¦è¾†æˆ–å«æ˜Ÿé—´åè°ƒç­‰è‡ªä¸»ç³»ç»Ÿã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>rtRTMPå®šä¹‰ä¸ºå®æ—¶é“è·¯äº¤é€šç®¡ç†é—®é¢˜çš„ä¼˜åŒ–æŒ‘æˆ˜ï¼Œæ¶‰åŠåˆ—è½¦ç§»åŠ¨çš„é«˜æ•ˆç®¡ç†ã€‚</li>
<li>è®ºæ–‡å°†rtRTMPé‡æ–°æ„å»ºä¸ºå¤šæ™ºèƒ½ä½“åè°ƒé—®é¢˜ï¼Œå¹¶å½¢å¼åŒ–ä¸ºDCOPã€‚</li>
<li>æå‡ºä¸€ç§åŸºäºDSAçš„æ–°å‹åè°ƒç®—æ³•ï¼Œä½¿åˆ—è½¦èƒ½è‡ªæˆ‘ç»„ç»‡å’Œè§£å†³è°ƒåº¦å†²çªã€‚</li>
<li>è¯¥ç®—æ³•åœ¨åˆæˆæ•°æ®é›†ä¸Šçš„æ¨¡æ‹Ÿç»“æœä¸ç»å…¸DSAç›¸æ¯”ï¼Œæ˜¾ç¤ºå‡ºåœ¨è§£å†³æ–¹æ¡ˆè´¨é‡å’Œæ”¶æ•›é€Ÿåº¦ä¸Šçš„æ˜¾è‘—æé«˜ã€‚</li>
<li>è®ºæ–‡è¯æ˜äº†è¯¥ç®—æ³•åœ¨ç®¡ç†å¤§è§„æ¨¡é“è·¯ç½‘ç»œä¸­çš„æœ‰æ•ˆæ€§å’Œå¯æ‰©å±•æ€§ã€‚</li>
<li>è¯¥æ¡†æ¶é™¤äº†åº”ç”¨äºé“è·¯é¢†åŸŸå¤–ï¼Œè¿˜å¯ç”¨äºè‡ªåŠ¨é©¾é©¶è½¦è¾†æˆ–å«æ˜Ÿé—´åè°ƒç­‰è‡ªä¸»ç³»ç»Ÿã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.08324">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-50f945360e3db5524b0b7c95a09aef0f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ce40fb115f8f2924983ce6d4b4750385.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-0608d0007321a04af8a891af9b7bd83e.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="The-Danger-of-Overthinking-Examining-the-Reasoning-Action-Dilemma-in-Agentic-Tasks"><a href="#The-Danger-of-Overthinking-Examining-the-Reasoning-Action-Dilemma-in-Agentic-Tasks" class="headerlink" title="The Danger of Overthinking: Examining the Reasoning-Action Dilemma in   Agentic Tasks"></a>The Danger of Overthinking: Examining the Reasoning-Action Dilemma in   Agentic Tasks</h2><p><strong>Authors:Alejandro Cuadron, Dacheng Li, Wenjie Ma, Xingyao Wang, Yichuan Wang, Siyuan Zhuang, Shu Liu, Luis Gaspar Schroeder, Tian Xia, Huanzhi Mao, Nicholas Thumiger, Aditya Desai, Ion Stoica, Ana Klimovic, Graham Neubig, Joseph E. Gonzalez</strong></p>
<p>Large Reasoning Models (LRMs) represent a breakthrough in AI problem-solving capabilities, but their effectiveness in interactive environments can be limited. This paper introduces and analyzes overthinking in LRMs. A phenomenon where models favor extended internal reasoning chains over environmental interaction. Through experiments on software engineering tasks using SWE Bench Verified, we observe three recurring patterns: Analysis Paralysis, Rogue Actions, and Premature Disengagement. We propose a framework to study these behaviors, which correlates with human expert assessments, and analyze 4018 trajectories. We observe that higher overthinking scores correlate with decreased performance, with reasoning models exhibiting stronger tendencies toward overthinking compared to non-reasoning models. Our analysis reveals that simple efforts to mitigate overthinking in agentic environments, such as selecting the solution with the lower overthinking score, can improve model performance by almost 30% while reducing computational costs by 43%. These results suggest that mitigating overthinking has strong practical implications. We suggest that by leveraging native function-calling capabilities and selective reinforcement learning overthinking tendencies could be mitigated. We also open-source our evaluation framework and dataset to facilitate research in this direction at <a target="_blank" rel="noopener" href="https://github.com/AlexCuadron/Overthinking">https://github.com/AlexCuadron/Overthinking</a>. </p>
<blockquote>
<p>å¤§å‹æ¨ç†æ¨¡å‹ï¼ˆLRMsï¼‰ä»£è¡¨äº†äººå·¥æ™ºèƒ½é—®é¢˜è§£å†³èƒ½åŠ›çš„çªç ´ï¼Œä½†å®ƒä»¬åœ¨äº¤äº’å¼ç¯å¢ƒä¸­çš„æœ‰æ•ˆæ€§å¯èƒ½ä¼šå—åˆ°é™åˆ¶ã€‚æœ¬æ–‡ä»‹ç»å¹¶åˆ†æäº†å¤§å‹æ¨ç†æ¨¡å‹ä¸­çš„è¿‡åº¦æ€è€ƒç°è±¡ã€‚è¿™æ˜¯ä¸€ç§æ¨¡å‹åå¥½å»¶é•¿å†…éƒ¨æ¨ç†é“¾è€Œéä¸ç¯å¢ƒäº’åŠ¨çš„ç°è±¡ã€‚é€šè¿‡SWE Bench Verifiedå¯¹è½¯ä»¶å·¥ç¨‹ä»»åŠ¡çš„å®éªŒï¼Œæˆ‘ä»¬è§‚å¯Ÿåˆ°ä¸‰ç§é‡å¤å‡ºç°çš„æ¨¡å¼ï¼šåˆ†æç˜«ç—ªã€æ„å¤–è¡ŒåŠ¨å’Œè¿‡æ—©è„±ç¦»ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªç ”ç©¶è¿™äº›è¡Œä¸ºçš„æ¡†æ¶ï¼Œè¯¥æ¡†æ¶ä¸äººç±»ä¸“å®¶è¯„ä¼°ç›¸å…³ï¼Œå¹¶åˆ†æäº†4018æ¡è½¨è¿¹ã€‚æˆ‘ä»¬å‘ç°è¾ƒé«˜çš„è¿‡åº¦æ€è€ƒåˆ†æ•°ä¸æ€§èƒ½ä¸‹é™æœ‰å…³ï¼Œæ¨ç†æ¨¡å‹è¡¨ç°å‡ºæ¯”éæ¨ç†æ¨¡å‹æ›´å¼ºçš„è¿‡åº¦æ€è€ƒå€¾å‘ã€‚æˆ‘ä»¬çš„åˆ†æè¡¨æ˜ï¼Œåœ¨ä»£ç†ç¯å¢ƒä¸­ç¼“è§£è¿‡åº¦æ€è€ƒçš„ç®€å•åŠªåŠ›ï¼Œå¦‚é€‰æ‹©å…·æœ‰è¾ƒä½è¿‡åº¦æ€è€ƒåˆ†æ•°çš„è§£å†³æ–¹æ¡ˆï¼Œå¯ä»¥åœ¨å‡ ä¹ä¸å½±å“æ€§èƒ½çš„æƒ…å†µä¸‹æé«˜æ¨¡å‹æ€§èƒ½çº¦30%ï¼ŒåŒæ—¶é™ä½è®¡ç®—æˆæœ¬43%ã€‚è¿™äº›ç»“æœè¡¨æ˜ç¼“è§£è¿‡åº¦æ€è€ƒå…·æœ‰é‡è¦çš„å®é™…åº”ç”¨æ„ä¹‰ã€‚æˆ‘ä»¬å»ºè®®é€šè¿‡åˆ©ç”¨åŸç”Ÿçš„å‡½æ•°è°ƒç”¨èƒ½åŠ›ä»¥åŠé€‰æ‹©æ€§å¼ºåŒ–å­¦ä¹ æ¥å‡è½»è¿‡åº¦æ€è€ƒçš„å€¾å‘ã€‚æˆ‘ä»¬è¿˜å…¬å¼€äº†æˆ‘ä»¬çš„è¯„ä¼°æ¡†æ¶å’Œæ•°æ®é›†ï¼Œä»¥ä¾¿åœ¨æ­¤æ–¹å‘ä¸Šè¿›è¡Œç ”ç©¶ï¼š<a target="_blank" rel="noopener" href="https://github.com/AlexCuadron/Overthinking">https://github.com/AlexCuadron/Overthinking</a>ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.08235v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤§å‹æ¨ç†æ¨¡å‹ï¼ˆLRMsï¼‰åœ¨AIé¢†åŸŸå…·æœ‰çªç ´æ€§çš„é—®é¢˜è§£å†³èƒ½åŠ›ï¼Œä½†åœ¨äº¤äº’å¼ç¯å¢ƒä¸­å…¶æœ‰æ•ˆæ€§å—é™ã€‚æœ¬æ–‡ä»‹ç»å¹¶åˆ†æäº†LRMsä¸­çš„è¿‡åº¦æ€è€ƒç°è±¡ï¼Œè¿™æ˜¯æ¨¡å‹å€¾å‘äºå»¶é•¿å†…éƒ¨æ¨ç†é“¾è€Œå¿½è§†ä¸ç¯å¢ƒäº’åŠ¨çš„ç°è±¡ã€‚é€šè¿‡å®éªŒåˆ†æï¼Œæˆ‘ä»¬å‘ç°è¿‡åº¦æ€è€ƒä¼šå¯¼è‡´åˆ†æç˜«ç—ªã€éšæœºè¡ŒåŠ¨å’Œè¿‡æ—©åœæ­¢ç­‰ä¸‰ç§æ¨¡å¼ã€‚æå‡ºä¸€ä¸ªç ”ç©¶è¿™äº›è¡Œä¸ºçš„æ¡†æ¶ï¼Œè¯¥æ¡†æ¶ä¸äººç±»ä¸“å®¶è¯„ä¼°ç›¸å…³ï¼Œå¹¶åˆ†æäº†4018æ¡è½¨è¿¹ã€‚ç ”ç©¶å‘ç°ï¼Œè¿‡åº¦æ€è€ƒå¾—åˆ†è¾ƒé«˜ä¸è¡¨ç°ä¸‹é™æœ‰å…³ï¼Œæ¨ç†æ¨¡å‹è¾ƒéæ¨ç†æ¨¡å‹æ›´æ˜“å‡ºç°è¿‡åº¦æ€è€ƒå€¾å‘ã€‚é€šè¿‡é€‰æ‹©å…·æœ‰è¾ƒä½è¿‡åº¦æ€è€ƒå¾—åˆ†çš„è§£å†³æ–¹æ¡ˆç­‰ç®€å•æ–¹æ³•ï¼Œå¯ä»¥æ”¹å–„æ¨¡å‹æ€§èƒ½è¿‘30%ï¼ŒåŒæ—¶é™ä½è®¡ç®—æˆæœ¬43%ã€‚ç»“æœè¡¨æ˜å‡è½»è¿‡åº¦æ€è€ƒå…·æœ‰å®é™…åº”ç”¨çš„å¼ºå¤§æ„ä¹‰ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹æ¨ç†æ¨¡å‹ï¼ˆLRMsï¼‰åœ¨AIé¢†åŸŸå…·æœ‰çªç ´æ€§çš„èƒ½åŠ›ï¼Œä½†åœ¨äº¤äº’å¼ç¯å¢ƒä¸­æœ‰æ•ˆæ€§å—é™ã€‚</li>
<li>è¿‡åº¦æ€è€ƒæ˜¯LRMsä¸­çš„ä¸€ç§ç°è±¡ï¼Œè¡¨ç°ä¸ºæ¨¡å‹å€¾å‘äºå»¶é•¿å†…éƒ¨æ¨ç†é“¾è€Œå¿½è§†ä¸ç¯å¢ƒäº’åŠ¨ã€‚</li>
<li>è¿‡åº¦æ€è€ƒä¼šå¯¼è‡´åˆ†æç˜«ç—ªã€éšæœºè¡ŒåŠ¨å’Œè¿‡æ—©åœæ­¢ç­‰ä¸‰ç§æ¨¡å¼ã€‚</li>
<li>è¿‡åº¦æ€è€ƒå¾—åˆ†è¾ƒé«˜ä¸è¡¨ç°ä¸‹é™æœ‰å…³ã€‚</li>
<li>ç›¸æ¯”éæ¨ç†æ¨¡å‹ï¼Œæ¨ç†æ¨¡å‹æ›´å®¹æ˜“å‡ºç°è¿‡åº¦æ€è€ƒå€¾å‘ã€‚</li>
<li>é€šè¿‡é€‰æ‹©å…·æœ‰è¾ƒä½è¿‡åº¦æ€è€ƒå¾—åˆ†çš„è§£å†³æ–¹æ¡ˆï¼Œå¯ä»¥æ”¹å–„æ¨¡å‹æ€§èƒ½å¹¶é™ä½è®¡ç®—æˆæœ¬ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.08235">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-be5f99abe3708e0ca0c6f80b7c85b2c7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-902029ba7cc7d4375fde490c2a3fbc84.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-fefc12aafc7cddbe385c66391ba664f1.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-49d25720f9ff0c2f479a74399d104068.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-26a33b209cd1126ee32f9c6f93121623.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="TRISHUL-Towards-Region-Identification-and-Screen-Hierarchy-Understanding-for-Large-VLM-based-GUI-Agents"><a href="#TRISHUL-Towards-Region-Identification-and-Screen-Hierarchy-Understanding-for-Large-VLM-based-GUI-Agents" class="headerlink" title="TRISHUL: Towards Region Identification and Screen Hierarchy   Understanding for Large VLM based GUI Agents"></a>TRISHUL: Towards Region Identification and Screen Hierarchy   Understanding for Large VLM based GUI Agents</h2><p><strong>Authors:Kunal Singh, Shreyas Singh, Mukund Khanna</strong></p>
<p>Recent advancements in Large Vision Language Models (LVLMs) have enabled the development of LVLM-based Graphical User Interface (GUI) agents under various paradigms. Training-based approaches, such as CogAgent and SeeClick, struggle with cross-dataset and cross-platform generalization due to their reliance on dataset-specific training. Generalist LVLMs, such as GPT-4V, employ Set-of-Marks (SoM) for action grounding, but obtaining SoM labels requires metadata like HTML source, which is not consistently available across platforms. Moreover, existing methods often specialize in singular GUI tasks rather than achieving comprehensive GUI understanding. To address these limitations, we introduce TRISHUL, a novel, training-free agentic framework that enhances generalist LVLMs for holistic GUI comprehension. Unlike prior works that focus on either action grounding (mapping instructions to GUI elements) or GUI referring (describing GUI elements given a location), TRISHUL seamlessly integrates both. At its core, TRISHUL employs Hierarchical Screen Parsing (HSP) and the Spatially Enhanced Element Description (SEED) module, which work synergistically to provide multi-granular, spatially, and semantically enriched representations of GUI elements. Our results demonstrate TRISHULâ€™s superior performance in action grounding across the ScreenSpot, VisualWebBench, AITW, and Mind2Web datasets. Additionally, for GUI referring, TRISHUL surpasses the ToL agent on the ScreenPR benchmark, setting a new standard for robust and adaptable GUI comprehension. </p>
<blockquote>
<p>è¿‘æœŸå¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆLVLMsï¼‰çš„è¿›æ­¥æ¨åŠ¨äº†åŸºäºLVLMçš„å›¾å½¢ç”¨æˆ·ç•Œé¢ï¼ˆGUIï¼‰ä»£ç†åœ¨å„ç§èŒƒå¼ä¸‹çš„å¼€å‘ã€‚åŸºäºè®­ç»ƒçš„æ–¹æ³•ï¼Œå¦‚CogAgentå’ŒSeeClickï¼Œç”±äºä¾èµ–äºç‰¹å®šæ•°æ®é›†çš„è®­ç»ƒï¼Œå› æ­¤åœ¨è·¨æ•°æ®é›†å’Œè·¨å¹³å°æ³›åŒ–æ–¹é¢å­˜åœ¨å›°éš¾ã€‚é€šç”¨LVLMsï¼Œå¦‚GPT-4Vï¼Œé‡‡ç”¨æ ‡è®°é›†ï¼ˆSoMï¼‰è¿›è¡ŒåŠ¨ä½œå®šä½ï¼Œä½†è·å–SoMæ ‡ç­¾éœ€è¦HTMLæºä»£ç ç­‰å…ƒæ•°æ®ï¼Œè€Œè¿™äº›å…ƒæ•°æ®åœ¨å¹³å°é—´å¹¶ä¸æ€»èƒ½è·å–ã€‚æ­¤å¤–ï¼Œç°æœ‰æ–¹æ³•å¾€å¾€ä¸“æ³¨äºå•ä¸€GUIä»»åŠ¡ï¼Œè€Œéå®ç°å…¨é¢çš„GUIç†è§£ã€‚ä¸ºäº†å…‹æœè¿™äº›å±€é™æ€§ï¼Œæˆ‘ä»¬å¼•å…¥äº†TRISHULï¼Œè¿™æ˜¯ä¸€ç§æ–°å‹çš„æ— è®­ç»ƒä»£ç†æ¡†æ¶ï¼Œå¯å¢å¼ºé€šç”¨LVLMsçš„å…¨å±€GUIç†è§£èƒ½åŠ›ã€‚ä¸åŒäºä»¥å¾€ä¸“æ³¨äºåŠ¨ä½œå®šä½ï¼ˆå°†æŒ‡ä»¤æ˜ å°„åˆ°GUIå…ƒç´ ï¼‰æˆ–GUIå¼•ç”¨ï¼ˆæ ¹æ®ä½ç½®æè¿°GUIå…ƒç´ ï¼‰çš„ç ”ç©¶ï¼ŒTRISHULæ— ç¼é›†æˆäº†ä¸¤è€…ã€‚å…¶æ ¸å¿ƒé‡‡ç”¨åˆ†å±‚å±å¹•è§£æï¼ˆHSPï¼‰å’Œç©ºé—´å¢å¼ºå…ƒç´ æè¿°ï¼ˆSEEDï¼‰æ¨¡å—ï¼ŒååŒå·¥ä½œä»¥æä¾›å¤šç²’åº¦ã€ç©ºé—´ä¸Šä¸°å¯Œã€è¯­ä¹‰ä¸°å¯Œçš„GUIå…ƒç´ è¡¨ç¤ºã€‚æˆ‘ä»¬çš„ç»“æœè¡¨æ˜ï¼ŒTRISHULåœ¨ScreenSpotã€VisualWebBenchã€AITWå’ŒMind2Webæ•°æ®é›†ä¸Šçš„åŠ¨ä½œå®šä½æ€§èƒ½ä¼˜è¶Šã€‚æ­¤å¤–ï¼Œå¯¹äºGUIå¼•ç”¨ï¼ŒTRISHULåœ¨ScreenPRåŸºå‡†æµ‹è¯•ä¸Šçš„è¡¨ç°è¶…è¶Šäº†ToLä»£ç†ï¼Œä¸ºç¨³å¥å’Œå¯é€‚åº”çš„GUIç†è§£è®¾å®šäº†æ–°çš„æ ‡å‡†ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.08226v1">PDF</a> Under review at ICML 2025, 8 pages 5 figures</p>
<p><strong>Summary</strong></p>
<p>å¤§è§„æ¨¡è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆLVLMï¼‰çš„æœ€æ–°è¿›å±•æ¨åŠ¨äº†åŸºäºLVLMçš„å›¾å½¢ç”¨æˆ·ç•Œé¢ï¼ˆGUIï¼‰ä»£ç†åœ¨å„ç§èŒƒå¼ä¸‹çš„å¼€å‘ã€‚ç°æœ‰æ–¹æ³•å¦‚CogAgentå’ŒSeeClickç­‰åŸºäºè®­ç»ƒçš„æ–¹æ³•åœ¨è·¨æ•°æ®é›†å’Œè·¨å¹³å°æ³›åŒ–æ–¹é¢å­˜åœ¨å±€é™ã€‚è€ŒGPT-4Vç­‰é€šç”¨LVLMåˆ™é‡‡ç”¨æ ‡è®°é›†ï¼ˆSoMï¼‰è¿›è¡ŒåŠ¨ä½œå®šä½ï¼Œä½†è·å–SoMæ ‡ç­¾éœ€è¦HTMLæºç­‰å…ƒæ•°æ®ï¼Œè¿™åœ¨å„å¹³å°ä¹‹é—´å¹¶ä¸ä¸€è‡´ã€‚ä¸ºè§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†TRISHULï¼Œè¿™æ˜¯ä¸€ç§æ–°å‹çš„æ— è®­ç»ƒä»£ç†æ¡†æ¶ï¼Œæ—¨åœ¨æé«˜é€šç”¨LVLMå¯¹GUIçš„å…¨é¢ç†è§£ã€‚TRISHULä¸ä»…èšç„¦äºåŠ¨ä½œå®šä½ï¼ˆå°†æŒ‡ä»¤æ˜ å°„åˆ°GUIå…ƒç´ ï¼‰ï¼Œè¿˜èåˆäº†GUIå¼•ç”¨ï¼ˆæ ¹æ®ä½ç½®æè¿°GUIå…ƒç´ ï¼‰ã€‚å…¶æ ¸å¿ƒé‡‡ç”¨åˆ†å±‚å±å¹•è§£æï¼ˆHSPï¼‰å’Œç©ºé—´å¢å¼ºå…ƒç´ æè¿°ï¼ˆSEEDï¼‰æ¨¡å—ï¼ŒååŒå·¥ä½œä»¥æä¾›å¤šç²’åº¦ã€ç©ºé—´ä¸Šã€è¯­ä¹‰ä¸°å¯Œçš„GUIå…ƒç´ è¡¨ç¤ºã€‚å®éªŒç»“æœè¯æ˜ï¼ŒTRISHULåœ¨ScreenSpotã€VisualWebBenchã€AITWå’ŒMind2Webæ•°æ®é›†ä¸Šçš„åŠ¨ä½œå®šä½æ€§èƒ½ä¼˜è¶Šï¼Œå¹¶åœ¨ScreenPRåŸºå‡†æµ‹è¯•ä¸Šçš„GUIå¼•ç”¨è¡¨ç°è¶…è¶Šç°æœ‰ä»£ç†ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LVLMsçš„è¿›æ­¥æ¨åŠ¨äº†GUIä»£ç†çš„å¼€å‘ï¼Œä½†ç°æœ‰æ–¹æ³•å­˜åœ¨è·¨æ•°æ®é›†å’Œè·¨å¹³å°æ³›åŒ–å›°éš¾çš„é—®é¢˜ã€‚</li>
<li>é€šç”¨LVLMå¦‚GPT-4Vé‡‡ç”¨SoMè¿›è¡ŒåŠ¨ä½œå®šä½ï¼Œä½†å…ƒæ•°æ®è·å–ä¸ä¸€è‡´ã€‚</li>
<li>TRISHULæ˜¯ä¸€ç§æ–°å‹æ— è®­ç»ƒä»£ç†æ¡†æ¶ï¼Œæ—¨åœ¨æé«˜LVLMå¯¹GUIçš„å…¨é¢ç†è§£ã€‚</li>
<li>TRISHULèåˆäº†åŠ¨ä½œå®šä½å’ŒGUIå¼•ç”¨ï¼Œæä¾›ä¸°å¯Œçš„GUIå…ƒç´ è¡¨ç¤ºã€‚</li>
<li>TRISHULé‡‡ç”¨HSPå’ŒSEEDæ¨¡å—ååŒå·¥ä½œï¼Œå®ç°å¤šç²’åº¦ã€ç©ºé—´ä¸Šã€è¯­ä¹‰ä¸°å¯Œçš„è¡¨ç¤ºã€‚</li>
<li>TRISHULåœ¨å¤šä¸ªæ•°æ®é›†ä¸Šçš„åŠ¨ä½œå®šä½æ€§èƒ½ä¼˜è¶Šã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.08226">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-1d19cdb25bd767f9d1d50796a4c5e3af.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e803897d201132fa7e0a2477a4b5500d.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-102babfc430ed5557f5ef1400b312300.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6d069aa21313077e36654c9ac62714e1.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-064d0b53118c44eb674856b65a12ac6b.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="From-Clicks-to-Conversations-Evaluating-the-Effectiveness-of-Conversational-Agents-in-Statistical-Analysis"><a href="#From-Clicks-to-Conversations-Evaluating-the-Effectiveness-of-Conversational-Agents-in-Statistical-Analysis" class="headerlink" title="From Clicks to Conversations: Evaluating the Effectiveness of   Conversational Agents in Statistical Analysis"></a>From Clicks to Conversations: Evaluating the Effectiveness of   Conversational Agents in Statistical Analysis</h2><p><strong>Authors:Qifu Wen, Prishita Kochhar, Sherif Zeyada, Tahereh Javaheri, Reza Rawassizadeh</strong></p>
<p>The rapid proliferation of data science forced different groups of individuals with different backgrounds to adapt to statistical analysis. We hypothesize that conversational agents are better suited for statistical analysis than traditional graphical user interfaces (GUI). In this work, we propose a novel conversational agent, StatZ, for statistical analysis. We evaluate the efficacy of StatZ relative to established statistical software:SPSS, SAS, Stata, and JMP in terms of accuracy, task completion time, user experience, and user satisfaction. We combined the proposed analysis question from state-of-the-art language models with suggestions from statistical analysis experts and tested with 51 participants from diverse backgrounds. Our experimental design assessed each participantâ€™s ability to perform statistical analysis tasks using traditional statistical analysis tools with GUI and our conversational agent. Results indicate that the proposed conversational agents significantly outperform GUI statistical software in all assessed metrics, including quantitative (task completion time, accuracy, and user experience), and qualitative (user satisfaction) metrics. Our findings underscore the potential of using conversational agents to enhance statistical analysis processes, reducing cognitive load and learning curves and thereby proliferating data analysis capabilities, to individuals with limited knowledge of statistics. </p>
<blockquote>
<p>æ•°æ®ç§‘å­¦çš„å¿«é€Ÿå‘å±•ä¿ƒä½¿ä¸åŒèƒŒæ™¯çš„ä¸ªä½“éœ€è¦é€‚åº”ç»Ÿè®¡åˆ†æã€‚æˆ‘ä»¬å‡è®¾å¯¹è¯ä»£ç†æ¯”ä¼ ç»Ÿçš„å›¾å½¢ç”¨æˆ·ç•Œé¢ï¼ˆGUIï¼‰æ›´é€‚åˆè¿›è¡Œç»Ÿè®¡åˆ†æã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°å‹çš„å¯¹è¯ä»£ç†StatZç”¨äºç»Ÿè®¡åˆ†æã€‚æˆ‘ä»¬è¯„ä¼°äº†StatZç›¸è¾ƒäºå·²å»ºç«‹çš„ç»Ÿè®¡è½¯ä»¶SPSSã€SASã€Stataå’ŒJMPåœ¨å‡†ç¡®æ€§ã€ä»»åŠ¡å®Œæˆæ—¶é—´ã€ç”¨æˆ·ä½“éªŒå’Œç”¨æˆ·æ»¡æ„åº¦æ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚æˆ‘ä»¬ç»“åˆäº†æœ€å…ˆè¿›çš„è¯­è¨€æ¨¡å‹çš„æè®®åˆ†æé—®é¢˜å’Œç»Ÿè®¡åˆ†æä¸“å®¶çš„å»ºè®®ï¼Œå¹¶å¯¹æ¥è‡ªä¸åŒèƒŒæ™¯çš„51åå‚ä¸è€…è¿›è¡Œäº†æµ‹è¯•ã€‚æˆ‘ä»¬çš„å®éªŒè®¾è®¡è¯„ä¼°äº†æ¯ä¸ªå‚ä¸è€…ä½¿ç”¨ä¼ ç»Ÿçš„å…·æœ‰GUIçš„ç»Ÿè®¡åˆ†æå·¥å…·å’Œæˆ‘ä»¬å¯¹è¯ä»£ç†æ‰§è¡Œç»Ÿè®¡åˆ†æä»»åŠ¡çš„èƒ½åŠ›ã€‚ç»“æœè¡¨æ˜ï¼Œæ‰€æå‡ºçš„å¯¹è¯ä»£ç†åœ¨æ‰€æœ‰è¯„ä¼°æŒ‡æ ‡ä¸Šå‡æ˜¾è‘—ä¼˜äºGUIç»Ÿè®¡è½¯ä»¶ï¼ŒåŒ…æ‹¬å®šé‡ï¼ˆä»»åŠ¡å®Œæˆæ—¶é—´ã€å‡†ç¡®æ€§å’Œç”¨æˆ·ä½“éªŒï¼‰å’Œå®šæ€§ï¼ˆç”¨æˆ·æ»¡æ„åº¦ï¼‰æŒ‡æ ‡ã€‚æˆ‘ä»¬çš„ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œä½¿ç”¨å¯¹è¯ä»£ç†å¯ä»¥å¢å¼ºç»Ÿè®¡åˆ†æè¿‡ç¨‹ï¼Œé™ä½è®¤çŸ¥è´Ÿè·å’Œå­¦ä¹ æ›²çº¿ï¼Œä»è€Œä½¿æ•°æ®åˆ†æèƒ½åŠ›å‘å¯¹ç»Ÿè®¡äº†è§£æœ‰é™çš„ä¸ªäººæ™®åŠã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.08114v1">PDF</a> 20 pages, 6 figures. Under review</p>
<p><strong>Summary</strong><br>æ•°æ®ç§‘å­¦çš„å¿«é€Ÿå‘å±•ä¿ƒä½¿ä¸åŒèƒŒæ™¯çš„äººä»¬éœ€è¦é€‚åº”ç»Ÿè®¡åˆ†æã€‚æœ¬ç ”ç©¶å‡è®¾å¯¹è¯ä»£ç†æ›´é€‚åˆç»Ÿè®¡åˆ†æï¼Œä¸ä¼ ç»Ÿå›¾å½¢ç”¨æˆ·ç•Œé¢ï¼ˆGUIï¼‰ç›¸æ¯”å…·æœ‰ä¼˜åŠ¿ã€‚æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§æ–°å‹å¯¹è¯ä»£ç†StatZç”¨äºç»Ÿè®¡åˆ†æï¼Œå¹¶è¯„ä¼°å…¶ä¸SPSSã€SASã€Stataå’ŒJMPç­‰ä¸»æµç»Ÿè®¡è½¯ä»¶çš„å‡†ç¡®æ€§ã€ä»»åŠ¡å®Œæˆæ—¶é—´ã€ç”¨æˆ·ä½“éªŒå’Œç”¨æˆ·æ»¡æ„åº¦ã€‚æœ¬ç ”ç©¶ç»“åˆæœ€æ–°è¯­è¨€æ¨¡å‹çš„åˆ†æé—®é¢˜å’Œç»Ÿè®¡ä¸“å®¶çš„å»ºè®®ï¼Œå¯¹æ¥è‡ªä¸åŒèƒŒæ™¯çš„å‚ä¸è€…è¿›è¡Œæµ‹è¯•ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œå¯¹è¯ä»£ç†åœ¨å„é¡¹æŒ‡æ ‡ä¸Šå‡æ˜¾è‘—ä¼˜äºGUIç»Ÿè®¡è½¯ä»¶ï¼ŒåŒ…æ‹¬å®šé‡æŒ‡æ ‡ï¼ˆä»»åŠ¡å®Œæˆæ—¶é—´ã€å‡†ç¡®æ€§å’Œç”¨æˆ·ä½“éªŒï¼‰å’Œå®šæ€§æŒ‡æ ‡ï¼ˆç”¨æˆ·æ»¡æ„åº¦ï¼‰ã€‚æœ¬ç ”ç©¶çªæ˜¾äº†å¯¹è¯ä»£ç†åœ¨å¢å¼ºç»Ÿè®¡åˆ†æè¿‡ç¨‹æ–¹é¢çš„æ½œåŠ›ï¼Œèƒ½é™ä½è®¤çŸ¥è´Ÿè·å’Œå­¦ä¹ æ›²çº¿ï¼Œä½¿ä¸ªä½“å³ä½¿å¯¹ç»Ÿè®¡äº†è§£æœ‰é™ä¹Ÿèƒ½è¿›è¡Œæ•°æ®è§£æã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ•°æ®ç§‘å­¦çš„å¿«é€Ÿå‘å±•ä¿ƒä½¿ä¸åŒèƒŒæ™¯çš„äººéœ€è¦é€‚åº”ç»Ÿè®¡åˆ†æã€‚</li>
<li>å¯¹è¯ä»£ç†åœ¨ç»Ÿè®¡åˆ†ææ–¹é¢å…·æœ‰ä¼˜åŠ¿ï¼Œä¸ä¼ ç»Ÿå›¾å½¢ç”¨æˆ·ç•Œé¢ï¼ˆGUIï¼‰ç›¸æ¯”æ›´é€‚åˆè¿›è¡Œç»Ÿè®¡åˆ†æå·¥ä½œã€‚</li>
<li>æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§æ–°å‹å¯¹è¯ä»£ç†StatZç”¨äºç»Ÿè®¡åˆ†æã€‚</li>
<li>StatZä¸ä¸»æµç»Ÿè®¡è½¯ä»¶SPSSã€SASã€Stataå’ŒJMPè¿›è¡Œäº†è¯„ä¼°æ¯”è¾ƒã€‚</li>
<li>å®éªŒç»“æœæ˜¾ç¤ºå¯¹è¯ä»£ç†åœ¨ä»»åŠ¡å®Œæˆæ—¶é—´ã€å‡†ç¡®æ€§ã€ç”¨æˆ·ä½“éªŒå’Œç”¨æˆ·æ»¡æ„åº¦ç­‰æ–¹é¢å‡æ˜¾è‘—ä¼˜äºGUIç»Ÿè®¡è½¯ä»¶ã€‚</li>
<li>å¯¹è¯ä»£ç†çš„æ½œåŠ›åœ¨äºèƒ½å¤Ÿå¢å¼ºç»Ÿè®¡åˆ†æè¿‡ç¨‹ï¼Œé™ä½è®¤çŸ¥è´Ÿè·å’Œå­¦ä¹ æ›²çº¿ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.08114">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-bfba18b3de6692ce4cfaa7be67283d23.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2770d64267be1773dc627a564add2751.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Multi-Agent-Performative-Prediction-Beyond-the-Insensitivity-Assumption-A-Case-Study-for-Mortgage-Competition"><a href="#Multi-Agent-Performative-Prediction-Beyond-the-Insensitivity-Assumption-A-Case-Study-for-Mortgage-Competition" class="headerlink" title="Multi-Agent Performative Prediction Beyond the Insensitivity Assumption:   A Case Study for Mortgage Competition"></a>Multi-Agent Performative Prediction Beyond the Insensitivity Assumption:   A Case Study for Mortgage Competition</h2><p><strong>Authors:Guanghui Wang, Krishna Acharya, Lokranjan Lakshmikanthan, Vidya Muthukumar, Juba Ziani</strong></p>
<p>Performative prediction models account for feedback loops in decision-making processes where predictions influence future data distributions. While existing work largely assumes insensitivity of data distributions to small strategy changes, this assumption usually fails in real-world competitive (i.e. multi-agent) settings. For example, in Bertrand-type competitions, a small reduction in one firmâ€™s price can lead that firm to capture the entire demand, while all others sharply lose all of their customers.   We study a representative setting of multi-agent performative prediction in which insensitivity assumptions do not hold, and investigate the convergence of natural dynamics. To do so, we focus on a specific game that we call the â€˜â€™Bank Gameâ€™â€™, where two lenders compete over interest rates and credit score thresholds. Consumers act similarly as to in a Bertrand Competition, with each consumer selecting the firm with the lowest interest rate that they are eligible for based on the firmsâ€™ credit thresholds. Our analysis characterizes the equilibria of this game and demonstrates that when both firms use a common and natural no-regret learning dynamic â€“ exponential weights â€“ with proper initialization, the dynamics always converge to stable outcomes despite the general-sum structure. Notably, our setting admits multiple stable equilibria, with convergence dependent on initial conditions. We also provide theoretical convergence results in the stochastic case when the utility matrix is not fully known, but each learner can observe sufficiently many samples of consumers at each time step to estimate it, showing robustness to slight mis-specifications. Finally, we provide experimental results that validate our theoretical findings. </p>
<blockquote>
<p>è¡¨ç°é¢„æµ‹æ¨¡å‹åœ¨å†³ç­–è¿‡ç¨‹ä¸­è€ƒè™‘äº†åé¦ˆç¯ï¼Œå…¶ä¸­é¢„æµ‹å½±å“æœªæ¥æ•°æ®åˆ†å¸ƒã€‚å°½ç®¡ç°æœ‰å·¥ä½œå¤§å¤šå‡è®¾æ•°æ®åˆ†å¸ƒå¯¹å°å¹…ç­–ç•¥å˜åŒ–ä¸æ•æ„Ÿï¼Œä½†åœ¨ç°å®ä¸–ç•Œçš„ç«äº‰ï¼ˆå³å¤šæ™ºèƒ½ä½“ï¼‰ç¯å¢ƒä¸­ï¼Œè¿™ä¸€å‡è®¾é€šå¸¸ä¼šå¤±æ•ˆã€‚ä¾‹å¦‚ï¼Œåœ¨ä¼¯ç‰¹å…°å¾·å¼ç«äº‰ä¸­ï¼Œä¸€å®¶å…¬å¸ä»·æ ¼çš„å°å¹…é™ä½å¯èƒ½ä¼šå¯¼è‡´å…¶è·å¾—å…¨éƒ¨éœ€æ±‚ï¼Œè€Œå…¶ä»–æ‰€æœ‰å…¬å¸éƒ½ä¼šå¤±å»æ‰€æœ‰å®¢æˆ·ã€‚æˆ‘ä»¬ç ”ç©¶äº†å¤šæ™ºèƒ½ä½“è¡¨ç°é¢„æµ‹çš„ä»£è¡¨æ€§è®¾ç½®ï¼Œå…¶ä¸­ä¸è€ƒè™‘æ•æ„Ÿæ€§çš„å‡è®¾ä¸æˆç«‹ï¼Œå¹¶ç ”ç©¶äº†è‡ªç„¶åŠ¨æ€çš„æ”¶æ•›æ€§ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬å…³æ³¨ä¸€ä¸ªç‰¹å®šçš„æ¸¸æˆï¼Œæˆ‘ä»¬ç§°ä¹‹ä¸ºâ€œé“¶è¡Œæ¸¸æˆâ€ï¼Œä¸¤å®¶è´·æ¬¾äººåœ¨åˆ©ç‡å’Œä¿¡ç”¨è¯„åˆ†é—¨æ§›ä¸Šè¿›è¡Œç«äº‰ã€‚æ¶ˆè´¹è€…çš„è¡Œä¸ºç±»ä¼¼äºä¼¯ç‰¹å…°å¾·ç«äº‰ï¼Œæ¯ä¸ªæ¶ˆè´¹è€…éƒ½ä¼šé€‰æ‹©åˆ©ç‡æœ€ä½çš„ç¬¦åˆå…¶ä¿¡ç”¨é—¨æ§›çš„å…¬å¸ã€‚æˆ‘ä»¬çš„åˆ†æå¯¹è¿™åœºæ¸¸æˆçš„å¹³è¡¡çŠ¶æ€è¿›è¡Œäº†æè¿°ï¼Œå¹¶è¯æ˜å½“ä¸¤å®¶å…¬å¸éƒ½é‡‡ç”¨ä¸€ç§é€šç”¨çš„æ— é—æ†¾å­¦ä¹ åŠ¨æ€â€”â€”æŒ‡æ•°æƒé‡ï¼Œå¹¶è¿›è¡Œäº†é€‚å½“çš„åˆå§‹åŒ–æ—¶ï¼Œå°½ç®¡æ˜¯æ€»æ”¶ç›Šç»“æ„ï¼Œè¿™ç§åŠ¨æ€æ€»æ˜¯ä¼šæ”¶æ•›åˆ°ç¨³å®šçš„ç»“æœã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œæˆ‘ä»¬çš„è®¾ç½®å…è®¸å­˜åœ¨å¤šä¸ªç¨³å®šå¹³è¡¡ç‚¹ï¼Œæ”¶æ•›æƒ…å†µå–å†³äºåˆå§‹æ¡ä»¶ã€‚å½“æ•ˆç”¨çŸ©é˜µä¸å®Œå…¨å·²çŸ¥ï¼Œä½†æ¯ä¸ªå­¦ä¹ è€…å¯ä»¥åœ¨æ¯ä¸ªæ—¶é—´æ­¥è§‚å¯Ÿè¶³å¤Ÿå¤šçš„æ¶ˆè´¹è€…æ ·æœ¬æ¥ä¼°è®¡å®ƒæ—¶ï¼Œæˆ‘ä»¬è¿˜æä¾›äº†ç†è®ºä¸Šçš„æ”¶æ•›ç»“æœï¼Œè¯æ˜äº†å…¶å¯¹è½»å¾®è¯¯æŒ‡å®šçš„ç¨³å¥æ€§ã€‚æœ€åï¼Œæˆ‘ä»¬æä¾›äº†å®éªŒç»“æœæ¥éªŒè¯æˆ‘ä»¬çš„ç†è®ºå‘ç°ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.08063v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ç ”ç©¶äº†å¤šæ™ºèƒ½ä½“è¡¨ç°é¢„æµ‹çš„ä¸€ä¸ªä»£è¡¨æ€§åœºæ™¯ï¼Œå…¶ä¸­æ•°æ®åˆ†å¸ƒå¯¹ç­–ç•¥å˜åŒ–ä¸æ•æ„Ÿå‡è®¾ä¸æˆç«‹ã€‚æ–‡ç« ä»¥ä¸€ä¸ªåä¸ºâ€œé“¶è¡Œæ¸¸æˆâ€çš„åšå¼ˆä¸ºä¾‹ï¼Œåˆ†æäº†ä¸¤å®¶è´·æ¬¾æœºæ„åœ¨åˆ©ç‡å’Œä¿¡ç”¨è¯„åˆ†é—¨æ§›ä¸Šçš„ç«äº‰ã€‚æ¶ˆè´¹è€…çš„è¡Œä¸ºç±»ä¼¼äºä¼¯ç‰¹å…°å¾·ç«äº‰ï¼Œä¼šé€‰æ‹©åˆ©ç‡æœ€ä½ä¸”ç¬¦åˆè‡ªèº«ä¿¡ç”¨é—¨æ§›çš„æœºæ„ã€‚æ–‡ç« é‡ç‚¹åˆ†æäº†è¯¥åšå¼ˆçš„å‡è¡¡çŠ¶æ€ï¼Œå¹¶æŒ‡å‡ºå½“ä¸¤å®¶æœºæ„é‡‡ç”¨é€šç”¨çš„æ— åæ‚”å­¦ä¹ åŠ¨æ€â€”â€”æŒ‡æ•°æƒé‡ï¼Œå¹¶é€‚å½“åˆå§‹åŒ–æ—¶ï¼Œå³ä¾¿æ˜¯åœ¨ä¸€èˆ¬å’Œåšå¼ˆç»“æ„ä¸­ï¼Œä¹Ÿèƒ½è¾¾åˆ°ç¨³å®šç»“æœã€‚æ–‡ç« è¿˜æ¢è®¨äº†ä¸ç¡®å®šæƒ…å¢ƒä¸‹çš„ç†è®ºæ”¶æ•›ç»“æœï¼Œå¹¶é€šè¿‡å®éªŒéªŒè¯äº†ç†è®ºå‘ç°ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è¡¨ç°é¢„æµ‹æ¨¡å‹è€ƒè™‘å†³ç­–è¿‡ç¨‹ä¸­çš„åé¦ˆç¯è·¯ï¼Œé¢„æµ‹å½±å“æœªæ¥æ•°æ®åˆ†å¸ƒã€‚</li>
<li>ç°æœ‰å·¥ä½œå¤§å¤šå‡è®¾æ•°æ®åˆ†å¸ƒå¯¹ç­–ç•¥å˜åŒ–ä¸æ•æ„Ÿï¼Œä½†åœ¨ç°å®ç«äº‰ç¯å¢ƒä¸­è¿™ä¸€å‡è®¾å¸¸ä¸æˆç«‹ã€‚</li>
<li>â€œé“¶è¡Œæ¸¸æˆâ€ä½œä¸ºå¤šæ™ºèƒ½ä½“è¡¨ç°é¢„æµ‹çš„ä»£è¡¨åœºæ™¯ï¼Œåˆ†æäº†ä¸¤å®¶è´·æ¬¾æœºæ„åœ¨åˆ©ç‡å’Œä¿¡ç”¨è¯„åˆ†é—¨æ§›ä¸Šçš„ç«äº‰ã€‚</li>
<li>æ¶ˆè´¹è€…è¡Œä¸ºä¸ä¼¯ç‰¹å…°å¾·ç«äº‰ç›¸ä¼¼ï¼Œé€‰æ‹©ç¬¦åˆè‡ªèº«ä¿¡ç”¨é—¨æ§›ä¸”åˆ©ç‡æœ€ä½çš„æœºæ„ã€‚</li>
<li>é‡‡ç”¨æ— åæ‚”å­¦ä¹ åŠ¨æ€å’Œé€‚å½“çš„åˆå§‹åŒ–ï¼Œèƒ½å¤Ÿè¾¾åˆ°ç¨³å®šç»“æœï¼Œä¸”æ”¶æ•›ä¸åˆå§‹æ¡ä»¶æœ‰å…³ã€‚</li>
<li>åšå¼ˆå­˜åœ¨å¤šä¸ªç¨³å®šå‡è¡¡çŠ¶æ€ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.08063">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-87021297b1542af67c7d0bd02e4343e2.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-026d01a6c40c00164a43426bc04df238.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="VSC-RL-Advancing-Autonomous-Vision-Language-Agents-with-Variational-Subgoal-Conditioned-Reinforcement-Learning"><a href="#VSC-RL-Advancing-Autonomous-Vision-Language-Agents-with-Variational-Subgoal-Conditioned-Reinforcement-Learning" class="headerlink" title="VSC-RL: Advancing Autonomous Vision-Language Agents with Variational   Subgoal-Conditioned Reinforcement Learning"></a>VSC-RL: Advancing Autonomous Vision-Language Agents with Variational   Subgoal-Conditioned Reinforcement Learning</h2><p><strong>Authors:Qingyuan Wu, Jianheng Liu, Jianye Hao, Jun Wang, Kun Shao</strong></p>
<p>State-of-the-art (SOTA) reinforcement learning (RL) methods enable the vision-language agents to learn from interactions with the environment without human supervision. However, they struggle with learning inefficiencies in tackling real-world complex sequential decision-making tasks, especially with sparse reward signals and long-horizon dependencies. To effectively address the issue, we introduce Variational Subgoal-Conditioned RL (VSC-RL), which reformulates the vision-language sequential decision-making task as a variational goal-conditioned RL problem, allowing us to leverage advanced optimization methods to enhance learning efficiency. Specifically, VSC-RL optimizes the SubGoal Evidence Lower BOund (SGC-ELBO), which consists of (a) maximizing the subgoal-conditioned return via RL and (b) minimizing the subgoal-conditioned difference with the reference policy. We theoretically demonstrate that SGC-ELBO is equivalent to the original optimization objective, ensuring improved learning efficiency without sacrificing performance guarantees. Additionally, for real-world complex decision-making tasks, VSC-RL leverages the vision-language model to autonomously decompose the goal into feasible subgoals, enabling efficient learning. Across various benchmarks, including challenging real-world mobile device control tasks, VSC-RL significantly outperforms the SOTA vision-language agents, achieving superior performance and remarkable improvement in learning efficiency. </p>
<blockquote>
<p>å½“å‰æœ€å…ˆè¿›çš„å¼ºåŒ–å­¦ä¹ æ–¹æ³•ä½¿è§†è§‰è¯­è¨€ä»£ç†èƒ½å¤Ÿåœ¨ä¸ç¯å¢ƒçš„äº¤äº’ä¸­å­¦ä¹ ï¼Œæ— éœ€äººç±»ç›‘ç£ã€‚ç„¶è€Œï¼Œå®ƒä»¬åœ¨å¤„ç†ç°å®ä¸–ç•Œä¸­çš„å¤æ‚åºåˆ—å†³ç­–ä»»åŠ¡æ—¶ï¼Œé¢ä¸´ç€å­¦ä¹ æ•ˆç‡ä½çš„é—®é¢˜ï¼Œç‰¹åˆ«æ˜¯åœ¨å¥–åŠ±ä¿¡å·ç¨€ç–å’Œé•¿æœŸä¾èµ–çš„æƒ…å†µä¸‹ã€‚ä¸ºäº†æœ‰æ•ˆè§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†å˜åˆ†å­ç›®æ ‡æ¡ä»¶å¼ºåŒ–å­¦ä¹ ï¼ˆVSC-RLï¼‰ï¼Œå®ƒå°†è§†è§‰è¯­è¨€åºåˆ—å†³ç­–ä»»åŠ¡é‡æ–°å®šä¹‰ä¸ºå˜åˆ†å­ç›®æ ‡æ¡ä»¶å¼ºåŒ–å­¦ä¹ é—®é¢˜ï¼Œä½¿æˆ‘ä»¬èƒ½å¤Ÿåˆ©ç”¨å…ˆè¿›çš„ä¼˜åŒ–æ–¹æ³•æ¥æé«˜å­¦ä¹ æ•ˆç‡ã€‚å…·ä½“æ¥è¯´ï¼ŒVSC-RLä¼˜åŒ–äº†å­ç›®æ ‡è¯æ®ä¸‹ç•Œï¼ˆSGC-ELBOï¼‰ï¼Œè¿™åŒ…æ‹¬ï¼ˆaï¼‰é€šè¿‡å¼ºåŒ–å­¦ä¹ æœ€å¤§åŒ–å­ç›®æ ‡æ¡ä»¶ä¸‹çš„å›æŠ¥å’Œï¼ˆbï¼‰æœ€å°åŒ–å­ç›®æ ‡æ¡ä»¶ä¸å‚è€ƒç­–ç•¥ä¹‹é—´çš„å·®å¼‚ã€‚æˆ‘ä»¬ä»ç†è®ºä¸Šè¯æ˜äº†SGC-ELBOç­‰åŒäºåŸå§‹çš„ä¼˜åŒ–ç›®æ ‡ï¼Œå¯ä»¥åœ¨ä¸ç‰ºç‰²æ€§èƒ½ä¿è¯çš„æƒ…å†µä¸‹æé«˜å­¦ä¹ æ•ˆç‡ã€‚æ­¤å¤–ï¼Œå¯¹äºç°å®ä¸–ç•Œçš„å¤æ‚å†³ç­–ä»»åŠ¡ï¼ŒVSC-RLåˆ©ç”¨è§†è§‰è¯­è¨€æ¨¡å‹è‡ªä¸»åœ°å°†ç›®æ ‡åˆ†è§£ä¸ºå¯è¡Œçš„å­ç›®æ ‡ï¼Œä»è€Œå®ç°é«˜æ•ˆå­¦ä¹ ã€‚åœ¨åŒ…æ‹¬å…·æœ‰æŒ‘æˆ˜æ€§çš„ç°å®ä¸–ç•Œç§»åŠ¨è®¾å¤‡æ§åˆ¶ä»»åŠ¡åœ¨å†…çš„å„ç§åŸºå‡†æµ‹è¯•ä¸­ï¼ŒVSC-RLæ˜¾è‘—ä¼˜äºæœ€å…ˆè¿›çš„è§†è§‰è¯­è¨€ä»£ç†ï¼Œåœ¨æ€§èƒ½å’Œå­¦ä¹ æ•ˆç‡æ–¹é¢å–å¾—äº†å“è¶Šçš„è¡¨ç°å’Œæ˜¾è‘—çš„æ”¹è¿›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.07949v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>è§†è§‰è¯­è¨€å¼ºåŒ–å­¦ä¹ æ¨¡å‹åœ¨é¢å¯¹çœŸå®ä¸–ç•Œå¤æ‚åºåˆ—å†³ç­–ä»»åŠ¡æ—¶é¢ä¸´å­¦ä¹ æ•ˆç‡ä½ä¸‹çš„é—®é¢˜ï¼Œå°¤å…¶æ˜¯åœ¨ç¨€ç–å¥–åŠ±ä¿¡å·å’Œé•¿æœŸä¾èµ–çš„é—®é¢˜ä¸‹ã€‚é’ˆå¯¹è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºå˜åˆ†å­ç›®æ ‡æ¡ä»¶å¼ºåŒ–å­¦ä¹ ï¼ˆVSC-RLï¼‰ï¼Œå°†è§†è§‰è¯­è¨€åºåˆ—å†³ç­–ä»»åŠ¡é‡æ–°è¡¨è¿°ä¸ºå˜åˆ†å­ç›®æ ‡æ¡ä»¶å¼ºåŒ–å­¦ä¹ é—®é¢˜ï¼Œåˆ©ç”¨å…ˆè¿›çš„ä¼˜åŒ–æ–¹æ³•æé«˜å­¦ä¹ æ•ˆç‡ã€‚VSC-RLä¼˜åŒ–å­ç›®æ ‡è¯æ®ä¸‹ç•Œï¼ˆSGC-ELBOï¼‰ï¼ŒåŒ…æ‹¬æœ€å¤§åŒ–å­ç›®æ ‡æ¡ä»¶ä¸‹çš„å›æŠ¥å’Œæœ€å°åŒ–å­ç›®æ ‡ä¸å‚è€ƒç­–ç•¥ä¹‹é—´çš„å·®å¼‚ã€‚VSC-RLåœ¨çœŸå®ä¸–ç•Œçš„å¤æ‚å†³ç­–ä»»åŠ¡ä¸­ï¼Œèƒ½å¤Ÿè‡ªä¸»åœ°å°†ç›®æ ‡åˆ†è§£ä¸ºå¯è¡Œçš„å­ç›®æ ‡ï¼Œä»è€Œå®ç°é«˜æ•ˆå­¦ä¹ ã€‚åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­ï¼ŒVSC-RLæ˜¾è‘—ä¼˜äºæœ€å…ˆè¿›çš„è§†è§‰è¯­è¨€æ¨¡å‹ï¼Œåœ¨å­¦ä¹ æ•ˆç‡å’Œæ€§èƒ½ä¸Šå‡è¡¨ç°å‡ºå“è¶Šçš„è¡¨ç°ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>å¼ºåŒ–å­¦ä¹ åœ¨è§†è§‰è¯­è¨€ä»£ç†ä¸­å¯¹äºç¯å¢ƒäº¤äº’çš„æ— ç›‘ç£å­¦ä¹ è‡³å…³é‡è¦ã€‚</li>
<li>ç„¶è€Œï¼Œæœ€å…ˆè¿›çš„å¼ºåŒ–å­¦ä¹ æ–¹æ³•åœ¨å¤„ç†çœŸå®ä¸–ç•Œçš„å¤æ‚åºåˆ—å†³ç­–ä»»åŠ¡æ—¶å­˜åœ¨å­¦ä¹ æ•ˆç‡ä½ä¸‹çš„é—®é¢˜ã€‚</li>
<li>å˜åˆ†å­ç›®æ ‡æ¡ä»¶å¼ºåŒ–å­¦ä¹ ï¼ˆVSC-RLï¼‰è¢«å¼•å…¥ä»¥è§£å†³è¿™ä¸€é—®é¢˜ã€‚</li>
<li>VSC-RLå°†è§†è§‰è¯­è¨€åºåˆ—å†³ç­–ä»»åŠ¡é‡æ–°è¡¨è¿°ä¸ºå˜åˆ†å­ç›®æ ‡æ¡ä»¶å¼ºåŒ–å­¦ä¹ é—®é¢˜ã€‚</li>
<li>VSC-RLä¼˜åŒ–å­ç›®æ ‡è¯æ®ä¸‹ç•Œï¼ˆSGC-ELBOï¼‰ï¼ŒåŒ…æ‹¬æœ€å¤§åŒ–å­ç›®æ ‡æ¡ä»¶ä¸‹çš„å›æŠ¥å’Œæœ€å°åŒ–ä¸å‚è€ƒç­–ç•¥çš„å·®å¼‚ã€‚</li>
<li>VSC-RLåœ¨è‡ªä¸»åˆ†è§£ç›®æ ‡ä¸ºå­ç›®æ ‡æ–¹é¢è¡¨ç°å‡ºä¼˜åŠ¿ï¼Œæé«˜äº†å¤æ‚å†³ç­–ä»»åŠ¡çš„å­¦ä¹ æ•ˆç‡ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.07949">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-1196ad2343cab812fcb892e399ab7d37.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d76540e1df68679899ec3d3dc9f29534.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-ee834b2a0326c2e5acefdb7a48b8bd0c.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Symbiotic-Cooperation-for-Web-Agents-Harnessing-Complementary-Strengths-of-Large-and-Small-LLMs"><a href="#Symbiotic-Cooperation-for-Web-Agents-Harnessing-Complementary-Strengths-of-Large-and-Small-LLMs" class="headerlink" title="Symbiotic Cooperation for Web Agents: Harnessing Complementary Strengths   of Large and Small LLMs"></a>Symbiotic Cooperation for Web Agents: Harnessing Complementary Strengths   of Large and Small LLMs</h2><p><strong>Authors:Ruichen Zhang, Mufan Qiu, Zhen Tan, Mohan Zhang, Vincent Lu, Jie Peng, Kaidi Xu, Leandro Z. Agudelo, Peter Qian, Tianlong Chen</strong></p>
<p>Web browsing agents powered by large language models (LLMs) have shown tremendous potential in automating complex web-based tasks. Existing approaches typically rely on large LLMs (e.g., GPT-4o) to explore web environments and generate trajectory data, which is then used either for demonstration retrieval (for large LLMs) or to distill small LLMs (e.g., Llama3) in a process that remains decoupled from the exploration. In this paper, we propose AgentSymbiotic, an iterative framework that couples data synthesis with task-performance, yielding a â€œsymbiotic improvementâ€ for both large and small LLMs. Our study uncovers a complementary dynamic between LLM types: while large LLMs excel at generating high-quality trajectories for distillation, the distilled small LLMs-owing to their distinct reasoning capabilities-often choose actions that diverge from those of their larger counterparts. This divergence drives the exploration of novel trajectories, thereby enriching the synthesized data. However, we also observe that the performance of small LLMs becomes a bottleneck in this iterative enhancement process. To address this, we propose two innovations in LLM distillation: a speculative data synthesis strategy that mitigates off-policy bias, and a multi-task learning approach designed to boost the reasoning capabilities of the student LLM. Furthermore, we introduce a Hybrid Mode for Privacy Preservation to address user privacy concerns. Evaluated on the WEBARENA benchmark, AgentSymbiotic achieves SOTA performance with both LLM types. Our best Large LLM agent reaches 52%, surpassing the previous best of 45%, while our 8B distilled model demonstrates a competitive 49%, exceeding the prior best of 28%. Code will be released upon acceptance. </p>
<blockquote>
<p>ç”±å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰é©±åŠ¨çš„ç½‘é¡µæµè§ˆä»£ç†åœ¨è‡ªåŠ¨åŒ–å¤æ‚çš„ç½‘ç»œä»»åŠ¡æ–¹é¢è¡¨ç°å‡ºäº†å·¨å¤§çš„æ½œåŠ›ã€‚ç°æœ‰æ–¹æ³•é€šå¸¸ä¾èµ–äºå¤§å‹LLMï¼ˆä¾‹å¦‚GPT-4oï¼‰æ¥æ¢ç´¢ç½‘ç»œç¯å¢ƒå’Œç”Ÿæˆè½¨è¿¹æ•°æ®ï¼Œè¿™äº›æ•°æ®ç„¶åç”¨äºæ¼”ç¤ºæ£€ç´¢ï¼ˆé’ˆå¯¹å¤§å‹LLMï¼‰æˆ–è’¸é¦å°å‹LLMï¼ˆä¾‹å¦‚Llama3ï¼‰ï¼Œè¿™ä¸€è¿‡ç¨‹ä¸æ¢ç´¢è¿‡ç¨‹ç›¸åˆ†ç¦»ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†AgentSymbioticï¼Œè¿™æ˜¯ä¸€ä¸ªè¿­ä»£æ¡†æ¶ï¼Œå®ƒå°†æ•°æ®åˆæˆä¸ä»»åŠ¡æ€§èƒ½ç›¸ç»“åˆï¼Œä¸ºå¤§å‹å’Œå°å‹LLMå¸¦æ¥â€œå…±ç”Ÿæ”¹è¿›â€ã€‚æˆ‘ä»¬çš„ç ”ç©¶å‘ç°å¤§å‹LLMå’Œå°å‹LLMä¹‹é—´å­˜åœ¨ä¸€ç§äº’è¡¥åŠ¨æ€ï¼šè™½ç„¶å¤§å‹LLMæ“…é•¿ç”Ÿæˆç”¨äºè’¸é¦çš„é«˜è´¨é‡è½¨è¿¹ï¼Œä½†ç”±äºå…¶ç‹¬ç‰¹çš„æ¨ç†èƒ½åŠ›ï¼Œè’¸é¦åçš„å°å‹LLMå¾€å¾€ä¼šé€‰æ‹©ä¸å…¶è¾ƒå¤§çš„åŒç±»ä¸åŒçš„è¡ŒåŠ¨ã€‚è¿™ç§åˆ†æ­§æ¨åŠ¨äº†æ–°è½¨è¿¹çš„æ¢ç´¢ï¼Œä»è€Œä¸°å¯Œäº†åˆæˆæ•°æ®ã€‚ç„¶è€Œï¼Œæˆ‘ä»¬ä¹Ÿè§‚å¯Ÿåˆ°å°å‹LLMçš„æ€§èƒ½æˆä¸ºè¿™ç§è¿­ä»£å¢å¼ºè¿‡ç¨‹çš„ç“¶é¢ˆã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬åœ¨LLMè’¸é¦æ–¹é¢æå‡ºäº†ä¸¤é¡¹åˆ›æ–°ï¼šä¸€ç§ç¼“è§£ç¦»ç­–ç•¥åå·®çš„æŠ•æœºæ•°æ®åˆæˆç­–ç•¥ï¼Œä»¥åŠä¸€ç§æ—¨åœ¨æå‡å­¦ç”ŸLLMæ¨ç†èƒ½åŠ›çš„å¤šä»»åŠ¡å­¦ä¹ æ³•ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§æ··åˆæ¨¡å¼æ¥è¿›è¡Œéšç§ä¿æŠ¤ï¼Œä»¥è§£å†³ç”¨æˆ·éšç§æ‹…å¿§ã€‚åœ¨WEBARENAåŸºå‡†æµ‹è¯•ä¸Šï¼ŒAgentSymbioticåœ¨ä¸¤ç§LLMç±»å‹ä¸Šéƒ½å®ç°äº†SOTAæ€§èƒ½ã€‚æˆ‘ä»¬æœ€å¥½çš„å¤§å‹LLMä»£ç†è¾¾åˆ°äº†52%ï¼Œè¶…è¿‡äº†ä¹‹å‰çš„æœ€ä½³æˆç»©45%ï¼Œè€Œæˆ‘ä»¬è’¸é¦çš„8Bæ¨¡å‹ä¹Ÿè¡¨ç°å‡ºç«äº‰åŠ›ï¼Œè¾¾åˆ°49%ï¼Œè¶…è¿‡äº†ä¹‹å‰çš„æœ€ä½³æˆç»©28%ã€‚ä»£ç å°†åœ¨æ¥å—åå‘å¸ƒã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.07942v1">PDF</a> </p>
<p><strong>Summary</strong><br>     å¤§å‹è¯­è¨€æ¨¡å‹é©±åŠ¨çš„ç½‘é¡µæµè§ˆä»£ç†åœ¨è‡ªåŠ¨åŒ–å¤æ‚ç½‘ç»œä»»åŠ¡æ–¹é¢å±•ç°å‡ºå·¨å¤§æ½œåŠ›ã€‚ç°æœ‰æ–¹æ³•é€šå¸¸ä¾èµ–å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆå¦‚GPT-4oï¼‰æ¥æ¢ç´¢ç½‘ç»œç¯å¢ƒå’Œç”Ÿæˆè½¨è¿¹æ•°æ®ï¼Œè¿™äº›æ•°æ®ç”¨äºæ¼”ç¤ºæ£€ç´¢æˆ–è’¸é¦å°å‹è¯­è¨€æ¨¡å‹ï¼ˆå¦‚Llama3ï¼‰ã€‚æœ¬æ–‡æå‡ºAgentSymbioticæ¡†æ¶ï¼Œå°†æ•°æ®æŒ–æ˜ä¸ä»»åŠ¡æ‰§è¡Œç›¸ç»“åˆï¼Œå®ç°äº†å¤§å‹å’Œå°å‹è¯­è¨€æ¨¡å‹çš„â€œå…±ç”Ÿæ”¹è¿›â€ã€‚ç ”ç©¶å‘ç°å¤§å‹è¯­è¨€æ¨¡å‹åœ¨ç”Ÿæˆé«˜è´¨é‡è½¨è¿¹ç”¨äºè’¸é¦æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œè€Œè’¸é¦å¾—åˆ°çš„å°å‹è¯­è¨€æ¨¡å‹å› å…¶ç‹¬ç‰¹çš„æ¨ç†èƒ½åŠ›ä¼šé€‰æ‹©ä¸å¤§å‹æ¨¡å‹ä¸åŒçš„è¡ŒåŠ¨ï¼Œæ¨åŠ¨æ¢ç´¢æ–°è½¨è¿¹ï¼Œä»è€Œä¸°å¯Œåˆæˆæ•°æ®ã€‚ä½†å°å‹è¯­è¨€æ¨¡å‹çš„æ€§èƒ½æˆä¸ºè¿­ä»£å¢å¼ºè¿‡ç¨‹çš„ç“¶é¢ˆã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºä¸¤ç§åˆ›æ–°çš„è¯­è¨€æ¨¡å‹è’¸é¦æ–¹æ³•ï¼šä¸€ç§ç¼“è§£ç¦»ç­–ç•¥åå·®çš„æ¨æµ‹æ•°æ®åˆæˆç­–ç•¥ï¼Œä»¥åŠä¸€ç§æ—¨åœ¨æå‡å­¦ç”Ÿè¯­è¨€æ¨¡å‹æ¨ç†èƒ½åŠ›çš„å¤šä»»åŠ¡å­¦ä¹ æ–¹æ³•ã€‚æ­¤å¤–ï¼Œæœ¬æ–‡è¿˜ä»‹ç»äº†æ··åˆæ¨¡å¼éšç§ä¿æŠ¤æ¥è§£å†³ç”¨æˆ·éšç§æ‹…å¿§ã€‚åœ¨WEBARENAåŸºå‡†ä¸Šçš„è¯„ä¼°æ˜¾ç¤ºï¼ŒAgentSymbioticåœ¨ä¸¤ç§è¯­è¨€æ¨¡å‹ç±»å‹ä¸Šéƒ½å®ç°äº†æœ€ä½³æ€§èƒ½ã€‚æœ€ä½³å¤§å‹è¯­è¨€æ¨¡å‹ä»£ç†è¾¾åˆ°52%ï¼Œè¶…è¿‡ä¹‹å‰çš„æœ€ä½³æˆç»©45%ï¼Œè€Œæˆ‘ä»¬çš„8Bè’¸é¦æ¨¡å‹è¡¨ç°å‡ºç«äº‰åŠ›ï¼Œè¾¾åˆ°49%ï¼Œè¶…è¿‡ä¹‹å‰çš„æœ€ä½³æˆç»©28%ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹åœ¨ç”Ÿæˆé«˜è´¨é‡è½¨è¿¹æ•°æ®æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œè¿™äº›æ•°æ®å¯ç”¨äºè‡ªåŠ¨åŒ–å¤æ‚ç½‘ç»œä»»åŠ¡ã€‚</li>
<li>AgentSymbioticæ¡†æ¶å®ç°äº†å¤§å‹å’Œå°å‹è¯­è¨€æ¨¡å‹çš„å…±ç”Ÿæ”¹è¿›ï¼Œé€šè¿‡ç»“åˆæ•°æ®æŒ–æ˜ä¸ä»»åŠ¡æ‰§è¡Œæå‡ä¸¤è€…æ€§èƒ½ã€‚</li>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ç”Ÿæˆçš„æ•°æ®å¯ç”¨äºè’¸é¦å°å‹è¯­è¨€æ¨¡å‹ï¼Œåè€…å› å…¶ç‹¬ç‰¹çš„æ¨ç†èƒ½åŠ›ä¼šé€‰æ‹©ä¸åŒäºå¤§å‹æ¨¡å‹çš„è¡ŒåŠ¨è·¯å¾„ï¼Œä»è€Œæ¨åŠ¨æ¢ç´¢æ–°çš„è½¨è¿¹ã€‚</li>
<li>è¿­ä»£å¢å¼ºè¿‡ç¨‹ä¸­å­˜åœ¨å°å‹è¯­è¨€æ¨¡å‹æ€§èƒ½ç“¶é¢ˆé—®é¢˜ï¼Œä¸ºæ­¤æå‡ºäº†ä¸¤ç§åˆ›æ–°çš„è¯­è¨€æ¨¡å‹è’¸é¦æ–¹æ³•ã€‚</li>
<li>å¼•å…¥ç¼“è§£ç¦»ç­–ç•¥åå·®çš„æ¨æµ‹æ•°æ®åˆæˆç­–ç•¥å’Œå¤šä»»åŠ¡å­¦ä¹ æ–¹æ³•æ¥æå‡å°å‹è¯­è¨€æ¨¡å‹çš„æ€§èƒ½ã€‚</li>
<li>ä»‹ç»äº†æ··åˆæ¨¡å¼éšç§ä¿æŠ¤æ¥è§£å†³ç”¨æˆ·éšç§æ‹…å¿§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.07942">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-04fc13813f6549fb5d8d50b7fb684543.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-007d40d0b7cf02dff1714a8e95ec2b94.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-ef13a853a4cac5d4383391f8c431efc9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-63273f0c1de23610ca3bed57b50755cf.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="On-Memory-Construction-and-Retrieval-for-Personalized-Conversational-Agents"><a href="#On-Memory-Construction-and-Retrieval-for-Personalized-Conversational-Agents" class="headerlink" title="On Memory Construction and Retrieval for Personalized Conversational   Agents"></a>On Memory Construction and Retrieval for Personalized Conversational   Agents</h2><p><strong>Authors:Zhuoshi Pan, Qianhui Wu, Huiqiang Jiang, Xufang Luo, Hao Cheng, Dongsheng Li, Yuqing Yang, Chin-Yew Lin, H. Vicky Zhao, Lili Qiu, Jianfeng Gao</strong></p>
<p>To deliver coherent and personalized experiences in long-term conversations, existing approaches typically perform retrieval augmented response generation by constructing memory banks from conversation history at either the turn-level, session-level, or through summarization techniques. In this paper, we present two key findings: (1) The granularity of memory unit matters: Turn-level, session-level, and summarization-based methods each exhibit limitations in both memory retrieval accuracy and the semantic quality of the retrieved content. (2) Prompt compression methods, such as \textit{LLMLingua-2}, can effectively serve as a denoising mechanism, enhancing memory retrieval accuracy across different granularities. Building on these insights, we propose SeCom, a method that constructs a memory bank with topical segments by introducing a conversation Segmentation model, while performing memory retrieval based on Compressed memory units. Experimental results show that SeCom outperforms turn-level, session-level, and several summarization-based methods on long-term conversation benchmarks such as LOCOMO and Long-MT-Bench+. Additionally, the proposed conversation segmentation method demonstrates superior performance on dialogue segmentation datasets such as DialSeg711, TIAGE, and SuperDialSeg. </p>
<blockquote>
<p>ä¸ºäº†åœ¨é•¿æœŸå¯¹è¯ä¸­æä¾›è¿è´¯ä¸”ä¸ªæ€§åŒ–çš„ä½“éªŒï¼Œç°æœ‰æ–¹æ³•é€šå¸¸é€šè¿‡æ„å»ºè®°å¿†åº“æ¥å¢å¼ºå“åº”ç”Ÿæˆï¼Œè¿™äº›è®°å¿†åº“æ˜¯é€šè¿‡å¯¹è¯å†å²æ„å»ºçš„ï¼Œæ— è®ºæ˜¯è½®æ¬¡çº§åˆ«ã€ä¼šè¯çº§åˆ«ï¼Œè¿˜æ˜¯é€šè¿‡æ‘˜è¦æŠ€æœ¯ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬å‘ç°äº†ä¸¤ä¸ªå…³é”®è§‚ç‚¹ï¼šï¼ˆ1ï¼‰è®°å¿†å•å…ƒçš„ç²’åº¦å¾ˆé‡è¦ï¼šè½®æ¬¡çº§åˆ«ã€ä¼šè¯çº§åˆ«å’ŒåŸºäºæ‘˜è¦çš„æ–¹æ³•åœ¨è®°å¿†æ£€ç´¢å‡†ç¡®æ€§å’Œæ£€ç´¢å†…å®¹çš„è¯­ä¹‰è´¨é‡æ–¹é¢éƒ½å­˜åœ¨å±€é™æ€§ã€‚ï¼ˆ2ï¼‰æç¤ºå‹ç¼©æ–¹æ³•ï¼Œå¦‚LLMLingua-2ï¼Œå¯ä»¥æœ‰æ•ˆåœ°ä½œä¸ºä¸€ç§å»å™ªæœºåˆ¶ï¼Œæé«˜ä¸åŒç²’åº¦ä¸‹çš„å†…å­˜æ£€ç´¢å‡†ç¡®æ€§ã€‚åŸºäºè¿™äº›è§è§£ï¼Œæˆ‘ä»¬æå‡ºäº†SeComæ–¹æ³•ï¼Œå®ƒé€šè¿‡å¼•å…¥å¯¹è¯åˆ†æ®µæ¨¡å‹æ„å»ºä»¥ä¸»é¢˜æ®µä¸ºè®°å¿†åº“ï¼ŒåŒæ—¶åŸºäºå‹ç¼©è®°å¿†å•å…ƒè¿›è¡Œå†…å­˜æ£€ç´¢ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒSeComåœ¨LOCOMOå’ŒLong-MT-Bench+ç­‰é•¿æœŸå¯¹è¯åŸºå‡†æµ‹è¯•ä¸­ä¼˜äºè½®æ¬¡çº§åˆ«ã€ä¼šè¯çº§åˆ«å’Œå‡ ç§åŸºäºæ‘˜è¦çš„æ–¹æ³•ã€‚æ­¤å¤–ï¼Œæ‰€æå‡ºçš„å¯¹è¯åˆ†å‰²æ–¹æ³•åœ¨DialSeg711ã€TIAGEå’ŒSuperDialSegç­‰å¯¹è¯åˆ†å‰²æ•°æ®é›†ä¸Šè¡¨ç°å‡ºå“è¶Šçš„æ€§èƒ½ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.05589v2">PDF</a> 10 pages, 5 figures, conference</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ¢è®¨äº†é•¿æœŸå¯¹è¯ä¸­çš„è¿è´¯æ€§å’Œä¸ªæ€§åŒ–ä½“éªŒå®ç°æ–¹æ³•ã€‚æ–‡ç« å‘ç°è®°å¿†å•å…ƒçš„ç²’åº¦å¯¹å¯¹è¯æ•ˆæœæœ‰å½±å“ï¼Œç°æœ‰æ–¹æ³•å­˜åœ¨è®°å¿†æ£€ç´¢å‡†ç¡®æ€§å’Œè¯­ä¹‰è´¨é‡ä¸Šçš„å±€é™ã€‚æ–‡ç« è¿˜æå‡ºä½¿ç”¨æç¤ºå‹ç¼©æ–¹æ³•ï¼ˆå¦‚LLMLingua-2ï¼‰èƒ½æœ‰æ•ˆæé«˜ä¸åŒç²’åº¦ä¸‹çš„è®°å¿†æ£€ç´¢å‡†ç¡®æ€§ã€‚åŸºäºæ­¤ï¼Œæ–‡ç« æå‡ºäº†ä¸€ç§æ–°çš„æ–¹æ³•SeComï¼Œé€šè¿‡å¼•å…¥å¯¹è¯åˆ†æ®µæ¨¡å‹æ„å»ºä»¥ä¸»é¢˜æ®µè½ä¸ºå•ä½çš„è®°å¿†åº“ï¼Œå¹¶åˆ©ç”¨å‹ç¼©è®°å¿†å•å…ƒè¿›è¡Œè®°å¿†æ£€ç´¢ã€‚å®éªŒè¯æ˜ï¼ŒSeComåœ¨LOCOMOå’ŒLong-MT-Bench+ç­‰é•¿æœŸå¯¹è¯åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜äºå…¶ä»–æ–¹æ³•ï¼ŒåŒæ—¶å¯¹è¯åˆ†æ®µæ–¹æ³•åœ¨å¯¹è¯åˆ†æ®µæ•°æ®é›†ä¸Šè¡¨ç°ä¼˜å¼‚ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è®°å¿†å•å…ƒçš„ç²’åº¦å¯¹é•¿æœŸå¯¹è¯çš„è¿è´¯æ€§å’Œä¸ªæ€§åŒ–ä½“éªŒæœ‰å½±å“ã€‚</li>
<li>ç°æœ‰æ–¹æ³•åœ¨è®°å¿†æ£€ç´¢å‡†ç¡®æ€§å’Œè¯­ä¹‰è´¨é‡ä¸Šå­˜åœ¨å±€é™ã€‚</li>
<li>æç¤ºå‹ç¼©æ–¹æ³•ï¼ˆå¦‚LLMLingua-2ï¼‰èƒ½æé«˜ä¸åŒç²’åº¦ä¸‹çš„è®°å¿†æ£€ç´¢å‡†ç¡®æ€§ã€‚</li>
<li>SeComæ–¹æ³•é€šè¿‡å¼•å…¥å¯¹è¯åˆ†æ®µæ¨¡å‹æ„å»ºè®°å¿†åº“ï¼Œèƒ½æé«˜é•¿æœŸå¯¹è¯çš„è¿è´¯æ€§å’Œæ•ˆæœã€‚</li>
<li>SeComæ–¹æ³•åœ¨LOCOMOå’ŒLong-MT-Bench+ç­‰é•¿æœŸå¯¹è¯åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚ã€‚</li>
<li>å¯¹è¯åˆ†æ®µæ–¹æ³•åœ¨å¯¹è¯åˆ†æ®µæ•°æ®é›†ä¸Šè¡¨ç°è‰¯å¥½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.05589">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-8226965108f3e57e68bf0ace1cba2fc3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-fb51fa5a195455219ce3d562e31b5a38.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-9e6575ed4342f3e9af5b33c042054cb9.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e795f8506b22e5560d8a17a2ee8c5903.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-20ba8a96820b6bf292dccc7ea10ef384.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="MedAgentBench-A-Realistic-Virtual-EHR-Environment-to-Benchmark-Medical-LLM-Agents"><a href="#MedAgentBench-A-Realistic-Virtual-EHR-Environment-to-Benchmark-Medical-LLM-Agents" class="headerlink" title="MedAgentBench: A Realistic Virtual EHR Environment to Benchmark Medical   LLM Agents"></a>MedAgentBench: A Realistic Virtual EHR Environment to Benchmark Medical   LLM Agents</h2><p><strong>Authors:Yixing Jiang, Kameron C. Black, Gloria Geng, Danny Park, James Zou, Andrew Y. Ng, Jonathan H. Chen</strong></p>
<p>Recent large language models (LLMs) have demonstrated significant advancements, particularly in their ability to serve as agents thereby surpassing their traditional role as chatbots. These agents can leverage their planning and tool utilization capabilities to address tasks specified at a high level. However, a standardized dataset to benchmark the agent capabilities of LLMs in medical applications is currently lacking, making the evaluation of LLMs on complex tasks in interactive healthcare environments challenging. To address this gap, we introduce MedAgentBench, a broad evaluation suite designed to assess the agent capabilities of large language models within medical records contexts. MedAgentBench encompasses 300 patient-specific clinically-derived tasks from 10 categories written by human physicians, realistic profiles of 100 patients with over 700,000 data elements, a FHIR-compliant interactive environment, and an accompanying codebase. The environment uses the standard APIs and communication infrastructure used in modern EMR systems, so it can be easily migrated into live EMR systems. MedAgentBench presents an unsaturated agent-oriented benchmark that current state-of-the-art LLMs exhibit some ability to succeed at. The best model (Claude 3.5 Sonnet v2) achieves a success rate of 69.67%. However, there is still substantial space for improvement which gives the community a next direction to optimize. Furthermore, there is significant variation in performance across task categories. MedAgentBench establishes this and is publicly available at <a target="_blank" rel="noopener" href="https://github.com/stanfordmlgroup/MedAgentBench">https://github.com/stanfordmlgroup/MedAgentBench</a> , offering a valuable framework for model developers to track progress and drive continuous improvements in the agent capabilities of large language models within the medical domain. </p>
<blockquote>
<p>æœ€è¿‘çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å·²ç»å–å¾—äº†é‡å¤§è¿›å±•ï¼Œå°¤å…¶æ˜¯åœ¨å®ƒä»¬èƒ½å¤Ÿä½œä¸ºä»£ç†æä¾›æœåŠ¡æ–¹é¢ï¼Œä»è€Œè¶…è¶Šäº†å®ƒä»¬ä½œä¸ºèŠå¤©æœºå™¨äººçš„ä¼ ç»Ÿè§’è‰²ã€‚è¿™äº›ä»£ç†å¯ä»¥åˆ©ç”¨å…¶è§„åˆ’å’Œå·¥å…·ä½¿ç”¨èƒ½åŠ›æ¥è§£å†³é«˜çº§æŒ‡å®šçš„ä»»åŠ¡ã€‚ç„¶è€Œï¼Œç›®å‰ç¼ºä¹ä¸€ä¸ªæ ‡å‡†åŒ–çš„æ•°æ®é›†æ¥è¯„ä¼°LLMåœ¨åŒ»ç–—åº”ç”¨ä¸­çš„ä»£ç†èƒ½åŠ›ï¼Œè¿™ä½¿å¾—åœ¨äº¤äº’å¼åŒ»ç–—ç¯å¢ƒä¸­å¯¹LLMè¿›è¡Œå¤æ‚ä»»åŠ¡çš„è¯„ä¼°å…·æœ‰æŒ‘æˆ˜æ€§ã€‚ä¸ºäº†å¡«è¡¥è¿™ä¸€ç©ºç™½ï¼Œæˆ‘ä»¬å¼•å…¥äº†MedAgentBenchï¼Œè¿™æ˜¯ä¸€å¥—å¹¿æ³›çš„è¯„ä¼°å·¥å…·ï¼Œæ—¨åœ¨è¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹åœ¨åŒ»ç–—è®°å½•ä¸Šä¸‹æ–‡ä¸­çš„ä»£ç†èƒ½åŠ›ã€‚MedAgentBenchæ¶µç›–äº†ç”±äººç±»åŒ»ç”Ÿç¼–å†™çš„10ç±»300ä¸ªé’ˆå¯¹æ‚£è€…çš„ä¸´åºŠä»»åŠ¡ã€100ä¸ªæ‚£è€…çš„ç°å®çŠ¶å†µï¼ŒåŒ…å«è¶…è¿‡70ä¸‡ä¸ªæ•°æ®å…ƒç´ ã€ç¬¦åˆFHIRæ ‡å‡†çš„äº¤äº’å¼ç¯å¢ƒä»¥åŠç›¸åº”çš„ä»£ç åº“ã€‚è¯¥ç¯å¢ƒä½¿ç”¨ç°ä»£EMRç³»ç»Ÿå¸¸ç”¨çš„æ ‡å‡†APIå’Œé€šä¿¡åŸºç¡€è®¾æ–½ï¼Œå› æ­¤å¯ä»¥è½»æ¾è¿ç§»åˆ°å®æ—¶EMRç³»ç»Ÿä¸­ã€‚MedAgentBenchæä¾›äº†ä¸€ä¸ªä¸é¥±å’Œçš„é¢å‘ä»£ç†çš„åŸºå‡†æµ‹è¯•ï¼Œå½“å‰æœ€å…ˆè¿›çš„å¤§å‹è¯­è¨€æ¨¡å‹å·²ç»å±•ç°å‡ºäº†ä¸€äº›æˆåŠŸçš„è¿¹è±¡ã€‚æœ€å¥½çš„æ¨¡å‹ï¼ˆClaude 3.5 Sonnet v2ï¼‰æˆåŠŸç‡ä¸º69.67%ã€‚ç„¶è€Œï¼Œä»æœ‰å¾ˆå¤§çš„æ”¹è¿›ç©ºé—´ï¼Œä¸ºç¤¾åŒºæä¾›äº†ä¸€ä¸ªä¼˜åŒ–çš„æ–¹å‘ã€‚æ­¤å¤–ï¼Œä¸åŒä»»åŠ¡ç±»åˆ«çš„æ€§èƒ½å­˜åœ¨æ˜¾è‘—å·®å¼‚ã€‚MedAgentBenchå»ºç«‹äº†è¿™ä¸€ç‚¹ï¼Œå¹¶åœ¨<a target="_blank" rel="noopener" href="https://github.com/stanfordmlgroup/MedAgentBench%E4%B8%8A%E5%85%AC%E5%BC%80%E5%8F%AF%E7%94%A8%EF%BC%8C%E4%B8%BA%E6%A8%A1%E5%9E%8B%E5%BC%80%E5%8F%91%E4%BA%BA%E5%91%98%E6%8F%90%E4%BE%9B%E4%BA%86%E4%B8%80%E4%B8%AA%E5%AE%9D%E8%B4%B5%E7%9A%84%E6%A1%86%E6%9E%B6%EF%BC%8C%E4%BB%A5%E8%B7%9F%E8%B8%AA%E8%BF%9B%E5%BA%A6%E5%B9%B6%E6%8E%A8%E5%8A%A8%E5%8C%BB%E7%96%97%E9%A2%86%E5%9F%9F%E5%86%85%E5%A4%A7%E5%9E%8B%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E4%BB%A3%E7%90%86%E8%83%BD%E5%8A%9B%E7%9A%84%E6%8C%81%E7%BB%AD%E6%94%B9%E8%BF%9B%E3%80%82">https://github.com/stanfordmlgroup/MedAgentBenchä¸Šå…¬å¼€å¯ç”¨ï¼Œä¸ºæ¨¡å‹å¼€å‘äººå‘˜æä¾›äº†ä¸€ä¸ªå®è´µçš„æ¡†æ¶ï¼Œä»¥è·Ÿè¸ªè¿›åº¦å¹¶æ¨åŠ¨åŒ»ç–—é¢†åŸŸå†…å¤§å‹è¯­è¨€æ¨¡å‹ä»£ç†èƒ½åŠ›çš„æŒç»­æ”¹è¿›ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.14654v2">PDF</a> </p>
<p><strong>æ‘˜è¦</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æœ€æ–°è¿›å±•ï¼Œç‰¹åˆ«æ˜¯åœ¨ä½œä¸ºä»£ç†äººçš„èƒ½åŠ›æ–¹é¢ï¼Œå·²ç»è¶…è¶Šäº†å…¶ä½œä¸ºèŠå¤©æœºå™¨äººçš„ä¼ ç»Ÿè§’è‰²ã€‚ç„¶è€Œï¼Œåœ¨åŒ»ç–—åº”ç”¨ä¸­è¯„ä¼°LLMä½œä¸ºä»£ç†äººçš„èƒ½åŠ›æ—¶ï¼Œç¼ºä¹æ ‡å‡†åŒ–çš„æ•°æ®é›†ä½¿å¾—åœ¨äº’åŠ¨åŒ»ç–—ç¯å¢ƒä¸­è¯„ä¼°LLMå¤„ç†å¤æ‚ä»»åŠ¡å…·æœ‰æŒ‘æˆ˜æ€§ã€‚ä¸ºè§£å†³è¿™ä¸€ç©ºç™½ï¼Œæˆ‘ä»¬æ¨å‡ºMedAgentBenchï¼Œè¿™æ˜¯ä¸€ä¸ªå¹¿æ³›çš„è¯„ä¼°å¥—ä»¶ï¼Œæ—¨åœ¨è¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹åœ¨åŒ»ç–—è®°å½•ä¸Šä¸‹æ–‡ä¸­çš„ä»£ç†èƒ½åŠ›ã€‚MedAgentBenchåŒ…å«ç”±äººç±»åŒ»ç”Ÿç¼–å†™çš„10ç±»ä¸­çš„300ä¸ªç‰¹å®šæ‚£è€…ä¸´åºŠä»»åŠ¡ã€100ä¸ªæ‚£è€…çš„ç°å®æ¡£æ¡ˆï¼ŒåŒ…å«è¶…è¿‡700,000ä¸ªæ•°æ®å…ƒç´ ã€ç¬¦åˆFHIRæ ‡å‡†äº¤äº’ç¯å¢ƒä»¥åŠç›¸åº”çš„ä»£ç åº“ã€‚è¯¥ç¯å¢ƒä½¿ç”¨ç°ä»£EMRç³»ç»Ÿä½¿ç”¨çš„æ ‡å‡†APIå’Œé€šä¿¡åŸºç¡€è®¾æ–½ï¼Œå› æ­¤å¯ä»¥è½»æ¾è¿ç§»åˆ°å®æ—¶EMRç³»ç»Ÿä¸­ã€‚å½“å‰æœ€å…ˆè¿›çš„LLMæ¨¡å‹ï¼ˆClaude 3.5 Sonnet v2ï¼‰åœ¨MedAgentBenchä¸Šçš„æˆåŠŸç‡ä¸º69.67%ï¼Œä½†ä»å­˜åœ¨å¾ˆå¤§çš„æ”¹è¿›ç©ºé—´ã€‚æ­¤å¤–ï¼Œä¸åŒä»»åŠ¡ç±»åˆ«çš„æ€§èƒ½å­˜åœ¨æ˜¾è‘—å·®å¼‚ã€‚MedAgentBenchçš„å»ºç«‹ä¸ºæ¨¡å‹å¼€å‘è€…æä¾›äº†ä¸€ä¸ªå®è´µçš„æ¡†æ¶ï¼Œå¯ä»¥è·Ÿè¸ªè¿›åº¦å¹¶æ¨åŠ¨å¤§å‹è¯­è¨€æ¨¡å‹åœ¨åŒ»ç–—é¢†åŸŸå†…ä»£ç†äººèƒ½åŠ›çš„æŒç»­æ”¹è¿›ã€‚è¯¥æ•°æ®é›†å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/stanfordmlgroup/MedAgentBench%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/stanfordmlgroup/MedAgentBenchè·å–ã€‚</a></p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨åŒ»ç–—é¢†åŸŸçš„ä»£ç†èƒ½åŠ›è¯„ä»·å­˜åœ¨æ ‡å‡†åŒ–æ•°æ®é›†çš„ç¼ºå¤±ã€‚</li>
<li>MedAgentBenchå¡«è¡¥äº†è¿™ä¸€ç©ºç™½ï¼Œæä¾›äº†ä¸€ä¸ªå¹¿æ³›çš„è¯„ä¼°å¥—ä»¶æ¥è¯„ä¼°LLMåœ¨åŒ»ç–—è®°å½•ä¸Šä¸‹æ–‡ä¸­çš„ä»£ç†èƒ½åŠ›ã€‚</li>
<li>MedAgentBenchåŒ…å«ç”±äººç±»åŒ»ç”Ÿç¼–å†™çš„æ‚£è€…ç‰¹å®šä¸´åºŠä»»åŠ¡ã€ç°å®æ‚£è€…æ¡£æ¡ˆã€ç¬¦åˆFHIRæ ‡å‡†çš„äº¤äº’ç¯å¢ƒä»¥åŠç›¸åº”çš„ä»£ç åº“ã€‚</li>
<li>å½“å‰LLMæ¨¡å‹åœ¨MedAgentBenchä¸Šçš„æˆåŠŸç‡æœ‰å¾…æé«˜ï¼Œæœ€ä½³æ¨¡å‹çš„æˆåŠŸç‡ä¸º69.67%ã€‚</li>
<li>ä¸åŒä»»åŠ¡ç±»åˆ«çš„æ€§èƒ½å·®å¼‚æ˜¾è‘—ï¼Œè¿™æä¾›äº†æ”¹è¿›æ¨¡å‹çš„æœºä¼šå’Œæ–¹å‘ã€‚</li>
<li>MedAgentBenchä¸ºæ¨¡å‹å¼€å‘è€…æä¾›äº†ä¸€ä¸ªè·Ÿè¸ªè¿›åº¦å’Œæ¨åŠ¨æŒç»­æ”¹è¿›çš„å®è´µæ¡†æ¶ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.14654">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-ef5c8334c8553d004f27a50c8fddaf6e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1f583414bd1c662ea811f481760ff17d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-29156185e8109e1f574c52be8b84e5c8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-bdef39ec685749430472084cbabc3802.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="Mitigating-Social-Bias-in-Large-Language-Models-A-Multi-Objective-Approach-within-a-Multi-Agent-Framework"><a href="#Mitigating-Social-Bias-in-Large-Language-Models-A-Multi-Objective-Approach-within-a-Multi-Agent-Framework" class="headerlink" title="Mitigating Social Bias in Large Language Models: A Multi-Objective   Approach within a Multi-Agent Framework"></a>Mitigating Social Bias in Large Language Models: A Multi-Objective   Approach within a Multi-Agent Framework</h2><p><strong>Authors:Zhenjie Xu, Wenqing Chen, Yi Tang, Xuanying Li, Cheng Hu, Zhixuan Chu, Kui Ren, Zibin Zheng, Zhichao Lu</strong></p>
<p>Natural language processing (NLP) has seen remarkable advancements with the development of large language models (LLMs). Despite these advancements, LLMs often produce socially biased outputs. Recent studies have mainly addressed this problem by prompting LLMs to behave ethically, but this approach results in unacceptable performance degradation. In this paper, we propose a multi-objective approach within a multi-agent framework (MOMA) to mitigate social bias in LLMs without significantly compromising their performance. The key idea of MOMA involves deploying multiple agents to perform causal interventions on bias-related contents of the input questions, breaking the shortcut connection between these contents and the corresponding answers. Unlike traditional debiasing techniques leading to performance degradation, MOMA substantially reduces bias while maintaining accuracy in downstream tasks. Our experiments conducted on two datasets and two models demonstrate that MOMA reduces bias scores by up to 87.7%, with only a marginal performance degradation of up to 6.8% in the BBQ dataset. Additionally, it significantly enhances the multi-objective metric icat in the StereoSet dataset by up to 58.1%. Code will be made available at <a target="_blank" rel="noopener" href="https://github.com/Cortantse/MOMA">https://github.com/Cortantse/MOMA</a>. </p>
<blockquote>
<p>è‡ªç„¶è¯­è¨€å¤„ç†ï¼ˆNLPï¼‰éšç€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„å‘å±•è€Œå–å¾—äº†æ˜¾è‘—çš„è¿›æ­¥ã€‚å°½ç®¡æœ‰è¿™äº›è¿›å±•ï¼ŒLLMå¸¸å¸¸äº§ç”Ÿç¤¾ä¼šåè§è¾“å‡ºã€‚æœ€è¿‘çš„ç ”ç©¶ä¸»è¦é€šè¿‡æç¤ºLLMä»¥ç¬¦åˆé“å¾·çš„è¡Œä¸ºæ¥è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œä½†è¿™ç§æ–¹æ³•ä¼šå¯¼è‡´æ€§èƒ½ä¸å¯æ¥å—çš„ä¸‹é™ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªå¤šç›®æ ‡å¤šæ™ºèƒ½ä½“æ¡†æ¶ï¼ˆMOMAï¼‰çš„æ–¹æ³•ï¼Œä»¥å‡è½»LLMä¸­çš„ç¤¾ä¼šåè§ï¼Œè€Œä¸ä¼šæ˜¾è‘—æŸå®³å…¶æ€§èƒ½ã€‚MOMAçš„å…³é”®æ€æƒ³æ˜¯åˆ©ç”¨å¤šä¸ªæ™ºèƒ½ä½“å¯¹è¾“å…¥é—®é¢˜ä¸­çš„åè§ç›¸å…³å†…å®¹è¿›è¡Œå› æœå¹²é¢„ï¼Œåˆ‡æ–­è¿™äº›å†…å®¹ä¸ç›¸åº”ç­”æ¡ˆä¹‹é—´çš„å¿«æ·æ–¹å¼è¿æ¥ã€‚ä¸ä¼ ç»Ÿçš„å¯¼è‡´æ€§èƒ½ä¸‹é™çš„æ¶ˆåæŠ€æœ¯ä¸åŒï¼ŒMOMAåœ¨ä¿æŒä¸‹æ¸¸ä»»åŠ¡å‡†ç¡®æ€§çš„åŒæ—¶ï¼Œå®è´¨æ€§åœ°å‡å°‘äº†åè§ã€‚æˆ‘ä»¬åœ¨ä¸¤ä¸ªæ•°æ®é›†å’Œä¸¤ä¸ªæ¨¡å‹ä¸Šè¿›è¡Œçš„å®éªŒè¡¨æ˜ï¼ŒMOMAå°†åè§åˆ†æ•°å‡å°‘äº†é«˜è¾¾87.7%ï¼Œä¸”åœ¨BBQæ•°æ®é›†ä¸­æ€§èƒ½ä»…ç•¥æœ‰ä¸‹é™ï¼Œä¸‹é™äº†6.8%ã€‚æ­¤å¤–ï¼Œå®ƒåœ¨StereoSetæ•°æ®é›†ä¸­æ˜¾è‘—æé«˜äº†å¤šç›®æ ‡æŒ‡æ ‡icatï¼Œæé«˜äº†é«˜è¾¾58.1%ã€‚ä»£ç å°†åœ¨<a target="_blank" rel="noopener" href="https://github.com/Cortantse/MOMA%E4%B8%8A%E6%8F%90%E4%BE%9B%E3%80%82">https://github.com/Cortantse/MOMAä¸Šæä¾›ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.15504v2">PDF</a> This work has been accepted at The 39th Annual AAAI Conference on   Artificial Intelligence (AAAI-2025)</p>
<p><strong>Summary</strong></p>
<p>éšç€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„å‘å±•ï¼Œè‡ªç„¶è¯­è¨€å¤„ç†ï¼ˆNLPï¼‰å–å¾—äº†æ˜¾è‘—è¿›å±•ã€‚ç„¶è€Œï¼ŒLLMå¸¸äº§ç”Ÿç¤¾ä¼šåè§è¾“å‡ºã€‚ç°æœ‰ç ”ç©¶ä¸»è¦é€šè¿‡æç¤ºLLMä»¥ç¬¦åˆä¼¦ç†çš„æ–¹å¼æ¥è§£å†³è¿™ä¸€é—®é¢˜ï¼Œä½†è¿™ä¼šå¯¼è‡´æ€§èƒ½ä¸‹é™ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§å¤šç›®æ ‡å¤šæ™ºèƒ½ä½“æ¡†æ¶ï¼ˆMOMAï¼‰ï¼Œæ—¨åœ¨å‡è½»LLMä¸­çš„ç¤¾ä¼šåè§ï¼ŒåŒæ—¶ä¸ä¼šæ˜¾è‘—æŸå®³æ€§èƒ½ã€‚MOMAçš„å…³é”®æ€æƒ³æ˜¯åˆ©ç”¨å¤šä¸ªæ™ºèƒ½ä½“å¯¹è¾“å…¥é—®é¢˜ä¸­çš„åè§ç›¸å…³å†…å®¹è¿›è¡Œå› æœå¹²é¢„ï¼Œåˆ‡æ–­è¿™äº›å†…å®¹ä¸ç›¸åº”ç­”æ¡ˆä¹‹é—´çš„ç›´æ¥è”ç³»ã€‚å®éªŒè¡¨æ˜ï¼ŒMOMAåœ¨å‡å°‘åè§çš„åŒæ—¶ï¼Œä¿æŒäº†ä¸‹æ¸¸ä»»åŠ¡çš„å‡†ç¡®æ€§ã€‚åœ¨ä¸¤ä¸ªæ•°æ®é›†å’Œä¸¤ä¸ªæ¨¡å‹ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒMOMAå°†åè§å¾—åˆ†é™ä½äº†é«˜è¾¾87.7%ï¼Œåœ¨BBQæ•°æ®é›†ä¸Šçš„æ€§èƒ½ä»…ç•¥æœ‰ä¸‹é™ï¼ˆæœ€å¤š6.8%ï¼‰ã€‚æ­¤å¤–ï¼Œå®ƒåœ¨StereoSetæ•°æ®é›†ä¸Šçš„å¤šç›®æ ‡æŒ‡æ ‡icatæé«˜äº†é«˜è¾¾58.1%ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰è™½ç„¶å–å¾—æ˜¾è‘—è¿›å±•ï¼Œä½†ä¼šäº§ç”Ÿç¤¾ä¼šåè§è¾“å‡ºã€‚</li>
<li>ç°æœ‰è§£å†³åè§é—®é¢˜çš„æ–¹æ³•ä¸»è¦é€šè¿‡æç¤ºLLMéµå¾ªä¼¦ç†ï¼Œä½†è¿™ä¼šé™ä½æ€§èƒ½ã€‚</li>
<li>æœ¬æ–‡æå‡ºçš„å¤šç›®æ ‡å¤šæ™ºèƒ½ä½“æ¡†æ¶ï¼ˆMOMAï¼‰æ—¨åœ¨å‡è½»LLMä¸­çš„ç¤¾ä¼šåè§ï¼ŒåŒæ—¶ä¸æŸå®³æ€§èƒ½ã€‚</li>
<li>MOMAé€šè¿‡å¤šä¸ªæ™ºèƒ½ä½“å¯¹è¾“å…¥é—®é¢˜ä¸­çš„åè§å†…å®¹è¿›è¡Œå› æœå¹²é¢„ï¼Œåˆ‡æ–­å…¶ä¸ç­”æ¡ˆçš„ç›´æ¥è”ç³»ã€‚</li>
<li>MOMAåœ¨å‡å°‘åè§çš„åŒæ—¶ï¼Œä¿æŒäº†ä¸‹æ¸¸ä»»åŠ¡çš„å‡†ç¡®æ€§ã€‚</li>
<li>å®éªŒè¡¨æ˜ï¼ŒMOMAåœ¨å¤šä¸ªæ•°æ®é›†å’Œæ¨¡å‹ä¸Šæœ‰æ•ˆï¼Œå°†åè§å¾—åˆ†é™ä½äº†é«˜è¾¾87.7%ï¼Œæ€§èƒ½ä¸‹é™æœ‰é™ã€‚</li>
<li>MOMAåœ¨StereoSetæ•°æ®é›†ä¸Šçš„å¤šç›®æ ‡æŒ‡æ ‡icatæœ‰æ˜¾è‘—æé«˜ï¼Œè¯æ˜äº†å…¶æœ‰æ•ˆæ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.15504">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-b8e73225f173fbb7d1c4a1650bafdaef.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3767a3ef61bb890b1828a8c390c469d0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6da4fd6183440d6811fe652b4685ab74.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-23ceac03383562c3553d75bf64aa71a3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-35d1a3238135c66172e4b9247f221c09.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-0fef089a6cc1c402bf10d9e558f75bc0.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-42714beabacc1a532ad95a60159f7849.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-03e918468288426512dc74cf02050b8c.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="Multi-Step-Time-Series-Inference-Agent-for-Reasoning-and-Automated-Task-Execution"><a href="#Multi-Step-Time-Series-Inference-Agent-for-Reasoning-and-Automated-Task-Execution" class="headerlink" title="Multi-Step Time Series Inference Agent for Reasoning and Automated Task   Execution"></a>Multi-Step Time Series Inference Agent for Reasoning and Automated Task   Execution</h2><p><strong>Authors:Wen Ye, Yizhou Zhang, Wei Yang, Defu Cao, Lumingyuan Tang, Jie Cai, Yan Liu</strong></p>
<p>Time series analysis is crucial in real-world applications, yet traditional methods focus on isolated tasks only, and recent studies on time series reasoning remain limited to simple, single-step inference constrained to natural language answer. In this work, we propose a practical novel task: multi-step time series inference that demands both compositional reasoning and computation precision of time series analysis. To address such challenge, we propose a simple but effective program-aided inference agent that leverages LLMsâ€™ reasoning ability to decompose complex tasks into structured execution pipelines. By integrating in-context learning, self-correction, and program-aided execution, our proposed approach ensures accurate and interpretable results. To benchmark performance, we introduce a new dataset and a unified evaluation framework with task-specific success criteria. Experiments show that our approach outperforms standalone general purpose LLMs in both basic time series concept understanding as well as multi-step time series inference task, highlighting the importance of hybrid approaches that combine reasoning with computational precision. </p>
<blockquote>
<p>æ—¶é—´åºåˆ—åˆ†æåœ¨ç°å®ä¸–ç•Œåº”ç”¨ä¸­è‡³å…³é‡è¦ï¼Œä½†ä¼ ç»Ÿæ–¹æ³•åªå…³æ³¨å­¤ç«‹ä»»åŠ¡ï¼Œæœ€è¿‘å…³äºæ—¶é—´åºåˆ—æ¨ç†çš„ç ”ç©¶ä»å±€é™äºå—è‡ªç„¶è¯­è¨€ç­”æ¡ˆçº¦æŸçš„å•æ­¥æ¨ç†ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªå®ç”¨çš„æ–°ä»»åŠ¡ï¼šå¤šæ­¥æ—¶é—´åºåˆ—æ¨ç†ï¼Œè¿™éœ€è¦æ—¶é—´åºåˆ—åˆ†æçš„ç»„åˆæ¨ç†å’Œè®¡ç®—ç²¾åº¦ã€‚ä¸ºäº†åº”å¯¹è¿™ä¸€æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ç®€å•æœ‰æ•ˆçš„ç¨‹åºè¾…åŠ©æ¨ç†ä»£ç†ï¼Œåˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ¨ç†èƒ½åŠ›å°†å¤æ‚ä»»åŠ¡åˆ†è§£ä¸ºç»“æ„åŒ–æ‰§è¡Œç®¡é“ã€‚é€šè¿‡æ•´åˆä¸Šä¸‹æ–‡å­¦ä¹ ã€è‡ªæˆ‘ä¿®æ­£å’Œç¨‹åºè¾…åŠ©æ‰§è¡Œï¼Œæˆ‘ä»¬æå‡ºçš„æ–¹æ³•ç¡®ä¿äº†å‡†ç¡®ä¸”å¯è§£é‡Šçš„ç»“æœã€‚ä¸ºäº†è¯„ä¼°æ€§èƒ½ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ªæ–°çš„æ•°æ®é›†å’Œç»Ÿä¸€çš„è¯„ä¼°æ¡†æ¶ï¼Œå¹¶åˆ¶å®šäº†ç‰¹å®šä»»åŠ¡çš„æˆåŠŸæ ‡å‡†ã€‚å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨åŸºæœ¬æ—¶é—´åºåˆ—æ¦‚å¿µç†è§£å’Œå¤šæ­¥æ—¶é—´åºåˆ—æ¨ç†ä»»åŠ¡ä¸Šçš„è¡¨ç°ä¼˜äºç‹¬ç«‹çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼Œè¿™çªå‡ºäº†ç»“åˆæ¨ç†å’Œè®¡ç®—ç²¾åº¦çš„æ··åˆæ–¹æ³•çš„é‡è¦æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2410.04047v3">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬å·¥ä½œæå‡ºäº†ä¸€é¡¹å®ç”¨çš„æ–°å‹ä»»åŠ¡ï¼šå¤šæ­¥éª¤æ—¶é—´åºåˆ—æ¨ç†ï¼Œå®ƒè¦æ±‚åŒæ—¶å…·å¤‡æ—¶é—´åºåˆ—åˆ†æçš„ç»„åˆæ¨ç†å’Œè®¡ç®—ç²¾åº¦ã€‚ä¸ºåº”å¯¹è¿™ä¸€æŒ‘æˆ˜ï¼Œç ”ç©¶å›¢é˜Ÿæå‡ºäº†ä¸€ç§ç®€å•æœ‰æ•ˆçš„ç¨‹åºè¾…åŠ©æ¨ç†ä»£ç†ï¼Œè¯¥ä»£ç†åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹çš„æ¨ç†èƒ½åŠ›å°†å¤æ‚ä»»åŠ¡åˆ†è§£ä¸ºç»“æ„åŒ–æ‰§è¡Œç®¡é“ã€‚é€šè¿‡ä¸Šä¸‹æ–‡å­¦ä¹ ã€è‡ªæˆ‘ä¿®æ­£å’Œç¨‹åºè¾…åŠ©æ‰§è¡Œï¼Œè¯¥æ–¹æ³•ç¡®ä¿äº†å‡†ç¡®ä¸”å¯è§£é‡Šçš„ç»“æœã€‚ç ”ç©¶å›¢é˜Ÿè¿˜æ¨å‡ºæ–°çš„æ•°æ®é›†å’Œç»Ÿä¸€è¯„ä¼°æ¡†æ¶ï¼Œå¹¶è®¾å®šç‰¹å®šä»»åŠ¡çš„æˆåŠŸæ ‡å‡†ã€‚å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨åŸºæœ¬æ—¶é—´åºåˆ—æ¦‚å¿µç†è§£å’Œå¤šæ­¥éª¤æ—¶é—´åºåˆ—æ¨ç†ä»»åŠ¡ä¸Šçš„è¡¨ç°ä¼˜äºå•ä¸€é€šç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼Œçªæ˜¾å‡ºç»“åˆæ¨ç†ä¸è®¡ç®—ç²¾åº¦çš„æ··åˆæ–¹æ³•çš„é‡è¦æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æå‡ºäº†ä¸€ç§æ–°å‹ä»»åŠ¡ï¼šå¤šæ­¥éª¤æ—¶é—´åºåˆ—æ¨ç†ï¼Œè¦æ±‚å…¼å…·ç»„åˆæ¨ç†å’Œè®¡ç®—ç²¾åº¦ã€‚</li>
<li>ä»‹ç»äº†ä¸€ç§ç¨‹åºè¾…åŠ©æ¨ç†ä»£ç†ï¼Œåˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹çš„æ¨ç†èƒ½åŠ›åˆ†è§£å¤æ‚ä»»åŠ¡ã€‚</li>
<li>ç»“åˆä¸Šä¸‹æ–‡å­¦ä¹ ã€è‡ªæˆ‘ä¿®æ­£å’Œç¨‹åºè¾…åŠ©æ‰§è¡Œï¼Œç¡®ä¿å‡†ç¡®ä¸”å¯è§£é‡Šçš„ç»“æœã€‚</li>
<li>æ¨å‡ºæ–°çš„æ•°æ®é›†å’Œç»Ÿä¸€è¯„ä¼°æ¡†æ¶ï¼Œè®¾å®šç‰¹å®šä»»åŠ¡çš„æˆåŠŸæ ‡å‡†ã€‚</li>
<li>å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨åŸºæœ¬æ—¶é—´åºåˆ—æ¦‚å¿µç†è§£å’Œå¤šæ­¥éª¤æ¨ç†ä¸Šè¡¨ç°ä¼˜è¶Šã€‚</li>
<li>ç›¸æ¯”å•ä¸€é€šç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼Œè¯¥æ–¹æ³•çš„æ€§èƒ½æ›´ä½³ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2410.04047">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-2de82c15c3e66e6b6b5402964838cc81.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-56e769c7c73c6b3823cac1411ab67549.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-5d04ec1202f32a47fb338705cc9084fa.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d37df067a2a863da9708b13f42c89dec.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-cb03c25e51bb3853ee8493fb6c231264.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="EIA-Environmental-Injection-Attack-on-Generalist-Web-Agents-for-Privacy-Leakage"><a href="#EIA-Environmental-Injection-Attack-on-Generalist-Web-Agents-for-Privacy-Leakage" class="headerlink" title="EIA: Environmental Injection Attack on Generalist Web Agents for Privacy   Leakage"></a>EIA: Environmental Injection Attack on Generalist Web Agents for Privacy   Leakage</h2><p><strong>Authors:Zeyi Liao, Lingbo Mo, Chejian Xu, Mintong Kang, Jiawei Zhang, Chaowei Xiao, Yuan Tian, Bo Li, Huan Sun</strong></p>
<p>Generalist web agents have demonstrated remarkable potential in autonomously completing a wide range of tasks on real websites, significantly boosting human productivity. However, web tasks, such as booking flights, usually involve usersâ€™ PII, which may be exposed to potential privacy risks if web agents accidentally interact with compromised websites, a scenario that remains largely unexplored in the literature. In this work, we narrow this gap by conducting the first study on the privacy risks of generalist web agents in adversarial environments. First, we present a realistic threat model for attacks on the website, where we consider two adversarial targets: stealing usersâ€™ specific PII or the entire user request. Then, we propose a novel attack method, termed Environmental Injection Attack (EIA). EIA injects malicious content designed to adapt well to environments where the agents operate and our work instantiates EIA specifically for privacy scenarios in web environments. We collect 177 action steps that involve diverse PII categories on realistic websites from the Mind2Web, and conduct experiments using one of the most capable generalist web agent frameworks to date. The results demonstrate that EIA achieves up to 70% ASR in stealing specific PII and 16% ASR for full user request. Additionally, by accessing the stealthiness and experimenting with a defensive system prompt, we indicate that EIA is hard to detect and mitigate. Notably, attacks that are not well adapted for a webpage can be detected via human inspection, leading to our discussion about the trade-off between security and autonomy. However, extra attackersâ€™ efforts can make EIA seamlessly adapted, rendering such supervision ineffective. Thus, we further discuss the defenses at the pre- and post-deployment stages of the websites without relying on human supervision and call for more advanced defense strategies. </p>
<blockquote>
<p>é€šç”¨Webä»£ç†åœ¨çœŸå®ç½‘ç«™ä¸Šè‡ªä¸»å®Œæˆå„ç§ä»»åŠ¡æ–¹é¢è¡¨ç°å‡ºæ˜¾è‘—æ½œåŠ›ï¼Œæå¤§åœ°æé«˜äº†äººç±»ç”Ÿäº§åŠ›ã€‚ç„¶è€Œï¼ŒWebä»»åŠ¡ï¼ˆå¦‚è®¢ç¥¨ï¼‰é€šå¸¸æ¶‰åŠç”¨æˆ·çš„ä¸ªäººèº«ä»½ä¿¡æ¯ï¼Œå¦‚æœWebä»£ç†æ„å¤–åœ°ä¸å—æ”»å‡»çš„ç½‘ç«™äº¤äº’ï¼Œå¯èƒ½ä¼šé¢ä¸´æ½œåœ¨çš„éšç§é£é™©ï¼Œè¿™ä¸€æƒ…æ™¯åœ¨æ–‡çŒ®ä¸­ä»æœªå¾—åˆ°å¹¿æ³›æ¢ç´¢ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬é€šè¿‡å¼€å±•å…³äºé€šç”¨Webä»£ç†åœ¨æ•Œå¯¹ç¯å¢ƒä¸­éšç§é£é™©çš„é¦–é¡¹ç ”ç©¶æ¥ç¼©å°è¿™ä¸€å·®è·ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬ä¸ºé’ˆå¯¹ç½‘ç«™çš„æ”»å‡»æå‡ºäº†ä¸€ä¸ªç°å®å¨èƒæ¨¡å‹ï¼Œå…¶ä¸­è€ƒè™‘äº†ä¸¤ä¸ªæ•Œå¯¹ç›®æ ‡ï¼šçªƒå–ç”¨æˆ·çš„ç‰¹å®šä¸ªäººèº«ä»½ä¿¡æ¯æˆ–æ•´ä¸ªç”¨æˆ·è¯·æ±‚ã€‚ç„¶åï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„æ”»å‡»æ–¹æ³•ï¼Œç§°ä¸ºç¯å¢ƒæ³¨å…¥æ”»å‡»ï¼ˆEIAï¼‰ã€‚EIAæ³¨å…¥æ¶æ„å†…å®¹ï¼Œä»¥é€‚åº”ä»£ç†è¿è¡Œçš„ç¯å¢ƒï¼Œæˆ‘ä»¬çš„å·¥ä½œé’ˆå¯¹Webç¯å¢ƒä¸­çš„éšç§åœºæ™¯å…·ä½“å®ç°äº†EIAã€‚æˆ‘ä»¬ä»Mind2Webæ”¶é›†äº†æ¶‰åŠç°å®ç½‘ç«™ä¸Šå„ç±»ä¸ªäººèº«ä»½ä¿¡æ¯çš„177ä¸ªæ“ä½œæ­¥éª¤ï¼Œå¹¶ä½¿ç”¨è¿„ä»Šä¸ºæ­¢æœ€å¼ºå¤§çš„é€šç”¨Webä»£ç†æ¡†æ¶è¿›è¡Œå®éªŒã€‚ç»“æœè¡¨æ˜ï¼ŒEIAåœ¨çªƒå–ç‰¹å®šä¸ªäººèº«ä»½ä¿¡æ¯æ—¶è¾¾åˆ°70%çš„æ”»å‡»æˆåŠŸç‡ï¼ˆASRï¼‰ï¼Œåœ¨çªƒå–å®Œæ•´ç”¨æˆ·è¯·æ±‚æ—¶è¾¾åˆ°16%çš„ASRã€‚æ­¤å¤–ï¼Œé€šè¿‡æµ‹è¯•éšè”½æ€§å’Œå®éªŒé˜²å¾¡ç³»ç»Ÿæç¤ºï¼Œæˆ‘ä»¬è¡¨æ˜EIAéš¾ä»¥æ£€æµ‹å’Œç¼“è§£ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œä¸é€‚åº”ç½‘é¡µçš„æ”»å‡»å¯ä»¥é€šè¿‡äººå·¥æ£€æŸ¥è¿›è¡Œæ£€æµ‹ï¼Œè¿™å¼•å‘äº†æˆ‘ä»¬å¯¹å®‰å…¨æ€§å’Œè‡ªä¸»æ€§ä¹‹é—´æƒè¡¡çš„è®¨è®ºã€‚ç„¶è€Œï¼Œé¢å¤–çš„æ”»å‡»è€…åŠªåŠ›å¯ä»¥ä½¿EIAæ— ç¼é€‚åº”ï¼Œä½¿è¿™ç§ç›‘ç£æ— æ•ˆã€‚å› æ­¤ï¼Œæˆ‘ä»¬è¿›ä¸€æ­¥è®¨è®ºäº†ä¸ä¾èµ–äººå·¥ç›‘ç£çš„ç½‘ç«™é¢„éƒ¨ç½²å’Œåç»­éƒ¨ç½²é˜¶æ®µçš„é˜²å¾¡æªæ–½ï¼Œå¹¶å‘¼åé‡‡ç”¨æ›´å…ˆè¿›çš„é˜²å¾¡ç­–ç•¥ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2409.11295v4">PDF</a> Accepted by ICLR 2025</p>
<p><strong>Summary</strong><br>     é€šç”¨ç½‘ç»œçˆ¬è™«åœ¨è‡ªä¸»å®ŒæˆçœŸå®ç½‘ç«™çš„å„ç±»ä»»åŠ¡æ—¶è¡¨ç°å‡ºæ˜¾è‘—æ½œåŠ›ï¼Œæå¤§åœ°æé«˜äº†äººç±»çš„ç”Ÿäº§æ•ˆç‡ã€‚ç„¶è€Œï¼Œç½‘ç«™ä»»åŠ¡å¦‚è®¢ç¥¨æ¶‰åŠç”¨æˆ·çš„ä¸ªäººä¿¡æ¯ï¼ˆPIIï¼‰ï¼Œç½‘ç»œçˆ¬è™«åœ¨æ„å¤–ä¸æ¶æ„ç½‘ç«™äº¤äº’æ—¶å¯èƒ½æ³„éœ²ç”¨æˆ·éšç§ï¼Œè¿™ä¸€åœºæ™¯åœ¨æ–‡çŒ®ä¸­å°šæœªå¾—åˆ°å……åˆ†ç ”ç©¶ã€‚æœ¬ç ”ç©¶é¦–æ¬¡å¯¹ç½‘ç»œçˆ¬è™«åœ¨æ•Œå¯¹ç¯å¢ƒä¸­çš„éšç§é£é™©è¿›è¡Œç ”ç©¶ã€‚æˆ‘ä»¬æ„å»ºäº†ä¸€ä¸ªé’ˆå¯¹ç½‘ç«™çš„ç°å®å¨èƒæ¨¡å‹ï¼Œè€ƒè™‘ä¸¤ç§æ”»å‡»ç›®æ ‡ï¼šçªƒå–ç”¨æˆ·çš„ç‰¹å®šä¸ªäººä¿¡æ¯æˆ–æ•´ä¸ªç”¨æˆ·è¯·æ±‚ã€‚æå‡ºäº†ä¸€ç§æ–°å‹æ”»å‡»æ–¹æ³•â€”â€”ç¯å¢ƒæ³¨å…¥æ”»å‡»ï¼ˆEIAï¼‰ï¼Œè¯¥æ–¹æ³•æ³¨å…¥æ¶æ„å†…å®¹ä»¥é€‚åº”çˆ¬è™«è¿è¡Œç¯å¢ƒï¼Œå¹¶é’ˆå¯¹ç½‘ç»œç¯å¢ƒä¸­çš„éšç§åœºæ™¯è¿›è¡Œå®ä¾‹åŒ–ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒEIAåœ¨çªƒå–ç‰¹å®šä¸ªäººä¿¡æ¯æ–¹é¢çš„æ”»å‡»æˆåŠŸç‡é«˜è¾¾70%ï¼Œçªƒå–å®Œæ•´ç”¨æˆ·è¯·æ±‚çš„æ”»å‡»æˆåŠŸç‡è¾¾16%ã€‚æ­¤å¤–ï¼ŒEIAéš¾ä»¥æ£€æµ‹å’Œç¼“è§£ã€‚å°½ç®¡ä¸é€‚åº”ç½‘é¡µçš„æ”»å‡»å¯é€šè¿‡äººå·¥æ£€æµ‹å‘ç°ï¼Œä½†é¢å¤–çš„æ”»å‡»è€…åŠªåŠ›å¯ä½¿EIAæ— ç¼é€‚åº”ï¼Œä½¿æ­¤ç±»ç›‘ç£å¤±æ•ˆã€‚å› æ­¤ï¼Œæˆ‘ä»¬è¿›ä¸€æ­¥è®¨è®ºäº†åœ¨ç½‘ç«™é¢„éƒ¨ç½²å’Œåéƒ¨ç½²é˜¶æ®µçš„é˜²å¾¡ç­–ç•¥ï¼Œå‘¼åé‡‡ç”¨æ›´å…ˆè¿›çš„é˜²å¾¡æ‰‹æ®µã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>é€šç”¨ç½‘ç»œçˆ¬è™«åœ¨è‡ªä¸»å®Œæˆå¤šç§ç½‘ç«™ä»»åŠ¡æ—¶å…·æœ‰æ˜¾è‘—æ½œåŠ›ï¼Œæé«˜äººç±»ç”Ÿäº§æ•ˆç‡ã€‚</li>
<li>ç½‘ç»œçˆ¬è™«åœ¨å¤„ç†æ¶‰åŠç”¨æˆ·ä¸ªäººä¿¡æ¯ï¼ˆPIIï¼‰çš„ä»»åŠ¡æ—¶ï¼Œåœ¨æ¶æ„ç½‘ç«™ç¯å¢ƒä¸­çš„éšç§æ³„éœ²é£é™©å°šæœªå¾—åˆ°å……åˆ†ç ”ç©¶ã€‚</li>
<li>æœ¬ç ”ç©¶æ„å»ºäº†ä¸€ä¸ªé’ˆå¯¹ç½‘ç«™çš„å¨èƒæ¨¡å‹ï¼Œè€ƒè™‘çªƒå–ç”¨æˆ·ä¸ªäººä¿¡æ¯æˆ–æ•´ä¸ªç”¨æˆ·è¯·æ±‚çš„æ”»å‡»ç›®æ ‡ã€‚</li>
<li>æå‡ºäº†ä¸€ç§æ–°å‹æ”»å‡»æ–¹æ³•â€”â€”ç¯å¢ƒæ³¨å…¥æ”»å‡»ï¼ˆEIAï¼‰ï¼Œè¯¥æ–¹æ³•é€‚åº”çˆ¬è™«è¿è¡Œç¯å¢ƒå¹¶æ³¨å…¥æ¶æ„å†…å®¹ã€‚</li>
<li>EIAåœ¨çªƒå–ç‰¹å®šä¸ªäººä¿¡æ¯æ–¹é¢çš„æ”»å‡»æˆåŠŸç‡è¾ƒé«˜ï¼Œä¸”éš¾ä»¥æ£€æµ‹å’Œç¼“è§£ã€‚</li>
<li>ä¸é€‚åº”ç½‘é¡µçš„æ”»å‡»å¯é€šè¿‡äººå·¥æ£€æµ‹å‘ç°ï¼Œä½†é«˜çº§æ”»å‡»è€…å¯é€‚åº”å¹¶ç»•è¿‡ç›‘ç£æªæ–½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2409.11295">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-3ab84dc5ed34aadf273d23ab7870013a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5e7f01d12d2042f3ad559e2a9a6813bb.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-d366a34e7d90734c72e931441fc957ad.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="COAST-Enhancing-the-Code-Debugging-Ability-of-LLMs-through-Communicative-Agent-Based-Data-Synthesis"><a href="#COAST-Enhancing-the-Code-Debugging-Ability-of-LLMs-through-Communicative-Agent-Based-Data-Synthesis" class="headerlink" title="COAST: Enhancing the Code Debugging Ability of LLMs through   Communicative Agent Based Data Synthesis"></a>COAST: Enhancing the Code Debugging Ability of LLMs through   Communicative Agent Based Data Synthesis</h2><p><strong>Authors:Weiqing Yang, Hanbin Wang, Zhenghao Liu, Xinze Li, Yukun Yan, Shuo Wang, Yu Gu, Minghe Yu, Zhiyuan Liu, Ge Yu</strong></p>
<p>Code debugging is a vital stage of software development, essential for ensuring the reliability and performance of Large Language Models (LLMs) in the code generation task. Human debugging typically follows a multi-stage process, which includes Bug Localization, Bug Identification, Code Repair, and Code Recognition. However, existing code debugging benchmarks predominantly focus on the Code Repair stage, which offers only a limited perspective on evaluating the debugging capabilities of LLMs. In this paper, we introduce DEBUGEVAL, a comprehensive benchmark for evaluating the debugging abilities of LLMs by emulating the multi-stage human debugging process. Through evaluating on DEBUGEVAL, we observe that 7B-scale models consistently underperform compared to their larger counterparts, highlighting their limitations in comprehending code semantics. In this case, we propose the COmmunicative Agent-based data SynThesis (COAST) framework, which employs a multi-agent system to generate high-quality training data for supervised fine-tuning (SFT). Experimental results demonstrate that COAST-generated data outperform human-curated and GPT-4-generated data, enabling 7B-scale LLMs to achieve debugging performance comparable to GPT-3.5. All data and codes are available at <a target="_blank" rel="noopener" href="https://github.com/NEUIR/COAST">https://github.com/NEUIR/COAST</a>. </p>
<blockquote>
<p>ä»£ç è°ƒè¯•æ˜¯è½¯ä»¶å¼€å‘çš„é‡è¦é˜¶æ®µï¼Œå¯¹äºç¡®ä¿å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨ä»£ç ç”Ÿæˆä»»åŠ¡ä¸­çš„å¯é æ€§å’Œæ€§èƒ½è‡³å…³é‡è¦ã€‚äººç±»è°ƒè¯•é€šå¸¸éµå¾ªåŒ…æ‹¬é”™è¯¯å®šä½ã€é”™è¯¯è¯†åˆ«ã€ä»£ç ä¿®å¤å’Œä»£ç è¯†åˆ«çš„å¤šé˜¶æ®µè¿‡ç¨‹ã€‚ç„¶è€Œï¼Œç°æœ‰çš„ä»£ç è°ƒè¯•åŸºå‡†æµ‹è¯•ä¸»è¦å…³æ³¨ä»£ç ä¿®å¤é˜¶æ®µï¼Œè¿™åªèƒ½æä¾›æœ‰é™çš„è§†è§’æ¥è¯„ä¼°LLMçš„è°ƒè¯•èƒ½åŠ›ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†DEBUGEVALï¼Œè¿™æ˜¯ä¸€ä¸ªé€šè¿‡æ¨¡æ‹Ÿå¤šé˜¶æ®µäººç±»è°ƒè¯•è¿‡ç¨‹æ¥è¯„ä¼°LLMè°ƒè¯•èƒ½åŠ›çš„å…¨é¢åŸºå‡†æµ‹è¯•ã€‚é€šè¿‡DEBUGEVALçš„è¯„ä¼°ï¼Œæˆ‘ä»¬å‘ç°ä¸è¾ƒå¤§è§„æ¨¡çš„æ¨¡å‹ç›¸æ¯”ï¼Œ7Bè§„æ¨¡æ¨¡å‹çš„æ€§èƒ½å§‹ç»ˆè¾ƒå·®ï¼Œå‡¸æ˜¾äº†å®ƒä»¬åœ¨ç†è§£ä»£ç è¯­ä¹‰æ–¹é¢çš„å±€é™æ€§ã€‚é’ˆå¯¹è¿™ç§æƒ…å†µï¼Œæˆ‘ä»¬æå‡ºäº†åŸºäºé€šä¿¡ä»£ç†çš„æ•°æ®åˆæˆï¼ˆCOASTï¼‰æ¡†æ¶ï¼Œè¯¥æ¡†æ¶é‡‡ç”¨å¤šä»£ç†ç³»ç»Ÿç”Ÿæˆé«˜è´¨é‡çš„è®­ç»ƒæ•°æ®ï¼Œç”¨äºæœ‰ç›‘ç£çš„å¾®è°ƒï¼ˆSFTï¼‰ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒCOASTç”Ÿæˆçš„æ•°æ®ä¼˜äºäººå·¥æ•´ç†å’ŒGPT-4ç”Ÿæˆçš„æ•°æ®ï¼Œä½¿7Bè§„æ¨¡çš„LLMè¾¾åˆ°ä¸GPT-3.5ç›¸å½“çš„è°ƒè¯•æ€§èƒ½ã€‚æ‰€æœ‰æ•°æ®å’Œä»£ç éƒ½å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/NEUIR/COAST%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/NEUIR/COASTæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2408.05006v3">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ä»£ç è°ƒè¯•åœ¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¸­çš„é‡è¦æ€§ï¼Œå¹¶æŒ‡å‡ºå½“å‰ä»£ç è°ƒè¯•åŸºå‡†æµ‹è¯•ä¸»è¦å…³æ³¨ä»£ç ä¿®å¤é˜¶æ®µï¼Œæ— æ³•å…¨é¢è¯„ä¼°LLMçš„è°ƒè¯•èƒ½åŠ›ã€‚ä¸ºæ­¤ï¼Œä½œè€…æå‡ºäº†ä¸€ç§æ–°çš„ç»¼åˆåŸºå‡†æµ‹è¯•DEBUGEVALï¼Œä»¥æ¨¡æ‹Ÿäººç±»çš„å¤šé˜¶æ®µè°ƒè¯•è¿‡ç¨‹æ¥è¯„ä¼°LLMçš„è°ƒè¯•èƒ½åŠ›ã€‚å®éªŒè¡¨æ˜ï¼Œè¾ƒå°è§„æ¨¡çš„LLMæ¨¡å‹åœ¨DEBUGEVALä¸Šçš„è¡¨ç°ä¸€ç›´è¾ƒå·®ï¼Œå­˜åœ¨ç†è§£ä»£ç è¯­ä¹‰ä¸Šçš„å±€é™ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œä½œè€…æå‡ºäº†åŸºäºé€šä¿¡ä»£ç†çš„æ•°æ®åˆæˆï¼ˆCOASTï¼‰æ¡†æ¶ï¼Œè¯¥æ¡†æ¶é‡‡ç”¨å¤šä»£ç†ç³»ç»Ÿç”Ÿæˆé«˜è´¨é‡çš„è®­ç»ƒæ•°æ®ï¼Œç”¨äºç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒCOASTç”Ÿæˆçš„æ•°æ®ä¼˜äºäººå·¥å’ŒGPT-4ç”Ÿæˆçš„æ•°æ®ï¼Œä½¿è¾ƒå°è§„æ¨¡çš„LLMæ¨¡å‹çš„è°ƒè¯•æ€§èƒ½ä¸GPT-3.5ç›¸å½“ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ä»£ç è°ƒè¯•åœ¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¸­æ˜¯è‡³å…³é‡è¦çš„ï¼Œå…³ä¹å…¶åœ¨ä»£ç ç”Ÿæˆä»»åŠ¡ä¸­çš„å¯é æ€§å’Œæ€§èƒ½ã€‚</li>
<li>å½“å‰ä»£ç è°ƒè¯•åŸºå‡†æµ‹è¯•ä¸»è¦å…³æ³¨ä»£ç ä¿®å¤é˜¶æ®µï¼Œæ— æ³•å…¨é¢è¯„ä¼°LLMçš„è°ƒè¯•èƒ½åŠ›ã€‚</li>
<li>DEBUGEVALåŸºå‡†æµ‹è¯•èƒ½å¤Ÿæ¨¡æ‹Ÿäººç±»çš„å¤šé˜¶æ®µè°ƒè¯•è¿‡ç¨‹ï¼Œæ›´å…¨é¢åœ°è¯„ä¼°LLMçš„è°ƒè¯•èƒ½åŠ›ã€‚</li>
<li>è¾ƒå°è§„æ¨¡çš„LLMæ¨¡å‹åœ¨DEBUGEVALä¸Šçš„è¡¨ç°è¾ƒå·®ï¼Œå­˜åœ¨ç†è§£ä»£ç è¯­ä¹‰çš„å±€é™ã€‚</li>
<li>COASTæ¡†æ¶é‡‡ç”¨å¤šä»£ç†ç³»ç»Ÿç”Ÿæˆé«˜è´¨é‡çš„è®­ç»ƒæ•°æ®ï¼Œç”¨äºç›‘ç£å¾®è°ƒLLMæ¨¡å‹ã€‚</li>
<li>COASTç”Ÿæˆçš„æ•°æ®åœ¨å®éªŒä¸­è¡¨ç°å‡ºä¼˜å¼‚æ€§èƒ½ï¼Œä½¿è¾ƒå°è§„æ¨¡çš„LLMæ¨¡å‹çš„è°ƒè¯•æ€§èƒ½å¾—åˆ°æå‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2408.05006">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-660be6f0d6cb1e936d2f20f30ff70ed2.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-26fd8991b181a4496f36af1aef5e57b7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b38faa9e05c6204aad4674ad4901edc5.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-0ed32f32d33c34829524093b77cd2691.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-853f1e137adffa7ee349722907e36a19.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="Towards-CausalGPT-A-Multi-Agent-Approach-for-Faithful-Knowledge-Reasoning-via-Promoting-Causal-Consistency-in-LLMs"><a href="#Towards-CausalGPT-A-Multi-Agent-Approach-for-Faithful-Knowledge-Reasoning-via-Promoting-Causal-Consistency-in-LLMs" class="headerlink" title="Towards CausalGPT: A Multi-Agent Approach for Faithful Knowledge   Reasoning via Promoting Causal Consistency in LLMs"></a>Towards CausalGPT: A Multi-Agent Approach for Faithful Knowledge   Reasoning via Promoting Causal Consistency in LLMs</h2><p><strong>Authors:Ziyi Tang, Ruilin Wang, Weixing Chen, Yongsen Zheng, Zechuan Chen, Yang Liu, Keze Wang, Tianshui Chen, Liang Lin</strong></p>
<p>Despite the progress of foundation models, knowledge-based reasoning remains a persistent challenge due to their limited capacity for knowledge recall and inference. Existing methods primarily focus on encouraging these models to plan and solve problems or extensively sample reasoning chains independently. However, these methods often overlook conceptual errors and inferential fallacies, inevitably leading to a series of notorious issues such as misleading conclusions, cognitive biases, and reduced decision quality. While explicit modeling of causality is argued to hold promise in addressing these issues, contemporary research efforts have thus far fallen short in achieving causality-based foundation models. Drawing inspiration from the orchestration of diverse specialized agents collaborating to tackle intricate tasks, we propose a framework named Causal-Consistency Chain-of-Thought (CaCo-CoT) that harnesses multi-agent collaboration to bolster the faithfulness and causality of foundation models, involving a set of reasoners and evaluators. These agents collaboratively work within a reasoning-and-consensus paradigm to improve faithfulness. The reasoners are tasked with generating reasoning chains for knowledge-intensive problems by mimicking human causal reasoning. Meanwhile, the evaluator scrutinizes the causal consistency of a reasonerâ€™s reasoning chain from a non-causal and a counterfactual perspective. Our framework demonstrates significant superiority over state-of-the-art methods through extensive and comprehensive evaluations across text-based and multi-modal knowledge reasoning tasks (e.g., science question answering and commonsense reasoning). </p>
<blockquote>
<p>å°½ç®¡åŸºç¡€æ¨¡å‹å–å¾—äº†è¿›å±•ï¼Œä½†ç”±äºå…¶åœ¨çŸ¥è¯†å›å¿†å’Œæ¨æ–­æ–¹é¢çš„æœ‰é™èƒ½åŠ›ï¼ŒåŸºäºçŸ¥è¯†çš„æ¨ç†ä»ç„¶æ˜¯ä¸€ä¸ªæŒä¹…æŒ‘æˆ˜ã€‚ç°æœ‰æ–¹æ³•ä¸»è¦ä¾§é‡äºé¼“åŠ±è¿™äº›æ¨¡å‹è§„åˆ’å’Œè§£å†³é—®é¢˜ï¼Œæˆ–ç‹¬ç«‹åœ°å¹¿æ³›é‡‡æ ·æ¨ç†é“¾ã€‚ç„¶è€Œï¼Œè¿™äº›æ–¹æ³•å¾€å¾€ä¼šå¿½è§†æ¦‚å¿µé”™è¯¯å’Œæ¨ç†è°¬è¯¯ï¼Œä¸å¯é¿å…åœ°å¯¼è‡´ä¸€ç³»åˆ—ä¸¥é‡é—®é¢˜ï¼Œå¦‚è¯¯å¯¼æ€§ç»“è®ºã€è®¤çŸ¥åè§å’Œå†³ç­–è´¨é‡ä¸‹é™ã€‚è™½ç„¶æœ‰äººè®¤ä¸ºæ˜¾å¼å»ºæ¨¡å› æœå…³ç³»æœ‰æœ›åœ¨è§£å†³è¿™äº›é—®é¢˜æ–¹é¢å‘æŒ¥ä½œç”¨ï¼Œä½†å½“å‰çš„ç ”ç©¶åŠªåŠ›è‡³ä»Šå°šæœªå®ç°åŸºäºå› æœå…³ç³»çš„åŸºç¡€æ¨¡å‹ã€‚ä»å¤šç§ä¸“ä¸šä»£ç†çš„ååŒåˆä½œè§£å†³å¤æ‚ä»»åŠ¡ä¸­æ±²å–çµæ„Ÿï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åä¸ºå› æœä¸€è‡´æ€§æ€ç»´é“¾ï¼ˆCaCo-CoTï¼‰çš„æ¡†æ¶ï¼Œè¯¥æ¡†æ¶åˆ©ç”¨å¤šä»£ç†åˆä½œå¢å¼ºåŸºç¡€æ¨¡å‹çš„å¿ è¯šåº¦å’Œå› æœæ€§ï¼Œæ¶‰åŠä¸€ç³»åˆ—æ¨ç†è€…å’Œè¯„ä¼°è€…ã€‚è¿™äº›ä»£ç†åœ¨æ¨ç†å’Œå…±è¯†èŒƒå¼å†…åä½œå·¥ä½œï¼Œä»¥æé«˜å¿ è¯šåº¦ã€‚æ¨ç†è€…çš„ä»»åŠ¡æ˜¯ç”Ÿæˆé’ˆå¯¹çŸ¥è¯†å¯†é›†å‹é—®é¢˜çš„æ¨ç†é“¾ï¼Œé€šè¿‡æ¨¡ä»¿äººç±»çš„å› æœæ¨ç†ã€‚åŒæ—¶ï¼Œè¯„ä¼°è€…ä»éå› æœå’Œå‡è®¾æ€§çš„è§’åº¦å¯¹æ¨ç†è€…çš„å› æœä¸€è‡´æ€§è¿›è¡Œä»”ç»†å®¡æŸ¥ã€‚æˆ‘ä»¬çš„æ¡†æ¶åœ¨æ–‡æœ¬å’Œå¤šæ¨¡æ€çŸ¥è¯†æ¨ç†ä»»åŠ¡ï¼ˆä¾‹å¦‚ç§‘å­¦é—®é¢˜å›ç­”å’Œå¸¸è¯†æ¨ç†ï¼‰çš„å¹¿æ³›å’Œå…¨é¢è¯„ä¼°ä¸­ï¼Œæ˜¾ç¤ºå‡ºå¯¹æœ€å…ˆè¿›æ–¹æ³•çš„æ˜¾è‘—ä¼˜åŠ¿ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11914v4">PDF</a> 12 pages, 9 figures. 3 tables</p>
<p><strong>Summary</strong></p>
<p>è¯¥æ–‡æŒ‡å‡ºï¼Œå°½ç®¡åŸºç¡€æ¨¡å‹æœ‰æ‰€è¿›å±•ï¼Œä½†ç”±äºå…¶åœ¨çŸ¥è¯†å›å¿†å’Œæ¨ç†æ–¹é¢çš„èƒ½åŠ›æœ‰é™ï¼ŒçŸ¥è¯†å‹æ¨ç†ä»ç„¶æ˜¯ä¸€ä¸ªæŒç»­å­˜åœ¨çš„æŒ‘æˆ˜ã€‚ç°æœ‰æ–¹æ³•ä¸»è¦é¼“åŠ±æ¨¡å‹ç‹¬ç«‹è§„åˆ’å¹¶è§£å†³é—®é¢˜æˆ–å¹¿æ³›é‡‡æ ·æ¨ç†é“¾ï¼Œä½†å¾€å¾€å¿½è§†äº†æ¦‚å¿µé”™è¯¯å’Œæ¨ç†è°¬è¯¯ï¼Œå¯¼è‡´è¯¯å¯¼æ€§ç»“è®ºã€è®¤çŸ¥åè§å’Œå†³ç­–è´¨é‡ä¸‹é™ç­‰é—®é¢˜ã€‚å—å¤šç§ä¸“ä¸šä»£ç†ååŒè§£å†³å¤æ‚ä»»åŠ¡çš„å¯å‘ï¼Œæå‡ºä¸€ä¸ªåä¸ºCaCo-CoTçš„æ¡†æ¶ï¼Œåˆ©ç”¨å¤šä»£ç†åä½œå¢å¼ºåŸºç¡€æ¨¡å‹çš„å¿ è¯šæ€§å’Œå› æœæ€§ï¼ŒåŒ…æ‹¬ä¸€ç»„æ¨ç†è€…å’Œè¯„ä¼°è€…ã€‚è¿™äº›ä»£ç†åœ¨æ¨ç†å’Œå…±è¯†èŒƒå¼å†…åä½œï¼Œä»¥æé«˜å¿ è¯šæ€§ã€‚æ¨ç†è€…é€šè¿‡æ¨¡ä»¿äººç±»çš„å› æœæ¨ç†ï¼Œä¸ºçŸ¥è¯†å¯†é›†å‹é—®é¢˜ç”Ÿæˆæ¨ç†é“¾ã€‚åŒæ—¶ï¼Œè¯„ä¼°è€…å¯¹æ¨ç†è€…çš„å› æœä¸€è‡´æ€§è¿›è¡Œéå› æœå’Œå‡è®¾æ€§çš„å®¡è§†ã€‚æˆ‘ä»¬çš„æ¡†æ¶åœ¨æ–‡æœ¬å’Œå¤šæ¨¡æ€çŸ¥è¯†æ¨ç†ä»»åŠ¡ï¼ˆå¦‚ç§‘å­¦é—®é¢˜å›ç­”å’Œå¸¸è¯†æ¨ç†ï¼‰çš„å¹¿æ³›å’Œå…¨é¢è¯„ä¼°ä¸­ï¼Œæ˜¾ç¤ºå‡ºæ˜¾è‘—çš„ä¼˜è¶Šæ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç°æœ‰åŸºç¡€æ¨¡å‹åœ¨çŸ¥è¯†å‹æ¨ç†æ–¹é¢å­˜åœ¨æŒ‘æˆ˜ï¼Œå› ä¸ºå®ƒä»¬æœ‰é™çš„çŸ¥è¯†å›å¿†å’Œæ¨ç†èƒ½åŠ›ã€‚</li>
<li>å½“å‰æ–¹æ³•ä¸»è¦å…³æ³¨é¼“åŠ±æ¨¡å‹ç‹¬ç«‹è§„åˆ’å’Œè§£å†³é—®é¢˜æˆ–å¹¿æ³›é‡‡æ ·æ¨ç†é“¾ï¼Œä½†å¿½è§†äº†æ¦‚å¿µé”™è¯¯å’Œæ¨ç†è°¬è¯¯ã€‚</li>
<li>è¿™å¯èƒ½å¯¼è‡´ä¸€ç³»åˆ—é—®é¢˜ï¼Œå¦‚è¯¯å¯¼æ€§ç»“è®ºã€è®¤çŸ¥åè§å’Œå†³ç­–è´¨é‡ä¸‹é™ã€‚</li>
<li>æ˜¾å¼å»ºæ¨¡å› æœå…³ç³»æ˜¯è§£å†³è¿™äº›é—®é¢˜çš„æœ‰å‰é€”çš„æ–¹æ³•ã€‚</li>
<li>å½“ä»£ç ”ç©¶å°šæœªå®ç°åŸºäºå› æœå…³ç³»çš„åŸºç¡€æ¨¡å‹ã€‚</li>
<li>æå‡ºçš„CaCo-CoTæ¡†æ¶åˆ©ç”¨å¤šä»£ç†åä½œå¢å¼ºæ¨¡å‹çš„å¿ è¯šæ€§å’Œå› æœæ€§ã€‚</li>
<li>è¯¥æ¡†æ¶åŒ…æ‹¬æ¨ç†è€…å’Œè¯„ä¼°è€…ï¼Œå‰è€…ç”Ÿæˆæ¨¡ä»¿äººç±»å› æœæ¨ç†çš„çŸ¥è¯†å¯†é›†å‹é—®é¢˜çš„æ¨ç†é“¾ï¼Œåè€…è¯„ä¼°æ¨ç†çš„å› æœä¸€è‡´æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2308.11914">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-6270272e78c566276d0e5c71d05c92fd.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-71ea3c0d8a6b5cfb20726ae05ead8390.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ea83f6d71997d81442cb40a002cb3219.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e1114275df4ba3d71b7886f1d9fb8acd.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-02-13/Agent/">https://kedreamix.github.io/Talk2Paper/Paper/2025-02-13/Agent/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Agent/">
                                    <span class="chip bg-color">Agent</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-02-13/Few-Shot/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-80d8acb33bdcaba3f7608b2be4ffc7a9.jpg" class="responsive-img" alt="Few-Shot">
                        
                        <span class="card-title">Few-Shot</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Few-Shot æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-02-13  Explanation based In-Context Demonstrations Retrieval for Multilingual   Grammatical Error Correction
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-02-13
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Few-Shot/" class="post-category">
                                    Few-Shot
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Few-Shot/">
                        <span class="chip bg-color">Few-Shot</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-02-13/LLM/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-e3ea5af80ebe75832fea69e3b3e9326b.jpg" class="responsive-img" alt="LLM">
                        
                        <span class="card-title">LLM</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            LLM æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-02-13  Examining Multilingual Embedding Models Cross-Lingually Through   LLM-Generated Adversarial Examples
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-02-13
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/LLM/" class="post-category">
                                    LLM
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/LLM/">
                        <span class="chip bg-color">LLM</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">32102k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
