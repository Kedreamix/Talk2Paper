<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="æ£€æµ‹/åˆ†å‰²/è·Ÿè¸ª">
    <meta name="description" content="æ£€æµ‹/åˆ†å‰²/è·Ÿè¸ª æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-02-13  Amnesia as a Catalyst for Enhancing Black Box Pixel Attacks in Image   Classification and Object Detection">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>æ£€æµ‹/åˆ†å‰²/è·Ÿè¸ª | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-ecc5a2b63abe41724fa1ac8438e80570.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">æ£€æµ‹/åˆ†å‰²/è·Ÿè¸ª</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/%E6%A3%80%E6%B5%8B-%E5%88%86%E5%89%B2-%E8%B7%9F%E8%B8%AA/">
                                <span class="chip bg-color">æ£€æµ‹/åˆ†å‰²/è·Ÿè¸ª</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/%E6%A3%80%E6%B5%8B-%E5%88%86%E5%89%B2-%E8%B7%9F%E8%B8%AA/" class="post-category">
                                æ£€æµ‹/åˆ†å‰²/è·Ÿè¸ª
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-02-13
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-05-14
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    6.8k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    28 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-02-13-æ›´æ–°"><a href="#2025-02-13-æ›´æ–°" class="headerlink" title="2025-02-13 æ›´æ–°"></a>2025-02-13 æ›´æ–°</h1><h2 id="Amnesia-as-a-Catalyst-for-Enhancing-Black-Box-Pixel-Attacks-in-Image-Classification-and-Object-Detection"><a href="#Amnesia-as-a-Catalyst-for-Enhancing-Black-Box-Pixel-Attacks-in-Image-Classification-and-Object-Detection" class="headerlink" title="Amnesia as a Catalyst for Enhancing Black Box Pixel Attacks in Image   Classification and Object Detection"></a>Amnesia as a Catalyst for Enhancing Black Box Pixel Attacks in Image   Classification and Object Detection</h2><p><strong>Authors:Dongsu Song, Daehwa Ko, Jay Hoon Jung</strong></p>
<p>It is well known that query-based attacks tend to have relatively higher success rates in adversarial black-box attacks. While research on black-box attacks is actively being conducted, relatively few studies have focused on pixel attacks that target only a limited number of pixels. In image classification, query-based pixel attacks often rely on patches, which heavily depend on randomness and neglect the fact that scattered pixels are more suitable for adversarial attacks. Moreover, to the best of our knowledge, query-based pixel attacks have not been explored in the field of object detection. To address these issues, we propose a novel pixel-based black-box attack called Remember and Forget Pixel Attack using Reinforcement Learning(RFPAR), consisting of two main components: the Remember and Forget processes. RFPAR mitigates randomness and avoids patch dependency by leveraging rewards generated through a one-step RL algorithm to perturb pixels. RFPAR effectively creates perturbed images that minimize the confidence scores while adhering to limited pixel constraints. Furthermore, we advance our proposed attack beyond image classification to object detection, where RFPAR reduces the confidence scores of detected objects to avoid detection. Experiments on the ImageNet-1K dataset for classification show that RFPAR outperformed state-of-the-art query-based pixel attacks. For object detection, using the MSCOCO dataset with YOLOv8 and DDQ, RFPAR demonstrates comparable mAP reduction to state-of-the-art query-based attack while requiring fewer query. Further experiments on the Argoverse dataset using YOLOv8 confirm that RFPAR effectively removed objects on a larger scale dataset. Our code is available at <a target="_blank" rel="noopener" href="https://github.com/KAU-QuantumAILab/RFPAR">https://github.com/KAU-QuantumAILab/RFPAR</a>. </p>
<blockquote>
<p>å·²çŸ¥åŸºäºæŸ¥è¯¢çš„æ”»å‡»åœ¨æ•Œå¯¹é»‘ç®±æ”»å‡»ä¸­å…·æœ‰ç›¸å¯¹è¾ƒé«˜çš„æˆåŠŸç‡ã€‚å°½ç®¡å…³äºé»‘ç®±æ”»å‡»çš„ç ”ç©¶æ­£åœ¨ç§¯æè¿›è¡Œï¼Œä½†é’ˆå¯¹ä»…é’ˆå¯¹æœ‰é™åƒç´ ç›®æ ‡çš„åƒç´ æ”»å‡»çš„ç ”ç©¶ç›¸å¯¹è¾ƒå°‘ã€‚åœ¨å›¾åƒåˆ†ç±»ä¸­ï¼ŒåŸºäºæŸ¥è¯¢çš„åƒç´ æ”»å‡»é€šå¸¸ä¾èµ–äºè¡¥ä¸ï¼Œè¿™å¾ˆå¤§ç¨‹åº¦ä¸Šä¾èµ–äºéšæœºæ€§ï¼Œå¹¶å¿½ç•¥äº†åˆ†æ•£çš„åƒç´ æ›´é€‚åˆäºå¯¹æŠ—æ€§æ”»å‡»çš„äº‹å®ã€‚è€Œä¸”ï¼Œæ®æˆ‘ä»¬æ‰€çŸ¥ï¼ŒåŸºäºæŸ¥è¯¢çš„åƒç´ æ”»å‡»åœ¨ç›®æ ‡æ£€æµ‹é¢†åŸŸå°šæœªå¾—åˆ°æ¢ç´¢ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ä½¿ç”¨å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰çš„åŸºäºåƒç´ çš„é»‘ç®±æ”»å‡»æ–¹æ³•ï¼Œç§°ä¸ºâ€œè®°ä½å’Œé—å¿˜åƒç´ æ”»å‡»â€ï¼ˆRFPARï¼‰ï¼Œå®ƒåŒ…å«ä¸¤ä¸ªä¸»è¦è¿‡ç¨‹ï¼šè®°ä½å’Œé—å¿˜ã€‚RFPARé€šè¿‡åˆ©ç”¨ä¸€æ­¥RLç®—æ³•ç”Ÿæˆçš„å¥–åŠ±æ¥æ‰°åŠ¨åƒç´ ï¼Œä»è€Œå‡è½»äº†éšæœºæ€§å¹¶é¿å…äº†è¡¥ä¸ä¾èµ–ã€‚RFPARæœ‰æ•ˆåœ°åˆ›å»ºäº†æ‰°åŠ¨å›¾åƒï¼Œè¿™äº›å›¾åƒåœ¨éµå¾ªæœ‰é™åƒç´ çº¦æŸçš„åŒæ—¶æœ€å°åŒ–äº†ç½®ä¿¡åº¦åˆ†æ•°ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å°†æå‡ºçš„æ”»å‡»æ¨è¿›åˆ°äº†ç›®æ ‡æ£€æµ‹é¢†åŸŸï¼ŒRFPARé€šè¿‡é™ä½æ£€æµ‹å¯¹è±¡çš„ç½®ä¿¡åº¦åˆ†æ•°æ¥é¿å…æ£€æµ‹ã€‚åœ¨ImageNet-1Kæ•°æ®é›†ä¸Šçš„åˆ†ç±»å®éªŒè¡¨æ˜ï¼ŒRFPARä¼˜äºæœ€å…ˆè¿›çš„åŸºäºæŸ¥è¯¢çš„åƒç´ æ”»å‡»ã€‚å¯¹äºç›®æ ‡æ£€æµ‹ï¼Œä½¿ç”¨MSCOCOæ•°æ®é›†ä¸YOLOv8å’ŒDDQçš„RFPARå®ç°äº†ä¸æœ€å…ˆè¿›çš„åŸºäºæŸ¥è¯¢çš„æ”»å‡»ç›¸å½“çš„mAPå‡å°‘ï¼ŒåŒæ—¶éœ€è¦è¾ƒå°‘çš„æŸ¥è¯¢æ¬¡æ•°ã€‚ä½¿ç”¨YOLOv8è¿›è¡Œçš„Argoverseæ•°æ®é›†ä¸Šçš„è¿›ä¸€æ­¥å®éªŒè¯å®ï¼ŒRFPARåœ¨å¤§è§„æ¨¡æ•°æ®é›†ä¸Šæœ‰æ•ˆåœ°ç§»é™¤äº†å¯¹è±¡ã€‚æˆ‘ä»¬çš„ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/KAU-QuantumAILab/RFPAR%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/KAU-QuantumAILab/RFPARæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.07821v1">PDF</a> Accepted as a poster at NeurIPS 2024</p>
<p><strong>Summary</strong><br>è¯¥æ–‡æœ¬æå‡ºäº†ä¸€ç§åä¸ºRFPARçš„åŸºäºåƒç´ çš„é»‘ç›’æ”»å‡»æ–¹æ³•ï¼Œç”¨äºå›¾åƒåˆ†ç±»å’Œå¯¹è±¡æ£€æµ‹ã€‚RFPARä½¿ç”¨å¼ºåŒ–å­¦ä¹ æ¥è®°ä½å¹¶å¿˜è®°åƒç´ æ”»å‡»ï¼Œé€šè¿‡ç”Ÿæˆå¥–åŠ±æ¥å‡å°‘éšæœºæ€§å¹¶é¿å…è¡¥ä¸ä¾èµ–ã€‚å®ƒåœ¨ImageNet-1Kæ•°æ®é›†ä¸Šçš„åˆ†ç±»è¡¨ç°ä¼˜äºå…¶ä»–åŸºäºæŸ¥è¯¢çš„åƒç´ æ”»å‡»ï¼Œå¹¶åœ¨MSCOCOæ•°æ®é›†ä¸Šä½¿ç”¨YOLOv8å’ŒDDQçš„å¯¹è±¡æ£€æµ‹ä¸­è¡¨ç°å‡ºè‰¯å¥½çš„æ€§èƒ½ã€‚ä»£ç å·²å…¬å¼€åœ¨GitHubä¸Šã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>RFPARæ˜¯ä¸€ç§åŸºäºåƒç´ çš„é»‘ç›’æ”»å‡»æ–¹æ³•ï¼Œæ—¨åœ¨è§£å†³é’ˆå¯¹å›¾åƒåˆ†ç±»å’Œå¯¹è±¡æ£€æµ‹çš„æŸ¥è¯¢åŸºäºåƒç´ æ”»å‡»çš„é—®é¢˜ã€‚</li>
<li>RFPARä½¿ç”¨å¼ºåŒ–å­¦ä¹ æ¥è®°ä½å’Œå¿˜è®°åƒç´ æ”»å‡»ï¼Œé€šè¿‡ç”Ÿæˆå¥–åŠ±æ¥å‡å°‘éšæœºæ€§å’Œé¿å…è¡¥ä¸ä¾èµ–ã€‚</li>
<li>RFPARå¯ä»¥åœ¨æœ‰é™çš„åƒç´ çº¦æŸå†…åˆ›å»ºæ‰°åŠ¨å›¾åƒï¼Œé™ä½ç½®ä¿¡åº¦åˆ†æ•°ã€‚</li>
<li>åœ¨ImageNet-1Kæ•°æ®é›†ä¸Šçš„åˆ†ç±»å®éªŒè¡¨æ˜ï¼ŒRFPARçš„æ€§èƒ½ä¼˜äºå…¶ä»–å…ˆè¿›çš„æŸ¥è¯¢åŸºäºåƒç´ çš„æ”»å‡»ã€‚</li>
<li>åœ¨ä½¿ç”¨YOLOv8å’ŒDDQçš„MSCOCOæ•°æ®é›†ä¸Šçš„å¯¹è±¡æ£€æµ‹å®éªŒä¸­ï¼ŒRFPARè¡¨ç°å‡ºä¸æœ€å…ˆè¿›çš„æŸ¥è¯¢æ”»å‡»ç›¸å½“çš„mAPå‡å°‘ï¼Œä½†éœ€è¦è¾ƒå°‘çš„æŸ¥è¯¢ã€‚</li>
<li>Argoverseæ•°æ®é›†ä¸Šçš„å®éªŒè¿›ä¸€æ­¥è¯æ˜äº†RFPARåœ¨å¤§è§„æ¨¡æ•°æ®é›†ä¸Šç§»é™¤å¯¹è±¡çš„æœ‰æ•ˆæ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.07821">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-d7e3a0353294caac48a499efc6434b95.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b8b513661ad93b60e97011628e33a2f1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-319d1a1f696baa8c07764d30c40f425b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d2fa4d1fe818e3e22b2c0fc15357c6fb.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="SparseFormer-Detecting-Objects-in-HRW-Shots-via-Sparse-Vision-Transformer"><a href="#SparseFormer-Detecting-Objects-in-HRW-Shots-via-Sparse-Vision-Transformer" class="headerlink" title="SparseFormer: Detecting Objects in HRW Shots via Sparse Vision   Transformer"></a>SparseFormer: Detecting Objects in HRW Shots via Sparse Vision   Transformer</h2><p><strong>Authors:Wenxi Li, Yuchen Guo, Jilai Zheng, Haozhe Lin, Chao Ma, Lu Fang, Xiaokang Yang</strong></p>
<p>Recent years have seen an increase in the use of gigapixel-level image and video capture systems and benchmarks with high-resolution wide (HRW) shots. However, unlike close-up shots in the MS COCO dataset, the higher resolution and wider field of view raise unique challenges, such as extreme sparsity and huge scale changes, causing existing close-up detectors inaccuracy and inefficiency. In this paper, we present a novel model-agnostic sparse vision transformer, dubbed SparseFormer, to bridge the gap of object detection between close-up and HRW shots. The proposed SparseFormer selectively uses attentive tokens to scrutinize the sparsely distributed windows that may contain objects. In this way, it can jointly explore global and local attention by fusing coarse- and fine-grained features to handle huge scale changes. SparseFormer also benefits from a novel Cross-slice non-maximum suppression (C-NMS) algorithm to precisely localize objects from noisy windows and a simple yet effective multi-scale strategy to improve accuracy. Extensive experiments on two HRW benchmarks, PANDA and DOTA-v1.0, demonstrate that the proposed SparseFormer significantly improves detection accuracy (up to 5.8%) and speed (up to 3x) over the state-of-the-art approaches. </p>
<blockquote>
<p>è¿‘å¹´æ¥ï¼Œä½¿ç”¨åƒå…†åƒç´ çº§åˆ«çš„å›¾åƒå’Œè§†é¢‘æ•è·ç³»ç»Ÿä»¥åŠé«˜åˆ†è¾¨ç‡å®½è§†è§’ï¼ˆHRWï¼‰é•œå¤´åŸºå‡†æµ‹è¯•çš„æƒ…å†µæ—¥ç›Šå¢å¤šã€‚ç„¶è€Œï¼Œä¸MS COCOæ•°æ®é›†ä¸­çš„ç‰¹å†™é•œå¤´ä¸åŒï¼Œæ›´é«˜çš„åˆ†è¾¨ç‡å’Œæ›´å®½çš„è§†é‡å¸¦æ¥äº†ç‹¬ç‰¹çš„æŒ‘æˆ˜ï¼Œä¾‹å¦‚æç«¯ç¨€ç–å’Œå·¨å¤§å°ºåº¦å˜åŒ–ï¼Œå¯¼è‡´ç°æœ‰ç‰¹å†™æ£€æµ‹å™¨çš„ä¸å‡†ç¡®å’Œæ•ˆç‡ä½ä¸‹ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°å‹æ¨¡å‹æ— å…³çš„ç¨€ç–è§†è§‰å˜å‹å™¨ï¼Œç§°ä¸ºSparseFormerï¼Œä»¥å¼¥è¡¥ç‰¹å†™é•œå¤´å’Œé«˜åˆ†è¾¨ç‡å®½è§†è§’é•œå¤´ä¹‹é—´ç‰©ä½“æ£€æµ‹çš„å·®è·ã€‚æå‡ºçš„SparseFormeræœ‰é€‰æ‹©åœ°ä½¿ç”¨æ³¨æ„åŠ›æ ‡è®°æ¥ä»”ç»†å®¡æŸ¥å¯èƒ½åŒ…å«ç‰©ä½“çš„ç¨€ç–åˆ†å¸ƒçª—å£ã€‚é€šè¿‡è¿™ç§æ–¹å¼ï¼Œå®ƒå¯ä»¥é€šè¿‡èåˆç²—ç²’åº¦å’Œç»†ç²’åº¦ç‰¹å¾æ¥å…±åŒæ¢ç´¢å…¨å±€å’Œå±€éƒ¨æ³¨æ„åŠ›ï¼Œä»¥å¤„ç†å·¨å¤§çš„å°ºåº¦å˜åŒ–ã€‚SparseFormerè¿˜å—ç›Šäºä¸€ç§æ–°çš„è·¨åˆ‡ç‰‡éæœ€å¤§æŠ‘åˆ¶ï¼ˆC-NMSï¼‰ç®—æ³•ï¼Œå¯ä»å˜ˆæ‚çš„çª—å£ä¸­ç²¾ç¡®å®šä½ç‰©ä½“ï¼Œä»¥åŠä¸€ç§ç®€å•æœ‰æ•ˆçš„å¤šå°ºåº¦ç­–ç•¥ï¼Œä»¥æé«˜å‡†ç¡®æ€§ã€‚åœ¨PANDAå’ŒDOTA-v1.0ä¸¤ä¸ªé«˜åˆ†è¾¨ç‡å®½è§†è§’åŸºå‡†æµ‹è¯•çš„å¤§é‡å®éªŒè¡¨æ˜ï¼Œä¸ä¼ ç»Ÿçš„å…ˆè¿›æ–¹æ³•ç›¸æ¯”ï¼Œæ‰€æå‡ºçš„SparseFormeråœ¨æ£€æµ‹ç²¾åº¦ä¸Šæé«˜äº†é«˜è¾¾5.8%ï¼Œé€Ÿåº¦æé«˜äº†é«˜è¾¾ä¸‰å€ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.07216v1">PDF</a> This paper is accepted to ACM MM 2024</p>
<p><strong>Summary</strong>ï¼šé’ˆå¯¹é«˜åˆ†è¾¨ç‡å®½è§†è§’å›¾åƒå’Œè§†é¢‘æ•è·ç³»ç»Ÿä¸­å­˜åœ¨çš„æç«¯ç¨€ç–æ€§å’Œå·¨å¤§å°ºåº¦å˜åŒ–ç­‰æŒ‘æˆ˜ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°å‹æ¨¡å‹æ— å…³çš„ç¨€ç–è§†è§‰è½¬æ¢å™¨SparseFormerï¼Œç”¨äºåœ¨è¿‘è·ç¦»æ‹æ‘„å’Œé«˜åˆ†è¾¨ç‡å®½è§†è§’æ‹æ‘„ä¹‹é—´å»ºç«‹ç›®æ ‡æ£€æµ‹æ¡¥æ¢ã€‚SparseFormerèƒ½å¤Ÿç»“åˆå…¨å±€å’Œå±€éƒ¨æ³¨æ„åŠ›ï¼Œå¤„ç†å¤§è§„æ¨¡å˜åŒ–é—®é¢˜ã€‚åŒæ—¶é‡‡ç”¨è·¨åˆ‡ç‰‡éæœ€å¤§æŠ‘åˆ¶ç®—æ³•å’Œå¤šå°ºåº¦ç­–ç•¥ï¼Œæé«˜äº†ç›®æ ‡å®šä½å’Œæ£€æµ‹ç²¾åº¦ã€‚åœ¨PANDAå’ŒDOTA-v1.0ä¸¤ä¸ªé«˜åˆ†è¾¨ç‡å®½è§†è§’æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒSparseFormeråœ¨æ£€æµ‹ç²¾åº¦å’Œé€Ÿåº¦ä¸Šå‡ä¼˜äºç°æœ‰æŠ€æœ¯ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>SparseFormeré’ˆå¯¹é«˜åˆ†è¾¨ç‡å®½è§†è§’å›¾åƒçš„ç›®æ ‡æ£€æµ‹æå‡ºäº†æ–°é¢–çš„è§£å†³æ–¹æ¡ˆã€‚</li>
<li>SparseFormeråˆ©ç”¨é€‰æ‹©æ€§æ³¨æ„åŠ›ä»¤ç‰Œå¤„ç†ç¨€ç–åˆ†å¸ƒçª—å£ä¸­çš„ç›®æ ‡ï¼Œè”åˆæ¢ç´¢å…¨å±€å’Œå±€éƒ¨æ³¨æ„åŠ›ã€‚</li>
<li>SparseFormeråˆ©ç”¨ç²—ç²’åº¦å’Œç»†ç²’åº¦ç‰¹å¾çš„èåˆæ¥å¤„ç†å¤§è§„æ¨¡å˜åŒ–é—®é¢˜ã€‚</li>
<li>SparseFormeré‡‡ç”¨è·¨åˆ‡ç‰‡éæœ€å¤§æŠ‘åˆ¶ç®—æ³•å’Œå¤šå°ºåº¦ç­–ç•¥æé«˜ç›®æ ‡å®šä½ç²¾åº¦å’Œæ£€æµ‹æ€§èƒ½ã€‚</li>
<li>SparseFormeråœ¨PANDAå’ŒDOTA-v1.0æ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¡¨æ˜å…¶æ˜¾è‘—æé«˜äº†æ£€æµ‹ç²¾åº¦å’Œé€Ÿåº¦ã€‚</li>
<li>SparseFormerå¯¹ç°æœ‰çš„ç›®æ ‡æ£€æµ‹å™¨è¿›è¡Œäº†æ”¹è¿›ï¼Œä½¿å…¶åœ¨é«˜åˆ†è¾¨ç‡å®½è§†è§’å›¾åƒä¸Šæ›´å‡†ç¡®ã€æ›´é«˜æ•ˆã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.07216">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-a2b7be62a3e952cb2ddd1d9116299954.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-30c471043c0fd321a18413388ae59ee5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3d2cdc67ffad1da4ef92eae946274358.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-23e9115e8d15a88c2a76a64c1024569c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-70553c81e46e6e730b4c5a632213eec0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2b23764d09ebeb991410aef8cb28b659.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Globality-Strikes-Back-Rethinking-the-Global-Knowledge-of-CLIP-in-Training-Free-Open-Vocabulary-Semantic-Segmentation"><a href="#Globality-Strikes-Back-Rethinking-the-Global-Knowledge-of-CLIP-in-Training-Free-Open-Vocabulary-Semantic-Segmentation" class="headerlink" title="Globality Strikes Back: Rethinking the Global Knowledge of CLIP in   Training-Free Open-Vocabulary Semantic Segmentation"></a>Globality Strikes Back: Rethinking the Global Knowledge of CLIP in   Training-Free Open-Vocabulary Semantic Segmentation</h2><p><strong>Authors:Jingyun Wang, Cilin Yan, Guoliang Kang</strong></p>
<p>Recent works modify CLIP to perform open-vocabulary semantic segmentation in a training-free manner (TF-OVSS). In CLIP, patch-wise image representations mainly encode the homogeneous image-level properties and thus are not discriminative enough, hindering its application to the dense prediction task. Previous works make image features more distinct across patches, through making each patch mainly attend to itself or the neighboring patches within a narrow local window. However, with their modifications, the ability of CLIP to aggregate global context information, which is known to be useful for distinguishing confusing categories, is largely weakened. In this paper, we propose a new method named GCLIP, which mines the beneficial global knowledge of CLIP to facilitate the TF-OVSS task. Firstly, we aim to equip the last-block attention with image-level properties while not introducing homogeneous attention patterns across patches. In GCLIP, we merge the attention from the global token emerging blocks with the Query-Query attention to realize this goal. Secondly, we aim to make the Value embeddings of the last-block attention module more distinct and semantically correlated. To realize this, we design a novel channel suppression strategy. As the representation of each patch is finally determined by the attention weights and the Value embeddings, our method can generate more discriminative patch-level image features while absorbing global context information. Extensive experiments on five standard benchmarks demonstrate that our method consistently outperforms previous state-of-the-arts. </p>
<blockquote>
<p>è¿‘æœŸçš„ç ”ç©¶å¯¹CLIPè¿›è¡Œäº†ä¿®æ”¹ï¼Œä»¥æ— è®­ç»ƒçš„æ–¹å¼æ‰§è¡Œå¼€æ”¾è¯æ±‡è¯­ä¹‰åˆ†å‰²ï¼ˆTF-OVSSï¼‰ã€‚åœ¨CLIPä¸­ï¼Œå›¾åƒåˆ†å—çš„è¡¨ç¤ºä¸»è¦ç¼–ç äº†å›¾åƒçº§åˆ«çš„åŒè´¨å±æ€§ï¼Œå› æ­¤åŒºåˆ†åº¦ä¸è¶³ï¼Œé˜»ç¢äº†å…¶åœ¨å¯†é›†é¢„æµ‹ä»»åŠ¡ä¸­çš„åº”ç”¨ã€‚æ—©æœŸçš„ç ”ç©¶ä½¿å›¾åƒç‰¹å¾åœ¨ä¸åŒçš„åˆ†å—ä¹‹é—´æ›´åŠ ä¸åŒï¼Œæ–¹æ³•æ˜¯é€šè¿‡ä½¿æ¯ä¸ªåˆ†å—ä¸»è¦å…³æ³¨è‡ªèº«æˆ–ç‹­çª„å±€éƒ¨çª—å£å†…çš„ç›¸é‚»åˆ†å—ã€‚ç„¶è€Œï¼Œè¿™äº›ä¿®æ”¹åœ¨å¾ˆå¤§ç¨‹åº¦ä¸Šå‰Šå¼±äº†CLIPèšåˆå…¨å±€ä¸Šä¸‹æ–‡ä¿¡æ¯çš„èƒ½åŠ›ï¼Œè¿™æ˜¯åŒºåˆ†æ··æ·†ç±»åˆ«æ‰€å¿…éœ€çš„ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åä¸ºGCLIPçš„æ–°æ–¹æ³•ï¼Œå®ƒæŒ–æ˜CLIPçš„æœ‰ç›Šå…¨å±€çŸ¥è¯†ä»¥ä¿ƒè¿›TF-OVSSä»»åŠ¡ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬çš„ç›®æ ‡æ˜¯ä½¿æœ€åä¸€å±‚æ³¨æ„åŠ›å…·å¤‡å›¾åƒçº§åˆ«çš„å±æ€§ï¼ŒåŒæ—¶ä¸åœ¨åˆ†å—ä¹‹é—´å¼•å…¥åŒè´¨æ³¨æ„åŠ›æ¨¡å¼ã€‚åœ¨GCLIPä¸­ï¼Œæˆ‘ä»¬å°†æ¥è‡ªå…¨å±€ä»¤ç‰Œç”Ÿæˆå—çš„æ³¨æ„åŠ›ä¸Query-Queryæ³¨æ„åŠ›åˆå¹¶æ¥å®ç°è¿™ä¸€ç›®æ ‡ã€‚å…¶æ¬¡ï¼Œæˆ‘ä»¬æ—¨åœ¨ä½¿æœ€åä¸€å±‚æ³¨æ„åŠ›æ¨¡å—çš„ValueåµŒå…¥æ›´åŠ ä¸åŒä¸”è¯­ä¹‰ç›¸å…³ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸€ç§æ–°å‹é€šé“æŠ‘åˆ¶ç­–ç•¥ã€‚ç”±äºæ¯ä¸ªåˆ†å—çš„è¡¨ç¤ºæœ€ç»ˆç”±æ³¨æ„åŠ›æƒé‡å’ŒValueåµŒå…¥å†³å®šï¼Œå› æ­¤æˆ‘ä»¬çš„æ–¹æ³•å¯ä»¥åœ¨å¸æ”¶å…¨å±€ä¸Šä¸‹æ–‡ä¿¡æ¯çš„åŒæ—¶ç”Ÿæˆæ›´å…·åŒºåˆ†æ€§çš„åˆ†å—çº§å›¾åƒç‰¹å¾ã€‚åœ¨äº”ä¸ªæ ‡å‡†åŸºå‡†æµ‹è¯•ä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•å§‹ç»ˆä¼˜äºå…ˆå‰çš„æŠ€æœ¯æ°´å¹³ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.06818v1">PDF</a> Under review</p>
<p><strong>Summary</strong><br>åŸºäºCLIPæ¨¡å‹ï¼Œè¯¥æ–‡æå‡ºäº†ä¸€ç§æ–°çš„æ–¹æ³•GCLIPï¼Œæ—¨åœ¨è§£å†³å›¾åƒè¯­ä¹‰åˆ†å‰²ä¸­é¢ä¸´çš„é—®é¢˜ã€‚é€šè¿‡æŒ–æ˜CLIPçš„å…¨å±€çŸ¥è¯†æ¥æå‡å¼€æ”¾è¯æ±‡è¡¨ä¸‹çš„æ— è®­ç»ƒè¯­ä¹‰åˆ†å‰²ï¼ˆTF-OVSSï¼‰æ€§èƒ½ã€‚GCLIPé€šè¿‡åˆå¹¶å…¨å±€ä»¤ç‰Œçªå‡ºå—çš„æ³¨æ„åŠ›ä¸Query-Queryæ³¨æ„åŠ›ï¼Œä½¿æœ€åä¸€ä¸ªæ³¨æ„åŠ›å—çš„æ³¨æ„åŠ›å…·æœ‰å›¾åƒçº§åˆ«å±æ€§åŒæ—¶ä¸å¼•å…¥åŒè´¨åŒ–çš„æ³¨æ„åŠ›æ¨¡å¼ã€‚æ­¤å¤–ï¼Œè¿˜è®¾è®¡äº†ä¸€ç§æ–°çš„é€šé“æŠ‘åˆ¶ç­–ç•¥ä½¿æœ€åä¸€ä¸ªæ³¨æ„åŠ›å—çš„ValueåµŒå…¥æ›´åŠ ç‹¬ç‰¹å’Œè¯­ä¹‰ç›¸å…³ã€‚å®éªŒè¯æ˜ï¼Œè¯¥æ–¹æ³•åœ¨äº”ä¸ªæ ‡å‡†æ•°æ®é›†ä¸Šå‡ä¼˜äºå…ˆå‰çš„æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>æ–‡ç« åŸºäºCLIPæ¨¡å‹è¿›è¡Œæ”¹è¿›ï¼Œæå‡ºGCLIPæ–¹æ³•ï¼Œæ—¨åœ¨è§£å†³å›¾åƒè¯­ä¹‰åˆ†å‰²ä¸­çš„éš¾é¢˜ã€‚</li>
<li>GCLIPé€šè¿‡åˆå¹¶å…¨å±€çŸ¥è¯†ä¸Query-Queryæ³¨æ„åŠ›æ¥æå‡æ€§èƒ½ï¼Œä½¿æœ€åä¸€ä¸ªæ³¨æ„åŠ›å—å…·æœ‰å›¾åƒçº§åˆ«å±æ€§ã€‚</li>
<li>è®¾è®¡äº†æ–°çš„é€šé“æŠ‘åˆ¶ç­–ç•¥ï¼Œä½¿ValueåµŒå…¥æ›´åŠ ç‹¬ç‰¹å’Œè¯­ä¹‰ç›¸å…³ã€‚</li>
<li>GCLIPåœ¨äº”ä¸ªæ ‡å‡†æ•°æ®é›†ä¸Šçš„è¡¨ç°å‡ä¼˜äºå…ˆå‰çš„æ–¹æ³•ã€‚</li>
<li>åŸæ–¹æ³•ä¸»è¦å…³æ³¨å±€éƒ¨çª—å£å†…çš„ç‰¹å¾ï¼Œè€ŒGCLIPåˆ™æ³¨é‡å…¨å±€ä¸Šä¸‹æ–‡ä¿¡æ¯çš„åˆ©ç”¨ã€‚</li>
<li>GCLIPèƒ½å¤Ÿç”Ÿæˆæ›´å…·åŒºåˆ†æ€§çš„è¡¥ä¸çº§å›¾åƒç‰¹å¾ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.06818">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-d3cd7f49f048255b9397db6be10c1a6a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-59d8829c6a69bd8cb712bf27c409394b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-8bfa55cc46779c120986d8864b79d884.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-0c61eaa361ec3744624cf34cda381a08.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Enhancing-Ground-to-Aerial-Image-Matching-for-Visual-Misinformation-Detection-Using-Semantic-Segmentation"><a href="#Enhancing-Ground-to-Aerial-Image-Matching-for-Visual-Misinformation-Detection-Using-Semantic-Segmentation" class="headerlink" title="Enhancing Ground-to-Aerial Image Matching for Visual Misinformation   Detection Using Semantic Segmentation"></a>Enhancing Ground-to-Aerial Image Matching for Visual Misinformation   Detection Using Semantic Segmentation</h2><p><strong>Authors:Emanuele Mule, Matteo Pannacci, Ali Ghasemi Goudarzi, Francesco Pro, Lorenzo Papa, Luca Maiano, Irene Amerini</strong></p>
<p>The recent advancements in generative AI techniques, which have significantly increased the online dissemination of altered images and videos, have raised serious concerns about the credibility of digital media available on the Internet and distributed through information channels and social networks. This issue particularly affects domains that rely heavily on trustworthy data, such as journalism, forensic analysis, and Earth observation. To address these concerns, the ability to geolocate a non-geo-tagged ground-view image without external information, such as GPS coordinates, has become increasingly critical. This study tackles the challenge of linking a ground-view image, potentially exhibiting varying fields of view (FoV), to its corresponding satellite image without the aid of GPS data. To achieve this, we propose a novel four-stream Siamese-like architecture, the Quadruple Semantic Align Net (SAN-QUAD), which extends previous state-of-the-art (SOTA) approaches by leveraging semantic segmentation applied to both ground and satellite imagery. Experimental results on a subset of the CVUSA dataset demonstrate significant improvements of up to 9.8% over prior methods across various FoV settings. </p>
<blockquote>
<p>æœ€è¿‘ç”Ÿæˆå¼äººå·¥æ™ºèƒ½æŠ€æœ¯çš„è¿›å±•å¤§å¤§å¢åŠ äº†ç½‘ç»œä¸Šçš„ç¯¡æ”¹å›¾åƒå’Œè§†é¢‘çš„ä¼ æ’­ï¼Œå¼•å‘äº†äººä»¬å¯¹äº’è”ç½‘ä¸Šå¯é€šè¿‡ä¿¡æ¯æ¸ é“å’Œç¤¾äº¤ç½‘ç»œè·å¾—çš„æ•°å­—åª’ä½“å¯ä¿¡åº¦çš„ä¸¥é‡å…³æ³¨ã€‚è¿™ä¸€é—®é¢˜å°¤å…¶å½±å“åˆ°é‚£äº›ä¸¥é‡ä¾èµ–äºå¯ä¿¡æ•°æ®çš„é¢†åŸŸï¼Œå¦‚æ–°é—»ä¸šã€æ³•åŒ»å­¦åˆ†æå’Œåœ°çƒè§‚æµ‹ã€‚ä¸ºäº†åº”å¯¹è¿™äº›æ‹…å¿§ï¼Œåœ¨ä¸ä¾èµ–å¤–éƒ¨ä¿¡æ¯ï¼ˆå¦‚GPSåæ ‡ï¼‰çš„æƒ…å†µä¸‹å¯¹æœªå¸¦åœ°ç†æ ‡ç­¾çš„åœ°é¢è§†å›¾å›¾åƒè¿›è¡Œåœ°ç†å®šä½çš„èƒ½åŠ›å˜å¾—è‡³å…³é‡è¦ã€‚æœ¬ç ”ç©¶è§£å†³äº†åœ¨æ²¡æœ‰GPSæ•°æ®è¾…åŠ©çš„æƒ…å†µä¸‹ï¼Œå°†å¯èƒ½å±•ç°ä¸åŒè§†é‡èŒƒå›´çš„åœ°é¢è§†å›¾å›¾åƒä¸å…¶å¯¹åº”çš„å«æ˜Ÿå›¾åƒç›¸å…³è”çš„æŒ‘æˆ˜ã€‚ä¸ºäº†å®ç°è¿™ä¸€ç‚¹ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°å‹çš„å››æµSiameseé£æ ¼æ¶æ„â€”â€”Quadruple Semantic Align Netï¼ˆSAN-QUADï¼‰ï¼Œå®ƒé€šè¿‡åº”ç”¨äºåœ°é¢å’Œå«æ˜Ÿå›¾åƒçš„è¯­ä¹‰åˆ†å‰²æ¥æ‰©å±•ä¹‹å‰æœ€å…ˆè¿›çš„ï¼ˆSOTAï¼‰æ–¹æ³•ã€‚åœ¨CVUSAæ•°æ®é›†çš„ä¸€ä¸ªå­é›†ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼Œåœ¨å„ç§è§†é‡è®¾ç½®ä¸‹ï¼Œç›¸è¾ƒäºä¹‹å‰çš„æ–¹æ³•ï¼Œæœ€å¤šå¯æé«˜9.8%ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.06288v2">PDF</a> 9 pages, 4 figures</p>
<p><strong>Summary</strong><br>     è¿‘æœŸç”Ÿæˆå¼AIæŠ€æœ¯çš„è¿›å±•å¯¼è‡´ç½‘ç»œä¸Šä¼ æ’­å¤§é‡ä¿®æ”¹åçš„å›¾åƒå’Œè§†é¢‘ï¼Œå¼•å‘å¯¹äº’è”ç½‘ä¸Šæ•°å­—åª’ä½“å¯ä¿¡åº¦ä¸¥é‡å…³åˆ‡çš„é—®é¢˜ï¼Œå½±å“äº†ä¾èµ–äºå¯ä¿¡æ•°æ®çš„é¢†åŸŸï¼Œå¦‚æ–°é—»ä¸šã€å¸æ³•åˆ†æå’Œåœ°çƒè§‚æµ‹ç­‰ã€‚ä¸ºè§£å†³è¿™ä¸ªé—®é¢˜ï¼Œä¸€ç§æ–°æŠ€æœ¯æ¨å‡ºèƒ½å¤Ÿä¸éœ€ä¾èµ–GPSä¿¡æ¯ç»™éåœ°ç†æ ‡ç­¾åœ°é¢è§†å›¾å›¾åƒè¿›è¡Œåœ°ç†å®šä½ã€‚è¯¥ç ”ç©¶æå‡ºäº†ä¸€ç§æ–°å‹çš„å››æµSiameseæ¶æ„â€”â€”Quadruple Semantic Align Net (SAN-QUAD)ï¼Œå®ƒé€šè¿‡è¯­ä¹‰åˆ†å‰²åº”ç”¨åœ¨åœ°é¢å’Œå«æ˜Ÿå›¾åƒä¸Šï¼Œåœ¨CVUSAæ•°æ®é›†å­é›†ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼Œåœ¨å„ç§è§†é‡è®¾ç½®ä¸‹ï¼Œä¸ä¹‹å‰çš„æ–¹æ³•ç›¸æ¯”ï¼Œå…¶æ”¹è¿›å¹…åº¦é«˜è¾¾9.8%ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>ç”Ÿæˆå¼AIæŠ€æœ¯çš„è¿›å±•å¯¼è‡´æ•°å­—åª’ä½“çš„å¯ä¿¡åº¦é—®é¢˜ã€‚</li>
<li>å¯¹éåœ°ç†æ ‡ç­¾åœ°é¢è§†å›¾å›¾åƒè¿›è¡Œåœ°ç†å®šä½çš„éœ€æ±‚æ—¥ç›Šè¿«åˆ‡ã€‚</li>
<li>æå‡ºäº†ä¸€ç§æ–°å‹çš„å››æµSiameseæ¶æ„â€”â€”Quadruple Semantic Align Net (SAN-QUAD)ã€‚</li>
<li>SAN-QUADåˆ©ç”¨è¯­ä¹‰åˆ†å‰²åº”ç”¨åœ¨åœ°é¢å’Œå«æ˜Ÿå›¾åƒä¸Šã€‚</li>
<li>åœ¨CVUSAæ•°æ®é›†å­é›†ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼ŒSAN-QUADç›¸è¾ƒäºä¹‹å‰çš„æ–¹æ³•æœ‰æ˜¾è‘—æå‡ã€‚</li>
<li>ä¸åŒè§†é‡è®¾ç½®ä¸‹ï¼ŒSAN-QUADçš„æ”¹è¿›å¹…åº¦é«˜è¾¾9.8%ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.06288">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-f0719c13b187d7e66004c93d2926cbf0.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-52a42d28c264162a7d11f31ed902fa7a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-539720d0c85b62836c5fc9802ca09c5e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d9ae9938b80f01d3fef04e9b3624b522.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Robot-Instance-Segmentation-with-Few-Annotations-for-Grasping"><a href="#Robot-Instance-Segmentation-with-Few-Annotations-for-Grasping" class="headerlink" title="Robot Instance Segmentation with Few Annotations for Grasping"></a>Robot Instance Segmentation with Few Annotations for Grasping</h2><p><strong>Authors:Moshe Kimhi, David Vainshtein, Chaim Baskin, Dotan Di Castro</strong></p>
<p>The ability of robots to manipulate objects relies heavily on their aptitude for visual perception. In domains characterized by cluttered scenes and high object variability, most methods call for vast labeled datasets, laboriously hand-annotated, with the aim of training capable models. Once deployed, the challenge of generalizing to unfamiliar objects implies that the model must evolve alongside its domain. To address this, we propose a novel framework that combines Semi-Supervised Learning (SSL) with Learning Through Interaction (LTI), allowing a model to learn by observing scene alterations and leverage visual consistency despite temporal gaps without requiring curated data of interaction sequences. As a result, our approach exploits partially annotated data through self-supervision and incorporates temporal context using pseudo-sequences generated from unlabeled still images. We validate our method on two common benchmarks, ARMBench mix-object-tote and OCID, where it achieves state-of-the-art performance. Notably, on ARMBench, we attain an $\text{AP}<em>{50}$ of $86.37$, almost a $20%$ improvement over existing work, and obtain remarkable results in scenarios with extremely low annotation, achieving an $\text{AP}</em>{50}$ score of $84.89$ with just $1 %$ of annotated data compared to $72$ presented in ARMBench on the fully annotated counterpart. </p>
<blockquote>
<p>æœºå™¨äººæ“ä½œç‰©ä½“çš„èƒ½åŠ›åœ¨å¾ˆå¤§ç¨‹åº¦ä¸Šå–å†³äºå®ƒä»¬çš„è§†è§‰æ„ŸçŸ¥èƒ½åŠ›ã€‚åœ¨åœºæ™¯æ··ä¹±ã€ç‰©ä½“å˜åŒ–æ€§é«˜çš„é¢†åŸŸä¸­ï¼Œå¤§å¤šæ•°æ–¹æ³•éƒ½éœ€è¦å¤§é‡ç»æ‰‹å·¥æ ‡æ³¨çš„æ•°æ®é›†ï¼Œæ—¨åœ¨è®­ç»ƒå…·å¤‡èƒ½åŠ›çš„æ¨¡å‹ã€‚ä¸€æ—¦éƒ¨ç½²ï¼Œå¦‚ä½•æ¨å¹¿åˆ°é™Œç”Ÿç‰©ä½“çš„æŒ‘æˆ˜æ„å‘³ç€æ¨¡å‹å¿…é¡»ä¸å…¶é¢†åŸŸå…±åŒè¿›åŒ–ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„æ¡†æ¶ï¼Œç»“åˆäº†åŠç›‘ç£å­¦ä¹ ï¼ˆSSLï¼‰å’Œé€šè¿‡äº¤äº’å­¦ä¹ ï¼ˆLTIï¼‰ï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿè§‚å¯Ÿåœºæ™¯å˜åŒ–å¹¶å€ŸåŠ©è§†è§‰ä¸€è‡´æ€§ï¼Œå³ä½¿å­˜åœ¨æ—¶é—´é—´éš”ï¼Œä¹Ÿä¸éœ€è¦äº¤äº’åºåˆ—çš„å®šåˆ¶æ•°æ®ã€‚å› æ­¤ï¼Œæˆ‘ä»¬çš„æ–¹æ³•é€šè¿‡è‡ªæˆ‘ç›‘ç£åˆ©ç”¨éƒ¨åˆ†æ³¨é‡Šçš„æ•°æ®ï¼Œå¹¶ä½¿ç”¨ä»æ— æ ‡ç­¾é™æ€å›¾åƒç”Ÿæˆçš„ä¼ªåºåˆ—æ¥èå…¥æ—¶é—´ä¸Šä¸‹æ–‡ã€‚æˆ‘ä»¬åœ¨ä¸¤ä¸ªå¸¸ç”¨åŸºå‡†æµ‹è¯•ï¼ˆARMBenchæ··åˆå¯¹è±¡æ‰˜æ¶å’ŒOCIDï¼‰ä¸ŠéªŒè¯äº†æˆ‘ä»¬çš„æ–¹æ³•ï¼Œå®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œåœ¨ARMBenchä¸Šï¼Œæˆ‘ä»¬çš„$\text{AP}<em>{50}$è¾¾åˆ°86.3fiï¼Œè¾ƒç°æœ‰å·¥ä½œå‡ ä¹æé«˜äº†20%ï¼Œå¹¶ä¸”åœ¨æä½çš„æ³¨é‡Šåœºæ™¯ä¸­å–å¾—äº†æ˜¾è‘—çš„ç»“æœï¼Œåœ¨ä»…æœ‰1%çš„æ³¨é‡Šæ•°æ®çš„æƒ…å†µä¸‹ï¼Œ$\text{AP}</em>{50}$å¾—åˆ†è¾¾åˆ°84.89ï¼Œè€ŒARMBenchåœ¨å…¨æ³¨é‡Šçš„å¯¹åº”ç‰©ä¸Šçš„å¾—åˆ†ä¸º72ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2407.01302v2">PDF</a> </p>
<p><strong>Summary</strong><br>     æœºå™¨äººæ“æ§ç‰©ä½“çš„èƒ½åŠ›ä¾èµ–äºå…¶è§†è§‰æ„ŸçŸ¥èƒ½åŠ›ã€‚é’ˆå¯¹æ‚ä¹±åœºæ™¯å’Œé«˜å¯¹è±¡å¯å˜æ€§çš„é¢†åŸŸï¼Œå¤§å¤šæ•°æ–¹æ³•éœ€è¦å¤§é‡æ‰‹å·¥æ ‡æ³¨çš„æ•°æ®é›†è¿›è¡Œæ¨¡å‹è®­ç»ƒã€‚ä¸ºè§£å†³æ¨¡å‹å¯¹æ–°ç‰©ä½“çš„æ³›åŒ–æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºç»“åˆåŠç›‘ç£å­¦ä¹ å’Œé€šè¿‡äº¤äº’å­¦ä¹ çš„æ–°æ¡†æ¶ã€‚æ¨¡å‹å¯é€šè¿‡è§‚å¯Ÿåœºæ™¯å˜åŒ–å’Œåˆ©ç”¨è§†è§‰ä¸€è‡´æ€§ï¼Œåœ¨æ— éœ€äº¤äº’åºåˆ—çš„å®šåˆ¶æ•°æ®æƒ…å†µä¸‹è¿›è¡Œè‡ªæˆ‘ç›‘ç£å­¦ä¹ ã€‚æˆ‘ä»¬çš„æ–¹æ³•åœ¨ARMBenchå’ŒOCIDä¸¤ä¸ªå¸¸ç”¨åŸºå‡†æµ‹è¯•ä¸­å®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œç‰¹åˆ«æ˜¯åœ¨ARMBenchä¸Šï¼Œæˆ‘ä»¬è¾¾åˆ°äº†86.37çš„AP50å¾—åˆ†ï¼Œè¾ƒç°æœ‰å·¥ä½œæœ‰è¿‘20%çš„æå‡ã€‚åœ¨æ ‡æ³¨æ•°æ®æä½çš„æƒ…å†µä¸‹ï¼Œä»…ä½¿ç”¨1%çš„æ ‡æ³¨æ•°æ®ä¾¿å–å¾—äº†84.89çš„AP50å¾—åˆ†ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æœºå™¨äººçš„ç‰©ä½“æ“æ§èƒ½åŠ›ä¾èµ–äºå…¶è§†è§‰æ„ŸçŸ¥èƒ½åŠ›ã€‚</li>
<li>åœ¨æ‚ä¹±åœºæ™¯å’Œé«˜å¯¹è±¡å¯å˜æ€§çš„é¢†åŸŸï¼Œæœºå™¨äººéœ€è¦å¤§è§„æ¨¡æ ‡æ³¨æ•°æ®é›†è¿›è¡Œæ¨¡å‹è®­ç»ƒã€‚</li>
<li>æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›å¯¹äºå¤„ç†æ–°ç‰©ä½“è‡³å…³é‡è¦ã€‚</li>
<li>æå‡ºç»“åˆåŠç›‘ç£å­¦ä¹ å’Œé€šè¿‡äº¤äº’å­¦ä¹ çš„æ–°æ¡†æ¶ï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿé€šè¿‡è§‚å¯Ÿåœºæ™¯å˜åŒ–å’Œåˆ©ç”¨è§†è§‰ä¸€è‡´æ€§è¿›è¡Œè‡ªæˆ‘ç›‘ç£å­¦ä¹ ã€‚</li>
<li>æ–°æ¡†æ¶åœ¨ARMBenchå’ŒOCIDåŸºå‡†æµ‹è¯•ä¸­å®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚</li>
<li>åœ¨ARMBenchæµ‹è¯•ä¸­ï¼Œæ–°æ¡†æ¶çš„AP50å¾—åˆ†è¾ƒç°æœ‰å·¥ä½œæœ‰è¿‘20%çš„æå‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2407.01302">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-f27cb1cac709f600b85e1fc395e5ca87.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5a84eee90e054dd4ab31aed7d768fa5c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1a2f4370f30cd23bc336064a515d97a8.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="LOGCAN-Adaptive-Local-global-class-aware-network-for-semantic-segmentation-of-remote-sensing-imagery"><a href="#LOGCAN-Adaptive-Local-global-class-aware-network-for-semantic-segmentation-of-remote-sensing-imagery" class="headerlink" title="LOGCAN++: Adaptive Local-global class-aware network for semantic   segmentation of remote sensing imagery"></a>LOGCAN++: Adaptive Local-global class-aware network for semantic   segmentation of remote sensing imagery</h2><p><strong>Authors:Xiaowen Ma, Rongrong Lian, Zhenkai Wu, Hongbo Guo, Mengting Ma, Sensen Wu, Zhenhong Du, Siyang Song, Wei Zhang</strong></p>
<p>Remote sensing images usually characterized by complex backgrounds, scale and orientation variations, and large intra-class variance. General semantic segmentation methods usually fail to fully investigate the above issues, and thus their performances on remote sensing image segmentation are limited. In this paper, we propose our LOGCAN++, a semantic segmentation model customized for remote sensing images, which is made up of a Global Class Awareness (GCA) module and several Local Class Awareness (LCA) modules. The GCA module captures global representations for class-level context modeling to reduce the interference of background noise. The LCA module generates local class representations as intermediate perceptual elements to indirectly associate pixels with the global class representations, targeting at dealing with the large intra-class variance problem. In particular, we introduce affine transformations in the LCA module for adaptive extraction of local class representations to effectively tolerate scale and orientation variations in remotely sensed images. Extensive experiments on three benchmark datasets show that our LOGCAN++ outperforms current mainstream general and remote sensing semantic segmentation methods and achieves a better trade-off between speed and accuracy. Code is available at <a target="_blank" rel="noopener" href="https://github.com/xwmaxwma/rssegmentation">https://github.com/xwmaxwma/rssegmentation</a>. </p>
<blockquote>
<p>é¥æ„Ÿå›¾åƒé€šå¸¸å…·æœ‰å¤æ‚çš„èƒŒæ™¯ã€å°ºåº¦å’Œæ–¹å‘å˜åŒ–ä»¥åŠè¾ƒå¤§çš„ç±»å†…å·®å¼‚ã€‚ä¸€èˆ¬çš„è¯­ä¹‰åˆ†å‰²æ–¹æ³•é€šå¸¸æ— æ³•å®Œå…¨è§£å†³ä¸Šè¿°é—®é¢˜ï¼Œå› æ­¤åœ¨é¥æ„Ÿå›¾åƒåˆ†å‰²æ–¹é¢çš„æ€§èƒ½å—åˆ°é™åˆ¶ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†é’ˆå¯¹é¥æ„Ÿå›¾åƒçš„å®šåˆ¶è¯­ä¹‰åˆ†å‰²æ¨¡å‹LOGCAN++ï¼Œå®ƒç”±å…¨å±€ç±»åˆ«æ„è¯†ï¼ˆGCAï¼‰æ¨¡å—å’Œå¤šä¸ªå±€éƒ¨ç±»åˆ«æ„è¯†ï¼ˆLCAï¼‰æ¨¡å—ç»„æˆã€‚GCAæ¨¡å—æ•è·å…¨å±€è¡¨ç¤ºæ¥è¿›è¡Œç±»åˆ«ä¸Šä¸‹æ–‡å»ºæ¨¡ï¼Œä»¥å‡å°‘èƒŒæ™¯å™ªå£°çš„å¹²æ‰°ã€‚LCAæ¨¡å—ç”Ÿæˆå±€éƒ¨ç±»åˆ«è¡¨ç¤ºä½œä¸ºä¸­é—´æ„ŸçŸ¥å…ƒç´ ï¼Œé—´æ¥åœ°å°†åƒç´ ä¸å…¨å±€ç±»åˆ«è¡¨ç¤ºå…³è”èµ·æ¥ï¼Œæ—¨åœ¨å¤„ç†è¾ƒå¤§çš„ç±»å†…å·®å¼‚é—®é¢˜ã€‚ç‰¹åˆ«æ˜¯ï¼Œæˆ‘ä»¬åœ¨LCAæ¨¡å—ä¸­å¼•å…¥äº†ä»¿å°„å˜æ¢ï¼Œä»¥è‡ªé€‚åº”æå–å±€éƒ¨ç±»åˆ«è¡¨ç¤ºï¼Œä»è€Œæœ‰æ•ˆåœ°å®¹å¿é¥æ„Ÿå›¾åƒä¸­çš„å°ºåº¦å’Œæ–¹å‘å˜åŒ–ã€‚åœ¨ä¸‰ä¸ªåŸºå‡†æ•°æ®é›†ä¸Šçš„å¤§é‡å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„LOGCAN++åœ¨é€Ÿåº¦å’Œå‡†ç¡®æ€§ä¹‹é—´å–å¾—äº†æ›´å¥½çš„æƒè¡¡ï¼Œä¼˜äºå½“å‰ä¸»æµçš„ä¸€èˆ¬å’Œé¥æ„Ÿè¯­ä¹‰åˆ†å‰²æ–¹æ³•ã€‚ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/xwmaxwma/rssegmentation">https://github.com/xwmaxwma/rssegmentation</a>æ‰¾åˆ°ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2406.16502v3">PDF</a> Accepted by TGRS2025</p>
<p><strong>Summary</strong></p>
<p>é’ˆå¯¹é¥æ„Ÿå›¾åƒå¤æ‚èƒŒæ™¯ã€å°ºåº¦ä¸æ–¹å‘å˜åŒ–ä»¥åŠå¤§ç±»å†…å·®å¼‚ç­‰é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†LOGCAN++æ¨¡å‹ï¼Œè¯¥æ¨¡å‹åŒ…å«å…¨å±€ç±»æ„ŸçŸ¥æ¨¡å—å’Œå¤šä¸ªå±€éƒ¨ç±»æ„ŸçŸ¥æ¨¡å—ã€‚å…¨å±€ç±»æ„ŸçŸ¥æ¨¡å—ç”¨äºæ•æ‰å…¨å±€ç±»ä¸Šä¸‹æ–‡è¡¨ç¤ºï¼Œå‡å°‘èƒŒæ™¯å™ªå£°çš„å¹²æ‰°ï¼›å±€éƒ¨ç±»æ„ŸçŸ¥æ¨¡å—ç”Ÿæˆå±€éƒ¨ç±»è¡¨ç¤ºï¼Œé—´æ¥å…³è”åƒç´ ä¸å…¨å±€ç±»è¡¨ç¤ºï¼Œä»¥å¤„ç†å¤§ç±»å†…å·®å¼‚é—®é¢˜ã€‚åœ¨ä¸‰ä¸ªåŸºå‡†æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒLOGCAN++åœ¨é€Ÿåº¦å’Œå‡†ç¡®æ€§ä¹‹é—´å–å¾—äº†æ›´å¥½çš„å¹³è¡¡ï¼Œä¼˜äºä¸»æµçš„ä¸€èˆ¬å’Œé¥æ„Ÿè¯­ä¹‰åˆ†å‰²æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>é¥æ„Ÿå›¾åƒé€šå¸¸å…·æœ‰å¤æ‚èƒŒæ™¯ã€å°ºåº¦ä¸æ–¹å‘å˜åŒ–å’Œè¾ƒå¤§çš„ç±»å†…å·®å¼‚ã€‚</li>
<li>ä¸€èˆ¬è¯­ä¹‰åˆ†å‰²æ–¹æ³•åœ¨é¥æ„Ÿå›¾åƒåˆ†å‰²ä¸Šæ€§èƒ½å—é™ã€‚</li>
<li>LOGCAN++æ˜¯ä¸€ä¸ªé’ˆå¯¹é¥æ„Ÿå›¾åƒçš„è¯­ä¹‰åˆ†å‰²æ¨¡å‹ã€‚</li>
<li>LOGCAN++åŒ…å«å…¨å±€ç±»æ„ŸçŸ¥æ¨¡å—ï¼ˆGCAï¼‰ï¼Œç”¨äºæ•æ‰å…¨å±€ç±»ä¸Šä¸‹æ–‡è¡¨ç¤ºã€‚</li>
<li>LOGCAN++åŒ…å«å±€éƒ¨ç±»æ„ŸçŸ¥æ¨¡å—ï¼ˆLCAï¼‰ï¼Œå¤„ç†å±€éƒ¨ç±»è¡¨ç¤ºå’Œåƒç´ å…³è”ã€‚</li>
<li>LCAæ¨¡å—å¼•å…¥ä»¿å°„å˜æ¢ï¼Œä»¥è‡ªé€‚åº”æå–å±€éƒ¨ç±»è¡¨ç¤ºï¼Œæœ‰æ•ˆåº”å¯¹é¥æ„Ÿå›¾åƒçš„å°ºåº¦å’Œæ–¹å‘å˜åŒ–ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2406.16502">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-d220bf93655691a8bcc6f23f6d46ac09.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-4925ec0f7ce95e87a1d80ccdd1c31731.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-dd374110d73ec55f2cf46f420c485897.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-708bae23da1df723a0f36f3b315386e0.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="UEMM-Air-A-Synthetic-Multi-modal-Dataset-for-Unmanned-Aerial-Vehicle-Object-Detection"><a href="#UEMM-Air-A-Synthetic-Multi-modal-Dataset-for-Unmanned-Aerial-Vehicle-Object-Detection" class="headerlink" title="UEMM-Air: A Synthetic Multi-modal Dataset for Unmanned Aerial Vehicle   Object Detection"></a>UEMM-Air: A Synthetic Multi-modal Dataset for Unmanned Aerial Vehicle   Object Detection</h2><p><strong>Authors:Liang Yao, Fan Liu, Shengxiang Xu, Chuanyi Zhang, Xing Ma, Jianyu Jiang, Zequan Wang, Shimin Di, Jun Zhou</strong></p>
<p>The development of multi-modal learning for Unmanned Aerial Vehicles (UAVs) typically relies on a large amount of pixel-aligned multi-modal image data. However, existing datasets face challenges such as limited modalities, high construction costs, and imprecise annotations. To this end, we propose a synthetic multi-modal UAV-based multi-task dataset, UEMM-Air. Specifically, we simulate various UAV flight scenarios and object types using the Unreal Engine (UE). Then we design the UAVâ€™s flight logic to automatically collect data from different scenarios, perspectives, and altitudes. Furthermore, we propose a novel heuristic automatic annotation algorithm to generate accurate object detection labels. Finally, we utilize labels to generate text descriptions of images to make our UEMM-Air support more cross-modality tasks. In total, our UEMM-Air consists of 120k pairs of images with 6 modalities and precise annotations. Moreover, we conduct numerous experiments and establish new benchmark results on our dataset. We also found that models pre-trained on UEMM-Air exhibit better performance on downstream tasks compared to other similar datasets. The dataset is publicly available (<a target="_blank" rel="noopener" href="https://github.com/1e12Leon/UEMM-Air">https://github.com/1e12Leon/UEMM-Air</a>) to support the research of multi-modal tasks on UAVs. </p>
<blockquote>
<p>æ— äººæœºå¤šæ¨¡æ€å­¦ä¹ çš„å¼€å‘é€šå¸¸ä¾èµ–äºå¤§é‡çš„åƒç´ å¯¹é½å¤šæ¨¡æ€å›¾åƒæ•°æ®ã€‚ç„¶è€Œï¼Œç°æœ‰æ•°æ®é›†é¢ä¸´æ¨¡æ€æœ‰é™ã€å»ºè®¾æˆæœ¬é«˜å’Œæ ‡æ³¨ä¸ç²¾ç¡®ç­‰æŒ‘æˆ˜ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†åŸºäºåˆæˆå¤šæ¨¡æ€çš„æ— äººæœºå¤šä»»åŠ¡æ•°æ®é›†UEMM-Airã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬ä½¿ç”¨Unreal Engineï¼ˆUEï¼‰æ¨¡æ‹Ÿå„ç§æ— äººæœºé£è¡Œåœºæ™¯å’Œå¯¹è±¡ç±»å‹ã€‚ç„¶åï¼Œæˆ‘ä»¬è®¾è®¡æ— äººæœºçš„é£è¡Œé€»è¾‘ï¼Œä»¥ä»ä¸åŒåœºæ™¯ã€è§’åº¦å’Œé«˜åº¦è‡ªåŠ¨æ”¶é›†æ•°æ®ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„å¯å‘å¼è‡ªåŠ¨æ ‡æ³¨ç®—æ³•ï¼Œä»¥ç”Ÿæˆç²¾ç¡®çš„ç›®æ ‡æ£€æµ‹æ ‡ç­¾ã€‚æœ€åï¼Œæˆ‘ä»¬åˆ©ç”¨è¿™äº›æ ‡ç­¾ç”Ÿæˆå›¾åƒæ–‡æœ¬æè¿°ï¼Œä½¿æˆ‘ä»¬çš„UEMM-Airæ”¯æŒæ›´å¤šè·¨æ¨¡æ€ä»»åŠ¡ã€‚æ€»å…±ï¼Œæˆ‘ä»¬çš„UEMM-AiråŒ…å«12ä¸‡å¯¹å›¾åƒï¼Œå…·æœ‰6ç§æ¨¡æ€å’Œç²¾ç¡®æ ‡æ³¨ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬åœ¨è‡ªå·±çš„æ•°æ®é›†ä¸Šè¿›è¡Œäº†å¤§é‡å®éªŒï¼Œå¹¶å»ºç«‹äº†æ–°çš„åŸºå‡†æµ‹è¯•ç»“æœã€‚æˆ‘ä»¬è¿˜å‘ç°ï¼Œåœ¨UEMM-Airä¸Šé¢„è®­ç»ƒçš„æ¨¡å‹åœ¨ä¸‹æ¸¸ä»»åŠ¡ä¸Šçš„æ€§èƒ½ä¼˜äºå…¶ä»–ç±»ä¼¼æ•°æ®é›†ã€‚è¯¥æ•°æ®é›†å…¬å¼€å¯ç”¨ï¼ˆ<a target="_blank" rel="noopener" href="https://github.com/1e12Leon/UEMM-Air%EF%BC%89%EF%BC%8C%E4%BB%A5%E6%94%AF%E6%8C%81%E6%97%A0%E4%BA%BA%E6%9C%BA%E5%A4%9A%E6%A8%A1%E6%80%81%E4%BB%BB%E5%8A%A1%E7%9A%84%E7%A0%94%E7%A9%B6%E3%80%82">https://github.com/1e12Leon/UEMM-Airï¼‰ï¼Œä»¥æ”¯æŒæ— äººæœºå¤šæ¨¡æ€ä»»åŠ¡çš„ç ”ç©¶ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2406.06230v2">PDF</a> </p>
<p><strong>Summary</strong><br>    é’ˆå¯¹æ— äººæœºå¤šæ¨¡æ€å­¦ä¹ å‘å±•é¢ä¸´çš„æŒ‘æˆ˜ï¼Œå¦‚æ•°æ®é‡ä¸è¶³ã€æ¨¡æ€æœ‰é™ã€æ„å»ºæˆæœ¬é«˜å’Œæ ‡æ³¨ä¸ç²¾ç¡®ç­‰é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªåŸºäºåˆæˆå¤šæ¨¡æ€çš„æ— äººæœºå¤šä»»åŠ¡æ•°æ®é›†UEMM-Airã€‚é€šè¿‡æ¨¡æ‹Ÿå¤šç§æ— äººæœºé£è¡Œåœºæ™¯å’Œç‰©ä½“ç±»å‹ï¼Œè‡ªåŠ¨æ”¶é›†æ•°æ®å¹¶è®¾è®¡å¯å‘å¼è‡ªåŠ¨æ ‡æ³¨ç®—æ³•ç”Ÿæˆç²¾ç¡®çš„ç›®æ ‡æ£€æµ‹æ ‡ç­¾ã€‚UEMM-AiråŒ…å«12ä¸‡å¯¹å›¾åƒæ•°æ®ï¼Œå…±6ç§æ¨¡æ€å’Œç²¾ç¡®æ ‡æ³¨ã€‚å®éªŒè¡¨æ˜ï¼Œé¢„è®­ç»ƒäºUEMM-Airçš„æ¨¡å‹åœ¨ä¸‹æ¸¸ä»»åŠ¡ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œè¯¥æ•°æ®é›†å·²å…¬å¼€ä¾›ç ”ç©¶ä½¿ç”¨ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>UEMM-Airæ˜¯ä¸€ä¸ªåŸºäºåˆæˆå¤šæ¨¡æ€çš„æ— äººæœºå¤šä»»åŠ¡æ•°æ®é›†ã€‚</li>
<li>é€šè¿‡Unreal Engineæ¨¡æ‹Ÿå¤šç§æ— äººæœºé£è¡Œåœºæ™¯å’Œç‰©ä½“ç±»å‹ã€‚</li>
<li>è®¾è®¡äº†è‡ªåŠ¨æ”¶é›†æ•°æ®çš„æ— äººæœºé£è¡Œé€»è¾‘ï¼Œæ¶µç›–ä¸åŒåœºæ™¯ã€è§†è§’å’Œé«˜åº¦ã€‚</li>
<li>æå‡ºå¯å‘å¼è‡ªåŠ¨æ ‡æ³¨ç®—æ³•ç”Ÿæˆç²¾ç¡®çš„ç›®æ ‡æ£€æµ‹æ ‡ç­¾ã€‚</li>
<li>UEMM-AiråŒ…å«12ä¸‡å¯¹å›¾åƒæ•°æ®ï¼Œå…±6ç§æ¨¡æ€å’Œç²¾ç¡®æ ‡æ³¨ã€‚</li>
<li>åœ¨UEMM-Airä¸Šé¢„è®­ç»ƒçš„æ¨¡å‹åœ¨ä¸‹æ¸¸ä»»åŠ¡ä¸Šè¡¨ç°ä¼˜å¼‚ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2406.06230">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-ecc5a2b63abe41724fa1ac8438e80570.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-44c0263b792322a7b00f9073af6f788f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-313a7bce748334ba8d0fdb148fb9ef23.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ecec2682c3915e4848d0b28f303395f7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e731c62b8dcf9e0a12f828d4cbc68bdf.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-932d055f5fa4d0ad49473ce355508a25.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-02-13/%E6%A3%80%E6%B5%8B_%E5%88%86%E5%89%B2_%E8%B7%9F%E8%B8%AA/">https://kedreamix.github.io/Talk2Paper/Paper/2025-02-13/%E6%A3%80%E6%B5%8B_%E5%88%86%E5%89%B2_%E8%B7%9F%E8%B8%AA/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/%E6%A3%80%E6%B5%8B-%E5%88%86%E5%89%B2-%E8%B7%9F%E8%B8%AA/">
                                    <span class="chip bg-color">æ£€æµ‹/åˆ†å‰²/è·Ÿè¸ª</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-02-13/%E6%97%A0%E7%9B%91%E7%9D%A3_%E5%8D%8A%E7%9B%91%E7%9D%A3_%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-29ac032dd825c1be24c383218b877c84.jpg" class="responsive-img" alt="æ— ç›‘ç£/åŠç›‘ç£/å¯¹æ¯”å­¦ä¹ ">
                        
                        <span class="card-title">æ— ç›‘ç£/åŠç›‘ç£/å¯¹æ¯”å­¦ä¹ </span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            æ— ç›‘ç£/åŠç›‘ç£/å¯¹æ¯”å­¦ä¹  æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-02-13  Supervised contrastive learning for cell stage classification of animal   embryos
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-02-13
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E6%97%A0%E7%9B%91%E7%9D%A3-%E5%8D%8A%E7%9B%91%E7%9D%A3-%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0/" class="post-category">
                                    æ— ç›‘ç£/åŠç›‘ç£/å¯¹æ¯”å­¦ä¹ 
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E6%97%A0%E7%9B%91%E7%9D%A3-%E5%8D%8A%E7%9B%91%E7%9D%A3-%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0/">
                        <span class="chip bg-color">æ— ç›‘ç£/åŠç›‘ç£/å¯¹æ¯”å­¦ä¹ </span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-02-13/Vision%20Transformer/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-edeb99732cb1da4e93c800a291f4cf89.jpg" class="responsive-img" alt="Vision Transformer">
                        
                        <span class="card-title">Vision Transformer</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Vision Transformer æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-02-13  ViLa-MIL Dual-scale Vision-Language Multiple Instance Learning for   Whole Slide Image Classification
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-02-13
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Vision-Transformer/" class="post-category">
                                    Vision Transformer
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Vision-Transformer/">
                        <span class="chip bg-color">Vision Transformer</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">28051.5k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
