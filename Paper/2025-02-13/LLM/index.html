<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="LLM">
    <meta name="description" content="LLM 方向最新论文已更新，请持续关注 Update in 2025-02-13  Examining Multilingual Embedding Models Cross-Lingually Through   LLM-Generated Adversarial Examples">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>LLM | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loader页面消失采用渐隐的方式*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logo出现动画 */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//使用渐隐的方法淡出loading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//强制显示loading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">嘘~  正在从服务器偷取页面 . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>首页</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>标签</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>分类</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>归档</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="搜索" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="深色/浅色模式" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			首页
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			标签
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			分类
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			归档
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pic1.zhimg.com/v2-e3ea5af80ebe75832fea69e3b3e9326b.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">LLM</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- 文章内容详情 -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/LLM/">
                                <span class="chip bg-color">LLM</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/LLM/" class="post-category">
                                LLM
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp;
                    2025-02-13
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>更新日期:&nbsp;&nbsp;
                    2025-02-22
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>文章字数:&nbsp;&nbsp;
                    18.7k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>阅读时长:&nbsp;&nbsp;
                    76 分
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>阅读次数:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>⚠️ 以下所有内容总结都来自于 大语言模型的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p>
</blockquote>
<h1 id="2025-02-13-更新"><a href="#2025-02-13-更新" class="headerlink" title="2025-02-13 更新"></a>2025-02-13 更新</h1><h2 id="Examining-Multilingual-Embedding-Models-Cross-Lingually-Through-LLM-Generated-Adversarial-Examples"><a href="#Examining-Multilingual-Embedding-Models-Cross-Lingually-Through-LLM-Generated-Adversarial-Examples" class="headerlink" title="Examining Multilingual Embedding Models Cross-Lingually Through   LLM-Generated Adversarial Examples"></a>Examining Multilingual Embedding Models Cross-Lingually Through   LLM-Generated Adversarial Examples</h2><p><strong>Authors:Andrianos Michail, Simon Clematide, Rico Sennrich</strong></p>
<p>The evaluation of cross-lingual semantic search capabilities of models is often limited to existing datasets from tasks such as information retrieval and semantic textual similarity. To allow for domain-specific evaluation, we introduce Cross Lingual Semantic Discrimination (CLSD), a novel cross-lingual semantic search task that requires only a set of parallel sentence pairs of the language pair of interest within the target domain. This task focuses on the ability of a model to cross-lingually rank the true parallel sentence higher than hard negatives generated by a large language model. We create four instances of our introduced CLSD task for the language pair German-French within the domain of news. Within this case study, we find that models that are also fine-tuned for retrieval tasks (e.g., multilingual E5) benefit from using English as the pivot language, while bitext mining models such as LaBSE perform best directly cross-lingually. We also show a fine-grained similarity analysis enabled by our distractor generation strategy, indicating that different embedding models are sensitive to different types of perturbations. </p>
<blockquote>
<p>对模型的跨语言语义搜索能力的评估通常仅限于信息检索和语义文本相似性之类的现有数据集。为了进行特定领域的评估，我们引入了跨语言语义辨别（CLSD），这是一种新的跨语言语义搜索任务，它只需要目标领域内感兴趣语言对的并行句子对集合。此任务侧重于模型将真正的并行句子排名高于由大型语言模型生成硬负样本的跨语言能力。我们为新闻领域的德法语言对创建了四个CLSD任务的实例。在此案例研究中，我们发现进行检索任务微调（例如多语言E5）的模型受益于使用英语作为中心语言，而直接进行跨语言处理的双文本挖掘模型（如LaBSE）表现最佳。我们还通过干扰项生成策略展示了精细的相似性分析，这表明不同的嵌入模型对不同类型的扰动有不同的敏感性。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.08638v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文介绍了跨语言语义搜索能力模型评估的新挑战。为应对领域特定的评估需求，提出了跨语言语义判别（CLSD）这一新任务。CLSD任务只需针对目标领域内的语言对设置平行句子对即可。研究发现，针对检索任务进行微调（如多语种E5）的模型使用英语作为中介语言受益较大，而直接进行跨语言处理的模型（如LaBSE）表现最佳。此外，本文通过干扰项生成策略展示了精细的相似性分析，表明不同的嵌入模型对不同类型的扰动有不同的敏感性。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>跨语言语义搜索能力模型的评估受限于现有数据集和信息检索等任务。</li>
<li>提出了一种新的跨语言语义判别（CLSD）任务，只需设置平行句子对即可进行领域特定评估。</li>
<li>在新闻领域的德语-法语语言对实例中，发现针对检索任务进行微调（如多语种E5）的模型在使用英语作为中介语言时表现出优势。</li>
<li>直接进行跨语言处理的模型（如LaBSE）表现最佳。</li>
<li>CLSD任务能够展示精细的相似性分析，这有助于理解不同嵌入模型的敏感性。</li>
<li>不同嵌入模型对不同类型的扰动反应不同，这为改进模型提供了方向。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.08638">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-d9147a8eddc33c146e614e22f88cd032.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-74b897af1638737523f1e8c770d9a742.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-14c24d088220547eb70338a676daa736.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-cb2b206a9ac9ac5d2bf28035feddf60a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a71c31b86eee2baf36aa5ce21701dea7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d8b51c942c1718ec3952856ab83489ac.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="QA-Expand-Multi-Question-Answer-Generation-for-Enhanced-Query-Expansion-in-Information-Retrieval"><a href="#QA-Expand-Multi-Question-Answer-Generation-for-Enhanced-Query-Expansion-in-Information-Retrieval" class="headerlink" title="QA-Expand: Multi-Question Answer Generation for Enhanced Query Expansion   in Information Retrieval"></a>QA-Expand: Multi-Question Answer Generation for Enhanced Query Expansion   in Information Retrieval</h2><p><strong>Authors:Wonduk Seo, Seunghyun Lee</strong></p>
<p>Query expansion is widely used in Information Retrieval (IR) to improve search outcomes by enriching queries with additional contextual information. Although recent Large Language Model (LLM) based methods generate pseudo-relevant content and expanded terms via multiple prompts, they often yield repetitive, narrow expansions that lack the diverse context needed to retrieve all relevant information. In this paper, we introduce QA-Expand, a novel and effective framework for query expansion. It first generates multiple relevant questions from the initial query and subsequently produces corresponding pseudo-answers as surrogate documents. A feedback model further rewrites and filters these answers to ensure only the most informative augmentations are incorporated. Extensive experiments on benchmarks such as BEIR and TREC demonstrate that QA-Expand enhances retrieval performance by up to 13% over state-of-the-art methods, offering a robust solution for modern retrieval challenges. </p>
<blockquote>
<p>查询扩展在信息检索（IR）中得到了广泛应用，通过添加额外的上下文信息来丰富查询，从而改进搜索结果。尽管最近基于大型语言模型（LLM）的方法通过多个提示生成伪相关内容并扩展术语，但它们通常会产生重复性高、范围狭窄的扩展内容，缺乏检索所有相关信息所需的各种上下文。在本文中，我们介绍了QA-Expand，这是一个用于查询扩展的新型有效框架。它首先根据初始查询生成多个相关问题，然后生成相应的伪答案作为替代文档。反馈模型进一步重写和过滤这些答案，以确保只合并最具有信息性的增强内容。在BEIR和TREC等基准测试上的大量实验表明，与最新技术相比，QA-Expand的检索性能提高了高达13%，为现代检索挑战提供了稳健的解决方案。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.08557v1">PDF</a> 8 pages</p>
<p><strong>Summary</strong></p>
<p>基于信息检索中的查询扩展技术，本文提出了一种新的查询扩展框架QA-Expand。该框架通过生成多个相关问题和对应的伪答案作为替代文档来扩展查询，并利用反馈模型重写和过滤答案，确保只融入最具有信息性的扩展内容。实验结果表明，QA-Expand在BEIR和TREC等基准测试上的检索性能提高了高达13%，为解决现代检索挑战提供了稳健的解决方案。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>查询扩展在信息检索中用于通过增加上下文信息提高搜索效果。</li>
<li>最新大型语言模型（LLM）方法虽然能生成伪相关内容和扩展词项，但常常产生重复性高、范围狭窄的扩展，缺乏多样化的上下文信息。</li>
<li>QA-Expand框架通过生成多个相关问题及其伪答案进行查询扩展。</li>
<li>反馈模型用于重写和过滤答案，确保仅融入最具有信息性的内容。</li>
<li>在基准测试如BEIR和TREC上的实验表明，QA-Expand提高了检索性能。</li>
<li>QA-Expand性能优于现有技术，提高幅度高达13%。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.08557">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-7fb07f83c4aaac32d319e0008cbbf86f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1ad65c0c90da3d07853e1d6cdba56bcd.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-156d15db59d2980f2dc5804b3cec291a.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="LLMs-can-implicitly-learn-from-mistakes-in-context"><a href="#LLMs-can-implicitly-learn-from-mistakes-in-context" class="headerlink" title="LLMs can implicitly learn from mistakes in-context"></a>LLMs can implicitly learn from mistakes in-context</h2><p><strong>Authors:Lisa Alazraki, Maximilian Mozes, Jon Ander Campos, Yi Chern Tan, Marek Rei, Max Bartolo</strong></p>
<p>Learning from mistakes is a fundamental feature of human intelligence. Previous work has shown that Large Language Models (LLMs) can also learn from incorrect answers when provided with a comprehensive rationale detailing why an answer is wrong or how to correct it. In this work, we examine whether LLMs can learn from mistakes in mathematical reasoning tasks when these explanations are not provided. We investigate if LLMs are able to implicitly infer such rationales simply from observing both incorrect and correct answers. Surprisingly, we find that LLMs perform better, on average, when rationales are eliminated from the context and incorrect answers are simply shown alongside correct ones. This approach also substantially outperforms chain-of-thought prompting in our evaluations. We show that these results are consistent across LLMs of different sizes and varying reasoning abilities. Further, we carry out an in-depth analysis, and show that prompting with both wrong and correct answers leads to greater performance and better generalisation than introducing additional, more diverse question-answer pairs into the context. Finally, we show that new rationales generated by models that have only observed incorrect and correct answers are scored equally as highly by humans as those produced with the aid of exemplar rationales. Our results demonstrate that LLMs are indeed capable of in-context implicit learning. </p>
<blockquote>
<p>从错误中学习是人类智能的基本特征。之前的研究表明，大型语言模型（LLM）能够在提供全面理由的情况下，详细说明答案为何错误或如何改正，从而从错误的答案中学习。在这项工作中，我们研究的是在没有提供这些解释的情况下，LLM是否能在数学推理任务中从错误中学习。我们调查LLM是否仅通过观察正确的和错误的答案就能隐式推断出这些理由。令人惊讶的是，我们发现，在上下文中消除理由，而只是简单地展示正确的和错误的答案时，LLM的平均表现更好。这种方法也在我们的评估中大大优于思考链提示法。我们证明了这些结果在不同规模和不同推理能力的LLM之间是一致的。此外，我们进行了深入的分析，并证明用正确和错误的答案提示比把更多多样的问题和答案对引入上下文能产生更好的性能和更好的泛化能力。最后，我们证明了只通过观察正确和错误的答案而生成的新理由，与人类辅助示例理由产生的评分相同。我们的结果表明，LLM确实具备上下文隐式学习能力。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.08550v1">PDF</a> </p>
<p><strong>Summary</strong>：</p>
<p>本文探讨了大型语言模型（LLMs）在数学推理任务中能否从错误中学习，而无需提供解释。研究发现，当从上下文中消除解释，仅展示正确和错误的答案时，LLMs的表现更佳。这种方法在评估中显著优于思维链提示。进一步的分析表明，同时呈现正确和错误的答案提示可以提高性能并增强模型的泛化能力，而引入更多的问题答案对则效果不大。最后，仅通过观察正确和错误的答案而生成的模型新解释被人类评价为与有示例解释的同样出色，证明了LLMs确实具备上下文隐式学习能力。</p>
<p><strong>Key Takeaways</strong>：</p>
<ol>
<li>LLMs可以从错误中学习，尤其在数学推理任务中。</li>
<li>当仅展示正确和错误的答案时，LLMs的表现更佳，且这种方法优于思维链提示。</li>
<li>同时呈现正确和错误的答案可以提高LLMs的性能和泛化能力。</li>
<li>引入更多的问题答案对并不一定能提高LLMs的学习效果。</li>
<li>LLMs能够通过观察正确和错误的答案生成新的解释，这些解释被人类评价为与有示例解释的同样出色。</li>
<li>LLMs具备上下文隐式学习能力。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.08550">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-a8840315dcd02ead22184bd60c753f57.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-dc48bbbb95ad0db7ed86180e7db36f17.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e5747c7b0d5e8fc6a61f65aa2c7afe81.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="LLM-Pretraining-with-Continuous-Concepts"><a href="#LLM-Pretraining-with-Continuous-Concepts" class="headerlink" title="LLM Pretraining with Continuous Concepts"></a>LLM Pretraining with Continuous Concepts</h2><p><strong>Authors:Jihoon Tack, Jack Lanchantin, Jane Yu, Andrew Cohen, Ilia Kulikov, Janice Lan, Shibo Hao, Yuandong Tian, Jason Weston, Xian Li</strong></p>
<p>Next token prediction has been the standard training objective used in large language model pretraining. Representations are learned as a result of optimizing for token-level perplexity. We propose Continuous Concept Mixing (CoCoMix), a novel pretraining framework that combines discrete next token prediction with continuous concepts. Specifically, CoCoMix predicts continuous concepts learned from a pretrained sparse autoencoder and mixes them into the model’s hidden state by interleaving with token hidden representations. Through experiments on multiple benchmarks, including language modeling and downstream reasoning tasks, we show that CoCoMix is more sample efficient and consistently outperforms standard next token prediction, knowledge distillation and inserting pause tokens. We find that combining both concept learning and interleaving in an end-to-end framework is critical to performance gains. Furthermore, CoCoMix enhances interpretability and steerability by allowing direct inspection and modification of the predicted concept, offering a transparent way to guide the model’s internal reasoning process. </p>
<blockquote>
<p>下一个词预测作为大型语言模型预训练的标准训练目标已经被广泛应用。表示是通过优化令牌级别的困惑度来学习的。我们提出了连续概念混合（CoCoMix），这是一种新型的预训练框架，结合了离散下一个词预测和连续概念。具体来说，CoCoMix预测从预训练的稀疏自动编码器中学得的连续概念，并通过与令牌隐藏表示交替的方式将它们混合到模型的隐藏状态。通过包括语言建模和下游推理任务等多个基准实验，我们证明了CoCoMix的样本效率更高，并且始终优于标准下一个词预测、知识蒸馏和插入暂停令牌。我们发现在一个端到端的框架中结合概念学习和交替方式对于性能提升至关重要。此外，CoCoMix通过允许直接检查和修改预测的概念，提供了一种透明的方式来引导模型的内部推理过程，从而增强了可解释性和可控制性。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.08524v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文提出了一个新的预训练框架——连续概念混合（CoCoMix），结合了离散下一个令牌预测和连续概念预测。CoCoMix利用预训练的稀疏自动编码器学习连续概念，并通过交错方式与令牌隐藏表示混合到模型的隐藏状态中。实验表明，CoCoMix在多个基准测试中表现出更高的样本效率和性能，优于标准下一个令牌预测、知识蒸馏和插入暂停令牌等方法。结合概念学习和交错在端到端框架中对性能提升至关重要。此外，CoCoMix通过允许直接检查和修改预测的概念，提供了透明的指导模型内部推理过程的方式，增强了模型的解释性和可操控性。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>CoCoMix是一个结合离散下一个令牌预测和连续概念预测的新型预训练框架。</li>
<li>CoCoMix利用预训练的稀疏自动编码器学习连续概念。</li>
<li>实验证明CoCoMix在多个基准测试中样本效率更高，性能更好。</li>
<li>结合概念学习和交错在端到端框架中对性能提升有关键作用。</li>
<li>CoCoMix增强了模型的解释性和可操控性，允许直接检查和修改预测的概念。</li>
<li>CoCoMix通过预测连续概念并混合到模型的隐藏状态中，提高了模型的性能。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.08524">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-f0e9da3de2b74fafc988201a1440c723.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-eec4666a1b32bf097331480d03e4c851.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-cf2442e25cae62866b1ebf54cd233edb.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-dd60156267912356b7279cd228375932.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-aada9a74a8fd543c04099431ada28d17.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4163866fd21a570b74fa91961aab25e7.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Measuring-Diversity-in-Synthetic-Datasets"><a href="#Measuring-Diversity-in-Synthetic-Datasets" class="headerlink" title="Measuring Diversity in Synthetic Datasets"></a>Measuring Diversity in Synthetic Datasets</h2><p><strong>Authors:Yuchang Zhu, Huizhe Zhang, Bingzhe Wu, Jintang Li, Zibin Zheng, Peilin Zhao, Liang Chen, Yatao Bian</strong></p>
<p>Large language models (LLMs) are widely adopted to generate synthetic datasets for various natural language processing (NLP) tasks, such as text classification and summarization. However, accurately measuring the diversity of these synthetic datasets-an aspect crucial for robust model performance-remains a significant challenge. In this paper, we introduce DCScore, a novel method for measuring synthetic dataset diversity from a classification perspective. Specifically, DCScore formulates diversity evaluation as a sample classification task, leveraging mutual relationships among samples. We further provide theoretical verification of the diversity-related axioms satisfied by DCScore, highlighting its role as a principled diversity evaluation method. Experimental results on synthetic datasets reveal that DCScore enjoys a stronger correlation with multiple diversity pseudo-truths of evaluated datasets, underscoring its effectiveness. Moreover, both empirical and theoretical evidence demonstrate that DCScore substantially reduces computational costs compared to existing approaches. Code is available at: <a target="_blank" rel="noopener" href="https://github.com/BlueWhaleLab/DCScore">https://github.com/BlueWhaleLab/DCScore</a>. </p>
<blockquote>
<p>大型语言模型（LLM）被广泛用于生成各种自然语言处理（NLP）任务（如文本分类和摘要）的合成数据集。然而，准确测量这些合成数据集的多样性——对模型稳健性能至关重要的一个方面——仍然是一个巨大的挑战。在本文中，我们介绍了DCScore，这是一种从分类角度衡量合成数据集多样性的新方法。具体来说，DCScore将多样性评估制定为样本分类任务，利用样本之间的相互作用关系。我们还提供了DCScore满足的多样性公理的理论验证，强调了其作为有原则的多样性评估方法的作用。在合成数据集上的实验结果表明，DCScore与评估数据集的多个多样性伪真相具有更强的相关性，这突出了其有效性。此外，实证和理论证据都表明，DCScore与现有方法相比大大减少了计算成本。代码可从以下网址获取：<a target="_blank" rel="noopener" href="https://github.com/BlueWhaleLab/DCScore%E3%80%82">https://github.com/BlueWhaleLab/DCScore。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.08512v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>大型语言模型（LLM）生成的合成数据集广泛应用于自然语言处理（NLP）任务，如文本分类和摘要。然而，准确测量这些合成数据集的多样性对于模型的稳健性能至关重要，仍是一个巨大挑战。本文介绍了一种新的测量合成数据集多样性的方法DCScore，从分类的角度进行评估。DCScore将多样性评估制定为样本分类任务，利用样本间的相互关联。实验结果表明，DCScore与评估数据集的多个多样性伪真实值具有更强的相关性，凸显了其有效性。同时，DCScore在理论和实践证据方面都显著降低了计算成本。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLMs被广泛用于生成合成数据集，用于NLP任务如文本分类和摘要。</li>
<li>合成数据集的多样性测量是一个关键挑战，对模型性能有重要影响。</li>
<li>DCScore是一种新的测量合成数据集多样性的方法，将多样性评估转化为样本分类任务。</li>
<li>DCScore利用样本间的相互关系来评估多样性。</li>
<li>实验证明DCScore与多个多样性伪真实值的相关性更强，表明其有效性。</li>
<li>DCScore在理论和实践两个方面都显著降低了计算成本。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.08512">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-7732b2346ec91e0f57718760fc6e0c94.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d1885554bc64b73bd6dd4ca283f73225.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-31ea643413fc754f045d1d040b8f99c5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-592b7660c08ef2218181b0d8999ed799.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-28969921f16c2568399d8cbe2de8441c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-08b57b6ae0e097c43cb4927383a2adde.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Explanation-based-In-Context-Demonstrations-Retrieval-for-Multilingual-Grammatical-Error-Correction"><a href="#Explanation-based-In-Context-Demonstrations-Retrieval-for-Multilingual-Grammatical-Error-Correction" class="headerlink" title="Explanation based In-Context Demonstrations Retrieval for Multilingual   Grammatical Error Correction"></a>Explanation based In-Context Demonstrations Retrieval for Multilingual   Grammatical Error Correction</h2><p><strong>Authors:Wei Li, Wen Luo, Guangyue Peng, Houfeng Wang</strong></p>
<p>Grammatical error correction (GEC) aims to correct grammatical, spelling, and semantic errors in natural language text. With the growing of large language models (LLMs), direct text generation has gradually become the focus of the GEC methods, and few-shot in-context learning presents a cost-effective solution. However, selecting effective in-context examples remains challenging, as the similarity between input texts does not necessarily correspond to similar grammatical error patterns. In this paper, we propose a novel retrieval method based on natural language grammatical error explanations (GEE) to address this issue. Our method retrieves suitable few-shot demonstrations by matching the GEE of the test input with that of pre-constructed database samples, where explanations for erroneous samples are generated by LLMs. We conducted multilingual GEC few-shot experiments on both major open-source and closed-source LLMs. Experiments across five languages show that our method outperforms existing semantic and BM25-based retrieval techniques, without requiring additional training or language adaptation. This also suggests that matching error patterns is key to selecting examples. </p>
<blockquote>
<p>语法错误修正（GEC）旨在修正自然语言文本中的语法、拼写和语义错误。随着大型语言模型（LLM）的增长，直接文本生成逐渐成为GEC方法的焦点，而少量上下文学习提供了一种具有成本效益的解决方案。然而，选择有效的上下文示例仍然具有挑战性，因为输入文本之间的相似性并不一定对应相似的语法错误模式。在本文中，我们提出了一种基于自然语言语法错误解释（GEE）的新型检索方法来解决这个问题。我们的方法通过匹配测试输入的GEE与预先构建的数据库样本的GEE来检索合适的少量示例，其中错误样本的解释是由LLM生成的。我们在主要的开源和闭源LLM上进行了多语言GEC少量实验。跨越五种语言的实验表明，我们的方法优于现有的语义和基于BM25的检索技术，且无需额外的训练或语言适应。这也表明匹配错误模式是选择示例的关键。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.08507v1">PDF</a> Accepted by NAACL 2025 main conference</p>
<p><strong>Summary</strong><br>文本主要介绍了基于自然语言语法错误解释（GEE）的检索方法在语法错误修正（GEC）中的应用。该方法通过匹配测试输入的GEE与预构建的数据库样本，检索出合适的少量示例。实验表明，该方法在多语言GEC中的表现优于现有的语义和BM25基于检索技术的检索方法，且无需额外的训练或语言适应。这表明匹配错误模式是选择示例的关键。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>语法错误修正（GEC）旨在纠正自然语言文本中的语法、拼写和语义错误。</li>
<li>随着大型语言模型（LLM）的发展，直接文本生成逐渐成为GEC方法的重点。</li>
<li>少样本上下文学习为GEC提供了一个具有成本效益的解决方案。</li>
<li>选择有效的上下文示例仍然具有挑战性，因为输入文本之间的相似性并不一定对应相似的语法错误模式。</li>
<li>提出了一种基于自然语言语法错误解释（GEE）的检索方法来解决这个问题。</li>
<li>该方法通过匹配测试输入的GEE与预构建的数据库样本，检索出合适的少量示例，以进行GEC。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.08507">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-bba5ff79cc04768b1ee6f363fe5d75fc.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-9264654bc5093f0686d4831d9a32b813.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-cba8b3293070385218ad7bba4215f106.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-bcc7da750505da19c6dafe5ee22f44b5.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="mmE5-Improving-Multimodal-Multilingual-Embeddings-via-High-quality-Synthetic-Data"><a href="#mmE5-Improving-Multimodal-Multilingual-Embeddings-via-High-quality-Synthetic-Data" class="headerlink" title="mmE5: Improving Multimodal Multilingual Embeddings via High-quality   Synthetic Data"></a>mmE5: Improving Multimodal Multilingual Embeddings via High-quality   Synthetic Data</h2><p><strong>Authors:Haonan Chen, Liang Wang, Nan Yang, Yutao Zhu, Ziliang Zhao, Furu Wei, Zhicheng Dou</strong></p>
<p>Multimodal embedding models have gained significant attention for their ability to map data from different modalities, such as text and images, into a unified representation space. However, the limited labeled multimodal data often hinders embedding performance. Recent approaches have leveraged data synthesis to address this problem, yet the quality of synthetic data remains a critical bottleneck. In this work, we identify three criteria for high-quality synthetic multimodal data. First, broad scope ensures that the generated data covers diverse tasks and modalities, making it applicable to various downstream scenarios. Second, robust cross-modal alignment makes different modalities semantically consistent. Third, high fidelity ensures that the synthetic data maintains realistic details to enhance its reliability. Guided by these principles, we synthesize datasets that: (1) cover a wide range of tasks, modality combinations, and languages, (2) are generated via a deep thinking process within a single pass of a multimodal large language model, and (3) incorporate real-world images with accurate and relevant texts, ensuring fidelity through self-evaluation and refinement. Leveraging these high-quality synthetic and labeled datasets, we train a multimodal multilingual E5 model mmE5. Extensive experiments demonstrate that mmE5 achieves state-of-the-art performance on the MMEB Benchmark and superior multilingual performance on the XTD benchmark. Our codes, datasets and models are released in <a target="_blank" rel="noopener" href="https://github.com/haon-chen/mmE5">https://github.com/haon-chen/mmE5</a>. </p>
<blockquote>
<p>多模态嵌入模型因其能够将文本和图像等不同模态的数据映射到统一表示空间的能力而受到广泛关注。然而，有限的标记多模态数据通常会影响嵌入性能。最近的方法利用数据合成来解决这个问题，但合成数据的质量仍然是关键瓶颈。在我们的研究中，我们确定了高质量合成多模态数据的三个标准。首先，广泛覆盖要求生成的数据涵盖各种任务和模态，使其适用于各种下游场景。其次，稳健的跨模态对齐使得不同模态在语义上保持一致。第三，高保真度确保合成数据保持现实世界的细节，以提高其可靠性。遵循这些原则，我们合成的数据集包括：（1）涵盖广泛的任务、模态组合和语言；（2）通过单通道多模态大型语言模型的深度思考过程生成；（3）结合现实世界的图像和准确相关的文本，通过自我评估和精炼确保保真度。利用这些高质量合成和标记的数据集，我们训练了一个多模态多语言E5模型mmE5。大量实验表明，mmE5在MMEB基准测试上达到了最先进的性能，并在XTD基准测试上表现出卓越的多语言能力。我们的代码、数据集和模型已在<a target="_blank" rel="noopener" href="https://github.com/haon-chen/mmE5%E4%B8%8A%E5%8F%91%E5%B8%83%E3%80%82">https://github.com/haon-chen/mmE5上发布。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.08468v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文关注多模态嵌入模型在数据映射方面的能力，特别是在处理不同模态数据（如文本和图像）时面临的挑战。针对有限标记多模态数据的问题，研究通过数据合成来解决，同时强调高质量合成数据的重要性。提出三个标准：范围广泛、跨模态对齐和高质量保真。基于这些标准，研究团队合成了一系列数据集并训练了mmE5模型。该模型在MMEB和XTD基准测试中表现出卓越性能。相关代码和数据集已发布在[链接地址]。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>多模态嵌入模型能够将不同模态的数据映射到统一表示空间。</li>
<li>有限的标记多模态数据限制了嵌入模型的性能。</li>
<li>数据合成是解决此问题的一种策略，但合成数据的质量是关键瓶颈。</li>
<li>高质量合成数据的三个标准是：范围广泛、跨模态对齐和高质量保真。</li>
<li>研究团队基于这些标准合成了一系列数据集，并训练了mmE5模型。</li>
<li>mmE5在MMEB和XTD基准测试中表现出卓越性能。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.08468">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-529193948940e795ffcfaba6cf38d591.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0e9492d16887d665286ba3a7e0c89504.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c02f76fc8e5ab12ea89d746596192145.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f6ae11e504487ffa4c478ee9d210c4f3.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a01ddc6ef2945f886948fb31def61874.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="From-Haystack-to-Needle-Label-Space-Reduction-for-Zero-shot-Classification"><a href="#From-Haystack-to-Needle-Label-Space-Reduction-for-Zero-shot-Classification" class="headerlink" title="From Haystack to Needle: Label Space Reduction for Zero-shot   Classification"></a>From Haystack to Needle: Label Space Reduction for Zero-shot   Classification</h2><p><strong>Authors:Nathan Vandemoortele, Bram Steenwinckel, Femke Ongenae, Sofie Van Hoecke</strong></p>
<p>We present Label Space Reduction (LSR), a novel method for improving zero-shot classification performance of Large Language Models (LLMs). LSR iteratively refines the classification label space by systematically ranking and reducing candidate classes, enabling the model to concentrate on the most relevant options. By leveraging unlabeled data with the statistical learning capabilities of data-driven models, LSR dynamically optimizes the label space representation at test time. Our experiments across seven benchmarks demonstrate that LSR improves macro-F1 scores by an average of 7.0% (up to 14.2%) with Llama-3.1-70B and 3.3% (up to 11.1%) with Claude-3.5-Sonnet compared to standard zero-shot classification baselines. To reduce the computational overhead of LSR, which requires an additional LLM call at each iteration, we propose distilling the model into a probabilistic classifier, allowing for efficient inference. </p>
<blockquote>
<p>我们提出了一种名为标签空间缩减（LSR）的新方法，用于提升大型语言模型（LLM）的零样本分类性能。LSR通过系统地排序和缩减候选类别，对分类标签空间进行迭代优化，使模型能够专注于最相关的选项。该方法利用无标签数据以及数据驱动模型的统计学习能力，在测试时动态优化标签空间表示。我们在七个基准测试上的实验表明，与标准的零样本分类基线相比，LSR使用Llama-3.1-70B将宏F1分数平均提高了7.0%（最高达14.2%），使用Claude-3.5-Sonnet提高了3.3%（最高达11.1%）。为了减少LSR的计算开销（需要在每次迭代时进行额外的LLM调用），我们提出将模型蒸馏成概率分类器，以实现高效推理。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.08436v1">PDF</a> Under review at ICML 2025</p>
<p><strong>摘要</strong><br>    本文提出了Label Space Reduction（LSR）这一新方法，旨在提高大型语言模型（LLM）的零样本分类性能。LSR通过系统地对候选类别进行排名和减少，迭代地优化分类标签空间，使模型能够专注于最相关的选项。该方法利用无标签数据的统计学习能力，在测试时动态优化标签空间表示。实验表明，在七个基准测试中，LSR与标准零样本分类基线相比，使用Llama-3.1-70B时的宏观F1分数平均提高了7.0%（最高达14.2%），使用Claude-3.5-Sonnet时提高了3.3%（最高达11.1%）。为了降低LSR的计算开销（每次迭代需要额外的LLM调用），我们提出了将其蒸馏成概率分类器的方法，以实现高效推理。</p>
<p><strong>关键见解</strong></p>
<ol>
<li>LSR是一种提高大型语言模型零样本分类性能的新方法。</li>
<li>LSR通过迭代地优化分类标签空间，提高了模型的关注度和性能。</li>
<li>LSR利用无标签数据的统计学习能力，在测试时动态调整标签空间。</li>
<li>实验表明，LSR在多个基准测试中显著提高了宏观F1分数。</li>
<li>与标准零样本分类基线相比，LSR在Llama-3.1-70B上的性能平均提高了7.0%，在Claude-3.5-Sonnet上提高了3.3%。</li>
<li>LSR方法虽然有效，但需要额外的计算开销。</li>
<li>为了提高推理效率，提出了将模型蒸馏成概率分类器的解决方案。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.08436">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-7a13bb0da4b21ea5ffe9e5ba952544f5.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-60bbf390f899d0c1a5f30c586bc56668.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7f9581087e599ec30d41f247f4396376.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Tractable-Transformers-for-Flexible-Conditional-Generation"><a href="#Tractable-Transformers-for-Flexible-Conditional-Generation" class="headerlink" title="Tractable Transformers for Flexible Conditional Generation"></a>Tractable Transformers for Flexible Conditional Generation</h2><p><strong>Authors:Anji Liu, Xuejie Liu, Dayuan Zhao, Mathias Niepert, Yitao Liang, Guy Van den Broeck</strong></p>
<p>Non-autoregressive (NAR) generative models are valuable because they can handle diverse conditional generation tasks in a more principled way than their autoregressive (AR) counterparts, which are constrained by sequential dependency requirements. Recent advancements in NAR models, such as diffusion language models, have demonstrated superior performance in unconditional generation compared to AR models (e.g., GPTs) of similar sizes. However, such improvements do not always lead to improved conditional generation performance. We show that a key reason for this gap is the difficulty in generalizing to conditional probability queries unseen during training. As a result, strong unconditional generation performance does not guarantee high-quality conditional generation. This paper proposes Tractable Transformers (Tracformer), a Transformer-based generative model that is more robust to different conditional generation tasks. Unlike existing models that rely solely on global contextual features derived from full inputs, Tracformers incorporate a sparse Transformer encoder to capture both local and global contextual information. This information is routed through a decoder for conditional generation. Empirical results demonstrate that Tracformers achieve state-of-the-art conditional generation performance on text modeling compared to recent diffusion and AR model baselines. </p>
<blockquote>
<p>非自回归（NAR）生成模型具有价值，因为它们能够以比自回归（AR）模型更原则化的方式处理多种条件生成任务。自回归模型受到序列依赖性的约束。近期NAR模型的进步，如扩散语言模型，在无条件生成方面表现出了与类似规模的AR模型（如GPT）相比的卓越性能。然而，这些改进并不总是导致条件生成性能的提高。我们表明，这一差距的关键原因是难以推广到训练期间未见过的条件概率查询。因此，强大的无条件生成性能并不保证高质量的条件生成。本文提出了“可处理变换器（Tracformer）”，这是一种基于Transformer的生成模型，对于不同的条件生成任务更具鲁棒性。与现有仅依赖全局上下文特征的模型不同，Tracformers结合稀疏Transformer编码器来捕获局部和全局上下文信息。这些信息通过解码器进行条件生成。经验结果表明，在文本建模方面，Tracformers实现了与最新扩散和AR模型基线相比的最先进条件生成性能。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.07616v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>近期非自回归（NAR）生成模型在处理条件生成任务时展现出优势，尤其是通过扩散语言模型等新技术。尽管在无条件生成方面性能优越，但在条件生成上不一定优于自回归（AR）模型。研究发现，这源于对新条件下概率查询的泛化难度。本文提出Tractable Transformers（Tracformer），结合局部与全局上下文信息，在条件生成上表现卓越。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>非自回归生成模型能更灵活地处理多样化的条件生成任务。</li>
<li>扩散语言模型等非自回归模型在无条件生成方面表现优于类似规模的自回归模型。</li>
<li>仅依赖全局上下文特征的现有模型在条件生成任务上可能存在局限性。</li>
<li>条件生成性能的提升需要模型对新条件下概率查询的泛化能力。</li>
<li>Tractable Transformers结合了局部和全局上下文信息，提高了条件生成的效果。</li>
<li>Tractable Transformers在文本建模上实现了对近期扩散模型和自回归模型基准测试的条件生成性能的最佳表现。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.07616">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-d6b1364a33502fbbc3d9e3fd891d0c03.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-d49e62f2cfd532fe33d95898bcb9f475.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b2e9e67a61b7932e1f6ad7f907587c8a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-9f62cc996a8965e632b61853d01007eb.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="Towards-Zero-Shot-Anomaly-Detection-and-Reasoning-with-Multimodal-Large-Language-Models"><a href="#Towards-Zero-Shot-Anomaly-Detection-and-Reasoning-with-Multimodal-Large-Language-Models" class="headerlink" title="Towards Zero-Shot Anomaly Detection and Reasoning with Multimodal Large   Language Models"></a>Towards Zero-Shot Anomaly Detection and Reasoning with Multimodal Large   Language Models</h2><p><strong>Authors:Jiacong Xu, Shao-Yuan Lo, Bardia Safaei, Vishal M. Patel, Isht Dwivedi</strong></p>
<p>Zero-Shot Anomaly Detection (ZSAD) is an emerging AD paradigm. Unlike the traditional unsupervised AD setting that requires a large number of normal samples to train a model, ZSAD is more practical for handling data-restricted real-world scenarios. Recently, Multimodal Large Language Models (MLLMs) have shown revolutionary reasoning capabilities in various vision tasks. However, the reasoning of image abnormalities remains underexplored due to the lack of corresponding datasets and benchmarks. To facilitate research in AD &amp; reasoning, we establish the first visual instruction tuning dataset, Anomaly-Instruct-125k, and the evaluation benchmark, VisA-D&amp;R. Through investigation with our benchmark, we reveal that current MLLMs like GPT-4o cannot accurately detect and describe fine-grained anomalous details in images. To address this, we propose Anomaly-OneVision (Anomaly-OV), the first specialist visual assistant for ZSAD and reasoning. Inspired by human behavior in visual inspection, Anomaly-OV leverages a Look-Twice Feature Matching (LTFM) mechanism to adaptively select and emphasize abnormal visual tokens. Extensive experiments demonstrate that Anomaly-OV achieves significant improvements over advanced generalist models in both detection and reasoning. Extensions to medical and 3D AD are provided for future study. The link to our project page: <a target="_blank" rel="noopener" href="https://xujiacong.github.io/Anomaly-OV/">https://xujiacong.github.io/Anomaly-OV/</a> </p>
<blockquote>
<p>零样本异常检测（ZSAD）是一种新兴的异常检测范式。与传统的需要大批量正常样本来训练模型的监督型异常检测不同，ZSAD在应对数据受限的实际情况时更具实用性。近期，多模态大型语言模型（MLLMs）在各种视觉任务中展现出革命性的推理能力。然而，由于缺乏相应的数据集和基准测试，图像异常的推理仍然被忽视。为了促进异常检测和推理的研究，我们建立了首个视觉指令调整数据集Anomaly-Instruct-125k和评估基准VisA-D&amp;R。通过我们的基准调查，我们发现当前的MLLMs如GPT-4o无法准确检测和描述图像中的精细异常细节。为了解决这一问题，我们提出了Anomaly-OneVision（Anomaly-OV），这是专为ZSAD和推理设计的首个专业视觉助理。Anomaly-OV受到人类视觉检查行为的启发，利用二次特征匹配（LTFM）机制自适应选择和强调异常的视觉标记。大量实验表明，在检测和推理方面，Anomaly-OV较先进的通用模型有显著改进。还为医学和3D异常检测提供了扩展研究的方向。我们的项目页面链接：<a target="_blank" rel="noopener" href="https://xujiacong.github.io/Anomaly-OV/">https://xujiacong.github.io/Anomaly-OV/</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.07601v1">PDF</a> 19 pages, 10 figures</p>
<p><strong>Summary</strong><br>基于零样本异常检测（ZSAD）的新兴发展趋势，当前面临缺乏训练数据和实际应用场景的挑战。本文通过建立首个视觉指令调整数据集Anomaly-Instruct-125k和评估基准VisA-D&amp;R，推动了对多模态大型语言模型（MLLMs）在异常检测和推理领域的研究。研究表明，现有的MLLMs无法准确检测和描述图像中的细微异常细节。为解决这一问题，本文提出了首个针对ZSAD和推理的专家视觉助手Anomaly-OneVision（Anomaly-OV）。它采用类似人类视觉检查的Look-Twice Feature Matching（LTFM）机制，自适应选择和强调异常的视觉标记。实验证明，Anomaly-OV在检测和推理方面都取得了显著的改进。此外，还为未来的医学和三维异常检测提供了扩展研究。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>零样本异常检测（ZSAD）是处理数据限制现实场景的更实用方法，与传统无监督异常检测设置相比，无需大量正常样本进行模型训练。</li>
<li>建立了首个视觉指令调整数据集Anomaly-Instruct-125k和评估基准VisA-D&amp;R，推动了对多模态大型语言模型（MLLMs）在异常检测和推理方面的应用。</li>
<li>当前MLLMs在图像异常检测方面存在局限性，无法准确识别和描述细微的异常细节。</li>
<li>Anomaly-OneVision（Anomaly-OV）作为一种专家视觉助手，通过采用Look-Twice Feature Matching（LTFM）机制解决了上述问题。</li>
<li>Anomaly-OV在异常检测和推理方面实现了显著改进，通过自适应选择和强调异常的视觉标记来提高性能。</li>
<li>Anomaly-OV的扩展研究包括医学和三维异常检测的潜在应用。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.07601">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-7e6d29caec561a8767f09e44bac6b507.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4ccea24de69adea98ef180bb29ffe6e9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f6e01cacb627a2b67b799a98ea163b17.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-cb102e6ee688632fa83304364091f085.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-22bf350f7a49c94cf29fdd159937bdb0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1c8b27acfed861f4829743ca05ed04ca.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-473fa97609bcb3f7bbebd7e5c65a2810.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="Exoplanet-Transit-Candidate-Identification-in-TESS-Full-Frame-Images-via-a-Transformer-Based-Algorithm"><a href="#Exoplanet-Transit-Candidate-Identification-in-TESS-Full-Frame-Images-via-a-Transformer-Based-Algorithm" class="headerlink" title="Exoplanet Transit Candidate Identification in TESS Full-Frame Images via   a Transformer-Based Algorithm"></a>Exoplanet Transit Candidate Identification in TESS Full-Frame Images via   a Transformer-Based Algorithm</h2><p><strong>Authors:Helem Salinas, Rafael Brahm, Greg Olmschenk, Richard K. Barry, Karim Pichara, Stela Ishitani Silva, Vladimir Araujo</strong></p>
<p>The Transiting Exoplanet Survey Satellite (TESS) is surveying a large fraction of the sky, generating a vast database of photometric time series data that requires thorough analysis to identify exoplanetary transit signals. Automated learning approaches have been successfully applied to identify transit signals. However, most existing methods focus on the classification and validation of candidates, while few efforts have explored new techniques for the search of candidates. To search for new exoplanet transit candidates, we propose an approach to identify exoplanet transit signals without the need for phase folding or assuming periodicity in the transit signals, such as those observed in multi-transit light curves. To achieve this, we implement a new neural network inspired by Transformers to directly process Full Frame Image (FFI) light curves to detect exoplanet transits. Transformers, originally developed for natural language processing, have recently demonstrated significant success in capturing long-range dependencies compared to previous approaches focused on sequential data. This ability allows us to employ multi-head self-attention to identify exoplanet transit signals directly from the complete light curves, combined with background and centroid time series, without requiring prior transit parameters. The network is trained to learn characteristics of the transit signal, like the dip shape, which helps distinguish planetary transits from other variability sources. Our model successfully identified 214 new planetary system candidates, including 122 multi-transit light curves, 88 single-transit and 4 multi-planet systems from TESS sectors 1-26 with a radius &gt; 0.27 $R_{\mathrm{Jupiter}}$, demonstrating its ability to detect transits regardless of their periodicity. </p>
<blockquote>
<p>凌日系外行星探测卫星（TESS）正在对天空进行大规模勘测，生成了大量光度时间序列数据，需要深入分析以识别系外行星凌日信号。自动化学习方法已成功应用于识别凌日信号。然而，大多数现有方法都集中在候选对象的分类和验证上，而很少有人探索寻找候选对象的新技术。为了寻找新的系外行星凌日候选对象，我们提出了一种无需相位折叠或假设凌日信号周期性的方法来识别系外行星凌日信号，例如多凌日光度曲线中所观察到的信号。为了实现这一点，我们受到Transformer启发的神经网络直接处理全帧图像（FFI）光度曲线来检测系外行星凌日现象。Transformer最初是为自然语言处理而开发的，最近的研究表明，与以前专注于顺序数据的方法相比，它在捕获长期依赖关系方面取得了显著的成功。这种能力使我们能够利用多头自注意力机制直接从完整的光度曲线中识别出系外行星凌日信号，并结合背景和质心时间序列，无需预先设定的凌日参数。网络经过训练，学习凌日信号的特征，如凹陷形状，这有助于区分行星凌日和其他变源。我们的模型成功识别了214个新的行星系统候选对象，包括122个多凌日光度曲线、88个单凌日和4个多行星系统，这些系统来自TESS的1-26个区域，其半径大于0.27个木星半径，证明了其检测凌日的能力，无论其周期性如何。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.07542v1">PDF</a> </p>
<p><strong>Summary</strong><br>     本研究使用受Transformer启发的神经网络直接处理全帧图像（FFI）光变曲线，无需相位折叠或假设周期性，即可检测行星过境信号。该方法成功识别了TESS扇区1-26中的214个新行星系统候选者，证明了其检测周期性不论的过境能力。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>TESS正在开展大规模天空调查，产生大量光度时间序列数据，需要深入分析以识别行星过境信号。</li>
<li>目前大多数方法集中在候选者的分类和验证上，很少有探索新方法寻找候选者。</li>
<li>研究提出了一种新的神经网络方法，直接处理全帧图像（FFI）光变曲线来检测行星过境信号，无需相位折叠或假设周期性。</li>
<li>该方法使用Transformer原理，能够捕捉长时间依赖性，成功识别出新的行星系统候选者。</li>
<li>该网络被训练学习过境信号的特征，如下降形状，有助于区分行星过境和其他变化源。</li>
<li>研究从TESS扇区1-26中成功识别了214个新的行星系统候选者，包括多过春光变曲线、单过光变和多重行星系统。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.07542">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-4fbb351f5632dd5a6c31c5cac18c786c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-751a0f349e754c4a8b2280ad20a413b4.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-74c6938214d33c1513b62e4ae30a8d27.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="Optimizing-Knowledge-Distillation-in-Transformers-Enabling-Multi-Head-Attention-without-Alignment-Barriers"><a href="#Optimizing-Knowledge-Distillation-in-Transformers-Enabling-Multi-Head-Attention-without-Alignment-Barriers" class="headerlink" title="Optimizing Knowledge Distillation in Transformers: Enabling Multi-Head   Attention without Alignment Barriers"></a>Optimizing Knowledge Distillation in Transformers: Enabling Multi-Head   Attention without Alignment Barriers</h2><p><strong>Authors:Zhaodong Bing, Linze Li, Jiajun Liang</strong></p>
<p>Knowledge distillation (KD) in transformers often faces challenges due to misalignment in the number of attention heads between teacher and student models. Existing methods either require identical head counts or introduce projectors to bridge dimensional gaps, limiting flexibility and efficiency. We propose Squeezing-Heads Distillation (SHD), a novel approach that enables seamless knowledge transfer between models with varying head counts by compressing multi-head attention maps via efficient linear approximation. Unlike prior work, SHD eliminates alignment barriers without additional parameters or architectural modifications. Our method dynamically approximates the combined effect of multiple teacher heads into fewer student heads, preserving fine-grained attention patterns while reducing redundancy. Experiments across language (LLaMA, GPT) and vision (DiT, MDT) generative and vision (DeiT) discriminative tasks demonstrate SHD’s effectiveness: it outperforms logit-based and feature-alignment KD baselines, achieving state-of-the-art results in image classification, image generation language fine-tuning, and language pre-training. The key innovations of flexible head compression, projector-free design, and linear-time complexity make SHD a versatile and scalable solution for distilling modern transformers. This work bridges a critical gap in KD, enabling efficient deployment of compact models without compromising performance. </p>
<blockquote>
<p>知识蒸馏（KD）在转换器（transformer）中常因教师和学生在注意力头数量上的不匹配而面临挑战。现有方法要么要求头数相同，要么引入投影器来弥补维度差距，从而限制了灵活性和效率。我们提出了挤压头蒸馏法（SHD），这是一种新型方法，通过有效的线性近似压缩多头注意力图，实现在不同头数的模型之间进行无缝知识转移。不同于以前的工作，SHD消除了对齐障碍，无需额外的参数或架构修改。我们的方法动态地将多个教师头的综合效应近似为少数学生头，既保留了精细的注意力模式又减少了冗余。在语言生成（LLaMA、GPT）和视觉生成（DiT、MDT）以及视觉判别（DeiT）任务上的实验表明SHD的有效性：它超越了基于对数概率和教师头特征的基准知识蒸馏方法，在图像分类、图像生成语言微调以及语言预训练方面取得了最先进的成果。头压缩灵活性、无投影器设计以及线性时间复杂度等关键创新使SHD成为一种适用于现代转换器蒸馏的通用和可扩展解决方案。这项工作解决了KD中的一个关键空白，能够实现紧凑模型的高效部署，而不损害性能。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.07436v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文提出了一个新的知识蒸馏（KD）方法——挤压头蒸馏（SHD），该方法能够在注意力头数量不同的模型间实现无缝知识迁移。通过高效线性近似压缩多头注意力图，SHD能够消除对齐障碍，无需额外的参数或架构修改。实验表明，SHD在跨语言生成和视觉判别任务上表现出卓越性能，实现了图像分类、图像生成语言微调以及语言预训练的最先进结果。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>SHD解决了知识蒸馏中由于教师模型和学生模型注意力头数量不匹配带来的挑战。</li>
<li>通过压缩多头注意力图，SHD实现了不同模型间的无缝知识迁移。</li>
<li>SHD采用高效线性近似，无需额外的参数或架构修改。</li>
<li>SHD在跨语言生成和视觉判别任务上表现出卓越性能。</li>
<li>SHD在图像分类、图像生成语言微调以及语言预训练领域实现了最先进结果。</li>
<li>SHD具有灵活的头压缩能力，使其成为知识蒸馏中通用且可伸缩的解决方案。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.07436">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-f9e7930cdeee67681bca2978561daeb5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2d2717f30b2ba21860dfd6f5c35455ed.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-99a4d01c0d286da05c8022633f188695.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="On-Iterative-Evaluation-and-Enhancement-of-Code-Quality-Using-GPT-4o"><a href="#On-Iterative-Evaluation-and-Enhancement-of-Code-Quality-Using-GPT-4o" class="headerlink" title="On Iterative Evaluation and Enhancement of Code Quality Using GPT-4o"></a>On Iterative Evaluation and Enhancement of Code Quality Using GPT-4o</h2><p><strong>Authors:Rundong Liu, Andre Frade, Amal Vaidya, Maxime Labonne, Marcus Kaiser, Bismayan Chakrabarti, Jonathan Budd, Sean Moran</strong></p>
<p>This paper introduces CodeQUEST, a novel framework leveraging Large Language Models (LLMs) to iteratively evaluate and enhance code quality across multiple dimensions, including readability, maintainability, efficiency, and security. The framework is divided into two main components: an Evaluator that assesses code quality across ten dimensions, providing both quantitative scores and qualitative summaries, and an Optimizer that iteratively improves the code based on the Evaluator’s feedback. Our study demonstrates that CodeQUEST can effectively and robustly evaluate code quality, with its assessments aligning closely with established code quality metrics. Through a series of experiments using a curated dataset of Python and JavaScript examples, CodeQUEST demonstrated significant improvements in code quality, achieving a mean relative percentage improvement of 52.6%. The framework’s evaluations were validated against a set of proxy metrics comprising of Pylint Score, Radon Maintainability Index, and Bandit output logs, showing a meaningful correlation. This highlights the potential of LLMs in automating code quality evaluation and improvement processes, presenting a significant advancement toward enhancing software development practices. The code implementation of the framework is available at: <a target="_blank" rel="noopener" href="https://github.com/jpmorganchase/CodeQuest">https://github.com/jpmorganchase/CodeQuest</a>. </p>
<blockquote>
<p>本文介绍了CodeQUEST，这是一个新型框架，利用大型语言模型（LLM）来多维度评估和提升代码质量，包括可读性、可维护性、效率和安全性。该框架分为两个主要组成部分：评估器，用于在十个维度上评估代码质量，提供定量评分和定性摘要；优化器，则基于评估器的反馈来迭代改进代码。我们的研究表明，CodeQUEST能够有效且稳定地评估代码质量，其评估结果与既定的代码质量指标高度吻合。通过一系列使用精选的Python和JavaScript示例数据集进行的实验，CodeQUEST在代码质量方面取得了显著的提升，平均相对百分比提升了52.6%。该框架的评估结果经过一组代理指标的验证，包括Pylint评分、Radon可维护性指数和Bandit输出日志，显示出有意义的相关性。这突显了大型语言模型在自动化代码质量评估和改进过程中的潜力，为改进软件开发实践带来了重大进展。该框架的代码实现可在<a target="_blank" rel="noopener" href="https://github.com/jpmorganchase/CodeQuest%E8%8E%B7%E5%BE%97%E3%80%82">https://github.com/jpmorganchase/CodeQuest获得。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.07399v1">PDF</a> </p>
<p><strong>Summary</strong><br>代码质量评估与提升框架CodeQUEST介绍。该框架利用大型语言模型（LLM）从可读性、可维护性、效率和安全性等多个维度评估代码质量，并提供反馈进行优化。实验表明，CodeQUEST能有效提升代码质量，平均相对提升率达52.6%。该框架的潜力在于能自动化代码质量评估和提升过程，有助于改进软件开发实践。</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>CodeQUEST是一个利用大型语言模型（LLM）评估和提升代码质量的框架。</li>
<li>框架包含两个主要组件：Evaluator用于多维度评估代码质量，Optimizer则基于反馈优化代码。</li>
<li>CodeQUEST能有效评估代码质量，与现有代码质量指标高度一致。</li>
<li>实验表明，CodeQUEST能显著提升代码质量，平均相对提升率达52.6%。</li>
<li>框架通过自动化代码质量评估和提升过程，有助于改进软件开发实践。</li>
<li>CodeQUEST已经实现了代码实施框架，并可通过特定链接获取。</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.07399">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-f71a8927217ade82f3e03fbe770bed91.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-aa4b3edaad4dbd663acd613517c65aa8.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="Space-Aware-Instruction-Tuning-Dataset-and-Benchmark-for-Guide-Dog-Robots-Assisting-the-Visually-Impaired"><a href="#Space-Aware-Instruction-Tuning-Dataset-and-Benchmark-for-Guide-Dog-Robots-Assisting-the-Visually-Impaired" class="headerlink" title="Space-Aware Instruction Tuning: Dataset and Benchmark for Guide Dog   Robots Assisting the Visually Impaired"></a>Space-Aware Instruction Tuning: Dataset and Benchmark for Guide Dog   Robots Assisting the Visually Impaired</h2><p><strong>Authors:ByungOk Han, Woo-han Yun, Beom-Su Seo, Jaehong Kim</strong></p>
<p>Guide dog robots offer promising solutions to enhance mobility and safety for visually impaired individuals, addressing the limitations of traditional guide dogs, particularly in perceptual intelligence and communication. With the emergence of Vision-Language Models (VLMs), robots are now capable of generating natural language descriptions of their surroundings, aiding in safer decision-making. However, existing VLMs often struggle to accurately interpret and convey spatial relationships, which is crucial for navigation in complex environments such as street crossings. We introduce the Space-Aware Instruction Tuning (SAIT) dataset and the Space-Aware Benchmark (SA-Bench) to address the limitations of current VLMs in understanding physical environments. Our automated data generation pipeline focuses on the virtual path to the destination in 3D space and the surroundings, enhancing environmental comprehension and enabling VLMs to provide more accurate guidance to visually impaired individuals. We also propose an evaluation protocol to assess VLM effectiveness in delivering walking guidance. Comparative experiments demonstrate that our space-aware instruction-tuned model outperforms state-of-the-art algorithms. We have fully open-sourced the SAIT dataset and SA-Bench, along with the related code, at <a target="_blank" rel="noopener" href="https://github.com/byungokhan/Space-awareVLM">https://github.com/byungokhan/Space-awareVLM</a> </p>
<blockquote>
<p>导航犬机器人对增强视障人士的机动性和安全性提供了有前景的解决方案，解决了传统导航犬在感知智能和沟通方面的局限。随着视觉语言模型（VLMs）的出现，机器人现在能够生成周围环境的自然语言描述，有助于做出更安全的决策。然而，现有的VLMs在解读和传达空间关系方面常常遇到困难，这在复杂环境（如十字路口）的导航中至关重要。为了解决当前VLMs在理解物理环境方面的局限性，我们推出了空间感知指令调优（SAIT）数据集和空间感知基准测试（SA-Bench）。我们的自动化数据生成管道专注于以目的地为中心的虚拟路径和空间周围环境，增强对环境的理解，使VLMs能够为视障人士提供更准确的导航。我们还提出了一个评估协议，以评估VLM在提供步行指导方面的有效性。对比实验表明，我们的空间感知指令调优模型优于最新算法。我们已在<a target="_blank" rel="noopener" href="https://github.com/byungokhan/Space-awareVLM%E4%B8%8A%E5%AE%8C%E5%85%A8%E5%BC%80%E6%BA%90%E4%BA%86SAIT%E6%95%B0%E6%8D%AE%E9%9B%86%E5%92%8CSA-Bench%E4%BB%A5%E5%8F%8A%E7%9B%B8%E5%85%B3%E4%BB%A3%E7%A0%81%E3%80%82">https://github.com/byungokhan/Space-awareVLM上完全开源了SAIT数据集和SA-Bench以及相关代码。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.07183v2">PDF</a> ICRA 2025</p>
<p><strong>Summary</strong></p>
<p>该指南犬机器人提供了增强视力障碍者行动能力和安全性的解决方案，解决了传统指南犬在感知智能和沟通方面的局限。借助视觉语言模型（VLMs）的兴起，机器人可以生成周围环境中的自然语言描述，辅助做出更安全的决策。然而，现有的VLMs在解读和传达空间关系方面存在困难，这对在街道交叉口等复杂环境中的导航至关重要。为解决当前VLMs在理解物理环境方面的局限性，我们推出了空间感知指令调优（SAIT）数据集和空间感知基准测试（SA-Bench）。我们的自动化数据生成管道侧重于虚拟目标路径的三维空间及周围环境，提升环境理解能力，使VLMs能为视力障碍者提供更准确的指导。我们还提出了一个评估协议来评估VLM在提供步行指导方面的有效性。对比实验表明，我们的空间感知指令调优模型优于目前最先进的算法。我们已在<a target="_blank" rel="noopener" href="https://github.com/byungokhan/Space-awareVLM%E4%B8%8A%E5%AE%8C%E5%85%A8%E5%BC%80%E6%BA%90%E4%BA%86SAIT%E6%95%B0%E6%8D%AE%E9%9B%86%E5%92%8CSA-Bench%E5%8F%8A%E7%9B%B8%E5%85%B3%E4%BB%A3%E7%A0%81%E3%80%82">https://github.com/byungokhan/Space-awareVLM上完全开源了SAIT数据集和SA-Bench及相关代码。</a></p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>指南犬机器人通过视觉语言模型（VLMs）提高视力障碍者的行动能力。</li>
<li>现有VLMs在解读和传达空间关系方面存在挑战，影响导航准确性。</li>
<li>为解决这一问题，推出了空间感知指令调优（SAIT）数据集和空间感知基准测试（SA-Bench）。</li>
<li>自动化数据生成管道集中于虚拟路径及其三维空间环境，强化环境理解。</li>
<li>开放源码的SAIT数据集和SA-Bench旨在促进VLM在导航领域的研究进展。</li>
<li>提出的评估协议可有效评估VLM在提供步行指导方面的性能。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.07183">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-b6207ba259c49579d63146b6fcaf12cd.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-cfc8a70569188536199de24e3aa82b1d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c20adcd91546917732b762dc5d881682.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7dce883e0055d85116841529a4c20749.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-70b01c56aff8f78903316a15a987c59d.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="Digital-Twin-Buildings-3D-Modeling-GIS-Integration-and-Visual-Descriptions-Using-Gaussian-Splatting-ChatGPT-Deepseek-and-Google-Maps-Platform"><a href="#Digital-Twin-Buildings-3D-Modeling-GIS-Integration-and-Visual-Descriptions-Using-Gaussian-Splatting-ChatGPT-Deepseek-and-Google-Maps-Platform" class="headerlink" title="Digital Twin Buildings: 3D Modeling, GIS Integration, and Visual   Descriptions Using Gaussian Splatting, ChatGPT&#x2F;Deepseek, and Google Maps   Platform"></a>Digital Twin Buildings: 3D Modeling, GIS Integration, and Visual   Descriptions Using Gaussian Splatting, ChatGPT&#x2F;Deepseek, and Google Maps   Platform</h2><p><strong>Authors:Kyle Gao, Dening Lu, Liangzhi Li, Nan Chen, Hongjie He, Linlin Xu, Jonathan Li</strong></p>
<p>Urban digital twins are virtual replicas of cities that use multi-source data and data analytics to optimize urban planning, infrastructure management, and decision-making. Towards this, we propose a framework focused on the single-building scale. By connecting to cloud mapping platforms such as Google Map Platforms APIs, by leveraging state-of-the-art multi-agent Large Language Models data analysis using ChatGPT(4o) and Deepseek-V3&#x2F;R1, and by using our Gaussian Splatting-based mesh extraction pipeline, our Digital Twin Buildings framework can retrieve a building’s 3D model, visual descriptions, and achieve cloud-based mapping integration with large language model-based data analytics using a building’s address, postal code, or geographic coordinates. </p>
<blockquote>
<p>城市数字双胞胎是利用多源数据和数据分析优化城市规划、基础设施管理和决策制定的城市虚拟副本。为此，我们提出了一个以单栋建筑规模为重点的框架。通过连接到谷歌地图平台API等云地图平台，利用最新的多智能体大型语言模型，使用ChatGPT（4o）和Deepseek-V3&#x2F;R1进行数据分析，以及利用我们的基于高斯喷涂技术的网格提取管道，我们的数字双胞胎建筑框架可以检索建筑的3D模型和视觉描述，并实现基于云的映射集成，通过大型语言模型数据分析与建筑地址、邮政编码或地理坐标相结合。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.05769v2">PDF</a> -Fixed minor typo</p>
<p><strong>Summary</strong><br>城市数字双胞胎是城市的虚拟副本，利用多源数据和数据分析优化城市规划、基础设施管理和决策制定。我们提出一个专注于单体建筑尺度的框架，通过连接云计算地图平台、利用最新的多智能体大数据模型和算法，实现数字双胞胎建筑的云映射集成，能检索建筑物的三维模型、视觉描述等。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>城市数字双胞胎是城市的虚拟副本，使用多源数据和数据分析优化城市规划和管理。</li>
<li>提出的框架专注于单体建筑尺度。</li>
<li>通过连接云计算地图平台获取建筑物的三维模型和视觉描述。</li>
<li>利用最新的多智能体大数据模型和算法，如ChatGPT和Deepseek-V3&#x2F;R1，进行数据分析。</li>
<li>数字双胞胎建筑框架可以通过建筑地址、邮政编码或地理坐标实现云映射集成。</li>
<li>该框架有助于优化决策制定，提升城市规划、基础设施管理的效率和准确性。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.05769">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-44afc3d2104c4d113003fd2f6c012be3.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ab7856b3c2d207d8e26d17ab282007ea.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-511989bdfc99fa44c3216465d43b8839.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f687c699bb65931360517387d7fae53f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-093fb16992977cba99aedd67067aeafb.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-cee475c836077e72d45aa97380cb700a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-fd69bacfb0b9c79536b9551b77ae9acc.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e3ea5af80ebe75832fea69e3b3e9326b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d83d478cf20703a2b1160e7153b264f1.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="ARR-Question-Answering-with-Large-Language-Models-via-Analyzing-Retrieving-and-Reasoning"><a href="#ARR-Question-Answering-with-Large-Language-Models-via-Analyzing-Retrieving-and-Reasoning" class="headerlink" title="ARR: Question Answering with Large Language Models via Analyzing,   Retrieving, and Reasoning"></a>ARR: Question Answering with Large Language Models via Analyzing,   Retrieving, and Reasoning</h2><p><strong>Authors:Yuwei Yin, Giuseppe Carenini</strong></p>
<p>Large language models (LLMs) achieve remarkable performance on challenging benchmarks that are often structured as multiple-choice question-answering (QA) tasks. Zero-shot Chain-of-Thought (CoT) prompting enhances reasoning in LLMs but provides only vague and generic guidance (“think step by step”). This paper introduces ARR, an intuitive and effective zero-shot prompting method that explicitly incorporates three key steps in QA solving: analyzing the intent of the question, retrieving relevant information, and reasoning step by step. Comprehensive experiments across diverse and challenging QA tasks demonstrate that ARR consistently improves the Baseline (without ARR prompting) and outperforms CoT. Ablation and case studies further validate the positive contributions of each component: analyzing, retrieving, and reasoning. Notably, intent analysis plays a vital role in ARR. Additionally, extensive evaluations across various model sizes, LLM series, and generation settings solidify the effectiveness, robustness, and generalizability of ARR. </p>
<blockquote>
<p>大型语言模型（LLM）在具有挑战性的基准测试中表现出卓越的性能，这些测试通常被构建为多选问答（QA）任务。零样本思维链（CoT）提示增强了LLM中的推理能力，但只提供了模糊和通用的指导（“逐步思考”）。本文介绍了一种直观有效的零样本提示方法ARR，该方法显式地结合了问答解决中的三个关键步骤：分析问题的意图、检索相关信息和逐步推理。在多样化和具有挑战性的问答任务上的综合实验表明，ARR始终改进了基线（无ARR提示）并优于CoT。消融研究和案例研究进一步验证了每个组件的正面贡献：分析、检索和推理。值得注意的是，意图分析在ARR中起着至关重要的作用。此外，在不同模型大小、LLM系列和生成设置上的广泛评估巩固了ARR的有效性、稳定性和通用性。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.04689v2">PDF</a> 20 pages. Code: <a target="_blank" rel="noopener" href="https://github.com/YuweiYin/ARR">https://github.com/YuweiYin/ARR</a></p>
<p><strong>Summary</strong></p>
<p>大型语言模型（LLM）在作为多选问答任务的结构化挑战基准测试中表现出卓越性能。零样本思维链（CoT）提示增强了LLM的推理能力，但仅提供模糊和通用的指导。本文介绍了一种直观有效的零样本提示方法ARR，该方法明确结合了问答解决的三个关键步骤：分析问题的意图、检索相关信息和逐步推理。在多样化和具有挑战性的问答任务上的综合实验表明，ARR在基线（无ARR提示）的基础上持续提高了性能，并优于CoT。此外，通过消除研究和案例研究进一步验证了分析、检索和推理每个组件的积极作用。意图分析在ARR中扮演了至关重要的角色。同时，跨不同模型大小、LLM系列和生成设置的广泛评估巩固了ARR的有效性、稳健性和通用性。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLM在多选问答任务的挑战基准测试中表现优秀。</li>
<li>零样本思维链（CoT）提示增强了LLM的推理能力，但指导较为模糊。</li>
<li>ARR是一种新的零样本提示方法，明确结合了QA解决的三个关键步骤：分析、检索和推理。</li>
<li>ARR在多种问答任务上表现出比CoT更好的性能。</li>
<li>消融和案例研究验证了ARR中分析、检索和推理每个组件的有效性。</li>
<li>意图分析在ARR中起关键作用。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.04689">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-26699f54bc94e8d581d7b7c2062886a4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5ebedcb18bb34b74b747361e5af089c5.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-4ec8e7cbca2f2ce219139bdc92787ddd.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-09db45d8497402eb6d1f3c691754d932.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-35694cf858bf7a43795e2f4607148a02.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="Ola-Pushing-the-Frontiers-of-Omni-Modal-Language-Model-with-Progressive-Modality-Alignment"><a href="#Ola-Pushing-the-Frontiers-of-Omni-Modal-Language-Model-with-Progressive-Modality-Alignment" class="headerlink" title="Ola: Pushing the Frontiers of Omni-Modal Language Model with Progressive   Modality Alignment"></a>Ola: Pushing the Frontiers of Omni-Modal Language Model with Progressive   Modality Alignment</h2><p><strong>Authors:Zuyan Liu, Yuhao Dong, Jiahui Wang, Ziwei Liu, Winston Hu, Jiwen Lu, Yongming Rao</strong></p>
<p>Recent advances in large language models, particularly following GPT-4o, have sparked increasing interest in developing omni-modal models capable of understanding more modalities. While some open-source alternatives have emerged, there is still a notable lag behind specialized single-modality models in performance. In this paper, we present Ola, an Omni-modal language model that achieves competitive performance across image, video, and audio understanding compared to specialized counterparts. The core design of Ola lies in its progressive modality alignment strategy that extends the supporting modality of the language model progressively. Our training pipeline begins with the most distinct modalities: image and text, then gradually expands the skill sets of the model using speech data that connects language and audio knowledge, and video data that connects all modalities. The progressive learning pipeline also enables us to maintain a relatively small size of the cross-modal alignment data, making developing omni-modal from existing vision-language models easy and less costly. Moreover, to unlock an advanced interactive experience like GPT-4o, we further design a sentence-wise decoding solution for streaming speech generation. Extensive experiments demonstrate that Ola surpasses existing open omni-modal LLMs across all modalities while achieving highly competitive performance compared to state-of-the-art specialized models of similar sizes. We aim to make Ola a fully open omni-modal understanding solution to advance future research in this emerging field. Model weights, code, and data are open-sourced at <a target="_blank" rel="noopener" href="https://github.com/Ola-Omni/Ola">https://github.com/Ola-Omni/Ola</a>. </p>
<blockquote>
<p>最近大型语言模型的进步，尤其是GPT-4o之后，引发了人们对开发能够理解更多模态的通模模型的浓厚兴趣。虽然已经出现了一些开源的替代品，但在性能上仍然明显落后于专业的单模态模型。在本文中，我们介绍了Ola，这是一款通模态语言模型，在图像、视频和音频理解方面与专业化模型相比具有竞争力。Ola的核心设计在于其渐进的模态对齐策略，该策略逐步扩展语言模型的支持模态。我们的训练管道始于最独特的模态：图像和文本，然后使用连接语言和音频知识的语音数据以及连接所有模态的视频数据，逐步扩大模型的技能集。这种渐进的学习管道还使我们能够保持相对较小的跨模态对齐数据集规模，从而更容易、更经济地从现有的视觉语言模型开发通模态模型。此外，为了解锁类似GPT-4o的高级交互体验，我们还设计了一种基于句子的解码解决方案，用于流式语音生成。大量实验表明，Ola在所有模态上超越了现有的开源通模态大型语言模型，同时在类似规模的专业模型中取得了高度竞争的性能。我们的目标是使Ola成为完全开放的通模态解决方案，以推动这一新兴领域未来的研究。模型权重、代码和数据已在<a target="_blank" rel="noopener" href="https://github.com/Ola-Omni/Ola%E4%B8%8A%E5%BC%80%E6%BA%90%E3%80%82">https://github.com/Ola-Omni/Ola上开源。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.04328v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>近期大型语言模型的进步，尤其是GPT-4o之后，引发了多模态模型的兴趣，它们能理解更多模态。本文介绍了一款名为Ola的通用多模态语言模型，在图像、视频和音频理解方面表现出竞争力。Ola的核心设计在于其渐进式模态对齐策略，逐步扩展语言模型的支持模态。其训练流程从最具特色的模态开始，即图像和文本，然后逐步使用连接语言和音频知识的语音数据以及连接所有模态的视频数据来扩展模型技能集。这种渐进式学习流程也让我们得以维持较小的跨模态对齐数据量，使从现有视觉语言模型发展出Omni-modal变得更加容易且成本更低。此外，为了解锁类似GPT-4o的高级交互体验，我们进一步设计了流式语音生成的句子级解码解决方案。实验表明，Ola在所有模态上都超越了现有的开源Omni-modal大型语言模型，并在类似规模的最新专业模型上取得了具有竞争力的表现。我们的目标是让Ola成为完全开源的多模态理解解决方案，以推动这一新兴领域的研究发展。模型权重、代码和数据已在GitHub上开源。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Ola是一款多模态语言模型，具备在图像、视频和音频理解方面的竞争力。</li>
<li>Ola采用渐进式模态对齐策略进行训练，逐步扩展语言模型的模态支持能力。</li>
<li>Ola的训练流程从图像和文本开始，然后逐步引入语音和视频数据来扩展模型技能集。</li>
<li>渐进式学习流程使得跨模态对齐的数据量相对较小，降低了开发成本。</li>
<li>Ola设计了句子级解码解决方案，以支持流式语音生成等高级交互体验。</li>
<li>Ola在多个实验上超越了现有的开源Omni-modal大型语言模型。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.04328">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-1a3a9952756be81ee4f0ceca05bb634b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-626c4fc88e7d0b71cec8356f0d2ae844.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3091b78a842b040147a3b51e76f39499.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8d2f0d54b6f8fa60a718ec67b9aa10d6.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-59fbfbd3e80017c232a18668c39d327b.jpg" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="TAID-Temporally-Adaptive-Interpolated-Distillation-for-Efficient-Knowledge-Transfer-in-Language-Models"><a href="#TAID-Temporally-Adaptive-Interpolated-Distillation-for-Efficient-Knowledge-Transfer-in-Language-Models" class="headerlink" title="TAID: Temporally Adaptive Interpolated Distillation for Efficient   Knowledge Transfer in Language Models"></a>TAID: Temporally Adaptive Interpolated Distillation for Efficient   Knowledge Transfer in Language Models</h2><p><strong>Authors:Makoto Shing, Kou Misaki, Han Bao, Sho Yokoi, Takuya Akiba</strong></p>
<p>Causal language models have demonstrated remarkable capabilities, but their size poses significant challenges for deployment in resource-constrained environments. Knowledge distillation, a widely-used technique for transferring knowledge from a large teacher model to a small student model, presents a promising approach for model compression. A significant remaining issue lies in the major differences between teacher and student models, namely the substantial capacity gap, mode averaging, and mode collapse, which pose barriers during distillation. To address these issues, we introduce $\textit{Temporally Adaptive Interpolated Distillation (TAID)}$, a novel knowledge distillation approach that dynamically interpolates student and teacher distributions through an adaptive intermediate distribution, gradually shifting from the student’s initial distribution towards the teacher’s distribution. We provide a theoretical analysis demonstrating TAID’s ability to prevent mode collapse and empirically show its effectiveness in addressing the capacity gap while balancing mode averaging and mode collapse. Our comprehensive experiments demonstrate TAID’s superior performance across various model sizes and architectures in both instruction tuning and pre-training scenarios. Furthermore, we showcase TAID’s practical impact by developing two state-of-the-art compact foundation models: $\texttt{TAID-LLM-1.5B}$ for language tasks and $\texttt{TAID-VLM-2B}$ for vision-language tasks. These results demonstrate TAID’s effectiveness in creating high-performing and efficient models, advancing the development of more accessible AI technologies. </p>
<blockquote>
<p>因果语言模型展现出了显著的能力，但它们的规模给在资源受限环境中部署带来了重大挑战。知识蒸馏是一种广泛使用的技术，可以从大型教师模型转移到小型学生模型，这为模型压缩提供了一种有前景的方法。然而，仍然存在一个主要问题，即教师模型和学生模型之间的差异，包括显著的能力差距、模式平均和模式崩溃，这些在蒸馏过程中构成了障碍。为了解决这些问题，我们引入了<em>时序自适应插值蒸馏（TAID）</em>，这是一种新型的知识蒸馏方法，通过自适应中间分布动态插值学生和教师的分布，从学生最初的分布逐渐转向教师的分布。我们进行了理论分析，证明了TAID在防止模式崩溃方面的能力，并实证证明了它在解决能力差距、平衡模式平均和模式崩溃方面的有效性。我们的综合实验表明，TAID在各种型号和架构的模型中，无论是在指令微调还是预训练场景中，都表现出卓越的性能。此外，我们通过开发两个最先进的紧凑基础模型：用于语言任务的TAID-LLM-1.5B和用于视觉语言任务的TAID-VLM-2B，展示了TAID的实际影响。这些结果证明了TAID在创建高性能和高效模型方面的有效性，推动了更可访问的人工智能技术的发展。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.16937v3">PDF</a> To appear at the 13th International Conference on Learning   Representations (ICLR 2025) as a Spotlight presentation</p>
<p><strong>Summary</strong></p>
<p>本文介绍了因果语言模型在资源受限环境中的部署挑战，以及知识蒸馏技术在此方面的应用。为解决知识蒸馏中教师模型与学生模型间存在的容量差距、模式平均和模式崩溃等问题，提出了一种新型知识蒸馏方法——Temporally Adaptive Interpolated Distillation (TAID)。该方法通过自适应中间分布动态插值学生模型和教师模型的分布，逐步从学生模型的初始分布向教师模型的分布转变。理论分析证明了TAID在防止模式崩溃方面的能力，并通过实验验证了其在解决容量差距、平衡模式平均和模式崩溃方面的有效性。此外，通过开发两个先进的紧凑基础模型TAID-LLM-1.5B和TAID-VLM-2B，展示了TAID的实际影响力。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>因果语言模型在资源受限环境中的部署面临挑战。</li>
<li>知识蒸馏是解决这一挑战的一种有前景的方法。</li>
<li>教师模型和学生模型之间存在显著的差异，如容量差距、模式平均和模式崩溃。</li>
<li>新型知识蒸馏方法——Temporally Adaptive Interpolated Distillation (TAID) 被提出以解决这些问题。</li>
<li>TAID通过自适应中间分布动态插值学生模型和教师模型的分布。</li>
<li>理论和实验证明了TAID在解决容量差距和平衡模式平均与模式崩溃方面的有效性。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.16937">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-3d23d30b6f672b190bc5c3419a8132e0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7d19b6b370cd647f9e7364ec50c82a4a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-65bc89ef8169783c6875b2aeb18c91a1.jpg" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="URSA-Understanding-and-Verifying-Chain-of-thought-Reasoning-in-Multimodal-Mathematics"><a href="#URSA-Understanding-and-Verifying-Chain-of-thought-Reasoning-in-Multimodal-Mathematics" class="headerlink" title="URSA: Understanding and Verifying Chain-of-thought Reasoning in   Multimodal Mathematics"></a>URSA: Understanding and Verifying Chain-of-thought Reasoning in   Multimodal Mathematics</h2><p><strong>Authors:Ruilin Luo, Zhuofan Zheng, Yifan Wang, Yiyao Yu, Xinzhe Ni, Zicheng Lin, Jin Zeng, Yujiu Yang</strong></p>
<p>Chain-of-Thought (CoT) reasoning is widely used to enhance the mathematical reasoning capabilities of large language models (LLMs). The introduction of process supervision for CoT trajectories has sparked discussions on improving test-time scaling, thereby unlocking the System 2-style thinking capabilities of these models. However, in multimodal mathematical reasoning, the scarcity of high-quality CoT training data has hindered existing models from achieving both deliberate reasoning and fine-grained verification. In this work, we propose a novel framework that introduces System 2-style thinking to multimodal mathematical reasoning. We introduce a three-module CoT data synthesis process that integrates CoT distillation, trajectory-format rewriting, and format unification. This process generates MMathCoT-1M, a high-quality CoT reasoning instruction fine-tuning dataset. Furthermore, we implement a dual-view trajectory labeling automation that targets both visual grounding fidelity and deductive chain validity, resulting in the DualMath-1.1M dataset. The URSA-8B model, trained on MMathCoT-1M, achieves new state-of-the-art (SOTA) performance among similarly sized multimodal LLMs on six popular reasoning benchmarks. Training URSA-8B further on the DualMath-1.1M dataset yields URSA-RM-8B, a verifier that enhances URSA-8B’s test-time performance and surpasses strong closed-source multimodal MLLMs like GPT-4o. The model weights, training data, and code have been open-sourced: <a target="_blank" rel="noopener" href="https://github.com/URSA-MATH/URSA-MATH">https://github.com/URSA-MATH/URSA-MATH</a>. </p>
<blockquote>
<p>链式思维（CoT）推理被广泛应用于增强大型语言模型（LLM）的数学推理能力。引入过程监督以优化CoT轨迹的讨论，从而解锁这些模型的System 2式思维能力。然而，在多模态数学推理中，高质量CoT训练数据的稀缺阻碍了现有模型实现深思熟虑的推理和精细的验证。在这项工作中，我们提出了一种引入System 2式思维到多模态数学推理的新型框架。我们引入了包含CoT蒸馏、轨迹格式重写和格式统一的三个模块CoT数据合成过程，生成了高质量的MMathCoT-1M CoT推理指令微调数据集。此外，我们实现了旨在实现视觉定位保真性和演绎链有效性的双视图轨迹标记自动化，创建了DualMath-1.1M数据集。在六个流行的推理基准测试上，训练在MMathCoT-1M上的URSA-8B模型取得了最新最先进的性能。进一步在DualMath-1.1M数据集上训练URSA-8B，产生了增强测试时性能和超越强大封闭源多模态LLLMs（如GPT-4o）的验证器URSA-RM-8B。模型权重、训练数据和代码均已开源：<a target="_blank" rel="noopener" href="https://github.com/URSA-MATH/URSA-MATH">https://github.com/URSA-MATH/URSA-MATH</a>。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.04686v3">PDF</a> Fix typos and add results. 27 pages, 11 tables, 17 figures. Models,   training data and code have been open-sourced. Project url:   <a target="_blank" rel="noopener" href="https://ursa-math.github.io/">https://ursa-math.github.io</a></p>
<p><strong>Summary</strong></p>
<p>本文介绍了如何将System 2风格的思考引入多模态数学推理中，通过提出一个包含三个模块的认知链数据合成过程（CoT蒸馏、轨迹格式重写和格式统一），生成了MMathCoT-1M高质量CoT推理指令微调数据集。此外，还实现了针对视觉定位准确性和演绎链有效性的双视图轨迹标签自动化，创建了DualMath-1.1M数据集。在六个流行的推理基准测试上，经过MMathCoT-1M数据训练的URSA-8B模型取得了最新状态的最优性能。进一步在DualMath-1.1M数据集上训练得到的URSA-RM-8B验证器提升了URSA-8B的测试性能，超越了强大的封闭源多模态大型语言模型，如GPT-4o。模型权重、训练数据和代码均已开源。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>System 2风格的思考被引入多模态数学推理中，以提高模型的推理能力。</li>
<li>提出一个包含CoT蒸馏、轨迹格式重写和格式统一的三模块CoT数据合成过程，用于生成高质量的CoT推理指令微调数据集MMathCoT-1M。</li>
<li>通过双视图轨迹标签自动化，创建了DualMath-1.1M数据集，该数据集旨在提高视觉定位准确性和演绎链有效性。</li>
<li>URSA-8B模型在多个推理基准测试上表现出最新状态的最优性能。</li>
<li>URSA-RM-8B验证器在URSA-8B的基础上进一步提升测试性能，超越了某些先进的大型语言模型。</li>
<li>模型权重、训练数据和代码已开源，便于公众访问和使用。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.04686">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-53909e10dbba86683dfa420fe5c7847d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-0f226cede3d49679337bb1b36dd4c8b3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-89b3fbf3abfc062c2d16ba8876dbaf78.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-78ce45eb4c967d1e1e1dfc060c7580e8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-99704b810299ad9a94b975805a7da611.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-41fd13f59f8d8462c25a47b7de17d269.jpg" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1><h2 id="Topic-Aware-Knowledge-Graph-with-Large-Language-Models-for-Interoperability-in-Recommender-Systems"><a href="#Topic-Aware-Knowledge-Graph-with-Large-Language-Models-for-Interoperability-in-Recommender-Systems" class="headerlink" title="Topic-Aware Knowledge Graph with Large Language Models for   Interoperability in Recommender Systems"></a>Topic-Aware Knowledge Graph with Large Language Models for   Interoperability in Recommender Systems</h2><p><strong>Authors:Minhye Jeon, Seokho Ahn, Young-Duk Seo</strong></p>
<p>The use of knowledge graphs in recommender systems has become one of the common approaches to addressing data sparsity and cold start problems. Recent advances in large language models (LLMs) offer new possibilities for processing side and context information within knowledge graphs. However, consistent integration across various systems remains challenging due to the need for domain expert intervention and differences in system characteristics. To address these issues, we propose a consistent approach that extracts both general and specific topics from both side and context information using LLMs. First, general topics are iteratively extracted and updated from side information. Then, specific topics are extracted using context information. Finally, to address synonymous topics generated during the specific topic extraction process, a refining algorithm processes and resolves these issues effectively. This approach allows general topics to capture broad knowledge across diverse item characteristics, while specific topics emphasize detailed attributes, providing a more comprehensive understanding of the semantic features of items and the preferences of users. Experimental results demonstrate significant improvements in recommendation performance across diverse knowledge graphs. </p>
<blockquote>
<p>推荐系统中使用知识图谱已成为解决数据稀疏和冷启动问题的常见方法之一。大型语言模型（LLM）的最新进展为处理知识图谱中的侧面和上下文信息提供了新的可能性。然而，由于需要领域专家干预和系统特性差异，跨各种系统的集成仍然具有挑战性。为了解决这些问题，我们提出了一种一致的方法，该方法使用LLM从侧面和上下文信息中提取一般和特定主题。首先，从侧信息中迭代提取和更新一般主题。然后，使用上下文信息提取特定主题。最后，为了解决特定主题提取过程中产生的同义主题问题，采用精炼算法对其进行有效处理。这种方法允许一般主题捕捉跨不同项目特征的广泛知识，而特定主题强调详细属性，从而更全面地理解项目的语义特征以及用户的偏好。实验结果表明，在多种知识图谱中推荐性能得到了显著提高。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.20163v3">PDF</a> Accepted by The 40th ACM&#x2F;SIGAPP Symposium On Applied Computing(SAC)   2025</p>
<p><strong>Summary</strong><br>知识图谱在推荐系统中应用广泛，以解决数据稀疏和冷启动问题。大型语言模型（LLM）的最新进展为处理知识图谱中的侧信息和上下文信息提供了新的可能性。本文提出一种方法，通过LLM从侧信息和上下文信息中提取一般和特定主题，解决跨系统整合中的挑战。实验结果表明，该方法在多种知识图谱中显著提高推荐性能。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>知识图谱在推荐系统中用于解决数据稀疏和冷启动问题。</li>
<li>大型语言模型（LLM）在处理知识图谱中的侧信息和上下文信息方面表现出新可能性。</li>
<li>本文方法通过LLM从侧信息和上下文信息中提取一般和特定主题，以应对跨系统整合的挑战。</li>
<li>一般主题能捕捉跨不同项目特征的广泛知识，而特定主题强调详细属性，更全面地理解项目的语义特征和用户偏好。</li>
<li>该方法通过迭代提取和更新一般主题，以及使用上下文信息提取特定主题来实施。</li>
<li>为解决特定主题提取过程中产生的同义词主题，采用精炼算法进行处理，有效提高效果。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.20163">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-1b723ac05d4b50c38f54a17bd557a0de.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f47c8a5c99d28e5dbb7aec441292b409.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e8301ef801c31cae1f611ab565192a5e.jpg" align="middle">
</details>


<h1 id="-19"><a href="#-19" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        文章作者:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        文章链接:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-02-13/LLM/">https://kedreamix.github.io/Talk2Paper/Paper/2025-02-13/LLM/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        版权声明:
                    </i>
                </span>
                <span class="reprint-info">
                    本博客所有文章除特別声明外，均采用
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    许可协议。转载请注明来源
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/LLM/">
                                    <span class="chip bg-color">LLM</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;上一篇</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-02-13/Agent/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-04fc13813f6549fb5d8d50b7fb684543.jpg" class="responsive-img" alt="Agent">
                        
                        <span class="card-title">Agent</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Agent 方向最新论文已更新，请持续关注 Update in 2025-02-13  Learning in Markets with Heterogeneous Agents Dynamics and Survival of   Bayesian vs. No-Regret Learners
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-02-13
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Agent/" class="post-category">
                                    Agent
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Agent/">
                        <span class="chip bg-color">Agent</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                下一篇&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-02-12/TTS/">
                    <div class="card-image">
                        
                        <img src="D:\MyBlog\AutoFX\arxiv\2025-02-12\./crop_TTS/2502.05236v1/page_0_0.jpg" class="responsive-img" alt="TTS">
                        
                        <span class="card-title">TTS</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            TTS 方向最新论文已更新，请持续关注 Update in 2025-02-12  Can 1B LLM Surpass 405B LLM? Rethinking Compute-Optimal Test-Time   Scaling
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-02-12
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/TTS/" class="post-category">
                                    TTS
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/TTS/">
                        <span class="chip bg-color">TTS</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- 代码块功能依赖 -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- 代码语言 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- 代码块复制 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- 代码块收缩 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC 悬浮按钮. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* 修复文章卡片 div 的宽度. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // 切换TOC目录展开收缩的相关操作.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;站点总字数:&nbsp;<span
                        class="white-color">11176.7k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- 运行天数提醒. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // 区分是否有年份.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = '本站已运行 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                daysTip = '本站已運行 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = '本站已运行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = '本站已運行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="关注我的知乎: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">知</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- 搜索遮罩框 -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span>
            <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- 白天和黑夜主题 -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- 回到顶部按钮 -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // 只在桌面版网页启用特效
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- 雪花特效 -->
    

    <!-- 鼠标星星特效 -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--腾讯兔小巢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- 动态标签 -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Σ(っ °Д °;)っ诶，页面崩溃了嘛？", clearTimeout(st)) : (document.title = "φ(゜▽゜*)♪咦，又好了！", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
