<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="LLM">
    <meta name="description" content="LLM æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-02-13  Examining Multilingual Embedding Models Cross-Lingually Through   LLM-Generated Adversarial Examples">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>LLM | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pic1.zhimg.com/v2-e3ea5af80ebe75832fea69e3b3e9326b.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">LLM</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/LLM/">
                                <span class="chip bg-color">LLM</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/LLM/" class="post-category">
                                LLM
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-02-13
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-05-14
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    18.7k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    76 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-02-13-æ›´æ–°"><a href="#2025-02-13-æ›´æ–°" class="headerlink" title="2025-02-13 æ›´æ–°"></a>2025-02-13 æ›´æ–°</h1><h2 id="Examining-Multilingual-Embedding-Models-Cross-Lingually-Through-LLM-Generated-Adversarial-Examples"><a href="#Examining-Multilingual-Embedding-Models-Cross-Lingually-Through-LLM-Generated-Adversarial-Examples" class="headerlink" title="Examining Multilingual Embedding Models Cross-Lingually Through   LLM-Generated Adversarial Examples"></a>Examining Multilingual Embedding Models Cross-Lingually Through   LLM-Generated Adversarial Examples</h2><p><strong>Authors:Andrianos Michail, Simon Clematide, Rico Sennrich</strong></p>
<p>The evaluation of cross-lingual semantic search capabilities of models is often limited to existing datasets from tasks such as information retrieval and semantic textual similarity. To allow for domain-specific evaluation, we introduce Cross Lingual Semantic Discrimination (CLSD), a novel cross-lingual semantic search task that requires only a set of parallel sentence pairs of the language pair of interest within the target domain. This task focuses on the ability of a model to cross-lingually rank the true parallel sentence higher than hard negatives generated by a large language model. We create four instances of our introduced CLSD task for the language pair German-French within the domain of news. Within this case study, we find that models that are also fine-tuned for retrieval tasks (e.g., multilingual E5) benefit from using English as the pivot language, while bitext mining models such as LaBSE perform best directly cross-lingually. We also show a fine-grained similarity analysis enabled by our distractor generation strategy, indicating that different embedding models are sensitive to different types of perturbations. </p>
<blockquote>
<p>å¯¹æ¨¡å‹çš„è·¨è¯­è¨€è¯­ä¹‰æœç´¢èƒ½åŠ›çš„è¯„ä¼°é€šå¸¸ä»…é™äºä¿¡æ¯æ£€ç´¢å’Œè¯­ä¹‰æ–‡æœ¬ç›¸ä¼¼æ€§ä¹‹ç±»çš„ç°æœ‰æ•°æ®é›†ã€‚ä¸ºäº†è¿›è¡Œç‰¹å®šé¢†åŸŸçš„è¯„ä¼°ï¼Œæˆ‘ä»¬å¼•å…¥äº†è·¨è¯­è¨€è¯­ä¹‰è¾¨åˆ«ï¼ˆCLSDï¼‰ï¼Œè¿™æ˜¯ä¸€ç§æ–°çš„è·¨è¯­è¨€è¯­ä¹‰æœç´¢ä»»åŠ¡ï¼Œå®ƒåªéœ€è¦ç›®æ ‡é¢†åŸŸå†…æ„Ÿå…´è¶£è¯­è¨€å¯¹çš„å¹¶è¡Œå¥å­å¯¹é›†åˆã€‚æ­¤ä»»åŠ¡ä¾§é‡äºæ¨¡å‹å°†çœŸæ­£çš„å¹¶è¡Œå¥å­æ’åé«˜äºç”±å¤§å‹è¯­è¨€æ¨¡å‹ç”Ÿæˆç¡¬è´Ÿæ ·æœ¬çš„è·¨è¯­è¨€èƒ½åŠ›ã€‚æˆ‘ä»¬ä¸ºæ–°é—»é¢†åŸŸçš„å¾·æ³•è¯­è¨€å¯¹åˆ›å»ºäº†å››ä¸ªCLSDä»»åŠ¡çš„å®ä¾‹ã€‚åœ¨æ­¤æ¡ˆä¾‹ç ”ç©¶ä¸­ï¼Œæˆ‘ä»¬å‘ç°è¿›è¡Œæ£€ç´¢ä»»åŠ¡å¾®è°ƒï¼ˆä¾‹å¦‚å¤šè¯­è¨€E5ï¼‰çš„æ¨¡å‹å—ç›Šäºä½¿ç”¨è‹±è¯­ä½œä¸ºä¸­å¿ƒè¯­è¨€ï¼Œè€Œç›´æ¥è¿›è¡Œè·¨è¯­è¨€å¤„ç†çš„åŒæ–‡æœ¬æŒ–æ˜æ¨¡å‹ï¼ˆå¦‚LaBSEï¼‰è¡¨ç°æœ€ä½³ã€‚æˆ‘ä»¬è¿˜é€šè¿‡å¹²æ‰°é¡¹ç”Ÿæˆç­–ç•¥å±•ç¤ºäº†ç²¾ç»†çš„ç›¸ä¼¼æ€§åˆ†æï¼Œè¿™è¡¨æ˜ä¸åŒçš„åµŒå…¥æ¨¡å‹å¯¹ä¸åŒç±»å‹çš„æ‰°åŠ¨æœ‰ä¸åŒçš„æ•æ„Ÿæ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.08638v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†è·¨è¯­è¨€è¯­ä¹‰æœç´¢èƒ½åŠ›æ¨¡å‹è¯„ä¼°çš„æ–°æŒ‘æˆ˜ã€‚ä¸ºåº”å¯¹é¢†åŸŸç‰¹å®šçš„è¯„ä¼°éœ€æ±‚ï¼Œæå‡ºäº†è·¨è¯­è¨€è¯­ä¹‰åˆ¤åˆ«ï¼ˆCLSDï¼‰è¿™ä¸€æ–°ä»»åŠ¡ã€‚CLSDä»»åŠ¡åªéœ€é’ˆå¯¹ç›®æ ‡é¢†åŸŸå†…çš„è¯­è¨€å¯¹è®¾ç½®å¹³è¡Œå¥å­å¯¹å³å¯ã€‚ç ”ç©¶å‘ç°ï¼Œé’ˆå¯¹æ£€ç´¢ä»»åŠ¡è¿›è¡Œå¾®è°ƒï¼ˆå¦‚å¤šè¯­ç§E5ï¼‰çš„æ¨¡å‹ä½¿ç”¨è‹±è¯­ä½œä¸ºä¸­ä»‹è¯­è¨€å—ç›Šè¾ƒå¤§ï¼Œè€Œç›´æ¥è¿›è¡Œè·¨è¯­è¨€å¤„ç†çš„æ¨¡å‹ï¼ˆå¦‚LaBSEï¼‰è¡¨ç°æœ€ä½³ã€‚æ­¤å¤–ï¼Œæœ¬æ–‡é€šè¿‡å¹²æ‰°é¡¹ç”Ÿæˆç­–ç•¥å±•ç¤ºäº†ç²¾ç»†çš„ç›¸ä¼¼æ€§åˆ†æï¼Œè¡¨æ˜ä¸åŒçš„åµŒå…¥æ¨¡å‹å¯¹ä¸åŒç±»å‹çš„æ‰°åŠ¨æœ‰ä¸åŒçš„æ•æ„Ÿæ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è·¨è¯­è¨€è¯­ä¹‰æœç´¢èƒ½åŠ›æ¨¡å‹çš„è¯„ä¼°å—é™äºç°æœ‰æ•°æ®é›†å’Œä¿¡æ¯æ£€ç´¢ç­‰ä»»åŠ¡ã€‚</li>
<li>æå‡ºäº†ä¸€ç§æ–°çš„è·¨è¯­è¨€è¯­ä¹‰åˆ¤åˆ«ï¼ˆCLSDï¼‰ä»»åŠ¡ï¼Œåªéœ€è®¾ç½®å¹³è¡Œå¥å­å¯¹å³å¯è¿›è¡Œé¢†åŸŸç‰¹å®šè¯„ä¼°ã€‚</li>
<li>åœ¨æ–°é—»é¢†åŸŸçš„å¾·è¯­-æ³•è¯­è¯­è¨€å¯¹å®ä¾‹ä¸­ï¼Œå‘ç°é’ˆå¯¹æ£€ç´¢ä»»åŠ¡è¿›è¡Œå¾®è°ƒï¼ˆå¦‚å¤šè¯­ç§E5ï¼‰çš„æ¨¡å‹åœ¨ä½¿ç”¨è‹±è¯­ä½œä¸ºä¸­ä»‹è¯­è¨€æ—¶è¡¨ç°å‡ºä¼˜åŠ¿ã€‚</li>
<li>ç›´æ¥è¿›è¡Œè·¨è¯­è¨€å¤„ç†çš„æ¨¡å‹ï¼ˆå¦‚LaBSEï¼‰è¡¨ç°æœ€ä½³ã€‚</li>
<li>CLSDä»»åŠ¡èƒ½å¤Ÿå±•ç¤ºç²¾ç»†çš„ç›¸ä¼¼æ€§åˆ†æï¼Œè¿™æœ‰åŠ©äºç†è§£ä¸åŒåµŒå…¥æ¨¡å‹çš„æ•æ„Ÿæ€§ã€‚</li>
<li>ä¸åŒåµŒå…¥æ¨¡å‹å¯¹ä¸åŒç±»å‹çš„æ‰°åŠ¨ååº”ä¸åŒï¼Œè¿™ä¸ºæ”¹è¿›æ¨¡å‹æä¾›äº†æ–¹å‘ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.08638">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-d9147a8eddc33c146e614e22f88cd032.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-74b897af1638737523f1e8c770d9a742.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-14c24d088220547eb70338a676daa736.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-cb2b206a9ac9ac5d2bf28035feddf60a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a71c31b86eee2baf36aa5ce21701dea7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d8b51c942c1718ec3952856ab83489ac.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="QA-Expand-Multi-Question-Answer-Generation-for-Enhanced-Query-Expansion-in-Information-Retrieval"><a href="#QA-Expand-Multi-Question-Answer-Generation-for-Enhanced-Query-Expansion-in-Information-Retrieval" class="headerlink" title="QA-Expand: Multi-Question Answer Generation for Enhanced Query Expansion   in Information Retrieval"></a>QA-Expand: Multi-Question Answer Generation for Enhanced Query Expansion   in Information Retrieval</h2><p><strong>Authors:Wonduk Seo, Seunghyun Lee</strong></p>
<p>Query expansion is widely used in Information Retrieval (IR) to improve search outcomes by enriching queries with additional contextual information. Although recent Large Language Model (LLM) based methods generate pseudo-relevant content and expanded terms via multiple prompts, they often yield repetitive, narrow expansions that lack the diverse context needed to retrieve all relevant information. In this paper, we introduce QA-Expand, a novel and effective framework for query expansion. It first generates multiple relevant questions from the initial query and subsequently produces corresponding pseudo-answers as surrogate documents. A feedback model further rewrites and filters these answers to ensure only the most informative augmentations are incorporated. Extensive experiments on benchmarks such as BEIR and TREC demonstrate that QA-Expand enhances retrieval performance by up to 13% over state-of-the-art methods, offering a robust solution for modern retrieval challenges. </p>
<blockquote>
<p>æŸ¥è¯¢æ‰©å±•åœ¨ä¿¡æ¯æ£€ç´¢ï¼ˆIRï¼‰ä¸­å¾—åˆ°äº†å¹¿æ³›åº”ç”¨ï¼Œé€šè¿‡æ·»åŠ é¢å¤–çš„ä¸Šä¸‹æ–‡ä¿¡æ¯æ¥ä¸°å¯ŒæŸ¥è¯¢ï¼Œä»è€Œæ”¹è¿›æœç´¢ç»“æœã€‚å°½ç®¡æœ€è¿‘åŸºäºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ–¹æ³•é€šè¿‡å¤šä¸ªæç¤ºç”Ÿæˆä¼ªç›¸å…³å†…å®¹å¹¶æ‰©å±•æœ¯è¯­ï¼Œä½†å®ƒä»¬é€šå¸¸ä¼šäº§ç”Ÿé‡å¤æ€§é«˜ã€èŒƒå›´ç‹­çª„çš„æ‰©å±•å†…å®¹ï¼Œç¼ºä¹æ£€ç´¢æ‰€æœ‰ç›¸å…³ä¿¡æ¯æ‰€éœ€çš„å„ç§ä¸Šä¸‹æ–‡ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†QA-Expandï¼Œè¿™æ˜¯ä¸€ä¸ªç”¨äºæŸ¥è¯¢æ‰©å±•çš„æ–°å‹æœ‰æ•ˆæ¡†æ¶ã€‚å®ƒé¦–å…ˆæ ¹æ®åˆå§‹æŸ¥è¯¢ç”Ÿæˆå¤šä¸ªç›¸å…³é—®é¢˜ï¼Œç„¶åç”Ÿæˆç›¸åº”çš„ä¼ªç­”æ¡ˆä½œä¸ºæ›¿ä»£æ–‡æ¡£ã€‚åé¦ˆæ¨¡å‹è¿›ä¸€æ­¥é‡å†™å’Œè¿‡æ»¤è¿™äº›ç­”æ¡ˆï¼Œä»¥ç¡®ä¿åªåˆå¹¶æœ€å…·æœ‰ä¿¡æ¯æ€§çš„å¢å¼ºå†…å®¹ã€‚åœ¨BEIRå’ŒTRECç­‰åŸºå‡†æµ‹è¯•ä¸Šçš„å¤§é‡å®éªŒè¡¨æ˜ï¼Œä¸æœ€æ–°æŠ€æœ¯ç›¸æ¯”ï¼ŒQA-Expandçš„æ£€ç´¢æ€§èƒ½æé«˜äº†é«˜è¾¾13%ï¼Œä¸ºç°ä»£æ£€ç´¢æŒ‘æˆ˜æä¾›äº†ç¨³å¥çš„è§£å†³æ–¹æ¡ˆã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.08557v1">PDF</a> 8 pages</p>
<p><strong>Summary</strong></p>
<p>åŸºäºä¿¡æ¯æ£€ç´¢ä¸­çš„æŸ¥è¯¢æ‰©å±•æŠ€æœ¯ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„æŸ¥è¯¢æ‰©å±•æ¡†æ¶QA-Expandã€‚è¯¥æ¡†æ¶é€šè¿‡ç”Ÿæˆå¤šä¸ªç›¸å…³é—®é¢˜å’Œå¯¹åº”çš„ä¼ªç­”æ¡ˆä½œä¸ºæ›¿ä»£æ–‡æ¡£æ¥æ‰©å±•æŸ¥è¯¢ï¼Œå¹¶åˆ©ç”¨åé¦ˆæ¨¡å‹é‡å†™å’Œè¿‡æ»¤ç­”æ¡ˆï¼Œç¡®ä¿åªèå…¥æœ€å…·æœ‰ä¿¡æ¯æ€§çš„æ‰©å±•å†…å®¹ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒQA-Expandåœ¨BEIRå’ŒTRECç­‰åŸºå‡†æµ‹è¯•ä¸Šçš„æ£€ç´¢æ€§èƒ½æé«˜äº†é«˜è¾¾13%ï¼Œä¸ºè§£å†³ç°ä»£æ£€ç´¢æŒ‘æˆ˜æä¾›äº†ç¨³å¥çš„è§£å†³æ–¹æ¡ˆã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æŸ¥è¯¢æ‰©å±•åœ¨ä¿¡æ¯æ£€ç´¢ä¸­ç”¨äºé€šè¿‡å¢åŠ ä¸Šä¸‹æ–‡ä¿¡æ¯æé«˜æœç´¢æ•ˆæœã€‚</li>
<li>æœ€æ–°å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ–¹æ³•è™½ç„¶èƒ½ç”Ÿæˆä¼ªç›¸å…³å†…å®¹å’Œæ‰©å±•è¯é¡¹ï¼Œä½†å¸¸å¸¸äº§ç”Ÿé‡å¤æ€§é«˜ã€èŒƒå›´ç‹­çª„çš„æ‰©å±•ï¼Œç¼ºä¹å¤šæ ·åŒ–çš„ä¸Šä¸‹æ–‡ä¿¡æ¯ã€‚</li>
<li>QA-Expandæ¡†æ¶é€šè¿‡ç”Ÿæˆå¤šä¸ªç›¸å…³é—®é¢˜åŠå…¶ä¼ªç­”æ¡ˆè¿›è¡ŒæŸ¥è¯¢æ‰©å±•ã€‚</li>
<li>åé¦ˆæ¨¡å‹ç”¨äºé‡å†™å’Œè¿‡æ»¤ç­”æ¡ˆï¼Œç¡®ä¿ä»…èå…¥æœ€å…·æœ‰ä¿¡æ¯æ€§çš„å†…å®¹ã€‚</li>
<li>åœ¨åŸºå‡†æµ‹è¯•å¦‚BEIRå’ŒTRECä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒQA-Expandæé«˜äº†æ£€ç´¢æ€§èƒ½ã€‚</li>
<li>QA-Expandæ€§èƒ½ä¼˜äºç°æœ‰æŠ€æœ¯ï¼Œæé«˜å¹…åº¦é«˜è¾¾13%ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.08557">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-7fb07f83c4aaac32d319e0008cbbf86f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1ad65c0c90da3d07853e1d6cdba56bcd.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-156d15db59d2980f2dc5804b3cec291a.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="LLMs-can-implicitly-learn-from-mistakes-in-context"><a href="#LLMs-can-implicitly-learn-from-mistakes-in-context" class="headerlink" title="LLMs can implicitly learn from mistakes in-context"></a>LLMs can implicitly learn from mistakes in-context</h2><p><strong>Authors:Lisa Alazraki, Maximilian Mozes, Jon Ander Campos, Yi Chern Tan, Marek Rei, Max Bartolo</strong></p>
<p>Learning from mistakes is a fundamental feature of human intelligence. Previous work has shown that Large Language Models (LLMs) can also learn from incorrect answers when provided with a comprehensive rationale detailing why an answer is wrong or how to correct it. In this work, we examine whether LLMs can learn from mistakes in mathematical reasoning tasks when these explanations are not provided. We investigate if LLMs are able to implicitly infer such rationales simply from observing both incorrect and correct answers. Surprisingly, we find that LLMs perform better, on average, when rationales are eliminated from the context and incorrect answers are simply shown alongside correct ones. This approach also substantially outperforms chain-of-thought prompting in our evaluations. We show that these results are consistent across LLMs of different sizes and varying reasoning abilities. Further, we carry out an in-depth analysis, and show that prompting with both wrong and correct answers leads to greater performance and better generalisation than introducing additional, more diverse question-answer pairs into the context. Finally, we show that new rationales generated by models that have only observed incorrect and correct answers are scored equally as highly by humans as those produced with the aid of exemplar rationales. Our results demonstrate that LLMs are indeed capable of in-context implicit learning. </p>
<blockquote>
<p>ä»é”™è¯¯ä¸­å­¦ä¹ æ˜¯äººç±»æ™ºèƒ½çš„åŸºæœ¬ç‰¹å¾ã€‚ä¹‹å‰çš„ç ”ç©¶è¡¨æ˜ï¼Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰èƒ½å¤Ÿåœ¨æä¾›å…¨é¢ç†ç”±çš„æƒ…å†µä¸‹ï¼Œè¯¦ç»†è¯´æ˜ç­”æ¡ˆä¸ºä½•é”™è¯¯æˆ–å¦‚ä½•æ”¹æ­£ï¼Œä»è€Œä»é”™è¯¯çš„ç­”æ¡ˆä¸­å­¦ä¹ ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬ç ”ç©¶çš„æ˜¯åœ¨æ²¡æœ‰æä¾›è¿™äº›è§£é‡Šçš„æƒ…å†µä¸‹ï¼ŒLLMæ˜¯å¦èƒ½åœ¨æ•°å­¦æ¨ç†ä»»åŠ¡ä¸­ä»é”™è¯¯ä¸­å­¦ä¹ ã€‚æˆ‘ä»¬è°ƒæŸ¥LLMæ˜¯å¦ä»…é€šè¿‡è§‚å¯Ÿæ­£ç¡®çš„å’Œé”™è¯¯çš„ç­”æ¡ˆå°±èƒ½éšå¼æ¨æ–­å‡ºè¿™äº›ç†ç”±ã€‚ä»¤äººæƒŠè®¶çš„æ˜¯ï¼Œæˆ‘ä»¬å‘ç°ï¼Œåœ¨ä¸Šä¸‹æ–‡ä¸­æ¶ˆé™¤ç†ç”±ï¼Œè€Œåªæ˜¯ç®€å•åœ°å±•ç¤ºæ­£ç¡®çš„å’Œé”™è¯¯çš„ç­”æ¡ˆæ—¶ï¼ŒLLMçš„å¹³å‡è¡¨ç°æ›´å¥½ã€‚è¿™ç§æ–¹æ³•ä¹Ÿåœ¨æˆ‘ä»¬çš„è¯„ä¼°ä¸­å¤§å¤§ä¼˜äºæ€è€ƒé“¾æç¤ºæ³•ã€‚æˆ‘ä»¬è¯æ˜äº†è¿™äº›ç»“æœåœ¨ä¸åŒè§„æ¨¡å’Œä¸åŒæ¨ç†èƒ½åŠ›çš„LLMä¹‹é—´æ˜¯ä¸€è‡´çš„ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿›è¡Œäº†æ·±å…¥çš„åˆ†æï¼Œå¹¶è¯æ˜ç”¨æ­£ç¡®å’Œé”™è¯¯çš„ç­”æ¡ˆæç¤ºæ¯”æŠŠæ›´å¤šå¤šæ ·çš„é—®é¢˜å’Œç­”æ¡ˆå¯¹å¼•å…¥ä¸Šä¸‹æ–‡èƒ½äº§ç”Ÿæ›´å¥½çš„æ€§èƒ½å’Œæ›´å¥½çš„æ³›åŒ–èƒ½åŠ›ã€‚æœ€åï¼Œæˆ‘ä»¬è¯æ˜äº†åªé€šè¿‡è§‚å¯Ÿæ­£ç¡®å’Œé”™è¯¯çš„ç­”æ¡ˆè€Œç”Ÿæˆçš„æ–°ç†ç”±ï¼Œä¸äººç±»è¾…åŠ©ç¤ºä¾‹ç†ç”±äº§ç”Ÿçš„è¯„åˆ†ç›¸åŒã€‚æˆ‘ä»¬çš„ç»“æœè¡¨æ˜ï¼ŒLLMç¡®å®å…·å¤‡ä¸Šä¸‹æ–‡éšå¼å­¦ä¹ èƒ½åŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.08550v1">PDF</a> </p>
<p><strong>Summary</strong>ï¼š</p>
<p>æœ¬æ–‡æ¢è®¨äº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨æ•°å­¦æ¨ç†ä»»åŠ¡ä¸­èƒ½å¦ä»é”™è¯¯ä¸­å­¦ä¹ ï¼Œè€Œæ— éœ€æä¾›è§£é‡Šã€‚ç ”ç©¶å‘ç°ï¼Œå½“ä»ä¸Šä¸‹æ–‡ä¸­æ¶ˆé™¤è§£é‡Šï¼Œä»…å±•ç¤ºæ­£ç¡®å’Œé”™è¯¯çš„ç­”æ¡ˆæ—¶ï¼ŒLLMsçš„è¡¨ç°æ›´ä½³ã€‚è¿™ç§æ–¹æ³•åœ¨è¯„ä¼°ä¸­æ˜¾è‘—ä¼˜äºæ€ç»´é“¾æç¤ºã€‚è¿›ä¸€æ­¥çš„åˆ†æè¡¨æ˜ï¼ŒåŒæ—¶å‘ˆç°æ­£ç¡®å’Œé”™è¯¯çš„ç­”æ¡ˆæç¤ºå¯ä»¥æé«˜æ€§èƒ½å¹¶å¢å¼ºæ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ï¼Œè€Œå¼•å…¥æ›´å¤šçš„é—®é¢˜ç­”æ¡ˆå¯¹åˆ™æ•ˆæœä¸å¤§ã€‚æœ€åï¼Œä»…é€šè¿‡è§‚å¯Ÿæ­£ç¡®å’Œé”™è¯¯çš„ç­”æ¡ˆè€Œç”Ÿæˆçš„æ¨¡å‹æ–°è§£é‡Šè¢«äººç±»è¯„ä»·ä¸ºä¸æœ‰ç¤ºä¾‹è§£é‡Šçš„åŒæ ·å‡ºè‰²ï¼Œè¯æ˜äº†LLMsç¡®å®å…·å¤‡ä¸Šä¸‹æ–‡éšå¼å­¦ä¹ èƒ½åŠ›ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>LLMså¯ä»¥ä»é”™è¯¯ä¸­å­¦ä¹ ï¼Œå°¤å…¶åœ¨æ•°å­¦æ¨ç†ä»»åŠ¡ä¸­ã€‚</li>
<li>å½“ä»…å±•ç¤ºæ­£ç¡®å’Œé”™è¯¯çš„ç­”æ¡ˆæ—¶ï¼ŒLLMsçš„è¡¨ç°æ›´ä½³ï¼Œä¸”è¿™ç§æ–¹æ³•ä¼˜äºæ€ç»´é“¾æç¤ºã€‚</li>
<li>åŒæ—¶å‘ˆç°æ­£ç¡®å’Œé”™è¯¯çš„ç­”æ¡ˆå¯ä»¥æé«˜LLMsçš„æ€§èƒ½å’Œæ³›åŒ–èƒ½åŠ›ã€‚</li>
<li>å¼•å…¥æ›´å¤šçš„é—®é¢˜ç­”æ¡ˆå¯¹å¹¶ä¸ä¸€å®šèƒ½æé«˜LLMsçš„å­¦ä¹ æ•ˆæœã€‚</li>
<li>LLMsèƒ½å¤Ÿé€šè¿‡è§‚å¯Ÿæ­£ç¡®å’Œé”™è¯¯çš„ç­”æ¡ˆç”Ÿæˆæ–°çš„è§£é‡Šï¼Œè¿™äº›è§£é‡Šè¢«äººç±»è¯„ä»·ä¸ºä¸æœ‰ç¤ºä¾‹è§£é‡Šçš„åŒæ ·å‡ºè‰²ã€‚</li>
<li>LLMså…·å¤‡ä¸Šä¸‹æ–‡éšå¼å­¦ä¹ èƒ½åŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.08550">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-a8840315dcd02ead22184bd60c753f57.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-dc48bbbb95ad0db7ed86180e7db36f17.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e5747c7b0d5e8fc6a61f65aa2c7afe81.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="LLM-Pretraining-with-Continuous-Concepts"><a href="#LLM-Pretraining-with-Continuous-Concepts" class="headerlink" title="LLM Pretraining with Continuous Concepts"></a>LLM Pretraining with Continuous Concepts</h2><p><strong>Authors:Jihoon Tack, Jack Lanchantin, Jane Yu, Andrew Cohen, Ilia Kulikov, Janice Lan, Shibo Hao, Yuandong Tian, Jason Weston, Xian Li</strong></p>
<p>Next token prediction has been the standard training objective used in large language model pretraining. Representations are learned as a result of optimizing for token-level perplexity. We propose Continuous Concept Mixing (CoCoMix), a novel pretraining framework that combines discrete next token prediction with continuous concepts. Specifically, CoCoMix predicts continuous concepts learned from a pretrained sparse autoencoder and mixes them into the modelâ€™s hidden state by interleaving with token hidden representations. Through experiments on multiple benchmarks, including language modeling and downstream reasoning tasks, we show that CoCoMix is more sample efficient and consistently outperforms standard next token prediction, knowledge distillation and inserting pause tokens. We find that combining both concept learning and interleaving in an end-to-end framework is critical to performance gains. Furthermore, CoCoMix enhances interpretability and steerability by allowing direct inspection and modification of the predicted concept, offering a transparent way to guide the modelâ€™s internal reasoning process. </p>
<blockquote>
<p>ä¸‹ä¸€ä¸ªè¯é¢„æµ‹ä½œä¸ºå¤§å‹è¯­è¨€æ¨¡å‹é¢„è®­ç»ƒçš„æ ‡å‡†è®­ç»ƒç›®æ ‡å·²ç»è¢«å¹¿æ³›åº”ç”¨ã€‚è¡¨ç¤ºæ˜¯é€šè¿‡ä¼˜åŒ–ä»¤ç‰Œçº§åˆ«çš„å›°æƒ‘åº¦æ¥å­¦ä¹ çš„ã€‚æˆ‘ä»¬æå‡ºäº†è¿ç»­æ¦‚å¿µæ··åˆï¼ˆCoCoMixï¼‰ï¼Œè¿™æ˜¯ä¸€ç§æ–°å‹çš„é¢„è®­ç»ƒæ¡†æ¶ï¼Œç»“åˆäº†ç¦»æ•£ä¸‹ä¸€ä¸ªè¯é¢„æµ‹å’Œè¿ç»­æ¦‚å¿µã€‚å…·ä½“æ¥è¯´ï¼ŒCoCoMixé¢„æµ‹ä»é¢„è®­ç»ƒçš„ç¨€ç–è‡ªåŠ¨ç¼–ç å™¨ä¸­å­¦å¾—çš„è¿ç»­æ¦‚å¿µï¼Œå¹¶é€šè¿‡ä¸ä»¤ç‰Œéšè—è¡¨ç¤ºäº¤æ›¿çš„æ–¹å¼å°†å®ƒä»¬æ··åˆåˆ°æ¨¡å‹çš„éšè—çŠ¶æ€ã€‚é€šè¿‡åŒ…æ‹¬è¯­è¨€å»ºæ¨¡å’Œä¸‹æ¸¸æ¨ç†ä»»åŠ¡ç­‰å¤šä¸ªåŸºå‡†å®éªŒï¼Œæˆ‘ä»¬è¯æ˜äº†CoCoMixçš„æ ·æœ¬æ•ˆç‡æ›´é«˜ï¼Œå¹¶ä¸”å§‹ç»ˆä¼˜äºæ ‡å‡†ä¸‹ä¸€ä¸ªè¯é¢„æµ‹ã€çŸ¥è¯†è’¸é¦å’Œæ’å…¥æš‚åœä»¤ç‰Œã€‚æˆ‘ä»¬å‘ç°åœ¨ä¸€ä¸ªç«¯åˆ°ç«¯çš„æ¡†æ¶ä¸­ç»“åˆæ¦‚å¿µå­¦ä¹ å’Œäº¤æ›¿æ–¹å¼å¯¹äºæ€§èƒ½æå‡è‡³å…³é‡è¦ã€‚æ­¤å¤–ï¼ŒCoCoMixé€šè¿‡å…è®¸ç›´æ¥æ£€æŸ¥å’Œä¿®æ”¹é¢„æµ‹çš„æ¦‚å¿µï¼Œæä¾›äº†ä¸€ç§é€æ˜çš„æ–¹å¼æ¥å¼•å¯¼æ¨¡å‹çš„å†…éƒ¨æ¨ç†è¿‡ç¨‹ï¼Œä»è€Œå¢å¼ºäº†å¯è§£é‡Šæ€§å’Œå¯æ§åˆ¶æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.08524v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ä¸ªæ–°çš„é¢„è®­ç»ƒæ¡†æ¶â€”â€”è¿ç»­æ¦‚å¿µæ··åˆï¼ˆCoCoMixï¼‰ï¼Œç»“åˆäº†ç¦»æ•£ä¸‹ä¸€ä¸ªä»¤ç‰Œé¢„æµ‹å’Œè¿ç»­æ¦‚å¿µé¢„æµ‹ã€‚CoCoMixåˆ©ç”¨é¢„è®­ç»ƒçš„ç¨€ç–è‡ªåŠ¨ç¼–ç å™¨å­¦ä¹ è¿ç»­æ¦‚å¿µï¼Œå¹¶é€šè¿‡äº¤é”™æ–¹å¼ä¸ä»¤ç‰Œéšè—è¡¨ç¤ºæ··åˆåˆ°æ¨¡å‹çš„éšè—çŠ¶æ€ä¸­ã€‚å®éªŒè¡¨æ˜ï¼ŒCoCoMixåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºæ›´é«˜çš„æ ·æœ¬æ•ˆç‡å’Œæ€§èƒ½ï¼Œä¼˜äºæ ‡å‡†ä¸‹ä¸€ä¸ªä»¤ç‰Œé¢„æµ‹ã€çŸ¥è¯†è’¸é¦å’Œæ’å…¥æš‚åœä»¤ç‰Œç­‰æ–¹æ³•ã€‚ç»“åˆæ¦‚å¿µå­¦ä¹ å’Œäº¤é”™åœ¨ç«¯åˆ°ç«¯æ¡†æ¶ä¸­å¯¹æ€§èƒ½æå‡è‡³å…³é‡è¦ã€‚æ­¤å¤–ï¼ŒCoCoMixé€šè¿‡å…è®¸ç›´æ¥æ£€æŸ¥å’Œä¿®æ”¹é¢„æµ‹çš„æ¦‚å¿µï¼Œæä¾›äº†é€æ˜çš„æŒ‡å¯¼æ¨¡å‹å†…éƒ¨æ¨ç†è¿‡ç¨‹çš„æ–¹å¼ï¼Œå¢å¼ºäº†æ¨¡å‹çš„è§£é‡Šæ€§å’Œå¯æ“æ§æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>CoCoMixæ˜¯ä¸€ä¸ªç»“åˆç¦»æ•£ä¸‹ä¸€ä¸ªä»¤ç‰Œé¢„æµ‹å’Œè¿ç»­æ¦‚å¿µé¢„æµ‹çš„æ–°å‹é¢„è®­ç»ƒæ¡†æ¶ã€‚</li>
<li>CoCoMixåˆ©ç”¨é¢„è®­ç»ƒçš„ç¨€ç–è‡ªåŠ¨ç¼–ç å™¨å­¦ä¹ è¿ç»­æ¦‚å¿µã€‚</li>
<li>å®éªŒè¯æ˜CoCoMixåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­æ ·æœ¬æ•ˆç‡æ›´é«˜ï¼Œæ€§èƒ½æ›´å¥½ã€‚</li>
<li>ç»“åˆæ¦‚å¿µå­¦ä¹ å’Œäº¤é”™åœ¨ç«¯åˆ°ç«¯æ¡†æ¶ä¸­å¯¹æ€§èƒ½æå‡æœ‰å…³é”®ä½œç”¨ã€‚</li>
<li>CoCoMixå¢å¼ºäº†æ¨¡å‹çš„è§£é‡Šæ€§å’Œå¯æ“æ§æ€§ï¼Œå…è®¸ç›´æ¥æ£€æŸ¥å’Œä¿®æ”¹é¢„æµ‹çš„æ¦‚å¿µã€‚</li>
<li>CoCoMixé€šè¿‡é¢„æµ‹è¿ç»­æ¦‚å¿µå¹¶æ··åˆåˆ°æ¨¡å‹çš„éšè—çŠ¶æ€ä¸­ï¼Œæé«˜äº†æ¨¡å‹çš„æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.08524">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-f0e9da3de2b74fafc988201a1440c723.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-eec4666a1b32bf097331480d03e4c851.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-cf2442e25cae62866b1ebf54cd233edb.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-dd60156267912356b7279cd228375932.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-aada9a74a8fd543c04099431ada28d17.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4163866fd21a570b74fa91961aab25e7.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Measuring-Diversity-in-Synthetic-Datasets"><a href="#Measuring-Diversity-in-Synthetic-Datasets" class="headerlink" title="Measuring Diversity in Synthetic Datasets"></a>Measuring Diversity in Synthetic Datasets</h2><p><strong>Authors:Yuchang Zhu, Huizhe Zhang, Bingzhe Wu, Jintang Li, Zibin Zheng, Peilin Zhao, Liang Chen, Yatao Bian</strong></p>
<p>Large language models (LLMs) are widely adopted to generate synthetic datasets for various natural language processing (NLP) tasks, such as text classification and summarization. However, accurately measuring the diversity of these synthetic datasets-an aspect crucial for robust model performance-remains a significant challenge. In this paper, we introduce DCScore, a novel method for measuring synthetic dataset diversity from a classification perspective. Specifically, DCScore formulates diversity evaluation as a sample classification task, leveraging mutual relationships among samples. We further provide theoretical verification of the diversity-related axioms satisfied by DCScore, highlighting its role as a principled diversity evaluation method. Experimental results on synthetic datasets reveal that DCScore enjoys a stronger correlation with multiple diversity pseudo-truths of evaluated datasets, underscoring its effectiveness. Moreover, both empirical and theoretical evidence demonstrate that DCScore substantially reduces computational costs compared to existing approaches. Code is available at: <a target="_blank" rel="noopener" href="https://github.com/BlueWhaleLab/DCScore">https://github.com/BlueWhaleLab/DCScore</a>. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è¢«å¹¿æ³›ç”¨äºç”Ÿæˆå„ç§è‡ªç„¶è¯­è¨€å¤„ç†ï¼ˆNLPï¼‰ä»»åŠ¡ï¼ˆå¦‚æ–‡æœ¬åˆ†ç±»å’Œæ‘˜è¦ï¼‰çš„åˆæˆæ•°æ®é›†ã€‚ç„¶è€Œï¼Œå‡†ç¡®æµ‹é‡è¿™äº›åˆæˆæ•°æ®é›†çš„å¤šæ ·æ€§â€”â€”å¯¹æ¨¡å‹ç¨³å¥æ€§èƒ½è‡³å…³é‡è¦çš„ä¸€ä¸ªæ–¹é¢â€”â€”ä»ç„¶æ˜¯ä¸€ä¸ªå·¨å¤§çš„æŒ‘æˆ˜ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†DCScoreï¼Œè¿™æ˜¯ä¸€ç§ä»åˆ†ç±»è§’åº¦è¡¡é‡åˆæˆæ•°æ®é›†å¤šæ ·æ€§çš„æ–°æ–¹æ³•ã€‚å…·ä½“æ¥è¯´ï¼ŒDCScoreå°†å¤šæ ·æ€§è¯„ä¼°åˆ¶å®šä¸ºæ ·æœ¬åˆ†ç±»ä»»åŠ¡ï¼Œåˆ©ç”¨æ ·æœ¬ä¹‹é—´çš„ç›¸äº’ä½œç”¨å…³ç³»ã€‚æˆ‘ä»¬è¿˜æä¾›äº†DCScoreæ»¡è¶³çš„å¤šæ ·æ€§å…¬ç†çš„ç†è®ºéªŒè¯ï¼Œå¼ºè°ƒäº†å…¶ä½œä¸ºæœ‰åŸåˆ™çš„å¤šæ ·æ€§è¯„ä¼°æ–¹æ³•çš„ä½œç”¨ã€‚åœ¨åˆæˆæ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼ŒDCScoreä¸è¯„ä¼°æ•°æ®é›†çš„å¤šä¸ªå¤šæ ·æ€§ä¼ªçœŸç›¸å…·æœ‰æ›´å¼ºçš„ç›¸å…³æ€§ï¼Œè¿™çªå‡ºäº†å…¶æœ‰æ•ˆæ€§ã€‚æ­¤å¤–ï¼Œå®è¯å’Œç†è®ºè¯æ®éƒ½è¡¨æ˜ï¼ŒDCScoreä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”å¤§å¤§å‡å°‘äº†è®¡ç®—æˆæœ¬ã€‚ä»£ç å¯ä»ä»¥ä¸‹ç½‘å€è·å–ï¼š<a target="_blank" rel="noopener" href="https://github.com/BlueWhaleLab/DCScore%E3%80%82">https://github.com/BlueWhaleLab/DCScoreã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.08512v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ç”Ÿæˆçš„åˆæˆæ•°æ®é›†å¹¿æ³›åº”ç”¨äºè‡ªç„¶è¯­è¨€å¤„ç†ï¼ˆNLPï¼‰ä»»åŠ¡ï¼Œå¦‚æ–‡æœ¬åˆ†ç±»å’Œæ‘˜è¦ã€‚ç„¶è€Œï¼Œå‡†ç¡®æµ‹é‡è¿™äº›åˆæˆæ•°æ®é›†çš„å¤šæ ·æ€§å¯¹äºæ¨¡å‹çš„ç¨³å¥æ€§èƒ½è‡³å…³é‡è¦ï¼Œä»æ˜¯ä¸€ä¸ªå·¨å¤§æŒ‘æˆ˜ã€‚æœ¬æ–‡ä»‹ç»äº†ä¸€ç§æ–°çš„æµ‹é‡åˆæˆæ•°æ®é›†å¤šæ ·æ€§çš„æ–¹æ³•DCScoreï¼Œä»åˆ†ç±»çš„è§’åº¦è¿›è¡Œè¯„ä¼°ã€‚DCScoreå°†å¤šæ ·æ€§è¯„ä¼°åˆ¶å®šä¸ºæ ·æœ¬åˆ†ç±»ä»»åŠ¡ï¼Œåˆ©ç”¨æ ·æœ¬é—´çš„ç›¸äº’å…³è”ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒDCScoreä¸è¯„ä¼°æ•°æ®é›†çš„å¤šä¸ªå¤šæ ·æ€§ä¼ªçœŸå®å€¼å…·æœ‰æ›´å¼ºçš„ç›¸å…³æ€§ï¼Œå‡¸æ˜¾äº†å…¶æœ‰æ•ˆæ€§ã€‚åŒæ—¶ï¼ŒDCScoreåœ¨ç†è®ºå’Œå®è·µè¯æ®æ–¹é¢éƒ½æ˜¾è‘—é™ä½äº†è®¡ç®—æˆæœ¬ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLMsè¢«å¹¿æ³›ç”¨äºç”Ÿæˆåˆæˆæ•°æ®é›†ï¼Œç”¨äºNLPä»»åŠ¡å¦‚æ–‡æœ¬åˆ†ç±»å’Œæ‘˜è¦ã€‚</li>
<li>åˆæˆæ•°æ®é›†çš„å¤šæ ·æ€§æµ‹é‡æ˜¯ä¸€ä¸ªå…³é”®æŒ‘æˆ˜ï¼Œå¯¹æ¨¡å‹æ€§èƒ½æœ‰é‡è¦å½±å“ã€‚</li>
<li>DCScoreæ˜¯ä¸€ç§æ–°çš„æµ‹é‡åˆæˆæ•°æ®é›†å¤šæ ·æ€§çš„æ–¹æ³•ï¼Œå°†å¤šæ ·æ€§è¯„ä¼°è½¬åŒ–ä¸ºæ ·æœ¬åˆ†ç±»ä»»åŠ¡ã€‚</li>
<li>DCScoreåˆ©ç”¨æ ·æœ¬é—´çš„ç›¸äº’å…³ç³»æ¥è¯„ä¼°å¤šæ ·æ€§ã€‚</li>
<li>å®éªŒè¯æ˜DCScoreä¸å¤šä¸ªå¤šæ ·æ€§ä¼ªçœŸå®å€¼çš„ç›¸å…³æ€§æ›´å¼ºï¼Œè¡¨æ˜å…¶æœ‰æ•ˆæ€§ã€‚</li>
<li>DCScoreåœ¨ç†è®ºå’Œå®è·µä¸¤ä¸ªæ–¹é¢éƒ½æ˜¾è‘—é™ä½äº†è®¡ç®—æˆæœ¬ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.08512">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-7732b2346ec91e0f57718760fc6e0c94.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d1885554bc64b73bd6dd4ca283f73225.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-31ea643413fc754f045d1d040b8f99c5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-592b7660c08ef2218181b0d8999ed799.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-28969921f16c2568399d8cbe2de8441c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-08b57b6ae0e097c43cb4927383a2adde.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Explanation-based-In-Context-Demonstrations-Retrieval-for-Multilingual-Grammatical-Error-Correction"><a href="#Explanation-based-In-Context-Demonstrations-Retrieval-for-Multilingual-Grammatical-Error-Correction" class="headerlink" title="Explanation based In-Context Demonstrations Retrieval for Multilingual   Grammatical Error Correction"></a>Explanation based In-Context Demonstrations Retrieval for Multilingual   Grammatical Error Correction</h2><p><strong>Authors:Wei Li, Wen Luo, Guangyue Peng, Houfeng Wang</strong></p>
<p>Grammatical error correction (GEC) aims to correct grammatical, spelling, and semantic errors in natural language text. With the growing of large language models (LLMs), direct text generation has gradually become the focus of the GEC methods, and few-shot in-context learning presents a cost-effective solution. However, selecting effective in-context examples remains challenging, as the similarity between input texts does not necessarily correspond to similar grammatical error patterns. In this paper, we propose a novel retrieval method based on natural language grammatical error explanations (GEE) to address this issue. Our method retrieves suitable few-shot demonstrations by matching the GEE of the test input with that of pre-constructed database samples, where explanations for erroneous samples are generated by LLMs. We conducted multilingual GEC few-shot experiments on both major open-source and closed-source LLMs. Experiments across five languages show that our method outperforms existing semantic and BM25-based retrieval techniques, without requiring additional training or language adaptation. This also suggests that matching error patterns is key to selecting examples. </p>
<blockquote>
<p>è¯­æ³•é”™è¯¯ä¿®æ­£ï¼ˆGECï¼‰æ—¨åœ¨ä¿®æ­£è‡ªç„¶è¯­è¨€æ–‡æœ¬ä¸­çš„è¯­æ³•ã€æ‹¼å†™å’Œè¯­ä¹‰é”™è¯¯ã€‚éšç€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„å¢é•¿ï¼Œç›´æ¥æ–‡æœ¬ç”Ÿæˆé€æ¸æˆä¸ºGECæ–¹æ³•çš„ç„¦ç‚¹ï¼Œè€Œå°‘é‡ä¸Šä¸‹æ–‡å­¦ä¹ æä¾›äº†ä¸€ç§å…·æœ‰æˆæœ¬æ•ˆç›Šçš„è§£å†³æ–¹æ¡ˆã€‚ç„¶è€Œï¼Œé€‰æ‹©æœ‰æ•ˆçš„ä¸Šä¸‹æ–‡ç¤ºä¾‹ä»ç„¶å…·æœ‰æŒ‘æˆ˜æ€§ï¼Œå› ä¸ºè¾“å…¥æ–‡æœ¬ä¹‹é—´çš„ç›¸ä¼¼æ€§å¹¶ä¸ä¸€å®šå¯¹åº”ç›¸ä¼¼çš„è¯­æ³•é”™è¯¯æ¨¡å¼ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºè‡ªç„¶è¯­è¨€è¯­æ³•é”™è¯¯è§£é‡Šï¼ˆGEEï¼‰çš„æ–°å‹æ£€ç´¢æ–¹æ³•æ¥è§£å†³è¿™ä¸ªé—®é¢˜ã€‚æˆ‘ä»¬çš„æ–¹æ³•é€šè¿‡åŒ¹é…æµ‹è¯•è¾“å…¥çš„GEEä¸é¢„å…ˆæ„å»ºçš„æ•°æ®åº“æ ·æœ¬çš„GEEæ¥æ£€ç´¢åˆé€‚çš„å°‘é‡ç¤ºä¾‹ï¼Œå…¶ä¸­é”™è¯¯æ ·æœ¬çš„è§£é‡Šæ˜¯ç”±LLMç”Ÿæˆçš„ã€‚æˆ‘ä»¬åœ¨ä¸»è¦çš„å¼€æºå’Œé—­æºLLMä¸Šè¿›è¡Œäº†å¤šè¯­è¨€GECå°‘é‡å®éªŒã€‚è·¨è¶Šäº”ç§è¯­è¨€çš„å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•ä¼˜äºç°æœ‰çš„è¯­ä¹‰å’ŒåŸºäºBM25çš„æ£€ç´¢æŠ€æœ¯ï¼Œä¸”æ— éœ€é¢å¤–çš„è®­ç»ƒæˆ–è¯­è¨€é€‚åº”ã€‚è¿™ä¹Ÿè¡¨æ˜åŒ¹é…é”™è¯¯æ¨¡å¼æ˜¯é€‰æ‹©ç¤ºä¾‹çš„å…³é”®ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.08507v1">PDF</a> Accepted by NAACL 2025 main conference</p>
<p><strong>Summary</strong><br>æ–‡æœ¬ä¸»è¦ä»‹ç»äº†åŸºäºè‡ªç„¶è¯­è¨€è¯­æ³•é”™è¯¯è§£é‡Šï¼ˆGEEï¼‰çš„æ£€ç´¢æ–¹æ³•åœ¨è¯­æ³•é”™è¯¯ä¿®æ­£ï¼ˆGECï¼‰ä¸­çš„åº”ç”¨ã€‚è¯¥æ–¹æ³•é€šè¿‡åŒ¹é…æµ‹è¯•è¾“å…¥çš„GEEä¸é¢„æ„å»ºçš„æ•°æ®åº“æ ·æœ¬ï¼Œæ£€ç´¢å‡ºåˆé€‚çš„å°‘é‡ç¤ºä¾‹ã€‚å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å¤šè¯­è¨€GECä¸­çš„è¡¨ç°ä¼˜äºç°æœ‰çš„è¯­ä¹‰å’ŒBM25åŸºäºæ£€ç´¢æŠ€æœ¯çš„æ£€ç´¢æ–¹æ³•ï¼Œä¸”æ— éœ€é¢å¤–çš„è®­ç»ƒæˆ–è¯­è¨€é€‚åº”ã€‚è¿™è¡¨æ˜åŒ¹é…é”™è¯¯æ¨¡å¼æ˜¯é€‰æ‹©ç¤ºä¾‹çš„å…³é”®ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è¯­æ³•é”™è¯¯ä¿®æ­£ï¼ˆGECï¼‰æ—¨åœ¨çº æ­£è‡ªç„¶è¯­è¨€æ–‡æœ¬ä¸­çš„è¯­æ³•ã€æ‹¼å†™å’Œè¯­ä¹‰é”™è¯¯ã€‚</li>
<li>éšç€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„å‘å±•ï¼Œç›´æ¥æ–‡æœ¬ç”Ÿæˆé€æ¸æˆä¸ºGECæ–¹æ³•çš„é‡ç‚¹ã€‚</li>
<li>å°‘æ ·æœ¬ä¸Šä¸‹æ–‡å­¦ä¹ ä¸ºGECæä¾›äº†ä¸€ä¸ªå…·æœ‰æˆæœ¬æ•ˆç›Šçš„è§£å†³æ–¹æ¡ˆã€‚</li>
<li>é€‰æ‹©æœ‰æ•ˆçš„ä¸Šä¸‹æ–‡ç¤ºä¾‹ä»ç„¶å…·æœ‰æŒ‘æˆ˜æ€§ï¼Œå› ä¸ºè¾“å…¥æ–‡æœ¬ä¹‹é—´çš„ç›¸ä¼¼æ€§å¹¶ä¸ä¸€å®šå¯¹åº”ç›¸ä¼¼çš„è¯­æ³•é”™è¯¯æ¨¡å¼ã€‚</li>
<li>æå‡ºäº†ä¸€ç§åŸºäºè‡ªç„¶è¯­è¨€è¯­æ³•é”™è¯¯è§£é‡Šï¼ˆGEEï¼‰çš„æ£€ç´¢æ–¹æ³•æ¥è§£å†³è¿™ä¸ªé—®é¢˜ã€‚</li>
<li>è¯¥æ–¹æ³•é€šè¿‡åŒ¹é…æµ‹è¯•è¾“å…¥çš„GEEä¸é¢„æ„å»ºçš„æ•°æ®åº“æ ·æœ¬ï¼Œæ£€ç´¢å‡ºåˆé€‚çš„å°‘é‡ç¤ºä¾‹ï¼Œä»¥è¿›è¡ŒGECã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.08507">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-bba5ff79cc04768b1ee6f363fe5d75fc.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-9264654bc5093f0686d4831d9a32b813.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-cba8b3293070385218ad7bba4215f106.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-bcc7da750505da19c6dafe5ee22f44b5.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="mmE5-Improving-Multimodal-Multilingual-Embeddings-via-High-quality-Synthetic-Data"><a href="#mmE5-Improving-Multimodal-Multilingual-Embeddings-via-High-quality-Synthetic-Data" class="headerlink" title="mmE5: Improving Multimodal Multilingual Embeddings via High-quality   Synthetic Data"></a>mmE5: Improving Multimodal Multilingual Embeddings via High-quality   Synthetic Data</h2><p><strong>Authors:Haonan Chen, Liang Wang, Nan Yang, Yutao Zhu, Ziliang Zhao, Furu Wei, Zhicheng Dou</strong></p>
<p>Multimodal embedding models have gained significant attention for their ability to map data from different modalities, such as text and images, into a unified representation space. However, the limited labeled multimodal data often hinders embedding performance. Recent approaches have leveraged data synthesis to address this problem, yet the quality of synthetic data remains a critical bottleneck. In this work, we identify three criteria for high-quality synthetic multimodal data. First, broad scope ensures that the generated data covers diverse tasks and modalities, making it applicable to various downstream scenarios. Second, robust cross-modal alignment makes different modalities semantically consistent. Third, high fidelity ensures that the synthetic data maintains realistic details to enhance its reliability. Guided by these principles, we synthesize datasets that: (1) cover a wide range of tasks, modality combinations, and languages, (2) are generated via a deep thinking process within a single pass of a multimodal large language model, and (3) incorporate real-world images with accurate and relevant texts, ensuring fidelity through self-evaluation and refinement. Leveraging these high-quality synthetic and labeled datasets, we train a multimodal multilingual E5 model mmE5. Extensive experiments demonstrate that mmE5 achieves state-of-the-art performance on the MMEB Benchmark and superior multilingual performance on the XTD benchmark. Our codes, datasets and models are released in <a target="_blank" rel="noopener" href="https://github.com/haon-chen/mmE5">https://github.com/haon-chen/mmE5</a>. </p>
<blockquote>
<p>å¤šæ¨¡æ€åµŒå…¥æ¨¡å‹å› å…¶èƒ½å¤Ÿå°†æ–‡æœ¬å’Œå›¾åƒç­‰ä¸åŒæ¨¡æ€çš„æ•°æ®æ˜ å°„åˆ°ç»Ÿä¸€è¡¨ç¤ºç©ºé—´çš„èƒ½åŠ›è€Œå—åˆ°å¹¿æ³›å…³æ³¨ã€‚ç„¶è€Œï¼Œæœ‰é™çš„æ ‡è®°å¤šæ¨¡æ€æ•°æ®é€šå¸¸ä¼šå½±å“åµŒå…¥æ€§èƒ½ã€‚æœ€è¿‘çš„æ–¹æ³•åˆ©ç”¨æ•°æ®åˆæˆæ¥è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œä½†åˆæˆæ•°æ®çš„è´¨é‡ä»ç„¶æ˜¯å…³é”®ç“¶é¢ˆã€‚åœ¨æˆ‘ä»¬çš„ç ”ç©¶ä¸­ï¼Œæˆ‘ä»¬ç¡®å®šäº†é«˜è´¨é‡åˆæˆå¤šæ¨¡æ€æ•°æ®çš„ä¸‰ä¸ªæ ‡å‡†ã€‚é¦–å…ˆï¼Œå¹¿æ³›è¦†ç›–è¦æ±‚ç”Ÿæˆçš„æ•°æ®æ¶µç›–å„ç§ä»»åŠ¡å’Œæ¨¡æ€ï¼Œä½¿å…¶é€‚ç”¨äºå„ç§ä¸‹æ¸¸åœºæ™¯ã€‚å…¶æ¬¡ï¼Œç¨³å¥çš„è·¨æ¨¡æ€å¯¹é½ä½¿å¾—ä¸åŒæ¨¡æ€åœ¨è¯­ä¹‰ä¸Šä¿æŒä¸€è‡´ã€‚ç¬¬ä¸‰ï¼Œé«˜ä¿çœŸåº¦ç¡®ä¿åˆæˆæ•°æ®ä¿æŒç°å®ä¸–ç•Œçš„ç»†èŠ‚ï¼Œä»¥æé«˜å…¶å¯é æ€§ã€‚éµå¾ªè¿™äº›åŸåˆ™ï¼Œæˆ‘ä»¬åˆæˆçš„æ•°æ®é›†åŒ…æ‹¬ï¼šï¼ˆ1ï¼‰æ¶µç›–å¹¿æ³›çš„ä»»åŠ¡ã€æ¨¡æ€ç»„åˆå’Œè¯­è¨€ï¼›ï¼ˆ2ï¼‰é€šè¿‡å•é€šé“å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹çš„æ·±åº¦æ€è€ƒè¿‡ç¨‹ç”Ÿæˆï¼›ï¼ˆ3ï¼‰ç»“åˆç°å®ä¸–ç•Œçš„å›¾åƒå’Œå‡†ç¡®ç›¸å…³çš„æ–‡æœ¬ï¼Œé€šè¿‡è‡ªæˆ‘è¯„ä¼°å’Œç²¾ç‚¼ç¡®ä¿ä¿çœŸåº¦ã€‚åˆ©ç”¨è¿™äº›é«˜è´¨é‡åˆæˆå’Œæ ‡è®°çš„æ•°æ®é›†ï¼Œæˆ‘ä»¬è®­ç»ƒäº†ä¸€ä¸ªå¤šæ¨¡æ€å¤šè¯­è¨€E5æ¨¡å‹mmE5ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼ŒmmE5åœ¨MMEBåŸºå‡†æµ‹è¯•ä¸Šè¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œå¹¶åœ¨XTDåŸºå‡†æµ‹è¯•ä¸Šè¡¨ç°å‡ºå“è¶Šçš„å¤šè¯­è¨€èƒ½åŠ›ã€‚æˆ‘ä»¬çš„ä»£ç ã€æ•°æ®é›†å’Œæ¨¡å‹å·²åœ¨<a target="_blank" rel="noopener" href="https://github.com/haon-chen/mmE5%E4%B8%8A%E5%8F%91%E5%B8%83%E3%80%82">https://github.com/haon-chen/mmE5ä¸Šå‘å¸ƒã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.08468v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡å…³æ³¨å¤šæ¨¡æ€åµŒå…¥æ¨¡å‹åœ¨æ•°æ®æ˜ å°„æ–¹é¢çš„èƒ½åŠ›ï¼Œç‰¹åˆ«æ˜¯åœ¨å¤„ç†ä¸åŒæ¨¡æ€æ•°æ®ï¼ˆå¦‚æ–‡æœ¬å’Œå›¾åƒï¼‰æ—¶é¢ä¸´çš„æŒ‘æˆ˜ã€‚é’ˆå¯¹æœ‰é™æ ‡è®°å¤šæ¨¡æ€æ•°æ®çš„é—®é¢˜ï¼Œç ”ç©¶é€šè¿‡æ•°æ®åˆæˆæ¥è§£å†³ï¼ŒåŒæ—¶å¼ºè°ƒé«˜è´¨é‡åˆæˆæ•°æ®çš„é‡è¦æ€§ã€‚æå‡ºä¸‰ä¸ªæ ‡å‡†ï¼šèŒƒå›´å¹¿æ³›ã€è·¨æ¨¡æ€å¯¹é½å’Œé«˜è´¨é‡ä¿çœŸã€‚åŸºäºè¿™äº›æ ‡å‡†ï¼Œç ”ç©¶å›¢é˜Ÿåˆæˆäº†ä¸€ç³»åˆ—æ•°æ®é›†å¹¶è®­ç»ƒäº†mmE5æ¨¡å‹ã€‚è¯¥æ¨¡å‹åœ¨MMEBå’ŒXTDåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºå“è¶Šæ€§èƒ½ã€‚ç›¸å…³ä»£ç å’Œæ•°æ®é›†å·²å‘å¸ƒåœ¨[é“¾æ¥åœ°å€]ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤šæ¨¡æ€åµŒå…¥æ¨¡å‹èƒ½å¤Ÿå°†ä¸åŒæ¨¡æ€çš„æ•°æ®æ˜ å°„åˆ°ç»Ÿä¸€è¡¨ç¤ºç©ºé—´ã€‚</li>
<li>æœ‰é™çš„æ ‡è®°å¤šæ¨¡æ€æ•°æ®é™åˆ¶äº†åµŒå…¥æ¨¡å‹çš„æ€§èƒ½ã€‚</li>
<li>æ•°æ®åˆæˆæ˜¯è§£å†³æ­¤é—®é¢˜çš„ä¸€ç§ç­–ç•¥ï¼Œä½†åˆæˆæ•°æ®çš„è´¨é‡æ˜¯å…³é”®ç“¶é¢ˆã€‚</li>
<li>é«˜è´¨é‡åˆæˆæ•°æ®çš„ä¸‰ä¸ªæ ‡å‡†æ˜¯ï¼šèŒƒå›´å¹¿æ³›ã€è·¨æ¨¡æ€å¯¹é½å’Œé«˜è´¨é‡ä¿çœŸã€‚</li>
<li>ç ”ç©¶å›¢é˜ŸåŸºäºè¿™äº›æ ‡å‡†åˆæˆäº†ä¸€ç³»åˆ—æ•°æ®é›†ï¼Œå¹¶è®­ç»ƒäº†mmE5æ¨¡å‹ã€‚</li>
<li>mmE5åœ¨MMEBå’ŒXTDåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºå“è¶Šæ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.08468">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-529193948940e795ffcfaba6cf38d591.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0e9492d16887d665286ba3a7e0c89504.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c02f76fc8e5ab12ea89d746596192145.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f6ae11e504487ffa4c478ee9d210c4f3.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a01ddc6ef2945f886948fb31def61874.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="From-Haystack-to-Needle-Label-Space-Reduction-for-Zero-shot-Classification"><a href="#From-Haystack-to-Needle-Label-Space-Reduction-for-Zero-shot-Classification" class="headerlink" title="From Haystack to Needle: Label Space Reduction for Zero-shot   Classification"></a>From Haystack to Needle: Label Space Reduction for Zero-shot   Classification</h2><p><strong>Authors:Nathan Vandemoortele, Bram Steenwinckel, Femke Ongenae, Sofie Van Hoecke</strong></p>
<p>We present Label Space Reduction (LSR), a novel method for improving zero-shot classification performance of Large Language Models (LLMs). LSR iteratively refines the classification label space by systematically ranking and reducing candidate classes, enabling the model to concentrate on the most relevant options. By leveraging unlabeled data with the statistical learning capabilities of data-driven models, LSR dynamically optimizes the label space representation at test time. Our experiments across seven benchmarks demonstrate that LSR improves macro-F1 scores by an average of 7.0% (up to 14.2%) with Llama-3.1-70B and 3.3% (up to 11.1%) with Claude-3.5-Sonnet compared to standard zero-shot classification baselines. To reduce the computational overhead of LSR, which requires an additional LLM call at each iteration, we propose distilling the model into a probabilistic classifier, allowing for efficient inference. </p>
<blockquote>
<p>æˆ‘ä»¬æå‡ºäº†ä¸€ç§åä¸ºæ ‡ç­¾ç©ºé—´ç¼©å‡ï¼ˆLSRï¼‰çš„æ–°æ–¹æ³•ï¼Œç”¨äºæå‡å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„é›¶æ ·æœ¬åˆ†ç±»æ€§èƒ½ã€‚LSRé€šè¿‡ç³»ç»Ÿåœ°æ’åºå’Œç¼©å‡å€™é€‰ç±»åˆ«ï¼Œå¯¹åˆ†ç±»æ ‡ç­¾ç©ºé—´è¿›è¡Œè¿­ä»£ä¼˜åŒ–ï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿä¸“æ³¨äºæœ€ç›¸å…³çš„é€‰é¡¹ã€‚è¯¥æ–¹æ³•åˆ©ç”¨æ— æ ‡ç­¾æ•°æ®ä»¥åŠæ•°æ®é©±åŠ¨æ¨¡å‹çš„ç»Ÿè®¡å­¦ä¹ èƒ½åŠ›ï¼Œåœ¨æµ‹è¯•æ—¶åŠ¨æ€ä¼˜åŒ–æ ‡ç­¾ç©ºé—´è¡¨ç¤ºã€‚æˆ‘ä»¬åœ¨ä¸ƒä¸ªåŸºå‡†æµ‹è¯•ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œä¸æ ‡å‡†çš„é›¶æ ·æœ¬åˆ†ç±»åŸºçº¿ç›¸æ¯”ï¼ŒLSRä½¿ç”¨Llama-3.1-70Bå°†å®F1åˆ†æ•°å¹³å‡æé«˜äº†7.0%ï¼ˆæœ€é«˜è¾¾14.2%ï¼‰ï¼Œä½¿ç”¨Claude-3.5-Sonnetæé«˜äº†3.3%ï¼ˆæœ€é«˜è¾¾11.1%ï¼‰ã€‚ä¸ºäº†å‡å°‘LSRçš„è®¡ç®—å¼€é”€ï¼ˆéœ€è¦åœ¨æ¯æ¬¡è¿­ä»£æ—¶è¿›è¡Œé¢å¤–çš„LLMè°ƒç”¨ï¼‰ï¼Œæˆ‘ä»¬æå‡ºå°†æ¨¡å‹è’¸é¦æˆæ¦‚ç‡åˆ†ç±»å™¨ï¼Œä»¥å®ç°é«˜æ•ˆæ¨ç†ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.08436v1">PDF</a> Under review at ICML 2025</p>
<p><strong>æ‘˜è¦</strong><br>    æœ¬æ–‡æå‡ºäº†Label Space Reductionï¼ˆLSRï¼‰è¿™ä¸€æ–°æ–¹æ³•ï¼Œæ—¨åœ¨æé«˜å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„é›¶æ ·æœ¬åˆ†ç±»æ€§èƒ½ã€‚LSRé€šè¿‡ç³»ç»Ÿåœ°å¯¹å€™é€‰ç±»åˆ«è¿›è¡Œæ’åå’Œå‡å°‘ï¼Œè¿­ä»£åœ°ä¼˜åŒ–åˆ†ç±»æ ‡ç­¾ç©ºé—´ï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿä¸“æ³¨äºæœ€ç›¸å…³çš„é€‰é¡¹ã€‚è¯¥æ–¹æ³•åˆ©ç”¨æ— æ ‡ç­¾æ•°æ®çš„ç»Ÿè®¡å­¦ä¹ èƒ½åŠ›ï¼Œåœ¨æµ‹è¯•æ—¶åŠ¨æ€ä¼˜åŒ–æ ‡ç­¾ç©ºé—´è¡¨ç¤ºã€‚å®éªŒè¡¨æ˜ï¼Œåœ¨ä¸ƒä¸ªåŸºå‡†æµ‹è¯•ä¸­ï¼ŒLSRä¸æ ‡å‡†é›¶æ ·æœ¬åˆ†ç±»åŸºçº¿ç›¸æ¯”ï¼Œä½¿ç”¨Llama-3.1-70Bæ—¶çš„å®è§‚F1åˆ†æ•°å¹³å‡æé«˜äº†7.0%ï¼ˆæœ€é«˜è¾¾14.2%ï¼‰ï¼Œä½¿ç”¨Claude-3.5-Sonnetæ—¶æé«˜äº†3.3%ï¼ˆæœ€é«˜è¾¾11.1%ï¼‰ã€‚ä¸ºäº†é™ä½LSRçš„è®¡ç®—å¼€é”€ï¼ˆæ¯æ¬¡è¿­ä»£éœ€è¦é¢å¤–çš„LLMè°ƒç”¨ï¼‰ï¼Œæˆ‘ä»¬æå‡ºäº†å°†å…¶è’¸é¦æˆæ¦‚ç‡åˆ†ç±»å™¨çš„æ–¹æ³•ï¼Œä»¥å®ç°é«˜æ•ˆæ¨ç†ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>LSRæ˜¯ä¸€ç§æé«˜å¤§å‹è¯­è¨€æ¨¡å‹é›¶æ ·æœ¬åˆ†ç±»æ€§èƒ½çš„æ–°æ–¹æ³•ã€‚</li>
<li>LSRé€šè¿‡è¿­ä»£åœ°ä¼˜åŒ–åˆ†ç±»æ ‡ç­¾ç©ºé—´ï¼Œæé«˜äº†æ¨¡å‹çš„å…³æ³¨åº¦å’Œæ€§èƒ½ã€‚</li>
<li>LSRåˆ©ç”¨æ— æ ‡ç­¾æ•°æ®çš„ç»Ÿè®¡å­¦ä¹ èƒ½åŠ›ï¼Œåœ¨æµ‹è¯•æ—¶åŠ¨æ€è°ƒæ•´æ ‡ç­¾ç©ºé—´ã€‚</li>
<li>å®éªŒè¡¨æ˜ï¼ŒLSRåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­æ˜¾è‘—æé«˜äº†å®è§‚F1åˆ†æ•°ã€‚</li>
<li>ä¸æ ‡å‡†é›¶æ ·æœ¬åˆ†ç±»åŸºçº¿ç›¸æ¯”ï¼ŒLSRåœ¨Llama-3.1-70Bä¸Šçš„æ€§èƒ½å¹³å‡æé«˜äº†7.0%ï¼Œåœ¨Claude-3.5-Sonnetä¸Šæé«˜äº†3.3%ã€‚</li>
<li>LSRæ–¹æ³•è™½ç„¶æœ‰æ•ˆï¼Œä½†éœ€è¦é¢å¤–çš„è®¡ç®—å¼€é”€ã€‚</li>
<li>ä¸ºäº†æé«˜æ¨ç†æ•ˆç‡ï¼Œæå‡ºäº†å°†æ¨¡å‹è’¸é¦æˆæ¦‚ç‡åˆ†ç±»å™¨çš„è§£å†³æ–¹æ¡ˆã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.08436">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-7a13bb0da4b21ea5ffe9e5ba952544f5.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-60bbf390f899d0c1a5f30c586bc56668.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7f9581087e599ec30d41f247f4396376.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Tractable-Transformers-for-Flexible-Conditional-Generation"><a href="#Tractable-Transformers-for-Flexible-Conditional-Generation" class="headerlink" title="Tractable Transformers for Flexible Conditional Generation"></a>Tractable Transformers for Flexible Conditional Generation</h2><p><strong>Authors:Anji Liu, Xuejie Liu, Dayuan Zhao, Mathias Niepert, Yitao Liang, Guy Van den Broeck</strong></p>
<p>Non-autoregressive (NAR) generative models are valuable because they can handle diverse conditional generation tasks in a more principled way than their autoregressive (AR) counterparts, which are constrained by sequential dependency requirements. Recent advancements in NAR models, such as diffusion language models, have demonstrated superior performance in unconditional generation compared to AR models (e.g., GPTs) of similar sizes. However, such improvements do not always lead to improved conditional generation performance. We show that a key reason for this gap is the difficulty in generalizing to conditional probability queries unseen during training. As a result, strong unconditional generation performance does not guarantee high-quality conditional generation. This paper proposes Tractable Transformers (Tracformer), a Transformer-based generative model that is more robust to different conditional generation tasks. Unlike existing models that rely solely on global contextual features derived from full inputs, Tracformers incorporate a sparse Transformer encoder to capture both local and global contextual information. This information is routed through a decoder for conditional generation. Empirical results demonstrate that Tracformers achieve state-of-the-art conditional generation performance on text modeling compared to recent diffusion and AR model baselines. </p>
<blockquote>
<p>éè‡ªå›å½’ï¼ˆNARï¼‰ç”Ÿæˆæ¨¡å‹å…·æœ‰ä»·å€¼ï¼Œå› ä¸ºå®ƒä»¬èƒ½å¤Ÿä»¥æ¯”è‡ªå›å½’ï¼ˆARï¼‰æ¨¡å‹æ›´åŸåˆ™åŒ–çš„æ–¹å¼å¤„ç†å¤šç§æ¡ä»¶ç”Ÿæˆä»»åŠ¡ã€‚è‡ªå›å½’æ¨¡å‹å—åˆ°åºåˆ—ä¾èµ–æ€§çš„çº¦æŸã€‚è¿‘æœŸNARæ¨¡å‹çš„è¿›æ­¥ï¼Œå¦‚æ‰©æ•£è¯­è¨€æ¨¡å‹ï¼Œåœ¨æ— æ¡ä»¶ç”Ÿæˆæ–¹é¢è¡¨ç°å‡ºäº†ä¸ç±»ä¼¼è§„æ¨¡çš„ARæ¨¡å‹ï¼ˆå¦‚GPTï¼‰ç›¸æ¯”çš„å“è¶Šæ€§èƒ½ã€‚ç„¶è€Œï¼Œè¿™äº›æ”¹è¿›å¹¶ä¸æ€»æ˜¯å¯¼è‡´æ¡ä»¶ç”Ÿæˆæ€§èƒ½çš„æé«˜ã€‚æˆ‘ä»¬è¡¨æ˜ï¼Œè¿™ä¸€å·®è·çš„å…³é”®åŸå› æ˜¯éš¾ä»¥æ¨å¹¿åˆ°è®­ç»ƒæœŸé—´æœªè§è¿‡çš„æ¡ä»¶æ¦‚ç‡æŸ¥è¯¢ã€‚å› æ­¤ï¼Œå¼ºå¤§çš„æ— æ¡ä»¶ç”Ÿæˆæ€§èƒ½å¹¶ä¸ä¿è¯é«˜è´¨é‡çš„æ¡ä»¶ç”Ÿæˆã€‚æœ¬æ–‡æå‡ºäº†â€œå¯å¤„ç†å˜æ¢å™¨ï¼ˆTracformerï¼‰â€ï¼Œè¿™æ˜¯ä¸€ç§åŸºäºTransformerçš„ç”Ÿæˆæ¨¡å‹ï¼Œå¯¹äºä¸åŒçš„æ¡ä»¶ç”Ÿæˆä»»åŠ¡æ›´å…·é²æ£’æ€§ã€‚ä¸ç°æœ‰ä»…ä¾èµ–å…¨å±€ä¸Šä¸‹æ–‡ç‰¹å¾çš„æ¨¡å‹ä¸åŒï¼ŒTracformersç»“åˆç¨€ç–Transformerç¼–ç å™¨æ¥æ•è·å±€éƒ¨å’Œå…¨å±€ä¸Šä¸‹æ–‡ä¿¡æ¯ã€‚è¿™äº›ä¿¡æ¯é€šè¿‡è§£ç å™¨è¿›è¡Œæ¡ä»¶ç”Ÿæˆã€‚ç»éªŒç»“æœè¡¨æ˜ï¼Œåœ¨æ–‡æœ¬å»ºæ¨¡æ–¹é¢ï¼ŒTracformerså®ç°äº†ä¸æœ€æ–°æ‰©æ•£å’ŒARæ¨¡å‹åŸºçº¿ç›¸æ¯”çš„æœ€å…ˆè¿›æ¡ä»¶ç”Ÿæˆæ€§èƒ½ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.07616v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>è¿‘æœŸéè‡ªå›å½’ï¼ˆNARï¼‰ç”Ÿæˆæ¨¡å‹åœ¨å¤„ç†æ¡ä»¶ç”Ÿæˆä»»åŠ¡æ—¶å±•ç°å‡ºä¼˜åŠ¿ï¼Œå°¤å…¶æ˜¯é€šè¿‡æ‰©æ•£è¯­è¨€æ¨¡å‹ç­‰æ–°æŠ€æœ¯ã€‚å°½ç®¡åœ¨æ— æ¡ä»¶ç”Ÿæˆæ–¹é¢æ€§èƒ½ä¼˜è¶Šï¼Œä½†åœ¨æ¡ä»¶ç”Ÿæˆä¸Šä¸ä¸€å®šä¼˜äºè‡ªå›å½’ï¼ˆARï¼‰æ¨¡å‹ã€‚ç ”ç©¶å‘ç°ï¼Œè¿™æºäºå¯¹æ–°æ¡ä»¶ä¸‹æ¦‚ç‡æŸ¥è¯¢çš„æ³›åŒ–éš¾åº¦ã€‚æœ¬æ–‡æå‡ºTractable Transformersï¼ˆTracformerï¼‰ï¼Œç»“åˆå±€éƒ¨ä¸å…¨å±€ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼Œåœ¨æ¡ä»¶ç”Ÿæˆä¸Šè¡¨ç°å“è¶Šã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>éè‡ªå›å½’ç”Ÿæˆæ¨¡å‹èƒ½æ›´çµæ´»åœ°å¤„ç†å¤šæ ·åŒ–çš„æ¡ä»¶ç”Ÿæˆä»»åŠ¡ã€‚</li>
<li>æ‰©æ•£è¯­è¨€æ¨¡å‹ç­‰éè‡ªå›å½’æ¨¡å‹åœ¨æ— æ¡ä»¶ç”Ÿæˆæ–¹é¢è¡¨ç°ä¼˜äºç±»ä¼¼è§„æ¨¡çš„è‡ªå›å½’æ¨¡å‹ã€‚</li>
<li>ä»…ä¾èµ–å…¨å±€ä¸Šä¸‹æ–‡ç‰¹å¾çš„ç°æœ‰æ¨¡å‹åœ¨æ¡ä»¶ç”Ÿæˆä»»åŠ¡ä¸Šå¯èƒ½å­˜åœ¨å±€é™æ€§ã€‚</li>
<li>æ¡ä»¶ç”Ÿæˆæ€§èƒ½çš„æå‡éœ€è¦æ¨¡å‹å¯¹æ–°æ¡ä»¶ä¸‹æ¦‚ç‡æŸ¥è¯¢çš„æ³›åŒ–èƒ½åŠ›ã€‚</li>
<li>Tractable Transformersç»“åˆäº†å±€éƒ¨å’Œå…¨å±€ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼Œæé«˜äº†æ¡ä»¶ç”Ÿæˆçš„æ•ˆæœã€‚</li>
<li>Tractable Transformersåœ¨æ–‡æœ¬å»ºæ¨¡ä¸Šå®ç°äº†å¯¹è¿‘æœŸæ‰©æ•£æ¨¡å‹å’Œè‡ªå›å½’æ¨¡å‹åŸºå‡†æµ‹è¯•çš„æ¡ä»¶ç”Ÿæˆæ€§èƒ½çš„æœ€ä½³è¡¨ç°ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.07616">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-d6b1364a33502fbbc3d9e3fd891d0c03.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-d49e62f2cfd532fe33d95898bcb9f475.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b2e9e67a61b7932e1f6ad7f907587c8a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-9f62cc996a8965e632b61853d01007eb.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="Towards-Zero-Shot-Anomaly-Detection-and-Reasoning-with-Multimodal-Large-Language-Models"><a href="#Towards-Zero-Shot-Anomaly-Detection-and-Reasoning-with-Multimodal-Large-Language-Models" class="headerlink" title="Towards Zero-Shot Anomaly Detection and Reasoning with Multimodal Large   Language Models"></a>Towards Zero-Shot Anomaly Detection and Reasoning with Multimodal Large   Language Models</h2><p><strong>Authors:Jiacong Xu, Shao-Yuan Lo, Bardia Safaei, Vishal M. Patel, Isht Dwivedi</strong></p>
<p>Zero-Shot Anomaly Detection (ZSAD) is an emerging AD paradigm. Unlike the traditional unsupervised AD setting that requires a large number of normal samples to train a model, ZSAD is more practical for handling data-restricted real-world scenarios. Recently, Multimodal Large Language Models (MLLMs) have shown revolutionary reasoning capabilities in various vision tasks. However, the reasoning of image abnormalities remains underexplored due to the lack of corresponding datasets and benchmarks. To facilitate research in AD &amp; reasoning, we establish the first visual instruction tuning dataset, Anomaly-Instruct-125k, and the evaluation benchmark, VisA-D&amp;R. Through investigation with our benchmark, we reveal that current MLLMs like GPT-4o cannot accurately detect and describe fine-grained anomalous details in images. To address this, we propose Anomaly-OneVision (Anomaly-OV), the first specialist visual assistant for ZSAD and reasoning. Inspired by human behavior in visual inspection, Anomaly-OV leverages a Look-Twice Feature Matching (LTFM) mechanism to adaptively select and emphasize abnormal visual tokens. Extensive experiments demonstrate that Anomaly-OV achieves significant improvements over advanced generalist models in both detection and reasoning. Extensions to medical and 3D AD are provided for future study. The link to our project page: <a target="_blank" rel="noopener" href="https://xujiacong.github.io/Anomaly-OV/">https://xujiacong.github.io/Anomaly-OV/</a> </p>
<blockquote>
<p>é›¶æ ·æœ¬å¼‚å¸¸æ£€æµ‹ï¼ˆZSADï¼‰æ˜¯ä¸€ç§æ–°å…´çš„å¼‚å¸¸æ£€æµ‹èŒƒå¼ã€‚ä¸ä¼ ç»Ÿçš„éœ€è¦å¤§æ‰¹é‡æ­£å¸¸æ ·æœ¬æ¥è®­ç»ƒæ¨¡å‹çš„ç›‘ç£å‹å¼‚å¸¸æ£€æµ‹ä¸åŒï¼ŒZSADåœ¨åº”å¯¹æ•°æ®å—é™çš„å®é™…æƒ…å†µæ—¶æ›´å…·å®ç”¨æ€§ã€‚è¿‘æœŸï¼Œå¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨å„ç§è§†è§‰ä»»åŠ¡ä¸­å±•ç°å‡ºé©å‘½æ€§çš„æ¨ç†èƒ½åŠ›ã€‚ç„¶è€Œï¼Œç”±äºç¼ºä¹ç›¸åº”çš„æ•°æ®é›†å’ŒåŸºå‡†æµ‹è¯•ï¼Œå›¾åƒå¼‚å¸¸çš„æ¨ç†ä»ç„¶è¢«å¿½è§†ã€‚ä¸ºäº†ä¿ƒè¿›å¼‚å¸¸æ£€æµ‹å’Œæ¨ç†çš„ç ”ç©¶ï¼Œæˆ‘ä»¬å»ºç«‹äº†é¦–ä¸ªè§†è§‰æŒ‡ä»¤è°ƒæ•´æ•°æ®é›†Anomaly-Instruct-125kå’Œè¯„ä¼°åŸºå‡†VisA-D&amp;Rã€‚é€šè¿‡æˆ‘ä»¬çš„åŸºå‡†è°ƒæŸ¥ï¼Œæˆ‘ä»¬å‘ç°å½“å‰çš„MLLMså¦‚GPT-4oæ— æ³•å‡†ç¡®æ£€æµ‹å’Œæè¿°å›¾åƒä¸­çš„ç²¾ç»†å¼‚å¸¸ç»†èŠ‚ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†Anomaly-OneVisionï¼ˆAnomaly-OVï¼‰ï¼Œè¿™æ˜¯ä¸“ä¸ºZSADå’Œæ¨ç†è®¾è®¡çš„é¦–ä¸ªä¸“ä¸šè§†è§‰åŠ©ç†ã€‚Anomaly-OVå—åˆ°äººç±»è§†è§‰æ£€æŸ¥è¡Œä¸ºçš„å¯å‘ï¼Œåˆ©ç”¨äºŒæ¬¡ç‰¹å¾åŒ¹é…ï¼ˆLTFMï¼‰æœºåˆ¶è‡ªé€‚åº”é€‰æ‹©å’Œå¼ºè°ƒå¼‚å¸¸çš„è§†è§‰æ ‡è®°ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼Œåœ¨æ£€æµ‹å’Œæ¨ç†æ–¹é¢ï¼ŒAnomaly-OVè¾ƒå…ˆè¿›çš„é€šç”¨æ¨¡å‹æœ‰æ˜¾è‘—æ”¹è¿›ã€‚è¿˜ä¸ºåŒ»å­¦å’Œ3Då¼‚å¸¸æ£€æµ‹æä¾›äº†æ‰©å±•ç ”ç©¶çš„æ–¹å‘ã€‚æˆ‘ä»¬çš„é¡¹ç›®é¡µé¢é“¾æ¥ï¼š<a target="_blank" rel="noopener" href="https://xujiacong.github.io/Anomaly-OV/">https://xujiacong.github.io/Anomaly-OV/</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.07601v1">PDF</a> 19 pages, 10 figures</p>
<p><strong>Summary</strong><br>åŸºäºé›¶æ ·æœ¬å¼‚å¸¸æ£€æµ‹ï¼ˆZSADï¼‰çš„æ–°å…´å‘å±•è¶‹åŠ¿ï¼Œå½“å‰é¢ä¸´ç¼ºä¹è®­ç»ƒæ•°æ®å’Œå®é™…åº”ç”¨åœºæ™¯çš„æŒ‘æˆ˜ã€‚æœ¬æ–‡é€šè¿‡å»ºç«‹é¦–ä¸ªè§†è§‰æŒ‡ä»¤è°ƒæ•´æ•°æ®é›†Anomaly-Instruct-125kå’Œè¯„ä¼°åŸºå‡†VisA-D&amp;Rï¼Œæ¨åŠ¨äº†å¯¹å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨å¼‚å¸¸æ£€æµ‹å’Œæ¨ç†é¢†åŸŸçš„ç ”ç©¶ã€‚ç ”ç©¶è¡¨æ˜ï¼Œç°æœ‰çš„MLLMsæ— æ³•å‡†ç¡®æ£€æµ‹å’Œæè¿°å›¾åƒä¸­çš„ç»†å¾®å¼‚å¸¸ç»†èŠ‚ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†é¦–ä¸ªé’ˆå¯¹ZSADå’Œæ¨ç†çš„ä¸“å®¶è§†è§‰åŠ©æ‰‹Anomaly-OneVisionï¼ˆAnomaly-OVï¼‰ã€‚å®ƒé‡‡ç”¨ç±»ä¼¼äººç±»è§†è§‰æ£€æŸ¥çš„Look-Twice Feature Matchingï¼ˆLTFMï¼‰æœºåˆ¶ï¼Œè‡ªé€‚åº”é€‰æ‹©å’Œå¼ºè°ƒå¼‚å¸¸çš„è§†è§‰æ ‡è®°ã€‚å®éªŒè¯æ˜ï¼ŒAnomaly-OVåœ¨æ£€æµ‹å’Œæ¨ç†æ–¹é¢éƒ½å–å¾—äº†æ˜¾è‘—çš„æ”¹è¿›ã€‚æ­¤å¤–ï¼Œè¿˜ä¸ºæœªæ¥çš„åŒ»å­¦å’Œä¸‰ç»´å¼‚å¸¸æ£€æµ‹æä¾›äº†æ‰©å±•ç ”ç©¶ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>é›¶æ ·æœ¬å¼‚å¸¸æ£€æµ‹ï¼ˆZSADï¼‰æ˜¯å¤„ç†æ•°æ®é™åˆ¶ç°å®åœºæ™¯çš„æ›´å®ç”¨æ–¹æ³•ï¼Œä¸ä¼ ç»Ÿæ— ç›‘ç£å¼‚å¸¸æ£€æµ‹è®¾ç½®ç›¸æ¯”ï¼Œæ— éœ€å¤§é‡æ­£å¸¸æ ·æœ¬è¿›è¡Œæ¨¡å‹è®­ç»ƒã€‚</li>
<li>å»ºç«‹äº†é¦–ä¸ªè§†è§‰æŒ‡ä»¤è°ƒæ•´æ•°æ®é›†Anomaly-Instruct-125kå’Œè¯„ä¼°åŸºå‡†VisA-D&amp;Rï¼Œæ¨åŠ¨äº†å¯¹å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨å¼‚å¸¸æ£€æµ‹å’Œæ¨ç†æ–¹é¢çš„åº”ç”¨ã€‚</li>
<li>å½“å‰MLLMsåœ¨å›¾åƒå¼‚å¸¸æ£€æµ‹æ–¹é¢å­˜åœ¨å±€é™æ€§ï¼Œæ— æ³•å‡†ç¡®è¯†åˆ«å’Œæè¿°ç»†å¾®çš„å¼‚å¸¸ç»†èŠ‚ã€‚</li>
<li>Anomaly-OneVisionï¼ˆAnomaly-OVï¼‰ä½œä¸ºä¸€ç§ä¸“å®¶è§†è§‰åŠ©æ‰‹ï¼Œé€šè¿‡é‡‡ç”¨Look-Twice Feature Matchingï¼ˆLTFMï¼‰æœºåˆ¶è§£å†³äº†ä¸Šè¿°é—®é¢˜ã€‚</li>
<li>Anomaly-OVåœ¨å¼‚å¸¸æ£€æµ‹å’Œæ¨ç†æ–¹é¢å®ç°äº†æ˜¾è‘—æ”¹è¿›ï¼Œé€šè¿‡è‡ªé€‚åº”é€‰æ‹©å’Œå¼ºè°ƒå¼‚å¸¸çš„è§†è§‰æ ‡è®°æ¥æé«˜æ€§èƒ½ã€‚</li>
<li>Anomaly-OVçš„æ‰©å±•ç ”ç©¶åŒ…æ‹¬åŒ»å­¦å’Œä¸‰ç»´å¼‚å¸¸æ£€æµ‹çš„æ½œåœ¨åº”ç”¨ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.07601">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-7e6d29caec561a8767f09e44bac6b507.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4ccea24de69adea98ef180bb29ffe6e9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f6e01cacb627a2b67b799a98ea163b17.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-cb102e6ee688632fa83304364091f085.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-22bf350f7a49c94cf29fdd159937bdb0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1c8b27acfed861f4829743ca05ed04ca.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-473fa97609bcb3f7bbebd7e5c65a2810.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="Exoplanet-Transit-Candidate-Identification-in-TESS-Full-Frame-Images-via-a-Transformer-Based-Algorithm"><a href="#Exoplanet-Transit-Candidate-Identification-in-TESS-Full-Frame-Images-via-a-Transformer-Based-Algorithm" class="headerlink" title="Exoplanet Transit Candidate Identification in TESS Full-Frame Images via   a Transformer-Based Algorithm"></a>Exoplanet Transit Candidate Identification in TESS Full-Frame Images via   a Transformer-Based Algorithm</h2><p><strong>Authors:Helem Salinas, Rafael Brahm, Greg Olmschenk, Richard K. Barry, Karim Pichara, Stela Ishitani Silva, Vladimir Araujo</strong></p>
<p>The Transiting Exoplanet Survey Satellite (TESS) is surveying a large fraction of the sky, generating a vast database of photometric time series data that requires thorough analysis to identify exoplanetary transit signals. Automated learning approaches have been successfully applied to identify transit signals. However, most existing methods focus on the classification and validation of candidates, while few efforts have explored new techniques for the search of candidates. To search for new exoplanet transit candidates, we propose an approach to identify exoplanet transit signals without the need for phase folding or assuming periodicity in the transit signals, such as those observed in multi-transit light curves. To achieve this, we implement a new neural network inspired by Transformers to directly process Full Frame Image (FFI) light curves to detect exoplanet transits. Transformers, originally developed for natural language processing, have recently demonstrated significant success in capturing long-range dependencies compared to previous approaches focused on sequential data. This ability allows us to employ multi-head self-attention to identify exoplanet transit signals directly from the complete light curves, combined with background and centroid time series, without requiring prior transit parameters. The network is trained to learn characteristics of the transit signal, like the dip shape, which helps distinguish planetary transits from other variability sources. Our model successfully identified 214 new planetary system candidates, including 122 multi-transit light curves, 88 single-transit and 4 multi-planet systems from TESS sectors 1-26 with a radius &gt; 0.27 $R_{\mathrm{Jupiter}}$, demonstrating its ability to detect transits regardless of their periodicity. </p>
<blockquote>
<p>å‡Œæ—¥ç³»å¤–è¡Œæ˜Ÿæ¢æµ‹å«æ˜Ÿï¼ˆTESSï¼‰æ­£åœ¨å¯¹å¤©ç©ºè¿›è¡Œå¤§è§„æ¨¡å‹˜æµ‹ï¼Œç”Ÿæˆäº†å¤§é‡å…‰åº¦æ—¶é—´åºåˆ—æ•°æ®ï¼Œéœ€è¦æ·±å…¥åˆ†æä»¥è¯†åˆ«ç³»å¤–è¡Œæ˜Ÿå‡Œæ—¥ä¿¡å·ã€‚è‡ªåŠ¨åŒ–å­¦ä¹ æ–¹æ³•å·²æˆåŠŸåº”ç”¨äºè¯†åˆ«å‡Œæ—¥ä¿¡å·ã€‚ç„¶è€Œï¼Œå¤§å¤šæ•°ç°æœ‰æ–¹æ³•éƒ½é›†ä¸­åœ¨å€™é€‰å¯¹è±¡çš„åˆ†ç±»å’ŒéªŒè¯ä¸Šï¼Œè€Œå¾ˆå°‘æœ‰äººæ¢ç´¢å¯»æ‰¾å€™é€‰å¯¹è±¡çš„æ–°æŠ€æœ¯ã€‚ä¸ºäº†å¯»æ‰¾æ–°çš„ç³»å¤–è¡Œæ˜Ÿå‡Œæ—¥å€™é€‰å¯¹è±¡ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ— éœ€ç›¸ä½æŠ˜å æˆ–å‡è®¾å‡Œæ—¥ä¿¡å·å‘¨æœŸæ€§çš„æ–¹æ³•æ¥è¯†åˆ«ç³»å¤–è¡Œæ˜Ÿå‡Œæ—¥ä¿¡å·ï¼Œä¾‹å¦‚å¤šå‡Œæ—¥å…‰åº¦æ›²çº¿ä¸­æ‰€è§‚å¯Ÿåˆ°çš„ä¿¡å·ã€‚ä¸ºäº†å®ç°è¿™ä¸€ç‚¹ï¼Œæˆ‘ä»¬å—åˆ°Transformerå¯å‘çš„ç¥ç»ç½‘ç»œç›´æ¥å¤„ç†å…¨å¸§å›¾åƒï¼ˆFFIï¼‰å…‰åº¦æ›²çº¿æ¥æ£€æµ‹ç³»å¤–è¡Œæ˜Ÿå‡Œæ—¥ç°è±¡ã€‚Transformeræœ€åˆæ˜¯ä¸ºè‡ªç„¶è¯­è¨€å¤„ç†è€Œå¼€å‘çš„ï¼Œæœ€è¿‘çš„ç ”ç©¶è¡¨æ˜ï¼Œä¸ä»¥å‰ä¸“æ³¨äºé¡ºåºæ•°æ®çš„æ–¹æ³•ç›¸æ¯”ï¼Œå®ƒåœ¨æ•è·é•¿æœŸä¾èµ–å…³ç³»æ–¹é¢å–å¾—äº†æ˜¾è‘—çš„æˆåŠŸã€‚è¿™ç§èƒ½åŠ›ä½¿æˆ‘ä»¬èƒ½å¤Ÿåˆ©ç”¨å¤šå¤´è‡ªæ³¨æ„åŠ›æœºåˆ¶ç›´æ¥ä»å®Œæ•´çš„å…‰åº¦æ›²çº¿ä¸­è¯†åˆ«å‡ºç³»å¤–è¡Œæ˜Ÿå‡Œæ—¥ä¿¡å·ï¼Œå¹¶ç»“åˆèƒŒæ™¯å’Œè´¨å¿ƒæ—¶é—´åºåˆ—ï¼Œæ— éœ€é¢„å…ˆè®¾å®šçš„å‡Œæ—¥å‚æ•°ã€‚ç½‘ç»œç»è¿‡è®­ç»ƒï¼Œå­¦ä¹ å‡Œæ—¥ä¿¡å·çš„ç‰¹å¾ï¼Œå¦‚å‡¹é™·å½¢çŠ¶ï¼Œè¿™æœ‰åŠ©äºåŒºåˆ†è¡Œæ˜Ÿå‡Œæ—¥å’Œå…¶ä»–å˜æºã€‚æˆ‘ä»¬çš„æ¨¡å‹æˆåŠŸè¯†åˆ«äº†214ä¸ªæ–°çš„è¡Œæ˜Ÿç³»ç»Ÿå€™é€‰å¯¹è±¡ï¼ŒåŒ…æ‹¬122ä¸ªå¤šå‡Œæ—¥å…‰åº¦æ›²çº¿ã€88ä¸ªå•å‡Œæ—¥å’Œ4ä¸ªå¤šè¡Œæ˜Ÿç³»ç»Ÿï¼Œè¿™äº›ç³»ç»Ÿæ¥è‡ªTESSçš„1-26ä¸ªåŒºåŸŸï¼Œå…¶åŠå¾„å¤§äº0.27ä¸ªæœ¨æ˜ŸåŠå¾„ï¼Œè¯æ˜äº†å…¶æ£€æµ‹å‡Œæ—¥çš„èƒ½åŠ›ï¼Œæ— è®ºå…¶å‘¨æœŸæ€§å¦‚ä½•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.07542v1">PDF</a> </p>
<p><strong>Summary</strong><br>     æœ¬ç ”ç©¶ä½¿ç”¨å—Transformerå¯å‘çš„ç¥ç»ç½‘ç»œç›´æ¥å¤„ç†å…¨å¸§å›¾åƒï¼ˆFFIï¼‰å…‰å˜æ›²çº¿ï¼Œæ— éœ€ç›¸ä½æŠ˜å æˆ–å‡è®¾å‘¨æœŸæ€§ï¼Œå³å¯æ£€æµ‹è¡Œæ˜Ÿè¿‡å¢ƒä¿¡å·ã€‚è¯¥æ–¹æ³•æˆåŠŸè¯†åˆ«äº†TESSæ‰‡åŒº1-26ä¸­çš„214ä¸ªæ–°è¡Œæ˜Ÿç³»ç»Ÿå€™é€‰è€…ï¼Œè¯æ˜äº†å…¶æ£€æµ‹å‘¨æœŸæ€§ä¸è®ºçš„è¿‡å¢ƒèƒ½åŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>TESSæ­£åœ¨å¼€å±•å¤§è§„æ¨¡å¤©ç©ºè°ƒæŸ¥ï¼Œäº§ç”Ÿå¤§é‡å…‰åº¦æ—¶é—´åºåˆ—æ•°æ®ï¼Œéœ€è¦æ·±å…¥åˆ†æä»¥è¯†åˆ«è¡Œæ˜Ÿè¿‡å¢ƒä¿¡å·ã€‚</li>
<li>ç›®å‰å¤§å¤šæ•°æ–¹æ³•é›†ä¸­åœ¨å€™é€‰è€…çš„åˆ†ç±»å’ŒéªŒè¯ä¸Šï¼Œå¾ˆå°‘æœ‰æ¢ç´¢æ–°æ–¹æ³•å¯»æ‰¾å€™é€‰è€…ã€‚</li>
<li>ç ”ç©¶æå‡ºäº†ä¸€ç§æ–°çš„ç¥ç»ç½‘ç»œæ–¹æ³•ï¼Œç›´æ¥å¤„ç†å…¨å¸§å›¾åƒï¼ˆFFIï¼‰å…‰å˜æ›²çº¿æ¥æ£€æµ‹è¡Œæ˜Ÿè¿‡å¢ƒä¿¡å·ï¼Œæ— éœ€ç›¸ä½æŠ˜å æˆ–å‡è®¾å‘¨æœŸæ€§ã€‚</li>
<li>è¯¥æ–¹æ³•ä½¿ç”¨TransformeråŸç†ï¼Œèƒ½å¤Ÿæ•æ‰é•¿æ—¶é—´ä¾èµ–æ€§ï¼ŒæˆåŠŸè¯†åˆ«å‡ºæ–°çš„è¡Œæ˜Ÿç³»ç»Ÿå€™é€‰è€…ã€‚</li>
<li>è¯¥ç½‘ç»œè¢«è®­ç»ƒå­¦ä¹ è¿‡å¢ƒä¿¡å·çš„ç‰¹å¾ï¼Œå¦‚ä¸‹é™å½¢çŠ¶ï¼Œæœ‰åŠ©äºåŒºåˆ†è¡Œæ˜Ÿè¿‡å¢ƒå’Œå…¶ä»–å˜åŒ–æºã€‚</li>
<li>ç ”ç©¶ä»TESSæ‰‡åŒº1-26ä¸­æˆåŠŸè¯†åˆ«äº†214ä¸ªæ–°çš„è¡Œæ˜Ÿç³»ç»Ÿå€™é€‰è€…ï¼ŒåŒ…æ‹¬å¤šè¿‡æ˜¥å…‰å˜æ›²çº¿ã€å•è¿‡å…‰å˜å’Œå¤šé‡è¡Œæ˜Ÿç³»ç»Ÿã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.07542">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-4fbb351f5632dd5a6c31c5cac18c786c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-751a0f349e754c4a8b2280ad20a413b4.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-74c6938214d33c1513b62e4ae30a8d27.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="Optimizing-Knowledge-Distillation-in-Transformers-Enabling-Multi-Head-Attention-without-Alignment-Barriers"><a href="#Optimizing-Knowledge-Distillation-in-Transformers-Enabling-Multi-Head-Attention-without-Alignment-Barriers" class="headerlink" title="Optimizing Knowledge Distillation in Transformers: Enabling Multi-Head   Attention without Alignment Barriers"></a>Optimizing Knowledge Distillation in Transformers: Enabling Multi-Head   Attention without Alignment Barriers</h2><p><strong>Authors:Zhaodong Bing, Linze Li, Jiajun Liang</strong></p>
<p>Knowledge distillation (KD) in transformers often faces challenges due to misalignment in the number of attention heads between teacher and student models. Existing methods either require identical head counts or introduce projectors to bridge dimensional gaps, limiting flexibility and efficiency. We propose Squeezing-Heads Distillation (SHD), a novel approach that enables seamless knowledge transfer between models with varying head counts by compressing multi-head attention maps via efficient linear approximation. Unlike prior work, SHD eliminates alignment barriers without additional parameters or architectural modifications. Our method dynamically approximates the combined effect of multiple teacher heads into fewer student heads, preserving fine-grained attention patterns while reducing redundancy. Experiments across language (LLaMA, GPT) and vision (DiT, MDT) generative and vision (DeiT) discriminative tasks demonstrate SHDâ€™s effectiveness: it outperforms logit-based and feature-alignment KD baselines, achieving state-of-the-art results in image classification, image generation language fine-tuning, and language pre-training. The key innovations of flexible head compression, projector-free design, and linear-time complexity make SHD a versatile and scalable solution for distilling modern transformers. This work bridges a critical gap in KD, enabling efficient deployment of compact models without compromising performance. </p>
<blockquote>
<p>çŸ¥è¯†è’¸é¦ï¼ˆKDï¼‰åœ¨è½¬æ¢å™¨ï¼ˆtransformerï¼‰ä¸­å¸¸å› æ•™å¸ˆå’Œå­¦ç”Ÿåœ¨æ³¨æ„åŠ›å¤´æ•°é‡ä¸Šçš„ä¸åŒ¹é…è€Œé¢ä¸´æŒ‘æˆ˜ã€‚ç°æœ‰æ–¹æ³•è¦ä¹ˆè¦æ±‚å¤´æ•°ç›¸åŒï¼Œè¦ä¹ˆå¼•å…¥æŠ•å½±å™¨æ¥å¼¥è¡¥ç»´åº¦å·®è·ï¼Œä»è€Œé™åˆ¶äº†çµæ´»æ€§å’Œæ•ˆç‡ã€‚æˆ‘ä»¬æå‡ºäº†æŒ¤å‹å¤´è’¸é¦æ³•ï¼ˆSHDï¼‰ï¼Œè¿™æ˜¯ä¸€ç§æ–°å‹æ–¹æ³•ï¼Œé€šè¿‡æœ‰æ•ˆçš„çº¿æ€§è¿‘ä¼¼å‹ç¼©å¤šå¤´æ³¨æ„åŠ›å›¾ï¼Œå®ç°åœ¨ä¸åŒå¤´æ•°çš„æ¨¡å‹ä¹‹é—´è¿›è¡Œæ— ç¼çŸ¥è¯†è½¬ç§»ã€‚ä¸åŒäºä»¥å‰çš„å·¥ä½œï¼ŒSHDæ¶ˆé™¤äº†å¯¹é½éšœç¢ï¼Œæ— éœ€é¢å¤–çš„å‚æ•°æˆ–æ¶æ„ä¿®æ”¹ã€‚æˆ‘ä»¬çš„æ–¹æ³•åŠ¨æ€åœ°å°†å¤šä¸ªæ•™å¸ˆå¤´çš„ç»¼åˆæ•ˆåº”è¿‘ä¼¼ä¸ºå°‘æ•°å­¦ç”Ÿå¤´ï¼Œæ—¢ä¿ç•™äº†ç²¾ç»†çš„æ³¨æ„åŠ›æ¨¡å¼åˆå‡å°‘äº†å†—ä½™ã€‚åœ¨è¯­è¨€ç”Ÿæˆï¼ˆLLaMAã€GPTï¼‰å’Œè§†è§‰ç”Ÿæˆï¼ˆDiTã€MDTï¼‰ä»¥åŠè§†è§‰åˆ¤åˆ«ï¼ˆDeiTï¼‰ä»»åŠ¡ä¸Šçš„å®éªŒè¡¨æ˜SHDçš„æœ‰æ•ˆæ€§ï¼šå®ƒè¶…è¶Šäº†åŸºäºå¯¹æ•°æ¦‚ç‡å’Œæ•™å¸ˆå¤´ç‰¹å¾çš„åŸºå‡†çŸ¥è¯†è’¸é¦æ–¹æ³•ï¼Œåœ¨å›¾åƒåˆ†ç±»ã€å›¾åƒç”Ÿæˆè¯­è¨€å¾®è°ƒä»¥åŠè¯­è¨€é¢„è®­ç»ƒæ–¹é¢å–å¾—äº†æœ€å…ˆè¿›çš„æˆæœã€‚å¤´å‹ç¼©çµæ´»æ€§ã€æ— æŠ•å½±å™¨è®¾è®¡ä»¥åŠçº¿æ€§æ—¶é—´å¤æ‚åº¦ç­‰å…³é”®åˆ›æ–°ä½¿SHDæˆä¸ºä¸€ç§é€‚ç”¨äºç°ä»£è½¬æ¢å™¨è’¸é¦çš„é€šç”¨å’Œå¯æ‰©å±•è§£å†³æ–¹æ¡ˆã€‚è¿™é¡¹å·¥ä½œè§£å†³äº†KDä¸­çš„ä¸€ä¸ªå…³é”®ç©ºç™½ï¼Œèƒ½å¤Ÿå®ç°ç´§å‡‘æ¨¡å‹çš„é«˜æ•ˆéƒ¨ç½²ï¼Œè€Œä¸æŸå®³æ€§èƒ½ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.07436v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ä¸ªæ–°çš„çŸ¥è¯†è’¸é¦ï¼ˆKDï¼‰æ–¹æ³•â€”â€”æŒ¤å‹å¤´è’¸é¦ï¼ˆSHDï¼‰ï¼Œè¯¥æ–¹æ³•èƒ½å¤Ÿåœ¨æ³¨æ„åŠ›å¤´æ•°é‡ä¸åŒçš„æ¨¡å‹é—´å®ç°æ— ç¼çŸ¥è¯†è¿ç§»ã€‚é€šè¿‡é«˜æ•ˆçº¿æ€§è¿‘ä¼¼å‹ç¼©å¤šå¤´æ³¨æ„åŠ›å›¾ï¼ŒSHDèƒ½å¤Ÿæ¶ˆé™¤å¯¹é½éšœç¢ï¼Œæ— éœ€é¢å¤–çš„å‚æ•°æˆ–æ¶æ„ä¿®æ”¹ã€‚å®éªŒè¡¨æ˜ï¼ŒSHDåœ¨è·¨è¯­è¨€ç”Ÿæˆå’Œè§†è§‰åˆ¤åˆ«ä»»åŠ¡ä¸Šè¡¨ç°å‡ºå“è¶Šæ€§èƒ½ï¼Œå®ç°äº†å›¾åƒåˆ†ç±»ã€å›¾åƒç”Ÿæˆè¯­è¨€å¾®è°ƒä»¥åŠè¯­è¨€é¢„è®­ç»ƒçš„æœ€å…ˆè¿›ç»“æœã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>SHDè§£å†³äº†çŸ¥è¯†è’¸é¦ä¸­ç”±äºæ•™å¸ˆæ¨¡å‹å’Œå­¦ç”Ÿæ¨¡å‹æ³¨æ„åŠ›å¤´æ•°é‡ä¸åŒ¹é…å¸¦æ¥çš„æŒ‘æˆ˜ã€‚</li>
<li>é€šè¿‡å‹ç¼©å¤šå¤´æ³¨æ„åŠ›å›¾ï¼ŒSHDå®ç°äº†ä¸åŒæ¨¡å‹é—´çš„æ— ç¼çŸ¥è¯†è¿ç§»ã€‚</li>
<li>SHDé‡‡ç”¨é«˜æ•ˆçº¿æ€§è¿‘ä¼¼ï¼Œæ— éœ€é¢å¤–çš„å‚æ•°æˆ–æ¶æ„ä¿®æ”¹ã€‚</li>
<li>SHDåœ¨è·¨è¯­è¨€ç”Ÿæˆå’Œè§†è§‰åˆ¤åˆ«ä»»åŠ¡ä¸Šè¡¨ç°å‡ºå“è¶Šæ€§èƒ½ã€‚</li>
<li>SHDåœ¨å›¾åƒåˆ†ç±»ã€å›¾åƒç”Ÿæˆè¯­è¨€å¾®è°ƒä»¥åŠè¯­è¨€é¢„è®­ç»ƒé¢†åŸŸå®ç°äº†æœ€å…ˆè¿›ç»“æœã€‚</li>
<li>SHDå…·æœ‰çµæ´»çš„å¤´å‹ç¼©èƒ½åŠ›ï¼Œä½¿å…¶æˆä¸ºçŸ¥è¯†è’¸é¦ä¸­é€šç”¨ä¸”å¯ä¼¸ç¼©çš„è§£å†³æ–¹æ¡ˆã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.07436">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-f9e7930cdeee67681bca2978561daeb5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2d2717f30b2ba21860dfd6f5c35455ed.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-99a4d01c0d286da05c8022633f188695.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="On-Iterative-Evaluation-and-Enhancement-of-Code-Quality-Using-GPT-4o"><a href="#On-Iterative-Evaluation-and-Enhancement-of-Code-Quality-Using-GPT-4o" class="headerlink" title="On Iterative Evaluation and Enhancement of Code Quality Using GPT-4o"></a>On Iterative Evaluation and Enhancement of Code Quality Using GPT-4o</h2><p><strong>Authors:Rundong Liu, Andre Frade, Amal Vaidya, Maxime Labonne, Marcus Kaiser, Bismayan Chakrabarti, Jonathan Budd, Sean Moran</strong></p>
<p>This paper introduces CodeQUEST, a novel framework leveraging Large Language Models (LLMs) to iteratively evaluate and enhance code quality across multiple dimensions, including readability, maintainability, efficiency, and security. The framework is divided into two main components: an Evaluator that assesses code quality across ten dimensions, providing both quantitative scores and qualitative summaries, and an Optimizer that iteratively improves the code based on the Evaluatorâ€™s feedback. Our study demonstrates that CodeQUEST can effectively and robustly evaluate code quality, with its assessments aligning closely with established code quality metrics. Through a series of experiments using a curated dataset of Python and JavaScript examples, CodeQUEST demonstrated significant improvements in code quality, achieving a mean relative percentage improvement of 52.6%. The frameworkâ€™s evaluations were validated against a set of proxy metrics comprising of Pylint Score, Radon Maintainability Index, and Bandit output logs, showing a meaningful correlation. This highlights the potential of LLMs in automating code quality evaluation and improvement processes, presenting a significant advancement toward enhancing software development practices. The code implementation of the framework is available at: <a target="_blank" rel="noopener" href="https://github.com/jpmorganchase/CodeQuest">https://github.com/jpmorganchase/CodeQuest</a>. </p>
<blockquote>
<p>æœ¬æ–‡ä»‹ç»äº†CodeQUESTï¼Œè¿™æ˜¯ä¸€ä¸ªæ–°å‹æ¡†æ¶ï¼Œåˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ¥å¤šç»´åº¦è¯„ä¼°å’Œæå‡ä»£ç è´¨é‡ï¼ŒåŒ…æ‹¬å¯è¯»æ€§ã€å¯ç»´æŠ¤æ€§ã€æ•ˆç‡å’Œå®‰å…¨æ€§ã€‚è¯¥æ¡†æ¶åˆ†ä¸ºä¸¤ä¸ªä¸»è¦ç»„æˆéƒ¨åˆ†ï¼šè¯„ä¼°å™¨ï¼Œç”¨äºåœ¨åä¸ªç»´åº¦ä¸Šè¯„ä¼°ä»£ç è´¨é‡ï¼Œæä¾›å®šé‡è¯„åˆ†å’Œå®šæ€§æ‘˜è¦ï¼›ä¼˜åŒ–å™¨ï¼Œåˆ™åŸºäºè¯„ä¼°å™¨çš„åé¦ˆæ¥è¿­ä»£æ”¹è¿›ä»£ç ã€‚æˆ‘ä»¬çš„ç ”ç©¶è¡¨æ˜ï¼ŒCodeQUESTèƒ½å¤Ÿæœ‰æ•ˆä¸”ç¨³å®šåœ°è¯„ä¼°ä»£ç è´¨é‡ï¼Œå…¶è¯„ä¼°ç»“æœä¸æ—¢å®šçš„ä»£ç è´¨é‡æŒ‡æ ‡é«˜åº¦å»åˆã€‚é€šè¿‡ä¸€ç³»åˆ—ä½¿ç”¨ç²¾é€‰çš„Pythonå’ŒJavaScriptç¤ºä¾‹æ•°æ®é›†è¿›è¡Œçš„å®éªŒï¼ŒCodeQUESTåœ¨ä»£ç è´¨é‡æ–¹é¢å–å¾—äº†æ˜¾è‘—çš„æå‡ï¼Œå¹³å‡ç›¸å¯¹ç™¾åˆ†æ¯”æå‡äº†52.6%ã€‚è¯¥æ¡†æ¶çš„è¯„ä¼°ç»“æœç»è¿‡ä¸€ç»„ä»£ç†æŒ‡æ ‡çš„éªŒè¯ï¼ŒåŒ…æ‹¬Pylintè¯„åˆ†ã€Radonå¯ç»´æŠ¤æ€§æŒ‡æ•°å’ŒBanditè¾“å‡ºæ—¥å¿—ï¼Œæ˜¾ç¤ºå‡ºæœ‰æ„ä¹‰çš„ç›¸å…³æ€§ã€‚è¿™çªæ˜¾äº†å¤§å‹è¯­è¨€æ¨¡å‹åœ¨è‡ªåŠ¨åŒ–ä»£ç è´¨é‡è¯„ä¼°å’Œæ”¹è¿›è¿‡ç¨‹ä¸­çš„æ½œåŠ›ï¼Œä¸ºæ”¹è¿›è½¯ä»¶å¼€å‘å®è·µå¸¦æ¥äº†é‡å¤§è¿›å±•ã€‚è¯¥æ¡†æ¶çš„ä»£ç å®ç°å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/jpmorganchase/CodeQuest%E8%8E%B7%E5%BE%97%E3%80%82">https://github.com/jpmorganchase/CodeQuestè·å¾—ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.07399v1">PDF</a> </p>
<p><strong>Summary</strong><br>ä»£ç è´¨é‡è¯„ä¼°ä¸æå‡æ¡†æ¶CodeQUESTä»‹ç»ã€‚è¯¥æ¡†æ¶åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä»å¯è¯»æ€§ã€å¯ç»´æŠ¤æ€§ã€æ•ˆç‡å’Œå®‰å…¨æ€§ç­‰å¤šä¸ªç»´åº¦è¯„ä¼°ä»£ç è´¨é‡ï¼Œå¹¶æä¾›åé¦ˆè¿›è¡Œä¼˜åŒ–ã€‚å®éªŒè¡¨æ˜ï¼ŒCodeQUESTèƒ½æœ‰æ•ˆæå‡ä»£ç è´¨é‡ï¼Œå¹³å‡ç›¸å¯¹æå‡ç‡è¾¾52.6%ã€‚è¯¥æ¡†æ¶çš„æ½œåŠ›åœ¨äºèƒ½è‡ªåŠ¨åŒ–ä»£ç è´¨é‡è¯„ä¼°å’Œæå‡è¿‡ç¨‹ï¼Œæœ‰åŠ©äºæ”¹è¿›è½¯ä»¶å¼€å‘å®è·µã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>CodeQUESTæ˜¯ä¸€ä¸ªåˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è¯„ä¼°å’Œæå‡ä»£ç è´¨é‡çš„æ¡†æ¶ã€‚</li>
<li>æ¡†æ¶åŒ…å«ä¸¤ä¸ªä¸»è¦ç»„ä»¶ï¼šEvaluatorç”¨äºå¤šç»´åº¦è¯„ä¼°ä»£ç è´¨é‡ï¼ŒOptimizeråˆ™åŸºäºåé¦ˆä¼˜åŒ–ä»£ç ã€‚</li>
<li>CodeQUESTèƒ½æœ‰æ•ˆè¯„ä¼°ä»£ç è´¨é‡ï¼Œä¸ç°æœ‰ä»£ç è´¨é‡æŒ‡æ ‡é«˜åº¦ä¸€è‡´ã€‚</li>
<li>å®éªŒè¡¨æ˜ï¼ŒCodeQUESTèƒ½æ˜¾è‘—æå‡ä»£ç è´¨é‡ï¼Œå¹³å‡ç›¸å¯¹æå‡ç‡è¾¾52.6%ã€‚</li>
<li>æ¡†æ¶é€šè¿‡è‡ªåŠ¨åŒ–ä»£ç è´¨é‡è¯„ä¼°å’Œæå‡è¿‡ç¨‹ï¼Œæœ‰åŠ©äºæ”¹è¿›è½¯ä»¶å¼€å‘å®è·µã€‚</li>
<li>CodeQUESTå·²ç»å®ç°äº†ä»£ç å®æ–½æ¡†æ¶ï¼Œå¹¶å¯é€šè¿‡ç‰¹å®šé“¾æ¥è·å–ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.07399">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-f71a8927217ade82f3e03fbe770bed91.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-aa4b3edaad4dbd663acd613517c65aa8.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="Space-Aware-Instruction-Tuning-Dataset-and-Benchmark-for-Guide-Dog-Robots-Assisting-the-Visually-Impaired"><a href="#Space-Aware-Instruction-Tuning-Dataset-and-Benchmark-for-Guide-Dog-Robots-Assisting-the-Visually-Impaired" class="headerlink" title="Space-Aware Instruction Tuning: Dataset and Benchmark for Guide Dog   Robots Assisting the Visually Impaired"></a>Space-Aware Instruction Tuning: Dataset and Benchmark for Guide Dog   Robots Assisting the Visually Impaired</h2><p><strong>Authors:ByungOk Han, Woo-han Yun, Beom-Su Seo, Jaehong Kim</strong></p>
<p>Guide dog robots offer promising solutions to enhance mobility and safety for visually impaired individuals, addressing the limitations of traditional guide dogs, particularly in perceptual intelligence and communication. With the emergence of Vision-Language Models (VLMs), robots are now capable of generating natural language descriptions of their surroundings, aiding in safer decision-making. However, existing VLMs often struggle to accurately interpret and convey spatial relationships, which is crucial for navigation in complex environments such as street crossings. We introduce the Space-Aware Instruction Tuning (SAIT) dataset and the Space-Aware Benchmark (SA-Bench) to address the limitations of current VLMs in understanding physical environments. Our automated data generation pipeline focuses on the virtual path to the destination in 3D space and the surroundings, enhancing environmental comprehension and enabling VLMs to provide more accurate guidance to visually impaired individuals. We also propose an evaluation protocol to assess VLM effectiveness in delivering walking guidance. Comparative experiments demonstrate that our space-aware instruction-tuned model outperforms state-of-the-art algorithms. We have fully open-sourced the SAIT dataset and SA-Bench, along with the related code, at <a target="_blank" rel="noopener" href="https://github.com/byungokhan/Space-awareVLM">https://github.com/byungokhan/Space-awareVLM</a> </p>
<blockquote>
<p>å¯¼èˆªçŠ¬æœºå™¨äººå¯¹å¢å¼ºè§†éšœäººå£«çš„æœºåŠ¨æ€§å’Œå®‰å…¨æ€§æä¾›äº†æœ‰å‰æ™¯çš„è§£å†³æ–¹æ¡ˆï¼Œè§£å†³äº†ä¼ ç»Ÿå¯¼èˆªçŠ¬åœ¨æ„ŸçŸ¥æ™ºèƒ½å’Œæ²Ÿé€šæ–¹é¢çš„å±€é™ã€‚éšç€è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰çš„å‡ºç°ï¼Œæœºå™¨äººç°åœ¨èƒ½å¤Ÿç”Ÿæˆå‘¨å›´ç¯å¢ƒçš„è‡ªç„¶è¯­è¨€æè¿°ï¼Œæœ‰åŠ©äºåšå‡ºæ›´å®‰å…¨çš„å†³ç­–ã€‚ç„¶è€Œï¼Œç°æœ‰çš„VLMsåœ¨è§£è¯»å’Œä¼ è¾¾ç©ºé—´å…³ç³»æ–¹é¢å¸¸å¸¸é‡åˆ°å›°éš¾ï¼Œè¿™åœ¨å¤æ‚ç¯å¢ƒï¼ˆå¦‚åå­—è·¯å£ï¼‰çš„å¯¼èˆªä¸­è‡³å…³é‡è¦ã€‚ä¸ºäº†è§£å†³å½“å‰VLMsåœ¨ç†è§£ç‰©ç†ç¯å¢ƒæ–¹é¢çš„å±€é™æ€§ï¼Œæˆ‘ä»¬æ¨å‡ºäº†ç©ºé—´æ„ŸçŸ¥æŒ‡ä»¤è°ƒä¼˜ï¼ˆSAITï¼‰æ•°æ®é›†å’Œç©ºé—´æ„ŸçŸ¥åŸºå‡†æµ‹è¯•ï¼ˆSA-Benchï¼‰ã€‚æˆ‘ä»¬çš„è‡ªåŠ¨åŒ–æ•°æ®ç”Ÿæˆç®¡é“ä¸“æ³¨äºä»¥ç›®çš„åœ°ä¸ºä¸­å¿ƒçš„è™šæ‹Ÿè·¯å¾„å’Œç©ºé—´å‘¨å›´ç¯å¢ƒï¼Œå¢å¼ºå¯¹ç¯å¢ƒçš„ç†è§£ï¼Œä½¿VLMsèƒ½å¤Ÿä¸ºè§†éšœäººå£«æä¾›æ›´å‡†ç¡®çš„å¯¼èˆªã€‚æˆ‘ä»¬è¿˜æå‡ºäº†ä¸€ä¸ªè¯„ä¼°åè®®ï¼Œä»¥è¯„ä¼°VLMåœ¨æä¾›æ­¥è¡ŒæŒ‡å¯¼æ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚å¯¹æ¯”å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„ç©ºé—´æ„ŸçŸ¥æŒ‡ä»¤è°ƒä¼˜æ¨¡å‹ä¼˜äºæœ€æ–°ç®—æ³•ã€‚æˆ‘ä»¬å·²åœ¨<a target="_blank" rel="noopener" href="https://github.com/byungokhan/Space-awareVLM%E4%B8%8A%E5%AE%8C%E5%85%A8%E5%BC%80%E6%BA%90%E4%BA%86SAIT%E6%95%B0%E6%8D%AE%E9%9B%86%E5%92%8CSA-Bench%E4%BB%A5%E5%8F%8A%E7%9B%B8%E5%85%B3%E4%BB%A3%E7%A0%81%E3%80%82">https://github.com/byungokhan/Space-awareVLMä¸Šå®Œå…¨å¼€æºäº†SAITæ•°æ®é›†å’ŒSA-Benchä»¥åŠç›¸å…³ä»£ç ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.07183v2">PDF</a> ICRA 2025</p>
<p><strong>Summary</strong></p>
<p>è¯¥æŒ‡å—çŠ¬æœºå™¨äººæä¾›äº†å¢å¼ºè§†åŠ›éšœç¢è€…è¡ŒåŠ¨èƒ½åŠ›å’Œå®‰å…¨æ€§çš„è§£å†³æ–¹æ¡ˆï¼Œè§£å†³äº†ä¼ ç»ŸæŒ‡å—çŠ¬åœ¨æ„ŸçŸ¥æ™ºèƒ½å’Œæ²Ÿé€šæ–¹é¢çš„å±€é™ã€‚å€ŸåŠ©è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰çš„å…´èµ·ï¼Œæœºå™¨äººå¯ä»¥ç”Ÿæˆå‘¨å›´ç¯å¢ƒä¸­çš„è‡ªç„¶è¯­è¨€æè¿°ï¼Œè¾…åŠ©åšå‡ºæ›´å®‰å…¨çš„å†³ç­–ã€‚ç„¶è€Œï¼Œç°æœ‰çš„VLMsåœ¨è§£è¯»å’Œä¼ è¾¾ç©ºé—´å…³ç³»æ–¹é¢å­˜åœ¨å›°éš¾ï¼Œè¿™å¯¹åœ¨è¡—é“äº¤å‰å£ç­‰å¤æ‚ç¯å¢ƒä¸­çš„å¯¼èˆªè‡³å…³é‡è¦ã€‚ä¸ºè§£å†³å½“å‰VLMsåœ¨ç†è§£ç‰©ç†ç¯å¢ƒæ–¹é¢çš„å±€é™æ€§ï¼Œæˆ‘ä»¬æ¨å‡ºäº†ç©ºé—´æ„ŸçŸ¥æŒ‡ä»¤è°ƒä¼˜ï¼ˆSAITï¼‰æ•°æ®é›†å’Œç©ºé—´æ„ŸçŸ¥åŸºå‡†æµ‹è¯•ï¼ˆSA-Benchï¼‰ã€‚æˆ‘ä»¬çš„è‡ªåŠ¨åŒ–æ•°æ®ç”Ÿæˆç®¡é“ä¾§é‡äºè™šæ‹Ÿç›®æ ‡è·¯å¾„çš„ä¸‰ç»´ç©ºé—´åŠå‘¨å›´ç¯å¢ƒï¼Œæå‡ç¯å¢ƒç†è§£èƒ½åŠ›ï¼Œä½¿VLMsèƒ½ä¸ºè§†åŠ›éšœç¢è€…æä¾›æ›´å‡†ç¡®çš„æŒ‡å¯¼ã€‚æˆ‘ä»¬è¿˜æå‡ºäº†ä¸€ä¸ªè¯„ä¼°åè®®æ¥è¯„ä¼°VLMåœ¨æä¾›æ­¥è¡ŒæŒ‡å¯¼æ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚å¯¹æ¯”å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„ç©ºé—´æ„ŸçŸ¥æŒ‡ä»¤è°ƒä¼˜æ¨¡å‹ä¼˜äºç›®å‰æœ€å…ˆè¿›çš„ç®—æ³•ã€‚æˆ‘ä»¬å·²åœ¨<a target="_blank" rel="noopener" href="https://github.com/byungokhan/Space-awareVLM%E4%B8%8A%E5%AE%8C%E5%85%A8%E5%BC%80%E6%BA%90%E4%BA%86SAIT%E6%95%B0%E6%8D%AE%E9%9B%86%E5%92%8CSA-Bench%E5%8F%8A%E7%9B%B8%E5%85%B3%E4%BB%A3%E7%A0%81%E3%80%82">https://github.com/byungokhan/Space-awareVLMä¸Šå®Œå…¨å¼€æºäº†SAITæ•°æ®é›†å’ŒSA-BenchåŠç›¸å…³ä»£ç ã€‚</a></p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æŒ‡å—çŠ¬æœºå™¨äººé€šè¿‡è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰æé«˜è§†åŠ›éšœç¢è€…çš„è¡ŒåŠ¨èƒ½åŠ›ã€‚</li>
<li>ç°æœ‰VLMsåœ¨è§£è¯»å’Œä¼ è¾¾ç©ºé—´å…³ç³»æ–¹é¢å­˜åœ¨æŒ‘æˆ˜ï¼Œå½±å“å¯¼èˆªå‡†ç¡®æ€§ã€‚</li>
<li>ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œæ¨å‡ºäº†ç©ºé—´æ„ŸçŸ¥æŒ‡ä»¤è°ƒä¼˜ï¼ˆSAITï¼‰æ•°æ®é›†å’Œç©ºé—´æ„ŸçŸ¥åŸºå‡†æµ‹è¯•ï¼ˆSA-Benchï¼‰ã€‚</li>
<li>è‡ªåŠ¨åŒ–æ•°æ®ç”Ÿæˆç®¡é“é›†ä¸­äºè™šæ‹Ÿè·¯å¾„åŠå…¶ä¸‰ç»´ç©ºé—´ç¯å¢ƒï¼Œå¼ºåŒ–ç¯å¢ƒç†è§£ã€‚</li>
<li>å¼€æ”¾æºç çš„SAITæ•°æ®é›†å’ŒSA-Benchæ—¨åœ¨ä¿ƒè¿›VLMåœ¨å¯¼èˆªé¢†åŸŸçš„ç ”ç©¶è¿›å±•ã€‚</li>
<li>æå‡ºçš„è¯„ä¼°åè®®å¯æœ‰æ•ˆè¯„ä¼°VLMåœ¨æä¾›æ­¥è¡ŒæŒ‡å¯¼æ–¹é¢çš„æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.07183">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-b6207ba259c49579d63146b6fcaf12cd.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-cfc8a70569188536199de24e3aa82b1d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c20adcd91546917732b762dc5d881682.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7dce883e0055d85116841529a4c20749.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-70b01c56aff8f78903316a15a987c59d.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="Digital-Twin-Buildings-3D-Modeling-GIS-Integration-and-Visual-Descriptions-Using-Gaussian-Splatting-ChatGPT-Deepseek-and-Google-Maps-Platform"><a href="#Digital-Twin-Buildings-3D-Modeling-GIS-Integration-and-Visual-Descriptions-Using-Gaussian-Splatting-ChatGPT-Deepseek-and-Google-Maps-Platform" class="headerlink" title="Digital Twin Buildings: 3D Modeling, GIS Integration, and Visual   Descriptions Using Gaussian Splatting, ChatGPT&#x2F;Deepseek, and Google Maps   Platform"></a>Digital Twin Buildings: 3D Modeling, GIS Integration, and Visual   Descriptions Using Gaussian Splatting, ChatGPT&#x2F;Deepseek, and Google Maps   Platform</h2><p><strong>Authors:Kyle Gao, Dening Lu, Liangzhi Li, Nan Chen, Hongjie He, Linlin Xu, Jonathan Li</strong></p>
<p>Urban digital twins are virtual replicas of cities that use multi-source data and data analytics to optimize urban planning, infrastructure management, and decision-making. Towards this, we propose a framework focused on the single-building scale. By connecting to cloud mapping platforms such as Google Map Platforms APIs, by leveraging state-of-the-art multi-agent Large Language Models data analysis using ChatGPT(4o) and Deepseek-V3&#x2F;R1, and by using our Gaussian Splatting-based mesh extraction pipeline, our Digital Twin Buildings framework can retrieve a buildingâ€™s 3D model, visual descriptions, and achieve cloud-based mapping integration with large language model-based data analytics using a buildingâ€™s address, postal code, or geographic coordinates. </p>
<blockquote>
<p>åŸå¸‚æ•°å­—åŒèƒèƒæ˜¯åˆ©ç”¨å¤šæºæ•°æ®å’Œæ•°æ®åˆ†æä¼˜åŒ–åŸå¸‚è§„åˆ’ã€åŸºç¡€è®¾æ–½ç®¡ç†å’Œå†³ç­–åˆ¶å®šçš„åŸå¸‚è™šæ‹Ÿå‰¯æœ¬ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªä»¥å•æ ‹å»ºç­‘è§„æ¨¡ä¸ºé‡ç‚¹çš„æ¡†æ¶ã€‚é€šè¿‡è¿æ¥åˆ°è°·æ­Œåœ°å›¾å¹³å°APIç­‰äº‘åœ°å›¾å¹³å°ï¼Œåˆ©ç”¨æœ€æ–°çš„å¤šæ™ºèƒ½ä½“å¤§å‹è¯­è¨€æ¨¡å‹ï¼Œä½¿ç”¨ChatGPTï¼ˆ4oï¼‰å’ŒDeepseek-V3&#x2F;R1è¿›è¡Œæ•°æ®åˆ†æï¼Œä»¥åŠåˆ©ç”¨æˆ‘ä»¬çš„åŸºäºé«˜æ–¯å–·æ¶‚æŠ€æœ¯çš„ç½‘æ ¼æå–ç®¡é“ï¼Œæˆ‘ä»¬çš„æ•°å­—åŒèƒèƒå»ºç­‘æ¡†æ¶å¯ä»¥æ£€ç´¢å»ºç­‘çš„3Dæ¨¡å‹å’Œè§†è§‰æè¿°ï¼Œå¹¶å®ç°åŸºäºäº‘çš„æ˜ å°„é›†æˆï¼Œé€šè¿‡å¤§å‹è¯­è¨€æ¨¡å‹æ•°æ®åˆ†æä¸å»ºç­‘åœ°å€ã€é‚®æ”¿ç¼–ç æˆ–åœ°ç†åæ ‡ç›¸ç»“åˆã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.05769v2">PDF</a> -Fixed minor typo</p>
<p><strong>Summary</strong><br>åŸå¸‚æ•°å­—åŒèƒèƒæ˜¯åŸå¸‚çš„è™šæ‹Ÿå‰¯æœ¬ï¼Œåˆ©ç”¨å¤šæºæ•°æ®å’Œæ•°æ®åˆ†æä¼˜åŒ–åŸå¸‚è§„åˆ’ã€åŸºç¡€è®¾æ–½ç®¡ç†å’Œå†³ç­–åˆ¶å®šã€‚æˆ‘ä»¬æå‡ºä¸€ä¸ªä¸“æ³¨äºå•ä½“å»ºç­‘å°ºåº¦çš„æ¡†æ¶ï¼Œé€šè¿‡è¿æ¥äº‘è®¡ç®—åœ°å›¾å¹³å°ã€åˆ©ç”¨æœ€æ–°çš„å¤šæ™ºèƒ½ä½“å¤§æ•°æ®æ¨¡å‹å’Œç®—æ³•ï¼Œå®ç°æ•°å­—åŒèƒèƒå»ºç­‘çš„äº‘æ˜ å°„é›†æˆï¼Œèƒ½æ£€ç´¢å»ºç­‘ç‰©çš„ä¸‰ç»´æ¨¡å‹ã€è§†è§‰æè¿°ç­‰ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>åŸå¸‚æ•°å­—åŒèƒèƒæ˜¯åŸå¸‚çš„è™šæ‹Ÿå‰¯æœ¬ï¼Œä½¿ç”¨å¤šæºæ•°æ®å’Œæ•°æ®åˆ†æä¼˜åŒ–åŸå¸‚è§„åˆ’å’Œç®¡ç†ã€‚</li>
<li>æå‡ºçš„æ¡†æ¶ä¸“æ³¨äºå•ä½“å»ºç­‘å°ºåº¦ã€‚</li>
<li>é€šè¿‡è¿æ¥äº‘è®¡ç®—åœ°å›¾å¹³å°è·å–å»ºç­‘ç‰©çš„ä¸‰ç»´æ¨¡å‹å’Œè§†è§‰æè¿°ã€‚</li>
<li>åˆ©ç”¨æœ€æ–°çš„å¤šæ™ºèƒ½ä½“å¤§æ•°æ®æ¨¡å‹å’Œç®—æ³•ï¼Œå¦‚ChatGPTå’ŒDeepseek-V3&#x2F;R1ï¼Œè¿›è¡Œæ•°æ®åˆ†æã€‚</li>
<li>æ•°å­—åŒèƒèƒå»ºç­‘æ¡†æ¶å¯ä»¥é€šè¿‡å»ºç­‘åœ°å€ã€é‚®æ”¿ç¼–ç æˆ–åœ°ç†åæ ‡å®ç°äº‘æ˜ å°„é›†æˆã€‚</li>
<li>è¯¥æ¡†æ¶æœ‰åŠ©äºä¼˜åŒ–å†³ç­–åˆ¶å®šï¼Œæå‡åŸå¸‚è§„åˆ’ã€åŸºç¡€è®¾æ–½ç®¡ç†çš„æ•ˆç‡å’Œå‡†ç¡®æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.05769">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-44afc3d2104c4d113003fd2f6c012be3.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ab7856b3c2d207d8e26d17ab282007ea.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-511989bdfc99fa44c3216465d43b8839.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f687c699bb65931360517387d7fae53f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-093fb16992977cba99aedd67067aeafb.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-cee475c836077e72d45aa97380cb700a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-fd69bacfb0b9c79536b9551b77ae9acc.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e3ea5af80ebe75832fea69e3b3e9326b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d83d478cf20703a2b1160e7153b264f1.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="ARR-Question-Answering-with-Large-Language-Models-via-Analyzing-Retrieving-and-Reasoning"><a href="#ARR-Question-Answering-with-Large-Language-Models-via-Analyzing-Retrieving-and-Reasoning" class="headerlink" title="ARR: Question Answering with Large Language Models via Analyzing,   Retrieving, and Reasoning"></a>ARR: Question Answering with Large Language Models via Analyzing,   Retrieving, and Reasoning</h2><p><strong>Authors:Yuwei Yin, Giuseppe Carenini</strong></p>
<p>Large language models (LLMs) achieve remarkable performance on challenging benchmarks that are often structured as multiple-choice question-answering (QA) tasks. Zero-shot Chain-of-Thought (CoT) prompting enhances reasoning in LLMs but provides only vague and generic guidance (â€œthink step by stepâ€). This paper introduces ARR, an intuitive and effective zero-shot prompting method that explicitly incorporates three key steps in QA solving: analyzing the intent of the question, retrieving relevant information, and reasoning step by step. Comprehensive experiments across diverse and challenging QA tasks demonstrate that ARR consistently improves the Baseline (without ARR prompting) and outperforms CoT. Ablation and case studies further validate the positive contributions of each component: analyzing, retrieving, and reasoning. Notably, intent analysis plays a vital role in ARR. Additionally, extensive evaluations across various model sizes, LLM series, and generation settings solidify the effectiveness, robustness, and generalizability of ARR. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨å…·æœ‰æŒ‘æˆ˜æ€§çš„åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºå“è¶Šçš„æ€§èƒ½ï¼Œè¿™äº›æµ‹è¯•é€šå¸¸è¢«æ„å»ºä¸ºå¤šé€‰é—®ç­”ï¼ˆQAï¼‰ä»»åŠ¡ã€‚é›¶æ ·æœ¬æ€ç»´é“¾ï¼ˆCoTï¼‰æç¤ºå¢å¼ºäº†LLMä¸­çš„æ¨ç†èƒ½åŠ›ï¼Œä½†åªæä¾›äº†æ¨¡ç³Šå’Œé€šç”¨çš„æŒ‡å¯¼ï¼ˆâ€œé€æ­¥æ€è€ƒâ€ï¼‰ã€‚æœ¬æ–‡ä»‹ç»äº†ä¸€ç§ç›´è§‚æœ‰æ•ˆçš„é›¶æ ·æœ¬æç¤ºæ–¹æ³•ARRï¼Œè¯¥æ–¹æ³•æ˜¾å¼åœ°ç»“åˆäº†é—®ç­”è§£å†³ä¸­çš„ä¸‰ä¸ªå…³é”®æ­¥éª¤ï¼šåˆ†æé—®é¢˜çš„æ„å›¾ã€æ£€ç´¢ç›¸å…³ä¿¡æ¯å’Œé€æ­¥æ¨ç†ã€‚åœ¨å¤šæ ·åŒ–å’Œå…·æœ‰æŒ‘æˆ˜æ€§çš„é—®ç­”ä»»åŠ¡ä¸Šçš„ç»¼åˆå®éªŒè¡¨æ˜ï¼ŒARRå§‹ç»ˆæ”¹è¿›äº†åŸºçº¿ï¼ˆæ— ARRæç¤ºï¼‰å¹¶ä¼˜äºCoTã€‚æ¶ˆèç ”ç©¶å’Œæ¡ˆä¾‹ç ”ç©¶è¿›ä¸€æ­¥éªŒè¯äº†æ¯ä¸ªç»„ä»¶çš„æ­£é¢è´¡çŒ®ï¼šåˆ†æã€æ£€ç´¢å’Œæ¨ç†ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œæ„å›¾åˆ†æåœ¨ARRä¸­èµ·ç€è‡³å…³é‡è¦çš„ä½œç”¨ã€‚æ­¤å¤–ï¼Œåœ¨ä¸åŒæ¨¡å‹å¤§å°ã€LLMç³»åˆ—å’Œç”Ÿæˆè®¾ç½®ä¸Šçš„å¹¿æ³›è¯„ä¼°å·©å›ºäº†ARRçš„æœ‰æ•ˆæ€§ã€ç¨³å®šæ€§å’Œé€šç”¨æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.04689v2">PDF</a> 20 pages. Code: <a target="_blank" rel="noopener" href="https://github.com/YuweiYin/ARR">https://github.com/YuweiYin/ARR</a></p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨ä½œä¸ºå¤šé€‰é—®ç­”ä»»åŠ¡çš„ç»“æ„åŒ–æŒ‘æˆ˜åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºå“è¶Šæ€§èƒ½ã€‚é›¶æ ·æœ¬æ€ç»´é“¾ï¼ˆCoTï¼‰æç¤ºå¢å¼ºäº†LLMçš„æ¨ç†èƒ½åŠ›ï¼Œä½†ä»…æä¾›æ¨¡ç³Šå’Œé€šç”¨çš„æŒ‡å¯¼ã€‚æœ¬æ–‡ä»‹ç»äº†ä¸€ç§ç›´è§‚æœ‰æ•ˆçš„é›¶æ ·æœ¬æç¤ºæ–¹æ³•ARRï¼Œè¯¥æ–¹æ³•æ˜ç¡®ç»“åˆäº†é—®ç­”è§£å†³çš„ä¸‰ä¸ªå…³é”®æ­¥éª¤ï¼šåˆ†æé—®é¢˜çš„æ„å›¾ã€æ£€ç´¢ç›¸å…³ä¿¡æ¯å’Œé€æ­¥æ¨ç†ã€‚åœ¨å¤šæ ·åŒ–å’Œå…·æœ‰æŒ‘æˆ˜æ€§çš„é—®ç­”ä»»åŠ¡ä¸Šçš„ç»¼åˆå®éªŒè¡¨æ˜ï¼ŒARRåœ¨åŸºçº¿ï¼ˆæ— ARRæç¤ºï¼‰çš„åŸºç¡€ä¸ŠæŒç»­æé«˜äº†æ€§èƒ½ï¼Œå¹¶ä¼˜äºCoTã€‚æ­¤å¤–ï¼Œé€šè¿‡æ¶ˆé™¤ç ”ç©¶å’Œæ¡ˆä¾‹ç ”ç©¶è¿›ä¸€æ­¥éªŒè¯äº†åˆ†æã€æ£€ç´¢å’Œæ¨ç†æ¯ä¸ªç»„ä»¶çš„ç§¯æä½œç”¨ã€‚æ„å›¾åˆ†æåœ¨ARRä¸­æ‰®æ¼”äº†è‡³å…³é‡è¦çš„è§’è‰²ã€‚åŒæ—¶ï¼Œè·¨ä¸åŒæ¨¡å‹å¤§å°ã€LLMç³»åˆ—å’Œç”Ÿæˆè®¾ç½®çš„å¹¿æ³›è¯„ä¼°å·©å›ºäº†ARRçš„æœ‰æ•ˆæ€§ã€ç¨³å¥æ€§å’Œé€šç”¨æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLMåœ¨å¤šé€‰é—®ç­”ä»»åŠ¡çš„æŒ‘æˆ˜åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜ç§€ã€‚</li>
<li>é›¶æ ·æœ¬æ€ç»´é“¾ï¼ˆCoTï¼‰æç¤ºå¢å¼ºäº†LLMçš„æ¨ç†èƒ½åŠ›ï¼Œä½†æŒ‡å¯¼è¾ƒä¸ºæ¨¡ç³Šã€‚</li>
<li>ARRæ˜¯ä¸€ç§æ–°çš„é›¶æ ·æœ¬æç¤ºæ–¹æ³•ï¼Œæ˜ç¡®ç»“åˆäº†QAè§£å†³çš„ä¸‰ä¸ªå…³é”®æ­¥éª¤ï¼šåˆ†æã€æ£€ç´¢å’Œæ¨ç†ã€‚</li>
<li>ARRåœ¨å¤šç§é—®ç­”ä»»åŠ¡ä¸Šè¡¨ç°å‡ºæ¯”CoTæ›´å¥½çš„æ€§èƒ½ã€‚</li>
<li>æ¶ˆèå’Œæ¡ˆä¾‹ç ”ç©¶éªŒè¯äº†ARRä¸­åˆ†æã€æ£€ç´¢å’Œæ¨ç†æ¯ä¸ªç»„ä»¶çš„æœ‰æ•ˆæ€§ã€‚</li>
<li>æ„å›¾åˆ†æåœ¨ARRä¸­èµ·å…³é”®ä½œç”¨ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.04689">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-26699f54bc94e8d581d7b7c2062886a4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5ebedcb18bb34b74b747361e5af089c5.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-4ec8e7cbca2f2ce219139bdc92787ddd.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-09db45d8497402eb6d1f3c691754d932.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-35694cf858bf7a43795e2f4607148a02.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="Ola-Pushing-the-Frontiers-of-Omni-Modal-Language-Model-with-Progressive-Modality-Alignment"><a href="#Ola-Pushing-the-Frontiers-of-Omni-Modal-Language-Model-with-Progressive-Modality-Alignment" class="headerlink" title="Ola: Pushing the Frontiers of Omni-Modal Language Model with Progressive   Modality Alignment"></a>Ola: Pushing the Frontiers of Omni-Modal Language Model with Progressive   Modality Alignment</h2><p><strong>Authors:Zuyan Liu, Yuhao Dong, Jiahui Wang, Ziwei Liu, Winston Hu, Jiwen Lu, Yongming Rao</strong></p>
<p>Recent advances in large language models, particularly following GPT-4o, have sparked increasing interest in developing omni-modal models capable of understanding more modalities. While some open-source alternatives have emerged, there is still a notable lag behind specialized single-modality models in performance. In this paper, we present Ola, an Omni-modal language model that achieves competitive performance across image, video, and audio understanding compared to specialized counterparts. The core design of Ola lies in its progressive modality alignment strategy that extends the supporting modality of the language model progressively. Our training pipeline begins with the most distinct modalities: image and text, then gradually expands the skill sets of the model using speech data that connects language and audio knowledge, and video data that connects all modalities. The progressive learning pipeline also enables us to maintain a relatively small size of the cross-modal alignment data, making developing omni-modal from existing vision-language models easy and less costly. Moreover, to unlock an advanced interactive experience like GPT-4o, we further design a sentence-wise decoding solution for streaming speech generation. Extensive experiments demonstrate that Ola surpasses existing open omni-modal LLMs across all modalities while achieving highly competitive performance compared to state-of-the-art specialized models of similar sizes. We aim to make Ola a fully open omni-modal understanding solution to advance future research in this emerging field. Model weights, code, and data are open-sourced at <a target="_blank" rel="noopener" href="https://github.com/Ola-Omni/Ola">https://github.com/Ola-Omni/Ola</a>. </p>
<blockquote>
<p>æœ€è¿‘å¤§å‹è¯­è¨€æ¨¡å‹çš„è¿›æ­¥ï¼Œå°¤å…¶æ˜¯GPT-4oä¹‹åï¼Œå¼•å‘äº†äººä»¬å¯¹å¼€å‘èƒ½å¤Ÿç†è§£æ›´å¤šæ¨¡æ€çš„é€šæ¨¡æ¨¡å‹çš„æµ“åšå…´è¶£ã€‚è™½ç„¶å·²ç»å‡ºç°äº†ä¸€äº›å¼€æºçš„æ›¿ä»£å“ï¼Œä½†åœ¨æ€§èƒ½ä¸Šä»ç„¶æ˜æ˜¾è½åäºä¸“ä¸šçš„å•æ¨¡æ€æ¨¡å‹ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†Olaï¼Œè¿™æ˜¯ä¸€æ¬¾é€šæ¨¡æ€è¯­è¨€æ¨¡å‹ï¼Œåœ¨å›¾åƒã€è§†é¢‘å’ŒéŸ³é¢‘ç†è§£æ–¹é¢ä¸ä¸“ä¸šåŒ–æ¨¡å‹ç›¸æ¯”å…·æœ‰ç«äº‰åŠ›ã€‚Olaçš„æ ¸å¿ƒè®¾è®¡åœ¨äºå…¶æ¸è¿›çš„æ¨¡æ€å¯¹é½ç­–ç•¥ï¼Œè¯¥ç­–ç•¥é€æ­¥æ‰©å±•è¯­è¨€æ¨¡å‹çš„æ”¯æŒæ¨¡æ€ã€‚æˆ‘ä»¬çš„è®­ç»ƒç®¡é“å§‹äºæœ€ç‹¬ç‰¹çš„æ¨¡æ€ï¼šå›¾åƒå’Œæ–‡æœ¬ï¼Œç„¶åä½¿ç”¨è¿æ¥è¯­è¨€å’ŒéŸ³é¢‘çŸ¥è¯†çš„è¯­éŸ³æ•°æ®ä»¥åŠè¿æ¥æ‰€æœ‰æ¨¡æ€çš„è§†é¢‘æ•°æ®ï¼Œé€æ­¥æ‰©å¤§æ¨¡å‹çš„æŠ€èƒ½é›†ã€‚è¿™ç§æ¸è¿›çš„å­¦ä¹ ç®¡é“è¿˜ä½¿æˆ‘ä»¬èƒ½å¤Ÿä¿æŒç›¸å¯¹è¾ƒå°çš„è·¨æ¨¡æ€å¯¹é½æ•°æ®é›†è§„æ¨¡ï¼Œä»è€Œæ›´å®¹æ˜“ã€æ›´ç»æµåœ°ä»ç°æœ‰çš„è§†è§‰è¯­è¨€æ¨¡å‹å¼€å‘é€šæ¨¡æ€æ¨¡å‹ã€‚æ­¤å¤–ï¼Œä¸ºäº†è§£é”ç±»ä¼¼GPT-4oçš„é«˜çº§äº¤äº’ä½“éªŒï¼Œæˆ‘ä»¬è¿˜è®¾è®¡äº†ä¸€ç§åŸºäºå¥å­çš„è§£ç è§£å†³æ–¹æ¡ˆï¼Œç”¨äºæµå¼è¯­éŸ³ç”Ÿæˆã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼ŒOlaåœ¨æ‰€æœ‰æ¨¡æ€ä¸Šè¶…è¶Šäº†ç°æœ‰çš„å¼€æºé€šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ŒåŒæ—¶åœ¨ç±»ä¼¼è§„æ¨¡çš„ä¸“ä¸šæ¨¡å‹ä¸­å–å¾—äº†é«˜åº¦ç«äº‰çš„æ€§èƒ½ã€‚æˆ‘ä»¬çš„ç›®æ ‡æ˜¯ä½¿Olaæˆä¸ºå®Œå…¨å¼€æ”¾çš„é€šæ¨¡æ€è§£å†³æ–¹æ¡ˆï¼Œä»¥æ¨åŠ¨è¿™ä¸€æ–°å…´é¢†åŸŸæœªæ¥çš„ç ”ç©¶ã€‚æ¨¡å‹æƒé‡ã€ä»£ç å’Œæ•°æ®å·²åœ¨<a target="_blank" rel="noopener" href="https://github.com/Ola-Omni/Ola%E4%B8%8A%E5%BC%80%E6%BA%90%E3%80%82">https://github.com/Ola-Omni/Olaä¸Šå¼€æºã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.04328v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>è¿‘æœŸå¤§å‹è¯­è¨€æ¨¡å‹çš„è¿›æ­¥ï¼Œå°¤å…¶æ˜¯GPT-4oä¹‹åï¼Œå¼•å‘äº†å¤šæ¨¡æ€æ¨¡å‹çš„å…´è¶£ï¼Œå®ƒä»¬èƒ½ç†è§£æ›´å¤šæ¨¡æ€ã€‚æœ¬æ–‡ä»‹ç»äº†ä¸€æ¬¾åä¸ºOlaçš„é€šç”¨å¤šæ¨¡æ€è¯­è¨€æ¨¡å‹ï¼Œåœ¨å›¾åƒã€è§†é¢‘å’ŒéŸ³é¢‘ç†è§£æ–¹é¢è¡¨ç°å‡ºç«äº‰åŠ›ã€‚Olaçš„æ ¸å¿ƒè®¾è®¡åœ¨äºå…¶æ¸è¿›å¼æ¨¡æ€å¯¹é½ç­–ç•¥ï¼Œé€æ­¥æ‰©å±•è¯­è¨€æ¨¡å‹çš„æ”¯æŒæ¨¡æ€ã€‚å…¶è®­ç»ƒæµç¨‹ä»æœ€å…·ç‰¹è‰²çš„æ¨¡æ€å¼€å§‹ï¼Œå³å›¾åƒå’Œæ–‡æœ¬ï¼Œç„¶åé€æ­¥ä½¿ç”¨è¿æ¥è¯­è¨€å’ŒéŸ³é¢‘çŸ¥è¯†çš„è¯­éŸ³æ•°æ®ä»¥åŠè¿æ¥æ‰€æœ‰æ¨¡æ€çš„è§†é¢‘æ•°æ®æ¥æ‰©å±•æ¨¡å‹æŠ€èƒ½é›†ã€‚è¿™ç§æ¸è¿›å¼å­¦ä¹ æµç¨‹ä¹Ÿè®©æˆ‘ä»¬å¾—ä»¥ç»´æŒè¾ƒå°çš„è·¨æ¨¡æ€å¯¹é½æ•°æ®é‡ï¼Œä½¿ä»ç°æœ‰è§†è§‰è¯­è¨€æ¨¡å‹å‘å±•å‡ºOmni-modalå˜å¾—æ›´åŠ å®¹æ˜“ä¸”æˆæœ¬æ›´ä½ã€‚æ­¤å¤–ï¼Œä¸ºäº†è§£é”ç±»ä¼¼GPT-4oçš„é«˜çº§äº¤äº’ä½“éªŒï¼Œæˆ‘ä»¬è¿›ä¸€æ­¥è®¾è®¡äº†æµå¼è¯­éŸ³ç”Ÿæˆçš„å¥å­çº§è§£ç è§£å†³æ–¹æ¡ˆã€‚å®éªŒè¡¨æ˜ï¼ŒOlaåœ¨æ‰€æœ‰æ¨¡æ€ä¸Šéƒ½è¶…è¶Šäº†ç°æœ‰çš„å¼€æºOmni-modalå¤§å‹è¯­è¨€æ¨¡å‹ï¼Œå¹¶åœ¨ç±»ä¼¼è§„æ¨¡çš„æœ€æ–°ä¸“ä¸šæ¨¡å‹ä¸Šå–å¾—äº†å…·æœ‰ç«äº‰åŠ›çš„è¡¨ç°ã€‚æˆ‘ä»¬çš„ç›®æ ‡æ˜¯è®©Olaæˆä¸ºå®Œå…¨å¼€æºçš„å¤šæ¨¡æ€ç†è§£è§£å†³æ–¹æ¡ˆï¼Œä»¥æ¨åŠ¨è¿™ä¸€æ–°å…´é¢†åŸŸçš„ç ”ç©¶å‘å±•ã€‚æ¨¡å‹æƒé‡ã€ä»£ç å’Œæ•°æ®å·²åœ¨GitHubä¸Šå¼€æºã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Olaæ˜¯ä¸€æ¬¾å¤šæ¨¡æ€è¯­è¨€æ¨¡å‹ï¼Œå…·å¤‡åœ¨å›¾åƒã€è§†é¢‘å’ŒéŸ³é¢‘ç†è§£æ–¹é¢çš„ç«äº‰åŠ›ã€‚</li>
<li>Olaé‡‡ç”¨æ¸è¿›å¼æ¨¡æ€å¯¹é½ç­–ç•¥è¿›è¡Œè®­ç»ƒï¼Œé€æ­¥æ‰©å±•è¯­è¨€æ¨¡å‹çš„æ¨¡æ€æ”¯æŒèƒ½åŠ›ã€‚</li>
<li>Olaçš„è®­ç»ƒæµç¨‹ä»å›¾åƒå’Œæ–‡æœ¬å¼€å§‹ï¼Œç„¶åé€æ­¥å¼•å…¥è¯­éŸ³å’Œè§†é¢‘æ•°æ®æ¥æ‰©å±•æ¨¡å‹æŠ€èƒ½é›†ã€‚</li>
<li>æ¸è¿›å¼å­¦ä¹ æµç¨‹ä½¿å¾—è·¨æ¨¡æ€å¯¹é½çš„æ•°æ®é‡ç›¸å¯¹è¾ƒå°ï¼Œé™ä½äº†å¼€å‘æˆæœ¬ã€‚</li>
<li>Olaè®¾è®¡äº†å¥å­çº§è§£ç è§£å†³æ–¹æ¡ˆï¼Œä»¥æ”¯æŒæµå¼è¯­éŸ³ç”Ÿæˆç­‰é«˜çº§äº¤äº’ä½“éªŒã€‚</li>
<li>Olaåœ¨å¤šä¸ªå®éªŒä¸Šè¶…è¶Šäº†ç°æœ‰çš„å¼€æºOmni-modalå¤§å‹è¯­è¨€æ¨¡å‹ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.04328">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-1a3a9952756be81ee4f0ceca05bb634b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-626c4fc88e7d0b71cec8356f0d2ae844.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3091b78a842b040147a3b51e76f39499.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8d2f0d54b6f8fa60a718ec67b9aa10d6.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-59fbfbd3e80017c232a18668c39d327b.jpg" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="TAID-Temporally-Adaptive-Interpolated-Distillation-for-Efficient-Knowledge-Transfer-in-Language-Models"><a href="#TAID-Temporally-Adaptive-Interpolated-Distillation-for-Efficient-Knowledge-Transfer-in-Language-Models" class="headerlink" title="TAID: Temporally Adaptive Interpolated Distillation for Efficient   Knowledge Transfer in Language Models"></a>TAID: Temporally Adaptive Interpolated Distillation for Efficient   Knowledge Transfer in Language Models</h2><p><strong>Authors:Makoto Shing, Kou Misaki, Han Bao, Sho Yokoi, Takuya Akiba</strong></p>
<p>Causal language models have demonstrated remarkable capabilities, but their size poses significant challenges for deployment in resource-constrained environments. Knowledge distillation, a widely-used technique for transferring knowledge from a large teacher model to a small student model, presents a promising approach for model compression. A significant remaining issue lies in the major differences between teacher and student models, namely the substantial capacity gap, mode averaging, and mode collapse, which pose barriers during distillation. To address these issues, we introduce $\textit{Temporally Adaptive Interpolated Distillation (TAID)}$, a novel knowledge distillation approach that dynamically interpolates student and teacher distributions through an adaptive intermediate distribution, gradually shifting from the studentâ€™s initial distribution towards the teacherâ€™s distribution. We provide a theoretical analysis demonstrating TAIDâ€™s ability to prevent mode collapse and empirically show its effectiveness in addressing the capacity gap while balancing mode averaging and mode collapse. Our comprehensive experiments demonstrate TAIDâ€™s superior performance across various model sizes and architectures in both instruction tuning and pre-training scenarios. Furthermore, we showcase TAIDâ€™s practical impact by developing two state-of-the-art compact foundation models: $\texttt{TAID-LLM-1.5B}$ for language tasks and $\texttt{TAID-VLM-2B}$ for vision-language tasks. These results demonstrate TAIDâ€™s effectiveness in creating high-performing and efficient models, advancing the development of more accessible AI technologies. </p>
<blockquote>
<p>å› æœè¯­è¨€æ¨¡å‹å±•ç°å‡ºäº†æ˜¾è‘—çš„èƒ½åŠ›ï¼Œä½†å®ƒä»¬çš„è§„æ¨¡ç»™åœ¨èµ„æºå—é™ç¯å¢ƒä¸­éƒ¨ç½²å¸¦æ¥äº†é‡å¤§æŒ‘æˆ˜ã€‚çŸ¥è¯†è’¸é¦æ˜¯ä¸€ç§å¹¿æ³›ä½¿ç”¨çš„æŠ€æœ¯ï¼Œå¯ä»¥ä»å¤§å‹æ•™å¸ˆæ¨¡å‹è½¬ç§»åˆ°å°å‹å­¦ç”Ÿæ¨¡å‹ï¼Œè¿™ä¸ºæ¨¡å‹å‹ç¼©æä¾›äº†ä¸€ç§æœ‰å‰æ™¯çš„æ–¹æ³•ã€‚ç„¶è€Œï¼Œä»ç„¶å­˜åœ¨ä¸€ä¸ªä¸»è¦é—®é¢˜ï¼Œå³æ•™å¸ˆæ¨¡å‹å’Œå­¦ç”Ÿæ¨¡å‹ä¹‹é—´çš„å·®å¼‚ï¼ŒåŒ…æ‹¬æ˜¾è‘—çš„èƒ½åŠ›å·®è·ã€æ¨¡å¼å¹³å‡å’Œæ¨¡å¼å´©æºƒï¼Œè¿™äº›åœ¨è’¸é¦è¿‡ç¨‹ä¸­æ„æˆäº†éšœç¢ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†<em>æ—¶åºè‡ªé€‚åº”æ’å€¼è’¸é¦ï¼ˆTAIDï¼‰</em>ï¼Œè¿™æ˜¯ä¸€ç§æ–°å‹çš„çŸ¥è¯†è’¸é¦æ–¹æ³•ï¼Œé€šè¿‡è‡ªé€‚åº”ä¸­é—´åˆ†å¸ƒåŠ¨æ€æ’å€¼å­¦ç”Ÿå’Œæ•™å¸ˆçš„åˆ†å¸ƒï¼Œä»å­¦ç”Ÿæœ€åˆçš„åˆ†å¸ƒé€æ¸è½¬å‘æ•™å¸ˆçš„åˆ†å¸ƒã€‚æˆ‘ä»¬è¿›è¡Œäº†ç†è®ºåˆ†æï¼Œè¯æ˜äº†TAIDåœ¨é˜²æ­¢æ¨¡å¼å´©æºƒæ–¹é¢çš„èƒ½åŠ›ï¼Œå¹¶å®è¯è¯æ˜äº†å®ƒåœ¨è§£å†³èƒ½åŠ›å·®è·ã€å¹³è¡¡æ¨¡å¼å¹³å‡å’Œæ¨¡å¼å´©æºƒæ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚æˆ‘ä»¬çš„ç»¼åˆå®éªŒè¡¨æ˜ï¼ŒTAIDåœ¨å„ç§å‹å·å’Œæ¶æ„çš„æ¨¡å‹ä¸­ï¼Œæ— è®ºæ˜¯åœ¨æŒ‡ä»¤å¾®è°ƒè¿˜æ˜¯é¢„è®­ç»ƒåœºæ™¯ä¸­ï¼Œéƒ½è¡¨ç°å‡ºå“è¶Šçš„æ€§èƒ½ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬é€šè¿‡å¼€å‘ä¸¤ä¸ªæœ€å…ˆè¿›çš„ç´§å‡‘åŸºç¡€æ¨¡å‹ï¼šç”¨äºè¯­è¨€ä»»åŠ¡çš„TAID-LLM-1.5Bå’Œç”¨äºè§†è§‰è¯­è¨€ä»»åŠ¡çš„TAID-VLM-2Bï¼Œå±•ç¤ºäº†TAIDçš„å®é™…å½±å“ã€‚è¿™äº›ç»“æœè¯æ˜äº†TAIDåœ¨åˆ›å»ºé«˜æ€§èƒ½å’Œé«˜æ•ˆæ¨¡å‹æ–¹é¢çš„æœ‰æ•ˆæ€§ï¼Œæ¨åŠ¨äº†æ›´å¯è®¿é—®çš„äººå·¥æ™ºèƒ½æŠ€æœ¯çš„å‘å±•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.16937v3">PDF</a> To appear at the 13th International Conference on Learning   Representations (ICLR 2025) as a Spotlight presentation</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†å› æœè¯­è¨€æ¨¡å‹åœ¨èµ„æºå—é™ç¯å¢ƒä¸­çš„éƒ¨ç½²æŒ‘æˆ˜ï¼Œä»¥åŠçŸ¥è¯†è’¸é¦æŠ€æœ¯åœ¨æ­¤æ–¹é¢çš„åº”ç”¨ã€‚ä¸ºè§£å†³çŸ¥è¯†è’¸é¦ä¸­æ•™å¸ˆæ¨¡å‹ä¸å­¦ç”Ÿæ¨¡å‹é—´å­˜åœ¨çš„å®¹é‡å·®è·ã€æ¨¡å¼å¹³å‡å’Œæ¨¡å¼å´©æºƒç­‰é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§æ–°å‹çŸ¥è¯†è’¸é¦æ–¹æ³•â€”â€”Temporally Adaptive Interpolated Distillation (TAID)ã€‚è¯¥æ–¹æ³•é€šè¿‡è‡ªé€‚åº”ä¸­é—´åˆ†å¸ƒåŠ¨æ€æ’å€¼å­¦ç”Ÿæ¨¡å‹å’Œæ•™å¸ˆæ¨¡å‹çš„åˆ†å¸ƒï¼Œé€æ­¥ä»å­¦ç”Ÿæ¨¡å‹çš„åˆå§‹åˆ†å¸ƒå‘æ•™å¸ˆæ¨¡å‹çš„åˆ†å¸ƒè½¬å˜ã€‚ç†è®ºåˆ†æè¯æ˜äº†TAIDåœ¨é˜²æ­¢æ¨¡å¼å´©æºƒæ–¹é¢çš„èƒ½åŠ›ï¼Œå¹¶é€šè¿‡å®éªŒéªŒè¯äº†å…¶åœ¨è§£å†³å®¹é‡å·®è·ã€å¹³è¡¡æ¨¡å¼å¹³å‡å’Œæ¨¡å¼å´©æºƒæ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚æ­¤å¤–ï¼Œé€šè¿‡å¼€å‘ä¸¤ä¸ªå…ˆè¿›çš„ç´§å‡‘åŸºç¡€æ¨¡å‹TAID-LLM-1.5Bå’ŒTAID-VLM-2Bï¼Œå±•ç¤ºäº†TAIDçš„å®é™…å½±å“åŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å› æœè¯­è¨€æ¨¡å‹åœ¨èµ„æºå—é™ç¯å¢ƒä¸­çš„éƒ¨ç½²é¢ä¸´æŒ‘æˆ˜ã€‚</li>
<li>çŸ¥è¯†è’¸é¦æ˜¯è§£å†³è¿™ä¸€æŒ‘æˆ˜çš„ä¸€ç§æœ‰å‰æ™¯çš„æ–¹æ³•ã€‚</li>
<li>æ•™å¸ˆæ¨¡å‹å’Œå­¦ç”Ÿæ¨¡å‹ä¹‹é—´å­˜åœ¨æ˜¾è‘—çš„å·®å¼‚ï¼Œå¦‚å®¹é‡å·®è·ã€æ¨¡å¼å¹³å‡å’Œæ¨¡å¼å´©æºƒã€‚</li>
<li>æ–°å‹çŸ¥è¯†è’¸é¦æ–¹æ³•â€”â€”Temporally Adaptive Interpolated Distillation (TAID) è¢«æå‡ºä»¥è§£å†³è¿™äº›é—®é¢˜ã€‚</li>
<li>TAIDé€šè¿‡è‡ªé€‚åº”ä¸­é—´åˆ†å¸ƒåŠ¨æ€æ’å€¼å­¦ç”Ÿæ¨¡å‹å’Œæ•™å¸ˆæ¨¡å‹çš„åˆ†å¸ƒã€‚</li>
<li>ç†è®ºå’Œå®éªŒè¯æ˜äº†TAIDåœ¨è§£å†³å®¹é‡å·®è·å’Œå¹³è¡¡æ¨¡å¼å¹³å‡ä¸æ¨¡å¼å´©æºƒæ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.16937">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-3d23d30b6f672b190bc5c3419a8132e0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7d19b6b370cd647f9e7364ec50c82a4a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-65bc89ef8169783c6875b2aeb18c91a1.jpg" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="URSA-Understanding-and-Verifying-Chain-of-thought-Reasoning-in-Multimodal-Mathematics"><a href="#URSA-Understanding-and-Verifying-Chain-of-thought-Reasoning-in-Multimodal-Mathematics" class="headerlink" title="URSA: Understanding and Verifying Chain-of-thought Reasoning in   Multimodal Mathematics"></a>URSA: Understanding and Verifying Chain-of-thought Reasoning in   Multimodal Mathematics</h2><p><strong>Authors:Ruilin Luo, Zhuofan Zheng, Yifan Wang, Yiyao Yu, Xinzhe Ni, Zicheng Lin, Jin Zeng, Yujiu Yang</strong></p>
<p>Chain-of-Thought (CoT) reasoning is widely used to enhance the mathematical reasoning capabilities of large language models (LLMs). The introduction of process supervision for CoT trajectories has sparked discussions on improving test-time scaling, thereby unlocking the System 2-style thinking capabilities of these models. However, in multimodal mathematical reasoning, the scarcity of high-quality CoT training data has hindered existing models from achieving both deliberate reasoning and fine-grained verification. In this work, we propose a novel framework that introduces System 2-style thinking to multimodal mathematical reasoning. We introduce a three-module CoT data synthesis process that integrates CoT distillation, trajectory-format rewriting, and format unification. This process generates MMathCoT-1M, a high-quality CoT reasoning instruction fine-tuning dataset. Furthermore, we implement a dual-view trajectory labeling automation that targets both visual grounding fidelity and deductive chain validity, resulting in the DualMath-1.1M dataset. The URSA-8B model, trained on MMathCoT-1M, achieves new state-of-the-art (SOTA) performance among similarly sized multimodal LLMs on six popular reasoning benchmarks. Training URSA-8B further on the DualMath-1.1M dataset yields URSA-RM-8B, a verifier that enhances URSA-8Bâ€™s test-time performance and surpasses strong closed-source multimodal MLLMs like GPT-4o. The model weights, training data, and code have been open-sourced: <a target="_blank" rel="noopener" href="https://github.com/URSA-MATH/URSA-MATH">https://github.com/URSA-MATH/URSA-MATH</a>. </p>
<blockquote>
<p>é“¾å¼æ€ç»´ï¼ˆCoTï¼‰æ¨ç†è¢«å¹¿æ³›åº”ç”¨äºå¢å¼ºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ•°å­¦æ¨ç†èƒ½åŠ›ã€‚å¼•å…¥è¿‡ç¨‹ç›‘ç£ä»¥ä¼˜åŒ–CoTè½¨è¿¹çš„è®¨è®ºï¼Œä»è€Œè§£é”è¿™äº›æ¨¡å‹çš„System 2å¼æ€ç»´èƒ½åŠ›ã€‚ç„¶è€Œï¼Œåœ¨å¤šæ¨¡æ€æ•°å­¦æ¨ç†ä¸­ï¼Œé«˜è´¨é‡CoTè®­ç»ƒæ•°æ®çš„ç¨€ç¼ºé˜»ç¢äº†ç°æœ‰æ¨¡å‹å®ç°æ·±æ€ç†Ÿè™‘çš„æ¨ç†å’Œç²¾ç»†çš„éªŒè¯ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§å¼•å…¥System 2å¼æ€ç»´åˆ°å¤šæ¨¡æ€æ•°å­¦æ¨ç†çš„æ–°å‹æ¡†æ¶ã€‚æˆ‘ä»¬å¼•å…¥äº†åŒ…å«CoTè’¸é¦ã€è½¨è¿¹æ ¼å¼é‡å†™å’Œæ ¼å¼ç»Ÿä¸€çš„ä¸‰ä¸ªæ¨¡å—CoTæ•°æ®åˆæˆè¿‡ç¨‹ï¼Œç”Ÿæˆäº†é«˜è´¨é‡çš„MMathCoT-1M CoTæ¨ç†æŒ‡ä»¤å¾®è°ƒæ•°æ®é›†ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å®ç°äº†æ—¨åœ¨å®ç°è§†è§‰å®šä½ä¿çœŸæ€§å’Œæ¼”ç»é“¾æœ‰æ•ˆæ€§çš„åŒè§†å›¾è½¨è¿¹æ ‡è®°è‡ªåŠ¨åŒ–ï¼Œåˆ›å»ºäº†DualMath-1.1Mæ•°æ®é›†ã€‚åœ¨å…­ä¸ªæµè¡Œçš„æ¨ç†åŸºå‡†æµ‹è¯•ä¸Šï¼Œè®­ç»ƒåœ¨MMathCoT-1Mä¸Šçš„URSA-8Bæ¨¡å‹å–å¾—äº†æœ€æ–°æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚è¿›ä¸€æ­¥åœ¨DualMath-1.1Mæ•°æ®é›†ä¸Šè®­ç»ƒURSA-8Bï¼Œäº§ç”Ÿäº†å¢å¼ºæµ‹è¯•æ—¶æ€§èƒ½å’Œè¶…è¶Šå¼ºå¤§å°é—­æºå¤šæ¨¡æ€LLLMsï¼ˆå¦‚GPT-4oï¼‰çš„éªŒè¯å™¨URSA-RM-8Bã€‚æ¨¡å‹æƒé‡ã€è®­ç»ƒæ•°æ®å’Œä»£ç å‡å·²å¼€æºï¼š<a target="_blank" rel="noopener" href="https://github.com/URSA-MATH/URSA-MATH">https://github.com/URSA-MATH/URSA-MATH</a>ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.04686v3">PDF</a> Fix typos and add results. 27 pages, 11 tables, 17 figures. Models,   training data and code have been open-sourced. Project url:   <a target="_blank" rel="noopener" href="https://ursa-math.github.io/">https://ursa-math.github.io</a></p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†å¦‚ä½•å°†System 2é£æ ¼çš„æ€è€ƒå¼•å…¥å¤šæ¨¡æ€æ•°å­¦æ¨ç†ä¸­ï¼Œé€šè¿‡æå‡ºä¸€ä¸ªåŒ…å«ä¸‰ä¸ªæ¨¡å—çš„è®¤çŸ¥é“¾æ•°æ®åˆæˆè¿‡ç¨‹ï¼ˆCoTè’¸é¦ã€è½¨è¿¹æ ¼å¼é‡å†™å’Œæ ¼å¼ç»Ÿä¸€ï¼‰ï¼Œç”Ÿæˆäº†MMathCoT-1Mé«˜è´¨é‡CoTæ¨ç†æŒ‡ä»¤å¾®è°ƒæ•°æ®é›†ã€‚æ­¤å¤–ï¼Œè¿˜å®ç°äº†é’ˆå¯¹è§†è§‰å®šä½å‡†ç¡®æ€§å’Œæ¼”ç»é“¾æœ‰æ•ˆæ€§çš„åŒè§†å›¾è½¨è¿¹æ ‡ç­¾è‡ªåŠ¨åŒ–ï¼Œåˆ›å»ºäº†DualMath-1.1Mæ•°æ®é›†ã€‚åœ¨å…­ä¸ªæµè¡Œçš„æ¨ç†åŸºå‡†æµ‹è¯•ä¸Šï¼Œç»è¿‡MMathCoT-1Mæ•°æ®è®­ç»ƒçš„URSA-8Bæ¨¡å‹å–å¾—äº†æœ€æ–°çŠ¶æ€çš„æœ€ä¼˜æ€§èƒ½ã€‚è¿›ä¸€æ­¥åœ¨DualMath-1.1Mæ•°æ®é›†ä¸Šè®­ç»ƒå¾—åˆ°çš„URSA-RM-8BéªŒè¯å™¨æå‡äº†URSA-8Bçš„æµ‹è¯•æ€§èƒ½ï¼Œè¶…è¶Šäº†å¼ºå¤§çš„å°é—­æºå¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼Œå¦‚GPT-4oã€‚æ¨¡å‹æƒé‡ã€è®­ç»ƒæ•°æ®å’Œä»£ç å‡å·²å¼€æºã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>System 2é£æ ¼çš„æ€è€ƒè¢«å¼•å…¥å¤šæ¨¡æ€æ•°å­¦æ¨ç†ä¸­ï¼Œä»¥æé«˜æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ã€‚</li>
<li>æå‡ºä¸€ä¸ªåŒ…å«CoTè’¸é¦ã€è½¨è¿¹æ ¼å¼é‡å†™å’Œæ ¼å¼ç»Ÿä¸€çš„ä¸‰æ¨¡å—CoTæ•°æ®åˆæˆè¿‡ç¨‹ï¼Œç”¨äºç”Ÿæˆé«˜è´¨é‡çš„CoTæ¨ç†æŒ‡ä»¤å¾®è°ƒæ•°æ®é›†MMathCoT-1Mã€‚</li>
<li>é€šè¿‡åŒè§†å›¾è½¨è¿¹æ ‡ç­¾è‡ªåŠ¨åŒ–ï¼Œåˆ›å»ºäº†DualMath-1.1Mæ•°æ®é›†ï¼Œè¯¥æ•°æ®é›†æ—¨åœ¨æé«˜è§†è§‰å®šä½å‡†ç¡®æ€§å’Œæ¼”ç»é“¾æœ‰æ•ˆæ€§ã€‚</li>
<li>URSA-8Bæ¨¡å‹åœ¨å¤šä¸ªæ¨ç†åŸºå‡†æµ‹è¯•ä¸Šè¡¨ç°å‡ºæœ€æ–°çŠ¶æ€çš„æœ€ä¼˜æ€§èƒ½ã€‚</li>
<li>URSA-RM-8BéªŒè¯å™¨åœ¨URSA-8Bçš„åŸºç¡€ä¸Šè¿›ä¸€æ­¥æå‡æµ‹è¯•æ€§èƒ½ï¼Œè¶…è¶Šäº†æŸäº›å…ˆè¿›çš„å¤§å‹è¯­è¨€æ¨¡å‹ã€‚</li>
<li>æ¨¡å‹æƒé‡ã€è®­ç»ƒæ•°æ®å’Œä»£ç å·²å¼€æºï¼Œä¾¿äºå…¬ä¼—è®¿é—®å’Œä½¿ç”¨ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.04686">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-53909e10dbba86683dfa420fe5c7847d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-0f226cede3d49679337bb1b36dd4c8b3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-89b3fbf3abfc062c2d16ba8876dbaf78.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-78ce45eb4c967d1e1e1dfc060c7580e8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-99704b810299ad9a94b975805a7da611.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-41fd13f59f8d8462c25a47b7de17d269.jpg" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1><h2 id="Topic-Aware-Knowledge-Graph-with-Large-Language-Models-for-Interoperability-in-Recommender-Systems"><a href="#Topic-Aware-Knowledge-Graph-with-Large-Language-Models-for-Interoperability-in-Recommender-Systems" class="headerlink" title="Topic-Aware Knowledge Graph with Large Language Models for   Interoperability in Recommender Systems"></a>Topic-Aware Knowledge Graph with Large Language Models for   Interoperability in Recommender Systems</h2><p><strong>Authors:Minhye Jeon, Seokho Ahn, Young-Duk Seo</strong></p>
<p>The use of knowledge graphs in recommender systems has become one of the common approaches to addressing data sparsity and cold start problems. Recent advances in large language models (LLMs) offer new possibilities for processing side and context information within knowledge graphs. However, consistent integration across various systems remains challenging due to the need for domain expert intervention and differences in system characteristics. To address these issues, we propose a consistent approach that extracts both general and specific topics from both side and context information using LLMs. First, general topics are iteratively extracted and updated from side information. Then, specific topics are extracted using context information. Finally, to address synonymous topics generated during the specific topic extraction process, a refining algorithm processes and resolves these issues effectively. This approach allows general topics to capture broad knowledge across diverse item characteristics, while specific topics emphasize detailed attributes, providing a more comprehensive understanding of the semantic features of items and the preferences of users. Experimental results demonstrate significant improvements in recommendation performance across diverse knowledge graphs. </p>
<blockquote>
<p>æ¨èç³»ç»Ÿä¸­ä½¿ç”¨çŸ¥è¯†å›¾è°±å·²æˆä¸ºè§£å†³æ•°æ®ç¨€ç–å’Œå†·å¯åŠ¨é—®é¢˜çš„å¸¸è§æ–¹æ³•ä¹‹ä¸€ã€‚å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æœ€æ–°è¿›å±•ä¸ºå¤„ç†çŸ¥è¯†å›¾è°±ä¸­çš„ä¾§é¢å’Œä¸Šä¸‹æ–‡ä¿¡æ¯æä¾›äº†æ–°çš„å¯èƒ½æ€§ã€‚ç„¶è€Œï¼Œç”±äºéœ€è¦é¢†åŸŸä¸“å®¶å¹²é¢„å’Œç³»ç»Ÿç‰¹æ€§å·®å¼‚ï¼Œè·¨å„ç§ç³»ç»Ÿçš„é›†æˆä»ç„¶å…·æœ‰æŒ‘æˆ˜æ€§ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ä¸€è‡´çš„æ–¹æ³•ï¼Œè¯¥æ–¹æ³•ä½¿ç”¨LLMä»ä¾§é¢å’Œä¸Šä¸‹æ–‡ä¿¡æ¯ä¸­æå–ä¸€èˆ¬å’Œç‰¹å®šä¸»é¢˜ã€‚é¦–å…ˆï¼Œä»ä¾§ä¿¡æ¯ä¸­è¿­ä»£æå–å’Œæ›´æ–°ä¸€èˆ¬ä¸»é¢˜ã€‚ç„¶åï¼Œä½¿ç”¨ä¸Šä¸‹æ–‡ä¿¡æ¯æå–ç‰¹å®šä¸»é¢˜ã€‚æœ€åï¼Œä¸ºäº†è§£å†³ç‰¹å®šä¸»é¢˜æå–è¿‡ç¨‹ä¸­äº§ç”Ÿçš„åŒä¹‰ä¸»é¢˜é—®é¢˜ï¼Œé‡‡ç”¨ç²¾ç‚¼ç®—æ³•å¯¹å…¶è¿›è¡Œæœ‰æ•ˆå¤„ç†ã€‚è¿™ç§æ–¹æ³•å…è®¸ä¸€èˆ¬ä¸»é¢˜æ•æ‰è·¨ä¸åŒé¡¹ç›®ç‰¹å¾çš„å¹¿æ³›çŸ¥è¯†ï¼Œè€Œç‰¹å®šä¸»é¢˜å¼ºè°ƒè¯¦ç»†å±æ€§ï¼Œä»è€Œæ›´å…¨é¢åœ°ç†è§£é¡¹ç›®çš„è¯­ä¹‰ç‰¹å¾ä»¥åŠç”¨æˆ·çš„åå¥½ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œåœ¨å¤šç§çŸ¥è¯†å›¾è°±ä¸­æ¨èæ€§èƒ½å¾—åˆ°äº†æ˜¾è‘—æé«˜ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.20163v3">PDF</a> Accepted by The 40th ACM&#x2F;SIGAPP Symposium On Applied Computing(SAC)   2025</p>
<p><strong>Summary</strong><br>çŸ¥è¯†å›¾è°±åœ¨æ¨èç³»ç»Ÿä¸­åº”ç”¨å¹¿æ³›ï¼Œä»¥è§£å†³æ•°æ®ç¨€ç–å’Œå†·å¯åŠ¨é—®é¢˜ã€‚å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æœ€æ–°è¿›å±•ä¸ºå¤„ç†çŸ¥è¯†å›¾è°±ä¸­çš„ä¾§ä¿¡æ¯å’Œä¸Šä¸‹æ–‡ä¿¡æ¯æä¾›äº†æ–°çš„å¯èƒ½æ€§ã€‚æœ¬æ–‡æå‡ºä¸€ç§æ–¹æ³•ï¼Œé€šè¿‡LLMä»ä¾§ä¿¡æ¯å’Œä¸Šä¸‹æ–‡ä¿¡æ¯ä¸­æå–ä¸€èˆ¬å’Œç‰¹å®šä¸»é¢˜ï¼Œè§£å†³è·¨ç³»ç»Ÿæ•´åˆä¸­çš„æŒ‘æˆ˜ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å¤šç§çŸ¥è¯†å›¾è°±ä¸­æ˜¾è‘—æé«˜æ¨èæ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>çŸ¥è¯†å›¾è°±åœ¨æ¨èç³»ç»Ÿä¸­ç”¨äºè§£å†³æ•°æ®ç¨€ç–å’Œå†·å¯åŠ¨é—®é¢˜ã€‚</li>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨å¤„ç†çŸ¥è¯†å›¾è°±ä¸­çš„ä¾§ä¿¡æ¯å’Œä¸Šä¸‹æ–‡ä¿¡æ¯æ–¹é¢è¡¨ç°å‡ºæ–°å¯èƒ½æ€§ã€‚</li>
<li>æœ¬æ–‡æ–¹æ³•é€šè¿‡LLMä»ä¾§ä¿¡æ¯å’Œä¸Šä¸‹æ–‡ä¿¡æ¯ä¸­æå–ä¸€èˆ¬å’Œç‰¹å®šä¸»é¢˜ï¼Œä»¥åº”å¯¹è·¨ç³»ç»Ÿæ•´åˆçš„æŒ‘æˆ˜ã€‚</li>
<li>ä¸€èˆ¬ä¸»é¢˜èƒ½æ•æ‰è·¨ä¸åŒé¡¹ç›®ç‰¹å¾çš„å¹¿æ³›çŸ¥è¯†ï¼Œè€Œç‰¹å®šä¸»é¢˜å¼ºè°ƒè¯¦ç»†å±æ€§ï¼Œæ›´å…¨é¢åœ°ç†è§£é¡¹ç›®çš„è¯­ä¹‰ç‰¹å¾å’Œç”¨æˆ·åå¥½ã€‚</li>
<li>è¯¥æ–¹æ³•é€šè¿‡è¿­ä»£æå–å’Œæ›´æ–°ä¸€èˆ¬ä¸»é¢˜ï¼Œä»¥åŠä½¿ç”¨ä¸Šä¸‹æ–‡ä¿¡æ¯æå–ç‰¹å®šä¸»é¢˜æ¥å®æ–½ã€‚</li>
<li>ä¸ºè§£å†³ç‰¹å®šä¸»é¢˜æå–è¿‡ç¨‹ä¸­äº§ç”Ÿçš„åŒä¹‰è¯ä¸»é¢˜ï¼Œé‡‡ç”¨ç²¾ç‚¼ç®—æ³•è¿›è¡Œå¤„ç†ï¼Œæœ‰æ•ˆæé«˜æ•ˆæœã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.20163">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-1b723ac05d4b50c38f54a17bd557a0de.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f47c8a5c99d28e5dbb7aec441292b409.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e8301ef801c31cae1f611ab565192a5e.jpg" align="middle">
</details>


<h1 id="-19"><a href="#-19" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-02-13/LLM/">https://kedreamix.github.io/Talk2Paper/Paper/2025-02-13/LLM/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/LLM/">
                                    <span class="chip bg-color">LLM</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-02-13/Agent/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-04fc13813f6549fb5d8d50b7fb684543.jpg" class="responsive-img" alt="Agent">
                        
                        <span class="card-title">Agent</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Agent æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-02-13  Learning in Markets with Heterogeneous Agents Dynamics and Survival of   Bayesian vs. No-Regret Learners
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-02-13
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Agent/" class="post-category">
                                    Agent
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Agent/">
                        <span class="chip bg-color">Agent</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-02-12/TTS/">
                    <div class="card-image">
                        
                        <img src="D:\MyBlog\AutoFX\arxiv\2025-02-12\./crop_TTS/2502.05236v1/page_0_0.jpg" class="responsive-img" alt="TTS">
                        
                        <span class="card-title">TTS</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            TTS æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-02-12  Can 1B LLM Surpass 405B LLM? Rethinking Compute-Optimal Test-Time   Scaling
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-02-12
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/TTS/" class="post-category">
                                    TTS
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/TTS/">
                        <span class="chip bg-color">TTS</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">33446.1k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
