<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Diffusion Models">
    <meta name="description" content="Diffusion Models æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-02-13  SwiftSketch A Diffusion Model for Image-to-Vector Sketch Generation">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Diffusion Models | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-e3f3b3154fabd339535264a798f82277.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Diffusion Models</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Diffusion-Models/">
                                <span class="chip bg-color">Diffusion Models</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Diffusion-Models/" class="post-category">
                                Diffusion Models
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-02-13
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-03-07
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    19.8k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    80 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-02-13-æ›´æ–°"><a href="#2025-02-13-æ›´æ–°" class="headerlink" title="2025-02-13 æ›´æ–°"></a>2025-02-13 æ›´æ–°</h1><h2 id="SwiftSketch-A-Diffusion-Model-for-Image-to-Vector-Sketch-Generation"><a href="#SwiftSketch-A-Diffusion-Model-for-Image-to-Vector-Sketch-Generation" class="headerlink" title="SwiftSketch: A Diffusion Model for Image-to-Vector Sketch Generation"></a>SwiftSketch: A Diffusion Model for Image-to-Vector Sketch Generation</h2><p><strong>Authors:Ellie Arar, Yarden Frenkel, Daniel Cohen-Or, Ariel Shamir, Yael Vinker</strong></p>
<p>Recent advancements in large vision-language models have enabled highly expressive and diverse vector sketch generation. However, state-of-the-art methods rely on a time-consuming optimization process involving repeated feedback from a pretrained model to determine stroke placement. Consequently, despite producing impressive sketches, these methods are limited in practical applications. In this work, we introduce SwiftSketch, a diffusion model for image-conditioned vector sketch generation that can produce high-quality sketches in less than a second. SwiftSketch operates by progressively denoising stroke control points sampled from a Gaussian distribution. Its transformer-decoder architecture is designed to effectively handle the discrete nature of vector representation and capture the inherent global dependencies between strokes. To train SwiftSketch, we construct a synthetic dataset of image-sketch pairs, addressing the limitations of existing sketch datasets, which are often created by non-artists and lack professional quality. For generating these synthetic sketches, we introduce ControlSketch, a method that enhances SDS-based techniques by incorporating precise spatial control through a depth-aware ControlNet. We demonstrate that SwiftSketch generalizes across diverse concepts, efficiently producing sketches that combine high fidelity with a natural and visually appealing style. </p>
<blockquote>
<p>è¿‘æœŸçš„å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹çš„è¿›å±•ä½¿å¾—å…·æœ‰è¡¨ç°åŠ›å’Œå¤šæ ·æ€§çš„çŸ¢é‡ç´ æç”Ÿæˆæˆä¸ºå¯èƒ½ã€‚ç„¶è€Œï¼Œæœ€å…ˆè¿›çš„è¿™äº›æ–¹æ³•ä¾èµ–äºè€—æ—¶ä¼˜åŒ–è¿‡ç¨‹ï¼Œæ¶‰åŠé¢„è®­ç»ƒæ¨¡å‹çš„é‡å¤åé¦ˆæ¥ç¡®å®šç¬”è§¦ä½ç½®ã€‚å› æ­¤ï¼Œå°½ç®¡äº§ç”Ÿäº†ä»¤äººå°è±¡æ·±åˆ»çš„ç´ æï¼Œè¿™äº›æ–¹æ³•åœ¨å®é™…åº”ç”¨ä¸­çš„å±€é™æ€§ä»ç„¶æ˜æ˜¾ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†SwiftSketchï¼Œè¿™æ˜¯ä¸€ç§åŸºäºå›¾åƒæ¡ä»¶çš„çŸ¢é‡ç´ æç”Ÿæˆçš„æ‰©æ•£æ¨¡å‹ï¼Œå¯ä»¥åœ¨ä¸åˆ°ä¸€ç§’é’Ÿçš„æ—¶é—´å†…äº§ç”Ÿé«˜è´¨é‡çš„ç´ æã€‚SwiftSketché€šè¿‡é€æ­¥å»é™¤ä»é«˜æ–¯åˆ†å¸ƒä¸­é‡‡æ ·çš„ç¬”ç”»æ§åˆ¶ç‚¹çš„å™ªå£°æ¥å·¥ä½œã€‚å…¶åŸºäºtransformerçš„è§£ç å™¨æ¶æ„æ—¨åœ¨æœ‰æ•ˆå¤„ç†çŸ¢é‡è¡¨ç¤ºçš„ç¦»æ•£æ€§è´¨ï¼Œå¹¶æ•æ‰ç¬”ç”»ä¹‹é—´çš„å†…åœ¨å…¨å±€ä¾èµ–æ€§ã€‚ä¸ºäº†è®­ç»ƒSwiftSketchï¼Œæˆ‘ä»¬æ„å»ºäº†ä¸€ä¸ªå›¾åƒ-ç´ æå¯¹çš„åˆæˆæ•°æ®é›†ï¼Œè§£å†³äº†ç°æœ‰ç´ ææ•°æ®é›†çš„å±€é™æ€§ï¼Œè¿™äº›æ•°æ®é›†é€šå¸¸æ˜¯ç”±éè‰ºæœ¯å®¶åˆ›å»ºçš„ï¼Œç¼ºä¹ä¸“ä¸šè´¨é‡ã€‚ä¸ºäº†ç”Ÿæˆè¿™äº›åˆæˆç´ æï¼Œæˆ‘ä»¬å¼•å…¥äº†ControlSketchæ–¹æ³•ï¼Œå®ƒé€šè¿‡æ·±åº¦æ„ŸçŸ¥çš„ControlNetèå…¥ç²¾ç¡®çš„ç©ºé—´æ§åˆ¶ï¼Œå¢å¼ºäº†åŸºäºSDSçš„æŠ€æœ¯ã€‚æˆ‘ä»¬è¯æ˜äº†SwiftSketchèƒ½å¤Ÿæ¨å¹¿å„ç§æ¦‚å¿µï¼Œæœ‰æ•ˆç”Ÿæˆå…·æœ‰é«˜ä¿çœŸåº¦ã€è‡ªç„¶å’Œè§†è§‰å¸å¼•åŠ›çš„é£æ ¼çš„ç´ æã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.08642v1">PDF</a> <a target="_blank" rel="noopener" href="https://swiftsketch.github.io/">https://swiftsketch.github.io/</a></p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹çš„æœ€æ–°è¿›å±•å·²å®ç°äº†è¡¨è¾¾ä¸°å¯Œã€å¤šæ ·åŒ–çš„å‘é‡è‰å›¾ç”Ÿæˆã€‚ç„¶è€Œï¼Œç°æœ‰æ–¹æ³•ä¾èµ–äºè€—æ—¶çš„ä¼˜åŒ–è¿‡ç¨‹ï¼Œæ¶‰åŠé¢„è®­ç»ƒæ¨¡å‹çš„åå¤åé¦ˆä»¥ç¡®å®šç¬”è§¦ä½ç½®ã€‚æœ¬ç ”ç©¶å¼•å…¥SwiftSketchï¼Œä¸€ç§ç”¨äºå›¾åƒæ¡ä»¶å‘é‡è‰å›¾ç”Ÿæˆçš„æ‰©æ•£æ¨¡å‹ï¼Œå¯åœ¨ä¸åˆ°ä¸€ç§’é’Ÿå†…ç”Ÿæˆé«˜è´¨é‡è‰å›¾ã€‚SwiftSketché€šè¿‡é€æ­¥å»å™ªä»é«˜æ–¯åˆ†å¸ƒä¸­é‡‡æ ·çš„æ§åˆ¶ç‚¹æ¥å·¥ä½œã€‚å…¶è®¾è®¡çš„transformer-decoderæ¶æ„èƒ½æœ‰æ•ˆå¤„ç†å‘é‡è¡¨ç¤ºçš„ç¦»æ•£æ€§å’Œæ•æ‰ç¬”è§¦é—´çš„å…¨å±€ä¾èµ–å…³ç³»ã€‚ä¸ºäº†è®­ç»ƒSwiftSketchï¼Œæˆ‘ä»¬æ„å»ºäº†ä¸€ä¸ªå›¾åƒè‰å›¾å¯¹çš„åˆæˆæ•°æ®é›†ï¼Œè§£å†³äº†ç°æœ‰è‰å›¾æ•°æ®é›†çš„é—®é¢˜ï¼Œè¿™äº›æ•°æ®é›†å¾€å¾€ç”±éè‰ºæœ¯å®¶åˆ›å»ºï¼Œç¼ºä¹ä¸“ä¸šè´¨é‡ã€‚æˆ‘ä»¬è¿˜ä»‹ç»äº†ControlSketchæ–¹æ³•ï¼Œå®ƒé€šè¿‡æ·±åº¦æ„ŸçŸ¥ControlNetæŠ€æœ¯æé«˜äº†SDSæ–¹æ³•çš„ç²¾åº¦ç©ºé—´æ§åˆ¶ã€‚æˆ‘ä»¬è¯æ˜äº†SwiftSketchåœ¨ä¸åŒæ¦‚å¿µä¸Šçš„æ³›åŒ–èƒ½åŠ›ï¼Œèƒ½å¤Ÿé«˜æ•ˆç”Ÿæˆå…¼å…·é«˜ä¿çœŸåº¦å’Œè‡ªç„¶ã€ç¾è§‚é£æ ¼çš„è‰å›¾ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹çš„æœ€æ–°è¿›å±•ä½¿å¾—è¡¨è¾¾ä¸°å¯Œã€å¤šæ ·åŒ–çš„å‘é‡è‰å›¾ç”Ÿæˆæˆä¸ºå¯èƒ½ã€‚</li>
<li>å½“å‰æ–¹æ³•ä¾èµ–äºè€—æ—¶çš„ä¼˜åŒ–è¿‡ç¨‹ï¼Œæ¶‰åŠé¢„è®­ç»ƒæ¨¡å‹çš„åå¤åé¦ˆã€‚</li>
<li>SwiftSketchæ˜¯ä¸€ç§ç”¨äºå›¾åƒæ¡ä»¶å‘é‡è‰å›¾ç”Ÿæˆçš„æ‰©æ•£æ¨¡å‹ï¼Œç”Ÿæˆé€Ÿåº¦å¿«ï¼Œå¯åœ¨ä¸åˆ°ä¸€ç§’é’Ÿå†…ç”Ÿæˆé«˜è´¨é‡è‰å›¾ã€‚</li>
<li>SwiftSketché€šè¿‡é€æ­¥å»å™ªæ§åˆ¶ç‚¹æ¥å·¥ä½œï¼Œå…¶æ¶æ„èƒ½æœ‰æ•ˆå¤„ç†å‘é‡è¡¨ç¤ºçš„ç¦»æ•£æ€§å’Œæ•æ‰ç¬”è§¦é—´çš„ä¾èµ–å…³ç³»ã€‚</li>
<li>ä¸ºäº†è®­ç»ƒSwiftSketchï¼Œæ„å»ºäº†ä¸€ä¸ªå›¾åƒè‰å›¾å¯¹çš„åˆæˆæ•°æ®é›†ï¼Œè§£å†³ç°æœ‰æ•°æ®é›†çš„é—®é¢˜ã€‚</li>
<li>å¼•å…¥äº†ControlSketchæ–¹æ³•ï¼Œæé«˜äº†SDSæ–¹æ³•çš„ç²¾åº¦ç©ºé—´æ§åˆ¶ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.08642">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-fbb69ebaf3b87b7d381e36e04a3ea243.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-2e3bf9cf3c95d87e959f1edc7ea08ac8.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-c0fd1e87a1bac53515913e8770a13b4a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-8fe39a585586b8244c2c3b7f6eec423b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d16645ec182745cdbd55fd9f09c5815c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-366f1562a7b9075cd61c71e4c7a5b022.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-bb80255de3de036a61034627aaed8c0f.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Chasing-Charge-Carriers-Diffusion-Dynamics-in-Mixed-n-Quasi-Two-Dimensional-Colloidal-MAPbBr3-Perovskites"><a href="#Chasing-Charge-Carriers-Diffusion-Dynamics-in-Mixed-n-Quasi-Two-Dimensional-Colloidal-MAPbBr3-Perovskites" class="headerlink" title="Chasing Charge Carriers: Diffusion Dynamics in Mixed-n   Quasi-Two-Dimensional Colloidal MAPbBr3 Perovskites"></a>Chasing Charge Carriers: Diffusion Dynamics in Mixed-n   Quasi-Two-Dimensional Colloidal MAPbBr3 Perovskites</h2><p><strong>Authors:Ronja Maria Piehler, Eugen Klein, Francisco M. Gomez-Campos, Oliver KÃ¼hn, Rostyslav Lesyuk, Christian Klinke</strong></p>
<p>In optoelectronic applications, metal halide perovskites (MHPs) are compelling materials because of their highly tuneable and intensely competitive optical properties. Colloidal synthesis enables the controlled formation of various morphologies of MHP nanocrystals, all with different carrier properties and, hence, different optical and carrier transport behaviours. We characterized three different methylammonium lead tribromide perovskite (MAPbBr3) morphologies: nanoplatelets (NPLs), nanosheets (NSs), and nanostripes (NSTs) synthesized by hot-injection synthesis protocols with slightly different parameters. A fluorescence imaging microscope (FLIM) for time- and space-resolved measurements of the carrier migration was employed to quantify the charge carriersâ€™ migration process upon photoexcitation. The results are rationalized in the two-dimensional diffusion model framework, considering funnelling and trapping processes in mixed-n colloidal MHPs. Subdiffusion mode was found to prevail in the nanocrystals, whereby the highest carrier diffusivity was found for bulk-like NSTs, followed by layered NSs and a film of NPLs. These findings provide a better understanding of optoelectronic processes in perovskites relevant to photovoltaic and light-emitting devices. </p>
<blockquote>
<p>åœ¨å…‰ç”µå­åº”ç”¨ä¸­ï¼Œé‡‘å±å¤åŒ–ç‰©é’™é’›çŸ¿ï¼ˆMHPsï¼‰æ˜¯éå¸¸å¸å¼•äººçš„ææ–™ï¼Œå› ä¸ºå®ƒä»¬å…·æœ‰é«˜åº¦å¯è°ƒä¸”ç«äº‰åŠ›å¼ºçš„å…‰å­¦ç‰¹æ€§ã€‚èƒ¶ä½“åˆæˆä½¿MHPçº³ç±³æ™¶ä½“çš„å„ç§å½¢æ€å¯æ§å½¢æˆæˆä¸ºå¯èƒ½ï¼Œæ‰€æœ‰è¿™äº›å½¢æ€éƒ½å…·æœ‰ä¸åŒçš„è½½æµå­ç‰¹æ€§ï¼Œä»è€Œè¡¨ç°å‡ºä¸åŒçš„å…‰å­¦å’Œè½½æµå­ä¼ è¾“è¡Œä¸ºã€‚æˆ‘ä»¬è¡¨å¾äº†é€šè¿‡çƒ­æ³¨å…¥åˆæˆåè®®ï¼ˆç•¥æœ‰ä¸åŒå‚æ•°ï¼‰åˆæˆçš„ä¸‰ç§ä¸åŒç”²åŸºé“µé“…æº´åŒ–ç‰©é’™é’›çŸ¿ï¼ˆMAPbBr3ï¼‰å½¢æ€ï¼šçº³ç±³è¡€å°æ¿ï¼ˆNPLsï¼‰ã€çº³ç±³ç‰‡ï¼ˆNSsï¼‰å’Œçº³ç±³æ¡çº¹ï¼ˆNSTsï¼‰ã€‚é‡‡ç”¨è§å…‰æˆåƒæ˜¾å¾®é•œï¼ˆFLIMï¼‰å¯¹è½½æµå­è¿ç§»è¿›è¡Œæ—¶-ç©ºè§£ææµ‹é‡ï¼Œä»¥é‡åŒ–å…‰æ¿€å‘ä¸‹è½½æµå­çš„è¿ç§»è¿‡ç¨‹ã€‚ç»“æœä»¥äºŒç»´æ‰©æ•£æ¨¡å‹æ¡†æ¶è¿›è¡Œåˆç†åŒ–åˆ†æï¼Œè€ƒè™‘äº†æ··åˆnèƒ¶ä½“MHPsä¸­çš„å¼•æµå’Œä¿˜è·è¿‡ç¨‹ã€‚åœ¨çº³ç±³æ™¶ä½“ä¸­å‘ç°äºšæ‰©æ•£æ¨¡å¼å ä¸»å¯¼åœ°ä½ï¼Œå…¶ä¸­å—çŠ¶çš„NSTså…·æœ‰æœ€é«˜çš„è½½æµå­æ‰©æ•£ç‡ï¼Œå…¶æ¬¡æ˜¯å±‚çŠ¶çš„NSså’ŒNPLsè–„è†œã€‚è¿™äº›å‘ç°æœ‰åŠ©äºæ›´å¥½åœ°äº†è§£ä¸å…‰ä¼å’Œå‘å…‰å™¨ä»¶ç›¸å…³çš„é’™é’›çŸ¿ä¸­çš„å…‰ç”µå­è¿‡ç¨‹ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.08601v1">PDF</a> 14 pages, 6 figures</p>
<p><strong>Summary</strong></p>
<p>é‡‘å±å¤åŒ–ç‰©é’™é’›çŸ¿ï¼ˆMHPsï¼‰åœ¨å…‰ç”µå­åº”ç”¨ä¸­å…·æœ‰å¼•äººæ³¨ç›®çš„å…‰å­¦ç‰¹æ€§ï¼Œå¯é€šè¿‡èƒ¶ä½“åˆæˆæ§åˆ¶å½¢æˆå„ç§å½¢æ€ã€‚æœ¬æ–‡ç ”ç©¶äº†ä¸‰ç§ä¸åŒå½¢æ€çš„ç”²åŸºé“µé“…æº´åŒ–ç‰©é’™é’›çŸ¿ï¼ˆMAPbBr3ï¼‰ï¼šçº³ç±³ç‰‡å±‚ï¼ˆNPLsï¼‰ã€çº³ç±³ç‰‡ï¼ˆNSsï¼‰å’Œçº³ç±³æ¡çº¹ï¼ˆNSTsï¼‰ã€‚é‡‡ç”¨è§å…‰æˆåƒæ˜¾å¾®é•œï¼ˆFLIMï¼‰ç ”ç©¶è½½æµå­çš„è¿ç§»è¿‡ç¨‹ï¼Œå¹¶åœ¨äºŒç»´æ‰©æ•£æ¨¡å‹æ¡†æ¶ä¸‹è§£é‡Šç»“æœã€‚ç ”ç©¶å‘ç°çº³ç±³æ™¶ä½“ä¸­æ™®éå­˜åœ¨å­æ‰©æ•£æ¨¡å¼ï¼Œå…¶ä¸­å—çŠ¶NSTsçš„è½½æµå­æ‰©æ•£æ€§æœ€é«˜ï¼Œå…¶æ¬¡æ˜¯åˆ†å±‚NSså’ŒNPLsè–„è†œã€‚è¿™äº›å‘ç°æœ‰åŠ©äºæ›´å¥½åœ°ç†è§£é’™é’›çŸ¿ä¸­ä¸å…‰ä¼å’Œå‘å…‰å™¨ä»¶ç›¸å…³çš„å…‰ç”µå­è¿‡ç¨‹ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>é‡‘å±å¤åŒ–ç‰©é’™é’›çŸ¿ï¼ˆMHPsï¼‰åœ¨å…‰ç”µå­åº”ç”¨ä¸­å…·æœ‰é«˜åº¦å¯è°ƒä¸”ç«äº‰æ€§çš„å…‰å­¦ç‰¹æ€§ã€‚</li>
<li>é€šè¿‡èƒ¶ä½“åˆæˆå¯ä»¥å½¢æˆä¸åŒå½¢æ€çš„é‡‘å±å¤åŒ–ç‰©é’™é’›çŸ¿çº³ç±³æ™¶ä½“ã€‚</li>
<li>çº³ç±³ç‰‡å±‚ï¼ˆNPLsï¼‰ã€çº³ç±³ç‰‡ï¼ˆNSsï¼‰å’Œçº³ç±³æ¡çº¹ï¼ˆNSTsï¼‰æ˜¯ä¸‰ç§ä¸åŒçš„å½¢æ€ã€‚</li>
<li>é‡‡ç”¨è§å…‰æˆåƒæ˜¾å¾®é•œï¼ˆFLIMï¼‰ç ”ç©¶è½½æµå­çš„è¿ç§»è¿‡ç¨‹ã€‚</li>
<li>ç ”ç©¶ç»“æœåœ¨äºŒç»´æ‰©æ•£æ¨¡å‹æ¡†æ¶ä¸‹è§£é‡Šï¼Œè€ƒè™‘äº†æ··åˆnå‹èƒ¶ä½“MHPsä¸­çš„æ¼æ–—å’Œé™·é˜±è¿‡ç¨‹ã€‚</li>
<li>åœ¨çº³ç±³æ™¶ä½“ä¸­å‘ç°å­æ‰©æ•£æ¨¡å¼å æ®ä¸»å¯¼åœ°ä½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.08601">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-126ef6d940fbd3728b515fbf7adf8441.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d39bb596a3e717b0352b3e16d73ba095.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-9e320adeb25500ffc0736b3054dc3fd5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4389b40d22d9adcd3f2af45bb8f98e76.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Light-A-Video-Training-free-Video-Relighting-via-Progressive-Light-Fusion"><a href="#Light-A-Video-Training-free-Video-Relighting-via-Progressive-Light-Fusion" class="headerlink" title="Light-A-Video: Training-free Video Relighting via Progressive Light   Fusion"></a>Light-A-Video: Training-free Video Relighting via Progressive Light   Fusion</h2><p><strong>Authors:Yujie Zhou, Jiazi Bu, Pengyang Ling, Pan Zhang, Tong Wu, Qidong Huang, Jinsong Li, Xiaoyi Dong, Yuhang Zang, Yuhang Cao, Anyi Rao, Jiaqi Wang, Li Niu</strong></p>
<p>Recent advancements in image relighting models, driven by large-scale datasets and pre-trained diffusion models, have enabled the imposition of consistent lighting. However, video relighting still lags, primarily due to the excessive training costs and the scarcity of diverse, high-quality video relighting datasets. A simple application of image relighting models on a frame-by-frame basis leads to several issues: lighting source inconsistency and relighted appearance inconsistency, resulting in flickers in the generated videos. In this work, we propose Light-A-Video, a training-free approach to achieve temporally smooth video relighting. Adapted from image relighting models, Light-A-Video introduces two key techniques to enhance lighting consistency. First, we design a Consistent Light Attention (CLA) module, which enhances cross-frame interactions within the self-attention layers to stabilize the generation of the background lighting source. Second, leveraging the physical principle of light transport independence, we apply linear blending between the source videoâ€™s appearance and the relighted appearance, using a Progressive Light Fusion (PLF) strategy to ensure smooth temporal transitions in illumination. Experiments show that Light-A-Video improves the temporal consistency of relighted video while maintaining the image quality, ensuring coherent lighting transitions across frames. Project page: <a target="_blank" rel="noopener" href="https://bujiazi.github.io/light-a-video.github.io/">https://bujiazi.github.io/light-a-video.github.io/</a>. </p>
<blockquote>
<p>è¿‘æœŸï¼Œç”±äºå¤§è§„æ¨¡æ•°æ®é›†å’Œé¢„è®­ç»ƒæ‰©æ•£æ¨¡å‹çš„æ¨åŠ¨ï¼Œå›¾åƒç…§æ˜æ¨¡å‹å–å¾—äº†è¿›å±•ï¼Œå·²å®ç°ä¸€è‡´ç…§æ˜çš„åº”ç”¨ã€‚ç„¶è€Œï¼Œè§†é¢‘é‡ç…§æ˜ä»ç„¶æ»åï¼Œä¸»è¦æ˜¯ç”±äºè®­ç»ƒæˆæœ¬è¿‡é«˜å’Œå¤šæ ·åŒ–ã€é«˜è´¨é‡è§†é¢‘é‡ç…§æ˜æ•°æ®é›†çš„ç¨€ç¼ºã€‚ç®€å•åœ°å°†å›¾åƒé‡ç…§æ˜æ¨¡å‹é€å¸§åº”ç”¨ä¼šå¯¼è‡´ä¸€äº›é—®é¢˜ï¼šå…‰æºä¸ä¸€è‡´å’Œé‡æ–°ç…§æ˜åçš„å¤–è§‚ä¸ä¸€è‡´ï¼Œå¯¼è‡´ç”Ÿæˆçš„è§†é¢‘å‡ºç°é—ªçƒã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†Light-A-Videoï¼Œè¿™æ˜¯ä¸€ç§æ— éœ€è®­ç»ƒå³å¯å®ç°æ—¶é—´å¹³æ»‘è§†é¢‘é‡ç…§æ˜çš„æ–¹æ³•ã€‚Light-A-Videoä»å›¾åƒé‡ç…§æ˜æ¨¡å‹ä¸­æ±²å–çµæ„Ÿï¼Œå¼•å…¥äº†ä¸¤ç§å…³é”®æŠ€æœ¯æ¥æé«˜ç…§æ˜ä¸€è‡´æ€§ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸€ä¸ªä¸€è‡´å…‰ç…§æ³¨æ„åŠ›ï¼ˆCLAï¼‰æ¨¡å—ï¼Œè¯¥æ¨¡å—å¢å¼ºäº†è‡ªæ³¨æ„åŠ›å±‚å†…çš„è·¨å¸§äº¤äº’ï¼Œä»¥ç¨³å®šèƒŒæ™¯å…‰æºçš„ç”Ÿæˆã€‚å…¶æ¬¡ï¼Œåˆ©ç”¨å…‰çº¿ä¼ è¾“ç‹¬ç«‹æ€§çš„ç‰©ç†åŸç†ï¼Œæˆ‘ä»¬é‡‡ç”¨çº¿æ€§æ··åˆæ–¹æ³•ï¼Œå°†æºè§†é¢‘çš„å¤–è§‚å’Œé‡æ–°ç…§æ˜åçš„å¤–è§‚ç»“åˆèµ·æ¥ï¼Œå¹¶ä½¿ç”¨æ¸è¿›å…‰èåˆï¼ˆPLFï¼‰ç­–ç•¥ï¼Œä»¥ç¡®ä¿ç…§æ˜çš„æ—¶é—´è¿‡æ¸¡å¹³æ»‘ã€‚å®éªŒè¡¨æ˜ï¼ŒLight-A-Videoåœ¨æé«˜é‡ç…§æ˜è§†é¢‘çš„æ—¶é—´ä¸€è‡´æ€§çš„åŒæ—¶ï¼Œä¿æŒäº†å›¾åƒè´¨é‡ï¼Œç¡®ä¿äº†è·¨å¸§çš„å…‰ç…§è¿‡æ¸¡è¿è´¯ã€‚é¡¹ç›®é¡µé¢ï¼š<a target="_blank" rel="noopener" href="https://bujiazi.github.io/light-a-video.github.io/%E3%80%82">https://bujiazi.github.io/light-a-video.github.io/ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.08590v1">PDF</a> Project Page: <a target="_blank" rel="noopener" href="https://bujiazi.github.io/light-a-video.github.io/">https://bujiazi.github.io/light-a-video.github.io/</a></p>
<p><strong>Summary</strong></p>
<p>è§†é¢‘é‡å…‰ç…§æŠ€æœ¯åœ¨è¿‘å¹´æ¥å› ä¸ºå¤§è§„æ¨¡æ•°æ®é›†å’Œé¢„è®­ç»ƒæ‰©æ•£æ¨¡å‹çš„æ¨åŠ¨è€Œæœ‰æ‰€å‘å±•ï¼Œä½†ä»é¢ä¸´è®­ç»ƒæˆæœ¬é«˜æ˜‚åŠé«˜è´¨é‡è§†é¢‘é‡å…‰ç…§æ•°æ®é›†ç¨€ç¼ºçš„é—®é¢˜ã€‚ç›´æ¥åº”ç”¨å›¾åƒé‡å…‰ç…§æ¨¡å‹äºå¸§ä¼šå¯¼è‡´å…‰æºä¸ä¸€è‡´å’Œé‡å…‰ç…§å¤–è§‚ä¸ä¸€è‡´ï¼Œé€ æˆç”Ÿæˆè§†é¢‘å‡ºç°é—ªçƒç°è±¡ã€‚æœ¬ç ”ç©¶æå‡ºä¸€ç§åä¸ºLight-A-Videoçš„æ–¹æ³•ï¼Œæ— éœ€è®­ç»ƒå³å¯å®ç°æ—¶é—´å¹³æ»‘çš„è§†é¢‘é‡å…‰ç…§ã€‚è¯¥æ–¹æ³•å¼•å…¥ä¸¤ç§å…³é”®æŠ€æœ¯ä»¥å¢å¼ºç…§æ˜ä¸€è‡´æ€§ï¼šä¸€æ˜¯è®¾è®¡ä¸€è‡´å…‰æ³¨æ„åŠ›æ¨¡å—ï¼ˆCLAï¼‰ï¼Œå¢å¼ºè·¨å¸§äº¤äº’ä»¥ç¨³å®šèƒŒæ™¯å…‰æºçš„ç”Ÿæˆï¼›äºŒæ˜¯åˆ©ç”¨å…‰çº¿ä¼ è¾“çš„ç‹¬ç«‹ç‰©ç†åŸç†ï¼Œé‡‡ç”¨æ¸è¿›å…‰èåˆç­–ç•¥ï¼Œåœ¨æºè§†é¢‘çš„å¤–è§‚å’Œé‡å…‰ç…§å¤–è§‚ä¹‹é—´è¿›è¡Œçº¿æ€§æ··åˆï¼Œç¡®ä¿ç…§æ˜çš„æ—¶é—´è¿‡æ¸¡å¹³æ»‘ã€‚å®éªŒè¡¨æ˜ï¼ŒLight-A-Videoåœ¨æé«˜é‡å…‰ç…§è§†é¢‘çš„æ—¶é—´ä¸€è‡´æ€§çš„åŒæ—¶ï¼Œä¿æŒäº†å›¾åƒè´¨é‡ï¼Œç¡®ä¿äº†è·¨å¸§çš„ç…§æ˜è¿‡æ¸¡è¿è´¯ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å›¾åƒé‡å…‰ç…§æŠ€æœ¯åœ¨è§†é¢‘é¢†åŸŸçš„åº”ç”¨å—åˆ°è®­ç»ƒæˆæœ¬å’Œé«˜è´¨é‡æ•°æ®é›†çš„é™åˆ¶ã€‚</li>
<li>ç›´æ¥å°†å›¾åƒé‡å…‰ç…§æ¨¡å‹åº”ç”¨äºè§†é¢‘å¸§ä¼šå¯¼è‡´å…‰æºå’Œå¤–è§‚çš„ä¸ä¸€è‡´ï¼Œé€ æˆé—ªçƒé—®é¢˜ã€‚</li>
<li>Light-A-Videoæ˜¯ä¸€ç§æ— éœ€è®­ç»ƒçš„è§†é¢‘é‡å…‰ç…§æ–¹æ³•ï¼Œé€šè¿‡å¼•å…¥ä¸€è‡´å…‰æ³¨æ„åŠ›æ¨¡å—ï¼ˆCLAï¼‰å’Œæ¸è¿›å…‰èåˆç­–ç•¥è§£å†³å…‰æºä¸ä¸€è‡´é—®é¢˜ã€‚</li>
<li>CLAæ¨¡å—é€šè¿‡å¢å¼ºè·¨å¸§äº¤äº’æ¥ç¨³å®šèƒŒæ™¯å…‰æºçš„ç”Ÿæˆã€‚</li>
<li>æ¸è¿›å…‰èåˆç­–ç•¥ç¡®ä¿ç…§æ˜çš„æ—¶é—´è¿‡æ¸¡å¹³æ»‘ã€‚</li>
<li>Light-A-Videoæé«˜äº†é‡å…‰ç…§è§†é¢‘çš„æ—¶é—´ä¸€è‡´æ€§ï¼ŒåŒæ—¶ä¿æŒäº†å›¾åƒè´¨é‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.08590">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-402d31f5fc1359647055c9e8750f65c7.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-8516e0c43ef83f33a547881047e61056.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-85abef7b065b84650615652d27390f8f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-dbdd71b4e1a5d926bfd9fd477e712a4e.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Ultrasound-Image-Generation-using-Latent-Diffusion-Models"><a href="#Ultrasound-Image-Generation-using-Latent-Diffusion-Models" class="headerlink" title="Ultrasound Image Generation using Latent Diffusion Models"></a>Ultrasound Image Generation using Latent Diffusion Models</h2><p><strong>Authors:Benoit Freiche, Anthony El-Khoury, Ali Nasiri-Sarvi, Mahdi S. Hosseini, Damien Garcia, Adrian Basarab, Mathieu Boily, Hassan Rivaz</strong></p>
<p>Diffusion models for image generation have been a subject of increasing interest due to their ability to generate diverse, high-quality images. Image generation has immense potential in medical imaging because open-source medical images are difficult to obtain compared to natural images, especially for rare conditions. The generated images can be used later to train classification and segmentation models. In this paper, we propose simulating realistic ultrasound (US) images by successive fine-tuning of large diffusion models on different publicly available databases. To do so, we fine-tuned Stable Diffusion, a state-of-the-art latent diffusion model, on BUSI (Breast US Images) an ultrasound breast image dataset. We successfully generated high-quality US images of the breast using simple prompts that specify the organ and pathology, which appeared realistic to three experienced US scientists and a US radiologist. Additionally, we provided user control by conditioning the model with segmentations through ControlNet. We will release the source code at <a target="_blank" rel="noopener" href="http://code.sonography.ai/">http://code.sonography.ai/</a> to allow fast US image generation to the scientific community. </p>
<blockquote>
<p>å›¾åƒç”Ÿæˆä¸­çš„æ‰©æ•£æ¨¡å‹å› å…¶èƒ½å¤Ÿç”Ÿæˆå¤šæ ·ã€é«˜è´¨é‡å›¾åƒè€Œæ—¥ç›Šå—åˆ°å…³æ³¨ã€‚åŒ»å­¦æˆåƒä¸­çš„å›¾åƒç”Ÿæˆå…·æœ‰å·¨å¤§æ½œåŠ›ï¼Œå› ä¸ºä¸è‡ªç„¶å›¾åƒç›¸æ¯”ï¼Œè·å–å¼€æºåŒ»å­¦å›¾åƒæ›´åŠ å›°éš¾ï¼Œå°¤å…¶æ˜¯å¯¹äºç½•è§ç—…ç—‡ã€‚ç”Ÿæˆçš„å›¾åƒä¹‹åå¯ç”¨äºè®­ç»ƒå’Œåˆ†å‰²æ¨¡å‹ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬é€šè¿‡åœ¨ä¸åŒçš„å…¬å¼€æ•°æ®åº“ä¸Šè¿ç»­å¾®è°ƒå¤§å‹æ‰©æ•£æ¨¡å‹æ¥æ¨¡æ‹Ÿç°å®è¶…å£°ï¼ˆUSï¼‰å›¾åƒã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬å¯¹å‰æ²¿çš„æ½œåœ¨æ‰©æ•£æ¨¡å‹Stable Diffusionè¿›è¡Œäº†å¾®è°ƒï¼Œä½¿ç”¨BUSIï¼ˆä¹³è…ºè¶…å£°å›¾åƒï¼‰æ•°æ®é›†è¿›è¡Œè¶…å£°ä¹³è…ºå›¾åƒæ•°æ®é›†çš„è®­ç»ƒã€‚æˆ‘ä»¬æˆåŠŸä½¿ç”¨ç®€å•çš„æç¤ºç”Ÿæˆäº†é«˜è´¨é‡çš„ä¹³è…ºè¶…å£°å›¾åƒï¼Œè¿™äº›æç¤ºæŒ‡å®šäº†å™¨å®˜å’Œç—…ç†ç‰¹å¾ï¼Œå¯¹ä¸‰ä½ç»éªŒä¸°å¯Œçš„è¶…å£°ç§‘å­¦å®¶å’Œä¸€ä½è¶…å£°åŒ»å¸ˆæ¥è¯´éƒ½å¾ˆé€¼çœŸã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬é€šè¿‡å°†åˆ†å‰²ä¸ControlNetç»“åˆæ¥ä¸ºæ¨¡å‹æä¾›ç”¨æˆ·æ§åˆ¶ã€‚æˆ‘ä»¬å°†åœ¨<a target="_blank" rel="noopener" href="http://code.sonography.ai/%E5%8F%91%E5%B8%83%E6%BA%90%E4%BB%A3%E7%A0%81%EF%BC%8C%E4%BB%A5%E5%85%81%E8%AE%B8%E7%A7%91%E5%AD%A6%E7%95%8C%E5%BF%AB%E9%80%9F%E7%94%9F%E6%88%90%E8%B6%85%E5%A3%B0%E5%9B%BE%E5%83%8F%E3%80%82">http://code.sonography.ai/å‘å¸ƒæºä»£ç ï¼Œä»¥å…è®¸ç§‘å­¦ç•Œå¿«é€Ÿç”Ÿæˆè¶…å£°å›¾åƒã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.08580v1">PDF</a> 6 pages conference paper for SPIE medical imaging</p>
<p><strong>Summary</strong><br>æ‰©æ•£æ¨¡å‹åœ¨å›¾åƒç”Ÿæˆé¢†åŸŸå› å…¶èƒ½ç”Ÿæˆå¤šæ ·ã€é«˜è´¨é‡å›¾åƒè€Œå¤‡å—å…³æ³¨ã€‚åœ¨åŒ»å­¦æˆåƒé¢†åŸŸï¼Œç”±äºå…¬å¼€åŒ»ç–—å›¾åƒè·å–å›°éš¾ï¼Œå°¤å…¶æ˜¯ç½•è§ç—…ç—‡çš„å›¾åƒï¼Œå›¾åƒç”Ÿæˆå…·æœ‰å·¨å¤§æ½œåŠ›ã€‚ç”Ÿæˆçš„å›¾åƒå¯ç”¨äºè®­ç»ƒå’Œåˆ†å‰²æ¨¡å‹ã€‚æœ¬æ–‡æå‡ºé€šè¿‡å¤§å‹æ‰©æ•£æ¨¡å‹çš„è¿ç»­å¾®è°ƒï¼Œåœ¨å…¬å¼€æ•°æ®åº“ä¸Šæ¨¡æ‹Ÿç°å®è¶…å£°æ³¢ï¼ˆUSï¼‰å›¾åƒã€‚æˆ‘ä»¬æˆåŠŸåœ°å¯¹æœ€å…ˆè¿›çš„æ½œåœ¨æ‰©æ•£æ¨¡å‹Stable Diffusionè¿›è¡Œäº†å¾®è°ƒï¼Œåœ¨BUSIï¼ˆä¹³è…ºè¶…å£°å›¾åƒï¼‰æ•°æ®é›†ä¸Šç”Ÿæˆäº†é«˜è´¨é‡çš„ä¹³è…ºè¶…å£°å›¾åƒã€‚ä½¿ç”¨ç®€å•çš„æç¤ºï¼ˆå¦‚å™¨å®˜å’Œç—…ç†ï¼‰ç”Ÿæˆçš„å›¾åƒçœ‹èµ·æ¥éå¸¸é€¼çœŸï¼Œå¾—åˆ°äº†ä¸‰ä½ç»éªŒä¸°å¯Œçš„è¶…å£°ç§‘å­¦å®¶å’Œä¸€ä½è¶…å£°åŒ»å¸ˆçš„è®¤å¯ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬é€šè¿‡ControlNetæ¨¡å‹æ¡ä»¶åŒ–æä¾›äº†ç”¨æˆ·æ§åˆ¶ã€‚æˆ‘ä»¬å°†æŠŠæºä»£ç å‘å¸ƒåœ¨<a target="_blank" rel="noopener" href="http://code.sonography.ai/%E4%B8%8A%EF%BC%8C%E4%BB%A5%E5%8A%A0%E5%BF%AB%E7%A7%91%E5%AD%A6%E7%95%8C%E8%B6%85%E5%A3%B0%E6%B3%A2%E5%9B%BE%E5%83%8F%E7%9A%84%E7%94%9F%E6%88%90%E9%80%9F%E5%BA%A6%E3%80%82">http://code.sonography.ai/ä¸Šï¼Œä»¥åŠ å¿«ç§‘å­¦ç•Œè¶…å£°æ³¢å›¾åƒçš„ç”Ÿæˆé€Ÿåº¦ã€‚</a></p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ‰©æ•£æ¨¡å‹å› å…¶ç”Ÿæˆå¤šæ ·ã€é«˜è´¨é‡å›¾åƒçš„èƒ½åŠ›åœ¨å›¾åƒç”Ÿæˆé¢†åŸŸå—åˆ°å…³æ³¨ã€‚</li>
<li>åœ¨åŒ»å­¦æˆåƒé¢†åŸŸï¼Œç”±äºå…¬å¼€åŒ»ç–—å›¾åƒè·å–å›°éš¾ï¼Œå›¾åƒç”Ÿæˆå°¤å…¶é‡è¦ã€‚</li>
<li>æœ¬æ–‡æå‡ºé€šè¿‡è¿ç»­å¾®è°ƒå¤§å‹æ‰©æ•£æ¨¡å‹æ¥æ¨¡æ‹Ÿç°å®è¶…å£°æ³¢ï¼ˆUSï¼‰å›¾åƒã€‚</li>
<li>æˆåŠŸä½¿ç”¨Stable Diffusionæ¨¡å‹åœ¨BUSIæ•°æ®é›†ä¸Šç”Ÿæˆé«˜è´¨é‡çš„ä¹³è…ºè¶…å£°å›¾åƒã€‚</li>
<li>ç”Ÿæˆçš„å›¾åƒä½¿ç”¨ç®€å•çš„æç¤ºï¼ˆå¦‚å™¨å®˜å’Œç—…ç†ï¼‰ï¼Œçœ‹èµ·æ¥éå¸¸é€¼çœŸï¼Œå¾—åˆ°äº†ä¸“ä¸šäººå£«çš„è®¤å¯ã€‚</li>
<li>é€šè¿‡ControlNetæ¨¡å‹æ¡ä»¶åŒ–æä¾›äº†ç”¨æˆ·æ§åˆ¶ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.08580">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-3b0bc59a15b3a1e7436f31c2dbccb398.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-4abce4bfa95bd288485184f519066a13.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a4753911170ebd2a5eb70c29d43b0e53.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-775ff4a4932e08c1a181e0737e5d4366.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8e287699bc107b2ef3558225744c2b8f.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="BCDDM-Branch-Corrected-Denoising-Diffusion-Model-for-Black-Hole-Image-Generation"><a href="#BCDDM-Branch-Corrected-Denoising-Diffusion-Model-for-Black-Hole-Image-Generation" class="headerlink" title="BCDDM: Branch-Corrected Denoising Diffusion Model for Black Hole Image   Generation"></a>BCDDM: Branch-Corrected Denoising Diffusion Model for Black Hole Image   Generation</h2><p><strong>Authors:Ao liu, Zelin Zhang, Songbai Chen, Cuihong Wen</strong></p>
<p>The properties of black holes and accretion flows can be inferred by fitting Event Horizon Telescope (EHT) data to simulated images generated through general relativistic ray tracing (GRRT). However, due to the computationally intensive nature of GRRT, the efficiency of generating specific radiation flux images needs to be improved. This paper introduces the Branch Correction Denoising Diffusion Model (BCDDM), which uses a branch correction mechanism and a weighted mixed loss function to improve the accuracy of generated black hole images based on seven physical parameters of the radiatively inefficient accretion flow (RIAF) model. Our experiments show a strong correlation between the generated images and their physical parameters. By enhancing the GRRT dataset with BCDDM-generated images and using ResNet50 for parameter regression, we achieve significant improvements in parameter prediction performance. This approach reduces computational costs and provides a faster, more efficient method for dataset expansion, parameter estimation, and model fitting. </p>
<blockquote>
<p>é€šè¿‡å°†é€šè¿‡å¹¿ä¹‰ç›¸å¯¹è®ºå…‰çº¿è¿½è¸ªï¼ˆGRRTï¼‰ç”Ÿæˆçš„æ¨¡æ‹Ÿå›¾åƒä¸äº‹ä»¶è§†ç•Œæœ›è¿œé•œï¼ˆEHTï¼‰æ•°æ®è¿›è¡Œæ‹Ÿåˆï¼Œå¯ä»¥æ¨æ–­å‡ºé»‘æ´å’Œå¸ç§¯æµçš„ç‰¹æ€§ã€‚ç„¶è€Œï¼Œç”±äºGRRTçš„è®¡ç®—é‡å¤§ï¼Œç”Ÿæˆç‰¹å®šè¾å°„æµé‡å›¾åƒçš„æ•ˆç‡éœ€è¦æé«˜ã€‚æœ¬æ–‡ä»‹ç»äº†åˆ†æ”¯æ ¡æ­£å»å™ªæ‰©æ•£æ¨¡å‹ï¼ˆBCDDMï¼‰ï¼Œå®ƒåˆ©ç”¨åˆ†æ”¯æ ¡æ­£æœºåˆ¶å’ŒåŠ æƒæ··åˆæŸå¤±å‡½æ•°ï¼ŒåŸºäºè¾å°„æ— æ•ˆç‡å¸ç§¯æµï¼ˆRIAFï¼‰æ¨¡å‹çš„ä¸ƒä¸ªç‰©ç†å‚æ•°ï¼Œæé«˜äº†ç”Ÿæˆçš„é»‘æ´å›¾åƒçš„å‡†ç¡®æ€§ã€‚æˆ‘ä»¬çš„å®éªŒè¡¨æ˜ï¼Œç”Ÿæˆå›¾åƒä¸å…¶ç‰©ç†å‚æ•°ä¹‹é—´å­˜åœ¨å¼ºçƒˆçš„ç›¸å…³æ€§ã€‚é€šè¿‡åˆ©ç”¨BCDDMç”Ÿæˆçš„å›¾åƒå¢å¼ºGRRTæ•°æ®é›†ï¼Œå¹¶ä½¿ç”¨ResNet50è¿›è¡Œå‚æ•°å›å½’ï¼Œæˆ‘ä»¬åœ¨å‚æ•°é¢„æµ‹æ€§èƒ½ä¸Šå–å¾—äº†æ˜¾è‘—çš„æå‡ã€‚è¿™ç§æ–¹æ³•é™ä½äº†è®¡ç®—æˆæœ¬ï¼Œæä¾›äº†ä¸€ç§æ›´å¿«ã€æ›´é«˜æ•ˆçš„æ•°æ®é›†æ‰©å±•ã€å‚æ•°ä¼°è®¡å’Œæ¨¡å‹æ‹Ÿåˆæ–¹æ³•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.08528v1">PDF</a> </p>
<p><strong>Summary</strong>ï¼š<br>é€šè¿‡äº‹ä»¶è§†ç•Œæœ›è¿œé•œï¼ˆEHTï¼‰æ•°æ®æ‹Ÿåˆæ¨¡æ‹Ÿå›¾åƒï¼Œå¯æ¨æ–­é»‘æ´å’Œå¸ç§¯æµç‰¹æ€§ã€‚ä¸ºæé«˜ç”Ÿæˆç‰¹å®šè¾å°„æµé‡å›¾åƒçš„æ•ˆç‡ï¼Œå¼•å…¥åˆ†æ”¯æ ¡æ­£é™å™ªæ‰©æ•£æ¨¡å‹ï¼ˆBCDDMï¼‰ï¼Œæé«˜åŸºäºä¸ƒä¸ªç‰©ç†å‚æ•°çš„è¾å°„æ— æ•ˆå¸ç§¯æµæ¨¡å‹ï¼ˆRIAFï¼‰ç”Ÿæˆé»‘æ´å›¾åƒçš„å‡†ç¡®æ€§ã€‚å®éªŒè¡¨æ˜ç”Ÿæˆå›¾åƒä¸ç‰©ç†å‚æ•°ä¹‹é—´å­˜åœ¨å¼ºçƒˆç›¸å…³æ€§ã€‚é€šè¿‡å¢å¼ºGRRTæ•°æ®é›†ä¸BCDDMç”Ÿæˆçš„å›¾åƒï¼Œå¹¶ä½¿ç”¨ResNet50è¿›è¡Œå‚æ•°å›å½’ï¼Œå®ç°äº†å‚æ•°é¢„æµ‹æ€§èƒ½çš„æ˜¾è‘—æé«˜ã€‚æ­¤æ–¹æ³•é™ä½äº†è®¡ç®—æˆæœ¬ï¼Œä¸ºæ•°æ®é›†æ‰©å±•ã€å‚æ•°ä¼°è®¡å’Œæ¨¡å‹æ‹Ÿåˆæä¾›äº†æ›´å¿«ã€æ›´æœ‰æ•ˆçš„æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>Event Horizon Telescope (EHT) æ•°æ®ä¸æ¨¡æ‹Ÿå›¾åƒæ‹Ÿåˆå¯ç”¨äºæ¨æ–­é»‘æ´å’Œå¸ç§¯æµç‰¹æ€§ã€‚</li>
<li>ç”±äºä¸€èˆ¬ç›¸å¯¹è®ºå°„çº¿è¿½è¸ªï¼ˆGRRTï¼‰çš„è®¡ç®—å¯†é›†æ€§ï¼Œéœ€è¦æé«˜ç”Ÿæˆç‰¹å®šè¾å°„æµé‡å›¾åƒçš„æ•ˆç‡ã€‚</li>
<li>å¼•å…¥Branch Correction Denoising Diffusion Model (BCDDM)ä»¥æé«˜é»‘æ´å›¾åƒç”Ÿæˆçš„å‡†ç¡®æ€§ã€‚</li>
<li>BCDDMæ¨¡å‹ä½¿ç”¨åˆ†æ”¯æ ¡æ­£æœºåˆ¶å’ŒåŠ æƒæ··åˆæŸå¤±å‡½æ•°ï¼ŒåŸºäºä¸ƒä¸ªç‰©ç†å‚æ•°ï¼ˆRIAFæ¨¡å‹ï¼‰å·¥ä½œã€‚</li>
<li>å®éªŒæ˜¾ç¤ºç”Ÿæˆå›¾åƒä¸ç‰©ç†å‚æ•°ä¹‹é—´å­˜åœ¨å¼ºçƒˆç›¸å…³æ€§ã€‚</li>
<li>é€šè¿‡ç»“åˆBCDDMç”Ÿæˆçš„å›¾åƒå’ŒResNet50è¿›è¡Œå‚æ•°å›å½’ï¼Œæ˜¾è‘—æé«˜äº†å‚æ•°é¢„æµ‹æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.08528">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-295da45f86214cbe47b30df0005c4ef7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e49e40784599d57a80b530d4ef492a21.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-7c66ce0feb33af053c9edd02cc5f1e0f.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Training-Free-Safe-Denoisers-for-Safe-Use-of-Diffusion-Models"><a href="#Training-Free-Safe-Denoisers-for-Safe-Use-of-Diffusion-Models" class="headerlink" title="Training-Free Safe Denoisers for Safe Use of Diffusion Models"></a>Training-Free Safe Denoisers for Safe Use of Diffusion Models</h2><p><strong>Authors:Mingyu Kim, Dongjun Kim, Amman Yusuf, Stefano Ermon, Mi Jung Park</strong></p>
<p>There is growing concern over the safety of powerful diffusion models (DMs), as they are often misused to produce inappropriate, not-safe-for-work (NSFW) content or generate copyrighted material or data of individuals who wish to be forgotten. Many existing methods tackle these issues by heavily relying on text-based negative prompts or extensively retraining DMs to eliminate certain features or samples. In this paper, we take a radically different approach, directly modifying the sampling trajectory by leveraging a negation set (e.g., unsafe images, copyrighted data, or datapoints needed to be excluded) to avoid specific regions of data distribution, without needing to retrain or fine-tune DMs. We formally derive the relationship between the expected denoised samples that are safe and those that are not safe, leading to our $\textit{safe}$ denoiser which ensures its final samples are away from the area to be negated. Inspired by the derivation, we develop a practical algorithm that successfully produces high-quality samples while avoiding negation areas of the data distribution in text-conditional, class-conditional, and unconditional image generation scenarios. These results hint at the great potential of our training-free safe denoiser for using DMs more safely. </p>
<blockquote>
<p>å…³äºå¼ºå¤§çš„æ‰©æ•£æ¨¡å‹ï¼ˆDMsï¼‰çš„å®‰å…¨é—®é¢˜æ—¥ç›Šå—åˆ°å…³æ³¨ï¼Œå› ä¸ºå®ƒä»¬ç»å¸¸è¢«è¯¯ç”¨äºäº§ç”Ÿä¸é€‚å½“ã€ä¸å®‰å…¨çš„å·¥ä½œï¼ˆNSFWï¼‰å†…å®¹æˆ–ç”Ÿæˆç‰ˆæƒææ–™æˆ–å¸Œæœ›è¢«é—å¿˜çš„ä¸ªäººæ•°æ®ã€‚è®¸å¤šç°æœ‰æ–¹æ³•é€šè¿‡ä¾èµ–åŸºäºæ–‡æœ¬çš„è´Ÿé¢æç¤ºæˆ–å¤§é‡é‡æ–°è®­ç»ƒDMsæ¥æ¶ˆé™¤æŸäº›ç‰¹å¾æˆ–æ ·æœ¬æ¥è§£å†³è¿™äº›é—®é¢˜ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬é‡‡å–äº†æˆªç„¶ä¸åŒçš„æ–¹æ³•ï¼Œé€šè¿‡åˆ©ç”¨å¦å®šé›†ï¼ˆä¾‹å¦‚ï¼Œä¸å®‰å…¨çš„å›¾åƒã€ç‰ˆæƒæ•°æ®æˆ–éœ€è¦æ’é™¤çš„æ•°æ®ç‚¹ï¼‰ç›´æ¥ä¿®æ”¹é‡‡æ ·è½¨è¿¹ï¼Œé¿å…æ•°æ®åˆ†å¸ƒçš„ç‰¹å®šåŒºåŸŸï¼Œè€Œæ— éœ€é‡æ–°è®­ç»ƒæˆ–å¾®è°ƒDMsã€‚æˆ‘ä»¬æ­£å¼æ¨å¯¼å‡ºå®‰å…¨å’Œä¸å®‰å…¨é¢„æœŸå»å™ªæ ·æœ¬ä¹‹é—´çš„å…³ç³»ï¼Œä»è€Œå½¢æˆäº†æˆ‘ä»¬çš„å®‰å…¨å»å™ªå™¨ï¼Œç¡®ä¿æœ€ç»ˆæ ·æœ¬è¿œç¦»è¦å¦å®šçš„åŒºåŸŸã€‚å—æ¨å¯¼çš„å¯å‘ï¼Œæˆ‘ä»¬å¼€å‘äº†ä¸€ç§å®ç”¨ç®—æ³•ï¼Œè¯¥ç®—æ³•åœ¨æ–‡æœ¬æ¡ä»¶ã€ç±»åˆ«æ¡ä»¶å’Œæ— æ¡ä»¶å›¾åƒç”Ÿæˆåœºæ™¯ä¸­æˆåŠŸäº§ç”Ÿäº†é«˜è´¨é‡æ ·æœ¬ï¼ŒåŒæ—¶é¿å…äº†æ•°æ®åˆ†å¸ƒçš„å¦å®šåŒºåŸŸã€‚è¿™äº›ç»“æœæš—ç¤ºäº†æˆ‘ä»¬çš„æ— è®­ç»ƒå®‰å…¨å»å™ªå™¨åœ¨æ›´å®‰å…¨åœ°ä½¿ç”¨DMæ–¹é¢çš„å·¨å¤§æ½œåŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.08011v1">PDF</a> Preprint</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡å…³æ³¨æ‰©æ•£æ¨¡å‹çš„å®‰å…¨æ€§é—®é¢˜ï¼Œæå‡ºä¸€ç§æ–°é¢–çš„æ–¹æ³•ç›´æ¥ä¿®æ”¹é‡‡æ ·è½¨è¿¹ä»¥é¿å…ç‰¹å®šæ•°æ®åˆ†å¸ƒåŒºåŸŸã€‚è¯¥æ–¹æ³•é€šè¿‡åˆ©ç”¨å¦å®šé›†åˆæ¥é¿å…ç”Ÿæˆä¸å®‰å…¨çš„æ ·æœ¬ï¼Œæ— éœ€é‡æ–°è®­ç»ƒæˆ–å¾®è°ƒæ‰©æ•£æ¨¡å‹ã€‚ç ”ç©¶å›¢é˜ŸæˆåŠŸå¼€å‘å‡ºä¸€ç§å®ç”¨ç®—æ³•ï¼Œèƒ½å¤Ÿåœ¨æ–‡æœ¬æ¡ä»¶ã€ç±»åˆ«æ¡ä»¶å’Œæ— æ¡ä»¶å›¾åƒç”Ÿæˆåœºæ™¯ä¸­é¿å…å¦å®šåŒºåŸŸï¼ŒåŒæ—¶ç”Ÿæˆé«˜è´¨é‡æ ·æœ¬ã€‚è¿™ä¸ºæ›´å®‰å…¨åœ°ä½¿ç”¨æ‰©æ•£æ¨¡å‹æä¾›äº†å·¨å¤§æ½œåŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ‰©æ•£æ¨¡å‹çš„å®‰å…¨æ€§é—®é¢˜æ—¥ç›Šå—åˆ°å…³æ³¨ï¼ŒåŒ…æ‹¬è¯¯ç”¨ç”Ÿæˆä¸é€‚å½“å†…å®¹ã€ä¾µçŠ¯ç‰ˆæƒæˆ–ä¸ªäººéšç§ç­‰é—®é¢˜ã€‚</li>
<li>ç°æœ‰æ–¹æ³•ä¸»è¦ä¾èµ–æ–‡æœ¬è´Ÿé¢æç¤ºæˆ–é‡æ–°è®­ç»ƒæ‰©æ•£æ¨¡å‹æ¥æ¶ˆé™¤ç‰¹å®šç‰¹å¾æˆ–æ ·æœ¬ã€‚</li>
<li>æœ¬æ–‡é‡‡ç”¨ä¸€ç§æ–°æ–¹æ³•ï¼Œé€šè¿‡åˆ©ç”¨å¦å®šé›†åˆç›´æ¥ä¿®æ”¹é‡‡æ ·è½¨è¿¹ï¼Œé¿å…ç‰¹å®šæ•°æ®åˆ†å¸ƒåŒºåŸŸã€‚</li>
<li>ç ”ç©¶å›¢é˜Ÿå¼€å‘å‡ºå®ç”¨ç®—æ³•ï¼Œåœ¨å¤šç§å›¾åƒç”Ÿæˆåœºæ™¯ä¸­æˆåŠŸé¿å…ç”Ÿæˆä¸å®‰å…¨çš„æ ·æœ¬ã€‚</li>
<li>è¯¥æ–¹æ³•æ— éœ€é‡æ–°è®­ç»ƒæˆ–å¾®è°ƒæ‰©æ•£æ¨¡å‹ï¼Œæé«˜äº†ä½¿ç”¨æ‰©æ•£æ¨¡å‹çš„å®‰å…¨æ€§ã€‚</li>
<li>æœ¬æ–‡æå‡ºçš„å®‰å…¨å»å™ªå™¨å…·æœ‰å·¨å¤§æ½œåŠ›ï¼Œä¸ºæ›´å®‰å…¨åœ°ä½¿ç”¨æ‰©æ•£æ¨¡å‹æä¾›äº†æ–°é€”å¾„ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.08011">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-3e5d31ad60f4dd0aa281856691ef48a5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f60bd99a4b69ee4a485e447cb891ac3f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ddc420a256935f81765be36d74f10308.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c9385144f4fdcbdf028b7ca4b6be6338.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-303b62d6f7a01ddf70a982bf0b7bf411.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e3f3b3154fabd339535264a798f82277.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="SurGrID-Controllable-Surgical-Simulation-via-Scene-Graph-to-Image-Diffusion"><a href="#SurGrID-Controllable-Surgical-Simulation-via-Scene-Graph-to-Image-Diffusion" class="headerlink" title="SurGrID: Controllable Surgical Simulation via Scene Graph to Image   Diffusion"></a>SurGrID: Controllable Surgical Simulation via Scene Graph to Image   Diffusion</h2><p><strong>Authors:Yannik Frisch, Ssharvien Kumar Sivakumar, Ã‡aÄŸhan KÃ¶ksal, Elsa BÃ¶hm, Felix Wagner, Adrian Gericke, Ghazal Ghazaei, Anirban Mukhopadhyay</strong></p>
<p>Surgical simulation offers a promising addition to conventional surgical training. However, available simulation tools lack photorealism and rely on hardcoded behaviour. Denoising Diffusion Models are a promising alternative for high-fidelity image synthesis, but existing state-of-the-art conditioning methods fall short in providing precise control or interactivity over the generated scenes.   We introduce SurGrID, a Scene Graph to Image Diffusion Model, allowing for controllable surgical scene synthesis by leveraging Scene Graphs. These graphs encode a surgical sceneâ€™s componentsâ€™ spatial and semantic information, which are then translated into an intermediate representation using our novel pre-training step that explicitly captures local and global information.   Our proposed method improves the fidelity of generated images and their coherence with the graph input over the state-of-the-art. Further, we demonstrate the simulationâ€™s realism and controllability in a user assessment study involving clinical experts.   Scene Graphs can be effectively used for precise and interactive conditioning of Denoising Diffusion Models for simulating surgical scenes, enabling high fidelity and interactive control over the generated content. </p>
<blockquote>
<p>æ‰‹æœ¯æ¨¡æ‹Ÿä¸ºä¼ ç»Ÿçš„æ‰‹æœ¯è®­ç»ƒæä¾›äº†ä¸€ä¸ªæœ‰å‰æ™¯çš„è¡¥å……ã€‚ç„¶è€Œï¼Œç°æœ‰çš„æ¨¡æ‹Ÿå·¥å…·ç¼ºä¹é€¼çœŸçš„è§†è§‰æ•ˆæœï¼Œå¹¶ä¾èµ–äºç¡¬ç¼–ç çš„è¡Œä¸ºã€‚é™å™ªæ‰©æ•£æ¨¡å‹ï¼ˆDenoising Diffusion Modelsï¼‰åœ¨é«˜ä¿çœŸå›¾åƒåˆæˆæ–¹é¢æ˜¯ä¸€ä¸ªæœ‰å‰æ™¯çš„æ›¿ä»£æ–¹æ¡ˆï¼Œä½†ç°æœ‰çš„æœ€å…ˆè¿›çš„æ¡ä»¶è®¾å®šæ–¹æ³•åœ¨æä¾›å¯¹ç”Ÿæˆåœºæ™¯çš„ç²¾ç¡®æ§åˆ¶å’Œäº¤äº’æ€§æ–¹é¢å­˜åœ¨ä¸è¶³ã€‚æˆ‘ä»¬å¼•å…¥äº†SurGrIDï¼Œä¸€ä¸ªåœºæ™¯å›¾åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹ï¼Œåˆ©ç”¨åœºæ™¯å›¾å®ç°å¯æ§æ‰‹æœ¯åœºæ™¯åˆæˆã€‚è¿™äº›å›¾ç¼–ç äº†æ‰‹æœ¯åœºæ™¯ç»„ä»¶çš„ç©ºé—´å’Œè¯­ä¹‰ä¿¡æ¯ï¼Œç„¶åé€šè¿‡æˆ‘ä»¬æ–°é¢–çš„é¢„è®­ç»ƒæ­¥éª¤å°†å…¶ç¿»è¯‘æˆä¸€ä¸ªä¸­é—´è¡¨ç¤ºï¼Œè¿™ä¸ªé¢„è®­ç»ƒæ­¥éª¤æ˜¾å¼åœ°æ•æ‰å±€éƒ¨å’Œå…¨å±€ä¿¡æ¯ã€‚æˆ‘ä»¬æå‡ºçš„æ–¹æ³•æé«˜äº†ç”Ÿæˆå›¾åƒçš„çœŸå®æ€§å’Œä¸å›¾å½¢è¾“å…¥çš„è¿è´¯æ€§ï¼Œè¶…è¿‡äº†å½“å‰æœ€å…ˆè¿›çš„æŠ€æœ¯ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜é€šè¿‡æ¶‰åŠä¸´åºŠä¸“å®¶çš„ç”¨æˆ·è¯„ä¼°ç ”ç©¶è¯æ˜äº†æ¨¡æ‹Ÿçš„çœŸå®æ€§å’Œå¯æ§æ€§ã€‚åœºæ™¯å›¾å¯æœ‰æ•ˆåœ°ç”¨äºç²¾ç¡®å’Œäº¤äº’å¼åœ°è°ƒèŠ‚é™å™ªæ‰©æ•£æ¨¡å‹ï¼Œä»¥æ¨¡æ‹Ÿæ‰‹æœ¯åœºæ™¯ï¼Œå®ç°å¯¹ç”Ÿæˆå†…å®¹çš„é«˜ä¿çœŸå’Œäº¤äº’å¼æ§åˆ¶ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.07945v1">PDF</a> </p>
<p><strong>æ‘˜è¦</strong></p>
<p>æ‰‹æœ¯æ¨¡æ‹Ÿä¸ºä¼ ç»Ÿæ‰‹æœ¯è®­ç»ƒæä¾›äº†æœ‰å‰é€”çš„è¡¥å……ã€‚ç„¶è€Œï¼Œç°æœ‰æ¨¡æ‹Ÿå·¥å…·ç¼ºä¹é€¼çœŸåº¦ï¼Œå¹¶ä¾èµ–äºç¡¬ç¼–ç è¡Œä¸ºã€‚å»å™ªæ‰©æ•£æ¨¡å‹ï¼ˆDenoising Diffusion Modelsï¼‰åœ¨é«˜ä¿çœŸå›¾åƒåˆæˆä¸­å±•ç°å‡ºå·¨å¤§æ½œåŠ›ï¼Œä½†å½“å‰æœ€å…ˆè¿›çš„æ¡ä»¶è®¾ç½®æ–¹æ³•æ— æ³•æä¾›å¯¹ç”Ÿæˆåœºæ™¯çš„ç²¾ç¡®æ§åˆ¶å’Œäº¤äº’æ€§ã€‚æˆ‘ä»¬å¼•å…¥äº†SurGrIDï¼Œä¸€ç§åœºæ™¯å›¾åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹ï¼Œé€šè¿‡åˆ©ç”¨åœºæ™¯å›¾ï¼Œå®ç°å¯æ§æ‰‹æœ¯åœºæ™¯åˆæˆã€‚è¿™äº›å›¾ç¼–ç äº†æ‰‹æœ¯åœºæ™¯ç»„ä»¶çš„ç©ºé—´å’Œè¯­ä¹‰ä¿¡æ¯ï¼Œç„¶åé€šè¿‡æˆ‘ä»¬çš„æ–°å‹é¢„è®­ç»ƒæ­¥éª¤å°†å…¶ç¿»è¯‘ä¸ºä¸€ç§ä¸­é—´è¡¨ç¤ºï¼Œè¯¥æ­¥éª¤æ˜¾å¼æ•è·å±€éƒ¨å’Œå…¨å±€ä¿¡æ¯ã€‚æ‰€æå‡ºçš„æ–¹æ³•æé«˜äº†ç”Ÿæˆå›¾åƒçš„çœŸå®æ€§å’Œä¸å›¾å½¢è¾“å…¥çš„è¿è´¯æ€§ï¼Œè¶…è¿‡äº†å½“å‰æœ€å…ˆè¿›çš„æŠ€æœ¯ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬é€šè¿‡æ¶‰åŠä¸´åºŠä¸“å®¶çš„ç”¨æˆ·è¯„ä¼°ç ”ç©¶è¯æ˜äº†æ¨¡æ‹Ÿçš„çœŸå®æ€§å’Œå¯æ§æ€§ã€‚åœºæ™¯å›¾å¯æœ‰æ•ˆç”¨äºå¯¹å»å™ªæ‰©æ•£æ¨¡å‹è¿›è¡Œç²¾ç¡®å’Œäº¤äº’å¼æ¡ä»¶è®¾ç½®ï¼Œä»¥æ¨¡æ‹Ÿæ‰‹æœ¯åœºæ™¯ï¼Œå®ç°å¯¹ç”Ÿæˆå†…å®¹çš„é«˜ä¿çœŸå’Œäº¤äº’å¼æ§åˆ¶ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>ç°æœ‰æ‰‹æœ¯æ¨¡æ‹Ÿå·¥å…·ç¼ºä¹é€¼çœŸåº¦ä¸”ä¾èµ–ç¡¬ç¼–ç è¡Œä¸ºã€‚</li>
<li>å»å™ªæ‰©æ•£æ¨¡å‹åœ¨é«˜ä¿çœŸå›¾åƒåˆæˆä¸­æœ‰å·¨å¤§æ½œåŠ›ã€‚</li>
<li>å½“å‰çš„æ¡ä»¶è®¾ç½®æ–¹æ³•æ— æ³•ä¸ºæ¨¡æ‹Ÿæ‰‹æœ¯åœºæ™¯æä¾›ç²¾ç¡®æ§åˆ¶å’Œäº¤äº’æ€§ã€‚</li>
<li>å¼•å…¥SurGrIDæ¨¡å‹ï¼Œé€šè¿‡åœºæ™¯å›¾å®ç°å¯æ§æ‰‹æœ¯åœºæ™¯åˆæˆã€‚</li>
<li>åœºæ™¯å›¾ç¼–ç äº†æ‰‹æœ¯åœºæ™¯ç»„ä»¶çš„ç©ºé—´å’Œè¯­ä¹‰ä¿¡æ¯ã€‚</li>
<li>SurGrIDé€šè¿‡é¢„è®­ç»ƒæ­¥éª¤å°†åœºæ™¯å›¾ä¿¡æ¯è½¬åŒ–ä¸ºä¸­é—´è¡¨ç¤ºï¼Œæé«˜äº†ç”Ÿæˆå›¾åƒçš„çœŸå®æ€§å’Œä¸å›¾å½¢è¾“å…¥çš„è¿è´¯æ€§ã€‚</li>
<li>ç”¨æˆ·è¯„ä¼°ç ”ç©¶è¯æ˜äº†SurGrIDæ¨¡æ‹Ÿçš„çœŸå®æ€§ã€å¯æ§æ€§å’Œæœ‰æ•ˆæ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.07945">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-316d50d1fbd86c8be6f4e33dddf527c5.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-bc54086dc98a19669fe9bd9b6e24f970.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d6edeae3ac3f2919e40929ba457c6690.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="MRS-A-Fast-Sampler-for-Mean-Reverting-Diffusion-based-on-ODE-and-SDE-Solvers"><a href="#MRS-A-Fast-Sampler-for-Mean-Reverting-Diffusion-based-on-ODE-and-SDE-Solvers" class="headerlink" title="MRS: A Fast Sampler for Mean Reverting Diffusion based on ODE and SDE   Solvers"></a>MRS: A Fast Sampler for Mean Reverting Diffusion based on ODE and SDE   Solvers</h2><p><strong>Authors:Ao Li, Wei Fang, Hongbo Zhao, Le Lu, Ge Yang, Minfeng Xu</strong></p>
<p>In applications of diffusion models, controllable generation is of practical significance, but is also challenging. Current methods for controllable generation primarily focus on modifying the score function of diffusion models, while Mean Reverting (MR) Diffusion directly modifies the structure of the stochastic differential equation (SDE), making the incorporation of image conditions simpler and more natural. However, current training-free fast samplers are not directly applicable to MR Diffusion. And thus MR Diffusion requires hundreds of NFEs (number of function evaluations) to obtain high-quality samples. In this paper, we propose a new algorithm named MRS (MR Sampler) to reduce the sampling NFEs of MR Diffusion. We solve the reverse-time SDE and the probability flow ordinary differential equation (PF-ODE) associated with MR Diffusion, and derive semi-analytical solutions. The solutions consist of an analytical function and an integral parameterized by a neural network. Based on this solution, we can generate high-quality samples in fewer steps. Our approach does not require training and supports all mainstream parameterizations, including noise prediction, data prediction and velocity prediction. Extensive experiments demonstrate that MR Sampler maintains high sampling quality with a speedup of 10 to 20 times across ten different image restoration tasks. Our algorithm accelerates the sampling procedure of MR Diffusion, making it more practical in controllable generation. </p>
<blockquote>
<p>åœ¨æ‰©æ•£æ¨¡å‹çš„åº”ç”¨ä¸­ï¼Œå¯æ§ç”Ÿæˆå…·æœ‰å®é™…æ„ä¹‰ï¼Œä½†ä¹Ÿå…·æœ‰æŒ‘æˆ˜æ€§ã€‚å½“å‰çš„å¯æ§ç”Ÿæˆæ–¹æ³•ä¸»è¦é›†ä¸­åœ¨ä¿®æ”¹æ‰©æ•£æ¨¡å‹çš„åˆ†æ•°å‡½æ•°ï¼Œè€ŒMean Revertingï¼ˆMRï¼‰æ‰©æ•£åˆ™ç›´æ¥ä¿®æ”¹éšæœºå¾®åˆ†æ–¹ç¨‹ï¼ˆSDEï¼‰çš„ç»“æ„ï¼Œä½¿å›¾åƒæ¡ä»¶çš„èå…¥æ›´ç®€å•è‡ªç„¶ã€‚ç„¶è€Œï¼Œç°æœ‰çš„æ— è®­ç»ƒå¿«é€Ÿé‡‡æ ·å™¨å¹¶ä¸ç›´æ¥é€‚ç”¨äºMRæ‰©æ•£ã€‚å› æ­¤ï¼ŒMRæ‰©æ•£éœ€è¦æ•°ç™¾ä¸ªåŠŸèƒ½è¯„ä¼°ï¼ˆNFEï¼‰æ‰èƒ½è·å¾—é«˜è´¨é‡çš„æ ·æœ¬ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åä¸ºMRSï¼ˆMRé‡‡æ ·å™¨ï¼‰çš„æ–°ç®—æ³•ï¼Œä»¥å‡å°‘MRæ‰©æ•£çš„é‡‡æ ·NFEã€‚æˆ‘ä»¬è§£å†³äº†åå‘æ—¶é—´SDEå’Œä¸MRæ‰©æ•£ç›¸å…³çš„æ¦‚ç‡æµå¸¸å¾®åˆ†æ–¹ç¨‹ï¼ˆPF-ODEï¼‰ï¼Œå¹¶å¾—å‡ºåŠè§£æè§£ã€‚è¯¥è§£å†³æ–¹æ¡ˆç”±ä¸€ä¸ªåˆ†æå‡½æ•°å’Œä¸€ä¸ªç”±ç¥ç»ç½‘ç»œå‚æ•°åŒ–çš„ç§¯åˆ†ç»„æˆã€‚åŸºäºè¿™ä¸ªè§£å†³æ–¹æ¡ˆï¼Œæˆ‘ä»¬å¯ä»¥ä»¥æ›´å°‘çš„æ­¥éª¤ç”Ÿæˆé«˜è´¨é‡çš„æ ·æœ¬ã€‚æˆ‘ä»¬çš„æ–¹æ³•ä¸éœ€è¦è®­ç»ƒï¼Œæ”¯æŒåŒ…æ‹¬å™ªå£°é¢„æµ‹ã€æ•°æ®é¢„æµ‹å’Œé€Ÿåº¦é¢„æµ‹åœ¨å†…çš„æ‰€æœ‰ä¸»æµå‚æ•°åŒ–ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼ŒMRé‡‡æ ·å™¨åœ¨10ä¸ªä¸åŒçš„å›¾åƒæ¢å¤ä»»åŠ¡ä¸­ä¿æŒäº†é«˜é‡‡æ ·è´¨é‡ï¼Œå¹¶å®ç°äº†10åˆ°20å€çš„åŠ é€Ÿã€‚æˆ‘ä»¬çš„ç®—æ³•åŠ é€Ÿäº†MRæ‰©æ•£çš„é‡‡æ ·è¿‡ç¨‹ï¼Œä½¿å…¶åœ¨å¯æ§ç”Ÿæˆä¸­æ›´å®ç”¨ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.07856v1">PDF</a> Accepted by ICLR 2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†æ‰©æ•£æ¨¡å‹ä¸­çš„å¯æ§ç”Ÿæˆçš„é‡è¦æ€§å’ŒæŒ‘æˆ˜æ€§ã€‚ç°æœ‰æ–¹æ³•ä¸»è¦é€šè¿‡ä¿®æ”¹æ‰©æ•£æ¨¡å‹çš„è¯„åˆ†å‡½æ•°æ¥å®ç°ï¼Œè€ŒMean Revertingï¼ˆMRï¼‰Diffusionåˆ™ç›´æ¥ä¿®æ”¹éšæœºå¾®åˆ†æ–¹ç¨‹ï¼ˆSDEï¼‰çš„ç»“æ„ã€‚ä½†ç°æœ‰çš„è®­ç»ƒå¿«é€Ÿé‡‡æ ·å™¨ä¸é€‚ç”¨äºMR Diffusionã€‚å› æ­¤ï¼Œæœ¬æ–‡æå‡ºä¸€ç§åä¸ºMR Samplerçš„æ–°ç®—æ³•æ¥å‡å°‘MR Diffusionçš„é‡‡æ ·æ¬¡æ•°ã€‚é€šè¿‡è§£å†³é€†å‘æ—¶é—´SDEå’Œä¸MR Diffusionç›¸å…³çš„æ¦‚ç‡æµå¸¸å¾®åˆ†æ–¹ç¨‹ï¼ˆPF-ODEï¼‰ï¼Œæˆ‘ä»¬å¾—åˆ°äº†åŠè§£æè§£ã€‚è¯¥è§£å†³æ–¹æ¡ˆåŒ…æ‹¬ä¸€ä¸ªåˆ†æå‡½æ•°å’Œä¸€ä¸ªç”±ç¥ç»ç½‘ç»œå‚æ•°åŒ–çš„ç§¯åˆ†ã€‚åŸºäºè¯¥è§£å†³æ–¹æ¡ˆï¼Œæˆ‘ä»¬å¯ä»¥ä»¥è¾ƒå°‘çš„æ­¥éª¤ç”Ÿæˆé«˜è´¨é‡æ ·æœ¬ï¼Œæ— éœ€è®­ç»ƒï¼Œå¹¶é€‚ç”¨äºæ‰€æœ‰ä¸»æµå‚æ•°åŒ–æ–¹æ³•ã€‚å®éªŒè¯æ˜ï¼ŒMR Sampleråœ¨åä¸ªä¸åŒçš„å›¾åƒæ¢å¤ä»»åŠ¡ä¸­ï¼Œé‡‡æ ·é€Ÿåº¦æé«˜äº†10åˆ°20å€ï¼Œæé«˜äº†MR Diffusionçš„å®ç”¨æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ‰©æ•£æ¨¡å‹ä¸­çš„å¯æ§ç”Ÿæˆå…·æœ‰å®è·µæ„ä¹‰ä¸”å…·æŒ‘æˆ˜æ€§ã€‚</li>
<li>Mean Revertingï¼ˆMRï¼‰Diffusionç›´æ¥ä¿®æ”¹SDEç»“æ„ï¼Œç®€åŒ–å›¾åƒæ¡ä»¶çš„èå…¥ã€‚</li>
<li>ç°æœ‰è®­ç»ƒå¿«é€Ÿé‡‡æ ·å™¨ä¸é€‚ç”¨äºMR Diffusionï¼Œéœ€æ•°ç™¾æ¬¡NFEsè·å¾—é«˜è´¨é‡æ ·æœ¬ã€‚</li>
<li>æå‡ºæ–°çš„MR Samplerç®—æ³•ï¼ŒåŸºäºåŠè§£æè§£ç”Ÿæˆé«˜è´¨é‡æ ·æœ¬ï¼Œæ­¥éª¤å‡å°‘ã€‚</li>
<li>MR Sampleræ— éœ€è®­ç»ƒï¼Œé€‚ç”¨äºå„ç§å‚æ•°åŒ–æ–¹æ³•ï¼ŒåŒ…æ‹¬å™ªå£°é¢„æµ‹ã€æ•°æ®é¢„æµ‹å’Œé€Ÿåº¦é¢„æµ‹ã€‚</li>
<li>å®éªŒè¯æ˜ï¼ŒMR Sampleråœ¨å¤šä¸ªå›¾åƒæ¢å¤ä»»åŠ¡ä¸­æ˜¾è‘—æé«˜é‡‡æ ·é€Ÿåº¦ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.07856">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-773329b7838d2d0ff3c427ab452c2a33.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-715f6f4c7a6a31bc86cffcf0024918fd.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-10991a13c40b06e24c616fa3a30f9b1d.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Spread-them-Apart-Towards-Robust-Watermarking-of-Generated-Content"><a href="#Spread-them-Apart-Towards-Robust-Watermarking-of-Generated-Content" class="headerlink" title="Spread them Apart: Towards Robust Watermarking of Generated Content"></a>Spread them Apart: Towards Robust Watermarking of Generated Content</h2><p><strong>Authors:Mikhail Pautov, Danil Ivanov, Andrey V. Galichin, Oleg Rogov, Ivan Oseledets</strong></p>
<p>Generative models that can produce realistic images have improved significantly in recent years. The quality of the generated content has increased drastically, so sometimes it is very difficult to distinguish between the real images and the generated ones. Such an improvement comes at a price of ethical concerns about the usage of the generative models: the users of generative models can improperly claim ownership of the generated content protected by a license. In this paper, we propose an approach to embed watermarks into the generated content to allow future detection of the generated content and identification of the user who generated it. The watermark is embedded during the inference of the model, so the proposed approach does not require the retraining of the latter. We prove that watermarks embedded are guaranteed to be robust against additive perturbations of a bounded magnitude. We apply our method to watermark diffusion models and show that it matches state-of-the-art watermarking schemes in terms of robustness to different types of synthetic watermark removal attacks. </p>
<blockquote>
<p>è¿‘å¹´æ¥ï¼Œèƒ½å¤Ÿç”Ÿæˆé€¼çœŸå›¾åƒçš„ç”Ÿæˆæ¨¡å‹å¾—åˆ°äº†æ˜¾è‘—æ”¹è¿›ã€‚ç”Ÿæˆå†…å®¹çš„è´¨é‡å¤§å¹…æé«˜ï¼Œå› æ­¤æœ‰æ—¶å¾ˆéš¾åŒºåˆ†çœŸå®å›¾åƒå’Œç”Ÿæˆå›¾åƒã€‚è¿™ç§æ”¹è¿›å¼•å‘äº†å…³äºä½¿ç”¨ç”Ÿæˆæ¨¡å‹çš„é“å¾·æ‹…å¿§ï¼šç”Ÿæˆæ¨¡å‹çš„ç”¨æˆ·å¯èƒ½ä¼šä¸å½“åœ°å£°ç§°æ‹¥æœ‰å—è®¸å¯è¯ä¿æŠ¤çš„ç”Ÿæˆå†…å®¹ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§å°†æ°´å°åµŒå…¥ç”Ÿæˆå†…å®¹çš„æ–¹æ³•ï¼Œä»¥ä¾¿æœªæ¥æ£€æµ‹ç”Ÿæˆå†…å®¹å¹¶è¯†åˆ«ç”Ÿæˆå†…å®¹çš„ç”¨æˆ·ã€‚æ°´å°æ˜¯åœ¨æ¨¡å‹çš„æ¨ç†è¿‡ç¨‹ä¸­åµŒå…¥çš„ï¼Œå› æ­¤æ‰€æå‡ºçš„æ–¹æ³•ä¸éœ€è¦é‡æ–°è®­ç»ƒæ¨¡å‹ã€‚æˆ‘ä»¬è¯æ˜åµŒå…¥çš„æ°´å°ä¿è¯å¯¹ä¸€å®šå¹…åº¦çš„é™„åŠ æ‰°åŠ¨å…·æœ‰é²æ£’æ€§ã€‚æˆ‘ä»¬å°†è¯¥æ–¹æ³•åº”ç”¨äºæ°´å°æ‰©æ•£æ¨¡å‹ï¼Œå¹¶è¯æ˜å…¶åœ¨é¢å¯¹ä¸åŒç±»å‹çš„åˆæˆæ°´å°å»é™¤æ”»å‡»æ—¶ï¼Œä¸æœ€å…ˆè¿›çš„æ°´å°æ–¹æ¡ˆåœ¨é²æ£’æ€§æ–¹é¢ç›¸åŒ¹é…ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.07845v1">PDF</a> </p>
<p><strong>Summary</strong><br>     è¿‘å¹´ç”Ÿæˆæ¨¡å‹ç”Ÿæˆé€¼çœŸå›¾åƒçš„èƒ½åŠ›æ˜¾è‘—æé«˜ï¼Œç”Ÿæˆå†…å®¹çš„è´¨é‡å¤§å¹…æå‡ï¼Œæœ‰æ—¶éš¾ä»¥åŒºåˆ†çœŸå®å›¾åƒå’Œç”Ÿæˆå›¾åƒã€‚ç„¶è€Œï¼Œè¿™ä¹Ÿå¼•å‘äº†å…³äºç”Ÿæˆæ¨¡å‹ä½¿ç”¨çš„é“å¾·é—®é¢˜ï¼Œå¦‚ç”¨æˆ·å¯èƒ½ä¸æ­£å½“å£°ç§°å¯¹ç”Ÿæˆå†…å®¹çš„æ‰€æœ‰æƒã€‚æœ¬æ–‡æå‡ºä¸€ç§åœ¨ç”Ÿæˆå†…å®¹ä¸­åµŒå…¥æ°´å°çš„æ–¹æ³•ï¼Œä»¥ä¾¿æœªæ¥æ£€æµ‹ç”Ÿæˆå†…å®¹å¹¶è¯†åˆ«ç”Ÿæˆå®ƒçš„ç”¨æˆ·ã€‚æ°´å°æ˜¯åœ¨æ¨¡å‹çš„æ¨ç†è¿‡ç¨‹ä¸­åµŒå…¥çš„ï¼Œå› æ­¤ä¸éœ€è¦é‡æ–°è®­ç»ƒæ¨¡å‹ã€‚å®éªŒè¯æ˜ï¼ŒåµŒå…¥çš„æ°´å°å¯¹ä¸€å®šå¹…åº¦çš„é™„åŠ æ‰°åŠ¨å…·æœ‰é²æ£’æ€§ã€‚æˆ‘ä»¬å°†è¯¥æ–¹æ³•åº”ç”¨äºæ°´å°æ‰©æ•£æ¨¡å‹ï¼Œå¹¶æ˜¾ç¤ºå…¶åœ¨é¢å¯¹ä¸åŒç±»å‹çš„åˆæˆæ°´å°å»é™¤æ”»å‡»æ—¶ï¼Œå…¶ç¨³å¥æ€§ä¸æœ€å…ˆè¿›çš„æ°´å°æ–¹æ¡ˆç›¸åŒ¹é…ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç”Ÿæˆæ¨¡å‹ç”Ÿæˆçš„å›¾åƒè´¨é‡æ˜¾è‘—æé«˜ï¼Œéš¾ä»¥åŒºåˆ†çœŸå®å’Œç”Ÿæˆå›¾åƒã€‚</li>
<li>ç”Ÿæˆæ¨¡å‹çš„ä½¿ç”¨å¼•å‘é“å¾·é—®é¢˜ï¼Œå¦‚æ‰€æœ‰æƒäº‰è®®ã€‚</li>
<li>æå‡ºåœ¨ç”Ÿæˆå†…å®¹ä¸­åµŒå…¥æ°´å°çš„æ–¹æ³•ï¼Œä»¥æ£€æµ‹ç”Ÿæˆå†…å®¹å¹¶è¯†åˆ«ç”Ÿæˆç”¨æˆ·ã€‚</li>
<li>åµŒå…¥æ°´å°çš„è¿‡ç¨‹ä¸éœ€è¦é‡æ–°è®­ç»ƒæ¨¡å‹ã€‚</li>
<li>åµŒå…¥çš„æ°´å°å¯¹ä¸€å®šå¹…åº¦çš„é™„åŠ æ‰°åŠ¨å…·æœ‰é²æ£’æ€§ã€‚</li>
<li>è¯¥æ–¹æ³•åº”ç”¨äºæ°´å°æ‰©æ•£æ¨¡å‹ï¼Œå…¶ç¨³å¥æ€§ä¸æœ€å…ˆè¿›çš„æ–¹æ¡ˆç›¸åŒ¹é…ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.07845">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-7a77be4a380b3898d98eebe2d7d64d84.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-bde7df753f0302f3f19fe63f128903eb.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="Less-is-More-Masking-Elements-in-Image-Condition-Features-Avoids-Content-Leakages-in-Style-Transfer-Diffusion-Models"><a href="#Less-is-More-Masking-Elements-in-Image-Condition-Features-Avoids-Content-Leakages-in-Style-Transfer-Diffusion-Models" class="headerlink" title="Less is More: Masking Elements in Image Condition Features Avoids   Content Leakages in Style Transfer Diffusion Models"></a>Less is More: Masking Elements in Image Condition Features Avoids   Content Leakages in Style Transfer Diffusion Models</h2><p><strong>Authors:Lin Zhu, Xinbing Wang, Chenghu Zhou, Qinying Gu, Nanyang Ye</strong></p>
<p>Given a style-reference image as the additional image condition, text-to-image diffusion models have demonstrated impressive capabilities in generating images that possess the content of text prompts while adopting the visual style of the reference image. However, current state-of-the-art methods often struggle to disentangle content and style from style-reference images, leading to issues such as content leakages. To address this issue, we propose a masking-based method that efficiently decouples content from style without the need of tuning any model parameters. By simply masking specific elements in the style referenceâ€™s image features, we uncover a critical yet under-explored principle: guiding with appropriately-selected fewer conditions (e.g., dropping several image feature elements) can efficiently avoid unwanted content flowing into the diffusion models, enhancing the style transfer performances of text-to-image diffusion models. In this paper, we validate this finding both theoretically and experimentally. Extensive experiments across various styles demonstrate the effectiveness of our masking-based method and support our theoretical results. </p>
<blockquote>
<p>ç»™å®šé£æ ¼å‚è€ƒå›¾åƒä½œä¸ºé¢å¤–çš„å›¾åƒæ¡ä»¶ï¼Œæ–‡æœ¬åˆ°å›¾åƒçš„æ‰©æ•£æ¨¡å‹åœ¨ç”Ÿæˆæ‹¥æœ‰æ–‡æœ¬æç¤ºå†…å®¹çš„åŒæ—¶é‡‡ç”¨å‚è€ƒå›¾åƒè§†è§‰é£æ ¼çš„å›¾åƒæ–¹é¢è¡¨ç°å‡ºäº†ä»¤äººå°è±¡æ·±åˆ»çš„èƒ½åŠ›ã€‚ç„¶è€Œï¼Œå½“å‰æœ€å…ˆè¿›çš„æ–¹æ³•å¾€å¾€éš¾ä»¥ä»é£æ ¼å‚è€ƒå›¾åƒä¸­åˆ†ç¦»å†…å®¹å’Œé£æ ¼ï¼Œå¯¼è‡´å†…å®¹æ³„éœ²ç­‰é—®é¢˜ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºæ©ç çš„æ–¹æ³•ï¼Œè¯¥æ–¹æ³•èƒ½å¤Ÿé«˜æ•ˆåœ°è§£è€¦å†…å®¹ä¸é£æ ¼ï¼Œè€Œæ— éœ€è°ƒæ•´ä»»ä½•æ¨¡å‹å‚æ•°ã€‚é€šè¿‡ç®€å•åœ°æ©ç›–é£æ ¼å‚è€ƒå›¾åƒç‰¹å¾ä¸­çš„ç‰¹å®šå…ƒç´ ï¼Œæˆ‘ä»¬å‘ç°äº†ä¸€ä¸ªå…³é”®ä½†å°šæœªè¢«å……åˆ†æ¢ç´¢çš„åŸç†ï¼šé€šè¿‡é€‚å½“é€‰æ‹©è¾ƒå°‘çš„æ¡ä»¶è¿›è¡Œå¼•å¯¼ï¼ˆä¾‹å¦‚ï¼Œåˆ é™¤å‡ ä¸ªå›¾åƒç‰¹å¾å…ƒç´ ï¼‰å¯ä»¥æœ‰æ•ˆåœ°é¿å…ä¸æƒ³è¦çš„å†…å®¹æµå…¥æ‰©æ•£æ¨¡å‹ï¼Œä»è€Œæé«˜æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹çš„é£æ ¼è½¬ç§»æ€§èƒ½ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æ—¢ä»ç†è®ºä¸Šä¹Ÿä»å®éªŒä¸ŠéªŒè¯äº†è¿™ä¸€å‘ç°ã€‚è·¨å„ç§é£æ ¼çš„å¹¿æ³›å®éªŒè¯æ˜äº†æˆ‘ä»¬åŸºäºæ©ç çš„æ–¹æ³•çš„æœ‰æ•ˆæ€§ï¼Œå¹¶æ”¯æŒäº†æˆ‘ä»¬çš„ç†è®ºç»“æœã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.07466v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>ç»™å®šé£æ ¼å‚è€ƒå›¾åƒä½œä¸ºé¢å¤–çš„å›¾åƒæ¡ä»¶ï¼Œæ–‡æœ¬åˆ°å›¾åƒçš„æ‰©æ•£æ¨¡å‹åœ¨ç”Ÿæˆå›¾åƒæ—¶è¡¨ç°å‡ºäº†ä»¤äººå°è±¡æ·±åˆ»çš„èƒ½åŠ›ï¼Œèƒ½å¤Ÿç”Ÿæˆæ—¢åŒ…å«æ–‡æœ¬æç¤ºå†…å®¹åˆé‡‡ç”¨å‚è€ƒå›¾åƒè§†è§‰é£æ ¼çš„å›¾åƒã€‚ç„¶è€Œï¼Œå½“å‰å…ˆè¿›çš„æ–¹æ³•åœ¨ä»æœªé£æ ¼å‚è€ƒå›¾åƒä¸­åˆ†ç¦»å†…å®¹å’Œé£æ ¼æ—¶ç»å¸¸é‡åˆ°å›°éš¾ï¼Œå¯¼è‡´å†…å®¹æ³„æ¼ç­‰é—®é¢˜ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºæ©è†œçš„æ–¹æ³•ï¼Œè¯¥æ–¹æ³•èƒ½å¤Ÿæœ‰æ•ˆåœ°å°†å†…å®¹ä»é£æ ¼ä¸­åˆ†ç¦»ï¼Œè€Œæ— éœ€è°ƒæ•´ä»»ä½•æ¨¡å‹å‚æ•°ã€‚é€šè¿‡ç®€å•åœ°æ©ç›–é£æ ¼å‚è€ƒå›¾åƒç‰¹å¾ä¸­çš„ç‰¹å®šå…ƒç´ ï¼Œæˆ‘ä»¬å‘ç°äº†å…³é”®ä½†å°šæœªè¢«å……åˆ†æ¢ç´¢çš„åŸç†ï¼šä½¿ç”¨é€‚å½“é€‰æ‹©çš„è¾ƒå°‘æ¡ä»¶è¿›è¡Œå¼•å¯¼ï¼ˆä¾‹å¦‚åˆ é™¤å‡ ä¸ªå›¾åƒç‰¹å¾å…ƒç´ ï¼‰å¯ä»¥æœ‰æ•ˆåœ°é˜²æ­¢ä¸éœ€è¦çš„å†…å®¹æµå…¥æ‰©æ•£æ¨¡å‹ï¼Œä»è€Œæé«˜æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹çš„é£æ ¼è½¬ç§»æ€§èƒ½ã€‚æœ¬æ–‡åœ¨ç†è®ºå’Œå®è·µä¸Šå‡éªŒè¯äº†è¿™ä¸€å‘ç°çš„æœ‰æ•ˆæ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ–‡æœ¬åˆ°å›¾åƒçš„æ‰©æ•£æ¨¡å‹èƒ½å¤Ÿç»“åˆæ–‡æœ¬æç¤ºå’Œå‚è€ƒå›¾åƒçš„é£æ ¼ç”Ÿæˆå›¾åƒã€‚</li>
<li>å½“å‰æ–¹æ³•é¢ä¸´ä»é£æ ¼å‚è€ƒå›¾åƒä¸­åˆ†ç¦»å†…å®¹å’Œé£æ ¼çš„æŒ‘æˆ˜ï¼Œå¯¼è‡´å†…å®¹æ³„æ¼ã€‚</li>
<li>æå‡ºäº†ä¸€ç§åŸºäºæ©è†œçš„æ–¹æ³•ï¼Œæ— éœ€è°ƒæ•´æ¨¡å‹å‚æ•°å³å¯åˆ†ç¦»å†…å®¹å’Œé£æ ¼ã€‚</li>
<li>é€šè¿‡æ©ç›–é£æ ¼å‚è€ƒå›¾åƒç‰¹å¾ä¸­çš„ç‰¹å®šå…ƒç´ ï¼Œå‘ç°äº†å…³é”®åŸç†ã€‚</li>
<li>é€‚å½“é€‰æ‹©è¾ƒå°‘çš„æ¡ä»¶è¿›è¡Œå¼•å¯¼å¯ä»¥æé«˜æ‰©æ•£æ¨¡å‹çš„é£æ ¼è½¬ç§»æ€§èƒ½ã€‚</li>
<li>å¹¿æ³›å®éªŒè¯æ˜è¯¥æ–¹æ³•å¯¹å„ç§é£æ ¼éƒ½æœ‰æ•ˆã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.07466">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-f6d00f425600d2c6471f31aaab104271.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-708caa28198b8a2c509a916912d0e1f3.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-15f47e4830da25b3f29e0a760ebfb724.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="Semantic-to-Structure-Learning-Structural-Representations-for-Infringement-Detection"><a href="#Semantic-to-Structure-Learning-Structural-Representations-for-Infringement-Detection" class="headerlink" title="Semantic to Structure: Learning Structural Representations for   Infringement Detection"></a>Semantic to Structure: Learning Structural Representations for   Infringement Detection</h2><p><strong>Authors:Chuanwei Huang, Zexi Jia, Hongyan Fei, Yeshuang Zhu, Zhiqiang Yuan, Jinchao Zhang, Jie Zhou</strong></p>
<p>Structural information in images is crucial for aesthetic assessment, and it is widely recognized in the artistic field that imitating the structure of other works significantly infringes on creatorsâ€™ rights. The advancement of diffusion models has led to AI-generated content imitating artistsâ€™ structural creations, yet effective detection methods are still lacking. In this paper, we define this phenomenon as â€œstructural infringementâ€ and propose a corresponding detection method. Additionally, we develop quantitative metrics and create manually annotated datasets for evaluation: the SIA dataset of synthesized data, and the SIR dataset of real data. Due to the current lack of datasets for structural infringement detection, we propose a new data synthesis strategy based on diffusion models and LLM, successfully training a structural infringement detection model. Experimental results show that our method can successfully detect structural infringements and achieve notable improvements on annotated test sets. </p>
<blockquote>
<p>å›¾åƒä¸­çš„ç»“æ„ä¿¡æ¯å¯¹äºç¾å­¦è¯„ä¼°è‡³å…³é‡è¦ï¼Œåœ¨è‰ºæœ¯é¢†åŸŸæ™®éè®¤è¯†åˆ°ï¼Œæ¨¡ä»¿å…¶ä»–ä½œå“çš„ç»“æ„ä¼šä¸¥é‡ä¾µçŠ¯åˆ›ä½œè€…çš„æƒç›Šã€‚æ‰©æ•£æ¨¡å‹çš„è¿›æ­¥å¯¼è‡´äº†AIç”Ÿæˆçš„å†…å®¹æ¨¡ä»¿è‰ºæœ¯å®¶çš„ç»“æ„åˆ›ä½œï¼Œç„¶è€Œæœ‰æ•ˆçš„æ£€æµ‹æ–¹æ³•ä»ç„¶ç¼ºä¹ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬å°†è¿™ç§ç°è±¡å®šä¹‰ä¸ºâ€œç»“æ„ä¾µæƒâ€ï¼Œå¹¶æå‡ºäº†ä¸€ç§ç›¸åº”çš„æ£€æµ‹æ–¹æ³•ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å¼€å‘äº†å®šé‡æŒ‡æ ‡ï¼Œå¹¶åˆ›å»ºäº†ç”¨äºè¯„ä¼°çš„æ‰‹åŠ¨æ³¨é‡Šæ•°æ®é›†ï¼šåˆæˆæ•°æ®çš„SIAæ•°æ®é›†å’ŒçœŸå®æ•°æ®çš„SIRæ•°æ®é›†ã€‚ç”±äºç›®å‰ç¼ºä¹ç”¨äºæ£€æµ‹ç»“æ„ä¾µæƒçš„æ•°æ®é›†ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºæ‰©æ•£æ¨¡å‹å’ŒLLMçš„æ–°æ•°æ®åˆæˆç­–ç•¥ï¼ŒæˆåŠŸè®­ç»ƒäº†ä¸€ä¸ªç»“æ„ä¾µæƒæ£€æµ‹æ¨¡å‹ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•å¯ä»¥æˆåŠŸæ£€æµ‹ç»“æ„ä¾µæƒï¼Œå¹¶åœ¨æ³¨é‡Šæµ‹è¯•é›†ä¸Šå–å¾—äº†æ˜¾è‘—çš„æ”¹è¿›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.07323v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å›¾åƒä¸­çš„ç»“æ„ä¿¡æ¯å¯¹äºå®¡ç¾è¯„ä¼°è‡³å…³é‡è¦ï¼Œæ¨¡ä»¿ä»–äººä½œå“çš„ç»“æ„ä¾µçŠ¯äº†åˆ›ä½œè€…çš„æƒç›Šã€‚éšç€æ‰©æ•£æ¨¡å‹çš„è¿›æ­¥ï¼ŒAIç”Ÿæˆçš„å†…å®¹ä¹Ÿå¼€å§‹æ¨¡ä»¿è‰ºæœ¯å®¶çš„ç»“æ„åˆ›ä½œã€‚æœ¬æ–‡å®šä¹‰äº†è¿™ç§ç°è±¡ä¸ºâ€œç»“æ„æ€§ä¾µæƒâ€ï¼Œå¹¶æå‡ºäº†ä¸€ç§ç›¸åº”çš„æ£€æµ‹æ–¹æ³•ã€‚åŒæ—¶ï¼Œæˆ‘ä»¬å¼€å‘äº†å®šé‡æŒ‡æ ‡ï¼Œå¹¶åˆ›å»ºäº†æ‰‹åŠ¨æ ‡æ³¨çš„æ•°æ®é›†è¿›è¡Œè¯„ä¼°ï¼šåˆæˆæ•°æ®çš„SIAæ•°æ®é›†å’ŒçœŸå®æ•°æ®çš„SIRæ•°æ®é›†ã€‚ç”±äºå½“å‰ç¼ºä¹ç”¨äºæ£€æµ‹ç»“æ„æ€§ä¾µæƒçš„æ•°æ®é›†ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„åŸºäºæ‰©æ•£æ¨¡å‹å’ŒLLMçš„æ•°æ®åˆæˆç­–ç•¥ï¼ŒæˆåŠŸè®­ç»ƒäº†ä¸€ä¸ªç»“æ„æ€§ä¾µæƒæ£€æµ‹æ¨¡å‹ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•èƒ½å¤ŸæˆåŠŸæ£€æµ‹ç»“æ„æ€§ä¾µæƒå¹¶åœ¨æ ‡æ³¨æµ‹è¯•é›†ä¸Šå–å¾—äº†æ˜¾è‘—çš„æ”¹è¿›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç»“æ„æ€§ä¿¡æ¯åœ¨å›¾åƒå®¡ç¾è¯„ä¼°ä¸­èµ·å…³é”®ä½œç”¨ï¼Œä¸”æ¨¡ä»¿è‰ºæœ¯å®¶ä½œå“ç»“æ„ä¾µçŠ¯äº†å…¶æƒç›Šã€‚</li>
<li>æ‰©æ•£æ¨¡å‹çš„è¿›æ­¥ä½¿å¾—AIç”Ÿæˆå†…å®¹å¼€å§‹æ¨¡ä»¿è‰ºæœ¯ç»“æ„ã€‚</li>
<li>é¦–æ¬¡æå‡ºâ€œç»“æ„æ€§ä¾µæƒâ€ç°è±¡ï¼Œå¹¶å€¡å¯¼åˆ¶å®šç›¸åº”çš„æ£€æµ‹æ–¹æ³•å’Œè¯„ä¼°æŒ‡æ ‡ã€‚</li>
<li>å¼€å‘äº†å®šé‡æŒ‡æ ‡æ¥è¯„ä¼°ç»“æ„æ€§ä¾µæƒç¨‹åº¦ã€‚</li>
<li>åˆ›å»ºäº†SIAåˆæˆæ•°æ®é›†å’ŒSIRçœŸå®æ•°æ®é›†ç”¨äºè¯„ä¼°ç»“æ„æ€§ä¾µæƒæ£€æµ‹æ¨¡å‹ã€‚</li>
<li>æå‡ºåŸºäºæ‰©æ•£æ¨¡å‹å’ŒLLMçš„æ–°æ•°æ®åˆæˆç­–ç•¥ä»¥è®­ç»ƒæ£€æµ‹æ¨¡å‹ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.07323">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-d44c7da24007913d5140da0ade663ce9.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-5699000110e2f0be3ae8b37e48f26e91.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-722cbba71d011577bf72987f41060f95.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ed4e46a108e2ff66afe8f284e9703190.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-5932a5df6568e0e3e8e7a7b66e117251.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-7ecbf7c23c364abe3047405bc2326f88.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="One-Diffusion-Step-to-Real-World-Super-Resolution-via-Flow-Trajectory-Distillation"><a href="#One-Diffusion-Step-to-Real-World-Super-Resolution-via-Flow-Trajectory-Distillation" class="headerlink" title="One Diffusion Step to Real-World Super-Resolution via Flow Trajectory   Distillation"></a>One Diffusion Step to Real-World Super-Resolution via Flow Trajectory   Distillation</h2><p><strong>Authors:Jianze Li, Jiezhang Cao, Yong Guo, Wenbo Li, Yulun Zhang</strong></p>
<p>Diffusion models (DMs) have significantly advanced the development of real-world image super-resolution (Real-ISR), but the computational cost of multi-step diffusion models limits their application. One-step diffusion models generate high-quality images in a one sampling step, greatly reducing computational overhead and inference latency. However, most existing one-step diffusion methods are constrained by the performance of the teacher model, where poor teacher performance results in image artifacts. To address this limitation, we propose FluxSR, a novel one-step diffusion Real-ISR technique based on flow matching models. We use the state-of-the-art diffusion model FLUX.1-dev as both the teacher model and the base model. First, we introduce Flow Trajectory Distillation (FTD) to distill a multi-step flow matching model into a one-step Real-ISR. Second, to improve image realism and address high-frequency artifact issues in generated images, we propose TV-LPIPS as a perceptual loss and introduce Attention Diversification Loss (ADL) as a regularization term to reduce token similarity in transformer, thereby eliminating high-frequency artifacts. Comprehensive experiments demonstrate that our method outperforms existing one-step diffusion-based Real-ISR methods. The code and model will be released at <a target="_blank" rel="noopener" href="https://github.com/JianzeLi-114/FluxSR">https://github.com/JianzeLi-114/FluxSR</a>. </p>
<blockquote>
<p>æ‰©æ•£æ¨¡å‹ï¼ˆDMsï¼‰å·²ç»æå¤§åœ°æ¨åŠ¨äº†çœŸå®ä¸–ç•Œå›¾åƒè¶…åˆ†è¾¨ç‡ï¼ˆReal-ISRï¼‰çš„å‘å±•ï¼Œä½†å¤šæ­¥æ‰©æ•£æ¨¡å‹çš„é«˜è®¡ç®—æˆæœ¬é™åˆ¶äº†å…¶åº”ç”¨ã€‚ä¸€æ­¥æ‰©æ•£æ¨¡å‹åœ¨ä¸€æ­¥é‡‡æ ·è¿‡ç¨‹ä¸­ç”Ÿæˆé«˜è´¨é‡å›¾åƒï¼Œå¤§å¤§é™ä½äº†è®¡ç®—å¼€é”€å’Œæ¨ç†å»¶è¿Ÿã€‚ç„¶è€Œï¼Œå¤§å¤šæ•°ç°æœ‰çš„ä¸€æ­¥æ‰©æ•£æ–¹æ³•å—åˆ°æ•™å¸ˆæ¨¡å‹æ€§èƒ½çš„åˆ¶çº¦ï¼Œæ•™å¸ˆæ¨¡å‹æ€§èƒ½ä¸ä½³ä¼šå¯¼è‡´å›¾åƒå‡ºç°ä¼ªå½±ã€‚ä¸ºäº†è§£å†³è¿™ä¸€å±€é™æ€§ï¼Œæˆ‘ä»¬æå‡ºäº†FluxSRï¼Œè¿™æ˜¯ä¸€ç§åŸºäºæµåŒ¹é…æ¨¡å‹çš„æ–°å‹ä¸€æ­¥æ‰©æ•£Real-ISRæŠ€æœ¯ã€‚æˆ‘ä»¬ä½¿ç”¨æœ€å…ˆè¿›çš„æ‰©æ•£æ¨¡å‹FLUX.1-devåŒæ—¶ä½œä¸ºæ•™å¸ˆæ¨¡å‹å’ŒåŸºç¡€æ¨¡å‹ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬å¼•å…¥æµè½¨è¿¹è’¸é¦ï¼ˆFTDï¼‰æŠ€æœ¯ï¼Œå°†å¤šæ­¥æµåŒ¹é…æ¨¡å‹è½¬åŒ–ä¸ºä¸€æ­¥Real-ISRã€‚å…¶æ¬¡ï¼Œä¸ºäº†æé«˜å›¾åƒçš„çœŸå®æ€§å’Œè§£å†³ç”Ÿæˆå›¾åƒä¸­çš„é«˜é¢‘ä¼ªå½±é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä»¥TV-LPIPSä½œä¸ºæ„ŸçŸ¥æŸå¤±ï¼Œå¹¶å¼•å…¥æ³¨æ„åŠ›å¤šæ ·åŒ–æŸå¤±ï¼ˆADLï¼‰ä½œä¸ºæ­£åˆ™åŒ–é¡¹ï¼Œä»¥å‡å°‘transformerä¸­çš„ä»¤ç‰Œç›¸ä¼¼æ€§ï¼Œä»è€Œæ¶ˆé™¤é«˜é¢‘ä¼ªå½±ã€‚ç»¼åˆå®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•ä¼˜äºç°æœ‰çš„åŸºäºä¸€æ­¥æ‰©æ•£çš„Real-ISRæ–¹æ³•ã€‚ä»£ç å’Œæ¨¡å‹å°†åœ¨<a target="_blank" rel="noopener" href="https://github.com/JianzeLi-114/FluxSR%E5%8F%91%E5%B8%83%E3%80%82">https://github.com/JianzeLi-114/FluxSRå‘å¸ƒã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.01993v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºæµåŒ¹é…æ¨¡å‹çš„æ–°å‹ä¸€æ­¥æ‰©æ•£çœŸå®å›¾åƒè¶…åˆ†è¾¨ç‡æŠ€æœ¯FluxSRã€‚å®ƒå¼•å…¥äº†æµè½¨è¿¹è’¸é¦ï¼ˆFTDï¼‰å°†å¤šæ­¥æµåŒ¹é…æ¨¡å‹ç®€åŒ–ä¸ºä¸€æ­¥çœŸå®å›¾åƒè¶…åˆ†è¾¨ç‡ã€‚ä¸ºæé«˜å›¾åƒçœŸå®æ€§å’Œè§£å†³ç”Ÿæˆå›¾åƒçš„é«˜é¢‘ä¼ªå½±é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†åŸºäºTV-LPIPSçš„æ„ŸçŸ¥æŸå¤±å’Œæ³¨æ„åŠ›å¤šæ ·åŒ–æŸå¤±ï¼ˆADLï¼‰æ­£åˆ™åŒ–é¡¹ã€‚å®éªŒè¯æ˜ï¼Œè¯¥æ–¹æ³•ä¼˜äºç°æœ‰çš„ä¸€æ­¥æ‰©æ•£çœŸå®å›¾åƒè¶…åˆ†è¾¨ç‡æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ‰©æ•£æ¨¡å‹ï¼ˆDMsï¼‰åœ¨çœŸå®å›¾åƒè¶…åˆ†è¾¨ç‡ï¼ˆReal-ISRï¼‰é¢†åŸŸæœ‰é‡å¤§å‘å±•ï¼Œä½†å¤šæ­¥æ‰©æ•£æ¨¡å‹è®¡ç®—æˆæœ¬é«˜ï¼Œé™åˆ¶äº†å…¶åº”ç”¨ã€‚</li>
<li>ä¸€æ­¥æ‰©æ•£æ¨¡å‹èƒ½åœ¨ä¸€é‡‡æ ·æ­¥éª¤ä¸­ç”Ÿæˆé«˜è´¨é‡å›¾åƒï¼Œå¤§å¹…é™ä½è®¡ç®—å¼€é”€å’Œæ¨ç†å»¶è¿Ÿã€‚</li>
<li>ç°æœ‰ä¸€æ­¥æ‰©æ•£æ–¹æ³•å—æ•™å¸ˆæ¨¡å‹æ€§èƒ½é™åˆ¶ï¼Œæ•™å¸ˆæ€§èƒ½å·®ä¼šå¯¼è‡´å›¾åƒä¼ªå½±ã€‚</li>
<li>æå‡ºçš„FluxSRæ˜¯ä¸€ç§æ–°å‹ä¸€æ­¥æ‰©æ•£Real-ISRæŠ€æœ¯ï¼ŒåŸºäºæµåŒ¹é…æ¨¡å‹ã€‚</li>
<li>FluxSRä½¿ç”¨FTDå°†å¤šæ­¥æµåŒ¹é…æ¨¡å‹ç®€åŒ–ä¸ºä¸€æ­¥Real-ISRã€‚</li>
<li>ä¸ºæé«˜å›¾åƒçœŸå®æ€§å’Œè§£å†³é«˜é¢‘ä¼ªå½±ï¼ŒFluxSRå¼•å…¥äº†åŸºäºTV-LPIPSçš„æ„ŸçŸ¥æŸå¤±å’ŒADLæ­£åˆ™åŒ–é¡¹ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.01993">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-1591154387c92afc41c062e212e659a8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-15cbe48ad39d695218fa74d2aa5934e5.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ab07c2d4905d5e12fa9cff99ccd10140.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d7dd3f2c76169ea257d47167904cd648.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="DGQ-Distribution-Aware-Group-Quantization-for-Text-to-Image-Diffusion-Models"><a href="#DGQ-Distribution-Aware-Group-Quantization-for-Text-to-Image-Diffusion-Models" class="headerlink" title="DGQ: Distribution-Aware Group Quantization for Text-to-Image Diffusion   Models"></a>DGQ: Distribution-Aware Group Quantization for Text-to-Image Diffusion   Models</h2><p><strong>Authors:Hyogon Ryu, NaHyeon Park, Hyunjung Shim</strong></p>
<p>Despite the widespread use of text-to-image diffusion models across various tasks, their computational and memory demands limit practical applications. To mitigate this issue, quantization of diffusion models has been explored. It reduces memory usage and computational costs by compressing weights and activations into lower-bit formats. However, existing methods often struggle to preserve both image quality and text-image alignment, particularly in lower-bit($&lt;$ 8bits) quantization. In this paper, we analyze the challenges associated with quantizing text-to-image diffusion models from a distributional perspective. Our analysis reveals that activation outliers play a crucial role in determining image quality. Additionally, we identify distinctive patterns in cross-attention scores, which significantly affects text-image alignment. To address these challenges, we propose Distribution-aware Group Quantization (DGQ), a method that identifies and adaptively handles pixel-wise and channel-wise outliers to preserve image quality. Furthermore, DGQ applies prompt-specific logarithmic quantization scales to maintain text-image alignment. Our method demonstrates remarkable performance on datasets such as MS-COCO and PartiPrompts. We are the first to successfully achieve low-bit quantization of text-to-image diffusion models without requiring additional fine-tuning of weight quantization parameters. Code is available at <a target="_blank" rel="noopener" href="https://github.com/ugonfor/DGQ">https://github.com/ugonfor/DGQ</a>. </p>
<blockquote>
<p>å°½ç®¡æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹åœ¨å„ç§ä»»åŠ¡ä¸­å¾—åˆ°äº†å¹¿æ³›åº”ç”¨ï¼Œä½†å…¶è®¡ç®—å’Œå†…å­˜éœ€æ±‚é™åˆ¶äº†å®é™…åº”ç”¨ã€‚ä¸ºäº†ç¼“è§£è¿™ä¸ªé—®é¢˜ï¼Œå·²ç»æ¢ç´¢äº†æ‰©æ•£æ¨¡å‹çš„é‡åŒ–ã€‚å®ƒé€šè¿‡å‹ç¼©æƒé‡å’Œæ¿€æ´»å€¼åˆ°ä½ä½æ ¼å¼æ¥å‡å°‘å†…å­˜ä½¿ç”¨å’Œè®¡ç®—æˆæœ¬ã€‚ç„¶è€Œï¼Œç°æœ‰æ–¹æ³•å¾€å¾€éš¾ä»¥åœ¨ä¿æŒå›¾åƒè´¨é‡å’Œæ–‡æœ¬-å›¾åƒå¯¹é½æ–¹é¢å–å¾—å¹³è¡¡ï¼Œç‰¹åˆ«æ˜¯åœ¨ä½ä½ï¼ˆ&lt;8ä½ï¼‰é‡åŒ–ä¸­ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬ä»åˆ†å¸ƒçš„è§’åº¦åˆ†æäº†æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹é‡åŒ–çš„æŒ‘æˆ˜ã€‚æˆ‘ä»¬çš„åˆ†æè¡¨æ˜ï¼Œæ¿€æ´»å¼‚å¸¸å€¼åœ¨å†³å®šå›¾åƒè´¨é‡æ–¹é¢èµ·ç€è‡³å…³é‡è¦çš„ä½œç”¨ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜å‘ç°äº†è·¨æ³¨æ„åŠ›åˆ†æ•°çš„ç‹¬ç‰¹æ¨¡å¼ï¼Œè¿™æ˜¾è‘—å½±å“äº†æ–‡æœ¬-å›¾åƒå¯¹é½ã€‚ä¸ºäº†è§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†åˆ†å¸ƒæ„ŸçŸ¥ç»„é‡åŒ–ï¼ˆDGQï¼‰æ–¹æ³•ï¼Œè¯¥æ–¹æ³•èƒ½å¤Ÿè¯†åˆ«å’Œè‡ªé€‚åº”å¤„ç†åƒç´ çº§å’Œé€šé“çº§çš„å¼‚å¸¸å€¼ï¼Œä»¥ä¿æŒå›¾åƒè´¨é‡ã€‚æ­¤å¤–ï¼ŒDGQè¿˜é‡‡ç”¨æç¤ºç‰¹å®šçš„å¯¹æ•°é‡åŒ–å°ºåº¦æ¥ä¿æŒæ–‡æœ¬-å›¾åƒå¯¹é½ã€‚æˆ‘ä»¬çš„æ–¹æ³•åœ¨MS-COCOå’ŒPartiPromptsç­‰æ•°æ®é›†ä¸Šè¡¨ç°å‡ºå“è¶Šçš„æ€§èƒ½ã€‚æˆ‘ä»¬æ˜¯é¦–æ¬¡æˆåŠŸå®ç°æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹çš„ä½ä½é‡åŒ–ï¼Œè€Œæ— éœ€å¯¹æƒé‡é‡åŒ–å‚æ•°è¿›è¡Œé¢å¤–çš„å¾®è°ƒã€‚ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/ugonfor/DGQ%E4%B8%8A%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/ugonfor/DGQä¸Šæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.04304v2">PDF</a> Accepted ICLR 2025. Project page: <a target="_blank" rel="noopener" href="https://ugonfor.kr/DGQ">https://ugonfor.kr/DGQ</a></p>
<p><strong>Summary</strong><br>     æ–‡æœ¬ä»‹ç»äº†æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹çš„å¹¿æ³›åº”ç”¨åŠå…¶è®¡ç®—ä¸å†…å­˜éœ€æ±‚çš„é—®é¢˜ã€‚ä¸ºç¼“è§£è¿™ä¸€é—®é¢˜ï¼Œç ”ç©¶äººå‘˜å°è¯•å¯¹æ‰©æ•£æ¨¡å‹è¿›è¡Œé‡åŒ–ï¼Œé€šè¿‡å‹ç¼©æƒé‡å’Œæ¿€æ´»å€¼æ¥é™ä½å†…å­˜ä½¿ç”¨å’Œè®¡ç®—æˆæœ¬ã€‚ä½†ç°æœ‰æ–¹æ³•éš¾ä»¥åœ¨è¾ƒä½ä½ï¼ˆ&lt;8ä½ï¼‰é‡åŒ–ä¸­ä¿æŒå›¾åƒè´¨é‡å’Œæ–‡æœ¬-å›¾åƒå¯¹é½ã€‚æœ¬æ–‡åˆ†ææŒ‡å‡ºæ¿€æ´»å€¼å¼‚å¸¸å¯¹å›¾åƒè´¨é‡è‡³å…³é‡è¦ï¼Œå¹¶å‘ç°è·¨æ³¨æ„åŠ›åˆ†æ•°çš„ç‹¬ç‰¹æ¨¡å¼å½±å“æ–‡æœ¬-å›¾åƒå¯¹é½ã€‚ä¸ºè§£å†³è¿™äº›é—®é¢˜ï¼Œæå‡ºäº†åˆ†å¸ƒæ„ŸçŸ¥ç»„é‡åŒ–ï¼ˆDGQï¼‰æ–¹æ³•ï¼Œå¯è¯†åˆ«å’Œè‡ªé€‚åº”å¤„ç†åƒç´ çº§å’Œé€šé“çº§çš„å¼‚å¸¸å€¼ä»¥ä¿æŒå›¾åƒè´¨é‡ï¼Œå¹¶åº”ç”¨æç¤ºç‰¹å®šçš„å¯¹æ•°é‡åŒ–å°ºåº¦æ¥ç»´æŒæ–‡æœ¬-å›¾åƒå¯¹é½ã€‚DGQåœ¨MS-COCOå’ŒPartiPromptsç­‰æ•°æ®é›†ä¸Šè¡¨ç°å‡ºå“è¶Šæ€§èƒ½ï¼Œé¦–æ¬¡å®ç°äº†æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹çš„ä½ä½é‡åŒ–ï¼Œæ— éœ€å¯¹æƒé‡é‡åŒ–å‚æ•°è¿›è¡Œé¢å¤–å¾®è°ƒã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹å¹¿æ³›åº”ç”¨äºå„ç§ä»»åŠ¡ï¼Œä½†è®¡ç®—ä¸å†…å­˜éœ€æ±‚é™åˆ¶äº†å®é™…åº”ç”¨ã€‚</li>
<li>é‡åŒ–æ˜¯è§£å†³è¿™ä¸€é—®é¢˜çš„æœ‰æ•ˆæ–¹æ³•ï¼Œé€šè¿‡å‹ç¼©æƒé‡å’Œæ¿€æ´»å€¼æ¥é™ä½å†…å­˜ä½¿ç”¨å’Œè®¡ç®—æˆæœ¬ã€‚</li>
<li>åœ¨è¾ƒä½ä½é‡åŒ–ä¸­ï¼Œä¿æŒå›¾åƒè´¨é‡å’Œæ–‡æœ¬-å›¾åƒå¯¹é½æ˜¯ä¸€å¤§æŒ‘æˆ˜ã€‚</li>
<li>æ¿€æ´»å€¼å¼‚å¸¸å¯¹å›¾åƒè´¨é‡è‡³å…³é‡è¦ï¼Œè€Œè·¨æ³¨æ„åŠ›åˆ†æ•°çš„ç‹¬ç‰¹æ¨¡å¼å½±å“æ–‡æœ¬-å›¾åƒå¯¹é½ã€‚</li>
<li>æå‡ºäº†åˆ†å¸ƒæ„ŸçŸ¥ç»„é‡åŒ–ï¼ˆDGQï¼‰æ–¹æ³•æ¥è§£å†³è¿™äº›æŒ‘æˆ˜ã€‚</li>
<li>DGQé€šè¿‡è¯†åˆ«å’Œè‡ªé€‚åº”å¤„ç†åƒç´ çº§å’Œé€šé“çº§çš„å¼‚å¸¸å€¼ä¿æŒå›¾åƒè´¨é‡ã€‚</li>
<li>DGQæˆåŠŸå®ç°äº†æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹çš„ä½ä½é‡åŒ–ï¼Œæ€§èƒ½å“è¶Šï¼Œæ— éœ€é¢å¤–å¾®è°ƒæƒé‡é‡åŒ–å‚æ•°ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.04304">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-79f394b42f453bfd1f8228a8d5cf947d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d1d507e0baf9a5fb8f560518aa02abc8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-30859f22dd561c297d591b64a7843f7d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-799ee0d0d25e894e7fb2fbc6ef319358.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-2c1f3dfbece5bf8afb452828b13b8ea8.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="Generative-Inbetweening-Adapting-Image-to-Video-Models-for-Keyframe-Interpolation"><a href="#Generative-Inbetweening-Adapting-Image-to-Video-Models-for-Keyframe-Interpolation" class="headerlink" title="Generative Inbetweening: Adapting Image-to-Video Models for Keyframe   Interpolation"></a>Generative Inbetweening: Adapting Image-to-Video Models for Keyframe   Interpolation</h2><p><strong>Authors:Xiaojuan Wang, Boyang Zhou, Brian Curless, Ira Kemelmacher-Shlizerman, Aleksander Holynski, Steven M. Seitz</strong></p>
<p>We present a method for generating video sequences with coherent motion between a pair of input key frames. We adapt a pretrained large-scale image-to-video diffusion model (originally trained to generate videos moving forward in time from a single input image) for key frame interpolation, i.e., to produce a video in between two input frames. We accomplish this adaptation through a lightweight fine-tuning technique that produces a version of the model that instead predicts videos moving backwards in time from a single input image. This model (along with the original forward-moving model) is subsequently used in a dual-directional diffusion sampling process that combines the overlapping model estimates starting from each of the two keyframes. Our experiments show that our method outperforms both existing diffusion-based methods and traditional frame interpolation techniques. </p>
<blockquote>
<p>æˆ‘ä»¬æå‡ºäº†ä¸€ç§ç”Ÿæˆå…·æœ‰è¿è´¯è¿åŠ¨è§†é¢‘åºåˆ—çš„æ–¹æ³•ï¼Œè¯¥è§†é¢‘åºåˆ—ä½äºä¸€å¯¹è¾“å…¥å…³é”®å¸§ä¹‹é—´ã€‚æˆ‘ä»¬é‡‡ç”¨é¢„è®­ç»ƒçš„å¤§è§„æ¨¡å›¾åƒåˆ°è§†é¢‘çš„æ‰©æ•£æ¨¡å‹ï¼ˆæœ€åˆç”¨äºä»å•ä¸ªè¾“å…¥å›¾åƒç”Ÿæˆå‘å‰æ¨è¿›çš„è§†é¢‘ï¼‰è¿›è¡Œå…³é”®å¸§æ’å€¼ï¼Œå³ç”Ÿæˆä¸¤ä¸ªè¾“å…¥å¸§ä¹‹é—´çš„è§†é¢‘ã€‚æˆ‘ä»¬é€šè¿‡ä¸€ç§è½»é‡çº§çš„å¾®è°ƒæŠ€æœ¯æ¥å®Œæˆè¿™ç§é€‚åº”ï¼Œè¯¥æŠ€æœ¯äº§ç”Ÿäº†ä¸€ç§ç‰ˆæœ¬æ¨¡å‹ï¼Œè¯¥æ¨¡å‹ä»å•ä¸ªè¾“å…¥å›¾åƒé¢„æµ‹åå‘ç§»åŠ¨çš„è§†é¢‘ã€‚æ­¤æ¨¡å‹ï¼ˆä»¥åŠåŸå§‹çš„å‘å‰ç§»åŠ¨æ¨¡å‹ï¼‰éšåè¢«ç”¨äºåŒå‘æ‰©æ•£é‡‡æ ·è¿‡ç¨‹ï¼Œè¯¥è¿‡ç¨‹ç»“åˆäº†ä»ä¸¤ä¸ªå…³é”®å¸§å¼€å§‹çš„é‡å æ¨¡å‹ä¼°è®¡ã€‚æˆ‘ä»¬çš„å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•ä¼˜äºç°æœ‰çš„åŸºäºæ‰©æ•£çš„æ–¹æ³•å’Œä¼ ç»Ÿçš„å¸§æ’å€¼æŠ€æœ¯ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2408.15239v2">PDF</a> Published at ICLR 2025; Project page:   <a target="_blank" rel="noopener" href="https://svd-keyframe-interpolation.github.io/">https://svd-keyframe-interpolation.github.io/</a></p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–¹æ³•é€šè¿‡ä½¿ç”¨é¢„è®­ç»ƒçš„å›¾åƒåˆ°è§†é¢‘çš„æ‰©æ•£æ¨¡å‹ï¼Œå¯¹ä¸¤ä¸ªè¾“å…¥å…³é”®å¸§ä¹‹é—´çš„è§†é¢‘åºåˆ—è¿›è¡Œç”Ÿæˆï¼Œå®ç°äº†å…³é”®å¸§æ’å€¼ã€‚é€šè¿‡è½»é‡çº§å¾®è°ƒæŠ€æœ¯ï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿé¢„æµ‹ä»å•ä¸€è¾“å…¥å›¾åƒå¼€å§‹å‘åç§»åŠ¨çš„è§†é¢‘ã€‚ç»“åˆåŸå§‹å‘å‰ç§»åŠ¨çš„æ¨¡å‹ï¼Œç”¨äºåŒå‘æ‰©æ•£é‡‡æ ·è¿‡ç¨‹ï¼Œç»“åˆä¸¤ä¸ªå…³é”®å¸§å¼€å§‹çš„æ¨¡å‹ä¼°è®¡ã€‚å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•ä¼˜äºç°æœ‰çš„æ‰©æ•£æ–¹æ³•å’Œä¼ ç»Ÿçš„å¸§æ’å€¼æŠ€æœ¯ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ä½¿ç”¨äº†é¢„è®­ç»ƒçš„å›¾åƒåˆ°è§†é¢‘çš„æ‰©æ•£æ¨¡å‹è¿›è¡Œå…³é”®å¸§æ’å€¼ã€‚</li>
<li>é€šè¿‡è½»é‡çº§å¾®è°ƒæŠ€æœ¯ï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿé¢„æµ‹ä»å•ä¸€è¾“å…¥å›¾åƒå¼€å§‹å‘åç§»åŠ¨çš„è§†é¢‘ã€‚</li>
<li>ç»“åˆäº†åŸå§‹å‘å‰ç§»åŠ¨çš„æ¨¡å‹å’Œå‘åç§»åŠ¨çš„æ¨¡å‹ï¼Œè¿›è¡ŒåŒå‘æ‰©æ•£é‡‡æ ·ã€‚</li>
<li>æ¨¡å‹ä¼°è®¡ç»“åˆäº†æ¥è‡ªä¸¤ä¸ªå…³é”®å¸§çš„èµ·å§‹ç‚¹ã€‚</li>
<li>è¯¥æ–¹æ³•èƒ½å¤Ÿåœ¨ä¸¤ä¸ªè¾“å…¥å…³é”®å¸§ä¹‹é—´ç”Ÿæˆå…·æœ‰è¿è´¯è¿åŠ¨çš„è§†é¢‘åºåˆ—ã€‚</li>
<li>å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•ä¼˜äºç°æœ‰çš„æ‰©æ•£æ–¹æ³•å’Œä¼ ç»Ÿçš„å¸§æ’å€¼æŠ€æœ¯ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2408.15239">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-1ae7ff51627f97fbd1e70805f03f8093.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-366df47c862e90a088cd1aaee6395917.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-27b99841ea3f012a3251358c685a7ea6.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="Efficient-Image-to-Image-Diffusion-Classifier-for-Adversarial-Robustness"><a href="#Efficient-Image-to-Image-Diffusion-Classifier-for-Adversarial-Robustness" class="headerlink" title="Efficient Image-to-Image Diffusion Classifier for Adversarial Robustness"></a>Efficient Image-to-Image Diffusion Classifier for Adversarial Robustness</h2><p><strong>Authors:Hefei Mei, Minjing Dong, Chang Xu</strong></p>
<p>Diffusion models (DMs) have demonstrated great potential in the field of adversarial robustness, where DM-based defense methods can achieve superior defense capability without adversarial training. However, they all require huge computational costs due to the usage of large-scale pre-trained DMs, making it difficult to conduct full evaluation under strong attacks and compare with traditional CNN-based methods. Simply reducing the network size and timesteps in DMs could significantly harm the image generation quality, which invalidates previous frameworks. To alleviate this issue, we redesign the diffusion framework from generating high-quality images to predicting distinguishable image labels. Specifically, we employ an image translation framework to learn many-to-one mapping from input samples to designed orthogonal image labels. Based on this framework, we introduce an efficient Image-to-Image diffusion classifier with a pruned U-Net structure and reduced diffusion timesteps. Besides the framework, we redesign the optimization objective of DMs to fit the target of image classification, where a new classification loss is incorporated in the DM-based image translation framework to distinguish the generated label from those of other classes. We conduct sufficient evaluations of the proposed classifier under various attacks on popular benchmarks. Extensive experiments show that our method achieves better adversarial robustness with fewer computational costs than DM-based and CNN-based methods. The code is available at <a target="_blank" rel="noopener" href="https://github.com/hfmei/IDC">https://github.com/hfmei/IDC</a> </p>
<blockquote>
<p>æ‰©æ•£æ¨¡å‹ï¼ˆDMsï¼‰åœ¨å¯¹æŠ—é²æ£’æ€§é¢†åŸŸè¡¨ç°å‡ºå·¨å¤§æ½œåŠ›ï¼ŒåŸºäºDMçš„é˜²å¾¡æ–¹æ³•å¯ä»¥åœ¨æ— éœ€å¯¹æŠ—è®­ç»ƒçš„æƒ…å†µä¸‹å®ç°ä¼˜è¶Šçš„é˜²å¾¡èƒ½åŠ›ã€‚ç„¶è€Œï¼Œç”±äºä½¿ç”¨äº†å¤§è§„æ¨¡é¢„è®­ç»ƒçš„DMï¼Œå®ƒä»¬éƒ½éœ€è¦å·¨å¤§çš„è®¡ç®—æˆæœ¬ï¼Œéš¾ä»¥åœ¨å¼ºæ”»å‡»ä¸‹è¿›è¡Œå…¨é¢è¯„ä¼°ï¼Œå¹¶ä¸ä¼ ç»Ÿçš„åŸºäºCNNçš„æ–¹æ³•è¿›è¡Œæ¯”è¾ƒã€‚ç®€å•ç¼©å°DMçš„ç½‘ç»œè§„æ¨¡å’Œå‡å°‘æ—¶é—´æ­¥é•¿å¯èƒ½ä¼šæ˜¾è‘—æŸå®³å›¾åƒç”Ÿæˆè´¨é‡ï¼Œä»è€Œä½¿ç°æœ‰æ¡†æ¶å¤±æ•ˆã€‚ä¸ºäº†ç¼“è§£è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬é‡æ–°è®¾è®¡äº†æ‰©æ•£æ¡†æ¶ï¼Œä»ç”Ÿæˆé«˜è´¨é‡å›¾åƒè½¬å‘é¢„æµ‹å¯åŒºåˆ†çš„å›¾åƒæ ‡ç­¾ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬é‡‡ç”¨å›¾åƒç¿»è¯‘æ¡†æ¶æ¥å­¦ä¹ ä»è¾“å…¥æ ·æœ¬åˆ°è®¾è®¡çš„æ­£äº¤å›¾åƒæ ‡ç­¾çš„å¤šä¸ªåˆ°å•ä¸ªçš„æ˜ å°„ã€‚åŸºäºæ­¤æ¡†æ¶ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§é«˜æ•ˆçš„å›¾åƒåˆ°å›¾åƒæ‰©æ•£åˆ†ç±»å™¨ï¼Œå®ƒå…·æœ‰ä¿®å‰ªçš„U-Netç»“æ„å’Œå‡å°‘çš„æ‰©æ•£æ—¶é—´æ­¥é•¿ã€‚é™¤äº†æ¡†æ¶ä¹‹å¤–ï¼Œæˆ‘ä»¬è¿˜é‡æ–°è®¾è®¡äº†DMçš„ä¼˜åŒ–ç›®æ ‡ä»¥é€‚åº”å›¾åƒåˆ†ç±»çš„ç›®æ ‡ï¼Œå…¶ä¸­æ–°çš„åˆ†ç±»æŸå¤±è¢«çº³å…¥åŸºäºDMçš„å›¾åƒç¿»è¯‘æ¡†æ¶ä¸­ï¼Œä»¥åŒºåˆ†ç”Ÿæˆçš„æ ‡ç­¾ä¸å…¶ä»–ç±»åˆ«çš„æ ‡ç­¾ã€‚æˆ‘ä»¬åœ¨æµè¡Œçš„åŸºå‡†æµ‹è¯•ä¸Šå¯¹æå‡ºçš„åˆ†ç±»å™¨è¿›è¡Œäº†å„ç§æ”»å‡»ä¸‹çš„å……åˆ†è¯„ä¼°ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨è®¡ç®—æˆæœ¬æ›´ä½çš„æƒ…å†µä¸‹å®ç°äº†æ¯”åŸºäºDMå’ŒåŸºäºCNNçš„æ–¹æ³•æ›´å¥½çš„å¯¹æŠ—é²æ£’æ€§ã€‚ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/hfmei/IDC%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/hfmei/IDCæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2408.08502v2">PDF</a> </p>
<p><strong>Summary</strong>ï¼š<br>æ‰©æ•£æ¨¡å‹ï¼ˆDMsï¼‰åœ¨å¯¹æŠ—é²æ£’æ€§é¢†åŸŸå…·æœ‰å·¨å¤§æ½œåŠ›ï¼Œæ— éœ€å¯¹æŠ—è®­ç»ƒå³å¯å®ç°é«˜çº§é˜²å¾¡èƒ½åŠ›ã€‚ä½†å› å…¶è®¡ç®—æˆæœ¬é«˜æ˜‚ï¼Œéš¾ä»¥åœ¨å¼ºæ”»å‡»ä¸‹è¿›è¡Œå…¨é¢è¯„ä¼°å¹¶ä¸ä¼ ç»ŸCNNæ–¹æ³•è¿›è¡Œæ¯”è¾ƒã€‚ç ”ç©¶å›¢é˜Ÿé€šè¿‡è®¾è®¡æ–°çš„æ‰©æ•£æ¡†æ¶æ¥è§£å†³è¿™ä¸€é—®é¢˜ï¼Œä»ç”Ÿæˆé«˜è´¨é‡å›¾åƒè½¬å˜ä¸ºé¢„æµ‹å¯è¯†åˆ«çš„å›¾åƒæ ‡ç­¾ã€‚é‡‡ç”¨å›¾åƒç¿»è¯‘æ¡†æ¶å®ç°è¾“å…¥æ ·æœ¬åˆ°è®¾è®¡çš„æ­£äº¤å›¾åƒæ ‡ç­¾çš„å¤šå¯¹ä¸€æ˜ å°„ï¼Œå¹¶å¼•å…¥é«˜æ•ˆçš„å›¾åƒåˆ°å›¾åƒæ‰©æ•£åˆ†ç±»å™¨ã€‚åŒæ—¶é‡æ–°è®¾è®¡DMçš„ä¼˜åŒ–ç›®æ ‡ä»¥é€‚åº”å›¾åƒåˆ†ç±»ä»»åŠ¡ï¼Œå¹¶åœ¨DMåŸºäºå›¾åƒç¿»è¯‘æ¡†æ¶ä¸­èå…¥æ–°çš„åˆ†ç±»æŸå¤±ä»¥åŒºåˆ†ç”Ÿæˆçš„æ ‡ç­¾ä¸å…¶ä»–ç±»åˆ«ã€‚å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å¯¹æŠ—æ”»å‡»ä¸‹å…·æœ‰æ›´å¥½çš„é²æ£’æ€§ï¼Œè®¡ç®—æˆæœ¬ä½äºDMå’ŒCNNæ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong>:</p>
<ol>
<li>æ‰©æ•£æ¨¡å‹åœ¨å¯¹æŠ—é²æ£’æ€§é¢†åŸŸæœ‰å·¨å¤§æ½œåŠ›ï¼Œä½†è®¡ç®—æˆæœ¬é«˜æ˜‚ã€‚</li>
<li>ç ”ç©¶å›¢é˜Ÿé€šè¿‡è®¾è®¡æ–°çš„æ‰©æ•£æ¡†æ¶æ¥è§£å†³è®¡ç®—æˆæœ¬é—®é¢˜ï¼Œä»å›¾åƒç”Ÿæˆè½¬å‘å›¾åƒæ ‡ç­¾é¢„æµ‹ã€‚</li>
<li>é‡‡ç”¨å›¾åƒç¿»è¯‘æ¡†æ¶å®ç°è¾“å…¥æ ·æœ¬åˆ°æ­£äº¤å›¾åƒæ ‡ç­¾çš„å¤šå¯¹ä¸€æ˜ å°„ã€‚</li>
<li>å¼•å…¥é«˜æ•ˆçš„å›¾åƒåˆ°å›¾åƒæ‰©æ•£åˆ†ç±»å™¨ï¼Œé‡‡ç”¨ä¿®å‰ªçš„U-Netç»“æ„å’Œå‡å°‘çš„æ‰©æ•£æ—¶é—´æ­¥ã€‚</li>
<li>é‡æ–°è®¾è®¡DMçš„ä¼˜åŒ–ç›®æ ‡ä»¥é€‚åº”å›¾åƒåˆ†ç±»ä»»åŠ¡ã€‚</li>
<li>åœ¨DMåŸºäºå›¾åƒç¿»è¯‘æ¡†æ¶ä¸­èå…¥æ–°çš„åˆ†ç±»æŸå¤±ä»¥åŒºåˆ†æ ‡ç­¾ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2408.08502">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-f210132023f34e878e58e0c3b9d875f5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f97af9c9769d34765cea25490ed9b2bc.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3a5ed8cbb338025ebd0b1878cf9d92ef.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b398304448e4440fb1c3d8060718bf4e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2057c61709b56dfe23d52e86ea5edeae.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="GlyphDraw2-Automatic-Generation-of-Complex-Glyph-Posters-with-Diffusion-Models-and-Large-Language-Models"><a href="#GlyphDraw2-Automatic-Generation-of-Complex-Glyph-Posters-with-Diffusion-Models-and-Large-Language-Models" class="headerlink" title="GlyphDraw2: Automatic Generation of Complex Glyph Posters with Diffusion   Models and Large Language Models"></a>GlyphDraw2: Automatic Generation of Complex Glyph Posters with Diffusion   Models and Large Language Models</h2><p><strong>Authors:Jian Ma, Yonglin Deng, Chen Chen, Nanyang Du, Haonan Lu, Zhenyu Yang</strong></p>
<p>Posters play a crucial role in marketing and advertising by enhancing visual communication and brand visibility, making significant contributions to industrial design. With the latest advancements in controllable T2I diffusion models, increasing research has focused on rendering text within synthesized images. Despite improvements in text rendering accuracy, the field of automatic poster generation remains underexplored. In this paper, we propose an automatic poster generation framework with text rendering capabilities leveraging LLMs, utilizing a triple-cross attention mechanism based on alignment learning. This framework aims to create precise poster text within a detailed contextual background. Additionally, the framework supports controllable fonts, adjustable image resolution, and the rendering of posters with descriptions and text in both English and Chinese.Furthermore, we introduce a high-resolution font dataset and a poster dataset with resolutions exceeding 1024 pixels. Our approach leverages the SDXL architecture. Extensive experiments validate our methodâ€™s capability in generating poster images with complex and contextually rich backgrounds.Codes is available at <a target="_blank" rel="noopener" href="https://github.com/OPPO-Mente-Lab/GlyphDraw2">https://github.com/OPPO-Mente-Lab/GlyphDraw2</a>. </p>
<blockquote>
<p>æµ·æŠ¥åœ¨è¥é”€å’Œå¹¿å‘Šä¸­èµ·ç€è‡³å…³é‡è¦çš„ä½œç”¨ï¼Œé€šè¿‡å¢å¼ºè§†è§‰äº¤æµå’Œå“ç‰Œå¯è§åº¦ï¼Œä¸ºå·¥ä¸šè®¾è®¡åšå‡ºäº†é‡å¤§è´¡çŒ®ã€‚éšç€æœ€æ–°çš„å¯æ§T2Iæ‰©æ•£æ¨¡å‹çš„è¿›å±•ï¼Œè¶Šæ¥è¶Šå¤šçš„ç ”ç©¶é›†ä¸­åœ¨åˆæˆå›¾åƒä¸­çš„æ–‡æœ¬æ¸²æŸ“ä¸Šã€‚å°½ç®¡æ–‡æœ¬æ¸²æŸ“ç²¾åº¦æœ‰æ‰€æé«˜ï¼Œä½†è‡ªåŠ¨æµ·æŠ¥ç”Ÿæˆé¢†åŸŸä»ç„¶æœªè¢«å……åˆ†æ¢ç´¢ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹è¿›è¡Œæ–‡æœ¬æ¸²æŸ“çš„è‡ªåŠ¨æµ·æŠ¥ç”Ÿæˆæ¡†æ¶ï¼Œè¯¥æ¡†æ¶é‡‡ç”¨åŸºäºå¯¹é½å­¦ä¹ çš„ä¸‰é‡äº¤å‰æ³¨æ„åŠ›æœºåˆ¶ã€‚è¯¥æ¡†æ¶æ—¨åœ¨åœ¨ä¸€ä¸ªè¯¦ç»†çš„ä¸Šä¸‹æ–‡èƒŒæ™¯ä¸­åˆ›å»ºç²¾ç¡®çš„æµ·æŠ¥æ–‡æœ¬ã€‚æ­¤å¤–ï¼Œè¯¥æ¡†æ¶æ”¯æŒå¯æ§å­—ä½“ã€å¯è°ƒå›¾åƒåˆ†è¾¨ç‡ï¼Œä»¥åŠèƒ½å¤Ÿå‘ˆç°ä¸­è‹±æ–‡æè¿°å’Œæ–‡æœ¬çš„æµ·æŠ¥ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜å¼•å…¥äº†ä¸€ä¸ªé«˜åˆ†è¾¨ç‡å­—ä½“æ•°æ®é›†å’Œä¸€ä¸ªåˆ†è¾¨ç‡è¶…è¿‡1024åƒç´ çš„æµ·æŠ¥æ•°æ®é›†ã€‚æˆ‘ä»¬çš„æ–¹æ³•åˆ©ç”¨SDXLæ¶æ„ã€‚å¤§é‡å®éªŒéªŒè¯äº†æˆ‘ä»¬æ–¹æ³•åœ¨ç”Ÿæˆå…·æœ‰å¤æ‚å’Œä¸°å¯Œä¸Šä¸‹æ–‡èƒŒæ™¯çš„æµ·æŠ¥å›¾åƒæ–¹é¢çš„èƒ½åŠ›ã€‚ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/OPPO-Mente-Lab/GlyphDraw2%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/OPPO-Mente-Lab/GlyphDraw2æ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2407.02252v4">PDF</a> Accepted by AAAI2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†åˆ©ç”¨æœ€æ–°å¯æ§T2Iæ‰©æ•£æ¨¡å‹ï¼Œç»“åˆLLMså’Œä¸‰å…ƒäº¤å‰æ³¨æ„åŠ›æœºåˆ¶ï¼Œæå‡ºä¸€ç§å…·æœ‰æ–‡æœ¬æ¸²æŸ“èƒ½åŠ›çš„è‡ªåŠ¨æµ·æŠ¥ç”Ÿæˆæ¡†æ¶ã€‚è¯¥æ¡†æ¶æ—¨åœ¨åˆ›å»ºå…·æœ‰è¯¦ç»†ä¸Šä¸‹æ–‡èƒŒæ™¯çš„æµ·æŠ¥æ–‡æœ¬ï¼Œæ”¯æŒå¯æ§å­—ä½“ã€å¯è°ƒæ•´çš„å›¾åƒåˆ†è¾¨ç‡ï¼Œä»¥åŠä¸­è‹±æ–‡æè¿°å’Œæ–‡æœ¬çš„æµ·æŠ¥æ¸²æŸ“ã€‚åŒæ—¶ï¼Œå¼•å…¥é«˜åˆ†è¾¨ç‡å­—ä½“æ•°æ®é›†å’Œè¶…è¿‡1024åƒç´ åˆ†è¾¨ç‡çš„æµ·æŠ¥æ•°æ®é›†ã€‚å®éªŒè¯æ˜ï¼Œè¯¥æ–¹æ³•èƒ½å¤Ÿç”Ÿæˆå…·æœ‰å¤æ‚å’Œä¸°å¯Œä¸Šä¸‹æ–‡èƒŒæ™¯çš„æµ·æŠ¥å›¾åƒã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ‰©æ•£æ¨¡å‹åœ¨æµ·æŠ¥è®¾è®¡ä¸­çš„åº”ç”¨ï¼šåˆ©ç”¨æœ€æ–°å¯æ§T2Iæ‰©æ•£æ¨¡å‹ï¼Œä¸ºæµ·æŠ¥ç”Ÿæˆæä¾›æ–°çš„è§£å†³æ–¹æ¡ˆã€‚</li>
<li>æ–‡æœ¬æ¸²æŸ“èƒ½åŠ›ï¼šç»“åˆLLMsï¼Œä½¿æ¡†æ¶å…·å¤‡åœ¨åˆæˆå›¾åƒä¸­æ¸²æŸ“æ–‡æœ¬çš„èƒ½åŠ›ã€‚</li>
<li>ä¸‰å…ƒäº¤å‰æ³¨æ„åŠ›æœºåˆ¶ï¼šåŸºäºå¯¹é½å­¦ä¹ ï¼Œå®ç°ç²¾ç¡®çš„æµ·æŠ¥æ–‡æœ¬ä¸ä¸Šä¸‹æ–‡èƒŒæ™¯çš„èåˆã€‚</li>
<li>å¤šè¯­è¨€æ”¯æŒï¼šæ¡†æ¶æ”¯æŒä¸­è‹±æ–‡æè¿°å’Œæ–‡æœ¬çš„æµ·æŠ¥æ¸²æŸ“ã€‚</li>
<li>é«˜åˆ†è¾¨ç‡æ•°æ®é›†ï¼šå¼•å…¥é«˜åˆ†è¾¨ç‡å­—ä½“æ•°æ®é›†å’Œè¶…è¿‡1024åƒç´ åˆ†è¾¨ç‡çš„æµ·æŠ¥æ•°æ®é›†ï¼Œæå‡ç”Ÿæˆå›¾åƒçš„è´¨é‡ã€‚</li>
<li>ä½¿ç”¨SDXLæ¶æ„ï¼šè¯¥æ¡†æ¶é‡‡ç”¨SDXLæ¶æ„ï¼Œæœ‰æ•ˆæé«˜æµ·æŠ¥ç”Ÿæˆçš„æ•ˆç‡å’Œå‡†ç¡®æ€§ã€‚</li>
<li>å®éªŒéªŒè¯ï¼šå¤§é‡å®éªŒè¯æ˜è¯¥æ–¹æ³•èƒ½å¤Ÿç”Ÿæˆå…·æœ‰å¤æ‚å’Œä¸°å¯Œä¸Šä¸‹æ–‡èƒŒæ™¯çš„æµ·æŠ¥å›¾åƒã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2407.02252">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-3e4d216faef6643eb2ae7fe1b518a2b2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-efb25ad2005f40b029459258372ee168.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b3f98df45dbccbfa58e8162d46c29fe6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-cfab3794dc454afcf58961c6212c4b9c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-070be88a127805c1794b2b20449f5096.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="Not-All-Prompts-Are-Made-Equal-Prompt-based-Pruning-of-Text-to-Image-Diffusion-Models"><a href="#Not-All-Prompts-Are-Made-Equal-Prompt-based-Pruning-of-Text-to-Image-Diffusion-Models" class="headerlink" title="Not All Prompts Are Made Equal: Prompt-based Pruning of Text-to-Image   Diffusion Models"></a>Not All Prompts Are Made Equal: Prompt-based Pruning of Text-to-Image   Diffusion Models</h2><p><strong>Authors:Alireza Ganjdanesh, Reza Shirkavand, Shangqian Gao, Heng Huang</strong></p>
<p>Text-to-image (T2I) diffusion models have demonstrated impressive image generation capabilities. Still, their computational intensity prohibits resource-constrained organizations from deploying T2I models after fine-tuning them on their internal target data. While pruning techniques offer a potential solution to reduce the computational burden of T2I models, static pruning methods use the same pruned model for all input prompts, overlooking the varying capacity requirements of different prompts. Dynamic pruning addresses this issue by utilizing a separate sub-network for each prompt, but it prevents batch parallelism on GPUs. To overcome these limitations, we introduce Adaptive Prompt-Tailored Pruning (APTP), a novel prompt-based pruning method designed for T2I diffusion models. Central to our approach is a prompt router model, which learns to determine the required capacity for an input text prompt and routes it to an architecture code, given a total desired compute budget for prompts. Each architecture code represents a specialized model tailored to the prompts assigned to it, and the number of codes is a hyperparameter. We train the prompt router and architecture codes using contrastive learning, ensuring that similar prompts are mapped to nearby codes. Further, we employ optimal transport to prevent the codes from collapsing into a single one. We demonstrate APTPâ€™s effectiveness by pruning Stable Diffusion (SD) V2.1 using CC3M and COCO as target datasets. APTP outperforms the single-model pruning baselines in terms of FID, CLIP, and CMMD scores. Our analysis of the clusters learned by APTP reveals they are semantically meaningful. We also show that APTP can automatically discover previously empirically found challenging prompts for SD, e.g. prompts for generating text images, assigning them to higher capacity codes. </p>
<blockquote>
<p>æ–‡æœ¬åˆ°å›¾åƒï¼ˆT2Iï¼‰æ‰©æ•£æ¨¡å‹å·²ç»è¡¨ç°å‡ºäº†ä»¤äººå°è±¡æ·±åˆ»çš„å›¾åƒç”Ÿæˆèƒ½åŠ›ã€‚ç„¶è€Œï¼Œå…¶è®¡ç®—å¼ºåº¦ä½¿å¾—èµ„æºå—é™çš„ç»„ç»‡æ— æ³•åœ¨å¯¹å…¶å†…éƒ¨ç›®æ ‡æ•°æ®è¿›è¡Œå¾®è°ƒåéƒ¨ç½²T2Iæ¨¡å‹ã€‚è™½ç„¶å‰ªææŠ€æœ¯æä¾›äº†å‡å°‘T2Iæ¨¡å‹è®¡ç®—è´Ÿæ‹…çš„æ½œåœ¨è§£å†³æ–¹æ¡ˆï¼Œä½†é™æ€å‰ªææ–¹æ³•ä½¿ç”¨ç›¸åŒçš„å‰ªææ¨¡å‹æ¥å¤„ç†æ‰€æœ‰è¾“å…¥æç¤ºï¼Œå¿½è§†äº†ä¸åŒæç¤ºæ‰€éœ€çš„èƒ½åŠ›å·®å¼‚ã€‚åŠ¨æ€å‰ªæé€šè¿‡ä¸ºæ¯ä¸ªæç¤ºä½¿ç”¨å•ç‹¬çš„å­ç½‘ç»œæ¥è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œä½†å®ƒé˜»æ­¢äº†GPUä¸Šçš„æ‰¹é‡å¹¶è¡Œæ€§ã€‚ä¸ºäº†å…‹æœè¿™äº›é™åˆ¶ï¼Œæˆ‘ä»¬å¼•å…¥äº†è‡ªé€‚åº”æç¤ºå®šåˆ¶å‰ªæï¼ˆAPTPï¼‰ï¼Œè¿™æ˜¯ä¸€ç§é’ˆå¯¹T2Iæ‰©æ•£æ¨¡å‹çš„æ–°å‹åŸºäºæç¤ºçš„å‰ªææ–¹æ³•ã€‚æˆ‘ä»¬çš„æ–¹æ³•çš„æ ¸å¿ƒæ˜¯ä¸€ä¸ªæç¤ºè·¯ç”±å™¨æ¨¡å‹ï¼Œå®ƒå­¦ä¹ ç¡®å®šè¾“å…¥æ–‡æœ¬æç¤ºæ‰€éœ€çš„è®¡ç®—èƒ½åŠ›ï¼Œå¹¶æ ¹æ®ç»™å®šçš„æ€»æç¤ºè®¡ç®—é¢„ç®—æ¥è·¯ç”±åˆ°æ¶æ„ä»£ç ã€‚æ¯ä¸ªæ¶æ„ä»£ç ä»£è¡¨ä¸€ä¸ªä¸“ä¸ºå…¶åˆ†é…çš„æç¤ºå®šåˆ¶çš„æ¨¡å‹ï¼Œä»£ç çš„æ•°é‡æ˜¯ä¸€ä¸ªè¶…å‚æ•°ã€‚æˆ‘ä»¬ä½¿ç”¨å¯¹æ¯”å­¦ä¹ æ¥è®­ç»ƒæç¤ºè·¯ç”±å™¨å’Œæ¶æ„ä»£ç ï¼Œç¡®ä¿ç›¸ä¼¼çš„æç¤ºè¢«æ˜ å°„åˆ°é™„è¿‘çš„ä»£ç ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬é‡‡ç”¨æœ€ä¼˜ä¼ è¾“æ¥é˜²æ­¢ä»£ç å´©æºƒä¸ºå•ä¸€ä»£ç ã€‚æˆ‘ä»¬é€šè¿‡ä½¿ç”¨CC3Må’ŒCOCOä½œä¸ºç›®æ ‡æ•°æ®é›†å¯¹Stable Diffusionï¼ˆSDï¼‰V2.1è¿›è¡Œå‰ªææ¥å±•ç¤ºAPTPçš„æœ‰æ•ˆæ€§ã€‚APTPåœ¨FIDã€CLIPå’ŒCMMDåˆ†æ•°æ–¹é¢ä¼˜äºå•æ¨¡å‹å‰ªæåŸºçº¿ã€‚æˆ‘ä»¬å¯¹APTPå­¦ä¹ çš„é›†ç¾¤çš„åˆ†æè¡¨æ˜å®ƒä»¬æ˜¯è¯­ä¹‰ä¸Šæœ‰æ„ä¹‰çš„ã€‚æˆ‘ä»¬è¿˜è¡¨æ˜ï¼ŒAPTPå¯ä»¥è‡ªåŠ¨å‘ç°ä¹‹å‰ç»éªŒä¸Šå‘ç°çš„SDçš„æŒ‘æˆ˜æ€§æç¤ºï¼Œä¾‹å¦‚ç”¨äºç”Ÿæˆæ–‡æœ¬å›¾åƒçš„æç¤ºï¼Œå¹¶å°†å®ƒä»¬åˆ†é…ç»™æ›´é«˜å®¹é‡çš„ä»£ç ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2406.12042v3">PDF</a> </p>
<p><strong>æ‘˜è¦</strong><br>T2Iæ‰©æ•£æ¨¡å‹ç”Ÿæˆå›¾åƒèƒ½åŠ›å¼ºå¤§ï¼Œä½†å…¶è®¡ç®—å¼ºåº¦é™åˆ¶äº†èµ„æºå—é™ç»„ç»‡åœ¨å†…éƒ¨ç›®æ ‡æ•°æ®ä¸Šå¾®è°ƒåéƒ¨ç½²T2Iæ¨¡å‹ã€‚ç°æœ‰æ–¹æ³•æ— æ³•é’ˆå¯¹ä¸åŒè¾“å…¥æç¤ºå˜åŒ–å…¶è®¡ç®—èƒ½åŠ›ï¼ŒåŠ¨æ€å‰ªææ³•é‡‡ç”¨æ¯ä¸ªæç¤ºä½¿ç”¨å•ç‹¬çš„å­ç½‘ç»œè™½è§£å†³æ­¤é—®é¢˜ï¼Œå´é˜»ç¢GPUæ‰¹é‡å¹¶è¡Œå¤„ç†ã€‚æœ¬æ–‡å¼•å…¥è‡ªé€‚åº”æç¤ºå®šåˆ¶å‰ªæï¼ˆAPTPï¼‰ï¼Œä¸€ç§ä¸“ä¸ºT2Iæ‰©æ•£æ¨¡å‹è®¾è®¡çš„åŸºäºæç¤ºçš„å‰ªææ–¹æ³•ã€‚APTPé‡‡ç”¨æç¤ºè·¯ç”±å™¨æ¨¡å‹ç¡®å®šè¾“å…¥æ–‡æœ¬æç¤ºæ‰€éœ€è®¡ç®—èƒ½åŠ›ï¼Œå¹¶æ ¹æ®è®¡ç®—é¢„ç®—å°†å…¶è·¯ç”±è‡³ç‰¹å®šæ¶æ„ä»£ç ã€‚ä½¿ç”¨å¯¹æ¯”å­¦ä¹ è®­ç»ƒæç¤ºè·¯ç”±å™¨å’Œæ¶æ„ä»£ç ï¼Œç¡®ä¿ç›¸ä¼¼æç¤ºæ˜ å°„è‡³ç›¸è¿‘ä»£ç ã€‚é€šè¿‡æœ€ä¼˜ä¼ è¾“é˜²æ­¢ä»£ç å´©æºƒè‡³å•ä¸€çŠ¶æ€ã€‚å®éªŒæ˜¾ç¤ºï¼ŒAPTPåœ¨ä¿®å‰ªStable Diffusion V2.1æ—¶ä¼˜äºå•æ¨¡å‹å‰ªæåŸºçº¿ï¼Œé€šè¿‡FIDã€CLIPå’ŒCMMDå¾—åˆ†è¯„ä¼°æ•ˆæœã€‚åˆ†ææ˜¾ç¤ºAPTPå­¦ä¹ çš„é›†ç¾¤è¯­ä¹‰ä¸Šæ›´æœ‰å®é™…æ„ä¹‰ï¼Œè¿˜èƒ½è‡ªåŠ¨å‘ç°ä¹‹å‰éš¾ä»¥å¤„ç†çš„ä»»åŠ¡æç¤ºå¹¶ä¸ºå…¶åˆ†é…æ›´é«˜è®¡ç®—èƒ½åŠ›ä»£ç ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>T2Iæ‰©æ•£æ¨¡å‹å…·å¤‡å‡ºè‰²çš„å›¾åƒç”Ÿæˆèƒ½åŠ›ï¼Œä½†è®¡ç®—å¼ºåº¦é«˜ï¼Œé™åˆ¶äº†å…¶åœ¨èµ„æºå—é™ç¯å¢ƒä¸­çš„éƒ¨ç½²ã€‚</li>
<li>ç°æœ‰å‰ªææŠ€æœ¯æ— æ³•é€‚åº”ä¸åŒè¾“å…¥æç¤ºçš„è®¡ç®—éœ€æ±‚å˜åŒ–ã€‚</li>
<li>åŠ¨æ€å‰ªææ–¹æ³•é‡‡ç”¨å­ç½‘ç»œå¤„ç†æ¯ä¸ªæç¤ºï¼Œä½†é™åˆ¶äº†GPUçš„æ‰¹é‡å¹¶è¡Œå¤„ç†ã€‚</li>
<li>å¼•å…¥è‡ªé€‚åº”æç¤ºå®šåˆ¶å‰ªæï¼ˆAPTPï¼‰ï¼Œåˆ©ç”¨æç¤ºè·¯ç”±å™¨æ¨¡å‹ç¡®å®šä¸åŒæç¤ºçš„è®¡ç®—éœ€æ±‚ã€‚</li>
<li>é‡‡ç”¨å¯¹æ¯”å­¦ä¹ è®­ç»ƒæç¤ºè·¯ç”±å™¨å’Œæ¶æ„ä»£ç ï¼Œç¡®ä¿ç›¸ä¼¼æç¤ºæ˜ å°„è‡³ç›¸è¿‘æ¶æ„ä»£ç ã€‚</li>
<li>ä½¿ç”¨æœ€ä¼˜ä¼ è¾“æŠ€æœ¯é˜²æ­¢æ¶æ„ä»£ç å´©æºƒè‡³å•ä¸€çŠ¶æ€ã€‚</li>
<li>APTPåœ¨ä¿®å‰ªStable Diffusion V2.1æ¨¡å‹æ—¶è¡¨ç°å‡ºä¼˜å¼‚æ€§èƒ½ï¼Œå¹¶é€šè¿‡FIDã€CLIPå’ŒCMMDå¾—åˆ†è¯„ä¼°ä¼˜äºåŸºçº¿æ–¹æ³•ã€‚è¯­ä¹‰åˆ†ææ˜¾ç¤ºAPTPå­¦ä¹ çš„é›†ç¾¤å…·æœ‰å®é™…æ„ä¹‰ï¼Œå¹¶èƒ½è‡ªåŠ¨å¤„ç†å¤æ‚ä»»åŠ¡æç¤ºã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2406.12042">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-36a193fa557f10209625c218ace41225.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3db3a1f7d5733b99c971da53f0f241cb.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0b3dc8d349048543dff78ab46e44affb.jpg" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="Information-Theoretic-Text-to-Image-Alignment"><a href="#Information-Theoretic-Text-to-Image-Alignment" class="headerlink" title="Information Theoretic Text-to-Image Alignment"></a>Information Theoretic Text-to-Image Alignment</h2><p><strong>Authors:Chao Wang, Giulio Franzese, Alessandro Finamore, Massimo Gallo, Pietro Michiardi</strong></p>
<p>Diffusion models for Text-to-Image (T2I) conditional generation have recently achieved tremendous success. Yet, aligning these models with userâ€™s intentions still involves a laborious trial-and-error process, and this challenging alignment problem has attracted considerable attention from the research community. In this work, instead of relying on fine-grained linguistic analyses of prompts, human annotation, or auxiliary vision-language models, we use Mutual Information (MI) to guide model alignment. In brief, our method uses self-supervised fine-tuning and relies on a point-wise (MI) estimation between prompts and images to create a synthetic fine-tuning set for improving model alignment. Our analysis indicates that our method is superior to the state-of-the-art, yet it only requires the pre-trained denoising network of the T2I model itself to estimate MI, and a simple fine-tuning strategy that improves alignment while maintaining image quality. Code available at <a target="_blank" rel="noopener" href="https://github.com/Chao0511/mitune">https://github.com/Chao0511/mitune</a>. </p>
<blockquote>
<p>æ–‡æœ¬åˆ°å›¾åƒï¼ˆT2Iï¼‰çš„æ¡ä»¶ç”Ÿæˆæ‰©æ•£æ¨¡å‹è¿‘æœŸå–å¾—äº†å·¨å¤§çš„æˆåŠŸã€‚ç„¶è€Œï¼Œå°†è¿™äº›æ¨¡å‹ä¸ç”¨æˆ·æ„å›¾å¯¹é½ä»ç„¶éœ€è¦è€—è´¹å¤§é‡æ—¶é—´å’Œç²¾åŠ›çš„è¯•é”™è¿‡ç¨‹ï¼Œè¿™ä¸€å…·æœ‰æŒ‘æˆ˜æ€§çš„å¯¹é½é—®é¢˜å·²å¼•èµ·äº†ç ”ç©¶ç•Œçš„å¹¿æ³›å…³æ³¨ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æ²¡æœ‰ä¾èµ–æç¤ºçš„ç²¾ç»†è¯­è¨€åˆ†æã€äººå·¥æ ‡æ³¨æˆ–è¾…åŠ©çš„è§†è§‰è¯­è¨€æ¨¡å‹ï¼Œè€Œæ˜¯ä½¿ç”¨äº’ä¿¡æ¯ï¼ˆMIï¼‰æ¥å¼•å¯¼æ¨¡å‹å¯¹é½ã€‚ç®€è€Œè¨€ä¹‹ï¼Œæˆ‘ä»¬çš„æ–¹æ³•ä½¿ç”¨è‡ªç›‘ç£å¾®è°ƒï¼Œå¹¶ä¾èµ–äºæç¤ºå’Œå›¾åƒä¹‹é—´çš„ç‚¹å¼ï¼ˆMIï¼‰ä¼°è®¡æ¥åˆ›å»ºä¸€ä¸ªåˆæˆå¾®è°ƒé›†ï¼Œä»¥æé«˜æ¨¡å‹å¯¹é½æ•ˆæœã€‚æˆ‘ä»¬çš„åˆ†æè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•ä¼˜äºå½“å‰æœ€å…ˆè¿›çš„å¯¹é½æ–¹æ³•ï¼Œè€Œä¸”å®ƒåªéœ€è¦T2Iæ¨¡å‹æœ¬èº«çš„é¢„è®­ç»ƒå»å™ªç½‘ç»œæ¥ä¼°è®¡äº’ä¿¡æ¯ï¼Œä»¥åŠä¸€ç§ç®€å•çš„å¾®è°ƒç­–ç•¥ï¼Œå¯ä»¥åœ¨ä¿æŒå›¾åƒè´¨é‡çš„åŒæ—¶æé«˜å¯¹é½æ•ˆæœã€‚ç›¸å…³ä»£ç å¯é€šè¿‡<a target="_blank" rel="noopener" href="https://github.com/Chao0511/mitune%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/Chao0511/mituneè·å–ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2405.20759v3">PDF</a> to appear at ICLR25</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ä½¿ç”¨äº’ä¿¡æ¯ï¼ˆMIï¼‰æ¥æŒ‡å¯¼æ–‡æœ¬åˆ°å›¾åƒï¼ˆT2Iï¼‰æ‰©æ•£æ¨¡å‹çš„æ¨¡å‹å¯¹é½æ–¹æ³•ã€‚è¯¥æ–¹æ³•é‡‡ç”¨è‡ªç›‘ç£å¾®è°ƒï¼Œä¾èµ–äºæç¤ºå’Œå›¾åƒä¹‹é—´çš„ç‚¹çº§äº’ä¿¡æ¯ä¼°è®¡ï¼Œä»¥åˆ›å»ºåˆæˆå¾®è°ƒé›†ï¼Œä»è€Œæé«˜æ¨¡å‹å¯¹é½èƒ½åŠ›ã€‚è¯¥æ–¹æ³•ä»…ä½¿ç”¨T2Iæ¨¡å‹çš„é¢„è®­ç»ƒå»å™ªç½‘ç»œæ¥ä¼°è®¡äº’ä¿¡æ¯ï¼Œå¹¶é‡‡ç”¨ç®€å•çš„å¾®è°ƒç­–ç•¥ï¼Œæ—¢æé«˜äº†å¯¹é½èƒ½åŠ›åˆä¿æŒäº†å›¾åƒè´¨é‡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ–‡æœ¬åˆ°å›¾åƒï¼ˆT2Iï¼‰æ‰©æ•£æ¨¡å‹çš„æ¨¡å‹å¯¹é½é—®é¢˜å¸å¼•äº†ç ”ç©¶äººå‘˜çš„å…³æ³¨ã€‚</li>
<li>è¯¥æ–¹æ³•ä½¿ç”¨äº’ä¿¡æ¯ï¼ˆMIï¼‰æ¥æŒ‡å¯¼æ¨¡å‹å¯¹é½ï¼Œè¿™æ˜¯ä¸€ç§æ–°çš„å°è¯•ã€‚</li>
<li>é‡‡ç”¨è‡ªç›‘ç£å¾®è°ƒæ–¹å¼ï¼Œä»…ä¾èµ–T2Iæ¨¡å‹çš„é¢„è®­ç»ƒå»å™ªç½‘ç»œæ¥ä¼°è®¡äº’ä¿¡æ¯ã€‚</li>
<li>é€šè¿‡ç‚¹çº§äº’ä¿¡æ¯ä¼°è®¡ï¼Œåˆ›å»ºåˆæˆå¾®è°ƒé›†ä»¥æé«˜æ¨¡å‹å¯¹é½èƒ½åŠ›ã€‚</li>
<li>è¯¥æ–¹æ³•æé«˜äº†æ¨¡å‹çš„å¯¹é½èƒ½åŠ›ï¼ŒåŒæ—¶ä¿æŒäº†å›¾åƒè´¨é‡ã€‚</li>
<li>è¯¥æ–¹æ³•æä¾›äº†ä¸€ç§ç®€å•æœ‰æ•ˆçš„å¾®è°ƒç­–ç•¥ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2405.20759">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-d4cfb213bf6056d311d6c2eca5508b19.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9d2cba772f764aea799ed47790c3233a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-9653907ec2cb3ebf920220006b0e2763.jpg" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="MemControl-Mitigating-Memorization-in-Diffusion-Models-via-Automated-Parameter-Selection"><a href="#MemControl-Mitigating-Memorization-in-Diffusion-Models-via-Automated-Parameter-Selection" class="headerlink" title="MemControl: Mitigating Memorization in Diffusion Models via Automated   Parameter Selection"></a>MemControl: Mitigating Memorization in Diffusion Models via Automated   Parameter Selection</h2><p><strong>Authors:Raman Dutt, Ondrej Bohdal, Pedro Sanchez, Sotirios A. Tsaftaris, Timothy Hospedales</strong></p>
<p>Diffusion models excel in generating images that closely resemble their training data but are also susceptible to data memorization, raising privacy, ethical, and legal concerns, particularly in sensitive domains such as medical imaging. We hypothesize that this memorization stems from the overparameterization of deep models and propose that regularizing model capacity during fine-tuning can mitigate this issue. Firstly, we empirically show that regulating the model capacity via Parameter-efficient fine-tuning (PEFT) mitigates memorization to some extent, however, it further requires the identification of the exact parameter subsets to be fine-tuned for high-quality generation. To identify these subsets, we introduce a bi-level optimization framework, MemControl, that automates parameter selection using memorization and generation quality metrics as rewards during fine-tuning. The parameter subsets discovered through MemControl achieve a superior tradeoff between generation quality and memorization. For the task of medical image generation, our approach outperforms existing state-of-the-art memorization mitigation strategies by fine-tuning as few as 0.019% of model parameters. Moreover, we demonstrate that the discovered parameter subsets are transferable to non-medical domains. Our framework is scalable to large datasets, agnostic to reward functions, and can be integrated with existing approaches for further memorization mitigation. To the best of our knowledge, this is the first study to empirically evaluate memorization in medical images and propose a targeted yet universal mitigation strategy. The code is available at <a target="_blank" rel="noopener" href="https://github.com/Raman1121/Diffusion_Memorization_HPO">https://github.com/Raman1121/Diffusion_Memorization_HPO</a>. </p>
<blockquote>
<p>æ‰©æ•£æ¨¡å‹åœ¨ç”Ÿæˆä¸è®­ç»ƒæ•°æ®éå¸¸ç›¸ä¼¼çš„å›¾åƒæ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œä½†ä¹Ÿå®¹æ˜“å­˜åœ¨æ•°æ®è®°å¿†çš„é—®é¢˜ï¼Œç‰¹åˆ«æ˜¯åœ¨åŒ»å­¦å½±åƒç­‰æ•æ„Ÿé¢†åŸŸå¼•å‘äº†éšç§ã€ä¼¦ç†å’Œæ³•å¾‹çš„æ‹…å¿§ã€‚æˆ‘ä»¬å‡è®¾è¿™ç§è®°å¿†æºäºæ·±åº¦æ¨¡å‹çš„è¿‡åº¦å‚æ•°åŒ–ï¼Œå¹¶æå‡ºåœ¨å¾®è°ƒè¿‡ç¨‹ä¸­è¿›è¡Œæ¨¡å‹å®¹é‡æ­£åˆ™åŒ–å¯ä»¥ç¼“è§£è¿™ä¸ªé—®é¢˜ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬é€šè¿‡å®éªŒè¯æ˜ï¼Œé€šè¿‡å‚æ•°æœ‰æ•ˆçš„å¾®è°ƒï¼ˆPEFTï¼‰æ¥è°ƒæ§æ¨¡å‹å®¹é‡å¯ä»¥åœ¨ä¸€å®šç¨‹åº¦ä¸Šå‡è½»è®°å¿†é—®é¢˜ã€‚ç„¶è€Œï¼Œè¿™è¿˜éœ€è¦ç¡®å®šè¦è¿›è¡Œå¾®è°ƒçš„ç¡®åˆ‡å‚æ•°å­é›†ä»¥å®ç°é«˜è´¨é‡çš„ç”Ÿæˆã€‚ä¸ºäº†è¯†åˆ«è¿™äº›å­é›†ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§åŒå±‚ä¼˜åŒ–æ¡†æ¶MemControlï¼Œå®ƒä½¿ç”¨è®°å¿†å’Œç”Ÿæˆè´¨é‡æŒ‡æ ‡ä½œä¸ºå¥–åŠ±æ¥è‡ªåŠ¨åŒ–å‚æ•°é€‰æ‹©ï¼Œä»è€Œåœ¨å¾®è°ƒè¿‡ç¨‹ä¸­è¿›è¡Œé€‰æ‹©ã€‚é€šè¿‡MemControlå‘ç°çš„å‚æ•°å­é›†åœ¨ç”Ÿæˆè´¨é‡å’Œè®°å¿†ä¹‹é—´è¾¾åˆ°äº†ä¼˜è¶Šçš„æƒè¡¡ã€‚åœ¨åŒ»å­¦å›¾åƒç”Ÿæˆä»»åŠ¡ä¸­ï¼Œæˆ‘ä»¬çš„æ–¹æ³•é€šè¿‡å¾®è°ƒä»…0.019%çš„æ¨¡å‹å‚æ•°ï¼Œå°±ä¼˜äºç°æœ‰çš„æœ€å…ˆè¿›çš„è®°å¿†ç¼“è§£ç­–ç•¥ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜è¯æ˜äº†æ‰€å‘ç°çš„å‚æ•°å­é›†å¯è½¬ç§»åˆ°éåŒ»å­¦é¢†åŸŸã€‚æˆ‘ä»¬çš„æ¡†æ¶å¯æ‰©å±•åˆ°å¤§å‹æ•°æ®é›†ï¼Œå¯¹å¥–åŠ±å‡½æ•°æ— åè§ï¼Œå¹¶å¯ä¸å…¶ä»–ç°æœ‰æ–¹æ³•ç›¸ç»“åˆè¿›ä¸€æ­¥ç¼“è§£è®°å¿†é—®é¢˜ã€‚æ®æˆ‘ä»¬æ‰€çŸ¥ï¼Œè¿™æ˜¯ç¬¬ä¸€é¡¹é’ˆå¯¹åŒ»å­¦å›¾åƒä¸­çš„è®°å¿†é—®é¢˜è¿›è¡Œå®è¯è¯„ä¼°å¹¶æå‡ºæœ‰é’ˆå¯¹æ€§çš„é€šç”¨ç¼“è§£ç­–ç•¥çš„ç ”ç©¶ã€‚ä»£ç å¯é€šè¿‡<a target="_blank" rel="noopener" href="https://github.com/Raman1121/Diffusion_Memorization_HPO%E8%AE%BF%E9%97%AE%E3%80%82">https://github.com/Raman1121/Diffusion_Memorization_HPOè®¿é—®ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2405.19458v4">PDF</a> Accepted into WACV 2025 (Applications Track)</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ¢è®¨äº†æ‰©æ•£æ¨¡å‹åœ¨ç”Ÿæˆå›¾åƒæ—¶å­˜åœ¨çš„æ•°æ®è®°å¿†åŒ–é—®é¢˜ï¼Œç‰¹åˆ«æ˜¯åœ¨åŒ»ç–—å›¾åƒç­‰æ•æ„Ÿé¢†åŸŸã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œæå‡ºäº†é€šè¿‡å‚æ•°æ•ˆç‡å¾®è°ƒï¼ˆPEFTï¼‰æ¥è§„èŒƒæ¨¡å‹å®¹é‡ï¼Œå¹¶å¼•å…¥äº†ä¸€ç§åŒå±‚ä¼˜åŒ–æ¡†æ¶MemControlï¼Œä»¥è‡ªåŠ¨åŒ–é€‰æ‹©å‚æ•°ï¼Œåœ¨å¾®è°ƒè¿‡ç¨‹ä¸­ä»¥è®°å¿†åŒ–å’Œç”Ÿæˆè´¨é‡æŒ‡æ ‡ä½œä¸ºå¥–åŠ±ã€‚è¯¥æ–¹æ³•åœ¨åŒ»ç–—å›¾åƒç”Ÿæˆä»»åŠ¡ä¸Šè¡¨ç°å‡ºä¼˜å¼‚çš„æ•ˆæœï¼Œä¸”å¯åº”ç”¨äºéåŒ»ç–—é¢†åŸŸã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ‰©æ•£æ¨¡å‹è™½èƒ½ç”Ÿæˆä¸è®­ç»ƒæ•°æ®ç›¸ä¼¼çš„å›¾åƒï¼Œä½†å­˜åœ¨æ•°æ®è®°å¿†åŒ–é—®é¢˜ï¼Œå¼•å‘éšç§ã€ä¼¦ç†å’Œæ³•å¾‹æ‹…å¿§ï¼Œå°¤å…¶åœ¨åŒ»ç–—å›¾åƒç­‰æ•æ„Ÿé¢†åŸŸã€‚</li>
<li>å‚æ•°æ•ˆç‡å¾®è°ƒï¼ˆPEFTï¼‰æ˜¯ä¸€ç§æœ‰æ•ˆçš„ç­–ç•¥ï¼Œèƒ½å¤Ÿåœ¨ä¸€å®šç¨‹åº¦ä¸Šç¼“è§£è®°å¿†åŒ–é—®é¢˜ã€‚</li>
<li>å¼•å…¥åŒå±‚ä¼˜åŒ–æ¡†æ¶MemControlï¼Œé€šè¿‡è‡ªåŠ¨åŒ–å‚æ•°é€‰æ‹©ï¼Œä»¥è®°å¿†åŒ–å’Œç”Ÿæˆè´¨é‡æŒ‡æ ‡ä¸ºå¥–åŠ±ï¼Œå®ç°äº†é«˜è´¨é‡çš„ç”Ÿæˆä¸è®°å¿†åŒ–çš„å¹³è¡¡ã€‚</li>
<li>åœ¨åŒ»ç–—å›¾åƒç”Ÿæˆä»»åŠ¡ä¸Šï¼ŒMemControlç­–ç•¥ä¼˜äºç°æœ‰ç­–ç•¥ï¼Œä»…å¾®è°ƒ0.019%çš„å‚æ•°å³å¯å®ç°æ˜¾è‘—æ•ˆæœã€‚</li>
<li>å‘ç°çš„å‚æ•°å­é›†å¯åº”ç”¨äºéåŒ»ç–—é¢†åŸŸï¼Œæ˜¾ç¤ºå‡ºäº†è‰¯å¥½çš„é€šç”¨æ€§ã€‚</li>
<li>è¯¥æ¡†æ¶å¯æ‰©å±•è‡³å¤§æ•°æ®é›†ï¼Œå¯¹å¥–åŠ±å‡½æ•°å…·æœ‰æ— å…³æ€§ï¼Œå¹¶å¯ä¸å…¶ä»–ç­–ç•¥ç»“åˆè¿›ä¸€æ­¥ç¼“è§£è®°å¿†åŒ–é—®é¢˜ã€‚</li>
<li>æœ¬æ–‡æ˜¯é¦–ä¸ªé’ˆå¯¹åŒ»ç–—å›¾åƒè®°å¿†åŒ–é—®é¢˜è¿›è¡Œå®è¯ç ”ç©¶å¹¶æå‡ºé€šç”¨ç¼“è§£ç­–ç•¥çš„ç ”ç©¶ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2405.19458">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-7525cc61dadce7982fcb846568ccef92.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ee487af96cd922292ac7cb0e9849eae6.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-8250e33c6e34cf374e3d6a8fc9f6c008.jpg" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1><h2 id="X-Diffusion-Generating-Detailed-3D-MRI-Volumes-From-a-Single-Image-Using-Cross-Sectional-Diffusion-Models"><a href="#X-Diffusion-Generating-Detailed-3D-MRI-Volumes-From-a-Single-Image-Using-Cross-Sectional-Diffusion-Models" class="headerlink" title="X-Diffusion: Generating Detailed 3D MRI Volumes From a Single Image   Using Cross-Sectional Diffusion Models"></a>X-Diffusion: Generating Detailed 3D MRI Volumes From a Single Image   Using Cross-Sectional Diffusion Models</h2><p><strong>Authors:Emmanuelle Bourigault, Abdullah Hamdi, Amir Jamaludin</strong></p>
<p>Magnetic Resonance Imaging (MRI) is a crucial diagnostic tool, but high-resolution scans are often slow and expensive due to extensive data acquisition requirements. Traditional MRI reconstruction methods aim to expedite this process by filling in missing frequency components in the K-space, performing 3D-to-3D reconstructions that demand full 3D scans. In contrast, we introduce X-Diffusion, a novel cross-sectional diffusion model that reconstructs detailed 3D MRI volumes from extremely sparse spatial-domain inputs, achieving 2D-to-3D reconstruction from as little as a single 2D MRI slice or few slices. A key aspect of X-Diffusion is that it models MRI data as holistic 3D volumes during the cross-sectional training and inference, unlike previous learning approaches that treat MRI scans as collections of 2D slices in standard planes (coronal, axial, sagittal). We evaluated X-Diffusion on brain tumor MRIs from the BRATS dataset and full-body MRIs from the UK Biobank dataset. Our results demonstrate that X-Diffusion not only surpasses state-of-the-art methods in quantitative accuracy (PSNR) on unseen data but also preserves critical anatomical features such as tumor profiles, spine curvature, and brain volume. Remarkably, the model generalizes beyond the training domain, successfully reconstructing knee MRIs despite being trained exclusively on brain data. Medical expert evaluations further confirm the clinical relevance and fidelity of the generated images.To our knowledge, X-Diffusion is the first method capable of producing detailed 3D MRIs from highly limited 2D input data, potentially accelerating MRI acquisition and reducing associated costs. The code is available on the project website <a target="_blank" rel="noopener" href="https://emmanuelleb985.github.io/XDiffusion/">https://emmanuelleb985.github.io/XDiffusion/</a> . </p>
<blockquote>
<p>ç£å…±æŒ¯æˆåƒï¼ˆMRIï¼‰æ˜¯ä¸€ç§é‡è¦çš„è¯Šæ–­å·¥å…·ï¼Œä½†é«˜åˆ†è¾¨ç‡æ‰«æé€šå¸¸ç”±äºéœ€è¦å¤§é‡æ•°æ®é‡‡é›†è€Œç¼“æ…¢ä¸”æ˜‚è´µã€‚ä¼ ç»Ÿçš„MRIé‡å»ºæ–¹æ³•æ—¨åœ¨é€šè¿‡å¡«å……Kç©ºé—´ä¸­çš„ç¼ºå¤±é¢‘ç‡æˆåˆ†æ¥åŠ é€Ÿè¿™ä¸€è¿‡ç¨‹ï¼Œè¿›è¡Œéœ€è¦å®Œæ•´3Dæ‰«æçš„3D-to-3Dé‡å»ºã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼Œæˆ‘ä»¬å¼•å…¥äº†X-Diffusionï¼Œè¿™æ˜¯ä¸€ç§æ–°å‹æˆªé¢æ‰©æ•£æ¨¡å‹ï¼Œèƒ½å¤Ÿä»æå…¶ç¨€ç–çš„ç©ºé—´åŸŸè¾“å…¥ä¸­é‡å»ºè¯¦ç»†çš„3D MRIä½“ç§¯ï¼Œå®ç°ä»å°‘é‡ç”šè‡³å•ä¸ª2D MRIåˆ‡ç‰‡çš„2D-to-3Dé‡å»ºã€‚X-Diffusionçš„å…³é”®åœ¨äºï¼Œå®ƒåœ¨æˆªé¢è®­ç»ƒå’Œæ¨æ–­è¿‡ç¨‹ä¸­å°†MRIæ•°æ®å»ºæ¨¡ä¸ºæ•´ä½“3Dä½“ç§¯ï¼Œä¸åŒäºä»¥å‰çš„å­¦ä¹ æ–¹æ³•ï¼Œåè€…å°†MRIæ‰«æè§†ä¸ºæ ‡å‡†å¹³é¢ï¼ˆå† çŠ¶é¢ã€è½´é¢ã€çŸ¢çŠ¶é¢ï¼‰ä¸­çš„2Dåˆ‡ç‰‡é›†åˆã€‚æˆ‘ä»¬å¯¹BRATSæ•°æ®é›†ä¸­çš„è„‘è‚¿ç˜¤MRIå’ŒUK Biobankæ•°æ®é›†ä¸­çš„å…¨èº«MRIè¯„ä¼°äº†X-Diffusionã€‚ç»“æœè¡¨æ˜ï¼ŒX-Diffusionä¸ä»…åœ¨æœªè§æ•°æ®ä¸Šçš„å®šé‡å‡†ç¡®æ€§ï¼ˆPSNRï¼‰ä¸Šè¶…è¶Šäº†æœ€å…ˆè¿›çš„æ–¹æ³•ï¼Œè€Œä¸”è¿˜ä¿ç•™äº†å…³é”®çš„ç»„ç»‡ç‰¹å¾ï¼Œå¦‚è‚¿ç˜¤è½®å»“ã€è„ŠæŸ±æ›²åº¦å’Œè„‘å®¹é‡ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œè¯¥æ¨¡å‹åœ¨è®­ç»ƒé¢†åŸŸä¹‹å¤–ä¹Ÿé€šç”¨åŒ–ï¼Œå°½ç®¡åªæ¥å—è„‘éƒ¨æ•°æ®è®­ç»ƒï¼Œä½†æˆåŠŸé‡å»ºäº†è†ç›–MRIã€‚åŒ»å­¦ä¸“å®¶è¯„ä¼°è¿›ä¸€æ­¥è¯å®äº†ç”Ÿæˆå›¾åƒçš„ä¸´åºŠç›¸å…³æ€§å’Œä¿çœŸåº¦ã€‚æ®æˆ‘ä»¬æ‰€çŸ¥ï¼ŒX-Diffusionæ˜¯ç¬¬ä¸€ç§èƒ½å¤Ÿä»é«˜åº¦æœ‰é™çš„2Dè¾“å…¥æ•°æ®ç”Ÿæˆè¯¦ç»†çš„3D MRIçš„æ–¹æ³•ï¼Œæœ‰å¯èƒ½åŠ é€ŸMRIé‡‡é›†å¹¶é™ä½ç›¸å…³æˆæœ¬ã€‚ä»£ç å¯åœ¨é¡¹ç›®ç½‘ç«™<a target="_blank" rel="noopener" href="https://emmanuelleb985.github.io/XDiffusion/%E4%B8%8A%E6%89%BE%E5%88%B0%E3%80%82">https://emmanuelleb985.github.io/XDiffusion/ä¸Šæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2404.19604v2">PDF</a> preprint, project website:   <a target="_blank" rel="noopener" href="https://emmanuelleb985.github.io/XDiffusion/">https://emmanuelleb985.github.io/XDiffusion/</a></p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åä¸ºX-Diffusionçš„æ–°å‹äº¤å‰æˆªé¢æ‰©æ•£æ¨¡å‹ï¼Œè¯¥æ¨¡å‹å¯ä»æç¨€ç–çš„ç©ºé—´åŸŸè¾“å…¥ä¸­é‡å»ºè¯¦ç»†çš„3D MRIä½“ç§¯ï¼Œå®ç°ä»å•ä¸ªæˆ–å°‘é‡çš„2D MRIåˆ‡ç‰‡åˆ°3Dé‡å»ºçš„è½¬åŒ–ã€‚ä¸ä¼ ç»Ÿçš„MRIé‡å»ºæ–¹æ³•ä¸åŒï¼ŒX-Diffusionå°†MRIæ•°æ®è§†ä¸ºæ•´ä½“3Dä½“ç§¯è¿›è¡Œå»ºæ¨¡ï¼Œçªç ´äº†ä»¥å¾€å°†MRIæ‰«æè§†ä¸ºæ ‡å‡†å¹³é¢å†…2Dåˆ‡ç‰‡é›†åˆçš„å¤„ç†æ–¹å¼ã€‚åœ¨BRATSæ•°æ®é›†ä¸Šçš„è„‘è‚¿ç˜¤MRIå’ŒUK Biobankæ•°æ®é›†ä¸Šçš„å…¨èº«MRIçš„è¯„ä¼°ç»“æœæ˜¾ç¤ºï¼ŒX-Diffusionä¸ä»…åœ¨å®šé‡ç²¾åº¦ï¼ˆPSNRï¼‰ä¸Šè¶…è¶Šäº†ç°æœ‰æ–¹æ³•ï¼Œè€Œä¸”åœ¨ä¿æŒå…³é”®è§£å‰–ç‰¹å¾æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œå¦‚è‚¿ç˜¤åˆ†å¸ƒã€è„ŠæŸ±æ›²åº¦å’Œè„‘å®¹é‡ç­‰ã€‚æ­¤å¤–ï¼Œè¯¥æ¨¡å‹åœ¨è®­ç»ƒåŸŸä¹‹å¤–ä¹Ÿèƒ½è¿›è¡Œæ¨å¹¿ï¼ŒæˆåŠŸé‡å»ºäº†è†ç›–MRIï¼Œå°½ç®¡å…¶ä»…æ¥å—äº†è„‘æ•°æ®è®­ç»ƒã€‚åŒ»å­¦ä¸“å®¶è¯„ä»·è¿›ä¸€æ­¥è¯å®äº†ç”Ÿæˆå›¾åƒçš„ä¸´åºŠç›¸å…³æ€§å’Œä¿çœŸåº¦ã€‚X-Diffusionæ˜¯é¦–ä¸ªèƒ½å¤Ÿä»æœ‰é™çš„2Dè¾“å…¥æ•°æ®ç”Ÿæˆè¯¦ç»†çš„3D MRIå›¾åƒçš„æ–¹æ³•ï¼Œæœ‰æœ›åŠ å¿«MRIé‡‡é›†å¹¶é™ä½ç›¸å…³æˆæœ¬ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>X-Diffusionæ˜¯ä¸€ç§æ–°å‹çš„äº¤å‰æˆªé¢æ‰©æ•£æ¨¡å‹ï¼Œèƒ½å¤Ÿä»æç¨€ç–çš„ç©ºé—´åŸŸè¾“å…¥ä¸­é‡å»ºè¯¦ç»†çš„3D MRIä½“ç§¯ã€‚</li>
<li>ä¼ ç»ŸMRIé‡å»ºæ–¹æ³•é€šå¸¸éœ€è¦å®Œæ•´çš„3Dæ‰«æï¼Œè€ŒX-Diffusionèƒ½å¤Ÿå®ç°ä»å•ä¸ªæˆ–å°‘é‡çš„2D MRIåˆ‡ç‰‡åˆ°3Dé‡å»ºçš„è½¬å˜ã€‚</li>
<li>X-Diffusionå°†MRIæ•°æ®è§†ä¸ºæ•´ä½“3Dä½“ç§¯è¿›è¡Œå»ºæ¨¡ï¼Œçªç ´äº†å¯¹MRIæ‰«æçš„ä¼ ç»Ÿå¤„ç†æ–¹å¼ã€‚</li>
<li>X-Diffusionåœ¨ä¿æŒå…³é”®è§£å‰–ç‰¹å¾æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œå¦‚è‚¿ç˜¤åˆ†å¸ƒã€è„ŠæŸ±æ›²åº¦å’Œè„‘å®¹é‡ç­‰ã€‚</li>
<li>è¯¥æ¨¡å‹åœ¨æœªè§è¿‡çš„æ•°æ®ä¸Šè¶…è¶Šäº†ç°æœ‰æ–¹æ³•çš„å®šé‡ç²¾åº¦ï¼ˆPSNRï¼‰ã€‚</li>
<li>X-Diffusionèƒ½å¤ŸæˆåŠŸé‡å»ºè†ç›–MRIï¼Œå°½ç®¡å…¶ä»…æ¥å—äº†è„‘æ•°æ®è®­ç»ƒï¼Œæ˜¾ç¤ºå‡ºè‰¯å¥½çš„æ³›åŒ–èƒ½åŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2404.19604">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-6ee3e95325fd3c67d7ad5956791eafc7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d5442c69b1523a6f6c8ab9eac63a7a1e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4a150c0c505a622d1cd8aa45655a21c5.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a3a4e56f753e6f104d072e2da06b8d6a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-51550ff5ad461c0aa6b204a94b79cd05.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-53a6d4aace4c2b3c2d0a17c3cfcf8757.jpg" align="middle">
</details>


<h1 id="-19"><a href="#-19" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-02-13/Diffusion%20Models/">https://kedreamix.github.io/Talk2Paper/Paper/2025-02-13/Diffusion%20Models/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Diffusion-Models/">
                                    <span class="chip bg-color">Diffusion Models</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-02-13/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-d2ffc8617d4b2111ad537d20e0b74a40.jpg" class="responsive-img" alt="åŒ»å­¦å›¾åƒ">
                        
                        <span class="card-title">åŒ»å­¦å›¾åƒ</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            åŒ»å­¦å›¾åƒ æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-02-13  Rapid Whole Brain Mesoscale In-vivo MR Imaging using Multi-scale   Implicit Neural Representation
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-02-13
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/" class="post-category">
                                    åŒ»å­¦å›¾åƒ
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                        <span class="chip bg-color">åŒ»å­¦å›¾åƒ</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-02-13/NeRF/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-0932b33274244d439b130360e28b13b3.jpg" class="responsive-img" alt="NeRF">
                        
                        <span class="card-title">NeRF</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            NeRF æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-02-13  Sat-DN Implicit Surface Reconstruction from Multi-View Satellite Images   with Depth and Normal Supervision
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-02-13
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/NeRF/" class="post-category">
                                    NeRF
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/NeRF/">
                        <span class="chip bg-color">NeRF</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">12289.4k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
