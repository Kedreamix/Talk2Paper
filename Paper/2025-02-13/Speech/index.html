<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Speech">
    <meta name="description" content="Speech 方向最新论文已更新，请持续关注 Update in 2025-02-13  Causal Analysis of ASR Errors for Children Quantifying the Impact of   Physiological, Cognitive, and Extrinsic Factors">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Speech | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loader页面消失采用渐隐的方式*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logo出现动画 */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//使用渐隐的方法淡出loading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//强制显示loading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">嘘~  正在从服务器偷取页面 . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>首页</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>标签</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>分类</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>归档</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="搜索" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="深色/浅色模式" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			首页
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			标签
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			分类
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			归档
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pica.zhimg.com/v2-5aefbf71a87a7476b8b3c5e427e85b14.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Speech</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- 文章内容详情 -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Speech/">
                                <span class="chip bg-color">Speech</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Speech/" class="post-category">
                                Speech
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp;
                    2025-02-13
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>更新日期:&nbsp;&nbsp;
                    2025-03-07
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>文章字数:&nbsp;&nbsp;
                    6.5k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>阅读时长:&nbsp;&nbsp;
                    26 分
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>阅读次数:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>⚠️ 以下所有内容总结都来自于 大语言模型的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p>
</blockquote>
<h1 id="2025-02-13-更新"><a href="#2025-02-13-更新" class="headerlink" title="2025-02-13 更新"></a>2025-02-13 更新</h1><h2 id="Causal-Analysis-of-ASR-Errors-for-Children-Quantifying-the-Impact-of-Physiological-Cognitive-and-Extrinsic-Factors"><a href="#Causal-Analysis-of-ASR-Errors-for-Children-Quantifying-the-Impact-of-Physiological-Cognitive-and-Extrinsic-Factors" class="headerlink" title="Causal Analysis of ASR Errors for Children: Quantifying the Impact of   Physiological, Cognitive, and Extrinsic Factors"></a>Causal Analysis of ASR Errors for Children: Quantifying the Impact of   Physiological, Cognitive, and Extrinsic Factors</h2><p><strong>Authors:Vishwanath Pratap Singh, Md. Sahidullah, Tomi Kinnunen</strong></p>
<p>The increasing use of children’s automatic speech recognition (ASR) systems has spurred research efforts to improve the accuracy of models designed for children’s speech in recent years. The current approach utilizes either open-source speech foundation models (SFMs) directly or fine-tuning them with children’s speech data. These SFMs, whether open-source or fine-tuned for children, often exhibit higher word error rates (WERs) compared to adult speech. However, there is a lack of systemic analysis of the cause of this degraded performance of SFMs. Understanding and addressing the reasons behind this performance disparity is crucial for improving the accuracy of SFMs for children’s speech. Our study addresses this gap by investigating the causes of accuracy degradation and the primary contributors to WER in children’s speech. In the first part of the study, we conduct a comprehensive benchmarking study on two self-supervised SFMs (Wav2Vec2.0 and Hubert) and two weakly supervised SFMs (Whisper and MMS) across various age groups on two children speech corpora, establishing the raw data for the causal inference analysis in the second part. In the second part of the study, we analyze the impact of physiological factors (age, gender), cognitive factors (pronunciation ability), and external factors (vocabulary difficulty, background noise, and word count) on SFM accuracy in children’s speech using causal inference. The results indicate that physiology (age) and particular external factor (number of words in audio) have the highest impact on accuracy, followed by background noise and pronunciation ability. Fine-tuning SFMs on children’s speech reduces sensitivity to physiological and cognitive factors, while sensitivity to the number of words in audio persists.   Keywords: Children’s ASR, Speech Foundational Models, Causal Inference, Physiology, Cognition, Pronunciation </p>
<blockquote>
<p>近年来，儿童自动语音识别（ASR）系统的使用越来越频繁，这刺激了努力提高针对儿童语音设计的模型的准确性。目前的方法直接使用开源语音基础模型（SFMs）或对儿童语音数据进行微调。这些SFMs，无论是开源的还是针对儿童进行微调，与成人语音相比，往往表现出更高的词错误率（WERs）。然而，缺乏对SFM性能下降的系统性分析。理解和解决这种性能差异的原因对于提高儿童语音的SFM准确性至关重要。我们的研究通过调查准确性的下降原因和儿童语音中WER的主要贡献者来弥补这一空白。在研究的第一部分中，我们对两种自监督的SFMs（Wav2Vec 2.0和Hubert）和两种弱监督的SFMs（Whisper和MMS）进行了全面的基准测试研究，并在两个儿童语音语料库中针对不同年龄组进行了评估，为第二部分中的因果推理分析提供了原始数据。在研究的第二部分中，我们利用因果推理分析了生理因素（年龄、性别）、认知因素（发音能力）和外部因素（词汇难度、背景噪音和单词数量）对儿童语音中SFM精度的影响。结果表明，生理（年龄）和特定外部因素（音频中的单词数）对精度的影响最大，其次是背景噪音和发音能力。对儿童语音数据进行微调SFMs可以减少对生理和认知因素的敏感性，但对音频中的单词数量的敏感性仍然存在。关键词：儿童ASR、语音基础模型、因果推理、生理学、认知、发音。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.08587v1">PDF</a> Submitted to Computer Speech &amp; Language</p>
<p><strong>Summary</strong></p>
<p>近年来，随着儿童自动语音识别（ASR）系统的广泛应用，针对儿童语音设计的模型准确性提升成为了研究热点。当前的研究方法包括直接使用开源语音基础模型（SFMs）或对其进行儿童语音数据的微调。然而，这些模型在识别儿童语音时相较于成人语音存在较高的词错误率（WERs）。本研究旨在填补这一空白，探究模型准确性下降的原因以及对儿童语音WER的主要贡献因素。研究分为两部分：首先，对两款自监督的SFMs和两款弱监督的SFMs在不同年龄组进行基准测试；其次，利用因果推理分析生理因素（年龄、性别）、认知因素（发音能力）和外部因素（词汇难度、背景噪音和单词数量）对模型准确性的影响。结果显示，生理因素（年龄）和特定外部因素（音频中的单词数量）对准确性的影响最大，其次是背景噪音和发音能力。对SFMs进行儿童语音微调能减少生理和认知因素的影响，但对音频中单词数量的敏感性仍然存在。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>儿童自动语音识别（ASR）系统的应用促使了对提高儿童语音模型准确性的研究。</li>
<li>当前使用的语音基础模型（SFMs）在识别儿童语音时存在较高的词错误率（WERs）。</li>
<li>研究分为基准测试与因果分析两部分，旨在找出模型准确性下降的原因。</li>
<li>生理因素（年龄）和对音频中单词数量的特定外部因素对模型准确性的影响最大。</li>
<li>背景噪音和发音能力也对模型准确性产生影响。</li>
<li>对SFMs进行儿童语音微调能减少生理和认知因素的影响。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.08587">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-efaf7a6c5cf5153226977dfe761e76a1.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-6bbcd1034fbf8490eccf527850d1d01c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-bcbacf3bb9744161f5d015f2926d4de6.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="LoRP-TTS-Low-Rank-Personalized-Text-To-Speech"><a href="#LoRP-TTS-Low-Rank-Personalized-Text-To-Speech" class="headerlink" title="LoRP-TTS: Low-Rank Personalized Text-To-Speech"></a>LoRP-TTS: Low-Rank Personalized Text-To-Speech</h2><p><strong>Authors:Łukasz Bondaruk, Jakub Kubiak</strong></p>
<p>Speech synthesis models convert written text into natural-sounding audio. While earlier models were limited to a single speaker, recent advancements have led to the development of zero-shot systems that generate realistic speech from a wide range of speakers using their voices as additional prompts. However, they still struggle with imitating non-studio-quality samples that differ significantly from the training datasets. In this work, we demonstrate that utilizing Low-Rank Adaptation (LoRA) allows us to successfully use even single recordings of spontaneous speech in noisy environments as prompts. This approach enhances speaker similarity by up to $30pp$ while preserving content and naturalness. It represents a significant step toward creating truly diverse speech corpora, that is crucial in all speech-related tasks. </p>
<blockquote>
<p>语音合成模型将书面文字转换为自然音频。虽然早期模型仅限于单一发言人，但最近的进步已经开发出零样本系统，这些系统可以使用各种发言人的声音作为额外提示来生成逼真的语音。然而，它们仍然难以模仿与训练数据集差异很大的非工作室质量样本。在这项工作中，我们证明利用低秩适应（LoRA）技术，即使在使用单一的环境嘈杂语音样本作为提示时，也能成功应用。这种方法在提高说话人相似度方面提高了高达30个百分点，同时保持了内容和自然度。这朝着创建真正多样化的语音语料库迈出了重要的一步，这在所有与语音相关的任务中都至关重要。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.07562v1">PDF</a> </p>
<p><strong>摘要</strong></p>
<p>本研究探讨语音合成模型的最新发展。早期模型仅支持单一发言人，但现在的零样本系统可以利用不同发言人的声音进行生成。尽管如此，它们仍然难以模仿非专业录音样本。本研究展示低秩适应（LoRA）技术如何成功使用单一的自然环境录音作为提示，增强发言人相似性达30pp，同时保持内容和自然性。这标志着在创建真正多样化的语音语料库方面取得了重要进展，这对所有语音任务都至关重要。</p>
<p><strong>关键见解</strong></p>
<ol>
<li>语音合成模型可将文字转换为自然的声音。</li>
<li>早期模型受限于单一发言人，但现代系统通过使用不同发言人的声音进行生成，实现了零样本。</li>
<li>当前模型在模仿非专业录音样本方面存在挑战。</li>
<li>低秩适应（LoRA）技术允许使用单一的自然环境录音作为提示。</li>
<li>LoRA技术增强了发言人相似性达30pp。</li>
<li>LoRA技术在保持内容和自然性的同时，增强了语音合成的质量。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.07562">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-fd20ee549340c6b06601076bec73516b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-87d800ca0a132b127d32991befd3e24a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-9ddad5f64d492bd3e508413ba04c3284.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-b69341ba08bf5ffe0d6589f81bee6a5d.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-5aefbf71a87a7476b8b3c5e427e85b14.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-22c82fd4e9d13167cbf50aef22bb57a8.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-1f4ac8ee225143b28d8f271d170c009b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-56960cea8812d8d75034be2fd6e2e479.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="VINP-Variational-Bayesian-Inference-with-Neural-Speech-Prior-for-Joint-ASR-Effective-Speech-Dereverberation-and-Blind-RIR-Identification"><a href="#VINP-Variational-Bayesian-Inference-with-Neural-Speech-Prior-for-Joint-ASR-Effective-Speech-Dereverberation-and-Blind-RIR-Identification" class="headerlink" title="VINP: Variational Bayesian Inference with Neural Speech Prior for Joint   ASR-Effective Speech Dereverberation and Blind RIR Identification"></a>VINP: Variational Bayesian Inference with Neural Speech Prior for Joint   ASR-Effective Speech Dereverberation and Blind RIR Identification</h2><p><strong>Authors:Pengyu Wang, Ying Fang, Xiaofei Li</strong></p>
<p>Reverberant speech, denoting the speech signal degraded by the process of reverberation, contains crucial knowledge of both anechoic source speech and room impulse response (RIR). This work proposes a variational Bayesian inference (VBI) framework with neural speech prior (VINP) for joint speech dereverberation and blind RIR identification. In VINP, a probabilistic signal model is constructed in the time-frequency (T-F) domain based on convolution transfer function (CTF) approximation. For the first time, we propose using an arbitrary discriminative dereverberation deep neural network (DNN) to predict the prior distribution of anechoic speech within a probabilistic model. By integrating both reverberant speech and the anechoic speech prior, VINP yields the maximum a posteriori (MAP) and maximum likelihood (ML) estimations of the anechoic speech spectrum and CTF filter, respectively. After simple transformations, the waveforms of anechoic speech and RIR are estimated. Moreover, VINP is effective for automatic speech recognition (ASR) systems, which sets it apart from most deep learning (DL)-based single-channel dereverberation approaches. Experiments on single-channel speech dereverberation demonstrate that VINP reaches an advanced level in most metrics related to human perception and displays unquestionable state-of-the-art (SOTA) performance in ASR-related metrics. For blind RIR identification, experiments indicate that VINP attains the SOTA level in blind estimation of reverberation time at 60 dB (RT60) and direct-to-reverberation ratio (DRR). Codes and audio samples are available online. </p>
<blockquote>
<p>带有混响的语音信号包含了关于无混响源语音和房间冲击响应（RIR）的关键知识。这项工作提出了一种基于神经语音先验（VINP）的变贝叶斯推断（VBI）框架，用于联合语音去混响和盲RIR识别。在VINP中，基于卷积传递函数（CTF）近似值在时频（T-F）域中构建概率信号模型。我们首次提出使用任意判别去混响深度神经网络（DNN）在概率模型中预测无混响语音的先验分布。通过整合带混响语音和无混响语音先验，VINP获得无混响语音谱和CTF滤波器的最大后验（MAP）和最大似然（ML）估计。经过简单变换后，可以估算出无混响语音和RIR的波形。此外，VINP对于自动语音识别（ASR）系统也有效，这与大多数基于深度学习的单通道去混响方法形成了鲜明对比。在单通道语音去混响方面的实验表明，VINP在人类感知相关的大多数指标上达到了先进水平，并在与ASR相关的指标中表现出无可争议的最先进技术水平。对于盲RIR识别，实验表明VINP在60分贝（RT60）的混响时间盲估计和直接-混响比（DRR）方面达到了最先进水平。代码和音频样本可在网上获得。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.07205v1">PDF</a> Submitted to IEEE&#x2F;ACM Trans. on TASLP</p>
<p><strong>摘要</strong></p>
<p>该文本介绍了基于神经语音先验的变分贝叶斯推理框架，用于联合语音去混响和盲房间脉冲响应识别。该框架在时频域构建了基于卷积传递函数近似的概率信号模型。首次提出使用任意判别去混响深度神经网络预测概率模型中的无混响语音先验分布。通过结合混响语音和无混响语音先验，该框架获得了无混响语音谱和卷积传递函数滤波器的最大后验和最大似然估计。此外，该框架对提高自动语音识别系统的性能有效，区别于大多数基于深度学习的单通道去混响方法。实验表明，该框架在与人类感知相关的大多数指标上达到先进水平，并且在语音识别相关指标上表现出最佳性能。对于盲房间脉冲响应识别，实验显示该框架在60分贝的混响时间和直接混响比率方面达到了最佳水平。</p>
<p><strong>关键见解</strong></p>
<ol>
<li>文本介绍了基于神经语音先验的变分贝叶斯推理框架，旨在联合处理语音去混响和盲房间脉冲响应识别。</li>
<li>构建了概率信号模型，该模型在时频域基于卷积传递函数近似。</li>
<li>首次提出使用深度神经网络预测无混响语音的先验分布。</li>
<li>该框架能获得无混响语音谱和卷积传递函数的最大后验和最大似然估计。</li>
<li>框架对自动语音识别系统的性能提升有效。</li>
<li>实验表明，该框架在人类感知和语音识别相关指标上达到最佳性能。</li>
<li>对于盲房间脉冲响应识别，该框架在混响时间的估计上达到最佳水平。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.07205">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-f23d5c250e1b6ea5ba3b5c9b903dbbb9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ea812fe21f4940fde73a93158c1d8760.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-ab8d8083f73aecbf707ea2724ad84601.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-97d39fb86c8b2300c823bbe41181f3fc.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="GraNNite-Enabling-High-Performance-Execution-of-Graph-Neural-Networks-on-Resource-Constrained-Neural-Processing-Units"><a href="#GraNNite-Enabling-High-Performance-Execution-of-Graph-Neural-Networks-on-Resource-Constrained-Neural-Processing-Units" class="headerlink" title="GraNNite: Enabling High-Performance Execution of Graph Neural Networks   on Resource-Constrained Neural Processing Units"></a>GraNNite: Enabling High-Performance Execution of Graph Neural Networks   on Resource-Constrained Neural Processing Units</h2><p><strong>Authors:Arghadip Das, Shamik Kundu, Arnab Raha, Soumendu Ghosh, Deepak Mathaikutty, Vijay Raghunathan</strong></p>
<p>Graph Neural Networks (GNNs) are vital for learning from graph-structured data, enabling applications in network analysis, recommendation systems, and speech analytics. Deploying them on edge devices like client PCs and laptops enhances real-time processing, privacy, and cloud independence. GNNs aid Retrieval-Augmented Generation (RAG) for Large Language Models (LLMs) and enable event-based vision tasks. However, irregular memory access, sparsity, and dynamic structures cause high latency and energy overhead on resource-constrained devices. While modern edge processors integrate CPUs, GPUs, and NPUs, NPUs designed for data-parallel tasks struggle with irregular GNN computations. We introduce GraNNite, the first hardware-aware framework optimizing GNN execution on commercial-off-the-shelf (COTS) SOTA DNN accelerators via a structured three-step methodology: (1) enabling NPU execution, (2) optimizing performance, and (3) trading accuracy for efficiency gains. Step 1 employs GraphSplit for workload distribution and StaGr for static aggregation, while GrAd and NodePad handle dynamic graphs. Step 2 boosts performance using EffOp for control-heavy tasks and GraSp for sparsity exploitation. Graph Convolution optimizations PreG, SymG, and CacheG reduce redundancy and memory transfers. Step 3 balances quality versus efficiency, where QuantGr applies INT8 quantization, and GrAx1, GrAx2, and GrAx3 accelerate attention, broadcast-add, and SAGE-max aggregation. On Intel Core Ultra AI PCs, GraNNite achieves 2.6X to 7.6X speedups over default NPU mappings and up to 8.6X energy gains over CPUs and GPUs, delivering 10.8X and 6.7X higher performance than CPUs and GPUs, respectively, across GNN models. </p>
<blockquote>
<p>图神经网络（GNNs）对于从图结构数据中学习至关重要，可应用于网络分析、推荐系统和语音分析等应用。在客户端PC和笔记本电脑等边缘设备上部署它们，增强了实时处理、隐私和云独立性。GNNs有助于大型语言模型的检索增强生成（RAG），并可用于基于事件的视觉任务。然而，不规则内存访问、稀疏性和动态结构会在资源受限的设备上造成高延迟和能源开销。尽管现代边缘处理器融合了CPU、GPU和NPU，但为数据并行任务设计的NPU在处理不规则GNN计算时却表现挣扎。我们引入了GraNNite，这是第一个硬件感知框架，通过结构化三步方法优化在商用现货（COTS）SOTA DNN加速器上执行GNN：（1）实现NPU执行，（2）优化性能，（3）以效率增益换取准确性。第1步采用GraphSplit进行工作量分配和StaGr进行静态聚合，而GrAd和NodePad处理动态图。第2步使用EffOp控制密集型任务并提高性能，并使用GraSp进行稀疏性利用。图卷积优化PreG、SymG和CacheG减少冗余和内存传输。第3步平衡质量与效率，其中QuantGr应用INT8量化，GrAx1、GrAx2和GrAx3加速注意力、广播加法和SAGE-max聚合。在Intel Core Ultra AI PC上，GraNNite相对于默认的NPU映射实现了2.6倍至7.6倍的加速，并且相对于CPU和GPU实现了高达8.6倍的能源效益。相对于CPU和GPU，GraNNite在GNN模型上的性能分别提高了10.8倍和6.7倍。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.06921v1">PDF</a> </p>
<p><strong>摘要</strong></p>
<p>图神经网络（GNNs）对于从图形结构数据中学习至关重要，被广泛应用于网络分析、推荐系统和语音分析等领域。它们在边缘设备上部署有助于提升实时处理性能、隐私保护和云独立性。然而，由于不规则内存访问、稀疏性和动态结构等问题，在资源受限的设备上执行GNNs会产生高延迟和能源开销。GraNNite是一个硬件感知框架，通过结构化三步方法优化在现成的最先进的深度神经网络加速器上执行GNNs的任务。三步包括启用NPU执行、优化性能和权衡精度与效率收益。它在Intel Core Ultra AI PC上取得了显著的速度和能源效率提升。</p>
<p><strong>关键见解</strong></p>
<ol>
<li>图神经网络（GNNs）在图形结构数据学习中起关键作用，广泛应用于网络分析、推荐系统和语音分析。</li>
<li>在边缘设备上部署GNNs可提升实时处理、隐私保护和云独立性。</li>
<li>GNNs在资源受限的设备上执行面临高延迟和能源开销的挑战。</li>
<li>GraNNite是首个硬件感知框架，优化在深度神经网络加速器上执行GNNs的任务。</li>
<li>GraNNite采用结构化三步方法：启用NPU执行、优化性能和权衡精度与效率。</li>
<li>GraNNite在Intel Core Ultra AI PC上实现了显著的速度提升和能源效率。</li>
<li>GraNNite相对于默认NPU映射实现了2.6倍至7.6倍的加速，相对于CPU和GPU实现了最高8.6倍的能源效率提升。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.06921">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-d262f11264f9bef7c45e9adf61298298.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-53d687b4c5952e5be747257e6b2112bc.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-7c6a58116ad970a058cd6b537a46e6a1.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-8accd10d6adf124e984ee480433b28ff.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f9d98d96914610b4e087a91470588778.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-c8cb2b5353fcb1b1d3dd76c2e09010b5.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="mWhisper-Flamingo-for-Multilingual-Audio-Visual-Noise-Robust-Speech-Recognition"><a href="#mWhisper-Flamingo-for-Multilingual-Audio-Visual-Noise-Robust-Speech-Recognition" class="headerlink" title="mWhisper-Flamingo for Multilingual Audio-Visual Noise-Robust Speech   Recognition"></a>mWhisper-Flamingo for Multilingual Audio-Visual Noise-Robust Speech   Recognition</h2><p><strong>Authors:Andrew Rouditchenko, Samuel Thomas, Hilde Kuehne, Rogerio Feris, James Glass</strong></p>
<p>Audio-Visual Speech Recognition (AVSR) combines lip-based video with audio and can improve performance in noise, but most methods are trained only on English data. One limitation is the lack of large-scale multilingual video data, which makes it hard hard to train models from scratch. In this work, we propose mWhisper-Flamingo for multilingual AVSR which combines the strengths of a pre-trained audio model (Whisper) and video model (AV-HuBERT). To enable better multi-modal integration and improve the noisy multilingual performance, we introduce decoder modality dropout where the model is trained both on paired audio-visual inputs and separate audio&#x2F;visual inputs. mWhisper-Flamingo achieves state-of-the-art WER on MuAViC, an AVSR dataset of 9 languages. Audio-visual mWhisper-Flamingo consistently outperforms audio-only Whisper on all languages in noisy conditions. </p>
<blockquote>
<p>视听语音识别（AVSR）结合了基于唇语的视频和音频，可以提高噪声中的性能，但大多数方法仅针对英语数据进行训练。一个局限性在于缺乏大规模的多语言视频数据，这使得从头开始训练模型变得困难。在这项工作中，我们针对多语言AVSR提出了mWhisper-Flamingo，它结合了预训练音频模型（Whisper）和视频模型（AV-HuBERT）的优点。为了实更好的多模式集成并提高嘈杂的多语言性能，我们引入了解码器模态丢弃训练策略，即模型同时接受配对音频视觉输入和单独的音频&#x2F;视觉输入。mWhisper-Flamingo在MuAViC上实现了最先进的词错误率（WER），MuAViC是一个包含9种语言的AVSR数据集。在嘈杂条件下，视听mWhisper-Flamingo在所有语言上的性能均优于仅使用音频的Whisper。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.01547v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文介绍了音频视觉语音识别（AVSR）结合唇语视频与音频，在噪声环境下可提高性能。然而，大多数方法仅针对英语数据进行训练，缺乏大规模多语言视频数据，使得从头开始训练模型变得困难。为此，本文提出了多语言AVSR模型mWhisper-Flamingo，结合了预训练音频模型（Whisper）和视频模型（AV-HuBERT）的优势。通过引入解码器模态丢弃（decoder modality dropout），使模型能够在配对音频视觉输入和单独音频&#x2F;视觉输入上进行训练，实现更好的多模态集成并改善噪声多语言环境性能。在MuAViC数据集上，mWhisper-Flamingo实现了最先进的词错误率（WER），该数据集包含9种语言。在噪声条件下，视听mWhisper-Flamingo在所有语言上的表现均优于仅音频的Whisper。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>音频视觉语音识别（AVSR）结合了唇语视频与音频，以改善噪声环境下的性能。</li>
<li>现有方法主要针对英语数据训练，缺乏多语言视频数据，导致训练困难。</li>
<li>提出mWhisper-Flamingo模型，结合预训练音频模型（Whisper）和视频模型（AV-HuBERT）的优势。</li>
<li>通过引入解码器模态丢弃（decoder modality dropout），提高多模态集成效果，并改善噪声多语言环境性能。</li>
<li>mWhisper-Flamingo在MuAViC数据集上实现最先进的词错误率（WER），涵盖9种语言。</li>
<li>在噪声条件下，mWhisper-Flamingo在所有语言上的表现均优于仅使用音频的识别模型。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.01547">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-1a140b0efb0f9b5966deb29396b731a7.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-567837bd940d438cea9f4dc7293a1031.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9cbc394c7466c9428601ccf9430319db.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7ce278697a9fb4b36106dd4af2e1dad8.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-86dd878b9a7196772d26758085878d8d.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="MT2KD-Towards-A-General-Purpose-Encoder-for-Speech-Speaker-and-Audio-Events"><a href="#MT2KD-Towards-A-General-Purpose-Encoder-for-Speech-Speaker-and-Audio-Events" class="headerlink" title="MT2KD: Towards A General-Purpose Encoder for Speech, Speaker, and Audio   Events"></a>MT2KD: Towards A General-Purpose Encoder for Speech, Speaker, and Audio   Events</h2><p><strong>Authors:Xiaoyu Yang, Qiujia Li, Chao Zhang, Phil Woodland</strong></p>
<p>With the advances in deep learning, the performance of end-to-end (E2E) single-task models for speech and audio processing has been constantly improving. However, it is still challenging to build a general-purpose model with high performance on multiple tasks, since different speech and audio processing tasks usually require different training data, input features, or model architectures to achieve optimal performance. In this work, MT2KD, a novel two-stage multi-task learning framework is proposed to build a general-purpose speech and audio encoder that jointly performs three fundamental tasks: automatic speech recognition (ASR), audio tagging (AT) and speaker verification (SV). In the first stage, multi-teacher knowledge distillation (KD) is applied to align the feature spaces of three single-task high-performance teacher encoders into a single student encoder using the same unlabelled data. In the second stage, multi-task supervised fine-tuning is carried out by initialising the model from the first stage and training on the separate labelled data of each single task. Experiments demonstrate that the proposed multi-task training pipeline significantly outperforms a baseline model trained with multi-task learning from scratch. The final system achieves good performance on ASR, AT and SV: with less than 4% relative word-error-rate increase on ASR, only 1.9 lower mean averaged precision on AT and 0.23% absolute higher equal error rate on SV compared to the best-performing single-task encoders, using only a 66M total model parameters. </p>
<blockquote>
<p>随着深度学习的发展，端到端（E2E）单任务模型在语音和音频处理方面的性能不断提升。然而，构建能够在多个任务上实现高性能的通用模型仍然是一个挑战，因为不同的语音和音频处理任务通常需要不同的训练数据、输入特征或模型架构来实现最佳性能。在此工作中，提出了MT2KD这一新型两阶段多任务学习框架，用于构建通用语音和音频编码器，该编码器可联合执行三个基本任务：自动语音识别（ASR）、音频标签（AT）和说话人验证（SV）。在第一阶段，应用多教师知识蒸馏（KD）技术，使用相同的无标签数据将三个高性能单任务教师编码器的特征空间对齐到一个单一的学生编码器中。在第二阶段，通过初始化第一阶段的模型，并在每个单独任务的标记数据上进行训练，进行多任务监督微调。实验表明，所提出的多任务训练流程显著优于从头开始进行多任务学习的基线模型。最终系统在ASR、AT和SV方面取得了良好的性能：与最佳性能的单任务编码器相比，ASR的相对词错误率增加不到4%，AT的平均精度仅低1.9，SV的等错误率绝对提高0.23%，而总模型参数仅为66M。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2409.17010v2">PDF</a> Disclaimer: This work has been submitted to the IEEE for possible   publication</p>
<p><strong>摘要</strong></p>
<p>随着深度学习的发展，端到端单一任务模型在语音和音频处理方面的性能不断提升。然而，构建能在多个任务上表现良好的通用模型仍然是一个挑战。不同的语音和音频处理任务通常需要不同的训练数据、输入特征或模型架构来达到最佳性能。本研究提出了MT2KD，一种新型两阶段多任务学习框架，用于构建通用语音和音频编码器，可联合执行三个基本任务：自动语音识别（ASR）、音频标签（AT）和说话人验证（SV）。第一阶段采用多教师知识蒸馏（KD），使用相同的无标签数据，将三个高性能教师编码器的特征空间对齐到一个单一的学生编码器。第二阶段，通过初始化第一阶段的模型，并在每个单一任务的标记数据上进行训练，进行多任务监督微调。实验表明，与从头开始训练的多任务学习基线模型相比，所提出的多任务训练流程具有显著优势。最终系统在与最佳的单任务编码器相比时，在ASR上的相对单词错误率增加不到4%，在AT上的平均精度仅下降1.9，在SV上的等错误率绝对提高仅为0.23%，而总模型参数仅为66M。</p>
<p><strong>关键见解</strong></p>
<ol>
<li>端到端单一任务模型在语音和音频处理方面的性能随深度学习的发展而提升。</li>
<li>构建能在多个任务上表现良好的通用模型仍然是一个挑战。</li>
<li>不同语音和音频处理任务需要不同的训练数据、输入特征和模型架构来达到最佳性能。</li>
<li>研究提出了MT2KD两阶段多任务学习框架，包括多教师知识蒸馏和多任务监督微调。</li>
<li>使用无标签数据对齐教师编码器的特征空间，再初始化到单一学生编码器。</li>
<li>与基线模型相比，所提出的多任务训练流程具有显著优势。</li>
<li>最终系统在与最佳单任务编码器相比时，在ASR、AT和SV任务上的性能损失较小。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2409.17010">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-8068142247bf8ad37feb287a366eca7d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-96870bebe383a7e026db69c2b22a6bdc.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-58b3408412588edeb8c16371af7cb616.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-15e67e0c5ed3d31f2504af5082533e69.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        文章作者:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        文章链接:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-02-13/Speech/">https://kedreamix.github.io/Talk2Paper/Paper/2025-02-13/Speech/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        版权声明:
                    </i>
                </span>
                <span class="reprint-info">
                    本博客所有文章除特別声明外，均采用
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    许可协议。转载请注明来源
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Speech/">
                                    <span class="chip bg-color">Speech</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;上一篇</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-02-13/GAN/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-3f2cde5b4a6985306796625f04326668.jpg" class="responsive-img" alt="GAN">
                        
                        <span class="card-title">GAN</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            GAN 方向最新论文已更新，请持续关注 Update in 2025-02-13  BF-GAN Development of an AI-driven Bubbly Flow Image Generation Model   Using Generative Adversarial Networks
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-02-13
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/GAN/" class="post-category">
                                    GAN
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/GAN/">
                        <span class="chip bg-color">GAN</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                下一篇&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-02-13/%E5%8C%BB%E5%AD%A6%E5%BD%B1%E5%83%8F_Breast%20Ultrasound/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-4abce4bfa95bd288485184f519066a13.jpg" class="responsive-img" alt="医学影像/Breast Ultrasound">
                        
                        <span class="card-title">医学影像/Breast Ultrasound</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            医学影像/Breast Ultrasound 方向最新论文已更新，请持续关注 Update in 2025-02-13  Ultrasound Image Generation using Latent Diffusion Models
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-02-13
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E5%8C%BB%E5%AD%A6%E5%BD%B1%E5%83%8F-Breast-Ultrasound/" class="post-category">
                                    医学影像/Breast Ultrasound
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%BD%B1%E5%83%8F-Breast-Ultrasound/">
                        <span class="chip bg-color">医学影像/Breast Ultrasound</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- 代码块功能依赖 -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- 代码语言 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- 代码块复制 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- 代码块收缩 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC 悬浮按钮. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* 修复文章卡片 div 的宽度. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // 切换TOC目录展开收缩的相关操作.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;站点总字数:&nbsp;<span
                        class="white-color">17259.4k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- 运行天数提醒. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // 区分是否有年份.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = '本站已运行 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                daysTip = '本站已運行 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = '本站已运行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = '本站已運行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="关注我的知乎: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">知</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- 搜索遮罩框 -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span>
            <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- 白天和黑夜主题 -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- 回到顶部按钮 -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // 只在桌面版网页启用特效
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- 雪花特效 -->
    

    <!-- 鼠标星星特效 -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--腾讯兔小巢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- 动态标签 -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Σ(っ °Д °;)っ诶，页面崩溃了嘛？", clearTimeout(st)) : (document.title = "φ(゜▽゜*)♪咦，又好了！", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
