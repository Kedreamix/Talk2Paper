<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="医学图像">
    <meta name="description" content="医学图像 方向最新论文已更新，请持续关注 Update in 2025-02-13  Rapid Whole Brain Mesoscale In-vivo MR Imaging using Multi-scale   Implicit Neural Representation">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>医学图像 | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loader页面消失采用渐隐的方式*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logo出现动画 */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//使用渐隐的方法淡出loading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//强制显示loading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">嘘~  正在从服务器偷取页面 . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>首页</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>标签</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>分类</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>归档</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="搜索" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="深色/浅色模式" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			首页
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			标签
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			分类
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			归档
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-d2ffc8617d4b2111ad537d20e0b74a40.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">医学图像</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- 文章内容详情 -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                                <span class="chip bg-color">医学图像</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/" class="post-category">
                                医学图像
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp;
                    2025-02-13
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>更新日期:&nbsp;&nbsp;
                    2025-03-07
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>文章字数:&nbsp;&nbsp;
                    19.1k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>阅读时长:&nbsp;&nbsp;
                    79 分
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>阅读次数:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>⚠️ 以下所有内容总结都来自于 大语言模型的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p>
</blockquote>
<h1 id="2025-02-13-更新"><a href="#2025-02-13-更新" class="headerlink" title="2025-02-13 更新"></a>2025-02-13 更新</h1><h2 id="Rapid-Whole-Brain-Mesoscale-In-vivo-MR-Imaging-using-Multi-scale-Implicit-Neural-Representation"><a href="#Rapid-Whole-Brain-Mesoscale-In-vivo-MR-Imaging-using-Multi-scale-Implicit-Neural-Representation" class="headerlink" title="Rapid Whole Brain Mesoscale In-vivo MR Imaging using Multi-scale   Implicit Neural Representation"></a>Rapid Whole Brain Mesoscale In-vivo MR Imaging using Multi-scale   Implicit Neural Representation</h2><p><strong>Authors:Jun Lyu, Lipeng Ning, William Consagra, Qiang Liu, Richard J. Rushmore, Berkin Bilgic, Yogesh Rathi</strong></p>
<p>Purpose: To develop and validate a novel image reconstruction technique using implicit neural representations (INR) for multi-view thick-slice acquisitions while reducing the scan time but maintaining high signal-to-noise ratio (SNR). Methods: We propose Rotating-view super-resolution (ROVER)-MRI, an unsupervised neural network-based algorithm designed to reconstruct MRI data from multi-view thick slices, effectively reducing scan time by 2-fold while maintaining fine anatomical details. We compare our method to both bicubic interpolation and the current state-of-the-art regularized least-squares super-resolution reconstruction (LS-SRR) technique. Validation is performed using ground-truth ex-vivo monkey brain data, and we demonstrate superior reconstruction quality across several in-vivo human datasets. Notably, we achieve the reconstruction of a whole human brain in-vivo T2-weighted image with an unprecedented 180{\mu}m isotropic spatial resolution, accomplished in just 17 minutes of scan time on a 7T MRI scanner. Results: ROVER-MRI outperformed LS-SRR method in terms of reconstruction quality with 22.4% lower relative error (RE) and 7.5% lower full-width half maximum (FWHM) indicating better preservation of fine structural details in nearly half the scan time. Conclusion: ROVER-MRI offers an efficient and robust approach for mesoscale MR imaging, enabling rapid, high-resolution whole-brain scans. Its versatility holds great promise for research applications requiring anatomical details and time-efficient imaging. </p>
<blockquote>
<p>目的：开发并验证一种新型图像重建技术，利用隐神经表示（INR）进行多视角厚层采集，在缩短扫描时间的同时保持较高的信噪比（SNR）。</p>
</blockquote>
<p>方法：我们提出旋转视图超分辨率（ROVER）-MRI，这是一种基于无监督神经网络算法的MRI数据重建方法，可从多视角厚层重建数据，通过有效手段将扫描时间减少一倍，同时保留精细的解剖细节。我们将该方法与双三次插值以及当前最先进的正则化最小二乘超分辨率重建（LS-SRR）技术进行比较。使用真实离体猴子脑部数据进行验证，并在若干体内人体数据集上展示了更高的重建质量。值得注意的是，我们在7T MRI扫描仪上仅用了17分钟的扫描时间，就实现了体内整个人脑T2加权图像的重建，具有前所未有的180μm各向同性空间分辨率。</p>
<p>结果：ROVER-MRI在重建质量方面优于LS-SRR方法，相对误差（RE）降低22.4%，全宽半高（FWHM）降低7.5%，表明在近乎减半的扫描时间内更好地保留了精细结构细节。</p>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.08634v1">PDF</a> </p>
<p><strong>Summary</strong><br>    本研究开发并验证了一种使用隐神经表示（INR）的新型图像重建技术，用于多视角厚切片采集，可在减少扫描时间的同时保持高信噪比（SNR）。提出的旋转视图超分辨率（ROVER）-MRI算法可有效从多视角厚切片重建MRI数据，将扫描时间减少一倍，同时保持精细的解剖细节。研究使用真实离体猴脑数据验证了该方法，并在多个体内人脑数据集上展示了卓越的重建质量。特别地，该研究在7T MRI扫描仪上仅用了17分钟就重建了一个整个人脑的T2加权图像，具有前所未有的180μm等距空间分辨率。结果表明，ROVER-MRI在重建质量方面优于现有的最小二乘超分辨率重建（LS-SRR）方法，具有更低的相对误差（RE）和全宽半最大值（FWHM），能够在几乎减半的扫描时间内更好地保留精细结构细节。总体而言，ROVER-MRI为快速高效的大规模MR成像提供了可能，在需要解剖细节和时间效率要求的领域具有巨大潜力。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>研究提出了一种基于隐神经表示（INR）的新型图像重建技术——旋转视图超分辨率（ROVER）-MRI。</li>
<li>ROVER-MRI旨在从多视角厚切片重建MRI数据，可在减少扫描时间的同时保持高质量图像。</li>
<li>相较于传统的图像重建方法和现有的最小二乘超分辨率重建（LS-SRR）技术，ROVER-MRI在重建质量上表现更优秀。</li>
<li>ROVER-MRI成功在多个体内人脑数据集上实现了高质量重建，并且在离体猴脑数据上得到了验证。</li>
<li>研究实现了在7T MRI扫描仪上仅17分钟内完成的高分辨率整个人脑T2加权图像的重建。</li>
<li>ROVER-MRI具有更低的相对误差（RE）和全宽半最大值（FWHM），能更好地保留图像中的精细结构细节。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.08634">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-9f7805d32882c2c5c3da40748041d29b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-57a5ac7d4d848218e6b472d30fb7a4d2.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Ultrasound-Image-Generation-using-Latent-Diffusion-Models"><a href="#Ultrasound-Image-Generation-using-Latent-Diffusion-Models" class="headerlink" title="Ultrasound Image Generation using Latent Diffusion Models"></a>Ultrasound Image Generation using Latent Diffusion Models</h2><p><strong>Authors:Benoit Freiche, Anthony El-Khoury, Ali Nasiri-Sarvi, Mahdi S. Hosseini, Damien Garcia, Adrian Basarab, Mathieu Boily, Hassan Rivaz</strong></p>
<p>Diffusion models for image generation have been a subject of increasing interest due to their ability to generate diverse, high-quality images. Image generation has immense potential in medical imaging because open-source medical images are difficult to obtain compared to natural images, especially for rare conditions. The generated images can be used later to train classification and segmentation models. In this paper, we propose simulating realistic ultrasound (US) images by successive fine-tuning of large diffusion models on different publicly available databases. To do so, we fine-tuned Stable Diffusion, a state-of-the-art latent diffusion model, on BUSI (Breast US Images) an ultrasound breast image dataset. We successfully generated high-quality US images of the breast using simple prompts that specify the organ and pathology, which appeared realistic to three experienced US scientists and a US radiologist. Additionally, we provided user control by conditioning the model with segmentations through ControlNet. We will release the source code at <a target="_blank" rel="noopener" href="http://code.sonography.ai/">http://code.sonography.ai/</a> to allow fast US image generation to the scientific community. </p>
<blockquote>
<p>图像生成中的扩散模型因其能够生成多样化、高质量图像而日益受到关注。医学成像领域具有巨大的潜力，因为与自然图像相比，获取开源医学图像更加困难，尤其是对于罕见病症。生成的图像之后可用于训练和分割模型。在本文中，我们通过连续微调大型扩散模型来模拟逼真的超声（US）图像，这些模型在不同可用的公开数据库上具有良好的性能。为此，我们对最新的潜在扩散模型Stable Diffusion进行了微调，以在BUSI（乳腺超声图像）数据集上生成乳腺超声图像。我们成功使用简单的提示（指定器官和病理）生成高质量的乳腺超声图像，这些图像对三位经验丰富的超声科学家和一位超声放射科医生来说看起来非常逼真。此外，我们通过使用ControlNet对模型进行分段控制，提供了用户控制功能。我们将在<a target="_blank" rel="noopener" href="http://code.sonography.ai/%E4%B8%8A%E5%8F%91%E5%B8%83%E6%BA%90%E4%BB%A3%E7%A0%81%EF%BC%8C%E4%BB%A5%E5%85%81%E8%AE%B8%E7%A7%91%E5%AD%A6%E7%95%8C%E5%BF%AB%E9%80%9F%E7%94%9F%E6%88%90%E8%B6%85%E5%A3%B0%E5%9B%BE%E5%83%8F%E3%80%82">http://code.sonography.ai/上发布源代码，以允许科学界快速生成超声图像。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.08580v1">PDF</a> 6 pages conference paper for SPIE medical imaging</p>
<p><strong>Summary</strong><br>     本文利用扩散模型生成模拟的超声图像，通过在不同公共数据库上对大型扩散模型进行微调实现。使用稳定扩散模型对乳腺超声图像数据集进行微调，成功生成高质量乳腺超声图像。生成的图像具有真实感，并可通过控制模型与分割相结合实现用户控制。将在[链接地址]发布源代码，供科学界快速生成超声图像。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>扩散模型用于图像生成正受到越来越多的关注，具有生成多样化、高质量图像的能力。</li>
<li>在医学成像领域，图像生成潜力巨大，特别是在获取公开医疗图像困难的情况下，能为罕见病症生成图像。</li>
<li>本文提出通过微调大型扩散模型来模拟真实的超声图像，使用了现有的公开数据库。</li>
<li>利用稳定扩散模型对乳腺超声图像数据集进行微调，成功生成高质量的乳腺超声图像。</li>
<li>生成的图像具有真实感，能够欺骗经验丰富的超声科学家和放射科医生。</li>
<li>用户可以通过控制模型与分割的结合来控制模型的生成结果。</li>
<li>源码将在指定链接发布，以供科学界使用，促进快速超声图像生成。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.08580">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-3b0bc59a15b3a1e7436f31c2dbccb398.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-4abce4bfa95bd288485184f519066a13.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a4753911170ebd2a5eb70c29d43b0e53.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-775ff4a4932e08c1a181e0737e5d4366.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-8e287699bc107b2ef3558225744c2b8f.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Brain-Latent-Progression-Individual-based-Spatiotemporal-Disease-Progression-on-3D-Brain-MRIs-via-Latent-Diffusion"><a href="#Brain-Latent-Progression-Individual-based-Spatiotemporal-Disease-Progression-on-3D-Brain-MRIs-via-Latent-Diffusion" class="headerlink" title="Brain Latent Progression: Individual-based Spatiotemporal Disease   Progression on 3D Brain MRIs via Latent Diffusion"></a>Brain Latent Progression: Individual-based Spatiotemporal Disease   Progression on 3D Brain MRIs via Latent Diffusion</h2><p><strong>Authors:Lemuel Puglisi, Daniel C. Alexander, Daniele Ravì</strong></p>
<p>The growing availability of longitudinal Magnetic Resonance Imaging (MRI) datasets has facilitated Artificial Intelligence (AI)-driven modeling of disease progression, making it possible to predict future medical scans for individual patients. However, despite significant advancements in AI, current methods continue to face challenges including achieving patient-specific individualization, ensuring spatiotemporal consistency, efficiently utilizing longitudinal data, and managing the substantial memory demands of 3D scans. To address these challenges, we propose Brain Latent Progression (BrLP), a novel spatiotemporal model designed to predict individual-level disease progression in 3D brain MRIs. The key contributions in BrLP are fourfold: (i) it operates in a small latent space, mitigating the computational challenges posed by high-dimensional imaging data; (ii) it explicitly integrates subject metadata to enhance the individualization of predictions; (iii) it incorporates prior knowledge of disease dynamics through an auxiliary model, facilitating the integration of longitudinal data; and (iv) it introduces the Latent Average Stabilization (LAS) algorithm, which (a) enforces spatiotemporal consistency in the predicted progression at inference time and (b) allows us to derive a measure of the uncertainty for the prediction. We train and evaluate BrLP on 11,730 T1-weighted (T1w) brain MRIs from 2,805 subjects and validate its generalizability on an external test set comprising 2,257 MRIs from 962 subjects. Our experiments compare BrLP-generated MRI scans with real follow-up MRIs, demonstrating state-of-the-art accuracy compared to existing methods. The code is publicly available at: <a target="_blank" rel="noopener" href="https://github.com/LemuelPuglisi/BrLP">https://github.com/LemuelPuglisi/BrLP</a>. </p>
<blockquote>
<p>随着纵向磁共振成像（MRI）数据集的日益普及，基于人工智能（AI）的疾病进展建模变得更为便利，这使得能够针对个体患者预测未来的医学扫描结果。然而，尽管人工智能取得了重大进展，当前的方法仍然面临挑战，包括实现患者特异性个性化、确保时空一致性、有效利用纵向数据以及管理庞大的三维扫描内存需求。为了应对这些挑战，我们提出了Brain Latent Progression（BrLP）模型，这是一种新型时空预测模型，旨在预测三维脑部MRI的个体疾病进展。BrLP的主要贡献体现在四个方面：（i）它在小潜空间中运行，减轻了高维成像数据带来的计算挑战；（ii）它明确地集成了受试者元数据，以增强预测的个性化；（iii）它通过辅助模型融入疾病动态的先验知识，促进纵向数据的整合；（iv）它引入了Latent Average Stabilization（LAS）算法，该算法（a）在推理时强制预测的进展具有时空一致性，（b）使我们能够得出预测的确定性度量。我们在包含来自2805名受试者的11730张T1加权（T1w）脑MRI上训练和评估BrLP模型，并在包含来自962名受试者的外部测试集上验证其通用性。我们的实验将BrLP生成的MRI扫描与真实的随访MRI进行比较，证明了其相较于现有方法的卓越准确性。代码公开在：<a target="_blank" rel="noopener" href="https://github.com/LemuelPuglisi/BrLP">https://github.com/LemuelPuglisi/BrLP</a>。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.08560v1">PDF</a> arXiv admin note: text overlap with arXiv:2405.03328</p>
<p><strong>Summary</strong><br>     提出的Brain Latent Progression（BrLP）模型能预测患者个体化的疾病进展情况。BrLP在四维时空模型中操作，通过降低高维图像数据带来的计算挑战、集成患者元数据以提升预测个性化程度、引入疾病动态先验知识辅助模型并利用Latent Average Stabilization（LAS）算法确保预测时空一致性和不确定性度量，实现了对3D脑MRI中疾病进展的精准预测。经过大规模实验验证，BrLP在公开数据集上的表现优于现有方法。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Brain Latent Progression（BrLP）是一个用于预测3D脑MRI中疾病进展的时空模型。</li>
<li>BrLP在小型潜在空间内操作，降低高维图像数据的计算挑战。</li>
<li>BrLP集成了患者的元数据，以增强预测的个性化和准确性。</li>
<li>BrLP通过引入疾病动态的先验知识，促进纵向数据的整合。</li>
<li>BrLP引入了Latent Average Stabilization（LAS）算法，确保预测的时空一致性和不确定性度量。</li>
<li>BrLP在包含大量受试者的大规模数据集上进行训练和评估，表现出优异的性能。</li>
<li>BrLP的代码已公开可用，为研究人员提供方便的访问途径。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.08560">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-5d0fed920e5822457f9abda28738946b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4f3738d1893036b2ae9ca1526837015d.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Referring-Remote-Sensing-Image-Segmentation-via-Bidirectional-Alignment-Guided-Joint-Prediction"><a href="#Referring-Remote-Sensing-Image-Segmentation-via-Bidirectional-Alignment-Guided-Joint-Prediction" class="headerlink" title="Referring Remote Sensing Image Segmentation via Bidirectional Alignment   Guided Joint Prediction"></a>Referring Remote Sensing Image Segmentation via Bidirectional Alignment   Guided Joint Prediction</h2><p><strong>Authors:Tianxiang Zhang, Zhaokun Wen, Bo Kong, Kecheng Liu, Yisi Zhang, Peixian Zhuang, Jiangyun Li</strong></p>
<p>Referring Remote Sensing Image Segmentation (RRSIS) is critical for ecological monitoring, urban planning, and disaster management, requiring precise segmentation of objects in remote sensing imagery guided by textual descriptions. This task is uniquely challenging due to the considerable vision-language gap, the high spatial resolution and broad coverage of remote sensing imagery with diverse categories and small targets, and the presence of clustered, unclear targets with blurred edges. To tackle these issues, we propose \ours, a novel framework designed to bridge the vision-language gap, enhance multi-scale feature interaction, and improve fine-grained object differentiation. Specifically, \ours introduces: (1) the Bidirectional Spatial Correlation (BSC) for improved vision-language feature alignment, (2) the Target-Background TwinStream Decoder (T-BTD) for precise distinction between targets and non-targets, and (3) the Dual-Modal Object Learning Strategy (D-MOLS) for robust multimodal feature reconstruction. Extensive experiments on the benchmark datasets RefSegRS and RRSIS-D demonstrate that \ours achieves state-of-the-art performance. Specifically, \ours improves the overall IoU (oIoU) by 3.76 percentage points (80.57) and 1.44 percentage points (79.23) on the two datasets, respectively. Additionally, it outperforms previous methods in the mean IoU (mIoU) by 5.37 percentage points (67.95) and 1.84 percentage points (66.04), effectively addressing the core challenges of RRSIS with enhanced precision and robustness. </p>
<blockquote>
<p>遥感图像分割参照（RRSIS）对于生态监测、城市规划和灾害管理至关重要，它要求根据文本描述精确地分割遥感图像中的对象。由于视语言差距较大，遥感图像的高空间分辨率和广泛覆盖范围，具有多种类别和小目标，以及存在边界模糊、目标不明确的情况，这项任务具有独特的挑战性。为了解决这些问题，我们提出了一个新颖框架\ours，旨在弥合视语言差距，增强多尺度特征交互，并改善精细粒度对象区分。具体来说，\ours引入了：（1）双向空间相关性（BSC）以改进视语言特征对齐；（2）目标背景TwinStream解码器（T-BTD）以精确区分目标和非目标；（3）双模态对象学习策略（D-MOLS）用于稳健的多模态特征重建。在RefSegRS和RRSIS-D基准数据集上的大量实验表明，\ours达到了最先进的性能。具体来说，\ours在两个数据集上的整体IoU（oIoU）分别提高了3.76个百分点（达到80.57）和1.44个百分点（达到79.23）。此外，它在平均IoU（mIoU）上较之前的方法提高了5.37个百分点（达到67.95）和1.84个百分点（达到66.04），有效地解决了RRSIS的核心挑战，提高了精度和稳健性。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.08486v1">PDF</a> </p>
<p><strong>Summary</strong><br>     所提出的新方法通过引入双向空间相关性（BSC）、目标背景TwinStream解码器（T-BTD）和双模态对象学习策略（D-MOLS），成功解决了遥感图像分割中的视觉语言差距、多尺度特征交互以及精细对象区分等问题，在RefSegRS和RRSIS-D等基准数据集上实现了卓越的性能。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>新提出的框架解决了远程遥感图像分割（RRSIS）中的视觉语言差距问题。</li>
<li>通过引入双向空间相关性（BSC），改进了视觉语言特征的匹配。</li>
<li>目标背景TwinStream解码器（T-BTD）的设计，能够精确区分目标和非目标。</li>
<li>双模态对象学习策略（D-MOLS）增强了多模态特征的重建能力。</li>
<li>在RefSegRS和RRSIS-D数据集上的实验表明，新框架实现了最先进的性能。</li>
<li>新框架提高了整体IoU（oIoU）指标，相比之前的方法有明显优势。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.08486">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-c91ab3cea34ed049300a7c5eafc00e11.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-67f36af0d91fad95fadf2773a21c7e34.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-cfe53a2cdeb5e585a3abc90fc944b2e6.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-8ac373a363779db11172344d362a8d24.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-1c468a0a6093531c437bc6b96d801b86.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e8bd7be7053d54129a2abc50bfae3369.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Hi-End-MAE-Hierarchical-encoder-driven-masked-autoencoders-are-stronger-vision-learners-for-medical-image-segmentation"><a href="#Hi-End-MAE-Hierarchical-encoder-driven-masked-autoencoders-are-stronger-vision-learners-for-medical-image-segmentation" class="headerlink" title="Hi-End-MAE: Hierarchical encoder-driven masked autoencoders are stronger   vision learners for medical image segmentation"></a>Hi-End-MAE: Hierarchical encoder-driven masked autoencoders are stronger   vision learners for medical image segmentation</h2><p><strong>Authors:Fenghe Tang, Qingsong Yao, Wenxin Ma, Chenxu Wu, Zihang Jiang, S. Kevin Zhou</strong></p>
<p>Medical image segmentation remains a formidable challenge due to the label scarcity. Pre-training Vision Transformer (ViT) through masked image modeling (MIM) on large-scale unlabeled medical datasets presents a promising solution, providing both computational efficiency and model generalization for various downstream tasks. However, current ViT-based MIM pre-training frameworks predominantly emphasize local aggregation representations in output layers and fail to exploit the rich representations across different ViT layers that better capture fine-grained semantic information needed for more precise medical downstream tasks. To fill the above gap, we hereby present Hierarchical Encoder-driven MAE (Hi-End-MAE), a simple yet effective ViT-based pre-training solution, which centers on two key innovations: (1) Encoder-driven reconstruction, which encourages the encoder to learn more informative features to guide the reconstruction of masked patches; and (2) Hierarchical dense decoding, which implements a hierarchical decoding structure to capture rich representations across different layers. We pre-train Hi-End-MAE on a large-scale dataset of 10K CT scans and evaluated its performance across seven public medical image segmentation benchmarks. Extensive experiments demonstrate that Hi-End-MAE achieves superior transfer learning capabilities across various downstream tasks, revealing the potential of ViT in medical imaging applications. The code is available at: <a target="_blank" rel="noopener" href="https://github.com/FengheTan9/Hi-End-MAE">https://github.com/FengheTan9/Hi-End-MAE</a> </p>
<blockquote>
<p>医学图像分割仍然是一个巨大的挑战，主要是由于缺乏标签。通过大规模无标签医学数据集对视觉转换器（ViT）进行掩模图像建模（MIM）的预训练，呈现出一种很有前景的解决方案，为各种下游任务提供了计算效率和模型泛化能力。然而，当前的基于ViT的MIM预训练框架主要侧重于输出层的局部聚合表示，并未能充分利用不同ViT层中的丰富表示，这些层能更好地捕捉用于更精确医学下游任务的细粒度语义信息。为了填补上述空白，我们在此提出分层编码器驱动的MAE（Hi-End-MAE），这是一种简单有效的基于ViT的预训练解决方案，其核心包含两项重要创新：（1）编码器驱动重建，这鼓励编码器学习更多信息特征以指导掩码块的重建；（2）分层密集解码，这实现了一种分层解码结构，以捕获不同层的丰富表示。我们在包含10K次CT扫描的大规模数据集上预训练Hi-End-MAE，并在七个公共医学图像分割基准上评估其性能。大量实验表明，Hi-End-MAE在各种下游任务上实现了卓越的迁移学习能力，揭示了ViT在医学成像应用中的潜力。代码可用在：<a target="_blank" rel="noopener" href="https://github.com/FengheTan9/Hi-End-MAE">https://github.com/FengheTan9/Hi-End-MAE</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.08347v1">PDF</a> 19 pages, Code: <a target="_blank" rel="noopener" href="https://github.com/FengheTan9/Hi-End-MAE">https://github.com/FengheTan9/Hi-End-MAE</a></p>
<p><strong>Summary</strong><br>     医学图像分割因标签稀缺而面临挑战。通过大规模无标签医学数据集对视觉转换器（ViT）进行掩模图像建模（MIM）的预训练展现出一种解决方案的潜力，能提高计算效率和模型在各种下游任务中的泛化能力。然而，现有的基于ViT的MIM预训练框架主要侧重于输出层的局部聚合表示，未能充分利用不同ViT层中的丰富表示，这些表示能更好地捕捉用于更精确医学下游任务的细粒度语义信息。为填补上述空白，我们提出层次编码器驱动的MAE（Hi-End-MAE），这是一种简单有效的基于ViT的预训练解决方案，其核心创新点在于：一是编码器驱动的重构，鼓励编码器学习更多信息特征以指导掩码补丁的重构；二是层次密集解码，实现层次解码结构以捕获不同层次的丰富表示。我们在包含一万次CT扫描的大规模数据集上预训练Hi-End-MAE，并在七个公开医学图像分割基准上评估其性能。实验表明，Hi-End-MAE在各种下游任务中具有出色的迁移学习能力，展现了ViT在医学成像应用中的潜力。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>医学图像分割面临标签稀缺的挑战。</li>
<li>预训练视觉转换器（ViT）通过掩模图像建模（MIM）在大型无标签医学数据集上是一种有前景的解决方案。</li>
<li>当前基于ViT的MIM预训练框架主要关注输出层的局部聚合表示。</li>
<li>Hi-End-MAE通过编码器驱动的重构和层次密集解码来解决上述问题。</li>
<li>编码器驱动的重构鼓励编码器学习更多信息特征以指导掩码补丁的重构。</li>
<li>层次密集解码旨在捕获不同层次的丰富表示。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.08347">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-5178ece69358f5c154c14087166dca3f.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-498a4896204c168f5018e6a0d4e6cb32.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-31a24c243e039d4fee3a395c329cdaa7.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-82b5ca3f28882dab571a3a64a25ac750.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b1d7a9cced67687a8ab65afd3013d3a2.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Screener-Self-supervised-Pathology-Segmentation-Model-for-3D-Medical-Images"><a href="#Screener-Self-supervised-Pathology-Segmentation-Model-for-3D-Medical-Images" class="headerlink" title="Screener: Self-supervised Pathology Segmentation Model for 3D Medical   Images"></a>Screener: Self-supervised Pathology Segmentation Model for 3D Medical   Images</h2><p><strong>Authors:Mikhail Goncharov, Eugenia Soboleva, Mariia Donskova, Ivan Oseledets, Marina Munkhoeva, Maxim Panov</strong></p>
<p>Accurate segmentation of all pathological findings in 3D medical images remains a significant challenge, as supervised models are limited to detecting only the few pathology classes annotated in existing datasets. To address this, we frame pathology segmentation as an unsupervised visual anomaly segmentation (UVAS) problem, leveraging the inherent rarity of pathological patterns compared to healthy ones. We enhance the existing density-based UVAS framework with two key innovations: (1) dense self-supervised learning (SSL) for feature extraction, eliminating the need for supervised pre-training, and (2) learned, masking-invariant dense features as conditioning variables, replacing hand-crafted positional encodings. Trained on over 30,000 unlabeled 3D CT volumes, our model, Screener, outperforms existing UVAS methods on four large-scale test datasets comprising 1,820 scans with diverse pathologies. Code and pre-trained models will be made publicly available. </p>
<blockquote>
<p>在3D医学图像中对所有病理表现进行精确分割仍然是一个巨大的挑战，因为监督模型仅限于检测现有数据集中已标注的少数病理类别。为了解决这一问题，我们将病理分割作为一个无监督的视觉异常分割（UVAS）问题来解决，利用病理模式与健康模式相比固有的稀有性。我们对现有的基于密度的UVAS框架进行了两项关键创新：一是密集的自监督学习（SSL）用于特征提取，无需监督预训练；二是学习得到的、对遮挡具有不变性的密集特征作为条件变量，取代了手工位置编码。在超过3万个无标签的3D CT体积上进行训练后，我们的模型——筛选器在由多个包含各种病理表现的扫描组成的大规模测试数据集上的表现优于现有的UVAS方法，共涵盖1820个扫描图像。模型和预训练模型将公开提供。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.08321v1">PDF</a> </p>
<p><strong>摘要</strong></p>
<p>本文提出一种针对三维医学图像中所有病理特征的分割问题，通过构建基于密度分析的无监督视觉异常分割框架，实现了对多种病理特征的准确分割。本文创新性地引入两项技术：一是通过密集自监督学习提取特征，避免了需要标注数据进行预训练的问题；二是利用训练出的、具有遮挡不变性的密集特征作为条件变量，取代了手工设计的位置编码方式。该模型在超过三万份无标签的三维CT图像数据上进行训练，在包含千余张含有不同病理的扫描数据的测试集上表现出了优异的性能。此外，作者将公开模型和代码供公众使用。</p>
<p><strong>关键见解</strong></p>
<ol>
<li>提出将病理分割问题视为无监督视觉异常分割（UVAS）问题，利用病理模式与健康模式之间的固有差异进行识别。</li>
<li>创新性地引入密集自监督学习进行特征提取，简化了对标注数据的依赖。</li>
<li>利用训练出的、具有遮挡不变性的密集特征作为条件变量替代传统手工设计的位置编码。</li>
<li>模型在大量无标签的三维CT图像数据上进行训练，证明了其对不同病理特征的有效识别能力。</li>
<li>模型在包含多种病理的大规模测试数据集上表现优异，证明了其广泛的应用潜力。</li>
<li>模型和代码将公开供公众使用，为相关研究提供便利。</li>
<li>此方法提供了一个全新的视角来解决医学图像分割中的挑战性问题，具有广泛的应用前景和重要的实用价值。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.08321">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-106cfbaec8009d3329be69854f3cc4ee.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3e65b87e0bc16d644c94dbbbcb48c926.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-dea1a3e224d9797998eda888b4f4b2b5.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-c45eea0b973bc66568570a038f004e81.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-4d54b306d0b932a9f89b0b508e255666.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-17c8033f462087a3a0d80ee4bca2e08d.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Soft-X-ray-Imager-of-the-Xtend-system-onboard-XRISM"><a href="#Soft-X-ray-Imager-of-the-Xtend-system-onboard-XRISM" class="headerlink" title="Soft X-ray Imager of the Xtend system onboard XRISM"></a>Soft X-ray Imager of the Xtend system onboard XRISM</h2><p><strong>Authors:Hirofumi Noda, Koji Mori, Hiroshi Tomida, Hiroshi Nakajima, Takaaki Tanaka, Hiroshi Murakami, Hiroyuki Uchida, Hiromasa Suzuki, Shogo Benjamin Kobayashi, Tomokage Yoneyama, Kouichi Hagino, Kumiko Nobukawa, Hideki Uchiyama, Masayoshi Nobukawa, Hironori Matsumoto, Takeshi Go Tsuru, Makoto Yamauchi, Isamu Hatsukade, Hirokazu Odaka, Takayoshi Kohmura, Kazutaka Yamaoka, Tessei Yoshida, Yoshiaki Kanemaru, Junko Hiraga, Tadayasu Dotani, Masanobu Ozaki, Hiroshi Tsunemi, Jin Sato, Toshiyuki Takaki, Yuta Terada, Keitaro Miyazaki, Kohei Kusunoki, Yoshinori Otsuka, Haruhiko Yokosu, Wakana Yonemaru, Kazuhiro Ichikawa, Hanako Nakano, Reo Takemoto, Tsukasa Matsushima, Reika Urase, Jun Kurashima, Kotomi Fuchi, Kaito Hayakawa, Masahiro Fukuda, Takamitsu Kamei, Yoh Asahina, Shun Inoue, Amano Yuki, Yuma Aoki, Yamato Ito, Tomoya Kamatani, Kouta Takayama, Takashi Sako, Marina Yoshimoto, Kohei Shima, Mayu Higuchi, Kaito Ninoyu, Daiki Aoki, Shun Tsunomachi, Kiyoshi Hayashida</strong></p>
<p>The Soft X-ray Imager (SXI) is the X-ray charge-coupled device (CCD) camera for the soft X-ray imaging telescope Xtend installed on the X-ray Imaging and Spectroscopy Mission (XRISM), which was adopted as a recovery mission for the Hitomi X-ray satellite and was successfully launched on 2023 September 7 (JST). In order to maximize the science output of XRISM, we set the requirements for Xtend and find that the CCD set employed in the Hitomi&#x2F;SXI or similar, i.e., a $2 \times 2$ array of back-illuminated CCDs with a $200~\mu$m-thick depletion layer, would be practically best among available choices, when used in combination with the X-ray mirror assembly. We design the XRISM&#x2F;SXI, based on the Hitomi&#x2F;SXI, to have a wide field of view of $38’ \times 38’$ in the $0.4-13$ keV energy range. We incorporated several significant improvements from the Hitomi&#x2F;SXI into the CCD chip design to enhance the optical-light blocking capability and to increase the cosmic-ray tolerance, reducing the degradation of charge-transfer efficiency in orbit. By the time of the launch of XRISM, the imaging and spectroscopic capabilities of the SXI has been extensively studied in on-ground experiments with the full flight-model configuration or equivalent setups and confirmed to meet the requirements. The optical blocking capability, the cooling and temperature control performance, and the transmissivity and quantum efficiency to incident X-rays of the CCDs are also all confirmed to meet the requirements. Thus, we successfully complete the pre-flight development of the SXI for XRISM. </p>
<blockquote>
<p>软X射线成像仪（SXI）是安装在X射线成像和光谱任务（XRISM）上的软X射线成像望远镜Xtend的X射线电荷耦合器件（CCD）相机。XRISM是被Hitomi X射线卫星的复苏任务所采纳的，并于2023年9月7日（日本标准时间）成功发射。为了最大化XRISM的科学产出，我们对Xtend提出了要求，并发现与X射线镜面组合装配时，采用Hitomi&#x2F;SXI或类似的CCD集，即采用背照式CCD的$2\times 2$阵列，其中含一个厚为$200\mu m$的耗尽层，在现有选择中，这种配置实际上最为理想。我们基于Hitomi&#x2F;SXI设计XRISM&#x2F;SXI，在$0.4-13$keV能量范围内具有广阔的视野，即$38’\times 38’$。我们对CCD芯片设计进行了重大改进，增强了光学光屏蔽能力和宇宙射线容忍度，减少了轨道上的电荷传输效率下降。截至XRISM发射时，SXI的成像和光谱能力已经通过地面实验进行了广泛的研究，实验采用全飞行模型配置或等效设置，并确认满足要求。此外，我们还确认了CCD的光学屏蔽能力、冷却与温度控制性能以及对入射X射线的透射率和量子效率均符合要求。因此，我们成功完成了为XRISM准备的SXI的飞行前开发。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.08030v1">PDF</a> 14 pages, 11 figures, 3 tables, Accepted for publication in PASJ   XRISM special issue</p>
<p><strong>Summary</strong><br>     成功研发软X射线成像仪（SXI），搭载于X射线成像与光谱任务（XRISM）的望远镜Xtend上，应用于Hitomi X射线卫星的后续任务。采用由Hitomi&#x2F;SXI实践验证的背照式CCD阵列，结合X射线镜组件，设计具有宽广视野和高效能表现的XRISM&#x2F;SXI。经过地面实验验证，其成像和光谱能力符合任务要求。预飞行开发成功完成。</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>SXI是搭载于XRISM任务中的软X射线成像望远镜Xtend的X射线电荷耦合器件（CCD）相机。</li>
<li>SXI的设计基于Hitomi&#x2F;SXI，采用背照式CCD阵列，具有$2 \times 2$的CCD集阵和200微米的耗尽层厚度。</li>
<li>XRISM&#x2F;SXI具有宽广的视野范围，能在$0.4-13$ keV能量范围内进行成像。</li>
<li>对Hitomi&#x2F;SXI的CCD芯片设计进行了重大改进，提高了光学光屏蔽能力和宇宙射线耐受性，减少了轨道上电荷转移效率的下降。</li>
<li>地面实验验证了SXI的成像和光谱能力、光学屏蔽能力、冷却与温度控制性能以及CCDs对入射X射线的透射性和量子效率等符合任务要求。</li>
<li>成功完成了SXI的预飞行开发。</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.08030">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-d35a978a0384dbb38768468466fafbec.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d2ffc8617d4b2111ad537d20e0b74a40.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e07011c9b6f2edc3dff6871568584254.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e213be9e26adab4e2ff613438b393f41.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e0e1df34155285bd5757459f4d9bd27c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ef621a5a639297fae32dcaece71eea7e.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Joint-Modelling-Histology-and-Molecular-Markers-for-Cancer-Classification"><a href="#Joint-Modelling-Histology-and-Molecular-Markers-for-Cancer-Classification" class="headerlink" title="Joint Modelling Histology and Molecular Markers for Cancer   Classification"></a>Joint Modelling Histology and Molecular Markers for Cancer   Classification</h2><p><strong>Authors:Xiaofei Wang, Hanyu Liu, Yupei Zhang, Boyang Zhao, Hao Duan, Wanming Hu, Yonggao Mou, Stephen Price, Chao Li</strong></p>
<p>Cancers are characterized by remarkable heterogeneity and diverse prognosis. Accurate cancer classification is essential for patient stratification and clinical decision-making. Although digital pathology has been advancing cancer diagnosis and prognosis, the paradigm in cancer pathology has shifted from purely relying on histology features to incorporating molecular markers. There is an urgent need for digital pathology methods to meet the needs of the new paradigm. We introduce a novel digital pathology approach to jointly predict molecular markers and histology features and model their interactions for cancer classification. Firstly, to mitigate the challenge of cross-magnification information propagation, we propose a multi-scale disentangling module, enabling the extraction of multi-scale features from high-magnification (cellular-level) to low-magnification (tissue-level) whole slide images. Further, based on the multi-scale features, we propose an attention-based hierarchical multi-task multi-instance learning framework to simultaneously predict histology and molecular markers. Moreover, we propose a co-occurrence probability-based label correlation graph network to model the co-occurrence of molecular markers. Lastly, we design a cross-modal interaction module with the dynamic confidence constrain loss and a cross-modal gradient modulation strategy, to model the interactions of histology and molecular markers. Our experiments demonstrate that our method outperforms other state-of-the-art methods in classifying glioma, histology features and molecular markers. Our method promises to promote precise oncology with the potential to advance biomedical research and clinical applications. The code is available at <a target="_blank" rel="noopener" href="https://github.com/LHY1007/M3C2">https://github.com/LHY1007/M3C2</a> </p>
<blockquote>
<p>癌症具有显著的异质性和多样的预后。准确的癌症分类对于患者分层和临床决策至关重要。尽管数字病理学已经推动了癌症的诊断和预后，但癌症病理学的范式已经从单纯依赖组织特征转变为结合分子标志物。因此，迫切需要数字病理学方法满足新范式的需求。我们介绍了一种新型数字病理学方法，可联合预测分子标志物和组织学特征，并建模它们之间的相互作用以进行癌症分类。首先，为了减轻跨放大倍数信息传播的挑战，我们提出了一种多尺度分解模块，能够从高倍（细胞水平）到低倍（组织水平）的全幻灯片图像中提取多尺度特征。其次，基于多尺度特征，我们提出了一个基于注意力的分层多任务多实例学习框架，可以同时预测组织学和分子标志物。此外，我们提出了基于共发生概率的标签关联图网络，以模拟分子标志物的共发生。最后，我们设计了一个跨模态交互模块，采用动态置信约束损失和跨模态梯度调制策略，以模拟组织学和分子标志物的相互作用。实验表明，我们的方法在分类胶质瘤、组织学特征和分子标志物方面优于其他最先进的方法。我们的方法有望推动精准肿瘤学的发展，具有推动生物医学研究和临床应用的潜力。代码可在<a target="_blank" rel="noopener" href="https://github.com/LHY1007/M3C2%E4%B8%8A%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/LHY1007/M3C2上找到。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.07979v1">PDF</a> accepted by Medical Image Analysis</p>
<p><strong>Summary</strong><br>在癌症诊断和治疗中，数字病理学方法已经变得越来越重要。本文介绍了一种新型数字病理学方法，通过联合预测分子标记和组织学特征，以及模拟两者间的交互作用来进行癌症分类。该方法采用多尺度分离模块解决跨尺度信息传播的挑战，并利用基于注意力机制的分层多任务多实例学习框架同时预测组织学和分子标记。实验证明该方法在胶质细胞瘤的分类上表现优于其他最先进的方法。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>癌症具有显著异质性和多样的预后，准确的癌症分类对于患者分层和临床决策至关重要。</li>
<li>数字病理学已经逐渐从单纯依赖组织学特征转变为结合分子标记，以更好地适应癌症病理学的变化。</li>
<li>介绍了一种新型数字病理学方法，联合预测分子标记和组织学特征，并模拟其交互作用进行分类。</li>
<li>该方法通过使用多尺度分离模块处理跨尺度信息传播的挑战。</li>
<li>利用基于注意力机制的分层多任务多实例学习框架进行预测。</li>
<li>通过建立基于共现概率的标签关联图网络来模拟分子标记的共现。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.07979">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-d3d6d13fccbd5f8e74136265174b2a55.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e5a8ca1113b39dc1fe0d6891cb9ed17d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d2a790f724ea663f591691ec81541697.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Markarian-501-reincarnates-with-vigor-as-a-temporary-EHBL-during-VHE-flaring-in-July-2014"><a href="#Markarian-501-reincarnates-with-vigor-as-a-temporary-EHBL-during-VHE-flaring-in-July-2014" class="headerlink" title="Markarian 501 reincarnates with vigor as a temporary EHBL during VHE   flaring in July 2014"></a>Markarian 501 reincarnates with vigor as a temporary EHBL during VHE   flaring in July 2014</h2><p><strong>Authors:Sarira Sahu A. U. Puga Oliveros, D. I. Páez-Sánchez, G. Sánchez-Colón, 2 Subhash Rajpoot, M. E. Iglesias Martínez, José Guerra Carmenate, P. Fernández de Córdoba, Gaetano Lambiase</strong></p>
<p>Markarian 501, a well known high energy BL Lac object, has exhibited several epochs of very high energy (VHE) gamma-ray flaring events when its synchrotron peak frequency shifted above $10^{17}$ Hz, a signature of extreme behavior. From July 16 to July 31, 2014 such flaring events were observed for 15 days by various telescopes. On July 19 (MJD 56857.98), the X-ray outburst from the source was at its highest and on the same day an intriguing narrow peak-like feature around 3 TeV was observed by MAGIC telescopes, a feature inconsistent with standard interpretations. Using the well known two-zone photohadronic model, we study these VHE gamma-ray spectra on a day-by-day basis and offer explanation. Our two-zone photohadronic scenario shows that, on MJD 56857.98, the peak-like feature appears at a cutoff energy of $E^c_{\gamma}$ &#x3D; 3.18 TeV. Below this energy the observed VHE spectrum increases slowly and above $E^c_{\gamma}$ it falls faster, thus resulting in a mild peak-like feature. </p>
<blockquote>
<p>马克里安501是一颗著名的高能BL Lac天体，当它的同步辐射峰值频率移到$10^{17}$赫兹以上时，它表现出了多个非常高能（VHE）的伽马射线耀斑事件时段，这是极端行为的一个标志。从2014年7月16日至7月31日，这样的耀斑事件被各种望远镜持续观察了15天。在7月19日（MJD 56857.98），该源的X射线爆发达到最高，同一天，MAGIC望远镜观察到了一个有趣的在3TeV附近的窄峰状特征，这一特征与标准解释不符。我们使用众所周知的双区光子强子模型，逐日研究这些非常高能伽马射线光谱，并给出解释。我们的双区光子强子场景显示，在MJD 56857.98，峰值特征出现在截止能量$E^c_{\gamma}$&#x3D; 3.18 TeV。在此能量以下，观察到的VHE光谱增长缓慢，而在$E^c_{\gamma}$以上则下降得更快，从而产生了一个温和的峰值特征。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.07940v1">PDF</a> 14 pages, 8 figures, 1 table</p>
<p><strong>Summary</strong><br>     马克瑞安501号天体在观测期间展现了极高能伽马射线耀斑活动，特别是其同步加速器峰值频率高于$10^{17}$ Hz时，这暗示其极端行为状态。基于新的模型——双区光子中子模型进行研究分析后揭示了在特定的日期（MJD 56857.98），观察到高能伽马射线谱的峰值特征，并解释了其成因。这一特征表现为在截止能量$E^c_{\gamma}$ &#x3D; 3.18 TeV处出现一个峰值，该峰值以下观测到的VHE谱增长缓慢，而以上则迅速下降，形成轻微峰值特征。</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>Markarian 501在高能伽马射线耀斑活动期间表现出极端行为状态，特别是在同步加速器峰值频率高于特定值的情况下。</li>
<li>在特定日期（MJD 56857.98），观察到高能伽马射线谱的峰值特征。</li>
<li>使用双区光子中子模型进行解释，揭示出峰值特征在截止能量处形成。</li>
<li>在截止能量以下，观测到的VHE谱增长缓慢；而超过该能量时，则迅速下降。这种特征表现为轻微峰值形态。</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.07940">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-2f6fa1396253aee364863c4cdab9f65b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a0aaa0e49d839f12729a47463dd76577.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3609befe8e1daecb7c71a99ec5ecbb8c.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="Visual-Haptic-Model-Mediated-Teleoperation-for-Remote-Ultrasound"><a href="#Visual-Haptic-Model-Mediated-Teleoperation-for-Remote-Ultrasound" class="headerlink" title="Visual-Haptic Model Mediated Teleoperation for Remote Ultrasound"></a>Visual-Haptic Model Mediated Teleoperation for Remote Ultrasound</h2><p><strong>Authors:David Black, Maria Tirindelli, Septimiu Salcudean, Wolfgang Wein, Marco Esposito</strong></p>
<p>Tele-ultrasound has the potential greatly to improve health equity for countless remote communities. However, practical scenarios involve potentially large time delays which cause current implementations of telerobotic ultrasound (US) to fail. Using a local model of the remote environment to provide haptics to the expert operator can decrease teleoperation instability, but the delayed visual feedback remains problematic. This paper introduces a robotic tele-US system in which the local model is not only haptic, but also visual, by re-slicing and rendering a pre-acquired US sweep in real time to provide the operator a preview of what the delayed image will resemble. A prototype system is presented and tested with 15 volunteer operators. It is found that visual-haptic model-mediated teleoperation (MMT) compensates completely for time delays up to 1000 ms round trip in terms of operator effort and completion time while conventional MMT does not. Visual-haptic MMT also significantly outperforms MMT for longer time delays in terms of motion accuracy and force control. This proof-of-concept study suggests that visual-haptic MMT may facilitate remote robotic tele-US. </p>
<blockquote>
<p>远程超声具有为无数偏远地区社区提升健康公平的巨大潜力。然而，实际应用场景中涉及的时间延迟可能导致当前远程机器人超声（US）实施失败。使用远程环境的本地模型为专业操作人员提供触觉可以减小远程操作的稳定性问题，但延迟的视觉反馈仍然是个难题。本文介绍了一种机器人远程超声系统，该系统不仅通过触觉提供本地模型，还通过视觉提供模型，通过实时重新切片和渲染预先获取的超声扫描来为操作人员提供延迟图像预览。该系统的一个原型被展示给并测试了15名志愿者操作人员。研究发现，视觉触觉模型介导的远程操作（MMT）在操作者努力和完成时间方面可以完全补偿高达1000毫秒的往返时间延迟，而传统的MMT则不能。视觉触觉MMT在运动准确性和力量控制方面也显著优于更长时间延迟下的MMT。这项概念验证研究结果表明，视觉触觉MMT可能有助于远程机器人超声操作。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.07922v1">PDF</a> Supplementary video: <a target="_blank" rel="noopener" href="https://youtu.be/fDLBah7bPeo">https://youtu.be/fDLBah7bPeo</a> . This work has   been submitted to the IEEE for possible publication</p>
<p><strong>Summary</strong></p>
<p>远程医疗超声（Tele-ultrasound）对改善偏远地区的医疗公平具有巨大潜力。然而，实际应用中存在时间延迟问题，导致当前远程遥控超声机器人（Telerobotic ultrasound，简称US）的实施失败。本文引入了一种机器人远程超声系统，该系统不仅提供触觉反馈，还通过实时重新切片和渲染预先获取的超声扫描提供视觉反馈，为操作者提供了一个延迟图像的预览。实验表明，视觉触觉模型介导的遥控操作（Visual-haptic Model-Mediated Teleoperation，简称MMT）能够在时间延迟达1000毫秒往返的情况下完全补偿操作者的努力和完成时间，而传统的MMT则无法实现。此外，视觉触觉MMT在运动准确性和力量控制方面也显著优于传统MMT。这为远程遥控超声机器人的应用提供了新的可能性。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>远程医疗超声有望改善偏远地区的医疗公平问题。</li>
<li>时间延迟是遥控超声机器人实际应用中的主要挑战之一。</li>
<li>通过视觉触觉模型介导的遥控操作（MMT）可以有效补偿时间延迟的影响。</li>
<li>MMT能在时间延迟达1000毫秒往返的情况下改善操作者的努力和完成时间。</li>
<li>在运动准确性和力量控制方面，视觉触觉MMT显著优于传统MMT。</li>
<li>本文提出的新型遥控超声机器人系统结合了触觉和视觉反馈，通过实时重新切片和渲染预先获取的超声扫描来提供延迟图像的预览。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.07922">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-20313827537ec0b12699027ac04f3a6a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-71648e94eb10c8c70f76493fd6327e5c.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-bfcc364af5aa26f47fbcb14c15928f42.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-1d7dd3530baf3eead35704f1661f31e1.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-0c0876807436eb2e8dcd75101756addf.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-dd07e8a22a5ea2d61fe135df033f793f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9ec85680094f429f3c778ea36e3282b6.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="Curvature-Tuning-Provable-Training-free-Model-Steering-From-a-Single-Parameter"><a href="#Curvature-Tuning-Provable-Training-free-Model-Steering-From-a-Single-Parameter" class="headerlink" title="Curvature Tuning: Provable Training-free Model Steering From a Single   Parameter"></a>Curvature Tuning: Provable Training-free Model Steering From a Single   Parameter</h2><p><strong>Authors:Leyang Hu, Randall Balestriero</strong></p>
<p>The scaling of model size and data size has reshaped the paradigm of AI. As a result, the common protocol to leverage the latest models is to steer them towards a specific downstream task of interest through {\em fine-tuning}. Despite its importance, the main methods for fine-tuning remain limited to full or low-rank adapters–containing countless hyper-parameters and lacking interpretability. In this paper, we take a step back and demonstrate how novel and explainable post-training steering solutions can be derived theoretically from {\em spline operators}, a rich mathematical framing of Deep Networks that was recently developed. Our method–coined \textbf{Curvature Tuning (CT)}–has a single parameter that provably modulates the curvature of the model’s decision boundary henceforth allowing training-free steering. This makes CT both more efficient and interpretable than conventional fine-tuning methods. We empirically validate its effectiveness in improving generalization and robustness of pretrained models. For example, CT improves out-of-distribution transfer performances of ResNet-18&#x2F;50 by 2.57%&#x2F;1.74% across seventeen downstream datasets, and improves RobustBench robust accuracy by 11.76%&#x2F;348.44%. Additionally, we apply CT to ReLU-based Swin-T&#x2F;S, improving their generalization on nine downstream datasets by 2.43%&#x2F;3.33%. Our code is available at \href{<a target="_blank" rel="noopener" href="https://github.com/Leon-Leyang/curvature-tuning%7D%7Bhttps://github.com/Leon-Leyang/curvature-tuning%7D">https://github.com/Leon-Leyang/curvature-tuning}{https://github.com/Leon-Leyang/curvature-tuning}</a>. </p>
<blockquote>
<p>模型规模和数据规模的扩展已经重塑了人工智能的范式。因此，利用最新模型的通用协议是通过微调将它们导向特定的下游任务。尽管微调很重要，但主要方法仍然局限于全参数或低秩适配器——包含无数超参数且缺乏可解释性。在本文中，我们退一步，展示了如何从深度网络的丰富数学框架“样条算子”中理论上推导出新颖且可解释的后训练微调解决方案。我们的方法——被称为“曲率调整（CT）”，有一个参数可以调整模型决策边界的曲率，从而实现无训练微调。这使得CT比传统微调方法更高效且更具可解释性。我们通过实验验证了其在提高预训练模型的通用性和鲁棒性方面的有效性。例如，CT在十七个下游数据集上提高了ResNet-18&#x2F;50的跨分布迁移性能，分别提高了2.57%&#x2F;1.74%，并在RobustBench上的鲁棒精度提高了11.76%&#x2F;348.44%。此外，我们将CT应用于ReLU基础的Swin-T&#x2F;S，在九个下游数据集上提高了它们的通用性，分别提高了2.43%&#x2F;3.33%。我们的代码可在<a target="_blank" rel="noopener" href="https://github.com/Leon-Leyang/curvature-tuning%E9%93%BE%E6%8E%A5%E4%B8%AD%E8%AE%BF%E9%97%AE%E3%80%82">https://github.com/Leon-Leyang/curvature-tuning链接中访问。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.07783v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>模型规模和数据规模的扩展重塑了人工智能范式，其中的关键手段之一是对模型进行微调。现有的微调方法多依赖大量参数并且缺乏可解释性。本研究提出了基于曲线操作的理论方法——弯曲度微调（Curvature Tuning, CT）。CT通过一个参数即可调控模型决策边界的曲率，从而实现训练分离调整策略，显著提升了预训练模型的泛化和鲁棒性。本研究的结果证明CT较传统微调方法更具效能和可解释性。该成果已在GitHub上公开代码。</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>模型规模和数据规模的扩展重塑了人工智能的范式。</li>
<li>现有微调方法多采用大量参数的调整策略，但缺乏解释性。</li>
<li>研究者提出新的训练后微调方法——弯曲度微调（CT），利用曲线操作理论来实现模型调整。</li>
<li>CT通过一个参数调整模型决策边界的曲率，实现了训练分离的调整策略。</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.07783">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-ad08b416bd3acd8f6155b9afd84c9103.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-55597e9dee0a9b4186568fadaf4d4dc3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5c9f992411ac9ed0d279429d9fed7f7c.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="Quantitative-evaluation-of-unsupervised-clustering-algorithms-for-dynamic-total-body-PET-image-analysis"><a href="#Quantitative-evaluation-of-unsupervised-clustering-algorithms-for-dynamic-total-body-PET-image-analysis" class="headerlink" title="Quantitative evaluation of unsupervised clustering algorithms for   dynamic total-body PET image analysis"></a>Quantitative evaluation of unsupervised clustering algorithms for   dynamic total-body PET image analysis</h2><p><strong>Authors:Oona Rainio, Maria K. Jaakkola, Riku Klén</strong></p>
<p>Background. Recently, dynamic total-body positron emission tomography (PET) imaging has become possible due to new scanner devices. While clustering algorithms have been proposed for PET analysis already earlier, there is still little research systematically evaluating these algorithms for processing of dynamic total-body PET images. Materials and methods. Here, we compare the performance of 15 unsupervised clustering methods, including K-means either by itself or after principal component analysis (PCA) or independent component analysis (ICA), Gaussian mixture model (GMM), fuzzy c-means (FCM), agglomerative clustering, spectral clustering, and several newer clustering algorithms, for classifying time activity curves (TACs) in dynamic PET images. We use dynamic total-body $^{15}$O-water PET images collected from 30 patients with suspected or confirmed coronary artery disease. To evaluate the clustering algorithms in a quantitative way, we use them to classify 5000 TACs from each image based on whether the curve is taken from brain, right heart ventricle, right kidney, lower right lung lobe, or urinary bladder. Results. According to our results, the best methods are GMM, FCM, and ICA combined with mini batch K-means, which classified the TACs with a median accuracies of 89%, 83%, and 81%, respectively, in a processing time of half a second or less on average for each image. Conclusion. GMM, FCM, and ICA with mini batch K-means show promise for dynamic total-body PET analysis. </p>
<blockquote>
<p><strong>背景</strong>。最近，由于新的扫描设备，动态全身正电子发射断层扫描（PET）成像已经变得可能。虽然聚类算法在较早的时候就已经被提出用于PET分析，但对于处理动态全身PET图像，仍少有研究系统地评估这些算法。<strong>材料和方法</strong>。在这里，我们对15种无监督聚类方法进行了比较，包括K-means（单独或与主成分分析（PCA）或独立成分分析（ICA）结合使用）、高斯混合模型（GMM）、模糊c-均值（FCM）、凝聚聚类、谱聚类和几种新的聚类算法，用于对动态PET图像中的时间活动曲线（TACs）进行分类。我们使用从疑似或确诊冠状动脉疾病的30例患者收集的动态全身$^{15}$O-水PET图像。为了定量评估聚类算法，我们使用它们根据曲线是否来自大脑、右心室、右肾、右下肺叶或膀胱来分类每个图像的5000条TACs。<strong>结果</strong>。根据我们的结果，最佳方法是GMM、FCM和与mini batch K-means结合的ICA，它们分类TACs的中值准确度分别为89%、83%和81%，每张图像的平均处理时间不超过半秒。<strong>结论</strong>。对于动态全身PET分析，GMM、FCM以及与mini batch K-means结合的ICA显示出良好的前景。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.07511v1">PDF</a> 12 pages, 2 figures</p>
<p><strong>Summary</strong></p>
<p>动态全身正电子发射断层扫描（PET）成像技术的最新发展对聚类算法提出了新的挑战与机遇。本文对比了15种无监督聚类算法在处理动态全身PET图像中的表现。研究发现，高斯混合模型（GMM）、模糊C-均值（FCM）和独立成分分析（ICA）结合小批量K-均值的方法表现最佳，对时间活动曲线（TACs）的分类准确率中位数分别为89%、83%和81%，且处理时间平均每秒不到半秒。</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>动态全身PET成像技术对聚类算法提出了新的挑战。</li>
<li>对比了多种无监督聚类算法在处理动态PET图像中的性能。</li>
<li>高斯混合模型（GMM）、模糊C-均值（FCM）和独立成分分析（ICA）结合小批量K-均值的方法在分类TACs方面表现最佳。</li>
<li>这些最佳方法的分类准确率中位数超过80%，处理时间迅速。</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.07511">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-9814fcbe9f346d2b65b774a4b4ac34f2.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ded55bf92e4c70805b68e008f8613c46.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-5bee5810932bea92e7f584d04694dd6b.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="Multi-Task-oriented-Nighttime-Haze-Imaging-Enhancer-for-Vision-driven-Measurement-Systems"><a href="#Multi-Task-oriented-Nighttime-Haze-Imaging-Enhancer-for-Vision-driven-Measurement-Systems" class="headerlink" title="Multi-Task-oriented Nighttime Haze Imaging Enhancer for Vision-driven   Measurement Systems"></a>Multi-Task-oriented Nighttime Haze Imaging Enhancer for Vision-driven   Measurement Systems</h2><p><strong>Authors:Ai Chen, Yuxu Lu, Dong Yang, Junlin Zhou, Yan Fu, Duanbing Chen</strong></p>
<p>Salient object detection (SOD) plays a critical role in vision-driven measurement systems (VMS), facilitating the detection and segmentation of key visual elements in an image. However, adverse imaging conditions such as haze during the day, low light, and haze at night severely degrade image quality, and complicating the SOD process. To address these challenges, we propose a multi-task-oriented nighttime haze imaging enhancer (MToIE), which integrates three tasks: daytime dehazing, low-light enhancement, and nighttime dehazing. The MToIE incorporates two key innovative components: First, the network employs a task-oriented node learning mechanism to handle three specific degradation types: day-time haze, low light, and night-time haze conditions, with an embedded self-attention module enhancing its performance in nighttime imaging. In addition, multi-receptive field enhancement module that efficiently extracts multi-scale features through three parallel depthwise separable convolution branches with different dilation rates, capturing comprehensive spatial information with minimal computational overhead. To ensure optimal image reconstruction quality and visual characteristics, we suggest a hybrid loss function. Extensive experiments on different types of weather&#x2F;imaging conditions illustrate that MToIE surpasses existing methods, significantly enhancing the accuracy and reliability of vision systems across diverse imaging scenarios. The code is available at <a target="_blank" rel="noopener" href="https://github.com/Ai-Chen-Lab/MToIE">https://github.com/Ai-Chen-Lab/MToIE</a>. </p>
<blockquote>
<p>显著性目标检测（SOD）在视觉驱动测量系统（VMS）中发挥着关键作用，可以促进图像中关键视觉元素的检测和分割。然而，不利的成像条件，如白天的雾霾、低光照和夜间雾霾，严重降低了图像质量，并使得SOD过程复杂化。为了应对这些挑战，我们提出了一种多任务导向的夜间雾霾成像增强器（MToIE），它结合了三项任务：白天去雾、低光增强和夜间去雾。MToIE包含两个关键的创新组件：首先，网络采用任务导向的节点学习机制来处理三种特定的退化类型：白天的雾霾、低光和夜间雾霾条件，其中嵌入的自注意模块增强了其在夜间成像的性能。此外，多感受野增强模块通过三个并行深度可分离卷积分支高效地提取多尺度特征，这些分支具有不同的膨胀率，以最小的计算开销捕获全面的空间信息。为了确保最佳的图像重建质量和视觉特性，我们提出了一种混合损失函数。在不同天气&#x2F;成像条件下的大量实验表明，MToIE超越了现有方法，显著提高了不同成像场景中视觉系统的准确性和可靠性。代码可在<a target="_blank" rel="noopener" href="https://github.com/Ai-Chen-Lab/MToIE%E4%B8%8A%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/Ai-Chen-Lab/MToIE上找到。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.07351v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文提出了一种多任务导向的夜间雾霾成像增强器（MToIE），集成了日间去雾、低光增强和夜间去雾三个任务。该增强器采用任务导向节点学习机制和带有自注意力模块的网络，提高了在夜间成像中的性能。同时，引入了一种高效的多尺度特征提取模块，通过三个具有不同膨胀率的并行深度可分离卷积分支捕获全面的空间信息。此外，为了确保图像重建质量和视觉特性达到最佳，建议使用混合损失函数。在多种天气和成像条件下的广泛实验表明，MToIE超越了现有方法，显著提高了视觉系统在不同成像场景中的准确性和可靠性。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>MToIE针对三种不同的成像条件（日间雾霾、低光和夜间雾霾）进行任务导向的设计。</li>
<li>采用任务导向节点学习机制和自注意力模块提高夜间成像性能。</li>
<li>多尺度特征提取模块通过并行深度可分离卷积分支有效捕获全面的空间信息。</li>
<li>混合损失函数确保图像重建质量和视觉特性的优化。</li>
<li>MToIE在多种天气和成像条件下的表现超越了现有方法。</li>
<li>MToIE显著提高视觉系统在不同成像场景中的准确性和可靠性。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.07351">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-3a8582974b280c3a559a15c13003d61b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-33807507bfead39f46c229a3cdb755ba.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b95c38c574a1e04aacf66acfa29bcd24.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-867bc86f555a71d53e22662406142d0d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5f54fcb2fe71e1a597f7afbfdf2c693f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f9c32f17028afe5cccb41ba22fa03de7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4b51faff93178ba05c14f2a0f89d319b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-bb91b8718623023a0ca305a52f708895.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="Color-Quality-Invariance-for-Robust-Medical-Image-Segmentation"><a href="#Color-Quality-Invariance-for-Robust-Medical-Image-Segmentation" class="headerlink" title="Color-Quality Invariance for Robust Medical Image Segmentation"></a>Color-Quality Invariance for Robust Medical Image Segmentation</h2><p><strong>Authors:Ravi Shah, Atsushi Fukuda, Quan Huu Cap</strong></p>
<p>Single-source domain generalization (SDG) in medical image segmentation remains a significant challenge, particularly for images with varying color distributions and qualities. Previous approaches often struggle when models trained on high-quality images fail to generalize to low-quality test images due to these color and quality shifts. In this work, we propose two novel techniques to enhance generalization: dynamic color image normalization (DCIN) module and color-quality generalization (CQG) loss. The DCIN dynamically normalizes the color of test images using two reference image selection strategies. Specifically, the DCIN utilizes a global reference image selection (GRIS), which finds a universal reference image, and a local reference image selection (LRIS), which selects a semantically similar reference image per test sample. Additionally, CQG loss enforces invariance to color and quality variations by ensuring consistent segmentation predictions across transformed image pairs. Experimental results show that our proposals significantly improve segmentation performance over the baseline on two target domain datasets, despite being trained solely on a single source domain. Notably, our model achieved up to a 32.3-point increase in Dice score compared to the baseline, consistently producing robust and usable results even under substantial domain shifts. Our work contributes to the development of more robust medical image segmentation models that generalize across unseen domains. The implementation code is available at <a target="_blank" rel="noopener" href="https://github.com/RaviShah1/DCIN-CQG">https://github.com/RaviShah1/DCIN-CQG</a>. </p>
<blockquote>
<p>医学图像分割中的单源域泛化（SDG）仍然是一个重大挑战，特别是对于颜色分布和质量各异的图像。以前的方法常常在训练于高质量图像上的模型无法泛化到低质量测试图像时遇到困扰，这是由于颜色和质量的偏移导致的。在这项工作中，我们提出了两种增强泛化的新技术：动态彩色图像归一化（DCIN）模块和颜色质量泛化（CQG）损失。DCIN通过两种参考图像选择策略动态地归一化测试图像的颜色。具体来说，DCIN利用全局参考图像选择（GRIS），找到通用参考图像；局部参考图像选择（LRIS）则为每个测试样本选择语义上相似的参考图像。此外，CQG损失通过确保变换图像对之间的分割预测一致，来强制执行颜色和质量变化的不变性。实验结果表明，尽管只在一个源域上进行训练，但我们的方法在两个目标域数据集上的分割性能都有了显著提高。值得注意的是，我们的模型在迪克分数上最高提高了32.3分，即使在很大的域偏移下也始终产生稳健和可用的结果。我们的工作为开发更稳健的医学图像分割模型做出了贡献，这些模型可以在未见过的领域进行泛化。实现代码可在<a target="_blank" rel="noopener" href="https://github.com/RaviShah1/DCIN-CQG%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/RaviShah1/DCIN-CQG找到。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.07200v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文提出两种技术来解决医学图像分割中的单源域泛化问题，即动态色彩图像归一化模块（DCIN）和色彩质量泛化（CQG）损失。DCIN通过两种参考图像选择策略动态归一化测试图像的颜色，而CQG损失通过确保对变换图像对的分割预测的一致性来强制执行颜色和质量的变异不变性。实验结果表明，与基线相比，该方法在目标域数据集上的分割性能显著提高，并且在单一源域训练的情况下也能实现良好的泛化效果。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>医学图像分割中的单源域泛化（SDG）是一个挑战，特别是在颜色分布和质量变化大的图像中。</li>
<li>提出的DCIN模块通过两种参考图像选择策略动态归一化测试图像颜色。</li>
<li>CQG损失通过确保对变换图像对分割预测的一致性，实现对颜色和质量的变异不变性。</li>
<li>实验结果显示，与基线相比，该方法在目标域数据集上的分割性能显著提高，Dice得分提高达32.3个点。</li>
<li>该模型在显著域偏移下仍能产生稳健和可用的结果。</li>
<li>该工作的贡献在于开发出更稳健的医学图像分割模型，能够在未见过的领域进行泛化。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.07200">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-fe88677ef992b6455489521ac15adebd.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c837548f986d9b7c6c110ae5f135543b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3104949abfa337221e3091c81e658397.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-636840586d721d9228fb4cb833e0ee48.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8ef1e86265a72139c3dc8ddbb05754eb.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b2c00bc084359a9ff94b6cd35578473a.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="HDCompression-Hybrid-Diffusion-Image-Compression-for-Ultra-Low-Bitrates"><a href="#HDCompression-Hybrid-Diffusion-Image-Compression-for-Ultra-Low-Bitrates" class="headerlink" title="HDCompression: Hybrid-Diffusion Image Compression for Ultra-Low Bitrates"></a>HDCompression: Hybrid-Diffusion Image Compression for Ultra-Low Bitrates</h2><p><strong>Authors:Lei Lu, Yize Li, Yanzhi Wang, Wei Wang, Wei Jiang</strong></p>
<p>Image compression under ultra-low bitrates remains challenging for both conventional learned image compression (LIC) and generative vector-quantized (VQ) modeling. Conventional LIC suffers from severe artifacts due to heavy quantization, while generative VQ modeling gives poor fidelity due to the mismatch between learned generative priors and specific inputs. In this work, we propose Hybrid-Diffusion Image Compression (HDCompression), a dual-stream framework that utilizes both generative VQ-modeling and diffusion models, as well as conventional LIC, to achieve both high fidelity and high perceptual quality. Different from previous hybrid methods that directly use pre-trained LIC models to generate low-quality fidelity-preserving information from heavily quantized latent, we use diffusion models to extract high-quality complimentary fidelity information from the ground-truth input, which can enhance the system performance in several aspects: improving indices map prediction, enhancing the fidelity-preserving output of the LIC stream, and refining conditioned image reconstruction with VQ-latent correction. In addition, our diffusion model is based on a dense representative vector (DRV), which is lightweight with very simple sampling schedulers. Extensive experiments demonstrate that our HDCompression outperforms the previous conventional LIC, generative VQ-modeling, and hybrid frameworks in both quantitative metrics and qualitative visualization, providing balanced robust compression performance at ultra-low bitrates. </p>
<blockquote>
<p>在超低比特率下，图像压缩对于传统的图像压缩（LIC）和生成向量量化（VQ）建模仍然具有挑战性。传统LIC由于重量化而容易出现严重伪影，而生成VQ建模由于学习到的生成先验与特定输入之间的不匹配而导致保真度较低。在这项工作中，我们提出了混合扩散图像压缩（HDCompression），这是一种双流框架，结合了生成VQ建模和扩散模型以及传统LIC，以实现高保真和高感知质量。不同于之前直接使用预训练的LIC模型从重度量化的潜在信息中产生低质量保真度保持信息的混合方法，我们使用扩散模型从原始真实输入中提取高质量补充保真信息，这可以在多方面提升系统性能：改善索引映射预测，提高LIC流的保真度保持输出，并通过VQ潜在校正进行条件图像重建。此外，我们的扩散模型基于密集表示向量（DRV），具有轻量级和非常简单的采样调度器。大量实验表明，我们的HDCompression在定量指标和定性可视化方面均优于之前的传统LIC、生成VQ建模和混合框架，在超低比特率下提供了平衡的稳健压缩性能。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.07160v1">PDF</a> Under Review</p>
<p><strong>Summary</strong></p>
<p>本工作提出一种名为Hybrid-Diffusion Image Compression（HDCompression）的混合扩散图像压缩技术，该技术结合了生成式VQ建模、传统图像压缩和扩散模型，旨在实现高保真度和高感知质量。不同于以往直接使用预训练的传统图像压缩模型生成低质量保真度信息的方法，我们的扩散模型从原始真实输入中提取高质量互补保真信息，从而提高了系统性能。此外，我们的扩散模型基于轻量级的密集代表向量（DRV），采样调度器非常简单。实验证明，在超低比特率下，HDCompression在定量指标和定性可视化方面都优于传统的图像压缩和生成式VQ建模方法。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>HDCompression结合了生成式VQ建模、传统图像压缩和扩散模型，旨在解决超低比特率下的图像压缩挑战。</li>
<li>传统的图像压缩方法面临严重的量化失真问题，而生成式VQ建模则由于生成先验与特定输入的不匹配而导致保真度低。</li>
<li>HDCompression使用扩散模型从原始真实输入中提取高质量互补保真信息，提高系统性能。</li>
<li>HDCompression通过改进索引映射预测、增强传统图像压缩流的保真度保持输出以及细化基于VQ潜在校正的条件图像重建来增强系统性能。</li>
<li>与其他混合方法不同，HDCompression使用基于轻量级密集代表向量（DRV）的扩散模型，具有非常简单的采样调度器。</li>
<li>实验证明，HDCompression在定量指标和定性可视化方面优于传统的图像压缩和生成式VQ建模方法。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.07160">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-6ff9076c2325696ae82fc6eb7ef6edcf.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4a46c4a4b63e6126703b406339e85cb8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-839ebbafb28640c5cd2d041eb04adec9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-88b7851930ad7fc7f390c7fc29ef87d6.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-b4f26c7faad21302a27a4ae71a9f54c2.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="Conditional-diffusion-model-with-spatial-attention-and-latent-embedding-for-medical-image-segmentation"><a href="#Conditional-diffusion-model-with-spatial-attention-and-latent-embedding-for-medical-image-segmentation" class="headerlink" title="Conditional diffusion model with spatial attention and latent embedding   for medical image segmentation"></a>Conditional diffusion model with spatial attention and latent embedding   for medical image segmentation</h2><p><strong>Authors:Behzad Hejrati, Soumyanil Banerjee, Carri Glide-Hurst, Ming Dong</strong></p>
<p>Diffusion models have been used extensively for high quality image and video generation tasks. In this paper, we propose a novel conditional diffusion model with spatial attention and latent embedding (cDAL) for medical image segmentation. In cDAL, a convolutional neural network (CNN) based discriminator is used at every time-step of the diffusion process to distinguish between the generated labels and the real ones. A spatial attention map is computed based on the features learned by the discriminator to help cDAL generate more accurate segmentation of discriminative regions in an input image. Additionally, we incorporated a random latent embedding into each layer of our model to significantly reduce the number of training and sampling time-steps, thereby making it much faster than other diffusion models for image segmentation. We applied cDAL on 3 publicly available medical image segmentation datasets (MoNuSeg, Chest X-ray and Hippocampus) and observed significant qualitative and quantitative improvements with higher Dice scores and mIoU over the state-of-the-art algorithms. The source code is publicly available at <a target="_blank" rel="noopener" href="https://github.com/Hejrati/cDAL/">https://github.com/Hejrati/cDAL/</a>. </p>
<blockquote>
<p>扩散模型已被广泛应用于高质量图像和视频生成任务。在本文中，我们提出了一种具有空间注意力和潜在嵌入（cDAL）的新型条件扩散模型，用于医学图像分割。在cDAL中，扩散过程的每一步都使用基于卷积神经网络（CNN）的鉴别器来区分生成的标签和真实的标签。基于鉴别器学习的特征计算空间注意力图，帮助cDAL生成输入图像中判别区域的更准确分割。此外，我们将随机潜在嵌入融入模型中的每一层，以大大减少训练和采样步骤的数量，从而使它比其他用于图像分割的扩散模型更快。我们在三个公开的医学图像分割数据集（MoNuSeg、Chest X-ray和Hippocampus）上应用了cDAL，与最先进的算法相比，在定性和定量方面都取得了显著的改进，Dice得分和mIoU更高。源代码可在<a target="_blank" rel="noopener" href="https://github.com/Hejrati/cDAL/%E5%85%AC%E5%BC%80%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/Hejrati/cDAL/公开获取。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.06997v1">PDF</a> 11 pages, 2 figures, 3 tables, Accepted in MICCAI 2024</p>
<p><strong>Summary</strong><br>扩散模型在医学图像分割中展现出显著优势。本文提出了一种具有空间注意力和潜在嵌入的新条件扩散模型（cDAL）。利用基于卷积神经网络（CNN）的鉴别器提高图像生成质量，通过空间注意力图更精确地分割输入图像中的判别区域。此外，模型中嵌入随机潜在向量以加快训练和采样过程，显著优于其他图像分割扩散模型。应用于三个公开医学图像分割数据集上的结果表明，cDAL实现了更高的狄克系数和平均交并比，优于现有算法。源代码已公开。</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>本文提出了一种新型条件扩散模型cDAL，用于医学图像分割。</li>
<li>cDAL模型利用CNN鉴别器在扩散过程的每个时间步长中区分生成的标签和真实标签，以提高图像生成质量。</li>
<li>空间注意力图有助于cDAL更精确地分割输入图像中的判别区域。</li>
<li>cDAL模型通过嵌入随机潜在向量，显著减少了训练和采样的时间步长，从而提高了速度。</li>
<li>cDAL在三个公开的医学图像分割数据集上的表现优于现有算法，具有更高的狄克系数和平均交并比。</li>
<li>cDAL模型的源代码已经公开，便于其他研究者使用和改进。</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.06997">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-256395d647a1c7449dba2a89e57c658e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1a77ce743221a1e323f776121fce2cc5.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="ViSIR-Vision-Transformer-Single-Image-Reconstruction-Method-for-Earth-System-Models"><a href="#ViSIR-Vision-Transformer-Single-Image-Reconstruction-Method-for-Earth-System-Models" class="headerlink" title="ViSIR: Vision Transformer Single Image Reconstruction Method for Earth   System Models"></a>ViSIR: Vision Transformer Single Image Reconstruction Method for Earth   System Models</h2><p><strong>Authors:Ehsan Zeraatkar, Salah Faroughi, Jelena Tešić</strong></p>
<p>Purpose: Earth system models (ESMs) integrate the interactions of the atmosphere, ocean, land, ice, and biosphere to estimate the state of regional and global climate under a wide variety of conditions. The ESMs are highly complex, and thus, deep neural network architectures are used to model the complexity and store the down-sampled data. In this paper, we propose the Vision Transformer Sinusoidal Representation Networks (ViSIR) to improve the single image SR (SR) reconstruction task for the ESM data.   Methods: ViSIR combines the SR capability of Vision Transformers (ViT) with the high-frequency detail preservation of the Sinusoidal Representation Network (SIREN) to address the spectral bias observed in SR tasks.   Results: The ViSIR outperforms ViT by 4.1 dB, SIREN by 7.5 dB, and SR-Generative Adversarial (SR-GANs) by 7.1dB PSNR on average for three different measurements.   Conclusion: The proposed ViSIR is evaluated and compared with state-of-the-art methods. The results show that the proposed algorithm is outperforming other methods in terms of Mean Square Error(MSE), Peak-Signal-to-Noise-Ratio(PSNR), and Structural Similarity Index Measure(SSIM). </p>
<blockquote>
<p>目的：地球系统模型（ESM）整合了大气、海洋、陆地、冰层和生物圈的相互作用，以在多种条件下估计区域和全球气候的状态。由于ESM高度复杂，因此使用深度神经网络架构来对其复杂性进行建模并存储降采样数据。在本文中，我们提出了Vision Transformer Sinusoidal Representation Networks（ViSIR），旨在改进ESM数据的单图像超分辨率（SR）重建任务。</p>
</blockquote>
<p>方法：ViSIR结合了Vision Transformer（ViT）的SR能力和Sinusoidal Representation Network（SIREN）的高频细节保留能力，以解决SR任务中观察到的频谱偏见。</p>
<p>结果：ViSIR在三种不同测量上平均比ViT高出4.1 dB，比SIREN高出7.5 dB，比SR-生成对抗网络（SR-GANs）高出7.1 dB的峰值信噪比（PSNR）。</p>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.06741v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文提出一种新的基于Vision Transformer和Sinusoidal Representation Network结合的模型——Vision Transformer Sinusoidal Representation Networks（ViSIR），旨在改善地球系统模型中超分辨率重建（SR）任务的效果。该模型结合了ViT的超分辨率能力和SIREN的高频细节保留特性，解决了SR任务中的光谱偏差问题。实验结果显示，ViSIR相较于其他模型在PSNR等指标上有所提升。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ESMs利用深度神经网络进行复杂模拟并处理降采样数据。</li>
<li>Vision Transformer Sinusoidal Representation Networks (ViSIR)结合ViT和SIREN的优点，旨在改进ESM数据的超分辨率重建（SR）。</li>
<li>ViSIR解决了SR任务中的光谱偏差问题。</li>
<li>ViSIR在PSNR、MSE和SSIM等指标上的性能优于其他模型。</li>
<li>ViSIR相较于ViT、SIREN和SR-GANs平均提高了4.1dB、7.5dB和7.1dB的PSNR。</li>
<li>ViSIR模型在改善SR任务方面的效果显著，具有实际应用潜力。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.06741">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-cf9048c3b816cee988efb911d2fe6083.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-88028bcf0c1828b06f6099fcd8baf426.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-ee43ee84dd1032a6ae63795dd0fd3c89.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-0272a4ba28fe565af009c03462bf7be8.jpg" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="WGM-microprobe-device-for-high-sensitivity-and-broadband-ultrasound-detection"><a href="#WGM-microprobe-device-for-high-sensitivity-and-broadband-ultrasound-detection" class="headerlink" title="WGM microprobe device for high-sensitivity and broadband ultrasound   detection"></a>WGM microprobe device for high-sensitivity and broadband ultrasound   detection</h2><p><strong>Authors:Jialve Sun, Shengnan Huangfu, Tinglan Chen, Zijing Cai, Bowen Ruan, Fangxing Zhang</strong></p>
<p>Whispering-gallery-mode (WGM) microcavities have emerged as a promising alternative to traditional ultrasound probes, offering high sensitivity and wide bandwidth. In our research, we propose a novel silica WGM microprobe device, with impressive Q factors up to 10^7.The side-coupled approach and special encapsulation design make the device small, robust, and capable of utilizing in both gaseous and liquid environments.We have successfully conducted photoacoustic (PA) imaging on various samples using this device which demonstrates a high sensitivity of 5.4 mPa&#x2F;sqrt(Hz) and a board bandwidth of 41 MHz at -6 dB for ultrasound. What’s more, it’s capable of capturing the vibration spectrum of microparticles up to a few hundred megahertz. Our compact and lightweight device exhibits significant application potential in PA endoscopic detection, near-field ultrasound sensing and other aspects. </p>
<blockquote>
<p>微腔中的嗡嗡模式（WGM）已成为传统超声探针的一种很有前途的替代品，具有灵敏度高和带宽大的特点。在我们的研究中，我们提出了一种新型二氧化硅WGM微型探针装置，其Q因子高达10^7。侧面耦合的方法和特殊的封装设计使该设备体积小、稳定性高，既能在气态环境中使用也能在液态环境中使用。我们已经成功使用该设备对各种样品进行光声成像，其灵敏度高达5.4 mPa&#x2F;sqrt(Hz)，在超声-6 dB下的带宽为41 MHz。此外，它能捕捉到高达数百兆赫兹的微粒子振动光谱。我们的紧凑轻便的设备在光声内窥检测、近场超声传感和其他方面表现出巨大的应用潜力。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.04627v2">PDF</a> </p>
<p><strong>Summary</strong><br>微腔中的轻声模式（WGM）微腔提供了一种对传统超声探针颇具潜力的替代方案，具有灵敏度高和带宽宽的特点。研究中提出了一种新型二氧化硅WGM微探针器件，具有高达10^7的Q值。其侧耦合方式和特殊封装设计使器件小巧、稳健，既可用于气体环境也可用于液体环境。利用该器件成功进行了光声成像实验，展示了高灵敏度和宽频带特性，并能够捕捉到高达数百兆赫兹的微颗粒振动谱。此紧凑轻便的器件在光声内窥检测、近场超声传感等方面具有显著的应用潜力。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>WGM微腔作为传统超声探针的替代方案，具有高灵敏度和宽频带特性。</li>
<li>新型二氧化硅WGM微探针器件具有高达10^7的Q值。</li>
<li>侧耦合和特殊封装设计使器件适应于各种环境，包括气体和液体。</li>
<li>成功进行光声成像实验，展示了高灵敏度（5.4 mPa&#x2F;sqrt(Hz)）和宽频带（-6 dB下41 MHz）。</li>
<li>器件能够捕捉到高达数百兆赫兹的微颗粒振动谱。</li>
<li>器件紧凑轻便，具有显著的应用潜力。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.04627">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-99c799c78ac2892ab5bf05ed48de471d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c2934d60cb2c951db362fc60b1d1cef3.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-07f6526b592d8d45cf1c225b5600863e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e053710ca7242a781bcd2ee1730b7b68.jpg" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="Generating-crossmodal-gene-expression-from-cancer-histopathology-improves-multimodal-AI-predictions"><a href="#Generating-crossmodal-gene-expression-from-cancer-histopathology-improves-multimodal-AI-predictions" class="headerlink" title="Generating crossmodal gene expression from cancer histopathology   improves multimodal AI predictions"></a>Generating crossmodal gene expression from cancer histopathology   improves multimodal AI predictions</h2><p><strong>Authors:Samiran Dey, Christopher R. S. Banerji, Partha Basuchowdhuri, Sanjoy K. Saha, Deepak Parashar, Tapabrata Chakraborti</strong></p>
<p>Emerging research has highlighted that artificial intelligence based multimodal fusion of digital pathology and transcriptomic features can improve cancer diagnosis (grading&#x2F;subtyping) and prognosis (survival risk) prediction. However, such direct fusion for joint decision is impractical in real clinical settings, where histopathology is still the gold standard for diagnosis and transcriptomic tests are rarely requested, at least in the public healthcare system. With our novel diffusion based crossmodal generative AI model PathGen, we show that genomic expressions synthesized from digital histopathology jointly predicts cancer grading and patient survival risk with high accuracy (state-of-the-art performance), certainty (through conformal coverage guarantee) and interpretability (through distributed attention maps). PathGen code is available for open use by the research community through GitHub at <a target="_blank" rel="noopener" href="https://github.com/Samiran-Dey/PathGen">https://github.com/Samiran-Dey/PathGen</a>. </p>
<blockquote>
<p>新兴研究突出表明，基于人工智能的数字病理和转录组特征的多模式融合可以提高癌症诊断（分级&#x2F;亚型）和预后（生存风险）预测的准确性。然而，在实际临床环境中，这种直接融合进行联合决策并不切实际。特别是在公共医疗体系中，组织病理学仍是诊断的金标准，转录组测试很少被要求。通过我们新颖的基于扩散的跨模态生成人工智能模型PathGen，我们展示了由数字病理学合成的基因表达能够联合预测癌症分级和患者生存风险，具有很高的准确性（达到最新性能）、确定性（通过合形覆盖保证）和可解释性（通过分布式注意力图）。PathGen代码可通过GitHub供研究界开放使用，网址为：<a target="_blank" rel="noopener" href="https://github.com/Samiran-Dey/PathGen%E3%80%82">https://github.com/Samiran-Dey/PathGen。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.00568v3">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文探讨了人工智能在数字病理与转录组特征融合方面的新兴研究，指出这种融合能提高癌症诊断（分级&#x2F;亚型）和预后（生存风险）预测的准确性。然而，在实际临床环境中直接融合进行联合决策并不现实。通过使用新型的基于扩散的跨模态生成人工智能模型PathGen，我们展示了从数字病理合成的基因组表达联合预测癌症分级和患者生存风险的高准确性、确定性和可解释性。PathGen代码已开源，供研究社区通过GitHub使用。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>人工智能在数字病理和转录组特征的融合方面能提高癌症诊断和预后预测的准确性。</li>
<li>直接融合进行联合决策在实际临床环境中并不现实，因为病理组织学仍是诊断的金标准，转录组测试在公共医疗系统中很少被要求。</li>
<li>新型扩散基于跨模态生成人工智能模型PathGen能够合成基因组表达，联合预测癌症分级和患者生存风险。</li>
<li>PathGen具有高准确性、达到最新性能水平，提供确定性（通过覆盖保证）和可解释性（通过分布式注意力图）。</li>
<li>PathGen代码已开源，供研究社区使用。</li>
<li>该模型的应用有助于推动人工智能在医学图像分析领域的进一步发展。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.00568">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-12ffdd9e2963ed31e07eeda826de6605.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7657b0524762cd643e4da4f8019fb60e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-6b8b5c480999519c5c61b2c6c02d142c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3779e081e955aeed5bc098a958af6b95.jpg" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1><h2 id="Beyond-Labels-Advancing-Open-Vocabulary-Segmentation-With-Vision-Language-Models"><a href="#Beyond-Labels-Advancing-Open-Vocabulary-Segmentation-With-Vision-Language-Models" class="headerlink" title="Beyond-Labels: Advancing Open-Vocabulary Segmentation With   Vision-Language Models"></a>Beyond-Labels: Advancing Open-Vocabulary Segmentation With   Vision-Language Models</h2><p><strong>Authors:Muhammad Atta ur Rahman</strong></p>
<p>Self-supervised learning can resolve numerous image or linguistic processing problems when effectively trained. This study investigated simple yet efficient methods for adapting previously learned foundation models for open-vocabulary semantic segmentation tasks. Our research proposed “Beyond-Labels,” a lightweight transformer-based fusion module that uses a handful of image segmentation data to fuse frozen image representations with language concepts. This strategy allows the model to successfully actualize enormous knowledge from pretrained models without requiring extensive retraining, making the model data-efficient and scalable. Furthermore, we efficiently captured positional information in images using Fourier embeddings, thus improving the generalization across various image sizes, addressing one of the key limitations of previous methods. Extensive ablation tests were performed to investigate the important components of our proposed method; when tested against the common benchmark PASCAL-5i, it demonstrated superior performance despite being trained on frozen image and language characteristics. </p>
<blockquote>
<p>自监督学习在得到有效的训练后，可以解决许多图像或语言处理方面的问题。本研究探讨了适应先前学习的基础模型用于开放词汇语义分割任务的简单而高效的方法。我们的研究提出了“超越标签”的概念，这是一个轻量级的基于transformer的融合模块，它使用少量的图像分割数据来融合冻结的图像表示和语言概念。这一策略使得模型能够成功地从预训练模型中实现大量知识，而无需进行大规模的重训，从而提高了模型的数据效率和可扩展性。此外，我们利用傅里叶嵌入有效地捕获了图像中的位置信息，从而提高了模型在不同图像大小上的泛化能力，解决了之前方法的关键限制之一。我们进行了广泛的消融实验，以研究我们提出方法的重要组件；在针对常用基准PASCAL-5i进行的测试中，即使在冻结的图像和语言特征上进行的训练，也表现出了卓越的性能。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.16769v4">PDF</a> </p>
<p><strong>Summary</strong><br>     本研究探讨了简单而高效的方法，用于将先前学习的基本模型适应于开放词汇语义分割任务。研究提出了“超越标签”的轻量级转换器融合模块，该模块使用少量的图像分割数据将冻结的图像表示与语言概念相融合。此方法使模型能够在不需要大量重新训练的情况下成功实现预训练模型中的巨大知识，使模型具有数据高效性和可扩展性。此外，通过傅立叶嵌入有效地捕获了图像中的位置信息，从而提高了不同图像大小的泛化能力，解决了以前方法的关键局限性之一。广泛的消融测试验证了所提出方法的重要成分；在针对常用基准PASCAL-5i的测试中，尽管是在冻结的图像和语言特征上训练的，但其性能仍表现出卓越。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>本研究使用自监督学习来解决图像或语言处理中的许多问题。</li>
<li>提出了一种名为“超越标签”的融合模块，该模块可以适应开放词汇语义分割任务。</li>
<li>通过将冻结的图像表示与语言概念相融合，实现了预训练模型中的知识转移。</li>
<li>使用傅立叶嵌入捕获图像中的位置信息，提高了模型的泛化能力。</li>
<li>所提出的方法在数据效率和可扩展性方面表现出优势。</li>
<li>通过广泛的消融测试验证了方法的关键成分。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.16769">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-36224e7591d39b9b869f1f92300f4ef4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e7fbd6815556591fbf3eca71cf1596e3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ee8a6f59f40c6583ddc83f694f6caab6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2bc086708430ffc1eefaf086fed70f3c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-81643d88ae1b7769cad39a37918b5c5c.jpg" align="middle">
</details>


<h1 id="-19"><a href="#-19" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        文章作者:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        文章链接:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-02-13/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">https://kedreamix.github.io/Talk2Paper/Paper/2025-02-13/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        版权声明:
                    </i>
                </span>
                <span class="reprint-info">
                    本博客所有文章除特別声明外，均采用
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    许可协议。转载请注明来源
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                                    <span class="chip bg-color">医学图像</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;上一篇</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-02-13/TTS/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-87d800ca0a132b127d32991befd3e24a.jpg" class="responsive-img" alt="TTS">
                        
                        <span class="card-title">TTS</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            TTS 方向最新论文已更新，请持续关注 Update in 2025-02-13  LoRP-TTS Low-Rank Personalized Text-To-Speech
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-02-13
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/TTS/" class="post-category">
                                    TTS
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/TTS/">
                        <span class="chip bg-color">TTS</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                下一篇&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-02-13/Diffusion%20Models/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-e3f3b3154fabd339535264a798f82277.jpg" class="responsive-img" alt="Diffusion Models">
                        
                        <span class="card-title">Diffusion Models</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Diffusion Models 方向最新论文已更新，请持续关注 Update in 2025-02-13  SwiftSketch A Diffusion Model for Image-to-Vector Sketch Generation
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-02-13
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Diffusion-Models/" class="post-category">
                                    Diffusion Models
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Diffusion-Models/">
                        <span class="chip bg-color">Diffusion Models</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- 代码块功能依赖 -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- 代码语言 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- 代码块复制 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- 代码块收缩 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC 悬浮按钮. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* 修复文章卡片 div 的宽度. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // 切换TOC目录展开收缩的相关操作.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;站点总字数:&nbsp;<span
                        class="white-color">15230.2k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- 运行天数提醒. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // 区分是否有年份.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = '本站已运行 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                daysTip = '本站已運行 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = '本站已运行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = '本站已運行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="关注我的知乎: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">知</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- 搜索遮罩框 -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span>
            <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- 白天和黑夜主题 -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- 回到顶部按钮 -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // 只在桌面版网页启用特效
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- 雪花特效 -->
    

    <!-- 鼠标星星特效 -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--腾讯兔小巢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- 动态标签 -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Σ(っ °Д °;)っ诶，页面崩溃了嘛？", clearTimeout(st)) : (document.title = "φ(゜▽゜*)♪咦，又好了！", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
