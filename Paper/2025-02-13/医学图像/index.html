<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="åŒ»å­¦å›¾åƒ">
    <meta name="description" content="åŒ»å­¦å›¾åƒ æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-02-13  Rapid Whole Brain Mesoscale In-vivo MR Imaging using Multi-scale   Implicit Neural Representation">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>åŒ»å­¦å›¾åƒ | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-d2ffc8617d4b2111ad537d20e0b74a40.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">åŒ»å­¦å›¾åƒ</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                                <span class="chip bg-color">åŒ»å­¦å›¾åƒ</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/" class="post-category">
                                åŒ»å­¦å›¾åƒ
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-02-13
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-03-07
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    19.1k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    79 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-02-13-æ›´æ–°"><a href="#2025-02-13-æ›´æ–°" class="headerlink" title="2025-02-13 æ›´æ–°"></a>2025-02-13 æ›´æ–°</h1><h2 id="Rapid-Whole-Brain-Mesoscale-In-vivo-MR-Imaging-using-Multi-scale-Implicit-Neural-Representation"><a href="#Rapid-Whole-Brain-Mesoscale-In-vivo-MR-Imaging-using-Multi-scale-Implicit-Neural-Representation" class="headerlink" title="Rapid Whole Brain Mesoscale In-vivo MR Imaging using Multi-scale   Implicit Neural Representation"></a>Rapid Whole Brain Mesoscale In-vivo MR Imaging using Multi-scale   Implicit Neural Representation</h2><p><strong>Authors:Jun Lyu, Lipeng Ning, William Consagra, Qiang Liu, Richard J. Rushmore, Berkin Bilgic, Yogesh Rathi</strong></p>
<p>Purpose: To develop and validate a novel image reconstruction technique using implicit neural representations (INR) for multi-view thick-slice acquisitions while reducing the scan time but maintaining high signal-to-noise ratio (SNR). Methods: We propose Rotating-view super-resolution (ROVER)-MRI, an unsupervised neural network-based algorithm designed to reconstruct MRI data from multi-view thick slices, effectively reducing scan time by 2-fold while maintaining fine anatomical details. We compare our method to both bicubic interpolation and the current state-of-the-art regularized least-squares super-resolution reconstruction (LS-SRR) technique. Validation is performed using ground-truth ex-vivo monkey brain data, and we demonstrate superior reconstruction quality across several in-vivo human datasets. Notably, we achieve the reconstruction of a whole human brain in-vivo T2-weighted image with an unprecedented 180{\mu}m isotropic spatial resolution, accomplished in just 17 minutes of scan time on a 7T MRI scanner. Results: ROVER-MRI outperformed LS-SRR method in terms of reconstruction quality with 22.4% lower relative error (RE) and 7.5% lower full-width half maximum (FWHM) indicating better preservation of fine structural details in nearly half the scan time. Conclusion: ROVER-MRI offers an efficient and robust approach for mesoscale MR imaging, enabling rapid, high-resolution whole-brain scans. Its versatility holds great promise for research applications requiring anatomical details and time-efficient imaging. </p>
<blockquote>
<p>ç›®çš„ï¼šå¼€å‘å¹¶éªŒè¯ä¸€ç§æ–°å‹å›¾åƒé‡å»ºæŠ€æœ¯ï¼Œåˆ©ç”¨éšç¥ç»è¡¨ç¤ºï¼ˆINRï¼‰è¿›è¡Œå¤šè§†è§’åšå±‚é‡‡é›†ï¼Œåœ¨ç¼©çŸ­æ‰«ææ—¶é—´çš„åŒæ—¶ä¿æŒè¾ƒé«˜çš„ä¿¡å™ªæ¯”ï¼ˆSNRï¼‰ã€‚</p>
</blockquote>
<p>æ–¹æ³•ï¼šæˆ‘ä»¬æå‡ºæ—‹è½¬è§†å›¾è¶…åˆ†è¾¨ç‡ï¼ˆROVERï¼‰-MRIï¼Œè¿™æ˜¯ä¸€ç§åŸºäºæ— ç›‘ç£ç¥ç»ç½‘ç»œç®—æ³•çš„MRIæ•°æ®é‡å»ºæ–¹æ³•ï¼Œå¯ä»å¤šè§†è§’åšå±‚é‡å»ºæ•°æ®ï¼Œé€šè¿‡æœ‰æ•ˆæ‰‹æ®µå°†æ‰«ææ—¶é—´å‡å°‘ä¸€å€ï¼ŒåŒæ—¶ä¿ç•™ç²¾ç»†çš„è§£å‰–ç»†èŠ‚ã€‚æˆ‘ä»¬å°†è¯¥æ–¹æ³•ä¸åŒä¸‰æ¬¡æ’å€¼ä»¥åŠå½“å‰æœ€å…ˆè¿›çš„æ­£åˆ™åŒ–æœ€å°äºŒä¹˜è¶…åˆ†è¾¨ç‡é‡å»ºï¼ˆLS-SRRï¼‰æŠ€æœ¯è¿›è¡Œæ¯”è¾ƒã€‚ä½¿ç”¨çœŸå®ç¦»ä½“çŒ´å­è„‘éƒ¨æ•°æ®è¿›è¡ŒéªŒè¯ï¼Œå¹¶åœ¨è‹¥å¹²ä½“å†…äººä½“æ•°æ®é›†ä¸Šå±•ç¤ºäº†æ›´é«˜çš„é‡å»ºè´¨é‡ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œæˆ‘ä»¬åœ¨7T MRIæ‰«æä»ªä¸Šä»…ç”¨äº†17åˆ†é’Ÿçš„æ‰«ææ—¶é—´ï¼Œå°±å®ç°äº†ä½“å†…æ•´ä¸ªäººè„‘T2åŠ æƒå›¾åƒçš„é‡å»ºï¼Œå…·æœ‰å‰æ‰€æœªæœ‰çš„180Î¼må„å‘åŒæ€§ç©ºé—´åˆ†è¾¨ç‡ã€‚</p>
<p>ç»“æœï¼šROVER-MRIåœ¨é‡å»ºè´¨é‡æ–¹é¢ä¼˜äºLS-SRRæ–¹æ³•ï¼Œç›¸å¯¹è¯¯å·®ï¼ˆREï¼‰é™ä½22.4%ï¼Œå…¨å®½åŠé«˜ï¼ˆFWHMï¼‰é™ä½7.5%ï¼Œè¡¨æ˜åœ¨è¿‘ä¹å‡åŠçš„æ‰«ææ—¶é—´å†…æ›´å¥½åœ°ä¿ç•™äº†ç²¾ç»†ç»“æ„ç»†èŠ‚ã€‚</p>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.08634v1">PDF</a> </p>
<p><strong>Summary</strong><br>    æœ¬ç ”ç©¶å¼€å‘å¹¶éªŒè¯äº†ä¸€ç§ä½¿ç”¨éšç¥ç»è¡¨ç¤ºï¼ˆINRï¼‰çš„æ–°å‹å›¾åƒé‡å»ºæŠ€æœ¯ï¼Œç”¨äºå¤šè§†è§’åšåˆ‡ç‰‡é‡‡é›†ï¼Œå¯åœ¨å‡å°‘æ‰«ææ—¶é—´çš„åŒæ—¶ä¿æŒé«˜ä¿¡å™ªæ¯”ï¼ˆSNRï¼‰ã€‚æå‡ºçš„æ—‹è½¬è§†å›¾è¶…åˆ†è¾¨ç‡ï¼ˆROVERï¼‰-MRIç®—æ³•å¯æœ‰æ•ˆä»å¤šè§†è§’åšåˆ‡ç‰‡é‡å»ºMRIæ•°æ®ï¼Œå°†æ‰«ææ—¶é—´å‡å°‘ä¸€å€ï¼ŒåŒæ—¶ä¿æŒç²¾ç»†çš„è§£å‰–ç»†èŠ‚ã€‚ç ”ç©¶ä½¿ç”¨çœŸå®ç¦»ä½“çŒ´è„‘æ•°æ®éªŒè¯äº†è¯¥æ–¹æ³•ï¼Œå¹¶åœ¨å¤šä¸ªä½“å†…äººè„‘æ•°æ®é›†ä¸Šå±•ç¤ºäº†å“è¶Šçš„é‡å»ºè´¨é‡ã€‚ç‰¹åˆ«åœ°ï¼Œè¯¥ç ”ç©¶åœ¨7T MRIæ‰«æä»ªä¸Šä»…ç”¨äº†17åˆ†é’Ÿå°±é‡å»ºäº†ä¸€ä¸ªæ•´ä¸ªäººè„‘çš„T2åŠ æƒå›¾åƒï¼Œå…·æœ‰å‰æ‰€æœªæœ‰çš„180Î¼mç­‰è·ç©ºé—´åˆ†è¾¨ç‡ã€‚ç»“æœè¡¨æ˜ï¼ŒROVER-MRIåœ¨é‡å»ºè´¨é‡æ–¹é¢ä¼˜äºç°æœ‰çš„æœ€å°äºŒä¹˜è¶…åˆ†è¾¨ç‡é‡å»ºï¼ˆLS-SRRï¼‰æ–¹æ³•ï¼Œå…·æœ‰æ›´ä½çš„ç›¸å¯¹è¯¯å·®ï¼ˆREï¼‰å’Œå…¨å®½åŠæœ€å¤§å€¼ï¼ˆFWHMï¼‰ï¼Œèƒ½å¤Ÿåœ¨å‡ ä¹å‡åŠçš„æ‰«ææ—¶é—´å†…æ›´å¥½åœ°ä¿ç•™ç²¾ç»†ç»“æ„ç»†èŠ‚ã€‚æ€»ä½“è€Œè¨€ï¼ŒROVER-MRIä¸ºå¿«é€Ÿé«˜æ•ˆçš„å¤§è§„æ¨¡MRæˆåƒæä¾›äº†å¯èƒ½ï¼Œåœ¨éœ€è¦è§£å‰–ç»†èŠ‚å’Œæ—¶é—´æ•ˆç‡è¦æ±‚çš„é¢†åŸŸå…·æœ‰å·¨å¤§æ½œåŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç ”ç©¶æå‡ºäº†ä¸€ç§åŸºäºéšç¥ç»è¡¨ç¤ºï¼ˆINRï¼‰çš„æ–°å‹å›¾åƒé‡å»ºæŠ€æœ¯â€”â€”æ—‹è½¬è§†å›¾è¶…åˆ†è¾¨ç‡ï¼ˆROVERï¼‰-MRIã€‚</li>
<li>ROVER-MRIæ—¨åœ¨ä»å¤šè§†è§’åšåˆ‡ç‰‡é‡å»ºMRIæ•°æ®ï¼Œå¯åœ¨å‡å°‘æ‰«ææ—¶é—´çš„åŒæ—¶ä¿æŒé«˜è´¨é‡å›¾åƒã€‚</li>
<li>ç›¸è¾ƒäºä¼ ç»Ÿçš„å›¾åƒé‡å»ºæ–¹æ³•å’Œç°æœ‰çš„æœ€å°äºŒä¹˜è¶…åˆ†è¾¨ç‡é‡å»ºï¼ˆLS-SRRï¼‰æŠ€æœ¯ï¼ŒROVER-MRIåœ¨é‡å»ºè´¨é‡ä¸Šè¡¨ç°æ›´ä¼˜ç§€ã€‚</li>
<li>ROVER-MRIæˆåŠŸåœ¨å¤šä¸ªä½“å†…äººè„‘æ•°æ®é›†ä¸Šå®ç°äº†é«˜è´¨é‡é‡å»ºï¼Œå¹¶ä¸”åœ¨ç¦»ä½“çŒ´è„‘æ•°æ®ä¸Šå¾—åˆ°äº†éªŒè¯ã€‚</li>
<li>ç ”ç©¶å®ç°äº†åœ¨7T MRIæ‰«æä»ªä¸Šä»…17åˆ†é’Ÿå†…å®Œæˆçš„é«˜åˆ†è¾¨ç‡æ•´ä¸ªäººè„‘T2åŠ æƒå›¾åƒçš„é‡å»ºã€‚</li>
<li>ROVER-MRIå…·æœ‰æ›´ä½çš„ç›¸å¯¹è¯¯å·®ï¼ˆREï¼‰å’Œå…¨å®½åŠæœ€å¤§å€¼ï¼ˆFWHMï¼‰ï¼Œèƒ½æ›´å¥½åœ°ä¿ç•™å›¾åƒä¸­çš„ç²¾ç»†ç»“æ„ç»†èŠ‚ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.08634">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-9f7805d32882c2c5c3da40748041d29b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-57a5ac7d4d848218e6b472d30fb7a4d2.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Ultrasound-Image-Generation-using-Latent-Diffusion-Models"><a href="#Ultrasound-Image-Generation-using-Latent-Diffusion-Models" class="headerlink" title="Ultrasound Image Generation using Latent Diffusion Models"></a>Ultrasound Image Generation using Latent Diffusion Models</h2><p><strong>Authors:Benoit Freiche, Anthony El-Khoury, Ali Nasiri-Sarvi, Mahdi S. Hosseini, Damien Garcia, Adrian Basarab, Mathieu Boily, Hassan Rivaz</strong></p>
<p>Diffusion models for image generation have been a subject of increasing interest due to their ability to generate diverse, high-quality images. Image generation has immense potential in medical imaging because open-source medical images are difficult to obtain compared to natural images, especially for rare conditions. The generated images can be used later to train classification and segmentation models. In this paper, we propose simulating realistic ultrasound (US) images by successive fine-tuning of large diffusion models on different publicly available databases. To do so, we fine-tuned Stable Diffusion, a state-of-the-art latent diffusion model, on BUSI (Breast US Images) an ultrasound breast image dataset. We successfully generated high-quality US images of the breast using simple prompts that specify the organ and pathology, which appeared realistic to three experienced US scientists and a US radiologist. Additionally, we provided user control by conditioning the model with segmentations through ControlNet. We will release the source code at <a target="_blank" rel="noopener" href="http://code.sonography.ai/">http://code.sonography.ai/</a> to allow fast US image generation to the scientific community. </p>
<blockquote>
<p>å›¾åƒç”Ÿæˆä¸­çš„æ‰©æ•£æ¨¡å‹å› å…¶èƒ½å¤Ÿç”Ÿæˆå¤šæ ·åŒ–ã€é«˜è´¨é‡å›¾åƒè€Œæ—¥ç›Šå—åˆ°å…³æ³¨ã€‚åŒ»å­¦æˆåƒé¢†åŸŸå…·æœ‰å·¨å¤§çš„æ½œåŠ›ï¼Œå› ä¸ºä¸è‡ªç„¶å›¾åƒç›¸æ¯”ï¼Œè·å–å¼€æºåŒ»å­¦å›¾åƒæ›´åŠ å›°éš¾ï¼Œå°¤å…¶æ˜¯å¯¹äºç½•è§ç—…ç—‡ã€‚ç”Ÿæˆçš„å›¾åƒä¹‹åå¯ç”¨äºè®­ç»ƒå’Œåˆ†å‰²æ¨¡å‹ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬é€šè¿‡è¿ç»­å¾®è°ƒå¤§å‹æ‰©æ•£æ¨¡å‹æ¥æ¨¡æ‹Ÿé€¼çœŸçš„è¶…å£°ï¼ˆUSï¼‰å›¾åƒï¼Œè¿™äº›æ¨¡å‹åœ¨ä¸åŒå¯ç”¨çš„å…¬å¼€æ•°æ®åº“ä¸Šå…·æœ‰è‰¯å¥½çš„æ€§èƒ½ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬å¯¹æœ€æ–°çš„æ½œåœ¨æ‰©æ•£æ¨¡å‹Stable Diffusionè¿›è¡Œäº†å¾®è°ƒï¼Œä»¥åœ¨BUSIï¼ˆä¹³è…ºè¶…å£°å›¾åƒï¼‰æ•°æ®é›†ä¸Šç”Ÿæˆä¹³è…ºè¶…å£°å›¾åƒã€‚æˆ‘ä»¬æˆåŠŸä½¿ç”¨ç®€å•çš„æç¤ºï¼ˆæŒ‡å®šå™¨å®˜å’Œç—…ç†ï¼‰ç”Ÿæˆé«˜è´¨é‡çš„ä¹³è…ºè¶…å£°å›¾åƒï¼Œè¿™äº›å›¾åƒå¯¹ä¸‰ä½ç»éªŒä¸°å¯Œçš„è¶…å£°ç§‘å­¦å®¶å’Œä¸€ä½è¶…å£°æ”¾å°„ç§‘åŒ»ç”Ÿæ¥è¯´çœ‹èµ·æ¥éå¸¸é€¼çœŸã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬é€šè¿‡ä½¿ç”¨ControlNetå¯¹æ¨¡å‹è¿›è¡Œåˆ†æ®µæ§åˆ¶ï¼Œæä¾›äº†ç”¨æˆ·æ§åˆ¶åŠŸèƒ½ã€‚æˆ‘ä»¬å°†åœ¨<a target="_blank" rel="noopener" href="http://code.sonography.ai/%E4%B8%8A%E5%8F%91%E5%B8%83%E6%BA%90%E4%BB%A3%E7%A0%81%EF%BC%8C%E4%BB%A5%E5%85%81%E8%AE%B8%E7%A7%91%E5%AD%A6%E7%95%8C%E5%BF%AB%E9%80%9F%E7%94%9F%E6%88%90%E8%B6%85%E5%A3%B0%E5%9B%BE%E5%83%8F%E3%80%82">http://code.sonography.ai/ä¸Šå‘å¸ƒæºä»£ç ï¼Œä»¥å…è®¸ç§‘å­¦ç•Œå¿«é€Ÿç”Ÿæˆè¶…å£°å›¾åƒã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.08580v1">PDF</a> 6 pages conference paper for SPIE medical imaging</p>
<p><strong>Summary</strong><br>     æœ¬æ–‡åˆ©ç”¨æ‰©æ•£æ¨¡å‹ç”Ÿæˆæ¨¡æ‹Ÿçš„è¶…å£°å›¾åƒï¼Œé€šè¿‡åœ¨ä¸åŒå…¬å…±æ•°æ®åº“ä¸Šå¯¹å¤§å‹æ‰©æ•£æ¨¡å‹è¿›è¡Œå¾®è°ƒå®ç°ã€‚ä½¿ç”¨ç¨³å®šæ‰©æ•£æ¨¡å‹å¯¹ä¹³è…ºè¶…å£°å›¾åƒæ•°æ®é›†è¿›è¡Œå¾®è°ƒï¼ŒæˆåŠŸç”Ÿæˆé«˜è´¨é‡ä¹³è…ºè¶…å£°å›¾åƒã€‚ç”Ÿæˆçš„å›¾åƒå…·æœ‰çœŸå®æ„Ÿï¼Œå¹¶å¯é€šè¿‡æ§åˆ¶æ¨¡å‹ä¸åˆ†å‰²ç›¸ç»“åˆå®ç°ç”¨æˆ·æ§åˆ¶ã€‚å°†åœ¨[é“¾æ¥åœ°å€]å‘å¸ƒæºä»£ç ï¼Œä¾›ç§‘å­¦ç•Œå¿«é€Ÿç”Ÿæˆè¶…å£°å›¾åƒã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ‰©æ•£æ¨¡å‹ç”¨äºå›¾åƒç”Ÿæˆæ­£å—åˆ°è¶Šæ¥è¶Šå¤šçš„å…³æ³¨ï¼Œå…·æœ‰ç”Ÿæˆå¤šæ ·åŒ–ã€é«˜è´¨é‡å›¾åƒçš„èƒ½åŠ›ã€‚</li>
<li>åœ¨åŒ»å­¦æˆåƒé¢†åŸŸï¼Œå›¾åƒç”Ÿæˆæ½œåŠ›å·¨å¤§ï¼Œç‰¹åˆ«æ˜¯åœ¨è·å–å…¬å¼€åŒ»ç–—å›¾åƒå›°éš¾çš„æƒ…å†µä¸‹ï¼Œèƒ½ä¸ºç½•è§ç—…ç—‡ç”Ÿæˆå›¾åƒã€‚</li>
<li>æœ¬æ–‡æå‡ºé€šè¿‡å¾®è°ƒå¤§å‹æ‰©æ•£æ¨¡å‹æ¥æ¨¡æ‹ŸçœŸå®çš„è¶…å£°å›¾åƒï¼Œä½¿ç”¨äº†ç°æœ‰çš„å…¬å¼€æ•°æ®åº“ã€‚</li>
<li>åˆ©ç”¨ç¨³å®šæ‰©æ•£æ¨¡å‹å¯¹ä¹³è…ºè¶…å£°å›¾åƒæ•°æ®é›†è¿›è¡Œå¾®è°ƒï¼ŒæˆåŠŸç”Ÿæˆé«˜è´¨é‡çš„ä¹³è…ºè¶…å£°å›¾åƒã€‚</li>
<li>ç”Ÿæˆçš„å›¾åƒå…·æœ‰çœŸå®æ„Ÿï¼Œèƒ½å¤Ÿæ¬ºéª—ç»éªŒä¸°å¯Œçš„è¶…å£°ç§‘å­¦å®¶å’Œæ”¾å°„ç§‘åŒ»ç”Ÿã€‚</li>
<li>ç”¨æˆ·å¯ä»¥é€šè¿‡æ§åˆ¶æ¨¡å‹ä¸åˆ†å‰²çš„ç»“åˆæ¥æ§åˆ¶æ¨¡å‹çš„ç”Ÿæˆç»“æœã€‚</li>
<li>æºç å°†åœ¨æŒ‡å®šé“¾æ¥å‘å¸ƒï¼Œä»¥ä¾›ç§‘å­¦ç•Œä½¿ç”¨ï¼Œä¿ƒè¿›å¿«é€Ÿè¶…å£°å›¾åƒç”Ÿæˆã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.08580">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-3b0bc59a15b3a1e7436f31c2dbccb398.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-4abce4bfa95bd288485184f519066a13.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a4753911170ebd2a5eb70c29d43b0e53.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-775ff4a4932e08c1a181e0737e5d4366.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-8e287699bc107b2ef3558225744c2b8f.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Brain-Latent-Progression-Individual-based-Spatiotemporal-Disease-Progression-on-3D-Brain-MRIs-via-Latent-Diffusion"><a href="#Brain-Latent-Progression-Individual-based-Spatiotemporal-Disease-Progression-on-3D-Brain-MRIs-via-Latent-Diffusion" class="headerlink" title="Brain Latent Progression: Individual-based Spatiotemporal Disease   Progression on 3D Brain MRIs via Latent Diffusion"></a>Brain Latent Progression: Individual-based Spatiotemporal Disease   Progression on 3D Brain MRIs via Latent Diffusion</h2><p><strong>Authors:Lemuel Puglisi, Daniel C. Alexander, Daniele RavÃ¬</strong></p>
<p>The growing availability of longitudinal Magnetic Resonance Imaging (MRI) datasets has facilitated Artificial Intelligence (AI)-driven modeling of disease progression, making it possible to predict future medical scans for individual patients. However, despite significant advancements in AI, current methods continue to face challenges including achieving patient-specific individualization, ensuring spatiotemporal consistency, efficiently utilizing longitudinal data, and managing the substantial memory demands of 3D scans. To address these challenges, we propose Brain Latent Progression (BrLP), a novel spatiotemporal model designed to predict individual-level disease progression in 3D brain MRIs. The key contributions in BrLP are fourfold: (i) it operates in a small latent space, mitigating the computational challenges posed by high-dimensional imaging data; (ii) it explicitly integrates subject metadata to enhance the individualization of predictions; (iii) it incorporates prior knowledge of disease dynamics through an auxiliary model, facilitating the integration of longitudinal data; and (iv) it introduces the Latent Average Stabilization (LAS) algorithm, which (a) enforces spatiotemporal consistency in the predicted progression at inference time and (b) allows us to derive a measure of the uncertainty for the prediction. We train and evaluate BrLP on 11,730 T1-weighted (T1w) brain MRIs from 2,805 subjects and validate its generalizability on an external test set comprising 2,257 MRIs from 962 subjects. Our experiments compare BrLP-generated MRI scans with real follow-up MRIs, demonstrating state-of-the-art accuracy compared to existing methods. The code is publicly available at: <a target="_blank" rel="noopener" href="https://github.com/LemuelPuglisi/BrLP">https://github.com/LemuelPuglisi/BrLP</a>. </p>
<blockquote>
<p>éšç€çºµå‘ç£å…±æŒ¯æˆåƒï¼ˆMRIï¼‰æ•°æ®é›†çš„æ—¥ç›Šæ™®åŠï¼ŒåŸºäºäººå·¥æ™ºèƒ½ï¼ˆAIï¼‰çš„ç–¾ç—…è¿›å±•å»ºæ¨¡å˜å¾—æ›´ä¸ºä¾¿åˆ©ï¼Œè¿™ä½¿å¾—èƒ½å¤Ÿé’ˆå¯¹ä¸ªä½“æ‚£è€…é¢„æµ‹æœªæ¥çš„åŒ»å­¦æ‰«æç»“æœã€‚ç„¶è€Œï¼Œå°½ç®¡äººå·¥æ™ºèƒ½å–å¾—äº†é‡å¤§è¿›å±•ï¼Œå½“å‰çš„æ–¹æ³•ä»ç„¶é¢ä¸´æŒ‘æˆ˜ï¼ŒåŒ…æ‹¬å®ç°æ‚£è€…ç‰¹å¼‚æ€§ä¸ªæ€§åŒ–ã€ç¡®ä¿æ—¶ç©ºä¸€è‡´æ€§ã€æœ‰æ•ˆåˆ©ç”¨çºµå‘æ•°æ®ä»¥åŠç®¡ç†åºå¤§çš„ä¸‰ç»´æ‰«æå†…å­˜éœ€æ±‚ã€‚ä¸ºäº†åº”å¯¹è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†Brain Latent Progressionï¼ˆBrLPï¼‰æ¨¡å‹ï¼Œè¿™æ˜¯ä¸€ç§æ–°å‹æ—¶ç©ºé¢„æµ‹æ¨¡å‹ï¼Œæ—¨åœ¨é¢„æµ‹ä¸‰ç»´è„‘éƒ¨MRIçš„ä¸ªä½“ç–¾ç—…è¿›å±•ã€‚BrLPçš„ä¸»è¦è´¡çŒ®ä½“ç°åœ¨å››ä¸ªæ–¹é¢ï¼šï¼ˆiï¼‰å®ƒåœ¨å°æ½œç©ºé—´ä¸­è¿è¡Œï¼Œå‡è½»äº†é«˜ç»´æˆåƒæ•°æ®å¸¦æ¥çš„è®¡ç®—æŒ‘æˆ˜ï¼›ï¼ˆiiï¼‰å®ƒæ˜ç¡®åœ°é›†æˆäº†å—è¯•è€…å…ƒæ•°æ®ï¼Œä»¥å¢å¼ºé¢„æµ‹çš„ä¸ªæ€§åŒ–ï¼›ï¼ˆiiiï¼‰å®ƒé€šè¿‡è¾…åŠ©æ¨¡å‹èå…¥ç–¾ç—…åŠ¨æ€çš„å…ˆéªŒçŸ¥è¯†ï¼Œä¿ƒè¿›çºµå‘æ•°æ®çš„æ•´åˆï¼›ï¼ˆivï¼‰å®ƒå¼•å…¥äº†Latent Average Stabilizationï¼ˆLASï¼‰ç®—æ³•ï¼Œè¯¥ç®—æ³•ï¼ˆaï¼‰åœ¨æ¨ç†æ—¶å¼ºåˆ¶é¢„æµ‹çš„è¿›å±•å…·æœ‰æ—¶ç©ºä¸€è‡´æ€§ï¼Œï¼ˆbï¼‰ä½¿æˆ‘ä»¬èƒ½å¤Ÿå¾—å‡ºé¢„æµ‹çš„ç¡®å®šæ€§åº¦é‡ã€‚æˆ‘ä»¬åœ¨åŒ…å«æ¥è‡ª2805åå—è¯•è€…çš„11730å¼ T1åŠ æƒï¼ˆT1wï¼‰è„‘MRIä¸Šè®­ç»ƒå’Œè¯„ä¼°BrLPæ¨¡å‹ï¼Œå¹¶åœ¨åŒ…å«æ¥è‡ª962åå—è¯•è€…çš„å¤–éƒ¨æµ‹è¯•é›†ä¸ŠéªŒè¯å…¶é€šç”¨æ€§ã€‚æˆ‘ä»¬çš„å®éªŒå°†BrLPç”Ÿæˆçš„MRIæ‰«æä¸çœŸå®çš„éšè®¿MRIè¿›è¡Œæ¯”è¾ƒï¼Œè¯æ˜äº†å…¶ç›¸è¾ƒäºç°æœ‰æ–¹æ³•çš„å“è¶Šå‡†ç¡®æ€§ã€‚ä»£ç å…¬å¼€åœ¨ï¼š<a target="_blank" rel="noopener" href="https://github.com/LemuelPuglisi/BrLP">https://github.com/LemuelPuglisi/BrLP</a>ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.08560v1">PDF</a> arXiv admin note: text overlap with arXiv:2405.03328</p>
<p><strong>Summary</strong><br>     æå‡ºçš„Brain Latent Progressionï¼ˆBrLPï¼‰æ¨¡å‹èƒ½é¢„æµ‹æ‚£è€…ä¸ªä½“åŒ–çš„ç–¾ç—…è¿›å±•æƒ…å†µã€‚BrLPåœ¨å››ç»´æ—¶ç©ºæ¨¡å‹ä¸­æ“ä½œï¼Œé€šè¿‡é™ä½é«˜ç»´å›¾åƒæ•°æ®å¸¦æ¥çš„è®¡ç®—æŒ‘æˆ˜ã€é›†æˆæ‚£è€…å…ƒæ•°æ®ä»¥æå‡é¢„æµ‹ä¸ªæ€§åŒ–ç¨‹åº¦ã€å¼•å…¥ç–¾ç—…åŠ¨æ€å…ˆéªŒçŸ¥è¯†è¾…åŠ©æ¨¡å‹å¹¶åˆ©ç”¨Latent Average Stabilizationï¼ˆLASï¼‰ç®—æ³•ç¡®ä¿é¢„æµ‹æ—¶ç©ºä¸€è‡´æ€§å’Œä¸ç¡®å®šæ€§åº¦é‡ï¼Œå®ç°äº†å¯¹3Dè„‘MRIä¸­ç–¾ç—…è¿›å±•çš„ç²¾å‡†é¢„æµ‹ã€‚ç»è¿‡å¤§è§„æ¨¡å®éªŒéªŒè¯ï¼ŒBrLPåœ¨å…¬å¼€æ•°æ®é›†ä¸Šçš„è¡¨ç°ä¼˜äºç°æœ‰æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Brain Latent Progressionï¼ˆBrLPï¼‰æ˜¯ä¸€ä¸ªç”¨äºé¢„æµ‹3Dè„‘MRIä¸­ç–¾ç—…è¿›å±•çš„æ—¶ç©ºæ¨¡å‹ã€‚</li>
<li>BrLPåœ¨å°å‹æ½œåœ¨ç©ºé—´å†…æ“ä½œï¼Œé™ä½é«˜ç»´å›¾åƒæ•°æ®çš„è®¡ç®—æŒ‘æˆ˜ã€‚</li>
<li>BrLPé›†æˆäº†æ‚£è€…çš„å…ƒæ•°æ®ï¼Œä»¥å¢å¼ºé¢„æµ‹çš„ä¸ªæ€§åŒ–å’Œå‡†ç¡®æ€§ã€‚</li>
<li>BrLPé€šè¿‡å¼•å…¥ç–¾ç—…åŠ¨æ€çš„å…ˆéªŒçŸ¥è¯†ï¼Œä¿ƒè¿›çºµå‘æ•°æ®çš„æ•´åˆã€‚</li>
<li>BrLPå¼•å…¥äº†Latent Average Stabilizationï¼ˆLASï¼‰ç®—æ³•ï¼Œç¡®ä¿é¢„æµ‹çš„æ—¶ç©ºä¸€è‡´æ€§å’Œä¸ç¡®å®šæ€§åº¦é‡ã€‚</li>
<li>BrLPåœ¨åŒ…å«å¤§é‡å—è¯•è€…çš„å¤§è§„æ¨¡æ•°æ®é›†ä¸Šè¿›è¡Œè®­ç»ƒå’Œè¯„ä¼°ï¼Œè¡¨ç°å‡ºä¼˜å¼‚çš„æ€§èƒ½ã€‚</li>
<li>BrLPçš„ä»£ç å·²å…¬å¼€å¯ç”¨ï¼Œä¸ºç ”ç©¶äººå‘˜æä¾›æ–¹ä¾¿çš„è®¿é—®é€”å¾„ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.08560">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-5d0fed920e5822457f9abda28738946b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4f3738d1893036b2ae9ca1526837015d.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Referring-Remote-Sensing-Image-Segmentation-via-Bidirectional-Alignment-Guided-Joint-Prediction"><a href="#Referring-Remote-Sensing-Image-Segmentation-via-Bidirectional-Alignment-Guided-Joint-Prediction" class="headerlink" title="Referring Remote Sensing Image Segmentation via Bidirectional Alignment   Guided Joint Prediction"></a>Referring Remote Sensing Image Segmentation via Bidirectional Alignment   Guided Joint Prediction</h2><p><strong>Authors:Tianxiang Zhang, Zhaokun Wen, Bo Kong, Kecheng Liu, Yisi Zhang, Peixian Zhuang, Jiangyun Li</strong></p>
<p>Referring Remote Sensing Image Segmentation (RRSIS) is critical for ecological monitoring, urban planning, and disaster management, requiring precise segmentation of objects in remote sensing imagery guided by textual descriptions. This task is uniquely challenging due to the considerable vision-language gap, the high spatial resolution and broad coverage of remote sensing imagery with diverse categories and small targets, and the presence of clustered, unclear targets with blurred edges. To tackle these issues, we propose \ours, a novel framework designed to bridge the vision-language gap, enhance multi-scale feature interaction, and improve fine-grained object differentiation. Specifically, \ours introduces: (1) the Bidirectional Spatial Correlation (BSC) for improved vision-language feature alignment, (2) the Target-Background TwinStream Decoder (T-BTD) for precise distinction between targets and non-targets, and (3) the Dual-Modal Object Learning Strategy (D-MOLS) for robust multimodal feature reconstruction. Extensive experiments on the benchmark datasets RefSegRS and RRSIS-D demonstrate that \ours achieves state-of-the-art performance. Specifically, \ours improves the overall IoU (oIoU) by 3.76 percentage points (80.57) and 1.44 percentage points (79.23) on the two datasets, respectively. Additionally, it outperforms previous methods in the mean IoU (mIoU) by 5.37 percentage points (67.95) and 1.84 percentage points (66.04), effectively addressing the core challenges of RRSIS with enhanced precision and robustness. </p>
<blockquote>
<p>é¥æ„Ÿå›¾åƒåˆ†å‰²å‚ç…§ï¼ˆRRSISï¼‰å¯¹äºç”Ÿæ€ç›‘æµ‹ã€åŸå¸‚è§„åˆ’å’Œç¾å®³ç®¡ç†è‡³å…³é‡è¦ï¼Œå®ƒè¦æ±‚æ ¹æ®æ–‡æœ¬æè¿°ç²¾ç¡®åœ°åˆ†å‰²é¥æ„Ÿå›¾åƒä¸­çš„å¯¹è±¡ã€‚ç”±äºè§†è¯­è¨€å·®è·è¾ƒå¤§ï¼Œé¥æ„Ÿå›¾åƒçš„é«˜ç©ºé—´åˆ†è¾¨ç‡å’Œå¹¿æ³›è¦†ç›–èŒƒå›´ï¼Œå…·æœ‰å¤šç§ç±»åˆ«å’Œå°ç›®æ ‡ï¼Œä»¥åŠå­˜åœ¨è¾¹ç•Œæ¨¡ç³Šã€ç›®æ ‡ä¸æ˜ç¡®çš„æƒ…å†µï¼Œè¿™é¡¹ä»»åŠ¡å…·æœ‰ç‹¬ç‰¹çš„æŒ‘æˆ˜æ€§ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªæ–°é¢–æ¡†æ¶\oursï¼Œæ—¨åœ¨å¼¥åˆè§†è¯­è¨€å·®è·ï¼Œå¢å¼ºå¤šå°ºåº¦ç‰¹å¾äº¤äº’ï¼Œå¹¶æ”¹å–„ç²¾ç»†ç²’åº¦å¯¹è±¡åŒºåˆ†ã€‚å…·ä½“æ¥è¯´ï¼Œ\ourså¼•å…¥äº†ï¼šï¼ˆ1ï¼‰åŒå‘ç©ºé—´ç›¸å…³æ€§ï¼ˆBSCï¼‰ä»¥æ”¹è¿›è§†è¯­è¨€ç‰¹å¾å¯¹é½ï¼›ï¼ˆ2ï¼‰ç›®æ ‡èƒŒæ™¯TwinStreamè§£ç å™¨ï¼ˆT-BTDï¼‰ä»¥ç²¾ç¡®åŒºåˆ†ç›®æ ‡å’Œéç›®æ ‡ï¼›ï¼ˆ3ï¼‰åŒæ¨¡æ€å¯¹è±¡å­¦ä¹ ç­–ç•¥ï¼ˆD-MOLSï¼‰ç”¨äºç¨³å¥çš„å¤šæ¨¡æ€ç‰¹å¾é‡å»ºã€‚åœ¨RefSegRSå’ŒRRSIS-DåŸºå‡†æ•°æ®é›†ä¸Šçš„å¤§é‡å®éªŒè¡¨æ˜ï¼Œ\oursè¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚å…·ä½“æ¥è¯´ï¼Œ\oursåœ¨ä¸¤ä¸ªæ•°æ®é›†ä¸Šçš„æ•´ä½“IoUï¼ˆoIoUï¼‰åˆ†åˆ«æé«˜äº†3.76ä¸ªç™¾åˆ†ç‚¹ï¼ˆè¾¾åˆ°80.57ï¼‰å’Œ1.44ä¸ªç™¾åˆ†ç‚¹ï¼ˆè¾¾åˆ°79.23ï¼‰ã€‚æ­¤å¤–ï¼Œå®ƒåœ¨å¹³å‡IoUï¼ˆmIoUï¼‰ä¸Šè¾ƒä¹‹å‰çš„æ–¹æ³•æé«˜äº†5.37ä¸ªç™¾åˆ†ç‚¹ï¼ˆè¾¾åˆ°67.95ï¼‰å’Œ1.84ä¸ªç™¾åˆ†ç‚¹ï¼ˆè¾¾åˆ°66.04ï¼‰ï¼Œæœ‰æ•ˆåœ°è§£å†³äº†RRSISçš„æ ¸å¿ƒæŒ‘æˆ˜ï¼Œæé«˜äº†ç²¾åº¦å’Œç¨³å¥æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.08486v1">PDF</a> </p>
<p><strong>Summary</strong><br>     æ‰€æå‡ºçš„æ–°æ–¹æ³•é€šè¿‡å¼•å…¥åŒå‘ç©ºé—´ç›¸å…³æ€§ï¼ˆBSCï¼‰ã€ç›®æ ‡èƒŒæ™¯TwinStreamè§£ç å™¨ï¼ˆT-BTDï¼‰å’ŒåŒæ¨¡æ€å¯¹è±¡å­¦ä¹ ç­–ç•¥ï¼ˆD-MOLSï¼‰ï¼ŒæˆåŠŸè§£å†³äº†é¥æ„Ÿå›¾åƒåˆ†å‰²ä¸­çš„è§†è§‰è¯­è¨€å·®è·ã€å¤šå°ºåº¦ç‰¹å¾äº¤äº’ä»¥åŠç²¾ç»†å¯¹è±¡åŒºåˆ†ç­‰é—®é¢˜ï¼Œåœ¨RefSegRSå’ŒRRSIS-Dç­‰åŸºå‡†æ•°æ®é›†ä¸Šå®ç°äº†å“è¶Šçš„æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ–°æå‡ºçš„æ¡†æ¶è§£å†³äº†è¿œç¨‹é¥æ„Ÿå›¾åƒåˆ†å‰²ï¼ˆRRSISï¼‰ä¸­çš„è§†è§‰è¯­è¨€å·®è·é—®é¢˜ã€‚</li>
<li>é€šè¿‡å¼•å…¥åŒå‘ç©ºé—´ç›¸å…³æ€§ï¼ˆBSCï¼‰ï¼Œæ”¹è¿›äº†è§†è§‰è¯­è¨€ç‰¹å¾çš„åŒ¹é…ã€‚</li>
<li>ç›®æ ‡èƒŒæ™¯TwinStreamè§£ç å™¨ï¼ˆT-BTDï¼‰çš„è®¾è®¡ï¼Œèƒ½å¤Ÿç²¾ç¡®åŒºåˆ†ç›®æ ‡å’Œéç›®æ ‡ã€‚</li>
<li>åŒæ¨¡æ€å¯¹è±¡å­¦ä¹ ç­–ç•¥ï¼ˆD-MOLSï¼‰å¢å¼ºäº†å¤šæ¨¡æ€ç‰¹å¾çš„é‡å»ºèƒ½åŠ›ã€‚</li>
<li>åœ¨RefSegRSå’ŒRRSIS-Dæ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œæ–°æ¡†æ¶å®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚</li>
<li>æ–°æ¡†æ¶æé«˜äº†æ•´ä½“IoUï¼ˆoIoUï¼‰æŒ‡æ ‡ï¼Œç›¸æ¯”ä¹‹å‰çš„æ–¹æ³•æœ‰æ˜æ˜¾ä¼˜åŠ¿ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.08486">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-c91ab3cea34ed049300a7c5eafc00e11.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-67f36af0d91fad95fadf2773a21c7e34.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-cfe53a2cdeb5e585a3abc90fc944b2e6.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-8ac373a363779db11172344d362a8d24.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-1c468a0a6093531c437bc6b96d801b86.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e8bd7be7053d54129a2abc50bfae3369.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Hi-End-MAE-Hierarchical-encoder-driven-masked-autoencoders-are-stronger-vision-learners-for-medical-image-segmentation"><a href="#Hi-End-MAE-Hierarchical-encoder-driven-masked-autoencoders-are-stronger-vision-learners-for-medical-image-segmentation" class="headerlink" title="Hi-End-MAE: Hierarchical encoder-driven masked autoencoders are stronger   vision learners for medical image segmentation"></a>Hi-End-MAE: Hierarchical encoder-driven masked autoencoders are stronger   vision learners for medical image segmentation</h2><p><strong>Authors:Fenghe Tang, Qingsong Yao, Wenxin Ma, Chenxu Wu, Zihang Jiang, S. Kevin Zhou</strong></p>
<p>Medical image segmentation remains a formidable challenge due to the label scarcity. Pre-training Vision Transformer (ViT) through masked image modeling (MIM) on large-scale unlabeled medical datasets presents a promising solution, providing both computational efficiency and model generalization for various downstream tasks. However, current ViT-based MIM pre-training frameworks predominantly emphasize local aggregation representations in output layers and fail to exploit the rich representations across different ViT layers that better capture fine-grained semantic information needed for more precise medical downstream tasks. To fill the above gap, we hereby present Hierarchical Encoder-driven MAE (Hi-End-MAE), a simple yet effective ViT-based pre-training solution, which centers on two key innovations: (1) Encoder-driven reconstruction, which encourages the encoder to learn more informative features to guide the reconstruction of masked patches; and (2) Hierarchical dense decoding, which implements a hierarchical decoding structure to capture rich representations across different layers. We pre-train Hi-End-MAE on a large-scale dataset of 10K CT scans and evaluated its performance across seven public medical image segmentation benchmarks. Extensive experiments demonstrate that Hi-End-MAE achieves superior transfer learning capabilities across various downstream tasks, revealing the potential of ViT in medical imaging applications. The code is available at: <a target="_blank" rel="noopener" href="https://github.com/FengheTan9/Hi-End-MAE">https://github.com/FengheTan9/Hi-End-MAE</a> </p>
<blockquote>
<p>åŒ»å­¦å›¾åƒåˆ†å‰²ä»ç„¶æ˜¯ä¸€ä¸ªå·¨å¤§çš„æŒ‘æˆ˜ï¼Œä¸»è¦æ˜¯ç”±äºç¼ºä¹æ ‡ç­¾ã€‚é€šè¿‡å¤§è§„æ¨¡æ— æ ‡ç­¾åŒ»å­¦æ•°æ®é›†å¯¹è§†è§‰è½¬æ¢å™¨ï¼ˆViTï¼‰è¿›è¡Œæ©æ¨¡å›¾åƒå»ºæ¨¡ï¼ˆMIMï¼‰çš„é¢„è®­ç»ƒï¼Œå‘ˆç°å‡ºä¸€ç§å¾ˆæœ‰å‰æ™¯çš„è§£å†³æ–¹æ¡ˆï¼Œä¸ºå„ç§ä¸‹æ¸¸ä»»åŠ¡æä¾›äº†è®¡ç®—æ•ˆç‡å’Œæ¨¡å‹æ³›åŒ–èƒ½åŠ›ã€‚ç„¶è€Œï¼Œå½“å‰çš„åŸºäºViTçš„MIMé¢„è®­ç»ƒæ¡†æ¶ä¸»è¦ä¾§é‡äºè¾“å‡ºå±‚çš„å±€éƒ¨èšåˆè¡¨ç¤ºï¼Œå¹¶æœªèƒ½å……åˆ†åˆ©ç”¨ä¸åŒViTå±‚ä¸­çš„ä¸°å¯Œè¡¨ç¤ºï¼Œè¿™äº›å±‚èƒ½æ›´å¥½åœ°æ•æ‰ç”¨äºæ›´ç²¾ç¡®åŒ»å­¦ä¸‹æ¸¸ä»»åŠ¡çš„ç»†ç²’åº¦è¯­ä¹‰ä¿¡æ¯ã€‚ä¸ºäº†å¡«è¡¥ä¸Šè¿°ç©ºç™½ï¼Œæˆ‘ä»¬åœ¨æ­¤æå‡ºåˆ†å±‚ç¼–ç å™¨é©±åŠ¨çš„MAEï¼ˆHi-End-MAEï¼‰ï¼Œè¿™æ˜¯ä¸€ç§ç®€å•æœ‰æ•ˆçš„åŸºäºViTçš„é¢„è®­ç»ƒè§£å†³æ–¹æ¡ˆï¼Œå…¶æ ¸å¿ƒåŒ…å«ä¸¤é¡¹é‡è¦åˆ›æ–°ï¼šï¼ˆ1ï¼‰ç¼–ç å™¨é©±åŠ¨é‡å»ºï¼Œè¿™é¼“åŠ±ç¼–ç å™¨å­¦ä¹ æ›´å¤šä¿¡æ¯ç‰¹å¾ä»¥æŒ‡å¯¼æ©ç å—çš„é‡å»ºï¼›ï¼ˆ2ï¼‰åˆ†å±‚å¯†é›†è§£ç ï¼Œè¿™å®ç°äº†ä¸€ç§åˆ†å±‚è§£ç ç»“æ„ï¼Œä»¥æ•è·ä¸åŒå±‚çš„ä¸°å¯Œè¡¨ç¤ºã€‚æˆ‘ä»¬åœ¨åŒ…å«10Kæ¬¡CTæ‰«æçš„å¤§è§„æ¨¡æ•°æ®é›†ä¸Šé¢„è®­ç»ƒHi-End-MAEï¼Œå¹¶åœ¨ä¸ƒä¸ªå…¬å…±åŒ»å­¦å›¾åƒåˆ†å‰²åŸºå‡†ä¸Šè¯„ä¼°å…¶æ€§èƒ½ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼ŒHi-End-MAEåœ¨å„ç§ä¸‹æ¸¸ä»»åŠ¡ä¸Šå®ç°äº†å“è¶Šçš„è¿ç§»å­¦ä¹ èƒ½åŠ›ï¼Œæ­ç¤ºäº†ViTåœ¨åŒ»å­¦æˆåƒåº”ç”¨ä¸­çš„æ½œåŠ›ã€‚ä»£ç å¯ç”¨åœ¨ï¼š<a target="_blank" rel="noopener" href="https://github.com/FengheTan9/Hi-End-MAE">https://github.com/FengheTan9/Hi-End-MAE</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.08347v1">PDF</a> 19 pages, Code: <a target="_blank" rel="noopener" href="https://github.com/FengheTan9/Hi-End-MAE">https://github.com/FengheTan9/Hi-End-MAE</a></p>
<p><strong>Summary</strong><br>     åŒ»å­¦å›¾åƒåˆ†å‰²å› æ ‡ç­¾ç¨€ç¼ºè€Œé¢ä¸´æŒ‘æˆ˜ã€‚é€šè¿‡å¤§è§„æ¨¡æ— æ ‡ç­¾åŒ»å­¦æ•°æ®é›†å¯¹è§†è§‰è½¬æ¢å™¨ï¼ˆViTï¼‰è¿›è¡Œæ©æ¨¡å›¾åƒå»ºæ¨¡ï¼ˆMIMï¼‰çš„é¢„è®­ç»ƒå±•ç°å‡ºä¸€ç§è§£å†³æ–¹æ¡ˆçš„æ½œåŠ›ï¼Œèƒ½æé«˜è®¡ç®—æ•ˆç‡å’Œæ¨¡å‹åœ¨å„ç§ä¸‹æ¸¸ä»»åŠ¡ä¸­çš„æ³›åŒ–èƒ½åŠ›ã€‚ç„¶è€Œï¼Œç°æœ‰çš„åŸºäºViTçš„MIMé¢„è®­ç»ƒæ¡†æ¶ä¸»è¦ä¾§é‡äºè¾“å‡ºå±‚çš„å±€éƒ¨èšåˆè¡¨ç¤ºï¼Œæœªèƒ½å……åˆ†åˆ©ç”¨ä¸åŒViTå±‚ä¸­çš„ä¸°å¯Œè¡¨ç¤ºï¼Œè¿™äº›è¡¨ç¤ºèƒ½æ›´å¥½åœ°æ•æ‰ç”¨äºæ›´ç²¾ç¡®åŒ»å­¦ä¸‹æ¸¸ä»»åŠ¡çš„ç»†ç²’åº¦è¯­ä¹‰ä¿¡æ¯ã€‚ä¸ºå¡«è¡¥ä¸Šè¿°ç©ºç™½ï¼Œæˆ‘ä»¬æå‡ºå±‚æ¬¡ç¼–ç å™¨é©±åŠ¨çš„MAEï¼ˆHi-End-MAEï¼‰ï¼Œè¿™æ˜¯ä¸€ç§ç®€å•æœ‰æ•ˆçš„åŸºäºViTçš„é¢„è®­ç»ƒè§£å†³æ–¹æ¡ˆï¼Œå…¶æ ¸å¿ƒåˆ›æ–°ç‚¹åœ¨äºï¼šä¸€æ˜¯ç¼–ç å™¨é©±åŠ¨çš„é‡æ„ï¼Œé¼“åŠ±ç¼–ç å™¨å­¦ä¹ æ›´å¤šä¿¡æ¯ç‰¹å¾ä»¥æŒ‡å¯¼æ©ç è¡¥ä¸çš„é‡æ„ï¼›äºŒæ˜¯å±‚æ¬¡å¯†é›†è§£ç ï¼Œå®ç°å±‚æ¬¡è§£ç ç»“æ„ä»¥æ•è·ä¸åŒå±‚æ¬¡çš„ä¸°å¯Œè¡¨ç¤ºã€‚æˆ‘ä»¬åœ¨åŒ…å«ä¸€ä¸‡æ¬¡CTæ‰«æçš„å¤§è§„æ¨¡æ•°æ®é›†ä¸Šé¢„è®­ç»ƒHi-End-MAEï¼Œå¹¶åœ¨ä¸ƒä¸ªå…¬å¼€åŒ»å­¦å›¾åƒåˆ†å‰²åŸºå‡†ä¸Šè¯„ä¼°å…¶æ€§èƒ½ã€‚å®éªŒè¡¨æ˜ï¼ŒHi-End-MAEåœ¨å„ç§ä¸‹æ¸¸ä»»åŠ¡ä¸­å…·æœ‰å‡ºè‰²çš„è¿ç§»å­¦ä¹ èƒ½åŠ›ï¼Œå±•ç°äº†ViTåœ¨åŒ»å­¦æˆåƒåº”ç”¨ä¸­çš„æ½œåŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>åŒ»å­¦å›¾åƒåˆ†å‰²é¢ä¸´æ ‡ç­¾ç¨€ç¼ºçš„æŒ‘æˆ˜ã€‚</li>
<li>é¢„è®­ç»ƒè§†è§‰è½¬æ¢å™¨ï¼ˆViTï¼‰é€šè¿‡æ©æ¨¡å›¾åƒå»ºæ¨¡ï¼ˆMIMï¼‰åœ¨å¤§å‹æ— æ ‡ç­¾åŒ»å­¦æ•°æ®é›†ä¸Šæ˜¯ä¸€ç§æœ‰å‰æ™¯çš„è§£å†³æ–¹æ¡ˆã€‚</li>
<li>å½“å‰åŸºäºViTçš„MIMé¢„è®­ç»ƒæ¡†æ¶ä¸»è¦å…³æ³¨è¾“å‡ºå±‚çš„å±€éƒ¨èšåˆè¡¨ç¤ºã€‚</li>
<li>Hi-End-MAEé€šè¿‡ç¼–ç å™¨é©±åŠ¨çš„é‡æ„å’Œå±‚æ¬¡å¯†é›†è§£ç æ¥è§£å†³ä¸Šè¿°é—®é¢˜ã€‚</li>
<li>ç¼–ç å™¨é©±åŠ¨çš„é‡æ„é¼“åŠ±ç¼–ç å™¨å­¦ä¹ æ›´å¤šä¿¡æ¯ç‰¹å¾ä»¥æŒ‡å¯¼æ©ç è¡¥ä¸çš„é‡æ„ã€‚</li>
<li>å±‚æ¬¡å¯†é›†è§£ç æ—¨åœ¨æ•è·ä¸åŒå±‚æ¬¡çš„ä¸°å¯Œè¡¨ç¤ºã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.08347">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-5178ece69358f5c154c14087166dca3f.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-498a4896204c168f5018e6a0d4e6cb32.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-31a24c243e039d4fee3a395c329cdaa7.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-82b5ca3f28882dab571a3a64a25ac750.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b1d7a9cced67687a8ab65afd3013d3a2.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Screener-Self-supervised-Pathology-Segmentation-Model-for-3D-Medical-Images"><a href="#Screener-Self-supervised-Pathology-Segmentation-Model-for-3D-Medical-Images" class="headerlink" title="Screener: Self-supervised Pathology Segmentation Model for 3D Medical   Images"></a>Screener: Self-supervised Pathology Segmentation Model for 3D Medical   Images</h2><p><strong>Authors:Mikhail Goncharov, Eugenia Soboleva, Mariia Donskova, Ivan Oseledets, Marina Munkhoeva, Maxim Panov</strong></p>
<p>Accurate segmentation of all pathological findings in 3D medical images remains a significant challenge, as supervised models are limited to detecting only the few pathology classes annotated in existing datasets. To address this, we frame pathology segmentation as an unsupervised visual anomaly segmentation (UVAS) problem, leveraging the inherent rarity of pathological patterns compared to healthy ones. We enhance the existing density-based UVAS framework with two key innovations: (1) dense self-supervised learning (SSL) for feature extraction, eliminating the need for supervised pre-training, and (2) learned, masking-invariant dense features as conditioning variables, replacing hand-crafted positional encodings. Trained on over 30,000 unlabeled 3D CT volumes, our model, Screener, outperforms existing UVAS methods on four large-scale test datasets comprising 1,820 scans with diverse pathologies. Code and pre-trained models will be made publicly available. </p>
<blockquote>
<p>åœ¨3DåŒ»å­¦å›¾åƒä¸­å¯¹æ‰€æœ‰ç—…ç†è¡¨ç°è¿›è¡Œç²¾ç¡®åˆ†å‰²ä»ç„¶æ˜¯ä¸€ä¸ªå·¨å¤§çš„æŒ‘æˆ˜ï¼Œå› ä¸ºç›‘ç£æ¨¡å‹ä»…é™äºæ£€æµ‹ç°æœ‰æ•°æ®é›†ä¸­å·²æ ‡æ³¨çš„å°‘æ•°ç—…ç†ç±»åˆ«ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬å°†ç—…ç†åˆ†å‰²ä½œä¸ºä¸€ä¸ªæ— ç›‘ç£çš„è§†è§‰å¼‚å¸¸åˆ†å‰²ï¼ˆUVASï¼‰é—®é¢˜æ¥è§£å†³ï¼Œåˆ©ç”¨ç—…ç†æ¨¡å¼ä¸å¥åº·æ¨¡å¼ç›¸æ¯”å›ºæœ‰çš„ç¨€æœ‰æ€§ã€‚æˆ‘ä»¬å¯¹ç°æœ‰çš„åŸºäºå¯†åº¦çš„UVASæ¡†æ¶è¿›è¡Œäº†ä¸¤é¡¹å…³é”®åˆ›æ–°ï¼šä¸€æ˜¯å¯†é›†çš„è‡ªç›‘ç£å­¦ä¹ ï¼ˆSSLï¼‰ç”¨äºç‰¹å¾æå–ï¼Œæ— éœ€ç›‘ç£é¢„è®­ç»ƒï¼›äºŒæ˜¯å­¦ä¹ å¾—åˆ°çš„ã€å¯¹é®æŒ¡å…·æœ‰ä¸å˜æ€§çš„å¯†é›†ç‰¹å¾ä½œä¸ºæ¡ä»¶å˜é‡ï¼Œå–ä»£äº†æ‰‹å·¥ä½ç½®ç¼–ç ã€‚åœ¨è¶…è¿‡3ä¸‡ä¸ªæ— æ ‡ç­¾çš„3D CTä½“ç§¯ä¸Šè¿›è¡Œè®­ç»ƒåï¼Œæˆ‘ä»¬çš„æ¨¡å‹â€”â€”ç­›é€‰å™¨åœ¨ç”±å¤šä¸ªåŒ…å«å„ç§ç—…ç†è¡¨ç°çš„æ‰«æç»„æˆçš„å¤§è§„æ¨¡æµ‹è¯•æ•°æ®é›†ä¸Šçš„è¡¨ç°ä¼˜äºç°æœ‰çš„UVASæ–¹æ³•ï¼Œå…±æ¶µç›–1820ä¸ªæ‰«æå›¾åƒã€‚æ¨¡å‹å’Œé¢„è®­ç»ƒæ¨¡å‹å°†å…¬å¼€æä¾›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.08321v1">PDF</a> </p>
<p><strong>æ‘˜è¦</strong></p>
<p>æœ¬æ–‡æå‡ºä¸€ç§é’ˆå¯¹ä¸‰ç»´åŒ»å­¦å›¾åƒä¸­æ‰€æœ‰ç—…ç†ç‰¹å¾çš„åˆ†å‰²é—®é¢˜ï¼Œé€šè¿‡æ„å»ºåŸºäºå¯†åº¦åˆ†æçš„æ— ç›‘ç£è§†è§‰å¼‚å¸¸åˆ†å‰²æ¡†æ¶ï¼Œå®ç°äº†å¯¹å¤šç§ç—…ç†ç‰¹å¾çš„å‡†ç¡®åˆ†å‰²ã€‚æœ¬æ–‡åˆ›æ–°æ€§åœ°å¼•å…¥ä¸¤é¡¹æŠ€æœ¯ï¼šä¸€æ˜¯é€šè¿‡å¯†é›†è‡ªç›‘ç£å­¦ä¹ æå–ç‰¹å¾ï¼Œé¿å…äº†éœ€è¦æ ‡æ³¨æ•°æ®è¿›è¡Œé¢„è®­ç»ƒçš„é—®é¢˜ï¼›äºŒæ˜¯åˆ©ç”¨è®­ç»ƒå‡ºçš„ã€å…·æœ‰é®æŒ¡ä¸å˜æ€§çš„å¯†é›†ç‰¹å¾ä½œä¸ºæ¡ä»¶å˜é‡ï¼Œå–ä»£äº†æ‰‹å·¥è®¾è®¡çš„ä½ç½®ç¼–ç æ–¹å¼ã€‚è¯¥æ¨¡å‹åœ¨è¶…è¿‡ä¸‰ä¸‡ä»½æ— æ ‡ç­¾çš„ä¸‰ç»´CTå›¾åƒæ•°æ®ä¸Šè¿›è¡Œè®­ç»ƒï¼Œåœ¨åŒ…å«åƒä½™å¼ å«æœ‰ä¸åŒç—…ç†çš„æ‰«ææ•°æ®çš„æµ‹è¯•é›†ä¸Šè¡¨ç°å‡ºäº†ä¼˜å¼‚çš„æ€§èƒ½ã€‚æ­¤å¤–ï¼Œä½œè€…å°†å…¬å¼€æ¨¡å‹å’Œä»£ç ä¾›å…¬ä¼—ä½¿ç”¨ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>æå‡ºå°†ç—…ç†åˆ†å‰²é—®é¢˜è§†ä¸ºæ— ç›‘ç£è§†è§‰å¼‚å¸¸åˆ†å‰²ï¼ˆUVASï¼‰é—®é¢˜ï¼Œåˆ©ç”¨ç—…ç†æ¨¡å¼ä¸å¥åº·æ¨¡å¼ä¹‹é—´çš„å›ºæœ‰å·®å¼‚è¿›è¡Œè¯†åˆ«ã€‚</li>
<li>åˆ›æ–°æ€§åœ°å¼•å…¥å¯†é›†è‡ªç›‘ç£å­¦ä¹ è¿›è¡Œç‰¹å¾æå–ï¼Œç®€åŒ–äº†å¯¹æ ‡æ³¨æ•°æ®çš„ä¾èµ–ã€‚</li>
<li>åˆ©ç”¨è®­ç»ƒå‡ºçš„ã€å…·æœ‰é®æŒ¡ä¸å˜æ€§çš„å¯†é›†ç‰¹å¾ä½œä¸ºæ¡ä»¶å˜é‡æ›¿ä»£ä¼ ç»Ÿæ‰‹å·¥è®¾è®¡çš„ä½ç½®ç¼–ç ã€‚</li>
<li>æ¨¡å‹åœ¨å¤§é‡æ— æ ‡ç­¾çš„ä¸‰ç»´CTå›¾åƒæ•°æ®ä¸Šè¿›è¡Œè®­ç»ƒï¼Œè¯æ˜äº†å…¶å¯¹ä¸åŒç—…ç†ç‰¹å¾çš„æœ‰æ•ˆè¯†åˆ«èƒ½åŠ›ã€‚</li>
<li>æ¨¡å‹åœ¨åŒ…å«å¤šç§ç—…ç†çš„å¤§è§„æ¨¡æµ‹è¯•æ•°æ®é›†ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œè¯æ˜äº†å…¶å¹¿æ³›çš„åº”ç”¨æ½œåŠ›ã€‚</li>
<li>æ¨¡å‹å’Œä»£ç å°†å…¬å¼€ä¾›å…¬ä¼—ä½¿ç”¨ï¼Œä¸ºç›¸å…³ç ”ç©¶æä¾›ä¾¿åˆ©ã€‚</li>
<li>æ­¤æ–¹æ³•æä¾›äº†ä¸€ä¸ªå…¨æ–°çš„è§†è§’æ¥è§£å†³åŒ»å­¦å›¾åƒåˆ†å‰²ä¸­çš„æŒ‘æˆ˜æ€§é—®é¢˜ï¼Œå…·æœ‰å¹¿æ³›çš„åº”ç”¨å‰æ™¯å’Œé‡è¦çš„å®ç”¨ä»·å€¼ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.08321">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-106cfbaec8009d3329be69854f3cc4ee.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3e65b87e0bc16d644c94dbbbcb48c926.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-dea1a3e224d9797998eda888b4f4b2b5.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-c45eea0b973bc66568570a038f004e81.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-4d54b306d0b932a9f89b0b508e255666.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-17c8033f462087a3a0d80ee4bca2e08d.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Soft-X-ray-Imager-of-the-Xtend-system-onboard-XRISM"><a href="#Soft-X-ray-Imager-of-the-Xtend-system-onboard-XRISM" class="headerlink" title="Soft X-ray Imager of the Xtend system onboard XRISM"></a>Soft X-ray Imager of the Xtend system onboard XRISM</h2><p><strong>Authors:Hirofumi Noda, Koji Mori, Hiroshi Tomida, Hiroshi Nakajima, Takaaki Tanaka, Hiroshi Murakami, Hiroyuki Uchida, Hiromasa Suzuki, Shogo Benjamin Kobayashi, Tomokage Yoneyama, Kouichi Hagino, Kumiko Nobukawa, Hideki Uchiyama, Masayoshi Nobukawa, Hironori Matsumoto, Takeshi Go Tsuru, Makoto Yamauchi, Isamu Hatsukade, Hirokazu Odaka, Takayoshi Kohmura, Kazutaka Yamaoka, Tessei Yoshida, Yoshiaki Kanemaru, Junko Hiraga, Tadayasu Dotani, Masanobu Ozaki, Hiroshi Tsunemi, Jin Sato, Toshiyuki Takaki, Yuta Terada, Keitaro Miyazaki, Kohei Kusunoki, Yoshinori Otsuka, Haruhiko Yokosu, Wakana Yonemaru, Kazuhiro Ichikawa, Hanako Nakano, Reo Takemoto, Tsukasa Matsushima, Reika Urase, Jun Kurashima, Kotomi Fuchi, Kaito Hayakawa, Masahiro Fukuda, Takamitsu Kamei, Yoh Asahina, Shun Inoue, Amano Yuki, Yuma Aoki, Yamato Ito, Tomoya Kamatani, Kouta Takayama, Takashi Sako, Marina Yoshimoto, Kohei Shima, Mayu Higuchi, Kaito Ninoyu, Daiki Aoki, Shun Tsunomachi, Kiyoshi Hayashida</strong></p>
<p>The Soft X-ray Imager (SXI) is the X-ray charge-coupled device (CCD) camera for the soft X-ray imaging telescope Xtend installed on the X-ray Imaging and Spectroscopy Mission (XRISM), which was adopted as a recovery mission for the Hitomi X-ray satellite and was successfully launched on 2023 September 7 (JST). In order to maximize the science output of XRISM, we set the requirements for Xtend and find that the CCD set employed in the Hitomi&#x2F;SXI or similar, i.e., a $2 \times 2$ array of back-illuminated CCDs with a $200~\mu$m-thick depletion layer, would be practically best among available choices, when used in combination with the X-ray mirror assembly. We design the XRISM&#x2F;SXI, based on the Hitomi&#x2F;SXI, to have a wide field of view of $38â€™ \times 38â€™$ in the $0.4-13$ keV energy range. We incorporated several significant improvements from the Hitomi&#x2F;SXI into the CCD chip design to enhance the optical-light blocking capability and to increase the cosmic-ray tolerance, reducing the degradation of charge-transfer efficiency in orbit. By the time of the launch of XRISM, the imaging and spectroscopic capabilities of the SXI has been extensively studied in on-ground experiments with the full flight-model configuration or equivalent setups and confirmed to meet the requirements. The optical blocking capability, the cooling and temperature control performance, and the transmissivity and quantum efficiency to incident X-rays of the CCDs are also all confirmed to meet the requirements. Thus, we successfully complete the pre-flight development of the SXI for XRISM. </p>
<blockquote>
<p>è½¯Xå°„çº¿æˆåƒä»ªï¼ˆSXIï¼‰æ˜¯å®‰è£…åœ¨Xå°„çº¿æˆåƒå’Œå…‰è°±ä»»åŠ¡ï¼ˆXRISMï¼‰ä¸Šçš„è½¯Xå°„çº¿æˆåƒæœ›è¿œé•œXtendçš„Xå°„çº¿ç”µè·è€¦åˆå™¨ä»¶ï¼ˆCCDï¼‰ç›¸æœºã€‚XRISMæ˜¯è¢«Hitomi Xå°„çº¿å«æ˜Ÿçš„å¤è‹ä»»åŠ¡æ‰€é‡‡çº³çš„ï¼Œå¹¶äº2023å¹´9æœˆ7æ—¥ï¼ˆæ—¥æœ¬æ ‡å‡†æ—¶é—´ï¼‰æˆåŠŸå‘å°„ã€‚ä¸ºäº†æœ€å¤§åŒ–XRISMçš„ç§‘å­¦äº§å‡ºï¼Œæˆ‘ä»¬å¯¹Xtendæå‡ºäº†è¦æ±‚ï¼Œå¹¶å‘ç°ä¸Xå°„çº¿é•œé¢ç»„åˆè£…é…æ—¶ï¼Œé‡‡ç”¨Hitomi&#x2F;SXIæˆ–ç±»ä¼¼çš„CCDé›†ï¼Œå³é‡‡ç”¨èƒŒç…§å¼CCDçš„$2\times 2$é˜µåˆ—ï¼Œå…¶ä¸­å«ä¸€ä¸ªåšä¸º$200\mu m$çš„è€—å°½å±‚ï¼Œåœ¨ç°æœ‰é€‰æ‹©ä¸­ï¼Œè¿™ç§é…ç½®å®é™…ä¸Šæœ€ä¸ºç†æƒ³ã€‚æˆ‘ä»¬åŸºäºHitomi&#x2F;SXIè®¾è®¡XRISM&#x2F;SXIï¼Œåœ¨$0.4-13$keVèƒ½é‡èŒƒå›´å†…å…·æœ‰å¹¿é˜”çš„è§†é‡ï¼Œå³$38â€™\times 38â€™$ã€‚æˆ‘ä»¬å¯¹CCDèŠ¯ç‰‡è®¾è®¡è¿›è¡Œäº†é‡å¤§æ”¹è¿›ï¼Œå¢å¼ºäº†å…‰å­¦å…‰å±è”½èƒ½åŠ›å’Œå®‡å®™å°„çº¿å®¹å¿åº¦ï¼Œå‡å°‘äº†è½¨é“ä¸Šçš„ç”µè·ä¼ è¾“æ•ˆç‡ä¸‹é™ã€‚æˆªè‡³XRISMå‘å°„æ—¶ï¼ŒSXIçš„æˆåƒå’Œå…‰è°±èƒ½åŠ›å·²ç»é€šè¿‡åœ°é¢å®éªŒè¿›è¡Œäº†å¹¿æ³›çš„ç ”ç©¶ï¼Œå®éªŒé‡‡ç”¨å…¨é£è¡Œæ¨¡å‹é…ç½®æˆ–ç­‰æ•ˆè®¾ç½®ï¼Œå¹¶ç¡®è®¤æ»¡è¶³è¦æ±‚ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜ç¡®è®¤äº†CCDçš„å…‰å­¦å±è”½èƒ½åŠ›ã€å†·å´ä¸æ¸©åº¦æ§åˆ¶æ€§èƒ½ä»¥åŠå¯¹å…¥å°„Xå°„çº¿çš„é€å°„ç‡å’Œé‡å­æ•ˆç‡å‡ç¬¦åˆè¦æ±‚ã€‚å› æ­¤ï¼Œæˆ‘ä»¬æˆåŠŸå®Œæˆäº†ä¸ºXRISMå‡†å¤‡çš„SXIçš„é£è¡Œå‰å¼€å‘ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.08030v1">PDF</a> 14 pages, 11 figures, 3 tables, Accepted for publication in PASJ   XRISM special issue</p>
<p><strong>Summary</strong><br>     æˆåŠŸç ”å‘è½¯Xå°„çº¿æˆåƒä»ªï¼ˆSXIï¼‰ï¼Œæ­è½½äºXå°„çº¿æˆåƒä¸å…‰è°±ä»»åŠ¡ï¼ˆXRISMï¼‰çš„æœ›è¿œé•œXtendä¸Šï¼Œåº”ç”¨äºHitomi Xå°„çº¿å«æ˜Ÿçš„åç»­ä»»åŠ¡ã€‚é‡‡ç”¨ç”±Hitomi&#x2F;SXIå®è·µéªŒè¯çš„èƒŒç…§å¼CCDé˜µåˆ—ï¼Œç»“åˆXå°„çº¿é•œç»„ä»¶ï¼Œè®¾è®¡å…·æœ‰å®½å¹¿è§†é‡å’Œé«˜æ•ˆèƒ½è¡¨ç°çš„XRISM&#x2F;SXIã€‚ç»è¿‡åœ°é¢å®éªŒéªŒè¯ï¼Œå…¶æˆåƒå’Œå…‰è°±èƒ½åŠ›ç¬¦åˆä»»åŠ¡è¦æ±‚ã€‚é¢„é£è¡Œå¼€å‘æˆåŠŸå®Œæˆã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>SXIæ˜¯æ­è½½äºXRISMä»»åŠ¡ä¸­çš„è½¯Xå°„çº¿æˆåƒæœ›è¿œé•œXtendçš„Xå°„çº¿ç”µè·è€¦åˆå™¨ä»¶ï¼ˆCCDï¼‰ç›¸æœºã€‚</li>
<li>SXIçš„è®¾è®¡åŸºäºHitomi&#x2F;SXIï¼Œé‡‡ç”¨èƒŒç…§å¼CCDé˜µåˆ—ï¼Œå…·æœ‰$2 \times 2$çš„CCDé›†é˜µå’Œ200å¾®ç±³çš„è€—å°½å±‚åšåº¦ã€‚</li>
<li>XRISM&#x2F;SXIå…·æœ‰å®½å¹¿çš„è§†é‡èŒƒå›´ï¼Œèƒ½åœ¨$0.4-13$ keVèƒ½é‡èŒƒå›´å†…è¿›è¡Œæˆåƒã€‚</li>
<li>å¯¹Hitomi&#x2F;SXIçš„CCDèŠ¯ç‰‡è®¾è®¡è¿›è¡Œäº†é‡å¤§æ”¹è¿›ï¼Œæé«˜äº†å…‰å­¦å…‰å±è”½èƒ½åŠ›å’Œå®‡å®™å°„çº¿è€å—æ€§ï¼Œå‡å°‘äº†è½¨é“ä¸Šç”µè·è½¬ç§»æ•ˆç‡çš„ä¸‹é™ã€‚</li>
<li>åœ°é¢å®éªŒéªŒè¯äº†SXIçš„æˆåƒå’Œå…‰è°±èƒ½åŠ›ã€å…‰å­¦å±è”½èƒ½åŠ›ã€å†·å´ä¸æ¸©åº¦æ§åˆ¶æ€§èƒ½ä»¥åŠCCDså¯¹å…¥å°„Xå°„çº¿çš„é€å°„æ€§å’Œé‡å­æ•ˆç‡ç­‰ç¬¦åˆä»»åŠ¡è¦æ±‚ã€‚</li>
<li>æˆåŠŸå®Œæˆäº†SXIçš„é¢„é£è¡Œå¼€å‘ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.08030">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-d35a978a0384dbb38768468466fafbec.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d2ffc8617d4b2111ad537d20e0b74a40.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e07011c9b6f2edc3dff6871568584254.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e213be9e26adab4e2ff613438b393f41.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e0e1df34155285bd5757459f4d9bd27c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ef621a5a639297fae32dcaece71eea7e.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Joint-Modelling-Histology-and-Molecular-Markers-for-Cancer-Classification"><a href="#Joint-Modelling-Histology-and-Molecular-Markers-for-Cancer-Classification" class="headerlink" title="Joint Modelling Histology and Molecular Markers for Cancer   Classification"></a>Joint Modelling Histology and Molecular Markers for Cancer   Classification</h2><p><strong>Authors:Xiaofei Wang, Hanyu Liu, Yupei Zhang, Boyang Zhao, Hao Duan, Wanming Hu, Yonggao Mou, Stephen Price, Chao Li</strong></p>
<p>Cancers are characterized by remarkable heterogeneity and diverse prognosis. Accurate cancer classification is essential for patient stratification and clinical decision-making. Although digital pathology has been advancing cancer diagnosis and prognosis, the paradigm in cancer pathology has shifted from purely relying on histology features to incorporating molecular markers. There is an urgent need for digital pathology methods to meet the needs of the new paradigm. We introduce a novel digital pathology approach to jointly predict molecular markers and histology features and model their interactions for cancer classification. Firstly, to mitigate the challenge of cross-magnification information propagation, we propose a multi-scale disentangling module, enabling the extraction of multi-scale features from high-magnification (cellular-level) to low-magnification (tissue-level) whole slide images. Further, based on the multi-scale features, we propose an attention-based hierarchical multi-task multi-instance learning framework to simultaneously predict histology and molecular markers. Moreover, we propose a co-occurrence probability-based label correlation graph network to model the co-occurrence of molecular markers. Lastly, we design a cross-modal interaction module with the dynamic confidence constrain loss and a cross-modal gradient modulation strategy, to model the interactions of histology and molecular markers. Our experiments demonstrate that our method outperforms other state-of-the-art methods in classifying glioma, histology features and molecular markers. Our method promises to promote precise oncology with the potential to advance biomedical research and clinical applications. The code is available at <a target="_blank" rel="noopener" href="https://github.com/LHY1007/M3C2">https://github.com/LHY1007/M3C2</a> </p>
<blockquote>
<p>ç™Œç—‡å…·æœ‰æ˜¾è‘—çš„å¼‚è´¨æ€§å’Œå¤šæ ·çš„é¢„åã€‚å‡†ç¡®çš„ç™Œç—‡åˆ†ç±»å¯¹äºæ‚£è€…åˆ†å±‚å’Œä¸´åºŠå†³ç­–è‡³å…³é‡è¦ã€‚å°½ç®¡æ•°å­—ç—…ç†å­¦å·²ç»æ¨åŠ¨äº†ç™Œç—‡çš„è¯Šæ–­å’Œé¢„åï¼Œä½†ç™Œç—‡ç—…ç†å­¦çš„èŒƒå¼å·²ç»ä»å•çº¯ä¾èµ–ç»„ç»‡ç‰¹å¾è½¬å˜ä¸ºç»“åˆåˆ†å­æ ‡å¿—ç‰©ã€‚å› æ­¤ï¼Œè¿«åˆ‡éœ€è¦æ•°å­—ç—…ç†å­¦æ–¹æ³•æ»¡è¶³æ–°èŒƒå¼çš„éœ€æ±‚ã€‚æˆ‘ä»¬ä»‹ç»äº†ä¸€ç§æ–°å‹æ•°å­—ç—…ç†å­¦æ–¹æ³•ï¼Œå¯è”åˆé¢„æµ‹åˆ†å­æ ‡å¿—ç‰©å’Œç»„ç»‡å­¦ç‰¹å¾ï¼Œå¹¶å»ºæ¨¡å®ƒä»¬ä¹‹é—´çš„ç›¸äº’ä½œç”¨ä»¥è¿›è¡Œç™Œç—‡åˆ†ç±»ã€‚é¦–å…ˆï¼Œä¸ºäº†å‡è½»è·¨æ”¾å¤§å€æ•°ä¿¡æ¯ä¼ æ’­çš„æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§å¤šå°ºåº¦åˆ†è§£æ¨¡å—ï¼Œèƒ½å¤Ÿä»é«˜å€ï¼ˆç»†èƒæ°´å¹³ï¼‰åˆ°ä½å€ï¼ˆç»„ç»‡æ°´å¹³ï¼‰çš„å…¨å¹»ç¯ç‰‡å›¾åƒä¸­æå–å¤šå°ºåº¦ç‰¹å¾ã€‚å…¶æ¬¡ï¼ŒåŸºäºå¤šå°ºåº¦ç‰¹å¾ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªåŸºäºæ³¨æ„åŠ›çš„åˆ†å±‚å¤šä»»åŠ¡å¤šå®ä¾‹å­¦ä¹ æ¡†æ¶ï¼Œå¯ä»¥åŒæ—¶é¢„æµ‹ç»„ç»‡å­¦å’Œåˆ†å­æ ‡å¿—ç‰©ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æå‡ºäº†åŸºäºå…±å‘ç”Ÿæ¦‚ç‡çš„æ ‡ç­¾å…³è”å›¾ç½‘ç»œï¼Œä»¥æ¨¡æ‹Ÿåˆ†å­æ ‡å¿—ç‰©çš„å…±å‘ç”Ÿã€‚æœ€åï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸€ä¸ªè·¨æ¨¡æ€äº¤äº’æ¨¡å—ï¼Œé‡‡ç”¨åŠ¨æ€ç½®ä¿¡çº¦æŸæŸå¤±å’Œè·¨æ¨¡æ€æ¢¯åº¦è°ƒåˆ¶ç­–ç•¥ï¼Œä»¥æ¨¡æ‹Ÿç»„ç»‡å­¦å’Œåˆ†å­æ ‡å¿—ç‰©çš„ç›¸äº’ä½œç”¨ã€‚å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨åˆ†ç±»èƒ¶è´¨ç˜¤ã€ç»„ç»‡å­¦ç‰¹å¾å’Œåˆ†å­æ ‡å¿—ç‰©æ–¹é¢ä¼˜äºå…¶ä»–æœ€å…ˆè¿›çš„æ–¹æ³•ã€‚æˆ‘ä»¬çš„æ–¹æ³•æœ‰æœ›æ¨åŠ¨ç²¾å‡†è‚¿ç˜¤å­¦çš„å‘å±•ï¼Œå…·æœ‰æ¨åŠ¨ç”Ÿç‰©åŒ»å­¦ç ”ç©¶å’Œä¸´åºŠåº”ç”¨çš„æ½œåŠ›ã€‚ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/LHY1007/M3C2%E4%B8%8A%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/LHY1007/M3C2ä¸Šæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.07979v1">PDF</a> accepted by Medical Image Analysis</p>
<p><strong>Summary</strong><br>åœ¨ç™Œç—‡è¯Šæ–­å’Œæ²»ç–—ä¸­ï¼Œæ•°å­—ç—…ç†å­¦æ–¹æ³•å·²ç»å˜å¾—è¶Šæ¥è¶Šé‡è¦ã€‚æœ¬æ–‡ä»‹ç»äº†ä¸€ç§æ–°å‹æ•°å­—ç—…ç†å­¦æ–¹æ³•ï¼Œé€šè¿‡è”åˆé¢„æµ‹åˆ†å­æ ‡è®°å’Œç»„ç»‡å­¦ç‰¹å¾ï¼Œä»¥åŠæ¨¡æ‹Ÿä¸¤è€…é—´çš„äº¤äº’ä½œç”¨æ¥è¿›è¡Œç™Œç—‡åˆ†ç±»ã€‚è¯¥æ–¹æ³•é‡‡ç”¨å¤šå°ºåº¦åˆ†ç¦»æ¨¡å—è§£å†³è·¨å°ºåº¦ä¿¡æ¯ä¼ æ’­çš„æŒ‘æˆ˜ï¼Œå¹¶åˆ©ç”¨åŸºäºæ³¨æ„åŠ›æœºåˆ¶çš„åˆ†å±‚å¤šä»»åŠ¡å¤šå®ä¾‹å­¦ä¹ æ¡†æ¶åŒæ—¶é¢„æµ‹ç»„ç»‡å­¦å’Œåˆ†å­æ ‡è®°ã€‚å®éªŒè¯æ˜è¯¥æ–¹æ³•åœ¨èƒ¶è´¨ç»†èƒç˜¤çš„åˆ†ç±»ä¸Šè¡¨ç°ä¼˜äºå…¶ä»–æœ€å…ˆè¿›çš„æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç™Œç—‡å…·æœ‰æ˜¾è‘—å¼‚è´¨æ€§å’Œå¤šæ ·çš„é¢„åï¼Œå‡†ç¡®çš„ç™Œç—‡åˆ†ç±»å¯¹äºæ‚£è€…åˆ†å±‚å’Œä¸´åºŠå†³ç­–è‡³å…³é‡è¦ã€‚</li>
<li>æ•°å­—ç—…ç†å­¦å·²ç»é€æ¸ä»å•çº¯ä¾èµ–ç»„ç»‡å­¦ç‰¹å¾è½¬å˜ä¸ºç»“åˆåˆ†å­æ ‡è®°ï¼Œä»¥æ›´å¥½åœ°é€‚åº”ç™Œç—‡ç—…ç†å­¦çš„å˜åŒ–ã€‚</li>
<li>ä»‹ç»äº†ä¸€ç§æ–°å‹æ•°å­—ç—…ç†å­¦æ–¹æ³•ï¼Œè”åˆé¢„æµ‹åˆ†å­æ ‡è®°å’Œç»„ç»‡å­¦ç‰¹å¾ï¼Œå¹¶æ¨¡æ‹Ÿå…¶äº¤äº’ä½œç”¨è¿›è¡Œåˆ†ç±»ã€‚</li>
<li>è¯¥æ–¹æ³•é€šè¿‡ä½¿ç”¨å¤šå°ºåº¦åˆ†ç¦»æ¨¡å—å¤„ç†è·¨å°ºåº¦ä¿¡æ¯ä¼ æ’­çš„æŒ‘æˆ˜ã€‚</li>
<li>åˆ©ç”¨åŸºäºæ³¨æ„åŠ›æœºåˆ¶çš„åˆ†å±‚å¤šä»»åŠ¡å¤šå®ä¾‹å­¦ä¹ æ¡†æ¶è¿›è¡Œé¢„æµ‹ã€‚</li>
<li>é€šè¿‡å»ºç«‹åŸºäºå…±ç°æ¦‚ç‡çš„æ ‡ç­¾å…³è”å›¾ç½‘ç»œæ¥æ¨¡æ‹Ÿåˆ†å­æ ‡è®°çš„å…±ç°ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.07979">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-d3d6d13fccbd5f8e74136265174b2a55.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e5a8ca1113b39dc1fe0d6891cb9ed17d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d2a790f724ea663f591691ec81541697.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Markarian-501-reincarnates-with-vigor-as-a-temporary-EHBL-during-VHE-flaring-in-July-2014"><a href="#Markarian-501-reincarnates-with-vigor-as-a-temporary-EHBL-during-VHE-flaring-in-July-2014" class="headerlink" title="Markarian 501 reincarnates with vigor as a temporary EHBL during VHE   flaring in July 2014"></a>Markarian 501 reincarnates with vigor as a temporary EHBL during VHE   flaring in July 2014</h2><p><strong>Authors:Sarira Sahu A. U. Puga Oliveros, D. I. PÃ¡ez-SÃ¡nchez, G. SÃ¡nchez-ColÃ³n, 2 Subhash Rajpoot, M. E. Iglesias MartÃ­nez, JosÃ© Guerra Carmenate, P. FernÃ¡ndez de CÃ³rdoba, Gaetano Lambiase</strong></p>
<p>Markarian 501, a well known high energy BL Lac object, has exhibited several epochs of very high energy (VHE) gamma-ray flaring events when its synchrotron peak frequency shifted above $10^{17}$ Hz, a signature of extreme behavior. From July 16 to July 31, 2014 such flaring events were observed for 15 days by various telescopes. On July 19 (MJD 56857.98), the X-ray outburst from the source was at its highest and on the same day an intriguing narrow peak-like feature around 3 TeV was observed by MAGIC telescopes, a feature inconsistent with standard interpretations. Using the well known two-zone photohadronic model, we study these VHE gamma-ray spectra on a day-by-day basis and offer explanation. Our two-zone photohadronic scenario shows that, on MJD 56857.98, the peak-like feature appears at a cutoff energy of $E^c_{\gamma}$ &#x3D; 3.18 TeV. Below this energy the observed VHE spectrum increases slowly and above $E^c_{\gamma}$ it falls faster, thus resulting in a mild peak-like feature. </p>
<blockquote>
<p>é©¬å…‹é‡Œå®‰501æ˜¯ä¸€é¢—è‘—åçš„é«˜èƒ½BL Lacå¤©ä½“ï¼Œå½“å®ƒçš„åŒæ­¥è¾å°„å³°å€¼é¢‘ç‡ç§»åˆ°$10^{17}$èµ«å…¹ä»¥ä¸Šæ—¶ï¼Œå®ƒè¡¨ç°å‡ºäº†å¤šä¸ªéå¸¸é«˜èƒ½ï¼ˆVHEï¼‰çš„ä¼½é©¬å°„çº¿è€€æ–‘äº‹ä»¶æ—¶æ®µï¼Œè¿™æ˜¯æç«¯è¡Œä¸ºçš„ä¸€ä¸ªæ ‡å¿—ã€‚ä»2014å¹´7æœˆ16æ—¥è‡³7æœˆ31æ—¥ï¼Œè¿™æ ·çš„è€€æ–‘äº‹ä»¶è¢«å„ç§æœ›è¿œé•œæŒç»­è§‚å¯Ÿäº†15å¤©ã€‚åœ¨7æœˆ19æ—¥ï¼ˆMJD 56857.98ï¼‰ï¼Œè¯¥æºçš„Xå°„çº¿çˆ†å‘è¾¾åˆ°æœ€é«˜ï¼ŒåŒä¸€å¤©ï¼ŒMAGICæœ›è¿œé•œè§‚å¯Ÿåˆ°äº†ä¸€ä¸ªæœ‰è¶£çš„åœ¨3TeVé™„è¿‘çš„çª„å³°çŠ¶ç‰¹å¾ï¼Œè¿™ä¸€ç‰¹å¾ä¸æ ‡å‡†è§£é‡Šä¸ç¬¦ã€‚æˆ‘ä»¬ä½¿ç”¨ä¼—æ‰€å‘¨çŸ¥çš„åŒåŒºå…‰å­å¼ºå­æ¨¡å‹ï¼Œé€æ—¥ç ”ç©¶è¿™äº›éå¸¸é«˜èƒ½ä¼½é©¬å°„çº¿å…‰è°±ï¼Œå¹¶ç»™å‡ºè§£é‡Šã€‚æˆ‘ä»¬çš„åŒåŒºå…‰å­å¼ºå­åœºæ™¯æ˜¾ç¤ºï¼Œåœ¨MJD 56857.98ï¼Œå³°å€¼ç‰¹å¾å‡ºç°åœ¨æˆªæ­¢èƒ½é‡$E^c_{\gamma}$&#x3D; 3.18 TeVã€‚åœ¨æ­¤èƒ½é‡ä»¥ä¸‹ï¼Œè§‚å¯Ÿåˆ°çš„VHEå…‰è°±å¢é•¿ç¼“æ…¢ï¼Œè€Œåœ¨$E^c_{\gamma}$ä»¥ä¸Šåˆ™ä¸‹é™å¾—æ›´å¿«ï¼Œä»è€Œäº§ç”Ÿäº†ä¸€ä¸ªæ¸©å’Œçš„å³°å€¼ç‰¹å¾ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.07940v1">PDF</a> 14 pages, 8 figures, 1 table</p>
<p><strong>Summary</strong><br>     é©¬å…‹ç‘å®‰501å·å¤©ä½“åœ¨è§‚æµ‹æœŸé—´å±•ç°äº†æé«˜èƒ½ä¼½é©¬å°„çº¿è€€æ–‘æ´»åŠ¨ï¼Œç‰¹åˆ«æ˜¯å…¶åŒæ­¥åŠ é€Ÿå™¨å³°å€¼é¢‘ç‡é«˜äº$10^{17}$ Hzæ—¶ï¼Œè¿™æš—ç¤ºå…¶æç«¯è¡Œä¸ºçŠ¶æ€ã€‚åŸºäºæ–°çš„æ¨¡å‹â€”â€”åŒåŒºå…‰å­ä¸­å­æ¨¡å‹è¿›è¡Œç ”ç©¶åˆ†æåæ­ç¤ºäº†åœ¨ç‰¹å®šçš„æ—¥æœŸï¼ˆMJD 56857.98ï¼‰ï¼Œè§‚å¯Ÿåˆ°é«˜èƒ½ä¼½é©¬å°„çº¿è°±çš„å³°å€¼ç‰¹å¾ï¼Œå¹¶è§£é‡Šäº†å…¶æˆå› ã€‚è¿™ä¸€ç‰¹å¾è¡¨ç°ä¸ºåœ¨æˆªæ­¢èƒ½é‡$E^c_{\gamma}$ &#x3D; 3.18 TeVå¤„å‡ºç°ä¸€ä¸ªå³°å€¼ï¼Œè¯¥å³°å€¼ä»¥ä¸‹è§‚æµ‹åˆ°çš„VHEè°±å¢é•¿ç¼“æ…¢ï¼Œè€Œä»¥ä¸Šåˆ™è¿…é€Ÿä¸‹é™ï¼Œå½¢æˆè½»å¾®å³°å€¼ç‰¹å¾ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>Markarian 501åœ¨é«˜èƒ½ä¼½é©¬å°„çº¿è€€æ–‘æ´»åŠ¨æœŸé—´è¡¨ç°å‡ºæç«¯è¡Œä¸ºçŠ¶æ€ï¼Œç‰¹åˆ«æ˜¯åœ¨åŒæ­¥åŠ é€Ÿå™¨å³°å€¼é¢‘ç‡é«˜äºç‰¹å®šå€¼çš„æƒ…å†µä¸‹ã€‚</li>
<li>åœ¨ç‰¹å®šæ—¥æœŸï¼ˆMJD 56857.98ï¼‰ï¼Œè§‚å¯Ÿåˆ°é«˜èƒ½ä¼½é©¬å°„çº¿è°±çš„å³°å€¼ç‰¹å¾ã€‚</li>
<li>ä½¿ç”¨åŒåŒºå…‰å­ä¸­å­æ¨¡å‹è¿›è¡Œè§£é‡Šï¼Œæ­ç¤ºå‡ºå³°å€¼ç‰¹å¾åœ¨æˆªæ­¢èƒ½é‡å¤„å½¢æˆã€‚</li>
<li>åœ¨æˆªæ­¢èƒ½é‡ä»¥ä¸‹ï¼Œè§‚æµ‹åˆ°çš„VHEè°±å¢é•¿ç¼“æ…¢ï¼›è€Œè¶…è¿‡è¯¥èƒ½é‡æ—¶ï¼Œåˆ™è¿…é€Ÿä¸‹é™ã€‚è¿™ç§ç‰¹å¾è¡¨ç°ä¸ºè½»å¾®å³°å€¼å½¢æ€ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.07940">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-2f6fa1396253aee364863c4cdab9f65b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a0aaa0e49d839f12729a47463dd76577.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3609befe8e1daecb7c71a99ec5ecbb8c.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="Visual-Haptic-Model-Mediated-Teleoperation-for-Remote-Ultrasound"><a href="#Visual-Haptic-Model-Mediated-Teleoperation-for-Remote-Ultrasound" class="headerlink" title="Visual-Haptic Model Mediated Teleoperation for Remote Ultrasound"></a>Visual-Haptic Model Mediated Teleoperation for Remote Ultrasound</h2><p><strong>Authors:David Black, Maria Tirindelli, Septimiu Salcudean, Wolfgang Wein, Marco Esposito</strong></p>
<p>Tele-ultrasound has the potential greatly to improve health equity for countless remote communities. However, practical scenarios involve potentially large time delays which cause current implementations of telerobotic ultrasound (US) to fail. Using a local model of the remote environment to provide haptics to the expert operator can decrease teleoperation instability, but the delayed visual feedback remains problematic. This paper introduces a robotic tele-US system in which the local model is not only haptic, but also visual, by re-slicing and rendering a pre-acquired US sweep in real time to provide the operator a preview of what the delayed image will resemble. A prototype system is presented and tested with 15 volunteer operators. It is found that visual-haptic model-mediated teleoperation (MMT) compensates completely for time delays up to 1000 ms round trip in terms of operator effort and completion time while conventional MMT does not. Visual-haptic MMT also significantly outperforms MMT for longer time delays in terms of motion accuracy and force control. This proof-of-concept study suggests that visual-haptic MMT may facilitate remote robotic tele-US. </p>
<blockquote>
<p>è¿œç¨‹è¶…å£°å…·æœ‰ä¸ºæ— æ•°åè¿œåœ°åŒºç¤¾åŒºæå‡å¥åº·å…¬å¹³çš„å·¨å¤§æ½œåŠ›ã€‚ç„¶è€Œï¼Œå®é™…åº”ç”¨åœºæ™¯ä¸­æ¶‰åŠçš„æ—¶é—´å»¶è¿Ÿå¯èƒ½å¯¼è‡´å½“å‰è¿œç¨‹æœºå™¨äººè¶…å£°ï¼ˆUSï¼‰å®æ–½å¤±è´¥ã€‚ä½¿ç”¨è¿œç¨‹ç¯å¢ƒçš„æœ¬åœ°æ¨¡å‹ä¸ºä¸“ä¸šæ“ä½œäººå‘˜æä¾›è§¦è§‰å¯ä»¥å‡å°è¿œç¨‹æ“ä½œçš„ç¨³å®šæ€§é—®é¢˜ï¼Œä½†å»¶è¿Ÿçš„è§†è§‰åé¦ˆä»ç„¶æ˜¯ä¸ªéš¾é¢˜ã€‚æœ¬æ–‡ä»‹ç»äº†ä¸€ç§æœºå™¨äººè¿œç¨‹è¶…å£°ç³»ç»Ÿï¼Œè¯¥ç³»ç»Ÿä¸ä»…é€šè¿‡è§¦è§‰æä¾›æœ¬åœ°æ¨¡å‹ï¼Œè¿˜é€šè¿‡è§†è§‰æä¾›æ¨¡å‹ï¼Œé€šè¿‡å®æ—¶é‡æ–°åˆ‡ç‰‡å’Œæ¸²æŸ“é¢„å…ˆè·å–çš„è¶…å£°æ‰«ææ¥ä¸ºæ“ä½œäººå‘˜æä¾›å»¶è¿Ÿå›¾åƒé¢„è§ˆã€‚è¯¥ç³»ç»Ÿçš„ä¸€ä¸ªåŸå‹è¢«å±•ç¤ºç»™å¹¶æµ‹è¯•äº†15åå¿—æ„¿è€…æ“ä½œäººå‘˜ã€‚ç ”ç©¶å‘ç°ï¼Œè§†è§‰è§¦è§‰æ¨¡å‹ä»‹å¯¼çš„è¿œç¨‹æ“ä½œï¼ˆMMTï¼‰åœ¨æ“ä½œè€…åŠªåŠ›å’Œå®Œæˆæ—¶é—´æ–¹é¢å¯ä»¥å®Œå…¨è¡¥å¿é«˜è¾¾1000æ¯«ç§’çš„å¾€è¿”æ—¶é—´å»¶è¿Ÿï¼Œè€Œä¼ ç»Ÿçš„MMTåˆ™ä¸èƒ½ã€‚è§†è§‰è§¦è§‰MMTåœ¨è¿åŠ¨å‡†ç¡®æ€§å’ŒåŠ›é‡æ§åˆ¶æ–¹é¢ä¹Ÿæ˜¾è‘—ä¼˜äºæ›´é•¿æ—¶é—´å»¶è¿Ÿä¸‹çš„MMTã€‚è¿™é¡¹æ¦‚å¿µéªŒè¯ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œè§†è§‰è§¦è§‰MMTå¯èƒ½æœ‰åŠ©äºè¿œç¨‹æœºå™¨äººè¶…å£°æ“ä½œã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.07922v1">PDF</a> Supplementary video: <a target="_blank" rel="noopener" href="https://youtu.be/fDLBah7bPeo">https://youtu.be/fDLBah7bPeo</a> . This work has   been submitted to the IEEE for possible publication</p>
<p><strong>Summary</strong></p>
<p>è¿œç¨‹åŒ»ç–—è¶…å£°ï¼ˆTele-ultrasoundï¼‰å¯¹æ”¹å–„åè¿œåœ°åŒºçš„åŒ»ç–—å…¬å¹³å…·æœ‰å·¨å¤§æ½œåŠ›ã€‚ç„¶è€Œï¼Œå®é™…åº”ç”¨ä¸­å­˜åœ¨æ—¶é—´å»¶è¿Ÿé—®é¢˜ï¼Œå¯¼è‡´å½“å‰è¿œç¨‹é¥æ§è¶…å£°æœºå™¨äººï¼ˆTelerobotic ultrasoundï¼Œç®€ç§°USï¼‰çš„å®æ–½å¤±è´¥ã€‚æœ¬æ–‡å¼•å…¥äº†ä¸€ç§æœºå™¨äººè¿œç¨‹è¶…å£°ç³»ç»Ÿï¼Œè¯¥ç³»ç»Ÿä¸ä»…æä¾›è§¦è§‰åé¦ˆï¼Œè¿˜é€šè¿‡å®æ—¶é‡æ–°åˆ‡ç‰‡å’Œæ¸²æŸ“é¢„å…ˆè·å–çš„è¶…å£°æ‰«ææä¾›è§†è§‰åé¦ˆï¼Œä¸ºæ“ä½œè€…æä¾›äº†ä¸€ä¸ªå»¶è¿Ÿå›¾åƒçš„é¢„è§ˆã€‚å®éªŒè¡¨æ˜ï¼Œè§†è§‰è§¦è§‰æ¨¡å‹ä»‹å¯¼çš„é¥æ§æ“ä½œï¼ˆVisual-haptic Model-Mediated Teleoperationï¼Œç®€ç§°MMTï¼‰èƒ½å¤Ÿåœ¨æ—¶é—´å»¶è¿Ÿè¾¾1000æ¯«ç§’å¾€è¿”çš„æƒ…å†µä¸‹å®Œå…¨è¡¥å¿æ“ä½œè€…çš„åŠªåŠ›å’Œå®Œæˆæ—¶é—´ï¼Œè€Œä¼ ç»Ÿçš„MMTåˆ™æ— æ³•å®ç°ã€‚æ­¤å¤–ï¼Œè§†è§‰è§¦è§‰MMTåœ¨è¿åŠ¨å‡†ç¡®æ€§å’ŒåŠ›é‡æ§åˆ¶æ–¹é¢ä¹Ÿæ˜¾è‘—ä¼˜äºä¼ ç»ŸMMTã€‚è¿™ä¸ºè¿œç¨‹é¥æ§è¶…å£°æœºå™¨äººçš„åº”ç”¨æä¾›äº†æ–°çš„å¯èƒ½æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è¿œç¨‹åŒ»ç–—è¶…å£°æœ‰æœ›æ”¹å–„åè¿œåœ°åŒºçš„åŒ»ç–—å…¬å¹³é—®é¢˜ã€‚</li>
<li>æ—¶é—´å»¶è¿Ÿæ˜¯é¥æ§è¶…å£°æœºå™¨äººå®é™…åº”ç”¨ä¸­çš„ä¸»è¦æŒ‘æˆ˜ä¹‹ä¸€ã€‚</li>
<li>é€šè¿‡è§†è§‰è§¦è§‰æ¨¡å‹ä»‹å¯¼çš„é¥æ§æ“ä½œï¼ˆMMTï¼‰å¯ä»¥æœ‰æ•ˆè¡¥å¿æ—¶é—´å»¶è¿Ÿçš„å½±å“ã€‚</li>
<li>MMTèƒ½åœ¨æ—¶é—´å»¶è¿Ÿè¾¾1000æ¯«ç§’å¾€è¿”çš„æƒ…å†µä¸‹æ”¹å–„æ“ä½œè€…çš„åŠªåŠ›å’Œå®Œæˆæ—¶é—´ã€‚</li>
<li>åœ¨è¿åŠ¨å‡†ç¡®æ€§å’ŒåŠ›é‡æ§åˆ¶æ–¹é¢ï¼Œè§†è§‰è§¦è§‰MMTæ˜¾è‘—ä¼˜äºä¼ ç»ŸMMTã€‚</li>
<li>æœ¬æ–‡æå‡ºçš„æ–°å‹é¥æ§è¶…å£°æœºå™¨äººç³»ç»Ÿç»“åˆäº†è§¦è§‰å’Œè§†è§‰åé¦ˆï¼Œé€šè¿‡å®æ—¶é‡æ–°åˆ‡ç‰‡å’Œæ¸²æŸ“é¢„å…ˆè·å–çš„è¶…å£°æ‰«ææ¥æä¾›å»¶è¿Ÿå›¾åƒçš„é¢„è§ˆã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.07922">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-20313827537ec0b12699027ac04f3a6a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-71648e94eb10c8c70f76493fd6327e5c.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-bfcc364af5aa26f47fbcb14c15928f42.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-1d7dd3530baf3eead35704f1661f31e1.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-0c0876807436eb2e8dcd75101756addf.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-dd07e8a22a5ea2d61fe135df033f793f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9ec85680094f429f3c778ea36e3282b6.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="Curvature-Tuning-Provable-Training-free-Model-Steering-From-a-Single-Parameter"><a href="#Curvature-Tuning-Provable-Training-free-Model-Steering-From-a-Single-Parameter" class="headerlink" title="Curvature Tuning: Provable Training-free Model Steering From a Single   Parameter"></a>Curvature Tuning: Provable Training-free Model Steering From a Single   Parameter</h2><p><strong>Authors:Leyang Hu, Randall Balestriero</strong></p>
<p>The scaling of model size and data size has reshaped the paradigm of AI. As a result, the common protocol to leverage the latest models is to steer them towards a specific downstream task of interest through {\em fine-tuning}. Despite its importance, the main methods for fine-tuning remain limited to full or low-rank adaptersâ€“containing countless hyper-parameters and lacking interpretability. In this paper, we take a step back and demonstrate how novel and explainable post-training steering solutions can be derived theoretically from {\em spline operators}, a rich mathematical framing of Deep Networks that was recently developed. Our methodâ€“coined \textbf{Curvature Tuning (CT)}â€“has a single parameter that provably modulates the curvature of the modelâ€™s decision boundary henceforth allowing training-free steering. This makes CT both more efficient and interpretable than conventional fine-tuning methods. We empirically validate its effectiveness in improving generalization and robustness of pretrained models. For example, CT improves out-of-distribution transfer performances of ResNet-18&#x2F;50 by 2.57%&#x2F;1.74% across seventeen downstream datasets, and improves RobustBench robust accuracy by 11.76%&#x2F;348.44%. Additionally, we apply CT to ReLU-based Swin-T&#x2F;S, improving their generalization on nine downstream datasets by 2.43%&#x2F;3.33%. Our code is available at \href{<a target="_blank" rel="noopener" href="https://github.com/Leon-Leyang/curvature-tuning%7D%7Bhttps://github.com/Leon-Leyang/curvature-tuning%7D">https://github.com/Leon-Leyang/curvature-tuning}{https://github.com/Leon-Leyang/curvature-tuning}</a>. </p>
<blockquote>
<p>æ¨¡å‹è§„æ¨¡å’Œæ•°æ®è§„æ¨¡çš„æ‰©å±•å·²ç»é‡å¡‘äº†äººå·¥æ™ºèƒ½çš„èŒƒå¼ã€‚å› æ­¤ï¼Œåˆ©ç”¨æœ€æ–°æ¨¡å‹çš„é€šç”¨åè®®æ˜¯é€šè¿‡å¾®è°ƒå°†å®ƒä»¬å¯¼å‘ç‰¹å®šçš„ä¸‹æ¸¸ä»»åŠ¡ã€‚å°½ç®¡å¾®è°ƒå¾ˆé‡è¦ï¼Œä½†ä¸»è¦æ–¹æ³•ä»ç„¶å±€é™äºå…¨å‚æ•°æˆ–ä½ç§©é€‚é…å™¨â€”â€”åŒ…å«æ— æ•°è¶…å‚æ•°ä¸”ç¼ºä¹å¯è§£é‡Šæ€§ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬é€€ä¸€æ­¥ï¼Œå±•ç¤ºäº†å¦‚ä½•ä»æ·±åº¦ç½‘ç»œçš„ä¸°å¯Œæ•°å­¦æ¡†æ¶â€œæ ·æ¡ç®—å­â€ä¸­ç†è®ºä¸Šæ¨å¯¼å‡ºæ–°é¢–ä¸”å¯è§£é‡Šçš„åè®­ç»ƒå¾®è°ƒè§£å†³æ–¹æ¡ˆã€‚æˆ‘ä»¬çš„æ–¹æ³•â€”â€”è¢«ç§°ä¸ºâ€œæ›²ç‡è°ƒæ•´ï¼ˆCTï¼‰â€ï¼Œæœ‰ä¸€ä¸ªå‚æ•°å¯ä»¥è°ƒæ•´æ¨¡å‹å†³ç­–è¾¹ç•Œçš„æ›²ç‡ï¼Œä»è€Œå®ç°æ— è®­ç»ƒå¾®è°ƒã€‚è¿™ä½¿å¾—CTæ¯”ä¼ ç»Ÿå¾®è°ƒæ–¹æ³•æ›´é«˜æ•ˆä¸”æ›´å…·å¯è§£é‡Šæ€§ã€‚æˆ‘ä»¬é€šè¿‡å®éªŒéªŒè¯äº†å…¶åœ¨æé«˜é¢„è®­ç»ƒæ¨¡å‹çš„é€šç”¨æ€§å’Œé²æ£’æ€§æ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚ä¾‹å¦‚ï¼ŒCTåœ¨åä¸ƒä¸ªä¸‹æ¸¸æ•°æ®é›†ä¸Šæé«˜äº†ResNet-18&#x2F;50çš„è·¨åˆ†å¸ƒè¿ç§»æ€§èƒ½ï¼Œåˆ†åˆ«æé«˜äº†2.57%&#x2F;1.74%ï¼Œå¹¶åœ¨RobustBenchä¸Šçš„é²æ£’ç²¾åº¦æé«˜äº†11.76%&#x2F;348.44%ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å°†CTåº”ç”¨äºReLUåŸºç¡€çš„Swin-T&#x2F;Sï¼Œåœ¨ä¹ä¸ªä¸‹æ¸¸æ•°æ®é›†ä¸Šæé«˜äº†å®ƒä»¬çš„é€šç”¨æ€§ï¼Œåˆ†åˆ«æé«˜äº†2.43%&#x2F;3.33%ã€‚æˆ‘ä»¬çš„ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/Leon-Leyang/curvature-tuning%E9%93%BE%E6%8E%A5%E4%B8%AD%E8%AE%BF%E9%97%AE%E3%80%82">https://github.com/Leon-Leyang/curvature-tuningé“¾æ¥ä¸­è®¿é—®ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.07783v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æ¨¡å‹è§„æ¨¡å’Œæ•°æ®è§„æ¨¡çš„æ‰©å±•é‡å¡‘äº†äººå·¥æ™ºèƒ½èŒƒå¼ï¼Œå…¶ä¸­çš„å…³é”®æ‰‹æ®µä¹‹ä¸€æ˜¯å¯¹æ¨¡å‹è¿›è¡Œå¾®è°ƒã€‚ç°æœ‰çš„å¾®è°ƒæ–¹æ³•å¤šä¾èµ–å¤§é‡å‚æ•°å¹¶ä¸”ç¼ºä¹å¯è§£é‡Šæ€§ã€‚æœ¬ç ”ç©¶æå‡ºäº†åŸºäºæ›²çº¿æ“ä½œçš„ç†è®ºæ–¹æ³•â€”â€”å¼¯æ›²åº¦å¾®è°ƒï¼ˆCurvature Tuning, CTï¼‰ã€‚CTé€šè¿‡ä¸€ä¸ªå‚æ•°å³å¯è°ƒæ§æ¨¡å‹å†³ç­–è¾¹ç•Œçš„æ›²ç‡ï¼Œä»è€Œå®ç°è®­ç»ƒåˆ†ç¦»è°ƒæ•´ç­–ç•¥ï¼Œæ˜¾è‘—æå‡äº†é¢„è®­ç»ƒæ¨¡å‹çš„æ³›åŒ–å’Œé²æ£’æ€§ã€‚æœ¬ç ”ç©¶çš„ç»“æœè¯æ˜CTè¾ƒä¼ ç»Ÿå¾®è°ƒæ–¹æ³•æ›´å…·æ•ˆèƒ½å’Œå¯è§£é‡Šæ€§ã€‚è¯¥æˆæœå·²åœ¨GitHubä¸Šå…¬å¼€ä»£ç ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>æ¨¡å‹è§„æ¨¡å’Œæ•°æ®è§„æ¨¡çš„æ‰©å±•é‡å¡‘äº†äººå·¥æ™ºèƒ½çš„èŒƒå¼ã€‚</li>
<li>ç°æœ‰å¾®è°ƒæ–¹æ³•å¤šé‡‡ç”¨å¤§é‡å‚æ•°çš„è°ƒæ•´ç­–ç•¥ï¼Œä½†ç¼ºä¹è§£é‡Šæ€§ã€‚</li>
<li>ç ”ç©¶è€…æå‡ºæ–°çš„è®­ç»ƒåå¾®è°ƒæ–¹æ³•â€”â€”å¼¯æ›²åº¦å¾®è°ƒï¼ˆCTï¼‰ï¼Œåˆ©ç”¨æ›²çº¿æ“ä½œç†è®ºæ¥å®ç°æ¨¡å‹è°ƒæ•´ã€‚</li>
<li>CTé€šè¿‡ä¸€ä¸ªå‚æ•°è°ƒæ•´æ¨¡å‹å†³ç­–è¾¹ç•Œçš„æ›²ç‡ï¼Œå®ç°äº†è®­ç»ƒåˆ†ç¦»çš„è°ƒæ•´ç­–ç•¥ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.07783">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-ad08b416bd3acd8f6155b9afd84c9103.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-55597e9dee0a9b4186568fadaf4d4dc3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5c9f992411ac9ed0d279429d9fed7f7c.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="Quantitative-evaluation-of-unsupervised-clustering-algorithms-for-dynamic-total-body-PET-image-analysis"><a href="#Quantitative-evaluation-of-unsupervised-clustering-algorithms-for-dynamic-total-body-PET-image-analysis" class="headerlink" title="Quantitative evaluation of unsupervised clustering algorithms for   dynamic total-body PET image analysis"></a>Quantitative evaluation of unsupervised clustering algorithms for   dynamic total-body PET image analysis</h2><p><strong>Authors:Oona Rainio, Maria K. Jaakkola, Riku KlÃ©n</strong></p>
<p>Background. Recently, dynamic total-body positron emission tomography (PET) imaging has become possible due to new scanner devices. While clustering algorithms have been proposed for PET analysis already earlier, there is still little research systematically evaluating these algorithms for processing of dynamic total-body PET images. Materials and methods. Here, we compare the performance of 15 unsupervised clustering methods, including K-means either by itself or after principal component analysis (PCA) or independent component analysis (ICA), Gaussian mixture model (GMM), fuzzy c-means (FCM), agglomerative clustering, spectral clustering, and several newer clustering algorithms, for classifying time activity curves (TACs) in dynamic PET images. We use dynamic total-body $^{15}$O-water PET images collected from 30 patients with suspected or confirmed coronary artery disease. To evaluate the clustering algorithms in a quantitative way, we use them to classify 5000 TACs from each image based on whether the curve is taken from brain, right heart ventricle, right kidney, lower right lung lobe, or urinary bladder. Results. According to our results, the best methods are GMM, FCM, and ICA combined with mini batch K-means, which classified the TACs with a median accuracies of 89%, 83%, and 81%, respectively, in a processing time of half a second or less on average for each image. Conclusion. GMM, FCM, and ICA with mini batch K-means show promise for dynamic total-body PET analysis. </p>
<blockquote>
<p><strong>èƒŒæ™¯</strong>ã€‚æœ€è¿‘ï¼Œç”±äºæ–°çš„æ‰«æè®¾å¤‡ï¼ŒåŠ¨æ€å…¨èº«æ­£ç”µå­å‘å°„æ–­å±‚æ‰«æï¼ˆPETï¼‰æˆåƒå·²ç»å˜å¾—å¯èƒ½ã€‚è™½ç„¶èšç±»ç®—æ³•åœ¨è¾ƒæ—©çš„æ—¶å€™å°±å·²ç»è¢«æå‡ºç”¨äºPETåˆ†æï¼Œä½†å¯¹äºå¤„ç†åŠ¨æ€å…¨èº«PETå›¾åƒï¼Œä»å°‘æœ‰ç ”ç©¶ç³»ç»Ÿåœ°è¯„ä¼°è¿™äº›ç®—æ³•ã€‚<strong>ææ–™å’Œæ–¹æ³•</strong>ã€‚åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬å¯¹15ç§æ— ç›‘ç£èšç±»æ–¹æ³•è¿›è¡Œäº†æ¯”è¾ƒï¼ŒåŒ…æ‹¬K-meansï¼ˆå•ç‹¬æˆ–ä¸ä¸»æˆåˆ†åˆ†æï¼ˆPCAï¼‰æˆ–ç‹¬ç«‹æˆåˆ†åˆ†æï¼ˆICAï¼‰ç»“åˆä½¿ç”¨ï¼‰ã€é«˜æ–¯æ··åˆæ¨¡å‹ï¼ˆGMMï¼‰ã€æ¨¡ç³Šc-å‡å€¼ï¼ˆFCMï¼‰ã€å‡èšèšç±»ã€è°±èšç±»å’Œå‡ ç§æ–°çš„èšç±»ç®—æ³•ï¼Œç”¨äºå¯¹åŠ¨æ€PETå›¾åƒä¸­çš„æ—¶é—´æ´»åŠ¨æ›²çº¿ï¼ˆTACsï¼‰è¿›è¡Œåˆ†ç±»ã€‚æˆ‘ä»¬ä½¿ç”¨ä»ç–‘ä¼¼æˆ–ç¡®è¯Šå† çŠ¶åŠ¨è„‰ç–¾ç—…çš„30ä¾‹æ‚£è€…æ”¶é›†çš„åŠ¨æ€å…¨èº«$^{15}$O-æ°´PETå›¾åƒã€‚ä¸ºäº†å®šé‡è¯„ä¼°èšç±»ç®—æ³•ï¼Œæˆ‘ä»¬ä½¿ç”¨å®ƒä»¬æ ¹æ®æ›²çº¿æ˜¯å¦æ¥è‡ªå¤§è„‘ã€å³å¿ƒå®¤ã€å³è‚¾ã€å³ä¸‹è‚ºå¶æˆ–è†€èƒ±æ¥åˆ†ç±»æ¯ä¸ªå›¾åƒçš„5000æ¡TACsã€‚<strong>ç»“æœ</strong>ã€‚æ ¹æ®æˆ‘ä»¬çš„ç»“æœï¼Œæœ€ä½³æ–¹æ³•æ˜¯GMMã€FCMå’Œä¸mini batch K-meansç»“åˆçš„ICAï¼Œå®ƒä»¬åˆ†ç±»TACsçš„ä¸­å€¼å‡†ç¡®åº¦åˆ†åˆ«ä¸º89%ã€83%å’Œ81%ï¼Œæ¯å¼ å›¾åƒçš„å¹³å‡å¤„ç†æ—¶é—´ä¸è¶…è¿‡åŠç§’ã€‚<strong>ç»“è®º</strong>ã€‚å¯¹äºåŠ¨æ€å…¨èº«PETåˆ†æï¼ŒGMMã€FCMä»¥åŠä¸mini batch K-meansç»“åˆçš„ICAæ˜¾ç¤ºå‡ºè‰¯å¥½çš„å‰æ™¯ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.07511v1">PDF</a> 12 pages, 2 figures</p>
<p><strong>Summary</strong></p>
<p>åŠ¨æ€å…¨èº«æ­£ç”µå­å‘å°„æ–­å±‚æ‰«æï¼ˆPETï¼‰æˆåƒæŠ€æœ¯çš„æœ€æ–°å‘å±•å¯¹èšç±»ç®—æ³•æå‡ºäº†æ–°çš„æŒ‘æˆ˜ä¸æœºé‡ã€‚æœ¬æ–‡å¯¹æ¯”äº†15ç§æ— ç›‘ç£èšç±»ç®—æ³•åœ¨å¤„ç†åŠ¨æ€å…¨èº«PETå›¾åƒä¸­çš„è¡¨ç°ã€‚ç ”ç©¶å‘ç°ï¼Œé«˜æ–¯æ··åˆæ¨¡å‹ï¼ˆGMMï¼‰ã€æ¨¡ç³ŠC-å‡å€¼ï¼ˆFCMï¼‰å’Œç‹¬ç«‹æˆåˆ†åˆ†æï¼ˆICAï¼‰ç»“åˆå°æ‰¹é‡K-å‡å€¼çš„æ–¹æ³•è¡¨ç°æœ€ä½³ï¼Œå¯¹æ—¶é—´æ´»åŠ¨æ›²çº¿ï¼ˆTACsï¼‰çš„åˆ†ç±»å‡†ç¡®ç‡ä¸­ä½æ•°åˆ†åˆ«ä¸º89%ã€83%å’Œ81%ï¼Œä¸”å¤„ç†æ—¶é—´å¹³å‡æ¯ç§’ä¸åˆ°åŠç§’ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>åŠ¨æ€å…¨èº«PETæˆåƒæŠ€æœ¯å¯¹èšç±»ç®—æ³•æå‡ºäº†æ–°çš„æŒ‘æˆ˜ã€‚</li>
<li>å¯¹æ¯”äº†å¤šç§æ— ç›‘ç£èšç±»ç®—æ³•åœ¨å¤„ç†åŠ¨æ€PETå›¾åƒä¸­çš„æ€§èƒ½ã€‚</li>
<li>é«˜æ–¯æ··åˆæ¨¡å‹ï¼ˆGMMï¼‰ã€æ¨¡ç³ŠC-å‡å€¼ï¼ˆFCMï¼‰å’Œç‹¬ç«‹æˆåˆ†åˆ†æï¼ˆICAï¼‰ç»“åˆå°æ‰¹é‡K-å‡å€¼çš„æ–¹æ³•åœ¨åˆ†ç±»TACsæ–¹é¢è¡¨ç°æœ€ä½³ã€‚</li>
<li>è¿™äº›æœ€ä½³æ–¹æ³•çš„åˆ†ç±»å‡†ç¡®ç‡ä¸­ä½æ•°è¶…è¿‡80%ï¼Œå¤„ç†æ—¶é—´è¿…é€Ÿã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.07511">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-9814fcbe9f346d2b65b774a4b4ac34f2.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ded55bf92e4c70805b68e008f8613c46.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-5bee5810932bea92e7f584d04694dd6b.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="Multi-Task-oriented-Nighttime-Haze-Imaging-Enhancer-for-Vision-driven-Measurement-Systems"><a href="#Multi-Task-oriented-Nighttime-Haze-Imaging-Enhancer-for-Vision-driven-Measurement-Systems" class="headerlink" title="Multi-Task-oriented Nighttime Haze Imaging Enhancer for Vision-driven   Measurement Systems"></a>Multi-Task-oriented Nighttime Haze Imaging Enhancer for Vision-driven   Measurement Systems</h2><p><strong>Authors:Ai Chen, Yuxu Lu, Dong Yang, Junlin Zhou, Yan Fu, Duanbing Chen</strong></p>
<p>Salient object detection (SOD) plays a critical role in vision-driven measurement systems (VMS), facilitating the detection and segmentation of key visual elements in an image. However, adverse imaging conditions such as haze during the day, low light, and haze at night severely degrade image quality, and complicating the SOD process. To address these challenges, we propose a multi-task-oriented nighttime haze imaging enhancer (MToIE), which integrates three tasks: daytime dehazing, low-light enhancement, and nighttime dehazing. The MToIE incorporates two key innovative components: First, the network employs a task-oriented node learning mechanism to handle three specific degradation types: day-time haze, low light, and night-time haze conditions, with an embedded self-attention module enhancing its performance in nighttime imaging. In addition, multi-receptive field enhancement module that efficiently extracts multi-scale features through three parallel depthwise separable convolution branches with different dilation rates, capturing comprehensive spatial information with minimal computational overhead. To ensure optimal image reconstruction quality and visual characteristics, we suggest a hybrid loss function. Extensive experiments on different types of weather&#x2F;imaging conditions illustrate that MToIE surpasses existing methods, significantly enhancing the accuracy and reliability of vision systems across diverse imaging scenarios. The code is available at <a target="_blank" rel="noopener" href="https://github.com/Ai-Chen-Lab/MToIE">https://github.com/Ai-Chen-Lab/MToIE</a>. </p>
<blockquote>
<p>æ˜¾è‘—æ€§ç›®æ ‡æ£€æµ‹ï¼ˆSODï¼‰åœ¨è§†è§‰é©±åŠ¨æµ‹é‡ç³»ç»Ÿï¼ˆVMSï¼‰ä¸­å‘æŒ¥ç€å…³é”®ä½œç”¨ï¼Œå¯ä»¥ä¿ƒè¿›å›¾åƒä¸­å…³é”®è§†è§‰å…ƒç´ çš„æ£€æµ‹å’Œåˆ†å‰²ã€‚ç„¶è€Œï¼Œä¸åˆ©çš„æˆåƒæ¡ä»¶ï¼Œå¦‚ç™½å¤©çš„é›¾éœ¾ã€ä½å…‰ç…§å’Œå¤œé—´é›¾éœ¾ï¼Œä¸¥é‡é™ä½äº†å›¾åƒè´¨é‡ï¼Œå¹¶ä½¿å¾—SODè¿‡ç¨‹å¤æ‚åŒ–ã€‚ä¸ºäº†åº”å¯¹è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§å¤šä»»åŠ¡å¯¼å‘çš„å¤œé—´é›¾éœ¾æˆåƒå¢å¼ºå™¨ï¼ˆMToIEï¼‰ï¼Œå®ƒç»“åˆäº†ä¸‰é¡¹ä»»åŠ¡ï¼šç™½å¤©å»é›¾ã€ä½å…‰å¢å¼ºå’Œå¤œé—´å»é›¾ã€‚MToIEåŒ…å«ä¸¤ä¸ªå…³é”®çš„åˆ›æ–°ç»„ä»¶ï¼šé¦–å…ˆï¼Œç½‘ç»œé‡‡ç”¨ä»»åŠ¡å¯¼å‘çš„èŠ‚ç‚¹å­¦ä¹ æœºåˆ¶æ¥å¤„ç†ä¸‰ç§ç‰¹å®šçš„é€€åŒ–ç±»å‹ï¼šç™½å¤©çš„é›¾éœ¾ã€ä½å…‰å’Œå¤œé—´é›¾éœ¾æ¡ä»¶ï¼Œå…¶ä¸­åµŒå…¥çš„è‡ªæ³¨æ„æ¨¡å—å¢å¼ºäº†å…¶åœ¨å¤œé—´æˆåƒçš„æ€§èƒ½ã€‚æ­¤å¤–ï¼Œå¤šæ„Ÿå—é‡å¢å¼ºæ¨¡å—é€šè¿‡ä¸‰ä¸ªå¹¶è¡Œæ·±åº¦å¯åˆ†ç¦»å·ç§¯åˆ†æ”¯é«˜æ•ˆåœ°æå–å¤šå°ºåº¦ç‰¹å¾ï¼Œè¿™äº›åˆ†æ”¯å…·æœ‰ä¸åŒçš„è†¨èƒ€ç‡ï¼Œä»¥æœ€å°çš„è®¡ç®—å¼€é”€æ•è·å…¨é¢çš„ç©ºé—´ä¿¡æ¯ã€‚ä¸ºäº†ç¡®ä¿æœ€ä½³çš„å›¾åƒé‡å»ºè´¨é‡å’Œè§†è§‰ç‰¹æ€§ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ··åˆæŸå¤±å‡½æ•°ã€‚åœ¨ä¸åŒå¤©æ°”&#x2F;æˆåƒæ¡ä»¶ä¸‹çš„å¤§é‡å®éªŒè¡¨æ˜ï¼ŒMToIEè¶…è¶Šäº†ç°æœ‰æ–¹æ³•ï¼Œæ˜¾è‘—æé«˜äº†ä¸åŒæˆåƒåœºæ™¯ä¸­è§†è§‰ç³»ç»Ÿçš„å‡†ç¡®æ€§å’Œå¯é æ€§ã€‚ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/Ai-Chen-Lab/MToIE%E4%B8%8A%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/Ai-Chen-Lab/MToIEä¸Šæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.07351v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§å¤šä»»åŠ¡å¯¼å‘çš„å¤œé—´é›¾éœ¾æˆåƒå¢å¼ºå™¨ï¼ˆMToIEï¼‰ï¼Œé›†æˆäº†æ—¥é—´å»é›¾ã€ä½å…‰å¢å¼ºå’Œå¤œé—´å»é›¾ä¸‰ä¸ªä»»åŠ¡ã€‚è¯¥å¢å¼ºå™¨é‡‡ç”¨ä»»åŠ¡å¯¼å‘èŠ‚ç‚¹å­¦ä¹ æœºåˆ¶å’Œå¸¦æœ‰è‡ªæ³¨æ„åŠ›æ¨¡å—çš„ç½‘ç»œï¼Œæé«˜äº†åœ¨å¤œé—´æˆåƒä¸­çš„æ€§èƒ½ã€‚åŒæ—¶ï¼Œå¼•å…¥äº†ä¸€ç§é«˜æ•ˆçš„å¤šå°ºåº¦ç‰¹å¾æå–æ¨¡å—ï¼Œé€šè¿‡ä¸‰ä¸ªå…·æœ‰ä¸åŒè†¨èƒ€ç‡çš„å¹¶è¡Œæ·±åº¦å¯åˆ†ç¦»å·ç§¯åˆ†æ”¯æ•è·å…¨é¢çš„ç©ºé—´ä¿¡æ¯ã€‚æ­¤å¤–ï¼Œä¸ºäº†ç¡®ä¿å›¾åƒé‡å»ºè´¨é‡å’Œè§†è§‰ç‰¹æ€§è¾¾åˆ°æœ€ä½³ï¼Œå»ºè®®ä½¿ç”¨æ··åˆæŸå¤±å‡½æ•°ã€‚åœ¨å¤šç§å¤©æ°”å’Œæˆåƒæ¡ä»¶ä¸‹çš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼ŒMToIEè¶…è¶Šäº†ç°æœ‰æ–¹æ³•ï¼Œæ˜¾è‘—æé«˜äº†è§†è§‰ç³»ç»Ÿåœ¨ä¸åŒæˆåƒåœºæ™¯ä¸­çš„å‡†ç¡®æ€§å’Œå¯é æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>MToIEé’ˆå¯¹ä¸‰ç§ä¸åŒçš„æˆåƒæ¡ä»¶ï¼ˆæ—¥é—´é›¾éœ¾ã€ä½å…‰å’Œå¤œé—´é›¾éœ¾ï¼‰è¿›è¡Œä»»åŠ¡å¯¼å‘çš„è®¾è®¡ã€‚</li>
<li>é‡‡ç”¨ä»»åŠ¡å¯¼å‘èŠ‚ç‚¹å­¦ä¹ æœºåˆ¶å’Œè‡ªæ³¨æ„åŠ›æ¨¡å—æé«˜å¤œé—´æˆåƒæ€§èƒ½ã€‚</li>
<li>å¤šå°ºåº¦ç‰¹å¾æå–æ¨¡å—é€šè¿‡å¹¶è¡Œæ·±åº¦å¯åˆ†ç¦»å·ç§¯åˆ†æ”¯æœ‰æ•ˆæ•è·å…¨é¢çš„ç©ºé—´ä¿¡æ¯ã€‚</li>
<li>æ··åˆæŸå¤±å‡½æ•°ç¡®ä¿å›¾åƒé‡å»ºè´¨é‡å’Œè§†è§‰ç‰¹æ€§çš„ä¼˜åŒ–ã€‚</li>
<li>MToIEåœ¨å¤šç§å¤©æ°”å’Œæˆåƒæ¡ä»¶ä¸‹çš„è¡¨ç°è¶…è¶Šäº†ç°æœ‰æ–¹æ³•ã€‚</li>
<li>MToIEæ˜¾è‘—æé«˜è§†è§‰ç³»ç»Ÿåœ¨ä¸åŒæˆåƒåœºæ™¯ä¸­çš„å‡†ç¡®æ€§å’Œå¯é æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.07351">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-3a8582974b280c3a559a15c13003d61b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-33807507bfead39f46c229a3cdb755ba.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b95c38c574a1e04aacf66acfa29bcd24.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-867bc86f555a71d53e22662406142d0d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5f54fcb2fe71e1a597f7afbfdf2c693f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f9c32f17028afe5cccb41ba22fa03de7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4b51faff93178ba05c14f2a0f89d319b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-bb91b8718623023a0ca305a52f708895.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="Color-Quality-Invariance-for-Robust-Medical-Image-Segmentation"><a href="#Color-Quality-Invariance-for-Robust-Medical-Image-Segmentation" class="headerlink" title="Color-Quality Invariance for Robust Medical Image Segmentation"></a>Color-Quality Invariance for Robust Medical Image Segmentation</h2><p><strong>Authors:Ravi Shah, Atsushi Fukuda, Quan Huu Cap</strong></p>
<p>Single-source domain generalization (SDG) in medical image segmentation remains a significant challenge, particularly for images with varying color distributions and qualities. Previous approaches often struggle when models trained on high-quality images fail to generalize to low-quality test images due to these color and quality shifts. In this work, we propose two novel techniques to enhance generalization: dynamic color image normalization (DCIN) module and color-quality generalization (CQG) loss. The DCIN dynamically normalizes the color of test images using two reference image selection strategies. Specifically, the DCIN utilizes a global reference image selection (GRIS), which finds a universal reference image, and a local reference image selection (LRIS), which selects a semantically similar reference image per test sample. Additionally, CQG loss enforces invariance to color and quality variations by ensuring consistent segmentation predictions across transformed image pairs. Experimental results show that our proposals significantly improve segmentation performance over the baseline on two target domain datasets, despite being trained solely on a single source domain. Notably, our model achieved up to a 32.3-point increase in Dice score compared to the baseline, consistently producing robust and usable results even under substantial domain shifts. Our work contributes to the development of more robust medical image segmentation models that generalize across unseen domains. The implementation code is available at <a target="_blank" rel="noopener" href="https://github.com/RaviShah1/DCIN-CQG">https://github.com/RaviShah1/DCIN-CQG</a>. </p>
<blockquote>
<p>åŒ»å­¦å›¾åƒåˆ†å‰²ä¸­çš„å•æºåŸŸæ³›åŒ–ï¼ˆSDGï¼‰ä»ç„¶æ˜¯ä¸€ä¸ªé‡å¤§æŒ‘æˆ˜ï¼Œç‰¹åˆ«æ˜¯å¯¹äºé¢œè‰²åˆ†å¸ƒå’Œè´¨é‡å„å¼‚çš„å›¾åƒã€‚ä»¥å‰çš„æ–¹æ³•å¸¸å¸¸åœ¨è®­ç»ƒäºé«˜è´¨é‡å›¾åƒä¸Šçš„æ¨¡å‹æ— æ³•æ³›åŒ–åˆ°ä½è´¨é‡æµ‹è¯•å›¾åƒæ—¶é‡åˆ°å›°æ‰°ï¼Œè¿™æ˜¯ç”±äºé¢œè‰²å’Œè´¨é‡çš„åç§»å¯¼è‡´çš„ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸¤ç§å¢å¼ºæ³›åŒ–çš„æ–°æŠ€æœ¯ï¼šåŠ¨æ€å½©è‰²å›¾åƒå½’ä¸€åŒ–ï¼ˆDCINï¼‰æ¨¡å—å’Œé¢œè‰²è´¨é‡æ³›åŒ–ï¼ˆCQGï¼‰æŸå¤±ã€‚DCINé€šè¿‡ä¸¤ç§å‚è€ƒå›¾åƒé€‰æ‹©ç­–ç•¥åŠ¨æ€åœ°å½’ä¸€åŒ–æµ‹è¯•å›¾åƒçš„é¢œè‰²ã€‚å…·ä½“æ¥è¯´ï¼ŒDCINåˆ©ç”¨å…¨å±€å‚è€ƒå›¾åƒé€‰æ‹©ï¼ˆGRISï¼‰ï¼Œæ‰¾åˆ°é€šç”¨å‚è€ƒå›¾åƒï¼›å±€éƒ¨å‚è€ƒå›¾åƒé€‰æ‹©ï¼ˆLRISï¼‰åˆ™ä¸ºæ¯ä¸ªæµ‹è¯•æ ·æœ¬é€‰æ‹©è¯­ä¹‰ä¸Šç›¸ä¼¼çš„å‚è€ƒå›¾åƒã€‚æ­¤å¤–ï¼ŒCQGæŸå¤±é€šè¿‡ç¡®ä¿å˜æ¢å›¾åƒå¯¹ä¹‹é—´çš„åˆ†å‰²é¢„æµ‹ä¸€è‡´ï¼Œæ¥å¼ºåˆ¶æ‰§è¡Œé¢œè‰²å’Œè´¨é‡å˜åŒ–çš„ä¸å˜æ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œå°½ç®¡åªåœ¨ä¸€ä¸ªæºåŸŸä¸Šè¿›è¡Œè®­ç»ƒï¼Œä½†æˆ‘ä»¬çš„æ–¹æ³•åœ¨ä¸¤ä¸ªç›®æ ‡åŸŸæ•°æ®é›†ä¸Šçš„åˆ†å‰²æ€§èƒ½éƒ½æœ‰äº†æ˜¾è‘—æé«˜ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œæˆ‘ä»¬çš„æ¨¡å‹åœ¨è¿ªå…‹åˆ†æ•°ä¸Šæœ€é«˜æé«˜äº†32.3åˆ†ï¼Œå³ä½¿åœ¨å¾ˆå¤§çš„åŸŸåç§»ä¸‹ä¹Ÿå§‹ç»ˆäº§ç”Ÿç¨³å¥å’Œå¯ç”¨çš„ç»“æœã€‚æˆ‘ä»¬çš„å·¥ä½œä¸ºå¼€å‘æ›´ç¨³å¥çš„åŒ»å­¦å›¾åƒåˆ†å‰²æ¨¡å‹åšå‡ºäº†è´¡çŒ®ï¼Œè¿™äº›æ¨¡å‹å¯ä»¥åœ¨æœªè§è¿‡çš„é¢†åŸŸè¿›è¡Œæ³›åŒ–ã€‚å®ç°ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/RaviShah1/DCIN-CQG%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/RaviShah1/DCIN-CQGæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.07200v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºä¸¤ç§æŠ€æœ¯æ¥è§£å†³åŒ»å­¦å›¾åƒåˆ†å‰²ä¸­çš„å•æºåŸŸæ³›åŒ–é—®é¢˜ï¼Œå³åŠ¨æ€è‰²å½©å›¾åƒå½’ä¸€åŒ–æ¨¡å—ï¼ˆDCINï¼‰å’Œè‰²å½©è´¨é‡æ³›åŒ–ï¼ˆCQGï¼‰æŸå¤±ã€‚DCINé€šè¿‡ä¸¤ç§å‚è€ƒå›¾åƒé€‰æ‹©ç­–ç•¥åŠ¨æ€å½’ä¸€åŒ–æµ‹è¯•å›¾åƒçš„é¢œè‰²ï¼Œè€ŒCQGæŸå¤±é€šè¿‡ç¡®ä¿å¯¹å˜æ¢å›¾åƒå¯¹çš„åˆ†å‰²é¢„æµ‹çš„ä¸€è‡´æ€§æ¥å¼ºåˆ¶æ‰§è¡Œé¢œè‰²å’Œè´¨é‡çš„å˜å¼‚ä¸å˜æ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œä¸åŸºçº¿ç›¸æ¯”ï¼Œè¯¥æ–¹æ³•åœ¨ç›®æ ‡åŸŸæ•°æ®é›†ä¸Šçš„åˆ†å‰²æ€§èƒ½æ˜¾è‘—æé«˜ï¼Œå¹¶ä¸”åœ¨å•ä¸€æºåŸŸè®­ç»ƒçš„æƒ…å†µä¸‹ä¹Ÿèƒ½å®ç°è‰¯å¥½çš„æ³›åŒ–æ•ˆæœã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>åŒ»å­¦å›¾åƒåˆ†å‰²ä¸­çš„å•æºåŸŸæ³›åŒ–ï¼ˆSDGï¼‰æ˜¯ä¸€ä¸ªæŒ‘æˆ˜ï¼Œç‰¹åˆ«æ˜¯åœ¨é¢œè‰²åˆ†å¸ƒå’Œè´¨é‡å˜åŒ–å¤§çš„å›¾åƒä¸­ã€‚</li>
<li>æå‡ºçš„DCINæ¨¡å—é€šè¿‡ä¸¤ç§å‚è€ƒå›¾åƒé€‰æ‹©ç­–ç•¥åŠ¨æ€å½’ä¸€åŒ–æµ‹è¯•å›¾åƒé¢œè‰²ã€‚</li>
<li>CQGæŸå¤±é€šè¿‡ç¡®ä¿å¯¹å˜æ¢å›¾åƒå¯¹åˆ†å‰²é¢„æµ‹çš„ä¸€è‡´æ€§ï¼Œå®ç°å¯¹é¢œè‰²å’Œè´¨é‡çš„å˜å¼‚ä¸å˜æ€§ã€‚</li>
<li>å®éªŒç»“æœæ˜¾ç¤ºï¼Œä¸åŸºçº¿ç›¸æ¯”ï¼Œè¯¥æ–¹æ³•åœ¨ç›®æ ‡åŸŸæ•°æ®é›†ä¸Šçš„åˆ†å‰²æ€§èƒ½æ˜¾è‘—æé«˜ï¼ŒDiceå¾—åˆ†æé«˜è¾¾32.3ä¸ªç‚¹ã€‚</li>
<li>è¯¥æ¨¡å‹åœ¨æ˜¾è‘—åŸŸåç§»ä¸‹ä»èƒ½äº§ç”Ÿç¨³å¥å’Œå¯ç”¨çš„ç»“æœã€‚</li>
<li>è¯¥å·¥ä½œçš„è´¡çŒ®åœ¨äºå¼€å‘å‡ºæ›´ç¨³å¥çš„åŒ»å­¦å›¾åƒåˆ†å‰²æ¨¡å‹ï¼Œèƒ½å¤Ÿåœ¨æœªè§è¿‡çš„é¢†åŸŸè¿›è¡Œæ³›åŒ–ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.07200">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-fe88677ef992b6455489521ac15adebd.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c837548f986d9b7c6c110ae5f135543b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3104949abfa337221e3091c81e658397.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-636840586d721d9228fb4cb833e0ee48.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8ef1e86265a72139c3dc8ddbb05754eb.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b2c00bc084359a9ff94b6cd35578473a.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="HDCompression-Hybrid-Diffusion-Image-Compression-for-Ultra-Low-Bitrates"><a href="#HDCompression-Hybrid-Diffusion-Image-Compression-for-Ultra-Low-Bitrates" class="headerlink" title="HDCompression: Hybrid-Diffusion Image Compression for Ultra-Low Bitrates"></a>HDCompression: Hybrid-Diffusion Image Compression for Ultra-Low Bitrates</h2><p><strong>Authors:Lei Lu, Yize Li, Yanzhi Wang, Wei Wang, Wei Jiang</strong></p>
<p>Image compression under ultra-low bitrates remains challenging for both conventional learned image compression (LIC) and generative vector-quantized (VQ) modeling. Conventional LIC suffers from severe artifacts due to heavy quantization, while generative VQ modeling gives poor fidelity due to the mismatch between learned generative priors and specific inputs. In this work, we propose Hybrid-Diffusion Image Compression (HDCompression), a dual-stream framework that utilizes both generative VQ-modeling and diffusion models, as well as conventional LIC, to achieve both high fidelity and high perceptual quality. Different from previous hybrid methods that directly use pre-trained LIC models to generate low-quality fidelity-preserving information from heavily quantized latent, we use diffusion models to extract high-quality complimentary fidelity information from the ground-truth input, which can enhance the system performance in several aspects: improving indices map prediction, enhancing the fidelity-preserving output of the LIC stream, and refining conditioned image reconstruction with VQ-latent correction. In addition, our diffusion model is based on a dense representative vector (DRV), which is lightweight with very simple sampling schedulers. Extensive experiments demonstrate that our HDCompression outperforms the previous conventional LIC, generative VQ-modeling, and hybrid frameworks in both quantitative metrics and qualitative visualization, providing balanced robust compression performance at ultra-low bitrates. </p>
<blockquote>
<p>åœ¨è¶…ä½æ¯”ç‰¹ç‡ä¸‹ï¼Œå›¾åƒå‹ç¼©å¯¹äºä¼ ç»Ÿçš„å›¾åƒå‹ç¼©ï¼ˆLICï¼‰å’Œç”Ÿæˆå‘é‡é‡åŒ–ï¼ˆVQï¼‰å»ºæ¨¡ä»ç„¶å…·æœ‰æŒ‘æˆ˜æ€§ã€‚ä¼ ç»ŸLICç”±äºé‡é‡åŒ–è€Œå®¹æ˜“å‡ºç°ä¸¥é‡ä¼ªå½±ï¼Œè€Œç”ŸæˆVQå»ºæ¨¡ç”±äºå­¦ä¹ åˆ°çš„ç”Ÿæˆå…ˆéªŒä¸ç‰¹å®šè¾“å…¥ä¹‹é—´çš„ä¸åŒ¹é…è€Œå¯¼è‡´ä¿çœŸåº¦è¾ƒä½ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†æ··åˆæ‰©æ•£å›¾åƒå‹ç¼©ï¼ˆHDCompressionï¼‰ï¼Œè¿™æ˜¯ä¸€ç§åŒæµæ¡†æ¶ï¼Œç»“åˆäº†ç”ŸæˆVQå»ºæ¨¡å’Œæ‰©æ•£æ¨¡å‹ä»¥åŠä¼ ç»ŸLICï¼Œä»¥å®ç°é«˜ä¿çœŸå’Œé«˜æ„ŸçŸ¥è´¨é‡ã€‚ä¸åŒäºä¹‹å‰ç›´æ¥ä½¿ç”¨é¢„è®­ç»ƒçš„LICæ¨¡å‹ä»é‡åº¦é‡åŒ–çš„æ½œåœ¨ä¿¡æ¯ä¸­äº§ç”Ÿä½è´¨é‡ä¿çœŸåº¦ä¿æŒä¿¡æ¯çš„æ··åˆæ–¹æ³•ï¼Œæˆ‘ä»¬ä½¿ç”¨æ‰©æ•£æ¨¡å‹ä»åŸå§‹çœŸå®è¾“å…¥ä¸­æå–é«˜è´¨é‡è¡¥å……ä¿çœŸä¿¡æ¯ï¼Œè¿™å¯ä»¥åœ¨å¤šæ–¹é¢æå‡ç³»ç»Ÿæ€§èƒ½ï¼šæ”¹å–„ç´¢å¼•æ˜ å°„é¢„æµ‹ï¼Œæé«˜LICæµçš„ä¿çœŸåº¦ä¿æŒè¾“å‡ºï¼Œå¹¶é€šè¿‡VQæ½œåœ¨æ ¡æ­£è¿›è¡Œæ¡ä»¶å›¾åƒé‡å»ºã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬çš„æ‰©æ•£æ¨¡å‹åŸºäºå¯†é›†è¡¨ç¤ºå‘é‡ï¼ˆDRVï¼‰ï¼Œå…·æœ‰è½»é‡çº§å’Œéå¸¸ç®€å•çš„é‡‡æ ·è°ƒåº¦å™¨ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„HDCompressionåœ¨å®šé‡æŒ‡æ ‡å’Œå®šæ€§å¯è§†åŒ–æ–¹é¢å‡ä¼˜äºä¹‹å‰çš„ä¼ ç»ŸLICã€ç”ŸæˆVQå»ºæ¨¡å’Œæ··åˆæ¡†æ¶ï¼Œåœ¨è¶…ä½æ¯”ç‰¹ç‡ä¸‹æä¾›äº†å¹³è¡¡çš„ç¨³å¥å‹ç¼©æ€§èƒ½ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.07160v1">PDF</a> Under Review</p>
<p><strong>Summary</strong></p>
<p>æœ¬å·¥ä½œæå‡ºä¸€ç§åä¸ºHybrid-Diffusion Image Compressionï¼ˆHDCompressionï¼‰çš„æ··åˆæ‰©æ•£å›¾åƒå‹ç¼©æŠ€æœ¯ï¼Œè¯¥æŠ€æœ¯ç»“åˆäº†ç”Ÿæˆå¼VQå»ºæ¨¡ã€ä¼ ç»Ÿå›¾åƒå‹ç¼©å’Œæ‰©æ•£æ¨¡å‹ï¼Œæ—¨åœ¨å®ç°é«˜ä¿çœŸåº¦å’Œé«˜æ„ŸçŸ¥è´¨é‡ã€‚ä¸åŒäºä»¥å¾€ç›´æ¥ä½¿ç”¨é¢„è®­ç»ƒçš„ä¼ ç»Ÿå›¾åƒå‹ç¼©æ¨¡å‹ç”Ÿæˆä½è´¨é‡ä¿çœŸåº¦ä¿¡æ¯çš„æ–¹æ³•ï¼Œæˆ‘ä»¬çš„æ‰©æ•£æ¨¡å‹ä»åŸå§‹çœŸå®è¾“å…¥ä¸­æå–é«˜è´¨é‡äº’è¡¥ä¿çœŸä¿¡æ¯ï¼Œä»è€Œæé«˜äº†ç³»ç»Ÿæ€§èƒ½ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬çš„æ‰©æ•£æ¨¡å‹åŸºäºè½»é‡çº§çš„å¯†é›†ä»£è¡¨å‘é‡ï¼ˆDRVï¼‰ï¼Œé‡‡æ ·è°ƒåº¦å™¨éå¸¸ç®€å•ã€‚å®éªŒè¯æ˜ï¼Œåœ¨è¶…ä½æ¯”ç‰¹ç‡ä¸‹ï¼ŒHDCompressionåœ¨å®šé‡æŒ‡æ ‡å’Œå®šæ€§å¯è§†åŒ–æ–¹é¢éƒ½ä¼˜äºä¼ ç»Ÿçš„å›¾åƒå‹ç¼©å’Œç”Ÿæˆå¼VQå»ºæ¨¡æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>HDCompressionç»“åˆäº†ç”Ÿæˆå¼VQå»ºæ¨¡ã€ä¼ ç»Ÿå›¾åƒå‹ç¼©å’Œæ‰©æ•£æ¨¡å‹ï¼Œæ—¨åœ¨è§£å†³è¶…ä½æ¯”ç‰¹ç‡ä¸‹çš„å›¾åƒå‹ç¼©æŒ‘æˆ˜ã€‚</li>
<li>ä¼ ç»Ÿçš„å›¾åƒå‹ç¼©æ–¹æ³•é¢ä¸´ä¸¥é‡çš„é‡åŒ–å¤±çœŸé—®é¢˜ï¼Œè€Œç”Ÿæˆå¼VQå»ºæ¨¡åˆ™ç”±äºç”Ÿæˆå…ˆéªŒä¸ç‰¹å®šè¾“å…¥çš„ä¸åŒ¹é…è€Œå¯¼è‡´ä¿çœŸåº¦ä½ã€‚</li>
<li>HDCompressionä½¿ç”¨æ‰©æ•£æ¨¡å‹ä»åŸå§‹çœŸå®è¾“å…¥ä¸­æå–é«˜è´¨é‡äº’è¡¥ä¿çœŸä¿¡æ¯ï¼Œæé«˜ç³»ç»Ÿæ€§èƒ½ã€‚</li>
<li>HDCompressioné€šè¿‡æ”¹è¿›ç´¢å¼•æ˜ å°„é¢„æµ‹ã€å¢å¼ºä¼ ç»Ÿå›¾åƒå‹ç¼©æµçš„ä¿çœŸåº¦ä¿æŒè¾“å‡ºä»¥åŠç»†åŒ–åŸºäºVQæ½œåœ¨æ ¡æ­£çš„æ¡ä»¶å›¾åƒé‡å»ºæ¥å¢å¼ºç³»ç»Ÿæ€§èƒ½ã€‚</li>
<li>ä¸å…¶ä»–æ··åˆæ–¹æ³•ä¸åŒï¼ŒHDCompressionä½¿ç”¨åŸºäºè½»é‡çº§å¯†é›†ä»£è¡¨å‘é‡ï¼ˆDRVï¼‰çš„æ‰©æ•£æ¨¡å‹ï¼Œå…·æœ‰éå¸¸ç®€å•çš„é‡‡æ ·è°ƒåº¦å™¨ã€‚</li>
<li>å®éªŒè¯æ˜ï¼ŒHDCompressionåœ¨å®šé‡æŒ‡æ ‡å’Œå®šæ€§å¯è§†åŒ–æ–¹é¢ä¼˜äºä¼ ç»Ÿçš„å›¾åƒå‹ç¼©å’Œç”Ÿæˆå¼VQå»ºæ¨¡æ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.07160">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-6ff9076c2325696ae82fc6eb7ef6edcf.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4a46c4a4b63e6126703b406339e85cb8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-839ebbafb28640c5cd2d041eb04adec9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-88b7851930ad7fc7f390c7fc29ef87d6.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-b4f26c7faad21302a27a4ae71a9f54c2.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="Conditional-diffusion-model-with-spatial-attention-and-latent-embedding-for-medical-image-segmentation"><a href="#Conditional-diffusion-model-with-spatial-attention-and-latent-embedding-for-medical-image-segmentation" class="headerlink" title="Conditional diffusion model with spatial attention and latent embedding   for medical image segmentation"></a>Conditional diffusion model with spatial attention and latent embedding   for medical image segmentation</h2><p><strong>Authors:Behzad Hejrati, Soumyanil Banerjee, Carri Glide-Hurst, Ming Dong</strong></p>
<p>Diffusion models have been used extensively for high quality image and video generation tasks. In this paper, we propose a novel conditional diffusion model with spatial attention and latent embedding (cDAL) for medical image segmentation. In cDAL, a convolutional neural network (CNN) based discriminator is used at every time-step of the diffusion process to distinguish between the generated labels and the real ones. A spatial attention map is computed based on the features learned by the discriminator to help cDAL generate more accurate segmentation of discriminative regions in an input image. Additionally, we incorporated a random latent embedding into each layer of our model to significantly reduce the number of training and sampling time-steps, thereby making it much faster than other diffusion models for image segmentation. We applied cDAL on 3 publicly available medical image segmentation datasets (MoNuSeg, Chest X-ray and Hippocampus) and observed significant qualitative and quantitative improvements with higher Dice scores and mIoU over the state-of-the-art algorithms. The source code is publicly available at <a target="_blank" rel="noopener" href="https://github.com/Hejrati/cDAL/">https://github.com/Hejrati/cDAL/</a>. </p>
<blockquote>
<p>æ‰©æ•£æ¨¡å‹å·²è¢«å¹¿æ³›åº”ç”¨äºé«˜è´¨é‡å›¾åƒå’Œè§†é¢‘ç”Ÿæˆä»»åŠ¡ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§å…·æœ‰ç©ºé—´æ³¨æ„åŠ›å’Œæ½œåœ¨åµŒå…¥ï¼ˆcDALï¼‰çš„æ–°å‹æ¡ä»¶æ‰©æ•£æ¨¡å‹ï¼Œç”¨äºåŒ»å­¦å›¾åƒåˆ†å‰²ã€‚åœ¨cDALä¸­ï¼Œæ‰©æ•£è¿‡ç¨‹çš„æ¯ä¸€æ­¥éƒ½ä½¿ç”¨åŸºäºå·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNï¼‰çš„é‰´åˆ«å™¨æ¥åŒºåˆ†ç”Ÿæˆçš„æ ‡ç­¾å’ŒçœŸå®çš„æ ‡ç­¾ã€‚åŸºäºé‰´åˆ«å™¨å­¦ä¹ çš„ç‰¹å¾è®¡ç®—ç©ºé—´æ³¨æ„åŠ›å›¾ï¼Œå¸®åŠ©cDALç”Ÿæˆè¾“å…¥å›¾åƒä¸­åˆ¤åˆ«åŒºåŸŸçš„æ›´å‡†ç¡®åˆ†å‰²ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å°†éšæœºæ½œåœ¨åµŒå…¥èå…¥æ¨¡å‹ä¸­çš„æ¯ä¸€å±‚ï¼Œä»¥å¤§å¤§å‡å°‘è®­ç»ƒå’Œé‡‡æ ·æ­¥éª¤çš„æ•°é‡ï¼Œä»è€Œä½¿å®ƒæ¯”å…¶ä»–ç”¨äºå›¾åƒåˆ†å‰²çš„æ‰©æ•£æ¨¡å‹æ›´å¿«ã€‚æˆ‘ä»¬åœ¨ä¸‰ä¸ªå…¬å¼€çš„åŒ»å­¦å›¾åƒåˆ†å‰²æ•°æ®é›†ï¼ˆMoNuSegã€Chest X-rayå’ŒHippocampusï¼‰ä¸Šåº”ç”¨äº†cDALï¼Œä¸æœ€å…ˆè¿›çš„ç®—æ³•ç›¸æ¯”ï¼Œåœ¨å®šæ€§å’Œå®šé‡æ–¹é¢éƒ½å–å¾—äº†æ˜¾è‘—çš„æ”¹è¿›ï¼ŒDiceå¾—åˆ†å’ŒmIoUæ›´é«˜ã€‚æºä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/Hejrati/cDAL/%E5%85%AC%E5%BC%80%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/Hejrati/cDAL/å…¬å¼€è·å–ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.06997v1">PDF</a> 11 pages, 2 figures, 3 tables, Accepted in MICCAI 2024</p>
<p><strong>Summary</strong><br>æ‰©æ•£æ¨¡å‹åœ¨åŒ»å­¦å›¾åƒåˆ†å‰²ä¸­å±•ç°å‡ºæ˜¾è‘—ä¼˜åŠ¿ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§å…·æœ‰ç©ºé—´æ³¨æ„åŠ›å’Œæ½œåœ¨åµŒå…¥çš„æ–°æ¡ä»¶æ‰©æ•£æ¨¡å‹ï¼ˆcDALï¼‰ã€‚åˆ©ç”¨åŸºäºå·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNï¼‰çš„é‰´åˆ«å™¨æé«˜å›¾åƒç”Ÿæˆè´¨é‡ï¼Œé€šè¿‡ç©ºé—´æ³¨æ„åŠ›å›¾æ›´ç²¾ç¡®åœ°åˆ†å‰²è¾“å…¥å›¾åƒä¸­çš„åˆ¤åˆ«åŒºåŸŸã€‚æ­¤å¤–ï¼Œæ¨¡å‹ä¸­åµŒå…¥éšæœºæ½œåœ¨å‘é‡ä»¥åŠ å¿«è®­ç»ƒå’Œé‡‡æ ·è¿‡ç¨‹ï¼Œæ˜¾è‘—ä¼˜äºå…¶ä»–å›¾åƒåˆ†å‰²æ‰©æ•£æ¨¡å‹ã€‚åº”ç”¨äºä¸‰ä¸ªå…¬å¼€åŒ»å­¦å›¾åƒåˆ†å‰²æ•°æ®é›†ä¸Šçš„ç»“æœè¡¨æ˜ï¼ŒcDALå®ç°äº†æ›´é«˜çš„ç‹„å…‹ç³»æ•°å’Œå¹³å‡äº¤å¹¶æ¯”ï¼Œä¼˜äºç°æœ‰ç®—æ³•ã€‚æºä»£ç å·²å…¬å¼€ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°å‹æ¡ä»¶æ‰©æ•£æ¨¡å‹cDALï¼Œç”¨äºåŒ»å­¦å›¾åƒåˆ†å‰²ã€‚</li>
<li>cDALæ¨¡å‹åˆ©ç”¨CNNé‰´åˆ«å™¨åœ¨æ‰©æ•£è¿‡ç¨‹çš„æ¯ä¸ªæ—¶é—´æ­¥é•¿ä¸­åŒºåˆ†ç”Ÿæˆçš„æ ‡ç­¾å’ŒçœŸå®æ ‡ç­¾ï¼Œä»¥æé«˜å›¾åƒç”Ÿæˆè´¨é‡ã€‚</li>
<li>ç©ºé—´æ³¨æ„åŠ›å›¾æœ‰åŠ©äºcDALæ›´ç²¾ç¡®åœ°åˆ†å‰²è¾“å…¥å›¾åƒä¸­çš„åˆ¤åˆ«åŒºåŸŸã€‚</li>
<li>cDALæ¨¡å‹é€šè¿‡åµŒå…¥éšæœºæ½œåœ¨å‘é‡ï¼Œæ˜¾è‘—å‡å°‘äº†è®­ç»ƒå’Œé‡‡æ ·çš„æ—¶é—´æ­¥é•¿ï¼Œä»è€Œæé«˜äº†é€Ÿåº¦ã€‚</li>
<li>cDALåœ¨ä¸‰ä¸ªå…¬å¼€çš„åŒ»å­¦å›¾åƒåˆ†å‰²æ•°æ®é›†ä¸Šçš„è¡¨ç°ä¼˜äºç°æœ‰ç®—æ³•ï¼Œå…·æœ‰æ›´é«˜çš„ç‹„å…‹ç³»æ•°å’Œå¹³å‡äº¤å¹¶æ¯”ã€‚</li>
<li>cDALæ¨¡å‹çš„æºä»£ç å·²ç»å…¬å¼€ï¼Œä¾¿äºå…¶ä»–ç ”ç©¶è€…ä½¿ç”¨å’Œæ”¹è¿›ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.06997">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-256395d647a1c7449dba2a89e57c658e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1a77ce743221a1e323f776121fce2cc5.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="ViSIR-Vision-Transformer-Single-Image-Reconstruction-Method-for-Earth-System-Models"><a href="#ViSIR-Vision-Transformer-Single-Image-Reconstruction-Method-for-Earth-System-Models" class="headerlink" title="ViSIR: Vision Transformer Single Image Reconstruction Method for Earth   System Models"></a>ViSIR: Vision Transformer Single Image Reconstruction Method for Earth   System Models</h2><p><strong>Authors:Ehsan Zeraatkar, Salah Faroughi, Jelena TeÅ¡iÄ‡</strong></p>
<p>Purpose: Earth system models (ESMs) integrate the interactions of the atmosphere, ocean, land, ice, and biosphere to estimate the state of regional and global climate under a wide variety of conditions. The ESMs are highly complex, and thus, deep neural network architectures are used to model the complexity and store the down-sampled data. In this paper, we propose the Vision Transformer Sinusoidal Representation Networks (ViSIR) to improve the single image SR (SR) reconstruction task for the ESM data.   Methods: ViSIR combines the SR capability of Vision Transformers (ViT) with the high-frequency detail preservation of the Sinusoidal Representation Network (SIREN) to address the spectral bias observed in SR tasks.   Results: The ViSIR outperforms ViT by 4.1 dB, SIREN by 7.5 dB, and SR-Generative Adversarial (SR-GANs) by 7.1dB PSNR on average for three different measurements.   Conclusion: The proposed ViSIR is evaluated and compared with state-of-the-art methods. The results show that the proposed algorithm is outperforming other methods in terms of Mean Square Error(MSE), Peak-Signal-to-Noise-Ratio(PSNR), and Structural Similarity Index Measure(SSIM). </p>
<blockquote>
<p>ç›®çš„ï¼šåœ°çƒç³»ç»Ÿæ¨¡å‹ï¼ˆESMï¼‰æ•´åˆäº†å¤§æ°”ã€æµ·æ´‹ã€é™†åœ°ã€å†°å±‚å’Œç”Ÿç‰©åœˆçš„ç›¸äº’ä½œç”¨ï¼Œä»¥åœ¨å¤šç§æ¡ä»¶ä¸‹ä¼°è®¡åŒºåŸŸå’Œå…¨çƒæ°”å€™çš„çŠ¶æ€ã€‚ç”±äºESMé«˜åº¦å¤æ‚ï¼Œå› æ­¤ä½¿ç”¨æ·±åº¦ç¥ç»ç½‘ç»œæ¶æ„æ¥å¯¹å…¶å¤æ‚æ€§è¿›è¡Œå»ºæ¨¡å¹¶å­˜å‚¨é™é‡‡æ ·æ•°æ®ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†Vision Transformer Sinusoidal Representation Networksï¼ˆViSIRï¼‰ï¼Œæ—¨åœ¨æ”¹è¿›ESMæ•°æ®çš„å•å›¾åƒè¶…åˆ†è¾¨ç‡ï¼ˆSRï¼‰é‡å»ºä»»åŠ¡ã€‚</p>
</blockquote>
<p>æ–¹æ³•ï¼šViSIRç»“åˆäº†Vision Transformerï¼ˆViTï¼‰çš„SRèƒ½åŠ›å’ŒSinusoidal Representation Networkï¼ˆSIRENï¼‰çš„é«˜é¢‘ç»†èŠ‚ä¿ç•™èƒ½åŠ›ï¼Œä»¥è§£å†³SRä»»åŠ¡ä¸­è§‚å¯Ÿåˆ°çš„é¢‘è°±åè§ã€‚</p>
<p>ç»“æœï¼šViSIRåœ¨ä¸‰ç§ä¸åŒæµ‹é‡ä¸Šå¹³å‡æ¯”ViTé«˜å‡º4.1 dBï¼Œæ¯”SIRENé«˜å‡º7.5 dBï¼Œæ¯”SR-ç”Ÿæˆå¯¹æŠ—ç½‘ç»œï¼ˆSR-GANsï¼‰é«˜å‡º7.1 dBçš„å³°å€¼ä¿¡å™ªæ¯”ï¼ˆPSNRï¼‰ã€‚</p>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.06741v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºä¸€ç§æ–°çš„åŸºäºVision Transformerå’ŒSinusoidal Representation Networkç»“åˆçš„æ¨¡å‹â€”â€”Vision Transformer Sinusoidal Representation Networksï¼ˆViSIRï¼‰ï¼Œæ—¨åœ¨æ”¹å–„åœ°çƒç³»ç»Ÿæ¨¡å‹ä¸­è¶…åˆ†è¾¨ç‡é‡å»ºï¼ˆSRï¼‰ä»»åŠ¡çš„æ•ˆæœã€‚è¯¥æ¨¡å‹ç»“åˆäº†ViTçš„è¶…åˆ†è¾¨ç‡èƒ½åŠ›å’ŒSIRENçš„é«˜é¢‘ç»†èŠ‚ä¿ç•™ç‰¹æ€§ï¼Œè§£å†³äº†SRä»»åŠ¡ä¸­çš„å…‰è°±åå·®é—®é¢˜ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒViSIRç›¸è¾ƒäºå…¶ä»–æ¨¡å‹åœ¨PSNRç­‰æŒ‡æ ‡ä¸Šæœ‰æ‰€æå‡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ESMsåˆ©ç”¨æ·±åº¦ç¥ç»ç½‘ç»œè¿›è¡Œå¤æ‚æ¨¡æ‹Ÿå¹¶å¤„ç†é™é‡‡æ ·æ•°æ®ã€‚</li>
<li>Vision Transformer Sinusoidal Representation Networks (ViSIR)ç»“åˆViTå’ŒSIRENçš„ä¼˜ç‚¹ï¼Œæ—¨åœ¨æ”¹è¿›ESMæ•°æ®çš„è¶…åˆ†è¾¨ç‡é‡å»ºï¼ˆSRï¼‰ã€‚</li>
<li>ViSIRè§£å†³äº†SRä»»åŠ¡ä¸­çš„å…‰è°±åå·®é—®é¢˜ã€‚</li>
<li>ViSIRåœ¨PSNRã€MSEå’ŒSSIMç­‰æŒ‡æ ‡ä¸Šçš„æ€§èƒ½ä¼˜äºå…¶ä»–æ¨¡å‹ã€‚</li>
<li>ViSIRç›¸è¾ƒäºViTã€SIRENå’ŒSR-GANså¹³å‡æé«˜äº†4.1dBã€7.5dBå’Œ7.1dBçš„PSNRã€‚</li>
<li>ViSIRæ¨¡å‹åœ¨æ”¹å–„SRä»»åŠ¡æ–¹é¢çš„æ•ˆæœæ˜¾è‘—ï¼Œå…·æœ‰å®é™…åº”ç”¨æ½œåŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.06741">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-cf9048c3b816cee988efb911d2fe6083.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-88028bcf0c1828b06f6099fcd8baf426.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-ee43ee84dd1032a6ae63795dd0fd3c89.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-0272a4ba28fe565af009c03462bf7be8.jpg" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="WGM-microprobe-device-for-high-sensitivity-and-broadband-ultrasound-detection"><a href="#WGM-microprobe-device-for-high-sensitivity-and-broadband-ultrasound-detection" class="headerlink" title="WGM microprobe device for high-sensitivity and broadband ultrasound   detection"></a>WGM microprobe device for high-sensitivity and broadband ultrasound   detection</h2><p><strong>Authors:Jialve Sun, Shengnan Huangfu, Tinglan Chen, Zijing Cai, Bowen Ruan, Fangxing Zhang</strong></p>
<p>Whispering-gallery-mode (WGM) microcavities have emerged as a promising alternative to traditional ultrasound probes, offering high sensitivity and wide bandwidth. In our research, we propose a novel silica WGM microprobe device, with impressive Q factors up to 10^7.The side-coupled approach and special encapsulation design make the device small, robust, and capable of utilizing in both gaseous and liquid environments.We have successfully conducted photoacoustic (PA) imaging on various samples using this device which demonstrates a high sensitivity of 5.4 mPa&#x2F;sqrt(Hz) and a board bandwidth of 41 MHz at -6 dB for ultrasound. Whatâ€™s more, itâ€™s capable of capturing the vibration spectrum of microparticles up to a few hundred megahertz. Our compact and lightweight device exhibits significant application potential in PA endoscopic detection, near-field ultrasound sensing and other aspects. </p>
<blockquote>
<p>å¾®è…”ä¸­çš„å—¡å—¡æ¨¡å¼ï¼ˆWGMï¼‰å·²æˆä¸ºä¼ ç»Ÿè¶…å£°æ¢é’ˆçš„ä¸€ç§å¾ˆæœ‰å‰é€”çš„æ›¿ä»£å“ï¼Œå…·æœ‰çµæ•åº¦é«˜å’Œå¸¦å®½å¤§çš„ç‰¹ç‚¹ã€‚åœ¨æˆ‘ä»¬çš„ç ”ç©¶ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°å‹äºŒæ°§åŒ–ç¡…WGMå¾®å‹æ¢é’ˆè£…ç½®ï¼Œå…¶Qå› å­é«˜è¾¾10^7ã€‚ä¾§é¢è€¦åˆçš„æ–¹æ³•å’Œç‰¹æ®Šçš„å°è£…è®¾è®¡ä½¿è¯¥è®¾å¤‡ä½“ç§¯å°ã€ç¨³å®šæ€§é«˜ï¼Œæ—¢èƒ½åœ¨æ°”æ€ç¯å¢ƒä¸­ä½¿ç”¨ä¹Ÿèƒ½åœ¨æ¶²æ€ç¯å¢ƒä¸­ä½¿ç”¨ã€‚æˆ‘ä»¬å·²ç»æˆåŠŸä½¿ç”¨è¯¥è®¾å¤‡å¯¹å„ç§æ ·å“è¿›è¡Œå…‰å£°æˆåƒï¼Œå…¶çµæ•åº¦é«˜è¾¾5.4 mPa&#x2F;sqrt(Hz)ï¼Œåœ¨è¶…å£°-6 dBä¸‹çš„å¸¦å®½ä¸º41 MHzã€‚æ­¤å¤–ï¼Œå®ƒèƒ½æ•æ‰åˆ°é«˜è¾¾æ•°ç™¾å…†èµ«å…¹çš„å¾®ç²’å­æŒ¯åŠ¨å…‰è°±ã€‚æˆ‘ä»¬çš„ç´§å‡‘è½»ä¾¿çš„è®¾å¤‡åœ¨å…‰å£°å†…çª¥æ£€æµ‹ã€è¿‘åœºè¶…å£°ä¼ æ„Ÿå’Œå…¶ä»–æ–¹é¢è¡¨ç°å‡ºå·¨å¤§çš„åº”ç”¨æ½œåŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.04627v2">PDF</a> </p>
<p><strong>Summary</strong><br>å¾®è…”ä¸­çš„è½»å£°æ¨¡å¼ï¼ˆWGMï¼‰å¾®è…”æä¾›äº†ä¸€ç§å¯¹ä¼ ç»Ÿè¶…å£°æ¢é’ˆé¢‡å…·æ½œåŠ›çš„æ›¿ä»£æ–¹æ¡ˆï¼Œå…·æœ‰çµæ•åº¦é«˜å’Œå¸¦å®½å®½çš„ç‰¹ç‚¹ã€‚ç ”ç©¶ä¸­æå‡ºäº†ä¸€ç§æ–°å‹äºŒæ°§åŒ–ç¡…WGMå¾®æ¢é’ˆå™¨ä»¶ï¼Œå…·æœ‰é«˜è¾¾10^7çš„Qå€¼ã€‚å…¶ä¾§è€¦åˆæ–¹å¼å’Œç‰¹æ®Šå°è£…è®¾è®¡ä½¿å™¨ä»¶å°å·§ã€ç¨³å¥ï¼Œæ—¢å¯ç”¨äºæ°”ä½“ç¯å¢ƒä¹Ÿå¯ç”¨äºæ¶²ä½“ç¯å¢ƒã€‚åˆ©ç”¨è¯¥å™¨ä»¶æˆåŠŸè¿›è¡Œäº†å…‰å£°æˆåƒå®éªŒï¼Œå±•ç¤ºäº†é«˜çµæ•åº¦å’Œå®½é¢‘å¸¦ç‰¹æ€§ï¼Œå¹¶èƒ½å¤Ÿæ•æ‰åˆ°é«˜è¾¾æ•°ç™¾å…†èµ«å…¹çš„å¾®é¢—ç²’æŒ¯åŠ¨è°±ã€‚æ­¤ç´§å‡‘è½»ä¾¿çš„å™¨ä»¶åœ¨å…‰å£°å†…çª¥æ£€æµ‹ã€è¿‘åœºè¶…å£°ä¼ æ„Ÿç­‰æ–¹é¢å…·æœ‰æ˜¾è‘—çš„åº”ç”¨æ½œåŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>WGMå¾®è…”ä½œä¸ºä¼ ç»Ÿè¶…å£°æ¢é’ˆçš„æ›¿ä»£æ–¹æ¡ˆï¼Œå…·æœ‰é«˜çµæ•åº¦å’Œå®½é¢‘å¸¦ç‰¹æ€§ã€‚</li>
<li>æ–°å‹äºŒæ°§åŒ–ç¡…WGMå¾®æ¢é’ˆå™¨ä»¶å…·æœ‰é«˜è¾¾10^7çš„Qå€¼ã€‚</li>
<li>ä¾§è€¦åˆå’Œç‰¹æ®Šå°è£…è®¾è®¡ä½¿å™¨ä»¶é€‚åº”äºå„ç§ç¯å¢ƒï¼ŒåŒ…æ‹¬æ°”ä½“å’Œæ¶²ä½“ã€‚</li>
<li>æˆåŠŸè¿›è¡Œå…‰å£°æˆåƒå®éªŒï¼Œå±•ç¤ºäº†é«˜çµæ•åº¦ï¼ˆ5.4 mPa&#x2F;sqrt(Hz)ï¼‰å’Œå®½é¢‘å¸¦ï¼ˆ-6 dBä¸‹41 MHzï¼‰ã€‚</li>
<li>å™¨ä»¶èƒ½å¤Ÿæ•æ‰åˆ°é«˜è¾¾æ•°ç™¾å…†èµ«å…¹çš„å¾®é¢—ç²’æŒ¯åŠ¨è°±ã€‚</li>
<li>å™¨ä»¶ç´§å‡‘è½»ä¾¿ï¼Œå…·æœ‰æ˜¾è‘—çš„åº”ç”¨æ½œåŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.04627">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-99c799c78ac2892ab5bf05ed48de471d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c2934d60cb2c951db362fc60b1d1cef3.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-07f6526b592d8d45cf1c225b5600863e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e053710ca7242a781bcd2ee1730b7b68.jpg" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="Generating-crossmodal-gene-expression-from-cancer-histopathology-improves-multimodal-AI-predictions"><a href="#Generating-crossmodal-gene-expression-from-cancer-histopathology-improves-multimodal-AI-predictions" class="headerlink" title="Generating crossmodal gene expression from cancer histopathology   improves multimodal AI predictions"></a>Generating crossmodal gene expression from cancer histopathology   improves multimodal AI predictions</h2><p><strong>Authors:Samiran Dey, Christopher R. S. Banerji, Partha Basuchowdhuri, Sanjoy K. Saha, Deepak Parashar, Tapabrata Chakraborti</strong></p>
<p>Emerging research has highlighted that artificial intelligence based multimodal fusion of digital pathology and transcriptomic features can improve cancer diagnosis (grading&#x2F;subtyping) and prognosis (survival risk) prediction. However, such direct fusion for joint decision is impractical in real clinical settings, where histopathology is still the gold standard for diagnosis and transcriptomic tests are rarely requested, at least in the public healthcare system. With our novel diffusion based crossmodal generative AI model PathGen, we show that genomic expressions synthesized from digital histopathology jointly predicts cancer grading and patient survival risk with high accuracy (state-of-the-art performance), certainty (through conformal coverage guarantee) and interpretability (through distributed attention maps). PathGen code is available for open use by the research community through GitHub at <a target="_blank" rel="noopener" href="https://github.com/Samiran-Dey/PathGen">https://github.com/Samiran-Dey/PathGen</a>. </p>
<blockquote>
<p>æ–°å…´ç ”ç©¶çªå‡ºè¡¨æ˜ï¼ŒåŸºäºäººå·¥æ™ºèƒ½çš„æ•°å­—ç—…ç†å’Œè½¬å½•ç»„ç‰¹å¾çš„å¤šæ¨¡å¼èåˆå¯ä»¥æé«˜ç™Œç—‡è¯Šæ–­ï¼ˆåˆ†çº§&#x2F;äºšå‹ï¼‰å’Œé¢„åï¼ˆç”Ÿå­˜é£é™©ï¼‰é¢„æµ‹çš„å‡†ç¡®æ€§ã€‚ç„¶è€Œï¼Œåœ¨å®é™…ä¸´åºŠç¯å¢ƒä¸­ï¼Œè¿™ç§ç›´æ¥èåˆè¿›è¡Œè”åˆå†³ç­–å¹¶ä¸åˆ‡å®é™…ã€‚ç‰¹åˆ«æ˜¯åœ¨å…¬å…±åŒ»ç–—ä½“ç³»ä¸­ï¼Œç»„ç»‡ç—…ç†å­¦ä»æ˜¯è¯Šæ–­çš„é‡‘æ ‡å‡†ï¼Œè½¬å½•ç»„æµ‹è¯•å¾ˆå°‘è¢«è¦æ±‚ã€‚é€šè¿‡æˆ‘ä»¬æ–°é¢–çš„åŸºäºæ‰©æ•£çš„è·¨æ¨¡æ€ç”Ÿæˆäººå·¥æ™ºèƒ½æ¨¡å‹PathGenï¼Œæˆ‘ä»¬å±•ç¤ºäº†ç”±æ•°å­—ç—…ç†å­¦åˆæˆçš„åŸºå› è¡¨è¾¾èƒ½å¤Ÿè”åˆé¢„æµ‹ç™Œç—‡åˆ†çº§å’Œæ‚£è€…ç”Ÿå­˜é£é™©ï¼Œå…·æœ‰å¾ˆé«˜çš„å‡†ç¡®æ€§ï¼ˆè¾¾åˆ°æœ€æ–°æ€§èƒ½ï¼‰ã€ç¡®å®šæ€§ï¼ˆé€šè¿‡åˆå½¢è¦†ç›–ä¿è¯ï¼‰å’Œå¯è§£é‡Šæ€§ï¼ˆé€šè¿‡åˆ†å¸ƒå¼æ³¨æ„åŠ›å›¾ï¼‰ã€‚PathGenä»£ç å¯é€šè¿‡GitHubä¾›ç ”ç©¶ç•Œå¼€æ”¾ä½¿ç”¨ï¼Œç½‘å€ä¸ºï¼š<a target="_blank" rel="noopener" href="https://github.com/Samiran-Dey/PathGen%E3%80%82">https://github.com/Samiran-Dey/PathGenã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.00568v3">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ¢è®¨äº†äººå·¥æ™ºèƒ½åœ¨æ•°å­—ç—…ç†ä¸è½¬å½•ç»„ç‰¹å¾èåˆæ–¹é¢çš„æ–°å…´ç ”ç©¶ï¼ŒæŒ‡å‡ºè¿™ç§èåˆèƒ½æé«˜ç™Œç—‡è¯Šæ–­ï¼ˆåˆ†çº§&#x2F;äºšå‹ï¼‰å’Œé¢„åï¼ˆç”Ÿå­˜é£é™©ï¼‰é¢„æµ‹çš„å‡†ç¡®æ€§ã€‚ç„¶è€Œï¼Œåœ¨å®é™…ä¸´åºŠç¯å¢ƒä¸­ç›´æ¥èåˆè¿›è¡Œè”åˆå†³ç­–å¹¶ä¸ç°å®ã€‚é€šè¿‡ä½¿ç”¨æ–°å‹çš„åŸºäºæ‰©æ•£çš„è·¨æ¨¡æ€ç”Ÿæˆäººå·¥æ™ºèƒ½æ¨¡å‹PathGenï¼Œæˆ‘ä»¬å±•ç¤ºäº†ä»æ•°å­—ç—…ç†åˆæˆçš„åŸºå› ç»„è¡¨è¾¾è”åˆé¢„æµ‹ç™Œç—‡åˆ†çº§å’Œæ‚£è€…ç”Ÿå­˜é£é™©çš„é«˜å‡†ç¡®æ€§ã€ç¡®å®šæ€§å’Œå¯è§£é‡Šæ€§ã€‚PathGenä»£ç å·²å¼€æºï¼Œä¾›ç ”ç©¶ç¤¾åŒºé€šè¿‡GitHubä½¿ç”¨ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>äººå·¥æ™ºèƒ½åœ¨æ•°å­—ç—…ç†å’Œè½¬å½•ç»„ç‰¹å¾çš„èåˆæ–¹é¢èƒ½æé«˜ç™Œç—‡è¯Šæ–­å’Œé¢„åé¢„æµ‹çš„å‡†ç¡®æ€§ã€‚</li>
<li>ç›´æ¥èåˆè¿›è¡Œè”åˆå†³ç­–åœ¨å®é™…ä¸´åºŠç¯å¢ƒä¸­å¹¶ä¸ç°å®ï¼Œå› ä¸ºç—…ç†ç»„ç»‡å­¦ä»æ˜¯è¯Šæ–­çš„é‡‘æ ‡å‡†ï¼Œè½¬å½•ç»„æµ‹è¯•åœ¨å…¬å…±åŒ»ç–—ç³»ç»Ÿä¸­å¾ˆå°‘è¢«è¦æ±‚ã€‚</li>
<li>æ–°å‹æ‰©æ•£åŸºäºè·¨æ¨¡æ€ç”Ÿæˆäººå·¥æ™ºèƒ½æ¨¡å‹PathGenèƒ½å¤ŸåˆæˆåŸºå› ç»„è¡¨è¾¾ï¼Œè”åˆé¢„æµ‹ç™Œç—‡åˆ†çº§å’Œæ‚£è€…ç”Ÿå­˜é£é™©ã€‚</li>
<li>PathGenå…·æœ‰é«˜å‡†ç¡®æ€§ã€è¾¾åˆ°æœ€æ–°æ€§èƒ½æ°´å¹³ï¼Œæä¾›ç¡®å®šæ€§ï¼ˆé€šè¿‡è¦†ç›–ä¿è¯ï¼‰å’Œå¯è§£é‡Šæ€§ï¼ˆé€šè¿‡åˆ†å¸ƒå¼æ³¨æ„åŠ›å›¾ï¼‰ã€‚</li>
<li>PathGenä»£ç å·²å¼€æºï¼Œä¾›ç ”ç©¶ç¤¾åŒºä½¿ç”¨ã€‚</li>
<li>è¯¥æ¨¡å‹çš„åº”ç”¨æœ‰åŠ©äºæ¨åŠ¨äººå·¥æ™ºèƒ½åœ¨åŒ»å­¦å›¾åƒåˆ†æé¢†åŸŸçš„è¿›ä¸€æ­¥å‘å±•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.00568">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-12ffdd9e2963ed31e07eeda826de6605.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7657b0524762cd643e4da4f8019fb60e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-6b8b5c480999519c5c61b2c6c02d142c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3779e081e955aeed5bc098a958af6b95.jpg" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1><h2 id="Beyond-Labels-Advancing-Open-Vocabulary-Segmentation-With-Vision-Language-Models"><a href="#Beyond-Labels-Advancing-Open-Vocabulary-Segmentation-With-Vision-Language-Models" class="headerlink" title="Beyond-Labels: Advancing Open-Vocabulary Segmentation With   Vision-Language Models"></a>Beyond-Labels: Advancing Open-Vocabulary Segmentation With   Vision-Language Models</h2><p><strong>Authors:Muhammad Atta ur Rahman</strong></p>
<p>Self-supervised learning can resolve numerous image or linguistic processing problems when effectively trained. This study investigated simple yet efficient methods for adapting previously learned foundation models for open-vocabulary semantic segmentation tasks. Our research proposed â€œBeyond-Labels,â€ a lightweight transformer-based fusion module that uses a handful of image segmentation data to fuse frozen image representations with language concepts. This strategy allows the model to successfully actualize enormous knowledge from pretrained models without requiring extensive retraining, making the model data-efficient and scalable. Furthermore, we efficiently captured positional information in images using Fourier embeddings, thus improving the generalization across various image sizes, addressing one of the key limitations of previous methods. Extensive ablation tests were performed to investigate the important components of our proposed method; when tested against the common benchmark PASCAL-5i, it demonstrated superior performance despite being trained on frozen image and language characteristics. </p>
<blockquote>
<p>è‡ªç›‘ç£å­¦ä¹ åœ¨å¾—åˆ°æœ‰æ•ˆçš„è®­ç»ƒåï¼Œå¯ä»¥è§£å†³è®¸å¤šå›¾åƒæˆ–è¯­è¨€å¤„ç†æ–¹é¢çš„é—®é¢˜ã€‚æœ¬ç ”ç©¶æ¢è®¨äº†é€‚åº”å…ˆå‰å­¦ä¹ çš„åŸºç¡€æ¨¡å‹ç”¨äºå¼€æ”¾è¯æ±‡è¯­ä¹‰åˆ†å‰²ä»»åŠ¡çš„ç®€å•è€Œé«˜æ•ˆçš„æ–¹æ³•ã€‚æˆ‘ä»¬çš„ç ”ç©¶æå‡ºäº†â€œè¶…è¶Šæ ‡ç­¾â€çš„æ¦‚å¿µï¼Œè¿™æ˜¯ä¸€ä¸ªè½»é‡çº§çš„åŸºäºtransformerçš„èåˆæ¨¡å—ï¼Œå®ƒä½¿ç”¨å°‘é‡çš„å›¾åƒåˆ†å‰²æ•°æ®æ¥èåˆå†»ç»“çš„å›¾åƒè¡¨ç¤ºå’Œè¯­è¨€æ¦‚å¿µã€‚è¿™ä¸€ç­–ç•¥ä½¿å¾—æ¨¡å‹èƒ½å¤ŸæˆåŠŸåœ°ä»é¢„è®­ç»ƒæ¨¡å‹ä¸­å®ç°å¤§é‡çŸ¥è¯†ï¼Œè€Œæ— éœ€è¿›è¡Œå¤§è§„æ¨¡çš„é‡è®­ï¼Œä»è€Œæé«˜äº†æ¨¡å‹çš„æ•°æ®æ•ˆç‡å’Œå¯æ‰©å±•æ€§ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬åˆ©ç”¨å‚…é‡Œå¶åµŒå…¥æœ‰æ•ˆåœ°æ•è·äº†å›¾åƒä¸­çš„ä½ç½®ä¿¡æ¯ï¼Œä»è€Œæé«˜äº†æ¨¡å‹åœ¨ä¸åŒå›¾åƒå¤§å°ä¸Šçš„æ³›åŒ–èƒ½åŠ›ï¼Œè§£å†³äº†ä¹‹å‰æ–¹æ³•çš„å…³é”®é™åˆ¶ä¹‹ä¸€ã€‚æˆ‘ä»¬è¿›è¡Œäº†å¹¿æ³›çš„æ¶ˆèå®éªŒï¼Œä»¥ç ”ç©¶æˆ‘ä»¬æå‡ºæ–¹æ³•çš„é‡è¦ç»„ä»¶ï¼›åœ¨é’ˆå¯¹å¸¸ç”¨åŸºå‡†PASCAL-5iè¿›è¡Œçš„æµ‹è¯•ä¸­ï¼Œå³ä½¿åœ¨å†»ç»“çš„å›¾åƒå’Œè¯­è¨€ç‰¹å¾ä¸Šè¿›è¡Œçš„è®­ç»ƒï¼Œä¹Ÿè¡¨ç°å‡ºäº†å“è¶Šçš„æ€§èƒ½ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.16769v4">PDF</a> </p>
<p><strong>Summary</strong><br>     æœ¬ç ”ç©¶æ¢è®¨äº†ç®€å•è€Œé«˜æ•ˆçš„æ–¹æ³•ï¼Œç”¨äºå°†å…ˆå‰å­¦ä¹ çš„åŸºæœ¬æ¨¡å‹é€‚åº”äºå¼€æ”¾è¯æ±‡è¯­ä¹‰åˆ†å‰²ä»»åŠ¡ã€‚ç ”ç©¶æå‡ºäº†â€œè¶…è¶Šæ ‡ç­¾â€çš„è½»é‡çº§è½¬æ¢å™¨èåˆæ¨¡å—ï¼Œè¯¥æ¨¡å—ä½¿ç”¨å°‘é‡çš„å›¾åƒåˆ†å‰²æ•°æ®å°†å†»ç»“çš„å›¾åƒè¡¨ç¤ºä¸è¯­è¨€æ¦‚å¿µç›¸èåˆã€‚æ­¤æ–¹æ³•ä½¿æ¨¡å‹èƒ½å¤Ÿåœ¨ä¸éœ€è¦å¤§é‡é‡æ–°è®­ç»ƒçš„æƒ…å†µä¸‹æˆåŠŸå®ç°é¢„è®­ç»ƒæ¨¡å‹ä¸­çš„å·¨å¤§çŸ¥è¯†ï¼Œä½¿æ¨¡å‹å…·æœ‰æ•°æ®é«˜æ•ˆæ€§å’Œå¯æ‰©å±•æ€§ã€‚æ­¤å¤–ï¼Œé€šè¿‡å‚…ç«‹å¶åµŒå…¥æœ‰æ•ˆåœ°æ•è·äº†å›¾åƒä¸­çš„ä½ç½®ä¿¡æ¯ï¼Œä»è€Œæé«˜äº†ä¸åŒå›¾åƒå¤§å°çš„æ³›åŒ–èƒ½åŠ›ï¼Œè§£å†³äº†ä»¥å‰æ–¹æ³•çš„å…³é”®å±€é™æ€§ä¹‹ä¸€ã€‚å¹¿æ³›çš„æ¶ˆèæµ‹è¯•éªŒè¯äº†æ‰€æå‡ºæ–¹æ³•çš„é‡è¦æˆåˆ†ï¼›åœ¨é’ˆå¯¹å¸¸ç”¨åŸºå‡†PASCAL-5içš„æµ‹è¯•ä¸­ï¼Œå°½ç®¡æ˜¯åœ¨å†»ç»“çš„å›¾åƒå’Œè¯­è¨€ç‰¹å¾ä¸Šè®­ç»ƒçš„ï¼Œä½†å…¶æ€§èƒ½ä»è¡¨ç°å‡ºå“è¶Šã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æœ¬ç ”ç©¶ä½¿ç”¨è‡ªç›‘ç£å­¦ä¹ æ¥è§£å†³å›¾åƒæˆ–è¯­è¨€å¤„ç†ä¸­çš„è®¸å¤šé—®é¢˜ã€‚</li>
<li>æå‡ºäº†ä¸€ç§åä¸ºâ€œè¶…è¶Šæ ‡ç­¾â€çš„èåˆæ¨¡å—ï¼Œè¯¥æ¨¡å—å¯ä»¥é€‚åº”å¼€æ”¾è¯æ±‡è¯­ä¹‰åˆ†å‰²ä»»åŠ¡ã€‚</li>
<li>é€šè¿‡å°†å†»ç»“çš„å›¾åƒè¡¨ç¤ºä¸è¯­è¨€æ¦‚å¿µç›¸èåˆï¼Œå®ç°äº†é¢„è®­ç»ƒæ¨¡å‹ä¸­çš„çŸ¥è¯†è½¬ç§»ã€‚</li>
<li>ä½¿ç”¨å‚…ç«‹å¶åµŒå…¥æ•è·å›¾åƒä¸­çš„ä½ç½®ä¿¡æ¯ï¼Œæé«˜äº†æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ã€‚</li>
<li>æ‰€æå‡ºçš„æ–¹æ³•åœ¨æ•°æ®æ•ˆç‡å’Œå¯æ‰©å±•æ€§æ–¹é¢è¡¨ç°å‡ºä¼˜åŠ¿ã€‚</li>
<li>é€šè¿‡å¹¿æ³›çš„æ¶ˆèæµ‹è¯•éªŒè¯äº†æ–¹æ³•çš„å…³é”®æˆåˆ†ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.16769">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-36224e7591d39b9b869f1f92300f4ef4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e7fbd6815556591fbf3eca71cf1596e3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ee8a6f59f40c6583ddc83f694f6caab6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2bc086708430ffc1eefaf086fed70f3c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-81643d88ae1b7769cad39a37918b5c5c.jpg" align="middle">
</details>


<h1 id="-19"><a href="#-19" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-02-13/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">https://kedreamix.github.io/Talk2Paper/Paper/2025-02-13/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                                    <span class="chip bg-color">åŒ»å­¦å›¾åƒ</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-02-13/TTS/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-87d800ca0a132b127d32991befd3e24a.jpg" class="responsive-img" alt="TTS">
                        
                        <span class="card-title">TTS</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            TTS æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-02-13  LoRP-TTS Low-Rank Personalized Text-To-Speech
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-02-13
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/TTS/" class="post-category">
                                    TTS
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/TTS/">
                        <span class="chip bg-color">TTS</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-02-13/Diffusion%20Models/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-e3f3b3154fabd339535264a798f82277.jpg" class="responsive-img" alt="Diffusion Models">
                        
                        <span class="card-title">Diffusion Models</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Diffusion Models æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-02-13  SwiftSketch A Diffusion Model for Image-to-Vector Sketch Generation
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-02-13
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Diffusion-Models/" class="post-category">
                                    Diffusion Models
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Diffusion-Models/">
                        <span class="chip bg-color">Diffusion Models</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">15230.2k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
