<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Few-Shot">
    <meta name="description" content="Few-Shot 方向最新论文已更新，请持续关注 Update in 2025-02-13  Explanation based In-Context Demonstrations Retrieval for Multilingual   Grammatical Error Correction">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Few-Shot | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loader页面消失采用渐隐的方式*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logo出现动画 */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//使用渐隐的方法淡出loading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//强制显示loading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">嘘~  正在从服务器偷取页面 . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>首页</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>标签</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>分类</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>归档</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="搜索" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="深色/浅色模式" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			首页
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			标签
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			分类
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			归档
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pic1.zhimg.com/v2-80d8acb33bdcaba3f7608b2be4ffc7a9.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Few-Shot</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- 文章内容详情 -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Few-Shot/">
                                <span class="chip bg-color">Few-Shot</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Few-Shot/" class="post-category">
                                Few-Shot
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp;
                    2025-02-13
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>更新日期:&nbsp;&nbsp;
                    2025-03-07
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>文章字数:&nbsp;&nbsp;
                    10.3k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>阅读时长:&nbsp;&nbsp;
                    41 分
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>阅读次数:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>⚠️ 以下所有内容总结都来自于 大语言模型的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p>
</blockquote>
<h1 id="2025-02-13-更新"><a href="#2025-02-13-更新" class="headerlink" title="2025-02-13 更新"></a>2025-02-13 更新</h1><h2 id="Explanation-based-In-Context-Demonstrations-Retrieval-for-Multilingual-Grammatical-Error-Correction"><a href="#Explanation-based-In-Context-Demonstrations-Retrieval-for-Multilingual-Grammatical-Error-Correction" class="headerlink" title="Explanation based In-Context Demonstrations Retrieval for Multilingual   Grammatical Error Correction"></a>Explanation based In-Context Demonstrations Retrieval for Multilingual   Grammatical Error Correction</h2><p><strong>Authors:Wei Li, Wen Luo, Guangyue Peng, Houfeng Wang</strong></p>
<p>Grammatical error correction (GEC) aims to correct grammatical, spelling, and semantic errors in natural language text. With the growing of large language models (LLMs), direct text generation has gradually become the focus of the GEC methods, and few-shot in-context learning presents a cost-effective solution. However, selecting effective in-context examples remains challenging, as the similarity between input texts does not necessarily correspond to similar grammatical error patterns. In this paper, we propose a novel retrieval method based on natural language grammatical error explanations (GEE) to address this issue. Our method retrieves suitable few-shot demonstrations by matching the GEE of the test input with that of pre-constructed database samples, where explanations for erroneous samples are generated by LLMs. We conducted multilingual GEC few-shot experiments on both major open-source and closed-source LLMs. Experiments across five languages show that our method outperforms existing semantic and BM25-based retrieval techniques, without requiring additional training or language adaptation. This also suggests that matching error patterns is key to selecting examples. </p>
<blockquote>
<p>语法错误修正（GEC）旨在修正自然语言文本中的语法、拼写和语义错误。随着大型语言模型（LLM）的增长，直接文本生成逐渐成为GEC方法的重点，而少样本上下文学习提供了一种具有成本效益的解决方案。然而，选择有效的上下文示例仍然具有挑战性，因为输入文本之间的相似性并不一定对应相似的语法错误模式。针对这一问题，我们在本文中提出了一种基于自然语言语法错误解释（GEE）的检索方法。我们的方法通过匹配测试输入的GEE与预先构建的数据库样本的GEE来检索合适的少样本演示，其中错误样本的解释是由LLM生成的。我们在主要的开源和闭源LLM上进行了多语言GEC少样本实验。跨越五种语言的实验表明，我们的方法优于现有的语义和基于BM25的检索技术，无需额外的训练或语言适应。这也表明匹配错误模式是选择示例的关键。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.08507v1">PDF</a> Accepted by NAACL 2025 main conference</p>
<p><strong>Summary</strong><br>自然语言文本中的语法错误修正（GEC）旨在纠正语法、拼写和语义错误。随着大型语言模型（LLM）的发展，直接文本生成逐渐成为GEC方法的重点，而少样本上下文学习则提供了具有成本效益的解决方案。然而，选择有效的上下文实例仍然具有挑战性，因为输入文本之间的相似性并不一定对应相似的语法错误模式。本文提出了一种基于自然语言语法错误解释（GEE）的检索方法来解决这个问题。该方法通过匹配测试输入的GEE与预构建的数据库样本，检索合适的少样本示例，其中错误样本的解释是由LLM生成的。我们在主要开源和闭源的LLM上进行了多语言GEC少样本实验。跨越五种语言的实验表明，该方法优于现有的语义和BM25基础检索技术，无需额外的训练或语言适应。这也表明匹配错误模式是选择示例的关键。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>GEC旨在纠正自然语言文本中的语法、拼写和语义错误。</li>
<li>随着LLM的发展，直接文本生成成为GEC方法的重点，少样本上下文学习提供成本效益解决方案。</li>
<li>选择有效的上下文实例具有挑战性，因为输入文本相似性不一定对应相似的语法错误模式。</li>
<li>论文提出了一种基于GEE的检索方法来解决这一问题。</li>
<li>该方法通过匹配测试输入的GEE与预构建数据库样本进行检索。</li>
<li>实验表明，该方法在多种语言上均优于现有语义和BM25基础检索技术。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.08507">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-bba5ff79cc04768b1ee6f363fe5d75fc.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-9264654bc5093f0686d4831d9a32b813.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-cba8b3293070385218ad7bba4215f106.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-bcc7da750505da19c6dafe5ee22f44b5.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="MEMHD-Memory-Efficient-Multi-Centroid-Hyperdimensional-Computing-for-Fully-Utilized-In-Memory-Computing-Architectures"><a href="#MEMHD-Memory-Efficient-Multi-Centroid-Hyperdimensional-Computing-for-Fully-Utilized-In-Memory-Computing-Architectures" class="headerlink" title="MEMHD: Memory-Efficient Multi-Centroid Hyperdimensional Computing for   Fully-Utilized In-Memory Computing Architectures"></a>MEMHD: Memory-Efficient Multi-Centroid Hyperdimensional Computing for   Fully-Utilized In-Memory Computing Architectures</h2><p><strong>Authors:Do Yeong Kang, Yeong Hwan Oh, Chanwook Hwang, Jinhee Kim, Kang Eun Jeon, Jong Hwan Ko</strong></p>
<p>The implementation of Hyperdimensional Computing (HDC) on In-Memory Computing (IMC) architectures faces significant challenges due to the mismatch between highdimensional vectors and IMC array sizes, leading to inefficient memory utilization and increased computation cycles. This paper presents MEMHD, a Memory-Efficient Multi-centroid HDC framework designed to address these challenges. MEMHD introduces a clustering-based initialization method and quantization aware iterative learning for multi-centroid associative memory. Through these approaches and its overall architecture, MEMHD achieves a significant reduction in memory requirements while maintaining or improving classification accuracy. Our approach achieves full utilization of IMC arrays and enables one-shot (or few-shot) associative search. Experimental results demonstrate that MEMHD outperforms state-of-the-art binary HDC models, achieving up to 13.69% higher accuracy with the same memory usage, or 13.25x more memory efficiency at the same accuracy level. Moreover, MEMHD reduces computation cycles by up to 80x and array usage by up to 71x compared to baseline IMC mapping methods when mapped to 128x128 IMC arrays, while significantly improving energy and computation cycle efficiency. </p>
<blockquote>
<p>将超维计算（HDC）在内存计算（IMC）架构上的实现面临着重大挑战，这是由于高维向量与IMC数组大小之间的不匹配导致的，从而导致了内存利用率低下和计算周期增加。本文针对这些挑战，提出了一种名为MEMHD的高效内存多中心HDC框架。MEMHD引入了一种基于聚类的初始化方法，以及面向多中心关联内存的量化感知迭代学习。通过这些方法及其整体架构，MEMHD在保持或提高分类准确性的同时，显著降低了内存要求。我们的方法实现了IMC阵列的完全利用，并实现了一次（或少数几次）关联搜索。实验结果表明，MEMHD优于最新的二进制HDC模型，在相同内存使用量的情况下，准确率提高了高达13.69%，或在相同准确率水平下，内存使用效率提高了13.25倍。此外，与基准IMC映射方法相比，MEMHD在映射到128x128 IMC阵列时，计算周期减少了高达80倍，阵列使用率减少了高达71倍，同时显著提高了能源和计算周期的效率。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.07834v1">PDF</a> Accepted to appear at DATE 2025</p>
<p><strong>Summary</strong><br>     内存高效的多中心超维计算框架MEMHD解决了高维向量与内存计算阵列大小不匹配的问题，提高了内存利用率并减少了计算周期。通过聚类初始化方法和量化感知迭代学习等方法，MEMHD在维持或提高分类准确率的同时显著降低了内存要求。实验结果显示，MEMHD相较于最先进的全二进制超维计算模型表现出更高的性能，并且在相同的内存使用量下准确率提高了高达13.69%，或在相同准确率水平下内存效率提高了高达13.25倍。此外，相较于基线IMC映射方法，MEMHD在计算周期上减少了高达80倍，在阵列使用上减少了高达71倍。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>MEMHD是一个针对内存计算架构的超维计算框架，解决了高维向量与内存计算阵列大小不匹配的问题。</li>
<li>MEMHD引入聚类初始化方法和量化感知迭代学习技术以提高效率和准确性。</li>
<li>通过这些技术和整体架构，MEMHD显著降低了内存要求，同时维持或提高了分类准确率。</li>
<li>实验结果显示，MEMHD相较于现有技术表现出更高的性能，包括更高的准确率和更高的内存效率。</li>
<li>MEMHD能够实现一次性或少数几次的关联搜索。</li>
<li>与基线IMC映射方法相比，MEMHD在计算周期和阵列使用上实现了显著优化。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.07834">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-80d8acb33bdcaba3f7608b2be4ffc7a9.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-bf84dd9e22b49bf37db007f04a6f1a56.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-7347ab6e2395e0afcc22e76f817f881b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0bbcb8c8557e13edf6e3f3d7390588c5.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-370527c4a43ef79a73bd3c06bd414b65.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-d51c5d2e8a15a6a6801b3cc935c02a08.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="URECA-The-Chain-of-Two-Minimum-Set-Cover-Problems-exists-behind-Adaptation-to-Shifts-in-Semantic-Code-Search"><a href="#URECA-The-Chain-of-Two-Minimum-Set-Cover-Problems-exists-behind-Adaptation-to-Shifts-in-Semantic-Code-Search" class="headerlink" title="URECA: The Chain of Two Minimum Set Cover Problems exists behind   Adaptation to Shifts in Semantic Code Search"></a>URECA: The Chain of Two Minimum Set Cover Problems exists behind   Adaptation to Shifts in Semantic Code Search</h2><p><strong>Authors:Seok-Ung Choi, Joonghyuk Hahn, Yo-Sub Han</strong></p>
<p>Adaptation is to make model learn the patterns shifted from the training distribution. In general, this adaptation is formulated as the minimum entropy problem. However, the minimum entropy problem has inherent limitation – shifted initialization cascade phenomenon. We extend the relationship between the minimum entropy problem and the minimum set cover problem via Lebesgue integral. This extension reveals that internal mechanism of the minimum entropy problem ignores the relationship between disentangled representations, which leads to shifted initialization cascade. From the analysis, we introduce a new clustering algorithm, Union-find based Recursive Clustering Algorithm~(URECA). URECA is an efficient clustering algorithm for the leverage of the relationships between disentangled representations. The update rule of URECA depends on Thresholdly-Updatable Stationary Assumption to dynamics as a released version of Stationary Assumption. This assumption helps URECA to transport disentangled representations with no errors based on the relationships between disentangled representations. URECA also utilize simulation trick to efficiently cluster disentangled representations. The wide range of evaluations show that URECA achieves consistent performance gains for the few-shot adaptation to diverse types of shifts along with advancement to State-of-The-Art performance in CoSQA in the scenario of query shift. </p>
<blockquote>
<p>适应是指使模型学习从训练分布中转移的模式。通常，这种适应被制定为最小熵问题。然而，最小熵问题具有固有的局限性——转移初始化级联现象。我们通过勒贝格积分扩展最小熵问题和最小集覆盖问题之间的关系。这种扩展揭示了最小熵问题的内部机制忽略了解纠缠表示之间的关系，从而导致转移初始化级联。通过分析，我们引入了一种新的聚类算法——基于联合查找的递归聚类算法（URECA）。URECA是一种利用解纠缠表示之间关系的有效聚类算法。URECA的更新规则取决于阈值可更新稳态假设，作为稳态假设的发布版本。这一假设有助于URECA基于解纠缠表示之间的关系无误地传输。URECA还使用仿真技巧有效地聚类解纠缠表示。广泛的评估表明，URECA在少量适应各种类型的变化时实现了性能上的持续增益，并在查询变化的情况下提高了CoSQA的最新性能水平。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.07494v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>模型适应训练分布变化的问题被一般性地表述为最小熵问题，但存在固有的限制——初始化级联现象。本文通过勒贝格积分扩展了最小熵问题和最小集覆盖问题之间的关系，揭示了最小熵问题内在机制忽视了解纠缠表示之间的关系，导致初始化级联。基于此分析，我们提出了一种新的聚类算法——基于并查集的递归聚类算法（URECA）。URECA是一种高效的聚类算法，能够利用解纠缠表示之间的关系。其更新规则依赖于阈值可更新平稳假设，有助于基于解纠缠表示的关系进行无误传输。URECA还采用模拟技巧以有效地对解纠缠表示进行聚类。广泛评估表明，URECA在多种类型的迁移适应中实现了性能提升，并在查询迁移的场景下达到了CoSQA领域的最新技术水平。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>模型适应训练分布变化的问题可表述为最小熵问题，但存在初始化级联现象。</li>
<li>通过勒贝格积分扩展了最小熵问题和最小集覆盖问题之间的关系。</li>
<li>最小熵问题忽视了解纠缠表示之间的关系。</li>
<li>提出了基于并查集的递归聚类算法（URECA），有效利用解纠缠表示之间的关系。</li>
<li>URECA的更新规则依赖于阈值可更新平稳假设，实现无误传输。</li>
<li>URECA采用模拟技巧以有效聚类解纠缠表示。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.07494">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-4509df9a4a52b06f1645f7c64526a35c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-afa1e055dfa31f474f2e5cf5fd543279.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Don’t-Just-Demo-Teach-Me-the-Principles-A-Principle-Based-Multi-Agent-Prompting-Strategy-for-Text-Classification"><a href="#Don’t-Just-Demo-Teach-Me-the-Principles-A-Principle-Based-Multi-Agent-Prompting-Strategy-for-Text-Classification" class="headerlink" title="Don’t Just Demo, Teach Me the Principles: A Principle-Based Multi-Agent   Prompting Strategy for Text Classification"></a>Don’t Just Demo, Teach Me the Principles: A Principle-Based Multi-Agent   Prompting Strategy for Text Classification</h2><p><strong>Authors:Peipei Wei, Dimitris Dimitriadis, Yan Xu, Mingwei Shen</strong></p>
<p>We present PRINCIPLE-BASED PROMPTING, a simple but effective multi-agent prompting strategy for text classification. It first asks multiple LLM agents to independently generate candidate principles based on analysis of demonstration samples with or without labels, consolidates them into final principles via a finalizer agent, and then sends them to a classifier agent to perform downstream classification tasks. Extensive experiments on binary and multi-class classification datasets with different sizes of LLMs show that our approach not only achieves substantial performance gains (1.55% - 19.37%) over zero-shot prompting on macro-F1 score but also outperforms other strong baselines (CoT and stepback prompting). Principles generated by our approach help LLMs perform better on classification tasks than human crafted principles on two private datasets. Our multi-agent PRINCIPLE-BASED PROMPTING approach also shows on-par or better performance compared to demonstration-based few-shot prompting approaches, yet with substantially lower inference costs. Ablation studies show that label information and the multi-agent cooperative LLM framework play an important role in generating high-quality principles to facilitate downstream classification tasks. </p>
<blockquote>
<p>我们提出了基于原则的提示（PRINCIPLE-BASED PROMPTING），这是一种简单有效的多智能体提示策略，用于文本分类。它首先要求多个大型语言模型（LLM）智能体基于有或没有标签的演示样本进行独立分析，生成候选原则，并通过终结者智能体将它们整合为最终原则，然后将其发送到分类器智能体执行下游分类任务。在二元和多类分类数据集上的大量实验，以及不同规模的大型语言模型显示，我们的方法不仅在宏观F1分数上实现了显著的性能提升（1.55%-19.3.3%），而且还优于其他强大的基线（如连贯性提示和退步提示）。我们的方法生成的原则有助于大型语言模型在分类任务上比两个私有数据集上的人工原则表现得更好。我们的多智能体基于原则的提示方法还显示出与基于演示的少量提示方法相当或更好的性能，同时大大降低了推理成本。消融研究表明，标签信息和多智能体合作的大型语言模型框架在生成高质量原则以促进下游分类任务中起着重要作用。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.07165v1">PDF</a> To be published in AAAI 2025 Workshop on Advancing LLM-Based   Multi-Agent Collaboration</p>
<p><strong>Summary</strong></p>
<p>基于原则的多智能体提示策略为文本分类提供了一种简单有效的多智能体提示方法。该策略让多个大型语言模型（LLM）智能体独立生成基于样本分析的原则候选，通过整合器智能体合并成最终原则，并发送给分类器智能体执行下游分类任务。实验证明，该方法在二元及多类别分类数据集上表现优异，实现了可观的性能提升，超越了零射提示方法和其它强大的基线方法。该方法生成的原则有助于LLM在分类任务上超越人类制定的原则。此外，基于多智能体的原则提示方法展现出与基于演示的少量样本提示方法相当或更好的性能，同时大大降低了推理成本。消融研究表明，标签信息和多智能体合作框架在生成高质量原则以促进下游分类任务中起到了重要作用。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>PRINCIPLE-BASED PROMPTING是一种多智能体提示策略，用于文本分类。</li>
<li>该方法通过多个LLM智能体独立生成原则候选，经整合后用于下游分类任务。</li>
<li>实验证明，该方法在多种分类数据集上实现了显著性能提升。</li>
<li>与零射提示和其它基线方法相比，该方法表现更优。</li>
<li>该方法生成的原则有助于LLM在分类任务上超越人类制定的原则。</li>
<li>与基于演示的少量样本提示方法相比，该方法的性能相当或更好，同时降低了推理成本。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.07165">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-ec9253093f51346b9c445ca1d0b66b97.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-469a46aa9405519443472793417bf68b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-22b851e40c5825b83379aac99f7b33b1.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Few-Shot-Multi-Human-Neural-Rendering-Using-Geometry-Constraints"><a href="#Few-Shot-Multi-Human-Neural-Rendering-Using-Geometry-Constraints" class="headerlink" title="Few-Shot Multi-Human Neural Rendering Using Geometry Constraints"></a>Few-Shot Multi-Human Neural Rendering Using Geometry Constraints</h2><p><strong>Authors:Qian li, Victoria Fernàndez Abrevaya, Franck Multon, Adnane Boukhayma</strong></p>
<p>We present a method for recovering the shape and radiance of a scene consisting of multiple people given solely a few images. Multi-human scenes are complex due to additional occlusion and clutter. For single-human settings, existing approaches using implicit neural representations have achieved impressive results that deliver accurate geometry and appearance. However, it remains challenging to extend these methods for estimating multiple humans from sparse views. We propose a neural implicit reconstruction method that addresses the inherent challenges of this task through the following contributions: First, we propose to use geometry constraints by exploiting pre-computed meshes using a human body model (SMPL). Specifically, we regularize the signed distances using the SMPL mesh and leverage bounding boxes for improved rendering. Second, we propose a ray regularization scheme to minimize rendering inconsistencies, and a saturation regularization for robust optimization in variable illumination. Extensive experiments on both real and synthetic datasets demonstrate the benefits of our approach and show state-of-the-art performance against existing neural reconstruction methods. </p>
<blockquote>
<p>我们提出了一种仅通过少量图像恢复由多人组成的场景的形状和辐射亮度的方法。多人场景由于额外的遮挡和杂乱而更加复杂。对于单人设置，使用隐式神经表示方法的现有技术已经取得了令人印象深刻的结果，能够准确提供几何结构和外观。然而，从稀疏视角估计多个行人仍然是一个挑战。我们提出了一种隐式神经重建方法，通过以下贡献来解决此任务固有的挑战：首先，我们通过利用人体模型（SMPL）的预先计算网格提出使用几何约束。具体来说，我们用SMPL网格规则化符号距离并利用边界框进行改进渲染。其次，我们提出了一种光线规则化方案，以最小化渲染的不一致性，并提出了用于可变光照的稳健优化的饱和度规则化方法。在真实和合成数据集上的大量实验证明了我们的方法的好处，与现有的神经重建方法相比，表现出了卓越的性能。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.07140v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文提出一种仅通过少量图像恢复多人场景的形状和辐射度的方法。针对多人物场景额外的遮挡和杂乱带来的复杂性，提出一种神经隐式重建方法，通过利用人体模型（SMPL）的预计算网格实现几何约束，通过正则化有符号距离和利用边界框改进渲染。同时，提出一种射线正则化方案来最小化渲染不一致性，以及一种饱和正则化以在可变照明中实现稳健优化。在真实和合成数据集上的广泛实验证明了该方法的好处并显示出与现有神经重建方法相比的卓越性能。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>提出一种针对多人物场景的神经隐式重建方法，解决从稀疏视角估计多个人的挑战。</li>
<li>利用人体模型（SMPL）的预计算网格实现几何约束。</li>
<li>通过正则化有符号距离和边界框改进渲染。</li>
<li>提出射线正则化方案以最小化渲染不一致性。</li>
<li>饱和正则化用于在可变照明中实现稳健优化。</li>
<li>在真实和合成数据集上的实验证明了该方法的有效性。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.07140">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-a5e6968e17b3ab43f978c5a975722e7b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7e373d66d6607deac5cb21e2ef4c9b1a.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-c0e1ba2d621a9bc66be03badcfc8bb50.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Model-Diffusion-for-Certifiable-Few-shot-Transfer-Learning"><a href="#Model-Diffusion-for-Certifiable-Few-shot-Transfer-Learning" class="headerlink" title="Model Diffusion for Certifiable Few-shot Transfer Learning"></a>Model Diffusion for Certifiable Few-shot Transfer Learning</h2><p><strong>Authors:Fady Rezk, Royson Lee, Henry Gouk, Timothy Hospedales, Minyoung Kim</strong></p>
<p>In modern large-scale deep learning, a prevalent and effective workflow for solving low-data problems is adapting powerful pre-trained foundation models (FMs) to new tasks via parameter-efficient fine-tuning (PEFT). However, while empirically effective, the resulting solutions lack generalisation guarantees to certify their accuracy - which may be required for ethical or legal reasons prior to deployment in high-importance applications. In this paper we develop a novel transfer learning approach that is designed to facilitate non-vacuous learning theoretic generalisation guarantees for downstream tasks, even in the low-shot regime. Specifically, we first use upstream tasks to train a distribution over PEFT parameters. We then learn the downstream task by a sample-and-evaluate procedure – sampling plausible PEFTs from the trained diffusion model and selecting the one with the highest likelihood on the downstream data. Crucially, this confines our model hypothesis to a finite set of PEFT samples. In contrast to learning in the typical continuous hypothesis spaces of neural network weights, this facilitates tighter risk certificates. We instantiate our bound and show non-trivial generalization guarantees compared to existing learning approaches which lead to vacuous bounds in the low-shot regime. </p>
<blockquote>
<p>在现代大规模深度学习领域，针对低数据问题的一种流行且有效的工作流程是通过参数有效微调（PEFT）将强大的预训练基础模型（FMs）适应到新任务中。然而，尽管在经验上有效，但所得解决方案缺乏泛化保证来证明其准确性，这可能会在部署到高重要性应用之前出于道德或法律原因而要求证明。在本文中，我们开发了一种新型迁移学习方法，旨在促进下游任务的非空洞学习理论泛化保证，即使在低射击情况下也是如此。具体来说，我们首先使用上游任务来训练PEFT参数的分布。然后，我们通过采样和评估程序来学习下游任务——从训练的扩散模型中采样合理的PEFTs，并选择在下游数据上可能性最高的一个。关键的是，这将我们的模型假设限制在PEFT样本的有限集合中。与在神经网络权重的典型连续假设空间中学习不同，这有助于获得更严格的风险证书。我们实例化我们的界限，并显示出与非空洞泛化保证的现有学习方法相比，我们在低射击情况下具有非空洞的泛化保证。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.06970v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>预训练模型通过参数高效微调（PEFT）适应新任务，是现代大规模深度学习解决低数据问题的有效方法。然而，尽管这种方法在实践中有效，但缺乏泛化保证来验证其在高优先级应用部署前的准确性。本文提出了一种新型迁移学习方法，旨在促进下游任务的非空洞学习理论泛化保证，即使在低资源情况下也是如此。通过上游任务训练一个PEFT参数的分布，然后通过采样和评估程序学习下游任务，从训练的扩散模型中采样可能的PEFTs，并选择在下游数据上可能性最高的一个。通过将模型假设限制在PEFT样本的有限集合中，与传统的神经网络权重连续假设空间学习相比，这有助于获得更严格的风险证书。我们实例化了我们的界限，与现有学习方法相比，在低资源情况下提供了非空洞的泛化保证。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>现代深度学习中的低数据问题可以通过预训练模型的参数高效微调（PEFT）解决。</li>
<li>尽管PEFT方法在实践中有效，但在高优先级应用部署前需要泛化保证来验证其准确性。</li>
<li>本文提出了一种新型迁移学习方法，旨在促进下游任务的非空洞学习理论泛化保证。</li>
<li>方法通过上游任务训练一个关于PEFT参数的分布。</li>
<li>通过采样和评估程序学习下游任务，采样可能的PEFTs并从扩散模型中选出最佳的一个。</li>
<li>将模型假设限制在PEFT样本的有限集合中，有助于获得更严格的风险证书。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.06970">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-dceef0931cbc4df5ae2bc4c6c66e4d86.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6ab0a2857fde201b5d5a68d5a19ae3de.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3e6d2c391ed4dd4331466aec01ee1776.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-23d65ea662ac8dc68d60342b4d591757.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Exploring-Few-Shot-Defect-Segmentation-in-General-Industrial-Scenarios-with-Metric-Learning-and-Vision-Foundation-Models"><a href="#Exploring-Few-Shot-Defect-Segmentation-in-General-Industrial-Scenarios-with-Metric-Learning-and-Vision-Foundation-Models" class="headerlink" title="Exploring Few-Shot Defect Segmentation in General Industrial Scenarios   with Metric Learning and Vision Foundation Models"></a>Exploring Few-Shot Defect Segmentation in General Industrial Scenarios   with Metric Learning and Vision Foundation Models</h2><p><strong>Authors:Tongkun Liu, Bing Li, Xiao Jin, Yupeng Shi, Qiuying Li, Xiang Wei</strong></p>
<p>Industrial defect segmentation is critical for manufacturing quality control. Due to the scarcity of training defect samples, few-shot semantic segmentation (FSS) holds significant value in this field. However, existing studies mostly apply FSS to tackle defects on simple textures, without considering more diverse scenarios. This paper aims to address this gap by exploring FSS in broader industrial products with various defect types. To this end, we contribute a new real-world dataset and reorganize some existing datasets to build a more comprehensive few-shot defect segmentation (FDS) benchmark. On this benchmark, we thoroughly investigate metric learning-based FSS methods, including those based on meta-learning and those based on Vision Foundation Models (VFMs). We observe that existing meta-learning-based methods are generally not well-suited for this task, while VFMs hold great potential. We further systematically study the applicability of various VFMs in this task, involving two paradigms: feature matching and the use of Segment Anything (SAM) models. We propose a novel efficient FDS method based on feature matching. Meanwhile, we find that SAM2 is particularly effective for addressing FDS through its video track mode. The contributed dataset and code will be available at: <a target="_blank" rel="noopener" href="https://github.com/liutongkun/GFDS">https://github.com/liutongkun/GFDS</a>. </p>
<blockquote>
<p>工业缺陷分割对于制造质量控制至关重要。由于训练缺陷样本的稀缺性，小样本语义分割（FSS）在该领域具有重大意义。然而，现有的研究大多将FSS应用于简单纹理上的缺陷检测，并未考虑更多样化的场景。本文旨在通过探索FSS在更广泛的工业产品中的多种缺陷类型来解决这一差距。为此，我们贡献了一个新的真实世界数据集，并重组了一些现有数据集以建立更全面的少数缺陷分割（FDS）基准测试。在这个基准测试上，我们深入研究了基于度量学习的FSS方法，包括基于元学习和基于视觉基础模型（VFMs）的方法。我们发现现有的基于元学习的方法通常不适合这项任务，而VFMs具有巨大潜力。我们进一步系统地研究了各种VFMs在此任务中的应用，涉及特征匹配和使用Segment Anything（SAM）模型的两种范式。我们提出了一种基于特征匹配的高效FDS方法。同时，我们发现SAM2通过其视频跟踪模式在解决FDS问题时特别有效。相关数据集和代码将在<a target="_blank" rel="noopener" href="https://github.com/liutongkun/GFDS%E4%B8%8A%E6%8F%90%E4%BE%9B%E3%80%82">https://github.com/liutongkun/GFDS上提供。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.01216v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文关注工业缺陷分割领域中的小样本语义分割（FSS）问题。针对现有研究主要集中在简单纹理缺陷上，本文旨在探索FSS在更广泛的工业产品中的多样场景应用，并为此贡献新的真实世界数据集和构建全面的少样本缺陷分割（FDS）基准测试集。通过对基于度量学习的FSS方法进行深入研究，发现基于元学习的方法不适用于此任务，而基于视觉基础模型（VFMs）的方法具有潜力。同时系统地研究了各种VFMs在此任务中的应用，并基于特征匹配提出了一种高效的FDS方法。此外，发现SAM模型中的视频跟踪模式对于解决FDS特别有效。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>工业缺陷分割中，小样本语义分割（FSS）具有重要价值，现有研究主要集中在简单纹理缺陷上，本文旨在拓宽其应用范围。</li>
<li>贡献了新的真实世界数据集和构建了全面的少样本缺陷分割（FDS）基准测试集。</li>
<li>基于度量学习的FSS方法被深入研究，发现基于元学习的方法不适用于此任务。</li>
<li>视觉基础模型（VFMs）在此任务中展现出潜力。</li>
<li>针对不同VFMs的应用进行了系统研究，包括特征匹配和Segment Anything（SAM）模型。</li>
<li>提出了一种基于特征匹配的高效的FDS方法。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.01216">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-ea9a3135a407f73637fa36ba0c7a024d.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-99d8a384eefdee79192c00ca6e1df719.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-311f2e2c682c470e30a72c9bdad79bd6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-af72cc79e3660d0f7d4d34c818820ef4.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-8167fb787c69aaf5d66852400b7e2063.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1fb3494d0b537e648abfde399e6484f2.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Holistic-Semantic-Representation-for-Navigational-Trajectory-Generation"><a href="#Holistic-Semantic-Representation-for-Navigational-Trajectory-Generation" class="headerlink" title="Holistic Semantic Representation for Navigational Trajectory Generation"></a>Holistic Semantic Representation for Navigational Trajectory Generation</h2><p><strong>Authors:Ji Cao, Tongya Zheng, Qinghong Guo, Yu Wang, Junshu Dai, Shunyu Liu, Jie Yang, Jie Song, Mingli Song</strong></p>
<p>Trajectory generation has garnered significant attention from researchers in the field of spatio-temporal analysis, as it can generate substantial synthesized human mobility trajectories that enhance user privacy and alleviate data scarcity. However, existing trajectory generation methods often focus on improving trajectory generation quality from a singular perspective, lacking a comprehensive semantic understanding across various scales. Consequently, we are inspired to develop a HOlistic SEmantic Representation (HOSER) framework for navigational trajectory generation. Given an origin-and-destination (OD) pair and the starting time point of a latent trajectory, we first propose a Road Network Encoder to expand the receptive field of road- and zone-level semantics. Second, we design a Multi-Granularity Trajectory Encoder to integrate the spatio-temporal semantics of the generated trajectory at both the point and trajectory levels. Finally, we employ a Destination-Oriented Navigator to seamlessly integrate destination-oriented guidance. Extensive experiments on three real-world datasets demonstrate that HOSER outperforms state-of-the-art baselines by a significant margin. Moreover, the model’s performance in few-shot learning and zero-shot learning scenarios further verifies the effectiveness of our holistic semantic representation. </p>
<blockquote>
<p>轨迹生成已引起时空分析领域研究人员的广泛关注，因为它可以生成大量合成的人类移动轨迹，增强用户隐私并缓解数据稀缺问题。然而，现有的轨迹生成方法往往从单一角度着眼于提高轨迹生成质量，缺乏跨不同尺度的全面语义理解。因此，我们受到启发，开发了一个用于导航轨迹生成的HOlistic SEmantic Representation（HOSER）框架。给定起点和终点（OD）对以及潜在轨迹的起始时间点，我们首先提出一个道路网络编码器，以扩大道路和区域级别的语义感受野。其次，我们设计了一个多粒度轨迹编码器，以整合生成轨迹在点和轨迹两个级别的时空语义。最后，我们采用目的导向的导航器，无缝集成以目的地为中心的导航。在三个真实世界数据集上的大量实验表明，HOSER显著优于最新基线。此外，该模型在少样本学习和零样本学习场景中的表现进一步验证了我们的整体语义表示的有效性。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.02737v2">PDF</a> Accepted by AAAI 2025</p>
<p><strong>Summary</strong></p>
<p>本文提出了一种全新的导航轨迹生成框架HOSER（HOlistic SEmantic Representation），该框架结合了道路网络编码器和多粒度轨迹编码器，旨在全面理解不同尺度的语义信息。通过引入目的地导向导航器，生成更符合人类实际驾驶行为的轨迹。在三个真实世界数据集上的实验证明，HOSER相较于其他最先进的方法有着显著优势，并在少样本学习和零样本学习场景中证明了其效能。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>HOSER框架结合了道路网络编码器和多粒度轨迹编码器，旨在全面理解不同尺度的语义信息。</li>
<li>通过引入道路网络编码器，扩大对道路和区域级别语义的感受野。</li>
<li>多粒度轨迹编码器能够整合生成的轨迹在点和轨迹两个层次上的时空语义。</li>
<li>目的地导向导航器的引入使得生成的轨迹更符合人类实际驾驶行为。</li>
<li>在三个真实世界数据集上的实验证明HOSER相较于其他方法具有显著优势。</li>
<li>HOSER在少样本学习场景中表现出良好的性能。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.02737">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-80424f36c8c9e99884d068cc5ac0f1be.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-56642d1816dcc57e2c7cda260022ddef.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-24ca6454840c08c68ce4aa5865168bbf.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-589977983682146fe063ebdbe9fb5859.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-aecd38687e7bdf493cb0e2ed03b3b86c.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="All-You-Need-in-Knowledge-Distillation-Is-a-Tailored-Coordinate-System"><a href="#All-You-Need-in-Knowledge-Distillation-Is-a-Tailored-Coordinate-System" class="headerlink" title="All You Need in Knowledge Distillation Is a Tailored Coordinate System"></a>All You Need in Knowledge Distillation Is a Tailored Coordinate System</h2><p><strong>Authors:Junjie Zhou, Ke Zhu, Jianxin Wu</strong></p>
<p>Knowledge Distillation (KD) is essential in transferring dark knowledge from a large teacher to a small student network, such that the student can be much more efficient than the teacher but with comparable accuracy. Existing KD methods, however, rely on a large teacher trained specifically for the target task, which is both very inflexible and inefficient. In this paper, we argue that a SSL-pretrained model can effectively act as the teacher and its dark knowledge can be captured by the coordinate system or linear subspace where the features lie in. We then need only one forward pass of the teacher, and then tailor the coordinate system (TCS) for the student network. Our TCS method is teacher-free and applies to diverse architectures, works well for KD and practical few-shot learning, and allows cross-architecture distillation with large capacity gap. Experiments show that TCS achieves significantly higher accuracy than state-of-the-art KD methods, while only requiring roughly half of their training time and GPU memory costs. </p>
<blockquote>
<p>知识蒸馏（KD）对于将从大型教师网络转移到小型学生网络中的暗知识至关重要，这样学生可以比教师更有效率，但具有相当的准确性。然而，现有的KD方法依赖于专门为目标任务训练的大型教师，这既非常不灵活又效率低下。在本文中，我们认为SSL预训练模型可以有效地作为教师，其暗知识可以通过特征所在的坐标系或线性子空间来捕获。然后我们只需要教师的一次前向传递，然后为学生网络定制坐标系（TCS）。我们的TCS方法无需教师，适用于各种架构，对于KD和实用的少样本学习效果很好，并允许具有大容量差距的跨架构蒸馏。实验表明，TCS方法在准确率上明显优于最新KD方法，同时只需大致减半的训练时间和GPU内存成本。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.09388v2">PDF</a> Accepted by AAAI 2025</p>
<p><strong>摘要</strong></p>
<p>本文介绍了知识蒸馏（KD）在将大型教师网络的暗知识转移到小型学生网络中的重要性，使得学生网络可以在效率上比教师网络更高，同时保持相当的准确性。然而，现有的KD方法依赖于针对目标任务专门训练的大型教师网络，这既非常不灵活又效率低下。本文主张使用SSL预训练模型作为教师网络，通过特征所在的坐标系或线性子空间捕获其暗知识。只需教师网络的一次前向传递，然后为学生网络定制坐标系（TCS）。我们的TCS方法无需教师网络，适用于各种架构，对于KD和实际少样本学习效果很好，并允许跨架构蒸馏，具有较大的容量差距。实验表明，TCS实现了比现有KD方法更高的准确性，同时仅需它们大约一半的训练时间和GPU内存成本。</p>
<p><strong>关键见解</strong></p>
<ol>
<li>知识蒸馏（KD）能有效将大型教师网络的暗知识转移至小型学生网络。</li>
<li>现有KD方法依赖专门训练的教师网络，缺乏灵活性和效率。</li>
<li>SSL预训练模型可有效地作为教师网络。</li>
<li>通过特征所在的坐标系或线性子空间捕获教师网络的暗知识。</li>
<li>只需教师网络一次前向传递，定制坐标系（TCS）为学生网络。</li>
<li>TCS方法无需教师网络，适用于多种架构。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.09388">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-fc179e2864a57ed1e9a272a6e194b58b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-2851a5433a84297e669cc17eb1804b2b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b76fbf49d3c102fb870666a0516c5ab6.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e2ee85537b0f8a2b0ed8e3bcdc34c628.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-829e0ef8897d47c82e9fc676aa5814aa.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ff79d4a13f99916760db8a42c05d75cd.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="Observe-Then-Act-Asynchronous-Active-Vision-Action-Model-for-Robotic-Manipulation"><a href="#Observe-Then-Act-Asynchronous-Active-Vision-Action-Model-for-Robotic-Manipulation" class="headerlink" title="Observe Then Act: Asynchronous Active Vision-Action Model for Robotic   Manipulation"></a>Observe Then Act: Asynchronous Active Vision-Action Model for Robotic   Manipulation</h2><p><strong>Authors:Guokang Wang, Hang Li, Shuyuan Zhang, Di Guo, Yanhong Liu, Huaping Liu</strong></p>
<p>In real-world scenarios, many robotic manipulation tasks are hindered by occlusions and limited fields of view, posing significant challenges for passive observation-based models that rely on fixed or wrist-mounted cameras. In this paper, we investigate the problem of robotic manipulation under limited visual observation and propose a task-driven asynchronous active vision-action model.Our model serially connects a camera Next-Best-View (NBV) policy with a gripper Next-Best Pose (NBP) policy, and trains them in a sensor-motor coordination framework using few-shot reinforcement learning. This approach allows the agent to adjust a third-person camera to actively observe the environment based on the task goal, and subsequently infer the appropriate manipulation actions.We trained and evaluated our model on 8 viewpoint-constrained tasks in RLBench. The results demonstrate that our model consistently outperforms baseline algorithms, showcasing its effectiveness in handling visual constraints in manipulation tasks. </p>
<blockquote>
<p>在真实场景中，许多机器人操作任务受到遮挡和视野有限的阻碍，这给依赖固定或手腕式相机的基于被动观察模型带来了巨大的挑战。在本文中，我们研究了在有限视觉观察下的机器人操作问题，并提出了一种任务驱动的异步主动视觉动作模型。我们的模型将相机的下一个最佳视图（NBV）策略与夹具的下一个最佳姿态（NBP）策略串联起来，在一个传感器-电机协调框架中使用少量的强化学习进行训练。这种方法允许智能体根据任务目标主动调整第三人称相机的视角来观察环境，然后推断适当的操作动作。我们在RLBench的8个视点约束任务上训练和评估了我们的模型。结果表明，我们的模型始终优于基线算法，展示了在处理操作任务中的视觉约束方面的有效性。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2409.14891v3">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文研究了在有限视觉观察下的机器人操作问题，并提出了一种任务驱动的异步主动视觉动作模型。该模型通过串联相机最佳视角（NBV）政策和抓手最佳姿态（NBP）政策，在传感器-电机协调框架中使用小样本强化学习进行训练。该方法使机器人能够基于任务目标主动调整第三人称相机观察环境，并据此推断适当的操作动作。在RLBench的8个视角受限任务中，该模型的结果表明其在处理操作任务中的视觉约束方面始终优于基准算法。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>机器人操作任务在实际场景中常常受到遮挡和视野限制的挑战。</li>
<li>本文提出了一种任务驱动的异步主动视觉动作模型，解决有限视觉观察下的机器人操作问题。</li>
<li>模型通过串联相机最佳视角（NBV）政策和抓手最佳姿态（NBP）政策，以应对视角和操作约束。</li>
<li>该模型在传感器-电机协调框架中使用小样本强化学习进行训练。</li>
<li>机器人能够基于任务目标主动调整相机观察环境。</li>
<li>模型在RLBench的多个视角受限任务中进行了训练和评估，结果优于基准算法。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2409.14891">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-98feafc1c51d6658157a01eb7de228c8.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-039cdd79b2b8a595052ec048e159ddc6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-63003be4f18381d8fe99a01328fe6c01.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-261c40f8a19841abc7a4e032f114d0c1.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-aa866b6d69fd6cd15af3f508fabcbfb0.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-8dc9662a143f04bb19966e0decb61029.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-fb8fdb02018337ef69d02ff5f14eb543.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="Self-Harmonized-Chain-of-Thought"><a href="#Self-Harmonized-Chain-of-Thought" class="headerlink" title="Self-Harmonized Chain of Thought"></a>Self-Harmonized Chain of Thought</h2><p><strong>Authors:Ziqi Jin, Wei Lu</strong></p>
<p>Chain-of-thought (CoT) prompting has demonstrated the capacity of large language models to perform complex reasoning through intermediate steps. While effective, current CoT methods face challenges: Zero-shot-CoT can lead to reasoning errors, and Few-shot-CoT requires labor-intensive manual demonstrations. Auto-CoT attempts to address these issues by automatically generating diverse demonstrations, but this diversity can lead to inconsistent reasoning patterns. We propose ECHO (Self-Harmonized Chain of Thought), a novel method that unifies diverse solution paths into a consistent and effective reasoning pattern. ECHO employs an iterative process to refine and harmonize automatically generated demonstrations, mitigating the limitations of existing approaches. Our comprehensive experiments across arithmetic, commonsense, and symbolic reasoning tasks demonstrate that ECHO outperforms Auto-CoT by an average of 2.8%. These findings suggest that ECHO represents a significant step towards more robust and generalizable automated reasoning in large language models. </p>
<blockquote>
<p>思维链（CoT）提示展现了大型语言模型通过中间步骤进行复杂推理的能力。虽然现有CoT方法有效，但面临挑战：零镜头CoT可能导致推理错误，而少镜头CoT需要繁琐的手动演示。Auto-CoT试图通过自动生成多样化的演示来解决这些问题，但这种多样性可能导致推理模式的不一致。我们提出了ECHO（自我协调思维链），这是一种将多样化解决方案路径统一为一致有效推理模式的新方法。ECHO采用迭代过程来优化和协调自动生成的演示，缓解了现有方法的局限性。我们在算术、常识和符号推理任务上的综合实验表明，ECHO的平均性能优于Auto-CoT 2.8%。这些发现表明，ECHO在大型语言模型中实现更稳健和可推广的自动化推理方面迈出了重要一步。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2409.04057v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>大型语言模型的链式思维（CoT）提示展现出了执行复杂推理的能力，通过中间步骤进行。尽管现有CoT方法有效，但仍面临挑战：零步CoT可能导致推理错误，而少步CoT需要繁琐的手动演示。自动CoT试图通过自动生成多样的演示来解决这些问题，但其多样性可能导致推理模式的不一致。本文提出一种名为ECHO（自我协调的链式思维）的新方法，它将不同的解决方案路径统一为一个一致且有效的推理模式。ECHO采用迭代过程来优化和协调自动生成的演示，缓解了现有方法的局限性。在算术、常识和符号推理任务上的综合实验表明，ECHO的平均性能比自动CoT提高了2.8%。这表明ECHO在朝着大型语言模型中更稳健和更通用的自动化推理迈出了重要一步。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>链式思维（CoT）提示大型语言模型执行复杂推理的中间步骤能力。</li>
<li>当前CoT方法面临推理错误和需要繁琐手动演示的问题。</li>
<li>自动CoT方法试图通过自动生成多样的演示来解决这些问题，但可能导致推理模式的不一致。</li>
<li>ECHO（自我协调的链式思维）提出一种将不同解决方案路径统一为一致且有效推理模式的新方法。</li>
<li>ECHO采用迭代过程优化和协调自动生成的演示。</li>
<li>在多个任务上的实验表明，ECHO的性能优于自动CoT。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2409.04057">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-52f615c04b189b74241460464ca4ef67.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a38b8415d4b737fce0ce40422b6ac02d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-4898fb4f2c26baadec0ae3ba02bc35bc.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-1591e92d90782abd2ad4144391de8f73.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        文章作者:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        文章链接:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-02-13/Few-Shot/">https://kedreamix.github.io/Talk2Paper/Paper/2025-02-13/Few-Shot/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        版权声明:
                    </i>
                </span>
                <span class="reprint-info">
                    本博客所有文章除特別声明外，均采用
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    许可协议。转载请注明来源
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Few-Shot/">
                                    <span class="chip bg-color">Few-Shot</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;上一篇</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-02-13/I2I%20Translation/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-42702818ec49ca577b43d88be668d7aa.jpg" class="responsive-img" alt="I2I Translation">
                        
                        <span class="card-title">I2I Translation</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            I2I Translation 方向最新论文已更新，请持续关注 Update in 2025-02-13  Demonstration of Fourier-domain Quantum Optical Coherence Tomography for   a fast tomographic quantum imaging
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-02-13
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/I2I-Translation/" class="post-category">
                                    I2I Translation
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/I2I-Translation/">
                        <span class="chip bg-color">I2I Translation</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                下一篇&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-02-13/Agent/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-04fc13813f6549fb5d8d50b7fb684543.jpg" class="responsive-img" alt="Agent">
                        
                        <span class="card-title">Agent</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Agent 方向最新论文已更新，请持续关注 Update in 2025-02-13  Learning in Markets with Heterogeneous Agents Dynamics and Survival of   Bayesian vs. No-Regret Learners
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-02-13
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Agent/" class="post-category">
                                    Agent
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Agent/">
                        <span class="chip bg-color">Agent</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- 代码块功能依赖 -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- 代码语言 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- 代码块复制 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- 代码块收缩 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC 悬浮按钮. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* 修复文章卡片 div 的宽度. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // 切换TOC目录展开收缩的相关操作.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;站点总字数:&nbsp;<span
                        class="white-color">14918.2k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- 运行天数提醒. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // 区分是否有年份.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = '本站已运行 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                daysTip = '本站已運行 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = '本站已运行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = '本站已運行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="关注我的知乎: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">知</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- 搜索遮罩框 -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span>
            <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- 白天和黑夜主题 -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- 回到顶部按钮 -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // 只在桌面版网页启用特效
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- 雪花特效 -->
    

    <!-- 鼠标星星特效 -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--腾讯兔小巢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- 动态标签 -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Σ(っ °Д °;)っ诶，页面崩溃了嘛？", clearTimeout(st)) : (document.title = "φ(゜▽゜*)♪咦，又好了！", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
