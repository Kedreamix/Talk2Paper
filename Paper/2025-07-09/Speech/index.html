<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Speech">
    <meta name="description" content="Speech æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-07-09  Grounded Gesture Generation Language, Motion, and Space">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Speech | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pic1.zhimg.com/v2-ad174257bce8b6b417541d9d214da64e.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Speech</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Speech/">
                                <span class="chip bg-color">Speech</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Speech/" class="post-category">
                                Speech
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-07-09
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-07-17
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    8.4k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    34 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-07-09-æ›´æ–°"><a href="#2025-07-09-æ›´æ–°" class="headerlink" title="2025-07-09 æ›´æ–°"></a>2025-07-09 æ›´æ–°</h1><h2 id="Grounded-Gesture-Generation-Language-Motion-and-Space"><a href="#Grounded-Gesture-Generation-Language-Motion-and-Space" class="headerlink" title="Grounded Gesture Generation: Language, Motion, and Space"></a>Grounded Gesture Generation: Language, Motion, and Space</h2><p><strong>Authors:Anna Deichler, Jim Oâ€™Regan, Teo Guichoux, David Johansson, Jonas Beskow</strong></p>
<p>Human motion generation has advanced rapidly in recent years, yet the critical problem of creating spatially grounded, context-aware gestures has been largely overlooked. Existing models typically specialize either in descriptive motion generation, such as locomotion and object interaction, or in isolated co-speech gesture synthesis aligned with utterance semantics. However, both lines of work often treat motion and environmental grounding separately, limiting advances toward embodied, communicative agents. To address this gap, our work introduces a multimodal dataset and framework for grounded gesture generation, combining two key resources: (1) a synthetic dataset of spatially grounded referential gestures, and (2) MM-Conv, a VR-based dataset capturing two-party dialogues. Together, they provide over 7.7 hours of synchronized motion, speech, and 3D scene information, standardized in the HumanML3D format. Our framework further connects to a physics-based simulator, enabling synthetic data generation and situated evaluation. By bridging gesture modeling and spatial grounding, our contribution establishes a foundation for advancing research in situated gesture generation and grounded multimodal interaction.   Project page: <a target="_blank" rel="noopener" href="https://groundedgestures.github.io/">https://groundedgestures.github.io/</a> </p>
<blockquote>
<p>äººç±»åŠ¨ä½œç”ŸæˆæŠ€æœ¯åœ¨è¿‘å¹´æ¥å·²ç»è¿…é€Ÿå‘å±•ï¼Œä½†åˆ›å»ºå…·æœ‰ç©ºé—´æ€§å’Œä¸Šä¸‹æ–‡æ„ŸçŸ¥çš„æ‰‹åŠ¿è¿™ä¸€å…³é”®é—®é¢˜å´è¢«å¤§å¤§å¿½è§†äº†ã€‚ç°æœ‰æ¨¡å‹é€šå¸¸ä¸“æ³¨äºæè¿°æ€§åŠ¨ä½œç”Ÿæˆï¼Œå¦‚è¡Œèµ°å’Œå¯¹è±¡äº¤äº’ï¼Œæˆ–åœ¨ä¸è¯è¯­è¯­ä¹‰å¯¹é½çš„å­¤ç«‹è¯­éŸ³æ‰‹åŠ¿åˆæˆä¸­ã€‚ç„¶è€Œï¼Œè¿™ä¸¤ç±»å·¥ä½œé€šå¸¸å°†åŠ¨ä½œå’Œç¯å¢ƒåŸºç¡€åˆ†å¼€å¤„ç†ï¼Œé™åˆ¶äº†å‘ç€å…·ä½“åŒ–ã€äº¤é™…æ€§ä»£ç†çš„å‘å±•ã€‚ä¸ºäº†å¼¥è¡¥è¿™ä¸€ç©ºç™½ï¼Œæˆ‘ä»¬çš„å·¥ä½œå¼•å…¥äº†åŸºäºåŸºç¡€æ‰‹åŠ¿ç”Ÿæˆçš„å¤šæ¨¡æ€æ•°æ®é›†å’Œæ¡†æ¶ï¼Œç»“åˆäº†ä¸¤ç§å…³é”®èµ„æºï¼šï¼ˆ1ï¼‰ç©ºé—´åŸºç¡€å‚ç…§æ‰‹åŠ¿çš„åˆæˆæ•°æ®é›†ï¼Œï¼ˆ2ï¼‰åŸºäºVRçš„MM-Convæ•°æ®é›†ï¼Œæ•æ‰åŒæ–¹å¯¹è¯ã€‚å®ƒä»¬å…±åŒæä¾›äº†è¶…è¿‡7.7å°æ—¶åŒæ­¥çš„åŠ¨ä½œã€è¯­éŸ³å’Œ3Dåœºæ™¯ä¿¡æ¯ï¼Œä»¥HumanML3Dæ ¼å¼æ ‡å‡†åŒ–ã€‚æˆ‘ä»¬çš„æ¡†æ¶è¿›ä¸€æ­¥ä¸åŸºäºç‰©ç†çš„æ¨¡æ‹Ÿå™¨ç›¸è¿ï¼Œèƒ½å¤Ÿå®ç°åˆæˆæ•°æ®çš„ç”Ÿæˆå’Œæƒ…å¢ƒè¯„ä¼°ã€‚é€šè¿‡æ¡¥æ¥æ‰‹åŠ¿å»ºæ¨¡å’Œç©ºé—´åŸºç¡€ï¼Œæˆ‘ä»¬çš„è´¡çŒ®ä¸ºæƒ…å¢ƒæ‰‹åŠ¿ç”Ÿæˆå’ŒåŸºç¡€å¤šæ¨¡å¼äº¤äº’çš„ç ”ç©¶å¥ å®šäº†åŸºç¡€ã€‚é¡¹ç›®é¡µé¢ï¼š<a target="_blank" rel="noopener" href="https://groundedgestures.github.io/">https://groundedgestures.github.io/</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.04522v1">PDF</a> Accepted as a non-archival paper at the CVPR 2025 Humanoid Agents   Workshop. Project page: <a target="_blank" rel="noopener" href="https://groundedgestures.github.io/">https://groundedgestures.github.io</a></p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡å…³æ³¨äººç±»åŠ¨ä½œç”Ÿæˆé¢†åŸŸä¸­çš„ç©ºé—´èƒŒæ™¯ä¸è¯­å¢ƒæ„ŸçŸ¥æ‰‹åŠ¿çš„ç”Ÿæˆé—®é¢˜ã€‚ç°æœ‰æ¨¡å‹ä¸»è¦ä¸“æ³¨äºæè¿°æ€§åŠ¨ä½œç”Ÿæˆæˆ–ä¸è¯­è¨€è¯­ä¹‰å¯¹é½çš„å­¤ç«‹æ€§å…±è¯­æ‰‹åŠ¿åˆæˆï¼Œä½†å¾€å¾€å°†åŠ¨ä½œä¸ç¯å¢ƒèƒŒæ™¯åˆ†ç¦»å¤„ç†ï¼Œé˜»ç¢äº†èº«ä½“æ€§æ²Ÿé€šä»£ç†çš„å‘å±•ã€‚æœ¬æ–‡æå‡ºä¸€ä¸ªç»¼åˆæ•°æ®é›†ä¸æ¡†æ¶æ¥è§£å†³è¿™ä¸€é—®é¢˜ï¼ŒåŒ…å«åˆæˆçš„æ‰‹åŠ¿å‚è€ƒæ•°æ®é›†å’ŒåŸºäºVRçš„åŒå‘å¯¹è¯æ•°æ®é›†ï¼Œæä¾›è¶…è¿‡7.7å°æ—¶çš„åŒæ­¥åŠ¨ä½œã€è¯­éŸ³å’Œ3Dåœºæ™¯ä¿¡æ¯ã€‚æ­¤å¤–ï¼Œè¯¥æ¡†æ¶ä¸ç‰©ç†æ¨¡æ‹Ÿå™¨è¿æ¥ï¼Œä¿ƒè¿›åˆæˆæ•°æ®çš„ç”Ÿæˆå’Œæƒ…å¢ƒè¯„ä¼°ã€‚é€šè¿‡è¿æ¥æ‰‹åŠ¿å»ºæ¨¡å’Œç©ºé—´èƒŒæ™¯ï¼Œæœ¬æ–‡å»ºç«‹äº†ä¸€ä¸ªç ”ç©¶æƒ…å¢ƒæ‰‹åŠ¿ç”Ÿæˆå’ŒåŸºäºèƒŒæ™¯çš„å¤šæ¨¡æ€äº¤äº’çš„åŸºç¡€ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>äººç±»åŠ¨ä½œç”Ÿæˆé¢†åŸŸå­˜åœ¨ç©ºé—´èƒŒæ™¯ä¸è¯­å¢ƒæ„ŸçŸ¥æ‰‹åŠ¿ç”Ÿæˆçš„ä¸è¶³ã€‚</li>
<li>ç°æœ‰æ¨¡å‹åœ¨åŠ¨ä½œä¸ç¯å¢ƒèƒŒæ™¯çš„å¤„ç†ä¸Šå­˜åœ¨åˆ†ç¦»ç°è±¡ï¼Œé™åˆ¶äº†è¿›å±•ã€‚</li>
<li>æœ¬æ–‡æå‡ºä¸€ä¸ªç»¼åˆæ•°æ®é›†ç”¨äºè§£å†³è¿™ä¸€é—®é¢˜ï¼ŒåŒ…å«åˆæˆæ‰‹åŠ¿å‚è€ƒæ•°æ®é›†å’ŒåŸºäºVRçš„åŒå‘å¯¹è¯æ•°æ®é›†ã€‚</li>
<li>æ•°æ®é›†æä¾›äº†è¶…è¿‡7.7å°æ—¶çš„åŒæ­¥åŠ¨ä½œã€è¯­éŸ³å’Œ3Dåœºæ™¯ä¿¡æ¯ã€‚</li>
<li>æå‡ºçš„æ•°æ®é›†é‡‡ç”¨HumanML3Dæ ¼å¼æ ‡å‡†åŒ–ï¼Œæ”¯æŒæƒ…å¢ƒè¯„ä¼°å’Œåˆæˆæ•°æ®ç”Ÿæˆã€‚</li>
<li>é€šè¿‡è¿æ¥æ‰‹åŠ¿å»ºæ¨¡å’Œç©ºé—´èƒŒæ™¯ï¼Œä¸ºæƒ…å¢ƒæ‰‹åŠ¿ç”Ÿæˆå’Œå¤šæ¨¡æ€äº¤äº’ç ”ç©¶å¥ å®šåŸºç¡€ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.04522">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-40d422cc5bd53ccacae648baac8a9b4e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-3f56b83248c7ad3438a616576cbdc793.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-14eb942276f77b9a2739f79694dd8ecb.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-13663982cae82b5a4c6084d675983569.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c5c18d6cc3c19b8db21ccf258d9cf750.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-79b1611a1ef0c11e26a1488fa747e370.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6e83b0a17017a783581dc846c0f46d4e.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Long-Context-Modeling-Networks-for-Monaural-Speech-Enhancement-A-Comparative-Study"><a href="#Long-Context-Modeling-Networks-for-Monaural-Speech-Enhancement-A-Comparative-Study" class="headerlink" title="Long-Context Modeling Networks for Monaural Speech Enhancement: A   Comparative Study"></a>Long-Context Modeling Networks for Monaural Speech Enhancement: A   Comparative Study</h2><p><strong>Authors:Qiquan Zhang, Moran Chen, Zeyang Song, Hexin Liu, Xiangyu Zhang, Haizhou Li</strong></p>
<p>Advanced long-context modeling backbone networks, such as Transformer, Conformer, and Mamba, have demonstrated state-of-the-art performance in speech enhancement. However, a systematic and comprehensive comparative study of these backbones within a unified speech enhancement framework remains lacking. In addition, xLSTM, a more recent and efficient variant of LSTM, has shown promising results in language modeling and as a general-purpose vision backbone. In this paper, we investigate the capability of xLSTM in speech enhancement, and conduct a comprehensive comparison and analysis of the Transformer, Conformer, Mamba, and xLSTM backbones within a unified framework, considering both causal and noncausal configurations. Overall, xLSTM and Mamba achieve better performance than Transformer and Conformer. Mamba demonstrates significantly superior training and inference efficiency, particularly for long speech inputs, whereas xLSTM suffers from the slowest processing speed. </p>
<blockquote>
<p>å…ˆè¿›çš„é•¿æœŸä¸Šä¸‹æ–‡å»ºæ¨¡ä¸»å¹²ç½‘ç»œï¼Œå¦‚Transformerã€Conformerå’ŒMambaï¼Œåœ¨è¯­éŸ³å¢å¼ºæ–¹é¢å·²ç»è¡¨ç°å‡ºäº†å“è¶Šçš„æ€§èƒ½ã€‚ç„¶è€Œï¼Œåœ¨ä¸€ä¸ªç»Ÿä¸€çš„è¯­éŸ³å¢å¼ºæ¡†æ¶å†…ï¼Œå¯¹è¿™äº›ä¸»å¹²ç½‘è¿›è¡Œç³»ç»Ÿå’Œå…¨é¢çš„æ¯”è¾ƒç ”ç©¶ä»ç„¶ç¼ºä¹ã€‚æ­¤å¤–ï¼ŒxLSTMä½œä¸ºLSTMçš„æ›´æ–°ã€æ›´é«˜æ•ˆçš„å½¢å¼ï¼Œåœ¨è¯­è¨€å»ºæ¨¡å’Œé€šç”¨è§†è§‰ä¸»å¹²æ–¹é¢å·²ç»å±•ç°å‡ºæœ‰å‰æ™¯çš„ç»“æœã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬ç ”ç©¶äº†xLSTMåœ¨è¯­éŸ³å¢å¼ºæ–¹é¢çš„èƒ½åŠ›ï¼Œå¹¶åœ¨ä¸€ä¸ªç»Ÿä¸€çš„æ¡†æ¶å†…å¯¹Transformerã€Conformerã€Mambaå’ŒxLSTMä¸»å¹²è¿›è¡Œäº†å…¨é¢çš„æ¯”è¾ƒå’Œåˆ†æï¼ŒåŒæ—¶è€ƒè™‘äº†å› æœå’Œéå› æœé…ç½®ã€‚æ€»ä½“è€Œè¨€ï¼ŒxLSTMå’ŒMambaçš„æ€§èƒ½ä¼˜äºTransformerå’ŒConformerã€‚Mambaåœ¨è®­ç»ƒå’Œæ¨ç†æ•ˆç‡æ–¹é¢è¡¨ç°å‡ºæ˜¾è‘—çš„ä¼˜åŠ¿ï¼Œç‰¹åˆ«æ˜¯å¯¹äºé•¿è¯­éŸ³è¾“å…¥ï¼Œè€ŒxLSTMçš„å¤„ç†é€Ÿåº¦æœ€æ…¢ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.04368v1">PDF</a> Accepted by WASPAA 2025, 5 pages</p>
<p><strong>Summary</strong><br>é«˜çº§é•¿ä¸Šä¸‹æ–‡å»ºæ¨¡ä¸»å¹²ç½‘ç»œå¦‚Transformerã€Conformerå’ŒMambaåœ¨è¯­éŸ³å¢å¼ºæ–¹é¢è¡¨ç°å‡ºå“è¶Šçš„æ€§èƒ½ã€‚ç„¶è€Œï¼Œç¼ºä¹åœ¨ä¸€ä¸ªç»Ÿä¸€çš„è¯­éŸ³å¢å¼ºæ¡†æ¶ä¸‹å¯¹è¿™äº›ä¸»å¹²ç½‘è¿›è¡Œç³»ç»Ÿå’Œå…¨é¢çš„æ¯”è¾ƒç ”ç©¶ã€‚æœ¬ç ”ç©¶æ¢è®¨äº†xLSTMåœ¨è¯­éŸ³å¢å¼ºä¸­çš„èƒ½åŠ›ï¼Œå¹¶åœ¨ç»Ÿä¸€æ¡†æ¶ä¸‹å¯¹Transformerã€Conformerã€Mambaå’ŒxLSTMè¿›è¡Œäº†å…¨é¢çš„æ¯”è¾ƒä¸åˆ†æï¼Œè€ƒè™‘äº†å› æœå’Œéå› æœé…ç½®ã€‚æ€»ä½“è€Œè¨€ï¼ŒxLSTMå’ŒMambaçš„æ€§èƒ½ä¼˜äºTransformerå’ŒConformerã€‚Mambaåœ¨è®­ç»ƒä¸æ¨ç†æ•ˆç‡ä¸Šè¡¨ç°æ˜¾è‘—ä¼˜è¶Šï¼Œå°¤å…¶å¯¹äºé•¿è¯­éŸ³è¾“å…¥ï¼Œè€ŒxLSTMå¤„ç†é€Ÿåº¦è¾ƒæ…¢ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>é«˜çº§é•¿ä¸Šä¸‹æ–‡å»ºæ¨¡ä¸»å¹²ç½‘ç»œå¦‚Transformerã€Conformerå’ŒMambaåœ¨è¯­éŸ³å¢å¼ºæ–¹é¢è¡¨ç°å‡ºå“è¶Šæ€§èƒ½ã€‚</li>
<li>ç¼ºä¹åœ¨ä¸€ä¸ªç»Ÿä¸€çš„è¯­éŸ³å¢å¼ºæ¡†æ¶ä¸‹å¯¹è¿™äº›ä¸»å¹²ç½‘ç»œè¿›è¡Œç³»ç»Ÿæ€§å’Œå…¨é¢æ€§çš„æ¯”è¾ƒç ”ç©¶ã€‚</li>
<li>xLSTMåœ¨è¯­éŸ³å¢å¼ºä¸­çš„èƒ½åŠ›å¾—åˆ°äº†ç ”ç©¶ã€‚</li>
<li>åœ¨ç»Ÿä¸€æ¡†æ¶ä¸‹è¿›è¡Œäº†Transformerã€Conformerã€Mambaå’ŒxLSTMçš„å…¨é¢æ¯”è¾ƒã€‚</li>
<li>xLSTMå’ŒMambaçš„æ€§èƒ½æ€»ä½“ä¼˜äºTransformerå’ŒConformerã€‚</li>
<li>Mambaåœ¨è®­ç»ƒå’Œæ¨ç†æ•ˆç‡ä¸Šè¡¨ç°æ˜¾è‘—ä¼˜è¶Šï¼Œå°¤å…¶é€‚ç”¨äºé•¿è¯­éŸ³è¾“å…¥ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.04368">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-3315b59336bd6eeb85b7b2375c74a39a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-6745d1a0e0449159ffbb30f59400021a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-bf996eb0c619d9c3aba5daa50e112e6b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d50b636e27c763ef1f4d87022969c921.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-aed285bd64fbd0be5e7450db9e86e440.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4dc0dc0d4936999f9b554f8007587836.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="MMMOS-Multi-domain-Multi-axis-Audio-Quality-Assessment"><a href="#MMMOS-Multi-domain-Multi-axis-Audio-Quality-Assessment" class="headerlink" title="MMMOS: Multi-domain Multi-axis Audio Quality Assessment"></a>MMMOS: Multi-domain Multi-axis Audio Quality Assessment</h2><p><strong>Authors:Yi-Cheng Lin, Jia-Hung Chen, Hung-yi Lee</strong></p>
<p>Accurate audio quality estimation is essential for developing and evaluating audio generation, retrieval, and enhancement systems. Existing non-intrusive assessment models predict a single Mean Opinion Score (MOS) for speech, merging diverse perceptual factors and failing to generalize beyond speech. We propose MMMOS, a no-reference, multi-domain audio quality assessment system that estimates four orthogonal axes: Production Quality, Production Complexity, Content Enjoyment, and Content Usefulness across speech, music, and environmental sounds. MMMOS fuses frame-level embeddings from three pretrained encoders (WavLM, MuQ, and M2D) and evaluates three aggregation strategies with four loss functions. By ensembling the top eight models, MMMOS shows a 20-30% reduction in mean squared error and a 4-5% increase in Kendallâ€™s {\tau} versus baseline, gains first place in six of eight Production Complexity metrics, and ranks among the top three on 17 of 32 challenge metrics. </p>
<blockquote>
<p>å‡†ç¡®çš„éŸ³é¢‘è´¨é‡è¯„ä¼°å¯¹äºå¼€å‘å’Œè¯„ä¼°éŸ³é¢‘ç”Ÿæˆã€æ£€ç´¢å’Œå¢å¼ºç³»ç»Ÿè‡³å…³é‡è¦ã€‚ç°æœ‰çš„éä¾µå…¥æ€§è¯„ä¼°æ¨¡å‹ä¸ºè¯­éŸ³é¢„æµ‹å•ä¸€çš„ä¸»è§‚æ„è§å¾—åˆ†ï¼ˆMOSï¼‰ï¼Œèåˆå„ç§æ„ŸçŸ¥å› ç´ ï¼Œä½†æ— æ³•æ¨å¹¿åˆ°è¯­éŸ³ä¹‹å¤–ã€‚æˆ‘ä»¬æå‡ºMMMOSï¼Œè¿™æ˜¯ä¸€ç§æ— å‚è€ƒã€å¤šé¢†åŸŸçš„éŸ³é¢‘è´¨é‡è¯„ä¼°ç³»ç»Ÿï¼Œå¯ä»¥ä¼°è®¡å››ä¸ªæ­£äº¤è½´ï¼šç”Ÿäº§è´¨é‡ã€ç”Ÿäº§å¤æ‚æ€§ã€å†…å®¹äº«å—å’Œå†…å®¹å®ç”¨æ€§ï¼Œæ¶µç›–è¯­éŸ³ã€éŸ³ä¹å’Œç¯å¢ƒå£°éŸ³ã€‚MMMOSèåˆäº†ä¸‰ä¸ªé¢„è®­ç»ƒç¼–ç å™¨ï¼ˆWavLMã€MuQå’ŒM2Dï¼‰çš„å¸§çº§åµŒå…¥ï¼Œå¹¶è¯„ä»·äº†å››ç§æŸå¤±å‡½æ•°ä¸‹çš„ä¸‰ç§èšåˆç­–ç•¥ã€‚é€šè¿‡å¯¹å‰å…«åæ¨¡å‹çš„ç»¼åˆè¯„ä¼°ï¼ŒMMMOSå°†å‡æ–¹è¯¯å·®é™ä½äº†20%-30%ï¼ŒKendallç³»æ•°æé«˜äº†4%-5%ï¼Œç›¸è¾ƒäºåŸºçº¿åœ¨å…­ä¸ªç”Ÿäº§å¤æ‚æ€§æŒ‡æ ‡ä¸­è·å¾—ç¬¬ä¸€åï¼Œå¹¶åœ¨32ä¸ªæŒ‘æˆ˜æŒ‡æ ‡ä¸­æœ‰17ä¸ªè¿›å…¥å‰ä¸‰åã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.04094v1">PDF</a> 4 pages including 1 page of reference. ASRU Audio MOS 2025 Challenge   paper</p>
<p><strong>æ‘˜è¦</strong></p>
<p>æ— å‚è€ƒã€å¤šé¢†åŸŸçš„éŸ³é¢‘è´¨é‡è¯„ä¼°ç³»ç»ŸMMMOSï¼Œå¯¹è¯­éŸ³ã€éŸ³ä¹å’Œç¯å¢ƒå£°éŸ³çš„è´¨é‡è¿›è¡Œæ›´å…¨é¢çš„è¯„ä¼°ã€‚å®ƒé€šè¿‡èåˆä¸‰ç§é¢„è®­ç»ƒç¼–ç å™¨ï¼ˆWavLMã€MuQå’ŒM2Dï¼‰çš„å¸§çº§åµŒå…¥ï¼Œè¯„ä»·äº†ä¸‰ç§èšåˆç­–ç•¥å’Œå››ç§æŸå¤±å‡½æ•°ã€‚MMMOSåœ¨å¤šä¸ªè¯„ä¼°æŒ‡æ ‡ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œç›¸å¯¹äºåŸºå‡†æµ‹è¯•ï¼Œå‡æ–¹è¯¯å·®é™ä½äº†20-30%ï¼ŒKendallçš„Ï„ç³»æ•°æé«˜äº†4-5%ï¼Œåœ¨ç”Ÿäº§å¤æ‚åº¦æŒ‡æ ‡ä¸­çš„å…­ä¸ªé¡¹ç›®ä¸­è·å¾—ç¬¬ä¸€åï¼Œå¹¶åœ¨32ä¸ªæŒ‘æˆ˜æŒ‡æ ‡ä¸­çš„17ä¸ªé¡¹ç›®ä¸Šè·»èº«å‰ä¸‰åã€‚</p>
<p><strong>è¦ç‚¹</strong></p>
<ol>
<li>éŸ³é¢‘è´¨é‡è¯„ä¼°åœ¨éŸ³é¢‘ç”Ÿæˆã€æ£€ç´¢å’Œå¢å¼ºç³»ç»Ÿçš„å¼€å‘ä¸è¯„ä¼°ä¸­è‡³å…³é‡è¦ã€‚</li>
<li>ç°æœ‰éä¾µå…¥å¼è¯„ä¼°æ¨¡å‹åœ¨è¯­éŸ³æ–¹é¢çš„é¢„æµ‹å­˜åœ¨å±€é™æ€§ï¼Œæ— æ³•æ¦‚æ‹¬ä¸åŒçš„æ„ŸçŸ¥å› ç´ ã€‚</li>
<li>MMMOSç³»ç»Ÿæ˜¯ä¸€ç§æ— å‚è€ƒã€å¤šé¢†åŸŸçš„éŸ³é¢‘è´¨é‡è¯„ä¼°æ–¹æ³•ï¼Œå¯ä¼°è®¡å››ä¸ªæ­£äº¤è½´ï¼šç”Ÿäº§è´¨é‡ã€ç”Ÿäº§å¤æ‚åº¦ã€å†…å®¹äº«å—å’Œå†…å®¹æœ‰ç”¨æ€§ã€‚</li>
<li>MMMOSèåˆäº†ä¸‰ç§é¢„è®­ç»ƒç¼–ç å™¨çš„å¸§çº§åµŒå…¥ï¼Œå¹¶è¯„ä»·äº†ä¸åŒçš„èšåˆç­–ç•¥å’ŒæŸå¤±å‡½æ•°ã€‚</li>
<li>MMMOSåœ¨å¤šä¸ªè¯„ä¼°æŒ‡æ ‡ä¸Šè¡¨ç°ä¼˜ç§€ï¼Œç›¸å¯¹åŸºå‡†æµ‹è¯•æœ‰æ˜¾è‘—æ”¹å–„ã€‚</li>
<li>MMMOSåœ¨ç”Ÿäº§å¤æ‚åº¦æŒ‡æ ‡ä¸­çš„å…­ä¸ªé¡¹ç›®ä¸­è·å¾—ç¬¬ä¸€åï¼Œå¹¶åœ¨å¤§å¤šæ•°æŒ‘æˆ˜æŒ‡æ ‡ä¸­è·»èº«å‰ä¸‰åã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.04094">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-91b229abd67139e573ce9dd8e5d032a7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ce2155c076f8b79d65b948d563f0db9c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-cae4988b93c4096cf0cf50b919d33b5a.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-fccacb0645ec072f406c1859d63e4f3d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-eff242a616b5965dc6d4a77212239126.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="CLEP-DG-Contrastive-Learning-for-Speech-Emotion-Domain-Generalization-via-Soft-Prompt-Tuning"><a href="#CLEP-DG-Contrastive-Learning-for-Speech-Emotion-Domain-Generalization-via-Soft-Prompt-Tuning" class="headerlink" title="CLEP-DG: Contrastive Learning for Speech Emotion Domain Generalization   via Soft Prompt Tuning"></a>CLEP-DG: Contrastive Learning for Speech Emotion Domain Generalization   via Soft Prompt Tuning</h2><p><strong>Authors:Jiacheng Shi, Yanfu Zhang, Ye Gao</strong></p>
<p>Speech Emotion Recognition (SER) is fundamental to affective computing and human-computer interaction, yet existing models struggle to generalize across diverse acoustic conditions. While Contrastive Language-Audio Pretraining (CLAP) provides strong multimodal alignment, it lacks dedicated mechanisms for capturing emotional cues, making it suboptimal for SER. To address this, we propose CLEP-DG, a framework that enhances CLAPâ€™s robustness in emotion recognition. First, we fine-tune CLAP to obtain CLEP, adapting it on large-scale emotional speech datasets to better encode emotion-relevant features. Then, we introduce Acoustic Context Prompt Tuning (ACPT), a text-driven augmentation strategy that optimizes learnable prompt vectors to model diverse acoustic environments without additional labeled audio. Finally, leveraging cross-modal transferability, we train a classifier on text-derived embeddings and apply it to the audio encoder during inference, mitigating domain shifts between textual supervision and audio-based emotion recognition. Experiments across five benchmark datasets show that CLEP-DG outperforms prior CLAP-based approaches, achieving state-of-the-art performance in both supervised and domain generalization settings. </p>
<blockquote>
<p>è¯­éŸ³æƒ…æ„Ÿè¯†åˆ«ï¼ˆSERï¼‰å¯¹äºæƒ…æ„Ÿè®¡ç®—å’Œäººæœºäº¤äº’è‡³å…³é‡è¦ï¼Œä½†ç°æœ‰æ¨¡å‹åœ¨ä¸åŒçš„å£°å­¦æ¡ä»¶ä¸‹éš¾ä»¥è¿›è¡Œæ¨å¹¿ã€‚è™½ç„¶å¯¹æ¯”è¯­è¨€éŸ³é¢‘é¢„è®­ç»ƒï¼ˆCLAPï¼‰æä¾›äº†å¼ºå¤§çš„å¤šæ¨¡å¼å¯¹é½ï¼Œä½†å®ƒç¼ºä¹æ•æ‰æƒ…æ„Ÿçº¿ç´¢çš„ä¸“é—¨æœºåˆ¶ï¼Œä½¿å…¶ä¸é€‚ç”¨äºSERã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†CLEP-DGæ¡†æ¶ï¼Œè¯¥æ¡†æ¶å¢å¼ºäº†CLAPåœ¨æƒ…æ„Ÿè¯†åˆ«æ–¹é¢çš„ç¨³å¥æ€§ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬å¯¹CLAPè¿›è¡Œå¾®è°ƒï¼Œè·å¾—CLEPï¼Œé€šè¿‡å¤§è§„æ¨¡æƒ…æ„Ÿè¯­éŸ³æ•°æ®é›†è¿›è¡Œé€‚åº”ï¼Œä»¥æ›´å¥½åœ°ç¼–ç ä¸æƒ…æ„Ÿç›¸å…³çš„ç‰¹å¾ã€‚ç„¶åï¼Œæˆ‘ä»¬å¼•å…¥äº†å£°å­¦ä¸Šä¸‹æ–‡æç¤ºè°ƒæ•´ï¼ˆACPTï¼‰ï¼Œè¿™æ˜¯ä¸€ç§æ–‡æœ¬é©±åŠ¨çš„æ•°æ®å¢å¼ºç­–ç•¥ï¼Œé€šè¿‡ä¼˜åŒ–å¯å­¦ä¹ çš„æç¤ºå‘é‡æ¥æ¨¡æ‹Ÿå„ç§å£°å­¦ç¯å¢ƒï¼Œè€Œæ— éœ€é¢å¤–çš„æ ‡è®°éŸ³é¢‘ã€‚æœ€åï¼Œåˆ©ç”¨è·¨æ¨¡æ€è¿ç§»æ€§ï¼Œæˆ‘ä»¬åœ¨æ–‡æœ¬æ´¾ç”ŸåµŒå…¥ä¸Šè®­ç»ƒä¸€ä¸ªåˆ†ç±»å™¨ï¼Œå¹¶åœ¨æ¨ç†æœŸé—´å°†å…¶åº”ç”¨äºéŸ³é¢‘ç¼–ç å™¨ï¼Œä»è€Œç¼“è§£æ–‡æœ¬ç›‘ç£å’ŒåŸºäºéŸ³é¢‘çš„æƒ…æ„Ÿè¯†åˆ«ä¹‹é—´çš„åŸŸè½¬ç§»é—®é¢˜ã€‚åœ¨äº”ä¸ªåŸºå‡†æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒCLEP-DGåœ¨åŸºäºCLAPçš„æ–¹æ³•ä¸Šè¡¨ç°æ›´å¥½ï¼Œåœ¨ç›‘ç£å­¦ä¹ å’ŒåŸŸæ³›åŒ–è®¾ç½®ä¸­å‡è¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.04048v1">PDF</a> Accepted to Interspeech2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†CLEP-DGæ¡†æ¶ï¼Œæ—¨åœ¨å¢å¼ºContrastive Language-Audio Pretrainingï¼ˆCLAPï¼‰åœ¨æƒ…æ„Ÿè¯†åˆ«æ–¹é¢çš„ç¨³å¥æ€§ã€‚é€šè¿‡å¾®è°ƒCLAPè·å¾—CLEPï¼Œå¹¶å¼•å…¥Acoustic Context Prompt Tuningï¼ˆACPTï¼‰ç­–ç•¥ï¼Œä¼˜åŒ–å¯å­¦ä¹ æç¤ºå‘é‡ä»¥æ¨¡æ‹Ÿä¸åŒçš„å£°å­¦ç¯å¢ƒã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒCLEP-DGåœ¨äº”ä¸ªåŸºå‡†æ•°æ®é›†ä¸Šçš„æ€§èƒ½ä¼˜äºåŸºäºCLAPçš„æ–¹æ³•ï¼Œå®ç°äº†ç›‘ç£å­¦ä¹ å’Œé¢†åŸŸæ³›åŒ–è®¾ç½®ä¸­çš„æœ€ä½³æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Speech Emotion Recognition (SER) åœ¨æƒ…æ„Ÿè®¡ç®—å’Œäººæœºäº¤äº’ä¸­æ‰®æ¼”é‡è¦è§’è‰²ï¼Œä½†ç°æœ‰æ¨¡å‹åœ¨å¤šæ ·åŒ–å£°å­¦æ¡ä»¶ä¸‹çš„æ³›åŒ–èƒ½åŠ›æœ‰é™ã€‚</li>
<li>Contrastive Language-Audio Pretraining (CLAP) æä¾›å¼ºå¤§çš„å¤šæ¨¡å¼å¯¹é½ï¼Œä½†ç¼ºä¹æ•æ‰æƒ…æ„Ÿçº¿ç´¢çš„ä¸“é—¨æœºåˆ¶ï¼Œå¯¹äºSERæ¥è¯´ä¸å¤Ÿç†æƒ³ã€‚</li>
<li>CLEP-DGæ¡†æ¶é€šè¿‡å¾®è°ƒCLAPå¹¶å¼•å…¥Acoustic Context Prompt Tuning (ACPT)ç­–ç•¥ï¼Œå¢å¼ºäº†CLAPåœ¨æƒ…æ„Ÿè¯†åˆ«æ–¹é¢çš„ç¨³å¥æ€§ã€‚</li>
<li>ACPTæ˜¯ä¸€ç§æ–‡æœ¬é©±åŠ¨çš„æ•°æ®å¢å¼ºç­–ç•¥ï¼Œé€šè¿‡ä¼˜åŒ–å¯å­¦ä¹ æç¤ºå‘é‡æ¥æ¨¡æ‹Ÿä¸åŒçš„å£°å­¦ç¯å¢ƒï¼Œæ— éœ€é¢å¤–çš„æ ‡è®°éŸ³é¢‘ã€‚</li>
<li>åˆ©ç”¨è·¨æ¨¡æ€å¯è¿ç§»æ€§ï¼ŒCLEP-DGè®­ç»ƒäº†ä¸€ä¸ªåŸºäºæ–‡æœ¬çš„åµŒå…¥åˆ†ç±»å™¨ï¼Œå¹¶å°†å…¶åº”ç”¨äºéŸ³é¢‘ç¼–ç å™¨è¿›è¡Œæ¨æ–­ï¼Œå‡è½»äº†æ–‡æœ¬ç›‘ç£å’ŒéŸ³é¢‘æƒ…æ„Ÿè¯†åˆ«ä¹‹é—´çš„é¢†åŸŸå·®å¼‚ã€‚</li>
<li>å®éªŒç»“æœå±•ç¤ºäº†CLEP-DGåœ¨å¤šä¸ªåŸºå‡†æ•°æ®é›†ä¸Šçš„å“è¶Šæ€§èƒ½ï¼Œä¼˜äºåŸºäºCLAPçš„æ–¹æ³•ã€‚</li>
<li>CLEP-DGå®ç°äº†ç›‘ç£å­¦ä¹ å’Œé¢†åŸŸæ³›åŒ–è®¾ç½®ä¸­çš„æœ€ä½³æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.04048">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-4ce5b268837532a90ad1a771724c75e8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e1eb9596f71b395764abed8e6e1437c5.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ecfb790615e83c3b4329d34dc223e659.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4750169df1fea6cfbdfd48506e96a039.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-75513347892ed5e9a38bac353cfb63fd.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-bbbc2f1d04d62a73881175f129277570.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="RECA-PD-A-Robust-Explainable-Cross-Attention-Method-for-Speech-based-Parkinsonâ€™s-Disease-Classification"><a href="#RECA-PD-A-Robust-Explainable-Cross-Attention-Method-for-Speech-based-Parkinsonâ€™s-Disease-Classification" class="headerlink" title="RECA-PD: A Robust Explainable Cross-Attention Method for Speech-based   Parkinsonâ€™s Disease Classification"></a>RECA-PD: A Robust Explainable Cross-Attention Method for Speech-based   Parkinsonâ€™s Disease Classification</h2><p><strong>Authors:Terry Yi Zhong, Cristian Tejedor-Garcia, Martha Larson, Bastiaan R. Bloem</strong></p>
<p>Parkinsonâ€™s Disease (PD) affects over 10 million people globally, with speech impairments often preceding motor symptoms by years, making speech a valuable modality for early, non-invasive detection. While recent deep-learning models achieve high accuracy, they typically lack the explainability required for clinical use. To address this, we propose RECA-PD, a novel, robust, and explainable cross-attention architecture that combines interpretable speech features with self-supervised representations. RECA-PD matches state-of-the-art performance in Speech-based PD detection while providing explanations that are more consistent and more clinically meaningful. Additionally, we demonstrate that performance degradation in certain speech tasks (e.g., monologue) can be mitigated by segmenting long recordings. Our findings indicate that performance and explainability are not necessarily mutually exclusive. Future work will enhance the usability of explanations for non-experts and explore severity estimation to increase the real-world clinical relevance. </p>
<blockquote>
<p>å¸•é‡‘æ£®ç—…ï¼ˆPDï¼‰å…¨çƒå½±å“è¶…è¿‡1000ä¸‡äººï¼Œè¨€è¯­éšœç¢é€šå¸¸æ•°å¹´å‰å…ˆäºè¿åŠ¨ç—‡çŠ¶å‡ºç°ï¼Œè¿™ä½¿å¾—è¨€è¯­æˆä¸ºæ—©æœŸã€æ— åˆ›æ£€æµ‹çš„æœ‰ä»·å€¼æ–¹å¼ã€‚è™½ç„¶æœ€è¿‘çš„æ·±åº¦å­¦ä¹ æ¨¡å‹å…·æœ‰å¾ˆé«˜çš„å‡†ç¡®æ€§ï¼Œä½†å®ƒä»¬é€šå¸¸ç¼ºä¹ä¸´åºŠä½¿ç”¨æ‰€éœ€çš„è§£é‡Šæ€§ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†RECA-PDï¼Œè¿™æ˜¯ä¸€ç§æ–°é¢–ã€ç¨³å¥ä¸”å¯è§£é‡Šçš„å…³æ³¨äº¤å‰æ¶æ„ï¼Œå®ƒç»“åˆäº†å¯è§£é‡Šçš„è¯­éŸ³ç‰¹å¾ä¸è‡ªç›‘ç£è¡¨ç¤ºã€‚RECA-PDåœ¨åŸºäºè¯­éŸ³çš„PDæ£€æµ‹æ–¹é¢è¾¾åˆ°äº†æœ€å…ˆè¿›çš„è¡¨ç°ï¼ŒåŒæ—¶æä¾›äº†æ›´åŠ ä¸€è‡´å’Œä¸´åºŠæ„ä¹‰æ›´å¼ºçš„è§£é‡Šã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¯æ˜é€šè¿‡åˆ†æ®µé•¿å½•éŸ³å¯ä»¥ç¼“è§£æŸäº›è¯­éŸ³ä»»åŠ¡ï¼ˆå¦‚ç‹¬ç™½ï¼‰ä¸­çš„æ€§èƒ½ä¸‹é™ã€‚æˆ‘ä»¬çš„ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œæ€§èƒ½å’Œè§£é‡Šæ€§å¹¶ä¸ä¸€å®šç›¸äº’æ’æ–¥ã€‚æœªæ¥çš„å·¥ä½œå°†æé«˜è§£é‡Šå¯¹äºéä¸“å®¶çš„å¯ç”¨æ€§ï¼Œå¹¶æ¢ç´¢ä¸¥é‡ç¨‹åº¦è¯„ä¼°ï¼Œä»¥å¢åŠ ç°å®ä¸–ç•Œä¸­çš„ä¸´åºŠç›¸å…³æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.03594v1">PDF</a> Accepted for TSD 2025</p>
<p><strong>Summary</strong>ï¼šå¸•é‡‘æ£®ç—…å½±å“å…¨çƒè¶…è¿‡ä¸€åƒä¸‡äººç¾¤ï¼Œè¯­è¨€éšœç¢å¾€å¾€æ—©äºè¿åŠ¨ç—‡çŠ¶æ•°å¹´å‡ºç°ï¼Œä½¿å¾—è¯­è¨€æˆä¸ºæ—©æœŸéä¾µå…¥æ€§æ£€æµ‹çš„é‡è¦æ¨¡æ€ã€‚é’ˆå¯¹ç¼ºä¹ä¸´åºŠä½¿ç”¨æ‰€éœ€è§£é‡Šæ€§çš„é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†RECA-PDæ¨¡å‹ï¼Œè¿™æ˜¯ä¸€ç§æ–°é¢–ã€ç¨³å¥ä¸”å¯è§£é‡Šçš„äº¤å‰æ³¨æ„åŠ›æ¶æ„ï¼Œç»“åˆäº†å¯è§£é‡Šçš„è¯­è¨€ç‰¹å¾å’Œè‡ªç›‘ç£è¡¨ç¤ºã€‚RECA-PDåœ¨åŸºäºè¯­éŸ³çš„å¸•é‡‘æ£®ç—…æ£€æµ‹æ–¹é¢è¾¾åˆ°äº†å…ˆè¿›æ°´å¹³ï¼ŒåŒæ—¶æä¾›äº†æ›´åŠ ä¸€è‡´å’Œä¸´åºŠæ„ä¹‰çš„è§£é‡Šã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬çš„ç ”ç©¶è¿˜æ˜¾ç¤ºé€šè¿‡åˆ†æ®µé•¿å½•éŸ³å¯ä»¥æ”¹å–„æŸäº›è¯­éŸ³ä»»åŠ¡çš„æ€§èƒ½ä¸‹é™é—®é¢˜ã€‚æˆ‘ä»¬çš„å‘ç°è¡¨æ˜æ€§èƒ½å’Œè§£é‡Šæ€§å¹¶ä¸ä¸€å®šç›¸äº’æ’æ–¥ã€‚æœªæ¥çš„å·¥ä½œå°†æé«˜è§£é‡Šå¯¹éä¸“ä¸šäººå£«çš„å¯ç”¨æ€§ï¼Œå¹¶æ¢ç´¢ä¸¥é‡ç¨‹åº¦è¯„ä¼°ä»¥å¢åŠ å…¶åœ¨ç°å®ä¸–ç•Œä¸­çš„ä¸´åºŠç›¸å…³æ€§ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>å¸•é‡‘æ£®ç—…å½±å“å…¨çƒè¶…è¿‡ä¸€åƒä¸‡äººï¼Œè¯­è¨€éšœç¢åœ¨æ—©æœŸå‡ºç°ã€‚</li>
<li>å½“å‰æ·±åº¦æ¨¡å‹åœ¨è¯­éŸ³å¸•é‡‘æ£®ç—…æ£€æµ‹æ–¹é¢è¡¨ç°å‡ºé«˜å‡†ç¡®æ€§ï¼Œä½†ç¼ºä¹å¿…è¦çš„è§£é‡Šæ€§ã€‚</li>
<li>RECA-PDæ¨¡å‹ç»“åˆäº†å¯è§£é‡Šçš„è¯­è¨€ç‰¹å¾å’Œè‡ªç›‘ç£è¡¨ç¤ºï¼Œå®ç°äº†é«˜æ°´å¹³çš„æ€§èƒ½å¹¶æä¾›äº†è§£é‡Šæ€§ã€‚</li>
<li>RECA-PDæ¨¡å‹æä¾›çš„è§£é‡Šæ›´å…·ä¸€è‡´æ€§å’Œä¸´åºŠæ„ä¹‰ã€‚</li>
<li>å¯¹é•¿å½•éŸ³è¿›è¡Œåˆ†æ®µå¯ä»¥æ”¹å–„æŸäº›è¯­éŸ³ä»»åŠ¡çš„æ€§èƒ½ä¸‹é™é—®é¢˜ã€‚</li>
<li>ç ”ç©¶è¡¨æ˜æ€§èƒ½å’Œè§£é‡Šæ€§ä¸å¿…ç›¸äº’æ’æ–¥ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.03594">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-d68a7fd7a4c344374dcf2c81b70d7c01.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ae10fd673179cc93df808ddab5d7cba9.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="SHNU-Multilingual-Conversational-Speech-Recognition-System-for-INTERSPEECH-2025-MLC-SLM-Challenge"><a href="#SHNU-Multilingual-Conversational-Speech-Recognition-System-for-INTERSPEECH-2025-MLC-SLM-Challenge" class="headerlink" title="SHNU Multilingual Conversational Speech Recognition System for   INTERSPEECH 2025 MLC-SLM Challenge"></a>SHNU Multilingual Conversational Speech Recognition System for   INTERSPEECH 2025 MLC-SLM Challenge</h2><p><strong>Authors:Yuxiang Mei, Yuang Zheng, Dongxing Xu, Yanhua Long</strong></p>
<p>This paper describes SHNU multilingual conversational speech recognition system (SHNU-mASR, team name-â€œmaybeâ€), submitted to Track 1 of the INTERSPEECH 2025 MLC-SLM Challenge. Our system integrates a parallel-speech-encoder architecture with a large language model (LLM) to form a unified multilingual ASR framework. The parallel-speech-encoder consists of two pre-trained encoders, the Whisper-large-v3 encoder and mHuBERT-147 encoder. Their output embeddings are concatenated and fed into the LLM, enabling the model to leverage complementary acoustic and linguistic knowledge and achieve competitive performance. Moreover, we adopt a tri-stage training strategy to jointly update the low-rank adaptation modules and projector parameters of both the speech encoders and the LLM. In addition, we incorporate an additional language-aware prompt at the LLM input to enhance language-specific text generation. The SHNU-mASR system achieves an overall character&#x2F;word error rate (CER&#x2F;WER) of 11.76% on the blind evaluation set of the challenge, outperforming the official MLC-SLM baseline by 8.41 absolute CER&#x2F;WER, without increasing the baseline training data. </p>
<blockquote>
<p>æœ¬æ–‡ä»‹ç»äº†SHNUå¤šè¯­ç§å¯¹è¯è¯­éŸ³è¯†åˆ«ç³»ç»Ÿï¼ˆSHNU-mASRï¼Œå›¢é˜Ÿåä¸ºâ€œmaybeâ€ï¼‰ï¼Œè¯¥ç³»ç»Ÿæäº¤è‡³INTERSPEECH 2025 MLC-SLM Challengeçš„Track 1ã€‚æˆ‘ä»¬çš„ç³»ç»Ÿå°†å¹¶è¡Œè¯­éŸ³ç¼–ç å™¨æ¶æ„ä¸å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ç›¸ç»“åˆï¼Œå½¢æˆä¸€ä¸ªç»Ÿä¸€çš„å¤šè¯­ç§ASRæ¡†æ¶ã€‚å¹¶è¡Œè¯­éŸ³ç¼–ç å™¨ç”±ä¸¤ä¸ªé¢„è®­ç»ƒç¼–ç å™¨ç»„æˆï¼Œå³Whisper-large-v3ç¼–ç å™¨å’ŒmHuBERT-147ç¼–ç å™¨ã€‚ä»–ä»¬çš„è¾“å‡ºåµŒå…¥è¿›è¡Œæ‹¼æ¥ï¼Œå¹¶è¾“å…¥åˆ°LLMä¸­ï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿåˆ©ç”¨äº’è¡¥çš„å£°å­¦å’Œè¯­è¨€çŸ¥è¯†ï¼Œå®ç°æœ‰ç«äº‰åŠ›çš„æ€§èƒ½ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬é‡‡ç”¨ä¸‰é˜¶æ®µè®­ç»ƒç­–ç•¥ï¼Œè”åˆæ›´æ–°ä½ç§©é€‚åº”æ¨¡å—å’Œè¯­éŸ³ç¼–ç å™¨å’ŒLLMçš„æŠ•å½±ä»ªå‚æ•°ã€‚å¦å¤–ï¼Œæˆ‘ä»¬åœ¨LLMçš„è¾“å…¥ç«¯å¢åŠ äº†ä¸€ä¸ªé¢å¤–çš„è¯­è¨€æ„ŸçŸ¥æç¤ºï¼Œä»¥å¢å¼ºç‰¹å®šè¯­è¨€çš„æ–‡æœ¬ç”Ÿæˆã€‚SHNU-mASRç³»ç»Ÿåœ¨æŒ‘æˆ˜ç›²è¯„æµ‹æ•°æ®é›†ä¸Šå®ç°äº†æ€»ä½“å­—ç¬¦&#x2F;å•è¯é”™è¯¯ç‡ï¼ˆCER&#x2F;WERï¼‰ä¸º11.76%ï¼Œæ¯”å®˜æ–¹MLC-SLMåŸºçº¿é«˜å‡º8.41ä¸ªç»å¯¹CER&#x2F;WERï¼Œä¸”æœªå¢åŠ åŸºçº¿è®­ç»ƒæ•°æ®ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.03343v1">PDF</a> Accepted by Interspeech 2025 MLC-SLM workshop</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†SHNUå¤šè¯­ç§å¯¹è¯è¯­éŸ³è¯†åˆ«ç³»ç»Ÿï¼ˆSHNU-mASRï¼Œå›¢é˜Ÿåä¸ºâ€œmaybeâ€ï¼‰ã€‚è¯¥ç³»ç»Ÿç»“åˆäº†å¹¶è¡Œè¯­éŸ³ç¼–ç å™¨æ¶æ„å’Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ï¼Œå½¢æˆä¸€ä¸ªç»Ÿä¸€çš„å¤šè¯­ç§ASRæ¡†æ¶ã€‚é€šè¿‡é‡‡ç”¨åŒç¼–ç å™¨ç»“æ„ã€ä¸‰é˜¶æ®µè®­ç»ƒç­–ç•¥å’Œè¯­è¨€æ„ŸçŸ¥æç¤ºï¼ŒSHNU-mASRç³»ç»Ÿåœ¨INTERSPEECH 2025 MLC-SLMæŒ‘æˆ˜ä¸­å–å¾—äº†å‡ºè‰²çš„æ€§èƒ½ï¼Œç›¸è¾ƒäºå®˜æ–¹MLC-SLMåŸºçº¿æ¨¡å‹ï¼Œåœ¨æŒ‘æˆ˜ç›²è¯„æµ‹æ•°æ®é›†ä¸Šçš„å­—ç¬¦&#x2F;å•è¯é”™è¯¯ç‡ï¼ˆCER&#x2F;WERï¼‰é™ä½äº†8.41ä¸ªç™¾åˆ†ç‚¹ï¼Œè¾¾åˆ°11.76%ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>SHNU-mASRç³»ç»Ÿèåˆäº†å¹¶è¡Œè¯­éŸ³ç¼–ç å™¨æ¶æ„å’Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼Œå½¢æˆç»Ÿä¸€çš„å¤šè¯­ç§ASRæ¡†æ¶ã€‚</li>
<li>ç³»ç»Ÿé‡‡ç”¨åŒç¼–ç å™¨ç»“æ„ï¼ŒåŒ…æ‹¬Whisper-large-v3ç¼–ç å™¨å’ŒmHuBERT-147ç¼–ç å™¨ã€‚</li>
<li>é€šè¿‡ä¸‰é˜¶æ®µè®­ç»ƒç­–ç•¥ï¼Œè”åˆæ›´æ–°ä½ç§©é€‚åº”æ¨¡å—å’ŒæŠ•å½±å‚æ•°ã€‚</li>
<li>åœ¨LLMè¾“å…¥ç«¯å¢åŠ è¯­è¨€æ„ŸçŸ¥æç¤ºï¼Œå¢å¼ºè¯­è¨€ç‰¹å®šæ–‡æœ¬ç”Ÿæˆèƒ½åŠ›ã€‚</li>
<li>SHNU-mASRç³»ç»Ÿåœ¨INTERSPEECH 2025 MLC-SLMæŒ‘æˆ˜ä¸­çš„ç›²è¯„æµ‹æ•°æ®é›†ä¸Šå–å¾—äº†11.76%çš„CER&#x2F;WERã€‚</li>
<li>ä¸å®˜æ–¹MLC-SLMåŸºçº¿æ¨¡å‹ç›¸æ¯”ï¼ŒSHNU-mASRç³»ç»Ÿçš„æ€§èƒ½æé«˜äº†8.41ä¸ªç™¾åˆ†ç‚¹ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.03343">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-109f15444de25c366d810633ae99d7fd.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-78728db70d2a6f96c98bfc01bd386bbd.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-868c350da442b3a2455601df5b764564.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c6393ee73c68038700dfe5e4af37aab1.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-8ff5b76954df33b73e355b423be7e28e.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Qwen-vs-Gemma-Integration-with-Whisper-A-Comparative-Study-in-Multilingual-SpeechLLM-Systems"><a href="#Qwen-vs-Gemma-Integration-with-Whisper-A-Comparative-Study-in-Multilingual-SpeechLLM-Systems" class="headerlink" title="Qwen vs. Gemma Integration with Whisper: A Comparative Study in   Multilingual SpeechLLM Systems"></a>Qwen vs. Gemma Integration with Whisper: A Comparative Study in   Multilingual SpeechLLM Systems</h2><p><strong>Authors:Tuan Nguyen, Long-Vu Hoang, Huy-Dat Tran</strong></p>
<p>This paper presents our system for the MLC-SLM Challenge 2025, focusing on multilingual speech recognition and language modeling with large language models (LLMs). Our approach combines a fine-tuned Whisper-large-v3 encoder with efficient projector architectures and various decoder configurations. We employ a three-stage training methodology that progressively optimizes the encoder, projector, and LLM components. Our system achieves competitive performance with a private test average WER&#x2F;CER result of 16.63% using the Gemma3-12B and 18.6% using the Qwen2.5-7B as decoder-only language model. </p>
<blockquote>
<p>æœ¬æ–‡ä»‹ç»äº†æˆ‘ä»¬ä¸ºMLC-SLMæŒ‘æˆ˜2025è®¾è®¡çš„ç³»ç»Ÿï¼Œé‡ç‚¹æ˜¯å¤šè¯­ç§è¯­éŸ³è¯†åˆ«å’ŒåŸºäºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„è¯­è¨€å»ºæ¨¡ã€‚æˆ‘ä»¬çš„æ–¹æ³•ç»“åˆäº†ç»è¿‡å¾®è°ƒåçš„Whisper-large-v3ç¼–ç å™¨ã€é«˜æ•ˆçš„æŠ•å½±ä»ªæ¶æ„å’Œå„ç§è§£ç å™¨é…ç½®ã€‚æˆ‘ä»¬é‡‡ç”¨ä¸‰é˜¶æ®µè®­ç»ƒæ–¹æ³•ï¼Œé€æ­¥ä¼˜åŒ–ç¼–ç å™¨ã€æŠ•å½±ä»ªå’ŒLLMç»„ä»¶ã€‚æˆ‘ä»¬çš„ç³»ç»Ÿåœ¨ä»…ä½¿ç”¨è§£ç å™¨è¯­è¨€æ¨¡å‹çš„æƒ…å†µä¸‹ï¼Œé€šè¿‡Gemma3-12Bå–å¾—äº†å¹³å‡è¯é”™è¯¯ç‡ï¼ˆWERï¼‰&#x2F;å­—ç¬¦é”™è¯¯ç‡ï¼ˆCERï¼‰ä¸º16.63%ï¼Œé€šè¿‡Qwen2.5-7Bå–å¾—äº†å¹³å‡è¯é”™è¯¯ç‡ä¸º18.6%ï¼Œè¡¨ç°å‡ºè‰¯å¥½çš„ç«äº‰åŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.13596v2">PDF</a> Accepted to Interspeech MLCSLM-2025 Workshop</p>
<p><strong>Summary</strong><br>æœ¬æ–‡ä»‹ç»äº†é’ˆå¯¹MLC-SLMæŒ‘æˆ˜çš„ç³»ç»Ÿç ”ç©¶ï¼Œèšç„¦äºä½¿ç”¨å¤§å‹è¯­è¨€æ¨¡å‹çš„å¤šè¯­ç§è¯­éŸ³è¯†åˆ«å’Œè¯­è¨€å»ºæ¨¡ã€‚ç ”ç©¶å›¢é˜Ÿé‡‡ç”¨ç²¾ç»†è°ƒæ•´è¿‡çš„Whisper-large-v3ç¼–ç å™¨ä¸é«˜æ•ˆæŠ•å½±æ¶æ„å’Œå„ç§è§£ç å™¨é…ç½®ç›¸ç»“åˆçš„æ–¹å¼ï¼Œå¹¶é€šè¿‡ä¸‰é˜¶æ®µè®­ç»ƒæ³•é€æ­¥ä¼˜åŒ–å„ç»„ä»¶ã€‚è¯¥ç³»ç»Ÿåœ¨æµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œä½¿ç”¨Gemma3-12Bå’ŒQwen2.5-7Bè§£ç å™¨æ¨¡å‹åˆ†åˆ«å–å¾—äº†å¹³å‡è¯é”™è¯¯ç‡ï¼ˆWERï¼‰ä¸º16.63%å’Œå¹³å‡å­—ç¬¦é”™è¯¯ç‡ï¼ˆCERï¼‰ä¸º18.6%ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç³»ç»Ÿé’ˆå¯¹MLC-SLMæŒ‘æˆ˜æ„å»ºï¼Œä¸“æ³¨äºå¤šè¯­ç§è¯­éŸ³è¯†åˆ«å’Œè¯­è¨€å»ºæ¨¡ã€‚</li>
<li>é‡‡ç”¨Whisper-large-v3ç¼–ç å™¨ç»“åˆé«˜æ•ˆæŠ•å½±æ¶æ„å’Œå¤šç§è§£ç å™¨é…ç½®ã€‚</li>
<li>é€šè¿‡ä¸‰é˜¶æ®µè®­ç»ƒæ³•é€æ­¥ä¼˜åŒ–ç¼–ç å™¨ã€æŠ•å½±å™¨å’Œè¯­è¨€æ¨¡å‹ç»„ä»¶ã€‚</li>
<li>ç³»ç»Ÿåœ¨æµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œä½¿ç”¨Gemma3-12Bè§£ç å™¨æ¨¡å‹å–å¾—å¹³å‡è¯é”™è¯¯ç‡ï¼ˆWERï¼‰ä¸º16.63%ã€‚</li>
<li>ä½¿ç”¨Qwen2.5-7Bè§£ç å™¨æ¨¡å‹å–å¾—å¹³å‡å­—ç¬¦é”™è¯¯ç‡ï¼ˆCERï¼‰ä¸º18.6%ã€‚</li>
<li>ç ”ç©¶å±•ç¤ºå¤§å‹è¯­è¨€æ¨¡å‹åœ¨è¯­éŸ³è¯†åˆ«é¢†åŸŸçš„æœ‰æ•ˆæ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.13596">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-18571333c5b40417c279091aa132e858.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-314d141c51ce3f4e82bb3df040bde2a5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-17ad4db7d352ef84fa43d9c5f63b2a3a.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Bi-directional-Context-Enhanced-Speech-Large-Language-Models-for-Multilingual-Conversational-ASR"><a href="#Bi-directional-Context-Enhanced-Speech-Large-Language-Models-for-Multilingual-Conversational-ASR" class="headerlink" title="Bi-directional Context-Enhanced Speech Large Language Models for   Multilingual Conversational ASR"></a>Bi-directional Context-Enhanced Speech Large Language Models for   Multilingual Conversational ASR</h2><p><strong>Authors:Yizhou Peng, Hexin Liu, Eng Siong Chng</strong></p>
<p>This paper introduces the integration of language-specific bi-directional context into a speech large language model (SLLM) to improve multilingual continuous conversational automatic speech recognition (ASR). We propose a character-level contextual masking strategy during training, which randomly removes portions of the context to enhance robustness and better emulate the flawed transcriptions that may occur during inference. For decoding, a two-stage pipeline is utilized: initial isolated segment decoding followed by context-aware re-decoding using neighboring hypotheses. Evaluated on the 1500-hour Multilingual Conversational Speech and Language Model (MLC-SLM) corpus covering eleven languages, our method achieves an 18% relative improvement compared to a strong baseline, outperforming even the model trained on 6000 hours of data for the MLC-SLM competition. These results underscore the significant benefit of incorporating contextual information in multilingual continuous conversational ASR. </p>
<blockquote>
<p>æœ¬æ–‡ä»‹ç»äº†å°†è¯­è¨€ç‰¹å®šçš„åŒå‘ä¸Šä¸‹æ–‡é›†æˆåˆ°è¯­éŸ³å¤§è¯­è¨€æ¨¡å‹ï¼ˆSLLMï¼‰ä¸­ï¼Œä»¥æ”¹è¿›å¤šè¯­è¨€è¿ç»­å¯¹è¯è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§å­—ç¬¦çº§ä¸Šä¸‹æ–‡æ©ç ç­–ç•¥ï¼Œåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­éšæœºç§»é™¤éƒ¨åˆ†ä¸Šä¸‹æ–‡ï¼Œä»¥å¢å¼ºæ¨¡å‹çš„ç¨³å¥æ€§ï¼Œå¹¶æ›´å¥½åœ°æ¨¡æ‹Ÿæ¨æ–­è¿‡ç¨‹ä¸­å¯èƒ½å‘ç”Ÿçš„è½¬å½•é”™è¯¯ã€‚å¯¹äºè§£ç ï¼Œæˆ‘ä»¬é‡‡ç”¨äº†ä¸¤é˜¶æ®µæµç¨‹ï¼šé¦–å…ˆæ˜¯åˆå§‹çš„å­¤ç«‹æ®µè§£ç ï¼Œç„¶åæ˜¯ä½¿ç”¨ç›¸é‚»å‡è®¾è¿›è¡Œä¸Šä¸‹æ–‡æ„ŸçŸ¥çš„é‡æ–°è§£ç ã€‚åœ¨æ¶µç›–11ç§è¯­è¨€çš„1500å°æ—¶å¤šè¯­è¨€å¯¹è¯è¯­éŸ³å’Œè¯­è¨€æ¨¡å‹ï¼ˆMLC-SLMï¼‰è¯­æ–™åº“ä¸Šè¿›è¡Œçš„è¯„ä¼°è¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•ä¸å¼ºå¤§çš„åŸºçº¿ç›¸æ¯”ï¼Œå®ç°äº†ç›¸å¯¹18%çš„æ”¹è¿›ï¼Œç”šè‡³è¶…è¶Šäº†ä¸ºMLC-SLMç«èµ›è®­ç»ƒçš„6000å°æ—¶æ•°æ®æ¨¡å‹ã€‚è¿™äº›ç»“æœå¼ºè°ƒäº†åœ¨å¤šè¯­è¨€è¿ç»­å¯¹è¯ASRä¸­èå…¥ä¸Šä¸‹æ–‡ä¿¡æ¯çš„é‡å¤§ç›Šå¤„ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.13396v2">PDF</a> Accepted By Interspeech 2025 MLC-SLM workshop as a Research Paper</p>
<p><strong>Summary</strong>ï¼š<br>æ­¤è®ºæ–‡æ¢è®¨äº†å°†è¯­è¨€ç‰¹å®šçš„åŒå‘ä¸Šä¸‹æ–‡é›†æˆåˆ°è¯­éŸ³å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆSLLMï¼‰ä¸­ï¼Œä»¥æ”¹è¿›å¤šè¯­è¨€è¿ç»­å¯¹è¯è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰ã€‚è®ºæ–‡æå‡ºåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ä½¿ç”¨å­—ç¬¦çº§åˆ«çš„ä¸Šä¸‹æ–‡æ©ç›–ç­–ç•¥ï¼Œé€šè¿‡éšæœºç§»é™¤éƒ¨åˆ†ä¸Šä¸‹æ–‡å†…å®¹æé«˜æ¨¡å‹çš„ç¨³å¥æ€§ï¼Œæ›´å¥½åœ°æ¨¡æ‹Ÿæ¨æ–­è¿‡ç¨‹ä¸­å¯èƒ½å‡ºç°çš„ç¼ºé™·è½¬å½•ã€‚è§£ç æ—¶é‡‡ç”¨ä¸¤é˜¶æ®µæµç¨‹ï¼šå…ˆè¿›è¡Œåˆæ­¥çš„ç‹¬ç«‹ç‰‡æ®µè§£ç ï¼Œå†åˆ©ç”¨é‚»è¿‘å‡è®¾è¿›è¡Œä¸Šä¸‹æ–‡æ„ŸçŸ¥çš„é‡æ–°è§£ç ã€‚åœ¨æ¶µç›–åä¸€ç§è¯­è¨€çš„1500å°æ—¶å¤šè¯­è¨€å¯¹è¯è¯­éŸ³å’Œè¯­è¨€æ¨¡å‹ï¼ˆMLC-SLMï¼‰è¯­æ–™åº“ä¸Šè¯„ä¼°ï¼Œè¯¥æ–¹æ³•ç›¸è¾ƒäºå¼ºå¤§çš„åŸºçº¿æ¨¡å‹å®ç°äº†18%çš„ç›¸å¯¹æ”¹è¿›ï¼Œç”šè‡³åœ¨MLC-SLMç«èµ›ä¸­è¡¨ç°ä¼˜äºç»è¿‡6000å°æ—¶æ•°æ®è®­ç»ƒçš„æ¨¡å‹ã€‚è¿™ä¸€ç»“æœå‡¸æ˜¾äº†åœ¨å¤šè¯­è¨€è¿ç»­å¯¹è¯ASRä¸­èå…¥ä¸Šä¸‹æ–‡ä¿¡æ¯çš„æ˜¾è‘—ä¼˜åŠ¿ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>è®ºæ–‡ä»‹ç»äº†ä¸€ç§é›†æˆè¯­è¨€ç‰¹å®šåŒå‘ä¸Šä¸‹æ–‡åˆ°è¯­éŸ³å¤§å‹è¯­è¨€æ¨¡å‹çš„æ–¹æ³•ï¼Œæ—¨åœ¨æ”¹è¿›å¤šè¯­è¨€è¿ç»­å¯¹è¯è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰ã€‚</li>
<li>æå‡ºäº†å­—ç¬¦çº§åˆ«çš„ä¸Šä¸‹æ–‡æ©ç›–ç­–ç•¥ï¼Œä»¥å¢å¼ºæ¨¡å‹çš„ç¨³å¥æ€§å¹¶æ¨¡æ‹Ÿå¯èƒ½çš„ç¼ºé™·è½¬å½•ã€‚</li>
<li>é‡‡ç”¨ä¸¤é˜¶æ®µè§£ç æµç¨‹ï¼Œå…ˆè¿›è¡Œåˆæ­¥ç‹¬ç«‹ç‰‡æ®µè§£ç ï¼Œå†è¿›è¡Œä¸Šä¸‹æ–‡æ„ŸçŸ¥çš„é‡æ–°è§£ç ã€‚</li>
<li>åœ¨æ¶µç›–å¤šç§è¯­è¨€çš„è¯­æ–™åº“ä¸Šè¯„ä¼°ï¼Œè¯¥æ–¹æ³•ç›¸è¾ƒäºåŸºçº¿æ¨¡å‹å®ç°äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚</li>
<li>ç›¸è¾ƒäºä½¿ç”¨æ›´å¤šæ•°æ®è®­ç»ƒçš„æ¨¡å‹ï¼Œæ‰€ææ–¹æ³•ä»ç„¶è¡¨ç°ä¼˜å¼‚ã€‚</li>
<li>ç»“æœçªæ˜¾äº†èå…¥ä¸Šä¸‹æ–‡ä¿¡æ¯åœ¨å¤šè¯­è¨€è¿ç»­å¯¹è¯ASRä¸­çš„é‡è¦ä½œç”¨ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.13396">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-081e0c9167b02260c7e17f93855ec1d4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7a8daa33f63b024de5dc584119aca715.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ad174257bce8b6b417541d9d214da64e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-334378040cf240dde107c32dc4077074.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-78f310ab047a401019abe0923b9cbadf.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ab894683560554c8e9dfd01b5ca6e9b1.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="NTU-Speechlab-LLM-Based-Multilingual-ASR-System-for-Interspeech-MLC-SLM-Challenge-2025"><a href="#NTU-Speechlab-LLM-Based-Multilingual-ASR-System-for-Interspeech-MLC-SLM-Challenge-2025" class="headerlink" title="NTU Speechlab LLM-Based Multilingual ASR System for Interspeech MLC-SLM   Challenge 2025"></a>NTU Speechlab LLM-Based Multilingual ASR System for Interspeech MLC-SLM   Challenge 2025</h2><p><strong>Authors:Yizhou Peng, Bin Wang, Yi-Wen Chao, Ziyang Ma, Haoyang Zhang, Hexin Liu, Xie Chen, Eng Siong Chng</strong></p>
<p>This report details the NTU Speechlab system developed for the Interspeech 2025 Multilingual Conversational Speech and Language Model (MLC-SLM) Challenge (Task I), where we achieved 5th place. We present comprehensive analyses of our multilingual automatic speech recognition system, highlighting key advancements in model architecture, data selection, and training strategies. In particular, language-specific prompts and model averaging techniques were instrumental in boosting system performance across diverse languages. Compared to the initial baseline system, our final model reduced the average Mix Error Rate from 20.2% to 10.6%, representing an absolute improvement of 9.6% (a relative improvement of 48%) on the evaluation set. Our results demonstrate the effectiveness of our approach and offer practical insights for future Speech Large Language Models. </p>
<blockquote>
<p>æœ¬æŠ¥å‘Šè¯¦ç»†ä»‹ç»äº†ä¸ºInterspeech 2025å¤šè¯­è¨€å¯¹è¯è¯­éŸ³å’Œè¯­è¨€æ¨¡å‹ï¼ˆMLC-SLMï¼‰æŒ‘æˆ˜èµ›ï¼ˆä»»åŠ¡ä¸€ï¼‰å¼€å‘çš„NTU Speechlabç³»ç»Ÿï¼Œæˆ‘ä»¬åœ¨æ¯”èµ›ä¸­è·å¾—ç¬¬äº”åã€‚æˆ‘ä»¬å¯¹æˆ‘ä»¬çš„å¤šè¯­è¨€è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ç³»ç»Ÿè¿›è¡Œäº†å…¨é¢åˆ†æï¼Œé‡ç‚¹ä»‹ç»äº†æ¨¡å‹æ¶æ„ã€æ•°æ®é€‰æ‹©å’Œè®­ç»ƒç­–ç•¥æ–¹é¢çš„å…³é”®è¿›å±•ã€‚ç‰¹åˆ«æ˜¯ï¼Œè¯­è¨€ç‰¹å®šçš„æç¤ºå’Œæ¨¡å‹å¹³å‡æŠ€æœ¯å¯¹æé«˜ä¸åŒè¯­è¨€çš„ç³»ç»Ÿæ€§èƒ½èµ·åˆ°äº†å…³é”®ä½œç”¨ã€‚ä¸åˆå§‹åŸºçº¿ç³»ç»Ÿç›¸æ¯”ï¼Œæˆ‘ä»¬çš„æœ€ç»ˆæ¨¡å‹å°†å¹³å‡æ··åˆé”™è¯¯ç‡ä»20.2%é™ä½åˆ°10.6%ï¼Œåœ¨è¯„ä¼°é›†ä¸Šå®ç°äº†9.6%çš„ç»å¯¹æ”¹è¿›ï¼ˆç›¸å¯¹æ”¹è¿›48%ï¼‰ã€‚æˆ‘ä»¬çš„ç»“æœè¯æ˜äº†æˆ‘ä»¬çš„æ–¹æ³•çš„æœ‰æ•ˆæ€§ï¼Œå¹¶ä¸ºæœªæ¥çš„è¯­éŸ³å¤§è¯­è¨€æ¨¡å‹æä¾›äº†å®é™…è§è§£ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.13339v2">PDF</a> Accepted by Interspeech 2025 MLC-SLM challenge (5th place). System   report</p>
<p><strong>Summary</strong><br>ç³»ç»Ÿå‚ä¸äº†Interspeech 2025å¤šè¯­ç§å¯¹è¯è¯­éŸ³å’Œè¯­è¨€æ¨¡å‹æŒ‘æˆ˜èµ›ï¼Œå¹¶åœ¨æ¯”èµ›ä¸­å–å¾—äº†ç¬¬äº”åçš„å¥½æˆç»©ã€‚æ–‡ç« è¯¦ç»†ä»‹ç»äº†å¼€å‘çš„NTU Speechlabç³»ç»ŸåŠå…¶å¤šè¯­ç§è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ç³»ç»Ÿçš„å…³é”®è¿›å±•ï¼ŒåŒ…æ‹¬æ¨¡å‹æ¶æ„ã€æ•°æ®é€‰æ‹©å’Œè®­ç»ƒç­–ç•¥ç­‰æ–¹é¢çš„åˆ†æã€‚é‡‡ç”¨è¯­è¨€ç‰¹å®šæç¤ºå’Œæ¨¡å‹å¹³å‡æŠ€æœ¯æé«˜äº†ç³»ç»Ÿåœ¨ä¸åŒè¯­è¨€ä¸­çš„æ€§èƒ½ã€‚æœ€ç»ˆæ¨¡å‹ç›¸è¾ƒäºåˆå§‹åŸºçº¿ç³»ç»Ÿå¹³å‡æ··åˆé”™è¯¯ç‡é™ä½äº†9.6%ï¼Œæ˜¾ç¤ºå‡ºè¯¥æ–¹æ³•çš„æœ‰æ•ˆæ€§ï¼Œå¹¶ä¸ºæœªæ¥çš„è¯­éŸ³å¤§å‹è¯­è¨€æ¨¡å‹æä¾›äº†å®é™…è§è§£ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>NTU Speechlabç³»ç»Ÿå‚ä¸äº†Interspeech 2025 MLC-SLMæŒ‘æˆ˜èµ›å¹¶è·å¾—äº†ç¬¬äº”åã€‚</li>
<li>æ–‡ç« è¯¦ç»†åˆ†æäº†è¯¥ç³»ç»Ÿçš„å¤šè¯­ç§è‡ªåŠ¨è¯­éŸ³è¯†åˆ«æŠ€æœ¯ã€‚</li>
<li>ç³»ç»Ÿæ”¹è¿›åŒ…æ‹¬æ¨¡å‹æ¶æ„ã€æ•°æ®é€‰æ‹©å’Œè®­ç»ƒç­–ç•¥ç­‰æ–¹é¢ã€‚</li>
<li>è¯­è¨€ç‰¹å®šæç¤ºå’Œæ¨¡å‹å¹³å‡æŠ€æœ¯æé«˜äº†ç³»ç»Ÿæ€§èƒ½ã€‚</li>
<li>æœ€ç»ˆæ¨¡å‹çš„å¹³å‡Mix Error Rateç›¸è¾ƒäºåˆå§‹åŸºçº¿ç³»ç»Ÿé™ä½äº†9.6%ã€‚</li>
<li>è¯¥æ–¹æ³•çš„æœ‰æ•ˆæ€§å¾—åˆ°éªŒè¯ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.13339">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-6271ccfde6e9a54b75c72836020ace90.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e9ecc8b2b371ccb62d563a94527a98f3.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-5a18d47d7e6084887dd915f1515a91f1.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-8a8bac7ecc1887d23adeb1ac8ec6f313.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-5bbab8169e4838e32d244cc729207270.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d61636e3923645158f9918dc9dfe00a3.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="Image-Generation-with-Supervised-Selection-Based-on-Multimodal-Features-for-Semantic-Communications"><a href="#Image-Generation-with-Supervised-Selection-Based-on-Multimodal-Features-for-Semantic-Communications" class="headerlink" title="Image Generation with Supervised Selection Based on Multimodal Features   for Semantic Communications"></a>Image Generation with Supervised Selection Based on Multimodal Features   for Semantic Communications</h2><p><strong>Authors:Chengyang Liang, Dong Li</strong></p>
<p>Semantic communication (SemCom) has emerged as a promising technique for the next-generation communication systems, in which the generation at the receiver side is allowed with semantic featuresâ€™ recovery. However, the majority of existing research predominantly utilizes a singular type of semantic information, such as text, images, or speech, to supervise and choose the generated source signals, which may not sufficiently encapsulate the comprehensive and accurate semantic information, and thus creating a performance bottleneck. In order to bridge this gap, in this paper, we propose and investigate a SemCom framework using multimodal information to supervise the generated image. To be specific, in this framework, we first extract semantic features at both the image and text levels utilizing the Convolutional Neural Network (CNN) architecture and the Contrastive Language-Image Pre-Training (CLIP) model before transmission. Then, we employ a generative diffusion model at the receiver to generate multiple images. In order to ensure the accurate extraction and facilitate high-fidelity image reconstruction, we select the â€œbestâ€ image with the minimum reconstruction errors by taking both the aided image and text semantic features into account. We further extend multimodal semantic communication (MMSemCom) system to the multiuser scenario for orthogonal transmission. Experimental results demonstrate that the proposed framework can not only achieve the enhanced fidelity and robustness in image transmission compared with existing communication systems but also sustain a high performance in the low signal-to-noise ratio (SNR) conditions. </p>
<blockquote>
<p>è¯­ä¹‰é€šä¿¡ï¼ˆSemComï¼‰å·²æˆä¸ºä¸‹ä¸€ä»£é€šä¿¡ç³»ç»Ÿçš„æœ‰å‰é€”çš„æŠ€æœ¯ã€‚åœ¨è¯¥æŠ€æœ¯ä¸­ï¼Œæ¥æ”¶ç«¯çš„ç”Ÿæˆå…è®¸æ¢å¤è¯­ä¹‰ç‰¹å¾ã€‚ç„¶è€Œï¼Œç°æœ‰çš„å¤§å¤šæ•°ç ”ç©¶ä¸»è¦åˆ©ç”¨å•ä¸€ç±»å‹çš„è¯­ä¹‰ä¿¡æ¯ï¼ˆå¦‚æ–‡æœ¬ã€å›¾åƒæˆ–è¯­éŸ³ï¼‰æ¥ç›‘ç£å’Œé€‰æ‹©ç”Ÿæˆçš„æºä¿¡å·ã€‚è¿™ç§æ–¹å¼å¯èƒ½æ— æ³•å……åˆ†åŒ…å«å…¨é¢å’Œå‡†ç¡®çš„è¯­ä¹‰ä¿¡æ¯ï¼Œä»è€Œé™åˆ¶äº†æ€§èƒ½æå‡ã€‚ä¸ºäº†å¼¥è¡¥è¿™ä¸€å·®è·ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ä¸ªä½¿ç”¨å¤šæ¨¡æ€ä¿¡æ¯ç›‘ç£ç”Ÿæˆå›¾åƒçš„SemComæ¡†æ¶ï¼Œå¹¶è¿›è¡Œäº†æ·±å…¥ç ”ç©¶ã€‚å…·ä½“æ¥è¯´ï¼Œåœ¨æ­¤æ¡†æ¶ä¸­ï¼Œæˆ‘ä»¬é¦–å…ˆåœ¨ä¼ è¾“å‰åˆ©ç”¨å·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNï¼‰æ¶æ„å’Œå¯¹æ¯”è¯­è¨€å›¾åƒé¢„è®­ç»ƒï¼ˆCLIPï¼‰æ¨¡å‹åœ¨å›¾åƒå’Œæ–‡æœ¬å±‚é¢æå–è¯­ä¹‰ç‰¹å¾ã€‚ç„¶åï¼Œæˆ‘ä»¬åœ¨æ¥æ”¶ç«¯é‡‡ç”¨ç”Ÿæˆæ‰©æ•£æ¨¡å‹ç”Ÿæˆå¤šä¸ªå›¾åƒã€‚ä¸ºäº†ç¡®ä¿å‡†ç¡®çš„æå–å’Œé«˜è´¨é‡çš„å›¾åƒé‡å»ºï¼Œæˆ‘ä»¬é€šè¿‡è€ƒè™‘è¾…åŠ©å›¾åƒå’Œæ–‡æœ¬è¯­ä¹‰ç‰¹å¾æ¥é€‰æ‹©é‡å»ºè¯¯å·®æœ€å°çš„â€œæœ€ä½³â€å›¾åƒã€‚æˆ‘ä»¬å°†å¤šæ¨¡æ€è¯­ä¹‰é€šä¿¡ï¼ˆMMSemComï¼‰ç³»ç»Ÿè¿›ä¸€æ­¥æ‰©å±•åˆ°å¤šç”¨æˆ·åœºæ™¯ï¼Œä»¥å®ç°æ­£äº¤ä¼ è¾“ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæ‰€æå‡ºçš„æ¡†æ¶ä¸ç°æœ‰é€šä¿¡ç³»ç»Ÿç›¸æ¯”ï¼Œåœ¨å›¾åƒä¼ è¾“ä¸­ä¸ä»…æé«˜äº†ä¿çœŸåº¦å’Œç¨³å¥æ€§ï¼Œè€Œä¸”åœ¨ä½ä¿¡å™ªæ¯”ï¼ˆSNRï¼‰æ¡ä»¶ä¸‹ä¹Ÿä¿æŒäº†é«˜æ€§èƒ½ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2411.17428v3">PDF</a> </p>
<p><strong>æ‘˜è¦</strong><br>æœ¬ç ”ç©¶é’ˆå¯¹è¯­ä¹‰é€šä¿¡ï¼ˆSemComï¼‰ä¸­çš„æ€§èƒ½ç“¶é¢ˆé—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§åŸºäºå¤šæ¨¡æ€ä¿¡æ¯çš„SemComæ¡†æ¶ã€‚è¯¥æ¡†æ¶åœ¨ä¼ è¾“å‰åˆ©ç”¨å·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNï¼‰å’Œå¯¹æ¯”è¯­è¨€å›¾åƒé¢„è®­ç»ƒï¼ˆCLIPï¼‰æ¨¡å‹æå–å›¾åƒå’Œæ–‡æœ¬çº§åˆ«çš„è¯­ä¹‰ç‰¹å¾ã€‚åœ¨æ¥æ”¶ç«¯é‡‡ç”¨ç”Ÿæˆæ€§æ‰©æ•£æ¨¡å‹ç”Ÿæˆå¤šå¼ å›¾åƒï¼Œå¹¶é€šè¿‡è€ƒè™‘è¾…åŠ©å›¾åƒå’Œæ–‡æœ¬è¯­ä¹‰ç‰¹å¾æ¥é€‰å–é‡å»ºè¯¯å·®æœ€å°çš„â€œæœ€ä½³â€å›¾åƒã€‚æ­¤å¤–ï¼Œè¯¥ç ”ç©¶è¿˜å°†å¤šæ¨¡æ€è¯­ä¹‰é€šä¿¡ï¼ˆMMSemComï¼‰ç³»ç»Ÿæ‰©å±•åˆ°å¤šç”¨æˆ·åœºæ™¯ï¼Œå®ç°æ­£äº¤ä¼ è¾“ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ¡†æ¶åœ¨å›¾åƒä¼ è¾“ä¸­ä¸ä»…æé«˜äº†ä¿çœŸåº¦å’Œç¨³å¥æ€§ï¼Œè€Œä¸”åœ¨ä½ä¿¡å™ªæ¯”æ¡ä»¶ä¸‹ä¹Ÿä¿æŒäº†é«˜æ€§èƒ½ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>å¤šæ¨¡æ€ä¿¡æ¯è¢«ç”¨äºç›‘ç£ç”Ÿæˆçš„å›¾åƒï¼Œæ—¨åœ¨è§£å†³ç°æœ‰SemComä¸­æ€§èƒ½ç“¶é¢ˆçš„é—®é¢˜ã€‚</li>
<li>åˆ©ç”¨CNNå’ŒCLIPæ¨¡å‹åœ¨ä¼ è¾“å‰æå–å›¾åƒå’Œæ–‡æœ¬çº§åˆ«çš„è¯­ä¹‰ç‰¹å¾ã€‚</li>
<li>åœ¨æ¥æ”¶ç«¯é‡‡ç”¨ç”Ÿæˆæ€§æ‰©æ•£æ¨¡å‹ç”Ÿæˆå¤šä¸ªå›¾åƒã€‚</li>
<li>é€šè¿‡è€ƒè™‘è¾…åŠ©å›¾åƒå’Œæ–‡æœ¬è¯­ä¹‰ç‰¹å¾ï¼Œé€‰æ‹©é‡å»ºè¯¯å·®æœ€å°çš„â€œæœ€ä½³â€å›¾åƒã€‚</li>
<li>å¤šæ¨¡æ€è¯­ä¹‰é€šä¿¡ï¼ˆMMSemComï¼‰ç³»ç»Ÿè¢«æ‰©å±•åˆ°å¤šç”¨æˆ·åœºæ™¯ï¼Œæ”¯æŒæ­£äº¤ä¼ è¾“ã€‚</li>
<li>å®éªŒç»“æœæ˜¾ç¤ºï¼Œæ–°æ¡†æ¶åœ¨å›¾åƒä¼ è¾“çš„ä¿çœŸåº¦å’Œç¨³å¥æ€§ä¸Šæœ‰æ‰€æå‡ã€‚</li>
<li>è¯¥æ¡†æ¶åœ¨ä½ä¿¡å™ªæ¯”æ¡ä»¶ä¸‹ä¹Ÿèƒ½ä¿æŒé«˜æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2411.17428">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-dfeab85621bb19d299102e3b3c6fc756.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3baac709a459885445243332b039e8ff.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-1b666c23f70e4266cfc6ae42505c44c6.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-71c7faaa7327b4be31a3747d7038a817.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-21f1d8c7ede8bb2252e92fc08e2ab923.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-07-09/Speech/">https://kedreamix.github.io/Talk2Paper/Paper/2025-07-09/Speech/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Speech/">
                                    <span class="chip bg-color">Speech</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-07-09/Face%20Swapping/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-540af709c16fde65f6f1c101677775cb.jpg" class="responsive-img" alt="Face Swapping">
                        
                        <span class="card-title">Face Swapping</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Face Swapping æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-07-09  De-Fake Style based Anomaly Deepfake Detection
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-07-09
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Face-Swapping/" class="post-category">
                                    Face Swapping
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Face-Swapping/">
                        <span class="chip bg-color">Face Swapping</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-07-09/%E6%97%A0%E7%9B%91%E7%9D%A3_%E5%8D%8A%E7%9B%91%E7%9D%A3_%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0/">
                    <div class="card-image">
                        
                        <img src="https://pica.zhimg.com/v2-6eca5caf497be878e7106b53e04ff5c5.jpg" class="responsive-img" alt="æ— ç›‘ç£/åŠç›‘ç£/å¯¹æ¯”å­¦ä¹ ">
                        
                        <span class="card-title">æ— ç›‘ç£/åŠç›‘ç£/å¯¹æ¯”å­¦ä¹ </span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            æ— ç›‘ç£/åŠç›‘ç£/å¯¹æ¯”å­¦ä¹  æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-07-09  CLIP-RL Surgical Scene Segmentation Using Contrastive Language-Vision   Pretraining & Reinforcement Learning
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-07-09
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E6%97%A0%E7%9B%91%E7%9D%A3-%E5%8D%8A%E7%9B%91%E7%9D%A3-%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0/" class="post-category">
                                    æ— ç›‘ç£/åŠç›‘ç£/å¯¹æ¯”å­¦ä¹ 
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E6%97%A0%E7%9B%91%E7%9D%A3-%E5%8D%8A%E7%9B%91%E7%9D%A3-%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0/">
                        <span class="chip bg-color">æ— ç›‘ç£/åŠç›‘ç£/å¯¹æ¯”å­¦ä¹ </span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">32883.8k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
