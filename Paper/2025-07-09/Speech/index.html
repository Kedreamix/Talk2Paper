<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Speech">
    <meta name="description" content="Speech 方向最新论文已更新，请持续关注 Update in 2025-07-09  Grounded Gesture Generation Language, Motion, and Space">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Speech | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loader页面消失采用渐隐的方式*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logo出现动画 */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//使用渐隐的方法淡出loading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//强制显示loading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">嘘~  正在从服务器偷取页面 . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>首页</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>标签</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>分类</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>归档</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="搜索" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="深色/浅色模式" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			首页
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			标签
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			分类
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			归档
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pic1.zhimg.com/v2-ad174257bce8b6b417541d9d214da64e.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Speech</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- 文章内容详情 -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Speech/">
                                <span class="chip bg-color">Speech</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Speech/" class="post-category">
                                Speech
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp;
                    2025-07-09
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>更新日期:&nbsp;&nbsp;
                    2025-07-09
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>文章字数:&nbsp;&nbsp;
                    8.4k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>阅读时长:&nbsp;&nbsp;
                    34 分
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>阅读次数:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>⚠️ 以下所有内容总结都来自于 大语言模型的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p>
</blockquote>
<h1 id="2025-07-09-更新"><a href="#2025-07-09-更新" class="headerlink" title="2025-07-09 更新"></a>2025-07-09 更新</h1><h2 id="Grounded-Gesture-Generation-Language-Motion-and-Space"><a href="#Grounded-Gesture-Generation-Language-Motion-and-Space" class="headerlink" title="Grounded Gesture Generation: Language, Motion, and Space"></a>Grounded Gesture Generation: Language, Motion, and Space</h2><p><strong>Authors:Anna Deichler, Jim O’Regan, Teo Guichoux, David Johansson, Jonas Beskow</strong></p>
<p>Human motion generation has advanced rapidly in recent years, yet the critical problem of creating spatially grounded, context-aware gestures has been largely overlooked. Existing models typically specialize either in descriptive motion generation, such as locomotion and object interaction, or in isolated co-speech gesture synthesis aligned with utterance semantics. However, both lines of work often treat motion and environmental grounding separately, limiting advances toward embodied, communicative agents. To address this gap, our work introduces a multimodal dataset and framework for grounded gesture generation, combining two key resources: (1) a synthetic dataset of spatially grounded referential gestures, and (2) MM-Conv, a VR-based dataset capturing two-party dialogues. Together, they provide over 7.7 hours of synchronized motion, speech, and 3D scene information, standardized in the HumanML3D format. Our framework further connects to a physics-based simulator, enabling synthetic data generation and situated evaluation. By bridging gesture modeling and spatial grounding, our contribution establishes a foundation for advancing research in situated gesture generation and grounded multimodal interaction.   Project page: <a target="_blank" rel="noopener" href="https://groundedgestures.github.io/">https://groundedgestures.github.io/</a> </p>
<blockquote>
<p>人类动作生成技术在近年来已经迅速发展，但创建具有空间性和上下文感知的手势这一关键问题却被大大忽视了。现有模型通常专注于描述性动作生成，如行走和对象交互，或在与话语语义对齐的孤立语音手势合成中。然而，这两类工作通常将动作和环境基础分开处理，限制了向着具体化、交际性代理的发展。为了弥补这一空白，我们的工作引入了基于基础手势生成的多模态数据集和框架，结合了两种关键资源：（1）空间基础参照手势的合成数据集，（2）基于VR的MM-Conv数据集，捕捉双方对话。它们共同提供了超过7.7小时同步的动作、语音和3D场景信息，以HumanML3D格式标准化。我们的框架进一步与基于物理的模拟器相连，能够实现合成数据的生成和情境评估。通过桥接手势建模和空间基础，我们的贡献为情境手势生成和基础多模式交互的研究奠定了基础。项目页面：<a target="_blank" rel="noopener" href="https://groundedgestures.github.io/">https://groundedgestures.github.io/</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.04522v1">PDF</a> Accepted as a non-archival paper at the CVPR 2025 Humanoid Agents   Workshop. Project page: <a target="_blank" rel="noopener" href="https://groundedgestures.github.io/">https://groundedgestures.github.io</a></p>
<p><strong>Summary</strong></p>
<p>本文关注人类动作生成领域中的空间背景与语境感知手势的生成问题。现有模型主要专注于描述性动作生成或与语言语义对齐的孤立性共语手势合成，但往往将动作与环境背景分离处理，阻碍了身体性沟通代理的发展。本文提出一个综合数据集与框架来解决这一问题，包含合成的手势参考数据集和基于VR的双向对话数据集，提供超过7.7小时的同步动作、语音和3D场景信息。此外，该框架与物理模拟器连接，促进合成数据的生成和情境评估。通过连接手势建模和空间背景，本文建立了一个研究情境手势生成和基于背景的多模态交互的基础。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>人类动作生成领域存在空间背景与语境感知手势生成的不足。</li>
<li>现有模型在动作与环境背景的处理上存在分离现象，限制了进展。</li>
<li>本文提出一个综合数据集用于解决这一问题，包含合成手势参考数据集和基于VR的双向对话数据集。</li>
<li>数据集提供了超过7.7小时的同步动作、语音和3D场景信息。</li>
<li>提出的数据集采用HumanML3D格式标准化，支持情境评估和合成数据生成。</li>
<li>通过连接手势建模和空间背景，为情境手势生成和多模态交互研究奠定基础。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.04522">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-40d422cc5bd53ccacae648baac8a9b4e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-3f56b83248c7ad3438a616576cbdc793.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-14eb942276f77b9a2739f79694dd8ecb.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-13663982cae82b5a4c6084d675983569.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c5c18d6cc3c19b8db21ccf258d9cf750.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-79b1611a1ef0c11e26a1488fa747e370.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6e83b0a17017a783581dc846c0f46d4e.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Long-Context-Modeling-Networks-for-Monaural-Speech-Enhancement-A-Comparative-Study"><a href="#Long-Context-Modeling-Networks-for-Monaural-Speech-Enhancement-A-Comparative-Study" class="headerlink" title="Long-Context Modeling Networks for Monaural Speech Enhancement: A   Comparative Study"></a>Long-Context Modeling Networks for Monaural Speech Enhancement: A   Comparative Study</h2><p><strong>Authors:Qiquan Zhang, Moran Chen, Zeyang Song, Hexin Liu, Xiangyu Zhang, Haizhou Li</strong></p>
<p>Advanced long-context modeling backbone networks, such as Transformer, Conformer, and Mamba, have demonstrated state-of-the-art performance in speech enhancement. However, a systematic and comprehensive comparative study of these backbones within a unified speech enhancement framework remains lacking. In addition, xLSTM, a more recent and efficient variant of LSTM, has shown promising results in language modeling and as a general-purpose vision backbone. In this paper, we investigate the capability of xLSTM in speech enhancement, and conduct a comprehensive comparison and analysis of the Transformer, Conformer, Mamba, and xLSTM backbones within a unified framework, considering both causal and noncausal configurations. Overall, xLSTM and Mamba achieve better performance than Transformer and Conformer. Mamba demonstrates significantly superior training and inference efficiency, particularly for long speech inputs, whereas xLSTM suffers from the slowest processing speed. </p>
<blockquote>
<p>先进的长期上下文建模主干网络，如Transformer、Conformer和Mamba，在语音增强方面已经表现出了卓越的性能。然而，在一个统一的语音增强框架内，对这些主干网进行系统和全面的比较研究仍然缺乏。此外，xLSTM作为LSTM的更新、更高效的形式，在语言建模和通用视觉主干方面已经展现出有前景的结果。在本文中，我们研究了xLSTM在语音增强方面的能力，并在一个统一的框架内对Transformer、Conformer、Mamba和xLSTM主干进行了全面的比较和分析，同时考虑了因果和非因果配置。总体而言，xLSTM和Mamba的性能优于Transformer和Conformer。Mamba在训练和推理效率方面表现出显著的优势，特别是对于长语音输入，而xLSTM的处理速度最慢。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.04368v1">PDF</a> Accepted by WASPAA 2025, 5 pages</p>
<p><strong>Summary</strong><br>高级长上下文建模主干网络如Transformer、Conformer和Mamba在语音增强方面表现出卓越的性能。然而，缺乏在一个统一的语音增强框架下对这些主干网进行系统和全面的比较研究。本研究探讨了xLSTM在语音增强中的能力，并在统一框架下对Transformer、Conformer、Mamba和xLSTM进行了全面的比较与分析，考虑了因果和非因果配置。总体而言，xLSTM和Mamba的性能优于Transformer和Conformer。Mamba在训练与推理效率上表现显著优越，尤其对于长语音输入，而xLSTM处理速度较慢。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>高级长上下文建模主干网络如Transformer、Conformer和Mamba在语音增强方面表现出卓越性能。</li>
<li>缺乏在一个统一的语音增强框架下对这些主干网络进行系统性和全面性的比较研究。</li>
<li>xLSTM在语音增强中的能力得到了研究。</li>
<li>在统一框架下进行了Transformer、Conformer、Mamba和xLSTM的全面比较。</li>
<li>xLSTM和Mamba的性能总体优于Transformer和Conformer。</li>
<li>Mamba在训练和推理效率上表现显著优越，尤其适用于长语音输入。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.04368">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-3315b59336bd6eeb85b7b2375c74a39a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-6745d1a0e0449159ffbb30f59400021a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-bf996eb0c619d9c3aba5daa50e112e6b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d50b636e27c763ef1f4d87022969c921.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-aed285bd64fbd0be5e7450db9e86e440.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4dc0dc0d4936999f9b554f8007587836.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="MMMOS-Multi-domain-Multi-axis-Audio-Quality-Assessment"><a href="#MMMOS-Multi-domain-Multi-axis-Audio-Quality-Assessment" class="headerlink" title="MMMOS: Multi-domain Multi-axis Audio Quality Assessment"></a>MMMOS: Multi-domain Multi-axis Audio Quality Assessment</h2><p><strong>Authors:Yi-Cheng Lin, Jia-Hung Chen, Hung-yi Lee</strong></p>
<p>Accurate audio quality estimation is essential for developing and evaluating audio generation, retrieval, and enhancement systems. Existing non-intrusive assessment models predict a single Mean Opinion Score (MOS) for speech, merging diverse perceptual factors and failing to generalize beyond speech. We propose MMMOS, a no-reference, multi-domain audio quality assessment system that estimates four orthogonal axes: Production Quality, Production Complexity, Content Enjoyment, and Content Usefulness across speech, music, and environmental sounds. MMMOS fuses frame-level embeddings from three pretrained encoders (WavLM, MuQ, and M2D) and evaluates three aggregation strategies with four loss functions. By ensembling the top eight models, MMMOS shows a 20-30% reduction in mean squared error and a 4-5% increase in Kendall’s {\tau} versus baseline, gains first place in six of eight Production Complexity metrics, and ranks among the top three on 17 of 32 challenge metrics. </p>
<blockquote>
<p>准确的音频质量评估对于开发和评估音频生成、检索和增强系统至关重要。现有的非侵入性评估模型为语音预测单一的主观意见得分（MOS），融合各种感知因素，但无法推广到语音之外。我们提出MMMOS，这是一种无参考、多领域的音频质量评估系统，可以估计四个正交轴：生产质量、生产复杂性、内容享受和内容实用性，涵盖语音、音乐和环境声音。MMMOS融合了三个预训练编码器（WavLM、MuQ和M2D）的帧级嵌入，并评价了四种损失函数下的三种聚合策略。通过对前八名模型的综合评估，MMMOS将均方误差降低了20%-30%，Kendall系数提高了4%-5%，相较于基线在六个生产复杂性指标中获得第一名，并在32个挑战指标中有17个进入前三名。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.04094v1">PDF</a> 4 pages including 1 page of reference. ASRU Audio MOS 2025 Challenge   paper</p>
<p><strong>摘要</strong></p>
<p>无参考、多领域的音频质量评估系统MMMOS，对语音、音乐和环境声音的质量进行更全面的评估。它通过融合三种预训练编码器（WavLM、MuQ和M2D）的帧级嵌入，评价了三种聚合策略和四种损失函数。MMMOS在多个评估指标上表现优异，相对于基准测试，均方误差降低了20-30%，Kendall的τ系数提高了4-5%，在生产复杂度指标中的六个项目中获得第一名，并在32个挑战指标中的17个项目上跻身前三名。</p>
<p><strong>要点</strong></p>
<ol>
<li>音频质量评估在音频生成、检索和增强系统的开发与评估中至关重要。</li>
<li>现有非侵入式评估模型在语音方面的预测存在局限性，无法概括不同的感知因素。</li>
<li>MMMOS系统是一种无参考、多领域的音频质量评估方法，可估计四个正交轴：生产质量、生产复杂度、内容享受和内容有用性。</li>
<li>MMMOS融合了三种预训练编码器的帧级嵌入，并评价了不同的聚合策略和损失函数。</li>
<li>MMMOS在多个评估指标上表现优秀，相对基准测试有显著改善。</li>
<li>MMMOS在生产复杂度指标中的六个项目中获得第一名，并在大多数挑战指标中跻身前三名。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.04094">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-91b229abd67139e573ce9dd8e5d032a7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ce2155c076f8b79d65b948d563f0db9c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-cae4988b93c4096cf0cf50b919d33b5a.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-fccacb0645ec072f406c1859d63e4f3d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-eff242a616b5965dc6d4a77212239126.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="CLEP-DG-Contrastive-Learning-for-Speech-Emotion-Domain-Generalization-via-Soft-Prompt-Tuning"><a href="#CLEP-DG-Contrastive-Learning-for-Speech-Emotion-Domain-Generalization-via-Soft-Prompt-Tuning" class="headerlink" title="CLEP-DG: Contrastive Learning for Speech Emotion Domain Generalization   via Soft Prompt Tuning"></a>CLEP-DG: Contrastive Learning for Speech Emotion Domain Generalization   via Soft Prompt Tuning</h2><p><strong>Authors:Jiacheng Shi, Yanfu Zhang, Ye Gao</strong></p>
<p>Speech Emotion Recognition (SER) is fundamental to affective computing and human-computer interaction, yet existing models struggle to generalize across diverse acoustic conditions. While Contrastive Language-Audio Pretraining (CLAP) provides strong multimodal alignment, it lacks dedicated mechanisms for capturing emotional cues, making it suboptimal for SER. To address this, we propose CLEP-DG, a framework that enhances CLAP’s robustness in emotion recognition. First, we fine-tune CLAP to obtain CLEP, adapting it on large-scale emotional speech datasets to better encode emotion-relevant features. Then, we introduce Acoustic Context Prompt Tuning (ACPT), a text-driven augmentation strategy that optimizes learnable prompt vectors to model diverse acoustic environments without additional labeled audio. Finally, leveraging cross-modal transferability, we train a classifier on text-derived embeddings and apply it to the audio encoder during inference, mitigating domain shifts between textual supervision and audio-based emotion recognition. Experiments across five benchmark datasets show that CLEP-DG outperforms prior CLAP-based approaches, achieving state-of-the-art performance in both supervised and domain generalization settings. </p>
<blockquote>
<p>语音情感识别（SER）对于情感计算和人机交互至关重要，但现有模型在不同的声学条件下难以进行推广。虽然对比语言音频预训练（CLAP）提供了强大的多模式对齐，但它缺乏捕捉情感线索的专门机制，使其不适用于SER。为了解决这一问题，我们提出了CLEP-DG框架，该框架增强了CLAP在情感识别方面的稳健性。首先，我们对CLAP进行微调，获得CLEP，通过大规模情感语音数据集进行适应，以更好地编码与情感相关的特征。然后，我们引入了声学上下文提示调整（ACPT），这是一种文本驱动的数据增强策略，通过优化可学习的提示向量来模拟各种声学环境，而无需额外的标记音频。最后，利用跨模态迁移性，我们在文本派生嵌入上训练一个分类器，并在推理期间将其应用于音频编码器，从而缓解文本监督和基于音频的情感识别之间的域转移问题。在五个基准数据集上的实验表明，CLEP-DG在基于CLAP的方法上表现更好，在监督学习和域泛化设置中均达到了最先进的性能。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.04048v1">PDF</a> Accepted to Interspeech2025</p>
<p><strong>Summary</strong></p>
<p>本文提出了CLEP-DG框架，旨在增强Contrastive Language-Audio Pretraining（CLAP）在情感识别方面的稳健性。通过微调CLAP获得CLEP，并引入Acoustic Context Prompt Tuning（ACPT）策略，优化可学习提示向量以模拟不同的声学环境。实验结果表明，CLEP-DG在五个基准数据集上的性能优于基于CLAP的方法，实现了监督学习和领域泛化设置中的最佳性能。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Speech Emotion Recognition (SER) 在情感计算和人机交互中扮演重要角色，但现有模型在多样化声学条件下的泛化能力有限。</li>
<li>Contrastive Language-Audio Pretraining (CLAP) 提供强大的多模式对齐，但缺乏捕捉情感线索的专门机制，对于SER来说不够理想。</li>
<li>CLEP-DG框架通过微调CLAP并引入Acoustic Context Prompt Tuning (ACPT)策略，增强了CLAP在情感识别方面的稳健性。</li>
<li>ACPT是一种文本驱动的数据增强策略，通过优化可学习提示向量来模拟不同的声学环境，无需额外的标记音频。</li>
<li>利用跨模态可迁移性，CLEP-DG训练了一个基于文本的嵌入分类器，并将其应用于音频编码器进行推断，减轻了文本监督和音频情感识别之间的领域差异。</li>
<li>实验结果展示了CLEP-DG在多个基准数据集上的卓越性能，优于基于CLAP的方法。</li>
<li>CLEP-DG实现了监督学习和领域泛化设置中的最佳性能。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.04048">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-4ce5b268837532a90ad1a771724c75e8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e1eb9596f71b395764abed8e6e1437c5.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ecfb790615e83c3b4329d34dc223e659.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4750169df1fea6cfbdfd48506e96a039.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-75513347892ed5e9a38bac353cfb63fd.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-bbbc2f1d04d62a73881175f129277570.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="RECA-PD-A-Robust-Explainable-Cross-Attention-Method-for-Speech-based-Parkinson’s-Disease-Classification"><a href="#RECA-PD-A-Robust-Explainable-Cross-Attention-Method-for-Speech-based-Parkinson’s-Disease-Classification" class="headerlink" title="RECA-PD: A Robust Explainable Cross-Attention Method for Speech-based   Parkinson’s Disease Classification"></a>RECA-PD: A Robust Explainable Cross-Attention Method for Speech-based   Parkinson’s Disease Classification</h2><p><strong>Authors:Terry Yi Zhong, Cristian Tejedor-Garcia, Martha Larson, Bastiaan R. Bloem</strong></p>
<p>Parkinson’s Disease (PD) affects over 10 million people globally, with speech impairments often preceding motor symptoms by years, making speech a valuable modality for early, non-invasive detection. While recent deep-learning models achieve high accuracy, they typically lack the explainability required for clinical use. To address this, we propose RECA-PD, a novel, robust, and explainable cross-attention architecture that combines interpretable speech features with self-supervised representations. RECA-PD matches state-of-the-art performance in Speech-based PD detection while providing explanations that are more consistent and more clinically meaningful. Additionally, we demonstrate that performance degradation in certain speech tasks (e.g., monologue) can be mitigated by segmenting long recordings. Our findings indicate that performance and explainability are not necessarily mutually exclusive. Future work will enhance the usability of explanations for non-experts and explore severity estimation to increase the real-world clinical relevance. </p>
<blockquote>
<p>帕金森病（PD）全球影响超过1000万人，言语障碍通常数年前先于运动症状出现，这使得言语成为早期、无创检测的有价值方式。虽然最近的深度学习模型具有很高的准确性，但它们通常缺乏临床使用所需的解释性。为解决这一问题，我们提出了RECA-PD，这是一种新颖、稳健且可解释的关注交叉架构，它结合了可解释的语音特征与自监督表示。RECA-PD在基于语音的PD检测方面达到了最先进的表现，同时提供了更加一致和临床意义更强的解释。此外，我们证明通过分段长录音可以缓解某些语音任务（如独白）中的性能下降。我们的研究结果表明，性能和解释性并不一定相互排斥。未来的工作将提高解释对于非专家的可用性，并探索严重程度评估，以增加现实世界中的临床相关性。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.03594v1">PDF</a> Accepted for TSD 2025</p>
<p><strong>Summary</strong>：帕金森病影响全球超过一千万人群，语言障碍往往早于运动症状数年出现，使得语言成为早期非侵入性检测的重要模态。针对缺乏临床使用所需解释性的问题，我们提出了RECA-PD模型，这是一种新颖、稳健且可解释的交叉注意力架构，结合了可解释的语言特征和自监督表示。RECA-PD在基于语音的帕金森病检测方面达到了先进水平，同时提供了更加一致和临床意义的解释。此外，我们的研究还显示通过分段长录音可以改善某些语音任务的性能下降问题。我们的发现表明性能和解释性并不一定相互排斥。未来的工作将提高解释对非专业人士的可用性，并探索严重程度评估以增加其在现实世界中的临床相关性。</p>
<p><strong>Key Takeaways</strong>：</p>
<ol>
<li>帕金森病影响全球超过一千万人，语言障碍在早期出现。</li>
<li>当前深度模型在语音帕金森病检测方面表现出高准确性，但缺乏必要的解释性。</li>
<li>RECA-PD模型结合了可解释的语言特征和自监督表示，实现了高水平的性能并提供了解释性。</li>
<li>RECA-PD模型提供的解释更具一致性和临床意义。</li>
<li>对长录音进行分段可以改善某些语音任务的性能下降问题。</li>
<li>研究表明性能和解释性不必相互排斥。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.03594">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-d68a7fd7a4c344374dcf2c81b70d7c01.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ae10fd673179cc93df808ddab5d7cba9.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="SHNU-Multilingual-Conversational-Speech-Recognition-System-for-INTERSPEECH-2025-MLC-SLM-Challenge"><a href="#SHNU-Multilingual-Conversational-Speech-Recognition-System-for-INTERSPEECH-2025-MLC-SLM-Challenge" class="headerlink" title="SHNU Multilingual Conversational Speech Recognition System for   INTERSPEECH 2025 MLC-SLM Challenge"></a>SHNU Multilingual Conversational Speech Recognition System for   INTERSPEECH 2025 MLC-SLM Challenge</h2><p><strong>Authors:Yuxiang Mei, Yuang Zheng, Dongxing Xu, Yanhua Long</strong></p>
<p>This paper describes SHNU multilingual conversational speech recognition system (SHNU-mASR, team name-“maybe”), submitted to Track 1 of the INTERSPEECH 2025 MLC-SLM Challenge. Our system integrates a parallel-speech-encoder architecture with a large language model (LLM) to form a unified multilingual ASR framework. The parallel-speech-encoder consists of two pre-trained encoders, the Whisper-large-v3 encoder and mHuBERT-147 encoder. Their output embeddings are concatenated and fed into the LLM, enabling the model to leverage complementary acoustic and linguistic knowledge and achieve competitive performance. Moreover, we adopt a tri-stage training strategy to jointly update the low-rank adaptation modules and projector parameters of both the speech encoders and the LLM. In addition, we incorporate an additional language-aware prompt at the LLM input to enhance language-specific text generation. The SHNU-mASR system achieves an overall character&#x2F;word error rate (CER&#x2F;WER) of 11.76% on the blind evaluation set of the challenge, outperforming the official MLC-SLM baseline by 8.41 absolute CER&#x2F;WER, without increasing the baseline training data. </p>
<blockquote>
<p>本文介绍了SHNU多语种对话语音识别系统（SHNU-mASR，团队名为“maybe”），该系统提交至INTERSPEECH 2025 MLC-SLM Challenge的Track 1。我们的系统将并行语音编码器架构与大型语言模型（LLM）相结合，形成一个统一的多语种ASR框架。并行语音编码器由两个预训练编码器组成，即Whisper-large-v3编码器和mHuBERT-147编码器。他们的输出嵌入进行拼接，并输入到LLM中，使模型能够利用互补的声学和语言知识，实现有竞争力的性能。此外，我们采用三阶段训练策略，联合更新低秩适应模块和语音编码器和LLM的投影仪参数。另外，我们在LLM的输入端增加了一个额外的语言感知提示，以增强特定语言的文本生成。SHNU-mASR系统在挑战盲评测数据集上实现了总体字符&#x2F;单词错误率（CER&#x2F;WER）为11.76%，比官方MLC-SLM基线高出8.41个绝对CER&#x2F;WER，且未增加基线训练数据。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.03343v1">PDF</a> Accepted by Interspeech 2025 MLC-SLM workshop</p>
<p><strong>Summary</strong></p>
<p>本文介绍了SHNU多语种对话语音识别系统（SHNU-mASR，团队名为“maybe”）。该系统结合了并行语音编码器架构和大型语言模型（LLM），形成一个统一的多语种ASR框架。通过采用双编码器结构、三阶段训练策略和语言感知提示，SHNU-mASR系统在INTERSPEECH 2025 MLC-SLM挑战中取得了出色的性能，相较于官方MLC-SLM基线模型，在挑战盲评测数据集上的字符&#x2F;单词错误率（CER&#x2F;WER）降低了8.41个百分点，达到11.76%。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>SHNU-mASR系统融合了并行语音编码器架构和大型语言模型，形成统一的多语种ASR框架。</li>
<li>系统采用双编码器结构，包括Whisper-large-v3编码器和mHuBERT-147编码器。</li>
<li>通过三阶段训练策略，联合更新低秩适应模块和投影参数。</li>
<li>在LLM输入端增加语言感知提示，增强语言特定文本生成能力。</li>
<li>SHNU-mASR系统在INTERSPEECH 2025 MLC-SLM挑战中的盲评测数据集上取得了11.76%的CER&#x2F;WER。</li>
<li>与官方MLC-SLM基线模型相比，SHNU-mASR系统的性能提高了8.41个百分点。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.03343">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-109f15444de25c366d810633ae99d7fd.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-78728db70d2a6f96c98bfc01bd386bbd.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-868c350da442b3a2455601df5b764564.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c6393ee73c68038700dfe5e4af37aab1.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-8ff5b76954df33b73e355b423be7e28e.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Qwen-vs-Gemma-Integration-with-Whisper-A-Comparative-Study-in-Multilingual-SpeechLLM-Systems"><a href="#Qwen-vs-Gemma-Integration-with-Whisper-A-Comparative-Study-in-Multilingual-SpeechLLM-Systems" class="headerlink" title="Qwen vs. Gemma Integration with Whisper: A Comparative Study in   Multilingual SpeechLLM Systems"></a>Qwen vs. Gemma Integration with Whisper: A Comparative Study in   Multilingual SpeechLLM Systems</h2><p><strong>Authors:Tuan Nguyen, Long-Vu Hoang, Huy-Dat Tran</strong></p>
<p>This paper presents our system for the MLC-SLM Challenge 2025, focusing on multilingual speech recognition and language modeling with large language models (LLMs). Our approach combines a fine-tuned Whisper-large-v3 encoder with efficient projector architectures and various decoder configurations. We employ a three-stage training methodology that progressively optimizes the encoder, projector, and LLM components. Our system achieves competitive performance with a private test average WER&#x2F;CER result of 16.63% using the Gemma3-12B and 18.6% using the Qwen2.5-7B as decoder-only language model. </p>
<blockquote>
<p>本文介绍了我们为MLC-SLM挑战2025设计的系统，重点是多语种语音识别和基于大型语言模型（LLM）的语言建模。我们的方法结合了经过微调后的Whisper-large-v3编码器、高效的投影仪架构和各种解码器配置。我们采用三阶段训练方法，逐步优化编码器、投影仪和LLM组件。我们的系统在仅使用解码器语言模型的情况下，通过Gemma3-12B取得了平均词错误率（WER）&#x2F;字符错误率（CER）为16.63%，通过Qwen2.5-7B取得了平均词错误率为18.6%，表现出良好的竞争力。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.13596v2">PDF</a> Accepted to Interspeech MLCSLM-2025 Workshop</p>
<p><strong>Summary</strong><br>本文介绍了针对MLC-SLM挑战的系统研究，聚焦于使用大型语言模型的多语种语音识别和语言建模。研究团队采用精细调整过的Whisper-large-v3编码器与高效投影架构和各种解码器配置相结合的方式，并通过三阶段训练法逐步优化各组件。该系统在测试中表现优异，使用Gemma3-12B和Qwen2.5-7B解码器模型分别取得了平均词错误率（WER）为16.63%和平均字符错误率（CER）为18.6%。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>系统针对MLC-SLM挑战构建，专注于多语种语音识别和语言建模。</li>
<li>采用Whisper-large-v3编码器结合高效投影架构和多种解码器配置。</li>
<li>通过三阶段训练法逐步优化编码器、投影器和语言模型组件。</li>
<li>系统在测试中表现优异，使用Gemma3-12B解码器模型取得平均词错误率（WER）为16.63%。</li>
<li>使用Qwen2.5-7B解码器模型取得平均字符错误率（CER）为18.6%。</li>
<li>研究展示大型语言模型在语音识别领域的有效性。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.13596">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-18571333c5b40417c279091aa132e858.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-314d141c51ce3f4e82bb3df040bde2a5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-17ad4db7d352ef84fa43d9c5f63b2a3a.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Bi-directional-Context-Enhanced-Speech-Large-Language-Models-for-Multilingual-Conversational-ASR"><a href="#Bi-directional-Context-Enhanced-Speech-Large-Language-Models-for-Multilingual-Conversational-ASR" class="headerlink" title="Bi-directional Context-Enhanced Speech Large Language Models for   Multilingual Conversational ASR"></a>Bi-directional Context-Enhanced Speech Large Language Models for   Multilingual Conversational ASR</h2><p><strong>Authors:Yizhou Peng, Hexin Liu, Eng Siong Chng</strong></p>
<p>This paper introduces the integration of language-specific bi-directional context into a speech large language model (SLLM) to improve multilingual continuous conversational automatic speech recognition (ASR). We propose a character-level contextual masking strategy during training, which randomly removes portions of the context to enhance robustness and better emulate the flawed transcriptions that may occur during inference. For decoding, a two-stage pipeline is utilized: initial isolated segment decoding followed by context-aware re-decoding using neighboring hypotheses. Evaluated on the 1500-hour Multilingual Conversational Speech and Language Model (MLC-SLM) corpus covering eleven languages, our method achieves an 18% relative improvement compared to a strong baseline, outperforming even the model trained on 6000 hours of data for the MLC-SLM competition. These results underscore the significant benefit of incorporating contextual information in multilingual continuous conversational ASR. </p>
<blockquote>
<p>本文介绍了将语言特定的双向上下文集成到语音大语言模型（SLLM）中，以改进多语言连续对话自动语音识别（ASR）。我们提出了一种字符级上下文掩码策略，在训练过程中随机移除部分上下文，以增强模型的稳健性，并更好地模拟推断过程中可能发生的转录错误。对于解码，我们采用了两阶段流程：首先是初始的孤立段解码，然后是使用相邻假设进行上下文感知的重新解码。在涵盖11种语言的1500小时多语言对话语音和语言模型（MLC-SLM）语料库上进行的评估表明，我们的方法与强大的基线相比，实现了相对18%的改进，甚至超越了为MLC-SLM竞赛训练的6000小时数据模型。这些结果强调了在多语言连续对话ASR中融入上下文信息的重大益处。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.13396v2">PDF</a> Accepted By Interspeech 2025 MLC-SLM workshop as a Research Paper</p>
<p><strong>Summary</strong>：<br>此论文探讨了将语言特定的双向上下文集成到语音大型语言模型（SLLM）中，以改进多语言连续对话自动语音识别（ASR）。论文提出在训练过程中使用字符级别的上下文掩盖策略，通过随机移除部分上下文内容提高模型的稳健性，更好地模拟推断过程中可能出现的缺陷转录。解码时采用两阶段流程：先进行初步的独立片段解码，再利用邻近假设进行上下文感知的重新解码。在涵盖十一种语言的1500小时多语言对话语音和语言模型（MLC-SLM）语料库上评估，该方法相较于强大的基线模型实现了18%的相对改进，甚至在MLC-SLM竞赛中表现优于经过6000小时数据训练的模型。这一结果凸显了在多语言连续对话ASR中融入上下文信息的显著优势。</p>
<p><strong>Key Takeaways</strong>：</p>
<ol>
<li>论文介绍了一种集成语言特定双向上下文到语音大型语言模型的方法，旨在改进多语言连续对话自动语音识别（ASR）。</li>
<li>提出了字符级别的上下文掩盖策略，以增强模型的稳健性并模拟可能的缺陷转录。</li>
<li>采用两阶段解码流程，先进行初步独立片段解码，再进行上下文感知的重新解码。</li>
<li>在涵盖多种语言的语料库上评估，该方法相较于基线模型实现了显著的性能提升。</li>
<li>相较于使用更多数据训练的模型，所提方法仍然表现优异。</li>
<li>结果突显了融入上下文信息在多语言连续对话ASR中的重要作用。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.13396">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-081e0c9167b02260c7e17f93855ec1d4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7a8daa33f63b024de5dc584119aca715.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ad174257bce8b6b417541d9d214da64e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-334378040cf240dde107c32dc4077074.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-78f310ab047a401019abe0923b9cbadf.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ab894683560554c8e9dfd01b5ca6e9b1.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="NTU-Speechlab-LLM-Based-Multilingual-ASR-System-for-Interspeech-MLC-SLM-Challenge-2025"><a href="#NTU-Speechlab-LLM-Based-Multilingual-ASR-System-for-Interspeech-MLC-SLM-Challenge-2025" class="headerlink" title="NTU Speechlab LLM-Based Multilingual ASR System for Interspeech MLC-SLM   Challenge 2025"></a>NTU Speechlab LLM-Based Multilingual ASR System for Interspeech MLC-SLM   Challenge 2025</h2><p><strong>Authors:Yizhou Peng, Bin Wang, Yi-Wen Chao, Ziyang Ma, Haoyang Zhang, Hexin Liu, Xie Chen, Eng Siong Chng</strong></p>
<p>This report details the NTU Speechlab system developed for the Interspeech 2025 Multilingual Conversational Speech and Language Model (MLC-SLM) Challenge (Task I), where we achieved 5th place. We present comprehensive analyses of our multilingual automatic speech recognition system, highlighting key advancements in model architecture, data selection, and training strategies. In particular, language-specific prompts and model averaging techniques were instrumental in boosting system performance across diverse languages. Compared to the initial baseline system, our final model reduced the average Mix Error Rate from 20.2% to 10.6%, representing an absolute improvement of 9.6% (a relative improvement of 48%) on the evaluation set. Our results demonstrate the effectiveness of our approach and offer practical insights for future Speech Large Language Models. </p>
<blockquote>
<p>本报告详细介绍了为Interspeech 2025多语言对话语音和语言模型（MLC-SLM）挑战赛（任务一）开发的NTU Speechlab系统，我们在比赛中获得第五名。我们对我们的多语言自动语音识别系统进行了全面分析，重点介绍了模型架构、数据选择和训练策略方面的关键进展。特别是，语言特定的提示和模型平均技术对提高不同语言的系统性能起到了关键作用。与初始基线系统相比，我们的最终模型将平均混合错误率从20.2%降低到10.6%，在评估集上实现了9.6%的绝对改进（相对改进48%）。我们的结果证明了我们的方法的有效性，并为未来的语音大语言模型提供了实际见解。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.13339v2">PDF</a> Accepted by Interspeech 2025 MLC-SLM challenge (5th place). System   report</p>
<p><strong>Summary</strong><br>系统参与了Interspeech 2025多语种对话语音和语言模型挑战赛，并在比赛中取得了第五名的好成绩。文章详细介绍了开发的NTU Speechlab系统及其多语种自动语音识别系统的关键进展，包括模型架构、数据选择和训练策略等方面的分析。采用语言特定提示和模型平均技术提高了系统在不同语言中的性能。最终模型相较于初始基线系统平均混合错误率降低了9.6%，显示出该方法的有效性，并为未来的语音大型语言模型提供了实际见解。</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>NTU Speechlab系统参与了Interspeech 2025 MLC-SLM挑战赛并获得了第五名。</li>
<li>文章详细分析了该系统的多语种自动语音识别技术。</li>
<li>系统改进包括模型架构、数据选择和训练策略等方面。</li>
<li>语言特定提示和模型平均技术提高了系统性能。</li>
<li>最终模型的平均Mix Error Rate相较于初始基线系统降低了9.6%。</li>
<li>该方法的有效性得到验证。</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.13339">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-6271ccfde6e9a54b75c72836020ace90.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e9ecc8b2b371ccb62d563a94527a98f3.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-5a18d47d7e6084887dd915f1515a91f1.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-8a8bac7ecc1887d23adeb1ac8ec6f313.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-5bbab8169e4838e32d244cc729207270.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d61636e3923645158f9918dc9dfe00a3.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="Image-Generation-with-Supervised-Selection-Based-on-Multimodal-Features-for-Semantic-Communications"><a href="#Image-Generation-with-Supervised-Selection-Based-on-Multimodal-Features-for-Semantic-Communications" class="headerlink" title="Image Generation with Supervised Selection Based on Multimodal Features   for Semantic Communications"></a>Image Generation with Supervised Selection Based on Multimodal Features   for Semantic Communications</h2><p><strong>Authors:Chengyang Liang, Dong Li</strong></p>
<p>Semantic communication (SemCom) has emerged as a promising technique for the next-generation communication systems, in which the generation at the receiver side is allowed with semantic features’ recovery. However, the majority of existing research predominantly utilizes a singular type of semantic information, such as text, images, or speech, to supervise and choose the generated source signals, which may not sufficiently encapsulate the comprehensive and accurate semantic information, and thus creating a performance bottleneck. In order to bridge this gap, in this paper, we propose and investigate a SemCom framework using multimodal information to supervise the generated image. To be specific, in this framework, we first extract semantic features at both the image and text levels utilizing the Convolutional Neural Network (CNN) architecture and the Contrastive Language-Image Pre-Training (CLIP) model before transmission. Then, we employ a generative diffusion model at the receiver to generate multiple images. In order to ensure the accurate extraction and facilitate high-fidelity image reconstruction, we select the “best” image with the minimum reconstruction errors by taking both the aided image and text semantic features into account. We further extend multimodal semantic communication (MMSemCom) system to the multiuser scenario for orthogonal transmission. Experimental results demonstrate that the proposed framework can not only achieve the enhanced fidelity and robustness in image transmission compared with existing communication systems but also sustain a high performance in the low signal-to-noise ratio (SNR) conditions. </p>
<blockquote>
<p>语义通信（SemCom）已成为下一代通信系统的有前途的技术。在该技术中，接收端的生成允许恢复语义特征。然而，现有的大多数研究主要利用单一类型的语义信息（如文本、图像或语音）来监督和选择生成的源信号。这种方式可能无法充分包含全面和准确的语义信息，从而限制了性能提升。为了弥补这一差距，本文提出了一个使用多模态信息监督生成图像的SemCom框架，并进行了深入研究。具体来说，在此框架中，我们首先在传输前利用卷积神经网络（CNN）架构和对比语言图像预训练（CLIP）模型在图像和文本层面提取语义特征。然后，我们在接收端采用生成扩散模型生成多个图像。为了确保准确的提取和高质量的图像重建，我们通过考虑辅助图像和文本语义特征来选择重建误差最小的“最佳”图像。我们将多模态语义通信（MMSemCom）系统进一步扩展到多用户场景，以实现正交传输。实验结果表明，所提出的框架与现有通信系统相比，在图像传输中不仅提高了保真度和稳健性，而且在低信噪比（SNR）条件下也保持了高性能。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2411.17428v3">PDF</a> </p>
<p><strong>摘要</strong><br>本研究针对语义通信（SemCom）中的性能瓶颈问题，提出了一种基于多模态信息的SemCom框架。该框架在传输前利用卷积神经网络（CNN）和对比语言图像预训练（CLIP）模型提取图像和文本级别的语义特征。在接收端采用生成性扩散模型生成多张图像，并通过考虑辅助图像和文本语义特征来选取重建误差最小的“最佳”图像。此外，该研究还将多模态语义通信（MMSemCom）系统扩展到多用户场景，实现正交传输。实验结果表明，该框架在图像传输中不仅提高了保真度和稳健性，而且在低信噪比条件下也保持了高性能。</p>
<p><strong>关键见解</strong></p>
<ol>
<li>多模态信息被用于监督生成的图像，旨在解决现有SemCom中性能瓶颈的问题。</li>
<li>利用CNN和CLIP模型在传输前提取图像和文本级别的语义特征。</li>
<li>在接收端采用生成性扩散模型生成多个图像。</li>
<li>通过考虑辅助图像和文本语义特征，选择重建误差最小的“最佳”图像。</li>
<li>多模态语义通信（MMSemCom）系统被扩展到多用户场景，支持正交传输。</li>
<li>实验结果显示，新框架在图像传输的保真度和稳健性上有所提升。</li>
<li>该框架在低信噪比条件下也能保持高性能。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2411.17428">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-dfeab85621bb19d299102e3b3c6fc756.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3baac709a459885445243332b039e8ff.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-1b666c23f70e4266cfc6ae42505c44c6.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-71c7faaa7327b4be31a3747d7038a817.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-21f1d8c7ede8bb2252e92fc08e2ab923.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        文章作者:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        文章链接:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-07-09/Speech/">https://kedreamix.github.io/Talk2Paper/Paper/2025-07-09/Speech/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        版权声明:
                    </i>
                </span>
                <span class="reprint-info">
                    本博客所有文章除特別声明外，均采用
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    许可协议。转载请注明来源
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Speech/">
                                    <span class="chip bg-color">Speech</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;上一篇</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-07-09/Face%20Swapping/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-540af709c16fde65f6f1c101677775cb.jpg" class="responsive-img" alt="Face Swapping">
                        
                        <span class="card-title">Face Swapping</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Face Swapping 方向最新论文已更新，请持续关注 Update in 2025-07-09  De-Fake Style based Anomaly Deepfake Detection
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-07-09
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Face-Swapping/" class="post-category">
                                    Face Swapping
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Face-Swapping/">
                        <span class="chip bg-color">Face Swapping</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                下一篇&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-07-09/%E6%97%A0%E7%9B%91%E7%9D%A3_%E5%8D%8A%E7%9B%91%E7%9D%A3_%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0/">
                    <div class="card-image">
                        
                        <img src="https://pica.zhimg.com/v2-6eca5caf497be878e7106b53e04ff5c5.jpg" class="responsive-img" alt="无监督/半监督/对比学习">
                        
                        <span class="card-title">无监督/半监督/对比学习</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            无监督/半监督/对比学习 方向最新论文已更新，请持续关注 Update in 2025-07-09  CLIP-RL Surgical Scene Segmentation Using Contrastive Language-Vision   Pretraining & Reinforcement Learning
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-07-09
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E6%97%A0%E7%9B%91%E7%9D%A3-%E5%8D%8A%E7%9B%91%E7%9D%A3-%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0/" class="post-category">
                                    无监督/半监督/对比学习
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E6%97%A0%E7%9B%91%E7%9D%A3-%E5%8D%8A%E7%9B%91%E7%9D%A3-%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0/">
                        <span class="chip bg-color">无监督/半监督/对比学习</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- 代码块功能依赖 -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- 代码语言 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- 代码块复制 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- 代码块收缩 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC 悬浮按钮. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* 修复文章卡片 div 的宽度. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // 切换TOC目录展开收缩的相关操作.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;站点总字数:&nbsp;<span
                        class="white-color">23154.4k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- 运行天数提醒. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // 区分是否有年份.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = '本站已运行 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                daysTip = '本站已運行 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = '本站已运行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = '本站已運行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="关注我的知乎: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">知</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- 搜索遮罩框 -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span>
            <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- 白天和黑夜主题 -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- 回到顶部按钮 -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // 只在桌面版网页启用特效
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- 雪花特效 -->
    

    <!-- 鼠标星星特效 -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--腾讯兔小巢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- 动态标签 -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Σ(っ °Д °;)っ诶，页面崩溃了嘛？", clearTimeout(st)) : (document.title = "φ(゜▽゜*)♪咦，又好了！", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
