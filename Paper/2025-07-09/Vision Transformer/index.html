<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Vision Transformer">
    <meta name="description" content="Vision Transformer æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-07-09  Multi-modal Representations for Fine-grained Multi-label Critical View   of Safety Recognition">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Vision Transformer | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pica.zhimg.com/v2-6236f6d3febe66dcf8b706f1b83798bf.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Vision Transformer</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Vision-Transformer/">
                                <span class="chip bg-color">Vision Transformer</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Vision-Transformer/" class="post-category">
                                Vision Transformer
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-07-09
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-07-17
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    11k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    45 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-07-09-æ›´æ–°"><a href="#2025-07-09-æ›´æ–°" class="headerlink" title="2025-07-09 æ›´æ–°"></a>2025-07-09 æ›´æ–°</h1><h2 id="Multi-modal-Representations-for-Fine-grained-Multi-label-Critical-View-of-Safety-Recognition"><a href="#Multi-modal-Representations-for-Fine-grained-Multi-label-Critical-View-of-Safety-Recognition" class="headerlink" title="Multi-modal Representations for Fine-grained Multi-label Critical View   of Safety Recognition"></a>Multi-modal Representations for Fine-grained Multi-label Critical View   of Safety Recognition</h2><p><strong>Authors:Britty Baby, Vinkle Srivastav, Pooja P. Jain, Kun Yuan, Pietro Mascagni, Nicolas Padoy</strong></p>
<p>The Critical View of Safety (CVS) is crucial for safe laparoscopic cholecystectomy, yet assessing CVS criteria remains a complex and challenging task, even for experts. Traditional models for CVS recognition depend on vision-only models learning with costly, labor-intensive spatial annotations. This study investigates how text can be harnessed as a powerful tool for both training and inference in multi-modal surgical foundation models to automate CVS recognition. Unlike many existing multi-modal models, which are primarily adapted for multi-class classification, CVS recognition requires a multi-label framework. Zero-shot evaluation of existing multi-modal surgical models shows a significant performance gap for this task. To address this, we propose CVS-AdaptNet, a multi-label adaptation strategy that enhances fine-grained, binary classification across multiple labels by aligning image embeddings with textual descriptions of each CVS criterion using positive and negative prompts. By adapting PeskaVLP, a state-of-the-art surgical foundation model, on the Endoscapes-CVS201 dataset, CVS-AdaptNet achieves 57.6 mAP, improving over the ResNet50 image-only baseline (51.5 mAP) by 6 points. Our results show that CVS-AdaptNetâ€™s multi-label, multi-modal framework, enhanced by textual prompts, boosts CVS recognition over image-only methods. We also propose text-specific inference methods, that helps in analysing the image-text alignment. While further work is needed to match state-of-the-art spatial annotation-based methods, this approach highlights the potential of adapting generalist models to specialized surgical tasks. Code: <a target="_blank" rel="noopener" href="https://github.com/CAMMA-public/CVS-AdaptNet">https://github.com/CAMMA-public/CVS-AdaptNet</a> </p>
<blockquote>
<p>å¯¹å®‰å…¨æ€§çš„æ‰¹åˆ¤æ€§è§‚ç‚¹ï¼ˆCVSï¼‰å¯¹äºå®‰å…¨çš„è…¹è…”é•œèƒ†å›Šåˆ‡é™¤æœ¯è‡³å…³é‡è¦ï¼Œä½†è¯„ä¼°CVSæ ‡å‡†ä»ç„¶æ˜¯ä¸€ä¸ªå¤æ‚ä¸”å…·æœ‰æŒ‘æˆ˜æ€§çš„ä»»åŠ¡ï¼Œå³ä½¿æ˜¯ä¸“å®¶ä¹Ÿæ˜¯å¦‚æ­¤ã€‚ä¼ ç»Ÿçš„CVSè¯†åˆ«æ¨¡å‹ä¾èµ–äºä»…è§†è§‰æ¨¡å‹çš„å­¦ä¹ ï¼Œè¿™éœ€è¦æ˜‚è´µçš„ã€åŠ³åŠ¨å¯†é›†çš„ç©ºé—´æ³¨é‡Šã€‚æœ¬ç ”ç©¶æ¢è®¨äº†æ–‡æœ¬å¦‚ä½•æˆä¸ºå¤šæ¨¡æ€æ‰‹æœ¯åŸºç¡€æ¨¡å‹ä¸­è®­ç»ƒå’Œæ¨ç†çš„å¼ºå¤§å·¥å…·ï¼Œä»¥è‡ªåŠ¨åŒ–CVSè¯†åˆ«ã€‚ä¸è®¸å¤šç°æœ‰çš„å¤šæ¨¡æ€æ¨¡å‹ä¸åŒï¼ŒCVSè¯†åˆ«éœ€è¦å¤šæ ‡ç­¾æ¡†æ¶ï¼Œè€Œä¸æ˜¯ä¸»è¦é€‚åº”äºå¤šç±»åˆ†ç±»ã€‚å¯¹ç°æœ‰å¤šæ¨¡æ€æ‰‹æœ¯æ¨¡å‹çš„é›¶æ ·æœ¬è¯„ä¼°æ˜¾ç¤ºï¼Œæ­¤ä»»åŠ¡å­˜åœ¨æ˜¾è‘—çš„æ€§èƒ½å·®è·ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†CVS-AdaptNetï¼Œè¿™æ˜¯ä¸€ç§å¤šæ ‡ç­¾é€‚åº”ç­–ç•¥ï¼Œé€šè¿‡åˆ©ç”¨æ­£é¢å’Œè´Ÿé¢æç¤ºï¼Œå°†å›¾åƒåµŒå…¥ä¸æ¯ä¸ªCVSæ ‡å‡†çš„æ–‡æœ¬æè¿°å¯¹é½ï¼Œä»è€Œå¢å¼ºè·¨å¤šä¸ªæ ‡ç­¾çš„ç²¾ç»†äºŒè¿›åˆ¶åˆ†ç±»ã€‚é€šè¿‡é€‚åº”æœ€å…ˆè¿›çš„æ‰‹æœ¯åŸºç¡€æ¨¡å‹PeskaVLPå’Œåœ¨Endoscapes-CVS201æ•°æ®é›†ä¸Šï¼ŒCVS-AdaptNetè¾¾åˆ°äº†57.6çš„mAPï¼Œæ¯”ä»…ä½¿ç”¨ResNet50å›¾åƒçš„åŸºçº¿ï¼ˆ51.5 mAPï¼‰æé«˜äº†6ä¸ªç™¾åˆ†ç‚¹ã€‚æˆ‘ä»¬çš„ç»“æœè¡¨æ˜ï¼ŒCVS-AdaptNetçš„å¤šæ ‡ç­¾ã€å¤šæ¨¡æ€æ¡†æ¶é€šè¿‡æ–‡æœ¬æç¤ºè€Œå¢å¼ºï¼Œåœ¨ä»…ä½¿ç”¨å›¾åƒçš„æ–¹æ³•ä¸Šæ¨åŠ¨äº†CVSè¯†åˆ«ã€‚æˆ‘ä»¬è¿˜æå‡ºäº†æ–‡æœ¬ç‰¹å®šçš„æ¨ç†æ–¹æ³•ï¼Œæœ‰åŠ©äºåˆ†æå›¾åƒæ–‡æœ¬å¯¹é½ã€‚è™½ç„¶éœ€è¦è¿›ä¸€æ­¥çš„å·¥ä½œæ‰èƒ½åŒ¹é…åŸºäºç©ºé—´æ³¨é‡Šçš„å…ˆè¿›æ–¹æ³•ï¼Œä½†è¿™ç§æ–¹æ³•çªæ˜¾äº†å°†é€šç”¨æ¨¡å‹é€‚åº”ä¸“é—¨æ‰‹æœ¯ä»»åŠ¡çš„æ½œåŠ›ã€‚ä»£ç ï¼š<a target="_blank" rel="noopener" href="https://github.com/CAMMA-public/CVS-AdaptNet">https://github.com/CAMMA-public/CVS-AdaptNet</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.05007v1">PDF</a> </p>
<p><strong>æ‘˜è¦</strong></p>
<p>å®‰å…¨è§†å›¾ä¸­å¯¹äºè…¹è…”é•œèƒ†å›Šåˆ‡é™¤æœ¯çš„å…³é”®è§‚å¯Ÿç‚¹ï¼ˆCVSï¼‰è¯„ä¼°éå¸¸é‡è¦ä½†åˆéå¸¸å¤æ‚å’Œå…·æœ‰æŒ‘æˆ˜æ€§ï¼Œç‰¹åˆ«æ˜¯å¯¹äºä¸“å®¶æ¥è¯´ã€‚ä¼ ç»ŸCVSè¯†åˆ«æ¨¡å‹ä¾èµ–äºè§†è§‰æ¨¡å‹è¿›è¡Œæ˜‚è´µå’Œç¹ççš„ç©ºé—´æ³¨é‡Šå­¦ä¹ ã€‚æœ¬ç ”ç©¶æ¢è®¨äº†æ–‡æœ¬å¦‚ä½•ä½œä¸ºå¤šæ¨¡æ€æ‰‹æœ¯åŸºç¡€æ¨¡å‹çš„è®­ç»ƒå’Œæ¨ç†çš„å¼ºå¤§å·¥å…·ï¼Œä»¥è‡ªåŠ¨è¿›è¡ŒCVSè¯†åˆ«ã€‚ä¸è®¸å¤šç°æœ‰çš„å¤šæ¨¡æ€æ¨¡å‹ä¸åŒï¼ŒCVSè¯†åˆ«éœ€è¦å¤šæ ‡ç­¾æ¡†æ¶ã€‚å¯¹ç°æœ‰å¤šæ¨¡æ€æ‰‹æœ¯æ¨¡å‹çš„é›¶æ ·æœ¬è¯„ä¼°æ˜¾ç¤ºï¼Œè¯¥ä»»åŠ¡å­˜åœ¨æ˜¾è‘—çš„æ€§èƒ½å·®è·ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†CVS-AdaptNetï¼Œè¿™æ˜¯ä¸€ç§å¤šæ ‡ç­¾é€‚åº”ç­–ç•¥ï¼Œé€šè¿‡å›¾åƒåµŒå…¥ä¸æ¯ä¸ªCVSæ ‡å‡†çš„æ–‡æœ¬æè¿°å¯¹é½ï¼Œä½¿ç”¨æ­£é¢å’Œè´Ÿé¢æç¤ºæ¥å¢å¼ºè·¨å¤šä¸ªæ ‡ç­¾çš„ç²¾ç»†ç²’åº¦äºŒè¿›åˆ¶åˆ†ç±»ã€‚é€šè¿‡åœ¨Endoscapes-CVSæ•°æ®é›†ä¸Šé€‚åº”å…ˆè¿›çš„æ‰‹æœ¯åŸºç¡€æ¨¡å‹PeskaVLPï¼ŒCVS-AdaptNetå®ç°äº†57.6çš„mAPï¼Œæ¯”ä»…ä½¿ç”¨ResNetçš„å›¾åƒåŸºçº¿ï¼ˆmAPä¸º51.5ï¼‰æé«˜äº†å…­ä¸ªç™¾åˆ†ç‚¹ã€‚æˆ‘ä»¬çš„ç»“æœè¡¨æ˜ï¼Œé€šè¿‡æ–‡æœ¬æç¤ºå¢å¼ºçš„å¤šæ ‡ç­¾å¤šæ¨¡æ€æ¡†æ¶CVS-AdaptNetåœ¨å›¾åƒæ–¹æ³•ä¸Šæ¨åŠ¨äº†CVSè¯†åˆ«çš„è¿›æ­¥ã€‚æˆ‘ä»¬è¿˜æå‡ºäº†æ–‡æœ¬ç‰¹å®šçš„æ¨ç†æ–¹æ³•ï¼Œæœ‰åŠ©äºåˆ†æå›¾åƒå’Œæ–‡æœ¬çš„åŒ¹é…åº¦ã€‚å°½ç®¡éœ€è¦è¿›ä¸€æ­¥çš„å·¥ä½œæ¥åŒ¹é…åŸºäºç©ºé—´æ³¨é‡Šçš„æ–¹æ³•çš„æœ€å…ˆè¿›æ°´å¹³ï¼Œä½†è¿™ç§æ–¹æ³•çªå‡ºäº†å°†é€šç”¨æ¨¡å‹é€‚åº”äºç‰¹æ®Šæ‰‹æœ¯ä»»åŠ¡çš„æ½œåŠ›ã€‚å…·ä½“ä»£ç å®ç°å·²åœ¨Githubä¸Šè¿›è¡Œå…¬å¼€ï¼š<a target="_blank" rel="noopener" href="https://github.com/CAMMA-public/CVS-AdaptNet">https://github.com/CAMMA-public/CVS-AdaptNet</a> ã€‚â€‹â€‹<br>â€‹æ€»ç»“ç®€çŸ­å¹¶ä¸”ä¸è¶…è¿‡è¦æ±‚é•¿åº¦ï¼Œè¡¨è¿°ç®€æ´ä¸”æ˜“äºç†è§£å³å¯â€‹â€‹ã€‚æœ¬æ®µç ”ç©¶é’ˆå¯¹åœ¨è…¹è…”é•œèƒ†å›Šåˆ‡é™¤æœ¯ä¸­å…³äºå…³é”®è§‚å¯Ÿç‚¹çš„å®‰å…¨è§†å›¾è¯„ä¼°çš„æŒ‘æˆ˜æ€§ä»»åŠ¡å±•å¼€ç ”ç©¶ã€‚è¯¥ç ”ç©¶æå‡ºäº†ä¸€ç§æ–°çš„å¤šæ¨¡æ€æ¨¡å‹CVS-AdaptNetï¼Œé€šè¿‡ç»“åˆå›¾åƒå’Œæ–‡æœ¬ä¿¡æ¯æé«˜è‡ªåŠ¨åŒ–è¯†åˆ«å…³é”®è§‚å¯Ÿç‚¹çš„ç²¾åº¦å¹¶å±•ç°äº†å¯¹è¯„ä¼°çš„å®‰å…¨è§†å›¾çš„è¿›ä¸€æ­¥äº†è§£çš„æ–¹æ³•å’Œæµç¨‹å·²åœ¨GitHubå…¬å¼€äº¤æµä¿ƒè¿›æŠ€æœ¯è®¨è®ºåˆ†äº«æ¨å¹¿åŠå…¶åœ¨ç°å®æƒ…å†µä¸‹çš„æ”¹è¿›å®ç°ä½œç”¨ï¼›ä½†æŠ€æœ¯çš„æé™ä»æœ‰å¾…æ¢ç´¢æ”¹è¿›å’Œæ‹“å±•æ¨å¹¿çš„æ½œåŠ›ã€‚å…¶ä¼˜ç‚¹åœ¨äºåˆ©ç”¨äº†å¤šæ¨¡æ€ä¿¡æ¯ä»¥æé«˜è¯†åˆ«ç²¾åº¦å’Œé€‚åº”ä¸åŒä»»åŠ¡éœ€æ±‚ã€‚è™½ç„¶è¿˜éœ€è¦è¿›ä¸€æ­¥çš„ç ”ç©¶å’Œæ”¹è¿›ä»¥è¾¾åˆ°æœ€å…ˆè¿›çš„æŠ€æœ¯æ°´å¹³ä½†åŒæ—¶ä¹Ÿæ¨åŠ¨äº†æŠ€æœ¯çš„æ™®åŠå’Œæ¨å¹¿çš„å‰æ™¯â€‹â€‹ ç®€è¦æ€»ç»“èµ·æ¥ä¾¿å¦‚ä¸‹ï¼šæ–°å‹æ¨¡å‹å®ç°é«˜æ•ˆè·¨æ¨¡æ€å¤„ç†å¤æ‚è¯†åˆ«æŒ‘æˆ˜ä¸é«˜çº§æŠ€æœ¯çš„æ·±å…¥æ¢ç´¢å’Œæ”¹è¿›è¿˜æœ‰å¹¿é˜”æ½œåŠ›å¯æŒ–æ˜å®ç°æ›´å¥½çš„å®‰å…¨æ€§å’Œæ€§èƒ½è¡¨ç°ã€‚â€‹â€‹</p>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.05007">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-564d67acf7eefba843eba49d3b9eccc4.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-aba328f82664932462b22c9617f7a9a9.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="HiLa-Hierarchical-Vision-Language-Collaboration-for-Cancer-Survival-Prediction"><a href="#HiLa-Hierarchical-Vision-Language-Collaboration-for-Cancer-Survival-Prediction" class="headerlink" title="HiLa: Hierarchical Vision-Language Collaboration for Cancer Survival   Prediction"></a>HiLa: Hierarchical Vision-Language Collaboration for Cancer Survival   Prediction</h2><p><strong>Authors:Jiaqi Cui, Lu Wen, Yuchen Fei, Bo Liu, Luping Zhou, Dinggang Shen, Yan Wang</strong></p>
<p>Survival prediction using whole-slide images (WSIs) is crucial in cancer re-search. Despite notable success, existing approaches are limited by their reliance on sparse slide-level labels, which hinders the learning of discriminative repre-sentations from gigapixel WSIs. Recently, vision language (VL) models, which incorporate additional language supervision, have emerged as a promising solu-tion. However, VL-based survival prediction remains largely unexplored due to two key challenges. First, current methods often rely on only one simple lan-guage prompt and basic cosine similarity, which fails to learn fine-grained associ-ations between multi-faceted linguistic information and visual features within WSI, resulting in inadequate vision-language alignment. Second, these methods primarily exploit patch-level information, overlooking the intrinsic hierarchy of WSIs and their interactions, causing ineffective modeling of hierarchical interac-tions. To tackle these problems, we propose a novel Hierarchical vision-Language collaboration (HiLa) framework for improved survival prediction. Specifically, HiLa employs pretrained feature extractors to generate hierarchical visual features from WSIs at both patch and region levels. At each level, a series of language prompts describing various survival-related attributes are constructed and aligned with visual features via Optimal Prompt Learning (OPL). This ap-proach enables the comprehensive learning of discriminative visual features cor-responding to different survival-related attributes from prompts, thereby improv-ing vision-language alignment. Furthermore, we introduce two modules, i.e., Cross-Level Propagation (CLP) and Mutual Contrastive Learning (MCL) to maximize hierarchical cooperation by promoting interactions and consistency be-tween patch and region levels. Experiments on three TCGA datasets demonstrate our SOTA performance. </p>
<blockquote>
<p>ä½¿ç”¨å…¨åˆ‡ç‰‡å›¾åƒï¼ˆWSIï¼‰è¿›è¡Œç”Ÿå­˜é¢„æµ‹åœ¨ç™Œç—‡ç ”ç©¶ä¸­è‡³å…³é‡è¦ã€‚å°½ç®¡å·²æœ‰æ˜¾è‘—çš„æˆåŠŸï¼Œä½†ç°æœ‰æ–¹æ³•å—é™äºå¯¹ç¨€ç–åˆ‡ç‰‡çº§åˆ«æ ‡ç­¾çš„ä¾èµ–ï¼Œè¿™é˜»ç¢äº†ä»åƒå…†åƒç´ WSIä¸­å­¦ä¹ åˆ¤åˆ«è¡¨ç¤ºã€‚æœ€è¿‘ï¼Œèå…¥é¢å¤–è¯­è¨€ç›‘ç£çš„è§†è§‰è¯­è¨€ï¼ˆVLï¼‰æ¨¡å‹çš„å‡ºç°ä½œä¸ºä¸€ç§æœ‰å‰é€”çš„è§£å†³æ–¹æ¡ˆã€‚ç„¶è€Œï¼ŒåŸºäºVLçš„ç”Ÿå­˜é¢„æµ‹ä»ç„¶å› ä¸¤ä¸ªå…³é”®æŒ‘æˆ˜è€Œæœªè¢«å……åˆ†æ¢ç´¢ã€‚é¦–å…ˆï¼Œå½“å‰çš„æ–¹æ³•é€šå¸¸ä»…ä¾èµ–äºä¸€ä¸ªç®€å•çš„è¯­è¨€æç¤ºå’ŒåŸºæœ¬çš„ä½™å¼¦ç›¸ä¼¼æ€§ï¼Œè¿™æ— æ³•å­¦ä¹ WSIå†…å¤šé¢è¯­è¨€ä¿¡æ¯å’Œè§†è§‰ç‰¹å¾ä¹‹é—´çš„ç²¾ç»†å…³è”ï¼Œå¯¼è‡´è§†è§‰è¯­è¨€å¯¹é½ä¸è¶³ã€‚å…¶æ¬¡ï¼Œè¿™äº›æ–¹æ³•ä¸»è¦åˆ©ç”¨è¡¥ä¸çº§åˆ«çš„ä¿¡æ¯ï¼Œå¿½ç•¥äº†WSIçš„å†…åœ¨å±‚æ¬¡åŠå…¶ç›¸äº’ä½œç”¨ï¼Œå¯¼è‡´å±‚æ¬¡äº¤äº’çš„å»ºæ¨¡æ— æ•ˆã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ç”¨äºæ”¹è¿›ç”Ÿå­˜é¢„æµ‹çš„æ–°å‹åˆ†å±‚è§†è§‰è¯­è¨€åä½œï¼ˆHiLaï¼‰æ¡†æ¶ã€‚å…·ä½“æ¥è¯´ï¼ŒHiLaä½¿ç”¨é¢„è®­ç»ƒçš„ç‰¹å¾æå–å™¨ä»WSIç”Ÿæˆå±‚æ¬¡åŒ–çš„è§†è§‰ç‰¹å¾ï¼Œè¿™äº›ç‰¹å¾æ—¢åŒ…å«è¡¥ä¸çº§åˆ«åˆåŒ…å«åŒºåŸŸçº§åˆ«ã€‚åœ¨æ¯ä¸ªçº§åˆ«ä¸Šï¼Œæ„å»ºäº†ä¸€ç³»åˆ—æè¿°ä¸ç”Ÿå­˜ç›¸å…³å±æ€§çš„è¯­è¨€æç¤ºï¼Œå¹¶é€šè¿‡æœ€ä½³æç¤ºå­¦ä¹ ï¼ˆOPLï¼‰ä¸è§†è§‰ç‰¹å¾å¯¹é½ã€‚è¿™ç§æ–¹æ³•èƒ½å¤Ÿå…¨é¢å­¦ä¹ å¯¹åº”äºä¸åŒç”Ÿå­˜ç›¸å…³å±æ€§çš„åˆ¤åˆ«æ€§è§†è§‰ç‰¹å¾ï¼Œä»è€Œæé«˜è§†è§‰è¯­è¨€å¯¹é½ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å¼•å…¥äº†è·¨çº§ä¼ æ’­ï¼ˆCLPï¼‰å’Œç›¸äº’å¯¹æ¯”å­¦ä¹ ï¼ˆMCLï¼‰ä¸¤ä¸ªæ¨¡å—ï¼Œé€šè¿‡ä¿ƒè¿›è¡¥ä¸å’ŒåŒºåŸŸçº§åˆ«ä¹‹é—´çš„äº¤äº’å’Œä¸€è‡´æ€§ï¼Œä»¥æœ€å¤§åŒ–å±‚æ¬¡åä½œã€‚åœ¨ä¸‰ä¸ªTCGAæ•°æ®é›†ä¸Šçš„å®éªŒè¯æ˜äº†æˆ‘ä»¬çš„æ€§èƒ½å¤„äºSOTAæ°´å¹³ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.04613v1">PDF</a> Accepted by MICCAI2025</p>
<p><strong>Summary</strong><br>åœ¨ç™Œç—‡ç ”ç©¶ä¸­ï¼ŒåŸºäºå…¨å¹»ç¯ç‰‡å›¾åƒï¼ˆWSIï¼‰çš„ç”Ÿå­˜é¢„æµ‹è‡³å…³é‡è¦ã€‚ç°æœ‰æ–¹æ³•å—é™äºç¨€ç–çš„å¹»ç¯ç‰‡çº§æ ‡ç­¾ï¼Œéš¾ä»¥ä»å‰åƒç´ WSIä¸­å­¦ä¹ åˆ¤åˆ«è¡¨ç¤ºã€‚æ–°å…´çš„è§†è§‰è¯­è¨€ï¼ˆVLï¼‰æ¨¡å‹é€šè¿‡å¼•å…¥é¢å¤–çš„è¯­è¨€ç›‘ç£è§£å†³äº†è¿™ä¸ªé—®é¢˜ã€‚ç„¶è€Œï¼ŒåŸºäºVLçš„ç”Ÿå­˜é¢„æµ‹ä»é¢ä¸´ä¸¤å¤§æŒ‘æˆ˜ã€‚ä¸ºè§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†åˆ†å±‚è§†è§‰è¯­è¨€åä½œï¼ˆHiLaï¼‰æ¡†æ¶ï¼Œä»¥æ”¹è¿›ç”Ÿå­˜é¢„æµ‹ã€‚HiLaä½¿ç”¨é¢„è®­ç»ƒçš„ç‰¹å¾æå–å™¨ç”Ÿæˆå±‚æ¬¡åŒ–çš„è§†è§‰ç‰¹å¾ï¼Œå¹¶é€šè¿‡æœ€ä½³æç¤ºå­¦ä¹ ï¼ˆOPLï¼‰ä¸è¯­è¨€æç¤ºå¯¹é½ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜å¼•å…¥äº†è·¨å±‚æ¬¡ä¼ æ’­ï¼ˆCLPï¼‰å’Œç›¸äº’å¯¹æ¯”å­¦ä¹ ï¼ˆMCLï¼‰ä¸¤ä¸ªæ¨¡å—ï¼Œä»¥æœ€å¤§åŒ–å±‚æ¬¡é—´çš„åˆä½œä¸ä¸€è‡´æ€§ã€‚åœ¨ä¸‰ä¸ªTCGAæ•°æ®é›†ä¸Šçš„å®éªŒè¯æ˜äº†è¯¥æ–¹æ³•çš„æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>åŸºäºå…¨å¹»ç¯ç‰‡å›¾åƒï¼ˆWSIï¼‰çš„ç”Ÿå­˜é¢„æµ‹åœ¨ç™Œç—‡ç ”ç©¶ä¸­å¾ˆé‡è¦ã€‚</li>
<li>å½“å‰æ–¹æ³•å—é™äºç¨€ç–çš„æ ‡ç­¾ï¼Œéš¾ä»¥ä»WSIä¸­å­¦ä¹ åˆ¤åˆ«ç‰¹å¾ã€‚</li>
<li>è§†è§‰è¯­è¨€ï¼ˆVLï¼‰æ¨¡å‹é€šè¿‡å¼•å…¥è¯­è¨€ç›‘ç£æ¥è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œä½†åŸºäºVLçš„ç”Ÿå­˜é¢„æµ‹é¢ä¸´æŒ‘æˆ˜ã€‚</li>
<li>æå‡ºçš„HiLaæ¡†æ¶é€šè¿‡ç”Ÿæˆå±‚æ¬¡åŒ–çš„è§†è§‰ç‰¹å¾å’Œä½¿ç”¨æœ€ä½³æç¤ºå­¦ä¹ ï¼ˆOPLï¼‰è§£å†³è¿™äº›é—®é¢˜ã€‚</li>
<li>HiLaæ¡†æ¶å¼•å…¥äº†è·¨å±‚æ¬¡ä¼ æ’­ï¼ˆCLPï¼‰å’Œç›¸äº’å¯¹æ¯”å­¦ä¹ ï¼ˆMCLï¼‰æ¨¡å—ï¼Œä»¥æœ€å¤§åŒ–å±‚æ¬¡é—´çš„åˆä½œä¸ä¸€è‡´æ€§ã€‚</li>
<li>å®éªŒè¯æ˜HiLaæ¡†æ¶åœ¨ä¸‰ä¸ªTCGAæ•°æ®é›†ä¸Šçš„æ€§èƒ½ä¼˜äºç°æœ‰æ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.04613">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-b1dec8972dd9bdbc529978bc9cadae70.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-30a2068165abec94173d76cbdf83d566.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="FA-Forced-Prompt-Learning-of-Vision-Language-Models-for-Out-of-Distribution-Detection"><a href="#FA-Forced-Prompt-Learning-of-Vision-Language-Models-for-Out-of-Distribution-Detection" class="headerlink" title="FA: Forced Prompt Learning of Vision-Language Models for   Out-of-Distribution Detection"></a>FA: Forced Prompt Learning of Vision-Language Models for   Out-of-Distribution Detection</h2><p><strong>Authors:Xinhua Lu, Runhe Lai, Yanqi Wu, Kanghao Chen, Wei-Shi Zheng, Ruixuan Wang</strong></p>
<p>Pre-trained vision-language models (VLMs) have advanced out-of-distribution (OOD) detection recently. However, existing CLIP-based methods often focus on learning OOD-related knowledge to improve OOD detection, showing limited generalization or reliance on external large-scale auxiliary datasets. In this study, instead of delving into the intricate OOD-related knowledge, we propose an innovative CLIP-based framework based on Forced prompt leArning (FA), designed to make full use of the In-Distribution (ID) knowledge and ultimately boost the effectiveness of OOD detection. Our key insight is to learn a prompt (i.e., forced prompt) that contains more diversified and richer descriptions of the ID classes beyond the textual semantics of class labels. Specifically, it promotes better discernment for ID images, by forcing more notable semantic similarity between ID images and the learnable forced prompt. Moreover, we introduce a forced coefficient, encouraging the forced prompt to learn more comprehensive and nuanced descriptions of the ID classes. In this way, FA is capable of achieving notable improvements in OOD detection, even when trained without any external auxiliary datasets, while maintaining an identical number of trainable parameters as CoOp. Extensive empirical evaluations confirm our method consistently outperforms current state-of-the-art methods. Code is available at <a target="_blank" rel="noopener" href="https://github.com/0xFAFA/FA">https://github.com/0xFAFA/FA</a>. </p>
<blockquote>
<p>é¢„è®­ç»ƒè§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰æœ€è¿‘åœ¨ç¦»ç¾¤å€¼æ£€æµ‹ï¼ˆOODï¼‰æ–¹é¢å–å¾—äº†è¿›å±•ã€‚ç„¶è€Œï¼Œç°æœ‰çš„åŸºäºCLIPçš„æ–¹æ³•é€šå¸¸ä¾§é‡äºå­¦ä¹ OODç›¸å…³çŸ¥è¯†ä»¥æé«˜OODæ£€æµ‹èƒ½åŠ›ï¼Œæ˜¾ç¤ºå‡ºæœ‰é™çš„æ³›åŒ–èƒ½åŠ›æˆ–å¯¹å¤–éƒ¨å¤§è§„æ¨¡è¾…åŠ©æ•°æ®é›†çš„ä¾èµ–ã€‚æœ¬ç ”ç©¶ä¸­ï¼Œæˆ‘ä»¬å¹¶æ²¡æœ‰æ·±å…¥ç ”ç©¶å¤æ‚çš„OODç›¸å…³çŸ¥è¯†ï¼Œè€Œæ˜¯æå‡ºäº†ä¸€ç§åŸºäºå¼ºåˆ¶æç¤ºå­¦ä¹ ï¼ˆFAï¼‰çš„CLIPæ¡†æ¶ï¼Œæ—¨åœ¨å……åˆ†åˆ©ç”¨åˆ†å¸ƒå†…ï¼ˆIDï¼‰çŸ¥è¯†ï¼Œå¹¶æœ€ç»ˆæé«˜OODæ£€æµ‹çš„æœ‰æ•ˆæ€§ã€‚æˆ‘ä»¬çš„å…³é”®è§è§£æ˜¯å­¦ä¹ ä¸€ç§åŒ…å«è¶…è¿‡ç±»åˆ«æ ‡ç­¾æ–‡æœ¬è¯­ä¹‰çš„æ›´å…·å¤šæ ·åŒ–å’Œä¸°å¯Œæè¿°çš„æç¤ºï¼ˆå³å¼ºåˆ¶æç¤ºï¼‰ã€‚å…·ä½“æ¥è¯´ï¼Œå®ƒé€šè¿‡å¼ºåˆ¶IDå›¾åƒå’Œå¯å­¦ä¹ çš„å¼ºåˆ¶æç¤ºä¹‹é—´æ›´æ˜¾è‘—çš„è¯­ä¹‰ç›¸ä¼¼æ€§ï¼Œä¿ƒè¿›äº†å¯¹IDå›¾åƒçš„æ›´ä½³è¾¨åˆ«åŠ›ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ªå¼ºåˆ¶ç³»æ•°ï¼Œé¼“åŠ±å¼ºåˆ¶æç¤ºå­¦ä¹ æ›´å…¨é¢ã€æ›´å¾®å¦™çš„IDç±»åˆ«æè¿°ã€‚é€šè¿‡è¿™ç§æ–¹å¼ï¼Œå³ä½¿åœ¨æ²¡æœ‰ä»»ä½•å¤–éƒ¨è¾…åŠ©æ•°æ®é›†è¿›è¡Œè®­ç»ƒçš„æƒ…å†µä¸‹ï¼ŒFAä¹Ÿèƒ½åœ¨OODæ£€æµ‹æ–¹é¢å®ç°æ˜¾è‘—çš„æ”¹è¿›ï¼ŒåŒæ—¶ä¿æŒä¸CoOpç›¸åŒæ•°é‡çš„å¯è®­ç»ƒå‚æ•°ã€‚å¤§é‡çš„ç»éªŒè¯„ä¼°è¯æ˜æˆ‘ä»¬çš„æ–¹æ³•å§‹ç»ˆä¼˜äºå½“å‰æœ€å…ˆè¿›çš„æ–¹æ³•ã€‚ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/0xFAFA/FA%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/0xFAFA/FAæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.04511v1">PDF</a> </p>
<p><strong>Summary</strong>ï¼šåŸºäºé¢„è®­ç»ƒçš„è¯­è¨€è§†è§‰æ¨¡å‹ï¼Œè¯¥ç ”ç©¶æå‡ºäº†ä¸€ç§æ–°çš„CLIPæ¡†æ¶ï¼Œåä¸ºåŸºäºå¼ºåˆ¶æç¤ºå­¦ä¹ ï¼ˆFAï¼‰çš„æ–¹æ³•ï¼Œæ—¨åœ¨å……åˆ†åˆ©ç”¨åˆ†å¸ƒå†…ï¼ˆIDï¼‰çŸ¥è¯†ï¼Œä»è€Œæé«˜å¼‚å¸¸æ£€æµ‹æ•ˆæœã€‚è¯¥æ–¹æ³•é€šè¿‡å¼ºåˆ¶å­¦ä¹ æ›´ä¸°å¯Œçš„æè¿°æ¥åŒºåˆ†åˆ†å¸ƒå†…å›¾åƒï¼Œä¸”æ— éœ€ä¾èµ–å¤–éƒ¨å¤§è§„æ¨¡è¾…åŠ©æ•°æ®é›†ã€‚åœ¨æ— éœ€é¢å¤–æ•°æ®é›†çš„æƒ…å†µä¸‹ï¼Œè¯¥æ–¹æ³•åœ¨å¼‚å¸¸æ£€æµ‹æ–¹é¢å–å¾—äº†æ˜¾è‘—æ”¹è¿›ï¼Œä¸å½“å‰çš„æœ€ä¼˜æ–¹æ³•ç›¸æ¯”æœ‰æ˜æ˜¾ä¼˜åŠ¿ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ul>
<li>é¢„è®­ç»ƒçš„è¯­è¨€è§†è§‰æ¨¡å‹ï¼ˆVLMsï¼‰å·²ç»ä¿ƒè¿›äº†å¼‚å¸¸æ£€æµ‹æŠ€æœ¯çš„å‘å±•ã€‚</li>
<li>å½“å‰CLIPæ–¹æ³•ä¾§é‡äºå­¦ä¹ å¼‚å¸¸æ£€æµ‹ç›¸å…³çŸ¥è¯†ä»¥æé«˜æ£€æµ‹æ•ˆæœï¼Œä½†æ³›åŒ–æ€§å—é™å¹¶ä¾èµ–å¤–éƒ¨å¤§è§„æ¨¡æ•°æ®é›†ã€‚</li>
<li>ç ”ç©¶æå‡ºåŸºäºå¼ºåˆ¶æç¤ºå­¦ä¹ ï¼ˆFAï¼‰çš„CLIPæ¡†æ¶ï¼Œå……åˆ†åˆ©ç”¨åˆ†å¸ƒå†…ï¼ˆIDï¼‰çŸ¥è¯†æé«˜å¼‚å¸¸æ£€æµ‹æ•ˆæœã€‚</li>
<li>é€šè¿‡å¼ºåˆ¶å­¦ä¹ åŒ…å«æ›´ä¸°å¯Œæè¿°çš„æç¤ºä¿¡æ¯ï¼Œå¢å¼ºäº†åˆ†å¸ƒå†…å›¾åƒçš„è¾¨è¯†èƒ½åŠ›ã€‚</li>
<li>å¼•å…¥å¼ºåˆ¶ç³»æ•°æ¥é¼“åŠ±å­¦ä¹ æ›´å…¨é¢çš„æè¿°ä¿¡æ¯ã€‚</li>
<li>è¯¥æ–¹æ³•åœ¨ä¸ä½¿ç”¨å¤–éƒ¨è¾…åŠ©æ•°æ®é›†çš„æƒ…å†µä¸‹å®ç°äº†æ˜¾è‘—çš„å¼‚å¸¸æ£€æµ‹æ€§èƒ½æå‡ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.04511">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-e80ca0d3ac12d181387dad1d6b63886c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-09e7b8065334785550ddf3dba3d40a03.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-068cd0fcce0ee0e2ff7f5bef51205db0.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="LVLM-Composerâ€™s-Explicit-Planning-for-Image-Generation"><a href="#LVLM-Composerâ€™s-Explicit-Planning-for-Image-Generation" class="headerlink" title="LVLM-Composerâ€™s Explicit Planning for Image Generation"></a>LVLM-Composerâ€™s Explicit Planning for Image Generation</h2><p><strong>Authors:Spencer Ramsey, Jeffrey Lee, Amina Grant</strong></p>
<p>The burgeoning field of generative artificial intelligence has fundamentally reshaped our approach to content creation, with Large Vision-Language Models (LVLMs) standing at its forefront. While current LVLMs have demonstrated impressive capabilities in text-to-image generation, they often falter when confronted with complex textual descriptions demanding precise compositional understanding and visual planning. This limitation particularly impacts the accurate rendering of multiple objects, their attributes, spatial relationships, and specific poses within intricate scenes, as evidenced by benchmarks like LongBench-T2I. To address these challenges, we introduce LVLM-Composer, a novel 10-billion parameter scale LVLM specifically engineered for enhanced compositional image synthesis. Our method incorporates a Hierarchical Semantic Planning Module for structured prompt decomposition and a Fine-Grained Feature Alignment Mechanism for precise visual guidance during generation. We propose a multi-stage training paradigm, featuring Hierarchical Semantic-Visual Grounding Pre-training and Compositional Planning Reinforcement Learning with Self-Correction, to instill robust compositional reasoning. Extensive experiments on the LongBench-T2I benchmark, utilizing automatic evaluation by Gemini-2.0-Flash and InternVL3-78B, demonstrate LVLM-Composerâ€™s superior performance across critical compositional dimensions including object accuracy, composition fidelity, and pose accuracy, significantly outperforming state-of-the-art baselines. An in-depth ablation study further validates the indispensable contribution of our proposed modules, while human evaluations confirm the perceptual superiority of our generated images. LVLM-Composer represents a significant step towards truly controllable and compositionally accurate open-ended text-to-image generation. </p>
<blockquote>
<p>ç”Ÿæˆäººå·¥æ™ºèƒ½é¢†åŸŸæ—¥æ–°æœˆå¼‚ï¼Œå·²ç»ä»æ ¹æœ¬ä¸Šæ”¹å˜äº†æˆ‘ä»¬çš„å†…å®¹åˆ›ä½œæ–¹å¼ï¼Œå¤§è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆLVLMsï¼‰æ­£å¤„äºè¿™ä¸€é¢†åŸŸçš„å‰æ²¿ã€‚å°½ç®¡å½“å‰çš„LVLMsåœ¨æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆæ–¹é¢å±•ç°å‡ºäº†ä»¤äººå°è±¡æ·±åˆ»çš„èƒ½åŠ›ï¼Œä½†å½“é¢å¯¹å¤æ‚çš„æ–‡æœ¬æè¿°ï¼Œéœ€è¦ç²¾ç¡®çš„ç»„åˆç†è§£å’Œè§†è§‰è§„åˆ’æ—¶ï¼Œå®ƒä»¬å¾€å¾€ä¼šé‡åˆ°å›°éš¾ã€‚è¿™ä¸€å±€é™æ€§å¯¹åœ¨å¤æ‚åœºæ™¯ä¸­å‡†ç¡®å‘ˆç°å¤šä¸ªç‰©ä½“ã€å…¶å±æ€§ã€ç©ºé—´å…³ç³»ä»¥åŠç‰¹å®šå§¿æ€äº§ç”Ÿäº†æ˜¾è‘—å½±å“ï¼ŒLongBench-T2Iç­‰åŸºå‡†æµ‹è¯•è¯æ˜äº†è¿™ä¸€ç‚¹ã€‚ä¸ºäº†åº”å¯¹è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æ¨å‡ºäº†LVLM-Composerï¼Œè¿™æ˜¯ä¸€ä¸ªæ–°å‹çš„å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹ï¼Œä¸“é—¨é’ˆå¯¹å¢å¼ºç»„åˆå›¾åƒåˆæˆè€Œè®¾è®¡ï¼Œè§„æ¨¡è¾¾åˆ°10äº¿å‚æ•°çº§åˆ«ã€‚æˆ‘ä»¬çš„æ–¹æ³•èå…¥äº†åˆ†å±‚è¯­ä¹‰è§„åˆ’æ¨¡å—ï¼Œç”¨äºç»“æ„åŒ–æç¤ºåˆ†è§£å’Œç²¾ç»†ç‰¹å¾å¯¹é½æœºåˆ¶ï¼Œä»¥åœ¨ç”Ÿæˆè¿‡ç¨‹ä¸­æä¾›ç²¾ç¡®çš„è§†è§‰æŒ‡å¯¼ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§å¤šé˜¶æ®µè®­ç»ƒèŒƒå¼ï¼ŒåŒ…å«åˆ†å±‚è¯­ä¹‰è§†è§‰å®šä½é¢„è®­ç»ƒä»¥åŠç»„åˆè§„åˆ’å¼ºåŒ–å­¦ä¹ ä¸è‡ªæˆ‘ä¿®æ­£ï¼Œä»¥å»ºç«‹ç¨³å¥çš„ç»„åˆæ¨ç†ã€‚åœ¨LongBench-T2IåŸºå‡†æµ‹è¯•ä¸Šè¿›è¡Œçš„å¹¿æ³›å®éªŒï¼Œé‡‡ç”¨Gemini-2.0-Flashå’ŒInternVL3-78Bè¿›è¡Œè‡ªåŠ¨è¯„ä¼°ï¼Œè¯æ˜LVLM-Composeråœ¨å…³é”®ç»„åˆç»´åº¦ä¸Šè¡¨ç°å‡ºå“è¶Šæ€§èƒ½ï¼ŒåŒ…æ‹¬ç‰©ä½“å‡†ç¡®æ€§ã€ç»„åˆä¿çœŸåº¦å’Œå§¿æ€å‡†ç¡®æ€§ï¼Œæ˜¾è‘—ä¼˜äºæœ€æ–°åŸºçº¿ã€‚æ·±å…¥çš„æ¶ˆèç ”ç©¶è¿›ä¸€æ­¥éªŒè¯äº†æˆ‘ä»¬æ‰€æå‡ºæ¨¡å—çš„ä¸å¯æˆ–ç¼ºä½œç”¨ï¼Œè€Œäººç±»è¯„ä¼°åˆ™è¯å®äº†æˆ‘ä»¬ç”Ÿæˆå›¾åƒçš„æ„Ÿè§‰ä¼˜è¶Šæ€§ã€‚LVLM-Composeræ˜¯æœç€çœŸæ­£å¯æ§å’Œç»„åˆå‡†ç¡®çš„å¼€æ”¾å¼æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆè¿ˆå‡ºçš„é‡è¦ä¸€æ­¥ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.04152v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æ–‡æœ¬ä¸»è¦ä»‹ç»äº†ç”Ÿæˆå¼äººå·¥æ™ºèƒ½é¢†åŸŸçš„è“¬å‹ƒå‘å±•åŠå…¶åœ¨å†…å®¹åˆ›ä½œæ–¹é¢çš„é©å‘½æ€§å˜é©ã€‚æ–‡ç« é‡ç‚¹ä»‹ç»äº†å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆLVLMsï¼‰åœ¨æ–‡æœ¬ç”Ÿæˆå›¾åƒé¢†åŸŸçš„çªå‡ºè¡¨ç°åŠå…¶é¢ä¸´çš„æŒ‘æˆ˜ã€‚æ–‡ç« è¿˜ä»‹ç»äº†ä¸€ç§æ–°å‹çš„å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹â€”â€”LVLM-Composerï¼Œå®ƒé€šè¿‡ä¸€ç³»åˆ—çš„åˆ›æ–°æŠ€æœ¯å’Œè®­ç»ƒæ–¹æ³•ï¼Œå¢å¼ºäº†ç»“æ„åŒ–æç¤ºåˆ†è§£å’Œç²¾ç»†ç‰¹å¾å¯¹é½æœºåˆ¶ï¼Œä»è€Œæé«˜äº†å¤æ‚åœºæ™¯ä¸­çš„å¤šç‰©ä½“ã€å±æ€§ã€ç©ºé—´å…³ç³»å’Œå§¿æ€çš„å‡†ç¡®æ¸²æŸ“èƒ½åŠ›ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒLVLM-Composeråœ¨å…³é”®æŒ‡æ ‡ä¸Šè¶…è¶Šäº†ç°æœ‰æŠ€æœ¯ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>ç”Ÿæˆå¼äººå·¥æ™ºèƒ½å·²å½»åº•æ”¹å˜äº†å†…å®¹åˆ›ä½œæ–¹å¼ï¼Œå…¶ä¸­å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆLVLMsï¼‰å¤„äºå‰æ²¿åœ°ä½ã€‚</li>
<li>å½“å‰LVLMsåœ¨å¤„ç†å¤æ‚æ–‡æœ¬æè¿°æ—¶å­˜åœ¨æŒ‘æˆ˜ï¼Œå°¤å…¶æ˜¯åœ¨å‡†ç¡®æ¸²æŸ“å¤šä¸ªç‰©ä½“åŠå…¶å±æ€§ã€ç©ºé—´å…³ç³»å’Œç‰¹å®šå§¿æ€æ–¹é¢ã€‚</li>
<li>LVLM-Composeræ˜¯ä¸€ç§æ–°å‹çš„å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹ï¼Œå…·æœ‰å¢å¼ºçš„ç»“æ„åŒ–æç¤ºåˆ†è§£å’Œç²¾ç»†ç‰¹å¾å¯¹é½æœºåˆ¶ï¼Œæ—¨åœ¨è§£å†³ä¸Šè¿°é—®é¢˜ã€‚</li>
<li>LVLM-Composeré€šè¿‡å¤šé˜¶æ®µè®­ç»ƒèŒƒå¼ï¼ŒåŒ…æ‹¬åˆ†å±‚è¯­ä¹‰è§†è§‰å®šä½é¢„è®­ç»ƒå’Œç»„åˆè§„åˆ’å¼ºåŒ–å­¦ä¹ è¿›è¡Œè‡ªæˆ‘æ ¡æ­£ï¼Œä»¥çŒè¾“å¼ºå¤§çš„ç»„åˆæ¨ç†èƒ½åŠ›ã€‚</li>
<li>å®éªŒç»“æœè¡¨æ˜ï¼ŒLVLM-Composeråœ¨å…³é”®æŒ‡æ ‡ä¸Šæ˜¾è‘—ä¼˜äºç°æœ‰æŠ€æœ¯ï¼Œå¦‚å¯¹è±¡å‡†ç¡®æ€§ã€ç»„åˆä¿çœŸåº¦å’Œå§¿æ€å‡†ç¡®æ€§ã€‚</li>
<li>æ·±å…¥çš„åˆ†æç ”ç©¶éªŒè¯äº†LVLM-Composerä¸­æå‡ºçš„æ¨¡å—ä¸å¯æˆ–ç¼ºçš„é‡è¦æ€§ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.04152">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-e1f2cda34e35ad4038c3e94c07f708e5.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Habitat-Classification-from-Ground-Level-Imagery-Using-Deep-Neural-Networks"><a href="#Habitat-Classification-from-Ground-Level-Imagery-Using-Deep-Neural-Networks" class="headerlink" title="Habitat Classification from Ground-Level Imagery Using Deep Neural   Networks"></a>Habitat Classification from Ground-Level Imagery Using Deep Neural   Networks</h2><p><strong>Authors:Hongrui Shi, Lisa Norton, Lucy Ridding, Simon Rolph, Tom August, Claire M Wood, Lan Qie, Petra Bosilj, James M Brown</strong></p>
<p>Habitat assessment at local scales â€“ critical for enhancing biodiversity and guiding conservation priorities â€“ often relies on expert field survey that can be costly, motivating the exploration of AI-driven tools to automate and refine this process. While most AI-driven habitat mapping depends on remote sensing, it is often constrained by sensor availability, weather, and coarse resolution. In contrast, ground-level imagery captures essential structural and compositional cues invisible from above and remains underexplored for robust, fine-grained habitat classification. This study addresses this gap by applying state-of-the-art deep neural network architectures to ground-level habitat imagery. Leveraging data from the UK Countryside Survey covering 18 broad habitat types, we evaluate two families of models â€“ convolutional neural networks (CNNs) and vision transformers (ViTs) â€“ under both supervised and supervised contrastive learning paradigms. Our results demonstrate that ViTs consistently outperform state-of-the-art CNN baselines on key classification metrics (Top-3 accuracy &#x3D; 91%, MCC &#x3D; 0.66) and offer more interpretable scene understanding tailored to ground-level images. Moreover, supervised contrastive learning significantly reduces misclassification rates among visually similar habitats (e.g., Improved vs. Neutral Grassland), driven by a more discriminative embedding space. Finally, our best model performs on par with experienced ecological experts in habitat classification from images, underscoring the promise of expert-level automated assessment. By integrating advanced AI with ecological expertise, this research establishes a scalable, cost-effective framework for ground-level habitat monitoring to accelerate biodiversity conservation and inform land-use decisions at the national scale. </p>
<blockquote>
<p>ç”Ÿå¢ƒè¯„ä¼°åœ¨æœ¬åœ°å°ºåº¦ä¸Šå¯¹äºå¢å¼ºç”Ÿç‰©å¤šæ ·æ€§å’ŒæŒ‡å¯¼ä¿æŠ¤ä¼˜å…ˆäº‹é¡¹è‡³å…³é‡è¦ï¼Œé€šå¸¸ä¾èµ–äºæ˜‚è´µçš„ä¸“å®¶å®åœ°è°ƒæŸ¥ï¼Œè¿™æ¿€å‘äº†äººä»¬æ¢ç´¢ä½¿ç”¨äººå·¥æ™ºèƒ½å·¥å…·æ¥è‡ªåŠ¨åŒ–å’Œå®Œå–„è¿™ä¸€è¿‡ç¨‹ã€‚è™½ç„¶å¤§å¤šæ•°äººå·¥æ™ºèƒ½é©±åŠ¨çš„ç”Ÿå¢ƒåœ°å›¾åˆ¶ä½œä¾èµ–äºé¥æ„ŸæŠ€æœ¯ï¼Œä½†å®ƒé€šå¸¸å—åˆ°ä¼ æ„Ÿå™¨å¯ç”¨æ€§ã€å¤©æ°”å’Œåˆ†è¾¨ç‡ç²—ç³™çš„é™åˆ¶ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼Œåœ°é¢æ°´å¹³çš„å›¾åƒæ•æ‰äº†ä»ä¸Šé¢çœ‹ä¸è§çš„è‡³å…³é‡è¦çš„ç»“æ„å’Œç»„æˆçº¿ç´¢ï¼Œå¹¶ä¸”å¯¹äºç¨³å¥çš„ã€ç²¾ç»†çš„ç”Ÿå¢ƒåˆ†ç±»ä»ç„¶æœªè¢«å……åˆ†æ¢ç´¢ã€‚æœ¬ç ”ç©¶é€šè¿‡åº”ç”¨æœ€æ–°æ·±åº¦ç¥ç»ç½‘ç»œæ¶æ„è¿›è¡Œåœ°é¢ç”Ÿå¢ƒå›¾åƒåˆ†ææ¥è§£å†³è¿™ä¸€ç©ºç™½ã€‚æˆ‘ä»¬åˆ©ç”¨è¦†ç›–18ç§å¹¿æ³›ç”Ÿå¢ƒç±»å‹çš„è‹±å›½ä¹¡æ‘è°ƒæŸ¥æ•°æ®ï¼Œè¯„ä¼°äº†ä¸¤ç§æ¨¡å‹å®¶æ—ï¼Œå³å·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNï¼‰å’Œè§†è§‰å˜å‹å™¨ï¼ˆViTï¼‰ï¼Œè¿™ä¸¤ç§æ¨¡å‹åœ¨ç›‘ç£å­¦ä¹ å’Œç›‘ç£å¯¹æ¯”å­¦ä¹ èŒƒå¼ä¸‹è¿›è¡Œäº†è¯„ä¼°ã€‚æˆ‘ä»¬çš„ç»“æœè¡¨æ˜ï¼Œè§†è§‰å˜å‹å™¨åœ¨å…³é”®åˆ†ç±»æŒ‡æ ‡ä¸Šå§‹ç»ˆä¼˜äºæœ€æ–°çš„CNNåŸºå‡†æµ‹è¯•ï¼ˆå‰ä¸‰åå‡†ç¡®ç‡&#x3D; 91%ï¼ŒMCC &#x3D; 0.66ï¼‰ï¼Œå¹¶ä¸”æä¾›æ›´é’ˆå¯¹åœ°é¢æ°´å¹³å›¾åƒçš„å¯è§£é‡Šåœºæ™¯ç†è§£ã€‚æ­¤å¤–ï¼Œç›‘ç£å¯¹æ¯”å­¦ä¹ æ˜¾è‘—é™ä½äº†è§†è§‰ä¸Šç›¸ä¼¼ç”Ÿå¢ƒä¹‹é—´çš„è¯¯åˆ†ç±»ç‡ï¼ˆä¾‹å¦‚ï¼Œæ”¹è‰¯è‰åœ°ä¸ä¸­æ€§è‰åœ°ï¼‰ï¼Œè¿™å¾—ç›Šäºæ›´å…·åŒºåˆ†æ€§çš„åµŒå…¥ç©ºé—´ã€‚æœ€åï¼Œæˆ‘ä»¬æœ€å¥½çš„æ¨¡å‹åœ¨å›¾åƒç”Ÿå¢ƒåˆ†ç±»æ–¹é¢ä¸ç»éªŒä¸°å¯Œçš„ç”Ÿæ€ä¸“å®¶è¡¨ç°ç›¸å½“ï¼Œè¿™çªæ˜¾äº†ä¸“å®¶çº§è‡ªåŠ¨åŒ–è¯„ä¼°çš„æ½œåŠ›ã€‚é€šè¿‡æ•´åˆå…ˆè¿›çš„äººå·¥æ™ºèƒ½å’Œç”Ÿæ€ä¸“ä¸šçŸ¥è¯†ï¼Œè¿™é¡¹ç ”ç©¶å»ºç«‹äº†ä¸€ä¸ªå¯æ‰©å±•çš„ã€æˆæœ¬æ•ˆç›Šé«˜çš„åœ°é¢ç”Ÿå¢ƒç›‘æµ‹æ¡†æ¶ï¼Œä»¥åŠ é€Ÿç”Ÿç‰©å¤šæ ·æ€§ä¿æŠ¤å¹¶ä¸ºå…¨å›½å°ºåº¦çš„åœŸåœ°åˆ©ç”¨å†³ç­–æä¾›ä¿¡æ¯ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.04017v1">PDF</a> 26 pages, 12 figures, 6 tables</p>
<p><strong>Summary</strong><br>åŸºäºæ·±åº¦ç¥ç»ç½‘ç»œæ¶æ„å’Œåœ°é¢çº§å›¾åƒæ•°æ®ï¼Œæœ¬ç ”ç©¶å®ç°äº†å¯¹åœ°é¢çº§æ –æ¯åœ°ç²¾ç»†åˆ†ç±»çš„è‡ªåŠ¨åŒ–è¯„ä¼°ã€‚é€šè¿‡å¯¹æ¯”å·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNï¼‰å’Œè§†è§‰è½¬æ¢å™¨ï¼ˆViTï¼‰ä¸¤ç§æ¨¡å‹ï¼Œå‘ç°ViTåœ¨å…³é”®åˆ†ç±»æŒ‡æ ‡ä¸Šè¡¨ç°æ›´ä¼˜ç§€ï¼Œå…·æœ‰æ›´é«˜çš„å¯è§£é‡Šæ€§å’Œé€‚ç”¨æ€§ã€‚åŒæ—¶ï¼Œè¯¥ç ”ç©¶ä¹Ÿé€šè¿‡ç›‘ç£å¯¹æ¯”å­¦ä¹ å‡å°‘äº†è§†è§‰ç›¸ä¼¼æ –æ¯åœ°ä¹‹é—´çš„è¯¯åˆ†ç±»ç‡ã€‚æ­¤ç ”ç©¶ç»“åˆå…ˆè¿›çš„äººå·¥æ™ºèƒ½æŠ€æœ¯å’Œç”Ÿæ€ä¸“ä¸šçŸ¥è¯†ï¼Œä¸ºåœ°é¢çº§æ –æ¯åœ°ç›‘æµ‹æä¾›äº†ä¸€ä¸ªå¯æ‰©å±•ã€æˆæœ¬æ•ˆç›Šé«˜çš„æ¡†æ¶ï¼Œæœ‰åŠ©äºåŠ é€Ÿç”Ÿç‰©å¤šæ ·æ€§ä¿æŠ¤å’ŒåœŸåœ°åˆ©ç”¨å†³ç­–ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æœ¬ç ”ç©¶ä½¿ç”¨AIå·¥å…·è‡ªåŠ¨åŒ–è¯„ä¼°åœ°é¢çº§æ –æ¯åœ°åˆ†ç±»ï¼Œé™ä½ä¸“å®¶ç°åœºè°ƒæŸ¥æˆæœ¬ã€‚</li>
<li>é€šè¿‡å¯¹æ¯”CNNå’ŒViTæ¨¡å‹ï¼Œå‘ç°ViTåœ¨ç²¾ç»†æ –æ¯åœ°åˆ†ç±»æ–¹é¢è¡¨ç°æ›´ä¼˜ç§€ã€‚</li>
<li>ViTæ¨¡å‹æä¾›æ›´é«˜çš„åˆ†ç±»å‡†ç¡®æ€§ï¼Œå¹¶ä¸”å…·æœ‰æ›´å¥½çš„å¯è§£é‡Šæ€§ã€‚</li>
<li>ç›‘ç£å¯¹æ¯”å­¦ä¹ æé«˜äº†æ¨¡å‹å¯¹è§†è§‰ç›¸ä¼¼æ –æ¯åœ°çš„åŒºåˆ†èƒ½åŠ›ã€‚</li>
<li>æœ¬ç ”ç©¶ç»“åˆAIæŠ€æœ¯ä¸ç”Ÿæ€ä¸“ä¸šçŸ¥è¯†ï¼Œä¸ºåœ°é¢çº§æ –æ¯åœ°ç›‘æµ‹æä¾›æœ‰æ•ˆæ¡†æ¶ã€‚</li>
<li>è¯¥æ¡†æ¶æœ‰åŠ©äºå®ç°å¤§è§„æ¨¡ã€æˆæœ¬æ•ˆç›Šé«˜çš„æ –æ¯åœ°ç›‘æµ‹ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.04017">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-162aa85b281a91a601541f2da57d1442.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-3ba9abb1e2b8ddbaadd8d807205b7fde.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e4ce792a934949bbd954a2556caa6746.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5fab8fb5abbe1f55bc8394764daa9119.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="ChestGPT-Integrating-Large-Language-Models-and-Vision-Transformers-for-Disease-Detection-and-Localization-in-Chest-X-Rays"><a href="#ChestGPT-Integrating-Large-Language-Models-and-Vision-Transformers-for-Disease-Detection-and-Localization-in-Chest-X-Rays" class="headerlink" title="ChestGPT: Integrating Large Language Models and Vision Transformers for   Disease Detection and Localization in Chest X-Rays"></a>ChestGPT: Integrating Large Language Models and Vision Transformers for   Disease Detection and Localization in Chest X-Rays</h2><p><strong>Authors:Shehroz S. Khan, Petar Przulj, Ahmed Ashraf, Ali Abedi</strong></p>
<p>The global demand for radiologists is increasing rapidly due to a growing reliance on medical imaging services, while the supply of radiologists is not keeping pace. Advances in computer vision and image processing technologies present significant potential to address this gap by enhancing radiologistsâ€™ capabilities and improving diagnostic accuracy. Large language models (LLMs), particularly generative pre-trained transformers (GPTs), have become the primary approach for understanding and generating textual data. In parallel, vision transformers (ViTs) have proven effective at converting visual data into a format that LLMs can process efficiently. In this paper, we present ChestGPT, a deep-learning framework that integrates the EVA ViT with the Llama 2 LLM to classify diseases and localize regions of interest in chest X-ray images. The ViT converts X-ray images into tokens, which are then fed, together with engineered prompts, into the LLM, enabling joint classification and localization of diseases. This approach incorporates transfer learning techniques to enhance both explainability and performance. The proposed method achieved strong global disease classification performance on the VinDr-CXR dataset, with an F1 score of 0.76, and successfully localized pathologies by generating bounding boxes around the regions of interest. We also outline several task-specific prompts, in addition to general-purpose prompts, for scenarios radiologists might encounter. Overall, this framework offers an assistive tool that can lighten radiologistsâ€™ workload by providing preliminary findings and regions of interest to facilitate their diagnostic process. </p>
<blockquote>
<p>ç”±äºåŒ»ç–—æˆåƒæœåŠ¡æ—¥ç›Šå¢é•¿çš„ä¾èµ–ï¼Œå¯¹æ”¾å°„ç§‘åŒ»å¸ˆçš„å…¨çƒéœ€æ±‚è¿…é€Ÿå¢åŠ ï¼Œè€Œæ”¾å°„ç§‘åŒ»å¸ˆçš„ä¾›ç»™å´è·Ÿä¸ä¸Šè¿™ä¸€éœ€æ±‚ã€‚è®¡ç®—æœºè§†è§‰å’Œå›¾åƒå¤„ç†æŠ€æœ¯çš„è¿›æ­¥åœ¨å¢å¼ºæ”¾å°„ç§‘åŒ»å¸ˆçš„èƒ½åŠ›å’Œæé«˜è¯Šæ–­å‡†ç¡®æ€§æ–¹é¢å±•ç°å‡ºå·¨å¤§çš„æ½œåŠ›æ¥è§£å†³è¿™ä¸€å·®è·ã€‚å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ç‰¹åˆ«æ˜¯é¢„è®­ç»ƒç”Ÿæˆå¼è½¬æ¢å™¨ï¼ˆGPTï¼‰å·²æˆä¸ºç†è§£å’Œç”Ÿæˆæ–‡æœ¬æ•°æ®çš„ä¸»è¦æ–¹æ³•ã€‚åŒæ—¶ï¼Œè§†è§‰è½¬æ¢å™¨ï¼ˆViTï¼‰å·²è¯æ˜åœ¨å°†è§†è§‰æ•°æ®è½¬æ¢ä¸ºLLMå¯ä»¥é«˜æ•ˆå¤„ç†çš„å½¢å¼æ–¹é¢éå¸¸æœ‰æ•ˆã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ChestGPTï¼Œè¿™æ˜¯ä¸€ä¸ªæ·±åº¦å­¦ä¹ æ¡†æ¶ï¼Œå®ƒå°†EVA ViTä¸Llama 2 LLMç›¸ç»“åˆï¼Œç”¨äºå¯¹èƒ¸éƒ¨Xå°„çº¿å›¾åƒä¸­çš„ç–¾ç—…è¿›è¡Œåˆ†ç±»å¹¶å®šä½æ„Ÿå…´è¶£åŒºåŸŸã€‚ViTå°†Xå°„çº¿å›¾åƒè½¬æ¢ä¸ºä»¤ç‰Œï¼Œç„¶åè¿åŒå·¥ç¨‹æç¤ºä¸€èµ·è¾“å…¥åˆ°LLMä¸­ï¼Œå®ç°äº†ç–¾ç—…çš„è”åˆåˆ†ç±»å’Œå®šä½ã€‚è¿™ç§æ–¹æ³•ç»“åˆäº†è¿ç§»å­¦ä¹ æŠ€æœ¯ï¼Œæé«˜äº†å¯è§£é‡Šæ€§å’Œæ€§èƒ½ã€‚æ‰€æå‡ºçš„æ–¹æ³•åœ¨VinDr-CXRæ•°æ®é›†ä¸Šå–å¾—äº†å¼ºå¤§çš„å…¨çƒç–¾ç—…åˆ†ç±»æ€§èƒ½ï¼ŒF1åˆ†æ•°ä¸º0.76ï¼Œå¹¶æˆåŠŸå®šä½äº†ç—…ç†ï¼Œåœ¨æ„Ÿå…´è¶£åŒºåŸŸå‘¨å›´ç”Ÿæˆäº†è¾¹ç•Œæ¡†ã€‚æˆ‘ä»¬è¿˜æ¦‚è¿°äº†é™¤é€šç”¨æç¤ºä¹‹å¤–é’ˆå¯¹ç‰¹å®šä»»åŠ¡çš„æç¤ºï¼Œç”¨äºåº”å¯¹æ”¾å°„ç§‘åŒ»ç”Ÿå¯èƒ½é‡åˆ°çš„åœºæ™¯ã€‚æ€»çš„æ¥è¯´ï¼Œè¿™ä¸ªæ¡†æ¶æä¾›äº†ä¸€ä¸ªè¾…åŠ©å·¥å…·ï¼Œé€šè¿‡æä¾›åˆæ­¥æ£€æŸ¥ç»“æœå’Œæ„Ÿå…´è¶£åŒºåŸŸæ¥å‡è½»æ”¾å°„ç§‘åŒ»ç”Ÿçš„å·¥ä½œé‡ï¼Œä»è€Œä¿ƒè¿›å…¶è¯Šæ–­è¿‡ç¨‹ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.03739v1">PDF</a> 8 pages, 5 figures, 4 tables</p>
<p><strong>Summary</strong><br>     éšç€åŒ»ç–—æˆåƒæœåŠ¡çš„ä¾èµ–åº¦å¢åŠ ï¼Œå…¨çƒå¯¹æ”¾å°„ç§‘åŒ»ç”Ÿçš„éœ€æ±‚è¿…é€Ÿå¢é•¿ï¼Œè€Œæ”¾å°„ç§‘åŒ»ç”Ÿçš„ä¾›åº”å´è·Ÿä¸ä¸Šè¿™ä¸€é€Ÿåº¦ã€‚è®¡ç®—æœºè§†è§‰å’Œå›¾åƒå¤„ç†æŠ€æœ¯çš„è¿›å±•ä¸ºè§£å†³è¿™ä¸€å·®è·æä¾›äº†å·¨å¤§æ½œåŠ›ï¼Œé€šè¿‡å¢å¼ºæ”¾å°„ç§‘åŒ»ç”Ÿçš„èƒ½åŠ›å’Œæé«˜è¯Šæ–­å‡†ç¡®æ€§æ¥å®ç°ã€‚æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§åä¸ºChestGPTçš„æ·±åº¦å­¦ä¹ æ¡†æ¶ï¼Œå®ƒç»“åˆäº†EVA Vision Transformerï¼ˆViTï¼‰å’ŒLlama 2å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ï¼Œç”¨äºå¯¹èƒ¸éƒ¨Xå°„çº¿å›¾åƒè¿›è¡Œç–¾ç—…åˆ†ç±»å’Œæ„Ÿå…´è¶£åŒºåŸŸå®šä½ã€‚è¯¥æ¡†æ¶åˆ©ç”¨è¿ç§»å­¦ä¹ æŠ€æœ¯ï¼Œæé«˜äº†å¯è§£é‡Šæ€§å’Œæ€§èƒ½ï¼Œåœ¨VinDr-CXRæ•°æ®é›†ä¸Šå®ç°äº†å¼ºå¤§çš„å…¨å±€ç–¾ç—…åˆ†ç±»æ€§èƒ½ï¼ŒF1åˆ†æ•°ä¸º0.76ï¼Œå¹¶æˆåŠŸå®šä½äº†ç—…ç†åŒºåŸŸã€‚æ€»ä½“è€Œè¨€ï¼Œè¯¥æ¡†æ¶ä¸ºæ”¾å°„ç§‘åŒ»ç”Ÿæä¾›äº†ä¸€ç§è¾…åŠ©å·¥å…·ï¼Œå¯ä»¥é€šè¿‡æä¾›åˆæ­¥å‘ç°å’Œæ„Ÿå…´è¶£åŒºåŸŸæ¥å‡è½»ä»–ä»¬çš„å·¥ä½œé‡ï¼Œä»è€Œæœ‰åŠ©äºè¯Šæ–­è¿‡ç¨‹ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>åŒ»å­¦æˆåƒæœåŠ¡çš„ä¾èµ–åº¦å¢åŠ å¯¼è‡´å¯¹æ”¾å°„ç§‘åŒ»ç”Ÿçš„éœ€æ±‚è¿…é€Ÿå¢é•¿ã€‚</li>
<li>æ”¾å°„ç§‘åŒ»ç”Ÿçš„ä¾›åº”æ— æ³•æ»¡è¶³æ—¥ç›Šå¢é•¿çš„éœ€æ±‚ã€‚</li>
<li>è®¡ç®—æœºè§†è§‰å’Œå›¾åƒå¤„ç†æŠ€æœ¯æœ‰æ½œåŠ›é€šè¿‡å¢å¼ºæ”¾å°„ç§‘åŒ»ç”Ÿçš„èƒ½åŠ›å’Œæé«˜è¯Šæ–­å‡†ç¡®æ€§æ¥è§£å†³è¿™ä¸€å·®è·ã€‚</li>
<li>ChestGPTæ˜¯ä¸€ä¸ªç»“åˆEVA Vision Transformerï¼ˆViTï¼‰å’ŒLlama 2å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ·±åº¦å­¦ä¹ æ¡†æ¶ã€‚</li>
<li>ChestGPTèƒ½å¤Ÿå®ç°ç–¾ç—…åˆ†ç±»å’Œèƒ¸éƒ¨Xå°„çº¿å›¾åƒä¸­çš„æ„Ÿå…´è¶£åŒºåŸŸå®šä½ã€‚</li>
<li>è¯¥æ¡†æ¶åˆ©ç”¨è¿ç§»å­¦ä¹ æŠ€æœ¯ï¼Œæé«˜äº†æ€§èƒ½å’Œå¯è§£é‡Šæ€§ã€‚</li>
<li>åœ¨VinDr-CXRæ•°æ®é›†ä¸Šï¼ŒChestGPTå®ç°äº†F1åˆ†æ•°0.76çš„å¼ºæ•ˆå…¨å±€ç–¾ç—…åˆ†ç±»æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.03739">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-db88ede29c1abd31b63991df0ebb5723.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-2ed04ad5f23d1db131702c8528519669.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8d800fea9816b4ffe409770b295f2590.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-71cbb5a4649418693f278bd593808692.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Dual-frequency-Selected-Knowledge-Distillation-with-Statistical-based-Sample-Rectification-for-PolSAR-Image-Classification"><a href="#Dual-frequency-Selected-Knowledge-Distillation-with-Statistical-based-Sample-Rectification-for-PolSAR-Image-Classification" class="headerlink" title="Dual-frequency Selected Knowledge Distillation with Statistical-based   Sample Rectification for PolSAR Image Classification"></a>Dual-frequency Selected Knowledge Distillation with Statistical-based   Sample Rectification for PolSAR Image Classification</h2><p><strong>Authors:Xinyue Xin, Ming Li, Yan Wu, Xiang Li, Peng Zhang, Dazhi Xu</strong></p>
<p>The collaborative classification of dual-frequency PolSAR images is a meaningful but also challenging research. The effect of regional consistency on classification information learning and the rational use of dual-frequency data are two main difficulties for dual-frequency collaborative classification. To tackle these problems, a selected knowledge distillation network with statistical-based sample rectification (SKDNet-SSR) is proposed in this article. First, in addition to applying CNN and ViT as local and global feature extractors, a statistical-based dynamic sample rectification (SDSR) module is designed to avoid the impact of poor regional consistency on spatial information learning process. Specifically, based on the fact that the PolSAR covariance matrix conforms to the complex Wishart distribution, SDSR first dynamically evaluates the sample purity, and then performs pixel selection and pixel generation to remove noisy pixels, thereby avoiding the feature interaction between informative pixels and noisy pixels and improving the classification feature extraction process. Next, a dual-frequency gate-selected distillation (DGSD) module is constructed to emphasize the advantages of different frequency bands and perform complementary learning on dual-frequency data. It uses the dominant single-frequency branch on each sample as teacher model to train the dual-frequency student model, enabling the student model to learn the optimal results and realizing complementary utilization of dual-frequency data on different terrain objects. Comprehensive experiments on four measured dual-frequency PolSAR data demonstrate that the proposed SKDNet-SSR outperforms other related methods. </p>
<blockquote>
<p>åŒé¢‘PolSARå›¾åƒååŒåˆ†ç±»æ˜¯ä¸€é¡¹æœ‰æ„ä¹‰ä½†ä¹Ÿå…·æœ‰æŒ‘æˆ˜æ€§çš„ç ”ç©¶ã€‚åŒºåŸŸä¸€è‡´æ€§å¯¹åˆ†ç±»ä¿¡æ¯å­¦ä¹ çš„å½±å“ä»¥åŠå¦‚ä½•åˆç†åˆ©ç”¨åŒé¢‘æ•°æ®æ˜¯åŒé¢‘ååŒåˆ†ç±»çš„ä¸¤ä¸ªä¸»è¦éš¾é¢˜ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºç»Ÿè®¡æ ·æœ¬ä¿®æ­£çš„é€‰æ‹©çŸ¥è¯†è’¸é¦ç½‘ç»œï¼ˆSKDNet-SSRï¼‰ã€‚é¦–å…ˆï¼Œé™¤äº†ä½¿ç”¨CNNå’ŒViTä½œä¸ºå±€éƒ¨å’Œå…¨å±€ç‰¹å¾æå–å™¨å¤–ï¼Œè¿˜è®¾è®¡äº†ä¸€ä¸ªåŸºäºç»Ÿè®¡çš„åŠ¨æ€æ ·æœ¬ä¿®æ­£ï¼ˆSDSRï¼‰æ¨¡å—ï¼Œä»¥é¿å…åŒºåŸŸä¸€è‡´æ€§å·®å¯¹ç©ºé—´ä¿¡æ¯å­¦ä¹ è¿‡ç¨‹çš„å½±å“ã€‚å…·ä½“è€Œè¨€ï¼ŒåŸºäºPolSARåæ–¹å·®çŸ©é˜µç¬¦åˆå¤æ‚Wishartåˆ†å¸ƒçš„äº‹å®ï¼ŒSDSRé¦–å…ˆåŠ¨æ€è¯„ä¼°æ ·æœ¬çº¯åº¦ï¼Œç„¶åè¿›è¡Œåƒç´ é€‰æ‹©å’Œåƒç´ ç”Ÿæˆï¼Œä»¥å»é™¤å™ªå£°åƒç´ ï¼Œä»è€Œé¿å…ä¿¡æ¯åƒç´ å’Œå™ªå£°åƒç´ ä¹‹é—´çš„ç‰¹å¾äº¤äº’ï¼Œæ”¹è¿›åˆ†ç±»ç‰¹å¾æå–è¿‡ç¨‹ã€‚æ¥ä¸‹æ¥ï¼Œæ„å»ºäº†ä¸€ä¸ªåŒé¢‘é—¨é€‰è’¸é¦ï¼ˆDGSDï¼‰æ¨¡å—ï¼Œä»¥å¼ºè°ƒä¸åŒé¢‘æ®µçš„ä¼˜ç‚¹ï¼Œå¹¶å¯¹åŒé¢‘æ•°æ®è¿›è¡Œäº’è¡¥å­¦ä¹ ã€‚å®ƒä½¿ç”¨æ¯ä¸ªæ ·æœ¬ä¸Šçš„ä¸»å¯¼å•é¢‘åˆ†æ”¯ä½œä¸ºæ•™å¸ˆæ¨¡å‹æ¥è®­ç»ƒåŒé¢‘å­¦ç”Ÿæ¨¡å‹ï¼Œä½¿å­¦ç”Ÿæ¨¡å‹èƒ½å¤Ÿå­¦ä¹ æœ€ä½³ç»“æœï¼Œå¹¶åœ¨ä¸åŒåœ°å½¢å¯¹è±¡ä¸Šå®ç°åŒé¢‘æ•°æ®çš„äº’è¡¥åˆ©ç”¨ã€‚åœ¨å››ä¸ªå®æµ‹åŒé¢‘PolSARæ•°æ®ä¸Šçš„ç»¼åˆå®éªŒè¡¨æ˜ï¼Œæ‰€æå‡ºçš„SKDNet-SSRä¼˜äºå…¶ä»–ç›¸å…³æ–¹æ³•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.03268v1">PDF</a> </p>
<p><strong>Summary</strong><br>åœ¨ååŒåˆ†ç±»æåŒ–SARå›¾åƒçš„ç ”ç©¶ä¸­ï¼Œå­˜åœ¨å› åŒºåŸŸä¸€è‡´æ€§å’Œæœ‰æ•ˆåˆ©ç”¨åŒé¢‘æ•°æ®æ‰€å¸¦æ¥çš„æŒ‘æˆ˜ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§ç»“åˆç»Ÿè®¡æ ·æœ¬æ ¡æ­£çš„çŸ¥è¯†è’¸é¦ç½‘ç»œï¼ˆSKDNet-SSRï¼‰æ¥åº”å¯¹è¿™äº›æŒ‘æˆ˜ã€‚è¯¥ç½‘ç»œä¸ä»…ä½¿ç”¨CNNå’ŒViTä½œä¸ºå±€éƒ¨å’Œå…¨å±€ç‰¹å¾æå–å™¨ï¼Œè¿˜è®¾è®¡äº†ä¸€ä¸ªåŸºäºç»Ÿè®¡çš„åŠ¨æ€æ ·æœ¬æ ¡æ­£ï¼ˆSDSRï¼‰æ¨¡å—ï¼Œä»¥é¿å…ä¸è‰¯åŒºåŸŸä¸€è‡´æ€§å¯¹ç©ºé—´ä¿¡æ¯å­¦ä¹ è¿‡ç¨‹çš„å½±å“ã€‚æ­¤å¤–ï¼Œè¿˜æ„å»ºäº†ä¸€ä¸ªåŒé¢‘é—¨é€‰è’¸é¦ï¼ˆDGSDï¼‰æ¨¡å—ï¼Œä»¥å¼ºè°ƒä¸åŒé¢‘æ®µçš„ä¼˜ç‚¹å¹¶å¯¹åŒé¢‘æ•°æ®è¿›è¡Œäº’è¡¥å­¦ä¹ ã€‚å®éªŒè¯æ˜ï¼ŒSKDNet-SSRæ–¹æ³•ç›¸æ¯”å…¶ä»–ç›¸å…³æ–¹æ³•å…·æœ‰ä¼˜åŠ¿ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ååŒåˆ†ç±»åŒé¢‘æåŒ–SARå›¾åƒç ”ç©¶å…·æœ‰æ„ä¹‰å’ŒæŒ‘æˆ˜æ€§ã€‚</li>
<li>åŒºåŸŸä¸€è‡´æ€§å’Œæœ‰æ•ˆåˆ©ç”¨åŒé¢‘æ•°æ®æ˜¯ä¸¤å¤§éš¾é¢˜ã€‚</li>
<li>æå‡ºäº†ä¸€ç§ç»“åˆç»Ÿè®¡æ ·æœ¬æ ¡æ­£çš„çŸ¥è¯†è’¸é¦ç½‘ç»œï¼ˆSKDNet-SSRï¼‰ã€‚</li>
<li>SKDNet-SSRä½¿ç”¨CNNå’ŒViTä½œä¸ºç‰¹å¾æå–å™¨ã€‚</li>
<li>è®¾è®¡äº†åŸºäºç»Ÿè®¡çš„åŠ¨æ€æ ·æœ¬æ ¡æ­£ï¼ˆSDSRï¼‰æ¨¡å—ï¼Œä»¥æé«˜åˆ†ç±»ç‰¹å¾æå–è¿‡ç¨‹ã€‚</li>
<li>æ„å»ºåŒé¢‘é—¨é€‰è’¸é¦ï¼ˆDGSDï¼‰æ¨¡å—ï¼Œå®ç°ä¸åŒé¢‘æ®µçš„äº’è¡¥å­¦ä¹ ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.03268">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-2a1080e23c3c1483ed59d8aa89af8476.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c88bb50bc33b486d03b9d30d1ece44ef.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-1bf15c09c0237c577a80d3b8268dbe76.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-9b73315ac860c22f4dfbf37ba83ef986.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="LLaVA-SP-Enhancing-Visual-Representation-with-Visual-Spatial-Tokens-for-MLLMs"><a href="#LLaVA-SP-Enhancing-Visual-Representation-with-Visual-Spatial-Tokens-for-MLLMs" class="headerlink" title="LLaVA-SP: Enhancing Visual Representation with Visual Spatial Tokens for   MLLMs"></a>LLaVA-SP: Enhancing Visual Representation with Visual Spatial Tokens for   MLLMs</h2><p><strong>Authors:Haoran Lou, Chunxiao Fan, Ziyan Liu, Yuexin Wu, Xinliang Wang</strong></p>
<p>The architecture of multimodal large language models (MLLMs) commonly connects a vision encoder, often based on CLIP-ViT, to a large language model. While CLIP-ViT works well for capturing global image features, it struggles to model local relationships between adjacent patches, leading to weaker visual representation, which in turn affects the detailed understanding ability of MLLMs. To solve this, we propose LLaVA-SP, which only adds six spatial visual tokens to the original visual tokens to enhance the visual representation. Our approach offers three key advantages: 1) We propose a novel Projector, which uses convolutional kernels to derive visual spatial tokens from ViT patch features, simulating two visual spatial ordering approaches: â€œfrom central region to globalâ€ and â€œfrom abstract to specificâ€. Then, a cross-attention mechanism is applied to fuse fine-grained visual information, enriching the overall visual representation. 2) We present two model variants: LLaVA-SP-Cropping, which focuses on detail features through progressive cropping, and LLaVA-SP-Pooling, which captures global semantics through adaptive pooling, enabling the model to handle diverse visual understanding tasks. 3) Extensive experiments show that LLaVA-SP, fine-tuned with LoRA, achieves significant performance improvements across various multimodal benchmarks, outperforming the state-of-the-art LLaVA-1.5 model in multiple tasks with nearly identical inference latency. The code and models are available at <a target="_blank" rel="noopener" href="https://github.com/CnFaker/LLaVA-SP">https://github.com/CnFaker/LLaVA-SP</a>. </p>
<blockquote>
<p>å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰çš„æ¶æ„é€šå¸¸å°†åŸºäºCLIP-ViTçš„è§†è§‰ç¼–ç å™¨ä¸å¤§å‹è¯­è¨€æ¨¡å‹ç›¸è¿æ¥ã€‚è™½ç„¶CLIP-ViTåœ¨æ•è·å…¨å±€å›¾åƒç‰¹å¾æ–¹é¢è¡¨ç°è‰¯å¥½ï¼Œä½†åœ¨å¯¹ç›¸é‚»è¡¥ä¸ä¹‹é—´çš„å±€éƒ¨å…³ç³»è¿›è¡Œå»ºæ¨¡æ—¶é‡åˆ°å›°éš¾ï¼Œå¯¼è‡´è§†è§‰è¡¨ç¤ºè¾ƒå¼±ï¼Œè¿›è€Œå½±å“äº†MLLMsçš„è¯¦ç»†ç†è§£èƒ½åŠ›ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†LLaVA-SPï¼Œå®ƒåªåœ¨åŸå§‹è§†è§‰ä»¤ç‰Œä¸Šæ·»åŠ å…­ä¸ªç©ºé—´è§†è§‰ä»¤ç‰Œï¼Œä»¥å¢å¼ºè§†è§‰è¡¨ç¤ºã€‚æˆ‘ä»¬çš„æ–¹æ³•æä¾›äº†ä¸‰ä¸ªå…³é”®ä¼˜åŠ¿ï¼š</p>
</blockquote>
<p>1ï¼‰æˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªä½¿ç”¨å·ç§¯æ ¸ä»ViTè¡¥ä¸ç‰¹å¾ä¸­æ¨å¯¼å‡ºè§†è§‰ç©ºé—´ä»¤ç‰Œçš„æ–°å‹æŠ•å½±ä»ªï¼Œæ¨¡æ‹Ÿä¸¤ç§è§†è§‰ç©ºé—´æ’åºæ–¹æ³•ï¼šâ€œä»ä¸­å¿ƒåŒºåŸŸåˆ°å…¨å±€â€å’Œâ€œä»æŠ½è±¡åˆ°å…·ä½“â€ã€‚ç„¶åï¼Œåº”ç”¨äº¤å‰æ³¨æ„åŠ›æœºåˆ¶èåˆç»†ç²’åº¦è§†è§‰ä¿¡æ¯ï¼Œä¸°å¯Œæ€»ä½“è§†è§‰è¡¨ç¤ºã€‚</p>
<p>2ï¼‰æˆ‘ä»¬æ¨å‡ºäº†ä¸¤ä¸ªæ¨¡å‹å˜ä½“ï¼šLLaVA-SP-Croppingï¼Œå®ƒé€šè¿‡é€æ­¥è£å‰ªå…³æ³¨ç»†èŠ‚ç‰¹å¾ï¼›ä»¥åŠLLaVA-SP-Poolingï¼Œå®ƒé€šè¿‡è‡ªé€‚åº”æ± åŒ–æ•è·å…¨å±€è¯­ä¹‰ï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿå¤„ç†å„ç§è§†è§‰ç†è§£ä»»åŠ¡ã€‚</p>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.00505v3">PDF</a> Accepted to ICCV 2025</p>
<p><strong>Summary</strong><br>     å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹çš„æ¶æ„é€šå¸¸å°†åŸºäºCLIP-ViTçš„è§†è§‰ç¼–ç å™¨ä¸å¤§å‹è¯­è¨€æ¨¡å‹ç›¸è¿æ¥ã€‚CLIP-ViTåœ¨æ•æ‰å…¨å±€å›¾åƒç‰¹å¾æ–¹é¢è¡¨ç°è‰¯å¥½ï¼Œä½†åœ¨å»ºæ¨¡ç›¸é‚»è¡¥ä¸ä¹‹é—´çš„å±€éƒ¨å…³ç³»æ–¹é¢å­˜åœ¨å›°éš¾ï¼Œå¯¼è‡´è§†è§‰è¡¨å¾è¾ƒå¼±ï¼Œè¿›è€Œå½±å“MLLLMçš„è¯¦ç»†ç†è§£èƒ½åŠ›ã€‚ä¸ºè§£å†³æ­¤é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†LLaVA-SPï¼Œå®ƒä»…æ·»åŠ å…­ä¸ªç©ºé—´è§†è§‰ä»¤ç‰Œæ¥å¢å¼ºåŸå§‹è§†è§‰ä»¤ç‰Œçš„è§†è§‰è¡¨å¾ã€‚è¯¥æ–¹æ³•å…·æœ‰ä¸‰ä¸ªå…³é”®ä¼˜åŠ¿ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLLMï¼‰ç»“åˆè§†è§‰ç¼–ç å™¨ï¼ˆå¦‚CLIP-ViTï¼‰ä¸å¤§å‹è¯­è¨€æ¨¡å‹ã€‚</li>
<li>CLIP-ViTåœ¨æ•æ‰å…¨å±€å›¾åƒç‰¹å¾ä¸Šè¡¨ç°è‰¯å¥½ï¼Œä½†åœ¨å»ºæ¨¡å±€éƒ¨å…³ç³»æ–¹é¢å­˜åœ¨ç¼ºé™·ã€‚</li>
<li>LLaVA-SPé€šè¿‡æ·»åŠ ç©ºé—´è§†è§‰ä»¤ç‰Œå¢å¼ºè§†è§‰è¡¨å¾ã€‚</li>
<li>LLaVA-SPä½¿ç”¨å·ç§¯æ ¸è¡ç”Ÿè§†è§‰ç©ºé—´ä»¤ç‰Œï¼Œæ¨¡æ‹Ÿä¸¤ç§è§†è§‰ç©ºé—´æ’åºæ–¹æ³•ã€‚</li>
<li>LLaVA-SPæä¾›ä¸¤ç§æ¨¡å‹å˜ä½“ï¼šLLaVA-SP-Croppingå’ŒLLaVA-SP-Poolingï¼Œåˆ†åˆ«å…³æ³¨ç»†èŠ‚ç‰¹å¾å’Œå…¨å±€è¯­ä¹‰ã€‚</li>
<li>LLaVA-SPç»è¿‡LoRAå¾®è°ƒåï¼Œåœ¨å¤šç§å¤šæ¨¡æ€åŸºå‡†æµ‹è¯•ä¸­å®ç°äº†æ˜¾è‘—çš„æ€§èƒ½æ”¹è¿›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.00505">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-be5c9fd00231d3f2a2a7b7cae5913311.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-dfa2a46ddb2dbd8cabad494288767ebf.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-18f3e8c70e804262427fc3823790f4f9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f55cbef21c35cb975f495f56ca2d935f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8d52eb47551eef98db42a77d24551c0b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-6cfdb8c6fef107017a5f302bb4515b1a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-aa62791c3b052a3b73e1e5b761d537a2.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-912cc9e4ec1be86150becbb0139f1604.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="RL4Med-DDPO-Reinforcement-Learning-for-Controlled-Guidance-Towards-Diverse-Medical-Image-Generation-using-Vision-Language-Foundation-Models"><a href="#RL4Med-DDPO-Reinforcement-Learning-for-Controlled-Guidance-Towards-Diverse-Medical-Image-Generation-using-Vision-Language-Foundation-Models" class="headerlink" title="RL4Med-DDPO: Reinforcement Learning for Controlled Guidance Towards   Diverse Medical Image Generation using Vision-Language Foundation Models"></a>RL4Med-DDPO: Reinforcement Learning for Controlled Guidance Towards   Diverse Medical Image Generation using Vision-Language Foundation Models</h2><p><strong>Authors:Parham Saremi, Amar Kumar, Mohamed Mohamed, Zahra TehraniNasab, Tal Arbel</strong></p>
<p>Vision-Language Foundation Models (VLFM) have shown a tremendous increase in performance in terms of generating high-resolution, photorealistic natural images. While VLFMs show a rich understanding of semantic content across modalities, they often struggle with fine-grained alignment tasks that require precise correspondence between image regions and textual descriptions, a limitation in medical imaging, where accurate localization and detection of clinical features are essential for diagnosis and analysis. To address this issue, we propose a multi-stage architecture where a pre-trained VLFM (e.g. Stable Diffusion) provides a cursory semantic understanding, while a reinforcement learning (RL) algorithm refines the alignment through an iterative process that optimizes for understanding semantic context. The reward signal is designed to align the semantic information of the text with synthesized images. Experiments on the public ISIC2019 skin lesion dataset demonstrate that the proposed method improves (a) the quality of the generated images, and (b) the alignment with the text prompt over the original fine-tuned Stable Diffusion baseline. We also show that the synthesized samples could be used to improve disease classifier performance for underrepresented subgroups through augmentation. Our code is accessible through the project website: <a target="_blank" rel="noopener" href="https://parhamsaremi.github.io/rl4med-ddpo">https://parhamsaremi.github.io/rl4med-ddpo</a> </p>
<blockquote>
<p>è§†è§‰è¯­è¨€åŸºç¡€æ¨¡å‹ï¼ˆVLFMï¼‰åœ¨ç”Ÿæˆé«˜åˆ†è¾¨ç‡ã€é€¼çœŸè‡ªç„¶å›¾åƒæ–¹é¢çš„æ€§èƒ½å¾—åˆ°äº†æå¤§çš„æå‡ã€‚è™½ç„¶VLFMåœ¨å¤šæ¨¡æ€çš„è¯­ä¹‰å†…å®¹ç†è§£æ–¹é¢è¡¨ç°å‡ºä¸°å¯Œçš„èƒ½åŠ›ï¼Œä½†åœ¨éœ€è¦å›¾åƒåŒºåŸŸå’Œæ–‡æœ¬æè¿°ä¹‹é—´ç²¾ç¡®å¯¹åº”çš„ç²¾ç»†å¯¹é½ä»»åŠ¡ä¸­ï¼Œå®ƒä»¬å¸¸å¸¸ä¼šé‡åˆ°å›°éš¾ã€‚åœ¨åŒ»å­¦æˆåƒé¢†åŸŸï¼Œå‡†ç¡®å®šä½å’Œæ£€æµ‹ä¸´åºŠç‰¹å¾æ˜¯è¯Šæ–­å’Œæ²»ç–—çš„å…³é”®ï¼Œè¿™ä¸€é™åˆ¶å°¤ä¸ºé‡è¦ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§å¤šé˜¶æ®µæ¶æ„ï¼Œå…¶ä¸­é¢„è®­ç»ƒçš„VLFMï¼ˆä¾‹å¦‚Stable Diffusionï¼‰æä¾›åŸºæœ¬çš„è¯­ä¹‰ç†è§£ï¼Œè€Œå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰ç®—æ³•é€šè¿‡ä¼˜åŒ–è¯­ä¹‰ä¸Šä¸‹æ–‡ç†è§£çš„è¿­ä»£è¿‡ç¨‹æ¥ç²¾ç»†å¯¹é½ã€‚å¥–åŠ±ä¿¡å·è¢«è®¾è®¡ä¸ºå°†æ–‡æœ¬ä¸åˆæˆå›¾åƒçš„è¯­ä¹‰ä¿¡æ¯å¯¹é½ã€‚åœ¨å…¬å¼€çš„ISIC2019çš®è‚¤ç—…å˜æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œæ‰€æå‡ºçš„æ–¹æ³•æ”¹è¿›äº†ï¼ˆaï¼‰ç”Ÿæˆå›¾åƒçš„è´¨é‡å’Œï¼ˆbï¼‰ä¸æ–‡æœ¬æç¤ºçš„å¯¹é½ç¨‹åº¦ï¼Œè¶…è¿‡äº†åŸå§‹å¾®è°ƒè¿‡çš„Stable DiffusionåŸºçº¿ã€‚æˆ‘ä»¬è¿˜è¡¨æ˜ï¼Œåˆæˆçš„æ ·æœ¬å¯ä»¥é€šè¿‡å¢å¼ºæ¥æ”¹å–„ä»£è¡¨æ€§ä¸è¶³çš„å­ç¾¤ä½“çš„ç–¾ç—…åˆ†ç±»å™¨æ€§èƒ½ã€‚æˆ‘ä»¬çš„ä»£ç å¯é€šè¿‡é¡¹ç›®ç½‘ç«™è·å–ï¼š<a target="_blank" rel="noopener" href="https://parhamsaremi.github.io/rl4med-ddpo">https://parhamsaremi.github.io/rl4med-ddpo</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.15784v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>åŸºäºè§†è§‰è¯­è¨€åŸºç¡€æ¨¡å‹ï¼ˆVLFMï¼‰çš„å¼ºå¤§æ€§èƒ½ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§å¤šé˜¶æ®µæ¶æ„ï¼Œæ—¨åœ¨è§£å†³åŒ»å­¦å›¾åƒä¸­ç²¾ç»†ç²’åº¦å¯¹é½ä»»åŠ¡çš„æŒ‘æˆ˜ã€‚é€šè¿‡é¢„è®­ç»ƒçš„VLFMæä¾›åˆæ­¥è¯­ä¹‰ç†è§£ï¼Œå¹¶ä½¿ç”¨å¼ºåŒ–å­¦ä¹ ç®—æ³•é€šè¿‡è¿­ä»£è¿‡ç¨‹è¿›è¡Œç²¾ç»†åŒ–å¯¹é½ã€‚è¯¥ç ”ç©¶ä½¿ç”¨å…¬å…±çš„ISIC2019çš®è‚¤ç—…å˜æ•°æ®é›†è¿›è¡Œå®éªŒéªŒè¯ï¼Œè¯æ˜è¯¥æ–¹æ³•æé«˜äº†å›¾åƒç”Ÿæˆè´¨é‡å’Œæ–‡æœ¬å¯¹é½åº¦ã€‚æ­¤å¤–ï¼Œåˆæˆçš„æ ·æœ¬å¯ç”¨äºæé«˜ä»£è¡¨æ€§ä¸è¶³çš„äºšç»„ç–¾ç—…åˆ†ç±»å™¨çš„æ€§èƒ½ã€‚ä»£ç å·²ä¸Šä¼ è‡³é¡¹ç›®ç½‘ç«™ä¾›å…¬ä¼—è®¿é—®ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>VLFMåœ¨ç”Ÿæˆé«˜åˆ†è¾¨ç‡ã€é€¼çœŸçš„è‡ªç„¶å›¾åƒæ–¹é¢è¡¨ç°å‡ºå“è¶Šçš„æ€§èƒ½ï¼Œä½†åœ¨åŒ»å­¦æˆåƒä¸­çš„ç²¾ç»†ç²’åº¦å¯¹é½ä»»åŠ¡ä¸Šå­˜åœ¨å±€é™æ€§ã€‚</li>
<li>æå‡ºäº†ä¸€ç§å¤šé˜¶æ®µæ¶æ„ï¼Œç»“åˆé¢„è®­ç»ƒçš„VLFMå’Œå¼ºåŒ–å­¦ä¹ ç®—æ³•ï¼Œä»¥æé«˜å›¾åƒä¸æ–‡æœ¬ä¹‹é—´çš„å¯¹é½ç²¾åº¦ã€‚</li>
<li>åœ¨ISIC2019çš®è‚¤ç—…å˜æ•°æ®é›†ä¸Šçš„å®éªŒéªŒè¯äº†è¯¥æ–¹æ³•çš„æœ‰æ•ˆæ€§ï¼Œæé«˜äº†å›¾åƒç”Ÿæˆè´¨é‡å’Œæ–‡æœ¬å¯¹é½åº¦ã€‚</li>
<li>åˆæˆçš„æ ·æœ¬å¯ç”¨äºå¢å¼ºä»£è¡¨æ€§ä¸è¶³çš„äºšç»„ç–¾ç—…åˆ†ç±»å™¨çš„æ€§èƒ½ã€‚</li>
<li>è¯¥æ–¹æ³•çš„ä»£ç å·²ä¸Šä¼ è‡³é¡¹ç›®ç½‘ç«™ä¾›å…¬ä¼—è®¿é—®å’Œä½¿ç”¨ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.15784">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-30b7f3f323e52035e50679e7b08c6af7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f0d139c364a58464815fbea7ae390756.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e63e87d5f0e3d0ca57e31dad1d36a04d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-aeae1981ea809340dd8b82fedcbdde41.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="Semantic-Alignment-and-Reinforcement-for-Data-Free-Quantization-of-Vision-Transformers"><a href="#Semantic-Alignment-and-Reinforcement-for-Data-Free-Quantization-of-Vision-Transformers" class="headerlink" title="Semantic Alignment and Reinforcement for Data-Free Quantization of   Vision Transformers"></a>Semantic Alignment and Reinforcement for Data-Free Quantization of   Vision Transformers</h2><p><strong>Authors:Yunshan Zhong, Yuyao Zhou, Yuxin Zhang, Wanchen Sui, Shen Li, Yong Li, Fei Chao, Rongrong Ji</strong></p>
<p>Data-free quantization (DFQ) enables model quantization without accessing real data, addressing concerns regarding data security and privacy. With the growing adoption of Vision Transformers (ViTs), DFQ for ViTs has garnered significant attention. However, existing DFQ methods exhibit two limitations: (1) semantic distortion, where the semantics of synthetic images deviate substantially from those of real images, and (2) semantic inadequacy, where synthetic images contain extensive regions with limited content and oversimplified textures, leading to suboptimal quantization performance. To address these limitations, we propose SARDFQ, a novel Semantics Alignment and Reinforcement Data-Free Quantization method for ViTs. To address semantic distortion, SARDFQ incorporates Attention Priors Alignment (APA), which optimizes synthetic images to follow randomly generated structure attention priors. To mitigate semantic inadequacy, SARDFQ introduces Multi-Semantic Reinforcement (MSR), leveraging localized patch optimization to enhance semantic richness across synthetic images. Furthermore, SARDFQ employs Soft-Label Learning (SL), wherein multiple semantic targets are adapted to facilitate the learning of multi-semantic images augmented by MSR. Extensive experiments demonstrate the effectiveness of SARDFQ, significantly surpassing existing methods. For example, SARDFQ improves top-1 accuracy on ImageNet by 15.52% for W4A4 ViT-B. The code is at <a target="_blank" rel="noopener" href="https://github.com/zysxmu/SARDFQ">https://github.com/zysxmu/SARDFQ</a>. </p>
<blockquote>
<p>æ— æ•°æ®é‡åŒ–ï¼ˆDFQï¼‰èƒ½å¤Ÿåœ¨ä¸è®¿é—®çœŸå®æ•°æ®çš„æƒ…å†µä¸‹å®ç°æ¨¡å‹é‡åŒ–ï¼Œè§£å†³äº†å¯¹æ•°æ®å®‰å…¨å’Œéšç§çš„æ‹…å¿§ã€‚éšç€è§†è§‰å˜å‹å™¨ï¼ˆViTï¼‰çš„å¹¿æ³›åº”ç”¨ï¼ŒViTçš„æ— æ•°æ®é‡åŒ–ï¼ˆDFQï¼‰å·²å¼•èµ·å¹¿æ³›å…³æ³¨ã€‚ç„¶è€Œï¼Œç°æœ‰çš„DFQæ–¹æ³•å­˜åœ¨ä¸¤ä¸ªå±€é™æ€§ï¼šï¼ˆ1ï¼‰è¯­ä¹‰å¤±çœŸï¼Œå³åˆæˆå›¾åƒçš„è¯­ä¹‰ä¸çœŸå®å›¾åƒçš„è¯­ä¹‰å­˜åœ¨è¾ƒå¤§åå·®ï¼›ï¼ˆ2ï¼‰è¯­ä¹‰ä¸è¶³ï¼Œåˆæˆå›¾åƒåŒ…å«å¤§é‡å†…å®¹æœ‰é™ã€çº¹ç†è¿‡äºç®€å•çš„åŒºåŸŸï¼Œå¯¼è‡´é‡åŒ–æ€§èƒ½ä¸ä½³ã€‚ä¸ºäº†è§£å†³è¿™äº›å±€é™æ€§ï¼Œæˆ‘ä»¬æå‡ºäº†SARDFQï¼Œä¸€ç§æ–°å‹çš„ç”¨äºè§†è§‰å˜å‹å™¨çš„è¯­ä¹‰å¯¹é½å’Œå¼ºåŒ–æ— æ•°æ®é‡åŒ–æ–¹æ³•ã€‚ä¸ºè§£å†³è¯­ä¹‰å¤±çœŸé—®é¢˜ï¼ŒSARDFQå¼•å…¥äº†æ³¨æ„åŠ›å…ˆéªŒå¯¹é½ï¼ˆAPAï¼‰ï¼Œä¼˜åŒ–åˆæˆå›¾åƒä»¥éµå¾ªéšæœºç”Ÿæˆçš„ç»“æ„æ³¨æ„åŠ›å…ˆéªŒã€‚ä¸ºäº†ç¼“è§£è¯­ä¹‰ä¸è¶³çš„é—®é¢˜ï¼ŒSARDFQå¼•å…¥äº†å¤šè¯­ä¹‰å¼ºåŒ–ï¼ˆMSRï¼‰ï¼Œåˆ©ç”¨å±€éƒ¨è¡¥ä¸ä¼˜åŒ–æ¥å¢å¼ºåˆæˆå›¾åƒä¸­çš„è¯­ä¹‰ä¸°å¯Œæ€§ã€‚æ­¤å¤–ï¼ŒSARDFQè¿˜é‡‡ç”¨äº†è½¯æ ‡ç­¾å­¦ä¹ ï¼ˆSLï¼‰ï¼Œé€‚åº”å¤šä¸ªè¯­ä¹‰ç›®æ ‡ï¼Œä»¥ä¿ƒè¿›ç”±MSRå¢å¼ºçš„å¤šè¯­ä¹‰å›¾åƒçš„å­¦ä¹ ã€‚å¤§é‡å®éªŒè¯æ˜äº†SARDFQçš„æœ‰æ•ˆæ€§ï¼Œæ˜¾è‘—è¶…è¶Šäº†ç°æœ‰æ–¹æ³•ã€‚ä¾‹å¦‚ï¼ŒSARDFQåœ¨ImageNetä¸Šçš„top-1å‡†ç¡®ç‡æé«˜äº†15.52%ï¼Œé€‚ç”¨äºW4A4 ViT-Bã€‚ä»£ç åœ°å€ä¸ºï¼š<a target="_blank" rel="noopener" href="https://github.com/zysxmu/SARDFQ%E3%80%82">https://github.com/zysxmu/SARDFQã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.16553v4">PDF</a> ICCV2025</p>
<p><strong>Summary</strong><br>æ•°æ®æ— å…³é‡åŒ–ï¼ˆDFQï¼‰å¯åœ¨ä¸æ¥è§¦çœŸå®æ•°æ®çš„æƒ…å†µä¸‹å®ç°æ¨¡å‹é‡åŒ–ï¼Œå…³æ³¨æ•°æ®å®‰å…¨å’Œéšç§ä¿æŠ¤ã€‚é’ˆå¯¹è§†è§‰å˜å‹å™¨ï¼ˆViTsï¼‰çš„DFQå­˜åœ¨è¯­ä¹‰å¤±çœŸå’Œè¯­ä¹‰ä¸è¶³çš„é—®é¢˜ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†SARDFQï¼Œä¸€ç§æ–°é¢–çš„è¯­ä¹‰å¯¹é½å’Œå¼ºåŒ–æ•°æ®æ— å…³é‡åŒ–æ–¹æ³•ã€‚å®ƒé€šè¿‡Attention Prior Alignmentä¼˜åŒ–åˆæˆå›¾åƒä»¥éµå¾ªéšæœºç”Ÿæˆçš„ç»“æ„æ³¨æ„åŠ›å…ˆéªŒæ¥è§£å†³è¯­ä¹‰å¤±çœŸé—®é¢˜ã€‚æ­¤å¤–ï¼ŒSARDFQé‡‡ç”¨å¤šè¯­ä¹‰å¼ºåŒ–å’Œå¤šè¯­ä¹‰å›¾åƒå­¦ä¹ æ¥è§£å†³è¯­ä¹‰ä¸è¶³çš„é—®é¢˜ã€‚å®éªŒè¡¨æ˜ï¼ŒSARDFQæ•ˆæœæ˜¾è‘—ï¼Œåœ¨ImageNetä¸Šï¼ŒW4A4 ViT-Bçš„top-1å‡†ç¡®ç‡æé«˜äº†15.52%ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ•°æ®æ— å…³é‡åŒ–ï¼ˆDFQï¼‰å¯ä»¥åœ¨ä¸æ¥è§¦çœŸå®æ•°æ®çš„æƒ…å†µä¸‹å®ç°æ¨¡å‹é‡åŒ–ï¼Œé‡è§†æ•°æ®å®‰å…¨å’Œéšç§ä¿æŠ¤ã€‚</li>
<li>ç°æœ‰DFQæ–¹æ³•å­˜åœ¨è¯­ä¹‰å¤±çœŸå’Œè¯­ä¹‰ä¸è¶³çš„é—®é¢˜ã€‚</li>
<li>SARDFQæ˜¯ä¸€ç§é’ˆå¯¹è§†è§‰å˜å‹å™¨ï¼ˆViTsï¼‰çš„æ–°çš„DFQæ–¹æ³•ï¼Œé€šè¿‡Attention Prior Alignmentè§£å†³è¯­ä¹‰å¤±çœŸé—®é¢˜ã€‚</li>
<li>SARDFQé‡‡ç”¨å¤šè¯­ä¹‰å¼ºåŒ–ï¼ˆMSRï¼‰å’Œè½¯æ ‡ç­¾å­¦ä¹ ï¼ˆSLï¼‰æ¥è§£å†³è¯­ä¹‰ä¸è¶³çš„é—®é¢˜ï¼Œæé«˜åˆæˆå›¾åƒçš„è¯­ä¹‰ä¸°å¯Œæ€§ã€‚</li>
<li>SARDFQé€šè¿‡ä¼˜åŒ–åˆæˆå›¾åƒçš„ç»“æ„å’Œè¯­ä¹‰ï¼Œå®ç°äº†å¯¹ç°æœ‰æ–¹æ³•çš„æ˜¾è‘—è¶…è¶Šã€‚</li>
<li>åœ¨ImageNetä¸Šï¼ŒSARDFQå¯¹W4A4 ViT-Bçš„top-1å‡†ç¡®ç‡æé«˜äº†15.52%ã€‚</li>
<li>SARDFQçš„ä»£ç å·²å…¬å¼€åœ¨<a target="_blank" rel="noopener" href="https://github.com/zysxmu/SARDFQ%E3%80%82">https://github.com/zysxmu/SARDFQã€‚</a></li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.16553">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-6236f6d3febe66dcf8b706f1b83798bf.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-fd05a03ab7e79073e1962828e14e023a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-18fa203703979bf85a9848cdeea83d76.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-27d74ebe7e661fa8002e28903c1a4c75.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-2c766735de2aff444b0351c2a19c5c39.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-07-09/Vision%20Transformer/">https://kedreamix.github.io/Talk2Paper/Paper/2025-07-09/Vision%20Transformer/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Vision-Transformer/">
                                    <span class="chip bg-color">Vision Transformer</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-07-09/%E6%A3%80%E6%B5%8B_%E5%88%86%E5%89%B2_%E8%B7%9F%E8%B8%AA/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-597ea0e761098ab072596ed0b825dc8b.jpg" class="responsive-img" alt="æ£€æµ‹/åˆ†å‰²/è·Ÿè¸ª">
                        
                        <span class="card-title">æ£€æµ‹/åˆ†å‰²/è·Ÿè¸ª</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            æ£€æµ‹/åˆ†å‰²/è·Ÿè¸ª æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-07-09  CVFusion Cross-View Fusion of 4D Radar and Camera for 3D Object   Detection
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-07-09
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E6%A3%80%E6%B5%8B-%E5%88%86%E5%89%B2-%E8%B7%9F%E8%B8%AA/" class="post-category">
                                    æ£€æµ‹/åˆ†å‰²/è·Ÿè¸ª
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E6%A3%80%E6%B5%8B-%E5%88%86%E5%89%B2-%E8%B7%9F%E8%B8%AA/">
                        <span class="chip bg-color">æ£€æµ‹/åˆ†å‰²/è·Ÿè¸ª</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-07-09/%E8%A7%86%E9%A2%91%E7%90%86%E8%A7%A3/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-a2584c8354bd85346cf7bb100ad236ec.jpg" class="responsive-img" alt="è§†é¢‘ç†è§£">
                        
                        <span class="card-title">è§†é¢‘ç†è§£</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            è§†é¢‘ç†è§£ æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-07-09  M$^3$-Med A Benchmark for Multi-lingual, Multi-modal, and Multi-hop   Reasoning in Medical Instructional Video Understanding
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-07-09
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E8%A7%86%E9%A2%91%E7%90%86%E8%A7%A3/" class="post-category">
                                    è§†é¢‘ç†è§£
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E8%A7%86%E9%A2%91%E7%90%86%E8%A7%A3/">
                        <span class="chip bg-color">è§†é¢‘ç†è§£</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">28879.4k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
