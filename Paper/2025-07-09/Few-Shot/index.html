<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Few-Shot">
    <meta name="description" content="Few-Shot æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-07-09  AI-Driven Cytomorphology Image Synthesis for Medical Diagnostics">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Few-Shot | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-cbfeb5646c4011c48924e41eea6906a4.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Few-Shot</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Few-Shot/">
                                <span class="chip bg-color">Few-Shot</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Few-Shot/" class="post-category">
                                Few-Shot
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-07-09
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-07-17
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    14.5k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    59 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-07-09-æ›´æ–°"><a href="#2025-07-09-æ›´æ–°" class="headerlink" title="2025-07-09 æ›´æ–°"></a>2025-07-09 æ›´æ–°</h1><h2 id="AI-Driven-Cytomorphology-Image-Synthesis-for-Medical-Diagnostics"><a href="#AI-Driven-Cytomorphology-Image-Synthesis-for-Medical-Diagnostics" class="headerlink" title="AI-Driven Cytomorphology Image Synthesis for Medical Diagnostics"></a>AI-Driven Cytomorphology Image Synthesis for Medical Diagnostics</h2><p><strong>Authors:Jan Carreras Boada, Rao Muhammad Umer, Carsten Marr</strong></p>
<p>Biomedical datasets often contain a large sample imbalance and are subject to strict privacy constraints, which together hinder the development of accurate machine learning models. One potential solution is to generate synthetic images, as this can improve data availability while preserving patient privacy. However, it remains difficult to generate synthetic images of sufficient quality for training robust classifiers. In this work, we focus on the classification of single white blood cells, a key component in the diagnosis of hematological diseases such as acute myeloid leukemia (AML), a severe blood cancer. We demonstrate how synthetic images generated with a fine-tuned stable diffusion model using LoRA weights when guided by real few-shot samples of the target white blood cell classes, can enhance classifier performance for limited data. When training a ResNet classifier, accuracy increased from 27.3% to 78.4% (+51.1%) by adding 5000 synthetic images per class to a small and highly imbalanced real dataset. For a CLIP-based classifier, the accuracy improved from 61.8% to 76.8% (+15.0%). The synthetic images are highly similar to real images, and they can help overcome dataset limitations, enhancing model generalization. Our results establish synthetic images as a tool in biomedical research, improving machine learning models, and facilitating medical diagnosis and research. </p>
<blockquote>
<p>ç”Ÿç‰©åŒ»å­¦æ•°æ®é›†é€šå¸¸åŒ…å«å¤§é‡çš„æ ·æœ¬ä¸å¹³è¡¡é—®é¢˜ï¼Œå¹¶å—åˆ°ä¸¥æ ¼çš„éšç§çº¦æŸçš„é™åˆ¶ï¼Œè¿™ä¸¤è€…å…±åŒé˜»ç¢äº†å‡†ç¡®æœºå™¨å­¦ä¹ æ¨¡å‹çš„å‘å±•ã€‚ä¸€ç§æ½œåœ¨çš„è§£å†³æ–¹æ¡ˆæ˜¯ç”Ÿæˆåˆæˆå›¾åƒï¼Œå› ä¸ºè¿™å¯ä»¥æé«˜æ•°æ®å¯ç”¨æ€§åŒæ—¶ä¿æŠ¤æ‚£è€…éšç§ã€‚ç„¶è€Œï¼Œç”Ÿæˆè¶³å¤Ÿè´¨é‡çš„åˆæˆå›¾åƒä»¥è®­ç»ƒç¨³å¥çš„åˆ†ç±»å™¨ä»ç„¶æ˜¯ä¸€é¡¹æŒ‘æˆ˜ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬ä¸“æ³¨äºå•ä¸€ç™½è¡€çƒçš„åˆ†ç±»ï¼Œè¿™æ˜¯æ€¥æ€§é«“ç³»ç™½è¡€ç—…ï¼ˆAMLï¼‰ç­‰è¡€æ¶²ç™Œç–¾ç—…è¯Šæ–­çš„å…³é”®ç»„æˆéƒ¨åˆ†ã€‚æˆ‘ä»¬å±•ç¤ºäº†å¦‚ä½•ä½¿ç”¨ç»è¿‡å¾®è°ƒçš„ç¨³å®šæ‰©æ•£æ¨¡å‹ï¼Œé€šè¿‡çœŸå®å°‘é‡ç›®æ ‡ç™½è¡€çƒç±»åˆ«çš„æ ·æœ¬ä½œä¸ºæŒ‡å¯¼æ¥ç”Ÿæˆåˆæˆå›¾åƒï¼Œä»¥æé«˜æœ‰é™æ•°æ®çš„åˆ†ç±»å™¨æ€§èƒ½ã€‚å½“è®­ç»ƒResNetåˆ†ç±»å™¨æ—¶ï¼Œé€šè¿‡åœ¨é«˜åº¦ä¸å¹³è¡¡çš„å°å‹çœŸå®æ•°æ®é›†ä¸­æ¯ç±»æ·»åŠ 5000å¼ åˆæˆå›¾åƒï¼Œå‡†ç¡®ç‡ä»27.3%æé«˜åˆ°78.4%ï¼ˆ+51.1%ï¼‰ã€‚å¯¹äºåŸºäºCLIPçš„åˆ†ç±»å™¨ï¼Œå‡†ç¡®ç‡ä»61.8%æé«˜åˆ°76.8%ï¼ˆ+15.0%ï¼‰ã€‚åˆæˆå›¾åƒä¸çœŸå®å›¾åƒé«˜åº¦ç›¸ä¼¼ï¼Œæœ‰åŠ©äºå…‹æœæ•°æ®é›†é™åˆ¶ï¼Œæé«˜æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ã€‚æˆ‘ä»¬çš„ç»“æœå°†åˆæˆå›¾åƒç¡®ç«‹ä¸ºç”Ÿç‰©åŒ»å­¦ç ”ç©¶ä¸­çš„ä¸€ç§å·¥å…·ï¼Œå¯æ”¹å–„æœºå™¨å­¦ä¹ æ¨¡å‹ï¼Œå¹¶ä¿ƒè¿›åŒ»å­¦è¯Šæ–­å’Œç ”ç©¶çš„è¿›è¡Œã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.05063v1">PDF</a> 8 pages, 6 figures, 2 tables. Final Degree Project (TFG) submitted at   ESCI-UPF and conducted at Helmholtz Munich</p>
<p><strong>Summary</strong><br>ç”Ÿç‰©åŒ»å­¦æ•°æ®é›†ä¸­å­˜åœ¨æ ·æœ¬ä¸å‡è¡¡å’Œéšç§ä¿æŠ¤éš¾é¢˜ï¼Œåˆæˆå›¾åƒèƒ½å¢åŠ æ•°æ®å¯ç”¨æ€§å’Œä¿æŠ¤æ‚£è€…éšç§ã€‚ç ”ç©¶ä½¿ç”¨å¾®è°ƒåçš„ç¨³å®šæ‰©æ•£æ¨¡å‹ç”Ÿæˆç›®æ ‡ç™½ç»†èƒç±»åˆ«çš„çœŸå®å°‘æ ·æœ¬å¼•å¯¼çš„åˆæˆå›¾åƒï¼Œèƒ½æé«˜åˆ†ç±»å™¨æ€§èƒ½ã€‚ä½¿ç”¨åˆæˆå›¾åƒåï¼Œåˆ†ç±»å™¨å‡†ç¡®ç‡æ˜¾è‘—æé«˜ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç”Ÿç‰©åŒ»å­¦æ•°æ®é›†é¢ä¸´æ ·æœ¬ä¸å‡è¡¡å’Œéšç§ä¿æŠ¤æŒ‘æˆ˜ã€‚</li>
<li>åˆæˆå›¾åƒèƒ½æé«˜æ•°æ®å¯ç”¨æ€§å’Œä¿æŠ¤æ‚£è€…éšç§ã€‚</li>
<li>ç ”ç©¶ä½¿ç”¨å¾®è°ƒç¨³å®šæ‰©æ•£æ¨¡å‹ç”Ÿæˆåˆæˆå›¾åƒï¼Œç”¨äºç™½ç»†èƒåˆ†ç±»ã€‚</li>
<li>ä½¿ç”¨åˆæˆå›¾åƒåï¼Œåˆ†ç±»å™¨æ€§èƒ½æ˜¾è‘—æé«˜ï¼Œå‡†ç¡®ç‡å¢åŠ ã€‚</li>
<li>åˆæˆå›¾åƒä¸çœŸå®å›¾åƒé«˜åº¦ç›¸ä¼¼ã€‚</li>
<li>åˆæˆå›¾åƒæœ‰åŠ©äºå…‹æœæ•°æ®é›†é™åˆ¶ï¼Œæé«˜æ¨¡å‹æ³›åŒ–èƒ½åŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.05063">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-bf4af644bf24010333f4fb8bda09822e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-740645f663f87532e673d997004ae128.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c6ebe5d1c347b5b6edace32166af640b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-49aad50fd218bb392795b92cc1f8e6d9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c468a876f50d547475b6a0ca545eeb8a.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Geometric-Guided-Few-Shot-Dental-Landmark-Detection-with-Human-Centric-Foundation-Model"><a href="#Geometric-Guided-Few-Shot-Dental-Landmark-Detection-with-Human-Centric-Foundation-Model" class="headerlink" title="Geometric-Guided Few-Shot Dental Landmark Detection with Human-Centric   Foundation Model"></a>Geometric-Guided Few-Shot Dental Landmark Detection with Human-Centric   Foundation Model</h2><p><strong>Authors:Anbang Wang, Marawan Elbatel, Keyuan Liu, Lizhuo Lin, Meng Lan, Yanqi Yang, Xiaomeng Li</strong></p>
<p>Accurate detection of anatomic landmarks is essential for assessing alveolar bone and root conditions, thereby optimizing clinical outcomes in orthodontics, periodontics, and implant dentistry. Manual annotation of landmarks on cone-beam computed tomography (CBCT) by dentists is time-consuming, labor-intensive, and subject to inter-observer variability. Deep learning-based automated methods present a promising approach to streamline this process efficiently. However, the scarcity of training data and the high cost of expert annotations hinder the adoption of conventional deep learning techniques. To overcome these challenges, we introduce GeoSapiens, a novel few-shot learning framework designed for robust dental landmark detection using limited annotated CBCT of anterior teeth. Our GeoSapiens framework comprises two key components: (1) a robust baseline adapted from Sapiens, a foundational model that has achieved state-of-the-art performance in human-centric vision tasks, and (2) a novel geometric loss function that improves the modelâ€™s capacity to capture critical geometric relationships among anatomical structures. Experiments conducted on our collected dataset of anterior teeth landmarks revealed that GeoSapiens surpassed existing landmark detection methods, outperforming the leading approach by an 8.18% higher success detection rate at a strict 0.5 mm threshold-a standard widely recognized in dental diagnostics. Code is available at: <a target="_blank" rel="noopener" href="https://github.com/xmed-lab/GeoSapiens">https://github.com/xmed-lab/GeoSapiens</a>. </p>
<blockquote>
<p>å‡†ç¡®æ£€æµ‹è§£å‰–éƒ¨ä½æ ‡å¿—å¯¹äºè¯„ä¼°ç‰™æ§½éª¨å’Œç‰™æ ¹çŠ¶å†µè‡³å…³é‡è¦ï¼Œè¿™å¯¹äºä¼˜åŒ–å£è…”æ­£ç•¸å­¦ã€ç‰™å‘¨ç—…å­¦å’Œç‰™ç§‘æ¤å…¥ç‰©çš„ä¸´åºŠç»“æœè‡³å…³é‡è¦ã€‚ç‰™åŒ»åœ¨é”¥å½¢æŸè®¡ç®—æœºæ–­å±‚æ‰«æï¼ˆCBCTï¼‰ä¸Šå¯¹æ ‡å¿—è¿›è¡Œæ‰‹åŠ¨æ³¨é‡Šè€—æ—¶ä¸”åŠ³åŠ›å¯†é›†ï¼Œå¹¶å­˜åœ¨è§‚å¯Ÿè€…é—´å˜å¼‚ã€‚åŸºäºæ·±åº¦å­¦ä¹ çš„è‡ªåŠ¨åŒ–æ–¹æ³•æä¾›äº†ä¸€ä¸ªæœ‰æ•ˆä¼˜åŒ–æ­¤æµç¨‹çš„æœ‰å‰é€”çš„æ–¹æ³•ã€‚ç„¶è€Œï¼Œè®­ç»ƒæ•°æ®çš„ç¨€ç¼ºå’Œä¸“å®¶æ³¨é‡Šçš„é«˜æˆæœ¬é˜»ç¢äº†ä¼ ç»Ÿæ·±åº¦å­¦ä¹ æŠ€æœ¯çš„é‡‡ç”¨ã€‚ä¸ºäº†å…‹æœè¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†GeoSapiensï¼Œè¿™æ˜¯ä¸€ä¸ªç”¨äºç¨³å¥ç‰™é½¿æ ‡å¿—æ£€æµ‹çš„æ–°å‹å°‘æ ·æœ¬å­¦ä¹ æ¡†æ¶ï¼Œè¯¥æ¡†æ¶ä½¿ç”¨æœ‰é™æ³¨é‡Šçš„ä¸Šå‰ç‰™CBCTæ•°æ®ã€‚æˆ‘ä»¬çš„GeoSapiensæ¡†æ¶åŒ…å«ä¸¤ä¸ªå…³é”®ç»„æˆéƒ¨åˆ†ï¼šï¼ˆ1ï¼‰ä¸€ä¸ªåŸºäºSapiensçš„ç¨³å¥åŸºçº¿ï¼ŒSapiensæ˜¯ä¸€ä¸ªåœ¨ä»¥äººä¸ºä¸­å¿ƒçš„ä»»åŠ¡ä¸­è¾¾åˆ°æœ€æ–°æ°´å¹³çš„åŸºå‡†æ¨¡å‹ï¼›ï¼ˆ2ï¼‰ä¸€ä¸ªæ–°çš„å‡ ä½•æŸå¤±å‡½æ•°ï¼Œæé«˜äº†æ¨¡å‹æ•æ‰è§£å‰–ç»“æ„ä¹‹é—´å…³é”®å‡ ä½•å…³ç³»çš„èƒ½åŠ›ã€‚åœ¨æˆ‘ä»¬æ”¶é›†çš„ä¸Šå‰ç‰™æ ‡å¿—æ•°æ®é›†ä¸Šè¿›è¡Œçš„å®éªŒè¡¨æ˜ï¼ŒGeoSapiensè¶…è¶Šäº†ç°æœ‰çš„æ ‡å¿—æ£€æµ‹æ–¹æ³•ï¼Œåœ¨ä¸¥æ ¼çš„0.5æ¯«ç±³é˜ˆå€¼æ ‡å‡†ä¸‹ï¼ŒæˆåŠŸæ£€æµ‹ç‡æ¯”é¢†å…ˆçš„æ–¹æ³•é«˜å‡º8.18%ï¼Œè¿™ä¸€æ ‡å‡†åœ¨ç‰™ç§‘è¯Šæ–­ä¸­å¾—åˆ°äº†å¹¿æ³›è®¤å¯ã€‚ä»£ç å¯åœ¨ï¼š<a target="_blank" rel="noopener" href="https://github.com/xmed-lab/GeoSapiens%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/xmed-lab/GeoSapiensè·å–ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.04710v1">PDF</a> MICCAI 2025</p>
<p><strong>Summary</strong>ï¼š</p>
<p>æœ¬æ–‡ä»‹ç»äº†åŸºäºæ·±åº¦å­¦ä¹ æŠ€æœ¯çš„GeoSapiensæ¡†æ¶ï¼Œç”¨äºé€šè¿‡å°‘é‡æ ‡æ³¨çš„é”¥å½¢æŸè®¡ç®—æœºæ–­å±‚æ‰«æï¼ˆCBCTï¼‰æ•°æ®ï¼Œå®ç°å¯¹ç‰™é½¿è§£å‰–æ ‡å¿—ç‚¹çš„å‡†ç¡®ã€é«˜æ•ˆæ£€æµ‹ã€‚è¯¥æ¡†æ¶ç»“åˆäº†å¼ºå¤§çš„åŸºçº¿æ¨¡å‹å’Œæ–°å‹å‡ ä½•æŸå¤±å‡½æ•°ï¼Œèƒ½æœ‰æ•ˆæ•æ‰è§£å‰–ç»“æ„é—´çš„å‡ ä½•å…³ç³»ã€‚åœ¨ç‰™é½¿å‰éƒ¨æ ‡å¿—ç‚¹æ£€æµ‹æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒGeoSapiensåœ¨æ£€æµ‹æˆåŠŸç‡ä¸Šè¶…è¶Šäº†ç°æœ‰æ–¹æ³•ï¼Œè¾¾åˆ°äº†ä¸šç•Œé¢†å…ˆçš„æ€§èƒ½æ°´å¹³ã€‚è¯¥ä»£ç å·²å…¬å¼€äºGitHubä¸Šã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>GeoSapiensæ¡†æ¶å¯¹äºç‰™é½¿è§£å‰–æ ‡å¿—ç‚¹çš„è‡ªåŠ¨æ£€æµ‹å…·æœ‰é‡å¤§æ„ä¹‰ï¼Œå¯åº”ç”¨äºæ­£ç•¸ã€ç‰™å‘¨ç—…å’Œç§æ¤ç‰™ç­‰é¢†åŸŸçš„ä¸´åºŠè¯„ä¼°ã€‚</li>
<li>ä¼ ç»Ÿçš„æ·±åº¦å­¦ä¹ æŠ€æœ¯å› è®­ç»ƒæ•°æ®ç¨€ç¼ºå’Œæ ‡æ³¨æˆæœ¬é«˜æ˜‚è€Œé¢ä¸´æŒ‘æˆ˜ï¼ŒGeoSapiensé€šè¿‡å¼•å…¥å°‘æ ·æœ¬å­¦ä¹ æŠ€æœ¯è§£å†³äº†è¿™ä¸€é—®é¢˜ã€‚</li>
<li>GeoSapiensåŒ…å«ä¸¤ä¸ªå…³é”®ç»„ä»¶ï¼šåŸºäºSapiensæ¨¡å‹çš„ç¨³å¥åŸºçº¿ï¼Œä»¥åŠç”¨äºå¢å¼ºæ¨¡å‹æ•æ‰å‡ ä½•å…³ç³»èƒ½åŠ›çš„å‡ ä½•æŸå¤±å‡½æ•°ã€‚</li>
<li>å®éªŒç»“æœæ˜¾ç¤ºï¼ŒGeoSapiensåœ¨ç‰™é½¿å‰éƒ¨æ ‡å¿—ç‚¹æ£€æµ‹ä¸Šå–å¾—äº†çªç ´æ€§çš„æˆæœï¼ŒæˆåŠŸæ£€æµ‹ç‡é«˜äºç°æœ‰æ–¹æ³•8.18%ã€‚</li>
<li>è¯¥æ¡†æ¶çš„åº”ç”¨æœ‰åŠ©äºç®€åŒ–é”¥å½¢æŸè®¡ç®—æœºæ–­å±‚æ‰«ææ•°æ®çš„å¤„ç†æµç¨‹ï¼Œæé«˜è¯Šæ–­æ•ˆç‡åŠå‡†ç¡®æ€§ã€‚</li>
<li>ä»£ç å·²åœ¨GitHubä¸Šå…¬å¼€ï¼Œä¾¿äºç ”ç©¶è€…ä½¿ç”¨å’Œè¿›ä¸€æ­¥å¼€å‘ã€‚</li>
<li>è¯¥ç ”ç©¶å±•ç¤ºäº†æ·±åº¦å­¦ä¹ å’Œå°‘æ ·æœ¬å­¦ä¹ åœ¨åŒ»å­¦å›¾åƒå¤„ç†ä¸­çš„å·¨å¤§æ½œåŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.04710">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-0f2510955ae636004b14c885de732c42.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d86e7c9116b28dbc59a422d07f743fee.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-88ece415f1a4fe4d45ab81e89bd21961.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="ZERO-Multi-modal-Prompt-based-Visual-Grounding"><a href="#ZERO-Multi-modal-Prompt-based-Visual-Grounding" class="headerlink" title="ZERO: Multi-modal Prompt-based Visual Grounding"></a>ZERO: Multi-modal Prompt-based Visual Grounding</h2><p><strong>Authors:Sangbum Choi, Kyeongryeol Go</strong></p>
<p>Recent advances in artificial intelligence have led to the emergence of foundation models, large-scale pre-trained neural networks that serve as versatile starting points for a wide range of downstream tasks. In this work, we present ZERO, a zero-shot multi-prompt object detection model specifically designed for robust, production-ready deployment across diverse industrial domains. ZERO integrates direct image input with multiple user-defined prompts, which can include both textual and visual cues, and processes them through dedicated encoders to generate accurate detection outputs. The model architecture is optimized for scalability, with a total of 1.033 TFLOPS and 622.346 million parameters, and is trained using a domain-specific image database exceeding one billion images. For the CVPR 2025 Foundational Few-Shot Object Detection (FSOD) Challenge, we introduce a domain-specific fine-tuning strategy that emphasizes prompt diversity and conservative pseudo-labeling, enabling effective adaptation to new domains with minimal supervision. Our approach demonstrates practical advantages in flexibility, efficiency, and real-world applicability, achieving strong performance on the RF20VL-fsod benchmark despite limited annotation budgets. The results highlight the potential of prompt-driven, data-centric AI for scalable and adaptive object detection in dynamic industrial environments. </p>
<blockquote>
<p>è¿‘æœŸäººå·¥æ™ºèƒ½çš„è¿›æ­¥å‚¬ç”Ÿäº†åŸºç¡€æ¨¡å‹çš„å‡ºç°ï¼Œè¿™äº›åŸºç¡€æ¨¡å‹æ˜¯å¤§å‹é¢„è®­ç»ƒç¥ç»ç½‘ç»œï¼Œå¯ä½œä¸ºå¤šç§ä¸‹æ¸¸ä»»åŠ¡çš„é€šç”¨èµ·ç‚¹ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æ¨å‡ºäº†ZEROï¼Œè¿™æ˜¯ä¸€æ¬¾é›¶æ ·æœ¬å¤šæç¤ºå¯¹è±¡æ£€æµ‹æ¨¡å‹ï¼Œä¸“ä¸ºåœ¨å„ç§å·¥ä¸šé¢†åŸŸä¸­ç¨³å¥ã€æŠ•å…¥ç”Ÿäº§çš„éƒ¨ç½²è€Œè®¾è®¡ã€‚ZEROç›´æ¥æ•´åˆå›¾åƒè¾“å…¥ä¸ç”¨æˆ·å®šä¹‰çš„å¤šä¸ªæç¤ºï¼Œè¿™äº›æç¤ºå¯ä»¥åŒ…æ‹¬æ–‡æœ¬å’Œè§†è§‰çº¿ç´¢ï¼Œå¹¶é€šè¿‡ä¸“ç”¨ç¼–ç å™¨è¿›è¡Œå¤„ç†ï¼Œä»¥ç”Ÿæˆå‡†ç¡®çš„æ£€æµ‹ç»“æœã€‚è¯¥æ¨¡å‹æ¶æ„è¿›è¡Œäº†ä¼˜åŒ–ï¼Œä»¥å®ç°å¯æ‰©å±•æ€§ï¼Œæ€»è®¡1.033 TFLOPSå’Œ6.223äº¿ä¸ªå‚æ•°ï¼Œå¹¶ä½¿ç”¨è¶…è¿‡1äº¿å¼ å›¾åƒçš„ä¸“ä¸šå›¾åƒæ•°æ®åº“è¿›è¡Œè®­ç»ƒã€‚é’ˆå¯¹CVPR 2025åŸºç¡€å°‘æ ·æœ¬å¯¹è±¡æ£€æµ‹ï¼ˆFSODï¼‰æŒ‘æˆ˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§é¢†åŸŸç‰¹å®šçš„å¾®è°ƒç­–ç•¥ï¼Œè¯¥ç­–ç•¥å¼ºè°ƒæç¤ºå¤šæ ·æ€§å’Œä¿å®ˆä¼ªæ ‡ç­¾ï¼Œèƒ½å¤Ÿåœ¨æ–°é¢†åŸŸè¿›è¡Œæœ‰æ•ˆé€‚åº”ï¼Œç›‘ç£æœ€å°‘ã€‚æˆ‘ä»¬çš„æ–¹æ³•åœ¨çµæ´»æ€§ã€æ•ˆç‡å’Œç°å®ä¸–ç•Œåº”ç”¨æ–¹é¢å±•ç°äº†å®é™…ä¼˜åŠ¿ï¼Œåœ¨RF20VL-fsodåŸºå‡†æµ‹è¯•ä¸­å–å¾—äº†å¼ºåŠ²çš„è¡¨ç°ï¼Œå°½ç®¡æ ‡æ³¨é¢„ç®—æœ‰é™ã€‚ç»“æœçªå‡ºäº†æç¤ºé©±åŠ¨ã€ä»¥æ•°æ®ä¸ºä¸­å¿ƒçš„AIåœ¨åŠ¨æ€å·¥ä¸šç¯å¢ƒä¸­è¿›è¡Œå¯æ‰©å±•å’Œè‡ªé€‚åº”å¯¹è±¡æ£€æµ‹çš„æ½œåŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.04270v1">PDF</a> A solution report for CVPR2025 Foundational FSOD Challenge</p>
<p><strong>Summary</strong></p>
<p>åŸºäºäººå·¥æ™ºèƒ½çš„æœ€æ–°è¿›å±•ï¼Œå‡ºç°äº†ä¸€ç§åä¸ºâ€œZEROâ€çš„é›¶æ ·æœ¬å¤šæç¤ºç›®æ ‡æ£€æµ‹æ¨¡å‹ã€‚è¯¥æ¨¡å‹èƒ½å¤Ÿè·¨å¤šä¸ªå·¥ä¸šé¢†åŸŸè¿›è¡Œç¨³å¥ã€ç”Ÿäº§å°±ç»ªéƒ¨ç½²ï¼Œå¹¶å…·æœ‰å‡ºè‰²çš„çµæ´»æ€§å’Œæ•ˆç‡ã€‚å®ƒç»“åˆäº†ç›´æ¥å›¾åƒè¾“å…¥å’Œç”¨æˆ·å®šä¹‰çš„å¤šä¸ªæç¤ºï¼ˆåŒ…æ‹¬æ–‡æœ¬å’Œè§†è§‰çº¿ç´¢ï¼‰ï¼Œé€šè¿‡ä¸“ç”¨ç¼–ç å™¨è¿›è¡Œå¤„ç†ä»¥ç”Ÿæˆå‡†ç¡®çš„æ£€æµ‹ç»“æœã€‚ä¼˜åŒ–çš„æ¨¡å‹æ¶æ„ä½¿å¾—å®ƒèƒ½å¤Ÿé€‚åº”å¤§è§„æ¨¡çš„æ•°æ®åº“å¹¶å…·æœ‰å¾ˆé«˜çš„æ‰©å±•æ€§ï¼Œå¹¶ä½¿ç”¨è¶…è¿‡ä¸€äº¿å¼ å›¾ç‰‡çš„é¢†åŸŸç‰¹å®šå›¾åƒæ•°æ®åº“è¿›è¡Œè®­ç»ƒã€‚æ­¤å¤–ï¼Œåœ¨CVPR 2025 Few-Shot Object DetectionæŒ‘æˆ˜ä¸­å¼•å…¥äº†ä¸€ç§ç‰¹å®šé¢†åŸŸçš„å¾®è°ƒç­–ç•¥ï¼Œå®ƒé€šè¿‡å¼ºè°ƒæç¤ºå¤šæ ·æ€§å’Œä¿å®ˆä¼ªæ ‡ç­¾æŠ€æœ¯å®ç°äº†å¯¹æ— æ ‡ç­¾ç¯å¢ƒçš„æœ‰æ•ˆé€‚åº”ã€‚è¯¥ç ”ç©¶çªæ˜¾äº†æç¤ºé©±åŠ¨å‹æ•°æ®ä¸­å¿ƒäººå·¥æ™ºèƒ½åœ¨åŠ¨æ€å·¥ä¸šç¯å¢ƒä¸­å®ç°å¯æ‰©å±•å’Œè‡ªé€‚åº”ç›®æ ‡æ£€æµ‹çš„æ½œåŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>â€œZEROâ€æ¨¡å‹æ˜¯ä¸€ä¸ªç”¨äºå¹¿æ³›ä¸‹æ¸¸ä»»åŠ¡çš„é›¶æ ·æœ¬å¤šæç¤ºç›®æ ‡æ£€æµ‹æ¨¡å‹ã€‚</li>
<li>è¯¥æ¨¡å‹ç»“åˆäº†ç›´æ¥å›¾åƒè¾“å…¥å’Œç”¨æˆ·å®šä¹‰çš„æç¤ºï¼ˆåŒ…æ‹¬æ–‡æœ¬å’Œè§†è§‰çº¿ç´¢ï¼‰ã€‚</li>
<li>æ¨¡å‹æ¶æ„ç»è¿‡ä¼˜åŒ–ï¼Œå…·æœ‰å¯æ‰©å±•æ€§ï¼Œé€‚ç”¨äºå¤§è§„æ¨¡æ•°æ®åº“å’Œå¤§è§„æ¨¡è®­ç»ƒã€‚</li>
<li>ä½¿ç”¨è¶…è¿‡ä¸€äº¿å¼ å›¾ç‰‡çš„ç‰¹å®šé¢†åŸŸå›¾åƒæ•°æ®åº“è¿›è¡Œè®­ç»ƒã€‚</li>
<li>ç‰¹å®šé¢†åŸŸçš„å¾®è°ƒç­–ç•¥ç»“åˆäº†æç¤ºå¤šæ ·æ€§å’Œä¿å®ˆä¼ªæ ‡ç­¾æŠ€æœ¯ä»¥é€‚åº”æ— æ ‡ç­¾ç¯å¢ƒã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.04270">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-2c0e51f2d301df444ffa2633d7ac0bae.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6395ccf3b3bd1bc59cf9455f93e067c0.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-5a5e3e5a4bd8043bee70bc3af327f579.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e5b80702171f9e6f308dde125c599c8e.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-c59c09616ab3904d6de80d517c438aa3.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Context-Tuning-for-In-Context-Optimization"><a href="#Context-Tuning-for-In-Context-Optimization" class="headerlink" title="Context Tuning for In-Context Optimization"></a>Context Tuning for In-Context Optimization</h2><p><strong>Authors:Jack Lu, Ryan Teehan, Zhenbang Yang, Mengye Ren</strong></p>
<p>We introduce Context Tuning, a simple and effective method to significantly enhance few-shot adaptation of language models (LLMs) without fine-tuning model parameters. While prompt-based adaptation techniques have demonstrated the effectiveness of lightweight adaptation methods for large language models (LLMs), they typically initialize a trainable prompt or prefix with irrelevant tokens for the task at hand. In contrast, Context Tuning initializes the trainable prompt or prefix with task-specific demonstration examples, leveraging the modelâ€™s inherent In-Context Learning (ICL) ability to extract relevant information for improved few-shot learning performance. Extensive evaluations on benchmarks such as CrossFit, UnifiedQA, MMLU, BIG-Bench Hard, and ARC demonstrate that Context Tuning outperforms traditional prompt-based adaptation methods and achieves competitive accuracy to Test-Time Training with significantly higher training efficiency. </p>
<blockquote>
<p>æˆ‘ä»¬ä»‹ç»äº†Context Tuningï¼Œè¿™æ˜¯ä¸€ç§ç®€å•æœ‰æ•ˆçš„æ–¹æ³•ï¼Œå¯ä»¥åœ¨ä¸å¾®è°ƒæ¨¡å‹å‚æ•°çš„æƒ…å†µä¸‹æ˜¾è‘—æé«˜è¯­è¨€æ¨¡å‹çš„å°‘é‡é€‚åº”ï¼ˆLLMsï¼‰ã€‚è™½ç„¶åŸºäºæç¤ºçš„é€‚åº”æŠ€æœ¯å·²ç»è¯æ˜äº†ä¸ºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰è¿›è¡Œè½»é‡åŒ–é€‚åº”æ–¹æ³•çš„æœ‰æ•ˆæ€§ï¼Œä½†å®ƒä»¬é€šå¸¸ä½¿ç”¨ä¸æ‰‹å¤´ä»»åŠ¡ä¸ç›¸å…³çš„ä»¤ç‰Œæ¥åˆå§‹åŒ–å¯è®­ç»ƒçš„æç¤ºæˆ–å‰ç¼€ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼ŒContext Tuningä½¿ç”¨ä»»åŠ¡ç‰¹å®šçš„æ¼”ç¤ºç¤ºä¾‹æ¥åˆå§‹åŒ–å¯è®­ç»ƒçš„æç¤ºæˆ–å‰ç¼€ï¼Œåˆ©ç”¨æ¨¡å‹å›ºæœ‰çš„ä¸Šä¸‹æ–‡å­¦ä¹ ï¼ˆICLï¼‰èƒ½åŠ›æ¥æå–ç›¸å…³ä¿¡æ¯ï¼Œä»¥æé«˜å°‘é‡å­¦ä¹ çš„æ€§èƒ½ã€‚åœ¨CrossFitã€UnifiedQAã€MMLUã€BIG-Bench Hardå’ŒARCç­‰åŸºå‡†æµ‹è¯•ä¸Šçš„å¹¿æ³›è¯„ä¼°è¡¨æ˜ï¼ŒContext Tuningä¼˜äºä¼ ç»Ÿçš„åŸºäºæç¤ºçš„é€‚åº”æ–¹æ³•ï¼Œå…¶æµ‹è¯•ç²¾åº¦ä¸æµ‹è¯•æ—¶çš„è®­ç»ƒç›¸å½“ï¼Œä½†è®­ç»ƒæ•ˆç‡æ›´é«˜ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.04221v1">PDF</a> A short version of this paper has been accepted for publication in   the Workshop on Test-Time Adaptation (PUT) at the International Conference on   Machine Learning (ICML) 2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†Context Tuningï¼Œè¿™æ˜¯ä¸€ç§ç®€å•æœ‰æ•ˆçš„æ–¹æ³•ï¼Œå¯åœ¨ä¸å¾®è°ƒè¯­è¨€æ¨¡å‹å‚æ•°çš„æƒ…å†µä¸‹æ˜¾è‘—æé«˜è¯­è¨€æ¨¡å‹çš„å°‘é‡é€‚åº”æ€§ã€‚ç›¸è¾ƒäºä¼ ç»ŸåŸºäºæç¤ºçš„é€‚åº”æ–¹æ³•ä½¿ç”¨æ— å…³ä»»åŠ¡çš„æ ‡è®°æ¥åˆå§‹åŒ–å¯è®­ç»ƒæç¤ºæˆ–å‰ç¼€ï¼ŒContext Tuningé€šè¿‡ä»¥ä»»åŠ¡ç‰¹å®šæ¼”ç¤ºç¤ºä¾‹æ¥åˆå§‹åŒ–å¯è®­ç»ƒæç¤ºæˆ–å‰ç¼€ï¼Œå¹¶åˆ©ç”¨æ¨¡å‹çš„å›ºæœ‰ä¸Šä¸‹æ–‡å­¦ä¹ èƒ½åŠ›æå–ç›¸å…³ä¿¡æ¯ï¼Œä»¥æé«˜å°‘æ ·æœ¬å­¦ä¹ çš„æ€§èƒ½ã€‚åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸Šçš„è¯„ä¼°ç»“æœè¡¨æ˜ï¼ŒContext Tuningä¼˜äºä¼ ç»ŸåŸºäºæç¤ºçš„é€‚åº”æ–¹æ³•ï¼Œå¹¶å®ç°äº†ä¸æµ‹è¯•æ—¶é—´è®­ç»ƒç›¸å½“çš„ç²¾åº¦ï¼ŒåŒæ—¶æ˜¾è‘—æé«˜è®­ç»ƒæ•ˆç‡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Context Tuningæ˜¯ä¸€ç§å¢å¼ºè¯­è¨€æ¨¡å‹å°‘é‡é€‚åº”æ€§çš„æ–¹æ³•ï¼Œæ— éœ€å¾®è°ƒæ¨¡å‹å‚æ•°ã€‚</li>
<li>ä¼ ç»ŸåŸºäºæç¤ºçš„é€‚åº”æ–¹æ³•ä½¿ç”¨æ— å…³ä»»åŠ¡çš„æ ‡è®°åˆå§‹åŒ–å¯è®­ç»ƒæç¤ºæˆ–å‰ç¼€ï¼Œè€ŒContext Tuningåˆ©ç”¨ä»»åŠ¡ç‰¹å®šæ¼”ç¤ºç¤ºä¾‹è¿›è¡Œåˆå§‹åŒ–ã€‚</li>
<li>Context Tuningåˆ©ç”¨è¯­è¨€æ¨¡å‹çš„å›ºæœ‰ä¸Šä¸‹æ–‡å­¦ä¹ èƒ½åŠ›æå–ç›¸å…³ä¿¡æ¯ï¼Œæé«˜å°‘æ ·æœ¬å­¦ä¹ æ€§èƒ½ã€‚</li>
<li>Context Tuningåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸Šçš„è¡¨ç°ä¼˜äºä¼ ç»ŸåŸºäºæç¤ºçš„é€‚åº”æ–¹æ³•ã€‚</li>
<li>Context Tuningå®ç°äº†ä¸æµ‹è¯•æ—¶é—´è®­ç»ƒç›¸å½“çš„ç²¾åº¦ã€‚</li>
<li>Context Tuningæ˜¾è‘—æé«˜è®­ç»ƒæ•ˆç‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.04221">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-529b3dfb19b7db481c507eecc02e71ce.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f2bed9fbcdc3b71d241dbd075c29a6f2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e65e8fa6a820d04b9c6d59860f4f6eba.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Efficient-Detection-of-Intermittent-Job-Failures-Using-Few-Shot-Learning"><a href="#Efficient-Detection-of-Intermittent-Job-Failures-Using-Few-Shot-Learning" class="headerlink" title="Efficient Detection of Intermittent Job Failures Using Few-Shot Learning"></a>Efficient Detection of Intermittent Job Failures Using Few-Shot Learning</h2><p><strong>Authors:Henri AÃ¯dasso, Francis Bordeleau, Ali Tizghadam</strong></p>
<p>One of the main challenges developers face in the use of continuous integration (CI) and deployment pipelines is the occurrence of intermittent job failures, which result from unexpected non-deterministic issues (e.g., flaky tests or infrastructure problems) rather than regular code-related errors such as bugs. Prior studies developed machine-learning (ML) models trained on large datasets of job logs to classify job failures as either intermittent or regular. As an alternative to costly manual labeling of large datasets, the state-of-the-art (SOTA) approach leveraged a heuristic based on non-deterministic job reruns. However, this method mislabels intermittent job failures as regular in contexts where rerunning suspicious job failures is not an explicit policy, and therefore limits the SOTAâ€™s performance in practice. In fact, our manual analysis of 2,125 job failures from 5 industrial and 1 open-source projects reveals that, on average, 32% of intermittent job failures are mislabeled as regular. To address these limitations, this paper introduces a novel approach to intermittent job failure detection using few-shot learning (FSL). Specifically, we fine-tune a small language model using a few number of manually labeled log examples to generate rich embeddings, which are then used to train an ML classifier. Our FSL-based approach achieves 70-88% F1-score with only 12 shots in all projects, outperforming the SOTA, which proved ineffective (34-52% F1-score) in 4 projects. Overall, this study underlines the importance of data quality over quantity and provides a more efficient and practical framework for the detection of intermittent job failures in organizations. </p>
<blockquote>
<p>å¼€å‘è€…åœ¨ä½¿ç”¨æŒç»­é›†æˆï¼ˆCIï¼‰å’Œéƒ¨ç½²ç®¡é“æ—¶é¢ä¸´çš„ä¸»è¦æŒ‘æˆ˜ä¹‹ä¸€æ˜¯é—´æ­‡æ€§ä½œä¸šå¤±è´¥çš„å‡ºç°ã€‚è¿™äº›å¤±è´¥æ˜¯ç”±äºæ„å¤–çš„éç¡®å®šæ€§é—®é¢˜ï¼ˆä¾‹å¦‚ï¼Œæµ‹è¯•ä¸ç¨³å®šæˆ–åŸºç¡€è®¾æ–½é—®é¢˜ï¼‰ï¼Œè€Œä¸æ˜¯å¸¸è§„çš„ä»£ç ç›¸å…³é”™è¯¯ï¼ˆå¦‚é”™è¯¯ï¼‰ã€‚å…ˆå‰çš„ç ”ç©¶å¼€å‘äº†ç»è¿‡å¤§å‹ä½œä¸šæ—¥å¿—æ•°æ®é›†è®­ç»ƒçš„æœºå™¨å­¦ä¹ ï¼ˆMLï¼‰æ¨¡å‹ï¼Œä»¥å°†ä½œä¸šå¤±è´¥åˆ†ç±»ä¸ºé—´æ­‡æ€§æˆ–å¸¸è§„æ€§ã€‚ä½œä¸ºæ˜‚è´µçš„æ‰‹åŠ¨æ ‡è®°å¤§å‹æ•°æ®é›†çš„æ›¿ä»£æ–¹æ¡ˆï¼Œæœ€æ–°æ–¹æ³•åˆ©ç”¨åŸºäºéç¡®å®šæ€§ä½œä¸šé‡è·‘çš„å¯å‘å¼æ–¹æ³•ã€‚ç„¶è€Œï¼Œåœ¨é‡è¯•å¯ç–‘ä½œä¸šå¤±è´¥ä¸æ˜¯æ˜ç¡®ç­–ç•¥çš„æƒ…å†µä¸‹ï¼Œæ­¤æ–¹æ³•ä¼šå°†é—´æ­‡æ€§ä½œä¸šå¤±è´¥è¯¯æ ‡è®°ä¸ºå¸¸è§„æ€§ï¼Œä»è€Œé™åˆ¶äº†å…¶åœ¨å®é™…åº”ç”¨ä¸­çš„æ€§èƒ½ã€‚å®é™…ä¸Šï¼Œæˆ‘ä»¬å¯¹æ¥è‡ªäº”ä¸ªå·¥ä¸šé¡¹ç›®å’Œä¸¤ä¸ªå¼€æºé¡¹ç›®çš„2,125ä¸ªä½œä¸šå¤±è´¥çš„æ‰‹åŠ¨åˆ†æè¡¨æ˜ï¼Œå¹³å‡è€Œè¨€ï¼Œæœ‰32ï¼…çš„é—´æ­‡æ€§ä½œä¸šå¤±è´¥è¢«è¯¯æ ‡è®°ä¸ºå¸¸è§„æ€§ã€‚ä¸ºäº†è§£å†³è¿™äº›å±€é™æ€§ï¼Œæœ¬æ–‡å¼•å…¥äº†ä¸€ç§ä½¿ç”¨å°‘é‡å­¦ä¹ ï¼ˆFSLï¼‰è¿›è¡Œé—´æ­‡æ€§ä½œä¸šæ•…éšœæ£€æµ‹çš„æ–°æ–¹æ³•ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬ä½¿ç”¨å°‘é‡æ‰‹åŠ¨æ ‡è®°çš„æ—¥å¿—ç¤ºä¾‹å¯¹å°å‹è¯­è¨€æ¨¡å‹è¿›è¡Œå¾®è°ƒï¼Œä»¥ç”Ÿæˆä¸°å¯Œçš„åµŒå…¥ï¼Œç„¶åç”¨äºè®­ç»ƒMLåˆ†ç±»å™¨ã€‚åŸºäºFSLçš„æ–¹æ³•åœ¨æ‰€æœ‰é¡¹ç›®ä¸­ä»…ä½¿ç”¨12ä¸ªæ ·æœ¬å°±è¾¾åˆ°äº†70-88ï¼…çš„F1åˆ†æ•°ï¼Œä¼˜äºçŠ¶æ€æœ€æ–°æŠ€æœ¯ï¼ˆåœ¨å››ä¸ªé¡¹ç›®ä¸­è¡¨ç°ä¸ä½³ï¼ŒF1åˆ†æ•°ä¸º34-52ï¼…ï¼‰ã€‚æ€»ä½“è€Œè¨€ï¼Œè¿™é¡¹ç ”ç©¶å¼ºè°ƒäº†æ•°æ®è´¨é‡æ¯”æ•°é‡æ›´é‡è¦ï¼Œå¹¶æä¾›äº†ä¸€ç§æ›´é«˜æ•ˆä¸”å®ç”¨çš„æ¡†æ¶æ¥æ£€æµ‹ç»„ç»‡ä¸­çš„é—´æ­‡æ€§ä½œä¸šå¤±è´¥ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.04173v1">PDF</a> Accepted at the 41st International Conference on Software Maintenance   and Evolution - ICSME 2025, Industry Track</p>
<p><strong>Summary</strong><br>     å¼€å‘è€…åœ¨ä½¿ç”¨æŒç»­é›†æˆï¼ˆCIï¼‰å’Œéƒ¨ç½²ç®¡é“æ—¶é¢ä¸´çš„æŒ‘æˆ˜ä¹‹ä¸€æ˜¯é—´æ­‡æ€§ä½œä¸šå¤±è´¥ã€‚å…ˆå‰çš„ç ”ç©¶é€šè¿‡è®­ç»ƒå¤§å‹ä½œä¸šæ—¥å¿—æ•°æ®é›†å¼€å‘æœºå™¨å­¦ä¹ æ¨¡å‹æ¥å¯¹ä½œä¸šå¤±è´¥è¿›è¡Œåˆ†ç±»ï¼ŒåŒ…æ‹¬é—´æ­‡æ€§å¤±è´¥å’Œå¸¸è§„å¤±è´¥ã€‚æœ€æ–°æ–¹æ³•åŸºäºéç¡®å®šæ€§ä½œä¸šé‡è¿è¡Œå¯å‘å¼æ–¹æ³•ï¼Œä½†å­˜åœ¨è¯¯åˆ¤é—´æ­‡æ€§ä½œä¸šå¤±è´¥ä¸ºå¸¸è§„å¤±è´¥çš„ç¼ºé™·ã€‚æœ¬ç ”ç©¶é€šè¿‡å¯¹æ‰‹åŠ¨åˆ†æå‘ç°ï¼Œå¹³å‡æœ‰32%çš„é—´æ­‡æ€§ä½œä¸šå¤±è´¥è¢«è¯¯åˆ¤ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œæœ¬ç ”ç©¶æå‡ºä½¿ç”¨åŸºäºå°æ ·æœ¬å­¦ä¹ çš„é—´æ­‡æ€§ä½œä¸šå¤±è´¥æ£€æµ‹æ–°æ–¹æ³•ï¼Œé€šè¿‡å¾®è°ƒå°å‹è¯­è¨€æ¨¡å‹ç”Ÿæˆä¸°å¯Œçš„åµŒå…¥ï¼Œç”¨äºè®­ç»ƒæœºå™¨å­¦ä¹ åˆ†ç±»å™¨ã€‚æ–°æ–¹æ³•çš„F1åˆ†æ•°åœ¨70-88%ä¹‹é—´ï¼Œä»…åœ¨æ‰€æœ‰é¡¹ç›®ä¸­ä½¿ç”¨12ä¸ªæ ·æœ¬å³å¯å®ç°ï¼Œä¼˜äºå…ˆå‰æ–¹æ³•çš„34-52%ã€‚æ€»ä½“è€Œè¨€ï¼Œæœ¬ç ”ç©¶å¼ºè°ƒæ•°æ®è´¨é‡æ¯”æ•°é‡æ›´é‡è¦ï¼Œå¹¶ä¸ºç»„ç»‡æ£€æµ‹é—´æ­‡æ€§ä½œä¸šå¤±è´¥æä¾›äº†æ›´å®ç”¨ã€é«˜æ•ˆçš„æ¡†æ¶ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¼€å‘äººå‘˜åœ¨CIå’Œéƒ¨ç½²ç®¡é“ä¸­é¢ä¸´é—´æ­‡æ€§ä½œä¸šå¤±è´¥çš„æŒ‘æˆ˜ã€‚</li>
<li>ä»¥å¾€çš„æœºå™¨å­¦ä¹ æ¨¡å‹é€šè¿‡å¤§å‹ä½œä¸šæ—¥å¿—æ•°æ®é›†å¯¹ä½œä¸šå¤±è´¥è¿›è¡Œåˆ†ç±»ï¼Œå­˜åœ¨è¯¯åˆ¤é—®é¢˜ã€‚</li>
<li>æœ€æ–°æ–¹æ³•åŸºäºéç¡®å®šæ€§ä½œä¸šé‡è¿è¡Œå¯å‘å¼ï¼Œä½†åœ¨æŸäº›æƒ…å†µä¸‹ä¼šè¯¯åˆ¤é—´æ­‡æ€§ä½œä¸šå¤±è´¥ä¸ºå¸¸è§„å¤±è´¥ã€‚</li>
<li>æ‰‹åŠ¨åˆ†ææ˜¾ç¤ºï¼Œå¹³å‡æœ‰32%çš„é—´æ­‡æ€§ä½œä¸šå¤±è´¥è¢«è¯¯åˆ¤ã€‚</li>
<li>ç ”ç©¶æå‡ºä½¿ç”¨åŸºäºå°æ ·æœ¬å­¦ä¹ ï¼ˆFSLï¼‰çš„æ–°æ–¹æ³•ï¼Œé€šè¿‡å¾®è°ƒå°å‹è¯­è¨€æ¨¡å‹æ¥ç”Ÿæˆä¸°å¯Œçš„åµŒå…¥ï¼Œç”¨äºè®­ç»ƒåˆ†ç±»å™¨ã€‚</li>
<li>æ–°æ–¹æ³•åœ¨æ‰€æœ‰é¡¹ç›®ä¸­çš„F1åˆ†æ•°è¾¾åˆ°70-88%ï¼Œä»…ä½¿ç”¨12ä¸ªæ ·æœ¬å³å¯å®ç°ï¼Œä¼˜äºå…ˆå‰æ–¹æ³•ã€‚</li>
<li>ç ”ç©¶å¼ºè°ƒæ•°æ®è´¨é‡çš„é‡è¦æ€§ï¼Œä¸ºç»„ç»‡æä¾›æ›´å®ç”¨ã€é«˜æ•ˆçš„é—´æ­‡æ€§ä½œä¸šå¤±è´¥æ£€æµ‹æ¡†æ¶ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.04173">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-b2941b919cbd2900ea15d1efac9e5665.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-b6eca8ddb4e4248beb561748cad07a95.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-3c2f2e5e2a41bb812e8e0151c2521329.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-0e15d541c83e97a180a769e1cff37892.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-ef2531920378b6d944cc487092f16698.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Demystifying-ChatGPT-How-It-Masters-Genre-Recognition"><a href="#Demystifying-ChatGPT-How-It-Masters-Genre-Recognition" class="headerlink" title="Demystifying ChatGPT: How It Masters Genre Recognition"></a>Demystifying ChatGPT: How It Masters Genre Recognition</h2><p><strong>Authors:Subham Raj, Sriparna Saha, Brijraj Singh, Niranjan Pedanekar</strong></p>
<p>The introduction of ChatGPT has garnered significant attention within the NLP community and beyond. Previous studies have demonstrated ChatGPTâ€™s substantial advancements across various downstream NLP tasks, highlighting its adaptability and potential to revolutionize language-related applications. However, its capabilities and limitations in genre prediction remain unclear. This work analyzes three Large Language Models (LLMs) using the MovieLens-100K dataset to assess their genre prediction capabilities. Our findings show that ChatGPT, without fine-tuning, outperformed other LLMs, and fine-tuned ChatGPT performed best overall. We set up zero-shot and few-shot prompts using audio transcripts&#x2F;subtitles from movie trailers in the MovieLens-100K dataset, covering 1682 movies of 18 genres, where each movie can have multiple genres. Additionally, we extended our study by extracting IMDb movie posters to utilize a Vision Language Model (VLM) with prompts for poster information. This fine-grained information was used to enhance existing LLM prompts. In conclusion, our study reveals ChatGPTâ€™s remarkable genre prediction capabilities, surpassing other language models. The integration of VLM further enhances our findings, showcasing ChatGPTâ€™s potential for content-related applications by incorporating visual information from movie posters. </p>
<blockquote>
<p>ChatGPTçš„å¼•å…¥ä¸ä»…åœ¨NLPé¢†åŸŸå¼•èµ·äº†å¹¿æ³›å…³æ³¨ï¼Œè€Œä¸”è¶…å‡ºäº†è¯¥é¢†åŸŸã€‚ä¹‹å‰çš„ç ”ç©¶å·²ç»è¯æ˜äº†ChatGPTåœ¨å„ç§ä¸‹æ¸¸NLPä»»åŠ¡ä¸Šçš„å·¨å¤§è¿›æ­¥ï¼Œçªå‡ºäº†å…¶é€‚åº”æ€§å’Œæ”¹å˜è¯­è¨€ç›¸å…³åº”ç”¨çš„å¯èƒ½æ€§ã€‚ç„¶è€Œï¼Œå…¶åœ¨ä½“è£é¢„æµ‹æ–¹é¢çš„èƒ½åŠ›å’Œå±€é™æ€§å°šä¸æ¸…æ¥šã€‚è¿™é¡¹å·¥ä½œä½¿ç”¨MovieLens-100Kæ•°æ®é›†åˆ†æäº†ä¸‰ç§å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ï¼Œä»¥è¯„ä¼°å®ƒä»¬çš„ä½“è£é¢„æµ‹èƒ½åŠ›ã€‚æˆ‘ä»¬çš„ç ”ç©¶å‘ç°ï¼Œæœªç»å¾®è°ƒçš„ChatGPTåœ¨å…¶ä»–LLMä¸­è¡¨ç°æœ€ä½³ï¼Œè€Œç»è¿‡å¾®è°ƒçš„ChatGPTæ€»ä½“è¡¨ç°æœ€ä½³ã€‚æˆ‘ä»¬ä½¿ç”¨MovieLens-100Kæ•°æ®é›†ä¸­çš„ç”µå½±é¢„å‘ŠéŸ³é¢‘è½¬å½•&#x2F;å­—å¹•æ¥è®¾ç½®é›¶æ ·æœ¬å’Œå°‘æ ·æœ¬æç¤ºï¼Œæ¶µç›–äº†18ä¸ªä½“è£çš„1682éƒ¨ç”µå½±ï¼Œæ¯éƒ¨ç”µå½±å¯èƒ½åŒ…å«å¤šä¸ªä½“è£ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜é€šè¿‡æå–IMDbç”µå½±æµ·æŠ¥å¹¶åˆ©ç”¨å¸¦æœ‰æµ·æŠ¥ä¿¡æ¯æç¤ºçš„è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰æ¥æ‰©å±•æˆ‘ä»¬çš„ç ”ç©¶ã€‚è¿™ç§ç²¾ç»†çš„ä¿¡æ¯è¢«ç”¨æ¥å¢å¼ºç°æœ‰çš„LLMæç¤ºã€‚æ€»ä¹‹ï¼Œæˆ‘ä»¬çš„ç ”ç©¶è¡¨æ˜ï¼ŒChatGPTåœ¨ä½“è£é¢„æµ‹æ–¹é¢è¡¨ç°å‡ºå“è¶Šçš„èƒ½åŠ›ï¼Œè¶…è¶Šäº†å…¶ä»–è¯­è¨€æ¨¡å‹ã€‚è§†è§‰è¯­è¨€æ¨¡å‹çš„æ•´åˆè¿›ä¸€æ­¥å¢å¼ºäº†æˆ‘ä»¬çš„å‘ç°ï¼Œå±•ç¤ºäº†ChatGPTé€šè¿‡èå…¥ç”µå½±æµ·æŠ¥çš„è§†è§‰ä¿¡æ¯åœ¨å†…å®¹ç›¸å…³åº”ç”¨ä¸­çš„æ½œåŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.03875v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ç ”ç©¶äº†ChatGPTåœ¨MovieLens-100Kæ•°æ®é›†ä¸Šçš„ç”µå½±ç±»å‹é¢„æµ‹èƒ½åŠ›ï¼Œå¹¶ä¸å…¶ä»–å¤§å‹è¯­è¨€æ¨¡å‹è¿›è¡Œå¯¹æ¯”ã€‚å®éªŒè¡¨æ˜ï¼Œæœªç»å¾®è°ƒChatGPTçš„é¢„æµ‹æ€§èƒ½ä¼˜äºå…¶ä»–å¤§å‹è¯­è¨€æ¨¡å‹ï¼Œç»è¿‡å¾®è°ƒåè¡¨ç°æœ€ä½³ã€‚ç ”ç©¶è¿˜ç»“åˆäº†è§†è§‰è¯­è¨€æ¨¡å‹ï¼Œåˆ©ç”¨ç”µå½±æµ·æŠ¥ä¿¡æ¯å¢å¼ºå¤§å‹è¯­è¨€æ¨¡å‹çš„æç¤ºï¼Œè¿›ä¸€æ­¥æé«˜äº†ChatGPTçš„ç±»å‹é¢„æµ‹èƒ½åŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ChatGPTåœ¨æœªç»å¾®è°ƒçš„æƒ…å†µä¸‹ï¼Œåœ¨ç”µå½±ç±»å‹é¢„æµ‹æ–¹é¢è¡¨ç°å‡ºæ˜¾è‘—çš„ä¼˜åŠ¿ã€‚</li>
<li>åœ¨ä½¿ç”¨MovieLens-100Kæ•°æ®é›†è¿›è¡Œçš„å®éªŒä¸­ï¼ŒChatGPTçš„è¡¨ç°ä¼˜äºå…¶ä»–å¤§å‹è¯­è¨€æ¨¡å‹ã€‚</li>
<li>é€šè¿‡å¾®è°ƒï¼ŒChatGPTçš„æ€§èƒ½å¾—åˆ°è¿›ä¸€æ­¥æå‡ï¼Œè¡¨ç°æœ€ä½³ã€‚</li>
<li>ç ”ç©¶ç»“åˆäº†è§†è§‰è¯­è¨€æ¨¡å‹ï¼Œåˆ©ç”¨ç”µå½±æµ·æŠ¥ä¿¡æ¯ï¼Œæé«˜äº†å¤§å‹è¯­è¨€æ¨¡å‹çš„æ€§èƒ½ã€‚</li>
<li>ChatGPTåœ¨ç»“åˆè§†è§‰ä¿¡æ¯åï¼Œç±»å‹é¢„æµ‹èƒ½åŠ›å¾—åˆ°è¿›ä¸€æ­¥å¢å¼ºã€‚</li>
<li>è¯¥ç ”ç©¶å±•ç¤ºäº†ChatGPTåœ¨å†…å®¹ç›¸å…³åº”ç”¨ä¸­çš„æ½œåŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.03875">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-27efcc82d2cee0f9cc535ba3b62d00be.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-74692b12176bdd70d576cb3c11d87547.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7b659fad49ae07a8d8a35353690972e3.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-22b7bf1cc259b6f56f4df83301e12a2c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c6ebb50762313d6b987770b9a758ca70.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f31ac2993af3defcc99b03fdffa3e560.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="MolVision-Molecular-Property-Prediction-with-Vision-Language-Models"><a href="#MolVision-Molecular-Property-Prediction-with-Vision-Language-Models" class="headerlink" title="MolVision: Molecular Property Prediction with Vision Language Models"></a>MolVision: Molecular Property Prediction with Vision Language Models</h2><p><strong>Authors:Deepan Adak, Yogesh Singh Rawat, Shruti Vyas</strong></p>
<p>Molecular property prediction is a fundamental task in computational chemistry with critical applications in drug discovery and materials science. While recent works have explored Large Language Models (LLMs) for this task, they primarily rely on textual molecular representations such as SMILES&#x2F;SELFIES, which can be ambiguous and structurally less informative. In this work, we introduce MolVision, a novel approach that leverages Vision-Language Models (VLMs) by integrating both molecular structure as images and textual descriptions to enhance property prediction. We construct a benchmark spanning ten diverse datasets, covering classification, regression and description tasks. Evaluating nine different VLMs in zero-shot, few-shot, and fine-tuned settings, we find that visual information improves prediction performance, particularly when combined with efficient fine-tuning strategies such as LoRA. Our results reveal that while visual information alone is insufficient, multimodal fusion significantly enhances generalization across molecular properties. Adaptation of vision encoder for molecular images in conjunction with LoRA further improves the performance. The code and data is available at : $\href{<a target="_blank" rel="noopener" href="https://molvision.github.io/MolVision/%7D%7Bhttps://molvision.github.io/MolVision/%7D$">https://molvision.github.io/MolVision/}{https://molvision.github.io/MolVision/}$</a>. </p>
<blockquote>
<p>åˆ†å­å±æ€§é¢„æµ‹æ˜¯è®¡ç®—åŒ–å­¦ä¸­çš„ä¸€é¡¹åŸºæœ¬ä»»åŠ¡ï¼Œåœ¨è¯ç‰©å‘ç°å’Œææ–™ç§‘å­¦ä¸­æœ‰ç€å…³é”®çš„åº”ç”¨ã€‚è™½ç„¶è¿‘æœŸçš„ç ”ç©¶å·²ç»æ¢ç´¢äº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨æ­¤ä»»åŠ¡ä¸­çš„åº”ç”¨ï¼Œä½†å®ƒä»¬ä¸»è¦ä¾èµ–äºSMILES&#x2F;SELFIESç­‰æ–‡æœ¬åˆ†å­è¡¨ç¤ºæ³•ï¼Œè¿™äº›è¡¨ç¤ºæ³•å¯èƒ½å…·æœ‰æ­§ä¹‰å¹¶ä¸”ç»“æ„ä¿¡æ¯è¾ƒå°‘ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†MolVisionï¼Œè¿™æ˜¯ä¸€ç§æ–°æ–¹æ³•ï¼Œå®ƒé€šè¿‡æ•´åˆåˆ†å­ç»“æ„å›¾åƒå’Œæ–‡æœ¬æè¿°æ¥åˆ©ç”¨è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰ï¼Œä»¥å¢å¼ºå±æ€§é¢„æµ‹ã€‚æˆ‘ä»¬æ„å»ºäº†ä¸€ä¸ªè·¨è¶Šåä¸ªä¸åŒæ•°æ®é›†çš„åŸºå‡†æµ‹è¯•ï¼Œæ¶µç›–åˆ†ç±»ã€å›å½’å’Œæè¿°ä»»åŠ¡ã€‚åœ¨é›¶æ ·æœ¬ã€å°‘æ ·æœ¬å’Œå¾®è°ƒè®¾ç½®ä¸‹è¯„ä¼°äº†ä¹ç§ä¸åŒçš„VLMï¼Œæˆ‘ä»¬å‘ç°è§†è§‰ä¿¡æ¯æé«˜äº†é¢„æµ‹æ€§èƒ½ï¼Œç‰¹åˆ«æ˜¯å½“ä¸LoRAç­‰é«˜æ•ˆçš„å¾®è°ƒç­–ç•¥ç›¸ç»“åˆæ—¶ã€‚æˆ‘ä»¬çš„ç»“æœè¡¨æ˜ï¼Œè™½ç„¶å•ç‹¬çš„è§†è§‰ä¿¡æ¯ä¸è¶³ä»¥è¿›è¡Œé¢„æµ‹ï¼Œä½†å¤šæ¨¡å¼èåˆæ˜¾è‘—å¢å¼ºäº†è·¨åˆ†å­å±æ€§çš„æ³›åŒ–èƒ½åŠ›ã€‚ç»“åˆLoRAå¯¹åˆ†å­å›¾åƒè§†è§‰ç¼–ç å™¨çš„é€‚åº”æ€§è°ƒæ•´è¿›ä¸€æ­¥æé«˜äº†æ€§èƒ½ã€‚ç›¸å…³ä»£ç å’Œæ•°æ®é›†å¯é€šè¿‡ä»¥ä¸‹é“¾æ¥è·å–ï¼š[<a target="_blank" rel="noopener" href="https://molvision.github.io/MolVision/]">https://molvision.github.io/MolVision/]</a> ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.03283v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>MolVisionç»“åˆåˆ†å­ç»“æ„å›¾åƒå’Œæ–‡æœ¬æè¿°ï¼Œåˆ©ç”¨è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰è¿›è¡Œåˆ†å­å±æ€§é¢„æµ‹ï¼Œæé«˜äº†é¢„æµ‹æ€§èƒ½ã€‚è¯¥ç ”ç©¶æ„å»ºäº†ä¸€ä¸ªè·¨è¶Šåä¸ªä¸åŒæ•°æ®é›†çš„åŸºå‡†æµ‹è¯•ï¼Œå¹¶å‘ç°è§†è§‰ä¿¡æ¯å¯¹äºé¢„æµ‹ç‰¹åˆ«æ˜¯ä¸é«˜æ•ˆå¾®è°ƒç­–ç•¥ç»“åˆæ—¶è¡¨ç°æ›´ä½³ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>MolVisionåˆ©ç”¨è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰è¿›è¡Œåˆ†å­å±æ€§é¢„æµ‹ã€‚</li>
<li>è¯¥æ–¹æ³•ç»“åˆäº†åˆ†å­ç»“æ„å›¾åƒå’Œæ–‡æœ¬æè¿°ã€‚</li>
<li>ç ”ç©¶æ„å»ºäº†åŒ…å«åˆ†ç±»ã€å›å½’å’Œæè¿°ä»»åŠ¡çš„åŸºå‡†æµ‹è¯•æ•°æ®é›†ã€‚</li>
<li>è§†è§‰ä¿¡æ¯èƒ½æé«˜é¢„æµ‹æ€§èƒ½ï¼Œç‰¹åˆ«æ˜¯ä¸é«˜æ•ˆå¾®è°ƒç­–ç•¥å¦‚LoRAç»“åˆæ—¶ã€‚</li>
<li>å•ä¸€çš„è§†è§‰ä¿¡æ¯ä¸è¶³ä»¥è¿›è¡Œå‡†ç¡®çš„é¢„æµ‹ï¼Œä½†å¤šæ¨¡å¼èåˆèƒ½æ˜¾è‘—æé«˜è·¨åˆ†å­å±æ€§çš„æ³›åŒ–èƒ½åŠ›ã€‚</li>
<li>é€‚é…ç”¨äºåˆ†å­å›¾åƒçš„è§†è§‰ç¼–ç å™¨ä¸LoRAå¾®è°ƒç­–ç•¥å¯è¿›ä¸€æ­¥æé«˜æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.03283">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-4d410af26af505eb88542e6aff642a9c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d3e24ecbb27420ef7acaab88dd8de814.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4b34a75692840b899ae22cedd5f57674.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-943ca4b98acf33a0c71da0035b6cf33e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c642503a6f9db7a518968a715ef42014.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-352ffa5510ccee365a77537ea45baae7.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="No-time-to-train-Training-Free-Reference-Based-Instance-Segmentation"><a href="#No-time-to-train-Training-Free-Reference-Based-Instance-Segmentation" class="headerlink" title="No time to train! Training-Free Reference-Based Instance Segmentation"></a>No time to train! Training-Free Reference-Based Instance Segmentation</h2><p><strong>Authors:Miguel Espinosa, Chenhongyi Yang, Linus Ericsson, Steven McDonagh, Elliot J. Crowley</strong></p>
<p>The performance of image segmentation models has historically been constrained by the high cost of collecting large-scale annotated data. The Segment Anything Model (SAM) alleviates this original problem through a promptable, semantics-agnostic, segmentation paradigm and yet still requires manual visual-prompts or complex domain-dependent prompt-generation rules to process a new image. Towards reducing this new burden, our work investigates the task of object segmentation when provided with, alternatively, only a small set of reference images. Our key insight is to leverage strong semantic priors, as learned by foundation models, to identify corresponding regions between a reference and a target image. We find that correspondences enable automatic generation of instance-level segmentation masks for downstream tasks and instantiate our ideas via a multi-stage, training-free method incorporating (1) memory bank construction; (2) representation aggregation and (3) semantic-aware feature matching. Our experiments show significant improvements on segmentation metrics, leading to state-of-the-art performance on COCO FSOD (36.8% nAP), PASCAL VOC Few-Shot (71.2% nAP50) and outperforming existing training-free approaches on the Cross-Domain FSOD benchmark (22.4% nAP). </p>
<blockquote>
<p>å†å²ä¸Šï¼Œå›¾åƒåˆ†å‰²æ¨¡å‹çš„æ€§èƒ½ä¸€ç›´å—åˆ°æ”¶é›†å¤§è§„æ¨¡æ ‡æ³¨æ•°æ®çš„é«˜æˆæœ¬çš„é™åˆ¶ã€‚Segment Anything Modelï¼ˆSAMï¼‰é€šè¿‡ä¸€ç§å¯æç¤ºçš„ã€ä¸è¯­ä¹‰æ— å…³çš„åˆ†å‰²èŒƒå¼ç¼“è§£äº†è¿™ä¸€åŸå§‹é—®é¢˜ï¼Œä½†å¤„ç†æ–°å›¾åƒæ—¶ä»éœ€è¦æ‰‹åŠ¨è§†è§‰æç¤ºæˆ–å¤æ‚çš„åŸŸç›¸å…³æç¤ºç”Ÿæˆè§„åˆ™ã€‚ä¸ºäº†å‡è½»è¿™ä¸€æ–°è´Ÿæ‹…ï¼Œæˆ‘ä»¬çš„å·¥ä½œç ”ç©¶äº†åœ¨ä»…æä¾›ä¸€å°éƒ¨åˆ†å‚è€ƒå›¾åƒçš„æƒ…å†µä¸‹è¿›è¡Œå¯¹è±¡åˆ†å‰²çš„ä»»åŠ¡ã€‚æˆ‘ä»¬çš„å…³é”®è§è§£æ˜¯åˆ©ç”¨åŸºç¡€æ¨¡å‹å­¦åˆ°çš„å¼ºå¤§è¯­ä¹‰å…ˆéªŒçŸ¥è¯†ï¼Œæ¥è¯†åˆ«å‚è€ƒå›¾åƒå’Œç›®æ ‡å›¾åƒä¹‹é—´çš„ç›¸åº”åŒºåŸŸã€‚æˆ‘ä»¬å‘ç°è¿™ç§å¯¹åº”å…³ç³»èƒ½å¤Ÿè‡ªåŠ¨ç”Ÿæˆç”¨äºä¸‹æ¸¸ä»»åŠ¡çš„å®ä¾‹çº§åˆ†å‰²æ©è†œï¼Œå¹¶é€šè¿‡ä¸€ä¸ªå¤šé˜¶æ®µã€æ— éœ€è®­ç»ƒçš„æ–¹æ³•å®ç°æˆ‘ä»¬çš„æƒ³æ³•ï¼ŒåŒ…æ‹¬ï¼ˆ1ï¼‰æ„å»ºå†…å­˜é“¶è¡Œï¼›ï¼ˆ2ï¼‰è¡¨ç¤ºèšåˆå’Œï¼ˆ3ï¼‰è¯­ä¹‰æ„ŸçŸ¥ç‰¹å¾åŒ¹é…ã€‚æˆ‘ä»¬çš„å®éªŒæ˜¾ç¤ºï¼Œåœ¨åˆ†å‰²æŒ‡æ ‡ä¸Šå–å¾—äº†æ˜¾è‘—æ”¹è¿›ï¼Œå¹¶åœ¨COCO FSODï¼ˆ36.8% nAPï¼‰ã€PASCAL VOC Few-Shotï¼ˆ71.2% nAP50ï¼‰ä¸Šè¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œä¸”åœ¨è·¨åŸŸFSODåŸºå‡†æµ‹è¯•ä¸Šè¶…è¶Šäº†ç°æœ‰çš„æ— è®­ç»ƒæ–¹æ³•ï¼ˆ22.4% nAPï¼‰ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.02798v2">PDF</a> Preprint</p>
<p><strong>Summary</strong>ï¼š</p>
<p>æœ¬ç ”ç©¶é€šè¿‡é‡‡ç”¨å¯æç¤ºã€è¯­ä¹‰æ— å…³çš„åˆ†å‰²èŒƒå¼è§£å†³äº†å¤§è§„æ¨¡æ ‡æ³¨æ•°æ®æ”¶é›†æˆæœ¬é«˜çš„é—®é¢˜ã€‚ä½†ä¸ºäº†å¤„ç†æ–°å›¾åƒï¼Œä»éœ€æ‰‹åŠ¨è§†è§‰æç¤ºæˆ–å¤æ‚çš„ç‰¹å®šé¢†åŸŸæç¤ºç”Ÿæˆè§„åˆ™ï¼Œé€ æˆæ–°çš„è´Ÿæ‹…ã€‚æœ¬ç ”ç©¶è¿›ä¸€æ­¥æ¢ç´¢äº†ä»…æä¾›å°‘é‡å‚è€ƒå›¾åƒæ—¶çš„ç›®æ ‡åˆ†å‰²ä»»åŠ¡ï¼Œåˆ©ç”¨åŸºç¡€æ¨¡å‹å­¦ä¹ çš„å¼ºè¯­ä¹‰å…ˆéªŒçŸ¥è¯†ï¼Œåœ¨å‚è€ƒå›¾åƒå’Œç›®æ ‡å›¾åƒä¹‹é—´è¯†åˆ«å¯¹åº”åŒºåŸŸã€‚å¯¹åº”æ€§ä½¿ä¸‹æ¸¸ä»»åŠ¡èƒ½å¤Ÿè‡ªåŠ¨ç”Ÿæˆå®ä¾‹çº§åˆ«çš„åˆ†å‰²æ©è†œï¼Œå¹¶é€šè¿‡ä¸€ä¸ªåŒ…æ‹¬ï¼ˆ1ï¼‰å†…å­˜åº“æ„å»ºã€ï¼ˆ2ï¼‰è¡¨ç¤ºèšåˆå’Œï¼ˆ3ï¼‰è¯­ä¹‰æ„ŸçŸ¥ç‰¹å¾åŒ¹é…çš„ã€æ— éœ€è®­ç»ƒçš„å¤šé˜¶æ®µæ–¹æ³•å®ç°äº†æƒ³æ³•çš„å®ä¾‹åŒ–ã€‚å®éªŒç»“æœæ˜¾ç¤ºåœ¨åˆ†å‰²æŒ‡æ ‡ä¸Šæœ‰æ˜¾è‘—æ”¹å–„ï¼Œå¹¶åœ¨COCO FSODã€PASCAL VOC Few-Shotç­‰æ•°æ®é›†ä¸Šè¾¾åˆ°äº†ä¸šç•Œé¢†å…ˆæ°´å¹³ï¼Œä¸”åœ¨è·¨åŸŸFSODåŸºå‡†æµ‹è¯•ä¸­è¶…è¶Šäº†ç°æœ‰çš„æ— éœ€è®­ç»ƒçš„æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>Segment Anything Model (SAM) è§£å†³äº†æ•°æ®æ”¶é›†æˆæœ¬é«˜çš„é—®é¢˜ï¼Œä½†å¤„ç†æ–°å›¾åƒä»éœ€å¤æ‚æ“ä½œã€‚</li>
<li>ç ”ç©¶æ¢ç´¢äº†åœ¨ä»…æä¾›å°‘é‡å‚è€ƒå›¾åƒæ—¶çš„ç›®æ ‡åˆ†å‰²ä»»åŠ¡ã€‚</li>
<li>åˆ©ç”¨åŸºç¡€æ¨¡å‹å­¦ä¹ çš„å¼ºè¯­ä¹‰å…ˆéªŒçŸ¥è¯†ï¼Œè¯†åˆ«å‚è€ƒå›¾åƒå’Œç›®æ ‡å›¾åƒä¹‹é—´çš„å¯¹åº”åŒºåŸŸã€‚</li>
<li>å¯¹åº”æ€§æœ‰åŠ©äºè‡ªåŠ¨ç”Ÿæˆå®ä¾‹çº§åˆ«çš„åˆ†å‰²æ©è†œï¼Œç”¨äºä¸‹æ¸¸ä»»åŠ¡ã€‚</li>
<li>æå‡ºäº†ä¸€ç§æ— éœ€è®­ç»ƒçš„å¤šé˜¶æ®µæ–¹æ³•ï¼ŒåŒ…æ‹¬å†…å­˜åº“æ„å»ºã€è¡¨ç¤ºèšåˆå’Œè¯­ä¹‰æ„ŸçŸ¥ç‰¹å¾åŒ¹é…ã€‚</li>
<li>å®éªŒç»“æœæ˜¾è‘—æ”¹è¿›äº†åˆ†å‰²æŒ‡æ ‡ï¼Œå¹¶åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šè¾¾åˆ°é¢†å…ˆæ°´å¹³ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.02798">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-3978c1c075fc37ae0437a0e8189b5766.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-60929355fb8a4bad99c25d2e219e5af4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-34ef610aa985554711ae9ea98c2158ba.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-07b1ec1b74b110fec2dba51c7bb4fd11.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="MAGIC-Mask-Guided-Diffusion-Inpainting-with-Multi-Level-Perturbations-and-Context-Aware-Alignment-for-Few-Shot-Anomaly-Generation"><a href="#MAGIC-Mask-Guided-Diffusion-Inpainting-with-Multi-Level-Perturbations-and-Context-Aware-Alignment-for-Few-Shot-Anomaly-Generation" class="headerlink" title="MAGIC: Mask-Guided Diffusion Inpainting with Multi-Level Perturbations   and Context-Aware Alignment for Few-Shot Anomaly Generation"></a>MAGIC: Mask-Guided Diffusion Inpainting with Multi-Level Perturbations   and Context-Aware Alignment for Few-Shot Anomaly Generation</h2><p><strong>Authors:JaeHyuck Choi, MinJun Kim, JeHyeong Hong</strong></p>
<p>Few-shot anomaly generation is emerging as a practical solution for augmenting the scarce anomaly data in industrial quality control settings. An ideal generator would meet three demands at once, namely (i) keep the normal background intact, (ii) inpaint anomalous regions to tightly overlap with the corresponding anomaly masks, and (iii) generate anomalous regions in a semantically valid location, while still producing realistic, diverse appearances from only a handful of real examples. Existing diffusion-based methods usually satisfy at most two of these requirements: global anomaly generators corrupt the background, whereas mask-guided ones often falter when the mask is imprecise or misplaced. We propose MAGICâ€“Mask-guided inpainting with multi-level perturbations and Context-aware alignmentâ€“to resolve all three issues. At its core, MAGIC fine-tunes a Stable Diffusion inpainting backbone that preserves normal regions and ensures strict adherence of the synthesized anomaly to the supplied mask, directly addressing background corruption and misalignment. To offset the diversity loss that fine-tuning can cause, MAGIC adds two complementary perturbation strategies: (i) Gaussian prompt-level perturbation applied during fine-tuning and inference that broadens the global appearance of anomalies while avoiding low-fidelity textual appearances, and (ii) mask-guided spatial noise injection that enriches local texture variations. Additionally, the context-aware mask alignment module forms semantic correspondences and relocates masks so that every anomaly remains plausibly contained within the host object, eliminating out-of-boundary artifacts. Under a consistent identical evaluation protocol on the MVTec-AD dataset, MAGIC outperforms previous state-of-the-arts in downstream anomaly tasks. </p>
<blockquote>
<p>å°‘æ•°é•œå¤´å¼‚å¸¸ç”Ÿæˆï¼ˆFew-shot anomaly generationï¼‰å·²æˆä¸ºå·¥ä¸šè´¨é‡æ§åˆ¶ç¯å¢ƒä¸­æ‰©å……ç¨€ç¼ºå¼‚å¸¸æ•°æ®çš„ä¸€ç§å®ç”¨è§£å†³æ–¹æ¡ˆã€‚ç†æƒ³çš„ç”Ÿæˆå™¨éœ€è¦åŒæ—¶æ»¡è¶³ä¸‰ä¸ªè¦æ±‚ï¼Œå³ï¼ˆiï¼‰ä¿æŒæ­£å¸¸èƒŒæ™¯ä¸å˜ï¼Œï¼ˆiiï¼‰å°†å¼‚å¸¸åŒºåŸŸå¡«å……å¾—ä¸ç›¸åº”çš„å¼‚å¸¸æ©è†œç´§å¯†é‡å ï¼Œï¼ˆiiiï¼‰åœ¨è¯­ä¹‰æœ‰æ•ˆçš„ä½ç½®ç”Ÿæˆå¼‚å¸¸åŒºåŸŸï¼ŒåŒæ—¶ä»…ä»å°‘æ•°çœŸå®ç¤ºä¾‹ä¸­ç”ŸæˆçœŸå®ã€å¤šæ ·åŒ–çš„å¤–è§‚ã€‚ç°æœ‰çš„åŸºäºæ‰©æ•£çš„æ–¹æ³•é€šå¸¸æœ€å¤šæ»¡è¶³å…¶ä¸­ä¸¤ä¸ªè¦æ±‚ï¼šå…¨å±€å¼‚å¸¸ç”Ÿæˆå™¨ä¼šç ´åèƒŒæ™¯ï¼Œè€Œæ©è†œå¼•å¯¼çš„æ–¹æ³•åœ¨æ©è†œä¸ç²¾ç¡®æˆ–é”™ä½æ—¶å¸¸ä¼šå‡ºé”™ã€‚æˆ‘ä»¬æå‡ºäº†MAGICï¼ˆå¸¦æœ‰å¤šçº§æ‰°åŠ¨å’Œä¸Šä¸‹æ–‡æ„ŸçŸ¥å¯¹é½çš„æ©è†œå¼•å¯¼å¡«å……ï¼‰æ¥è§£å†³è¿™ä¸‰ä¸ªé—®é¢˜ã€‚MAGICçš„æ ¸å¿ƒæ˜¯å¾®è°ƒStable Diffusionå¡«å……ä¸»å¹²ï¼Œå®ƒä¿ç•™äº†æ­£å¸¸åŒºåŸŸå¹¶ç¡®ä¿åˆæˆçš„å¼‚å¸¸ä¸¥æ ¼éµå¾ªæä¾›çš„æ©è†œï¼Œç›´æ¥è§£å†³äº†èƒŒæ™¯ç ´åå’Œé”™ä½çš„é—®é¢˜ã€‚ä¸ºäº†æŠµæ¶ˆå¾®è°ƒå¯èƒ½å¯¼è‡´çš„å¤šæ ·æ€§æŸå¤±ï¼ŒMAGICå¢åŠ äº†ä¸¤ç§äº’è¡¥çš„æ‰°åŠ¨ç­–ç•¥ï¼šï¼ˆiï¼‰åœ¨å¾®è°ƒæœŸé—´å’Œæ¨ç†æœŸé—´åº”ç”¨é«˜æ–¯æç¤ºçº§æ‰°åŠ¨ï¼Œä»¥æ‰©å¤§å¼‚å¸¸çš„å…¨å±€å¤–è§‚ï¼ŒåŒæ—¶é¿å…ä½ä¿çœŸæ–‡æœ¬å¤–è§‚ï¼Œï¼ˆiiï¼‰æ©è†œå¼•å¯¼çš„ç©ºé—´å™ªå£°æ³¨å…¥ï¼Œä»¥ä¸°å¯Œå±€éƒ¨çº¹ç†å˜åŒ–ã€‚æ­¤å¤–ï¼Œä¸Šä¸‹æ–‡æ„ŸçŸ¥æ©è†œå¯¹é½æ¨¡å—å½¢æˆè¯­ä¹‰å¯¹åº”å…³ç³»å¹¶é‡æ–°å®šä½æ©è†œï¼Œä½¿æ¯ä¸ªå¼‚å¸¸éƒ½åˆç†åœ°åŒ…å«åœ¨å®¿ä¸»å¯¹è±¡ä¸­ï¼Œæ¶ˆé™¤äº†è¾¹ç•Œå¤–çš„ä¼ªå½±ã€‚åœ¨MVTec-ADæ•°æ®é›†ä¸Šé‡‡ç”¨ä¸€è‡´çš„è¯„ä»·åè®®ï¼ŒMAGICåœ¨ä¸‹æ¸¸å¼‚å¸¸ä»»åŠ¡ä¸­çš„æ€§èƒ½è¶…è¿‡äº†ä»¥å‰çš„æœ€å…ˆè¿›æ–¹æ³•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.02314v2">PDF</a> 10 pages, 6 figures. Code:   <a target="_blank" rel="noopener" href="https://github.com/Jaeihk/MAGIC-Anomaly-generation">https://github.com/Jaeihk/MAGIC-Anomaly-generation</a></p>
<p><strong>Summary</strong><br>    åŸºäºå°‘æ ·æœ¬çš„å¼‚å¸¸ç”Ÿæˆæˆä¸ºå·¥ä¸šè´¨é‡æ§åˆ¶è®¾ç½®ä¸­å¢å¼ºç¨€ç¼ºå¼‚å¸¸æ•°æ®çš„å®ç”¨è§£å†³æ–¹æ¡ˆã€‚ç°æœ‰æ–¹æ³•å­˜åœ¨ç¼ºé™·ï¼Œæ— æ³•æ»¡è¶³æ‰€æœ‰è¦æ±‚ã€‚æœ¬æ–‡æå‡ºMAGICæ–¹æ³•ï¼Œé€šè¿‡å¾®è°ƒStable Diffusionæ¨¡å‹è§£å†³èƒŒæ™¯ç ´åå’Œå¯¹é½é—®é¢˜ï¼Œå¹¶å¼•å…¥ä¸¤ç§æ‰°åŠ¨ç­–ç•¥å¢å¼ºå¤šæ ·æ€§ã€‚åœ¨MVTec-ADæ•°æ®é›†ä¸Šï¼ŒMAGICåœ¨ä¸‹æ¸¸å¼‚å¸¸æ£€æµ‹ä»»åŠ¡ä¸­è¡¨ç°ä¼˜äºç°æœ‰æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>Few-shot anomaly generationæ˜¯è§£å†³å·¥ä¸šè´¨é‡æ§åˆ¶ä¸­ç¨€ç¼ºå¼‚å¸¸æ•°æ®é—®é¢˜çš„å®ç”¨æ–¹æ³•ã€‚</li>
<li>ç°æœ‰æ–¹æ³•å¦‚å…¨å±€å¼‚å¸¸ç”Ÿæˆå’Œmaskå¼•å¯¼ç”Ÿæˆå­˜åœ¨ç¼ºé™·ï¼Œæ— æ³•æ»¡è¶³æ‰€æœ‰è¦æ±‚ã€‚</li>
<li>MAGICæ–¹æ³•é€šè¿‡å¾®è°ƒStable Diffusionæ¨¡å‹è§£å†³èƒŒæ™¯ç ´åå’Œå¯¹é½é—®é¢˜ã€‚</li>
<li>MAGICå¼•å…¥ä¸¤ç§æ‰°åŠ¨ç­–ç•¥ï¼šé«˜æ–¯æç¤ºçº§åˆ«æ‰°åŠ¨å’Œmaskå¼•å¯¼çš„ç©ºé—´å™ªå£°æ³¨å…¥ï¼Œä»¥å¢å¼ºå¤šæ ·æ€§å¹¶é¿å…ä½è´¨é‡æ–‡æœ¬å¤–è§‚ã€‚</li>
<li>ä¸Šä¸‹æ–‡æ„ŸçŸ¥maskå¯¹é½æ¨¡å—å½¢æˆè¯­ä¹‰å¯¹åº”å…³ç³»å¹¶é‡æ–°å®šä½maskï¼Œé¿å…å¼‚å¸¸å‡ºç°åœ¨è¾¹ç•Œä¹‹å¤–ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.02314">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-cd88a85826f03fb5f2018e4d6f0429bb.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-89e94bef23373798e52db88ac11fc486.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c62ff6dcca77bdc9e02fa203b7c54120.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-3c0bf3ad11975ff46ae929829d4433b4.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-20755c3100328bc94775cf660ea35a33.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="CHIME-Conditional-Hallucination-and-Integrated-Multi-scale-Enhancement-for-Time-Series-Diffusion-Model"><a href="#CHIME-Conditional-Hallucination-and-Integrated-Multi-scale-Enhancement-for-Time-Series-Diffusion-Model" class="headerlink" title="CHIME: Conditional Hallucination and Integrated Multi-scale Enhancement   for Time Series Diffusion Model"></a>CHIME: Conditional Hallucination and Integrated Multi-scale Enhancement   for Time Series Diffusion Model</h2><p><strong>Authors:Yuxuan Chen, Haipeng Xie</strong></p>
<p>The denoising diffusion probabilistic model has become a mainstream generative model, achieving significant success in various computer vision tasks. Recently, there has been initial exploration of applying diffusion models to time series tasks. However, existing studies still face challenges in multi-scale feature alignment and generative capabilities across different entities and long-time scales. In this paper, we propose CHIME, a conditional hallucination and integrated multi-scale enhancement framework for time series diffusion models. By employing multi-scale decomposition and integration, CHIME captures the decomposed features of time series, achieving in-domain distribution alignment between generated and original samples. In addition, we introduce a feature hallucination module in the conditional denoising process, enabling the temporal features transfer across long-time scales. Experimental results on publicly available real-world datasets demonstrate that CHIME achieves state-of-the-art performance and exhibits excellent generative generalization capabilities in few-shot scenarios. </p>
<blockquote>
<p>å»å™ªæ‰©æ•£æ¦‚ç‡æ¨¡å‹å·²ç»æˆä¸ºä¸»æµç”Ÿæˆæ¨¡å‹ï¼Œåœ¨å„ç§è®¡ç®—æœºè§†è§‰ä»»åŠ¡ä¸­å–å¾—äº†å·¨å¤§æˆåŠŸã€‚æœ€è¿‘ï¼Œäººä»¬å¼€å§‹æ¢ç´¢å°†æ‰©æ•£æ¨¡å‹åº”ç”¨äºæ—¶é—´åºåˆ—ä»»åŠ¡ã€‚ç„¶è€Œï¼Œç°æœ‰ç ”ç©¶åœ¨å¤šå°ºåº¦ç‰¹å¾å¯¹é½å’Œè·¨ä¸åŒå®ä½“å’Œé•¿æ—¶é—´å°ºåº¦çš„ç”Ÿæˆèƒ½åŠ›æ–¹é¢ä»é¢ä¸´æŒ‘æˆ˜ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†CHIMEï¼Œè¿™æ˜¯ä¸€ä¸ªé’ˆå¯¹æ—¶é—´åºåˆ—æ‰©æ•£æ¨¡å‹çš„åŸºäºæ¡ä»¶å¹»æƒ³å’Œé›†æˆå¤šå°ºåº¦å¢å¼ºæ¡†æ¶ã€‚é€šè¿‡é‡‡ç”¨å¤šå°ºåº¦åˆ†è§£å’Œé›†æˆï¼ŒCHIMEèƒ½å¤Ÿæ•è·æ—¶é—´åºåˆ—çš„åˆ†è§£ç‰¹å¾ï¼Œå®ç°ç”Ÿæˆæ ·æœ¬å’ŒåŸå§‹æ ·æœ¬ä¹‹é—´çš„åŸŸå†…åˆ†å¸ƒå¯¹é½ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬åœ¨æ¡ä»¶å»å™ªè¿‡ç¨‹ä¸­å¼•å…¥äº†ä¸€ä¸ªç‰¹å¾å¹»è§‰æ¨¡å—ï¼Œèƒ½å¤Ÿå®ç°è·¨é•¿æ—¶é—´å°ºåº¦çš„æ—¶æ€ç‰¹å¾è½¬ç§»ã€‚åœ¨å…¬å¼€çš„ç°å®ä¸–ç•Œæ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼ŒCHIMEè¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œå¹¶åœ¨å°æ ·åœºæ™¯ä¸­è¡¨ç°å‡ºå‡ºè‰²çš„ç”Ÿæˆæ³›åŒ–èƒ½åŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.03502v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æ‰©æ•£æ¦‚ç‡æ¨¡å‹å·²æˆä¸ºä¸»æµç”Ÿæˆæ¨¡å‹ï¼Œåœ¨è®¡ç®—æœºè§†è§‰ä»»åŠ¡ä¸­å–å¾—äº†å·¨å¤§æˆåŠŸã€‚æœ€è¿‘ï¼Œè¯¥æ¨¡å‹å¼€å§‹åº”ç”¨äºæ—¶é—´åºåˆ—ä»»åŠ¡ï¼Œä½†ç°æœ‰ç ”ç©¶ä»é¢ä¸´è·¨ä¸åŒå®ä½“å’Œé•¿æ—¶é—´å°ºåº¦çš„å¤šå°ºåº¦ç‰¹å¾å¯¹é½å’Œç”Ÿæˆèƒ½åŠ›æ–¹é¢çš„æŒ‘æˆ˜ã€‚æœ¬æ–‡æå‡ºäº†é¢å‘æ—¶é—´åºåˆ—æ‰©æ•£æ¨¡å‹çš„CHIMEæ¡†æ¶ï¼Œé‡‡ç”¨å¤šå°ºåº¦åˆ†è§£ä¸é›†æˆæ–¹æ³•ï¼Œå®ç°äº†æ—¶é—´åºåˆ—ç‰¹å¾çš„åˆ†è§£æ•è·ï¼Œç¡®ä¿äº†ç”Ÿæˆæ ·æœ¬ä¸åŸå§‹æ ·æœ¬åœ¨åŸŸå†…çš„åˆ†å¸ƒå¯¹é½ã€‚æ­¤å¤–ï¼Œåœ¨æ¡ä»¶å»å™ªè¿‡ç¨‹ä¸­å¼•å…¥äº†ç‰¹å¾å¹»åƒæ¨¡å—ï¼Œå®ç°äº†é•¿æ—¶é—´å°ºåº¦ä¸‹çš„æ—¶é—´ç‰¹å¾è½¬ç§»ã€‚åœ¨å…¬å¼€çš„ç°å®æ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼ŒCHIMEè¾¾åˆ°äº†æœ€æ–°æŠ€æœ¯æ°´å¹³ï¼Œå¹¶åœ¨æœ‰é™æ•°æ®åœºæ™¯ä¸­å±•ç°å‡ºå“è¶Šçš„ç”Ÿæˆæ³›åŒ–èƒ½åŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ‰©æ•£æ¦‚ç‡æ¨¡å‹åœ¨å¤šç§è®¡ç®—æœºè§†è§‰ä»»åŠ¡ä¸­è¡¨ç°ä¼˜ç§€ã€‚</li>
<li>CHIMEæ¡†æ¶è¢«åº”ç”¨äºæ—¶é—´åºåˆ—æ‰©æ•£æ¨¡å‹ã€‚</li>
<li>CHIMEè§£å†³äº†ç°æœ‰ç ”ç©¶ä¸­è·¨ä¸åŒå®ä½“å’Œé•¿æ—¶é—´å°ºåº¦çš„å¤šå°ºåº¦ç‰¹å¾å¯¹é½é—®é¢˜ã€‚</li>
<li>CHIMEé‡‡ç”¨å¤šå°ºåº¦åˆ†è§£ä¸é›†æˆï¼Œå®ç°äº†æ—¶é—´åºåˆ—ç‰¹å¾çš„åˆ†è§£æ•è·ã€‚</li>
<li>CHIMEåœ¨æ¡ä»¶å»å™ªè¿‡ç¨‹ä¸­å¼•å…¥äº†ç‰¹å¾å¹»åƒæ¨¡å—ï¼Œæœ‰åŠ©äºæ—¶é—´ç‰¹å¾çš„é•¿æœŸè½¬ç§»ã€‚</li>
<li>åœ¨å…¬å¼€æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒCHIMEå…·æœ‰æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.03502">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-4e626b77ce28c7f420026b09b16eb0b0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e5f9662c78828ec2e94cd9a337ce5d8e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-81b9a95050c3279d2fdd794f3aaf308f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3113c6139086d44392fab4877aa1480c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a3e59230e5cdb51dc55c45d637c0fe77.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="TerraMind-Large-Scale-Generative-Multimodality-for-Earth-Observation"><a href="#TerraMind-Large-Scale-Generative-Multimodality-for-Earth-Observation" class="headerlink" title="TerraMind: Large-Scale Generative Multimodality for Earth Observation"></a>TerraMind: Large-Scale Generative Multimodality for Earth Observation</h2><p><strong>Authors:Johannes Jakubik, Felix Yang, Benedikt Blumenstiel, Erik Scheurer, Rocco Sedona, Stefano Maurogiovanni, Jente Bosmans, Nikolaos Dionelis, Valerio Marsocci, Niklas Kopp, Rahul Ramachandran, Paolo Fraccaro, Thomas Brunschwiler, Gabriele Cavallaro, Juan Bernabe-Moreno, Nicolas LongÃ©pÃ©</strong></p>
<p>We present TerraMind, the first any-to-any generative, multimodal foundation model for Earth observation (EO). Unlike other multimodal models, TerraMind is pretrained on dual-scale representations combining both token-level and pixel-level data across modalities. On a token level, TerraMind encodes high-level contextual information to learn cross-modal relationships, while on a pixel level, TerraMind leverages fine-grained representations to capture critical spatial nuances. We pretrained TerraMind on nine geospatial modalities of a global, large-scale dataset. In this paper, we demonstrate that (i) TerraMindâ€™s dual-scale early fusion approach unlocks a range of zero-shot and few-shot applications for Earth observation, (ii) TerraMind introduces â€œThinking-in-Modalitiesâ€ (TiM) â€“ the capability of generating additional artificial data during finetuning and inference to improve the model output â€“ and (iii) TerraMind achieves beyond state-of-the-art performance in community-standard benchmarks for EO like PANGAEA. The pretraining dataset, the model weights, and our code are open-sourced under a permissive license. </p>
<blockquote>
<p>æˆ‘ä»¬æ¨å‡ºTerraMindï¼Œè¿™æ˜¯é¦–ä¸ªé¢å‘åœ°çƒè§‚æµ‹ï¼ˆEOï¼‰çš„ä»»æ„åˆ°ä»»æ„ç”Ÿæˆã€å¤šæ¨¡å¼åŸºç¡€æ¨¡å‹ã€‚ä¸å…¶ä»–å¤šæ¨¡å¼æ¨¡å‹ä¸åŒï¼ŒTerraMindæ˜¯åœ¨åŒå°ºåº¦è¡¨ç¤ºä¸Šé¢„è®­ç»ƒçš„ï¼Œç»“åˆäº†è·¨æ¨¡å¼çš„ä»¤ç‰Œçº§åˆ«å’Œåƒç´ çº§åˆ«æ•°æ®ã€‚åœ¨ä»¤ç‰Œçº§åˆ«ä¸Šï¼ŒTerraMindç¼–ç é«˜çº§ä¸Šä¸‹æ–‡ä¿¡æ¯ä»¥å­¦ä¹ è·¨æ¨¡å¼å…³ç³»ï¼Œè€Œåœ¨åƒç´ çº§åˆ«ä¸Šï¼ŒTerraMindåˆ©ç”¨ç²¾ç»†è¡¨ç¤ºæ¥æ•æ‰å…³é”®çš„ç©ºé—´ç»†å¾®å·®åˆ«ã€‚æˆ‘ä»¬åœ¨å…¨çƒå¤§è§„æ¨¡æ•°æ®é›†çš„ä¹ä¸ªåœ°ç†ç©ºé—´æ¨¡å¼ä¸Šé¢„è®­ç»ƒäº†TerraMindã€‚æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬è¯æ˜äº†ï¼ˆiï¼‰TerraMindçš„åŒå°ºåº¦æ—©æœŸèåˆæ–¹æ³•å¼€å¯äº†åœ°çƒè§‚æµ‹çš„é›¶æ ·æœ¬å’Œå°‘æ ·æœ¬åº”ç”¨èŒƒå›´ï¼Œï¼ˆiiï¼‰TerraMindå¼•å…¥äº†â€œæ¨¡æ€æ€è€ƒâ€ï¼ˆTiMï¼‰â€”â€”åœ¨å¾®è°ƒï¼ˆfinetuningï¼‰å’Œæ¨ç†æœŸé—´ç”Ÿæˆé¢å¤–çš„æ¨¡æ‹Ÿæ•°æ®ä»¥æé«˜æ¨¡å‹è¾“å‡ºçš„èƒ½åŠ›ï¼Œï¼ˆiiiï¼‰TerraMindåœ¨åƒPANGAEAè¿™æ ·çš„ç¤¾åŒºæ ‡å‡†åŸºå‡†æµ‹è¯•ä¸­å®ç°äº†è¶…è¶Šæœ€æ–°æŠ€æœ¯çš„æ€§èƒ½ã€‚é¢„è®­ç»ƒæ•°æ®é›†ã€æ¨¡å‹æƒé‡å’Œæˆ‘ä»¬çš„ä»£ç éƒ½æ˜¯åœ¨è®¸å¯ä¸‹å¼€æºçš„ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.11171v3">PDF</a> Accepted at ICCVâ€™25</p>
<p><strong>Summary</strong></p>
<p>TerraMindæ˜¯é¦–ä¸ªé’ˆå¯¹åœ°çƒè§‚æµ‹çš„ä»»æ„åˆ°ä»»æ„ç”Ÿæˆã€å¤šæ¨¡æ€åŸºç¡€æ¨¡å‹ã€‚å®ƒé‡‡ç”¨åŒå°ºåº¦è¡¨ç¤ºæ³•ï¼Œç»“åˆæ ‡è®°çº§åˆ«å’Œåƒç´ çº§åˆ«çš„æ•°æ®è·¨æ¨¡æ€è¿›è¡Œé¢„è®­ç»ƒï¼Œèƒ½å­¦ä¹ è·¨æ¨¡æ€å…³ç³»å¹¶æ•æ‰å…³é”®ç©ºé—´ç»†èŠ‚ã€‚TerraMindå¼•å…¥â€œæ¨¡æ€æ€è€ƒâ€ï¼ˆThinking-in-Modalitiesï¼‰èƒ½åŠ›ï¼Œèƒ½åœ¨å¾®è°ƒåŠæ¨ç†è¿‡ç¨‹ä¸­ç”Ÿæˆé¢å¤–çš„äººå·¥æ•°æ®ï¼Œä»¥æå‡æ¨¡å‹è¾“å‡ºæ•ˆæœã€‚è¯¥æ¨¡å‹åœ¨åœ°çƒè§‚æµ‹çš„ç¤¾åŒºæ ‡å‡†åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°è¶…è¶Šç°æœ‰æŠ€æœ¯ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>TerraMindæ˜¯é¦–ä¸ªåœ°çƒè§‚æµ‹çš„å¤šæ¨¡æ€åŸºç¡€æ¨¡å‹ã€‚</li>
<li>TerraMindé‡‡ç”¨åŒå°ºåº¦è¡¨ç¤ºæ³•ï¼Œç»“åˆæ ‡è®°å’Œåƒç´ çº§åˆ«æ•°æ®è¿›è¡Œé¢„è®­ç»ƒã€‚</li>
<li>TerraMindèƒ½å­¦ä¹ è·¨æ¨¡æ€å…³ç³»å¹¶æ•æ‰å…³é”®ç©ºé—´ç»†èŠ‚ã€‚</li>
<li>TerraMindå¼•å…¥â€œæ¨¡æ€æ€è€ƒâ€ï¼ˆThinking-in-Modalitiesï¼‰èƒ½åŠ›ï¼Œæå‡æ¨¡å‹è¾“å‡ºæ•ˆæœã€‚</li>
<li>TerraMindå¯åœ¨é›¶æ ·æœ¬å’Œå°‘æ ·æœ¬æƒ…å†µä¸‹åº”ç”¨ã€‚</li>
<li>TerraMindåœ¨åœ°çƒè§‚æµ‹çš„ç¤¾åŒºæ ‡å‡†åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°è¶…è¶Šç°æœ‰æŠ€æœ¯ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.11171">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-5aba830dc2c3130ffb3fbe4fd8c17952.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3347939d427941279cfe77bf0ababf97.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-8b652173cc8f70650183827763fd12d6.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-36245c755aff9a356a0b6cede4891cc7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7536641f71f28dded866d249d4218319.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="7B-Fully-Open-Source-Moxin-LLM-VLM-â€“-From-Pretraining-to-GRPO-based-Reinforcement-Learning-Enhancement"><a href="#7B-Fully-Open-Source-Moxin-LLM-VLM-â€“-From-Pretraining-to-GRPO-based-Reinforcement-Learning-Enhancement" class="headerlink" title="7B Fully Open Source Moxin-LLM&#x2F;VLM â€“ From Pretraining to GRPO-based   Reinforcement Learning Enhancement"></a>7B Fully Open Source Moxin-LLM&#x2F;VLM â€“ From Pretraining to GRPO-based   Reinforcement Learning Enhancement</h2><p><strong>Authors:Pu Zhao, Xuan Shen, Zhenglun Kong, Yixin Shen, Sung-En Chang, Arash Akbari, Timothy Rupprecht, Lei Lu, Enfu Nan, Changdi Yang, Yumei He, Weiyan Shi, Xingchen Xu, Yu Huang, Wei Jiang, Wei Wang, Yue Chen, Yong He, Yanzhi Wang</strong></p>
<p>Recently, Large Language Models (LLMs) have undergone a significant transformation, marked by a rapid rise in both their popularity and capabilities. Leading this evolution are proprietary LLMs like GPT-4 and GPT-o1, which have captured widespread attention in the AI community due to their remarkable performance and versatility. Simultaneously, open-source LLMs, such as LLaMA, have made great contributions to the ever-increasing popularity of LLMs due to the ease to customize and deploy the models across diverse applications. Although open-source LLMs present unprecedented opportunities for innovation and research, the commercialization of LLMs has raised concerns about transparency, reproducibility, and safety. Many open-source LLMs fail to meet fundamental transparency requirements by withholding essential components like training code and data, which may hinder further innovations on LLMs. To mitigate this issue, we introduce Moxin 7B, a fully open-source LLM developed, adhering to principles of open science, open source, open data, and open access. We release the pre-training code and configurations, training and fine-tuning datasets, and intermediate and final checkpoints, aiming to make continuous commitments to fully open-source LLMs. After pre-training the base model, we finetune the Moxin Base model with SOTA post-training framework and instruction data to obtain Moxin Instruct model. To improve the reasoning capability, we further finetune our Instruct model with chain-of-thought data distilled from DeepSeek R1, and then use Group Relative Policy Optimization (GRPO) following DeepSeek R1 to finetune our model, leading to the Moxin Reasoning model. Moreover, we develop our vision language model based on our Moxin model. Experiments show that our models achieve superior performance in various evaluations such as zero-shot evaluation, few-shot evaluation, and CoT evaluation. </p>
<blockquote>
<p>æœ€è¿‘ï¼Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ç»å†äº†é‡å¤§è½¬å˜ï¼Œå…¶å—æ¬¢è¿ç¨‹åº¦å’Œèƒ½åŠ›éƒ½è¿…é€Ÿä¸Šå‡ã€‚å¼•é¢†è¿™ä¸€å˜é©çš„æ˜¯åƒGPT-4å’ŒGPT-o1è¿™æ ·çš„ä¸“æœ‰å¤§å‹è¯­è¨€æ¨¡å‹ï¼Œå®ƒä»¬å› å‡ºè‰²çš„æ€§èƒ½å’Œå¤šåŠŸèƒ½æ€§è€Œå¤‡å—äººå·¥æ™ºèƒ½ç¤¾åŒºçš„å¹¿æ³›å…³æ³¨ã€‚åŒæ—¶ï¼Œå¼€æºå¤§å‹è¯­è¨€æ¨¡å‹ï¼Œå¦‚LLaMAï¼Œç”±äºå¯¹æ¨¡å‹è¿›è¡Œå®šåˆ¶å’Œè·¨å¤šç§åº”ç”¨ç¨‹åºéƒ¨ç½²çš„ä¾¿æ·æ€§ï¼Œä¹Ÿä¸ºå¤§å‹è¯­è¨€æ¨¡å‹æ—¥ç›Šæ™®åŠåšå‡ºäº†å·¨å¤§è´¡çŒ®ã€‚ç„¶è€Œï¼Œå¤§å‹è¯­è¨€æ¨¡å‹çš„å•†ä¸šåŒ–å¼•å‘äº†å…³äºé€æ˜åº¦ã€å¯é‡å¤æ€§å’Œå®‰å…¨çš„æ‹…å¿§ã€‚è®¸å¤šå¼€æºçš„å¤§å‹è¯­è¨€æ¨¡å‹æœªèƒ½æ»¡è¶³åŸºæœ¬çš„é€æ˜åº¦è¦æ±‚ï¼Œéšç’äº†å…³é”®ç»„ä»¶ï¼Œå¦‚è®­ç»ƒä»£ç å’Œæ•°æ®ï¼Œè¿™å¯èƒ½é˜»ç¢å¤§å‹è¯­è¨€æ¨¡å‹çš„è¿›ä¸€æ­¥åˆ›æ–°ã€‚ä¸ºäº†ç¼“è§£è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æ¨å‡ºäº†Moxin 7Bï¼Œè¿™æ˜¯ä¸€ä¸ªéµå¾ªå…¬å¼€ç§‘å­¦ã€å¼€æºã€å¼€æ”¾æ•°æ®å’Œå¼€æ”¾è®¿é—®åŸåˆ™çš„å…¨å¼€æºå¤§å‹è¯­è¨€æ¨¡å‹ã€‚æˆ‘ä»¬å‘å¸ƒäº†é¢„è®­ç»ƒä»£ç å’Œé…ç½®ã€è®­ç»ƒå’Œå¾®è°ƒæ•°æ®é›†ä»¥åŠä¸­é—´å’Œæœ€ç»ˆæ£€æŸ¥ç‚¹ï¼Œè‡´åŠ›äºå®Œå…¨å¼€æºçš„å¤§å‹è¯­è¨€æ¨¡å‹çš„æŒç»­æ‰¿è¯ºã€‚åœ¨é¢„è®­ç»ƒåŸºç¡€æ¨¡å‹åï¼Œæˆ‘ä»¬ä½¿ç”¨å…ˆè¿›çš„åè®­ç»ƒæ¡†æ¶å’ŒæŒ‡ä»¤æ•°æ®å¯¹MoxinåŸºç¡€æ¨¡å‹è¿›è¡Œå¾®è°ƒï¼Œä»¥è·å¾—MoxinæŒ‡ä»¤æ¨¡å‹ã€‚ä¸ºäº†æé«˜æ¨ç†èƒ½åŠ›ï¼Œæˆ‘ä»¬è¿›ä¸€æ­¥ä½¿ç”¨ä»DeepSeek R1è’¸é¦å¾—åˆ°çš„æ€ç»´é“¾æ•°æ®å¯¹æŒ‡ä»¤æ¨¡å‹è¿›è¡Œå¾®è°ƒï¼Œç„¶åéµå¾ªDeepSeek R1ä½¿ç”¨ç»„ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆGRPOï¼‰å¯¹æˆ‘ä»¬çš„æ¨¡å‹è¿›è¡Œå¾®è°ƒï¼Œä»è€Œå¾—åˆ°Moxinæ¨ç†æ¨¡å‹ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬åŸºäºMoxinæ¨¡å‹å¼€å‘äº†æˆ‘ä»¬çš„è§†è§‰è¯­è¨€æ¨¡å‹ã€‚å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ¨¡å‹åœ¨é›¶æ ·æœ¬è¯„ä¼°ã€å°‘æ ·æœ¬è¯„ä¼°å’ŒCoTè¯„ä¼°ç­‰å„ç§è¯„ä¼°ä¸­éƒ½å–å¾—äº†ä¼˜è¶Šçš„æ€§èƒ½ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.06845v6">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è¿‘æœŸç»å†æ˜¾è‘—å˜é©ï¼Œå—åˆ°å¹¿æ³›å…³æ³¨ã€‚GPT-4å’ŒGPT-o1ç­‰ä¸“æœ‰LLMå¼•èµ·AIç¤¾åŒºå¹¿æ³›å…³æ³¨ï¼ŒåŒæ—¶å¼€æºLLMå¦‚LLaMAä¹Ÿä¸ºLLMçš„æ™®åŠåšå‡ºè´¡çŒ®ã€‚ç„¶è€Œï¼Œå¼€æºLLMçš„å•†ä¸šåŒ–å¼•å‘é€æ˜åº¦ã€å¯å¤åˆ¶æ€§å’Œå®‰å…¨æ€§çš„æ‹…å¿§ã€‚éƒ¨åˆ†å¼€æºLLMæœªèƒ½æ»¡è¶³åŸºæœ¬é€æ˜åº¦è¦æ±‚ï¼Œéšè—äº†è®­ç»ƒä»£ç å’Œæ•°æ®ç­‰é‡è¦ç»„ä»¶ï¼Œå¯èƒ½é˜»ç¢LLMçš„è¿›ä¸€æ­¥åˆ›æ–°ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æ¨å‡ºå®Œå…¨å¼€æºçš„LLMâ€”â€”Moxin 7Bï¼Œéµå¾ªå…¬å¼€ç§‘å­¦ã€å¼€æºã€å¼€æ”¾æ•°æ®å’Œå¼€æ”¾è®¿é—®çš„åŸåˆ™ã€‚æˆ‘ä»¬å…¬å¼€é¢„è®­ç»ƒä»£ç å’Œé…ç½®ã€è®­ç»ƒå’Œå¾®è°ƒæ•°æ®é›†ä»¥åŠä¸­é—´å’Œæœ€ç»ˆæ£€æŸ¥ç‚¹ã€‚åŸºäºMoxin Baseæ¨¡å‹ï¼Œä½¿ç”¨æœ€æ–°åè®­ç»ƒæ¡†æ¶å’ŒæŒ‡ä»¤æ•°æ®å¾®è°ƒå¾—åˆ°Moxin Instructæ¨¡å‹ã€‚é€šè¿‡è¿›ä¸€æ­¥ä½¿ç”¨æ¥è‡ªDeepSeek R1çš„æ€ç»´é“¾æ•°æ®ä¼˜åŒ–ï¼Œå¹¶ä½¿ç”¨Group Relative Policy Optimization (GRPO)ç­–ç•¥å¾®è°ƒæ¨¡å‹ï¼Œæ¨å‡ºMoxin Reasoningæ¨¡å‹ã€‚åŒæ—¶ï¼ŒåŸºäºMoxinæ¨¡å‹å¼€å‘è§†è§‰è¯­è¨€æ¨¡å‹ï¼Œå®éªŒæ˜¾ç¤ºæ¨¡å‹åœ¨é›¶æ ·æœ¬ã€å°‘æ ·æœ¬å’Œæ€ç»´é“¾è¯„ä¼°ä¸­è¡¨ç°ä¼˜å¼‚ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è¿‘æœŸå—åˆ°å¹¿æ³›å…³æ³¨å’Œå¿«é€Ÿå‘å±•ã€‚</li>
<li>ä¸“æœ‰LLMå¦‚GPT-4å’ŒGPT-o1å¼•èµ·AIç¤¾åŒºå…³æ³¨ï¼Œè€Œå¼€æºLLMå¦‚LLaMAä¹Ÿä¸ºæ™®åŠåšå‡ºè´¡çŒ®ã€‚</li>
<li>å•†ä¸šåŒ–å¼€æºLLMå¼•å‘é€æ˜åº¦ã€å¯å¤åˆ¶æ€§å’Œå®‰å…¨æ€§çš„æ‹…å¿§ã€‚</li>
<li>éƒ¨åˆ†å¼€æºLLMæœªèƒ½æ»¡è¶³é€æ˜åº¦è¦æ±‚ï¼Œéšè—é‡è¦ç»„ä»¶å¦‚è®­ç»ƒä»£ç å’Œæ•°æ®ã€‚</li>
<li>æ¨å‡ºå®Œå…¨å¼€æºçš„LLMâ€”â€”Moxin 7Bï¼Œéµå¾ªå…¬å¼€ç§‘å­¦åŸåˆ™ï¼Œå…¬å¼€é¢„è®­ç»ƒä»£ç ã€é…ç½®ã€æ•°æ®é›†å’Œæ£€æŸ¥ç‚¹ã€‚</li>
<li>Moxin Instructæ¨¡å‹å’ŒMoxin Reasoningæ¨¡å‹é€šè¿‡å¾®è°ƒå’ŒæŠ€æœ¯ä¼˜åŒ–è¾¾åˆ°ä¼˜å¼‚æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.06845">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-23da92114be2ce0b2b6f7021b2c60ba7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1ba681a6d008d329ac5864b7d051a1d2.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="The-Super-Weight-in-Large-Language-Models"><a href="#The-Super-Weight-in-Large-Language-Models" class="headerlink" title="The Super Weight in Large Language Models"></a>The Super Weight in Large Language Models</h2><p><strong>Authors:Mengxia Yu, De Wang, Qi Shan, Colorado J Reed, Alvin Wan</strong></p>
<p>Recent works have shown a surprising result: a small fraction of Large Language Model (LLM) parameter outliers are disproportionately important to the quality of the model. LLMs contain billions of parameters, so these small fractions, such as 0.01%, translate to hundreds of thousands of parameters. In this work, we present an even more surprising finding: Pruning as few as a single parameter can destroy an LLMâ€™s ability to generate text â€“ increasing perplexity by 3 orders of magnitude and reducing zero-shot accuracy to guessing. We propose a data-free method for identifying such parameters, termed super weights, using a single forward pass through the model. We additionally find that these super weights induce correspondingly rare and large activation outliers, termed super activations. When preserved with high precision, super activations can improve simple round-to-nearest quantization to become competitive with state-of-the-art methods. For weight quantization, we similarly find that by preserving the super weight and clipping other weight outliers, round-to-nearest quantization can scale to much larger block sizes than previously considered. To facilitate further research into super weights, we provide an index of super weight coordinates for common, openly available LLMs. </p>
<blockquote>
<p>è¿‘æœŸçš„ç ”ç©¶ç»“æœä»¤äººæƒŠè®¶ï¼šå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¸­çš„ä¸€å°éƒ¨åˆ†å‚æ•°å¼‚å¸¸å€¼å¯¹æ¨¡å‹è´¨é‡æœ‰ç€æå…¶é‡è¦çš„ä½œç”¨ã€‚LLMåŒ…å«æ•°åäº¿å‚æ•°ï¼Œå› æ­¤è¿™äº›å°éƒ¨åˆ†ï¼Œä¾‹å¦‚0.01%ï¼Œç›¸å½“äºæ•°åä¸‡ä¸ªå‚æ•°ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªæ›´ä»¤äººæƒŠè®¶çš„å‘ç°ï¼šä»…åˆ é™¤ä¸€ä¸ªå‚æ•°å°±èƒ½ç ´åLLMç”Ÿæˆæ–‡æœ¬çš„èƒ½åŠ›â€”â€”ä½¿å›°æƒ‘åº¦å¢åŠ ä¸‰ä¸ªæ•°é‡çº§ï¼Œå¹¶å°†é›¶å°„å‡»å‡†ç¡®ç‡é™ä½åˆ°çŒœæµ‹æ°´å¹³ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ— éœ€æ•°æ®çš„æ–¹æ³•ï¼Œé€šè¿‡å•ä¸€çš„å‰å‘ä¼ é€’æ¨¡å‹æ¥è¯†åˆ«è¿™äº›å‚æ•°ï¼Œç§°ä¸ºè¶…çº§æƒé‡ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜å‘ç°è¿™äº›è¶…çº§æƒé‡ä¼šå¼•å‘ç›¸åº”ç½•è§çš„è¶…å¤§æ¿€æ´»å¼‚å¸¸å€¼ï¼Œç§°ä¸ºè¶…çº§æ¿€æ´»ã€‚å½“ä»¥é«˜ç²¾ç¡®åº¦ä¿ç•™æ—¶ï¼Œè¶…çº§æ¿€æ´»å¯ä»¥æ”¹å–„ç®€å•çš„å››èˆäº”å…¥é‡åŒ–æ–¹æ³•ï¼Œä½¿å…¶ä¸æœ€æ–°æŠ€æœ¯ç›¸ç«äº‰ã€‚å¯¹äºæƒé‡é‡åŒ–ï¼Œæˆ‘ä»¬ä¹Ÿå‘ç°é€šè¿‡ä¿ç•™è¶…çº§æƒé‡å¹¶å‰ªåˆ‡å…¶ä»–æƒé‡å¼‚å¸¸å€¼ï¼Œå››èˆäº”å…¥é‡åŒ–æ–¹æ³•å¯ä»¥æ‰©å±•åˆ°æ¯”ä»¥å¾€æ›´å¤§çš„å—å¤§å°ã€‚ä¸ºäº†ä¿ƒè¿›å¯¹è¶…çº§æƒé‡çš„è¿›ä¸€æ­¥ç ”ç©¶ï¼Œæˆ‘ä»¬ä¸ºå¸¸è§çš„å…¬å¼€LLMæä¾›äº†è¶…çº§æƒé‡åæ ‡çš„ç´¢å¼•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2411.07191v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>è¿‘æœŸç ”ç©¶å‘ç°ï¼Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¸­çš„ä¸€å°éƒ¨åˆ†å‚æ•°å¼‚å¸¸å€¼å¯¹æ¨¡å‹è´¨é‡è‡³å…³é‡è¦ã€‚é€šè¿‡ä»…ä¿®å‰ªä¸€ä¸ªå‚æ•°ï¼ŒLLMç”Ÿæˆæ–‡æœ¬çš„èƒ½åŠ›å¯èƒ½è¢«ç ´åï¼Œè¡¨ç°ä¸ºå›°æƒ‘åº¦å¢åŠ ä¸‰ä¸ªæ•°é‡çº§ï¼Œé›¶æ ·æœ¬å‡†ç¡®ç‡é™è‡³çŒœæµ‹æ°´å¹³ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§æ— éœ€æ•°æ®çš„è¶…çº§æƒé‡è¯†åˆ«æ–¹æ³•ï¼Œå¹¶å‘ç°è¶…çº§æƒé‡å¯¹åº”äº§ç”Ÿç¨€æœ‰ä¸”å¤§å‹æ¿€æ´»å¼‚å¸¸å€¼â€”â€”è¶…çº§æ¿€æ´»ã€‚å½“ç²¾ç¡®ä¿ç•™è¶…çº§æ¿€æ´»æ—¶ï¼Œç®€å•çš„å››èˆäº”å…¥é‡åŒ–æ–¹æ³•å¯æé«˜è‡³ä¸æœ€æ–°æŠ€æœ¯ç›¸å½“çš„æ°´å¹³ã€‚å¯¹äºæƒé‡é‡åŒ–ï¼Œé€šè¿‡ä¿ç•™è¶…çº§æƒé‡å¹¶è£å‰ªå…¶ä»–æƒé‡å¼‚å¸¸å€¼ï¼Œç®€å•çš„å››èˆäº”å…¥é‡åŒ–æ–¹æ³•å¯ä»¥æ‰©å±•åˆ°æ›´å¤§çš„å—å¤§å°ã€‚ä¸ºäº†æ–¹ä¾¿å¯¹è¶…çº§æƒé‡è¿›è¡Œè¿›ä¸€æ­¥ç ”ç©¶ï¼Œæˆ‘ä»¬ä¸ºå¸¸è§çš„å…¬å¼€LLMæä¾›äº†è¶…çº§æƒé‡åæ ‡ç´¢å¼•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLMä¸­çš„ä¸€å°éƒ¨åˆ†å‚æ•°å¼‚å¸¸å€¼å¯¹æ¨¡å‹è´¨é‡è‡³å…³é‡è¦ã€‚</li>
<li>ä»…ä¿®å‰ªä¸€ä¸ªå‚æ•°å¯èƒ½å¯¼è‡´LLMç”Ÿæˆæ–‡æœ¬èƒ½åŠ›çš„å¤§å¹…ä¸‹é™ã€‚</li>
<li>æå‡ºäº†æ•°æ®æ— å…³çš„è¶…çº§æƒé‡è¯†åˆ«æ–¹æ³•ã€‚</li>
<li>è¶…çº§æƒé‡ä¼šå¼•å‘è¶…çº§æ¿€æ´»ç°è±¡ã€‚</li>
<li>ç²¾ç¡®ä¿ç•™è¶…çº§æ¿€æ´»èƒ½æé«˜é‡åŒ–æ–¹æ³•çš„æ€§èƒ½ã€‚</li>
<li>å¯¹äºæƒé‡é‡åŒ–ï¼Œä¿ç•™è¶…çº§æƒé‡å¹¶è£å‰ªå…¶ä»–æƒé‡å¼‚å¸¸å€¼èƒ½ä½¿é‡åŒ–æ–¹æ³•æ›´æœ‰æ•ˆç‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2411.07191">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-b9e407456aea67e6f0d029024a5fc331.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c39ed429afb5dd532031cff1d76e91c2.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-4d6c7c97eb3c6f67a4a3c15b3ded2a73.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-351564f1ad696b77ab98ba8ade65960e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-8175f3d78567c124bb86ec7d6c4c3d6c.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-e646d15d7c683ce70167ef2c1c8447a8.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="Large-scale-Independent-and-Comprehensive-study-of-the-power-of-LLMs-for-test-case-generation"><a href="#Large-scale-Independent-and-Comprehensive-study-of-the-power-of-LLMs-for-test-case-generation" class="headerlink" title="Large-scale, Independent and Comprehensive study of the power of LLMs   for test case generation"></a>Large-scale, Independent and Comprehensive study of the power of LLMs   for test case generation</h2><p><strong>Authors:WendkÃ»uni C. OuÃ©draogo, Kader KaborÃ©, Yinghua Li, Haoye Tian, Anil Koyuncu, Jacques Klein, David Lo, TegawendÃ© F. BissyandÃ©</strong></p>
<p>Unit testing is essential for software reliability, yet manual test creation is time-consuming and often neglected. Although search-based software testing improves efficiency, it produces tests with poor readability and maintainability. Although LLMs show promise for test generation, existing research lacks comprehensive evaluation across execution-driven assessment, reasoning-based prompting, and real-world testing scenarios. This study presents the first large-scale empirical evaluation of LLM-generated unit tests at the class level, systematically analyzing four state-of-the-art models - GPT-3.5, GPT-4, Mistral 7B, and Mixtral 8x7B - against EvoSuite across 216,300 test cases from Defects4J, SF110, and CMD (a dataset mitigating LLM training data leakage). We evaluate five prompting techniques - Zero-Shot Learning (ZSL), Few-Shot Learning (FSL), Chain-of-Thought (CoT), Tree-of-Thought (ToT), and Guided Tree-of-Thought (GToT) - assessing syntactic correctness, compilability, hallucination-driven failures, readability, code coverage metrics, and fault detection capabilities. Our findings challenge prior claims that in-context learning is ineffective for test generation in code-specialized LLMs. Reasoning-based prompting - particularly GToT - significantly enhances test reliability, compilability, and structural adherence in general-purpose LLMs. However, hallucination-driven failures remain a persistent challenge, manifesting as non-existent symbol references, incorrect API calls, and fabricated dependencies, resulting in high compilation failure rates (up to 86%). Execution-based classification and mutation testing reveal that many failing tests stem from hallucinated dependencies, limiting effective fault detection. </p>
<blockquote>
<p>å•å…ƒæµ‹è¯•å¯¹äºè½¯ä»¶å¯é æ€§è‡³å…³é‡è¦ï¼Œä½†æ‰‹åŠ¨åˆ›å»ºæµ‹è¯•éå¸¸è€—æ—¶ä¸”ç»å¸¸è¢«å¿½è§†ã€‚å°½ç®¡åŸºäºæœç´¢çš„è½¯ä»¶æµ‹è¯•æé«˜äº†æ•ˆç‡ï¼Œä½†å®ƒäº§ç”Ÿçš„æµ‹è¯•å¯è¯»æ€§è¾ƒå·®ä¸”éš¾ä»¥ç»´æŠ¤ã€‚è™½ç„¶å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨æµ‹è¯•ç”Ÿæˆæ–¹é¢æ˜¾ç¤ºå‡ºæ½œåŠ›ï¼Œä½†ç°æœ‰ç ”ç©¶åœ¨æ‰§è¡Œé©±åŠ¨è¯„ä¼°ã€åŸºäºæ¨ç†çš„æç¤ºå’ŒçœŸå®ä¸–ç•Œæµ‹è¯•åœºæ™¯æ–¹é¢çš„ç»¼åˆè¯„ä¼°ä»ç„¶ä¸è¶³ã€‚æœ¬ç ”ç©¶é¦–æ¬¡å¯¹LLMç”Ÿæˆçš„ç±»çº§å•å…ƒæµ‹è¯•è¿›è¡Œå¤§è§„æ¨¡å®è¯ç ”ç©¶ï¼Œç³»ç»Ÿåˆ†æäº†å››ç§æœ€å…ˆè¿›çš„æ¨¡å‹â€”â€”GPT-3.5ã€GPT-4ã€Mistral 7Bå’ŒMixtral 8x7Bï¼Œä¸EvoSuiteç›¸å¯¹æ¯”ï¼Œç ”ç©¶å¯¹è±¡ä¸ºæ¥è‡ªDefects4Jã€SF110å’ŒCMDï¼ˆä¸€ç§å‡è½»LLMè®­ç»ƒæ•°æ®æ³„éœ²çš„æ•°æ®é›†ï¼‰çš„216,300ä¸ªæµ‹è¯•ç”¨ä¾‹ã€‚æˆ‘ä»¬è¯„ä¼°äº†äº”ç§æç¤ºæŠ€æœ¯â€”â€”é›¶æ ·æœ¬å­¦ä¹ ï¼ˆZSLï¼‰ã€å°æ ·æœ¬å­¦ä¹ ï¼ˆFSLï¼‰ã€æ€ç»´é“¾ï¼ˆCoTï¼‰ã€æ€ç»´æ ‘ï¼ˆToTï¼‰å’Œå¼•å¯¼æ€ç»´æ ‘ï¼ˆGToTï¼‰â€”â€”è¯„ä¼°è¯­æ³•æ­£ç¡®æ€§ã€å¯ç¼–è¯‘æ€§ã€å¹»è§‰é©±åŠ¨æ•…éšœã€å¯è¯»æ€§ã€ä»£ç è¦†ç›–ç‡æŒ‡æ ‡å’Œæ•…éšœæ£€æµ‹èƒ½åŠ›ã€‚æˆ‘ä»¬çš„ç ”ç©¶ç»“æœè´¨ç–‘äº†å…ˆå‰å…³äºä¸Šä¸‹æ–‡å­¦ä¹ å¯¹ä»£ç ä¸“ç”¨LLMä¸­çš„æµ‹è¯•ç”Ÿæˆæ— æ•ˆçš„è¯´æ³•ã€‚åŸºäºæ¨ç†çš„æç¤ºâ€”â€”å°¤å…¶æ˜¯GToTâ€”â€”æ˜¾è‘—æé«˜äº†é€šç”¨LLMçš„æµ‹è¯•å¯é æ€§ã€å¯ç¼–è¯‘æ€§å’Œç»“æ„ä¾ä»æ€§ã€‚ç„¶è€Œï¼Œå¹»è§‰é©±åŠ¨æ•…éšœä»ç„¶æ˜¯ä¸€ä¸ªæŒç»­å­˜åœ¨çš„æŒ‘æˆ˜ï¼Œè¡¨ç°ä¸ºä¸å­˜åœ¨çš„ç¬¦å·å¼•ç”¨ã€é”™è¯¯çš„APIè°ƒç”¨å’Œè™šæ„çš„ä¾èµ–å…³ç³»ï¼Œå¯¼è‡´é«˜ç¼–è¯‘å¤±è´¥ç‡ï¼ˆé«˜è¾¾86%ï¼‰ã€‚åŸºäºæ‰§è¡Œçš„åˆ†ç±»å’Œå˜å¼‚æµ‹è¯•è¡¨æ˜ï¼Œè®¸å¤šå¤±è´¥çš„æµ‹è¯•æºäºè™šæ„çš„ä¾èµ–å…³ç³»ï¼Œé™åˆ¶äº†æœ‰æ•ˆçš„æ•…éšœæ£€æµ‹ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2407.00225v3">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ç ”ç©¶äº†LLMæ¨¡å‹åœ¨ç”Ÿæˆå•å…ƒæµ‹è¯•æ–¹é¢çš„æ€§èƒ½ï¼Œå¯¹å››ç§å…ˆè¿›æ¨¡å‹è¿›è¡Œäº†å¤§è§„æ¨¡å®è¯è¯„ä¼°ï¼Œæ¢è®¨äº†äº”ç§æç¤ºæŠ€æœ¯ï¼Œå¹¶å‘ç°åŸºäºæ¨ç†çš„æç¤ºæŠ€æœ¯èƒ½æ˜¾è‘—æé«˜æµ‹è¯•å¯é æ€§ã€ç¼–è¯‘èƒ½åŠ›å’Œç»“æ„éµå¾ªæ€§ã€‚ç„¶è€Œï¼Œå¹»æƒ³é©±åŠ¨æ•…éšœä»ç„¶æ˜¯ä¸€ä¸ªæŒç»­å­˜åœ¨çš„æŒ‘æˆ˜ï¼Œå¯¼è‡´é«˜ç¼–è¯‘å¤±è´¥ç‡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLMæ¨¡å‹åœ¨ç”Ÿæˆå•å…ƒæµ‹è¯•æ—¶æ˜¾ç¤ºå‡ºæ½œåŠ›ï¼Œä½†å­˜åœ¨æŒ‘æˆ˜ã€‚</li>
<li>å¯¹å››ç§å…ˆè¿›çš„LLMæ¨¡å‹è¿›è¡Œäº†å¤§è§„æ¨¡å®è¯è¯„ä¼°ã€‚</li>
<li>æ¢è®¨äº†äº”ç§æç¤ºæŠ€æœ¯ï¼ŒåŒ…æ‹¬Zero-Shot Learningã€Few-Shot Learningç­‰ã€‚</li>
<li>åŸºäºæ¨ç†çš„æç¤ºæŠ€æœ¯ï¼ˆå¦‚GToTï¼‰èƒ½æé«˜æµ‹è¯•å¯é æ€§ã€ç¼–è¯‘èƒ½åŠ›å’Œç»“æ„éµå¾ªæ€§ã€‚</li>
<li>å¹»æƒ³é©±åŠ¨æ•…éšœæ˜¯ä¸€ä¸ªæŒç»­å­˜åœ¨çš„æŒ‘æˆ˜ï¼Œå¯¼è‡´é«˜ç¼–è¯‘å¤±è´¥ç‡ã€‚</li>
<li>æ‰§è¡Œåˆ†ç±»å’Œçªå˜æµ‹è¯•è¡¨æ˜ï¼Œè®¸å¤šå¤±è´¥çš„æµ‹è¯•æºäºå¹»æƒ³ä¾èµ–ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2407.00225">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-bc7cb0998d6905a1309572c809afdda3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-906bac14528a46dec7d84a10af913263.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="On-the-Utility-of-Domain-Adjacent-Fine-Tuned-Model-Ensembles-for-Few-shot-Problems"><a href="#On-the-Utility-of-Domain-Adjacent-Fine-Tuned-Model-Ensembles-for-Few-shot-Problems" class="headerlink" title="On the Utility of Domain-Adjacent Fine-Tuned Model Ensembles for   Few-shot Problems"></a>On the Utility of Domain-Adjacent Fine-Tuned Model Ensembles for   Few-shot Problems</h2><p><strong>Authors:Md Ibrahim Ibne Alam, Parikshit Ram, Soham Dan, Horst Samulowitz, Koushik Kar</strong></p>
<p>Large Language Models (LLMs) have been observed to perform well on a wide range of downstream tasks when fine-tuned on domain-specific data. However, such data may not be readily available in many applications, motivating zero-shot or few-shot approaches using domain-adjacent models. While several fine-tuned models for various tasks are available, finding an appropriate domain-adjacent model for a given task is often not straight forward. In this paper, we study DAFT-E, a framework that utilizes an Ensemble of Domain-Adjacent Fine-Tuned Foundation Models for few-shot problems. We show that for zero-shot problems, this ensembling method provides an accuracy performance close to that of the single best model. With few-shot problems, this performance improves further, at which point DEFT-E can outperform any single domain-adjacent model while requiring much less data for domain-specific fine-tuning. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨é’ˆå¯¹ç‰¹å®šé¢†åŸŸæ•°æ®è¿›è¡Œå¾®è°ƒåï¼Œåœ¨å¤šç§ä¸‹æ¸¸ä»»åŠ¡ä¸Šè¡¨ç°å‡ºè‰¯å¥½çš„æ€§èƒ½ã€‚ç„¶è€Œï¼Œåœ¨è®¸å¤šåº”ç”¨ä¸­å¯èƒ½æ— æ³•è½»æ¾è·å¾—æ­¤ç±»æ•°æ®ï¼Œè¿™ä¿ƒä½¿äººä»¬é‡‡ç”¨ä½¿ç”¨é¢†åŸŸç›¸é‚»æ¨¡å‹çš„é›¶æ ·æœ¬æˆ–å°‘æ ·æœ¬æ–¹æ³•ã€‚è™½ç„¶é’ˆå¯¹å„ç§ä»»åŠ¡çš„å¤šä¸ªå¾®è°ƒæ¨¡å‹æ˜¯å¯ç”¨çš„ï¼Œä½†ä¸ºç»™å®šä»»åŠ¡æ‰¾åˆ°é€‚å½“çš„é¢†åŸŸç›¸é‚»æ¨¡å‹é€šå¸¸å¹¶ä¸ç®€å•ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬ç ”ç©¶äº†DAFT-Eï¼Œè¿™æ˜¯ä¸€ä¸ªåˆ©ç”¨é¢†åŸŸç›¸é‚»å¾®è°ƒåŸºç¡€æ¨¡å‹çš„é›†åˆæ¥è§£å†³å°‘æ ·æœ¬é—®é¢˜çš„æ¡†æ¶ã€‚æˆ‘ä»¬è¡¨æ˜ï¼Œå¯¹äºé›¶æ ·æœ¬é—®é¢˜ï¼Œè¿™ç§é›†æˆæ–¹æ³•æä¾›çš„å‡†ç¡®æ€§è¡¨ç°æ¥è¿‘å•ä¸ªæœ€ä½³æ¨¡å‹ã€‚å¯¹äºå°‘æ ·æœ¬é—®é¢˜ï¼Œæ€§èƒ½ä¼šè¿›ä¸€æ­¥æé«˜ï¼Œæ­¤æ—¶DEFT-Eå¯ä»¥è¶…è¶Šä»»ä½•å•ä¸ªé¢†åŸŸç›¸é‚»æ¨¡å‹ï¼ŒåŒæ—¶éœ€è¦æ›´å°‘çš„æ•°æ®è¿›è¡Œé¢†åŸŸç‰¹å®šå¾®è°ƒã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2406.13720v2">PDF</a> Main paper is 14 pages, followed by references and appendix</p>
<p><strong>Summary</strong></p>
<p>å¤§è§„æ¨¡è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨ç‰¹å®šé¢†åŸŸæ•°æ®ä¸Šå¾®è°ƒåï¼Œèƒ½å¤Ÿåœ¨å„ç§ä¸‹æ¸¸ä»»åŠ¡ä¸Šè¡¨ç°è‰¯å¥½ã€‚ä½†åœ¨è®¸å¤šåº”ç”¨ä¸­ï¼Œæ­¤ç±»æ•°æ®å¯èƒ½ä¸æ˜“è·å¾—ï¼Œå› æ­¤æå€¡ä½¿ç”¨ä¸é¢†åŸŸç›¸é‚»çš„æ¨¡å‹è¿›è¡Œé›¶æ ·æœ¬æˆ–å°æ ·æœ¬æ–¹æ³•ã€‚å°½ç®¡å­˜åœ¨å¤šç§é’ˆå¯¹å„ç§ä»»åŠ¡çš„å¾®è°ƒæ¨¡å‹ï¼Œä½†æ‰¾åˆ°åˆé€‚çš„ä¸ç»™å®šä»»åŠ¡ç›¸é‚»çš„æ¨¡å‹å¹¶ä¸æ€»æ˜¯ç›´æ¥æ˜äº†ã€‚æœ¬æ–‡ç ”ç©¶äº†DAFT-Eæ¡†æ¶ï¼Œå®ƒåˆ©ç”¨ä¸€ç»„ä¸é¢†åŸŸç›¸é‚»çš„å¾®è°ƒåŸºç¡€æ¨¡å‹ï¼ˆEnsembleï¼‰æ¥è§£å†³å°æ ·æœ¬é—®é¢˜ã€‚æˆ‘ä»¬è¡¨æ˜ï¼Œå¯¹äºé›¶æ ·æœ¬é—®é¢˜ï¼Œè¿™ç§é›†æˆæ–¹æ³•æä¾›çš„å‡†ç¡®ç‡æ€§èƒ½æ¥è¿‘å•ä¸€æœ€ä½³æ¨¡å‹ã€‚åœ¨å°æ ·æœ¬é—®é¢˜çš„æƒ…å†µä¸‹ï¼Œæ€§èƒ½è¿›ä¸€æ­¥æé«˜ï¼Œæ­¤æ—¶DEFT-Eå¯ä»¥è¶…è¶Šä»»ä½•å•ä¸€çš„é¢†åŸŸç›¸é‚»æ¨¡å‹ï¼ŒåŒæ—¶éœ€è¦æ›´å°‘çš„æ•°æ®è¿›è¡Œé¢†åŸŸç‰¹å®šå¾®è°ƒã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLMsåœ¨ç‰¹å®šé¢†åŸŸæ•°æ®ä¸Šå¾®è°ƒåï¼Œåœ¨å¤šç§ä¸‹æ¸¸ä»»åŠ¡ä¸Šè¡¨ç°è‰¯å¥½ã€‚</li>
<li>åœ¨è®¸å¤šåº”ç”¨ä¸­ï¼Œç‰¹å®šé¢†åŸŸæ•°æ®å¯èƒ½éš¾ä»¥è·å–ï¼Œå› æ­¤æå€¡ä½¿ç”¨é›¶æ ·æœ¬æˆ–å°æ ·æœ¬æ–¹æ³•ä¸é¢†åŸŸç›¸é‚»çš„æ¨¡å‹ã€‚</li>
<li>æ‰¾åˆ°ä¸ç»™å®šä»»åŠ¡åˆé€‚çš„é¢†åŸŸç›¸é‚»æ¨¡å‹å¹¶ä¸æ€»æ˜¯ç›´æ¥æ˜äº†ã€‚</li>
<li>DAFT-Eæ¡†æ¶åˆ©ç”¨ä¸€ç»„ä¸é¢†åŸŸç›¸é‚»çš„å¾®è°ƒåŸºç¡€æ¨¡å‹ï¼ˆEnsembleï¼‰æ¥è§£å†³å°æ ·æœ¬é—®é¢˜ã€‚</li>
<li>å¯¹äºé›¶æ ·æœ¬é—®é¢˜ï¼ŒDAFT-Eé›†æˆæ–¹æ³•çš„å‡†ç¡®ç‡æ€§èƒ½æ¥è¿‘å•ä¸€æœ€ä½³æ¨¡å‹ã€‚</li>
<li>åœ¨å°æ ·æœ¬é—®é¢˜çš„æƒ…å†µä¸‹ï¼ŒDAFT-Eæ€§èƒ½ä¼˜äºå•ä¸€é¢†åŸŸç›¸é‚»æ¨¡å‹ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2406.13720">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-c0b51be4db3e4c8e0aed56104360e894.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-ea969ce64736743848ed40a98d6cccd7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-cbfeb5646c4011c48924e41eea6906a4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-704a770fa4ad280adfe1d35a814fa409.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-09f8d9a607dc9b7ba6fc261bc883a601.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-07-09/Few-Shot/">https://kedreamix.github.io/Talk2Paper/Paper/2025-07-09/Few-Shot/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Few-Shot/">
                                    <span class="chip bg-color">Few-Shot</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-07-09/I2I%20Translation/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-14e8cfa060ec00336da46edd7995f14c.jpg" class="responsive-img" alt="I2I Translation">
                        
                        <span class="card-title">I2I Translation</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            I2I Translation æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-07-09  Music2Palette Emotion-aligned Color Palette Generation via Cross-Modal   Representation Learning
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-07-09
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/I2I-Translation/" class="post-category">
                                    I2I Translation
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/I2I-Translation/">
                        <span class="chip bg-color">I2I Translation</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-07-09/Agent/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-4c5851f0c5fba71136fdaee90cf924fd.jpg" class="responsive-img" alt="Agent">
                        
                        <span class="card-title">Agent</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Agent æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-07-09  Modeling Latent Partner Strategies for Adaptive Zero-Shot Human-Agent   Collaboration
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-07-09
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Agent/" class="post-category">
                                    Agent
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Agent/">
                        <span class="chip bg-color">Agent</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">30341.4k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
