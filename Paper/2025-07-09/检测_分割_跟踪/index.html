<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="检测/分割/跟踪">
    <meta name="description" content="检测/分割/跟踪 方向最新论文已更新，请持续关注 Update in 2025-07-09  CVFusion Cross-View Fusion of 4D Radar and Camera for 3D Object   Detection">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>检测/分割/跟踪 | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loader页面消失采用渐隐的方式*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logo出现动画 */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//使用渐隐的方法淡出loading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//强制显示loading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">嘘~  正在从服务器偷取页面 . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>首页</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>标签</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>分类</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>归档</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="搜索" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="深色/浅色模式" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			首页
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			标签
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			分类
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			归档
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-597ea0e761098ab072596ed0b825dc8b.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">检测/分割/跟踪</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- 文章内容详情 -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/%E6%A3%80%E6%B5%8B-%E5%88%86%E5%89%B2-%E8%B7%9F%E8%B8%AA/">
                                <span class="chip bg-color">检测/分割/跟踪</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/%E6%A3%80%E6%B5%8B-%E5%88%86%E5%89%B2-%E8%B7%9F%E8%B8%AA/" class="post-category">
                                检测/分割/跟踪
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp;
                    2025-07-09
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>更新日期:&nbsp;&nbsp;
                    2025-07-17
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>文章字数:&nbsp;&nbsp;
                    8.2k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>阅读时长:&nbsp;&nbsp;
                    33 分
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>阅读次数:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>⚠️ 以下所有内容总结都来自于 大语言模型的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p>
</blockquote>
<h1 id="2025-07-09-更新"><a href="#2025-07-09-更新" class="headerlink" title="2025-07-09 更新"></a>2025-07-09 更新</h1><h2 id="CVFusion-Cross-View-Fusion-of-4D-Radar-and-Camera-for-3D-Object-Detection"><a href="#CVFusion-Cross-View-Fusion-of-4D-Radar-and-Camera-for-3D-Object-Detection" class="headerlink" title="CVFusion: Cross-View Fusion of 4D Radar and Camera for 3D Object   Detection"></a>CVFusion: Cross-View Fusion of 4D Radar and Camera for 3D Object   Detection</h2><p><strong>Authors:Hanzhi Zhong, Zhiyu Xiang, Ruoyu Xu, Jingyun Fu, Peng Xu, Shaohong Wang, Zhihao Yang, Tianyu Pu, Eryun Liu</strong></p>
<p>4D radar has received significant attention in autonomous driving thanks to its robustness under adverse weathers. Due to the sparse points and noisy measurements of the 4D radar, most of the research finish the 3D object detection task by integrating images from camera and perform modality fusion in BEV space. However, the potential of the radar and the fusion mechanism is still largely unexplored, hindering the performance improvement. In this study, we propose a cross-view two-stage fusion network called CVFusion. In the first stage, we design a radar guided iterative (RGIter) BEV fusion module to generate high-recall 3D proposal boxes. In the second stage, we aggregate features from multiple heterogeneous views including points, image, and BEV for each proposal. These comprehensive instance level features greatly help refine the proposals and generate high-quality predictions. Extensive experiments on public datasets show that our method outperforms the previous state-of-the-art methods by a large margin, with 9.10% and 3.68% mAP improvements on View-of-Delft (VoD) and TJ4DRadSet, respectively. Our code will be made publicly available. </p>
<blockquote>
<p>4D雷达由于其恶劣天气下的稳健性而在自动驾驶领域备受关注。由于4D雷达的点数稀疏和测量噪声，大多数研究通过整合相机图像并在BEV空间进行模态融合来完成3D目标检测任务。然而，雷达的潜力和融合机制尚未得到充分探索，阻碍了性能的提升。在本研究中，我们提出了一种跨视图两阶段融合网络，名为CVFusion。在第一阶段，我们设计了雷达引导迭代（RGIter）BEV融合模块，以生成高召回率的3D候选框。在第二阶段，我们从多个异构视图（包括点、图像和BEV）为每个候选框聚合特征。这些全面的实例级特征有助于改进候选框并生成高质量预测。在公开数据集上的大量实验表明，我们的方法大幅超越了以前的最先进方法，在Delft视角（VoD）和TJ4DRadSet上的mAP分别提高了9.10%和3.68%。我们的代码将公开提供。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.04587v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>这是一篇关于4D雷达在自动驾驶中应用的论文摘要。文章介绍了当前自主驾驶中对4D雷达的研究现状及其面临的主要挑战。研究提出了一种名为CVFusion的跨视角两阶段融合网络，旨在通过雷达引导迭代（RGIter）BEV融合模块生成高质量的三维提议框，并通过多视角特征聚合进一步提高预测精度。实验结果表明，该方法在公共数据集上的性能优于先前的方法，特别是在VoD和TJ4DRadSet数据集上的mAP指标有明显提升。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>4D雷达在自动驾驶中受到广泛关注，因其恶劣天气下的稳健性。</li>
<li>研究提出了CVFusion网络，这是一种跨视角两阶段融合网络。</li>
<li>第一阶段设计了一个雷达引导迭代（RGIter）BEV融合模块，用于生成高质量的三维提议框。</li>
<li>第二阶段从不同视角（点、图像和BEV）聚合特征，以优化提议并生成高质量预测。</li>
<li>该方法在公共数据集上的性能显著优于先前的方法。</li>
<li>在VoD和TJ4DRadSet数据集上的mAP指标有明显提升。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.04587">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-d7fba0b977511079458b7483b10df00d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b1ac13800172bd7eb36eefb3bf7c27f2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3777948e88d7731f091085ab0c4323ea.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-f3239d359fb677057af6beb39f386916.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Towards-Accurate-and-Efficient-3D-Object-Detection-for-Autonomous-Driving-A-Mixture-of-Experts-Computing-System-on-Edge"><a href="#Towards-Accurate-and-Efficient-3D-Object-Detection-for-Autonomous-Driving-A-Mixture-of-Experts-Computing-System-on-Edge" class="headerlink" title="Towards Accurate and Efficient 3D Object Detection for Autonomous   Driving: A Mixture of Experts Computing System on Edge"></a>Towards Accurate and Efficient 3D Object Detection for Autonomous   Driving: A Mixture of Experts Computing System on Edge</h2><p><strong>Authors:Linshen Liu, Boyan Su, Junyue Jiang, Guanlin Wu, Cong Guo, Ceyu Xu, Hao Frank Yang</strong></p>
<p>This paper presents Edge-based Mixture of Experts (MoE) Collaborative Computing (EMC2), an optimal computing system designed for autonomous vehicles (AVs) that simultaneously achieves low-latency and high-accuracy 3D object detection. Unlike conventional approaches, EMC2 incorporates a scenario-aware MoE architecture specifically optimized for edge platforms. By effectively fusing LiDAR and camera data, the system leverages the complementary strengths of sparse 3D point clouds and dense 2D images to generate robust multimodal representations. To enable this, EMC2 employs an adaptive multimodal data bridge that performs multi-scale preprocessing on sensor inputs, followed by a scenario-aware routing mechanism that dynamically dispatches features to dedicated expert models based on object visibility and distance. In addition, EMC2 integrates joint hardware-software optimizations, including hardware resource utilization optimization and computational graph simplification, to ensure efficient and real-time inference on resource-constrained edge devices. Experiments on open-source benchmarks clearly show the EMC2 advancements as a end-to-end system. On the KITTI dataset, it achieves an average accuracy improvement of 3.58% and a 159.06% inference speedup compared to 15 baseline methods on Jetson platforms, with similar performance gains on the nuScenes dataset, highlighting its capability to advance reliable, real-time 3D object detection tasks for AVs. </p>
<blockquote>
<p>本文提出了基于边缘计算的混合专家（MoE）协同计算（EMC2）系统。这是一种为自动驾驶车辆设计的最佳计算系统，能够同时实现低延迟和高精度的3D目标检测。不同于传统方法，EMC2采用了一种针对边缘平台的情景感知MoE架构。通过有效地融合激光雷达和相机数据，该系统利用稀疏的3D点云和密集的2D图像的互补优势，生成稳健的多模态表示。为此，EMC2采用自适应多模态数据桥，对传感器输入进行多尺度预处理，然后通过情景感知路由机制，根据目标可见性和距离动态将特征派发到专用专家模型。此外，EMC2集成了联合硬件软件优化，包括硬件资源利用优化和计算图简化，以确保在资源受限的边缘设备上实现高效实时推理。在公开基准测试上的实验清楚地表明了EMC2作为端到端系统的优势。在KITTI数据集上，与Jetson平台上的15种基准方法相比，它的平均精度提高了3.58%，推理速度提高了159.06%，在nuScenes数据集上也取得了类似的性能提升，这突显了其在推动自动驾驶车辆可靠实时3D目标检测任务方面的能力。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.04123v1">PDF</a> Accepted at ICCV 2025</p>
<p><strong>Summary</strong></p>
<p>该论文提出了一种面向自动驾驶汽车的边缘计算系统——边缘基础混合专家协同计算系统（EMC2）。系统实现了低延迟高准确度的3D物体检测，结合了场景感知的混合专家架构和边缘平台优化技术。通过融合激光雷达和相机数据，系统利用稀疏的3D点云和密集的2D图像生成稳健的多模态表示。实验表明，与基准方法相比，在KITTI数据集上平均精度提高了3.58%，推理速度提高了159.06%，展示了其在自动驾驶车辆中的可靠实时3D物体检测能力。</p>
<p><strong>Key Takeaways</strong></p>
<p>以下是文本的主要观点摘要：</p>
<ul>
<li>EMC2是一个针对自动驾驶汽车的优化计算系统，实现了低延迟和高精度的3D物体检测。</li>
<li>系统结合了场景感知的混合专家架构和边缘平台优化技术。</li>
<li>通过融合激光雷达和相机数据，生成稳健的多模态表示。</li>
<li>EMC2采用自适应多模态数据桥和场景感知路由机制，根据物体可见性和距离动态调度特征到专家模型。</li>
<li>系统集成了硬件和软件优化，包括硬件资源利用优化和计算图简化，确保在资源受限的边缘设备上实现高效实时推理。</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.04123">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-1a2a13b0a7287c5b66e6b60ca99f2fc7.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-48b28c8327a2f4073f0b6ac6a5f8da52.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-5dee352aee7a6c1fb770fd9c172e4766.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ef92d1030d276492e653627732835095.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-7cd925cb9a4df213d10b363412ac697d.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="2-5D-Object-Detection-for-Intelligent-Roadside-Infrastructure"><a href="#2-5D-Object-Detection-for-Intelligent-Roadside-Infrastructure" class="headerlink" title="2.5D Object Detection for Intelligent Roadside Infrastructure"></a>2.5D Object Detection for Intelligent Roadside Infrastructure</h2><p><strong>Authors:Nikolai Polley, Yacin Boualili, Ferdinand Mütsch, Maximilian Zipfl, Tobias Fleck, J. Marius Zöllner</strong></p>
<p>On-board sensors of autonomous vehicles can be obstructed, occluded, or limited by restricted fields of view, complicating downstream driving decisions. Intelligent roadside infrastructure perception systems, installed at elevated vantage points, can provide wide, unobstructed intersection coverage, supplying a complementary information stream to autonomous vehicles via vehicle-to-everything (V2X) communication. However, conventional 3D object-detection algorithms struggle to generalize under the domain shift introduced by top-down perspectives and steep camera angles. We introduce a 2.5D object detection framework, tailored specifically for infrastructure roadside-mounted cameras. Unlike conventional 2D or 3D object detection, we employ a prediction approach to detect ground planes of vehicles as parallelograms in the image frame. The parallelogram preserves the planar position, size, and orientation of objects while omitting their height, which is unnecessary for most downstream applications. For training, a mix of real-world and synthetically generated scenes is leveraged. We evaluate generalizability on a held-out camera viewpoint and in adverse-weather scenarios absent from the training set. Our results show high detection accuracy, strong cross-viewpoint generalization, and robustness to diverse lighting and weather conditions. Model weights and inference code are provided at: <a target="_blank" rel="noopener" href="https://gitlab.kit.edu/kit/aifb/ATKS/public/digit4taf/2.5d-object-detection">https://gitlab.kit.edu/kit/aifb/ATKS/public/digit4taf/2.5d-object-detection</a> </p>
<blockquote>
<p>自动驾驶车辆的车载传感器可能会受到遮挡、视野受限或视角变化的影响，导致下游驾驶决策复杂化。安装在较高有利位置的智能路边基础设施感知系统可以提供宽阔、无遮挡的交叉路口覆盖范围，并通过车对万物（V2X）通信为自动驾驶车辆提供补充信息流。然而，传统的3D目标检测算法在由上而下的视角和陡峭的相机角度引起的领域变化下难以通用化。我们引入了一个专门针对基础设施路边安装的相机设计的2.5D目标检测框架。与传统的二维或三维目标检测不同，我们采用预测方法来检测图像帧中的车辆地面平面作为平行四边形。平行四边形保留了物体的平面位置、大小和方位，同时省略了高度信息，这在大多数下游应用中是不必要的。为了训练模型，我们结合了现实场景和合成场景。我们在未参与训练的相机视角和恶劣天气场景下评估了模型的通用性。我们的结果显示出较高的检测精度、较强的跨视角通用性以及适应不同光照和天气条件的稳健性。模型权重和推理代码可通过以下链接获取：<a target="_blank" rel="noopener" href="https://gitlab.kit.edu/kit/aifb/ATKS/public/digit4taf/2.5d-object-detection">https://gitlab.kit.edu/kit/aifb/ATKS/public/digit4taf/2.5d-object-detection</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.03564v1">PDF</a> Accepted at 2025 IEEE 28th International Conference on Intelligent   Transportation Systems (ITSC)</p>
<p><strong>Summary</strong>：<br>自主驾驶车辆的车载传感器可能受到遮挡、视野受限的影响，影响下游驾驶决策。为解决这一问题，研究者引入了智能路边感知系统，该系统可从高处监控交叉口并为自主车辆提供补充信息。然而，传统的三维物体检测算法难以应对由上而下的视角和陡峭相机角度带来的领域偏移。研究者提出一种适用于路边安装的2.5维物体检测框架，采用预测方法检测车辆的地面平面作为图像帧中的平行四边形。此框架在忽略不必要的高度信息的同时，保留了物体的平面位置、大小和方位。通过结合真实场景和合成场景进行训练，该框架具有良好的检测精度、跨视角的泛化能力以及适应各种照明和天气条件的稳健性。</p>
<p><strong>Key Takeaways</strong>：</p>
<ol>
<li>自主驾驶车辆的车载传感器可能受到遮挡或视野限制的影响。</li>
<li>智能路边基础设施感知系统可以提供广泛的、无遮挡的交叉口覆盖，为自主车辆提供补充信息。</li>
<li>传统三维物体检测算法难以适应由上而下的视角和陡峭相机角度带来的变化。</li>
<li>引入了一种2.5维物体检测框架，通过预测车辆地面平面为平行四边形进行检测。</li>
<li>该框架忽略物体高度信息，同时保留平面位置、大小和方位。</li>
<li>结合真实和合成场景进行训练，具有良好的检测精度和泛化能力。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.03564">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-3cf80026a59dcb2992e48afd4e363676.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-dd2ec2283d5459a8d6137ec966211fb7.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-93487c7daf065a2a9659fd0db5ee151b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-433052a0433b464385ce31e5b34be465.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-836262d3fff973c60180c7a6ed664cb9.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b09125ad74428e4e45e428beb885024f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b8a5a360eb0e919aa8e7f88c3cfa5205.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="No-time-to-train-Training-Free-Reference-Based-Instance-Segmentation"><a href="#No-time-to-train-Training-Free-Reference-Based-Instance-Segmentation" class="headerlink" title="No time to train! Training-Free Reference-Based Instance Segmentation"></a>No time to train! Training-Free Reference-Based Instance Segmentation</h2><p><strong>Authors:Miguel Espinosa, Chenhongyi Yang, Linus Ericsson, Steven McDonagh, Elliot J. Crowley</strong></p>
<p>The performance of image segmentation models has historically been constrained by the high cost of collecting large-scale annotated data. The Segment Anything Model (SAM) alleviates this original problem through a promptable, semantics-agnostic, segmentation paradigm and yet still requires manual visual-prompts or complex domain-dependent prompt-generation rules to process a new image. Towards reducing this new burden, our work investigates the task of object segmentation when provided with, alternatively, only a small set of reference images. Our key insight is to leverage strong semantic priors, as learned by foundation models, to identify corresponding regions between a reference and a target image. We find that correspondences enable automatic generation of instance-level segmentation masks for downstream tasks and instantiate our ideas via a multi-stage, training-free method incorporating (1) memory bank construction; (2) representation aggregation and (3) semantic-aware feature matching. Our experiments show significant improvements on segmentation metrics, leading to state-of-the-art performance on COCO FSOD (36.8% nAP), PASCAL VOC Few-Shot (71.2% nAP50) and outperforming existing training-free approaches on the Cross-Domain FSOD benchmark (22.4% nAP). </p>
<blockquote>
<p>历史上，图像分割模型的性能一直受到收集大规模标注数据的高成本的限制。Segment Anything Model（SAM）通过一种可提示的、与语义无关的分割范式缓解了这一原始问题，但仍然需要手动视觉提示或复杂的域相关提示生成规则来处理新图像。为了减轻这一新负担，我们的工作研究了在仅提供一小部分参考图像的情况下进行对象分割的任务。我们的关键见解是利用基础模型学到的强大语义先验知识，来识别参考图像和目标图像之间的相应区域。我们发现这种对应关系能够自动生成用于下游任务的实例级分割掩膜，并通过一个多阶段、无需训练的方法实现我们的想法，包括（1）构建内存银行；（2）表示聚合和（3）语义感知特征匹配。我们的实验显示在分割指标上取得了显著改进，并在COCO FSOD（36.8% nAP）、PASCAL VOC Few-Shot（71.2% nAP50）上达到了最先进的性能，并且在跨域FSOD基准测试上超越了现有的无训练方法（22.4% nAP）。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.02798v2">PDF</a> Preprint</p>
<p><strong>Summary</strong></p>
<p>图像分割模型的性能长期受到大规模标注数据收集成本高昂的制约。Segment Anything Model（SAM）通过可提示的、语义无关的分割范式缓解了这一问题，但仍需手动视觉提示或复杂的领域相关提示生成规则来处理新图像。为减轻这一新负担，我们的工作研究在仅提供少量参考图像的情况下进行目标分割的任务。我们的关键见解是利用基础模型学习的强语义先验知识，识别参考图像和目标图像之间的对应区域。我们发现这种对应关系能够自动生成用于下游任务的实例级分割掩膜，并通过一个包含（1）内存库构建、（2）表示聚合和（3）语义感知特征匹配的、无需训练的多阶段方法实现了我们的想法。实验表明，该方法在分割指标上取得了显著改进，并在COCO FSOD（36.8% nAP）、PASCAL VOC少数镜头（71.2% nAP50）上达到了最先进的性能，且在跨域FSOD基准测试上优于现有的无需训练的方法（22.4% nAP）。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>图像分割模型长期受大规模标注数据制约。</li>
<li>Segment Anything Model（SAM）引入可提示的分割范式，但仍需复杂提示操作。</li>
<li>研究在仅提供少量参考图像情况下的目标分割任务。</li>
<li>利用基础模型的强语义先验知识识别参考图像与目标图像间的对应区域。</li>
<li>对应关系能自动生成实例级分割掩膜，用于下游任务。</li>
<li>提出一个无需训练的多阶段方法，包括内存库构建、表示聚合和语义感知特征匹配。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.02798">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-3978c1c075fc37ae0437a0e8189b5766.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-60929355fb8a4bad99c25d2e219e5af4.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-34ef610aa985554711ae9ea98c2158ba.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-07b1ec1b74b110fec2dba51c7bb4fd11.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="CRISP-SAM2-SAM2-with-Cross-Modal-Interaction-and-Semantic-Prompting-for-Multi-Organ-Segmentation"><a href="#CRISP-SAM2-SAM2-with-Cross-Modal-Interaction-and-Semantic-Prompting-for-Multi-Organ-Segmentation" class="headerlink" title="CRISP-SAM2: SAM2 with Cross-Modal Interaction and Semantic Prompting for   Multi-Organ Segmentation"></a>CRISP-SAM2: SAM2 with Cross-Modal Interaction and Semantic Prompting for   Multi-Organ Segmentation</h2><p><strong>Authors:Xinlei Yu, Changmiao Wang, Hui Jin, Ahmed Elazab, Gangyong Jia, Xiang Wan, Changqing Zou, Ruiquan Ge</strong></p>
<p>Multi-organ medical segmentation is a crucial component of medical image processing, essential for doctors to make accurate diagnoses and develop effective treatment plans. Despite significant progress in this field, current multi-organ segmentation models often suffer from inaccurate details, dependence on geometric prompts and loss of spatial information. Addressing these challenges, we introduce a novel model named CRISP-SAM2 with CRoss-modal Interaction and Semantic Prompting based on SAM2. This model represents a promising approach to multi-organ medical segmentation guided by textual descriptions of organs. Our method begins by converting visual and textual inputs into cross-modal contextualized semantics using a progressive cross-attention interaction mechanism. These semantics are then injected into the image encoder to enhance the detailed understanding of visual information. To eliminate reliance on geometric prompts, we use a semantic prompting strategy, replacing the original prompt encoder to sharpen the perception of challenging targets. In addition, a similarity-sorting self-updating strategy for memory and a mask-refining process is applied to further adapt to medical imaging and enhance localized details. Comparative experiments conducted on seven public datasets indicate that CRISP-SAM2 outperforms existing models. Extensive analysis also demonstrates the effectiveness of our method, thereby confirming its superior performance, especially in addressing the limitations mentioned earlier. Our code is available at: <a target="_blank" rel="noopener" href="https://github.com/YU-deep/CRISP_SAM2.git">https://github.com/YU-deep/CRISP_SAM2.git</a>. </p>
<blockquote>
<p>医学多器官分割是医学图像处理的重要组成部分，对于医生进行准确的诊断和制定有效的治疗方案至关重要。尽管该领域取得了重大进展，但现有的多器官分割模型往往存在细节不准确、依赖几何提示以及空间信息丢失等问题。为了解决这些挑战，我们引入了一种名为CRISP-SAM2的新模型，该模型基于SAM2具有跨模态交互和语义提示。该模型是一种很有前途的方法，可以通过器官的文本描述来进行多器官医学分割。我们的方法首先通过渐进的交叉注意力交互机制将视觉和文本输入转换为跨模态上下文语义。然后，这些语义被注入图像编码器，以增强对视觉信息的详细理解。为了减少对几何提示的依赖，我们采用了一种语义提示策略，以替代原始提示编码器，提高了对具有挑战性的目标的感知能力。此外，还采用了相似度排序的自更新策略对内存进行更新和掩膜细化过程，进一步适应医学影像并增强局部细节。在七个公共数据集上进行的对比实验表明，CRISP-SAM2优于现有模型。广泛的分析也证明了我们的方法的有效性，从而证实了其卓越性能，特别是在解决上述提到的局限性方面。我们的代码可在：<a target="_blank" rel="noopener" href="https://github.com/YU-deep/CRISP_SAM2.git%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/YU-deep/CRISP_SAM2.git获取。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.23121v2">PDF</a> Accepted By ACMMM25</p>
<p><strong>Summary</strong></p>
<p>本文介绍了一种名为CRISP-SAM2的多器官医学分割模型，该模型通过跨模态交互和语义提示技术解决了当前模型在细节准确性、几何提示依赖性和空间信息损失等方面的问题。CRISP-SAM2模型通过将视觉和文本输入转换为跨模态上下文语义，并注入图像编码器以增强对视觉信息的详细理解。此外，采用语义提示策略消除对几何提示的依赖，并采用相似度排序自更新策略和掩膜细化过程进一步适应医学成像并增强局部细节。在七个公共数据集上的对比实验表明，CRISP-SAM2模型优于现有模型。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>多器官医学分割是医疗图像处理中的关键部分，对于医生的准确诊断和治疗计划制定至关重要。</li>
<li>当前的多器官分割模型存在细节不准确、依赖几何提示和损失空间信息等问题。</li>
<li>引入的CRISP-SAM2模型通过跨模态交互和语义提示技术解决上述问题。</li>
<li>CRISP-SAM2将视觉和文本输入转换为跨模态上下文语义，提高视觉信息的理解。</li>
<li>语义提示策略消除对几何提示的依赖，并采用相似度排序自更新策略和掩膜细化过程来提高模型的性能。</li>
<li>在七个公共数据集上的对比实验证明CRISP-SAM2模型优于现有模型。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.23121">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-57da5d89a7496d16c96039457ca0792f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d55eabbad866383adbd05223597f84e0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e6595d78001f04a7058b24c04f7c1ab2.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-849ef97942faad6a6f3872cd1c9d9fc6.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Pillar-Voxel-Fusion-Network-for-3D-Object-Detection-in-Airborne-Hyperspectral-Point-Clouds"><a href="#Pillar-Voxel-Fusion-Network-for-3D-Object-Detection-in-Airborne-Hyperspectral-Point-Clouds" class="headerlink" title="Pillar-Voxel Fusion Network for 3D Object Detection in Airborne   Hyperspectral Point Clouds"></a>Pillar-Voxel Fusion Network for 3D Object Detection in Airborne   Hyperspectral Point Clouds</h2><p><strong>Authors:Yanze Jiang, Yanfeng Gu, Xian Li</strong></p>
<p>Hyperspectral point clouds (HPCs) can simultaneously characterize 3D spatial and spectral information of ground objects, offering excellent 3D perception and target recognition capabilities. Current approaches for generating HPCs often involve fusion techniques with hyperspectral images and LiDAR point clouds, which inevitably lead to geometric-spectral distortions due to fusion errors and obstacle occlusions. These adverse effects limit their performance in downstream fine-grained tasks across multiple scenarios, particularly in airborne applications. To address these issues, we propose PiV-AHPC, a 3D object detection network for airborne HPCs. To the best of our knowledge, this is the first attempt at this HPCs task. Specifically, we first develop a pillar-voxel dual-branch encoder, where the former captures spectral and vertical structural features from HPCs to overcome spectral distortion, while the latter emphasizes extracting accurate 3D spatial features from point clouds. A multi-level feature fusion mechanism is devised to enhance information interaction between the two branches, achieving neighborhood feature alignment and channel-adaptive selection, thereby organically integrating heterogeneous features and mitigating geometric distortion. Extensive experiments on two airborne HPCs datasets demonstrate that PiV-AHPC possesses state-of-the-art detection performance and high generalization capability. </p>
<blockquote>
<p>高光谱点云（HPC）可以同时表征地面物体的3D空间和光谱信息，提供出色的3D感知和目标识别能力。目前生成HPC的方法通常涉及高光谱图像和激光雷达点云的融合技术，由于融合误差和障碍物遮挡，不可避免地会导致几何光谱失真。这些不利影响在多场景下的下游精细任务中限制了其性能，特别是在空中应用方面。为了解决这些问题，我们提出了PiV-AHPC，这是一个用于空中HPC的3D对象检测网络。据我们所知，这是HPC任务上的首次尝试。具体来说，我们首先开发了一个柱体-体素双分支编码器，前者从HPC中提取光谱和垂直结构特征以克服光谱失真，而后者则侧重于从点云中提取准确的3D空间特征。设计了一种多层次特征融合机制，以增强两个分支之间的信息交互，实现邻域特征对齐和通道自适应选择，从而有机地融合异质特征并减轻几何失真。在两个空中HPC数据集上的大量实验表明，PiV-AHPC具有最先进的检测性能和高泛化能力。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.09506v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>超光谱点云（HPCs）能同时描述地面物体的三维空间与光谱信息，具备出色的三维感知和目标识别能力。当前生成HPCs的方法大多采用与超光谱图像和激光雷达点云的融合技术，但由于融合误差和遮挡问题，不可避免地会出现几何光谱失真。这些问题在多场景下游精细任务中，特别是在空中应用中的性能表现尤为突出。为解决这些问题，我们首次提出PiV-AHPC——一种用于空中HPCs的三维目标检测网络。该网络通过构建支柱体素双分支编码器，克服光谱失真并提取准确的3D空间特征，同时设计多层次特征融合机制，强化信息交互，实现邻域特征对齐和通道自适应选择，从而有机融合异构图特征并减轻几何失真。在两组空中HPCs数据集上的实验表明，PiV-AHPC具备一流检测性能和高泛化能力。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>超光谱点云（HPCs）融合了三维空间和光谱信息，为地面物体提供了卓越的三维感知和目标识别功能。</li>
<li>当前HPC生成方法主要基于融合技术，但存在几何光谱失真问题。</li>
<li>PiV-AHPC网络首次尝试解决空中HPCs的三维目标检测问题。</li>
<li>PiV-AHPC采用支柱体素双分支编码器，分别处理光谱和垂直结构特征以及准确的3D空间特征。</li>
<li>多层次特征融合机制强化了信息交互，实现了特征对齐和通道自适应选择。</li>
<li>实验证明，PiV-AHPC在检测性能和泛化能力上均达到一流水平。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.09506">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-fda8ee2b200b8cb5ffcf6bc4d747dc1e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-7db62f7bee30523b3c3e2102acc4db9e.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-3f13fd1179becdb093b11f7d269cbdb6.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-6fb46b7f33704f0354e8b2c97e6ee17d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-bac45d0ea605e7f9facd14be7079930e.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Hallucinatory-Image-Tokens-A-Training-free-EAZY-Approach-on-Detecting-and-Mitigating-Object-Hallucinations-in-LVLMs"><a href="#Hallucinatory-Image-Tokens-A-Training-free-EAZY-Approach-on-Detecting-and-Mitigating-Object-Hallucinations-in-LVLMs" class="headerlink" title="Hallucinatory Image Tokens: A Training-free EAZY Approach on Detecting   and Mitigating Object Hallucinations in LVLMs"></a>Hallucinatory Image Tokens: A Training-free EAZY Approach on Detecting   and Mitigating Object Hallucinations in LVLMs</h2><p><strong>Authors:Liwei Che, Tony Qingze Liu, Jing Jia, Weiyi Qin, Ruixiang Tang, Vladimir Pavlovic</strong></p>
<p>Despite their remarkable potential, Large Vision-Language Models (LVLMs) still face challenges with object hallucination, a problem where their generated outputs mistakenly incorporate objects that do not actually exist. Although most works focus on addressing this issue within the language-model backbone, our work shifts the focus to the image input source, investigating how specific image tokens contribute to hallucinations. Our analysis reveals a striking finding: a small subset of image tokens with high attention scores are the primary drivers of object hallucination. By removing these hallucinatory image tokens (only 1.5% of all image tokens), the issue can be effectively mitigated. This finding holds consistently across different models and datasets. Building on this insight, we introduce EAZY, a novel, training-free method that automatically identifies and Eliminates hAllucinations by Zeroing out hallucinatorY image tokens. We utilize EAZY for unsupervised object hallucination detection, achieving 15% improvement compared to previous methods. Additionally, EAZY demonstrates remarkable effectiveness in mitigating hallucinations while preserving model utility and seamlessly adapting to various LVLM architectures. </p>
<blockquote>
<p>尽管大型视觉语言模型（LVLMs）具有显著潜力，但它们仍面临对象幻觉的挑战，这是一个生成输出中误包含实际不存在的对象的问题。虽然大多数工作都集中在解决语言模型主干中的问题，但我们的工作将重点转向图像输入源，研究特定的图像标记是如何导致幻觉的。我们的分析揭示了一个惊人的发现：具有高注意力分数的少量图像标记是对象幻觉的主要驱动力。通过移除这些产生幻觉的图像标记（仅占所有图像标记的1.5%），可以有效地减轻问题。这一发现在不同的模型和数据集上始终适用。在此基础上，我们引入了无需训练的EAZY方法，该方法能够自动识别和消除通过清零幻觉标记的图像来消除幻觉标记（Zeroing out hallucinatorY）。我们利用EAZY进行无监督对象幻觉检测，与之前的方法相比取得了15%的改进。此外，EAZY在缓解幻觉的同时保持模型效用，并能无缝适应各种LVLM架构。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.07772v2">PDF</a> Accepted to ICCV2025</p>
<p><strong>Summary</strong>：大型视觉语言模型（LVLMs）虽潜力巨大，但仍面临对象幻觉问题，即其生成输出会错误地融入实际不存在的对象。现有研究大多关注语言模型的内部结构来解决这个问题，但本研究转向图像输入源，探讨特定的图像令牌如何导致幻觉。研究发现：具有较高注意力分数的一小部分图像令牌是对象幻觉的主要驱动力。移除这些幻觉图像令牌（仅占所有图像令牌的1.5%），可以有效地缓解这一问题。本研究还引入了一种名为EAZY的新方法，该方法无需训练即可自动识别并消除幻觉，通过对产生幻觉的图像令牌进行清零操作，实现了无监督对象幻觉检测的显著改善，提高了15%。同时，EAZY在保持模型实用性的同时，轻松适应各种LVLM架构。</p>
<p><strong>Key Takeaways</strong>：</p>
<ol>
<li>大型视觉语言模型（LVLMs）存在对象幻觉问题，即生成输出会错误地融入实际不存在的对象。</li>
<li>研究发现，具有较高注意力分数的少部分图像令牌是对象幻觉的主要驱动力。</li>
<li>通过移除这些幻觉图像令牌，可以有效地缓解对象幻觉问题。</li>
<li>引入了一种新的方法EAZY，无需训练即可自动识别并消除幻觉。</li>
<li>EAZY通过对产生幻觉的图像令牌进行清零操作，实现了无监督对象幻觉检测的显著改善。</li>
<li>EAZY相较于之前的方法有15%的改进。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.07772">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-daead031a8926ff475931b986de7f291.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-324503d22086771414772d362d99b67e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4350fc65286ec99e62ba62048bc7c8a4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1d122d236f79f9dcd0a5dcb7a0c8cea3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a6686ad9f6da474741357d5d16d8be21.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a24c268d5e223ed6f933872af372ad30.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-a9ddf6754482135872bd413db56e65f3.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="UnitModule-A-Lightweight-Joint-Image-Enhancement-Module-for-Underwater-Object-Detection"><a href="#UnitModule-A-Lightweight-Joint-Image-Enhancement-Module-for-Underwater-Object-Detection" class="headerlink" title="UnitModule: A Lightweight Joint Image Enhancement Module for Underwater   Object Detection"></a>UnitModule: A Lightweight Joint Image Enhancement Module for Underwater   Object Detection</h2><p><strong>Authors:Zhuoyan Liu, Bo Wang, Ye Li, Jiaxian He, Yunfeng Li</strong></p>
<p>Underwater object detection faces the problem of underwater image degradation, which affects the performance of the detector. Underwater object detection methods based on noise reduction and image enhancement usually do not provide images preferred by the detector or require additional datasets. In this paper, we propose a plug-and-play \textbf{U}nderwater joi\textbf{n}t \textbf{i}mage enhancemen\textbf{t} \textbf{Module} (UnitModule) that provides the input image preferred by the detector. We design an unsupervised learning loss for the joint training of UnitModule with the detector without additional datasets to improve the interaction between UnitModule and the detector. Furthermore, a color cast predictor with the assisting color cast loss and a data augmentation called Underwater Color Random Transfer (UCRT) are designed to improve the performance of UnitModule on underwater images with different color casts. Extensive experiments are conducted on DUO for different object detection models, where UnitModule achieves the highest performance improvement of 2.6 AP for YOLOv5-S and gains the improvement of 3.3 AP on the brand-new test set ((\text{URPC}_{test})). And UnitModule significantly improves the performance of all object detection models we test, especially for models with a small number of parameters. In addition, UnitModule with a small number of parameters of 31K has little effect on the inference speed of the original object detection model. Our quantitative and visual analysis also demonstrates the effectiveness of UnitModule in enhancing the input image and improving the perception ability of the detector for object features. The code is available at <a target="_blank" rel="noopener" href="https://github.com/LEFTeyex/UnitModule">https://github.com/LEFTeyex/UnitModule</a>. </p>
<blockquote>
<p>水下目标检测面临着水下图像退化的问题，这一问题会影响检测器的性能。基于噪声减少和图像增强的水下目标检测方法通常不提供检测器所偏好的图像，或者需要额外的数据集。在本文中，我们提出了一种即插即用的水下联合图像增强模块（UnitModule），该模块可提供检测器所偏好的输入图像。我们为UnitModule与检测器的联合训练设计了一种无监督学习损失，而无需额外数据集，以提高UnitModule与检测器之间的交互。此外，还设计了带有辅助色彩投射损失的色彩投射预测器，以及一种名为水下色彩随机转换（UCRT）的数据增强方法，以提高UnitModule在不同色彩投射的水下图像上的性能。在DUO数据集上对不同的目标检测模型进行了大量实验，UnitModule在YOLOv5-S上实现了最高的性能提升2.6 AP，并在全新测试集URPCtest上实现了3.3 AP的提升。UnitModule显著提高了我们所测试的所有目标检测模型的性能，尤其是对于参数数量较少的模型。此外，UnitModule参数数量较少，仅有31K个参数，对原始目标检测模型的推理速度几乎没有影响。我们的定量和视觉分析还证明了UnitModule在增强输入图像和提高检测器对目标特征的感知能力方面的有效性。代码可在<a target="_blank" rel="noopener" href="https://github.com/LEFTeyex/UnitModule%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/LEFTeyex/UnitModule找到。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.04708v2">PDF</a> 15 pages, 10 figures, 13 tables, accepted by PR</p>
<p><strong>Summary</strong><br>水下目标检测面临图像退化问题，影响检测性能。本文提出了一种即插即用型的水下联合图像增强模块（UnitModule），为检测器提供首选的输入图像。该模块设计了一种无监督学习损失，可与检测器进行联合训练，无需额外数据集即可改善模块与检测器之间的交互。此外，还设计了色彩投射预测器和数据增强方法，以提高UnitModule在不同色彩投射水下图像上的性能。实验表明，UnitModule对多种目标检测模型都有显著的性能提升，特别是在参数较少的模型上。该模块参数较少，对原始目标检测模型的推理速度影响较小。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>水下目标检测面临图像退化问题。</li>
<li>提出了一种即插即用型的水下联合图像增强模块（UnitModule），为检测器提供优化的输入图像。</li>
<li>UnitModule通过无监督学习损失与检测器联合训练，无需额外数据集。</li>
<li>色彩投射预测器和数据增强方法用于提高UnitModule在不同水下图像上的性能。</li>
<li>UnitModule对多种目标检测模型有显著性能提升，尤其适用于参数较少的模型。</li>
<li>UnitModule参数较少，对原始目标检测模型的推理速度影响较小。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2309.04708">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-5777970f343fbc646cd9d60b9921214e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-3f322a9aa8506e990bd63b75e1e605c0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-597ea0e761098ab072596ed0b825dc8b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-fcbc6a79bf9a793faa9f73a89c6a68fa.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-cfcd9db1fe9ec5093df4861cd883e2a3.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        文章作者:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        文章链接:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-07-09/%E6%A3%80%E6%B5%8B_%E5%88%86%E5%89%B2_%E8%B7%9F%E8%B8%AA/">https://kedreamix.github.io/Talk2Paper/Paper/2025-07-09/%E6%A3%80%E6%B5%8B_%E5%88%86%E5%89%B2_%E8%B7%9F%E8%B8%AA/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        版权声明:
                    </i>
                </span>
                <span class="reprint-info">
                    本博客所有文章除特別声明外，均采用
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    许可协议。转载请注明来源
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/%E6%A3%80%E6%B5%8B-%E5%88%86%E5%89%B2-%E8%B7%9F%E8%B8%AA/">
                                    <span class="chip bg-color">检测/分割/跟踪</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;上一篇</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-07-09/%E6%97%A0%E7%9B%91%E7%9D%A3_%E5%8D%8A%E7%9B%91%E7%9D%A3_%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0/">
                    <div class="card-image">
                        
                        <img src="https://pica.zhimg.com/v2-6eca5caf497be878e7106b53e04ff5c5.jpg" class="responsive-img" alt="无监督/半监督/对比学习">
                        
                        <span class="card-title">无监督/半监督/对比学习</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            无监督/半监督/对比学习 方向最新论文已更新，请持续关注 Update in 2025-07-09  CLIP-RL Surgical Scene Segmentation Using Contrastive Language-Vision   Pretraining & Reinforcement Learning
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-07-09
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E6%97%A0%E7%9B%91%E7%9D%A3-%E5%8D%8A%E7%9B%91%E7%9D%A3-%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0/" class="post-category">
                                    无监督/半监督/对比学习
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E6%97%A0%E7%9B%91%E7%9D%A3-%E5%8D%8A%E7%9B%91%E7%9D%A3-%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0/">
                        <span class="chip bg-color">无监督/半监督/对比学习</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                下一篇&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-07-09/Vision%20Transformer/">
                    <div class="card-image">
                        
                        <img src="https://pica.zhimg.com/v2-6236f6d3febe66dcf8b706f1b83798bf.jpg" class="responsive-img" alt="Vision Transformer">
                        
                        <span class="card-title">Vision Transformer</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Vision Transformer 方向最新论文已更新，请持续关注 Update in 2025-07-09  Multi-modal Representations for Fine-grained Multi-label Critical View   of Safety Recognition
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-07-09
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Vision-Transformer/" class="post-category">
                                    Vision Transformer
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Vision-Transformer/">
                        <span class="chip bg-color">Vision Transformer</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- 代码块功能依赖 -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- 代码语言 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- 代码块复制 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- 代码块收缩 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC 悬浮按钮. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* 修复文章卡片 div 的宽度. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // 切换TOC目录展开收缩的相关操作.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;站点总字数:&nbsp;<span
                        class="white-color">24595.3k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- 运行天数提醒. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // 区分是否有年份.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = '本站已运行 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                daysTip = '本站已運行 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = '本站已运行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = '本站已運行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="关注我的知乎: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">知</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- 搜索遮罩框 -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span>
            <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- 白天和黑夜主题 -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- 回到顶部按钮 -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // 只在桌面版网页启用特效
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- 雪花特效 -->
    

    <!-- 鼠标星星特效 -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--腾讯兔小巢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- 动态标签 -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Σ(っ °Д °;)っ诶，页面崩溃了嘛？", clearTimeout(st)) : (document.title = "φ(゜▽゜*)♪咦，又好了！", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
