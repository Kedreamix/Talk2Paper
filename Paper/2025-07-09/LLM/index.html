<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="LLM">
    <meta name="description" content="LLM 方向最新论文已更新，请持续关注 Update in 2025-07-09  Beyond Simple Edits X-Planner for Complex Instruction-Based Image   Editing">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>LLM | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loader页面消失采用渐隐的方式*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logo出现动画 */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//使用渐隐的方法淡出loading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//强制显示loading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">嘘~  正在从服务器偷取页面 . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>首页</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>标签</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>分类</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>归档</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="搜索" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="深色/浅色模式" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			首页
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			标签
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			分类
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			归档
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pic1.zhimg.com/v2-8488f236b5f9be3b34f1f4606e1c9fe7.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">LLM</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- 文章内容详情 -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/LLM/">
                                <span class="chip bg-color">LLM</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/LLM/" class="post-category">
                                LLM
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp;
                    2025-07-09
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>更新日期:&nbsp;&nbsp;
                    2025-07-17
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>文章字数:&nbsp;&nbsp;
                    21k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>阅读时长:&nbsp;&nbsp;
                    85 分
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>阅读次数:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>⚠️ 以下所有内容总结都来自于 大语言模型的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p>
</blockquote>
<h1 id="2025-07-09-更新"><a href="#2025-07-09-更新" class="headerlink" title="2025-07-09 更新"></a>2025-07-09 更新</h1><h2 id="Beyond-Simple-Edits-X-Planner-for-Complex-Instruction-Based-Image-Editing"><a href="#Beyond-Simple-Edits-X-Planner-for-Complex-Instruction-Based-Image-Editing" class="headerlink" title="Beyond Simple Edits: X-Planner for Complex Instruction-Based Image   Editing"></a>Beyond Simple Edits: X-Planner for Complex Instruction-Based Image   Editing</h2><p><strong>Authors:Chun-Hsiao Yeh, Yilin Wang, Nanxuan Zhao, Richard Zhang, Yuheng Li, Yi Ma, Krishna Kumar Singh</strong></p>
<p>Recent diffusion-based image editing methods have significantly advanced text-guided tasks but often struggle to interpret complex, indirect instructions. Moreover, current models frequently suffer from poor identity preservation, unintended edits, or rely heavily on manual masks. To address these challenges, we introduce X-Planner, a Multimodal Large Language Model (MLLM)-based planning system that effectively bridges user intent with editing model capabilities. X-Planner employs chain-of-thought reasoning to systematically decompose complex instructions into simpler, clear sub-instructions. For each sub-instruction, X-Planner automatically generates precise edit types and segmentation masks, eliminating manual intervention and ensuring localized, identity-preserving edits. Additionally, we propose a novel automated pipeline for generating large-scale data to train X-Planner which achieves state-of-the-art results on both existing benchmarks and our newly introduced complex editing benchmark. </p>
<blockquote>
<p>最近基于扩散的图像编辑方法在很大程度上推动了文本引导的任务，但在解释复杂、间接指令时经常遇到困难。此外，当前模型在身份保留方面表现不佳，会出现非预期编辑，或过于依赖手动遮罩。为了解决这些挑战，我们引入了X-Planner，这是一个基于多模态大型语言模型（MLLM）的规划系统，它能够有效地将用户意图与编辑模型功能联系起来。X-Planner采用链式思维推理，系统地分解复杂指令为更简单、清晰的子指令。对于每个子指令，X-Planner会自动生成精确的编辑类型和分割遮罩，消除了人工干预，确保了局部化和身份保留的编辑。此外，我们还提出了一种新的自动化流程来生成大规模数据以训练X-Planner，该流程在现有基准测试和我们新引入的复杂编辑基准测试中均达到了最新水平的结果。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.05259v1">PDF</a> Project page: <a target="_blank" rel="noopener" href="https://danielchyeh.github.io/x-planner/">https://danielchyeh.github.io/x-planner/</a></p>
<p><strong>Summary</strong></p>
<p>近期扩散式图像编辑方法虽在文本引导的任务上有显著进步，但在处理复杂、间接指令时存在挑战。为解决此问题，我们推出X-Planner，一个基于多模态大型语言模型（MLLM）的规划系统，有效桥接用户意图与编辑模型能力。X-Planner采用链式思维推理，将复杂指令分解为简单清晰的子指令。针对每个子指令，X-Planner自动生成精确的编辑类型和分割掩码，消除了手动干预，确保了局部、身份保留的编辑。此外，我们还提出了一种新的自动化管道用于生成大规模数据以训练X-Planner，其在现有基准测试和我们新引入的复杂编辑基准测试中均达到了最佳效果。</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>扩散式图像编辑方法在文本引导任务上的进步显著。</li>
<li>当前图像编辑方法在处理复杂、间接指令时存在挑战。</li>
<li>X-Planner是一个基于多模态大型语言模型的规划系统，能有效桥接用户意图与编辑模型能力。</li>
<li>X-Planner通过链式思维推理分解复杂指令为简单清晰的子指令。</li>
<li>X-Planner自动生成精确的编辑类型和分割掩码，消除了手动干预。</li>
<li>X-Planner确保了局部、身份保留的编辑。</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.05259">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-11230dbeae4e436fecedcda1f98da1d0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-41346a1e5e8e2acda643968c82c398ea.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-6ca6f268b7f03e1fdbe01e687cf0c1c4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-11a65ce66943ced7b17f0c8838d1ebc8.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-2db46a58d4ed61e2cfe6e67fdd4fe4b6.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Spatio-Temporal-LLM-Reasoning-about-Environments-and-Actions"><a href="#Spatio-Temporal-LLM-Reasoning-about-Environments-and-Actions" class="headerlink" title="Spatio-Temporal LLM: Reasoning about Environments and Actions"></a>Spatio-Temporal LLM: Reasoning about Environments and Actions</h2><p><strong>Authors:Haozhen Zheng, Beitong Tian, Mingyuan Wu, Zhenggang Tang, Klara Nahrstedt, Alex Schwing</strong></p>
<p>Despite the significant recent progress of Multimodal Large Language Models (MLLMs), MLLMs still struggle to correctly answer prompts that require a holistic spatio-temporal understanding. Specifically, it is challenging to address prompts that refer to 1) the entirety of an environment that an agent equipped with an MLLM can operate in; and simultaneously also refer to 2) recent actions that just happened and are encoded in a video clip. However, such a holistic spatio-temporal understanding is important for agents operating in the real world. To address this issue, we first develop a framework to collect a large-scale dataset. Using the collected “Reasoning about Environments and Actions” (REA) dataset, we show that recent methods indeed struggle to correctly answer the prompts. To improve, we develop a “spatio-temporal LLM” (ST-LLM), a model equipped with projectors to improve both spatial understanding of an environment and temporal understanding of recent observations. On the collected REA data, we show that the proposed method significantly improves results compared to prior work. Code and data are available at <a target="_blank" rel="noopener" href="https://zoezheng126.github.io/STLLM-website/">https://zoezheng126.github.io/STLLM-website/</a>. </p>
<blockquote>
<p>尽管最近多模态大型语言模型（MLLMs）取得了重大进展，但在需要全面时空理解提示的情境中，MLLMs仍然难以正确回答问题。具体来说，解决涉及以下两个方面的提示具有挑战性：1）一个配备MLLM的代理可以操作的整体环境；同时涉及2）刚刚发生的动作，这些动作被编码在视频剪辑中。然而，对于在现实世界中操作的代理来说，这种全面的时空理解非常重要。为了解决这个问题，我们首先开发了一个框架来收集大规模数据集。“关于环境和行动推理”（REA）数据集显示，最近的方法确实难以正确回答提示。为了改进这一点，我们开发了一种“时空LLM”（ST-LLM），这是一种配备投影仪的模型，旨在提高对环境空间和对最近观察的时间理解。在收集的REA数据上，我们证明了所提出的方法与先前的工作相比，结果得到了显著改善。相关代码和数据可以在<a target="_blank" rel="noopener" href="https://zoezheng126.github.io/STLLM-website/%E6%89%BE%E5%88%B0%E3%80%82">https://zoezheng126.github.io/STLLM-website/找到。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.05258v1">PDF</a> Code and data are available at   <a target="_blank" rel="noopener" href="https://zoezheng126.github.io/STLLM-website/">https://zoezheng126.github.io/STLLM-website/</a></p>
<p><strong>Summary</strong></p>
<p>本文介绍了尽管多模态大型语言模型（MLLMs）近期取得了显著进展，但在应对需要整体时空理解能力的提示时仍面临挑战。为应对此问题，研究团队首先构建了一个大型数据集“关于环境与行为推理”（REA），并发现现有方法难以正确回答这些提示。为提高模型性能，研究团队提出了配备投影器的时空LLM（ST-LLM），以提高对环境和最近观察结果的时空理解能力。在REA数据集上进行的实验表明，相较于之前的方法，所提出的ST-LLM能显著改善结果。代码和数据集可访问于<a target="_blank" rel="noopener" href="https://zoezheng126.github.io/STLLM-website/%E3%80%82">https://zoezheng126.github.io/STLLM-website/。</a></p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>MLLMs在面对需要整体时空理解的提示时仍有困难，尤其是在理解和处理环境和最新行动信息方面。</li>
<li>研究人员创建了一个名为REA的大型数据集，用于评估模型在应对涉及环境和行为理解的提示方面的性能。</li>
<li>现有方法在应对这些提示时表现不佳。</li>
<li>为了改善模型的性能，提出了配备投影器的ST-LLM模型，该模型旨在提高对环境空间的理解和最近的观察结果的时空理解。</li>
<li>在REA数据集上的实验表明，ST-LLM模型相较于之前的方法有显著改善。</li>
<li>ST-LLM模型的代码和数据集已公开发布，便于公众访问和使用。</li>
<li>这种模型对于开发能在现实世界中运作的智能代理具有重要意义。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.05258">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-12fc8033021b2f9a799773b5ce635297.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-5079c80e2fc378a16c514f6a221b54e7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a650cc85a7e6ee4d23c6d0569fd3ee3e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c679d5950bd295f99db3a237199c3560.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-401b55310f0d5968788f7a3160e0a2e6.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Open-Vision-Reasoner-Transferring-Linguistic-Cognitive-Behavior-for-Visual-Reasoning"><a href="#Open-Vision-Reasoner-Transferring-Linguistic-Cognitive-Behavior-for-Visual-Reasoning" class="headerlink" title="Open Vision Reasoner: Transferring Linguistic Cognitive Behavior for   Visual Reasoning"></a>Open Vision Reasoner: Transferring Linguistic Cognitive Behavior for   Visual Reasoning</h2><p><strong>Authors:Yana Wei, Liang Zhao, Jianjian Sun, Kangheng Lin, Jisheng Yin, Jingcheng Hu, Yinmin Zhang, En Yu, Haoran Lv, Zejia Weng, Jia Wang, Chunrui Han, Yuang Peng, Qi Han, Zheng Ge, Xiangyu Zhang, Daxin Jiang, Vishal M. Patel</strong></p>
<p>The remarkable reasoning capability of large language models (LLMs) stems from cognitive behaviors that emerge through reinforcement with verifiable rewards. This work investigates how to transfer this principle to Multimodal LLMs (MLLMs) to unlock advanced visual reasoning. We introduce a two-stage paradigm built on Qwen2.5-VL-7B: a massive linguistic cold-start fine-tuning, followed by multimodal reinforcement learning (RL) spanning nearly 1,000 steps, surpassing all previous open-source efforts in scale. This pioneering work reveals three fundamental insights: 1) Behavior transfer emerges surprisingly early in cold start due to linguistic mental imagery. 2) Cold start broadly memorizes visual behaviors, while RL critically discerns and scales up effective patterns. 3) Transfer strategically favors high-utility behaviors such as visual reflection. Our resulting model, Open-Vision-Reasoner (OVR), achieves state-of-the-art performance on a suite of reasoning benchmarks, including 95.3% on MATH500, 51.8% on MathVision and 54.6% on MathVerse. We release our model, data, and training dynamics to catalyze the development of more capable, behavior-aligned multimodal reasoners. </p>
<blockquote>
<p>大型语言模型（LLM）的出色推理能力来源于通过可验证奖励进行强化后出现的认知行为。这项工作研究了如何将这一原理应用于多模态LLM（MLLM），以解锁高级视觉推理。我们基于Qwen2.5-VL-7B引入了一个两阶段范式：大规模语言冷启动微调，接着是近1000步的多模态强化学习（RL）。这项工作在规模上超越了之前所有的开源工作。这项开创性的工作揭示了三个基本见解：1）由于语言心理意象，行为转移在冷启动阶段出乎意料地早期出现。2）冷启动广泛记忆视觉行为，而强化学习可以精准地区分并扩大有效模式。3）转移策略有利于高实用性的行为，如视觉反射。我们得到的模型Open-Vision-Reasoner（OVR）在一系列推理基准测试上达到了最先进的性能，包括MATH500上的95.3%，MathVision上的51.8%和MathVerse上的54.6%。我们发布我们的模型、数据和训练动态，以推动开发更强大、行为一致的多模态推理器。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.05255v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>大型语言模型的出色推理能力源于通过可验证的奖励进行强化后出现的认知行为。本研究探索了如何将这一原则应用于多模态大型语言模型，以解锁高级视觉推理。研究提出了一种基于Qwen2.5-VL-7B的两阶段范式，先进行大规模语言冷启动微调，然后进行近1000步的多模态强化学习。本研究揭示了行为转移在冷启动早期就会出现，多模态强化学习能够有效提升模型的性能。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>大型语言模型的推理能力源于强化学习和可验证的奖励。</li>
<li>多模态大型语言模型具有解锁高级视觉推理的潜力。</li>
<li>两阶段范式结合了冷启动微调与多模态强化学习，规模超越之前所有开源努力。</li>
<li>行为转移在冷启动早期阶段就会出现，源于语言心理图像。</li>
<li>冷启动广泛记忆视觉行为，而强化学习则关键地区分和扩大有效模式。</li>
<li>转移策略有利于高实用性的行为，如视觉反射。</li>
<li>研究的模型Open-Vision-Reasoner在多个推理基准测试上达到最新水平，包括MATH500的95.3%、MathVision的51.8%和MathVerse的54.6%。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.05255">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-fc49516c89e3d0567aface1442a2f7be.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-fd80b4e4108801b3ea459af6851d736d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2a58e9c1a7abda30e20e5f0290049619.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-fcf2a13672256444a1608d5467af1585.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Response-Attack-Exploiting-Contextual-Priming-to-Jailbreak-Large-Language-Models"><a href="#Response-Attack-Exploiting-Contextual-Priming-to-Jailbreak-Large-Language-Models" class="headerlink" title="Response Attack: Exploiting Contextual Priming to Jailbreak Large   Language Models"></a>Response Attack: Exploiting Contextual Priming to Jailbreak Large   Language Models</h2><p><strong>Authors:Ziqi Miao, Lijun Li, Yuan Xiong, Zhenhua Liu, Pengyu Zhu, Jing Shao</strong></p>
<p>Contextual priming, where earlier stimuli covertly bias later judgments, offers an unexplored attack surface for large language models (LLMs). We uncover a contextual priming vulnerability in which the previous response in the dialogue can steer its subsequent behavior toward policy-violating content. Building on this insight, we propose Response Attack, which uses an auxiliary LLM to generate a mildly harmful response to a paraphrased version of the original malicious query. They are then formatted into the dialogue and followed by a succinct trigger prompt, thereby priming the target model to generate harmful content. Across eight open-source and proprietary LLMs, RA consistently outperforms seven state-of-the-art jailbreak techniques, achieving higher attack success rates. To mitigate this threat, we construct and release a context-aware safety fine-tuning dataset, which significantly reduces the attack success rate while preserving model capabilities. The code and data are available at <a target="_blank" rel="noopener" href="https://github.com/Dtc7w3PQ/Response-Attack">https://github.com/Dtc7w3PQ/Response-Attack</a>. </p>
<blockquote>
<p>语境提示法，即早期刺激无形中影响后续判断，为大型语言模型（LLM）提供了一个尚未探索的攻击面。我们发现了一种语境提示的漏洞，即对话中的前一个回答可以引导其后续行为走向违反政策的内容。基于这一发现，我们提出了响应攻击法，该方法使用辅助LLM对原始恶意查询的另一种表述生成轻度有害的响应。然后它们被整理成对话形式，紧接着是简短的触发提示，从而引导目标模型生成有害内容。在八个开源和专有LLM中，RA始终优于七种最先进的越狱技术，并实现了更高的攻击成功率。为了缓解这一威胁，我们构建并发布了一个上下文感知的安全微调数据集，它在降低攻击成功率的同时保留了模型的能力。代码和数据可在<a target="_blank" rel="noopener" href="https://github.com/Dtc7w3PQ/Response-Attack%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/Dtc7w3PQ/Response-Attack获取。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.05248v1">PDF</a> 21 pages, 9 figures. Code and data available at   <a target="_blank" rel="noopener" href="https://github.com/Dtc7w3PQ/Response-Attack">https://github.com/Dtc7w3PQ/Response-Attack</a></p>
<p><strong>Summary</strong>：</p>
<p>语境提示对大型语言模型（LLM）构成未探索的攻击面，先前的对话响应可能影响后续行为，导致违反策略内容。基于此，提出Response Attack方法，使用辅助LLM生成轻微有害响应并针对原始恶意查询进行改编，随后通过简洁触发提示，使目标模型生成有害内容。该方法在多个开源和专有LLM上表现优异，成功率高且超越七种先进的越狱技术。为应对这一威胁，构建了上下文感知的安全微调数据集，既能显著降低攻击成功率，同时保留模型能力。相关代码和数据集已在GitHub上公开。</p>
<p><strong>Key Takeaways</strong>：</p>
<ol>
<li>语境提示成为大型语言模型（LLM）的新攻击面。</li>
<li>对话中的先前响应能够影响后续模型行为，可能导致策略违规内容生成。</li>
<li>提出Response Attack方法，利用辅助LLM生成针对目标模型的恶意响应。</li>
<li>Response Attack方法在不同LLM上表现稳定，成功率高且优于其他越狱技术。</li>
<li>为应对该威胁，开发上下文感知的安全微调数据集。</li>
<li>安全微调数据集能显著降低攻击成功率，同时保持模型性能。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.05248">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-a357f7aa6db7c0ad6ad3635ce25807fa.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9c319060f932ffc5bb7b511d1e79a96f.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-bafd88deea4d3b186fd82c3606fcf903.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-343062dfa8575359de38af28450b9ee6.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="StreamVLN-Streaming-Vision-and-Language-Navigation-via-SlowFast-Context-Modeling"><a href="#StreamVLN-Streaming-Vision-and-Language-Navigation-via-SlowFast-Context-Modeling" class="headerlink" title="StreamVLN: Streaming Vision-and-Language Navigation via SlowFast Context   Modeling"></a>StreamVLN: Streaming Vision-and-Language Navigation via SlowFast Context   Modeling</h2><p><strong>Authors:Meng Wei, Chenyang Wan, Xiqian Yu, Tai Wang, Yuqiang Yang, Xiaohan Mao, Chenming Zhu, Wenzhe Cai, Hanqing Wang, Yilun Chen, Xihui Liu, Jiangmiao Pang</strong></p>
<p>Vision-and-Language Navigation (VLN) in real-world settings requires agents to process continuous visual streams and generate actions with low latency grounded in language instructions. While Video-based Large Language Models (Video-LLMs) have driven recent progress, current VLN methods based on Video-LLM often face trade-offs among fine-grained visual understanding, long-term context modeling and computational efficiency. We introduce StreamVLN, a streaming VLN framework that employs a hybrid slow-fast context modeling strategy to support multi-modal reasoning over interleaved vision, language and action inputs. The fast-streaming dialogue context facilitates responsive action generation through a sliding-window of active dialogues, while the slow-updating memory context compresses historical visual states using a 3D-aware token pruning strategy. With this slow-fast design, StreamVLN achieves coherent multi-turn dialogue through efficient KV cache reuse, supporting long video streams with bounded context size and inference cost. Experiments on VLN-CE benchmarks demonstrate state-of-the-art performance with stable low latency, ensuring robustness and efficiency in real-world deployment. The project page is: \href{<a target="_blank" rel="noopener" href="https://streamvln.github.io/%7D%7Bhttps://streamvln.github.io/%7D">https://streamvln.github.io/}{https://streamvln.github.io/}</a>. </p>
<blockquote>
<p>在真实世界环境中，视觉与语言导航（VLN）要求智能体处理连续视觉流，并基于语言指令生成低延迟的动作。尽管基于视频的的大型语言模型（Video-LLM）已经推动了近期的进展，但当前基于Video-LLM的VLN方法通常在精细的视觉理解、长期上下文建模和计算效率之间面临权衡。我们引入了StreamVLN，这是一个流式VLN框架，采用混合的慢快上下文建模策略，支持对交织的视觉、语言和动作输入的跨模态推理。快速流式对话上下文通过活动对话的滑动窗口促进响应性动作生成，而缓慢更新的内存上下文使用三维感知令牌修剪策略压缩历史视觉状态。通过这种慢快设计，StreamVLN通过高效的关键值缓存重用实现了连贯的多轮对话，支持具有有限上下文大小和推理成本的长视频流。在VLN-CE基准测试上的实验展示了最先进的性能以及稳定的低延迟，确保了在实际部署中的稳健性和效率。项目页面为：<a target="_blank" rel="noopener" href="https://streamvln.github.io./">https://streamvln.github.io/。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.05240v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>基于视频的大型语言模型（Video-LLM）推动了视觉与语言导航（VLN）领域的进展，但现有方法面临精细视觉理解、长期上下文建模和计算效率之间的权衡问题。本文提出了StreamVLN框架，采用快慢上下文混合建模策略，支持对交织的视觉、语言和动作输入的跨模态推理。快速流对话上下文通过活动对话的滑动窗口促进响应动作生成，而慢速更新记忆上下文则使用3D感知令牌修剪策略压缩历史视觉状态。通过快慢设计，StreamVLN实现了高效KV缓存的连贯多轮对话，支持具有有界上下文大小和推理成本的长视频流。在VLN-CE基准测试上的实验表明，其性能达到最新水平，具有稳定的低延迟，确保了在实际部署中的稳健性和效率。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>视频大型语言模型（Video-LLM）推动了视觉与语言导航（VLN）的进步。</li>
<li>当前VLN方法面临精细视觉理解、长期上下文建模和计算效率的权衡问题。</li>
<li>StreamVLN框架采用快慢上下文混合建模策略支持跨模态推理。</li>
<li>快速流对话上下文促进响应动作生成。</li>
<li>慢速更新记忆上下文压缩历史视觉状态。</li>
<li>StreamVLN实现了连贯的多轮对话，支持长视频流处理。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.05240">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-f7262023e1d0445c724b3b1424f9c88c.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-824d75dae424940c9d7956d3caa84f35.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ecb7bddae15356e6f2bc6ea0f51403b1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-46e56b811b0df41e6bab87003252cdb5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6d1eaf832c97bd68ba14f8a5e74839e5.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-3595d64ce3d88aba12c2b42fb77bb9d0.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Cascade-Token-Sharded-Private-LLM-Inference"><a href="#Cascade-Token-Sharded-Private-LLM-Inference" class="headerlink" title="Cascade: Token-Sharded Private LLM Inference"></a>Cascade: Token-Sharded Private LLM Inference</h2><p><strong>Authors:Rahul Thomas, Louai Zahran, Erica Choi, Akilesh Potti, Micah Goldblum, Arka Pal</strong></p>
<p>As LLMs continue to increase in parameter size, the computational resources required to run them are available to fewer parties. Therefore, third-party inference services – where LLMs are hosted by third parties with significant computational resources – are becoming increasingly popular. However, third party inference raises critical concerns about user data privacy. To mitigate these risks, privacy researchers have developed provably secure schemes for third-party inference, such as Secure Multi-Party Computation (SMPC). However, SMPC protocols have significant computational and communication overhead, and do not scale to large models. In this work, we propose a new multi-party inference protocol, Cascade, that avoids these punitive costs by leveraging sharding in the sequence dimension to maintain privacy, trading off cryptographic privacy guarantees for increased performance and scalability. We demonstrate that Cascade is resistant to a generalization of a recent attack that is highly effective against other statistical privacy schemes, and that it is further resistant to learning-based attacks. As Cascade is orders of magnitude faster than existing schemes, our findings offer practical solutions for secure deployment of modern state-of-the-art LLMs. </p>
<blockquote>
<p>随着LLM的参数规模继续增加，运行它们所需的计算资源可用的方越来越少。因此，第三方推理服务——LLM由具有大量计算资源的第三方托管——越来越受欢迎。然而，第三方推理引发了关于用户数据隐私的关键担忧。为了减轻这些风险，隐私研究人员已经为第三方推理开发了可证明的安全方案，如安全多方计算（SMPC）。然而，SMPC协议具有重大的计算和通信开销，并不能扩展到大型模型。在这项工作中，我们提出了一种新的多方推理协议Cascade，它通过利用序列维度上的分片来保持隐私，避免了这些惩罚性成本，以牺牲部分加密隐私保证来换取更高的性能和可扩展性。我们证明Cascade能够抵抗一种针对其他统计隐私方案的最新攻击的一般化形式，并且进一步抵抗基于学习的攻击。由于Cascade比现有方案快几个数量级，我们的发现为现代最新LLM的安全部署提供了实用解决方案。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.05228v1">PDF</a> To be published in ICML 2025 Main Proceedings as “Hidden No More:   Attacking and Defending Private Third-Party LLM Inference”, together with   arXiv:2505.18332</p>
<p><strong>Summary</strong></p>
<p>大型语言模型（LLM）参数规模的不断增长对计算资源的需求越来越高，使得第三方推理服务愈发流行。然而，这引发了用户数据隐私的重大担忧。为缓解这些风险，隐私研究人员已开发出一些如安全多方计算（SMPC）的安全方案。但SMPC协议存在计算与通信开销大、无法扩展到大型模型的问题。本研究提出了一种新的多方推理协议Cascade，通过利用序列维度的分片来保持隐私，以牺牲部分加密隐私保证来换取更高的性能和可扩展性。Cascade对一种针对其他统计隐私方案的最新攻击具有抵抗力，并且能抵抗基于学习的攻击。由于Cascade比现有方案速度快几个数量级，因此为现代前沿的大型语言模型的安全部署提供了实际解决方案。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLMs参数规模增长导致计算资源需求增加，第三方推理服务因此变得流行。</li>
<li>第三方推理服务引发用户数据隐私的重大担忧。</li>
<li>隐私研究人员已经开发了一些安全方案如SMPC来解决这个问题，但存在计算开销大、无法扩展到大型模型的缺点。</li>
<li>本研究提出了一种新的多方推理协议Cascade，通过利用序列维度的分片来保持隐私，牺牲部分加密隐私保证以换取更高性能和可扩展性。</li>
<li>Cascade能有效抵抗针对其他统计隐私方案的最新攻击，也能抵抗基于学习的攻击。</li>
<li>Cascade相比现有方案速度更快，为现代前沿的大型语言模型的安全部署提供了实际解决方案。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.05228">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-9d56016dfe65266fff3d7896153e9855.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-fa5ca70eba29004f6bd29eec05261bb0.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-678d84049019372a5cd34b9d5b9dd447.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-4036186af98113e7dc402f8b7a7739d1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-712e57d24f77b561aed1e90ef2f46615.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="All-in-One-Visual-Description-Guided-Unified-Point-Cloud-Segmentation"><a href="#All-in-One-Visual-Description-Guided-Unified-Point-Cloud-Segmentation" class="headerlink" title="All in One: Visual-Description-Guided Unified Point Cloud Segmentation"></a>All in One: Visual-Description-Guided Unified Point Cloud Segmentation</h2><p><strong>Authors:Zongyan Han, Mohamed El Amine Boudjoghra, Jiahua Dong, Jinhong Wang, Rao Muhammad Anwer</strong></p>
<p>Unified segmentation of 3D point clouds is crucial for scene understanding, but is hindered by its sparse structure, limited annotations, and the challenge of distinguishing fine-grained object classes in complex environments. Existing methods often struggle to capture rich semantic and contextual information due to limited supervision and a lack of diverse multimodal cues, leading to suboptimal differentiation of classes and instances. To address these challenges, we propose VDG-Uni3DSeg, a novel framework that integrates pre-trained vision-language models (e.g., CLIP) and large language models (LLMs) to enhance 3D segmentation. By leveraging LLM-generated textual descriptions and reference images from the internet, our method incorporates rich multimodal cues, facilitating fine-grained class and instance separation. We further design a Semantic-Visual Contrastive Loss to align point features with multimodal queries and a Spatial Enhanced Module to model scene-wide relationships efficiently. Operating within a closed-set paradigm that utilizes multimodal knowledge generated offline, VDG-Uni3DSeg achieves state-of-the-art results in semantic, instance, and panoptic segmentation, offering a scalable and practical solution for 3D understanding. Our code is available at <a target="_blank" rel="noopener" href="https://github.com/Hanzy1996/VDG-Uni3DSeg">https://github.com/Hanzy1996/VDG-Uni3DSeg</a>. </p>
<blockquote>
<p>三维点云的统一分割对于场景理解至关重要，但其稀疏结构、有限的标注以及复杂环境中精细对象类别的区分挑战阻碍了其发展。现有方法由于监督有限和缺乏多样的多模式线索，往往难以捕捉丰富的语义和上下文信息，导致类别和实例的区分不佳。为了解决这些挑战，我们提出了VDG-Uni3DSeg，一个结合预训练的语言视觉模型（例如CLIP）和大型语言模型（LLM）来增强三维分割的新型框架。通过利用LLM生成的文本描述和互联网上的参考图像，我们的方法融入了丰富的多模式线索，促进了精细的类别和实例分离。我们进一步设计了一种语义视觉对比损失，将点特征与多模式查询对齐，并设计了一个空间增强模块，以有效地建模场景范围内的关系。VDG-Uni3DSeg在语义、实例和全景分割方面采用了利用离线生成的多模式知识的封闭集范式，实现了最先进的成果，为三维理解提供了可扩展和实用的解决方案。我们的代码可在&lt;<a target="_blank" rel="noopener" href="https://github.com/Hanzy199">https://github.com/Hanzy199</a> 6&#x2F;VDG-Uni3DSeg&gt;获得。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.05211v1">PDF</a> Accepted by ICCV2025</p>
<p><strong>Summary</strong></p>
<p>本文提出一种名为VDG-Uni3DSeg的新型框架，用于解决三维点云统一分割中的挑战。该框架结合了预训练的视觉语言模型和大型语言模型（LLM），通过利用LLM生成的文本描述和互联网上的参考图像，融入丰富的多模态线索，提升对精细类别和实例的辨识能力。设计语义视觉对比损失和空间增强模块，分别用于对齐点特征和多模态查询以及有效建模场景内关系。在采用离线生成的多模态知识闭集模式下，VDG-Uni3DSeg在语义、实例和全景分割上取得领先结果，为三维理解提供可扩展且实用的解决方案。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>三维点云统一分割对于场景理解至关重要，面临稀疏结构、有限标注和复杂环境中精细类别区分等挑战。</li>
<li>现有方法因缺乏丰富语义和上下文信息以及多样的多模态线索，导致类别和实例区分不足。</li>
<li>VDG-Uni3DSeg框架结合了预训练的视觉语言模型和大型语言模型（LLM），融入多模态线索，提升精细类别和实例辨识能力。</li>
<li>通过利用LLM生成的文本描述和互联网上的参考图像，丰富了点云数据的特征和上下文信息。</li>
<li>设计的语义视觉对比损失有助于对齐点特征和多模态查询。</li>
<li>空间增强模块能够高效建模场景内关系。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.05211">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-2d161c01e43733bbf1faeb5c44771f84.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-bdd6c964b80d5bff0eb32309942851ae.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-343f85ba71b6426fbd0da6be42292b9f.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="CREW-WILDFIRE-Benchmarking-Agentic-Multi-Agent-Collaborations-at-Scale"><a href="#CREW-WILDFIRE-Benchmarking-Agentic-Multi-Agent-Collaborations-at-Scale" class="headerlink" title="CREW-WILDFIRE: Benchmarking Agentic Multi-Agent Collaborations at Scale"></a>CREW-WILDFIRE: Benchmarking Agentic Multi-Agent Collaborations at Scale</h2><p><strong>Authors:Jonathan Hyun, Nicholas R Waytowich, Boyuan Chen</strong></p>
<p>Despite rapid progress in large language model (LLM)-based multi-agent systems, current benchmarks fall short in evaluating their scalability, robustness, and coordination capabilities in complex, dynamic, real-world tasks. Existing environments typically focus on small-scale, fully observable, or low-complexity domains, limiting their utility for developing and assessing next-generation multi-agent Agentic AI frameworks. We introduce CREW-Wildfire, an open-source benchmark designed to close this gap. Built atop the human-AI teaming CREW simulation platform, CREW-Wildfire offers procedurally generated wildfire response scenarios featuring large maps, heterogeneous agents, partial observability, stochastic dynamics, and long-horizon planning objectives. The environment supports both low-level control and high-level natural language interactions through modular Perception and Execution modules. We implement and evaluate several state-of-the-art LLM-based multi-agent Agentic AI frameworks, uncovering significant performance gaps that highlight the unsolved challenges in large-scale coordination, communication, spatial reasoning, and long-horizon planning under uncertainty. By providing more realistic complexity, scalable architecture, and behavioral evaluation metrics, CREW-Wildfire establishes a critical foundation for advancing research in scalable multi-agent Agentic intelligence. All code, environments, data, and baselines will be released to support future research in this emerging domain. </p>
<blockquote>
<p>尽管基于大型语言模型（LLM）的多智能体系统取得了快速进展，但当前的基准测试在评估其在复杂、动态、现实任务的可扩展性、稳健性和协调能力方面还存在不足。现有环境通常专注于小规模、完全可观察或低复杂度的领域，这限制了它们在开发下一代多智能体Agentic人工智能框架中的实用性。我们推出了CREW-Wildfire，这是一个旨在弥补这一差距的开源基准测试。它建立在人类人工智能团队CREW仿真平台之上，提供了程序化生成的野火应对场景，包括大型地图、异构智能体、部分可观察性、随机动态以及长期规划目标。该环境通过模块化感知和执行模块支持低级控制和高级自然语言交互。我们实现并评估了多种基于最新大型语言模型的多智能体Agentic人工智能框架，发现了显著的性能差距，凸显出在解决大规模协调、通信、空间推理和长期不确定性规划方面的挑战。通过提供更现实的复杂性、可扩展的架构和行为评估指标，CREW-Wildfire为推进可扩展多智能体Agentic智能的研究奠定了关键基础。所有代码、环境、数据和基准测试都将发布，以支持这一新兴领域的未来研究。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.05178v1">PDF</a> Our project website is at:   <a target="_blank" rel="noopener" href="http://generalroboticslab.com/CREW-Wildfire">http://generalroboticslab.com/CREW-Wildfire</a></p>
<p><strong>Summary</strong>：</p>
<p>虽然大型语言模型（LLM）在多智能体系统中的应用取得了快速进展，但现有评估基准测试在评估其在复杂动态现实世界任务中的可扩展性、稳健性和协调能力方面存在不足。为解决这一问题，本文引入了CREW-Wildfire这一开放源码基准测试，其基于人类-人工智能团队合作的CREW模拟平台，具有程序生成的大型地图、异种智能体、部分可观察性、随机动态性和长期规划目标等特性。本文实现了多个先进的LLM多智能体Agentic人工智能框架并对其进行了评估，揭示了大规模协调、沟通、空间推理和长期不确定性规划等方面的显著性能差距。通过提供更现实的复杂性、可扩展的架构和行为评估指标，CREW-Wildfire为推进多智能体Agentic智能的研究奠定了重要基础。</p>
<p><strong>Key Takeaways</strong>：</p>
<ol>
<li>当前LLM多智能体系统的评估基准测试在评估其在实际复杂环境中的性能时存在局限性。</li>
<li>CREW-Wildfire是一个新的开放源码基准测试，用于评估LLM多智能体系统在模拟现实世界场景中的性能。</li>
<li>CREW-Wildfire提供了程序生成的大型地图、不同种类的智能体、部分可观察性、随机动态性和长期规划目标等特性。</li>
<li>通过在CREW-Wildfire环境中实施和评估多个先进的LLM多智能体Agentic人工智能框架，发现了在协调、沟通、空间推理和长期规划方面的挑战。</li>
<li>CREW-Wildfire填补了现有评估基准测试的不足，为多智能体Agentic智能的研究提供了更真实、可扩展的评估环境。</li>
<li>CREW-Wildfire的发布包括代码、环境、数据和基准线，以支持未来在该领域的研究。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.05178">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-21680c2a1c88b72ca715a9f4c9084147.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-984afccab52b84ffb08ad3765ebdde61.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-3f0b51f277906237d09bb5ed9fc9c6a1.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-1e91e5b306b5e621646eebe639299854.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="OpenS2S-Advancing-Open-Source-End-to-End-Empathetic-Large-Speech-Language-Model"><a href="#OpenS2S-Advancing-Open-Source-End-to-End-Empathetic-Large-Speech-Language-Model" class="headerlink" title="OpenS2S: Advancing Open-Source End-to-End Empathetic Large Speech   Language Model"></a>OpenS2S: Advancing Open-Source End-to-End Empathetic Large Speech   Language Model</h2><p><strong>Authors:Chen Wang, Tianyu Peng, Wen Yang, Yinan Bai, Guangfu Wang, Jun Lin, Lanpeng Jia, Lingxiang Wu, Jinqiao Wang, Chengqing Zong, Jiajun Zhang</strong></p>
<p>Empathetic interaction is a cornerstone of human-machine communication, due to the need for understanding speech enriched with paralinguistic cues and generating emotional and expressive responses. However, the most powerful empathetic LSLMs are increasingly closed off, leaving the crucial details about the architecture, data and development opaque to researchers. Given the critical need for transparent research into the LSLMs and empathetic behavior, we present OpenS2S, a fully open-source, transparent and end-to-end LSLM designed to enable empathetic speech interactions. Based on our empathetic speech-to-text model BLSP-Emo, OpenS2S further employs a streaming interleaved decoding architecture to achieve low-latency speech generation. To facilitate end-to-end training, OpenS2S incorporates an automated data construction pipeline that synthesizes diverse, high-quality empathetic speech dialogues at low cost. By leveraging large language models to generate empathetic content and controllable text-to-speech systems to introduce speaker and emotional variation, we construct a scalable training corpus with rich paralinguistic diversity and minimal human supervision. We release the fully open-source OpenS2S model, including the dataset, model weights, pre-training and fine-tuning codes, to empower the broader research community and accelerate innovation in empathetic speech systems. The project webpage can be accessed at <a target="_blank" rel="noopener" href="https://casia-lm.github.io/OpenS2S">https://casia-lm.github.io/OpenS2S</a> </p>
<blockquote>
<p>共情交互是人类与机器通信的基石，这是因为需要理解融入副语言线索的语音并生成情感和表达性响应。然而，最强大的共情LSLM越来越封闭，使得关于架构、数据以及开发的关键细节对研究者来说模糊不清。鉴于对LSLM和共情行为透明研究的迫切需求，我们推出了OpenS2S，这是一个完全开源、透明和端到端的LSLM，旨在实现共情语音交互。基于我们的共情语音到文本模型BLSP-Emo，OpenS2S进一步采用流式交织解码架构实现低延迟语音生成。为了促进端到端训练，OpenS2S采用自动化数据构建管道，以低成本合成多样、高质量的共情语音对话。通过利用大型语言模型生成共情内容以及可控的文本到语音系统引入发言者和情感变化，我们构建了一个可扩展的训练语料库，具有丰富的副语言多样性和最小的人工监督。我们公开发布了完全开源的OpenS2S模型，包括数据集、模型权重、预训练和微调代码，以支持更广泛的研究群体并加速共情语音系统的创新。项目网页可通过<a target="_blank" rel="noopener" href="https://casia-lm.github.io/OpenS2S%E8%AE%BF%E9%97%AE%E3%80%82">https://casia-lm.github.io/OpenS2S访问。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.05177v1">PDF</a> Technical Report</p>
<p><strong>Summary</strong></p>
<p>本文强调了在人机通信中，理解带有副语言线索的语音和生成情感丰富、有表现力的回应是关键所在。为应对强大的类情绪交流学习模型的不透明性挑战，研究人员推出了一个完全开源的类情绪语言模型——OpenS2S，以实现有同理心的语音交互。该模型具有低延迟的语音识别特点，利用数据流交错解码架构，并且设有自动数据构建管道以实现端到端的训练。OpenS2S旨在通过大规模语言模型生成有同理心的内容，并利用可控的文本到语音系统引入说话者和情感变化，构建具有丰富副语言多样性和最小人工监督的可扩展训练语料库。该项目已完全开源，包括数据集、模型权重、预训练和微调代码等，旨在助力更广泛的研究群体并加速同理心语音系统的创新。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>人机交互需要理解带有副语言线索的语音和生成情感丰富的回应。</li>
<li>OpenS2S是一个旨在实现有同理心的语音交互的完全开源的类情绪语言模型。</li>
<li>OpenS2S模型具有低延迟的语音识别特点。</li>
<li>数据流交错解码架构在OpenS2S模型中得以应用。</li>
<li>OpenS2S项目包括自动数据构建管道，可实现端到端的训练。</li>
<li>OpenS2S通过使用大规模语言模型生成有同理心的内容并引入可控的文本到语音系统来构建训练语料库。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.05177">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-21948ff441f4448eb4f3b0b7cb089dd4.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-e7d40f4f974b108affe286909cb3d7e3.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-8488f236b5f9be3b34f1f4606e1c9fe7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2eafc4bdd5dab338e91b63b710eefdb4.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="LERa-Replanning-with-Visual-Feedback-in-Instruction-Following"><a href="#LERa-Replanning-with-Visual-Feedback-in-Instruction-Following" class="headerlink" title="LERa: Replanning with Visual Feedback in Instruction Following"></a>LERa: Replanning with Visual Feedback in Instruction Following</h2><p><strong>Authors:Svyatoslav Pchelintsev, Maxim Patratskiy, Anatoly Onishchenko, Alexandr Korchemnyi, Aleksandr Medvedev, Uliana Vinogradova, Ilya Galuzinsky, Aleksey Postnikov, Alexey K. Kovalev, Aleksandr I. Panov</strong></p>
<p>Large Language Models are increasingly used in robotics for task planning, but their reliance on textual inputs limits their adaptability to real-world changes and failures. To address these challenges, we propose LERa - Look, Explain, Replan - a Visual Language Model-based replanning approach that utilizes visual feedback. Unlike existing methods, LERa requires only a raw RGB image, a natural language instruction, an initial task plan, and failure detection - without additional information such as object detection or predefined conditions that may be unavailable in a given scenario. The replanning process consists of three steps: (i) Look, where LERa generates a scene description and identifies errors; (ii) Explain, where it provides corrective guidance; and (iii) Replan, where it modifies the plan accordingly. LERa is adaptable to various agent architectures and can handle errors from both dynamic scene changes and task execution failures. We evaluate LERa on the newly introduced ALFRED-ChaOS and VirtualHome-ChaOS datasets, achieving a 40% improvement over baselines in dynamic environments. In tabletop manipulation tasks with a predefined probability of task failure within the PyBullet simulator, LERa improves success rates by up to 67%. Further experiments, including real-world trials with a tabletop manipulator robot, confirm LERa’s effectiveness in replanning. We demonstrate that LERa is a robust and adaptable solution for error-aware task execution in robotics. The code is available at <a target="_blank" rel="noopener" href="https://lera-robo.github.io/">https://lera-robo.github.io</a>. </p>
<blockquote>
<p>大型语言模型在机器人任务规划中的应用越来越广泛，但它们对文本输入的依赖限制了其在现实世界变化和故障中的适应性。为了解决这些挑战，我们提出了LERa——一种基于视觉语言模型的重新规划方法，它利用视觉反馈，包括“观察”、“解释”、“重新规划”三个步骤。不同于现有方法，LERa仅需要原始RGB图像、自然语言指令、初始任务计划和故障检测，而无需额外的信息，如对象检测或可能在特定场景中不可用的预定义条件。在LERa的重新规划过程中：（i）“观察”阶段，LERa生成场景描述并识别错误；（ii）“解释”阶段，它提供纠正指导；（iii）“重新规划”阶段，它相应地修改计划。LERa可以适应各种代理架构，并可以处理来自动态场景变化和任务执行失败的错误。我们在新引入的ALFRED-ChaOS和VirtualHome-ChaOS数据集上评估了LERa的性能，在动态环境中实现了比基线方法高40%的改进。在PyBullet模拟器中，对于具有预定任务失败概率的桌面操作任务，LERa将成功率提高了高达67%。进一步的实验，包括与桌面操作机器人进行的真实世界试验，证实了LERa在重新规划中的有效性。我们证明LERa是机器人错误感知任务执行中稳健且适应性强的解决方案。相关代码可访问<a target="_blank" rel="noopener" href="https://lera-robo.github.io./">https://lera-robo.github.io。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.05135v1">PDF</a> IROS 2025</p>
<p><strong>Summary</strong></p>
<p>大型语言模型在机器人任务规划中的应用日益广泛，但其对文本输入的依赖限制了其在现实世界变化和故障中的适应性。为解决此问题，提出一种基于视觉语言模型的再规划方法LERa（Look, Explain, Replan），该方法利用视觉反馈。LERa仅需原始RGB图像、自然语言指令、初始任务计划和故障检测，无需在给定场景中可能无法获得的对象检测或预设条件。再规划过程包括三个步骤：观察、解释和再规划。LERa可适应各种代理架构，可处理动态场景变化和任务执行故障。在全新引入的ALFRED-ChaOS和VirtualHome-ChaOS数据集上评估LERa，其在动态环境中比基线提高了40%。在PyBullet模拟器中，对于具有预设任务失败概率的桌面操作任务，LERa将成功率提高了67%。进一步的实验，包括与桌面操作机器人进行的真实世界试验，证实了LERa在重新规划中的有效性。表明LERa是机器人任务执行中错误感知的稳健且可适应的解决方案。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LERa是一种基于视觉语言模型的再规划方法，用于机器人任务规划，解决了大型语言模型对文本输入的依赖问题。</li>
<li>LERa通过利用视觉反馈，能够适应现实世界的变化和故障。</li>
<li>LERa仅需要RGB图像、自然语言指令、初始任务计划和故障检测，无需其他可能在特定场景中无法获得的信息。</li>
<li>LERa的再规划过程包括观察、解释和再规划三个步骤。</li>
<li>LERa可适应各种代理架构，并能处理动态场景变化和任务执行故障。</li>
<li>在多个数据集上的实验表明，LERa在动态环境中比基线有显著改善。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.05135">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-56ba977680cc6f4b4d3046be06bf3ccb.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b53a960b02d595adffd8f3bd8d793a9f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ae53b35bece955a875e2ad5d5b9130b3.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-192aba7e35bbaf3044c81e9fffb27a35.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="VerifyLLM-LLM-Based-Pre-Execution-Task-Plan-Verification-for-Robots"><a href="#VerifyLLM-LLM-Based-Pre-Execution-Task-Plan-Verification-for-Robots" class="headerlink" title="VerifyLLM: LLM-Based Pre-Execution Task Plan Verification for Robots"></a>VerifyLLM: LLM-Based Pre-Execution Task Plan Verification for Robots</h2><p><strong>Authors:Danil S. Grigorev, Alexey K. Kovalev, Aleksandr I. Panov</strong></p>
<p>In the field of robotics, researchers face a critical challenge in ensuring reliable and efficient task planning. Verifying high-level task plans before execution significantly reduces errors and enhance the overall performance of these systems. In this paper, we propose an architecture for automatically verifying high-level task plans before their execution in simulator or real-world environments. Leveraging Large Language Models (LLMs), our approach consists of two key steps: first, the conversion of natural language instructions into Linear Temporal Logic (LTL), followed by a comprehensive analysis of action sequences. The module uses the reasoning capabilities of the LLM to evaluate logical coherence and identify potential gaps in the plan. Rigorous testing on datasets of varying complexity demonstrates the broad applicability of the module to household tasks. We contribute to improving the reliability and efficiency of task planning and addresses the critical need for robust pre-execution verification in autonomous systems. The code is available at <a target="_blank" rel="noopener" href="https://verifyllm.github.io/">https://verifyllm.github.io</a>. </p>
<blockquote>
<p>在机器人领域，研究者面临一个确保可靠高效的任务规划的重大挑战。在执行任务之前对高级任务计划进行验证，可以大大减少错误并增强系统的整体性能。在本文中，我们提出了一种在模拟器或真实环境中执行高级任务计划之前自动验证这些计划的架构。通过利用大型语言模型（LLM），我们的方法分为两个关键步骤：首先，将自然语言指令转换为线性时序逻辑（LTL），然后对动作序列进行全面分析。该模块使用LLM的推理能力来评估逻辑连贯性并识别计划中可能存在的差距。在多种复杂数据集上的严格测试证明了该模块在家庭任务中的广泛应用性。我们为提高任务规划和执行的可靠性和效率做出了贡献，并解决了自主系统中对鲁棒的预执行验证的迫切需求。代码可在<a target="_blank" rel="noopener" href="https://verifyllm.github.io获取./">https://verifyllm.github.io获取。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.05118v1">PDF</a> IROS 2025</p>
<p><strong>Summary</strong></p>
<p>在机器人领域，对任务规划进行可靠且高效的验证是保证系统性能的关键。本文提出了一种在模拟器或真实环境中自动验证高级任务计划的架构。该架构利用大型语言模型（LLM）进行自然语言指令的转换，并通过线性时序逻辑（LTL）对动作序列进行全面分析。利用LLM的推理能力评估逻辑连贯性并识别计划中的潜在漏洞。测试证明该模块具有广泛的适用性，为提高任务规划的可靠性和效率做出贡献，满足自主系统的关键预执行验证需求。相关代码可在“<a target="_blank" rel="noopener" href="https://verifyllm.github.io/">链接</a>”获取。</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>研究领域聚焦于机器人技术中的任务规划验证。</li>
<li>提出一种基于大型语言模型的自动验证架构。</li>
<li>架构包含两个关键步骤：自然语言指令转换为线性时序逻辑和动作序列的全面分析。</li>
<li>利用LLM的推理能力评估逻辑连贯性。</li>
<li>架构能够在模拟器或真实环境中进行预执行验证。</li>
<li>测试证明了该模块在多种复杂任务中的适用性。</li>
<li>该研究有助于提高任务规划的可靠性和效率。</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.05118">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-deae4d7afe21a19531df05014c477d80.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b5cb1e7cc1885f6b2183b801f6c93746.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d8e2c649030f846685d0560682aa34f9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-82e76d843d48719c4af25d9628f3d36d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8fd392510c6d5254957bbc9257bbd63d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-305c0671e298a1ad42b5f252c2e81ceb.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-50f90dec641b65efea6019441bc217f8.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="What-Shapes-User-Trust-in-ChatGPT-A-Mixed-Methods-Study-of-User-Attributes-Trust-Dimensions-Task-Context-and-Societal-Perceptions-among-University-Students"><a href="#What-Shapes-User-Trust-in-ChatGPT-A-Mixed-Methods-Study-of-User-Attributes-Trust-Dimensions-Task-Context-and-Societal-Perceptions-among-University-Students" class="headerlink" title="What Shapes User Trust in ChatGPT? A Mixed-Methods Study of User   Attributes, Trust Dimensions, Task Context, and Societal Perceptions among   University Students"></a>What Shapes User Trust in ChatGPT? A Mixed-Methods Study of User   Attributes, Trust Dimensions, Task Context, and Societal Perceptions among   University Students</h2><p><strong>Authors:Kadija Bouyzourn, Alexandra Birch</strong></p>
<p>This mixed-methods inquiry examined four domains that shape university students’ trust in ChatGPT: user attributes, seven delineated trust dimensions, task context, and perceived societal impact. Data were collected through a survey of 115 UK undergraduate and postgraduate students and four complementary semi-structured interviews. Behavioural engagement outweighed demographics: frequent use increased trust, whereas self-reported understanding of large-language-model mechanics reduced it. Among the dimensions, perceived expertise and ethical risk were the strongest predictors of overall trust; ease of use and transparency had secondary effects, while human-likeness and reputation were non-significant. Trust was highly task-contingent; highest for coding and summarising, lowest for entertainment and citation generation, yet confidence in ChatGPT’s referencing ability, despite known inaccuracies, was the single strongest correlate of global trust, indicating automation bias. Computer-science students surpassed peers only in trusting the system for proofreading and writing, suggesting technical expertise refines rather than inflates reliance. Finally, students who viewed AI’s societal impact positively reported the greatest trust, whereas mixed or negative outlooks dampened confidence. These findings show that trust in ChatGPT hinges on task verifiability, perceived competence, ethical alignment and direct experience, and they underscore the need for transparency, accuracy cues and user education when deploying LLMs in academic settings. </p>
<blockquote>
<p>这项混合方法研究调查了影响大学生对ChatGPT信任程度的四个领域：用户属性、七个划分的信任维度、任务上下文和感知的社会影响。数据是通过一项针对英国115名本科和研究生学生的调查以及四次补充的半结构化访谈收集的。行为参与度超过了人口统计学特征：频繁使用增加了信任度，而自我报告的对大型语言模型机制的理解则降低了信任度。在维度中，感知的专业知识和道德风险是整体信任的最强预测因素；易用性和透明度有次要影响，而人类亲和力和声誉则不具显著性。信任高度依赖于任务；对编码和总结的信任度最高，对娱乐和引用生成的信任度最低。尽管存在已知的不准确之处，但对ChatGPT引用能力的信任仍是全球信任中最强烈的单一相关因素，这显示了自动化偏见。计算机科学专业的学生仅在检查写作方面超越同龄人，这表明技术专业知识会细化而非夸大依赖。最后，那些对人工智能的社会影响持积极看法的学生报告的信任度最高，而混合或消极前景则会削弱信心。这些发现表明，对ChatGPT的信任取决于任务的可验证性、感知的能力、道德观念和直接经验，并强调在学术环境中部署大型语言模型时，需要透明度、准确性提示和用户教育。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.05046v1">PDF</a> 25 pages, 11 tables, 6 figures</p>
<p><strong>Summary</strong>：本研究通过混合方法研究，探讨了影响大学生对ChatGPT信任度的四个领域，包括用户属性、七个划分的信任维度、任务上下文和感知的社会影响。通过调查115名英国本科生和研究生以及四次补充的半结构化访谈收集数据。行为参与度比人口统计学更重要：频繁使用增加了信任度，而对自己对大型语言模型机制的理解减少了信任度。在维度中，感知的专业知识和道德风险是整体信任的最强预测因素；易用性和透明度有次要影响，而人性化和声誉则不具显著性。信任与任务密切相关；编码和总结方面最高，娱乐和引用生成方面最低。尽管存在已知的不准确之处，但对ChatGPT引用能力的信任仍是全球信任的最强烈相关因素，这表明存在自动化偏见。计算机科学专业的学生只在校对和写作方面超过同龄人，这表明技术专业知识会加强而不是夸大依赖。最后，认为人工智能社会影响积极的学生报告了最高的信任度，而混合或负面的看法削弱了信心。这些发现表明，对ChatGPT的信任取决于任务的可验证性、感知的能力、道德和直接经验。强调了在使用语言模型时需要透明度、准确性提示和用户教育的重要性。</p>
<p><strong>Key Takeaways</strong>：</p>
<ol>
<li>学生用户属性和行为参与度对ChatGPT的信任度有重要影响。频繁使用会增加信任度，对大型语言模型机制的理解则减少信任度。</li>
<li>在七个划分的信任维度中，感知的专业知识和道德风险是整体信任的最强预测因素。</li>
<li>任务上下文对ChatGPT的信任度有很大影响，编码和总结任务的信任度最高，娱乐和引用生成任务的信任度最低。</li>
<li>尽管存在准确性问题，但对ChatGPT引用能力的信任是全球信任的最强烈相关因素，这表明存在自动化偏见。</li>
<li>计算机科学专业的学生在证明阅读和写作方面对ChatGPT的信任度较高，表明技术专业知识有助于加强依赖。</li>
<li>学生对AI的社会影响的看法影响他们对ChatGPT的信任度。积极的社会影响观点会导致更高的信任度。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.05046">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-a7c5607c404d044d02d0a308e7369252.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-190356d8be62d21e2feefef94501d26e.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="MoLink-Distributed-and-Efficient-Serving-Framework-for-Large-Models"><a href="#MoLink-Distributed-and-Efficient-Serving-Framework-for-Large-Models" class="headerlink" title="MoLink: Distributed and Efficient Serving Framework for Large Models"></a>MoLink: Distributed and Efficient Serving Framework for Large Models</h2><p><strong>Authors:Lewei Jin, Yongqi Chen, Kui Zhang, Yifan Zhuo, Yi Gao, Bowei Yang, Zhengong Cai, Wei Dong</strong></p>
<p>Large language models represent a groundbreaking shift in generative AI. Yet, these advances come with a significant challenge: the high cost of model serving. To mitigate these costs, consumer-grade GPUs emerge as a more affordable alternative. This presents an opportunity for more cost-efficient LLM serving by leveraging these GPUs.   However, it is non-trivial to achieve high-efficiency LLM serving on consumer-grade GPUs, mainly due to two challenges: 1) these GPUs are often deployed in limited network conditions; 2) these GPUs often exhibit heterogeneity in host systems. To address these challenges, we present MoLink, a distributed LLM serving system for large models. It incorporates several key techniques, enabling efficient LLM serving on heterogeneous and weakly connected consumer-grade GPUs. Our experiments demonstrate that it achieves throughput improvements of up to 458% and cost-profit margin improvements of up to 151%, compared to state-of-the-art systems. MoLink allows users on Windows, Linux, and containerized VMs to seamlessly integrate GPUs with just a few lines of code over Ethernet or public networks. Currently, it supports 18 mainstream architectures of open-source large language models. </p>
<blockquote>
<p>大型语言模型代表了生成式人工智能的突破性进展。然而，这些进展伴随着一个巨大的挑战：模型服务的成本高昂。为了缓解这些成本，消费级GPU作为一种更经济的替代方案而出现。这为利用这些GPU实现更具成本效益的大型语言模型服务提供了机会。然而，在消费级GPU上实现高效的大型语言模型服务并非易事，主要面临两个挑战：1）这些GPU通常在网络条件有限的情况下部署；2）这些GPU在主机系统中经常表现出异质性。为了解决这些挑战，我们推出了MoLink，这是一个用于大型模型的分布式大型语言模型服务系统。它采用了几种关键技术，能够在异构和连接弱的消费级GPU上实现高效的大型语言模型服务。我们的实验表明，与最先进的系统相比，它实现了高达458%的吞吐量改进和高达151%的成本利润率改进。MoLink允许Windows、Linux和容器化VM的用户通过以太网或公共网络只需几行代码即可无缝集成GPU。目前，它支持18种主流开源大型语言模型架构。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.05043v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>大型语言模型（LLM）的突破带来了生成式AI的重大变革，但同时也面临着模型服务的高成本挑战。消费者级GPU作为更经济的选择，为降低LLM服务成本提供了机会。然而，在消费者级GPU上实现高效的LLM服务并非易事，主要面临网络条件有限和设备系统异质性的两大挑战。为解决这些问题，我们提出了MoLink分布式LLM服务系统，通过几项关键技术实现了在异构和弱连接的消费者级GPU上的高效LLM服务。实验表明，与现有系统相比，MoLink的吞吐量提升可达458%，成本利润率提升可达151%。它支持Windows、Linux和容器化VM，只需几行代码即可通过以太网或公共网络无缝集成GPU，并且兼容18种主流的开源大型语言模型架构。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>大型语言模型（LLM）推动了生成式AI的进步，但模型服务成本高。</li>
<li>消费者级GPU为降低LLM服务成本提供了机会。</li>
<li>在消费者级GPU上实现高效LLM服务面临网络条件有限和设备系统异质性两大挑战。</li>
<li>MoLink系统通过几项关键技术解决了这些挑战，实现了高效LLM服务。</li>
<li>MoLink系统吞吐量提升显著，成本利润率也有明显提升。</li>
<li>MoLink支持Windows、Linux和容器化VM，易于集成GPU。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.05043">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-904557b65716e5bbd365259121298230.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1eb9fe149152d7718a5f0e49244aa11b.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-4b9983fa558fa2c1069cd4cb3ff88e80.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-73f19c5d02fc64be0217fa7a98c0cf67.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-cf04698e82feb52a73825eea9cce550b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b546c08e8335f7a1fdf8e4dba0d600c0.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-912f6248ea84e67dfc2b685fc5912f59.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="A-Comparative-Study-of-Specialized-LLMs-as-Dense-Retrievers"><a href="#A-Comparative-Study-of-Specialized-LLMs-as-Dense-Retrievers" class="headerlink" title="A Comparative Study of Specialized LLMs as Dense Retrievers"></a>A Comparative Study of Specialized LLMs as Dense Retrievers</h2><p><strong>Authors:Hengran Zhang, Keping Bi, Jiafeng Guo</strong></p>
<p>While large language models (LLMs) are increasingly deployed as dense retrievers, the impact of their domain-specific specialization on retrieval effectiveness remains underexplored. This investigation systematically examines how task-specific adaptations in LLMs influence their retrieval capabilities, an essential step toward developing unified retrievers capable of handling text, code, images, and multimodal content. We conduct extensive experiments with eight Qwen2.5 7B LLMs, including base, instruction-tuned, code&#x2F;math-specialized, long reasoning, and vision-language models across zero-shot retrieval settings and the supervised setting. For the zero-shot retrieval settings, we consider text retrieval from the BEIR benchmark and code retrieval from the CoIR benchmark. Further, to evaluate supervised performance, all LLMs are fine-tuned on the MS MARCO dataset. We find that mathematical specialization and the long reasoning capability cause consistent degradation in three settings, indicating conflicts between mathematical reasoning and semantic matching. The vision-language model and code-specialized LLMs demonstrate superior zero-shot performance compared to other LLMs, even surpassing BM25 on the code retrieval task, and maintain comparable performance to base LLMs in supervised settings. These findings suggest promising directions for the unified retrieval task leveraging cross-domain and cross-modal fusion. </p>
<blockquote>
<p>随着大型语言模型（LLM）越来越多地被部署为密集检索器，其针对特定领域的专业化对检索效果的影响尚未得到充分探索。这项研究系统地探讨了LLM中的任务特定适应如何影响其检索能力，这是朝着开发能够处理文本、代码、图像和多模态内容的统一检索器迈出的重要一步。我们在八个Qwen2.5 7B LLM上进行了广泛实验，包括基础模型、指令调优模型、针对代码&#x2F;数学的专用模型、长期推理模型和视觉语言模型，涵盖了零射击检索设置和有监督设置。对于零射击检索设置，我们考虑从BEIR基准测试中进行文本检索，从CoIR基准测试中进行代码检索。此外，为了评估监督性能，所有LLM都在MS MARCO数据集上进行微调。我们发现数学专业化和长期推理能力会在三种设置下导致持续的退化，表明数学推理和语义匹配之间存在冲突。视觉语言模型和针对代码专用的LLM在零射击场景下的性能表现优于其他LLM，甚至在代码检索任务上超越了BM25，并且在有监督设置下保持与基础LLM相当的性能。这些发现表明，利用跨域和跨模态融合的统一检索任务具有广阔的发展前景。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.03958v1">PDF</a> Accepted by CCIR25 and published by Springer LNCS or LNAI</p>
<p><strong>Summary</strong></p>
<p>大型语言模型（LLM）作为密集检索器的应用越来越广泛，但其领域特定专业化对检索效果的影响尚未得到充分研究。本研究系统地探讨了LLM的任务特定适应性对其检索能力的影响，这是开发能够处理文本、代码、图像和多模态内容的统一检索器的关键步骤。通过对八种不同专业领域的LLM进行广泛实验，包括基础模型、指令调优模型、代码&#x2F;数学专业模型、逻辑推理模型和视觉语言模型，在零样本检索设置和监管设置下，发现数学专业化和逻辑推理能力在三中设置下表现出持续退步。相反，视觉语言模型和代码专业LLM在零样本表现优于其他LLM，在代码检索任务上甚至超过了BM25，并在监督设置下保持了与基础模型相当的性能。这些发现表明，利用跨域和跨模态融合的统一检索任务具有广阔的发展前景。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>大型语言模型（LLM）作为密集检索器的应用越来越广泛，但其领域特定专业化对检索效果的影响尚未充分研究。</li>
<li>LLM的任务特定适应性对其检索能力有重要影响。</li>
<li>数学专业化和逻辑推理能力在某些设置下可能导致LLM的检索性能下降。</li>
<li>视觉语言模型和代码专业LLM在零样本检索中表现优异，特别是在代码检索任务上。</li>
<li>视觉语言模型和代码专业LLM在监督设置下的性能与基础模型相当。</li>
<li>统一检索任务需要考虑到跨域和跨模态融合的重要性。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.03958">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-ffb853bcbb83504fc0261e28b3c54ebd.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="Demystifying-ChatGPT-How-It-Masters-Genre-Recognition"><a href="#Demystifying-ChatGPT-How-It-Masters-Genre-Recognition" class="headerlink" title="Demystifying ChatGPT: How It Masters Genre Recognition"></a>Demystifying ChatGPT: How It Masters Genre Recognition</h2><p><strong>Authors:Subham Raj, Sriparna Saha, Brijraj Singh, Niranjan Pedanekar</strong></p>
<p>The introduction of ChatGPT has garnered significant attention within the NLP community and beyond. Previous studies have demonstrated ChatGPT’s substantial advancements across various downstream NLP tasks, highlighting its adaptability and potential to revolutionize language-related applications. However, its capabilities and limitations in genre prediction remain unclear. This work analyzes three Large Language Models (LLMs) using the MovieLens-100K dataset to assess their genre prediction capabilities. Our findings show that ChatGPT, without fine-tuning, outperformed other LLMs, and fine-tuned ChatGPT performed best overall. We set up zero-shot and few-shot prompts using audio transcripts&#x2F;subtitles from movie trailers in the MovieLens-100K dataset, covering 1682 movies of 18 genres, where each movie can have multiple genres. Additionally, we extended our study by extracting IMDb movie posters to utilize a Vision Language Model (VLM) with prompts for poster information. This fine-grained information was used to enhance existing LLM prompts. In conclusion, our study reveals ChatGPT’s remarkable genre prediction capabilities, surpassing other language models. The integration of VLM further enhances our findings, showcasing ChatGPT’s potential for content-related applications by incorporating visual information from movie posters. </p>
<blockquote>
<p>ChatGPT的引入在NLP领域内外都引起了极大的关注。先前的研究已经证明了ChatGPT在各种下游NLP任务上的巨大进步，突出了其适应性和改变语言相关应用的可能性。然而，其在体裁预测方面的能力和局限性尚不清楚。本研究使用MovieLens-100K数据集分析了三款大型语言模型（LLM）的体裁预测能力。我们的研究发现，未经微调的ChatGPT在其他LLM中表现最佳，而经过微调的ChatGPT总体表现最佳。我们使用MovieLens-100K数据集中的电影预告片音频转录&#x2F;字幕来设置零样本和少样本提示，涵盖了18个体裁的1682部电影，每部电影可能有多个体裁。此外，我们还通过提取IMDb电影海报来扩展我们的研究，使用视觉语言模型（VLM）的提示进行海报信息分析。这种精细的信息被用来增强现有的LLM提示。总之，我们的研究表明，ChatGPT在体裁预测方面表现出卓越的能力，超越了其他语言模型。VLM的集成进一步增强了我们的发现，展示了ChatGPT通过结合电影海报的视觉信息在内容相关应用中的潜力。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.03875v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>ChatGPT在NLP领域引起了广泛关注，并展现出强大的性能。本研究使用MovieLens-100K数据集分析了三种大型语言模型（LLM）的体裁预测能力。结果显示，未经微调ChatGPT表现优于其他LLM，而经过微调的ChatGPT表现最佳。此外，本研究还结合了视觉语言模型（VLM），通过电影海报的提示信息增强了LLM的提示，进一步突出了ChatGPT在内容相关应用中的潜力。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ChatGPT在NLP领域受到广泛关注，其性能显著。</li>
<li>研究采用MovieLens-100K数据集分析了三种LLM的体裁预测能力。</li>
<li>未经调教的ChatGPT在体裁预测方面表现优于其他LLM。</li>
<li>经过调教的ChatGPT表现最佳，体现了其强大的潜力。</li>
<li>研究结合了VLM，通过电影海报的提示信息增强了LLM的提示。</li>
<li>ChatGPT在内容相关应用中具有巨大潜力，能够结合视觉信息进行体裁预测。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.03875">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-27efcc82d2cee0f9cc535ba3b62d00be.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-74692b12176bdd70d576cb3c11d87547.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7b659fad49ae07a8d8a35353690972e3.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-22b7bf1cc259b6f56f4df83301e12a2c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c6ebb50762313d6b987770b9a758ca70.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f31ac2993af3defcc99b03fdffa3e560.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="From-Query-to-Explanation-Uni-RAG-for-Multi-Modal-Retrieval-Augmented-Learning-in-STEM"><a href="#From-Query-to-Explanation-Uni-RAG-for-Multi-Modal-Retrieval-Augmented-Learning-in-STEM" class="headerlink" title="From Query to Explanation: Uni-RAG for Multi-Modal Retrieval-Augmented   Learning in STEM"></a>From Query to Explanation: Uni-RAG for Multi-Modal Retrieval-Augmented   Learning in STEM</h2><p><strong>Authors:Xinyi Wu, Yanhao Jia, Luwei Xiao, Shuai Zhao, Fengkuang Chiang, Erik Cambria</strong></p>
<p>In AI-facilitated teaching, leveraging various query styles to interpret abstract educational content is crucial for delivering effective and accessible learning experiences. However, existing retrieval systems predominantly focus on natural text-image matching and lack the capacity to address the diversity and ambiguity inherent in real-world educational scenarios. To address this limitation, we develop a lightweight and efficient multi-modal retrieval module, named Uni-Retrieval, which extracts query-style prototypes and dynamically matches them with tokens from a continually updated Prompt Bank. This Prompt Bank encodes and stores domain-specific knowledge by leveraging a Mixture-of-Expert Low-Rank Adaptation (MoE-LoRA) module and can be adapted to enhance Uni-Retrieval’s capability to accommodate unseen query types at test time. To enable natural language educational content generation, we integrate the original Uni-Retrieval with a compact instruction-tuned language model, forming a complete retrieval-augmented generation pipeline named Uni-RAG. Given a style-conditioned query, Uni-RAG first retrieves relevant educational materials and then generates human-readable explanations, feedback, or instructional content aligned with the learning objective. Experimental results on SER and other multi-modal benchmarks show that Uni-RAG outperforms baseline retrieval and RAG systems in both retrieval accuracy and generation quality, while maintaining low computational cost. Our framework provides a scalable, pedagogically grounded solution for intelligent educational systems, bridging retrieval and generation to support personalized, explainable, and efficient learning assistance across diverse STEM scenarios. </p>
<blockquote>
<p>在人工智能辅助教学中，利用不同的查询风格来解释抽象的教育内容对于提供有效且可访问的学习体验至关重要。然而，现有的检索系统主要关注自然语言文本与图像的匹配，缺乏应对现实世界教育场景中固有的多样性和模糊性的能力。为了解决这一局限性，我们开发了一个轻便高效的多模态检索模块，名为Uni-Retrieval。该模块能够提取查询样式原型，并与来自持续更新的Prompt Bank中的标记动态匹配。Prompt Bank通过利用混合专家低秩适应（MoE-LoRA）模块来编码和存储领域特定知识，并且能够适应以增强Uni-Retrieval在测试时适应未见过的查询类型的能力。为了实现自然语言教育内容的生成，我们将原始的Uni-Retrieval与经过指令调整的小型语言模型集成，形成了一个完整的检索增强生成管道，称为Uni-RAG。给定风格控制的查询，Uni-RAG首先检索相关的教育材料，然后生成与学习目标对齐的可读解释、反馈或指令内容。在SER和其他多模态基准测试上的实验结果表明，Uni-RAG在检索准确性和生成质量方面都优于基线检索和RAG系统，同时保持较低的计算成本。我们的框架为智能教育系统提供了一个可扩展的、以教学法为基础解决方案，通过桥接检索和生成来支持个性化的、可解释的、高效的学习辅助，涵盖多样化的STEM场景。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.03868v1">PDF</a> </p>
<p><strong>总结</strong><br>在AI辅助教学领域，采用不同查询风格来解读抽象的教育内容对于提供有效且可访问的学习体验至关重要。然而，现有的检索系统主要关注自然文本图像匹配，缺乏应对现实世界教育场景中多样性和模糊性的能力。为解决此局限，我们开发了一个轻便高效的多模式检索模块，名为Uni-Retrieval，该模块能够提取查询样式原型并与来自持续更新的Prompt Bank的标记进行动态匹配。Prompt Bank通过利用混合专家低阶适应（MoE-LoRA）模块来编码和存储特定领域的知识，并可在测试时适应以增强Uni-Retrieval适应未见查询类型的能力。为实现自然语言教育内容生成，我们将原始的Uni-Retrieval与紧凑的指令调整语言模型集成，形成一个完整的检索增强生成管道，称为Uni-RAG。给定风格条件下的查询，Uni-RAG首先检索相关的教育材料，然后生成与学习目标对齐的人类可读解释、反馈或教学内容。在SER和其他多模式基准测试上的实验结果表明，Uni-RAG在检索准确性和生成质量方面均优于基线检索和RAG系统，同时保持较低的计算成本。我们的框架为智能教育系统提供了可扩展的、以教学为基础解决方案，通过桥接检索和生成来支持个性化、可解释和高效的学习辅助，适用于各种STEM场景。</p>
<p><strong>要点</strong></p>
<ol>
<li>在AI辅助教学中，利用不同查询风格解读抽象教育内容至关重要。</li>
<li>现有检索系统主要关注自然文本图像匹配，缺乏应对教育场景多样性和模糊性的能力。</li>
<li>Uni-Retrieval是一种新的多模式检索模块，可以提取查询样式原型并与Prompt Bank中的标记动态匹配。</li>
<li>Prompt Bank利用MoE-LoRA模块编码和存储特定领域知识，并适应增强Uni-Retrieval的能力。</li>
<li>Uni-RAG集成了Uni-Retrieval和指令调整语言模型，形成完整的检索增强生成管道。</li>
<li>Uni-RAG通过检索相关教育材料和生成人类可读内容来支持个性化、可解释和高效的学习。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.03868">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-451755b5af82f9784ac09760107d2246.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0127829e091d3d01b5de1ce6dd90ff6e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-1f9904c2b89f1f3962444e77f0b16f78.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-82f056c4fdbcc9e6a8fb51199a040d97.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="ChestGPT-Integrating-Large-Language-Models-and-Vision-Transformers-for-Disease-Detection-and-Localization-in-Chest-X-Rays"><a href="#ChestGPT-Integrating-Large-Language-Models-and-Vision-Transformers-for-Disease-Detection-and-Localization-in-Chest-X-Rays" class="headerlink" title="ChestGPT: Integrating Large Language Models and Vision Transformers for   Disease Detection and Localization in Chest X-Rays"></a>ChestGPT: Integrating Large Language Models and Vision Transformers for   Disease Detection and Localization in Chest X-Rays</h2><p><strong>Authors:Shehroz S. Khan, Petar Przulj, Ahmed Ashraf, Ali Abedi</strong></p>
<p>The global demand for radiologists is increasing rapidly due to a growing reliance on medical imaging services, while the supply of radiologists is not keeping pace. Advances in computer vision and image processing technologies present significant potential to address this gap by enhancing radiologists’ capabilities and improving diagnostic accuracy. Large language models (LLMs), particularly generative pre-trained transformers (GPTs), have become the primary approach for understanding and generating textual data. In parallel, vision transformers (ViTs) have proven effective at converting visual data into a format that LLMs can process efficiently. In this paper, we present ChestGPT, a deep-learning framework that integrates the EVA ViT with the Llama 2 LLM to classify diseases and localize regions of interest in chest X-ray images. The ViT converts X-ray images into tokens, which are then fed, together with engineered prompts, into the LLM, enabling joint classification and localization of diseases. This approach incorporates transfer learning techniques to enhance both explainability and performance. The proposed method achieved strong global disease classification performance on the VinDr-CXR dataset, with an F1 score of 0.76, and successfully localized pathologies by generating bounding boxes around the regions of interest. We also outline several task-specific prompts, in addition to general-purpose prompts, for scenarios radiologists might encounter. Overall, this framework offers an assistive tool that can lighten radiologists’ workload by providing preliminary findings and regions of interest to facilitate their diagnostic process. </p>
<blockquote>
<p>随着对医学影像服务依赖程度的不断增加，全球对放射科医师的需求迅速增长，然而放射科医师的供给却跟不上这一需求。计算机视觉和图像处理技术的进步为解决这一差距提供了巨大潜力，通过增强放射科医师的能力和提高诊断准确性来实现。大型语言模型（LLM），特别是预训练生成式转换器（GPT），已成为理解和生成文本数据的主要方法。同时，视觉转换器（ViT）在将视觉数据转换为LLM可以高效处理的形式方面表现出色。在本文中，我们提出了ChestGPT，这是一个深度学习框架，它将EVA ViT与Llama 2 LLM相结合，用于对胸部X射线图像中的疾病进行分类并定位感兴趣区域。ViT将X射线图像转换为令牌，然后这些令牌与工程提示一起输入到LLM中，以实现疾病的联合分类和定位。这种方法结合了迁移学习技术，以提高可解释性和性能。所提出的方法在VinDr-CXR数据集上取得了强大的全球疾病分类性能，F1分数为0.76，并通过围绕感兴趣区域生成边界框成功地定位了病理。除了通用提示外，我们还概述了针对放射科医生可能遇到的各种情景的特定任务提示。总的来说，该框架提供了一个辅助工具，可以通过提供初步发现和感兴趣区域来减轻放射科医生的工作量，从而帮助他们进行诊断过程。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.03739v1">PDF</a> 8 pages, 5 figures, 4 tables</p>
<p><strong>Summary</strong><br>     随着对医学影像服务依赖度不断增加，全球对放射科医生的需求迅速增长，而放射科医生供给不足。计算机视觉和图像处理技术的进展为解决这一差距提供了巨大潜力，通过增强放射科医生的能力和提高诊断准确性。本文介绍了一个深度学习框架ChestGPT，它结合了EVA ViT和Llama 2 LLM，用于对胸部X射线图像的疾病进行分类并定位感兴趣区域。该框架使用ViT将X射线图像转换为令牌，然后与工程提示一起输入LLM，实现疾病的联合分类和定位。该方法采用迁移学习技术，提高了可解释性和性能。在VinDr-CXR数据集上，该方法实现了强大的全球疾病分类性能，F1分数为0.76，并成功定位了病理区域。总体而言，该框架为放射科医生提供了一种辅助工具，可通过提供初步检查结果和感兴趣区域，减轻他们的工作量并促进诊断过程。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>全球对放射科医生的需求迅速增长，而供给不足，需要新技术协助解决这一差距。</li>
<li>计算机视觉和图像处理技术有助于提高放射科医生的诊断能力和准确性。</li>
<li>LLMs（特别是GPTs）和ViTs在处理医学图像数据方面表现出巨大潜力。</li>
<li>ChestGPT是一个结合EVA ViT和Llama 2 LLM的深度学习框架，用于对胸部X射线图像的疾病进行分类和定位。</li>
<li>该框架使用迁移学习技术来提高性能并增强解释性。</li>
<li>在VinDr-CXR数据集上，ChestGPT实现了F1分数为0.76的出色性能。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.03739">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-db88ede29c1abd31b63991df0ebb5723.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-2ed04ad5f23d1db131702c8528519669.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8d800fea9816b4ffe409770b295f2590.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-71cbb5a4649418693f278bd593808692.jpg" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="Beyond-SEO-A-Transformer-Based-Approach-for-Reinventing-Web-Content-Optimisation"><a href="#Beyond-SEO-A-Transformer-Based-Approach-for-Reinventing-Web-Content-Optimisation" class="headerlink" title="Beyond SEO: A Transformer-Based Approach for Reinventing Web Content   Optimisation"></a>Beyond SEO: A Transformer-Based Approach for Reinventing Web Content   Optimisation</h2><p><strong>Authors:Florian Lüttgenau, Imar Colic, Gervasio Ramirez</strong></p>
<p>The rise of generative AI search engines is disrupting traditional SEO, with Gartner predicting 25% reduction in conventional search usage by 2026. This necessitates new approaches for web content visibility in AI-driven search environments. We present a domain-specific fine-tuning approach for Generative Engine Optimization (GEO) that transforms web content to improve discoverability in large language model outputs. Our method fine-tunes a BART-base transformer on synthetically generated training data comprising 1,905 cleaned travel website content pairs. Each pair consists of raw website text and its GEO-optimized counterpart incorporating credible citations, statistical evidence, and improved linguistic fluency. We evaluate using intrinsic metrics (ROUGE-L, BLEU) and extrinsic visibility assessments through controlled experiments with Llama-3.3-70B. The fine-tuned model achieves significant improvements over baseline BART: ROUGE-L scores of 0.249 (vs. 0.226) and BLEU scores of 0.200 (vs. 0.173). Most importantly, optimized content demonstrates substantial visibility gains in generative search responses with 15.63% improvement in absolute word count and 30.96% improvement in position-adjusted word count metrics. This work provides the first empirical demonstration that targeted transformer fine-tuning can effectively enhance web content visibility in generative search engines with modest computational resources. Our results suggest GEO represents a tractable approach for content optimization in the AI-driven search landscape, offering concrete evidence that small-scale, domain-focused fine-tuning yields meaningful improvements in content discoverability. </p>
<blockquote>
<p>生成式AI搜索引擎的崛起正在颠覆传统的SEO，据加特纳预测，到2026年，传统搜索使用率将减少25%。这要求在AI驱动的搜索环境中采用新的方法来提高网页内容的可见性。我们提出了一种针对生成引擎优化（GEO）的特定领域微调方法，该方法可以转换网页内容，以提高在大语言模型输出中的可发现性。我们的方法对基于BART的基础转换器进行微调，使用合成生成的训练数据，包含1905组经过清理的旅行网站内容。每对内容都包含原始网站文本和其经过GEO优化的对应文本，后者融入了可信的引用、统计证据和更流畅的语言表达。我们采用内在指标（ROUGE-L，BLEU）和通过Llama-3.3-70B进行的外在可见性评估实验来评估其效果。经过微调的模型在基线BART上实现了显著改进：ROUGE-L得分为0.249（对比基线模型的0.226），BLEU得分为0.200（对比基线模型的0.173）。最重要的是，优化后的内容在生成式搜索响应中显示出显著的可见性提升，绝对字数增加了15.63%，位置调整后的字数指标增加了30.96%。这项工作首次实证了有针对性的转换器微调可以有效地提高在生成式搜索引擎中的网页内容可见性，并且只需要适度的计算资源。我们的结果表明，GEO是AI驱动搜索环境中内容优化的可行方法，提供了具体证据表明小规模、针对特定领域的微调可以在内容可发现性方面实现有意义的改进。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.03169v1">PDF</a> 9 pages, 3 figures</p>
<p><strong>摘要</strong><br>    随着生成式AI搜索引擎的兴起，传统SEO面临颠覆。预计到2026年，传统搜索使用率将减少25%。为适应AI驱动搜索环境，提出领域特定的微调方法，用于生成式引擎优化（GEO）。通过对合成训练数据的1905组旅游网站内容的清洗和配对，对BART基础转换器进行微调。每对内容包含原始网站文本和经过GEO优化的对应文本，同时融入可信引用、统计证据和改进的语言流畅性。通过内在度量（ROUGE-L，BLEU）和外在可见性评估实验，对比Llama-3.3-70B模型，微调模型在基线BART上实现了显著改进：ROUGE-L得分0.249（对比0.226），BLEU得分0.200（对比0.173）。最重要的是，优化内容在生成式搜索响应中表现出更高的可见性，绝对词数增加15.63%，位置调整词数增加30.96%。本研究首次实证，有针对性的转换器微调能有效提高AI驱动搜索引擎中网页内容的可见性，利用有限的计算资源即可实现内容优化。</p>
<p><strong>关键见解</strong></p>
<ol>
<li>生成式AI搜索引擎的崛起正在改变传统的SEO策略。</li>
<li>到2026年，预计传统搜索的使用率将减少25%。</li>
<li>提出了一种针对特定领域的微调方法——生成式引擎优化（GEO），以适应AI驱动的搜索环境。</li>
<li>通过合成训练数据对BART基础转换器进行微调，提高了网站内容在搜索引擎中的可见性。</li>
<li>优化后的内容在生成式搜索响应中表现出更高的可见性，绝对词数和位置调整词数均有显著提高。</li>
<li>研究显示，有针对性的转换器微调可以有效提高网页内容在AI驱动搜索引擎中的可见性。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.03169">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-76364267e87ff66502daa45d418fe545.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f1e73dd662d0d157b4856cf9e1e42470.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-1bcf44cc766ce6f8acf0ade413a96bf6.jpg" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="BERT4Traj-Transformer-Based-Trajectory-Reconstruction-for-Sparse-Mobility-Data"><a href="#BERT4Traj-Transformer-Based-Trajectory-Reconstruction-for-Sparse-Mobility-Data" class="headerlink" title="BERT4Traj: Transformer Based Trajectory Reconstruction for Sparse   Mobility Data"></a>BERT4Traj: Transformer Based Trajectory Reconstruction for Sparse   Mobility Data</h2><p><strong>Authors:Hao Yang, Angela Yao, Christopher Whalen, Gengchen Mai</strong></p>
<p>Understanding human mobility is essential for applications in public health, transportation, and urban planning. However, mobility data often suffers from sparsity due to limitations in data collection methods, such as infrequent GPS sampling or call detail record (CDR) data that only capture locations during communication events. To address this challenge, we propose BERT4Traj, a transformer based model that reconstructs complete mobility trajectories by predicting hidden visits in sparse movement sequences. Inspired by BERT’s masked language modeling objective and self_attention mechanisms, BERT4Traj leverages spatial embeddings, temporal embeddings, and contextual background features such as demographics and anchor points. We evaluate BERT4Traj on real world CDR and GPS datasets collected in Kampala, Uganda, demonstrating that our approach significantly outperforms traditional models such as Markov Chains, KNN, RNNs, and LSTMs. Our results show that BERT4Traj effectively reconstructs detailed and continuous mobility trajectories, enhancing insights into human movement patterns. </p>
<blockquote>
<p>理解人类移动性对于公共卫生、交通运输和城市规划等领域的应用至关重要。然而，由于数据收集方法的局限性，如GPS采样频率低或仅能在通信事件期间捕获位置的呼叫详细记录（CDR）数据，移动性数据常常存在稀疏性问题。为了应对这一挑战，我们提出了BERT4Traj，这是一个基于变压器的模型，通过预测稀疏移动序列中的隐藏访问点来重建完整的移动轨迹。BERT4Traj受到BERT的掩码语言建模目标和自我注意机制的启发，利用空间嵌入、时间嵌入以及如人口统计学和锚点等上下文背景特征。我们在乌干达坎帕拉收集的真实世界CDR和GPS数据集上评估了BERT4Traj，结果表明我们的方法显著优于传统模型，如马尔可夫链、KNN、RNNs和LSTM。我们的结果表明，BERT4Traj能够有效地重建详细且连续的移动轨迹，从而加深对人类移动模式的洞察。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.03062v1">PDF</a> This paper was accepted at GIScience 2025</p>
<p><strong>摘要</strong></p>
<p>文本指出理解人类移动性对于公共卫生、交通运输和城市规划等应用至关重要。然而，由于数据收集方法的局限性，如GPS采样不频繁或仅能在通信事件期间捕获位置的呼叫详细记录（CDR）数据等，移动性数据常常存在稀疏性问题。为应对这一挑战，文本提出了BERT4Traj模型，它通过预测稀疏移动序列中的隐藏访问点来重建完整的移动轨迹。BERT4Traj借鉴了BERT的掩码语言建模目标和自我关注机制，并利用空间嵌入、时间嵌入以及背景特征如人口统计学和锚点等。文本通过乌干达坎帕拉市的现实CDR和GPS数据集对BERT4Traj进行评估，证明了其在传统的Markov链、KNN、RNN和LSTM模型中的优越性。结果显示，BERT4Traj能有效重建详细且连续的移动轨迹，有助于深入了解人类移动模式。</p>
<p><strong>关键见解</strong></p>
<ol>
<li>理解人类移动性对于多个领域的应用至关重要，包括公共卫生、交通运输和城市规划。</li>
<li>移动性数据经常因数据收集方法的局限性而稀疏。</li>
<li>BERT4Traj是一种基于变压器的模型，能够重建完整的移动轨迹，通过预测稀疏移动序列中的隐藏访问点。</li>
<li>BERT4Traj借鉴了BERT的掩码语言建模目标和自我关注机制。</li>
<li>BERT4Traj利用空间嵌入、时间嵌入以及背景特征如人口统计学和锚点。</li>
<li>在现实世界的CDR和GPS数据集上，BERT4Traj的表现显著优于传统模型。</li>
<li>BERT4Traj能重建详细且连续的移动轨迹，有助于深入了解人类移动模式。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.03062">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-187540250cfdeb84e3753e8ab688a63a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-4572f18042ac658fe722fa87230792e2.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e5750dbfa6e61ef0c4d87777d44fe016.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-2a68b7d896b6ea3f3fb552249c468b2f.jpg" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1><h2 id="AIGI-Holmes-Towards-Explainable-and-Generalizable-AI-Generated-Image-Detection-via-Multimodal-Large-Language-Models"><a href="#AIGI-Holmes-Towards-Explainable-and-Generalizable-AI-Generated-Image-Detection-via-Multimodal-Large-Language-Models" class="headerlink" title="AIGI-Holmes: Towards Explainable and Generalizable AI-Generated Image   Detection via Multimodal Large Language Models"></a>AIGI-Holmes: Towards Explainable and Generalizable AI-Generated Image   Detection via Multimodal Large Language Models</h2><p><strong>Authors:Ziyin Zhou, Yunpeng Luo, Yuanchen Wu, Ke Sun, Jiayi Ji, Ke Yan, Shouhong Ding, Xiaoshuai Sun, Yunsheng Wu, Rongrong Ji</strong></p>
<p>The rapid development of AI-generated content (AIGC) technology has led to the misuse of highly realistic AI-generated images (AIGI) in spreading misinformation, posing a threat to public information security. Although existing AIGI detection techniques are generally effective, they face two issues: 1) a lack of human-verifiable explanations, and 2) a lack of generalization in the latest generation technology. To address these issues, we introduce a large-scale and comprehensive dataset, Holmes-Set, which includes the Holmes-SFTSet, an instruction-tuning dataset with explanations on whether images are AI-generated, and the Holmes-DPOSet, a human-aligned preference dataset. Our work introduces an efficient data annotation method called the Multi-Expert Jury, enhancing data generation through structured MLLM explanations and quality control via cross-model evaluation, expert defect filtering, and human preference modification. In addition, we propose Holmes Pipeline, a meticulously designed three-stage training framework comprising visual expert pre-training, supervised fine-tuning, and direct preference optimization. Holmes Pipeline adapts multimodal large language models (MLLMs) for AIGI detection while generating human-verifiable and human-aligned explanations, ultimately yielding our model AIGI-Holmes. During the inference stage, we introduce a collaborative decoding strategy that integrates the model perception of the visual expert with the semantic reasoning of MLLMs, further enhancing the generalization capabilities. Extensive experiments on three benchmarks validate the effectiveness of our AIGI-Holmes. </p>
<blockquote>
<p>人工智能生成内容（AIGC）技术的快速发展导致高度逼真的AI生成图像（AIGI）被滥用，传播错误信息，对公众信息安全构成威胁。尽管现有的AIGI检测技术通常有效，但它们面临两个问题：1）缺乏可验证的人为解释；2）最新技术中缺乏通用性。为了解决这些问题，我们引入了一个大规模且综合的数据集Holmes-Set，其中包括Holmes-SFTSet，这是一个带有解释指令的数据集，解释图像是否是AI生成的，以及Holmes-DPOSet，这是一个与人为偏好对齐的数据集。我们的工作引入了一种高效的数据注释方法，称为多专家陪审团，通过结构化的MLLM解释和质量控制来提高数据生成效率，质量控制包括跨模型评估、专家缺陷过滤和人为偏好修改。此外，我们提出了精心设计的三阶段训练框架Holmes Pipeline，包括视觉专家预训练、监督微调以及直接偏好优化。Holmes Pipeline使多模态大型语言模型（MLLMs）适应于AIGI检测，同时生成可验证的、与人为偏好对齐的解释，最终生成我们的模型AIGI-Holmes。在推理阶段，我们引入了一种协同解码策略，将视觉专家的模型感知与MLLMs的语义推理相结合，进一步提高模型的通用性。在三个基准上的大量实验验证了我们的AIGI-Holmes的有效性。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.02664v2">PDF</a> Accepted to ICCV 2025</p>
<p><strong>Summary</strong><br>     人工智能生成内容（AIGC）技术的快速发展导致AI生成图像（AIGI）被滥用，传播错误信息，威胁公众信息安全。针对现有AIGI检测技术在可验证解释性和最新技术普及性方面的问题，我们引入了大规模综合数据集Holmes-Set，包括带解释的指令调整数据集Holmes-SFTSet和人类对齐偏好数据集Holmes-DPOSet。我们提出一种高效的数据标注方法——多元专家陪审团，通过结构化MLLM解释和质量控制增强数据生成，包括跨模型评估、专家缺陷过滤和人类偏好修正。我们还设计了精心设计的三阶段训练框架Holmes Pipeline，包括视觉专家预训练、监督微调以及直接偏好优化等。我们的模型AIGI-Holmes在三个基准测试上进行了广泛实验验证，实现了人类可验证和符合人类偏好的解释。在推理阶段，我们采用协同解码策略，将视觉专家的感知与MLLM的语义推理相结合，进一步提高模型的泛化能力。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>AI-generated content (AIGC)技术的快速发展引发公众信息安全威胁，主要因为AI-generated images (AIGI)被滥用以传播错误信息。</li>
<li>现有AIGI检测技术虽一般有效，但缺乏人类可验证的解释性和最新技术的普及性。</li>
<li>引入大规模综合数据集Holmes-Set，包括带解释的指令调整数据集和人类对齐偏好数据集，以改善上述问题。</li>
<li>提出高效数据标注方法——多元专家陪审团，通过结构化MLLM解释和质量控制增强数据生成。</li>
<li>设计三阶段训练框架Holmes Pipeline，包括视觉专家预训练、监督微调以及直接偏好优化等，以训练AI模型进行AIGI检测。</li>
<li>在推理阶段采用协同解码策略，结合视觉专家感知与MLLM语义推理，提高模型的泛化能力。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.02664">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-4e80e266fadd191fd678cb20a98c7f6f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-36a2d26239472c644959493de853681e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-555ab8b2cdca9007b241b61c16d388c2.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-42a0f3ca8b6587a61ff2f5bad2640090.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-cd7ea3ea1f82521b7ce150fefa86af9e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-59e647b3c4d7e56df48c7b62475a98fc.jpg" align="middle">
</details>


<h1 id="-19"><a href="#-19" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        文章作者:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        文章链接:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-07-09/LLM/">https://kedreamix.github.io/Talk2Paper/Paper/2025-07-09/LLM/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        版权声明:
                    </i>
                </span>
                <span class="reprint-info">
                    本博客所有文章除特別声明外，均采用
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    许可协议。转载请注明来源
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/LLM/">
                                    <span class="chip bg-color">LLM</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;上一篇</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-07-09/Agent/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-4c5851f0c5fba71136fdaee90cf924fd.jpg" class="responsive-img" alt="Agent">
                        
                        <span class="card-title">Agent</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Agent 方向最新论文已更新，请持续关注 Update in 2025-07-09  Modeling Latent Partner Strategies for Adaptive Zero-Shot Human-Agent   Collaboration
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-07-09
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Agent/" class="post-category">
                                    Agent
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Agent/">
                        <span class="chip bg-color">Agent</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                下一篇&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-07-09/R1_Reasoning/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-5079c80e2fc378a16c514f6a221b54e7.jpg" class="responsive-img" alt="R1_Reasoning">
                        
                        <span class="card-title">R1_Reasoning</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            R1_Reasoning 方向最新论文已更新，请持续关注 Update in 2025-07-09  Spatio-Temporal LLM Reasoning about Environments and Actions
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-07-09
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/R1-Reasoning/" class="post-category">
                                    R1_Reasoning
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/R1-Reasoning/">
                        <span class="chip bg-color">R1_Reasoning</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- 代码块功能依赖 -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- 代码语言 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- 代码块复制 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- 代码块收缩 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC 悬浮按钮. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* 修复文章卡片 div 的宽度. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // 切换TOC目录展开收缩的相关操作.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;站点总字数:&nbsp;<span
                        class="white-color">27083.1k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- 运行天数提醒. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // 区分是否有年份.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = '本站已运行 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                daysTip = '本站已運行 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = '本站已运行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = '本站已運行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="关注我的知乎: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">知</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- 搜索遮罩框 -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span>
            <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- 白天和黑夜主题 -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- 回到顶部按钮 -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // 只在桌面版网页启用特效
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- 雪花特效 -->
    

    <!-- 鼠标星星特效 -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--腾讯兔小巢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- 动态标签 -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Σ(っ °Д °;)っ诶，页面崩溃了嘛？", clearTimeout(st)) : (document.title = "φ(゜▽゜*)♪咦，又好了！", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
