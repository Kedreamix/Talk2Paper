<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="LLM">
    <meta name="description" content="LLM æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-07-09  Beyond Simple Edits X-Planner for Complex Instruction-Based Image   Editing">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>LLM | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pic1.zhimg.com/v2-8488f236b5f9be3b34f1f4606e1c9fe7.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">LLM</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/LLM/">
                                <span class="chip bg-color">LLM</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/LLM/" class="post-category">
                                LLM
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-07-09
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-07-17
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    21k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    85 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-07-09-æ›´æ–°"><a href="#2025-07-09-æ›´æ–°" class="headerlink" title="2025-07-09 æ›´æ–°"></a>2025-07-09 æ›´æ–°</h1><h2 id="Beyond-Simple-Edits-X-Planner-for-Complex-Instruction-Based-Image-Editing"><a href="#Beyond-Simple-Edits-X-Planner-for-Complex-Instruction-Based-Image-Editing" class="headerlink" title="Beyond Simple Edits: X-Planner for Complex Instruction-Based Image   Editing"></a>Beyond Simple Edits: X-Planner for Complex Instruction-Based Image   Editing</h2><p><strong>Authors:Chun-Hsiao Yeh, Yilin Wang, Nanxuan Zhao, Richard Zhang, Yuheng Li, Yi Ma, Krishna Kumar Singh</strong></p>
<p>Recent diffusion-based image editing methods have significantly advanced text-guided tasks but often struggle to interpret complex, indirect instructions. Moreover, current models frequently suffer from poor identity preservation, unintended edits, or rely heavily on manual masks. To address these challenges, we introduce X-Planner, a Multimodal Large Language Model (MLLM)-based planning system that effectively bridges user intent with editing model capabilities. X-Planner employs chain-of-thought reasoning to systematically decompose complex instructions into simpler, clear sub-instructions. For each sub-instruction, X-Planner automatically generates precise edit types and segmentation masks, eliminating manual intervention and ensuring localized, identity-preserving edits. Additionally, we propose a novel automated pipeline for generating large-scale data to train X-Planner which achieves state-of-the-art results on both existing benchmarks and our newly introduced complex editing benchmark. </p>
<blockquote>
<p>æœ€è¿‘åŸºäºæ‰©æ•£çš„å›¾åƒç¼–è¾‘æ–¹æ³•åœ¨å¾ˆå¤§ç¨‹åº¦ä¸Šæ¨åŠ¨äº†æ–‡æœ¬å¼•å¯¼çš„ä»»åŠ¡ï¼Œä½†åœ¨è§£é‡Šå¤æ‚ã€é—´æ¥æŒ‡ä»¤æ—¶ç»å¸¸é‡åˆ°å›°éš¾ã€‚æ­¤å¤–ï¼Œå½“å‰æ¨¡å‹åœ¨èº«ä»½ä¿ç•™æ–¹é¢è¡¨ç°ä¸ä½³ï¼Œä¼šå‡ºç°éé¢„æœŸç¼–è¾‘ï¼Œæˆ–è¿‡äºä¾èµ–æ‰‹åŠ¨é®ç½©ã€‚ä¸ºäº†è§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†X-Plannerï¼Œè¿™æ˜¯ä¸€ä¸ªåŸºäºå¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰çš„è§„åˆ’ç³»ç»Ÿï¼Œå®ƒèƒ½å¤Ÿæœ‰æ•ˆåœ°å°†ç”¨æˆ·æ„å›¾ä¸ç¼–è¾‘æ¨¡å‹åŠŸèƒ½è”ç³»èµ·æ¥ã€‚X-Planneré‡‡ç”¨é“¾å¼æ€ç»´æ¨ç†ï¼Œç³»ç»Ÿåœ°åˆ†è§£å¤æ‚æŒ‡ä»¤ä¸ºæ›´ç®€å•ã€æ¸…æ™°çš„å­æŒ‡ä»¤ã€‚å¯¹äºæ¯ä¸ªå­æŒ‡ä»¤ï¼ŒX-Plannerä¼šè‡ªåŠ¨ç”Ÿæˆç²¾ç¡®çš„ç¼–è¾‘ç±»å‹å’Œåˆ†å‰²é®ç½©ï¼Œæ¶ˆé™¤äº†äººå·¥å¹²é¢„ï¼Œç¡®ä¿äº†å±€éƒ¨åŒ–å’Œèº«ä»½ä¿ç•™çš„ç¼–è¾‘ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜æå‡ºäº†ä¸€ç§æ–°çš„è‡ªåŠ¨åŒ–æµç¨‹æ¥ç”Ÿæˆå¤§è§„æ¨¡æ•°æ®ä»¥è®­ç»ƒX-Plannerï¼Œè¯¥æµç¨‹åœ¨ç°æœ‰åŸºå‡†æµ‹è¯•å’Œæˆ‘ä»¬æ–°å¼•å…¥çš„å¤æ‚ç¼–è¾‘åŸºå‡†æµ‹è¯•ä¸­å‡è¾¾åˆ°äº†æœ€æ–°æ°´å¹³çš„ç»“æœã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.05259v1">PDF</a> Project page: <a target="_blank" rel="noopener" href="https://danielchyeh.github.io/x-planner/">https://danielchyeh.github.io/x-planner/</a></p>
<p><strong>Summary</strong></p>
<p>è¿‘æœŸæ‰©æ•£å¼å›¾åƒç¼–è¾‘æ–¹æ³•è™½åœ¨æ–‡æœ¬å¼•å¯¼çš„ä»»åŠ¡ä¸Šæœ‰æ˜¾è‘—è¿›æ­¥ï¼Œä½†åœ¨å¤„ç†å¤æ‚ã€é—´æ¥æŒ‡ä»¤æ—¶å­˜åœ¨æŒ‘æˆ˜ã€‚ä¸ºè§£å†³æ­¤é—®é¢˜ï¼Œæˆ‘ä»¬æ¨å‡ºX-Plannerï¼Œä¸€ä¸ªåŸºäºå¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰çš„è§„åˆ’ç³»ç»Ÿï¼Œæœ‰æ•ˆæ¡¥æ¥ç”¨æˆ·æ„å›¾ä¸ç¼–è¾‘æ¨¡å‹èƒ½åŠ›ã€‚X-Planneré‡‡ç”¨é“¾å¼æ€ç»´æ¨ç†ï¼Œå°†å¤æ‚æŒ‡ä»¤åˆ†è§£ä¸ºç®€å•æ¸…æ™°çš„å­æŒ‡ä»¤ã€‚é’ˆå¯¹æ¯ä¸ªå­æŒ‡ä»¤ï¼ŒX-Plannerè‡ªåŠ¨ç”Ÿæˆç²¾ç¡®çš„ç¼–è¾‘ç±»å‹å’Œåˆ†å‰²æ©ç ï¼Œæ¶ˆé™¤äº†æ‰‹åŠ¨å¹²é¢„ï¼Œç¡®ä¿äº†å±€éƒ¨ã€èº«ä»½ä¿ç•™çš„ç¼–è¾‘ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜æå‡ºäº†ä¸€ç§æ–°çš„è‡ªåŠ¨åŒ–ç®¡é“ç”¨äºç”Ÿæˆå¤§è§„æ¨¡æ•°æ®ä»¥è®­ç»ƒX-Plannerï¼Œå…¶åœ¨ç°æœ‰åŸºå‡†æµ‹è¯•å’Œæˆ‘ä»¬æ–°å¼•å…¥çš„å¤æ‚ç¼–è¾‘åŸºå‡†æµ‹è¯•ä¸­å‡è¾¾åˆ°äº†æœ€ä½³æ•ˆæœã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>æ‰©æ•£å¼å›¾åƒç¼–è¾‘æ–¹æ³•åœ¨æ–‡æœ¬å¼•å¯¼ä»»åŠ¡ä¸Šçš„è¿›æ­¥æ˜¾è‘—ã€‚</li>
<li>å½“å‰å›¾åƒç¼–è¾‘æ–¹æ³•åœ¨å¤„ç†å¤æ‚ã€é—´æ¥æŒ‡ä»¤æ—¶å­˜åœ¨æŒ‘æˆ˜ã€‚</li>
<li>X-Planneræ˜¯ä¸€ä¸ªåŸºäºå¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹çš„è§„åˆ’ç³»ç»Ÿï¼Œèƒ½æœ‰æ•ˆæ¡¥æ¥ç”¨æˆ·æ„å›¾ä¸ç¼–è¾‘æ¨¡å‹èƒ½åŠ›ã€‚</li>
<li>X-Planneré€šè¿‡é“¾å¼æ€ç»´æ¨ç†åˆ†è§£å¤æ‚æŒ‡ä»¤ä¸ºç®€å•æ¸…æ™°çš„å­æŒ‡ä»¤ã€‚</li>
<li>X-Plannerè‡ªåŠ¨ç”Ÿæˆç²¾ç¡®çš„ç¼–è¾‘ç±»å‹å’Œåˆ†å‰²æ©ç ï¼Œæ¶ˆé™¤äº†æ‰‹åŠ¨å¹²é¢„ã€‚</li>
<li>X-Plannerç¡®ä¿äº†å±€éƒ¨ã€èº«ä»½ä¿ç•™çš„ç¼–è¾‘ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.05259">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-11230dbeae4e436fecedcda1f98da1d0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-41346a1e5e8e2acda643968c82c398ea.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-6ca6f268b7f03e1fdbe01e687cf0c1c4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-11a65ce66943ced7b17f0c8838d1ebc8.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-2db46a58d4ed61e2cfe6e67fdd4fe4b6.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Spatio-Temporal-LLM-Reasoning-about-Environments-and-Actions"><a href="#Spatio-Temporal-LLM-Reasoning-about-Environments-and-Actions" class="headerlink" title="Spatio-Temporal LLM: Reasoning about Environments and Actions"></a>Spatio-Temporal LLM: Reasoning about Environments and Actions</h2><p><strong>Authors:Haozhen Zheng, Beitong Tian, Mingyuan Wu, Zhenggang Tang, Klara Nahrstedt, Alex Schwing</strong></p>
<p>Despite the significant recent progress of Multimodal Large Language Models (MLLMs), MLLMs still struggle to correctly answer prompts that require a holistic spatio-temporal understanding. Specifically, it is challenging to address prompts that refer to 1) the entirety of an environment that an agent equipped with an MLLM can operate in; and simultaneously also refer to 2) recent actions that just happened and are encoded in a video clip. However, such a holistic spatio-temporal understanding is important for agents operating in the real world. To address this issue, we first develop a framework to collect a large-scale dataset. Using the collected â€œReasoning about Environments and Actionsâ€ (REA) dataset, we show that recent methods indeed struggle to correctly answer the prompts. To improve, we develop a â€œspatio-temporal LLMâ€ (ST-LLM), a model equipped with projectors to improve both spatial understanding of an environment and temporal understanding of recent observations. On the collected REA data, we show that the proposed method significantly improves results compared to prior work. Code and data are available at <a target="_blank" rel="noopener" href="https://zoezheng126.github.io/STLLM-website/">https://zoezheng126.github.io/STLLM-website/</a>. </p>
<blockquote>
<p>å°½ç®¡æœ€è¿‘å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰å–å¾—äº†é‡å¤§è¿›å±•ï¼Œä½†åœ¨éœ€è¦å…¨é¢æ—¶ç©ºç†è§£æç¤ºçš„æƒ…å¢ƒä¸­ï¼ŒMLLMsä»ç„¶éš¾ä»¥æ­£ç¡®å›ç­”é—®é¢˜ã€‚å…·ä½“æ¥è¯´ï¼Œè§£å†³æ¶‰åŠä»¥ä¸‹ä¸¤ä¸ªæ–¹é¢çš„æç¤ºå…·æœ‰æŒ‘æˆ˜æ€§ï¼š1ï¼‰ä¸€ä¸ªé…å¤‡MLLMçš„ä»£ç†å¯ä»¥æ“ä½œçš„æ•´ä½“ç¯å¢ƒï¼›åŒæ—¶æ¶‰åŠ2ï¼‰åˆšåˆšå‘ç”Ÿçš„åŠ¨ä½œï¼Œè¿™äº›åŠ¨ä½œè¢«ç¼–ç åœ¨è§†é¢‘å‰ªè¾‘ä¸­ã€‚ç„¶è€Œï¼Œå¯¹äºåœ¨ç°å®ä¸–ç•Œä¸­æ“ä½œçš„ä»£ç†æ¥è¯´ï¼Œè¿™ç§å…¨é¢çš„æ—¶ç©ºç†è§£éå¸¸é‡è¦ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬é¦–å…ˆå¼€å‘äº†ä¸€ä¸ªæ¡†æ¶æ¥æ”¶é›†å¤§è§„æ¨¡æ•°æ®é›†ã€‚â€œå…³äºç¯å¢ƒå’Œè¡ŒåŠ¨æ¨ç†â€ï¼ˆREAï¼‰æ•°æ®é›†æ˜¾ç¤ºï¼Œæœ€è¿‘çš„æ–¹æ³•ç¡®å®éš¾ä»¥æ­£ç¡®å›ç­”æç¤ºã€‚ä¸ºäº†æ”¹è¿›è¿™ä¸€ç‚¹ï¼Œæˆ‘ä»¬å¼€å‘äº†ä¸€ç§â€œæ—¶ç©ºLLMâ€ï¼ˆST-LLMï¼‰ï¼Œè¿™æ˜¯ä¸€ç§é…å¤‡æŠ•å½±ä»ªçš„æ¨¡å‹ï¼Œæ—¨åœ¨æé«˜å¯¹ç¯å¢ƒç©ºé—´å’Œå¯¹æœ€è¿‘è§‚å¯Ÿçš„æ—¶é—´ç†è§£ã€‚åœ¨æ”¶é›†çš„REAæ•°æ®ä¸Šï¼Œæˆ‘ä»¬è¯æ˜äº†æ‰€æå‡ºçš„æ–¹æ³•ä¸å…ˆå‰çš„å·¥ä½œç›¸æ¯”ï¼Œç»“æœå¾—åˆ°äº†æ˜¾è‘—æ”¹å–„ã€‚ç›¸å…³ä»£ç å’Œæ•°æ®å¯ä»¥åœ¨<a target="_blank" rel="noopener" href="https://zoezheng126.github.io/STLLM-website/%E6%89%BE%E5%88%B0%E3%80%82">https://zoezheng126.github.io/STLLM-website/æ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.05258v1">PDF</a> Code and data are available at   <a target="_blank" rel="noopener" href="https://zoezheng126.github.io/STLLM-website/">https://zoezheng126.github.io/STLLM-website/</a></p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†å°½ç®¡å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰è¿‘æœŸå–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œä½†åœ¨åº”å¯¹éœ€è¦æ•´ä½“æ—¶ç©ºç†è§£èƒ½åŠ›çš„æç¤ºæ—¶ä»é¢ä¸´æŒ‘æˆ˜ã€‚ä¸ºåº”å¯¹æ­¤é—®é¢˜ï¼Œç ”ç©¶å›¢é˜Ÿé¦–å…ˆæ„å»ºäº†ä¸€ä¸ªå¤§å‹æ•°æ®é›†â€œå…³äºç¯å¢ƒä¸è¡Œä¸ºæ¨ç†â€ï¼ˆREAï¼‰ï¼Œå¹¶å‘ç°ç°æœ‰æ–¹æ³•éš¾ä»¥æ­£ç¡®å›ç­”è¿™äº›æç¤ºã€‚ä¸ºæé«˜æ¨¡å‹æ€§èƒ½ï¼Œç ”ç©¶å›¢é˜Ÿæå‡ºäº†é…å¤‡æŠ•å½±å™¨çš„æ—¶ç©ºLLMï¼ˆST-LLMï¼‰ï¼Œä»¥æé«˜å¯¹ç¯å¢ƒå’Œæœ€è¿‘è§‚å¯Ÿç»“æœçš„æ—¶ç©ºç†è§£èƒ½åŠ›ã€‚åœ¨REAæ•°æ®é›†ä¸Šè¿›è¡Œçš„å®éªŒè¡¨æ˜ï¼Œç›¸è¾ƒäºä¹‹å‰çš„æ–¹æ³•ï¼Œæ‰€æå‡ºçš„ST-LLMèƒ½æ˜¾è‘—æ”¹å–„ç»“æœã€‚ä»£ç å’Œæ•°æ®é›†å¯è®¿é—®äº<a target="_blank" rel="noopener" href="https://zoezheng126.github.io/STLLM-website/%E3%80%82">https://zoezheng126.github.io/STLLM-website/ã€‚</a></p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>MLLMsåœ¨é¢å¯¹éœ€è¦æ•´ä½“æ—¶ç©ºç†è§£çš„æç¤ºæ—¶ä»æœ‰å›°éš¾ï¼Œå°¤å…¶æ˜¯åœ¨ç†è§£å’Œå¤„ç†ç¯å¢ƒå’Œæœ€æ–°è¡ŒåŠ¨ä¿¡æ¯æ–¹é¢ã€‚</li>
<li>ç ”ç©¶äººå‘˜åˆ›å»ºäº†ä¸€ä¸ªåä¸ºREAçš„å¤§å‹æ•°æ®é›†ï¼Œç”¨äºè¯„ä¼°æ¨¡å‹åœ¨åº”å¯¹æ¶‰åŠç¯å¢ƒå’Œè¡Œä¸ºç†è§£çš„æç¤ºæ–¹é¢çš„æ€§èƒ½ã€‚</li>
<li>ç°æœ‰æ–¹æ³•åœ¨åº”å¯¹è¿™äº›æç¤ºæ—¶è¡¨ç°ä¸ä½³ã€‚</li>
<li>ä¸ºäº†æ”¹å–„æ¨¡å‹çš„æ€§èƒ½ï¼Œæå‡ºäº†é…å¤‡æŠ•å½±å™¨çš„ST-LLMæ¨¡å‹ï¼Œè¯¥æ¨¡å‹æ—¨åœ¨æé«˜å¯¹ç¯å¢ƒç©ºé—´çš„ç†è§£å’Œæœ€è¿‘çš„è§‚å¯Ÿç»“æœçš„æ—¶ç©ºç†è§£ã€‚</li>
<li>åœ¨REAæ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒST-LLMæ¨¡å‹ç›¸è¾ƒäºä¹‹å‰çš„æ–¹æ³•æœ‰æ˜¾è‘—æ”¹å–„ã€‚</li>
<li>ST-LLMæ¨¡å‹çš„ä»£ç å’Œæ•°æ®é›†å·²å…¬å¼€å‘å¸ƒï¼Œä¾¿äºå…¬ä¼—è®¿é—®å’Œä½¿ç”¨ã€‚</li>
<li>è¿™ç§æ¨¡å‹å¯¹äºå¼€å‘èƒ½åœ¨ç°å®ä¸–ç•Œä¸­è¿ä½œçš„æ™ºèƒ½ä»£ç†å…·æœ‰é‡è¦æ„ä¹‰ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.05258">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-12fc8033021b2f9a799773b5ce635297.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-5079c80e2fc378a16c514f6a221b54e7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a650cc85a7e6ee4d23c6d0569fd3ee3e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c679d5950bd295f99db3a237199c3560.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-401b55310f0d5968788f7a3160e0a2e6.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Open-Vision-Reasoner-Transferring-Linguistic-Cognitive-Behavior-for-Visual-Reasoning"><a href="#Open-Vision-Reasoner-Transferring-Linguistic-Cognitive-Behavior-for-Visual-Reasoning" class="headerlink" title="Open Vision Reasoner: Transferring Linguistic Cognitive Behavior for   Visual Reasoning"></a>Open Vision Reasoner: Transferring Linguistic Cognitive Behavior for   Visual Reasoning</h2><p><strong>Authors:Yana Wei, Liang Zhao, Jianjian Sun, Kangheng Lin, Jisheng Yin, Jingcheng Hu, Yinmin Zhang, En Yu, Haoran Lv, Zejia Weng, Jia Wang, Chunrui Han, Yuang Peng, Qi Han, Zheng Ge, Xiangyu Zhang, Daxin Jiang, Vishal M. Patel</strong></p>
<p>The remarkable reasoning capability of large language models (LLMs) stems from cognitive behaviors that emerge through reinforcement with verifiable rewards. This work investigates how to transfer this principle to Multimodal LLMs (MLLMs) to unlock advanced visual reasoning. We introduce a two-stage paradigm built on Qwen2.5-VL-7B: a massive linguistic cold-start fine-tuning, followed by multimodal reinforcement learning (RL) spanning nearly 1,000 steps, surpassing all previous open-source efforts in scale. This pioneering work reveals three fundamental insights: 1) Behavior transfer emerges surprisingly early in cold start due to linguistic mental imagery. 2) Cold start broadly memorizes visual behaviors, while RL critically discerns and scales up effective patterns. 3) Transfer strategically favors high-utility behaviors such as visual reflection. Our resulting model, Open-Vision-Reasoner (OVR), achieves state-of-the-art performance on a suite of reasoning benchmarks, including 95.3% on MATH500, 51.8% on MathVision and 54.6% on MathVerse. We release our model, data, and training dynamics to catalyze the development of more capable, behavior-aligned multimodal reasoners. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„å‡ºè‰²æ¨ç†èƒ½åŠ›æ¥æºäºé€šè¿‡å¯éªŒè¯å¥–åŠ±è¿›è¡Œå¼ºåŒ–åå‡ºç°çš„è®¤çŸ¥è¡Œä¸ºã€‚è¿™é¡¹å·¥ä½œç ”ç©¶äº†å¦‚ä½•å°†è¿™ä¸€åŸç†åº”ç”¨äºå¤šæ¨¡æ€LLMï¼ˆMLLMï¼‰ï¼Œä»¥è§£é”é«˜çº§è§†è§‰æ¨ç†ã€‚æˆ‘ä»¬åŸºäºQwen2.5-VL-7Bå¼•å…¥äº†ä¸€ä¸ªä¸¤é˜¶æ®µèŒƒå¼ï¼šå¤§è§„æ¨¡è¯­è¨€å†·å¯åŠ¨å¾®è°ƒï¼Œæ¥ç€æ˜¯è¿‘1000æ­¥çš„å¤šæ¨¡æ€å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰ã€‚è¿™é¡¹å·¥ä½œåœ¨è§„æ¨¡ä¸Šè¶…è¶Šäº†ä¹‹å‰æ‰€æœ‰çš„å¼€æºå·¥ä½œã€‚è¿™é¡¹å¼€åˆ›æ€§çš„å·¥ä½œæ­ç¤ºäº†ä¸‰ä¸ªåŸºæœ¬è§è§£ï¼š1ï¼‰ç”±äºè¯­è¨€å¿ƒç†æ„è±¡ï¼Œè¡Œä¸ºè½¬ç§»åœ¨å†·å¯åŠ¨é˜¶æ®µå‡ºä¹æ„æ–™åœ°æ—©æœŸå‡ºç°ã€‚2ï¼‰å†·å¯åŠ¨å¹¿æ³›è®°å¿†è§†è§‰è¡Œä¸ºï¼Œè€Œå¼ºåŒ–å­¦ä¹ å¯ä»¥ç²¾å‡†åœ°åŒºåˆ†å¹¶æ‰©å¤§æœ‰æ•ˆæ¨¡å¼ã€‚3ï¼‰è½¬ç§»ç­–ç•¥æœ‰åˆ©äºé«˜å®ç”¨æ€§çš„è¡Œä¸ºï¼Œå¦‚è§†è§‰åå°„ã€‚æˆ‘ä»¬å¾—åˆ°çš„æ¨¡å‹Open-Vision-Reasonerï¼ˆOVRï¼‰åœ¨ä¸€ç³»åˆ—æ¨ç†åŸºå‡†æµ‹è¯•ä¸Šè¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼ŒåŒ…æ‹¬MATH500ä¸Šçš„95.3%ï¼ŒMathVisionä¸Šçš„51.8%å’ŒMathVerseä¸Šçš„54.6%ã€‚æˆ‘ä»¬å‘å¸ƒæˆ‘ä»¬çš„æ¨¡å‹ã€æ•°æ®å’Œè®­ç»ƒåŠ¨æ€ï¼Œä»¥æ¨åŠ¨å¼€å‘æ›´å¼ºå¤§ã€è¡Œä¸ºä¸€è‡´çš„å¤šæ¨¡æ€æ¨ç†å™¨ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.05255v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹çš„å‡ºè‰²æ¨ç†èƒ½åŠ›æºäºé€šè¿‡å¯éªŒè¯çš„å¥–åŠ±è¿›è¡Œå¼ºåŒ–åå‡ºç°çš„è®¤çŸ¥è¡Œä¸ºã€‚æœ¬ç ”ç©¶æ¢ç´¢äº†å¦‚ä½•å°†è¿™ä¸€åŸåˆ™åº”ç”¨äºå¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼Œä»¥è§£é”é«˜çº§è§†è§‰æ¨ç†ã€‚ç ”ç©¶æå‡ºäº†ä¸€ç§åŸºäºQwen2.5-VL-7Bçš„ä¸¤é˜¶æ®µèŒƒå¼ï¼Œå…ˆè¿›è¡Œå¤§è§„æ¨¡è¯­è¨€å†·å¯åŠ¨å¾®è°ƒï¼Œç„¶åè¿›è¡Œè¿‘1000æ­¥çš„å¤šæ¨¡æ€å¼ºåŒ–å­¦ä¹ ã€‚æœ¬ç ”ç©¶æ­ç¤ºäº†è¡Œä¸ºè½¬ç§»åœ¨å†·å¯åŠ¨æ—©æœŸå°±ä¼šå‡ºç°ï¼Œå¤šæ¨¡æ€å¼ºåŒ–å­¦ä¹ èƒ½å¤Ÿæœ‰æ•ˆæå‡æ¨¡å‹çš„æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹çš„æ¨ç†èƒ½åŠ›æºäºå¼ºåŒ–å­¦ä¹ å’Œå¯éªŒè¯çš„å¥–åŠ±ã€‚</li>
<li>å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹å…·æœ‰è§£é”é«˜çº§è§†è§‰æ¨ç†çš„æ½œåŠ›ã€‚</li>
<li>ä¸¤é˜¶æ®µèŒƒå¼ç»“åˆäº†å†·å¯åŠ¨å¾®è°ƒä¸å¤šæ¨¡æ€å¼ºåŒ–å­¦ä¹ ï¼Œè§„æ¨¡è¶…è¶Šä¹‹å‰æ‰€æœ‰å¼€æºåŠªåŠ›ã€‚</li>
<li>è¡Œä¸ºè½¬ç§»åœ¨å†·å¯åŠ¨æ—©æœŸé˜¶æ®µå°±ä¼šå‡ºç°ï¼Œæºäºè¯­è¨€å¿ƒç†å›¾åƒã€‚</li>
<li>å†·å¯åŠ¨å¹¿æ³›è®°å¿†è§†è§‰è¡Œä¸ºï¼Œè€Œå¼ºåŒ–å­¦ä¹ åˆ™å…³é”®åœ°åŒºåˆ†å’Œæ‰©å¤§æœ‰æ•ˆæ¨¡å¼ã€‚</li>
<li>è½¬ç§»ç­–ç•¥æœ‰åˆ©äºé«˜å®ç”¨æ€§çš„è¡Œä¸ºï¼Œå¦‚è§†è§‰åå°„ã€‚</li>
<li>ç ”ç©¶çš„æ¨¡å‹Open-Vision-Reasoneråœ¨å¤šä¸ªæ¨ç†åŸºå‡†æµ‹è¯•ä¸Šè¾¾åˆ°æœ€æ–°æ°´å¹³ï¼ŒåŒ…æ‹¬MATH500çš„95.3%ã€MathVisionçš„51.8%å’ŒMathVerseçš„54.6%ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.05255">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-fc49516c89e3d0567aface1442a2f7be.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-fd80b4e4108801b3ea459af6851d736d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2a58e9c1a7abda30e20e5f0290049619.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-fcf2a13672256444a1608d5467af1585.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Response-Attack-Exploiting-Contextual-Priming-to-Jailbreak-Large-Language-Models"><a href="#Response-Attack-Exploiting-Contextual-Priming-to-Jailbreak-Large-Language-Models" class="headerlink" title="Response Attack: Exploiting Contextual Priming to Jailbreak Large   Language Models"></a>Response Attack: Exploiting Contextual Priming to Jailbreak Large   Language Models</h2><p><strong>Authors:Ziqi Miao, Lijun Li, Yuan Xiong, Zhenhua Liu, Pengyu Zhu, Jing Shao</strong></p>
<p>Contextual priming, where earlier stimuli covertly bias later judgments, offers an unexplored attack surface for large language models (LLMs). We uncover a contextual priming vulnerability in which the previous response in the dialogue can steer its subsequent behavior toward policy-violating content. Building on this insight, we propose Response Attack, which uses an auxiliary LLM to generate a mildly harmful response to a paraphrased version of the original malicious query. They are then formatted into the dialogue and followed by a succinct trigger prompt, thereby priming the target model to generate harmful content. Across eight open-source and proprietary LLMs, RA consistently outperforms seven state-of-the-art jailbreak techniques, achieving higher attack success rates. To mitigate this threat, we construct and release a context-aware safety fine-tuning dataset, which significantly reduces the attack success rate while preserving model capabilities. The code and data are available at <a target="_blank" rel="noopener" href="https://github.com/Dtc7w3PQ/Response-Attack">https://github.com/Dtc7w3PQ/Response-Attack</a>. </p>
<blockquote>
<p>è¯­å¢ƒæç¤ºæ³•ï¼Œå³æ—©æœŸåˆºæ¿€æ— å½¢ä¸­å½±å“åç»­åˆ¤æ–­ï¼Œä¸ºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æä¾›äº†ä¸€ä¸ªå°šæœªæ¢ç´¢çš„æ”»å‡»é¢ã€‚æˆ‘ä»¬å‘ç°äº†ä¸€ç§è¯­å¢ƒæç¤ºçš„æ¼æ´ï¼Œå³å¯¹è¯ä¸­çš„å‰ä¸€ä¸ªå›ç­”å¯ä»¥å¼•å¯¼å…¶åç»­è¡Œä¸ºèµ°å‘è¿åæ”¿ç­–çš„å†…å®¹ã€‚åŸºäºè¿™ä¸€å‘ç°ï¼Œæˆ‘ä»¬æå‡ºäº†å“åº”æ”»å‡»æ³•ï¼Œè¯¥æ–¹æ³•ä½¿ç”¨è¾…åŠ©LLMå¯¹åŸå§‹æ¶æ„æŸ¥è¯¢çš„å¦ä¸€ç§è¡¨è¿°ç”Ÿæˆè½»åº¦æœ‰å®³çš„å“åº”ã€‚ç„¶åå®ƒä»¬è¢«æ•´ç†æˆå¯¹è¯å½¢å¼ï¼Œç´§æ¥ç€æ˜¯ç®€çŸ­çš„è§¦å‘æç¤ºï¼Œä»è€Œå¼•å¯¼ç›®æ ‡æ¨¡å‹ç”Ÿæˆæœ‰å®³å†…å®¹ã€‚åœ¨å…«ä¸ªå¼€æºå’Œä¸“æœ‰LLMä¸­ï¼ŒRAå§‹ç»ˆä¼˜äºä¸ƒç§æœ€å…ˆè¿›çš„è¶Šç‹±æŠ€æœ¯ï¼Œå¹¶å®ç°äº†æ›´é«˜çš„æ”»å‡»æˆåŠŸç‡ã€‚ä¸ºäº†ç¼“è§£è¿™ä¸€å¨èƒï¼Œæˆ‘ä»¬æ„å»ºå¹¶å‘å¸ƒäº†ä¸€ä¸ªä¸Šä¸‹æ–‡æ„ŸçŸ¥çš„å®‰å…¨å¾®è°ƒæ•°æ®é›†ï¼Œå®ƒåœ¨é™ä½æ”»å‡»æˆåŠŸç‡çš„åŒæ—¶ä¿ç•™äº†æ¨¡å‹çš„èƒ½åŠ›ã€‚ä»£ç å’Œæ•°æ®å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/Dtc7w3PQ/Response-Attack%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/Dtc7w3PQ/Response-Attackè·å–ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.05248v1">PDF</a> 21 pages, 9 figures. Code and data available at   <a target="_blank" rel="noopener" href="https://github.com/Dtc7w3PQ/Response-Attack">https://github.com/Dtc7w3PQ/Response-Attack</a></p>
<p><strong>Summary</strong>ï¼š</p>
<p>è¯­å¢ƒæç¤ºå¯¹å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ„æˆæœªæ¢ç´¢çš„æ”»å‡»é¢ï¼Œå…ˆå‰çš„å¯¹è¯å“åº”å¯èƒ½å½±å“åç»­è¡Œä¸ºï¼Œå¯¼è‡´è¿åç­–ç•¥å†…å®¹ã€‚åŸºäºæ­¤ï¼Œæå‡ºResponse Attackæ–¹æ³•ï¼Œä½¿ç”¨è¾…åŠ©LLMç”Ÿæˆè½»å¾®æœ‰å®³å“åº”å¹¶é’ˆå¯¹åŸå§‹æ¶æ„æŸ¥è¯¢è¿›è¡Œæ”¹ç¼–ï¼Œéšåé€šè¿‡ç®€æ´è§¦å‘æç¤ºï¼Œä½¿ç›®æ ‡æ¨¡å‹ç”Ÿæˆæœ‰å®³å†…å®¹ã€‚è¯¥æ–¹æ³•åœ¨å¤šä¸ªå¼€æºå’Œä¸“æœ‰LLMä¸Šè¡¨ç°ä¼˜å¼‚ï¼ŒæˆåŠŸç‡é«˜ä¸”è¶…è¶Šä¸ƒç§å…ˆè¿›çš„è¶Šç‹±æŠ€æœ¯ã€‚ä¸ºåº”å¯¹è¿™ä¸€å¨èƒï¼Œæ„å»ºäº†ä¸Šä¸‹æ–‡æ„ŸçŸ¥çš„å®‰å…¨å¾®è°ƒæ•°æ®é›†ï¼Œæ—¢èƒ½æ˜¾è‘—é™ä½æ”»å‡»æˆåŠŸç‡ï¼ŒåŒæ—¶ä¿ç•™æ¨¡å‹èƒ½åŠ›ã€‚ç›¸å…³ä»£ç å’Œæ•°æ®é›†å·²åœ¨GitHubä¸Šå…¬å¼€ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>è¯­å¢ƒæç¤ºæˆä¸ºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ–°æ”»å‡»é¢ã€‚</li>
<li>å¯¹è¯ä¸­çš„å…ˆå‰å“åº”èƒ½å¤Ÿå½±å“åç»­æ¨¡å‹è¡Œä¸ºï¼Œå¯èƒ½å¯¼è‡´ç­–ç•¥è¿è§„å†…å®¹ç”Ÿæˆã€‚</li>
<li>æå‡ºResponse Attackæ–¹æ³•ï¼Œåˆ©ç”¨è¾…åŠ©LLMç”Ÿæˆé’ˆå¯¹ç›®æ ‡æ¨¡å‹çš„æ¶æ„å“åº”ã€‚</li>
<li>Response Attackæ–¹æ³•åœ¨ä¸åŒLLMä¸Šè¡¨ç°ç¨³å®šï¼ŒæˆåŠŸç‡é«˜ä¸”ä¼˜äºå…¶ä»–è¶Šç‹±æŠ€æœ¯ã€‚</li>
<li>ä¸ºåº”å¯¹è¯¥å¨èƒï¼Œå¼€å‘ä¸Šä¸‹æ–‡æ„ŸçŸ¥çš„å®‰å…¨å¾®è°ƒæ•°æ®é›†ã€‚</li>
<li>å®‰å…¨å¾®è°ƒæ•°æ®é›†èƒ½æ˜¾è‘—é™ä½æ”»å‡»æˆåŠŸç‡ï¼ŒåŒæ—¶ä¿æŒæ¨¡å‹æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.05248">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-a357f7aa6db7c0ad6ad3635ce25807fa.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9c319060f932ffc5bb7b511d1e79a96f.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-bafd88deea4d3b186fd82c3606fcf903.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-343062dfa8575359de38af28450b9ee6.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="StreamVLN-Streaming-Vision-and-Language-Navigation-via-SlowFast-Context-Modeling"><a href="#StreamVLN-Streaming-Vision-and-Language-Navigation-via-SlowFast-Context-Modeling" class="headerlink" title="StreamVLN: Streaming Vision-and-Language Navigation via SlowFast Context   Modeling"></a>StreamVLN: Streaming Vision-and-Language Navigation via SlowFast Context   Modeling</h2><p><strong>Authors:Meng Wei, Chenyang Wan, Xiqian Yu, Tai Wang, Yuqiang Yang, Xiaohan Mao, Chenming Zhu, Wenzhe Cai, Hanqing Wang, Yilun Chen, Xihui Liu, Jiangmiao Pang</strong></p>
<p>Vision-and-Language Navigation (VLN) in real-world settings requires agents to process continuous visual streams and generate actions with low latency grounded in language instructions. While Video-based Large Language Models (Video-LLMs) have driven recent progress, current VLN methods based on Video-LLM often face trade-offs among fine-grained visual understanding, long-term context modeling and computational efficiency. We introduce StreamVLN, a streaming VLN framework that employs a hybrid slow-fast context modeling strategy to support multi-modal reasoning over interleaved vision, language and action inputs. The fast-streaming dialogue context facilitates responsive action generation through a sliding-window of active dialogues, while the slow-updating memory context compresses historical visual states using a 3D-aware token pruning strategy. With this slow-fast design, StreamVLN achieves coherent multi-turn dialogue through efficient KV cache reuse, supporting long video streams with bounded context size and inference cost. Experiments on VLN-CE benchmarks demonstrate state-of-the-art performance with stable low latency, ensuring robustness and efficiency in real-world deployment. The project page is: \href{<a target="_blank" rel="noopener" href="https://streamvln.github.io/%7D%7Bhttps://streamvln.github.io/%7D">https://streamvln.github.io/}{https://streamvln.github.io/}</a>. </p>
<blockquote>
<p>åœ¨çœŸå®ä¸–ç•Œç¯å¢ƒä¸­ï¼Œè§†è§‰ä¸è¯­è¨€å¯¼èˆªï¼ˆVLNï¼‰è¦æ±‚æ™ºèƒ½ä½“å¤„ç†è¿ç»­è§†è§‰æµï¼Œå¹¶åŸºäºè¯­è¨€æŒ‡ä»¤ç”Ÿæˆä½å»¶è¿Ÿçš„åŠ¨ä½œã€‚å°½ç®¡åŸºäºè§†é¢‘çš„çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆVideo-LLMï¼‰å·²ç»æ¨åŠ¨äº†è¿‘æœŸçš„è¿›å±•ï¼Œä½†å½“å‰åŸºäºVideo-LLMçš„VLNæ–¹æ³•é€šå¸¸åœ¨ç²¾ç»†çš„è§†è§‰ç†è§£ã€é•¿æœŸä¸Šä¸‹æ–‡å»ºæ¨¡å’Œè®¡ç®—æ•ˆç‡ä¹‹é—´é¢ä¸´æƒè¡¡ã€‚æˆ‘ä»¬å¼•å…¥äº†StreamVLNï¼Œè¿™æ˜¯ä¸€ä¸ªæµå¼VLNæ¡†æ¶ï¼Œé‡‡ç”¨æ··åˆçš„æ…¢å¿«ä¸Šä¸‹æ–‡å»ºæ¨¡ç­–ç•¥ï¼Œæ”¯æŒå¯¹äº¤ç»‡çš„è§†è§‰ã€è¯­è¨€å’ŒåŠ¨ä½œè¾“å…¥çš„è·¨æ¨¡æ€æ¨ç†ã€‚å¿«é€Ÿæµå¼å¯¹è¯ä¸Šä¸‹æ–‡é€šè¿‡æ´»åŠ¨å¯¹è¯çš„æ»‘åŠ¨çª—å£ä¿ƒè¿›å“åº”æ€§åŠ¨ä½œç”Ÿæˆï¼Œè€Œç¼“æ…¢æ›´æ–°çš„å†…å­˜ä¸Šä¸‹æ–‡ä½¿ç”¨ä¸‰ç»´æ„ŸçŸ¥ä»¤ç‰Œä¿®å‰ªç­–ç•¥å‹ç¼©å†å²è§†è§‰çŠ¶æ€ã€‚é€šè¿‡è¿™ç§æ…¢å¿«è®¾è®¡ï¼ŒStreamVLNé€šè¿‡é«˜æ•ˆçš„å…³é”®å€¼ç¼“å­˜é‡ç”¨å®ç°äº†è¿è´¯çš„å¤šè½®å¯¹è¯ï¼Œæ”¯æŒå…·æœ‰æœ‰é™ä¸Šä¸‹æ–‡å¤§å°å’Œæ¨ç†æˆæœ¬çš„é•¿è§†é¢‘æµã€‚åœ¨VLN-CEåŸºå‡†æµ‹è¯•ä¸Šçš„å®éªŒå±•ç¤ºäº†æœ€å…ˆè¿›çš„æ€§èƒ½ä»¥åŠç¨³å®šçš„ä½å»¶è¿Ÿï¼Œç¡®ä¿äº†åœ¨å®é™…éƒ¨ç½²ä¸­çš„ç¨³å¥æ€§å’Œæ•ˆç‡ã€‚é¡¹ç›®é¡µé¢ä¸ºï¼š<a target="_blank" rel="noopener" href="https://streamvln.github.io./">https://streamvln.github.io/ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.05240v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>åŸºäºè§†é¢‘çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆVideo-LLMï¼‰æ¨åŠ¨äº†è§†è§‰ä¸è¯­è¨€å¯¼èˆªï¼ˆVLNï¼‰é¢†åŸŸçš„è¿›å±•ï¼Œä½†ç°æœ‰æ–¹æ³•é¢ä¸´ç²¾ç»†è§†è§‰ç†è§£ã€é•¿æœŸä¸Šä¸‹æ–‡å»ºæ¨¡å’Œè®¡ç®—æ•ˆç‡ä¹‹é—´çš„æƒè¡¡é—®é¢˜ã€‚æœ¬æ–‡æå‡ºäº†StreamVLNæ¡†æ¶ï¼Œé‡‡ç”¨å¿«æ…¢ä¸Šä¸‹æ–‡æ··åˆå»ºæ¨¡ç­–ç•¥ï¼Œæ”¯æŒå¯¹äº¤ç»‡çš„è§†è§‰ã€è¯­è¨€å’ŒåŠ¨ä½œè¾“å…¥çš„è·¨æ¨¡æ€æ¨ç†ã€‚å¿«é€Ÿæµå¯¹è¯ä¸Šä¸‹æ–‡é€šè¿‡æ´»åŠ¨å¯¹è¯çš„æ»‘åŠ¨çª—å£ä¿ƒè¿›å“åº”åŠ¨ä½œç”Ÿæˆï¼Œè€Œæ…¢é€Ÿæ›´æ–°è®°å¿†ä¸Šä¸‹æ–‡åˆ™ä½¿ç”¨3Dæ„ŸçŸ¥ä»¤ç‰Œä¿®å‰ªç­–ç•¥å‹ç¼©å†å²è§†è§‰çŠ¶æ€ã€‚é€šè¿‡å¿«æ…¢è®¾è®¡ï¼ŒStreamVLNå®ç°äº†é«˜æ•ˆKVç¼“å­˜çš„è¿è´¯å¤šè½®å¯¹è¯ï¼Œæ”¯æŒå…·æœ‰æœ‰ç•Œä¸Šä¸‹æ–‡å¤§å°å’Œæ¨ç†æˆæœ¬çš„é•¿è§†é¢‘æµã€‚åœ¨VLN-CEåŸºå‡†æµ‹è¯•ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œå…¶æ€§èƒ½è¾¾åˆ°æœ€æ–°æ°´å¹³ï¼Œå…·æœ‰ç¨³å®šçš„ä½å»¶è¿Ÿï¼Œç¡®ä¿äº†åœ¨å®é™…éƒ¨ç½²ä¸­çš„ç¨³å¥æ€§å’Œæ•ˆç‡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è§†é¢‘å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆVideo-LLMï¼‰æ¨åŠ¨äº†è§†è§‰ä¸è¯­è¨€å¯¼èˆªï¼ˆVLNï¼‰çš„è¿›æ­¥ã€‚</li>
<li>å½“å‰VLNæ–¹æ³•é¢ä¸´ç²¾ç»†è§†è§‰ç†è§£ã€é•¿æœŸä¸Šä¸‹æ–‡å»ºæ¨¡å’Œè®¡ç®—æ•ˆç‡çš„æƒè¡¡é—®é¢˜ã€‚</li>
<li>StreamVLNæ¡†æ¶é‡‡ç”¨å¿«æ…¢ä¸Šä¸‹æ–‡æ··åˆå»ºæ¨¡ç­–ç•¥æ”¯æŒè·¨æ¨¡æ€æ¨ç†ã€‚</li>
<li>å¿«é€Ÿæµå¯¹è¯ä¸Šä¸‹æ–‡ä¿ƒè¿›å“åº”åŠ¨ä½œç”Ÿæˆã€‚</li>
<li>æ…¢é€Ÿæ›´æ–°è®°å¿†ä¸Šä¸‹æ–‡å‹ç¼©å†å²è§†è§‰çŠ¶æ€ã€‚</li>
<li>StreamVLNå®ç°äº†è¿è´¯çš„å¤šè½®å¯¹è¯ï¼Œæ”¯æŒé•¿è§†é¢‘æµå¤„ç†ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.05240">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-f7262023e1d0445c724b3b1424f9c88c.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-824d75dae424940c9d7956d3caa84f35.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ecb7bddae15356e6f2bc6ea0f51403b1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-46e56b811b0df41e6bab87003252cdb5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6d1eaf832c97bd68ba14f8a5e74839e5.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-3595d64ce3d88aba12c2b42fb77bb9d0.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Cascade-Token-Sharded-Private-LLM-Inference"><a href="#Cascade-Token-Sharded-Private-LLM-Inference" class="headerlink" title="Cascade: Token-Sharded Private LLM Inference"></a>Cascade: Token-Sharded Private LLM Inference</h2><p><strong>Authors:Rahul Thomas, Louai Zahran, Erica Choi, Akilesh Potti, Micah Goldblum, Arka Pal</strong></p>
<p>As LLMs continue to increase in parameter size, the computational resources required to run them are available to fewer parties. Therefore, third-party inference services â€“ where LLMs are hosted by third parties with significant computational resources â€“ are becoming increasingly popular. However, third party inference raises critical concerns about user data privacy. To mitigate these risks, privacy researchers have developed provably secure schemes for third-party inference, such as Secure Multi-Party Computation (SMPC). However, SMPC protocols have significant computational and communication overhead, and do not scale to large models. In this work, we propose a new multi-party inference protocol, Cascade, that avoids these punitive costs by leveraging sharding in the sequence dimension to maintain privacy, trading off cryptographic privacy guarantees for increased performance and scalability. We demonstrate that Cascade is resistant to a generalization of a recent attack that is highly effective against other statistical privacy schemes, and that it is further resistant to learning-based attacks. As Cascade is orders of magnitude faster than existing schemes, our findings offer practical solutions for secure deployment of modern state-of-the-art LLMs. </p>
<blockquote>
<p>éšç€LLMçš„å‚æ•°è§„æ¨¡ç»§ç»­å¢åŠ ï¼Œè¿è¡Œå®ƒä»¬æ‰€éœ€çš„è®¡ç®—èµ„æºå¯ç”¨çš„æ–¹è¶Šæ¥è¶Šå°‘ã€‚å› æ­¤ï¼Œç¬¬ä¸‰æ–¹æ¨ç†æœåŠ¡â€”â€”LLMç”±å…·æœ‰å¤§é‡è®¡ç®—èµ„æºçš„ç¬¬ä¸‰æ–¹æ‰˜ç®¡â€”â€”è¶Šæ¥è¶Šå—æ¬¢è¿ã€‚ç„¶è€Œï¼Œç¬¬ä¸‰æ–¹æ¨ç†å¼•å‘äº†å…³äºç”¨æˆ·æ•°æ®éšç§çš„å…³é”®æ‹…å¿§ã€‚ä¸ºäº†å‡è½»è¿™äº›é£é™©ï¼Œéšç§ç ”ç©¶äººå‘˜å·²ç»ä¸ºç¬¬ä¸‰æ–¹æ¨ç†å¼€å‘äº†å¯è¯æ˜çš„å®‰å…¨æ–¹æ¡ˆï¼Œå¦‚å®‰å…¨å¤šæ–¹è®¡ç®—ï¼ˆSMPCï¼‰ã€‚ç„¶è€Œï¼ŒSMPCåè®®å…·æœ‰é‡å¤§çš„è®¡ç®—å’Œé€šä¿¡å¼€é”€ï¼Œå¹¶ä¸èƒ½æ‰©å±•åˆ°å¤§å‹æ¨¡å‹ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„å¤šæ–¹æ¨ç†åè®®Cascadeï¼Œå®ƒé€šè¿‡åˆ©ç”¨åºåˆ—ç»´åº¦ä¸Šçš„åˆ†ç‰‡æ¥ä¿æŒéšç§ï¼Œé¿å…äº†è¿™äº›æƒ©ç½šæ€§æˆæœ¬ï¼Œä»¥ç‰ºç‰²éƒ¨åˆ†åŠ å¯†éšç§ä¿è¯æ¥æ¢å–æ›´é«˜çš„æ€§èƒ½å’Œå¯æ‰©å±•æ€§ã€‚æˆ‘ä»¬è¯æ˜Cascadeèƒ½å¤ŸæŠµæŠ—ä¸€ç§é’ˆå¯¹å…¶ä»–ç»Ÿè®¡éšç§æ–¹æ¡ˆçš„æœ€æ–°æ”»å‡»çš„ä¸€èˆ¬åŒ–å½¢å¼ï¼Œå¹¶ä¸”è¿›ä¸€æ­¥æŠµæŠ—åŸºäºå­¦ä¹ çš„æ”»å‡»ã€‚ç”±äºCascadeæ¯”ç°æœ‰æ–¹æ¡ˆå¿«å‡ ä¸ªæ•°é‡çº§ï¼Œæˆ‘ä»¬çš„å‘ç°ä¸ºç°ä»£æœ€æ–°LLMçš„å®‰å…¨éƒ¨ç½²æä¾›äº†å®ç”¨è§£å†³æ–¹æ¡ˆã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.05228v1">PDF</a> To be published in ICML 2025 Main Proceedings as â€œHidden No More:   Attacking and Defending Private Third-Party LLM Inferenceâ€, together with   arXiv:2505.18332</p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å‚æ•°è§„æ¨¡çš„ä¸æ–­å¢é•¿å¯¹è®¡ç®—èµ„æºçš„éœ€æ±‚è¶Šæ¥è¶Šé«˜ï¼Œä½¿å¾—ç¬¬ä¸‰æ–¹æ¨ç†æœåŠ¡æ„ˆå‘æµè¡Œã€‚ç„¶è€Œï¼Œè¿™å¼•å‘äº†ç”¨æˆ·æ•°æ®éšç§çš„é‡å¤§æ‹…å¿§ã€‚ä¸ºç¼“è§£è¿™äº›é£é™©ï¼Œéšç§ç ”ç©¶äººå‘˜å·²å¼€å‘å‡ºä¸€äº›å¦‚å®‰å…¨å¤šæ–¹è®¡ç®—ï¼ˆSMPCï¼‰çš„å®‰å…¨æ–¹æ¡ˆã€‚ä½†SMPCåè®®å­˜åœ¨è®¡ç®—ä¸é€šä¿¡å¼€é”€å¤§ã€æ— æ³•æ‰©å±•åˆ°å¤§å‹æ¨¡å‹çš„é—®é¢˜ã€‚æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§æ–°çš„å¤šæ–¹æ¨ç†åè®®Cascadeï¼Œé€šè¿‡åˆ©ç”¨åºåˆ—ç»´åº¦çš„åˆ†ç‰‡æ¥ä¿æŒéšç§ï¼Œä»¥ç‰ºç‰²éƒ¨åˆ†åŠ å¯†éšç§ä¿è¯æ¥æ¢å–æ›´é«˜çš„æ€§èƒ½å’Œå¯æ‰©å±•æ€§ã€‚Cascadeå¯¹ä¸€ç§é’ˆå¯¹å…¶ä»–ç»Ÿè®¡éšç§æ–¹æ¡ˆçš„æœ€æ–°æ”»å‡»å…·æœ‰æŠµæŠ—åŠ›ï¼Œå¹¶ä¸”èƒ½æŠµæŠ—åŸºäºå­¦ä¹ çš„æ”»å‡»ã€‚ç”±äºCascadeæ¯”ç°æœ‰æ–¹æ¡ˆé€Ÿåº¦å¿«å‡ ä¸ªæ•°é‡çº§ï¼Œå› æ­¤ä¸ºç°ä»£å‰æ²¿çš„å¤§å‹è¯­è¨€æ¨¡å‹çš„å®‰å…¨éƒ¨ç½²æä¾›äº†å®é™…è§£å†³æ–¹æ¡ˆã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLMså‚æ•°è§„æ¨¡å¢é•¿å¯¼è‡´è®¡ç®—èµ„æºéœ€æ±‚å¢åŠ ï¼Œç¬¬ä¸‰æ–¹æ¨ç†æœåŠ¡å› æ­¤å˜å¾—æµè¡Œã€‚</li>
<li>ç¬¬ä¸‰æ–¹æ¨ç†æœåŠ¡å¼•å‘ç”¨æˆ·æ•°æ®éšç§çš„é‡å¤§æ‹…å¿§ã€‚</li>
<li>éšç§ç ”ç©¶äººå‘˜å·²ç»å¼€å‘äº†ä¸€äº›å®‰å…¨æ–¹æ¡ˆå¦‚SMPCæ¥è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œä½†å­˜åœ¨è®¡ç®—å¼€é”€å¤§ã€æ— æ³•æ‰©å±•åˆ°å¤§å‹æ¨¡å‹çš„ç¼ºç‚¹ã€‚</li>
<li>æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§æ–°çš„å¤šæ–¹æ¨ç†åè®®Cascadeï¼Œé€šè¿‡åˆ©ç”¨åºåˆ—ç»´åº¦çš„åˆ†ç‰‡æ¥ä¿æŒéšç§ï¼Œç‰ºç‰²éƒ¨åˆ†åŠ å¯†éšç§ä¿è¯ä»¥æ¢å–æ›´é«˜æ€§èƒ½å’Œå¯æ‰©å±•æ€§ã€‚</li>
<li>Cascadeèƒ½æœ‰æ•ˆæŠµæŠ—é’ˆå¯¹å…¶ä»–ç»Ÿè®¡éšç§æ–¹æ¡ˆçš„æœ€æ–°æ”»å‡»ï¼Œä¹Ÿèƒ½æŠµæŠ—åŸºäºå­¦ä¹ çš„æ”»å‡»ã€‚</li>
<li>Cascadeç›¸æ¯”ç°æœ‰æ–¹æ¡ˆé€Ÿåº¦æ›´å¿«ï¼Œä¸ºç°ä»£å‰æ²¿çš„å¤§å‹è¯­è¨€æ¨¡å‹çš„å®‰å…¨éƒ¨ç½²æä¾›äº†å®é™…è§£å†³æ–¹æ¡ˆã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.05228">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-9d56016dfe65266fff3d7896153e9855.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-fa5ca70eba29004f6bd29eec05261bb0.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-678d84049019372a5cd34b9d5b9dd447.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-4036186af98113e7dc402f8b7a7739d1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-712e57d24f77b561aed1e90ef2f46615.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="All-in-One-Visual-Description-Guided-Unified-Point-Cloud-Segmentation"><a href="#All-in-One-Visual-Description-Guided-Unified-Point-Cloud-Segmentation" class="headerlink" title="All in One: Visual-Description-Guided Unified Point Cloud Segmentation"></a>All in One: Visual-Description-Guided Unified Point Cloud Segmentation</h2><p><strong>Authors:Zongyan Han, Mohamed El Amine Boudjoghra, Jiahua Dong, Jinhong Wang, Rao Muhammad Anwer</strong></p>
<p>Unified segmentation of 3D point clouds is crucial for scene understanding, but is hindered by its sparse structure, limited annotations, and the challenge of distinguishing fine-grained object classes in complex environments. Existing methods often struggle to capture rich semantic and contextual information due to limited supervision and a lack of diverse multimodal cues, leading to suboptimal differentiation of classes and instances. To address these challenges, we propose VDG-Uni3DSeg, a novel framework that integrates pre-trained vision-language models (e.g., CLIP) and large language models (LLMs) to enhance 3D segmentation. By leveraging LLM-generated textual descriptions and reference images from the internet, our method incorporates rich multimodal cues, facilitating fine-grained class and instance separation. We further design a Semantic-Visual Contrastive Loss to align point features with multimodal queries and a Spatial Enhanced Module to model scene-wide relationships efficiently. Operating within a closed-set paradigm that utilizes multimodal knowledge generated offline, VDG-Uni3DSeg achieves state-of-the-art results in semantic, instance, and panoptic segmentation, offering a scalable and practical solution for 3D understanding. Our code is available at <a target="_blank" rel="noopener" href="https://github.com/Hanzy1996/VDG-Uni3DSeg">https://github.com/Hanzy1996/VDG-Uni3DSeg</a>. </p>
<blockquote>
<p>ä¸‰ç»´ç‚¹äº‘çš„ç»Ÿä¸€åˆ†å‰²å¯¹äºåœºæ™¯ç†è§£è‡³å…³é‡è¦ï¼Œä½†å…¶ç¨€ç–ç»“æ„ã€æœ‰é™çš„æ ‡æ³¨ä»¥åŠå¤æ‚ç¯å¢ƒä¸­ç²¾ç»†å¯¹è±¡ç±»åˆ«çš„åŒºåˆ†æŒ‘æˆ˜é˜»ç¢äº†å…¶å‘å±•ã€‚ç°æœ‰æ–¹æ³•ç”±äºç›‘ç£æœ‰é™å’Œç¼ºä¹å¤šæ ·çš„å¤šæ¨¡å¼çº¿ç´¢ï¼Œå¾€å¾€éš¾ä»¥æ•æ‰ä¸°å¯Œçš„è¯­ä¹‰å’Œä¸Šä¸‹æ–‡ä¿¡æ¯ï¼Œå¯¼è‡´ç±»åˆ«å’Œå®ä¾‹çš„åŒºåˆ†ä¸ä½³ã€‚ä¸ºäº†è§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†VDG-Uni3DSegï¼Œä¸€ä¸ªç»“åˆé¢„è®­ç»ƒçš„è¯­è¨€è§†è§‰æ¨¡å‹ï¼ˆä¾‹å¦‚CLIPï¼‰å’Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ¥å¢å¼ºä¸‰ç»´åˆ†å‰²çš„æ–°å‹æ¡†æ¶ã€‚é€šè¿‡åˆ©ç”¨LLMç”Ÿæˆçš„æ–‡æœ¬æè¿°å’Œäº’è”ç½‘ä¸Šçš„å‚è€ƒå›¾åƒï¼Œæˆ‘ä»¬çš„æ–¹æ³•èå…¥äº†ä¸°å¯Œçš„å¤šæ¨¡å¼çº¿ç´¢ï¼Œä¿ƒè¿›äº†ç²¾ç»†çš„ç±»åˆ«å’Œå®ä¾‹åˆ†ç¦»ã€‚æˆ‘ä»¬è¿›ä¸€æ­¥è®¾è®¡äº†ä¸€ç§è¯­ä¹‰è§†è§‰å¯¹æ¯”æŸå¤±ï¼Œå°†ç‚¹ç‰¹å¾ä¸å¤šæ¨¡å¼æŸ¥è¯¢å¯¹é½ï¼Œå¹¶è®¾è®¡äº†ä¸€ä¸ªç©ºé—´å¢å¼ºæ¨¡å—ï¼Œä»¥æœ‰æ•ˆåœ°å»ºæ¨¡åœºæ™¯èŒƒå›´å†…çš„å…³ç³»ã€‚VDG-Uni3DSegåœ¨è¯­ä¹‰ã€å®ä¾‹å’Œå…¨æ™¯åˆ†å‰²æ–¹é¢é‡‡ç”¨äº†åˆ©ç”¨ç¦»çº¿ç”Ÿæˆçš„å¤šæ¨¡å¼çŸ¥è¯†çš„å°é—­é›†èŒƒå¼ï¼Œå®ç°äº†æœ€å…ˆè¿›çš„æˆæœï¼Œä¸ºä¸‰ç»´ç†è§£æä¾›äº†å¯æ‰©å±•å’Œå®ç”¨çš„è§£å†³æ–¹æ¡ˆã€‚æˆ‘ä»¬çš„ä»£ç å¯åœ¨&lt;<a target="_blank" rel="noopener" href="https://github.com/Hanzy199">https://github.com/Hanzy199</a> 6&#x2F;VDG-Uni3DSeg&gt;è·å¾—ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.05211v1">PDF</a> Accepted by ICCV2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºä¸€ç§åä¸ºVDG-Uni3DSegçš„æ–°å‹æ¡†æ¶ï¼Œç”¨äºè§£å†³ä¸‰ç»´ç‚¹äº‘ç»Ÿä¸€åˆ†å‰²ä¸­çš„æŒ‘æˆ˜ã€‚è¯¥æ¡†æ¶ç»“åˆäº†é¢„è®­ç»ƒçš„è§†è§‰è¯­è¨€æ¨¡å‹å’Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ï¼Œé€šè¿‡åˆ©ç”¨LLMç”Ÿæˆçš„æ–‡æœ¬æè¿°å’Œäº’è”ç½‘ä¸Šçš„å‚è€ƒå›¾åƒï¼Œèå…¥ä¸°å¯Œçš„å¤šæ¨¡æ€çº¿ç´¢ï¼Œæå‡å¯¹ç²¾ç»†ç±»åˆ«å’Œå®ä¾‹çš„è¾¨è¯†èƒ½åŠ›ã€‚è®¾è®¡è¯­ä¹‰è§†è§‰å¯¹æ¯”æŸå¤±å’Œç©ºé—´å¢å¼ºæ¨¡å—ï¼Œåˆ†åˆ«ç”¨äºå¯¹é½ç‚¹ç‰¹å¾å’Œå¤šæ¨¡æ€æŸ¥è¯¢ä»¥åŠæœ‰æ•ˆå»ºæ¨¡åœºæ™¯å†…å…³ç³»ã€‚åœ¨é‡‡ç”¨ç¦»çº¿ç”Ÿæˆçš„å¤šæ¨¡æ€çŸ¥è¯†é—­é›†æ¨¡å¼ä¸‹ï¼ŒVDG-Uni3DSegåœ¨è¯­ä¹‰ã€å®ä¾‹å’Œå…¨æ™¯åˆ†å‰²ä¸Šå–å¾—é¢†å…ˆç»“æœï¼Œä¸ºä¸‰ç»´ç†è§£æä¾›å¯æ‰©å±•ä¸”å®ç”¨çš„è§£å†³æ–¹æ¡ˆã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ä¸‰ç»´ç‚¹äº‘ç»Ÿä¸€åˆ†å‰²å¯¹äºåœºæ™¯ç†è§£è‡³å…³é‡è¦ï¼Œé¢ä¸´ç¨€ç–ç»“æ„ã€æœ‰é™æ ‡æ³¨å’Œå¤æ‚ç¯å¢ƒä¸­ç²¾ç»†ç±»åˆ«åŒºåˆ†ç­‰æŒ‘æˆ˜ã€‚</li>
<li>ç°æœ‰æ–¹æ³•å› ç¼ºä¹ä¸°å¯Œè¯­ä¹‰å’Œä¸Šä¸‹æ–‡ä¿¡æ¯ä»¥åŠå¤šæ ·çš„å¤šæ¨¡æ€çº¿ç´¢ï¼Œå¯¼è‡´ç±»åˆ«å’Œå®ä¾‹åŒºåˆ†ä¸è¶³ã€‚</li>
<li>VDG-Uni3DSegæ¡†æ¶ç»“åˆäº†é¢„è®­ç»ƒçš„è§†è§‰è¯­è¨€æ¨¡å‹å’Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ï¼Œèå…¥å¤šæ¨¡æ€çº¿ç´¢ï¼Œæå‡ç²¾ç»†ç±»åˆ«å’Œå®ä¾‹è¾¨è¯†èƒ½åŠ›ã€‚</li>
<li>é€šè¿‡åˆ©ç”¨LLMç”Ÿæˆçš„æ–‡æœ¬æè¿°å’Œäº’è”ç½‘ä¸Šçš„å‚è€ƒå›¾åƒï¼Œä¸°å¯Œäº†ç‚¹äº‘æ•°æ®çš„ç‰¹å¾å’Œä¸Šä¸‹æ–‡ä¿¡æ¯ã€‚</li>
<li>è®¾è®¡çš„è¯­ä¹‰è§†è§‰å¯¹æ¯”æŸå¤±æœ‰åŠ©äºå¯¹é½ç‚¹ç‰¹å¾å’Œå¤šæ¨¡æ€æŸ¥è¯¢ã€‚</li>
<li>ç©ºé—´å¢å¼ºæ¨¡å—èƒ½å¤Ÿé«˜æ•ˆå»ºæ¨¡åœºæ™¯å†…å…³ç³»ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.05211">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-2d161c01e43733bbf1faeb5c44771f84.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-bdd6c964b80d5bff0eb32309942851ae.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-343f85ba71b6426fbd0da6be42292b9f.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="CREW-WILDFIRE-Benchmarking-Agentic-Multi-Agent-Collaborations-at-Scale"><a href="#CREW-WILDFIRE-Benchmarking-Agentic-Multi-Agent-Collaborations-at-Scale" class="headerlink" title="CREW-WILDFIRE: Benchmarking Agentic Multi-Agent Collaborations at Scale"></a>CREW-WILDFIRE: Benchmarking Agentic Multi-Agent Collaborations at Scale</h2><p><strong>Authors:Jonathan Hyun, Nicholas R Waytowich, Boyuan Chen</strong></p>
<p>Despite rapid progress in large language model (LLM)-based multi-agent systems, current benchmarks fall short in evaluating their scalability, robustness, and coordination capabilities in complex, dynamic, real-world tasks. Existing environments typically focus on small-scale, fully observable, or low-complexity domains, limiting their utility for developing and assessing next-generation multi-agent Agentic AI frameworks. We introduce CREW-Wildfire, an open-source benchmark designed to close this gap. Built atop the human-AI teaming CREW simulation platform, CREW-Wildfire offers procedurally generated wildfire response scenarios featuring large maps, heterogeneous agents, partial observability, stochastic dynamics, and long-horizon planning objectives. The environment supports both low-level control and high-level natural language interactions through modular Perception and Execution modules. We implement and evaluate several state-of-the-art LLM-based multi-agent Agentic AI frameworks, uncovering significant performance gaps that highlight the unsolved challenges in large-scale coordination, communication, spatial reasoning, and long-horizon planning under uncertainty. By providing more realistic complexity, scalable architecture, and behavioral evaluation metrics, CREW-Wildfire establishes a critical foundation for advancing research in scalable multi-agent Agentic intelligence. All code, environments, data, and baselines will be released to support future research in this emerging domain. </p>
<blockquote>
<p>å°½ç®¡åŸºäºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„å¤šæ™ºèƒ½ä½“ç³»ç»Ÿå–å¾—äº†å¿«é€Ÿè¿›å±•ï¼Œä½†å½“å‰çš„åŸºå‡†æµ‹è¯•åœ¨è¯„ä¼°å…¶åœ¨å¤æ‚ã€åŠ¨æ€ã€ç°å®ä»»åŠ¡çš„å¯æ‰©å±•æ€§ã€ç¨³å¥æ€§å’Œåè°ƒèƒ½åŠ›æ–¹é¢è¿˜å­˜åœ¨ä¸è¶³ã€‚ç°æœ‰ç¯å¢ƒé€šå¸¸ä¸“æ³¨äºå°è§„æ¨¡ã€å®Œå…¨å¯è§‚å¯Ÿæˆ–ä½å¤æ‚åº¦çš„é¢†åŸŸï¼Œè¿™é™åˆ¶äº†å®ƒä»¬åœ¨å¼€å‘ä¸‹ä¸€ä»£å¤šæ™ºèƒ½ä½“Agenticäººå·¥æ™ºèƒ½æ¡†æ¶ä¸­çš„å®ç”¨æ€§ã€‚æˆ‘ä»¬æ¨å‡ºäº†CREW-Wildfireï¼Œè¿™æ˜¯ä¸€ä¸ªæ—¨åœ¨å¼¥è¡¥è¿™ä¸€å·®è·çš„å¼€æºåŸºå‡†æµ‹è¯•ã€‚å®ƒå»ºç«‹åœ¨äººç±»äººå·¥æ™ºèƒ½å›¢é˜ŸCREWä»¿çœŸå¹³å°ä¹‹ä¸Šï¼Œæä¾›äº†ç¨‹åºåŒ–ç”Ÿæˆçš„é‡ç«åº”å¯¹åœºæ™¯ï¼ŒåŒ…æ‹¬å¤§å‹åœ°å›¾ã€å¼‚æ„æ™ºèƒ½ä½“ã€éƒ¨åˆ†å¯è§‚å¯Ÿæ€§ã€éšæœºåŠ¨æ€ä»¥åŠé•¿æœŸè§„åˆ’ç›®æ ‡ã€‚è¯¥ç¯å¢ƒé€šè¿‡æ¨¡å—åŒ–æ„ŸçŸ¥å’Œæ‰§è¡Œæ¨¡å—æ”¯æŒä½çº§æ§åˆ¶å’Œé«˜çº§è‡ªç„¶è¯­è¨€äº¤äº’ã€‚æˆ‘ä»¬å®ç°å¹¶è¯„ä¼°äº†å¤šç§åŸºäºæœ€æ–°å¤§å‹è¯­è¨€æ¨¡å‹çš„å¤šæ™ºèƒ½ä½“Agenticäººå·¥æ™ºèƒ½æ¡†æ¶ï¼Œå‘ç°äº†æ˜¾è‘—çš„æ€§èƒ½å·®è·ï¼Œå‡¸æ˜¾å‡ºåœ¨è§£å†³å¤§è§„æ¨¡åè°ƒã€é€šä¿¡ã€ç©ºé—´æ¨ç†å’Œé•¿æœŸä¸ç¡®å®šæ€§è§„åˆ’æ–¹é¢çš„æŒ‘æˆ˜ã€‚é€šè¿‡æä¾›æ›´ç°å®çš„å¤æ‚æ€§ã€å¯æ‰©å±•çš„æ¶æ„å’Œè¡Œä¸ºè¯„ä¼°æŒ‡æ ‡ï¼ŒCREW-Wildfireä¸ºæ¨è¿›å¯æ‰©å±•å¤šæ™ºèƒ½ä½“Agenticæ™ºèƒ½çš„ç ”ç©¶å¥ å®šäº†å…³é”®åŸºç¡€ã€‚æ‰€æœ‰ä»£ç ã€ç¯å¢ƒã€æ•°æ®å’ŒåŸºå‡†æµ‹è¯•éƒ½å°†å‘å¸ƒï¼Œä»¥æ”¯æŒè¿™ä¸€æ–°å…´é¢†åŸŸçš„æœªæ¥ç ”ç©¶ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.05178v1">PDF</a> Our project website is at:   <a target="_blank" rel="noopener" href="http://generalroboticslab.com/CREW-Wildfire">http://generalroboticslab.com/CREW-Wildfire</a></p>
<p><strong>Summary</strong>ï¼š</p>
<p>è™½ç„¶å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨å¤šæ™ºèƒ½ä½“ç³»ç»Ÿä¸­çš„åº”ç”¨å–å¾—äº†å¿«é€Ÿè¿›å±•ï¼Œä½†ç°æœ‰è¯„ä¼°åŸºå‡†æµ‹è¯•åœ¨è¯„ä¼°å…¶åœ¨å¤æ‚åŠ¨æ€ç°å®ä¸–ç•Œä»»åŠ¡ä¸­çš„å¯æ‰©å±•æ€§ã€ç¨³å¥æ€§å’Œåè°ƒèƒ½åŠ›æ–¹é¢å­˜åœ¨ä¸è¶³ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œæœ¬æ–‡å¼•å…¥äº†CREW-Wildfireè¿™ä¸€å¼€æ”¾æºç åŸºå‡†æµ‹è¯•ï¼Œå…¶åŸºäºäººç±»-äººå·¥æ™ºèƒ½å›¢é˜Ÿåˆä½œçš„CREWæ¨¡æ‹Ÿå¹³å°ï¼Œå…·æœ‰ç¨‹åºç”Ÿæˆçš„å¤§å‹åœ°å›¾ã€å¼‚ç§æ™ºèƒ½ä½“ã€éƒ¨åˆ†å¯è§‚å¯Ÿæ€§ã€éšæœºåŠ¨æ€æ€§å’Œé•¿æœŸè§„åˆ’ç›®æ ‡ç­‰ç‰¹æ€§ã€‚æœ¬æ–‡å®ç°äº†å¤šä¸ªå…ˆè¿›çš„LLMå¤šæ™ºèƒ½ä½“Agenticäººå·¥æ™ºèƒ½æ¡†æ¶å¹¶å¯¹å…¶è¿›è¡Œäº†è¯„ä¼°ï¼Œæ­ç¤ºäº†å¤§è§„æ¨¡åè°ƒã€æ²Ÿé€šã€ç©ºé—´æ¨ç†å’Œé•¿æœŸä¸ç¡®å®šæ€§è§„åˆ’ç­‰æ–¹é¢çš„æ˜¾è‘—æ€§èƒ½å·®è·ã€‚é€šè¿‡æä¾›æ›´ç°å®çš„å¤æ‚æ€§ã€å¯æ‰©å±•çš„æ¶æ„å’Œè¡Œä¸ºè¯„ä¼°æŒ‡æ ‡ï¼ŒCREW-Wildfireä¸ºæ¨è¿›å¤šæ™ºèƒ½ä½“Agenticæ™ºèƒ½çš„ç ”ç©¶å¥ å®šäº†é‡è¦åŸºç¡€ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>å½“å‰LLMå¤šæ™ºèƒ½ä½“ç³»ç»Ÿçš„è¯„ä¼°åŸºå‡†æµ‹è¯•åœ¨è¯„ä¼°å…¶åœ¨å®é™…å¤æ‚ç¯å¢ƒä¸­çš„æ€§èƒ½æ—¶å­˜åœ¨å±€é™æ€§ã€‚</li>
<li>CREW-Wildfireæ˜¯ä¸€ä¸ªæ–°çš„å¼€æ”¾æºç åŸºå‡†æµ‹è¯•ï¼Œç”¨äºè¯„ä¼°LLMå¤šæ™ºèƒ½ä½“ç³»ç»Ÿåœ¨æ¨¡æ‹Ÿç°å®ä¸–ç•Œåœºæ™¯ä¸­çš„æ€§èƒ½ã€‚</li>
<li>CREW-Wildfireæä¾›äº†ç¨‹åºç”Ÿæˆçš„å¤§å‹åœ°å›¾ã€ä¸åŒç§ç±»çš„æ™ºèƒ½ä½“ã€éƒ¨åˆ†å¯è§‚å¯Ÿæ€§ã€éšæœºåŠ¨æ€æ€§å’Œé•¿æœŸè§„åˆ’ç›®æ ‡ç­‰ç‰¹æ€§ã€‚</li>
<li>é€šè¿‡åœ¨CREW-Wildfireç¯å¢ƒä¸­å®æ–½å’Œè¯„ä¼°å¤šä¸ªå…ˆè¿›çš„LLMå¤šæ™ºèƒ½ä½“Agenticäººå·¥æ™ºèƒ½æ¡†æ¶ï¼Œå‘ç°äº†åœ¨åè°ƒã€æ²Ÿé€šã€ç©ºé—´æ¨ç†å’Œé•¿æœŸè§„åˆ’æ–¹é¢çš„æŒ‘æˆ˜ã€‚</li>
<li>CREW-Wildfireå¡«è¡¥äº†ç°æœ‰è¯„ä¼°åŸºå‡†æµ‹è¯•çš„ä¸è¶³ï¼Œä¸ºå¤šæ™ºèƒ½ä½“Agenticæ™ºèƒ½çš„ç ”ç©¶æä¾›äº†æ›´çœŸå®ã€å¯æ‰©å±•çš„è¯„ä¼°ç¯å¢ƒã€‚</li>
<li>CREW-Wildfireçš„å‘å¸ƒåŒ…æ‹¬ä»£ç ã€ç¯å¢ƒã€æ•°æ®å’ŒåŸºå‡†çº¿ï¼Œä»¥æ”¯æŒæœªæ¥åœ¨è¯¥é¢†åŸŸçš„ç ”ç©¶ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.05178">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-21680c2a1c88b72ca715a9f4c9084147.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-984afccab52b84ffb08ad3765ebdde61.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-3f0b51f277906237d09bb5ed9fc9c6a1.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-1e91e5b306b5e621646eebe639299854.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="OpenS2S-Advancing-Open-Source-End-to-End-Empathetic-Large-Speech-Language-Model"><a href="#OpenS2S-Advancing-Open-Source-End-to-End-Empathetic-Large-Speech-Language-Model" class="headerlink" title="OpenS2S: Advancing Open-Source End-to-End Empathetic Large Speech   Language Model"></a>OpenS2S: Advancing Open-Source End-to-End Empathetic Large Speech   Language Model</h2><p><strong>Authors:Chen Wang, Tianyu Peng, Wen Yang, Yinan Bai, Guangfu Wang, Jun Lin, Lanpeng Jia, Lingxiang Wu, Jinqiao Wang, Chengqing Zong, Jiajun Zhang</strong></p>
<p>Empathetic interaction is a cornerstone of human-machine communication, due to the need for understanding speech enriched with paralinguistic cues and generating emotional and expressive responses. However, the most powerful empathetic LSLMs are increasingly closed off, leaving the crucial details about the architecture, data and development opaque to researchers. Given the critical need for transparent research into the LSLMs and empathetic behavior, we present OpenS2S, a fully open-source, transparent and end-to-end LSLM designed to enable empathetic speech interactions. Based on our empathetic speech-to-text model BLSP-Emo, OpenS2S further employs a streaming interleaved decoding architecture to achieve low-latency speech generation. To facilitate end-to-end training, OpenS2S incorporates an automated data construction pipeline that synthesizes diverse, high-quality empathetic speech dialogues at low cost. By leveraging large language models to generate empathetic content and controllable text-to-speech systems to introduce speaker and emotional variation, we construct a scalable training corpus with rich paralinguistic diversity and minimal human supervision. We release the fully open-source OpenS2S model, including the dataset, model weights, pre-training and fine-tuning codes, to empower the broader research community and accelerate innovation in empathetic speech systems. The project webpage can be accessed at <a target="_blank" rel="noopener" href="https://casia-lm.github.io/OpenS2S">https://casia-lm.github.io/OpenS2S</a> </p>
<blockquote>
<p>å…±æƒ…äº¤äº’æ˜¯äººç±»ä¸æœºå™¨é€šä¿¡çš„åŸºçŸ³ï¼Œè¿™æ˜¯å› ä¸ºéœ€è¦ç†è§£èå…¥å‰¯è¯­è¨€çº¿ç´¢çš„è¯­éŸ³å¹¶ç”Ÿæˆæƒ…æ„Ÿå’Œè¡¨è¾¾æ€§å“åº”ã€‚ç„¶è€Œï¼Œæœ€å¼ºå¤§çš„å…±æƒ…LSLMè¶Šæ¥è¶Šå°é—­ï¼Œä½¿å¾—å…³äºæ¶æ„ã€æ•°æ®ä»¥åŠå¼€å‘çš„å…³é”®ç»†èŠ‚å¯¹ç ”ç©¶è€…æ¥è¯´æ¨¡ç³Šä¸æ¸…ã€‚é‰´äºå¯¹LSLMå’Œå…±æƒ…è¡Œä¸ºé€æ˜ç ”ç©¶çš„è¿«åˆ‡éœ€æ±‚ï¼Œæˆ‘ä»¬æ¨å‡ºäº†OpenS2Sï¼Œè¿™æ˜¯ä¸€ä¸ªå®Œå…¨å¼€æºã€é€æ˜å’Œç«¯åˆ°ç«¯çš„LSLMï¼Œæ—¨åœ¨å®ç°å…±æƒ…è¯­éŸ³äº¤äº’ã€‚åŸºäºæˆ‘ä»¬çš„å…±æƒ…è¯­éŸ³åˆ°æ–‡æœ¬æ¨¡å‹BLSP-Emoï¼ŒOpenS2Sè¿›ä¸€æ­¥é‡‡ç”¨æµå¼äº¤ç»‡è§£ç æ¶æ„å®ç°ä½å»¶è¿Ÿè¯­éŸ³ç”Ÿæˆã€‚ä¸ºäº†ä¿ƒè¿›ç«¯åˆ°ç«¯è®­ç»ƒï¼ŒOpenS2Sé‡‡ç”¨è‡ªåŠ¨åŒ–æ•°æ®æ„å»ºç®¡é“ï¼Œä»¥ä½æˆæœ¬åˆæˆå¤šæ ·ã€é«˜è´¨é‡çš„å…±æƒ…è¯­éŸ³å¯¹è¯ã€‚é€šè¿‡åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ç”Ÿæˆå…±æƒ…å†…å®¹ä»¥åŠå¯æ§çš„æ–‡æœ¬åˆ°è¯­éŸ³ç³»ç»Ÿå¼•å…¥å‘è¨€è€…å’Œæƒ…æ„Ÿå˜åŒ–ï¼Œæˆ‘ä»¬æ„å»ºäº†ä¸€ä¸ªå¯æ‰©å±•çš„è®­ç»ƒè¯­æ–™åº“ï¼Œå…·æœ‰ä¸°å¯Œçš„å‰¯è¯­è¨€å¤šæ ·æ€§å’Œæœ€å°çš„äººå·¥ç›‘ç£ã€‚æˆ‘ä»¬å…¬å¼€å‘å¸ƒäº†å®Œå…¨å¼€æºçš„OpenS2Sæ¨¡å‹ï¼ŒåŒ…æ‹¬æ•°æ®é›†ã€æ¨¡å‹æƒé‡ã€é¢„è®­ç»ƒå’Œå¾®è°ƒä»£ç ï¼Œä»¥æ”¯æŒæ›´å¹¿æ³›çš„ç ”ç©¶ç¾¤ä½“å¹¶åŠ é€Ÿå…±æƒ…è¯­éŸ³ç³»ç»Ÿçš„åˆ›æ–°ã€‚é¡¹ç›®ç½‘é¡µå¯é€šè¿‡<a target="_blank" rel="noopener" href="https://casia-lm.github.io/OpenS2S%E8%AE%BF%E9%97%AE%E3%80%82">https://casia-lm.github.io/OpenS2Sè®¿é—®ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.05177v1">PDF</a> Technical Report</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡å¼ºè°ƒäº†åœ¨äººæœºé€šä¿¡ä¸­ï¼Œç†è§£å¸¦æœ‰å‰¯è¯­è¨€çº¿ç´¢çš„è¯­éŸ³å’Œç”Ÿæˆæƒ…æ„Ÿä¸°å¯Œã€æœ‰è¡¨ç°åŠ›çš„å›åº”æ˜¯å…³é”®æ‰€åœ¨ã€‚ä¸ºåº”å¯¹å¼ºå¤§çš„ç±»æƒ…ç»ªäº¤æµå­¦ä¹ æ¨¡å‹çš„ä¸é€æ˜æ€§æŒ‘æˆ˜ï¼Œç ”ç©¶äººå‘˜æ¨å‡ºäº†ä¸€ä¸ªå®Œå…¨å¼€æºçš„ç±»æƒ…ç»ªè¯­è¨€æ¨¡å‹â€”â€”OpenS2Sï¼Œä»¥å®ç°æœ‰åŒç†å¿ƒçš„è¯­éŸ³äº¤äº’ã€‚è¯¥æ¨¡å‹å…·æœ‰ä½å»¶è¿Ÿçš„è¯­éŸ³è¯†åˆ«ç‰¹ç‚¹ï¼Œåˆ©ç”¨æ•°æ®æµäº¤é”™è§£ç æ¶æ„ï¼Œå¹¶ä¸”è®¾æœ‰è‡ªåŠ¨æ•°æ®æ„å»ºç®¡é“ä»¥å®ç°ç«¯åˆ°ç«¯çš„è®­ç»ƒã€‚OpenS2Sæ—¨åœ¨é€šè¿‡å¤§è§„æ¨¡è¯­è¨€æ¨¡å‹ç”Ÿæˆæœ‰åŒç†å¿ƒçš„å†…å®¹ï¼Œå¹¶åˆ©ç”¨å¯æ§çš„æ–‡æœ¬åˆ°è¯­éŸ³ç³»ç»Ÿå¼•å…¥è¯´è¯è€…å’Œæƒ…æ„Ÿå˜åŒ–ï¼Œæ„å»ºå…·æœ‰ä¸°å¯Œå‰¯è¯­è¨€å¤šæ ·æ€§å’Œæœ€å°äººå·¥ç›‘ç£çš„å¯æ‰©å±•è®­ç»ƒè¯­æ–™åº“ã€‚è¯¥é¡¹ç›®å·²å®Œå…¨å¼€æºï¼ŒåŒ…æ‹¬æ•°æ®é›†ã€æ¨¡å‹æƒé‡ã€é¢„è®­ç»ƒå’Œå¾®è°ƒä»£ç ç­‰ï¼Œæ—¨åœ¨åŠ©åŠ›æ›´å¹¿æ³›çš„ç ”ç©¶ç¾¤ä½“å¹¶åŠ é€ŸåŒç†å¿ƒè¯­éŸ³ç³»ç»Ÿçš„åˆ›æ–°ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>äººæœºäº¤äº’éœ€è¦ç†è§£å¸¦æœ‰å‰¯è¯­è¨€çº¿ç´¢çš„è¯­éŸ³å’Œç”Ÿæˆæƒ…æ„Ÿä¸°å¯Œçš„å›åº”ã€‚</li>
<li>OpenS2Sæ˜¯ä¸€ä¸ªæ—¨åœ¨å®ç°æœ‰åŒç†å¿ƒçš„è¯­éŸ³äº¤äº’çš„å®Œå…¨å¼€æºçš„ç±»æƒ…ç»ªè¯­è¨€æ¨¡å‹ã€‚</li>
<li>OpenS2Sæ¨¡å‹å…·æœ‰ä½å»¶è¿Ÿçš„è¯­éŸ³è¯†åˆ«ç‰¹ç‚¹ã€‚</li>
<li>æ•°æ®æµäº¤é”™è§£ç æ¶æ„åœ¨OpenS2Sæ¨¡å‹ä¸­å¾—ä»¥åº”ç”¨ã€‚</li>
<li>OpenS2Sé¡¹ç›®åŒ…æ‹¬è‡ªåŠ¨æ•°æ®æ„å»ºç®¡é“ï¼Œå¯å®ç°ç«¯åˆ°ç«¯çš„è®­ç»ƒã€‚</li>
<li>OpenS2Sé€šè¿‡ä½¿ç”¨å¤§è§„æ¨¡è¯­è¨€æ¨¡å‹ç”Ÿæˆæœ‰åŒç†å¿ƒçš„å†…å®¹å¹¶å¼•å…¥å¯æ§çš„æ–‡æœ¬åˆ°è¯­éŸ³ç³»ç»Ÿæ¥æ„å»ºè®­ç»ƒè¯­æ–™åº“ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.05177">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-21948ff441f4448eb4f3b0b7cb089dd4.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-e7d40f4f974b108affe286909cb3d7e3.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-8488f236b5f9be3b34f1f4606e1c9fe7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2eafc4bdd5dab338e91b63b710eefdb4.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="LERa-Replanning-with-Visual-Feedback-in-Instruction-Following"><a href="#LERa-Replanning-with-Visual-Feedback-in-Instruction-Following" class="headerlink" title="LERa: Replanning with Visual Feedback in Instruction Following"></a>LERa: Replanning with Visual Feedback in Instruction Following</h2><p><strong>Authors:Svyatoslav Pchelintsev, Maxim Patratskiy, Anatoly Onishchenko, Alexandr Korchemnyi, Aleksandr Medvedev, Uliana Vinogradova, Ilya Galuzinsky, Aleksey Postnikov, Alexey K. Kovalev, Aleksandr I. Panov</strong></p>
<p>Large Language Models are increasingly used in robotics for task planning, but their reliance on textual inputs limits their adaptability to real-world changes and failures. To address these challenges, we propose LERa - Look, Explain, Replan - a Visual Language Model-based replanning approach that utilizes visual feedback. Unlike existing methods, LERa requires only a raw RGB image, a natural language instruction, an initial task plan, and failure detection - without additional information such as object detection or predefined conditions that may be unavailable in a given scenario. The replanning process consists of three steps: (i) Look, where LERa generates a scene description and identifies errors; (ii) Explain, where it provides corrective guidance; and (iii) Replan, where it modifies the plan accordingly. LERa is adaptable to various agent architectures and can handle errors from both dynamic scene changes and task execution failures. We evaluate LERa on the newly introduced ALFRED-ChaOS and VirtualHome-ChaOS datasets, achieving a 40% improvement over baselines in dynamic environments. In tabletop manipulation tasks with a predefined probability of task failure within the PyBullet simulator, LERa improves success rates by up to 67%. Further experiments, including real-world trials with a tabletop manipulator robot, confirm LERaâ€™s effectiveness in replanning. We demonstrate that LERa is a robust and adaptable solution for error-aware task execution in robotics. The code is available at <a target="_blank" rel="noopener" href="https://lera-robo.github.io/">https://lera-robo.github.io</a>. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹åœ¨æœºå™¨äººä»»åŠ¡è§„åˆ’ä¸­çš„åº”ç”¨è¶Šæ¥è¶Šå¹¿æ³›ï¼Œä½†å®ƒä»¬å¯¹æ–‡æœ¬è¾“å…¥çš„ä¾èµ–é™åˆ¶äº†å…¶åœ¨ç°å®ä¸–ç•Œå˜åŒ–å’Œæ•…éšœä¸­çš„é€‚åº”æ€§ã€‚ä¸ºäº†è§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†LERaâ€”â€”ä¸€ç§åŸºäºè§†è§‰è¯­è¨€æ¨¡å‹çš„é‡æ–°è§„åˆ’æ–¹æ³•ï¼Œå®ƒåˆ©ç”¨è§†è§‰åé¦ˆï¼ŒåŒ…æ‹¬â€œè§‚å¯Ÿâ€ã€â€œè§£é‡Šâ€ã€â€œé‡æ–°è§„åˆ’â€ä¸‰ä¸ªæ­¥éª¤ã€‚ä¸åŒäºç°æœ‰æ–¹æ³•ï¼ŒLERaä»…éœ€è¦åŸå§‹RGBå›¾åƒã€è‡ªç„¶è¯­è¨€æŒ‡ä»¤ã€åˆå§‹ä»»åŠ¡è®¡åˆ’å’Œæ•…éšœæ£€æµ‹ï¼Œè€Œæ— éœ€é¢å¤–çš„ä¿¡æ¯ï¼Œå¦‚å¯¹è±¡æ£€æµ‹æˆ–å¯èƒ½åœ¨ç‰¹å®šåœºæ™¯ä¸­ä¸å¯ç”¨çš„é¢„å®šä¹‰æ¡ä»¶ã€‚åœ¨LERaçš„é‡æ–°è§„åˆ’è¿‡ç¨‹ä¸­ï¼šï¼ˆiï¼‰â€œè§‚å¯Ÿâ€é˜¶æ®µï¼ŒLERaç”Ÿæˆåœºæ™¯æè¿°å¹¶è¯†åˆ«é”™è¯¯ï¼›ï¼ˆiiï¼‰â€œè§£é‡Šâ€é˜¶æ®µï¼Œå®ƒæä¾›çº æ­£æŒ‡å¯¼ï¼›ï¼ˆiiiï¼‰â€œé‡æ–°è§„åˆ’â€é˜¶æ®µï¼Œå®ƒç›¸åº”åœ°ä¿®æ”¹è®¡åˆ’ã€‚LERaå¯ä»¥é€‚åº”å„ç§ä»£ç†æ¶æ„ï¼Œå¹¶å¯ä»¥å¤„ç†æ¥è‡ªåŠ¨æ€åœºæ™¯å˜åŒ–å’Œä»»åŠ¡æ‰§è¡Œå¤±è´¥çš„é”™è¯¯ã€‚æˆ‘ä»¬åœ¨æ–°å¼•å…¥çš„ALFRED-ChaOSå’ŒVirtualHome-ChaOSæ•°æ®é›†ä¸Šè¯„ä¼°äº†LERaçš„æ€§èƒ½ï¼Œåœ¨åŠ¨æ€ç¯å¢ƒä¸­å®ç°äº†æ¯”åŸºçº¿æ–¹æ³•é«˜40%çš„æ”¹è¿›ã€‚åœ¨PyBulletæ¨¡æ‹Ÿå™¨ä¸­ï¼Œå¯¹äºå…·æœ‰é¢„å®šä»»åŠ¡å¤±è´¥æ¦‚ç‡çš„æ¡Œé¢æ“ä½œä»»åŠ¡ï¼ŒLERaå°†æˆåŠŸç‡æé«˜äº†é«˜è¾¾67%ã€‚è¿›ä¸€æ­¥çš„å®éªŒï¼ŒåŒ…æ‹¬ä¸æ¡Œé¢æ“ä½œæœºå™¨äººè¿›è¡Œçš„çœŸå®ä¸–ç•Œè¯•éªŒï¼Œè¯å®äº†LERaåœ¨é‡æ–°è§„åˆ’ä¸­çš„æœ‰æ•ˆæ€§ã€‚æˆ‘ä»¬è¯æ˜LERaæ˜¯æœºå™¨äººé”™è¯¯æ„ŸçŸ¥ä»»åŠ¡æ‰§è¡Œä¸­ç¨³å¥ä¸”é€‚åº”æ€§å¼ºçš„è§£å†³æ–¹æ¡ˆã€‚ç›¸å…³ä»£ç å¯è®¿é—®<a target="_blank" rel="noopener" href="https://lera-robo.github.io./">https://lera-robo.github.ioã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.05135v1">PDF</a> IROS 2025</p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹åœ¨æœºå™¨äººä»»åŠ¡è§„åˆ’ä¸­çš„åº”ç”¨æ—¥ç›Šå¹¿æ³›ï¼Œä½†å…¶å¯¹æ–‡æœ¬è¾“å…¥çš„ä¾èµ–é™åˆ¶äº†å…¶åœ¨ç°å®ä¸–ç•Œå˜åŒ–å’Œæ•…éšœä¸­çš„é€‚åº”æ€§ã€‚ä¸ºè§£å†³æ­¤é—®é¢˜ï¼Œæå‡ºä¸€ç§åŸºäºè§†è§‰è¯­è¨€æ¨¡å‹çš„å†è§„åˆ’æ–¹æ³•LERaï¼ˆLook, Explain, Replanï¼‰ï¼Œè¯¥æ–¹æ³•åˆ©ç”¨è§†è§‰åé¦ˆã€‚LERaä»…éœ€åŸå§‹RGBå›¾åƒã€è‡ªç„¶è¯­è¨€æŒ‡ä»¤ã€åˆå§‹ä»»åŠ¡è®¡åˆ’å’Œæ•…éšœæ£€æµ‹ï¼Œæ— éœ€åœ¨ç»™å®šåœºæ™¯ä¸­å¯èƒ½æ— æ³•è·å¾—çš„å¯¹è±¡æ£€æµ‹æˆ–é¢„è®¾æ¡ä»¶ã€‚å†è§„åˆ’è¿‡ç¨‹åŒ…æ‹¬ä¸‰ä¸ªæ­¥éª¤ï¼šè§‚å¯Ÿã€è§£é‡Šå’Œå†è§„åˆ’ã€‚LERaå¯é€‚åº”å„ç§ä»£ç†æ¶æ„ï¼Œå¯å¤„ç†åŠ¨æ€åœºæ™¯å˜åŒ–å’Œä»»åŠ¡æ‰§è¡Œæ•…éšœã€‚åœ¨å…¨æ–°å¼•å…¥çš„ALFRED-ChaOSå’ŒVirtualHome-ChaOSæ•°æ®é›†ä¸Šè¯„ä¼°LERaï¼Œå…¶åœ¨åŠ¨æ€ç¯å¢ƒä¸­æ¯”åŸºçº¿æé«˜äº†40%ã€‚åœ¨PyBulletæ¨¡æ‹Ÿå™¨ä¸­ï¼Œå¯¹äºå…·æœ‰é¢„è®¾ä»»åŠ¡å¤±è´¥æ¦‚ç‡çš„æ¡Œé¢æ“ä½œä»»åŠ¡ï¼ŒLERaå°†æˆåŠŸç‡æé«˜äº†67%ã€‚è¿›ä¸€æ­¥çš„å®éªŒï¼ŒåŒ…æ‹¬ä¸æ¡Œé¢æ“ä½œæœºå™¨äººè¿›è¡Œçš„çœŸå®ä¸–ç•Œè¯•éªŒï¼Œè¯å®äº†LERaåœ¨é‡æ–°è§„åˆ’ä¸­çš„æœ‰æ•ˆæ€§ã€‚è¡¨æ˜LERaæ˜¯æœºå™¨äººä»»åŠ¡æ‰§è¡Œä¸­é”™è¯¯æ„ŸçŸ¥çš„ç¨³å¥ä¸”å¯é€‚åº”çš„è§£å†³æ–¹æ¡ˆã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LERaæ˜¯ä¸€ç§åŸºäºè§†è§‰è¯­è¨€æ¨¡å‹çš„å†è§„åˆ’æ–¹æ³•ï¼Œç”¨äºæœºå™¨äººä»»åŠ¡è§„åˆ’ï¼Œè§£å†³äº†å¤§å‹è¯­è¨€æ¨¡å‹å¯¹æ–‡æœ¬è¾“å…¥çš„ä¾èµ–é—®é¢˜ã€‚</li>
<li>LERaé€šè¿‡åˆ©ç”¨è§†è§‰åé¦ˆï¼Œèƒ½å¤Ÿé€‚åº”ç°å®ä¸–ç•Œçš„å˜åŒ–å’Œæ•…éšœã€‚</li>
<li>LERaä»…éœ€è¦RGBå›¾åƒã€è‡ªç„¶è¯­è¨€æŒ‡ä»¤ã€åˆå§‹ä»»åŠ¡è®¡åˆ’å’Œæ•…éšœæ£€æµ‹ï¼Œæ— éœ€å…¶ä»–å¯èƒ½åœ¨ç‰¹å®šåœºæ™¯ä¸­æ— æ³•è·å¾—çš„ä¿¡æ¯ã€‚</li>
<li>LERaçš„å†è§„åˆ’è¿‡ç¨‹åŒ…æ‹¬è§‚å¯Ÿã€è§£é‡Šå’Œå†è§„åˆ’ä¸‰ä¸ªæ­¥éª¤ã€‚</li>
<li>LERaå¯é€‚åº”å„ç§ä»£ç†æ¶æ„ï¼Œå¹¶èƒ½å¤„ç†åŠ¨æ€åœºæ™¯å˜åŒ–å’Œä»»åŠ¡æ‰§è¡Œæ•…éšœã€‚</li>
<li>åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒLERaåœ¨åŠ¨æ€ç¯å¢ƒä¸­æ¯”åŸºçº¿æœ‰æ˜¾è‘—æ”¹å–„ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.05135">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-56ba977680cc6f4b4d3046be06bf3ccb.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b53a960b02d595adffd8f3bd8d793a9f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ae53b35bece955a875e2ad5d5b9130b3.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-192aba7e35bbaf3044c81e9fffb27a35.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="VerifyLLM-LLM-Based-Pre-Execution-Task-Plan-Verification-for-Robots"><a href="#VerifyLLM-LLM-Based-Pre-Execution-Task-Plan-Verification-for-Robots" class="headerlink" title="VerifyLLM: LLM-Based Pre-Execution Task Plan Verification for Robots"></a>VerifyLLM: LLM-Based Pre-Execution Task Plan Verification for Robots</h2><p><strong>Authors:Danil S. Grigorev, Alexey K. Kovalev, Aleksandr I. Panov</strong></p>
<p>In the field of robotics, researchers face a critical challenge in ensuring reliable and efficient task planning. Verifying high-level task plans before execution significantly reduces errors and enhance the overall performance of these systems. In this paper, we propose an architecture for automatically verifying high-level task plans before their execution in simulator or real-world environments. Leveraging Large Language Models (LLMs), our approach consists of two key steps: first, the conversion of natural language instructions into Linear Temporal Logic (LTL), followed by a comprehensive analysis of action sequences. The module uses the reasoning capabilities of the LLM to evaluate logical coherence and identify potential gaps in the plan. Rigorous testing on datasets of varying complexity demonstrates the broad applicability of the module to household tasks. We contribute to improving the reliability and efficiency of task planning and addresses the critical need for robust pre-execution verification in autonomous systems. The code is available at <a target="_blank" rel="noopener" href="https://verifyllm.github.io/">https://verifyllm.github.io</a>. </p>
<blockquote>
<p>åœ¨æœºå™¨äººé¢†åŸŸï¼Œç ”ç©¶è€…é¢ä¸´ä¸€ä¸ªç¡®ä¿å¯é é«˜æ•ˆçš„ä»»åŠ¡è§„åˆ’çš„é‡å¤§æŒ‘æˆ˜ã€‚åœ¨æ‰§è¡Œä»»åŠ¡ä¹‹å‰å¯¹é«˜çº§ä»»åŠ¡è®¡åˆ’è¿›è¡ŒéªŒè¯ï¼Œå¯ä»¥å¤§å¤§å‡å°‘é”™è¯¯å¹¶å¢å¼ºç³»ç»Ÿçš„æ•´ä½“æ€§èƒ½ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åœ¨æ¨¡æ‹Ÿå™¨æˆ–çœŸå®ç¯å¢ƒä¸­æ‰§è¡Œé«˜çº§ä»»åŠ¡è®¡åˆ’ä¹‹å‰è‡ªåŠ¨éªŒè¯è¿™äº›è®¡åˆ’çš„æ¶æ„ã€‚é€šè¿‡åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åˆ†ä¸ºä¸¤ä¸ªå…³é”®æ­¥éª¤ï¼šé¦–å…ˆï¼Œå°†è‡ªç„¶è¯­è¨€æŒ‡ä»¤è½¬æ¢ä¸ºçº¿æ€§æ—¶åºé€»è¾‘ï¼ˆLTLï¼‰ï¼Œç„¶åå¯¹åŠ¨ä½œåºåˆ—è¿›è¡Œå…¨é¢åˆ†æã€‚è¯¥æ¨¡å—ä½¿ç”¨LLMçš„æ¨ç†èƒ½åŠ›æ¥è¯„ä¼°é€»è¾‘è¿è´¯æ€§å¹¶è¯†åˆ«è®¡åˆ’ä¸­å¯èƒ½å­˜åœ¨çš„å·®è·ã€‚åœ¨å¤šç§å¤æ‚æ•°æ®é›†ä¸Šçš„ä¸¥æ ¼æµ‹è¯•è¯æ˜äº†è¯¥æ¨¡å—åœ¨å®¶åº­ä»»åŠ¡ä¸­çš„å¹¿æ³›åº”ç”¨æ€§ã€‚æˆ‘ä»¬ä¸ºæé«˜ä»»åŠ¡è§„åˆ’å’Œæ‰§è¡Œçš„å¯é æ€§å’Œæ•ˆç‡åšå‡ºäº†è´¡çŒ®ï¼Œå¹¶è§£å†³äº†è‡ªä¸»ç³»ç»Ÿä¸­å¯¹é²æ£’çš„é¢„æ‰§è¡ŒéªŒè¯çš„è¿«åˆ‡éœ€æ±‚ã€‚ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://verifyllm.github.ioè·å–./">https://verifyllm.github.ioè·å–ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.05118v1">PDF</a> IROS 2025</p>
<p><strong>Summary</strong></p>
<p>åœ¨æœºå™¨äººé¢†åŸŸï¼Œå¯¹ä»»åŠ¡è§„åˆ’è¿›è¡Œå¯é ä¸”é«˜æ•ˆçš„éªŒè¯æ˜¯ä¿è¯ç³»ç»Ÿæ€§èƒ½çš„å…³é”®ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§åœ¨æ¨¡æ‹Ÿå™¨æˆ–çœŸå®ç¯å¢ƒä¸­è‡ªåŠ¨éªŒè¯é«˜çº§ä»»åŠ¡è®¡åˆ’çš„æ¶æ„ã€‚è¯¥æ¶æ„åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è¿›è¡Œè‡ªç„¶è¯­è¨€æŒ‡ä»¤çš„è½¬æ¢ï¼Œå¹¶é€šè¿‡çº¿æ€§æ—¶åºé€»è¾‘ï¼ˆLTLï¼‰å¯¹åŠ¨ä½œåºåˆ—è¿›è¡Œå…¨é¢åˆ†æã€‚åˆ©ç”¨LLMçš„æ¨ç†èƒ½åŠ›è¯„ä¼°é€»è¾‘è¿è´¯æ€§å¹¶è¯†åˆ«è®¡åˆ’ä¸­çš„æ½œåœ¨æ¼æ´ã€‚æµ‹è¯•è¯æ˜è¯¥æ¨¡å—å…·æœ‰å¹¿æ³›çš„é€‚ç”¨æ€§ï¼Œä¸ºæé«˜ä»»åŠ¡è§„åˆ’çš„å¯é æ€§å’Œæ•ˆç‡åšå‡ºè´¡çŒ®ï¼Œæ»¡è¶³è‡ªä¸»ç³»ç»Ÿçš„å…³é”®é¢„æ‰§è¡ŒéªŒè¯éœ€æ±‚ã€‚ç›¸å…³ä»£ç å¯åœ¨â€œ<a target="_blank" rel="noopener" href="https://verifyllm.github.io/">é“¾æ¥</a>â€è·å–ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>ç ”ç©¶é¢†åŸŸèšç„¦äºæœºå™¨äººæŠ€æœ¯ä¸­çš„ä»»åŠ¡è§„åˆ’éªŒè¯ã€‚</li>
<li>æå‡ºä¸€ç§åŸºäºå¤§å‹è¯­è¨€æ¨¡å‹çš„è‡ªåŠ¨éªŒè¯æ¶æ„ã€‚</li>
<li>æ¶æ„åŒ…å«ä¸¤ä¸ªå…³é”®æ­¥éª¤ï¼šè‡ªç„¶è¯­è¨€æŒ‡ä»¤è½¬æ¢ä¸ºçº¿æ€§æ—¶åºé€»è¾‘å’ŒåŠ¨ä½œåºåˆ—çš„å…¨é¢åˆ†æã€‚</li>
<li>åˆ©ç”¨LLMçš„æ¨ç†èƒ½åŠ›è¯„ä¼°é€»è¾‘è¿è´¯æ€§ã€‚</li>
<li>æ¶æ„èƒ½å¤Ÿåœ¨æ¨¡æ‹Ÿå™¨æˆ–çœŸå®ç¯å¢ƒä¸­è¿›è¡Œé¢„æ‰§è¡ŒéªŒè¯ã€‚</li>
<li>æµ‹è¯•è¯æ˜äº†è¯¥æ¨¡å—åœ¨å¤šç§å¤æ‚ä»»åŠ¡ä¸­çš„é€‚ç”¨æ€§ã€‚</li>
<li>è¯¥ç ”ç©¶æœ‰åŠ©äºæé«˜ä»»åŠ¡è§„åˆ’çš„å¯é æ€§å’Œæ•ˆç‡ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.05118">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-deae4d7afe21a19531df05014c477d80.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b5cb1e7cc1885f6b2183b801f6c93746.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d8e2c649030f846685d0560682aa34f9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-82e76d843d48719c4af25d9628f3d36d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8fd392510c6d5254957bbc9257bbd63d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-305c0671e298a1ad42b5f252c2e81ceb.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-50f90dec641b65efea6019441bc217f8.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="What-Shapes-User-Trust-in-ChatGPT-A-Mixed-Methods-Study-of-User-Attributes-Trust-Dimensions-Task-Context-and-Societal-Perceptions-among-University-Students"><a href="#What-Shapes-User-Trust-in-ChatGPT-A-Mixed-Methods-Study-of-User-Attributes-Trust-Dimensions-Task-Context-and-Societal-Perceptions-among-University-Students" class="headerlink" title="What Shapes User Trust in ChatGPT? A Mixed-Methods Study of User   Attributes, Trust Dimensions, Task Context, and Societal Perceptions among   University Students"></a>What Shapes User Trust in ChatGPT? A Mixed-Methods Study of User   Attributes, Trust Dimensions, Task Context, and Societal Perceptions among   University Students</h2><p><strong>Authors:Kadija Bouyzourn, Alexandra Birch</strong></p>
<p>This mixed-methods inquiry examined four domains that shape university studentsâ€™ trust in ChatGPT: user attributes, seven delineated trust dimensions, task context, and perceived societal impact. Data were collected through a survey of 115 UK undergraduate and postgraduate students and four complementary semi-structured interviews. Behavioural engagement outweighed demographics: frequent use increased trust, whereas self-reported understanding of large-language-model mechanics reduced it. Among the dimensions, perceived expertise and ethical risk were the strongest predictors of overall trust; ease of use and transparency had secondary effects, while human-likeness and reputation were non-significant. Trust was highly task-contingent; highest for coding and summarising, lowest for entertainment and citation generation, yet confidence in ChatGPTâ€™s referencing ability, despite known inaccuracies, was the single strongest correlate of global trust, indicating automation bias. Computer-science students surpassed peers only in trusting the system for proofreading and writing, suggesting technical expertise refines rather than inflates reliance. Finally, students who viewed AIâ€™s societal impact positively reported the greatest trust, whereas mixed or negative outlooks dampened confidence. These findings show that trust in ChatGPT hinges on task verifiability, perceived competence, ethical alignment and direct experience, and they underscore the need for transparency, accuracy cues and user education when deploying LLMs in academic settings. </p>
<blockquote>
<p>è¿™é¡¹æ··åˆæ–¹æ³•ç ”ç©¶è°ƒæŸ¥äº†å½±å“å¤§å­¦ç”Ÿå¯¹ChatGPTä¿¡ä»»ç¨‹åº¦çš„å››ä¸ªé¢†åŸŸï¼šç”¨æˆ·å±æ€§ã€ä¸ƒä¸ªåˆ’åˆ†çš„ä¿¡ä»»ç»´åº¦ã€ä»»åŠ¡ä¸Šä¸‹æ–‡å’Œæ„ŸçŸ¥çš„ç¤¾ä¼šå½±å“ã€‚æ•°æ®æ˜¯é€šè¿‡ä¸€é¡¹é’ˆå¯¹è‹±å›½115åæœ¬ç§‘å’Œç ”ç©¶ç”Ÿå­¦ç”Ÿçš„è°ƒæŸ¥ä»¥åŠå››æ¬¡è¡¥å……çš„åŠç»“æ„åŒ–è®¿è°ˆæ”¶é›†çš„ã€‚è¡Œä¸ºå‚ä¸åº¦è¶…è¿‡äº†äººå£ç»Ÿè®¡å­¦ç‰¹å¾ï¼šé¢‘ç¹ä½¿ç”¨å¢åŠ äº†ä¿¡ä»»åº¦ï¼Œè€Œè‡ªæˆ‘æŠ¥å‘Šçš„å¯¹å¤§å‹è¯­è¨€æ¨¡å‹æœºåˆ¶çš„ç†è§£åˆ™é™ä½äº†ä¿¡ä»»åº¦ã€‚åœ¨ç»´åº¦ä¸­ï¼Œæ„ŸçŸ¥çš„ä¸“ä¸šçŸ¥è¯†å’Œé“å¾·é£é™©æ˜¯æ•´ä½“ä¿¡ä»»çš„æœ€å¼ºé¢„æµ‹å› ç´ ï¼›æ˜“ç”¨æ€§å’Œé€æ˜åº¦æœ‰æ¬¡è¦å½±å“ï¼Œè€Œäººç±»äº²å’ŒåŠ›å’Œå£°èª‰åˆ™ä¸å…·æ˜¾è‘—æ€§ã€‚ä¿¡ä»»é«˜åº¦ä¾èµ–äºä»»åŠ¡ï¼›å¯¹ç¼–ç å’Œæ€»ç»“çš„ä¿¡ä»»åº¦æœ€é«˜ï¼Œå¯¹å¨±ä¹å’Œå¼•ç”¨ç”Ÿæˆçš„ä¿¡ä»»åº¦æœ€ä½ã€‚å°½ç®¡å­˜åœ¨å·²çŸ¥çš„ä¸å‡†ç¡®ä¹‹å¤„ï¼Œä½†å¯¹ChatGPTå¼•ç”¨èƒ½åŠ›çš„ä¿¡ä»»ä»æ˜¯å…¨çƒä¿¡ä»»ä¸­æœ€å¼ºçƒˆçš„å•ä¸€ç›¸å…³å› ç´ ï¼Œè¿™æ˜¾ç¤ºäº†è‡ªåŠ¨åŒ–åè§ã€‚è®¡ç®—æœºç§‘å­¦ä¸“ä¸šçš„å­¦ç”Ÿä»…åœ¨æ£€æŸ¥å†™ä½œæ–¹é¢è¶…è¶ŠåŒé¾„äººï¼Œè¿™è¡¨æ˜æŠ€æœ¯ä¸“ä¸šçŸ¥è¯†ä¼šç»†åŒ–è€Œéå¤¸å¤§ä¾èµ–ã€‚æœ€åï¼Œé‚£äº›å¯¹äººå·¥æ™ºèƒ½çš„ç¤¾ä¼šå½±å“æŒç§¯æçœ‹æ³•çš„å­¦ç”ŸæŠ¥å‘Šçš„ä¿¡ä»»åº¦æœ€é«˜ï¼Œè€Œæ··åˆæˆ–æ¶ˆæå‰æ™¯åˆ™ä¼šå‰Šå¼±ä¿¡å¿ƒã€‚è¿™äº›å‘ç°è¡¨æ˜ï¼Œå¯¹ChatGPTçš„ä¿¡ä»»å–å†³äºä»»åŠ¡çš„å¯éªŒè¯æ€§ã€æ„ŸçŸ¥çš„èƒ½åŠ›ã€é“å¾·è§‚å¿µå’Œç›´æ¥ç»éªŒï¼Œå¹¶å¼ºè°ƒåœ¨å­¦æœ¯ç¯å¢ƒä¸­éƒ¨ç½²å¤§å‹è¯­è¨€æ¨¡å‹æ—¶ï¼Œéœ€è¦é€æ˜åº¦ã€å‡†ç¡®æ€§æç¤ºå’Œç”¨æˆ·æ•™è‚²ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.05046v1">PDF</a> 25 pages, 11 tables, 6 figures</p>
<p><strong>Summary</strong>ï¼šæœ¬ç ”ç©¶é€šè¿‡æ··åˆæ–¹æ³•ç ”ç©¶ï¼Œæ¢è®¨äº†å½±å“å¤§å­¦ç”Ÿå¯¹ChatGPTä¿¡ä»»åº¦çš„å››ä¸ªé¢†åŸŸï¼ŒåŒ…æ‹¬ç”¨æˆ·å±æ€§ã€ä¸ƒä¸ªåˆ’åˆ†çš„ä¿¡ä»»ç»´åº¦ã€ä»»åŠ¡ä¸Šä¸‹æ–‡å’Œæ„ŸçŸ¥çš„ç¤¾ä¼šå½±å“ã€‚é€šè¿‡è°ƒæŸ¥115åè‹±å›½æœ¬ç§‘ç”Ÿå’Œç ”ç©¶ç”Ÿä»¥åŠå››æ¬¡è¡¥å……çš„åŠç»“æ„åŒ–è®¿è°ˆæ”¶é›†æ•°æ®ã€‚è¡Œä¸ºå‚ä¸åº¦æ¯”äººå£ç»Ÿè®¡å­¦æ›´é‡è¦ï¼šé¢‘ç¹ä½¿ç”¨å¢åŠ äº†ä¿¡ä»»åº¦ï¼Œè€Œå¯¹è‡ªå·±å¯¹å¤§å‹è¯­è¨€æ¨¡å‹æœºåˆ¶çš„ç†è§£å‡å°‘äº†ä¿¡ä»»åº¦ã€‚åœ¨ç»´åº¦ä¸­ï¼Œæ„ŸçŸ¥çš„ä¸“ä¸šçŸ¥è¯†å’Œé“å¾·é£é™©æ˜¯æ•´ä½“ä¿¡ä»»çš„æœ€å¼ºé¢„æµ‹å› ç´ ï¼›æ˜“ç”¨æ€§å’Œé€æ˜åº¦æœ‰æ¬¡è¦å½±å“ï¼Œè€Œäººæ€§åŒ–å’Œå£°èª‰åˆ™ä¸å…·æ˜¾è‘—æ€§ã€‚ä¿¡ä»»ä¸ä»»åŠ¡å¯†åˆ‡ç›¸å…³ï¼›ç¼–ç å’Œæ€»ç»“æ–¹é¢æœ€é«˜ï¼Œå¨±ä¹å’Œå¼•ç”¨ç”Ÿæˆæ–¹é¢æœ€ä½ã€‚å°½ç®¡å­˜åœ¨å·²çŸ¥çš„ä¸å‡†ç¡®ä¹‹å¤„ï¼Œä½†å¯¹ChatGPTå¼•ç”¨èƒ½åŠ›çš„ä¿¡ä»»ä»æ˜¯å…¨çƒä¿¡ä»»çš„æœ€å¼ºçƒˆç›¸å…³å› ç´ ï¼Œè¿™è¡¨æ˜å­˜åœ¨è‡ªåŠ¨åŒ–åè§ã€‚è®¡ç®—æœºç§‘å­¦ä¸“ä¸šçš„å­¦ç”Ÿåªåœ¨æ ¡å¯¹å’Œå†™ä½œæ–¹é¢è¶…è¿‡åŒé¾„äººï¼Œè¿™è¡¨æ˜æŠ€æœ¯ä¸“ä¸šçŸ¥è¯†ä¼šåŠ å¼ºè€Œä¸æ˜¯å¤¸å¤§ä¾èµ–ã€‚æœ€åï¼Œè®¤ä¸ºäººå·¥æ™ºèƒ½ç¤¾ä¼šå½±å“ç§¯æçš„å­¦ç”ŸæŠ¥å‘Šäº†æœ€é«˜çš„ä¿¡ä»»åº¦ï¼Œè€Œæ··åˆæˆ–è´Ÿé¢çš„çœ‹æ³•å‰Šå¼±äº†ä¿¡å¿ƒã€‚è¿™äº›å‘ç°è¡¨æ˜ï¼Œå¯¹ChatGPTçš„ä¿¡ä»»å–å†³äºä»»åŠ¡çš„å¯éªŒè¯æ€§ã€æ„ŸçŸ¥çš„èƒ½åŠ›ã€é“å¾·å’Œç›´æ¥ç»éªŒã€‚å¼ºè°ƒäº†åœ¨ä½¿ç”¨è¯­è¨€æ¨¡å‹æ—¶éœ€è¦é€æ˜åº¦ã€å‡†ç¡®æ€§æç¤ºå’Œç”¨æˆ·æ•™è‚²çš„é‡è¦æ€§ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>å­¦ç”Ÿç”¨æˆ·å±æ€§å’Œè¡Œä¸ºå‚ä¸åº¦å¯¹ChatGPTçš„ä¿¡ä»»åº¦æœ‰é‡è¦å½±å“ã€‚é¢‘ç¹ä½¿ç”¨ä¼šå¢åŠ ä¿¡ä»»åº¦ï¼Œå¯¹å¤§å‹è¯­è¨€æ¨¡å‹æœºåˆ¶çš„ç†è§£åˆ™å‡å°‘ä¿¡ä»»åº¦ã€‚</li>
<li>åœ¨ä¸ƒä¸ªåˆ’åˆ†çš„ä¿¡ä»»ç»´åº¦ä¸­ï¼Œæ„ŸçŸ¥çš„ä¸“ä¸šçŸ¥è¯†å’Œé“å¾·é£é™©æ˜¯æ•´ä½“ä¿¡ä»»çš„æœ€å¼ºé¢„æµ‹å› ç´ ã€‚</li>
<li>ä»»åŠ¡ä¸Šä¸‹æ–‡å¯¹ChatGPTçš„ä¿¡ä»»åº¦æœ‰å¾ˆå¤§å½±å“ï¼Œç¼–ç å’Œæ€»ç»“ä»»åŠ¡çš„ä¿¡ä»»åº¦æœ€é«˜ï¼Œå¨±ä¹å’Œå¼•ç”¨ç”Ÿæˆä»»åŠ¡çš„ä¿¡ä»»åº¦æœ€ä½ã€‚</li>
<li>å°½ç®¡å­˜åœ¨å‡†ç¡®æ€§é—®é¢˜ï¼Œä½†å¯¹ChatGPTå¼•ç”¨èƒ½åŠ›çš„ä¿¡ä»»æ˜¯å…¨çƒä¿¡ä»»çš„æœ€å¼ºçƒˆç›¸å…³å› ç´ ï¼Œè¿™è¡¨æ˜å­˜åœ¨è‡ªåŠ¨åŒ–åè§ã€‚</li>
<li>è®¡ç®—æœºç§‘å­¦ä¸“ä¸šçš„å­¦ç”Ÿåœ¨è¯æ˜é˜…è¯»å’Œå†™ä½œæ–¹é¢å¯¹ChatGPTçš„ä¿¡ä»»åº¦è¾ƒé«˜ï¼Œè¡¨æ˜æŠ€æœ¯ä¸“ä¸šçŸ¥è¯†æœ‰åŠ©äºåŠ å¼ºä¾èµ–ã€‚</li>
<li>å­¦ç”Ÿå¯¹AIçš„ç¤¾ä¼šå½±å“çš„çœ‹æ³•å½±å“ä»–ä»¬å¯¹ChatGPTçš„ä¿¡ä»»åº¦ã€‚ç§¯æçš„ç¤¾ä¼šå½±å“è§‚ç‚¹ä¼šå¯¼è‡´æ›´é«˜çš„ä¿¡ä»»åº¦ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.05046">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-a7c5607c404d044d02d0a308e7369252.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-190356d8be62d21e2feefef94501d26e.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="MoLink-Distributed-and-Efficient-Serving-Framework-for-Large-Models"><a href="#MoLink-Distributed-and-Efficient-Serving-Framework-for-Large-Models" class="headerlink" title="MoLink: Distributed and Efficient Serving Framework for Large Models"></a>MoLink: Distributed and Efficient Serving Framework for Large Models</h2><p><strong>Authors:Lewei Jin, Yongqi Chen, Kui Zhang, Yifan Zhuo, Yi Gao, Bowei Yang, Zhengong Cai, Wei Dong</strong></p>
<p>Large language models represent a groundbreaking shift in generative AI. Yet, these advances come with a significant challenge: the high cost of model serving. To mitigate these costs, consumer-grade GPUs emerge as a more affordable alternative. This presents an opportunity for more cost-efficient LLM serving by leveraging these GPUs.   However, it is non-trivial to achieve high-efficiency LLM serving on consumer-grade GPUs, mainly due to two challenges: 1) these GPUs are often deployed in limited network conditions; 2) these GPUs often exhibit heterogeneity in host systems. To address these challenges, we present MoLink, a distributed LLM serving system for large models. It incorporates several key techniques, enabling efficient LLM serving on heterogeneous and weakly connected consumer-grade GPUs. Our experiments demonstrate that it achieves throughput improvements of up to 458% and cost-profit margin improvements of up to 151%, compared to state-of-the-art systems. MoLink allows users on Windows, Linux, and containerized VMs to seamlessly integrate GPUs with just a few lines of code over Ethernet or public networks. Currently, it supports 18 mainstream architectures of open-source large language models. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ä»£è¡¨äº†ç”Ÿæˆå¼äººå·¥æ™ºèƒ½çš„çªç ´æ€§è¿›å±•ã€‚ç„¶è€Œï¼Œè¿™äº›è¿›å±•ä¼´éšç€ä¸€ä¸ªå·¨å¤§çš„æŒ‘æˆ˜ï¼šæ¨¡å‹æœåŠ¡çš„æˆæœ¬é«˜æ˜‚ã€‚ä¸ºäº†ç¼“è§£è¿™äº›æˆæœ¬ï¼Œæ¶ˆè´¹çº§GPUä½œä¸ºä¸€ç§æ›´ç»æµçš„æ›¿ä»£æ–¹æ¡ˆè€Œå‡ºç°ã€‚è¿™ä¸ºåˆ©ç”¨è¿™äº›GPUå®ç°æ›´å…·æˆæœ¬æ•ˆç›Šçš„å¤§å‹è¯­è¨€æ¨¡å‹æœåŠ¡æä¾›äº†æœºä¼šã€‚ç„¶è€Œï¼Œåœ¨æ¶ˆè´¹çº§GPUä¸Šå®ç°é«˜æ•ˆçš„å¤§å‹è¯­è¨€æ¨¡å‹æœåŠ¡å¹¶éæ˜“äº‹ï¼Œä¸»è¦é¢ä¸´ä¸¤ä¸ªæŒ‘æˆ˜ï¼š1ï¼‰è¿™äº›GPUé€šå¸¸åœ¨ç½‘ç»œæ¡ä»¶æœ‰é™çš„æƒ…å†µä¸‹éƒ¨ç½²ï¼›2ï¼‰è¿™äº›GPUåœ¨ä¸»æœºç³»ç»Ÿä¸­ç»å¸¸è¡¨ç°å‡ºå¼‚è´¨æ€§ã€‚ä¸ºäº†è§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æ¨å‡ºäº†MoLinkï¼Œè¿™æ˜¯ä¸€ä¸ªç”¨äºå¤§å‹æ¨¡å‹çš„åˆ†å¸ƒå¼å¤§å‹è¯­è¨€æ¨¡å‹æœåŠ¡ç³»ç»Ÿã€‚å®ƒé‡‡ç”¨äº†å‡ ç§å…³é”®æŠ€æœ¯ï¼Œèƒ½å¤Ÿåœ¨å¼‚æ„å’Œè¿æ¥å¼±çš„æ¶ˆè´¹çº§GPUä¸Šå®ç°é«˜æ•ˆçš„å¤§å‹è¯­è¨€æ¨¡å‹æœåŠ¡ã€‚æˆ‘ä»¬çš„å®éªŒè¡¨æ˜ï¼Œä¸æœ€å…ˆè¿›çš„ç³»ç»Ÿç›¸æ¯”ï¼Œå®ƒå®ç°äº†é«˜è¾¾458%çš„ååé‡æ”¹è¿›å’Œé«˜è¾¾151%çš„æˆæœ¬åˆ©æ¶¦ç‡æ”¹è¿›ã€‚MoLinkå…è®¸Windowsã€Linuxå’Œå®¹å™¨åŒ–VMçš„ç”¨æˆ·é€šè¿‡ä»¥å¤ªç½‘æˆ–å…¬å…±ç½‘ç»œåªéœ€å‡ è¡Œä»£ç å³å¯æ— ç¼é›†æˆGPUã€‚ç›®å‰ï¼Œå®ƒæ”¯æŒ18ç§ä¸»æµå¼€æºå¤§å‹è¯­è¨€æ¨¡å‹æ¶æ„ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.05043v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„çªç ´å¸¦æ¥äº†ç”Ÿæˆå¼AIçš„é‡å¤§å˜é©ï¼Œä½†åŒæ—¶ä¹Ÿé¢ä¸´ç€æ¨¡å‹æœåŠ¡çš„é«˜æˆæœ¬æŒ‘æˆ˜ã€‚æ¶ˆè´¹è€…çº§GPUä½œä¸ºæ›´ç»æµçš„é€‰æ‹©ï¼Œä¸ºé™ä½LLMæœåŠ¡æˆæœ¬æä¾›äº†æœºä¼šã€‚ç„¶è€Œï¼Œåœ¨æ¶ˆè´¹è€…çº§GPUä¸Šå®ç°é«˜æ•ˆçš„LLMæœåŠ¡å¹¶éæ˜“äº‹ï¼Œä¸»è¦é¢ä¸´ç½‘ç»œæ¡ä»¶æœ‰é™å’Œè®¾å¤‡ç³»ç»Ÿå¼‚è´¨æ€§çš„ä¸¤å¤§æŒ‘æˆ˜ã€‚ä¸ºè§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†MoLinkåˆ†å¸ƒå¼LLMæœåŠ¡ç³»ç»Ÿï¼Œé€šè¿‡å‡ é¡¹å…³é”®æŠ€æœ¯å®ç°äº†åœ¨å¼‚æ„å’Œå¼±è¿æ¥çš„æ¶ˆè´¹è€…çº§GPUä¸Šçš„é«˜æ•ˆLLMæœåŠ¡ã€‚å®éªŒè¡¨æ˜ï¼Œä¸ç°æœ‰ç³»ç»Ÿç›¸æ¯”ï¼ŒMoLinkçš„ååé‡æå‡å¯è¾¾458%ï¼Œæˆæœ¬åˆ©æ¶¦ç‡æå‡å¯è¾¾151%ã€‚å®ƒæ”¯æŒWindowsã€Linuxå’Œå®¹å™¨åŒ–VMï¼Œåªéœ€å‡ è¡Œä»£ç å³å¯é€šè¿‡ä»¥å¤ªç½‘æˆ–å…¬å…±ç½‘ç»œæ— ç¼é›†æˆGPUï¼Œå¹¶ä¸”å…¼å®¹18ç§ä¸»æµçš„å¼€æºå¤§å‹è¯­è¨€æ¨¡å‹æ¶æ„ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ¨åŠ¨äº†ç”Ÿæˆå¼AIçš„è¿›æ­¥ï¼Œä½†æ¨¡å‹æœåŠ¡æˆæœ¬é«˜ã€‚</li>
<li>æ¶ˆè´¹è€…çº§GPUä¸ºé™ä½LLMæœåŠ¡æˆæœ¬æä¾›äº†æœºä¼šã€‚</li>
<li>åœ¨æ¶ˆè´¹è€…çº§GPUä¸Šå®ç°é«˜æ•ˆLLMæœåŠ¡é¢ä¸´ç½‘ç»œæ¡ä»¶æœ‰é™å’Œè®¾å¤‡ç³»ç»Ÿå¼‚è´¨æ€§ä¸¤å¤§æŒ‘æˆ˜ã€‚</li>
<li>MoLinkç³»ç»Ÿé€šè¿‡å‡ é¡¹å…³é”®æŠ€æœ¯è§£å†³äº†è¿™äº›æŒ‘æˆ˜ï¼Œå®ç°äº†é«˜æ•ˆLLMæœåŠ¡ã€‚</li>
<li>MoLinkç³»ç»Ÿååé‡æå‡æ˜¾è‘—ï¼Œæˆæœ¬åˆ©æ¶¦ç‡ä¹Ÿæœ‰æ˜æ˜¾æå‡ã€‚</li>
<li>MoLinkæ”¯æŒWindowsã€Linuxå’Œå®¹å™¨åŒ–VMï¼Œæ˜“äºé›†æˆGPUã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.05043">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-904557b65716e5bbd365259121298230.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1eb9fe149152d7718a5f0e49244aa11b.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-4b9983fa558fa2c1069cd4cb3ff88e80.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-73f19c5d02fc64be0217fa7a98c0cf67.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-cf04698e82feb52a73825eea9cce550b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b546c08e8335f7a1fdf8e4dba0d600c0.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-912f6248ea84e67dfc2b685fc5912f59.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="A-Comparative-Study-of-Specialized-LLMs-as-Dense-Retrievers"><a href="#A-Comparative-Study-of-Specialized-LLMs-as-Dense-Retrievers" class="headerlink" title="A Comparative Study of Specialized LLMs as Dense Retrievers"></a>A Comparative Study of Specialized LLMs as Dense Retrievers</h2><p><strong>Authors:Hengran Zhang, Keping Bi, Jiafeng Guo</strong></p>
<p>While large language models (LLMs) are increasingly deployed as dense retrievers, the impact of their domain-specific specialization on retrieval effectiveness remains underexplored. This investigation systematically examines how task-specific adaptations in LLMs influence their retrieval capabilities, an essential step toward developing unified retrievers capable of handling text, code, images, and multimodal content. We conduct extensive experiments with eight Qwen2.5 7B LLMs, including base, instruction-tuned, code&#x2F;math-specialized, long reasoning, and vision-language models across zero-shot retrieval settings and the supervised setting. For the zero-shot retrieval settings, we consider text retrieval from the BEIR benchmark and code retrieval from the CoIR benchmark. Further, to evaluate supervised performance, all LLMs are fine-tuned on the MS MARCO dataset. We find that mathematical specialization and the long reasoning capability cause consistent degradation in three settings, indicating conflicts between mathematical reasoning and semantic matching. The vision-language model and code-specialized LLMs demonstrate superior zero-shot performance compared to other LLMs, even surpassing BM25 on the code retrieval task, and maintain comparable performance to base LLMs in supervised settings. These findings suggest promising directions for the unified retrieval task leveraging cross-domain and cross-modal fusion. </p>
<blockquote>
<p>éšç€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è¶Šæ¥è¶Šå¤šåœ°è¢«éƒ¨ç½²ä¸ºå¯†é›†æ£€ç´¢å™¨ï¼Œå…¶é’ˆå¯¹ç‰¹å®šé¢†åŸŸçš„ä¸“ä¸šåŒ–å¯¹æ£€ç´¢æ•ˆæœçš„å½±å“å°šæœªå¾—åˆ°å……åˆ†æ¢ç´¢ã€‚è¿™é¡¹ç ”ç©¶ç³»ç»Ÿåœ°æ¢è®¨äº†LLMä¸­çš„ä»»åŠ¡ç‰¹å®šé€‚åº”å¦‚ä½•å½±å“å…¶æ£€ç´¢èƒ½åŠ›ï¼Œè¿™æ˜¯æœç€å¼€å‘èƒ½å¤Ÿå¤„ç†æ–‡æœ¬ã€ä»£ç ã€å›¾åƒå’Œå¤šæ¨¡æ€å†…å®¹çš„ç»Ÿä¸€æ£€ç´¢å™¨è¿ˆå‡ºçš„é‡è¦ä¸€æ­¥ã€‚æˆ‘ä»¬åœ¨å…«ä¸ªQwen2.5 7B LLMä¸Šè¿›è¡Œäº†å¹¿æ³›å®éªŒï¼ŒåŒ…æ‹¬åŸºç¡€æ¨¡å‹ã€æŒ‡ä»¤è°ƒä¼˜æ¨¡å‹ã€é’ˆå¯¹ä»£ç &#x2F;æ•°å­¦çš„ä¸“ç”¨æ¨¡å‹ã€é•¿æœŸæ¨ç†æ¨¡å‹å’Œè§†è§‰è¯­è¨€æ¨¡å‹ï¼Œæ¶µç›–äº†é›¶å°„å‡»æ£€ç´¢è®¾ç½®å’Œæœ‰ç›‘ç£è®¾ç½®ã€‚å¯¹äºé›¶å°„å‡»æ£€ç´¢è®¾ç½®ï¼Œæˆ‘ä»¬è€ƒè™‘ä»BEIRåŸºå‡†æµ‹è¯•ä¸­è¿›è¡Œæ–‡æœ¬æ£€ç´¢ï¼Œä»CoIRåŸºå‡†æµ‹è¯•ä¸­è¿›è¡Œä»£ç æ£€ç´¢ã€‚æ­¤å¤–ï¼Œä¸ºäº†è¯„ä¼°ç›‘ç£æ€§èƒ½ï¼Œæ‰€æœ‰LLMéƒ½åœ¨MS MARCOæ•°æ®é›†ä¸Šè¿›è¡Œå¾®è°ƒã€‚æˆ‘ä»¬å‘ç°æ•°å­¦ä¸“ä¸šåŒ–å’Œé•¿æœŸæ¨ç†èƒ½åŠ›ä¼šåœ¨ä¸‰ç§è®¾ç½®ä¸‹å¯¼è‡´æŒç»­çš„é€€åŒ–ï¼Œè¡¨æ˜æ•°å­¦æ¨ç†å’Œè¯­ä¹‰åŒ¹é…ä¹‹é—´å­˜åœ¨å†²çªã€‚è§†è§‰è¯­è¨€æ¨¡å‹å’Œé’ˆå¯¹ä»£ç ä¸“ç”¨çš„LLMåœ¨é›¶å°„å‡»åœºæ™¯ä¸‹çš„æ€§èƒ½è¡¨ç°ä¼˜äºå…¶ä»–LLMï¼Œç”šè‡³åœ¨ä»£ç æ£€ç´¢ä»»åŠ¡ä¸Šè¶…è¶Šäº†BM25ï¼Œå¹¶ä¸”åœ¨æœ‰ç›‘ç£è®¾ç½®ä¸‹ä¿æŒä¸åŸºç¡€LLMç›¸å½“çš„æ€§èƒ½ã€‚è¿™äº›å‘ç°è¡¨æ˜ï¼Œåˆ©ç”¨è·¨åŸŸå’Œè·¨æ¨¡æ€èåˆçš„ç»Ÿä¸€æ£€ç´¢ä»»åŠ¡å…·æœ‰å¹¿é˜”çš„å‘å±•å‰æ™¯ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.03958v1">PDF</a> Accepted by CCIR25 and published by Springer LNCS or LNAI</p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä½œä¸ºå¯†é›†æ£€ç´¢å™¨çš„åº”ç”¨è¶Šæ¥è¶Šå¹¿æ³›ï¼Œä½†å…¶é¢†åŸŸç‰¹å®šä¸“ä¸šåŒ–å¯¹æ£€ç´¢æ•ˆæœçš„å½±å“å°šæœªå¾—åˆ°å……åˆ†ç ”ç©¶ã€‚æœ¬ç ”ç©¶ç³»ç»Ÿåœ°æ¢è®¨äº†LLMçš„ä»»åŠ¡ç‰¹å®šé€‚åº”æ€§å¯¹å…¶æ£€ç´¢èƒ½åŠ›çš„å½±å“ï¼Œè¿™æ˜¯å¼€å‘èƒ½å¤Ÿå¤„ç†æ–‡æœ¬ã€ä»£ç ã€å›¾åƒå’Œå¤šæ¨¡æ€å†…å®¹çš„ç»Ÿä¸€æ£€ç´¢å™¨çš„å…³é”®æ­¥éª¤ã€‚é€šè¿‡å¯¹å…«ç§ä¸åŒä¸“ä¸šé¢†åŸŸçš„LLMè¿›è¡Œå¹¿æ³›å®éªŒï¼ŒåŒ…æ‹¬åŸºç¡€æ¨¡å‹ã€æŒ‡ä»¤è°ƒä¼˜æ¨¡å‹ã€ä»£ç &#x2F;æ•°å­¦ä¸“ä¸šæ¨¡å‹ã€é€»è¾‘æ¨ç†æ¨¡å‹å’Œè§†è§‰è¯­è¨€æ¨¡å‹ï¼Œåœ¨é›¶æ ·æœ¬æ£€ç´¢è®¾ç½®å’Œç›‘ç®¡è®¾ç½®ä¸‹ï¼Œå‘ç°æ•°å­¦ä¸“ä¸šåŒ–å’Œé€»è¾‘æ¨ç†èƒ½åŠ›åœ¨ä¸‰ä¸­è®¾ç½®ä¸‹è¡¨ç°å‡ºæŒç»­é€€æ­¥ã€‚ç›¸åï¼Œè§†è§‰è¯­è¨€æ¨¡å‹å’Œä»£ç ä¸“ä¸šLLMåœ¨é›¶æ ·æœ¬è¡¨ç°ä¼˜äºå…¶ä»–LLMï¼Œåœ¨ä»£ç æ£€ç´¢ä»»åŠ¡ä¸Šç”šè‡³è¶…è¿‡äº†BM25ï¼Œå¹¶åœ¨ç›‘ç£è®¾ç½®ä¸‹ä¿æŒäº†ä¸åŸºç¡€æ¨¡å‹ç›¸å½“çš„æ€§èƒ½ã€‚è¿™äº›å‘ç°è¡¨æ˜ï¼Œåˆ©ç”¨è·¨åŸŸå’Œè·¨æ¨¡æ€èåˆçš„ç»Ÿä¸€æ£€ç´¢ä»»åŠ¡å…·æœ‰å¹¿é˜”çš„å‘å±•å‰æ™¯ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä½œä¸ºå¯†é›†æ£€ç´¢å™¨çš„åº”ç”¨è¶Šæ¥è¶Šå¹¿æ³›ï¼Œä½†å…¶é¢†åŸŸç‰¹å®šä¸“ä¸šåŒ–å¯¹æ£€ç´¢æ•ˆæœçš„å½±å“å°šæœªå……åˆ†ç ”ç©¶ã€‚</li>
<li>LLMçš„ä»»åŠ¡ç‰¹å®šé€‚åº”æ€§å¯¹å…¶æ£€ç´¢èƒ½åŠ›æœ‰é‡è¦å½±å“ã€‚</li>
<li>æ•°å­¦ä¸“ä¸šåŒ–å’Œé€»è¾‘æ¨ç†èƒ½åŠ›åœ¨æŸäº›è®¾ç½®ä¸‹å¯èƒ½å¯¼è‡´LLMçš„æ£€ç´¢æ€§èƒ½ä¸‹é™ã€‚</li>
<li>è§†è§‰è¯­è¨€æ¨¡å‹å’Œä»£ç ä¸“ä¸šLLMåœ¨é›¶æ ·æœ¬æ£€ç´¢ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œç‰¹åˆ«æ˜¯åœ¨ä»£ç æ£€ç´¢ä»»åŠ¡ä¸Šã€‚</li>
<li>è§†è§‰è¯­è¨€æ¨¡å‹å’Œä»£ç ä¸“ä¸šLLMåœ¨ç›‘ç£è®¾ç½®ä¸‹çš„æ€§èƒ½ä¸åŸºç¡€æ¨¡å‹ç›¸å½“ã€‚</li>
<li>ç»Ÿä¸€æ£€ç´¢ä»»åŠ¡éœ€è¦è€ƒè™‘åˆ°è·¨åŸŸå’Œè·¨æ¨¡æ€èåˆçš„é‡è¦æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.03958">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-ffb853bcbb83504fc0261e28b3c54ebd.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="Demystifying-ChatGPT-How-It-Masters-Genre-Recognition"><a href="#Demystifying-ChatGPT-How-It-Masters-Genre-Recognition" class="headerlink" title="Demystifying ChatGPT: How It Masters Genre Recognition"></a>Demystifying ChatGPT: How It Masters Genre Recognition</h2><p><strong>Authors:Subham Raj, Sriparna Saha, Brijraj Singh, Niranjan Pedanekar</strong></p>
<p>The introduction of ChatGPT has garnered significant attention within the NLP community and beyond. Previous studies have demonstrated ChatGPTâ€™s substantial advancements across various downstream NLP tasks, highlighting its adaptability and potential to revolutionize language-related applications. However, its capabilities and limitations in genre prediction remain unclear. This work analyzes three Large Language Models (LLMs) using the MovieLens-100K dataset to assess their genre prediction capabilities. Our findings show that ChatGPT, without fine-tuning, outperformed other LLMs, and fine-tuned ChatGPT performed best overall. We set up zero-shot and few-shot prompts using audio transcripts&#x2F;subtitles from movie trailers in the MovieLens-100K dataset, covering 1682 movies of 18 genres, where each movie can have multiple genres. Additionally, we extended our study by extracting IMDb movie posters to utilize a Vision Language Model (VLM) with prompts for poster information. This fine-grained information was used to enhance existing LLM prompts. In conclusion, our study reveals ChatGPTâ€™s remarkable genre prediction capabilities, surpassing other language models. The integration of VLM further enhances our findings, showcasing ChatGPTâ€™s potential for content-related applications by incorporating visual information from movie posters. </p>
<blockquote>
<p>ChatGPTçš„å¼•å…¥åœ¨NLPé¢†åŸŸå†…å¤–éƒ½å¼•èµ·äº†æå¤§çš„å…³æ³¨ã€‚å…ˆå‰çš„ç ”ç©¶å·²ç»è¯æ˜äº†ChatGPTåœ¨å„ç§ä¸‹æ¸¸NLPä»»åŠ¡ä¸Šçš„å·¨å¤§è¿›æ­¥ï¼Œçªå‡ºäº†å…¶é€‚åº”æ€§å’Œæ”¹å˜è¯­è¨€ç›¸å…³åº”ç”¨çš„å¯èƒ½æ€§ã€‚ç„¶è€Œï¼Œå…¶åœ¨ä½“è£é¢„æµ‹æ–¹é¢çš„èƒ½åŠ›å’Œå±€é™æ€§å°šä¸æ¸…æ¥šã€‚æœ¬ç ”ç©¶ä½¿ç”¨MovieLens-100Kæ•°æ®é›†åˆ†æäº†ä¸‰æ¬¾å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„ä½“è£é¢„æµ‹èƒ½åŠ›ã€‚æˆ‘ä»¬çš„ç ”ç©¶å‘ç°ï¼Œæœªç»å¾®è°ƒçš„ChatGPTåœ¨å…¶ä»–LLMä¸­è¡¨ç°æœ€ä½³ï¼Œè€Œç»è¿‡å¾®è°ƒçš„ChatGPTæ€»ä½“è¡¨ç°æœ€ä½³ã€‚æˆ‘ä»¬ä½¿ç”¨MovieLens-100Kæ•°æ®é›†ä¸­çš„ç”µå½±é¢„å‘Šç‰‡éŸ³é¢‘è½¬å½•&#x2F;å­—å¹•æ¥è®¾ç½®é›¶æ ·æœ¬å’Œå°‘æ ·æœ¬æç¤ºï¼Œæ¶µç›–äº†18ä¸ªä½“è£çš„1682éƒ¨ç”µå½±ï¼Œæ¯éƒ¨ç”µå½±å¯èƒ½æœ‰å¤šä¸ªä½“è£ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜é€šè¿‡æå–IMDbç”µå½±æµ·æŠ¥æ¥æ‰©å±•æˆ‘ä»¬çš„ç ”ç©¶ï¼Œä½¿ç”¨è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰çš„æç¤ºè¿›è¡Œæµ·æŠ¥ä¿¡æ¯åˆ†æã€‚è¿™ç§ç²¾ç»†çš„ä¿¡æ¯è¢«ç”¨æ¥å¢å¼ºç°æœ‰çš„LLMæç¤ºã€‚æ€»ä¹‹ï¼Œæˆ‘ä»¬çš„ç ”ç©¶è¡¨æ˜ï¼ŒChatGPTåœ¨ä½“è£é¢„æµ‹æ–¹é¢è¡¨ç°å‡ºå“è¶Šçš„èƒ½åŠ›ï¼Œè¶…è¶Šäº†å…¶ä»–è¯­è¨€æ¨¡å‹ã€‚VLMçš„é›†æˆè¿›ä¸€æ­¥å¢å¼ºäº†æˆ‘ä»¬çš„å‘ç°ï¼Œå±•ç¤ºäº†ChatGPTé€šè¿‡ç»“åˆç”µå½±æµ·æŠ¥çš„è§†è§‰ä¿¡æ¯åœ¨å†…å®¹ç›¸å…³åº”ç”¨ä¸­çš„æ½œåŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.03875v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>ChatGPTåœ¨NLPé¢†åŸŸå¼•èµ·äº†å¹¿æ³›å…³æ³¨ï¼Œå¹¶å±•ç°å‡ºå¼ºå¤§çš„æ€§èƒ½ã€‚æœ¬ç ”ç©¶ä½¿ç”¨MovieLens-100Kæ•°æ®é›†åˆ†æäº†ä¸‰ç§å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„ä½“è£é¢„æµ‹èƒ½åŠ›ã€‚ç»“æœæ˜¾ç¤ºï¼Œæœªç»å¾®è°ƒChatGPTè¡¨ç°ä¼˜äºå…¶ä»–LLMï¼Œè€Œç»è¿‡å¾®è°ƒçš„ChatGPTè¡¨ç°æœ€ä½³ã€‚æ­¤å¤–ï¼Œæœ¬ç ”ç©¶è¿˜ç»“åˆäº†è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰ï¼Œé€šè¿‡ç”µå½±æµ·æŠ¥çš„æç¤ºä¿¡æ¯å¢å¼ºäº†LLMçš„æç¤ºï¼Œè¿›ä¸€æ­¥çªå‡ºäº†ChatGPTåœ¨å†…å®¹ç›¸å…³åº”ç”¨ä¸­çš„æ½œåŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ChatGPTåœ¨NLPé¢†åŸŸå—åˆ°å¹¿æ³›å…³æ³¨ï¼Œå…¶æ€§èƒ½æ˜¾è‘—ã€‚</li>
<li>ç ”ç©¶é‡‡ç”¨MovieLens-100Kæ•°æ®é›†åˆ†æäº†ä¸‰ç§LLMçš„ä½“è£é¢„æµ‹èƒ½åŠ›ã€‚</li>
<li>æœªç»è°ƒæ•™çš„ChatGPTåœ¨ä½“è£é¢„æµ‹æ–¹é¢è¡¨ç°ä¼˜äºå…¶ä»–LLMã€‚</li>
<li>ç»è¿‡è°ƒæ•™çš„ChatGPTè¡¨ç°æœ€ä½³ï¼Œä½“ç°äº†å…¶å¼ºå¤§çš„æ½œåŠ›ã€‚</li>
<li>ç ”ç©¶ç»“åˆäº†VLMï¼Œé€šè¿‡ç”µå½±æµ·æŠ¥çš„æç¤ºä¿¡æ¯å¢å¼ºäº†LLMçš„æç¤ºã€‚</li>
<li>ChatGPTåœ¨å†…å®¹ç›¸å…³åº”ç”¨ä¸­å…·æœ‰å·¨å¤§æ½œåŠ›ï¼Œèƒ½å¤Ÿç»“åˆè§†è§‰ä¿¡æ¯è¿›è¡Œä½“è£é¢„æµ‹ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.03875">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-27efcc82d2cee0f9cc535ba3b62d00be.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-74692b12176bdd70d576cb3c11d87547.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7b659fad49ae07a8d8a35353690972e3.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-22b7bf1cc259b6f56f4df83301e12a2c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c6ebb50762313d6b987770b9a758ca70.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f31ac2993af3defcc99b03fdffa3e560.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="From-Query-to-Explanation-Uni-RAG-for-Multi-Modal-Retrieval-Augmented-Learning-in-STEM"><a href="#From-Query-to-Explanation-Uni-RAG-for-Multi-Modal-Retrieval-Augmented-Learning-in-STEM" class="headerlink" title="From Query to Explanation: Uni-RAG for Multi-Modal Retrieval-Augmented   Learning in STEM"></a>From Query to Explanation: Uni-RAG for Multi-Modal Retrieval-Augmented   Learning in STEM</h2><p><strong>Authors:Xinyi Wu, Yanhao Jia, Luwei Xiao, Shuai Zhao, Fengkuang Chiang, Erik Cambria</strong></p>
<p>In AI-facilitated teaching, leveraging various query styles to interpret abstract educational content is crucial for delivering effective and accessible learning experiences. However, existing retrieval systems predominantly focus on natural text-image matching and lack the capacity to address the diversity and ambiguity inherent in real-world educational scenarios. To address this limitation, we develop a lightweight and efficient multi-modal retrieval module, named Uni-Retrieval, which extracts query-style prototypes and dynamically matches them with tokens from a continually updated Prompt Bank. This Prompt Bank encodes and stores domain-specific knowledge by leveraging a Mixture-of-Expert Low-Rank Adaptation (MoE-LoRA) module and can be adapted to enhance Uni-Retrievalâ€™s capability to accommodate unseen query types at test time. To enable natural language educational content generation, we integrate the original Uni-Retrieval with a compact instruction-tuned language model, forming a complete retrieval-augmented generation pipeline named Uni-RAG. Given a style-conditioned query, Uni-RAG first retrieves relevant educational materials and then generates human-readable explanations, feedback, or instructional content aligned with the learning objective. Experimental results on SER and other multi-modal benchmarks show that Uni-RAG outperforms baseline retrieval and RAG systems in both retrieval accuracy and generation quality, while maintaining low computational cost. Our framework provides a scalable, pedagogically grounded solution for intelligent educational systems, bridging retrieval and generation to support personalized, explainable, and efficient learning assistance across diverse STEM scenarios. </p>
<blockquote>
<p>åœ¨äººå·¥æ™ºèƒ½è¾…åŠ©æ•™å­¦ä¸­ï¼Œåˆ©ç”¨ä¸åŒçš„æŸ¥è¯¢é£æ ¼æ¥è§£é‡ŠæŠ½è±¡çš„æ•™è‚²å†…å®¹å¯¹äºæä¾›æœ‰æ•ˆä¸”å¯è®¿é—®çš„å­¦ä¹ ä½“éªŒè‡³å…³é‡è¦ã€‚ç„¶è€Œï¼Œç°æœ‰çš„æ£€ç´¢ç³»ç»Ÿä¸»è¦å…³æ³¨è‡ªç„¶è¯­è¨€æ–‡æœ¬ä¸å›¾åƒçš„åŒ¹é…ï¼Œç¼ºä¹åº”å¯¹ç°å®ä¸–ç•Œæ•™è‚²åœºæ™¯ä¸­å›ºæœ‰çš„å¤šæ ·æ€§å’Œæ¨¡ç³Šæ€§çš„èƒ½åŠ›ã€‚ä¸ºäº†è§£å†³è¿™ä¸€å±€é™æ€§ï¼Œæˆ‘ä»¬å¼€å‘äº†ä¸€ä¸ªè½»ä¾¿é«˜æ•ˆçš„å¤šæ¨¡æ€æ£€ç´¢æ¨¡å—ï¼Œåä¸ºUni-Retrievalã€‚è¯¥æ¨¡å—èƒ½å¤Ÿæå–æŸ¥è¯¢æ ·å¼åŸå‹ï¼Œå¹¶ä¸æ¥è‡ªæŒç»­æ›´æ–°çš„Prompt Bankä¸­çš„æ ‡è®°åŠ¨æ€åŒ¹é…ã€‚Prompt Banké€šè¿‡åˆ©ç”¨æ··åˆä¸“å®¶ä½ç§©é€‚åº”ï¼ˆMoE-LoRAï¼‰æ¨¡å—æ¥ç¼–ç å’Œå­˜å‚¨é¢†åŸŸç‰¹å®šçŸ¥è¯†ï¼Œå¹¶ä¸”èƒ½å¤Ÿé€‚åº”ä»¥å¢å¼ºUni-Retrievalåœ¨æµ‹è¯•æ—¶é€‚åº”æœªè§è¿‡çš„æŸ¥è¯¢ç±»å‹çš„èƒ½åŠ›ã€‚ä¸ºäº†å®ç°è‡ªç„¶è¯­è¨€æ•™è‚²å†…å®¹çš„ç”Ÿæˆï¼Œæˆ‘ä»¬å°†åŸå§‹çš„Uni-Retrievalä¸ç»è¿‡æŒ‡ä»¤è°ƒæ•´çš„å°å‹è¯­è¨€æ¨¡å‹é›†æˆï¼Œå½¢æˆäº†ä¸€ä¸ªå®Œæ•´çš„æ£€ç´¢å¢å¼ºç”Ÿæˆç®¡é“ï¼Œç§°ä¸ºUni-RAGã€‚ç»™å®šé£æ ¼æ§åˆ¶çš„æŸ¥è¯¢ï¼ŒUni-RAGé¦–å…ˆæ£€ç´¢ç›¸å…³çš„æ•™è‚²ææ–™ï¼Œç„¶åç”Ÿæˆä¸å­¦ä¹ ç›®æ ‡å¯¹é½çš„å¯è¯»è§£é‡Šã€åé¦ˆæˆ–æŒ‡ä»¤å†…å®¹ã€‚åœ¨SERå’Œå…¶ä»–å¤šæ¨¡æ€åŸºå‡†æµ‹è¯•ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼ŒUni-RAGåœ¨æ£€ç´¢å‡†ç¡®æ€§å’Œç”Ÿæˆè´¨é‡æ–¹é¢éƒ½ä¼˜äºåŸºçº¿æ£€ç´¢å’ŒRAGç³»ç»Ÿï¼ŒåŒæ—¶ä¿æŒè¾ƒä½çš„è®¡ç®—æˆæœ¬ã€‚æˆ‘ä»¬çš„æ¡†æ¶ä¸ºæ™ºèƒ½æ•™è‚²ç³»ç»Ÿæä¾›äº†ä¸€ä¸ªå¯æ‰©å±•çš„ã€ä»¥æ•™å­¦æ³•ä¸ºåŸºç¡€è§£å†³æ–¹æ¡ˆï¼Œé€šè¿‡æ¡¥æ¥æ£€ç´¢å’Œç”Ÿæˆæ¥æ”¯æŒä¸ªæ€§åŒ–çš„ã€å¯è§£é‡Šçš„ã€é«˜æ•ˆçš„å­¦ä¹ è¾…åŠ©ï¼Œæ¶µç›–å¤šæ ·åŒ–çš„STEMåœºæ™¯ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.03868v1">PDF</a> </p>
<p><strong>æ€»ç»“</strong><br>åœ¨AIè¾…åŠ©æ•™å­¦é¢†åŸŸï¼Œé‡‡ç”¨ä¸åŒæŸ¥è¯¢é£æ ¼æ¥è§£è¯»æŠ½è±¡çš„æ•™è‚²å†…å®¹å¯¹äºæä¾›æœ‰æ•ˆä¸”å¯è®¿é—®çš„å­¦ä¹ ä½“éªŒè‡³å…³é‡è¦ã€‚ç„¶è€Œï¼Œç°æœ‰çš„æ£€ç´¢ç³»ç»Ÿä¸»è¦å…³æ³¨è‡ªç„¶æ–‡æœ¬å›¾åƒåŒ¹é…ï¼Œç¼ºä¹åº”å¯¹ç°å®ä¸–ç•Œæ•™è‚²åœºæ™¯ä¸­å¤šæ ·æ€§å’Œæ¨¡ç³Šæ€§çš„èƒ½åŠ›ã€‚ä¸ºè§£å†³æ­¤å±€é™ï¼Œæˆ‘ä»¬å¼€å‘äº†ä¸€ä¸ªè½»ä¾¿é«˜æ•ˆçš„å¤šæ¨¡å¼æ£€ç´¢æ¨¡å—ï¼Œåä¸ºUni-Retrievalï¼Œè¯¥æ¨¡å—èƒ½å¤Ÿæå–æŸ¥è¯¢æ ·å¼åŸå‹å¹¶ä¸æ¥è‡ªæŒç»­æ›´æ–°çš„Prompt Bankçš„æ ‡è®°è¿›è¡ŒåŠ¨æ€åŒ¹é…ã€‚Prompt Banké€šè¿‡åˆ©ç”¨æ··åˆä¸“å®¶ä½é˜¶é€‚åº”ï¼ˆMoE-LoRAï¼‰æ¨¡å—æ¥ç¼–ç å’Œå­˜å‚¨ç‰¹å®šé¢†åŸŸçš„çŸ¥è¯†ï¼Œå¹¶å¯åœ¨æµ‹è¯•æ—¶é€‚åº”ä»¥å¢å¼ºUni-Retrievalé€‚åº”æœªè§æŸ¥è¯¢ç±»å‹çš„èƒ½åŠ›ã€‚ä¸ºå®ç°è‡ªç„¶è¯­è¨€æ•™è‚²å†…å®¹ç”Ÿæˆï¼Œæˆ‘ä»¬å°†åŸå§‹çš„Uni-Retrievalä¸ç´§å‡‘çš„æŒ‡ä»¤è°ƒæ•´è¯­è¨€æ¨¡å‹é›†æˆï¼Œå½¢æˆä¸€ä¸ªå®Œæ•´çš„æ£€ç´¢å¢å¼ºç”Ÿæˆç®¡é“ï¼Œç§°ä¸ºUni-RAGã€‚ç»™å®šé£æ ¼æ¡ä»¶ä¸‹çš„æŸ¥è¯¢ï¼ŒUni-RAGé¦–å…ˆæ£€ç´¢ç›¸å…³çš„æ•™è‚²ææ–™ï¼Œç„¶åç”Ÿæˆä¸å­¦ä¹ ç›®æ ‡å¯¹é½çš„äººç±»å¯è¯»è§£é‡Šã€åé¦ˆæˆ–æ•™å­¦å†…å®¹ã€‚åœ¨SERå’Œå…¶ä»–å¤šæ¨¡å¼åŸºå‡†æµ‹è¯•ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼ŒUni-RAGåœ¨æ£€ç´¢å‡†ç¡®æ€§å’Œç”Ÿæˆè´¨é‡æ–¹é¢å‡ä¼˜äºåŸºçº¿æ£€ç´¢å’ŒRAGç³»ç»Ÿï¼ŒåŒæ—¶ä¿æŒè¾ƒä½çš„è®¡ç®—æˆæœ¬ã€‚æˆ‘ä»¬çš„æ¡†æ¶ä¸ºæ™ºèƒ½æ•™è‚²ç³»ç»Ÿæä¾›äº†å¯æ‰©å±•çš„ã€ä»¥æ•™å­¦ä¸ºåŸºç¡€è§£å†³æ–¹æ¡ˆï¼Œé€šè¿‡æ¡¥æ¥æ£€ç´¢å’Œç”Ÿæˆæ¥æ”¯æŒä¸ªæ€§åŒ–ã€å¯è§£é‡Šå’Œé«˜æ•ˆçš„å­¦ä¹ è¾…åŠ©ï¼Œé€‚ç”¨äºå„ç§STEMåœºæ™¯ã€‚</p>
<p><strong>è¦ç‚¹</strong></p>
<ol>
<li>åœ¨AIè¾…åŠ©æ•™å­¦ä¸­ï¼Œåˆ©ç”¨ä¸åŒæŸ¥è¯¢é£æ ¼è§£è¯»æŠ½è±¡æ•™è‚²å†…å®¹è‡³å…³é‡è¦ã€‚</li>
<li>ç°æœ‰æ£€ç´¢ç³»ç»Ÿä¸»è¦å…³æ³¨è‡ªç„¶æ–‡æœ¬å›¾åƒåŒ¹é…ï¼Œç¼ºä¹åº”å¯¹æ•™è‚²åœºæ™¯å¤šæ ·æ€§å’Œæ¨¡ç³Šæ€§çš„èƒ½åŠ›ã€‚</li>
<li>Uni-Retrievalæ˜¯ä¸€ç§æ–°çš„å¤šæ¨¡å¼æ£€ç´¢æ¨¡å—ï¼Œå¯ä»¥æå–æŸ¥è¯¢æ ·å¼åŸå‹å¹¶ä¸Prompt Bankä¸­çš„æ ‡è®°åŠ¨æ€åŒ¹é…ã€‚</li>
<li>Prompt Bankåˆ©ç”¨MoE-LoRAæ¨¡å—ç¼–ç å’Œå­˜å‚¨ç‰¹å®šé¢†åŸŸçŸ¥è¯†ï¼Œå¹¶é€‚åº”å¢å¼ºUni-Retrievalçš„èƒ½åŠ›ã€‚</li>
<li>Uni-RAGé›†æˆäº†Uni-Retrievalå’ŒæŒ‡ä»¤è°ƒæ•´è¯­è¨€æ¨¡å‹ï¼Œå½¢æˆå®Œæ•´çš„æ£€ç´¢å¢å¼ºç”Ÿæˆç®¡é“ã€‚</li>
<li>Uni-RAGé€šè¿‡æ£€ç´¢ç›¸å…³æ•™è‚²ææ–™å’Œç”Ÿæˆäººç±»å¯è¯»å†…å®¹æ¥æ”¯æŒä¸ªæ€§åŒ–ã€å¯è§£é‡Šå’Œé«˜æ•ˆçš„å­¦ä¹ ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.03868">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-451755b5af82f9784ac09760107d2246.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0127829e091d3d01b5de1ce6dd90ff6e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-1f9904c2b89f1f3962444e77f0b16f78.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-82f056c4fdbcc9e6a8fb51199a040d97.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="ChestGPT-Integrating-Large-Language-Models-and-Vision-Transformers-for-Disease-Detection-and-Localization-in-Chest-X-Rays"><a href="#ChestGPT-Integrating-Large-Language-Models-and-Vision-Transformers-for-Disease-Detection-and-Localization-in-Chest-X-Rays" class="headerlink" title="ChestGPT: Integrating Large Language Models and Vision Transformers for   Disease Detection and Localization in Chest X-Rays"></a>ChestGPT: Integrating Large Language Models and Vision Transformers for   Disease Detection and Localization in Chest X-Rays</h2><p><strong>Authors:Shehroz S. Khan, Petar Przulj, Ahmed Ashraf, Ali Abedi</strong></p>
<p>The global demand for radiologists is increasing rapidly due to a growing reliance on medical imaging services, while the supply of radiologists is not keeping pace. Advances in computer vision and image processing technologies present significant potential to address this gap by enhancing radiologistsâ€™ capabilities and improving diagnostic accuracy. Large language models (LLMs), particularly generative pre-trained transformers (GPTs), have become the primary approach for understanding and generating textual data. In parallel, vision transformers (ViTs) have proven effective at converting visual data into a format that LLMs can process efficiently. In this paper, we present ChestGPT, a deep-learning framework that integrates the EVA ViT with the Llama 2 LLM to classify diseases and localize regions of interest in chest X-ray images. The ViT converts X-ray images into tokens, which are then fed, together with engineered prompts, into the LLM, enabling joint classification and localization of diseases. This approach incorporates transfer learning techniques to enhance both explainability and performance. The proposed method achieved strong global disease classification performance on the VinDr-CXR dataset, with an F1 score of 0.76, and successfully localized pathologies by generating bounding boxes around the regions of interest. We also outline several task-specific prompts, in addition to general-purpose prompts, for scenarios radiologists might encounter. Overall, this framework offers an assistive tool that can lighten radiologistsâ€™ workload by providing preliminary findings and regions of interest to facilitate their diagnostic process. </p>
<blockquote>
<p>éšç€å¯¹åŒ»å­¦å½±åƒæœåŠ¡ä¾èµ–ç¨‹åº¦çš„ä¸æ–­å¢åŠ ï¼Œå…¨çƒå¯¹æ”¾å°„ç§‘åŒ»å¸ˆçš„éœ€æ±‚è¿…é€Ÿå¢é•¿ï¼Œç„¶è€Œæ”¾å°„ç§‘åŒ»å¸ˆçš„ä¾›ç»™å´è·Ÿä¸ä¸Šè¿™ä¸€éœ€æ±‚ã€‚è®¡ç®—æœºè§†è§‰å’Œå›¾åƒå¤„ç†æŠ€æœ¯çš„è¿›æ­¥ä¸ºè§£å†³è¿™ä¸€å·®è·æä¾›äº†å·¨å¤§æ½œåŠ›ï¼Œé€šè¿‡å¢å¼ºæ”¾å°„ç§‘åŒ»å¸ˆçš„èƒ½åŠ›å’Œæé«˜è¯Šæ–­å‡†ç¡®æ€§æ¥å®ç°ã€‚å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ï¼Œç‰¹åˆ«æ˜¯é¢„è®­ç»ƒç”Ÿæˆå¼è½¬æ¢å™¨ï¼ˆGPTï¼‰ï¼Œå·²æˆä¸ºç†è§£å’Œç”Ÿæˆæ–‡æœ¬æ•°æ®çš„ä¸»è¦æ–¹æ³•ã€‚åŒæ—¶ï¼Œè§†è§‰è½¬æ¢å™¨ï¼ˆViTï¼‰åœ¨å°†è§†è§‰æ•°æ®è½¬æ¢ä¸ºLLMå¯ä»¥é«˜æ•ˆå¤„ç†çš„å½¢å¼æ–¹é¢è¡¨ç°å‡ºè‰²ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ChestGPTï¼Œè¿™æ˜¯ä¸€ä¸ªæ·±åº¦å­¦ä¹ æ¡†æ¶ï¼Œå®ƒå°†EVA ViTä¸Llama 2 LLMç›¸ç»“åˆï¼Œç”¨äºå¯¹èƒ¸éƒ¨Xå°„çº¿å›¾åƒä¸­çš„ç–¾ç—…è¿›è¡Œåˆ†ç±»å¹¶å®šä½æ„Ÿå…´è¶£åŒºåŸŸã€‚ViTå°†Xå°„çº¿å›¾åƒè½¬æ¢ä¸ºä»¤ç‰Œï¼Œç„¶åè¿™äº›ä»¤ç‰Œä¸å·¥ç¨‹æç¤ºä¸€èµ·è¾“å…¥åˆ°LLMä¸­ï¼Œä»¥å®ç°ç–¾ç—…çš„è”åˆåˆ†ç±»å’Œå®šä½ã€‚è¿™ç§æ–¹æ³•ç»“åˆäº†è¿ç§»å­¦ä¹ æŠ€æœ¯ï¼Œä»¥æé«˜å¯è§£é‡Šæ€§å’Œæ€§èƒ½ã€‚æ‰€æå‡ºçš„æ–¹æ³•åœ¨VinDr-CXRæ•°æ®é›†ä¸Šå–å¾—äº†å¼ºå¤§çš„å…¨çƒç–¾ç—…åˆ†ç±»æ€§èƒ½ï¼ŒF1åˆ†æ•°ä¸º0.76ï¼Œå¹¶é€šè¿‡å›´ç»•æ„Ÿå…´è¶£åŒºåŸŸç”Ÿæˆè¾¹ç•Œæ¡†æˆåŠŸåœ°å®šä½äº†ç—…ç†ã€‚é™¤äº†é€šç”¨æç¤ºå¤–ï¼Œæˆ‘ä»¬è¿˜æ¦‚è¿°äº†é’ˆå¯¹æ”¾å°„ç§‘åŒ»ç”Ÿå¯èƒ½é‡åˆ°çš„å„ç§æƒ…æ™¯çš„ç‰¹å®šä»»åŠ¡æç¤ºã€‚æ€»çš„æ¥è¯´ï¼Œè¯¥æ¡†æ¶æä¾›äº†ä¸€ä¸ªè¾…åŠ©å·¥å…·ï¼Œå¯ä»¥é€šè¿‡æä¾›åˆæ­¥å‘ç°å’Œæ„Ÿå…´è¶£åŒºåŸŸæ¥å‡è½»æ”¾å°„ç§‘åŒ»ç”Ÿçš„å·¥ä½œé‡ï¼Œä»è€Œå¸®åŠ©ä»–ä»¬è¿›è¡Œè¯Šæ–­è¿‡ç¨‹ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.03739v1">PDF</a> 8 pages, 5 figures, 4 tables</p>
<p><strong>Summary</strong><br>     éšç€å¯¹åŒ»å­¦å½±åƒæœåŠ¡ä¾èµ–åº¦ä¸æ–­å¢åŠ ï¼Œå…¨çƒå¯¹æ”¾å°„ç§‘åŒ»ç”Ÿçš„éœ€æ±‚è¿…é€Ÿå¢é•¿ï¼Œè€Œæ”¾å°„ç§‘åŒ»ç”Ÿä¾›ç»™ä¸è¶³ã€‚è®¡ç®—æœºè§†è§‰å’Œå›¾åƒå¤„ç†æŠ€æœ¯çš„è¿›å±•ä¸ºè§£å†³è¿™ä¸€å·®è·æä¾›äº†å·¨å¤§æ½œåŠ›ï¼Œé€šè¿‡å¢å¼ºæ”¾å°„ç§‘åŒ»ç”Ÿçš„èƒ½åŠ›å’Œæé«˜è¯Šæ–­å‡†ç¡®æ€§ã€‚æœ¬æ–‡ä»‹ç»äº†ä¸€ä¸ªæ·±åº¦å­¦ä¹ æ¡†æ¶ChestGPTï¼Œå®ƒç»“åˆäº†EVA ViTå’ŒLlama 2 LLMï¼Œç”¨äºå¯¹èƒ¸éƒ¨Xå°„çº¿å›¾åƒçš„ç–¾ç—…è¿›è¡Œåˆ†ç±»å¹¶å®šä½æ„Ÿå…´è¶£åŒºåŸŸã€‚è¯¥æ¡†æ¶ä½¿ç”¨ViTå°†Xå°„çº¿å›¾åƒè½¬æ¢ä¸ºä»¤ç‰Œï¼Œç„¶åä¸å·¥ç¨‹æç¤ºä¸€èµ·è¾“å…¥LLMï¼Œå®ç°ç–¾ç—…çš„è”åˆåˆ†ç±»å’Œå®šä½ã€‚è¯¥æ–¹æ³•é‡‡ç”¨è¿ç§»å­¦ä¹ æŠ€æœ¯ï¼Œæé«˜äº†å¯è§£é‡Šæ€§å’Œæ€§èƒ½ã€‚åœ¨VinDr-CXRæ•°æ®é›†ä¸Šï¼Œè¯¥æ–¹æ³•å®ç°äº†å¼ºå¤§çš„å…¨çƒç–¾ç—…åˆ†ç±»æ€§èƒ½ï¼ŒF1åˆ†æ•°ä¸º0.76ï¼Œå¹¶æˆåŠŸå®šä½äº†ç—…ç†åŒºåŸŸã€‚æ€»ä½“è€Œè¨€ï¼Œè¯¥æ¡†æ¶ä¸ºæ”¾å°„ç§‘åŒ»ç”Ÿæä¾›äº†ä¸€ç§è¾…åŠ©å·¥å…·ï¼Œå¯é€šè¿‡æä¾›åˆæ­¥æ£€æŸ¥ç»“æœå’Œæ„Ÿå…´è¶£åŒºåŸŸï¼Œå‡è½»ä»–ä»¬çš„å·¥ä½œé‡å¹¶ä¿ƒè¿›è¯Šæ–­è¿‡ç¨‹ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å…¨çƒå¯¹æ”¾å°„ç§‘åŒ»ç”Ÿçš„éœ€æ±‚è¿…é€Ÿå¢é•¿ï¼Œè€Œä¾›ç»™ä¸è¶³ï¼Œéœ€è¦æ–°æŠ€æœ¯ååŠ©è§£å†³è¿™ä¸€å·®è·ã€‚</li>
<li>è®¡ç®—æœºè§†è§‰å’Œå›¾åƒå¤„ç†æŠ€æœ¯æœ‰åŠ©äºæé«˜æ”¾å°„ç§‘åŒ»ç”Ÿçš„è¯Šæ–­èƒ½åŠ›å’Œå‡†ç¡®æ€§ã€‚</li>
<li>LLMsï¼ˆç‰¹åˆ«æ˜¯GPTsï¼‰å’ŒViTsåœ¨å¤„ç†åŒ»å­¦å›¾åƒæ•°æ®æ–¹é¢è¡¨ç°å‡ºå·¨å¤§æ½œåŠ›ã€‚</li>
<li>ChestGPTæ˜¯ä¸€ä¸ªç»“åˆEVA ViTå’ŒLlama 2 LLMçš„æ·±åº¦å­¦ä¹ æ¡†æ¶ï¼Œç”¨äºå¯¹èƒ¸éƒ¨Xå°„çº¿å›¾åƒçš„ç–¾ç—…è¿›è¡Œåˆ†ç±»å’Œå®šä½ã€‚</li>
<li>è¯¥æ¡†æ¶ä½¿ç”¨è¿ç§»å­¦ä¹ æŠ€æœ¯æ¥æé«˜æ€§èƒ½å¹¶å¢å¼ºè§£é‡Šæ€§ã€‚</li>
<li>åœ¨VinDr-CXRæ•°æ®é›†ä¸Šï¼ŒChestGPTå®ç°äº†F1åˆ†æ•°ä¸º0.76çš„å‡ºè‰²æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.03739">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-db88ede29c1abd31b63991df0ebb5723.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-2ed04ad5f23d1db131702c8528519669.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8d800fea9816b4ffe409770b295f2590.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-71cbb5a4649418693f278bd593808692.jpg" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="Beyond-SEO-A-Transformer-Based-Approach-for-Reinventing-Web-Content-Optimisation"><a href="#Beyond-SEO-A-Transformer-Based-Approach-for-Reinventing-Web-Content-Optimisation" class="headerlink" title="Beyond SEO: A Transformer-Based Approach for Reinventing Web Content   Optimisation"></a>Beyond SEO: A Transformer-Based Approach for Reinventing Web Content   Optimisation</h2><p><strong>Authors:Florian LÃ¼ttgenau, Imar Colic, Gervasio Ramirez</strong></p>
<p>The rise of generative AI search engines is disrupting traditional SEO, with Gartner predicting 25% reduction in conventional search usage by 2026. This necessitates new approaches for web content visibility in AI-driven search environments. We present a domain-specific fine-tuning approach for Generative Engine Optimization (GEO) that transforms web content to improve discoverability in large language model outputs. Our method fine-tunes a BART-base transformer on synthetically generated training data comprising 1,905 cleaned travel website content pairs. Each pair consists of raw website text and its GEO-optimized counterpart incorporating credible citations, statistical evidence, and improved linguistic fluency. We evaluate using intrinsic metrics (ROUGE-L, BLEU) and extrinsic visibility assessments through controlled experiments with Llama-3.3-70B. The fine-tuned model achieves significant improvements over baseline BART: ROUGE-L scores of 0.249 (vs. 0.226) and BLEU scores of 0.200 (vs. 0.173). Most importantly, optimized content demonstrates substantial visibility gains in generative search responses with 15.63% improvement in absolute word count and 30.96% improvement in position-adjusted word count metrics. This work provides the first empirical demonstration that targeted transformer fine-tuning can effectively enhance web content visibility in generative search engines with modest computational resources. Our results suggest GEO represents a tractable approach for content optimization in the AI-driven search landscape, offering concrete evidence that small-scale, domain-focused fine-tuning yields meaningful improvements in content discoverability. </p>
<blockquote>
<p>ç”Ÿæˆå¼AIæœç´¢å¼•æ“çš„å´›èµ·æ­£åœ¨é¢ è¦†ä¼ ç»Ÿçš„SEOï¼Œæ®åŠ ç‰¹çº³é¢„æµ‹ï¼Œåˆ°2026å¹´ï¼Œä¼ ç»Ÿæœç´¢ä½¿ç”¨ç‡å°†å‡å°‘25%ã€‚è¿™è¦æ±‚åœ¨AIé©±åŠ¨çš„æœç´¢ç¯å¢ƒä¸­é‡‡ç”¨æ–°çš„æ–¹æ³•æ¥æé«˜ç½‘é¡µå†…å®¹çš„å¯è§æ€§ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§é’ˆå¯¹ç”Ÿæˆå¼•æ“ä¼˜åŒ–ï¼ˆGEOï¼‰çš„ç‰¹å®šé¢†åŸŸå¾®è°ƒæ–¹æ³•ï¼Œè¯¥æ–¹æ³•å¯ä»¥è½¬æ¢ç½‘é¡µå†…å®¹ï¼Œä»¥æé«˜åœ¨å¤§è¯­è¨€æ¨¡å‹è¾“å‡ºä¸­çš„å¯å‘ç°æ€§ã€‚æˆ‘ä»¬çš„æ–¹æ³•å¯¹åŸºäºBARTçš„åŸºç¡€è½¬æ¢å™¨è¿›è¡Œå¾®è°ƒï¼Œä½¿ç”¨åˆæˆç”Ÿæˆçš„è®­ç»ƒæ•°æ®ï¼ŒåŒ…å«1905ç»„ç»è¿‡æ¸…ç†çš„æ—…è¡Œç½‘ç«™å†…å®¹ã€‚æ¯å¯¹å†…å®¹éƒ½åŒ…å«åŸå§‹ç½‘ç«™æ–‡æœ¬å’Œå…¶ç»è¿‡GEOä¼˜åŒ–çš„å¯¹åº”æ–‡æœ¬ï¼Œåè€…èå…¥äº†å¯ä¿¡çš„å¼•ç”¨ã€ç»Ÿè®¡è¯æ®å’Œæ›´æµç•…çš„è¯­è¨€è¡¨è¾¾ã€‚æˆ‘ä»¬é‡‡ç”¨å†…åœ¨æŒ‡æ ‡ï¼ˆROUGE-Lï¼ŒBLEUï¼‰å’Œé€šè¿‡Llama-3.3-70Bè¿›è¡Œçš„å¤–åœ¨å¯è§æ€§è¯„ä¼°å®éªŒæ¥è¯„ä¼°å…¶æ•ˆæœã€‚ç»è¿‡å¾®è°ƒçš„æ¨¡å‹åœ¨åŸºçº¿BARTä¸Šå®ç°äº†æ˜¾è‘—æ”¹è¿›ï¼šROUGE-Lå¾—åˆ†ä¸º0.249ï¼ˆå¯¹æ¯”åŸºçº¿æ¨¡å‹çš„0.226ï¼‰ï¼ŒBLEUå¾—åˆ†ä¸º0.200ï¼ˆå¯¹æ¯”åŸºçº¿æ¨¡å‹çš„0.173ï¼‰ã€‚æœ€é‡è¦çš„æ˜¯ï¼Œä¼˜åŒ–åçš„å†…å®¹åœ¨ç”Ÿæˆå¼æœç´¢å“åº”ä¸­æ˜¾ç¤ºå‡ºæ˜¾è‘—çš„å¯è§æ€§æå‡ï¼Œç»å¯¹å­—æ•°å¢åŠ äº†15.63%ï¼Œä½ç½®è°ƒæ•´åçš„å­—æ•°æŒ‡æ ‡å¢åŠ äº†30.96%ã€‚è¿™é¡¹å·¥ä½œé¦–æ¬¡å®è¯äº†æœ‰é’ˆå¯¹æ€§çš„è½¬æ¢å™¨å¾®è°ƒå¯ä»¥æœ‰æ•ˆåœ°æé«˜åœ¨ç”Ÿæˆå¼æœç´¢å¼•æ“ä¸­çš„ç½‘é¡µå†…å®¹å¯è§æ€§ï¼Œå¹¶ä¸”åªéœ€è¦é€‚åº¦çš„è®¡ç®—èµ„æºã€‚æˆ‘ä»¬çš„ç»“æœè¡¨æ˜ï¼ŒGEOæ˜¯AIé©±åŠ¨æœç´¢ç¯å¢ƒä¸­å†…å®¹ä¼˜åŒ–çš„å¯è¡Œæ–¹æ³•ï¼Œæä¾›äº†å…·ä½“è¯æ®è¡¨æ˜å°è§„æ¨¡ã€é’ˆå¯¹ç‰¹å®šé¢†åŸŸçš„å¾®è°ƒå¯ä»¥åœ¨å†…å®¹å¯å‘ç°æ€§æ–¹é¢å®ç°æœ‰æ„ä¹‰çš„æ”¹è¿›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.03169v1">PDF</a> 9 pages, 3 figures</p>
<p><strong>æ‘˜è¦</strong><br>    éšç€ç”Ÿæˆå¼AIæœç´¢å¼•æ“çš„å…´èµ·ï¼Œä¼ ç»ŸSEOé¢ä¸´é¢ è¦†ã€‚é¢„è®¡åˆ°2026å¹´ï¼Œä¼ ç»Ÿæœç´¢ä½¿ç”¨ç‡å°†å‡å°‘25%ã€‚ä¸ºé€‚åº”AIé©±åŠ¨æœç´¢ç¯å¢ƒï¼Œæå‡ºé¢†åŸŸç‰¹å®šçš„å¾®è°ƒæ–¹æ³•ï¼Œç”¨äºç”Ÿæˆå¼å¼•æ“ä¼˜åŒ–ï¼ˆGEOï¼‰ã€‚é€šè¿‡å¯¹åˆæˆè®­ç»ƒæ•°æ®çš„1905ç»„æ—…æ¸¸ç½‘ç«™å†…å®¹çš„æ¸…æ´—å’Œé…å¯¹ï¼Œå¯¹BARTåŸºç¡€è½¬æ¢å™¨è¿›è¡Œå¾®è°ƒã€‚æ¯å¯¹å†…å®¹åŒ…å«åŸå§‹ç½‘ç«™æ–‡æœ¬å’Œç»è¿‡GEOä¼˜åŒ–çš„å¯¹åº”æ–‡æœ¬ï¼ŒåŒæ—¶èå…¥å¯ä¿¡å¼•ç”¨ã€ç»Ÿè®¡è¯æ®å’Œæ”¹è¿›çš„è¯­è¨€æµç•…æ€§ã€‚é€šè¿‡å†…åœ¨åº¦é‡ï¼ˆROUGE-Lï¼ŒBLEUï¼‰å’Œå¤–åœ¨å¯è§æ€§è¯„ä¼°å®éªŒï¼Œå¯¹æ¯”Llama-3.3-70Bæ¨¡å‹ï¼Œå¾®è°ƒæ¨¡å‹åœ¨åŸºçº¿BARTä¸Šå®ç°äº†æ˜¾è‘—æ”¹è¿›ï¼šROUGE-Lå¾—åˆ†0.249ï¼ˆå¯¹æ¯”0.226ï¼‰ï¼ŒBLEUå¾—åˆ†0.200ï¼ˆå¯¹æ¯”0.173ï¼‰ã€‚æœ€é‡è¦çš„æ˜¯ï¼Œä¼˜åŒ–å†…å®¹åœ¨ç”Ÿæˆå¼æœç´¢å“åº”ä¸­è¡¨ç°å‡ºæ›´é«˜çš„å¯è§æ€§ï¼Œç»å¯¹è¯æ•°å¢åŠ 15.63%ï¼Œä½ç½®è°ƒæ•´è¯æ•°å¢åŠ 30.96%ã€‚æœ¬ç ”ç©¶é¦–æ¬¡å®è¯ï¼Œæœ‰é’ˆå¯¹æ€§çš„è½¬æ¢å™¨å¾®è°ƒèƒ½æœ‰æ•ˆæé«˜AIé©±åŠ¨æœç´¢å¼•æ“ä¸­ç½‘é¡µå†…å®¹çš„å¯è§æ€§ï¼Œåˆ©ç”¨æœ‰é™çš„è®¡ç®—èµ„æºå³å¯å®ç°å†…å®¹ä¼˜åŒ–ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>ç”Ÿæˆå¼AIæœç´¢å¼•æ“çš„å´›èµ·æ­£åœ¨æ”¹å˜ä¼ ç»Ÿçš„SEOç­–ç•¥ã€‚</li>
<li>åˆ°2026å¹´ï¼Œé¢„è®¡ä¼ ç»Ÿæœç´¢çš„ä½¿ç”¨ç‡å°†å‡å°‘25%ã€‚</li>
<li>æå‡ºäº†ä¸€ç§é’ˆå¯¹ç‰¹å®šé¢†åŸŸçš„å¾®è°ƒæ–¹æ³•â€”â€”ç”Ÿæˆå¼å¼•æ“ä¼˜åŒ–ï¼ˆGEOï¼‰ï¼Œä»¥é€‚åº”AIé©±åŠ¨çš„æœç´¢ç¯å¢ƒã€‚</li>
<li>é€šè¿‡åˆæˆè®­ç»ƒæ•°æ®å¯¹BARTåŸºç¡€è½¬æ¢å™¨è¿›è¡Œå¾®è°ƒï¼Œæé«˜äº†ç½‘ç«™å†…å®¹åœ¨æœç´¢å¼•æ“ä¸­çš„å¯è§æ€§ã€‚</li>
<li>ä¼˜åŒ–åçš„å†…å®¹åœ¨ç”Ÿæˆå¼æœç´¢å“åº”ä¸­è¡¨ç°å‡ºæ›´é«˜çš„å¯è§æ€§ï¼Œç»å¯¹è¯æ•°å’Œä½ç½®è°ƒæ•´è¯æ•°å‡æœ‰æ˜¾è‘—æé«˜ã€‚</li>
<li>ç ”ç©¶æ˜¾ç¤ºï¼Œæœ‰é’ˆå¯¹æ€§çš„è½¬æ¢å™¨å¾®è°ƒå¯ä»¥æœ‰æ•ˆæé«˜ç½‘é¡µå†…å®¹åœ¨AIé©±åŠ¨æœç´¢å¼•æ“ä¸­çš„å¯è§æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.03169">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-76364267e87ff66502daa45d418fe545.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f1e73dd662d0d157b4856cf9e1e42470.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-1bcf44cc766ce6f8acf0ade413a96bf6.jpg" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="BERT4Traj-Transformer-Based-Trajectory-Reconstruction-for-Sparse-Mobility-Data"><a href="#BERT4Traj-Transformer-Based-Trajectory-Reconstruction-for-Sparse-Mobility-Data" class="headerlink" title="BERT4Traj: Transformer Based Trajectory Reconstruction for Sparse   Mobility Data"></a>BERT4Traj: Transformer Based Trajectory Reconstruction for Sparse   Mobility Data</h2><p><strong>Authors:Hao Yang, Angela Yao, Christopher Whalen, Gengchen Mai</strong></p>
<p>Understanding human mobility is essential for applications in public health, transportation, and urban planning. However, mobility data often suffers from sparsity due to limitations in data collection methods, such as infrequent GPS sampling or call detail record (CDR) data that only capture locations during communication events. To address this challenge, we propose BERT4Traj, a transformer based model that reconstructs complete mobility trajectories by predicting hidden visits in sparse movement sequences. Inspired by BERTâ€™s masked language modeling objective and self_attention mechanisms, BERT4Traj leverages spatial embeddings, temporal embeddings, and contextual background features such as demographics and anchor points. We evaluate BERT4Traj on real world CDR and GPS datasets collected in Kampala, Uganda, demonstrating that our approach significantly outperforms traditional models such as Markov Chains, KNN, RNNs, and LSTMs. Our results show that BERT4Traj effectively reconstructs detailed and continuous mobility trajectories, enhancing insights into human movement patterns. </p>
<blockquote>
<p>ç†è§£äººç±»ç§»åŠ¨æ€§å¯¹äºå…¬å…±å«ç”Ÿã€äº¤é€šè¿è¾“å’ŒåŸå¸‚è§„åˆ’ç­‰é¢†åŸŸçš„åº”ç”¨è‡³å…³é‡è¦ã€‚ç„¶è€Œï¼Œç”±äºæ•°æ®æ”¶é›†æ–¹æ³•çš„å±€é™æ€§ï¼Œå¦‚GPSé‡‡æ ·é¢‘ç‡ä½æˆ–ä»…èƒ½åœ¨é€šä¿¡äº‹ä»¶æœŸé—´æ•è·ä½ç½®çš„å‘¼å«è¯¦ç»†è®°å½•ï¼ˆCDRï¼‰æ•°æ®ï¼Œç§»åŠ¨æ€§æ•°æ®å¸¸å¸¸å­˜åœ¨ç¨€ç–æ€§é—®é¢˜ã€‚ä¸ºäº†åº”å¯¹è¿™ä¸€æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†BERT4Trajï¼Œè¿™æ˜¯ä¸€ä¸ªåŸºäºå˜å‹å™¨çš„æ¨¡å‹ï¼Œé€šè¿‡é¢„æµ‹ç¨€ç–ç§»åŠ¨åºåˆ—ä¸­çš„éšè—è®¿é—®ç‚¹æ¥é‡å»ºå®Œæ•´çš„ç§»åŠ¨è½¨è¿¹ã€‚BERT4Trajå—åˆ°BERTçš„æ©ç è¯­è¨€å»ºæ¨¡ç›®æ ‡å’Œè‡ªæˆ‘æ³¨æ„æœºåˆ¶çš„å¯å‘ï¼Œåˆ©ç”¨ç©ºé—´åµŒå…¥ã€æ—¶é—´åµŒå…¥ä»¥åŠå¦‚äººå£ç»Ÿè®¡å­¦å’Œé”šç‚¹ç­‰ä¸Šä¸‹æ–‡èƒŒæ™¯ç‰¹å¾ã€‚æˆ‘ä»¬åœ¨ä¹Œå¹²è¾¾åå¸•æ‹‰æ”¶é›†çš„çœŸå®ä¸–ç•ŒCDRå’ŒGPSæ•°æ®é›†ä¸Šè¯„ä¼°äº†BERT4Trajï¼Œç»“æœè¡¨æ˜æˆ‘ä»¬çš„æ–¹æ³•æ˜¾è‘—ä¼˜äºä¼ ç»Ÿæ¨¡å‹ï¼Œå¦‚é©¬å°”å¯å¤«é“¾ã€KNNã€RNNså’ŒLSTMã€‚æˆ‘ä»¬çš„ç»“æœè¡¨æ˜ï¼ŒBERT4Trajèƒ½å¤Ÿæœ‰æ•ˆåœ°é‡å»ºè¯¦ç»†ä¸”è¿ç»­çš„ç§»åŠ¨è½¨è¿¹ï¼Œä»è€ŒåŠ æ·±å¯¹äººç±»ç§»åŠ¨æ¨¡å¼çš„æ´å¯Ÿã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.03062v1">PDF</a> This paper was accepted at GIScience 2025</p>
<p><strong>æ‘˜è¦</strong></p>
<p>æ–‡æœ¬æŒ‡å‡ºç†è§£äººç±»ç§»åŠ¨æ€§å¯¹äºå…¬å…±å«ç”Ÿã€äº¤é€šè¿è¾“å’ŒåŸå¸‚è§„åˆ’ç­‰åº”ç”¨è‡³å…³é‡è¦ã€‚ç„¶è€Œï¼Œç”±äºæ•°æ®æ”¶é›†æ–¹æ³•çš„å±€é™æ€§ï¼Œå¦‚GPSé‡‡æ ·ä¸é¢‘ç¹æˆ–ä»…èƒ½åœ¨é€šä¿¡äº‹ä»¶æœŸé—´æ•è·ä½ç½®çš„å‘¼å«è¯¦ç»†è®°å½•ï¼ˆCDRï¼‰æ•°æ®ç­‰ï¼Œç§»åŠ¨æ€§æ•°æ®å¸¸å¸¸å­˜åœ¨ç¨€ç–æ€§é—®é¢˜ã€‚ä¸ºåº”å¯¹è¿™ä¸€æŒ‘æˆ˜ï¼Œæ–‡æœ¬æå‡ºäº†BERT4Trajæ¨¡å‹ï¼Œå®ƒé€šè¿‡é¢„æµ‹ç¨€ç–ç§»åŠ¨åºåˆ—ä¸­çš„éšè—è®¿é—®ç‚¹æ¥é‡å»ºå®Œæ•´çš„ç§»åŠ¨è½¨è¿¹ã€‚BERT4Trajå€Ÿé‰´äº†BERTçš„æ©ç è¯­è¨€å»ºæ¨¡ç›®æ ‡å’Œè‡ªæˆ‘å…³æ³¨æœºåˆ¶ï¼Œå¹¶åˆ©ç”¨ç©ºé—´åµŒå…¥ã€æ—¶é—´åµŒå…¥ä»¥åŠèƒŒæ™¯ç‰¹å¾å¦‚äººå£ç»Ÿè®¡å­¦å’Œé”šç‚¹ç­‰ã€‚æ–‡æœ¬é€šè¿‡ä¹Œå¹²è¾¾åå¸•æ‹‰å¸‚çš„ç°å®CDRå’ŒGPSæ•°æ®é›†å¯¹BERT4Trajè¿›è¡Œè¯„ä¼°ï¼Œè¯æ˜äº†å…¶åœ¨ä¼ ç»Ÿçš„Markové“¾ã€KNNã€RNNå’ŒLSTMæ¨¡å‹ä¸­çš„ä¼˜è¶Šæ€§ã€‚ç»“æœæ˜¾ç¤ºï¼ŒBERT4Trajèƒ½æœ‰æ•ˆé‡å»ºè¯¦ç»†ä¸”è¿ç»­çš„ç§»åŠ¨è½¨è¿¹ï¼Œæœ‰åŠ©äºæ·±å…¥äº†è§£äººç±»ç§»åŠ¨æ¨¡å¼ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>ç†è§£äººç±»ç§»åŠ¨æ€§å¯¹äºå¤šä¸ªé¢†åŸŸçš„åº”ç”¨è‡³å…³é‡è¦ï¼ŒåŒ…æ‹¬å…¬å…±å«ç”Ÿã€äº¤é€šè¿è¾“å’ŒåŸå¸‚è§„åˆ’ã€‚</li>
<li>ç§»åŠ¨æ€§æ•°æ®ç»å¸¸å› æ•°æ®æ”¶é›†æ–¹æ³•çš„å±€é™æ€§è€Œç¨€ç–ã€‚</li>
<li>BERT4Trajæ˜¯ä¸€ç§åŸºäºå˜å‹å™¨çš„æ¨¡å‹ï¼Œèƒ½å¤Ÿé‡å»ºå®Œæ•´çš„ç§»åŠ¨è½¨è¿¹ï¼Œé€šè¿‡é¢„æµ‹ç¨€ç–ç§»åŠ¨åºåˆ—ä¸­çš„éšè—è®¿é—®ç‚¹ã€‚</li>
<li>BERT4Trajå€Ÿé‰´äº†BERTçš„æ©ç è¯­è¨€å»ºæ¨¡ç›®æ ‡å’Œè‡ªæˆ‘å…³æ³¨æœºåˆ¶ã€‚</li>
<li>BERT4Trajåˆ©ç”¨ç©ºé—´åµŒå…¥ã€æ—¶é—´åµŒå…¥ä»¥åŠèƒŒæ™¯ç‰¹å¾å¦‚äººå£ç»Ÿè®¡å­¦å’Œé”šç‚¹ã€‚</li>
<li>åœ¨ç°å®ä¸–ç•Œçš„CDRå’ŒGPSæ•°æ®é›†ä¸Šï¼ŒBERT4Trajçš„è¡¨ç°æ˜¾è‘—ä¼˜äºä¼ ç»Ÿæ¨¡å‹ã€‚</li>
<li>BERT4Trajèƒ½é‡å»ºè¯¦ç»†ä¸”è¿ç»­çš„ç§»åŠ¨è½¨è¿¹ï¼Œæœ‰åŠ©äºæ·±å…¥äº†è§£äººç±»ç§»åŠ¨æ¨¡å¼ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.03062">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-187540250cfdeb84e3753e8ab688a63a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-4572f18042ac658fe722fa87230792e2.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e5750dbfa6e61ef0c4d87777d44fe016.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-2a68b7d896b6ea3f3fb552249c468b2f.jpg" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1><h2 id="AIGI-Holmes-Towards-Explainable-and-Generalizable-AI-Generated-Image-Detection-via-Multimodal-Large-Language-Models"><a href="#AIGI-Holmes-Towards-Explainable-and-Generalizable-AI-Generated-Image-Detection-via-Multimodal-Large-Language-Models" class="headerlink" title="AIGI-Holmes: Towards Explainable and Generalizable AI-Generated Image   Detection via Multimodal Large Language Models"></a>AIGI-Holmes: Towards Explainable and Generalizable AI-Generated Image   Detection via Multimodal Large Language Models</h2><p><strong>Authors:Ziyin Zhou, Yunpeng Luo, Yuanchen Wu, Ke Sun, Jiayi Ji, Ke Yan, Shouhong Ding, Xiaoshuai Sun, Yunsheng Wu, Rongrong Ji</strong></p>
<p>The rapid development of AI-generated content (AIGC) technology has led to the misuse of highly realistic AI-generated images (AIGI) in spreading misinformation, posing a threat to public information security. Although existing AIGI detection techniques are generally effective, they face two issues: 1) a lack of human-verifiable explanations, and 2) a lack of generalization in the latest generation technology. To address these issues, we introduce a large-scale and comprehensive dataset, Holmes-Set, which includes the Holmes-SFTSet, an instruction-tuning dataset with explanations on whether images are AI-generated, and the Holmes-DPOSet, a human-aligned preference dataset. Our work introduces an efficient data annotation method called the Multi-Expert Jury, enhancing data generation through structured MLLM explanations and quality control via cross-model evaluation, expert defect filtering, and human preference modification. In addition, we propose Holmes Pipeline, a meticulously designed three-stage training framework comprising visual expert pre-training, supervised fine-tuning, and direct preference optimization. Holmes Pipeline adapts multimodal large language models (MLLMs) for AIGI detection while generating human-verifiable and human-aligned explanations, ultimately yielding our model AIGI-Holmes. During the inference stage, we introduce a collaborative decoding strategy that integrates the model perception of the visual expert with the semantic reasoning of MLLMs, further enhancing the generalization capabilities. Extensive experiments on three benchmarks validate the effectiveness of our AIGI-Holmes. </p>
<blockquote>
<p>äººå·¥æ™ºèƒ½ç”Ÿæˆå†…å®¹ï¼ˆAIGCï¼‰æŠ€æœ¯çš„å¿«é€Ÿå‘å±•å¯¼è‡´é«˜åº¦é€¼çœŸçš„AIç”Ÿæˆå›¾åƒï¼ˆAIGIï¼‰è¢«æ»¥ç”¨ï¼Œä¼ æ’­é”™è¯¯ä¿¡æ¯ï¼Œå¯¹å…¬ä¼—ä¿¡æ¯å®‰å…¨æ„æˆå¨èƒã€‚å°½ç®¡ç°æœ‰çš„AIGIæ£€æµ‹æŠ€æœ¯é€šå¸¸æœ‰æ•ˆï¼Œä½†å®ƒä»¬é¢ä¸´ä¸¤ä¸ªé—®é¢˜ï¼š1ï¼‰ç¼ºä¹å¯éªŒè¯çš„äººä¸ºè§£é‡Šï¼›2ï¼‰æœ€æ–°æŠ€æœ¯ä¸­ç¼ºä¹é€šç”¨æ€§ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ªå¤§è§„æ¨¡ä¸”ç»¼åˆçš„æ•°æ®é›†Holmes-Setï¼Œå…¶ä¸­åŒ…æ‹¬Holmes-SFTSetï¼Œè¿™æ˜¯ä¸€ä¸ªå¸¦æœ‰è§£é‡ŠæŒ‡ä»¤çš„æ•°æ®é›†ï¼Œè§£é‡Šå›¾åƒæ˜¯å¦æ˜¯AIç”Ÿæˆçš„ï¼Œä»¥åŠHolmes-DPOSetï¼Œè¿™æ˜¯ä¸€ä¸ªä¸äººä¸ºåå¥½å¯¹é½çš„æ•°æ®é›†ã€‚æˆ‘ä»¬çš„å·¥ä½œå¼•å…¥äº†ä¸€ç§é«˜æ•ˆçš„æ•°æ®æ³¨é‡Šæ–¹æ³•ï¼Œç§°ä¸ºå¤šä¸“å®¶é™ªå®¡å›¢ï¼Œé€šè¿‡ç»“æ„åŒ–çš„MLLMè§£é‡Šå’Œè´¨é‡æ§åˆ¶æ¥æé«˜æ•°æ®ç”Ÿæˆæ•ˆç‡ï¼Œè´¨é‡æ§åˆ¶åŒ…æ‹¬è·¨æ¨¡å‹è¯„ä¼°ã€ä¸“å®¶ç¼ºé™·è¿‡æ»¤å’Œäººä¸ºåå¥½ä¿®æ”¹ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æå‡ºäº†ç²¾å¿ƒè®¾è®¡çš„ä¸‰é˜¶æ®µè®­ç»ƒæ¡†æ¶Holmes Pipelineï¼ŒåŒ…æ‹¬è§†è§‰ä¸“å®¶é¢„è®­ç»ƒã€ç›‘ç£å¾®è°ƒä»¥åŠç›´æ¥åå¥½ä¼˜åŒ–ã€‚Holmes Pipelineä½¿å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰é€‚åº”äºAIGIæ£€æµ‹ï¼ŒåŒæ—¶ç”Ÿæˆå¯éªŒè¯çš„ã€ä¸äººä¸ºåå¥½å¯¹é½çš„è§£é‡Šï¼Œæœ€ç»ˆç”Ÿæˆæˆ‘ä»¬çš„æ¨¡å‹AIGI-Holmesã€‚åœ¨æ¨ç†é˜¶æ®µï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§ååŒè§£ç ç­–ç•¥ï¼Œå°†è§†è§‰ä¸“å®¶çš„æ¨¡å‹æ„ŸçŸ¥ä¸MLLMsçš„è¯­ä¹‰æ¨ç†ç›¸ç»“åˆï¼Œè¿›ä¸€æ­¥æé«˜æ¨¡å‹çš„é€šç”¨æ€§ã€‚åœ¨ä¸‰ä¸ªåŸºå‡†ä¸Šçš„å¤§é‡å®éªŒéªŒè¯äº†æˆ‘ä»¬çš„AIGI-Holmesçš„æœ‰æ•ˆæ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.02664v2">PDF</a> Accepted to ICCV 2025</p>
<p><strong>Summary</strong><br>     äººå·¥æ™ºèƒ½ç”Ÿæˆå†…å®¹ï¼ˆAIGCï¼‰æŠ€æœ¯çš„å¿«é€Ÿå‘å±•å¯¼è‡´AIç”Ÿæˆå›¾åƒï¼ˆAIGIï¼‰è¢«æ»¥ç”¨ï¼Œä¼ æ’­é”™è¯¯ä¿¡æ¯ï¼Œå¨èƒå…¬ä¼—ä¿¡æ¯å®‰å…¨ã€‚é’ˆå¯¹ç°æœ‰AIGIæ£€æµ‹æŠ€æœ¯åœ¨å¯éªŒè¯è§£é‡Šæ€§å’Œæœ€æ–°æŠ€æœ¯æ™®åŠæ€§æ–¹é¢çš„é—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†å¤§è§„æ¨¡ç»¼åˆæ•°æ®é›†Holmes-Setï¼ŒåŒ…æ‹¬å¸¦è§£é‡Šçš„æŒ‡ä»¤è°ƒæ•´æ•°æ®é›†Holmes-SFTSetå’Œäººç±»å¯¹é½åå¥½æ•°æ®é›†Holmes-DPOSetã€‚æˆ‘ä»¬æå‡ºä¸€ç§é«˜æ•ˆçš„æ•°æ®æ ‡æ³¨æ–¹æ³•â€”â€”å¤šå…ƒä¸“å®¶é™ªå®¡å›¢ï¼Œé€šè¿‡ç»“æ„åŒ–MLLMè§£é‡Šå’Œè´¨é‡æ§åˆ¶å¢å¼ºæ•°æ®ç”Ÿæˆï¼ŒåŒ…æ‹¬è·¨æ¨¡å‹è¯„ä¼°ã€ä¸“å®¶ç¼ºé™·è¿‡æ»¤å’Œäººç±»åå¥½ä¿®æ­£ã€‚æˆ‘ä»¬è¿˜è®¾è®¡äº†ç²¾å¿ƒè®¾è®¡çš„ä¸‰é˜¶æ®µè®­ç»ƒæ¡†æ¶Holmes Pipelineï¼ŒåŒ…æ‹¬è§†è§‰ä¸“å®¶é¢„è®­ç»ƒã€ç›‘ç£å¾®è°ƒä»¥åŠç›´æ¥åå¥½ä¼˜åŒ–ç­‰ã€‚æˆ‘ä»¬çš„æ¨¡å‹AIGI-Holmesåœ¨ä¸‰ä¸ªåŸºå‡†æµ‹è¯•ä¸Šè¿›è¡Œäº†å¹¿æ³›å®éªŒéªŒè¯ï¼Œå®ç°äº†äººç±»å¯éªŒè¯å’Œç¬¦åˆäººç±»åå¥½çš„è§£é‡Šã€‚åœ¨æ¨ç†é˜¶æ®µï¼Œæˆ‘ä»¬é‡‡ç”¨ååŒè§£ç ç­–ç•¥ï¼Œå°†è§†è§‰ä¸“å®¶çš„æ„ŸçŸ¥ä¸MLLMçš„è¯­ä¹‰æ¨ç†ç›¸ç»“åˆï¼Œè¿›ä¸€æ­¥æé«˜æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>AI-generated content (AIGC)æŠ€æœ¯çš„å¿«é€Ÿå‘å±•å¼•å‘å…¬ä¼—ä¿¡æ¯å®‰å…¨å¨èƒï¼Œä¸»è¦å› ä¸ºAI-generated images (AIGI)è¢«æ»¥ç”¨ä»¥ä¼ æ’­é”™è¯¯ä¿¡æ¯ã€‚</li>
<li>ç°æœ‰AIGIæ£€æµ‹æŠ€æœ¯è™½ä¸€èˆ¬æœ‰æ•ˆï¼Œä½†ç¼ºä¹äººç±»å¯éªŒè¯çš„è§£é‡Šæ€§å’Œæœ€æ–°æŠ€æœ¯çš„æ™®åŠæ€§ã€‚</li>
<li>å¼•å…¥å¤§è§„æ¨¡ç»¼åˆæ•°æ®é›†Holmes-Setï¼ŒåŒ…æ‹¬å¸¦è§£é‡Šçš„æŒ‡ä»¤è°ƒæ•´æ•°æ®é›†å’Œäººç±»å¯¹é½åå¥½æ•°æ®é›†ï¼Œä»¥æ”¹å–„ä¸Šè¿°é—®é¢˜ã€‚</li>
<li>æå‡ºé«˜æ•ˆæ•°æ®æ ‡æ³¨æ–¹æ³•â€”â€”å¤šå…ƒä¸“å®¶é™ªå®¡å›¢ï¼Œé€šè¿‡ç»“æ„åŒ–MLLMè§£é‡Šå’Œè´¨é‡æ§åˆ¶å¢å¼ºæ•°æ®ç”Ÿæˆã€‚</li>
<li>è®¾è®¡ä¸‰é˜¶æ®µè®­ç»ƒæ¡†æ¶Holmes Pipelineï¼ŒåŒ…æ‹¬è§†è§‰ä¸“å®¶é¢„è®­ç»ƒã€ç›‘ç£å¾®è°ƒä»¥åŠç›´æ¥åå¥½ä¼˜åŒ–ç­‰ï¼Œä»¥è®­ç»ƒAIæ¨¡å‹è¿›è¡ŒAIGIæ£€æµ‹ã€‚</li>
<li>åœ¨æ¨ç†é˜¶æ®µé‡‡ç”¨ååŒè§£ç ç­–ç•¥ï¼Œç»“åˆè§†è§‰ä¸“å®¶æ„ŸçŸ¥ä¸MLLMè¯­ä¹‰æ¨ç†ï¼Œæé«˜æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.02664">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-4e80e266fadd191fd678cb20a98c7f6f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-36a2d26239472c644959493de853681e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-555ab8b2cdca9007b241b61c16d388c2.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-42a0f3ca8b6587a61ff2f5bad2640090.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-cd7ea3ea1f82521b7ce150fefa86af9e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-59e647b3c4d7e56df48c7b62475a98fc.jpg" align="middle">
</details>


<h1 id="-19"><a href="#-19" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-07-09/LLM/">https://kedreamix.github.io/Talk2Paper/Paper/2025-07-09/LLM/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/LLM/">
                                    <span class="chip bg-color">LLM</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-07-09/Agent/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-4c5851f0c5fba71136fdaee90cf924fd.jpg" class="responsive-img" alt="Agent">
                        
                        <span class="card-title">Agent</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Agent æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-07-09  Modeling Latent Partner Strategies for Adaptive Zero-Shot Human-Agent   Collaboration
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-07-09
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Agent/" class="post-category">
                                    Agent
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Agent/">
                        <span class="chip bg-color">Agent</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-07-09/R1_Reasoning/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-5079c80e2fc378a16c514f6a221b54e7.jpg" class="responsive-img" alt="R1_Reasoning">
                        
                        <span class="card-title">R1_Reasoning</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            R1_Reasoning æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-07-09  Spatio-Temporal LLM Reasoning about Environments and Actions
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-07-09
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/R1-Reasoning/" class="post-category">
                                    R1_Reasoning
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/R1-Reasoning/">
                        <span class="chip bg-color">R1_Reasoning</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">32714.1k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
