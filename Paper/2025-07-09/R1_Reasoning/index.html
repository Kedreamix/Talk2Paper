<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="R1_Reasoning">
    <meta name="description" content="R1_Reasoning æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-07-09  Spatio-Temporal LLM Reasoning about Environments and Actions">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>R1_Reasoning | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-5079c80e2fc378a16c514f6a221b54e7.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">R1_Reasoning</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/R1-Reasoning/">
                                <span class="chip bg-color">R1_Reasoning</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/R1-Reasoning/" class="post-category">
                                R1_Reasoning
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-07-09
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-07-13
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    19.8k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    80 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-07-09-æ›´æ–°"><a href="#2025-07-09-æ›´æ–°" class="headerlink" title="2025-07-09 æ›´æ–°"></a>2025-07-09 æ›´æ–°</h1><h2 id="Spatio-Temporal-LLM-Reasoning-about-Environments-and-Actions"><a href="#Spatio-Temporal-LLM-Reasoning-about-Environments-and-Actions" class="headerlink" title="Spatio-Temporal LLM: Reasoning about Environments and Actions"></a>Spatio-Temporal LLM: Reasoning about Environments and Actions</h2><p><strong>Authors:Haozhen Zheng, Beitong Tian, Mingyuan Wu, Zhenggang Tang, Klara Nahrstedt, Alex Schwing</strong></p>
<p>Despite the significant recent progress of Multimodal Large Language Models (MLLMs), MLLMs still struggle to correctly answer prompts that require a holistic spatio-temporal understanding. Specifically, it is challenging to address prompts that refer to 1) the entirety of an environment that an agent equipped with an MLLM can operate in; and simultaneously also refer to 2) recent actions that just happened and are encoded in a video clip. However, such a holistic spatio-temporal understanding is important for agents operating in the real world. To address this issue, we first develop a framework to collect a large-scale dataset. Using the collected â€œReasoning about Environments and Actionsâ€ (REA) dataset, we show that recent methods indeed struggle to correctly answer the prompts. To improve, we develop a â€œspatio-temporal LLMâ€ (ST-LLM), a model equipped with projectors to improve both spatial understanding of an environment and temporal understanding of recent observations. On the collected REA data, we show that the proposed method significantly improves results compared to prior work. Code and data are available at <a target="_blank" rel="noopener" href="https://zoezheng126.github.io/STLLM-website/">https://zoezheng126.github.io/STLLM-website/</a>. </p>
<blockquote>
<p>å°½ç®¡æœ€è¿‘å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰å–å¾—äº†é‡å¤§è¿›å±•ï¼Œä½†åœ¨éœ€è¦å…¨é¢æ—¶ç©ºç†è§£çš„ä»»åŠ¡æç¤ºä¸­ï¼ŒMLLMsä»ç„¶éš¾ä»¥æ­£ç¡®å›ç­”é—®é¢˜ã€‚å…·ä½“æ¥è¯´ï¼Œè§£å†³æ¶‰åŠä»¥ä¸‹ä¸¤ä¸ªæ–¹é¢çš„æç¤ºå…·æœ‰æŒ‘æˆ˜æ€§ï¼š1ï¼‰MLLMsæ‰€åœ¨ä»£ç†å¯ä»¥æ“ä½œçš„æ•´ä½“ç¯å¢ƒï¼›åŒæ—¶æ¶‰åŠ2ï¼‰åˆšåˆšå‘ç”Ÿå¹¶è¢«ç¼–ç åœ¨è§†é¢‘å‰ªè¾‘ä¸­çš„æœ€è¿‘åŠ¨ä½œã€‚ç„¶è€Œï¼Œè¿™ç§å…¨é¢çš„æ—¶ç©ºç†è§£å¯¹äºåœ¨ç°å®ä¸–ç•Œä¸­æ“ä½œçš„ä»£ç†å¾ˆé‡è¦ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬é¦–å…ˆå¼€å‘äº†ä¸€ä¸ªæ¡†æ¶æ¥æ”¶é›†å¤§è§„æ¨¡æ•°æ®é›†ã€‚â€œå…³äºç¯å¢ƒå’Œè¡ŒåŠ¨çš„æ€è€ƒâ€ï¼ˆREAï¼‰æ•°æ®é›†æ˜¾ç¤ºï¼Œæœ€è¿‘çš„æ–¹æ³•ç¡®å®éš¾ä»¥æ­£ç¡®å›ç­”æç¤ºã€‚ä¸ºäº†æ”¹è¿›ï¼Œæˆ‘ä»¬å¼€å‘äº†ä¸€ç§â€œæ—¶ç©ºLLMâ€ï¼ˆST-LLMï¼‰ï¼Œè¿™æ˜¯ä¸€ç§é…å¤‡æŠ•å½±ä»ªçš„æ¨¡å‹ï¼Œå¯ä»¥æé«˜å¯¹ç¯å¢ƒçš„ç©ºé—´ç†è§£å’Œæœ€è¿‘è§‚å¯Ÿçš„æ—¶é—´ç†è§£ã€‚åœ¨æ”¶é›†çš„REAæ•°æ®ä¸Šï¼Œæˆ‘ä»¬è¯æ˜æ‰€æå‡ºçš„æ–¹æ³•ä¸å…ˆå‰çš„å·¥ä½œç›¸æ¯”ï¼Œç»“æœæ˜¾è‘—æ”¹è¿›ã€‚ä»£ç å’Œæ•°æ®å¯åœ¨ <a target="_blank" rel="noopener" href="https://zoezheng126.github.io/STLLM-website/">https://zoezheng126.github.io/STLLM-website/</a> æ‰¾åˆ°ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.05258v1">PDF</a> Code and data are available at   <a target="_blank" rel="noopener" href="https://zoezheng126.github.io/STLLM-website/">https://zoezheng126.github.io/STLLM-website/</a></p>
<p><strong>Summary</strong></p>
<p>æ–‡ç« ä»‹ç»äº†å°½ç®¡æœ€è¿‘çš„å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰å–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œä½†åœ¨åº”å¯¹éœ€è¦å…¨é¢æ—¶ç©ºç†è§£èƒ½åŠ›çš„æç¤ºæ—¶ä»é¢ä¸´æŒ‘æˆ˜ã€‚é’ˆå¯¹è¿™ç§æƒ…å†µï¼Œç ”ç©¶äººå‘˜å¼€å‘äº†ä¸€ä¸ªæ¡†æ¶æ¥æ”¶é›†å¤§è§„æ¨¡æ•°æ®é›†ï¼Œå³â€œç¯å¢ƒä¸è¡Œä¸ºæ¨ç†â€ï¼ˆREAï¼‰æ•°æ®é›†ï¼Œå¹¶åœ¨æ­¤åŸºç¡€ä¸Šæå‡ºäº†ä¸€ä¸ªåä¸ºâ€œæ—¶ç©ºLLMâ€ï¼ˆST-LLMï¼‰çš„æ¨¡å‹æ¥æ”¹å–„ç¯å¢ƒå’Œæœ€æ–°è§‚å¯Ÿçš„æ—¶åºç†è§£é—®é¢˜ã€‚ç ”ç©¶ç»“æœæ˜¾ç¤ºï¼Œè¯¥æ–¹æ³•æ˜¾è‘—æé«˜äº†ä¸ä¹‹å‰å·¥ä½œçš„ç»“æœå¯¹æ¯”ã€‚ä»£ç å’Œæ•°æ®å¯ä»¥åœ¨ç½‘ä¸Šæ‰¾åˆ°ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>MLLMsåœ¨åº”å¯¹éœ€è¦å…¨é¢æ—¶ç©ºç†è§£çš„æç¤ºæ—¶å­˜åœ¨æŒ‘æˆ˜ã€‚</li>
<li>ç¯å¢ƒå’Œæœ€æ–°è¡Œä¸ºçš„æ—¶ç©ºç†è§£å¯¹ç°å®ä¸–ç•Œçš„æ™ºèƒ½ä½“æ“ä½œè‡³å…³é‡è¦ã€‚</li>
<li>ç ”ç©¶äººå‘˜é€šè¿‡å¼€å‘æ¡†æ¶åˆ›å»ºäº†å¤§è§„æ¨¡çš„â€œç¯å¢ƒä¸è¡Œä¸ºæ¨ç†â€ï¼ˆREAï¼‰æ•°æ®é›†ã€‚</li>
<li>åœ¨REAæ•°æ®é›†ä¸Šï¼Œç°æœ‰æ–¹æ³•çš„è¡¨ç°å¹¶ä¸ç†æƒ³ã€‚</li>
<li>ä¸ºäº†æ”¹å–„è¿™ç§æƒ…å†µï¼Œæå‡ºäº†â€œæ—¶ç©ºLLMâ€ï¼ˆST-LLMï¼‰æ¨¡å‹ï¼Œè¯¥æ¨¡å‹é…å¤‡äº†æŠ•å½±ä»ªä»¥æé«˜å¯¹ç¯å¢ƒå’Œæœ€æ–°è§‚å¯Ÿçš„ç†è§£ã€‚</li>
<li>åœ¨REAæ•°æ®é›†ä¸Šè¿›è¡Œçš„å®éªŒè¡¨æ˜ï¼ŒST-LLMæ¨¡å‹çš„ç»“æœæ˜¾è‘—ä¼˜äºå…ˆå‰çš„å·¥ä½œã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.05258">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-12fc8033021b2f9a799773b5ce635297.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5079c80e2fc378a16c514f6a221b54e7.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-a650cc85a7e6ee4d23c6d0569fd3ee3e.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-c679d5950bd295f99db3a237199c3560.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-401b55310f0d5968788f7a3160e0a2e6.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Open-Vision-Reasoner-Transferring-Linguistic-Cognitive-Behavior-for-Visual-Reasoning"><a href="#Open-Vision-Reasoner-Transferring-Linguistic-Cognitive-Behavior-for-Visual-Reasoning" class="headerlink" title="Open Vision Reasoner: Transferring Linguistic Cognitive Behavior for   Visual Reasoning"></a>Open Vision Reasoner: Transferring Linguistic Cognitive Behavior for   Visual Reasoning</h2><p><strong>Authors:Yana Wei, Liang Zhao, Jianjian Sun, Kangheng Lin, Jisheng Yin, Jingcheng Hu, Yinmin Zhang, En Yu, Haoran Lv, Zejia Weng, Jia Wang, Chunrui Han, Yuang Peng, Qi Han, Zheng Ge, Xiangyu Zhang, Daxin Jiang, Vishal M. Patel</strong></p>
<p>The remarkable reasoning capability of large language models (LLMs) stems from cognitive behaviors that emerge through reinforcement with verifiable rewards. This work investigates how to transfer this principle to Multimodal LLMs (MLLMs) to unlock advanced visual reasoning. We introduce a two-stage paradigm built on Qwen2.5-VL-7B: a massive linguistic cold-start fine-tuning, followed by multimodal reinforcement learning (RL) spanning nearly 1,000 steps, surpassing all previous open-source efforts in scale. This pioneering work reveals three fundamental insights: 1) Behavior transfer emerges surprisingly early in cold start due to linguistic mental imagery. 2) Cold start broadly memorizes visual behaviors, while RL critically discerns and scales up effective patterns. 3) Transfer strategically favors high-utility behaviors such as visual reflection. Our resulting model, Open-Vision-Reasoner (OVR), achieves state-of-the-art performance on a suite of reasoning benchmarks, including 95.3% on MATH500, 51.8% on MathVision and 54.6% on MathVerse. We release our model, data, and training dynamics to catalyze the development of more capable, behavior-aligned multimodal reasoners. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹çš„å“è¶Šæ¨ç†èƒ½åŠ›æ¥æºäºé€šè¿‡å¯éªŒè¯çš„å¥–åŠ±è¿›è¡Œå¼ºåŒ–åå‡ºç°çš„è®¤çŸ¥è¡Œä¸ºã€‚æœ¬ç ”ç©¶æ—¨åœ¨æ¢ç´¢å¦‚ä½•å°†è¿™ä¸€åŸç†åº”ç”¨äºå¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼Œä»¥è§£é”é«˜çº§è§†è§‰æ¨ç†èƒ½åŠ›ã€‚æˆ‘ä»¬åŸºäºQwen2.5-VL-7Bå¼•å…¥äº†ä¸€ä¸ªä¸¤é˜¶æ®µçš„èŒƒå¼ï¼šé¦–å…ˆæ˜¯å¤§è§„æ¨¡è¯­è¨€å†·å¯åŠ¨å¾®è°ƒï¼Œå…¶æ¬¡æ˜¯æ¶µç›–è¿‘1000æ­¥çš„å¤šæ¨¡æ€å¼ºåŒ–å­¦ä¹ ã€‚è¿™é¡¹å·¥ä½œè§„æ¨¡è¶…è¿‡äº†ä¹‹å‰æ‰€æœ‰å¼€æºå·¥ä½œçš„è§„æ¨¡ã€‚è¿™ä¸€å¼€åˆ›æ€§ç ”ç©¶æ­ç¤ºäº†ä¸‰ä¸ªåŸºæœ¬è§‚ç‚¹ï¼š1ï¼‰ç”±äºè¯­è¨€å¿ƒç†å›¾åƒï¼Œè¡Œä¸ºè½¬ç§»åœ¨å†·å¯åŠ¨çš„æ—©æœŸé˜¶æ®µå°±ä¼šå‡ºç°ï¼›2ï¼‰å†·å¯åŠ¨å¹¿æ³›è®°å¿†è§†è§‰è¡Œä¸ºï¼Œè€Œå¼ºåŒ–å­¦ä¹ åˆ™èƒ½è¾¨åˆ«å¹¶æ‰©å¤§æœ‰æ•ˆæ¨¡å¼ï¼›3ï¼‰è½¬ç§»ç­–ç•¥æœ‰åˆ©äºé«˜å®ç”¨æ€§çš„è¡Œä¸ºï¼Œå¦‚è§†è§‰åå°„ã€‚æˆ‘ä»¬å¾—åˆ°çš„æ¨¡å‹â€”â€”Open-Vision-Reasonerï¼ˆOVRï¼‰åœ¨ä¸€ç³»åˆ—æ¨ç†åŸºå‡†æµ‹è¯•ä¸­è¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼ŒåŒ…æ‹¬MATH500ä¸Šçš„95.3%ï¼ŒMathVisionä¸Šçš„51.8%å’ŒMathVerseä¸Šçš„54.6%ã€‚æˆ‘ä»¬å‘å¸ƒæˆ‘ä»¬çš„æ¨¡å‹ã€æ•°æ®å’Œè®­ç»ƒåŠ¨æ€ï¼Œä»¥æ¨åŠ¨å¼€å‘æ›´å…·èƒ½åŠ›ã€æ›´ç¬¦åˆè¡Œä¸ºçš„å¤šæ¨¡æ€æ¨ç†å™¨ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.05255v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹çš„å“è¶Šæ¨ç†èƒ½åŠ›æ¥æºäºé€šè¿‡å¯éªŒè¯çš„å¥–åŠ±è¿›è¡Œå¼ºåŒ–åæ‰€å±•ç°çš„è®¤çŸ¥è¡Œä¸ºã€‚æœ¬ç ”ç©¶æ¢ç´¢å¦‚ä½•å°†è¿™ä¸€åŸåˆ™åº”ç”¨äºå¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼Œä»¥è§£é”é«˜çº§è§†è§‰æ¨ç†èƒ½åŠ›ã€‚æœ¬ç ”ç©¶å¼•å…¥äº†ä¸€ä¸ªåŸºäºQwen2.5-VL-7Bçš„ä¸¤é˜¶æ®µèŒƒå¼ï¼Œå…ˆè¿›è¡Œå¤§è§„æ¨¡è¯­è¨€å†·å¯åŠ¨å¾®è°ƒï¼Œç„¶åè¿›è¡Œè¿‘1000æ­¥çš„å¤šæ¨¡æ€å¼ºåŒ–å­¦ä¹ ã€‚æœ¬ç ”ç©¶æ­ç¤ºäº†ä¸‰ä¸ªå…³é”®è§è§£ï¼Œå¹¶æˆåŠŸå¼€å‘å‡ºä¸€ä¸ªåä¸ºOpen-Vision-Reasonerï¼ˆOVRï¼‰çš„æ¨¡å‹ï¼Œè¯¥æ¨¡å‹åœ¨å¤šä¸ªæ¨ç†åŸºå‡†æµ‹è¯•ä¸Šè¾¾åˆ°æœ€æ–°æ°´å¹³ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹çš„æ¨ç†èƒ½åŠ›æ¥æºäºé€šè¿‡å¯éªŒè¯å¥–åŠ±è¿›è¡Œå¼ºåŒ–åæ‰€å±•ç°çš„è®¤çŸ¥è¡Œä¸ºã€‚</li>
<li>é€šè¿‡ä¸¤é˜¶æ®µèŒƒå¼ï¼ˆåŸºäºQwen2.5-VL-7Bçš„å†·å¯åŠ¨å¾®è°ƒå’Œå¤šæ¨¡æ€å¼ºåŒ–å­¦ä¹ ï¼‰å°†å¼ºåŒ–å­¦ä¹ åŸåˆ™åº”ç”¨äºå¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼Œä»¥è§£é”é«˜çº§è§†è§‰æ¨ç†èƒ½åŠ›ã€‚</li>
<li>è¡Œä¸ºè½¬ç§»åœ¨å†·å¯åŠ¨é˜¶æ®µç”±äºè¯­è¨€å¿ƒç†å›¾åƒè€Œæ„å¤–åœ°æ—©æœŸå‡ºç°ã€‚</li>
<li>å†·å¯åŠ¨å¹¿æ³›è®°å¿†è§†è§‰è¡Œä¸ºï¼Œè€Œå¼ºåŒ–å­¦ä¹ åˆ™å…³é”®åœ°åŒºåˆ†å’Œæ‰©å¤§æœ‰æ•ˆæ¨¡å¼ã€‚</li>
<li>è½¬ç§»ç­–ç•¥æœ‰åˆ©äºé«˜å®ç”¨æ€§çš„è¡Œä¸ºï¼Œå¦‚è§†è§‰åå°„ã€‚</li>
<li>å¼€å‘çš„Open-Vision-Reasonerï¼ˆOVRï¼‰æ¨¡å‹åœ¨å¤šä¸ªæ¨ç†åŸºå‡†æµ‹è¯•ä¸Šè¾¾åˆ°æœ€æ–°æ°´å¹³ï¼ŒåŒ…æ‹¬MATH500çš„95.3%ã€MathVisionçš„51.8%å’ŒMathVerseçš„54.6%ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.05255">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-fc49516c89e3d0567aface1442a2f7be.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-fd80b4e4108801b3ea459af6851d736d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2a58e9c1a7abda30e20e5f0290049619.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-fcf2a13672256444a1608d5467af1585.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Supported-Abstract-Argumentation-for-Case-Based-Reasoning"><a href="#Supported-Abstract-Argumentation-for-Case-Based-Reasoning" class="headerlink" title="Supported Abstract Argumentation for Case-Based Reasoning"></a>Supported Abstract Argumentation for Case-Based Reasoning</h2><p><strong>Authors:Adam Gould, Gabriel de Olim Gaul, Francesca Toni</strong></p>
<p>We introduce Supported Abstract Argumentation for Case-Based Reasoning (sAA-CBR), a binary classification model in which past cases engage in debates by arguing in favour of their labelling and attacking or supporting those with opposing or agreeing labels. With supports, sAA-CBR overcomes the limitation of its precursor AA-CBR, which can contain extraneous cases (or spikes) that are not included in the debates. We prove that sAA-CBR contains no spikes, without trading off key model properties </p>
<blockquote>
<p>æˆ‘ä»¬ä»‹ç»äº†åŸºäºæ¡ˆä¾‹æ¨ç†çš„æ”¯æŒæŠ½è±¡è®ºè¯ï¼ˆsAA-CBRï¼‰æ–¹æ³•ï¼Œè¿™æ˜¯ä¸€ç§äºŒåˆ†ç±»æ¨¡å‹ï¼Œè¿‡å»æ¡ˆä¾‹ä¸­ä¼šå±•å¼€è¾©è®ºæ¥ä¸ºè‡ªèº«æ ‡ç­¾è¾©æŠ¤ï¼Œå¹¶æ”»å‡»æˆ–æ”¯æŒé‚£äº›æŒæœ‰åå¯¹æˆ–ç›¸åŒæ ‡ç­¾çš„æ¡ˆä¾‹ã€‚æœ‰äº†è¿™äº›æ”¯æŒï¼ŒsAA-CBRå…‹æœäº†å…¶å‰èº«AA-CBRçš„å±€é™æ€§ï¼Œåè€…å¯èƒ½åŒ…å«æœªå‚ä¸è¾©è®ºçš„é¢å¤–æ¡ˆä¾‹ï¼ˆæˆ–å³°å€¼ï¼‰ã€‚æˆ‘ä»¬è¯æ˜sAA-CBRä¸åŒ…å«å³°å€¼ï¼Œä¸ä¼šç‰ºç‰²å…³é”®æ¨¡å‹å±æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.04994v1">PDF</a> Accepted to IARML@ICJAI2025: Workshop on the Interactions between   Analogical Reasoning and Machine Learning</p>
<p><strong>Summary</strong>ï¼šä»‹ç»äº†åŸºäºæ¡ˆä¾‹æ¨ç†çš„æ”¯æŒæŠ½è±¡è®ºè¯ï¼ˆsAA-CBRï¼‰äºŒå…ƒåˆ†ç±»æ¨¡å‹ï¼Œå®ƒé€šè¿‡æ”¯æŒè¿‡å»çš„æ¡ˆä¾‹è¾©è®ºï¼Œå¯¹æ ‡ç­¾è¿›è¡Œè®ºè¯æˆ–æ”»å‡»æˆ–æ”¯æŒé‚£äº›å…·æœ‰å¯¹ç«‹æˆ–ç›¸åŒæ ‡ç­¾çš„æ¡ˆä¾‹ã€‚ç›¸è¾ƒäºAA-CBRæ¨¡å‹ï¼ŒsAA-CBRé€šè¿‡æ”¯æŒå…‹æœäº†åŒ…å«é¢å¤–æ¡ˆä¾‹ï¼ˆæˆ–å³°å€¼ï¼‰çš„å±€é™æ€§ï¼ŒåŒæ—¶è¯æ˜å…¶ä¸åŒ…å«å³°å€¼ä¸”ä¸ä¼šç‰ºç‰²å…³é”®æ¨¡å‹å±æ€§ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>sAA-CBRæ˜¯ä¸€ç§äºŒå…ƒåˆ†ç±»æ¨¡å‹ï¼Œåˆ©ç”¨è¿‡å»çš„æ¡ˆä¾‹è¿›è¡Œè¾©è®ºã€‚</li>
<li>å®ƒé€šè¿‡æ”¯æŒæˆ–æ”»å‡»æ¡ˆä¾‹æ ‡ç­¾æ¥è¿›è¡Œåˆ†ç±»ã€‚</li>
<li>sAA-CBRå…‹æœäº†AA-CBRæ¨¡å‹çš„å±€é™æ€§ï¼Œå³åŒ…å«é¢å¤–æ¡ˆä¾‹ï¼ˆæˆ–å³°å€¼ï¼‰ã€‚</li>
<li>sAA-CBRåœ¨ä¿è¯æ¨¡å‹å…³é”®å±æ€§çš„æƒ…å†µä¸‹æ¶ˆé™¤äº†å³°å€¼é—®é¢˜ã€‚</li>
<li>è¯¥æ¨¡å‹é€šè¿‡å¯¹è¾©è®ºçš„æ”¯æŒæ¥æ”¹å–„æ¡ˆä¾‹çš„æ ‡æ³¨è¿‡ç¨‹ã€‚</li>
<li>sAA-CBRèƒ½å¤Ÿæ›´æœ‰æ•ˆåœ°å¤„ç†åŒ…å«å¯¹ç«‹æˆ–ç›¸åŒæ ‡ç­¾çš„æ¡ˆä¾‹ä¹‹é—´çš„è¾©è®ºã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.04994">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-b01313a514d62dd793163c61790e10c7.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-1c5a4fba7bf37973aef4ec3ced9daba9.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Can-Video-LLMs-Refuse-to-Answer-Alignment-for-Answerability-in-Video-Large-Language-Models"><a href="#Can-Video-LLMs-Refuse-to-Answer-Alignment-for-Answerability-in-Video-Large-Language-Models" class="headerlink" title="Can Video LLMs Refuse to Answer? Alignment for Answerability in Video   Large Language Models"></a>Can Video LLMs Refuse to Answer? Alignment for Answerability in Video   Large Language Models</h2><p><strong>Authors:Eunseop Yoon, Hee Suk Yoon, Mark A. Hasegawa-Johnson, Chang D. Yoo</strong></p>
<p>In the broader context of deep learning, Multimodal Large Language Models have achieved significant breakthroughs by leveraging powerful Large Language Models as a backbone to align different modalities into the language space. A prime exemplification is the development of Video Large Language Models (Video-LLMs). While numerous advancements have been proposed to enhance the video understanding capabilities of these models, they are predominantly trained on questions generated directly from video content. However, in real-world scenarios, users often pose questions that extend beyond the informational scope of the video, highlighting the need for Video-LLMs to assess the relevance of the question. We demonstrate that even the best-performing Video-LLMs fail to reject unfit questions-not necessarily due to a lack of video understanding, but because they have not been trained to identify and refuse such questions. To address this limitation, we propose alignment for answerability, a framework that equips Video-LLMs with the ability to evaluate the relevance of a question based on the input video and appropriately decline to answer when the question exceeds the scope of the video, as well as an evaluation framework with a comprehensive set of metrics designed to measure model behavior before and after alignment. Furthermore, we present a pipeline for creating a dataset specifically tailored for alignment for answerability, leveraging existing video-description paired datasets. </p>
<blockquote>
<p>åœ¨æ·±åº¦å­¦ä¹ çš„å¤§èƒŒæ™¯ä¸‹ï¼Œå¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹é€šè¿‡åˆ©ç”¨å¼ºå¤§çš„å¤§å‹è¯­è¨€æ¨¡å‹ä½œä¸ºéª¨å¹²ï¼Œå°†ä¸åŒæ¨¡æ€å¯¹é½åˆ°è¯­è¨€ç©ºé—´ï¼Œå–å¾—äº†é‡å¤§çªç ´ã€‚ä¸€ä¸ªå…¸å‹çš„ä¾‹å­æ˜¯è§†é¢‘å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆVideo-LLMsï¼‰çš„å‘å±•ã€‚è™½ç„¶å·²æå‡ºäº†è®¸å¤šæ”¹è¿›æ–¹æ¡ˆæ¥æé«˜è¿™äº›æ¨¡å‹å¯¹è§†é¢‘çš„ç†è§£èƒ½åŠ›ï¼Œä½†å®ƒä»¬ä¸»è¦è®­ç»ƒäºç›´æ¥ä»è§†é¢‘å†…å®¹ç”Ÿæˆçš„é—®é¢˜ã€‚ç„¶è€Œï¼Œåœ¨ç°å®åœºæ™¯ä¸­ï¼Œç”¨æˆ·å¾€å¾€æå‡ºçš„é—®é¢˜è¶…å‡ºäº†è§†é¢‘çš„ä¿¡æ¯èŒƒå›´ï¼Œè¿™å‡¸æ˜¾å‡ºè§†é¢‘LLMéœ€è¦è¯„ä¼°é—®é¢˜çš„ç›¸å…³æ€§çš„éœ€æ±‚ã€‚æˆ‘ä»¬è¯æ˜ï¼Œå³ä½¿è¡¨ç°æœ€å¥½çš„è§†é¢‘LLMä¹Ÿæ— æ³•æ‹’ç»ä¸åˆé€‚çš„é—®é¢˜â€”â€”è¿™å¹¶ä¸ä¸€å®šæ˜¯å› ä¸ºç¼ºä¹è§†é¢‘ç†è§£ï¼Œè€Œæ˜¯å› ä¸ºå®ƒä»¬æ²¡æœ‰ç»è¿‡è®­ç»ƒæ¥è¯†åˆ«å’Œæ‹’ç»è¿™æ ·çš„é—®é¢˜ã€‚ä¸ºäº†è§£å†³è¿™ä¸€å±€é™æ€§ï¼Œæˆ‘ä»¬æå‡ºäº†é’ˆå¯¹å›ç­”æ€§çš„å¯¹é½æ–¹æ³•ï¼Œè¯¥æ–¹æ³•ä¸ºè§†é¢‘LLMæä¾›äº†åŸºäºè¾“å…¥è§†é¢‘è¯„ä¼°é—®é¢˜ç›¸å…³æ€§çš„èƒ½åŠ›ï¼Œå¹¶åœ¨é—®é¢˜è¶…å‡ºè§†é¢‘èŒƒå›´æ—¶é€‚å½“åœ°æ‹’ç»å›ç­”ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜æå‡ºäº†ä¸€ä¸ªè¯„ä¼°æ¡†æ¶ï¼Œè¯¥æ¡†æ¶åŒ…å«ä¸€ç³»åˆ—æŒ‡æ ‡ï¼Œæ—¨åœ¨è¡¡é‡æ¨¡å‹åœ¨å¯¹é½å‰åçš„è¡Œä¸ºã€‚æˆ‘ä»¬è¿˜æå‡ºäº†ä¸€ä¸ªä¸“é—¨é’ˆå¯¹å¯¹é½å›ç­”èƒ½åŠ›çš„æ•°æ®é›†åˆ›å»ºç®¡é“ï¼Œåˆ©ç”¨ç°æœ‰çš„è§†é¢‘æè¿°é…å¯¹æ•°æ®é›†ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.04976v1">PDF</a> ICLR 2025</p>
<p><strong>Summary</strong><br>å¤šåª’ä½“å¤§å‹è¯­è¨€æ¨¡å‹é€šè¿‡å¯¹ä¸åŒæ¨¡æ€è¿›è¡Œè¯­è¨€ç©ºé—´å¯¹é½ï¼Œåœ¨æ·±åº¦å­¦ä¹ é¢†åŸŸå–å¾—äº†é‡å¤§çªç ´ã€‚ä»¥è§†é¢‘å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆVideo-LLMï¼‰ä¸ºä¾‹ï¼Œè™½ç„¶å·²æœ‰è®¸å¤šæé«˜å…¶è§†é¢‘ç†è§£èƒ½åŠ›çš„æè®®ï¼Œä½†å®ƒä»¬ä¸»è¦è®­ç»ƒäºç›´æ¥ä»è§†é¢‘å†…å®¹ç”Ÿæˆçš„é—®é¢˜ã€‚ç„¶è€Œï¼Œåœ¨ç°å®åœºæ™¯ä¸­ï¼Œç”¨æˆ·æå‡ºçš„é—®é¢˜å¾€å¾€è¶…å‡ºè§†é¢‘ä¿¡æ¯èŒƒå›´ï¼Œéœ€è¦Video-LLMè¯„ä¼°é—®é¢˜çš„ç›¸å…³æ€§ã€‚ä¸ºè§£å†³æ­¤å±€é™ï¼Œæˆ‘ä»¬æå‡ºäº†ç­”æ¡ˆå¯¹é½æ¡†æ¶ï¼Œä½¿Video-LLMå…·å¤‡æ ¹æ®è¾“å…¥è§†é¢‘è¯„ä¼°é—®é¢˜ç›¸å…³æ€§çš„èƒ½åŠ›ï¼Œå¹¶é€‚å½“æ‹’ç»å›ç­”è¶…å‡ºè§†é¢‘èŒƒå›´çš„é—®é¢˜ã€‚åŒæ—¶ï¼Œæˆ‘ä»¬è¿˜æä¾›äº†ä¸€å¥—è¯„ä¼°æ¨¡å‹åœ¨å¯¹é½å‰åçš„è¡Œä¸ºçš„ç»¼åˆæŒ‡æ ‡ï¼Œä»¥åŠä¸€ä¸ªä¸“é—¨ç”¨äºç­”æ¡ˆå¯¹é½çš„æ•°æ®é›†åˆ›å»ºæµç¨‹ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤šåª’ä½“å¤§å‹è¯­è¨€æ¨¡å‹é€šè¿‡è¯­è¨€ç©ºé—´å¯¹é½ä¸åŒæ¨¡æ€ï¼Œå®ç°æ·±åº¦å­¦ä¹ é¢†åŸŸçš„çªç ´ã€‚</li>
<li>Video-LLMè™½èƒ½æé«˜è§†é¢‘ç†è§£èƒ½åŠ›ï¼Œä½†ä¸»è¦è®­ç»ƒäºè§†é¢‘å†…å®¹ç”Ÿæˆçš„é—®é¢˜ã€‚</li>
<li>ç°å®åœºæ™¯ä¸­ï¼Œç”¨æˆ·é—®é¢˜å¾€å¾€è¶…å‡ºè§†é¢‘ä¿¡æ¯èŒƒå›´ï¼Œéœ€è¦Video-LLMè¯„ä¼°é—®é¢˜ç›¸å…³æ€§ã€‚</li>
<li>æå‡ºçš„ç­”æ¡ˆå¯¹é½æ¡†æ¶ä½¿Video-LLMå…·å¤‡è¯„ä¼°é—®é¢˜ç›¸å…³æ€§çš„èƒ½åŠ›ï¼Œå¹¶é€‚å½“æ‹’ç»å›ç­”ä¸ç›¸å…³é—®é¢˜ã€‚</li>
<li>ç»¼åˆè¯„ä¼°æŒ‡æ ‡ç”¨äºè¡¡é‡æ¨¡å‹åœ¨å¯¹é½å‰åçš„è¡Œä¸ºã€‚</li>
<li>åˆ›å»ºä¸€ä¸ªä¸“é—¨ç”¨äºç­”æ¡ˆå¯¹é½çš„æ•°æ®é›†æµç¨‹ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.04976">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-8ec775841787c454217fa50294f4f125.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-72ad28eaefefc0d30f4b841325c33a38.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-767e063f48fbbbee13f925173e536ab8.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-72ffac6ccffb6799890cc2c0f8dc6cef.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="MARBLE-A-Multi-Agent-Rule-Based-LLM-Reasoning-Engine-for-Accident-Severity-Prediction"><a href="#MARBLE-A-Multi-Agent-Rule-Based-LLM-Reasoning-Engine-for-Accident-Severity-Prediction" class="headerlink" title="MARBLE: A Multi-Agent Rule-Based LLM Reasoning Engine for Accident   Severity Prediction"></a>MARBLE: A Multi-Agent Rule-Based LLM Reasoning Engine for Accident   Severity Prediction</h2><p><strong>Authors:Kaleem Ullah Qasim, Jiashu Zhang</strong></p>
<p>Accident severity prediction plays a critical role in transportation safety systems but is a persistently difficult task due to incomplete data, strong feature dependencies, and severe class imbalance in which rare but high-severity cases are underrepresented and hard to detect. Existing methods often rely on monolithic models or black box prompting, which struggle to scale in noisy, real-world settings and offer limited interpretability. To address these challenges, we propose MARBLE a multiagent rule based LLM engine that decomposes the severity prediction task across a team of specialized reasoning agents, including an interchangeable ML-backed agent. Each agent focuses on a semantic subset of features (e.g., spatial, environmental, temporal), enabling scoped reasoning and modular prompting without the risk of prompt saturation. Predictions are coordinated through either rule-based or LLM-guided consensus mechanisms that account for class rarity and confidence dynamics. The system retains structured traces of agent-level reasoning and coordination outcomes, supporting in-depth interpretability and post-hoc performance diagnostics. Across both UK and US datasets, MARBLE consistently outperforms traditional machine learning classifiers and state-of-the-art (SOTA) prompt-based reasoning methods including Chain-of-Thought (CoT), Least-to-Most (L2M), and Tree-of-Thought (ToT) achieving nearly 90% accuracy where others plateau below 48%. This performance redefines the practical ceiling for accident severity classification under real world noise and extreme class imbalance. Our results position MARBLE as a generalizable and interpretable framework for reasoning under uncertainty in safety-critical applications. </p>
<blockquote>
<p>äº‹æ•…ä¸¥é‡æ€§çš„é¢„æµ‹åœ¨äº¤é€šè¿è¾“å®‰å…¨ç³»ç»Ÿä¸­èµ·ç€è‡³å…³é‡è¦çš„ä½œç”¨ï¼Œä½†è¿™æ˜¯ä¸€é¡¹æŒç»­å›°éš¾çš„ä»»åŠ¡ï¼ŒåŸå› åœ¨äºæ•°æ®ä¸å®Œæ•´ã€ç‰¹å¾ä¾èµ–æ€§å¼ºä»¥åŠç±»åˆ«ä¸¥é‡ä¸å¹³è¡¡ï¼ˆç½•è§ä½†é«˜ä¸¥é‡æ€§æ¡ˆä¾‹ä»£è¡¨æ€§ä¸è¶³ä¸”éš¾ä»¥æ£€æµ‹ï¼‰ã€‚ç°æœ‰æ–¹æ³•é€šå¸¸ä¾èµ–äºå•ä¸€æ¨¡å‹æˆ–é»‘ç®±æç¤ºï¼Œéš¾ä»¥åœ¨å˜ˆæ‚çš„ç°å®ç”Ÿæ´»ä¸­æ‰©å±•ï¼Œä¸”æä¾›æœ‰é™çš„è§£é‡Šæ€§ã€‚ä¸ºäº†åº”å¯¹è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†MARBLEï¼Œè¿™æ˜¯ä¸€ä¸ªåŸºäºå¤šæ™ºèƒ½ä½“çš„è§„åˆ™åŒ–å¤§å‹è¯­è¨€æ¨¡å‹å¼•æ“ï¼Œå®ƒå°†ä¸¥é‡æ€§é¢„æµ‹ä»»åŠ¡åˆ†è§£åˆ°ä¸€ä¸ªä¸“ä¸šæ¨ç†æ™ºèƒ½ä½“å›¢é˜Ÿä¸­ï¼ŒåŒ…æ‹¬ä¸€ä¸ªå¯äº’æ¢çš„MLæ”¯æŒæ™ºèƒ½ä½“ã€‚æ¯ä¸ªæ™ºèƒ½ä½“ä¸“æ³¨äºç‰¹å¾çš„ä¸€ä¸ªè¯­ä¹‰å­é›†ï¼ˆä¾‹å¦‚ç©ºé—´ã€ç¯å¢ƒã€æ—¶é—´ï¼‰ï¼Œèƒ½å¤Ÿè¿›è¡Œå±€éƒ¨æ¨ç†å’Œæ¨¡å—åŒ–æç¤ºï¼Œè€Œä¸ä¼šå¯¼è‡´æç¤ºé¥±å’Œçš„é£é™©ã€‚é¢„æµ‹æ˜¯é€šè¿‡åŸºäºè§„åˆ™æˆ–å¤§å‹è¯­è¨€æ¨¡å‹å¼•å¯¼çš„å…±è¯†æœºåˆ¶åè°ƒçš„ï¼Œè¿™äº›æœºåˆ¶è€ƒè™‘äº†ç±»åˆ«çš„ç¨€æœ‰æ€§å’Œä¿¡å¿ƒåŠ¨æ€ã€‚è¯¥ç³»ç»Ÿä¿ç•™äº†æ™ºèƒ½ä½“å±‚é¢æ¨ç†å’Œåè°ƒç»“æœçš„ç»“æ„åŒ–è·Ÿè¸ªï¼Œæ”¯æŒæ·±åº¦è§£é‡Šå’Œäº‹åæ€§èƒ½è¯Šæ–­ã€‚åœ¨è‹±å›½å’Œç¾å›½çš„æ•°æ®é›†ä¸Šï¼ŒMARBLEå§‹ç»ˆä¼˜äºä¼ ç»Ÿçš„æœºå™¨å­¦ä¹ åˆ†ç±»å™¨ä»¥åŠæœ€æ–°åŸºäºæç¤ºçš„æ¨ç†æ–¹æ³•ï¼ˆå¦‚Chain-of-Thought (CoT), Least-to-Most (L2M), å’Œ Tree-of-Thought (ToT)ï¼‰ï¼Œå‡†ç¡®ç‡æ¥è¿‘90%ï¼Œè€Œå…¶ä»–æ–¹æ³•ä½äº48%ã€‚è¿™ä¸€è¡¨ç°é‡æ–°å®šä¹‰äº†ç°å®ä¸–ç•Œå™ªå£°å’Œæç«¯ç±»åˆ«ä¸å¹³è¡¡æ¡ä»¶ä¸‹äº‹æ•…ä¸¥é‡æ€§åˆ†ç±»çš„å®é™…ä¸Šé™ã€‚æˆ‘ä»¬çš„ç»“æœå°†MARBLEå®šä½ä¸ºå®‰å…¨å…³é”®åº”ç”¨ä¸ç¡®å®šæ€§æ¨ç†çš„å¯æ¨å¹¿å’Œå¯è§£é‡Šæ¡†æ¶ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.04893v1">PDF</a> 13 pages, 5 figures</p>
<p><strong>Summary</strong>ï¼š<br>å¤šä¸»ä½“è§„åˆ™é©±åŠ¨çš„æ¨¡å‹MARBLEè¢«æå‡ºä»¥è§£å†³äº¤é€šäº‹æ•…ä¸¥é‡æ€§é¢„æµ‹é—®é¢˜ã€‚é€šè¿‡åˆ†è§£ä»»åŠ¡è‡³å¤šä¸ªä¸“é¡¹æ¨ç†ä¸»ä½“ï¼Œå¹¶ä½¿ç”¨ç»“æ„åŒ–æ–¹æ³•ç®¡ç†ï¼Œè¯¥æ¨¡å‹èƒ½å¤Ÿåœ¨å¤æ‚çš„ç°å®ç¯å¢ƒä¸‹ä¿æŒæ€§èƒ½ï¼Œæ˜¾è‘—æé«˜é¢„æµ‹å‡†ç¡®ç‡ï¼Œå¹¶å®ç°æ¨¡å‹å†…éƒ¨æ¨ç†çš„æ·±å…¥è§£è¯»å’Œäº‹åæ€§èƒ½è¯Šæ–­ã€‚MARBLEè¡¨ç°å‡ºå“è¶Šçš„æ€§èƒ½å’Œæ³›åŒ–èƒ½åŠ›ï¼Œé€‚ç”¨äºå®‰å…¨å…³é”®åº”ç”¨ä¸­çš„ä¸ç¡®å®šæ€§æ¨ç†ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>MARBLEæ¨¡å‹è§£å†³äº†äº‹æ•…ä¸¥é‡æ€§é¢„æµ‹ä¸­çš„å¤šé‡æŒ‘æˆ˜ï¼ŒåŒ…æ‹¬æ•°æ®ä¸å…¨ã€ç‰¹å¾ä¾èµ–æ€§é«˜ä»¥åŠç±»åˆ†å¸ƒä¸å‡çš„é—®é¢˜ã€‚å®ƒå®ç°äº†é«˜åº¦ä¸“é—¨åŒ–çš„ç‰¹æ€§å¤„ç†å’Œå¯¹ç½•è§çš„é«˜ä¸¥é‡æ€§æ¡ˆä¾‹çš„æ•æ‰ã€‚</li>
<li>MARBLEé‡‡ç”¨å¤šä¸»ä½“è§„åˆ™é©±åŠ¨æ¶æ„ï¼Œé€šè¿‡åˆ†è§£ä»»åŠ¡è‡³å¤šä¸ªä¸“é¡¹æ¨ç†ä¸»ä½“ä»¥æå‡é¢„æµ‹æ€§èƒ½å¹¶é¿å…å•ä¸€æ¨¡å‹çš„å±€é™æ€§ã€‚æ¯ä¸ªä¸»ä½“ä¸“æ³¨äºç‰¹å®šçš„è¯­ä¹‰ç‰¹å¾å­é›†ï¼Œå®ç°æ¨¡å—åŒ–çš„æç¤ºå’ŒèŒƒå›´æ¨ç†ã€‚</li>
<li>è¯¥æ¨¡å‹ä½¿ç”¨è§„åˆ™æˆ–å¤§å‹è¯­è¨€æ¨¡å‹å¼•å¯¼çš„ä¸€è‡´æœºåˆ¶è¿›è¡Œé¢„æµ‹ï¼Œè€ƒè™‘åˆ°ç±»çš„ç¨€æœ‰æ€§å’Œä¿¡å¿ƒåŠ¨æ€å˜åŒ–ã€‚è¿™ç§æœºåˆ¶ä½¿å¾—æ¨¡å‹èƒ½å¤Ÿåº”å¯¹ä¸åŒåœºæ™¯ä¸‹çš„å˜åŒ–ï¼Œå¹¶ä¿æŒé¢„æµ‹çš„ç¨³å®šæ€§ã€‚</li>
<li>MARBLEä¿ç•™æ¯ä¸ªä¸»ä½“çš„æ¨ç†å’Œåè°ƒç»“æœçš„ç»“æ„åŒ–è¿½è¸ªä¿¡æ¯ï¼Œä¸ºæ·±åº¦è§£é‡Šå’Œåè¯„ä¼°æ€§èƒ½è¯Šæ–­æä¾›äº†å¯èƒ½ã€‚è¿™ç§è®¾è®¡ä½¿å¾—æ¨¡å‹å…·å¤‡é«˜åº¦çš„é€æ˜æ€§å’Œå¯è§£é‡Šæ€§ã€‚</li>
<li>å¯¹æ¯”ä¼ ç»Ÿæœºå™¨å­¦ä¹ åˆ†ç±»å™¨å’Œæœ€æ–°çš„æç¤ºæ¨ç†æ–¹æ³•ï¼ŒMARBLEåœ¨çœŸå®ä¸–ç•Œçš„å™ªå£°å’Œæç«¯ç±»åˆ«ä¸å¹³è¡¡æ¡ä»¶ä¸‹è¡¨ç°å‡ºæ›´é«˜çš„å‡†ç¡®æ€§ã€‚å…¶åœ¨è‹±å›½å’Œç¾å›½æ•°æ®é›†ä¸Šçš„è¡¨ç°å‡è¶…è¿‡å…¶ä»–æ–¹æ³•ï¼Œå‡†ç¡®æ€§æ¥è¿‘90%ã€‚</li>
<li>MARBLEå…·å¤‡å‡ºè‰²çš„æ³›åŒ–èƒ½åŠ›ï¼Œèƒ½å¤Ÿé€‚åº”å¤æ‚ç¯å¢ƒä¸‹çš„ä¸ç¡®å®šæ€§æ¨ç†éœ€æ±‚ï¼Œç‰¹åˆ«æ˜¯åœ¨å®‰å…¨å…³é”®åº”ç”¨ä¸­è¡¨ç°çªå‡ºã€‚è¿™ä¸€ç‰¹æ€§ä½¿å¾—MARBLEå…·æœ‰å¹¿æ³›çš„åº”ç”¨å‰æ™¯å’Œå®ç”¨ä»·å€¼ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.04893">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-7d32d02a531c68219a42c6b08ab4643e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d837d8426ecf240c33fa2107f9b39868.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-6e3c614ef1d62c8d2d072905c7897d89.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="FurniMAS-Language-Guided-Furniture-Decoration-using-Multi-Agent-System"><a href="#FurniMAS-Language-Guided-Furniture-Decoration-using-Multi-Agent-System" class="headerlink" title="FurniMAS: Language-Guided Furniture Decoration using Multi-Agent System"></a>FurniMAS: Language-Guided Furniture Decoration using Multi-Agent System</h2><p><strong>Authors:Toan Nguyen, Tri Le, Quang Nguyen, Anh Nguyen</strong></p>
<p>Furniture decoration is an important task in various industrial applications. However, achieving a high-quality decorative result is often time-consuming and requires specialized artistic expertise. To tackle these challenges, we explore how multi-agent systems can assist in automating the decoration process. We propose FurniMAS, a multi-agent system for automatic furniture decoration. Specifically, given a human prompt and a household furniture item such as a working desk or a TV stand, our system suggests relevant assets with appropriate styles and materials, and arranges them on the item, ensuring the decorative result meets functionality, aesthetic, and ambiance preferences. FurniMAS assembles a hybrid team of LLM-based and non-LLM agents, each fulfilling distinct roles in a typical decoration project. These agents collaborate through communication, logical reasoning, and validation to transform the requirements into the final outcome. Extensive experiments demonstrate that our FurniMAS significantly outperforms other baselines in generating high-quality 3D decor. </p>
<blockquote>
<p>å®¶å…·è£…é¥°åœ¨å„ç§å·¥ä¸šåº”ç”¨ä¸­æ˜¯ä¸€é¡¹é‡è¦çš„ä»»åŠ¡ã€‚ç„¶è€Œï¼Œå®ç°é«˜è´¨é‡çš„è£…é¥°æ•ˆæœé€šå¸¸å¾ˆè€—æ—¶ï¼Œå¹¶ä¸”éœ€è¦ä¸“ä¸šçš„è‰ºæœ¯ä¸“ä¸šçŸ¥è¯†ã€‚ä¸ºäº†åº”å¯¹è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æ¢ç´¢äº†å¤šæ™ºèƒ½ä½“ç³»ç»Ÿå¦‚ä½•ååŠ©è‡ªåŠ¨åŒ–è£…é¥°è¿‡ç¨‹ã€‚æˆ‘ä»¬æå‡ºäº†FurniMASï¼Œè¿™æ˜¯ä¸€ä¸ªç”¨äºè‡ªåŠ¨å®¶å…·è£…é¥°çš„å¤šæ™ºèƒ½ä½“ç³»ç»Ÿã€‚å…·ä½“æ¥è¯´ï¼Œç»™å®šä¸€ä¸ªäººå·¥æç¤ºå’Œä¸€ä»¶å®¶å…·ç‰©å“ï¼ˆå¦‚åŠå…¬æ¡Œæˆ–ç”µè§†æ¶ï¼‰ï¼Œæˆ‘ä»¬çš„ç³»ç»Ÿä¼šæ ¹æ®é£æ ¼å’Œææ–™æå‡ºç›¸å…³èµ„äº§ï¼Œå¹¶å°†å®ƒä»¬æ’åˆ—åœ¨ç‰©å“ä¸Šï¼Œç¡®ä¿è£…é¥°ç»“æœç¬¦åˆåŠŸèƒ½ã€ç¾å­¦å’Œæ°›å›´çš„åå¥½ã€‚FurniMASç»“åˆäº†åŸºäºLLMå’ŒéLLMçš„æ™ºèƒ½ä½“æ··åˆå›¢é˜Ÿï¼Œæ¯ä¸ªæ™ºèƒ½ä½“åœ¨å…¸å‹çš„è£…é¥°é¡¹ç›®ä¸­æ‰®æ¼”ç€ä¸åŒçš„è§’è‰²ã€‚è¿™äº›æ™ºèƒ½ä½“é€šè¿‡æ²Ÿé€šã€é€»è¾‘æ¨ç†å’ŒéªŒè¯æ¥åä½œï¼Œå°†è¦æ±‚è½¬åŒ–ä¸ºæœ€ç»ˆæˆæœã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„FurniMASåœ¨ç”Ÿæˆé«˜è´¨é‡3Dè£…é¥°æ–¹é¢æ˜¾è‘—ä¼˜äºå…¶ä»–åŸºçº¿ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.04770v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ¢è®¨äº†å®¶å…·è£…é¥°åœ¨å·¥ä¸šåº”ç”¨ä¸­çš„é‡è¦æ€§åŠå…¶æ‰€é¢ä¸´çš„æŒ‘æˆ˜ã€‚ä¸ºè§£å†³é«˜è´¨é‡å®¶å…·è£…é¥°éœ€è¦å¤§é‡æ—¶é—´å’Œä¸“ä¸šåŒ–è‰ºæœ¯æŠ€èƒ½çš„é—®é¢˜ï¼Œç ”ç©¶äº†ä¸€ç§åˆ©ç”¨å¤šæ™ºèƒ½ä½“ç³»ç»Ÿè¾…åŠ©è‡ªåŠ¨åŒ–è£…é¥°è¿‡ç¨‹çš„æ–¹æ³•ã€‚æå‡ºä¸€ä¸ªåä¸ºFurniMASçš„å¤šæ™ºèƒ½ä½“ç³»ç»Ÿï¼Œèƒ½å¤Ÿæ ¹æ®äººçš„æç¤ºå’Œå®¶å…·ç‰©å“ï¼ˆå¦‚åŠå…¬æ¡Œæˆ–ç”µè§†æŸœç­‰ï¼‰ï¼Œè‡ªåŠ¨å»ºè®®ç¬¦åˆé£æ ¼å’Œææ–™è¦æ±‚çš„è£…é¥°å…ƒç´ ï¼Œå¹¶å°†å…¶å¸ƒç½®åœ¨ç‰©å“ä¸Šï¼Œç¡®ä¿è£…é¥°ç»“æœæ»¡è¶³åŠŸèƒ½ã€ç¾å­¦å’Œç¯å¢ƒåå¥½ã€‚è¯¥ç³»ç»Ÿç»“åˆäº†åŸºäºLLMå’ŒéLLMçš„æ™ºèƒ½ä½“ï¼Œå„è‡ªåœ¨è£…é¥°é¡¹ç›®ä¸­æ‰¿æ‹…ä¸åŒçš„è§’è‰²ï¼Œé€šè¿‡æ²Ÿé€šã€é€»è¾‘æ¨ç†å’ŒéªŒè¯åˆä½œå®Œæˆä»»åŠ¡ã€‚å®éªŒè¯æ˜ï¼Œç›¸è¾ƒäºå…¶ä»–åŸºå‡†ç³»ç»Ÿï¼ŒFurniMASèƒ½æ›´æœ‰æ•ˆåœ°ç”Ÿæˆé«˜è´¨é‡çš„ä¸‰ç»´è£…é¥°æ•ˆæœã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å®¶å…·è£…é¥°åœ¨å·¥ä¸šåº”ç”¨ä¸­çš„é‡è¦æ€§åŠå…¶é¢ä¸´çš„æŒ‘æˆ˜ã€‚</li>
<li>æå‡ºåˆ©ç”¨å¤šæ™ºèƒ½ä½“ç³»ç»Ÿï¼ˆFurniMASï¼‰è‡ªåŠ¨åŒ–å®¶å…·è£…é¥°è¿‡ç¨‹çš„æ–¹æ³•ã€‚</li>
<li>FurniMASèƒ½å¤Ÿæ ¹æ®äººçš„æç¤ºå’Œå®¶å…·ç‰©å“è‡ªåŠ¨å»ºè®®è£…é¥°å…ƒç´ ã€‚</li>
<li>FurniMASç»“åˆäº†åŸºäºLLMå’ŒéLLMçš„æ™ºèƒ½ä½“ï¼Œå„è‡ªæ‰¿æ‹…ä¸åŒçš„è§’è‰²ã€‚</li>
<li>å¤šæ™ºèƒ½ä½“ç³»ç»Ÿé€šè¿‡æ²Ÿé€šã€é€»è¾‘æ¨ç†å’ŒéªŒè¯åˆä½œå®Œæˆä»»åŠ¡ã€‚</li>
<li>FurniMASèƒ½ç”Ÿæˆé«˜è´¨é‡çš„ä¸‰ç»´è£…é¥°æ•ˆæœã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.04770">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-b54dcd264589ab4cb127998eede017b4.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-41006ed275c2250935757f0ad363b42a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-4c5851f0c5fba71136fdaee90cf924fd.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ff713105766df50ab198bbd0037ceb33.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-78b1e4fb85921fcdf15145bb722e82d1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a97f337833bc68a0d163fd6ca328b678.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="ABench-Physics-Benchmarking-Physical-Reasoning-in-LLMs-via-High-Difficulty-and-Dynamic-Physics-Problems"><a href="#ABench-Physics-Benchmarking-Physical-Reasoning-in-LLMs-via-High-Difficulty-and-Dynamic-Physics-Problems" class="headerlink" title="ABench-Physics: Benchmarking Physical Reasoning in LLMs via   High-Difficulty and Dynamic Physics Problems"></a>ABench-Physics: Benchmarking Physical Reasoning in LLMs via   High-Difficulty and Dynamic Physics Problems</h2><p><strong>Authors:Yiming Zhang, Yingfan Ma, Yanmei Gu, Zhengkai Yang, Yihong Zhuang, Feng Wang, Zenan Huang, Yuanyuan Wang, Chao Huang, Bowen Song, Cheng Lin, Junbo Zhao</strong></p>
<p>Large Language Models (LLMs) have shown impressive performance in domains such as mathematics and programming, yet their capabilities in physics remain underexplored and poorly understood. Physics poses unique challenges that demand not only precise computation but also deep conceptual understanding and physical modeling skills. Existing benchmarks often fall short due to limited difficulty, multiple-choice formats, and static evaluation settings that fail to capture physical modeling ability. In this paper, we introduce ABench-Physics, a novel benchmark designed to rigorously evaluate LLMsâ€™ physical reasoning and generalization capabilities. ABench-Physics consists of two components: Phy_A, a static set of 400 graduate- or Olympiad-level problems; and Phy_B, a dynamic subset of 100 problems equipped with an automatic variation engine to test model robustness across changing conditions. All questions require precise numerical answers, with strict formatting and tolerance constraints. Our evaluation of several state-of-the-art LLMs reveals substantial performance gaps, highlighting persistent limitations in physical reasoning, especially in generalization to dynamic variants. ABench-Physics provides a challenging and diagnostic framework for advancing scientific reasoning in LLMs. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨æ•°å­¦å’Œç¼–ç¨‹ç­‰é¢†åŸŸè¡¨ç°å‡ºäº†ä»¤äººå°è±¡æ·±åˆ»çš„æ€§èƒ½ï¼Œä½†å®ƒä»¬åœ¨ç‰©ç†æ–¹é¢çš„èƒ½åŠ›ä»ç„¶è¢«æ¢ç´¢å¾—ä¸å¤Ÿä¸”ç†è§£ä¸è¶³ã€‚ç‰©ç†æå‡ºäº†ç‹¬ç‰¹çš„æŒ‘æˆ˜ï¼Œä¸ä»…è¦æ±‚ç²¾ç¡®è®¡ç®—ï¼Œè¿˜è¦æ±‚æ·±å±‚æ¬¡çš„æ¦‚å¿µç†è§£å’Œç‰©ç†å»ºæ¨¡æŠ€èƒ½ã€‚ç°æœ‰çš„åŸºå‡†æµ‹è¯•é€šå¸¸ç”±äºéš¾åº¦æœ‰é™ã€é‡‡ç”¨å¤šé¡¹é€‰æ‹©æ ¼å¼å’Œé™æ€è¯„ä¼°è®¾ç½®è€Œæ— æ³•æ•æ‰ç‰©ç†å»ºæ¨¡èƒ½åŠ›è€Œæ˜¾å¾—ä¸è¶³ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†ABench-Physicsï¼Œè¿™æ˜¯ä¸€ä¸ªæ—¨åœ¨ä¸¥æ ¼è¯„ä¼°LLMç‰©ç†æ¨ç†å’Œæ³›åŒ–èƒ½åŠ›çš„æ–°å‹åŸºå‡†æµ‹è¯•ã€‚ABench-Physicsç”±ä¸¤éƒ¨åˆ†ç»„æˆï¼šPhy_Aï¼Œä¸€ç»„é™æ€çš„400ä¸ªç ”ç©¶ç”Ÿæˆ–å¥¥æ—åŒ¹å…‹çº§åˆ«çš„é—®é¢˜ï¼›ä»¥åŠPhy_Bï¼Œä¸€ç»„åŠ¨æ€çš„100ä¸ªé—®é¢˜ï¼Œé…å¤‡æœ‰è‡ªåŠ¨å˜åŒ–å¼•æ“ï¼Œä»¥æµ‹è¯•æ¨¡å‹åœ¨ä¸åŒæ¡ä»¶ä¸‹çš„ç¨³å¥æ€§ã€‚æ‰€æœ‰é—®é¢˜éƒ½éœ€è¦ç²¾ç¡®çš„æ•°å­—ç­”æ¡ˆï¼Œå¹¶å¸¦æœ‰ä¸¥æ ¼çš„æ ¼å¼å’Œå®¹å¿åº¦çº¦æŸã€‚æˆ‘ä»¬å¯¹å‡ ç§æœ€æ–°LLMçš„è¯„ä¼°æ˜¾ç¤ºï¼Œå­˜åœ¨æ˜¾è‘—çš„æ€§èƒ½å·®è·ï¼Œçªæ˜¾äº†åœ¨ç‰©ç†æ¨ç†æ–¹é¢çš„æŒç»­å±€é™ï¼Œå°¤å…¶æ˜¯åœ¨æ¨å¹¿åˆ°åŠ¨æ€å˜ä½“æ—¶ã€‚ABench-Physicsä¸ºæ¨è¿›LLMä¸­çš„ç§‘å­¦æ¨ç†æä¾›äº†ä¸€ä¸ªå…·æœ‰æŒ‘æˆ˜æ€§å’Œè¯Šæ–­æ€§çš„æ¡†æ¶ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.04766v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨æ•°å­¦ã€ç¼–ç¨‹ç­‰é¢†åŸŸè¡¨ç°å‡ºè‰²ï¼Œä½†åœ¨ç‰©ç†å­¦é¢†åŸŸçš„èƒ½åŠ›å´è¢«å¿½è§†ä¸”ç†è§£ä¸è¶³ã€‚ç‰©ç†å­¦å…·æœ‰ç‹¬ç‰¹æŒ‘æˆ˜ï¼Œè¦æ±‚ä¸ä»…ç²¾ç¡®è®¡ç®—ï¼Œè¿˜éœ€æ·±åº¦æ¦‚å¿µç†è§£å’Œç‰©ç†å»ºæ¨¡æŠ€èƒ½ã€‚ç°æœ‰åŸºå‡†æµ‹è¯•å¾€å¾€å› éš¾åº¦æœ‰é™ã€å¤šé€‰æ ¼å¼å’Œé™æ€è¯„ä¼°ç¯å¢ƒè€Œæ— æ³•æ•æ‰ç‰©ç†å»ºæ¨¡èƒ½åŠ›ã€‚æœ¬æ–‡ä»‹ç»ABench-Physicsï¼Œä¸€ä¸ªæ—¨åœ¨ä¸¥æ ¼è¯„ä¼°LLMsç‰©ç†æ¨ç†å’Œæ³›åŒ–èƒ½åŠ›çš„æ–°å‹åŸºå‡†æµ‹è¯•ã€‚ABench-Physicsç”±ä¸¤éƒ¨åˆ†ç»„æˆï¼šPhy_Aï¼ŒåŒ…å«400ä¸ªç ”ç©¶ç”Ÿæˆ–å¥¥æ—åŒ¹å…‹çº§åˆ«çš„é™æ€é—®é¢˜ï¼›Phy_Bï¼Œé…å¤‡è‡ªåŠ¨å˜åŒ–å¼•æ“çš„100ä¸ªåŠ¨æ€é—®é¢˜å­é›†ï¼Œä»¥æµ‹è¯•æ¨¡å‹åœ¨ä¸åŒæ¡ä»¶ä¸‹çš„ç¨³å¥æ€§ã€‚æ‰€æœ‰é—®é¢˜å‡éœ€ç²¾ç¡®æ•°å€¼å›ç­”ï¼Œæœ‰ä¸¥æ ¼çš„æ ¼å¼å’Œå®¹å¿åº¦çº¦æŸã€‚æˆ‘ä»¬å¯¹å‡ æ¬¾å…ˆè¿›LLMsçš„è¯„ä¼°æ˜¾ç¤ºï¼Œå­˜åœ¨æ˜¾è‘—æ€§èƒ½å·®è·ï¼Œå°¤å…¶åœ¨æ³›åŒ–åˆ°åŠ¨æ€å˜é‡æ–¹é¢çš„ç‰©ç†æ¨ç†èƒ½åŠ›ä¸Šã€‚ABench-Physicsä¸ºæ¨è¿›LLMsä¸­çš„ç§‘å­¦æ¨ç†æä¾›äº†ä¸€ä¸ªå…·æœ‰æŒ‘æˆ˜æ€§å’Œè¯Šæ–­æ€§çš„æ¡†æ¶ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨ç‰©ç†å­¦é¢†åŸŸçš„èƒ½åŠ›è¢«å¿½è§†ä¸”ç†è§£ä¸è¶³ã€‚</li>
<li>ç‰©ç†å­¦è¦æ±‚ç²¾ç¡®è®¡ç®—ã€æ·±åº¦æ¦‚å¿µç†è§£å’Œç‰©ç†å»ºæ¨¡æŠ€èƒ½ã€‚</li>
<li>ç°æœ‰åŸºå‡†æµ‹è¯•æ— æ³•æœ‰æ•ˆè¯„ä¼°LLMsåœ¨ç‰©ç†å»ºæ¨¡æ–¹é¢çš„èƒ½åŠ›ã€‚</li>
<li>ABench-Physicsæ˜¯ä¸€ä¸ªæ–°å‹åŸºå‡†æµ‹è¯•ï¼Œæ—¨åœ¨è¯„ä¼°LLMsçš„ç‰©ç†æ¨ç†å’Œæ³›åŒ–èƒ½åŠ›ã€‚</li>
<li>ABench-PhysicsåŒ…å«é™æ€å’ŒåŠ¨æ€é—®é¢˜ï¼Œä»¥æ•æ‰LLMsåœ¨ä¸åŒæ¡ä»¶ä¸‹çš„æ€§èƒ½ã€‚</li>
<li>LLMsåœ¨ç‰©ç†æ¨ç†æ–¹é¢å­˜åœ¨æ˜¾è‘—æ€§èƒ½å·®è·ï¼Œå°¤å…¶åœ¨æ³›åŒ–èƒ½åŠ›æ–¹é¢ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.04766">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-0712c498b60b91ee05b6f32fc5b14dc1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-bd548c1e19657c6009e1f3cb98369161.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-adb97ae4e8ffd1f96f6723b70774c9e1.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-03eac42e0505d38ccab9a4b3b716c563.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="ChipSeek-R1-Generating-Human-Surpassing-RTL-with-LLM-via-Hierarchical-Reward-Driven-Reinforcement-Learning"><a href="#ChipSeek-R1-Generating-Human-Surpassing-RTL-with-LLM-via-Hierarchical-Reward-Driven-Reinforcement-Learning" class="headerlink" title="ChipSeek-R1: Generating Human-Surpassing RTL with LLM via Hierarchical   Reward-Driven Reinforcement Learning"></a>ChipSeek-R1: Generating Human-Surpassing RTL with LLM via Hierarchical   Reward-Driven Reinforcement Learning</h2><p><strong>Authors:Zhirong Chen, Kaiyan Chang, Zhuolin Li, Xinyang He, Chujie Chen, Cangyuan Li, Mengdi Wang, Haobo Xu, Yinhe Han, Ying Wang</strong></p>
<p>Large Language Models (LLMs) show significant potential for automating Register-Transfer Level (RTL) code generation. However, current approaches face a critical challenge: they can not simultaneously optimize for functional correctness and hardware quality (Power, Performance, Area - PPA). Methods based on supervised fine-tuning often generate functionally correct but PPA-suboptimal code, lacking mechanisms to learn optimization principles. In contrast, post-processing techniques that attempt to improve PPA metrics after generation are often inefficient because they operate externally without updating the LLMâ€™s parameters, thus failing to enhance the modelâ€™s intrinsic design capabilities.   To bridge this gap, we introduce ChipSeek-R1, a hierarchical reward-driven reinforcement learning framework to train LLMs to generate RTL code that achieves both functional correctness and optimized PPA metrics. ChipSeek-R1 employs a hierarchical reward system, which incorporates direct feedback on syntax, functional correctness (from simulators) and PPA metrics (from synthesis tools) during reinforcement learning. This enables the model to learn complex hardware design trade-offs via trial-and-error, generating RTL code that is both functionally correct and PPA-optimized. Evaluating ChipSeek-R1 on standard benchmarks (VerilogEval, RTLLM), we achieve state-of-the-art results in functional correctness. Notably, on the RTLLM benchmark, ChipSeek-R1 generated 27 RTL designs surpassing the PPA metrics of the original human-written code. Our findings demonstrate the effectiveness of integrating toolchain feedback into LLM training and highlight the potential for reinforcement learning to enable automated generation of human-surpassing RTL code. We open-source our code in anonymous github. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨è‡ªåŠ¨åŒ–å¯„å­˜å™¨ä¼ è¾“çº§åˆ«ï¼ˆRTLï¼‰ä»£ç ç”Ÿæˆæ–¹é¢æ˜¾ç¤ºå‡ºå·¨å¤§æ½œåŠ›ã€‚ç„¶è€Œï¼Œå½“å‰çš„æ–¹æ³•é¢ä¸´ä¸€ä¸ªå…³é”®æŒ‘æˆ˜ï¼šå®ƒä»¬ä¸èƒ½åŒæ—¶ä¼˜åŒ–åŠŸèƒ½æ­£ç¡®æ€§å’Œç¡¬ä»¶è´¨é‡ï¼ˆåŠŸç‡ã€æ€§èƒ½ã€é¢ç§¯-PPAï¼‰ã€‚åŸºäºç›‘ç£å¾®è°ƒçš„æ–¹æ³•é€šå¸¸ä¼šç”ŸæˆåŠŸèƒ½æ­£ç¡®ä½†PPAæŒ‡æ ‡ä¸ä½³çš„ä»£ç ï¼Œç¼ºä¹å­¦ä¹ ä¼˜åŒ–åŸåˆ™çš„æœºåˆ¶ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼Œè¯•å›¾åœ¨ç”Ÿæˆåæ”¹è¿›PPAæŒ‡æ ‡çš„åå¤„ç†æŠ€æœ¯é€šå¸¸æ•ˆç‡ä½ä¸‹ï¼Œå› ä¸ºå®ƒä»¬å¤–éƒ¨æ“ä½œè€Œä¸æ›´æ–°LLMçš„å‚æ•°ï¼Œå› æ­¤æ— æ³•æé«˜æ¨¡å‹çš„å†…åœ¨è®¾è®¡èƒ½åŠ›ã€‚ä¸ºäº†å¼¥è¡¥è¿™ä¸€å·®è·ï¼Œæˆ‘ä»¬å¼•å…¥äº†ChipSeek-R1ï¼Œè¿™æ˜¯ä¸€ä¸ªåˆ†å±‚çš„å¥–åŠ±é©±åŠ¨å¼ºåŒ–å­¦ä¹ æ¡†æ¶ï¼Œç”¨äºè®­ç»ƒLLMç”Ÿæˆå®ç°åŠŸèƒ½æ­£ç¡®æ€§å’Œä¼˜åŒ–PPAæŒ‡æ ‡çš„RTLä»£ç ã€‚ChipSeek-R1é‡‡ç”¨åˆ†å±‚å¥–åŠ±ç³»ç»Ÿï¼Œåœ¨å¼ºåŒ–å­¦ä¹ è¿‡ç¨‹ä¸­èå…¥å¯¹è¯­æ³•ã€åŠŸèƒ½æ­£ç¡®æ€§ï¼ˆæ¥è‡ªæ¨¡æ‹Ÿå™¨ï¼‰å’ŒPPAæŒ‡æ ‡ï¼ˆæ¥è‡ªåˆæˆå·¥å…·ï¼‰çš„ç›´æ¥åé¦ˆã€‚è¿™ä½¿æ¨¡å‹èƒ½å¤Ÿé€šè¿‡è¯•é”™å­¦ä¹ ç¡¬ä»¶è®¾è®¡çš„å¤æ‚æƒè¡¡ï¼Œç”Ÿæˆæ—¢åŠŸèƒ½æ­£ç¡®åˆPPAä¼˜åŒ–çš„RTLä»£ç ã€‚åœ¨æ ‡å‡†åŸºå‡†æµ‹è¯•ï¼ˆVerilogEvalã€RTLLMï¼‰ä¸Šè¯„ä¼°ChipSeek-R1ï¼Œæˆ‘ä»¬åœ¨åŠŸèƒ½æ­£ç¡®æ€§æ–¹é¢è¾¾åˆ°äº†æœ€æ–°æ°´å¹³ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œåœ¨RTLLMåŸºå‡†æµ‹è¯•ä¸­ï¼ŒChipSeek-R1ç”Ÿæˆçš„27ä¸ªRTLè®¾è®¡åœ¨PPAæŒ‡æ ‡ä¸Šè¶…è¶Šäº†åŸå§‹äººå·¥ç¼–å†™çš„ä»£ç ã€‚æˆ‘ä»¬çš„ç ”ç©¶ç»“æœè¡¨æ˜å°†å·¥å…·é“¾åé¦ˆé›†æˆåˆ°LLMè®­ç»ƒä¸­çš„æœ‰æ•ˆæ€§ï¼Œå¹¶å¼ºè°ƒäº†å¼ºåŒ–å­¦ä¹ åœ¨è‡ªåŠ¨ç”Ÿæˆè¶…è¶Šäººç±»çš„RTLä»£ç æ–¹é¢çš„æ½œåŠ›ã€‚æˆ‘ä»¬åœ¨åŒ¿ågithubä¸Šå…¬å¼€äº†æˆ‘ä»¬çš„ä»£ç ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.04736v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†Large Language Modelsï¼ˆLLMsï¼‰åœ¨è‡ªåŠ¨åŒ–ç”ŸæˆRegister-Transfer Levelï¼ˆRTLï¼‰ä»£ç æ–¹é¢çš„å·¨å¤§æ½œåŠ›ï¼Œä½†ç°æœ‰æ–¹æ³•æ— æ³•åŒæ—¶å®ç°åŠŸèƒ½æ­£ç¡®æ€§å’Œç¡¬ä»¶è´¨é‡ï¼ˆPPAï¼‰çš„ä¼˜åŒ–ã€‚ä¸ºæ­¤ï¼Œæå‡ºäº†ä¸€ç§æ–°çš„æ–¹æ³•ChipSeek-R1ï¼Œé‡‡ç”¨åˆ†å±‚å¥–åŠ±é©±åŠ¨çš„å¼ºåŒ–å­¦ä¹ æ¡†æ¶ï¼Œæ—¨åœ¨è®­ç»ƒLLMsç”Ÿæˆæ—¢ç¬¦åˆåŠŸèƒ½è¦æ±‚åˆä¼˜åŒ–PPAæŒ‡æ ‡çš„RTLä»£ç ã€‚è¯¥æ–¹æ³•é€šè¿‡ç»“åˆè¯­æ³•ã€åŠŸèƒ½æ­£ç¡®æ€§ï¼ˆæ¥è‡ªæ¨¡æ‹Ÿå™¨ï¼‰å’ŒPPAæŒ‡æ ‡ï¼ˆæ¥è‡ªç»¼åˆå·¥å…·ï¼‰çš„åˆ†å±‚å¥–åŠ±ç³»ç»Ÿï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿé€šè¿‡è¯•é”™å­¦ä¹ ç¡¬ä»¶è®¾è®¡çš„å¤æ‚æƒè¡¡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒChipSeek-R1åœ¨æ ‡å‡†åŸºå‡†æµ‹è¯•ä¸Šå®ç°äº†æœ€æ–°çš„åŠŸèƒ½æ­£ç¡®æ€§ï¼Œå¹¶åœ¨RTLLMåŸºå‡†æµ‹è¯•ä¸­ç”Ÿæˆçš„RTLè®¾è®¡åœ¨PPAæŒ‡æ ‡ä¸Šè¶…è¶Šäº†åŸå§‹äººå·¥ç¼–å†™çš„ä»£ç ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLMsåœ¨RTLä»£ç ç”Ÿæˆæ–¹é¢å…·æœ‰å·¨å¤§æ½œåŠ›ï¼Œä½†é¢ä¸´åŠŸèƒ½æ­£ç¡®æ€§ä¸ç¡¬ä»¶è´¨é‡ï¼ˆPPAï¼‰ä¼˜åŒ–ä¸èƒ½å…¼é¡¾çš„æŒ‘æˆ˜ã€‚</li>
<li>ç°æœ‰æ–¹æ³•å¦‚ç›‘ç£å¾®è°ƒæˆ–åå¤„ç†æŠ€æœ¯åœ¨ä¼˜åŒ–PPAæ–¹é¢å­˜åœ¨ç¼ºé™·ã€‚</li>
<li>ChipSeek-R1ä½¿ç”¨åˆ†å±‚å¥–åŠ±é©±åŠ¨çš„å¼ºåŒ–å­¦ä¹ æ¡†æ¶ï¼Œç»“åˆè¯­æ³•ã€åŠŸèƒ½æ­£ç¡®æ€§å’ŒPPAæŒ‡æ ‡çš„åé¦ˆï¼Œè®­ç»ƒLLMsç”Ÿæˆä¼˜åŒ–çš„RTLä»£ç ã€‚</li>
<li>ChipSeek-R1åœ¨æ ‡å‡†åŸºå‡†æµ‹è¯•ä¸Šå®ç°æœ€æ–°æˆæœï¼Œå°¤å…¶æ˜¯åœ¨PPAæŒ‡æ ‡ä¸Šè¶…è¶Šäº†äººå·¥ç¼–å†™çš„ä»£ç ã€‚</li>
<li>å°†å·¥å…·é“¾åé¦ˆé›†æˆåˆ°LLMè®­ç»ƒä¸­æ˜¯æœ‰æ•ˆçš„æ–¹æ³•ã€‚</li>
<li>å¼ºåŒ–å­¦ä¹ åœ¨è‡ªåŠ¨åŒ–ç”Ÿæˆè¶…è¶Šäººç±»çš„RTLä»£ç ä¸­å…·æœ‰å·¨å¤§æ½œåŠ›ã€‚</li>
<li>ChipSeek-R1çš„ä»£ç å·²å¼€æºã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.04736">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-7ecc052e39e9048072db6d72c947c5d9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-47dbdde72f307930ab2e2fe03150b761.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-69019e9ddedaa43ff575f4b9c3bdaaf6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d732878b2e4af1bdc62674095cb32f07.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-a163286e8365c47f24b7ff74698ba164.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c0c89cbb9c1081d42ec2dca5cb266a25.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Tempo-R0-A-Video-MLLM-for-Temporal-Video-Grounding-through-Efficient-Temporal-Sensing-Reinforcement-Learning"><a href="#Tempo-R0-A-Video-MLLM-for-Temporal-Video-Grounding-through-Efficient-Temporal-Sensing-Reinforcement-Learning" class="headerlink" title="Tempo-R0: A Video-MLLM for Temporal Video Grounding through Efficient   Temporal Sensing Reinforcement Learning"></a>Tempo-R0: A Video-MLLM for Temporal Video Grounding through Efficient   Temporal Sensing Reinforcement Learning</h2><p><strong>Authors:Feng Yue, Zhaoxing Zhang, Junming Jiao, Zhengyu Liang, Shiwen Cao, Feifei Zhang, Rong Shen</strong></p>
<p>Temporal Video Grounding (TVG), which requires pinpointing relevant temporal segments from video based on language query, has always been a highly challenging task in the field of video understanding. Videos often have a larger volume of information and redundancy than texts or images. Models should present comprehensive understanding of the whole video to accurately retrieve query-relevant clips. We thus propose Tempo-R0: a Video Multimodal Large Language Model (Video-MLLM) for the temporal video grounding task via multimodal temporal sensing reinforcement. Specifically, during the preprocessing stage of our pipeline, we employ Self-adaptive Attention Allocation (SAA) method based on frame content variation to efficiently use the MLLMâ€™s limited attention. The Explicit Timestamp-modal Aligned (ETA) method is also utilized to strengthen our modelâ€™s capability to perceive the boundaries of events in the video. In the fine-tuning part of our pipeline, we creatively apply Partial Irrelevance Refusing-based Group Relative Policy Optimization (PIR-GRPO) in TVG area to foster modelâ€™s temporal reasoning from not only accepting relevant video-query pairs but also refusing irrelevant ones. Experiments demonstrate that our method accomplishes a notable advantage over SOTA solutions by around 3.5% on both the original QVHighlights testbench and its corrected version with more reasonable ground truth annotations. </p>
<blockquote>
<p>è§†é¢‘æ—¶åºå®šä½ï¼ˆTVGï¼‰ä¸€ç›´æ˜¯è§†é¢‘ç†è§£é¢†åŸŸä¸­çš„ä¸€ä¸ªæå…·æŒ‘æˆ˜æ€§çš„ä»»åŠ¡ï¼Œå®ƒè¦æ±‚ä»è§†é¢‘ä¸­æ‰¾åˆ°ä¸è¯­è¨€æŸ¥è¯¢ç›¸å…³çš„æ—¶åºç‰‡æ®µã€‚è§†é¢‘é€šå¸¸åŒ…å«æ¯”æ–‡æœ¬æˆ–å›¾åƒæ›´å¤§é‡çš„ä¿¡æ¯å’Œå†—ä½™ã€‚ä¸ºäº†å‡†ç¡®æ£€ç´¢ä¸æŸ¥è¯¢ç›¸å…³çš„ç‰‡æ®µï¼Œæ¨¡å‹éœ€è¦å¯¹æ•´ä¸ªè§†é¢‘è¿›è¡Œå…¨é¢ç†è§£ã€‚å› æ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†Tempo-R0ï¼šä¸€ç§ç”¨äºæ—¶åºè§†é¢‘å®šä½ä»»åŠ¡çš„è§†é¢‘å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆVideo-MLLMï¼‰ï¼Œé€šè¿‡å¤šæ¨¡æ€æ—¶åºæ„ŸçŸ¥å¢å¼ºæ¥å®ç°ã€‚å…·ä½“æ¥è¯´ï¼Œåœ¨æˆ‘ä»¬çš„ç®¡é“é¢„å¤„ç†é˜¶æ®µï¼Œæˆ‘ä»¬é‡‡ç”¨åŸºäºå¸§å†…å®¹å˜åŒ–çš„è‡ªé€‚åº”æ³¨æ„åŠ›åˆ†é…ï¼ˆSAAï¼‰æ–¹æ³•ï¼Œä»¥æœ‰æ•ˆåˆ©ç”¨MLLMçš„æœ‰é™æ³¨æ„åŠ›ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜ä½¿ç”¨æ˜¾å¼æ—¶é—´æˆ³æ¨¡æ€å¯¹é½ï¼ˆETAï¼‰æ–¹æ³•æ¥å¢å¼ºæˆ‘ä»¬æ¨¡å‹æ„ŸçŸ¥è§†é¢‘äº‹ä»¶è¾¹ç•Œçš„èƒ½åŠ›ã€‚åœ¨æˆ‘ä»¬ç®¡é“çš„å¾®è°ƒéƒ¨åˆ†ï¼Œæˆ‘ä»¬åˆ›é€ æ€§åœ°åº”ç”¨åŸºäºéƒ¨åˆ†ä¸ç›¸å…³æ‹’ç»åˆ†ç»„ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆPIR-GRPOï¼‰çš„TVGé¢†åŸŸæ–¹æ³•ï¼Œä»¥ä¿ƒè¿›æ¨¡å‹ä¸ä»…æ¥å—ç›¸å…³çš„è§†é¢‘æŸ¥è¯¢å¯¹ï¼Œè€Œä¸”æ‹’ç»ä¸ç›¸å…³çš„å¯¹ã€‚å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨åŸå§‹QVHighlightsæµ‹è¯•å¹³å°å’Œå…¶æ›´æ­£ç‰ˆæœ¬ï¼ˆå…·æœ‰æ›´åˆç†çš„çœŸå®æ ‡æ³¨ï¼‰ä¸Šçš„è¡¨ç°å‡ä¼˜äºç°æœ‰æœ€ä½³è§£å†³æ–¹æ¡ˆï¼Œä¼˜åŠ¿çº¦ä¸º3.5%ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.04702v1">PDF</a> </p>
<p><strong>Summary</strong>ï¼šè§†é¢‘å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆVideo-MLLMï¼‰åœ¨è§†é¢‘ç†è§£é¢†åŸŸå–å¾—äº†é‡å¤§è¿›å±•ï¼Œå°¤å…¶æ˜¯åœ¨æ—¶åºè§†é¢‘å®šä½ä»»åŠ¡ä¸­ã€‚è¯¥ç ”ç©¶é€šè¿‡å¼•å…¥è‡ªé€‚åº”æ³¨æ„åŠ›åˆ†é…ï¼ˆSAAï¼‰å’Œæ˜¾å¼æ—¶é—´æˆ³æ¨¡æ€å¯¹é½ï¼ˆETAï¼‰æ–¹æ³•ï¼Œä»¥åŠåŸºäºéƒ¨åˆ†ä¸ç›¸å…³æ‹’ç»çš„ç›¸å¯¹ç»„ç­–ç•¥ä¼˜åŒ–ï¼ˆPIR-GRPOï¼‰ï¼Œæé«˜äº†æ¨¡å‹å¯¹è§†é¢‘å†…å®¹çš„å…¨é¢ç†è§£å’Œæ—¶åºæ¨ç†èƒ½åŠ›ã€‚å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨QVHighlightsæµ‹è¯•é›†ä¸Šè¾ƒç°æœ‰æŠ€æœ¯æé«˜äº†çº¦3.5%ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>è§†é¢‘å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆVideo-MLLMï¼‰ç”¨äºæ—¶åºè§†é¢‘å®šä½ä»»åŠ¡ã€‚</li>
<li>é€šè¿‡å¼•å…¥è‡ªé€‚åº”æ³¨æ„åŠ›åˆ†é…ï¼ˆSAAï¼‰æ–¹æ³•æé«˜æ¨¡å‹çš„æ³¨æ„åŠ›ä½¿ç”¨æ•ˆç‡ã€‚</li>
<li>æ˜¾å¼æ—¶é—´æˆ³æ¨¡æ€å¯¹é½ï¼ˆETAï¼‰æ–¹æ³•ç”¨äºå¢å¼ºæ¨¡å‹å¯¹è§†é¢‘äº‹ä»¶è¾¹ç•Œçš„æ„ŸçŸ¥èƒ½åŠ›ã€‚</li>
<li>æå‡ºåŸºäºéƒ¨åˆ†ä¸ç›¸å…³æ‹’ç»çš„ç›¸å¯¹ç»„ç­–ç•¥ä¼˜åŒ–ï¼ˆPIR-GRPOï¼‰ï¼Œä¿ƒè¿›æ¨¡å‹çš„æ—¶åºæ¨ç†èƒ½åŠ›ã€‚</li>
<li>æ¨¡å‹ä¸ä»…æ¥å—ç›¸å…³çš„è§†é¢‘æŸ¥è¯¢å¯¹ï¼Œè¿˜æ‹’ç»ä¸ç›¸å…³çš„å¯¹ï¼Œæé«˜äº†æ¨¡å‹çš„å‡†ç¡®æ€§ã€‚</li>
<li>å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨QVHighlightsæµ‹è¯•é›†ä¸Šè¾ƒç°æœ‰æŠ€æœ¯æœ‰æ˜¾è‘—æé«˜ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.04702">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-4c84ab7bd03c033c83cc516f7c8bbbbe.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-16e56fe85a850280335e192c9c0d8acf.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-09c00c0dc7d963497231e65090335c6e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-24c3b8e35a9363f471707b607b382905.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-37754cbf8a66a297f12a49491ae7eed6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d53599486bd8a2098849a920e9936083.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-0f8050e3222f911c4ad8dbc95b235ac8.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e8b914aa083acd8be808add067cdac0f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-340963c67263784ca6ba0ca98bd151f1.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="A-Visual-Leap-in-CLIP-Compositionality-Reasoning-through-Generation-of-Counterfactual-Sets"><a href="#A-Visual-Leap-in-CLIP-Compositionality-Reasoning-through-Generation-of-Counterfactual-Sets" class="headerlink" title="A Visual Leap in CLIP Compositionality Reasoning through Generation of   Counterfactual Sets"></a>A Visual Leap in CLIP Compositionality Reasoning through Generation of   Counterfactual Sets</h2><p><strong>Authors:Zexi Jia, Chuanwei Huang, Hongyan Fei, Yeshuang Zhu, Zhiqiang Yuan, Ying Deng, Jiapei Zhang, Jinchao Zhang, Jie Zhou</strong></p>
<p>Vision-language models (VLMs) often struggle with compositional reasoning due to insufficient high-quality image-text data. To tackle this challenge, we propose a novel block-based diffusion approach that automatically generates counterfactual datasets without manual annotation. Our method utilizes large language models to identify entities and their spatial relationships. It then independently generates image blocks as â€œpuzzle piecesâ€ coherently arranged according to specified compositional rules. This process creates diverse, high-fidelity counterfactual image-text pairs with precisely controlled variations. In addition, we introduce a specialized loss function that differentiates inter-set from intra-set samples, enhancing training efficiency and reducing the need for negative samples. Experiments demonstrate that fine-tuning VLMs with our counterfactual datasets significantly improves visual reasoning performance. Our approach achieves state-of-the-art results across multiple benchmarks while using substantially less training data than existing methods. </p>
<blockquote>
<p>è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰ç”±äºé«˜è´¨é‡å›¾åƒæ–‡æœ¬æ•°æ®çš„ä¸è¶³ï¼Œå¸¸å¸¸åœ¨ç»„åˆæ¨ç†æ–¹é¢é‡åˆ°å›°éš¾ã€‚ä¸ºäº†åº”å¯¹è¿™ä¸€æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºåŒºå—çš„æ‰©æ•£æ–¹æ³•ï¼Œè¯¥æ–¹æ³•èƒ½å¤Ÿè‡ªåŠ¨ç”Ÿæˆæ— éœ€æ‰‹åŠ¨æ³¨é‡Šçš„åäº‹å®æ•°æ®é›†ã€‚æˆ‘ä»¬çš„æ–¹æ³•åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹æ¥è¯†åˆ«å®ä½“åŠå…¶ç©ºé—´å…³ç³»ã€‚ç„¶åï¼Œå®ƒç‹¬ç«‹åœ°ç”Ÿæˆå›¾åƒåŒºå—ï¼Œä½œä¸ºæ ¹æ®ç‰¹å®šç»„åˆè§„åˆ™è¿è´¯æ’åˆ—çš„â€œæ‹¼å›¾â€ã€‚è¿™ä¸€è¿‡ç¨‹åˆ›å»ºäº†å¤šæ ·ä¸”é«˜ä¿çœŸåº¦çš„åäº‹å®å›¾åƒæ–‡æœ¬å¯¹ï¼Œå…·æœ‰ç²¾ç¡®æ§åˆ¶çš„å˜ä½“ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜å¼•å…¥äº†ä¸€ç§ä¸“é—¨çš„æŸå¤±å‡½æ•°ï¼Œç”¨äºåŒºåˆ†é›†åˆä¹‹é—´å’Œé›†åˆå†…éƒ¨çš„æ ·æœ¬ï¼Œä»è€Œæé«˜è®­ç»ƒæ•ˆç‡ï¼Œå‡å°‘è´Ÿæ ·æœ¬çš„éœ€æ±‚ã€‚å®éªŒè¡¨æ˜ï¼Œä½¿ç”¨æˆ‘ä»¬çš„åäº‹å®æ•°æ®é›†å¾®è°ƒVLMså¯ä»¥æ˜¾è‘—æé«˜è§†è§‰æ¨ç†æ€§èƒ½ã€‚æˆ‘ä»¬çš„æ–¹æ³•åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸Šå–å¾—äº†æœ€å…ˆè¿›çš„æˆæœï¼ŒåŒæ—¶ä½¿ç”¨çš„è®­ç»ƒæ•°æ®é‡è¿œå°‘äºç°æœ‰æ–¹æ³•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.04699v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>åœ¨é¢ä¸´è§†è§‰è¯­è¨€æ¨¡å‹å› ç¼ºä¹é«˜è´¨é‡å›¾åƒæ–‡æœ¬æ•°æ®è€Œå¯¼è‡´çš„ç»„åˆæ¨ç†æŒ‘æˆ˜æ—¶ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºå—æ‰©æ•£çš„æ–°æ–¹æ³•è‡ªåŠ¨ç”Ÿæˆæ— éœ€äººå·¥æ ‡æ³¨çš„å‡è®¾æ•°æ®é›†ã€‚æ­¤æ–¹æ³•åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹è¯†åˆ«å®ä½“åŠå…¶ç©ºé—´å…³ç³»ï¼Œå¹¶æ ¹æ®ç‰¹å®šçš„ç»„åˆè§„åˆ™ç‹¬ç«‹ç”Ÿæˆå›¾åƒå—ä½œä¸ºâ€œæ‹¼å›¾å—â€è¿›è¡Œå¸ƒå±€å®‰æ’ã€‚è¿™ç§æ–¹æ³•ç”Ÿæˆäº†å¤šæ ·åŒ–ä¸”é«˜ä¿çœŸåº¦çš„å‡è®¾å›¾åƒæ–‡æœ¬å¯¹ï¼Œå¹¶å…·å¤‡ç²¾ç¡®æ§åˆ¶çš„å˜ä½“ã€‚æ­¤å¤–ï¼Œå¼•å…¥äº†ä¸€ç§ä¸“é—¨çš„æŸå¤±å‡½æ•°ï¼Œä»¥åŒºåˆ†æ ·æœ¬é—´çš„å·®å¼‚ï¼Œæé«˜è®­ç»ƒæ•ˆç‡å¹¶å‡å°‘è´Ÿæ ·æœ¬çš„éœ€æ±‚ã€‚å®éªŒè¡¨æ˜ï¼Œä½¿ç”¨æˆ‘ä»¬çš„å‡è®¾æ•°æ®é›†å¾®è°ƒè§†è§‰è¯­è¨€æ¨¡å‹å¯æ˜¾è‘—æé«˜è§†è§‰æ¨ç†æ€§èƒ½ã€‚è¯¥æ–¹æ³•åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¾¾åˆ°é¢†å…ˆæ°´å¹³ï¼ŒåŒæ—¶ä½¿ç”¨çš„æ•°æ®é‡è¿œä½äºç°æœ‰æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æœ¬æ–‡æå‡ºäº†åŸºäºå—æ‰©æ•£çš„æ–¹æ³•è‡ªåŠ¨ç”Ÿæˆå‡è®¾æ•°æ®é›†æ¥è§£å†³è§†è§‰è¯­è¨€æ¨¡å‹é¢å¯¹çš„ç»„åˆæ¨ç†æŒ‘æˆ˜ã€‚</li>
<li>è¯¥æ–¹æ³•åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹è¯†åˆ«å®ä½“å’Œç©ºé—´å…³ç³»ï¼Œç”Ÿæˆå›¾åƒå—ä½œä¸ºæ‹¼å›¾å—è¿›è¡Œå¸ƒå±€å®‰æ’ã€‚</li>
<li>é€šè¿‡ç‰¹å®šçš„ç»„åˆè§„åˆ™ç”Ÿæˆå¤šæ ·åŒ–ä¸”é«˜ä¿çœŸåº¦çš„å›¾åƒæ–‡æœ¬å¯¹ã€‚</li>
<li>å¼•å…¥äº†ä¸€ç§ä¸“é—¨çš„æŸå¤±å‡½æ•°ä»¥æé«˜è®­ç»ƒæ•ˆç‡å¹¶å‡å°‘è´Ÿæ ·æœ¬éœ€æ±‚ã€‚</li>
<li>å®éªŒæ˜¾ç¤ºä½¿ç”¨è¯¥æ–¹æ³•ç”Ÿæˆçš„å‡è®¾æ•°æ®é›†å¯æ˜¾è‘—æé«˜è§†è§‰è¯­è¨€æ¨¡å‹çš„æ¨ç†æ€§èƒ½ã€‚</li>
<li>è¯¥æ–¹æ³•åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°é¢†å…ˆã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.04699">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-f777a5dcee3a844f28c135d39429d925.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-11dd7600390d7bdde2cc5b39f9bf6fbe.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-400e8060287fd6a422eead9e6577628a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3731f9b17f44064f4552f50cbc321319.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="DRAE-Dynamic-Retrieval-Augmented-Expert-Networks-for-Lifelong-Learning-and-Task-Adaptation-in-Robotics"><a href="#DRAE-Dynamic-Retrieval-Augmented-Expert-Networks-for-Lifelong-Learning-and-Task-Adaptation-in-Robotics" class="headerlink" title="DRAE: Dynamic Retrieval-Augmented Expert Networks for Lifelong Learning   and Task Adaptation in Robotics"></a>DRAE: Dynamic Retrieval-Augmented Expert Networks for Lifelong Learning   and Task Adaptation in Robotics</h2><p><strong>Authors:Yayu Long, Kewei Chen, Long Jin, Mingsheng Shang</strong></p>
<p>We introduce Dynamic Retrieval-Augmented Expert Networks (DRAE), a groundbreaking architecture that addresses the challenges of lifelong learning, catastrophic forgetting, and task adaptation by combining the dynamic routing capabilities of Mixture-of-Experts (MoE); leveraging the knowledge-enhancement power of Retrieval-Augmented Generation (RAG); incorporating a novel hierarchical reinforcement learning (RL) framework; and coordinating through ReflexNet-SchemaPlanner-HyperOptima (RSHO).DRAE dynamically routes expert models via a sparse MoE gating mechanism, enabling efficient resource allocation while leveraging external knowledge through parametric retrieval (P-RAG) to augment the learning process. We propose a new RL framework with ReflexNet for low-level task execution, SchemaPlanner for symbolic reasoning, and HyperOptima for long-term context modeling, ensuring continuous adaptation and memory retention. Experimental results show that DRAE significantly outperforms baseline approaches in long-term task retention and knowledge reuse, achieving an average task success rate of 82.5% across a set of dynamic robotic manipulation tasks, compared to 74.2% for traditional MoE models. Furthermore, DRAE maintains an extremely low forgetting rate, outperforming state-of-the-art methods in catastrophic forgetting mitigation. These results demonstrate the effectiveness of our approach in enabling flexible, scalable, and efficient lifelong learning for robotics. </p>
<blockquote>
<p>æˆ‘ä»¬ä»‹ç»äº†åŠ¨æ€æ£€ç´¢å¢å¼ºä¸“å®¶ç½‘ç»œï¼ˆDRAEï¼‰ï¼Œè¿™æ˜¯ä¸€ç§çªç ´æ€§æ¶æ„ï¼Œå®ƒé€šè¿‡ç»“åˆä¸“å®¶æ··åˆï¼ˆMoEï¼‰çš„åŠ¨æ€è·¯ç”±èƒ½åŠ›ï¼›åˆ©ç”¨æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰çš„çŸ¥è¯†å¢å¼ºèƒ½åŠ›ï¼›èå…¥æ–°å‹åˆ†å±‚å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰æ¡†æ¶ï¼›ä»¥åŠé€šè¿‡ReflexNet-SchemaPlanner-HyperOptimaï¼ˆRSHOï¼‰è¿›è¡Œåè°ƒï¼Œè§£å†³äº†ç»ˆèº«å­¦ä¹ ã€ç¾éš¾æ€§é—å¿˜å’Œä»»åŠ¡é€‚åº”æ–¹é¢çš„æŒ‘æˆ˜ã€‚DRAEé€šè¿‡ç¨€ç–çš„MoEé—¨æ§æœºåˆ¶åŠ¨æ€è·¯ç”±ä¸“å®¶æ¨¡å‹ï¼Œå®ç°æœ‰æ•ˆçš„èµ„æºé…ç½®ï¼ŒåŒæ—¶åˆ©ç”¨å‚æ•°æ£€ç´¢ï¼ˆP-RAGï¼‰æ¥å¢å¼ºå­¦ä¹ è¿‡ç¨‹ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°å‹çš„RLæ¡†æ¶ï¼Œå…¶ä¸­åŒ…æ‹¬ç”¨äºä½çº§ä»»åŠ¡æ‰§è¡Œçš„ReflexNetã€ç”¨äºç¬¦å·æ¨ç†çš„SchemaPlannerå’Œç”¨äºé•¿æœŸä¸Šä¸‹æ–‡å»ºæ¨¡çš„HyperOptimaï¼Œç¡®ä¿æŒç»­çš„é€‚åº”æ€§å’Œè®°å¿†åŠ›ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒDRAEåœ¨é•¿æœŸä»»åŠ¡ä¿æŒå’ŒçŸ¥è¯†å†åˆ©ç”¨æ–¹é¢æ˜¾è‘—ä¼˜äºåŸºå‡†æ–¹æ³•ï¼Œåœ¨ä¸€ç³»åˆ—åŠ¨æ€æœºå™¨äººæ“ä½œä»»åŠ¡ä¸Šå¹³å‡ä»»åŠ¡æˆåŠŸç‡ä¸º82.5%ï¼Œè€Œä¼ ç»ŸMoEæ¨¡å‹ä¸º74.2%ã€‚æ­¤å¤–ï¼ŒDRAEä¿æŒæä½çš„é—å¿˜ç‡ï¼Œåœ¨ç¾éš¾æ€§é—å¿˜å‡è½»æ–¹é¢ä¼˜äºæœ€æ–°æŠ€æœ¯ã€‚è¿™äº›ç»“æœè¯æ˜äº†æˆ‘ä»¬çš„æ–¹æ³•åœ¨æœºå™¨äººç»ˆèº«å­¦ä¹ ä¸­çš„çµæ´»æ€§ã€å¯æ‰©å±•æ€§å’Œæ•ˆç‡æ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.04661v1">PDF</a> Accepted to the main conference of the Annual Meeting of the   Association for Computational Linguistics (ACL 2025)</p>
<p><strong>Summary</strong>ï¼š</p>
<p>DRAEæ˜¯ä¸€ä¸ªç»“åˆäº†Mixture-of-Expertsï¼ˆMoEï¼‰åŠ¨æ€è·¯ç”±èƒ½åŠ›ã€æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰çš„çŸ¥è¯†å¢å¼ºèƒ½åŠ›ã€æ–°å‹åˆ†å±‚å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰æ¡†æ¶å’ŒReflexNet-SchemaPlanner-HyperOptimaï¼ˆRSHOï¼‰åè°ƒæœºåˆ¶çš„åˆ›æ–°æ¶æ„ã€‚å®ƒè§£å†³äº†ç»ˆç”Ÿå­¦ä¹ ã€ç¾éš¾æ€§é—å¿˜å’Œä»»åŠ¡é€‚åº”æ€§æ–¹é¢çš„æŒ‘æˆ˜ã€‚é€šè¿‡åŠ¨æ€è·¯ç”±ä¸“å®¶æ¨¡å‹å’Œåˆ©ç”¨å‚æ•°æ£€ç´¢å¢å¼ºå­¦ä¹ ï¼ŒDRAEåœ¨èµ„æºåˆ†é…å’Œå¤–éƒ¨çŸ¥è¯†åˆ©ç”¨æ–¹é¢è¡¨ç°å‡ºè‰²ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒDRAEåœ¨é•¿æœŸä»»åŠ¡ä¿ç•™å’ŒçŸ¥è¯†å†åˆ©ç”¨æ–¹é¢æ˜¾è‘—ä¼˜äºä¼ ç»ŸMoEæ¨¡å‹ï¼Œåœ¨åŠ¨æ€æœºå™¨äººæ“ä½œä»»åŠ¡ä¸Šçš„å¹³å‡ä»»åŠ¡æˆåŠŸç‡è¾¾åˆ°82.5%ï¼Œå¹¶ä¸”ç»´æŒäº†æä½çš„é—å¿˜ç‡ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>DRAEæ˜¯ä¸€ä¸ªç»“åˆäº†å¤šç§æŠ€æœ¯çš„åˆ›æ–°æ¶æ„ï¼Œæ—¨åœ¨è§£å†³ç»ˆç”Ÿå­¦ä¹ ã€ç¾éš¾æ€§é—å¿˜å’Œä»»åŠ¡é€‚åº”æ€§çš„æŒ‘æˆ˜ã€‚</li>
<li>é€šè¿‡ç»“åˆMoEçš„åŠ¨æ€è·¯ç”±èƒ½åŠ›ï¼ŒDRAEå®ç°äº†é«˜æ•ˆçš„èµ„æºåˆ†é…ã€‚</li>
<li>åˆ©ç”¨RAGçš„æ£€ç´¢å¢å¼ºåŠŸèƒ½ï¼ŒDRAEèƒ½å¤Ÿåˆ©ç”¨å¤–éƒ¨çŸ¥è¯†æ¥å¢å¼ºå­¦ä¹ è¿‡ç¨‹ã€‚</li>
<li>å¼•å…¥æ–°å‹åˆ†å±‚å¼ºåŒ–å­¦ä¹ æ¡†æ¶ï¼Œé€šè¿‡ReflexNetã€SchemaPlannerå’ŒHyperOptimaçš„åè°ƒï¼Œå®ç°æŒç»­é€‚åº”å’Œè®°å¿†ä¿ç•™ã€‚</li>
<li>å®éªŒç»“æœæ˜¾ç¤ºï¼ŒDRAEåœ¨é•¿æœŸä»»åŠ¡ä¿ç•™å’ŒçŸ¥è¯†å†åˆ©ç”¨æ–¹é¢è¡¨ç°ä¼˜å¼‚ã€‚</li>
<li>åœ¨åŠ¨æ€æœºå™¨äººæ“ä½œä»»åŠ¡ä¸Šï¼ŒDRAEçš„å¹³å‡ä»»åŠ¡æˆåŠŸç‡è¾¾åˆ°82.5%ï¼Œä¼˜äºä¼ ç»ŸMoEæ¨¡å‹çš„74.2%ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.04661">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-87c7c6a488cac26d0b29a8b64682ace2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a326948bba005537ab38f800a94431b5.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-4edd0c05f3ddfb74de19b6fa4f0b5835.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="CoT-lized-Diffusion-Letâ€™s-Reinforce-T2I-Generation-Step-by-step"><a href="#CoT-lized-Diffusion-Letâ€™s-Reinforce-T2I-Generation-Step-by-step" class="headerlink" title="CoT-lized Diffusion: Letâ€™s Reinforce T2I Generation Step-by-step"></a>CoT-lized Diffusion: Letâ€™s Reinforce T2I Generation Step-by-step</h2><p><strong>Authors:Zheyuan Liu, Munan Ning, Qihui Zhang, Shuo Yang, Zhongrui Wang, Yiwei Yang, Xianzhe Xu, Yibing Song, Weihua Chen, Fan Wang, Li Yuan</strong></p>
<p>Current text-to-image (T2I) generation models struggle to align spatial composition with the input text, especially in complex scenes. Even layout-based approaches yield suboptimal spatial control, as their generation process is decoupled from layout planning, making it difficult to refine the layout during synthesis. We present CoT-Diff, a framework that brings step-by-step CoT-style reasoning into T2I generation by tightly integrating Multimodal Large Language Model (MLLM)-driven 3D layout planning with the diffusion process. CoT-Diff enables layout-aware reasoning inline within a single diffusion round: at each denoising step, the MLLM evaluates intermediate predictions, dynamically updates the 3D scene layout, and continuously guides the generation process. The updated layout is converted into semantic conditions and depth maps, which are fused into the diffusion model via a condition-aware attention mechanism, enabling precise spatial control and semantic injection. Experiments on 3D Scene benchmarks show that CoT-Diff significantly improves spatial alignment and compositional fidelity, and outperforms the state-of-the-art method by 34.7% in complex scene spatial accuracy, thereby validating the effectiveness of this entangled generation paradigm. </p>
<blockquote>
<p>å½“å‰æ–‡æœ¬åˆ°å›¾åƒï¼ˆT2Iï¼‰ç”Ÿæˆæ¨¡å‹åœ¨å¯¹é½ç©ºé—´æ„å›¾ä¸è¾“å…¥æ–‡æœ¬æ–¹é¢å­˜åœ¨å›°éš¾ï¼Œå°¤å…¶æ˜¯åœ¨å¤æ‚åœºæ™¯ä¸­ã€‚å³ä½¿åŸºäºå¸ƒå±€çš„æ–¹æ³•ä¹Ÿä¼šäº§ç”Ÿæ¬¡ä¼˜çš„ç©ºé—´æ§åˆ¶ï¼Œå› ä¸ºå®ƒä»¬çš„ç”Ÿæˆè¿‡ç¨‹ä¸å¸ƒå±€è§„åˆ’æ˜¯è§£è€¦çš„ï¼Œä½¿å¾—åœ¨åˆæˆè¿‡ç¨‹ä¸­éš¾ä»¥å¯¹å¸ƒå±€è¿›è¡Œå¾®è°ƒã€‚æˆ‘ä»¬æå‡ºäº†CoT-Diffæ¡†æ¶ï¼Œå®ƒé€šè¿‡ç´§å¯†é›†æˆå¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰é©±åŠ¨çš„3Då¸ƒå±€è§„åˆ’ä¸æ‰©æ•£è¿‡ç¨‹ï¼Œå°†é€æ­¥çš„CoTé£æ ¼æ¨ç†å¼•å…¥åˆ°T2Iç”Ÿæˆä¸­ã€‚CoT-Diffä½¿å¸ƒå±€æ„ŸçŸ¥æ¨ç†èƒ½å¤Ÿåœ¨å•ä¸ªæ‰©æ•£å›åˆå†…å®Œæˆï¼šåœ¨æ¯ä¸ªå»å™ªæ­¥éª¤ä¸­ï¼ŒMLLMè¯„ä¼°ä¸­é—´é¢„æµ‹ï¼ŒåŠ¨æ€æ›´æ–°3Dåœºæ™¯å¸ƒå±€ï¼Œå¹¶æŒç»­æŒ‡å¯¼ç”Ÿæˆè¿‡ç¨‹ã€‚æ›´æ–°çš„å¸ƒå±€è¢«è½¬æ¢ä¸ºè¯­ä¹‰æ¡ä»¶å’Œæ·±åº¦å›¾ï¼Œé€šè¿‡æ¡ä»¶æ„ŸçŸ¥æ³¨æ„åŠ›æœºåˆ¶èåˆåˆ°æ‰©æ•£æ¨¡å‹ä¸­ï¼Œä»è€Œå®ç°ç²¾ç¡®çš„ç©ºé—´æ§åˆ¶å’Œè¯­ä¹‰æ³¨å…¥ã€‚åœ¨3Dåœºæ™¯åŸºå‡†æµ‹è¯•ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒCoT-Diffåœ¨ç©ºé—´å¯¹é½å’Œç»„åˆä¿çœŸåº¦æ–¹é¢æ˜¾è‘—æ”¹è¿›ï¼Œå¹¶ä¸”åœ¨å¤æ‚åœºæ™¯çš„ç©ºé—´å‡†ç¡®æ€§æ–¹é¢æ¯”ç°æœ‰æŠ€æœ¯å…ˆè¿›æ–¹æ³•é«˜å‡º34.7%ï¼Œä»è€ŒéªŒè¯äº†è¿™ç§çº ç¼ ç”Ÿæˆæ¨¡å¼çš„æœ‰æ•ˆæ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.04451v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºä¸€ç§åä¸ºCoT-Diffçš„æ–‡æœ¬è½¬å›¾åƒç”Ÿæˆæ¡†æ¶ï¼Œå®ƒå°†å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹é©±åŠ¨çš„3Då¸ƒå±€è§„åˆ’ä¸æ‰©æ•£è¿‡ç¨‹ç´§å¯†ç»“åˆï¼Œå®ç°äº†é€æ­¥çš„CoTé£æ ¼æ¨ç†ã€‚è¯¥æ¡†æ¶èƒ½åœ¨å•ä¸ªæ‰©æ•£å›åˆå†…è¿›è¡Œå¸ƒå±€æ„ŸçŸ¥æ¨ç†ï¼Œå¹¶åœ¨æ¯ä¸ªå»å™ªæ­¥éª¤ä¸­åŠ¨æ€æ›´æ–°3Dåœºæ™¯å¸ƒå±€ï¼ŒæŒ‡å¯¼ç”Ÿæˆè¿‡ç¨‹ã€‚å®éªŒè¡¨æ˜ï¼ŒCoT-Diffèƒ½æ˜¾è‘—æé«˜ç©ºé—´å¯¹é½å’Œç»„åˆä¿çœŸåº¦ï¼Œå¹¶åœ¨å¤æ‚åœºæ™¯çš„ç©ºé—´å‡†ç¡®æ€§ä¸Šä¼˜äºç°æœ‰æŠ€æœ¯ï¼ŒéªŒè¯äº†è¿™ç§çº ç¼ ç”ŸæˆèŒƒå¼çš„æœ‰æ•ˆæ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å½“å‰æ–‡æœ¬è½¬å›¾åƒç”Ÿæˆæ¨¡å‹åœ¨å¤æ‚åœºæ™¯ä¸­çš„ç©ºé—´ç»„åˆå¯¹é½å­˜åœ¨æŒ‘æˆ˜ã€‚</li>
<li>CoT-Diffæ¡†æ¶é€šè¿‡ç»“åˆå¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹é©±åŠ¨çš„3Då¸ƒå±€è§„åˆ’ä¸æ‰©æ•£è¿‡ç¨‹ï¼Œå®ç°é€æ­¥çš„CoTé£æ ¼æ¨ç†ã€‚</li>
<li>CoT-Diffåœ¨å•ä¸ªæ‰©æ•£å›åˆå†…è¿›è¡Œå¸ƒå±€æ„ŸçŸ¥æ¨ç†ï¼Œå®ç°æ›´ç²¾å‡†çš„ç©ºé—´æ§åˆ¶ã€‚</li>
<li>åœ¨æ¯ä¸ªå»å™ªæ­¥éª¤ä¸­ï¼ŒMLLMè¯„ä¼°ä¸­é—´é¢„æµ‹ï¼ŒåŠ¨æ€æ›´æ–°3Dåœºæ™¯å¸ƒå±€ã€‚</li>
<li>æ¡ä»¶æ„ŸçŸ¥æ³¨æ„æœºåˆ¶å°†æ›´æ–°çš„å¸ƒå±€è½¬æ¢ä¸ºè¯­ä¹‰æ¡ä»¶å’Œæ·±åº¦å›¾ï¼Œèå…¥æ‰©æ•£æ¨¡å‹ã€‚</li>
<li>CoT-Diffæ˜¾è‘—æé«˜ç©ºé—´å¯¹é½å’Œç»„åˆä¿çœŸåº¦ï¼ŒéªŒè¯çº ç¼ ç”ŸæˆèŒƒå¼çš„æœ‰æ•ˆæ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.04451">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-87460ac0ca7148d3f884049e6b5e4782.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6ef185e8d8d6e0d9601eba85b715101a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-3ae4f783a5c50d3a40bd32ccfeee61a2.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b6d981d95996e2776cb0f1ebbb515eb2.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="Answer-Set-Programming-Modulo-Theories-and-Reasoning-about-Continuous-Changes"><a href="#Answer-Set-Programming-Modulo-Theories-and-Reasoning-about-Continuous-Changes" class="headerlink" title="Answer Set Programming Modulo Theories and Reasoning about Continuous   Changes"></a>Answer Set Programming Modulo Theories and Reasoning about Continuous   Changes</h2><p><strong>Authors:Joohyung Lee, Yunsong Meng</strong></p>
<p>Answer Set Programming Modulo Theories (ASPMT) is a new framework of tight integration of answer set programming (ASP) and satisfiability modulo theories (SMT). Similar to the relationship between first-order logic and SMT, it is based on a recent proposal of the functional stable model semantics by fixing interpretations of background theories. Analogously to a known relationship between ASP and SAT, &#96;&#96;tightâ€™â€™ ASPMT programs can be translated into SMT instances. We demonstrate the usefulness of ASPMT by enhancing action language C+ to handle continuous changes as well as discrete changes. We reformulate the semantics of C+ in terms ofASPMT, and show that SMT solvers can be used to compute the language. We also show how the language can represent cumulative effects on continuous resources. </p>
<blockquote>
<p>ç­”æ¡ˆé›†è§„åˆ’æ¨¡è®ºï¼ˆASPMTï¼‰æ˜¯ç­”æ¡ˆé›†è§„åˆ’ï¼ˆASPï¼‰å’Œå¯æ»¡è¶³æ€§æ¨¡è®ºï¼ˆSMTï¼‰ç´§å¯†é›†æˆçš„å…¨æ–°æ¡†æ¶ã€‚å®ƒç±»ä¼¼äºä¸€é˜¶é€»è¾‘ä¸SMTä¹‹é—´çš„å…³ç³»ï¼ŒåŸºäºèƒŒæ™¯ç†è®ºçš„è§£é‡Šå›ºå®šåŠŸèƒ½ç¨³å®šæ¨¡å‹è¯­ä¹‰çš„æœ€æ–°æè®®ã€‚ä¸ASPå’ŒSATä¹‹é—´çš„å·²çŸ¥å…³ç³»ç±»ä¼¼ï¼Œâ€œç´§å¯†â€çš„ASPMTç¨‹åºå¯ä»¥è½¬æ¢ä¸ºSMTå®ä¾‹ã€‚æˆ‘ä»¬é€šè¿‡å¢å¼ºåŠ¨ä½œè¯­è¨€C+æ¥å¤„ç†è¿ç»­å˜åŒ–å’Œç¦»æ•£å˜åŒ–ï¼Œå±•ç¤ºäº†ASPMTçš„å®ç”¨æ€§ã€‚æˆ‘ä»¬æ ¹æ®ASPMTé‡æ–°å®šä¹‰äº†C+çš„è¯­ä¹‰ï¼Œå¹¶å±•ç¤ºäº†SMTæ±‚è§£å™¨å¯ä»¥ç”¨äºè®¡ç®—è¯¥è¯­è¨€ã€‚æˆ‘ä»¬è¿˜å±•ç¤ºäº†è¯¥è¯­è¨€å¦‚ä½•è¡¨ç¤ºå¯¹è¿ç»­èµ„æºçš„ç´¯ç§¯å½±å“ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.04299v1">PDF</a> In Proceedings of the 23rd International Joint Conference on   Artificial Intelligence (IJCAI 2013), pages 990-996, 2013</p>
<p><strong>Summary</strong></p>
<p>ASPMTï¼ˆç­”æ¡ˆé›†ç¼–ç¨‹æ¨¡ç†è®ºï¼‰æ˜¯ç­”æ¡ˆé›†ç¼–ç¨‹ï¼ˆASPï¼‰å’Œå¯æ»¡è¶³æ€§æ¨¡ç†è®ºï¼ˆSMTï¼‰ç´§å¯†ç»“åˆçš„æ–°æ¡†æ¶ã€‚å®ƒåŸºäºæœ€è¿‘æå‡ºçš„åŠŸèƒ½ç¨³å®šæ¨¡å‹è¯­ä¹‰ï¼Œé€šè¿‡è§£é‡ŠèƒŒæ™¯ç†è®ºæ¥å®ç°ä¸SMTçš„å…³è”ã€‚ç±»ä¼¼äºASPä¸SATä¹‹é—´çš„å…³ç³»ï¼Œç´§å¯†çš„ASPMTç¨‹åºå¯ä»¥è½¬åŒ–ä¸ºSMTå®ä¾‹ã€‚é€šè¿‡å¢å¼ºåŠ¨ä½œè¯­è¨€C+æ¥å¤„ç†è¿ç»­å˜åŒ–å’Œç¦»æ•£å˜åŒ–ï¼Œå±•ç¤ºäº†ASPMTçš„å®ç”¨æ€§ã€‚æˆ‘ä»¬ä»¥ASPMTé‡æ–°è¡¨è¿°äº†C+çš„è¯­ä¹‰ï¼Œå¹¶å±•ç¤ºäº†å¦‚ä½•ä½¿ç”¨SMTæ±‚è§£å™¨æ¥è®¡ç®—è¯­è¨€ã€‚åŒæ—¶ï¼Œå±•ç¤ºäº†è¯¥è¯­è¨€å¦‚ä½•è¡¨ç¤ºå¯¹è¿ç»­èµ„æºçš„ç´¯ç§¯å½±å“ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ASPMTæ˜¯ASPå’ŒSMTçš„ç´§å¯†ç»“åˆæ¡†æ¶ã€‚</li>
<li>ASPMTåŸºäºåŠŸèƒ½ç¨³å®šæ¨¡å‹è¯­ä¹‰ï¼Œè§£é‡ŠèƒŒæ™¯ç†è®ºã€‚</li>
<li>ç´§å¯†çš„ASPMTç¨‹åºå¯è½¬åŒ–ä¸ºSMTå®ä¾‹ã€‚</li>
<li>C+è¯­è¨€é€šè¿‡ASPMTå¾—åˆ°å¢å¼ºï¼Œèƒ½å¤„ç†è¿ç»­å’Œç¦»æ•£å˜åŒ–ã€‚</li>
<li>ä½¿ç”¨SMTæ±‚è§£å™¨è®¡ç®—åŸºäºASPMTçš„è¯­è¨€ã€‚</li>
<li>è¯¥è¯­è¨€èƒ½è¡¨ç¤ºå¯¹è¿ç»­èµ„æºçš„ç´¯ç§¯å½±å“ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.04299">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-2e3ac48e6ed9081bb2ead94357155fc9.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-c64b230e8c7856bc1d23adab1c993aeb.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-31f8619f1ac7f4038d384d544c12410f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2472b0dda31bde9e48faaad8c9bf96cf.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-58d8305acdbac7c5506adb6851ffe98a.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="M-3-Med-A-Benchmark-for-Multi-lingual-Multi-modal-and-Multi-hop-Reasoning-in-Medical-Instructional-Video-Understanding"><a href="#M-3-Med-A-Benchmark-for-Multi-lingual-Multi-modal-and-Multi-hop-Reasoning-in-Medical-Instructional-Video-Understanding" class="headerlink" title="M$^3$-Med: A Benchmark for Multi-lingual, Multi-modal, and Multi-hop   Reasoning in Medical Instructional Video Understanding"></a>M$^3$-Med: A Benchmark for Multi-lingual, Multi-modal, and Multi-hop   Reasoning in Medical Instructional Video Understanding</h2><p><strong>Authors:Shenxi Liu, Kan Li, Mingyang Zhao, Yuhang Tian, Bin Li, Shoujun Zhou, Hongliang Li, Fuxia Yang</strong></p>
<p>With the rapid progress of artificial intelligence (AI) in multi-modal understanding, there is increasing potential for video comprehension technologies to support professional domains such as medical education. However, existing benchmarks suffer from two primary limitations: (1) Linguistic Singularity: they are largely confined to English, neglecting the need for multilingual resources; and (2) Shallow Reasoning: their questions are often designed for surface-level information retrieval, failing to properly assess deep multi-modal integration. To address these limitations, we present M3-Med, the first benchmark for Multi-lingual, Multi-modal, and Multi-hop reasoning in Medical instructional video understanding. M3-Med consists of medical questions paired with corresponding video segments, annotated by a team of medical experts. A key innovation of M3-Med is its multi-hop reasoning task, which requires a model to first locate a key entity in the text, then find corresponding visual evidence in the video, and finally synthesize information across both modalities to derive the answer. This design moves beyond simple text matching and poses a substantial challenge to a modelâ€™s deep cross-modal understanding capabilities. We define two tasks: Temporal Answer Grounding in Single Video (TAGSV) and Temporal Answer Grounding in Video Corpus (TAGVC). We evaluated several state-of-the-art models and Large Language Models (LLMs) on M3-Med. The results reveal a significant performance gap between all models and human experts, especially on the complex multi-hop questions where model performance drops sharply. M3-Med effectively highlights the current limitations of AI models in deep cross-modal reasoning within specialized domains and provides a new direction for future research. </p>
<blockquote>
<p>éšç€äººå·¥æ™ºèƒ½åœ¨å¤šæ¨¡æ€ç†è§£æ–¹é¢çš„å¿«é€Ÿå‘å±•ï¼Œè§†é¢‘ç†è§£æŠ€æœ¯åœ¨åŒ»ç–—æ•™è‚²ç­‰ä¸“ä¸šé¢†åŸŸçš„åº”ç”¨æ½œåŠ›è¶Šæ¥è¶Šå¤§ã€‚ç„¶è€Œï¼Œç°æœ‰çš„åŸºå‡†æµ‹è¯•å­˜åœ¨ä¸¤ä¸ªä¸»è¦å±€é™æ€§ï¼šï¼ˆ1ï¼‰è¯­è¨€å•ä¸€æ€§ï¼šå®ƒä»¬å¤§å¤šä»…é™äºè‹±è¯­ï¼Œå¿½è§†äº†å¤šè¯­è¨€èµ„æºçš„éœ€æ±‚ï¼›ï¼ˆ2ï¼‰æµ…å±‚æ¨ç†ï¼šå®ƒä»¬çš„é—®é¢˜å¾€å¾€æ˜¯ä¸ºäº†è¡¨é¢ä¿¡æ¯çš„æ£€ç´¢è€Œè®¾è®¡çš„ï¼Œæ— æ³•é€‚å½“åœ°è¯„ä¼°æ·±åº¦å¤šæ¨¡æ€èåˆã€‚ä¸ºäº†è§£å†³è¿™äº›å±€é™æ€§ï¼Œæˆ‘ä»¬æå‡ºäº†M3-Medï¼Œè¿™æ˜¯åŒ»ç–—æ•™å­¦è§†é¢‘ç†è§£ä¸­å¤šè¯­è¨€ã€å¤šæ¨¡æ€ã€å¤šè·³æ¨ç†çš„é¦–ä¸ªåŸºå‡†æµ‹è¯•ã€‚M3-Medç”±åŒ»ç–—é—®é¢˜åŠå…¶å¯¹åº”çš„è§†é¢‘ç‰‡æ®µç»„æˆï¼Œç”±åŒ»ç–—ä¸“å®¶å›¢é˜Ÿè¿›è¡Œæ³¨é‡Šã€‚M3-Medçš„å…³é”®åˆ›æ–°ä¹‹å¤„åœ¨äºå…¶å¤šè·³æ¨ç†ä»»åŠ¡ï¼Œè¿™éœ€è¦æ¨¡å‹é¦–å…ˆå®šä½æ–‡æœ¬ä¸­çš„å…³é”®å®ä½“ï¼Œç„¶ååœ¨è§†é¢‘ä¸­æ‰¾åˆ°ç›¸åº”çš„è§†è§‰è¯æ®ï¼Œæœ€åèåˆä¸¤ç§æ¨¡å¼çš„ä¿¡æ¯æ¥å¾—å‡ºç­”æ¡ˆã€‚è¿™ç§è®¾è®¡è¶…è¶Šäº†ç®€å•çš„æ–‡æœ¬åŒ¹é…ï¼Œå¯¹æ¨¡å‹çš„è·¨æ¨¡æ€æ·±åº¦ç†è§£èƒ½åŠ›æå‡ºäº†å®è´¨æ€§çš„æŒ‘æˆ˜ã€‚æˆ‘ä»¬å®šä¹‰äº†ä¸¤ä¸ªä»»åŠ¡ï¼šå•è§†é¢‘æ—¶åºç­”æ¡ˆå®šä½ï¼ˆTAGSVï¼‰å’Œè§†é¢‘è¯­æ–™åº“æ—¶åºç­”æ¡ˆå®šä½ï¼ˆTAGVCï¼‰ã€‚æˆ‘ä»¬åœ¨M3-Medä¸Šè¯„ä¼°äº†å‡ ç§æœ€å‰æ²¿çš„æ¨¡å‹å’Œå¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ã€‚ç»“æœè¡¨æ˜ï¼Œæ‰€æœ‰æ¨¡å‹ä¸äººç±»ä¸“å®¶ä¹‹é—´å­˜æ˜¾è‘—çš„æ€§èƒ½å·®è·ï¼Œå°¤å…¶æ˜¯åœ¨å¤æ‚çš„å¤šè·³é—®é¢˜ä¸Šï¼Œæ¨¡å‹æ€§èƒ½æ€¥å‰§ä¸‹é™ã€‚M3-Medæœ‰æ•ˆåœ°çªå‡ºäº†äººå·¥æ™ºèƒ½æ¨¡å‹åœ¨ä¸“ä¸šé¢†åŸŸçš„æ·±åº¦è·¨æ¨¡æ€æ¨ç†æ–¹é¢çš„å½“å‰å±€é™æ€§ï¼Œå¹¶ä¸ºæœªæ¥çš„ç ”ç©¶æä¾›äº†æ–°çš„æ–¹å‘ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.04289v1">PDF</a> 19 pages, 8 figures, 7 tables</p>
<p><strong>Summary</strong></p>
<p>éšç€äººå·¥æ™ºèƒ½åœ¨å¤šæ¨¡æ€ç†è§£æ–¹é¢çš„å¿«é€Ÿå‘å±•ï¼Œè§†é¢‘ç†è§£æŠ€æœ¯åœ¨åŒ»ç–—æ•™è‚²ç­‰èŒä¸šé¢†åŸŸçš„åº”ç”¨æ½œåŠ›æ—¥ç›Šå¢å¼ºã€‚ç„¶è€Œï¼Œç°æœ‰çš„åŸºå‡†æµ‹è¯•ä¸»è¦å­˜åœ¨ä¸¤ä¸ªå±€é™æ€§ï¼šä¸€æ˜¯è¯­è¨€å•ä¸€æ€§ï¼Œå¤§éƒ¨åˆ†å±€é™äºè‹±è¯­ï¼Œå¿½è§†äº†å¤šè¯­è¨€èµ„æºçš„éœ€è¦ï¼›äºŒæ˜¯æµ…å±‚æ¨ç†ï¼Œé—®é¢˜è®¾è®¡å¾€å¾€åªé’ˆå¯¹è¡¨é¢ä¿¡æ¯çš„æ£€ç´¢ï¼Œæ— æ³•æ°å½“è¯„ä¼°æ·±åº¦å¤šæ¨¡æ€èåˆã€‚ä¸ºè§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æ¨å‡ºäº†M3-Medï¼Œè¿™æ˜¯é¦–ä¸ªé’ˆå¯¹åŒ»ç–—æ•™å­¦è§†é¢‘ç†è§£çš„å¤šè¯­è¨€ã€å¤šæ¨¡æ€ã€å¤šè·³æ¨ç†çš„åŸºå‡†æµ‹è¯•ã€‚M3-Medç”±åŒ»ç–—é—®é¢˜åŠå…¶å¯¹åº”çš„è§†é¢‘ç‰‡æ®µç»„æˆï¼Œç”±åŒ»ç–—ä¸“å®¶å›¢é˜Ÿè¿›è¡Œæ³¨é‡Šã€‚å…¶å…³é”®åˆ›æ–°åœ¨äºå¤šè·³æ¨ç†ä»»åŠ¡ï¼Œéœ€è¦æ¨¡å‹é¦–å…ˆåœ¨æ–‡æœ¬ä¸­æ‰¾åˆ°å…³é”®å®ä½“ï¼Œç„¶ååœ¨è§†é¢‘ä¸­æ‰¾åˆ°ç›¸åº”çš„è§†è§‰è¯æ®ï¼Œæœ€åèåˆä¸¤ç§æ¨¡å¼çš„ä¿¡æ¯å¾—å‡ºç­”æ¡ˆã€‚æˆ‘ä»¬å®šä¹‰äº†ä¸¤ä¸ªä»»åŠ¡ï¼šå•è§†é¢‘ä¸­çš„æ—¶é—´ç­”æ¡ˆå®šä½ï¼ˆTAGSVï¼‰å’Œè§†é¢‘è¯­æ–™åº“ä¸­çš„æ—¶é—´ç­”æ¡ˆå®šä½ï¼ˆTAGVCï¼‰ã€‚æˆ‘ä»¬å¯¹ä¸€äº›æœ€å…ˆè¿›çš„æ¨¡å‹å’Œå¤§è¯­è¨€æ¨¡å‹åœ¨M3-Medä¸Šè¿›è¡Œäº†è¯„ä¼°ã€‚ç»“æœæ˜¾ç¤ºï¼Œåœ¨æ‰€æœ‰æ¨¡å‹ä¸­ï¼Œä¸äººç±»çš„ä¸“å®¶ç›¸æ¯”ï¼Œæ¨¡å‹çš„æ€§èƒ½ä»å­˜åœ¨æ˜¾è‘—å·®è·ï¼Œç‰¹åˆ«æ˜¯åœ¨å¤æ‚çš„å¤šè·³é—®é¢˜ä¸Šï¼Œæ¨¡å‹æ€§èƒ½æ€¥å‰§ä¸‹é™ã€‚M3-Medæœ‰æ•ˆåœ°æŒ‡å‡ºäº†å½“å‰äººå·¥æ™ºèƒ½æ¨¡å‹åœ¨ç‰¹å®šé¢†åŸŸçš„æ·±åº¦è·¨æ¨¡æ€æ¨ç†æ–¹é¢çš„å±€é™æ€§ï¼Œå¹¶ä¸ºæœªæ¥çš„ç ”ç©¶æä¾›äº†æ–°çš„æ–¹å‘ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>äººå·¥æ™ºèƒ½åœ¨å¤šæ¨¡æ€ç†è§£æ–¹é¢å–å¾—è¿›å±•ï¼Œè§†é¢‘ç†è§£æŠ€æœ¯åœ¨åŒ»ç–—æ•™è‚²ç­‰é¢†åŸŸæœ‰æ½œåŠ›ã€‚</li>
<li>ç°æœ‰åŸºå‡†æµ‹è¯•å­˜åœ¨å±€é™æ€§ï¼šä¸»è¦é¢å‘è‹±è¯­äººç¾¤å’Œæ³¨é‡æµ…å±‚ä¿¡æ¯æ£€ç´¢çš„é—®é¢˜è®¾è®¡ã€‚</li>
<li>M3-Medæ˜¯é¦–ä¸ªé’ˆå¯¹åŒ»ç–—æ•™å­¦è§†é¢‘ç†è§£çš„å¤šè¯­è¨€ã€å¤šæ¨¡æ€ã€å¤šè·³æ¨ç†çš„åŸºå‡†æµ‹è¯•ã€‚</li>
<li>M3-MedåŒ…æ‹¬ä¸¤ä¸ªä»»åŠ¡ï¼šå•è§†é¢‘ä¸­çš„æ—¶é—´ç­”æ¡ˆå®šä½ï¼ˆTAGSVï¼‰å’Œè§†é¢‘è¯­æ–™åº“ä¸­çš„æ—¶é—´ç­”æ¡ˆå®šä½ï¼ˆTAGVCï¼‰ã€‚æµ‹è¯•ä»»åŠ¡æ¶‰åŠæ‰¾åˆ°å…³é”®å®ä½“å’Œå¯¹åº”çš„è§†è§‰è¯æ®ä»¥åŠèåˆä¿¡æ¯å¾—å‡ºç­”æ¡ˆçš„è¿‡ç¨‹ã€‚</li>
<li>å½“å‰äººå·¥æ™ºèƒ½æ¨¡å‹åœ¨æ·±åº¦è·¨æ¨¡æ€æ¨ç†æ–¹é¢ä»è½åäºäººç±»ä¸“å®¶ï¼Œç‰¹åˆ«æ˜¯åœ¨å¤æ‚é—®é¢˜ä¸Šè¡¨ç°æ¬ ä½³ã€‚</li>
<li>M3-Medæ­ç¤ºäº†äººå·¥æ™ºèƒ½æ¨¡å‹åœ¨ç‰¹å®šé¢†åŸŸçš„å±€é™æ€§å¹¶ä¸ºæœªæ¥çš„ç ”ç©¶æä¾›äº†æ–°çš„æ–¹å‘ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.04289">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-74b79af820e0b5a091ae3d3e1b7cdb85.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-c67cecf06b06fb5a68f06cbf1b490f89.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-e849de9351923242464a9df148a26cb5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-649749b4ec94adaf5b0ac821759c3d39.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a04eff0907562556057c4b1792d03e55.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-920c96c0d23e1291679a1cb3e86234e9.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="FIXME-Towards-End-to-End-Benchmarking-of-LLM-Aided-Design-Verification"><a href="#FIXME-Towards-End-to-End-Benchmarking-of-LLM-Aided-Design-Verification" class="headerlink" title="FIXME: Towards End-to-End Benchmarking of LLM-Aided Design Verification"></a>FIXME: Towards End-to-End Benchmarking of LLM-Aided Design Verification</h2><p><strong>Authors:Gwok-Waa Wan, Shengchu Su, Ruihu Wang, Qixiang Chen, Sam-Zaak Wong, Mengnv Xing, Hefei Feng, Yubo Wang, Yinan Zhu, Jingyi Zhang, Jianmin Ye, Xinlai Wan, Tao Ni, Qiang Xu, Nan Guan, Zhe Jiang, Xi Wang, Yang Jun</strong></p>
<p>Despite the transformative potential of Large Language Models (LLMs) in hardware design, a comprehensive evaluation of their capabilities in design verification remains underexplored. Current efforts predominantly focus on RTL generation and basic debugging, overlooking the critical domain of functional verification, which is the primary bottleneck in modern design methodologies due to the rapid escalation of hardware complexity. We present FIXME, the first end-to-end, multi-model, and open-source evaluation framework for assessing LLM performance in hardware functional verification (FV) to address this crucial gap. FIXME introduces a structured three-level difficulty hierarchy spanning six verification sub-domains and 180 diverse tasks, enabling in-depth analysis across the design lifecycle. Leveraging a collaborative AI-human approach, we construct a high-quality dataset using 100% silicon-proven designs, ensuring comprehensive coverage of real-world challenges. Furthermore, we enhance the functional coverage by 45.57% through expert-guided optimization. By rigorously evaluating state-of-the-art LLMs such as GPT-4, Claude3, and LlaMA3, we identify key areas for improvement and outline promising research directions to unlock the full potential of LLM-driven automation in hardware design verification. The benchmark is available at <a target="_blank" rel="noopener" href="https://github.com/ChatDesignVerification/FIXME">https://github.com/ChatDesignVerification/FIXME</a>. </p>
<blockquote>
<p>å°½ç®¡å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨ç¡¬ä»¶è®¾è®¡æ–¹é¢å…·æœ‰å˜é©æ€§æ½œåŠ›ï¼Œä½†å¯¹å…¶åœ¨è®¾è®¡éªŒè¯æ–¹é¢çš„èƒ½åŠ›è¿›è¡Œå…¨é¢è¯„ä¼°çš„ç ”ç©¶ä»ç„¶ä¸è¶³ã€‚ç›®å‰çš„ç ”ç©¶ä¸»è¦é›†ä¸­å¯„å­˜å™¨ä¼ è¾“çº§åˆ«ï¼ˆRTLï¼‰ç”Ÿæˆå’ŒåŸºæœ¬è°ƒè¯•ä¸Šï¼Œå¿½è§†äº†åŠŸèƒ½éªŒè¯è¿™ä¸€å…³é”®é¢†åŸŸã€‚ç”±äºç¡¬ä»¶å¤æ‚æ€§è¿…é€Ÿå‡çº§ï¼ŒåŠŸèƒ½éªŒè¯å·²æˆä¸ºç°ä»£è®¾è®¡æ–¹æ³•è®ºä¸­çš„ä¸»è¦ç“¶é¢ˆã€‚ä¸ºäº†å¼¥è¡¥è¿™ä¸€å…³é”®ç©ºç™½ï¼Œæˆ‘ä»¬æå‡ºäº†FIXMEï¼Œè¿™æ˜¯ä¸€ä¸ªç«¯åˆ°ç«¯ã€å¤šæ¨¡å¼ã€å¼€æºçš„è¯„ä¼°æ¡†æ¶ï¼Œæ—¨åœ¨è¯„ä¼°LLMåœ¨ç¡¬ä»¶åŠŸèƒ½éªŒè¯ï¼ˆFVï¼‰ä¸­çš„æ€§èƒ½ã€‚FIXMEå¼•å…¥äº†ä¸€ä¸ªç»“æ„åŒ–çš„ä¸‰çº§éš¾åº¦å±‚æ¬¡ç»“æ„ï¼Œæ¶µç›–å…­ä¸ªéªŒè¯å­åŸŸå’Œ180ä¸ªå¤šæ ·åŒ–ä»»åŠ¡ï¼Œä»è€Œèƒ½å¤Ÿåœ¨æ•´ä¸ªè®¾è®¡ç”Ÿå‘½å‘¨æœŸä¸­è¿›è¡Œæ·±å…¥åˆ†æã€‚é€šè¿‡åä½œçš„äººæœºæ–¹æ³•ï¼Œæˆ‘ä»¬ä½¿ç”¨ç»è¿‡ç¡…éªŒè¯çš„100%è®¾è®¡æ„å»ºäº†ä¸€ä¸ªé«˜è´¨é‡æ•°æ®é›†ï¼Œç¡®ä¿å…¨é¢è¦†ç›–ç°å®ä¸–ç•Œä¸­çš„æŒ‘æˆ˜ã€‚æ­¤å¤–ï¼Œé€šè¿‡ä¸“å®¶æŒ‡å¯¼çš„ä¼˜åŒ–ï¼Œæˆ‘ä»¬æé«˜äº†åŠŸèƒ½è¦†ç›–ç‡45.57%ã€‚æˆ‘ä»¬ä¸¥æ ¼è¯„ä¼°äº†æœ€å…ˆè¿›çš„LLMï¼Œå¦‚GPT-4ã€Claude3å’ŒLlaMA3ç­‰ï¼Œç¡®å®šäº†æ”¹è¿›çš„å…³é”®é¢†åŸŸï¼Œå¹¶æŒ‡å‡ºäº†æœ‰å‰æ™¯çš„ç ”ç©¶æ–¹å‘ï¼Œä»¥é‡Šæ”¾LLMé©±åŠ¨è‡ªåŠ¨åŒ–åœ¨ç¡¬ä»¶è®¾è®¡éªŒè¯ä¸­çš„å…¨éƒ¨æ½œåŠ›ã€‚è¯¥åŸºå‡†æµ‹è¯•å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/ChatDesignVerification/FIXME">https://github.com/ChatDesignVerification/FIXME</a>è·å¾—ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.04276v1">PDF</a> </p>
<p><strong>Summary</strong>ï¼š</p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨ç¡¬ä»¶è®¾è®¡æ–¹é¢å…·æœ‰å˜é©æ€§æ½œåŠ›ï¼Œä½†åœ¨è®¾è®¡éªŒè¯æ–¹é¢çš„èƒ½åŠ›è¯„ä¼°ä»ç„¶ä¸è¶³ã€‚é’ˆå¯¹æ­¤é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†FIXMEè¯„ä¼°æ¡†æ¶ï¼Œè¯¥æ¡†æ¶æ—¨åœ¨è¯„ä¼°LLMåœ¨ç¡¬ä»¶åŠŸèƒ½éªŒè¯ï¼ˆFVï¼‰æ–¹é¢çš„æ€§èƒ½ã€‚FIXMEå¼•å…¥äº†ä¸€ä¸ªç»“æ„åŒ–ä¸‰çº§éš¾åº¦å±‚æ¬¡ç»“æ„ï¼Œæ¶µç›–å…­ä¸ªéªŒè¯å­åŸŸå’Œ180ä¸ªä¸åŒä»»åŠ¡ï¼Œå®ç°è®¾è®¡ç”Ÿå‘½å‘¨æœŸçš„æ·±åº¦åˆ†æã€‚é€šè¿‡åˆ©ç”¨åä½œå¼äººå·¥æ™ºèƒ½ä¸äººç±»çš„æ–¹æ³•ï¼Œæˆ‘ä»¬æ„å»ºäº†ä¸€ä¸ªé«˜è´¨é‡çš„æ•°æ®é›†ï¼Œç¡®ä¿å…¨é¢è¦†ç›–ç°å®æŒ‘æˆ˜ã€‚å¯¹æœ€å‰æ²¿çš„LLMå¦‚GPT-4ã€Claude3å’ŒLlaMA3è¿›è¡Œä¸¥æ ¼è¯„ä¼°ï¼Œæˆ‘ä»¬ç¡®å®šäº†å…³é”®æ”¹è¿›é¢†åŸŸï¼Œå¹¶æŒ‡å‡ºäº†åœ¨ç¡¬ä»¶è®¾è®¡éªŒè¯ä¸­è§£é”LLMé©±åŠ¨è‡ªåŠ¨åŒ–çš„æ½œåŠ›æ–¹å‘ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨ç¡¬ä»¶è®¾è®¡é¢†åŸŸå…·æœ‰å·¨å¤§æ½œåŠ›ï¼Œç‰¹åˆ«æ˜¯åœ¨è®¾è®¡éªŒè¯æ–¹é¢ã€‚</li>
<li>å½“å‰å¯¹LLMåœ¨ç¡¬ä»¶åŠŸèƒ½éªŒè¯æ–¹é¢çš„èƒ½åŠ›è¯„ä¼°ä»ç„¶ä¸è¶³ï¼Œéœ€è¦æ›´å¤šçš„ç ”ç©¶å…³æ³¨ã€‚</li>
<li>FIXMEæ˜¯é¦–ä¸ªé’ˆå¯¹ç¡¬ä»¶åŠŸèƒ½éªŒè¯è¯„ä¼°LLMæ€§èƒ½çš„å…¨æµç¨‹ã€å¤šæ¨¡å‹ã€å¼€æºæ¡†æ¶ã€‚</li>
<li>FIXMEå¼•å…¥äº†ä¸€ä¸ªç»“æ„åŒ–éš¾åº¦å±‚æ¬¡ç»“æ„ï¼Œæ¶µç›–å¤šä¸ªéªŒè¯å­åŸŸå’Œä¸åŒä»»åŠ¡ï¼Œä»¥è¿›è¡Œå…¨é¢åˆ†æã€‚</li>
<li>è¯¥æ¡†æ¶é‡‡ç”¨åä½œå¼äººå·¥æ™ºèƒ½ä¸äººç±»çš„æ–¹æ³•ï¼Œåˆ©ç”¨é«˜è´¨é‡æ•°æ®é›†ï¼Œç¡®ä¿ç°å®æŒ‘æˆ˜çš„å…¨é¢è¦†ç›–ã€‚</li>
<li>å¯¹ç°æœ‰LLMçš„è¯„ä¼°ç¡®å®šäº†å…³é”®æ”¹è¿›é¢†åŸŸï¼Œå¹¶æŒ‡å‡ºäº†æœªæ¥ç ”ç©¶çš„æ–¹å‘ã€‚</li>
<li>è¯¥è¯„ä¼°æ¡†æ¶æœ‰åŠ©äºè§£é”LLMåœ¨ç¡¬ä»¶è®¾è®¡éªŒè¯ä¸­çš„å…¨æ½œåŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.04276">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-ac9ed7c4c557e5d1874d11b4e193c591.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3ef8562f8edd1901f08eaf9641cf0dbe.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-069f9c263ce3292f09738671058433f7.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a98103ba1d2b9a1235cdc796bb8191e3.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-dc9086e2f0afb875e304fb4a2066d3dc.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="Can-Large-Language-Models-Automate-the-Refinement-of-Cellular-Network-Specifications"><a href="#Can-Large-Language-Models-Automate-the-Refinement-of-Cellular-Network-Specifications" class="headerlink" title="Can Large Language Models Automate the Refinement of Cellular Network   Specifications?"></a>Can Large Language Models Automate the Refinement of Cellular Network   Specifications?</h2><p><strong>Authors:Jianshuo Dong, Tianyi Zhang, Feng Yan, Yuanjie Li, Hewu Li, Han Qiu</strong></p>
<p>Cellular networks serve billions of users globally, yet concerns about reliability and security persist due to weaknesses in 3GPP standards. However, traditional analysis methods, including manual inspection and automated tools, struggle with increasingly expanding cellular network specifications. This paper investigates the feasibility of Large Language Models (LLMs) for automated cellular network specification refinement. To advance it, we leverage 200,000+ approved 3GPP Change Requests (CRs) that document specification revisions, constructing a valuable dataset for domain tasks. We introduce CR-eval, a principled evaluation framework, and benchmark 16 state-of-the-art LLMs, demonstrating that top models can discover security-related weaknesses in over 127 out of 200 test cases within five trials. To bridge potential gaps, we explore LLM specialization techniques, including fine-tuning an 8B model to match or surpass advanced LLMs like GPT-4o and DeepSeek-R1. Evaluations on 30 cellular attacks identify open challenges for achieving full automation. These findings confirm that LLMs can automate the refinement of cellular network specifications and provide valuable insights to guide future research in this direction. </p>
<blockquote>
<p>å…¨çƒèŒƒå›´å†…ï¼Œèœ‚çªç½‘ç»œä¸ºæ•°äº¿ç”¨æˆ·æä¾›æœåŠ¡ï¼Œä½†ç”±äºå¯¹ç¬¬ä¸‰ä»£åˆä½œä¼™ä¼´è®¡åˆ’ï¼ˆ3GPPï¼‰æ ‡å‡†çš„å¼±ç‚¹ï¼Œå…³äºå¯é æ€§å’Œå®‰å…¨çš„æ‹…å¿§ä¾ç„¶å­˜åœ¨ã€‚ç„¶è€Œï¼Œä¼ ç»Ÿåˆ†ææ–¹æ³•ï¼ŒåŒ…æ‹¬æ‰‹åŠ¨æ£€æŸ¥å’Œè‡ªåŠ¨åŒ–å·¥å…·ï¼Œåœ¨å¤„ç†ä¸æ–­æ‰©å±•çš„èœ‚çªç½‘ç»œè§„æ ¼æ—¶é‡åˆ°äº†å›°éš¾ã€‚æœ¬æ–‡ç ”ç©¶äº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨è‡ªåŠ¨èœ‚çªç½‘ç»œè§„æ ¼ä¼˜åŒ–æ–¹é¢çš„å¯è¡Œæ€§ã€‚ä¸ºäº†æ¨è¿›è¿™ä¸€ç ”ç©¶ï¼Œæˆ‘ä»¬åˆ©ç”¨è¶…è¿‡äºŒåä¸‡ä»½ç»è¿‡æ‰¹å‡†çš„3GPPå˜æ›´è¯·æ±‚ï¼ˆCRï¼‰ï¼Œè¿™äº›å˜æ›´è¯·æ±‚è®°å½•äº†è§„æ ¼ä¿®è®¢æƒ…å†µï¼Œæ„å»ºäº†é’ˆå¯¹é¢†åŸŸä»»åŠ¡çš„å®è´µæ•°æ®é›†ã€‚æˆ‘ä»¬ä»‹ç»äº†CRè¯„ä¼°åŸåˆ™æ¡†æ¶CR-evalï¼Œå¹¶å¯¹16æ¬¾æœ€æ–°å¤§å‹è¯­è¨€æ¨¡å‹è¿›è¡Œäº†åŸºå‡†æµ‹è¯•ã€‚ç»“æœè¡¨æ˜ï¼Œé¡¶çº§æ¨¡å‹åœ¨äº”è½®æµ‹è¯•ä¸­èƒ½åœ¨è¶…è¿‡127ä¸ªæµ‹è¯•æ¡ˆä¾‹ä¸­æ‰¾å‡ºä¸å®‰å…¨æ€§ç›¸å…³çš„å¼±ç‚¹ã€‚ä¸ºäº†å¼¥åˆæ½œåœ¨å·®è·ï¼Œæˆ‘ä»¬æ¢ç´¢äº†å¤§å‹è¯­è¨€æ¨¡å‹çš„ç‰¹æ®ŠåŒ–æŠ€æœ¯ï¼ŒåŒ…æ‹¬å¾®è°ƒä¸€ä¸ªæ‹¥æœ‰åºå¤§è§„æ¨¡çš„å¤§å‹è¯­è¨€æ¨¡å‹æ¨¡å‹æ¥è¾¾åˆ°æˆ–è¶…è¶Šè¯¸å¦‚GPT-4oå’ŒDeepSeek-Rç­‰å°–ç«¯å¤§å‹è¯­è¨€æ¨¡å‹çš„æ°´å¹³ã€‚é€šè¿‡å¯¹ä¸‰åç§èœ‚çªæ”»å‡»çš„è¯„ä»·å‘ç°å®ç°å®Œå…¨è‡ªåŠ¨åŒ–çš„å¼€æ”¾æŒ‘æˆ˜ã€‚è¿™äº›å‘ç°è¯å®å¤§å‹è¯­è¨€æ¨¡å‹èƒ½å¤Ÿè‡ªåŠ¨åŒ–èœ‚çªç½‘ç»œè§„æ ¼çš„å®Œå–„å·¥ä½œï¼Œå¹¶ä¸ºæœªæ¥è¿™ä¸€æ–¹å‘çš„ç ”ç©¶æä¾›äº†å®è´µçš„è§è§£ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.04214v1">PDF</a> </p>
<p><strong>Summary</strong>ï¼š<br>éšç€å…¨çƒç”¨æˆ·æ•°é‡çš„å¢é•¿ï¼Œå¯¹èœ‚çªç½‘ç»œå¯é æ€§åŠå®‰å…¨æ€§çš„æ‹…å¿§ä¹Ÿåœ¨å¢åŠ ï¼Œç‰¹åˆ«æ˜¯åœ¨3GPPæ ‡å‡†ä¸Šçš„å¼±ç‚¹ã€‚ä¼ ç»Ÿåˆ†ææ–¹æ³•å¦‚æ‰‹åŠ¨æ£€æŸ¥å’Œè‡ªåŠ¨åŒ–å·¥å…·å·²æ— æ³•æ»¡è¶³æ—¥ç›Šæ‰©å¤§çš„èœ‚çªç½‘ç»œè§„æ ¼çš„éœ€æ±‚ã€‚æœ¬ç ”ç©¶åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰æ¢ç´¢è‡ªåŠ¨åŒ–èœ‚çªç½‘ç»œè§„æ ¼ä¼˜åŒ–çš„å¯è¡Œæ€§ã€‚æˆ‘ä»¬ä½¿ç”¨äº†è¶…è¿‡äºŒåä¸‡ä»½æ‰¹å‡†çš„3GPPæ›´æ”¹è¯·æ±‚ï¼ˆCRsï¼‰ï¼Œæ„å»ºäº†å®è´µçš„æ•°æ®é›†ï¼Œä¸ºä»»åŠ¡é¢†åŸŸæä¾›æ”¯æŒã€‚æˆ‘ä»¬å¼•å…¥äº†CR-evalè¯„ä¼°æ¡†æ¶ï¼Œå¹¶å¯¹åå…­ç§æœ€æ–°çš„å¤§å‹è¯­è¨€æ¨¡å‹è¿›è¡Œäº†åŸºå‡†æµ‹è¯•ã€‚ç»“æœæ˜¾ç¤ºï¼Œé¡¶çº§æ¨¡å‹åœ¨äº”è½®æµ‹è¯•ä¸­èƒ½åœ¨è¶…è¿‡ä¸€ç™¾äºŒåä¸ƒä¾‹æµ‹è¯•æ¡ˆä¾‹ä¸­å‘ç°äº†å®‰å…¨ç›¸å…³çš„å¼±ç‚¹ã€‚ä¸ºäº†ç¼©å°æ½œåœ¨å·®è·ï¼Œæˆ‘ä»¬æ¢ç´¢äº†å¤§å‹è¯­è¨€æ¨¡å‹çš„ç‰¹æ®ŠåŒ–æŠ€æœ¯ï¼ŒåŒ…æ‹¬å¾®è°ƒå¤§å‹è¯­è¨€æ¨¡å‹ä»¥é€‚åº”æˆ–è¶…è¶Šå…ˆè¿›çš„æ¨¡å‹å¦‚GPT-4oå’ŒDeepSeek-R1ã€‚å¯¹ä¸‰åç§èœ‚çªæ”»å‡»çš„è¯„ä»·æŒ‡å‡ºäº†å®ç°å…¨é¢è‡ªåŠ¨åŒ–çš„æŒ‘æˆ˜ã€‚è¿™äº›å‘ç°è¯å®å¤§å‹è¯­è¨€æ¨¡å‹èƒ½å¤Ÿè‡ªåŠ¨åŒ–èœ‚çªç½‘ç»œè§„æ ¼çš„å®Œå–„ï¼Œä¸ºæœªæ¥åœ¨è¿™ä¸€æ–¹å‘çš„ç ”ç©¶æä¾›äº†å®è´µçš„è§è§£ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>èœ‚çªç½‘ç»œé¢ä¸´å¯é æ€§åŠå®‰å…¨æ€§é—®é¢˜ï¼Œå°¤å…¶æ˜¯æ¶‰åŠ3GPPæ ‡å‡†çš„å¼±ç‚¹é—®é¢˜äºŸå¾…è§£å†³ã€‚</li>
<li>ä¼ ç»Ÿåˆ†ææ–¹æ³•åœ¨å¤„ç†å¤§è§„æ¨¡èœ‚çªç½‘ç»œè§„æ ¼æ—¶é¢ä¸´æŒ‘æˆ˜ã€‚</li>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨è‡ªåŠ¨åŒ–èœ‚çªç½‘ç»œè§„æ ¼ä¼˜åŒ–æ–¹é¢å…·æœ‰æ½œåŠ›ã€‚</li>
<li>åˆ©ç”¨å¤§é‡æ‰¹å‡†çš„3GPPæ›´æ”¹è¯·æ±‚æ„å»ºäº†æ•°æ®é›†ç”¨äºè®­ç»ƒå’Œä¼˜åŒ–å¤§å‹è¯­è¨€æ¨¡å‹ã€‚</li>
<li>é¡¶çº§çš„å¤§å‹è¯­è¨€æ¨¡å‹èƒ½å¤Ÿå‘ç°å¤§é‡å®‰å…¨ç›¸å…³çš„å¼±ç‚¹ï¼Œæ˜¾ç¤ºå…¶åœ¨è¯†åˆ«å®‰å…¨æ€§é—®é¢˜æ–¹é¢çš„æ•ˆèƒ½ã€‚</li>
<li>å­˜åœ¨æ½œåœ¨çš„ç¼ºé™·å’Œæ”¹è¿›ç©ºé—´ï¼Œå¯èƒ½éœ€è¦ç‰¹å®šçš„ç²¾ç»†åŒ–è°ƒæ•´æˆ–æ›´æ·±å…¥çš„æŠ€æœ¯æ”¹è¿›ä»¥åº”å¯¹æ‰€æœ‰èœ‚çªç½‘ç»œè§„æ ¼ä¼˜åŒ–çš„é—®é¢˜ã€‚ä¾‹å¦‚éœ€è¦è¿›ä¸€æ­¥æ¢ç´¢å¦‚ä½•æ”¹è¿›å¤§å‹è¯­è¨€æ¨¡å‹çš„ç‰¹æ®ŠåŒ–æŠ€æœ¯ï¼Œä»¥ä¾¿é€‚åº”ä¸åŒé¢†åŸŸéœ€æ±‚ï¼Œç‰¹åˆ«æ˜¯é¢å¯¹ç‰¹å®šçš„å®‰å…¨é—®é¢˜æ—¶å¦‚ä½•å®ç°æ›´å¥½çš„ä¼˜åŒ–æ•ˆæœã€‚åŒæ—¶è¿˜éœ€è¦è§£å†³å®ç°å…¨é¢è‡ªåŠ¨åŒ–çš„æŒ‘æˆ˜ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.04214">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-1370c3b56e02c7587027513f138722f3.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-9136bfbeaa93c6e3d906ae0db667a743.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-19d4e0a8e8a092de1a1e42f1c23e4034.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-dfaa0ed3853ca02a0f22f86c8e479f05.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="LVLM-Composerâ€™s-Explicit-Planning-for-Image-Generation"><a href="#LVLM-Composerâ€™s-Explicit-Planning-for-Image-Generation" class="headerlink" title="LVLM-Composerâ€™s Explicit Planning for Image Generation"></a>LVLM-Composerâ€™s Explicit Planning for Image Generation</h2><p><strong>Authors:Spencer Ramsey, Jeffrey Lee, Amina Grant</strong></p>
<p>The burgeoning field of generative artificial intelligence has fundamentally reshaped our approach to content creation, with Large Vision-Language Models (LVLMs) standing at its forefront. While current LVLMs have demonstrated impressive capabilities in text-to-image generation, they often falter when confronted with complex textual descriptions demanding precise compositional understanding and visual planning. This limitation particularly impacts the accurate rendering of multiple objects, their attributes, spatial relationships, and specific poses within intricate scenes, as evidenced by benchmarks like LongBench-T2I. To address these challenges, we introduce LVLM-Composer, a novel 10-billion parameter scale LVLM specifically engineered for enhanced compositional image synthesis. Our method incorporates a Hierarchical Semantic Planning Module for structured prompt decomposition and a Fine-Grained Feature Alignment Mechanism for precise visual guidance during generation. We propose a multi-stage training paradigm, featuring Hierarchical Semantic-Visual Grounding Pre-training and Compositional Planning Reinforcement Learning with Self-Correction, to instill robust compositional reasoning. Extensive experiments on the LongBench-T2I benchmark, utilizing automatic evaluation by Gemini-2.0-Flash and InternVL3-78B, demonstrate LVLM-Composerâ€™s superior performance across critical compositional dimensions including object accuracy, composition fidelity, and pose accuracy, significantly outperforming state-of-the-art baselines. An in-depth ablation study further validates the indispensable contribution of our proposed modules, while human evaluations confirm the perceptual superiority of our generated images. LVLM-Composer represents a significant step towards truly controllable and compositionally accurate open-ended text-to-image generation. </p>
<blockquote>
<p>ç”Ÿæˆäººå·¥æ™ºèƒ½é¢†åŸŸè“¬å‹ƒå‘å±•ï¼Œä»æ ¹æœ¬ä¸Šæ”¹å˜äº†æˆ‘ä»¬çš„å†…å®¹åˆ›ä½œæ–¹å¼ï¼Œå¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆLVLMsï¼‰å¤„äºå…¶å‰æ²¿åœ°ä½ã€‚è™½ç„¶å½“å‰çš„LVLMsåœ¨æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆæ–¹é¢è¡¨ç°å‡ºäº†ä»¤äººå°è±¡æ·±åˆ»çš„èƒ½åŠ›ï¼Œä½†å®ƒä»¬åœ¨é¢å¯¹å¤æ‚çš„æ–‡æœ¬æè¿°æ—¶ï¼Œå¾€å¾€ä¼šåœ¨éœ€è¦ç²¾ç¡®çš„ç»„åˆç†è§£å’Œè§†è§‰è§„åˆ’æ–¹é¢é‡åˆ°å›°éš¾ã€‚è¿™ä¸€å±€é™æ€§ç‰¹åˆ«å½±å“äº†åœ¨å¤æ‚åœºæ™¯ä¸­å¤šä¸ªç‰©ä½“ã€å±æ€§ã€ç©ºé—´å…³ç³»å’Œç‰¹å®šå§¿æ€çš„å‡†ç¡®å‘ˆç°ï¼ŒLongBench-T2Iç­‰åŸºå‡†æµ‹è¯•è¯æ˜äº†è¿™ä¸€ç‚¹ã€‚ä¸ºäº†è§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†LVLM-Composerï¼Œè¿™æ˜¯ä¸€ä¸ªä¸“é—¨ä¸ºå¢å¼ºç»„åˆå›¾åƒåˆæˆè€Œè®¾è®¡çš„æ–°å‹10äº¿å‚æ•°è§„æ¨¡LVLMã€‚æˆ‘ä»¬çš„æ–¹æ³•ç»“åˆäº†åˆ†å±‚è¯­ä¹‰è§„åˆ’æ¨¡å—è¿›è¡Œç»“æ„åŒ–æç¤ºåˆ†è§£å’Œç²¾ç»†ç‰¹å¾å¯¹é½æœºåˆ¶ï¼Œä»¥åœ¨ç”Ÿæˆè¿‡ç¨‹ä¸­æä¾›ç²¾ç¡®çš„è§†è§‰æŒ‡å¯¼ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§å¤šé˜¶æ®µè®­ç»ƒèŒƒå¼ï¼ŒåŒ…æ‹¬åˆ†å±‚è¯­ä¹‰-è§†è§‰æ¥åœ°é¢„è®­ç»ƒä»¥åŠç»„åˆè§„åˆ’å¼ºåŒ–å­¦ä¹ ä¸è‡ªæˆ‘æ ¡æ­£ï¼Œä»¥çŒè¾“ç¨³å¥çš„ç»„åˆæ¨ç†ã€‚åœ¨LongBench-T2IåŸºå‡†æµ‹è¯•ä¸Šè¿›è¡Œçš„å¹¿æ³›å®éªŒï¼Œä½¿ç”¨Gemini-2.0-Flashå’ŒInternVL3-78Bè¿›è¡Œè‡ªåŠ¨è¯„ä¼°ï¼Œè¯æ˜LVLM-Composeråœ¨å…³é”®ç»„åˆç»´åº¦ä¸Šçš„å“è¶Šæ€§èƒ½ï¼ŒåŒ…æ‹¬ç›®æ ‡å‡†ç¡®æ€§ã€ç»„åˆä¿çœŸåº¦å’Œå§¿æ€å‡†ç¡®æ€§ï¼Œæ˜¾è‘—ä¼˜äºæœ€æ–°æŠ€æœ¯åŸºçº¿ã€‚æ·±å…¥çš„æ¶ˆèç ”ç©¶è¿›ä¸€æ­¥éªŒè¯äº†æˆ‘ä»¬æ‰€æå‡ºæ¨¡å—çš„ä¸å¯æˆ–ç¼ºè´¡çŒ®ï¼Œè€Œäººç±»è¯„ä¼°åˆ™è¯å®äº†æˆ‘ä»¬ç”Ÿæˆå›¾åƒçš„å¯è§‚æ„ŸçŸ¥ä¼˜è¶Šæ€§ã€‚LVLM-Composeræ˜¯æœç€çœŸæ­£å¯æ§å’Œç»„åˆå‡†ç¡®çš„å¼€æ”¾å¼æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆè¿ˆå‡ºçš„é‡è¦ä¸€æ­¥ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.04152v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>åœ¨ç”Ÿæˆå¼äººå·¥æ™ºèƒ½é¢†åŸŸï¼Œå¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆLVLMsï¼‰å·²ç»æˆä¸ºå†…å®¹åˆ›ä½œæ–¹æ³•é©æ–°çš„å‰æ²¿ã€‚å½“å‰LVLMsåœ¨æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆæ–¹é¢å±•ç°å‡ºå“è¶Šèƒ½åŠ›ï¼Œä½†åœ¨å¤„ç†å¤æ‚æ–‡æœ¬æè¿°æ—¶é¢ä¸´æŒ‘æˆ˜ï¼Œå°¤å…¶æ˜¯åœ¨å‘ˆç°å¤šä¸ªå¯¹è±¡ã€å±æ€§ã€ç©ºé—´å…³ç³»å’Œç‰¹å®šå§¿æ€çš„å¤æ‚åœºæ™¯æ—¶è¡¨ç°æ¬ ä½³ã€‚ä¸ºåº”å¯¹è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æ¨å‡ºLVLM-Composeræ¨¡å‹ï¼Œè¯¥æ¨¡å‹å…·å¤‡å¢å¼ºçš„ç»„åˆå›¾åƒåˆæˆèƒ½åŠ›ã€‚é€šè¿‡åˆ†å±‚è¯­ä¹‰è§„åˆ’æ¨¡å—å’Œç²¾ç»†ç‰¹å¾å¯¹é½æœºåˆ¶ï¼Œä»¥åŠå¤šé˜¶æ®µè®­ç»ƒæ¨¡å¼ï¼Œè¯¥æ¨¡å‹åœ¨å…³é”®è¯„ä¼°æ ‡å‡†ä¸Šè¡¨ç°å‡ºå“è¶Šæ€§èƒ½ã€‚å®éªŒè¯æ˜ï¼ŒLVLM-Composeråœ¨å¯¹è±¡å‡†ç¡®æ€§ã€ç»„åˆä¿çœŸåº¦å’Œå§¿æ€å‡†ç¡®æ€§ç­‰æ–¹é¢æ˜¾è‘—ä¼˜äºç°æœ‰æŠ€æœ¯åŸºçº¿ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>ç”Ÿæˆå¼äººå·¥æ™ºèƒ½é¢†åŸŸä¸­ï¼Œå¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆLVLMsï¼‰æ¨åŠ¨å†…å®¹åˆ›ä½œé©æ–°ã€‚</li>
<li>å½“å‰LVLMsåœ¨å¤„ç†å¤æ‚æ–‡æœ¬æè¿°æ—¶é¢ä¸´æŒ‘æˆ˜ï¼Œç‰¹åˆ«æ˜¯åœ¨å‘ˆç°å¤šä¸ªå¯¹è±¡çš„å¤æ‚åœºæ™¯æ—¶ã€‚</li>
<li>LVLM-Composeræ¨¡å‹å…·å¤‡å¢å¼ºçš„ç»„åˆå›¾åƒåˆæˆèƒ½åŠ›ã€‚</li>
<li>LVLM-Composeré€šè¿‡åˆ†å±‚è¯­ä¹‰è§„åˆ’æ¨¡å—å’Œç²¾ç»†ç‰¹å¾å¯¹é½æœºåˆ¶è¿›è¡Œç»“æ„åŒ–æç¤ºåˆ†è§£å’Œç²¾ç¡®è§†è§‰å¼•å¯¼ç”Ÿæˆã€‚</li>
<li>å¤šé˜¶æ®µè®­ç»ƒæ¨¡å¼å¼ºåŒ–ç»„åˆæ¨ç†èƒ½åŠ›ã€‚</li>
<li>LVLM-Composeråœ¨å…³é”®è¯„ä¼°æ ‡å‡†ä¸Šè¡¨ç°å‡ºå“è¶Šæ€§èƒ½ï¼Œæ˜¾è‘—ä¼˜äºç°æœ‰æŠ€æœ¯åŸºçº¿ã€‚</li>
<li>å®éªŒç»“æœç»è‡ªåŠ¨è¯„ä¼°å·¥å…·ç¡®è®¤ï¼Œå¹¶é€šè¿‡æ·±åº¦å‰–æç ”ç©¶éªŒè¯æ¨¡å‹å„ç»„æˆéƒ¨åˆ†çš„è´¡çŒ®ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.04152">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-e1f2cda34e35ad4038c3e94c07f708e5.jpg" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="How-to-Train-Your-LLM-Web-Agent-A-Statistical-Diagnosis"><a href="#How-to-Train-Your-LLM-Web-Agent-A-Statistical-Diagnosis" class="headerlink" title="How to Train Your LLM Web Agent: A Statistical Diagnosis"></a>How to Train Your LLM Web Agent: A Statistical Diagnosis</h2><p><strong>Authors:Dheeraj Vattikonda, Santhoshi Ravichandran, Emiliano Penaloza, Hadi Nekoei, Megh Thakkar, Thibault Le Sellier de Chezelles, Nicolas Gontier, Miguel MuÃ±oz-MÃ¡rmol, Sahar Omidi Shayegan, Stefania Raimondo, Xue Liu, Alexandre Drouin, Laurent Charlin, Alexandre PichÃ©, Alexandre Lacoste, Massimo Caccia</strong></p>
<p>LLM-based web agents have recently made significant progress, but much of it has occurred in closed-source systems, widening the gap with open-source alternatives. Progress has been held back by two key challenges: first, a narrow focus on single-step tasks that overlooks the complexity of multi-step web interactions; and second, the high compute costs required to post-train LLM-based web agents. To address this, we present the first statistically grounded study on compute allocation for LLM web-agent post-training. Our approach uses a two-stage pipeline, training a Llama 3.1 8B student to imitate a Llama 3.3 70B teacher via supervised fine-tuning (SFT), followed by on-policy reinforcement learning. We find this process highly sensitive to hyperparameter choices, making exhaustive sweeps impractical. To spare others from expensive trial-and-error, we sample 1,370 configurations and use bootstrapping to estimate effective hyperparameters. Our results show that combining SFT with on-policy RL consistently outperforms either approach alone on both WorkArena and MiniWob++. Further, this strategy requires only 55% of the compute to match the peak performance of pure SFT on MiniWob++, effectively pushing the compute-performance Pareto frontier, and is the only strategy that can close the gap with closed-source models. </p>
<blockquote>
<p>åŸºäºLLMçš„Webä»£ç†æœ€è¿‘å–å¾—äº†é‡å¤§è¿›å±•ï¼Œä½†å¤§éƒ¨åˆ†è¿›å±•å‡ºç°åœ¨é—­æºç³»ç»Ÿä¸­ï¼Œä¸å¼€æºæ›¿ä»£æ–¹æ¡ˆçš„å·®è·è¢«æ‹‰å¤§ã€‚è¿›å±•å—åˆ°ä¸¤ä¸ªä¸»è¦æŒ‘æˆ˜çš„é™åˆ¶ï¼šé¦–å…ˆï¼Œå¯¹å•æ­¥ä»»åŠ¡çš„ç‹­çª„å…³æ³¨ï¼Œå¿½è§†äº†å¤šæ­¥Webäº¤äº’çš„å¤æ‚æ€§ï¼›å…¶æ¬¡ï¼ŒåŸºäºLLMçš„Webä»£ç†åè®­ç»ƒæ‰€éœ€çš„è®¡ç®—æˆæœ¬å¾ˆé«˜ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬å¯¹LLM Webä»£ç†åè®­ç»ƒçš„è®¡ç®—åˆ†é…è¿›è¡Œäº†é¦–æ¬¡ç»Ÿè®¡ç ”ç©¶ã€‚æˆ‘ä»¬çš„æ–¹æ³•é‡‡ç”¨ä¸¤é˜¶æ®µç®¡é“ï¼Œè®­ç»ƒä¸€ä¸ªLlama 3.1 8Bå­¦ç”Ÿæ¨¡å‹é€šè¿‡ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰æ¥æ¨¡ä»¿Llama 3.3 70Bæ•™å¸ˆæ¨¡å‹ï¼Œéšåè¿›è¡Œç­–ç•¥æ€§å¼ºåŒ–å­¦ä¹ ã€‚æˆ‘ä»¬å‘ç°è¿™ä¸ªè¿‡ç¨‹å¯¹è¶…å‚æ•°é€‰æ‹©éå¸¸æ•æ„Ÿï¼Œä½¿å…¨é¢çš„æ‰«æå˜å¾—ä¸åˆ‡å®é™…ã€‚ä¸ºäº†èŠ‚çœä»–äººæ˜‚è´µçš„è¯•é”™è¿‡ç¨‹ï¼Œæˆ‘ä»¬å¯¹1370ç§é…ç½®è¿›è¡Œé‡‡æ ·ï¼Œå¹¶ä½¿ç”¨è‡ªåŠ©æ³•æ¥ä¼°è®¡æœ‰æ•ˆçš„è¶…å‚æ•°ã€‚æˆ‘ä»¬çš„ç»“æœè¡¨æ˜ï¼Œç»“åˆSFTå’Œç­–ç•¥æ€§RLåœ¨å·¥ä½œç«æŠ€åœºï¼ˆWorkArenaï¼‰å’ŒMiniWob++ä¸Šçš„è¡¨ç°å§‹ç»ˆä¼˜äºå•ç‹¬é‡‡ç”¨ä»»ä½•ä¸€ç§æ–¹æ³•ã€‚æ­¤å¤–ï¼Œè¯¥ç­–ç•¥ä»…éœ€55%çš„è®¡ç®—é‡å³å¯è¾¾åˆ°çº¯SFTåœ¨MiniWob++ä¸Šçš„å³°å€¼æ€§èƒ½ï¼Œæœ‰æ•ˆåœ°æ¨åŠ¨äº†è®¡ç®—æ€§èƒ½å¸•ç´¯æ‰˜å‰æ²¿ï¼Œè€Œä¸”æ˜¯å”¯ä¸€èƒ½å¤Ÿç¼©å°ä¸é—­æºæ¨¡å‹å·®è·çš„ç­–ç•¥ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.04103v1">PDF</a> </p>
<p><strong>Summary</strong>ï¼š</p>
<p>è¿‘æœŸï¼ŒLLMï¼ˆå¤§å‹è¯­è¨€æ¨¡å‹ï¼‰åŸºç¡€çš„webä»£ç†åœ¨å°é—­æºä»£ç ç³»ç»Ÿä¸­å–å¾—æ˜¾è‘—è¿›å±•ï¼Œä½†ä»é¢ä¸´å¤šç§æŒ‘æˆ˜ï¼Œå¯¼è‡´å¼€æºæ›¿ä»£æ–¹æ¡ˆçš„å·®è·æ‰©å¤§ã€‚ä¸»è¦æŒ‘æˆ˜åŒ…æ‹¬ï¼šä¸€æ˜¯è¿‡äºå…³æ³¨å•æ­¥éª¤ä»»åŠ¡ï¼Œå¿½è§†äº†å¤šæ­¥éª¤webäº¤äº’çš„å¤æ‚æ€§ï¼›äºŒæ˜¯LLMåŸºç¡€webä»£ç†åè®­ç»ƒçš„è®¡ç®—æˆæœ¬é«˜æ˜‚ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬é¦–æ¬¡è¿›è¡Œäº†å…³äºLLM webä»£ç†åè®­ç»ƒçš„è®¡ç®—åˆ†é…ç»Ÿè®¡ç ”ç©¶ã€‚æˆ‘ä»¬é‡‡ç”¨ä¸¤é˜¶æ®µç®¡é“æ–¹æ³•ï¼Œé€šè¿‡ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰è®­ç»ƒLlama 3.1 8Bå­¦ç”Ÿæ¨¡å‹ä»¥æ¨¡ä»¿Llama 3.3 70Bæ•™å¸ˆæ¨¡å‹ï¼Œéšåè¿›è¡ŒåŸºäºç­–ç•¥å¼ºåŒ–å­¦ä¹ ã€‚æˆ‘ä»¬å‘ç°è¿™ä¸€è¿‡ç¨‹å¯¹è¶…å‚æ•°é€‰æ‹©é«˜åº¦æ•æ„Ÿï¼Œä½¿å¾—å…¨é¢æ‰«æä¸åˆ‡å®é™…ã€‚ä¸ºèŠ‚çœä»–äººæ˜‚è´µçš„è¯•é”™æˆæœ¬ï¼Œæˆ‘ä»¬å¯¹1370ä¸ªé…ç½®è¿›è¡Œé‡‡æ ·ï¼Œå¹¶ä½¿ç”¨bootstrapä¼°è®¡æœ‰æ•ˆè¶…å‚æ•°ã€‚ç»“æœæ˜¾ç¤ºï¼Œç»“åˆSFTå’ŒåŸºäºç­–ç•¥çš„å¼ºåŒ–å­¦ä¹ å§‹ç»ˆä¼˜äºå•ä¸€æ–¹æ³•ï¼Œåœ¨å·¥ä½œç«æŠ€åœºï¼ˆWorkArenaï¼‰å’ŒMiniWob++ä¸Šçš„è¡¨ç°å°¤ä¸ºæ˜æ˜¾ã€‚æ­¤å¤–ï¼Œè¯¥ç­–ç•¥ä»…éœ€55%çš„è®¡ç®—é‡å³å¯è¾¾åˆ°çº¯SFTåœ¨MiniWob++ä¸Šçš„å³°å€¼æ€§èƒ½ï¼Œæœ‰æ•ˆæ¨åŠ¨äº†è®¡ç®—æ€§èƒ½å¸•ç´¯æ‰˜å‰æ²¿ï¼Œæ˜¯å”¯ä¸€èƒ½å¤Ÿç¼©å°ä¸å°é—­æºä»£ç æ¨¡å‹å·®è·çš„ç­–ç•¥ã€‚</p>
<p><strong>Key Takeaways</strong>:</p>
<ol>
<li>LLM-based webä»£ç†åœ¨å°é—­æºä»£ç ç³»ç»Ÿä¸­å–å¾—è¿›å±•ï¼Œä½†å¼€æºæ›¿ä»£æ–¹æ¡ˆå·®è·æ‰©å¤§ã€‚</li>
<li>ä¸»è¦é¢ä¸´ä¸¤ä¸ªæŒ‘æˆ˜ï¼šå…³æ³¨å•æ­¥éª¤ä»»åŠ¡å¿½è§†å¤šæ­¥éª¤äº¤äº’çš„å¤æ‚æ€§å’Œé«˜è®¡ç®—æˆæœ¬ã€‚</li>
<li>é‡‡ç”¨ä¸¤é˜¶æ®µç®¡é“æ–¹æ³•è§£å†³æ­¤é—®é¢˜ï¼ŒåŒ…æ‹¬ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰å’ŒåŸºäºç­–ç•¥çš„å¼ºåŒ–å­¦ä¹ ã€‚</li>
<li>å‘ç°è¿‡ç¨‹å¯¹è¶…å‚æ•°é€‰æ‹©é«˜åº¦æ•æ„Ÿï¼Œå…¨é¢æ‰«æä¸åˆ‡å®é™…ã€‚</li>
<li>ç»“åˆSFTå’ŒåŸºäºç­–ç•¥çš„å¼ºåŒ–å­¦ä¹ è¡¨ç°ä¼˜äºå•ä¸€æ–¹æ³•ï¼Œåœ¨å·¥ä½œç«æŠ€åœºå’ŒMiniWob++ä¸Šè¡¨ç°æ˜¾è‘—ã€‚</li>
<li>è¯¥ç­–ç•¥é™ä½è®¡ç®—éœ€æ±‚ï¼Œæœ‰æ•ˆæ¨åŠ¨è®¡ç®—æ€§èƒ½å¸•ç´¯æ‰˜å‰æ²¿ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.04103">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-ca60a1e5c49659792125ebaec31cc76e.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-f15e727ee7c02ffd31c2340cc89068e0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f2da968765e51a4dc44b6d54a6e41c97.jpg" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="Conversation-Forests-The-Key-to-Fine-Tuning-Large-Language-Models-for-Multi-Turn-Medical-Conversations-is-Branching"><a href="#Conversation-Forests-The-Key-to-Fine-Tuning-Large-Language-Models-for-Multi-Turn-Medical-Conversations-is-Branching" class="headerlink" title="Conversation Forests: The Key to Fine Tuning Large Language Models for   Multi-Turn Medical Conversations is Branching"></a>Conversation Forests: The Key to Fine Tuning Large Language Models for   Multi-Turn Medical Conversations is Branching</h2><p><strong>Authors:Thomas Savage</strong></p>
<p>Fine-tuning methods such as Direct Preference Optimization (DPO) and Group Relative Policy Optimization (GRPO) have demonstrated success in training large language models (LLMs) for single-turn tasks. However, these methods fall short in multi-turn applications, such as diagnostic patient interviewing, where understanding how early conversational turns influence downstream completions and outcomes is essential. In medicine, a multi-turn perspective is critical for learning diagnostic schemas and better understanding conversation dynamics. To address this gap, I introduce Savage Conversation Forests (SCF), a reinforcement learning framework that leverages a branched conversation architecture to fine-tune LLMs for multi-turn dialogue. SCF generates multiple possible conversation continuations at each turn, enabling the model to learn how different early responses affect downstream interactions and diagnostic outcomes. In experiments simulating doctor-patient conversations, SCF with branching outperforms linear conversation architectures on diagnostic accuracy. I hypothesize that SCFâ€™s improvements stem from its ability to provide richer, interdependent training signals across conversation turns. These results suggest that a branched training architecture is an important strategy for fine tuning LLMs in complex multi-turn conversational tasks. </p>
<blockquote>
<p>å¾®è°ƒæ–¹æ³•ï¼Œå¦‚ç›´æ¥åå¥½ä¼˜åŒ–ï¼ˆDPOï¼‰å’Œç»„ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆGRPOï¼‰ï¼Œåœ¨é’ˆå¯¹å•è½®ä»»åŠ¡çš„è®­ç»ƒå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ–¹é¢å–å¾—äº†æˆåŠŸã€‚ç„¶è€Œï¼Œè¿™äº›æ–¹æ³•åœ¨å¤šè½®åº”ç”¨æ–¹é¢è¡¨ç°ä¸è¶³ï¼Œæ¯”å¦‚åœ¨è¯Šæ–­ç—…äººè®¿è°ˆä¸­ï¼Œç†è§£æ—©æœŸå¯¹è¯è½®æ¬¡å¦‚ä½•å½±å“ä¸‹æ¸¸å®Œæˆå’Œç»“æœè‡³å…³é‡è¦ã€‚åœ¨åŒ»å­¦é¢†åŸŸï¼Œå¤šè½®è§†è§’å¯¹äºå­¦ä¹ è¯Šæ–­æ¨¡å¼å’Œæ›´å¥½åœ°äº†è§£å¯¹è¯åŠ¨æ€è‡³å…³é‡è¦ã€‚ä¸ºäº†è§£å†³è¿™ä¸€å·®è·ï¼Œæˆ‘å¼•å…¥äº†é‡è›®å¯¹è¯æ£®æ—ï¼ˆSCFï¼‰ï¼Œè¿™æ˜¯ä¸€ä¸ªåˆ©ç”¨åˆ†æ”¯å¯¹è¯æ¶æ„çš„å¼ºåŒ–å­¦ä¹ æ¡†æ¶ï¼Œç”¨äºå¯¹å¤šè½®å¯¹è¯è¿›è¡Œå¾®è°ƒçš„å¤§å‹è¯­è¨€æ¨¡å‹ã€‚SCFåœ¨æ¯ä¸€è½®ç”Ÿæˆå¤šä¸ªå¯èƒ½çš„å¯¹è¯å»¶ç»­ï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿå­¦ä¹ ä¸åŒçš„æ—©æœŸå›åº”å¦‚ä½•å½±å“ä¸‹æ¸¸äº’åŠ¨å’Œè¯Šæ–­ç»“æœã€‚åœ¨æ¨¡æ‹ŸåŒ»æ‚£å¯¹è¯çš„å®éªŒä¸­ï¼Œå¸¦æœ‰åˆ†æ”¯çš„SCFåœ¨è¯Šæ–­å‡†ç¡®æ€§æ–¹é¢ä¼˜äºçº¿æ€§å¯¹è¯æ¶æ„ã€‚æˆ‘å‡è®¾SCFçš„æ”¹è¿›æºäºå…¶åœ¨å¯¹è¯è½®æ¬¡ä¹‹é—´æä¾›ä¸°å¯Œã€ç›¸äº’ä¾å­˜è®­ç»ƒä¿¡å·çš„èƒ½åŠ›ã€‚è¿™äº›ç»“æœè¡¨æ˜ï¼Œåˆ†æ”¯è®­ç»ƒæ¶æ„æ˜¯å¾®è°ƒå¤æ‚å¤šè½®å¯¹è¯ä»»åŠ¡ä¸­å¤§å‹è¯­è¨€æ¨¡å‹çš„é‡è¦ç­–ç•¥ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.04099v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨å•å›åˆä»»åŠ¡ä¸­é€šè¿‡ç›´æ¥åå¥½ä¼˜åŒ–ï¼ˆDPOï¼‰å’Œç¾¤ä½“ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆGRPOï¼‰ç­‰å¾®è°ƒæ–¹æ³•å–å¾—äº†æˆåŠŸã€‚ä½†åœ¨å¤šå›åˆåº”ç”¨ï¼Œå¦‚è¯Šæ–­ç—…äººè®¿è°ˆä¸­ï¼Œè¿™äº›æ–¹æ³•å­˜åœ¨ä¸è¶³ã€‚ä¸ºæ­¤ï¼Œå¼•å…¥é‡è›®å¯¹è¯æ£®æ—ï¼ˆSCFï¼‰è¿™ä¸€å¼ºåŒ–å­¦ä¹ æ¡†æ¶ï¼Œé‡‡ç”¨åˆ†æ”¯å¯¹è¯æ¶æ„å¾®è°ƒLLMï¼Œç”¨äºå¤šå›åˆå¯¹è¯ã€‚SCFåœ¨æ¨¡æ‹ŸåŒ»æ‚£å¯¹è¯çš„å®éªŒä¸­ï¼Œä»¥åˆ†æ”¯æ–¹å¼åœ¨è¯Šæ–­å‡†ç¡®æ€§ä¸Šä¼˜äºçº¿æ€§å¯¹è¯æ¶æ„ã€‚é¢„è®¡SCFçš„æ”¹è¿›æºäºå…¶æä¾›è·¨å¯¹è¯å›åˆçš„ä¸°å¯Œã€ç›¸äº’ä¾èµ–çš„è®­ç»ƒä¿¡å·çš„èƒ½åŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨å•å›åˆä»»åŠ¡ä¸­å·²æœ‰æˆåŠŸåº”ç”¨ã€‚</li>
<li>åœ¨å¤šå›åˆå¯¹è¯ä»»åŠ¡ä¸­ï¼Œå¦‚è¯Šæ–­ç—…äººè®¿è°ˆï¼Œç°æœ‰å¾®è°ƒæ–¹æ³•å­˜åœ¨å±€é™æ€§ã€‚</li>
<li>é‡è›®å¯¹è¯æ£®æ—ï¼ˆSCFï¼‰æ˜¯ä¸€ä¸ªå¼ºåŒ–å­¦ä¹ æ¡†æ¶ï¼Œé‡‡ç”¨åˆ†æ”¯å¯¹è¯æ¶æ„æ¥å¾®è°ƒLLMã€‚</li>
<li>SCFèƒ½å¤Ÿç”Ÿæˆæ¯ä¸€å›åˆçš„å¤šç§å¯èƒ½å¯¹è¯å»¶ç»­ã€‚</li>
<li>SCFé€šè¿‡å­¦ä¹ ä¸åŒæ—©æœŸå›åº”å¯¹åç»­äº’åŠ¨å’Œè¯Šæ–­ç»“æœçš„å½±å“æ¥æé«˜è¯Šæ–­å‡†ç¡®æ€§ã€‚</li>
<li>SCFåœ¨æ¨¡æ‹ŸåŒ»æ‚£å¯¹è¯çš„å®éªŒä¸­è¡¨ç°ä¼˜äºçº¿æ€§å¯¹è¯æ¶æ„ã€‚</li>
<li>SCFçš„æ”¹è¿›å¾—ç›Šäºå…¶åœ¨ä¸åŒå¯¹è¯å›åˆæä¾›ä¸°å¯Œã€ç›¸äº’ä¾èµ–çš„è®­ç»ƒä¿¡å·çš„èƒ½åŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.04099">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-69359dc61d539ecafac5b234ad1b5e94.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-3cd4e15ec373b13b3a016e880e007bbe.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-fb7560e5ff18f9e64988bc0ea54c738f.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-34e43544546ec15afd965f333d88e791.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2a7fb634b951f4a4d0875090342f1e1d.jpg" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1><h2 id="Ready-Jurist-One-Benchmarking-Language-Agents-for-Legal-Intelligence-in-Dynamic-Environments"><a href="#Ready-Jurist-One-Benchmarking-Language-Agents-for-Legal-Intelligence-in-Dynamic-Environments" class="headerlink" title="Ready Jurist One: Benchmarking Language Agents for Legal Intelligence in   Dynamic Environments"></a>Ready Jurist One: Benchmarking Language Agents for Legal Intelligence in   Dynamic Environments</h2><p><strong>Authors:Zheng Jia, Shengbin Yue, Wei Chen, Siyuan Wang, Yidong Liu, Yun Song, Zhongyu Wei</strong></p>
<p>The gap between static benchmarks and the dynamic nature of real-world legal practice poses a key barrier to advancing legal intelligence. To this end, we introduce J1-ENVS, the first interactive and dynamic legal environment tailored for LLM-based agents. Guided by legal experts, it comprises six representative scenarios from Chinese legal practices across three levels of environmental complexity. We further introduce J1-EVAL, a fine-grained evaluation framework, designed to assess both task performance and procedural compliance across varying levels of legal proficiency. Extensive experiments on 17 LLM agents reveal that, while many models demonstrate solid legal knowledge, they struggle with procedural execution in dynamic settings. Even the SOTA model, GPT-4o, falls short of 60% overall performance. These findings highlight persistent challenges in achieving dynamic legal intelligence and offer valuable insights to guide future research. </p>
<blockquote>
<p>é™æ€åŸºå‡†æµ‹è¯•ä¸çœŸå®ä¸–ç•Œæ³•å¾‹å®è·µçš„åŠ¨æ€æ€§è´¨ä¹‹é—´çš„å·®è·æ˜¯æ¨è¿›æ³•å¾‹æ™ºèƒ½çš„ä¸»è¦éšœç¢ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬å¼•å…¥äº†J1-ENVSï¼Œè¿™æ˜¯ç¬¬ä¸€ä¸ªé’ˆå¯¹å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä»£ç†é‡èº«å®šåˆ¶çš„äº’åŠ¨å¼åŠ¨æ€æ³•å¾‹ç¯å¢ƒã€‚åœ¨ä¸“å®¶æŒ‡å¯¼ä¸‹ï¼Œå®ƒæ¶µç›–äº†ä¸­å›½æ³•å¾‹å®è·µä¸­ä¸‰ä¸ªä¸åŒç¯å¢ƒå¤æ‚ç¨‹åº¦ç­‰çº§çš„å…­ä¸ªå…¸å‹åœºæ™¯ã€‚æˆ‘ä»¬è¿˜å¼•å…¥äº†J1-EVALè¿™ä¸€ç²¾ç»†çš„è¯„ä¼°æ¡†æ¶ï¼Œæ—¨åœ¨è¯„ä¼°ä¸åŒæ³•å¾‹ä¸“ä¸šæ°´å¹³çš„ä»»åŠ¡æ‰§è¡Œæƒ…å†µå’Œç¨‹åºåˆè§„æ€§ã€‚å¯¹17ä¸ªå¤§å‹è¯­è¨€æ¨¡å‹ä»£ç†è¿›è¡Œçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼Œè™½ç„¶è®¸å¤šæ¨¡å‹å±•ç°å‡ºåšå®çš„æ³•å¾‹çŸ¥è¯†åŸºç¡€ï¼Œä½†åœ¨åŠ¨æ€ç¯å¢ƒä¸­æ‰§è¡Œç¨‹åºæ–¹é¢å´é‡åˆ°äº†å›°éš¾ã€‚å³ä½¿æ˜¯ç›®å‰æœ€å…ˆè¿›çš„æ¨¡å‹GPT-4oï¼Œæ€»ä½“æ€§èƒ½ä¹Ÿä¸è¶³60%ã€‚è¿™äº›å‘ç°çªæ˜¾äº†å®ç°åŠ¨æ€æ³•å¾‹æ™ºèƒ½æ‰€é¢ä¸´çš„æŒä¹…æŒ‘æˆ˜ï¼Œå¹¶ä¸ºæœªæ¥çš„ç ”ç©¶æä¾›äº†å®è´µçš„æŒ‡å¯¼ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.04037v1">PDF</a> </p>
<p><strong>Summary</strong>ï¼š<br>é™æ€åŸºå‡†ä¸çœŸå®ä¸–ç•Œæ³•å¾‹å®è·µçš„åŠ¨æ€æ€§è´¨ä¹‹é—´å­˜åœ¨å·®è·ï¼Œè¿™æ˜¯æ¨è¿›æ³•å¾‹æ™ºèƒ½çš„ä¸»è¦éšœç¢ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æ¨å‡ºäº†J1-ENVSï¼Œè¿™æ˜¯ç¬¬ä¸€ä¸ªé’ˆå¯¹LLMä»£ç†è®¾è®¡çš„äº¤äº’å¼åŠ¨æ€æ³•å¾‹ç¯å¢ƒã€‚å®ƒç”±æ³•å¾‹ä¸“å®¶å¼•å¯¼ï¼ŒåŒ…å«ä¸­å›½æ³•å¾‹å®è·µä¸­ä¸‰ä¸ªä¸åŒç¯å¢ƒå¤æ‚ç¨‹åº¦å±‚æ¬¡çš„å…­ä¸ªä»£è¡¨æ€§åœºæ™¯ã€‚æˆ‘ä»¬è¿˜å¼•å…¥äº†J1-EVALï¼Œè¿™æ˜¯ä¸€ä¸ªç²¾ç»†çš„è¯„ä»·æ¡†æ¶ï¼Œæ—¨åœ¨è¯„ä¼°ä¸åŒæ³•å¾‹ç†Ÿç»ƒåº¦ç­‰çº§çš„ä»»åŠ¡æ‰§è¡Œæƒ…å†µå’Œç¨‹åºåˆè§„æ€§ã€‚å¯¹17ä¸ªLLMä»£ç†çš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼Œå°½ç®¡è®¸å¤šæ¨¡å‹å±•ç°å‡ºåšå®çš„æ³•å¾‹çŸ¥è¯†ï¼Œä½†åœ¨åŠ¨æ€ç¯å¢ƒä¸­æ‰§è¡Œç¨‹åºæ—¶å´é‡åˆ°äº†å›°éš¾ã€‚å³ä½¿æ˜¯æœ€å…ˆè¿›çš„æ¨¡å‹GPT-4oï¼Œæ€»ä½“æ€§èƒ½ä¹Ÿä¸åˆ°60%ã€‚è¿™äº›å‘ç°çªæ˜¾äº†å®ç°åŠ¨æ€æ³•å¾‹æ™ºèƒ½çš„æŒä¹…æŒ‘æˆ˜ï¼Œå¹¶ä¸ºæœªæ¥çš„ç ”ç©¶æä¾›äº†å®è´µçš„æŒ‡å¯¼ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>é™æ€åŸºå‡†ä¸çœŸå®ä¸–ç•Œæ³•å¾‹å®è·µçš„åŠ¨æ€æ€§è´¨ä¹‹é—´çš„å·®è·æ˜¯æ¨è¿›æ³•å¾‹æ™ºèƒ½çš„ä¸»è¦éšœç¢ã€‚</li>
<li>J1-ENVSæ˜¯é¦–ä¸ªé’ˆå¯¹LLMä»£ç†è®¾è®¡çš„äº¤äº’å¼åŠ¨æ€æ³•å¾‹ç¯å¢ƒï¼ŒåŒ…å«å…­ä¸ªä»£è¡¨ä¸­å›½æ³•å¾‹å®è·µçš„å…¸å‹åœºæ™¯ã€‚</li>
<li>J1-EVALæ˜¯ä¸€ä¸ªè¯„ä¼°LLMä»£ç†åœ¨ä»»åŠ¡æ‰§è¡Œå’Œç¨‹åºåˆè§„æ€§æ–¹é¢çš„è¡¨ç°çš„è¯„ä»·æ¡†æ¶ã€‚</li>
<li>å¹¿æ³›å®éªŒæ˜¾ç¤ºï¼Œå°½ç®¡LLMæ¨¡å‹å…·å¤‡æ³•å¾‹çŸ¥è¯†ï¼Œä½†åœ¨åŠ¨æ€ç¯å¢ƒä¸­æ‰§è¡Œç¨‹åºæ—¶ä»é¢ä¸´å›°éš¾ã€‚</li>
<li>ç°æœ‰æ¨¡å‹åœ¨åŠ¨æ€æ³•å¾‹æ™ºèƒ½æ–¹é¢æ€§èƒ½ä¸ä½³ï¼Œå³ä½¿æ˜¯æœ€å…ˆè¿›çš„æ¨¡å‹GPT-4oä¹Ÿæ˜¯å¦‚æ­¤ã€‚</li>
<li>å®éªŒç»“æœçªæ˜¾äº†å®ç°åŠ¨æ€æ³•å¾‹æ™ºèƒ½çš„æŒä¹…æŒ‘æˆ˜ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.04037">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-38db2d4a6e732fab4c9463757c27407a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-faa04bd7cd405f79718212149301cf87.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-9dbf1fdfd652aaacec44e88425ea2a7d.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-a45afa14b4363b674a02aaf0c51fb9b9.jpg" align="middle">
</details>


<h1 id="-19"><a href="#-19" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-07-09/R1_Reasoning/">https://kedreamix.github.io/Talk2Paper/Paper/2025-07-09/R1_Reasoning/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/R1-Reasoning/">
                                    <span class="chip bg-color">R1_Reasoning</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-07-09/LLM/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-8488f236b5f9be3b34f1f4606e1c9fe7.jpg" class="responsive-img" alt="LLM">
                        
                        <span class="card-title">LLM</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            LLM æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-07-09  Beyond Simple Edits X-Planner for Complex Instruction-Based Image   Editing
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-07-09
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/LLM/" class="post-category">
                                    LLM
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/LLM/">
                        <span class="chip bg-color">LLM</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-07-08/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-37f628b13b37a070dce6d6ea394c4542.jpg" class="responsive-img" alt="åŒ»å­¦å›¾åƒ">
                        
                        <span class="card-title">åŒ»å­¦å›¾åƒ</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            åŒ»å­¦å›¾åƒ æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-07-08  VHU-Net Variational Hadamard U-Net for Body MRI Bias Field Correction
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-07-08
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/" class="post-category">
                                    åŒ»å­¦å›¾åƒ
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                        <span class="chip bg-color">åŒ»å­¦å›¾åƒ</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">23539.7k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
