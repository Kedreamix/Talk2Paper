<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Diffusion Models">
    <meta name="description" content="Diffusion Models æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-07-09  SV-DRR High-Fidelity Novel View X-Ray Synthesis Using Diffusion Model">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Diffusion Models | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pic1.zhimg.com/v2-e8d923a955cd158ea91fda65852e1b9d.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Diffusion Models</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Diffusion-Models/">
                                <span class="chip bg-color">Diffusion Models</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Diffusion-Models/" class="post-category">
                                Diffusion Models
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-07-09
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-07-17
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    18.6k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    76 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-07-09-æ›´æ–°"><a href="#2025-07-09-æ›´æ–°" class="headerlink" title="2025-07-09 æ›´æ–°"></a>2025-07-09 æ›´æ–°</h1><h2 id="SV-DRR-High-Fidelity-Novel-View-X-Ray-Synthesis-Using-Diffusion-Model"><a href="#SV-DRR-High-Fidelity-Novel-View-X-Ray-Synthesis-Using-Diffusion-Model" class="headerlink" title="SV-DRR: High-Fidelity Novel View X-Ray Synthesis Using Diffusion Model"></a>SV-DRR: High-Fidelity Novel View X-Ray Synthesis Using Diffusion Model</h2><p><strong>Authors:Chun Xie, Yuichi Yoshii, Itaru Kitahara</strong></p>
<p>X-ray imaging is a rapid and cost-effective tool for visualizing internal human anatomy. While multi-view X-ray imaging provides complementary information that enhances diagnosis, intervention, and education, acquiring images from multiple angles increases radiation exposure and complicates clinical workflows. To address these challenges, we propose a novel view-conditioned diffusion model for synthesizing multi-view X-ray images from a single view. Unlike prior methods, which are limited in angular range, resolution, and image quality, our approach leverages the Diffusion Transformer to preserve fine details and employs a weak-to-strong training strategy for stable high-resolution image generation. Experimental results demonstrate that our method generates higher-resolution outputs with improved control over viewing angles. This capability has significant implications not only for clinical applications but also for medical education and data extension, enabling the creation of diverse, high-quality datasets for training and analysis. Our code is available at GitHub. </p>
<blockquote>
<p>Xå°„çº¿æˆåƒæ˜¯ä¸€ç§å¿«é€Ÿä¸”ç»æµå®æƒ çš„å·¥å…·ï¼Œå¯ç”¨äºå¯è§†åŒ–äººä½“å†…éƒ¨ç»“æ„ã€‚è™½ç„¶å¤šè§†è§’Xå°„çº¿æˆåƒæä¾›äº†å¢å¼ºè¯Šæ–­ã€å¹²é¢„å’Œæ•™è‚²çš„è¡¥å……ä¿¡æ¯ï¼Œä½†ä»å¤šä¸ªè§’åº¦è·å–å›¾åƒä¼šå¢åŠ è¾å°„æš´éœ²å¹¶å¤æ‚åŒ–ä¸´åºŠå·¥ä½œæµç¨‹ã€‚ä¸ºäº†è§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºè§†å›¾è°ƒèŠ‚çš„æ‰©æ•£æ¨¡å‹ï¼Œè¯¥æ¨¡å‹å¯ä»å•ä¸ªè§†è§’åˆæˆå¤šè§†è§’Xå°„çº¿å›¾åƒã€‚ä¸åŒäºåœ¨è§’åº¦èŒƒå›´ã€åˆ†è¾¨ç‡å’Œå›¾åƒè´¨é‡ä¸Šæœ‰æ‰€å±€é™çš„å…ˆå‰æ–¹æ³•ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åˆ©ç”¨æ‰©æ•£å˜å‹å™¨æ¥ä¿ç•™ç»†èŠ‚ï¼Œå¹¶é‡‡ç”¨ä»å¼±åˆ°å¼ºçš„è®­ç»ƒç­–ç•¥æ¥è¿›è¡Œç¨³å®šçš„é«˜åˆ†è¾¨ç‡å›¾åƒç”Ÿæˆã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•ç”Ÿæˆäº†æ›´é«˜åˆ†è¾¨ç‡çš„è¾“å‡ºï¼Œå¯¹è§‚å¯Ÿè§’åº¦æœ‰æ›´å¥½çš„æ§åˆ¶ã€‚è¿™é¡¹èƒ½åŠ›ä¸ä»…å¯¹ä¸´åºŠåº”ç”¨æœ‰é‡è¦æ„ä¹‰ï¼Œè€Œä¸”å¯¹åŒ»å­¦æ•™è‚²å’Œæ•°æ®æ‰©å±•ä¹Ÿæœ‰é‡è¦æ„ä¹‰ï¼Œèƒ½å¤Ÿåˆ›å»ºç”¨äºåŸ¹è®­å’Œåˆ†æçš„å¤šæ ·ã€é«˜è´¨é‡æ•°æ®é›†ã€‚æˆ‘ä»¬çš„ä»£ç å¯åœ¨GitHubä¸Šè·å¾—ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.05148v1">PDF</a> Accepted by MICCAI2025</p>
<p><strong>Summary</strong></p>
<p>å¤šè§†è§’Xå°„çº¿æˆåƒèƒ½å¤Ÿæä¾›äº’è¡¥ä¿¡æ¯ï¼Œæé«˜è¯Šæ–­ã€æ²»ç–—å’Œæ•™è‚²çš„æ•ˆæœï¼Œä½†è¾å°„æš´éœ²å¢åŠ åŠä¸´åºŠæ“ä½œå¤æ‚åŒ–çš„é—®é¢˜ä¹Ÿéšä¹‹è€Œæ¥ã€‚ä¸ºè§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºæ‰©æ•£æ¨¡å‹çš„æ–°å‹å•è§†è§’åˆæˆå¤šè§†è§’Xå°„çº¿å›¾åƒæ–¹æ³•ã€‚ä¸ä¼ ç»Ÿæ–¹æ³•ç›¸æ¯”ï¼Œæˆ‘ä»¬çš„æ–¹æ³•ä½¿ç”¨æ‰©æ•£å˜å‹å™¨ä¿å­˜ç²¾ç»†ç»†èŠ‚å¹¶é‡‡ç”¨ç”±å¼±è‡³å¼ºçš„è®­ç»ƒç­–ç•¥ä»¥ç”Ÿæˆç¨³å®šçš„é«˜åˆ†è¾¨ç‡å›¾åƒã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¯¥æ–¹æ³•ç”Ÿæˆçš„å›¾åƒåˆ†è¾¨ç‡æ›´é«˜ï¼Œå¯¹è§†è§’çš„æ§åˆ¶æ›´ä¸ºç²¾å‡†ã€‚è¿™ä¸ä»…å¯¹ä¸´åºŠåº”ç”¨æœ‰é‡è¦æ„ä¹‰ï¼Œè¿˜å¯¹æé«˜åŒ»å­¦æ•™è‚²å’Œæ•°æ®æ‰©å±•çš„æ•ˆç‡æœ‰ç§¯æå½±å“ï¼Œèƒ½å¤Ÿåˆ›å»ºå¤šæ ·ä¸”é«˜è´¨é‡çš„æ•°æ®é›†ä»¥ä¾›åˆ†æå’Œè®­ç»ƒã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤šè§†è§’Xå°„çº¿æˆåƒå¯ä»¥æä¾›æ›´å…¨é¢çš„å†…éƒ¨ç»“æ„ä¿¡æ¯ä»¥å¢å¼ºè¯Šæ–­æ•ˆæœå’Œæ‚£è€…æ²»ç–—å¹²é¢„çš„ç²¾ç¡®æ€§ã€‚ç„¶è€Œï¼Œå®ƒä¼šå¢åŠ è¾å°„æš´éœ²å¹¶å¤æ‚åŒ–ä¸´åºŠå·¥ä½œæµç¨‹ã€‚</li>
<li>æå‡ºäº†ä¸€ç§åŸºäºæ‰©æ•£æ¨¡å‹çš„å…¨æ–°æ–¹æ³•ï¼Œèƒ½å¤Ÿä»å•ä¸€è§†è§’åˆæˆå¤šè§†è§’Xå°„çº¿å›¾åƒã€‚</li>
<li>ä¸ä¼ ç»Ÿæ–¹æ³•ç›¸æ¯”ï¼Œæ–°æ–¹æ³•ä½¿ç”¨æ‰©æ•£å˜å‹å™¨æŠ€æœ¯ï¼Œå¯ä»¥æ›´å¥½åœ°ä¿å­˜å›¾åƒä¸­çš„ç²¾ç»†ç»†èŠ‚ã€‚</li>
<li>é‡‡ç”¨ç”±å¼±è‡³å¼ºçš„è®­ç»ƒç­–ç•¥ï¼Œç¡®ä¿ç¨³å®šçš„é«˜åˆ†è¾¨ç‡å›¾åƒç”Ÿæˆã€‚</li>
<li>å®éªŒéªŒè¯æ˜¾ç¤ºæ–°æ–¹æ³•ç”Ÿæˆçš„å›¾åƒåˆ†è¾¨ç‡æ›´é«˜ï¼Œå¯¹è§†è§’æ§åˆ¶æ›´ä¸ºç²¾å‡†ã€‚</li>
<li>è¯¥æŠ€æœ¯ä¸ä»…æœ‰åŠ©äºä¸´åºŠåº”ç”¨ï¼Œè¿˜èƒ½ä¿ƒè¿›åŒ»å­¦æ•™è‚²å’Œæ•°æ®æ‰©å±•çš„è¿›æ­¥ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.05148">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-0032098c9d5f8965e23647d08006d7f1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9d5e7fd55a80dc55ba7f842ff3712c7a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-019c69b8a9aa9d30242cd83bfa391eca.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="VERITAS-Verification-and-Explanation-of-Realness-in-Images-for-Transparency-in-AI-Systems"><a href="#VERITAS-Verification-and-Explanation-of-Realness-in-Images-for-Transparency-in-AI-Systems" class="headerlink" title="VERITAS: Verification and Explanation of Realness in Images for   Transparency in AI Systems"></a>VERITAS: Verification and Explanation of Realness in Images for   Transparency in AI Systems</h2><p><strong>Authors:Aadi Srivastava, Vignesh Natarajkumar, Utkarsh Bheemanaboyna, Devisree Akashapu, Nagraj Gaonkar, Archit Joshi</strong></p>
<p>The widespread and rapid adoption of AI-generated content, created by models such as Generative Adversarial Networks (GANs) and Diffusion Models, has revolutionized the digital media landscape by allowing efficient and creative content generation. However, these models also blur the difference between real images and AI-generated synthetic images, raising concerns regarding content authenticity and integrity. While many existing solutions to detect fake images focus solely on classification and higher-resolution images, they often lack transparency in their decision-making, making it difficult for users to understand why an image is classified as fake. In this paper, we present VERITAS, a comprehensive framework that not only accurately detects whether a small (32x32) image is AI-generated but also explains why it was classified that way through artifact localization and semantic reasoning. VERITAS produces human-readable explanations that describe key artifacts in synthetic images. We show that this architecture offers clear explanations of the basis of zero-shot synthetic image detection tasks. Code and relevant prompts can be found at <a target="_blank" rel="noopener" href="https://github.com/V-i-g-n-e-s-h-N/VERITAS">https://github.com/V-i-g-n-e-s-h-N/VERITAS</a> . </p>
<blockquote>
<p>ç”±ç”Ÿæˆå¯¹æŠ—ç½‘ç»œï¼ˆGANsï¼‰å’Œæ‰©æ•£æ¨¡å‹ç­‰æ¨¡å‹ç”Ÿæˆçš„AIç”Ÿæˆå†…å®¹å¾—åˆ°å¹¿æ³›ä¸”è¿…é€Ÿçš„é‡‡çº³ï¼Œè¿™é€šè¿‡å…è®¸é«˜æ•ˆå’Œæœ‰åˆ›æ„çš„å†…å®¹ç”Ÿæˆï¼Œå·²ç»å½»åº•æ”¹å˜äº†æ•°å­—åª’ä½“é¢†åŸŸã€‚ç„¶è€Œï¼Œè¿™äº›æ¨¡å‹ä¹Ÿæ¨¡ç³Šäº†çœŸå®å›¾åƒå’ŒAIç”Ÿæˆçš„åˆæˆå›¾åƒä¹‹é—´çš„å·®å¼‚ï¼Œå¼•å‘äº†äººä»¬å¯¹å†…å®¹çœŸå®æ€§å’Œå®Œæ•´æ€§çš„æ‹…å¿§ã€‚è™½ç„¶è®¸å¤šç°æœ‰çš„æ£€æµ‹è™šå‡å›¾åƒè§£å†³æ–¹æ¡ˆåªå…³æ³¨åˆ†ç±»å’Œæ›´é«˜åˆ†è¾¨ç‡çš„å›¾åƒï¼Œä½†å®ƒä»¬åœ¨åšå†³ç­–æ—¶å¾€å¾€ç¼ºä¹é€æ˜åº¦ï¼Œä½¿å¾—ç”¨æˆ·éš¾ä»¥ç†è§£ä¸ºä½•æŸå›¾åƒä¼šè¢«å½’ç±»ä¸ºè™šå‡å›¾åƒã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†VERITASï¼Œè¿™æ˜¯ä¸€ä¸ªå…¨é¢çš„æ¡†æ¶ï¼Œä¸ä»…å‡†ç¡®æ£€æµ‹å°åˆ°ï¼ˆ32x32ï¼‰çš„å›¾åƒæ˜¯å¦æ˜¯AIç”Ÿæˆçš„ï¼Œè€Œä¸”è¿˜é€šè¿‡ä¼ªå½±å®šä½å’Œè¯­ä¹‰æ¨ç†æ¥è§£é‡Šå°†å…¶åˆ†ç±»ä¸ºè™šå‡çš„åŸå› ã€‚VERITASäº§ç”Ÿäººç±»å¯è¯»çš„è§£é‡Šï¼Œæè¿°åˆæˆå›¾åƒä¸­çš„å…³é”®ä¼ªå½±ã€‚æˆ‘ä»¬è¯æ˜è¯¥æ¶æ„åœ¨é›¶æ ·æœ¬åˆæˆå›¾åƒæ£€æµ‹ä»»åŠ¡ä¸­æä¾›äº†æ¸…æ™°çš„è§£é‡Šã€‚ä»£ç å’Œç›¸å…³æç¤ºå¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/V-i-g-n-e-s-h-N/VERITAS%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/V-i-g-n-e-s-h-N/VERITASæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.05146v1">PDF</a> </p>
<p><strong>Summary</strong><br>     äººå·¥æ™ºèƒ½ç”Ÿæˆçš„å†…å®¹ï¼Œå¦‚é€šè¿‡ç”Ÿæˆå¯¹æŠ—ç½‘ç»œï¼ˆGANsï¼‰å’Œæ‰©æ•£æ¨¡å‹åˆ›å»ºçš„ï¼Œå·²ç»å½»åº•æ”¹å˜äº†æ•°å­—åª’ä½“é¢†åŸŸï¼Œå®ç°äº†é«˜æ•ˆä¸”å¯Œæœ‰åˆ›æ„çš„å†…å®¹ç”Ÿæˆã€‚ç„¶è€Œï¼Œè¿™äº›æ¨¡å‹ä¹Ÿæ¨¡ç³Šäº†çœŸå®å›¾åƒå’Œäººå·¥æ™ºèƒ½ç”Ÿæˆçš„åˆæˆå›¾åƒä¹‹é—´çš„å·®å¼‚ï¼Œå¼•å‘äº†å…³äºå†…å®¹çœŸå®æ€§å’Œå®Œæ•´æ€§çš„æ‹…å¿§ã€‚VERITASæ¡†æ¶ä¸ä»…èƒ½å‡†ç¡®æ£€æµ‹å°åˆ°ï¼ˆ32x32ï¼‰çš„å›¾åƒæ˜¯å¦ç”±äººå·¥æ™ºèƒ½ç”Ÿæˆï¼Œè¿˜èƒ½é€šè¿‡ä¼ªå½±å®šä½å’Œè¯­ä¹‰æ¨ç†è§£é‡Šåˆ†ç±»åŸå› ï¼Œä¸ºç”¨æˆ·æä¾›å¯è¯»çš„è§£é‡Šã€‚è¯¥æ¶æ„ä¸ºåŸºäºé›¶æ ·æœ¬çš„åˆæˆå›¾åƒæ£€æµ‹ä»»åŠ¡æä¾›äº†æ¸…æ™°çš„è§£é‡ŠåŸºç¡€ã€‚æœ‰å…³ä»£ç å’Œç›¸å…³æç¤ºï¼Œè¯·è®¿é—®ï¼š<a target="_blank" rel="noopener" href="https://github.com/V-i-g-n-e-s-h-N/VERITAS">é“¾æ¥åœ°å€</a>ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>AIç”Ÿæˆçš„å†…å®¹å·²ç»æ”¹å˜äº†æ•°å­—åª’ä½“é¢†åŸŸï¼Œå¸¦æ¥äº†é«˜æ•ˆå’Œåˆ›é€ æ€§çš„å†…å®¹ç”Ÿæˆæ–¹å¼ã€‚</li>
<li>AIæ¨¡å‹ï¼ˆå¦‚GANså’ŒDiffusion Modelsï¼‰ç”Ÿæˆçš„å›¾åƒä¸çœŸå®å›¾åƒä¹‹é—´çš„ç•Œé™æ¨¡ç³Šï¼Œå¼•å‘å¯¹å†…å®¹çœŸå®æ€§çš„å…³æ³¨ã€‚</li>
<li>VERITASæ¡†æ¶èƒ½å‡†ç¡®æ£€æµ‹å°åˆ°ï¼ˆ32x32ï¼‰çš„å›¾åƒæ˜¯å¦ç”±AIç”Ÿæˆã€‚</li>
<li>VERITASä¸ä»…æ£€æµ‹å›¾åƒï¼Œè¿˜èƒ½é€šè¿‡ä¼ªå½±å®šä½å’Œè¯­ä¹‰æ¨ç†è§£é‡Šæ£€æµ‹ç»“æœï¼Œä¸ºç”¨æˆ·æä¾›æ¸…æ™°çš„å¯è¯»è§£é‡Šã€‚</li>
<li>VERITASæ¡†æ¶åœ¨åˆæˆå›¾åƒæ£€æµ‹ä»»åŠ¡ä¸­æä¾›äº†æ¸…æ™°çš„è§£é‡ŠåŸºç¡€ã€‚</li>
<li>è¯¥è®ºæ–‡æä¾›äº†ä¸€ä¸ªGitHubé“¾æ¥ï¼Œå…¶ä¸­åŒ…å«ä»£ç å’Œç›¸å…³æç¤ºï¼Œä¾¿äºç”¨æˆ·è®¿é—®å’Œä½¿ç”¨VERITASæ¡†æ¶ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.05146">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-f35c5f89440b418db34a0e475c8b6f78.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e5dbd9662ce17318decd2c7aaf054087.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-78956d47ec765b77e63d1d3923498627.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="AI-Driven-Cytomorphology-Image-Synthesis-for-Medical-Diagnostics"><a href="#AI-Driven-Cytomorphology-Image-Synthesis-for-Medical-Diagnostics" class="headerlink" title="AI-Driven Cytomorphology Image Synthesis for Medical Diagnostics"></a>AI-Driven Cytomorphology Image Synthesis for Medical Diagnostics</h2><p><strong>Authors:Jan Carreras Boada, Rao Muhammad Umer, Carsten Marr</strong></p>
<p>Biomedical datasets often contain a large sample imbalance and are subject to strict privacy constraints, which together hinder the development of accurate machine learning models. One potential solution is to generate synthetic images, as this can improve data availability while preserving patient privacy. However, it remains difficult to generate synthetic images of sufficient quality for training robust classifiers. In this work, we focus on the classification of single white blood cells, a key component in the diagnosis of hematological diseases such as acute myeloid leukemia (AML), a severe blood cancer. We demonstrate how synthetic images generated with a fine-tuned stable diffusion model using LoRA weights when guided by real few-shot samples of the target white blood cell classes, can enhance classifier performance for limited data. When training a ResNet classifier, accuracy increased from 27.3% to 78.4% (+51.1%) by adding 5000 synthetic images per class to a small and highly imbalanced real dataset. For a CLIP-based classifier, the accuracy improved from 61.8% to 76.8% (+15.0%). The synthetic images are highly similar to real images, and they can help overcome dataset limitations, enhancing model generalization. Our results establish synthetic images as a tool in biomedical research, improving machine learning models, and facilitating medical diagnosis and research. </p>
<blockquote>
<p>ç”Ÿç‰©åŒ»å­¦æ•°æ®é›†å¸¸å¸¸åŒ…å«å¤§é‡çš„æ ·æœ¬ä¸å‡è¡¡é—®é¢˜ï¼Œå¹¶å—åˆ°ä¸¥æ ¼çš„éšç§çº¦æŸçš„é™åˆ¶ï¼Œè¿™ä¸¤è€…å…±åŒé˜»ç¢äº†å‡†ç¡®æœºå™¨å­¦ä¹ æ¨¡å‹çš„å‘å±•ã€‚ä¸€ç§æ½œåœ¨çš„è§£å†³æ–¹æ¡ˆæ˜¯ç”Ÿæˆåˆæˆå›¾åƒï¼Œå› ä¸ºè¿™å¯ä»¥æ”¹å–„æ•°æ®å¯ç”¨æ€§åŒæ—¶ä¿æŠ¤ç—…äººéšç§ã€‚ç„¶è€Œï¼Œç”Ÿæˆè¶³å¤Ÿè´¨é‡çš„åˆæˆå›¾åƒä»¥è®­ç»ƒç¨³å¥çš„åˆ†ç±»å™¨ä»ç„¶æ˜¯ä¸€é¡¹æŒ‘æˆ˜ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬ä¸“æ³¨äºå•ä¸€ç™½è¡€çƒçš„åˆ†ç±»ï¼Œè¿™æ˜¯æ€¥æ€§é«“ç³»ç™½è¡€ç—…ï¼ˆAMLï¼‰ç­‰è¡€æ¶²ç–¾ç—…è¯Šæ–­çš„å…³é”®ç»„æˆéƒ¨åˆ†ï¼Œè¿™æ˜¯ä¸€ç§ä¸¥é‡çš„è¡€æ¶²ç™Œç—‡ã€‚æˆ‘ä»¬å±•ç¤ºäº†å¦‚ä½•ä½¿ç”¨å¾®è°ƒåçš„ç¨³å®šæ‰©æ•£æ¨¡å‹ï¼Œç»“åˆç›®æ ‡ç™½è¡€çƒç±»åˆ«çš„çœŸå®å°‘é‡æ ·æœ¬ä½œä¸ºæŒ‡å¯¼ï¼Œç”Ÿæˆåˆæˆå›¾åƒï¼Œä»¥æé«˜æœ‰é™æ•°æ®çš„åˆ†ç±»å™¨æ€§èƒ½ã€‚å½“å¯¹ResNetåˆ†ç±»å™¨è¿›è¡Œè®­ç»ƒæ—¶ï¼Œé€šè¿‡å‘ä¸€ä¸ªå°è€Œé«˜åº¦ä¸å¹³è¡¡çš„çœŸå®æ•°æ®é›†æ·»åŠ æ¯ç±»5000å¼ åˆæˆå›¾åƒï¼Œå‡†ç¡®ç‡ä»27.3%æé«˜åˆ°78.4%ï¼ˆ+51.1%ï¼‰ã€‚å¯¹äºåŸºäºCLIPçš„åˆ†ç±»å™¨ï¼Œå‡†ç¡®ç‡ä»61.8%æé«˜åˆ°76.8%ï¼ˆ+15.0%ï¼‰ã€‚åˆæˆå›¾åƒä¸çœŸå®å›¾åƒé«˜åº¦ç›¸ä¼¼ï¼Œæœ‰åŠ©äºå…‹æœæ•°æ®é›†é™åˆ¶ï¼Œæé«˜æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ã€‚æˆ‘ä»¬çš„ç»“æœå°†åˆæˆå›¾åƒç¡®ç«‹ä¸ºç”Ÿç‰©åŒ»å­¦ç ”ç©¶ä¸­çš„ä¸€ç§å·¥å…·ï¼Œå¯æ”¹å–„æœºå™¨å­¦ä¹ æ¨¡å‹ï¼Œä¿ƒè¿›åŒ»å­¦è¯Šæ–­å’Œç ”ç©¶çš„è¿›è¡Œã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.05063v1">PDF</a> 8 pages, 6 figures, 2 tables. Final Degree Project (TFG) submitted at   ESCI-UPF and conducted at Helmholtz Munich</p>
<p><strong>Summary</strong><br>ç”Ÿç‰©åŒ»å­¦æ•°æ®é›†ä¸­å­˜åœ¨æ ·æœ¬ä¸å¹³è¡¡å’Œéšç§ä¿æŠ¤é™åˆ¶çš„é—®é¢˜ï¼Œå½±å“äº†æœºå™¨å­¦ä¹ æ¨¡å‹çš„å‡†ç¡®æ€§ã€‚æœ¬ç ”ç©¶é€šè¿‡åˆ©ç”¨å¾®è°ƒåçš„ç¨³å®šæ‰©æ•£æ¨¡å‹ç”Ÿæˆåˆæˆå›¾åƒï¼Œç»“åˆçœŸå®æ ·æœ¬è®­ç»ƒåˆ†ç±»å™¨ï¼Œæé«˜äº†å¯¹ç™½è¡€ç—…ç»†èƒçš„åˆ†ç±»æ€§èƒ½ã€‚åˆæˆå›¾åƒåœ¨å¢åŠ æœ‰é™æ•°æ®çš„åŒæ—¶ï¼Œæé«˜äº†åˆ†ç±»å™¨çš„å‡†ç¡®ç‡ï¼Œå¹¶æœ‰åŠ©äºå…‹æœæ•°æ®é›†é™åˆ¶ï¼Œå¢å¼ºæ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ã€‚è¿™ä¸ºç”Ÿç‰©åŒ»å­¦ç ”ç©¶ä¸­çš„æœºå™¨å­¦ä¹ æ¨¡å‹å’ŒåŒ»ç–—è¯Šæ–­æä¾›äº†æ–°çš„å·¥å…·ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç”Ÿç‰©åŒ»å­¦æ•°æ®é›†é¢ä¸´æ ·æœ¬ä¸å¹³è¡¡å’Œéšç§ä¿æŠ¤é™åˆ¶çš„æŒ‘æˆ˜ã€‚</li>
<li>åˆæˆå›¾åƒç”Ÿæˆæ˜¯è§£å†³è¿™äº›é—®é¢˜çš„ä¸€ç§æ½œåœ¨è§£å†³æ–¹æ¡ˆã€‚</li>
<li>åˆ©ç”¨å¾®è°ƒåçš„ç¨³å®šæ‰©æ•£æ¨¡å‹å’ŒçœŸå®æ ·æœ¬å¼•å¯¼ï¼Œå¯ä»¥ç”Ÿæˆé«˜è´¨é‡åˆæˆå›¾åƒã€‚</li>
<li>åˆæˆå›¾åƒå¯æé«˜åˆ†ç±»å™¨çš„æ€§èƒ½ï¼Œç‰¹åˆ«æ˜¯å¯¹äºæœ‰é™æ•°æ®çš„æƒ…å†µã€‚</li>
<li>åœ¨ä½¿ç”¨ResNetåˆ†ç±»å™¨çš„æƒ…å†µä¸‹ï¼Œæ·»åŠ åˆæˆå›¾åƒä½¿å‡†ç¡®ç‡ä»27.3%æé«˜åˆ°78.4%ï¼ˆ+51.1%ï¼‰ã€‚</li>
<li>å¯¹äºåŸºäºCLIPçš„åˆ†ç±»å™¨ï¼Œå‡†ç¡®ç‡ä»61.8%æé«˜åˆ°76.8%ï¼ˆ+15.0%ï¼‰ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.05063">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-bf4af644bf24010333f4fb8bda09822e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-740645f663f87532e673d997004ae128.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c6ebe5d1c347b5b6edace32166af640b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-49aad50fd218bb392795b92cc1f8e6d9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c468a876f50d547475b6a0ca545eeb8a.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="TLB-VFI-Temporal-Aware-Latent-Brownian-Bridge-Diffusion-for-Video-Frame-Interpolation"><a href="#TLB-VFI-Temporal-Aware-Latent-Brownian-Bridge-Diffusion-for-Video-Frame-Interpolation" class="headerlink" title="TLB-VFI: Temporal-Aware Latent Brownian Bridge Diffusion for Video Frame   Interpolation"></a>TLB-VFI: Temporal-Aware Latent Brownian Bridge Diffusion for Video Frame   Interpolation</h2><p><strong>Authors:Zonglin Lyu, Chen Chen</strong></p>
<p>Video Frame Interpolation (VFI) aims to predict the intermediate frame $I_n$ (we use n to denote time in videos to avoid notation overload with the timestep $t$ in diffusion models) based on two consecutive neighboring frames $I_0$ and $I_1$. Recent approaches apply diffusion models (both image-based and video-based) in this task and achieve strong performance. However, image-based diffusion models are unable to extract temporal information and are relatively inefficient compared to non-diffusion methods. Video-based diffusion models can extract temporal information, but they are too large in terms of training scale, model size, and inference time. To mitigate the above issues, we propose Temporal-Aware Latent Brownian Bridge Diffusion for Video Frame Interpolation (TLB-VFI), an efficient video-based diffusion model. By extracting rich temporal information from video inputs through our proposed 3D-wavelet gating and temporal-aware autoencoder, our method achieves 20% improvement in FID on the most challenging datasets over recent SOTA of image-based diffusion models. Meanwhile, due to the existence of rich temporal information, our method achieves strong performance while having 3times fewer parameters. Such a parameter reduction results in 2.3x speed up. By incorporating optical flow guidance, our method requires 9000x less training data and achieves over 20x fewer parameters than video-based diffusion models. Codes and results are available at our project page: <a target="_blank" rel="noopener" href="https://zonglinl.github.io/tlbvfi_page">https://zonglinl.github.io/tlbvfi_page</a>. </p>
<blockquote>
<p>è§†é¢‘å¸§æ’å€¼ï¼ˆVFIï¼‰æ—¨åœ¨åŸºäºä¸¤ä¸ªè¿ç»­ç›¸é‚»å¸§$I_0$å’Œ$I_1$æ¥é¢„æµ‹ä¸­é—´å¸§$I_n$ï¼ˆæˆ‘ä»¬ç”¨næ¥è¡¨ç¤ºè§†é¢‘ä¸­çš„æ—¶é—´ï¼Œä»¥é¿å…ä¸æ‰©æ•£æ¨¡å‹ä¸­çš„æ—¶é—´æ­¥é•¿tçš„ç¬¦å·è¿‡è½½ï¼‰ã€‚æœ€è¿‘çš„æ–¹æ³•åœ¨è¯¥ä»»åŠ¡ä¸­åº”ç”¨äº†æ‰©æ•£æ¨¡å‹ï¼ˆåŸºäºå›¾åƒå’ŒåŸºäºè§†é¢‘çš„ï¼‰ï¼Œå¹¶å–å¾—äº†å¼ºå¤§çš„æ€§èƒ½ã€‚ç„¶è€Œï¼ŒåŸºäºå›¾åƒçš„æ‰©æ•£æ¨¡å‹æ— æ³•æå–æ—¶é—´ä¿¡æ¯ï¼Œä¸éæ‰©æ•£æ–¹æ³•ç›¸æ¯”æ•ˆç‡ç›¸å¯¹è¾ƒä½ã€‚åŸºäºè§†é¢‘çš„æ‰©æ•£æ¨¡å‹å¯ä»¥æå–æ—¶é—´ä¿¡æ¯ï¼Œä½†åœ¨è®­ç»ƒè§„æ¨¡ã€æ¨¡å‹å¤§å°å’Œæ¨ç†æ—¶é—´æ–¹é¢è¿‡äºåºå¤§ã€‚ä¸ºäº†ç¼“è§£ä¸Šè¿°é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ç”¨äºè§†é¢‘å¸§æ’å€¼çš„æ—¶åºæ„ŸçŸ¥æ½œåœ¨å¸ƒæœ—æ¡¥æ‰©æ•£ï¼ˆTLB-VFIï¼‰ï¼Œä¸€ç§é«˜æ•ˆçš„åŸºäºè§†é¢‘æ‰©æ•£æ¨¡å‹ã€‚é€šè¿‡æˆ‘ä»¬æå‡ºçš„3Då°æ³¢é—¨æ§å’Œæ—¶åºæ„ŸçŸ¥è‡ªåŠ¨ç¼–ç å™¨ä»è§†é¢‘è¾“å…¥ä¸­æå–ä¸°å¯Œçš„æ—¶é—´ä¿¡æ¯ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨æœ€å…·æŒ‘æˆ˜æ€§çš„æ•°æ®é›†ä¸Šçš„FIDæé«˜äº†20%ï¼Œè¶…è¿‡äº†æœ€è¿‘çš„åŸºäºå›¾åƒçš„æ‰©æ•£æ¨¡å‹çš„æœ€å…ˆè¿›æ–¹æ³•ã€‚åŒæ—¶ï¼Œç”±äºå­˜åœ¨ä¸°å¯Œçš„æ—¶é—´ä¿¡æ¯ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨å‚æ•°æ›´å°‘çš„æƒ…å†µä¸‹å®ç°äº†å¼ºå¤§çš„æ€§èƒ½ï¼Œå‚æ•°å‡å°‘äº†ä¸‰å€ã€‚è¿™ç§å‚æ•°å‡å°‘å¯¼è‡´äº†2.3å€çš„åŠ é€Ÿã€‚é€šè¿‡ç»“åˆå…‰æµå¼•å¯¼ï¼Œæˆ‘ä»¬çš„æ–¹æ³•éœ€è¦9000å€æ›´å°‘çš„è®­ç»ƒæ•°æ®ï¼Œå¹¶ä¸”ä¸åŸºäºè§†é¢‘çš„æ‰©æ•£æ¨¡å‹ç›¸æ¯”ï¼Œå‚æ•°å‡å°‘äº†è¶…è¿‡20å€ã€‚ç›¸å…³ä»£ç å’Œç»“æœå¯åœ¨æˆ‘ä»¬çš„é¡¹ç›®é¡µé¢æŸ¥çœ‹ï¼š<a target="_blank" rel="noopener" href="https://zonglinl.github.io/tlbvfi_page">https://zonglinl.github.io/tlbvfi_page</a>ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.04984v1">PDF</a> </p>
<p><strong>æ‘˜è¦</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†é’ˆå¯¹è§†é¢‘å¸§æ’å€¼ï¼ˆVFIï¼‰ä»»åŠ¡çš„Temporal-Aware Latent Brownian Bridge Diffusionæ¨¡å‹ï¼ˆTLB-VFIï¼‰ã€‚è¯¥æ¨¡å‹èƒ½å¤Ÿæœ‰æ•ˆç»“åˆå›¾åƒä¸è§†é¢‘æ‰©æ•£æ¨¡å‹çš„ä¼˜ç‚¹ï¼Œè§£å†³äº†ç°æœ‰æ¨¡å‹åœ¨æå–æ—¶é—´ä¿¡æ¯å’Œæ•ˆç‡æ–¹é¢çš„é—®é¢˜ã€‚é€šè¿‡å¼•å…¥3D-waveleté—¨æ§æœºåˆ¶å’Œæ—¶ç©ºæ„ŸçŸ¥è‡ªåŠ¨ç¼–ç å™¨ï¼ŒTLB-VFIåœ¨æœ€å…·æŒ‘æˆ˜æ€§çš„æ•°æ®é›†ä¸Šå®ç°äº†æ¯”å½“å‰å…ˆè¿›çš„å›¾åƒæ‰©æ•£æ¨¡å‹æ›´ä½çš„FIDå¾—åˆ†ï¼Œæ€§èƒ½æå‡20%ã€‚åŒæ—¶ï¼Œç”±äºå……åˆ†æå–äº†æ—¶é—´ä¿¡æ¯ï¼Œè¯¥æ¨¡å‹åœ¨å‡å°‘å‚æ•°çš„åŒæ—¶ä¿æŒäº†å¼ºå¤§çš„æ€§èƒ½ï¼Œå‚æ•°å‡å°‘ä¸‰å€ï¼Œé€Ÿåº¦æå‡2.3å€ã€‚é€šè¿‡å¼•å…¥å…‰æµå¼•å¯¼æœºåˆ¶ï¼ŒTLB-VFIè¿›ä¸€æ­¥å‡å°‘äº†è®­ç»ƒæ•°æ®éœ€æ±‚ï¼Œä¸ä¼ ç»Ÿè§†é¢‘æ‰©æ•£æ¨¡å‹ç›¸æ¯”ï¼Œå‚æ•°å‡å°‘äº†è¶…è¿‡20å€ã€‚ç›¸å…³ä»£ç å’Œç»“æœå¯è®¿é—®é¡¹ç›®é¡µé¢æŸ¥çœ‹ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>è§†é¢‘å¸§æ’å€¼ï¼ˆVFIï¼‰ä»»åŠ¡æ—¨åœ¨åŸºäºä¸¤ä¸ªè¿ç»­å¸§é¢„æµ‹ä¸­é—´å¸§ã€‚</li>
<li>ç°æœ‰å›¾åƒæ‰©æ•£æ¨¡å‹æ— æ³•æœ‰æ•ˆæå–æ—¶é—´ä¿¡æ¯ï¼Œæ•ˆç‡è¾ƒä½ã€‚</li>
<li>è§†é¢‘æ‰©æ•£æ¨¡å‹è™½èƒ½æå–æ—¶é—´ä¿¡æ¯ï¼Œä½†è®­ç»ƒè§„æ¨¡ã€æ¨¡å‹å¤§å°å’Œæ¨ç†æ—¶é—´æˆæœ¬è¾ƒé«˜ã€‚<br>4.TLB-VFIæ¨¡å‹ç»“åˆäº†å›¾åƒä¸è§†é¢‘æ‰©æ•£æ¨¡å‹çš„ä¼˜ç‚¹ï¼Œè§£å†³äº†ä¸Šè¿°é—®é¢˜ã€‚</li>
<li>é€šè¿‡å¼•å…¥3D-waveleté—¨æ§æœºåˆ¶å’Œæ—¶ç©ºæ„ŸçŸ¥è‡ªåŠ¨ç¼–ç å™¨ï¼ŒTLB-VFIå®ç°äº†æ€§èƒ½æå‡ã€‚</li>
<li>åœ¨æœ€å…·æŒ‘æˆ˜æ€§çš„æ•°æ®é›†ä¸Šå®ç°äº†å‚æ•°å‡å°‘å’Œé€Ÿåº¦æå‡çš„ä¼˜åŒ–ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.04984">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-c1ecb21ccbcfbe999b4b591c6ce8af62.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-1dee5df0d87cd9469361d42ff1d75a03.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-15e5cc8d91148c6709926bab0cc830e7.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="DC-AR-Efficient-Masked-Autoregressive-Image-Generation-with-Deep-Compression-Hybrid-Tokenizer"><a href="#DC-AR-Efficient-Masked-Autoregressive-Image-Generation-with-Deep-Compression-Hybrid-Tokenizer" class="headerlink" title="DC-AR: Efficient Masked Autoregressive Image Generation with Deep   Compression Hybrid Tokenizer"></a>DC-AR: Efficient Masked Autoregressive Image Generation with Deep   Compression Hybrid Tokenizer</h2><p><strong>Authors:Yecheng Wu, Junyu Chen, Zhuoyang Zhang, Enze Xie, Jincheng Yu, Junsong Chen, Jinyi Hu, Yao Lu, Song Han, Han Cai</strong></p>
<p>We introduce DC-AR, a novel masked autoregressive (AR) text-to-image generation framework that delivers superior image generation quality with exceptional computational efficiency. Due to the tokenizersâ€™ limitations, prior masked AR models have lagged behind diffusion models in terms of quality or efficiency. We overcome this limitation by introducing DC-HT - a deep compression hybrid tokenizer for AR models that achieves a 32x spatial compression ratio while maintaining high reconstruction fidelity and cross-resolution generalization ability. Building upon DC-HT, we extend MaskGIT and create a new hybrid masked autoregressive image generation framework that first produces the structural elements through discrete tokens and then applies refinements via residual tokens. DC-AR achieves state-of-the-art results with a gFID of 5.49 on MJHQ-30K and an overall score of 0.69 on GenEval, while offering 1.5-7.9x higher throughput and 2.0-3.5x lower latency compared to prior leading diffusion and autoregressive models. </p>
<blockquote>
<p>æˆ‘ä»¬ä»‹ç»äº†DC-ARï¼Œè¿™æ˜¯ä¸€ç§æ–°å‹çš„é¢ç½©è‡ªå›å½’ï¼ˆARï¼‰æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆæ¡†æ¶ï¼Œå®ƒä»¥å‡ºè‰²çš„è®¡ç®—æ•ˆç‡æä¾›äº†å“è¶Šçš„å›¾åƒç”Ÿæˆè´¨é‡ã€‚ç”±äºæ ‡è®°å™¨çš„é™åˆ¶ï¼Œå…ˆå‰çš„é¢ç½©è‡ªå›å½’æ¨¡å‹åœ¨è´¨é‡æˆ–æ•ˆç‡æ–¹é¢è½åäºæ‰©æ•£æ¨¡å‹ã€‚æˆ‘ä»¬é€šè¿‡å¼•å…¥DC-HTâ€”â€”ä¸€ç§ä¸ºARæ¨¡å‹è®¾è®¡çš„æ·±åº¦å‹ç¼©æ··åˆæ ‡è®°å™¨æ¥å…‹æœè¿™ä¸€é™åˆ¶ï¼Œå®ƒåœ¨ä¿æŒé«˜é‡å»ºä¿çœŸåº¦å’Œè·¨åˆ†è¾¨ç‡æ³›åŒ–èƒ½åŠ›çš„åŒæ—¶ï¼Œå®ç°äº†32å€çš„ç©ºé—´å‹ç¼©æ¯”ã€‚åŸºäºDC-HTï¼Œæˆ‘ä»¬æ‰©å±•äº†MaskGITï¼Œå¹¶åˆ›å»ºäº†ä¸€ä¸ªæ–°çš„æ··åˆé¢ç½©è‡ªå›å½’å›¾åƒç”Ÿæˆæ¡†æ¶ï¼Œè¯¥æ¡†æ¶é¦–å…ˆé€šè¿‡ç¦»æ•£ä»¤ç‰Œç”Ÿæˆç»“æ„å…ƒç´ ï¼Œç„¶åé€šè¿‡å‰©ä½™ä»¤ç‰Œè¿›è¡Œå¾®è°ƒã€‚DC-ARåœ¨MJHQ-30Kä¸Šçš„gFIDä¸º5.49ï¼Œåœ¨GenEvalä¸Šçš„æ€»ä½“å¾—åˆ†ä¸º0.69ï¼ŒåŒæ—¶ä¸å…ˆå‰çš„é¢†å…ˆæ‰©æ•£å’Œè‡ªå›å½’æ¨¡å‹ç›¸æ¯”ï¼Œæä¾›1.5-7.9å€çš„é«˜ååé‡ä»¥åŠ2.0-3.5å€çš„ä½å»¶è¿Ÿã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.04947v1">PDF</a> ICCV 2025</p>
<p><strong>Summary</strong><br>     æˆ‘ä»¬æ¨å‡ºäº†DC-ARï¼Œè¿™æ˜¯ä¸€ç§æ–°å‹çš„é¢å…·è‡ªå›å½’ï¼ˆARï¼‰æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆæ¡†æ¶ï¼Œå®ƒä»¥å“è¶Šçš„è®¡ç®—æ•ˆç‡æä¾›äº†ä¼˜è´¨çš„å›¾åƒç”Ÿæˆè´¨é‡ã€‚ç”±äºä»¤ç‰Œå™¨çš„é™åˆ¶ï¼Œå…ˆå‰çš„æ©æ¨¡ARæ¨¡å‹åœ¨è´¨é‡æˆ–æ•ˆç‡æ–¹é¢è½åäºæ‰©æ•£æ¨¡å‹ã€‚é€šè¿‡å¼•å…¥DC-HTâ€”â€”ä¸€ç§å®ç°32å€ç©ºé—´å‹ç¼©æ¯”çš„æ·±åº¦å‹ç¼©æ··åˆä»¤ç‰Œå™¨ï¼Œæˆ‘ä»¬å…‹æœäº†è¿™ä¸€é™åˆ¶ï¼ŒåŒæ—¶ä¿æŒäº†é«˜é‡å»ºä¿çœŸåº¦å’Œè·¨åˆ†è¾¨ç‡çš„é€šç”¨æ€§ã€‚åŸºäºDC-HTï¼Œæˆ‘ä»¬æ‰©å±•äº†MaskGITï¼Œåˆ›å»ºäº†ä¸€ä¸ªæ–°çš„æ··åˆæ©æ¨¡è‡ªå›å½’å›¾åƒç”Ÿæˆæ¡†æ¶ï¼Œè¯¥æ¡†æ¶é¦–å…ˆé€šè¿‡ç¦»æ•£ä»¤ç‰Œç”Ÿæˆç»“æ„å…ƒç´ ï¼Œç„¶åé€šè¿‡æ®‹å·®ä»¤ç‰Œè¿›è¡Œç»†åŒ–ã€‚DC-ARåœ¨MJHQ-30Kä¸Šå®ç°äº†5.49çš„gFIDå’ŒGenEvalä¸Šçš„0.69çš„ç»¼åˆå¾—åˆ†ï¼ŒåŒæ—¶ä¸å…ˆå‰çš„é¢†å…ˆæ‰©æ•£å’Œè‡ªå›å½’æ¨¡å‹ç›¸æ¯”ï¼Œæä¾›äº†1.5-7.9å€çš„æ›´é«˜ååé‡ä»¥åŠ2.0-3.5å€æ›´ä½çš„å»¶è¿Ÿã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>DC-ARæ˜¯ä¸€ä¸ªæ–°å‹çš„æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆæ¡†æ¶ï¼ŒåŸºäºæ©æ¨¡è‡ªå›å½’ï¼ˆARï¼‰æŠ€æœ¯ã€‚</li>
<li>ç›¸æ¯”å…¶ä»–æ¨¡å‹ï¼ŒDC-ARåœ¨å›¾åƒç”Ÿæˆè´¨é‡å’Œè®¡ç®—æ•ˆç‡ä¸Šè¡¨ç°æ›´ä¼˜ç§€ã€‚</li>
<li>DC-HTæ˜¯DC-ARçš„ä¸€ä¸ªé‡è¦ç»„æˆéƒ¨åˆ†ï¼Œæ˜¯ä¸€ç§æ·±åº¦å‹ç¼©æ··åˆä»¤ç‰Œå™¨ï¼Œå¯ä»¥å®ç°é«˜å‹ç¼©æ¯”åŒæ—¶ä¿æŒé‡å»ºè´¨é‡ã€‚</li>
<li>DC-ARæ¡†æ¶ç»“åˆäº†ç¦»æ•£ä»¤ç‰Œå’Œæ®‹å·®ä»¤ç‰Œï¼Œé¦–å…ˆç”Ÿæˆç»“æ„å…ƒç´ å†è¿›è¡Œç»†åŒ–ã€‚</li>
<li>DC-ARåœ¨å¤šä¸ªè¯„ä¼°æŒ‡æ ‡ä¸Šè¾¾åˆ°äº†å…ˆè¿›æ°´å¹³ï¼ŒåŒ…æ‹¬MJHQ-30Kä¸Šçš„gFIDå’ŒGenEvalçš„ç»¼åˆå¾—åˆ†ã€‚</li>
<li>DC-ARç›¸æ¯”å…¶ä»–é¢†å…ˆæ¨¡å‹ï¼Œæä¾›äº†æ›´é«˜çš„ååé‡å’Œæ›´ä½çš„å»¶è¿Ÿã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.04947">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-69c18ff790e54b7ca1f1f82658d8c1f7.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-2085553e7e6f312b23705e5eda303f87.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-8226241be331c2882ca1336261d71510.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-5dd4731767b7bb8b98419a647acdaa82.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-8a23340c5dcae25e4cdeb3276c34cfaf.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Music-Boomerang-Reusing-Diffusion-Models-for-Data-Augmentation-and-Audio-Manipulation"><a href="#Music-Boomerang-Reusing-Diffusion-Models-for-Data-Augmentation-and-Audio-Manipulation" class="headerlink" title="Music Boomerang: Reusing Diffusion Models for Data Augmentation and   Audio Manipulation"></a>Music Boomerang: Reusing Diffusion Models for Data Augmentation and   Audio Manipulation</h2><p><strong>Authors:Alexander Fichtinger, Jan SchlÃ¼ter, Gerhard Widmer</strong></p>
<p>Generative models of music audio are typically used to generate output based solely on a text prompt or melody. Boomerang sampling, recently proposed for the image domain, allows generating output close to an existing example, using any pretrained diffusion model. In this work, we explore its application in the audio domain as a tool for data augmentation or content manipulation. Specifically, implementing Boomerang sampling for Stable Audio Open, we augment training data for a state-of-the-art beat tracker, and attempt to replace musical instruments in recordings. Our results show that the rhythmic structure of existing examples is mostly preserved, that it improves performance of the beat tracker, but only in scenarios of limited training data, and that it can accomplish text-based instrument replacement on monophonic inputs. We publish our implementation to invite experiments on data augmentation in other tasks and explore further applications. </p>
<blockquote>
<p>éŸ³ä¹éŸ³é¢‘çš„ç”Ÿæˆæ¨¡å‹é€šå¸¸ä»…åŸºäºæ–‡æœ¬æç¤ºæˆ–æ—‹å¾‹æ¥ç”Ÿæˆè¾“å‡ºã€‚æœ€è¿‘ä¸ºå›¾åƒé¢†åŸŸæå‡ºçš„Boomerangé‡‡æ ·å…è®¸ä½¿ç”¨ä»»ä½•é¢„è®­ç»ƒçš„æ‰©æ•£æ¨¡å‹æ¥ç”Ÿæˆæ¥è¿‘ç°æœ‰ç¤ºä¾‹çš„è¾“å‡ºã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æ¢ç´¢å…¶åœ¨éŸ³é¢‘é¢†åŸŸä½œä¸ºæ•°æ®å¢å¼ºæˆ–å†…å®¹æ“ä½œå·¥å…·çš„åº”ç”¨ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬ä¸ºStable Audio Openå®ç°äº†Boomerangé‡‡æ ·ï¼Œä¸ºæœ€å…ˆè¿›çš„èŠ‚æ‹è·Ÿè¸ªå™¨å¢åŠ äº†è®­ç»ƒæ•°æ®ï¼Œå¹¶å°è¯•åœ¨å½•éŸ³ä¸­æ›¿æ¢ä¹å™¨ã€‚æˆ‘ä»¬çš„ç»“æœè¡¨æ˜ï¼Œç°æœ‰ç¤ºä¾‹çš„èŠ‚å¥ç»“æ„å¤§å¤šå¾—ä»¥ä¿ç•™ï¼Œå®ƒæé«˜äº†åœ¨è®­ç»ƒæ•°æ®æœ‰é™åœºæ™¯ä¸‹çš„èŠ‚æ‹è·Ÿè¸ªå™¨æ€§èƒ½ï¼Œå¹¶ä¸”èƒ½å¤Ÿåœ¨å•å£°é“è¾“å…¥ä¸Šå®ç°åŸºäºæ–‡æœ¬çš„ä¹å™¨æ›¿æ¢ã€‚æˆ‘ä»¬å‘å¸ƒæˆ‘ä»¬çš„å®ç°ï¼Œä»¥é‚€è¯·å…¶ä»–ä»»åŠ¡çš„å®éªŒå¯¹å¢å¼ºçš„æ•°æ®è¿›è¡Œæ¢ç´¢å¹¶è¿›ä¸€æ­¥ç ”ç©¶å…¶åº”ç”¨ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.04864v1">PDF</a> Accepted at SMC 2025. Code at <a target="_blank" rel="noopener" href="https://malex1106.github.io/boomify/">https://malex1106.github.io/boomify/</a></p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ¢ç´¢äº†Boomerangé‡‡æ ·åœ¨éŸ³é¢‘é¢†åŸŸçš„åº”ç”¨ï¼Œåˆ©ç”¨é¢„è®­ç»ƒçš„æ‰©æ•£æ¨¡å‹ç”Ÿæˆæ¥è¿‘ç°æœ‰æ ·æœ¬çš„è¾“å‡ºï¼Œç”¨äºæ•°æ®å¢å¼ºæˆ–å†…å®¹æ“çºµã€‚å®éªŒè¡¨æ˜ï¼Œè¯¥é‡‡æ ·æ–¹æ³•å¯ä»¥ä¿ç•™ç°æœ‰ç¤ºä¾‹çš„èŠ‚å¥ç»“æ„ï¼Œæå‡æ‰“æ‹å™¨åœ¨æœ‰é™è®­ç»ƒæ•°æ®åœºæ™¯ä¸‹çš„æ€§èƒ½ï¼Œå¹¶èƒ½åœ¨å•å£°é“è¾“å…¥ä¸Šå®ç°åŸºäºæ–‡æœ¬çš„ä¹å™¨æ›¿æ¢ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Boomerangé‡‡æ ·è¢«å¼•å…¥åˆ°éŸ³é¢‘é¢†åŸŸï¼Œå…è®¸ä½¿ç”¨é¢„è®­ç»ƒçš„æ‰©æ•£æ¨¡å‹ç”Ÿæˆæ¥è¿‘ç°æœ‰æ ·æœ¬çš„è¾“å‡ºã€‚</li>
<li>è¯¥æ–¹æ³•ä¸»è¦ç”¨äºæ•°æ®å¢å¼ºå’Œå†…å®¹æ“çºµã€‚</li>
<li>å®æ–½Boomerangé‡‡æ ·åï¼Œå¯ä»¥ä¿ç•™ç°æœ‰ç¤ºä¾‹çš„èŠ‚å¥ç»“æ„ã€‚</li>
<li>åœ¨æœ‰é™è®­ç»ƒæ•°æ®åœºæ™¯ä¸‹ï¼ŒBoomerangé‡‡æ ·å¯ä»¥æå‡æ‰“æ‹å™¨çš„æ€§èƒ½ã€‚</li>
<li>Boomerangé‡‡æ ·èƒ½å¤Ÿå®ç°åŸºäºæ–‡æœ¬çš„ä¹å™¨æ›¿æ¢ã€‚</li>
<li>è¯¥ç ”ç©¶å…¬å¼€äº†å®æ–½æ–¹æ³•ï¼Œé‚€è¯·å…¶ä»–ä»»åŠ¡è¿›è¡Œæ•°æ®å¢å¼ºå®éªŒã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.04864">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-3517f997cf6cca4029e45967c7482252.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9806bd41eb8636b29d5b2c8305fa02dc.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7fc82e6ad241822c5dc86e660d619006.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="ChangeBridge-Spatiotemporal-Image-Generation-with-Multimodal-Controls-for-Remote-Sensing"><a href="#ChangeBridge-Spatiotemporal-Image-Generation-with-Multimodal-Controls-for-Remote-Sensing" class="headerlink" title="ChangeBridge: Spatiotemporal Image Generation with Multimodal Controls   for Remote Sensing"></a>ChangeBridge: Spatiotemporal Image Generation with Multimodal Controls   for Remote Sensing</h2><p><strong>Authors:Zhenghui Zhao, Chen Wu, Di Wang, Hongruixuan Chen, Zhuo Zheng</strong></p>
<p>Recent advancements in generative methods, especially diffusion models, have made great progress in remote sensing image synthesis. Despite these advancements, existing methods have not explored the simulation of future scenarios based on given scenario images. This simulation capability has wide applications for urban planning, land managementChangeBridge: Spatiotemporal Image Generation with Multimodal Controls, and beyond. In this work, we propose ChangeBridge, a conditional spatiotemporal diffusion model. Given pre-event images and conditioned on multimodal spatial controls (e.g., text prompts, instance layouts, and semantic maps), ChangeBridge can synthesize post-event images. The core idea behind ChangeBridge is to modeling the noise-to-image diffusion model, as a pre-to-post diffusion bridge. Conditioned on multimodal controls, ChangeBridge leverages a stochastic Brownian-bridge diffusion, directly modeling the spatiotemporal evolution between pre-event and post-event states. To the best of our knowledge, ChangeBridge is the first spatiotemporal generative model with multimodal controls for remote sensing. Experimental results demonstrate that ChangeBridge can simulate high-fidelity future scenarios aligned with given conditions, including event and event-driven background variations. Code will be available. </p>
<blockquote>
<p>è¿‘å¹´æ¥ï¼Œç”Ÿæˆæ–¹æ³•ï¼Œå°¤å…¶æ˜¯æ‰©æ•£æ¨¡å‹ï¼Œåœ¨é¥æ„Ÿå›¾åƒåˆæˆæ–¹é¢å–å¾—äº†å·¨å¤§è¿›å±•ã€‚å°½ç®¡æœ‰è¿™äº›è¿›å±•ï¼Œç°æœ‰æ–¹æ³•å°šæœªæ¢ç´¢åŸºäºç»™å®šåœºæ™¯å›¾åƒæ¨¡æ‹Ÿæœªæ¥æƒ…æ™¯çš„èƒ½åŠ›ã€‚è¿™ç§æ¨¡æ‹Ÿèƒ½åŠ›åœ¨åŸå¸‚è§„åˆ’ã€åœŸåœ°ç®¡ç†å’Œæ›´å¹¿æ³›çš„åº”ç”¨ä¸­å…·æœ‰å¹¿æ³›çš„åº”ç”¨ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ChangeBridgeï¼Œä¸€ä¸ªæ¡ä»¶æ€§çš„æ—¶ç©ºæ‰©æ•£æ¨¡å‹ã€‚ç»™å®šé¢„äº‹ä»¶å›¾åƒï¼Œå¹¶åŸºäºå¤šæ¨¡å¼ç©ºé—´æ§åˆ¶ï¼ˆå¦‚æ–‡æœ¬æç¤ºã€å®ä¾‹å¸ƒå±€å’Œè¯­ä¹‰åœ°å›¾ï¼‰ï¼ŒChangeBridgeå¯ä»¥åˆæˆåäº‹ä»¶å›¾åƒã€‚ChangeBridgeçš„æ ¸å¿ƒæ€æƒ³æ˜¯å°†å™ªå£°åˆ°å›¾åƒçš„æ‰©æ•£æ¨¡å‹ä½œä¸ºé¢„åˆ°åæ‰©æ•£çš„æ¡¥æ¢ã€‚åŸºäºå¤šæ¨¡å¼æ§åˆ¶ï¼ŒChangeBridgeåˆ©ç”¨éšæœºå¸ƒæœ—æ¡¥æ‰©æ•£ï¼Œç›´æ¥æ¨¡æ‹Ÿé¢„äº‹ä»¶å’Œåäº‹ä»¶çŠ¶æ€ä¹‹é—´çš„æ—¶ç©ºæ¼”åŒ–ã€‚æ®æˆ‘ä»¬æ‰€çŸ¥ï¼ŒChangeBridgeæ˜¯é¥æ„Ÿé¢†åŸŸä¸­å…·æœ‰å¤šæ¨¡å¼æ§åˆ¶çš„ç¬¬ä¸€ä¸ªæ—¶ç©ºç”Ÿæˆæ¨¡å‹ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒChangeBridgeå¯ä»¥æ¨¡æ‹Ÿä¸ç»™å®šæ¡ä»¶ç›¸ç¬¦çš„é«˜ä¿çœŸæœªæ¥æƒ…æ™¯ï¼ŒåŒ…æ‹¬äº‹ä»¶å’Œäº‹ä»¶é©±åŠ¨çš„èƒŒæ™¯å˜åŒ–ã€‚ä»£ç å°†å¯ç”¨ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.04678v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æ‰©æ•£æ¨¡å‹åœ¨é¥æ„Ÿå›¾åƒåˆæˆé¢†åŸŸå–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œä½†ç°æœ‰æ–¹æ³•å°šæœªæ¢ç´¢åŸºäºç»™å®šåœºæ™¯å›¾åƒæ¨¡æ‹Ÿæœªæ¥æƒ…æ™¯çš„èƒ½åŠ›ã€‚æœ¬æ–‡æå‡ºChangeBridgeï¼Œä¸€ç§æ¡ä»¶æ—¶ç©ºæ‰©æ•£æ¨¡å‹ï¼Œç»™å®šé¢„äº‹ä»¶å›¾åƒå’Œå¤šæ¨¡æ€ç©ºé—´æ§åˆ¶ï¼ˆå¦‚æ–‡æœ¬æç¤ºã€å®ä¾‹å¸ƒå±€å’Œè¯­ä¹‰åœ°å›¾ï¼‰ï¼Œå¯åˆæˆåäº‹ä»¶å›¾åƒã€‚å…¶æ ¸å¿ƒæ€æƒ³æ˜¯å°†å™ªå£°åˆ°å›¾åƒçš„æ‰©æ•£æ¨¡å‹ä½œä¸ºé¢„åˆ°åæ‰©æ•£æ¡¥æ¢ã€‚å®éªŒç»“æœè¯æ˜ï¼ŒChangeBridgeèƒ½å¤Ÿæ¨¡æ‹Ÿä¸ç»™å®šæ¡ä»¶é«˜åº¦ä¸€è‡´çš„é«˜ä¿çœŸæœªæ¥æƒ…æ™¯ï¼ŒåŒ…æ‹¬äº‹ä»¶å’Œäº‹ä»¶é©±åŠ¨çš„èƒŒæ™¯å˜åŒ–ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ‰©æ•£æ¨¡å‹åœ¨é¥æ„Ÿå›¾åƒåˆæˆä¸­å–å¾—é‡å¤§è¿›å±•ã€‚</li>
<li>ç°æœ‰æ–¹æ³•æœªå……åˆ†æ¢ç´¢åŸºäºç»™å®šåœºæ™¯å›¾åƒçš„æ¨¡æ‹Ÿæœªæ¥æƒ…æ™¯èƒ½åŠ›ã€‚</li>
<li>ChangeBridgeæ˜¯ä¸€ç§æ¡ä»¶æ—¶ç©ºæ‰©æ•£æ¨¡å‹ï¼Œèƒ½åˆæˆåäº‹ä»¶å›¾åƒã€‚</li>
<li>ChangeBridgeçš„æ ¸å¿ƒæ€æƒ³æ˜¯å°†å™ªå£°åˆ°å›¾åƒçš„æ‰©æ•£æ¨¡å‹ä½œä¸ºé¢„åˆ°åæ‰©æ•£æ¡¥æ¢ã€‚</li>
<li>ChangeBridgeæ”¯æŒå¤šæ¨¡æ€æ§åˆ¶ï¼ŒåŒ…æ‹¬æ–‡æœ¬æç¤ºã€å®ä¾‹å¸ƒå±€å’Œè¯­ä¹‰åœ°å›¾ã€‚</li>
<li>ChangeBridgeæ˜¯é¦–ä¸ªå…·æœ‰å¤šæ¨¡æ€æ§åˆ¶çš„æ—¶ç©ºé¥æ„Ÿç”Ÿæˆæ¨¡å‹ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.04678">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-a1726ee34737ab8f428b8f13d4d8cb3e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ecec96d6416166726c9ed8e38c00a483.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-94f81c7acc3132af205bb21640d2365d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-cfde473cbdf79c95e672e2ed3664764c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-1fccdfcada4a15a2579168a21e11e61e.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Information-Guided-Diffusion-Sampling-for-Dataset-Distillation"><a href="#Information-Guided-Diffusion-Sampling-for-Dataset-Distillation" class="headerlink" title="Information-Guided Diffusion Sampling for Dataset Distillation"></a>Information-Guided Diffusion Sampling for Dataset Distillation</h2><p><strong>Authors:Linfeng Ye, Shayan Mohajer Hamidi, Guang Li, Takahiro Ogawa, Miki Haseyama, Konstantinos N. Plataniotis</strong></p>
<p>Dataset distillation aims to create a compact dataset that retains essential information while maintaining model performance. Diffusion models (DMs) have shown promise for this task but struggle in low images-per-class (IPC) settings, where generated samples lack diversity. In this paper, we address this issue from an information-theoretic perspective by identifying two key types of information that a distilled dataset must preserve: ($i$) prototype information $\mathrm{I}(X;Y)$, which captures label-relevant features; and ($ii$) contextual information $\mathrm{H}(X | Y)$, which preserves intra-class variability. Here, $(X,Y)$ represents the pair of random variables corresponding to the input data and its ground truth label, respectively. Observing that the required contextual information scales with IPC, we propose maximizing $\mathrm{I}(X;Y) + \beta \mathrm{H}(X | Y)$ during the DM sampling process, where $\beta$ is IPC-dependent. Since directly computing $\mathrm{I}(X;Y)$ and $\mathrm{H}(X | Y)$ is intractable, we develop variational estimations to tightly lower-bound these quantities via a data-driven approach. Our approach, information-guided diffusion sampling (IGDS), seamlessly integrates with diffusion models and improves dataset distillation across all IPC settings. Experiments on Tiny ImageNet and ImageNet subsets show that IGDS significantly outperforms existing methods, particularly in low-IPC regimes. The code will be released upon acceptance. </p>
<blockquote>
<p>æ•°æ®é›†è’¸é¦æ—¨åœ¨åˆ›å»ºä¸€ä¸ªç´§å‡‘çš„æ•°æ®é›†ï¼Œè¯¥æ•°æ®é›†åœ¨ä¿æŒæ¨¡å‹æ€§èƒ½çš„åŒæ—¶ä¿ç•™å…³é”®ä¿¡æ¯ã€‚æ‰©æ•£æ¨¡å‹ï¼ˆDMsï¼‰åœ¨æ­¤ä»»åŠ¡ä¸­æ˜¾ç¤ºå‡ºæ½œåŠ›ï¼Œä½†åœ¨æ¯ç±»å›¾åƒï¼ˆIPCï¼‰è¾ƒå°‘çš„è®¾ç½®ä¸­é¢ä¸´æŒ‘æˆ˜ï¼Œç”Ÿæˆçš„æ ·æœ¬ç¼ºä¹å¤šæ ·æ€§ã€‚æœ¬æ–‡ä»ä¿¡æ¯è®ºçš„è§’åº¦è§£å†³äº†è¿™ä¸ªé—®é¢˜ï¼Œç¡®å®šäº†è’¸é¦æ•°æ®é›†å¿…é¡»ä¿ç•™çš„ä¸¤ç§å…³é”®ç±»å‹çš„ä¿¡æ¯ï¼šï¼ˆiï¼‰åŸå‹ä¿¡æ¯I(Xï¼›Y)ï¼Œè¿™æ•è·äº†ä¸æ ‡ç­¾ç›¸å…³çš„ç‰¹å¾ï¼›ï¼ˆiiï¼‰ä¸Šä¸‹æ–‡ä¿¡æ¯H(X|Y)ï¼Œè¿™ä¿ç•™äº†ç±»å†…å˜å¼‚æ€§ã€‚åœ¨è¿™é‡Œï¼Œ(Xï¼ŒY)ä»£è¡¨ä¸è¾“å…¥æ•°æ®åŠå…¶çœŸå®æ ‡ç­¾å¯¹åº”çš„éšæœºå˜é‡å¯¹ã€‚è§‚å¯Ÿåˆ°æ‰€éœ€çš„ä¸Šä¸‹æ–‡ä¿¡æ¯éšIPCè€Œå˜åŒ–ï¼Œæˆ‘ä»¬å»ºè®®åœ¨DMé‡‡æ ·è¿‡ç¨‹ä¸­æœ€å¤§åŒ–I(Xï¼›Y)+Î²H(X|Y)ï¼Œå…¶ä¸­Î²å–å†³äºIPCã€‚ç”±äºç›´æ¥è®¡ç®—I(Xï¼›Y)å’ŒH(X|Y)æ˜¯ä¸å¯è¡Œçš„ï¼Œæˆ‘ä»¬é€šè¿‡æ•°æ®é©±åŠ¨çš„æ–¹æ³•ç´§å¯†åœ°ä¼°ç®—è¿™äº›é‡çš„ä¸‹é™ã€‚æˆ‘ä»¬çš„æ–¹æ³•ï¼Œä¿¡æ¯å¼•å¯¼æ‰©æ•£é‡‡æ ·ï¼ˆIGDSï¼‰ï¼Œå¯ä»¥æ— ç¼åœ°ä¸æ‰©æ•£æ¨¡å‹é›†æˆï¼Œå¹¶åœ¨æ‰€æœ‰IPCè®¾ç½®ä¸­éƒ½æ”¹å–„äº†æ•°æ®é›†è’¸é¦çš„æ•ˆæœã€‚åœ¨Tiny ImageNetå’ŒImageNetå­é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒIGDSæ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œç‰¹åˆ«æ˜¯åœ¨ä½IPCç¯å¢ƒä¸­ã€‚ä»£ç å°†åœ¨æ¥å—åå‘å¸ƒã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.04619v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†åŸºäºä¿¡æ¯ç†è®ºçš„æ‰©æ•£æ¨¡å‹æ•°æ®é›†è’¸é¦æ–¹æ³•ï¼Œæ—¨åœ¨è§£å†³åœ¨ä½å›¾åƒæ¯ç±»åˆ«ï¼ˆIPCï¼‰è®¾ç½®ä¸‹ç”Ÿæˆæ ·æœ¬ç¼ºä¹å¤šæ ·æ€§çš„æŒ‘æˆ˜ã€‚æ–‡ç« æŒ‡å‡ºï¼Œè’¸é¦æ•°æ®é›†å¿…é¡»ä¿ç•™ä¸¤ç§å…³é”®ä¿¡æ¯ï¼šåŸå‹ä¿¡æ¯å’Œä¸Šä¸‹æ–‡ä¿¡æ¯ã€‚æ–‡ç« è¿˜æå‡ºäº†ä¸€ç§æ–°çš„æ–¹æ³•â€”â€”ä¿¡æ¯å¼•å¯¼æ‰©æ•£é‡‡æ ·ï¼ˆIGDSï¼‰ï¼Œè¯¥æ–¹æ³•é€šè¿‡æœ€å¤§åŒ–è¿™ä¸¤ç§ä¿¡æ¯çš„ç»„åˆæ¥æ”¹è¿›æ‰©æ•£æ¨¡å‹çš„æ•°æ®é›†è’¸é¦æ•ˆæœï¼Œå¹¶åœ¨Tiny ImageNetå’ŒImageNetå­é›†ä¸Šçš„å®éªŒè¯æ˜äº†å…¶æ˜¾è‘—ä¼˜åŠ¿ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ‰©æ•£æ¨¡å‹ç”¨äºæ•°æ®é›†è’¸é¦ï¼Œæ—¨åœ¨åˆ›å»ºä¿ç•™å…³é”®ä¿¡æ¯çš„ç´§å‡‘æ•°æ®é›†ã€‚</li>
<li>åœ¨ä½å›¾åƒæ¯ç±»åˆ«ï¼ˆIPCï¼‰è®¾ç½®ä¸‹ï¼Œæ‰©æ•£æ¨¡å‹ç”Ÿæˆçš„æ ·æœ¬ç¼ºä¹å¤šæ ·æ€§ã€‚</li>
<li>è’¸é¦æ•°æ®é›†å¿…é¡»ä¿ç•™ä¸¤ç§å…³é”®ä¿¡æ¯ï¼šåŸå‹ä¿¡æ¯å’Œä¸Šä¸‹æ–‡ä¿¡æ¯ã€‚</li>
<li>åŸå‹ä¿¡æ¯æ•è·ä¸æ ‡ç­¾ç›¸å…³çš„ç‰¹å¾ï¼Œè€Œä¸Šä¸‹æ–‡ä¿¡æ¯ä¿ç•™ç±»å†…å˜å¼‚æ€§ã€‚</li>
<li>æå‡ºäº†ä¿¡æ¯å¼•å¯¼æ‰©æ•£é‡‡æ ·ï¼ˆIGDSï¼‰æ–¹æ³•ï¼Œé€šè¿‡æœ€å¤§åŒ–åŸå‹ä¿¡æ¯å’Œä¸Šä¸‹æ–‡ä¿¡æ¯çš„ç»„åˆæ¥æ”¹å–„æ‰©æ•£æ¨¡å‹çš„æ•°æ®é›†è’¸é¦æ•ˆæœã€‚</li>
<li>IGDSæ–¹æ³•é€šè¿‡æ•°æ®é©±åŠ¨çš„æ–¹å¼ç´§å¯†åœ°ä¼°ç®—å¹¶æœ€å¤§åŒ–ä¿¡æ¯ç»„åˆã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.04619">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-b0cd41ec451331f648970697f8d01094.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-895030dac0b22c445e0da232170f79bc.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="S-2-Edit-Text-Guided-Image-Editing-with-Precise-Semantic-and-Spatial-Control"><a href="#S-2-Edit-Text-Guided-Image-Editing-with-Precise-Semantic-and-Spatial-Control" class="headerlink" title="S$^2$Edit: Text-Guided Image Editing with Precise Semantic and Spatial   Control"></a>S$^2$Edit: Text-Guided Image Editing with Precise Semantic and Spatial   Control</h2><p><strong>Authors:Xudong Liu, Zikun Chen, Ruowei Jiang, Ziyi Wu, Kejia Yin, Han Zhao, Parham Aarabi, Igor Gilitschenski</strong></p>
<p>Recent advances in diffusion models have enabled high-quality generation and manipulation of images guided by texts, as well as concept learning from images. However, naive applications of existing methods to editing tasks that require fine-grained control, e.g., face editing, often lead to suboptimal solutions with identity information and high-frequency details lost during the editing process, or irrelevant image regions altered due to entangled concepts. In this work, we propose S$^2$Edit, a novel method based on a pre-trained text-to-image diffusion model that enables personalized editing with precise semantic and spatial control. We first fine-tune our model to embed the identity information into a learnable text token. During fine-tuning, we disentangle the learned identity token from attributes to be edited by enforcing an orthogonality constraint in the textual feature space. To ensure that the identity token only affects regions of interest, we apply object masks to guide the cross-attention maps. At inference time, our method performs localized editing while faithfully preserving the original identity with semantically disentangled and spatially focused identity token learned. Extensive experiments demonstrate the superiority of S$^2$Edit over state-of-the-art methods both quantitatively and qualitatively. Additionally, we showcase several compositional image editing applications of S$^2$Edit such as makeup transfer. </p>
<blockquote>
<p>æœ€è¿‘æ‰©æ•£æ¨¡å‹çš„æŠ€æœ¯è¿›æ­¥å·²ç»èƒ½å¤Ÿå®ç°ç”±æ–‡æœ¬å¼•å¯¼çš„é«˜è´¨é‡å›¾åƒç”Ÿæˆå’Œæ“çºµï¼Œä»¥åŠä»å›¾åƒä¸­è¿›è¡Œæ¦‚å¿µå­¦ä¹ ã€‚ç„¶è€Œï¼Œå°†ç°æœ‰æ–¹æ³•ç›´æ¥åº”ç”¨äºéœ€è¦ç²¾ç»†æ§åˆ¶çš„ç¼–è¾‘ä»»åŠ¡ï¼ˆä¾‹å¦‚é¢éƒ¨ç¼–è¾‘ï¼‰ï¼Œé€šå¸¸ä¼šå¯¼è‡´æ¬¡ä¼˜è§£å†³æ–¹æ¡ˆï¼Œç¼–è¾‘è¿‡ç¨‹ä¸­ä¼šä¸¢å¤±èº«ä»½ä¿¡æ¯å’Œé«˜é¢‘ç»†èŠ‚ï¼Œæˆ–è€…ç”±äºæ¦‚å¿µçº ç¼ è€Œæ”¹å˜æ— å…³çš„å›¾åƒåŒºåŸŸã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†S^2Editï¼Œè¿™æ˜¯ä¸€ç§åŸºäºé¢„è®­ç»ƒçš„æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹çš„æ–°æ–¹æ³•ï¼Œèƒ½å¤Ÿå®ç°ä¸ªæ€§åŒ–çš„ç²¾ç¡®è¯­ä¹‰å’Œç©ºé—´æ§åˆ¶ç¼–è¾‘ã€‚æˆ‘ä»¬é¦–å…ˆå¯¹æˆ‘ä»¬çš„æ¨¡å‹è¿›è¡Œå¾®è°ƒï¼Œå°†èº«ä»½åµŒå…¥åˆ°ä¸€ä¸ªå¯å­¦ä¹ çš„æ–‡æœ¬ä»¤ç‰Œä¸­ã€‚åœ¨å¾®è°ƒè¿‡ç¨‹ä¸­ï¼Œæˆ‘ä»¬é€šè¿‡å¼ºåˆ¶æ–‡æœ¬ç‰¹å¾ç©ºé—´ä¸­çš„æ­£äº¤æ€§çº¦æŸï¼Œå°†å­¦ä¹ åˆ°çš„èº«ä»½ä»¤ç‰Œä¸è¦ç¼–è¾‘çš„å±æ€§åˆ†å¼€ã€‚ä¸ºäº†ç¡®ä¿èº«ä»½ä»¤ç‰Œåªå½±å“æ„Ÿå…´è¶£åŒºåŸŸï¼Œæˆ‘ä»¬åº”ç”¨å¯¹è±¡æ©ç æ¥å¼•å¯¼äº¤å‰æ³¨æ„åŠ›å›¾ã€‚åœ¨æ¨ç†é˜¶æ®µï¼Œæˆ‘ä»¬çš„æ–¹æ³•èƒ½å¤Ÿæ‰§è¡Œå±€éƒ¨ç¼–è¾‘ï¼ŒåŒæ—¶å¿ å®ä¿ç•™åŸå§‹èº«ä»½ï¼Œå¹¶å­¦ä¹ åˆ°è¯­ä¹‰ä¸Šè§£è€¦å’Œç©ºé—´ä¸Šé›†ä¸­çš„èº«ä»½ä»¤ç‰Œã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼ŒS^2Editåœ¨å®šé‡å’Œå®šæ€§æ–¹é¢å‡ä¼˜äºç°æœ‰å…ˆè¿›æŠ€æœ¯ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜å±•ç¤ºäº†S^2Editåœ¨åŒ–å¦†è½¬ç§»ç­‰ç»„åˆå›¾åƒç¼–è¾‘æ–¹é¢çš„å‡ ä¸ªåº”ç”¨ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.04584v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†åŸºäºé¢„è®­ç»ƒæ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹çš„S^2Editæ–¹æ³•ï¼Œå¯å®ç°ä¸ªæ€§åŒ–ç¼–è¾‘ï¼Œå…·æœ‰ç²¾ç¡®è¯­ä¹‰å’Œç©ºé—´æ§åˆ¶ã€‚é€šè¿‡å¾®è°ƒæ¨¡å‹å°†èº«ä»½ä¿¡æ¯å¤„ç†ä¸ºå¯å­¦ä¹ çš„æ–‡æœ¬ä»¤ç‰Œï¼Œå¹¶åœ¨æ–‡æœ¬ç‰¹å¾ç©ºé—´ä¸­é€šè¿‡æ­£äº¤çº¦æŸè¿›è¡Œè§£è€¦ï¼Œç¡®ä¿èº«ä»½ä»¤ç‰Œåªå½±å“æ„Ÿå…´è¶£åŒºåŸŸã€‚S^2Editèƒ½è¿›è¡Œå±€éƒ¨ç¼–è¾‘ï¼ŒåŒæ—¶å¿ å®ä¿ç•™åŸå§‹èº«ä»½ï¼Œå¹¶é€šè¿‡å¯¹è±¡è’™ç‰ˆå¼•å¯¼è·¨æ³¨æ„åŠ›å›¾å®ç°ç©ºé—´èšç„¦ã€‚å®éªŒè¯æ˜ï¼ŒS^2Editåœ¨å®šé‡å’Œå®šæ€§ä¸Šå‡ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œå¹¶å±•ç¤ºäº†åŒ–å¦†è½¬ç§»ç­‰å›¾åƒç¼–è¾‘åº”ç”¨ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>S^2Editåˆ©ç”¨æ‰©æ•£æ¨¡å‹å®ç°é«˜è´¨é‡å›¾åƒç”Ÿæˆå’Œæ–‡æœ¬å¼•å¯¼ç¼–è¾‘ã€‚</li>
<li>é€šè¿‡å¾®è°ƒæ¨¡å‹ï¼Œå°†èº«ä»½ä¿¡æ¯å¤„ç†ä¸ºå¯å­¦ä¹ çš„æ–‡æœ¬ä»¤ç‰Œã€‚</li>
<li>åœ¨æ–‡æœ¬ç‰¹å¾ç©ºé—´ä¸­é€šè¿‡æ­£äº¤çº¦æŸè§£è€¦èº«ä»½ä»¤ç‰Œå’Œè¦ç¼–è¾‘çš„å±æ€§ã€‚</li>
<li>åº”ç”¨å¯¹è±¡è’™ç‰ˆæ¥å¼•å¯¼è·¨æ³¨æ„åŠ›å›¾ï¼Œç¡®ä¿èº«ä»½ä»¤ç‰Œä»…å½±å“æ„Ÿå…´è¶£åŒºåŸŸã€‚</li>
<li>S^2Editèƒ½å®ç°å±€éƒ¨ç¼–è¾‘ï¼ŒåŒæ—¶ä¿ç•™åŸå§‹èº«ä»½ã€‚</li>
<li>å®éªŒç»“æœè¯æ˜S^2Editåœ¨æ€§èƒ½ä¸Šä¼˜äºç°æœ‰æ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.04584">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-ed0d1a6af9f03e30b7ee08168a892317.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-0af547ab0d9ec7b9c74f4d6e7f8c7aed.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-df9ffc380a311a1bad14b735b153d3e4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4339185225b9c4f23d6ecfc65b3fd839.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a158bf4ece392f8f490d980a1fe475b2.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="FB-Diff-Fourier-Basis-guided-Diffusion-for-Temporal-Interpolation-of-4D-Medical-Imaging"><a href="#FB-Diff-Fourier-Basis-guided-Diffusion-for-Temporal-Interpolation-of-4D-Medical-Imaging" class="headerlink" title="FB-Diff: Fourier Basis-guided Diffusion for Temporal Interpolation of 4D   Medical Imaging"></a>FB-Diff: Fourier Basis-guided Diffusion for Temporal Interpolation of 4D   Medical Imaging</h2><p><strong>Authors:Xin You, Runze Yang, Chuyan Zhang, Zhongliang Jiang, Jie Yang, Nassir Navab</strong></p>
<p>The temporal interpolation task for 4D medical imaging, plays a crucial role in clinical practice of respiratory motion modeling. Following the simplified linear-motion hypothesis, existing approaches adopt optical flow-based models to interpolate intermediate frames. However, realistic respiratory motions should be nonlinear and quasi-periodic with specific frequencies. Intuited by this property, we resolve the temporal interpolation task from the frequency perspective, and propose a Fourier basis-guided Diffusion model, termed FB-Diff. Specifically, due to the regular motion discipline of respiration, physiological motion priors are introduced to describe general characteristics of temporal data distributions. Then a Fourier motion operator is elaborately devised to extract Fourier bases by incorporating physiological motion priors and case-specific spectral information in the feature space of Variational Autoencoder. Well-learned Fourier bases can better simulate respiratory motions with motion patterns of specific frequencies. Conditioned on starting and ending frames, the diffusion model further leverages well-learned Fourier bases via the basis interaction operator, which promotes the temporal interpolation task in a generative manner. Extensive results demonstrate that FB-Diff achieves state-of-the-art (SOTA) perceptual performance with better temporal consistency while maintaining promising reconstruction metrics. Codes are available. </p>
<blockquote>
<p>åœ¨å››ç»´åŒ»å­¦æˆåƒçš„æ—¶é—´æ’å€¼ä»»åŠ¡ä¸­ï¼Œå¯¹äºå‘¼å¸è¿åŠ¨å»ºæ¨¡çš„ä¸´åºŠå®è·µèµ·ç€è‡³å…³é‡è¦çš„ä½œç”¨ã€‚éµå¾ªç®€åŒ–çš„çº¿æ€§è¿åŠ¨å‡è®¾ï¼Œç°æœ‰æ–¹æ³•é‡‡ç”¨åŸºäºå…‰å­¦æµçš„æ¨¡å‹è¿›è¡Œä¸­é—´å¸§æ’å€¼ã€‚ç„¶è€Œï¼Œå®é™…çš„å‘¼å¸è¿åŠ¨åº”è¯¥æ˜¯éçº¿æ€§ã€å‡†å‘¨æœŸæ€§çš„ï¼Œå¹¶å…·æœ‰ç‰¹å®šçš„é¢‘ç‡ã€‚åŸºäºè¿™ä¸€ç‰¹æ€§ï¼Œæˆ‘ä»¬ä»é¢‘ç‡è§’åº¦è§£å†³æ—¶é—´æ’å€¼é—®é¢˜ï¼Œå¹¶æå‡ºäº†ä¸€ç§å‚…é‡Œå¶åŸºç¡€å¼•å¯¼çš„æ‰©æ•£æ¨¡å‹ï¼Œç§°ä¸ºFB-Diffã€‚å…·ä½“æ¥è¯´ï¼Œç”±äºå‘¼å¸çš„å¸¸è§„è¿åŠ¨è§„å¾‹ï¼Œæˆ‘ä»¬å¼•å…¥ç”Ÿç†è¿åŠ¨å…ˆéªŒæ¥æè¿°æ—¶é—´æ•°æ®åˆ†å¸ƒçš„ä¸€èˆ¬ç‰¹å¾ã€‚ç„¶åç²¾å¿ƒè®¾è®¡äº†ä¸€ä¸ªå‚…é‡Œå¶è¿åŠ¨ç®—å­ï¼Œé€šè¿‡ç»“åˆç”Ÿç†è¿åŠ¨å…ˆéªŒå’Œæ¡ˆä¾‹ç‰¹å®šçš„å…‰è°±ä¿¡æ¯ï¼Œåœ¨å˜åˆ†è‡ªåŠ¨ç¼–ç å™¨çš„ç‰¹å¾ç©ºé—´ä¸­æå–å‚…é‡Œå¶åŸºç¡€ã€‚å­¦ä¹ è‰¯å¥½çš„å‚…é‡Œå¶åŸºç¡€èƒ½æ›´å¥½åœ°æ¨¡æ‹Ÿå…·æœ‰ç‰¹å®šé¢‘ç‡çš„è¿åŠ¨æ¨¡å¼ã€‚åŸºäºèµ·å§‹å¸§å’Œç»“æŸå¸§çš„æ¡ä»¶ï¼Œæ‰©æ•£æ¨¡å‹è¿›ä¸€æ­¥åˆ©ç”¨å­¦ä¹ è‰¯å¥½çš„å‚…é‡Œå¶åŸºç¡€ï¼Œé€šè¿‡åŸºç¡€äº¤äº’ç®—å­ï¼Œä»¥ç”Ÿæˆçš„æ–¹å¼ä¿ƒè¿›æ—¶é—´æ’å€¼ä»»åŠ¡ã€‚å¤§é‡ç»“æœè¡¨æ˜ï¼ŒFB-Diffè¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ„ŸçŸ¥æ€§èƒ½ï¼Œå…·æœ‰è¾ƒå¥½çš„æ—¶é—´ä¸€è‡´æ€§ï¼ŒåŒæ—¶ä¿æŒäº†æœ‰å¸Œæœ›çš„é‡å»ºæŒ‡æ ‡ã€‚ç›¸å…³ä»£ç å·²å…¬å¼€ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.04547v1">PDF</a> Accepted by ICCV 2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ä¸ªåŸºäºå‚…é‡Œå¶åŸºç¡€çš„æ‰©æ•£æ¨¡å‹ï¼ˆFB-Diffï¼‰ï¼Œç”¨äºè§£å†³å››ç»´åŒ»å­¦æˆåƒä¸­çš„æ—¶é—´æ’å€¼é—®é¢˜ã€‚è¯¥æ¨¡å‹ç»“åˆç”Ÿç†è¿åŠ¨å…ˆéªŒå’Œå˜åˆ†è‡ªç¼–ç å™¨çš„ç‰¹å¾ç©ºé—´ï¼Œä»é¢‘ç‡è§’åº¦è§£å†³æ—¶é—´æ’å€¼é—®é¢˜ã€‚é€šè¿‡å¼•å…¥å‚…é‡Œå¶è¿åŠ¨ç®—å­ï¼Œæå–å‚…é‡Œå¶åŸºç¡€ï¼Œæ¨¡æ‹Ÿå…·æœ‰ç‰¹å®šé¢‘ç‡çš„è¿åŠ¨æ¨¡å¼ï¼Œå®ç°ç”Ÿæˆå¼çš„æ—¶é—´æ’å€¼ä»»åŠ¡ï¼Œå–å¾—äº†æœ€ä½³æ„ŸçŸ¥æ€§èƒ½å’Œè‰¯å¥½çš„æ—¶é—´ä¸€è‡´æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>4DåŒ»å­¦æˆåƒä¸­çš„æ—¶é—´æ’å€¼ä»»åŠ¡å¯¹å‘¼å¸è¿åŠ¨å»ºæ¨¡çš„ä¸´åºŠå®è·µè‡³å…³é‡è¦ã€‚</li>
<li>ç°æœ‰æ–¹æ³•é‡‡ç”¨åŸºäºå…‰å­¦æµåŠ¨çš„æ¨¡å‹è¿›è¡Œæ’å€¼ï¼Œä½†å¿½ç•¥äº†å‘¼å¸è¿åŠ¨çš„éçº¿æ€§ã€å‡†å‘¨æœŸæ€§ç‰¹ç‚¹ã€‚</li>
<li>æœ¬æ–‡ä»é¢‘ç‡è§’åº¦è§£å†³æ—¶é—´æ’å€¼é—®é¢˜ï¼Œæå‡ºFB-Diffæ¨¡å‹ã€‚</li>
<li>è¯¥æ¨¡å‹å¼•å…¥ç”Ÿç†è¿åŠ¨å…ˆéªŒæ¥æè¿°æ—¶é—´æ•°æ®åˆ†å¸ƒçš„ä¸€èˆ¬ç‰¹å¾ã€‚</li>
<li>é€šè¿‡å‚…é‡Œå¶è¿åŠ¨ç®—å­æå–å‚…é‡Œå¶åŸºç¡€ï¼Œç»“åˆç”Ÿç†è¿åŠ¨å…ˆéªŒå’Œç‰¹å®šå…‰è°±ä¿¡æ¯ã€‚</li>
<li>FB-Diffæ¨¡å‹èƒ½å¤Ÿå®ç°ç”Ÿæˆå¼çš„æ—¶é—´æ’å€¼ä»»åŠ¡ï¼Œè¾¾åˆ°æœ€ä½³æ„ŸçŸ¥æ€§èƒ½ï¼Œä¿æŒè‰¯å¥½çš„æ—¶é—´ä¸€è‡´æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.04547">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-4de01ca49a0b3a5944fe1a316fd66b55.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-395fa22256dc03e84718653f6b5ac703.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-2d8cad7080218a5ef2bf7dca51c1bb5e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b28033422ea31876d122ddd210efc3ee.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e12517680e6b37b3e92e3d03fe89d4de.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-816a0f0240c97cd29c0c0989a037f20e.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="CoT-lized-Diffusion-Letâ€™s-Reinforce-T2I-Generation-Step-by-step"><a href="#CoT-lized-Diffusion-Letâ€™s-Reinforce-T2I-Generation-Step-by-step" class="headerlink" title="CoT-lized Diffusion: Letâ€™s Reinforce T2I Generation Step-by-step"></a>CoT-lized Diffusion: Letâ€™s Reinforce T2I Generation Step-by-step</h2><p><strong>Authors:Zheyuan Liu, Munan Ning, Qihui Zhang, Shuo Yang, Zhongrui Wang, Yiwei Yang, Xianzhe Xu, Yibing Song, Weihua Chen, Fan Wang, Li Yuan</strong></p>
<p>Current text-to-image (T2I) generation models struggle to align spatial composition with the input text, especially in complex scenes. Even layout-based approaches yield suboptimal spatial control, as their generation process is decoupled from layout planning, making it difficult to refine the layout during synthesis. We present CoT-Diff, a framework that brings step-by-step CoT-style reasoning into T2I generation by tightly integrating Multimodal Large Language Model (MLLM)-driven 3D layout planning with the diffusion process. CoT-Diff enables layout-aware reasoning inline within a single diffusion round: at each denoising step, the MLLM evaluates intermediate predictions, dynamically updates the 3D scene layout, and continuously guides the generation process. The updated layout is converted into semantic conditions and depth maps, which are fused into the diffusion model via a condition-aware attention mechanism, enabling precise spatial control and semantic injection. Experiments on 3D Scene benchmarks show that CoT-Diff significantly improves spatial alignment and compositional fidelity, and outperforms the state-of-the-art method by 34.7% in complex scene spatial accuracy, thereby validating the effectiveness of this entangled generation paradigm. </p>
<blockquote>
<p>å½“å‰æ–‡æœ¬åˆ°å›¾åƒï¼ˆT2Iï¼‰ç”Ÿæˆæ¨¡å‹åœ¨å¤æ‚çš„åœºæ™¯ä¸­å¾ˆéš¾å°†ç©ºé—´æ„å›¾ä¸è¾“å…¥æ–‡æœ¬å¯¹é½ã€‚å³ä½¿æ˜¯åŸºäºå¸ƒå±€çš„æ–¹æ³•ä¹Ÿä¼šäº§ç”Ÿæ¬¡ä¼˜çš„ç©ºé—´æ§åˆ¶ï¼Œå› ä¸ºå®ƒä»¬çš„ç”Ÿæˆè¿‡ç¨‹ä¸å¸ƒå±€è§„åˆ’æ˜¯åˆ†ç¦»çš„ï¼Œä½¿å¾—åœ¨åˆæˆè¿‡ç¨‹ä¸­éš¾ä»¥è°ƒæ•´å¸ƒå±€ã€‚æˆ‘ä»¬æå‡ºäº†CoT-Diffæ¡†æ¶ï¼Œå®ƒé€šè¿‡ç´§å¯†ç»“åˆå¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰é©±åŠ¨çš„3Då¸ƒå±€è§„åˆ’ä¸æ‰©æ•£è¿‡ç¨‹ï¼Œå°†åˆ†æ­¥çš„CoTé£æ ¼æ¨ç†å¸¦å…¥T2Iç”Ÿæˆã€‚CoT-Diffä½¿å¸ƒå±€æ„ŸçŸ¥æ¨ç†èƒ½å¤Ÿåœ¨å•ä¸ªæ‰©æ•£å›åˆå†…å®Œæˆï¼šåœ¨æ¯ä¸ªå»å™ªæ­¥éª¤ä¸­ï¼ŒMLLMè¯„ä¼°ä¸­é—´é¢„æµ‹ï¼ŒåŠ¨æ€æ›´æ–°3Dåœºæ™¯å¸ƒå±€ï¼Œå¹¶æŒç»­æŒ‡å¯¼ç”Ÿæˆè¿‡ç¨‹ã€‚æ›´æ–°çš„å¸ƒå±€è¢«è½¬æ¢ä¸ºè¯­ä¹‰æ¡ä»¶å’Œæ·±åº¦å›¾ï¼Œé€šè¿‡æ¡ä»¶æ„ŸçŸ¥æ³¨æ„åŠ›æœºåˆ¶èåˆåˆ°æ‰©æ•£æ¨¡å‹ä¸­ï¼Œå®ç°äº†ç²¾ç¡®çš„ç©ºé—´æ§åˆ¶å’Œè¯­ä¹‰æ³¨å…¥ã€‚åœ¨3Dåœºæ™¯åŸºå‡†æµ‹è¯•ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒCoT-Diffåœ¨ç©ºé—´å’Œç»„æˆä¿çœŸåº¦æ–¹é¢æ˜¾è‘—æé«˜ï¼Œå¹¶ä¸”åœ¨å¤æ‚åœºæ™¯çš„ç©ºé—´å‡†ç¡®æ€§æ–¹é¢æ¯”ç°æœ‰æŠ€æœ¯é¢†å…ˆäº†34.7%ï¼Œä»è€ŒéªŒè¯äº†è¿™ç§çº ç¼ ç”Ÿæˆæ¨¡å¼çš„æœ‰æ•ˆæ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.04451v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†å½“å‰æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆæ¨¡å‹åœ¨å¤æ‚åœºæ™¯ä¸­çš„ç©ºé—´å¸ƒå±€å¯¹é½é—®é¢˜ã€‚ä¸ºæ­¤ï¼Œæå‡ºäº†ä¸€ç§åä¸ºCoT-Diffçš„æ–°æ¡†æ¶ï¼Œè¯¥æ¡†æ¶é€šè¿‡ç´§å¯†ç»“åˆå¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹é©±åŠ¨çš„3Då¸ƒå±€è§„åˆ’ä¸æ‰©æ•£è¿‡ç¨‹ï¼Œå°†åˆ†æ­¥çš„CoTé£æ ¼æ¨ç†å¼•å…¥T2Iç”Ÿæˆã€‚CoT-Diffä½¿å¸ƒå±€æ„ŸçŸ¥æ¨ç†èƒ½å¤Ÿåœ¨å•ä¸ªæ‰©æ•£å›åˆå†…å®Œæˆï¼šåœ¨æ¯ä¸ªå»å™ªæ­¥éª¤ä¸­ï¼ŒMLLMè¯„ä¼°ä¸­é—´é¢„æµ‹ï¼ŒåŠ¨æ€æ›´æ–°3Dåœºæ™¯å¸ƒå±€ï¼Œå¹¶è¿ç»­æŒ‡å¯¼ç”Ÿæˆè¿‡ç¨‹ã€‚å®éªŒè¡¨æ˜ï¼ŒCoT-Diffåœ¨å¤æ‚åœºæ™¯ä¸­æ˜¾è‘—æé«˜äº†ç©ºé—´å¯¹é½å’Œç»„åˆä¿çœŸåº¦ï¼Œå¹¶ä¼˜äºå½“å‰æœ€å…ˆè¿›çš„æ–¹æ³•ï¼Œæé«˜äº†34.7%çš„ç©ºé—´ç²¾åº¦ã€‚éªŒè¯äº†è¿™ç§çº ç¼ ç”Ÿæˆæ¨¡å¼çš„æœ‰æ•ˆæ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å½“å‰æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆæ¨¡å‹åœ¨å¤æ‚åœºæ™¯ä¸­çš„ç©ºé—´å¸ƒå±€å¯¹é½å­˜åœ¨æŒ‘æˆ˜ã€‚</li>
<li>CoT-Diffæ¡†æ¶é€šè¿‡ç»“åˆå¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹å’Œæ‰©æ•£è¿‡ç¨‹æ¥è§£å†³è¿™ä¸ªé—®é¢˜ã€‚</li>
<li>CoT-Diffå®ç°äº†å¸ƒå±€æ„ŸçŸ¥æ¨ç†ï¼Œåœ¨æ¯ä¸ªæ‰©æ•£å›åˆä¸­åŠ¨æ€æ›´æ–°3Dåœºæ™¯å¸ƒå±€ã€‚</li>
<li>MLLMåœ¨ä¸­é—´é¢„æµ‹é˜¶æ®µè¯„ä¼°å¹¶æŒ‡å¯¼ç”Ÿæˆè¿‡ç¨‹ã€‚</li>
<li>CoT-Diffé€šè¿‡æ¡ä»¶æ„ŸçŸ¥æ³¨æ„åŠ›æœºåˆ¶å°†æ›´æ–°çš„å¸ƒå±€èåˆåˆ°æ‰©æ•£æ¨¡å‹ä¸­ã€‚</li>
<li>å®éªŒè¡¨æ˜ï¼ŒCoT-Diffåœ¨å¤æ‚åœºæ™¯ä¸­æ˜¾è‘—æé«˜äº†ç©ºé—´å¯¹é½å’Œç»„åˆä¿çœŸåº¦ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.04451">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-87460ac0ca7148d3f884049e6b5e4782.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6ef185e8d8d6e0d9601eba85b715101a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-3ae4f783a5c50d3a40bd32ccfeee61a2.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b6d981d95996e2776cb0f1ebbb515eb2.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="Sat2City-3D-City-Generation-from-A-Single-Satellite-Image-with-Cascaded-Latent-Diffusion"><a href="#Sat2City-3D-City-Generation-from-A-Single-Satellite-Image-with-Cascaded-Latent-Diffusion" class="headerlink" title="Sat2City: 3D City Generation from A Single Satellite Image with Cascaded   Latent Diffusion"></a>Sat2City: 3D City Generation from A Single Satellite Image with Cascaded   Latent Diffusion</h2><p><strong>Authors:Tongyan Hua, Lutao Jiang, Ying-Cong Chen, Wufan Zhao</strong></p>
<p>Recent advancements in generative models have enabled 3D urban scene generation from satellite imagery, unlocking promising applications in gaming, digital twins, and beyond. However, most existing methods rely heavily on neural rendering techniques, which hinder their ability to produce detailed 3D structures on a broader scale, largely due to the inherent structural ambiguity derived from relatively limited 2D observations. To address this challenge, we propose Sat2City, a novel framework that synergizes the representational capacity of sparse voxel grids with latent diffusion models, tailored specifically for our novel 3D city dataset. Our approach is enabled by three key components: (1) A cascaded latent diffusion framework that progressively recovers 3D city structures from satellite imagery, (2) a Re-Hash operation at its Variational Autoencoder (VAE) bottleneck to compute multi-scale feature grids for stable appearance optimization and (3) an inverse sampling strategy enabling implicit supervision for smooth appearance transitioning.To overcome the challenge of collecting real-world city-scale 3D models with high-quality geometry and appearance, we introduce a dataset of synthesized large-scale 3D cities paired with satellite-view height maps. Validated on this dataset, our framework generates detailed 3D structures from a single satellite image, achieving superior fidelity compared to existing city generation models. </p>
<blockquote>
<p>æœ€è¿‘ç”Ÿæˆæ¨¡å‹çš„è¿›å±•ä½¿å¾—ä»å«æ˜Ÿå›¾åƒç”Ÿæˆä¸‰ç»´åŸå¸‚åœºæ™¯æˆä¸ºå¯èƒ½ï¼Œè¿™åœ¨æ¸¸æˆã€æ•°å­—å­ªç”Ÿç­‰é¢†åŸŸå…·æœ‰å¹¿é˜”çš„åº”ç”¨å‰æ™¯ã€‚ç„¶è€Œï¼Œå¤§å¤šæ•°ç°æœ‰æ–¹æ³•ä¸¥é‡ä¾èµ–äºç¥ç»æ¸²æŸ“æŠ€æœ¯ï¼Œè¿™é™åˆ¶äº†å®ƒä»¬åœ¨æ›´å¤§è§„æ¨¡ä¸Šç”Ÿæˆè¯¦ç»†ä¸‰ç»´ç»“æ„çš„èƒ½åŠ›ï¼Œè¿™ä¸»è¦æ˜¯ç”±äºç›¸å¯¹æœ‰é™çš„äºŒç»´è§‚æµ‹æ‰€å›ºæœ‰çš„ç»“æ„æ¨¡ç³Šæ€§ã€‚ä¸ºäº†è§£å†³è¿™ä¸€æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†Sat2Cityæ¡†æ¶ï¼Œè¯¥æ¡†æ¶ç»“åˆäº†ç¨€ç–ä½“ç´ ç½‘æ ¼çš„è¡¨ç¤ºèƒ½åŠ›ä¸æ½œåœ¨æ‰©æ•£æ¨¡å‹ï¼Œä¸“é—¨ç”¨äºæˆ‘ä»¬æ–°çš„ä¸‰ç»´åŸå¸‚æ•°æ®é›†ã€‚æˆ‘ä»¬çš„æ–¹æ³•ç”±ä¸‰ä¸ªå…³é”®ç»„ä»¶æä¾›æ”¯æŒï¼šï¼ˆ1ï¼‰çº§è”æ½œåœ¨æ‰©æ•£æ¡†æ¶ï¼Œè¯¥æ¡†æ¶ä»å«æ˜Ÿå›¾åƒé€æ­¥æ¢å¤ä¸‰ç»´åŸå¸‚ç»“æ„ï¼›ï¼ˆ2ï¼‰åœ¨å…¶å˜åˆ†è‡ªåŠ¨ç¼–ç å™¨ï¼ˆVAEï¼‰ç“¶é¢ˆå¤„è¿›è¡ŒRe-Hashæ“ä½œï¼Œä»¥è®¡ç®—ç”¨äºç¨³å®šå¤–è§‚ä¼˜åŒ–çš„å¤šå°ºåº¦ç‰¹å¾ç½‘æ ¼ï¼›ï¼ˆ3ï¼‰åå‘é‡‡æ ·ç­–ç•¥ï¼Œå®ç°å¯¹å¹³æ»‘å¤–è§‚è¿‡æ¸¡çš„éšå¼ç›‘ç£ã€‚ä¸ºäº†å…‹æœæ”¶é›†å…·æœ‰é«˜è´¨é‡å‡ ä½•å’Œå¤–è§‚çš„ç°å®ä¸–ç•ŒåŸå¸‚è§„æ¨¡ä¸‰ç»´æ¨¡å‹çš„æŒ‘æˆ˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ªåˆæˆçš„å¤§è§„æ¨¡ä¸‰ç»´åŸå¸‚æ•°æ®é›†ï¼Œè¯¥æ•°æ®é›†é…æœ‰å«æ˜Ÿè§†å›¾çš„é«˜åº¦å›¾ã€‚åœ¨æˆ‘ä»¬çš„æ•°æ®é›†ä¸Šè¿›è¡ŒéªŒè¯ï¼Œæˆ‘ä»¬çš„æ¡†æ¶èƒ½å¤Ÿä»å•å¼ å«æ˜Ÿå›¾åƒç”Ÿæˆè¯¦ç»†çš„ä¸‰ç»´ç»“æ„ï¼Œä¸ç°æœ‰çš„åŸå¸‚ç”Ÿæˆæ¨¡å‹ç›¸æ¯”ï¼Œå…·æœ‰æ›´é«˜çš„ä¿çœŸåº¦ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.04403v1">PDF</a> ICCV 2025</p>
<p><strong>Summary</strong></p>
<p>è¿‘æœŸç”Ÿæˆæ¨¡å‹çš„æŠ€æœ¯è¿›æ­¥èƒ½å¤Ÿå®ç°ä»å«æ˜Ÿå›¾åƒç”Ÿæˆä¸‰ç»´åŸå¸‚åœºæ™¯ï¼Œä¸ºæ¸¸æˆã€æ•°å­—å­ªç”Ÿç­‰é¢†åŸŸå¸¦æ¥åº”ç”¨å‰æ™¯ã€‚ä½†ç°æœ‰æ–¹æ³•å¤šä¾èµ–ç¥ç»æ¸²æŸ“æŠ€æœ¯ï¼Œéš¾ä»¥åœ¨æ›´å¤§è§„æ¨¡ä¸Šç”Ÿæˆè¯¦ç»†çš„ä¸‰ç»´ç»“æ„ã€‚ä¸ºæ­¤ï¼Œæœ¬æ–‡æå‡ºSat2Cityæ¡†æ¶ï¼Œç»“åˆç¨€ç–ä½“ç´ ç½‘æ ¼çš„è¡¨ç¤ºèƒ½åŠ›ä¸æ½œåœ¨æ‰©æ•£æ¨¡å‹ï¼Œé€šè¿‡ä¸‰ä¸ªå…³é”®ç»„ä»¶è§£å†³æŒ‘æˆ˜ã€‚è¯¥æ¡†æ¶åœ¨æ–°å‹ä¸‰ç»´åŸå¸‚æ•°æ®é›†ä¸Šè¡¨ç°ä¼˜è¶Šï¼Œèƒ½ä»å•å¼ å«æ˜Ÿå›¾åƒç”Ÿæˆé«˜ç²¾åº¦ä¸‰ç»´ç»“æ„ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç”Ÿæˆæ¨¡å‹æœ€æ–°è¿›å±•å®ç°äº†ä»å«æ˜Ÿå›¾åƒç”Ÿæˆä¸‰ç»´åŸå¸‚åœºæ™¯ï¼Œåº”ç”¨äºæ¸¸æˆã€æ•°å­—å­ªç”Ÿç­‰é¢†åŸŸã€‚</li>
<li>ç°æœ‰æ–¹æ³•ä¸»è¦ä¾èµ–ç¥ç»æ¸²æŸ“æŠ€æœ¯ï¼Œéš¾ä»¥åœ¨æ›´å¤§è§„æ¨¡ä¸Šç”Ÿæˆè¯¦ç»†çš„ä¸‰ç»´ç»“æ„ã€‚</li>
<li>Sat2Cityæ¡†æ¶ç»“åˆç¨€ç–ä½“ç´ ç½‘æ ¼ä¸æ½œåœ¨æ‰©æ•£æ¨¡å‹ï¼Œè§£å†³ç°æœ‰æŒ‘æˆ˜ã€‚</li>
<li>æ¡†æ¶åŒ…å«ä¸‰ä¸ªå…³é”®ç»„ä»¶ï¼šçº§è”æ½œåœ¨æ‰©æ•£æ¡†æ¶ã€é‡å“ˆå¸Œæ“ä½œå’Œé€†å‘é‡‡æ ·ç­–ç•¥ã€‚</li>
<li>æ–°å‹ä¸‰ç»´åŸå¸‚æ•°æ®é›†é…å¤‡å«æ˜Ÿè§†å›¾é«˜åº¦å›¾ã€‚</li>
<li>è¯¥æ¡†æ¶åœ¨æ–°å‹æ•°æ®é›†ä¸Šè¡¨ç°ä¼˜è¶Šï¼Œå®ç°å•å¼ å«æ˜Ÿå›¾åƒç”Ÿæˆé«˜ç²¾åº¦ä¸‰ç»´ç»“æ„ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.04403">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-ab7361431c63a2da32a3fe2e7a41e656.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-5e38c00895d5ea82d9a0974719b6afa9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-803a55bea5a6b2b5b88dc9739cf8cd91.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3d0f2fae1bdbbdc2bc5f28486458a37d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-33e989311026c9396095beec71c544db.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-deee8fd18dcd378d8bd3b55d690f5aa9.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="AvatarMakeup-Realistic-Makeup-Transfer-for-3D-Animatable-Head-Avatars"><a href="#AvatarMakeup-Realistic-Makeup-Transfer-for-3D-Animatable-Head-Avatars" class="headerlink" title="AvatarMakeup: Realistic Makeup Transfer for 3D Animatable Head Avatars"></a>AvatarMakeup: Realistic Makeup Transfer for 3D Animatable Head Avatars</h2><p><strong>Authors:Yiming Zhong, Xiaolin Zhang, Ligang Liu, Yao Zhao, Yunchao Wei</strong></p>
<p>Similar to facial beautification in real life, 3D virtual avatars require personalized customization to enhance their visual appeal, yet this area remains insufficiently explored. Although current 3D Gaussian editing methods can be adapted for facial makeup purposes, these methods fail to meet the fundamental requirements for achieving realistic makeup effects: 1) ensuring a consistent appearance during drivable expressions, 2) preserving the identity throughout the makeup process, and 3) enabling precise control over fine details. To address these, we propose a specialized 3D makeup method named AvatarMakeup, leveraging a pretrained diffusion model to transfer makeup patterns from a single reference photo of any individual. We adopt a coarse-to-fine idea to first maintain the consistent appearance and identity, and then to refine the details. In particular, the diffusion model is employed to generate makeup images as supervision. Due to the uncertainties in diffusion process, the generated images are inconsistent across different viewpoints and expressions. Therefore, we propose a Coherent Duplication method to coarsely apply makeup to the target while ensuring consistency across dynamic and multiview effects. Coherent Duplication optimizes a global UV map by recoding the averaged facial attributes among the generated makeup images. By querying the global UV map, it easily synthesizes coherent makeup guidance from arbitrary views and expressions to optimize the target avatar. Given the coarse makeup avatar, we further enhance the makeup by incorporating a Refinement Module into the diffusion model to achieve high makeup quality. Experiments demonstrate that AvatarMakeup achieves state-of-the-art makeup transfer quality and consistency throughout animation. </p>
<blockquote>
<p>ä¸ç°å®ç”Ÿæ´»ä¸­çš„é¢éƒ¨ç¾å®¹ç±»ä¼¼ï¼Œ3Dè™šæ‹ŸåŒ–èº«éœ€è¦ä¸ªæ€§åŒ–å®šåˆ¶ä»¥å¢å¼ºå…¶è§†è§‰å¸å¼•åŠ›ï¼Œä½†è¿™ä¸ªé¢†åŸŸä»ç„¶æ²¡æœ‰å¾—åˆ°è¶³å¤Ÿçš„æ¢ç´¢ã€‚è™½ç„¶å½“å‰çš„3Dé«˜æ–¯ç¼–è¾‘æ–¹æ³•å¯ä»¥é€‚åº”é¢éƒ¨åŒ–å¦†çš„ç›®çš„ï¼Œä½†è¿™äº›æ–¹æ³•æœªèƒ½æ»¡è¶³å®ç°çœŸå®åŒ–å¦†æ•ˆæœçš„åŸºæœ¬è¦æ±‚ï¼š1ï¼‰åœ¨å¯é©±åŠ¨çš„è¡¨æƒ…ä¸­ä¿æŒå¤–è§‚çš„ä¸€è‡´æ€§ï¼Œ2ï¼‰åœ¨åŒ–å¦†è¿‡ç¨‹ä¸­ä¿æŒèº«ä»½è¯†åˆ«ï¼Œä»¥åŠ3ï¼‰å¯¹ç»†èŠ‚è¿›è¡Œç²¾ç¡®æ§åˆ¶ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ä¸“é—¨çš„3DåŒ–å¦†æ–¹æ³•ï¼Œåä¸ºAvatarMakeupï¼Œå®ƒåˆ©ç”¨é¢„è®­ç»ƒçš„æ‰©æ•£æ¨¡å‹ä»ä»»ä½•ä¸ªäººçš„å•å¼ å‚è€ƒç…§ç‰‡ä¸­è½¬ç§»åŒ–å¦†æ¨¡å¼ã€‚æˆ‘ä»¬é‡‡ç”¨ç”±ç²—åˆ°ç»†çš„ç†å¿µï¼Œé¦–å…ˆä¿æŒå¤–è§‚å’Œèº«ä»½çš„çš„ä¸€è‡´æ€§ï¼Œç„¶åç»†åŒ–ç»†èŠ‚ã€‚ç‰¹åˆ«æ˜¯ï¼Œæ‰©æ•£æ¨¡å‹è¢«ç”¨äºç”ŸæˆåŒ–å¦†å›¾åƒä½œä¸ºç›‘ç£ã€‚ç”±äºæ‰©æ•£è¿‡ç¨‹ä¸­çš„ä¸ç¡®å®šæ€§ï¼Œç”Ÿæˆçš„å›¾åƒåœ¨ä¸åŒçš„è§†è§’å’Œè¡¨æƒ…ä¸‹å­˜åœ¨ä¸ä¸€è‡´æ€§ã€‚å› æ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§è¿è´¯å¤åˆ¶æ–¹æ³•ï¼Œå¯¹ç›®æ ‡è¿›è¡Œç²—ç•¥åŒ–å¦†ï¼ŒåŒæ—¶ç¡®ä¿åŠ¨æ€å’Œå¤šè§†è§’æ•ˆæœçš„ä¸€è‡´æ€§ã€‚è¿è´¯å¤åˆ¶é€šè¿‡é‡æ–°ç¼–ç ç”ŸæˆåŒ–å¦†å›¾åƒä¹‹é—´çš„å¹³å‡é¢éƒ¨å±æ€§æ¥ä¼˜åŒ–å…¨å±€UVåœ°å›¾ã€‚é€šè¿‡æŸ¥è¯¢å…¨å±€UVåœ°å›¾ï¼Œå®ƒå¾ˆå®¹æ˜“åˆæˆæ¥è‡ªä»»æ„è§†å›¾å’Œè¡¨æƒ…çš„è¿è´¯åŒ–å¦†æŒ‡å¯¼ï¼Œä»¥ä¼˜åŒ–ç›®æ ‡åŒ–èº«ã€‚ç»™å®šç²—ç•¥çš„åŒ–å¦†åŒ–èº«ï¼Œæˆ‘ä»¬è¿›ä¸€æ­¥é€šè¿‡å°†ç»†åŒ–æ¨¡å—èå…¥æ‰©æ•£æ¨¡å‹ä¸­ï¼Œæé«˜åŒ–å¦†æ•ˆæœï¼Œä»¥å®ç°é«˜è´¨é‡çš„åŒ–å¦†ã€‚å®éªŒè¡¨æ˜ï¼ŒAvatarMakeupåœ¨åŠ¨ç”»ä¸­å®ç°äº†æœ€å…ˆè¿›çš„åŒ–å¦†è½¬ç§»è´¨é‡å’Œä¸€è‡´æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.02419v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>è¯¥æ–‡æœ¬æè¿°äº†ä¸€ç§åä¸ºAvatarMakeupçš„3Dè™šæ‹Ÿè§’è‰²åŒ–å¦†æ–¹æ³•ã€‚æ­¤æ–¹æ³•å€ŸåŠ©é¢„è®­ç»ƒçš„æ‰©æ•£æ¨¡å‹ï¼Œä»å•ä¸€å‚è€ƒç…§ç‰‡è½¬ç§»å¦†å®¹ã€‚ä¸ºç¡®ä¿å¦†å®¹åœ¨åŠ¨æ€å’Œå¤šç§è§†è§’ä¸‹çš„è¿è´¯æ€§ï¼Œæå‡ºäº†Coherent Duplicationæ–¹æ³•ã€‚æ­¤å¤–ï¼Œè¿˜åŠ å…¥äº†ä¸€ä¸ªç»†åŒ–æ¨¡å—æ¥æå‡å¦†å®¹è´¨é‡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç°æœ‰3Dé«˜æ–¯ç¼–è¾‘æ–¹æ³•åœ¨è™šæ‹Ÿè§’è‰²åŒ–å¦†æ–¹é¢å­˜åœ¨å±€é™æ€§ï¼Œä¸èƒ½æ»¡è¶³çœŸå®å¦†å®¹æ•ˆæœçš„è¦æ±‚ã€‚</li>
<li>AvatarMakeupæ–¹æ³•åˆ©ç”¨é¢„è®­ç»ƒçš„æ‰©æ•£æ¨¡å‹ï¼Œä»å•ä¸€å‚è€ƒç…§ç‰‡è½¬ç§»å¦†å®¹ï¼Œå®ç°äº†ä¸ªæ€§åŒ–çš„åŒ–å¦†ã€‚</li>
<li>Coherent Duplicationæ–¹æ³•ç”¨äºç¡®ä¿å¦†å®¹åœ¨åŠ¨æ€å’Œå¤šç§è§†è§’ä¸‹çš„è¿è´¯æ€§ã€‚</li>
<li>æ‰©æ•£æ¨¡å‹ç”Ÿæˆå¦†å®¹å›¾åƒä½œä¸ºç›‘ç£ã€‚</li>
<li>ç”Ÿæˆå¦†å®¹å›¾åƒåœ¨ä¸åŒè§†è§’å’Œè¡¨æƒ…ä¸‹çš„ä¸ä¸€è‡´æ€§ï¼Œé€šè¿‡Coherent Duplicationæ–¹æ³•è¿›è¡Œä¼˜åŒ–ã€‚</li>
<li>é€šè¿‡æŸ¥è¯¢å…¨å±€UVåœ°å›¾ï¼Œå¯ä»¥è½»æ¾åˆæˆä»»æ„è§†è§’å’Œè¡¨æƒ…çš„è¿è´¯å¦†å®¹æŒ‡å¯¼ã€‚</li>
<li>å¼•å…¥ç»†åŒ–æ¨¡å—è¿›ä¸€æ­¥æå‡å¦†å®¹è´¨é‡ï¼Œå®ç°äº†é«˜æ°´å¹³çš„åŒ–å¦†æ•ˆæœã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.02419">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-6e3e6c52834cf18ae0fadeecd5453e5b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-780f7ed110bf1397c8d4ff88e95e0f05.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-78edc1c533121545a22adf2ffe1cc1a1.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-e8a6e4afb0f824163399f06eff68027c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8163a2160dd5abd48450c8e3796778c3.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="AniCrafter-Customizing-Realistic-Human-Centric-Animation-via-Avatar-Background-Conditioning-in-Video-Diffusion-Models"><a href="#AniCrafter-Customizing-Realistic-Human-Centric-Animation-via-Avatar-Background-Conditioning-in-Video-Diffusion-Models" class="headerlink" title="AniCrafter: Customizing Realistic Human-Centric Animation via   Avatar-Background Conditioning in Video Diffusion Models"></a>AniCrafter: Customizing Realistic Human-Centric Animation via   Avatar-Background Conditioning in Video Diffusion Models</h2><p><strong>Authors:Muyao Niu, Mingdeng Cao, Yifan Zhan, Qingtian Zhu, Mingze Ma, Jiancheng Zhao, Yanhong Zeng, Zhihang Zhong, Xiao Sun, Yinqiang Zheng</strong></p>
<p>Recent advances in video diffusion models have significantly improved character animation techniques. However, current approaches rely on basic structural conditions such as DWPose or SMPL-X to animate character images, limiting their effectiveness in open-domain scenarios with dynamic backgrounds or challenging human poses. In this paper, we introduce \textbf{AniCrafter}, a diffusion-based human-centric animation model that can seamlessly integrate and animate a given character into open-domain dynamic backgrounds while following given human motion sequences. Built on cutting-edge Image-to-Video (I2V) diffusion architectures, our model incorporates an innovative â€˜â€™avatar-backgroundâ€™â€™ conditioning mechanism that reframes open-domain human-centric animation as a restoration task, enabling more stable and versatile animation outputs. Experimental results demonstrate the superior performance of our method. Codes are available at <a target="_blank" rel="noopener" href="https://github.com/MyNiuuu/AniCrafter">https://github.com/MyNiuuu/AniCrafter</a>. </p>
<blockquote>
<p>è§†é¢‘æ‰©æ•£æ¨¡å‹çš„æœ€æ–°è¿›å±•æå¤§åœ°æ”¹è¿›äº†è§’è‰²åŠ¨ç”»æŠ€æœ¯ã€‚ç„¶è€Œï¼Œå½“å‰çš„æ–¹æ³•ä¾èµ–äºåŸºæœ¬ç»“æ„æ¡ä»¶ï¼Œå¦‚DWPoseæˆ–SMPL-Xæ¥é©±åŠ¨è§’è‰²å›¾åƒï¼Œè¿™åœ¨å…·æœ‰åŠ¨æ€èƒŒæ™¯æˆ–å¤æ‚äººç±»å§¿æ€çš„å¼€æ”¾åœºæ™¯ä¸­é™åˆ¶äº†å…¶æœ‰æ•ˆæ€§ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†<strong>AniCrafter</strong>ï¼Œè¿™æ˜¯ä¸€ä¸ªåŸºäºæ‰©æ•£çš„äººç±»ä¸­å¿ƒåŠ¨ç”»æ¨¡å‹ï¼Œå¯ä»¥æ— ç¼é›†æˆå¹¶é©±åŠ¨ç»™å®šçš„è§’è‰²è¿›å…¥å¼€æ”¾é¢†åŸŸçš„åŠ¨æ€èƒŒæ™¯ï¼ŒåŒæ—¶éµå¾ªç»™å®šçš„äººç±»è¿åŠ¨åºåˆ—ã€‚æˆ‘ä»¬çš„æ¨¡å‹å»ºç«‹åœ¨æœ€å…ˆè¿›çš„å›¾åƒåˆ°è§†é¢‘ï¼ˆI2Vï¼‰æ‰©æ•£æ¶æ„ä¸Šï¼Œèå…¥äº†ä¸€ç§åˆ›æ–°çš„â€œåŒ–èº«èƒŒæ™¯â€æ¡ä»¶æœºåˆ¶ï¼Œå°†å¼€æ”¾é¢†åŸŸçš„äººç±»ä¸­å¿ƒåŠ¨ç”»é‡æ–°æ„å»ºä¸ºæ¢å¤ä»»åŠ¡ï¼Œä»è€Œå®ç°æ›´ç¨³å®šå’Œæ›´é€šç”¨çš„åŠ¨ç”»è¾“å‡ºã€‚å®éªŒç»“æœè¯æ˜äº†æˆ‘ä»¬çš„æ–¹æ³•çš„ä¼˜è¶Šæ€§ã€‚ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/MyNiuuu/AniCrafter%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/MyNiuuu/AniCrafterè·å–ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.20255v2">PDF</a> Homepage: <a target="_blank" rel="noopener" href="https://myniuuu.github.io/AniCrafter">https://myniuuu.github.io/AniCrafter</a> ; Codes:   <a target="_blank" rel="noopener" href="https://github.com/MyNiuuu/AniCrafter">https://github.com/MyNiuuu/AniCrafter</a></p>
<p><strong>Summary</strong></p>
<p>è¿‘æœŸè§†é¢‘æ‰©æ•£æ¨¡å‹çš„è¿›æ­¥æ˜¾è‘—æå‡äº†è§’è‰²åŠ¨ç”»æŠ€æœ¯ã€‚ç„¶è€Œï¼Œå½“å‰æ–¹æ³•ä¾èµ–äºåŸºæœ¬ç»“æ„æ¡ä»¶ï¼ˆå¦‚DWPoseæˆ–SMPL-Xï¼‰æ¥é©±åŠ¨è§’è‰²å›¾åƒï¼Œè¿™åœ¨å¼€æ”¾åŸŸåŠ¨æ€èƒŒæ™¯æˆ–å¤æ‚äººç±»å§¿åŠ¿çš„åœºæ™¯ä¸­æ•ˆæœæœ‰é™ã€‚æœ¬æ–‡ä»‹ç»äº†ä¸€æ¬¾åŸºäºæ‰©æ•£çš„äººç±»ä¸­å¿ƒåŠ¨ç”»æ¨¡å‹â€”â€”AniCrafterï¼Œå®ƒèƒ½æ— ç¼é›†æˆå¹¶é©±åŠ¨ç»™å®šè§’è‰²è¿›å…¥å¼€æ”¾åŸŸåŠ¨æ€èƒŒæ™¯ï¼ŒåŒæ—¶éµå¾ªç»™å®šçš„äººç±»è¿åŠ¨åºåˆ—ã€‚è¯¥æ¨¡å‹é‡‡ç”¨å…ˆè¿›çš„å›¾åƒåˆ°è§†é¢‘ï¼ˆI2Vï¼‰æ‰©æ•£æ¶æ„ï¼Œå¼•å…¥äº†ä¸€ç§åˆ›æ–°çš„â€œAvatar-èƒŒæ™¯â€æ¡ä»¶æœºåˆ¶ï¼Œå°†å¼€æ”¾åŸŸäººç±»ä¸­å¿ƒåŠ¨ç”»é‡æ„ä¸ºæ¢å¤ä»»åŠ¡ï¼Œä»è€Œå®ç°æ›´ç¨³å®šå’Œå¤šæ ·åŒ–çš„åŠ¨ç”»è¾“å‡ºã€‚å®éªŒç»“æœè¯æ˜äº†è¯¥æ–¹æ³•çš„å“è¶Šæ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è¿‘æœŸè§†é¢‘æ‰©æ•£æ¨¡å‹çš„è¿›æ­¥å·²ç»æ˜¾è‘—æé«˜äº†è§’è‰²åŠ¨ç”»æŠ€æœ¯ã€‚</li>
<li>å½“å‰çš„è§’è‰²åŠ¨ç”»æ–¹æ³•å—é™äºåŸºæœ¬ç»“æ„æ¡ä»¶ï¼Œéš¾ä»¥å¤„ç†å¼€æ”¾åŸŸåŠ¨æ€èƒŒæ™¯å’Œå¤æ‚äººç±»å§¿åŠ¿ã€‚</li>
<li>è®ºæ–‡æå‡ºäº†åŸºäºæ‰©æ•£çš„äººç±»ä¸­å¿ƒåŠ¨ç”»æ¨¡å‹â€”â€”AniCrafterã€‚</li>
<li>AniCrafterèƒ½å¤Ÿæ— ç¼é›†æˆå¹¶é©±åŠ¨ç»™å®šè§’è‰²è¿›å…¥å¼€æ”¾åŸŸåŠ¨æ€èƒŒæ™¯ï¼Œéµå¾ªç»™å®šçš„äººç±»è¿åŠ¨åºåˆ—ã€‚</li>
<li>AniCrafteré‡‡ç”¨å…ˆè¿›çš„å›¾åƒåˆ°è§†é¢‘ï¼ˆI2Vï¼‰æ‰©æ•£æ¶æ„ã€‚</li>
<li>AniCrafterå¼•å…¥äº†åˆ›æ–°çš„â€œAvatar-èƒŒæ™¯â€æ¡ä»¶æœºåˆ¶ï¼Œå°†åŠ¨ç”»é‡æ„ä¸ºæ¢å¤ä»»åŠ¡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.20255">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-80009901d1d6eaf7732810057e40e1f2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-550328af642ac85ea2e2ec91a5e6c73d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c4b8eafe704519e8391534976f8146e1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-dc35214e8274550e784c50f8d5522189.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-023eb9317253da6a54214db5f4360f09.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="Joint-Source-Channel-Noise-Adding-with-Adaptive-Denoising-for-Diffusion-Based-Semantic-Communications"><a href="#Joint-Source-Channel-Noise-Adding-with-Adaptive-Denoising-for-Diffusion-Based-Semantic-Communications" class="headerlink" title="Joint Source-Channel Noise Adding with Adaptive Denoising for   Diffusion-Based Semantic Communications"></a>Joint Source-Channel Noise Adding with Adaptive Denoising for   Diffusion-Based Semantic Communications</h2><p><strong>Authors:Chengyang Liang, Dong Li</strong></p>
<p>Semantic communication (SemCom) aims to convey the intended meaning of messages rather than merely transmitting bits, thereby offering greater efficiency and robustness, particularly in resource-constrained or noisy environments. In this paper, we propose a novel framework which is referred to as joint source-channel noise adding with adaptive denoising (JSCNA-AD) for SemCom based on a diffusion model (DM). Unlike conventional encoder-decoder designs, our approach intentionally incorporates the channel noise during transmission, effectively transforming the harmful channel noise into a constructive component of the diffusion-based semantic reconstruction process. Besides, we introduce an attention-based adaptive denoising mechanism, in which transmitted images are divided into multiple regions, and the number of denoising steps is dynamically allocated based on the semantic importance of each region. This design effectively balances the reception quality and the inference latency by prioritizing the critical semantic information. Extensive experiments demonstrate that our method significantly outperforms existing SemCom schemes under various noise conditions, underscoring the potential of diffusion-based models in next-generation communication systems. </p>
<blockquote>
<p>è¯­ä¹‰é€šä¿¡ï¼ˆSemComï¼‰æ—¨åœ¨ä¼ è¾¾ä¿¡æ¯çš„æ„å›¾ï¼Œè€Œä¸ä»…ä»…æ˜¯ä¼ è¾“æ¯”ç‰¹ï¼Œä»è€Œæä¾›æ›´é«˜æ•ˆå’Œç¨³å¥çš„é€šä¿¡ï¼Œç‰¹åˆ«æ˜¯åœ¨èµ„æºå—é™æˆ–å˜ˆæ‚çš„ç¯å¢ƒä¸­ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºæ‰©æ•£æ¨¡å‹ï¼ˆDMï¼‰çš„è¯­ä¹‰é€šä¿¡çš„æ–°å‹æ¡†æ¶ï¼Œç§°ä¸ºè”åˆæºä¿¡é“å™ªå£°æ·»åŠ ä¸è‡ªé€‚åº”å»å™ªï¼ˆJSCNA-ADï¼‰ã€‚ä¸ä¼ ç»Ÿçš„ç¼–ç å™¨-è§£ç å™¨è®¾è®¡ä¸åŒï¼Œæˆ‘ä»¬çš„æ–¹æ³•æ•…æ„åœ¨ä¼ è¾“è¿‡ç¨‹ä¸­åŠ å…¥ä¿¡é“å™ªå£°ï¼Œæœ‰æ•ˆåœ°å°†æœ‰å®³çš„ä¿¡é“å™ªå£°è½¬åŒ–ä¸ºæ‰©æ•£è¯­ä¹‰é‡å»ºè¿‡ç¨‹çš„å»ºè®¾æ€§ç»„æˆéƒ¨åˆ†ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§åŸºäºæ³¨æ„åŠ›çš„è‡ªé€‚åº”å»å™ªæœºåˆ¶ï¼Œå…¶ä¸­ä¼ è¾“çš„å›¾åƒè¢«åˆ†å‰²æˆå¤šä¸ªåŒºåŸŸï¼Œå¹¶æ ¹æ®æ¯ä¸ªåŒºåŸŸçš„è¯­ä¹‰é‡è¦æ€§åŠ¨æ€åˆ†é…å»å™ªæ­¥éª¤çš„æ•°é‡ã€‚è¿™ç§è®¾è®¡é€šè¿‡ä¼˜å…ˆå¤„ç†å…³é”®çš„è¯­ä¹‰ä¿¡æ¯ï¼Œæœ‰æ•ˆåœ°å¹³è¡¡äº†æ¥æ”¶è´¨é‡å’Œæ¨ç†å»¶è¿Ÿã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼Œåœ¨å„ç§å™ªå£°æ¡ä»¶ä¸‹ï¼Œæˆ‘ä»¬çš„æ–¹æ³•æ˜¾è‘—ä¼˜äºç°æœ‰çš„SemComæ–¹æ¡ˆï¼Œçªæ˜¾äº†æ‰©æ•£æ¨¡å‹åœ¨ä¸‹ä¸€ä»£é€šä¿¡ç³»ç»Ÿä¸­çš„åº”ç”¨æ½œåŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.09644v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>åŸºäºæ‰©æ•£æ¨¡å‹çš„è¯­ä¹‰é€šä¿¡æ–°å‹æ¡†æ¶ç ”ç©¶ã€‚è¯¥æ¡†æ¶é€šè¿‡è”åˆæºä¿¡é“å™ªå£°æ·»åŠ å’Œè‡ªé€‚åº”å»å™ªæŠ€æœ¯ï¼Œå°†æœ‰å®³çš„ä¿¡é“å™ªå£°è½¬åŒ–ä¸ºå»ºè®¾æ€§æˆåˆ†ï¼Œä¼˜åŒ–äº†è¯­ä¹‰é‡å»ºè¿‡ç¨‹ã€‚å¼•å…¥æ³¨æ„åŠ›æœºåˆ¶è‡ªé€‚åº”å»å™ªæœºåˆ¶ï¼Œæ ¹æ®å›¾åƒåŒºåŸŸçš„è¯­ä¹‰é‡è¦æ€§åŠ¨æ€åˆ†é…å»å™ªæ­¥éª¤ï¼Œæå‡æ¥æ”¶è´¨é‡å’Œé™ä½æ¨ç†å»¶è¿Ÿã€‚å®éªŒè¯æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å„ç§å™ªå£°æ¡ä»¶ä¸‹æ˜¾è‘—ä¼˜äºç°æœ‰è¯­ä¹‰é€šä¿¡æ–¹æ¡ˆã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>SemComæ—¨åœ¨ä¼ è¾¾æ¶ˆæ¯çš„æ„å›¾æ„ä¹‰ï¼Œè€Œéä»…ä¼ è¾“æ¯”ç‰¹ï¼Œä»è€Œæé«˜æ•ˆç‡å’Œç¨³å¥æ€§ï¼Œå°¤å…¶åœ¨èµ„æºå—é™æˆ–å˜ˆæ‚ç¯å¢ƒä¸­ã€‚</li>
<li>æå‡ºåŸºäºæ‰©æ•£æ¨¡å‹çš„æ–°å‹JSCNA-ADæ¡†æ¶ï¼Œæ•…æ„å°†ä¿¡é“å™ªå£°çº³å…¥ä¼ è¾“è¿‡ç¨‹ï¼Œè½¬åŒ–ä¸ºå»ºè®¾æ€§æˆåˆ†ã€‚</li>
<li>å¼•å…¥æ³¨æ„åŠ›æœºåˆ¶è‡ªé€‚åº”å»å™ªæŠ€æœ¯ï¼Œæ ¹æ®å›¾åƒåŒºåŸŸçš„è¯­ä¹‰é‡è¦æ€§åŠ¨æ€è°ƒæ•´å»å™ªæ­¥éª¤ã€‚</li>
<li>è¯¥è®¾è®¡æœ‰æ•ˆå¹³è¡¡æ¥æ”¶è´¨é‡å’Œæ¨ç†å»¶è¿Ÿï¼Œä¼˜å…ˆå¤„ç†å…³é”®è¯­ä¹‰ä¿¡æ¯ã€‚</li>
<li>å®éªŒè¯æ˜ï¼Œè¯¥æ¡†æ¶åœ¨å„ç§å™ªå£°æ¡ä»¶ä¸‹æ˜¾è‘—ä¼˜äºç°æœ‰SemComæ–¹æ¡ˆã€‚</li>
<li>æ‰©æ•£æ¨¡å‹åœ¨ä¸‹ä¸€ä»£é€šä¿¡ç³»ç»Ÿä¸­å…·æœ‰æ½œåœ¨åº”ç”¨ä»·å€¼ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.09644">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-b64f9007e247940dd56de310da65b3af.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-81176502139956544656b6cfbe4ac5c2.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-ff587e62c9ab7d11f1d6b20a6a4b50e7.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-740fcb9a5191aac20bd9ac8db7cd12ea.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="CMD-Controllable-Multiview-Diffusion-for-3D-Editing-and-Progressive-Generation"><a href="#CMD-Controllable-Multiview-Diffusion-for-3D-Editing-and-Progressive-Generation" class="headerlink" title="CMD: Controllable Multiview Diffusion for 3D Editing and Progressive   Generation"></a>CMD: Controllable Multiview Diffusion for 3D Editing and Progressive   Generation</h2><p><strong>Authors:Peng Li, Suizhi Ma, Jialiang Chen, Yuan Liu, Congyi Zhang, Wei Xue, Wenhan Luo, Alla Sheffer, Wenping Wang, Yike Guo</strong></p>
<p>Recently, 3D generation methods have shown their powerful ability to automate 3D model creation. However, most 3D generation methods only rely on an input image or a text prompt to generate a 3D model, which lacks the control of each component of the generated 3D model. Any modifications of the input image lead to an entire regeneration of the 3D models. In this paper, we introduce a new method called CMD that generates a 3D model from an input image while enabling flexible local editing of each component of the 3D model. In CMD, we formulate the 3D generation as a conditional multiview diffusion model, which takes the existing or known parts as conditions and generates the edited or added components. This conditional multiview diffusion model not only allows the generation of 3D models part by part but also enables local editing of 3D models according to the local revision of the input image without changing other 3D parts. Extensive experiments are conducted to demonstrate that CMD decomposes a complex 3D generation task into multiple components, improving the generation quality. Meanwhile, CMD enables efficient and flexible local editing of a 3D model by just editing one rendered image. </p>
<blockquote>
<p>æœ€è¿‘ï¼Œ3Dç”Ÿæˆæ–¹æ³•å·²ç»æ˜¾ç¤ºå‡ºå…¶è‡ªåŠ¨åŒ–åˆ›å»º3Dæ¨¡å‹çš„å¼ºå¤§èƒ½åŠ›ã€‚ç„¶è€Œï¼Œå¤§å¤šæ•°3Dç”Ÿæˆæ–¹æ³•ä»…ä¾èµ–äºè¾“å…¥å›¾åƒæˆ–æ–‡æœ¬æç¤ºæ¥ç”Ÿæˆ3Dæ¨¡å‹ï¼Œè¿™ç¼ºä¹å¯¹ç”Ÿæˆ3Dæ¨¡å‹å„ä¸ªç»„ä»¶çš„æ§åˆ¶ã€‚è¾“å…¥å›¾åƒçš„ä»»ä½•ä¿®æ”¹éƒ½ä¼šå¯¼è‡´3Dæ¨¡å‹çš„æ•´ä¸ªé‡æ–°ç”Ÿæˆã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†ä¸€ç§æ–°æ–¹æ³•CMDï¼Œå®ƒå¯ä»¥ä»è¾“å…¥å›¾åƒç”Ÿæˆ3Dæ¨¡å‹ï¼ŒåŒæ—¶å®ç°å¯¹ç”Ÿæˆ3Dæ¨¡å‹çš„å„ä¸ªç»„ä»¶è¿›è¡Œçµæ´»å±€éƒ¨ç¼–è¾‘ã€‚åœ¨CMDä¸­ï¼Œæˆ‘ä»¬å°†3Dç”Ÿæˆåˆ¶å®šä¸ºæ¡ä»¶å¤šè§†å›¾æ‰©æ•£æ¨¡å‹ï¼Œä»¥ç°æœ‰æˆ–å·²çŸ¥éƒ¨åˆ†ä½œä¸ºæ¡ä»¶ï¼Œç”Ÿæˆç¼–è¾‘æˆ–æ·»åŠ çš„ç»„ä»¶ã€‚è¿™ç§æ¡ä»¶å¤šè§†å›¾æ‰©æ•£æ¨¡å‹ä¸ä»…å…è®¸é€ä¸ªéƒ¨åˆ†åœ°ç”Ÿæˆ3Dæ¨¡å‹ï¼Œè€Œä¸”èƒ½å¤Ÿåœ¨ä¸æ”¹å˜å…¶ä»–3Déƒ¨åˆ†çš„æƒ…å†µä¸‹ï¼Œæ ¹æ®è¾“å…¥å›¾åƒçš„å±€éƒ¨ä¿®è®¢å¯¹3Dæ¨¡å‹è¿›è¡Œå±€éƒ¨ç¼–è¾‘ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼ŒCMDå°†å¤æ‚çš„3Dç”Ÿæˆä»»åŠ¡åˆ†è§£ä¸ºå¤šä¸ªç»„ä»¶ï¼Œæé«˜äº†ç”Ÿæˆè´¨é‡ã€‚åŒæ—¶ï¼ŒCMDé€šè¿‡ä»…ç¼–è¾‘ä¸€ä¸ªæ¸²æŸ“å›¾åƒå°±èƒ½å®ç°å¯¹3Dæ¨¡å‹çš„é«˜æ•ˆå’Œçµæ´»å±€éƒ¨ç¼–è¾‘ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.07003v2">PDF</a> SIGGRAPH 2025, Page: <a target="_blank" rel="noopener" href="https://penghtyx.github.io/CMD/">https://penghtyx.github.io/CMD/</a></p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åä¸ºCMDçš„æ–°æ–¹æ³•ï¼Œèƒ½å¤Ÿä»è¾“å…¥å›¾åƒç”Ÿæˆ3Dæ¨¡å‹ï¼Œå¹¶å®ç°å¯¹æ¨¡å‹å„ç»„ä»¶çš„çµæ´»å±€éƒ¨ç¼–è¾‘ã€‚è¯¥æ–¹æ³•é‡‡ç”¨æ¡ä»¶å¤šè§†è§’æ‰©æ•£æ¨¡å‹è¿›è¡Œå»ºæ¨¡ï¼Œä»¥ç°æœ‰éƒ¨åˆ†ä½œä¸ºæ¡ä»¶ç”Ÿæˆç¼–è¾‘æˆ–æ·»åŠ çš„ç»„ä»¶ã€‚è¿™ç§æ¨¡å‹ä¸ä»…èƒ½å¤Ÿåˆ†æ­¥éª¤ç”Ÿæˆ3Dæ¨¡å‹ï¼Œè€Œä¸”èƒ½å¤Ÿæ ¹æ®è¾“å…¥å›¾åƒçš„å±€éƒ¨ä¿®æ”¹è¿›è¡Œå±€éƒ¨ç¼–è¾‘ï¼Œè€Œä¸ä¼šå½±å“åˆ°å…¶ä»–éƒ¨åˆ†çš„3Dæ¨¡å‹ã€‚å®éªŒè¯æ˜ï¼ŒCMDæ–¹æ³•èƒ½å¤Ÿæé«˜ç”Ÿæˆè´¨é‡ï¼Œå¹¶èƒ½é«˜æ•ˆçµæ´»åœ°ç¼–è¾‘3Dæ¨¡å‹ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>CMDæ–¹æ³•èƒ½å¤Ÿä»è¾“å…¥å›¾åƒç”Ÿæˆ3Dæ¨¡å‹ï¼Œå¹¶å…è®¸å¯¹ç”Ÿæˆçš„æ¨¡å‹è¿›è¡Œçµæ´»å±€éƒ¨ç¼–è¾‘ã€‚</li>
<li>CMDé‡‡ç”¨æ¡ä»¶å¤šè§†è§’æ‰©æ•£æ¨¡å‹è¿›è¡Œå»ºæ¨¡ï¼Œå°†ç°æœ‰éƒ¨åˆ†ä½œä¸ºæ¡ä»¶æ¥ç”Ÿæˆæ–°çš„ç»„ä»¶ã€‚</li>
<li>è¯¥æ–¹æ³•èƒ½å¤Ÿå°†å¤æ‚çš„3Dç”Ÿæˆä»»åŠ¡åˆ†è§£ä¸ºå¤šä¸ªç»„ä»¶ï¼Œæé«˜äº†ç”Ÿæˆè´¨é‡ã€‚</li>
<li>CMDé€šè¿‡ä»…ç¼–è¾‘ä¸€ä¸ªæ¸²æŸ“å›¾åƒå°±èƒ½å®ç°å¯¹3Dæ¨¡å‹çš„å±€éƒ¨é«˜æ•ˆçµæ´»ç¼–è¾‘ã€‚</li>
<li>å®éªŒè¯æ˜äº†CMDæ–¹æ³•çš„ä¼˜è¶Šæ€§èƒ½ï¼ŒåŒ…æ‹¬ç”Ÿæˆè´¨é‡å’Œç¼–è¾‘æ•ˆç‡ç­‰æ–¹é¢ã€‚</li>
<li>CMDæ–¹æ³•ä¸ºè§£å†³å½“å‰3Dç”Ÿæˆæ–¹æ³•ä¸­ç¼ºä¹ç»„ä»¶æ§åˆ¶çš„é—®é¢˜æä¾›äº†æ–°çš„è§£å†³æ–¹æ¡ˆã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.07003">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-88d8216b28adaf72e2907fadc6e69bb8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a88b5399c1f4addeaa9f3b63a65a4c7b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-9908e9dece2f14de1083c1950d27a476.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-0eb2fe9dbdd0ebf1ddf4a15751168337.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-bf78b2795aaadf87a8896d9662a1ab62.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="Diffusion-based-Adversarial-Identity-Manipulation-for-Facial-Privacy-Protection"><a href="#Diffusion-based-Adversarial-Identity-Manipulation-for-Facial-Privacy-Protection" class="headerlink" title="Diffusion-based Adversarial Identity Manipulation for Facial Privacy   Protection"></a>Diffusion-based Adversarial Identity Manipulation for Facial Privacy   Protection</h2><p><strong>Authors:Liqin Wang, Qianyue Hu, Wei Lu, Xiangyang Luo</strong></p>
<p>The success of face recognition (FR) systems has led to serious privacy concerns due to potential unauthorized surveillance and user tracking on social networks. Existing methods for enhancing privacy fail to generate natural face images that can protect facial privacy. In this paper, we propose diffusion-based adversarial identity manipulation (DiffAIM) to generate natural and highly transferable adversarial faces against malicious FR systems. To be specific, we manipulate facial identity within the low-dimensional latent space of a diffusion model. This involves iteratively injecting gradient-based adversarial identity guidance during the reverse diffusion process, progressively steering the generation toward the desired adversarial faces. The guidance is optimized for identity convergence towards a target while promoting semantic divergence from the source, facilitating effective impersonation while maintaining visual naturalness. We further incorporate structure-preserving regularization to preserve facial structure consistency during manipulation. Extensive experiments on both face verification and identification tasks demonstrate that compared with the state-of-the-art, DiffAIM achieves stronger black-box attack transferability while maintaining superior visual quality. We also demonstrate the effectiveness of the proposed approach for commercial FR APIs, including Face++ and Aliyun. </p>
<blockquote>
<p>äººè„¸è¯†åˆ«ï¼ˆFRï¼‰ç³»ç»Ÿçš„æˆåŠŸå¼•å‘äº†ä¸¥é‡çš„éšç§æ‹…å¿§ï¼Œå› ä¸ºå¯èƒ½å­˜åœ¨æœªç»æˆæƒçš„ç›‘ç£å’Œç¤¾ä¼šç½‘ç»œä¸Šçš„ç”¨æˆ·è·Ÿè¸ªã€‚ç°æœ‰çš„å¢å¼ºéšç§çš„æ–¹æ³•æ— æ³•ç”Ÿæˆèƒ½å¤Ÿä¿æŠ¤é¢éƒ¨éšç§çš„è‡ªç„¶é¢éƒ¨å›¾åƒã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†åŸºäºæ‰©æ•£çš„å¯¹æŠ—èº«ä»½æ“çºµï¼ˆDiffAIMï¼‰æ–¹æ³•ï¼Œä»¥ç”Ÿæˆè‡ªç„¶ä¸”é«˜åº¦å¯è½¬ç§»çš„å¯¹æŠ—é¢éƒ¨å›¾åƒï¼Œå¯¹æŠ—æ¶æ„FRç³»ç»Ÿã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬åœ¨æ‰©æ•£æ¨¡å‹çš„ä½ç»´æ½œåœ¨ç©ºé—´ä¸­æ“ä½œé¢éƒ¨èº«ä»½ã€‚è¿™æ¶‰åŠåœ¨åå‘æ‰©æ•£è¿‡ç¨‹ä¸­è¿­ä»£åœ°æ³¨å…¥åŸºäºæ¢¯åº¦çš„å¯¹æŠ—æ€§èº«ä»½æŒ‡å¯¼ï¼Œé€æ­¥å¼•å¯¼ç”Ÿæˆèµ°å‘æ‰€éœ€çš„å¯¹æŠ—æ€§é¢éƒ¨å›¾åƒã€‚è¯¥æŒ‡å¯¼é’ˆå¯¹èº«ä»½æ”¶æ•›åˆ°ç›®æ ‡è¿›è¡Œä¼˜åŒ–ï¼ŒåŒæ—¶ä¿ƒè¿›ä¸æºä¹‹é—´çš„è¯­ä¹‰å‘æ•£ï¼Œä»è€Œåœ¨ä¿æŒè§†è§‰è‡ªç„¶æ€§çš„åŒæ—¶å®ç°æœ‰æ•ˆçš„ä¼ªè£…ã€‚æˆ‘ä»¬è¿›ä¸€æ­¥ç»“åˆäº†ç»“æ„ä¿æŒæ­£åˆ™åŒ–ï¼Œä»¥ä¿æŒé¢éƒ¨ç»“æ„ä¸€è‡´æ€§åœ¨æ“çºµè¿‡ç¨‹ä¸­ã€‚å¯¹é¢éƒ¨éªŒè¯å’Œè¯†åˆ«ä»»åŠ¡çš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼Œä¸æœ€æ–°æŠ€æœ¯ç›¸æ¯”ï¼ŒDiffAIMå®ç°äº†æ›´å¼ºçš„é»‘ç›’æ”»å‡»è½¬ç§»èƒ½åŠ›ï¼ŒåŒæ—¶ä¿æŒäº†è¾ƒé«˜çš„è§†è§‰è´¨é‡ã€‚æˆ‘ä»¬è¿˜è¯æ˜äº†æ‰€æå‡ºæ–¹æ³•åœ¨åŒ…æ‹¬Face++å’Œé˜¿é‡Œäº‘åœ¨å†…çš„å•†ä¸šFR APIä¸Šçš„æœ‰æ•ˆæ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.21646v2">PDF</a> Accepted by ACM MM 2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†åŸºäºæ‰©æ•£æ¨¡å‹çš„å¯¹æŠ—æ€§èº«ä»½æ“çºµæŠ€æœ¯ï¼ˆDiffAIMï¼‰ï¼Œç”¨äºç”Ÿæˆè‡ªç„¶çš„ã€é«˜åº¦å¯è¿ç§»çš„å¯¹æŠ—æ€§é¢éƒ¨å›¾åƒï¼Œä»¥ä¿æŠ¤é¢éƒ¨éšç§å¹¶å¯¹æŠ—æ¶æ„äººè„¸è¯†åˆ«ï¼ˆFRï¼‰ç³»ç»Ÿã€‚é€šè¿‡æ“çºµä½ç»´æ½œåœ¨ç©ºé—´ä¸­çš„é¢éƒ¨èº«ä»½ï¼Œå¹¶åœ¨åå‘æ‰©æ•£è¿‡ç¨‹ä¸­é€æ­¥å¼•å¯¼ç”Ÿæˆé¢å‘ç›®æ ‡å¯¹æŠ—æ€§é¢éƒ¨ï¼Œå®ç°æœ‰æ•ˆä¼ªè£…åŒæ—¶ä¿æŒè§†è§‰è‡ªç„¶æ€§ã€‚ç»“åˆç»“æ„ä¿æŒæ­£åˆ™åŒ–ï¼Œä¿æŒé¢éƒ¨ç»“æ„ä¸€è‡´æ€§ã€‚å®éªŒè¡¨æ˜ï¼Œä¸ç°æœ‰æŠ€æœ¯ç›¸æ¯”ï¼ŒDiffAIMåœ¨äººè„¸è¯†åˆ«å’Œèº«ä»½éªŒè¯ä»»åŠ¡ä¸Šå®ç°äº†æ›´å¼ºçš„é»‘ç›’æ”»å‡»è¿ç§»æ€§å’Œæ›´é«˜çš„è§†è§‰è´¨é‡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ‰©æ•£æ¨¡å‹ç”¨äºç”Ÿæˆå¯¹æŠ—æ€§é¢éƒ¨å›¾åƒã€‚</li>
<li>é€šè¿‡æ“çºµä½ç»´æ½œåœ¨ç©ºé—´ä¸­çš„é¢éƒ¨èº«ä»½æ¥ç”Ÿæˆè‡ªç„¶çš„é¢éƒ¨å›¾åƒã€‚</li>
<li>åœ¨åå‘æ‰©æ•£è¿‡ç¨‹ä¸­æ³¨å…¥åŸºäºæ¢¯åº¦çš„å¯¹æŠ—æ€§èº«ä»½æŒ‡å¯¼ã€‚</li>
<li>ä¼˜åŒ–æŒ‡å¯¼ä»¥å®ç°èº«ä»½æ”¶æ•›å¹¶ä¿ƒè¿›è¯­ä¹‰å‘æ•£ã€‚</li>
<li>ç»“åˆç»“æ„ä¿æŒæ­£åˆ™åŒ–ä»¥ä¿æŒé¢éƒ¨ç»“æ„ä¸€è‡´æ€§ã€‚</li>
<li>ç›¸å¯¹äºç°æœ‰æŠ€æœ¯ï¼ŒDiffAIMå…·æœ‰æ›´å¼ºçš„é»‘ç›’æ”»å‡»è¿ç§»æ€§å¹¶ä¿æŒä¼˜è¶Šçš„è§†è§‰è´¨é‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.21646">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-90ccebc2a3259030bdac1fe77070e139.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3d82d52452e5f49bfc0f3a224c873176.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-58cc7ead29eb1b6f99da38c2be1ad145.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-7c3ffd0ffde01cea604bc3781c6b8a48.jpg" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="DiT4SR-Taming-Diffusion-Transformer-for-Real-World-Image-Super-Resolution"><a href="#DiT4SR-Taming-Diffusion-Transformer-for-Real-World-Image-Super-Resolution" class="headerlink" title="DiT4SR: Taming Diffusion Transformer for Real-World Image   Super-Resolution"></a>DiT4SR: Taming Diffusion Transformer for Real-World Image   Super-Resolution</h2><p><strong>Authors:Zheng-Peng Duan, Jiawei Zhang, Xin Jin, Ziheng Zhang, Zheng Xiong, Dongqing Zou, Jimmy S. Ren, Chun-Le Guo, Chongyi Li</strong></p>
<p>Large-scale pre-trained diffusion models are becoming increasingly popular in solving the Real-World Image Super-Resolution (Real-ISR) problem because of their rich generative priors. The recent development of diffusion transformer (DiT) has witnessed overwhelming performance over the traditional UNet-based architecture in image generation, which also raises the question: Can we adopt the advanced DiT-based diffusion model for Real-ISR? To this end, we propose our DiT4SR, one of the pioneering works to tame the large-scale DiT model for Real-ISR. Instead of directly injecting embeddings extracted from low-resolution (LR) images like ControlNet, we integrate the LR embeddings into the original attention mechanism of DiT, allowing for the bidirectional flow of information between the LR latent and the generated latent. The sufficient interaction of these two streams allows the LR stream to evolve with the diffusion process, producing progressively refined guidance that better aligns with the generated latent at each diffusion step. Additionally, the LR guidance is injected into the generated latent via a cross-stream convolution layer, compensating for DiTâ€™s limited ability to capture local information. These simple but effective designs endow the DiT model with superior performance in Real-ISR, which is demonstrated by extensive experiments. Project Page: <a target="_blank" rel="noopener" href="https://adam-duan.github.io/projects/dit4sr/">https://adam-duan.github.io/projects/dit4sr/</a>. </p>
<blockquote>
<p>å¤§è§„æ¨¡é¢„è®­ç»ƒæ‰©æ•£æ¨¡å‹ç”±äºå…¶ä¸°å¯Œçš„ç”Ÿæˆå…ˆéªŒï¼Œåœ¨è§£å†³ç°å®å›¾åƒè¶…åˆ†è¾¨ç‡ï¼ˆReal-ISRï¼‰é—®é¢˜ä¸Šè¶Šæ¥è¶Šå—æ¬¢è¿ã€‚æœ€è¿‘çš„æ‰©æ•£å˜å‹å™¨ï¼ˆDiTï¼‰çš„å‘å±•åœ¨å›¾åƒç”Ÿæˆæ–¹é¢å·²ç»è¶…è¶Šäº†ä¼ ç»Ÿçš„åŸºäºUNetçš„æ¶æ„ï¼Œè¿™ä¹Ÿå¼•å‘äº†ä¸€ä¸ªé—®é¢˜ï¼šæˆ‘ä»¬å¯ä»¥é‡‡ç”¨å…ˆè¿›çš„åŸºäºDiTçš„æ‰©æ•£æ¨¡å‹è¿›è¡ŒReal-ISRå—ï¼Ÿä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†DiT4SRï¼Œè¿™æ˜¯é¦–æ‰¹å°è¯•å°†å¤§è§„æ¨¡DiTæ¨¡å‹ç”¨äºReal-ISRçš„å·¥ä½œä¹‹ä¸€ã€‚ä¸åŒäºControlNetç›´æ¥æ³¨å…¥ä»ä½åˆ†è¾¨ç‡ï¼ˆLRï¼‰å›¾åƒä¸­æå–çš„åµŒå…¥ï¼Œæˆ‘ä»¬å°†LRåµŒå…¥é›†æˆåˆ°DiTçš„åŸå§‹æ³¨æ„æœºåˆ¶ä¸­ï¼Œå…è®¸LRæ½œä¼å’Œç”Ÿæˆæ½œä¼ä¹‹é—´çš„åŒå‘ä¿¡æ¯æµã€‚è¿™ä¸¤ä¸ªæµçš„å……åˆ†äº¤äº’ä½¿LRæµéšç€æ‰©æ•£è¿‡ç¨‹è€Œå‘å±•ï¼Œäº§ç”Ÿé€æ­¥ç²¾ç»†çš„æŒ‡å¯¼ï¼Œæ›´å¥½åœ°ä¸æ¯ä¸ªæ‰©æ•£æ­¥éª¤ä¸­çš„ç”Ÿæˆæ½œä¼å¯¹é½ã€‚æ­¤å¤–ï¼Œé€šè¿‡è·¨æµå·ç§¯å±‚å°†LRæŒ‡å¯¼æ³¨å…¥åˆ°ç”Ÿæˆçš„æ½œä¼ä¸­ï¼Œå¼¥è¡¥äº†DiTæ•æ‰å±€éƒ¨ä¿¡æ¯èƒ½åŠ›çš„ä¸è¶³ã€‚è¿™äº›ç®€å•è€Œæœ‰æ•ˆçš„è®¾è®¡ä½¿å¾—DiTæ¨¡å‹åœ¨Real-ISRæ–¹é¢çš„æ€§èƒ½å“è¶Šï¼Œè¿™å·²é€šè¿‡å¤§é‡å®éªŒå¾—åˆ°è¯æ˜ã€‚é¡¹ç›®é¡µé¢ï¼š<a target="_blank" rel="noopener" href="https://adam-duan.github.io/projects/dit4sr/%E3%80%82">https://adam-duan.github.io/projects/dit4sr/ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.23580v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<pre><code>åŸºäºæ‰©æ•£æ¨¡å‹çš„DiTæ¶æ„åœ¨å›¾åƒç”Ÿæˆé¢†åŸŸè¡¨ç°å‡ºå¼ºå¤§çš„æ€§èƒ½ä¼˜åŠ¿ï¼Œä¸ºæ­¤ç ”ç©¶è€…æå‡ºDiT4SRæ¨¡å‹ï¼Œæ—¨åœ¨å°†å¤§å‹DiTæ¨¡å‹åº”ç”¨äºè§£å†³ç°å®å›¾åƒè¶…åˆ†è¾¨ç‡é—®é¢˜ï¼ˆReal-ISRï¼‰ã€‚DiT4SRå°†ä½åˆ†è¾¨ç‡ï¼ˆLRï¼‰åµŒå…¥æ•´åˆåˆ°DiTçš„åŸç”Ÿæ³¨æ„åŠ›æœºåˆ¶ä¸­ï¼Œå®ç°äº†LRæ½œå€¼ä¸ç”Ÿæˆæ½œå€¼ä¹‹é—´çš„åŒå‘ä¿¡æ¯æµã€‚æ­¤å¤–ï¼Œé€šè¿‡è·¨æµå·ç§¯å±‚å°†LRæŒ‡å¯¼æ³¨å…¥ç”Ÿæˆæ½œå€¼ä¸­ï¼Œå¢å¼ºäº†DiTæ•æ‰å±€éƒ¨ä¿¡æ¯çš„èƒ½åŠ›ã€‚è¿™äº›è®¾è®¡ä½¿å¾—DiTæ¨¡å‹åœ¨Real-ISRä»»åŠ¡ä¸Šè¡¨ç°å‡ºå“è¶Šæ€§èƒ½ã€‚
</code></pre>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹é¢„è®­ç»ƒæ‰©æ•£æ¨¡å‹åœ¨è§£å†³ç°å®å›¾åƒè¶…åˆ†è¾¨ç‡ï¼ˆReal-ISRï¼‰é—®é¢˜ä¸Šæ—¥ç›Šæ™®åŠï¼Œå½’åŠŸäºå…¶ä¸°å¯Œçš„ç”Ÿæˆå…ˆéªŒã€‚</li>
<li>DiTæ¶æ„åœ¨å›¾åƒç”Ÿæˆé¢†åŸŸè¡¨ç°å‡ºå“è¶Šæ€§èƒ½ï¼Œæ¿€å‘äº†å°†å…¶åº”ç”¨äºReal-ISRé—®é¢˜çš„æ¢ç´¢ã€‚</li>
<li>DiT4SRæ¨¡å‹å®ç°äº†ä½åˆ†è¾¨ç‡ï¼ˆLRï¼‰ä¸ç”Ÿæˆæ½œå€¼ä¹‹é—´çš„åŒå‘ä¿¡æ¯æµï¼Œé€šè¿‡æ•´åˆLRåµŒå…¥åˆ°DiTçš„æ³¨æ„åŠ›æœºåˆ¶ä¸­ã€‚</li>
<li>è·¨æµå·ç§¯å±‚ç”¨äºå°†LRæŒ‡å¯¼æ³¨å…¥ç”Ÿæˆæ½œå€¼ä¸­ï¼Œä»¥è¡¥å¿DiTæ•æ‰å±€éƒ¨ä¿¡æ¯çš„å±€é™æ€§ã€‚</li>
<li>DiT4SRæ¨¡å‹è®¾è®¡ç®€æ´ä½†æœ‰æ•ˆï¼Œåœ¨Real-ISRä»»åŠ¡ä¸Šå±•ç°å‡ºå“è¶Šæ€§èƒ½ã€‚</li>
<li>è¯¥ç ”ç©¶é€šè¿‡å¹¿æ³›å®éªŒéªŒè¯äº†DiT4SRæ¨¡å‹çš„æ€§èƒ½ä¼˜åŠ¿ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.23580">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-e8d923a955cd158ea91fda65852e1b9d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-131176c0dbe7f18f49555e7c0e4c4e7e.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-8b0b3cdee32ad243cdfee709da33587c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8e3348ff864a66a29131e3b34fbbfce3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f873a44af8eb34caa48a8879c2ec6907.jpg" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="DynamicFace-High-Quality-and-Consistent-Face-Swapping-for-Image-and-Video-using-Composable-3D-Facial-Priors"><a href="#DynamicFace-High-Quality-and-Consistent-Face-Swapping-for-Image-and-Video-using-Composable-3D-Facial-Priors" class="headerlink" title="DynamicFace: High-Quality and Consistent Face Swapping for Image and   Video using Composable 3D Facial Priors"></a>DynamicFace: High-Quality and Consistent Face Swapping for Image and   Video using Composable 3D Facial Priors</h2><p><strong>Authors:Runqi Wang, Yang Chen, Sijie Xu, Tianyao He, Wei Zhu, Dejia Song, Nemo Chen, Xu Tang, Yao Hu</strong></p>
<p>Face swapping transfers the identity of a source face to a target face while retaining the attributes like expression, pose, hair, and background of the target face. Advanced face swapping methods have achieved attractive results. However, these methods often inadvertently transfer identity information from the target face, compromising expression-related details and accurate identity. We propose a novel method DynamicFace that leverages the power of diffusion models and plug-and-play adaptive attention layers for image and video face swapping. First, we introduce four fine-grained facial conditions using 3D facial priors. All conditions are designed to be disentangled from each other for precise and unique control. Then, we adopt Face Former and ReferenceNet for high-level and detailed identity injection. Through experiments on the FF++ dataset, we demonstrate that our method achieves state-of-the-art results in face swapping, showcasing superior image quality, identity preservation, and expression accuracy. Our framework seamlessly adapts to both image and video domains. Our code and results will be available on the project page: <a target="_blank" rel="noopener" href="https://dynamic-face.github.io/">https://dynamic-face.github.io/</a> </p>
<blockquote>
<p>é¢éƒ¨æ›¿æ¢æŠ€æœ¯å°†æºé¢éƒ¨çš„èº«ä»½è½¬ç§»åˆ°ç›®æ ‡é¢éƒ¨ï¼ŒåŒæ—¶ä¿ç•™ç›®æ ‡é¢éƒ¨çš„è¡¨æƒ…ã€å§¿æ€ã€å‘å‹å’ŒèƒŒæ™¯ç­‰å±æ€§ã€‚å…ˆè¿›çš„é¢éƒ¨æ›¿æ¢æ–¹æ³•å·²ç»å–å¾—äº†å¸å¼•äººçš„ç»“æœã€‚ç„¶è€Œï¼Œè¿™äº›æ–¹æ³•å¸¸å¸¸ä¼šåœ¨ä¸ç»æ„é—´ä»ç›®æ ‡é¢éƒ¨è½¬ç§»èº«ä»½ä¿¡æ¯ï¼Œä»è€ŒæŸå¤±è¡¨æƒ…ç›¸å…³çš„ç»†èŠ‚å’Œå‡†ç¡®çš„èº«ä»½ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„æ–¹æ³•DynamicFaceï¼Œå®ƒåˆ©ç”¨æ‰©æ•£æ¨¡å‹çš„å¼ºå¤§åŠŸèƒ½å’Œå³æ’å³ç”¨çš„è‡ªé€‚åº”æ³¨æ„åŠ›å±‚è¿›è¡Œå›¾åƒå’Œè§†é¢‘é¢éƒ¨æ›¿æ¢ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬å¼•å…¥å››ç§ç²¾ç»†çš„é¢éƒ¨æ¡ä»¶ï¼Œä½¿ç”¨3Dé¢éƒ¨å…ˆéªŒã€‚æ‰€æœ‰æ¡ä»¶éƒ½è¢«è®¾è®¡æˆç›¸äº’ç‹¬ç«‹ï¼Œä»¥å®ç°ç²¾ç¡®å’Œç‹¬ç‰¹çš„æ§åˆ¶ã€‚ç„¶åï¼Œæˆ‘ä»¬é‡‡ç”¨Face Formerå’ŒReferenceNetè¿›è¡Œé«˜çº§å’Œè¯¦ç»†çš„èº«ä»½æ³¨å…¥ã€‚é€šè¿‡åœ¨FF++æ•°æ®é›†ä¸Šçš„å®éªŒï¼Œæˆ‘ä»¬è¯æ˜æˆ‘ä»¬çš„æ–¹æ³•åœ¨é¢éƒ¨æ›¿æ¢æ–¹é¢è¾¾åˆ°äº†æœ€æ–°æ°´å¹³ï¼Œå±•ç¤ºäº†å“è¶Šçš„å›¾ç‰‡è´¨é‡ã€èº«ä»½ä¿ç•™å’Œè¡¨æƒ…å‡†ç¡®æ€§ã€‚æˆ‘ä»¬çš„æ¡†æ¶å¯ä»¥æ— ç¼é€‚åº”å›¾åƒå’Œè§†é¢‘é¢†åŸŸã€‚æˆ‘ä»¬çš„ä»£ç å’Œç»“æœå°†åœ¨é¡¹ç›®é¡µé¢ä¸Šè¿›è¡Œå…¬å¼€ï¼š<a target="_blank" rel="noopener" href="https://dynamic-face.github.io/">https://dynamic-face.github.io/</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.08553v2">PDF</a> Accepted by ICCV 2025. Project page: <a target="_blank" rel="noopener" href="https://dynamic-face.github.io/">https://dynamic-face.github.io/</a></p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åŸºäºæ‰©æ•£æ¨¡å‹çš„æ–°å‹åŠ¨æ€é¢éƒ¨æ›¿æ¢æ–¹æ³•ï¼Œè¯¥æ–¹æ³•åˆ©ç”¨ç²¾ç»†é¢éƒ¨æ¡ä»¶å’Œè‡ªé€‚åº”æ³¨æ„åŠ›å±‚ï¼Œåœ¨å›¾åƒå’Œè§†é¢‘é¢éƒ¨æ›¿æ¢ä¸­å®ç°äº†å‡ºè‰²çš„è¡¨ç°ã€‚é€šè¿‡å®éªŒç»“æœå±•ç¤ºï¼Œè¯¥æ–¹æ³•åœ¨é¢éƒ¨æ›¿æ¢é¢†åŸŸè¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ°´å¹³ï¼Œå…·æœ‰é«˜è´¨é‡å›¾åƒã€èº«ä»½ä¿ç•™å’Œç²¾ç¡®è¡¨è¾¾ç­‰ç‰¹ç‚¹ã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ³•è¿˜å¯ä»¥è½»æ¾é€‚åº”å›¾åƒå’Œè§†é¢‘é¢†åŸŸã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ–‡ä¸­æå‡ºäº†ä¸€ç§æ–°çš„åŠ¨æ€é¢éƒ¨æ›¿æ¢æ–¹æ³•ï¼Œåä¸ºDynamicFaceï¼Œè¯¥æ–¹æ³•ç»“åˆäº†æ‰©æ•£æ¨¡å‹å’Œè‡ªé€‚åº”æ³¨æ„åŠ›å±‚æŠ€æœ¯ã€‚</li>
<li>è¯¥æ–¹æ³•å¼•å…¥äº†å››ç§ç²¾ç»†é¢éƒ¨æ¡ä»¶ï¼Œåˆ©ç”¨3Dé¢éƒ¨å…ˆéªŒè¿›è¡Œè®¾è®¡ï¼Œä»¥å®ç°ç²¾ç¡®ä¸”ç‹¬ç‰¹çš„æ§åˆ¶ã€‚</li>
<li>é‡‡ç”¨Face Formerå’ŒReferenceNetè¿›è¡Œé«˜çº§å’Œè¯¦ç»†çš„èº«ä»½æ³¨å…¥ï¼Œä»¥æé«˜é¢éƒ¨æ›¿æ¢çš„è´¨é‡å’Œå‡†ç¡®æ€§ã€‚</li>
<li>åœ¨FF++æ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨é¢éƒ¨æ›¿æ¢é¢†åŸŸè¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ°´å¹³ï¼Œå…·æœ‰ä¼˜ç§€çš„å›¾åƒè´¨é‡ã€èº«ä»½ä¿ç•™å’Œè¡¨è¾¾å‡†ç¡®æ€§ã€‚</li>
<li>DynamicFaceæ¡†æ¶å¯ä»¥æ— ç¼é€‚åº”å›¾åƒå’Œè§†é¢‘é¢†åŸŸï¼Œä¸ºå¤šåª’ä½“å†…å®¹åˆ›ä½œæä¾›æ›´å¤§çš„çµæ´»æ€§ã€‚</li>
<li>è¯¥æ–¹æ³•é€šè¿‡åˆ©ç”¨æ‰©æ•£æ¨¡å‹çš„å¼ºå¤§èƒ½åŠ›ï¼Œæœ‰æ•ˆè§£å†³äº†ä¼ ç»Ÿé¢éƒ¨æ›¿æ¢æ–¹æ³•ä¸­èº«ä»½ä¿¡æ¯è½¬ç§»çš„é—®é¢˜ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.08553">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-531c47639659ab4111ad094d1c99e587.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a34ecef536e3b48c851ec619cf9e10a3.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-2cc126f2616e5cccc723906442df29ae.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a44a5c25559f0a39254dc399cd60cb04.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-56f26235eac682dc0524136f9fb9bee5.jpg" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1><h2 id="Mask-Approximation-Net-A-Novel-Diffusion-Model-Approach-for-Remote-Sensing-Change-Captioning"><a href="#Mask-Approximation-Net-A-Novel-Diffusion-Model-Approach-for-Remote-Sensing-Change-Captioning" class="headerlink" title="Mask Approximation Net: A Novel Diffusion Model Approach for Remote   Sensing Change Captioning"></a>Mask Approximation Net: A Novel Diffusion Model Approach for Remote   Sensing Change Captioning</h2><p><strong>Authors:Dongwei Sun, Jing Yao, Wu Xue, Changsheng Zhou, Pedram Ghamisi, Xiangyong Cao</strong></p>
<p>Remote sensing image change description represents an innovative multimodal task within the realm of remote sensing processing.This task not only facilitates the detection of alterations in surface conditions, but also provides comprehensive descriptions of these changes, thereby improving human interpretability and interactivity.Current deep learning methods typically adopt a three stage framework consisting of feature extraction, feature fusion, and change localization, followed by text generation. Most approaches focus heavily on designing complex network modules but lack solid theoretical guidance, relying instead on extensive empirical experimentation and iterative tuning of network components. This experience-driven design paradigm may lead to overfitting and design bottlenecks, thereby limiting the modelâ€™s generalizability and adaptability.To address these limitations, this paper proposes a paradigm that shift towards data distribution learning using diffusion models, reinforced by frequency-domain noise filtering, to provide a theoretically motivated and practically effective solution to multimodal remote sensing change description.The proposed method primarily includes a simple multi-scale change detection module, whose output features are subsequently refined by a well-designed diffusion model.Furthermore, we introduce a frequency-guided complex filter module to boost the model performance by managing high-frequency noise throughout the diffusion process. We validate the effectiveness of our proposed method across several datasets for remote sensing change detection and description, showcasing its superior performance compared to existing techniques. The code will be available at \href{<a target="_blank" rel="noopener" href="https://github.com/sundongwei%7D%7BMaskApproxNet%7D">https://github.com/sundongwei}{MaskApproxNet}</a>. </p>
<blockquote>
<p>é¥æ„Ÿå›¾åƒå˜åŒ–æè¿°æ˜¯é¥æ„Ÿå¤„ç†é¢†åŸŸçš„ä¸€é¡¹åˆ›æ–°çš„å¤šæ¨¡å¼ä»»åŠ¡ã€‚è¿™é¡¹ä»»åŠ¡ä¸ä»…æœ‰åŠ©äºæ£€æµ‹åœ°è¡¨æ¡ä»¶çš„å˜åŒ–ï¼Œè€Œä¸”æä¾›äº†å¯¹è¿™äº›å˜åŒ–çš„å…¨é¢æè¿°ï¼Œä»è€Œæé«˜äº†äººç±»å¯è§£é‡Šæ€§å’Œäº¤äº’æ€§ã€‚å½“å‰çš„æ·±åº¦å­¦ä¹ æ–¹æ³•é€šå¸¸é‡‡ç”¨ä¸€ä¸ªä¸‰é˜¶æ®µçš„æ¡†æ¶ï¼ŒåŒ…æ‹¬ç‰¹å¾æå–ã€ç‰¹å¾èåˆå’Œå˜åŒ–å®šä½ï¼Œç„¶åæ˜¯æ–‡æœ¬ç”Ÿæˆã€‚å¤§å¤šæ•°æ–¹æ³•è¿‡äºæ³¨é‡è®¾è®¡å¤æ‚çš„ç½‘ç»œæ¨¡å—ï¼Œä½†ç¼ºä¹åšå®çš„ç†è®ºæŒ‡å¯¼ï¼Œè€Œæ˜¯ä¾èµ–äºå¤§é‡çš„ç»éªŒå®éªŒå’Œç½‘ç»œç»„ä»¶çš„è¿­ä»£è°ƒæ•´ã€‚è¿™ç§ç»éªŒé©±åŠ¨çš„è®¾è®¡èŒƒå¼å¯èƒ½å¯¼è‡´è¿‡åº¦æ‹Ÿåˆå’Œè®¾è®¡ç“¶é¢ˆï¼Œä»è€Œé™åˆ¶äº†æ¨¡å‹çš„é€šç”¨æ€§å’Œé€‚åº”æ€§ã€‚ä¸ºäº†è§£å†³è¿™äº›å±€é™æ€§ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºæ‰©æ•£æ¨¡å‹çš„æ•°æ®åˆ†å¸ƒå­¦ä¹ èŒƒå¼ï¼Œè¾…ä»¥é¢‘åŸŸå™ªå£°æ»¤æ³¢ï¼Œä¸ºå¤šæ¨¡å¼é¥æ„Ÿå˜åŒ–æè¿°æä¾›ç†è®ºé©±åŠ¨å’Œå®è·µæœ‰æ•ˆçš„è§£å†³æ–¹æ¡ˆã€‚è¯¥æ–¹æ³•ä¸»è¦åŒ…æ‹¬ä¸€ä¸ªç®€å•çš„å¤šå°ºåº¦å˜åŒ–æ£€æµ‹æ¨¡å—ï¼Œå…¶è¾“å‡ºç‰¹å¾éšåç”±ä¸€ä¸ªè®¾è®¡ç²¾è‰¯çš„æ‰©æ•£æ¨¡å‹è¿›è¡Œç»†åŒ–ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ªé¢‘ç‡å¼•å¯¼å¤æ‚æ»¤æ³¢å™¨æ¨¡å—ï¼Œé€šè¿‡ç®¡ç†æ‰©æ•£è¿‡ç¨‹ä¸­çš„é«˜é¢‘å™ªå£°æ¥æé«˜æ¨¡å‹æ€§èƒ½ã€‚æˆ‘ä»¬åœ¨å¤šä¸ªé¥æ„Ÿå˜åŒ–æ£€æµ‹å’Œæè¿°çš„æ•°æ®é›†ä¸ŠéªŒè¯äº†æ‰€æå‡ºæ–¹æ³•çš„æœ‰æ•ˆæ€§ï¼Œå±•ç¤ºäº†å…¶ç›¸è¾ƒäºç°æœ‰æŠ€æœ¯çš„å“è¶Šæ€§èƒ½ã€‚ä»£ç å°†åœ¨MaskApproxNetï¼ˆ<a target="_blank" rel="noopener" href="https://github.com/sundongwei%EF%BC%89%E4%B8%8A%E6%8F%90%E4%BE%9B%E3%80%82">https://github.com/sundongweiï¼‰ä¸Šæä¾›ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.19179v3">PDF</a> </p>
<p><strong>Summary</strong><br>è¿œç¨‹é¥æ„Ÿå›¾åƒå˜åŒ–æè¿°æ˜¯ä¸€é¡¹æ–°é¢–çš„è·¨æ¨¡æ€ä»»åŠ¡ã€‚ä¼ ç»Ÿæ·±åº¦å­¦ä¹ æ–¹æ³•é€šå¸¸é‡‡ç”¨ä¸‰é˜¶æ®µæ¡†æ¶ï¼ŒåŒ…æ‹¬ç‰¹å¾æå–ã€ç‰¹å¾èåˆå’Œå˜åŒ–å®šä½ï¼Œç„¶åè¿›è¡Œæ–‡æœ¬ç”Ÿæˆã€‚ç„¶è€Œï¼Œè¿™äº›æ–¹æ³•ä¸»è¦ä¾èµ–äºç»éªŒå®éªŒå’Œç»„ä»¶è¿­ä»£è°ƒæ•´ï¼Œç¼ºä¹åšå®çš„ç†è®ºåŸºç¡€ã€‚ä¸ºè§£å†³è¿™äº›é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†åŸºäºæ‰©æ•£æ¨¡å‹çš„æ•°æ®åˆ†å¸ƒå­¦ä¹ èŒƒå¼ï¼Œå¹¶ç»“åˆé¢‘åŸŸå™ªå£°æ»¤æ³¢ï¼Œä¸ºè·¨æ¨¡æ€é¥æ„Ÿå˜åŒ–æè¿°æä¾›äº†ç†è®ºå’Œå®è·µä¸Šçš„è§£å†³æ–¹æ¡ˆã€‚è¯¥æ–¹æ³•åŒ…æ‹¬ä¸€ä¸ªç®€å•å¤šå°ºåº¦å˜åŒ–æ£€æµ‹æ¨¡å—å’Œä¸€ä¸ªç²¾å¿ƒè®¾è®¡æ‰©æ•£æ¨¡å‹ï¼Œé€šè¿‡é¢‘ç‡å¼•å¯¼å¤æ‚æ»¤æ³¢å™¨æ¨¡å—æå‡æ€§èƒ½ã€‚æœ¬æ–‡çš„æ–¹æ³•åœ¨å¤šæ•°æ®é›†ä¸Šè¿›è¡Œäº†éªŒè¯ï¼Œå±•ç°å‡ºå¯¹ç°æœ‰çš„ä¼˜è¶Šæ€§ã€‚ç›¸å…³ä»£ç å¯é€šè¿‡é“¾æ¥æŸ¥çœ‹ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è¿œç¨‹é¥æ„Ÿå›¾åƒå˜åŒ–æè¿°æ˜¯ä¸€é¡¹é‡è¦çš„è·¨æ¨¡æ€ä»»åŠ¡ï¼Œæœ‰åŠ©äºè¡¨é¢æ¡ä»¶å˜åŒ–çš„æ£€æµ‹å’Œæè¿°ã€‚</li>
<li>ä¼ ç»Ÿæ·±åº¦å­¦ä¹ æ–¹æ³•é‡‡ç”¨ä¸‰é˜¶æ®µæ¡†æ¶å¤„ç†è¿™ä¸ªä»»åŠ¡ï¼Œä½†å­˜åœ¨è¿‡åº¦ä¾èµ–ç»éªŒå®éªŒå’Œç»„ä»¶è°ƒæ•´çš„é—®é¢˜ã€‚</li>
<li>æœ¬æ–‡æå‡ºåŸºäºæ‰©æ•£æ¨¡å‹çš„è§£å†³æ–¹æ¡ˆï¼Œå¼ºè°ƒæ•°æ®åˆ†å¸ƒå­¦ä¹ ï¼Œä»¥æ”¹å–„æ¨¡å‹çš„æ³›åŒ–å’Œé€‚åº”æ€§ã€‚</li>
<li>æ–¹æ³•åŒ…æ‹¬ç®€å•å¤šå°ºåº¦å˜åŒ–æ£€æµ‹æ¨¡å—å’Œæ‰©æ•£æ¨¡å‹ï¼Œå…¶ä¸­æ‰©æ•£æ¨¡å‹ç»è¿‡ç²¾å¿ƒè®¾è®¡ä»¥ä¼˜åŒ–æ€§èƒ½ã€‚</li>
<li>ä¸ºæå‡æ€§èƒ½å¼•å…¥äº†é¢‘ç‡å¼•å¯¼å¤æ‚æ»¤æ³¢å™¨æ¨¡å—ï¼Œç®¡ç†æ‰©æ•£è¿‡ç¨‹ä¸­çš„é«˜é¢‘å™ªå£°ã€‚</li>
<li>åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šçš„éªŒè¯æ˜¾ç¤ºï¼Œæœ¬æ–‡æå‡ºçš„æ–¹æ³•åœ¨é¥æ„Ÿå˜åŒ–æ£€æµ‹å’Œæè¿°æ–¹é¢è¡¨ç°å‡ºå“è¶Šæ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.19179">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-136ede8ee68296dd9c73457d2c859186.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-84d28e92c516d3d6b9ebaaa1c6ce2d2d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ead758822872cb529da93ae638d0619b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3d5414a1c9f42ba332aa251f32a01daa.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5970c88f81dc2da909ef07400c432e92.jpg" align="middle">
</details>


<h1 id="-19"><a href="#-19" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-07-09/Diffusion%20Models/">https://kedreamix.github.io/Talk2Paper/Paper/2025-07-09/Diffusion%20Models/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Diffusion-Models/">
                                    <span class="chip bg-color">Diffusion Models</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-07-09/%E7%89%99%E9%BD%BF%E4%BF%AE%E5%A4%8D/">
                    <div class="card-image">
                        
                        <img src="https://pica.zhimg.com/v2-d86e7c9116b28dbc59a422d07f743fee.jpg" class="responsive-img" alt="ç‰™é½¿ä¿®å¤">
                        
                        <span class="card-title">ç‰™é½¿ä¿®å¤</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            ç‰™é½¿ä¿®å¤ æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-07-09  Geometric-Guided Few-Shot Dental Landmark Detection with Human-Centric   Foundation Model
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-07-09
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E7%89%99%E9%BD%BF%E4%BF%AE%E5%A4%8D/" class="post-category">
                                    ç‰™é½¿ä¿®å¤
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E7%89%99%E9%BD%BF%E4%BF%AE%E5%A4%8D/">
                        <span class="chip bg-color">ç‰™é½¿ä¿®å¤</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-07-09/NeRF/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-605934200d6c845cce73928ba3fecbfb.jpg" class="responsive-img" alt="NeRF">
                        
                        <span class="card-title">NeRF</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            NeRF æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-07-09  A View-consistent Sampling Method for Regularized Training of Neural   Radiance Fields
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-07-09
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/NeRF/" class="post-category">
                                    NeRF
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/NeRF/">
                        <span class="chip bg-color">NeRF</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">29058.5k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
