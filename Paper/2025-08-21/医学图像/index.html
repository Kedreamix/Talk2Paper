<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="åŒ»å­¦å›¾åƒ">
    <meta name="description" content="åŒ»å­¦å›¾åƒ æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-08-21  ASDFormer A Transformer with Mixtures of Pooling-Classifier Experts for   Robust Autism Diagnosis and Biomarker Discovery">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>åŒ»å­¦å›¾åƒ | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-e8eb254c468079debe400db19dd9614a.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">åŒ»å­¦å›¾åƒ</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                                <span class="chip bg-color">åŒ»å­¦å›¾åƒ</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/" class="post-category">
                                åŒ»å­¦å›¾åƒ
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-08-21
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-08-31
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    20.4k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    84 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-08-21-æ›´æ–°"><a href="#2025-08-21-æ›´æ–°" class="headerlink" title="2025-08-21 æ›´æ–°"></a>2025-08-21 æ›´æ–°</h1><h2 id="ASDFormer-A-Transformer-with-Mixtures-of-Pooling-Classifier-Experts-for-Robust-Autism-Diagnosis-and-Biomarker-Discovery"><a href="#ASDFormer-A-Transformer-with-Mixtures-of-Pooling-Classifier-Experts-for-Robust-Autism-Diagnosis-and-Biomarker-Discovery" class="headerlink" title="ASDFormer: A Transformer with Mixtures of Pooling-Classifier Experts for   Robust Autism Diagnosis and Biomarker Discovery"></a>ASDFormer: A Transformer with Mixtures of Pooling-Classifier Experts for   Robust Autism Diagnosis and Biomarker Discovery</h2><p><strong>Authors:Mohammad Izadi, Mehran Safayani</strong></p>
<p>Autism Spectrum Disorder (ASD) is a complex neurodevelopmental condition marked by disruptions in brain connectivity. Functional MRI (fMRI) offers a non-invasive window into large-scale neural dynamics by measuring blood-oxygen-level-dependent (BOLD) signals across the brain. These signals can be modeled as interactions among Regions of Interest (ROIs), which are grouped into functional communities based on their underlying roles in brain function. Emerging evidence suggests that connectivity patterns within and between these communities are particularly sensitive to ASD-related alterations. Effectively capturing these patterns and identifying interactions that deviate from typical development is essential for improving ASD diagnosis and enabling biomarker discovery. In this work, we introduce ASDFormer, a Transformer-based architecture that incorporates a Mixture of Pooling-Classifier Experts (MoE) to capture neural signatures associated with ASD. By integrating multiple specialized expert branches with attention mechanisms, ASDFormer adaptively emphasizes different brain regions and connectivity patterns relevant to autism. This enables both improved classification performance and more interpretable identification of disorder-related biomarkers. Applied to the ABIDE dataset, ASDFormer achieves state-of-the-art diagnostic accuracy and reveals robust insights into functional connectivity disruptions linked to ASD, highlighting its potential as a tool for biomarker discovery. </p>
<blockquote>
<p>è‡ªé—­ç—‡è°±ç³»éšœç¢ï¼ˆASDï¼‰æ˜¯ä¸€ç§å¤æ‚çš„ç¥ç»å‘è‚²æ€§ç–¾ç—…ï¼Œè¡¨ç°ä¸ºå¤§è„‘è¿æ¥æ€§çš„ä¸­æ–­ã€‚åŠŸèƒ½æ€§ç£å…±æŒ¯æˆåƒï¼ˆfMRIï¼‰é€šè¿‡æµ‹é‡å…¨è„‘çš„è¡€æ°§æ°´å¹³ä¾èµ–ï¼ˆBOLDï¼‰ä¿¡å·ï¼Œä¸ºéä¾µå…¥æ€§åœ°è§‚å¯Ÿå¤§è§„æ¨¡ç¥ç»åŠ¨æ€æä¾›äº†çª—å£ã€‚è¿™äº›ä¿¡å·å¯ä»¥å»ºæ¨¡ä¸ºæ„Ÿå…´è¶£åŒºåŸŸï¼ˆROIï¼‰ä¹‹é—´çš„ç›¸äº’ä½œç”¨ï¼Œè¿™äº›åŒºåŸŸæ ¹æ®å…¶åœ¨è„‘åŠŸèƒ½ä¸­çš„åŸºç¡€ä½œç”¨è¢«åˆ†ç»„ä¸ºåŠŸèƒ½ç¤¾åŒºã€‚è¶Šæ¥è¶Šå¤šçš„è¯æ®è¡¨æ˜ï¼Œè¿™äº›ç¤¾åŒºå†…éƒ¨å’Œä¹‹é—´çš„è¿æ¥æ¨¡å¼å¯¹ASDç›¸å…³æ”¹å˜ç‰¹åˆ«æ•æ„Ÿã€‚æœ‰æ•ˆåœ°æ•æ‰è¿™äº›æ¨¡å¼å¹¶è¯†åˆ«åç¦»å…¸å‹å‘å±•çš„ç›¸äº’ä½œç”¨å¯¹äºæé«˜ASDè¯Šæ–­å’Œæ²»ç–—ã€ä¿ƒè¿›ç”Ÿç‰©æ ‡å¿—ç‰©å‘ç°è‡³å…³é‡è¦ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬å¼•å…¥äº†ASDFormerï¼Œè¿™æ˜¯ä¸€ç§åŸºäºTransformerçš„æ¶æ„ï¼Œå®ƒç»“åˆäº†æ··åˆæ± åŒ–åˆ†ç±»ä¸“å®¶ï¼ˆMoEï¼‰æ¥æ•æ‰ä¸ASDç›¸å…³çš„ç¥ç»ç‰¹å¾ã€‚é€šè¿‡æ•´åˆå¤šä¸ªå…·æœ‰æ³¨æ„åŠ›æœºåˆ¶çš„ä¸“é—¨ä¸“å®¶åˆ†æ”¯ï¼ŒASDFormerè‡ªé€‚åº”åœ°å¼ºè°ƒä¸è‡ªé—­ç—‡ç›¸å…³çš„ä¸åŒå¤§è„‘åŒºåŸŸå’Œè¿æ¥æ¨¡å¼ã€‚è¿™æ—¢æé«˜äº†åˆ†ç±»æ€§èƒ½ï¼Œä¹Ÿæé«˜äº†è¯†åˆ«ä¸ç–¾ç—…ç›¸å…³çš„ç”Ÿç‰©æ ‡å¿—ç‰©çš„è§£é‡Šæ€§ã€‚åº”ç”¨äºABIDEæ•°æ®é›†ï¼ŒASDFormerè¾¾åˆ°äº†æœ€å…ˆè¿›çš„è¯Šæ–­å‡†ç¡®æ€§ï¼Œå¹¶æ­ç¤ºäº†ä¸ASDç›¸å…³çš„åŠŸèƒ½æ€§è¿æ¥ä¸­æ–­çš„ç¨³å¥è§è§£ï¼Œçªå‡ºäº†å…¶ä½œä¸ºç”Ÿç‰©æ ‡å¿—ç‰©å‘ç°å·¥å…·çš„æ½œåŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.14005v1">PDF</a> </p>
<p><strong>Summary</strong><br>     è‡ªé—­ç—‡è°±ç³»éšœç¢ï¼ˆASDï¼‰æ˜¯ä¸€ç§å¤æ‚çš„ç¥ç»å‘è‚²æ€§ç–¾ç—…ï¼Œè¡¨ç°ä¸ºå¤§è„‘è¿æ¥æ€§ä¸­æ–­ã€‚åŠŸèƒ½ç£å…±æŒ¯æˆåƒï¼ˆfMRIï¼‰é€šè¿‡æµ‹é‡å¤§è„‘ä¸­çš„è¡€æ°§æ°´å¹³ä¾èµ–ä¿¡å·ï¼ˆBOLDä¿¡å·ï¼‰æ¥è§‚å¯Ÿå¤§è§„æ¨¡ç¥ç»åŠ¨æ€ï¼Œä¸ºASDç›¸å…³çš„è„‘åŠŸèƒ½ç ”ç©¶æä¾›äº†éä¾µå…¥æ€§çª—å£ã€‚è¿™é¡¹ç ”ç©¶å¼•å…¥äº†ä¸€ç§åŸºäºTransformerçš„æ¶æ„â€”â€”ASDFormerï¼Œç»“åˆæ··åˆæ± åŒ–åˆ†ç±»ä¸“å®¶ï¼ˆMoEï¼‰æ¥æ•æ‰ä¸ASDç›¸å…³çš„ç¥ç»ç‰¹å¾ã€‚ASDFormeré€šè¿‡é›†æˆå¤šä¸ªä¸“ä¸šä¸“å®¶åˆ†æ”¯å’Œæ³¨æ„åŠ›æœºåˆ¶ï¼Œè‡ªé€‚åº”åœ°å¼ºè°ƒä¸è‡ªé—­ç—‡ç›¸å…³çš„ä¸åŒå¤§è„‘åŒºåŸŸå’Œè¿æ¥æ¨¡å¼ï¼Œä»è€Œæé«˜åˆ†ç±»æ€§èƒ½å¹¶æ›´å¯è§£é‡Šåœ°è¯†åˆ«ä¸ç–¾ç—…ç›¸å…³çš„ç”Ÿç‰©æ ‡å¿—ç‰©ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è‡ªé—­ç—‡è°±ç³»éšœç¢ï¼ˆASDï¼‰æ˜¯ä¸€ç§ç¥ç»å‘è‚²æ€§ç–¾ç—…ï¼Œæ¶‰åŠå¤§è„‘è¿æ¥æ€§çš„ä¸­æ–­ã€‚</li>
<li>åŠŸèƒ½ç£å…±æŒ¯æˆåƒï¼ˆfMRIï¼‰æ˜¯ç ”ç©¶ASDè„‘åŠŸèƒ½çš„é‡è¦éä¾µå…¥æ€§å·¥å…·ã€‚</li>
<li>ASDFormeræ˜¯ä¸€ä¸ªåŸºäºTransformerçš„æ¶æ„ï¼Œç”¨äºæ•æ‰ä¸ASDç›¸å…³çš„ç¥ç»ç‰¹å¾ã€‚</li>
<li>MoEï¼ˆæ··åˆæ± åŒ–åˆ†ç±»ä¸“å®¶ï¼‰åœ¨ASDFormerä¸­ç”¨äºå¢å¼ºåˆ†ç±»æ€§èƒ½å’Œè¯†åˆ«ç–¾ç—…ç›¸å…³ç”Ÿç‰©æ ‡å¿—ç‰©çš„å¯è§£é‡Šæ€§ã€‚</li>
<li>ASDFormeré€šè¿‡è‡ªé€‚åº”å¼ºè°ƒä¸è‡ªé—­ç—‡ç›¸å…³çš„ä¸åŒå¤§è„‘åŒºåŸŸå’Œè¿æ¥æ¨¡å¼æ¥æé«˜è¯Šæ–­å‡†ç¡®æ€§ã€‚</li>
<li>åœ¨ABIDEæ•°æ®é›†ä¸Šåº”ç”¨ASDFormerè¾¾åˆ°äº†å…ˆè¿›çš„è¯Šæ–­å‡†ç¡®æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.14005">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-6d9197f45e817ec2fc830903fc1b0a4a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0eb67f88e9a067c80aca5774a3beb47c.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="MMIS-Net-for-Retinal-Fluid-Segmentation-and-Detection"><a href="#MMIS-Net-for-Retinal-Fluid-Segmentation-and-Detection" class="headerlink" title="MMIS-Net for Retinal Fluid Segmentation and Detection"></a>MMIS-Net for Retinal Fluid Segmentation and Detection</h2><p><strong>Authors:Nchongmaje Ndipenocha, Alina Mirona, Kezhi Wanga, Yongmin Li</strong></p>
<p>Purpose: Deep learning methods have shown promising results in the segmentation, and detection of diseases in medical images. However, most methods are trained and tested on data from a single source, modality, organ, or disease type, overlooking the combined potential of other available annotated data. Numerous small annotated medical image datasets from various modalities, organs, and diseases are publicly available. In this work, we aim to leverage the synergistic potential of these datasets to improve performance on unseen data. Approach: To this end, we propose a novel algorithm called MMIS-Net (MultiModal Medical Image Segmentation Network), which features Similarity Fusion blocks that utilize supervision and pixel-wise similarity knowledge selection for feature map fusion. Additionally, to address inconsistent class definitions and label contradictions, we created a one-hot label space to handle classes absent in one dataset but annotated in another. MMIS-Net was trained on 10 datasets encompassing 19 organs across 2 modalities to build a single model. Results: The algorithm was evaluated on the RETOUCH grand challenge hidden test set, outperforming large foundation models for medical image segmentation and other state-of-the-art algorithms. We achieved the best mean Dice score of 0.83 and an absolute volume difference of 0.035 for the fluids segmentation task, as well as a perfect Area Under the Curve of 1 for the fluid detection task. Conclusion: The quantitative results highlight the effectiveness of our proposed model due to the incorporation of Similarity Fusion blocks into the networkâ€™s backbone for supervision and similarity knowledge selection, and the use of a one-hot label space to address label class inconsistencies and contradictions. </p>
<blockquote>
<p>ç›®çš„ï¼šæ·±åº¦å­¦ä¹ åœ¨åŒ»å­¦å›¾åƒçš„åˆ†å‰²å’Œç–¾ç—…æ£€æµ‹æ–¹é¢å·²ç»å–å¾—äº†æœ‰å‰æ™¯çš„ç»“æœã€‚ç„¶è€Œï¼Œå¤§å¤šæ•°æ–¹æ³•éƒ½æ˜¯åœ¨å•ä¸€æ¥æºã€æ¨¡æ€ã€å™¨å®˜æˆ–ç–¾ç—…ç±»å‹çš„æ•°æ®ä¸Šè¿›è¡Œè®­ç»ƒå’Œæµ‹è¯•çš„ï¼Œå¿½ç•¥äº†å…¶ä»–å¯ç”¨æ³¨é‡Šæ•°æ®çš„ç»¼åˆæ½œåŠ›ã€‚è®¸å¤šæ¥è‡ªä¸åŒæ¨¡æ€ã€å™¨å®˜å’Œç–¾ç—…çš„å°å‹æ³¨é‡ŠåŒ»å­¦å›¾åƒæ•°æ®é›†éƒ½æ˜¯å…¬å¼€å‘å¸ƒçš„ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬çš„ç›®æ ‡æ˜¯åˆ©ç”¨è¿™äº›æ•°æ®é›†çš„ååŒæ½œåŠ›æ¥æé«˜æœªè§æ•°æ®çš„æ€§èƒ½ã€‚æ–¹æ³•ï¼šä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°å‹ç®—æ³•MMIS-Netï¼ˆå¤šæ¨¡æ€åŒ»å­¦å›¾åƒåˆ†å‰²ç½‘ç»œï¼‰ï¼Œå®ƒé‡‡ç”¨Similarity Fusionå—ï¼Œåˆ©ç”¨ç›‘ç£å’Œåƒç´ çº§ç›¸ä¼¼æ€§çŸ¥è¯†é€‰æ‹©è¿›è¡Œç‰¹å¾å›¾èåˆã€‚æ­¤å¤–ï¼Œä¸ºäº†è§£å†³ç±»å®šä¹‰ä¸ä¸€è‡´å’Œæ ‡ç­¾çŸ›ç›¾çš„é—®é¢˜ï¼Œæˆ‘ä»¬åˆ›å»ºäº†ä¸€ä¸ªç‹¬çƒ­æ ‡ç­¾ç©ºé—´æ¥å¤„ç†åœ¨ä¸€ä¸ªæ•°æ®é›†ä¸­ç¼ºå¤±ä½†åœ¨å¦ä¸€ä¸ªæ•°æ®é›†ä¸­æœ‰æ³¨é‡Šçš„ç±»ã€‚MMIS-Netåœ¨æ¶µç›–2ç§æ¨¡æ€ã€æ¶‰åŠ19ä¸ªå™¨å®˜çš„10ä¸ªæ•°æ®é›†ä¸Šè¿›è¡Œè®­ç»ƒï¼Œä»¥æ„å»ºå•ä¸ªæ¨¡å‹ã€‚ç»“æœï¼šè¯¥ç®—æ³•åœ¨RETOUCHæŒ‘æˆ˜èµ›éšè—çš„æµ‹è¯•é›†ä¸Šè¿›è¡Œäº†è¯„ä¼°ï¼Œåœ¨åŒ»å­¦å›¾åƒåˆ†å‰²æ–¹é¢ä¼˜äºå¤§å‹åŸºç¡€æ¨¡å‹å’Œå…¶ä»–æœ€å…ˆè¿›çš„ç®—æ³•ã€‚åœ¨æµä½“åˆ†å‰²ä»»åŠ¡ä¸­ï¼Œæˆ‘ä»¬è·å¾—äº†æœ€ä½³çš„å¹³å‡Diceå¾—åˆ†0.83å’Œç»å¯¹ä½“ç§¯å·®å¼‚0.035ï¼›åœ¨æµä½“æ£€æµ‹ä»»åŠ¡ä¸­ï¼Œæˆ‘ä»¬è·å¾—äº†å®Œç¾çš„æ›²çº¿ä¸‹é¢ç§¯1ã€‚ç»“è®ºï¼šå®šé‡ç»“æœçªå‡ºäº†æˆ‘ä»¬æå‡ºçš„æ¨¡å‹çš„æœ‰æ•ˆæ€§ï¼Œè¿™æ˜¯ç”±äºå°†Similarity Fusionå—é›†æˆåˆ°ç½‘ç»œçš„ä¸»å¹²ä¸­è¿›è¡Œç›‘ç£å’Œç›¸ä¼¼æ€§çŸ¥è¯†é€‰æ‹©ï¼Œä»¥åŠä½¿ç”¨ç‹¬çƒ­æ ‡ç­¾ç©ºé—´æ¥è§£å†³æ ‡ç­¾ç±»çš„ä¸ä¸€è‡´æ€§å’ŒçŸ›ç›¾ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.13936v1">PDF</a> </p>
<p><strong>æ‘˜è¦</strong><br>æ·±åº¦å­¦ä¹ åœ¨åŒ»å­¦å›¾åƒåˆ†å‰²å’Œç–¾ç—…æ£€æµ‹æ–¹é¢å±•ç°å‡ºå·¨å¤§æ½œåŠ›ï¼Œä½†å¤§å¤šæ•°æ–¹æ³•ä»…é’ˆå¯¹å•ä¸€æ¥æºã€æ¨¡æ€ã€å™¨å®˜æˆ–ç–¾ç—…ç±»å‹çš„æ•°æ®è¿›è¡Œè®­ç»ƒå’Œæµ‹è¯•ï¼Œå¿½ç•¥äº†å…¶ä»–å¯ç”¨æ ‡æ³¨æ•°æ®çš„æ½œåŠ›ã€‚æœ¬ç ”ç©¶æ—¨åœ¨åˆ©ç”¨å¤šæ¨¡æ€åŒ»å­¦å›¾åƒæ•°æ®é›†çš„ååŒæ½œåŠ›æ¥æé«˜æœªè§æ•°æ®çš„æ€§èƒ½ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°å‹ç®—æ³•MMIS-Netï¼ˆå¤šæ¨¡æ€åŒ»å­¦å›¾åƒåˆ†å‰²ç½‘ç»œï¼‰ï¼Œé‡‡ç”¨ç›¸ä¼¼åº¦èåˆå—ï¼Œåˆ©ç”¨ç›‘ç£ä¿¡æ¯å’Œåƒç´ çº§ç›¸ä¼¼åº¦çŸ¥è¯†é€‰æ‹©è¿›è¡Œç‰¹å¾å›¾èåˆã€‚ä¸ºè§£å†³ç±»åˆ«å®šä¹‰ä¸ä¸€è‡´å’Œæ ‡ç­¾çŸ›ç›¾é—®é¢˜ï¼Œæˆ‘ä»¬åˆ›å»ºäº†ä¸€ä¸ªç‹¬çƒ­æ ‡ç­¾ç©ºé—´æ¥å¤„ç†æŸä¸€æ•°æ®é›†ç¼ºå¤±ä½†å¦ä¸€æ•°æ®é›†ä¸­å·²æ ‡æ³¨çš„ç±»åˆ«ã€‚MMIS-Netåœ¨æ¶µç›–ä¸¤ç§æ¨¡æ€ã€æ¶‰åŠ19ä¸ªå™¨å®˜çš„10ä¸ªæ•°æ®é›†ä¸Šè¿›è¡Œè®­ç»ƒï¼Œå»ºç«‹å•ä¸€æ¨¡å‹ã€‚ç®—æ³•åœ¨RETOUCHæŒ‘æˆ˜èµ›éšè—æµ‹è¯•é›†ä¸Šçš„è¡¨ç°ä¼˜äºåŒ»å­¦å›¾åƒåˆ†å‰²çš„å¤§å‹åŸºç¡€æ¨¡å‹å’Œå…¶ä»–å…ˆè¿›ç®—æ³•ã€‚åœ¨æµä½“åˆ†å‰²ä»»åŠ¡ä¸­ï¼Œæˆ‘ä»¬è·å¾—äº†æœ€ä½³çš„å¹³å‡Diceç³»æ•°0.83å’Œç»å¯¹ä½“ç§¯å·®å¼‚0.035ï¼Œä»¥åŠåœ¨æµä½“æ£€æµ‹ä»»åŠ¡ä¸­å®Œç¾çš„æ›²çº¿ä¸‹é¢ç§¯1ã€‚ç»“è®ºï¼šå®šé‡ç»“æœçªæ˜¾äº†æˆ‘ä»¬æå‡ºçš„æ¨¡å‹çš„æœ‰æ•ˆæ€§ï¼Œå¾—ç›Šäºç½‘ç»œä¸»å¹²ä¸­èå…¥ç›¸ä¼¼åº¦èåˆå—è¿›è¡Œç›‘ç£å’Œç›¸ä¼¼åº¦çŸ¥è¯†é€‰æ‹©ï¼Œä»¥åŠä½¿ç”¨ç‹¬çƒ­æ ‡ç­¾ç©ºé—´è§£å†³æ ‡ç­¾ç±»åˆ«ä¸ä¸€è‡´å’ŒçŸ›ç›¾é—®é¢˜ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>æ·±åº¦å­¦ä¹ åœ¨åŒ»å­¦å›¾åƒåˆ†å‰²å’Œç–¾ç—…æ£€æµ‹ä¸­å…·æœ‰å·¨å¤§æ½œåŠ›ï¼Œä½†éœ€è·¨å¤šæºæ•°æ®æå‡æ€§èƒ½ã€‚</li>
<li>æå‡ºMMIS-Netç®—æ³•ï¼Œèåˆå¤šæ¨¡æ€åŒ»å­¦å›¾åƒæ•°æ®ä»¥æé«˜æœªè§æ•°æ®æ€§èƒ½ã€‚</li>
<li>MMIS-Neté‡‡ç”¨ç›¸ä¼¼åº¦èåˆå—ï¼Œåˆ©ç”¨ç›‘ç£ä¿¡æ¯å’Œåƒç´ çº§ç›¸ä¼¼åº¦çŸ¥è¯†é€‰æ‹©è¿›è¡Œç‰¹å¾å›¾èåˆã€‚</li>
<li>ä¸ºè§£å†³ç±»åˆ«å®šä¹‰ä¸ä¸€è‡´å’Œæ ‡ç­¾çŸ›ç›¾é—®é¢˜ï¼Œåˆ›å»ºç‹¬çƒ­æ ‡ç­¾ç©ºé—´ã€‚</li>
<li>MMIS-Netåœ¨æ¶µç›–å¤šç§æ¨¡æ€ã€å™¨å®˜å’Œæ•°æ®é›†çš„å¹¿æ³›å®éªŒä¸Šè¡¨ç°å‡ºå“è¶Šæ€§èƒ½ã€‚</li>
<li>åœ¨RETOUCHæŒ‘æˆ˜èµ›éšè—æµ‹è¯•é›†ä¸Šï¼ŒMMIS-Netè¡¨ç°ä¼˜äºå…¶ä»–å…ˆè¿›ç®—æ³•ã€‚</li>
<li>å®šé‡ç»“æœçªæ˜¾MMIS-Netçš„æœ‰æ•ˆæ€§ï¼Œç‰¹åˆ«æ˜¯åœ¨æµä½“åˆ†å‰²å’Œæ£€æµ‹ä»»åŠ¡ä¸Šã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.13936">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-75822302c63592a249a21b8bea38f46d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1f6743aa0dbb9c2897a525ec5ffa84fa.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5c6cc453266e9ef3d24c12ca02d20361.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="SCRNet-Spatial-Channel-Regulation-Network-for-Medical-Ultrasound-Image-Segmentation"><a href="#SCRNet-Spatial-Channel-Regulation-Network-for-Medical-Ultrasound-Image-Segmentation" class="headerlink" title="SCRNet: Spatial-Channel Regulation Network for Medical Ultrasound Image   Segmentation"></a>SCRNet: Spatial-Channel Regulation Network for Medical Ultrasound Image   Segmentation</h2><p><strong>Authors:Weixin Xu, Ziliang Wang</strong></p>
<p>Medical ultrasound image segmentation presents a formidable challenge in the realm of computer vision. Traditional approaches rely on Convolutional Neural Networks (CNNs) and Transformer-based methods to address the intricacies of medical image segmentation. Nevertheless, inherent limitations persist, as CNN-based methods tend to disregard long-range dependencies, while Transformer-based methods may overlook local contextual information. To address these deficiencies, we propose a novel Feature Aggregation Module (FAM) designed to process two input features from the preceding layer. These features are seamlessly directed into two branches of the Convolution and Cross-Attention Parallel Module (CCAPM) to endow them with different roles in each of the two branches to help establish a strong connection between the two input features. This strategy enables our module to focus concurrently on both long-range dependencies and local contextual information by judiciously merging convolution operations with cross-attention mechanisms. Moreover, by integrating FAM within our proposed Spatial-Channel Regulation Module (SCRM), the ability to discern salient regions and informative features warranting increased attention is enhanced. Furthermore, by incorporating the SCRM into the encoder block of the UNet architecture, we introduce a novel framework dubbed Spatial-Channel Regulation Network (SCRNet). The results of our extensive experiments demonstrate the superiority of SCRNet, which consistently achieves state-of-the-art (SOTA) performance compared to existing methods. </p>
<blockquote>
<p>åŒ»å­¦è¶…å£°å›¾åƒåˆ†å‰²æ˜¯è®¡ç®—æœºè§†è§‰é¢†åŸŸçš„ä¸€ä¸ªå·¨å¤§æŒ‘æˆ˜ã€‚ä¼ ç»Ÿçš„æ–¹æ³•ä¾èµ–äºå·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNï¼‰å’ŒåŸºäºTransformerçš„æ–¹æ³•æ¥è§£å†³åŒ»å­¦å›¾åƒåˆ†å‰²çš„å¤æ‚æ€§ã€‚ç„¶è€Œï¼Œä»ç„¶å­˜åœ¨å›ºæœ‰çš„å±€é™æ€§ï¼Œå› ä¸ºåŸºäºCNNçš„æ–¹æ³•å¾€å¾€å¿½ç•¥äº†é•¿æœŸä¾èµ–å…³ç³»ï¼Œè€ŒåŸºäºTransformerçš„æ–¹æ³•å¯èƒ½ä¼šå¿½è§†å±€éƒ¨ä¸Šä¸‹æ–‡ä¿¡æ¯ã€‚ä¸ºäº†è§£å†³è¿™äº›ä¸è¶³ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°å‹çš„ç‰¹å¾èšåˆæ¨¡å—ï¼ˆFAMï¼‰ï¼Œç”¨äºå¤„ç†æ¥è‡ªå‰ä¸€å±‚çš„ä¸¤ä¸ªè¾“å…¥ç‰¹å¾ã€‚è¿™äº›ç‰¹å¾è¢«æ— ç¼åœ°å¼•å¯¼åˆ°å·ç§¯å’Œäº¤å‰æ³¨æ„å¹¶è¡Œæ¨¡å—ï¼ˆCCAPMï¼‰çš„ä¸¤ä¸ªåˆ†æ”¯ä¸­ï¼Œåœ¨æ¯ä¸ªåˆ†æ”¯ä¸­èµ‹äºˆå®ƒä»¬ä¸åŒçš„è§’è‰²ï¼Œæœ‰åŠ©äºå»ºç«‹ä¸¤ä¸ªè¾“å…¥ç‰¹å¾ä¹‹é—´çš„å¼ºè¿æ¥ã€‚è¿™ä¸€ç­–ç•¥ä½¿æˆ‘ä»¬çš„æ¨¡å—èƒ½å¤Ÿå¹¶ä¸“æ³¨äºé•¿æœŸä¾èµ–å…³ç³»å’Œå±€éƒ¨ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼Œé€šè¿‡å°†å·ç§¯æ“ä½œä¸äº¤å‰æ³¨æ„æœºåˆ¶è¿›è¡Œæ˜æ™ºçš„èåˆã€‚æ­¤å¤–ï¼Œé€šè¿‡å°†FAMé›†æˆåœ¨æˆ‘ä»¬æå‡ºçš„ç©ºé—´é€šé“è°ƒèŠ‚æ¨¡å—ï¼ˆSCRMï¼‰ä¸­ï¼Œå¢å¼ºäº†è¯†åˆ«å€¼å¾—æ›´å¤šå…³æ³¨çš„æ˜¾è‘—åŒºåŸŸå’Œç‰¹å¾çš„èƒ½åŠ›ã€‚æ­¤å¤–ï¼Œé€šè¿‡å°†SCRMèå…¥UNetæ¶æ„çš„ç¼–ç å™¨å—ä¸­ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ªåä¸ºç©ºé—´é€šé“è°ƒèŠ‚ç½‘ç»œï¼ˆSCRNetï¼‰çš„æ–°å‹æ¡†æ¶ã€‚æˆ‘ä»¬çš„å¹¿æ³›å®éªŒç»“æœè¡¨æ˜ï¼ŒSCRNetå…·æœ‰å“è¶Šçš„æ€§èƒ½ï¼Œä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼Œå§‹ç»ˆè¾¾åˆ°æœ€æ–°æ°´å¹³ï¼ˆSOTAï¼‰ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.13899v1">PDF</a> 8 pagegs</p>
<p><strong>Summary</strong><br>     åŒ»å­¦è¶…å£°å›¾åƒåˆ†å‰²æ˜¯è®¡ç®—æœºè§†è§‰é¢†åŸŸçš„ä¸€å¤§æŒ‘æˆ˜ã€‚ä¼ ç»Ÿæ–¹æ³•ä¸»è¦ä¾èµ–å·ç§¯ç¥ç»ç½‘ç»œå’ŒåŸºäºTransformerçš„æ–¹æ³•ï¼Œä½†ä»å­˜åœ¨CNNå¿½ç•¥é•¿è·ç¦»ä¾èµ–å’ŒTransformerå¿½ç•¥å±€éƒ¨ä¸Šä¸‹æ–‡ä¿¡æ¯çš„å±€é™æ€§ã€‚ä¸ºæ­¤ï¼Œæå‡ºä¸€ç§æ–°å‹ç‰¹å¾èšåˆæ¨¡å—ï¼ˆFAMï¼‰ï¼Œè¯¥æ¨¡å—èƒ½å¤Ÿå¤„ç†æ¥è‡ªå‰ä¸€å±‚çš„ä¸¤ä¸ªè¾“å…¥ç‰¹å¾ï¼Œå¹¶é€šè¿‡å·ç§¯å’Œäº¤å‰æ³¨æ„åŠ›å¹¶è¡Œæ¨¡å—ï¼ˆCCAPMï¼‰çš„ä¸¤ä¸ªåˆ†æ”¯èµ‹äºˆå®ƒä»¬ä¸åŒçš„è§’è‰²ï¼Œä»è€Œå»ºç«‹ä¸¤ä¸ªè¾“å…¥ç‰¹å¾ä¹‹é—´çš„å¼ºè¿æ¥ã€‚æ­¤å¤–ï¼Œé€šè¿‡å°†FAMä¸ç©ºé—´é€šé“è°ƒèŠ‚æ¨¡å—ï¼ˆSCRMï¼‰é›†æˆï¼Œæé«˜äº†è¾¨åˆ«é‡è¦åŒºåŸŸå’Œéœ€è¦æ›´å¤šå…³æ³¨çš„ä¿¡æ¯ç‰¹å¾çš„èƒ½åŠ›ã€‚å†å°†SCRMå¼•å…¥UNetæ¶æ„çš„ç¼–ç å™¨å—ä¸­ï¼Œæå‡ºä¸€ç§åä¸ºç©ºé—´é€šé“è°ƒèŠ‚ç½‘ç»œï¼ˆSCRNetï¼‰çš„æ–°å‹æ¡†æ¶ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒSCRNetæ€§èƒ½å“è¶Šï¼Œä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”æŒç»­è¾¾åˆ°æœ€ä½³æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<pre><code> 1. åŒ»å­¦è¶…å£°å›¾åƒåˆ†å‰²æ˜¯è®¡ç®—æœºè§†è§‰é¢†åŸŸçš„æŒ‘æˆ˜ã€‚
 2. ä¼ ç»Ÿæ–¹æ³•ä¾èµ–CNNå’ŒTransformerï¼Œä½†å­˜åœ¨å±€é™æ€§ã€‚
 3. æ–°å‹ç‰¹å¾èšåˆæ¨¡å—ï¼ˆFAMï¼‰èƒ½å¤„ç†ä¸¤ä¸ªè¾“å…¥ç‰¹å¾ï¼Œå»ºç«‹å¼ºè¿æ¥ã€‚
 4. FAMç»“åˆå·ç§¯å’Œäº¤å‰æ³¨æ„åŠ›æœºåˆ¶ï¼ŒåŒæ—¶å…³æ³¨é•¿è·ç¦»ä¾èµ–å’Œå±€éƒ¨ä¸Šä¸‹æ–‡ä¿¡æ¯ã€‚
 5. ç©ºé—´é€šé“è°ƒèŠ‚æ¨¡å—ï¼ˆSCRMï¼‰å¢å¼ºè¾¨åˆ«é‡è¦åŒºåŸŸå’Œä¿¡æ¯ç‰¹å¾çš„èƒ½åŠ›ã€‚
 6. å°†SCRMå¼•å…¥UNetæ¶æ„ï¼Œå½¢æˆæ–°å‹æ¡†æ¶SCRNetã€‚
</code></pre>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.13899">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-4290ac8fc6ebe843aa45217bf551a50f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-590bd4a53ec951cd840a65b34619cffe.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-89d03d66070c2feb746ce88691b55d92.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d76f3a8500710869608c5de26d60ba13.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="In-hoc-Concept-Representations-to-Regularise-Deep-Learning-in-Medical-Imaging"><a href="#In-hoc-Concept-Representations-to-Regularise-Deep-Learning-in-Medical-Imaging" class="headerlink" title="In-hoc Concept Representations to Regularise Deep Learning in Medical   Imaging"></a>In-hoc Concept Representations to Regularise Deep Learning in Medical   Imaging</h2><p><strong>Authors:Valentina Corbetta, Floris Six Dijkstra, Regina Beets-Tan, Hoel Kervadec, Kristoffer WickstrÃ¸m, Wilson Silva</strong></p>
<p>Deep learning models in medical imaging often achieve strong in-distribution performance but struggle to generalise under distribution shifts, frequently relying on spurious correlations instead of clinically meaningful features. We introduce LCRReg, a novel regularisation approach that leverages Latent Concept Representations (LCRs) (e.g., Concept Activation Vectors (CAVs)) to guide models toward semantically grounded representations. LCRReg requires no concept labels in the main training set and instead uses a small auxiliary dataset to synthesise high-quality, disentangled concept examples. We extract LCRs for predefined relevant features, and incorporate a regularisation term that guides a Convolutional Neural Network (CNN) to activate within latent subspaces associated with those concepts. We evaluate LCRReg across synthetic and real-world medical tasks. On a controlled toy dataset, it significantly improves robustness to injected spurious correlations and remains effective even in multi-concept and multiclass settings. On the diabetic retinopathy binary classification task, LCRReg enhances performance under both synthetic spurious perturbations and out-of-distribution (OOD) generalisation. Compared to baselines, including multitask learning, linear probing, and post-hoc concept-based models, LCRReg offers a lightweight, architecture-agnostic strategy for improving model robustness without requiring dense concept supervision. Code is available at the following link: <a target="_blank" rel="noopener" href="https://github.com/Trustworthy-AI-UU-NKI/lcr/_regularization">https://github.com/Trustworthy-AI-UU-NKI/lcr\_regularization</a> </p>
<blockquote>
<p>åœ¨åŒ»å­¦æˆåƒé¢†åŸŸï¼Œæ·±åº¦å­¦ä¹ æ¨¡å‹å¾€å¾€èƒ½åœ¨å†…éƒ¨æ•°æ®åˆ†å¸ƒä¸Šè¡¨ç°å¼ºåŠ²ï¼Œä½†åœ¨æ•°æ®åˆ†å¸ƒå‘ç”Ÿå˜åŒ–æ—¶å´éš¾ä»¥æ¨å¹¿ï¼Œç»å¸¸ä¾èµ–äºå¶ç„¶çš„å…³è”è€Œéå…·æœ‰ä¸´åºŠæ„ä¹‰çš„ç‰¹å¾ã€‚æˆ‘ä»¬å¼•å…¥äº†LCRRegï¼Œè¿™æ˜¯ä¸€ç§æ–°å‹æ­£åˆ™åŒ–æ–¹æ³•ï¼Œå®ƒåˆ©ç”¨æ½œåœ¨æ¦‚å¿µè¡¨ç¤ºï¼ˆLCRsï¼‰ï¼ˆä¾‹å¦‚æ¦‚å¿µæ¿€æ´»å‘é‡ï¼ˆCAVsï¼‰ï¼‰æ¥å¼•å¯¼æ¨¡å‹èµ°å‘è¯­ä¹‰åŸºç¡€è¡¨ç¤ºã€‚LCRRegä¸éœ€è¦ä¸»è®­ç»ƒé›†ä¸­çš„æ¦‚å¿µæ ‡ç­¾ï¼Œè€Œæ˜¯ä½¿ç”¨ä¸€ä¸ªå°å‹è¾…åŠ©æ•°æ®é›†æ¥åˆæˆé«˜è´¨é‡ã€è§£çº ç¼ çš„æ¦‚å¿µç¤ºä¾‹ã€‚æˆ‘ä»¬ä¸ºé¢„å®šä¹‰çš„ç›¸å…³ç‰¹å¾æå–LCRsï¼Œå¹¶å¼•å…¥ä¸€ä¸ªæ­£åˆ™åŒ–é¡¹ï¼Œè¯¥æ­£åˆ™åŒ–é¡¹å¯ä»¥å¼•å¯¼å·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNï¼‰åœ¨ä¸è¿™äº›æ¦‚å¿µç›¸å…³çš„æ½œåœ¨å­ç©ºé—´å†…æ¿€æ´»ã€‚æˆ‘ä»¬åœ¨åˆæˆå’Œç°å®ä¸–ç•ŒåŒ»å­¦ä»»åŠ¡ä¸Šè¯„ä¼°äº†LCRRegã€‚åœ¨ä¸€ä¸ªå—æ§çš„ç©å…·æ•°æ®é›†ä¸Šï¼Œå®ƒæ˜¾è‘—æé«˜äº†å¯¹æ³¨å…¥çš„å¶ç„¶å…³è”çš„é²æ£’æ€§ï¼Œå³ä½¿åœ¨å¤šæ¦‚å¿µå’Œå¤šç±»åˆ«ç¯å¢ƒä¸­ä¹Ÿä¾ç„¶æœ‰æ•ˆã€‚åœ¨ç³–å°¿ç—…è§†ç½‘è†œç—…å˜äºŒåˆ†ç±»ä»»åŠ¡ä¸­ï¼ŒLCRRegåœ¨åˆæˆå¶ç„¶æ‰°åŠ¨å’Œåˆ†å¸ƒå¤–ï¼ˆOODï¼‰æ³›åŒ–çš„æƒ…å†µä¸‹æé«˜äº†æ€§èƒ½ã€‚ä¸å¤šä»»åŠ¡å­¦ä¹ ã€çº¿æ€§æ¢æµ‹å’Œåç»­æ¦‚å¿µæ¨¡å‹ç­‰åŸºçº¿ç›¸æ¯”ï¼ŒLCRRegæä¾›äº†ä¸€ç§è½»ä¾¿ã€æ¶æ„ä¸­ç«‹çš„ç­–ç•¥ï¼Œå¯åœ¨ä¸éœ€è¦å¯†é›†æ¦‚å¿µç›‘ç£çš„æƒ…å†µä¸‹æé«˜æ¨¡å‹çš„ç¨³å¥æ€§ã€‚ç›¸å…³ä»£ç å¯åœ¨ä»¥ä¸‹é“¾æ¥ä¸­æ‰¾åˆ°ï¼š<a target="_blank" rel="noopener" href="https://github.com/Trustworthy-AI-UU-NKI/lcr_regularization">https://github.com/Trustworthy-AI-UU-NKI/lcr_regularization</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.13880v1">PDF</a> 13 pages, 13 figures, 2 tables, accepted at PHAROS-AFE-AIMI Workshop   in conjunction with the International Conference on Computer Vision (ICCV),   2025. This is the submitted manuscript with added link to the github repo,   funding acknowledgments and author names and affiliations, and a correction   to numbers in Table 1. Final version not published yet</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸€ç§æ–°å‹çš„æ·±åº¦å­¦ä¹ æ¨¡å‹æ­£åˆ™åŒ–æ–¹æ³•LCRRegï¼Œè¯¥æ–¹æ³•åˆ©ç”¨æ½œåœ¨æ¦‚å¿µè¡¨ç¤ºï¼ˆLCRsï¼‰å¼•å¯¼æ¨¡å‹å­¦ä¹ è¯­ä¹‰åŒ–çš„ç‰¹å¾è¡¨ç¤ºï¼Œä»è€Œæé«˜åŒ»å­¦æˆåƒæ¨¡å‹çš„é€šç”¨æ€§å’Œç¨³å¥æ€§ã€‚LCRRegé€šè¿‡åˆ©ç”¨è¾…åŠ©æ•°æ®é›†åˆæˆé«˜è´¨é‡ã€è§£çº ç¼ çš„æ¦‚å¿µç¤ºä¾‹æ¥æå–é¢„å®šä¹‰çš„ç›¸å…³ç‰¹å¾ï¼Œå¹¶é€šè¿‡æ­£åˆ™åŒ–é¡¹å¼•å¯¼å·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNï¼‰åœ¨ç›¸å…³æ¦‚å¿µçš„æ½œåœ¨å­ç©ºé—´ä¸­è¿›è¡Œæ¿€æ´»ã€‚åœ¨åˆæˆå’ŒçœŸå®ä¸–ç•ŒåŒ»å­¦ä»»åŠ¡ä¸Šçš„è¯„ä¼°è¡¨æ˜ï¼ŒLCRRegåœ¨åº”å¯¹æ³¨å…¥çš„è™šå‡å…³è”å’Œè·¨æ¦‚å¿µã€è·¨ç±»åˆ«çš„è®¾ç½®ä¸‹ï¼Œéƒ½èƒ½æ˜¾è‘—æé«˜æ¨¡å‹çš„ç¨³å¥æ€§ã€‚æ­¤å¤–ï¼Œåœ¨ç³–å°¿ç—…è§†ç½‘è†œç—…å˜çš„äºŒåˆ†ç±»ä»»åŠ¡ä¸­ï¼ŒLCRRegåœ¨åº”å¯¹åˆæˆè™šå‡æ‰°åŠ¨å’Œè·¨åˆ†å¸ƒï¼ˆOODï¼‰æ³›åŒ–æ—¶ï¼Œä¹Ÿå¢å¼ºäº†æ¨¡å‹çš„æ€§èƒ½ã€‚ç›¸æ¯”äºå¤šä»»åŠ¡å­¦ä¹ ã€çº¿æ€§æ¢æµ‹å’ŒåéªŒæ¦‚å¿µæ¨¡å‹ç­‰åŸºçº¿æ–¹æ³•ï¼ŒLCRRegæä¾›äº†ä¸€ç§è½»ä¾¿ã€ç»“æ„æ— å…³çš„ç­–ç•¥ï¼Œå¯åœ¨æ— éœ€å¯†é›†æ¦‚å¿µç›‘ç£çš„æƒ…å†µä¸‹æé«˜æ¨¡å‹çš„ç¨³å¥æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LCRRegæ˜¯ä¸€ç§æ–°å‹çš„æ·±åº¦å­¦ä¹ æ¨¡å‹æ­£åˆ™åŒ–æ–¹æ³•ï¼Œæ—¨åœ¨æé«˜åŒ»å­¦æˆåƒæ¨¡å‹çš„é€šç”¨æ€§å’Œç¨³å¥æ€§ã€‚</li>
<li>LCRRegåˆ©ç”¨æ½œåœ¨æ¦‚å¿µè¡¨ç¤ºï¼ˆLCRsï¼‰æ¥å¼•å¯¼æ¨¡å‹å­¦ä¹ è¯­ä¹‰åŒ–çš„ç‰¹å¾è¡¨ç¤ºã€‚</li>
<li>LCRRegé€šè¿‡åˆ©ç”¨è¾…åŠ©æ•°æ®é›†åˆæˆé«˜è´¨é‡ã€è§£çº ç¼ çš„æ¦‚å¿µç¤ºä¾‹æ¥æå–é¢„å®šä¹‰çš„ç›¸å…³ç‰¹å¾ã€‚</li>
<li>LCRRegé€šè¿‡æ­£åˆ™åŒ–é¡¹å¼•å¯¼å·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNï¼‰åœ¨ç›¸å…³æ¦‚å¿µçš„æ½œåœ¨å­ç©ºé—´ä¸­è¿›è¡Œæ¿€æ´»ã€‚</li>
<li>åœ¨åˆæˆå’ŒçœŸå®ä¸–ç•ŒåŒ»å­¦ä»»åŠ¡ä¸Šçš„è¯„ä¼°è¡¨æ˜ï¼ŒLCRRegèƒ½æ˜¾è‘—æé«˜æ¨¡å‹çš„ç¨³å¥æ€§ï¼Œç‰¹åˆ«æ˜¯åœ¨å¤„ç†è™šå‡å…³è”å’Œè·¨ç±»åˆ«é—®é¢˜æ—¶ã€‚</li>
<li>åœ¨ç³–å°¿ç—…è§†ç½‘è†œç—…å˜åˆ†ç±»ä»»åŠ¡ä¸­ï¼ŒLCRRegå¢å¼ºäº†æ¨¡å‹çš„æ€§èƒ½ï¼Œç‰¹åˆ«æ˜¯åœ¨åº”å¯¹åˆæˆè™šå‡æ‰°åŠ¨å’Œè·¨åˆ†å¸ƒæ³›åŒ–æ—¶ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.13880">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-ef78aa17fdfed08dc218e5c00ae13303.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8664636ff625b2324af9ad94eab1a61f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-4aa9a340b6ff348d8d3e628f6323d3ee.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-2ec0aa873240e0c39d363bdef0be678c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-cb4562274047cae7ee10dbc4d9301fee.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Improving-Deep-Learning-for-Accelerated-MRI-With-Data-Filtering"><a href="#Improving-Deep-Learning-for-Accelerated-MRI-With-Data-Filtering" class="headerlink" title="Improving Deep Learning for Accelerated MRI With Data Filtering"></a>Improving Deep Learning for Accelerated MRI With Data Filtering</h2><p><strong>Authors:Kang Lin, Anselm Krainovic, Kun Wang, Reinhard Heckel</strong></p>
<p>Deep neural networks achieve state-of-the-art results for accelerated MRI reconstruction. Most research on deep learning based imaging focuses on improving neural network architectures trained and evaluated on fixed and homogeneous training and evaluation data. In this work, we investigate data curation strategies for improving MRI reconstruction. We assemble a large dataset of raw k-space data from 18 public sources consisting of 1.1M images and construct a diverse evaluation set comprising 48 test sets, capturing variations in anatomy, contrast, number of coils, and other key factors. We propose and study different data filtering strategies to enhance performance of current state-of-the-art neural networks for accelerated MRI reconstruction. Our experiments show that filtering the training data leads to consistent, albeit modest, performance gains. These performance gains are robust across different training set sizes and accelerations, and we find that filtering is particularly beneficial when the proportion of in-distribution data in the unfiltered training set is low. </p>
<blockquote>
<p>æ·±åº¦ç¥ç»ç½‘ç»œåœ¨åŠ é€ŸMRIé‡å»ºæ–¹é¢å–å¾—äº†æœ€å…ˆè¿›çš„æˆæœã€‚å¤§å¤šæ•°åŸºäºæ·±åº¦å­¦ä¹ çš„æˆåƒç ”ç©¶éƒ½é›†ä¸­åœ¨æ”¹è¿›åœ¨å›ºå®šå’Œå‡åŒ€çš„è®­ç»ƒå’Œè¯„ä¼°æ•°æ®ä¸Šè®­ç»ƒçš„ç¥ç»ç½‘ç»œæ¶æ„ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬ç ”ç©¶äº†æ•°æ®æ•´ç†ç­–ç•¥ä»¥æ”¹è¿›MRIé‡å»ºã€‚æˆ‘ä»¬ä»18ä¸ªå…¬å…±æºä¸­æå–åŸå§‹kç©ºé—´æ•°æ®ï¼Œæ„å»ºäº†ä¸€ä¸ªåŒ…å«çº¦è¾¾ä¸€åƒä¸‡å¼ å›¾åƒçš„å¤§å‹æ•°æ®é›†ï¼Œå¹¶å»ºç«‹ä¸€ä¸ªå¤šå…ƒåŒ–çš„è¯„ä¼°é›†ï¼ŒåŒ…æ‹¬æœ‰åŒ…å«è§£å‰–å­¦ã€å¯¹æ¯”åº¦ã€çº¿åœˆæ•°é‡å’Œå…¶ä»–å…³é”®å› ç´ çš„æµ‹è¯•é›†å…±è®¡æœ‰48å¥—ã€‚æˆ‘ä»¬æå‡ºå¹¶ç ”ç©¶äº†ä¸åŒçš„æ•°æ®è¿‡æ»¤ç­–ç•¥æ¥æé«˜å½“å‰æœ€å…ˆè¿›ç¥ç»ç½‘ç»œç”¨äºåŠ é€ŸMRIé‡å»ºçš„æ€§èƒ½ã€‚æˆ‘ä»¬çš„å®éªŒè¡¨æ˜ï¼Œè¿‡æ»¤è®­ç»ƒæ•°æ®å¯ä»¥å¸¦æ¥ä¸€è‡´ä½†å¹…åº¦é€‚ä¸­çš„æ€§èƒ½æå‡ã€‚è¿™äº›æ€§èƒ½æå‡åœ¨ä¸åŒå¤§å°è®­ç»ƒé›†å’Œä¸åŒåŠ é€Ÿæƒ…å†µä¸‹å‡è¡¨ç°ç¨³å¥ï¼Œæˆ‘ä»¬å‘ç°å½“æœªè¿‡æ»¤è®­ç»ƒé›†ä¸­åˆ†å¸ƒå†…æ•°æ®çš„æ¯”ä¾‹è¾ƒä½æ—¶ï¼Œè¿‡æ»¤ç‰¹åˆ«æœ‰ç›Šã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.13822v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ç ”ç©¶äº†åŸºäºæ·±åº¦å­¦ä¹ çš„MRIé‡å»ºä¸­çš„æ•°æ®ç­›é€‰ç­–ç•¥ã€‚é€šè¿‡æ”¶é›†å¤§é‡åŸå§‹kç©ºé—´æ•°æ®å¹¶æ„å»ºå¤šæ ·åŒ–çš„è¯„ä¼°é›†ï¼Œå®éªŒè¡¨æ˜å¯¹è®­ç»ƒæ•°æ®è¿›è¡Œç­›é€‰å¯ä»¥å¸¦æ¥ç¨³å¥çš„æ€§èƒ½æå‡ï¼Œç‰¹åˆ«æ˜¯åœ¨æœªç­›é€‰çš„è®­ç»ƒé›†ä¸­å†…éƒ¨æ•°æ®æ¯”ä¾‹è¾ƒä½æ—¶æ›´æ˜¯å¦‚æ­¤ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç ”ç©¶å…³æ³¨äºæ·±åº¦å­¦ä¹ åœ¨MRIé‡å»ºä¸­çš„æ•°æ®ç­›é€‰ç­–ç•¥ã€‚</li>
<li>æ”¶é›†äº†å¤§é‡åŸå§‹kç©ºé—´æ•°æ®å¹¶æ„å»ºäº†ä¸€ä¸ªå¤šæ ·åŒ–çš„è¯„ä¼°é›†ã€‚</li>
<li>é€šè¿‡å¯¹è®­ç»ƒæ•°æ®è¿›è¡Œç­›é€‰ï¼Œå¯ä»¥å¸¦æ¥ç¨³å¥çš„æ€§èƒ½æå‡ã€‚</li>
<li>æ€§èƒ½æå‡åœ¨ä¸åŒè®­ç»ƒé›†å¤§å°å’ŒåŠ é€Ÿæƒ…å†µä¸‹å‡å­˜åœ¨ã€‚</li>
<li>æ•°æ®ç­›é€‰åœ¨è®­ç»ƒé›†ä¸­å†…éƒ¨æ•°æ®æ¯”ä¾‹è¾ƒä½æ—¶ç‰¹åˆ«æœ‰ç›Šã€‚</li>
<li>æ­¤æ–¹æ³•å¯æé«˜MRIé‡å»ºçš„å‡†ç¡®æ€§å’Œé²æ£’æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.13822">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-fab8de021f60a3c803e0920c53ae9ae8.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-14343e96b6c55da8a3e03daa1099575e.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-8839379a680fb74e6968e2a373f708f7.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-c9d513a1d687b2b06530eb8f2cd131dc.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f56d30b759896dcc398511b8859a773c.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Direct-vascular-territory-segmentation-on-cerebral-digital-subtraction-angiography"><a href="#Direct-vascular-territory-segmentation-on-cerebral-digital-subtraction-angiography" class="headerlink" title="Direct vascular territory segmentation on cerebral digital subtraction   angiography"></a>Direct vascular territory segmentation on cerebral digital subtraction   angiography</h2><p><strong>Authors:P. Matthijs van der Sluijs, Lotte Strong, Frank G. te Nijenhuis, Sandra Cornelissen, Pieter Jan van Doormaal, Geert Lycklama a Nijeholt, Wim van Zwam, Ad van Es, Diederik Dippel, Aad van der Lugt, Danny Ruijters, Ruisheng Su, Theo van Walsum</strong></p>
<p>X-ray digital subtraction angiography (DSA) is frequently used when evaluating minimally invasive medical interventions. DSA predominantly visualizes vessels, and soft tissue anatomy is less visible or invisible in DSA. Visualization of cerebral anatomy could aid physicians during treatment. This study aimed to develop and evaluate a deep learning model to predict vascular territories that are not explicitly visible in DSA imaging acquired during ischemic stroke treatment. We trained an nnUNet model with manually segmented intracranial carotid artery and middle cerebral artery vessel territories on minimal intensity projection DSA acquired during ischemic stroke treatment. We compared the model to a traditional atlas registration model using the Dice similarity coefficient (DSC) and average surface distance (ASD). Additionally, we qualitatively assessed the success rate in both models using an external test. The segmentation model was trained on 1224 acquisitions from 361 patients with ischemic stroke. The segmentation model had a significantly higher DSC (0.96 vs 0.82, p&lt;0.001) and lower ASD compared to the atlas model (13.8 vs 47.3, p&lt;0.001). The success rate of the segmentation model (85%) was higher compared to the atlas registration model (66%) in the external test set. A deep learning method for the segmentation of vascular territories without explicit borders on cerebral DSA demonstrated superior accuracy and quality compared to the traditional atlas-based method. This approach has the potential to be applied to other anatomical structures for enhanced visualization during X-ray guided medical procedures. The code is publicly available at <a target="_blank" rel="noopener" href="https://github.com/RuishengSu/autoTICI">https://github.com/RuishengSu/autoTICI</a>. </p>
<blockquote>
<p>Xå°„çº¿æ•°å­—å‡å½±è¡€ç®¡é€ å½±æœ¯ï¼ˆDSAï¼‰åœ¨è¯„ä¼°å¾®åˆ›åŒ»å­¦å¹²é¢„æ—¶ç»å¸¸è¢«ä½¿ç”¨ã€‚DSAä¸»è¦å¯è§†åŒ–è¡€ç®¡ï¼Œè€Œè½¯ç»„ç»‡ç»“æ„åœ¨DSAä¸­å¯è§åº¦è¾ƒä½æˆ–ä¸å¯è§ã€‚å¯è§†åŒ–è„‘ç»“æ„æœ‰åŠ©äºåŒ»ç”Ÿåœ¨æ²»ç–—è¿‡ç¨‹ä¸­ã€‚æœ¬ç ”ç©¶æ—¨åœ¨å¼€å‘å¹¶è¯„ä¼°ä¸€ç§æ·±åº¦å­¦ä¹ æ¨¡å‹ï¼Œä»¥é¢„æµ‹åœ¨ç¼ºè¡€æ€§ä¸­é£æ²»ç–—æœŸé—´è·å¾—çš„DSAå›¾åƒä¸­æœªæ˜ç¡®å¯è§çš„è¡€ç®¡åŒºåŸŸã€‚æˆ‘ä»¬ä½¿ç”¨æ‰‹åŠ¨åˆ†å‰²çš„é¢…å†…é¢ˆåŠ¨è„‰å’Œå¤§è„‘ä¸­åŠ¨è„‰è¡€ç®¡åŒºåŸŸï¼Œè®­ç»ƒäº†ä¸€ä¸ªnnUNetæ¨¡å‹ï¼Œè¯¥æ¨¡å‹é€‚ç”¨äºç¼ºè¡€æ€§ä¸­é£æ²»ç–—æœŸé—´è·å¾—çš„æœ€ä½å¼ºåº¦æŠ•å½±DSAã€‚æˆ‘ä»¬å°†è¯¥æ¨¡å‹ä¸ä½¿ç”¨Diceç›¸ä¼¼ç³»æ•°ï¼ˆDSCï¼‰å’Œå¹³å‡è¡¨é¢è·ç¦»ï¼ˆASDï¼‰çš„ä¼ ç»Ÿå›¾è°±æ³¨å†Œæ¨¡å‹è¿›è¡Œäº†æ¯”è¾ƒã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜é€šè¿‡å¤–éƒ¨æµ‹è¯•å®šæ€§è¯„ä¼°äº†ä¸¤ç§æ¨¡å‹çš„æˆåŠŸç‡ã€‚åˆ†å‰²æ¨¡å‹æ˜¯åœ¨361åç¼ºè¡€æ€§ä¸­é£æ‚£è€…çš„1224æ¬¡é‡‡é›†æ•°æ®ä¸Šè¿›è¡Œè®­ç»ƒçš„ã€‚ä¸å›¾è°±æ¨¡å‹ç›¸æ¯”ï¼Œåˆ†å‰²æ¨¡å‹çš„DSCå€¼æ˜¾è‘—æ›´é«˜ï¼ˆ0.96æ¯”0.82ï¼Œp&lt;0.001ï¼‰ï¼ŒASDå€¼æ›´ä½ï¼ˆ13.8æ¯”47.3ï¼Œp&lt;0.001ï¼‰ã€‚åœ¨å¤–éƒ¨æµ‹è¯•é›†ä¸­ï¼Œåˆ†å‰²æ¨¡å‹çš„æˆåŠŸç‡ï¼ˆ85%ï¼‰é«˜äºå›¾è°±æ³¨å†Œæ¨¡å‹ï¼ˆ66%ï¼‰ã€‚ä¸€ç§ç”¨äºåœ¨è„‘DSAä¸Šåˆ†å‰²æ— æ˜ç¡®è¾¹ç•Œçš„è¡€ç®¡åŒºåŸŸçš„æ·±åº¦å­¦ä¹ æ–¹æ³•æ˜¾ç¤ºå‡ºæ¯”ä¼ ç»Ÿçš„åŸºäºå›¾è°±çš„æ–¹æ³•æ›´é«˜çš„å‡†ç¡®æ€§å’Œè´¨é‡ã€‚è¿™ç§æ–¹æ³•æœ‰å¯èƒ½åº”ç”¨äºå…¶ä»–è§£å‰–ç»“æ„ï¼Œä»¥åœ¨Xå°„çº¿å¼•å¯¼çš„æ‰‹æœ¯è¿‡ç¨‹ä¸­å®ç°æ›´å¥½çš„å¯è§†åŒ–ã€‚ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/RuishengSu/autoTICI%E4%B8%8A%E5%85%AC%E5%BC%80%E8%8E%B7%E5%BE%97%E3%80%82">https://github.com/RuishengSu/autoTICIä¸Šå…¬å¼€è·å–ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.13821v1">PDF</a> Accepted to SWITCH 2025</p>
<p><strong>æ‘˜è¦</strong><br>     æœ¬ç ”ç©¶æ—¨åœ¨å¼€å‘å¹¶è¯„ä¼°ä¸€ç§æ·±åº¦å­¦ä¹ æ¨¡å‹ï¼Œç”¨äºé¢„æµ‹åœ¨ç¼ºè¡€æ€§ä¸­é£æ²»ç–—ä¸­è·å¾—æ•°å­—å‡å½±è¡€ç®¡é€ å½±ï¼ˆDSAï¼‰ä¸­ä¸å¯è§çš„è¡€ç®¡åŒºåŸŸã€‚ç ”ç©¶ä½¿ç”¨nnUNetæ¨¡å‹å¯¹é¢…å†…é¢ˆåŠ¨è„‰å’Œå¤§è„‘ä¸­åŠ¨è„‰è¡€ç®¡åŒºåŸŸè¿›è¡Œæ‰‹åŠ¨åˆ†å‰²ï¼Œå¹¶ä¸ä¼ ç»Ÿå›¾è°±æ³¨å†Œæ¨¡å‹çš„Diceç›¸ä¼¼ç³»æ•°ï¼ˆDSCï¼‰å’Œå¹³å‡è¡¨é¢è·ç¦»ï¼ˆASDï¼‰è¿›è¡Œæ¯”è¾ƒã€‚ç»“æœæ˜¾ç¤ºï¼Œåˆ†å‰²æ¨¡å‹çš„DSCæ˜¾è‘—é«˜äºå›¾è°±æ¨¡å‹ï¼ˆ0.96 vs 0.82ï¼Œp&lt;0.001ï¼‰ï¼ŒASDä¹Ÿè¾ƒä½ï¼ˆ13.8 vs 47.3ï¼Œp&lt;0.001ï¼‰ã€‚å¤–éƒ¨æµ‹è¯•é›†ä¸­ï¼Œåˆ†å‰²æ¨¡å‹çš„æˆåŠŸç‡ï¼ˆ85%ï¼‰ä¹Ÿé«˜äºå›¾è°±æ³¨å†Œæ¨¡å‹ï¼ˆ66%ï¼‰ã€‚è¯¥ç ”ç©¶æå‡ºçš„æ·±åº¦å­¦ä¹ åˆ†å‰²è¡€ç®¡åŒºåŸŸæ–¹æ³•ç›¸æ¯”ä¼ ç»Ÿå›¾è°±æ–¹æ³•å…·æœ‰æ›´é«˜çš„å‡†ç¡®æ€§å’Œè´¨é‡ã€‚è¯¥æ–¹æ³•æœ‰æœ›åº”ç”¨äºå…¶ä»–è§£å‰–ç»“æ„ï¼Œä»¥æé«˜Xå…‰å¼•å¯¼æ‰‹æœ¯è¿‡ç¨‹ä¸­çš„å¯è§†åŒ–æ•ˆæœã€‚ç›¸å…³ä»£ç å·²å…¬å¼€å‘å¸ƒåœ¨[ç½‘å€é“¾æ¥]ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>ç ”ç©¶ä½¿ç”¨æ·±åº¦å­¦ä¹ æ¨¡å‹é¢„æµ‹DSAä¸­ä¸å¯è§çš„è¡€ç®¡åŒºåŸŸï¼Œä»¥æé«˜ç¼ºè¡€æ€§ä¸­é£æ²»ç–—ä¸­çš„å¯è§†åŒ–æ•ˆæœã€‚</li>
<li>å¯¹æ¯”äº†æ·±åº¦å­¦ä¹ æ¨¡å‹ä¸ä¼ ç»Ÿå›¾è°±æ³¨å†Œæ¨¡å‹ï¼Œæ˜¾ç¤ºæ·±åº¦å­¦ä¹ æ¨¡å‹åœ¨DSCå’ŒASDæ–¹é¢è¡¨ç°æ›´ä¼˜ã€‚</li>
<li>æ·±åº¦å­¦ä¹ æ¨¡å‹åœ¨å¤–éƒ¨æµ‹è¯•é›†ä¸­çš„æˆåŠŸç‡è¾ƒé«˜ã€‚</li>
<li>è¯¥æ–¹æ³•æœ‰æœ›åº”ç”¨äºå…¶ä»–è§£å‰–ç»“æ„çš„å¯è§†åŒ–ï¼Œæé«˜Xå…‰å¼•å¯¼æ‰‹æœ¯çš„æ•ˆæœã€‚</li>
<li>ç›¸å…³ä»£ç å·²å…¬å¼€å‘å¸ƒï¼Œä¾¿äºä»–äººä½¿ç”¨ä¸è¿›ä¸€æ­¥ç ”ç©¶ã€‚</li>
<li>æœ¬ç ”ç©¶ä¸ºåŒ»å­¦å›¾åƒåˆ†ææä¾›äº†æ–°æ€è·¯ï¼Œå¯èƒ½æ¨åŠ¨åŒ»å­¦å½±åƒæŠ€æœ¯çš„è¿›ä¸€æ­¥å‘å±•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.13821">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-5b7d1b384c432b28543f35b9c3ef6cb4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ab6cc613d0e4a060b7140702e110af47.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-223be598e29ecac44866d2645b502ff6.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-1196ba20e9e79da64a9d513f610ce44f.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Blast-Hole-Seeking-and-Dipping-â€“-The-Navigation-and-Perception-Framework-in-a-Mine-Site-Inspection-Robot"><a href="#Blast-Hole-Seeking-and-Dipping-â€“-The-Navigation-and-Perception-Framework-in-a-Mine-Site-Inspection-Robot" class="headerlink" title="Blast Hole Seeking and Dipping â€“ The Navigation and Perception   Framework in a Mine Site Inspection Robot"></a>Blast Hole Seeking and Dipping â€“ The Navigation and Perception   Framework in a Mine Site Inspection Robot</h2><p><strong>Authors:Liyang Liu, Ehsan Mihankhah, Nathan Wallace, Javier Martinez, Andrew J. Hill</strong></p>
<p>In open-pit mining, holes are drilled into the surface of the excavation site and detonated with explosives to facilitate digging. These blast holes need to be inspected internally for investigation of downhole material types and properties. Knowing these properties can lead to significant savings in material handling costs in downstream processes. Manual hole inspection is slow and expensive, with major limitations in revealing the geometric and geological properties of the holes and their contents. This has been the motivation for the development of our autonomous mine-site inspection robot - â€œDIPPeRâ€. In this paper, the automation aspect of the project is explained. We present a robust blast hole seeking and detection framework that enables target-based navigation and accurate down-hole sensor positioning. The pipeline first processes point-cloud data collected by the on-board LiDAR sensors, extracting the cone-shaped volume of drill-waste above the ground. By projecting the 3D cone points into a virtual depth image, segmentation is achieved in the 2D domain, yielding a circular hole at the image centre and a collared cone face. We then identify the hole centre using a robust detection module while suppressing non-maximum candidates, ensuring precise sensor placement for down-hole inspection and avoiding collisions with the cavity wall. To enable autonomous hole-seeking, the pipeline automatically adjusts its projection parameters during robot navigation to account for variations in point sparsity and hole opening size, ensuring a consistent hole appearance in 2D images. This allows continuous tracking of the target hole as the robot approaches the goal point. We demonstrate the effectiveness of our navigation and perception system in both high-fidelity simulation environments and on-site field tests. A demonstration video is available at â€œ<a target="_blank" rel="noopener" href="https://www.youtube.com/watch?v=fRNbcBcaSqE">https://www.youtube.com/watch?v=fRNbcBcaSqE</a>â€œ. </p>
<blockquote>
<p>åœ¨éœ²å¤©é‡‡çŸ¿ä½œä¸šä¸­ï¼Œä¼šåœ¨æŒ–æ˜ç°åœºçš„è¡¨é¢é’»å­”ï¼Œå¹¶ä½¿ç”¨ç‚¸è¯çˆ†ç ´ä»¥åŠ©äºæŒ–æ˜ã€‚è¿™äº›çˆ†ç ´å­”éœ€è¦å†…éƒ¨è¿›è¡Œæ£€æŸ¥ï¼Œä»¥è°ƒæŸ¥å­”å†…ææ–™çš„ç±»å‹åŠå±æ€§ã€‚äº†è§£è¿™äº›å±æ€§å¯ä»¥åœ¨åç»­æµç¨‹ä¸­æ˜¾è‘—èŠ‚çœææ–™å¤„ç†æˆæœ¬ã€‚æ‰‹åŠ¨å­”å†…æ£€æŸ¥é€Ÿåº¦æ…¢ã€æˆæœ¬é«˜ï¼Œå¹¶ä¸”åœ¨æ­ç¤ºå­”çš„å‡ ä½•å’Œåœ°è´¨å±æ€§åŠå…¶å†…å®¹æ–¹é¢å­˜åœ¨ä¸»è¦å±€é™ã€‚è¿™æ¿€å‘äº†æˆ‘ä»¬å¼€å‘è‡ªä¸»çŸ¿å±±æ£€æŸ¥æœºå™¨äººâ€œDIPPeRâ€çš„åŠ¨æœºã€‚æœ¬æ–‡è§£é‡Šäº†è¯¥é¡¹ç›®çš„è‡ªåŠ¨åŒ–æ–¹é¢ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªç¨³å¥çš„çˆ†ç ´å­”æœå¯»å’Œæ£€æµ‹æ¡†æ¶ï¼Œè¯¥æ¡†æ¶å¯å®ç°åŸºäºç›®æ ‡çš„å¯¼èˆªå’Œå‡†ç¡®çš„ä¸‹å­”ä¼ æ„Ÿå™¨å®šä½ã€‚è¯¥ç®¡é“é¦–å…ˆå¤„ç†ç”±è½¦è½½æ¿€å…‰é›·è¾¾ä¼ æ„Ÿå™¨æ”¶é›†çš„ç‚¹äº‘æ•°æ®ï¼Œæå–å‡ºåœ°é¢ä»¥ä¸Šçš„é’»æ¸£å½¢æˆçš„é”¥å½¢ä½“ç§¯ã€‚é€šè¿‡å°†3Dé”¥å½¢ç‚¹æŠ•å½±åˆ°è™šæ‹Ÿæ·±åº¦å›¾åƒä¸­ï¼Œåœ¨2Dé¢†åŸŸå®ç°åˆ†å‰²ï¼Œåœ¨å›¾åƒä¸­å¿ƒå½¢æˆä¸€ä¸ªåœ†å½¢å­”ï¼Œå‘¨å›´æ˜¯ç¯å½¢é”¥é¢ã€‚ç„¶åæˆ‘ä»¬ä½¿ç”¨ä¸€ä¸ªç¨³å¥çš„æ£€æµ‹æ¨¡å—æ¥è¯†åˆ«å­”çš„ä¸­å¿ƒï¼ŒåŒæ—¶æŠ‘åˆ¶éæœ€å¤§å€™é€‰ç›®æ ‡ï¼Œç¡®ä¿ä¸‹å­”æ£€æŸ¥çš„ä¼ æ„Ÿå™¨ç²¾ç¡®æ”¾ç½®ï¼Œå¹¶é¿å…ä¸è…”å£å‘ç”Ÿç¢°æ’ã€‚ä¸ºäº†å®ç°è‡ªä¸»å¯»å­”ï¼Œç®¡é“åœ¨æœºå™¨äººå¯¼èˆªè¿‡ç¨‹ä¸­ä¼šè‡ªåŠ¨è°ƒæ•´å…¶æŠ•å½±å‚æ•°ï¼Œä»¥åº”å¯¹ç‚¹ç¨€ç–åº¦å’Œå­”å¼€å£å¤§å°çš„å˜åŒ–ï¼Œç¡®ä¿2Då›¾åƒä¸­å­”çš„å¤–è§‚ä¸€è‡´ã€‚è¿™å…è®¸æœºå™¨äººåœ¨æ¥è¿‘ç›®æ ‡ç‚¹æ—¶æŒç»­è·Ÿè¸ªç›®æ ‡å­”ã€‚æˆ‘ä»¬åœ¨é«˜ä¿çœŸæ¨¡æ‹Ÿç¯å¢ƒå’Œç°åœºæµ‹è¯•ä¸­å±•ç¤ºäº†æˆ‘ä»¬çš„å¯¼èˆªå’Œæ„ŸçŸ¥ç³»ç»Ÿçš„æœ‰æ•ˆæ€§ã€‚æ¼”ç¤ºè§†é¢‘è¯·è®¿é—®ï¼šâ€œ<a target="_blank" rel="noopener" href="https://www.youtube.com/watch?v=fRNbcBcaSqE%E2%80%9D%E3%80%82">https://www.youtube.com/watch?v=fRNbcBcaSqEâ€ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.13785v1">PDF</a> </p>
<p><strong>Summary</strong><br>    æœ¬æ–‡ä»‹ç»äº†ä¸€ç§è‡ªä¸»çŸ¿å±±æ£€æŸ¥æœºå™¨äººâ€œDIPPeRâ€çš„è‡ªåŠ¨åŒ–æ–¹é¢ã€‚æœºå™¨äººé€šè¿‡å¤„ç†ç‚¹äº‘æ•°æ®ï¼Œå®ç°é’»å­”æœå¯»ä¸æ£€æµ‹ï¼Œèƒ½è¿›è¡Œç›®æ ‡å¯¼èˆªä¸ç²¾ç¡®çš„ä¼ æ„Ÿå™¨å®šä½ã€‚å¤„ç†æµç¨‹åŒ…æ‹¬æå–åœ°é¢ä¸Šçš„é”¥å½¢åºŸæ–™ä½“ç§¯ã€æŠ•å½±è‡³è™šæ‹Ÿæ·±åº¦å›¾åƒä»¥è¿›è¡ŒäºŒç»´åŸŸåˆ†å‰²ï¼Œè¿›è€Œç¡®å®šé’»å­”ä¸­å¿ƒã€‚æ­¤å¤–ï¼Œè¯¥æµç¨‹èƒ½å¤Ÿè°ƒæ•´æŠ•å½±å‚æ•°ä»¥åº”å¯¹ç‚¹ç¨€ç–åº¦å’Œå¼€å£å¤§å°çš„å˜åŒ–ï¼Œç¡®ä¿äºŒç»´å›¾åƒä¸­ç›®æ ‡å­”çš„ä¸€è‡´æ€§ã€‚è¯¥ç³»ç»Ÿçš„å¯¼èˆªå’Œæ„ŸçŸ¥ç³»ç»Ÿåœ¨é«˜ä¿çœŸæ¨¡æ‹Ÿç¯å¢ƒå’Œç°åœºæµ‹è¯•ä¸­å‡è¡¨ç°å‡ºäº†æœ‰æ•ˆæ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è‡ªä¸»çŸ¿å±±æ£€æŸ¥æœºå™¨äººâ€œDIPPeRâ€å¯åŠ©åŠ›å¿«é€Ÿã€ç»æµåœ°å®ç°é’»ç©ºæ£€æµ‹ä¸ææ–™æ€§è´¨åˆ†æã€‚</li>
<li>é€šè¿‡å¤„ç†ç‚¹äº‘æ•°æ®å®ç°é’»å­”æœå¯»ä¸æ£€æµ‹ï¼Œæ”¯æŒç›®æ ‡å¯¼èˆªä¸ç²¾ç¡®ä¼ æ„Ÿå™¨å®šä½ã€‚</li>
<li>åˆ©ç”¨LiDARä¼ æ„Ÿå™¨é‡‡é›†çš„ç‚¹äº‘æ•°æ®è¿›è¡Œé”¥å½¢åºŸæ–™ä½“ç§¯æå–å’Œè™šæ‹Ÿæ·±åº¦å›¾åƒæŠ•å½±ã€‚</li>
<li>é€šè¿‡äºŒç»´åŸŸåˆ†å‰²è¯†åˆ«é’»å­”ä¸­å¿ƒï¼Œç¡®ä¿ç²¾ç¡®ä¼ æ„Ÿå™¨æ”¾ç½®å¹¶é¿å…ä¸ç©ºæ´å£ç¢°æ’ã€‚</li>
<li>ç³»ç»Ÿèƒ½è‡ªåŠ¨è°ƒæ•´æŠ•å½±å‚æ•°ä»¥åº”å¯¹ä¸åŒç¯å¢ƒæ¡ä»¶ä¸‹çš„é’»å­”ç‰¹å¾å˜åŒ–ã€‚</li>
<li>åœ¨é«˜ä¿çœŸæ¨¡æ‹Ÿç¯å¢ƒå’Œç°åœºæµ‹è¯•ä¸­éªŒè¯äº†å¯¼èˆªå’Œæ„ŸçŸ¥ç³»ç»Ÿçš„æœ‰æ•ˆæ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.13785">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-79990ffa97072285c99a5e8574771ac8.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-958795ef5c48f911b90c3d35053f4569.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b02f3d4dbe146e822733f482a5992c61.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-da822507a83901f6b5ef44aecfe1d0db.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b40bbbb860db64791b4074f44e6d5d3a.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Comparing-Conditional-Diffusion-Models-for-Synthesizing-Contrast-Enhanced-Breast-MRI-from-Pre-Contrast-Images"><a href="#Comparing-Conditional-Diffusion-Models-for-Synthesizing-Contrast-Enhanced-Breast-MRI-from-Pre-Contrast-Images" class="headerlink" title="Comparing Conditional Diffusion Models for Synthesizing   Contrast-Enhanced Breast MRI from Pre-Contrast Images"></a>Comparing Conditional Diffusion Models for Synthesizing   Contrast-Enhanced Breast MRI from Pre-Contrast Images</h2><p><strong>Authors:Sebastian Ibarra, Javier del Riego, Alessandro Catanese, Julian Cuba, Julian Cardona, Nataly Leon, Jonathan Infante, Karim Lekadir, Oliver Diaz, Richard Osuala</strong></p>
<p>Dynamic contrast-enhanced (DCE) MRI is essential for breast cancer diagnosis and treatment. However, its reliance on contrast agents introduces safety concerns, contraindications, increased cost, and workflow complexity. To this end, we present pre-contrast conditioned denoising diffusion probabilistic models to synthesize DCE-MRI, introducing, evaluating, and comparing a total of 22 generative model variants in both single-breast and full breast settings. Towards enhancing lesion fidelity, we introduce both tumor-aware loss functions and explicit tumor segmentation mask conditioning. Using a public multicenter dataset and comparing to respective pre-contrast baselines, we observe that subtraction image-based models consistently outperform post-contrast-based models across five complementary evaluation metrics. Apart from assessing the entire image, we also separately evaluate the region of interest, where both tumor-aware losses and segmentation mask inputs improve evaluation metrics. The latter notably enhance qualitative results capturing contrast uptake, albeit assuming access to tumor localization inputs that are not guaranteed to be available in screening settings. A reader study involving 2 radiologists and 4 MRI technologists confirms the high realism of the synthetic images, indicating an emerging clinical potential of generative contrast-enhancement. We share our codebase at <a target="_blank" rel="noopener" href="https://github.com/sebastibar/conditional-diffusion-breast-MRI">https://github.com/sebastibar/conditional-diffusion-breast-MRI</a>. </p>
<blockquote>
<p>åŠ¨æ€å¢å¼ºç£å…±æŒ¯æˆåƒï¼ˆDCE-MRIï¼‰åœ¨ä¹³è…ºç™Œè¯Šæ–­å’Œæ²»ç–—ä¸­å…·æœ‰é‡è¦æ„ä¹‰ã€‚ç„¶è€Œï¼Œå®ƒå¯¹é€ å½±å‰‚çš„ä¾èµ–å¼•å‘äº†å®‰å…¨æ€§é—®é¢˜ã€ç¦å¿Œç—‡ã€æˆæœ¬å¢åŠ å’Œå·¥ä½œæµç¨‹å¤æ‚ç­‰é—®é¢˜ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†åŸºäºé¢„é€ å½±æ¡ä»¶å»å™ªæ‰©æ•£æ¦‚ç‡æ¨¡å‹çš„DCE-MRIåˆæˆæ–¹æ³•ï¼Œåœ¨å•ä¹³å’Œå…¨ä¹³ç¯å¢ƒä¸­å…±ä»‹ç»äº†22ç§ç”Ÿæˆæ¨¡å‹å¹¶å¯¹å…¶è¿›è¡Œäº†è¯„ä¼°å’Œæ¯”è¾ƒã€‚ä¸ºæé«˜ç—…ç¶ä¿çœŸåº¦ï¼Œæˆ‘ä»¬å¼•å…¥äº†è‚¿ç˜¤æ„ŸçŸ¥æŸå¤±å‡½æ•°å’Œæ˜ç¡®çš„è‚¿ç˜¤åˆ†å‰²æ©æ¨¡æ¡ä»¶ã€‚ä½¿ç”¨å…¬å…±å¤šä¸­å¿ƒæ•°æ®é›†ï¼Œä¸ç›¸åº”çš„é¢„é€ å½±åŸºçº¿ç›¸æ¯”ï¼Œæˆ‘ä»¬å‘ç°åŸºäºå‡æ³•å›¾åƒçš„æ¨¡å‹åœ¨äº”ç§äº’è¡¥è¯„ä¼°æŒ‡æ ‡ä¸ŠæŒç»­ä¼˜äºåŸºäºåé€ å½±çš„æ¨¡å‹ã€‚é™¤äº†å¯¹æ•´ä¸ªå›¾åƒè¿›è¡Œè¯„ä¼°å¤–ï¼Œæˆ‘ä»¬è¿˜å¯¹æ„Ÿå…´è¶£åŒºåŸŸè¿›è¡Œäº†å•ç‹¬è¯„ä¼°ï¼Œå…¶ä¸­è‚¿ç˜¤æ„ŸçŸ¥æŸå¤±å’Œåˆ†å‰²æ©æ¨¡è¾“å…¥æé«˜äº†è¯„ä¼°æŒ‡æ ‡ã€‚åè€…æ˜¾è‘—æé«˜äº†å®šæ€§ç»“æœï¼Œæ•æ‰äº†é€ å½±å‰‚æ‘„å–æƒ…å†µï¼Œå°½ç®¡å®ƒå‡è®¾èƒ½è·å–è‚¿ç˜¤å®šä½è¾“å…¥ï¼Œè€Œåœ¨ç­›æŸ¥ç¯å¢ƒä¸­è¿™æ˜¯æ— æ³•ä¿è¯çš„ã€‚ä¸€é¡¹æ¶‰åŠä¸¤åæ”¾å°„ç§‘åŒ»ç”Ÿå’Œå››åMRIæŠ€æœ¯ä¸“å®¶çš„è¯»è€…ç ”ç©¶è¯å®äº†åˆæˆå›¾åƒçš„é«˜åº¦é€¼çœŸæ€§ï¼Œè¡¨æ˜ç”Ÿæˆé€ å½±å¢å¼ºçš„ä¸´åºŠæ½œåŠ›ã€‚æˆ‘ä»¬å·²å°†æˆ‘ä»¬çš„ä»£ç åº“å…±äº«è‡³ï¼š<a target="_blank" rel="noopener" href="https://github.com/sebastibar/conditional-diffusion-breast-MRI">https://github.com/sebastibar/conditional-diffusion-breast-MRI</a> ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.13776v1">PDF</a> 13 pages, 5 figures, submitted and accepted to MICCAI Deepbreath   workshop 2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†åŠ¨æ€å¢å¼ºç£å…±æŒ¯æˆåƒï¼ˆDCE-MRIï¼‰åœ¨ä¹³è…ºç™Œè¯Šæ–­å’Œæ²»ç–—ä¸­çš„é‡è¦æ€§ï¼Œä½†ç”±äºå…¶ä¾èµ–é€ å½±å‰‚è€Œå¸¦æ¥å®‰å…¨é¡¾è™‘ã€ç¦å¿Œç—‡ã€æˆæœ¬å¢åŠ å’Œå·¥ä½œæµç¨‹å¤æ‚ç­‰é—®é¢˜ã€‚ç ”ç©¶å›¢é˜Ÿæå‡ºäº†åŸºäºé¢„é€ å½±æ¡ä»¶çš„å»å™ªæ‰©æ•£æ¦‚ç‡æ¨¡å‹æ¥åˆæˆDCE-MRIå›¾åƒï¼Œè¯„ä¼°å’Œæ¯”è¾ƒäº†æ€»å…±22ç§ç”Ÿæˆæ¨¡å‹å˜ä½“åœ¨å•ä¹³å’Œå…¨ä¹³åœºæ™¯ä¸‹çš„è¡¨ç°ã€‚ä¸ºæé«˜ç—…ç¶ä¿çœŸåº¦ï¼Œç ”ç©¶å¼•å…¥äº†è‚¿ç˜¤æ„ŸçŸ¥æŸå¤±å‡½æ•°å’Œæ˜ç¡®çš„è‚¿ç˜¤åˆ†å‰²æ©è†œæ¡ä»¶ã€‚é€šè¿‡å…¬å…±å¤šä¸­å¿ƒæ•°æ®é›†çš„æ¯”è¾ƒï¼ŒåŸºäºå‡æ³•å›¾åƒæ¨¡å‹çš„æ€§èƒ½åœ¨äº”ç§äº’è¡¥è¯„ä¼°æŒ‡æ ‡ä¸ŠæŒç»­ä¼˜äºåŸºäºåé€ å½±æ¨¡å‹ã€‚æ­¤å¤–ï¼Œå¯¹æ„Ÿå…´è¶£åŒºåŸŸçš„è¯„ä¼°æ˜¾ç¤ºï¼Œè‚¿ç˜¤æ„ŸçŸ¥æŸå¤±å’Œåˆ†å‰²æ©è†œè¾“å…¥æ”¹å–„äº†è¯„ä¼°æŒ‡æ ‡ã€‚å°½ç®¡éœ€è¦å‡è®¾å­˜åœ¨è‚¿ç˜¤å®šä½è¾“å…¥ï¼Œè¿™åœ¨ç­›æŸ¥ç¯å¢ƒä¸­å¯èƒ½æ— æ³•ä¿è¯ï¼Œä½†åŒ…å«è‚¿ç˜¤æ„ŸçŸ¥æŸå¤±å’Œåˆ†å‰²æ©è†œè¾“å…¥çš„æ¨¡å‹åœ¨æ•æ‰é€ å½±å‰‚æ‘„å–æ–¹é¢æ˜¾è‘—æé«˜äº†å®šæ€§ç»“æœã€‚é€šè¿‡æ¶‰åŠä¸¤åæ”¾å°„ç§‘åŒ»ç”Ÿå’Œå››åMRIæŠ€æœ¯ä¸“å®¶çš„è¯»è€…ç ”ç©¶è¯å®äº†åˆæˆå›¾åƒçš„é«˜åº¦é€¼çœŸæ€§ï¼Œè¡¨æ˜ç”Ÿæˆå¯¹æ¯”å¢å¼ºæŠ€æœ¯çš„ä¸´åºŠæ½œåŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>DCE-MRIåœ¨ä¹³è…ºç™Œè¯Šæ–­å’Œæ²»ç–—ä¸­å…·æœ‰é‡è¦æ€§ï¼Œä½†ä¾èµ–é€ å½±å‰‚å­˜åœ¨å®‰å…¨é¡¾è™‘ã€ç¦å¿Œç—‡ã€æˆæœ¬å’Œå·¥ä½œæµç¨‹å¤æ‚ç­‰é—®é¢˜ã€‚</li>
<li>æå‡ºäº†åŸºäºé¢„é€ å½±æ¡ä»¶çš„å»å™ªæ‰©æ•£æ¦‚ç‡æ¨¡å‹æ¥åˆæˆDCE-MRIå›¾åƒï¼Œå¹¶è¯„ä¼°å’Œæ¯”è¾ƒäº†å¤šç§ç”Ÿæˆæ¨¡å‹ã€‚</li>
<li>ä¸ºæé«˜ç—…ç¶ä¿çœŸåº¦ï¼Œå¼•å…¥äº†è‚¿ç˜¤æ„ŸçŸ¥æŸå¤±å‡½æ•°å’Œæ˜ç¡®çš„è‚¿ç˜¤åˆ†å‰²æ©è†œæ¡ä»¶ã€‚</li>
<li>åŸºäºå‡æ³•å›¾åƒæ¨¡å‹çš„æ€§èƒ½ä¼˜äºåŸºäºåé€ å½±æ¨¡å‹ï¼Œè¿™åœ¨ä¸€é¡¹æ¶‰åŠå¤šç§è¯„ä¼°æŒ‡æ ‡çš„å…¬å…±å¤šä¸­å¿ƒæ•°æ®é›†çš„æ¯”è¾ƒä¸­å¾—åˆ°äº†è¯å®ã€‚</li>
<li>æ„Ÿå…´è¶£åŒºåŸŸçš„è¯„ä¼°æ˜¾ç¤ºè‚¿ç˜¤æ„ŸçŸ¥æŸå¤±å’Œåˆ†å‰²æ©è†œè¾“å…¥æ”¹å–„äº†è¯„ä¼°æ•ˆæœã€‚</li>
<li>åŒ…å«è‚¿ç˜¤æ„ŸçŸ¥æŸå¤±å’Œåˆ†å‰²æ©è†œè¾“å…¥çš„æ¨¡å‹åœ¨æ•æ‰é€ å½±å‰‚æ‘„å–æ–¹é¢æ˜¾è‘—æé«˜äº†å®šæ€§ç»“æœï¼Œä½†éœ€æ³¨æ„è‚¿ç˜¤å®šä½è¾“å…¥å¯èƒ½æ— æ³•ä¿è¯ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.13776">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-5e99db08f6962d9ead10244482693034.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4eb26901bee34583a0de5252aac78ae8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f92d2ba4fb3cf9592e2b8dcd516efec2.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-49c248c49e88a67343c7a9637eeff0e8.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Diversity-enhanced-Collaborative-Mamba-for-Semi-supervised-Medical-Image-Segmentation"><a href="#Diversity-enhanced-Collaborative-Mamba-for-Semi-supervised-Medical-Image-Segmentation" class="headerlink" title="Diversity-enhanced Collaborative Mamba for Semi-supervised Medical Image   Segmentation"></a>Diversity-enhanced Collaborative Mamba for Semi-supervised Medical Image   Segmentation</h2><p><strong>Authors:Shumeng Li, Jian Zhang, Lei Qi, Luping Zhou, Yinghuan Shi, Yang Gao</strong></p>
<p>Acquiring high-quality annotated data for medical image segmentation is tedious and costly. Semi-supervised segmentation techniques alleviate this burden by leveraging unlabeled data to generate pseudo labels. Recently, advanced state space models, represented by Mamba, have shown efficient handling of long-range dependencies. This drives us to explore their potential in semi-supervised medical image segmentation. In this paper, we propose a novel Diversity-enhanced Collaborative Mamba framework (namely DCMamba) for semi-supervised medical image segmentation, which explores and utilizes the diversity from data, network, and feature perspectives. Firstly, from the data perspective, we develop patch-level weak-strong mixing augmentation with Mambaâ€™s scanning modeling characteristics. Moreover, from the network perspective, we introduce a diverse-scan collaboration module, which could benefit from the prediction discrepancies arising from different scanning directions. Furthermore, from the feature perspective, we adopt an uncertainty-weighted contrastive learning mechanism to enhance the diversity of feature representation. Experiments demonstrate that our DCMamba significantly outperforms other semi-supervised medical image segmentation methods, e.g., yielding the latest SSM-based method by 6.69% on the Synapse dataset with 20% labeled data. </p>
<blockquote>
<p>è·å–é«˜è´¨é‡æ ‡æ³¨æ•°æ®å¯¹äºåŒ»å­¦å›¾åƒåˆ†å‰²è€Œè¨€æ—¢ç¹çåˆæˆæœ¬é«˜æ˜‚ã€‚åŠç›‘ç£åˆ†å‰²æŠ€æœ¯é€šè¿‡åˆ©ç”¨æœªæ ‡æ³¨æ•°æ®ç”Ÿæˆä¼ªæ ‡ç­¾ï¼Œå‡è½»äº†è¿™ä¸€è´Ÿæ‹…ã€‚æœ€è¿‘ï¼Œä»¥Mambaä¸ºä»£è¡¨çš„é«˜çº§çŠ¶æ€ç©ºé—´æ¨¡å‹æ˜¾ç¤ºå‡ºå¯¹é•¿è·ç¦»ä¾èµ–å…³ç³»çš„æœ‰æ•ˆå¤„ç†ã€‚è¿™ä¿ƒä½¿æˆ‘ä»¬æ¢ç´¢å…¶åœ¨åŠç›‘ç£åŒ»å­¦å›¾åƒåˆ†å‰²ä¸­çš„æ½œåŠ›ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ç”¨äºåŠç›‘ç£åŒ»å­¦å›¾åƒåˆ†å‰²çš„æ–°å‹å¤šæ ·æ€§å¢å¼ºååŒMambaæ¡†æ¶ï¼ˆå³DCMambaï¼‰ï¼Œè¯¥æ¡†æ¶ä»æ•°æ®ã€ç½‘ç»œå’Œç‰¹å¾è§’åº¦æ¢ç´¢å¹¶åˆ©ç”¨å¤šæ ·æ€§ã€‚é¦–å…ˆï¼Œä»æ•°æ®è§’åº¦ï¼Œæˆ‘ä»¬ç»“åˆMambaçš„æ‰«æå»ºæ¨¡ç‰¹æ€§ï¼Œå¼€å‘å‡ºåŸºäºè¡¥ä¸çº§åˆ«çš„å¼±å¼ºæ··åˆå¢å¼ºæŠ€æœ¯ã€‚æ­¤å¤–ï¼Œä»ç½‘ç»œè§’åº¦çœ‹ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ªå¤šæ ·åŒ–çš„æ‰«æåä½œæ¨¡å—ï¼Œå¯ä»¥åˆ©ç”¨ä¸åŒæ‰«ææ–¹å‘äº§ç”Ÿçš„é¢„æµ‹å·®å¼‚ã€‚è¿›ä¸€æ­¥åœ°ï¼Œä»ç‰¹å¾è§’åº¦ï¼Œæˆ‘ä»¬é‡‡ç”¨ä¸€ç§åŸºäºä¸ç¡®å®šæ€§çš„å¯¹æ¯”å­¦ä¹ æœºåˆ¶ï¼Œä»¥å¢å¼ºç‰¹å¾è¡¨ç¤ºçš„å¤šæ ·æ€§ã€‚å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„DCMambaåœ¨åŠç›‘ç£åŒ»å­¦å›¾åƒåˆ†å‰²æ–¹æ³•ä¸Šæ˜æ˜¾ä¼˜äºå…¶ä»–æ–¹æ³•ï¼Œä¾‹å¦‚ï¼Œåœ¨Synapseæ•°æ®é›†ä¸Šä½¿ç”¨20%çš„æ ‡æ³¨æ•°æ®ï¼Œç›¸è¾ƒäºæœ€æ–°çš„SSMæ–¹æ³•æå‡äº†6.69%ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.13712v1">PDF</a> </p>
<p><strong>Summary</strong><br>     åŠç›‘ç£åŒ»å­¦å›¾åƒåˆ†å‰²æŠ€æœ¯åˆ©ç”¨æœªæ ‡æ³¨æ•°æ®ç”Ÿæˆä¼ªæ ‡ç­¾ï¼Œå‡è½»æ ‡æ³¨è´Ÿæ‹…ã€‚æœ¬æ–‡æå‡ºä¸€ç§åŸºäºå…ˆè¿›çŠ¶æ€ç©ºé—´æ¨¡å‹ï¼ˆå¦‚Mambaï¼‰çš„åŠç›‘ç£åŒ»å­¦å›¾åƒåˆ†å‰²æ¡†æ¶DCMambaï¼Œä»æ•°æ®ã€ç½‘ç»œå’Œç‰¹å¾ä¸‰ä¸ªè§’åº¦æ¢ç´¢å’Œåˆ©ç”¨å¤šæ ·æ€§ã€‚å®éªŒè¯æ˜ï¼ŒDCMambaæ˜¾è‘—ä¼˜äºå…¶ä»–åŠç›‘ç£åŒ»å­¦å›¾åƒåˆ†å‰²æ–¹æ³•ï¼Œå¦‚åœ¨Synapseæ•°æ®é›†ä¸Šï¼Œä½¿ç”¨20%æ ‡æ³¨æ•°æ®çš„æƒ…å†µä¸‹ï¼Œæ¯”æœ€æ–°SSMæ–¹æ³•é«˜å‡º6.69%ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>åŠç›‘ç£åŒ»å­¦å›¾åƒåˆ†å‰²æŠ€æœ¯åˆ©ç”¨æœªæ ‡æ³¨æ•°æ®ç”Ÿæˆä¼ªæ ‡ç­¾ï¼Œå‡è½»é«˜æˆæœ¬å’Œé«˜éš¾åº¦çš„æ ‡æ³¨å·¥ä½œã€‚</li>
<li>DCMambaæ¡†æ¶åŸºäºå…ˆè¿›çŠ¶æ€ç©ºé—´æ¨¡å‹ï¼ˆå¦‚Mambaï¼‰ï¼Œæœ‰æ•ˆå¤„ç†é•¿è·ç¦»ä¾èµ–å…³ç³»ã€‚</li>
<li>DCMambaä»æ•°æ®ã€ç½‘ç»œå’Œç‰¹å¾ä¸‰ä¸ªè§’åº¦æ¢ç´¢å’Œåˆ©ç”¨å¤šæ ·æ€§ã€‚</li>
<li>æ•°æ®å±‚é¢ï¼Œåˆ©ç”¨å¼±å¼ºæ··åˆå¢å¼ºæŠ€æœ¯ä¸Mambaæ‰«æå»ºæ¨¡ç‰¹æ€§ç»“åˆã€‚</li>
<li>ç½‘ç»œå±‚é¢ï¼Œå¼•å…¥å¤šæ ·æ‰«æåä½œæ¨¡å—ï¼Œåˆ©ç”¨ä¸åŒæ‰«ææ–¹å‘äº§ç”Ÿçš„é¢„æµ‹å·®å¼‚ã€‚</li>
<li>ç‰¹å¾å±‚é¢ï¼Œé‡‡ç”¨åŠ æƒä¸ç¡®å®šæ€§å¯¹æ¯”å­¦ä¹ æœºåˆ¶ï¼Œæé«˜ç‰¹å¾è¡¨ç¤ºçš„å¤šæ ·æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.13712">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-ec3ee3f40ed5bc20caef50ae3435e9c2.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e61b9a44232f67c708f3d1bcbd25c383.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c5f14caaca09a22212d5f5b376e75d2a.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-e51a5cfed7d7632acad2f245bb810145.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-7934b99577bba6da10c75464b91b69cc.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7265944333aedcf1176f1d9b33fd11fc.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="X-ray-selected-broad-absorption-line-quasars-in-SDSS-V-BALs-and-non-BALs-span-the-same-range-of-X-ray-properties"><a href="#X-ray-selected-broad-absorption-line-quasars-in-SDSS-V-BALs-and-non-BALs-span-the-same-range-of-X-ray-properties" class="headerlink" title="X-ray selected broad absorption line quasars in SDSS-V: BALs and   non-BALs span the same range of X-ray properties"></a>X-ray selected broad absorption line quasars in SDSS-V: BALs and   non-BALs span the same range of X-ray properties</h2><p><strong>Authors:Pranavi Hiremath, Amy L. Rankine, James Aird, W. N. Brandt, Paola RodrÃ­guez Hidalgo, Scott F. Anderson, Catarina Aydar, Claudio Ricci, Donald P. Schneider, M. Vivek, Zsofi Igo, Sean Morrison, Mara Salvato</strong></p>
<p>Broad absorption line (BAL) quasars are often considered X-ray weak relative to their optical&#x2F;UV luminosity, whether intrinsically (i.e., the coronal emission is fainter) or due to large column densities of absorbing material. The SDSS-V is providing optical spectroscopy for samples of quasar candidates identified by eROSITA as well as Chandra, XMM or Swift, making the resulting datasets ideal for characterising the BAL quasar population within an X-ray selected sample. We use the Balnicity Index (BI) to identify the BAL quasars based on absorption of the CIV $\lambda,1549$ emission line in the optical spectra, finding 143 BAL quasars in our sample of 2317 X-ray selected quasars within $1.5\le z \le3.5$. This observed BAL fraction of $\approx$ 6 per cent is comparable to that found in optically selected samples. We also identify absorption systems via the Absorption Index (AI) which includes mini-BALs and NALs, finding 954 quasars with AI $&gt;0$. We consider the CIV emission space (equivalent width vs. blueshift) to study the BAL outflows within the context of the radiatively driven accretion disc-wind model. X-ray selection excludes the highest outflow velocities in emission but includes the full range of absorption velocities which we suggest is consistent with the BAL gas being located further from the X-ray corona than the emitting gas. We observe both X-ray weak and X-ray strong BALs (via the optical-to-X-ray spectral slope, $\alpha_\text{ox}$) and detect little evidence for differing column densities between the BAL and non-BAL quasars, suggesting the BALs and non-BALs have the same shielding gas and intrinsic X-ray emission. </p>
<blockquote>
<p>å·´å°”ï¼ˆBroad Absorption Lineï¼Œç®€ç§°BALï¼‰ç±»æ˜Ÿä½“é€šå¸¸è¢«è®¤ä¸ºåœ¨Xå°„çº¿ç›¸å¯¹äºå…¶å…‰å­¦&#x2F;ç´«å¤–å…‰åº¦è¾ƒå¼±ï¼Œæ— è®ºè¿™æ˜¯å…¶å›ºæœ‰ç‰¹æ€§ï¼ˆå³å†•å‘å°„è¾ƒå¼±ï¼‰ï¼Œè¿˜æ˜¯ç”±äºå¸æ”¶ææ–™çš„å¤§æŸ±å¯†åº¦æ‰€è‡´ã€‚SDSS-Vä¸ºeROSITAä»¥åŠChandraã€XMMæˆ–Swiftè¯†åˆ«çš„ç±»æ˜Ÿä½“å€™é€‰æ ·æœ¬æä¾›äº†å…‰å­¦å…‰è°±ï¼Œè¿™ä½¿å¾—æ•°æ®é›†éå¸¸é€‚åˆäºè¡¨å¾Xå°„çº¿é€‰å®šæ ·æœ¬ä¸­çš„å·´å°”ç±»æ˜Ÿä½“ã€‚æˆ‘ä»¬ä½¿ç”¨å·´å°”æŒ‡æ•°ï¼ˆBalnicity Indexï¼Œç®€ç§°BIï¼‰æ¥è¯†åˆ«åŸºäºå…‰å­¦å…‰è°±ä¸­CIV Î» 1549å‘å°„çº¿çš„å¸æ”¶è€Œå­˜åœ¨çš„å·´å°”ç±»æ˜Ÿä½“ï¼Œåœ¨æˆ‘ä»¬çš„æ ·æœ¬ä¸­æ‰¾åˆ°äº†å¤„äºçº¢ç§»èŒƒå›´ $1.5\le z \le3.5$ çš„2317ä¸ªXå°„çº¿é€‰å®šç±»æ˜Ÿä½“ä¸­çš„143ä¸ªå·´å°”ç±»æ˜Ÿä½“ã€‚è§‚å¯Ÿåˆ°çš„å·´å°”å æ¯”çº¦ä¸ºç™¾åˆ†ä¹‹å…­ï¼Œä¸å…‰å­¦é€‰å®šæ ·æœ¬ä¸­è§‚å¯Ÿåˆ°çš„ç»“æœç›¸å½“ã€‚æˆ‘ä»¬è¿˜é€šè¿‡åŒ…æ‹¬å¾®å·´å°”å’ŒNALçš„å¸å…‰åº¦æŒ‡æ•°ï¼ˆAbsorption Indexï¼Œç®€ç§°AIï¼‰æ¥è¯†åˆ«å¸æ”¶ç³»ç»Ÿï¼Œå‘ç°AIå¤§äºé›¶çš„ç±»æ˜Ÿä½“æœ‰954ä¸ªã€‚æˆ‘ä»¬è€ƒè™‘CIVå‘å°„ç©ºé—´ï¼ˆç­‰æ•ˆå®½åº¦ä¸è“ç§»ï¼‰æ¥ç ”ç©¶å·´å°”æµå‡ºåœ¨è¾å°„é©±åŠ¨å¸ç§¯ç›˜é£æ¨¡å‹ä¸­çš„èƒŒæ™¯ã€‚Xå°„çº¿é€‰æ‹©æ’é™¤äº†å‘å°„ä¸­çš„æœ€é«˜æµå‡ºé€Ÿåº¦ï¼Œä½†åŒ…æ‹¬äº†å®Œæ•´çš„å¸æ”¶é€Ÿåº¦èŒƒå›´ï¼Œæˆ‘ä»¬è®¤ä¸ºè¿™ä¸å·´å°”æ°”ä½“ä½äºè¿œç¦»Xå°„çº¿å†•çš„å‘å°„æ°”ä½“ä¹‹å¤–æ˜¯ä¸€è‡´çš„ã€‚æˆ‘ä»¬è§‚å¯Ÿåˆ°æ—¢æœ‰Xå°„çº¿å¼±çš„å·´å°”ç±»æ˜Ÿä½“ä¹Ÿæœ‰Xå°„çº¿å¼ºçš„å·´å°”ç±»æ˜Ÿä½“ï¼ˆé€šè¿‡å…‰å­¦åˆ°Xå°„çº¿çš„å…‰è°±æ–œç‡ $\alpha_\text{ox}$ ï¼‰ï¼Œå¹¶ä¸”åœ¨å·´å°”ç±»æ˜Ÿä½“å’Œéå·´å°”ç±»æ˜Ÿä½“ä¹‹é—´å‘ç°äº†æŸ±å¯†åº¦å‡ ä¹æ²¡æœ‰å·®å¼‚çš„è¯æ®ï¼Œè¿™è¡¨æ˜å®ƒä»¬å…·æœ‰ç›¸åŒçš„å±è”½æ°”ä½“å’Œå›ºæœ‰Xå°„çº¿å‘å°„ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.13682v1">PDF</a> 19 pages, 16 figures, 3 appendices, accepted for publication in MNRAS</p>
<p><strong>Summary</strong></p>
<p>åœ¨Xå°„çº¿é€‰æ‹©çš„æ ·æœ¬ä¸­ï¼Œå·´å°”æœ«å°”ï¼ˆBALï¼‰ç±»å¥è¨ï¼ˆquasarï¼‰è¡¨ç°å‡ºç›¸å¯¹è¾ƒå¼±çš„å…‰è°±ç‰¹å¾ã€‚é€šè¿‡å¯¹SDSS-Vå…‰å­¦å…‰è°±æ•°æ®çš„åˆ†æï¼Œåˆ©ç”¨å·´å°”æœ«å°”æŒ‡æ•°ï¼ˆBIï¼‰è¯†åˆ«å‡ºå› CIV Î» 1549å‘å°„çº¿å¸æ”¶è€Œäº§ç”Ÿçš„å·´å°”æœ«å°”ç±»å¥è¨ã€‚ç ”ç©¶å‘ç°åœ¨æ ·æœ¬ä¸­è§‚å¯Ÿåˆ°çº¦6%çš„å·´å°”æœ«å°”ç±»å¥è¨ï¼Œä¸å…‰å­¦é€‰æ‹©çš„æ ·æœ¬ç›¸å½“ã€‚æ­¤å¤–ï¼Œé€šè¿‡å¸æ”¶æŒ‡æ•°ï¼ˆAIï¼‰å‘ç°äº†åŒ…å«å¾®å‹å·´å°”æœ«å°”å’ŒNALsçš„å¸æ”¶ç³»ç»Ÿã€‚å¯¹CIVå‘å°„ç©ºé—´çš„ç ”ç©¶è¡¨æ˜ï¼Œè¾å°„é©±åŠ¨å‹å¸ç§¯ç›˜é£æ¨¡å‹ä¸­çš„å·´å°”æœ«å°”æµå‡ºç‰©è¡¨ç°å‡ºå¤šæ ·åŒ–çš„å¤–æµé€Ÿåº¦ç‰¹æ€§ã€‚Xå°„çº¿é€‰æ‹©æ’é™¤äº†æœ€é«˜çš„å‘å°„å¤–æµé€Ÿåº¦ï¼Œä½†åŒ…å«äº†å®Œæ•´çš„å¸æ”¶é€Ÿåº¦èŒƒå›´ï¼Œæš—ç¤ºå·´å°”æœ«å°”æ°”ä½“ä½äºXå°„çº¿å†•ä¹‹å¤–ã€‚åŒæ—¶è§‚å¯Ÿåˆ°æ—¢æœ‰Xå°„çº¿å¼±çš„ä¹Ÿæœ‰Xå°„çº¿å¼ºçš„å·´å°”æœ«å°”ç±»å¥è¨ï¼Œä¸”æœªå‘ç°BALå’ŒéBALå¥è¨ä¹‹é—´çš„åˆ—å¯†åº¦å·®å¼‚ï¼Œæš—ç¤ºå®ƒä»¬å¯èƒ½æœ‰ç›¸åŒçš„å±è”½æ°”ä½“å’Œå†…åœ¨Xå°„çº¿å‘å°„ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å·´å°”æœ«å°”ï¼ˆBALï¼‰ç±»å¥è¨åœ¨Xå°„çº¿å…‰è°±ä¸Šç›¸å¯¹è¾ƒå¼±ã€‚</li>
<li>é€šè¿‡SDSS-Vå…‰å­¦å…‰è°±æ•°æ®è¯†åˆ«äº†å·´å°”æœ«å°”ç±»å¥è¨ã€‚</li>
<li>åœ¨Xå°„çº¿é€‰æ‹©çš„æ ·æœ¬ä¸­è§‚å¯Ÿåˆ°çš„å·´å°”æœ«å°”ç±»å¥è¨æ¯”ä¾‹çº¦ä¸º6%ã€‚</li>
<li>é€šè¿‡å¸æ”¶æŒ‡æ•°å‘ç°äº†åŒ…å«å¾®å‹å·´å°”æœ«å°”å’ŒNALsçš„å¸æ”¶ç³»ç»Ÿã€‚</li>
<li>è¾å°„é©±åŠ¨å‹å¸ç§¯ç›˜é£æ¨¡å‹ä¸­çš„å·´å°”æœ«å°”æµå‡ºç‰©å…·æœ‰å¤šæ ·åŒ–çš„ç‰¹æ€§ã€‚</li>
<li>Xå°„çº¿é€‰æ‹©æ ·æœ¬ä¸­ï¼Œå·´å°”æœ«å°”æ°”ä½“å¯èƒ½ä½äºXå°„çº¿å†•ä¹‹å¤–ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.13682">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-82cbcd2075f075324c4da8bf4d258da1.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-9759e23a3012ddb3ab7ad124bd365ad0.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-f69d1da382c3824f0d9cddc205f3c28a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-62c91de5aaea4a2cabc2acb4756732e5.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="AIM-2025-challenge-on-Inverse-Tone-Mapping-Report-Methods-and-Results"><a href="#AIM-2025-challenge-on-Inverse-Tone-Mapping-Report-Methods-and-Results" class="headerlink" title="AIM 2025 challenge on Inverse Tone Mapping Report: Methods and Results"></a>AIM 2025 challenge on Inverse Tone Mapping Report: Methods and Results</h2><p><strong>Authors:Chao Wang, Francesco Banterle, Bin Ren, Radu Timofte, Xin Lu, Yufeng Peng, Chengjie Ge, Zhijing Sun, Ziang Zhou, Zihao Li, Zishun Liao, Qiyu Kang, Xueyang Fu, Zheng-Jun Zha, Zhijing Sun, Xingbo Wang, Kean Liu, Senyan Xu, Yang Qiu, Yifan Ding, Gabriel Eilertsen, Jonas Unger, Zihao Wang, Ke Wu, Jinshan Pan, Zhen Liu, Zhongyang Li, Shuaicheng Liu, S. M Nadim Uddin</strong></p>
<p>This paper presents a comprehensive review of the AIM 2025 Challenge on Inverse Tone Mapping (ITM). The challenge aimed to push forward the development of effective ITM algorithms for HDR image reconstruction from single LDR inputs, focusing on perceptual fidelity and numerical consistency. A total of \textbf{67} participants submitted \textbf{319} valid results, from which the best five teams were selected for detailed analysis. This report consolidates their methodologies and performance, with the lowest PU21-PSNR among the top entries reaching 29.22 dB. The analysis highlights innovative strategies for enhancing HDR reconstruction quality and establishes strong benchmarks to guide future research in inverse tone mapping. </p>
<blockquote>
<p>æœ¬æ–‡å…¨é¢å›é¡¾äº†AIM 2025å…³äºé€†è‰²è°ƒæ˜ å°„ï¼ˆITMï¼‰çš„æŒ‘æˆ˜ã€‚è¯¥æŒ‘æˆ˜æ—¨åœ¨æ¨åŠ¨ä»å•ä¸€LDRè¾“å…¥æ„å»ºæœ‰æ•ˆçš„ITMç®—æ³•çš„å‘å±•ï¼Œä¾§é‡äºæ„ŸçŸ¥ä¿çœŸåº¦å’Œæ•°å€¼ä¸€è‡´æ€§ã€‚å…±æœ‰\textbf{67}åå‚ä¸è€…æäº¤äº†\textbf{319}ä»½æœ‰æ•ˆç»“æœï¼Œä»ä¸­é€‰æ‹©äº†äº”æ”¯é¡¶å°–å›¢é˜Ÿè¿›è¡Œè¯¦ç»†åˆ†æã€‚æœ¬æŠ¥å‘Šæ€»ç»“äº†ä»–ä»¬çš„æ–¹æ³•å’Œæ€§èƒ½ï¼Œå…¶ä¸­æœ€ä½³ä½œå“çš„PU21-PSNRæœ€ä½å€¼è¾¾åˆ°29.22 dBã€‚åˆ†æçªå‡ºäº†æé«˜HDRé‡å»ºè´¨é‡çš„åˆ›æ–°ç­–ç•¥ï¼Œå¹¶å»ºç«‹äº†å¼ºæœ‰åŠ›çš„åŸºå‡†ï¼Œä¸ºæœªæ¥çš„é€†è‰²è°ƒæ˜ å°„ç ”ç©¶æä¾›äº†æŒ‡å¯¼ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.13479v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡å…¨é¢è¯„è¿°äº†AIM 2025æŒ‘æˆ˜èµ›ä¸­çš„é€†è‰²è°ƒæ˜ å°„ï¼ˆITMï¼‰æŠ€æœ¯ã€‚è¯¥æŒ‘æˆ˜èµ›æ—¨åœ¨æ¨åŠ¨ä»å•ä¸€LDRè¾“å…¥å®ç°HDRå›¾åƒé‡å»ºçš„æœ‰æ•ˆITMç®—æ³•çš„å‘å±•ï¼Œé‡ç‚¹å…³æ³¨æ„ŸçŸ¥ä¿çœŸåº¦å’Œæ•°å€¼ä¸€è‡´æ€§ã€‚å…±æœ‰67æ”¯é˜Ÿä¼æäº¤äº†319ä¸ªæœ‰æ•ˆç»“æœï¼Œå…¶ä¸­å‰äº”æ”¯é˜Ÿä¼è¢«é€‰ä¸­è¿›è¡Œè¯¦ç»†åˆ†æã€‚æœ¬æ–‡æ€»ç»“äº†è¿™äº›é˜Ÿä¼çš„æ–¹æ³•å’Œè¡¨ç°ï¼Œæœ€ä½³é˜Ÿä¼çš„PU21-PSNRè¾¾åˆ°29.22 dBã€‚åˆ†æçªå‡ºäº†æé«˜HDRé‡å»ºè´¨é‡çš„åˆ›æ–°ç­–ç•¥ï¼Œå¹¶ä¸ºæœªæ¥çš„é€†è‰²è°ƒæ˜ å°„ç ”ç©¶æä¾›äº†å¼ºæœ‰åŠ›çš„åŸºå‡†ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>AIM 2025æŒ‘æˆ˜èµ›èšç„¦äºé€†è‰²è°ƒæ˜ å°„ï¼ˆITMï¼‰æŠ€æœ¯çš„HDRå›¾åƒé‡å»ºã€‚</li>
<li>æŒ‘æˆ˜èµ›æœ‰67æ”¯é˜Ÿä¼å‚ä¸ï¼Œæäº¤äº†319ä¸ªæœ‰æ•ˆç»“æœã€‚</li>
<li>å‰äº”åé˜Ÿä¼çš„æ–¹æ³•å’Œè¡¨ç°è¢«è¯¦ç»†åˆ†æå¹¶æ€»ç»“ã€‚</li>
<li>æœ€ä½³é˜Ÿä¼çš„PU21-PSNRè¾¾åˆ°29.22 dBã€‚</li>
<li>åˆ†æå¼ºè°ƒäº†æé«˜HDRé‡å»ºè´¨é‡çš„åˆ›æ–°ç­–ç•¥ã€‚</li>
<li>æŠ¥å‘Šä¸ºæœªæ¥çš„é€†è‰²è°ƒæ˜ å°„ç ”ç©¶æä¾›äº†å¼ºæœ‰åŠ›çš„åŸºå‡†ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.13479">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-73190ac1236af29d44c87cf0aa5dc9e7.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ee1508ed2c3720336fdcefba78205b3f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e1ca21dcb1350a39b428a593bf5e0ffe.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-682466cd276479a3d2b1e1e77b90b023.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7d4561c15f5608a6c1c0124c1ee39202.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9e3cae8164acf77770c396e58b47e17e.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="Distribution-Aware-Hadamard-Quantization-for-Hardware-Efficient-Implicit-Neural-Representations"><a href="#Distribution-Aware-Hadamard-Quantization-for-Hardware-Efficient-Implicit-Neural-Representations" class="headerlink" title="Distribution-Aware Hadamard Quantization for Hardware-Efficient Implicit   Neural Representations"></a>Distribution-Aware Hadamard Quantization for Hardware-Efficient Implicit   Neural Representations</h2><p><strong>Authors:Wenyong Zhou, Jiachen Ren, Taiqiang Wu, Yuxin Cheng, Zhengwu Liu, Ngai Wong</strong></p>
<p>Implicit Neural Representations (INRs) encode discrete signals using Multi-Layer Perceptrons (MLPs) with complex activation functions. While INRs achieve superior performance, they depend on full-precision number representation for accurate computation, resulting in significant hardware overhead. Previous INR quantization approaches have primarily focused on weight quantization, offering only limited hardware savings due to the lack of activation quantization. To fully exploit the hardware benefits of quantization, we propose DHQ, a novel distribution-aware Hadamard quantization scheme that targets both weights and activations in INRs. Our analysis shows that the weights in the first and last layers have distributions distinct from those in the intermediate layers, while the activations in the last layer differ significantly from those in the preceding layers. Instead of customizing quantizers individually, we utilize the Hadamard transformation to standardize these diverse distributions into a unified bell-shaped form, supported by both empirical evidence and theoretical analysis, before applying a standard quantizer. To demonstrate the practical advantages of our approach, we present an FPGA implementation of DHQ that highlights its hardware efficiency. Experiments on diverse image reconstruction tasks show that DHQ outperforms previous quantization methods, reducing latency by 32.7%, energy consumption by 40.1%, and resource utilization by up to 98.3% compared to full-precision counterparts. </p>
<blockquote>
<p>éšå¼ç¥ç»ç½‘ç»œè¡¨ç¤ºï¼ˆINRï¼‰ä½¿ç”¨å¤šå±‚æ„ŸçŸ¥å™¨ï¼ˆMLPï¼‰å’Œå¤æ‚çš„æ¿€æ´»å‡½æ•°æ¥ç¼–ç ç¦»æ•£ä¿¡å·ã€‚å°½ç®¡INRè¾¾åˆ°äº†ä¼˜è¶Šçš„æ€§èƒ½ï¼Œä½†å®ƒä»¬ä¾èµ–äºå…¨ç²¾åº¦æ•°å­—è¡¨ç¤ºæ¥è¿›è¡Œç²¾ç¡®è®¡ç®—ï¼Œå¯¼è‡´äº†æ˜¾è‘—çš„ç¡¬ä»¶å¼€é”€ã€‚å…ˆå‰çš„INRé‡åŒ–æ–¹æ³•ä¸»è¦é›†ä¸­åœ¨æƒé‡é‡åŒ–ä¸Šï¼Œç”±äºç¼ºä¹æ¿€æ´»é‡åŒ–ï¼Œåªèƒ½æä¾›æœ‰é™çš„ç¡¬ä»¶èŠ‚çœã€‚ä¸ºäº†å……åˆ†åˆ©ç”¨é‡åŒ–çš„ç¡¬ä»¶ä¼˜åŠ¿ï¼Œæˆ‘ä»¬æå‡ºäº†DHQï¼Œè¿™æ˜¯ä¸€ç§æ–°å‹åˆ†å¸ƒæ„ŸçŸ¥çš„Hadamardé‡åŒ–æ–¹æ¡ˆï¼Œæ—¨åœ¨é’ˆå¯¹INRä¸­çš„æƒé‡å’Œæ¿€æ´»ã€‚æˆ‘ä»¬çš„åˆ†æè¡¨æ˜ï¼Œç¬¬ä¸€å±‚å’Œæœ€åä¸€å±‚çš„æƒé‡åˆ†å¸ƒä¸ä¸­é—´å±‚çš„åˆ†å¸ƒä¸åŒï¼Œè€Œæœ€åä¸€å±‚çš„æ¿€æ´»ä¸å‰å‡ å±‚çš„æ¿€æ´»æœ‰å¾ˆå¤§å·®å¼‚ã€‚æˆ‘ä»¬å¹¶ä¸é’ˆå¯¹æ¯ç§é‡åŒ–å™¨è¿›è¡Œå•ç‹¬å®šåˆ¶ï¼Œè€Œæ˜¯åˆ©ç”¨Hadamardå˜æ¢å°†å¤šæ ·åŒ–çš„åˆ†å¸ƒæ ‡å‡†åŒ–ä¸ºç»Ÿä¸€çš„é’Ÿå½¢å½¢å¼ï¼Œä¸ºåç»­åº”ç”¨æ ‡å‡†é‡åŒ–å™¨æä¾›äº†å¯èƒ½ï¼Œè¿™ä¸€è¿‡ç¨‹å¾—åˆ°äº†å®éªŒè¯æ®å’Œç†è®ºåˆ†æçš„æ”¯æ’‘ã€‚ä¸ºäº†å±•ç¤ºæˆ‘ä»¬çš„æ–¹æ³•çš„å®é™…ä¼˜åŠ¿ï¼Œæˆ‘ä»¬å±•ç¤ºäº†DHQçš„FPGAå®ç°ï¼Œçªå‡ºäº†å…¶ç¡¬ä»¶æ•ˆç‡ã€‚åœ¨å¤šç§å›¾åƒé‡å»ºä»»åŠ¡ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒDHQä¼˜äºå…ˆå‰çš„é‡åŒ–æ–¹æ³•ï¼Œä¸å…¨ç²¾åº¦ç›¸æ¯”ï¼Œå»¶è¿Ÿé™ä½äº†32.7%ï¼Œèƒ½è€—é™ä½äº†40.1%ï¼Œèµ„æºåˆ©ç”¨ç‡æé«˜äº†é«˜è¾¾98.3%ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.13478v1">PDF</a> 6 pages, 7 figures</p>
<p><strong>Summary</strong></p>
<p>åŸºäºéšç¥ç»è¡¨ç¤ºï¼ˆINRsï¼‰ä½¿ç”¨å¤šå±‚æ„ŸçŸ¥å™¨ï¼ˆMLPsï¼‰å’Œå¤æ‚æ¿€æ´»å‡½æ•°ç¼–ç ç¦»æ•£ä¿¡å·çš„æ–¹æ³•å–å¾—äº†ä¼˜è¶Šçš„æ€§èƒ½ï¼Œä½†å…¶è®¡ç®—ä¾èµ–äºå…¨ç²¾åº¦æ•°å­—è¡¨ç¤ºï¼Œå¯¼è‡´ç¡¬ä»¶å¼€é”€è¾ƒå¤§ã€‚ä¸ºå……åˆ†åˆ©ç”¨é‡åŒ–çš„ç¡¬ä»¶ä¼˜åŠ¿ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°å‹åˆ†å¸ƒæ„ŸçŸ¥å“ˆè¾¾ç›é‡åŒ–æ–¹æ¡ˆDHQï¼ŒåŒæ—¶é’ˆå¯¹INRsä¸­çš„æƒé‡å’Œæ¿€æ´»è¿›è¡Œé‡åŒ–ã€‚DHQåˆ©ç”¨å“ˆè¾¾ç›å˜æ¢å°†ä¸åŒåˆ†å¸ƒæ ‡å‡†åŒ–ä¸ºç»Ÿä¸€çš„é’Ÿå½¢å½¢å¼ï¼Œå†åº”ç”¨æ ‡å‡†é‡åŒ–å™¨ã€‚å®éªŒè¡¨æ˜ï¼ŒDHQåœ¨å›¾åƒé‡å»ºä»»åŠ¡ä¸Šä¼˜äºå…ˆå‰çš„é‡åŒ–æ–¹æ³•ï¼Œé™ä½äº†å»¶è¿Ÿã€èƒ½è€—å’Œèµ„æºåˆ©ç”¨ç‡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>éšç¥ç»è¡¨ç¤ºï¼ˆINRsï¼‰ä½¿ç”¨å¤šå±‚æ„ŸçŸ¥å™¨ï¼ˆMLPsï¼‰å’Œå¤æ‚æ¿€æ´»å‡½æ•°ç¼–ç ç¦»æ•£ä¿¡å·ã€‚</li>
<li>INRsçš„è®¡ç®—ä¾èµ–äºå…¨ç²¾åº¦æ•°å­—è¡¨ç¤ºï¼Œå¯¼è‡´ç¡¬ä»¶å¼€é”€å¤§ã€‚</li>
<li>ç°æœ‰çš„INRé‡åŒ–æ–¹æ³•ä¸»è¦é›†ä¸­åœ¨æƒé‡é‡åŒ–ä¸Šï¼Œå¿½ç•¥äº†æ¿€æ´»é‡åŒ–ã€‚</li>
<li>DHQæ˜¯ä¸€ç§æ–°å‹çš„åˆ†å¸ƒæ„ŸçŸ¥å“ˆè¾¾ç›é‡åŒ–æ–¹æ¡ˆï¼Œé’ˆå¯¹INRsä¸­çš„æƒé‡å’Œæ¿€æ´»è¿›è¡Œé‡åŒ–ã€‚</li>
<li>DHQåˆ©ç”¨å“ˆè¾¾ç›å˜æ¢å°†ä¸åŒåˆ†å¸ƒæ ‡å‡†åŒ–ä¸ºç»Ÿä¸€çš„é’Ÿå½¢å½¢å¼ã€‚</li>
<li>DHQåœ¨å›¾åƒé‡å»ºä»»åŠ¡ä¸Šè¡¨ç°å‡ºä¼˜è¶Šçš„æ€§èƒ½ï¼Œç›¸è¾ƒäºå…¨ç²¾åº¦è®¡ç®—é™ä½äº†å»¶è¿Ÿã€èƒ½è€—å’Œèµ„æºåˆ©ç”¨ç‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.13478">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-8c240583c2799d7edad3176464c94f52.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-41e93d6023a124cb00792e6c5dc1da06.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-0fe5753315e63cda1c7a171fadc3ec86.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ca7e83f224cbeabc9849e3d5f164f7f2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b041b40a939a8cbbe055ec1b43e257cf.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2e73381f50f769be86cc4e1457aa6f92.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="Benchmarking-GPT-5-for-Zero-Shot-Multimodal-Medical-Reasoning-in-Radiology-and-Radiation-Oncology"><a href="#Benchmarking-GPT-5-for-Zero-Shot-Multimodal-Medical-Reasoning-in-Radiology-and-Radiation-Oncology" class="headerlink" title="Benchmarking GPT-5 for Zero-Shot Multimodal Medical Reasoning in   Radiology and Radiation Oncology"></a>Benchmarking GPT-5 for Zero-Shot Multimodal Medical Reasoning in   Radiology and Radiation Oncology</h2><p><strong>Authors:Mingzhe Hu, Zach Eidex, Shansong Wang, Mojtaba Safari, Qiang Li, Xiaofeng Yang</strong></p>
<p>Radiology, radiation oncology, and medical physics require decision-making that integrates medical images, textual reports, and quantitative data under high-stakes conditions. With the introduction of GPT-5, it is critical to assess whether recent advances in large multimodal models translate into measurable gains in these safety-critical domains. We present a targeted zero-shot evaluation of GPT-5 and its smaller variants (GPT-5-mini, GPT-5-nano) against GPT-4o across three representative tasks. We present a targeted zero-shot evaluation of GPT-5 and its smaller variants (GPT-5-mini, GPT-5-nano) against GPT-4o across three representative tasks: (1) VQA-RAD, a benchmark for visual question answering in radiology; (2) SLAKE, a semantically annotated, multilingual VQA dataset testing cross-modal grounding; and (3) a curated Medical Physics Board Examination-style dataset of 150 multiple-choice questions spanning treatment planning, dosimetry, imaging, and quality assurance. Across all datasets, GPT-5 achieved the highest accuracy, with substantial gains over GPT-4o up to +20.00% in challenging anatomical regions such as the chest-mediastinal, +13.60% in lung-focused questions, and +11.44% in brain-tissue interpretation. On the board-style physics questions, GPT-5 attained 90.7% accuracy (136&#x2F;150), exceeding the estimated human passing threshold, while GPT-4o trailed at 78.0%. These results demonstrate that GPT-5 delivers consistent and often pronounced performance improvements over GPT-4o in both image-grounded reasoning and domain-specific numerical problem-solving, highlighting its potential to augment expert workflows in medical imaging and therapeutic physics. </p>
<blockquote>
<p>æ”¾å°„å­¦ã€æ”¾å°„è‚¿ç˜¤å­¦å’ŒåŒ»å­¦ç‰©ç†å­¦éœ€è¦åœ¨é«˜é£é™©çš„æ¡ä»¶ä¸‹ï¼Œç»“åˆåŒ»å­¦å›¾åƒã€æ–‡æœ¬æŠ¥å‘Šå’Œå®šé‡æ•°æ®è¿›è¡Œå†³ç­–ã€‚éšç€GPT-5çš„æ¨å‡ºï¼Œè¯„ä¼°è¿‘æœŸå¤§å‹å¤šæ¨¡å¼æ¨¡å‹çš„è¿›å±•æ˜¯å¦èƒ½ä¸ºè¿™äº›å®‰å…¨å…³é”®çš„é¢†åŸŸå¸¦æ¥å¯è¡¡é‡çš„æ”¶ç›Šè‡³å…³é‡è¦ã€‚æˆ‘ä»¬é’ˆå¯¹ä¸‰é¡¹ä»£è¡¨æ€§ä»»åŠ¡ï¼Œå¯¹GPT-5åŠå…¶å°å‹å˜ä½“ï¼ˆGPT-5-miniã€GPT-5-nanoï¼‰ä¸GPT-4oè¿›è¡Œäº†æœ‰é’ˆå¯¹æ€§çš„é›¶æ ·æœ¬è¯„ä¼°ã€‚</p>
</blockquote>
<p>ä¸€æ˜¯VQA-RADï¼Œè¿™æ˜¯æ”¾å°„å­¦ä¸­è§†è§‰é—®ç­”çš„åŸºå‡†æµ‹è¯•ï¼›äºŒæ˜¯SLAKEï¼Œè¿™æ˜¯ä¸€ä¸ªè¯­ä¹‰æ³¨é‡Šã€å¤šè¯­è¨€é—®ç­”æ•°æ®é›†ï¼Œç”¨äºæµ‹è¯•è·¨æ¨¡æ€å®šä½ï¼›ä¸‰æ˜¯ç²¾é€‰çš„åŒ»å­¦ç‰©ç†å§”å‘˜ä¼šè€ƒè¯•é£æ ¼æ•°æ®é›†ï¼ŒåŒ…å«150é“å…³äºæ²»ç–—è®¡åˆ’ã€å‰‚é‡æµ‹å®šã€æˆåƒå’Œè´¨é‡ä¿è¯çš„å¤šé¡¹é€‰æ‹©é¢˜ã€‚</p>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.13192v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä¸»è¦è¯„ä¼°äº†GPT-5åŠå…¶å°å‹ç‰ˆæœ¬ï¼ˆGPT-5-miniã€GPT-5-nanoï¼‰åœ¨æ”¾å°„å­¦ã€æ”¾å°„è‚¿ç˜¤å­¦å’ŒåŒ»å­¦ç‰©ç†ç­‰é¢†åŸŸå†…çš„å†³ç­–èƒ½åŠ›ï¼Œé€šè¿‡ä¸GPT-4oçš„æ¯”è¾ƒï¼Œå±•ç¤ºäº†GPT-5åœ¨è¿™äº›å®‰å…¨å…³é”®é¢†åŸŸä¸­çš„æ˜¾è‘—æ€§èƒ½æå‡ã€‚åœ¨è§†è§‰é—®ç­”ã€è·¨æ¨¡æ€æ¥åœ°æµ‹è¯•ä»¥åŠåŒ»å­¦ç‰©ç†è€ƒè¯•é£æ ¼çš„æ•°æ®é›†ä¸Šï¼ŒGPT-5å‡è¡¨ç°å‡ºæœ€é«˜å‡†ç¡®åº¦ï¼Œä¸”åœ¨ç‰¹å®šåŒºåŸŸå’Œé—®é¢˜ä¸Šè¾ƒGPT-4oæœ‰æ˜¾è‘—æé«˜ã€‚æœ€ç»ˆï¼Œç ”ç©¶è¡¨æ˜GPT-5å…·æœ‰å¢å¼ºåŒ»å­¦å½±åƒå’ŒåŒ»æ²»ç‰©ç†å·¥ä½œæµç¨‹çš„æ½œåŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>GPT-5åœ¨æ”¾å°„å­¦ã€æ”¾å°„è‚¿ç˜¤å­¦å’ŒåŒ»å­¦ç‰©ç†ç­‰é¢†åŸŸå±•ç°å‡ºå¼ºå¤§çš„å†³ç­–èƒ½åŠ›ã€‚</li>
<li>GPT-5åŠå…¶å°å‹ç‰ˆæœ¬åœ¨å¤šä¸ªä»£è¡¨æ€§ä»»åŠ¡ä¸­è¡¨ç°å‡ºæ¯”GPT-4oæ›´é«˜çš„å‡†ç¡®æ€§ã€‚</li>
<li>åœ¨è§†è§‰é—®ç­”ã€è·¨æ¨¡æ€æ¥åœ°æµ‹è¯•å’ŒåŒ»å­¦ç‰©ç†è€ƒè¯•é£æ ¼çš„æ•°æ®é›†ä¸Šï¼ŒGPT-5è¾¾åˆ°æœ€é«˜å‡†ç¡®åº¦ã€‚</li>
<li>GPT-5åœ¨ç‰¹å®šåŒºåŸŸå’Œé—®é¢˜ä¸Šè¾ƒGPT-4oæœ‰æ˜¾è‘—æé«˜ï¼Œå¦‚åœ¨æŒ‘æˆ˜åŒºåŸŸè§£å‰–å­¦ã€è‚ºéƒ¨é—®é¢˜å’Œè„‘ç»„ç»‡è§£é‡Šæ–¹é¢ã€‚</li>
<li>GPT-5åœ¨åŒ»å­¦ç‰©ç†è€ƒè¯•é£æ ¼çš„é—®é¢˜ä¸­è¾¾åˆ°90.7%çš„å‡†ç¡®ç‡ï¼Œè¶…è¿‡ä¼°è®¡çš„äººç±»åŠæ ¼é—¨æ§›ã€‚</li>
<li>GPT-4oåœ¨å¤šä¸ªä»»åŠ¡ä¸­çš„è¡¨ç°ä¸åŠGPT-5ï¼Œå°¤å…¶åœ¨å¤æ‚é—®é¢˜ä¸Šã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.13192">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-67e66eb3e9ef8338002d7084b062da2e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-89567955330e389f4a0664fb86fb080b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-74a0136430b7471b95bb168278e38c42.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-702ac3efea405747b376bd7c0c68a3e7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9bc022abc216e5e990e5793e5a752955.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c75a1e9fe98e9ec045b7e5b622db495b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-4c7f651f1878244ff0b0c11cac3954d7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-cdabee3480d81737282b391f638dd660.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="Fully-Automated-Segmentation-of-Fiber-Bundles-in-Anatomic-Tracing-Data"><a href="#Fully-Automated-Segmentation-of-Fiber-Bundles-in-Anatomic-Tracing-Data" class="headerlink" title="Fully Automated Segmentation of Fiber Bundles in Anatomic Tracing Data"></a>Fully Automated Segmentation of Fiber Bundles in Anatomic Tracing Data</h2><p><strong>Authors:Kyriaki-Margarita Bintsi, YaÃ«l Balbastre, Jingjing Wu, Julia F. Lehman, Suzanne N. Haber, Anastasia Yendiki</strong></p>
<p>Anatomic tracer studies are critical for validating and improving diffusion MRI (dMRI) tractography. However, large-scale analysis of data from such studies is hampered by the labor-intensive process of annotating fiber bundles manually on histological slides. Existing automated methods often miss sparse bundles or require complex post-processing across consecutive sections, limiting their flexibility and generalizability. We present a streamlined, fully automated framework for fiber bundle segmentation in macaque tracer data, based on a U-Net architecture with large patch sizes, foreground aware sampling, and semisupervised pre-training. Our approach eliminates common errors such as mislabeling terminals as bundles, improves detection of sparse bundles by over 20% and reduces the False Discovery Rate (FDR) by 40% compared to the state-of-the-art, all while enabling analysis of standalone slices. This new framework will facilitate the automated analysis of anatomic tracing data at a large scale, generating more ground-truth data that can be used to validate and optimize dMRI tractography methods. </p>
<blockquote>
<p>è§£å‰–è¿½è¸ªç ”ç©¶å¯¹äºéªŒè¯å’Œæ”¹è¿›æ‰©æ•£MRIï¼ˆdMRIï¼‰å›¾è°±æŠ€æœ¯è‡³å…³é‡è¦ã€‚ç„¶è€Œï¼Œæ­¤ç±»ç ”ç©¶çš„æ•°æ®å¤§è§„æ¨¡åˆ†æå—åˆ°æ‰‹åŠ¨åœ¨ç»„ç»‡åˆ‡ç‰‡ä¸Šæ³¨é‡Šçº¤ç»´æŸçš„åŠ³åŠ¨å¼ºåº¦å¤§çš„é˜»ç¢ã€‚ç°æœ‰çš„è‡ªåŠ¨åŒ–æ–¹æ³•å¾€å¾€ä¼šé—æ¼ç¨€ç–çš„æŸæˆ–éœ€è¦åœ¨è¿ç»­çš„åˆ‡ç‰‡ä¸Šè¿›è¡Œå¤æ‚çš„åå¤„ç†ï¼Œè¿™é™åˆ¶äº†å…¶çµæ´»æ€§å’Œé€šç”¨æ€§ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªåœ¨çŒ•çŒ´è¿½è¸ªæ•°æ®ä¸­çº¤ç»´æŸåˆ†å‰²çš„ç®€åŒ–ã€å…¨è‡ªåŠ¨æ¡†æ¶ï¼Œè¯¥æ¡†æ¶åŸºäºå…·æœ‰å¤§è¡¥ä¸å¤§å°ã€å‰æ™¯æ„ŸçŸ¥é‡‡æ ·å’ŒåŠç›‘ç£é¢„è®­ç»ƒçš„U-Netæ¶æ„ã€‚æˆ‘ä»¬çš„æ–¹æ³•æ¶ˆé™¤äº†å¸¸è§çš„é”™è¯¯ï¼Œå¦‚å°†ç»ˆç«¯è¯¯æ ‡è®°ä¸ºæŸï¼Œæé«˜äº†å¯¹ç¨€ç–æŸçš„æ£€æµ‹ç‡è¶…è¿‡20%ï¼Œå¹¶ä¸”ä¸æœ€æ–°æŠ€æœ¯ç›¸æ¯”ï¼Œé™ä½äº†40%çš„è¯¯æŠ¥ç‡ï¼ˆFDRï¼‰ï¼ŒåŒæ—¶æ”¯æŒå•ç‹¬åˆ‡ç‰‡çš„åˆ†æã€‚è¿™ä¸€æ–°æ¡†æ¶å°†ä¿ƒè¿›è§£å‰–è¿½è¸ªæ•°æ®çš„è‡ªåŠ¨å¤§è§„æ¨¡åˆ†æï¼Œç”Ÿæˆæ›´å¤šçš„çœŸå®æ•°æ®ï¼Œå¯ç”¨äºéªŒè¯å’Œä¼˜åŒ–dMRIå›¾è°±æŠ€æœ¯ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.12942v2">PDF</a> Accepted at CDMRI, MICCAI 2025</p>
<p><strong>Summary</strong></p>
<p>åŸºäºU-Netæ¶æ„ã€å¤§è¡¥ä¸å°ºå¯¸ã€å‰æ™¯æ„ŸçŸ¥é‡‡æ ·å’ŒåŠç›‘ç£é¢„è®­ç»ƒï¼Œæå‡ºä¸€ä¸ªç®€åŒ–ã€å…¨è‡ªåŠ¨åŒ–çš„æ¡†æ¶ï¼Œç”¨äºçŒ•çŒ´è¿½è¸ªæ•°æ®çš„çº¤ç»´æŸåˆ†å‰²ã€‚è¯¥æ–¹æ³•æ¶ˆé™¤äº†è¯¯æ ‡è®°ç»ˆç«¯ä¸ºæŸçš„å¸¸è§é”™è¯¯ï¼Œæé«˜äº†ç¨€ç–æŸçš„æ£€æµ‹ç‡ï¼Œé™ä½äº†è¯¯æŠ¥ç‡ï¼Œå¹¶èƒ½å¤Ÿå®ç°ç‹¬ç«‹åˆ‡ç‰‡çš„åˆ†æã€‚æ­¤æ–°æ¡†æ¶å°†ä¿ƒè¿›å¤§è§„æ¨¡è‡ªåŠ¨åˆ†æè§£å‰–è¿½è¸ªæ•°æ®ï¼Œç”Ÿæˆæ›´å¤šå¯ç”¨äºéªŒè¯å’Œä¼˜åŒ–æ‰©æ•£MRIæˆåƒæ–¹æ³•çš„åœ°é¢çœŸå®æ•°æ®ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>U-Netæ¶æ„ç»“åˆäº†å¤§è§„æ¨¡æ•°æ®å¤„ç†çš„èƒ½åŠ›ä¸å‰ç³æ„ŸçŸ¥é‡‡æ ·æŠ€æœ¯ç”¨äºçº¤ç»´æŸåˆ†å‰²ã€‚</li>
<li>æ­¤æ–¹æ³•å®ç°äº†å…¨è‡ªåŠ¨åŒ–çš„çº¤ç»´æŸåˆ†å‰²ï¼Œæé«˜äº†æ£€æµ‹ç¨€ç–çº¤ç»´æŸçš„æ•ˆç‡ã€‚</li>
<li>ä¸ç°æœ‰æŠ€æœ¯ç›¸æ¯”ï¼Œæ–°æ–¹æ³•é™ä½äº†è¯¯æŠ¥ç‡ï¼ˆFDRï¼‰å¹¶æ¶ˆé™¤äº†è¯¯æ ‡è®°ç»ˆç«¯ä¸ºçº¤ç»´æŸçš„é”™è¯¯ã€‚</li>
<li>æ–°æ¡†æ¶é€‚ç”¨äºç‹¬ç«‹åˆ‡ç‰‡çš„åˆ†æï¼Œå¢å¼ºäº†å…¶çµæ´»æ€§å’Œé€šç”¨æ€§ã€‚</li>
<li>è¯¥æ–¹æ³•æœ‰åŠ©äºå¤§è§„æ¨¡è‡ªåŠ¨åˆ†æè§£å‰–è¿½è¸ªæ•°æ®ï¼Œç”Ÿæˆæ›´å¤šåœ°é¢çœŸå®æ•°æ®ã€‚</li>
<li>è¿™äº›åœ°é¢çœŸå®æ•°æ®å¯ç”¨äºéªŒè¯å’Œä¼˜åŒ–æ‰©æ•£MRIæˆåƒï¼ˆdMRIï¼‰çš„è¿½è¸ªæ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.12942">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-b38490380fe29de5056ba4c50f0ca948.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0326875ea5dd1df30492175b513c6aba.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="SRMA-Mamba-Spatial-Reverse-Mamba-Attention-Network-for-Pathological-Liver-Segmentation-in-MRI-Volumes"><a href="#SRMA-Mamba-Spatial-Reverse-Mamba-Attention-Network-for-Pathological-Liver-Segmentation-in-MRI-Volumes" class="headerlink" title="SRMA-Mamba: Spatial Reverse Mamba Attention Network for Pathological   Liver Segmentation in MRI Volumes"></a>SRMA-Mamba: Spatial Reverse Mamba Attention Network for Pathological   Liver Segmentation in MRI Volumes</h2><p><strong>Authors:Jun Zeng, Yannan Huang, Elif Keles, Halil Ertugrul Aktas, Gorkem Durak, Nikhil Kumar Tomar, Quoc-Huy Trinh, Deepak Ranjan Nayak, Ulas Bagci, Debesh Jha</strong></p>
<p>Liver Cirrhosis plays a critical role in the prognosis of chronic liver disease. Early detection and timely intervention are critical in significantly reducing mortality rates. However, the intricate anatomical architecture and diverse pathological changes of liver tissue complicate the accurate detection and characterization of lesions in clinical settings. Existing methods underutilize the spatial anatomical details in volumetric MRI data, thereby hindering their clinical effectiveness and explainability. To address this challenge, we introduce a novel Mamba-based network, SRMA-Mamba, designed to model the spatial relationships within the complex anatomical structures of MRI volumes. By integrating the Spatial Anatomy-Based Mamba module (SABMamba), SRMA-Mamba performs selective Mamba scans within liver cirrhotic tissues and combines anatomical information from the sagittal, coronal, and axial planes to construct a global spatial context representation, enabling efficient volumetric segmentation of pathological liver structures. Furthermore, we introduce the Spatial Reverse Attention module (SRMA), designed to progressively refine cirrhotic details in the segmentation map, utilizing both the coarse segmentation map and hierarchical encoding features. Extensive experiments demonstrate that SRMA-Mamba surpasses state-of-the-art methods, delivering exceptional performance in 3D pathological liver segmentation. Our code is available for public: <a target="_blank" rel="noopener" href="https://github.com/JunZengz/SRMA-Mamba">https://github.com/JunZengz/SRMA-Mamba</a>. </p>
<blockquote>
<p>è‚ç¡¬åŒ–åœ¨æ…¢æ€§è‚ç—…çš„é¢„åä¸­æ‰®æ¼”ç€é‡è¦è§’è‰²ã€‚æ—©æœŸæ£€æµ‹å’ŒåŠæ—¶å¹²é¢„æ˜¯æ˜¾è‘—é™ä½æ­»äº¡ç‡çš„å…³é”®ã€‚ç„¶è€Œï¼Œè‚è„ç»„ç»‡çš„å¤æ‚è§£å‰–ç»“æ„å’Œå¤šæ ·çš„ç—…ç†å˜åŒ–ä½¿ä¸´åºŠç¯å¢ƒä¸­ç—…å˜çš„å‡†ç¡®æ£€æµ‹å’Œç‰¹å¾æè¿°å˜å¾—å¤æ‚ã€‚ç°æœ‰æ–¹æ³•æœªèƒ½å……åˆ†åˆ©ç”¨ä½“ç§¯MRIæ•°æ®ä¸­çš„ç©ºé—´è§£å‰–ç»†èŠ‚ï¼Œä»è€Œé˜»ç¢äº†å…¶åœ¨ä¸´åºŠä¸Šçš„æ•ˆæœå’Œå¯è§£é‡Šæ€§ã€‚ä¸ºäº†åº”å¯¹è¿™ä¸€æŒ‘æˆ˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§æ–°å‹çš„åŸºäºMambaçš„ç½‘ç»œï¼Œåä¸ºSRMA-Mambaï¼Œæ—¨åœ¨æ¨¡æ‹ŸMRIä½“ç§¯ä¸­å¤æ‚è§£å‰–ç»“æ„å†…çš„ç©ºé—´å…³ç³»ã€‚é€šè¿‡é›†æˆåŸºäºç©ºé—´è§£å‰–çš„Mambaæ¨¡å—ï¼ˆSABMambaï¼‰ï¼ŒSRMA-Mambaåœ¨è‚ç¡¬åŒ–ç»„ç»‡å†…è¿›è¡Œé€‰æ‹©æ€§Mambaæ‰«æï¼Œå¹¶ç»“åˆæ¥è‡ªçŸ¢çŠ¶é¢ã€å† çŠ¶é¢å’Œè½´é¢çš„è§£å‰–ä¿¡æ¯ï¼Œæ„å»ºå…¨å±€ç©ºé—´ä¸Šä¸‹æ–‡è¡¨ç¤ºï¼Œä»è€Œå®ç°ç—…ç†è‚è„ç»“æ„çš„é«˜æ•ˆä½“ç§¯åˆ†å‰²ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜å¼•å…¥äº†ç©ºé—´åå‘æ³¨æ„åŠ›æ¨¡å—ï¼ˆSRMAï¼‰ï¼Œæ—¨åœ¨åˆ©ç”¨ç²—åˆ†å‰²å›¾å’Œåˆ†å±‚ç¼–ç ç‰¹å¾é€æ­¥ä¼˜åŒ–è‚ç¡¬åŒ–ç»†èŠ‚åœ¨åˆ†å‰²å›¾ä¸Šçš„è¡¨ç°ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼ŒSRMA-Mambaè¶…è¶Šäº†æœ€å…ˆè¿›çš„æ–¹æ³•ï¼Œåœ¨3Dç—…ç†è‚è„åˆ†å‰²æ–¹é¢è¡¨ç°å‡ºå“è¶Šçš„æ€§èƒ½ã€‚æˆ‘ä»¬çš„ä»£ç å·²å…¬å¼€ï¼š<a target="_blank" rel="noopener" href="https://github.com/JunZengz/SRMA-Mamba%E3%80%82">https://github.com/JunZengz/SRMA-Mambaã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.12410v2">PDF</a> 9 pages, 4 figures</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†è‚è„ç¡¬åŒ–åœ¨æ…¢æ€§è‚ç—…é¢„åä¸­çš„é‡è¦ä½œç”¨ï¼Œå¼ºè°ƒæ—©æœŸæ£€æµ‹å’ŒåŠæ—¶å¹²é¢„å¯¹é™ä½æ­»äº¡ç‡çš„é‡è¦æ€§ã€‚é’ˆå¯¹è‚è„ç»„ç»‡å¤æ‚è§£å‰–ç»“æ„å’Œå¤šå˜ç—…ç†å˜åŒ–å¯¼è‡´çš„ä¸´åºŠå‡†ç¡®æ£€æµ‹å’Œè¡¨å¾ç—…ç¶çš„å›°éš¾ï¼Œæå‡ºäº†ä¸€ç§åŸºäºMambaç½‘ç»œçš„æ–°æ–¹æ³•SRMA-Mambaã€‚è¯¥æ–¹æ³•é€šè¿‡æ•´åˆç©ºé—´è§£å‰–åŸºç¡€ä¸Šçš„Mambaæ¨¡å—ï¼ˆSABMambaï¼‰ï¼Œèƒ½å¤Ÿåœ¨è‚è„ç¡¬åŒ–ç»„ç»‡ä¸­è¿›è¡Œé€‰æ‹©æ€§Mambaæ‰«æï¼Œå¹¶ç»“åˆçŸ¢çŠ¶é¢ã€å† çŠ¶é¢å’Œè½´é¢çš„è§£å‰–ä¿¡æ¯æ„å»ºå…¨å±€ç©ºé—´ä¸Šä¸‹æ–‡è¡¨ç¤ºï¼Œå®ç°ç—…ç†è‚è„ç»“æ„çš„é«˜æ•ˆä¸‰ç»´åˆ†å‰²ã€‚æ­¤å¤–ï¼Œè¿˜å¼•å…¥äº†ç©ºé—´åå‘æ³¨æ„åŠ›æ¨¡å—ï¼ˆSRMAï¼‰ï¼Œç”¨äºé€æ­¥ä¼˜åŒ–åˆ†å‰²å›¾ä¸­çš„è‚ç¡¬åŒ–ç»†èŠ‚ã€‚å®éªŒè¡¨æ˜ï¼ŒSRMA-Mambaè¶…è¶Šäº†æœ€æ–°æ–¹æ³•ï¼Œåœ¨ä¸‰ç»´ç—…ç†è‚è„åˆ†å‰²ä¸­è¡¨ç°å‡ºå“è¶Šæ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è‚è„ç¡¬åŒ–åœ¨æ…¢æ€§è‚ç—…é¢„åä¸­èµ·å…³é”®ä½œç”¨ï¼Œå¼ºè°ƒæ—©æœŸæ£€æµ‹å’ŒåŠæ—¶å¹²é¢„çš„é‡è¦æ€§ã€‚</li>
<li>ç°æœ‰æ–¹æ³•æœªèƒ½å……åˆ†åˆ©ç”¨MRIæ•°æ®çš„ç©ºé—´è§£å‰–ç»†èŠ‚ï¼Œå½±å“ä¸´åºŠæ•ˆæœå’Œè§£é‡Šæ€§ã€‚</li>
<li>æå‡ºäº†ä¸€ç§æ–°çš„åŸºäºMambaç½‘ç»œçš„SRMA-Mambaæ–¹æ³•ï¼Œç”¨äºå»ºæ¨¡MRIä½“ç§¯å†…å¤æ‚è§£å‰–ç»“æ„çš„ç©ºé—´å…³ç³»ã€‚</li>
<li>SABMambaæ¨¡å—å®ç°é€‰æ‹©æ€§Mambaæ‰«æï¼Œç»“åˆå¤šå¹³é¢è§£å‰–ä¿¡æ¯æ„å»ºå…¨å±€ç©ºé—´ä¸Šä¸‹æ–‡è¡¨ç¤ºã€‚</li>
<li>SRMAæ¨¡å—ç”¨äºé€æ­¥ä¼˜åŒ–åˆ†å‰²å›¾ä¸­çš„è‚ç¡¬åŒ–ç»†èŠ‚ï¼Œæé«˜åˆ†å‰²æ€§èƒ½ã€‚</li>
<li>SRMA-Mambaåœ¨ä¸‰ç»´ç—…ç†è‚è„åˆ†å‰²æ–¹é¢è¡¨ç°å‡ºå“è¶Šæ€§èƒ½ï¼Œè¶…è¶Šç°æœ‰æ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.12410">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-18eaaec98384d7476e85f461b3e08173.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-69186425c99836b908c282b775ed1704.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8cf0a33f0fe572d94e274670bd493564.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b0d42d54ab668328775ad1bef64db6d2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-16b430d57bfe2cf3cd17c0e8f41e904f.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="KonfAI-A-Modular-and-Fully-Configurable-Framework-for-Deep-Learning-in-Medical-Imaging"><a href="#KonfAI-A-Modular-and-Fully-Configurable-Framework-for-Deep-Learning-in-Medical-Imaging" class="headerlink" title="KonfAI: A Modular and Fully Configurable Framework for Deep Learning in   Medical Imaging"></a>KonfAI: A Modular and Fully Configurable Framework for Deep Learning in   Medical Imaging</h2><p><strong>Authors:Valentin Boussot, Jean-Louis Dillenseger</strong></p>
<p>KonfAI is a modular, extensible, and fully configurable deep learning framework specifically designed for medical imaging tasks. It enables users to define complete training, inference, and evaluation workflows through structured YAML configuration files, without modifying the underlying code. This declarative approach enhances reproducibility, transparency, and experimental traceability while reducing development time. Beyond the capabilities of standard pipelines, KonfAI provides native abstractions for advanced strategies including patch-based learning, test-time augmentation, model ensembling, and direct access to intermediate feature representations for deep supervision. It also supports complex multi-model training setups such as generative adversarial architectures. Thanks to its modular and extensible architecture, KonfAI can easily accommodate custom models, loss functions, and data processing components. The framework has been successfully applied to segmentation, registration, and image synthesis tasks, and has contributed to top-ranking results in several international medical imaging challenges. KonfAI is open source and available at \href{<a target="_blank" rel="noopener" href="https://github.com/vboussot/KonfAI%7D%7Bhttps://github.com/vboussot/KonfAI%7D">https://github.com/vboussot/KonfAI}{https://github.com/vboussot/KonfAI}</a>. </p>
<blockquote>
<p>KonfAIæ˜¯ä¸€ä¸ªä¸“ä¸ºåŒ»å­¦æˆåƒä»»åŠ¡è®¾è®¡çš„æ¨¡å—åŒ–ã€å¯æ‰©å±•å’Œå¯å®Œå…¨é…ç½®çš„æ·±åº¦å­¦ä¹ æ¡†æ¶ã€‚å®ƒä½¿ç”¨æˆ·èƒ½å¤Ÿé€šè¿‡ç»“æ„åŒ–çš„YAMLé…ç½®æ–‡ä»¶å®šä¹‰å®Œæ•´çš„è®­ç»ƒã€æ¨ç†å’Œè¯„ä¼°å·¥ä½œæµç¨‹ï¼Œè€Œæ— éœ€ä¿®æ”¹åº•å±‚ä»£ç ã€‚è¿™ç§å£°æ˜å¼æ–¹æ³•æé«˜äº†å¯é‡å¤æ€§ã€é€æ˜åº¦å’Œå®éªŒå¯è¿½æº¯æ€§ï¼ŒåŒæ—¶å‡å°‘äº†å¼€å‘æ—¶é—´ã€‚é™¤äº†æ ‡å‡†ç®¡é“çš„åŠŸèƒ½å¤–ï¼ŒKonfAIè¿˜æä¾›æœ¬åœ°æŠ½è±¡ä»¥æ”¯æŒé«˜çº§ç­–ç•¥ï¼ŒåŒ…æ‹¬åŸºäºè¡¥ä¸çš„å­¦ä¹ ã€æµ‹è¯•æ—¶å¢å¼ºã€æ¨¡å‹é›†æˆå’Œæ·±åº¦ç›‘ç£çš„ç›´æ¥è®¿é—®ä¸­é—´ç‰¹å¾è¡¨ç¤ºã€‚å®ƒè¿˜æ”¯æŒå¤æ‚çš„å¤šæ¨¡å‹è®­ç»ƒè®¾ç½®ï¼Œå¦‚ç”Ÿæˆå¯¹æŠ—æ¶æ„ã€‚ç”±äºå…¶æ¨¡å—åŒ–å’Œå¯æ‰©å±•çš„æ¶æ„ï¼ŒKonfAIå¯ä»¥è½»æ¾å®¹çº³è‡ªå®šä¹‰æ¨¡å‹ã€æŸå¤±å‡½æ•°å’Œæ•°æ®å¤„ç†ç»„ä»¶ã€‚è¯¥æ¡†æ¶å·²æˆåŠŸåº”ç”¨äºåˆ†å‰²ã€æ³¨å†Œå’Œå›¾åƒåˆæˆä»»åŠ¡ï¼Œå¹¶ä¸ºå¤šä¸ªå›½é™…åŒ»å­¦æˆåƒæŒ‘æˆ˜å¸¦æ¥äº†æ’åé å‰çš„ç»“æœã€‚KonfAIæ˜¯å¼€æºçš„ï¼Œå¯åœ¨[<a target="_blank" rel="noopener" href="https://github.com/vboussot/KonfAI]%E5%A4%84%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/vboussot/KonfAI]å¤„è·å–ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.09823v1">PDF</a> <a target="_blank" rel="noopener" href="https://github.com/vboussot/KonfAI">https://github.com/vboussot/KonfAI</a></p>
<p><strong>Summary</strong><br>     åº·è²AIæ˜¯ä¸€ä¸ªæ¨¡å—åŒ–ã€å¯æ‰©å±•ä¸”å¯é…ç½®çš„æ·±åº¦å­¦ä¹ æ¡†æ¶ï¼Œä¸“ä¸ºåŒ»å­¦å½±åƒä»»åŠ¡è®¾è®¡ã€‚å®ƒä½¿ç”¨æˆ·èƒ½å¤Ÿé€šè¿‡ç»“æ„åŒ–YAMLé…ç½®æ–‡ä»¶å®šä¹‰å®Œæ•´çš„è®­ç»ƒã€æ¨ç†å’Œè¯„ä¼°æµç¨‹ï¼Œæ— éœ€ä¿®æ”¹åº•å±‚ä»£ç ã€‚è¿™ä¸€å£°æ˜å¼æ–¹æ³•æé«˜äº†é‡å¤æ€§ã€é€æ˜æ€§å’Œå®éªŒå¯è¿½æº¯æ€§ï¼Œå¹¶ç¼©çŸ­äº†å¼€å‘æ—¶é—´ã€‚æ­¤å¤–ï¼Œåº·è²AIè¿˜æä¾›åŸç”ŸæŠ½è±¡ï¼Œæ”¯æŒé«˜çº§ç­–ç•¥å¦‚åŸºäºè¡¥ä¸çš„å­¦ä¹ ã€æµ‹è¯•æ—¶å¢å¼ºã€æ¨¡å‹é›†æˆå’Œæ·±åº¦ç›‘ç£çš„ç›´æ¥è®¿é—®ä¸­é—´ç‰¹å¾è¡¨ç¤ºï¼Œå¹¶æ”¯æŒå¤æ‚çš„å¤šæ¨¡å‹è®­ç»ƒè®¾ç½®ï¼Œå¦‚ç”Ÿæˆå¯¹æŠ—æ¶æ„ã€‚å…¶æ¨¡å—åŒ–å¯æ‰©å±•æ¶æ„å¯è½»æ¾å®¹çº³è‡ªå®šä¹‰æ¨¡å‹ã€æŸå¤±å‡½æ•°å’Œæ•°æ®å¤„ç†ç»„ä»¶ã€‚è¯¥æ¡†æ¶å·²æˆåŠŸåº”ç”¨äºåˆ†å‰²ã€æ³¨å†Œå’Œå›¾åƒåˆæˆä»»åŠ¡ï¼Œå¹¶åœ¨å¤šä¸ªå›½é™…åŒ»å­¦å½±åƒæŒ‘æˆ˜ä¸­å–å¾—ååˆ—å‰èŒ…çš„ç»“æœã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>åº·è²AIæ˜¯ä¸€ä¸ªç”¨äºåŒ»å­¦å½±åƒä»»åŠ¡çš„æ·±åº¦å­¦ä¹ æ¡†æ¶ã€‚</li>
<li>å®ƒé€šè¿‡ç»“æ„åŒ–YAMLé…ç½®æ–‡ä»¶è¿›è¡Œé…ç½®ï¼Œç”¨æˆ·æ— éœ€ä¿®æ”¹åº•å±‚ä»£ç ã€‚</li>
<li>åº·è²AIé‡‡ç”¨å£°æ˜å¼æ–¹æ³•ï¼Œæé«˜é‡å¤æ€§ã€é€æ˜æ€§å’Œå®éªŒå¯è¿½æº¯æ€§ã€‚</li>
<li>è¯¥æ¡†æ¶æ”¯æŒé«˜çº§ç­–ç•¥ï¼Œå¦‚åŸºäºè¡¥ä¸çš„å­¦ä¹ ã€æµ‹è¯•æ—¶å¢å¼ºå’Œæ¨¡å‹é›†æˆã€‚</li>
<li>åº·è²AIæä¾›å¯¹ä¸­é—´ç‰¹å¾è¡¨ç¤ºçš„æ·±åº¦ç›‘ç£çš„ç›´æ¥è®¿é—®ã€‚</li>
<li>å®ƒæ”¯æŒå¤æ‚çš„å¤šæ¨¡å‹è®­ç»ƒè®¾ç½®ï¼ŒåŒ…æ‹¬ç”Ÿæˆå¯¹æŠ—æ¶æ„ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.09823">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-c81d8a84b45cfff8029118c18a621ba3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-11bb7683b6251c8f65db59588d18c415.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-33b241cb061d40887a99d2862a7238a4.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="A-Chain-of-Diagnosis-Framework-for-Accurate-and-Explainable-Radiology-Report-Generation"><a href="#A-Chain-of-Diagnosis-Framework-for-Accurate-and-Explainable-Radiology-Report-Generation" class="headerlink" title="A Chain of Diagnosis Framework for Accurate and Explainable Radiology   Report Generation"></a>A Chain of Diagnosis Framework for Accurate and Explainable Radiology   Report Generation</h2><p><strong>Authors:Haibo Jin, Haoxuan Che, Sunan He, Hao Chen</strong></p>
<p>Despite the progress of radiology report generation (RRG), existing works face two challenges: 1) The performances in clinical efficacy are unsatisfactory, especially for lesion attributes description; 2) the generated text lacks explainability, making it difficult for radiologists to trust the results. To address the challenges, we focus on a trustworthy RRG model, which not only generates accurate descriptions of abnormalities, but also provides basis of its predictions. To this end, we propose a framework named chain of diagnosis (CoD), which maintains a chain of diagnostic process for clinically accurate and explainable RRG. It first generates question-answer (QA) pairs via diagnostic conversation to extract key findings, then prompts a large language model with QA diagnoses for accurate generation. To enhance explainability, a diagnosis grounding module is designed to match QA diagnoses and generated sentences, where the diagnoses act as a reference. Moreover, a lesion grounding module is designed to locate abnormalities in the image, further improving the working efficiency of radiologists. To facilitate label-efficient training, we propose an omni-supervised learning strategy with clinical consistency to leverage various types of annotations from different datasets. Our efforts lead to 1) an omni-labeled RRG dataset with QA pairs and lesion boxes; 2) a evaluation tool for assessing the accuracy of reports in describing lesion location and severity; 3) extensive experiments to demonstrate the effectiveness of CoD, where it outperforms both specialist and generalist models consistently on two RRG benchmarks and shows promising explainability by accurately grounding generated sentences to QA diagnoses and images. </p>
<blockquote>
<p>å°½ç®¡æ”¾å°„å­¦æŠ¥å‘Šç”Ÿæˆï¼ˆRRGï¼‰æœ‰æ‰€è¿›å±•ï¼Œä½†ç°æœ‰å·¥ä½œé¢ä¸´ä¸¤å¤§æŒ‘æˆ˜ï¼š1ï¼‰åœ¨ä¸´åºŠæ•ˆæœæ–¹é¢çš„è¡¨ç°ä¸å°½å¦‚äººæ„ï¼Œç‰¹åˆ«æ˜¯åœ¨ç—…ç¶å±æ€§æè¿°æ–¹é¢ï¼›2ï¼‰ç”Ÿæˆçš„æ–‡æœ¬ç¼ºä¹å¯è§£é‡Šæ€§ï¼Œä½¿æ”¾å°„ç§‘åŒ»ç”Ÿéš¾ä»¥ä¿¡ä»»ç»“æœã€‚ä¸ºäº†åº”å¯¹è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬ä¸“æ³¨äºä¸€ä¸ªå¯ä¿¡èµ–çš„RRGæ¨¡å‹ï¼Œè¯¥æ¨¡å‹ä¸ä»…èƒ½ç”Ÿæˆå¼‚å¸¸æƒ…å†µçš„å‡†ç¡®æè¿°ï¼Œè¿˜èƒ½ä¸ºé¢„æµ‹æä¾›ä¾æ®ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªåä¸ºâ€œè¯Šæ–­é“¾â€ï¼ˆCoDï¼‰çš„æ¡†æ¶ï¼Œå®ƒä¿æŒè¯Šæ–­è¿‡ç¨‹çš„è¿ç»­æ€§ï¼Œä»¥å®ç°ä¸´åºŠå‡†ç¡®å’Œå¯è§£é‡Šçš„RRGã€‚é¦–å…ˆï¼Œå®ƒé€šè¿‡è¯Šæ–­å¯¹è¯ç”Ÿæˆé—®ç­”ï¼ˆQAï¼‰å¯¹ï¼Œä»¥æå–å…³é”®å‘ç°ã€‚ç„¶åï¼Œä½¿ç”¨QAè¯Šæ–­æç¤ºå¤§å‹è¯­è¨€æ¨¡å‹è¿›è¡Œå‡†ç¡®ç”Ÿæˆã€‚ä¸ºäº†æé«˜å¯è§£é‡Šæ€§ï¼Œè®¾è®¡äº†ä¸€ä¸ªè¯Šæ–­æ¥åœ°æ¨¡å—æ¥åŒ¹é…QAè¯Šæ–­å’Œç”Ÿæˆçš„å¥å­ï¼Œå…¶ä¸­è¯Šæ–­ä½œä¸ºå‚è€ƒã€‚æ­¤å¤–ï¼Œè¿˜è®¾è®¡äº†ä¸€ä¸ªç—…ç¶å®šä½æ¨¡å—ï¼Œç”¨äºåœ¨å›¾åƒä¸­å®šä½å¼‚å¸¸ï¼Œè¿›ä¸€æ­¥æé«˜æ”¾å°„ç§‘åŒ»ç”Ÿçš„å·¥ä½œæ•ˆç‡ã€‚ä¸ºäº†ä¿ƒè¿›æ ‡ç­¾é«˜æ•ˆè®­ç»ƒï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§å…·æœ‰ä¸´åºŠä¸€è‡´æ€§çš„å…¨ç›‘ç£å­¦ä¹ ç­–ç•¥ï¼Œä»¥åˆ©ç”¨ä¸åŒæ•°æ®é›†çš„å„ç§ç±»å‹çš„æ³¨é‡Šã€‚æˆ‘ä»¬çš„åŠªåŠ›å¸¦æ¥äº†ä»¥ä¸‹æˆæœï¼š1ï¼‰ä¸€ä¸ªå¸¦æœ‰QAå¯¹å’Œç—…ç¶æ¡†çš„å…¨æ ‡ç­¾RRGæ•°æ®é›†ï¼›2ï¼‰ä¸€ä¸ªè¯„ä¼°æŠ¥å‘Šæè¿°ç—…ç¶ä½ç½®å’Œä¸¥é‡ç¨‹åº¦å‡†ç¡®æ€§çš„è¯„ä¼°å·¥å…·ï¼›3ï¼‰å¤§é‡å®éªŒè¯æ˜äº†CoDçš„æœ‰æ•ˆæ€§ï¼Œå®ƒåœ¨ä¸¤ä¸ªRRGåŸºå‡†æµ‹è¯•ä¸Šå§‹ç»ˆä¼˜äºä¸“ä¸šæ¨¡å‹å’Œé€šç”¨æ¨¡å‹ï¼Œå¹¶é€šè¿‡å°†ç”Ÿæˆçš„å¥å­å‡†ç¡®åœ°å¯¹æ¥åˆ°QAè¯Šæ–­å’Œå›¾åƒä¸Šï¼Œæ˜¾ç¤ºå‡ºæœ‰å‰æ™¯çš„å¯è§£é‡Šæ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.09566v1">PDF</a> Accepted to IEEE TMI</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºä¸€ä¸ªåä¸ºè¯Šæ–­é“¾ï¼ˆCoDï¼‰çš„å¯ä¿¡æ”¾å°„å­¦æŠ¥å‘Šç”Ÿæˆï¼ˆRRGï¼‰æ¨¡å‹æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³ç°æœ‰å·¥ä½œä¸­çš„ä¸¤å¤§æŒ‘æˆ˜ï¼šä¸´åºŠæ•ˆæœä¸ä½³å’Œç”Ÿæˆæ–‡æœ¬ç¼ºä¹è§£é‡Šæ€§ã€‚CoDé€šè¿‡ç”Ÿæˆé—®ç­”ï¼ˆQAï¼‰å¯¹æ¥æå–å…³é”®å‘ç°ï¼Œå¹¶ä½¿ç”¨QAè¯Šæ–­æç¤ºå¤§å‹è¯­è¨€æ¨¡å‹è¿›è¡Œå‡†ç¡®ç”Ÿæˆã€‚ä¸ºæé«˜è§£é‡Šæ€§ï¼Œè®¾è®¡äº†è¯Šæ–­æ¥åœ°æ¨¡å—æ¥åŒ¹é…QAè¯Šæ–­å’Œç”Ÿæˆçš„å¥å­ï¼ŒåŒæ—¶è®¾è®¡ç—…ç¶å®šä½æ¨¡å—ä»¥æé«˜æ”¾å°„ç§‘åŒ»ç”Ÿçš„å·¥ä½œæ•ˆç‡ã€‚æ­¤å¤–ï¼Œè¿˜æå‡ºäº†ä¸€ç§åˆ©ç”¨å¤šç§ç±»å‹æ³¨é‡Šå’Œä¸åŒæ•°æ®é›†çš„å…¨ç›‘ç£å­¦ä¹ ç­–ç•¥ï¼Œä»¥å®ç°æ ‡ç­¾æ•ˆç‡è®­ç»ƒã€‚è¯¥æ¡†æ¶èƒ½ç”Ÿæˆå…·æœ‰é—®ç­”å¯¹å’Œç—…ç¶æ¡†çš„å…¨æ–¹ä½æ ‡è®°RRGæ•°æ®é›†ï¼Œå¹¶æä¾›è¯„ä¼°æŠ¥å‘Šæè¿°ç—…ç¶ä½ç½®å’Œä¸¥é‡æ€§çš„å‡†ç¡®æ€§å·¥å…·ã€‚å®éªŒè¯æ˜ï¼ŒCoDåœ¨ä¸¤ç§RRGåŸºå‡†æµ‹è¯•ä¸Šè¡¨ç°ä¼˜äºä¸“ä¸šæ¨¡å‹å’Œé€šç”¨æ¨¡å‹ï¼Œä¸”é€šè¿‡å‡†ç¡®åœ°å°†ç”Ÿæˆçš„å¥å­ä¸QAè¯Šæ–­å’Œå›¾åƒè¿›è¡ŒåŒ¹é…ï¼Œå±•ç°å‡ºè‰¯å¥½çš„è§£é‡Šæ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç°æœ‰æ”¾å°„å­¦æŠ¥å‘Šç”Ÿæˆå·¥ä½œå­˜åœ¨ä¸´åºŠæ•ˆæœä¸ä½³å’Œç”Ÿæˆæ–‡æœ¬ç¼ºä¹è§£é‡Šæ€§çš„æŒ‘æˆ˜ã€‚</li>
<li>æå‡ºçš„è¯Šæ–­é“¾ï¼ˆCoDï¼‰æ¨¡å‹æ¡†æ¶æ—¨åœ¨è§£å†³è¿™äº›é—®é¢˜ï¼Œå®ç°å‡†ç¡®ä¸”å¯è§£é‡Šçš„æ”¾å°„å­¦æŠ¥å‘Šç”Ÿæˆã€‚</li>
<li>CoDé€šè¿‡ç”Ÿæˆé—®ç­”å¯¹æ¥æå–å…³é”®å‘ç°ï¼Œå¹¶ä½¿ç”¨QAè¯Šæ–­æç¤ºå¤§å‹è¯­è¨€æ¨¡å‹è¿›è¡Œæ–‡æœ¬ç”Ÿæˆã€‚</li>
<li>è¯Šæ–­æ¥åœ°æ¨¡å—å’Œç—…ç¶å®šä½æ¨¡å—çš„è®¾è®¡æé«˜äº†æ¨¡å‹çš„è§£é‡Šæ€§å’Œå·¥ä½œæ•ˆç‡ã€‚</li>
<li>é‡‡ç”¨äº†å…¨ç›‘ç£å­¦ä¹ ç­–ç•¥ï¼Œåˆ©ç”¨å¤šç§ç±»å‹æ³¨é‡Šå’Œä¸åŒæ•°æ®é›†è¿›è¡Œæ ‡ç­¾æ•ˆç‡è®­ç»ƒã€‚</li>
<li>CoDèƒ½ç”Ÿæˆå…·æœ‰é—®ç­”å¯¹å’Œç—…ç¶æ¡†çš„å…¨æ–¹ä½æ ‡è®°RRGæ•°æ®é›†ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.09566">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-ceb9018abdc745c654bd3766e226c783.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-41a3a84f14bbb99cc5ea6ba4abaf0aaf.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-f62b13c3965a19edca8fc3002150012e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a1c57142e9d23baa538c15ad45e0f939.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-337ca32e1a949dc0f1a983a3e65077f2.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-4a242af5d321e75b5c6741cb847d6ee9.jpg" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="MIND-A-Noise-Adaptive-Denoising-Framework-for-Medical-Images-Integrating-Multi-Scale-Transformer"><a href="#MIND-A-Noise-Adaptive-Denoising-Framework-for-Medical-Images-Integrating-Multi-Scale-Transformer" class="headerlink" title="MIND: A Noise-Adaptive Denoising Framework for Medical Images   Integrating Multi-Scale Transformer"></a>MIND: A Noise-Adaptive Denoising Framework for Medical Images   Integrating Multi-Scale Transformer</h2><p><strong>Authors:Tao Tang, Chengxu Yang</strong></p>
<p>The core role of medical images in disease diagnosis makes their quality directly affect the accuracy of clinical judgment. However, due to factors such as low-dose scanning, equipment limitations and imaging artifacts, medical images are often accompanied by non-uniform noise interference, which seriously affects structure recognition and lesion detection. This paper proposes a medical image adaptive denoising model (MI-ND) that integrates multi-scale convolutional and Transformer architecture, introduces a noise level estimator (NLE) and a noise adaptive attention module (NAAB), and realizes channel-spatial attention regulation and cross-modal feature fusion driven by noise perception. Systematic testing is carried out on multimodal public datasets. Experiments show that this method significantly outperforms the comparative methods in image quality indicators such as PSNR, SSIM, and LPIPS, and improves the F1 score and ROC-AUC in downstream diagnostic tasks, showing strong prac-tical value and promotional potential. The model has outstanding benefits in structural recovery, diagnostic sensitivity, and cross-modal robustness, and provides an effective solution for medical image enhancement and AI-assisted diagnosis and treatment. </p>
<blockquote>
<p>åŒ»ç–—å›¾åƒåœ¨ç–¾ç—…è¯Šæ–­ä¸­çš„æ ¸å¿ƒä½œç”¨ä½¿å…¶è´¨é‡ç›´æ¥å½±å“ä¸´åºŠåˆ¤æ–­çš„å‡†ç¡®æ€§ã€‚ç„¶è€Œï¼Œç”±äºä½å‰‚é‡æ‰«æã€è®¾å¤‡é™åˆ¶å’Œæˆåƒä¼ªå½±ç­‰å› ç´ ï¼ŒåŒ»ç–—å›¾åƒé€šå¸¸ä¼´éšç€éå‡åŒ€å™ªå£°å¹²æ‰°ï¼Œè¿™ä¸¥é‡å½±å“ç»“æ„è¯†åˆ«å’Œç—…ç¶æ£€æµ‹ã€‚æœ¬æ–‡é’ˆå¯¹è¿™ä¸€é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§ç»“åˆå¤šå°ºåº¦å·ç§¯å’ŒTransformeræ¶æ„çš„åŒ»ç–—å›¾åƒè‡ªé€‚åº”å»å™ªæ¨¡å‹ï¼ˆMI-NDï¼‰ã€‚è¯¥æ¨¡å‹å¼•å…¥äº†å™ªå£°æ°´å¹³ä¼°è®¡å™¨ï¼ˆNLEï¼‰å’Œå™ªå£°è‡ªé€‚åº”æ³¨æ„åŠ›æ¨¡å—ï¼ˆNAABï¼‰ï¼Œå®ç°äº†åŸºäºå™ªå£°æ„ŸçŸ¥çš„é€šé“ç©ºé—´æ³¨æ„åŠ›è°ƒèŠ‚å’Œè·¨æ¨¡æ€ç‰¹å¾èåˆã€‚åœ¨å¤šæ¨¡æ€å…¬å…±æ•°æ®é›†ä¸Šè¿›è¡Œäº†ç³»ç»Ÿæµ‹è¯•ã€‚å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å›¾åƒè´¨é‡æŒ‡æ ‡ï¼ˆå¦‚PSNRã€SSIMå’ŒLPIPSï¼‰ä¸Šæ˜¾è‘—ä¼˜äºå¯¹æ¯”æ–¹æ³•ï¼Œå¹¶åœ¨ä¸‹æ¸¸è¯Šæ–­ä»»åŠ¡ä¸­æé«˜äº†F1åˆ†æ•°å’ŒROC-AUCï¼Œè¡¨ç°å‡ºè¾ƒå¼ºçš„å®ç”¨ä»·å€¼å’Œæ¨å¹¿æ½œåŠ›ã€‚è¯¥æ¨¡å‹åœ¨ç»“æ„æ¢å¤ã€è¯Šæ–­æ•æ„Ÿåº¦å’Œè·¨æ¨¡æ€ç¨³å¥æ€§æ–¹é¢è¡¨ç°å‡ºæ˜¾è‘—ä¼˜åŠ¿ï¼Œä¸ºåŒ»ç–—å›¾åƒå¢å¼ºå’ŒAIè¾…åŠ©è¯Šæ–­å’Œæ²»ç–—æä¾›äº†æœ‰æ•ˆè§£å†³æ–¹æ¡ˆã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.07817v2">PDF</a> Accepted by the 7th International Conference on Intelligent Control,   Measurement and Signal Processing (ICMSP 2025). 6 pages, 6 figures</p>
<p><strong>Summary</strong></p>
<p>åŒ»ç–—å›¾åƒåœ¨ç–¾ç—…è¯Šæ–­ä¸­çš„æ ¸å¿ƒä½œç”¨è¦æ±‚å…¶è´¨é‡ç›´æ¥å½±å“ä¸´åºŠåˆ¤æ–­çš„å‡†ç¡®æ€§ã€‚æœ¬æ–‡æå‡ºä¸€ç§ç»“åˆå¤šå°ºåº¦å·ç§¯å’ŒTransformeræ¶æ„çš„åŒ»ç–—å›¾åƒè‡ªé€‚åº”å»å™ªæ¨¡å‹ï¼ˆMI-NDï¼‰ï¼Œé€šè¿‡å¼•å…¥å™ªå£°æ°´å¹³ä¼°è®¡å™¨ï¼ˆNLEï¼‰å’Œå™ªå£°è‡ªé€‚åº”æ³¨æ„åŠ›æ¨¡å—ï¼ˆNAABï¼‰ï¼Œå®ç°é€šé“-ç©ºé—´æ³¨æ„åŠ›è°ƒèŠ‚å’Œå™ªå£°æ„ŸçŸ¥é©±åŠ¨çš„è·¨æ¨¡æ€ç‰¹å¾èåˆã€‚è¯¥æ¨¡å‹åœ¨å¤šæ¨¡æ€å…¬å…±æ•°æ®é›†ä¸Šè¿›è¡Œç³»ç»Ÿæµ‹è¯•ï¼Œå®éªŒç»“æœè¡¨æ˜è¯¥æ–¹æ³•åœ¨å›¾åƒè´¨é‡æŒ‡æ ‡PSNRã€SSIMå’ŒLPIPSä¸Šæ˜¾è‘—ä¼˜äºå¯¹æ¯”æ–¹æ³•ï¼Œå¹¶åœ¨ä¸‹æ¸¸è¯Šæ–­ä»»åŠ¡ä¸­æé«˜F1åˆ†æ•°å’ŒROC-AUCï¼Œè¡¨ç°å‡ºå¼ºå¤§çš„å®ç”¨ä»·å€¼å’Œåº”ç”¨æ½œåŠ›ã€‚è¯¥æ¨¡å‹åœ¨ç»“æ„æ¢å¤ã€è¯Šæ–­æ•æ„Ÿæ€§å’Œè·¨æ¨¡æ€ç¨³å¥æ€§æ–¹é¢å…·æœ‰æ˜¾è‘—ä¼˜åŠ¿ï¼Œä¸ºåŒ»ç–—å›¾åƒå¢å¼ºå’ŒAIè¾…åŠ©è¯Šæ–­å’Œæ²»ç–—æä¾›äº†æœ‰æ•ˆè§£å†³æ–¹æ¡ˆã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>åŒ»ç–—å›¾åƒè´¨é‡ç›´æ¥å½±å“ç–¾ç—…è¯Šæ–­çš„å‡†ç¡®æ€§ã€‚</li>
<li>æå‡ºçš„åŒ»ç–—å›¾åƒè‡ªé€‚åº”å»å™ªæ¨¡å‹ï¼ˆMI-NDï¼‰ç»“åˆäº†å¤šå°ºåº¦å·ç§¯å’ŒTransformeræ¶æ„ã€‚</li>
<li>MI-NDæ¨¡å‹åŒ…æ‹¬å™ªå£°æ°´å¹³ä¼°è®¡å™¨ï¼ˆNLEï¼‰å’Œå™ªå£°è‡ªé€‚åº”æ³¨æ„åŠ›æ¨¡å—ï¼ˆNAABï¼‰ã€‚</li>
<li>MI-NDæ¨¡å‹å®ç°äº†é€šé“-ç©ºé—´æ³¨æ„åŠ›è°ƒèŠ‚å’Œè·¨æ¨¡æ€ç‰¹å¾èåˆã€‚</li>
<li>ç³»ç»Ÿæµ‹è¯•è¡¨æ˜MI-NDæ¨¡å‹åœ¨å›¾åƒè´¨é‡æŒ‡æ ‡ä¸Šä¼˜äºå…¶ä»–æ–¹æ³•ã€‚</li>
<li>MI-NDæ¨¡å‹åœ¨ä¸‹æ¸¸è¯Šæ–­ä»»åŠ¡ä¸­æé«˜äº†F1åˆ†æ•°å’ŒROC-AUCã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.07817">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-2f46fec5eb76e22ad3836a08bc3d80ca.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-114c560170c3de470797a8c185ada653.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f99be25554e7fd102aefe72715c5db02.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-db6e96fb933052e92aa900d50ba2f2a5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e8eb254c468079debe400db19dd9614a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-bee92e21e17386f8f9fa89ae02538882.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8d26b3a55995c149ace13318cfe541e3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-14862d1a86b98fe2d4e12135ccc7cc25.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e01edc47a6c63428770247e6ee4a0ee5.jpg" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="Early-Detection-of-Pancreatic-Cancer-Using-Multimodal-Learning-on-Electronic-Health-Records"><a href="#Early-Detection-of-Pancreatic-Cancer-Using-Multimodal-Learning-on-Electronic-Health-Records" class="headerlink" title="Early Detection of Pancreatic Cancer Using Multimodal Learning on   Electronic Health Records"></a>Early Detection of Pancreatic Cancer Using Multimodal Learning on   Electronic Health Records</h2><p><strong>Authors:Mosbah Aouad, Anirudh Choudhary, Awais Farooq, Steven Nevers, Lusine Demirkhanyan, Bhrandon Harris, Suguna Pappu, Christopher Gondi, Ravishankar Iyer</strong></p>
<p>Pancreatic ductal adenocarcinoma (PDAC) is one of the deadliest cancers, and early detection remains a major clinical challenge due to the absence of specific symptoms and reliable biomarkers. In this work, we propose a new multimodal approach that integrates longitudinal diagnosis code histories and routinely collected laboratory measurements from electronic health records to detect PDAC up to one year prior to clinical diagnosis. Our method combines neural controlled differential equations to model irregular lab time series, pretrained language models and recurrent networks to learn diagnosis code trajectory representations, and cross-attention mechanisms to capture interactions between the two modalities. We develop and evaluate our approach on a real-world dataset of nearly 4,700 patients and achieve significant improvements in AUC ranging from 6.5% to 15.5% over state-of-the-art methods. Furthermore, our model identifies diagnosis codes and laboratory panels associated with elevated PDAC risk, including both established and new biomarkers. Our code is available at <a target="_blank" rel="noopener" href="https://github.com/MosbahAouad/EarlyPDAC-MML">https://github.com/MosbahAouad/EarlyPDAC-MML</a>. </p>
<blockquote>
<p>èƒ°è…ºç™Œå¯¼ç®¡è…ºç™Œï¼ˆPDACï¼‰æ˜¯æœ€è‡´å‘½çš„ç™Œç—‡ä¹‹ä¸€ï¼Œç”±äºç¼ºå°‘ç‰¹å®šç—‡çŠ¶å’Œå¯é ç”Ÿç‰©æ ‡å¿—ç‰©ï¼Œæ—©æœŸæ£€æµ‹ä»ç„¶æ˜¯ä¸´åºŠä¸Šçš„ä¸€ä¸ªä¸»è¦æŒ‘æˆ˜ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„å¤šæ¨¡å¼æ–¹æ³•ï¼Œè¯¥æ–¹æ³•ç»“åˆäº†çºµå‘è¯Šæ–­ä»£ç å†å²è®°å½•å’Œç”µå­å¥åº·è®°å½•ä¸­å¸¸è§„æ”¶é›†çš„å®éªŒå®¤æµ‹é‡å€¼ï¼Œå¯åœ¨ä¸´åºŠè¯Šæ–­å‰ä¸€å¹´æ£€æµ‹åˆ°PDACã€‚æˆ‘ä»¬çš„æ–¹æ³•ç»“åˆäº†ç¥ç»æ§åˆ¶å¾®åˆ†æ–¹ç¨‹æ¥æ¨¡æ‹Ÿä¸è§„åˆ™å®éªŒå®¤æ—¶é—´åºåˆ—æ•°æ®ã€é¢„è®­ç»ƒè¯­è¨€æ¨¡å‹å’Œå¾ªç¯ç½‘ç»œæ¥å­¦ä¹ è¯Šæ–­ä»£ç è½¨è¿¹è¡¨ç¤ºï¼Œä»¥åŠäº¤å‰æ³¨æ„åŠ›æœºåˆ¶æ¥æ•æ‰ä¸¤ç§æ¨¡å¼ä¹‹é—´çš„äº¤äº’ã€‚æˆ‘ä»¬åœ¨è¿‘4700åæ‚£è€…çš„å®é™…æ•°æ®é›†ä¸Šå¼€å‘å’Œè¯„ä¼°äº†æˆ‘ä»¬çš„æ–¹æ³•ï¼Œä¸æœ€å…ˆè¿›çš„æ–¹æ³•ç›¸æ¯”ï¼ŒAUCæœ‰6.5%è‡³15.5%çš„æ˜¾è‘—æé«˜ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬çš„æ¨¡å‹è¿˜ç¡®å®šäº†ä¸èƒ°è…ºç™Œé£é™©å¢åŠ ç›¸å…³çš„è¯Šæ–­ä»£ç å’Œå®éªŒå®¤æ£€æµ‹æ¿ï¼ŒåŒ…æ‹¬å·²çŸ¥å’Œæ–°ç”Ÿç‰©æ ‡å¿—ç‰©ã€‚æˆ‘ä»¬çš„ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/MosbahAouad/EarlyPDAC-MML">https://github.com/MosbahAouad/EarlyPDAC-MML</a>æ‰¾åˆ°ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.06627v3">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„å¤šæ¨¡å¼æ–¹æ³•ï¼Œé€šè¿‡æ•´åˆçºµå‘è¯Šæ–­ä»£ç å†å²å’Œç”µå­å¥åº·è®°å½•ä¸­å¸¸è§„æ”¶é›†çš„å®éªŒå®¤æµ‹é‡æ•°æ®ï¼Œå¯åœ¨ä¸´åºŠç¡®è¯Šå‰ä¸€å¹´é¢„æµ‹èƒ°è…ºç™Œã€‚è¯¥æ–¹æ³•ä½¿ç”¨ç¥ç»ç½‘ç»œæ§åˆ¶å¾®åˆ†æ–¹ç¨‹å¯¹ä¸è§„åˆ™å®éªŒå®¤æ—¶é—´åºåˆ—è¿›è¡Œå»ºæ¨¡ï¼Œå¹¶ç»“åˆé¢„è®­ç»ƒçš„è¯­è¨€æ¨¡å‹å’Œå¾ªç¯ç½‘ç»œæ¥å­¦ä¹ è¯Šæ–­ä»£ç è½¨è¿¹è¡¨ç¤ºï¼ŒåŒæ—¶ä½¿ç”¨äº¤å‰æ³¨æ„æœºåˆ¶æ•æ‰ä¸¤ç§æ¨¡å¼ä¹‹é—´çš„äº’åŠ¨ã€‚åœ¨æ¥è¿‘4700åæ‚£è€…çš„çœŸå®æ•°æ®é›†ä¸Šå¼€å‘å’Œè¯„ä¼°è¯¥æ–¹æ³•ï¼Œè¾ƒæœ€æ–°æ–¹æ³•æ˜¾è‘—æé«˜äº†AUCå€¼ï¼ˆä»6.5%åˆ°15.5%ï¼‰ã€‚æ­¤å¤–ï¼Œè¯¥æ¨¡å‹ç¡®å®šäº†ä¸èƒ°è…ºç™Œé£é™©å¢åŠ çš„ç›¸å…³çš„è¯Šæ–­ä»£ç å’Œå®éªŒå®¤æ£€æµ‹æ¿ï¼ŒåŒ…æ‹¬å·²çŸ¥å’Œæ–°ç”Ÿç‰©æ ‡å¿—ç‰©ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æå‡ºäº†ä¸€ç§æ–°çš„å¤šæ¨¡å¼æ–¹æ³•ç”¨äºèƒ°è…ºç™Œæ—©æœŸæ£€æµ‹ã€‚</li>
<li>é€šè¿‡æ•´åˆè¯Šæ–­ä»£ç å†å²å’Œå®éªŒå®¤æµ‹é‡æ•°æ®ï¼Œå¯åœ¨ä¸´åºŠç¡®è¯Šå‰ä¸€å¹´è¿›è¡Œé¢„æµ‹ã€‚</li>
<li>ä½¿ç”¨ç¥ç»ç½‘ç»œæ§åˆ¶å¾®åˆ†æ–¹ç¨‹å¯¹ä¸è§„åˆ™å®éªŒå®¤æ—¶é—´åºåˆ—è¿›è¡Œå»ºæ¨¡ã€‚</li>
<li>ç»“åˆé¢„è®­ç»ƒçš„è¯­è¨€æ¨¡å‹å’Œå¾ªç¯ç½‘ç»œå­¦ä¹ è¯Šæ–­ä»£ç è½¨è¿¹è¡¨ç¤ºã€‚</li>
<li>ä½¿ç”¨äº¤å‰æ³¨æ„æœºåˆ¶æ•æ‰è¯Šæ–­ä»£ç å’Œå®éªŒå®¤æ•°æ®é—´çš„äº’åŠ¨ã€‚</li>
<li>åœ¨çœŸå®æ•°æ®é›†ä¸Šçš„è¯„ä¼°ç»“æœæ˜¾ç¤ºï¼Œè¾ƒæœ€æ–°æ–¹æ³•æ˜¾è‘—æé«˜AUCå€¼ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.06627">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-47ee55398049891c9a263c09b44b1ea7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-18e2cb4103950ebd603505b89311b34e.jpg" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1><h2 id="Segment-Anything-in-Pathology-Images-with-Natural-Language"><a href="#Segment-Anything-in-Pathology-Images-with-Natural-Language" class="headerlink" title="Segment Anything in Pathology Images with Natural Language"></a>Segment Anything in Pathology Images with Natural Language</h2><p><strong>Authors:Zhixuan Chen, Junlin Hou, Liqi Lin, Yihui Wang, Yequan Bie, Xi Wang, Yanning Zhou, Ronald Cheong Kin Chan, Hao Chen</strong></p>
<p>Pathology image segmentation is crucial in computational pathology for analyzing histological features relevant to cancer diagnosis and prognosis. However, current methods face major challenges in clinical applications due to limited annotated data and restricted category definitions. To address these limitations, we propose PathSegmentor, the first text-prompted segmentation foundation model designed specifically for pathology images. We also introduce PathSeg, the largest and most comprehensive dataset for pathology segmentation, built from 21 public sources and containing 275k image-mask-label triples across 160 diverse categories. With PathSegmentor, users can perform semantic segmentation using natural language prompts, eliminating the need for laborious spatial inputs such as points or boxes. Extensive experiments demonstrate that PathSegmentor outperforms specialized models with higher accuracy and broader applicability, while maintaining a compact architecture. It significantly surpasses existing spatial- and text-prompted models by 0.145 and 0.429 in overall Dice scores, respectively, showing strong robustness in segmenting complex structures and generalizing to external datasets. Moreover, PathSegmentorâ€™s outputs enhance the interpretability of diagnostic models through feature importance estimation and imaging biomarker discovery, offering pathologists evidence-based support for clinical decision-making. This work advances the development of explainable AI in precision oncology. </p>
<blockquote>
<p>ç—…ç†å­¦å›¾åƒåˆ†å‰²åœ¨è®¡ç®—ç—…ç†å­¦ä¸­å¯¹åˆ†æä¸ç™Œç—‡è¯Šæ–­å’Œé¢„åç›¸å…³çš„ç»„ç»‡å­¦ç‰¹å¾è‡³å…³é‡è¦ã€‚ç„¶è€Œï¼Œç”±äºæ ‡æ³¨æ•°æ®æœ‰é™å’Œç±»åˆ«å®šä¹‰å—é™ï¼Œå½“å‰æ–¹æ³•åœ¨ä¸´åºŠåº”ç”¨æ–¹é¢é¢ä¸´é‡å¤§æŒ‘æˆ˜ã€‚ä¸ºäº†è§£å†³è¿™äº›å±€é™æ€§ï¼Œæˆ‘ä»¬æå‡ºäº†PathSegmentorï¼Œè¿™æ˜¯ä¸“é—¨ä¸ºç—…ç†å­¦å›¾åƒè®¾è®¡çš„é¦–ä¸ªæ–‡æœ¬æç¤ºåˆ†å‰²åŸºç¡€æ¨¡å‹ã€‚æˆ‘ä»¬è¿˜ä»‹ç»äº†PathSegï¼Œè¿™æ˜¯æœ€å¤§çš„æœ€å…¨é¢çš„ç—…ç†å­¦åˆ†å‰²æ•°æ®é›†ï¼Œç”±21ä¸ªå…¬å…±æ¥æºæ„å»ºï¼ŒåŒ…å«160å¤šä¸ªç±»åˆ«çš„27.5ä¸‡å¼ å›¾åƒ-æ©è†œ-æ ‡ç­¾ä¸‰å…ƒç»„ã€‚ä½¿ç”¨PathSegmentorï¼Œç”¨æˆ·å¯ä»¥ä½¿ç”¨è‡ªç„¶è¯­è¨€æç¤ºæ‰§è¡Œè¯­ä¹‰åˆ†å‰²ï¼Œæ— éœ€ç¹ççš„ç©ºé—´è¾“å…¥ï¼Œå¦‚ç‚¹æˆ–æ¡†ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼ŒPathSegmentoråœ¨å‡†ç¡®åº¦å’Œé€‚ç”¨æ€§æ–¹é¢è¶…è¶Šäº†ä¸“ä¸šæ¨¡å‹ï¼ŒåŒæ—¶ä¿æŒäº†ç´§å‡‘çš„æ¶æ„ã€‚åœ¨æ€»ä½“Diceå¾—åˆ†æ–¹é¢ï¼Œå®ƒåˆ†åˆ«ä»¥0.145å’Œ0.429çš„ä¼˜åŠ¿è¶…è¶Šäº†ç°æœ‰çš„ç©ºé—´æç¤ºæ¨¡å‹å’Œæ–‡æœ¬æç¤ºæ¨¡å‹ï¼Œåœ¨åˆ†å‰²å¤æ‚ç»“æ„å’Œé€‚åº”å¤–éƒ¨æ•°æ®é›†æ–¹é¢è¡¨ç°å‡ºå¼ºå¤§çš„ç¨³å¥æ€§ã€‚æ­¤å¤–ï¼ŒPathSegmentorçš„è¾“å‡ºé€šè¿‡ç‰¹å¾é‡è¦æ€§è¯„ä¼°å’Œæˆåƒç”Ÿç‰©æ ‡å¿—ç‰©å‘ç°ï¼Œå¢å¼ºäº†è¯Šæ–­æ¨¡å‹çš„è§£é‡Šæ€§ï¼Œä¸ºç—…ç†å­¦å®¶æä¾›åŸºäºè¯æ®çš„æ”¯æŒï¼Œæœ‰åŠ©äºä¸´åºŠå†³ç­–ã€‚è¿™é¡¹å·¥ä½œæ¨åŠ¨äº†ç²¾å‡†è‚¿ç˜¤å­¦ä¸­å¯è§£é‡Šäººå·¥æ™ºèƒ½çš„å‘å±•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.20988v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ç—…ç†å›¾åƒåˆ†å‰²åœ¨è®¡ç®—ç—…ç†å­¦ä¸­çš„é‡è¦æ€§ï¼Œå¹¶æŒ‡å‡ºäº†å½“å‰æ–¹æ³•åœ¨ä¸´åºŠåº”ç”¨ä¸­çš„æŒ‘æˆ˜ã€‚ä¸ºåº”å¯¹è¿™äº›æŒ‘æˆ˜ï¼Œæå‡ºäº†PathSegmentoræ¨¡å‹ï¼Œè¿™æ˜¯é¦–ä¸ªé’ˆå¯¹ç—…ç†å›¾åƒè®¾è®¡çš„æ–‡æœ¬æç¤ºåˆ†å‰²åŸºç¡€æ¨¡å‹ã€‚åŒæ—¶å¼•å…¥äº†PathSegæ•°æ®é›†ï¼Œç”¨äºå¹¿æ³›çš„ç—…ç†åˆ†å‰²ä»»åŠ¡ã€‚PathSegmentorå…è®¸ç”¨æˆ·åˆ©ç”¨è‡ªç„¶è¯­è¨€æç¤ºè¿›è¡Œè¯­ä¹‰åˆ†å‰²ï¼Œæ— éœ€ç¹ççš„ç©ºé—´è¾“å…¥ã€‚å®éªŒè¡¨æ˜ï¼ŒPathSegmentoråœ¨å‡†ç¡®æ€§å’Œé€‚ç”¨æ€§æ–¹é¢ä¼˜äºä¸“ä¸šæ¨¡å‹ï¼ŒåŒæ—¶åœ¨æ€»ä½“Diceå¾—åˆ†ä¸Šè¶…è¶Šç°æœ‰ç©ºé—´æç¤ºå’Œæ–‡æœ¬æç¤ºæ¨¡å‹ã€‚æ­¤å¤–ï¼ŒPathSegmentorçš„è¾“å‡ºé€šè¿‡ç‰¹å¾é‡è¦æ€§ä¼°è®¡å’Œæˆåƒç”Ÿç‰©æ ‡å¿—ç‰©å‘ç°æé«˜äº†è¯Šæ–­æ¨¡å‹çš„è§£é‡Šæ€§ï¼Œä¸ºç—…ç†åŒ»å¸ˆæä¾›å¾ªè¯æ”¯æŒï¼Œæ¨åŠ¨ç²¾å‡†è‚¿ç˜¤å­¦ä¸­çš„å¯è§£é‡Šäººå·¥æ™ºèƒ½çš„å‘å±•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç—…ç†å›¾åƒåˆ†å‰²åœ¨è®¡ç®—ç—…ç†å­¦é¢†åŸŸå¯¹äºç™Œç—‡è¯Šæ–­å’Œæ²»ç–—è‡³å…³é‡è¦ã€‚</li>
<li>å½“å‰æ–¹æ³•åœ¨åº”ç”¨ä¸­é¢ä¸´æœ‰é™æ ‡æ³¨æ•°æ®å’Œé™åˆ¶ç±»åˆ«å®šä¹‰çš„æŒ‘æˆ˜ã€‚</li>
<li>PathSegmentoræ¨¡å‹æ˜¯é¦–ä¸ªé’ˆå¯¹ç—…ç†å›¾åƒè®¾è®¡çš„æ–‡æœ¬æç¤ºåˆ†å‰²åŸºç¡€æ¨¡å‹ï¼Œè§£å†³äº†ä¸Šè¿°æŒ‘æˆ˜ã€‚</li>
<li>PathSegæ•°æ®é›†æ˜¯æœ€å¤§çš„ç»¼åˆæ•°æ®é›†ï¼ŒåŒ…å«å¤šä¸ªæ•°æ®æ¥æºå’Œä¸åŒç±»åˆ«çš„åˆ†å‰²å›¾åƒã€‚</li>
<li>PathSegmentoræ¨¡å‹æ— éœ€ç¹ççš„ç©ºé—´è¾“å…¥ï¼Œå¯ä»¥åˆ©ç”¨è‡ªç„¶è¯­è¨€æç¤ºè¿›è¡Œè¯­ä¹‰åˆ†å‰²ã€‚</li>
<li>å®éªŒç»“æœæ˜¾ç¤ºPathSegmentoræ¨¡å‹åœ¨å‡†ç¡®æ€§æ–¹é¢æ˜¾è‘—ä¼˜äºå…¶ä»–æ¨¡å‹ï¼Œå…·æœ‰æ›´å¼ºçš„ç¨³å¥æ€§å’Œæ³›åŒ–èƒ½åŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.20988">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-48dc191e59442cb58fac08f3973d2b1c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5b2ca86aabec697cf04f8cc1323b18ce.jpg" align="middle">
</details>


<h1 id="-19"><a href="#-19" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-08-21/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">https://kedreamix.github.io/Talk2Paper/Paper/2025-08-21/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                                    <span class="chip bg-color">åŒ»å­¦å›¾åƒ</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-08-21/TTS/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-363f1a70a3d3350f357af80f7c4319a3.jpg" class="responsive-img" alt="TTS">
                        
                        <span class="card-title">TTS</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            TTS æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-08-21  DiffIER Optimizing Diffusion Models with Iterative Error Reduction
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-08-21
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/TTS/" class="post-category">
                                    TTS
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/TTS/">
                        <span class="chip bg-color">TTS</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-08-21/Diffusion%20Models/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-6f8d303bf54cf2d273a6f5b383176b78.jpg" class="responsive-img" alt="Diffusion Models">
                        
                        <span class="card-title">Diffusion Models</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Diffusion Models æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-08-21  Latent Interpolation Learning Using Diffusion Models for Cardiac Volume   Reconstruction
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-08-21
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Diffusion-Models/" class="post-category">
                                    Diffusion Models
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Diffusion-Models/">
                        <span class="chip bg-color">Diffusion Models</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">26633.8k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
