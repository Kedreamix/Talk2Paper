<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="LLM">
    <meta name="description" content="LLM 方向最新论文已更新，请持续关注 Update in 2025-08-21  Unintended Misalignment from Agentic Fine-Tuning Risks and Mitigation">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>LLM | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loader页面消失采用渐隐的方式*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logo出现动画 */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//使用渐隐的方法淡出loading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//强制显示loading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">嘘~  正在从服务器偷取页面 . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>首页</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>标签</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>分类</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>归档</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="搜索" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="深色/浅色模式" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			首页
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			标签
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			分类
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			归档
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-a056dd40246c173f2f35e909473a12a2.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">LLM</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- 文章内容详情 -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/LLM/">
                                <span class="chip bg-color">LLM</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/LLM/" class="post-category">
                                LLM
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp;
                    2025-08-21
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>更新日期:&nbsp;&nbsp;
                    2025-09-08
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>文章字数:&nbsp;&nbsp;
                    19.5k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>阅读时长:&nbsp;&nbsp;
                    79 分
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>阅读次数:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>⚠️ 以下所有内容总结都来自于 大语言模型的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p>
</blockquote>
<h1 id="2025-08-21-更新"><a href="#2025-08-21-更新" class="headerlink" title="2025-08-21 更新"></a>2025-08-21 更新</h1><h2 id="Unintended-Misalignment-from-Agentic-Fine-Tuning-Risks-and-Mitigation"><a href="#Unintended-Misalignment-from-Agentic-Fine-Tuning-Risks-and-Mitigation" class="headerlink" title="Unintended Misalignment from Agentic Fine-Tuning: Risks and Mitigation"></a>Unintended Misalignment from Agentic Fine-Tuning: Risks and Mitigation</h2><p><strong>Authors:Dongyoon Hahm, Taywon Min, Woogyeol Jin, Kimin Lee</strong></p>
<p>Beyond simple text generation, Large Language Models (LLMs) have evolved into agentic systems capable of planning and interacting with external tools to solve complex tasks. This evolution involves fine-tuning LLMs on agent-specific tasks to enhance their proficiency. However, safety concerns are frequently overlooked during this fine-tuning process. In this work, we show that aligned LLMs can become unintentionally misaligned, leading to a higher likelihood of executing harmful tasks and a reduced tendency to refuse them when fine-tuned to execute agentic tasks. To address these safety challenges, we propose Prefix INjection Guard (PING), a simple yet effective method that prepends automatically generated natural language prefixes to agent responses, guiding them to refuse harmful requests while preserving performance on benign tasks. Specifically, we introduce an iterative approach that alternates between (1) generating candidate prefixes and (2) selecting those that optimize both task performance and refusal behavior. Experimental results demonstrate that PING significantly enhances the safety of fine-tuned LLM agents without sacrificing their effectiveness. PING consistently outperforms existing prompting approaches across diverse benchmarks in both web navigation and code generation tasks. Our analysis of internal hidden states via linear probes reveals that prefix tokens are crucial for behavior modification, explaining the performance gains. WARNING: This paper contains contents that are unethical or offensive in nature. </p>
<blockquote>
<p>除了简单的文本生成外，大型语言模型（LLM）已经发展成为了能够规划并与外部工具进行交互以解决复杂任务的代理系统。这种进化涉及到对特定任务进行微调，以增强LLM的专业能力。然而，在微调过程中，安全问题经常被忽视。在这项工作中，我们表明对齐的LLM可能会无意中失去对齐，导致更有可能执行有害任务，并且在微调以执行代理任务时减少拒绝它们的倾向。为了解决这些安全挑战，我们提出了Prefix INjection Guard（PING），这是一种简单而有效的方法，向代理响应自动生成的自然语言前缀，以指导它们拒绝有害的请求，同时保留在良性任务上的性能。具体来说，我们介绍了一种迭代方法，该方法交替进行（1）生成候选前缀和（2）选择那些既优化任务性能又优化拒绝行为的前缀。实验结果表明，PING在不影响LLM代理效率的情况下显著提高了其安全性。PING在各种基准测试中始终优于现有的提示方法，无论是在网页导航还是代码生成任务中。我们通过线性探针对内部隐藏状态的分析表明，前缀令牌对行为修改至关重要，解释了性能提升的原因。警告：本文包含不道德或具有冒犯性的内容。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.14031v1">PDF</a> Source code: <a target="_blank" rel="noopener" href="https://github.com/HahmDY/prefix_injection_guard">https://github.com/HahmDY/prefix_injection_guard</a></p>
<p><strong>Summary</strong></p>
<p>大型语言模型（LLMs）已进化为能够规划并与外部工具互动以完成复杂任务的代理系统。然而，在微调过程中，安全性常被忽视。本研究显示，对齐的LLMs可能会意外地出现不对齐的情况，导致更容易执行有害任务，并且在执行代理任务时，拒绝有害任务的倾向降低。为解决这些安全挑战，我们提出了Prefix INjection Guard（PING）方法，通过自动生成的自然语言前缀来引导响应，拒绝有害请求，同时保持对良性任务的性能。实验结果显示，PING在不影响LLM代理效能的前提下，显著提高了其安全性。PING在各种基准测试中表现优异，无论是网页导航还是代码生成任务都是如此。通过对内部隐藏状态的分析，我们发现前缀标记对行为改变至关重要，解释了性能提升的原因。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLMs已进化为能够完成复杂任务的代理系统，包括规划及与外部工具互动。</li>
<li>在微调LLMs以执行特定任务时，安全性常被忽视。</li>
<li>对齐的LLMs可能会意外地出现不对齐，导致更容易执行有害任务。</li>
<li>提出了一种名为PING的方法，通过自动添加自然语言前缀来引导LLM的响应，以提高其安全性。</li>
<li>PING方法能在不牺牲LLM代理效能的前提下提高安全性。</li>
<li>PING在各种任务基准测试中表现优异，包括网页导航和代码生成。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.14031">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-b88ab9ad84bf1dda7237ea976b024cb8.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-8862d04bf6dff131ca511ba3641de15a.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-a765d3cbe2433bd2b2f5429b80d78522.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-684cad5b774d387d7ced22e94aebebce.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-de083283a8b9ba75b0d4eb42037e8b33.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-24752f6793c0ba1b3a40b2aa66ae42b2.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Ask-Good-Questions-for-Large-Language-Models"><a href="#Ask-Good-Questions-for-Large-Language-Models" class="headerlink" title="Ask Good Questions for Large Language Models"></a>Ask Good Questions for Large Language Models</h2><p><strong>Authors:Qi Wu, Zhongqi Lu</strong></p>
<p>Recent advances in large language models (LLMs) have significantly improved the performance of dialog systems, yet current approaches often fail to provide accurate guidance of topic due to their inability to discern user confusion in related concepts. To address this, we introduce the Ask-Good-Question (AGQ) framework, which features an improved Concept-Enhanced Item Response Theory (CEIRT) model to better identify users’ knowledge levels. Our contributions include applying the CEIRT model along with LLMs to directly generate guiding questions based on the inspiring text, greatly improving information retrieval efficiency during the question &amp; answer process. Through comparisons with other baseline methods, our approach outperforms by significantly enhencing the users’ information retrieval experiences. </p>
<blockquote>
<p>最近的大型语言模型（LLM）的进步显著提高了对话系统的性能，然而，当前的方法往往由于无法辨别用户对相关概念的理解混淆，而无法提供准确的主题指导。为了解决这一问题，我们引入了Ask-Good-Question（AGQ）框架，该框架采用改进的概念增强项目反应理论（CEIRT）模型，以更好地识别用户的知识水平。我们的贡献包括将CEIRT模型与LLM相结合，直接根据启发文本生成引导问题，大大提高了问答过程中的信息检索效率。与其他基线方法的比较表明，我们的方法显著提升了用户的信息检索体验。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.14025v1">PDF</a> </p>
<p><strong>Summary</strong>：近期大型语言模型（LLM）的进步显著提升了对话系统的性能，但当前方法往往因无法辨识用户概念混淆而难以准确提供主题指导。为此，我们提出Ask-Good-Question（AGQ）框架，采用改进的Concept-Enhanced Item Response Theory（CEIRT）模型更好地识别用户知识水平，并结合LLM直接生成基于文本启发的问题，大大提高问答过程中的信息检索效率，相较于其他基础方法表现出更好的性能，增强了用户的信息检索体验。</p>
<p><strong>Key Takeaways</strong>：</p>
<ol>
<li>LLM的进步提升了对话系统性能，但存在无法准确提供主题指导的问题。</li>
<li>Ask-Good-Question（AGQ）框架被引入解决此问题。</li>
<li>AGQ使用改进的Concept-Enhanced Item Response Theory（CEIRT）模型，以更好地识别用户知识水平。</li>
<li>结合LLM，AGQ能直接生成基于文本启发的问题。</li>
<li>此方法提高问答过程中的信息检索效率。</li>
<li>与其他基础方法相比，AGQ表现出更好的性能。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.14025">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-2c9506904d7900f9c663d65dc310a6fe.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e96a2eb27c7f6791173a5a714335d573.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c9de413968d79c2d09b685e755c42453.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-a3aec8f35ec904029421cce198011d2a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a9247fa0c220fc17a1638ce1ce7c9131.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-99457d20782d10b777a3e084194a1873.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Chunks-as-Arms-Multi-Armed-Bandit-Guided-Sampling-for-Long-Context-LLM-Preference-Optimization"><a href="#Chunks-as-Arms-Multi-Armed-Bandit-Guided-Sampling-for-Long-Context-LLM-Preference-Optimization" class="headerlink" title="Chunks as Arms: Multi-Armed Bandit-Guided Sampling for Long-Context LLM   Preference Optimization"></a>Chunks as Arms: Multi-Armed Bandit-Guided Sampling for Long-Context LLM   Preference Optimization</h2><p><strong>Authors:Shaohua Duan, Xinze Li, Zhenghao Liu, Xiaoyuan Yi, Yukun Yan, Shuo Wang, Yu Gu, Ge Yu, Maosong Sun</strong></p>
<p>Long-context modeling is critical for a wide range of real-world tasks, including long-context question answering, summarization, and complex reasoning tasks. Recent studies have explored fine-tuning Large Language Models (LLMs) with synthetic data to enhance their long-context capabilities. However, the effectiveness of such approaches is often limited by the low diversity and factual inconsistencies in the generated data. To address these challenges, we propose LongMab-PO, a novel framework that leverages a Multi-Armed Bandit (MAB) rollout strategy to identify the most informative chunks from the given long context for sampling high-quality and diverse responses and constructing preference data pairs for Direct Preference Optimization (DPO) training. Specifically, we treat context chunks as arms of MAB, select chunks based on their expected reward scores to input into LLMs to generate responses, and iteratively update these scores based on reward feedback. This exploration and exploitation process enables the model to focus on the most relevant context segments, thereby generating and collecting high-quality and diverse responses. Finally, we collect these generated responses from the rollout process and apply the DPO method to further optimize the LLM. Experimental results show that LongMab-PO significantly improves the diversity and quality of preference data pairs, achieving state-of-the-art performance on long-context reasoning benchmarks. All code and data will be released on <a target="_blank" rel="noopener" href="https://github.com/NEUIR/LongMab-PO">https://github.com/NEUIR/LongMab-PO</a>. </p>
<blockquote>
<p>长语境建模对于一系列现实世界任务至关重要，包括长语境问答、摘要和复杂推理任务。最近的研究已经探索了通过合成数据对大型语言模型（LLM）进行微调以提高其长语境能力。然而，这种方法的效力通常受到生成数据低多样性和事实不一致性的限制。为了应对这些挑战，我们提出了LongMab-PO这一新框架，它利用多臂老虎机（MAB）滚动策略来识别给定长语境中最具信息量的片段，从而采样高质量且多样化的响应，并构建用于直接偏好优化（DPO）训练的偏好数据对。具体来说，我们将语境片段视为老虎机的手臂，根据预期的奖励分数选择片段，输入到LLM中生成响应，并根据奖励反馈迭代更新这些分数。这种探索与利用过程使模型能够关注最相关的语境片段，从而生成和收集高质量且多样化的响应。最后，我们收集这些从滚动过程中生成的响应，并应用DPO方法来进一步优化LLM。实验结果表明，LongMab-PO能显著提高偏好数据对的多样性和质量，在长语境推理基准测试中达到最新技术水平。所有代码和数据将在<a target="_blank" rel="noopener" href="https://github.com/NEUIR/LongMab-PO%E4%B8%8A%E5%8F%91%E5%B8%83%E3%80%82">https://github.com/NEUIR/LongMab-PO上发布。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.13993v1">PDF</a> </p>
<p><strong>Summary</strong><br>长文本建模对一系列现实任务至关重要，如长文本问答、摘要和复杂推理任务。尽管最近的研究尝试通过合成数据微调大型语言模型（LLM）来增强其长文本能力，但生成数据的多样性和事实准确性限制了其效果。为此，我们提出LongMab-PO框架，利用多臂老虎机（MAB）策略识别最有信息量的长文本片段来生成高质量且多样的回应并构建偏好数据对进行直接偏好优化（DPO）训练。具体地，我们利用多臂老虎机将上下文片段作为其行为选项并输入到LLM中进行反应生成，并根据反馈奖励更新分数。这种探索与利用过程使模型聚焦于最相关的上下文片段，从而生成和收集高质量且多样的回应。最终实验结果表明，LongMab-PO在提升偏好数据对的多样性和质量方面表现显著，并在长文本推理基准测试中达到最佳性能。更多代码和数据将在<a target="_blank" rel="noopener" href="https://github.com/NEUIR/LongMab-PO%E5%8F%91%E5%B8%83%E3%80%82">https://github.com/NEUIR/LongMab-PO发布。</a></p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>长文本建模在多个现实任务中起到关键作用。包括长文本问答、摘要以及复杂推理任务等都需要对其进行精细化处理。</li>
<li>通过合成数据微调大型语言模型的方法面临数据多样性和事实准确性的挑战。现有的方法往往不能有效地解决这些问题。</li>
<li>提出了一种名为LongMab-PO的新框架，它结合了多臂老虎机策略和直接偏好优化方法，解决了以上挑战。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.13993">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-abaefd5f7d9c6cb9f01523e48cd91b17.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-2196a2df82d4b65ad947d5d1b674620d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a056dd40246c173f2f35e909473a12a2.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c7bb8ffab06e614adf985e034f0b2b40.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b6024522db99c87b89cb80e342186a65.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="RotBench-Evaluating-Multimodal-Large-Language-Models-on-Identifying-Image-Rotation"><a href="#RotBench-Evaluating-Multimodal-Large-Language-Models-on-Identifying-Image-Rotation" class="headerlink" title="RotBench: Evaluating Multimodal Large Language Models on Identifying   Image Rotation"></a>RotBench: Evaluating Multimodal Large Language Models on Identifying   Image Rotation</h2><p><strong>Authors:Tianyi Niu, Jaemin Cho, Elias Stengel-Eskin, Mohit Bansal</strong></p>
<p>We investigate to what extent Multimodal Large Language Models (MLLMs) can accurately identify the orientation of input images rotated 0{\deg}, 90{\deg}, 180{\deg}, and 270{\deg}. This task demands robust visual reasoning capabilities to detect rotational cues and contextualize spatial relationships within images, regardless of their orientation. To evaluate MLLMs on these abilities, we introduce RotBench – a 350-image manually-filtered benchmark comprising lifestyle, portrait, and landscape images. Despite the relatively simple nature of this task, we show that several state-of-the-art open and proprietary MLLMs, including GPT-5, o3, and Gemini-2.5-Pro, do not reliably identify rotation in input images. Providing models with auxiliary information – including captions, depth maps, and more – or using chain-of-thought prompting offers only small and inconsistent improvements. Our results indicate that most models are able to reliably identify right-side-up (0{\deg}) images, while certain models are able to identify upside-down (180{\deg}) images. None can reliably distinguish between 90{\deg} and 270{\deg}. Simultaneously showing the image rotated in different orientations leads to moderate performance gains for reasoning models, while a modified setup using voting improves the performance of weaker models. We further show that fine-tuning does not improve models’ ability to distinguish 90{\deg} and 270{\deg} rotations, despite substantially improving the identification of 180{\deg} images. Together, these results reveal a significant gap between MLLMs’ spatial reasoning capabilities and human perception in identifying rotation. </p>
<blockquote>
<p>我们调查了多模态大型语言模型（MLLMs）在何种程度上能准确识别输入图像旋转了0°、90°、180°和270°的方向。这项任务需要强大的视觉推理能力来检测旋转线索并理解图像内的空间关系，无论其方向如何。为了评估MLLMs的这些能力，我们引入了RotBench——一个包含生活方式、肖像和风景图像的350张手动筛选的基准测试集。尽管任务相对简单，我们显示，包括GPT-5、o3和Gemini-2.5-Pro等多个先进的开源和专有MLLMs并不能可靠地识别输入图像的旋转。为模型提供辅助信息，如字幕、深度图等，或使用思维链提示，只能带来微小且不一致的改进。我们的结果表明，大多数模型能够可靠地识别正立（0°）的图像，而某些模型能够识别倒立（180°）的图像。没有任何模型能够可靠地区分90°和270°的旋转。同时展示不同方向旋转的图像会导致推理模型的性能适度提升，而使用投票的修改设置则提高了较弱模型的性能。我们进一步表明，微调并不会提高模型区分90°和270°旋转的能力，尽管它在很大程度上提高了对180°图像的识别能力。总的来说，这些结果揭示了MLLMs的空间推理能力和人类感知在识别旋转方面的重大差距。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.13968v1">PDF</a> 20 pages. Code and data: <a target="_blank" rel="noopener" href="https://github.com/tianyiniu/RotBench">https://github.com/tianyiniu/RotBench</a></p>
<p><strong>摘要</strong></p>
<p>本文探讨了多模态大型语言模型（MLLMs）在识别输入图像旋转角度方面的能力，具体涉及0°、90°、180°和270°的旋转。此任务需要模型具备稳健的视觉推理能力，以检测旋转线索并在图像中上下文化空间关系，无论其方向如何。为评估MLLMs在此方面的能力，本文引入了RotBench——一个包含350张手动筛选的生活、肖像和风景图像的基准测试集。尽管任务相对简单，但研究显示，包括GPT-5、o3和Gemini-2.5-Pro等在内的多个先进开源和专有MLLMs在识别图像旋转方面并不可靠。提供模型辅助信息（如字幕、深度图等）或使用链式思维提示只能带来微小且不一致的改进。结果表明，大多数模型能够可靠地识别正面（0°）图像，而某些模型能够识别倒置（180°）图像。没有任何模型能够可靠地区分90°和270°旋转。同时展示不同方向旋转的图像会导致推理模型的性能适度提升，而使用投票的改进设置则能提高较弱模型的性能。此外，尽管微调能显著提高180°图像的识别能力，但在区分90°和270°旋转方面并无改善。总体而言，这些结果揭示了MLLMs在空间推理能力方面与人类感知在识别旋转方面的重大差距。</p>
<p><strong>关键见解</strong></p>
<ol>
<li>多模态大型语言模型（MLLMs）在识别输入图像旋转角度方面存在挑战，即使对于简单的旋转角度如0°、90°、180°和270°。</li>
<li>现有MLLMs在识别图像空间关系和旋转方面表现不足，需要更强大的视觉推理能力。</li>
<li>附加信息（如字幕、深度图等）或链式思维提示对改善模型识别旋转角度的能力有限且效果不稳定。</li>
<li>大多数模型能可靠识别正面图像和倒置图像，但在区分90°和270°旋转时遇到困难。</li>
<li>同时展示不同方向旋转的图像以及使用投票机制有助于提高模型的性能。</li>
<li>微调能够提高模型对倒置图像的识别能力，但在区分特定旋转角度方面并无显著改进。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.13968">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-9010610abcef0add9c0b2c77b38586e9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-68e2f3bf7b1db8cc8f161d9f77c08946.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a1ec44c4b41cb09a453eadb3e6c0e701.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="ReviewGraph-A-Knowledge-Graph-Embedding-Based-Framework-for-Review-Rating-Prediction-with-Sentiment-Features"><a href="#ReviewGraph-A-Knowledge-Graph-Embedding-Based-Framework-for-Review-Rating-Prediction-with-Sentiment-Features" class="headerlink" title="ReviewGraph: A Knowledge Graph Embedding Based Framework for Review   Rating Prediction with Sentiment Features"></a>ReviewGraph: A Knowledge Graph Embedding Based Framework for Review   Rating Prediction with Sentiment Features</h2><p><strong>Authors:A. J. W. de Vink, Natalia Amat-Lefort, Lifeng Han</strong></p>
<p>In the hospitality industry, understanding the factors that drive customer review ratings is critical for improving guest satisfaction and business performance. This work proposes ReviewGraph for Review Rating Prediction (RRP), a novel framework that transforms textual customer reviews into knowledge graphs by extracting (subject, predicate, object) triples and associating sentiment scores. Using graph embeddings (Node2Vec) and sentiment features, the framework predicts review rating scores through machine learning classifiers. We compare ReviewGraph performance with traditional NLP baselines (such as Bag of Words, TF-IDF, and Word2Vec) and large language models (LLMs), evaluating them in the HotelRec dataset. In comparison to the state of the art literature, our proposed model performs similar to their best performing model but with lower computational cost (without ensemble).   While ReviewGraph achieves comparable predictive performance to LLMs and outperforms baselines on agreement-based metrics such as Cohen’s Kappa, it offers additional advantages in interpretability, visual exploration, and potential integration into Retrieval-Augmented Generation (RAG) systems. This work highlights the potential of graph-based representations for enhancing review analytics and lays the groundwork for future research integrating advanced graph neural networks and fine-tuned LLM-based extraction methods. We will share ReviewGraph output and platform open-sourced on our GitHub page <a target="_blank" rel="noopener" href="https://github.com/aaronlifenghan/ReviewGraph">https://github.com/aaronlifenghan/ReviewGraph</a> </p>
<blockquote>
<p>在旅游住宿行业中，了解推动客户评价等级的因素对于提高客户满意度和酒店业务表现至关重要。本研究提出了评价等级预测（RRP）的ReviewGraph框架，该框架通过提取（主语、谓语、宾语）三元组并关联情感分数，将客户文本评价转化为知识图谱。使用图嵌入（Node2Vec）和情感特征，该框架通过机器学习分类器预测评价等级分数。我们将ReviewGraph的性能与传统NLP基线（如词袋模型、TF-IDF和Word2Vec）以及大型语言模型（LLM）进行比较，并在HotelRec数据集上评估它们的性能。与最新的文献相比，我们提出的模型与他们的最佳模型表现相当，但计算成本更低（无需集成）。虽然ReviewGraph与LLM在预测性能上表现相当，并且在基于协议的指标（如Cohen的Kappa系数）上优于基线，但它还提供了可解释性、可视化探索以及与检索增强生成（RAG）系统集成的潜在优势。本研究突出了基于图的表示在增强评价分析方面的潜力，并为将来整合先进图神经网络和微调LLM提取方法的研究奠定了基础。我们将在我们的GitHub页面<a target="_blank" rel="noopener" href="https://github.com/aaronlifenghan/ReviewGraph%E4%B8%8A%E5%88%86%E4%BA%AB%E5%BC%80%E6%BA%90%E7%9A%84ReviewGraph%E8%BE%93%E5%87%BA%E5%92%8C%E5%B9%B3%E5%8F%B0%E3%80%82">https://github.com/aaronlifenghan/ReviewGraph上分享开源的ReviewGraph输出和平台。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.13953v1">PDF</a> </p>
<p><strong>Summary</strong>：</p>
<p>本文介绍了针对酒店评论评级预测的新框架ReviewGraph。该框架通过提取文本中的知识图谱三元组并关联情感分数，将文本评论转化为知识图谱。利用图嵌入和情绪特征，通过机器学习分类器预测评论评级分数。相较于传统NLP方法和大型语言模型，ReviewGraph在性能上表现良好，特别是在基于协议的度量标准上，如Cohen的Kappa系数。此外，它还具有可解释性、可视化探索以及与检索增强生成系统集成的潜力。该工作强调了图表示在增强评论分析方面的潜力，并为未来研究提供了基础。</p>
<p><strong>Key Takeaways</strong>：</p>
<ol>
<li>ReviewGraph是一个用于酒店评论评级预测的新框架，它将文本评论转化为知识图谱进行分析。</li>
<li>该框架通过提取（主语、谓语、宾语）三元组并关联情感分数来实现文本到知识图谱的转化。</li>
<li>ReviewGraph利用图嵌入和情绪特征进行评级预测，采用机器学习分类器。</li>
<li>与传统NLP方法和大型语言模型相比，ReviewGraph在性能上表现良好，特别是在基于协议的度量标准上。</li>
<li>ReviewGraph具有可解释性、可视化探索的优势，并且能与检索增强生成系统相结合。</li>
<li>该工作突出了图表示在增强评论分析方面的潜力。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.13953">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-61becdeaaf8aa5db3541f7288022815e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-233ea64a6160ddaebf7c53cc38bc8d46.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7d21fe07861773aa6d7ffe27de6ea657.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d3389834c41dce0c2c897e385816f4d0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-62b40f21f475a6a280e2d01c80c351d8.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Prompt-Orchestration-Markup-Language"><a href="#Prompt-Orchestration-Markup-Language" class="headerlink" title="Prompt Orchestration Markup Language"></a>Prompt Orchestration Markup Language</h2><p><strong>Authors:Yuge Zhang, Nan Chen, Jiahang Xu, Yuqing Yang</strong></p>
<p>Large Language Models (LLMs) require sophisticated prompting, yet current practices face challenges in structure, data integration, format sensitivity, and tooling. Existing methods lack comprehensive solutions for organizing complex prompts involving diverse data types (documents, tables, images) or managing presentation variations systematically. To address these gaps, we introduce POML (Prompt Orchestration Markup Language). POML employs component-based markup for logical structure (roles, tasks, examples), specialized tags for seamless data integration, and a CSS-like styling system to decouple content from presentation, reducing formatting sensitivity. It includes templating for dynamic prompts and a comprehensive developer toolkit (IDE support, SDKs) to improve version control and collaboration. We validate POML through two case studies demonstrating its impact on complex application integration (PomLink) and accuracy performance (TableQA), as well as a user study assessing its effectiveness in real-world development scenarios. </p>
<blockquote>
<p>大型语言模型（LLMs）需要复杂的提示，然而当前实践在结构、数据集成、格式敏感性和工具方面面临挑战。现有方法缺乏组织复杂提示的全面解决方案，这些提示涉及多种数据类型（文档、表格、图像）或系统地管理表达变体。为了解决这些差距，我们引入了POML（提示编排标记语言）。POML采用基于组件的标记来表示逻辑结构（角色、任务、示例），使用专用标签进行无缝数据集成，并采用类似CSS的样式系统来将内容与呈现相分离，降低格式敏感性。它包含用于动态提示的模板以及全面的开发者工具包（IDE支持、SDK），以改进版本控制和协作。我们通过两个案例研究验证了POML的影响，分别是其对复杂应用程序集成（PomLink）的影响以及其对准确性性能（TableQA）的影响，还通过用户研究评估了其在现实开发场景中的有效性。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.13948v1">PDF</a> All findings in this paper are derived from a POML snapshot as of   February 2025</p>
<p><strong>Summary</strong></p>
<p>LLM面临提示挑战，现有方法难以组织复杂的跨数据类型提示或系统化地管理呈现变化。为解决这些问题，引入POML（提示编排标记语言），通过基于组件的标记提供逻辑结构，使用专业标签无缝集成数据，并采用类似CSS的样式系统减少格式敏感性。POML还包括动态提示模板和全面的开发者工具包（IDE支持、SDK），可改善版本控制和协作能力。案例研究证实了其在复杂应用集成和准确性方面的效果。</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>LLM需要复杂的提示技术。</li>
<li>当前实践面临结构、数据集成、格式敏感性和工具方面的挑战。</li>
<li>POML解决了现有方法的不足，提供逻辑结构、无缝数据集成和减少格式敏感性。</li>
<li>POML具有动态提示模板和全面的开发者工具包。</li>
<li>通过两个案例研究证实了其在应用集成和准确性方面的优势。</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.13948">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-e3aac96b1b7d5630c35101c936c16a62.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b10c5fdfc94143a62b10ce9cbd3dc707.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="MME-SCI-A-Comprehensive-and-Challenging-Science-Benchmark-for-Multimodal-Large-Language-Models"><a href="#MME-SCI-A-Comprehensive-and-Challenging-Science-Benchmark-for-Multimodal-Large-Language-Models" class="headerlink" title="MME-SCI: A Comprehensive and Challenging Science Benchmark for   Multimodal Large Language Models"></a>MME-SCI: A Comprehensive and Challenging Science Benchmark for   Multimodal Large Language Models</h2><p><strong>Authors:Jiacheng Ruan, Dan Jiang, Xian Gao, Ting Liu, Yuzhuo Fu, Yangyang Kang</strong></p>
<p>Recently, multimodal large language models (MLLMs) have achieved significant advancements across various domains, and corresponding evaluation benchmarks have been continuously refined and improved. In this process, benchmarks in the scientific domain have played an important role in assessing the reasoning capabilities of MLLMs. However, existing benchmarks still face three key challenges: 1) Insufficient evaluation of models’ reasoning abilities in multilingual scenarios; 2) Inadequate assessment of MLLMs’ comprehensive modality coverage; 3) Lack of fine-grained annotation of scientific knowledge points. To address these gaps, we propose MME-SCI, a comprehensive and challenging benchmark. We carefully collected 1,019 high-quality question-answer pairs, which involve 3 distinct evaluation modes. These pairs cover four subjects, namely mathematics, physics, chemistry, and biology, and support five languages: Chinese, English, French, Spanish, and Japanese. We conducted extensive experiments on 16 open-source models and 4 closed-source models, and the results demonstrate that MME-SCI is widely challenging for existing MLLMs. For instance, under the Image-only evaluation mode, o4-mini achieved accuracy of only 52.11%, 24.73%, 36.57%, and 29.80% in mathematics, physics, chemistry, and biology, respectively, indicating a significantly higher difficulty level compared to existing benchmarks. More importantly, using MME-SCI’s multilingual and fine-grained knowledge attributes, we analyzed existing models’ performance in depth and identified their weaknesses in specific domains. The Data and Evaluation Code are available at <a target="_blank" rel="noopener" href="https://github.com/JCruan519/MME-SCI">https://github.com/JCruan519/MME-SCI</a>. </p>
<blockquote>
<p>最近，多模态大型语言模型（MLLMs）在各个领域都取得了显著的进步，相应的评估基准也在持续完善和改进。在此过程中，科学领域的基准在评估MLLM的推理能力方面发挥了重要作用。然而，现有基准仍面临三个关键挑战：1）在多语言场景下对模型的推理能力评估不足；2）对MLLM的综合模态覆盖评估不足；3）科学知识点的精细标注缺乏。为了解决这些差距，我们提出了MME-SCI这一全面且具有挑战性的基准。我们精心收集了1019组高质量的问题答案对，涉及三种独特的评估模式。这些配对涵盖了数学、物理、化学和生物四门学科，支持五种语言：中文、英语、法语、西班牙语和日语。我们对16个开源模型和4个闭源模型进行了广泛实验，结果表明MME-SCI对现有MLLMs具有广泛挑战性。例如，在仅图像评价模式下，o4-mini在数学、物理、化学和生物方面的准确率分别为52.11%、24.73%、36.57%和29.80%，表明其难度水平显著高于现有基准。更重要的是，利用MME-SCI的多语言和精细知识属性，我们深入分析了现有模型的性能，并识别了它们在特定领域的弱点。数据和评估代码可在<a target="_blank" rel="noopener" href="https://github.com/JCruan519/MME-SCI">https://github.com/JCruan519/MME-SCI</a>获得。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.13938v1">PDF</a> 9 pages, 6 figures, work in progress</p>
<p><strong>Summary</strong></p>
<p>多模态大型语言模型（MLLMs）在各领域取得显著进展，评估基准也在不断改进。然而，现有基准仍存在三个挑战：缺乏多语言场景中的推理能力评估、模态覆盖不足以及对科学知识点的精细标注缺失。为解决这些问题，提出了MME-SCI基准，包含1,019组高质量的问题答案对，涉及四种学科、三种评估模式以及五种语言。实验结果显示，MME-SCI对现有MLLMs具有挑战性。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>多模态大型语言模型（MLLMs）在多个领域取得进展，评估基准也在不断改进。</li>
<li>现有评估基准面临三个主要挑战：缺乏多语言场景中的推理能力评估、模态覆盖不足以及对科学知识点的精细标注缺失。</li>
<li>为解决这些挑战，提出了MME-SCI基准，包含高质量的问题答案对，覆盖多种学科和语言。</li>
<li>MME-SCI基准包括三种评估模式，对现有的MLLMs具有挑战性。</li>
<li>在图像仅评估模式下，某些模型在数学、物理、化学和生物学科的准确率较低，表明MME-SCI难度较高。</li>
<li>MME-SCI基准深入分析现有模型的性能，并指出其在特定领域的弱点。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.13938">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-954f542828854270e05f52ea3f8c16df.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a5df2ff4fdca0007006276e5e03bfff8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8eda865634b9591e1efcc2ba5d0b99d4.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-1c590b33c73c75ce17a70ef9d3eb5a39.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-6a1d2aa2da4c4e822c00cade6465cd3e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-1095c61aac86f68e5a70ce7c155aa9a8.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="InPars-Supercharging-Synthetic-Data-Generation-for-Information-Retrieval-Systems"><a href="#InPars-Supercharging-Synthetic-Data-Generation-for-Information-Retrieval-Systems" class="headerlink" title="InPars+: Supercharging Synthetic Data Generation for Information   Retrieval Systems"></a>InPars+: Supercharging Synthetic Data Generation for Information   Retrieval Systems</h2><p><strong>Authors:Matey Krastev, Miklos Hamar, Danilo Toapanta, Jesse Brouwers, Yibin Lei</strong></p>
<p>This work revisits and extends synthetic query generation pipelines for Neural Information Retrieval (NIR) by leveraging the InPars Toolkit, a reproducible, end-to-end framework for generating training data using large language models (LLMs). We first assess the reproducibility of the original InPars, InPars-V2, and Promptagator pipelines on the SciFact benchmark and validate their effectiveness using open-source reranker and generator models. Building on this foundation, we introduce two key extensions to the pipeline: (1) fine-tuning a query generator LLM via Contrastive Preference Optimization (CPO) to improve the signal quality in generated queries, and (2) replacing static prompt templates with dynamic, Chain-of-Thought (CoT) optimized prompts using the DSPy framework. Our results show that both extensions reduce the need for aggressive filtering while improving retrieval performance. All code, models, and synthetic datasets are publicly released to support further research at: \href{<a target="_blank" rel="noopener" href="https://github.com/danilotpnta/IR2-project%7D%7Bthis">https://github.com/danilotpnta/IR2-project}{this</a> https URL}. </p>
<blockquote>
<p>这项工作借助InPars工具包重新审查和扩展了神经信息检索（NIR）的合成查询生成管道。InPars工具包是一个可复制的端到端框架，用于利用大型语言模型（LLM）生成训练数据。我们首先评估了SciFact基准测试上原始InPars、InPars-V2和Promptagator管道的可重复性，并使用开源重新排序器和生成器模型验证其有效性。在此基础上，我们引入了管道的两个关键扩展：（1）通过对比偏好优化（CPO）对查询生成器LLM进行微调，以提高生成查询中的信号质量；（2）使用DSPy框架将静态提示模板替换为动态、经过思考链（CoT）优化的提示。我们的结果表明，这两个扩展在减少激烈过滤需求的同时提高了检索性能。所有代码、模型和合成数据集均已公开发布，以支持进一步的研究，可访问此链接：[<a target="_blank" rel="noopener" href="https://github.com/danilotpnta/IR2-project]%EF%BC%88%E8%AF%B7%E7%82%B9%E5%87%BB%E9%93%BE%E6%8E%A5%E6%9F%A5%E7%9C%8B%E8%AF%A6%E7%BB%86%E4%BF%A1%E6%81%AF%EF%BC%89%E3%80%82">https://github.com/danilotpnta/IR2-project]（请点击链接查看详细信息）。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.13930v1">PDF</a> </p>
<p><strong>摘要</strong><br>     该工作借助InPars工具包重新访问并扩展了用于神经信息检索的合成查询生成管道，该工具包是一个可复用的端到端框架，可用于借助大型语言模型生成训练数据。我们首先评估了原始InPars、InPars-V2和Promptagator管道在SciFact基准测试上的可重复性，并使用开源重排器和生成器模型验证了其有效性。在此基础上，我们对管道进行了两个关键扩展：（1）通过对比优化（CPO）对查询生成大型语言模型进行微调，以提高生成查询中的信号质量；（2）使用DSPy框架将静态提示模板替换为动态、经过思维链优化的提示。结果表明，这两个扩展在减少激烈过滤需求的同时提高了检索性能。所有代码、模型和合成数据集均已公开发布，以支持进一步研究：<a target="_blank" rel="noopener" href="https://github.com/danilotpnta/IR2-project">链接地址</a>。</p>
<p><strong>关键见解</strong></p>
<ol>
<li>利用InPars工具包重新访问和扩展神经信息检索的合成查询生成管道。</li>
<li>评估了多种现有管道在SciFact基准测试上的可重复性和有效性。</li>
<li>通过对比优化（CPO）微调查询生成的大型语言模型，提高生成查询的信号质量。</li>
<li>采用动态、经过思维链优化的提示，替代静态提示模板。</li>
<li>两个关键扩展在减少过滤需求的同时提高了检索性能。</li>
<li>所有相关代码、模型和合成数据集已公开，以便进一步研究和利用。</li>
<li>公开的代码和资源有助于推动该领域的研究进展。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.13930">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-0551edd14b78c664495f5b42066ac8b3.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a486179da26553cf9d66495d2b9f0529.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-a89c81b722f4800e9dd44cbab819d1eb.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-961f6ee0e1518460d32529fd79953a9d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9bf8eab573042ebe7a748e93e37db6dc.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="LLMind-2-0-Distributed-IoT-Automation-with-Natural-Language-M2M-Communication-and-Lightweight-LLM-Agents"><a href="#LLMind-2-0-Distributed-IoT-Automation-with-Natural-Language-M2M-Communication-and-Lightweight-LLM-Agents" class="headerlink" title="LLMind 2.0: Distributed IoT Automation with Natural Language M2M   Communication and Lightweight LLM Agents"></a>LLMind 2.0: Distributed IoT Automation with Natural Language M2M   Communication and Lightweight LLM Agents</h2><p><strong>Authors:Yuyang Du, Qun Yang, Liujianfu Wang, Jingqi Lin, Hongwei Cui, Soung Chang Liew</strong></p>
<p>Recent advances in large language models (LLMs) have sparked interest in their application to IoT and automation systems, particularly for facilitating device management through natural language instructions. However, existing centralized approaches face significant scalability challenges when managing and coordinating the collaboration between IoT devices of diverse capabilities in large-scale heterogeneous IoT systems. This paper introduces LLMind 2.0, a distributed IoT automation framework that addresses the scalability challenges through lightweight LLM-empowered device agents via natural language-based machine-to-machine (M2M) communication. Unlike previous LLM-controlled automation systems that rely on a centralized coordinator to generate device-specific code to be executed on individual devices, LLMind 2.0 distributes intelligence across individual devices through lightweight LLMs embedded in IoT devices. The central coordinator translates human instructions into simple subtasks described in natural human language, which are then processed by device-specific agents to generate device-specific code locally at the associated devices. This approach transcends device heterogeneity barriers by using natural language as a unified communication medium, enabling seamless collaboration between devices from different manufacturers. The system incorporates several key innovations: a Retrieval-Augmented Generation (RAG) mechanism for accurate subtask-to-API mapping, fine-tuned lightweight LLMs for reliable code generation, and a finite state machine-based task execution framework. Experimental validation in multi-robot warehouse scenarios and real-world WiFi network deployments demonstrates significant improvements in scalability, reliability, and privacy protection compared to the centralized approach. </p>
<blockquote>
<p>近期大型语言模型（LLM）的进步引发了人们对物联网（IoT）和自动化系统应用的关注，特别是在通过自然语言指令实现设备管理方面的应用。然而，现有的集中式方法在处理大规模异构物联网系统中不同功能的IoT设备的协作管理时，面临着巨大的可扩展性挑战。本文介绍了LLMind 2.0，这是一个分布式物联网自动化框架，它通过轻量级的LLM赋能设备代理，通过基于自然语言的机器对机器（M2M）通信来解决可扩展性挑战。不同于之前依赖于集中式协调器生成特定设备代码在单个设备上执行的LLM控制自动化系统，LLMind 2.0通过在IoT设备中嵌入轻量级的LLM，将智能分布在各个设备上。中央协调器将人类指令翻译成简单的子任务，用自然语言进行描述，然后这些任务由特定设备代理处理，生成特定设备的代码，在相关设备上本地执行。这种方法通过使用自然语言作为统一的通信媒介，超越了设备异构性的障碍，实现了不同制造商设备之间的无缝协作。该系统融合了多项关键创新技术：用于准确子任务到API映射的检索增强生成（RAG）机制、用于可靠代码生成的微调轻量级LLM以及基于有限状态机的任务执行框架。在多机器人仓库场景和真实WiFi网络部署中的实验验证表明，与集中式方法相比，它在可扩展性、可靠性和隐私保护方面都有显著提高。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.13920v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>近期大型语言模型（LLM）的进步激发了其在物联网（IoT）和自动化系统领域应用的关注。针对大规模异构物联网系统的设备管理，现有集中化方法面临可扩展性挑战。本文介绍LLMind 2.0，一个分布式物联网自动化框架，它通过自然语言基础的机器间（M2M）通信和轻量级LLM赋能的设备代理人来解决可扩展性问题。不同于依赖集中协调器生成针对特定设备的代码在个别设备上执行的先前LLM控制自动化系统，LLMind 2.0通过嵌入式轻量级LLM将智能分布在各个设备上。中央协调器将人类指令翻译成简单的子任务描述的自然语言，然后由设备特定代理人处理并在相关设备上本地生成特定设备的代码。这种方法通过使用自然语言作为统一的通信媒介，超越了设备异构性的障碍，实现了不同制造商设备之间的无缝协作。实验验证表明，与集中式方法相比，该系统在可扩展性、可靠性和隐私保护方面实现了显著改进。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLMind 2.0是一个用于物联网自动化的分布式框架，解决了集中化方法在大型异构物联网系统中的可扩展性问题。</li>
<li>通过轻量级LLM赋能的设备代理人实现智能分布，不同于依赖集中协调器生成设备特定代码的先前方法。</li>
<li>LLMind 2.0使用自然语言作为统一通信媒介，实现不同设备间的无缝协作，超越了设备异构性的障碍。</li>
<li>系统采用检索增强生成（RAG）机制，实现准确的任务到API的映射。</li>
<li>通过微调轻量级LLM实现可靠的代码生成。</li>
<li>采用有限状态机任务执行框架，提高系统效率和稳定性。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.13920">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-d84fcc47344943038bce9767cc8dfc95.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b314cb0a816adad26f83d55ea6396177.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-68bc0ccba117b81c808d41249ed1a6d7.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-8de939e53ea2e8147828a533b6cbd336.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-203d341ea8429f0943a94e7d1dc5f473.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="Structured-Agentic-Workflows-for-Financial-Time-Series-Modeling-with-LLMs-and-Reflective-Feedback"><a href="#Structured-Agentic-Workflows-for-Financial-Time-Series-Modeling-with-LLMs-and-Reflective-Feedback" class="headerlink" title="Structured Agentic Workflows for Financial Time-Series Modeling with   LLMs and Reflective Feedback"></a>Structured Agentic Workflows for Financial Time-Series Modeling with   LLMs and Reflective Feedback</h2><p><strong>Authors:Yihao Ang, Yifan Bao, Lei Jiang, Jiajie Tao, Anthony K. H. Tung, Lukasz Szpruch, Hao Ni</strong></p>
<p>Time-series data is central to decision-making in financial markets, yet building high-performing, interpretable, and auditable models remains a major challenge. While Automated Machine Learning (AutoML) frameworks streamline model development, they often lack adaptability and responsiveness to domain-specific needs and evolving objectives. Concurrently, Large Language Models (LLMs) have enabled agentic systems capable of reasoning, memory management, and dynamic code generation, offering a path toward more flexible workflow automation. In this paper, we introduce \textsf{TS-Agent}, a modular agentic framework designed to automate and enhance time-series modeling workflows for financial applications. The agent formalizes the pipeline as a structured, iterative decision process across three stages: model selection, code refinement, and fine-tuning, guided by contextual reasoning and experimental feedback. Central to our architecture is a planner agent equipped with structured knowledge banks, curated libraries of models and refinement strategies, which guide exploration, while improving interpretability and reducing error propagation. \textsf{TS-Agent} supports adaptive learning, robust debugging, and transparent auditing, key requirements for high-stakes environments such as financial services. Empirical evaluations on diverse financial forecasting and synthetic data generation tasks demonstrate that \textsf{TS-Agent} consistently outperforms state-of-the-art AutoML and agentic baselines, achieving superior accuracy, robustness, and decision traceability. </p>
<blockquote>
<p>时间序列数据在金融市场的决策制定中起着关键作用，然而，构建高性能、可解释和可审计的模型仍然是一个重大挑战。尽管自动化机器学习（AutoML）框架简化了模型开发，但它们往往缺乏适应性和对特定领域需求和不断变化目标的响应能力。同时，大型语言模型（LLM）使得能够推理、内存管理和动态代码生成的自主系统成为可能，从而为更灵活的工作流自动化提供了途径。在本文中，我们介绍了\textsf{TS-Agent}，这是一个模块化自主框架，旨在自动化和增强时间序列建模工作流程，用于金融应用程序。该代理将管道正式化为一个结构化的迭代决策过程，包括三个阶段：模型选择、代码精炼和微调，由上下文推理和实验反馈指导。我们架构的核心是配备结构化知识库、精选的模型和精炼策略库的规划代理，它引导探索，同时提高可解释性并减少错误传播。\textsf{TS-Agent}支持自适应学习、稳健的调试和透明的审计，这是金融服务等高风险环境中的关键要求。在多样化的金融预测和合成数据生成任务上的实证评估表明，\textsf{TS-Agent}持续优于最新的AutoML和自主基线，实现了更高的准确性、稳健性和决策可追溯性。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.13915v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文介绍了面向金融应用的自动化增强时间序列建模工作流的模块化智能框架TS-Agent。该框架将管道形式化为一个结构化迭代决策过程，包括模型选择、代码精炼和微调三个阶段，由上下文推理和实验反馈指导。TS-Agent支持自适应学习、稳健调试和透明审计，适应金融服务等高风险环境的关键要求。经验评估表明，TS-Agent在多种金融预测和合成数据生成任务上表现优于最新智能自动化和智能基线，具有准确性、稳健性和决策可追溯性的优势。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>时间序列数据在金融市场的决策中占据重要地位，但构建高性能、可解释和可审计的模型仍然是一个挑战。</li>
<li>当前自动化机器学习框架在适应特定领域需求和变化目标方面存在局限性。</li>
<li>大型语言模型为灵活的自动化工作流程提供了可能性，通过推理、内存管理和动态代码生成等功能。</li>
<li>TS-Agent是一个模块化智能框架，旨在自动化和改进时间序列建模工作流程，适用于金融应用。</li>
<li>TS-Agent将管道形式化为结构化迭代决策过程，包括模型选择、代码精炼和微调三个阶段。</li>
<li>TS-Agent的核心是一个配备结构化知识库和精炼策略和模型的规划智能体，它指导探索过程，提高可解释性并减少错误传播。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.13915">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-1f3210e86952085276b52c80703f45b1.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-82377df016cb7b99feee7a28380818d0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-da6611ae5533b12c6249dc2227cd029a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e7fd24b88fc1ba0eeeecbcbf168cdf9f.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="Driving-Style-Recognition-Like-an-Expert-Using-Semantic-Privileged-Information-from-Large-Language-Models"><a href="#Driving-Style-Recognition-Like-an-Expert-Using-Semantic-Privileged-Information-from-Large-Language-Models" class="headerlink" title="Driving Style Recognition Like an Expert Using Semantic Privileged   Information from Large Language Models"></a>Driving Style Recognition Like an Expert Using Semantic Privileged   Information from Large Language Models</h2><p><strong>Authors:Zhaokun Chen, Chaopeng Zhang, Xiaohan Li, Wenshuo Wang, Gentiane Venture, Junqiang Xi</strong></p>
<p>Existing driving style recognition systems largely depend on low-level sensor-derived features for training, neglecting the rich semantic reasoning capability inherent to human experts. This discrepancy results in a fundamental misalignment between algorithmic classifications and expert judgments. To bridge this gap, we propose a novel framework that integrates Semantic Privileged Information (SPI) derived from large language models (LLMs) to align recognition outcomes with human-interpretable reasoning. First, we introduce DriBehavGPT, an interactive LLM-based module that generates natural-language descriptions of driving behaviors. These descriptions are then encoded into machine learning-compatible representations via text embedding and dimensionality reduction. Finally, we incorporate them as privileged information into Support Vector Machine Plus (SVM+) for training, enabling the model to approximate human-like interpretation patterns. Experiments across diverse real-world driving scenarios demonstrate that our SPI-enhanced framework outperforms conventional methods, achieving F1-score improvements of 7.6% (car-following) and 7.9% (lane-changing). Importantly, SPI is exclusively used during training, while inference relies solely on sensor data, ensuring computational efficiency without sacrificing performance. These results highlight the pivotal role of semantic behavioral representations in improving recognition accuracy while advancing interpretable, human-centric driving systems. </p>
<blockquote>
<p>现有驾驶风格识别系统很大程度上依赖于用于训练的底层传感器衍生特征，从而忽视了人类专家所固有的丰富语义推理能力。这种差异导致了算法分类与专家判断之间的基本错位。为了弥补这一差距，我们提出了一种新型框架，该框架整合了源自大型语言模型的语义特权信息（SPI），以使识别结果与可解释的人类推理相一致。首先，我们引入了DriBehavGPT这一基于大型语言模型的交互式模块，用于生成驾驶行为的自然语言描述。这些描述随后通过文本嵌入和降维技术编码成机器学习兼容的表示形式。最后，我们将它们作为特权信息融入支持向量机增强版（SVM+）进行训练，使模型能够模拟人类般的解释模式。在多种真实世界驾驶场景中的实验表明，我们的SPI增强框架优于传统方法，在跟车和换道场景中F1分数分别提高了7.6%和7.9%。值得一提的是，SPI仅在训练过程中使用，而推理则完全依赖于传感器数据，确保了计算效率且不牺牲性能。这些结果凸显了语义行为表示在提升识别准确性以及推动可解释、以人类为中心的驾驶系统方面的重要作用。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.13881v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>在现有驾驶风格识别系统中，主要依赖于低级别传感器衍生的特征进行训练，忽略了人类专家丰富的语义推理能力。为了弥补这一差距，我们提出了一种新的框架，它集成了来自大型语言模型（LLM）的语义特权信息（SPI），以使识别结果与人类可解释的道理相一致。通过引入DriBehavGPT这一基于LLM的交互式模块，生成驾驶行为的自然语言描述，然后将其转化为机器学习兼容的表示形式。最后，我们将它们作为特权信息纳入SVM+（支持向量机增强版）进行训练，使模型能够模拟人类类似的解读模式。实验表明，在多种真实驾驶场景下，我们的SPI增强框架优于传统方法，车跟和车道变更的F1得分分别提高了7.6%和7.9%。SPI仅用于训练过程，而推理则依赖于传感器数据，确保了计算效率与性能。此研究强调了语义行为表示在提高识别精度和推动以人为本的驾驶系统方面的关键作用。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>当前驾驶风格识别系统主要依赖低级别传感器特征，忽略了人类专家的语义推理能力。</li>
<li>提出了一个集成语义特权信息（SPI）的新框架，以弥补与专家判断之间的差异。</li>
<li>引入LLM驱动的DriBehavGPT模块，生成驾驶行为的自然语言描述。</li>
<li>通过文本嵌入和降维技术，将自然语言描述转化为机器学习兼容的形式。</li>
<li>将SPI纳入SVM+进行训练，使模型能够模拟人类类似的解读模式。</li>
<li>在真实驾驶场景的实验中，SPI增强框架显著优于传统方法，F1得分有明显提升。</li>
<li>SPI仅用于训练，推理过程依赖传感器数据，保证了计算效率与性能。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.13881">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-3c78d90276787229285ad1ac02740712.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-b557b1bf9cd576667e1f23cce2b9aa17.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d1a5e206fe848a43835520085d2c62f9.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-1906f014b87696a9b6dcd229582339f2.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-4b4d65951dcc527ed2cdc368a226f99a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-872c0a2bee7d1c76b5987362958d146c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c820133fc2d5e8122d2c514bd4a6cc9e.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="Improved-Generalized-Planning-with-LLMs-through-Strategy-Refinement-and-Reflection"><a href="#Improved-Generalized-Planning-with-LLMs-through-Strategy-Refinement-and-Reflection" class="headerlink" title="Improved Generalized Planning with LLMs through Strategy Refinement and   Reflection"></a>Improved Generalized Planning with LLMs through Strategy Refinement and   Reflection</h2><p><strong>Authors:Katharina Stein, Nils Hodel, Daniel Fišer, Jörg Hoffmann, Michael Katz, Alexander Koller</strong></p>
<p>LLMs have recently been used to generate Python programs representing generalized plans in PDDL planning, i.e., plans that generalize across the tasks of a given PDDL domain. Previous work proposed a framework consisting of three steps: the LLM first generates a summary and then a strategy for the domain, both in natural language, and then implements that strategy as a Python program, that gets debugged on example planning tasks. In that work, only one strategy is generated and passed directly to the program generation. If the strategy is incorrect, its implementation will therefore result in an incorrect generalized plan. Here, we introduce an approach that generates the strategy in the form of pseudocode and enables automatic debugging of the pseudocode, hence allowing us to identify and fix errors prior to the generation of the generalized plan itself. Additionally, we extend the Python debugging phase with a reflection step prompting the LLM to pinpoint the reason for the observed plan failure. Finally, we take inspiration from LLM code generation to produce several program variants and pick the best one. Running experiments on 17 benchmark domains, we show that these extensions substantially improve (and never deteriorate) the quality of the generalized plans. In 12 of the domains, our best Python programs solve all tasks that can be generated with the respective instance generator. </p>
<blockquote>
<p>LLM最近被用于生成在PDDL规划中表示通用计划的Python程序，即适用于给定PDDL域的任务的通用计划。先前的工作提出了一个包含三个步骤的框架：LLM首先为领域生成摘要，然后生成自然语言策略，接着将该策略实现为Python程序，并在示例规划任务上进行调试。在那项工作中，只生成了一个策略并直接传递给程序生成。如果策略不正确，其实现将导致通用计划出错。在这里，我们介绍了一种以伪代码形式生成策略的方法，并实现了伪代码的自动调试，从而可以在生成通用计划本身之前识别和修复错误。此外，我们将Python调试阶段扩展为反思步骤，提示LLM指出观察到的计划失败的原因。最后，我们从LLM代码生成中汲取灵感，生成多个程序变体并选择最佳程序。在17个基准域上运行实验表明，这些扩展实质上改进了（从未恶化）通用计划的质量。在其中的12个领域，我们最佳的Python程序解决了可以使用相应实例生成器生成的所有任务。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.13876v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>LLMs在PDDL规划中的通用计划生成应用得到发展。先前的工作提出一个三步骤框架，通过LLM生成领域总结和策略（皆为自然语言），然后将其实现为Python程序，并在示例规划任务中进行调试。本工作引入生成策略伪代码的方法，并允许对伪代码进行自动调试，从而在生成广义计划本身之前发现和修复错误。此外，通过反思步骤扩展Python调试阶段，促使LLM指出观察到的计划失败的原因。从LLM代码生成中汲取灵感，生成多个程序变体并选择最佳方案。在17个基准域的实验中，这些扩展显著提高了广义计划的质量，并在其中12个领域中解决了所有可以由实例生成器生成的任务。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLMs已用于生成代表广义计划的Python程序。</li>
<li>先前的工作提出了一个三步骤框架，包括LLM生成领域总结和策略。</li>
<li>引入生成策略伪代码的方法，允许对伪代码进行自动调试。</li>
<li>扩展Python调试阶段，通过反思步骤识别计划失败的原因。</li>
<li>从LLM代码生成中获取灵感，生成多个程序变体。</li>
<li>这些扩展显著提高了广义计划的质量。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.13876">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-097b5453dec99c15fc452eaa914bdd12.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9987f8c0eac9e8d42c2944ff7aadd930.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0faa99151b07f4b232169583605db902.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b9ab26f6e126e6db9745a4ddaa0eadb7.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-1283fd876763454167a80a4e27f2d288.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-be8a2d9c5aabff7f8510a54e21127e42.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-29b697b927fa3a8a5976ab01da43786b.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="Learning-In-context-n-grams-with-Transformers-Sub-n-grams-Are-Near-stationary-Points"><a href="#Learning-In-context-n-grams-with-Transformers-Sub-n-grams-Are-Near-stationary-Points" class="headerlink" title="Learning In-context n-grams with Transformers: Sub-n-grams Are   Near-stationary Points"></a>Learning In-context n-grams with Transformers: Sub-n-grams Are   Near-stationary Points</h2><p><strong>Authors:Aditya Varre, Gizem Yüce, Nicolas Flammarion</strong></p>
<p>Motivated by empirical observations of prolonged plateaus and stage-wise progression during training, we investigate the loss landscape of transformer models trained on in-context next-token prediction tasks. In particular, we focus on learning in-context $n$-gram language models under cross-entropy loss, and establish a sufficient condition for parameter configurations to be stationary points. We then construct a set of parameter configurations for a simplified transformer model that represent $k$-gram estimators (for $k \leq n$), and show that the gradient of the population loss at these solutions vanishes in the limit of infinite sequence length and parameter norm. This reveals a key property of the loss landscape: {sub-$n$-grams are near-stationary points of the population cross-entropy loss}, offering theoretical insight into widely observed phenomena such as stage-wise learning dynamics and emergent phase transitions. These insights are further supported by numerical experiments that illustrate the learning dynamics of $n$-grams, characterized by discrete transitions between near-stationary solutions. </p>
<blockquote>
<p>受训练过程中的长期平稳期和分阶段进展的实证观察所驱动，我们研究了在上下文内令牌预测任务上训练的变压器模型的损失景观。特别是，我们关注在交叉熵损失下学习上下文中的n元语言模型，并为参数配置建立平稳点的充分条件。然后，我们为简化的变压器模型构建了一组参数配置，这些配置代表k元估算器（对于k≤n），并表明在这些解决方案处，人口损失的梯度在无限序列长度和参数范数的极限内消失。这揭示了损失景观的一个关键属性：{子n元是人口交叉熵损失的近似平稳点}，为广泛观察到的现象（如分阶段的学习动力和新兴的相变）提供了理论上的见解。这些见解得到了数值实验的进一步支持，这些实验说明了n元的学习动态，其特征是在近似平稳解之间的离散过渡。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.12837v2">PDF</a> ICML2025</p>
<p><strong>Summary</strong></p>
<p>本文探讨了训练过程中遇到的长期停滞和阶段性进展现象，研究了基于上下文预测任务的Transformer模型的损失景观。文章重点关注了交叉熵损失下的上下文内n元语言模型的学习过程，提出了参数配置的充分条件以成为驻点。通过构建简化Transformer模型的参数配置集，代表k元估算器（对于k≤n），文章展示了在无限序列长度和参数范数极限下，人口损失的梯度在这些解决方案处消失。这表明损失景观的一个关键属性：子n元是人口交叉熵损失的近似驻点，为广泛观察到的现象（如阶段性学习动力和新兴相变）提供了理论洞察。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>探讨了Transformer模型在上下文预测任务中的损失景观。</li>
<li>分析了交叉熵损失下的上下文内n元语言模型的学习过程。</li>
<li>提出了参数配置的充分条件，使其成为驻点。</li>
<li>通过构建简化模型展示了子n元是人口交叉熵损失的近似驻点。</li>
<li>揭示了阶段性学习动力和新兴相变等广泛观察现象的理论原因。</li>
<li>数值实验支持了上述理论洞察，展示了n元的学习动态，表现为近驻点解之间的离散过渡。</li>
<li>文中分析了序列长度和参数范数对损失景观的影响。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.12837">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-a53ceb9c23a365c60e9ef28a669ec4f6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-816abe0aab5fb94db4e95636ee63409a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-067b7d3fe3e51a0d3de4a2c5962ea0f9.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="CRED-SQL-Enhancing-Real-world-Large-Scale-Database-Text-to-SQL-Parsing-through-Cluster-Retrieval-and-Execution-Description"><a href="#CRED-SQL-Enhancing-Real-world-Large-Scale-Database-Text-to-SQL-Parsing-through-Cluster-Retrieval-and-Execution-Description" class="headerlink" title="CRED-SQL: Enhancing Real-world Large Scale Database Text-to-SQL Parsing   through Cluster Retrieval and Execution Description"></a>CRED-SQL: Enhancing Real-world Large Scale Database Text-to-SQL Parsing   through Cluster Retrieval and Execution Description</h2><p><strong>Authors:Shaoming Duan, Zirui Wang, Chuanyi Liu, Zhibin Zhu, Yuhao Zhang, Peiyi Han, Liang Yan, Zewu Penge</strong></p>
<p>Recent advances in large language models (LLMs) have significantly improved the accuracy of Text-to-SQL systems. However, a critical challenge remains: the semantic mismatch between natural language questions (NLQs) and their corresponding SQL queries. This issue is exacerbated in large-scale databases, where semantically similar attributes hinder schema linking and semantic drift during SQL generation, ultimately reducing model accuracy. To address these challenges, we introduce CRED-SQL, a framework designed for large-scale databases that integrates Cluster Retrieval and Execution Description. CRED-SQL first performs cluster-based large-scale schema retrieval to pinpoint the tables and columns most relevant to a given NLQ, alleviating schema mismatch. It then introduces an intermediate natural language representation-Execution Description Language (EDL)-to bridge the gap between NLQs and SQL. This reformulation decomposes the task into two stages: Text-to-EDL and EDL-to-SQL, leveraging LLMs’ strong general reasoning capabilities while reducing semantic deviation. Extensive experiments on two large-scale, cross-domain benchmarks-SpiderUnion and BirdUnion-demonstrate that CRED-SQL achieves new state-of-the-art (SOTA) performance, validating its effectiveness and scalability. Our code is available at <a target="_blank" rel="noopener" href="https://github.com/smduan/CRED-SQL.git">https://github.com/smduan/CRED-SQL.git</a> </p>
<blockquote>
<p>大型语言模型（LLM）的最新进展显著提高了文本到SQL系统的准确性。然而，仍然存在一个关键挑战：自然语言问题（NLQ）与其对应的SQL查询之间的语义不匹配。在大规模数据库中，这个问题更为严重，语义上相似的属性阻碍了模式链接和SQL生成过程中的语义漂移，最终导致模型精度降低。为了解决这些挑战，我们引入了CRED-SQL框架，该框架适用于大规模数据库，集成了Cluster Retrieval和Execution Description。CRED-SQL首先执行基于聚类的大规模模式检索，以定位与给定NLQ最相关的表和列，缓解模式不匹配问题。然后，它引入了一种中间自然语言表示形式——执行描述语言（EDL）——来弥合NLQ和SQL之间的差距。这种重新表述将任务分解为两个阶段：文本到EDL和EDL到SQL，利用LLM的强大通用推理能力，同时减少语义偏差。在两个大规模、跨域基准测试SpiderUnion和BirdUnion上的广泛实验表明，CRED-SQL达到了最新的最佳性能水平，验证了其有效性和可扩展性。我们的代码可在<a target="_blank" rel="noopener" href="https://github.com/smduan/CRED-SQL.git%E4%B8%8A%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/smduan/CRED-SQL.git上找到。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.12769v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>大型语言模型（LLM）在文本到SQL系统的准确性方面取得了显著进展，但语义匹配问题仍是关键挑战。针对大型数据库中语义相似属性导致的模式链接和语义漂移问题，提出CRED-SQL框架。它通过集群检索执行描述，首先进行基于集群的大规模模式检索，以针对给定的自然语言问题定位相关的表和列，然后引入中间的自然语言表示形式——执行描述语言（EDL），缩小NLQ和SQL之间的差距。实验证明，CRED-SQL在大型跨域基准测试中实现了最新状态的艺术（SOTA）性能。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>大型语言模型（LLM）已提高文本到SQL系统的准确性。</li>
<li>语义匹配问题是文本到SQL转换中的关键挑战。</li>
<li>CRED-SQL框架针对大型数据库设计，通过集群检索和执行描述来解决语义匹配问题。</li>
<li>CRED-SQL利用集群检索定位与NLQ相关的表和列，缓解模式不匹配问题。</li>
<li>EDL作为中间的自然语言表示形式，缩小了NLQ和SQL之间的差距。</li>
<li>CRED-SQL将任务分为两个阶段：文本到EDL和EDL到SQL，利用LLM的强大通用推理能力。</li>
<li>实验证明CRED-SQL在大型跨域基准测试中实现了最新状态的艺术（SOTA）性能。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.12769">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-85a6bc96031ec9965af7f3de01fa216e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1b48eb7254e77ced700f8a9d1290a021.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-5b802327ed1dc1597e89b5ff60f532d7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5d8eb88987412355ca099794b58b563e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c1e4ee772fd2e12f8969357afa5f8a72.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="ConTextTab-A-Semantics-Aware-Tabular-In-Context-Learner"><a href="#ConTextTab-A-Semantics-Aware-Tabular-In-Context-Learner" class="headerlink" title="ConTextTab: A Semantics-Aware Tabular In-Context Learner"></a>ConTextTab: A Semantics-Aware Tabular In-Context Learner</h2><p><strong>Authors:Marco Spinaci, Marek Polewczyk, Maximilian Schambach, Sam Thelin</strong></p>
<p>Tabular in-context learning (ICL) has recently achieved state-of-the-art (SOTA) performance on several tabular prediction tasks. Previously restricted to classification problems on small tables, recent advances such as TabPFN and TabICL have extended its use to larger datasets. Although current table-native ICL architectures are architecturally efficient and well-adapted to tabular data structures, their exclusive training on synthetic data limits their ability to fully leverage the rich semantics and world knowledge contained in real-world tabular data. At the other end of the spectrum, tabular ICL models based on pretrained large language models such as TabuLa-8B integrate deep semantic understanding and world knowledge but are only able to make use of a small amount of context due to inherent architectural limitations. With the aim to combine the best of both these worlds, we introduce ConTextTab, integrating semantic understanding and alignment into a table-native ICL framework. By employing specialized embeddings for different data modalities and by training on large-scale real-world tabular data, our model is competitive with SOTA across a broad set of benchmarks while setting a new standard on the semantically rich CARTE benchmark. Code and model checkpoints are available at: <a target="_blank" rel="noopener" href="https://github.com/SAP-samples/contexttab">https://github.com/SAP-samples/contexttab</a> </p>
<blockquote>
<p>表格上下文学习（ICL）最近已在多个表格预测任务上达到了最新技术水平（SOTA）。之前仅限于小型表格的分类问题，最近的进展，如TabPFN和TabICL，已经将其应用扩展到了更大的数据集。尽管当前的表格原生ICL架构在效率上很高，并且很适应表格数据结构，但它们仅在合成数据进行训练，这限制了它们充分利用现实世界表格数据中的丰富语义和全球知识的能力。另一方面，基于预训练的大型语言模型的表格ICL模型，如TabuLa-8B，融合了深度语义理解和全球知识，但由于其固有的架构限制，只能利用少量的上下文。为了结合这两个方面的优点，我们引入了ConTextTab，将语义理解和对齐集成到表格原生ICL框架中。通过针对不同数据模态采用专用嵌入，并在大规模现实世界表格数据上进行训练，我们的模型在一系列基准测试中具有竞争力，并在语义丰富的CARTE基准测试上设定了新的标准。代码和模型检查点可通过以下链接获取：<a target="_blank" rel="noopener" href="https://github.com/SAP-samples/contexttab">https://github.com/SAP-samples/contexttab</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.10707v3">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>Tabular in-context learning（ICL）已在多个表格预测任务上达到最新技术水平。近期的发展如TabPFN和TabICL，将其应用范围扩展到了大型数据集上。虽然当前的表格原生ICL架构对表格数据结构进行了高效适应，但它们仅在合成数据上进行训练，无法充分利用现实世界表格数据中的丰富语义和全球知识。相反，基于预训练的大型语言模型的表格ICL模型（如TabuLa-8B）融合了深层语义理解和全球知识，但由于其固有的架构限制，只能使用少量的上下文信息。为了结合两者的优点，我们推出了ConTextTab，将语义理解和对齐集成到表格原生ICL框架中。通过为不同数据模式采用专业嵌入，并在大规模现实世界表格数据上进行训练，我们的模型在广泛的基准测试上达到最新技术水平，尤其是在语义丰富的CARTE基准上设定了新的标准。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Tabular in-context learning (ICL) 在多个表格预测任务上达到 state-of-the-art (SOTA) 性能。</li>
<li>ICL 近期扩展到了大型数据集上，如 TabPFN 和 TabICL 等发展。</li>
<li>表格原生ICL架构对表格数据结构进行了高效适应，但受限于仅在合成数据上的训练。</li>
<li>基于预训练的大型语言模型的表格ICL模型（如TabuLa-8B）能够利用深层语义理解和全球知识，但受限于上下文信息的使用量。</li>
<li>ConTextTab 结合了表格原生ICL和基于预训练的大型语言模型的优点。</li>
<li>ConTextTab 通过采用专业嵌入和大规模现实世界表格数据训练，提高了模型性能。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.10707">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-79f65a5d7367d989bb251c0a30c1097f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-abeb4fabae3f83d5f222495a006493bc.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="Flexible-Operator-Fusion-for-Fast-Sparse-Transformer-with-Diverse-Masking-on-GPU"><a href="#Flexible-Operator-Fusion-for-Fast-Sparse-Transformer-with-Diverse-Masking-on-GPU" class="headerlink" title="Flexible Operator Fusion for Fast Sparse Transformer with Diverse   Masking on GPU"></a>Flexible Operator Fusion for Fast Sparse Transformer with Diverse   Masking on GPU</h2><p><strong>Authors:Wenhao Dai, Haodong Deng, Mengfei Rong, Xinyu Yang, Hongyu Liu, Fangxin Liu, Hailong Yang, Qianwen Cao, Qingxiao Sun</strong></p>
<p>Large language models are popular around the world due to their powerful understanding capabilities. As the core component of LLMs, accelerating Transformer through parallelization has gradually become a hot research topic. Mask layers introduce sparsity into Transformer to reduce calculations. However, previous works rarely focus on the performance optimization of sparse Transformer. Moreover, rule-based mechanisms ignore the fusion opportunities of mixed-type operators and fail to adapt to various sequence lengths. To address the above problems, we propose STOF, a framework that incorporates optimizations for Sparse Transformer via flexible masking and operator fusion on GPU. We firstly unify the storage format and kernel implementation for the multi-head attention. Then, we map fusion schemes to compilation templates and determine the optimal parameter setting through a two-stage search engine. The experimental results show that compared to the state-of-the-art work, STOF achieves maximum speedups of 1.7x in MHA computation and 1.5x in end-to-end inference. </p>
<blockquote>
<p>大型语言模型因其强大的理解能力而在全球范围内广受欢迎。作为大型语言模型的核心组件，通过并行化加速Transformer逐渐成为一个热门研究课题。掩码层将稀疏性引入Transformer中以减少计算。然而，之前的工作很少关注稀疏Transformer的性能优化。此外，基于规则的方法忽略了混合类型操作符的融合机会，并且无法适应各种序列长度。为了解决上述问题，我们提出了STOF，这是一个通过灵活的掩码和GPU上的操作符融合对稀疏Transformer进行优化的框架。我们首先对多头注意力的存储格式和内核实现进行了统一。然后，我们将融合方案映射到编译模板上，并通过两阶段搜索引擎确定最佳参数设置。实验结果表明，与最新工作相比，STOF在多头注意力计算中实现了最高达1.7倍的加速，在端到端推理中实现了最高达1.5倍的加速。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.06095v2">PDF</a> </p>
<p><strong>Summary</strong>：大语言模型因强大的理解力而全球流行。作为其核心组件，通过并行化加速Transformer逐渐成为热门研究课题。掩码层将稀疏性引入Transformer以减少计算。然而，之前的研究很少关注稀疏Transformer的性能优化。此外，基于规则机制忽略了混合类型算子的融合机会，且不能适应各种序列长度。为解决上述问题，我们提出STOF框架，通过灵活的掩码和GPU上的算子融合对稀疏Transformer进行优化。我们统一了多头注意力存储格式和内核实现。然后，我们将融合方案映射到编译模板上，并通过两阶段搜索引擎确定最佳参数设置。实验结果表明，与最新工作相比，STOF在多头注意力计算和端到端推理方面分别实现了最高达1.7倍和1.5倍的加速。</p>
<p><strong>Key Takeaways</strong>：</p>
<ol>
<li>大语言模型因其强大的理解力而备受关注，其核心组件Transformer的并行化加速成为研究热点。</li>
<li>掩码层引入稀疏性以减少Transformer的计算，但之前的研究未充分关注稀疏Transformer的性能优化。</li>
<li>基于规则机制不能适应各种序列长度，且忽略了混合类型算子的融合机会。</li>
<li>STOF框架通过灵活的掩码和GPU上的算子融合来优化稀疏Transformer。</li>
<li>STOF统一了多头注意力的存储格式和内核实现。</li>
<li>STOF将融合方案映射到编译模板上，并通过两阶段搜索引擎优化参数设置。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.06095">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-9f5e5d8c0cf8e4c7dc0f64e7a453af49.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8db784096736753c613ab3c914ff0a1b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b4135a1b2d1f869f399f4af234e6fc48.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b558a42012cf4c513fb1f14d7222f86c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6d0d2ddd6e9a8a2f0f5d4efc6bb74f9b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f30666b6cfcd981956ca41ca313a2028.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c38f19bbd100a014d9e97a85a2dedfc3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b9a1502c111ecf03355e8aec436e711e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-de2cb796caa192ec2c1c45e07193545a.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="Soft-Reasoning-Navigating-Solution-Spaces-in-Large-Language-Models-through-Controlled-Embedding-Exploration"><a href="#Soft-Reasoning-Navigating-Solution-Spaces-in-Large-Language-Models-through-Controlled-Embedding-Exploration" class="headerlink" title="Soft Reasoning: Navigating Solution Spaces in Large Language Models   through Controlled Embedding Exploration"></a>Soft Reasoning: Navigating Solution Spaces in Large Language Models   through Controlled Embedding Exploration</h2><p><strong>Authors:Qinglin Zhu, Runcong Zhao, Hanqi Yan, Yulan He, Yudong Chen, Lin Gui</strong></p>
<p>Large Language Models (LLMs) struggle with complex reasoning due to limited diversity and inefficient search. We propose Soft Reasoning, an embedding-based search framework that optimises the embedding of the first token to guide generation. It combines (1) embedding perturbation for controlled exploration and (2) Bayesian optimisation to refine embeddings via a verifier-guided objective, balancing exploration and exploitation. This approach improves reasoning accuracy and coherence while avoiding reliance on heuristic search. Experiments demonstrate superior correctness with minimal computation, making it a scalable, model-agnostic solution. The code is released at <a target="_blank" rel="noopener" href="https://github.com/alickzhu/Soft-Reasoning">https://github.com/alickzhu/Soft-Reasoning</a>. </p>
<blockquote>
<p>大型语言模型（LLM）由于多样性有限和搜索效率低下，难以应对复杂推理。我们提出了Soft Reasoning，这是一种基于嵌入的搜索框架，通过优化第一个词的嵌入来引导生成。它结合了（1）嵌入扰动进行受控探索和（2）通过验证器引导的目标优化贝叶斯方法来平衡探索和利用。这种方法提高了推理的准确性和连贯性，同时避免了依赖启发式搜索。实验证明，该方法计算正确率极高且计算量极小，是一种可扩展且适用于多种模型的解决方案。代码已发布在<a target="_blank" rel="noopener" href="https://github.com/alickzhu/Soft-Reasoning">https://github.com/alickzhu/Soft-Reasoning</a>。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.24688v3">PDF</a> Accepted as a Spotlight at ICML 2025</p>
<p><strong>Summary</strong></p>
<p>大型语言模型（LLMs）在处理复杂推理时面临多样性有限和搜索效率低下的问题。本文提出一种基于嵌入的搜索框架——Soft Reasoning，通过优化第一个词的嵌入来引导生成。该框架结合了嵌入扰动实现可控探索，并利用贝叶斯优化通过验证器指导的目标来优化嵌入，平衡探索与利用。此方法提高了推理的准确性和连贯性，避免了依赖启发式搜索，实验表明其在计算量小的情况下能显著提高正确性，成为一种可扩展的模型无关解决方案。相关代码已发布在<a target="_blank" rel="noopener" href="https://github.com/alickzhu/Soft-Reasoning%E4%B8%8A%E3%80%82">https://github.com/alickzhu/Soft-Reasoning上。</a></p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>LLM在处理复杂推理时面临多样性不足和搜索效率低下的问题。</li>
<li>Soft Reasoning是一个基于嵌入的搜索框架，旨在解决这些问题。</li>
<li>Soft Reasoning通过优化第一个词的嵌入来引导生成。</li>
<li>该框架结合了嵌入扰动和贝叶斯优化来实现探索与利用的平衡。</li>
<li>Soft Reasoning提高了推理的准确性和连贯性，并避免了启发式搜索的依赖。</li>
<li>实验结果显示Soft Reasoning在减少计算量的同时能提高正确性。</li>
<li>Soft Reasoning是一种可扩展的模型无关解决方案。</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.24688">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-430df92858fe6bb56c66a01ffa6e4c7e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0a586f648fbc70a5d9bd14e4f8936c6b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-93a9cbc0df0a550af714909d034dd57e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-df09bf3de0a8a1e8faf6a32e501648ba.jpg" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="Building-Instruction-Tuning-Datasets-from-Human-Written-Instructions-with-Open-Weight-Large-Language-Models"><a href="#Building-Instruction-Tuning-Datasets-from-Human-Written-Instructions-with-Open-Weight-Large-Language-Models" class="headerlink" title="Building Instruction-Tuning Datasets from Human-Written Instructions   with Open-Weight Large Language Models"></a>Building Instruction-Tuning Datasets from Human-Written Instructions   with Open-Weight Large Language Models</h2><p><strong>Authors:Youmi Ma, Sakae Mizuki, Kazuki Fujii, Taishi Nakamura, Masanari Ohi, Hinari Shimada, Taihei Shiotani, Koshiro Saito, Koki Maeda, Kakeru Hattori, Takumi Okamoto, Shigeki Ishida, Rio Yokota, Hiroya Takamura, Naoaki Okazaki</strong></p>
<p>Instruction tuning is crucial for enabling Large Language Models (LLMs) to solve real-world tasks. Prior work has shown the effectiveness of instruction-tuning data synthesized solely from LLMs, raising a fundamental question: Do we still need human-originated signals for instruction tuning? This work answers the question affirmatively: we build state-of-the-art instruction-tuning datasets sourced from human-written instructions, by simply pairing them with LLM-generated responses. LLMs fine-tuned on our datasets consistently outperform those fine-tuned on existing ones. Our data construction approach can be easily adapted to other languages; we build datasets for Japanese and confirm that LLMs tuned with our data reach state-of-the-art performance. Analyses suggest that instruction-tuning in a new language allows LLMs to follow instructions, while the tuned models exhibit a notable lack of culture-specific knowledge in that language. The datasets and fine-tuned models will be publicly available. Our datasets, synthesized with open-weight LLMs, are openly distributed under permissive licenses, allowing for diverse use cases. </p>
<blockquote>
<p>指令微调对于使大型语言模型（LLM）解决现实世界任务至关重要。先前的工作已经证明了仅通过LLM合成的指令微调数据的有效性，这引发了一个根本问题：我们是否仍然需要人类产生的信号来进行指令微调？这项工作肯定地回答了这个问题：我们通过将人类编写的指令与LLM生成的响应简单配对，构建了最先进的指令微调数据集。在我们数据集上微调的LLM始终优于在现有数据集上微调的LLM。我们的数据构建方法可以轻松地适应其他语言；我们为日语构建了数据集，并确认使用我们的数据调校的LLM达到了最先进的性能。分析表明，在新语言中进行指令微调可以使LLM遵循指令，而经过调校的模型在该语言的特定文化知识方面表现出明显的缺乏。数据集和经过训练的模型将公开可用。我们的数据集是与开放权重LLM合成的，根据许可协议公开发布，支持多种用例。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.23714v2">PDF</a> COLM 2025; Datasets are available at   <a target="_blank" rel="noopener" href="https://huggingface.co/datasets/tokyotech-llm/lmsys-chat-1m-synth">https://huggingface.co/datasets/tokyotech-llm/lmsys-chat-1m-synth</a></p>
<p><strong>Summary</strong></p>
<p>大型语言模型（LLM）的指令微调对于解决现实世界任务至关重要。本研究肯定了人类原始信号在指令微调中的必要性，通过简单地将人类编写的指令与LLM生成的响应配对，构建了最先进的指令微调数据集。在数据集上微调的LLM性能始终优于在现有数据集上微调的模型。本数据集的构建方法可轻松适应其他语言，并为日语构建了数据集，经本数据训练的LLM达到了最先进的性能。分析表明，在新语言中的指令微调使LLM能够遵循指令，而经过训练的模型在该语言的特定文化知识方面存在明显的不足。数据集和经过训练的模型将公开可用。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>指令微调对LLM解决现实世界任务至关重要。</li>
<li>本研究肯定了在指令微调中人类原始信号的重要性。</li>
<li>通过配对人类编写的指令与LLM生成的响应，构建了最先进的指令微调数据集。</li>
<li>在该数据集上微调的LLM性能优于现有数据集上的性能。</li>
<li>数据集构建方法可适应其他语言，例如为日语构建的数据集。</li>
<li>在新语言中的指令微调使LLM能够遵循指令，但存在对特定文化的知识不足。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.23714">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-eec49a2596f7b3ab12d97addfe21d1b5.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-c2bc6c8c7f41c94b0d3ba25642418a54.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-dfd73e1fd47dac0730c7db59417a1860.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-bf6e1e06e6b754b13a148bfb5868875f.jpg" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="AutoComPose-Automatic-Generation-of-Pose-Transition-Descriptions-for-Composed-Pose-Retrieval-Using-Multimodal-LLMs"><a href="#AutoComPose-Automatic-Generation-of-Pose-Transition-Descriptions-for-Composed-Pose-Retrieval-Using-Multimodal-LLMs" class="headerlink" title="AutoComPose: Automatic Generation of Pose Transition Descriptions for   Composed Pose Retrieval Using Multimodal LLMs"></a>AutoComPose: Automatic Generation of Pose Transition Descriptions for   Composed Pose Retrieval Using Multimodal LLMs</h2><p><strong>Authors:Yi-Ting Shen, Sungmin Eum, Doheon Lee, Rohit Shete, Chiao-Yi Wang, Heesung Kwon, Shuvra S. Bhattacharyya</strong></p>
<p>Composed pose retrieval (CPR) enables users to search for human poses by specifying a reference pose and a transition description, but progress in this field is hindered by the scarcity and inconsistency of annotated pose transitions. Existing CPR datasets rely on costly human annotations or heuristic-based rule generation, both of which limit scalability and diversity. In this work, we introduce AutoComPose, the first framework that leverages multimodal large language models (MLLMs) to automatically generate rich and structured pose transition descriptions. Our method enhances annotation quality by structuring transitions into fine-grained body part movements and introducing mirrored&#x2F;swapped variations, while a cyclic consistency constraint ensures logical coherence between forward and reverse transitions. To advance CPR research, we construct and release two dedicated benchmarks, AIST-CPR and PoseFixCPR, supplementing prior datasets with enhanced attributes. Extensive experiments demonstrate that training retrieval models with AutoComPose yields superior performance over human-annotated and heuristic-based methods, significantly reducing annotation costs while improving retrieval quality. Our work pioneers the automatic annotation of pose transitions, establishing a scalable foundation for future CPR research. </p>
<blockquote>
<p>姿态检索（CPR）允许用户通过指定参考姿态和过渡描述来搜索人体姿态。然而，由于标注的姿态过渡缺乏且不一致，该领域的进展受到了阻碍。现有的CPR数据集依赖于昂贵的人工标注或基于启发式规则的生成，这两者都限制了其可扩展性和多样性。在这项工作中，我们引入了AutoComPose，这是第一个利用多模态大型语言模型（MLLMs）自动生成丰富且结构化的姿态过渡描述的框架。我们的方法通过将过渡结构化细粒度的身体部位运动并引入镜像&#x2F;交换变化来提高标注质量，同时循环一致性约束确保了正向和反向过渡之间的逻辑连贯性。为了推动CPR研究，我们构建并发布了两个专用基准测试集AIST-CPR和PoseFixCPR，补充了以前的数据集并增强了其属性。大量实验表明，使用AutoComPose训练检索模型比人工标注和基于启发式的方法取得了优越的性能，在降低标注成本的同时提高了检索质量。我们的工作开创了姿态过渡的自动标注先河，为未来的CPR研究建立了可扩展的基础。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.22884v2">PDF</a> ICCV 2025</p>
<p><strong>摘要</strong></p>
<p>本研究介绍了AutoComPose框架，它利用多模态大型语言模型（MLLMs）自动生成丰富且结构化的姿势过渡描述，解决了传统姿势检索中注释数据稀缺和不一致的问题。通过精细的身体部位运动结构化和引入镜像&#x2F;交换变化，提高了注释质量。循环一致性约束确保了正向和反向过渡之间的逻辑连贯性。同时构建了两个专用的基准测试AIST-CPR和PoseFixCPR，补充了以前的数据集以增强属性。实验表明，使用AutoComPose训练检索模型在性能和成本上均优于人工注释和基于启发式的方法，为未来的姿势检索研究建立了可扩展的基础。</p>
<p><strong>关键见解</strong></p>
<ol>
<li>AutoComPose框架利用多模态大型语言模型自动生成结构化姿势过渡描述，解决注释数据稀缺问题。</li>
<li>通过身体部位运动的结构化和镜像&#x2F;交换变化，提高了注释质量。</li>
<li>循环一致性约束确保正向和反向过渡的逻辑连贯性。</li>
<li>构建了两个基准测试AIST-CPR和PoseFixCPR，增强了以前数据集的属性。</li>
<li>与人工注释和基于启发式的方法相比，使用AutoComPose训练检索模型表现优越。</li>
<li>自动注释姿势过渡的引入为未来的姿势检索研究建立了可扩展的基础。</li>
<li>该方法显著降低了注释成本，同时提高了检索质量。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.22884">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-69dcfdbb697f0577cc53ab290ed46746.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-8f8f539639d3811a7f530a269a62c522.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c0c435f4a9d7de39795f2024be42371c.jpg" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1><h2 id="ScaffoldGPT-A-Scaffold-based-GPT-Model-for-Drug-Optimization"><a href="#ScaffoldGPT-A-Scaffold-based-GPT-Model-for-Drug-Optimization" class="headerlink" title="ScaffoldGPT: A Scaffold-based GPT Model for Drug Optimization"></a>ScaffoldGPT: A Scaffold-based GPT Model for Drug Optimization</h2><p><strong>Authors:Xuefeng Liu, Songhao Jiang, Ian Foster, Jinbo Xu, Rick Stevens</strong></p>
<p>Drug optimization has become increasingly crucial in light of fast-mutating virus strains and drug-resistant cancer cells. Nevertheless, it remains challenging as it necessitates retaining the beneficial properties of the original drug while simultaneously enhancing desired attributes beyond its scope. In this work, we aim to tackle this challenge by introducing ScaffoldGPT, a novel Generative Pretrained Transformer (GPT) designed for drug optimization based on molecular scaffolds. Our work comprises three key components: (1) A three-stage drug optimization approach that integrates pretraining, finetuning, and decoding optimization. (2) A novel two-phase incremental pre-training strategy for scaffold-based drug optimization. (3) A token-level decoding optimization strategy, Top-N, that enabling controlled, reward-guided generation using the pretrained or finetuned GPT. We demonstrate via a comprehensive evaluation on COVID and cancer benchmarks that ScaffoldGPT outperforms the competing baselines in drug optimization benchmarks, while excelling in preserving original functional scaffold and enhancing desired properties. </p>
<blockquote>
<p>随着病毒株的快速突变和药物耐药癌细胞的产生，药物优化变得愈发关键。然而，由于需要在保留原药物有益属性的同时，同步增强所需属性并扩展其应用范围，这仍然是一个挑战。在这项工作中，我们旨在通过引入ScaffoldGPT来解决这一挑战，ScaffoldGPT是一种基于分子骨架用于药物优化的新型生成预训练转换器（GPT）。我们的工作包含三个关键部分：（1）一个三阶段的药品优化方法，该方法结合了预训练、微调和解码优化。（2）一种用于骨架基础药物优化的新型两阶段增量预训练策略。（3）一种Token级别的解码优化策略Top-N，该策略使用预训练或微调后的GPT，通过控制奖励指导生成。我们通过针对COVID和癌症基准测试的综合评估证明，ScaffoldGPT在药物优化基准测试中优于竞争对手，同时在保留原始功能性骨架和增强所需属性方面表现出色。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.06891v3">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文介绍了针对药物优化挑战的新方法，即使用名为ScaffoldGPT的新型生成预训练转换器（GPT）进行基于分子骨架的药物优化。该研究包括三个关键部分：三阶段药物优化方法、两阶段增量预训练策略和名为Top-N的令牌级解码优化策略。实验证明，ScaffoldGPT在药物优化基准测试中优于竞争对手，同时在保留原始功能骨架和增强所需属性方面表现出色。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ScaffoldGPT是一种针对药物优化的新型生成预训练转换器（GPT）。</li>
<li>研究提出了一个三阶段药物优化方法，包括预训练、微调和解码优化。</li>
<li>采用了两阶段增量预训练策略，用于基于骨架的药物优化。</li>
<li>引入了Top-N令牌级解码优化策略，实现受控、奖励引导生成。</li>
<li>ScaffoldGPT在COVID和癌症基准测试上的表现优于其他基准药物优化方法。</li>
<li>ScaffoldGPT能够在保留药物原始功能骨架的同时增强其所需属性。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.06891">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-ca29b526240f2112941254eb7c4bc7a9.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-c0f519ab21fb7d942477ecb8352949ab.jpg" align="middle">
</details>


<h1 id="-19"><a href="#-19" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        文章作者:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        文章链接:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-08-21/LLM/">https://kedreamix.github.io/Talk2Paper/Paper/2025-08-21/LLM/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        版权声明:
                    </i>
                </span>
                <span class="reprint-info">
                    本博客所有文章除特別声明外，均采用
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    许可协议。转载请注明来源
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/LLM/">
                                    <span class="chip bg-color">LLM</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;上一篇</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-08-21/Agent/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-09044d5bf6f2fd548f1712059536c58c.jpg" class="responsive-img" alt="Agent">
                        
                        <span class="card-title">Agent</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Agent 方向最新论文已更新，请持续关注 Update in 2025-08-21  ComputerRL Scaling End-to-End Online Reinforcement Learning for   Computer Use Agents
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-08-21
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Agent/" class="post-category">
                                    Agent
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Agent/">
                        <span class="chip bg-color">Agent</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                下一篇&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-08-21/R1_Reasoning/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-0ceb93b5a52d5b1eea48b1129d771e6f.jpg" class="responsive-img" alt="R1_Reasoning">
                        
                        <span class="card-title">R1_Reasoning</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            R1_Reasoning 方向最新论文已更新，请持续关注 Update in 2025-08-21  Embodied-R1 Reinforced Embodied Reasoning for General Robotic   Manipulation
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-08-21
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/R1-Reasoning/" class="post-category">
                                    R1_Reasoning
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/R1-Reasoning/">
                        <span class="chip bg-color">R1_Reasoning</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- 代码块功能依赖 -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- 代码语言 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- 代码块复制 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- 代码块收缩 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC 悬浮按钮. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* 修复文章卡片 div 的宽度. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // 切换TOC目录展开收缩的相关操作.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;站点总字数:&nbsp;<span
                        class="white-color">29058.5k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- 运行天数提醒. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // 区分是否有年份.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = '本站已运行 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                daysTip = '本站已運行 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = '本站已运行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = '本站已運行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="关注我的知乎: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">知</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- 搜索遮罩框 -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span>
            <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- 白天和黑夜主题 -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- 回到顶部按钮 -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // 只在桌面版网页启用特效
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- 雪花特效 -->
    

    <!-- 鼠标星星特效 -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--腾讯兔小巢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- 动态标签 -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Σ(っ °Д °;)っ诶，页面崩溃了嘛？", clearTimeout(st)) : (document.title = "φ(゜▽゜*)♪咦，又好了！", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
