<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="LLM">
    <meta name="description" content="LLM æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-08-21  Unintended Misalignment from Agentic Fine-Tuning Risks and Mitigation">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>LLM | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-a056dd40246c173f2f35e909473a12a2.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">LLM</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/LLM/">
                                <span class="chip bg-color">LLM</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/LLM/" class="post-category">
                                LLM
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-08-21
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-09-08
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    19.5k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    79 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-08-21-æ›´æ–°"><a href="#2025-08-21-æ›´æ–°" class="headerlink" title="2025-08-21 æ›´æ–°"></a>2025-08-21 æ›´æ–°</h1><h2 id="Unintended-Misalignment-from-Agentic-Fine-Tuning-Risks-and-Mitigation"><a href="#Unintended-Misalignment-from-Agentic-Fine-Tuning-Risks-and-Mitigation" class="headerlink" title="Unintended Misalignment from Agentic Fine-Tuning: Risks and Mitigation"></a>Unintended Misalignment from Agentic Fine-Tuning: Risks and Mitigation</h2><p><strong>Authors:Dongyoon Hahm, Taywon Min, Woogyeol Jin, Kimin Lee</strong></p>
<p>Beyond simple text generation, Large Language Models (LLMs) have evolved into agentic systems capable of planning and interacting with external tools to solve complex tasks. This evolution involves fine-tuning LLMs on agent-specific tasks to enhance their proficiency. However, safety concerns are frequently overlooked during this fine-tuning process. In this work, we show that aligned LLMs can become unintentionally misaligned, leading to a higher likelihood of executing harmful tasks and a reduced tendency to refuse them when fine-tuned to execute agentic tasks. To address these safety challenges, we propose Prefix INjection Guard (PING), a simple yet effective method that prepends automatically generated natural language prefixes to agent responses, guiding them to refuse harmful requests while preserving performance on benign tasks. Specifically, we introduce an iterative approach that alternates between (1) generating candidate prefixes and (2) selecting those that optimize both task performance and refusal behavior. Experimental results demonstrate that PING significantly enhances the safety of fine-tuned LLM agents without sacrificing their effectiveness. PING consistently outperforms existing prompting approaches across diverse benchmarks in both web navigation and code generation tasks. Our analysis of internal hidden states via linear probes reveals that prefix tokens are crucial for behavior modification, explaining the performance gains. WARNING: This paper contains contents that are unethical or offensive in nature. </p>
<blockquote>
<p>é™¤äº†ç®€å•çš„æ–‡æœ¬ç”Ÿæˆå¤–ï¼Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å·²ç»å‘å±•æˆä¸ºäº†èƒ½å¤Ÿè§„åˆ’å¹¶ä¸å¤–éƒ¨å·¥å…·è¿›è¡Œäº¤äº’ä»¥è§£å†³å¤æ‚ä»»åŠ¡çš„ä»£ç†ç³»ç»Ÿã€‚è¿™ç§è¿›åŒ–æ¶‰åŠåˆ°å¯¹ç‰¹å®šä»»åŠ¡è¿›è¡Œå¾®è°ƒï¼Œä»¥å¢å¼ºLLMçš„ä¸“ä¸šèƒ½åŠ›ã€‚ç„¶è€Œï¼Œåœ¨å¾®è°ƒè¿‡ç¨‹ä¸­ï¼Œå®‰å…¨é—®é¢˜ç»å¸¸è¢«å¿½è§†ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬è¡¨æ˜å¯¹é½çš„LLMå¯èƒ½ä¼šæ— æ„ä¸­å¤±å»å¯¹é½ï¼Œå¯¼è‡´æ›´æœ‰å¯èƒ½æ‰§è¡Œæœ‰å®³ä»»åŠ¡ï¼Œå¹¶ä¸”åœ¨å¾®è°ƒä»¥æ‰§è¡Œä»£ç†ä»»åŠ¡æ—¶å‡å°‘æ‹’ç»å®ƒä»¬çš„å€¾å‘ã€‚ä¸ºäº†è§£å†³è¿™äº›å®‰å…¨æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†Prefix INjection Guardï¼ˆPINGï¼‰ï¼Œè¿™æ˜¯ä¸€ç§ç®€å•è€Œæœ‰æ•ˆçš„æ–¹æ³•ï¼Œå‘ä»£ç†å“åº”è‡ªåŠ¨ç”Ÿæˆçš„è‡ªç„¶è¯­è¨€å‰ç¼€ï¼Œä»¥æŒ‡å¯¼å®ƒä»¬æ‹’ç»æœ‰å®³çš„è¯·æ±‚ï¼ŒåŒæ—¶ä¿ç•™åœ¨è‰¯æ€§ä»»åŠ¡ä¸Šçš„æ€§èƒ½ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬ä»‹ç»äº†ä¸€ç§è¿­ä»£æ–¹æ³•ï¼Œè¯¥æ–¹æ³•äº¤æ›¿è¿›è¡Œï¼ˆ1ï¼‰ç”Ÿæˆå€™é€‰å‰ç¼€å’Œï¼ˆ2ï¼‰é€‰æ‹©é‚£äº›æ—¢ä¼˜åŒ–ä»»åŠ¡æ€§èƒ½åˆä¼˜åŒ–æ‹’ç»è¡Œä¸ºçš„å‰ç¼€ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒPINGåœ¨ä¸å½±å“LLMä»£ç†æ•ˆç‡çš„æƒ…å†µä¸‹æ˜¾è‘—æé«˜äº†å…¶å®‰å…¨æ€§ã€‚PINGåœ¨å„ç§åŸºå‡†æµ‹è¯•ä¸­å§‹ç»ˆä¼˜äºç°æœ‰çš„æç¤ºæ–¹æ³•ï¼Œæ— è®ºæ˜¯åœ¨ç½‘é¡µå¯¼èˆªè¿˜æ˜¯ä»£ç ç”Ÿæˆä»»åŠ¡ä¸­ã€‚æˆ‘ä»¬é€šè¿‡çº¿æ€§æ¢é’ˆå¯¹å†…éƒ¨éšè—çŠ¶æ€çš„åˆ†æè¡¨æ˜ï¼Œå‰ç¼€ä»¤ç‰Œå¯¹è¡Œä¸ºä¿®æ”¹è‡³å…³é‡è¦ï¼Œè§£é‡Šäº†æ€§èƒ½æå‡çš„åŸå› ã€‚è­¦å‘Šï¼šæœ¬æ–‡åŒ…å«ä¸é“å¾·æˆ–å…·æœ‰å†’çŠ¯æ€§çš„å†…å®¹ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.14031v1">PDF</a> Source code: <a target="_blank" rel="noopener" href="https://github.com/HahmDY/prefix_injection_guard">https://github.com/HahmDY/prefix_injection_guard</a></p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å·²è¿›åŒ–ä¸ºèƒ½å¤Ÿè§„åˆ’å¹¶ä¸å¤–éƒ¨å·¥å…·äº’åŠ¨ä»¥å®Œæˆå¤æ‚ä»»åŠ¡çš„ä»£ç†ç³»ç»Ÿã€‚ç„¶è€Œï¼Œåœ¨å¾®è°ƒè¿‡ç¨‹ä¸­ï¼Œå®‰å…¨æ€§å¸¸è¢«å¿½è§†ã€‚æœ¬ç ”ç©¶æ˜¾ç¤ºï¼Œå¯¹é½çš„LLMså¯èƒ½ä¼šæ„å¤–åœ°å‡ºç°ä¸å¯¹é½çš„æƒ…å†µï¼Œå¯¼è‡´æ›´å®¹æ˜“æ‰§è¡Œæœ‰å®³ä»»åŠ¡ï¼Œå¹¶ä¸”åœ¨æ‰§è¡Œä»£ç†ä»»åŠ¡æ—¶ï¼Œæ‹’ç»æœ‰å®³ä»»åŠ¡çš„å€¾å‘é™ä½ã€‚ä¸ºè§£å†³è¿™äº›å®‰å…¨æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†Prefix INjection Guardï¼ˆPINGï¼‰æ–¹æ³•ï¼Œé€šè¿‡è‡ªåŠ¨ç”Ÿæˆçš„è‡ªç„¶è¯­è¨€å‰ç¼€æ¥å¼•å¯¼å“åº”ï¼Œæ‹’ç»æœ‰å®³è¯·æ±‚ï¼ŒåŒæ—¶ä¿æŒå¯¹è‰¯æ€§ä»»åŠ¡çš„æ€§èƒ½ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒPINGåœ¨ä¸å½±å“LLMä»£ç†æ•ˆèƒ½çš„å‰æä¸‹ï¼Œæ˜¾è‘—æé«˜äº†å…¶å®‰å…¨æ€§ã€‚PINGåœ¨å„ç§åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œæ— è®ºæ˜¯ç½‘é¡µå¯¼èˆªè¿˜æ˜¯ä»£ç ç”Ÿæˆä»»åŠ¡éƒ½æ˜¯å¦‚æ­¤ã€‚é€šè¿‡å¯¹å†…éƒ¨éšè—çŠ¶æ€çš„åˆ†æï¼Œæˆ‘ä»¬å‘ç°å‰ç¼€æ ‡è®°å¯¹è¡Œä¸ºæ”¹å˜è‡³å…³é‡è¦ï¼Œè§£é‡Šäº†æ€§èƒ½æå‡çš„åŸå› ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLMså·²è¿›åŒ–ä¸ºèƒ½å¤Ÿå®Œæˆå¤æ‚ä»»åŠ¡çš„ä»£ç†ç³»ç»Ÿï¼ŒåŒ…æ‹¬è§„åˆ’åŠä¸å¤–éƒ¨å·¥å…·äº’åŠ¨ã€‚</li>
<li>åœ¨å¾®è°ƒLLMsä»¥æ‰§è¡Œç‰¹å®šä»»åŠ¡æ—¶ï¼Œå®‰å…¨æ€§å¸¸è¢«å¿½è§†ã€‚</li>
<li>å¯¹é½çš„LLMså¯èƒ½ä¼šæ„å¤–åœ°å‡ºç°ä¸å¯¹é½ï¼Œå¯¼è‡´æ›´å®¹æ˜“æ‰§è¡Œæœ‰å®³ä»»åŠ¡ã€‚</li>
<li>æå‡ºäº†ä¸€ç§åä¸ºPINGçš„æ–¹æ³•ï¼Œé€šè¿‡è‡ªåŠ¨æ·»åŠ è‡ªç„¶è¯­è¨€å‰ç¼€æ¥å¼•å¯¼LLMçš„å“åº”ï¼Œä»¥æé«˜å…¶å®‰å…¨æ€§ã€‚</li>
<li>PINGæ–¹æ³•èƒ½åœ¨ä¸ç‰ºç‰²LLMä»£ç†æ•ˆèƒ½çš„å‰æä¸‹æé«˜å®‰å…¨æ€§ã€‚</li>
<li>PINGåœ¨å„ç§ä»»åŠ¡åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚ï¼ŒåŒ…æ‹¬ç½‘é¡µå¯¼èˆªå’Œä»£ç ç”Ÿæˆã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.14031">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-b88ab9ad84bf1dda7237ea976b024cb8.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-8862d04bf6dff131ca511ba3641de15a.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-a765d3cbe2433bd2b2f5429b80d78522.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-684cad5b774d387d7ced22e94aebebce.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-de083283a8b9ba75b0d4eb42037e8b33.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-24752f6793c0ba1b3a40b2aa66ae42b2.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Ask-Good-Questions-for-Large-Language-Models"><a href="#Ask-Good-Questions-for-Large-Language-Models" class="headerlink" title="Ask Good Questions for Large Language Models"></a>Ask Good Questions for Large Language Models</h2><p><strong>Authors:Qi Wu, Zhongqi Lu</strong></p>
<p>Recent advances in large language models (LLMs) have significantly improved the performance of dialog systems, yet current approaches often fail to provide accurate guidance of topic due to their inability to discern user confusion in related concepts. To address this, we introduce the Ask-Good-Question (AGQ) framework, which features an improved Concept-Enhanced Item Response Theory (CEIRT) model to better identify usersâ€™ knowledge levels. Our contributions include applying the CEIRT model along with LLMs to directly generate guiding questions based on the inspiring text, greatly improving information retrieval efficiency during the question &amp; answer process. Through comparisons with other baseline methods, our approach outperforms by significantly enhencing the usersâ€™ information retrieval experiences. </p>
<blockquote>
<p>æœ€è¿‘çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„è¿›æ­¥æ˜¾è‘—æé«˜äº†å¯¹è¯ç³»ç»Ÿçš„æ€§èƒ½ï¼Œç„¶è€Œï¼Œå½“å‰çš„æ–¹æ³•å¾€å¾€ç”±äºæ— æ³•è¾¨åˆ«ç”¨æˆ·å¯¹ç›¸å…³æ¦‚å¿µçš„ç†è§£æ··æ·†ï¼Œè€Œæ— æ³•æä¾›å‡†ç¡®çš„ä¸»é¢˜æŒ‡å¯¼ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†Ask-Good-Questionï¼ˆAGQï¼‰æ¡†æ¶ï¼Œè¯¥æ¡†æ¶é‡‡ç”¨æ”¹è¿›çš„æ¦‚å¿µå¢å¼ºé¡¹ç›®ååº”ç†è®ºï¼ˆCEIRTï¼‰æ¨¡å‹ï¼Œä»¥æ›´å¥½åœ°è¯†åˆ«ç”¨æˆ·çš„çŸ¥è¯†æ°´å¹³ã€‚æˆ‘ä»¬çš„è´¡çŒ®åŒ…æ‹¬å°†CEIRTæ¨¡å‹ä¸LLMç›¸ç»“åˆï¼Œç›´æ¥æ ¹æ®å¯å‘æ–‡æœ¬ç”Ÿæˆå¼•å¯¼é—®é¢˜ï¼Œå¤§å¤§æé«˜äº†é—®ç­”è¿‡ç¨‹ä¸­çš„ä¿¡æ¯æ£€ç´¢æ•ˆç‡ã€‚ä¸å…¶ä»–åŸºçº¿æ–¹æ³•çš„æ¯”è¾ƒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•æ˜¾è‘—æå‡äº†ç”¨æˆ·çš„ä¿¡æ¯æ£€ç´¢ä½“éªŒã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.14025v1">PDF</a> </p>
<p><strong>Summary</strong>ï¼šè¿‘æœŸå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„è¿›æ­¥æ˜¾è‘—æå‡äº†å¯¹è¯ç³»ç»Ÿçš„æ€§èƒ½ï¼Œä½†å½“å‰æ–¹æ³•å¾€å¾€å› æ— æ³•è¾¨è¯†ç”¨æˆ·æ¦‚å¿µæ··æ·†è€Œéš¾ä»¥å‡†ç¡®æä¾›ä¸»é¢˜æŒ‡å¯¼ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºAsk-Good-Questionï¼ˆAGQï¼‰æ¡†æ¶ï¼Œé‡‡ç”¨æ”¹è¿›çš„Concept-Enhanced Item Response Theoryï¼ˆCEIRTï¼‰æ¨¡å‹æ›´å¥½åœ°è¯†åˆ«ç”¨æˆ·çŸ¥è¯†æ°´å¹³ï¼Œå¹¶ç»“åˆLLMç›´æ¥ç”ŸæˆåŸºäºæ–‡æœ¬å¯å‘çš„é—®é¢˜ï¼Œå¤§å¤§æé«˜é—®ç­”è¿‡ç¨‹ä¸­çš„ä¿¡æ¯æ£€ç´¢æ•ˆç‡ï¼Œç›¸è¾ƒäºå…¶ä»–åŸºç¡€æ–¹æ³•è¡¨ç°å‡ºæ›´å¥½çš„æ€§èƒ½ï¼Œå¢å¼ºäº†ç”¨æˆ·çš„ä¿¡æ¯æ£€ç´¢ä½“éªŒã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>LLMçš„è¿›æ­¥æå‡äº†å¯¹è¯ç³»ç»Ÿæ€§èƒ½ï¼Œä½†å­˜åœ¨æ— æ³•å‡†ç¡®æä¾›ä¸»é¢˜æŒ‡å¯¼çš„é—®é¢˜ã€‚</li>
<li>Ask-Good-Questionï¼ˆAGQï¼‰æ¡†æ¶è¢«å¼•å…¥è§£å†³æ­¤é—®é¢˜ã€‚</li>
<li>AGQä½¿ç”¨æ”¹è¿›çš„Concept-Enhanced Item Response Theoryï¼ˆCEIRTï¼‰æ¨¡å‹ï¼Œä»¥æ›´å¥½åœ°è¯†åˆ«ç”¨æˆ·çŸ¥è¯†æ°´å¹³ã€‚</li>
<li>ç»“åˆLLMï¼ŒAGQèƒ½ç›´æ¥ç”ŸæˆåŸºäºæ–‡æœ¬å¯å‘çš„é—®é¢˜ã€‚</li>
<li>æ­¤æ–¹æ³•æé«˜é—®ç­”è¿‡ç¨‹ä¸­çš„ä¿¡æ¯æ£€ç´¢æ•ˆç‡ã€‚</li>
<li>ä¸å…¶ä»–åŸºç¡€æ–¹æ³•ç›¸æ¯”ï¼ŒAGQè¡¨ç°å‡ºæ›´å¥½çš„æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.14025">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-2c9506904d7900f9c663d65dc310a6fe.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e96a2eb27c7f6791173a5a714335d573.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c9de413968d79c2d09b685e755c42453.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-a3aec8f35ec904029421cce198011d2a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a9247fa0c220fc17a1638ce1ce7c9131.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-99457d20782d10b777a3e084194a1873.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Chunks-as-Arms-Multi-Armed-Bandit-Guided-Sampling-for-Long-Context-LLM-Preference-Optimization"><a href="#Chunks-as-Arms-Multi-Armed-Bandit-Guided-Sampling-for-Long-Context-LLM-Preference-Optimization" class="headerlink" title="Chunks as Arms: Multi-Armed Bandit-Guided Sampling for Long-Context LLM   Preference Optimization"></a>Chunks as Arms: Multi-Armed Bandit-Guided Sampling for Long-Context LLM   Preference Optimization</h2><p><strong>Authors:Shaohua Duan, Xinze Li, Zhenghao Liu, Xiaoyuan Yi, Yukun Yan, Shuo Wang, Yu Gu, Ge Yu, Maosong Sun</strong></p>
<p>Long-context modeling is critical for a wide range of real-world tasks, including long-context question answering, summarization, and complex reasoning tasks. Recent studies have explored fine-tuning Large Language Models (LLMs) with synthetic data to enhance their long-context capabilities. However, the effectiveness of such approaches is often limited by the low diversity and factual inconsistencies in the generated data. To address these challenges, we propose LongMab-PO, a novel framework that leverages a Multi-Armed Bandit (MAB) rollout strategy to identify the most informative chunks from the given long context for sampling high-quality and diverse responses and constructing preference data pairs for Direct Preference Optimization (DPO) training. Specifically, we treat context chunks as arms of MAB, select chunks based on their expected reward scores to input into LLMs to generate responses, and iteratively update these scores based on reward feedback. This exploration and exploitation process enables the model to focus on the most relevant context segments, thereby generating and collecting high-quality and diverse responses. Finally, we collect these generated responses from the rollout process and apply the DPO method to further optimize the LLM. Experimental results show that LongMab-PO significantly improves the diversity and quality of preference data pairs, achieving state-of-the-art performance on long-context reasoning benchmarks. All code and data will be released on <a target="_blank" rel="noopener" href="https://github.com/NEUIR/LongMab-PO">https://github.com/NEUIR/LongMab-PO</a>. </p>
<blockquote>
<p>é•¿è¯­å¢ƒå»ºæ¨¡å¯¹äºä¸€ç³»åˆ—ç°å®ä¸–ç•Œä»»åŠ¡è‡³å…³é‡è¦ï¼ŒåŒ…æ‹¬é•¿è¯­å¢ƒé—®ç­”ã€æ‘˜è¦å’Œå¤æ‚æ¨ç†ä»»åŠ¡ã€‚æœ€è¿‘çš„ç ”ç©¶å·²ç»æ¢ç´¢äº†é€šè¿‡åˆæˆæ•°æ®å¯¹å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è¿›è¡Œå¾®è°ƒä»¥æé«˜å…¶é•¿è¯­å¢ƒèƒ½åŠ›ã€‚ç„¶è€Œï¼Œè¿™ç§æ–¹æ³•çš„æ•ˆåŠ›é€šå¸¸å—åˆ°ç”Ÿæˆæ•°æ®ä½å¤šæ ·æ€§å’Œäº‹å®ä¸ä¸€è‡´æ€§çš„é™åˆ¶ã€‚ä¸ºäº†åº”å¯¹è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†LongMab-POè¿™ä¸€æ–°æ¡†æ¶ï¼Œå®ƒåˆ©ç”¨å¤šè‡‚è€è™æœºï¼ˆMABï¼‰æ»šåŠ¨ç­–ç•¥æ¥è¯†åˆ«ç»™å®šé•¿è¯­å¢ƒä¸­æœ€å…·ä¿¡æ¯é‡çš„ç‰‡æ®µï¼Œä»è€Œé‡‡æ ·é«˜è´¨é‡ä¸”å¤šæ ·åŒ–çš„å“åº”ï¼Œå¹¶æ„å»ºç”¨äºç›´æ¥åå¥½ä¼˜åŒ–ï¼ˆDPOï¼‰è®­ç»ƒçš„åå¥½æ•°æ®å¯¹ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬å°†è¯­å¢ƒç‰‡æ®µè§†ä¸ºè€è™æœºçš„æ‰‹è‡‚ï¼Œæ ¹æ®é¢„æœŸçš„å¥–åŠ±åˆ†æ•°é€‰æ‹©ç‰‡æ®µï¼Œè¾“å…¥åˆ°LLMä¸­ç”Ÿæˆå“åº”ï¼Œå¹¶æ ¹æ®å¥–åŠ±åé¦ˆè¿­ä»£æ›´æ–°è¿™äº›åˆ†æ•°ã€‚è¿™ç§æ¢ç´¢ä¸åˆ©ç”¨è¿‡ç¨‹ä½¿æ¨¡å‹èƒ½å¤Ÿå…³æ³¨æœ€ç›¸å…³çš„è¯­å¢ƒç‰‡æ®µï¼Œä»è€Œç”Ÿæˆå’Œæ”¶é›†é«˜è´¨é‡ä¸”å¤šæ ·åŒ–çš„å“åº”ã€‚æœ€åï¼Œæˆ‘ä»¬æ”¶é›†è¿™äº›ä»æ»šåŠ¨è¿‡ç¨‹ä¸­ç”Ÿæˆçš„å“åº”ï¼Œå¹¶åº”ç”¨DPOæ–¹æ³•æ¥è¿›ä¸€æ­¥ä¼˜åŒ–LLMã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒLongMab-POèƒ½æ˜¾è‘—æé«˜åå¥½æ•°æ®å¯¹çš„å¤šæ ·æ€§å’Œè´¨é‡ï¼Œåœ¨é•¿è¯­å¢ƒæ¨ç†åŸºå‡†æµ‹è¯•ä¸­è¾¾åˆ°æœ€æ–°æŠ€æœ¯æ°´å¹³ã€‚æ‰€æœ‰ä»£ç å’Œæ•°æ®å°†åœ¨<a target="_blank" rel="noopener" href="https://github.com/NEUIR/LongMab-PO%E4%B8%8A%E5%8F%91%E5%B8%83%E3%80%82">https://github.com/NEUIR/LongMab-POä¸Šå‘å¸ƒã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.13993v1">PDF</a> </p>
<p><strong>Summary</strong><br>é•¿æ–‡æœ¬å»ºæ¨¡å¯¹ä¸€ç³»åˆ—ç°å®ä»»åŠ¡è‡³å…³é‡è¦ï¼Œå¦‚é•¿æ–‡æœ¬é—®ç­”ã€æ‘˜è¦å’Œå¤æ‚æ¨ç†ä»»åŠ¡ã€‚å°½ç®¡æœ€è¿‘çš„ç ”ç©¶å°è¯•é€šè¿‡åˆæˆæ•°æ®å¾®è°ƒå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ¥å¢å¼ºå…¶é•¿æ–‡æœ¬èƒ½åŠ›ï¼Œä½†ç”Ÿæˆæ•°æ®çš„å¤šæ ·æ€§å’Œäº‹å®å‡†ç¡®æ€§é™åˆ¶äº†å…¶æ•ˆæœã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºLongMab-POæ¡†æ¶ï¼Œåˆ©ç”¨å¤šè‡‚è€è™æœºï¼ˆMABï¼‰ç­–ç•¥è¯†åˆ«æœ€æœ‰ä¿¡æ¯é‡çš„é•¿æ–‡æœ¬ç‰‡æ®µæ¥ç”Ÿæˆé«˜è´¨é‡ä¸”å¤šæ ·çš„å›åº”å¹¶æ„å»ºåå¥½æ•°æ®å¯¹è¿›è¡Œç›´æ¥åå¥½ä¼˜åŒ–ï¼ˆDPOï¼‰è®­ç»ƒã€‚å…·ä½“åœ°ï¼Œæˆ‘ä»¬åˆ©ç”¨å¤šè‡‚è€è™æœºå°†ä¸Šä¸‹æ–‡ç‰‡æ®µä½œä¸ºå…¶è¡Œä¸ºé€‰é¡¹å¹¶è¾“å…¥åˆ°LLMä¸­è¿›è¡Œååº”ç”Ÿæˆï¼Œå¹¶æ ¹æ®åé¦ˆå¥–åŠ±æ›´æ–°åˆ†æ•°ã€‚è¿™ç§æ¢ç´¢ä¸åˆ©ç”¨è¿‡ç¨‹ä½¿æ¨¡å‹èšç„¦äºæœ€ç›¸å…³çš„ä¸Šä¸‹æ–‡ç‰‡æ®µï¼Œä»è€Œç”Ÿæˆå’Œæ”¶é›†é«˜è´¨é‡ä¸”å¤šæ ·çš„å›åº”ã€‚æœ€ç»ˆå®éªŒç»“æœè¡¨æ˜ï¼ŒLongMab-POåœ¨æå‡åå¥½æ•°æ®å¯¹çš„å¤šæ ·æ€§å’Œè´¨é‡æ–¹é¢è¡¨ç°æ˜¾è‘—ï¼Œå¹¶åœ¨é•¿æ–‡æœ¬æ¨ç†åŸºå‡†æµ‹è¯•ä¸­è¾¾åˆ°æœ€ä½³æ€§èƒ½ã€‚æ›´å¤šä»£ç å’Œæ•°æ®å°†åœ¨<a target="_blank" rel="noopener" href="https://github.com/NEUIR/LongMab-PO%E5%8F%91%E5%B8%83%E3%80%82">https://github.com/NEUIR/LongMab-POå‘å¸ƒã€‚</a></p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>é•¿æ–‡æœ¬å»ºæ¨¡åœ¨å¤šä¸ªç°å®ä»»åŠ¡ä¸­èµ·åˆ°å…³é”®ä½œç”¨ã€‚åŒ…æ‹¬é•¿æ–‡æœ¬é—®ç­”ã€æ‘˜è¦ä»¥åŠå¤æ‚æ¨ç†ä»»åŠ¡ç­‰éƒ½éœ€è¦å¯¹å…¶è¿›è¡Œç²¾ç»†åŒ–å¤„ç†ã€‚</li>
<li>é€šè¿‡åˆæˆæ•°æ®å¾®è°ƒå¤§å‹è¯­è¨€æ¨¡å‹çš„æ–¹æ³•é¢ä¸´æ•°æ®å¤šæ ·æ€§å’Œäº‹å®å‡†ç¡®æ€§çš„æŒ‘æˆ˜ã€‚ç°æœ‰çš„æ–¹æ³•å¾€å¾€ä¸èƒ½æœ‰æ•ˆåœ°è§£å†³è¿™äº›é—®é¢˜ã€‚</li>
<li>æå‡ºäº†ä¸€ç§åä¸ºLongMab-POçš„æ–°æ¡†æ¶ï¼Œå®ƒç»“åˆäº†å¤šè‡‚è€è™æœºç­–ç•¥å’Œç›´æ¥åå¥½ä¼˜åŒ–æ–¹æ³•ï¼Œè§£å†³äº†ä»¥ä¸ŠæŒ‘æˆ˜ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.13993">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-abaefd5f7d9c6cb9f01523e48cd91b17.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-2196a2df82d4b65ad947d5d1b674620d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a056dd40246c173f2f35e909473a12a2.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c7bb8ffab06e614adf985e034f0b2b40.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b6024522db99c87b89cb80e342186a65.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="RotBench-Evaluating-Multimodal-Large-Language-Models-on-Identifying-Image-Rotation"><a href="#RotBench-Evaluating-Multimodal-Large-Language-Models-on-Identifying-Image-Rotation" class="headerlink" title="RotBench: Evaluating Multimodal Large Language Models on Identifying   Image Rotation"></a>RotBench: Evaluating Multimodal Large Language Models on Identifying   Image Rotation</h2><p><strong>Authors:Tianyi Niu, Jaemin Cho, Elias Stengel-Eskin, Mohit Bansal</strong></p>
<p>We investigate to what extent Multimodal Large Language Models (MLLMs) can accurately identify the orientation of input images rotated 0{\deg}, 90{\deg}, 180{\deg}, and 270{\deg}. This task demands robust visual reasoning capabilities to detect rotational cues and contextualize spatial relationships within images, regardless of their orientation. To evaluate MLLMs on these abilities, we introduce RotBench â€“ a 350-image manually-filtered benchmark comprising lifestyle, portrait, and landscape images. Despite the relatively simple nature of this task, we show that several state-of-the-art open and proprietary MLLMs, including GPT-5, o3, and Gemini-2.5-Pro, do not reliably identify rotation in input images. Providing models with auxiliary information â€“ including captions, depth maps, and more â€“ or using chain-of-thought prompting offers only small and inconsistent improvements. Our results indicate that most models are able to reliably identify right-side-up (0{\deg}) images, while certain models are able to identify upside-down (180{\deg}) images. None can reliably distinguish between 90{\deg} and 270{\deg}. Simultaneously showing the image rotated in different orientations leads to moderate performance gains for reasoning models, while a modified setup using voting improves the performance of weaker models. We further show that fine-tuning does not improve modelsâ€™ ability to distinguish 90{\deg} and 270{\deg} rotations, despite substantially improving the identification of 180{\deg} images. Together, these results reveal a significant gap between MLLMsâ€™ spatial reasoning capabilities and human perception in identifying rotation. </p>
<blockquote>
<p>æˆ‘ä»¬è°ƒæŸ¥äº†å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨ä½•ç§ç¨‹åº¦ä¸Šèƒ½å‡†ç¡®è¯†åˆ«è¾“å…¥å›¾åƒæ—‹è½¬äº†0Â°ã€90Â°ã€180Â°å’Œ270Â°çš„æ–¹å‘ã€‚è¿™é¡¹ä»»åŠ¡éœ€è¦å¼ºå¤§çš„è§†è§‰æ¨ç†èƒ½åŠ›æ¥æ£€æµ‹æ—‹è½¬çº¿ç´¢å¹¶ç†è§£å›¾åƒå†…çš„ç©ºé—´å…³ç³»ï¼Œæ— è®ºå…¶æ–¹å‘å¦‚ä½•ã€‚ä¸ºäº†è¯„ä¼°MLLMsçš„è¿™äº›èƒ½åŠ›ï¼Œæˆ‘ä»¬å¼•å…¥äº†RotBenchâ€”â€”ä¸€ä¸ªåŒ…å«ç”Ÿæ´»æ–¹å¼ã€è‚–åƒå’Œé£æ™¯å›¾åƒçš„350å¼ æ‰‹åŠ¨ç­›é€‰çš„åŸºå‡†æµ‹è¯•é›†ã€‚å°½ç®¡ä»»åŠ¡ç›¸å¯¹ç®€å•ï¼Œæˆ‘ä»¬æ˜¾ç¤ºï¼ŒåŒ…æ‹¬GPT-5ã€o3å’ŒGemini-2.5-Proç­‰å¤šä¸ªå…ˆè¿›çš„å¼€æºå’Œä¸“æœ‰MLLMså¹¶ä¸èƒ½å¯é åœ°è¯†åˆ«è¾“å…¥å›¾åƒçš„æ—‹è½¬ã€‚ä¸ºæ¨¡å‹æä¾›è¾…åŠ©ä¿¡æ¯ï¼Œå¦‚å­—å¹•ã€æ·±åº¦å›¾ç­‰ï¼Œæˆ–ä½¿ç”¨æ€ç»´é“¾æç¤ºï¼Œåªèƒ½å¸¦æ¥å¾®å°ä¸”ä¸ä¸€è‡´çš„æ”¹è¿›ã€‚æˆ‘ä»¬çš„ç»“æœè¡¨æ˜ï¼Œå¤§å¤šæ•°æ¨¡å‹èƒ½å¤Ÿå¯é åœ°è¯†åˆ«æ­£ç«‹ï¼ˆ0Â°ï¼‰çš„å›¾åƒï¼Œè€ŒæŸäº›æ¨¡å‹èƒ½å¤Ÿè¯†åˆ«å€’ç«‹ï¼ˆ180Â°ï¼‰çš„å›¾åƒã€‚æ²¡æœ‰ä»»ä½•æ¨¡å‹èƒ½å¤Ÿå¯é åœ°åŒºåˆ†90Â°å’Œ270Â°çš„æ—‹è½¬ã€‚åŒæ—¶å±•ç¤ºä¸åŒæ–¹å‘æ—‹è½¬çš„å›¾åƒä¼šå¯¼è‡´æ¨ç†æ¨¡å‹çš„æ€§èƒ½é€‚åº¦æå‡ï¼Œè€Œä½¿ç”¨æŠ•ç¥¨çš„ä¿®æ”¹è®¾ç½®åˆ™æé«˜äº†è¾ƒå¼±æ¨¡å‹çš„æ€§èƒ½ã€‚æˆ‘ä»¬è¿›ä¸€æ­¥è¡¨æ˜ï¼Œå¾®è°ƒå¹¶ä¸ä¼šæé«˜æ¨¡å‹åŒºåˆ†90Â°å’Œ270Â°æ—‹è½¬çš„èƒ½åŠ›ï¼Œå°½ç®¡å®ƒåœ¨å¾ˆå¤§ç¨‹åº¦ä¸Šæé«˜äº†å¯¹180Â°å›¾åƒçš„è¯†åˆ«èƒ½åŠ›ã€‚æ€»çš„æ¥è¯´ï¼Œè¿™äº›ç»“æœæ­ç¤ºäº†MLLMsçš„ç©ºé—´æ¨ç†èƒ½åŠ›å’Œäººç±»æ„ŸçŸ¥åœ¨è¯†åˆ«æ—‹è½¬æ–¹é¢çš„é‡å¤§å·®è·ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.13968v1">PDF</a> 20 pages. Code and data: <a target="_blank" rel="noopener" href="https://github.com/tianyiniu/RotBench">https://github.com/tianyiniu/RotBench</a></p>
<p><strong>æ‘˜è¦</strong></p>
<p>æœ¬æ–‡æ¢è®¨äº†å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨è¯†åˆ«è¾“å…¥å›¾åƒæ—‹è½¬è§’åº¦æ–¹é¢çš„èƒ½åŠ›ï¼Œå…·ä½“æ¶‰åŠ0Â°ã€90Â°ã€180Â°å’Œ270Â°çš„æ—‹è½¬ã€‚æ­¤ä»»åŠ¡éœ€è¦æ¨¡å‹å…·å¤‡ç¨³å¥çš„è§†è§‰æ¨ç†èƒ½åŠ›ï¼Œä»¥æ£€æµ‹æ—‹è½¬çº¿ç´¢å¹¶åœ¨å›¾åƒä¸­ä¸Šä¸‹æ–‡åŒ–ç©ºé—´å…³ç³»ï¼Œæ— è®ºå…¶æ–¹å‘å¦‚ä½•ã€‚ä¸ºè¯„ä¼°MLLMsåœ¨æ­¤æ–¹é¢çš„èƒ½åŠ›ï¼Œæœ¬æ–‡å¼•å…¥äº†RotBenchâ€”â€”ä¸€ä¸ªåŒ…å«350å¼ æ‰‹åŠ¨ç­›é€‰çš„ç”Ÿæ´»ã€è‚–åƒå’Œé£æ™¯å›¾åƒçš„åŸºå‡†æµ‹è¯•é›†ã€‚å°½ç®¡ä»»åŠ¡ç›¸å¯¹ç®€å•ï¼Œä½†ç ”ç©¶æ˜¾ç¤ºï¼ŒåŒ…æ‹¬GPT-5ã€o3å’ŒGemini-2.5-Proç­‰åœ¨å†…çš„å¤šä¸ªå…ˆè¿›å¼€æºå’Œä¸“æœ‰MLLMsåœ¨è¯†åˆ«å›¾åƒæ—‹è½¬æ–¹é¢å¹¶ä¸å¯é ã€‚æä¾›æ¨¡å‹è¾…åŠ©ä¿¡æ¯ï¼ˆå¦‚å­—å¹•ã€æ·±åº¦å›¾ç­‰ï¼‰æˆ–ä½¿ç”¨é“¾å¼æ€ç»´æç¤ºåªèƒ½å¸¦æ¥å¾®å°ä¸”ä¸ä¸€è‡´çš„æ”¹è¿›ã€‚ç»“æœè¡¨æ˜ï¼Œå¤§å¤šæ•°æ¨¡å‹èƒ½å¤Ÿå¯é åœ°è¯†åˆ«æ­£é¢ï¼ˆ0Â°ï¼‰å›¾åƒï¼Œè€ŒæŸäº›æ¨¡å‹èƒ½å¤Ÿè¯†åˆ«å€’ç½®ï¼ˆ180Â°ï¼‰å›¾åƒã€‚æ²¡æœ‰ä»»ä½•æ¨¡å‹èƒ½å¤Ÿå¯é åœ°åŒºåˆ†90Â°å’Œ270Â°æ—‹è½¬ã€‚åŒæ—¶å±•ç¤ºä¸åŒæ–¹å‘æ—‹è½¬çš„å›¾åƒä¼šå¯¼è‡´æ¨ç†æ¨¡å‹çš„æ€§èƒ½é€‚åº¦æå‡ï¼Œè€Œä½¿ç”¨æŠ•ç¥¨çš„æ”¹è¿›è®¾ç½®åˆ™èƒ½æé«˜è¾ƒå¼±æ¨¡å‹çš„æ€§èƒ½ã€‚æ­¤å¤–ï¼Œå°½ç®¡å¾®è°ƒèƒ½æ˜¾è‘—æé«˜180Â°å›¾åƒçš„è¯†åˆ«èƒ½åŠ›ï¼Œä½†åœ¨åŒºåˆ†90Â°å’Œ270Â°æ—‹è½¬æ–¹é¢å¹¶æ— æ”¹å–„ã€‚æ€»ä½“è€Œè¨€ï¼Œè¿™äº›ç»“æœæ­ç¤ºäº†MLLMsåœ¨ç©ºé—´æ¨ç†èƒ½åŠ›æ–¹é¢ä¸äººç±»æ„ŸçŸ¥åœ¨è¯†åˆ«æ—‹è½¬æ–¹é¢çš„é‡å¤§å·®è·ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨è¯†åˆ«è¾“å…¥å›¾åƒæ—‹è½¬è§’åº¦æ–¹é¢å­˜åœ¨æŒ‘æˆ˜ï¼Œå³ä½¿å¯¹äºç®€å•çš„æ—‹è½¬è§’åº¦å¦‚0Â°ã€90Â°ã€180Â°å’Œ270Â°ã€‚</li>
<li>ç°æœ‰MLLMsåœ¨è¯†åˆ«å›¾åƒç©ºé—´å…³ç³»å’Œæ—‹è½¬æ–¹é¢è¡¨ç°ä¸è¶³ï¼Œéœ€è¦æ›´å¼ºå¤§çš„è§†è§‰æ¨ç†èƒ½åŠ›ã€‚</li>
<li>é™„åŠ ä¿¡æ¯ï¼ˆå¦‚å­—å¹•ã€æ·±åº¦å›¾ç­‰ï¼‰æˆ–é“¾å¼æ€ç»´æç¤ºå¯¹æ”¹å–„æ¨¡å‹è¯†åˆ«æ—‹è½¬è§’åº¦çš„èƒ½åŠ›æœ‰é™ä¸”æ•ˆæœä¸ç¨³å®šã€‚</li>
<li>å¤§å¤šæ•°æ¨¡å‹èƒ½å¯é è¯†åˆ«æ­£é¢å›¾åƒå’Œå€’ç½®å›¾åƒï¼Œä½†åœ¨åŒºåˆ†90Â°å’Œ270Â°æ—‹è½¬æ—¶é‡åˆ°å›°éš¾ã€‚</li>
<li>åŒæ—¶å±•ç¤ºä¸åŒæ–¹å‘æ—‹è½¬çš„å›¾åƒä»¥åŠä½¿ç”¨æŠ•ç¥¨æœºåˆ¶æœ‰åŠ©äºæé«˜æ¨¡å‹çš„æ€§èƒ½ã€‚</li>
<li>å¾®è°ƒèƒ½å¤Ÿæé«˜æ¨¡å‹å¯¹å€’ç½®å›¾åƒçš„è¯†åˆ«èƒ½åŠ›ï¼Œä½†åœ¨åŒºåˆ†ç‰¹å®šæ—‹è½¬è§’åº¦æ–¹é¢å¹¶æ— æ˜¾è‘—æ”¹è¿›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.13968">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-9010610abcef0add9c0b2c77b38586e9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-68e2f3bf7b1db8cc8f161d9f77c08946.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a1ec44c4b41cb09a453eadb3e6c0e701.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="ReviewGraph-A-Knowledge-Graph-Embedding-Based-Framework-for-Review-Rating-Prediction-with-Sentiment-Features"><a href="#ReviewGraph-A-Knowledge-Graph-Embedding-Based-Framework-for-Review-Rating-Prediction-with-Sentiment-Features" class="headerlink" title="ReviewGraph: A Knowledge Graph Embedding Based Framework for Review   Rating Prediction with Sentiment Features"></a>ReviewGraph: A Knowledge Graph Embedding Based Framework for Review   Rating Prediction with Sentiment Features</h2><p><strong>Authors:A. J. W. de Vink, Natalia Amat-Lefort, Lifeng Han</strong></p>
<p>In the hospitality industry, understanding the factors that drive customer review ratings is critical for improving guest satisfaction and business performance. This work proposes ReviewGraph for Review Rating Prediction (RRP), a novel framework that transforms textual customer reviews into knowledge graphs by extracting (subject, predicate, object) triples and associating sentiment scores. Using graph embeddings (Node2Vec) and sentiment features, the framework predicts review rating scores through machine learning classifiers. We compare ReviewGraph performance with traditional NLP baselines (such as Bag of Words, TF-IDF, and Word2Vec) and large language models (LLMs), evaluating them in the HotelRec dataset. In comparison to the state of the art literature, our proposed model performs similar to their best performing model but with lower computational cost (without ensemble).   While ReviewGraph achieves comparable predictive performance to LLMs and outperforms baselines on agreement-based metrics such as Cohenâ€™s Kappa, it offers additional advantages in interpretability, visual exploration, and potential integration into Retrieval-Augmented Generation (RAG) systems. This work highlights the potential of graph-based representations for enhancing review analytics and lays the groundwork for future research integrating advanced graph neural networks and fine-tuned LLM-based extraction methods. We will share ReviewGraph output and platform open-sourced on our GitHub page <a target="_blank" rel="noopener" href="https://github.com/aaronlifenghan/ReviewGraph">https://github.com/aaronlifenghan/ReviewGraph</a> </p>
<blockquote>
<p>åœ¨æ—…æ¸¸ä½å®¿è¡Œä¸šä¸­ï¼Œäº†è§£æ¨åŠ¨å®¢æˆ·è¯„ä»·ç­‰çº§çš„å› ç´ å¯¹äºæé«˜å®¢æˆ·æ»¡æ„åº¦å’Œé…’åº—ä¸šåŠ¡è¡¨ç°è‡³å…³é‡è¦ã€‚æœ¬ç ”ç©¶æå‡ºäº†è¯„ä»·ç­‰çº§é¢„æµ‹ï¼ˆRRPï¼‰çš„ReviewGraphæ¡†æ¶ï¼Œè¯¥æ¡†æ¶é€šè¿‡æå–ï¼ˆä¸»è¯­ã€è°“è¯­ã€å®¾è¯­ï¼‰ä¸‰å…ƒç»„å¹¶å…³è”æƒ…æ„Ÿåˆ†æ•°ï¼Œå°†å®¢æˆ·æ–‡æœ¬è¯„ä»·è½¬åŒ–ä¸ºçŸ¥è¯†å›¾è°±ã€‚ä½¿ç”¨å›¾åµŒå…¥ï¼ˆNode2Vecï¼‰å’Œæƒ…æ„Ÿç‰¹å¾ï¼Œè¯¥æ¡†æ¶é€šè¿‡æœºå™¨å­¦ä¹ åˆ†ç±»å™¨é¢„æµ‹è¯„ä»·ç­‰çº§åˆ†æ•°ã€‚æˆ‘ä»¬å°†ReviewGraphçš„æ€§èƒ½ä¸ä¼ ç»ŸNLPåŸºçº¿ï¼ˆå¦‚è¯è¢‹æ¨¡å‹ã€TF-IDFå’ŒWord2Vecï¼‰ä»¥åŠå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è¿›è¡Œæ¯”è¾ƒï¼Œå¹¶åœ¨HotelRecæ•°æ®é›†ä¸Šè¯„ä¼°å®ƒä»¬çš„æ€§èƒ½ã€‚ä¸æœ€æ–°çš„æ–‡çŒ®ç›¸æ¯”ï¼Œæˆ‘ä»¬æå‡ºçš„æ¨¡å‹ä¸ä»–ä»¬çš„æœ€ä½³æ¨¡å‹è¡¨ç°ç›¸å½“ï¼Œä½†è®¡ç®—æˆæœ¬æ›´ä½ï¼ˆæ— éœ€é›†æˆï¼‰ã€‚è™½ç„¶ReviewGraphä¸LLMåœ¨é¢„æµ‹æ€§èƒ½ä¸Šè¡¨ç°ç›¸å½“ï¼Œå¹¶ä¸”åœ¨åŸºäºåè®®çš„æŒ‡æ ‡ï¼ˆå¦‚Cohençš„Kappaç³»æ•°ï¼‰ä¸Šä¼˜äºåŸºçº¿ï¼Œä½†å®ƒè¿˜æä¾›äº†å¯è§£é‡Šæ€§ã€å¯è§†åŒ–æ¢ç´¢ä»¥åŠä¸æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰ç³»ç»Ÿé›†æˆçš„æ½œåœ¨ä¼˜åŠ¿ã€‚æœ¬ç ”ç©¶çªå‡ºäº†åŸºäºå›¾çš„è¡¨ç¤ºåœ¨å¢å¼ºè¯„ä»·åˆ†ææ–¹é¢çš„æ½œåŠ›ï¼Œå¹¶ä¸ºå°†æ¥æ•´åˆå…ˆè¿›å›¾ç¥ç»ç½‘ç»œå’Œå¾®è°ƒLLMæå–æ–¹æ³•çš„ç ”ç©¶å¥ å®šäº†åŸºç¡€ã€‚æˆ‘ä»¬å°†åœ¨æˆ‘ä»¬çš„GitHubé¡µé¢<a target="_blank" rel="noopener" href="https://github.com/aaronlifenghan/ReviewGraph%E4%B8%8A%E5%88%86%E4%BA%AB%E5%BC%80%E6%BA%90%E7%9A%84ReviewGraph%E8%BE%93%E5%87%BA%E5%92%8C%E5%B9%B3%E5%8F%B0%E3%80%82">https://github.com/aaronlifenghan/ReviewGraphä¸Šåˆ†äº«å¼€æºçš„ReviewGraphè¾“å‡ºå’Œå¹³å°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.13953v1">PDF</a> </p>
<p><strong>Summary</strong>ï¼š</p>
<p>æœ¬æ–‡ä»‹ç»äº†é’ˆå¯¹é…’åº—è¯„è®ºè¯„çº§é¢„æµ‹çš„æ–°æ¡†æ¶ReviewGraphã€‚è¯¥æ¡†æ¶é€šè¿‡æå–æ–‡æœ¬ä¸­çš„çŸ¥è¯†å›¾è°±ä¸‰å…ƒç»„å¹¶å…³è”æƒ…æ„Ÿåˆ†æ•°ï¼Œå°†æ–‡æœ¬è¯„è®ºè½¬åŒ–ä¸ºçŸ¥è¯†å›¾è°±ã€‚åˆ©ç”¨å›¾åµŒå…¥å’Œæƒ…ç»ªç‰¹å¾ï¼Œé€šè¿‡æœºå™¨å­¦ä¹ åˆ†ç±»å™¨é¢„æµ‹è¯„è®ºè¯„çº§åˆ†æ•°ã€‚ç›¸è¾ƒäºä¼ ç»ŸNLPæ–¹æ³•å’Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼ŒReviewGraphåœ¨æ€§èƒ½ä¸Šè¡¨ç°è‰¯å¥½ï¼Œç‰¹åˆ«æ˜¯åœ¨åŸºäºåè®®çš„åº¦é‡æ ‡å‡†ä¸Šï¼Œå¦‚Cohençš„Kappaç³»æ•°ã€‚æ­¤å¤–ï¼Œå®ƒè¿˜å…·æœ‰å¯è§£é‡Šæ€§ã€å¯è§†åŒ–æ¢ç´¢ä»¥åŠä¸æ£€ç´¢å¢å¼ºç”Ÿæˆç³»ç»Ÿé›†æˆçš„æ½œåŠ›ã€‚è¯¥å·¥ä½œå¼ºè°ƒäº†å›¾è¡¨ç¤ºåœ¨å¢å¼ºè¯„è®ºåˆ†ææ–¹é¢çš„æ½œåŠ›ï¼Œå¹¶ä¸ºæœªæ¥ç ”ç©¶æä¾›äº†åŸºç¡€ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>ReviewGraphæ˜¯ä¸€ä¸ªç”¨äºé…’åº—è¯„è®ºè¯„çº§é¢„æµ‹çš„æ–°æ¡†æ¶ï¼Œå®ƒå°†æ–‡æœ¬è¯„è®ºè½¬åŒ–ä¸ºçŸ¥è¯†å›¾è°±è¿›è¡Œåˆ†æã€‚</li>
<li>è¯¥æ¡†æ¶é€šè¿‡æå–ï¼ˆä¸»è¯­ã€è°“è¯­ã€å®¾è¯­ï¼‰ä¸‰å…ƒç»„å¹¶å…³è”æƒ…æ„Ÿåˆ†æ•°æ¥å®ç°æ–‡æœ¬åˆ°çŸ¥è¯†å›¾è°±çš„è½¬åŒ–ã€‚</li>
<li>ReviewGraphåˆ©ç”¨å›¾åµŒå…¥å’Œæƒ…ç»ªç‰¹å¾è¿›è¡Œè¯„çº§é¢„æµ‹ï¼Œé‡‡ç”¨æœºå™¨å­¦ä¹ åˆ†ç±»å™¨ã€‚</li>
<li>ä¸ä¼ ç»ŸNLPæ–¹æ³•å’Œå¤§å‹è¯­è¨€æ¨¡å‹ç›¸æ¯”ï¼ŒReviewGraphåœ¨æ€§èƒ½ä¸Šè¡¨ç°è‰¯å¥½ï¼Œç‰¹åˆ«æ˜¯åœ¨åŸºäºåè®®çš„åº¦é‡æ ‡å‡†ä¸Šã€‚</li>
<li>ReviewGraphå…·æœ‰å¯è§£é‡Šæ€§ã€å¯è§†åŒ–æ¢ç´¢çš„ä¼˜åŠ¿ï¼Œå¹¶ä¸”èƒ½ä¸æ£€ç´¢å¢å¼ºç”Ÿæˆç³»ç»Ÿç›¸ç»“åˆã€‚</li>
<li>è¯¥å·¥ä½œçªå‡ºäº†å›¾è¡¨ç¤ºåœ¨å¢å¼ºè¯„è®ºåˆ†ææ–¹é¢çš„æ½œåŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.13953">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-61becdeaaf8aa5db3541f7288022815e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-233ea64a6160ddaebf7c53cc38bc8d46.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7d21fe07861773aa6d7ffe27de6ea657.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d3389834c41dce0c2c897e385816f4d0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-62b40f21f475a6a280e2d01c80c351d8.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Prompt-Orchestration-Markup-Language"><a href="#Prompt-Orchestration-Markup-Language" class="headerlink" title="Prompt Orchestration Markup Language"></a>Prompt Orchestration Markup Language</h2><p><strong>Authors:Yuge Zhang, Nan Chen, Jiahang Xu, Yuqing Yang</strong></p>
<p>Large Language Models (LLMs) require sophisticated prompting, yet current practices face challenges in structure, data integration, format sensitivity, and tooling. Existing methods lack comprehensive solutions for organizing complex prompts involving diverse data types (documents, tables, images) or managing presentation variations systematically. To address these gaps, we introduce POML (Prompt Orchestration Markup Language). POML employs component-based markup for logical structure (roles, tasks, examples), specialized tags for seamless data integration, and a CSS-like styling system to decouple content from presentation, reducing formatting sensitivity. It includes templating for dynamic prompts and a comprehensive developer toolkit (IDE support, SDKs) to improve version control and collaboration. We validate POML through two case studies demonstrating its impact on complex application integration (PomLink) and accuracy performance (TableQA), as well as a user study assessing its effectiveness in real-world development scenarios. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰éœ€è¦å¤æ‚çš„æç¤ºï¼Œç„¶è€Œå½“å‰å®è·µåœ¨ç»“æ„ã€æ•°æ®é›†æˆã€æ ¼å¼æ•æ„Ÿæ€§å’Œå·¥å…·æ–¹é¢é¢ä¸´æŒ‘æˆ˜ã€‚ç°æœ‰æ–¹æ³•ç¼ºä¹ç»„ç»‡å¤æ‚æç¤ºçš„å…¨é¢è§£å†³æ–¹æ¡ˆï¼Œè¿™äº›æç¤ºæ¶‰åŠå¤šç§æ•°æ®ç±»å‹ï¼ˆæ–‡æ¡£ã€è¡¨æ ¼ã€å›¾åƒï¼‰æˆ–ç³»ç»Ÿåœ°ç®¡ç†è¡¨è¾¾å˜ä½“ã€‚ä¸ºäº†è§£å†³è¿™äº›å·®è·ï¼Œæˆ‘ä»¬å¼•å…¥äº†POMLï¼ˆæç¤ºç¼–æ’æ ‡è®°è¯­è¨€ï¼‰ã€‚POMLé‡‡ç”¨åŸºäºç»„ä»¶çš„æ ‡è®°æ¥è¡¨ç¤ºé€»è¾‘ç»“æ„ï¼ˆè§’è‰²ã€ä»»åŠ¡ã€ç¤ºä¾‹ï¼‰ï¼Œä½¿ç”¨ä¸“ç”¨æ ‡ç­¾è¿›è¡Œæ— ç¼æ•°æ®é›†æˆï¼Œå¹¶é‡‡ç”¨ç±»ä¼¼CSSçš„æ ·å¼ç³»ç»Ÿæ¥å°†å†…å®¹ä¸å‘ˆç°ç›¸åˆ†ç¦»ï¼Œé™ä½æ ¼å¼æ•æ„Ÿæ€§ã€‚å®ƒåŒ…å«ç”¨äºåŠ¨æ€æç¤ºçš„æ¨¡æ¿ä»¥åŠå…¨é¢çš„å¼€å‘è€…å·¥å…·åŒ…ï¼ˆIDEæ”¯æŒã€SDKï¼‰ï¼Œä»¥æ”¹è¿›ç‰ˆæœ¬æ§åˆ¶å’Œåä½œã€‚æˆ‘ä»¬é€šè¿‡ä¸¤ä¸ªæ¡ˆä¾‹ç ”ç©¶éªŒè¯äº†POMLçš„å½±å“ï¼Œåˆ†åˆ«æ˜¯å…¶å¯¹å¤æ‚åº”ç”¨ç¨‹åºé›†æˆï¼ˆPomLinkï¼‰çš„å½±å“ä»¥åŠå…¶å¯¹å‡†ç¡®æ€§æ€§èƒ½ï¼ˆTableQAï¼‰çš„å½±å“ï¼Œè¿˜é€šè¿‡ç”¨æˆ·ç ”ç©¶è¯„ä¼°äº†å…¶åœ¨ç°å®å¼€å‘åœºæ™¯ä¸­çš„æœ‰æ•ˆæ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.13948v1">PDF</a> All findings in this paper are derived from a POML snapshot as of   February 2025</p>
<p><strong>Summary</strong></p>
<p>LLMé¢ä¸´æç¤ºæŒ‘æˆ˜ï¼Œç°æœ‰æ–¹æ³•éš¾ä»¥ç»„ç»‡å¤æ‚çš„è·¨æ•°æ®ç±»å‹æç¤ºæˆ–ç³»ç»ŸåŒ–åœ°ç®¡ç†å‘ˆç°å˜åŒ–ã€‚ä¸ºè§£å†³è¿™äº›é—®é¢˜ï¼Œå¼•å…¥POMLï¼ˆæç¤ºç¼–æ’æ ‡è®°è¯­è¨€ï¼‰ï¼Œé€šè¿‡åŸºäºç»„ä»¶çš„æ ‡è®°æä¾›é€»è¾‘ç»“æ„ï¼Œä½¿ç”¨ä¸“ä¸šæ ‡ç­¾æ— ç¼é›†æˆæ•°æ®ï¼Œå¹¶é‡‡ç”¨ç±»ä¼¼CSSçš„æ ·å¼ç³»ç»Ÿå‡å°‘æ ¼å¼æ•æ„Ÿæ€§ã€‚POMLè¿˜åŒ…æ‹¬åŠ¨æ€æç¤ºæ¨¡æ¿å’Œå…¨é¢çš„å¼€å‘è€…å·¥å…·åŒ…ï¼ˆIDEæ”¯æŒã€SDKï¼‰ï¼Œå¯æ”¹å–„ç‰ˆæœ¬æ§åˆ¶å’Œåä½œèƒ½åŠ›ã€‚æ¡ˆä¾‹ç ”ç©¶è¯å®äº†å…¶åœ¨å¤æ‚åº”ç”¨é›†æˆå’Œå‡†ç¡®æ€§æ–¹é¢çš„æ•ˆæœã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>LLMéœ€è¦å¤æ‚çš„æç¤ºæŠ€æœ¯ã€‚</li>
<li>å½“å‰å®è·µé¢ä¸´ç»“æ„ã€æ•°æ®é›†æˆã€æ ¼å¼æ•æ„Ÿæ€§å’Œå·¥å…·æ–¹é¢çš„æŒ‘æˆ˜ã€‚</li>
<li>POMLè§£å†³äº†ç°æœ‰æ–¹æ³•çš„ä¸è¶³ï¼Œæä¾›é€»è¾‘ç»“æ„ã€æ— ç¼æ•°æ®é›†æˆå’Œå‡å°‘æ ¼å¼æ•æ„Ÿæ€§ã€‚</li>
<li>POMLå…·æœ‰åŠ¨æ€æç¤ºæ¨¡æ¿å’Œå…¨é¢çš„å¼€å‘è€…å·¥å…·åŒ…ã€‚</li>
<li>é€šè¿‡ä¸¤ä¸ªæ¡ˆä¾‹ç ”ç©¶è¯å®äº†å…¶åœ¨åº”ç”¨é›†æˆå’Œå‡†ç¡®æ€§æ–¹é¢çš„ä¼˜åŠ¿ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.13948">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-e3aac96b1b7d5630c35101c936c16a62.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b10c5fdfc94143a62b10ce9cbd3dc707.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="MME-SCI-A-Comprehensive-and-Challenging-Science-Benchmark-for-Multimodal-Large-Language-Models"><a href="#MME-SCI-A-Comprehensive-and-Challenging-Science-Benchmark-for-Multimodal-Large-Language-Models" class="headerlink" title="MME-SCI: A Comprehensive and Challenging Science Benchmark for   Multimodal Large Language Models"></a>MME-SCI: A Comprehensive and Challenging Science Benchmark for   Multimodal Large Language Models</h2><p><strong>Authors:Jiacheng Ruan, Dan Jiang, Xian Gao, Ting Liu, Yuzhuo Fu, Yangyang Kang</strong></p>
<p>Recently, multimodal large language models (MLLMs) have achieved significant advancements across various domains, and corresponding evaluation benchmarks have been continuously refined and improved. In this process, benchmarks in the scientific domain have played an important role in assessing the reasoning capabilities of MLLMs. However, existing benchmarks still face three key challenges: 1) Insufficient evaluation of modelsâ€™ reasoning abilities in multilingual scenarios; 2) Inadequate assessment of MLLMsâ€™ comprehensive modality coverage; 3) Lack of fine-grained annotation of scientific knowledge points. To address these gaps, we propose MME-SCI, a comprehensive and challenging benchmark. We carefully collected 1,019 high-quality question-answer pairs, which involve 3 distinct evaluation modes. These pairs cover four subjects, namely mathematics, physics, chemistry, and biology, and support five languages: Chinese, English, French, Spanish, and Japanese. We conducted extensive experiments on 16 open-source models and 4 closed-source models, and the results demonstrate that MME-SCI is widely challenging for existing MLLMs. For instance, under the Image-only evaluation mode, o4-mini achieved accuracy of only 52.11%, 24.73%, 36.57%, and 29.80% in mathematics, physics, chemistry, and biology, respectively, indicating a significantly higher difficulty level compared to existing benchmarks. More importantly, using MME-SCIâ€™s multilingual and fine-grained knowledge attributes, we analyzed existing modelsâ€™ performance in depth and identified their weaknesses in specific domains. The Data and Evaluation Code are available at <a target="_blank" rel="noopener" href="https://github.com/JCruan519/MME-SCI">https://github.com/JCruan519/MME-SCI</a>. </p>
<blockquote>
<p>æœ€è¿‘ï¼Œå¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨å„ä¸ªé¢†åŸŸéƒ½å–å¾—äº†æ˜¾è‘—çš„è¿›æ­¥ï¼Œç›¸åº”çš„è¯„ä¼°åŸºå‡†ä¹Ÿåœ¨æŒç»­å®Œå–„å’Œæ”¹è¿›ã€‚åœ¨æ­¤è¿‡ç¨‹ä¸­ï¼Œç§‘å­¦é¢†åŸŸçš„åŸºå‡†åœ¨è¯„ä¼°MLLMçš„æ¨ç†èƒ½åŠ›æ–¹é¢å‘æŒ¥äº†é‡è¦ä½œç”¨ã€‚ç„¶è€Œï¼Œç°æœ‰åŸºå‡†ä»é¢ä¸´ä¸‰ä¸ªå…³é”®æŒ‘æˆ˜ï¼š1ï¼‰åœ¨å¤šè¯­è¨€åœºæ™¯ä¸‹å¯¹æ¨¡å‹çš„æ¨ç†èƒ½åŠ›è¯„ä¼°ä¸è¶³ï¼›2ï¼‰å¯¹MLLMçš„ç»¼åˆæ¨¡æ€è¦†ç›–è¯„ä¼°ä¸è¶³ï¼›3ï¼‰ç§‘å­¦çŸ¥è¯†ç‚¹çš„ç²¾ç»†æ ‡æ³¨ç¼ºä¹ã€‚ä¸ºäº†è§£å†³è¿™äº›å·®è·ï¼Œæˆ‘ä»¬æå‡ºäº†MME-SCIè¿™ä¸€å…¨é¢ä¸”å…·æœ‰æŒ‘æˆ˜æ€§çš„åŸºå‡†ã€‚æˆ‘ä»¬ç²¾å¿ƒæ”¶é›†äº†1019ç»„é«˜è´¨é‡çš„é—®é¢˜ç­”æ¡ˆå¯¹ï¼Œæ¶‰åŠä¸‰ç§ç‹¬ç‰¹çš„è¯„ä¼°æ¨¡å¼ã€‚è¿™äº›é…å¯¹æ¶µç›–äº†æ•°å­¦ã€ç‰©ç†ã€åŒ–å­¦å’Œç”Ÿç‰©å››é—¨å­¦ç§‘ï¼Œæ”¯æŒäº”ç§è¯­è¨€ï¼šä¸­æ–‡ã€è‹±è¯­ã€æ³•è¯­ã€è¥¿ç­ç‰™è¯­å’Œæ—¥è¯­ã€‚æˆ‘ä»¬å¯¹16ä¸ªå¼€æºæ¨¡å‹å’Œ4ä¸ªé—­æºæ¨¡å‹è¿›è¡Œäº†å¹¿æ³›å®éªŒï¼Œç»“æœè¡¨æ˜MME-SCIå¯¹ç°æœ‰MLLMså…·æœ‰å¹¿æ³›æŒ‘æˆ˜æ€§ã€‚ä¾‹å¦‚ï¼Œåœ¨ä»…å›¾åƒè¯„ä»·æ¨¡å¼ä¸‹ï¼Œo4-miniåœ¨æ•°å­¦ã€ç‰©ç†ã€åŒ–å­¦å’Œç”Ÿç‰©æ–¹é¢çš„å‡†ç¡®ç‡åˆ†åˆ«ä¸º52.11%ã€24.73%ã€36.57%å’Œ29.80%ï¼Œè¡¨æ˜å…¶éš¾åº¦æ°´å¹³æ˜¾è‘—é«˜äºç°æœ‰åŸºå‡†ã€‚æ›´é‡è¦çš„æ˜¯ï¼Œåˆ©ç”¨MME-SCIçš„å¤šè¯­è¨€å’Œç²¾ç»†çŸ¥è¯†å±æ€§ï¼Œæˆ‘ä»¬æ·±å…¥åˆ†æäº†ç°æœ‰æ¨¡å‹çš„æ€§èƒ½ï¼Œå¹¶è¯†åˆ«äº†å®ƒä»¬åœ¨ç‰¹å®šé¢†åŸŸçš„å¼±ç‚¹ã€‚æ•°æ®å’Œè¯„ä¼°ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/JCruan519/MME-SCI">https://github.com/JCruan519/MME-SCI</a>è·å¾—ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.13938v1">PDF</a> 9 pages, 6 figures, work in progress</p>
<p><strong>Summary</strong></p>
<p>å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨å„é¢†åŸŸå–å¾—æ˜¾è‘—è¿›å±•ï¼Œè¯„ä¼°åŸºå‡†ä¹Ÿåœ¨ä¸æ–­æ”¹è¿›ã€‚ç„¶è€Œï¼Œç°æœ‰åŸºå‡†ä»å­˜åœ¨ä¸‰ä¸ªæŒ‘æˆ˜ï¼šç¼ºä¹å¤šè¯­è¨€åœºæ™¯ä¸­çš„æ¨ç†èƒ½åŠ›è¯„ä¼°ã€æ¨¡æ€è¦†ç›–ä¸è¶³ä»¥åŠå¯¹ç§‘å­¦çŸ¥è¯†ç‚¹çš„ç²¾ç»†æ ‡æ³¨ç¼ºå¤±ã€‚ä¸ºè§£å†³è¿™äº›é—®é¢˜ï¼Œæå‡ºäº†MME-SCIåŸºå‡†ï¼ŒåŒ…å«1,019ç»„é«˜è´¨é‡çš„é—®é¢˜ç­”æ¡ˆå¯¹ï¼Œæ¶‰åŠå››ç§å­¦ç§‘ã€ä¸‰ç§è¯„ä¼°æ¨¡å¼ä»¥åŠäº”ç§è¯­è¨€ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒMME-SCIå¯¹ç°æœ‰MLLMså…·æœ‰æŒ‘æˆ˜æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨å¤šä¸ªé¢†åŸŸå–å¾—è¿›å±•ï¼Œè¯„ä¼°åŸºå‡†ä¹Ÿåœ¨ä¸æ–­æ”¹è¿›ã€‚</li>
<li>ç°æœ‰è¯„ä¼°åŸºå‡†é¢ä¸´ä¸‰ä¸ªä¸»è¦æŒ‘æˆ˜ï¼šç¼ºä¹å¤šè¯­è¨€åœºæ™¯ä¸­çš„æ¨ç†èƒ½åŠ›è¯„ä¼°ã€æ¨¡æ€è¦†ç›–ä¸è¶³ä»¥åŠå¯¹ç§‘å­¦çŸ¥è¯†ç‚¹çš„ç²¾ç»†æ ‡æ³¨ç¼ºå¤±ã€‚</li>
<li>ä¸ºè§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæå‡ºäº†MME-SCIåŸºå‡†ï¼ŒåŒ…å«é«˜è´¨é‡çš„é—®é¢˜ç­”æ¡ˆå¯¹ï¼Œè¦†ç›–å¤šç§å­¦ç§‘å’Œè¯­è¨€ã€‚</li>
<li>MME-SCIåŸºå‡†åŒ…æ‹¬ä¸‰ç§è¯„ä¼°æ¨¡å¼ï¼Œå¯¹ç°æœ‰çš„MLLMså…·æœ‰æŒ‘æˆ˜æ€§ã€‚</li>
<li>åœ¨å›¾åƒä»…è¯„ä¼°æ¨¡å¼ä¸‹ï¼ŒæŸäº›æ¨¡å‹åœ¨æ•°å­¦ã€ç‰©ç†ã€åŒ–å­¦å’Œç”Ÿç‰©å­¦ç§‘çš„å‡†ç¡®ç‡è¾ƒä½ï¼Œè¡¨æ˜MME-SCIéš¾åº¦è¾ƒé«˜ã€‚</li>
<li>MME-SCIåŸºå‡†æ·±å…¥åˆ†æç°æœ‰æ¨¡å‹çš„æ€§èƒ½ï¼Œå¹¶æŒ‡å‡ºå…¶åœ¨ç‰¹å®šé¢†åŸŸçš„å¼±ç‚¹ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.13938">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-954f542828854270e05f52ea3f8c16df.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a5df2ff4fdca0007006276e5e03bfff8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8eda865634b9591e1efcc2ba5d0b99d4.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-1c590b33c73c75ce17a70ef9d3eb5a39.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-6a1d2aa2da4c4e822c00cade6465cd3e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-1095c61aac86f68e5a70ce7c155aa9a8.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="InPars-Supercharging-Synthetic-Data-Generation-for-Information-Retrieval-Systems"><a href="#InPars-Supercharging-Synthetic-Data-Generation-for-Information-Retrieval-Systems" class="headerlink" title="InPars+: Supercharging Synthetic Data Generation for Information   Retrieval Systems"></a>InPars+: Supercharging Synthetic Data Generation for Information   Retrieval Systems</h2><p><strong>Authors:Matey Krastev, Miklos Hamar, Danilo Toapanta, Jesse Brouwers, Yibin Lei</strong></p>
<p>This work revisits and extends synthetic query generation pipelines for Neural Information Retrieval (NIR) by leveraging the InPars Toolkit, a reproducible, end-to-end framework for generating training data using large language models (LLMs). We first assess the reproducibility of the original InPars, InPars-V2, and Promptagator pipelines on the SciFact benchmark and validate their effectiveness using open-source reranker and generator models. Building on this foundation, we introduce two key extensions to the pipeline: (1) fine-tuning a query generator LLM via Contrastive Preference Optimization (CPO) to improve the signal quality in generated queries, and (2) replacing static prompt templates with dynamic, Chain-of-Thought (CoT) optimized prompts using the DSPy framework. Our results show that both extensions reduce the need for aggressive filtering while improving retrieval performance. All code, models, and synthetic datasets are publicly released to support further research at: \href{<a target="_blank" rel="noopener" href="https://github.com/danilotpnta/IR2-project%7D%7Bthis">https://github.com/danilotpnta/IR2-project}{this</a> https URL}. </p>
<blockquote>
<p>è¿™é¡¹å·¥ä½œå€ŸåŠ©InParså·¥å…·åŒ…é‡æ–°å®¡æŸ¥å’Œæ‰©å±•äº†ç¥ç»ä¿¡æ¯æ£€ç´¢ï¼ˆNIRï¼‰çš„åˆæˆæŸ¥è¯¢ç”Ÿæˆç®¡é“ã€‚InParså·¥å…·åŒ…æ˜¯ä¸€ä¸ªå¯å¤åˆ¶çš„ç«¯åˆ°ç«¯æ¡†æ¶ï¼Œç”¨äºåˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ç”Ÿæˆè®­ç»ƒæ•°æ®ã€‚æˆ‘ä»¬é¦–å…ˆè¯„ä¼°äº†SciFactåŸºå‡†æµ‹è¯•ä¸ŠåŸå§‹InParsã€InPars-V2å’ŒPromptagatorç®¡é“çš„å¯é‡å¤æ€§ï¼Œå¹¶ä½¿ç”¨å¼€æºé‡æ–°æ’åºå™¨å’Œç”Ÿæˆå™¨æ¨¡å‹éªŒè¯å…¶æœ‰æ•ˆæ€§ã€‚åœ¨æ­¤åŸºç¡€ä¸Šï¼Œæˆ‘ä»¬å¼•å…¥äº†ç®¡é“çš„ä¸¤ä¸ªå…³é”®æ‰©å±•ï¼šï¼ˆ1ï¼‰é€šè¿‡å¯¹æ¯”åå¥½ä¼˜åŒ–ï¼ˆCPOï¼‰å¯¹æŸ¥è¯¢ç”Ÿæˆå™¨LLMè¿›è¡Œå¾®è°ƒï¼Œä»¥æé«˜ç”ŸæˆæŸ¥è¯¢ä¸­çš„ä¿¡å·è´¨é‡ï¼›ï¼ˆ2ï¼‰ä½¿ç”¨DSPyæ¡†æ¶å°†é™æ€æç¤ºæ¨¡æ¿æ›¿æ¢ä¸ºåŠ¨æ€ã€ç»è¿‡æ€è€ƒé“¾ï¼ˆCoTï¼‰ä¼˜åŒ–çš„æç¤ºã€‚æˆ‘ä»¬çš„ç»“æœè¡¨æ˜ï¼Œè¿™ä¸¤ä¸ªæ‰©å±•åœ¨å‡å°‘æ¿€çƒˆè¿‡æ»¤éœ€æ±‚çš„åŒæ—¶æé«˜äº†æ£€ç´¢æ€§èƒ½ã€‚æ‰€æœ‰ä»£ç ã€æ¨¡å‹å’Œåˆæˆæ•°æ®é›†å‡å·²å…¬å¼€å‘å¸ƒï¼Œä»¥æ”¯æŒè¿›ä¸€æ­¥çš„ç ”ç©¶ï¼Œå¯è®¿é—®æ­¤é“¾æ¥ï¼š[<a target="_blank" rel="noopener" href="https://github.com/danilotpnta/IR2-project]%EF%BC%88%E8%AF%B7%E7%82%B9%E5%87%BB%E9%93%BE%E6%8E%A5%E6%9F%A5%E7%9C%8B%E8%AF%A6%E7%BB%86%E4%BF%A1%E6%81%AF%EF%BC%89%E3%80%82">https://github.com/danilotpnta/IR2-project]ï¼ˆè¯·ç‚¹å‡»é“¾æ¥æŸ¥çœ‹è¯¦ç»†ä¿¡æ¯ï¼‰ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.13930v1">PDF</a> </p>
<p><strong>æ‘˜è¦</strong><br>     è¯¥å·¥ä½œå€ŸåŠ©InParså·¥å…·åŒ…é‡æ–°è®¿é—®å¹¶æ‰©å±•äº†ç”¨äºç¥ç»ä¿¡æ¯æ£€ç´¢çš„åˆæˆæŸ¥è¯¢ç”Ÿæˆç®¡é“ï¼Œè¯¥å·¥å…·åŒ…æ˜¯ä¸€ä¸ªå¯å¤ç”¨çš„ç«¯åˆ°ç«¯æ¡†æ¶ï¼Œå¯ç”¨äºå€ŸåŠ©å¤§å‹è¯­è¨€æ¨¡å‹ç”Ÿæˆè®­ç»ƒæ•°æ®ã€‚æˆ‘ä»¬é¦–å…ˆè¯„ä¼°äº†åŸå§‹InParsã€InPars-V2å’ŒPromptagatorç®¡é“åœ¨SciFactåŸºå‡†æµ‹è¯•ä¸Šçš„å¯é‡å¤æ€§ï¼Œå¹¶ä½¿ç”¨å¼€æºé‡æ’å™¨å’Œç”Ÿæˆå™¨æ¨¡å‹éªŒè¯äº†å…¶æœ‰æ•ˆæ€§ã€‚åœ¨æ­¤åŸºç¡€ä¸Šï¼Œæˆ‘ä»¬å¯¹ç®¡é“è¿›è¡Œäº†ä¸¤ä¸ªå…³é”®æ‰©å±•ï¼šï¼ˆ1ï¼‰é€šè¿‡å¯¹æ¯”ä¼˜åŒ–ï¼ˆCPOï¼‰å¯¹æŸ¥è¯¢ç”Ÿæˆå¤§å‹è¯­è¨€æ¨¡å‹è¿›è¡Œå¾®è°ƒï¼Œä»¥æé«˜ç”ŸæˆæŸ¥è¯¢ä¸­çš„ä¿¡å·è´¨é‡ï¼›ï¼ˆ2ï¼‰ä½¿ç”¨DSPyæ¡†æ¶å°†é™æ€æç¤ºæ¨¡æ¿æ›¿æ¢ä¸ºåŠ¨æ€ã€ç»è¿‡æ€ç»´é“¾ä¼˜åŒ–çš„æç¤ºã€‚ç»“æœè¡¨æ˜ï¼Œè¿™ä¸¤ä¸ªæ‰©å±•åœ¨å‡å°‘æ¿€çƒˆè¿‡æ»¤éœ€æ±‚çš„åŒæ—¶æé«˜äº†æ£€ç´¢æ€§èƒ½ã€‚æ‰€æœ‰ä»£ç ã€æ¨¡å‹å’Œåˆæˆæ•°æ®é›†å‡å·²å…¬å¼€å‘å¸ƒï¼Œä»¥æ”¯æŒè¿›ä¸€æ­¥ç ”ç©¶ï¼š<a target="_blank" rel="noopener" href="https://github.com/danilotpnta/IR2-project">é“¾æ¥åœ°å€</a>ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>åˆ©ç”¨InParså·¥å…·åŒ…é‡æ–°è®¿é—®å’Œæ‰©å±•ç¥ç»ä¿¡æ¯æ£€ç´¢çš„åˆæˆæŸ¥è¯¢ç”Ÿæˆç®¡é“ã€‚</li>
<li>è¯„ä¼°äº†å¤šç§ç°æœ‰ç®¡é“åœ¨SciFactåŸºå‡†æµ‹è¯•ä¸Šçš„å¯é‡å¤æ€§å’Œæœ‰æ•ˆæ€§ã€‚</li>
<li>é€šè¿‡å¯¹æ¯”ä¼˜åŒ–ï¼ˆCPOï¼‰å¾®è°ƒæŸ¥è¯¢ç”Ÿæˆçš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼Œæé«˜ç”ŸæˆæŸ¥è¯¢çš„ä¿¡å·è´¨é‡ã€‚</li>
<li>é‡‡ç”¨åŠ¨æ€ã€ç»è¿‡æ€ç»´é“¾ä¼˜åŒ–çš„æç¤ºï¼Œæ›¿ä»£é™æ€æç¤ºæ¨¡æ¿ã€‚</li>
<li>ä¸¤ä¸ªå…³é”®æ‰©å±•åœ¨å‡å°‘è¿‡æ»¤éœ€æ±‚çš„åŒæ—¶æé«˜äº†æ£€ç´¢æ€§èƒ½ã€‚</li>
<li>æ‰€æœ‰ç›¸å…³ä»£ç ã€æ¨¡å‹å’Œåˆæˆæ•°æ®é›†å·²å…¬å¼€ï¼Œä»¥ä¾¿è¿›ä¸€æ­¥ç ”ç©¶å’Œåˆ©ç”¨ã€‚</li>
<li>å…¬å¼€çš„ä»£ç å’Œèµ„æºæœ‰åŠ©äºæ¨åŠ¨è¯¥é¢†åŸŸçš„ç ”ç©¶è¿›å±•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.13930">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-0551edd14b78c664495f5b42066ac8b3.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a486179da26553cf9d66495d2b9f0529.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-a89c81b722f4800e9dd44cbab819d1eb.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-961f6ee0e1518460d32529fd79953a9d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9bf8eab573042ebe7a748e93e37db6dc.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="LLMind-2-0-Distributed-IoT-Automation-with-Natural-Language-M2M-Communication-and-Lightweight-LLM-Agents"><a href="#LLMind-2-0-Distributed-IoT-Automation-with-Natural-Language-M2M-Communication-and-Lightweight-LLM-Agents" class="headerlink" title="LLMind 2.0: Distributed IoT Automation with Natural Language M2M   Communication and Lightweight LLM Agents"></a>LLMind 2.0: Distributed IoT Automation with Natural Language M2M   Communication and Lightweight LLM Agents</h2><p><strong>Authors:Yuyang Du, Qun Yang, Liujianfu Wang, Jingqi Lin, Hongwei Cui, Soung Chang Liew</strong></p>
<p>Recent advances in large language models (LLMs) have sparked interest in their application to IoT and automation systems, particularly for facilitating device management through natural language instructions. However, existing centralized approaches face significant scalability challenges when managing and coordinating the collaboration between IoT devices of diverse capabilities in large-scale heterogeneous IoT systems. This paper introduces LLMind 2.0, a distributed IoT automation framework that addresses the scalability challenges through lightweight LLM-empowered device agents via natural language-based machine-to-machine (M2M) communication. Unlike previous LLM-controlled automation systems that rely on a centralized coordinator to generate device-specific code to be executed on individual devices, LLMind 2.0 distributes intelligence across individual devices through lightweight LLMs embedded in IoT devices. The central coordinator translates human instructions into simple subtasks described in natural human language, which are then processed by device-specific agents to generate device-specific code locally at the associated devices. This approach transcends device heterogeneity barriers by using natural language as a unified communication medium, enabling seamless collaboration between devices from different manufacturers. The system incorporates several key innovations: a Retrieval-Augmented Generation (RAG) mechanism for accurate subtask-to-API mapping, fine-tuned lightweight LLMs for reliable code generation, and a finite state machine-based task execution framework. Experimental validation in multi-robot warehouse scenarios and real-world WiFi network deployments demonstrates significant improvements in scalability, reliability, and privacy protection compared to the centralized approach. </p>
<blockquote>
<p>è¿‘æœŸå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„è¿›æ­¥å¼•å‘äº†äººä»¬å¯¹ç‰©è”ç½‘ï¼ˆIoTï¼‰å’Œè‡ªåŠ¨åŒ–ç³»ç»Ÿåº”ç”¨çš„å…³æ³¨ï¼Œç‰¹åˆ«æ˜¯åœ¨é€šè¿‡è‡ªç„¶è¯­è¨€æŒ‡ä»¤å®ç°è®¾å¤‡ç®¡ç†æ–¹é¢çš„åº”ç”¨ã€‚ç„¶è€Œï¼Œç°æœ‰çš„é›†ä¸­å¼æ–¹æ³•åœ¨å¤„ç†å¤§è§„æ¨¡å¼‚æ„ç‰©è”ç½‘ç³»ç»Ÿä¸­ä¸åŒåŠŸèƒ½çš„IoTè®¾å¤‡çš„åä½œç®¡ç†æ—¶ï¼Œé¢ä¸´ç€å·¨å¤§çš„å¯æ‰©å±•æ€§æŒ‘æˆ˜ã€‚æœ¬æ–‡ä»‹ç»äº†LLMind 2.0ï¼Œè¿™æ˜¯ä¸€ä¸ªåˆ†å¸ƒå¼ç‰©è”ç½‘è‡ªåŠ¨åŒ–æ¡†æ¶ï¼Œå®ƒé€šè¿‡è½»é‡çº§çš„LLMèµ‹èƒ½è®¾å¤‡ä»£ç†ï¼Œé€šè¿‡åŸºäºè‡ªç„¶è¯­è¨€çš„æœºå™¨å¯¹æœºå™¨ï¼ˆM2Mï¼‰é€šä¿¡æ¥è§£å†³å¯æ‰©å±•æ€§æŒ‘æˆ˜ã€‚ä¸åŒäºä¹‹å‰ä¾èµ–äºé›†ä¸­å¼åè°ƒå™¨ç”Ÿæˆç‰¹å®šè®¾å¤‡ä»£ç åœ¨å•ä¸ªè®¾å¤‡ä¸Šæ‰§è¡Œçš„LLMæ§åˆ¶è‡ªåŠ¨åŒ–ç³»ç»Ÿï¼ŒLLMind 2.0é€šè¿‡åœ¨IoTè®¾å¤‡ä¸­åµŒå…¥è½»é‡çº§çš„LLMï¼Œå°†æ™ºèƒ½åˆ†å¸ƒåœ¨å„ä¸ªè®¾å¤‡ä¸Šã€‚ä¸­å¤®åè°ƒå™¨å°†äººç±»æŒ‡ä»¤ç¿»è¯‘æˆç®€å•çš„å­ä»»åŠ¡ï¼Œç”¨è‡ªç„¶è¯­è¨€è¿›è¡Œæè¿°ï¼Œç„¶åè¿™äº›ä»»åŠ¡ç”±ç‰¹å®šè®¾å¤‡ä»£ç†å¤„ç†ï¼Œç”Ÿæˆç‰¹å®šè®¾å¤‡çš„ä»£ç ï¼Œåœ¨ç›¸å…³è®¾å¤‡ä¸Šæœ¬åœ°æ‰§è¡Œã€‚è¿™ç§æ–¹æ³•é€šè¿‡ä½¿ç”¨è‡ªç„¶è¯­è¨€ä½œä¸ºç»Ÿä¸€çš„é€šä¿¡åª’ä»‹ï¼Œè¶…è¶Šäº†è®¾å¤‡å¼‚æ„æ€§çš„éšœç¢ï¼Œå®ç°äº†ä¸åŒåˆ¶é€ å•†è®¾å¤‡ä¹‹é—´çš„æ— ç¼åä½œã€‚è¯¥ç³»ç»Ÿèåˆäº†å¤šé¡¹å…³é”®åˆ›æ–°æŠ€æœ¯ï¼šç”¨äºå‡†ç¡®å­ä»»åŠ¡åˆ°APIæ˜ å°„çš„æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰æœºåˆ¶ã€ç”¨äºå¯é ä»£ç ç”Ÿæˆçš„å¾®è°ƒè½»é‡çº§LLMä»¥åŠåŸºäºæœ‰é™çŠ¶æ€æœºçš„ä»»åŠ¡æ‰§è¡Œæ¡†æ¶ã€‚åœ¨å¤šæœºå™¨äººä»“åº“åœºæ™¯å’ŒçœŸå®WiFiç½‘ç»œéƒ¨ç½²ä¸­çš„å®éªŒéªŒè¯è¡¨æ˜ï¼Œä¸é›†ä¸­å¼æ–¹æ³•ç›¸æ¯”ï¼Œå®ƒåœ¨å¯æ‰©å±•æ€§ã€å¯é æ€§å’Œéšç§ä¿æŠ¤æ–¹é¢éƒ½æœ‰æ˜¾è‘—æé«˜ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.13920v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>è¿‘æœŸå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„è¿›æ­¥æ¿€å‘äº†å…¶åœ¨ç‰©è”ç½‘ï¼ˆIoTï¼‰å’Œè‡ªåŠ¨åŒ–ç³»ç»Ÿé¢†åŸŸåº”ç”¨çš„å…³æ³¨ã€‚é’ˆå¯¹å¤§è§„æ¨¡å¼‚æ„ç‰©è”ç½‘ç³»ç»Ÿçš„è®¾å¤‡ç®¡ç†ï¼Œç°æœ‰é›†ä¸­åŒ–æ–¹æ³•é¢ä¸´å¯æ‰©å±•æ€§æŒ‘æˆ˜ã€‚æœ¬æ–‡ä»‹ç»LLMind 2.0ï¼Œä¸€ä¸ªåˆ†å¸ƒå¼ç‰©è”ç½‘è‡ªåŠ¨åŒ–æ¡†æ¶ï¼Œå®ƒé€šè¿‡è‡ªç„¶è¯­è¨€åŸºç¡€çš„æœºå™¨é—´ï¼ˆM2Mï¼‰é€šä¿¡å’Œè½»é‡çº§LLMèµ‹èƒ½çš„è®¾å¤‡ä»£ç†äººæ¥è§£å†³å¯æ‰©å±•æ€§é—®é¢˜ã€‚ä¸åŒäºä¾èµ–é›†ä¸­åè°ƒå™¨ç”Ÿæˆé’ˆå¯¹ç‰¹å®šè®¾å¤‡çš„ä»£ç åœ¨ä¸ªåˆ«è®¾å¤‡ä¸Šæ‰§è¡Œçš„å…ˆå‰LLMæ§åˆ¶è‡ªåŠ¨åŒ–ç³»ç»Ÿï¼ŒLLMind 2.0é€šè¿‡åµŒå…¥å¼è½»é‡çº§LLMå°†æ™ºèƒ½åˆ†å¸ƒåœ¨å„ä¸ªè®¾å¤‡ä¸Šã€‚ä¸­å¤®åè°ƒå™¨å°†äººç±»æŒ‡ä»¤ç¿»è¯‘æˆç®€å•çš„å­ä»»åŠ¡æè¿°çš„è‡ªç„¶è¯­è¨€ï¼Œç„¶åç”±è®¾å¤‡ç‰¹å®šä»£ç†äººå¤„ç†å¹¶åœ¨ç›¸å…³è®¾å¤‡ä¸Šæœ¬åœ°ç”Ÿæˆç‰¹å®šè®¾å¤‡çš„ä»£ç ã€‚è¿™ç§æ–¹æ³•é€šè¿‡ä½¿ç”¨è‡ªç„¶è¯­è¨€ä½œä¸ºç»Ÿä¸€çš„é€šä¿¡åª’ä»‹ï¼Œè¶…è¶Šäº†è®¾å¤‡å¼‚æ„æ€§çš„éšœç¢ï¼Œå®ç°äº†ä¸åŒåˆ¶é€ å•†è®¾å¤‡ä¹‹é—´çš„æ— ç¼åä½œã€‚å®éªŒéªŒè¯è¡¨æ˜ï¼Œä¸é›†ä¸­å¼æ–¹æ³•ç›¸æ¯”ï¼Œè¯¥ç³»ç»Ÿåœ¨å¯æ‰©å±•æ€§ã€å¯é æ€§å’Œéšç§ä¿æŠ¤æ–¹é¢å®ç°äº†æ˜¾è‘—æ”¹è¿›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLMind 2.0æ˜¯ä¸€ä¸ªç”¨äºç‰©è”ç½‘è‡ªåŠ¨åŒ–çš„åˆ†å¸ƒå¼æ¡†æ¶ï¼Œè§£å†³äº†é›†ä¸­åŒ–æ–¹æ³•åœ¨å¤§å‹å¼‚æ„ç‰©è”ç½‘ç³»ç»Ÿä¸­çš„å¯æ‰©å±•æ€§é—®é¢˜ã€‚</li>
<li>é€šè¿‡è½»é‡çº§LLMèµ‹èƒ½çš„è®¾å¤‡ä»£ç†äººå®ç°æ™ºèƒ½åˆ†å¸ƒï¼Œä¸åŒäºä¾èµ–é›†ä¸­åè°ƒå™¨ç”Ÿæˆè®¾å¤‡ç‰¹å®šä»£ç çš„å…ˆå‰æ–¹æ³•ã€‚</li>
<li>LLMind 2.0ä½¿ç”¨è‡ªç„¶è¯­è¨€ä½œä¸ºç»Ÿä¸€é€šä¿¡åª’ä»‹ï¼Œå®ç°ä¸åŒè®¾å¤‡é—´çš„æ— ç¼åä½œï¼Œè¶…è¶Šäº†è®¾å¤‡å¼‚æ„æ€§çš„éšœç¢ã€‚</li>
<li>ç³»ç»Ÿé‡‡ç”¨æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰æœºåˆ¶ï¼Œå®ç°å‡†ç¡®çš„ä»»åŠ¡åˆ°APIçš„æ˜ å°„ã€‚</li>
<li>é€šè¿‡å¾®è°ƒè½»é‡çº§LLMå®ç°å¯é çš„ä»£ç ç”Ÿæˆã€‚</li>
<li>é‡‡ç”¨æœ‰é™çŠ¶æ€æœºä»»åŠ¡æ‰§è¡Œæ¡†æ¶ï¼Œæé«˜ç³»ç»Ÿæ•ˆç‡å’Œç¨³å®šæ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.13920">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-d84fcc47344943038bce9767cc8dfc95.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b314cb0a816adad26f83d55ea6396177.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-68bc0ccba117b81c808d41249ed1a6d7.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-8de939e53ea2e8147828a533b6cbd336.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-203d341ea8429f0943a94e7d1dc5f473.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="Structured-Agentic-Workflows-for-Financial-Time-Series-Modeling-with-LLMs-and-Reflective-Feedback"><a href="#Structured-Agentic-Workflows-for-Financial-Time-Series-Modeling-with-LLMs-and-Reflective-Feedback" class="headerlink" title="Structured Agentic Workflows for Financial Time-Series Modeling with   LLMs and Reflective Feedback"></a>Structured Agentic Workflows for Financial Time-Series Modeling with   LLMs and Reflective Feedback</h2><p><strong>Authors:Yihao Ang, Yifan Bao, Lei Jiang, Jiajie Tao, Anthony K. H. Tung, Lukasz Szpruch, Hao Ni</strong></p>
<p>Time-series data is central to decision-making in financial markets, yet building high-performing, interpretable, and auditable models remains a major challenge. While Automated Machine Learning (AutoML) frameworks streamline model development, they often lack adaptability and responsiveness to domain-specific needs and evolving objectives. Concurrently, Large Language Models (LLMs) have enabled agentic systems capable of reasoning, memory management, and dynamic code generation, offering a path toward more flexible workflow automation. In this paper, we introduce \textsf{TS-Agent}, a modular agentic framework designed to automate and enhance time-series modeling workflows for financial applications. The agent formalizes the pipeline as a structured, iterative decision process across three stages: model selection, code refinement, and fine-tuning, guided by contextual reasoning and experimental feedback. Central to our architecture is a planner agent equipped with structured knowledge banks, curated libraries of models and refinement strategies, which guide exploration, while improving interpretability and reducing error propagation. \textsf{TS-Agent} supports adaptive learning, robust debugging, and transparent auditing, key requirements for high-stakes environments such as financial services. Empirical evaluations on diverse financial forecasting and synthetic data generation tasks demonstrate that \textsf{TS-Agent} consistently outperforms state-of-the-art AutoML and agentic baselines, achieving superior accuracy, robustness, and decision traceability. </p>
<blockquote>
<p>æ—¶é—´åºåˆ—æ•°æ®åœ¨é‡‘èå¸‚åœºçš„å†³ç­–åˆ¶å®šä¸­èµ·ç€å…³é”®ä½œç”¨ï¼Œç„¶è€Œï¼Œæ„å»ºé«˜æ€§èƒ½ã€å¯è§£é‡Šå’Œå¯å®¡è®¡çš„æ¨¡å‹ä»ç„¶æ˜¯ä¸€ä¸ªé‡å¤§æŒ‘æˆ˜ã€‚å°½ç®¡è‡ªåŠ¨åŒ–æœºå™¨å­¦ä¹ ï¼ˆAutoMLï¼‰æ¡†æ¶ç®€åŒ–äº†æ¨¡å‹å¼€å‘ï¼Œä½†å®ƒä»¬å¾€å¾€ç¼ºä¹é€‚åº”æ€§å’Œå¯¹ç‰¹å®šé¢†åŸŸéœ€æ±‚å’Œä¸æ–­å˜åŒ–ç›®æ ‡çš„å“åº”èƒ½åŠ›ã€‚åŒæ—¶ï¼Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä½¿å¾—èƒ½å¤Ÿæ¨ç†ã€å†…å­˜ç®¡ç†å’ŒåŠ¨æ€ä»£ç ç”Ÿæˆçš„è‡ªä¸»ç³»ç»Ÿæˆä¸ºå¯èƒ½ï¼Œä»è€Œä¸ºæ›´çµæ´»çš„å·¥ä½œæµè‡ªåŠ¨åŒ–æä¾›äº†é€”å¾„ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†\textsf{TS-Agent}ï¼Œè¿™æ˜¯ä¸€ä¸ªæ¨¡å—åŒ–è‡ªä¸»æ¡†æ¶ï¼Œæ—¨åœ¨è‡ªåŠ¨åŒ–å’Œå¢å¼ºæ—¶é—´åºåˆ—å»ºæ¨¡å·¥ä½œæµç¨‹ï¼Œç”¨äºé‡‘èåº”ç”¨ç¨‹åºã€‚è¯¥ä»£ç†å°†ç®¡é“æ­£å¼åŒ–ä¸ºä¸€ä¸ªç»“æ„åŒ–çš„è¿­ä»£å†³ç­–è¿‡ç¨‹ï¼ŒåŒ…æ‹¬ä¸‰ä¸ªé˜¶æ®µï¼šæ¨¡å‹é€‰æ‹©ã€ä»£ç ç²¾ç‚¼å’Œå¾®è°ƒï¼Œç”±ä¸Šä¸‹æ–‡æ¨ç†å’Œå®éªŒåé¦ˆæŒ‡å¯¼ã€‚æˆ‘ä»¬æ¶æ„çš„æ ¸å¿ƒæ˜¯é…å¤‡ç»“æ„åŒ–çŸ¥è¯†åº“ã€ç²¾é€‰çš„æ¨¡å‹å’Œç²¾ç‚¼ç­–ç•¥åº“çš„è§„åˆ’ä»£ç†ï¼Œå®ƒå¼•å¯¼æ¢ç´¢ï¼ŒåŒæ—¶æé«˜å¯è§£é‡Šæ€§å¹¶å‡å°‘é”™è¯¯ä¼ æ’­ã€‚\textsf{TS-Agent}æ”¯æŒè‡ªé€‚åº”å­¦ä¹ ã€ç¨³å¥çš„è°ƒè¯•å’Œé€æ˜çš„å®¡è®¡ï¼Œè¿™æ˜¯é‡‘èæœåŠ¡ç­‰é«˜é£é™©ç¯å¢ƒä¸­çš„å…³é”®è¦æ±‚ã€‚åœ¨å¤šæ ·åŒ–çš„é‡‘èé¢„æµ‹å’Œåˆæˆæ•°æ®ç”Ÿæˆä»»åŠ¡ä¸Šçš„å®è¯è¯„ä¼°è¡¨æ˜ï¼Œ\textsf{TS-Agent}æŒç»­ä¼˜äºæœ€æ–°çš„AutoMLå’Œè‡ªä¸»åŸºçº¿ï¼Œå®ç°äº†æ›´é«˜çš„å‡†ç¡®æ€§ã€ç¨³å¥æ€§å’Œå†³ç­–å¯è¿½æº¯æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.13915v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†é¢å‘é‡‘èåº”ç”¨çš„è‡ªåŠ¨åŒ–å¢å¼ºæ—¶é—´åºåˆ—å»ºæ¨¡å·¥ä½œæµçš„æ¨¡å—åŒ–æ™ºèƒ½æ¡†æ¶TS-Agentã€‚è¯¥æ¡†æ¶å°†ç®¡é“å½¢å¼åŒ–ä¸ºä¸€ä¸ªç»“æ„åŒ–è¿­ä»£å†³ç­–è¿‡ç¨‹ï¼ŒåŒ…æ‹¬æ¨¡å‹é€‰æ‹©ã€ä»£ç ç²¾ç‚¼å’Œå¾®è°ƒä¸‰ä¸ªé˜¶æ®µï¼Œç”±ä¸Šä¸‹æ–‡æ¨ç†å’Œå®éªŒåé¦ˆæŒ‡å¯¼ã€‚TS-Agentæ”¯æŒè‡ªé€‚åº”å­¦ä¹ ã€ç¨³å¥è°ƒè¯•å’Œé€æ˜å®¡è®¡ï¼Œé€‚åº”é‡‘èæœåŠ¡ç­‰é«˜é£é™©ç¯å¢ƒçš„å…³é”®è¦æ±‚ã€‚ç»éªŒè¯„ä¼°è¡¨æ˜ï¼ŒTS-Agentåœ¨å¤šç§é‡‘èé¢„æµ‹å’Œåˆæˆæ•°æ®ç”Ÿæˆä»»åŠ¡ä¸Šè¡¨ç°ä¼˜äºæœ€æ–°æ™ºèƒ½è‡ªåŠ¨åŒ–å’Œæ™ºèƒ½åŸºçº¿ï¼Œå…·æœ‰å‡†ç¡®æ€§ã€ç¨³å¥æ€§å’Œå†³ç­–å¯è¿½æº¯æ€§çš„ä¼˜åŠ¿ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ—¶é—´åºåˆ—æ•°æ®åœ¨é‡‘èå¸‚åœºçš„å†³ç­–ä¸­å æ®é‡è¦åœ°ä½ï¼Œä½†æ„å»ºé«˜æ€§èƒ½ã€å¯è§£é‡Šå’Œå¯å®¡è®¡çš„æ¨¡å‹ä»ç„¶æ˜¯ä¸€ä¸ªæŒ‘æˆ˜ã€‚</li>
<li>å½“å‰è‡ªåŠ¨åŒ–æœºå™¨å­¦ä¹ æ¡†æ¶åœ¨é€‚åº”ç‰¹å®šé¢†åŸŸéœ€æ±‚å’Œå˜åŒ–ç›®æ ‡æ–¹é¢å­˜åœ¨å±€é™æ€§ã€‚</li>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ä¸ºçµæ´»çš„è‡ªåŠ¨åŒ–å·¥ä½œæµç¨‹æä¾›äº†å¯èƒ½æ€§ï¼Œé€šè¿‡æ¨ç†ã€å†…å­˜ç®¡ç†å’ŒåŠ¨æ€ä»£ç ç”Ÿæˆç­‰åŠŸèƒ½ã€‚</li>
<li>TS-Agentæ˜¯ä¸€ä¸ªæ¨¡å—åŒ–æ™ºèƒ½æ¡†æ¶ï¼Œæ—¨åœ¨è‡ªåŠ¨åŒ–å’Œæ”¹è¿›æ—¶é—´åºåˆ—å»ºæ¨¡å·¥ä½œæµç¨‹ï¼Œé€‚ç”¨äºé‡‘èåº”ç”¨ã€‚</li>
<li>TS-Agentå°†ç®¡é“å½¢å¼åŒ–ä¸ºç»“æ„åŒ–è¿­ä»£å†³ç­–è¿‡ç¨‹ï¼ŒåŒ…æ‹¬æ¨¡å‹é€‰æ‹©ã€ä»£ç ç²¾ç‚¼å’Œå¾®è°ƒä¸‰ä¸ªé˜¶æ®µã€‚</li>
<li>TS-Agentçš„æ ¸å¿ƒæ˜¯ä¸€ä¸ªé…å¤‡ç»“æ„åŒ–çŸ¥è¯†åº“å’Œç²¾ç‚¼ç­–ç•¥å’Œæ¨¡å‹çš„è§„åˆ’æ™ºèƒ½ä½“ï¼Œå®ƒæŒ‡å¯¼æ¢ç´¢è¿‡ç¨‹ï¼Œæé«˜å¯è§£é‡Šæ€§å¹¶å‡å°‘é”™è¯¯ä¼ æ’­ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.13915">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-1f3210e86952085276b52c80703f45b1.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-82377df016cb7b99feee7a28380818d0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-da6611ae5533b12c6249dc2227cd029a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e7fd24b88fc1ba0eeeecbcbf168cdf9f.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="Driving-Style-Recognition-Like-an-Expert-Using-Semantic-Privileged-Information-from-Large-Language-Models"><a href="#Driving-Style-Recognition-Like-an-Expert-Using-Semantic-Privileged-Information-from-Large-Language-Models" class="headerlink" title="Driving Style Recognition Like an Expert Using Semantic Privileged   Information from Large Language Models"></a>Driving Style Recognition Like an Expert Using Semantic Privileged   Information from Large Language Models</h2><p><strong>Authors:Zhaokun Chen, Chaopeng Zhang, Xiaohan Li, Wenshuo Wang, Gentiane Venture, Junqiang Xi</strong></p>
<p>Existing driving style recognition systems largely depend on low-level sensor-derived features for training, neglecting the rich semantic reasoning capability inherent to human experts. This discrepancy results in a fundamental misalignment between algorithmic classifications and expert judgments. To bridge this gap, we propose a novel framework that integrates Semantic Privileged Information (SPI) derived from large language models (LLMs) to align recognition outcomes with human-interpretable reasoning. First, we introduce DriBehavGPT, an interactive LLM-based module that generates natural-language descriptions of driving behaviors. These descriptions are then encoded into machine learning-compatible representations via text embedding and dimensionality reduction. Finally, we incorporate them as privileged information into Support Vector Machine Plus (SVM+) for training, enabling the model to approximate human-like interpretation patterns. Experiments across diverse real-world driving scenarios demonstrate that our SPI-enhanced framework outperforms conventional methods, achieving F1-score improvements of 7.6% (car-following) and 7.9% (lane-changing). Importantly, SPI is exclusively used during training, while inference relies solely on sensor data, ensuring computational efficiency without sacrificing performance. These results highlight the pivotal role of semantic behavioral representations in improving recognition accuracy while advancing interpretable, human-centric driving systems. </p>
<blockquote>
<p>ç°æœ‰é©¾é©¶é£æ ¼è¯†åˆ«ç³»ç»Ÿå¾ˆå¤§ç¨‹åº¦ä¸Šä¾èµ–äºç”¨äºè®­ç»ƒçš„åº•å±‚ä¼ æ„Ÿå™¨è¡ç”Ÿç‰¹å¾ï¼Œä»è€Œå¿½è§†äº†äººç±»ä¸“å®¶æ‰€å›ºæœ‰çš„ä¸°å¯Œè¯­ä¹‰æ¨ç†èƒ½åŠ›ã€‚è¿™ç§å·®å¼‚å¯¼è‡´äº†ç®—æ³•åˆ†ç±»ä¸ä¸“å®¶åˆ¤æ–­ä¹‹é—´çš„åŸºæœ¬é”™ä½ã€‚ä¸ºäº†å¼¥è¡¥è¿™ä¸€å·®è·ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°å‹æ¡†æ¶ï¼Œè¯¥æ¡†æ¶æ•´åˆäº†æºè‡ªå¤§å‹è¯­è¨€æ¨¡å‹çš„è¯­ä¹‰ç‰¹æƒä¿¡æ¯ï¼ˆSPIï¼‰ï¼Œä»¥ä½¿è¯†åˆ«ç»“æœä¸å¯è§£é‡Šçš„äººç±»æ¨ç†ç›¸ä¸€è‡´ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬å¼•å…¥äº†DriBehavGPTè¿™ä¸€åŸºäºå¤§å‹è¯­è¨€æ¨¡å‹çš„äº¤äº’å¼æ¨¡å—ï¼Œç”¨äºç”Ÿæˆé©¾é©¶è¡Œä¸ºçš„è‡ªç„¶è¯­è¨€æè¿°ã€‚è¿™äº›æè¿°éšåé€šè¿‡æ–‡æœ¬åµŒå…¥å’Œé™ç»´æŠ€æœ¯ç¼–ç æˆæœºå™¨å­¦ä¹ å…¼å®¹çš„è¡¨ç¤ºå½¢å¼ã€‚æœ€åï¼Œæˆ‘ä»¬å°†å®ƒä»¬ä½œä¸ºç‰¹æƒä¿¡æ¯èå…¥æ”¯æŒå‘é‡æœºå¢å¼ºç‰ˆï¼ˆSVM+ï¼‰è¿›è¡Œè®­ç»ƒï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿæ¨¡æ‹Ÿäººç±»èˆ¬çš„è§£é‡Šæ¨¡å¼ã€‚åœ¨å¤šç§çœŸå®ä¸–ç•Œé©¾é©¶åœºæ™¯ä¸­çš„å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„SPIå¢å¼ºæ¡†æ¶ä¼˜äºä¼ ç»Ÿæ–¹æ³•ï¼Œåœ¨è·Ÿè½¦å’Œæ¢é“åœºæ™¯ä¸­F1åˆ†æ•°åˆ†åˆ«æé«˜äº†7.6%å’Œ7.9%ã€‚å€¼å¾—ä¸€æçš„æ˜¯ï¼ŒSPIä»…åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ä½¿ç”¨ï¼Œè€Œæ¨ç†åˆ™å®Œå…¨ä¾èµ–äºä¼ æ„Ÿå™¨æ•°æ®ï¼Œç¡®ä¿äº†è®¡ç®—æ•ˆç‡ä¸”ä¸ç‰ºç‰²æ€§èƒ½ã€‚è¿™äº›ç»“æœå‡¸æ˜¾äº†è¯­ä¹‰è¡Œä¸ºè¡¨ç¤ºåœ¨æå‡è¯†åˆ«å‡†ç¡®æ€§ä»¥åŠæ¨åŠ¨å¯è§£é‡Šã€ä»¥äººç±»ä¸ºä¸­å¿ƒçš„é©¾é©¶ç³»ç»Ÿæ–¹é¢çš„é‡è¦ä½œç”¨ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.13881v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>åœ¨ç°æœ‰é©¾é©¶é£æ ¼è¯†åˆ«ç³»ç»Ÿä¸­ï¼Œä¸»è¦ä¾èµ–äºä½çº§åˆ«ä¼ æ„Ÿå™¨è¡ç”Ÿçš„ç‰¹å¾è¿›è¡Œè®­ç»ƒï¼Œå¿½ç•¥äº†äººç±»ä¸“å®¶ä¸°å¯Œçš„è¯­ä¹‰æ¨ç†èƒ½åŠ›ã€‚ä¸ºäº†å¼¥è¡¥è¿™ä¸€å·®è·ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„æ¡†æ¶ï¼Œå®ƒé›†æˆäº†æ¥è‡ªå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„è¯­ä¹‰ç‰¹æƒä¿¡æ¯ï¼ˆSPIï¼‰ï¼Œä»¥ä½¿è¯†åˆ«ç»“æœä¸äººç±»å¯è§£é‡Šçš„é“ç†ç›¸ä¸€è‡´ã€‚é€šè¿‡å¼•å…¥DriBehavGPTè¿™ä¸€åŸºäºLLMçš„äº¤äº’å¼æ¨¡å—ï¼Œç”Ÿæˆé©¾é©¶è¡Œä¸ºçš„è‡ªç„¶è¯­è¨€æè¿°ï¼Œç„¶åå°†å…¶è½¬åŒ–ä¸ºæœºå™¨å­¦ä¹ å…¼å®¹çš„è¡¨ç¤ºå½¢å¼ã€‚æœ€åï¼Œæˆ‘ä»¬å°†å®ƒä»¬ä½œä¸ºç‰¹æƒä¿¡æ¯çº³å…¥SVM+ï¼ˆæ”¯æŒå‘é‡æœºå¢å¼ºç‰ˆï¼‰è¿›è¡Œè®­ç»ƒï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿæ¨¡æ‹Ÿäººç±»ç±»ä¼¼çš„è§£è¯»æ¨¡å¼ã€‚å®éªŒè¡¨æ˜ï¼Œåœ¨å¤šç§çœŸå®é©¾é©¶åœºæ™¯ä¸‹ï¼Œæˆ‘ä»¬çš„SPIå¢å¼ºæ¡†æ¶ä¼˜äºä¼ ç»Ÿæ–¹æ³•ï¼Œè½¦è·Ÿå’Œè½¦é“å˜æ›´çš„F1å¾—åˆ†åˆ†åˆ«æé«˜äº†7.6%å’Œ7.9%ã€‚SPIä»…ç”¨äºè®­ç»ƒè¿‡ç¨‹ï¼Œè€Œæ¨ç†åˆ™ä¾èµ–äºä¼ æ„Ÿå™¨æ•°æ®ï¼Œç¡®ä¿äº†è®¡ç®—æ•ˆç‡ä¸æ€§èƒ½ã€‚æ­¤ç ”ç©¶å¼ºè°ƒäº†è¯­ä¹‰è¡Œä¸ºè¡¨ç¤ºåœ¨æé«˜è¯†åˆ«ç²¾åº¦å’Œæ¨åŠ¨ä»¥äººä¸ºæœ¬çš„é©¾é©¶ç³»ç»Ÿæ–¹é¢çš„å…³é”®ä½œç”¨ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å½“å‰é©¾é©¶é£æ ¼è¯†åˆ«ç³»ç»Ÿä¸»è¦ä¾èµ–ä½çº§åˆ«ä¼ æ„Ÿå™¨ç‰¹å¾ï¼Œå¿½ç•¥äº†äººç±»ä¸“å®¶çš„è¯­ä¹‰æ¨ç†èƒ½åŠ›ã€‚</li>
<li>æå‡ºäº†ä¸€ä¸ªé›†æˆè¯­ä¹‰ç‰¹æƒä¿¡æ¯ï¼ˆSPIï¼‰çš„æ–°æ¡†æ¶ï¼Œä»¥å¼¥è¡¥ä¸ä¸“å®¶åˆ¤æ–­ä¹‹é—´çš„å·®å¼‚ã€‚</li>
<li>å¼•å…¥LLMé©±åŠ¨çš„DriBehavGPTæ¨¡å—ï¼Œç”Ÿæˆé©¾é©¶è¡Œä¸ºçš„è‡ªç„¶è¯­è¨€æè¿°ã€‚</li>
<li>é€šè¿‡æ–‡æœ¬åµŒå…¥å’Œé™ç»´æŠ€æœ¯ï¼Œå°†è‡ªç„¶è¯­è¨€æè¿°è½¬åŒ–ä¸ºæœºå™¨å­¦ä¹ å…¼å®¹çš„å½¢å¼ã€‚</li>
<li>å°†SPIçº³å…¥SVM+è¿›è¡Œè®­ç»ƒï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿæ¨¡æ‹Ÿäººç±»ç±»ä¼¼çš„è§£è¯»æ¨¡å¼ã€‚</li>
<li>åœ¨çœŸå®é©¾é©¶åœºæ™¯çš„å®éªŒä¸­ï¼ŒSPIå¢å¼ºæ¡†æ¶æ˜¾è‘—ä¼˜äºä¼ ç»Ÿæ–¹æ³•ï¼ŒF1å¾—åˆ†æœ‰æ˜æ˜¾æå‡ã€‚</li>
<li>SPIä»…ç”¨äºè®­ç»ƒï¼Œæ¨ç†è¿‡ç¨‹ä¾èµ–ä¼ æ„Ÿå™¨æ•°æ®ï¼Œä¿è¯äº†è®¡ç®—æ•ˆç‡ä¸æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.13881">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-3c78d90276787229285ad1ac02740712.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-b557b1bf9cd576667e1f23cce2b9aa17.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d1a5e206fe848a43835520085d2c62f9.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-1906f014b87696a9b6dcd229582339f2.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-4b4d65951dcc527ed2cdc368a226f99a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-872c0a2bee7d1c76b5987362958d146c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c820133fc2d5e8122d2c514bd4a6cc9e.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="Improved-Generalized-Planning-with-LLMs-through-Strategy-Refinement-and-Reflection"><a href="#Improved-Generalized-Planning-with-LLMs-through-Strategy-Refinement-and-Reflection" class="headerlink" title="Improved Generalized Planning with LLMs through Strategy Refinement and   Reflection"></a>Improved Generalized Planning with LLMs through Strategy Refinement and   Reflection</h2><p><strong>Authors:Katharina Stein, Nils Hodel, Daniel FiÅ¡er, JÃ¶rg Hoffmann, Michael Katz, Alexander Koller</strong></p>
<p>LLMs have recently been used to generate Python programs representing generalized plans in PDDL planning, i.e., plans that generalize across the tasks of a given PDDL domain. Previous work proposed a framework consisting of three steps: the LLM first generates a summary and then a strategy for the domain, both in natural language, and then implements that strategy as a Python program, that gets debugged on example planning tasks. In that work, only one strategy is generated and passed directly to the program generation. If the strategy is incorrect, its implementation will therefore result in an incorrect generalized plan. Here, we introduce an approach that generates the strategy in the form of pseudocode and enables automatic debugging of the pseudocode, hence allowing us to identify and fix errors prior to the generation of the generalized plan itself. Additionally, we extend the Python debugging phase with a reflection step prompting the LLM to pinpoint the reason for the observed plan failure. Finally, we take inspiration from LLM code generation to produce several program variants and pick the best one. Running experiments on 17 benchmark domains, we show that these extensions substantially improve (and never deteriorate) the quality of the generalized plans. In 12 of the domains, our best Python programs solve all tasks that can be generated with the respective instance generator. </p>
<blockquote>
<p>LLMæœ€è¿‘è¢«ç”¨äºç”Ÿæˆåœ¨PDDLè§„åˆ’ä¸­è¡¨ç¤ºé€šç”¨è®¡åˆ’çš„Pythonç¨‹åºï¼Œå³é€‚ç”¨äºç»™å®šPDDLåŸŸçš„ä»»åŠ¡çš„é€šç”¨è®¡åˆ’ã€‚å…ˆå‰çš„å·¥ä½œæå‡ºäº†ä¸€ä¸ªåŒ…å«ä¸‰ä¸ªæ­¥éª¤çš„æ¡†æ¶ï¼šLLMé¦–å…ˆä¸ºé¢†åŸŸç”Ÿæˆæ‘˜è¦ï¼Œç„¶åç”Ÿæˆè‡ªç„¶è¯­è¨€ç­–ç•¥ï¼Œæ¥ç€å°†è¯¥ç­–ç•¥å®ç°ä¸ºPythonç¨‹åºï¼Œå¹¶åœ¨ç¤ºä¾‹è§„åˆ’ä»»åŠ¡ä¸Šè¿›è¡Œè°ƒè¯•ã€‚åœ¨é‚£é¡¹å·¥ä½œä¸­ï¼Œåªç”Ÿæˆäº†ä¸€ä¸ªç­–ç•¥å¹¶ç›´æ¥ä¼ é€’ç»™ç¨‹åºç”Ÿæˆã€‚å¦‚æœç­–ç•¥ä¸æ­£ç¡®ï¼Œå…¶å®ç°å°†å¯¼è‡´é€šç”¨è®¡åˆ’å‡ºé”™ã€‚åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬ä»‹ç»äº†ä¸€ç§ä»¥ä¼ªä»£ç å½¢å¼ç”Ÿæˆç­–ç•¥çš„æ–¹æ³•ï¼Œå¹¶å®ç°äº†ä¼ªä»£ç çš„è‡ªåŠ¨è°ƒè¯•ï¼Œä»è€Œå¯ä»¥åœ¨ç”Ÿæˆé€šç”¨è®¡åˆ’æœ¬èº«ä¹‹å‰è¯†åˆ«å’Œä¿®å¤é”™è¯¯ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å°†Pythonè°ƒè¯•é˜¶æ®µæ‰©å±•ä¸ºåæ€æ­¥éª¤ï¼Œæç¤ºLLMæŒ‡å‡ºè§‚å¯Ÿåˆ°çš„è®¡åˆ’å¤±è´¥çš„åŸå› ã€‚æœ€åï¼Œæˆ‘ä»¬ä»LLMä»£ç ç”Ÿæˆä¸­æ±²å–çµæ„Ÿï¼Œç”Ÿæˆå¤šä¸ªç¨‹åºå˜ä½“å¹¶é€‰æ‹©æœ€ä½³ç¨‹åºã€‚åœ¨17ä¸ªåŸºå‡†åŸŸä¸Šè¿è¡Œå®éªŒè¡¨æ˜ï¼Œè¿™äº›æ‰©å±•å®è´¨ä¸Šæ”¹è¿›äº†ï¼ˆä»æœªæ¶åŒ–ï¼‰é€šç”¨è®¡åˆ’çš„è´¨é‡ã€‚åœ¨å…¶ä¸­çš„12ä¸ªé¢†åŸŸï¼Œæˆ‘ä»¬æœ€ä½³çš„Pythonç¨‹åºè§£å†³äº†å¯ä»¥ä½¿ç”¨ç›¸åº”å®ä¾‹ç”Ÿæˆå™¨ç”Ÿæˆçš„æ‰€æœ‰ä»»åŠ¡ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.13876v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>LLMsåœ¨PDDLè§„åˆ’ä¸­çš„é€šç”¨è®¡åˆ’ç”Ÿæˆåº”ç”¨å¾—åˆ°å‘å±•ã€‚å…ˆå‰çš„å·¥ä½œæå‡ºä¸€ä¸ªä¸‰æ­¥éª¤æ¡†æ¶ï¼Œé€šè¿‡LLMç”Ÿæˆé¢†åŸŸæ€»ç»“å’Œç­–ç•¥ï¼ˆçš†ä¸ºè‡ªç„¶è¯­è¨€ï¼‰ï¼Œç„¶åå°†å…¶å®ç°ä¸ºPythonç¨‹åºï¼Œå¹¶åœ¨ç¤ºä¾‹è§„åˆ’ä»»åŠ¡ä¸­è¿›è¡Œè°ƒè¯•ã€‚æœ¬å·¥ä½œå¼•å…¥ç”Ÿæˆç­–ç•¥ä¼ªä»£ç çš„æ–¹æ³•ï¼Œå¹¶å…è®¸å¯¹ä¼ªä»£ç è¿›è¡Œè‡ªåŠ¨è°ƒè¯•ï¼Œä»è€Œåœ¨ç”Ÿæˆå¹¿ä¹‰è®¡åˆ’æœ¬èº«ä¹‹å‰å‘ç°å’Œä¿®å¤é”™è¯¯ã€‚æ­¤å¤–ï¼Œé€šè¿‡åæ€æ­¥éª¤æ‰©å±•Pythonè°ƒè¯•é˜¶æ®µï¼Œä¿ƒä½¿LLMæŒ‡å‡ºè§‚å¯Ÿåˆ°çš„è®¡åˆ’å¤±è´¥çš„åŸå› ã€‚ä»LLMä»£ç ç”Ÿæˆä¸­æ±²å–çµæ„Ÿï¼Œç”Ÿæˆå¤šä¸ªç¨‹åºå˜ä½“å¹¶é€‰æ‹©æœ€ä½³æ–¹æ¡ˆã€‚åœ¨17ä¸ªåŸºå‡†åŸŸçš„å®éªŒä¸­ï¼Œè¿™äº›æ‰©å±•æ˜¾è‘—æé«˜äº†å¹¿ä¹‰è®¡åˆ’çš„è´¨é‡ï¼Œå¹¶åœ¨å…¶ä¸­12ä¸ªé¢†åŸŸä¸­è§£å†³äº†æ‰€æœ‰å¯ä»¥ç”±å®ä¾‹ç”Ÿæˆå™¨ç”Ÿæˆçš„ä»»åŠ¡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLMså·²ç”¨äºç”Ÿæˆä»£è¡¨å¹¿ä¹‰è®¡åˆ’çš„Pythonç¨‹åºã€‚</li>
<li>å…ˆå‰çš„å·¥ä½œæå‡ºäº†ä¸€ä¸ªä¸‰æ­¥éª¤æ¡†æ¶ï¼ŒåŒ…æ‹¬LLMç”Ÿæˆé¢†åŸŸæ€»ç»“å’Œç­–ç•¥ã€‚</li>
<li>å¼•å…¥ç”Ÿæˆç­–ç•¥ä¼ªä»£ç çš„æ–¹æ³•ï¼Œå…è®¸å¯¹ä¼ªä»£ç è¿›è¡Œè‡ªåŠ¨è°ƒè¯•ã€‚</li>
<li>æ‰©å±•Pythonè°ƒè¯•é˜¶æ®µï¼Œé€šè¿‡åæ€æ­¥éª¤è¯†åˆ«è®¡åˆ’å¤±è´¥çš„åŸå› ã€‚</li>
<li>ä»LLMä»£ç ç”Ÿæˆä¸­è·å–çµæ„Ÿï¼Œç”Ÿæˆå¤šä¸ªç¨‹åºå˜ä½“ã€‚</li>
<li>è¿™äº›æ‰©å±•æ˜¾è‘—æé«˜äº†å¹¿ä¹‰è®¡åˆ’çš„è´¨é‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.13876">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-097b5453dec99c15fc452eaa914bdd12.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9987f8c0eac9e8d42c2944ff7aadd930.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0faa99151b07f4b232169583605db902.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b9ab26f6e126e6db9745a4ddaa0eadb7.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-1283fd876763454167a80a4e27f2d288.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-be8a2d9c5aabff7f8510a54e21127e42.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-29b697b927fa3a8a5976ab01da43786b.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="Learning-In-context-n-grams-with-Transformers-Sub-n-grams-Are-Near-stationary-Points"><a href="#Learning-In-context-n-grams-with-Transformers-Sub-n-grams-Are-Near-stationary-Points" class="headerlink" title="Learning In-context n-grams with Transformers: Sub-n-grams Are   Near-stationary Points"></a>Learning In-context n-grams with Transformers: Sub-n-grams Are   Near-stationary Points</h2><p><strong>Authors:Aditya Varre, Gizem YÃ¼ce, Nicolas Flammarion</strong></p>
<p>Motivated by empirical observations of prolonged plateaus and stage-wise progression during training, we investigate the loss landscape of transformer models trained on in-context next-token prediction tasks. In particular, we focus on learning in-context $n$-gram language models under cross-entropy loss, and establish a sufficient condition for parameter configurations to be stationary points. We then construct a set of parameter configurations for a simplified transformer model that represent $k$-gram estimators (for $k \leq n$), and show that the gradient of the population loss at these solutions vanishes in the limit of infinite sequence length and parameter norm. This reveals a key property of the loss landscape: {sub-$n$-grams are near-stationary points of the population cross-entropy loss}, offering theoretical insight into widely observed phenomena such as stage-wise learning dynamics and emergent phase transitions. These insights are further supported by numerical experiments that illustrate the learning dynamics of $n$-grams, characterized by discrete transitions between near-stationary solutions. </p>
<blockquote>
<p>å—è®­ç»ƒè¿‡ç¨‹ä¸­çš„é•¿æœŸå¹³ç¨³æœŸå’Œåˆ†é˜¶æ®µè¿›å±•çš„å®è¯è§‚å¯Ÿæ‰€é©±åŠ¨ï¼Œæˆ‘ä»¬ç ”ç©¶äº†åœ¨ä¸Šä¸‹æ–‡å†…ä»¤ç‰Œé¢„æµ‹ä»»åŠ¡ä¸Šè®­ç»ƒçš„å˜å‹å™¨æ¨¡å‹çš„æŸå¤±æ™¯è§‚ã€‚ç‰¹åˆ«æ˜¯ï¼Œæˆ‘ä»¬å…³æ³¨åœ¨äº¤å‰ç†µæŸå¤±ä¸‹å­¦ä¹ ä¸Šä¸‹æ–‡ä¸­çš„nå…ƒè¯­è¨€æ¨¡å‹ï¼Œå¹¶ä¸ºå‚æ•°é…ç½®å»ºç«‹å¹³ç¨³ç‚¹çš„å……åˆ†æ¡ä»¶ã€‚ç„¶åï¼Œæˆ‘ä»¬ä¸ºç®€åŒ–çš„å˜å‹å™¨æ¨¡å‹æ„å»ºäº†ä¸€ç»„å‚æ•°é…ç½®ï¼Œè¿™äº›é…ç½®ä»£è¡¨kå…ƒä¼°ç®—å™¨ï¼ˆå¯¹äºkâ‰¤nï¼‰ï¼Œå¹¶è¡¨æ˜åœ¨è¿™äº›è§£å†³æ–¹æ¡ˆå¤„ï¼Œäººå£æŸå¤±çš„æ¢¯åº¦åœ¨æ— é™åºåˆ—é•¿åº¦å’Œå‚æ•°èŒƒæ•°çš„æé™å†…æ¶ˆå¤±ã€‚è¿™æ­ç¤ºäº†æŸå¤±æ™¯è§‚çš„ä¸€ä¸ªå…³é”®å±æ€§ï¼š{å­nå…ƒæ˜¯äººå£äº¤å‰ç†µæŸå¤±çš„è¿‘ä¼¼å¹³ç¨³ç‚¹}ï¼Œä¸ºå¹¿æ³›è§‚å¯Ÿåˆ°çš„ç°è±¡ï¼ˆå¦‚åˆ†é˜¶æ®µçš„å­¦ä¹ åŠ¨åŠ›å’Œæ–°å…´çš„ç›¸å˜ï¼‰æä¾›äº†ç†è®ºä¸Šçš„è§è§£ã€‚è¿™äº›è§è§£å¾—åˆ°äº†æ•°å€¼å®éªŒçš„è¿›ä¸€æ­¥æ”¯æŒï¼Œè¿™äº›å®éªŒè¯´æ˜äº†nå…ƒçš„å­¦ä¹ åŠ¨æ€ï¼Œå…¶ç‰¹å¾æ˜¯åœ¨è¿‘ä¼¼å¹³ç¨³è§£ä¹‹é—´çš„ç¦»æ•£è¿‡æ¸¡ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.12837v2">PDF</a> ICML2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ¢è®¨äº†è®­ç»ƒè¿‡ç¨‹ä¸­é‡åˆ°çš„é•¿æœŸåœæ»å’Œé˜¶æ®µæ€§è¿›å±•ç°è±¡ï¼Œç ”ç©¶äº†åŸºäºä¸Šä¸‹æ–‡é¢„æµ‹ä»»åŠ¡çš„Transformeræ¨¡å‹çš„æŸå¤±æ™¯è§‚ã€‚æ–‡ç« é‡ç‚¹å…³æ³¨äº†äº¤å‰ç†µæŸå¤±ä¸‹çš„ä¸Šä¸‹æ–‡å†…nå…ƒè¯­è¨€æ¨¡å‹çš„å­¦ä¹ è¿‡ç¨‹ï¼Œæå‡ºäº†å‚æ•°é…ç½®çš„å……åˆ†æ¡ä»¶ä»¥æˆä¸ºé©»ç‚¹ã€‚é€šè¿‡æ„å»ºç®€åŒ–Transformeræ¨¡å‹çš„å‚æ•°é…ç½®é›†ï¼Œä»£è¡¨kå…ƒä¼°ç®—å™¨ï¼ˆå¯¹äºkâ‰¤nï¼‰ï¼Œæ–‡ç« å±•ç¤ºäº†åœ¨æ— é™åºåˆ—é•¿åº¦å’Œå‚æ•°èŒƒæ•°æé™ä¸‹ï¼Œäººå£æŸå¤±çš„æ¢¯åº¦åœ¨è¿™äº›è§£å†³æ–¹æ¡ˆå¤„æ¶ˆå¤±ã€‚è¿™è¡¨æ˜æŸå¤±æ™¯è§‚çš„ä¸€ä¸ªå…³é”®å±æ€§ï¼šå­nå…ƒæ˜¯äººå£äº¤å‰ç†µæŸå¤±çš„è¿‘ä¼¼é©»ç‚¹ï¼Œä¸ºå¹¿æ³›è§‚å¯Ÿåˆ°çš„ç°è±¡ï¼ˆå¦‚é˜¶æ®µæ€§å­¦ä¹ åŠ¨åŠ›å’Œæ–°å…´ç›¸å˜ï¼‰æä¾›äº†ç†è®ºæ´å¯Ÿã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ¢è®¨äº†Transformeræ¨¡å‹åœ¨ä¸Šä¸‹æ–‡é¢„æµ‹ä»»åŠ¡ä¸­çš„æŸå¤±æ™¯è§‚ã€‚</li>
<li>åˆ†æäº†äº¤å‰ç†µæŸå¤±ä¸‹çš„ä¸Šä¸‹æ–‡å†…nå…ƒè¯­è¨€æ¨¡å‹çš„å­¦ä¹ è¿‡ç¨‹ã€‚</li>
<li>æå‡ºäº†å‚æ•°é…ç½®çš„å……åˆ†æ¡ä»¶ï¼Œä½¿å…¶æˆä¸ºé©»ç‚¹ã€‚</li>
<li>é€šè¿‡æ„å»ºç®€åŒ–æ¨¡å‹å±•ç¤ºäº†å­nå…ƒæ˜¯äººå£äº¤å‰ç†µæŸå¤±çš„è¿‘ä¼¼é©»ç‚¹ã€‚</li>
<li>æ­ç¤ºäº†é˜¶æ®µæ€§å­¦ä¹ åŠ¨åŠ›å’Œæ–°å…´ç›¸å˜ç­‰å¹¿æ³›è§‚å¯Ÿç°è±¡çš„ç†è®ºåŸå› ã€‚</li>
<li>æ•°å€¼å®éªŒæ”¯æŒäº†ä¸Šè¿°ç†è®ºæ´å¯Ÿï¼Œå±•ç¤ºäº†nå…ƒçš„å­¦ä¹ åŠ¨æ€ï¼Œè¡¨ç°ä¸ºè¿‘é©»ç‚¹è§£ä¹‹é—´çš„ç¦»æ•£è¿‡æ¸¡ã€‚</li>
<li>æ–‡ä¸­åˆ†æäº†åºåˆ—é•¿åº¦å’Œå‚æ•°èŒƒæ•°å¯¹æŸå¤±æ™¯è§‚çš„å½±å“ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.12837">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-a53ceb9c23a365c60e9ef28a669ec4f6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-816abe0aab5fb94db4e95636ee63409a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-067b7d3fe3e51a0d3de4a2c5962ea0f9.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="CRED-SQL-Enhancing-Real-world-Large-Scale-Database-Text-to-SQL-Parsing-through-Cluster-Retrieval-and-Execution-Description"><a href="#CRED-SQL-Enhancing-Real-world-Large-Scale-Database-Text-to-SQL-Parsing-through-Cluster-Retrieval-and-Execution-Description" class="headerlink" title="CRED-SQL: Enhancing Real-world Large Scale Database Text-to-SQL Parsing   through Cluster Retrieval and Execution Description"></a>CRED-SQL: Enhancing Real-world Large Scale Database Text-to-SQL Parsing   through Cluster Retrieval and Execution Description</h2><p><strong>Authors:Shaoming Duan, Zirui Wang, Chuanyi Liu, Zhibin Zhu, Yuhao Zhang, Peiyi Han, Liang Yan, Zewu Penge</strong></p>
<p>Recent advances in large language models (LLMs) have significantly improved the accuracy of Text-to-SQL systems. However, a critical challenge remains: the semantic mismatch between natural language questions (NLQs) and their corresponding SQL queries. This issue is exacerbated in large-scale databases, where semantically similar attributes hinder schema linking and semantic drift during SQL generation, ultimately reducing model accuracy. To address these challenges, we introduce CRED-SQL, a framework designed for large-scale databases that integrates Cluster Retrieval and Execution Description. CRED-SQL first performs cluster-based large-scale schema retrieval to pinpoint the tables and columns most relevant to a given NLQ, alleviating schema mismatch. It then introduces an intermediate natural language representation-Execution Description Language (EDL)-to bridge the gap between NLQs and SQL. This reformulation decomposes the task into two stages: Text-to-EDL and EDL-to-SQL, leveraging LLMsâ€™ strong general reasoning capabilities while reducing semantic deviation. Extensive experiments on two large-scale, cross-domain benchmarks-SpiderUnion and BirdUnion-demonstrate that CRED-SQL achieves new state-of-the-art (SOTA) performance, validating its effectiveness and scalability. Our code is available at <a target="_blank" rel="noopener" href="https://github.com/smduan/CRED-SQL.git">https://github.com/smduan/CRED-SQL.git</a> </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æœ€æ–°è¿›å±•æ˜¾è‘—æé«˜äº†æ–‡æœ¬åˆ°SQLç³»ç»Ÿçš„å‡†ç¡®æ€§ã€‚ç„¶è€Œï¼Œä»ç„¶å­˜åœ¨ä¸€ä¸ªå…³é”®æŒ‘æˆ˜ï¼šè‡ªç„¶è¯­è¨€é—®é¢˜ï¼ˆNLQï¼‰ä¸å…¶å¯¹åº”çš„SQLæŸ¥è¯¢ä¹‹é—´çš„è¯­ä¹‰ä¸åŒ¹é…ã€‚åœ¨å¤§è§„æ¨¡æ•°æ®åº“ä¸­ï¼Œè¿™ä¸ªé—®é¢˜æ›´ä¸ºä¸¥é‡ï¼Œè¯­ä¹‰ä¸Šç›¸ä¼¼çš„å±æ€§é˜»ç¢äº†æ¨¡å¼é“¾æ¥å’ŒSQLç”Ÿæˆè¿‡ç¨‹ä¸­çš„è¯­ä¹‰æ¼‚ç§»ï¼Œæœ€ç»ˆå¯¼è‡´æ¨¡å‹ç²¾åº¦é™ä½ã€‚ä¸ºäº†è§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†CRED-SQLæ¡†æ¶ï¼Œè¯¥æ¡†æ¶é€‚ç”¨äºå¤§è§„æ¨¡æ•°æ®åº“ï¼Œé›†æˆäº†Cluster Retrievalå’ŒExecution Descriptionã€‚CRED-SQLé¦–å…ˆæ‰§è¡ŒåŸºäºèšç±»çš„å¤§è§„æ¨¡æ¨¡å¼æ£€ç´¢ï¼Œä»¥å®šä½ä¸ç»™å®šNLQæœ€ç›¸å…³çš„è¡¨å’Œåˆ—ï¼Œç¼“è§£æ¨¡å¼ä¸åŒ¹é…é—®é¢˜ã€‚ç„¶åï¼Œå®ƒå¼•å…¥äº†ä¸€ç§ä¸­é—´è‡ªç„¶è¯­è¨€è¡¨ç¤ºå½¢å¼â€”â€”æ‰§è¡Œæè¿°è¯­è¨€ï¼ˆEDLï¼‰â€”â€”æ¥å¼¥åˆNLQå’ŒSQLä¹‹é—´çš„å·®è·ã€‚è¿™ç§é‡æ–°è¡¨è¿°å°†ä»»åŠ¡åˆ†è§£ä¸ºä¸¤ä¸ªé˜¶æ®µï¼šæ–‡æœ¬åˆ°EDLå’ŒEDLåˆ°SQLï¼Œåˆ©ç”¨LLMçš„å¼ºå¤§é€šç”¨æ¨ç†èƒ½åŠ›ï¼ŒåŒæ—¶å‡å°‘è¯­ä¹‰åå·®ã€‚åœ¨ä¸¤ä¸ªå¤§è§„æ¨¡ã€è·¨åŸŸåŸºå‡†æµ‹è¯•SpiderUnionå’ŒBirdUnionä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼ŒCRED-SQLè¾¾åˆ°äº†æœ€æ–°çš„æœ€ä½³æ€§èƒ½æ°´å¹³ï¼ŒéªŒè¯äº†å…¶æœ‰æ•ˆæ€§å’Œå¯æ‰©å±•æ€§ã€‚æˆ‘ä»¬çš„ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/smduan/CRED-SQL.git%E4%B8%8A%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/smduan/CRED-SQL.gitä¸Šæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.12769v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨æ–‡æœ¬åˆ°SQLç³»ç»Ÿçš„å‡†ç¡®æ€§æ–¹é¢å–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œä½†è¯­ä¹‰åŒ¹é…é—®é¢˜ä»æ˜¯å…³é”®æŒ‘æˆ˜ã€‚é’ˆå¯¹å¤§å‹æ•°æ®åº“ä¸­è¯­ä¹‰ç›¸ä¼¼å±æ€§å¯¼è‡´çš„æ¨¡å¼é“¾æ¥å’Œè¯­ä¹‰æ¼‚ç§»é—®é¢˜ï¼Œæå‡ºCRED-SQLæ¡†æ¶ã€‚å®ƒé€šè¿‡é›†ç¾¤æ£€ç´¢æ‰§è¡Œæè¿°ï¼Œé¦–å…ˆè¿›è¡ŒåŸºäºé›†ç¾¤çš„å¤§è§„æ¨¡æ¨¡å¼æ£€ç´¢ï¼Œä»¥é’ˆå¯¹ç»™å®šçš„è‡ªç„¶è¯­è¨€é—®é¢˜å®šä½ç›¸å…³çš„è¡¨å’Œåˆ—ï¼Œç„¶åå¼•å…¥ä¸­é—´çš„è‡ªç„¶è¯­è¨€è¡¨ç¤ºå½¢å¼â€”â€”æ‰§è¡Œæè¿°è¯­è¨€ï¼ˆEDLï¼‰ï¼Œç¼©å°NLQå’ŒSQLä¹‹é—´çš„å·®è·ã€‚å®éªŒè¯æ˜ï¼ŒCRED-SQLåœ¨å¤§å‹è·¨åŸŸåŸºå‡†æµ‹è¯•ä¸­å®ç°äº†æœ€æ–°çŠ¶æ€çš„è‰ºæœ¯ï¼ˆSOTAï¼‰æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å·²æé«˜æ–‡æœ¬åˆ°SQLç³»ç»Ÿçš„å‡†ç¡®æ€§ã€‚</li>
<li>è¯­ä¹‰åŒ¹é…é—®é¢˜æ˜¯æ–‡æœ¬åˆ°SQLè½¬æ¢ä¸­çš„å…³é”®æŒ‘æˆ˜ã€‚</li>
<li>CRED-SQLæ¡†æ¶é’ˆå¯¹å¤§å‹æ•°æ®åº“è®¾è®¡ï¼Œé€šè¿‡é›†ç¾¤æ£€ç´¢å’Œæ‰§è¡Œæè¿°æ¥è§£å†³è¯­ä¹‰åŒ¹é…é—®é¢˜ã€‚</li>
<li>CRED-SQLåˆ©ç”¨é›†ç¾¤æ£€ç´¢å®šä½ä¸NLQç›¸å…³çš„è¡¨å’Œåˆ—ï¼Œç¼“è§£æ¨¡å¼ä¸åŒ¹é…é—®é¢˜ã€‚</li>
<li>EDLä½œä¸ºä¸­é—´çš„è‡ªç„¶è¯­è¨€è¡¨ç¤ºå½¢å¼ï¼Œç¼©å°äº†NLQå’ŒSQLä¹‹é—´çš„å·®è·ã€‚</li>
<li>CRED-SQLå°†ä»»åŠ¡åˆ†ä¸ºä¸¤ä¸ªé˜¶æ®µï¼šæ–‡æœ¬åˆ°EDLå’ŒEDLåˆ°SQLï¼Œåˆ©ç”¨LLMçš„å¼ºå¤§é€šç”¨æ¨ç†èƒ½åŠ›ã€‚</li>
<li>å®éªŒè¯æ˜CRED-SQLåœ¨å¤§å‹è·¨åŸŸåŸºå‡†æµ‹è¯•ä¸­å®ç°äº†æœ€æ–°çŠ¶æ€çš„è‰ºæœ¯ï¼ˆSOTAï¼‰æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.12769">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-85a6bc96031ec9965af7f3de01fa216e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1b48eb7254e77ced700f8a9d1290a021.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-5b802327ed1dc1597e89b5ff60f532d7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5d8eb88987412355ca099794b58b563e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c1e4ee772fd2e12f8969357afa5f8a72.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="ConTextTab-A-Semantics-Aware-Tabular-In-Context-Learner"><a href="#ConTextTab-A-Semantics-Aware-Tabular-In-Context-Learner" class="headerlink" title="ConTextTab: A Semantics-Aware Tabular In-Context Learner"></a>ConTextTab: A Semantics-Aware Tabular In-Context Learner</h2><p><strong>Authors:Marco Spinaci, Marek Polewczyk, Maximilian Schambach, Sam Thelin</strong></p>
<p>Tabular in-context learning (ICL) has recently achieved state-of-the-art (SOTA) performance on several tabular prediction tasks. Previously restricted to classification problems on small tables, recent advances such as TabPFN and TabICL have extended its use to larger datasets. Although current table-native ICL architectures are architecturally efficient and well-adapted to tabular data structures, their exclusive training on synthetic data limits their ability to fully leverage the rich semantics and world knowledge contained in real-world tabular data. At the other end of the spectrum, tabular ICL models based on pretrained large language models such as TabuLa-8B integrate deep semantic understanding and world knowledge but are only able to make use of a small amount of context due to inherent architectural limitations. With the aim to combine the best of both these worlds, we introduce ConTextTab, integrating semantic understanding and alignment into a table-native ICL framework. By employing specialized embeddings for different data modalities and by training on large-scale real-world tabular data, our model is competitive with SOTA across a broad set of benchmarks while setting a new standard on the semantically rich CARTE benchmark. Code and model checkpoints are available at: <a target="_blank" rel="noopener" href="https://github.com/SAP-samples/contexttab">https://github.com/SAP-samples/contexttab</a> </p>
<blockquote>
<p>è¡¨æ ¼ä¸Šä¸‹æ–‡å­¦ä¹ ï¼ˆICLï¼‰æœ€è¿‘å·²åœ¨å¤šä¸ªè¡¨æ ¼é¢„æµ‹ä»»åŠ¡ä¸Šè¾¾åˆ°äº†æœ€æ–°æŠ€æœ¯æ°´å¹³ï¼ˆSOTAï¼‰ã€‚ä¹‹å‰ä»…é™äºå°å‹è¡¨æ ¼çš„åˆ†ç±»é—®é¢˜ï¼Œæœ€è¿‘çš„è¿›å±•ï¼Œå¦‚TabPFNå’ŒTabICLï¼Œå·²ç»å°†å…¶åº”ç”¨æ‰©å±•åˆ°äº†æ›´å¤§çš„æ•°æ®é›†ã€‚å°½ç®¡å½“å‰çš„è¡¨æ ¼åŸç”ŸICLæ¶æ„åœ¨æ•ˆç‡ä¸Šå¾ˆé«˜ï¼Œå¹¶ä¸”å¾ˆé€‚åº”è¡¨æ ¼æ•°æ®ç»“æ„ï¼Œä½†å®ƒä»¬ä»…åœ¨åˆæˆæ•°æ®è¿›è¡Œè®­ç»ƒï¼Œè¿™é™åˆ¶äº†å®ƒä»¬å……åˆ†åˆ©ç”¨ç°å®ä¸–ç•Œè¡¨æ ¼æ•°æ®ä¸­çš„ä¸°å¯Œè¯­ä¹‰å’Œå…¨çƒçŸ¥è¯†çš„èƒ½åŠ›ã€‚å¦ä¸€æ–¹é¢ï¼ŒåŸºäºé¢„è®­ç»ƒçš„å¤§å‹è¯­è¨€æ¨¡å‹çš„è¡¨æ ¼ICLæ¨¡å‹ï¼Œå¦‚TabuLa-8Bï¼Œèåˆäº†æ·±åº¦è¯­ä¹‰ç†è§£å’Œå…¨çƒçŸ¥è¯†ï¼Œä½†ç”±äºå…¶å›ºæœ‰çš„æ¶æ„é™åˆ¶ï¼Œåªèƒ½åˆ©ç”¨å°‘é‡çš„ä¸Šä¸‹æ–‡ã€‚ä¸ºäº†ç»“åˆè¿™ä¸¤ä¸ªæ–¹é¢çš„ä¼˜ç‚¹ï¼Œæˆ‘ä»¬å¼•å…¥äº†ConTextTabï¼Œå°†è¯­ä¹‰ç†è§£å’Œå¯¹é½é›†æˆåˆ°è¡¨æ ¼åŸç”ŸICLæ¡†æ¶ä¸­ã€‚é€šè¿‡é’ˆå¯¹ä¸åŒæ•°æ®æ¨¡æ€é‡‡ç”¨ä¸“ç”¨åµŒå…¥ï¼Œå¹¶åœ¨å¤§è§„æ¨¡ç°å®ä¸–ç•Œè¡¨æ ¼æ•°æ®ä¸Šè¿›è¡Œè®­ç»ƒï¼Œæˆ‘ä»¬çš„æ¨¡å‹åœ¨ä¸€ç³»åˆ—åŸºå‡†æµ‹è¯•ä¸­å…·æœ‰ç«äº‰åŠ›ï¼Œå¹¶åœ¨è¯­ä¹‰ä¸°å¯Œçš„CARTEåŸºå‡†æµ‹è¯•ä¸Šè®¾å®šäº†æ–°çš„æ ‡å‡†ã€‚ä»£ç å’Œæ¨¡å‹æ£€æŸ¥ç‚¹å¯é€šè¿‡ä»¥ä¸‹é“¾æ¥è·å–ï¼š<a target="_blank" rel="noopener" href="https://github.com/SAP-samples/contexttab">https://github.com/SAP-samples/contexttab</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.10707v3">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>Tabular in-context learningï¼ˆICLï¼‰å·²åœ¨å¤šä¸ªè¡¨æ ¼é¢„æµ‹ä»»åŠ¡ä¸Šè¾¾åˆ°æœ€æ–°æŠ€æœ¯æ°´å¹³ã€‚è¿‘æœŸçš„å‘å±•å¦‚TabPFNå’ŒTabICLï¼Œå°†å…¶åº”ç”¨èŒƒå›´æ‰©å±•åˆ°äº†å¤§å‹æ•°æ®é›†ä¸Šã€‚è™½ç„¶å½“å‰çš„è¡¨æ ¼åŸç”ŸICLæ¶æ„å¯¹è¡¨æ ¼æ•°æ®ç»“æ„è¿›è¡Œäº†é«˜æ•ˆé€‚åº”ï¼Œä½†å®ƒä»¬ä»…åœ¨åˆæˆæ•°æ®ä¸Šè¿›è¡Œè®­ç»ƒï¼Œæ— æ³•å……åˆ†åˆ©ç”¨ç°å®ä¸–ç•Œè¡¨æ ¼æ•°æ®ä¸­çš„ä¸°å¯Œè¯­ä¹‰å’Œå…¨çƒçŸ¥è¯†ã€‚ç›¸åï¼ŒåŸºäºé¢„è®­ç»ƒçš„å¤§å‹è¯­è¨€æ¨¡å‹çš„è¡¨æ ¼ICLæ¨¡å‹ï¼ˆå¦‚TabuLa-8Bï¼‰èåˆäº†æ·±å±‚è¯­ä¹‰ç†è§£å’Œå…¨çƒçŸ¥è¯†ï¼Œä½†ç”±äºå…¶å›ºæœ‰çš„æ¶æ„é™åˆ¶ï¼Œåªèƒ½ä½¿ç”¨å°‘é‡çš„ä¸Šä¸‹æ–‡ä¿¡æ¯ã€‚ä¸ºäº†ç»“åˆä¸¤è€…çš„ä¼˜ç‚¹ï¼Œæˆ‘ä»¬æ¨å‡ºäº†ConTextTabï¼Œå°†è¯­ä¹‰ç†è§£å’Œå¯¹é½é›†æˆåˆ°è¡¨æ ¼åŸç”ŸICLæ¡†æ¶ä¸­ã€‚é€šè¿‡ä¸ºä¸åŒæ•°æ®æ¨¡å¼é‡‡ç”¨ä¸“ä¸šåµŒå…¥ï¼Œå¹¶åœ¨å¤§è§„æ¨¡ç°å®ä¸–ç•Œè¡¨æ ¼æ•°æ®ä¸Šè¿›è¡Œè®­ç»ƒï¼Œæˆ‘ä»¬çš„æ¨¡å‹åœ¨å¹¿æ³›çš„åŸºå‡†æµ‹è¯•ä¸Šè¾¾åˆ°æœ€æ–°æŠ€æœ¯æ°´å¹³ï¼Œå°¤å…¶æ˜¯åœ¨è¯­ä¹‰ä¸°å¯Œçš„CARTEåŸºå‡†ä¸Šè®¾å®šäº†æ–°çš„æ ‡å‡†ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Tabular in-context learning (ICL) åœ¨å¤šä¸ªè¡¨æ ¼é¢„æµ‹ä»»åŠ¡ä¸Šè¾¾åˆ° state-of-the-art (SOTA) æ€§èƒ½ã€‚</li>
<li>ICL è¿‘æœŸæ‰©å±•åˆ°äº†å¤§å‹æ•°æ®é›†ä¸Šï¼Œå¦‚ TabPFN å’Œ TabICL ç­‰å‘å±•ã€‚</li>
<li>è¡¨æ ¼åŸç”ŸICLæ¶æ„å¯¹è¡¨æ ¼æ•°æ®ç»“æ„è¿›è¡Œäº†é«˜æ•ˆé€‚åº”ï¼Œä½†å—é™äºä»…åœ¨åˆæˆæ•°æ®ä¸Šçš„è®­ç»ƒã€‚</li>
<li>åŸºäºé¢„è®­ç»ƒçš„å¤§å‹è¯­è¨€æ¨¡å‹çš„è¡¨æ ¼ICLæ¨¡å‹ï¼ˆå¦‚TabuLa-8Bï¼‰èƒ½å¤Ÿåˆ©ç”¨æ·±å±‚è¯­ä¹‰ç†è§£å’Œå…¨çƒçŸ¥è¯†ï¼Œä½†å—é™äºä¸Šä¸‹æ–‡ä¿¡æ¯çš„ä½¿ç”¨é‡ã€‚</li>
<li>ConTextTab ç»“åˆäº†è¡¨æ ¼åŸç”ŸICLå’ŒåŸºäºé¢„è®­ç»ƒçš„å¤§å‹è¯­è¨€æ¨¡å‹çš„ä¼˜ç‚¹ã€‚</li>
<li>ConTextTab é€šè¿‡é‡‡ç”¨ä¸“ä¸šåµŒå…¥å’Œå¤§è§„æ¨¡ç°å®ä¸–ç•Œè¡¨æ ¼æ•°æ®è®­ç»ƒï¼Œæé«˜äº†æ¨¡å‹æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.10707">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-79f65a5d7367d989bb251c0a30c1097f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-abeb4fabae3f83d5f222495a006493bc.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="Flexible-Operator-Fusion-for-Fast-Sparse-Transformer-with-Diverse-Masking-on-GPU"><a href="#Flexible-Operator-Fusion-for-Fast-Sparse-Transformer-with-Diverse-Masking-on-GPU" class="headerlink" title="Flexible Operator Fusion for Fast Sparse Transformer with Diverse   Masking on GPU"></a>Flexible Operator Fusion for Fast Sparse Transformer with Diverse   Masking on GPU</h2><p><strong>Authors:Wenhao Dai, Haodong Deng, Mengfei Rong, Xinyu Yang, Hongyu Liu, Fangxin Liu, Hailong Yang, Qianwen Cao, Qingxiao Sun</strong></p>
<p>Large language models are popular around the world due to their powerful understanding capabilities. As the core component of LLMs, accelerating Transformer through parallelization has gradually become a hot research topic. Mask layers introduce sparsity into Transformer to reduce calculations. However, previous works rarely focus on the performance optimization of sparse Transformer. Moreover, rule-based mechanisms ignore the fusion opportunities of mixed-type operators and fail to adapt to various sequence lengths. To address the above problems, we propose STOF, a framework that incorporates optimizations for Sparse Transformer via flexible masking and operator fusion on GPU. We firstly unify the storage format and kernel implementation for the multi-head attention. Then, we map fusion schemes to compilation templates and determine the optimal parameter setting through a two-stage search engine. The experimental results show that compared to the state-of-the-art work, STOF achieves maximum speedups of 1.7x in MHA computation and 1.5x in end-to-end inference. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹å› å…¶å¼ºå¤§çš„ç†è§£èƒ½åŠ›è€Œåœ¨å…¨çƒèŒƒå›´å†…å¹¿å—æ¬¢è¿ã€‚ä½œä¸ºå¤§å‹è¯­è¨€æ¨¡å‹çš„æ ¸å¿ƒç»„ä»¶ï¼Œé€šè¿‡å¹¶è¡ŒåŒ–åŠ é€ŸTransformeré€æ¸æˆä¸ºä¸€ä¸ªçƒ­é—¨ç ”ç©¶è¯¾é¢˜ã€‚æ©ç å±‚å°†ç¨€ç–æ€§å¼•å…¥Transformerä¸­ä»¥å‡å°‘è®¡ç®—ã€‚ç„¶è€Œï¼Œä¹‹å‰çš„å·¥ä½œå¾ˆå°‘å…³æ³¨ç¨€ç–Transformerçš„æ€§èƒ½ä¼˜åŒ–ã€‚æ­¤å¤–ï¼ŒåŸºäºè§„åˆ™çš„æ–¹æ³•å¿½ç•¥äº†æ··åˆç±»å‹æ“ä½œç¬¦çš„èåˆæœºä¼šï¼Œå¹¶ä¸”æ— æ³•é€‚åº”å„ç§åºåˆ—é•¿åº¦ã€‚ä¸ºäº†è§£å†³ä¸Šè¿°é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†STOFï¼Œè¿™æ˜¯ä¸€ä¸ªé€šè¿‡çµæ´»çš„æ©ç å’ŒGPUä¸Šçš„æ“ä½œç¬¦èåˆå¯¹ç¨€ç–Transformerè¿›è¡Œä¼˜åŒ–çš„æ¡†æ¶ã€‚æˆ‘ä»¬é¦–å…ˆå¯¹å¤šå¤´æ³¨æ„åŠ›çš„å­˜å‚¨æ ¼å¼å’Œå†…æ ¸å®ç°è¿›è¡Œäº†ç»Ÿä¸€ã€‚ç„¶åï¼Œæˆ‘ä»¬å°†èåˆæ–¹æ¡ˆæ˜ å°„åˆ°ç¼–è¯‘æ¨¡æ¿ä¸Šï¼Œå¹¶é€šè¿‡ä¸¤é˜¶æ®µæœç´¢å¼•æ“ç¡®å®šæœ€ä½³å‚æ•°è®¾ç½®ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œä¸æœ€æ–°å·¥ä½œç›¸æ¯”ï¼ŒSTOFåœ¨å¤šå¤´æ³¨æ„åŠ›è®¡ç®—ä¸­å®ç°äº†æœ€é«˜è¾¾1.7å€çš„åŠ é€Ÿï¼Œåœ¨ç«¯åˆ°ç«¯æ¨ç†ä¸­å®ç°äº†æœ€é«˜è¾¾1.5å€çš„åŠ é€Ÿã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.06095v2">PDF</a> </p>
<p><strong>Summary</strong>ï¼šå¤§è¯­è¨€æ¨¡å‹å› å¼ºå¤§çš„ç†è§£åŠ›è€Œå…¨çƒæµè¡Œã€‚ä½œä¸ºå…¶æ ¸å¿ƒç»„ä»¶ï¼Œé€šè¿‡å¹¶è¡ŒåŒ–åŠ é€ŸTransformeré€æ¸æˆä¸ºçƒ­é—¨ç ”ç©¶è¯¾é¢˜ã€‚æ©ç å±‚å°†ç¨€ç–æ€§å¼•å…¥Transformerä»¥å‡å°‘è®¡ç®—ã€‚ç„¶è€Œï¼Œä¹‹å‰çš„ç ”ç©¶å¾ˆå°‘å…³æ³¨ç¨€ç–Transformerçš„æ€§èƒ½ä¼˜åŒ–ã€‚æ­¤å¤–ï¼ŒåŸºäºè§„åˆ™æœºåˆ¶å¿½ç•¥äº†æ··åˆç±»å‹ç®—å­çš„èåˆæœºä¼šï¼Œä¸”ä¸èƒ½é€‚åº”å„ç§åºåˆ—é•¿åº¦ã€‚ä¸ºè§£å†³ä¸Šè¿°é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºSTOFæ¡†æ¶ï¼Œé€šè¿‡çµæ´»çš„æ©ç å’ŒGPUä¸Šçš„ç®—å­èåˆå¯¹ç¨€ç–Transformerè¿›è¡Œä¼˜åŒ–ã€‚æˆ‘ä»¬ç»Ÿä¸€äº†å¤šå¤´æ³¨æ„åŠ›å­˜å‚¨æ ¼å¼å’Œå†…æ ¸å®ç°ã€‚ç„¶åï¼Œæˆ‘ä»¬å°†èåˆæ–¹æ¡ˆæ˜ å°„åˆ°ç¼–è¯‘æ¨¡æ¿ä¸Šï¼Œå¹¶é€šè¿‡ä¸¤é˜¶æ®µæœç´¢å¼•æ“ç¡®å®šæœ€ä½³å‚æ•°è®¾ç½®ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œä¸æœ€æ–°å·¥ä½œç›¸æ¯”ï¼ŒSTOFåœ¨å¤šå¤´æ³¨æ„åŠ›è®¡ç®—å’Œç«¯åˆ°ç«¯æ¨ç†æ–¹é¢åˆ†åˆ«å®ç°äº†æœ€é«˜è¾¾1.7å€å’Œ1.5å€çš„åŠ é€Ÿã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>å¤§è¯­è¨€æ¨¡å‹å› å…¶å¼ºå¤§çš„ç†è§£åŠ›è€Œå¤‡å—å…³æ³¨ï¼Œå…¶æ ¸å¿ƒç»„ä»¶Transformerçš„å¹¶è¡ŒåŒ–åŠ é€Ÿæˆä¸ºç ”ç©¶çƒ­ç‚¹ã€‚</li>
<li>æ©ç å±‚å¼•å…¥ç¨€ç–æ€§ä»¥å‡å°‘Transformerçš„è®¡ç®—ï¼Œä½†ä¹‹å‰çš„ç ”ç©¶æœªå……åˆ†å…³æ³¨ç¨€ç–Transformerçš„æ€§èƒ½ä¼˜åŒ–ã€‚</li>
<li>åŸºäºè§„åˆ™æœºåˆ¶ä¸èƒ½é€‚åº”å„ç§åºåˆ—é•¿åº¦ï¼Œä¸”å¿½ç•¥äº†æ··åˆç±»å‹ç®—å­çš„èåˆæœºä¼šã€‚</li>
<li>STOFæ¡†æ¶é€šè¿‡çµæ´»çš„æ©ç å’ŒGPUä¸Šçš„ç®—å­èåˆæ¥ä¼˜åŒ–ç¨€ç–Transformerã€‚</li>
<li>STOFç»Ÿä¸€äº†å¤šå¤´æ³¨æ„åŠ›çš„å­˜å‚¨æ ¼å¼å’Œå†…æ ¸å®ç°ã€‚</li>
<li>STOFå°†èåˆæ–¹æ¡ˆæ˜ å°„åˆ°ç¼–è¯‘æ¨¡æ¿ä¸Šï¼Œå¹¶é€šè¿‡ä¸¤é˜¶æ®µæœç´¢å¼•æ“ä¼˜åŒ–å‚æ•°è®¾ç½®ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.06095">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-9f5e5d8c0cf8e4c7dc0f64e7a453af49.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8db784096736753c613ab3c914ff0a1b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b4135a1b2d1f869f399f4af234e6fc48.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b558a42012cf4c513fb1f14d7222f86c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6d0d2ddd6e9a8a2f0f5d4efc6bb74f9b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f30666b6cfcd981956ca41ca313a2028.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c38f19bbd100a014d9e97a85a2dedfc3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b9a1502c111ecf03355e8aec436e711e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-de2cb796caa192ec2c1c45e07193545a.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="Soft-Reasoning-Navigating-Solution-Spaces-in-Large-Language-Models-through-Controlled-Embedding-Exploration"><a href="#Soft-Reasoning-Navigating-Solution-Spaces-in-Large-Language-Models-through-Controlled-Embedding-Exploration" class="headerlink" title="Soft Reasoning: Navigating Solution Spaces in Large Language Models   through Controlled Embedding Exploration"></a>Soft Reasoning: Navigating Solution Spaces in Large Language Models   through Controlled Embedding Exploration</h2><p><strong>Authors:Qinglin Zhu, Runcong Zhao, Hanqi Yan, Yulan He, Yudong Chen, Lin Gui</strong></p>
<p>Large Language Models (LLMs) struggle with complex reasoning due to limited diversity and inefficient search. We propose Soft Reasoning, an embedding-based search framework that optimises the embedding of the first token to guide generation. It combines (1) embedding perturbation for controlled exploration and (2) Bayesian optimisation to refine embeddings via a verifier-guided objective, balancing exploration and exploitation. This approach improves reasoning accuracy and coherence while avoiding reliance on heuristic search. Experiments demonstrate superior correctness with minimal computation, making it a scalable, model-agnostic solution. The code is released at <a target="_blank" rel="noopener" href="https://github.com/alickzhu/Soft-Reasoning">https://github.com/alickzhu/Soft-Reasoning</a>. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ç”±äºå¤šæ ·æ€§æœ‰é™å’Œæœç´¢æ•ˆç‡ä½ä¸‹ï¼Œéš¾ä»¥åº”å¯¹å¤æ‚æ¨ç†ã€‚æˆ‘ä»¬æå‡ºäº†Soft Reasoningï¼Œè¿™æ˜¯ä¸€ç§åŸºäºåµŒå…¥çš„æœç´¢æ¡†æ¶ï¼Œé€šè¿‡ä¼˜åŒ–ç¬¬ä¸€ä¸ªè¯çš„åµŒå…¥æ¥å¼•å¯¼ç”Ÿæˆã€‚å®ƒç»“åˆäº†ï¼ˆ1ï¼‰åµŒå…¥æ‰°åŠ¨è¿›è¡Œå—æ§æ¢ç´¢å’Œï¼ˆ2ï¼‰é€šè¿‡éªŒè¯å™¨å¼•å¯¼çš„ç›®æ ‡ä¼˜åŒ–è´å¶æ–¯æ–¹æ³•æ¥å¹³è¡¡æ¢ç´¢å’Œåˆ©ç”¨ã€‚è¿™ç§æ–¹æ³•æé«˜äº†æ¨ç†çš„å‡†ç¡®æ€§å’Œè¿è´¯æ€§ï¼ŒåŒæ—¶é¿å…äº†ä¾èµ–å¯å‘å¼æœç´¢ã€‚å®éªŒè¯æ˜ï¼Œè¯¥æ–¹æ³•è®¡ç®—æ­£ç¡®ç‡æé«˜ä¸”è®¡ç®—é‡æå°ï¼Œæ˜¯ä¸€ç§å¯æ‰©å±•ä¸”é€‚ç”¨äºå¤šç§æ¨¡å‹çš„è§£å†³æ–¹æ¡ˆã€‚ä»£ç å·²å‘å¸ƒåœ¨<a target="_blank" rel="noopener" href="https://github.com/alickzhu/Soft-Reasoning">https://github.com/alickzhu/Soft-Reasoning</a>ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.24688v3">PDF</a> Accepted as a Spotlight at ICML 2025</p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨å¤„ç†å¤æ‚æ¨ç†æ—¶é¢ä¸´å¤šæ ·æ€§æœ‰é™å’Œæœç´¢æ•ˆç‡ä½ä¸‹çš„é—®é¢˜ã€‚æœ¬æ–‡æå‡ºä¸€ç§åŸºäºåµŒå…¥çš„æœç´¢æ¡†æ¶â€”â€”Soft Reasoningï¼Œé€šè¿‡ä¼˜åŒ–ç¬¬ä¸€ä¸ªè¯çš„åµŒå…¥æ¥å¼•å¯¼ç”Ÿæˆã€‚è¯¥æ¡†æ¶ç»“åˆäº†åµŒå…¥æ‰°åŠ¨å®ç°å¯æ§æ¢ç´¢ï¼Œå¹¶åˆ©ç”¨è´å¶æ–¯ä¼˜åŒ–é€šè¿‡éªŒè¯å™¨æŒ‡å¯¼çš„ç›®æ ‡æ¥ä¼˜åŒ–åµŒå…¥ï¼Œå¹³è¡¡æ¢ç´¢ä¸åˆ©ç”¨ã€‚æ­¤æ–¹æ³•æé«˜äº†æ¨ç†çš„å‡†ç¡®æ€§å’Œè¿è´¯æ€§ï¼Œé¿å…äº†ä¾èµ–å¯å‘å¼æœç´¢ï¼Œå®éªŒè¡¨æ˜å…¶åœ¨è®¡ç®—é‡å°çš„æƒ…å†µä¸‹èƒ½æ˜¾è‘—æé«˜æ­£ç¡®æ€§ï¼Œæˆä¸ºä¸€ç§å¯æ‰©å±•çš„æ¨¡å‹æ— å…³è§£å†³æ–¹æ¡ˆã€‚ç›¸å…³ä»£ç å·²å‘å¸ƒåœ¨<a target="_blank" rel="noopener" href="https://github.com/alickzhu/Soft-Reasoning%E4%B8%8A%E3%80%82">https://github.com/alickzhu/Soft-Reasoningä¸Šã€‚</a></p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>LLMåœ¨å¤„ç†å¤æ‚æ¨ç†æ—¶é¢ä¸´å¤šæ ·æ€§ä¸è¶³å’Œæœç´¢æ•ˆç‡ä½ä¸‹çš„é—®é¢˜ã€‚</li>
<li>Soft Reasoningæ˜¯ä¸€ä¸ªåŸºäºåµŒå…¥çš„æœç´¢æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³è¿™äº›é—®é¢˜ã€‚</li>
<li>Soft Reasoningé€šè¿‡ä¼˜åŒ–ç¬¬ä¸€ä¸ªè¯çš„åµŒå…¥æ¥å¼•å¯¼ç”Ÿæˆã€‚</li>
<li>è¯¥æ¡†æ¶ç»“åˆäº†åµŒå…¥æ‰°åŠ¨å’Œè´å¶æ–¯ä¼˜åŒ–æ¥å®ç°æ¢ç´¢ä¸åˆ©ç”¨çš„å¹³è¡¡ã€‚</li>
<li>Soft Reasoningæé«˜äº†æ¨ç†çš„å‡†ç¡®æ€§å’Œè¿è´¯æ€§ï¼Œå¹¶é¿å…äº†å¯å‘å¼æœç´¢çš„ä¾èµ–ã€‚</li>
<li>å®éªŒç»“æœæ˜¾ç¤ºSoft Reasoningåœ¨å‡å°‘è®¡ç®—é‡çš„åŒæ—¶èƒ½æé«˜æ­£ç¡®æ€§ã€‚</li>
<li>Soft Reasoningæ˜¯ä¸€ç§å¯æ‰©å±•çš„æ¨¡å‹æ— å…³è§£å†³æ–¹æ¡ˆã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.24688">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-430df92858fe6bb56c66a01ffa6e4c7e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0a586f648fbc70a5d9bd14e4f8936c6b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-93a9cbc0df0a550af714909d034dd57e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-df09bf3de0a8a1e8faf6a32e501648ba.jpg" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="Building-Instruction-Tuning-Datasets-from-Human-Written-Instructions-with-Open-Weight-Large-Language-Models"><a href="#Building-Instruction-Tuning-Datasets-from-Human-Written-Instructions-with-Open-Weight-Large-Language-Models" class="headerlink" title="Building Instruction-Tuning Datasets from Human-Written Instructions   with Open-Weight Large Language Models"></a>Building Instruction-Tuning Datasets from Human-Written Instructions   with Open-Weight Large Language Models</h2><p><strong>Authors:Youmi Ma, Sakae Mizuki, Kazuki Fujii, Taishi Nakamura, Masanari Ohi, Hinari Shimada, Taihei Shiotani, Koshiro Saito, Koki Maeda, Kakeru Hattori, Takumi Okamoto, Shigeki Ishida, Rio Yokota, Hiroya Takamura, Naoaki Okazaki</strong></p>
<p>Instruction tuning is crucial for enabling Large Language Models (LLMs) to solve real-world tasks. Prior work has shown the effectiveness of instruction-tuning data synthesized solely from LLMs, raising a fundamental question: Do we still need human-originated signals for instruction tuning? This work answers the question affirmatively: we build state-of-the-art instruction-tuning datasets sourced from human-written instructions, by simply pairing them with LLM-generated responses. LLMs fine-tuned on our datasets consistently outperform those fine-tuned on existing ones. Our data construction approach can be easily adapted to other languages; we build datasets for Japanese and confirm that LLMs tuned with our data reach state-of-the-art performance. Analyses suggest that instruction-tuning in a new language allows LLMs to follow instructions, while the tuned models exhibit a notable lack of culture-specific knowledge in that language. The datasets and fine-tuned models will be publicly available. Our datasets, synthesized with open-weight LLMs, are openly distributed under permissive licenses, allowing for diverse use cases. </p>
<blockquote>
<p>æŒ‡ä»¤å¾®è°ƒå¯¹äºä½¿å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è§£å†³ç°å®ä¸–ç•Œä»»åŠ¡è‡³å…³é‡è¦ã€‚å…ˆå‰çš„å·¥ä½œå·²ç»è¯æ˜äº†ä»…é€šè¿‡LLMåˆæˆçš„æŒ‡ä»¤å¾®è°ƒæ•°æ®çš„æœ‰æ•ˆæ€§ï¼Œè¿™å¼•å‘äº†ä¸€ä¸ªæ ¹æœ¬é—®é¢˜ï¼šæˆ‘ä»¬æ˜¯å¦ä»ç„¶éœ€è¦äººç±»äº§ç”Ÿçš„ä¿¡å·æ¥è¿›è¡ŒæŒ‡ä»¤å¾®è°ƒï¼Ÿè¿™é¡¹å·¥ä½œè‚¯å®šåœ°å›ç­”äº†è¿™ä¸ªé—®é¢˜ï¼šæˆ‘ä»¬é€šè¿‡å°†äººç±»ç¼–å†™çš„æŒ‡ä»¤ä¸LLMç”Ÿæˆçš„å“åº”ç®€å•é…å¯¹ï¼Œæ„å»ºäº†æœ€å…ˆè¿›çš„æŒ‡ä»¤å¾®è°ƒæ•°æ®é›†ã€‚åœ¨æˆ‘ä»¬æ•°æ®é›†ä¸Šå¾®è°ƒçš„LLMå§‹ç»ˆä¼˜äºåœ¨ç°æœ‰æ•°æ®é›†ä¸Šå¾®è°ƒçš„LLMã€‚æˆ‘ä»¬çš„æ•°æ®æ„å»ºæ–¹æ³•å¯ä»¥è½»æ¾åœ°é€‚åº”å…¶ä»–è¯­è¨€ï¼›æˆ‘ä»¬ä¸ºæ—¥è¯­æ„å»ºäº†æ•°æ®é›†ï¼Œå¹¶ç¡®è®¤ä½¿ç”¨æˆ‘ä»¬çš„æ•°æ®è°ƒæ ¡çš„LLMè¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚åˆ†æè¡¨æ˜ï¼Œåœ¨æ–°è¯­è¨€ä¸­è¿›è¡ŒæŒ‡ä»¤å¾®è°ƒå¯ä»¥ä½¿LLMéµå¾ªæŒ‡ä»¤ï¼Œè€Œç»è¿‡è°ƒæ ¡çš„æ¨¡å‹åœ¨è¯¥è¯­è¨€çš„ç‰¹å®šæ–‡åŒ–çŸ¥è¯†æ–¹é¢è¡¨ç°å‡ºæ˜æ˜¾çš„ç¼ºä¹ã€‚æ•°æ®é›†å’Œç»è¿‡è®­ç»ƒçš„æ¨¡å‹å°†å…¬å¼€å¯ç”¨ã€‚æˆ‘ä»¬çš„æ•°æ®é›†æ˜¯ä¸å¼€æ”¾æƒé‡LLMåˆæˆçš„ï¼Œæ ¹æ®è®¸å¯åè®®å…¬å¼€å‘å¸ƒï¼Œæ”¯æŒå¤šç§ç”¨ä¾‹ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.23714v2">PDF</a> COLM 2025; Datasets are available at   <a target="_blank" rel="noopener" href="https://huggingface.co/datasets/tokyotech-llm/lmsys-chat-1m-synth">https://huggingface.co/datasets/tokyotech-llm/lmsys-chat-1m-synth</a></p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æŒ‡ä»¤å¾®è°ƒå¯¹äºè§£å†³ç°å®ä¸–ç•Œä»»åŠ¡è‡³å…³é‡è¦ã€‚æœ¬ç ”ç©¶è‚¯å®šäº†äººç±»åŸå§‹ä¿¡å·åœ¨æŒ‡ä»¤å¾®è°ƒä¸­çš„å¿…è¦æ€§ï¼Œé€šè¿‡ç®€å•åœ°å°†äººç±»ç¼–å†™çš„æŒ‡ä»¤ä¸LLMç”Ÿæˆçš„å“åº”é…å¯¹ï¼Œæ„å»ºäº†æœ€å…ˆè¿›çš„æŒ‡ä»¤å¾®è°ƒæ•°æ®é›†ã€‚åœ¨æ•°æ®é›†ä¸Šå¾®è°ƒçš„LLMæ€§èƒ½å§‹ç»ˆä¼˜äºåœ¨ç°æœ‰æ•°æ®é›†ä¸Šå¾®è°ƒçš„æ¨¡å‹ã€‚æœ¬æ•°æ®é›†çš„æ„å»ºæ–¹æ³•å¯è½»æ¾é€‚åº”å…¶ä»–è¯­è¨€ï¼Œå¹¶ä¸ºæ—¥è¯­æ„å»ºäº†æ•°æ®é›†ï¼Œç»æœ¬æ•°æ®è®­ç»ƒçš„LLMè¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚åˆ†æè¡¨æ˜ï¼Œåœ¨æ–°è¯­è¨€ä¸­çš„æŒ‡ä»¤å¾®è°ƒä½¿LLMèƒ½å¤Ÿéµå¾ªæŒ‡ä»¤ï¼Œè€Œç»è¿‡è®­ç»ƒçš„æ¨¡å‹åœ¨è¯¥è¯­è¨€çš„ç‰¹å®šæ–‡åŒ–çŸ¥è¯†æ–¹é¢å­˜åœ¨æ˜æ˜¾çš„ä¸è¶³ã€‚æ•°æ®é›†å’Œç»è¿‡è®­ç»ƒçš„æ¨¡å‹å°†å…¬å¼€å¯ç”¨ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æŒ‡ä»¤å¾®è°ƒå¯¹LLMè§£å†³ç°å®ä¸–ç•Œä»»åŠ¡è‡³å…³é‡è¦ã€‚</li>
<li>æœ¬ç ”ç©¶è‚¯å®šäº†åœ¨æŒ‡ä»¤å¾®è°ƒä¸­äººç±»åŸå§‹ä¿¡å·çš„é‡è¦æ€§ã€‚</li>
<li>é€šè¿‡é…å¯¹äººç±»ç¼–å†™çš„æŒ‡ä»¤ä¸LLMç”Ÿæˆçš„å“åº”ï¼Œæ„å»ºäº†æœ€å…ˆè¿›çš„æŒ‡ä»¤å¾®è°ƒæ•°æ®é›†ã€‚</li>
<li>åœ¨è¯¥æ•°æ®é›†ä¸Šå¾®è°ƒçš„LLMæ€§èƒ½ä¼˜äºç°æœ‰æ•°æ®é›†ä¸Šçš„æ€§èƒ½ã€‚</li>
<li>æ•°æ®é›†æ„å»ºæ–¹æ³•å¯é€‚åº”å…¶ä»–è¯­è¨€ï¼Œä¾‹å¦‚ä¸ºæ—¥è¯­æ„å»ºçš„æ•°æ®é›†ã€‚</li>
<li>åœ¨æ–°è¯­è¨€ä¸­çš„æŒ‡ä»¤å¾®è°ƒä½¿LLMèƒ½å¤Ÿéµå¾ªæŒ‡ä»¤ï¼Œä½†å­˜åœ¨å¯¹ç‰¹å®šæ–‡åŒ–çš„çŸ¥è¯†ä¸è¶³ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.23714">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-eec49a2596f7b3ab12d97addfe21d1b5.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-c2bc6c8c7f41c94b0d3ba25642418a54.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-dfd73e1fd47dac0730c7db59417a1860.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-bf6e1e06e6b754b13a148bfb5868875f.jpg" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="AutoComPose-Automatic-Generation-of-Pose-Transition-Descriptions-for-Composed-Pose-Retrieval-Using-Multimodal-LLMs"><a href="#AutoComPose-Automatic-Generation-of-Pose-Transition-Descriptions-for-Composed-Pose-Retrieval-Using-Multimodal-LLMs" class="headerlink" title="AutoComPose: Automatic Generation of Pose Transition Descriptions for   Composed Pose Retrieval Using Multimodal LLMs"></a>AutoComPose: Automatic Generation of Pose Transition Descriptions for   Composed Pose Retrieval Using Multimodal LLMs</h2><p><strong>Authors:Yi-Ting Shen, Sungmin Eum, Doheon Lee, Rohit Shete, Chiao-Yi Wang, Heesung Kwon, Shuvra S. Bhattacharyya</strong></p>
<p>Composed pose retrieval (CPR) enables users to search for human poses by specifying a reference pose and a transition description, but progress in this field is hindered by the scarcity and inconsistency of annotated pose transitions. Existing CPR datasets rely on costly human annotations or heuristic-based rule generation, both of which limit scalability and diversity. In this work, we introduce AutoComPose, the first framework that leverages multimodal large language models (MLLMs) to automatically generate rich and structured pose transition descriptions. Our method enhances annotation quality by structuring transitions into fine-grained body part movements and introducing mirrored&#x2F;swapped variations, while a cyclic consistency constraint ensures logical coherence between forward and reverse transitions. To advance CPR research, we construct and release two dedicated benchmarks, AIST-CPR and PoseFixCPR, supplementing prior datasets with enhanced attributes. Extensive experiments demonstrate that training retrieval models with AutoComPose yields superior performance over human-annotated and heuristic-based methods, significantly reducing annotation costs while improving retrieval quality. Our work pioneers the automatic annotation of pose transitions, establishing a scalable foundation for future CPR research. </p>
<blockquote>
<p>å§¿æ€æ£€ç´¢ï¼ˆCPRï¼‰å…è®¸ç”¨æˆ·é€šè¿‡æŒ‡å®šå‚è€ƒå§¿æ€å’Œè¿‡æ¸¡æè¿°æ¥æœç´¢äººä½“å§¿æ€ã€‚ç„¶è€Œï¼Œç”±äºæ ‡æ³¨çš„å§¿æ€è¿‡æ¸¡ç¼ºä¹ä¸”ä¸ä¸€è‡´ï¼Œè¯¥é¢†åŸŸçš„è¿›å±•å—åˆ°äº†é˜»ç¢ã€‚ç°æœ‰çš„CPRæ•°æ®é›†ä¾èµ–äºæ˜‚è´µçš„äººå·¥æ ‡æ³¨æˆ–åŸºäºå¯å‘å¼è§„åˆ™çš„ç”Ÿæˆï¼Œè¿™ä¸¤è€…éƒ½é™åˆ¶äº†å…¶å¯æ‰©å±•æ€§å’Œå¤šæ ·æ€§ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬å¼•å…¥äº†AutoComPoseï¼Œè¿™æ˜¯ç¬¬ä¸€ä¸ªåˆ©ç”¨å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰è‡ªåŠ¨ç”Ÿæˆä¸°å¯Œä¸”ç»“æ„åŒ–çš„å§¿æ€è¿‡æ¸¡æè¿°çš„æ¡†æ¶ã€‚æˆ‘ä»¬çš„æ–¹æ³•é€šè¿‡å°†è¿‡æ¸¡ç»“æ„åŒ–ç»†ç²’åº¦çš„èº«ä½“éƒ¨ä½è¿åŠ¨å¹¶å¼•å…¥é•œåƒ&#x2F;äº¤æ¢å˜åŒ–æ¥æé«˜æ ‡æ³¨è´¨é‡ï¼ŒåŒæ—¶å¾ªç¯ä¸€è‡´æ€§çº¦æŸç¡®ä¿äº†æ­£å‘å’Œåå‘è¿‡æ¸¡ä¹‹é—´çš„é€»è¾‘è¿è´¯æ€§ã€‚ä¸ºäº†æ¨åŠ¨CPRç ”ç©¶ï¼Œæˆ‘ä»¬æ„å»ºå¹¶å‘å¸ƒäº†ä¸¤ä¸ªä¸“ç”¨åŸºå‡†æµ‹è¯•é›†AIST-CPRå’ŒPoseFixCPRï¼Œè¡¥å……äº†ä»¥å‰çš„æ•°æ®é›†å¹¶å¢å¼ºäº†å…¶å±æ€§ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼Œä½¿ç”¨AutoComPoseè®­ç»ƒæ£€ç´¢æ¨¡å‹æ¯”äººå·¥æ ‡æ³¨å’ŒåŸºäºå¯å‘å¼çš„æ–¹æ³•å–å¾—äº†ä¼˜è¶Šçš„æ€§èƒ½ï¼Œåœ¨é™ä½æ ‡æ³¨æˆæœ¬çš„åŒæ—¶æé«˜äº†æ£€ç´¢è´¨é‡ã€‚æˆ‘ä»¬çš„å·¥ä½œå¼€åˆ›äº†å§¿æ€è¿‡æ¸¡çš„è‡ªåŠ¨æ ‡æ³¨å…ˆæ²³ï¼Œä¸ºæœªæ¥çš„CPRç ”ç©¶å»ºç«‹äº†å¯æ‰©å±•çš„åŸºç¡€ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.22884v2">PDF</a> ICCV 2025</p>
<p><strong>æ‘˜è¦</strong></p>
<p>æœ¬ç ”ç©¶ä»‹ç»äº†AutoComPoseæ¡†æ¶ï¼Œå®ƒåˆ©ç”¨å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰è‡ªåŠ¨ç”Ÿæˆä¸°å¯Œä¸”ç»“æ„åŒ–çš„å§¿åŠ¿è¿‡æ¸¡æè¿°ï¼Œè§£å†³äº†ä¼ ç»Ÿå§¿åŠ¿æ£€ç´¢ä¸­æ³¨é‡Šæ•°æ®ç¨€ç¼ºå’Œä¸ä¸€è‡´çš„é—®é¢˜ã€‚é€šè¿‡ç²¾ç»†çš„èº«ä½“éƒ¨ä½è¿åŠ¨ç»“æ„åŒ–å’Œå¼•å…¥é•œåƒ&#x2F;äº¤æ¢å˜åŒ–ï¼Œæé«˜äº†æ³¨é‡Šè´¨é‡ã€‚å¾ªç¯ä¸€è‡´æ€§çº¦æŸç¡®ä¿äº†æ­£å‘å’Œåå‘è¿‡æ¸¡ä¹‹é—´çš„é€»è¾‘è¿è´¯æ€§ã€‚åŒæ—¶æ„å»ºäº†ä¸¤ä¸ªä¸“ç”¨çš„åŸºå‡†æµ‹è¯•AIST-CPRå’ŒPoseFixCPRï¼Œè¡¥å……äº†ä»¥å‰çš„æ•°æ®é›†ä»¥å¢å¼ºå±æ€§ã€‚å®éªŒè¡¨æ˜ï¼Œä½¿ç”¨AutoComPoseè®­ç»ƒæ£€ç´¢æ¨¡å‹åœ¨æ€§èƒ½å’Œæˆæœ¬ä¸Šå‡ä¼˜äºäººå·¥æ³¨é‡Šå’ŒåŸºäºå¯å‘å¼çš„æ–¹æ³•ï¼Œä¸ºæœªæ¥çš„å§¿åŠ¿æ£€ç´¢ç ”ç©¶å»ºç«‹äº†å¯æ‰©å±•çš„åŸºç¡€ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>AutoComPoseæ¡†æ¶åˆ©ç”¨å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹è‡ªåŠ¨ç”Ÿæˆç»“æ„åŒ–å§¿åŠ¿è¿‡æ¸¡æè¿°ï¼Œè§£å†³æ³¨é‡Šæ•°æ®ç¨€ç¼ºé—®é¢˜ã€‚</li>
<li>é€šè¿‡èº«ä½“éƒ¨ä½è¿åŠ¨çš„ç»“æ„åŒ–å’Œé•œåƒ&#x2F;äº¤æ¢å˜åŒ–ï¼Œæé«˜äº†æ³¨é‡Šè´¨é‡ã€‚</li>
<li>å¾ªç¯ä¸€è‡´æ€§çº¦æŸç¡®ä¿æ­£å‘å’Œåå‘è¿‡æ¸¡çš„é€»è¾‘è¿è´¯æ€§ã€‚</li>
<li>æ„å»ºäº†ä¸¤ä¸ªåŸºå‡†æµ‹è¯•AIST-CPRå’ŒPoseFixCPRï¼Œå¢å¼ºäº†ä»¥å‰æ•°æ®é›†çš„å±æ€§ã€‚</li>
<li>ä¸äººå·¥æ³¨é‡Šå’ŒåŸºäºå¯å‘å¼çš„æ–¹æ³•ç›¸æ¯”ï¼Œä½¿ç”¨AutoComPoseè®­ç»ƒæ£€ç´¢æ¨¡å‹è¡¨ç°ä¼˜è¶Šã€‚</li>
<li>è‡ªåŠ¨æ³¨é‡Šå§¿åŠ¿è¿‡æ¸¡çš„å¼•å…¥ä¸ºæœªæ¥çš„å§¿åŠ¿æ£€ç´¢ç ”ç©¶å»ºç«‹äº†å¯æ‰©å±•çš„åŸºç¡€ã€‚</li>
<li>è¯¥æ–¹æ³•æ˜¾è‘—é™ä½äº†æ³¨é‡Šæˆæœ¬ï¼ŒåŒæ—¶æé«˜äº†æ£€ç´¢è´¨é‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.22884">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-69dcfdbb697f0577cc53ab290ed46746.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-8f8f539639d3811a7f530a269a62c522.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c0c435f4a9d7de39795f2024be42371c.jpg" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1><h2 id="ScaffoldGPT-A-Scaffold-based-GPT-Model-for-Drug-Optimization"><a href="#ScaffoldGPT-A-Scaffold-based-GPT-Model-for-Drug-Optimization" class="headerlink" title="ScaffoldGPT: A Scaffold-based GPT Model for Drug Optimization"></a>ScaffoldGPT: A Scaffold-based GPT Model for Drug Optimization</h2><p><strong>Authors:Xuefeng Liu, Songhao Jiang, Ian Foster, Jinbo Xu, Rick Stevens</strong></p>
<p>Drug optimization has become increasingly crucial in light of fast-mutating virus strains and drug-resistant cancer cells. Nevertheless, it remains challenging as it necessitates retaining the beneficial properties of the original drug while simultaneously enhancing desired attributes beyond its scope. In this work, we aim to tackle this challenge by introducing ScaffoldGPT, a novel Generative Pretrained Transformer (GPT) designed for drug optimization based on molecular scaffolds. Our work comprises three key components: (1) A three-stage drug optimization approach that integrates pretraining, finetuning, and decoding optimization. (2) A novel two-phase incremental pre-training strategy for scaffold-based drug optimization. (3) A token-level decoding optimization strategy, Top-N, that enabling controlled, reward-guided generation using the pretrained or finetuned GPT. We demonstrate via a comprehensive evaluation on COVID and cancer benchmarks that ScaffoldGPT outperforms the competing baselines in drug optimization benchmarks, while excelling in preserving original functional scaffold and enhancing desired properties. </p>
<blockquote>
<p>éšç€ç—…æ¯’æ ªçš„å¿«é€Ÿçªå˜å’Œè¯ç‰©è€è¯ç™Œç»†èƒçš„äº§ç”Ÿï¼Œè¯ç‰©ä¼˜åŒ–å˜å¾—æ„ˆå‘å…³é”®ã€‚ç„¶è€Œï¼Œç”±äºéœ€è¦åœ¨ä¿ç•™åŸè¯ç‰©æœ‰ç›Šå±æ€§çš„åŒæ—¶ï¼ŒåŒæ­¥å¢å¼ºæ‰€éœ€å±æ€§å¹¶æ‰©å±•å…¶åº”ç”¨èŒƒå›´ï¼Œè¿™ä»ç„¶æ˜¯ä¸€ä¸ªæŒ‘æˆ˜ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æ—¨åœ¨é€šè¿‡å¼•å…¥ScaffoldGPTæ¥è§£å†³è¿™ä¸€æŒ‘æˆ˜ï¼ŒScaffoldGPTæ˜¯ä¸€ç§åŸºäºåˆ†å­éª¨æ¶ç”¨äºè¯ç‰©ä¼˜åŒ–çš„æ–°å‹ç”Ÿæˆé¢„è®­ç»ƒè½¬æ¢å™¨ï¼ˆGPTï¼‰ã€‚æˆ‘ä»¬çš„å·¥ä½œåŒ…å«ä¸‰ä¸ªå…³é”®éƒ¨åˆ†ï¼šï¼ˆ1ï¼‰ä¸€ä¸ªä¸‰é˜¶æ®µçš„è¯å“ä¼˜åŒ–æ–¹æ³•ï¼Œè¯¥æ–¹æ³•ç»“åˆäº†é¢„è®­ç»ƒã€å¾®è°ƒå’Œè§£ç ä¼˜åŒ–ã€‚ï¼ˆ2ï¼‰ä¸€ç§ç”¨äºéª¨æ¶åŸºç¡€è¯ç‰©ä¼˜åŒ–çš„æ–°å‹ä¸¤é˜¶æ®µå¢é‡é¢„è®­ç»ƒç­–ç•¥ã€‚ï¼ˆ3ï¼‰ä¸€ç§Tokençº§åˆ«çš„è§£ç ä¼˜åŒ–ç­–ç•¥Top-Nï¼Œè¯¥ç­–ç•¥ä½¿ç”¨é¢„è®­ç»ƒæˆ–å¾®è°ƒåçš„GPTï¼Œé€šè¿‡æ§åˆ¶å¥–åŠ±æŒ‡å¯¼ç”Ÿæˆã€‚æˆ‘ä»¬é€šè¿‡é’ˆå¯¹COVIDå’Œç™Œç—‡åŸºå‡†æµ‹è¯•çš„ç»¼åˆè¯„ä¼°è¯æ˜ï¼ŒScaffoldGPTåœ¨è¯ç‰©ä¼˜åŒ–åŸºå‡†æµ‹è¯•ä¸­ä¼˜äºç«äº‰å¯¹æ‰‹ï¼ŒåŒæ—¶åœ¨ä¿ç•™åŸå§‹åŠŸèƒ½æ€§éª¨æ¶å’Œå¢å¼ºæ‰€éœ€å±æ€§æ–¹é¢è¡¨ç°å‡ºè‰²ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.06891v3">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†é’ˆå¯¹è¯ç‰©ä¼˜åŒ–æŒ‘æˆ˜çš„æ–°æ–¹æ³•ï¼Œå³ä½¿ç”¨åä¸ºScaffoldGPTçš„æ–°å‹ç”Ÿæˆé¢„è®­ç»ƒè½¬æ¢å™¨ï¼ˆGPTï¼‰è¿›è¡ŒåŸºäºåˆ†å­éª¨æ¶çš„è¯ç‰©ä¼˜åŒ–ã€‚è¯¥ç ”ç©¶åŒ…æ‹¬ä¸‰ä¸ªå…³é”®éƒ¨åˆ†ï¼šä¸‰é˜¶æ®µè¯ç‰©ä¼˜åŒ–æ–¹æ³•ã€ä¸¤é˜¶æ®µå¢é‡é¢„è®­ç»ƒç­–ç•¥å’Œåä¸ºTop-Nçš„ä»¤ç‰Œçº§è§£ç ä¼˜åŒ–ç­–ç•¥ã€‚å®éªŒè¯æ˜ï¼ŒScaffoldGPTåœ¨è¯ç‰©ä¼˜åŒ–åŸºå‡†æµ‹è¯•ä¸­ä¼˜äºç«äº‰å¯¹æ‰‹ï¼ŒåŒæ—¶åœ¨ä¿ç•™åŸå§‹åŠŸèƒ½éª¨æ¶å’Œå¢å¼ºæ‰€éœ€å±æ€§æ–¹é¢è¡¨ç°å‡ºè‰²ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ScaffoldGPTæ˜¯ä¸€ç§é’ˆå¯¹è¯ç‰©ä¼˜åŒ–çš„æ–°å‹ç”Ÿæˆé¢„è®­ç»ƒè½¬æ¢å™¨ï¼ˆGPTï¼‰ã€‚</li>
<li>ç ”ç©¶æå‡ºäº†ä¸€ä¸ªä¸‰é˜¶æ®µè¯ç‰©ä¼˜åŒ–æ–¹æ³•ï¼ŒåŒ…æ‹¬é¢„è®­ç»ƒã€å¾®è°ƒå’Œè§£ç ä¼˜åŒ–ã€‚</li>
<li>é‡‡ç”¨äº†ä¸¤é˜¶æ®µå¢é‡é¢„è®­ç»ƒç­–ç•¥ï¼Œç”¨äºåŸºäºéª¨æ¶çš„è¯ç‰©ä¼˜åŒ–ã€‚</li>
<li>å¼•å…¥äº†Top-Nä»¤ç‰Œçº§è§£ç ä¼˜åŒ–ç­–ç•¥ï¼Œå®ç°å—æ§ã€å¥–åŠ±å¼•å¯¼ç”Ÿæˆã€‚</li>
<li>ScaffoldGPTåœ¨COVIDå’Œç™Œç—‡åŸºå‡†æµ‹è¯•ä¸Šçš„è¡¨ç°ä¼˜äºå…¶ä»–åŸºå‡†è¯ç‰©ä¼˜åŒ–æ–¹æ³•ã€‚</li>
<li>ScaffoldGPTèƒ½å¤Ÿåœ¨ä¿ç•™è¯ç‰©åŸå§‹åŠŸèƒ½éª¨æ¶çš„åŒæ—¶å¢å¼ºå…¶æ‰€éœ€å±æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.06891">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-ca29b526240f2112941254eb7c4bc7a9.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-c0f519ab21fb7d942477ecb8352949ab.jpg" align="middle">
</details>


<h1 id="-19"><a href="#-19" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-08-21/LLM/">https://kedreamix.github.io/Talk2Paper/Paper/2025-08-21/LLM/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/LLM/">
                                    <span class="chip bg-color">LLM</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-08-21/Agent/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-09044d5bf6f2fd548f1712059536c58c.jpg" class="responsive-img" alt="Agent">
                        
                        <span class="card-title">Agent</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Agent æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-08-21  ComputerRL Scaling End-to-End Online Reinforcement Learning for   Computer Use Agents
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-08-21
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Agent/" class="post-category">
                                    Agent
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Agent/">
                        <span class="chip bg-color">Agent</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-08-21/R1_Reasoning/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-0ceb93b5a52d5b1eea48b1129d771e6f.jpg" class="responsive-img" alt="R1_Reasoning">
                        
                        <span class="card-title">R1_Reasoning</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            R1_Reasoning æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-08-21  Embodied-R1 Reinforced Embodied Reasoning for General Robotic   Manipulation
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-08-21
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/R1-Reasoning/" class="post-category">
                                    R1_Reasoning
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/R1-Reasoning/">
                        <span class="chip bg-color">R1_Reasoning</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">29058.5k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
