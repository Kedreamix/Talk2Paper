<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="R1_Reasoning">
    <meta name="description" content="R1_Reasoning æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-08-21  Embodied-R1 Reinforced Embodied Reasoning for General Robotic   Manipulation">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>R1_Reasoning | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-0ceb93b5a52d5b1eea48b1129d771e6f.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">R1_Reasoning</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/R1-Reasoning/">
                                <span class="chip bg-color">R1_Reasoning</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/R1-Reasoning/" class="post-category">
                                R1_Reasoning
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-08-21
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-08-21
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    21.1k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    86 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-08-21-æ›´æ–°"><a href="#2025-08-21-æ›´æ–°" class="headerlink" title="2025-08-21 æ›´æ–°"></a>2025-08-21 æ›´æ–°</h1><h2 id="Embodied-R1-Reinforced-Embodied-Reasoning-for-General-Robotic-Manipulation"><a href="#Embodied-R1-Reinforced-Embodied-Reasoning-for-General-Robotic-Manipulation" class="headerlink" title="Embodied-R1: Reinforced Embodied Reasoning for General Robotic   Manipulation"></a>Embodied-R1: Reinforced Embodied Reasoning for General Robotic   Manipulation</h2><p><strong>Authors:Yifu Yuan, Haiqin Cui, Yaoting Huang, Yibin Chen, Fei Ni, Zibin Dong, Pengyi Li, Yan Zheng, Jianye Hao</strong></p>
<p>Generalization in embodied AI is hindered by the â€œseeing-to-doing gap,â€ which stems from data scarcity and embodiment heterogeneity. To address this, we pioneer â€œpointingâ€ as a unified, embodiment-agnostic intermediate representation, defining four core embodied pointing abilities that bridge high-level vision-language comprehension with low-level action primitives. We introduce Embodied-R1, a 3B Vision-Language Model (VLM) specifically designed for embodied reasoning and pointing. We use a wide range of embodied and general visual reasoning datasets as sources to construct a large-scale dataset, Embodied-Points-200K, which supports key embodied pointing capabilities. We then train Embodied-R1 using a two-stage Reinforced Fine-tuning (RFT) curriculum with a specialized multi-task reward design. Embodied-R1 achieves state-of-the-art performance on 11 embodied spatial and pointing benchmarks. Critically, it demonstrates robust zero-shot generalization by achieving a 56.2% success rate in the SIMPLEREnv and 87.5% across 8 real-world XArm tasks without any task-specific fine-tuning, representing a 62% improvement over strong baselines. Furthermore, the model exhibits high robustness against diverse visual disturbances. Our work shows that a pointing-centric representation, combined with an RFT training paradigm, offers an effective and generalizable pathway to closing the perception-action gap in robotics. </p>
<blockquote>
<p>æ³›åœ¨äººå·¥æ™ºèƒ½ä¸­çš„æ³›åŒ–èƒ½åŠ›å—åˆ°â€œè§†è§‰åˆ°åŠ¨ä½œçš„å·®è·â€çš„åˆ¶çº¦ï¼Œå…¶æ ¹æºåœ¨äºæ•°æ®ç¨€ç¼ºå’Œä½“ç°å¼‚è´¨æ€§ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬é¦–åˆ›äº†ä½œä¸ºç»Ÿä¸€ã€ä¸å—ä½“ç°é™åˆ¶çš„ä¸­é—´è¡¨ç¤ºçš„â€œæŒ‡å‘â€ï¼Œå®šä¹‰äº†å››ç§æ ¸å¿ƒä½“ç°æŒ‡å‘èƒ½åŠ›ï¼Œè¿™äº›èƒ½åŠ›èƒ½å¤Ÿæ¡¥æ¥é«˜çº§è§†è§‰è¯­è¨€ç†è§£ä¸ä½çº§åŠ¨ä½œåŸå§‹å½¢æ€ã€‚æˆ‘ä»¬å¼•å…¥äº†ä¸“é—¨ä¸ºä½“ç°æ¨ç†å’ŒæŒ‡å‘è€Œè®¾è®¡çš„Embodied-R1ï¼Œè¿™æ˜¯ä¸€ä¸ª3Bçš„è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰ã€‚æˆ‘ä»¬ä½¿ç”¨å„ç§ä½“ç°å’Œé€šç”¨è§†è§‰æ¨ç†æ•°æ®é›†ä½œä¸ºæ¥æºæ¥æ„å»ºå¤§è§„æ¨¡æ•°æ®é›†Embodied-Points-200Kï¼Œè¯¥æ•°æ®é›†æ”¯æŒå…³é”®çš„ä½“ç°æŒ‡å‘èƒ½åŠ›ã€‚ç„¶åï¼Œæˆ‘ä»¬ä½¿ç”¨ä¸¤é˜¶æ®µå¼ºåŒ–å¾®è°ƒï¼ˆRFTï¼‰è¯¾ç¨‹å¹¶ä½¿ç”¨ç‰¹æ®Šçš„å¤šä»»åŠ¡å¥–åŠ±è®¾è®¡æ¥è®­ç»ƒEmbodied-R1ã€‚Embodied-R1åœ¨11ä¸ªä½“ç°ç©ºé—´å’ŒæŒ‡å‘åŸºå‡†æµ‹è¯•ä¸Šè¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚å…³é”®çš„æ˜¯ï¼Œå®ƒåœ¨SIMPLEREnvä¸­å®ç°äº†56.2%çš„æˆåŠŸç‡ï¼Œå¹¶åœ¨ä¸è¿›è¡Œä»»ä½•ç‰¹å®šä»»åŠ¡å¾®è°ƒçš„æƒ…å†µä¸‹åœ¨8ä¸ªçœŸå®ä¸–ç•Œçš„XArmä»»åŠ¡ä¸­è¾¾åˆ°äº†87.5%çš„æˆåŠŸç‡ï¼Œè¿™ä»£è¡¨äº†ç›¸æ¯”å¼ºå¤§çš„åŸºå‡†çº¿æœ‰62%çš„æ”¹è¿›ã€‚æ­¤å¤–ï¼Œè¯¥æ¨¡å‹å¯¹å„ç§å„æ ·çš„è§†è§‰å¹²æ‰°è¡¨ç°å‡ºäº†å¾ˆé«˜çš„ç¨³å¥æ€§ã€‚æˆ‘ä»¬çš„å·¥ä½œè¡¨æ˜ï¼Œä»¥æŒ‡å‘ä¸ºä¸­å¿ƒçš„è¡¨ç¤ºç»“åˆRFTè®­ç»ƒèŒƒå¼ï¼Œä¸ºç¼©å°æœºå™¨äººæ„ŸçŸ¥-åŠ¨ä½œå·®è·æä¾›äº†ä¸€ç§æœ‰æ•ˆä¸”å¯æ¨å¹¿çš„é€”å¾„ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.13998v1">PDF</a> Embodied-R1 technical report</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ¢è®¨äº†æœ‰å½¢ä½“AIä¸­çš„æ³›åŒ–é—®é¢˜ï¼Œä¸»è¦ç”±äºå­˜åœ¨â€œçœ‹åˆ°å³ä¼šæ“ä½œâ€çš„é¸¿æ²Ÿã€æ•°æ®ç¨€ç¼ºå’Œå®ä½“å½¢æ€å¤šæ ·æ€§ç­‰é—®é¢˜ã€‚ä¸ºè§£å†³è¿™äº›é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†ä»¥â€œæŒ‡å‘â€ä¸ºä¸­å¿ƒçš„ç»Ÿä¸€ã€å½¢æ€æ— å…³çš„è¡¨å¾æ–¹å¼ï¼Œå¹¶å®šä¹‰äº†å››ç§æ ¸å¿ƒçš„æœ‰å½¢ä½“æŒ‡å‘èƒ½åŠ›ï¼Œä»¥æ¡¥æ¥é«˜çº§è§†è§‰è¯­è¨€ç†è§£ä¸ä½çº§åŠ¨ä½œåŸå§‹å½¢æ€ã€‚åŒæ—¶å¼•å…¥äº†ä¸“é—¨ç”¨äºæœ‰å½¢ä½“æ¨ç†çš„Embodied-R1æ¨¡å‹ï¼Œé€šè¿‡å¤§è§„æ¨¡æ•°æ®é›†Embodied-Points-200Kçš„è®­ç»ƒï¼Œé‡‡ç”¨ä¸¤é˜¶æ®µå¼ºåŒ–å¾®è°ƒï¼ˆRFTï¼‰è¯¾ç¨‹ï¼Œå®ç°æœ‰å½¢ä½“ç©ºé—´ä»»åŠ¡å’ŒæŒ‡å‘ä»»åŠ¡çš„æ ‡æ†æ€§èƒ½ã€‚è¯¥æ¨¡å‹å±•ç°å‡ºå¼ºå¤§çš„é›¶æ ·æœ¬æ³›åŒ–èƒ½åŠ›ï¼Œåœ¨æœªè¿›è¡Œç‰¹å®šä»»åŠ¡å¾®è°ƒçš„æƒ…å†µä¸‹ï¼Œåœ¨SIMPLEREnvä»»åŠ¡ä¸­è¾¾åˆ°56.2%çš„æˆåŠŸç‡ï¼Œå¹¶åœ¨XArmä»»åŠ¡çš„8ä¸ªçœŸå®ä¸–ç•Œåœºæ™¯ä¸­è¾¾åˆ°87.5%çš„æˆåŠŸç‡ï¼Œç›¸è¾ƒäºå¼ºå¤§çš„åŸºçº¿æ¨¡å‹æœ‰ç€62%çš„æå‡ã€‚æ­¤å¤–ï¼Œè¯¥æ¨¡å‹å¯¹äºå„ç§è§†è§‰å¹²æ‰°ä¹Ÿè¡¨ç°å‡ºé«˜åº¦çš„ç¨³å¥æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æœ‰å½¢ä½“AIçš„æ³›åŒ–å—åˆ°â€œçœ‹åˆ°å³ä¼šæ“ä½œâ€é¸¿æ²Ÿã€æ•°æ®ç¨€ç¼ºå’Œå®ä½“å½¢æ€å¤šæ ·æ€§çš„å½±å“ã€‚</li>
<li>æå‡ºä»¥â€œæŒ‡å‘â€ä¸ºä¸­å¿ƒçš„ç»Ÿä¸€ã€å½¢æ€æ— å…³çš„è¡¨å¾æ–¹å¼ï¼Œæ¡¥æ¥é«˜çº§è§†è§‰è¯­è¨€ç†è§£ä¸ä½çº§åŠ¨ä½œåŸå§‹å½¢æ€ã€‚</li>
<li>ä»‹ç»äº†Embodied-R1æ¨¡å‹ï¼Œè¯¥æ¨¡å‹ä¸“é—¨ç”¨äºæœ‰å½¢ä½“æ¨ç†ã€‚</li>
<li>é€šè¿‡å¤§è§„æ¨¡æ•°æ®é›†Embodied-Points-200Kè®­ç»ƒEmbodied-R1æ¨¡å‹ã€‚</li>
<li>é‡‡ç”¨ä¸¤é˜¶æ®µå¼ºåŒ–å¾®è°ƒï¼ˆRFTï¼‰è¯¾ç¨‹ï¼Œå®ç°æœ‰å½¢ä½“ç©ºé—´ä»»åŠ¡å’ŒæŒ‡å‘ä»»åŠ¡çš„æ ‡æ†æ€§èƒ½ã€‚</li>
<li>Embodied-R1æ¨¡å‹å±•ç°å‡ºå¼ºå¤§çš„é›¶æ ·æœ¬æ³›åŒ–èƒ½åŠ›ï¼Œå…·æœ‰è¾ƒé«˜çš„æˆåŠŸç‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.13998">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-eaa29b31ae1bc4538fc9490d05895914.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-06305a09fefb8051aba9eafbff02fd13.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a5732da875a0a750ae1144394c53a6d7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-33b1fefb1678a56aa4aeccd18ecddd26.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Chunks-as-Arms-Multi-Armed-Bandit-Guided-Sampling-for-Long-Context-LLM-Preference-Optimization"><a href="#Chunks-as-Arms-Multi-Armed-Bandit-Guided-Sampling-for-Long-Context-LLM-Preference-Optimization" class="headerlink" title="Chunks as Arms: Multi-Armed Bandit-Guided Sampling for Long-Context LLM   Preference Optimization"></a>Chunks as Arms: Multi-Armed Bandit-Guided Sampling for Long-Context LLM   Preference Optimization</h2><p><strong>Authors:Shaohua Duan, Xinze Li, Zhenghao Liu, Xiaoyuan Yi, Yukun Yan, Shuo Wang, Yu Gu, Ge Yu, Maosong Sun</strong></p>
<p>Long-context modeling is critical for a wide range of real-world tasks, including long-context question answering, summarization, and complex reasoning tasks. Recent studies have explored fine-tuning Large Language Models (LLMs) with synthetic data to enhance their long-context capabilities. However, the effectiveness of such approaches is often limited by the low diversity and factual inconsistencies in the generated data. To address these challenges, we propose LongMab-PO, a novel framework that leverages a Multi-Armed Bandit (MAB) rollout strategy to identify the most informative chunks from the given long context for sampling high-quality and diverse responses and constructing preference data pairs for Direct Preference Optimization (DPO) training. Specifically, we treat context chunks as arms of MAB, select chunks based on their expected reward scores to input into LLMs to generate responses, and iteratively update these scores based on reward feedback. This exploration and exploitation process enables the model to focus on the most relevant context segments, thereby generating and collecting high-quality and diverse responses. Finally, we collect these generated responses from the rollout process and apply the DPO method to further optimize the LLM. Experimental results show that LongMab-PO significantly improves the diversity and quality of preference data pairs, achieving state-of-the-art performance on long-context reasoning benchmarks. All code and data will be released on <a target="_blank" rel="noopener" href="https://github.com/NEUIR/LongMab-PO">https://github.com/NEUIR/LongMab-PO</a>. </p>
<blockquote>
<p>é•¿æ–‡æœ¬å»ºæ¨¡å¯¹äºä¸€ç³»åˆ—ç°å®ä¸–ç•Œä»»åŠ¡è‡³å…³é‡è¦ï¼ŒåŒ…æ‹¬é•¿æ–‡æœ¬é—®ç­”ã€æ‘˜è¦å’Œå¤æ‚æ¨ç†ä»»åŠ¡ç­‰ã€‚è¿‘æœŸçš„ç ”ç©¶å°è¯•é€šè¿‡åˆæˆæ•°æ®å¾®è°ƒå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä»¥å¢å¼ºå…¶å¤„ç†é•¿æ–‡æœ¬çš„èƒ½åŠ›ã€‚ç„¶è€Œï¼Œæ­¤ç±»æ–¹æ³•çš„æ•ˆç”¨å¾€å¾€å—é™äºç”Ÿæˆæ•°æ®çš„å¤šæ ·æ€§ç¼ºä¹å’Œäº‹å®æ€§ä¸ä¸€è‡´ã€‚ä¸ºäº†åº”å¯¹è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†LongMab-POè¿™ä¸€æ–°å‹æ¡†æ¶ï¼Œå®ƒè¿ç”¨å¤šè‡‚è€è™æœºï¼ˆMABï¼‰æ»šåŠ¨ç­–ç•¥æ¥è¯†åˆ«ç»™å®šé•¿æ–‡æœ¬ä¸­æœ€å…·ä¿¡æ¯é‡çš„éƒ¨åˆ†ï¼Œä»¥é‡‡æ ·é«˜è´¨é‡ã€å¤šæ ·åŒ–çš„ç­”æ¡ˆï¼Œå¹¶æ„å»ºåå¥½æ•°æ®å¯¹ç”¨äºç›´æ¥åå¥½ä¼˜åŒ–ï¼ˆDPOï¼‰è®­ç»ƒã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬å°†æ–‡æœ¬ç‰‡æ®µè§†ä¸ºè€è™æœºçš„è‡‚è†€ï¼Œæ ¹æ®é¢„æœŸçš„å¥–åŠ±åˆ†æ•°é€‰æ‹©ç‰‡æ®µè¾“å…¥LLMç”Ÿæˆç­”æ¡ˆï¼Œå¹¶æ ¹æ®åé¦ˆå¥–åŠ±è¿­ä»£æ›´æ–°è¿™äº›åˆ†æ•°ã€‚è¿™ç§æ¢ç´¢ä¸åˆ©ç”¨çš„è¿‡ç¨‹ä½¿æ¨¡å‹èƒ½å¤Ÿä¸“æ³¨äºæœ€ç›¸å…³çš„æ–‡æœ¬ç‰‡æ®µï¼Œä»è€Œç”Ÿæˆå’Œæ”¶é›†é«˜è´¨é‡ã€å¤šæ ·åŒ–çš„ç­”æ¡ˆã€‚æœ€åï¼Œæˆ‘ä»¬æ”¶é›†è¿™äº›ç”Ÿæˆç­”æ¡ˆå¹¶åº”ç”¨DPOæ–¹æ³•æ¥è¿›ä¸€æ­¥ä¼˜åŒ–LLMã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒLongMab-POæ˜¾è‘—æé«˜äº†åå¥½æ•°æ®å¯¹çš„å¤šæ ·æ€§å’Œè´¨é‡ï¼Œåœ¨é•¿æ–‡æœ¬æ¨ç†åŸºå‡†æµ‹è¯•ä¸­è¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚æ‰€æœ‰ä»£ç å’Œæ•°æ®å°†åœ¨<a target="_blank" rel="noopener" href="https://github.com/NEUIR/LongMab-PO%E4%B8%8A%E5%8F%91%E5%B8%83%E3%80%82">https://github.com/NEUIR/LongMab-POä¸Šå‘å¸ƒã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.13993v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>é•¿è¯­å¢ƒå»ºæ¨¡å¯¹äºåŒ…æ‹¬é•¿è¯­å¢ƒé—®ç­”ã€æ‘˜è¦å’Œå¤æ‚æ¨ç†ä»»åŠ¡åœ¨å†…çš„å¤šç§å®é™…ä»»åŠ¡è‡³å…³é‡è¦ã€‚è¿‘æœŸç ”ç©¶å°è¯•é€šè¿‡åˆæˆæ•°æ®å¾®è°ƒå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä»¥å¢å¼ºå…¶é•¿è¯­å¢ƒèƒ½åŠ›ï¼Œä½†è¿™ç§æ–¹æ³•çš„æ•ˆæœå—é™äºç”Ÿæˆæ•°æ®çš„ä½å¤šæ ·æ€§å’Œäº‹å®æ€§ä¸ä¸€è‡´ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºLongMab-POæ¡†æ¶ï¼Œåˆ©ç”¨å¤šè‡‚è€è™æœºï¼ˆMABï¼‰ç­–ç•¥è¯†åˆ«æœ€æœ‰ä¿¡æ¯é‡çš„é•¿è¯­å¢ƒç‰‡æ®µï¼Œä»¥é‡‡æ ·é«˜è´¨é‡ã€å¤šæ ·åŒ–çš„å“åº”ï¼Œå¹¶æ„å»ºåå¥½æ•°æ®å¯¹è¿›è¡Œç›´æ¥åå¥½ä¼˜åŒ–ï¼ˆDPOï¼‰è®­ç»ƒã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒLongMab-POèƒ½æ˜¾è‘—æé«˜åå¥½æ•°æ®å¯¹çš„å¤šæ ·æ€§å’Œè´¨é‡ï¼Œåœ¨é•¿çŸ­è¯­å¢ƒæ¨ç†åŸºå‡†æµ‹è¯•ä¸­è¾¾åˆ°æœ€ä½³æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>é•¿è¯­å¢ƒå»ºæ¨¡å¯¹äºå¤šç§å®é™…ä»»åŠ¡è‡³å…³é‡è¦ã€‚</li>
<li>è¿‘æœŸç ”ç©¶é€šè¿‡åˆæˆæ•°æ®å¾®è°ƒå¤§å‹è¯­è¨€æ¨¡å‹ä»¥å¢å¼ºå…¶é•¿è¯­å¢ƒèƒ½åŠ›ï¼Œä½†å­˜åœ¨ç”Ÿæˆæ•°æ®å¤šæ ·æ€§å’Œäº‹å®æ€§ä¸ä¸€è‡´çš„é—®é¢˜ã€‚</li>
<li>æå‡ºLongMab-POæ¡†æ¶ï¼Œåˆ©ç”¨å¤šè‡‚è€è™æœºç­–ç•¥è¯†åˆ«æœ€æœ‰ä¿¡æ¯é‡çš„é•¿è¯­å¢ƒç‰‡æ®µã€‚</li>
<li>é€šè¿‡é‡‡æ ·é«˜è´¨é‡ã€å¤šæ ·åŒ–çš„å“åº”ï¼Œæ„å»ºåå¥½æ•°æ®å¯¹è¿›è¡Œç›´æ¥åå¥½ä¼˜åŒ–è®­ç»ƒã€‚</li>
<li>LongMab-POèƒ½æ˜¾è‘—æé«˜åå¥½æ•°æ®å¯¹çš„å¤šæ ·æ€§å’Œè´¨é‡ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.13993">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-abaefd5f7d9c6cb9f01523e48cd91b17.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-2196a2df82d4b65ad947d5d1b674620d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a056dd40246c173f2f35e909473a12a2.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-c7bb8ffab06e614adf985e034f0b2b40.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b6024522db99c87b89cb80e342186a65.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="MME-SCI-A-Comprehensive-and-Challenging-Science-Benchmark-for-Multimodal-Large-Language-Models"><a href="#MME-SCI-A-Comprehensive-and-Challenging-Science-Benchmark-for-Multimodal-Large-Language-Models" class="headerlink" title="MME-SCI: A Comprehensive and Challenging Science Benchmark for   Multimodal Large Language Models"></a>MME-SCI: A Comprehensive and Challenging Science Benchmark for   Multimodal Large Language Models</h2><p><strong>Authors:Jiacheng Ruan, Dan Jiang, Xian Gao, Ting Liu, Yuzhuo Fu, Yangyang Kang</strong></p>
<p>Recently, multimodal large language models (MLLMs) have achieved significant advancements across various domains, and corresponding evaluation benchmarks have been continuously refined and improved. In this process, benchmarks in the scientific domain have played an important role in assessing the reasoning capabilities of MLLMs. However, existing benchmarks still face three key challenges: 1) Insufficient evaluation of modelsâ€™ reasoning abilities in multilingual scenarios; 2) Inadequate assessment of MLLMsâ€™ comprehensive modality coverage; 3) Lack of fine-grained annotation of scientific knowledge points. To address these gaps, we propose MME-SCI, a comprehensive and challenging benchmark. We carefully collected 1,019 high-quality question-answer pairs, which involve 3 distinct evaluation modes. These pairs cover four subjects, namely mathematics, physics, chemistry, and biology, and support five languages: Chinese, English, French, Spanish, and Japanese. We conducted extensive experiments on 16 open-source models and 4 closed-source models, and the results demonstrate that MME-SCI is widely challenging for existing MLLMs. For instance, under the Image-only evaluation mode, o4-mini achieved accuracy of only 52.11%, 24.73%, 36.57%, and 29.80% in mathematics, physics, chemistry, and biology, respectively, indicating a significantly higher difficulty level compared to existing benchmarks. More importantly, using MME-SCIâ€™s multilingual and fine-grained knowledge attributes, we analyzed existing modelsâ€™ performance in depth and identified their weaknesses in specific domains. The Data and Evaluation Code are available at <a target="_blank" rel="noopener" href="https://github.com/JCruan519/MME-SCI">https://github.com/JCruan519/MME-SCI</a>. </p>
<blockquote>
<p>æœ€è¿‘ï¼Œå¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨å„ä¸ªé¢†åŸŸéƒ½å–å¾—äº†æ˜¾è‘—çš„è¿›å±•ï¼Œç›¸åº”çš„è¯„ä¼°åŸºå‡†ä¹Ÿåœ¨æŒç»­å®Œå–„å’Œæ”¹è¿›ã€‚åœ¨è¿™ä¸€è¿‡ç¨‹ä¸­ï¼Œç§‘å­¦é¢†åŸŸçš„åŸºå‡†è¯„ä¼°åœ¨è¯„ä¼°MLLMsçš„æ¨ç†èƒ½åŠ›æ–¹é¢å‘æŒ¥äº†é‡è¦ä½œç”¨ã€‚ç„¶è€Œï¼Œç°æœ‰åŸºå‡†ä»é¢ä¸´ä¸‰ä¸ªå…³é”®æŒ‘æˆ˜ï¼š1ï¼‰åœ¨å¤šè¯­ç§åœºæ™¯ä¸‹å¯¹æ¨¡å‹çš„æ¨ç†èƒ½åŠ›è¯„ä¼°ä¸è¶³ï¼›2ï¼‰å¯¹MLLMsçš„ç»¼åˆæ¨¡æ€è¦†ç›–è¯„ä¼°ä¸è¶³ï¼›3ï¼‰ç§‘å­¦çŸ¥è¯†ç‚¹çš„ç²¾ç»†æ ‡æ³¨ç¼ºä¹ã€‚ä¸ºäº†è§£å†³è¿™äº›ç©ºç™½ï¼Œæˆ‘ä»¬æå‡ºäº†MME-SCIè¿™ä¸€å…¨é¢ä¸”å…·æœ‰æŒ‘æˆ˜æ€§çš„åŸºå‡†ã€‚æˆ‘ä»¬ç²¾å¿ƒæ”¶é›†äº†1019ç»„é«˜è´¨é‡çš„é—®é¢˜ç­”æ¡ˆå¯¹ï¼Œæ¶‰åŠ3ç§ç‹¬ç‰¹çš„è¯„ä¼°æ¨¡å¼ã€‚è¿™äº›å¯¹æ¶µç›–äº†æ•°å­¦ã€ç‰©ç†ã€åŒ–å­¦å’Œç”Ÿç‰©å››é—¨å­¦ç§‘ï¼Œæ”¯æŒäº”ç§è¯­è¨€ï¼šä¸­æ–‡ã€è‹±æ–‡ã€æ³•è¯­ã€è¥¿ç­ç‰™è¯­å’Œæ—¥è¯­ã€‚æˆ‘ä»¬å¯¹16ä¸ªå¼€æºæ¨¡å‹å’Œ4ä¸ªé—­æºæ¨¡å‹è¿›è¡Œäº†å¹¿æ³›å®éªŒï¼Œç»“æœè¡¨æ˜MME-SCIå¯¹ç°æœ‰MLLMså…·æœ‰å¹¿æ³›æŒ‘æˆ˜æ€§ã€‚ä¾‹å¦‚ï¼Œåœ¨ä»…å›¾åƒè¯„ä¼°æ¨¡å¼ä¸‹ï¼Œo4-miniåœ¨æ•°å­¦ã€ç‰©ç†ã€åŒ–å­¦å’Œç”Ÿç‰©æ–¹é¢çš„å‡†ç¡®ç‡åˆ†åˆ«ä¸º52.11%ã€24.73%ã€36.57%å’Œ29.80%ï¼Œè¡¨æ˜å…¶éš¾åº¦æ°´å¹³æ˜¾è‘—é«˜äºç°æœ‰åŸºå‡†ã€‚æ›´é‡è¦çš„æ˜¯ï¼Œä½¿ç”¨MME-SCIçš„å¤šè¯­ç§å’Œç²¾ç»†çŸ¥è¯†ç‚¹å±æ€§ï¼Œæˆ‘ä»¬æ·±å…¥åˆ†æäº†ç°æœ‰æ¨¡å‹çš„æ€§èƒ½ï¼Œå¹¶è¯†åˆ«äº†å®ƒä»¬åœ¨ç‰¹å®šé¢†åŸŸçš„å¼±ç‚¹ã€‚æ•°æ®å’Œè¯„ä¼°ä»£ç å¯é€šè¿‡<a target="_blank" rel="noopener" href="https://github.com/JCruan519/MME-SCI">https://github.com/JCruan519/MME-SCI</a>è·å–ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.13938v1">PDF</a> 9 pages, 6 figures, work in progress</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨ç§‘ç ”é¢†åŸŸçš„è¯„ä¼°åŸºå‡†æ‰€é¢ä¸´çš„æŒ‘æˆ˜ï¼Œå¹¶æå‡ºäº†MME-SCIè¿™ä¸€å…¨é¢ä¸”å…·æœ‰æŒ‘æˆ˜æ€§çš„æ–°åŸºå‡†ã€‚è¯¥åŸºå‡†æ”¶é›†äº†1019ç»„é«˜è´¨é‡çš„é—®é¢˜ç­”æ¡ˆå¯¹ï¼Œæ¶‰åŠæ•°å­¦ã€ç‰©ç†ã€åŒ–å­¦å’Œç”Ÿç‰©å››ä¸ªå­¦ç§‘ï¼Œæ”¯æŒäº”ç§è¯­è¨€ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œç°æœ‰æ¨¡å‹åœ¨MME-SCIåŸºå‡†ä¸‹è¡¨ç°æŒ‘æˆ˜ï¼Œä¸”é€šè¿‡è¯¥åŸºå‡†æ·±å…¥åˆ†æäº†æ¨¡å‹çš„æ€§èƒ½å¹¶è¯†åˆ«äº†å…¶å¼±ç‚¹ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨ç§‘ç ”é¢†åŸŸè¯„ä¼°åŸºå‡†ä¸Šé¢ä¸´æŒ‘æˆ˜ã€‚</li>
<li>ç°æœ‰åŸºå‡†åœ¨è¯„ä¼°æ¨¡å‹çš„è·¨è¯­è¨€æ¨ç†èƒ½åŠ›ã€å¤šæ¨¡æ€è¦†ç›–å®Œæ•´æ€§ä»¥åŠç§‘å­¦çŸ¥è¯†ç‚¹çš„ç²¾ç»†æ ‡æ³¨æ–¹é¢å­˜åœ¨ä¸è¶³ã€‚</li>
<li>MME-SCIåŸºå‡†è¢«æå‡ºä»¥è§£å†³ç°æœ‰æŒ‘æˆ˜ï¼ŒåŒ…å«1019ç»„é«˜è´¨é‡é—®é¢˜ç­”æ¡ˆå¯¹ï¼Œè¦†ç›–æ•°å­¦ã€ç‰©ç†ã€åŒ–å­¦å’Œç”Ÿç‰©å››ä¸ªå­¦ç§‘ï¼Œæ”¯æŒäº”ç§è¯­è¨€ã€‚</li>
<li>å¹¿æ³›å®éªŒè¡¨æ˜ï¼ŒMME-SCIåŸºå‡†å¯¹ç°æœ‰MLLMså…·æœ‰æŒ‘æˆ˜æ€§ã€‚</li>
<li>åœ¨å›¾åƒä»…è¯„ä»·æ¨¡å¼ä¸‹ï¼ŒæŸäº›æ¨¡å‹åœ¨æ•°å­¦ã€ç‰©ç†ã€åŒ–å­¦å’Œç”Ÿç‰©æ–¹é¢çš„å‡†ç¡®ç‡è¾ƒä½ï¼Œè¡¨æ˜éš¾åº¦è¾ƒé«˜ã€‚</li>
<li>MME-SCIåŸºå‡†æ·±å…¥åˆ†æäº†ç°æœ‰æ¨¡å‹çš„æ€§èƒ½ï¼Œå¹¶è¯†åˆ«äº†å…¶åœ¨ç‰¹å®šé¢†åŸŸçš„å¼±ç‚¹ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.13938">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-954f542828854270e05f52ea3f8c16df.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a5df2ff4fdca0007006276e5e03bfff8.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-8eda865634b9591e1efcc2ba5d0b99d4.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-1c590b33c73c75ce17a70ef9d3eb5a39.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6a1d2aa2da4c4e822c00cade6465cd3e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1095c61aac86f68e5a70ce7c155aa9a8.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Neuro-Symbolic-Artificial-Intelligence-Towards-Improving-the-Reasoning-Abilities-of-Large-Language-Models"><a href="#Neuro-Symbolic-Artificial-Intelligence-Towards-Improving-the-Reasoning-Abilities-of-Large-Language-Models" class="headerlink" title="Neuro-Symbolic Artificial Intelligence: Towards Improving the Reasoning   Abilities of Large Language Models"></a>Neuro-Symbolic Artificial Intelligence: Towards Improving the Reasoning   Abilities of Large Language Models</h2><p><strong>Authors:Xiao-Wen Yang, Jie-Jing Shao, Lan-Zhe Guo, Bo-Wen Zhang, Zhi Zhou, Lin-Han Jia, Wang-Zhou Dai, Yu-Feng Li</strong></p>
<p>Large Language Models (LLMs) have shown promising results across various tasks, yet their reasoning capabilities remain a fundamental challenge. Developing AI systems with strong reasoning capabilities is regarded as a crucial milestone in the pursuit of Artificial General Intelligence (AGI) and has garnered considerable attention from both academia and industry. Various techniques have been explored to enhance the reasoning capabilities of LLMs, with neuro-symbolic approaches being a particularly promising way. This paper comprehensively reviews recent developments in neuro-symbolic approaches for enhancing LLM reasoning. We first present a formalization of reasoning tasks and give a brief introduction to the neurosymbolic learning paradigm. Then, we discuss neuro-symbolic methods for improving the reasoning capabilities of LLMs from three perspectives: Symbolic-&gt;LLM, LLM-&gt;Symbolic, and LLM+Symbolic. Finally, we discuss several key challenges and promising future directions. We have also released a GitHub repository including papers and resources related to this survey: <a target="_blank" rel="noopener" href="https://github.com/LAMDASZ-ML/Awesome-LLM-Reasoning-with-NeSy">https://github.com/LAMDASZ-ML/Awesome-LLM-Reasoning-with-NeSy</a>. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨å„ç§ä»»åŠ¡ä¸­å±•ç°å‡ºäº†æœ‰å‰æ™¯çš„ç»“æœï¼Œä½†å®ƒä»¬çš„æ¨ç†èƒ½åŠ›ä»ç„¶æ˜¯ä¸€ä¸ªåŸºæœ¬æŒ‘æˆ˜ã€‚å¼€å‘å…·æœ‰å¼ºå¤§æ¨ç†èƒ½åŠ›çš„AIç³»ç»Ÿè¢«è®¤ä¸ºæ˜¯å®ç°é€šç”¨äººå·¥æ™ºèƒ½ï¼ˆAGIï¼‰è¿½æ±‚è¿‡ç¨‹ä¸­çš„ä¸€ä¸ªé‡è¦é‡Œç¨‹ç¢‘ï¼Œå¹¶å¼•èµ·äº†å­¦æœ¯ç•Œå’Œå·¥ä¸šç•Œçš„æå¤§å…³æ³¨ã€‚ä¸ºäº†å¢å¼ºLLMsçš„æ¨ç†èƒ½åŠ›ï¼Œå·²ç»æ¢ç´¢äº†å„ç§æŠ€æœ¯ï¼Œå…¶ä¸­ç¥ç»ç¬¦å·æ–¹æ³•æ˜¯ä¸€ç§ç‰¹åˆ«æœ‰å‰æ™¯çš„æ–¹å¼ã€‚æœ¬æ–‡å…¨é¢å›é¡¾äº†ç¥ç»ç¬¦å·æ–¹æ³•åœ¨æé«˜LLMæ¨ç†èƒ½åŠ›æ–¹é¢çš„æœ€æ–°å‘å±•ã€‚æˆ‘ä»¬é¦–å…ˆå½¢å¼åŒ–æ¨ç†ä»»åŠ¡ï¼Œå¹¶ç®€è¦ä»‹ç»ç¥ç»ç¬¦å·å­¦ä¹ èŒƒå¼ã€‚ç„¶åï¼Œæˆ‘ä»¬ä»ç¬¦å·åˆ°LLMã€LLMåˆ°ç¬¦å·å’ŒLLM+ç¬¦å·ä¸‰ä¸ªè§’åº¦ï¼Œè®¨è®ºç¥ç»ç¬¦å·æ–¹æ³•åœ¨å¢å¼ºLLMsæ¨ç†èƒ½åŠ›æ–¹é¢çš„åº”ç”¨ã€‚æœ€åï¼Œæˆ‘ä»¬è®¨è®ºäº†å‡ ä¸ªå…³é”®æŒ‘æˆ˜å’Œæœªæ¥çš„æœ‰å‰é€”çš„æ–¹å‘ã€‚æˆ‘ä»¬è¿˜å‘å¸ƒäº†ä¸€ä¸ªGitHubä»“åº“ï¼ŒåŒ…å«ä¸æ­¤è°ƒæŸ¥ç›¸å…³çš„è®ºæ–‡å’Œèµ„æºï¼š<a target="_blank" rel="noopener" href="https://github.com/LAMDASZ-ML/Awesome-LLM-Reasoning-with-NeSy%E3%80%82">https://github.com/LAMDASZ-ML/Awesome-LLM-Reasoning-with-NeSyã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.13678v1">PDF</a> 9 pages, 3 figures, IJCAI 2025 Survey Track</p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨å„é¡¹ä»»åŠ¡ä¸­å±•ç°å‡ºè‰¯å¥½æ€§èƒ½ï¼Œä½†å…¶æ¨ç†èƒ½åŠ›ä»æ˜¯æ ¸å¿ƒæŒ‘æˆ˜ã€‚å¼€å‘å…·å¤‡å¼ºå¤§æ¨ç†èƒ½åŠ›çš„AIç³»ç»Ÿæ˜¯è¿ˆå‘é€šç”¨äººå·¥æ™ºèƒ½ï¼ˆAGIï¼‰çš„é‡è¦é‡Œç¨‹ç¢‘ï¼Œå¼•èµ·å­¦æœ¯ç•Œå’Œå·¥ä¸šç•Œçš„å¹¿æ³›å…³æ³¨ã€‚ä¸ºæå‡LLMsçš„æ¨ç†èƒ½åŠ›ï¼Œç ”ç©¶è€…æ¢ç´¢äº†å¤šç§æŠ€æœ¯ï¼Œå…¶ä¸­ç¥ç»ç¬¦å·æ–¹æ³•å‰æ™¯ç‰¹åˆ«å…‰æ˜ã€‚æœ¬æ–‡å…¨é¢ç»¼è¿°äº†è¿‘æœŸç”¨äºå¢å¼ºLLMæ¨ç†èƒ½åŠ›çš„ç¥ç»ç¬¦å·æ–¹æ³•ã€‚æ–‡ç« é¦–å…ˆå½¢å¼åŒ–æ¨ç†ä»»åŠ¡ï¼Œå¹¶ç®€è¦ä»‹ç»ç¥ç»ç¬¦å·å­¦ä¹ èŒƒå¼ã€‚æ¥ç€ï¼Œä»ç¬¦å·åˆ°LLMã€LLMåˆ°ç¬¦å·å’ŒLLM+ç¬¦å·ä¸‰ä¸ªè§’åº¦ï¼Œè®¨è®ºæå‡LLMæ¨ç†èƒ½åŠ›çš„ç¥ç»ç¬¦å·æ–¹æ³•ã€‚æœ€åï¼Œæ–‡ç« è¿˜æ¢è®¨äº†å…³é”®æŒ‘æˆ˜å’Œæœªæ¥ç ”ç©¶æ–¹å‘ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„æ¨ç†èƒ½åŠ›æ˜¯æ ¸å¿ƒæŒ‘æˆ˜ã€‚</li>
<li>å¼€å‘å…·å¤‡å¼ºå¤§æ¨ç†èƒ½åŠ›çš„AIç³»ç»Ÿæ˜¯è¿ˆå‘é€šç”¨äººå·¥æ™ºèƒ½ï¼ˆAGIï¼‰çš„é‡è¦é‡Œç¨‹ç¢‘ã€‚</li>
<li>ç¥ç»ç¬¦å·æ–¹æ³•æ˜¯ä¸€ç§æå‡LLMsæ¨ç†èƒ½åŠ›çš„æœ‰å‰é€”çš„æŠ€æœ¯ã€‚</li>
<li>æ–‡ç« å½¢å¼åŒ–æ¨ç†ä»»åŠ¡ï¼Œå¹¶ä»‹ç»äº†ç¥ç»ç¬¦å·å­¦ä¹ èŒƒå¼ã€‚</li>
<li>ç¥ç»ç¬¦å·æ–¹æ³•å¯ä»¥ä»ç¬¦å·åˆ°LLMã€LLMåˆ°ç¬¦å·å’ŒLLM+ç¬¦å·ä¸‰ä¸ªè§’åº¦æå‡LLMçš„æ¨ç†èƒ½åŠ›ã€‚</li>
<li>å½“å‰è¿˜å­˜åœ¨ä¸€äº›å…³é”®æŒ‘æˆ˜éœ€è¦è§£å†³ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.13678">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-95a05feffb2214b2587e43d372f2647a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c4b84ea4c4560b52ade1253efb783316.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-84133e1fade531303c9b724e848437d9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-99e7a416292cf00c86adf37d982ae0dc.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Input-Time-Scaling"><a href="#Input-Time-Scaling" class="headerlink" title="Input Time Scaling"></a>Input Time Scaling</h2><p><strong>Authors:Rapheal Huang, Weilong Guo</strong></p>
<p>Current Large Language Models (LLMs) are usually post-trained on large-scale carefully curated datasets (data &amp; training scaling) and doing reasoning in test time (inference time scaling). In this work, we present a new scaling paradigm, Input Time Scaling, to complement previous scaling methods by putting resources on queries (input time). During training and testing, we combine meta-knowledge from LLMs to refine inputs with different strategies. We also find a new phenomenon, training-testing co-design there. We need to apply query strategies during both training and testing. Only applying strategies on training or testing would seriously degrade the performance. We are also surprised to find that seemingly low data quality datasets can gain high performance. Adding irrelevant information to the queries, randomly selecting examples from a minimally filtered dataset, can even perform the best. These findings contradict the widely held inductive bias, â€œgarbage in, garbage outâ€. Curating datasets with seemingly high-quality data can even potentially limit the performance ceiling. In addition, models trained on more data with similar quality (15k VS 1k) perform worse, simple dataset size scaling should also be carefully inspected. The good news is that our findings are compatible with the Less is More phenomenon. A small set of examples is enough to evoke high-level reasoning ability. With experiments on models trained on Qwen2.5-32B-Instruct, we are able to reach SOTA performance among 32B models on AIME24(76.7%) and AIME25(76.7%) pass@1. We can further achieve AIME24(76.7%) and AIME25(80%) with a majority vote of three models. Starting from DeepSeek-R1-Distill-Qwen-32B, the best result would be 86.7% on AIME24 and 76.7% on AIME25. To facilitate reproducibility and further research, we are working on open-source our datasets, data pipelines, evaluation results, and checkpoints. </p>
<blockquote>
<p>å½“å‰çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰é€šå¸¸æ˜¯åœ¨å¤§è§„æ¨¡ç²¾å¿ƒç­–åˆ’çš„æ•°æ®é›†ä¸Šè¿›è¡Œåè®­ç»ƒï¼ˆæ•°æ®ä¸è®­ç»ƒè§„æ¨¡æ‰©å¤§ï¼‰ï¼Œå¹¶åœ¨æµ‹è¯•æ—¶é—´ï¼ˆæ¨ç†æ—¶é—´è§„æ¨¡ï¼‰è¿›è¡Œæ¨ç†ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„æ‰©å±•èŒƒå¼â€”â€”è¾“å…¥æ—¶é—´æ‰©å±•ï¼Œä»¥è¡¥å……ä¹‹å‰çš„æ‰©å±•æ–¹æ³•ï¼Œå°†èµ„æºé›†ä¸­åœ¨æŸ¥è¯¢ï¼ˆè¾“å…¥æ—¶é—´ï¼‰ä¸Šã€‚åœ¨è®­ç»ƒå’Œæµ‹è¯•è¿‡ç¨‹ä¸­ï¼Œæˆ‘ä»¬é‡‡ç”¨LLMçš„å…ƒçŸ¥è¯†ï¼Œé€šè¿‡ä¸åŒçš„ç­–ç•¥æ¥ä¼˜åŒ–è¾“å…¥ã€‚æˆ‘ä»¬è¿˜å‘ç°äº†ä¸€ä¸ªæ–°ç°è±¡ï¼Œå³è®­ç»ƒä¸æµ‹è¯•çš„ååŒè®¾è®¡ã€‚æˆ‘ä»¬éœ€è¦åœ¨è®­ç»ƒå’Œæµ‹è¯•æœŸé—´éƒ½åº”ç”¨æŸ¥è¯¢ç­–ç•¥ã€‚ä»…åœ¨è®­ç»ƒæˆ–æµ‹è¯•æœŸé—´åº”ç”¨ç­–ç•¥ä¼šä¸¥é‡é™ä½æ€§èƒ½ã€‚ä»¤äººæƒŠè®¶çš„æ˜¯ï¼Œæˆ‘ä»¬å‘ç°å³ä½¿æ˜¯çœ‹ä¼¼æ•°æ®è´¨é‡ä¸é«˜çš„æ•°æ®é›†ä¹Ÿèƒ½è·å¾—é«˜æ€§èƒ½ã€‚å‘æŸ¥è¯¢ä¸­æ·»åŠ æ— å…³ä¿¡æ¯ï¼Œä»ç»è¿‡è½»å¾®è¿‡æ»¤çš„æ•°æ®é›†ä¸­éšæœºé€‰æ‹©ç¤ºä¾‹ï¼Œç”šè‡³å¯èƒ½è¡¨ç°æœ€ä½³ã€‚è¿™äº›å‘ç°ä¸æ™®éå­˜åœ¨çš„å½’çº³åè§â€œåƒåœ¾è¿›ï¼Œåƒåœ¾å‡ºâ€ç›¸çŸ›ç›¾ã€‚ä½¿ç”¨çœ‹ä¼¼é«˜è´¨é‡æ•°æ®çš„æ•°æ®é›†ç”šè‡³å¯èƒ½é™åˆ¶æ€§èƒ½ä¸Šé™ã€‚å¦å¤–ï¼Œåœ¨ç±»ä¼¼è´¨é‡çš„æ›´å¤šæ•°æ®ä¸Šè®­ç»ƒçš„æ¨¡å‹ï¼ˆ15kå¯¹1kï¼‰è¡¨ç°æ›´å·®ï¼Œå› æ­¤ä¹Ÿåº”è°¨æ…æ£€æŸ¥ç®€å•çš„æ•°æ®é›†è§„æ¨¡æ‰©å±•ã€‚å¥½æ¶ˆæ¯æ˜¯ï¼Œæˆ‘ä»¬çš„ç ”ç©¶ä¸â€œå°‘å³æ˜¯å¤šâ€ç°è±¡ç›¸å»åˆã€‚å°‘é‡ç¤ºä¾‹è¶³ä»¥æ¿€å‘é«˜çº§æ¨ç†èƒ½åŠ›ã€‚é€šè¿‡åœ¨Qwen2.5-32B-Instructä¸Šè®­ç»ƒçš„æ¨¡å‹è¿›è¡Œçš„å®éªŒï¼Œæˆ‘ä»¬åœ¨32Bæ¨¡å‹çš„AIME24ï¼ˆ76.7%ï¼‰å’ŒAIME25ï¼ˆ76.7%ï¼‰ä¸Šè¾¾åˆ°äº†æœ€æ–°æ€§èƒ½ã€‚ä½¿ç”¨ä¸‰ä¸ªæ¨¡å‹çš„å¤§å¤šæ•°æŠ•ç¥¨ï¼Œæˆ‘ä»¬å¯ä»¥è¿›ä¸€æ­¥è¾¾åˆ°AIME24ï¼ˆ76.7%ï¼‰å’ŒAIME25ï¼ˆ80%ï¼‰ã€‚ä»DeepSeek-R1-Distill-Qwen-32Bå¼€å§‹ï¼ŒAIME24çš„æœ€ä½³ç»“æœå°†è¾¾åˆ°86.7%ï¼ŒAIME25çš„ç»“æœå°†è¾¾åˆ°76.7%ã€‚ä¸ºäº†ä¿ƒè¿›å¯é‡å¤æ€§å’Œè¿›ä¸€æ­¥ç ”ç©¶ï¼Œæˆ‘ä»¬æ­£åœ¨å¼€æºæˆ‘ä»¬çš„æ•°æ®é›†ã€æ•°æ®ç®¡é“ã€è¯„ä¼°ç»“æœå’Œæ£€æŸ¥ç‚¹ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.13654v1">PDF</a> </p>
<p><strong>Summary</strong><br>     æœ¬æ–‡ä»‹ç»äº†ä¸€ç§æ–°çš„ç¼©æ”¾èŒƒå¼â€”â€”è¾“å…¥æ—¶é—´ç¼©æ”¾ï¼Œä»¥è¡¥å……å…ˆå‰çš„ç¼©æ”¾æ–¹æ³•ã€‚é€šè¿‡æŸ¥è¯¢ï¼ˆè¾“å…¥æ—¶é—´ï¼‰åˆ†é…èµ„æºï¼Œç»“åˆLLMçš„å…ƒçŸ¥è¯†å¯¹è¾“å…¥è¿›è¡Œç²¾ç»†åŒ–å¤„ç†ã€‚ç ”ç©¶å‘ç°è®­ç»ƒä¸æµ‹è¯•çš„ååŒè®¾è®¡é‡è¦æ€§ï¼Œåœ¨ä¸¤è€…ä¸­éƒ½åº”ç”¨æŸ¥è¯¢ç­–ç•¥æ‰èƒ½å–å¾—æœ€ä½³æ€§èƒ½ã€‚æ­¤å¤–ï¼Œç ”ç©¶å‘ç°ä½è´¨é‡æ•°æ®é›†ä¹Ÿèƒ½å–å¾—é«˜æ€§èƒ½ï¼Œæ·»åŠ æ— å…³ä¿¡æ¯æˆ–éšæœºé€‰æ‹©ä¾‹å­å¯èƒ½æ•ˆæœæ›´å¥½ï¼Œè¿™ä¸æ™®éåè§ç›¸åã€‚æ¨¡å‹åœ¨ç±»ä¼¼è´¨é‡æ•°æ®ä¸Šçš„æ€§èƒ½å¹¶éæ•°æ®è§„æ¨¡è¶Šå¤§è¶Šå¥½ã€‚å®éªŒè¯æ˜ï¼Œåœ¨å°è§„æ¨¡æ•°æ®é›†ä¸Šè®­ç»ƒæ¨¡å‹ä¹Ÿèƒ½æ¿€å‘é«˜çº§æ¨ç†èƒ½åŠ›ã€‚é€šè¿‡å¼€æºæ•°æ®é›†ç­‰èµ„æºä¿ƒè¿›å¤åˆ¶æ€§å’Œè¿›ä¸€æ­¥ç ”ç©¶ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ä»‹ç»äº†æ–°çš„ç¼©æ”¾èŒƒå¼â€”â€”è¾“å…¥æ—¶é—´ç¼©æ”¾ï¼Œå…³æ³¨æŸ¥è¯¢ï¼ˆè¾“å…¥æ—¶é—´ï¼‰èµ„æºçš„åˆ†é…ï¼Œç»“åˆLLMçš„å…ƒçŸ¥è¯†ä¼˜åŒ–è¾“å…¥ã€‚</li>
<li>å‘ç°è®­ç»ƒä¸æµ‹è¯•çš„ååŒè®¾è®¡é‡è¦æ€§ï¼Œéœ€åœ¨ä¸¤è€…ä¸­éƒ½åº”ç”¨æŸ¥è¯¢ç­–ç•¥ã€‚</li>
<li>ä½è´¨é‡æ•°æ®é›†ä¹Ÿèƒ½å–å¾—é«˜æ€§èƒ½ï¼Œæ·»åŠ æ— å…³ä¿¡æ¯æˆ–éšæœºé€‰æ‹©ä¾‹å­å¯èƒ½æ•ˆæœæ›´å¥½ï¼ŒæŒ‘æˆ˜äº†â€œåƒåœ¾è¿›ï¼Œåƒåœ¾å‡ºâ€çš„åè§ã€‚</li>
<li>æ¨¡å‹æ€§èƒ½å¹¶éæ•°æ®è§„æ¨¡è¶Šå¤§è¶Šå¥½ï¼Œç±»ä¼¼è´¨é‡æ•°æ®çš„ç®€å•æ•°æ®é›†è§„æ¨¡ç¼©æ”¾ä¹Ÿéœ€è¦è°¨æ…æ£€æŸ¥ã€‚</li>
<li>å°è§„æ¨¡æ•°æ®é›†èƒ½æ¿€å‘é«˜çº§æ¨ç†èƒ½åŠ›ï¼Œä¸â€œå°‘å³æ˜¯å¤šâ€ç°è±¡å…¼å®¹ã€‚</li>
<li>ä½¿ç”¨Qwen2.5-32B-Instructæ¨¡å‹åœ¨AIME24å’ŒAIME25ä¸Šå–å¾—äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.13654">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-6c031a600a01197dcf67ad1f0ffff84e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-357807c89fbf336e225f11b552d25473.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3250c46796166e44f67e6ded8721fd2a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3217e2975586c188dd7c1c691a55b077.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-9048ed3662a42e86a1252400be874d4b.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Bounding-Causal-Effects-and-Counterfactuals"><a href="#Bounding-Causal-Effects-and-Counterfactuals" class="headerlink" title="Bounding Causal Effects and Counterfactuals"></a>Bounding Causal Effects and Counterfactuals</h2><p><strong>Authors:Tobias Maringgele</strong></p>
<p>Causal inference often hinges on strong assumptions - such as no unmeasured confounding or perfect compliance - that are rarely satisfied in practice. Partial identification offers a principled alternative: instead of relying on unverifiable assumptions to estimate causal effects precisely, it derives bounds that reflect the uncertainty inherent in the data. Despite its theoretical appeal, partial identification remains underutilized in applied work, in part due to the fragmented nature of existing methods and the lack of practical guidance. This thesis addresses these challenges by systematically comparing a diverse set of bounding algorithms across multiple causal scenarios. We implement, extend, and unify state-of-the-art methods - including symbolic, optimization-based, and information-theoretic approaches - within a common evaluation framework. In particular, we propose an extension of a recently introduced entropy-bounded method, making it applicable to counterfactual queries such as the Probability of Necessity and Sufficiency (PNS). Our empirical study spans thousands of randomized simulations involving both discrete and continuous data-generating processes. We assess each method in terms of bound tightness, computational efficiency, and robustness to assumption violations. To support practitioners, we distill our findings into a practical decision tree for algorithm selection and train a machine learning model to predict the best-performing method based on observable data characteristics.   All implementations are released as part of an open-source Python package, CausalBoundingEngine, which enables users to apply and compare bounding methods through a unified interface. </p>
<blockquote>
<p>å› æœæ¨æ–­å¾€å¾€ä¾èµ–äºå¼ºæœ‰åŠ›çš„å‡è®¾â€”â€”å¦‚æ— æœªæµ‹é‡çš„æ··æ‚å› ç´ æˆ–å®Œç¾éµå¾ªâ€”â€”è¿™äº›å‡è®¾åœ¨å®è·µä¸­å¾ˆå°‘å¾—åˆ°æ»¡è¶³ã€‚éƒ¨åˆ†è¯†åˆ«æä¾›äº†ä¸€ç§æœ‰åŸåˆ™æ€§çš„æ›¿ä»£æ–¹æ¡ˆï¼šå®ƒä¸å†ä¾èµ–æ— æ³•éªŒè¯çš„å‡è®¾æ¥ç²¾ç¡®ä¼°è®¡å› æœå…³ç³»çš„å½±å“ï¼Œè€Œæ˜¯æ¨å¯¼å‡ºåæ˜ æ•°æ®å›ºæœ‰ä¸ç¡®å®šæ€§çš„è¾¹ç•Œã€‚å°½ç®¡å®ƒåœ¨ç†è®ºä¸Šå¾ˆæœ‰å¸å¼•åŠ›ï¼Œä½†åœ¨å®é™…åº”ç”¨ä¸­ï¼Œéƒ¨åˆ†è¯†åˆ«ä»ç„¶ä½¿ç”¨ä¸è¶³ï¼Œéƒ¨åˆ†åŸå› æ˜¯ç°æœ‰æ–¹æ³•çš„ç¢ç‰‡åŒ–ä»¥åŠç¼ºä¹å®é™…æŒ‡å¯¼ã€‚æœ¬è®ºæ–‡é€šè¿‡ç³»ç»Ÿåœ°æ¯”è¾ƒå¤šä¸ªå› æœåœºæ™¯ä¸‹ä¸€ç»„å¤šæ ·åŒ–çš„è¾¹ç•Œç®—æ³•æ¥è§£å†³è¿™äº›æŒ‘æˆ˜ã€‚æˆ‘ä»¬åœ¨ä¸€ä¸ªç»Ÿä¸€çš„è¯„ä¼°æ¡†æ¶å†…å®ç°äº†æœ€å…ˆè¿›çš„æ–¹æ³•ï¼Œå¯¹å…¶è¿›è¡Œæ‰©å±•å’Œç»Ÿä¸€ï¼ŒåŒ…æ‹¬ç¬¦å·æ³•ã€ä¼˜åŒ–æ³•å’Œä¿¡æ¯ç†è®ºæ–¹æ³•ã€‚ç‰¹åˆ«æ˜¯ï¼Œæˆ‘ä»¬å¯¹æœ€è¿‘å¼•å…¥çš„ç†µè¾¹ç•Œæ–¹æ³•è¿›è¡Œäº†æ‰©å±•ï¼Œä½¿å…¶é€‚ç”¨äºå¿…ç„¶æ€§æ¦‚ç‡å’Œå……åˆ†æ€§æ¦‚ç‡ï¼ˆPNSï¼‰ç­‰åäº‹å®æŸ¥è¯¢ã€‚æˆ‘ä»¬çš„å®è¯ç ”ç©¶æ¶µç›–äº†æˆåƒä¸Šä¸‡æ¶‰åŠç¦»æ•£å’Œè¿ç»­æ•°æ®ç”Ÿæˆè¿‡ç¨‹çš„éšæœºæ¨¡æ‹Ÿã€‚æˆ‘ä»¬æ ¹æ®è¾¹ç•Œç´§å¯†æ€§ã€è®¡ç®—æ•ˆç‡å’Œè¿åå‡è®¾çš„ç¨³å¥æ€§æ¥è¯„ä¼°æ¯ç§æ–¹æ³•ã€‚ä¸ºäº†æ”¯æŒå®è·µè€…ï¼Œæˆ‘ä»¬å°†ç ”ç©¶ç»“æœæ€»ç»“æˆå®ç”¨çš„å†³ç­–æ ‘ï¼Œç”¨äºç®—æ³•é€‰æ‹©ï¼Œå¹¶è®­ç»ƒæœºå™¨å­¦ä¹ æ¨¡å‹ï¼Œæ ¹æ®å¯è§‚å¯Ÿçš„æ•°æ®ç‰¹å¾é¢„æµ‹è¡¨ç°æœ€ä½³çš„æ–¹æ³•ã€‚æ‰€æœ‰å®ç°éƒ½ä½œä¸ºå¼€æºPythonè½¯ä»¶åŒ…çš„ä¸€éƒ¨åˆ†å‘å¸ƒï¼Œå³å› æœè¾¹ç•Œå¼•æ“ï¼Œå®ƒä½¿ç”¨æˆ·èƒ½å¤Ÿé€šè¿‡ç»Ÿä¸€æ¥å£åº”ç”¨å’Œæ¯”è¾ƒè¾¹ç•Œæ–¹æ³•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.13607v1">PDF</a> Bachelorâ€™s thesis, Technical University of Munich, 2025. 102 pages,   20 figures</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†å› æœæ¨æ–­ä¸­åŸºäºå‡è®¾çš„æ–¹æ³•å­˜åœ¨çš„é—®é¢˜ï¼Œå¦‚å‡è®¾éš¾ä»¥æ»¡è¶³ç­‰ã€‚éƒ¨åˆ†è¯†åˆ«ä½œä¸ºä¸€ç§æ›¿ä»£æ–¹æ³•ï¼Œé€šè¿‡æ¨å¯¼åæ˜ æ•°æ®å›ºæœ‰ä¸ç¡®å®šæ€§çš„è¾¹ç•Œæ¥ä¼°è®¡å› æœæ•ˆåº”ï¼Œè€Œä¸æ˜¯ä¾èµ–æ— æ³•éªŒè¯çš„å‡è®¾ã€‚å°½ç®¡åœ¨ç†è®ºä¸Šæœ‰å…¶å¸å¼•åŠ›ï¼Œéƒ¨åˆ†è¯†åˆ«åœ¨å®é™…åº”ç”¨ä¸­ä»ç„¶ä½¿ç”¨ä¸è¶³ã€‚æœ¬æ–‡ä¸»è¦è§£å†³äº†è¿™ä¸ªé—®é¢˜ï¼Œç³»ç»Ÿæ€§åœ°æ¯”è¾ƒäº†å¤šç§è¾¹ç•Œç®—æ³•åœ¨å¤šä¸ªå› æœåœºæ™¯ä¸‹çš„è¡¨ç°ï¼Œæå‡ºå¹¶å®Œå–„äº†ä¸€ç§æ–°å‘å±•çš„ç†µè¾¹ç•Œæ–¹æ³•ï¼Œä½¿å…¶é€‚ç”¨äºåäº‹å®æŸ¥è¯¢ç­‰åœºæ™¯ã€‚é€šè¿‡å¤§é‡çš„éšæœºæ¨¡æ‹Ÿè¯„ä¼°äº†å„ç§æ–¹æ³•çš„è¾¹ç•Œç´§å¯†æ€§ã€è®¡ç®—æ•ˆç‡å’Œå‡è®¾è¿åçš„ç¨³å¥æ€§ã€‚åŒæ—¶ï¼Œæœ¬æ–‡è¿˜æä¾›äº†ä¸€ç§å†³ç­–æ ‘å’Œæœºå™¨å­¦ä¹ æ¨¡å‹æ¥æ”¯æŒä»ä¸šè€…é€‰æ‹©æœ€ä½³ç®—æ³•ã€‚æ‰€æœ‰å®ç°éƒ½è¢«æ•´åˆåˆ°ä¸€ä¸ªå¼€æºçš„PythonåŒ…ä¸­ï¼Œä¾¿äºç”¨æˆ·åº”ç”¨å’Œæ¯”è¾ƒå„ç§è¾¹ç•Œæ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å› æœæ¨æ–­ä¾èµ–äºå¼ºå‡è®¾ï¼Œè¿™äº›å‡è®¾åœ¨å®è·µä¸­å¾ˆå°‘æ»¡è¶³ã€‚</li>
<li>éƒ¨åˆ†è¯†åˆ«ä½œä¸ºä¸€ç§æ›¿ä»£æ–¹æ³•ï¼Œé€šè¿‡æ¨å¯¼åæ˜ æ•°æ®ä¸ç¡®å®šæ€§çš„è¾¹ç•Œæ¥ä¼°è®¡å› æœæ•ˆåº”ã€‚</li>
<li>æœ¬æ–‡ç³»ç»Ÿåœ°æ¯”è¾ƒäº†å¤šç§è¾¹ç•Œç®—æ³•åœ¨å¤šä¸ªå› æœåœºæ™¯ä¸‹çš„è¡¨ç°ã€‚</li>
<li>æå‡ºäº†ä¸€ç§æ”¹è¿›çš„ç†µè¾¹ç•Œæ–¹æ³•ï¼Œé€‚ç”¨äºåäº‹å®æŸ¥è¯¢ç­‰åœºæ™¯ã€‚</li>
<li>é€šè¿‡éšæœºæ¨¡æ‹Ÿè¯„ä¼°äº†å„ç§æ–¹æ³•çš„æ€§èƒ½ã€‚</li>
<li>æä¾›äº†ä¸€ç§å†³ç­–æ ‘å’Œæœºå™¨å­¦ä¹ æ¨¡å‹æ¥å¸®åŠ©ä»ä¸šè€…é€‰æ‹©æœ€ä½³ç®—æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.13607">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-a86f6d7ec257a36dde2e40b1812ca7c0.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Breaking-the-SFT-Plateau-Multimodal-Structured-Reinforcement-Learning-for-Chart-to-Code-Generation"><a href="#Breaking-the-SFT-Plateau-Multimodal-Structured-Reinforcement-Learning-for-Chart-to-Code-Generation" class="headerlink" title="Breaking the SFT Plateau: Multimodal Structured Reinforcement Learning   for Chart-to-Code Generation"></a>Breaking the SFT Plateau: Multimodal Structured Reinforcement Learning   for Chart-to-Code Generation</h2><p><strong>Authors:Lei Chen, Xuanle Zhao, Zhixiong Zeng, Jing Huang, Liming Zheng, Yufeng Zhong, Lin Ma</strong></p>
<p>While reinforcement learning (RL) has proven highly effective for general reasoning in vision-language models, its application to tasks requiring in-depth understanding of information-rich images and generation of structured outputs remains underexplored. Chart-to-code generation exemplifies this challenge, demanding complex reasoning over visual charts to generate structured code. Supervised fine-tuning (SFT) alone is often insufficient, highlighting the need for effective RL strategies that appropriately reward structured outputs. We systematically investigate the performance plateau in SFT through large-scale experiments and propose Multimodal Structured Reinforcement Learning (MSRL) for chart-to-code generation, which substantially breaks through this plateau. We construct the largest training corpus to date, containing 3 million chart-code pairs from real-world arXiv tables to mitigate simplistic patterns of prior synthetic data. Despite reaching state-of-the-art performance, our experiments show that scaling SFT data eventually hits a plateau where further increases yield negligible improvements. Our MSRL method leverages a multi-granularity structured reward system using multimodal textual and visual feedback. At the textual level, rule-based rewards validate fine-grained code details. At the visual level, model-based rewards assess structural similarity by rendering generated code into images and employing an evaluator model. We implement this within a two-stage curriculum for training stability. Results demonstrate that MSRL significantly breaks the SFT plateau, improving high-level metrics by 6.2% and 9.9% on ChartMimic and ReachQA benchmarks respectively, achieving competitive performance with advanced closed-source models. </p>
<blockquote>
<p>è™½ç„¶å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰åœ¨è§†è§‰è¯­è¨€æ¨¡å‹çš„é€šç”¨æ¨ç†ä¸­å·²è¢«è¯æ˜æ˜¯éå¸¸æœ‰æ•ˆçš„ï¼Œä½†å…¶åœ¨éœ€è¦æ·±å…¥ç†è§£ä¿¡æ¯ä¸°å¯Œçš„å›¾åƒå’Œç”Ÿæˆç»“æ„åŒ–è¾“å‡ºçš„ä»»åŠ¡ä¸­çš„åº”ç”¨ä»ç„¶è¢«æ¢ç´¢å¾—ä¸å¤Ÿæ·±å…¥ã€‚å›¾è¡¨åˆ°ä»£ç çš„ç”Ÿæˆå°±ä½“ç°äº†è¿™ä¸€æŒ‘æˆ˜ï¼Œå®ƒè¦æ±‚é€šè¿‡è§†è§‰å›¾è¡¨è¿›è¡Œå¤æ‚æ¨ç†æ¥ç”Ÿæˆç»“æ„åŒ–ä»£ç ã€‚ä»…ä»…ä½¿ç”¨ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰å¾€å¾€æ˜¯ä¸å¤Ÿçš„ï¼Œè¿™çªæ˜¾äº†éœ€è¦æœ‰æ•ˆçš„RLç­–ç•¥æ¥é€‚å½“å¥–åŠ±ç»“æ„åŒ–è¾“å‡ºçš„å¿…è¦æ€§ã€‚æˆ‘ä»¬é€šè¿‡å¤§è§„æ¨¡å®éªŒç³»ç»Ÿåœ°ç ”ç©¶äº†SFTçš„æ€§èƒ½ç“¶é¢ˆï¼Œå¹¶é’ˆå¯¹å›¾è¡¨åˆ°ä»£ç ç”Ÿæˆæå‡ºäº†å¤šæ¨¡æ€ç»“æ„åŒ–å¼ºåŒ–å­¦ä¹ ï¼ˆMSRLï¼‰ï¼Œè¿™å®è´¨æ€§åœ°çªç ´äº†è¿™ä¸€ç“¶é¢ˆã€‚æˆ‘ä»¬æ„å»ºäº†è¿„ä»Šä¸ºæ­¢æœ€å¤§çš„è®­ç»ƒè¯­æ–™åº“ï¼ŒåŒ…å«æ¥è‡ªç°å®ä¸–ç•ŒarXivè¡¨æ ¼çš„300ä¸‡å¼ å›¾è¡¨å’Œä»£ç å¯¹ï¼Œä»¥å‡è½»å…ˆå‰åˆæˆæ•°æ®çš„ç®€å•æ¨¡å¼ã€‚å°½ç®¡è¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œæˆ‘ä»¬çš„å®éªŒè¡¨æ˜ï¼Œæ‰©å¤§SFTæ•°æ®æœ€ç»ˆä¼šè¾¾åˆ°ä¸€ä¸ªå¹³å°æœŸï¼Œåœ¨æ­¤ä¹‹åï¼Œè¿›ä¸€æ­¥çš„å¢åŠ åªä¼šå¸¦æ¥å¾®ä¹å…¶å¾®çš„æ”¹è¿›ã€‚æˆ‘ä»¬çš„MSRLæ–¹æ³•åˆ©ç”¨å¤šç²’åº¦ç»“æ„åŒ–å¥–åŠ±ç³»ç»Ÿï¼Œä½¿ç”¨å¤šæ¨¡æ€æ–‡æœ¬å’Œè§†è§‰åé¦ˆã€‚åœ¨æ–‡æœ¬å±‚é¢ï¼ŒåŸºäºè§„åˆ™çš„å¥–åŠ±éªŒè¯äº†ç²¾ç»†çš„ä»£ç ç»†èŠ‚ã€‚åœ¨è§†è§‰å±‚é¢ï¼ŒåŸºäºæ¨¡å‹çš„å¥–åŠ±é€šè¿‡å°†ç”Ÿæˆçš„ä»£ç å‘ˆç°ä¸ºå›¾åƒå¹¶ä½¿ç”¨è¯„ä¼°æ¨¡å‹æ¥è¯„ä¼°ç»“æ„ç›¸ä¼¼æ€§ã€‚æˆ‘ä»¬åœ¨ä¸€ä¸ªä¸¤é˜¶æ®µçš„è¯¾ç¨‹ä¸­å®ç°äº†è¿™ä¸€ç‚¹ï¼Œä»¥ç¡®ä¿è®­ç»ƒç¨³å®šæ€§ã€‚ç»“æœè¡¨æ˜ï¼ŒMSRLæ˜¾è‘—çªç ´äº†SFTå¹³å°æœŸï¼Œåœ¨ChartMimicå’ŒReachQAåŸºå‡†æµ‹è¯•ä¸Šï¼Œé«˜çº§æŒ‡æ ‡çš„æ”¹è¿›åˆ†åˆ«ä¸º6.2%å’Œ9.9%ï¼Œä¸å…ˆè¿›çš„å°é—­æºä»£ç æ¨¡å‹ç›¸æ¯”å…·æœ‰ç«äº‰åŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.13587v1">PDF</a> technical report</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ¢è®¨äº†å¼ºåŒ–å­¦ä¹ åœ¨è§†è§‰è¯­è¨€æ¨¡å‹ä¸­çš„é€šç”¨æ¨ç†èƒ½åŠ›ï¼Œå°¤å…¶æ˜¯åœ¨å¤„ç†ä¿¡æ¯ä¸°å¯Œçš„å›¾åƒæ·±åº¦ç†è§£å’Œç”Ÿæˆç»“æ„åŒ–è¾“å‡ºä»»åŠ¡ä¸­çš„åº”ç”¨ã€‚é’ˆå¯¹å›¾è¡¨åˆ°ä»£ç ç”Ÿæˆè¿™ä¸€æŒ‘æˆ˜ï¼Œæ–‡ç« æŒ‡å‡ºç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰çš„å±€é™æ€§ï¼Œå¹¶ä»‹ç»äº†å¤šæ¨¡æ€ç»“æ„åŒ–å¼ºåŒ–å­¦ä¹ ï¼ˆMSRLï¼‰çš„æ–°æ–¹æ³•ã€‚é€šè¿‡å¤§è§„æ¨¡å®éªŒï¼Œä½œè€…æ„å»ºäº†åŒ…å«çœŸå®arXivè¡¨æ ¼çš„3ç™¾ä¸‡å›¾è¡¨ä»£ç å¯¹è®­ç»ƒè¯­æ–™åº“ï¼Œå¹¶é‡‡ç”¨å¤šç²’åº¦ç»“æ„åŒ–å¥–åŠ±ç³»ç»Ÿï¼Œåˆ©ç”¨æ–‡æœ¬å’Œè§†è§‰åé¦ˆã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒMSRLæ–¹æ³•åœ¨å›¾è¡¨æ¨¡ä»¿å’ŒReachQAåŸºå‡†æµ‹è¯•ä¸Šçš„é«˜çº§æŒ‡æ ‡åˆ†åˆ«æé«˜äº†6.2%å’Œ9.9%ï¼Œè¾¾åˆ°ä¸å…ˆè¿›å°é—­æ¨¡å‹ç›¸å½“çš„æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¼ºåŒ–å­¦ä¹ åœ¨è§†è§‰è¯­è¨€æ¨¡å‹ä¸­çš„é€šç”¨æ¨ç†èƒ½åŠ›å¾—åˆ°äº†éªŒè¯ï¼Œä½†åœ¨å¤„ç†æ·±åº¦å›¾åƒç†è§£å’Œç”Ÿæˆç»“æ„åŒ–è¾“å‡ºä»»åŠ¡æ–¹é¢çš„åº”ç”¨ä»ç„¶æœ‰é™ã€‚</li>
<li>ç›‘ç£å¾®è°ƒåœ¨å¤„ç†å¤æ‚ä»»åŠ¡æ—¶å­˜åœ¨å±€é™æ€§ï¼Œéœ€è¦æ›´æœ‰æ•ˆçš„å¼ºåŒ–å­¦ä¹ ç­–ç•¥æ¥é€‚å½“å¥–åŠ±ç»“æ„åŒ–è¾“å‡ºã€‚</li>
<li>æå‡ºäº†ä¸€ç§æ–°çš„å¤šæ¨¡æ€ç»“æ„åŒ–å¼ºåŒ–å­¦ä¹ ï¼ˆMSRLï¼‰æ–¹æ³•ï¼Œç”¨äºè§£å†³å›¾è¡¨åˆ°ä»£ç ç”Ÿæˆç­‰æŒ‘æˆ˜æ€§é—®é¢˜ã€‚</li>
<li>æ„å»ºäº†ä¸€ä¸ªåŒ…å«çœŸå®arXivè¡¨æ ¼çš„è¯­æ–™åº“ï¼ŒåŒ…å«è¶…è¿‡ç™¾ä¸‡çš„å›¾è¡¨ä»£ç å¯¹è®­ç»ƒæ•°æ®ã€‚</li>
<li>MSRLæ–¹æ³•é‡‡ç”¨å¤šç²’åº¦ç»“æ„åŒ–å¥–åŠ±ç³»ç»Ÿï¼Œç»“åˆæ–‡æœ¬å’Œè§†è§‰åé¦ˆï¼ŒåŒ…æ‹¬åŸºäºè§„åˆ™çš„å¥–åŠ±å’ŒåŸºäºæ¨¡å‹çš„å¥–åŠ±ã€‚</li>
<li>å®éªŒç»“æœè¡¨æ˜ï¼ŒMSRLæ–¹æ³•æ˜¾è‘—çªç ´äº†ç›‘ç£å¾®è°ƒçš„ç“¶é¢ˆï¼Œå®ç°äº†é«˜çº§æŒ‡æ ‡çš„æ˜¾è‘—æé«˜ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.13587">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-9319ff2e9536847c4af7482405fd3e1a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c8adc28573cdd14e2b30957a039b8aa3.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-9f65a3bdd191d2816ea93b6315bdc165.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Toward-Better-EHR-Reasoning-in-LLMs-Reinforcement-Learning-with-Expert-Attention-Guidance"><a href="#Toward-Better-EHR-Reasoning-in-LLMs-Reinforcement-Learning-with-Expert-Attention-Guidance" class="headerlink" title="Toward Better EHR Reasoning in LLMs: Reinforcement Learning with Expert   Attention Guidance"></a>Toward Better EHR Reasoning in LLMs: Reinforcement Learning with Expert   Attention Guidance</h2><p><strong>Authors:Yue Fang, Yuxin Guo, Jiaran Gao, Hongxin Ding, Xinke Jiang, Weibin Liao, Yongxin Xu, Yinghao Zhu, Zhibang Yang, Liantao Ma, Junfeng Zhao, Yasha Wang</strong></p>
<p>Improving large language models (LLMs) for electronic health record (EHR) reasoning is essential for enabling accurate and generalizable clinical predictions. While LLMs excel at medical text understanding, they underperform on EHR-based prediction tasks due to challenges in modeling temporally structured, high-dimensional data. Existing approaches often rely on hybrid paradigms, where LLMs serve merely as frozen prior retrievers while downstream deep learning (DL) models handle prediction, failing to improve the LLMâ€™s intrinsic reasoning capacity and inheriting the generalization limitations of DL models. To this end, we propose EAG-RL, a novel two-stage training framework designed to intrinsically enhance LLMsâ€™ EHR reasoning ability through expert attention guidance, where expert EHR models refer to task-specific DL models trained on EHR data. Concretely, EAG-RL first constructs high-quality, stepwise reasoning trajectories using expert-guided Monte Carlo Tree Search to effectively initialize the LLMâ€™s policy. Then, EAG-RL further optimizes the policy via reinforcement learning by aligning the LLMâ€™s attention with clinically salient features identified by expert EHR models. Extensive experiments on two real-world EHR datasets show that EAG-RL improves the intrinsic EHR reasoning ability of LLMs by an average of 14.62%, while also enhancing robustness to feature perturbations and generalization to unseen clinical domains. These results demonstrate the practical potential of EAG-RL for real-world deployment in clinical prediction tasks. Our code have been available at <a target="_blank" rel="noopener" href="https://github.com/devilran6/EAG-RL">https://github.com/devilran6/EAG-RL</a>. </p>
<blockquote>
<p>æå‡å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨ç”µå­å¥åº·è®°å½•ï¼ˆEHRï¼‰æ¨ç†ä¸­çš„èƒ½åŠ›ï¼Œå¯¹äºå®ç°å‡†ç¡®ä¸”å¯æ¨å¹¿çš„ä¸´åºŠé¢„æµ‹è‡³å…³é‡è¦ã€‚è™½ç„¶å¤§å‹è¯­è¨€æ¨¡å‹åœ¨åŒ»ç–—æ–‡æœ¬ç†è§£æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œä½†åœ¨åŸºäºç”µå­å¥åº·è®°å½•çš„é¢„æµ‹ä»»åŠ¡ä¸­è¡¨ç°ä¸ä½³ï¼Œè¿™æ˜¯ç”±äºå»ºæ¨¡æ—¶åºç»“æ„ã€é«˜ç»´æ•°æ®çš„æŒ‘æˆ˜æ‰€è‡´ã€‚ç°æœ‰æ–¹æ³•å¾€å¾€ä¾èµ–äºæ··åˆèŒƒå¼ï¼Œå…¶ä¸­å¤§å‹è¯­è¨€æ¨¡å‹ä»…ä½œä¸ºå†»ç»“çš„å…ˆéªŒæ£€ç´¢å™¨ï¼Œè€Œä¸‹æ¸¸æ·±åº¦å­¦ä¹ ï¼ˆDLï¼‰æ¨¡å‹å¤„ç†é¢„æµ‹ï¼Œè¿™æœªèƒ½æå‡å¤§å‹è¯­è¨€æ¨¡å‹çš„å†…ç”Ÿæ¨ç†èƒ½åŠ›ï¼Œå¹¶ç»§æ‰¿äº†æ·±åº¦å­¦ä¹ æ¨¡å‹çš„æ³›åŒ–å±€é™æ€§ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†EAG-RLï¼Œè¿™æ˜¯ä¸€ç§æ–°å‹çš„ä¸¤é˜¶æ®µè®­ç»ƒæ¡†æ¶ï¼Œæ—¨åœ¨é€šè¿‡ä¸“å®¶æ³¨æ„åŠ›æŒ‡å¯¼å†…åœ¨æå‡å¤§å‹è¯­è¨€æ¨¡å‹çš„ç”µå­å¥åº·è®°å½•æ¨ç†èƒ½åŠ›ï¼Œå…¶ä¸­ä¸“å®¶ç”µå­å¥åº·è®°å½•æ¨¡å‹æ˜¯æŒ‡åŸºäºç”µå­å¥åº·è®°å½•æ•°æ®è®­ç»ƒçš„ç‰¹å®šä»»åŠ¡æ·±åº¦å­¦ä¹ æ¨¡å‹ã€‚å…·ä½“æ¥è¯´ï¼ŒEAG-RLé¦–å…ˆä½¿ç”¨ä¸“å®¶æŒ‡å¯¼çš„è’™ç‰¹å¡æ´›æ ‘æœç´¢æ„å»ºé«˜è´¨é‡çš„ã€åˆ†æ­¥éª¤æ¨ç†è½¨è¿¹ï¼Œä»¥æœ‰æ•ˆåœ°åˆå§‹åŒ–å¤§å‹è¯­è¨€æ¨¡å‹çš„ç­–ç•¥ã€‚ç„¶åï¼ŒEAG-RLé€šè¿‡å¼ºåŒ–å­¦ä¹ è¿›ä¸€æ­¥ä¼˜åŒ–ç­–ç•¥ï¼Œä½¿å¤§å‹è¯­è¨€æ¨¡å‹çš„æ³¨æ„åŠ›ä¸ä¸“å®¶ç”µå­å¥åº·è®°å½•æ¨¡å‹è¯†åˆ«çš„ä¸´åºŠæ˜¾è‘—ç‰¹å¾å¯¹é½ã€‚åœ¨ä¸¤ä¸ªçœŸå®ä¸–ç•Œçš„ç”µå­å¥åº·è®°å½•æ•°æ®é›†ä¸Šçš„å¤§é‡å®éªŒè¡¨æ˜ï¼ŒEAG-RLå¹³å‡æå‡äº†å¤§å‹è¯­è¨€æ¨¡å‹çš„ç”µå­å¥åº·è®°å½•æ¨ç†èƒ½åŠ›14.62%ï¼ŒåŒæ—¶æé«˜äº†å¯¹ç‰¹å¾æ‰°åŠ¨çš„ç¨³å¥æ€§å’Œå¯¹æœªè§ä¸´åºŠé¢†åŸŸçš„æ³›åŒ–èƒ½åŠ›ã€‚è¿™äº›ç»“æœå±•ç¤ºäº†EAG-RLåœ¨å®é™…ä¸´åºŠé¢„æµ‹ä»»åŠ¡ä¸­çš„å®ç”¨æ½œåŠ›ã€‚æˆ‘ä»¬çš„ä»£ç å·²åœ¨<a target="_blank" rel="noopener" href="https://github.com/devilran6/EAG-RL%E4%B8%8A%E6%8F%90%E4%BE%9B%E3%80%82">https://github.com/devilran6/EAG-RLä¸Šæä¾›ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.13579v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ç€é‡ä»‹ç»äº†å¦‚ä½•æå‡å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨ç”µå­å¥åº·è®°å½•ï¼ˆEHRï¼‰æ–¹é¢çš„æ¨ç†èƒ½åŠ›ï¼Œä»¥å®ç°å‡†ç¡®ä¸”å¯æ¨å¹¿çš„ä¸´åºŠé¢„æµ‹ã€‚é’ˆå¯¹LLMåœ¨å¤„ç†ç»“æ„åŒ–ã€é«˜ç»´åº¦çš„EHRæ•°æ®æ—¶é¢ä¸´çš„æŒ‘æˆ˜ï¼Œæå‡ºäº†ä¸€ç§åä¸ºEAG-RLçš„æ–°å‹ä¸¤é˜¶æ®µè®­ç»ƒæ¡†æ¶ã€‚è¯¥æ¡†æ¶é€šè¿‡ä¸“å®¶æ³¨æ„åŠ›å¼•å¯¼å¢å¼ºLLMçš„å†…åœ¨EHRæ¨ç†èƒ½åŠ›ï¼Œå¹¶é€šè¿‡ä¸“å®¶EHRæ¨¡å‹æŒ‡å¯¼çš„è’™ç‰¹å¡æ´›æ ‘æœç´¢æ„å»ºé«˜è´¨é‡çš„é€æ­¥æ¨ç†è½¨è¿¹ï¼Œä»¥åˆå§‹åŒ–LLMçš„ç­–ç•¥ã€‚ç„¶åé€šè¿‡å¼ºåŒ–å­¦ä¹ è¿›ä¸€æ­¥ä¼˜åŒ–ç­–ç•¥ï¼Œä½¿LLMçš„æ³¨æ„åŠ›ä¸ä¸“å®¶EHRæ¨¡å‹è¯†åˆ«çš„ä¸´åºŠæ˜¾è‘—ç‰¹å¾å¯¹é½ã€‚åœ¨ä¸¤ä¸ªçœŸå®çš„EHRæ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒEAG-RLæé«˜äº†LLMçš„EHRæ¨ç†èƒ½åŠ›å¹³å‡è¾¾14.62%ï¼ŒåŒæ—¶æé«˜äº†å¯¹ç‰¹å¾æ‰°åŠ¨çš„ç¨³å¥æ€§å’Œå¯¹æœªè§ä¸´åºŠé¢†åŸŸçš„æ³›åŒ–èƒ½åŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æå‡å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨ç”µå­å¥åº·è®°å½•ï¼ˆEHRï¼‰æ–¹é¢çš„æ¨ç†èƒ½åŠ›æ˜¯å®ç°å‡†ç¡®ä¸´åºŠé¢„æµ‹çš„å…³é”®ã€‚</li>
<li>LLMåœ¨å¤„ç†ç»“æ„åŒ–ã€é«˜ç»´åº¦çš„EHRæ•°æ®æ—¶é¢ä¸´æŒ‘æˆ˜ã€‚</li>
<li>EAG-RLæ˜¯ä¸€ç§æ–°å‹ä¸¤é˜¶æ®µè®­ç»ƒæ¡†æ¶ï¼Œæ—¨åœ¨é€šè¿‡ä¸“å®¶æ³¨æ„åŠ›å¼•å¯¼å¢å¼ºLLMçš„EHRæ¨ç†èƒ½åŠ›ã€‚</li>
<li>EAG-RLä½¿ç”¨ä¸“å®¶EHRæ¨¡å‹æŒ‡å¯¼çš„è’™ç‰¹å¡æ´›æ ‘æœç´¢æ¥åˆå§‹åŒ–LLMçš„ç­–ç•¥ã€‚</li>
<li>é€šè¿‡å¼ºåŒ–å­¦ä¹ ï¼ŒEAG-RLè¿›ä¸€æ­¥ä¼˜åŒ–äº†LLMçš„ç­–ç•¥ï¼Œä½¿å…¶æ³¨æ„åŠ›ä¸ä¸´åºŠæ˜¾è‘—ç‰¹å¾å¯¹é½ã€‚</li>
<li>åœ¨çœŸå®EHRæ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒEAG-RLæé«˜äº†LLMçš„EHRæ¨ç†èƒ½åŠ›è¾¾14.62%ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.13579">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-5d6c9370445de1c6f8a352d71c3a537b.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-e12389f2ec760e0343fa820f87598b83.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-6de54db86bdc3a0c35dfa34c8b920b83.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a6fb297e0015f215b916487609cf91ee.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="The-9th-AI-City-Challenge"><a href="#The-9th-AI-City-Challenge" class="headerlink" title="The 9th AI City Challenge"></a>The 9th AI City Challenge</h2><p><strong>Authors:Zheng Tang, Shuo Wang, David C. Anastasiu, Ming-Ching Chang, Anuj Sharma, Quan Kong, Norimasa Kobori, Munkhjargal Gochoo, Ganzorig Batnasan, Munkh-Erdene Otgonbold, Fady Alnajjar, Jun-Wei Hsieh, Tomasz Kornuta, Xiaolong Li, Yilin Zhao, Han Zhang, Subhashree Radhakrishnan, Arihant Jain, Ratnesh Kumar, Vidya N. Murali, Yuxing Wang, Sameer Satish Pusegaonkar, Yizhou Wang, Sujit Biswas, Xunlei Wu, Zhedong Zheng, Pranamesh Chakraborty, Rama Chellappa</strong></p>
<p>The ninth AI City Challenge continues to advance real-world applications of computer vision and AI in transportation, industrial automation, and public safety. The 2025 edition featured four tracks and saw a 17% increase in participation, with 245 teams from 15 countries registered on the evaluation server. Public release of challenge datasets led to over 30,000 downloads to date. Track 1 focused on multi-class 3D multi-camera tracking, involving people, humanoids, autonomous mobile robots, and forklifts, using detailed calibration and 3D bounding box annotations. Track 2 tackled video question answering in traffic safety, with multi-camera incident understanding enriched by 3D gaze labels. Track 3 addressed fine-grained spatial reasoning in dynamic warehouse environments, requiring AI systems to interpret RGB-D inputs and answer spatial questions that combine perception, geometry, and language. Both Track 1 and Track 3 datasets were generated in NVIDIA Omniverse. Track 4 emphasized efficient road object detection from fisheye cameras, supporting lightweight, real-time deployment on edge devices. The evaluation framework enforced submission limits and used a partially held-out test set to ensure fair benchmarking. Final rankings were revealed after the competition concluded, fostering reproducibility and mitigating overfitting. Several teams achieved top-tier results, setting new benchmarks in multiple tasks. </p>
<blockquote>
<p>ç¬¬ä¹å±ŠAIåŸå¸‚æŒ‘æˆ˜èµ›ç»§ç»­æ¨åŠ¨è®¡ç®—æœºè§†è§‰å’Œäººå·¥æ™ºèƒ½åœ¨äº¤é€šã€å·¥ä¸šè‡ªåŠ¨åŒ–å’Œå…¬å…±å®‰å…¨é¢†åŸŸçš„å®é™…åº”ç”¨ã€‚2025å¹´ç‰ˆæ¯”èµ›è®¾æœ‰å››ä¸ªèµ›é“ï¼Œå‚èµ›äººæ•°å¢åŠ äº†17%ï¼Œè¯„ä»·æœåŠ¡å™¨ä¸Šæœ‰æ¥è‡ª15ä¸ªå›½å®¶çš„245æ”¯é˜Ÿä¼æ³¨å†Œã€‚æŒ‘æˆ˜æ•°æ®é›†å…¬å¼€å‘å¸ƒåï¼Œè‡³ä»Šä¸‹è½½é‡å·²è¶…è¿‡3ä¸‡æ¬¡ã€‚ç¬¬ä¸€èµ›é“å…³æ³¨å¤šç±»3Då¤šç›¸æœºè·Ÿè¸ªï¼Œæ¶‰åŠäººå‘˜ã€äººå½¢æœºå™¨äººã€è‡ªä¸»ç§»åŠ¨æœºå™¨äººå’Œå‰è½¦ï¼Œé‡‡ç”¨è¯¦ç»†çš„æ ¡å‡†å’Œ3Dè¾¹ç•Œæ¡†æ³¨é‡Šã€‚ç¬¬äºŒèµ›é“è§£å†³äº†äº¤é€šå®‰å…¨ä¸­çš„è§†é¢‘é—®ç­”é—®é¢˜ï¼Œé€šè¿‡å¤šç›¸æœºäº‹ä»¶ç†è§£ï¼Œè¾…ä»¥3Då‡è§†æ ‡ç­¾ã€‚ç¬¬ä¸‰èµ›é“è§£å†³åŠ¨æ€ä»“åº“ç¯å¢ƒä¸­çš„ç²¾ç»†ç©ºé—´æ¨ç†é—®é¢˜ï¼Œéœ€è¦AIç³»ç»Ÿè§£é‡ŠRGB-Dè¾“å…¥å¹¶å›ç­”ç»“åˆæ„ŸçŸ¥ã€å‡ ä½•å’Œè¯­è¨€çš„ç©ºé—´é—®é¢˜ã€‚ç¬¬ä¸€èµ›é“å’Œç¬¬ä¸‰èµ›é“çš„æ•°æ®é›†éƒ½æ˜¯åœ¨NVIDIA Omniverseä¸­ç”Ÿæˆçš„ã€‚ç¬¬å››èµ›é“ä¾§é‡äºé±¼çœ¼ç›¸æœºçš„é“è·¯ç›®æ ‡é«˜æ•ˆæ£€æµ‹ï¼Œæ”¯æŒåœ¨è¾¹ç¼˜è®¾å¤‡çš„è½»é‡çº§å®æ—¶éƒ¨ç½²ã€‚è¯„ä¼°æ¡†æ¶å¼ºåˆ¶æ‰§è¡Œæäº¤é™åˆ¶ï¼Œå¹¶ä½¿ç”¨éƒ¨åˆ†éšè—çš„æµ‹è¯•é›†ä»¥ç¡®ä¿å…¬å¹³è¯„ä¼°ã€‚åœ¨æ¯”èµ›ç»“æŸåå…¬å¸ƒæœ€ç»ˆæ’åï¼Œä¿ƒè¿›å¯é‡å¤æ€§å’Œå‡è½»è¿‡åº¦æ‹Ÿåˆã€‚å¤šä¸ªå›¢é˜Ÿå–å¾—é¡¶å°–æˆç»©ï¼Œåœ¨å¤šä¸ªä»»åŠ¡ä¸­åˆ›é€ æ–°çš„åŸºå‡†ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.13564v1">PDF</a> Summary of the 9th AI City Challenge Workshop in conjunction with   ICCV 2025</p>
<p><strong>Summary</strong></p>
<p>éšç€ç¬¬ä¹å±Šäººå·¥æ™ºèƒ½åŸå¸‚æŒ‘æˆ˜èµ›ï¼ˆAI City Challengeï¼‰çš„æŒç»­æ¨è¿›ï¼Œè®¡ç®—æœºè§†è§‰å’Œäººå·¥æ™ºèƒ½åœ¨äº¤é€šã€å·¥ä¸šè‡ªåŠ¨åŒ–å’Œå…¬å…±å®‰å…¨é¢†åŸŸçš„åº”ç”¨ä¸æ–­å‘å±•ã€‚å…¶ä¸­ï¼Œ2025å¹´ç‰ˆæœ¬åŒ…å«å››ä¸ªèµ›é“ï¼Œå‚èµ›é˜Ÿä¼æ•°é‡å¢åŠ äº†17%ï¼Œå…±æœ‰æ¥è‡ª15ä¸ªå›½å®¶çš„245æ”¯é˜Ÿä¼æ³¨å†Œå‚èµ›ã€‚å…¬å¼€å‘å¸ƒæŒ‘æˆ˜æ•°æ®é›†åï¼Œä¸‹è½½é‡å·²è¶…è¿‡3ä¸‡æ¬¡ã€‚å„èµ›é“æ¶µç›–äº†å¤šç±»åˆ«3Då¤šç›¸æœºè·Ÿè¸ªã€äº¤é€šå®‰å…¨è§†é¢‘é—®ç­”ã€ç²¾ç»†ç©ºé—´æ¨ç†å’Œé«˜æ•ˆé“è·¯å¯¹è±¡æ£€æµ‹ç­‰ä»»åŠ¡ï¼Œæ¨åŠ¨äº†äººå·¥æ™ºèƒ½åœ¨ç°å®åœºæ™¯çš„åº”ç”¨å’Œå‘å±•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç¬¬ä¹å±Šäººå·¥æ™ºèƒ½åŸå¸‚æŒ‘æˆ˜èµ›æ—¨åœ¨æ¨åŠ¨è®¡ç®—æœºè§†è§‰å’Œäººå·¥æ™ºèƒ½åœ¨äº¤é€šã€å·¥ä¸šè‡ªåŠ¨åŒ–å’Œå…¬å…±å®‰å…¨é¢†åŸŸçš„å®é™…åº”ç”¨ã€‚</li>
<li>2025å¹´èµ›äº‹è§„æ¨¡æ‰©å¤§ï¼Œå‚èµ›é˜Ÿä¼æ•°é‡å¢åŠ 17%ï¼Œæ¶‰åŠå¤šä¸ªå›½å®¶çš„å›¢é˜Ÿå‚ä¸ã€‚</li>
<li>æŒ‘æˆ˜æ•°æ®é›†å…¬å¼€ï¼Œä¸‹è½½é‡è¶…è¿‡3ä¸‡æ¬¡ï¼Œä¿ƒè¿›äº†æ•°æ®å…±äº«å’Œæ¨¡å‹è®­ç»ƒã€‚</li>
<li>èµ›äº‹åŒ…å«å››ä¸ªèµ›é“ï¼Œåˆ†åˆ«å…³æ³¨å¤šç±»åˆ«3Då¤šç›¸æœºè·Ÿè¸ªã€äº¤é€šå®‰å…¨è§†é¢‘é—®ç­”ã€ç²¾ç»†ç©ºé—´æ¨ç†å’Œé«˜æ•ˆé“è·¯å¯¹è±¡æ£€æµ‹ç­‰ä»»åŠ¡ã€‚</li>
<li>éƒ¨åˆ†èµ›é“æ•°æ®é›†é‡‡ç”¨NVIDIA Omniverseç”Ÿæˆï¼Œè´´è¿‘å®é™…åœºæ™¯ã€‚</li>
<li>è¯„ä»·æ¡†æ¶é‡‡ç”¨æäº¤é™åˆ¶å’Œéƒ¨åˆ†æœªå…¬å¼€æµ‹è¯•é›†ï¼Œç¡®ä¿å…¬å¹³è¯„ä¼°å’Œæ¨¡å‹æ³›åŒ–èƒ½åŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.13564">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-522326b1f089e859ee66fc002d903713.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-8348d68c94422a0caa55df288b4af5d7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-98aaedabe5f02bcd42784defe32bd0c1.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-20a2e3f6aedea34877765c16ee6a57fe.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-03a3a077b432c9d5e9866d6fff00e320.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="ProMed-Shapley-Information-Gain-Guided-Reinforcement-Learning-for-Proactive-Medical-LLMs"><a href="#ProMed-Shapley-Information-Gain-Guided-Reinforcement-Learning-for-Proactive-Medical-LLMs" class="headerlink" title="ProMed: Shapley Information Gain Guided Reinforcement Learning for   Proactive Medical LLMs"></a>ProMed: Shapley Information Gain Guided Reinforcement Learning for   Proactive Medical LLMs</h2><p><strong>Authors:Hongxin Ding, Baixiang Huang, Yue Fang, Weibin Liao, Xinke Jiang, Zheng Li, Junfeng Zhao, Yasha Wang</strong></p>
<p>Interactive medical questioning is essential in real-world clinical consultations, where physicians must actively gather information from patients. While medical Large Language Models (LLMs) have shown impressive capabilities in static medical question answering, they predominantly operate under a reactive paradigm: generating answers directly without seeking additional information, which risks incorrect diagnoses in such interactive settings. To address this limitation, we propose ProMed, a reinforcement learning (RL) framework that transitions medical LLMs toward a proactive paradigm, equipping them with the ability to ask clinically valuable questions before decision-making. At the core of ProMed is the Shapley Information Gain (SIG) reward, which quantifies the clinical utility of each question by combining the amount of newly acquired information with its contextual importance, estimated via Shapley values. We integrate SIG into a two-stage training pipeline: (1) SIG-Guided Model Initialization uses Monte Carlo Tree Search (MCTS) to construct high-reward interaction trajectories to supervise the model, and (2) SIG-Augmented Policy Optimization, which integrates SIG and enhances RL with a novel SIG-guided Reward Distribution Mechanism that assigns higher rewards to informative questions for targeted optimization. Extensive experiments on two newly curated partial-information medical benchmarks demonstrate that ProMed significantly outperforms state-of-the-art methods by an average of 6.29% and delivers a 54.45% gain over the reactive paradigm, while also generalizing robustly to out-of-domain cases. </p>
<blockquote>
<p>äº’åŠ¨åŒ»å­¦æé—®åœ¨ç°å®ä¸–ç•Œä¸­çš„ä¸´åºŠå’¨è¯¢ä¸­è‡³å…³é‡è¦ï¼ŒåŒ»ç”Ÿå¿…é¡»ä¸»åŠ¨ä»æ‚£è€…é‚£é‡Œæ”¶é›†ä¿¡æ¯ã€‚è™½ç„¶åŒ»ç–—å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨é™æ€åŒ»ç–—é—®é¢˜å›ç­”ä¸­è¡¨ç°å‡ºäº†ä»¤äººå°è±¡æ·±åˆ»çš„èƒ½åŠ›ï¼Œä½†å®ƒä»¬ä¸»è¦éµå¾ªååº”æ¨¡å¼ï¼šç›´æ¥ç”Ÿæˆç­”æ¡ˆè€Œä¸å¯»æ±‚é¢å¤–ä¿¡æ¯ï¼Œè¿™åœ¨äº¤äº’å¼ç¯å¢ƒä¸­å¯èƒ½å¯¼è‡´è¯¯è¯Šé£é™©ã€‚ä¸ºäº†è§£å†³è¿™ä¸€å±€é™æ€§ï¼Œæˆ‘ä»¬æå‡ºäº†ProMedï¼Œè¿™æ˜¯ä¸€ä¸ªå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰æ¡†æ¶ï¼Œä½¿åŒ»ç–—LLMè½¬å‘ä¸»åŠ¨æ¨¡å¼ï¼Œèµ‹äºˆå®ƒä»¬åœ¨å†³ç­–å‰æå‡ºå…·æœ‰ä¸´åºŠä»·å€¼é—®é¢˜çš„èƒ½åŠ›ã€‚ProMedçš„æ ¸å¿ƒæ˜¯Shapleyä¿¡æ¯å¢ç›Šï¼ˆSIGï¼‰å¥–åŠ±ï¼Œå®ƒé€šè¿‡ç»“åˆæ–°è·å–çš„ä¿¡æ¯é‡å’Œå…¶ä¸Šä¸‹æ–‡é‡è¦æ€§æ¥é‡åŒ–æ¯ä¸ªé—®é¢˜çš„ä¸´åºŠæ•ˆç”¨ï¼Œè¿™äº›ä¿¡æ¯çš„é‡è¦æ€§ä¼°è®¡æ˜¯é€šè¿‡Shapleyå€¼è¿›è¡Œçš„ã€‚æˆ‘ä»¬å°†SIGé›†æˆåˆ°ä¸€ä¸ªä¸¤é˜¶æ®µè®­ç»ƒç®¡é“ä¸­ï¼šï¼ˆ1ï¼‰SIGå¼•å¯¼æ¨¡å‹åˆå§‹åŒ–ä½¿ç”¨è’™ç‰¹å¡æ´›æ ‘æœç´¢ï¼ˆMCTSï¼‰æ¥æ„å»ºé«˜å¥–åŠ±äº¤äº’è½¨è¿¹ä»¥ç›‘ç£æ¨¡å‹ï¼›ï¼ˆ2ï¼‰SIGå¢å¼ºç­–ç•¥ä¼˜åŒ–ï¼Œå®ƒå°†SIGä¸å¼ºåŒ–å­¦ä¹ ç›¸ç»“åˆï¼Œå¹¶é€šè¿‡æ–°çš„SIGå¼•å¯¼å¥–åŠ±åˆ†é…æœºåˆ¶æ¥æé«˜å¯¹ä¿¡æ¯æ€§é—®é¢˜çš„å¥–åŠ±ï¼Œä»¥è¿›è¡Œæœ‰é’ˆå¯¹æ€§çš„ä¼˜åŒ–ã€‚åœ¨ä¸¤ä¸ªæ–°æ•´ç†çš„éƒ¨åˆ†ä¿¡æ¯åŒ»ç–—åŸºå‡†æµ‹è¯•ä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼ŒProMedå¹³å‡ä¼˜äºæœ€æ–°æŠ€æœ¯6.29%ï¼Œå¹¶ä¸”åœ¨ååº”æ€§æ¨¡å¼ä¸Šæœ‰54.45%çš„æ”¹è¿›ï¼ŒåŒæ—¶å¯¹äºç¦»åŸŸæƒ…å†µä¹Ÿå…·æœ‰ç¨³å¥çš„æ³›åŒ–èƒ½åŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.13514v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>è¯¥æ–‡æŒ‡å‡ºäº’åŠ¨åŒ»ç–—é—®è¯Šåœ¨ç°å®ä¸–ç•Œçš„ä¸´åºŠå’¨è¯¢ä¸­çš„é‡è¦æ€§ï¼ŒåŒ»ç”Ÿéœ€ä¸»åŠ¨æ”¶é›†ç—…äººçš„ä¿¡æ¯ã€‚å°½ç®¡åŒ»ç–—é¢†åŸŸçš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨é™æ€åŒ»ç–—é—®ç­”ä¸­è¡¨ç°å‡ºä»¤äººå°è±¡æ·±åˆ»çš„èƒ½åŠ›ï¼Œä½†å®ƒä»¬ä¸»è¦éµå¾ªååº”æ¨¡å¼ï¼Œå³ç›´æ¥ç”Ÿæˆç­”æ¡ˆè€Œä¸å¯»æ±‚é¢å¤–ä¿¡æ¯ï¼Œè¿™å¯èƒ½åœ¨äº’åŠ¨åœºæ™¯ä¸­å¯¼è‡´è¯¯è¯Šã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œæå‡ºProMedï¼Œä¸€ä¸ªå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰æ¡†æ¶ï¼Œä½¿åŒ»ç–—LLMå‘ä¸»åŠ¨æ¨¡å¼è½¬å˜ï¼Œèµ‹äºˆå®ƒä»¬åœ¨å†³ç­–å‰æå‡ºå…·æœ‰ä¸´åºŠä»·å€¼é—®é¢˜çš„èƒ½åŠ›ã€‚ProMedçš„æ ¸å¿ƒæ˜¯Shapleyä¿¡æ¯å¢ç›Šï¼ˆSIGï¼‰å¥–åŠ±ï¼Œå®ƒç»“åˆäº†æ–°è·å–çš„ä¿¡æ¯é‡å’Œå…¶ä¸Šä¸‹æ–‡é‡è¦æ€§æ¥é‡åŒ–æ¯ä¸ªé—®é¢˜çš„ä¸´åºŠå®ç”¨æ€§ã€‚æˆ‘ä»¬å°†SIGé›†æˆåˆ°ä¸€ä¸ªä¸¤é˜¶æ®µçš„è®­ç»ƒç®¡é“ä¸­ï¼Œç¬¬ä¸€é˜¶æ®µæ˜¯SIGæŒ‡å¯¼æ¨¡å‹åˆå§‹åŒ–ï¼Œä½¿ç”¨è’™ç‰¹å¡æ´›æ ‘æœç´¢ï¼ˆMCTSï¼‰æ„å»ºé«˜å¥–åŠ±äº’åŠ¨è½¨è¿¹æ¥ç›‘ç£æ¨¡å‹ï¼›ç¬¬äºŒé˜¶æ®µæ˜¯SIGå¢å¼ºç­–ç•¥ä¼˜åŒ–ï¼Œå®ƒç»“åˆäº†SIGå’Œä¸€ä¸ªæ–°çš„SIGæŒ‡å¯¼å¥–åŠ±åˆ†é…æœºåˆ¶ï¼Œä¸ºä¿¡æ¯ä¸°å¯Œçš„é—®é¢˜åˆ†é…æ›´é«˜çš„å¥–åŠ±ï¼Œä»¥å®ç°æœ‰é’ˆå¯¹æ€§çš„ä¼˜åŒ–ã€‚åœ¨å…¨æ–°æ•´ç†çš„å±€éƒ¨ä¿¡æ¯åŒ»ç–—åŸºå‡†æµ‹è¯•ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒProMedå¹³å‡ä¼˜äºæœ€æ–°æ–¹æ³•6.29%ï¼Œç›¸è¾ƒäºååº”æ¨¡å¼æœ‰54.45%çš„æå‡ï¼Œå¹¶ä¸”åœ¨è·¨åŸŸæ¡ˆä¾‹ä¸­å…·æœ‰ç¨³å¥çš„æ³›åŒ–èƒ½åŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>äº’åŠ¨åŒ»ç–—é—®è¯Šåœ¨ç°å®ä¸–ç•Œçš„ä¸´åºŠå’¨è¯¢ä¸­è‡³å…³é‡è¦ï¼ŒåŒ»ç”Ÿéœ€ä¸»åŠ¨æ”¶é›†ç—…äººä¿¡æ¯ã€‚</li>
<li>å½“å‰åŒ»ç–—å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ä¸»è¦éµå¾ªååº”æ¨¡å¼ï¼Œå¯èƒ½å¯¼è‡´è¯¯è¯Šã€‚</li>
<li>ProMedæ˜¯ä¸€ä¸ªå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰æ¡†æ¶ï¼Œä½¿åŒ»ç–—LLMå‘ä¸»åŠ¨æ¨¡å¼è½¬å˜ï¼Œå…·å¤‡åœ¨å†³ç­–å‰æå‡ºä¸´åºŠä»·å€¼é—®é¢˜çš„èƒ½åŠ›ã€‚</li>
<li>Shapleyä¿¡æ¯å¢ç›Šï¼ˆSIGï¼‰å¥–åŠ±æ˜¯ProMedçš„æ ¸å¿ƒï¼Œå®ƒé‡åŒ–é—®é¢˜çš„ä¸´åºŠå®ç”¨æ€§ã€‚</li>
<li>ProMedé‡‡ç”¨ä¸¤é˜¶æ®µè®­ç»ƒç®¡é“ï¼šSIGæŒ‡å¯¼æ¨¡å‹åˆå§‹åŒ–å’ŒSIGå¢å¼ºç­–ç•¥ä¼˜åŒ–ã€‚</li>
<li>Monte Carlo Tree Searchï¼ˆMCTSï¼‰ç”¨äºæ„å»ºé«˜å¥–åŠ±äº’åŠ¨è½¨è¿¹æ¥ç›‘ç£æ¨¡å‹ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.13514">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-d756d471aa4e38b50d18a7cea02f64eb.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3b0589443bdf075d977fb2c5cc12d3fb.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-833e0542c23b1b7e08eee2cdb8382483.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="Stands-to-Reason-Investigating-the-Effect-of-Reasoning-on-Idiomaticity-Detection"><a href="#Stands-to-Reason-Investigating-the-Effect-of-Reasoning-on-Idiomaticity-Detection" class="headerlink" title="Stands to Reason: Investigating the Effect of Reasoning on Idiomaticity   Detection"></a>Stands to Reason: Investigating the Effect of Reasoning on Idiomaticity   Detection</h2><p><strong>Authors:Dylan Phelps, Rodrigo Wilkens, Edward Gow-Smith, Thomas Pickard, Maggie Mi, Aline Villavicencio</strong></p>
<p>The recent trend towards utilisation of reasoning models has improved the performance of Large Language Models (LLMs) across many tasks which involve logical steps. One linguistic task that could benefit from this framing is idiomaticity detection, as a potentially idiomatic expression must first be understood before it can be disambiguated and serves as a basis for reasoning. In this paper, we explore how reasoning capabilities in LLMs affect idiomaticity detection performance and examine the effect of model size. We evaluate, as open source representative models, the suite of DeepSeek-R1 distillation models ranging from 1.5B to 70B parameters across four idiomaticity detection datasets. We find the effect of reasoning to be smaller and more varied than expected. For smaller models, producing chain-of-thought (CoT) reasoning increases performance from Math-tuned intermediate models, but not to the levels of the base models, whereas larger models (14B, 32B, and 70B) show modest improvements. Our in-depth analyses reveal that larger models demonstrate good understanding of idiomaticity, successfully producing accurate definitions of expressions, while smaller models often fail to output the actual meaning. For this reason, we also experiment with providing definitions in the prompts of smaller models, which we show can improve performance in some cases. </p>
<blockquote>
<p>è¿‘æœŸåˆ©ç”¨æ¨ç†æ¨¡å‹çš„è¶‹åŠ¿å·²æé«˜å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨æ¶‰åŠé€»è¾‘æ­¥éª¤çš„å¤šä¸ªä»»åŠ¡ä¸Šçš„æ€§èƒ½ã€‚å¯ä»¥ä»è¿™ç§æ¡†æ¶ä¸­å—ç›Šçš„è¯­è¨€ä»»åŠ¡ä¹‹ä¸€æ˜¯ä¹ è¯­æ£€æµ‹ï¼Œå› ä¸ºåœ¨ä¸€ä¸ªä¹ è¯­è¡¨è¾¾å¼è¢«ç†è§£ä¹‹åï¼Œæ‰èƒ½å¯¹å…¶è¿›è¡Œè§£æ­§ï¼Œå¹¶ä½œä¸ºæ¨ç†çš„åŸºç¡€ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æ¢è®¨äº†LLMä¸­çš„æ¨ç†èƒ½åŠ›å¦‚ä½•å½±å“ä¹ è¯­æ£€æµ‹æ€§èƒ½ï¼Œå¹¶ç ”ç©¶äº†æ¨¡å‹è§„æ¨¡çš„å½±å“ã€‚æˆ‘ä»¬è¯„ä¼°äº†ä½œä¸ºå¼€æºä»£è¡¨æ¨¡å‹çš„DeepSeek-R1è’¸é¦æ¨¡å‹ç³»åˆ—ï¼Œè¿™äº›æ¨¡å‹å‚æ•°ä»1.5Båˆ°70Bä¸ç­‰ï¼Œè·¨è¶Šå››ä¸ªä¹ è¯­æ£€æµ‹æ•°æ®é›†ã€‚æˆ‘ä»¬å‘ç°æ¨ç†çš„å½±å“æ¯”é¢„æœŸçš„è¦å°ä¸”æ›´åŠ å¤šå˜ã€‚å¯¹äºè¾ƒå°çš„æ¨¡å‹ï¼Œè¿›è¡Œæ€ç»´é“¾ï¼ˆCoTï¼‰æ¨ç†æé«˜äº†æ•°å­¦è°ƒæ•´ä¸­é—´æ¨¡å‹çš„æ€§èƒ½ï¼Œä½†å¹¶æœªè¾¾åˆ°åŸºç¡€æ¨¡å‹çš„æ°´å¹³ï¼Œè€Œè¾ƒå¤§çš„æ¨¡å‹ï¼ˆ14Bã€32Bå’Œ70Bï¼‰åˆ™æ˜¾ç¤ºå‡ºé€‚åº¦çš„æ”¹è¿›ã€‚æˆ‘ä»¬çš„æ·±å…¥åˆ†æè¡¨æ˜ï¼Œå¤§å‹æ¨¡å‹å¾ˆå¥½åœ°ç†è§£äº†ä¹ è¯­æ€§ï¼ŒæˆåŠŸåœ°å¯¹è¡¨è¾¾å¼ç»™å‡ºäº†å‡†ç¡®çš„å®šä¹‰ï¼Œè€Œå°å‹æ¨¡å‹å¾€å¾€æ— æ³•è¾“å‡ºå®é™…å«ä¹‰ã€‚å› æ­¤ï¼Œæˆ‘ä»¬è¿˜å°è¯•åœ¨å°å‹æ¨¡å‹çš„æç¤ºä¸­æä¾›å®šä¹‰ï¼Œæˆ‘ä»¬è¯æ˜åœ¨æŸäº›æƒ…å†µä¸‹è¿™å¯ä»¥æé«˜æ€§èƒ½ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.13365v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>åŸºäºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ¨ç†æ¨¡å‹è¶‹åŠ¿æå‡äº†é€»è¾‘ä»»åŠ¡ä¸Šçš„è¡¨ç°ã€‚æœ¬æ–‡æ¢è®¨äº†æ¨ç†èƒ½åŠ›å¯¹è¯­è¨€æ¨¡å‹åœ¨æˆè¯­æ£€æµ‹ä»»åŠ¡ä¸­çš„å½±å“ï¼Œå¹¶è€ƒå¯Ÿäº†æ¨¡å‹è§„æ¨¡çš„å½±å“ã€‚é€šè¿‡å¯¹ä¸åŒå‚æ•°çš„DeepSeek-R1è’¸é¦æ¨¡å‹åœ¨å››ä¸ªæˆè¯­æ£€æµ‹æ•°æ®é›†ä¸Šçš„è¯„ä¼°ï¼Œå‘ç°æ¨ç†çš„å½±å“è¾ƒå°ä¸”å˜åŒ–è¾ƒå¤§ã€‚å¯¹äºè¾ƒå°çš„æ¨¡å‹ï¼Œé€šè¿‡é“¾å¼æ€ç»´ï¼ˆCoTï¼‰æ¨ç†å¯ä»¥æé«˜æ€§èƒ½ï¼Œä½†æœªè¾¾åˆ°åŸºç¡€æ¨¡å‹çš„æ°´å¹³ï¼›è€Œè¾ƒå¤§çš„æ¨¡å‹æ˜¾ç¤ºå‡ºæ¸©å’Œçš„æå‡ã€‚æ·±å…¥çš„åˆ†æè¡¨æ˜ï¼Œå¤§æ¨¡å‹èƒ½å¤Ÿæ›´å¥½åœ°ç†è§£æˆè¯­å«ä¹‰ï¼ŒæˆåŠŸç»™å‡ºå‡†ç¡®çš„å®šä¹‰è¡¨è¾¾ï¼Œè€Œå°æ¨¡å‹åˆ™ç»å¸¸æ— æ³•è¾“å‡ºå®é™…æ„ä¹‰ã€‚ä¸ºæ­¤ï¼Œå®éªŒé€šè¿‡ç»™å°å‹æ¨¡å‹çš„æç¤ºæä¾›å®šä¹‰æ¥æé«˜æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<p>ä»¥ä¸‹æ˜¯å…³äºè¯¥æ–‡æœ¬çš„å…³é”®è§è§£ï¼š</p>
<ul>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ¨ç†æ¨¡å‹è¶‹åŠ¿å·²æé«˜äº†é€»è¾‘ä»»åŠ¡ä¸Šçš„æ€§èƒ½ã€‚</li>
<li>æ¨ç†èƒ½åŠ›å¯¹è¯­è¨€æ¨¡å‹åœ¨æˆè¯­æ£€æµ‹ä»»åŠ¡ä¸­çš„å½±å“è¢«ç ”ç©¶ã€‚</li>
<li>æ¨¡å‹è§„æ¨¡å¯¹æˆè¯­æ£€æµ‹æ€§èƒ½çš„å½±å“è¢«è€ƒå¯Ÿã€‚</li>
<li>æ¨ç†çš„å½±å“è¾ƒå°ä¸”å˜åŒ–è¾ƒå¤§ï¼Œå°å‹æ¨¡å‹çš„æ€§èƒ½æå‡æœ‰é™ã€‚</li>
<li>å¤§æ¨¡å‹èƒ½å¤Ÿæ›´å¥½åœ°ç†è§£æˆè¯­å«ä¹‰ï¼Œç»™å‡ºå‡†ç¡®çš„å®šä¹‰ã€‚</li>
<li>å°æ¨¡å‹åœ¨è¾“å‡ºå®é™…æ„ä¹‰ä¸Šç»å¸¸å¤±è´¥ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.13365">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-07578cfe56e8b12df746962411121058.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b293be6cd86242231419c454ac12d056.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d6b6d711ad9be55da4a7479810072e15.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-0512fd47807118649ccead68816ab26b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4cebd715551dae155abfcc61b68c038b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6a6ae02689a68dcbabd9fd729bbf4f91.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-467254c90b30c102bdb295b33c149aba.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="Explicit-v-s-Implicit-Memory-Exploring-Multi-hop-Complex-Reasoning-Over-Personalized-Information"><a href="#Explicit-v-s-Implicit-Memory-Exploring-Multi-hop-Complex-Reasoning-Over-Personalized-Information" class="headerlink" title="Explicit v.s. Implicit Memory: Exploring Multi-hop Complex Reasoning   Over Personalized Information"></a>Explicit v.s. Implicit Memory: Exploring Multi-hop Complex Reasoning   Over Personalized Information</h2><p><strong>Authors:Zeyu Zhang, Yang Zhang, Haoran Tan, Rui Li, Xu Chen</strong></p>
<p>In large language model-based agents, memory serves as a critical capability for achieving personalization by storing and utilizing usersâ€™ information. Although some previous studies have adopted memory to implement user personalization, they typically focus on preference alignment and simple question-answering. However, in the real world, complex tasks often require multi-hop reasoning on a large amount of user information, which poses significant challenges for current memory approaches. To address this limitation, we propose the multi-hop personalized reasoning task to explore how different memory mechanisms perform in multi-hop reasoning over personalized information. We explicitly define this task and construct a dataset along with a unified evaluation framework. Then, we implement various explicit and implicit memory methods and conduct comprehensive experiments. We evaluate their performance on this task from multiple perspectives and analyze their strengths and weaknesses. Besides, we explore hybrid approaches that combine both paradigms and propose the HybridMem method to address their limitations. We demonstrate the effectiveness of our proposed model through extensive experiments. To benefit the research community, we release this project at <a target="_blank" rel="noopener" href="https://github.com/nuster1128/MPR">https://github.com/nuster1128/MPR</a>. </p>
<blockquote>
<p>åœ¨åŸºäºå¤§å‹è¯­è¨€æ¨¡å‹çš„ä»£ç†ä¸­ï¼Œè®°å¿†å¯¹äºé€šè¿‡å­˜å‚¨å’Œåˆ©ç”¨ç”¨æˆ·ä¿¡æ¯æ¥å®ç°ä¸ªæ€§åŒ–è‡³å…³é‡è¦ã€‚è™½ç„¶ä¸€äº›æ—©æœŸçš„ç ”ç©¶å·²ç»é‡‡ç”¨è®°å¿†æ¥å®ç°ç”¨æˆ·ä¸ªæ€§åŒ–ï¼Œä½†å®ƒä»¬é€šå¸¸ä¾§é‡äºåå¥½å¯¹é½å’Œç®€å•é—®ç­”ã€‚ç„¶è€Œï¼Œåœ¨ç°å®ä¸–ç•Œï¼Œå¤æ‚çš„ä»»åŠ¡é€šå¸¸éœ€è¦åˆ©ç”¨å¤§é‡ç”¨æˆ·ä¿¡æ¯è¿›è¡Œå¤šè·³æ¨ç†ï¼Œè¿™å¯¹å½“å‰è®°å¿†æ–¹æ³•æ„æˆäº†é‡å¤§æŒ‘æˆ˜ã€‚ä¸ºäº†å…‹æœè¿™ä¸€å±€é™æ€§ï¼Œæˆ‘ä»¬æå‡ºäº†å¤šè·³ä¸ªæ€§åŒ–æ¨ç†ä»»åŠ¡ï¼Œä»¥æ¢ç´¢ä¸åŒè®°å¿†æœºåˆ¶åœ¨ä¸ªæ€§åŒ–ä¿¡æ¯ä¸Šçš„å¤šè·³æ¨ç†è¡¨ç°ã€‚æˆ‘ä»¬æ˜ç¡®å®šä¹‰äº†æ­¤ä»»åŠ¡ï¼Œå¹¶æ„å»ºäº†ä¸€ä¸ªæ•°æ®é›†ä»¥åŠä¸€ä¸ªç»Ÿä¸€çš„è¯„ä¼°æ¡†æ¶ã€‚ç„¶åï¼Œæˆ‘ä»¬å®ç°äº†å¤šç§æ˜¾å¼è®°å¿†æ–¹æ³•å’Œéšå¼è®°å¿†æ–¹æ³•ï¼Œå¹¶è¿›è¡Œäº†å…¨é¢çš„å®éªŒã€‚æˆ‘ä»¬ä»å¤šä¸ªè§’åº¦è¯„ä¼°äº†ä»–ä»¬åœ¨è¿™ä¸€ä»»åŠ¡ä¸Šçš„è¡¨ç°ï¼Œå¹¶åˆ†æäº†ä»–ä»¬çš„ä¼˜ç¼ºç‚¹ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜æ¢ç´¢äº†ç»“åˆè¿™ä¸¤ç§æ–¹æ³•çš„æ··åˆæ–¹æ³•ï¼Œå¹¶æå‡ºäº†HybridMemæ–¹æ³•æ¥è§£å†³å®ƒä»¬çš„å±€é™æ€§ã€‚æˆ‘ä»¬é€šè¿‡å¤§é‡å®éªŒè¯æ˜äº†æ‰€æå‡ºæ¨¡å‹çš„æœ‰æ•ˆæ€§ã€‚ä¸ºäº†é€ ç¦ç ”ç©¶ç¤¾åŒºï¼Œæˆ‘ä»¬åœ¨<a target="_blank" rel="noopener" href="https://github.com/nuster1128/MPR%E4%B8%8A%E5%8F%91%E5%B8%83%E4%BA%86%E6%AD%A4%E9%A1%B9%E7%9B%AE%E3%80%82">https://github.com/nuster1128/MPRä¸Šå‘å¸ƒäº†æ­¤é¡¹ç›®ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.13250v1">PDF</a> 15 pages, 13 figures, 3 tables</p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ä¸­çš„è®°å¿†èƒ½åŠ›å¯¹äºå®ç°ä¸ªæ€§åŒ–è‡³å…³é‡è¦ï¼Œå®ƒé€šè¿‡å­˜å‚¨å’Œåˆ©ç”¨ç”¨æˆ·ä¿¡æ¯æ¥å®ç°ä¸ªæ€§åŒ–ã€‚å…ˆå‰çš„ç ”ç©¶è™½ç„¶é‡‡ç”¨äº†è®°å¿†å®ç°ç”¨æˆ·ä¸ªæ€§åŒ–çš„æ–¹æ³•ï¼Œä½†å®ƒä»¬ä¸»è¦é›†ä¸­åœ¨åå¥½å¯¹é½å’Œç®€å•é—®ç­”ä¸Šã€‚ç°å®ä¸–ç•Œä¸­çš„å¤æ‚ä»»åŠ¡éœ€è¦å¤šè·³æ¨ç†ï¼Œè¿™å¯¹å½“å‰è®°å¿†æ–¹æ³•æå‡ºäº†æŒ‘æˆ˜ã€‚ä¸ºåº”å¯¹è¿™ä¸€æŒ‘æˆ˜ï¼Œæœ¬æ–‡æå‡ºäº†å¤šè·³ä¸ªæ€§åŒ–æ¨ç†ä»»åŠ¡ï¼Œæ¢ç´¢ä¸åŒè®°å¿†æœºåˆ¶åœ¨ä¸ªæ€§åŒ–ä¿¡æ¯ä¸Šçš„å¤šè·³æ¨ç†è¡¨ç°ã€‚æœ¬æ–‡å®šä¹‰äº†è¿™ä¸ªä»»åŠ¡ï¼Œæ„å»ºäº†æ•°æ®é›†å’Œç»Ÿä¸€è¯„ä¼°æ¡†æ¶ï¼Œå®ç°äº†å„ç§æ˜¾æ€§å’Œéšæ€§è®°å¿†æ–¹æ³•ï¼Œå¹¶è¿›è¡Œäº†ç»¼åˆå®éªŒè¯„ä¼°ã€‚æ­¤å¤–ï¼Œè¿˜æ¢ç´¢äº†ç»“åˆä¸¤ç§æ–¹æ³•çš„æ··åˆæ–¹æ³•ï¼Œå¹¶æå‡ºäº†HybridMemæ–¹æ³•æ¥è§£å†³å…¶å±€é™æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è®°å¿†èƒ½åŠ›åœ¨å¤§å‹è¯­è¨€æ¨¡å‹ä¸­çš„é‡è¦ä½œç”¨ï¼šå­˜å‚¨å’Œåˆ©ç”¨ç”¨æˆ·ä¿¡æ¯ä»¥å®ç°ä¸ªæ€§åŒ–ã€‚</li>
<li>å…ˆå‰ç ”ç©¶ä¸»è¦é›†ä¸­åœ¨ç®€å•é—®ç­”å’Œåå¥½å¯¹é½ä¸Šï¼Œå¯¹äºå¤æ‚ä»»åŠ¡çš„å¤šè·³æ¨ç†æŒ‘æˆ˜åº”å¯¹ä¸è¶³ã€‚</li>
<li>æå‡ºå¤šè·³ä¸ªæ€§åŒ–æ¨ç†ä»»åŠ¡ï¼Œä»¥æ¢ç´¢è®°å¿†æœºåˆ¶åœ¨ä¸ªæ€§åŒ–ä¿¡æ¯ä¸Šçš„å¤šè·³æ¨ç†è¡¨ç°ã€‚</li>
<li>æ„å»ºäº†æ•°æ®é›†å’Œç»Ÿä¸€è¯„ä¼°æ¡†æ¶ï¼Œä»¥æ”¯æŒè¯¥ä»»åŠ¡çš„ç ”ç©¶ã€‚</li>
<li>å®ç°äº†å¤šç§æ˜¾æ€§å’Œéšæ€§è®°å¿†æ–¹æ³•ï¼Œå¹¶è¿›è¡Œäº†ç»¼åˆå®éªŒè¯„ä¼°å…¶æ€§èƒ½ã€‚</li>
<li>æå‡ºæ··åˆæ–¹æ³•ç»“åˆä¸¤ç§è®°å¿†æ–¹æ³•ï¼Œè§£å†³å…¶å±€é™æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.13250">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-5b005c4467b57c5e6774210b9b705d63.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d42ca0b682344cb0017fdc0e524888d0.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-50a8c3f383deebb5419b0354ec7d0576.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d5a537572d23877ca1c9ce957f9e08d2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c87e49a7826d79bed09416eaa1aecb42.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-af1d41b86d8a20fb43989c798e26cf4c.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="RISE-Enhancing-VLM-Image-Annotation-with-Self-Supervised-Reasoning"><a href="#RISE-Enhancing-VLM-Image-Annotation-with-Self-Supervised-Reasoning" class="headerlink" title="RISE: Enhancing VLM Image Annotation with Self-Supervised Reasoning"></a>RISE: Enhancing VLM Image Annotation with Self-Supervised Reasoning</h2><p><strong>Authors:Suhang Hu, Wei Hu, Yuhang Su, Fan Zhang</strong></p>
<p>Vision-Language Models (VLMs) struggle with complex image annotation tasks, such as emotion classification and context-driven object detection, which demand sophisticated reasoning. Standard Supervised Fine-Tuning (SFT) focuses solely on annotation outcomes, ignoring underlying rationales, while Visual Reinforcement Fine-Tuning (Visual-RFT) produces inconsistent Chains of Thought (CoTs) due to the absence of high-quality, verified CoTs during pre-training. We introduce RISE (Reason-Inspire-Strengthen-Expertise), a two-stage framework to overcome these limitations. In the Reason stage (RISE-CoT), a reinforcement learning-driven â€œannotation-reasoning-annotationâ€ closed-loop generates visually grounded, logically consistent CoTs by verifying their ability to reconstruct original annotations without direct leakage. The Inspire and Strengthen stage (RISE-R1) leverages a high-quality CoT subset, filtered by RISE-CoT rewards, for supervised fine-tuning, followed by reinforcement fine-tuning to produce interpretable reasoning and accurate annotations, achieving Expertise in complex visual tasks. Evaluated on complex and simple image annotation tasks, RISE-trained Qwen2-VL-2B outperforms SFT and Visual-RFT, achieving robust performance and enhanced explainability. RISE offers a self-supervised solution for advancing VLM reasoning without requiring manually annotated CoTs. </p>
<blockquote>
<p>è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰åœ¨å¤„ç†å¤æ‚çš„å›¾åƒæ ‡æ³¨ä»»åŠ¡æ—¶é¢ä¸´æŒ‘æˆ˜ï¼Œä¾‹å¦‚æƒ…æ„Ÿåˆ†ç±»å’Œä¸Šä¸‹æ–‡é©±åŠ¨çš„å¯¹è±¡æ£€æµ‹ç­‰éœ€è¦å¤æ‚æ¨ç†çš„ä»»åŠ¡ã€‚æ ‡å‡†ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰åªå…³æ³¨æ ‡æ³¨ç»“æœï¼Œå¿½ç•¥äº†æ½œåœ¨çš„æ¨ç†è¿‡ç¨‹ï¼Œè€Œè§†è§‰å¼ºåŒ–å¾®è°ƒï¼ˆVisual-RFTï¼‰åˆ™ç”±äºé¢„è®­ç»ƒæœŸé—´ç¼ºä¹é«˜è´¨é‡ã€ç»è¿‡éªŒè¯çš„æ¨ç†é“¾ï¼ˆCoTsï¼‰ï¼Œäº§ç”Ÿäº†ä¸ä¸€è‡´çš„æ¨ç†é“¾ã€‚æˆ‘ä»¬å¼•å…¥äº†RISEï¼ˆReason-Inspire-Strengthen-Expertiseï¼‰ï¼Œè¿™æ˜¯ä¸€ä¸ªä¸¤é˜¶æ®µçš„æ¡†æ¶ï¼Œæ—¨åœ¨å…‹æœè¿™äº›é™åˆ¶ã€‚åœ¨æ¨ç†é˜¶æ®µï¼ˆRISE-CoTï¼‰ï¼Œä¸€ä¸ªå¼ºåŒ–å­¦ä¹ é©±åŠ¨çš„â€œæ ‡æ³¨-æ¨ç†-æ ‡æ³¨â€é—­ç¯é€šè¿‡éªŒè¯å…¶é‡å»ºåŸå§‹æ ‡æ³¨çš„èƒ½åŠ›ï¼Œç”Ÿæˆè§†è§‰åŸºç¡€ã€é€»è¾‘ä¸€è‡´çš„æ¨ç†é“¾ï¼Œè€Œæ— éœ€ç›´æ¥æ³„éœ²ä¿¡æ¯ã€‚åœ¨æ¿€åŠ±å’Œå¼ºåŒ–é˜¶æ®µï¼ˆRISE-R1ï¼‰åˆ™åˆ©ç”¨RISE-CoTå¥–åŠ±è¿‡æ»¤å‡ºçš„é«˜è´¨é‡æ¨ç†é“¾å­é›†è¿›è¡Œæœ‰ç›‘ç£å¾®è°ƒï¼Œç„¶åè¿›è¡Œå¼ºåŒ–å¾®è°ƒï¼Œä»¥äº§ç”Ÿå¯è§£é‡Šçš„æ¨ç†å’Œå‡†ç¡®çš„æ ‡æ³¨ï¼Œåœ¨å¤æ‚è§†è§‰ä»»åŠ¡ä¸­å®ç°ä¸“ä¸šçº§è¡¨ç°ã€‚åœ¨å¤æ‚å’Œç®€å•çš„å›¾åƒæ ‡æ³¨ä»»åŠ¡ä¸Šï¼Œç»è¿‡RISEè®­ç»ƒçš„Qwen2-VL-2Bè¶…è¶Šäº†SFTå’ŒVisual-RFTï¼Œå®ç°äº†ç¨³å¥çš„æ€§èƒ½å’Œå¢å¼ºçš„å¯è§£é‡Šæ€§ã€‚RISEæä¾›äº†ä¸€ç§è‡ªç›‘ç£çš„è§£å†³æ–¹æ¡ˆï¼Œæ— éœ€æ‰‹åŠ¨æ³¨é‡Šçš„æ¨ç†é“¾å³å¯æ¨åŠ¨VLMçš„æ¨ç†èƒ½åŠ›è¿›æ­¥ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.13229v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†Vision-Language Modelsï¼ˆVLMsï¼‰åœ¨å¤„ç†å¤æ‚å›¾åƒæ ‡æ³¨ä»»åŠ¡æ—¶çš„æŒ‘æˆ˜ï¼Œå¦‚æƒ…æ„Ÿåˆ†ç±»å’Œä¸Šä¸‹æ–‡é©±åŠ¨çš„ç›®æ ‡æ£€æµ‹ã€‚ä¸ºè§£å†³æ ‡å‡†ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰å’Œè§†è§‰å¼ºåŒ–å¾®è°ƒï¼ˆVisual-RFTï¼‰çš„å±€é™æ€§ï¼Œæå‡ºäº†ä¸€ç§åä¸ºRISEçš„ä¸¤é˜¶æ®µæ¡†æ¶ã€‚è¯¥æ¡†æ¶é€šè¿‡ç”Ÿæˆè§†è§‰åŸºç¡€ã€é€»è¾‘ä¸€è‡´çš„Chains of Thoughtï¼ˆCoTsï¼‰æ¥æé«˜VLMsçš„æ¨ç†èƒ½åŠ›ï¼Œå¹¶åœ¨å¤æ‚è§†è§‰ä»»åŠ¡ä¸­å®ç°ä¸“ä¸šæ€§èƒ½ã€‚ç»è¿‡RISEè®­ç»ƒçš„Qwen2-VL-2Båœ¨å¤æ‚å’Œç®€å•çš„å›¾åƒæ ‡æ³¨ä»»åŠ¡ä¸Šè¡¨ç°å‡ºå“è¶Šçš„æ€§èƒ½å’Œå¢å¼ºçš„å¯è§£é‡Šæ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>VLMsåœ¨å¤„ç†å¤æ‚å›¾åƒæ ‡æ³¨ä»»åŠ¡æ—¶é¢ä¸´æŒ‘æˆ˜ï¼Œéœ€è¦é«˜çº§æ¨ç†èƒ½åŠ›ã€‚</li>
<li>æ ‡å‡†ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰å’Œè§†è§‰å¼ºåŒ–å¾®è°ƒï¼ˆVisual-RFTï¼‰å­˜åœ¨å±€é™æ€§ï¼Œæ— æ³•å®Œå…¨æ»¡è¶³è¿™äº›éœ€æ±‚ã€‚</li>
<li>RISEæ¡†æ¶é€šè¿‡ç”Ÿæˆè§†è§‰åŸºç¡€ã€é€»è¾‘ä¸€è‡´çš„Chains of Thoughtï¼ˆCoTsï¼‰æ¥æé«˜VLMsçš„æ¨ç†èƒ½åŠ›ã€‚</li>
<li>RISEæ¡†æ¶åŒ…æ‹¬ä¸¤ä¸ªé˜¶æ®µï¼šReasoné˜¶æ®µï¼ˆRISE-CoTï¼‰ç”ŸæˆéªŒè¯åçš„CoTsï¼ŒInspireå’ŒStrengthené˜¶æ®µï¼ˆRISE-R1ï¼‰åˆ©ç”¨é«˜è´¨é‡çš„CoTså­é›†è¿›è¡Œç›‘ç£å’Œå¼ºåŒ–å¾®è°ƒã€‚</li>
<li>RISEè®­ç»ƒåçš„æ¨¡å‹åœ¨å¤æ‚å’Œç®€å•çš„å›¾åƒæ ‡æ³¨ä»»åŠ¡ä¸Šè¡¨ç°å‡ºå“è¶Šæ€§èƒ½ã€‚</li>
<li>RISEæ¡†æ¶æä¾›äº†ä¸€ç§è‡ªç›‘ç£çš„è§£å†³æ–¹æ¡ˆï¼Œå¯æ¨åŠ¨VLMæ¨ç†çš„è¿›æ­¥ï¼Œæ— éœ€æ‰‹åŠ¨æ³¨é‡Šçš„CoTsã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.13229">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-19d67bf1eecce990cb57b709fede8265.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-80f1dd924b767bb651fea150c2ca477e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-663f9d48e800f227c38b5aec6a7e1e39.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7699be2eda3ac4f0c1e5de25eec7e0f8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a12cc7a6d8cef0afcb131e31a2178cb3.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="PC-Sampler-Position-Aware-Calibration-of-Decoding-Bias-in-Masked-Diffusion-Models"><a href="#PC-Sampler-Position-Aware-Calibration-of-Decoding-Bias-in-Masked-Diffusion-Models" class="headerlink" title="PC-Sampler: Position-Aware Calibration of Decoding Bias in Masked   Diffusion Models"></a>PC-Sampler: Position-Aware Calibration of Decoding Bias in Masked   Diffusion Models</h2><p><strong>Authors:Pengcheng Huang, Shuhao Liu, Zhenghao Liu, Yukun Yan, Shuo Wang, Zulong Chen, Tong Xiao</strong></p>
<p>Recent advances in masked diffusion models (MDMs) have established them as powerful non-autoregressive alternatives for sequence generation. Nevertheless, our preliminary experiments reveal that the generation quality of MDMs is still highly sensitive to the choice of decoding strategy. In particular, widely adopted uncertainty-based samplers suffer from two key limitations: a lack of global trajectory control and a pronounced bias toward trivial tokens in the early stages of decoding. These shortcomings restrict the full potential of MDMs. In this work, we introduce Position-Aware Confidence-Calibrated Sampling (PC-Sampler), a novel decoding strategy that unifies global trajectory planning with content-aware informativeness maximization. PC-Sampler incorporates a position-aware weighting mechanism to regulate the decoding path and a calibrated confidence score to suppress the premature selection of trivial tokens. Extensive experiments on three advanced MDMs across seven challenging benchmarks-including logical reasoning and planning tasks-demonstrate that PC-Sampler consistently outperforms existing MDM decoding strategies by more than 10% on average, significantly narrowing the performance gap with state-of-the-art autoregressive models. All codes are available at <a target="_blank" rel="noopener" href="https://github.com/NEUIR/PC-Sampler">https://github.com/NEUIR/PC-Sampler</a>. </p>
<blockquote>
<p>è¿‘æœŸï¼Œæ©è†œæ‰©æ•£æ¨¡å‹ï¼ˆMDMsï¼‰çš„è¿›å±•ä¸ºåºåˆ—ç”Ÿæˆæä¾›äº†å¼ºå¤§çš„éè‡ªå›å½’æ›¿ä»£æ–¹æ¡ˆã€‚ç„¶è€Œï¼Œæˆ‘ä»¬çš„åˆæ­¥å®éªŒè¡¨æ˜ï¼ŒMDMsçš„ç”Ÿæˆè´¨é‡å¯¹è§£ç ç­–ç•¥çš„é€‰æ‹©ä»ç„¶é«˜åº¦æ•æ„Ÿã€‚ç‰¹åˆ«æ˜¯å¹¿æ³›é‡‡ç”¨çš„åŸºäºä¸ç¡®å®šæ€§çš„é‡‡æ ·å™¨å­˜åœ¨ä¸¤å¤§å±€é™ï¼šç¼ºä¹å…¨å±€è½¨è¿¹æ§åˆ¶å’Œåœ¨è§£ç æ—©æœŸé˜¶æ®µå¯¹å¹³å‡¡æ ‡è®°çš„æ˜æ˜¾åå‘ã€‚è¿™äº›ç¼ºç‚¹é™åˆ¶äº†MDMsçš„æ½œåŠ›ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä½ç½®æ„ŸçŸ¥ç½®ä¿¡æ ¡å‡†é‡‡æ ·ï¼ˆPC-Samplerï¼‰ï¼Œè¿™æ˜¯ä¸€ç§æ–°çš„è§£ç ç­–ç•¥ï¼Œå®ƒå°†å…¨å±€è½¨è¿¹è§„åˆ’ä¸å†…å®¹æ„ŸçŸ¥çš„ä¿¡æ¯æœ€å¤§åŒ–ç›¸ç»“åˆã€‚PC-Sampleré‡‡ç”¨ä½ç½®æ„ŸçŸ¥åŠ æƒæœºåˆ¶æ¥è°ƒèŠ‚è§£ç è·¯å¾„ï¼Œå¹¶ä½¿ç”¨æ ¡å‡†ç½®ä¿¡åº¦æ¥æŠ‘åˆ¶å¹³å‡¡æ ‡è®°çš„è¿‡æ—©é€‰æ‹©ã€‚åœ¨ä¸‰ä¸ªå…ˆè¿›çš„MDMså’Œä¸ƒä¸ªå…·æœ‰æŒ‘æˆ˜æ€§çš„åŸºå‡†æµ‹è¯•ï¼ˆåŒ…æ‹¬é€»è¾‘æ¨ç†å’Œä»»åŠ¡è§„åˆ’ï¼‰ä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼ŒPC-Sampleråœ¨å¹³å‡æ„ä¹‰ä¸Šå§‹ç»ˆä¼˜äºç°æœ‰çš„MDMè§£ç ç­–ç•¥ï¼Œè¶…è¿‡10%ï¼Œæ˜¾è‘—ç¼©å°äº†ä¸æœ€æ–°è‡ªå›å½’æ¨¡å‹ä¹‹é—´çš„æ€§èƒ½å·®è·ã€‚æ‰€æœ‰ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/NEUIR/PC-Sampler%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/NEUIR/PC-Sampleræ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.13021v2">PDF</a> 17 pages,13 figures</p>
<p><strong>Summary</strong></p>
<p>è¿‘æœŸï¼Œæ©ç æ‰©æ•£æ¨¡å‹ï¼ˆMDMsï¼‰æˆä¸ºéè‡ªå›å½’åºåˆ—ç”Ÿæˆä»»åŠ¡çš„å¼ºå¤§å·¥å…·ã€‚ä½†åˆæ­¥å®éªŒè¡¨æ˜ï¼ŒMDMsçš„ç”Ÿæˆè´¨é‡å¯¹è§£ç ç­–ç•¥çš„é€‰æ‹©éå¸¸æ•æ„Ÿã€‚å½“å‰å¹¿æ³›é‡‡ç”¨çš„ä¸ç¡®å®šæ€§é‡‡æ ·å™¨å­˜åœ¨å…¨å±€è½¨è¿¹æ§åˆ¶ä¸è¶³å’Œæ—©æœŸè§£ç é˜¶æ®µæ˜æ˜¾åå‘å¹³å‡¡ä»£å¸çš„ä¸¤å¤§å±€é™ã€‚æœ¬ç ”ç©¶æå‡ºä½ç½®æ„ŸçŸ¥ç½®ä¿¡æ ¡å‡†é‡‡æ ·ï¼ˆPC-Samplerï¼‰è¿™ä¸€æ–°é¢–è§£ç ç­–ç•¥ï¼Œç»“åˆå…¨å±€è½¨è¿¹è§„åˆ’ä¸å†…å®¹æ„ŸçŸ¥ä¿¡æ¯é‡æœ€å¤§åŒ–ã€‚PC-Sampleré€šè¿‡ä½ç½®æ„ŸçŸ¥åŠ æƒæœºåˆ¶è°ƒæ§è§£ç è·¯å¾„ï¼Œå¹¶ç”¨æ ¡å‡†ç½®ä¿¡åº¦å¾—åˆ†æŠ‘åˆ¶å¹³å‡¡ä»£å¸çš„è¿‡æ—©é€‰æ‹©ã€‚åœ¨å¤šä¸ªå…ˆè¿›MDMså’Œä¸ƒä¸ªæŒ‘æˆ˜æ€§åŸºå‡†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒPC-Samplerå¹³å‡æ€§èƒ½è¶…å‡ºç°æœ‰MDMè§£ç ç­–ç•¥10%ä»¥ä¸Šï¼Œæ˜¾è‘—ç¼©å°äº†ä¸é¡¶å°–è‡ªå›å½’æ¨¡å‹çš„æ€§èƒ½å·®è·ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ©ç æ‰©æ•£æ¨¡å‹ï¼ˆMDMsï¼‰å·²æˆä¸ºéè‡ªå›å½’åºåˆ—ç”Ÿæˆçš„æœ‰åŠ›å·¥å…·ã€‚</li>
<li>åˆæ­¥å®éªŒå‘ç°ï¼ŒMDMsçš„ç”Ÿæˆè´¨é‡å¯¹è§£ç ç­–ç•¥é€‰æ‹©æ•æ„Ÿã€‚</li>
<li>ç°æœ‰ä¸ç¡®å®šæ€§é‡‡æ ·å™¨å­˜åœ¨å…¨å±€è½¨è¿¹æ§åˆ¶ä¸è¶³å’Œæ—©æœŸè§£ç é˜¶æ®µåå‘å¹³å‡¡ä»£å¸çš„é—®é¢˜ã€‚</li>
<li>æå‡ºæ–°çš„è§£ç ç­–ç•¥â€”â€”ä½ç½®æ„ŸçŸ¥ç½®ä¿¡æ ¡å‡†é‡‡æ ·ï¼ˆPC-Samplerï¼‰ã€‚</li>
<li>PC-Samplerç»“åˆå…¨å±€è½¨è¿¹è§„åˆ’ä¸å†…å®¹æ„ŸçŸ¥ä¿¡æ¯é‡æœ€å¤§åŒ–ã€‚</li>
<li>PC-Sampleré€šè¿‡ä½ç½®æ„ŸçŸ¥åŠ æƒå’Œæ ¡å‡†ç½®ä¿¡åº¦å¾—åˆ†è§£å†³ç°æœ‰é‡‡æ ·å™¨çš„ç¼ºé™·ã€‚</li>
<li>å®éªŒè¡¨æ˜ï¼ŒPC-Sampleråœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­æ€§èƒ½æ˜¾è‘—ä¼˜äºç°æœ‰MDMè§£ç ç­–ç•¥ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.13021">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-a2c124633cc78d4cd86f298bda59820e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-dce819c0b616bcde2a4cc3476e5eee22.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-fcd32d1401bf26ee9a4bce80d4bb61a1.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-4ca5349275876a2543c9135fb1a81a0b.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="Atom-Searcher-Enhancing-Agentic-Deep-Research-via-Fine-Grained-Atomic-Thought-Reward"><a href="#Atom-Searcher-Enhancing-Agentic-Deep-Research-via-Fine-Grained-Atomic-Thought-Reward" class="headerlink" title="Atom-Searcher: Enhancing Agentic Deep Research via Fine-Grained Atomic   Thought Reward"></a>Atom-Searcher: Enhancing Agentic Deep Research via Fine-Grained Atomic   Thought Reward</h2><p><strong>Authors:Yong Deng, Guoqing Wang, Zhenzhe Ying, Xiaofeng Wu, Jinzhen Lin, Wenwen Xiong, Yuqin Dai, Shuo Yang, Zhanwei Zhang, Qiwen Wang, Yang Qin, Changhua Meng</strong></p>
<p>Large language models (LLMs) exhibit remarkable problem-solving abilities, but struggle with complex tasks due to static internal knowledge. Retrieval-Augmented Generation (RAG) enhances access to external information, yet remains limited in multi-hop reasoning and strategic search due to rigid workflows. Recent advancements in agentic deep research empower LLMs to autonomously reason, search, and synthesize information. However, current approaches relying on outcome-based reinforcement learning (RL) face critical issues such as conflicting gradients and reward sparsity, limiting performance gains and training efficiency. To address these, we first propose Atomic Thought, a novel LLM thinking paradigm that decomposes reasoning into fine-grained functional units. These units are supervised by Reasoning Reward Models (RRMs), which provide Atomic Thought Rewards (ATR) for fine-grained guidance. Building on this, we propose Atom-Searcher, a novel RL framework for agentic deep research that integrates Atomic Thought and ATR. Atom-Searcher uses a curriculum-inspired reward schedule, prioritizing process-level ATR early and transitioning to outcome rewards, accelerating convergence on effective reasoning paths. Experiments on seven benchmarks show consistent improvements over the state-of-the-art. Key advantages include: (1) Atom-Searcher scales computation at test-time. (2) Atomic Thought provides supervision anchors for RRMs, bridging deep research tasks and RRMs. (3) Atom-Searcher exhibits more interpretable, human-like reasoning patterns. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å±•ç°å‡ºå“è¶Šçš„è§£å†³é—®é¢˜èƒ½åŠ›ï¼Œä½†ç”±äºå†…éƒ¨çŸ¥è¯†çš„é™æ€æ€§ï¼Œåœ¨åº”å¯¹å¤æ‚ä»»åŠ¡æ—¶é‡åˆ°å›°éš¾ã€‚æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰å¢å¼ºäº†å¯¹å¤–éƒ¨ä¿¡æ¯çš„è®¿é—®èƒ½åŠ›ï¼Œä½†ç”±äºå·¥ä½œæµç¨‹åƒµåŒ–ï¼Œåœ¨å¤šå±‚æ¬¡æ¨ç†å’Œç­–ç•¥æœç´¢æ–¹é¢ä»å­˜åœ¨å±€é™ã€‚æœ€è¿‘çš„æ·±åº¦ç ”ç©¶ä»£ç†è¿›æ­¥ä½¿LLMèƒ½å¤Ÿè‡ªä¸»æ¨ç†ã€æœç´¢å’Œåˆæˆä¿¡æ¯ã€‚ç„¶è€Œï¼Œå½“å‰ä¾èµ–ç»“æœåŸºäºå¢å¼ºå­¦ä¹ ï¼ˆRLï¼‰çš„æ–¹æ³•é¢ä¸´å…³é”®æ€§é—®é¢˜ï¼Œå¦‚æ¢¯åº¦å†²çªå’Œå¥–åŠ±ç¨€ç–ï¼Œè¿™é™åˆ¶äº†æ€§èƒ½æå‡å’Œè®­ç»ƒæ•ˆç‡ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.12800v2">PDF</a> </p>
<p><strong>Summary</strong>ï¼š<br>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å…·æœ‰å‡ºè‰²çš„é—®é¢˜è§£å†³èƒ½åŠ›ï¼Œä½†åœ¨å¤æ‚ä»»åŠ¡æ–¹é¢å­˜åœ¨é™æ€çŸ¥è¯†é™åˆ¶ã€‚æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰æé«˜äº†å¯¹å¤–éƒ¨ä¿¡æ¯çš„è®¿é—®èƒ½åŠ›ï¼Œä½†åœ¨å¤šè·³æ¨ç†å’Œç­–ç•¥æœç´¢æ–¹é¢ä»å­˜åœ¨åˆšæ€§å·¥ä½œæµç¨‹çš„é™åˆ¶ã€‚æœ€è¿‘çš„ç ”ç©¶è¿›å±•ä½¿å¾—LLMèƒ½å¤Ÿè‡ªä¸»æ¨ç†ã€æœç´¢å’Œåˆæˆä¿¡æ¯ã€‚ç„¶è€Œï¼Œå½“å‰ä¾èµ–ç»“æœåŸºäºå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰çš„æ–¹æ³•é¢ä¸´å…³é”®æŒ‘æˆ˜ï¼Œå¦‚æ¢¯åº¦å†²çªå’Œå¥–åŠ±ç¨€ç–æ€§ï¼Œé™åˆ¶äº†æ€§èƒ½æå‡å’ŒåŸ¹è®­æ•ˆç‡ã€‚ä¸ºè§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†åŸå­æ€ç»´è¿™ä¸€æ–°å‹LLMæ€è€ƒèŒƒå¼ï¼Œå°†æ¨ç†åˆ†è§£ä¸ºç²¾ç»†çš„åŠŸèƒ½å•å…ƒï¼Œå¹¶ç”±æ¨ç†å¥–åŠ±æ¨¡å‹ï¼ˆRRMï¼‰æä¾›åŸå­æ€ç»´å¥–åŠ±ï¼ˆATRï¼‰è¿›è¡Œç²¾ç»†æŒ‡å¯¼ã€‚åœ¨æ­¤åŸºç¡€ä¸Šï¼Œæˆ‘ä»¬æå‡ºäº†Atom-Searcherè¿™ä¸€æ–°å‹çš„RLæ¡†æ¶ï¼Œèåˆäº†åŸå­æ€ç»´å’ŒATRã€‚Atom-Searcheré‡‡ç”¨è¯¾ç¨‹å¼å¥–åŠ±æ—¶é—´è¡¨ï¼Œæ—©æœŸä¾§é‡äºè¿‡ç¨‹çº§çš„ATRï¼Œç„¶åè¿‡æ¸¡åˆ°ç»“æœå¥–åŠ±ï¼ŒåŠ é€Ÿåœ¨æœ‰æ•ˆæ¨ç†è·¯å¾„ä¸Šçš„æ”¶æ•›ã€‚åœ¨ä¸ƒä¸ªåŸºå‡†æµ‹è¯•ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œä¸æœ€æ–°æŠ€æœ¯ç›¸æ¯”ï¼ŒAtom-Searcherå…·æœ‰ä¸€è‡´çš„æ”¹è¿›ä¼˜åŠ¿ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨å¤æ‚ä»»åŠ¡æ–¹é¢å­˜åœ¨é™æ€çŸ¥è¯†é™åˆ¶ã€‚</li>
<li>æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰åœ¨å¢å¼ºå¯¹å¤–éƒ¨ä¿¡æ¯çš„è®¿é—®èƒ½åŠ›çš„åŒæ—¶ï¼Œä»é¢ä¸´å¤šè·³æ¨ç†å’Œç­–ç•¥æœç´¢çš„åˆšæ€§å·¥ä½œæµç¨‹é™åˆ¶ã€‚</li>
<li>æœ€æ–°ç ”ç©¶è¿›å±•ä½¿å¾—LLMèƒ½å¤Ÿè‡ªä¸»æ¨ç†ã€æœç´¢å’Œåˆæˆä¿¡æ¯ã€‚</li>
<li>å½“å‰å¼ºåŒ–å­¦ä¹ æ–¹æ³•é¢ä¸´æ¢¯åº¦å†²çªå’Œå¥–åŠ±ç¨€ç–æ€§æŒ‘æˆ˜ã€‚</li>
<li>åŸå­æ€ç»´æ˜¯ä¸€ç§æ–°å‹çš„LLMæ€è€ƒèŒƒå¼ï¼Œå°†æ¨ç†åˆ†è§£ä¸ºç²¾ç»†çš„åŠŸèƒ½å•å…ƒï¼Œç”±æ¨ç†å¥–åŠ±æ¨¡å‹ï¼ˆRRMï¼‰æä¾›å¥–åŠ±è¿›è¡ŒæŒ‡å¯¼ã€‚</li>
<li>Atom-Searcheræ˜¯èåˆäº†åŸå­æ€ç»´å’ŒATRçš„RLæ¡†æ¶ï¼Œé‡‡ç”¨è¯¾ç¨‹å¼å¥–åŠ±æ—¶é—´è¡¨ä»¥åŠ é€Ÿæ”¶æ•›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.12800">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-f17e48c16e8f9ec35d90c2f4034cb14f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6d71e84c2ed3b992e40dd7cbadded5b4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-847fe97327e36bfe692c937f850560ee.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="TaoSR1-The-Thinking-Model-for-E-commerce-Relevance-Search"><a href="#TaoSR1-The-Thinking-Model-for-E-commerce-Relevance-Search" class="headerlink" title="TaoSR1: The Thinking Model for E-commerce Relevance Search"></a>TaoSR1: The Thinking Model for E-commerce Relevance Search</h2><p><strong>Authors:Chenhe Dong, Shaowei Yao, Pengkun Jiao, Jianhui Yang, Yiming Jin, Zerui Huang, Xiaojiang Zhou, Dan Ou, Haihong Tang</strong></p>
<p>Query-product relevance prediction is a core task in e-commerce search. BERT-based models excel at semantic matching but lack complex reasoning capabilities. While Large Language Models (LLMs) are explored, most still use discriminative fine-tuning or distill to smaller models for deployment. We propose a framework to directly deploy LLMs for this task, addressing key challenges: Chain-of-Thought (CoT) error accumulation, discriminative hallucination, and deployment feasibility. Our framework, TaoSR1, involves three stages: (1) Supervised Fine-Tuning (SFT) with CoT to instill reasoning; (2) Offline sampling with a pass@N strategy and Direct Preference Optimization (DPO) to improve generation quality; and (3) Difficulty-based dynamic sampling with Group Relative Policy Optimization (GRPO) to mitigate discriminative hallucination. Additionally, post-CoT processing and a cumulative probability-based partitioning method enable efficient online deployment. TaoSR1 significantly outperforms baselines on offline datasets and achieves substantial gains in online side-by-side human evaluations, introducing a novel paradigm for applying CoT reasoning to relevance classification. </p>
<blockquote>
<p>æŸ¥è¯¢å•†å“ç›¸å…³æ€§é¢„æµ‹æ˜¯ç”µå­å•†åŠ¡æœç´¢ä¸­çš„æ ¸å¿ƒä»»åŠ¡ã€‚åŸºäºBERTçš„æ¨¡å‹æ“…é•¿è¯­ä¹‰åŒ¹é…ï¼Œä½†ç¼ºä¹å¤æ‚çš„æ¨ç†èƒ½åŠ›ã€‚è™½ç„¶å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å·²è¢«æ¢ç´¢ï¼Œä½†å¤§å¤šæ•°ä»ä½¿ç”¨åˆ¤åˆ«å¾®è°ƒæˆ–è’¸é¦åˆ°è¾ƒå°çš„æ¨¡å‹è¿›è¡Œéƒ¨ç½²ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§ç›´æ¥éƒ¨ç½²LLMæ¥å®Œæˆæ­¤ä»»åŠ¡æ¡†æ¶ï¼Œè§£å†³äº†å…³é”®æŒ‘æˆ˜ï¼šæ€ç»´é“¾ï¼ˆCoTï¼‰è¯¯å·®ç´¯ç§¯ã€åˆ¤åˆ«æ€§å¹»è§‰å’Œéƒ¨ç½²å¯è¡Œæ€§ã€‚æˆ‘ä»¬çš„æ¡†æ¶TaoSR1åŒ…å«ä¸‰ä¸ªé˜¶æ®µï¼šï¼ˆ1ï¼‰ä½¿ç”¨CoTè¿›è¡Œæœ‰ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰ä»¥çŒè¾“æ¨ç†èƒ½åŠ›ï¼›ï¼ˆ2ï¼‰é‡‡ç”¨pass@Nç­–ç•¥å’Œç›´æ¥åå¥½ä¼˜åŒ–ï¼ˆDPOï¼‰è¿›è¡Œç¦»çº¿é‡‡æ ·ï¼Œä»¥æé«˜ç”Ÿæˆè´¨é‡ï¼›ï¼ˆ3ï¼‰åŸºäºéš¾åº¦çš„åŠ¨æ€é‡‡æ ·ä¸ç¾¤ä½“ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆGRPOï¼‰ï¼Œä»¥å‡è½»åˆ¤åˆ«æ€§å¹»è§‰ã€‚æ­¤å¤–ï¼ŒCoTå¤„ç†åçš„åŸºäºç´¯ç§¯æ¦‚ç‡çš„åˆ†åŒºæ–¹æ³•èƒ½å¤Ÿå®ç°æœ‰æ•ˆçš„åœ¨çº¿éƒ¨ç½²ã€‚TaoSR1åœ¨ç¦»çº¿æ•°æ®é›†ä¸Šçš„è¡¨ç°è¿œè¶…åŸºçº¿ï¼Œåœ¨çº¿å¹¶è¡Œäººç±»è¯„ä¼°ä¸­ä¹Ÿå–å¾—äº†å®è´¨æ€§è¿›æ­¥ï¼Œä¸ºå°†CoTæ¨ç†åº”ç”¨äºç›¸å…³æ€§åˆ†ç±»å¼•å…¥äº†ä¸€ç§æ–°çš„èŒƒå¼ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.12365v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ä¸ªåä¸ºTaoSR1çš„æ¡†æ¶ï¼Œç”¨äºåœ¨ç”µå­å•†åŠ¡æœç´¢ä¸­çš„æŸ¥è¯¢äº§å“ç›¸å…³æ€§é¢„æµ‹ä»»åŠ¡ä¸­ç›´æ¥éƒ¨ç½²å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ã€‚è¯¥æ¡†æ¶è§£å†³äº†é“¾å¼æ€ç»´ï¼ˆCoTï¼‰è¯¯å·®ç´¯ç§¯ã€åˆ¤åˆ«å¼å¹»è§‰å’Œéƒ¨ç½²å¯è¡Œæ€§ç­‰å…³é”®æŒ‘æˆ˜ã€‚é€šè¿‡ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰èµ‹äºˆæ¨¡å‹æ¨ç†èƒ½åŠ›ï¼Œé‡‡ç”¨ç¦»çº¿é‡‡æ ·å’Œç›´æ¥åå¥½ä¼˜åŒ–æé«˜ç”Ÿæˆè´¨é‡ï¼Œå¹¶é€šè¿‡éš¾åº¦åŸºç¡€ä¸Šçš„åŠ¨æ€é‡‡æ ·ä¼˜åŒ–ç»„ç›¸å¯¹ç­–ç•¥ï¼Œç¼“è§£åˆ¤åˆ«å¼å¹»è§‰é—®é¢˜ã€‚æ­¤å¤–ï¼Œæ¡†æ¶è¿˜åŒ…æ‹¬åCoTå¤„ç†å’ŒåŸºäºç´¯ç§¯æ¦‚ç‡çš„åˆ†åŒºæ–¹æ³•ï¼Œä»¥å®ç°é«˜æ•ˆåœ¨çº¿éƒ¨ç½²ã€‚TaoSR1åœ¨ç¦»çº¿æ•°æ®é›†ä¸Šæ˜¾è‘—ä¼˜äºåŸºçº¿ï¼Œå¹¶åœ¨åœ¨çº¿å®æ—¶äººç±»è¯„ä¼°ä¸­å–å¾—å®è´¨æ€§è¿›æ­¥ï¼Œä¸ºåº”ç”¨CoTæ¨ç†äºç›¸å…³æ€§åˆ†ç±»ä»»åŠ¡å¼€åˆ›äº†æ–°èŒƒå¼ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>BERTæ¨¡å‹åœ¨è¯­ä¹‰åŒ¹é…æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œä½†ç¼ºä¹å¤æ‚æ¨ç†èƒ½åŠ›ã€‚</li>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨æŸ¥è¯¢äº§å“ç›¸å…³æ€§é¢„æµ‹ä»»åŠ¡ä¸­å…·æœ‰æ½œåŠ›ã€‚</li>
<li>æå‡ºçš„TaoSR1æ¡†æ¶æ—¨åœ¨è§£å†³é“¾å¼æ€ç»´ï¼ˆCoTï¼‰è¯¯å·®ç´¯ç§¯ã€åˆ¤åˆ«å¼å¹»è§‰å’Œéƒ¨ç½²å¯è¡Œæ€§ç­‰æŒ‘æˆ˜ã€‚</li>
<li>TaoSR1æ¡†æ¶åŒ…æ‹¬ä¸‰ä¸ªé˜¶æ®µï¼šç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰èµ‹äºˆæ¨ç†èƒ½åŠ›ï¼Œç¦»çº¿é‡‡æ ·å’Œç›´æ¥åå¥½ä¼˜åŒ–æé«˜ç”Ÿæˆè´¨é‡ï¼Œéš¾åº¦åŸºç¡€ä¸Šçš„åŠ¨æ€é‡‡æ ·ä¼˜åŒ–ç»„ç›¸å¯¹ç­–ç•¥ã€‚</li>
<li>TaoSR1æ¡†æ¶é‡‡ç”¨åCoTå¤„ç†å’ŒåŸºäºç´¯ç§¯æ¦‚ç‡çš„åˆ†åŒºæ–¹æ³•ï¼Œå®ç°é«˜æ•ˆåœ¨çº¿éƒ¨ç½²ã€‚</li>
<li>TaoSR1åœ¨ç¦»çº¿æ•°æ®é›†ä¸Šæ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.12365">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-6a0b033d4757e845fc075167c6a04620.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-657f8cb4e4fd43ce8ac2beb8d4c108e1.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-15d07d15bc18d56384a7b252d6a4e2b8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f60b9a79110d60bcab23b96a20123332.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="Legal-Î”-Enhancing-Legal-Reasoning-in-LLMs-via-Reinforcement-Learning-with-Chain-of-Thought-Guided-Information-Gain"><a href="#Legal-Î”-Enhancing-Legal-Reasoning-in-LLMs-via-Reinforcement-Learning-with-Chain-of-Thought-Guided-Information-Gain" class="headerlink" title="Legal$Î”$: Enhancing Legal Reasoning in LLMs via Reinforcement   Learning with Chain-of-Thought Guided Information Gain"></a>Legal$Î”$: Enhancing Legal Reasoning in LLMs via Reinforcement   Learning with Chain-of-Thought Guided Information Gain</h2><p><strong>Authors:Xin Dai, Buqiang Xu, Zhenghao Liu, Yukun Yan, Huiyuan Xie, Xiaoyuan Yi, Shuo Wang, Ge Yu</strong></p>
<p>Legal Artificial Intelligence (LegalAI) has achieved notable advances in automating judicial decision-making with the support of Large Language Models (LLMs). However, existing legal LLMs still struggle to generate reliable and interpretable reasoning processes. They often default to fast-thinking behavior by producing direct answers without explicit multi-step reasoning, limiting their effectiveness in complex legal scenarios that demand rigorous justification. To address this challenge, we propose Legal$\Delta$, a reinforcement learning framework designed to enhance legal reasoning through chain-of-thought guided information gain. During training, Legal$\Delta$ employs a dual-mode input setup-comprising direct answer and reasoning-augmented modes-and maximizes the information gain between them. This encourages the model to acquire meaningful reasoning patterns rather than generating superficial or redundant explanations. Legal$\Delta$ follows a two-stage approach: (1) distilling latent reasoning capabilities from a powerful Large Reasoning Model (LRM), DeepSeek-R1, and (2) refining reasoning quality via differential comparisons, combined with a multidimensional reward mechanism that assesses both structural coherence and legal-domain specificity. Experimental results on multiple legal reasoning tasks demonstrate that Legal$\Delta$ outperforms strong baselines in both accuracy and interpretability. It consistently produces more robust and trustworthy legal judgments without relying on labeled preference data. All code and data will be released at <a target="_blank" rel="noopener" href="https://github.com/NEUIR/LegalDelta">https://github.com/NEUIR/LegalDelta</a>. </p>
<blockquote>
<p>æ³•å¾‹äººå·¥æ™ºèƒ½ï¼ˆLegalAIï¼‰åœ¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ”¯æŒä¸‹ï¼Œåœ¨è‡ªåŠ¨åŒ–å¸æ³•å†³ç­–æ–¹é¢å–å¾—äº†æ˜¾è‘—è¿›å±•ã€‚ç„¶è€Œï¼Œç°æœ‰çš„æ³•å¾‹LLMåœ¨ç”Ÿæˆå¯é ä¸”å¯è§£é‡Šçš„æ¨ç†è¿‡ç¨‹æ–¹é¢ä»ç„¶å­˜åœ¨å›°éš¾ã€‚å®ƒä»¬é€šå¸¸é‡‡å–å¿«é€Ÿæ€è€ƒçš„è¡Œä¸ºï¼Œç›´æ¥ç»™å‡ºç­”æ¡ˆï¼Œè€Œæ²¡æœ‰æ˜ç¡®çš„å¤šæ­¥éª¤æ¨ç†ï¼Œè¿™åœ¨éœ€è¦ä¸¥æ ¼è¯æ˜çš„å¤æ‚æ³•å¾‹åœºæ™¯ä¸­é™åˆ¶äº†å®ƒä»¬çš„æœ‰æ•ˆæ€§ã€‚ä¸ºäº†è§£å†³è¿™ä¸€æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†LegalÎ”ï¼Œä¸€ä¸ªé€šè¿‡æ€ç»´é“¾å¼•å¯¼ä¿¡æ¯å¢ç›Šæ¥å¢å¼ºæ³•å¾‹æ¨ç†çš„å¼ºåŒ–å­¦ä¹ æ¡†æ¶ã€‚åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ï¼ŒLegalÎ”é‡‡ç”¨åŒæ¨¡å¼è¾“å…¥è®¾ç½®ï¼ŒåŒ…æ‹¬ç›´æ¥ç­”æ¡ˆæ¨¡å¼å’Œæ¨ç†å¢å¼ºæ¨¡å¼ï¼Œå¹¶æœ€å¤§åŒ–ä¸¤è€…ä¹‹é—´çš„ä¿¡æ¯å¢ç›Šã€‚è¿™é¼“åŠ±æ¨¡å‹è·å–æœ‰æ„ä¹‰çš„æ¨ç†æ¨¡å¼ï¼Œè€Œä¸æ˜¯äº§ç”Ÿè‚¤æµ…æˆ–å†—ä½™çš„è§£é‡Šã€‚LegalÎ”é‡‡ç”¨ä¸¤é˜¶æ®µæ–¹æ³•ï¼š(1)ä»å¼ºå¤§çš„å¤§å‹æ¨ç†æ¨¡å‹ï¼ˆLRMï¼‰DeepSeek-R1ä¸­æç‚¼æ½œåœ¨çš„æ¨ç†èƒ½åŠ›ï¼›(2)é€šè¿‡å·®å¼‚æ¯”è¾ƒå’Œç»“åˆè¯„ä¼°ç»“æ„è¿è´¯æ€§å’Œæ³•å¾‹é¢†åŸŸç‰¹å¼‚æ€§çš„å¤šç»´å¥–åŠ±æœºåˆ¶æ¥ä¼˜åŒ–æ¨ç†è´¨é‡ã€‚åœ¨å¤šä¸ªæ³•å¾‹æ¨ç†ä»»åŠ¡ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼ŒLegalÎ”åœ¨å‡†ç¡®æ€§å’Œå¯è§£é‡Šæ€§æ–¹é¢è¶…è¶Šäº†å¼ºå¤§çš„åŸºå‡†æµ‹è¯•ã€‚å®ƒåœ¨ä¸ä¾èµ–æ ‡è®°åå¥½æ•°æ®çš„æƒ…å†µä¸‹ï¼Œå§‹ç»ˆäº§ç”Ÿæ›´ç¨³å¥å’Œå¯ä¿¡çš„æ³•å¾‹åˆ¤æ–­ã€‚æ‰€æœ‰ä»£ç å’Œæ•°æ®å°†åœ¨<a target="_blank" rel="noopener" href="https://github.com/NEUIR/LegalDelta">https://github.com/NEUIR/LegalDelta</a>ä¸Šå‘å¸ƒã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.12281v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æ³•å¾‹äººå·¥æ™ºèƒ½ï¼ˆLegalAIï¼‰å€ŸåŠ©å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨è‡ªåŠ¨åŒ–å¸æ³•å†³ç­–åˆ¶å®šæ–¹é¢å–å¾—æ˜¾è‘—è¿›å±•ï¼Œä½†åœ¨å¤æ‚æ³•å¾‹åœºæ™¯ä¸­ï¼Œç°æœ‰æ³•å¾‹LLMsåœ¨ç”Ÿæˆå¯é ä¸”å¯è§£é‡Šçš„æ¨ç†è¿‡ç¨‹æ–¹é¢ä»å­˜åœ¨å›°éš¾ã€‚å®ƒä»¬å¾€å¾€é‡‡ç”¨å¿«é€Ÿæ€è€ƒè¡Œä¸ºï¼Œç›´æ¥ç»™å‡ºç­”æ¡ˆï¼Œç¼ºä¹æ˜ç¡®çš„å¤šæ­¥éª¤æ¨ç†ï¼Œéš¾ä»¥åº”å¯¹éœ€è¦ä¸¥æ ¼è®ºè¯çš„æ³•å¾‹æƒ…å¢ƒã€‚ä¸ºåº”å¯¹è¿™ä¸€æŒ‘æˆ˜ï¼Œæå‡ºäº†LegalÎ”å¼ºåŒ–å­¦ä¹ æ¡†æ¶ï¼Œé€šè¿‡é“¾å¼æ€ç»´å¼•å¯¼çš„ä¿¡æ¯å¢ç›Šæ¥æå‡æ³•å¾‹æ¨ç†èƒ½åŠ›ã€‚LegalÎ”é‡‡ç”¨åŒæ¨¡å¼è¾“å…¥è®¾ç½®ï¼Œé¼“åŠ±æ¨¡å‹è·å–æœ‰æ„ä¹‰çš„æ¨ç†æ¨¡å¼ï¼Œè€Œéç”Ÿæˆè‚¤æµ…æˆ–å†—ä½™çš„è§£é‡Šã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒLegalÎ”åœ¨å¤šä¸ªæ³•å¾‹æ¨ç†ä»»åŠ¡ä¸Šçš„å‡†ç¡®æ€§å’Œå¯è§£é‡Šæ€§å‡ä¼˜äºå¼ºåŸºçº¿ï¼Œèƒ½å¤Ÿäº§ç”Ÿæ›´ç¨³å¥å’Œå¯ä¿¡èµ–çš„æ³•å¾‹åˆ¤æ–­ï¼Œä¸”æ— éœ€ä¾èµ–æ ‡è®°åå¥½æ•°æ®ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LegalAIå€ŸåŠ©LLMsåœ¨è‡ªåŠ¨åŒ–å¸æ³•å†³ç­–ä¸­å–å¾—è¿›å±•ã€‚</li>
<li>ç°æœ‰æ³•å¾‹LLMsåœ¨å¤æ‚æ³•å¾‹åœºæ™¯ä¸­é¢ä¸´ç”Ÿæˆå¯é å’Œå¯è§£é‡Šæ¨ç†çš„æŒ‘æˆ˜ã€‚</li>
<li>LegalÎ”å¼ºåŒ–å­¦ä¹ æ¡†æ¶é€šè¿‡é“¾å¼æ€ç»´å¼•å¯¼çš„ä¿¡æ¯å¢ç›Šæå‡æ³•å¾‹æ¨ç†ã€‚</li>
<li>LegalÎ”é‡‡ç”¨åŒæ¨¡å¼è¾“å…¥è®¾ç½®ï¼Œé¼“åŠ±æ¨¡å‹è·å–æœ‰æ„ä¹‰æ¨ç†æ¨¡å¼ã€‚</li>
<li>LegalÎ”é€šè¿‡ä¸¤é˜¶æ®µæ–¹æ³•ï¼šä»å¼ºå¤§çš„LRMä¸­æç‚¼æ½œåœ¨æ¨ç†èƒ½åŠ›ï¼Œå¹¶é€šè¿‡å·®å¼‚æ¯”è¾ƒå’Œå¤šç»´åº¦å¥–åŠ±æœºåˆ¶æé«˜æ¨ç†è´¨é‡ã€‚</li>
<li>LegalÎ”åœ¨å¤šä¸ªæ³•å¾‹æ¨ç†ä»»åŠ¡ä¸Šçš„å‡†ç¡®æ€§å’Œå¯è§£é‡Šæ€§ä¼˜äºå¼ºåŸºçº¿ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.12281">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-117245d92c4f5784345aab597483c4af.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-456871152a088983f85e652e58c08bd2.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-1f624f0142ee8deef419f53957cb9dad.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2bdfb02fbbeb4cd0787876faee68ca58.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a68c2c465b19a088f9bec69640426f59.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c7a06443545999a9d86fbb9aadfaab22.jpg" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="Few-shot-Vision-based-Human-Activity-Recognition-with-MLLM-based-Visual-Reinforcement-Learning"><a href="#Few-shot-Vision-based-Human-Activity-Recognition-with-MLLM-based-Visual-Reinforcement-Learning" class="headerlink" title="Few-shot Vision-based Human Activity Recognition with MLLM-based Visual   Reinforcement Learning"></a>Few-shot Vision-based Human Activity Recognition with MLLM-based Visual   Reinforcement Learning</h2><p><strong>Authors:Wenqi Zheng, Yutaka Arakawa</strong></p>
<p>Reinforcement learning in large reasoning models enables learning from feedback on their outputs, making it particularly valuable in scenarios where fine-tuning data is limited. However, its application in multi-modal human activity recognition (HAR) domains remains largely underexplored. Our work extends reinforcement learning to the human activity recognition domain with multimodal large language models. By incorporating visual reinforcement learning in the training process, the modelâ€™s generalization ability on few-shot recognition can be greatly improved. Additionally, visual reinforcement learning can enhance the modelâ€™s reasoning ability and enable explainable analysis in the inference stage. We name our few-shot human activity recognition method with visual reinforcement learning FAVOR. Specifically, our approach first utilizes a multimodal large language model (MLLM) to generate multiple candidate responses for the human activity image, each containing reasoning traces and final answers. These responses are then evaluated using reward functions, and the MLLM model is subsequently optimized using the Group Relative Policy Optimization (GRPO) algorithm. In this way, the MLLM model can be adapted to human activity recognition with only a few samples. Extensive experiments on four human activity recognition datasets and five different settings demonstrate the superiority of the proposed method. </p>
<blockquote>
<p>å¼ºåŒ–å­¦ä¹ åœ¨å¤§å‹æ¨ç†æ¨¡å‹ä¸­çš„åº”ç”¨ä½¿å¾—æ¨¡å‹èƒ½å¤Ÿæ ¹æ®è¾“å‡ºåé¦ˆè¿›è¡Œå­¦ä¹ ï¼Œè¿™åœ¨ç²¾ç»†è°ƒæ•´æ•°æ®æœ‰é™çš„æƒ…å†µä¸‹å°¤ä¸ºæœ‰ä»·å€¼ã€‚ç„¶è€Œï¼Œå…¶åœ¨å¤šæ¨¡æ€äººç±»æ´»åŠ¨è¯†åˆ«ï¼ˆHARï¼‰é¢†åŸŸçš„åº”ç”¨ä»è¢«å¤§å¤§å¿½è§†ã€‚æˆ‘ä»¬çš„å·¥ä½œå°†å¼ºåŒ–å­¦ä¹ æ‰©å±•åˆ°äººç±»æ´»åŠ¨è¯†åˆ«é¢†åŸŸï¼Œå¹¶å¼•å…¥å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ã€‚é€šè¿‡åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­èå…¥è§†è§‰å¼ºåŒ–å­¦ä¹ ï¼Œå¯ä»¥å¤§å¤§æé«˜æ¨¡å‹åœ¨å°‘æ ·æœ¬è¯†åˆ«æ–¹é¢çš„æ³›åŒ–èƒ½åŠ›ã€‚æ­¤å¤–ï¼Œè§†è§‰å¼ºåŒ–å­¦ä¹ è¿˜å¯ä»¥å¢å¼ºæ¨¡å‹çš„æ¨ç†èƒ½åŠ›ï¼Œå¹¶åœ¨æ¨ç†é˜¶æ®µå®ç°å¯è§£é‡Šæ€§åˆ†æã€‚æˆ‘ä»¬å°†é‡‡ç”¨è§†è§‰å¼ºåŒ–å­¦ä¹ çš„å°‘æ ·æœ¬äººç±»æ´»åŠ¨è¯†åˆ«æ–¹æ³•å‘½åä¸ºFAVORã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬çš„æ–¹æ³•é¦–å…ˆåˆ©ç”¨å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰ä¸ºäººä½“æ´»åŠ¨å›¾åƒç”Ÿæˆå¤šä¸ªå€™é€‰å“åº”ï¼Œæ¯ä¸ªå“åº”éƒ½åŒ…å«æ¨ç†è½¨è¿¹å’Œæœ€ç»ˆç­”æ¡ˆã€‚ç„¶åï¼Œä½¿ç”¨å¥–åŠ±å‡½æ•°å¯¹è¿™äº›å“åº”è¿›è¡Œè¯„ä¼°ï¼Œéšåä½¿ç”¨ç¾¤ä½“ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆGRPOï¼‰ç®—æ³•å¯¹MLLMæ¨¡å‹è¿›è¡Œä¼˜åŒ–ã€‚é€šè¿‡è¿™ç§æ–¹å¼ï¼ŒMLLMæ¨¡å‹å¯ä»¥é€‚åº”ä»…ä½¿ç”¨å°‘é‡æ ·æœ¬çš„äººç±»æ´»åŠ¨è¯†åˆ«ã€‚åœ¨å››ä¸ªäººç±»æ´»åŠ¨è¯†åˆ«æ•°æ®é›†å’Œäº”ç§ä¸åŒè®¾ç½®ä¸‹çš„å¹¿æ³›å®éªŒè¯æ˜äº†æ‰€æå‡ºæ–¹æ³•çš„ä¼˜è¶Šæ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.10371v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¼ºåŒ–å­¦ä¹ åœ¨å¤§è§„æ¨¡æ¨ç†æ¨¡å‹ä¸­çš„åº”ç”¨å¯ä»¥ä»è¾“å‡ºåé¦ˆä¸­å­¦ä¹ ï¼Œè¿™åœ¨ç²¾ç»†è°ƒæ•´æ•°æ®æœ‰é™çš„æƒ…å†µä¸‹å°¤å…¶æœ‰ä»·å€¼ã€‚æœ¬ç ”ç©¶å°†å¼ºåŒ–å­¦ä¹ æ‰©å±•åˆ°äº†äººç±»æ´»åŠ¨è¯†åˆ«é¢†åŸŸï¼Œå¹¶ç»“åˆå¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ã€‚é€šè¿‡å¼•å…¥è§†è§‰å¼ºåŒ–å­¦ä¹ ï¼Œæ¨¡å‹åœ¨å°‘é‡æ ·æœ¬è¯†åˆ«ä¸Šçš„æ³›åŒ–èƒ½åŠ›å¤§å¤§æé«˜ï¼ŒåŒæ—¶å¢å¼ºäº†æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ï¼Œå¹¶åœ¨æ¨ç†é˜¶æ®µå®ç°äº†å¯è§£é‡Šæ€§åˆ†æã€‚æœ¬ç ”ç©¶æå‡ºçš„åŸºäºè§†è§‰å¼ºåŒ–å­¦ä¹ çš„å°‘é‡äººç±»æ´»åŠ¨è¯†åˆ«æ–¹æ³•å‘½åä¸ºFAVORã€‚è¯¥æ–¹æ³•åˆ©ç”¨å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ç”Ÿæˆäººç±»æ´»åŠ¨å›¾åƒçš„å¤šå“åº”å€™é€‰ï¼Œåˆ©ç”¨å¥–åŠ±å‡½æ•°è¿›è¡Œè¯„ä¼°ï¼Œå¹¶ä½¿ç”¨é›†å›¢ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ç®—æ³•å¯¹æ¨¡å‹è¿›è¡Œä¼˜åŒ–ã€‚åœ¨å››ä¸ªæ•°æ®é›†å’Œäº”ç§ä¸åŒè®¾ç½®ä¸‹çš„å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•å…·æœ‰ä¼˜è¶Šæ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¼ºåŒ–å­¦ä¹ å¯åº”ç”¨äºå¤§è§„æ¨¡æ¨ç†æ¨¡å‹ï¼Œå¹¶ä»è¾“å‡ºåé¦ˆä¸­å­¦ä¹ ã€‚</li>
<li>è§†è§‰å¼ºåŒ–å­¦ä¹ è¢«å¼•å…¥åˆ°äººç±»æ´»åŠ¨è¯†åˆ«é¢†åŸŸï¼Œä¸å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ç»“åˆã€‚</li>
<li>è§†è§‰å¼ºåŒ–å­¦ä¹ å¯æ˜¾è‘—æé«˜æ¨¡å‹åœ¨å°‘é‡æ ·æœ¬è¯†åˆ«ä¸Šçš„æ³›åŒ–èƒ½åŠ›ã€‚</li>
<li>è§†è§‰å¼ºåŒ–å­¦ä¹ å¢å¼ºäº†æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ï¼Œå¹¶å®ç°äº†æ¨ç†é˜¶æ®µçš„å¯è§£é‡Šæ€§åˆ†æã€‚</li>
<li>æå‡ºäº†ä¸€ç§åŸºäºè§†è§‰å¼ºåŒ–å­¦ä¹ çš„å°‘é‡äººç±»æ´»åŠ¨è¯†åˆ«æ–¹æ³•ï¼Œåä¸ºFAVORã€‚</li>
<li>FAVORæ–¹æ³•åˆ©ç”¨å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ç”Ÿæˆå¤šä¸ªå“åº”å€™é€‰ï¼Œå¹¶ä½¿ç”¨å¥–åŠ±å‡½æ•°å’Œé›†å›¢ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ç®—æ³•è¿›è¡Œä¼˜åŒ–ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.10371">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-4d701f72dd0648272bd6ee6fd4bddc5d.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-286823a3a1d7792f71ef33c09cba0489.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-14027dd6d3c9cd50a4dd2533cdc7318b.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-c938b694e12f24aa0c80ea9ec1ec8671.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ced82577192fe3e31773064b8f6d1bea.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-319d03bdfb285d82d7db1da0449814d2.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-164b5b2e4b9bbe981a46977a09a50972.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0ceb93b5a52d5b1eea48b1129d771e6f.jpg" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="Making-Qwen3-Think-in-Korean-with-Reinforcement-Learning"><a href="#Making-Qwen3-Think-in-Korean-with-Reinforcement-Learning" class="headerlink" title="Making Qwen3 Think in Korean with Reinforcement Learning"></a>Making Qwen3 Think in Korean with Reinforcement Learning</h2><p><strong>Authors:Jungyup Lee, Jemin Kim, Sang Park, SeungJae Lee</strong></p>
<p>We present a two-stage fine-tuning approach to make the large language model Qwen3 14B â€œthinkâ€ natively in Korean. In the first stage, supervised fine-tuning (SFT) on a high-quality Korean reasoning dataset establishes a strong foundation in Korean logical reasoning, yielding notable improvements in Korean-language tasks and even some gains in general reasoning ability. In the second stage, we employ reinforcement learning with a customized Group Relative Policy Optimization (GRPO) algorithm to further enhance both Korean reasoning alignment and overall problem-solving performance. We address critical stability challenges in GRPO training - such as reward hacking and policy collapse - by introducing an oracle judge model that calibrates the reward signal. Our approach achieves stable learning (avoiding the collapse observed in naive GRPO) and leads to steady, incremental performance gains. The final RL-tuned model demonstrates substantially improved results on advanced reasoning benchmarks (particularly math and coding tasks) while maintaining knowledge and language proficiency, successfully conducting its internal chain-of-thought entirely in Korean. </p>
<blockquote>
<p>æˆ‘ä»¬æå‡ºäº†ä¸€ç§ä¸¤é˜¶æ®µçš„å¾®è°ƒæ–¹æ³•ï¼Œä½¿å¤§å‹è¯­è¨€æ¨¡å‹Qwen3 14Bèƒ½å¤Ÿâ€œä»¥éŸ©è¯­æ¯è¯­æ€è€ƒâ€ã€‚åœ¨ç¬¬ä¸€é˜¶æ®µï¼Œé€šè¿‡é«˜è´¨é‡éŸ©è¯­æ¨ç†æ•°æ®é›†çš„æœ‰ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰ä¸ºéŸ©è¯­é€»è¾‘æ¨ç†å»ºç«‹äº†åšå®çš„åŸºç¡€ï¼Œåœ¨éŸ©è¯­ä»»åŠ¡ä¸­å–å¾—äº†æ˜¾è‘—çš„æ”¹è¿›ï¼Œç”šè‡³åœ¨ä¸€èˆ¬æ¨ç†èƒ½åŠ›ä¸Šä¹Ÿæœ‰æ‰€æå‡ã€‚åœ¨ç¬¬äºŒé˜¶æ®µï¼Œæˆ‘ä»¬ä½¿ç”¨å¼ºåŒ–å­¦ä¹ ä¸è‡ªå®šä¹‰çš„Group Relative Policy Optimizationï¼ˆGRPOï¼‰ç®—æ³•è¿›ä¸€æ­¥æé«˜äº†éŸ©è¯­æ¨ç†å¯¹é½å’Œæ•´ä½“è§£å†³é—®é¢˜çš„èƒ½åŠ›ã€‚æˆ‘ä»¬é€šè¿‡å¼•å…¥æ ¡å‡†å¥–åŠ±ä¿¡å·çš„ä¸“å®¶è£åˆ¤æ¨¡å‹æ¥è§£å†³GRPOè®­ç»ƒä¸­çš„å…³é”®ç¨³å®šæ€§æŒ‘æˆ˜ï¼Œå¦‚å¥–åŠ±ç ´è§£å’Œæ”¿ç­–å´©æºƒç­‰ã€‚æˆ‘ä»¬çš„æ–¹æ³•å®ç°äº†ç¨³å®šçš„å­¦ä¹ ï¼ˆé¿å…äº†å¹¼ç¨šGRPOä¸­è§‚å¯Ÿåˆ°çš„å´©æºƒï¼‰ï¼Œå¹¶å¸¦æ¥äº†ç¨³å®šçš„ã€å¢é‡å¼çš„æ€§èƒ½æå‡ã€‚æœ€ç»ˆç»è¿‡RLè°ƒå‚çš„æ¨¡å‹åœ¨é«˜çº§æ¨ç†åŸºå‡†æµ‹è¯•ï¼ˆå°¤å…¶æ˜¯æ•°å­¦å’Œç¼–ç ä»»åŠ¡ï¼‰ä¸Šå–å¾—äº†æ˜¾è‘—æ”¹è¿›çš„ç»“æœï¼ŒåŒæ—¶ä¿æŒäº†çŸ¥è¯†å’Œè¯­è¨€æŠ€èƒ½ï¼ŒæˆåŠŸåœ°ä»¥éŸ©è¯­å®Œå…¨å®Œæˆäº†å…¶å†…éƒ¨æ€è€ƒé“¾ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.10355v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹Qwen3 14Bé€šè¿‡ä¸¤é˜¶æ®µå¾®è°ƒä½¿å…¶â€œå¤©ç”Ÿâ€å…·å¤‡éŸ©è¯­æ€è€ƒèƒ½åŠ›ã€‚ç¬¬ä¸€é˜¶æ®µé€šè¿‡é«˜è´¨é‡éŸ©è¯­æ¨ç†æ•°æ®é›†è¿›è¡Œæœ‰ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰ï¼Œä¸ºéŸ©è¯­é€»è¾‘æ¨ç†æ‰“ä¸‹äº†åšå®åŸºç¡€ï¼Œåœ¨éŸ©è¯­ä»»åŠ¡ä¸Šå–å¾—äº†æ˜¾è‘—æ”¹è¿›ï¼Œç”šè‡³åœ¨é€šç”¨æ¨ç†èƒ½åŠ›ä¸Šä¹Ÿæœ‰æ‰€æå‡ã€‚ç¬¬äºŒé˜¶æ®µé‡‡ç”¨å¼ºåŒ–å­¦ä¹ ï¼Œç»“åˆè‡ªå®šä¹‰çš„ç»„ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆGRPOï¼‰ç®—æ³•ï¼Œè¿›ä¸€æ­¥æé«˜éŸ©è¯­æ¨ç†å¯¹é½å’Œæ•´ä½“é—®é¢˜è§£å†³èƒ½åŠ›ã€‚é€šè¿‡å¼•å…¥æ ¡å‡†å¥–åŠ±ä¿¡å·çš„oracleåˆ¤æ–­æ¨¡å‹ï¼Œè§£å†³äº†GRPOè®­ç»ƒä¸­çš„å…³é”®ç¨³å®šæ€§æŒ‘æˆ˜ï¼Œå¦‚å¥–åŠ±é»‘å®¢å’Œç­–ç•¥å´©æºƒã€‚è¯¥æ–¹æ³•å®ç°äº†ç¨³å®šå­¦ä¹ ï¼Œå¸¦æ¥äº†ç¨³å®šçš„æ€§èƒ½æå‡ã€‚æœ€ç»ˆé€šè¿‡å¼ºåŒ–å­¦ä¹ è°ƒæ•´çš„æ¨¡å‹åœ¨é«˜çº§æ¨ç†åŸºå‡†æµ‹è¯•ï¼ˆå°¤å…¶æ˜¯æ•°å­¦å’Œç¼–ç ä»»åŠ¡ï¼‰ä¸Šå–å¾—äº†æ˜¾è‘—æ”¹å–„ï¼ŒåŒæ—¶ä¿æŒäº†çŸ¥è¯†å’Œè¯­è¨€æŠ€èƒ½ï¼Œå¹¶æˆåŠŸä»¥éŸ©è¯­å®Œæˆäº†å†…éƒ¨æ€ç»´é“¾æ¡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æå‡ºäº†ä¸€ä¸ªä¸¤é˜¶æ®µå¾®è°ƒæ–¹æ³•ï¼Œä½¿å¤§å‹è¯­è¨€æ¨¡å‹Qwen3 14Bèƒ½å¤Ÿâ€œä»¥éŸ©è¯­æ€è€ƒâ€ã€‚</li>
<li>ç¬¬ä¸€é˜¶æ®µé€šè¿‡é«˜è´¨é‡éŸ©è¯­æ¨ç†æ•°æ®é›†è¿›è¡Œæœ‰ç›‘ç£å¾®è°ƒï¼Œæé«˜äº†éŸ©è¯­ä»»åŠ¡å’Œé€šç”¨æ¨ç†èƒ½åŠ›ã€‚</li>
<li>ç¬¬äºŒé˜¶æ®µé‡‡ç”¨å¼ºåŒ–å­¦ä¹ å’Œè‡ªå®šä¹‰çš„GRPOç®—æ³•è¿›ä¸€æ­¥æé«˜éŸ©è¯­æ¨ç†å’Œé—®é¢˜è§£å†³èƒ½åŠ›ã€‚</li>
<li>å¼•å…¥oracleåˆ¤æ–­æ¨¡å‹è§£å†³GRPOè®­ç»ƒä¸­çš„ç¨³å®šæ€§æŒ‘æˆ˜ã€‚</li>
<li>æ–¹æ³•å®ç°äº†ç¨³å®šå­¦ä¹ ï¼Œå¹¶å¸¦æ¥æ€§èƒ½ä¸Šçš„æŒç»­æé«˜ã€‚</li>
<li>æœ€ç»ˆæ¨¡å‹åœ¨é«˜çº§æ¨ç†æµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œç‰¹åˆ«æ˜¯åœ¨æ•°å­¦å’Œç¼–ç ä»»åŠ¡ä¸Šã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.10355">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-5983ff4202cc3544a4859189946dc595.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7a9ab580a26ae6db2b70771579aaf660.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-267e6530584eb60505b1faf37fc729d3.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-82a65d9370ed787914a0c4635ca59d01.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f20f8eb9794662de6aff6e8a987559fd.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-300e54e5aff661eb175e0f059f877c64.jpg" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1><h2 id="From-Intent-to-Execution-Multimodal-Chain-of-Thought-Reinforcement-Learning-for-Precise-CAD-Code-Generation"><a href="#From-Intent-to-Execution-Multimodal-Chain-of-Thought-Reinforcement-Learning-for-Precise-CAD-Code-Generation" class="headerlink" title="From Intent to Execution: Multimodal Chain-of-Thought Reinforcement   Learning for Precise CAD Code Generation"></a>From Intent to Execution: Multimodal Chain-of-Thought Reinforcement   Learning for Precise CAD Code Generation</h2><p><strong>Authors:Ke Niu, Haiyang Yu, Zhuofan Chen, Mengyang Zhao, Teng Fu, Bin Li, Xiangyang Xue</strong></p>
<p>Computer-Aided Design (CAD) plays a vital role in engineering and manufacturing, yet current CAD workflows require extensive domain expertise and manual modeling effort. Recent advances in large language models (LLMs) have made it possible to generate code from natural language, opening new opportunities for automating parametric 3D modeling. However, directly translating human design intent into executable CAD code remains highly challenging, due to the need for logical reasoning, syntactic correctness, and numerical precision. In this work, we propose CAD-RL, a multimodal Chain-of-Thought (CoT) guided reinforcement learning post training framework for CAD modeling code generation. Our method combines CoT-based Cold Start with goal-driven reinforcement learning post training using three task-specific rewards: executability reward, geometric accuracy reward, and external evaluation reward. To ensure stable policy learning under sparse and high-variance reward conditions, we introduce three targeted optimization strategies: Trust Region Stretch for improved exploration, Precision Token Loss for enhanced dimensions parameter accuracy, and Overlong Filtering to reduce noisy supervision. To support training and benchmarking, we release ExeCAD, a noval dataset comprising 16,540 real-world CAD examples with paired natural language and structured design language descriptions, executable CADQuery scripts, and rendered 3D models. Experiments demonstrate that CAD-RL achieves significant improvements in reasoning quality, output precision, and code executability over existing VLMs. </p>
<blockquote>
<p>è®¡ç®—æœºè¾…åŠ©è®¾è®¡ï¼ˆCADï¼‰åœ¨å·¥ç¨‹å’Œåˆ¶é€ ä¸­æ‰®æ¼”ç€è‡³å…³é‡è¦çš„è§’è‰²ï¼Œç„¶è€Œå½“å‰çš„CADå·¥ä½œæµç¨‹éœ€è¦å¹¿æ³›çš„ä¸“ä¸šçŸ¥è¯†å’Œæ‰‹åŠ¨å»ºæ¨¡å·¥ä½œã€‚è‡ªç„¶è¯­è¨€ç”Ÿæˆä»£ç çš„æœ€æ–°è¿›å±•ä¸ºè‡ªåŠ¨åŒ–å‚æ•°åŒ–ä¸‰ç»´å»ºæ¨¡æä¾›äº†æ–°çš„æœºä¼šã€‚ç„¶è€Œï¼Œç›´æ¥å°†äººç±»çš„è®¾è®¡æ„å›¾è½¬åŒ–ä¸ºå¯æ‰§è¡Œçš„CADä»£ç ä»ç„¶é¢ä¸´å·¨å¤§çš„æŒ‘æˆ˜ï¼Œè¿™åŒ…æ‹¬é€»è¾‘ç†è§£ã€è¯­æ³•æ­£ç¡®å’Œæ•°å€¼ç²¾ç¡®çš„éœ€æ±‚ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†CAD-RLï¼Œè¿™æ˜¯ä¸€ç§ç”¨äºCADå»ºæ¨¡ä»£ç ç”Ÿæˆçš„å¤šæ¨¡æ€æ€ç»´é“¾å¼•å¯¼å¼ºåŒ–å­¦ä¹ è®­ç»ƒåæ¡†æ¶ã€‚æˆ‘ä»¬çš„æ–¹æ³•ç»“åˆäº†åŸºäºæ€ç»´é“¾çš„å†·å¯åŠ¨å’Œç›®æ ‡é©±åŠ¨å¼ºåŒ–å­¦ä¹ è®­ç»ƒåä½¿ç”¨ä¸‰ç§ç‰¹å®šä»»åŠ¡å¥–åŠ±ï¼šå¯æ‰§è¡Œæ€§å¥–åŠ±ã€å‡ ä½•ç²¾åº¦å¥–åŠ±å’Œå¤–éƒ¨è¯„ä¼°å¥–åŠ±ã€‚ä¸ºç¡®ä¿åœ¨ç¨€ç–å’Œé«˜æ–¹å·®å¥–åŠ±æ¡ä»¶ä¸‹ç¨³å®šå­¦ä¹ æ”¿ç­–ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸‰é¡¹æœ‰é’ˆå¯¹æ€§çš„ä¼˜åŒ–ç­–ç•¥ï¼šç”¨äºæ”¹è¿›æ¢ç´¢çš„ä¿¡ä»»åŒºåŸŸæ‰©å±•ã€ç”¨äºæé«˜ç»´åº¦å‚æ•°ç²¾åº¦çš„ç²¾ç¡®ä»¤ç‰Œä¸¢å¤±ä»¥åŠç”¨äºå‡å°‘å™ªå£°ç›‘ç£çš„é•¿è¿‡æ»¤ã€‚ä¸ºäº†æ”¯æŒè®­ç»ƒå’ŒåŸºå‡†æµ‹è¯•ï¼Œæˆ‘ä»¬å‘å¸ƒäº†ExeCADæ•°æ®é›†ï¼Œè¯¥æ•°æ®é›†åŒ…å«ç°å®ä¸–ç•Œä¸­çš„CADç¤ºä¾‹å…±16,540ä¸ªï¼Œæ¯ä¸ªç¤ºä¾‹é…å¤‡è‡ªç„¶è¯­è¨€æè¿°å’Œç»“æ„åŒ–è®¾è®¡è¯­è¨€æè¿°ã€å¯æ‰§è¡Œçš„CADQueryè„šæœ¬ä»¥åŠæ¸²æŸ“çš„3Dæ¨¡å‹ã€‚å®éªŒè¡¨æ˜ï¼Œåœ¨æ¨ç†è´¨é‡ã€è¾“å‡ºç²¾åº¦å’Œä»£ç å¯æ‰§è¡Œæ€§æ–¹é¢ï¼ŒCAD-RLç›¸è¾ƒäºç°æœ‰çš„å¤§å‹è¯­è¨€æ¨¡å‹å–å¾—äº†æ˜¾è‘—çš„æ”¹è¿›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.10118v2">PDF</a> </p>
<p><strong>Summary</strong><br>    è®¡ç®—æœºè¾…åŠ©è®¾è®¡ï¼ˆCADï¼‰åœ¨å·¥ç¨‹å¸ˆå’Œåˆ¶é€ è¡Œä¸šæ‰®æ¼”ç€è‡³å…³é‡è¦çš„è§’è‰²ï¼Œä½†å…¶æµç¨‹éœ€è¦å¤§é‡ä¸“ä¸šçŸ¥è¯†å’ŒæŠ€èƒ½è¿›è¡Œæ‰‹åŠ¨å»ºæ¨¡ã€‚éšç€è‡ªç„¶è¯­è¨€åˆ°ä»£ç ç”ŸæˆæŠ€æœ¯çš„è¿›æ­¥ï¼Œå…¨è‡ªåŠ¨çš„ä¸‰ç»´å‚æ•°å»ºæ¨¡æœ‰äº†æ–°æœºé‡ã€‚ç„¶è€Œï¼Œå°†äººç±»çš„è®¾è®¡æ„å›¾ç›´æ¥è½¬åŒ–ä¸ºå¯æ‰§è¡Œçš„CADä»£ç ä»é¢ä¸´é€»è¾‘ç†è§£ã€è¯­æ³•æ­£ç¡®æ€§å’Œæ•°å€¼ç²¾ç¡®æ€§çš„æŒ‘æˆ˜ã€‚æœ¬ç ”ç©¶æå‡ºäº†CAD-RLæ¨¡å‹ï¼Œé‡‡ç”¨å¤šæ¨¡æ€æ€ç»´é“¾æŒ‡å¯¼å¼ºåŒ–å­¦ä¹ å¯¹CADå»ºæ¨¡ä»£ç ç”Ÿæˆè¿›è¡Œåè®­ç»ƒã€‚è¯¥æ¨¡å‹ç»“åˆäº†æ€ç»´é“¾çš„å†·å¯åŠ¨æŠ€æœ¯ï¼Œä»¥ç›®æ ‡é©±åŠ¨å¼ºåŒ–å­¦ä¹ è¿›è¡Œåè®­ç»ƒï¼Œå¹¶ä½¿ç”¨ä¸‰ä¸ªç‰¹å®šä»»åŠ¡å¥–åŠ±ï¼šå¯æ‰§è¡Œæ€§å¥–åŠ±ã€å‡ ä½•ç²¾åº¦å¥–åŠ±å’Œå¤–éƒ¨è¯„ä»·å¥–åŠ±ã€‚åœ¨ç¨€ç–ä¸”å¤šå˜çš„å¥–åŠ±æ¡ä»¶ä¸‹ç¡®ä¿ç¨³å®šå­¦ä¹ ï¼Œæœ¬ç ”ç©¶è¿˜æ¨å‡ºäº†ä¸‰å¤§é’ˆå¯¹æ€§ä¼˜åŒ–ç­–ç•¥ã€‚åŒæ—¶å…¬å¼€äº†ä¸€ä¸ªæ”¯æŒè®­ç»ƒå’ŒåŸºå‡†æµ‹è¯•çš„æ–°æ•°æ®é›†ExeCADï¼Œå®ƒåŒ…å«äº†ç°å®ä¸–ç•Œä¸­CADå»ºæ¨¡çš„ä¾‹å­å’Œé…å¥—çš„è‡ªç„¶è¯­è¨€æè¿°ã€ç»“æ„åŒ–çš„è®¾è®¡è¯­è¨€ä»¥åŠå¯æ‰§è¡Œè„šæœ¬ç­‰ã€‚å®éªŒç»“æœè¯æ˜äº†CAD-RLæ¨¡å‹åœ¨æ¨ç†è´¨é‡ã€è¾“å‡ºç²¾ç¡®åº¦å’Œä»£ç å¯æ‰§è¡Œæ€§æ–¹é¢å–å¾—äº†æ˜¾è‘—è¿›æ­¥ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>CADåœ¨å·¥ç¨‹å¸ˆå’Œåˆ¶é€ è¡Œä¸šä¸­çš„é‡è¦æ€§åŠå…¶æ‰‹åŠ¨å»ºæ¨¡çš„éœ€æ±‚ã€‚</li>
<li>è‡ªç„¶è¯­è¨€åˆ°ä»£ç ç”ŸæˆæŠ€æœ¯åœ¨å…¨è‡ªåŠ¨ä¸‰ç»´å‚æ•°å»ºæ¨¡ä¸­çš„åº”ç”¨åŠæŒ‘æˆ˜ã€‚</li>
<li>CAD-RLæ¨¡å‹çš„æå‡ºï¼Œç»“åˆäº†å¤šæ¨¡æ€æ€ç»´é“¾ï¼ˆCoTï¼‰å’Œå¼ºåŒ–å­¦ä¹ ç”¨äºCADå»ºæ¨¡ä»£ç ç”Ÿæˆã€‚</li>
<li>CAD-RLæ¨¡å‹é‡‡ç”¨ä¸‰ç§ä»»åŠ¡ç‰¹å®šå¥–åŠ±å’Œä¸‰å¤§ä¼˜åŒ–ç­–ç•¥æ¥æé«˜æ€§èƒ½ã€‚</li>
<li>å‘å¸ƒäº†ä¸€ä¸ªæ–°çš„æ•°æ®é›†ExeCADï¼Œç”¨äºè®­ç»ƒå’ŒåŸºå‡†æµ‹è¯•ã€‚</li>
<li>CAD-RLæ¨¡å‹åœ¨æ¨ç†è´¨é‡ã€è¾“å‡ºç²¾ç¡®åº¦å’Œä»£ç å¯æ‰§è¡Œæ€§æ–¹é¢çš„æ˜¾è‘—æ”¹å–„ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.10118">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-1d85443476044b4ab68d36d4fd5db0c2.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-8ff1bbd8251ef77aa63e82f40ea57363.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-edf1b249ffe6b6397d7803fa47ff2769.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-2f4e6552499ff5ba0a1a1295218a5604.jpg" align="middle">
</details>


<h1 id="-19"><a href="#-19" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-08-21/R1_Reasoning/">https://kedreamix.github.io/Talk2Paper/Paper/2025-08-21/R1_Reasoning/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/R1-Reasoning/">
                                    <span class="chip bg-color">R1_Reasoning</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-08-21/LLM/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-a056dd40246c173f2f35e909473a12a2.jpg" class="responsive-img" alt="LLM">
                        
                        <span class="card-title">LLM</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            LLM æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-08-21  Unintended Misalignment from Agentic Fine-Tuning Risks and Mitigation
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-08-21
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/LLM/" class="post-category">
                                    LLM
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/LLM/">
                        <span class="chip bg-color">LLM</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-08-21/Talking%20Head%20Generation/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-b08dfe6bc5716b188ddf3d94e33faa57.jpg" class="responsive-img" alt="Talking Head Generation">
                        
                        <span class="card-title">Talking Head Generation</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Talking Head Generation æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-08-21  InfiniteTalk Audio-driven Video Generation for Sparse-Frame Video   Dubbing
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-08-21
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Talking-Head-Generation/" class="post-category">
                                    Talking Head Generation
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Talking-Head-Generation/">
                        <span class="chip bg-color">Talking Head Generation</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">26254.1k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
