<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Diffusion Models">
    <meta name="description" content="Diffusion Models 方向最新论文已更新，请持续关注 Update in 2025-08-21  Latent Interpolation Learning Using Diffusion Models for Cardiac Volume   Reconstruction">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Diffusion Models | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loader页面消失采用渐隐的方式*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logo出现动画 */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//使用渐隐的方法淡出loading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//强制显示loading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">嘘~  正在从服务器偷取页面 . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>首页</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>标签</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>分类</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>归档</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="搜索" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="深色/浅色模式" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			首页
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			标签
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			分类
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			归档
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-6f8d303bf54cf2d273a6f5b383176b78.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Diffusion Models</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- 文章内容详情 -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Diffusion-Models/">
                                <span class="chip bg-color">Diffusion Models</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Diffusion-Models/" class="post-category">
                                Diffusion Models
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp;
                    2025-08-21
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>更新日期:&nbsp;&nbsp;
                    2025-08-28
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>文章字数:&nbsp;&nbsp;
                    11.7k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>阅读时长:&nbsp;&nbsp;
                    47 分
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>阅读次数:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>⚠️ 以下所有内容总结都来自于 大语言模型的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p>
</blockquote>
<h1 id="2025-08-21-更新"><a href="#2025-08-21-更新" class="headerlink" title="2025-08-21 更新"></a>2025-08-21 更新</h1><h2 id="Latent-Interpolation-Learning-Using-Diffusion-Models-for-Cardiac-Volume-Reconstruction"><a href="#Latent-Interpolation-Learning-Using-Diffusion-Models-for-Cardiac-Volume-Reconstruction" class="headerlink" title="Latent Interpolation Learning Using Diffusion Models for Cardiac Volume   Reconstruction"></a>Latent Interpolation Learning Using Diffusion Models for Cardiac Volume   Reconstruction</h2><p><strong>Authors:Niklas Bubeck, Suprosanna Shit, Chen Chen, Can Zhao, Pengfei Guo, Dong Yang, Georg Zitzlsberger, Daguang Xu, Bernhard Kainz, Daniel Rueckert, Jiazhen Pan</strong></p>
<p>Cardiac Magnetic Resonance (CMR) imaging is a critical tool for diagnosing and managing cardiovascular disease, yet its utility is often limited by the sparse acquisition of 2D short-axis slices, resulting in incomplete volumetric information. Accurate 3D reconstruction from these sparse slices is essential for comprehensive cardiac assessment, but existing methods face challenges, including reliance on predefined interpolation schemes (e.g., linear or spherical), computational inefficiency, and dependence on additional semantic inputs such as segmentation labels or motion data. To address these limitations, we propose a novel \textbf{Ca}rdiac \textbf{L}atent \textbf{I}nterpolation \textbf{D}iffusion (CaLID) framework that introduces three key innovations. First, we present a data-driven interpolation scheme based on diffusion models, which can capture complex, non-linear relationships between sparse slices and improves reconstruction accuracy. Second, we design a computationally efficient method that operates in the latent space and speeds up 3D whole-heart upsampling time by a factor of 24, reducing computational overhead compared to previous methods. Third, with only sparse 2D CMR images as input, our method achieves SOTA performance against baseline methods, eliminating the need for auxiliary input such as morphological guidance, thus simplifying workflows. We further extend our method to 2D+T data, enabling the effective modeling of spatiotemporal dynamics and ensuring temporal coherence. Extensive volumetric evaluations and downstream segmentation tasks demonstrate that CaLID achieves superior reconstruction quality and efficiency. By addressing the fundamental limitations of existing approaches, our framework advances the state of the art for spatio and spatiotemporal whole-heart reconstruction, offering a robust and clinically practical solution for cardiovascular imaging. </p>
<blockquote>
<p>心脏磁共振（CMR）成像在心血管疾病的诊断和治疗中扮演着重要角色，但其应用常常受限于二维短轴切片的稀疏采集，导致体积信息不完整。从稀疏切片进行准确的3D重建对于全面的心脏评估至关重要，但现有方法面临挑战，包括依赖预定义的插值方案（例如线性或球形插值）、计算效率低下以及依赖于额外的语义输入（如分割标签或运动数据）。为了克服这些局限性，我们提出了一种新颖的<strong>心脏潜在插值扩散（CaLID）框架</strong>，它引入了三项关键创新。首先，我们提出了一种基于扩散模型的数据驱动插值方案，该方案可以捕捉稀疏切片之间的复杂非线性关系，提高重建精度。其次，我们设计了一种在潜在空间运行的高效方法，将心脏3D上采样的时间加快了24倍，与以前的方法相比减少了计算开销。第三，我们的方法仅使用稀疏的二维CMR图像作为输入，与基线方法相比达到了最先进的性能，无需辅助输入（如形态指导），从而简化了工作流程。我们还将该方法扩展到二维+时间数据上，可以有效地模拟时空动态变化并确保时间连贯性。广泛的体积评估和下游分割任务表明，CaLID在重建质量和效率方面达到了先进技术水平。通过解决现有方法的基本局限性，我们的框架推动了空间和时间维度上心脏重建的最新技术前沿，为心血管成像提供了稳健且实用的临床解决方案。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.13826v1">PDF</a> </p>
<p><strong>Summary</strong><br>     心脏磁共振（CMR）成像在诊断和治疗心血管疾病中至关重要，但其效用常受限于二维短轴切片的稀疏采集，导致体积信息不完整。为解决现有方法在心脏潜在插值方面的局限，提出了全新的心脏潜在插值扩散（CaLID）框架。此框架采用数据驱动插值方案，提高重建精度和计算效率，仅依赖稀疏的二维CMR图像即可实现卓越性能。此外，它还能处理二维加时间的时空动态数据，确保时间连贯性。综合体积评估和下游分割任务证明，CaLID框架具有出色的重建质量和效率，为心血管成像提供了稳健且实用的解决方案。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>CMR成像在心血管疾病的诊断和管理中扮演重要角色，但受限于二维切片的稀疏采集造成的体积信息缺失问题。</li>
<li>提出一种新颖的心脏潜在插值扩散（CaLID）框架，通过数据驱动的插值方案提高重建精度。</li>
<li>CaLID框架能在潜在空间内操作并实现高效计算，显著缩短了三维心脏整体放大时间。</li>
<li>该框架无需额外的语义输入如分割标签或运动数据，仅依赖稀疏的二维CMR图像即可实现卓越性能。</li>
<li>CaLID框架可扩展到二维加时间的时空数据处理，确保时间连贯性。</li>
<li>综合体积评估和下游分割任务显示，CaLID框架在时空全心脏重建方面取得了先进成果。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.13826">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-8866a8ba3da55723b601cdb42f474c6d.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-1b3c2aacf31192a2998523e57c1e2452.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9db76104ad21d445d80009b4eb9b5fd5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7b3e8fbd563620fc41599e9edf300cdb.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5b595fe2d116ad3426b59833e331eab7.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Sketch3DVE-Sketch-based-3D-Aware-Scene-Video-Editing"><a href="#Sketch3DVE-Sketch-based-3D-Aware-Scene-Video-Editing" class="headerlink" title="Sketch3DVE: Sketch-based 3D-Aware Scene Video Editing"></a>Sketch3DVE: Sketch-based 3D-Aware Scene Video Editing</h2><p><strong>Authors:Feng-Lin Liu, Shi-Yang Li, Yan-Pei Cao, Hongbo Fu, Lin Gao</strong></p>
<p>Recent video editing methods achieve attractive results in style transfer or appearance modification. However, editing the structural content of 3D scenes in videos remains challenging, particularly when dealing with significant viewpoint changes, such as large camera rotations or zooms. Key challenges include generating novel view content that remains consistent with the original video, preserving unedited regions, and translating sparse 2D inputs into realistic 3D video outputs. To address these issues, we propose Sketch3DVE, a sketch-based 3D-aware video editing method to enable detailed local manipulation of videos with significant viewpoint changes. To solve the challenge posed by sparse inputs, we employ image editing methods to generate edited results for the first frame, which are then propagated to the remaining frames of the video. We utilize sketching as an interaction tool for precise geometry control, while other mask-based image editing methods are also supported. To handle viewpoint changes, we perform a detailed analysis and manipulation of the 3D information in the video. Specifically, we utilize a dense stereo method to estimate a point cloud and the camera parameters of the input video. We then propose a point cloud editing approach that uses depth maps to represent the 3D geometry of newly edited components, aligning them effectively with the original 3D scene. To seamlessly merge the newly edited content with the original video while preserving the features of unedited regions, we introduce a 3D-aware mask propagation strategy and employ a video diffusion model to produce realistic edited videos. Extensive experiments demonstrate the superiority of Sketch3DVE in video editing. Homepage and code: <a target="_blank" rel="noopener" href="http://http//geometrylearning.com/Sketch3DVE/">http://http://geometrylearning.com/Sketch3DVE/</a> </p>
<blockquote>
<p>最近的视频编辑方法在风格转换或外观修改方面取得了吸引人的成果。然而，编辑包含视点变化的3D场景的视频中的结构内容仍然是一个挑战，特别是在处理重大视点变化，如大幅度的相机旋转或缩放时。主要挑战包括生成与原始视频保持一致的新视图内容、保留未编辑区域，以及将稀疏的二维输入转化为逼真的三维视频输出。为了解决这些问题，我们提出了Sketch3DVE，一种基于草图的三维视频编辑方法，实现对视点变化显著的视频的详细局部操作。为了解决稀疏输入带来的挑战，我们采用图像编辑方法为第一帧生成编辑结果，然后将这些结果传播到视频的其余帧。我们使用草图作为一种交互工具进行精确的几何控制，同时支持其他基于遮罩的图像编辑方法。为了处理视点变化，我们对视频中的三维信息进行详细的分析和操作。具体来说，我们采用密集立体方法估计点云和输入视频的相机参数。然后，我们提出了一种点云编辑方法，使用深度图来表示新编辑组件的三维几何形状，并将其有效地与原始三维场景对齐。为了无缝地将新编辑的内容与原始视频合并，同时保留未编辑区域的特点，我们引入了一种三维感知遮罩传播策略，并采用了视频扩散模型来生成逼真的编辑视频。大量实验表明，Sketch3DVE在视频编辑方面的优越性。主页和代码：<a target="_blank" rel="noopener" href="http://geometrylearning.com/Sketch3DVE/%EF%BC%88%E6%B3%A8%EF%BC%9A%E7%BD%91%E5%9D%80%E9%83%A8%E5%88%86%E4%BC%BC%E4%B9%8E%E6%9C%89%E8%AF%AF%E9%87%8D%E5%A4%8D%EF%BC%8C%E5%8E%9F%E6%96%87%E4%B8%AD%E7%9A%84%E7%BD%91%E5%9D%80%E9%93%BE%E6%8E%A5%E9%9C%80%E8%A6%81%E8%BF%9B%E4%B8%80%E6%AD%A5%E6%A3%80%E6%9F%A5%E5%B9%B6%E4%BF%AE%E6%AD%A3%EF%BC%89">http://geometrylearning.com/Sketch3DVE/（注：网址部分似乎有误重复，原文中的网址链接需要进一步检查并修正）</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.13797v1">PDF</a> SIGGRAPH 2025</p>
<p><strong>摘要</strong><br>视频编辑中结构内容的编辑仍然是难点，特别是面对大视角变化时的编辑挑战重重。为实现精确的局部操作，提出了一种基于草图的3D感知视频编辑方法Sketch3DVE。利用图像编辑方法解决稀疏输入问题，利用草图作为交互工具进行精确的几何控制。通过密集立体法估计点云和输入视频的摄像机参数，提出一种点云编辑方法，使用深度图表示新编辑组件的3D几何结构，有效地与原始3D场景对齐。无缝融合新内容与原始视频，同时保留未编辑区域特征，采用视频扩散模型生成真实编辑视频。</p>
<p><strong>关键见解</strong></p>
<ol>
<li>视频的结构内容编辑，特别是大视角变化的编辑，仍是研究的挑战。</li>
<li>Sketch3DVE方法实现了基于草图的3D感知视频编辑，支持详细局部操作。</li>
<li>利用图像编辑方法解决稀疏输入问题，通过草图进行精确的几何控制。</li>
<li>采用密集立体法估计点云和摄像机参数，处理视角变化。</li>
<li>点云编辑方法使用深度图与原始3D场景对齐新编辑的3D几何结构。</li>
<li>利用视频扩散模型生成真实且无缝融合新内容与原始视频。</li>
<li>Sketch3DVE在视频编辑方面的优越性得到了广泛实验的验证。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.13797">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-80c805b39ad2e1e74a0361e2984a8d21.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-d013a4311e283c7834fe5ea23dd7ea3d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-58a59461c5f99748fd9764c968638e27.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Eliminating-Rasterization-Direct-Vector-Floor-Plan-Generation-with-DiffPlanner"><a href="#Eliminating-Rasterization-Direct-Vector-Floor-Plan-Generation-with-DiffPlanner" class="headerlink" title="Eliminating Rasterization: Direct Vector Floor Plan Generation with   DiffPlanner"></a>Eliminating Rasterization: Direct Vector Floor Plan Generation with   DiffPlanner</h2><p><strong>Authors:Shidong Wang, Renato Pajarola</strong></p>
<p>The boundary-constrained floor plan generation problem aims to generate the topological and geometric properties of a set of rooms within a given boundary. Recently, learning-based methods have made significant progress in generating realistic floor plans. However, these methods involve a workflow of converting vector data into raster images, using image-based generative models, and then converting the results back into vector data. This process is complex and redundant, often resulting in information loss. Raster images, unlike vector data, cannot scale without losing detail and precision. To address these issues, we propose a novel deep learning framework called DiffPlanner for boundary-constrained floor plan generation, which operates entirely in vector space. Our framework is a Transformer-based conditional diffusion model that integrates an alignment mechanism in training, aligning the optimization trajectory of the model with the iterative design processes of designers. This enables our model to handle complex vector data, better fit the distribution of the predicted targets, accomplish the challenging task of floor plan layout design, and achieve user-controllable generation. We conduct quantitative comparisons, qualitative evaluations, ablation experiments, and perceptual studies to evaluate our method. Extensive experiments demonstrate that DiffPlanner surpasses existing state-of-the-art methods in generating floor plans and bubble diagrams in the creative stages, offering more controllability to users and producing higher-quality results that closely match the ground truths. </p>
<blockquote>
<p>边界约束的平面图生成问题旨在生成给定边界内的一组房间的拓扑和几何属性。最近，基于学习的方法在生成逼真的平面图方面取得了显著进展。然而，这些方法需要将矢量数据转换为栅格图像，使用基于图像的生成模型，然后将结果转回矢量数据的工作流程。这个过程复杂且冗余，经常导致信息丢失。与矢量数据不同，栅格图像无法在不损失细节和精度的情况下进行缩放。为了解决这些问题，我们提出了一种名为DiffPlanner的新型深度学习框架，用于边界约束的平面图生成，它完全在矢量空间中进行操作。我们的框架是一个基于Transformer的条件扩散模型，在训练中集成了对齐机制，使模型的优化轨迹与设计师的迭代设计过程对齐。这使得我们的模型能够处理复杂的矢量数据，更好地适应预测目标分布，完成具有挑战性的平面图布局设计任务，并实现用户可控的生成。我们进行了定量比较、定性评估、剔除实验和感知研究来评估我们的方法。大量实验表明，DiffPlanner在创意阶段生成平面图和气泡图方面超越了现有最先进的方法，为用户提供更多可控性，并产生更接近真实的高质量结果。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.13738v1">PDF</a> accepted to IEEE Transactions on Visualization and Computer Graphics</p>
<p><strong>Summary</strong></p>
<p>基于边界约束的平面图生成问题旨在生成给定边界内的一组房间的拓扑和几何属性。传统的基于学习的方法需要将矢量数据转换为栅格图像，使用基于图像的生成模型，然后再将结果转回矢量数据，这一过程复杂且冗余，常常导致信息丢失。为解决这些问题，我们提出了名为DiffPlanner的新型深度学习框架，用于在矢量空间内完全进行边界约束的平面图生成。通过集成对齐机制进行培训，DiffPlanner将模型优化轨迹与设计师的迭代设计过程对齐，从而更好地处理复杂矢量数据，更好地适应预测目标的分布，完成复杂的平面图布局设计任务，并实现用户可控的生成。实验证明，DiffPlanner在创意阶段的平面图生成和气泡图生成方面超过了现有最先进的方法，为用户提供更多控制力并产生更接近真实的高质量结果。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>边界约束的平面图生成问题旨在生成给定边界内的房间拓扑和几何属性。</li>
<li>现有学习方法的转换流程复杂且冗余，存在信息丢失的问题。</li>
<li>DiffPlanner是一个新型的深度学习框架，完全在矢量空间内进行边界约束的平面图生成。</li>
<li>DiffPlanner集成了对齐机制进行培训，以优化模型处理复杂矢量数据的能力。</li>
<li>DiffPlanner可以更好地适应预测目标的分布，完成复杂的平面图布局设计任务。</li>
<li>DiffPlanner实现了用户可控的生成，为用户提供了更多控制力。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.13738">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-b40b2406d1a4fb9eb4b80c07610d3820.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-020ab9a5a8c388ce8ac4d26514c2179c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a734c880fe89137110041f5e09beafbd.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-bcc1e2275233d41a0beba57d6fc12da7.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-bdab3f877dacd0e94ef927bba2342e5e.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="DiffIER-Optimizing-Diffusion-Models-with-Iterative-Error-Reduction"><a href="#DiffIER-Optimizing-Diffusion-Models-with-Iterative-Error-Reduction" class="headerlink" title="DiffIER: Optimizing Diffusion Models with Iterative Error Reduction"></a>DiffIER: Optimizing Diffusion Models with Iterative Error Reduction</h2><p><strong>Authors:Ao Chen, Lihe Ding, Tianfan Xue</strong></p>
<p>Diffusion models have demonstrated remarkable capabilities in generating high-quality samples and enhancing performance across diverse domains through Classifier-Free Guidance (CFG). However, the quality of generated samples is highly sensitive to the selection of the guidance weight. In this work, we identify a critical &#96;&#96;training-inference gap’’ and we argue that it is the presence of this gap that undermines the performance of conditional generation and renders outputs highly sensitive to the guidance weight. We quantify this gap by measuring the accumulated error during the inference stage and establish a correlation between the selection of guidance weight and minimizing this gap. Furthermore, to mitigate this gap, we propose DiffIER, an optimization-based method for high-quality generation. We demonstrate that the accumulated error can be effectively reduced by an iterative error minimization at each step during inference. By introducing this novel plug-and-play optimization framework, we enable the optimization of errors at every single inference step and enhance generation quality. Empirical results demonstrate that our proposed method outperforms baseline approaches in conditional generation tasks. Furthermore, the method achieves consistent success in text-to-image generation, image super-resolution, and text-to-speech generation, underscoring its versatility and potential for broad applications in future research. </p>
<blockquote>
<p>扩散模型通过无分类器引导（CFG）在生成高质量样本和提高不同领域的性能方面表现出了卓越的能力。然而，生成样本的质量对引导权重的选择非常敏感。在这项工作中，我们识别出了一个关键的“训练-推理差距”，我们认为正是这个差距影响了条件生成的表现，并使输出高度依赖于引导权重。我们通过测量推理阶段的累积误差来量化这个差距，并建立了选择引导权重与最小化这个差距之间的关联。此外，为了缓解这一差距，我们提出了DiffIER，这是一种基于优化的高质量生成方法。我们证明，通过推理过程中每一步的迭代误差最小化，可以有效减少累积误差。通过引入这种新颖即插即用的优化框架，我们能够在每一个单独推理步骤中优化误差，提高生成质量。经验结果表明，我们提出的方法在条件生成任务上优于基准方法。此外，该方法在文本到图像生成、图像超分辨率和文本到语音生成方面取得了持续的成功，这突显了其在未来研究中的通用性和广泛应用潜力。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.13628v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文探讨了扩散模型在生成高质量样本和提升性能方面的显著能力，特别是通过无分类器引导（CFG）实现。然而，生成的样本质量对引导权重的选择非常敏感。本文识别出关键的“训练-推理差距”，并认为这一差距影响了条件生成性能，使输出对引导权重高度敏感。为减少这一差距，提出了DiffIER这一基于优化的高质量生成方法。通过迭代误差最小化，在推理阶段每一步优化误差，提高了生成质量。实验结果显示，该方法在条件生成任务上优于基准方法，并在文本到图像生成、图像超分辨率和文本到语音生成等任务中取得了持续的成功。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>扩散模型通过无分类器引导（CFG）展现出强大的生成能力。</li>
<li>生成的样本质量受引导权重选择的显著影响。</li>
<li>文中识别出“训练-推理差距”，这一差距影响条件生成性能。</li>
<li>提出DiffIER方法，通过优化减少推理阶段的误差积累。</li>
<li>DiffIER方法通过迭代误差最小化，在推理每一步优化误差。</li>
<li>DiffIER方法在条件生成任务上表现优于其他方法。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.13628">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-ce8567172c5b65f3efbc7b60363c2a9a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-4b930ab27e19b587a0b375e5b562d4d3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4e391cf99658d7fa44fd322b3db1d0ef.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7d613ceafcd367c3788f918797f75b6e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-90754254ba6f2d1faf962c9d63bcf529.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Dataset-Condensation-with-Color-Compensation"><a href="#Dataset-Condensation-with-Color-Compensation" class="headerlink" title="Dataset Condensation with Color Compensation"></a>Dataset Condensation with Color Compensation</h2><p><strong>Authors:Huyu Wu, Duo Su, Junjie Hou, Guang Li</strong></p>
<p>Dataset condensation always faces a constitutive trade-off: balancing performance and fidelity under extreme compression. Existing methods struggle with two bottlenecks: image-level selection methods (Coreset Selection, Dataset Quantization) suffer from inefficiency condensation, while pixel-level optimization (Dataset Distillation) introduces semantic distortion due to over-parameterization. With empirical observations, we find that a critical problem in dataset condensation is the oversight of color’s dual role as an information carrier and a basic semantic representation unit. We argue that improving the colorfulness of condensed images is beneficial for representation learning. Motivated by this, we propose DC3: a Dataset Condensation framework with Color Compensation. After a calibrated selection strategy, DC3 utilizes the latent diffusion model to enhance the color diversity of an image rather than creating a brand-new one. Extensive experiments demonstrate the superior performance and generalization of DC3 that outperforms SOTA methods across multiple benchmarks. To the best of our knowledge, besides focusing on downstream tasks, DC3 is the first research to fine-tune pre-trained diffusion models with condensed datasets. The FID results prove that training networks with our high-quality datasets is feasible without model collapse or other degradation issues. Code and generated data are available at <a target="_blank" rel="noopener" href="https://github.com/528why/Dataset-Condensation-with-Color-Compensation">https://github.com/528why/Dataset-Condensation-with-Color-Compensation</a>. </p>
<blockquote>
<p>数据集压缩始终面临一个基本的权衡：在极端压缩情况下平衡性能和保真度。现有方法面临两个瓶颈：图像级别的选择方法（核心集选择、数据集量化）存在压缩效率低下的问题，而像素级别的优化（数据集蒸馏）由于过度参数化而导致语义失真。通过经验观察，我们发现数据集压缩中的关键问题是忽视了颜色作为信息载体和基本语义表示单元的双重作用。我们认为提高浓缩图像的色彩丰富度对表示学习是有益的。受此启发，我们提出了DC3：一种具有颜色补偿的数据集压缩框架。经过校准的选择策略后，DC3利用潜在的扩散模型来提高图像的颜色多样性，而不是创建全新的图像。大量实验证明了DC3的卓越性能和泛化能力，它在多个基准测试上超越了最先进的方法。据我们所知，除了关注下游任务外，DC3是首次使用浓缩数据集对预训练扩散模型进行微调的研究。FID结果证明，使用我们高质量的数据集训练网络是可行的，不会出现模型崩溃或其他退化问题。代码和生成的数据可在<a target="_blank" rel="noopener" href="https://github.com/528why/Dataset-Condensation-with-Color-Compensation%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/528why/Dataset-Condensation-with-Color-Compensation找到。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.01139v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文介绍了数据集压缩中颜色补偿的重要性，并提出了一种名为DC3的新的数据集压缩框架。该框架利用潜在扩散模型提高图像的颜色多样性，而非创建全新图像。实验证明，DC3在多个基准测试中表现优异，优于现有方法。此外，DC3还是首次尝试使用压缩数据集微调预训练扩散模型的研究。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>数据集压缩面临性能与保真度之间的权衡。</li>
<li>现存方法面临两个瓶颈：图像级别的选择方法效率低下，像素级别的优化则会导致语义失真。</li>
<li>颜色在数据集压缩中被忽视，其作为信息载体和基本语义表示单元的双重角色至关重要。</li>
<li>提高压缩图像的色彩丰富性对表示学习有益。</li>
<li>DC3框架利用潜在扩散模型增强图像颜色多样性，而非创建新图像。</li>
<li>DC3在多个基准测试中表现优异，优于现有方法。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.01139">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-630b00efd2d061afd9a11ab944027c66.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-56864f32a6be3157cf8419598b8a0ef3.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-cb4ca5253346b54422b59417f9e47a24.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-74b82e4cecdb2b1c651ed8d1c683724a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-9598b7bdd0694b6317b792c181241df0.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-8ad3d49e29ca448dc9ffb35a01428f9b.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="LoRA-Edit-Controllable-First-Frame-Guided-Video-Editing-via-Mask-Aware-LoRA-Fine-Tuning"><a href="#LoRA-Edit-Controllable-First-Frame-Guided-Video-Editing-via-Mask-Aware-LoRA-Fine-Tuning" class="headerlink" title="LoRA-Edit: Controllable First-Frame-Guided Video Editing via Mask-Aware   LoRA Fine-Tuning"></a>LoRA-Edit: Controllable First-Frame-Guided Video Editing via Mask-Aware   LoRA Fine-Tuning</h2><p><strong>Authors:Chenjian Gao, Lihe Ding, Xin Cai, Zhanpeng Huang, Zibin Wang, Tianfan Xue</strong></p>
<p>Video editing using diffusion models has achieved remarkable results in generating high-quality edits for videos. However, current methods often rely on large-scale pretraining, limiting flexibility for specific edits. First-frame-guided editing provides control over the first frame, but lacks flexibility over subsequent frames. To address this, we propose a mask-based LoRA (Low-Rank Adaptation) tuning method that adapts pretrained Image-to-Video (I2V) models for flexible video editing. Our key innovation is using a spatiotemporal mask to strategically guide the LoRA fine-tuning process. This teaches the model two distinct skills: first, to interpret the mask as a command to either preserve content from the source video or generate new content in designated regions. Second, for these generated regions, LoRA learns to synthesize either temporally consistent motion inherited from the video or novel appearances guided by user-provided reference frames. This dual-capability LoRA grants users control over the edit’s entire temporal evolution, allowing complex transformations like an object rotating or a flower blooming. Experimental results show our method achieves superior video editing performance compared to baseline methods. Project Page: <a target="_blank" rel="noopener" href="https://cjeen.github.io/LoRAEdit">https://cjeen.github.io/LoRAEdit</a> </p>
<blockquote>
<p>使用扩散模型进行视频编辑已经在为高质量视频生成编辑方面取得了显著成果。然而，当前的方法通常依赖于大规模预训练，这限制了特定编辑的灵活性。第一帧引导编辑提供了对第一帧的控制，但缺乏对后续帧的灵活性。为了解决这一问题，我们提出了一种基于掩膜的LoRA（低秩适应）调优方法，该方法可适应预训练的图生视频（I2V）模型，用于灵活视频编辑。我们的主要创新之处在于使用时空掩膜来战略性指导LoRA微调过程。这教会了模型两种独特技能：首先，将掩膜解释为来自源视频的指令，要么保留内容，要么在指定区域生成新内容。其次，对于这些生成的区域，LoRA学习合成从视频中继承的时间连贯运动或根据用户提供的参考帧引导的新外观。这种双功能的LoRA使用户能够控制整个时间轴的编辑过程，从而实现复杂的转换，如物体旋转或花朵绽放等。实验结果表明，我们的方法相较于基线方法实现了更出色的视频编辑性能。项目页面：<a target="_blank" rel="noopener" href="https://cjeen.github.io/LoRAEdit">https://cjeen.github.io/LoRAEdit</a> 。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.10082v5">PDF</a> 9 pages</p>
<p><strong>Summary</strong><br>扩散模型在视频编辑领域已取得显著成果，能够生成高质量的视频编辑。但现有方法常依赖大规模预训练，对特定编辑的灵活性有限。为解决这一问题，我们提出基于掩膜的LoRA（低秩适应）调优方法，用于适应预训练的图生视频（I2V）模型以实现灵活的视频编辑。实验结果显示，该方法相较于基线方法实现了更优越的视频编辑性能。</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>扩散模型在视频编辑中表现优异，能生成高质量编辑。</li>
<li>现有方法依赖大规模预训练，缺乏特定编辑的灵活性。</li>
<li>提出基于掩膜的LoRA调优方法，适应I2V模型进行灵活视频编辑。</li>
<li>使用时空掩膜指导LoRA微调过程，教会模型两种技能：保留源视频内容或在指定区域生成新内容。</li>
<li>LoRA学会合成与视频一致的临时运动或根据用户提供的参考帧生成新外观。</li>
<li>双能力LoRA让用户控制整个时间演化的编辑，实现复杂转换。</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.10082">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-8780bfbcc28ad387e3a2fa9ca6ff71b9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2b523bea842cbe6864369eb70527919c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-afc1b58b28c47fc05816d61943753412.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-cbd38fa37eb7e2f5814e8a2fceab919c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-52615007f314127bbbab7a1f524cf042.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e694cbae35f3db72a7c567e0dbd23157.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Hyperspectral-Image-Generation-with-Unmixing-Guided-Diffusion-Model"><a href="#Hyperspectral-Image-Generation-with-Unmixing-Guided-Diffusion-Model" class="headerlink" title="Hyperspectral Image Generation with Unmixing Guided Diffusion Model"></a>Hyperspectral Image Generation with Unmixing Guided Diffusion Model</h2><p><strong>Authors:Shiyu Shen, Bin Pan, Ziye Zhang, Zhenwei Shi</strong></p>
<p>We address hyperspectral image (HSI) synthesis, a problem that has garnered growing interest yet remains constrained by the conditional generative paradigms that limit sample diversity. While diffusion models have emerged as a state-of-the-art solution for high-fidelity image generation, their direct extension from RGB to hyperspectral domains is challenged by the high spectral dimensionality and strict physical constraints inherent to HSIs. To overcome the challenges, we introduce a diffusion framework explicitly guided by hyperspectral unmixing. The approach integrates two collaborative components: (i) an unmixing autoencoder that projects generation from the image domain into a low-dimensional abundance manifold, thereby reducing computational burden while maintaining spectral fidelity; and (ii) an abundance diffusion process that enforces non-negativity and sum-to-one constraints, ensuring physical consistency of the synthesized data. We further propose two evaluation metrics tailored to hyperspectral characteristics. Comprehensive experiments, assessed with both conventional measures and the proposed metrics, demonstrate that our method produces HSIs with both high quality and diversity, advancing the state of the art in hyperspectral data generation. </p>
<blockquote>
<p>我们关注高光谱图像（HSI）的合成问题。这一问题虽然引起了越来越多的兴趣，但仍受到有条件生成范式的约束，这些范式限制了样本的多样性。虽然扩散模型已成为高保真图像生成的最先进解决方案，但它们从RGB直接扩展到高光谱领域却面临着高光谱维度高和HSI固有的严格物理约束的挑战。为了克服这些挑战，我们引入了一个由高光谱解混明确指导的扩散框架。该方法结合了两个协作组件：（i）一个解混自编码器，将图像域的生成投影到低维丰度流形上，从而在保持光谱保真度的同时减少计算负担；（ii）一个丰度扩散过程，强制执行非负性和总和为一的约束，确保合成数据的物理一致性。我们还提出了两个针对高光谱特性定制的评价指标。通过常规度量方法和所提出的指标进行的综合实验表明，我们的方法生成的高光谱图像质量高、多样性好，推动了高光谱数据生成的最新技术进展。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.02601v3">PDF</a> </p>
<p><strong>Summary</strong>：</p>
<p>针对高光谱图像（HSI）合成问题，由于条件生成范式限制了样本多样性，虽然扩散模型在生成高质量图像方面表现出卓越性能，但其直接扩展到高光谱领域面临高光谱维度和严格物理约束的挑战。为克服这些挑战，我们提出了一个由高光谱解混引导的扩散框架，该框架包括两个协作组件：一）解混自编码器将图像域的生成投射到低维丰度流形上，以减少计算负担并保持光谱保真度；二）丰度扩散过程强制实施非负性和总和为一的约束，以确保合成数据的物理一致性。我们的方法生成的高光谱图像具有高质量和多样性，推动了高光谱数据生成的最新技术。</p>
<p><strong>Key Takeaways</strong>：</p>
<ol>
<li>高光谱图像合成问题受限于条件生成范式的样本多样性。</li>
<li>扩散模型在生成高质量图像方面具有卓越性能，但直接应用于高光谱领域面临挑战。</li>
<li>提出的扩散框架由高光谱解混引导，包括解混自编码器和丰度扩散过程两个协作组件。</li>
<li>解混自编码器将图像生成投射到低维丰度流形上，以维持光谱保真度并降低计算负担。</li>
<li>丰度扩散过程确保合成数据的物理一致性，通过实施非负性和总和为一的约束。</li>
<li>提出的方法生成的高光谱图像具有高质量和多样性。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.02601">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-1b8a07d4ee1d25cfc666ae508eb50897.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-adb3684ccff21516b9e30045d0378912.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-672383b77264f0a88ace6ac3bad1a191.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-4eab5c0bc29cd9fd2d904aeb92d34906.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a82671eaeabd16ced1b2872e6d6d1d9c.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Image-Augmentation-Agent-for-Weakly-Supervised-Semantic-Segmentation"><a href="#Image-Augmentation-Agent-for-Weakly-Supervised-Semantic-Segmentation" class="headerlink" title="Image Augmentation Agent for Weakly Supervised Semantic Segmentation"></a>Image Augmentation Agent for Weakly Supervised Semantic Segmentation</h2><p><strong>Authors:Wangyu Wu, Xianglin Qiu, Siqi Song, Zhenhong Chen, Xiaowei Huang, Fei Ma, Jimin Xiao</strong></p>
<p>Weakly-supervised semantic segmentation (WSSS) has achieved remarkable progress using only image-level labels. However, most existing WSSS methods focus on designing new network structures and loss functions to generate more accurate dense labels, overlooking the limitations imposed by fixed datasets, which can constrain performance improvements. We argue that more diverse trainable images provides WSSS richer information and help model understand more comprehensive semantic pattern. Therefore in this paper, we introduce a novel approach called Image Augmentation Agent (IAA) which shows that it is possible to enhance WSSS from data generation perspective. IAA mainly design an augmentation agent that leverages large language models (LLMs) and diffusion models to automatically generate additional images for WSSS. In practice, to address the instability in prompt generation by LLMs, we develop a prompt self-refinement mechanism. It allow LLMs to re-evaluate the rationality of generated prompts to produce more coherent prompts. Additionally, we insert an online filter into diffusion generation process to dynamically ensure the quality and balance of generated images. Experimental results show that our method significantly surpasses state-of-the-art WSSS approaches on the PASCAL VOC 2012 and MS COCO 2014 datasets. </p>
<blockquote>
<p>弱监督语义分割（WSSS）仅使用图像级标签取得了显著的进步。然而，大多数现有的WSSS方法主要集中在设计新的网络结构和损失函数来生成更准确的密集标签，忽视了固定数据集所带来的限制，这些限制可能会限制性能的提升。我们认为，提供更多可训练图像的多样性可以为WSSS提供更丰富的信息，并帮助模型理解更全面的语义模式。因此，在本文中，我们引入了一种新方法，称为图像增强代理（IAA），它表明从数据生成的角度增强WSSS是可能的。IAA主要设计一个增强代理，利用大型语言模型（LLM）和扩散模型自动为WSSS生成额外的图像。在实践中，为了解决LLM提示生成中的不稳定问题，我们开发了一种提示自我优化机制。它允许LLM重新评估生成的提示的合理性，以产生更连贯的提示。此外，我们在扩散生成过程中插入了一个在线过滤器，以动态确保生成的图像的质量和平衡。实验结果表明，我们的方法在PASCAL VOC 2012和MS COCO 2014数据集上显著超越了最先进的WSSS方法。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.20439v3">PDF</a> Accepted at Neurocomputing 2025</p>
<p><strong>Summary</strong><br>     本文提出一种名为Image Augmentation Agent（IAA）的方法，从数据生成的角度提升弱监督语义分割（WSSS）的性能。该方法利用大型语言模型（LLMs）和扩散模型自动生成额外的图像，增强WSSS的训练图像多样性。同时，为解决LLMs在生成提示时的不稳定性问题，提出了提示自我优化机制，并通过在线过滤器确保生成图像的质量和平衡性。在PASCAL VOC 2012和MS COCO 2014数据集上的实验结果显示，该方法显著超越了现有的WSSS方法。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>IAA方法从数据生成角度提升WSSS性能，通过自动生成额外图像增加训练图像多样性。</li>
<li>利用大型语言模型（LLMs）和扩散模型实现图像自动生成。</li>
<li>提出提示自我优化机制，解决LLMs在生成提示时的不稳定性问题。</li>
<li>通过在线过滤器确保生成图像的质量和平衡性。</li>
<li>IAA方法在PASCAL VOC 2012和MS COCO 2014数据集上表现优异，显著超越现有WSSS方法。</li>
<li>该方法丰富了语义信息，有助于模型理解更全面的语义模式。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.20439">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-2e7d8f479fa40cf856cd3813f060316f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e600ce67b4bc330051c553eed649b691.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Understanding-and-Mitigating-Memorization-in-Generative-Models-via-Sharpness-of-Probability-Landscapes"><a href="#Understanding-and-Mitigating-Memorization-in-Generative-Models-via-Sharpness-of-Probability-Landscapes" class="headerlink" title="Understanding and Mitigating Memorization in Generative Models via   Sharpness of Probability Landscapes"></a>Understanding and Mitigating Memorization in Generative Models via   Sharpness of Probability Landscapes</h2><p><strong>Authors:Dongjae Jeon, Dueun Kim, Albert No</strong></p>
<p>In this paper, we introduce a geometric framework to analyze memorization in diffusion models through the sharpness of the log probability density. We mathematically justify a previously proposed score-difference-based memorization metric by demonstrating its effectiveness in quantifying sharpness. Additionally, we propose a novel memorization metric that captures sharpness at the initial stage of image generation in latent diffusion models, offering early insights into potential memorization. Leveraging this metric, we develop a mitigation strategy that optimizes the initial noise of the generation process using a sharpness-aware regularization term. The code is publicly available at <a target="_blank" rel="noopener" href="https://github.com/Dongjae0324/sharpness_memorization_diffusion">https://github.com/Dongjae0324/sharpness_memorization_diffusion</a>. </p>
<blockquote>
<p>本文介绍了一个基于几何的框架，通过对数概率密度的尖锐程度来分析扩散模型中的记忆化。我们通过证明其在量化尖锐度方面的有效性，为之前提出的基于分数差异的记忆化度量提供了数学依据。此外，我们提出了一个新的记忆化度量标准，该标准能够捕捉潜在扩散模型的图像生成初始阶段的尖锐度，为潜在的记忆化提供早期见解。利用这一度量标准，我们开发了一种缓解策略，通过采用尖锐度感知正则化项来优化生成过程的初始噪声。代码公开在<a target="_blank" rel="noopener" href="https://github.com/Dongjae0324/sharpness_memorization_diffusion%E3%80%82">https://github.com/Dongjae0324/sharpness_memorization_diffusion。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.04140v5">PDF</a> Accepted at ICML 2025 (Spotlight). Code:   <a target="_blank" rel="noopener" href="https://github.com/Dongjae0324/sharpness_memorization_diffusion">https://github.com/Dongjae0324/sharpness_memorization_diffusion</a></p>
<p><strong>Summary</strong>：<br>本论文通过几何框架分析了扩散模型中的记忆能力，主要通过概率密度对数的锐度进行阐述。文章从数学角度证明了一个基于评分差异的记忆力指标的有效性，并展示其在量化锐度方面的作用。此外，文章还提出了一种新的记忆力指标，用于捕捉图像生成初期在潜在扩散模型中的锐度，为潜在记忆力提供早期洞察。利用这一指标，研究团队开发了一种优化生成过程初始噪声的策略，即采用锐度感知的正则化项。相关代码已公开在<a target="_blank" rel="noopener" href="https://github.com/Dongjae0324/sharpness_memorization_diffusion%E3%80%82">https://github.com/Dongjae0324/sharpness_memorization_diffusion。</a></p>
<p><strong>Key Takeaways</strong>：</p>
<ol>
<li>引入几何框架分析扩散模型中的记忆力。</li>
<li>通过概率密度对数的锐度进行阐述。</li>
<li>数学证明了基于评分差异的记忆力指标的有效性。</li>
<li>提出新的记忆力指标，捕捉图像生成初期的锐度。</li>
<li>利用新的记忆力指标开发了一种优化生成过程的方法。</li>
<li>优化策略聚焦于初始噪声的优化。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.04140">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-e9b546197bfa5c7c93e7db3bb2a17a16.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e2456f7bcce1b2687d2077f7b0e83408.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6f8d303bf54cf2d273a6f5b383176b78.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-5e692c9963094f7eef5a04af0ffab94f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2145a309848de8395686f607d60345dc.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="HouseCrafter-Lifting-Floorplans-to-3D-Scenes-with-2D-Diffusion-Model"><a href="#HouseCrafter-Lifting-Floorplans-to-3D-Scenes-with-2D-Diffusion-Model" class="headerlink" title="HouseCrafter: Lifting Floorplans to 3D Scenes with 2D Diffusion Model"></a>HouseCrafter: Lifting Floorplans to 3D Scenes with 2D Diffusion Model</h2><p><strong>Authors:Hieu T. Nguyen, Yiwen Chen, Vikram Voleti, Varun Jampani, Huaizu Jiang</strong></p>
<p>We introduce HouseCrafter, a novel approach that can lift a floorplan into a complete large 3D indoor scene (e.g., a house). Our key insight is to adapt a 2D diffusion model, which is trained on web-scale images, to generate consistent multi-view color (RGB) and depth (D) images across different locations of the scene. Specifically, the RGB-D images are generated autoregressively in a batch-wise manner along sampled locations based on the floorplan, where previously generated images are used as condition to the diffusion model to produce images at nearby locations. The global floorplan and attention design in the diffusion model ensures the consistency of the generated images, from which a 3D scene can be reconstructed. Through extensive evaluation on the 3D-Front dataset, we demonstrate that HouseCraft can generate high-quality house-scale 3D scenes. Ablation studies also validate the effectiveness of different design choices. We will release our code and model weights. Project page: <a target="_blank" rel="noopener" href="https://neu-vi.github.io/houseCrafter/">https://neu-vi.github.io/houseCrafter/</a> </p>
<blockquote>
<p>我们介绍了HouseCrafter这一新方法，它能够将平面地板设计转化为一个完整的大型室内三维场景（例如房子）。我们的主要见解是适应二维扩散模型，该模型在网页规模图像上进行训练，以生成不同场景位置的一致的多视角彩色（RGB）和深度（D）图像。具体而言，基于地板设计，沿着采样位置以批量方式生成RGB-D图像，其中先前生成的图像被用作扩散模型的调节条件以产生邻近位置的图像。全局的地板设计和扩散模型中的注意力机制确保了生成图像的一致性，从而可以从这些图像重建三维场景。我们在3D-Front数据集上进行了广泛评估，证明了HouseCraft可以生成高质量的房子规模三维场景。消融研究也验证了不同设计选择的有效性。我们将发布我们的代码和模型权重。项目页面：[链接]（待填写具体链接地址）。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2406.20077v2">PDF</a> </p>
<p><strong>Summary</strong>：我们推出HouseCrafter，这是一种新颖的方法，能将平面图提升为完整的大型室内三维场景（例如房屋）。我们的关键见解是适应二维扩散模型，该模型在网页规模图像上进行训练，以生成场景不同位置的连续多视图彩色（RGB）和深度（D）图像。具体来说，基于平面图采样的位置以批处理方式自动生成RGB-D图像，其中先前生成的图像用作扩散模型的条件来生成附近位置的图像。扩散模型中的全局平面图和注意力设计确保了生成图像的一致性，从中可以重建三维场景。在3D-Front数据集上的广泛评估表明，HouseCraft可以生成高质量的房子规模三维场景。</p>
<p><strong>Key Takeaways</strong>：</p>
<ol>
<li>HouseCrafter是一种将平面图转化为完整大型室内三维场景的新方法。</li>
<li>该方法利用二维扩散模型，在网页规模图像上进行训练。</li>
<li>HouseCrafter能生成场景不同位置的连续多视图彩色（RGB）和深度（D）图像。</li>
<li>RGB-D图像是基于平面图采样的位置以批处理方式生成的。</li>
<li>先前生成的图像被用作条件来生成附近位置的图像，确保了一致性。</li>
<li>通过全局平面图和扩散模型中的注意力设计，生成了连贯的三维场景。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2406.20077">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-d6a1f69ea2c303b2a8296869c0c40ba7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c727ce5a1be140cdd8f8875b68695801.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e4e36d928ce8c239f7ca2e529fa0934f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-5bdbfe2040708564a8127a017be6c823.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-cebe93f960c870aac06a74cb4db587af.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="CCDM-Continuous-Conditional-Diffusion-Models-for-Image-Generation"><a href="#CCDM-Continuous-Conditional-Diffusion-Models-for-Image-Generation" class="headerlink" title="CCDM: Continuous Conditional Diffusion Models for Image Generation"></a>CCDM: Continuous Conditional Diffusion Models for Image Generation</h2><p><strong>Authors:Xin Ding, Yongwei Wang, Kao Zhang, Z. Jane Wang</strong></p>
<p>Continuous Conditional Generative Modeling (CCGM) estimates high-dimensional data distributions, such as images, conditioned on scalar continuous variables (aka regression labels). While Continuous Conditional Generative Adversarial Networks (CcGANs) were designed for this task, their instability during adversarial learning often leads to suboptimal results. Conditional Diffusion Models (CDMs) offer a promising alternative, generating more realistic images, but their diffusion processes, label conditioning, and model fitting procedures are either not optimized for or incompatible with CCGM, making it difficult to integrate CcGANs’ vicinal approach. To address these issues, we introduce Continuous Conditional Diffusion Models (CCDMs), the first CDM specifically tailored for CCGM. CCDMs address existing limitations with specially designed conditional diffusion processes, a novel hard vicinal image denoising loss, a customized label embedding method, and efficient conditional sampling procedures. Through comprehensive experiments on four datasets with resolutions ranging from 64x64 to 192x192, we demonstrate that CCDMs outperform state-of-the-art CCGM models, establishing a new benchmark. Ablation studies further validate the model design and implementation, highlighting that some widely used CDM implementations are ineffective for the CCGM task. Our code is publicly available at <a target="_blank" rel="noopener" href="https://github.com/UBCDingXin/CCDM">https://github.com/UBCDingXin/CCDM</a>. </p>
<blockquote>
<p>连续条件生成模型（CCGM）用于估计高维数据分布，如图像，并根据标量连续变量（即回归标签）进行条件化。虽然连续条件生成对抗网络（CcGANs）是为这一任务而设计的，但它们在对抗学习过程中的不稳定性往往导致结果不尽人意。条件扩散模型（CDMs）提供了一个有前景的替代方案，能够生成更逼真的图像，但它们的扩散过程、标签条件和模型拟合程序并未针对CCGM进行优化或不兼容，这使得难以融入CcGANs的邻近方法。为了解决这些问题，我们引入了连续条件扩散模型（CCDMs），这是专门为CCGM设计的第一个CDM。CCDMs通过特殊设计的条件扩散过程、新颖的硬邻近图像去噪损失、定制的标签嵌入方法和高效的条件采样程序，解决了现有限制。我们在四个不同分辨率（从64x64到192x192）的数据集上进行了全面的实验，证明CCDMs超越了最新的CCGM模型，建立了新的基准。消融研究进一步验证了模型设计和实现，强调一些广泛使用的CDM实现对于CCGM任务无效。我们的代码公开在<a target="_blank" rel="noopener" href="https://github.com/UBCDingXin/CCDM%E3%80%82">https://github.com/UBCDingXin/CCDM。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2405.03546v4">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>连续条件生成模型（CCGM）用于估计图像等高维数据分布，该分布受标量连续变量（即回归标签）的制约。虽然连续条件生成对抗网络（CcGANs）被设计用于此任务，但其对抗学习过程中的不稳定性常常导致结果不尽人意。条件扩散模型（CDMs）提供了一个前景可观的替代方案，能生成更真实的图像，但它们对扩散过程、标签制约和模型拟合程序的设计并不优化甚至与CCGM不兼容。为解决这些问题，我们推出专门面向CCGM设计的连续条件扩散模型（CCDMs）。CCDMs解决了现有限制，具备特殊设计的条件扩散过程、新颖的硬vicinal图像去噪损失、定制化的标签嵌入方法和高效的条件采样程序。在四个不同分辨率（从64x64到192x192）的数据集上进行的综合实验表明，CCDMs的表现优于最先进的状态CCGM模型，并建立了新的基准。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>连续条件生成模型（CCGM）旨在估计高维数据分布，如图像，受连续变量（回归标签）制约。</li>
<li>连续条件生成对抗网络（CcGANs）虽然为此任务设计，但对抗学习过程中的不稳定性导致结果不佳。</li>
<li>条件扩散模型（CDMs）能生成更真实的图像，但对CCGM的扩散过程、标签制约和模型拟合程序并未优化或不兼容。</li>
<li>为解决上述问题，提出了连续条件扩散模型（CCDMs），针对CCGM进行了特殊设计。</li>
<li>CCDMs通过特殊的扩散过程、新的硬vicinal图像去噪损失、定制化的标签嵌入方法和高效的采样程序解决了现有问题。</li>
<li>综合实验证明，CCDMs在多个数据集上的表现优于其他先进CCGM模型，建立了新的性能基准。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2405.03546">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-710ef938794470bc2916ca31ca71f36c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a855bd5531f0739adfa236d176f472df.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-98d3eacfc06315a3bc806c69d4db2c33.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-8f41e4db9f37a179b36294affa7acf18.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="Diffusion-Noise-Feature-Accurate-and-Fast-Generated-Image-Detection"><a href="#Diffusion-Noise-Feature-Accurate-and-Fast-Generated-Image-Detection" class="headerlink" title="Diffusion Noise Feature: Accurate and Fast Generated Image Detection"></a>Diffusion Noise Feature: Accurate and Fast Generated Image Detection</h2><p><strong>Authors:Yichi Zhang, Xiaogang Xu</strong></p>
<p>Generative models now produce images with such stunning realism that they can easily deceive the human eye. While this progress unlocks vast creative potential, it also presents significant risks, such as the spread of misinformation. Consequently, detecting generated images has become a critical research challenge. However, current detection methods are often plagued by low accuracy and poor generalization. In this paper, to address these limitations and enhance the detection of generated images, we propose a novel representation, Diffusion Noise Feature (DNF). Derived from the inverse process of diffusion models, DNF effectively amplifies the subtle, high-frequency artifacts that act as fingerprints of artificial generation. Our key insight is that real and generated images exhibit distinct DNF signatures, providing a robust basis for differentiation. By training a simple classifier such as ResNet-50 on DNF, our approach achieves remarkable accuracy, robustness, and generalization in detecting generated images, including those from unseen generators or with novel content. Extensive experiments across four training datasets and five test sets confirm that DNF establishes a new state-of-the-art in generated image detection. The code is available at <a target="_blank" rel="noopener" href="https://github.com/YichiCS/Diffusion-Noise-Feature">https://github.com/YichiCS/Diffusion-Noise-Feature</a>. </p>
<blockquote>
<p>生成模型现在能够生成如此逼真的图像，以至于它们可以轻松欺骗人眼。虽然这一进展释放了巨大的创造力，但也带来了显著的风险，例如误导信息的传播。因此，检测生成图像已成为一项重要的研究挑战。然而，当前的检测方法往往存在准确性低和泛化能力差的困扰。针对这些局限性并增强对生成图像的检测能力，本文提出了一种新的表示方法：扩散噪声特征（DNF）。DNF来源于扩散模型的逆过程，有效地放大了作为人工生成指纹的微妙高频伪影。我们的关键见解是，真实和生成的图像表现出不同的DNF签名，这为区分它们提供了坚实的基础。通过在DNF上训练像ResNet-50这样的简单分类器，我们的方法在检测生成图像时实现了令人印象深刻的准确性、稳健性和泛化能力，包括来自未见过的生成器或具有新内容的情况。在四个训练数据集和五个测试集上进行的广泛实验证实，DNF在生成图像检测方面建立了最新的最先进的水平。代码可用在<a target="_blank" rel="noopener" href="https://github.com/YichiCS/Diffusion-Noise-Feature%E3%80%82">https://github.com/YichiCS/Diffusion-Noise-Feature。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2312.02625v3">PDF</a> Accepted by ECAI 2025</p>
<p><strong>Summary</strong></p>
<p>本文提出一种新型图像检测策略，即扩散噪声特征（DNF），用以识别生成模型的图像。该策略利用扩散模型的逆过程提取特征，有效放大生成图像的高频伪影，从而实现精准识别。实验证明，DNF在检测不同生成器甚至新型内容的生成图像时，具有出色的准确性、稳健性和泛化能力，为生成图像检测领域树立了新的标杆。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>生成模型的图像具有逼真的现实主义效果，但同时也存在传播误导信息的重大风险。</li>
<li>当前图像检测方法的准确性和泛化能力有待提高。</li>
<li>扩散噪声特征（DNF）是从扩散模型的逆过程中提取的新型表示方法。</li>
<li>DNF能有效放大生成图像的高频伪影，这些伪影作为人工生成的指纹。</li>
<li>DNF显示真实和生成图像之间的明显差异，为区分它们提供了稳健的基础。</li>
<li>通过使用如ResNet-50等简单分类器进行训练，DNF策略在检测生成图像方面取得了突破性成果。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2312.02625">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-462db27cd7deecd35d1eca7be1136b65.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7672dc6dd2de545464185f2697da87b0.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-61f41f8452d3b2c9077bb1a13b5b3989.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-756bc807e52e596e99d927d7be75ef83.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-3bf08cfe397336b3f2c6584c33750d16.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-5f411c4d6e85e52f77b56ebdc71e7a88.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        文章作者:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        文章链接:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-08-21/Diffusion%20Models/">https://kedreamix.github.io/Talk2Paper/Paper/2025-08-21/Diffusion%20Models/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        版权声明:
                    </i>
                </span>
                <span class="reprint-info">
                    本博客所有文章除特別声明外，均采用
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    许可协议。转载请注明来源
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Diffusion-Models/">
                                    <span class="chip bg-color">Diffusion Models</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;上一篇</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-08-21/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-e8eb254c468079debe400db19dd9614a.jpg" class="responsive-img" alt="医学图像">
                        
                        <span class="card-title">医学图像</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            医学图像 方向最新论文已更新，请持续关注 Update in 2025-08-21  ASDFormer A Transformer with Mixtures of Pooling-Classifier Experts for   Robust Autism Diagnosis and Biomarker Discovery
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-08-21
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/" class="post-category">
                                    医学图像
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                        <span class="chip bg-color">医学图像</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                下一篇&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-08-21/NeRF/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-f46bd541e3d005664e10464a34b8fe16.jpg" class="responsive-img" alt="NeRF">
                        
                        <span class="card-title">NeRF</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            NeRF 方向最新论文已更新，请持续关注 Update in 2025-08-21  Is-NeRF In-scattering Neural Radiance Field for Blurred Images
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-08-21
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/NeRF/" class="post-category">
                                    NeRF
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/NeRF/">
                        <span class="chip bg-color">NeRF</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- 代码块功能依赖 -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- 代码语言 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- 代码块复制 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- 代码块收缩 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC 悬浮按钮. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* 修复文章卡片 div 的宽度. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // 切换TOC目录展开收缩的相关操作.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;站点总字数:&nbsp;<span
                        class="white-color">26633.8k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- 运行天数提醒. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // 区分是否有年份.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = '本站已运行 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                daysTip = '本站已運行 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = '本站已运行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = '本站已運行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="关注我的知乎: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">知</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- 搜索遮罩框 -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span>
            <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- 白天和黑夜主题 -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- 回到顶部按钮 -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // 只在桌面版网页启用特效
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- 雪花特效 -->
    

    <!-- 鼠标星星特效 -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--腾讯兔小巢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- 动态标签 -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Σ(っ °Д °;)っ诶，页面崩溃了嘛？", clearTimeout(st)) : (document.title = "φ(゜▽゜*)♪咦，又好了！", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
