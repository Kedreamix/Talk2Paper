<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Few-Shot">
    <meta name="description" content="Few-Shot æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-08-21  DictAS A Framework for Class-Generalizable Few-Shot Anomaly   Segmentation via Dictionary Lookup">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Few-Shot | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-bd8c4a3b464e51d4bf1a247a25254577.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Few-Shot</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Few-Shot/">
                                <span class="chip bg-color">Few-Shot</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Few-Shot/" class="post-category">
                                Few-Shot
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-08-21
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-09-08
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    12.7k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    51 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-08-21-æ›´æ–°"><a href="#2025-08-21-æ›´æ–°" class="headerlink" title="2025-08-21 æ›´æ–°"></a>2025-08-21 æ›´æ–°</h1><h2 id="DictAS-A-Framework-for-Class-Generalizable-Few-Shot-Anomaly-Segmentation-via-Dictionary-Lookup"><a href="#DictAS-A-Framework-for-Class-Generalizable-Few-Shot-Anomaly-Segmentation-via-Dictionary-Lookup" class="headerlink" title="DictAS: A Framework for Class-Generalizable Few-Shot Anomaly   Segmentation via Dictionary Lookup"></a>DictAS: A Framework for Class-Generalizable Few-Shot Anomaly   Segmentation via Dictionary Lookup</h2><p><strong>Authors:Zhen Qu, Xian Tao, Xinyi Gong, ShiChen Qu, Xiaopei Zhang, Xingang Wang, Fei Shen, Zhengtao Zhang, Mukesh Prasad, Guiguang Ding</strong></p>
<p>Recent vision-language models (e.g., CLIP) have demonstrated remarkable class-generalizable ability to unseen classes in few-shot anomaly segmentation (FSAS), leveraging supervised prompt learning or fine-tuning on seen classes. However, their cross-category generalization largely depends on prior knowledge of real seen anomaly samples. In this paper, we propose a novel framework, namely DictAS, which enables a unified model to detect visual anomalies in unseen object categories without any retraining on the target data, only employing a few normal reference images as visual prompts. The insight behind DictAS is to transfer dictionary lookup capabilities to the FSAS task for unseen classes via self-supervised learning, instead of merely memorizing the normal and abnormal feature patterns from the training set. Specifically, DictAS mainly consists of three components: (1) <strong>Dictionary Construction</strong> - to simulate the index and content of a real dictionary using features from normal reference images. (2) <strong>Dictionary Lookup</strong> - to retrieve queried region features from the dictionary via a sparse lookup strategy. When a query feature cannot be retrieved, it is classified as an anomaly. (3) <strong>Query Discrimination Regularization</strong>- to enhance anomaly discrimination by making abnormal features harder to retrieve from the dictionary. To achieve this, Contrastive Query Constraint and Text Alignment Constraint are further proposed. Extensive experiments on seven public industrial and medical datasets demonstrate that DictAS consistently outperforms state-of-the-art FSAS methods. </p>
<blockquote>
<p>æœ€è¿‘çš„è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆä¾‹å¦‚CLIPï¼‰åœ¨å°‘é‡å¼‚å¸¸åˆ†å‰²ï¼ˆFSASï¼‰ä¸­æ˜¾ç¤ºå‡ºå¯¹æœªè§ç±»åˆ«çš„ç±»é€šç”¨èƒ½åŠ›ï¼Œè¿™å¾—ç›Šäºå¯¹å¯è§ç±»åˆ«çš„æœ‰ç›‘ç£æç¤ºå­¦ä¹ æˆ–å¾®è°ƒã€‚ç„¶è€Œï¼Œå®ƒä»¬çš„è·¨ç±»åˆ«æ³›åŒ–åœ¨å¾ˆå¤§ç¨‹åº¦ä¸Šä¾èµ–äºçœŸå®å¯è§å¼‚å¸¸æ ·æœ¬çš„å…ˆéªŒçŸ¥è¯†ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°å‹æ¡†æ¶ï¼Œå³DictASï¼Œå®ƒèƒ½å¤Ÿåœ¨æœªè§å¯¹è±¡ç±»åˆ«ä¸­æ£€æµ‹è§†è§‰å¼‚å¸¸ï¼Œè€Œæ— éœ€å¯¹ç›®æ ‡æ•°æ®è¿›è¡Œä»»ä½•é‡æ–°è®­ç»ƒï¼Œä»…ä½¿ç”¨å°‘æ•°æ­£å¸¸å‚è€ƒå›¾åƒä½œä¸ºè§†è§‰æç¤ºã€‚DictASçš„è§è§£æ˜¯é€šè¿‡è‡ªæˆ‘ç›‘ç£å­¦ä¹ å°†å­—å…¸æŸ¥æ‰¾èƒ½åŠ›è½¬ç§»åˆ°æœªè§ç±»åˆ«çš„FSASä»»åŠ¡ä¸Šï¼Œè€Œä¸æ˜¯ä»…ä»…ä»è®­ç»ƒé›†ä¸­è®°å¿†æ­£å¸¸å’Œå¼‚å¸¸ç‰¹å¾æ¨¡å¼ã€‚å…·ä½“æ¥è¯´ï¼ŒDictASä¸»è¦ç”±ä¸‰ä¸ªç»„ä»¶ç»„æˆï¼šï¼ˆ1ï¼‰<strong>å­—å…¸æ„å»º</strong>â€”â€”ä½¿ç”¨æ­£å¸¸å‚è€ƒå›¾åƒçš„ç‰¹å¾æ¨¡æ‹ŸçœŸå®å­—å…¸çš„ç´¢å¼•å’Œå†…å®¹ã€‚ï¼ˆ2ï¼‰<strong>å­—å…¸æŸ¥æ‰¾</strong>â€”â€”é€šè¿‡ç¨€ç–æŸ¥æ‰¾ç­–ç•¥ä»å­—å…¸ä¸­æ£€ç´¢æŸ¥è¯¢åŒºåŸŸç‰¹å¾ã€‚å½“æ— æ³•æ£€ç´¢æŸ¥è¯¢ç‰¹å¾æ—¶ï¼Œå®ƒä¼šè¢«å½’ç±»ä¸ºå¼‚å¸¸ã€‚ï¼ˆ3ï¼‰<strong>æŸ¥è¯¢åˆ¤åˆ«æ­£åˆ™åŒ–</strong>â€”â€”é€šè¿‡ä½¿å¼‚å¸¸ç‰¹å¾æ›´éš¾ä»å­—å…¸ä¸­æ£€ç´¢æ¥æé«˜å¼‚å¸¸åˆ¤åˆ«åŠ›ã€‚ä¸ºæ­¤ï¼Œè¿›ä¸€æ­¥æå‡ºäº†å¯¹æ¯”æŸ¥è¯¢çº¦æŸå’Œæ–‡æœ¬å¯¹é½çº¦æŸã€‚åœ¨ä¸ƒä¸ªå…¬å…±å·¥ä¸šå’ŒåŒ»ç–—æ•°æ®é›†ä¸Šçš„å¤§é‡å®éªŒè¡¨æ˜ï¼ŒDictASå§‹ç»ˆä¼˜äºæœ€æ–°çš„FSASæ–¹æ³•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.13560v1">PDF</a> Accepted by ICCV 2025, Project: <a target="_blank" rel="noopener" href="https://github.com/xiaozhen228/DictAS">https://github.com/xiaozhen228/DictAS</a></p>
<p><strong>Summary</strong></p>
<p>è¯¥è®ºæ–‡æå‡ºäº†ä¸€ç§åä¸ºDictASçš„æ–°æ¡†æ¶ï¼Œç”¨äºåœ¨æœªè®­ç»ƒçš„ç›®æ ‡æ•°æ®ä¸Šæ£€æµ‹æœªçŸ¥å¯¹è±¡ç±»åˆ«ä¸­çš„è§†è§‰å¼‚å¸¸ã€‚å®ƒä¸»è¦é€šè¿‡åˆ©ç”¨å°‘æ•°æ­£å¸¸å‚è€ƒå›¾åƒä½œä¸ºè§†è§‰æç¤ºæ¥å®ç°è¿™ä¸€ç›®æ ‡ã€‚DictASçš„æ ¸å¿ƒç†å¿µæ˜¯é€šè¿‡è‡ªæˆ‘ç›‘ç£å­¦ä¹ å°†å­—å…¸æŸ¥æ‰¾èƒ½åŠ›è½¬ç§»åˆ°å°‘æ•°æ‹æ‘„å¼‚å¸¸åˆ†å‰²ä»»åŠ¡ä¸Šï¼Œè€Œä¸æ˜¯ä»…ä»…ä¾é è®­ç»ƒé›†æ¥è®°å¿†æ­£å¸¸å’Œå¼‚å¸¸çš„ç‰¹å¾æ¨¡å¼ã€‚ç»è¿‡å¤§é‡å®éªŒè¯æ˜ï¼ŒDictASåœ¨å¤šä¸ªå…¬å…±å·¥ä¸šæ•°æ®é›†ä¸Šçš„è¡¨ç°å§‹ç»ˆä¼˜äºç°æœ‰çš„å°‘æ•°æ‹æ‘„å¼‚å¸¸åˆ†å‰²æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>DictASæ¡†æ¶èƒ½åœ¨æœªé‡æ–°è®­ç»ƒç›®æ ‡æ•°æ®çš„æƒ…å†µä¸‹ï¼Œåˆ©ç”¨å°‘æ•°æ­£å¸¸å‚è€ƒå›¾åƒæ£€æµ‹æœªçŸ¥å¯¹è±¡ç±»åˆ«ä¸­çš„è§†è§‰å¼‚å¸¸ã€‚</li>
<li>DictASé€šè¿‡æ¨¡æ‹ŸçœŸå®å­—å…¸çš„ç´¢å¼•å’Œå†…å®¹ï¼Œä½¿ç”¨æ­£å¸¸å‚è€ƒå›¾åƒçš„ç‰¹å¾è¿›è¡Œå­—å…¸æ„å»ºã€‚</li>
<li>é€šè¿‡ç¨€ç–æŸ¥æ‰¾ç­–ç•¥ä»å­—å…¸ä¸­æ£€ç´¢æŸ¥è¯¢åŒºåŸŸç‰¹å¾ï¼Œæ— æ³•æ£€ç´¢åˆ°çš„ç‰¹å¾è¢«åˆ†ç±»ä¸ºå¼‚å¸¸ã€‚</li>
<li>Query Discrimination Regularizationæ¨¡å—é€šè¿‡ä½¿å¼‚å¸¸ç‰¹å¾æ›´éš¾ä»å­—å…¸ä¸­æ£€ç´¢å‡ºæ¥ï¼Œæé«˜äº†å¼‚å¸¸è¯†åˆ«çš„èƒ½åŠ›ã€‚</li>
<li>è¯¥æ¡†æ¶å¼•å…¥äº†Contrastive Query Constraintå’ŒText Alignment Constraintï¼Œè¿›ä¸€æ­¥æé«˜äº†æ¨¡å‹çš„æ€§èƒ½ã€‚</li>
<li>åœ¨ä¸ƒä¸ªå…¬å…±å·¥ä¸šæ•°æ®é›†å’ŒåŒ»ç–—æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒDictASåœ¨å°‘æ•°æ‹æ‘„å¼‚å¸¸åˆ†å‰²ä»»åŠ¡ä¸Šçš„è¡¨ç°ä¼˜äºç°æœ‰æ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.13560">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-b8865a311ace7179c21006b1ae0ea7b4.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-5a2de08c504c3ec1eead8e0a557e7127.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-12cc42d6005eb574997280b4afb63b5a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-2a379e50efa4f973d4e20403d7a00f4d.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="MAGNeT-Multimodal-Adaptive-Gaussian-Networks-for-Intent-Inference-in-Moving-Target-Selection-across-Complex-Scenarios"><a href="#MAGNeT-Multimodal-Adaptive-Gaussian-Networks-for-Intent-Inference-in-Moving-Target-Selection-across-Complex-Scenarios" class="headerlink" title="MAGNeT: Multimodal Adaptive Gaussian Networks for Intent Inference in   Moving Target Selection across Complex Scenarios"></a>MAGNeT: Multimodal Adaptive Gaussian Networks for Intent Inference in   Moving Target Selection across Complex Scenarios</h2><p><strong>Authors:Xiangxian Li, Yawen Zheng, Baiqiao Zhang, Yijia Ma, Xianhui Cao, Juan Liu, Yulong Bian, Jin Huang, Chenglei Yang</strong></p>
<p>Moving target selection in multimedia interactive systems faces unprecedented challenges as users increasingly interact across diverse and dynamic contexts-from live streaming in moving vehicles to VR gaming in varying environments. Existing approaches rely on probabilistic models that relate endpoint distribution to target properties such as size and speed. However, these methods require substantial training data for each new context and lack transferability across scenarios, limiting their practical deployment in diverse multimedia environments where rich multimodal contextual information is readily available. This paper introduces MAGNeT (Multimodal Adaptive Gaussian Networks), which addresses these problems by combining classical statistical modeling with a context-aware multimodal method. MAGNeT dynamically fuses pre-fitted Ternary-Gaussian models from various scenarios based on real-time contextual cues, enabling effective adaptation with minimal training data while preserving model interpretability. We conduct experiments on self-constructed 2D and 3D moving target selection datasets under in-vehicle vibration conditions. Extensive experiments demonstrate that MAGNeT achieves lower error rates with few-shot samples by applying context-aware fusion of Gaussian experts from multi-factor conditions. </p>
<blockquote>
<p>å¤šåª’ä½“äº¤äº’ç³»ç»Ÿä¸­çš„åŠ¨æ€ç›®æ ‡é€‰æ‹©é¢ä¸´ç€å‰æ‰€æœªæœ‰çš„æŒ‘æˆ˜ï¼Œå› ä¸ºç”¨æˆ·åœ¨ä¸åŒçš„åŠ¨æ€ä¸Šä¸‹æ–‡ä¸­çš„äº¤äº’æ—¥ç›Šå¢å¤šï¼Œä¾‹å¦‚åœ¨ç§»åŠ¨è½¦è¾†ä¸­è¿›è¡Œç›´æ’­æˆ–åœ¨ä¸åŒçš„ç¯å¢ƒä¸­è¿›è¡ŒVRæ¸¸æˆã€‚ç°æœ‰æ–¹æ³•ä¾èµ–äºæ¦‚ç‡æ¨¡å‹ï¼Œè¯¥æ¨¡å‹å°†ç»ˆç‚¹åˆ†å¸ƒä¸ç›®æ ‡çš„å±æ€§ï¼ˆå¦‚å¤§å°å’Œé€Ÿåº¦ï¼‰å…³è”èµ·æ¥ã€‚ç„¶è€Œï¼Œè¿™äº›æ–¹æ³•å¯¹äºæ¯ç§æ–°ä¸Šä¸‹æ–‡éƒ½éœ€è¦å¤§é‡çš„è®­ç»ƒæ•°æ®ï¼Œå¹¶ä¸”åœ¨ä¸åŒåœºæ™¯ä¹‹é—´ç¼ºä¹å¯è¿ç§»æ€§ï¼Œé™åˆ¶äº†å®ƒä»¬åœ¨ä¸°å¯Œçš„å¤šåª’ä½“ç¯å¢ƒä¸­è¿›è¡Œå®é™…éƒ¨ç½²çš„èƒ½åŠ›ï¼Œåœ¨è¿™äº›ç¯å¢ƒä¸­ï¼Œä¸°å¯Œçš„å¤šæ¨¡æ€ä¸Šä¸‹æ–‡ä¿¡æ¯å¾ˆå®¹æ˜“è·å¾—ã€‚æœ¬æ–‡ä»‹ç»äº†MAGNeTï¼ˆå¤šæ¨¡æ€è‡ªé€‚åº”é«˜æ–¯ç½‘ç»œï¼‰ï¼Œå®ƒé€šè¿‡ç»“åˆç»å…¸ç»Ÿè®¡å»ºæ¨¡å’Œä¸Šä¸‹æ–‡æ„ŸçŸ¥çš„å¤šæ¨¡æ€æ–¹æ³•æ¥è§£å†³è¿™äº›é—®é¢˜ã€‚MAGNeTæ ¹æ®å®æ—¶çš„ä¸Šä¸‹æ–‡çº¿ç´¢åŠ¨æ€èåˆæ¥è‡ªå„ç§åœºæ™¯çš„é¢„å…ˆæ‹Ÿåˆçš„ä¸‰å…ƒé«˜æ–¯æ¨¡å‹ï¼Œä»¥æœ€å°çš„è®­ç»ƒæ•°æ®å®ç°æœ‰æ•ˆçš„é€‚åº”ï¼ŒåŒæ—¶ä¿æŒæ¨¡å‹çš„å¯è§£é‡Šæ€§ã€‚æˆ‘ä»¬åœ¨è‡ªè¡Œæ„å»ºçš„äºŒç»´å’Œä¸‰ç»´ç§»åŠ¨ç›®æ ‡é€‰æ‹©æ•°æ®é›†ä¸Šè¿›è¡Œäº†è½¦å†…æŒ¯åŠ¨æ¡ä»¶ä¸‹çš„å®éªŒã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼Œé€šè¿‡å¤šå› ç´ æ¡ä»¶ä¸‹çš„é«˜æ–¯ä¸“å®¶çš„ä¸Šä¸‹æ–‡æ„ŸçŸ¥èåˆï¼ŒMAGNeTåœ¨å°‘æ•°æ ·æœ¬æƒ…å†µä¸‹å®ç°äº†æ›´ä½çš„é”™è¯¯ç‡ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.12992v2">PDF</a> Accepted by ACM MM 2025</p>
<p><strong>Summary</strong><br>å¤šåª’ä½“äº¤äº’ç³»ç»Ÿä¸­ç§»åŠ¨ç›®æ ‡é€‰æ‹©é¢ä¸´å‰æ‰€æœªæœ‰çš„æŒ‘æˆ˜ï¼Œéšç€ç”¨æˆ·åœ¨ä¸åŒåŠ¨æ€ä¸Šä¸‹æ–‡ä¸­çš„äº¤äº’æ—¥ç›Šå¢å¤šï¼Œå¦‚ç§»åŠ¨è½¦è¾†ä¸­çš„ç›´æ’­åˆ°ä¸åŒç¯å¢ƒä¸­çš„VRæ¸¸æˆã€‚ç°æœ‰æ–¹æ³•ä¾èµ–äºä¸ç›®æ ‡å±æ€§ï¼ˆå¦‚å¤§å°å’Œé€Ÿåº¦ï¼‰ç›¸å…³çš„ç»ˆç«¯åˆ†å¸ƒçš„æ¦‚ç‡æ¨¡å‹ã€‚ç„¶è€Œï¼Œè¿™äº›æ–¹æ³•éœ€è¦ä¸ºæ¯ä¸ªæ–°ä¸Šä¸‹æ–‡è¿›è¡Œå¤§é‡è®­ç»ƒæ•°æ®ï¼Œå¹¶ä¸”ç¼ºä¹è·¨åœºæ™¯çš„è¿ç§»èƒ½åŠ›ï¼Œé™åˆ¶äº†å®ƒä»¬åœ¨å¤šåª’ä½“ç¯å¢ƒä¸­çš„å®é™…éƒ¨ç½²ï¼Œé‚£é‡Œæ‹¥æœ‰ä¸°å¯Œçš„å¤šæ¨¡æ€ä¸Šä¸‹æ–‡ä¿¡æ¯ã€‚æœ¬æ–‡ä»‹ç»äº†MAGNeTï¼ˆå¤šæ¨¡æ€è‡ªé€‚åº”é«˜æ–¯ç½‘ç»œï¼‰ï¼Œå®ƒé€šè¿‡ç»“åˆç»å…¸ç»Ÿè®¡å»ºæ¨¡å’Œä¸Šä¸‹æ–‡æ„ŸçŸ¥çš„å¤šæ¨¡æ€æ–¹æ³•æ¥è§£å†³è¿™äº›é—®é¢˜ã€‚MAGNeTæ ¹æ®å®æ—¶ä¸Šä¸‹æ–‡çº¿ç´¢åŠ¨æ€èåˆæ¥è‡ªå„ç§åœºæ™¯çš„é¢„å…ˆæ‹Ÿåˆçš„ä¸‰å…ƒé«˜æ–¯æ¨¡å‹ï¼Œå¯åœ¨å°‘é‡è®­ç»ƒæ•°æ®çš„æƒ…å†µä¸‹å®ç°æœ‰æ•ˆé€‚åº”ï¼ŒåŒæ—¶ä¿æŒæ¨¡å‹çš„å¯è§£é‡Šæ€§ã€‚æˆ‘ä»¬åœ¨è‡ªè¡Œæ„å»ºçš„äºŒç»´å’Œä¸‰ç»´ç§»åŠ¨ç›®æ ‡é€‰æ‹©æ•°æ®é›†ä¸Šè¿›è¡Œäº†å®éªŒï¼Œå®éªŒè¡¨æ˜ï¼ŒMAGNeTé€šè¿‡åº”ç”¨ä¸Šä¸‹æ–‡æ„ŸçŸ¥çš„é«˜æ–¯ä¸“å®¶èåˆï¼Œåœ¨å°‘æ•°æ ·æœ¬çš„æƒ…å†µä¸‹å®ç°äº†è¾ƒä½çš„é”™è¯¯ç‡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤šåª’ä½“äº¤äº’ç³»ç»Ÿä¸­ç§»åŠ¨ç›®æ ‡é€‰æ‹©é¢ä¸´æŒ‘æˆ˜ï¼Œå°¤å…¶æ˜¯åœ¨ä¸åŒåŠ¨æ€ä¸Šä¸‹æ–‡ä¸­çš„äº¤äº’ã€‚</li>
<li>ç°æœ‰æ–¹æ³•ä¾èµ–æ¦‚ç‡æ¨¡å‹ï¼Œéœ€è¦ä¸ºæ¯ç§æ–°ä¸Šä¸‹æ–‡è¿›è¡Œå¤§é‡è®­ç»ƒï¼Œä¸”ç¼ºä¹è·¨åœºæ™¯è¿ç§»èƒ½åŠ›ã€‚</li>
<li>MAGNeTï¼ˆå¤šæ¨¡æ€è‡ªé€‚åº”é«˜æ–¯ç½‘ç»œï¼‰ç»“åˆç»å…¸ç»Ÿè®¡å»ºæ¨¡å’Œä¸Šä¸‹æ–‡æ„ŸçŸ¥çš„å¤šæ¨¡æ€æ–¹æ³•æ¥è§£å†³è¿™äº›é—®é¢˜ã€‚</li>
<li>MAGNeTæ ¹æ®å®æ—¶ä¸Šä¸‹æ–‡çº¿ç´¢åŠ¨æ€èåˆé¢„æ‹Ÿåˆæ¨¡å‹ï¼Œå®ç°æœ‰æ•ˆé€‚åº”å¹¶ä¿ç•™æ¨¡å‹çš„å¯è§£é‡Šæ€§ã€‚</li>
<li>MAGNeTåœ¨ç§»åŠ¨ç›®æ ‡é€‰æ‹©æ–¹é¢è¡¨ç°å‡ºä¼˜å¼‚æ€§èƒ½ï¼Œç‰¹åˆ«æ˜¯åœ¨å°‘é‡è®­ç»ƒæ•°æ®çš„æƒ…å†µä¸‹ã€‚</li>
<li>å®éªŒè¯æ˜MAGNeTåœ¨å¤šç§ç§»åŠ¨ç›®æ ‡é€‰æ‹©æ•°æ®é›†ä¸Šå®ç°äº†è¾ƒä½çš„é”™è¯¯ç‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.12992">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-79683ed8dec29f7d0eb8a68d7da25899.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-480c5c368a498f6117db7d7ee05573dc.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-776c785c045799ea8cf882f11466556d.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-7ddd83def8b8af37b56935c69766ff23.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-deba86eab4f3a5091d01aca9bf1113eb.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-099e3f2ea114f18b2df3b57e3e684988.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="DCSCR-A-Class-Specific-Collaborative-Representation-based-Network-for-Image-Set-Classification"><a href="#DCSCR-A-Class-Specific-Collaborative-Representation-based-Network-for-Image-Set-Classification" class="headerlink" title="DCSCR: A Class-Specific Collaborative Representation based Network for   Image Set Classification"></a>DCSCR: A Class-Specific Collaborative Representation based Network for   Image Set Classification</h2><p><strong>Authors:Xizhan Gao, Wei Hu</strong></p>
<p>Image set classification (ISC), which can be viewed as a task of comparing similarities between sets consisting of unordered heterogeneous images with variable quantities and qualities, has attracted growing research attention in recent years. How to learn effective feature representations and how to explore the similarities between different image sets are two key yet challenging issues in this field. However, existing traditional ISC methods classify image sets based on raw pixel features, ignoring the importance of feature learning. Existing deep ISC methods can learn deep features, but they fail to adaptively adjust the features when measuring set distances, resulting in limited performance in few-shot ISC. To address the above issues, this paper combines traditional ISC methods with deep models and proposes a novel few-shot ISC approach called Deep Class-specific Collaborative Representation (DCSCR) network to simultaneously learn the frame- and concept-level feature representations of each image set and the distance similarities between different sets. Specifically, DCSCR consists of a fully convolutional deep feature extractor module, a global feature learning module, and a class-specific collaborative representation-based metric learning module. The deep feature extractor and global feature learning modules are used to learn (local and global) frame-level feature representations, while the class-specific collaborative representation-based metric learning module is exploit to adaptively learn the concept-level feature representation of each image set and thus obtain the distance similarities between different sets by developing a new CSCR-based contrastive loss function. Extensive experiments on several well-known few-shot ISC datasets demonstrate the effectiveness of the proposed method compared with some state-of-the-art image set classification algorithms. </p>
<blockquote>
<p>å›¾åƒé›†åˆ†ç±»ï¼ˆISCï¼‰æ˜¯ä¸€ä¸ªæ¯”è¾ƒæ— åºçš„å¼‚è´¨å›¾åƒé›†ä¹‹é—´ç›¸ä¼¼æ€§çš„ä»»åŠ¡ï¼Œè¿™äº›å›¾åƒé›†å…·æœ‰å¯å˜çš„æ•°é‡å’Œå“è´¨ï¼Œè¿‘å¹´æ¥å¼•èµ·äº†è¶Šæ¥è¶Šå¤šçš„ç ”ç©¶å…³æ³¨ã€‚å¦‚ä½•å­¦ä¹ æœ‰æ•ˆçš„ç‰¹å¾è¡¨ç¤ºä»¥åŠå¦‚ä½•æ¢ç´¢ä¸åŒå›¾åƒé›†ä¹‹é—´çš„ç›¸ä¼¼æ€§æ˜¯è¯¥é¢†åŸŸçš„ä¸¤ä¸ªå…³é”®ä¸”å…·æœ‰æŒ‘æˆ˜æ€§çš„é—®é¢˜ã€‚ç„¶è€Œï¼Œç°æœ‰çš„ä¼ ç»ŸISCæ–¹æ³•åŸºäºåŸå§‹åƒç´ ç‰¹å¾å¯¹å›¾åƒé›†è¿›è¡Œåˆ†ç±»ï¼Œå¿½ç•¥äº†ç‰¹å¾å­¦ä¹ çš„é‡è¦æ€§ã€‚ç°æœ‰çš„æ·±åº¦ISCæ–¹æ³•å¯ä»¥å­¦ä¹ æ·±åº¦ç‰¹å¾ï¼Œä½†åœ¨æµ‹é‡é›†åˆè·ç¦»æ—¶æ— æ³•è‡ªé€‚åº”è°ƒæ•´ç‰¹å¾ï¼Œå¯¼è‡´åœ¨å°‘æ ·æœ¬ISCä¸­çš„æ€§èƒ½æœ‰é™ã€‚ä¸ºäº†è§£å†³ä¸Šè¿°é—®é¢˜ï¼Œæœ¬æ–‡ç»“åˆäº†ä¼ ç»ŸISCæ–¹æ³•å’Œæ·±åº¦æ¨¡å‹ï¼Œæå‡ºäº†ä¸€ç§æ–°çš„å°‘æ ·æœ¬ISCæ–¹æ³•ï¼Œç§°ä¸ºæ·±åº¦ç±»ç‰¹å®šååŒè¡¨ç¤ºï¼ˆDCSCRï¼‰ç½‘ç»œï¼Œå¯ä»¥åŒæ—¶å­¦ä¹ æ¯ä¸ªå›¾åƒé›†çš„æ¡†æ¶å’Œæ¦‚å¿µçº§åˆ«çš„ç‰¹å¾è¡¨ç¤ºä»¥åŠä¸åŒé›†åˆä¹‹é—´çš„è·ç¦»ç›¸ä¼¼æ€§ã€‚å…·ä½“æ¥è¯´ï¼ŒDCSCRåŒ…æ‹¬å…¨å·ç§¯æ·±åº¦ç‰¹å¾æå–å™¨æ¨¡å—ã€å…¨å±€ç‰¹å¾å­¦ä¹ æ¨¡å—å’ŒåŸºäºç±»ç‰¹å®šååŒè¡¨ç¤ºçš„åº¦é‡å­¦ä¹ æ¨¡å—ã€‚æ·±åº¦ç‰¹å¾æå–å™¨å’Œå…¨å±€ç‰¹å¾å­¦ä¹ æ¨¡å—ç”¨äºå­¦ä¹ ï¼ˆå±€éƒ¨å’Œå…¨å±€ï¼‰æ¡†æ¶çº§åˆ«çš„ç‰¹å¾è¡¨ç¤ºï¼Œè€ŒåŸºäºç±»ç‰¹å®šçš„ååŒè¡¨ç¤ºåº¦é‡å­¦ä¹ æ¨¡å—åˆ™ç”¨äºè‡ªé€‚åº”åœ°å­¦ä¹ æ¯ä¸ªå›¾åƒé›†çš„æ¦‚å¿µçº§ç‰¹å¾è¡¨ç¤ºï¼Œå¹¶é€šè¿‡å¼€å‘ä¸€ç§æ–°çš„åŸºäºCSCRçš„å¯¹æ¯”æŸå¤±å‡½æ•°æ¥è·å¾—ä¸åŒé›†åˆä¹‹é—´çš„è·ç¦»ç›¸ä¼¼æ€§ã€‚åœ¨å‡ ä¸ªè‘—åçš„å°‘æ ·æœ¬ISCæ•°æ®é›†ä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼Œä¸ä¸€äº›æœ€å…ˆè¿›çš„å›¾åƒé›†åˆ†ç±»ç®—æ³•ç›¸æ¯”ï¼Œæ‰€æå‡ºçš„æ–¹æ³•å…·æœ‰æœ‰æ•ˆæ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.12745v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡å…³æ³¨å›¾åƒé›†åˆ†ç±»ï¼ˆISCï¼‰é¢†åŸŸï¼Œé’ˆå¯¹å¦‚ä½•å­¦ä¹ æœ‰æ•ˆçš„ç‰¹å¾è¡¨ç¤ºä»¥åŠå¦‚ä½•æ¢ç´¢ä¸åŒå›¾åƒé›†ä¹‹é—´çš„ç›¸ä¼¼æ€§ä¸¤ä¸ªå…³é”®é—®é¢˜ï¼Œæå‡ºä¸€ç§ç»“åˆä¼ ç»ŸISCæ–¹æ³•å’Œæ·±åº¦æ¨¡å‹çš„æ–°å‹å°‘æ ·æœ¬ISCæ–¹æ³•â€”â€”Deep Class-specific Collaborative Representationï¼ˆDCSCRï¼‰ç½‘ç»œã€‚è¯¥ç½‘ç»œèƒ½åŒæ—¶å­¦ä¹ æ¯ä¸ªå›¾åƒé›†çš„æ¡†æ¶å’Œæ¦‚å¿µçº§åˆ«çš„ç‰¹å¾è¡¨ç¤ºï¼Œä»¥åŠä¸åŒé›†ä¹‹é—´çš„è·ç¦»ç›¸ä¼¼æ€§ã€‚é€šè¿‡ä¸€ç³»åˆ—å®éªŒéªŒè¯ï¼Œè¯¥æ–¹æ³•åœ¨å‡ ä¸ªçŸ¥åçš„å°‘æ ·æœ¬ISCæ•°æ®é›†ä¸Šè¡¨ç°å‡ºä¼˜å¼‚çš„æ•ˆæœã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å›¾åƒé›†åˆ†ç±»ï¼ˆISCï¼‰æ˜¯è¿‘å¹´æ¥çš„ç ”ç©¶çƒ­ç‚¹ï¼Œæ¶‰åŠå¦‚ä½•å­¦ä¹ æœ‰æ•ˆçš„ç‰¹å¾è¡¨ç¤ºå’Œæ¢ç´¢ä¸åŒå›¾åƒé›†ä¹‹é—´çš„ç›¸ä¼¼æ€§ã€‚</li>
<li>ä¼ ç»ŸISCæ–¹æ³•åŸºäºåŸå§‹åƒç´ ç‰¹å¾è¿›è¡Œåˆ†ç±»ï¼Œå¿½ç•¥äº†ç‰¹å¾å­¦ä¹ çš„é‡è¦æ€§ã€‚</li>
<li>ç°æœ‰æ·±åº¦ISCæ–¹æ³•è™½ç„¶èƒ½å­¦ä¹ æ·±åº¦ç‰¹å¾ï¼Œä½†åœ¨æµ‹é‡é›†é—´è·ç¦»æ—¶æ— æ³•è‡ªé€‚åº”è°ƒæ•´ç‰¹å¾ï¼Œå¯¼è‡´åœ¨å°‘æ ·æœ¬ISCä¸­çš„æ€§èƒ½å—é™ã€‚</li>
<li>DCSCRç½‘ç»œç»“åˆäº†ä¼ ç»ŸISCæ–¹æ³•å’Œæ·±åº¦æ¨¡å‹ï¼Œèƒ½åŒæ—¶å­¦ä¹ æ¯ä¸ªå›¾åƒé›†çš„æ¡†æ¶å’Œæ¦‚å¿µçº§åˆ«çš„ç‰¹å¾è¡¨ç¤ºã€‚</li>
<li>DCSCRç½‘ç»œåŒ…å«å…¨å·ç§¯æ·±åº¦ç‰¹å¾æå–æ¨¡å—ã€å…¨å±€ç‰¹å¾å­¦ä¹ æ¨¡å—å’ŒåŸºäºç±»ç‰¹å®šçš„ååŒè¡¨ç¤ºåº¦é‡å­¦ä¹ æ¨¡å—ã€‚</li>
<li>é€šè¿‡å¼€å‘æ–°çš„CSCRå¯¹æ¯”æŸå¤±å‡½æ•°ï¼ŒDCSCRç½‘ç»œèƒ½è‡ªé€‚åº”åœ°å­¦ä¹ æ¯ä¸ªå›¾åƒé›†çš„æ¦‚å¿µçº§åˆ«ç‰¹å¾è¡¨ç¤ºï¼Œå¹¶è·å–ä¸åŒé›†ä¹‹é—´çš„è·ç¦»ç›¸ä¼¼æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.12745">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-c505ac92db2a7765d1e8890fad5c0043.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a1089efa9e95ba07a0f25dd617bfa6d3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7ceee75f854337c79f6801c1e81daba1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8c637cccdbfeb99cbdb68867cea9c71c.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="LLM-RIMSA-Large-Language-Models-driven-Reconfigurable-Intelligent-Metasurface-Antenna-Systems"><a href="#LLM-RIMSA-Large-Language-Models-driven-Reconfigurable-Intelligent-Metasurface-Antenna-Systems" class="headerlink" title="LLM-RIMSA: Large Language Models driven Reconfigurable Intelligent   Metasurface Antenna Systems"></a>LLM-RIMSA: Large Language Models driven Reconfigurable Intelligent   Metasurface Antenna Systems</h2><p><strong>Authors:Yunsong Huang, Hui-Ming Wang, Qingli Yan, Zhaowei Wang</strong></p>
<p>The evolution of 6G networks demands ultra-massive connectivity and intelligent radio environments, yet existing reconfigurable intelligent surface (RIS) technologies face critical limitations in hardware efficiency, dynamic control, and scalability. This paper introduces LLM-RIMSA, a transformative framework that integrates large language models (LLMs) with a novel reconfigurable intelligent metasurface antenna (RIMSA) architecture to address these challenges. Unlike conventional RIS designs, RIMSA employs parallel coaxial feeding and 2D metasurface integration, enabling each individual metamaterial element to independently adjust both its amplitude and phase. While traditional optimization and deep learning (DL) methods struggle with high-dimensional state spaces and prohibitive training costs for RIMSA control, LLM-RIMSA leverages pre-trained LLMs cross-modal reasoning and few-shot learning capabilities to dynamically optimize RIMSA configurations. Simulations demonstrate that LLM-RIMSA achieves state-of-the-art performance, outperforming conventional DL-based methods in sum rate while reducing training overhead. The proposed framework pave the way for LLM-driven intelligent radio environments. </p>
<blockquote>
<p>6Gç½‘ç»œçš„æ¼”è¿›éœ€è¦è¶…å¤§è§„æ¨¡è¿æ¥å’Œæ™ºèƒ½æ— çº¿ç”µç¯å¢ƒï¼Œç„¶è€Œç°æœ‰çš„å¯é‡æ„æ™ºèƒ½è¡¨é¢ï¼ˆRISï¼‰æŠ€æœ¯åœ¨ç¡¬ä»¶æ•ˆç‡ã€åŠ¨æ€æ§åˆ¶å’Œå¯æ‰©å±•æ€§æ–¹é¢é¢ä¸´å…³é”®æŒ‘æˆ˜ã€‚æœ¬æ–‡ä»‹ç»äº†LLM-RIMSAï¼Œè¿™æ˜¯ä¸€ä¸ªåˆ›æ–°æ€§æ¡†æ¶ï¼Œå®ƒå°†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¸æ–°å‹çš„å¯é‡æ„æ™ºèƒ½è¶…è¡¨é¢å¤©çº¿ï¼ˆRIMSAï¼‰æ¶æ„ç›¸ç»“åˆï¼Œä»¥åº”å¯¹è¿™äº›æŒ‘æˆ˜ã€‚ä¸ä¼ ç»Ÿçš„RISè®¾è®¡ä¸åŒï¼ŒRIMSAé‡‡ç”¨å¹¶è¡ŒåŒè½´é¦ˆç”µå’Œ2Dè¶…è¡¨é¢é›†æˆï¼Œä½¿æ¯ä¸ªå•ç‹¬çš„å…ƒææ–™å…ƒç´ éƒ½èƒ½ç‹¬ç«‹è°ƒæ•´å…¶æŒ¯å¹…å’Œç›¸ä½ã€‚ä¼ ç»Ÿçš„ä¼˜åŒ–å’Œæ·±åº¦å­¦ä¹ ï¼ˆDLï¼‰æ–¹æ³•åœ¨é«˜ç»´çŠ¶æ€ç©ºé—´å’ŒRIMSAæ§åˆ¶çš„æ˜‚è´µè®­ç»ƒæˆæœ¬æ–¹é¢é‡åˆ°äº†å›°éš¾ï¼Œè€ŒLLM-RIMSAåˆ©ç”¨é¢„è®­ç»ƒçš„å¤§å‹è¯­è¨€æ¨¡å‹çš„è·¨æ¨¡æ€æ¨ç†å’Œå°‘é‡å­¦ä¹ åŠŸèƒ½æ¥åŠ¨æ€ä¼˜åŒ–RIMSAé…ç½®ã€‚æ¨¡æ‹Ÿç»“æœè¡¨æ˜ï¼ŒLLM-RIMSAå®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œåœ¨æ€»å’Œé€Ÿç‡ä¸Šä¼˜äºä¼ ç»Ÿçš„åŸºäºDLçš„æ–¹æ³•ï¼ŒåŒæ—¶é™ä½äº†è®­ç»ƒå¼€é”€ã€‚æ‰€æå‡ºçš„æ¡†æ¶ä¸ºå¤§å‹è¯­è¨€æ¨¡å‹é©±åŠ¨çš„æ™ºèƒ½æ— çº¿ç”µç¯å¢ƒé“ºå¹³äº†é“è·¯ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.12728v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>LLM-RIMSAæ¡†æ¶ç»“åˆäº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å’Œå¯é‡æ„æ™ºèƒ½è¶…è¡¨é¢å¤©çº¿ï¼ˆRIMSAï¼‰æ¶æ„ï¼Œè§£å†³äº†ç°æœ‰å¯é‡æ„æ™ºèƒ½è¡¨é¢ï¼ˆRISï¼‰æŠ€æœ¯åœ¨ç¡¬ä»¶æ•ˆç‡ã€åŠ¨æ€æ§åˆ¶å’Œå¯æ‰©å±•æ€§æ–¹é¢çš„å±€é™ã€‚è¯¥æ¡†æ¶é‡‡ç”¨å¹¶è¡ŒåŒè½´é¦ˆç”µå’Œ2Dè¶…è¡¨é¢é›†æˆï¼Œåˆ©ç”¨LLMsçš„è·¨æ¨¡æ€æ¨ç†å’Œå°‘é‡å­¦ä¹ åŠŸèƒ½ï¼ŒåŠ¨æ€ä¼˜åŒ–RIMSAé…ç½®ï¼Œå®ç°è¶…é«˜è¿é€šæ€§å’Œæ™ºèƒ½æ— çº¿ç”µç¯å¢ƒã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç°æœ‰RISæŠ€æœ¯é¢ä¸´ç¡¬ä»¶æ•ˆç‡ã€åŠ¨æ€æ§åˆ¶å’Œå¯æ‰©å±•æ€§çš„æŒ‘æˆ˜ã€‚</li>
<li>LLM-RIMSAæ¡†æ¶ç»“åˆäº†LLMså’ŒRIMSAæ¶æ„ä»¥åº”å¯¹è¿™äº›æŒ‘æˆ˜ã€‚</li>
<li>RIMSAé‡‡ç”¨å¹¶è¡ŒåŒè½´é¦ˆç”µå’Œ2Dè¶…è¡¨é¢é›†æˆï¼Œä½¿æ¯ä¸ªè¶…ææ–™å…ƒç´ èƒ½ç‹¬ç«‹è°ƒæ•´å¹…åº¦å’Œç›¸ä½ã€‚</li>
<li>ä¼ ç»Ÿä¼˜åŒ–å’Œæ·±åº¦å­¦ä¹ æ–¹æ³•åœ¨RIMSAæ§åˆ¶ä¸Šé‡åˆ°é«˜ç»´çŠ¶æ€ç©ºé—´å’Œè®­ç»ƒæˆæœ¬é«˜æ˜‚çš„é—®é¢˜ã€‚</li>
<li>LLM-RIMSAåˆ©ç”¨LLMsçš„è·¨æ¨¡æ€æ¨ç†å’Œå°‘é‡å­¦ä¹ åŠŸèƒ½è¿›è¡ŒåŠ¨æ€ä¼˜åŒ–ã€‚</li>
<li>ä»¿çœŸæ˜¾ç¤ºLLM-RIMSAåœ¨æ€»é€Ÿç‡ä¸Šå®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œä¼˜äºä¼ ç»Ÿçš„DLæ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.12728">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-41a5d6c2f73e382d6688b750dd040227.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-0023e1a8e73fde998f0cf15046a4f91d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0bad963549c151fdda48ab25f00bf1a0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e509d707f68a24ed369204b5d9dc08a4.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Leveraging-Large-Language-Models-for-Predictive-Analysis-of-Human-Misery"><a href="#Leveraging-Large-Language-Models-for-Predictive-Analysis-of-Human-Misery" class="headerlink" title="Leveraging Large Language Models for Predictive Analysis of Human Misery"></a>Leveraging Large Language Models for Predictive Analysis of Human Misery</h2><p><strong>Authors:Bishanka Seal, Rahul Seetharaman, Aman Bansal, Abhilash Nandy</strong></p>
<p>This study investigates the use of Large Language Models (LLMs) for predicting human-perceived misery scores from natural language descriptions of real-world scenarios. The task is framed as a regression problem, where the model assigns a scalar value from 0 to 100 to each input statement. We evaluate multiple prompting strategies, including zero-shot, fixed-context few-shot, and retrieval-based prompting using BERT sentence embeddings. Few-shot approaches consistently outperform zero-shot baselines, underscoring the value of contextual examples in affective prediction. To move beyond static evaluation, we introduce the â€œMisery Game Showâ€, a novel gamified framework inspired by a television format. It tests LLMs through structured rounds involving ordinal comparison, binary classification, scalar estimation, and feedback-driven reasoning. This setup enables us to assess not only predictive accuracy but also the modelâ€™s ability to adapt based on corrective feedback. The gamified evaluation highlights the broader potential of LLMs in dynamic emotional reasoning tasks beyond standard regression. Code and data link: <a target="_blank" rel="noopener" href="https://github.com/abhi1nandy2/Misery_Data_Exps_GitHub">https://github.com/abhi1nandy2/Misery_Data_Exps_GitHub</a> </p>
<blockquote>
<p>æœ¬ç ”ç©¶æ¢è®¨äº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨æ ¹æ®çœŸå®ä¸–ç•Œåœºæ™¯çš„è‡ªç„¶è¯­è¨€æè¿°é¢„æµ‹äººç±»æ„ŸçŸ¥çš„ç—›è‹¦åˆ†æ•°æ–¹é¢çš„åº”ç”¨ã€‚è¯¥ä»»åŠ¡è¢«æ„å»ºä¸ºä¸€ä¸ªå›å½’é—®é¢˜ï¼Œæ¨¡å‹ä¸ºæ¯æ¡è¾“å…¥è¯­å¥åˆ†é…ä¸€ä¸ªä»0åˆ°100çš„æ ‡é‡å€¼ã€‚æˆ‘ä»¬è¯„ä¼°äº†å¤šç§æç¤ºç­–ç•¥ï¼ŒåŒ…æ‹¬é›¶æ ·æœ¬ã€å›ºå®šä¸Šä¸‹æ–‡å°æ ·ä¾‹å’ŒåŸºäºBERTå¥å­åµŒå…¥çš„æ£€ç´¢æç¤ºã€‚å°æ ·ä¾‹æ–¹æ³•å§‹ç»ˆä¼˜äºé›¶æ ·æœ¬åŸºçº¿ï¼Œè¿™å‡¸æ˜¾äº†ä¸Šä¸‹æ–‡ç¤ºä¾‹åœ¨æƒ…æ„Ÿé¢„æµ‹ä¸­çš„ä»·å€¼ã€‚ä¸ºäº†è¶…è¶Šé™æ€è¯„ä¼°ï¼Œæˆ‘ä»¬æ¨å‡ºäº†â€œè‹¦éš¾æ¸¸æˆç§€â€ï¼Œè¿™æ˜¯ä¸€ä¸ªå—ç”µè§†èŠ‚ç›®æ ¼å¼å¯å‘çš„æ–°å‹æ¸¸æˆåŒ–æ¡†æ¶ã€‚å®ƒé€šè¿‡æ¶‰åŠåºæ•°æ¯”è¾ƒã€äºŒå…ƒåˆ†ç±»ã€æ ‡é‡ä¼°è®¡å’Œåé¦ˆé©±åŠ¨æ¨ç†çš„ç»“æ„åŒ–å›åˆæ¥æµ‹è¯•LLMã€‚è¿™ç§è®¾ç½®ä½¿æˆ‘ä»¬ä¸ä»…èƒ½å¤Ÿè¯„ä¼°é¢„æµ‹å‡†ç¡®æ€§ï¼Œè¿˜èƒ½å¤Ÿè¯„ä¼°æ¨¡å‹æ ¹æ®çº æ­£æ€§åé¦ˆè¿›è¡Œé€‚åº”çš„èƒ½åŠ›ã€‚æ¸¸æˆåŒ–è¯„ä¼°çªæ˜¾äº†LLMåœ¨è¶…è¶Šæ ‡å‡†å›å½’çš„åŠ¨æ€æƒ…ç»ªæ¨ç†ä»»åŠ¡ä¸­çš„æ›´å¹¿æ³›æ½œåŠ›ã€‚ä»£ç å’Œæ•°æ®é“¾æ¥ï¼š<a target="_blank" rel="noopener" href="https://github.com/abhi1nandy2/Misery_Data_Exps_GitHub">https://github.com/abhi1nandy2/Misery_Data_Exps_GitHub</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.12669v1">PDF</a> 14 pages, 4 tables</p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ç”¨äºé¢„æµ‹åŸºäºç°å®ä¸–ç•Œæƒ…å¢ƒçš„è‡ªç„¶è¯­è¨€æè¿°çš„äººç±»æ„ŸçŸ¥ç—›è‹¦ç¨‹åº¦ã€‚ç ”ç©¶é‡‡ç”¨å›å½’é—®é¢˜æ¡†æ¶ï¼Œæ¨¡å‹ä¸ºæ¯ä¸ªè¾“å…¥è¯­å¥åˆ†é…ä¸€ä¸ªä»0åˆ°100çš„æ ‡é‡å€¼ã€‚è¯„ä¼°äº†å¤šç§æç¤ºç­–ç•¥ï¼ŒåŒ…æ‹¬é›¶æ ·æœ¬ã€å›ºå®šä¸Šä¸‹æ–‡å°‘æ ·æœ¬å’ŒåŸºäºBERTå¥å­åµŒå…¥çš„æ£€ç´¢æç¤ºã€‚å°‘æ ·æœ¬æ–¹æ³•å§‹ç»ˆä¼˜äºé›¶æ ·æœ¬åŸºçº¿ï¼Œçªæ˜¾äº†ä¸Šä¸‹æ–‡å®ä¾‹åœ¨æƒ…æ„Ÿé¢„æµ‹ä¸­çš„ä»·å€¼ã€‚ä¸ºè¶…è¶Šé™æ€è¯„ä¼°ï¼Œç ”ç©¶å¼•å…¥äº†â€œç—›è‹¦æ¸¸æˆç§€â€è¿™ä¸€æ–°å‹æ¸¸æˆåŒ–æ¡†æ¶ï¼Œè¯¥æ¡†æ¶ä»¥ç”µè§†èŠ‚ç›®å½¢å¼ä¸ºçµæ„Ÿã€‚é€šè¿‡æœ‰åºæ¯”è¾ƒã€äºŒå…ƒåˆ†ç±»ã€æ ‡é‡ä¼°ç®—å’Œåé¦ˆé©±åŠ¨æ¨ç†çš„ç»“æ„æ€§å›åˆæµ‹è¯•LLMï¼Œä¸ä»…è¯„ä¼°å…¶é¢„æµ‹å‡†ç¡®æ€§ï¼Œè¿˜è¯„ä¼°å…¶åŸºäºçº æ­£åé¦ˆçš„é€‚åº”èƒ½åŠ›ã€‚æ¸¸æˆåŒ–è¯„ä¼°çªæ˜¾äº†LLMåœ¨åŠ¨æ€æƒ…æ„Ÿæ¨ç†ä»»åŠ¡ä¸­çš„æ›´å¹¿æ³›æ½œåŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å¯ç”¨äºé¢„æµ‹äººç±»æ„ŸçŸ¥çš„ç—›è‹¦ç¨‹åº¦ï¼Œé‡‡ç”¨å›å½’é—®é¢˜æ¡†æ¶ã€‚</li>
<li>å°‘æ ·æœ¬æ–¹æ³•åœ¨è¿›è¡Œæƒ…æ„Ÿé¢„æµ‹æ—¶è¡¨ç°ä¼˜äºé›¶æ ·æœ¬åŸºçº¿ã€‚</li>
<li>ä¸Šä¸‹æ–‡å®ä¾‹åœ¨æƒ…æ„Ÿé¢„æµ‹ä»»åŠ¡ä¸­å…·æœ‰é‡è¦ä½œç”¨ã€‚</li>
<li>å¼•å…¥äº†ä¸€ç§æ–°å‹æ¸¸æˆåŒ–æ¡†æ¶â€”â€”â€œç—›è‹¦æ¸¸æˆç§€â€ï¼Œç”¨äºè¯„ä¼°LLMçš„åŠ¨æ€æƒ…æ„Ÿæ¨ç†èƒ½åŠ›ã€‚</li>
<li>è¯¥æ¡†æ¶å…è®¸æµ‹è¯•LLMçš„é¢„æµ‹å‡†ç¡®æ€§ä»¥åŠåŸºäºçº æ­£åé¦ˆçš„é€‚åº”èƒ½åŠ›ã€‚</li>
<li>æ¸¸æˆåŒ–è¯„ä¼°æ–¹æ³•æ‰©å¤§äº†LLMåœ¨æƒ…æ„Ÿè®¡ç®—é¢†åŸŸçš„åº”ç”¨æ½œåŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.12669">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-7d3724a224f8d1f2752ebdfadce8a5e8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5eb76527a456d3f876ed1e34c73f3a59.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Non-Iterative-Symbolic-Aided-Chain-of-Thought-for-Logical-Reasoning"><a href="#Non-Iterative-Symbolic-Aided-Chain-of-Thought-for-Logical-Reasoning" class="headerlink" title="Non-Iterative Symbolic-Aided Chain-of-Thought for Logical Reasoning"></a>Non-Iterative Symbolic-Aided Chain-of-Thought for Logical Reasoning</h2><p><strong>Authors:Phuong Minh Nguyen, Tien Huu Dang, Naoya Inoue</strong></p>
<p>This work introduces Symbolic-Aided Chain-of-Thought (CoT), an improved approach to standard CoT, for logical reasoning in large language models (LLMs). The key idea is to integrate lightweight symbolic representations into few-shot prompts, structuring the inference steps with a consistent strategy to make reasoning patterns more explicit within a non-iterative reasoning process. By incorporating these symbolic structures, our method preserves the generalizability of standard prompting techniques while enhancing the transparency, interpretability, and analyzability of LLM logical reasoning. Extensive experiments on four well-known logical reasoning benchmarks â€“ ProofWriter, FOLIO, ProntoQA, and LogicalDeduction, which cover diverse reasoning scenarios â€“ demonstrate the effectiveness of the proposed approach, particularly in complex reasoning tasks that require navigating multiple constraints or rules. Notably, Symbolic-Aided CoT consistently improves LLMsâ€™ reasoning capabilities across various model sizes and significantly outperforms conventional CoT on three out of four datasets, ProofWriter, ProntoQA, and LogicalDeduction. </p>
<blockquote>
<p>æœ¬æ–‡ä»‹ç»äº†ç¬¦å·è¾…åŠ©æ€ç»´é“¾ï¼ˆCoTï¼‰æ–¹æ³•ï¼Œè¿™æ˜¯ä¸€ç§å¯¹æ ‡å‡†æ€ç»´é“¾çš„æ”¹è¿›æ–¹æ³•ï¼Œç”¨äºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„é€»è¾‘æ¨ç†ã€‚å…¶æ ¸å¿ƒæ€æƒ³æ˜¯å°†è½»é‡çº§ç¬¦å·è¡¨ç¤ºé›†æˆåˆ°å°‘é‡æç¤ºä¸­ï¼Œä½¿ç”¨ä¸€è‡´çš„ç­–ç•¥æ¥ç»“æ„åŒ–æ¨ç†æ­¥éª¤ï¼Œåœ¨ä¸€ä¸ªéè¿­ä»£æ¨ç†è¿‡ç¨‹ä¸­ä½¿æ¨ç†æ¨¡å¼æ›´åŠ æ˜ç¡®ã€‚é€šè¿‡èå…¥è¿™äº›ç¬¦å·ç»“æ„ï¼Œæˆ‘ä»¬çš„æ–¹æ³•æ—¢ä¿ç•™äº†æ ‡å‡†æç¤ºæŠ€æœ¯çš„é€šç”¨æ€§ï¼Œåˆæé«˜äº†LLMé€»è¾‘æ¨ç†çš„é€æ˜åº¦ã€å¯è§£é‡Šæ€§å’Œå¯åˆ†ææ€§ã€‚åœ¨æ¶µç›–å¤šç§æ¨ç†åœºæ™¯çš„å››ä¸ªçŸ¥åé€»è¾‘æ¨ç†åŸºå‡†æµ‹è¯•â€”â€”ProofWriterã€FOLIOã€ProntoQAå’ŒLogicalDeductionä¸Šçš„å¤§é‡å®éªŒè¯æ˜äº†æ‰€æå‡ºæ–¹æ³•çš„æœ‰æ•ˆæ€§ï¼Œç‰¹åˆ«æ˜¯åœ¨éœ€è¦åº”å¯¹å¤šé‡çº¦æŸæˆ–è§„åˆ™çš„å¤æ‚æ¨ç†ä»»åŠ¡ä¸­ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œç¬¦å·è¾…åŠ©æ€ç»´é“¾æ–¹æ³•åœ¨å„ç§æ¨¡å‹å¤§å°ä¸‹éƒ½èƒ½æŒç»­æé«˜LLMçš„æ¨ç†èƒ½åŠ›ï¼Œå¹¶åœ¨å››ä¸ªæ•°æ®é›†ä¸­çš„ä¸‰ä¸ªï¼ˆProofWriterã€ProntoQAå’ŒLogicalDeductionï¼‰ä¸Šæ˜¾è‘—ä¼˜äºä¼ ç»Ÿæ€ç»´é“¾æ–¹æ³•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.12425v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>ç¬¦å·è¾…åŠ©æ€ç»´é“¾ï¼ˆSymbolic-Aided Chain-of-Thoughtï¼Œç®€ç§°Symbolic-Aided CoTï¼‰æ˜¯ä¸€ç§æ”¹è¿›çš„æ€ç»´é“¾æ–¹æ³•ï¼Œç”¨äºæé«˜å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„é€»è¾‘æ¨ç†èƒ½åŠ›ã€‚å®ƒé€šè¿‡æ•´åˆè½»é‡çº§ç¬¦å·è¡¨ç¤ºè¿›å…¥å°‘é‡æç¤ºï¼Œå°†æ¨ç†æ­¥éª¤ç»“æ„åŒ–ï¼Œå¹¶é‡‡ç”¨ä¸€è‡´ç­–ç•¥ï¼Œä½¿éè¿­ä»£æ¨ç†è¿‡ç¨‹ä¸­çš„æ¨ç†æ¨¡å¼æ›´åŠ æ˜ç¡®ã€‚è¿™ç§æ–¹æ³•æé«˜äº†LLMçš„é€»è¾‘æ¨ç†çš„é€šç”¨æ€§ã€é€æ˜åº¦ã€è§£é‡Šæ€§å’Œåˆ†ææ€§ã€‚åœ¨æ¶µç›–å¤šç§æ¨ç†åœºæ™¯çš„å››ä¸ªçŸ¥åé€»è¾‘æ¨ç†åŸºå‡†æµ‹è¯•ä¸Šè¿›è¡Œçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å¤æ‚æ¨ç†ä»»åŠ¡ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œç‰¹åˆ«æ˜¯åœ¨æ¶‰åŠå¤šä¸ªçº¦æŸæˆ–è§„åˆ™çš„ä»»åŠ¡ä¸­è¡¨ç°å°¤ä¸ºå‡ºè‰²ã€‚è¯¥æ–¹æ³•åœ¨å„ç§æ¨¡å‹å°ºå¯¸ä¸Šå‡æé«˜äº†LLMçš„æ¨ç†èƒ½åŠ›ï¼Œå¹¶åœ¨ä¸‰ä¸ªåŸºå‡†æµ‹è¯•ä¸Šæ˜¾è‘—ä¼˜äºä¼ ç»Ÿæ€ç»´é“¾æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>Symbolic-Aided CoTæ˜¯æ€ç»´é“¾ï¼ˆCoTï¼‰çš„ä¸€ç§æ”¹è¿›æ–¹æ³•ï¼Œç”¨äºå¢å¼ºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„é€»è¾‘æ¨ç†èƒ½åŠ›ã€‚</li>
<li>è¯¥æ–¹æ³•é€šè¿‡æ•´åˆè½»é‡çº§ç¬¦å·è¡¨ç¤ºè¿›å…¥å°‘é‡æç¤ºï¼Œæé«˜æ¨ç†çš„é€æ˜åº¦ã€è§£é‡Šæ€§å’Œåˆ†ææ€§ã€‚</li>
<li>å®éªŒè¡¨æ˜ï¼ŒSymbolic-Aided CoTåœ¨å¤æ‚æ¨ç†ä»»åŠ¡ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œç‰¹åˆ«æ˜¯åœ¨æ¶‰åŠå¤šä¸ªçº¦æŸæˆ–è§„åˆ™çš„ä»»åŠ¡ä¸­ã€‚</li>
<li>è¯¥æ–¹æ³•åœ¨å„ç§æ¨¡å‹å°ºå¯¸ä¸Šéƒ½æé«˜äº†LLMçš„æ¨ç†èƒ½åŠ›ã€‚</li>
<li>ä¸ä¼ ç»Ÿæ€ç»´é“¾æ–¹æ³•ç›¸æ¯”ï¼ŒSymbolic-Aided CoTåœ¨ä¸‰ä¸ªåŸºå‡†æµ‹è¯•ä¸Šè¡¨ç°æ›´ä½³ã€‚</li>
<li>è¿™ç§æ–¹æ³•ä¿ç•™äº†æ ‡å‡†æç¤ºæŠ€æœ¯çš„é€šç”¨æ€§ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.12425">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-a61175ccf3c29757a7c230adc1c4ee90.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-565a2f3cf199c6e909864cdeadaf57b5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-62211285c0f33f0f2f2df7a2c2e8c463.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-984a24c27dde6ee82dd5c6b55db4db1d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-bd8c4a3b464e51d4bf1a247a25254577.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="CC-Time-Cross-Model-and-Cross-Modality-Time-Series-Forecasting"><a href="#CC-Time-Cross-Model-and-Cross-Modality-Time-Series-Forecasting" class="headerlink" title="CC-Time: Cross-Model and Cross-Modality Time Series Forecasting"></a>CC-Time: Cross-Model and Cross-Modality Time Series Forecasting</h2><p><strong>Authors:Peng Chen, Yihang Wang, Yang Shu, Yunyao Cheng, Kai Zhao, Zhongwen Rao, Lujia Pan, Bin Yang, Chenjuan Guo</strong></p>
<p>With the success of pre-trained language models (PLMs) in various application fields beyond natural language processing, language models have raised emerging attention in the field of time series forecasting (TSF) and have shown great prospects. However, current PLM-based TSF methods still fail to achieve satisfactory prediction accuracy matching the strong sequential modeling power of language models. To address this issue, we propose Cross-Model and Cross-Modality Learning with PLMs for time series forecasting (CC-Time). We explore the potential of PLMs for time series forecasting from two aspects: 1) what time series features could be modeled by PLMs, and 2) whether relying solely on PLMs is sufficient for building time series models. In the first aspect, CC-Time incorporates cross-modality learning to model temporal dependency and channel correlations in the language model from both time series sequences and their corresponding text descriptions. In the second aspect, CC-Time further proposes the cross-model fusion block to adaptively integrate knowledge from the PLMs and time series model to form a more comprehensive modeling of time series patterns. Extensive experiments on nine real-world datasets demonstrate that CC-Time achieves state-of-the-art prediction accuracy in both full-data training and few-shot learning situations. </p>
<blockquote>
<p>éšç€é¢„è®­ç»ƒè¯­è¨€æ¨¡å‹ï¼ˆPLMsï¼‰åœ¨è‡ªç„¶è¯­è¨€å¤„ç†ä»¥å¤–çš„å„ç§åº”ç”¨é¢†åŸŸçš„æˆåŠŸï¼Œè¯­è¨€æ¨¡å‹åœ¨æ—¶é—´åºåˆ—é¢„æµ‹ï¼ˆTSFï¼‰é¢†åŸŸå¼•èµ·äº†å¹¿æ³›çš„å…³æ³¨ï¼Œå¹¶æ˜¾ç¤ºå‡ºå·¨å¤§çš„æ½œåŠ›ã€‚ç„¶è€Œï¼Œå½“å‰çš„åŸºäºPLMçš„æ—¶é—´åºåˆ—é¢„æµ‹æ–¹æ³•ä»ç„¶æ— æ³•å®ç°å¯¹æ—¶é—´åºåˆ—æ¨¡å¼è¿›è¡Œå¼ºå¤§å»ºæ¨¡çš„æ»¡æ„é¢„æµ‹ç²¾åº¦ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†åŸºäºPLMçš„æ—¶é—´åºåˆ—é¢„æµ‹äº¤å‰æ¨¡å‹å’Œå¤šæ¨¡æ€å­¦ä¹ ï¼ˆCC-Timeï¼‰ã€‚æˆ‘ä»¬ä»ä¸¤ä¸ªæ–¹é¢æ¢ç´¢PLMåœ¨æ—¶é—´åºåˆ—é¢„æµ‹ä¸­çš„æ½œåŠ›ï¼š1ï¼‰PLMèƒ½å¤Ÿå»ºæ¨¡çš„æ—¶é—´åºåˆ—ç‰¹å¾æ˜¯ä»€ä¹ˆï¼›ä»¥åŠæ˜¯å¦ä»…é PLMå°±èƒ½å……åˆ†å»ºç«‹æ—¶é—´åºåˆ—æ¨¡å‹ã€‚åœ¨ç¬¬ä¸€ä¸ªæ–¹é¢ï¼ŒCC-Timeç»“åˆå¤šæ¨¡æ€å­¦ä¹ æ¥å»ºæ¨¡è¯­è¨€æ¨¡å‹ä¸­çš„æ—¶åºä¾èµ–æ€§å’Œé€šé“ç›¸å…³æ€§ï¼Œè¿™æ¥æºäºæ—¶é—´åºåˆ—åºåˆ—åŠå…¶ç›¸åº”çš„æ–‡æœ¬æè¿°ã€‚åœ¨ç¬¬äºŒä¸ªæ–¹é¢ï¼ŒCC-Timeè¿›ä¸€æ­¥æå‡ºäº†è·¨æ¨¡å‹èåˆæ¨¡å—ï¼Œä»¥è‡ªé€‚åº”åœ°èåˆæ¥è‡ªPLMå’Œæ—¶åºæ¨¡å‹çš„çŸ¥è¯†ï¼Œä»¥å®ç°å¯¹æ—¶é—´åºåˆ—æ¨¡å¼çš„æ›´å…¨é¢çš„å»ºæ¨¡ã€‚åœ¨ä¹ä¸ªçœŸå®æ•°æ®é›†ä¸Šçš„å¤§é‡å®éªŒè¡¨æ˜ï¼ŒCC-Timeåœ¨å…¨æ•°æ®è®­ç»ƒå’Œå°‘æ ·æœ¬å­¦ä¹ çš„æƒ…å†µä¸‹å‡è¾¾åˆ°äº†æœ€å…ˆè¿›çš„é¢„æµ‹ç²¾åº¦ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.12235v1">PDF</a> </p>
<p><strong>Summary</strong><br>     åŸºäºé¢„è®­ç»ƒè¯­è¨€æ¨¡å‹ï¼ˆPLMsï¼‰åœ¨å¤šä¸ªé¢†åŸŸåº”ç”¨ä¸Šçš„æˆåŠŸï¼Œå…¶åœ¨æ—¶é—´åºåˆ—é¢„æµ‹ï¼ˆTSFï¼‰é¢†åŸŸä¹Ÿå±•ç°å‡ºå·¨å¤§æ½œåŠ›ã€‚ä¸ºè§£å†³å½“å‰PLMåœ¨TSFé¢„æµ‹ç²¾åº¦ä¸Šçš„ä¸è¶³ï¼Œæœ¬æ–‡æå‡ºäº†è·¨æ¨¡å‹ä¸è·¨æ¨¡æ€å­¦ä¹ æ–¹æ³•CC-Timeï¼Œæ—¨åœ¨æå‡è¯­è¨€æ¨¡å‹åœ¨æ—¶åºæ•°æ®é¢„æµ‹æ–¹é¢çš„èƒ½åŠ›ã€‚CC-Timeä»ä¸¤ä¸ªæ–¹é¢æ¢ç´¢äº†è¯­è¨€æ¨¡å‹åœ¨æ—¶åºæ•°æ®é¢„æµ‹ä¸­çš„æ½œåŠ›ï¼ŒåŒ…æ‹¬æ—¶åºç‰¹å¾å»ºæ¨¡å’Œå•çº¯ä¾èµ–è¯­è¨€æ¨¡å‹æ„å»ºæ—¶åºæ¨¡å‹çš„å……åˆ†æ€§ã€‚é€šè¿‡å¼•å…¥è·¨æ¨¡æ€å­¦ä¹ ï¼ŒCC-Timeå¯ä»¥æ¨¡æ‹Ÿè¯­è¨€æ¨¡å‹ä¸­æ—¶é—´åºåˆ—åºåˆ—å’Œæ—¶é—´åºåˆ—æè¿°ä¹‹é—´çš„æ—¶é—´ä¾èµ–å…³ç³»å’Œé€šé“ç›¸å…³æ€§ã€‚åŒæ—¶ï¼Œé€šè¿‡è·¨æ¨¡å‹èåˆæŠ€æœ¯è‡ªé€‚åº”æ•´åˆè¯­è¨€æ¨¡å‹å’Œæ—¶åºæ¨¡å‹çš„çŸ¥è¯†ï¼Œæ„å»ºæ›´å…¨é¢å’Œç²¾å‡†çš„æ—¶åºæ¨¡å¼æ¨¡å‹ã€‚å®éªŒè¡¨æ˜ï¼ŒCC-Timeåœ¨å¤šä¸ªçœŸå®æ•°æ®é›†ä¸Šå–å¾—äº†é¢†å…ˆçš„é¢„æµ‹ç²¾åº¦ï¼Œæ— è®ºæ˜¯åœ¨å…¨æ•°æ®è®­ç»ƒè¿˜æ˜¯å°‘æ ·æœ¬å­¦ä¹ æƒ…å†µä¸‹å‡è¡¨ç°ä¼˜å¼‚ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>é¢„è®­ç»ƒè¯­è¨€æ¨¡å‹ï¼ˆPLMsï¼‰åœ¨å¤šä¸ªé¢†åŸŸåº”ç”¨ä¸Šçš„æˆåŠŸå¼•èµ·äº†å¯¹æ—¶é—´åºåˆ—é¢„æµ‹ï¼ˆTSFï¼‰é¢†åŸŸçš„å…³æ³¨ã€‚</li>
<li>å½“å‰PLMåœ¨TSFé¢„æµ‹ç²¾åº¦ä¸Šä»æœ‰ä¸è¶³ï¼Œéœ€è¦æ–°çš„æ–¹æ³•æå‡å…¶åœ¨æ—¶åºæ•°æ®é¢„æµ‹æ–¹é¢çš„èƒ½åŠ›ã€‚</li>
<li>CC-Timeä»æ—¶åºç‰¹å¾å»ºæ¨¡å’Œå•çº¯ä¾èµ–è¯­è¨€æ¨¡å‹æ„å»ºæ—¶åºæ¨¡å‹çš„å……åˆ†æ€§ä¸¤ä¸ªæ–¹é¢æ¢ç´¢äº†è¯­è¨€æ¨¡å‹åœ¨TSFä¸­çš„æ½œåŠ›ã€‚</li>
<li>CC-Timeå¼•å…¥è·¨æ¨¡æ€å­¦ä¹ æ¥æ¨¡æ‹Ÿè¯­è¨€æ¨¡å‹ä¸­æ—¶é—´åºåˆ—åºåˆ—å’Œæ—¶é—´åºåˆ—æè¿°ä¹‹é—´çš„å…³ç³»ã€‚</li>
<li>CC-Timeé€šè¿‡è·¨æ¨¡å‹èåˆæŠ€æœ¯æ•´åˆè¯­è¨€æ¨¡å‹å’Œæ—¶åºæ¨¡å‹çš„çŸ¥è¯†ï¼Œæ„å»ºæ›´å…¨é¢å’Œç²¾å‡†çš„æ—¶åºæ¨¡å¼æ¨¡å‹ã€‚</li>
<li>å®éªŒè¡¨æ˜ï¼ŒCC-Timeåœ¨å¤šä¸ªçœŸå®æ•°æ®é›†ä¸Šå–å¾—äº†é¢†å…ˆçš„é¢„æµ‹ç²¾åº¦ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.12235">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-06f201bf93a018724b71c5ef6d28ef9e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3266066a560398864fdb4e08a8f59619.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-fee663e1083414b9a3765acf36766cef.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-dfbbfdc89027582bc4d94ccedfa3c8e8.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="RT-Cache-Training-Free-Retrieval-for-Real-Time-Manipulation"><a href="#RT-Cache-Training-Free-Retrieval-for-Real-Time-Manipulation" class="headerlink" title="RT-Cache: Training-Free Retrieval for Real-Time Manipulation"></a>RT-Cache: Training-Free Retrieval for Real-Time Manipulation</h2><p><strong>Authors:Owen Kwon, Abraham George, Alison Bartsch, Amir Barati Farimani</strong></p>
<p>Real robots are expected to repeat the same behavior in new environments with very little new data, yet modern controllers either incur heavy per-step inference or require deployment-time fine-tuning. We propose RT-Cache, a training-free retrieval-as-control pipeline that caches diverse image action trajectories in a unified vector memory and, at test time, embeds the current frame to retrieve and replay multi-step snippets, replacing per-step model calls. A hierarchical search keeps lookups sub-second at million scale, shifting cost from compute to storage and enabling real-time control on modest GPUs. Across real-robot tasks and large open logs, RT-Cache achieves higher success and lower completion time than strong retrieval baselines (approximately x2 higher success and ~30% faster in our settings), and a single-episode anchoring study shows immediate adaptation to a more complex, contact-rich task without fine-tuning. RT-Cache turns experience into an append-only memory, offering a simple, scalable path to few-shot deployment today and a foundation for multimodal keys and optional integration with high-level policies. Project page: <a target="_blank" rel="noopener" href="https://rt-cache.github.io/">https://rt-cache.github.io/</a>. </p>
<blockquote>
<p>ç°ä»£æœºå™¨äººè¢«æœŸæœ›åœ¨æ–°çš„ç¯å¢ƒä¸­ä½¿ç”¨å¾ˆå°‘çš„æ–°æ•°æ®å°±èƒ½é‡å¤æ‰§è¡Œç›¸åŒçš„è¡Œä¸ºï¼Œç„¶è€Œç°æœ‰çš„æ§åˆ¶å™¨è¦ä¹ˆåœ¨æ¯ä¸€æ­¥æ¨ç†æ—¶äº§ç”Ÿå·¨å¤§çš„å¼€é”€ï¼Œè¦ä¹ˆéœ€è¦åœ¨éƒ¨ç½²æ—¶è¿›è¡Œå¾®è°ƒã€‚æˆ‘ä»¬æå‡ºäº†RT-Cacheï¼Œè¿™æ˜¯ä¸€ç§æ— éœ€è®­ç»ƒçš„å›æ”¾æ§åˆ¶ç®¡é“ï¼Œå®ƒå¯ä»¥åœ¨ç»Ÿä¸€çš„å‘é‡å†…å­˜ä¸­ç¼“å­˜å„ç§å›¾åƒè¡Œä¸ºè½¨è¿¹ã€‚åœ¨æµ‹è¯•æ—¶ï¼Œå®ƒå°†å½“å‰å¸§åµŒå…¥å…¶ä¸­ï¼Œä»¥æ£€ç´¢å’Œå›æ”¾å¤šæ­¥ç‰‡æ®µï¼Œä»è€Œæ›¿ä»£æ¯ä¸€æ­¥çš„æ¨¡å‹è°ƒç”¨ã€‚åˆ†å±‚æœç´¢ä½¿æŸ¥æ‰¾æ—¶é—´ä¿æŒåœ¨å­ç§’çº§åˆ«ï¼Œå³ä½¿åœ¨ç™¾ä¸‡çº§åˆ«è§„æ¨¡ä¸Šä¹Ÿæ˜¯å¦‚æ­¤ï¼Œä»è€Œå°†æˆæœ¬ä»è®¡ç®—è½¬ç§»åˆ°å­˜å‚¨ï¼Œå¹¶åœ¨é€‚åº¦çš„GPUä¸Šå®ç°å®æ—¶æ§åˆ¶ã€‚åœ¨çœŸå®æœºå™¨äººä»»åŠ¡å’Œå¤§å‹å¼€æ”¾æ—¥å¿—ä¸­ï¼ŒRT-Cacheçš„æˆåŠŸç‡é«˜äºå¼ºå¤§çš„æ£€ç´¢åŸºçº¿ï¼Œå®Œæˆæ—¶é—´æ›´çŸ­ï¼ˆåœ¨æˆ‘ä»¬çš„è®¾ç½®ä¸­ï¼Œå¤§çº¦æˆåŠŸç‡æé«˜ä¸¤å€ï¼Œé€Ÿåº¦æé«˜çº¦30%ï¼‰ï¼Œä¸€é¡¹å•é›†é”šå®šç ”ç©¶è¡¨æ˜ï¼Œå®ƒèƒ½å¤Ÿç«‹å³é€‚åº”æ›´å¤æ‚ã€æ¥è§¦ä¸°å¯Œçš„ä»»åŠ¡è€Œæ— éœ€å¾®è°ƒã€‚RT-Cacheå°†ç»éªŒè½¬åŒ–ä¸ºä¸€ç§é™„åŠ å¼å†…å­˜ï¼Œä¸ºå½“ä»Šçš„å°‘é‡éƒ¨ç½²æä¾›äº†ä¸€æ¡ç®€å•ã€å¯æ‰©å±•çš„è·¯å¾„ï¼Œå¹¶ä¸ºå¤šæ¨¡å¼é”®å’Œä¸é«˜çº§æ”¿ç­–çš„å¯é€‰é›†æˆå¥ å®šäº†åŸºç¡€ã€‚é¡¹ç›®é¡µé¢ï¼š<a target="_blank" rel="noopener" href="https://rt-cache.github.io/%E3%80%82">https://rt-cache.github.io/ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.09040v2">PDF</a> 8 pages, 6 figures. Accepted to the 2025 IEEE-RAS 24th International   Conference on Humanoid Robots</p>
<p><strong>Summary</strong><br>     è¯¥æ–‡æœ¬ä»‹ç»äº†RT-Cacheç³»ç»Ÿï¼Œå®ƒé‡‡ç”¨æ— è®­ç»ƒæ£€ç´¢æ§åˆ¶æµç¨‹ï¼Œé€šè¿‡ç¼“å­˜å›¾åƒåŠ¨ä½œè½¨è¿¹å¹¶åœ¨æµ‹è¯•æ—¶åµŒå…¥å½“å‰å¸§æ¥æ£€ç´¢å’Œå›æ”¾å¤šæ­¥ç‰‡æ®µï¼Œä»è€Œå–ä»£äº†åˆ†æ­¥æ¨¡å‹è°ƒç”¨ã€‚è¯¥ç³»ç»Ÿé‡‡ç”¨åˆ†å±‚æœç´¢ï¼Œåœ¨ç™¾ä¸‡çº§è§„æ¨¡ä¸Šå®ç°äº†äºšç§’çº§çš„æŸ¥æ‰¾é€Ÿåº¦ï¼Œé™ä½äº†è®¡ç®—æˆæœ¬å¹¶å®ç°äº†å®æ—¶æ§åˆ¶ã€‚åœ¨çœŸå®æœºå™¨äººä»»åŠ¡å’Œå¤§è§„æ¨¡å¼€æ”¾æ—¥å¿—æµ‹è¯•ä¸­ï¼ŒRT-Cacheç›¸è¾ƒäºå¼ºå¤§çš„æ£€ç´¢åŸºçº¿å–å¾—äº†æ›´é«˜çš„æˆåŠŸç‡å’Œæ›´å¿«çš„å®Œæˆæ—¶é—´ã€‚æ­¤å¤–ï¼Œå…¶å³æ—¶é€‚åº”å¤æ‚æ¥è§¦å¯†é›†å‹ä»»åŠ¡çš„èƒ½åŠ›ï¼Œæ— éœ€å¾®è°ƒã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>RT-Cacheç³»ç»Ÿé‡‡ç”¨æ— è®­ç»ƒæ£€ç´¢æ§åˆ¶æµç¨‹ï¼Œé€‚ç”¨äºçœŸå®æœºå™¨äººåœºæ™¯ä¸­çš„å¿«é€Ÿé€‚åº”ä»»åŠ¡ã€‚</li>
<li>é€šè¿‡ç¼“å­˜å›¾åƒåŠ¨ä½œè½¨è¿¹å¹¶åµŒå…¥å½“å‰å¸§è¿›è¡Œæ£€ç´¢å’Œå›æ”¾ï¼Œå®ç°é«˜æ•ˆç‡ä¸”ä¸éœ€è¦åˆ†æ­¥æ¨¡å‹è°ƒç”¨ã€‚</li>
<li>åˆ†å±‚æœç´¢æŠ€æœ¯ä½¿å¾—æŸ¥æ‰¾é€Ÿåº¦åœ¨ç™¾ä¸‡çº§è§„æ¨¡ä¸Šå®ç°äºšç§’çº§å“åº”ã€‚</li>
<li>RT-Cacheå®ç°äº†å®æ—¶æ§åˆ¶ï¼Œé™ä½äº†è®¡ç®—æˆæœ¬ã€‚</li>
<li>åœ¨çœŸå®æœºå™¨äººä»»åŠ¡å’Œå¤§è§„æ¨¡å¼€æ”¾æ—¥å¿—æµ‹è¯•ä¸­ï¼ŒRT-Cacheæ¯”å¼ºå¤§çš„æ£€ç´¢åŸºçº¿è¡¨ç°å‡ºæ›´é«˜çš„æˆåŠŸç‡å’Œæ›´å¿«çš„å®Œæˆæ—¶é—´ã€‚</li>
<li>ç³»ç»Ÿèƒ½å¤Ÿå³æ—¶é€‚åº”å¤æ‚çš„æ¥è§¦å¯†é›†å‹ä»»åŠ¡ï¼Œæ— éœ€è¿›è¡Œå¾®è°ƒã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.09040">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-2bb2dd657629b0f3ccdac0b354b5da65.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c275cea08aa9eb68ea5b91104c5a9705.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9440bb0f6c1957d19c742a10534c6bf9.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-8fc098f1981f1081ad3e9cc80614f909.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-29666b3c6a14d523e000d37c2bf4a62d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a74a1b08a346ad94cb6c798f0bb219c8.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c0b775db71b7945606a1da08ad168620.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="MEGA-Second-Order-Gradient-Alignment-for-Catastrophic-Forgetting-Mitigation-in-GFSCIL"><a href="#MEGA-Second-Order-Gradient-Alignment-for-Catastrophic-Forgetting-Mitigation-in-GFSCIL" class="headerlink" title="MEGA: Second-Order Gradient Alignment for Catastrophic Forgetting   Mitigation in GFSCIL"></a>MEGA: Second-Order Gradient Alignment for Catastrophic Forgetting   Mitigation in GFSCIL</h2><p><strong>Authors:Jinhui Pang, Changqing Lin, Hao Lin, Zhihui Zhang, Long Chen, Weiping Ding, Yu Liu, Xiaoshuai Hao</strong></p>
<p>Graph Few-Shot Class-Incremental Learning (GFSCIL) enables models to continually learn from limited samples of novel tasks after initial training on a large base dataset. Existing GFSCIL approaches typically utilize Prototypical Networks (PNs) for metric-based class representations and fine-tune the model during the incremental learning stage. However, these PN-based methods oversimplify learning via novel query set fine-tuning and fail to integrate Graph Continual Learning (GCL) techniques due to architectural constraints. To address these challenges, we propose a more rigorous and practical setting for GFSCIL that excludes query sets during the incremental training phase. Building on this foundation, we introduce Model-Agnostic Meta Graph Continual Learning (MEGA), aimed at effectively alleviating catastrophic forgetting for GFSCIL. Specifically, by calculating the incremental second-order gradient during the meta-training stage, we endow the model to learn high-quality priors that enhance incremental learning by aligning its behaviors across both the meta-training and incremental learning stages. Extensive experiments on four mainstream graph datasets demonstrate that MEGA achieves state-of-the-art results and enhances the effectiveness of various GCL methods in GFSCIL. We believe that our proposed MEGA serves as a model-agnostic GFSCIL paradigm, paving the way for future research. </p>
<blockquote>
<p>å›¾å¢é‡å°‘æ ·æœ¬ç±»å¢é‡å­¦ä¹ ï¼ˆGFSCILï¼‰ä½¿æ¨¡å‹èƒ½å¤Ÿåœ¨å¤§é‡åŸºç¡€æ•°æ®é›†ä¸Šè¿›è¡Œåˆæ­¥è®­ç»ƒåï¼Œä»æ–°å¢ä»»åŠ¡çš„æœ‰é™æ ·æœ¬ä¸­æŒç»­å­¦ä¹ ã€‚ç°æœ‰çš„GFSCILæ–¹æ³•é€šå¸¸åˆ©ç”¨åŸå‹ç½‘ç»œï¼ˆPNsï¼‰è¿›è¡ŒåŸºäºåº¦é‡çš„ç±»è¡¨ç¤ºï¼Œå¹¶åœ¨å¢é‡å­¦ä¹ é˜¶æ®µå¯¹æ¨¡å‹è¿›è¡Œå¾®è°ƒã€‚ç„¶è€Œï¼Œè¿™äº›åŸºäºPNçš„æ–¹æ³•é€šè¿‡æ–°çš„æŸ¥è¯¢é›†å¾®è°ƒæ¥ç®€åŒ–å­¦ä¹ ï¼Œå¹¶ä¸”ç”±äºæ¶æ„çº¦æŸï¼Œæ— æ³•æ•´åˆå›¾æŒç»­å­¦ä¹ ï¼ˆGCLï¼‰æŠ€æœ¯ã€‚ä¸ºäº†è§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬ä¸ºGFSCILæå‡ºäº†ä¸€ä¸ªæ›´ä¸¥æ ¼ã€æ›´å®ç”¨çš„è®¾ç½®ï¼Œå³åœ¨å¢é‡è®­ç»ƒé˜¶æ®µæ’é™¤æŸ¥è¯¢é›†ã€‚åœ¨æ­¤åŸºç¡€ä¸Šï¼Œæˆ‘ä»¬å¼•å…¥äº†æ¨¡å‹æ— å…³çš„å…ƒå›¾æŒç»­å­¦ä¹ ï¼ˆMEGAï¼‰ï¼Œæ—¨åœ¨æœ‰æ•ˆç¼“è§£GFSCILçš„ç¾éš¾æ€§é—å¿˜é—®é¢˜ã€‚å…·ä½“æ¥è¯´ï¼Œé€šè¿‡è®¡ç®—å…ƒè®­ç»ƒé˜¶æ®µçš„å¢é‡äºŒé˜¶æ¢¯åº¦ï¼Œæˆ‘ä»¬èµ‹äºˆæ¨¡å‹å­¦ä¹ é«˜è´¨é‡å…ˆéªŒçš„èƒ½åŠ›ï¼Œé€šè¿‡å¯¹é½å…ƒè®­ç»ƒé˜¶æ®µå’Œå¢é‡å­¦ä¹ é˜¶æ®µçš„è¡Œä¸ºï¼Œå¢å¼ºå¢é‡å­¦ä¹ æ•ˆæœã€‚åœ¨å››ä¸ªä¸»æµå›¾æ•°æ®é›†ä¸Šçš„å¤§é‡å®éªŒè¡¨æ˜ï¼ŒMEGAè¾¾åˆ°äº†æœ€æ–°çš„ç»“æœï¼Œæé«˜äº†GFSCILä¸­å„ç§GCLæ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚æˆ‘ä»¬ç›¸ä¿¡ï¼Œæˆ‘ä»¬æå‡ºçš„MEGAä½œä¸ºä¸€ç§æ¨¡å‹æ— å…³çš„GFSCILèŒƒå¼ï¼Œä¸ºæœªæ¥çš„ç ”ç©¶é“ºå¹³äº†é“è·¯ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.13691v2">PDF</a> Under Review</p>
<p><strong>Summary</strong></p>
<p>åŸºäºå›¾æ•°æ®çš„Few-Shotç±»å¢é‡å­¦ä¹ ï¼ˆGFSCILï¼‰èƒ½å¤Ÿè®©æ¨¡å‹åœ¨åˆå§‹æ—¶è®­ç»ƒå¤§é‡åŸºç¡€æ•°æ®é›†åï¼Œç»§ç»­å­¦ä¹ æ–°ä»»åŠ¡çš„å°æ ·æœ¬æ•°æ®ã€‚ç°æœ‰çš„GFSCILæ–¹æ³•é€šå¸¸é‡‡ç”¨åŸå‹ç½‘ç»œï¼ˆPNsï¼‰è¿›è¡ŒåŸºäºåº¦é‡çš„ç±»è¡¨ç¤ºï¼Œå¹¶åœ¨å¢é‡å­¦ä¹ é˜¶æ®µå¾®è°ƒæ¨¡å‹ã€‚ç„¶è€Œï¼ŒåŸºäºPNçš„æ–¹æ³•ç®€åŒ–äº†é€šè¿‡æ–°æŸ¥è¯¢é›†è¿›è¡Œå¾®è°ƒçš„å­¦ä¹ è¿‡ç¨‹ï¼Œå¹¶å› æ¶æ„çº¦æŸæœªèƒ½æ•´åˆå›¾æŒç»­å­¦ä¹ ï¼ˆGCLï¼‰æŠ€æœ¯ã€‚ä¸ºè§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†æ›´ä¸¥æ ¼å’Œå®ç”¨çš„GFSCILè®¾ç½®ï¼Œå¢é‡è®­ç»ƒé˜¶æ®µæ’é™¤æŸ¥è¯¢é›†ã€‚åœ¨æ­¤åŸºç¡€ä¸Šï¼Œæˆ‘ä»¬å¼•å…¥äº†æ¨¡å‹æ— å…³çš„å…ƒå›¾æŒç»­å­¦ä¹ ï¼ˆMEGAï¼‰æ–¹æ³•ï¼Œæ—¨åœ¨æœ‰æ•ˆç¼“è§£GFSCILçš„ç¾éš¾æ€§é—å¿˜é—®é¢˜ã€‚é€šè¿‡è®¡ç®—å…ƒè®­ç»ƒé˜¶æ®µçš„äºŒé˜¶å¢é‡æ¢¯åº¦ï¼Œæˆ‘ä»¬ä½¿æ¨¡å‹å­¦ä¹ é«˜è´¨é‡å…ˆéªŒï¼Œå¢å¼ºå…ƒè®­ç»ƒå’Œå¢é‡å­¦ä¹ é˜¶æ®µçš„è¡Œä¸ºä¸€è‡´æ€§ã€‚åœ¨å››ä¸ªä¸»æµå›¾æ•°æ®é›†ä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼ŒMEGAè¾¾åˆ°äº†æœ€æ–°æ°´å¹³çš„ç»“æœï¼Œæé«˜äº†å„ç§GCLæ–¹æ³•åœ¨GFSCILä¸­çš„æœ‰æ•ˆæ€§ã€‚æˆ‘ä»¬ç›¸ä¿¡ï¼Œæˆ‘ä»¬æå‡ºçš„MEGAä¸ºGFSCILæä¾›äº†ä¸€ä¸ªæ¨¡å‹æ— å…³çš„è§£å†³æ–¹æ¡ˆï¼Œä¸ºæœªæ¥çš„ç ”ç©¶é“ºå¹³äº†é“è·¯ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Graph Few-Shot Class-Incremental Learning (GFSCIL) å…è®¸æ¨¡å‹åœ¨ä¸æ–­å­¦ä¹ æ–°ä»»åŠ¡çš„å°æ ·æœ¬æ•°æ®çš„åŒæ—¶ï¼Œä¿ç•™åœ¨åˆå§‹å¤§é‡åŸºç¡€æ•°æ®é›†ä¸Šè®­ç»ƒçš„ç»“æœã€‚</li>
<li>ç°æœ‰GFSCILæ–¹æ³•ä¸»è¦ä½¿ç”¨åŸå‹ç½‘ç»œï¼ˆPNsï¼‰è¿›è¡Œç±»è¡¨ç¤ºå’Œå¾®è°ƒæ¨¡å‹ï¼Œä½†è¿™ç§æ–¹æ³•è¿‡äºç®€åŒ–å­¦ä¹ è¿‡ç¨‹ã€‚</li>
<li>åŸºäºPNçš„æ–¹æ³•æœªèƒ½å……åˆ†åˆ©ç”¨å›¾æŒç»­å­¦ä¹ ï¼ˆGCLï¼‰æŠ€æœ¯ï¼Œä¸»è¦åŸå› æ˜¯æ¶æ„ä¸Šçš„é™åˆ¶ã€‚</li>
<li>ä¸ºæ”¹è¿›ç°æœ‰æ–¹æ³•ï¼Œæå‡ºäº†æ›´ä¸¥æ ¼å’Œå®ç”¨çš„GFSCILè®¾ç½®ï¼Œå³åœ¨å¢é‡è®­ç»ƒé˜¶æ®µæ’é™¤æŸ¥è¯¢é›†ã€‚</li>
<li>å¼•å…¥äº†ä¸€ç§æ–°çš„æ–¹æ³•â€”â€”æ¨¡å‹æ— å…³çš„å…ƒå›¾æŒç»­å­¦ä¹ ï¼ˆMEGAï¼‰â€”â€”æ¥ç¼“è§£GFSCILä¸­çš„ç¾éš¾æ€§é—å¿˜é—®é¢˜ã€‚</li>
<li>MEGAé€šè¿‡è®¡ç®—å…ƒè®­ç»ƒé˜¶æ®µçš„äºŒé˜¶å¢é‡æ¢¯åº¦æ¥å­¦ä¹ é«˜è´¨é‡å…ˆéªŒçŸ¥è¯†ï¼Œå¢å¼ºäº†æ¨¡å‹åœ¨å…ƒè®­ç»ƒå’Œå¢é‡å­¦ä¹ é˜¶æ®µçš„è¡Œä¸ºä¸€è‡´æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.13691">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-93f547e745031e2ee5675d1050b0a962.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-697cff06aa36c224265bf68866ec0822.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-76762afef9d330b19e4b1da7546077d2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5d73516765465589443291dc74bf929a.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-be152d1f77e9c2cb490d93192384c07f.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-2678f5bbd902bcfefa0afe733419b18e.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="Can-Masked-Autoencoders-Also-Listen-to-Birds"><a href="#Can-Masked-Autoencoders-Also-Listen-to-Birds" class="headerlink" title="Can Masked Autoencoders Also Listen to Birds?"></a>Can Masked Autoencoders Also Listen to Birds?</h2><p><strong>Authors:Lukas Rauch, RenÃ© Heinrich, Ilyass Moummad, Alexis Joly, Bernhard Sick, Christoph Scholz</strong></p>
<p>Masked Autoencoders (MAEs) learn rich semantic representations in audio classification through an efficient self-supervised reconstruction task. However, general-purpose models fail to generalize well when applied directly to fine-grained audio domains. Specifically, bird-sound classification requires distinguishing subtle inter-species differences and managing high intra-species acoustic variability, revealing the performance limitations of general-domain Audio-MAEs. This work demonstrates that bridging this domain gap domain gap requires full-pipeline adaptation, not just domain-specific pretraining data. We systematically revisit and adapt the pretraining recipe, fine-tuning methods, and frozen feature utilization to bird sounds using BirdSet, a large-scale bioacoustic dataset comparable to AudioSet. Our resulting Bird-MAE achieves new state-of-the-art results in BirdSetâ€™s multi-label classification benchmark. Additionally, we introduce the parameter-efficient prototypical probing, enhancing the utility of frozen MAE representations and closely approaching fine-tuning performance in low-resource settings. Bird-MAEâ€™s prototypical probes outperform linear probing by up to 37 percentage points in mean average precision and narrow the gap to fine-tuning across BirdSet downstream tasks. Bird-MAE also demonstrates robust few-shot capabilities with prototypical probing in our newly established few-shot benchmark on BirdSet, highlighting the potential of tailored self-supervised learning pipelines for fine-grained audio domains. </p>
<blockquote>
<p>Masked Autoencoders (MAEs)é€šè¿‡é«˜æ•ˆçš„è‡ªç›‘ç£é‡å»ºä»»åŠ¡ï¼Œåœ¨éŸ³é¢‘åˆ†ç±»ä¸­å­¦ä¹ ä¸°å¯Œçš„è¯­ä¹‰è¡¨ç¤ºã€‚ç„¶è€Œï¼Œå½“ç›´æ¥åº”ç”¨äºç»†ç²’åº¦éŸ³é¢‘åŸŸæ—¶ï¼Œé€šç”¨æ¨¡å‹å¾€å¾€æ— æ³•å¾ˆå¥½åœ°æ¨å¹¿ã€‚ç‰¹åˆ«æ˜¯ï¼Œé¸Ÿç±»å£°éŸ³åˆ†ç±»éœ€è¦åŒºåˆ†ç‰©ç§é—´çš„ç»†å¾®å·®å¼‚å¹¶å¤„ç†é«˜ç‰©ç§å†…éƒ¨å£°å­¦å˜åŒ–ï¼Œè¿™æ­ç¤ºäº†é€šç”¨é¢†åŸŸAudio-MAEçš„æ€§èƒ½å±€é™æ€§ã€‚è¿™é¡¹å·¥ä½œè¡¨æ˜ï¼Œå¼¥åˆè¿™ä¸€é¢†åŸŸå·®è·éœ€è¦å…¨ç®¡é“é€‚åº”ï¼Œè€Œä¸ä»…ä»…æ˜¯é¢†åŸŸç‰¹å®šçš„é¢„è®­ç»ƒæ•°æ®ã€‚æˆ‘ä»¬ç³»ç»Ÿåœ°å›é¡¾å¹¶é€‚åº”äº†é¢„è®­ç»ƒé…æ–¹ã€å¾®è°ƒæ–¹æ³•å’Œå†»ç»“ç‰¹å¾çš„åˆ©ç”¨é¸Ÿç±»å£°éŸ³ï¼Œä½¿ç”¨BirdSetâ€”â€”ä¸€ä¸ªå¯ä¸AudioSetç›¸æ¯”çš„å¤§è§„æ¨¡ç”Ÿç‰©å£°å­¦æ•°æ®é›†ã€‚æˆ‘ä»¬å¾—åˆ°çš„Bird-MAEåœ¨BirdSetçš„å¤šæ ‡ç­¾åˆ†ç±»åŸºå‡†æµ‹è¯•ä¸­è¾¾åˆ°äº†æœ€æ–°æ°´å¹³ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å¼•å…¥äº†å‚æ•°é«˜æ•ˆçš„åŸå‹æ¢æµ‹æŠ€æœ¯ï¼Œæé«˜äº†å†»ç»“MAEè¡¨ç¤ºçš„å®ç”¨æ€§ï¼Œå¹¶åœ¨ä½èµ„æºç¯å¢ƒä¸­æ¥è¿‘å¾®è°ƒæ€§èƒ½ã€‚Bird-MAEçš„åŸå‹æ¢é’ˆåœ¨å¹³å‡ç²¾åº¦å‡å€¼æ–¹é¢æ¯”çº¿æ€§æ¢é’ˆé«˜å‡ºé«˜è¾¾37ä¸ªç™¾åˆ†ç‚¹ï¼Œå¹¶ç¼©å°äº†åœ¨BirdSetä¸‹æ¸¸ä»»åŠ¡ä¸­çš„å¾®è°ƒå·®è·ã€‚Bird-MAEè¿˜å±•ç¤ºäº†æˆ‘ä»¬æ–°å»ºç«‹çš„BirdSetå°‘æ ·æœ¬åŸºå‡†æµ‹è¯•ä¸­åŸå‹æ¢é’ˆçš„ç¨³å¥çš„å°‘æ ·æœ¬èƒ½åŠ›ï¼Œçªæ˜¾äº†é’ˆå¯¹ç»†ç²’åº¦éŸ³é¢‘é¢†åŸŸé‡èº«å®šåˆ¶çš„è‡ªç›‘ç£å­¦ä¹ ç®¡é“çš„å·¨å¤§æ½œåŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.12880v4">PDF</a> accepted @TMLR: <a target="_blank" rel="noopener" href="https://openreview.net/forum?id=GIBWR0Xo2J">https://openreview.net/forum?id=GIBWR0Xo2J</a></p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†Masked Autoencodersï¼ˆMAEsï¼‰åœ¨éŸ³é¢‘åˆ†ç±»ä¸­çš„ä¸°å¯Œè¯­ä¹‰è¡¨ç¤ºå­¦ä¹ ã€‚ç„¶è€Œï¼Œé€šç”¨æ¨¡å‹åœ¨ç²¾ç»†ç²’åº¦éŸ³é¢‘é¢†åŸŸçš„åº”ç”¨ä¸­è¡¨ç°ä¸ä½³ã€‚é’ˆå¯¹é¸Ÿç±»å£°éŸ³åˆ†ç±»è¿™ä¸€ç‰¹å®šåœºæ™¯ï¼Œæœ¬ç ”ç©¶é€šè¿‡å…¨ç®¡é“é€‚åº”æ–¹æ³•ï¼Œè€Œéä»…ä¾èµ–ç‰¹å®šé¢†åŸŸçš„é¢„è®­ç»ƒæ•°æ®æ¥å¼¥è¡¥é¢†åŸŸå·®è·ã€‚é€šè¿‡è°ƒæ•´é¢„è®­ç»ƒé…æ–¹ã€å¾®è°ƒæ–¹æ³•å’Œå†»ç»“ç‰¹å¾çš„åˆ©ç”¨æ–¹å¼ï¼Œä½¿ç”¨å¤§è§„æ¨¡ç”Ÿç‰©å£°å­¦æ•°æ®é›†BirdSetï¼Œå®ç°äº†å¯¹é¸Ÿç±»å£°éŸ³çš„æ–°SOTAç»“æœã€‚æ­¤å¤–ï¼Œå¼•å…¥äº†å‚æ•°é«˜æ•ˆçš„åŸå‹æ¢æµ‹æŠ€æœ¯ï¼Œæé«˜äº†å†»ç»“MAEè¡¨ç¤ºçš„å®ç”¨æ€§ï¼Œå¹¶åœ¨ä½èµ„æºç¯å¢ƒä¸­æ¥è¿‘å¾®è°ƒæ€§èƒ½ã€‚Bird-MAEçš„åŸå‹æ¢é’ˆåœ¨å¹³å‡ç²¾åº¦ä¸Šæœ€å¤šé«˜å‡ºçº¿æ€§æ¢é’ˆ37ä¸ªç™¾åˆ†ç‚¹ï¼Œå¹¶ç¼©å°äº†åœ¨BirdSetä¸‹æ¸¸ä»»åŠ¡ä¸­çš„å¾®è°ƒå·®è·ã€‚åŒæ—¶ï¼ŒBird-MAEåœ¨å…¨æ–°å»ºç«‹çš„BirdSetå°‘æ ·æœ¬åŸºå‡†æµ‹è¯•ä¸­å±•ç°å‡ºå¼ºå¤§çš„å°‘æ ·æœ¬èƒ½åŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Masked Autoencoders (MAEs) èƒ½å¤Ÿæœ‰æ•ˆå­¦ä¹ éŸ³é¢‘åˆ†ç±»çš„ä¸°å¯Œè¯­ä¹‰è¡¨ç¤ºã€‚</li>
<li>é€šç”¨æ¨¡å‹åœ¨ç²¾ç»†ç²’åº¦éŸ³é¢‘é¢†åŸŸï¼ˆå¦‚é¸Ÿç±»å£°éŸ³åˆ†ç±»ï¼‰å­˜åœ¨æ€§èƒ½å±€é™ã€‚</li>
<li>è·¨è¶Šé¢†åŸŸå·®è·éœ€è¦å…¨ç®¡é“é€‚åº”æ–¹æ³•ï¼Œä¸ä»…ä¾èµ–ç‰¹å®šé¢†åŸŸçš„é¢„è®­ç»ƒæ•°æ®ã€‚</li>
<li>é€šè¿‡è°ƒæ•´é¢„è®­ç»ƒé…æ–¹ã€å¾®è°ƒæ–¹æ³•å’Œåˆ©ç”¨å†»ç»“ç‰¹å¾ï¼Œä½¿ç”¨BirdSetæ•°æ®é›†å®ç°äº†é¸Ÿå£°åˆ†ç±»çš„æ–°SOTAç»“æœã€‚</li>
<li>å¼•å…¥å‚æ•°é«˜æ•ˆçš„åŸå‹æ¢æµ‹æŠ€æœ¯ï¼Œæé«˜å†»ç»“MAEè¡¨ç¤ºçš„å®ç”¨æ€§ï¼Œå¹¶æ¥è¿‘ä½èµ„æºç¯å¢ƒä¸­çš„å¾®è°ƒæ€§èƒ½ã€‚</li>
<li>Bird-MAEçš„åŸå‹æ¢é’ˆåœ¨å¹³å‡ç²¾åº¦ä¸Šæœ‰æ˜¾è‘—æ”¹è¿›ï¼Œç¼©å°äº†ä¸å¾®è°ƒä¹‹é—´çš„å·®è·ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.12880">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-df44f685a39835e2694e81697a4c4a1d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-cfedc85a9dd737c7d2d0dffc62b57f40.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-3f4363adaf2c8eec1fe82d2372899ed5.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="MedSpaformer-a-Transferable-Transformer-with-Multi-granularity-Token-Sparsification-for-Medical-Time-Series-Classification"><a href="#MedSpaformer-a-Transferable-Transformer-with-Multi-granularity-Token-Sparsification-for-Medical-Time-Series-Classification" class="headerlink" title="MedSpaformer: a Transferable Transformer with Multi-granularity Token   Sparsification for Medical Time Series Classification"></a>MedSpaformer: a Transferable Transformer with Multi-granularity Token   Sparsification for Medical Time Series Classification</h2><p><strong>Authors:Jiexia Ye, Weiqi Zhang, Ziyue Li, Jia Li, Fugee Tsung</strong></p>
<p>Accurate medical time series (MedTS) classification is essential for effective clinical diagnosis, yet remains challenging due to complex multi-channel temporal dependencies, information redundancy, and label scarcity. While transformer-based models have shown promise in time series analysis, most are designed for forecasting tasks and fail to fully exploit the unique characteristics of MedTS. In this paper, we introduce MedSpaformer, a transformer-based framework tailored for MedTS classification. It incorporates a sparse token-based dual-attention mechanism that enables global context modeling and token sparsification, allowing dynamic feature refinement by focusing on informative tokens while reducing redundancy. This mechanism is integrated into a multi-granularity cross-channel encoding scheme to capture intra- and inter-granularity temporal dependencies and inter-channel correlations, enabling progressive refinement of task-relevant patterns in medical signals. The sparsification design allows our model to flexibly accommodate inputs with variable lengths and channel dimensions. We also introduce an adaptive label encoder to extract label semantics and address cross-dataset label space misalignment. Together, these components enhance the modelâ€™s transferability across heterogeneous medical datasets, which helps alleviate the challenge of label scarcity. Our model outperforms 13 baselines across 7 medical datasets under supervised learning. It also excels in few-shot learning and demonstrates zero-shot capability in both in-domain and cross-domain diagnostics. These results highlight MedSpaformerâ€™s robustness and its potential as a unified solution for MedTS classification across diverse settings. </p>
<blockquote>
<p>ç²¾ç¡®åŒ»ç–—æ—¶é—´åºåˆ—ï¼ˆMedTSï¼‰åˆ†ç±»å¯¹äºæœ‰æ•ˆçš„ä¸´åºŠè¯Šæ–­è‡³å…³é‡è¦ï¼Œä½†ç”±äºå¤æ‚çš„è·¨é€šé“æ—¶é—´ä¾èµ–æ€§ã€ä¿¡æ¯å†—ä½™å’Œæ ‡ç­¾ç¨€ç¼ºï¼Œå®ƒä»ç„¶æ˜¯ä¸€ä¸ªæŒ‘æˆ˜ã€‚è™½ç„¶åŸºäºå˜å‹å™¨çš„æ¨¡å‹åœ¨æ—¶é—´åºåˆ—åˆ†æä¸­æ˜¾ç¤ºå‡ºæ½œåŠ›ï¼Œä½†å¤§å¤šæ•°æ¨¡å‹éƒ½æ˜¯ä¸ºé¢„æµ‹ä»»åŠ¡è€Œè®¾è®¡çš„ï¼Œæœªèƒ½å……åˆ†åˆ©ç”¨MedTSçš„ç‹¬ç‰¹ç‰¹å¾ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†é’ˆå¯¹MedTSåˆ†ç±»çš„åŸºäºå˜å‹å™¨çš„æ¡†æ¶MedSpaformerã€‚å®ƒé‡‡ç”¨äº†ä¸€ç§ç¨€ç–çš„åŸºäºæ ‡è®°çš„åŒé‡æ³¨æ„åŠ›æœºåˆ¶ï¼Œèƒ½å¤Ÿå…¨å±€å»ºæ¨¡ä¸Šä¸‹æ–‡å¹¶ç²¾ç®€æ ‡è®°ï¼Œé€šè¿‡å…³æ³¨ä¿¡æ¯ä¸°å¯Œçš„æ ‡è®°è¿›è¡ŒåŠ¨æ€ç‰¹å¾ç»†åŒ–ï¼ŒåŒæ—¶å‡å°‘å†—ä½™ã€‚è¯¥æœºåˆ¶è¢«æ•´åˆåˆ°å¤šç²’åº¦è·¨é€šé“ç¼–ç æ–¹æ¡ˆä¸­ï¼Œä»¥æ•è·ç²’åº¦å’Œè·¨ç²’åº¦çš„æ—¶é—´ä¾èµ–æ€§ä»¥åŠé€šé“é—´çš„ç›¸å…³æ€§ï¼Œä»è€Œé€æ­¥ä¼˜åŒ–åŒ»ç–—ä¿¡å·ä¸­ä¸ä»»åŠ¡ç›¸å…³çš„æ¨¡å¼ã€‚ç²¾ç®€è®¾è®¡ä½¿å¾—æˆ‘ä»¬çš„æ¨¡å‹å¯ä»¥çµæ´»åœ°é€‚åº”å…·æœ‰ä¸åŒé•¿åº¦å’Œé€šé“ç»´åº¦çš„è¾“å…¥ã€‚æˆ‘ä»¬è¿˜å¼•å…¥äº†ä¸€ç§è‡ªé€‚åº”æ ‡ç­¾ç¼–ç å™¨æ¥æå–æ ‡ç­¾è¯­ä¹‰å¹¶è§£å†³è·¨æ•°æ®é›†æ ‡ç­¾ç©ºé—´çš„ä¸å¯¹é½é—®é¢˜ã€‚è¿™äº›ç»„ä»¶å…±åŒæé«˜äº†æ¨¡å‹åœ¨å¼‚è´¨åŒ»ç–—æ•°æ®é›†ä¸Šçš„è¿ç§»èƒ½åŠ›ï¼Œæœ‰åŠ©äºç¼“è§£æ ‡ç­¾ç¨€ç¼ºçš„æŒ‘æˆ˜ã€‚æˆ‘ä»¬çš„æ¨¡å‹åœ¨7ä¸ªåŒ»ç–—æ•°æ®é›†ä¸Šè¶…è¶Šäº†13ä¸ªåŸºçº¿æ¨¡å‹ï¼Œåœ¨ç›‘ç£å­¦ä¹ ä¸­çš„è¡¨ç°å°¤ä¸ºå‡ºè‰²ã€‚åœ¨å°‘é‡æ ·æœ¬å­¦ä¹ åœºæ™¯ä¸­ï¼Œè¯¥æ¨¡å‹è¡¨ç°å‡ºè‰²ï¼Œå¹¶åœ¨åŸŸå†…å’Œè·¨åŸŸè¯Šæ–­ä¸­å±•ç¤ºå‡ºé›¶æ ·æœ¬å­¦ä¹ èƒ½åŠ›ã€‚è¿™äº›ç»“æœå‡¸æ˜¾äº†MedSpaformerçš„ç¨³å¥æ€§ä»¥åŠå…¶åœ¨å¤šæ ·ç¯å¢ƒä¸‹ä½œä¸ºç»Ÿä¸€è§£å†³æ–¹æ¡ˆçš„æ½œåŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.15578v3">PDF</a> 4 figures, 9 pages, 4 tables</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§é’ˆå¯¹åŒ»å­¦æ—¶é—´åºåˆ—ï¼ˆMedTSï¼‰åˆ†ç±»çš„MedSpaformeræ¡†æ¶ã€‚å®ƒé€šè¿‡ç¨€ç–ä»¤ç‰ŒåŸºç¡€çš„åŒé‡æ³¨æ„åŠ›æœºåˆ¶ï¼Œå®ç°äº†å…¨å±€ä¸Šä¸‹æ–‡å»ºæ¨¡å’Œä»¤ç‰Œç¨€ç–åŒ–ï¼Œå‡å°‘äº†å†—ä½™ä¿¡æ¯å¹¶æå‡äº†ç‰¹å¾ç²¾ç‚¼èƒ½åŠ›ã€‚åŒæ—¶é›†æˆå¤šç²’åº¦è·¨é€šé“ç¼–ç æ–¹æ¡ˆï¼Œæ•æ‰ä¸åŒç²’åº¦çš„æ—¶åºä¾èµ–æ€§å’Œè·¨é€šé“ç›¸å…³æ€§ã€‚é€šè¿‡å¼•å…¥è‡ªé€‚åº”æ ‡ç­¾ç¼–ç å™¨è§£å†³è·¨æ•°æ®é›†æ ‡ç­¾ç©ºé—´ä¸ä¸€è‡´æ€§é—®é¢˜ï¼Œå¢å¼ºæ¨¡å‹åœ¨ä¸åŒåŒ»ç–—æ•°æ®é›†é—´çš„å¯è¿ç§»æ€§ã€‚å®éªŒæ˜¾ç¤ºè¯¥æ¨¡å‹åœ¨å¤šä¸ªåŒ»å­¦æ•°æ®é›†ä¸Šçš„è¡¨ç°ä¼˜äºå…¶ä»–åŸºçº¿æ¨¡å‹ï¼Œå°¤å…¶åœ¨å°‘æ ·æœ¬å­¦ä¹ æ–¹é¢è¡¨ç°ä¼˜å¼‚ï¼Œå¹¶å…·å¤‡é›¶æ ·æœ¬èƒ½åŠ›è¿›è¡ŒåŒåŸŸå’Œè·¨åŸŸè¯Šæ–­ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>MedSpaformeræ˜¯ä¸€ç§é’ˆå¯¹åŒ»å­¦æ—¶é—´åºåˆ—åˆ†ç±»çš„å˜å‹å™¨æ¨¡å‹æ¡†æ¶ã€‚</li>
<li>å®ƒé‡‡ç”¨äº†ç¨€ç–ä»¤ç‰ŒåŸºç¡€çš„åŒé‡æ³¨æ„åŠ›æœºåˆ¶ä»¥å®ç°å…¨å±€ä¸Šä¸‹æ–‡å»ºæ¨¡å’Œä»¤ç‰Œç¨€ç–åŒ–ã€‚</li>
<li>é›†æˆå¤šç²’åº¦è·¨é€šé“ç¼–ç æ–¹æ¡ˆï¼Œæœ‰æ•ˆæ•æ‰æ—¶åºæ•°æ®çš„ä¸åŒç²’åº¦ä¾èµ–æ€§å’Œè·¨é€šé“ç›¸å…³æ€§ã€‚</li>
<li>è‡ªé€‚åº”æ ‡ç­¾ç¼–ç å™¨ç”¨äºæå–æ ‡ç­¾è¯­ä¹‰å¹¶å¤„ç†è·¨æ•°æ®é›†æ ‡ç­¾ç©ºé—´ä¸ä¸€è‡´æ€§é—®é¢˜ã€‚</li>
<li>MedSpaformeræ¨¡å‹åœ¨å¤šä¸ªåŒ»å­¦æ•°æ®é›†ä¸Šçš„è¡¨ç°ä¼˜äºå…¶ä»–åŸºçº¿æ¨¡å‹ã€‚</li>
<li>è¯¥æ¨¡å‹åœ¨å°‘æ ·æœ¬å­¦ä¹ æ–¹é¢è¡¨ç°ä¼˜å¼‚ï¼Œå¹¶å…·å¤‡é›¶æ ·æœ¬èƒ½åŠ›è¿›è¡Œè¯Šæ–­ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.15578">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-5294a351417d2d84d6212825bbbdd547.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-67d937cfebb5978e0278141e23c9ab38.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-552015adbfb87c70e8eb7a2a881f8334.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-bc763bc37a858ffac3f10f97289d8dd3.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="Closed-Form-Feedback-Free-Learning-with-Forward-Projection"><a href="#Closed-Form-Feedback-Free-Learning-with-Forward-Projection" class="headerlink" title="Closed-Form Feedback-Free Learning with Forward Projection"></a>Closed-Form Feedback-Free Learning with Forward Projection</h2><p><strong>Authors:Robert Oâ€™Shea, Bipin Rajendran</strong></p>
<p>State-of-the-art methods for backpropagation-free learning employ local error feedback to direct iterative optimisation via gradient descent. In this study, we examine the more restrictive setting where retrograde communication from neuronal outputs is unavailable for pre-synaptic weight optimisation. To address this challenge, we propose Forward Projection (FP). This novel randomised closed-form training method requires only a single forward pass over the entire dataset for model fitting, without retrograde communication. Target values for pre-activation membrane potentials are generated layer-wise via nonlinear projections of pre-synaptic inputs and the labels. Local loss functions are optimised over pre-synaptic inputs using closed-form regression, without feedback from neuronal outputs or downstream layers. Interpretability is a key advantage of FP training; membrane potentials of hidden neurons in FP-trained networks encode information which is interpretable layer-wise as label predictions. We demonstrate the effectiveness of FP across four biomedical datasets. In few-shot learning tasks, FP yielded more generalisable models than those optimised via backpropagation. In large-sample tasks, FP-based models achieve generalisation comparable to gradient descent-based local learning methods while requiring only a single forward propagation step, achieving significant speed up for training. Interpretation functions defined on local neuronal activity in FP-based models successfully identified clinically salient features for diagnosis in two biomedical datasets. Forward Projection is a computationally efficient machine learning approach that yields interpretable neural network models without retrograde communication of neuronal activity during training. </p>
<blockquote>
<p>å½“å‰å…ˆè¿›çš„ä¸ä¾èµ–åå‘ä¼ æ’­çš„å­¦ä¹ æ–¹æ³•é‡‡ç”¨å±€éƒ¨è¯¯å·®åé¦ˆæ¥æŒ‡å¯¼é€šè¿‡æ¢¯åº¦ä¸‹é™æ³•çš„è¿­ä»£ä¼˜åŒ–ã€‚åœ¨è¿™é¡¹ç ”ç©¶ä¸­ï¼Œæˆ‘ä»¬è€ƒå¯Ÿäº†ä¸€ä¸ªæ›´ä¸¥æ ¼çš„è®¾ç½®ç¯å¢ƒï¼Œå³å½“ç¥ç»å…ƒè¾“å‡ºçš„é€†å‘é€šä¿¡æ— æ³•ç”¨äºçªè§¦å‰æƒé‡ä¼˜åŒ–æ—¶çš„æƒ…å†µã€‚ä¸ºäº†åº”å¯¹è¿™ä¸€æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†â€œå‰å‘æŠ•å½±â€ï¼ˆFPï¼‰ã€‚è¿™ç§æ–°é¢–çš„éšæœºé—­å¼è®­ç»ƒæ–¹æ³•ä»…åœ¨æ•´ä¸ªæ•°æ®é›†ä¸Šæ‰§è¡Œä¸€æ¬¡å‰å‘ä¼ é€’å³å¯è¿›è¡Œæ¨¡å‹æ‹Ÿåˆï¼Œæ— éœ€é€†å‘é€šä¿¡ã€‚ç›®æ ‡å€¼é€å±‚ç”Ÿæˆï¼Œé€šè¿‡çªè§¦å‰è¾“å…¥çš„éçº¿æ€§æŠ•å½±å’Œæ ‡ç­¾å®ç°ã€‚å±€éƒ¨æŸå¤±å‡½æ•°é€šè¿‡é—­å¼å›å½’åœ¨çªè§¦å‰è¾“å…¥ä¸Šè¿›è¡Œä¼˜åŒ–ï¼Œæ— éœ€ç¥ç»å…ƒè¾“å‡ºæˆ–ä¸‹æ¸¸å±‚çš„åé¦ˆã€‚å¯è§£é‡Šæ€§æ˜¯FPè®­ç»ƒçš„å…³é”®ä¼˜åŠ¿ï¼›FPè®­ç»ƒåçš„ç½‘ç»œä¸­éšè—ç¥ç»å…ƒçš„è†œç”µä½ç¼–ç çš„ä¿¡æ¯å¯ä»¥æŒ‰å±‚è§£è¯»ä¸ºæ ‡ç­¾é¢„æµ‹ã€‚æˆ‘ä»¬åœ¨å››ä¸ªç”Ÿç‰©åŒ»å­¦æ•°æ®é›†ä¸Šå±•ç¤ºäº†FPçš„æœ‰æ•ˆæ€§ã€‚åœ¨å°‘é‡æ ·æœ¬å­¦ä¹ ä»»åŠ¡ä¸­ï¼ŒFPäº§ç”Ÿçš„æ¨¡å‹æ¯”é€šè¿‡åå‘ä¼ æ’­ä¼˜åŒ–çš„æ¨¡å‹æ›´å…·æ³›åŒ–èƒ½åŠ›ã€‚åœ¨å¤§æ ·æœ¬ä»»åŠ¡ä¸­ï¼ŒFPæ¨¡å‹åŸºäºçš„æ³›åŒ–èƒ½åŠ›ä¸æ¢¯åº¦ä¸‹é™æ³•æœ¬åœ°å­¦ä¹ æ–¹æ³•ç›¸å½“ï¼Œä½†ä»…éœ€ä¸€æ¬¡å‰å‘ä¼ æ’­æ­¥éª¤ï¼Œå¤§å¤§åŠ å¿«äº†è®­ç»ƒé€Ÿåº¦ã€‚åœ¨FPæ¨¡å‹ä¸Šå®šä¹‰çš„å±€éƒ¨ç¥ç»å…ƒæ´»åŠ¨çš„è§£é‡Šå‡½æ•°æˆåŠŸåœ°åœ¨ä¸¤ä¸ªç”Ÿç‰©åŒ»å­¦æ•°æ®é›†ä¸­è¯†åˆ«å‡ºäº†ç”¨äºè¯Šæ–­çš„ä¸´åºŠé‡è¦ç‰¹å¾ã€‚å‰å‘æŠ•å½±æ˜¯ä¸€ç§è®¡ç®—æ•ˆç‡é«˜çš„æœºå™¨å­¦ä¹ æ–¹æ³•ï¼Œå¯åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­æ— éœ€ç¥ç»å…ƒæ´»åŠ¨çš„é€†å‘é€šä¿¡å³å¯äº§ç”Ÿå¯è§£é‡Šçš„ç¥ç»ç½‘ç»œæ¨¡å‹ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.16476v2">PDF</a> 26 pages, 5 figures. Study code available at   <a target="_blank" rel="noopener" href="https://github.com/robertoshea/forward_projection">https://github.com/robertoshea/forward_projection</a>. Study data available at   <a target="_blank" rel="noopener" href="https://data.mendeley.com/datasets/fb7xddyxs4/2">https://data.mendeley.com/datasets/fb7xddyxs4/2</a></p>
<p><strong>Summary</strong></p>
<p>æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§åä¸ºForward Projectionï¼ˆFPï¼‰çš„æ–°å‹éšæœºé—­å¼è®­ç»ƒæ–¹æ³•ï¼Œè¯¥æ–¹æ³•åœ¨æ— éœ€é€†å‘ç¥ç»å…ƒè¾“å‡ºé€šä¿¡çš„æƒ…å†µä¸‹ï¼Œé€šè¿‡å•æ¬¡å‰å‘ä¼ æ’­å³å¯å®Œæˆæ¨¡å‹æ‹Ÿåˆã€‚é€šè¿‡é€å±‚ç”Ÿæˆç›®æ ‡é¢„æ¿€æ´»è†œç”µä½å€¼ï¼Œå¹¶åˆ©ç”¨é—­å¼å›å½’ä¼˜åŒ–é¢„çªè§¦è¾“å…¥çš„å±€éƒ¨æŸå¤±å‡½æ•°ï¼ŒFPè®­ç»ƒæé«˜äº†æ¨¡å‹çš„è§£é‡Šæ€§å’Œé¢„æµ‹æ€§èƒ½ã€‚åœ¨å››ä¸ªç”Ÿç‰©åŒ»å­¦æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒFPåœ¨å°‘æ ·æœ¬å­¦ä¹ ä»»åŠ¡ä¸­è¡¨ç°å‡ºæ›´é«˜çš„æ³›åŒ–èƒ½åŠ›ï¼Œåœ¨å¤§æ ·æœ¬ä»»åŠ¡ä¸­ä¹Ÿèƒ½å®ç°ä¸åŸºäºæ¢¯åº¦ä¸‹é™çš„åœ°æ–¹å­¦ä¹ æ–¹æ³•ç›¸å½“çš„æ³›åŒ–æ€§èƒ½ï¼ŒåŒæ—¶åªéœ€ä¸€æ­¥å‰å‘ä¼ æ’­ï¼Œæ˜¾è‘—åŠ å¿«äº†è®­ç»ƒé€Ÿåº¦ã€‚æ­¤å¤–ï¼ŒFPæ¨¡å‹ä¸­çš„è§£é‡Šå‡½æ•°èƒ½å¤ŸæˆåŠŸè¯†åˆ«ä¸¤ä¸ªç”Ÿç‰©åŒ»å­¦æ•°æ®é›†ä¸­çš„ä¸´åºŠé‡è¦ç‰¹å¾ï¼Œä¸ºè¯Šæ–­æä¾›äº†æœ‰åŠ›æ”¯æŒã€‚æ€»ä½“è€Œè¨€ï¼ŒFPæ˜¯ä¸€ç§é«˜æ•ˆä¸”å¯è§£é‡Šçš„æœºå™¨å­¦ä¹ æ–°æ–¹æ³•ï¼Œæ— éœ€è®­ç»ƒè¿‡ç¨‹ä¸­çš„ç¥ç»å…ƒæ´»åŠ¨é€†å‘é€šä¿¡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Forward Projectionï¼ˆFPï¼‰æ˜¯ä¸€ç§æ–°å‹çš„éšæœºé—­å¼è®­ç»ƒæ–¹æ³•ï¼Œæ— éœ€é€†å‘ç¥ç»å…ƒè¾“å‡ºé€šä¿¡å³å¯å®Œæˆæ¨¡å‹æ‹Ÿåˆã€‚</li>
<li>FPé€šè¿‡é€å±‚ç”Ÿæˆç›®æ ‡é¢„æ¿€æ´»è†œç”µä½å€¼å¹¶åˆ©ç”¨é—­å¼å›å½’ä¼˜åŒ–é¢„çªè§¦è¾“å…¥çš„å±€éƒ¨æŸå¤±å‡½æ•°ï¼Œæé«˜æ¨¡å‹çš„è§£é‡Šæ€§å’Œé¢„æµ‹æ€§èƒ½ã€‚</li>
<li>åœ¨å°‘æ ·æœ¬å­¦ä¹ ä»»åŠ¡ä¸­ï¼ŒFPè¡¨ç°å‡ºæ›´é«˜çš„æ³›åŒ–èƒ½åŠ›ã€‚</li>
<li>åœ¨å¤§æ ·æœ¬ä»»åŠ¡ä¸­ï¼ŒFPæ¨¡å‹çš„æ³›åŒ–æ€§èƒ½ä¸åŸºäºæ¢¯åº¦ä¸‹é™çš„åœ°æ–¹å­¦ä¹ æ–¹æ³•ç›¸å½“ï¼Œä½†åªéœ€ä¸€æ­¥å‰å‘ä¼ æ’­ï¼ŒåŠ å¿«äº†è®­ç»ƒé€Ÿåº¦ã€‚</li>
<li>FPæ¨¡å‹ä¸­çš„è§£é‡Šå‡½æ•°èƒ½å¤Ÿè¯†åˆ«ä¸´åºŠé‡è¦ç‰¹å¾ï¼Œä¸ºè¯Šæ–­æä¾›æ”¯æŒã€‚</li>
<li>FPè®­ç»ƒæé«˜äº†æ¨¡å‹çš„è§£é‡Šæ€§ï¼Œä½¿å¾—è†œç”µä½ä¿¡æ¯å¯é€å±‚è§£é‡Šä¸ºæ ‡ç­¾é¢„æµ‹ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.16476">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-b0f7e634e46a4f7a8bdc8bd1e08eb1ca.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-05d5032080320071e0b0967bd816ef00.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c9879518d9b618f618dd5eb3be9c68e9.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-08-21/Few-Shot/">https://kedreamix.github.io/Talk2Paper/Paper/2025-08-21/Few-Shot/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Few-Shot/">
                                    <span class="chip bg-color">Few-Shot</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-08-21/Vision%20Transformer/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-fe4aa76d1eabdf424fb19ba1f6bc65b6.jpg" class="responsive-img" alt="Vision Transformer">
                        
                        <span class="card-title">Vision Transformer</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Vision Transformer æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-08-21  ViT-FIQA Assessing Face Image Quality using Vision Transformers
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-08-21
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Vision-Transformer/" class="post-category">
                                    Vision Transformer
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Vision-Transformer/">
                        <span class="chip bg-color">Vision Transformer</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-08-21/Agent/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-09044d5bf6f2fd548f1712059536c58c.jpg" class="responsive-img" alt="Agent">
                        
                        <span class="card-title">Agent</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Agent æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-08-21  ComputerRL Scaling End-to-End Online Reinforcement Learning for   Computer Use Agents
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-08-21
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Agent/" class="post-category">
                                    Agent
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Agent/">
                        <span class="chip bg-color">Agent</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">32271.8k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
