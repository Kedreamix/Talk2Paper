<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Few-Shot">
    <meta name="description" content="Few-Shot 方向最新论文已更新，请持续关注 Update in 2025-08-21  DictAS A Framework for Class-Generalizable Few-Shot Anomaly   Segmentation via Dictionary Lookup">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Few-Shot | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loader页面消失采用渐隐的方式*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logo出现动画 */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//使用渐隐的方法淡出loading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//强制显示loading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">嘘~  正在从服务器偷取页面 . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>首页</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>标签</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>分类</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>归档</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="搜索" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="深色/浅色模式" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			首页
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			标签
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			分类
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			归档
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-bd8c4a3b464e51d4bf1a247a25254577.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Few-Shot</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- 文章内容详情 -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Few-Shot/">
                                <span class="chip bg-color">Few-Shot</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Few-Shot/" class="post-category">
                                Few-Shot
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp;
                    2025-08-21
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>更新日期:&nbsp;&nbsp;
                    2025-09-08
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>文章字数:&nbsp;&nbsp;
                    12.7k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>阅读时长:&nbsp;&nbsp;
                    51 分
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>阅读次数:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>⚠️ 以下所有内容总结都来自于 大语言模型的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p>
</blockquote>
<h1 id="2025-08-21-更新"><a href="#2025-08-21-更新" class="headerlink" title="2025-08-21 更新"></a>2025-08-21 更新</h1><h2 id="DictAS-A-Framework-for-Class-Generalizable-Few-Shot-Anomaly-Segmentation-via-Dictionary-Lookup"><a href="#DictAS-A-Framework-for-Class-Generalizable-Few-Shot-Anomaly-Segmentation-via-Dictionary-Lookup" class="headerlink" title="DictAS: A Framework for Class-Generalizable Few-Shot Anomaly   Segmentation via Dictionary Lookup"></a>DictAS: A Framework for Class-Generalizable Few-Shot Anomaly   Segmentation via Dictionary Lookup</h2><p><strong>Authors:Zhen Qu, Xian Tao, Xinyi Gong, ShiChen Qu, Xiaopei Zhang, Xingang Wang, Fei Shen, Zhengtao Zhang, Mukesh Prasad, Guiguang Ding</strong></p>
<p>Recent vision-language models (e.g., CLIP) have demonstrated remarkable class-generalizable ability to unseen classes in few-shot anomaly segmentation (FSAS), leveraging supervised prompt learning or fine-tuning on seen classes. However, their cross-category generalization largely depends on prior knowledge of real seen anomaly samples. In this paper, we propose a novel framework, namely DictAS, which enables a unified model to detect visual anomalies in unseen object categories without any retraining on the target data, only employing a few normal reference images as visual prompts. The insight behind DictAS is to transfer dictionary lookup capabilities to the FSAS task for unseen classes via self-supervised learning, instead of merely memorizing the normal and abnormal feature patterns from the training set. Specifically, DictAS mainly consists of three components: (1) <strong>Dictionary Construction</strong> - to simulate the index and content of a real dictionary using features from normal reference images. (2) <strong>Dictionary Lookup</strong> - to retrieve queried region features from the dictionary via a sparse lookup strategy. When a query feature cannot be retrieved, it is classified as an anomaly. (3) <strong>Query Discrimination Regularization</strong>- to enhance anomaly discrimination by making abnormal features harder to retrieve from the dictionary. To achieve this, Contrastive Query Constraint and Text Alignment Constraint are further proposed. Extensive experiments on seven public industrial and medical datasets demonstrate that DictAS consistently outperforms state-of-the-art FSAS methods. </p>
<blockquote>
<p>最近的视觉语言模型（例如CLIP）在少量异常分割（FSAS）中显示出对未见类别的类通用能力，这得益于对可见类别的有监督提示学习或微调。然而，它们的跨类别泛化在很大程度上依赖于真实可见异常样本的先验知识。在本文中，我们提出了一种新型框架，即DictAS，它能够在未见对象类别中检测视觉异常，而无需对目标数据进行任何重新训练，仅使用少数正常参考图像作为视觉提示。DictAS的见解是通过自我监督学习将字典查找能力转移到未见类别的FSAS任务上，而不是仅仅从训练集中记忆正常和异常特征模式。具体来说，DictAS主要由三个组件组成：（1）<strong>字典构建</strong>——使用正常参考图像的特征模拟真实字典的索引和内容。（2）<strong>字典查找</strong>——通过稀疏查找策略从字典中检索查询区域特征。当无法检索查询特征时，它会被归类为异常。（3）<strong>查询判别正则化</strong>——通过使异常特征更难从字典中检索来提高异常判别力。为此，进一步提出了对比查询约束和文本对齐约束。在七个公共工业和医疗数据集上的大量实验表明，DictAS始终优于最新的FSAS方法。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.13560v1">PDF</a> Accepted by ICCV 2025, Project: <a target="_blank" rel="noopener" href="https://github.com/xiaozhen228/DictAS">https://github.com/xiaozhen228/DictAS</a></p>
<p><strong>Summary</strong></p>
<p>该论文提出了一种名为DictAS的新框架，用于在未训练的目标数据上检测未知对象类别中的视觉异常。它主要通过利用少数正常参考图像作为视觉提示来实现这一目标。DictAS的核心理念是通过自我监督学习将字典查找能力转移到少数拍摄异常分割任务上，而不是仅仅依靠训练集来记忆正常和异常的特征模式。经过大量实验证明，DictAS在多个公共工业数据集上的表现始终优于现有的少数拍摄异常分割方法。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>DictAS框架能在未重新训练目标数据的情况下，利用少数正常参考图像检测未知对象类别中的视觉异常。</li>
<li>DictAS通过模拟真实字典的索引和内容，使用正常参考图像的特征进行字典构建。</li>
<li>通过稀疏查找策略从字典中检索查询区域特征，无法检索到的特征被分类为异常。</li>
<li>Query Discrimination Regularization模块通过使异常特征更难从字典中检索出来，提高了异常识别的能力。</li>
<li>该框架引入了Contrastive Query Constraint和Text Alignment Constraint，进一步提高了模型的性能。</li>
<li>在七个公共工业数据集和医疗数据集上的实验表明，DictAS在少数拍摄异常分割任务上的表现优于现有方法。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.13560">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-b8865a311ace7179c21006b1ae0ea7b4.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-5a2de08c504c3ec1eead8e0a557e7127.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-12cc42d6005eb574997280b4afb63b5a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-2a379e50efa4f973d4e20403d7a00f4d.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="MAGNeT-Multimodal-Adaptive-Gaussian-Networks-for-Intent-Inference-in-Moving-Target-Selection-across-Complex-Scenarios"><a href="#MAGNeT-Multimodal-Adaptive-Gaussian-Networks-for-Intent-Inference-in-Moving-Target-Selection-across-Complex-Scenarios" class="headerlink" title="MAGNeT: Multimodal Adaptive Gaussian Networks for Intent Inference in   Moving Target Selection across Complex Scenarios"></a>MAGNeT: Multimodal Adaptive Gaussian Networks for Intent Inference in   Moving Target Selection across Complex Scenarios</h2><p><strong>Authors:Xiangxian Li, Yawen Zheng, Baiqiao Zhang, Yijia Ma, Xianhui Cao, Juan Liu, Yulong Bian, Jin Huang, Chenglei Yang</strong></p>
<p>Moving target selection in multimedia interactive systems faces unprecedented challenges as users increasingly interact across diverse and dynamic contexts-from live streaming in moving vehicles to VR gaming in varying environments. Existing approaches rely on probabilistic models that relate endpoint distribution to target properties such as size and speed. However, these methods require substantial training data for each new context and lack transferability across scenarios, limiting their practical deployment in diverse multimedia environments where rich multimodal contextual information is readily available. This paper introduces MAGNeT (Multimodal Adaptive Gaussian Networks), which addresses these problems by combining classical statistical modeling with a context-aware multimodal method. MAGNeT dynamically fuses pre-fitted Ternary-Gaussian models from various scenarios based on real-time contextual cues, enabling effective adaptation with minimal training data while preserving model interpretability. We conduct experiments on self-constructed 2D and 3D moving target selection datasets under in-vehicle vibration conditions. Extensive experiments demonstrate that MAGNeT achieves lower error rates with few-shot samples by applying context-aware fusion of Gaussian experts from multi-factor conditions. </p>
<blockquote>
<p>多媒体交互系统中的动态目标选择面临着前所未有的挑战，因为用户在不同的动态上下文中的交互日益增多，例如在移动车辆中进行直播或在不同的环境中进行VR游戏。现有方法依赖于概率模型，该模型将终点分布与目标的属性（如大小和速度）关联起来。然而，这些方法对于每种新上下文都需要大量的训练数据，并且在不同场景之间缺乏可迁移性，限制了它们在丰富的多媒体环境中进行实际部署的能力，在这些环境中，丰富的多模态上下文信息很容易获得。本文介绍了MAGNeT（多模态自适应高斯网络），它通过结合经典统计建模和上下文感知的多模态方法来解决这些问题。MAGNeT根据实时的上下文线索动态融合来自各种场景的预先拟合的三元高斯模型，以最小的训练数据实现有效的适应，同时保持模型的可解释性。我们在自行构建的二维和三维移动目标选择数据集上进行了车内振动条件下的实验。大量实验表明，通过多因素条件下的高斯专家的上下文感知融合，MAGNeT在少数样本情况下实现了更低的错误率。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.12992v2">PDF</a> Accepted by ACM MM 2025</p>
<p><strong>Summary</strong><br>多媒体交互系统中移动目标选择面临前所未有的挑战，随着用户在不同动态上下文中的交互日益增多，如移动车辆中的直播到不同环境中的VR游戏。现有方法依赖于与目标属性（如大小和速度）相关的终端分布的概率模型。然而，这些方法需要为每个新上下文进行大量训练数据，并且缺乏跨场景的迁移能力，限制了它们在多媒体环境中的实际部署，那里拥有丰富的多模态上下文信息。本文介绍了MAGNeT（多模态自适应高斯网络），它通过结合经典统计建模和上下文感知的多模态方法来解决这些问题。MAGNeT根据实时上下文线索动态融合来自各种场景的预先拟合的三元高斯模型，可在少量训练数据的情况下实现有效适应，同时保持模型的可解释性。我们在自行构建的二维和三维移动目标选择数据集上进行了实验，实验表明，MAGNeT通过应用上下文感知的高斯专家融合，在少数样本的情况下实现了较低的错误率。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>多媒体交互系统中移动目标选择面临挑战，尤其是在不同动态上下文中的交互。</li>
<li>现有方法依赖概率模型，需要为每种新上下文进行大量训练，且缺乏跨场景迁移能力。</li>
<li>MAGNeT（多模态自适应高斯网络）结合经典统计建模和上下文感知的多模态方法来解决这些问题。</li>
<li>MAGNeT根据实时上下文线索动态融合预拟合模型，实现有效适应并保留模型的可解释性。</li>
<li>MAGNeT在移动目标选择方面表现出优异性能，特别是在少量训练数据的情况下。</li>
<li>实验证明MAGNeT在多种移动目标选择数据集上实现了较低的错误率。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.12992">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-79683ed8dec29f7d0eb8a68d7da25899.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-480c5c368a498f6117db7d7ee05573dc.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-776c785c045799ea8cf882f11466556d.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-7ddd83def8b8af37b56935c69766ff23.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-deba86eab4f3a5091d01aca9bf1113eb.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-099e3f2ea114f18b2df3b57e3e684988.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="DCSCR-A-Class-Specific-Collaborative-Representation-based-Network-for-Image-Set-Classification"><a href="#DCSCR-A-Class-Specific-Collaborative-Representation-based-Network-for-Image-Set-Classification" class="headerlink" title="DCSCR: A Class-Specific Collaborative Representation based Network for   Image Set Classification"></a>DCSCR: A Class-Specific Collaborative Representation based Network for   Image Set Classification</h2><p><strong>Authors:Xizhan Gao, Wei Hu</strong></p>
<p>Image set classification (ISC), which can be viewed as a task of comparing similarities between sets consisting of unordered heterogeneous images with variable quantities and qualities, has attracted growing research attention in recent years. How to learn effective feature representations and how to explore the similarities between different image sets are two key yet challenging issues in this field. However, existing traditional ISC methods classify image sets based on raw pixel features, ignoring the importance of feature learning. Existing deep ISC methods can learn deep features, but they fail to adaptively adjust the features when measuring set distances, resulting in limited performance in few-shot ISC. To address the above issues, this paper combines traditional ISC methods with deep models and proposes a novel few-shot ISC approach called Deep Class-specific Collaborative Representation (DCSCR) network to simultaneously learn the frame- and concept-level feature representations of each image set and the distance similarities between different sets. Specifically, DCSCR consists of a fully convolutional deep feature extractor module, a global feature learning module, and a class-specific collaborative representation-based metric learning module. The deep feature extractor and global feature learning modules are used to learn (local and global) frame-level feature representations, while the class-specific collaborative representation-based metric learning module is exploit to adaptively learn the concept-level feature representation of each image set and thus obtain the distance similarities between different sets by developing a new CSCR-based contrastive loss function. Extensive experiments on several well-known few-shot ISC datasets demonstrate the effectiveness of the proposed method compared with some state-of-the-art image set classification algorithms. </p>
<blockquote>
<p>图像集分类（ISC）是一个比较无序的异质图像集之间相似性的任务，这些图像集具有可变的数量和品质，近年来引起了越来越多的研究关注。如何学习有效的特征表示以及如何探索不同图像集之间的相似性是该领域的两个关键且具有挑战性的问题。然而，现有的传统ISC方法基于原始像素特征对图像集进行分类，忽略了特征学习的重要性。现有的深度ISC方法可以学习深度特征，但在测量集合距离时无法自适应调整特征，导致在少样本ISC中的性能有限。为了解决上述问题，本文结合了传统ISC方法和深度模型，提出了一种新的少样本ISC方法，称为深度类特定协同表示（DCSCR）网络，可以同时学习每个图像集的框架和概念级别的特征表示以及不同集合之间的距离相似性。具体来说，DCSCR包括全卷积深度特征提取器模块、全局特征学习模块和基于类特定协同表示的度量学习模块。深度特征提取器和全局特征学习模块用于学习（局部和全局）框架级别的特征表示，而基于类特定的协同表示度量学习模块则用于自适应地学习每个图像集的概念级特征表示，并通过开发一种新的基于CSCR的对比损失函数来获得不同集合之间的距离相似性。在几个著名的少样本ISC数据集上的广泛实验表明，与一些最先进的图像集分类算法相比，所提出的方法具有有效性。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.12745v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文关注图像集分类（ISC）领域，针对如何学习有效的特征表示以及如何探索不同图像集之间的相似性两个关键问题，提出一种结合传统ISC方法和深度模型的新型少样本ISC方法——Deep Class-specific Collaborative Representation（DCSCR）网络。该网络能同时学习每个图像集的框架和概念级别的特征表示，以及不同集之间的距离相似性。通过一系列实验验证，该方法在几个知名的少样本ISC数据集上表现出优异的效果。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>图像集分类（ISC）是近年来的研究热点，涉及如何学习有效的特征表示和探索不同图像集之间的相似性。</li>
<li>传统ISC方法基于原始像素特征进行分类，忽略了特征学习的重要性。</li>
<li>现有深度ISC方法虽然能学习深度特征，但在测量集间距离时无法自适应调整特征，导致在少样本ISC中的性能受限。</li>
<li>DCSCR网络结合了传统ISC方法和深度模型，能同时学习每个图像集的框架和概念级别的特征表示。</li>
<li>DCSCR网络包含全卷积深度特征提取模块、全局特征学习模块和基于类特定的协同表示度量学习模块。</li>
<li>通过开发新的CSCR对比损失函数，DCSCR网络能自适应地学习每个图像集的概念级别特征表示，并获取不同集之间的距离相似性。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.12745">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-c505ac92db2a7765d1e8890fad5c0043.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a1089efa9e95ba07a0f25dd617bfa6d3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7ceee75f854337c79f6801c1e81daba1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8c637cccdbfeb99cbdb68867cea9c71c.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="LLM-RIMSA-Large-Language-Models-driven-Reconfigurable-Intelligent-Metasurface-Antenna-Systems"><a href="#LLM-RIMSA-Large-Language-Models-driven-Reconfigurable-Intelligent-Metasurface-Antenna-Systems" class="headerlink" title="LLM-RIMSA: Large Language Models driven Reconfigurable Intelligent   Metasurface Antenna Systems"></a>LLM-RIMSA: Large Language Models driven Reconfigurable Intelligent   Metasurface Antenna Systems</h2><p><strong>Authors:Yunsong Huang, Hui-Ming Wang, Qingli Yan, Zhaowei Wang</strong></p>
<p>The evolution of 6G networks demands ultra-massive connectivity and intelligent radio environments, yet existing reconfigurable intelligent surface (RIS) technologies face critical limitations in hardware efficiency, dynamic control, and scalability. This paper introduces LLM-RIMSA, a transformative framework that integrates large language models (LLMs) with a novel reconfigurable intelligent metasurface antenna (RIMSA) architecture to address these challenges. Unlike conventional RIS designs, RIMSA employs parallel coaxial feeding and 2D metasurface integration, enabling each individual metamaterial element to independently adjust both its amplitude and phase. While traditional optimization and deep learning (DL) methods struggle with high-dimensional state spaces and prohibitive training costs for RIMSA control, LLM-RIMSA leverages pre-trained LLMs cross-modal reasoning and few-shot learning capabilities to dynamically optimize RIMSA configurations. Simulations demonstrate that LLM-RIMSA achieves state-of-the-art performance, outperforming conventional DL-based methods in sum rate while reducing training overhead. The proposed framework pave the way for LLM-driven intelligent radio environments. </p>
<blockquote>
<p>6G网络的演进需要超大规模连接和智能无线电环境，然而现有的可重构智能表面（RIS）技术在硬件效率、动态控制和可扩展性方面面临关键挑战。本文介绍了LLM-RIMSA，这是一个创新性框架，它将大型语言模型（LLM）与新型的可重构智能超表面天线（RIMSA）架构相结合，以应对这些挑战。与传统的RIS设计不同，RIMSA采用并行同轴馈电和2D超表面集成，使每个单独的元材料元素都能独立调整其振幅和相位。传统的优化和深度学习（DL）方法在高维状态空间和RIMSA控制的昂贵训练成本方面遇到了困难，而LLM-RIMSA利用预训练的大型语言模型的跨模态推理和少量学习功能来动态优化RIMSA配置。模拟结果表明，LLM-RIMSA实现了最先进的性能，在总和速率上优于传统的基于DL的方法，同时降低了训练开销。所提出的框架为大型语言模型驱动的智能无线电环境铺平了道路。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.12728v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>LLM-RIMSA框架结合了大型语言模型（LLMs）和可重构智能超表面天线（RIMSA）架构，解决了现有可重构智能表面（RIS）技术在硬件效率、动态控制和可扩展性方面的局限。该框架采用并行同轴馈电和2D超表面集成，利用LLMs的跨模态推理和少量学习功能，动态优化RIMSA配置，实现超高连通性和智能无线电环境。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>现有RIS技术面临硬件效率、动态控制和可扩展性的挑战。</li>
<li>LLM-RIMSA框架结合了LLMs和RIMSA架构以应对这些挑战。</li>
<li>RIMSA采用并行同轴馈电和2D超表面集成，使每个超材料元素能独立调整幅度和相位。</li>
<li>传统优化和深度学习方法在RIMSA控制上遇到高维状态空间和训练成本高昂的问题。</li>
<li>LLM-RIMSA利用LLMs的跨模态推理和少量学习功能进行动态优化。</li>
<li>仿真显示LLM-RIMSA在总速率上实现了最先进的性能，优于传统的DL方法。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.12728">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-41a5d6c2f73e382d6688b750dd040227.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-0023e1a8e73fde998f0cf15046a4f91d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0bad963549c151fdda48ab25f00bf1a0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e509d707f68a24ed369204b5d9dc08a4.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Leveraging-Large-Language-Models-for-Predictive-Analysis-of-Human-Misery"><a href="#Leveraging-Large-Language-Models-for-Predictive-Analysis-of-Human-Misery" class="headerlink" title="Leveraging Large Language Models for Predictive Analysis of Human Misery"></a>Leveraging Large Language Models for Predictive Analysis of Human Misery</h2><p><strong>Authors:Bishanka Seal, Rahul Seetharaman, Aman Bansal, Abhilash Nandy</strong></p>
<p>This study investigates the use of Large Language Models (LLMs) for predicting human-perceived misery scores from natural language descriptions of real-world scenarios. The task is framed as a regression problem, where the model assigns a scalar value from 0 to 100 to each input statement. We evaluate multiple prompting strategies, including zero-shot, fixed-context few-shot, and retrieval-based prompting using BERT sentence embeddings. Few-shot approaches consistently outperform zero-shot baselines, underscoring the value of contextual examples in affective prediction. To move beyond static evaluation, we introduce the “Misery Game Show”, a novel gamified framework inspired by a television format. It tests LLMs through structured rounds involving ordinal comparison, binary classification, scalar estimation, and feedback-driven reasoning. This setup enables us to assess not only predictive accuracy but also the model’s ability to adapt based on corrective feedback. The gamified evaluation highlights the broader potential of LLMs in dynamic emotional reasoning tasks beyond standard regression. Code and data link: <a target="_blank" rel="noopener" href="https://github.com/abhi1nandy2/Misery_Data_Exps_GitHub">https://github.com/abhi1nandy2/Misery_Data_Exps_GitHub</a> </p>
<blockquote>
<p>本研究探讨了大型语言模型（LLM）在根据真实世界场景的自然语言描述预测人类感知的痛苦分数方面的应用。该任务被构建为一个回归问题，模型为每条输入语句分配一个从0到100的标量值。我们评估了多种提示策略，包括零样本、固定上下文小样例和基于BERT句子嵌入的检索提示。小样例方法始终优于零样本基线，这凸显了上下文示例在情感预测中的价值。为了超越静态评估，我们推出了“苦难游戏秀”，这是一个受电视节目格式启发的新型游戏化框架。它通过涉及序数比较、二元分类、标量估计和反馈驱动推理的结构化回合来测试LLM。这种设置使我们不仅能够评估预测准确性，还能够评估模型根据纠正性反馈进行适应的能力。游戏化评估突显了LLM在超越标准回归的动态情绪推理任务中的更广泛潜力。代码和数据链接：<a target="_blank" rel="noopener" href="https://github.com/abhi1nandy2/Misery_Data_Exps_GitHub">https://github.com/abhi1nandy2/Misery_Data_Exps_GitHub</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.12669v1">PDF</a> 14 pages, 4 tables</p>
<p><strong>Summary</strong></p>
<p>大型语言模型（LLM）用于预测基于现实世界情境的自然语言描述的人类感知痛苦程度。研究采用回归问题框架，模型为每个输入语句分配一个从0到100的标量值。评估了多种提示策略，包括零样本、固定上下文少样本和基于BERT句子嵌入的检索提示。少样本方法始终优于零样本基线，突显了上下文实例在情感预测中的价值。为超越静态评估，研究引入了“痛苦游戏秀”这一新型游戏化框架，该框架以电视节目形式为灵感。通过有序比较、二元分类、标量估算和反馈驱动推理的结构性回合测试LLM，不仅评估其预测准确性，还评估其基于纠正反馈的适应能力。游戏化评估突显了LLM在动态情感推理任务中的更广泛潜力。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>大型语言模型（LLM）可用于预测人类感知的痛苦程度，采用回归问题框架。</li>
<li>少样本方法在进行情感预测时表现优于零样本基线。</li>
<li>上下文实例在情感预测任务中具有重要作用。</li>
<li>引入了一种新型游戏化框架——“痛苦游戏秀”，用于评估LLM的动态情感推理能力。</li>
<li>该框架允许测试LLM的预测准确性以及基于纠正反馈的适应能力。</li>
<li>游戏化评估方法扩大了LLM在情感计算领域的应用潜力。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.12669">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-7d3724a224f8d1f2752ebdfadce8a5e8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5eb76527a456d3f876ed1e34c73f3a59.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Non-Iterative-Symbolic-Aided-Chain-of-Thought-for-Logical-Reasoning"><a href="#Non-Iterative-Symbolic-Aided-Chain-of-Thought-for-Logical-Reasoning" class="headerlink" title="Non-Iterative Symbolic-Aided Chain-of-Thought for Logical Reasoning"></a>Non-Iterative Symbolic-Aided Chain-of-Thought for Logical Reasoning</h2><p><strong>Authors:Phuong Minh Nguyen, Tien Huu Dang, Naoya Inoue</strong></p>
<p>This work introduces Symbolic-Aided Chain-of-Thought (CoT), an improved approach to standard CoT, for logical reasoning in large language models (LLMs). The key idea is to integrate lightweight symbolic representations into few-shot prompts, structuring the inference steps with a consistent strategy to make reasoning patterns more explicit within a non-iterative reasoning process. By incorporating these symbolic structures, our method preserves the generalizability of standard prompting techniques while enhancing the transparency, interpretability, and analyzability of LLM logical reasoning. Extensive experiments on four well-known logical reasoning benchmarks – ProofWriter, FOLIO, ProntoQA, and LogicalDeduction, which cover diverse reasoning scenarios – demonstrate the effectiveness of the proposed approach, particularly in complex reasoning tasks that require navigating multiple constraints or rules. Notably, Symbolic-Aided CoT consistently improves LLMs’ reasoning capabilities across various model sizes and significantly outperforms conventional CoT on three out of four datasets, ProofWriter, ProntoQA, and LogicalDeduction. </p>
<blockquote>
<p>本文介绍了符号辅助思维链（CoT）方法，这是一种对标准思维链的改进方法，用于大型语言模型（LLM）的逻辑推理。其核心思想是将轻量级符号表示集成到少量提示中，使用一致的策略来结构化推理步骤，在一个非迭代推理过程中使推理模式更加明确。通过融入这些符号结构，我们的方法既保留了标准提示技术的通用性，又提高了LLM逻辑推理的透明度、可解释性和可分析性。在涵盖多种推理场景的四个知名逻辑推理基准测试——ProofWriter、FOLIO、ProntoQA和LogicalDeduction上的大量实验证明了所提出方法的有效性，特别是在需要应对多重约束或规则的复杂推理任务中。值得注意的是，符号辅助思维链方法在各种模型大小下都能持续提高LLM的推理能力，并在四个数据集中的三个（ProofWriter、ProntoQA和LogicalDeduction）上显著优于传统思维链方法。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.12425v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>符号辅助思维链（Symbolic-Aided Chain-of-Thought，简称Symbolic-Aided CoT）是一种改进的思维链方法，用于提高大型语言模型（LLM）的逻辑推理能力。它通过整合轻量级符号表示进入少量提示，将推理步骤结构化，并采用一致策略，使非迭代推理过程中的推理模式更加明确。这种方法提高了LLM的逻辑推理的通用性、透明度、解释性和分析性。在涵盖多种推理场景的四个知名逻辑推理基准测试上进行的广泛实验表明，该方法在复杂推理任务中表现优异，特别是在涉及多个约束或规则的任务中表现尤为出色。该方法在各种模型尺寸上均提高了LLM的推理能力，并在三个基准测试上显著优于传统思维链方法。</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>Symbolic-Aided CoT是思维链（CoT）的一种改进方法，用于增强大型语言模型（LLM）的逻辑推理能力。</li>
<li>该方法通过整合轻量级符号表示进入少量提示，提高推理的透明度、解释性和分析性。</li>
<li>实验表明，Symbolic-Aided CoT在复杂推理任务上表现优异，特别是在涉及多个约束或规则的任务中。</li>
<li>该方法在各种模型尺寸上都提高了LLM的推理能力。</li>
<li>与传统思维链方法相比，Symbolic-Aided CoT在三个基准测试上表现更佳。</li>
<li>这种方法保留了标准提示技术的通用性。</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.12425">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-a61175ccf3c29757a7c230adc1c4ee90.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-565a2f3cf199c6e909864cdeadaf57b5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-62211285c0f33f0f2f2df7a2c2e8c463.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-984a24c27dde6ee82dd5c6b55db4db1d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-bd8c4a3b464e51d4bf1a247a25254577.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="CC-Time-Cross-Model-and-Cross-Modality-Time-Series-Forecasting"><a href="#CC-Time-Cross-Model-and-Cross-Modality-Time-Series-Forecasting" class="headerlink" title="CC-Time: Cross-Model and Cross-Modality Time Series Forecasting"></a>CC-Time: Cross-Model and Cross-Modality Time Series Forecasting</h2><p><strong>Authors:Peng Chen, Yihang Wang, Yang Shu, Yunyao Cheng, Kai Zhao, Zhongwen Rao, Lujia Pan, Bin Yang, Chenjuan Guo</strong></p>
<p>With the success of pre-trained language models (PLMs) in various application fields beyond natural language processing, language models have raised emerging attention in the field of time series forecasting (TSF) and have shown great prospects. However, current PLM-based TSF methods still fail to achieve satisfactory prediction accuracy matching the strong sequential modeling power of language models. To address this issue, we propose Cross-Model and Cross-Modality Learning with PLMs for time series forecasting (CC-Time). We explore the potential of PLMs for time series forecasting from two aspects: 1) what time series features could be modeled by PLMs, and 2) whether relying solely on PLMs is sufficient for building time series models. In the first aspect, CC-Time incorporates cross-modality learning to model temporal dependency and channel correlations in the language model from both time series sequences and their corresponding text descriptions. In the second aspect, CC-Time further proposes the cross-model fusion block to adaptively integrate knowledge from the PLMs and time series model to form a more comprehensive modeling of time series patterns. Extensive experiments on nine real-world datasets demonstrate that CC-Time achieves state-of-the-art prediction accuracy in both full-data training and few-shot learning situations. </p>
<blockquote>
<p>随着预训练语言模型（PLMs）在自然语言处理以外的各种应用领域的成功，语言模型在时间序列预测（TSF）领域引起了广泛的关注，并显示出巨大的潜力。然而，当前的基于PLM的时间序列预测方法仍然无法实现对时间序列模式进行强大建模的满意预测精度。为了解决这一问题，我们提出了基于PLM的时间序列预测交叉模型和多模态学习（CC-Time）。我们从两个方面探索PLM在时间序列预测中的潜力：1）PLM能够建模的时间序列特征是什么；以及是否仅靠PLM就能充分建立时间序列模型。在第一个方面，CC-Time结合多模态学习来建模语言模型中的时序依赖性和通道相关性，这来源于时间序列序列及其相应的文本描述。在第二个方面，CC-Time进一步提出了跨模型融合模块，以自适应地融合来自PLM和时序模型的知识，以实现对时间序列模式的更全面的建模。在九个真实数据集上的大量实验表明，CC-Time在全数据训练和少样本学习的情况下均达到了最先进的预测精度。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.12235v1">PDF</a> </p>
<p><strong>Summary</strong><br>     基于预训练语言模型（PLMs）在多个领域应用上的成功，其在时间序列预测（TSF）领域也展现出巨大潜力。为解决当前PLM在TSF预测精度上的不足，本文提出了跨模型与跨模态学习方法CC-Time，旨在提升语言模型在时序数据预测方面的能力。CC-Time从两个方面探索了语言模型在时序数据预测中的潜力，包括时序特征建模和单纯依赖语言模型构建时序模型的充分性。通过引入跨模态学习，CC-Time可以模拟语言模型中时间序列序列和时间序列描述之间的时间依赖关系和通道相关性。同时，通过跨模型融合技术自适应整合语言模型和时序模型的知识，构建更全面和精准的时序模式模型。实验表明，CC-Time在多个真实数据集上取得了领先的预测精度，无论是在全数据训练还是少样本学习情况下均表现优异。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>预训练语言模型（PLMs）在多个领域应用上的成功引起了对时间序列预测（TSF）领域的关注。</li>
<li>当前PLM在TSF预测精度上仍有不足，需要新的方法提升其在时序数据预测方面的能力。</li>
<li>CC-Time从时序特征建模和单纯依赖语言模型构建时序模型的充分性两个方面探索了语言模型在TSF中的潜力。</li>
<li>CC-Time引入跨模态学习来模拟语言模型中时间序列序列和时间序列描述之间的关系。</li>
<li>CC-Time通过跨模型融合技术整合语言模型和时序模型的知识，构建更全面和精准的时序模式模型。</li>
<li>实验表明，CC-Time在多个真实数据集上取得了领先的预测精度。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.12235">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-06f201bf93a018724b71c5ef6d28ef9e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3266066a560398864fdb4e08a8f59619.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-fee663e1083414b9a3765acf36766cef.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-dfbbfdc89027582bc4d94ccedfa3c8e8.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="RT-Cache-Training-Free-Retrieval-for-Real-Time-Manipulation"><a href="#RT-Cache-Training-Free-Retrieval-for-Real-Time-Manipulation" class="headerlink" title="RT-Cache: Training-Free Retrieval for Real-Time Manipulation"></a>RT-Cache: Training-Free Retrieval for Real-Time Manipulation</h2><p><strong>Authors:Owen Kwon, Abraham George, Alison Bartsch, Amir Barati Farimani</strong></p>
<p>Real robots are expected to repeat the same behavior in new environments with very little new data, yet modern controllers either incur heavy per-step inference or require deployment-time fine-tuning. We propose RT-Cache, a training-free retrieval-as-control pipeline that caches diverse image action trajectories in a unified vector memory and, at test time, embeds the current frame to retrieve and replay multi-step snippets, replacing per-step model calls. A hierarchical search keeps lookups sub-second at million scale, shifting cost from compute to storage and enabling real-time control on modest GPUs. Across real-robot tasks and large open logs, RT-Cache achieves higher success and lower completion time than strong retrieval baselines (approximately x2 higher success and ~30% faster in our settings), and a single-episode anchoring study shows immediate adaptation to a more complex, contact-rich task without fine-tuning. RT-Cache turns experience into an append-only memory, offering a simple, scalable path to few-shot deployment today and a foundation for multimodal keys and optional integration with high-level policies. Project page: <a target="_blank" rel="noopener" href="https://rt-cache.github.io/">https://rt-cache.github.io/</a>. </p>
<blockquote>
<p>现代机器人被期望在新的环境中使用很少的新数据就能重复执行相同的行为，然而现有的控制器要么在每一步推理时产生巨大的开销，要么需要在部署时进行微调。我们提出了RT-Cache，这是一种无需训练的回放控制管道，它可以在统一的向量内存中缓存各种图像行为轨迹。在测试时，它将当前帧嵌入其中，以检索和回放多步片段，从而替代每一步的模型调用。分层搜索使查找时间保持在子秒级别，即使在百万级别规模上也是如此，从而将成本从计算转移到存储，并在适度的GPU上实现实时控制。在真实机器人任务和大型开放日志中，RT-Cache的成功率高于强大的检索基线，完成时间更短（在我们的设置中，大约成功率提高两倍，速度提高约30%），一项单集锚定研究表明，它能够立即适应更复杂、接触丰富的任务而无需微调。RT-Cache将经验转化为一种附加式内存，为当今的少量部署提供了一条简单、可扩展的路径，并为多模式键和与高级政策的可选集成奠定了基础。项目页面：<a target="_blank" rel="noopener" href="https://rt-cache.github.io/%E3%80%82">https://rt-cache.github.io/。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.09040v2">PDF</a> 8 pages, 6 figures. Accepted to the 2025 IEEE-RAS 24th International   Conference on Humanoid Robots</p>
<p><strong>Summary</strong><br>     该文本介绍了RT-Cache系统，它采用无训练检索控制流程，通过缓存图像动作轨迹并在测试时嵌入当前帧来检索和回放多步片段，从而取代了分步模型调用。该系统采用分层搜索，在百万级规模上实现了亚秒级的查找速度，降低了计算成本并实现了实时控制。在真实机器人任务和大规模开放日志测试中，RT-Cache相较于强大的检索基线取得了更高的成功率和更快的完成时间。此外，其即时适应复杂接触密集型任务的能力，无需微调。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>RT-Cache系统采用无训练检索控制流程，适用于真实机器人场景中的快速适应任务。</li>
<li>通过缓存图像动作轨迹并嵌入当前帧进行检索和回放，实现高效率且不需要分步模型调用。</li>
<li>分层搜索技术使得查找速度在百万级规模上实现亚秒级响应。</li>
<li>RT-Cache实现了实时控制，降低了计算成本。</li>
<li>在真实机器人任务和大规模开放日志测试中，RT-Cache比强大的检索基线表现出更高的成功率和更快的完成时间。</li>
<li>系统能够即时适应复杂的接触密集型任务，无需进行微调。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.09040">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-2bb2dd657629b0f3ccdac0b354b5da65.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c275cea08aa9eb68ea5b91104c5a9705.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9440bb0f6c1957d19c742a10534c6bf9.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-8fc098f1981f1081ad3e9cc80614f909.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-29666b3c6a14d523e000d37c2bf4a62d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a74a1b08a346ad94cb6c798f0bb219c8.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c0b775db71b7945606a1da08ad168620.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="MEGA-Second-Order-Gradient-Alignment-for-Catastrophic-Forgetting-Mitigation-in-GFSCIL"><a href="#MEGA-Second-Order-Gradient-Alignment-for-Catastrophic-Forgetting-Mitigation-in-GFSCIL" class="headerlink" title="MEGA: Second-Order Gradient Alignment for Catastrophic Forgetting   Mitigation in GFSCIL"></a>MEGA: Second-Order Gradient Alignment for Catastrophic Forgetting   Mitigation in GFSCIL</h2><p><strong>Authors:Jinhui Pang, Changqing Lin, Hao Lin, Zhihui Zhang, Long Chen, Weiping Ding, Yu Liu, Xiaoshuai Hao</strong></p>
<p>Graph Few-Shot Class-Incremental Learning (GFSCIL) enables models to continually learn from limited samples of novel tasks after initial training on a large base dataset. Existing GFSCIL approaches typically utilize Prototypical Networks (PNs) for metric-based class representations and fine-tune the model during the incremental learning stage. However, these PN-based methods oversimplify learning via novel query set fine-tuning and fail to integrate Graph Continual Learning (GCL) techniques due to architectural constraints. To address these challenges, we propose a more rigorous and practical setting for GFSCIL that excludes query sets during the incremental training phase. Building on this foundation, we introduce Model-Agnostic Meta Graph Continual Learning (MEGA), aimed at effectively alleviating catastrophic forgetting for GFSCIL. Specifically, by calculating the incremental second-order gradient during the meta-training stage, we endow the model to learn high-quality priors that enhance incremental learning by aligning its behaviors across both the meta-training and incremental learning stages. Extensive experiments on four mainstream graph datasets demonstrate that MEGA achieves state-of-the-art results and enhances the effectiveness of various GCL methods in GFSCIL. We believe that our proposed MEGA serves as a model-agnostic GFSCIL paradigm, paving the way for future research. </p>
<blockquote>
<p>图增量少样本类增量学习（GFSCIL）使模型能够在大量基础数据集上进行初步训练后，从新增任务的有限样本中持续学习。现有的GFSCIL方法通常利用原型网络（PNs）进行基于度量的类表示，并在增量学习阶段对模型进行微调。然而，这些基于PN的方法通过新的查询集微调来简化学习，并且由于架构约束，无法整合图持续学习（GCL）技术。为了解决这些挑战，我们为GFSCIL提出了一个更严格、更实用的设置，即在增量训练阶段排除查询集。在此基础上，我们引入了模型无关的元图持续学习（MEGA），旨在有效缓解GFSCIL的灾难性遗忘问题。具体来说，通过计算元训练阶段的增量二阶梯度，我们赋予模型学习高质量先验的能力，通过对齐元训练阶段和增量学习阶段的行为，增强增量学习效果。在四个主流图数据集上的大量实验表明，MEGA达到了最新的结果，提高了GFSCIL中各种GCL方法的有效性。我们相信，我们提出的MEGA作为一种模型无关的GFSCIL范式，为未来的研究铺平了道路。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.13691v2">PDF</a> Under Review</p>
<p><strong>Summary</strong></p>
<p>基于图数据的Few-Shot类增量学习（GFSCIL）能够让模型在初始时训练大量基础数据集后，继续学习新任务的小样本数据。现有的GFSCIL方法通常采用原型网络（PNs）进行基于度量的类表示，并在增量学习阶段微调模型。然而，基于PN的方法简化了通过新查询集进行微调的学习过程，并因架构约束未能整合图持续学习（GCL）技术。为解决这些挑战，我们提出了更严格和实用的GFSCIL设置，增量训练阶段排除查询集。在此基础上，我们引入了模型无关的元图持续学习（MEGA）方法，旨在有效缓解GFSCIL的灾难性遗忘问题。通过计算元训练阶段的二阶增量梯度，我们使模型学习高质量先验，增强元训练和增量学习阶段的行为一致性。在四个主流图数据集上的广泛实验表明，MEGA达到了最新水平的结果，提高了各种GCL方法在GFSCIL中的有效性。我们相信，我们提出的MEGA为GFSCIL提供了一个模型无关的解决方案，为未来的研究铺平了道路。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Graph Few-Shot Class-Incremental Learning (GFSCIL) 允许模型在不断学习新任务的小样本数据的同时，保留在初始大量基础数据集上训练的结果。</li>
<li>现有GFSCIL方法主要使用原型网络（PNs）进行类表示和微调模型，但这种方法过于简化学习过程。</li>
<li>基于PN的方法未能充分利用图持续学习（GCL）技术，主要原因是架构上的限制。</li>
<li>为改进现有方法，提出了更严格和实用的GFSCIL设置，即在增量训练阶段排除查询集。</li>
<li>引入了一种新的方法——模型无关的元图持续学习（MEGA）——来缓解GFSCIL中的灾难性遗忘问题。</li>
<li>MEGA通过计算元训练阶段的二阶增量梯度来学习高质量先验知识，增强了模型在元训练和增量学习阶段的行为一致性。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.13691">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-93f547e745031e2ee5675d1050b0a962.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-697cff06aa36c224265bf68866ec0822.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-76762afef9d330b19e4b1da7546077d2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5d73516765465589443291dc74bf929a.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-be152d1f77e9c2cb490d93192384c07f.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-2678f5bbd902bcfefa0afe733419b18e.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="Can-Masked-Autoencoders-Also-Listen-to-Birds"><a href="#Can-Masked-Autoencoders-Also-Listen-to-Birds" class="headerlink" title="Can Masked Autoencoders Also Listen to Birds?"></a>Can Masked Autoencoders Also Listen to Birds?</h2><p><strong>Authors:Lukas Rauch, René Heinrich, Ilyass Moummad, Alexis Joly, Bernhard Sick, Christoph Scholz</strong></p>
<p>Masked Autoencoders (MAEs) learn rich semantic representations in audio classification through an efficient self-supervised reconstruction task. However, general-purpose models fail to generalize well when applied directly to fine-grained audio domains. Specifically, bird-sound classification requires distinguishing subtle inter-species differences and managing high intra-species acoustic variability, revealing the performance limitations of general-domain Audio-MAEs. This work demonstrates that bridging this domain gap domain gap requires full-pipeline adaptation, not just domain-specific pretraining data. We systematically revisit and adapt the pretraining recipe, fine-tuning methods, and frozen feature utilization to bird sounds using BirdSet, a large-scale bioacoustic dataset comparable to AudioSet. Our resulting Bird-MAE achieves new state-of-the-art results in BirdSet’s multi-label classification benchmark. Additionally, we introduce the parameter-efficient prototypical probing, enhancing the utility of frozen MAE representations and closely approaching fine-tuning performance in low-resource settings. Bird-MAE’s prototypical probes outperform linear probing by up to 37 percentage points in mean average precision and narrow the gap to fine-tuning across BirdSet downstream tasks. Bird-MAE also demonstrates robust few-shot capabilities with prototypical probing in our newly established few-shot benchmark on BirdSet, highlighting the potential of tailored self-supervised learning pipelines for fine-grained audio domains. </p>
<blockquote>
<p>Masked Autoencoders (MAEs)通过高效的自监督重建任务，在音频分类中学习丰富的语义表示。然而，当直接应用于细粒度音频域时，通用模型往往无法很好地推广。特别是，鸟类声音分类需要区分物种间的细微差异并处理高物种内部声学变化，这揭示了通用领域Audio-MAE的性能局限性。这项工作表明，弥合这一领域差距需要全管道适应，而不仅仅是领域特定的预训练数据。我们系统地回顾并适应了预训练配方、微调方法和冻结特征的利用鸟类声音，使用BirdSet——一个可与AudioSet相比的大规模生物声学数据集。我们得到的Bird-MAE在BirdSet的多标签分类基准测试中达到了最新水平。此外，我们引入了参数高效的原型探测技术，提高了冻结MAE表示的实用性，并在低资源环境中接近微调性能。Bird-MAE的原型探针在平均精度均值方面比线性探针高出高达37个百分点，并缩小了在BirdSet下游任务中的微调差距。Bird-MAE还展示了我们新建立的BirdSet少样本基准测试中原型探针的稳健的少样本能力，突显了针对细粒度音频领域量身定制的自监督学习管道的巨大潜力。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.12880v4">PDF</a> accepted @TMLR: <a target="_blank" rel="noopener" href="https://openreview.net/forum?id=GIBWR0Xo2J">https://openreview.net/forum?id=GIBWR0Xo2J</a></p>
<p><strong>Summary</strong></p>
<p>本文介绍了Masked Autoencoders（MAEs）在音频分类中的丰富语义表示学习。然而，通用模型在精细粒度音频领域的应用中表现不佳。针对鸟类声音分类这一特定场景，本研究通过全管道适应方法，而非仅依赖特定领域的预训练数据来弥补领域差距。通过调整预训练配方、微调方法和冻结特征的利用方式，使用大规模生物声学数据集BirdSet，实现了对鸟类声音的新SOTA结果。此外，引入了参数高效的原型探测技术，提高了冻结MAE表示的实用性，并在低资源环境中接近微调性能。Bird-MAE的原型探针在平均精度上最多高出线性探针37个百分点，并缩小了在BirdSet下游任务中的微调差距。同时，Bird-MAE在全新建立的BirdSet少样本基准测试中展现出强大的少样本能力。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Masked Autoencoders (MAEs) 能够有效学习音频分类的丰富语义表示。</li>
<li>通用模型在精细粒度音频领域（如鸟类声音分类）存在性能局限。</li>
<li>跨越领域差距需要全管道适应方法，不仅依赖特定领域的预训练数据。</li>
<li>通过调整预训练配方、微调方法和利用冻结特征，使用BirdSet数据集实现了鸟声分类的新SOTA结果。</li>
<li>引入参数高效的原型探测技术，提高冻结MAE表示的实用性，并接近低资源环境中的微调性能。</li>
<li>Bird-MAE的原型探针在平均精度上有显著改进，缩小了与微调之间的差距。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.12880">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-df44f685a39835e2694e81697a4c4a1d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-cfedc85a9dd737c7d2d0dffc62b57f40.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-3f4363adaf2c8eec1fe82d2372899ed5.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="MedSpaformer-a-Transferable-Transformer-with-Multi-granularity-Token-Sparsification-for-Medical-Time-Series-Classification"><a href="#MedSpaformer-a-Transferable-Transformer-with-Multi-granularity-Token-Sparsification-for-Medical-Time-Series-Classification" class="headerlink" title="MedSpaformer: a Transferable Transformer with Multi-granularity Token   Sparsification for Medical Time Series Classification"></a>MedSpaformer: a Transferable Transformer with Multi-granularity Token   Sparsification for Medical Time Series Classification</h2><p><strong>Authors:Jiexia Ye, Weiqi Zhang, Ziyue Li, Jia Li, Fugee Tsung</strong></p>
<p>Accurate medical time series (MedTS) classification is essential for effective clinical diagnosis, yet remains challenging due to complex multi-channel temporal dependencies, information redundancy, and label scarcity. While transformer-based models have shown promise in time series analysis, most are designed for forecasting tasks and fail to fully exploit the unique characteristics of MedTS. In this paper, we introduce MedSpaformer, a transformer-based framework tailored for MedTS classification. It incorporates a sparse token-based dual-attention mechanism that enables global context modeling and token sparsification, allowing dynamic feature refinement by focusing on informative tokens while reducing redundancy. This mechanism is integrated into a multi-granularity cross-channel encoding scheme to capture intra- and inter-granularity temporal dependencies and inter-channel correlations, enabling progressive refinement of task-relevant patterns in medical signals. The sparsification design allows our model to flexibly accommodate inputs with variable lengths and channel dimensions. We also introduce an adaptive label encoder to extract label semantics and address cross-dataset label space misalignment. Together, these components enhance the model’s transferability across heterogeneous medical datasets, which helps alleviate the challenge of label scarcity. Our model outperforms 13 baselines across 7 medical datasets under supervised learning. It also excels in few-shot learning and demonstrates zero-shot capability in both in-domain and cross-domain diagnostics. These results highlight MedSpaformer’s robustness and its potential as a unified solution for MedTS classification across diverse settings. </p>
<blockquote>
<p>精确医疗时间序列（MedTS）分类对于有效的临床诊断至关重要，但由于复杂的跨通道时间依赖性、信息冗余和标签稀缺，它仍然是一个挑战。虽然基于变压器的模型在时间序列分析中显示出潜力，但大多数模型都是为预测任务而设计的，未能充分利用MedTS的独特特征。在本文中，我们介绍了针对MedTS分类的基于变压器的框架MedSpaformer。它采用了一种稀疏的基于标记的双重注意力机制，能够全局建模上下文并精简标记，通过关注信息丰富的标记进行动态特征细化，同时减少冗余。该机制被整合到多粒度跨通道编码方案中，以捕获粒度和跨粒度的时间依赖性以及通道间的相关性，从而逐步优化医疗信号中与任务相关的模式。精简设计使得我们的模型可以灵活地适应具有不同长度和通道维度的输入。我们还引入了一种自适应标签编码器来提取标签语义并解决跨数据集标签空间的不对齐问题。这些组件共同提高了模型在异质医疗数据集上的迁移能力，有助于缓解标签稀缺的挑战。我们的模型在7个医疗数据集上超越了13个基线模型，在监督学习中的表现尤为出色。在少量样本学习场景中，该模型表现出色，并在域内和跨域诊断中展示出零样本学习能力。这些结果凸显了MedSpaformer的稳健性以及其在多样环境下作为统一解决方案的潜力。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.15578v3">PDF</a> 4 figures, 9 pages, 4 tables</p>
<p><strong>Summary</strong></p>
<p>本文提出了一种针对医学时间序列（MedTS）分类的MedSpaformer框架。它通过稀疏令牌基础的双重注意力机制，实现了全局上下文建模和令牌稀疏化，减少了冗余信息并提升了特征精炼能力。同时集成多粒度跨通道编码方案，捕捉不同粒度的时序依赖性和跨通道相关性。通过引入自适应标签编码器解决跨数据集标签空间不一致性问题，增强模型在不同医疗数据集间的可迁移性。实验显示该模型在多个医学数据集上的表现优于其他基线模型，尤其在少样本学习方面表现优异，并具备零样本能力进行同域和跨域诊断。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>MedSpaformer是一种针对医学时间序列分类的变压器模型框架。</li>
<li>它采用了稀疏令牌基础的双重注意力机制以实现全局上下文建模和令牌稀疏化。</li>
<li>集成多粒度跨通道编码方案，有效捕捉时序数据的不同粒度依赖性和跨通道相关性。</li>
<li>自适应标签编码器用于提取标签语义并处理跨数据集标签空间不一致性问题。</li>
<li>MedSpaformer模型在多个医学数据集上的表现优于其他基线模型。</li>
<li>该模型在少样本学习方面表现优异，并具备零样本能力进行诊断。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.15578">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-5294a351417d2d84d6212825bbbdd547.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-67d937cfebb5978e0278141e23c9ab38.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-552015adbfb87c70e8eb7a2a881f8334.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-bc763bc37a858ffac3f10f97289d8dd3.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="Closed-Form-Feedback-Free-Learning-with-Forward-Projection"><a href="#Closed-Form-Feedback-Free-Learning-with-Forward-Projection" class="headerlink" title="Closed-Form Feedback-Free Learning with Forward Projection"></a>Closed-Form Feedback-Free Learning with Forward Projection</h2><p><strong>Authors:Robert O’Shea, Bipin Rajendran</strong></p>
<p>State-of-the-art methods for backpropagation-free learning employ local error feedback to direct iterative optimisation via gradient descent. In this study, we examine the more restrictive setting where retrograde communication from neuronal outputs is unavailable for pre-synaptic weight optimisation. To address this challenge, we propose Forward Projection (FP). This novel randomised closed-form training method requires only a single forward pass over the entire dataset for model fitting, without retrograde communication. Target values for pre-activation membrane potentials are generated layer-wise via nonlinear projections of pre-synaptic inputs and the labels. Local loss functions are optimised over pre-synaptic inputs using closed-form regression, without feedback from neuronal outputs or downstream layers. Interpretability is a key advantage of FP training; membrane potentials of hidden neurons in FP-trained networks encode information which is interpretable layer-wise as label predictions. We demonstrate the effectiveness of FP across four biomedical datasets. In few-shot learning tasks, FP yielded more generalisable models than those optimised via backpropagation. In large-sample tasks, FP-based models achieve generalisation comparable to gradient descent-based local learning methods while requiring only a single forward propagation step, achieving significant speed up for training. Interpretation functions defined on local neuronal activity in FP-based models successfully identified clinically salient features for diagnosis in two biomedical datasets. Forward Projection is a computationally efficient machine learning approach that yields interpretable neural network models without retrograde communication of neuronal activity during training. </p>
<blockquote>
<p>当前先进的不依赖反向传播的学习方法采用局部误差反馈来指导通过梯度下降法的迭代优化。在这项研究中，我们考察了一个更严格的设置环境，即当神经元输出的逆向通信无法用于突触前权重优化时的情况。为了应对这一挑战，我们提出了“前向投影”（FP）。这种新颖的随机闭式训练方法仅在整个数据集上执行一次前向传递即可进行模型拟合，无需逆向通信。目标值逐层生成，通过突触前输入的非线性投影和标签实现。局部损失函数通过闭式回归在突触前输入上进行优化，无需神经元输出或下游层的反馈。可解释性是FP训练的关键优势；FP训练后的网络中隐藏神经元的膜电位编码的信息可以按层解读为标签预测。我们在四个生物医学数据集上展示了FP的有效性。在少量样本学习任务中，FP产生的模型比通过反向传播优化的模型更具泛化能力。在大样本任务中，FP模型基于的泛化能力与梯度下降法本地学习方法相当，但仅需一次前向传播步骤，大大加快了训练速度。在FP模型上定义的局部神经元活动的解释函数成功地在两个生物医学数据集中识别出了用于诊断的临床重要特征。前向投影是一种计算效率高的机器学习方法，可在训练过程中无需神经元活动的逆向通信即可产生可解释的神经网络模型。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.16476v2">PDF</a> 26 pages, 5 figures. Study code available at   <a target="_blank" rel="noopener" href="https://github.com/robertoshea/forward_projection">https://github.com/robertoshea/forward_projection</a>. Study data available at   <a target="_blank" rel="noopener" href="https://data.mendeley.com/datasets/fb7xddyxs4/2">https://data.mendeley.com/datasets/fb7xddyxs4/2</a></p>
<p><strong>Summary</strong></p>
<p>本研究提出了一种名为Forward Projection（FP）的新型随机闭式训练方法，该方法在无需逆向神经元输出通信的情况下，通过单次前向传播即可完成模型拟合。通过逐层生成目标预激活膜电位值，并利用闭式回归优化预突触输入的局部损失函数，FP训练提高了模型的解释性和预测性能。在四个生物医学数据集上的实验表明，FP在少样本学习任务中表现出更高的泛化能力，在大样本任务中也能实现与基于梯度下降的地方学习方法相当的泛化性能，同时只需一步前向传播，显著加快了训练速度。此外，FP模型中的解释函数能够成功识别两个生物医学数据集中的临床重要特征，为诊断提供了有力支持。总体而言，FP是一种高效且可解释的机器学习新方法，无需训练过程中的神经元活动逆向通信。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Forward Projection（FP）是一种新型的随机闭式训练方法，无需逆向神经元输出通信即可完成模型拟合。</li>
<li>FP通过逐层生成目标预激活膜电位值并利用闭式回归优化预突触输入的局部损失函数，提高模型的解释性和预测性能。</li>
<li>在少样本学习任务中，FP表现出更高的泛化能力。</li>
<li>在大样本任务中，FP模型的泛化性能与基于梯度下降的地方学习方法相当，但只需一步前向传播，加快了训练速度。</li>
<li>FP模型中的解释函数能够识别临床重要特征，为诊断提供支持。</li>
<li>FP训练提高了模型的解释性，使得膜电位信息可逐层解释为标签预测。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.16476">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-b0f7e634e46a4f7a8bdc8bd1e08eb1ca.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-05d5032080320071e0b0967bd816ef00.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c9879518d9b618f618dd5eb3be9c68e9.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        文章作者:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        文章链接:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-08-21/Few-Shot/">https://kedreamix.github.io/Talk2Paper/Paper/2025-08-21/Few-Shot/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        版权声明:
                    </i>
                </span>
                <span class="reprint-info">
                    本博客所有文章除特別声明外，均采用
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    许可协议。转载请注明来源
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Few-Shot/">
                                    <span class="chip bg-color">Few-Shot</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;上一篇</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-08-21/Vision%20Transformer/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-fe4aa76d1eabdf424fb19ba1f6bc65b6.jpg" class="responsive-img" alt="Vision Transformer">
                        
                        <span class="card-title">Vision Transformer</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Vision Transformer 方向最新论文已更新，请持续关注 Update in 2025-08-21  ViT-FIQA Assessing Face Image Quality using Vision Transformers
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-08-21
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Vision-Transformer/" class="post-category">
                                    Vision Transformer
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Vision-Transformer/">
                        <span class="chip bg-color">Vision Transformer</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                下一篇&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-08-21/Agent/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-09044d5bf6f2fd548f1712059536c58c.jpg" class="responsive-img" alt="Agent">
                        
                        <span class="card-title">Agent</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Agent 方向最新论文已更新，请持续关注 Update in 2025-08-21  ComputerRL Scaling End-to-End Online Reinforcement Learning for   Computer Use Agents
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-08-21
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Agent/" class="post-category">
                                    Agent
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Agent/">
                        <span class="chip bg-color">Agent</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- 代码块功能依赖 -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- 代码语言 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- 代码块复制 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- 代码块收缩 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC 悬浮按钮. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* 修复文章卡片 div 的宽度. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // 切换TOC目录展开收缩的相关操作.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;站点总字数:&nbsp;<span
                        class="white-color">27768.2k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- 运行天数提醒. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // 区分是否有年份.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = '本站已运行 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                daysTip = '本站已運行 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = '本站已运行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = '本站已運行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="关注我的知乎: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">知</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- 搜索遮罩框 -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span>
            <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- 白天和黑夜主题 -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- 回到顶部按钮 -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // 只在桌面版网页启用特效
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- 雪花特效 -->
    

    <!-- 鼠标星星特效 -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--腾讯兔小巢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- 动态标签 -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Σ(っ °Д °;)っ诶，页面崩溃了嘛？", clearTimeout(st)) : (document.title = "φ(゜▽゜*)♪咦，又好了！", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
