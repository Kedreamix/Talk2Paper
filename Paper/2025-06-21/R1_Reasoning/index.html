<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="R1_Reasoning">
    <meta name="description" content="R1_Reasoning æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-06-21  Seewo&#39;s Submission to MLC-SLM Lessons learned from Speech Reasoning   Language Models">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>R1_Reasoning | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pic1.zhimg.com/v2-bacceaa5b739488680cd4a31fc507c6f.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">R1_Reasoning</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/R1-Reasoning/">
                                <span class="chip bg-color">R1_Reasoning</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/R1-Reasoning/" class="post-category">
                                R1_Reasoning
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-06-21
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-07-06
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    16.2k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    67 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-06-21-æ›´æ–°"><a href="#2025-06-21-æ›´æ–°" class="headerlink" title="2025-06-21 æ›´æ–°"></a>2025-06-21 æ›´æ–°</h1><h2 id="Seewoâ€™s-Submission-to-MLC-SLM-Lessons-learned-from-Speech-Reasoning-Language-Models"><a href="#Seewoâ€™s-Submission-to-MLC-SLM-Lessons-learned-from-Speech-Reasoning-Language-Models" class="headerlink" title="Seewoâ€™s Submission to MLC-SLM: Lessons learned from Speech Reasoning   Language Models"></a>Seewoâ€™s Submission to MLC-SLM: Lessons learned from Speech Reasoning   Language Models</h2><p><strong>Authors:Bo Li, Chengben Xu, Wufeng Zhang</strong></p>
<p>This paper presents Seewoâ€™s systems for both tracks of the Multilingual Conversational Speech Language Model Challenge (MLC-SLM), addressing automatic speech recognition (ASR) and speaker diarization with ASR (SD-ASR). We introduce a multi-stage training pipeline that explicitly enhances reasoning and self-correction in speech language models for ASR. Our approach combines curriculum learning for progressive capability acquisition, Chain-of-Thought data augmentation to foster intermediate reflection, and Reinforcement Learning with Verifiable Rewards (RLVR) to further refine self-correction through reward-driven optimization. This approach achieves substantial improvements over the official challenge baselines. On the evaluation set, our best system attains a WER&#x2F;CER of 11.57% for Track 1 and a tcpWER&#x2F;tcpCER of 17.67% for Track 2. Comprehensive ablation studies demonstrate the effectiveness of each component under challenge constraints. </p>
<blockquote>
<p>æœ¬æ–‡ä»‹ç»äº†Seewoåœ¨å¤šè¯­ç§å¯¹è¯è¯­éŸ³è¯­è¨€æ¨¡å‹æŒ‘æˆ˜ï¼ˆMLC-SLMï¼‰ä¸¤ä¸ªèµ›é“ä¸­çš„ç³»ç»Ÿï¼ŒåŒ…æ‹¬è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰å’Œå¸¦æœ‰ASRçš„è¯´è¯äººæ—¥è®°åŒ–ï¼ˆSD-ASRï¼‰ã€‚æˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ªå¤šé˜¶æ®µè®­ç»ƒæµç¨‹ï¼Œè¯¥æµç¨‹åœ¨è¯­éŸ³è¯­è¨€æ¨¡å‹ä¸­æ˜¾å¼å¢å¼ºæ¨ç†å’Œè‡ªæˆ‘æ ¡æ­£åŠŸèƒ½ï¼Œç”¨äºASRã€‚æˆ‘ä»¬çš„æ–¹æ³•ç»“åˆäº†è¯¾ç¨‹å­¦ä¹ ä»¥å®ç°é€æ­¥èƒ½åŠ›è·å–ï¼Œé€šè¿‡â€œæ€è€ƒé“¾â€æ•°æ®å¢å¼ºæ¥ä¿ƒè¿›ä¸­é—´åæ€ï¼Œä»¥åŠå¼ºåŒ–å­¦ä¹ ä¸å¯éªŒè¯å¥–åŠ±ï¼ˆRLVRï¼‰è¿›ä¸€æ­¥é€šè¿‡å¥–åŠ±é©±åŠ¨ä¼˜åŒ–æ¥å®Œå–„è‡ªæˆ‘æ ¡æ­£ã€‚è¯¥æ–¹æ³•åœ¨å®˜æ–¹æŒ‘æˆ˜åŸºçº¿çš„åŸºç¡€ä¸Šå–å¾—äº†æ˜¾è‘—æ”¹è¿›ã€‚åœ¨è¯„ä¼°é›†ä¸Šï¼Œæˆ‘ä»¬æœ€å¥½çš„ç³»ç»Ÿåœ¨ç¬¬1èµ›é“ä¸Šè¾¾åˆ°äº†11.57%çš„WER&#x2F;CERï¼Œåœ¨ç¬¬2èµ›é“ä¸Šè¾¾åˆ°äº†17.67%çš„tcpWER&#x2F;tcpCERã€‚å…¨é¢çš„æ¶ˆèç ”ç©¶è¡¨æ˜ï¼Œåœ¨æŒ‘æˆ˜çº¦æŸä¸‹ï¼Œæ¯ä¸ªç»„ä»¶éƒ½æ˜¯æœ‰æ•ˆçš„ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.13300v3">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†Seewoåœ¨å¤šè¯­ç§å¯¹è¯è¯­éŸ³è¯­è¨€æ¨¡å‹æŒ‘æˆ˜ï¼ˆMLC-SLMï¼‰çš„ä¸¤æ¡èµ›é“ä¸Šçš„ç³»ç»Ÿï¼Œæ¶µç›–äº†è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰å’Œå¸¦ASRçš„è¯´è¯äººåˆ†å·ç§¯ï¼ˆSD-ASRï¼‰ã€‚æ–‡ç« æå‡ºäº†ä¸€ç§å¤šé˜¶æ®µè®­ç»ƒç®¡é“ï¼Œæ—¨åœ¨æé«˜è¯­éŸ³è¯­è¨€æ¨¡å‹åœ¨ASRä¸­çš„æ¨ç†å’Œè‡ªæˆ‘çº é”™èƒ½åŠ›ã€‚è¯¥æ–¹æ³•ç»“åˆäº†è¯¾ç¨‹å­¦ä¹ ä»¥ä¿ƒè¿›èƒ½åŠ›é€æ­¥ç§¯ç´¯ï¼ŒChain-of-Thoughtæ•°æ®å¢å¼ºä»¥ä¿ƒè¿›ä¸­é—´åæ€ï¼Œä»¥åŠå¼ºåŒ–å­¦ä¹ ä¸å¯éªŒè¯å¥–åŠ±ï¼ˆRLVRï¼‰ä»¥é€šè¿‡å¥–åŠ±é©±åŠ¨ä¼˜åŒ–è¿›ä¸€æ­¥æ”¹è¿›è‡ªæˆ‘çº é”™ã€‚è¯¥æ–¹æ³•åœ¨æŒ‘æˆ˜åŸºå‡†çº¿ä¸Šå–å¾—äº†æ˜¾è‘—æ”¹è¿›ã€‚åœ¨è¯„ä¼°é›†ä¸Šï¼Œæˆ‘ä»¬æœ€å¥½çš„ç³»ç»Ÿè¾¾åˆ°äº†Track 1çš„WER&#x2F;CERä¸º11.57%ï¼ŒTrack 2çš„tcpWER&#x2F;tcpCERä¸º17.67%ã€‚ç»¼åˆæ¶ˆèç ”ç©¶è¯æ˜äº†æ¯ä¸ªç»„ä»¶åœ¨æŒ‘æˆ˜çº¦æŸä¸‹çš„æœ‰æ•ˆæ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Seewoç³»ç»Ÿå‚åŠ äº†å¤šè¯­ç§å¯¹è¯è¯­éŸ³è¯­è¨€æ¨¡å‹æŒ‘æˆ˜ï¼ŒåŒ…å«è‡ªåŠ¨è¯­éŸ³è¯†åˆ«å’Œå¸¦ASRçš„è¯´è¯äººåˆ†å·ç§¯ä¸¤ä¸ªèµ›é“ã€‚</li>
<li>æå‡ºäº†ä¸€ç§å¤šé˜¶æ®µè®­ç»ƒç®¡é“ï¼Œæ—¨åœ¨æé«˜è¯­éŸ³è¯­è¨€æ¨¡å‹åœ¨ASRä¸­çš„æ¨ç†å’Œè‡ªæˆ‘çº é”™èƒ½åŠ›ã€‚</li>
<li>ç»“åˆè¯¾ç¨‹å­¦ä¹ ä¿ƒè¿›èƒ½åŠ›é€æ­¥ç§¯ç´¯ï¼Œé€šè¿‡Chain-of-Thoughtæ•°æ®å¢å¼ºä¸­é—´åæ€ã€‚</li>
<li>é‡‡ç”¨å¼ºåŒ–å­¦ä¹ ä¸å¯éªŒè¯å¥–åŠ±ï¼ˆRLVRï¼‰æ”¹è¿›è‡ªæˆ‘çº é”™ï¼Œé€šè¿‡å¥–åŠ±é©±åŠ¨ä¼˜åŒ–ã€‚</li>
<li>è¯¥æ–¹æ³•åœ¨æŒ‘æˆ˜åŸºå‡†çº¿ä¸Šå–å¾—äº†æ˜¾è‘—æ”¹è¿›ï¼Œæœ€ä½³ç³»ç»Ÿæ€§èƒ½è¡¨ç°ä¼˜å¼‚ã€‚</li>
<li>åœ¨è¯„ä¼°é›†ä¸Šï¼ŒTrack 1çš„WER&#x2F;CERä¸º11.57%ï¼ŒTrack 2çš„tcpWER&#x2F;tcpCERä¸º17.67%ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.13300">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-4699484f27576657f63c1b56aa36d56f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-fec32241e658ccd300ca24eac739ee88.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-5858e1fab9c34b9b9d8c086975b8b30e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7c05416f6933a7297c49f6794654285a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b2fc3b4c606bb924858746a864f0df6f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-420e87a51ff6e652b7bdbf25534b5ce2.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Multi-Task-Reward-Learning-from-Human-Ratings"><a href="#Multi-Task-Reward-Learning-from-Human-Ratings" class="headerlink" title="Multi-Task Reward Learning from Human Ratings"></a>Multi-Task Reward Learning from Human Ratings</h2><p><strong>Authors:Mingkang Wu, Devin White, Evelyn Rose, Vernon Lawhern, Nicholas R Waytowich, Yongcan Cao</strong></p>
<p>Reinforcement learning from human feedback (RLHF) has become a key factor in aligning model behavior with usersâ€™ goals. However, while humans integrate multiple strategies when making decisions, current RLHF approaches often simplify this process by modeling human reasoning through isolated tasks such as classification or regression. In this paper, we propose a novel reinforcement learning (RL) method that mimics human decision-making by jointly considering multiple tasks. Specifically, we leverage human ratings in reward-free environments to infer a reward function, introducing learnable weights that balance the contributions of both classification and regression models. This design captures the inherent uncertainty in human decision-making and allows the model to adaptively emphasize different strategies. We conduct several experiments using synthetic human ratings to validate the effectiveness of the proposed approach. Results show that our method consistently outperforms existing rating-based RL methods, and in some cases, even surpasses traditional RL approaches. </p>
<blockquote>
<p>å¼ºåŒ–å­¦ä¹ ä»äººç±»åé¦ˆï¼ˆRLHFï¼‰å·²ç»æˆä¸ºä½¿æ¨¡å‹è¡Œä¸ºä¸ç”¨æˆ·ç›®æ ‡ä¿æŒä¸€è‡´çš„å…³é”®å› ç´ ã€‚ç„¶è€Œï¼Œäººç±»åœ¨å†³ç­–æ—¶ä¼šç»“åˆå¤šç§ç­–ç•¥ï¼Œè€Œå½“å‰çš„RLHFæ–¹æ³•å¾€å¾€é€šè¿‡åˆ†ç±»æˆ–å›å½’ç­‰å­¤ç«‹ä»»åŠ¡æ¥æ¨¡æ‹Ÿäººç±»æ¨ç†ï¼Œä»è€Œç®€åŒ–äº†è¿™ä¸€è¿‡ç¨‹ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°å‹å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰æ–¹æ³•ï¼Œé€šè¿‡åŒæ—¶è€ƒè™‘å¤šä¸ªä»»åŠ¡æ¥æ¨¡æ‹Ÿäººç±»å†³ç­–è¿‡ç¨‹ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬åœ¨æ— å¥–åŠ±ç¯å¢ƒä¸­åˆ©ç”¨äººç±»è¯„åˆ†æ¥æ¨æ–­å¥–åŠ±å‡½æ•°ï¼Œå¹¶å¼•å…¥å¯å­¦ä¹ çš„æƒé‡æ¥å¹³è¡¡åˆ†ç±»å’Œå›å½’æ¨¡å‹çš„è´¡çŒ®ã€‚è¿™ç§è®¾è®¡æ•æ‰äº†äººç±»å†³ç­–ä¸­å›ºæœ‰çš„ä¸ç¡®å®šæ€§ï¼Œå¹¶å…è®¸æ¨¡å‹è‡ªé€‚åº”åœ°å¼ºè°ƒä¸åŒçš„ç­–ç•¥ã€‚æˆ‘ä»¬ä½¿ç”¨åˆæˆçš„äººç±»è¯„åˆ†è¿›è¡Œäº†å¤šæ¬¡å®éªŒï¼Œä»¥éªŒè¯æ‰€æå‡ºæ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚ç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•å§‹ç»ˆä¼˜äºåŸºäºè¯„åˆ†çš„ç°æœ‰RLæ–¹æ³•ï¼Œåœ¨æŸäº›æƒ…å†µä¸‹ï¼Œç”šè‡³è¶…è¿‡äº†ä¼ ç»ŸRLæ–¹æ³•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.09183v2">PDF</a> Accepted to the workshop on Models of Human Feedback for AI Alignment   at the 42nd International Conference on Machine Learning</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°å‹å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰æ–¹æ³•ï¼Œé€šè¿‡è”åˆè€ƒè™‘å¤šä»»åŠ¡æ¥æ¨¡æ‹Ÿäººç±»å†³ç­–è¿‡ç¨‹ã€‚è¯¥æ–¹æ³•åˆ©ç”¨æ— å¥–åŠ±ç¯å¢ƒä¸­çš„äººç±»è¯„åˆ†æ¥æ¨æ–­å¥–åŠ±å‡½æ•°ï¼Œå¹¶å¼•å…¥å¯å­¦ä¹ çš„æƒé‡æ¥å¹³è¡¡åˆ†ç±»å’Œå›å½’æ¨¡å‹çš„è´¡çŒ®ã€‚è¯¥ç ”ç©¶æ—¨åœ¨æ•æ‰äººç±»å†³ç­–è¿‡ç¨‹ä¸­çš„å†…åœ¨ä¸ç¡®å®šæ€§ï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿè‡ªé€‚åº”åœ°å¼ºè°ƒä¸åŒçš„ç­–ç•¥ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨åŸºäºè¯„åˆ†çš„RLæ–¹æ³•ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œç”šè‡³åœ¨æŸäº›æƒ…å†µä¸‹è¶…è¶Šäº†ä¼ ç»ŸRLæ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¼ºåŒ–å­¦ä¹ ä»äººç±»åé¦ˆï¼ˆRLHFï¼‰å·²æˆä¸ºå¯¹é½æ¨¡å‹è¡Œä¸ºä¸ç”¨æˆ·éœ€æ±‚çš„å…³é”®å› ç´ ã€‚</li>
<li>å½“å‰RLHFæ–¹æ³•å¾€å¾€é€šè¿‡å­¤ç«‹ä»»åŠ¡ï¼ˆå¦‚åˆ†ç±»æˆ–å›å½’ï¼‰æ¥æ¨¡æ‹Ÿäººç±»æ¨ç†ï¼Œç®€åŒ–äº†å†³ç­–è¿‡ç¨‹ã€‚</li>
<li>æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°å‹å¼ºåŒ–å­¦ä¹ æ–¹æ³•ï¼Œé€šè¿‡è”åˆè€ƒè™‘å¤šä»»åŠ¡æ¥æ¨¡æ‹Ÿäººç±»å†³ç­–è¿‡ç¨‹ã€‚</li>
<li>è¯¥æ–¹æ³•åˆ©ç”¨æ— å¥–åŠ±ç¯å¢ƒä¸­çš„äººç±»è¯„åˆ†æ¥æ¨æ–­å¥–åŠ±å‡½æ•°ï¼Œå¹¶å¼•å…¥å¯å­¦ä¹ æƒé‡ä»¥å¹³è¡¡åˆ†ç±»å’Œå›å½’æ¨¡å‹çš„è´¡çŒ®ã€‚</li>
<li>è¯¥è®¾è®¡æ•æ‰äº†äººç±»å†³ç­–è¿‡ç¨‹ä¸­çš„å†…åœ¨ä¸ç¡®å®šæ€§ã€‚</li>
<li>å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨åŸºäºè¯„åˆ†çš„RLæ–¹æ³•ä¸­è¡¨ç°ä¼˜å¼‚ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.09183">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-f323c547718f8e6bed2bd542986872fb.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-e24296dfd188c331f166b828fcdbdd8f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-69ab64e4edcab2365aa73d2c3046e9e2.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ee04fd8b0665bdcc08a9a8638c78bba1.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Router-R1-Teaching-LLMs-Multi-Round-Routing-and-Aggregation-via-Reinforcement-Learning"><a href="#Router-R1-Teaching-LLMs-Multi-Round-Routing-and-Aggregation-via-Reinforcement-Learning" class="headerlink" title="Router-R1: Teaching LLMs Multi-Round Routing and Aggregation via   Reinforcement Learning"></a>Router-R1: Teaching LLMs Multi-Round Routing and Aggregation via   Reinforcement Learning</h2><p><strong>Authors:Haozhen Zhang, Tao Feng, Jiaxuan You</strong></p>
<p>The rapid emergence of diverse large language models (LLMs) has spurred the development of LLM routers that assign user queries to the most suitable model. However, existing LLM routers typically perform a single-round, one-to-one mapping (\textit{i.e.}, assigning each query to a single model in isolation), which limits their capability to tackle complex tasks that demand the complementary strengths of multiple LLMs. In this paper, we present \textbf{Router-R1}, a reinforcement learning (RL)-based framework that formulates multi-LLM routing and aggregation as a sequential decision process. Router-R1 instantiates the router itself as a capable LLM, leveraging its reasoning ability to interleave â€œthinkâ€ actions (internal deliberation) with â€œrouteâ€ actions (dynamic model invocation), and integrates each response into its evolving context. To facilitate learning, we employ a lightweight rule-based reward comprising format rewards, final outcome rewards, and a novel cost reward for optimizing the balance between performance and cost, opening a pathway toward enhancing performance-cost trade-offs via RL. Router-R1 also conditions only on simple model descriptors such as pricing, latency, and example performance, enabling strong generalization to unseen model selection. Experiments on seven general and multi-hop QA benchmarks show that Router-R1 outperforms several strong baselines, achieving superior performance while maintaining robust generalization and cost management. </p>
<blockquote>
<p>å¤šæ ·çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„å¿«é€Ÿæ¶Œç°æ¨åŠ¨äº†LLMè·¯ç”±å™¨çš„å¼€å‘ï¼Œè¿™äº›è·¯ç”±å™¨å°†ç”¨æˆ·æŸ¥è¯¢åˆ†é…ç»™æœ€åˆé€‚çš„æ¨¡å‹ã€‚ç„¶è€Œï¼Œç°æœ‰çš„LLMè·¯ç”±å™¨é€šå¸¸æ‰§è¡Œå•è½®ä¸€å¯¹ä¸€æ˜ å°„ï¼ˆå³ï¼Œå°†æ¯ä¸ªæŸ¥è¯¢å•ç‹¬åˆ†é…ç»™ä¸€ä¸ªæ¨¡å‹ï¼‰ï¼Œè¿™é™åˆ¶äº†å®ƒä»¬å¤„ç†å¤æ‚ä»»åŠ¡çš„èƒ½åŠ›ï¼Œè¿™äº›å¤æ‚ä»»åŠ¡éœ€è¦å¤šä¸ªLLMçš„äº’è¡¥ä¼˜åŠ¿ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†åŸºäºå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰çš„æ¡†æ¶Router-R1ï¼Œå°†å¤šLLMè·¯ç”±å’Œèšåˆå…¬å¼åŒ–ä¸ºä¸€ä¸ªé¡ºåºå†³ç­–è¿‡ç¨‹ã€‚Router-R1å°†è·¯ç”±å™¨æœ¬èº«å®ä¾‹åŒ–ä¸ºä¸€ä¸ªåŠŸèƒ½å¼ºå¤§çš„LLMï¼Œåˆ©ç”¨å®ƒçš„æ¨ç†èƒ½åŠ›å°†â€œæ€è€ƒâ€è¡ŒåŠ¨ï¼ˆå†…éƒ¨æ€è€ƒï¼‰ä¸â€œè·¯ç”±â€è¡ŒåŠ¨ï¼ˆåŠ¨æ€æ¨¡å‹è°ƒç”¨ï¼‰äº¤ç»‡åœ¨ä¸€èµ·ï¼Œå¹¶å°†æ¯ä¸ªå“åº”é›†æˆåˆ°ä¸æ–­å‘å±•çš„ä¸Šä¸‹æ–‡ä¸­ã€‚ä¸ºäº†ä¿ƒè¿›å­¦ä¹ ï¼Œæˆ‘ä»¬é‡‡ç”¨äº†ä¸€ç§åŸºäºè§„åˆ™çš„è½»é‡çº§å¥–åŠ±æœºåˆ¶ï¼ŒåŒ…æ‹¬æ ¼å¼å¥–åŠ±ã€æœ€ç»ˆæˆæœå¥–åŠ±å’Œä¸€ç§ç”¨äºä¼˜åŒ–æ€§èƒ½å’Œæˆæœ¬ä¹‹é—´å¹³è¡¡çš„æ–°å‹æˆæœ¬å¥–åŠ±ï¼Œä»è€Œå¼€è¾Ÿäº†é€šè¿‡RLæé«˜æ€§èƒ½ä¸æˆæœ¬æƒè¡¡çš„é“è·¯ã€‚Router-R1ä»…ä¾èµ–äºç®€å•çš„æ¨¡å‹æè¿°ç¬¦ï¼ˆå¦‚ä»·æ ¼ã€å»¶è¿Ÿå’Œæ€§èƒ½ç¤ºä¾‹ï¼‰ï¼Œèƒ½å¤Ÿå¾ˆå¥½åœ°é€‚åº”çœ‹ä¸è§çš„æ¨¡å‹é€‰æ‹©ã€‚åœ¨ä¸ƒä¸ªé€šç”¨å’Œå¤šè·³é—®ç­”åŸºå‡†æµ‹è¯•ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒRouter-R1ä¼˜äºå¤šä¸ªå¼ºå¤§çš„åŸºçº¿æ¨¡å‹ï¼Œåœ¨ä¿æŒç¨³å¥çš„æ³›åŒ–å’Œæˆæœ¬ç®¡ç†çš„åŒæ—¶å®ç°äº†å“è¶Šçš„æ€§èƒ½ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.09033v2">PDF</a> Code is available at <a target="_blank" rel="noopener" href="https://github.com/ulab-uiuc/Router-R1">https://github.com/ulab-uiuc/Router-R1</a>. Models   and Datasets are available at   <a target="_blank" rel="noopener" href="https://huggingface.co/collections/ulab-ai/router-r1-6851bbe099c7a56914b5db03">https://huggingface.co/collections/ulab-ai/router-r1-6851bbe099c7a56914b5db03</a></p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†åŸºäºå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰çš„è·¯ç”±å™¨Router-R1æ¡†æ¶ï¼Œç”¨äºè§£å†³å¤šå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„è·¯ç”±å’Œèšåˆé—®é¢˜ã€‚Router-R1å°†è·¯ç”±å™¨æœ¬èº«å®ä¾‹åŒ–ä¸ºä¸€å°å…·æœ‰æ¨ç†èƒ½åŠ›çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼Œé€šè¿‡å†…éƒ¨æ€è€ƒå’ŒåŠ¨æ€æ¨¡å‹è°ƒç”¨è¿›è¡Œå†³ç­–ï¼Œå¹¶å°†æ¯ä¸ªå“åº”é›†æˆåˆ°ä¸æ–­å˜åŒ–çš„ä¸Šä¸‹æ–‡ä¸­ã€‚é‡‡ç”¨è½»é‡çº§è§„åˆ™å¥–åŠ±æ¥å¹³è¡¡æ€§èƒ½å’Œæˆæœ¬ï¼Œå®ç°æ€§èƒ½ä¸æˆæœ¬çš„ä¼˜åŒ–ã€‚åœ¨ä¸ƒä¸ªé€šç”¨å’Œå¤šè·³é—®ç­”åŸºå‡†æµ‹è¯•ä¸Šï¼ŒRouter-R1è¡¨ç°å‡ºè¶…è¶Šå¤šä¸ªå¼ºå¤§åŸºå‡†çš„æ€§èƒ½ï¼Œåœ¨ä¿æŒä¼˜ç§€æ€§èƒ½çš„åŒæ—¶å®ç°äº†å¼ºå¤§çš„æ³›åŒ–å’Œæˆæœ¬ç®¡ç†ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„å¿«é€Ÿå‘å±•æ¨åŠ¨äº†LLMè·¯ç”±å™¨çš„å‡ºç°ï¼Œç”¨äºå°†ç”¨æˆ·æŸ¥è¯¢åˆ†é…ç»™æœ€åˆé€‚çš„æ¨¡å‹ã€‚</li>
<li>ç°æœ‰LLMè·¯ç”±å™¨é€šå¸¸é‡‡ç”¨å•ä¸€è½®æ¬¡ä¸€å¯¹ä¸€æ˜ å°„ï¼Œé™åˆ¶äº†å…¶å¤„ç†å¤æ‚ä»»åŠ¡çš„èƒ½åŠ›ã€‚</li>
<li>Router-R1æ¡†æ¶ä½¿ç”¨å¼ºåŒ–å­¦ä¹ æ¥åˆ¶å®šå¤šLLMè·¯ç”±å’Œèšåˆçš„å†³ç­–è¿‡ç¨‹ï¼Œå®ç°æ›´é«˜çº§çš„ä»»åŠ¡å¤„ç†ã€‚</li>
<li>Router-R1å°†è·¯ç”±å™¨å®ä¾‹åŒ–ä¸ºä¸€å°å…·æœ‰æ¨ç†èƒ½åŠ›çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼Œé€šè¿‡å†…éƒ¨æ€è€ƒå’ŒåŠ¨æ€æ¨¡å‹è°ƒç”¨è¿›è¡Œå†³ç­–ã€‚</li>
<li>é‡‡ç”¨è½»é‡çº§è§„åˆ™å¥–åŠ±æ¥å¹³è¡¡Router-R1çš„æ€§èƒ½å’Œæˆæœ¬ã€‚</li>
<li>Router-R1å…·æœ‰è‰¯å¥½çš„æ³›åŒ–èƒ½åŠ›ï¼Œå¯ä»¥é€‚åº”æœªè§çš„æ¨¡å‹é€‰æ‹©ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.09033">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-0e19a4192b4e8fd2cfc5e4866033a41c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8d1a08e8c7dbb9ea8fc7bc567907c814.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-bafe07447f0b6bfb6ba0025348a119c8.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="MMedAgent-RL-Optimizing-Multi-Agent-Collaboration-for-Multimodal-Medical-Reasoning"><a href="#MMedAgent-RL-Optimizing-Multi-Agent-Collaboration-for-Multimodal-Medical-Reasoning" class="headerlink" title="MMedAgent-RL: Optimizing Multi-Agent Collaboration for Multimodal   Medical Reasoning"></a>MMedAgent-RL: Optimizing Multi-Agent Collaboration for Multimodal   Medical Reasoning</h2><p><strong>Authors:Peng Xia, Jinglu Wang, Yibo Peng, Kaide Zeng, Xian Wu, Xiangru Tang, Hongtu Zhu, Yun Li, Shujie Liu, Yan Lu, Huaxiu Yao</strong></p>
<p>Medical Large Vision-Language Models (Med-LVLMs) have shown strong potential in multimodal diagnostic tasks. However, existing single-agent models struggle to generalize across diverse medical specialties, limiting their performance. Recent efforts introduce multi-agent collaboration frameworks inspired by clinical workflows, where general practitioners (GPs) and specialists interact in a fixed sequence. Despite improvements, these static pipelines lack flexibility and adaptability in reasoning. To address this, we propose MMedAgent-RL, a reinforcement learning (RL)-based multi-agent framework that enables dynamic, optimized collaboration among medical agents. Specifically, we train two GP agents based on Qwen2.5-VL via RL: the triage doctor learns to assign patients to appropriate specialties, while the attending physician integrates the judgments from multi-specialists and its own knowledge to make final decisions. To address the inconsistency in specialist outputs, we introduce a curriculum learning (CL)-guided RL strategy that progressively teaches the attending physician to balance between imitating specialists and correcting their mistakes. Experiments on five medical VQA benchmarks demonstrate that MMedAgent-RL not only outperforms both open-source and proprietary Med-LVLMs, but also exhibits human-like reasoning patterns. Notably, it achieves an average performance gain of 20.7% over supervised fine-tuning baselines. </p>
<blockquote>
<p>åŒ»ç–—å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆMed-LVLMsï¼‰åœ¨å¤šæ¨¡æ€è¯Šæ–­ä»»åŠ¡ä¸­æ˜¾ç¤ºå‡ºå¼ºå¤§çš„æ½œåŠ›ã€‚ç„¶è€Œï¼Œç°æœ‰çš„å•æ™ºèƒ½ä½“æ¨¡å‹åœ¨è·¨ä¸åŒåŒ»å­¦ä¸“ä¸šé¢†åŸŸè¿›è¡Œæ³›åŒ–æ—¶é‡åˆ°å›°éš¾ï¼Œé™åˆ¶äº†å…¶æ€§èƒ½ã€‚æœ€è¿‘çš„åŠªåŠ›å¼•å…¥äº†å—ä¸´åºŠå·¥ä½œæµç¨‹å¯å‘çš„å¤šæ™ºèƒ½ä½“åä½œæ¡†æ¶ï¼Œå…¶ä¸­å…¨ç§‘åŒ»ç”Ÿï¼ˆGPsï¼‰å’Œä¸“å®¶ä»¥å›ºå®šåºåˆ—è¿›è¡Œäº¤äº’ã€‚å°½ç®¡æœ‰æ‰€æ”¹è¿›ï¼Œä½†è¿™äº›é™æ€ç®¡é“åœ¨æ¨ç†æ–¹é¢ç¼ºä¹çµæ´»æ€§å’Œé€‚åº”æ€§ã€‚é’ˆå¯¹è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†åŸºäºå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰çš„å¤šæ™ºèƒ½ä½“æ¡†æ¶MMedAgent-RLï¼Œèƒ½å¤Ÿå®ç°åŒ»ç–—æ™ºèƒ½ä½“ä¹‹é—´çš„åŠ¨æ€ä¼˜åŒ–åä½œã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬åŸºäºQwen2.5-VLè®­ç»ƒäº†ä¸¤ä¸ªGPæ™ºèƒ½ä½“é€šè¿‡RLï¼šåˆ†è¯ŠåŒ»ç”Ÿå­¦ä¹ å°†æ‚£è€…åˆ†é…ç»™åˆé€‚çš„ä¸“ç§‘ï¼Œè€Œä¸»æ²»åŒ»å¸ˆåˆ™æ•´åˆäº†å¤šä¸“ç§‘åŒ»ç”Ÿçš„åˆ¤æ–­å’Œè‡ªå·±çš„çŸ¥è¯†æ¥åšå‡ºæœ€ç»ˆå†³å®šã€‚ä¸ºäº†è§£å†³ä¸“å®¶è¾“å‡ºä¸ä¸€è‡´çš„é—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§ä»¥è¯¾ç¨‹å­¦ä¹ ï¼ˆCLï¼‰å¼•å¯¼çš„RLç­–ç•¥ï¼Œé€æ­¥æ•™å¯¼ä¸»æ²»åŒ»å¸ˆå¦‚ä½•åœ¨æ¨¡ä»¿ä¸“å®¶å’Œçº æ­£å…¶é”™è¯¯ä¹‹é—´å–å¾—å¹³è¡¡ã€‚åœ¨äº”ä¸ªåŒ»ç–—VQAåŸºå‡†æµ‹è¯•ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒMMedAgent-RLä¸ä»…ä¼˜äºå¼€æºå’Œä¸“æœ‰Med-LVLMsï¼Œè¿˜è¡¨ç°å‡ºäººç±»èˆ¬çš„æ¨ç†æ¨¡å¼ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œä¸ç›‘ç£å¾®è°ƒåŸºå‡†ç›¸æ¯”ï¼Œå®ƒå®ç°äº†å¹³å‡æ€§èƒ½æå‡20.7%ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.00555v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>åŸºäºå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰çš„å¤šæ™ºèƒ½ä½“åä½œæ¡†æ¶MMedAgent-RLè¢«æå‡ºç”¨äºåŠ¨æ€ä¼˜åŒ–åŒ»ç–—æ™ºèƒ½ä½“ä¹‹é—´çš„åä½œï¼Œä»¥æé«˜å¤šæ¨¡æ€è¯Šæ–­ä»»åŠ¡çš„æ€§èƒ½ã€‚MMedAgent-RLå¼•å…¥äº†ä¸€ç§è¯¾ç¨‹å­¦ä¹ ï¼ˆCLï¼‰æŒ‡å¯¼çš„RLç­–ç•¥ï¼Œä»¥è§£å†³ä¸“ä¸šæ™ºèƒ½ä½“è¾“å‡ºä¸ä¸€è‡´çš„é—®é¢˜ã€‚åœ¨äº”ä¸ªåŒ»å­¦è§†è§‰é—®ç­”åŸºå‡†æµ‹è¯•ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒMMedAgent-RLä¸ä»…ä¼˜äºå¼€æºå’Œä¸“æœ‰åŒ»ç–—å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆMed-LVLMsï¼‰ï¼Œè€Œä¸”å±•ç°å‡ºä¸äººç±»ç±»ä¼¼çš„æ¨ç†æ¨¡å¼ï¼Œå¹³å‡æ€§èƒ½æå‡è¾¾20.7%ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Med-LVLMsåœ¨å¤šæ¨¡æ€è¯Šæ–­ä»»åŠ¡ä¸­å…·æœ‰å¼ºå¤§æ½œåŠ›ï¼Œä½†ç°æœ‰å•ä¸€æ™ºèƒ½ä½“æ¨¡å‹åœ¨è·¨ä¸åŒåŒ»å­¦ä¸“ä¸šé¢†åŸŸæ—¶éš¾ä»¥æ³›åŒ–ã€‚</li>
<li>å¼•å…¥å¤šæ™ºèƒ½ä½“åä½œæ¡†æ¶ä»¥æ”¹å–„å•ä¸€æ™ºèƒ½ä½“çš„å±€é™æ€§ï¼Œæ¨¡æ‹Ÿä¸´åºŠå·¥ä½œæµç¨‹ä¸­çš„GPå’Œä¸“å®¶äº’åŠ¨ã€‚</li>
<li>MMedAgent-RLåŸºäºå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰æ„å»ºï¼Œä½¿åŒ»ç–—æ™ºèƒ½ä½“ä¹‹é—´çš„åä½œæ›´åŠ åŠ¨æ€å’Œä¼˜åŒ–ã€‚</li>
<li>MMedAgent-RLåŒ…å«ä¸¤ä¸ªæ™ºèƒ½ä½“ï¼šåˆçº§åŒ»ç”Ÿè´Ÿè´£åˆ†é…æ‚£è€…åˆ°åˆé€‚çš„ä¸“ä¸šé¢†åŸŸï¼Œè€Œä¸»æ²»åŒ»å¸ˆåˆ™ç»“åˆå¤šä¸“ä¸šåˆ¤æ–­å’Œè‡ªèº«çŸ¥è¯†åšå‡ºæœ€ç»ˆå†³ç­–ã€‚</li>
<li>ä¸ºè§£å†³ä¸“å®¶æ™ºèƒ½ä½“è¾“å‡ºä¸ä¸€è‡´çš„é—®é¢˜ï¼Œå¼•å…¥è¯¾ç¨‹å­¦ä¹ ï¼ˆCLï¼‰æŒ‡å¯¼çš„å¼ºåŒ–å­¦ä¹ ç­–ç•¥ã€‚</li>
<li>å®éªŒè¯æ˜MMedAgent-RLä¼˜äºç°æœ‰çš„Med-LVLMsï¼Œå¹¶å±•ç°å‡ºä¸äººç±»ç±»ä¼¼çš„æ¨ç†æ¨¡å¼ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.00555">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-08eba53dde9732359b959145e04089c9.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-8d48ad5b31ad74fb012cae12e66f5460.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-6a7a09bfcd38080d39c8013b38cb6d16.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-58caaf28df0541314836b32ce524d8b4.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="J4R-Learning-to-Judge-with-Equivalent-Initial-State-Group-Relative-Policy-Optimization"><a href="#J4R-Learning-to-Judge-with-Equivalent-Initial-State-Group-Relative-Policy-Optimization" class="headerlink" title="J4R: Learning to Judge with Equivalent Initial State Group Relative   Policy Optimization"></a>J4R: Learning to Judge with Equivalent Initial State Group Relative   Policy Optimization</h2><p><strong>Authors:Austin Xu, Yilun Zhou, Xuan-Phi Nguyen, Caiming Xiong, Shafiq Joty</strong></p>
<p>To keep pace with the increasing pace of large language models (LLM) development, model output evaluation has transitioned away from time-consuming human evaluation to automatic evaluation, where LLMs themselves are tasked with assessing and critiquing other model outputs. LLM-as-judge models are a class of generative evaluators that excel in evaluating relatively simple domains, like chat quality, but struggle in reasoning intensive domains where model responses contain more substantive and challenging content. To remedy existing judge shortcomings, we explore training judges with reinforcement learning (RL). We make three key contributions: (1) We propose the Equivalent Initial State Group Relative Policy Optimization (EIS-GRPO) algorithm, which allows us to train our judge to be robust to positional biases that arise in more complex evaluation settings. (2) We introduce ReasoningJudgeBench, a benchmark that evaluates judges in diverse reasoning settings not covered by prior work. (3) We train Judge for Reasoning (J4R), a 7B judge trained with EIS-GRPO that outperforms GPT-4o and the next best small judge by 6.7% and 9%, matching or exceeding the performance of larger GRPO-trained judges on both JudgeBench and ReasoningJudgeBench. </p>
<blockquote>
<p>éšç€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å‘å±•æ­¥ä¼çš„åŠ å¿«ï¼Œæ¨¡å‹è¾“å‡ºè¯„ä¼°å·²ä»è€—æ—¶çš„äººåŠ›è¯„ä¼°è½¬å‘è‡ªåŠ¨è¯„ä¼°ã€‚åœ¨è‡ªåŠ¨è¯„ä¼°ä¸­ï¼ŒLLMæœ¬èº«è¢«èµ‹äºˆè¯„ä¼°å’Œæ‰¹åˆ¤å…¶ä»–æ¨¡å‹è¾“å‡ºçš„ä»»åŠ¡ã€‚LLMä½œä¸ºè¯„åˆ¤è€…çš„æ¨¡å‹æ˜¯ä¸€ç±»ç”Ÿæˆå¼è¯„ä¼°å™¨ï¼Œæ“…é•¿è¯„ä¼°ç›¸å¯¹ç®€å•çš„é¢†åŸŸï¼Œå¦‚èŠå¤©è´¨é‡ï¼Œä½†åœ¨éœ€è¦å¤§é‡æ¨ç†çš„é¢†åŸŸå´è¡¨ç°æŒ£æ‰ï¼Œå› ä¸ºè¿™äº›é¢†åŸŸçš„æ¨¡å‹å›åº”åŒ…å«æ›´å¤šå®è´¨æ€§å’Œå…·æœ‰æŒ‘æˆ˜çš„å†…å®¹ã€‚ä¸ºäº†å¼¥è¡¥ç°æœ‰è¯„åˆ¤è€…çš„ä¸è¶³ï¼Œæˆ‘ä»¬æ¢ç´¢ä½¿ç”¨å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰æ¥è®­ç»ƒè¯„åˆ¤è€…ã€‚æˆ‘ä»¬åšå‡ºäº†ä¸‰ä¸ªå…³é”®è´¡çŒ®ï¼šï¼ˆ1ï¼‰æˆ‘ä»¬æå‡ºäº†ç­‰æ•ˆåˆå§‹çŠ¶æ€ç»„ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆEIS-GRPOï¼‰ç®—æ³•ï¼Œè¯¥ç®—æ³•ä½¿æˆ‘ä»¬èƒ½å¤Ÿè®­ç»ƒè¯„åˆ¤è€…ï¼Œä»¥åº”å¯¹åœ¨æ›´å¤æ‚è¯„ä¼°ç¯å¢ƒä¸­å‡ºç°çš„å®šä½åè§ã€‚ ï¼ˆ2ï¼‰æˆ‘ä»¬å¼•å…¥äº†ReasoningJudgeBenchï¼Œè¿™æ˜¯ä¸€ä¸ªå¯ä»¥åœ¨ä»¥å‰çš„å·¥ä½œæœªæ¶µç›–çš„å¤šç§æ¨ç†ç¯å¢ƒä¸­è¯„ä¼°è¯„åˆ¤è€…çš„åŸºå‡†æµ‹è¯•ã€‚ï¼ˆ3ï¼‰æˆ‘ä»¬ä½¿ç”¨EIS-GRPOè®­ç»ƒäº†ç”¨äºæ¨ç†çš„è¯„åˆ¤è€…ï¼ˆJ4Rï¼‰ï¼Œè¿™æ˜¯ä¸€ä¸ª7Bçš„è¯„åˆ¤è€…ï¼Œå…¶æ€§èƒ½ä¼˜äºGPT-4oå’Œä¸‹ä¸€ä¸ªæœ€å¥½çš„å°å‹è¯„åˆ¤è€…6.7%å’Œ9%ï¼Œåœ¨JudgeBenchå’ŒReasoningJudgeBenchä¸Šçš„è¡¨ç°ä¸è¾ƒå¤§çš„GRPOè®­ç»ƒè¿‡çš„è¯„åˆ¤è€…ç›¸åŒ¹é…æˆ–æ›´å¥½ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.13346v3">PDF</a> 25 pages, 4 figures, 6 tables. Updated with code and benchmark</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†éšç€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„å‘å±•ï¼Œæ¨¡å‹è¾“å‡ºè¯„ä¼°å·²ä»è€—æ—¶çš„äººåŠ›è¯„ä¼°è½¬å‘è‡ªåŠ¨è¯„ä¼°ã€‚æ–‡ç« æ¢è®¨äº†LLMä½œä¸ºè¯„åˆ¤è€…çš„è¯„ä»·æ–¹å¼ï¼Œå¹¶æå‡ºäº†ä¸€ç§é’ˆå¯¹å¤æ‚è¯„ä¼°ç¯å¢ƒçš„ä¼˜åŒ–ç®—æ³•â€”â€”ç­‰æ•ˆåˆå§‹çŠ¶æ€ç»„ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆEIS-GRPOï¼‰ã€‚æ­¤å¤–ï¼Œæ–‡ç« è¿˜ä»‹ç»äº†ä¸€ä¸ªç”¨äºè¯„ä¼°æ¨ç†åœºæ™¯çš„åŸºå‡†æµ‹è¯•ReasoningJudgeBenchï¼Œå¹¶è®­ç»ƒäº†ä¸€ä¸ªåä¸ºJ4Rçš„7Bå‚æ•°æ¨ç†è¯„å§”æ¨¡å‹ï¼Œè¯¥æ¨¡å‹åœ¨æ€§èƒ½æµ‹è¯•ä¸Šè¡¨ç°å‡ºä¼˜å¼‚çš„æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„å‘å±•æ¨åŠ¨äº†æ¨¡å‹è¾“å‡ºè¯„ä¼°ä»äººåŠ›è¯„ä¼°å‘è‡ªåŠ¨è¯„ä¼°çš„è½¬å˜ã€‚</li>
<li>LLMä½œä¸ºè¯„åˆ¤è€…çš„è¯„ä»·æ–¹å¼åœ¨ç®€å•é¢†åŸŸè¡¨ç°è‰¯å¥½ï¼Œä½†åœ¨éœ€è¦æ¨ç†çš„å¤æ‚é¢†åŸŸå­˜åœ¨æŒ‘æˆ˜ã€‚</li>
<li>æå‡ºäº†ç­‰æ•ˆåˆå§‹çŠ¶æ€ç»„ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆEIS-GRPOï¼‰ç®—æ³•ï¼Œæå‡è¯„å§”åœ¨å¤æ‚è¯„ä¼°ç¯å¢ƒä¸­çš„ç¨³å¥æ€§ã€‚</li>
<li>ä»‹ç»äº†ReasoningJudgeBenchåŸºå‡†æµ‹è¯•ï¼Œç”¨äºè¯„ä¼°æ¨ç†åœºæ™¯çš„è¯„å§”æ€§èƒ½ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.13346">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-2381be16a29f1d7bbbdac73f6e758eca.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-fd2e79f44555ed40cdff875cd770a026.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-eb972d15aa503412e3b0a4847f7f9899.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-a8b703b210ce54431ca9c70043f728f3.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Patho-R1-A-Multimodal-Reinforcement-Learning-Based-Pathology-Expert-Reasoner"><a href="#Patho-R1-A-Multimodal-Reinforcement-Learning-Based-Pathology-Expert-Reasoner" class="headerlink" title="Patho-R1: A Multimodal Reinforcement Learning-Based Pathology Expert   Reasoner"></a>Patho-R1: A Multimodal Reinforcement Learning-Based Pathology Expert   Reasoner</h2><p><strong>Authors:Wenchuan Zhang, Penghao Zhang, Jingru Guo, Tao Cheng, Jie Chen, Shuwan Zhang, Zhang Zhang, Yuhao Yi, Hong Bu</strong></p>
<p>Recent advances in vision language models (VLMs) have enabled broad progress in the general medical field. However, pathology still remains a more challenging subdomain, with current pathology specific VLMs exhibiting limitations in both diagnostic accuracy and reasoning plausibility. Such shortcomings are largely attributable to the nature of current pathology datasets, which are primarily composed of image description pairs that lack the depth and structured diagnostic paradigms employed by real world pathologists. In this study, we leverage pathology textbooks and real world pathology experts to construct high-quality, reasoning-oriented datasets. Building on this, we introduce Patho-R1, a multimodal RL-based pathology Reasoner, trained through a three-stage pipeline: (1) continued pretraining on 3.5 million image-text pairs for knowledge infusion; (2) supervised fine-tuning on 500k high-quality Chain-of-Thought samples for reasoning incentivizing; (3) reinforcement learning using Group Relative Policy Optimization and Decoupled Clip and Dynamic sAmpling Policy Optimization strategies for multimodal reasoning quality refinement. To further assess the alignment quality of our dataset, we propose Patho-CLIP, trained on the same figure-caption corpus used for continued pretraining. Comprehensive experimental results demonstrate that both Patho-CLIP and Patho-R1 achieve robust performance across a wide range of pathology-related tasks, including zero-shot classification, cross-modal retrieval, Visual Question Answering, and Multiple Choice Question. Our project is available at the Patho-R1 repository: <a target="_blank" rel="noopener" href="https://github.com/Wenchuan-Zhang/Patho-R1">https://github.com/Wenchuan-Zhang/Patho-R1</a>. </p>
<blockquote>
<p>åœ¨è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰æ–¹é¢çš„æœ€æ–°è¿›å±•ä¸ºä¸€èˆ¬åŒ»å­¦é¢†åŸŸå¸¦æ¥äº†å¹¿æ³›çš„è¿›æ­¥ã€‚ç„¶è€Œï¼Œç—…ç†å­¦ä»ç„¶æ˜¯ä¸€ä¸ªæ›´å…·æŒ‘æˆ˜æ€§çš„å­åŸŸã€‚å½“å‰é’ˆå¯¹ç—…ç†å­¦çš„ç‰¹å®šVLMsåœ¨è¯Šæ–­å‡†ç¡®æ€§å’Œæ¨ç†åˆç†æ€§æ–¹é¢å­˜åœ¨å±€é™æ€§ã€‚è¿™ç§ç¼ºç‚¹åœ¨å¾ˆå¤§ç¨‹åº¦ä¸Šå½’å› äºå½“å‰ç—…ç†å­¦æ•°æ®é›†çš„æ€§è´¨ï¼Œè¿™äº›æ•°æ®é›†ä¸»è¦ç”±ç¼ºä¹ç°å®ä¸–ç•Œç—…ç†å­¦å®¶æ‰€ä½¿ç”¨æ·±åº¦å’Œç»“æ„åŒ–è¯Šæ–­èŒƒå¼çš„å›¾åƒæè¿°å¯¹ç»„æˆã€‚åœ¨è¿™é¡¹ç ”ç©¶ä¸­ï¼Œæˆ‘ä»¬åˆ©ç”¨ç—…ç†å­¦æ•™æå’Œç°å®ä¸–ç•Œç—…ç†å­¦ä¸“å®¶æ¥æ„å»ºé«˜è´¨é‡ã€ä»¥æ¨ç†ä¸ºå¯¼å‘çš„æ•°æ®é›†ã€‚åœ¨æ­¤åŸºç¡€ä¸Šï¼Œæˆ‘ä»¬å¼•å…¥äº†Patho-R1ï¼Œè¿™æ˜¯ä¸€ä¸ªåŸºäºå¤šæ¨¡å¼å¼ºåŒ–å­¦ä¹ çš„ç—…ç†å­¦æ¨ç†å™¨ï¼Œé€šè¿‡ä¸‰ä¸ªé˜¶æ®µè¿›è¡Œè®­ç»ƒï¼š1ï¼‰åœ¨350ä¸‡å›¾åƒæ–‡æœ¬å¯¹ä¸ŠæŒç»­è¿›è¡Œé¢„è®­ç»ƒä»¥æ³¨å…¥çŸ¥è¯†ï¼›2ï¼‰åœ¨50ä¸‡ä¸ªé«˜è´¨é‡çš„æ€ç»´é“¾æ ·æœ¬ä¸Šè¿›è¡Œç›‘ç£å¾®è°ƒä»¥æ¿€åŠ±æ¨ç†ï¼›3ï¼‰ä½¿ç”¨é›†å›¢ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–å’Œè„±é’©Clipä»¥åŠåŠ¨æ€é‡‡æ ·ç­–ç•¥ä¼˜åŒ–ç­–ç•¥è¿›è¡Œå¼ºåŒ–å­¦ä¹ ï¼Œä»¥å®ç°å¤šæ¨¡å¼æ¨ç†è´¨é‡çš„æ”¹è¿›ã€‚ä¸ºäº†è¿›ä¸€æ­¥è¯„ä¼°æˆ‘ä»¬æ•°æ®é›†çš„å¯¹é½è´¨é‡ï¼Œæˆ‘ä»¬æå‡ºäº†åœ¨åŒä¸€å›¾-æ ‡é¢˜è¯­æ–™åº“ä¸Šè®­ç»ƒçš„Patho-CLIPï¼Œç”¨äºæŒç»­é¢„è®­ç»ƒã€‚ç»¼åˆå®éªŒç»“æœè¡¨æ˜ï¼ŒPatho-CLIPå’ŒPatho-R1åœ¨å¹¿æ³›çš„ç—…ç†å­¦ç›¸å…³ä»»åŠ¡ä¸­è¡¨ç°å‡ºç¨³å¥çš„æ€§èƒ½ï¼ŒåŒ…æ‹¬é›¶æ ·æœ¬åˆ†ç±»ã€è·¨æ¨¡æ€æ£€ç´¢ã€è§†è§‰é—®ç­”å’Œå¤šé¡¹é€‰æ‹©é¢˜ã€‚æˆ‘ä»¬çš„é¡¹ç›®å¯åœ¨Patho-R1ä»“åº“ä¸­æ‰¾åˆ°ï¼š<a target="_blank" rel="noopener" href="https://github.com/Wenchuan-Zhang/Patho-R1%E3%80%82">https://github.com/Wenchuan-Zhang/Patho-R1ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.11404v3">PDF</a> </p>
<p><strong>Summary</strong>ï¼š<br>æœ€æ–°è¿›å±•çš„è·¨æ¨¡æ€è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰åœ¨åŒ»å­¦é¢†åŸŸå–å¾—äº†å¹¿æ³›è¿›å±•ï¼Œä½†åœ¨ç—…ç†å­¦é¢†åŸŸä»å­˜åœ¨æŒ‘æˆ˜ã€‚å½“å‰ç—…ç†å­¦ç‰¹å®šVLMsåœ¨è¯Šæ–­å‡†ç¡®æ€§å’Œæ¨ç†åˆç†æ€§æ–¹é¢å­˜åœ¨å±€é™æ€§ã€‚æœ¬ç ”ç©¶é€šè¿‡åˆ©ç”¨ç—…ç†å­¦æ•™æå’Œç—…ç†å­¦ä¸“å®¶æ„å»ºé«˜è´¨é‡ã€æ¨ç†å¯¼å‘çš„æ•°æ®é›†ï¼Œå¹¶å¼•å…¥Patho-R1å¤šæ¨¡æ€å¼ºåŒ–å­¦ä¹ ç—…ç†å­¦æ¨ç†å™¨ï¼Œç»è¿‡ä¸‰é˜¶æ®µè®­ç»ƒï¼Œå®ç°äº†æ¨¡å‹ä¼˜åŒ–ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜æ¨å‡ºäº†Patho-CLIPï¼Œç”¨äºè¯„ä¼°æ•°æ®é›†çš„è´¨é‡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒPatho-CLIPå’ŒPatho-R1åœ¨å¤šç§ç—…ç†å­¦ç›¸å…³ä»»åŠ¡ä¸­è¡¨ç°ç¨³å¥ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>VLMsåœ¨åŒ»å­¦é¢†åŸŸå–å¾—è¿›å±•ï¼Œä½†ç—…ç†å­¦ä»æ˜¯æŒ‘æˆ˜é¢†åŸŸã€‚</li>
<li>å½“å‰ç—…ç†å­¦ç‰¹å®šVLMsåœ¨è¯Šæ–­å‡†ç¡®æ€§å’Œæ¨ç†åˆç†æ€§æ–¹é¢å­˜åœ¨å±€é™æ€§ã€‚</li>
<li>ç ”ç©¶åˆ©ç”¨ç—…ç†å­¦æ•™æå’Œä¸“å®¶æ„å»ºé«˜è´¨é‡æ•°æ®é›†ã€‚</li>
<li>å¼•å…¥Patho-R1å¤šæ¨¡æ€å¼ºåŒ–å­¦ä¹ ç—…ç†å­¦æ¨ç†å™¨ï¼Œç»è¿‡ä¸‰é˜¶æ®µè®­ç»ƒå®ç°æ¨¡å‹ä¼˜åŒ–ã€‚</li>
<li>Patho-CLIPç”¨äºè¯„ä¼°æ•°æ®é›†è´¨é‡ã€‚</li>
<li>Patho-CLIPå’ŒPatho-R1åœ¨å¤šç§ç—…ç†å­¦ç›¸å…³ä»»åŠ¡ä¸­è¡¨ç°ç¨³å¥ã€‚</li>
<li>é¡¹ç›®å·²åœ¨Patho-R1ä»“åº“ä¸­å¼€æ”¾è·å–ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.11404">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-93baa5de69a328cb936ba05c72f02848.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-933359ff6a19ce46474a6f31df7bb350.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-7065b3b41f7382fdee2101a8fa443e8c.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Trust-Region-Preference-Approximation-A-simple-and-stable-reinforcement-learning-algorithm-for-LLM-reasoning"><a href="#Trust-Region-Preference-Approximation-A-simple-and-stable-reinforcement-learning-algorithm-for-LLM-reasoning" class="headerlink" title="Trust Region Preference Approximation: A simple and stable reinforcement   learning algorithm for LLM reasoning"></a>Trust Region Preference Approximation: A simple and stable reinforcement   learning algorithm for LLM reasoning</h2><p><strong>Authors:Xuerui Su, Shufang Xie, Guoqing Liu, Yingce Xia, Renqian Luo, Peiran Jin, Zhiming Ma, Yue Wang, Zun Wang, Yuting Liu</strong></p>
<p>Recently, Large Language Models (LLMs) have rapidly evolved, approaching Artificial General Intelligence (AGI) while benefiting from large-scale reinforcement learning to enhance Human Alignment (HA) and Reasoning. Recent reward-based optimization algorithms, such as Proximal Policy Optimization (PPO) and Group Relative Policy Optimization (GRPO) have achieved significant performance on reasoning tasks, whereas preference-based optimization algorithms such as Direct Preference Optimization (DPO) significantly improve the performance of LLMs on human alignment. However, despite the strong performance of reward-based optimization methods in alignment tasks , they remain vulnerable to reward hacking. Furthermore, preference-based algorithms (such as Online DPO) havenâ€™t yet matched the performance of reward-based optimization algorithms (like PPO) on reasoning tasks, making their exploration in this specific area still a worthwhile pursuit. Motivated by these challenges, we propose the Trust Region Preference Approximation (TRPA) algorithm, which integrates rule-based optimization with preference-based optimization for reasoning tasks. As a preference-based algorithm, TRPA naturally eliminates the reward hacking issue. TRPA constructs preference levels using predefined rules, forms corresponding preference pairs, and leverages a novel optimization algorithm for RL training with a theoretical monotonic improvement guarantee. Experimental results demonstrate that TRPA not only achieves competitive performance on reasoning tasks but also exhibits robust stability. The code of this paper are released and updating on <a target="_blank" rel="noopener" href="https://github.com/XueruiSu/Trust-Region-Preference-Approximation.git">https://github.com/XueruiSu/Trust-Region-Preference-Approximation.git</a>. </p>
<blockquote>
<p>æœ€è¿‘ï¼Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è¿…é€Ÿè¿›åŒ–ï¼Œåœ¨å—ç›Šäºå¤§è§„æ¨¡å¼ºåŒ–å­¦ä¹ æé«˜äººç±»å¯¹é½ï¼ˆHAï¼‰å’Œæ¨ç†çš„åŒæ—¶ï¼Œé€æ¸æ¥è¿‘äººå·¥æ™ºèƒ½é€šç”¨æ™ºèƒ½ï¼ˆAGIï¼‰ã€‚æœ€è¿‘çš„åŸºäºå¥–åŠ±çš„ä¼˜åŒ–ç®—æ³•ï¼Œå¦‚è¿‘ç«¯ç­–ç•¥ä¼˜åŒ–ï¼ˆPPOï¼‰å’Œç¾¤ä½“ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆGRPOï¼‰ï¼Œåœ¨æ¨ç†ä»»åŠ¡ä¸Šå–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½ï¼›è€ŒåŸºäºåå¥½çš„ä¼˜åŒ–ç®—æ³•ï¼Œå¦‚ç›´æ¥åå¥½ä¼˜åŒ–ï¼ˆDPOï¼‰ï¼Œåˆ™æ˜¾è‘—æé«˜äº†è¯­è¨€æ¨¡å‹åœ¨äººå¯¹é½æ–¹é¢çš„æ€§èƒ½ã€‚ç„¶è€Œï¼Œå°½ç®¡åŸºäºå¥–åŠ±çš„ä¼˜åŒ–æ–¹æ³•åœ¨å¯¹é½ä»»åŠ¡ä¸­è¡¨ç°å‡ºå¼ºå¤§çš„æ€§èƒ½ï¼Œä½†å®ƒä»¬ä»ç„¶å®¹æ˜“å—åˆ°å¥–åŠ±é»‘å®¢æ”»å‡»çš„å½±å“ã€‚æ­¤å¤–ï¼ŒåŸºäºåå¥½çš„ç®—æ³•ï¼ˆå¦‚åœ¨çº¿DPOï¼‰åœ¨æ¨ç†ä»»åŠ¡ä¸Šçš„æ€§èƒ½å°šæœªè¾¾åˆ°åŸºäºå¥–åŠ±çš„ä¼˜åŒ–ç®—æ³•ï¼ˆå¦‚PPOï¼‰çš„æ°´å¹³ï¼Œè¿™ä½¿å¾—åœ¨è¿™ä¸€ç‰¹å®šé¢†åŸŸå¯¹å…¶è¿›è¡Œæ¢ç´¢ä»ç„¶æ˜¯æœ‰ä»·å€¼çš„ã€‚é’ˆå¯¹è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¿¡ä»»åŒºåŸŸåå¥½é€¼è¿‘ï¼ˆTRPAï¼‰ç®—æ³•ï¼Œè¯¥ç®—æ³•å°†åŸºäºè§„åˆ™çš„ä¼˜åŒ–ä¸åŸºäºåå¥½çš„ä¼˜åŒ–ç›¸ç»“åˆï¼Œç”¨äºæ¨ç†ä»»åŠ¡ã€‚ä½œä¸ºåŸºäºåå¥½çš„ç®—æ³•ï¼ŒTRPAè‡ªç„¶åœ°æ¶ˆé™¤äº†å¥–åŠ±é»‘å®¢æ”»å‡»çš„é—®é¢˜ã€‚TRPAä½¿ç”¨é¢„å®šä¹‰è§„åˆ™æ„å»ºåå¥½å±‚æ¬¡ï¼Œå½¢æˆç›¸åº”çš„åå¥½å¯¹ï¼Œå¹¶åˆ©ç”¨ä¸€ç§æ–°çš„ä¼˜åŒ–ç®—æ³•è¿›è¡ŒRLè®­ç»ƒï¼Œè¯¥ç®—æ³•å…·æœ‰ç†è®ºä¸Šçš„å•è°ƒæ”¹è¿›ä¿è¯ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒTRPAä¸ä»…åœ¨æ¨ç†ä»»åŠ¡ä¸Šå…·æœ‰ç«äº‰åŠ›ï¼Œè€Œä¸”è¡¨ç°å‡ºç¨³å¥çš„ç¨³å®šæ€§ã€‚æœ¬æ–‡çš„ä»£ç å·²å‘å¸ƒå¹¶åœ¨<a target="_blank" rel="noopener" href="https://github.com/XueruiSu/Trust-Region-Preference-Approximation.git%E4%B8%8A%E8%BF%9B%E8%A1%8C%E6%9B%B4%E6%96%B0%E3%80%82">https://github.com/XueruiSu/Trust-Region-Preference-Approximation.gitä¸Šè¿›è¡Œæ›´æ–°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.04524v2">PDF</a> 10pages</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ¢è®¨äº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨äººå·¥é€šç”¨æ™ºèƒ½ï¼ˆAGIï¼‰æ–¹é¢çš„æœ€æ–°è¿›å±•ã€‚æ–‡ç« ä»‹ç»äº†åŸºäºå¥–åŠ±çš„ä¼˜åŒ–ç®—æ³•ï¼Œå¦‚è¿‘ç«¯ç­–ç•¥ä¼˜åŒ–ï¼ˆPPOï¼‰å’Œç¾¤ä½“ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆGRPOï¼‰ï¼Œåœ¨æ¨ç†ä»»åŠ¡ä¸Šçš„è¡¨ç°ã€‚åŒæ—¶ï¼Œä¹Ÿæåˆ°äº†åŸºäºåå¥½çš„ä¼˜åŒ–ç®—æ³•ï¼Œå¦‚ç›´æ¥åå¥½ä¼˜åŒ–ï¼ˆDPOï¼‰ï¼Œåœ¨LLMçš„äººç±»å¯¹é½æ–¹é¢è¡¨ç°å‡ºè‰²ã€‚é’ˆå¯¹å¥–åŠ±åŸºäºçš„ä¼˜åŒ–æ–¹æ³•åœ¨äººç±»å¯¹é½ä»»åŠ¡ä¸­çš„æ˜“å—æ”»å‡»é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§æ–°çš„ç®—æ³•â€”â€”ä¿¡ä»»åŒºåŸŸåå¥½è¿‘ä¼¼ï¼ˆTRPAï¼‰ã€‚è¯¥ç®—æ³•ç»“åˆäº†åŸºäºè§„åˆ™çš„ä¼˜åŒ–å’ŒåŸºäºåå¥½çš„ä¼˜åŒ–ï¼Œåœ¨æ¨ç†ä»»åŠ¡ä¸Šå…·æœ‰ç«äº‰åŠ›ï¼Œä¸”è¡¨ç°ç¨³å¥ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨äººå·¥é€šç”¨æ™ºèƒ½ï¼ˆAGIï¼‰æ–¹é¢å–å¾—é‡è¦è¿›å±•ã€‚</li>
<li>åŸºäºå¥–åŠ±çš„ä¼˜åŒ–ç®—æ³•ï¼Œå¦‚è¿‘ç«¯ç­–ç•¥ä¼˜åŒ–ï¼ˆPPOï¼‰å’Œç¾¤ä½“ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆGRPOï¼‰ï¼Œåœ¨æ¨ç†ä»»åŠ¡ä¸Šè¡¨ç°å‡ºæ˜¾è‘—æ€§èƒ½ã€‚</li>
<li>åŸºäºåå¥½çš„ä¼˜åŒ–ç®—æ³•ï¼Œå¦‚ç›´æ¥åå¥½ä¼˜åŒ–ï¼ˆDPOï¼‰ï¼Œæå‡äº†LLMåœ¨äººç±»å¯¹é½æ–¹é¢çš„æ€§èƒ½ã€‚</li>
<li>ç°æœ‰å¥–åŠ±åŸºäºçš„ä¼˜åŒ–æ–¹æ³•åœ¨äººç±»å¯¹é½ä»»åŠ¡ä¸­æ˜“å—åˆ°â€œå¥–åŠ±é»‘å®¢â€æ”»å‡»ã€‚</li>
<li>ä¿¡ä»»åŒºåŸŸåå¥½è¿‘ä¼¼ï¼ˆTRPAï¼‰ç®—æ³•ç»“åˆäº†åŸºäºè§„åˆ™å’ŒåŸºäºåå¥½çš„ä¼˜åŒ–ï¼Œè§£å†³äº†å¥–åŠ±é»‘å®¢é—®é¢˜ã€‚</li>
<li>TRPAç®—æ³•åœ¨æ¨ç†ä»»åŠ¡ä¸Šå…·æœ‰ç«äº‰åŠ›ï¼Œä¸”è¡¨ç°ç¨³å¥ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.04524">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-fc4c4b4c6a67b069c15370c9d1f8254f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-7d765a4de0862ee789740c4c99305307.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2bc7b8d18cfb7ac01c976d1733a6f0e2.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Training-Hybrid-Deep-Quantum-Neural-Network-for-Efficient-Reinforcement-Learning"><a href="#Training-Hybrid-Deep-Quantum-Neural-Network-for-Efficient-Reinforcement-Learning" class="headerlink" title="Training Hybrid Deep Quantum Neural Network for Efficient Reinforcement   Learning"></a>Training Hybrid Deep Quantum Neural Network for Efficient Reinforcement   Learning</h2><p><strong>Authors:Jie Luo, Xueyin Chen, Jeremy Kulcsar, Georgios Korpas, Giulio Giaconi</strong></p>
<p>Quantum circuits embed data in a Hilbert space whose dimensionality grows exponentially with the number of qubits, allowing even shallow parameterised quantum circuits (PQCs) to represent highly-correlated probability distributions that are costly for classical networks to capture. Reinforcement-learning (RL) agents, which must reason over long-horizon, continuous-control tasks, stand to benefit from this expressive quantum feature space, but only if the quantum layers can be trained jointly with the surrounding deep-neural components. Current gradient-estimation techniques (e.g., parameter-shift rule) make such hybrid training impractical for realistic RL workloads, because every gradient step requires a prohibitive number of circuit evaluations and thus erodes the potential quantum advantage. We introduce qtDNN, a tangential surrogate that locally approximates a PQC with a small differentiable network trained on-the-fly from the same minibatch. Embedding qtDNN inside the computation graph yields scalable batch gradients while keeping the original quantum layer for inference. Building on qtDNN we design hDQNN-TD3, a hybrid deep quantum neural network for continuous-control reinforcement learning based on the TD3 architecture. On the high-dimensional Humanoid-v4 benchmark, our agent reaches a test return that surpasses classical TD3, SAC and PPO baselines trained with identical compute. To our knowledge this is the first PQC-enhanced policy that matches or exceeds state-of-the-art classical performance on Humanoid. qtDNN has the potential to reduce quantum-hardware calls significantly and is designed to be compatible with todayâ€™s NISQ devices. The method opens a path toward applying hybrid quantum models to large-scale RL and other gradient-intensive machine-learning tasks. </p>
<blockquote>
<p>é‡å­ç”µè·¯å°†æ•°æ®åµŒå…¥åˆ°ä¸€ä¸ªå¸Œå°”ä¼¯ç‰¹ç©ºé—´ä¸­ï¼Œå…¶ç»´åº¦éšç€é‡å­æ¯”ç‰¹æ•°é‡çš„å¢åŠ è€Œå‘ˆæŒ‡æ•°å¢é•¿ã€‚è¿™ä½¿å¾—å³ä½¿æ˜¯æµ…å‚æ•°åŒ–é‡å­ç”µè·¯ï¼ˆPQCsï¼‰ä¹Ÿèƒ½è¡¨ç¤ºé«˜åº¦ç›¸å…³çš„æ¦‚ç‡åˆ†å¸ƒï¼Œè¿™å¯¹äºç»å…¸ç½‘ç»œæ¥è¯´å¾ˆéš¾æ•æ‰ã€‚å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰ä»£ç†éœ€è¦åœ¨é•¿æœŸå’Œè¿ç»­æ§åˆ¶ä»»åŠ¡ä¸­è¿›è¡Œæ¨ç†ï¼Œå¯ä»¥ä»è¿™ç§è¡¨è¾¾æ€§é‡å­ç‰¹å¾ç©ºé—´ä¸­å—ç›Šï¼Œä½†å‰ææ¡ä»¶æ˜¯é‡å­å±‚èƒ½å¤Ÿä¸å‘¨å›´çš„æ·±å±‚ç¥ç»ç½‘ç»œç»„ä»¶è”åˆè®­ç»ƒã€‚å½“å‰çš„æ¢¯åº¦ä¼°è®¡æŠ€æœ¯ï¼ˆä¾‹å¦‚å‚æ•°ç§»åŠ¨è§„åˆ™ï¼‰ä½¿å¾—è¿™ç§æ··åˆè®­ç»ƒå¯¹äºå®é™…çš„RLå·¥ä½œé‡æ¥è¯´ä¸åˆ‡å®é™…ï¼Œå› ä¸ºæ¯ä¸ªæ¢¯åº¦æ­¥éª¤éƒ½éœ€è¦å¤§é‡çš„ç”µè·¯è¯„ä¼°ï¼Œä»è€Œå‰Šå¼±äº†æ½œåœ¨çš„é‡å­ä¼˜åŠ¿ã€‚æˆ‘ä»¬å¼•å…¥äº†qtDNNï¼Œå®ƒæ˜¯ä¸€ç§å±€éƒ¨ä½¿ç”¨å°å‹å¯å¾®ç½‘ç»œè¿‘ä¼¼PQCçš„åˆ‡çº¿æ›¿ä»£å“ï¼Œè¯¥ç½‘ç»œæ ¹æ®ç›¸åŒçš„mini-batchè¿›è¡Œå®æ—¶è®­ç»ƒã€‚å°†qtDNNåµŒå…¥è®¡ç®—å›¾ä¸­å¯ä»¥åœ¨ä¿æŒåŸå§‹é‡å­å±‚è¿›è¡Œæ¨ç†çš„åŒæ—¶ï¼Œå®ç°å¯æ‰©å±•çš„æ‰¹é‡æ¢¯åº¦ã€‚åŸºäºqtDNNï¼Œæˆ‘ä»¬è®¾è®¡äº†ç”¨äºåŸºäºTD3æ¶æ„çš„è¿ç»­æ§åˆ¶å¼ºåŒ–å­¦ä¹ çš„hDQNN-TD3æ··åˆæ·±åº¦é‡å­ç¥ç»ç½‘ç»œã€‚åœ¨é«˜ç»´Humanoid-v4åŸºå‡†æµ‹è¯•ä¸­ï¼Œæˆ‘ä»¬çš„ä»£ç†çš„æµ‹è¯•ç»“æœè¶…è¿‡äº†ä½¿ç”¨ç›¸åŒè®¡ç®—èµ„æºè®­ç»ƒçš„ç»å…¸TD3ã€SACå’ŒPPOåŸºå‡†çº¿ã€‚æ®æˆ‘ä»¬æ‰€çŸ¥ï¼Œè¿™æ˜¯ç¬¬ä¸€ä¸ªå¢å¼ºç­–ç•¥çš„PQCï¼Œåœ¨Humanoidä¸Šè¾¾åˆ°äº†æˆ–è¶…è¿‡äº†æœ€æ–°ç»å…¸æŠ€æœ¯çš„æ€§èƒ½ã€‚qtDNNæœ‰æœ›æ˜¾è‘—é™ä½å¯¹é‡å­ç¡¬ä»¶çš„è°ƒç”¨ï¼Œå¹¶ä¸”è®¾è®¡å…¼å®¹å½“ä»Šçš„NISQè®¾å¤‡ã€‚è¯¥æ–¹æ³•ä¸ºå°†æ··åˆé‡å­æ¨¡å‹åº”ç”¨äºå¤§è§„æ¨¡RLå’Œå…¶ä»–æ¢¯åº¦å¯†é›†å‹æœºå™¨å­¦ä¹ ä»»åŠ¡å¼€è¾Ÿäº†ä¸€æ¡é“è·¯ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.09119v5">PDF</a> </p>
<p><strong>Summary</strong>ï¼šé‡å­ç”µè·¯åµŒå…¥Hilbertç©ºé—´çš„æ•°æ®è¡¨ç°å‡ºæŒ‡æ•°çº§å¢é•¿çš„ç‰¹ç‚¹ï¼Œå…è®¸æµ…å±‚å‚æ•°åŒ–é‡å­ç”µè·¯ï¼ˆPQCsï¼‰è¡¨å¾é«˜åº¦ç›¸å…³çš„æ¦‚ç‡åˆ†å¸ƒï¼Œè¿™å¯¹äºç»å…¸ç½‘ç»œè€Œè¨€æ˜¯æˆæœ¬é«˜æ˜‚çš„ã€‚å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰åœ¨å¤„ç†é•¿æœŸè¿ç»­æ§åˆ¶ä»»åŠ¡æ—¶ï¼Œæœ‰æœ›å—ç›Šäºè¿™ç§è¡¨è¾¾æ€§å¼ºçš„é‡å­ç‰¹å¾ç©ºé—´ï¼Œä½†å‰ææ˜¯é‡å­å±‚èƒ½ä¸å‘¨å›´çš„æ·±åº¦ç¥ç»ç½‘ç»œç»„ä»¶è”åˆè®­ç»ƒã€‚å½“å‰æ¢¯åº¦ä¼°è®¡æŠ€æœ¯ä½¿å¾—è¿™ç§æ··åˆè®­ç»ƒå¯¹äºå®é™…RLå·¥ä½œè´Ÿè½½ä¸åˆ‡å®é™…ï¼Œå› ä¸ºæ¯ä¸ªæ¢¯åº¦æ­¥éª¤éƒ½éœ€è¦å¤§é‡çš„ç”µè·¯è¯„ä¼°ï¼Œä»è€Œå‰Šå¼±äº†æ½œåœ¨çš„é‡å­ä¼˜åŠ¿ã€‚æœ¬ç ”ç©¶å¼•å…¥äº†qtDNNï¼Œä¸€ç§å±€éƒ¨ä½¿ç”¨å°å‹å¯å¾®ç½‘ç»œé€¼è¿‘PQCçš„æ–¹æ³•ï¼Œè¯¥ç½‘ç»œåœ¨åŒä¸€å°æ‰¹é‡æ•°æ®ä¸­å³æ—¶è®­ç»ƒã€‚å°†qtDNNåµŒå…¥è®¡ç®—å›¾ä¸­å¯å®ç°å¯æ‰©å±•çš„æ‰¹é‡æ¢¯åº¦ï¼ŒåŒæ—¶ä¿ç•™åŸå§‹é‡å­å±‚è¿›è¡Œæ¨ç†ã€‚åŸºäºqtDNNï¼Œæœ¬ç ”ç©¶è®¾è®¡äº†ç”¨äºè¿ç»­æ§åˆ¶å¼ºåŒ–å­¦ä¹ çš„æ··åˆæ·±åº¦é‡å­ç¥ç»ç½‘ç»œhDQNN-TD3ï¼Œè¯¥ç½‘ç»œåŸºäºTD3æ¶æ„ã€‚åœ¨é«˜ç»´Humanoid-v4åŸºå‡†æµ‹è¯•ä¸­ï¼Œæˆ‘ä»¬çš„ä»£ç†äººçš„æµ‹è¯•å›æŠ¥ç‡è¶…è¿‡äº†ä½¿ç”¨ç›¸åŒè®¡ç®—èµ„æºè®­ç»ƒçš„ç»å…¸TD3ã€SACå’ŒPPOåŸºå‡†çº¿ã€‚è¿™æ˜¯æ®æˆ‘ä»¬æ‰€çŸ¥é¦–æ¬¡å®ç°PQCå¢å¼ºçš„ç­–ç•¥åœ¨Humanoidä¸Šè¾¾åˆ°æˆ–è¶…è¿‡æœ€æ–°ç»å…¸æŠ€æœ¯çš„æ€§èƒ½ã€‚qtDNNæœ‰æœ›æ˜¾è‘—é™ä½å¯¹é‡å­ç¡¬ä»¶çš„è°ƒç”¨ï¼Œå¹¶ä¸”è®¾è®¡å…¼å®¹å½“ä»Šçš„NISQè®¾å¤‡ã€‚è¯¥æ–¹æ³•ä¸ºå°†æ··åˆé‡å­æ¨¡å‹åº”ç”¨äºå¤§è§„æ¨¡RLå’Œå…¶ä»–æ¢¯åº¦å¯†é›†å‹æœºå™¨å­¦ä¹ ä»»åŠ¡å¼€è¾Ÿäº†ä¸€æ¡é“è·¯ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>é‡å­ç”µè·¯å…è®¸å‚æ•°åŒ–é‡å­ç”µè·¯ï¼ˆPQCsï¼‰è¡¨å¾é«˜åº¦ç›¸å…³çš„æ¦‚ç‡åˆ†å¸ƒï¼Œè¿™ç»å…¸ç½‘ç»œéš¾ä»¥æœ‰æ•ˆå¤„ç†ã€‚</li>
<li>å¼ºåŒ–å­¦ä¹ åœ¨å¤„ç†é•¿æœŸè¿ç»­æ§åˆ¶ä»»åŠ¡æ—¶èƒ½ä»é‡å­ç‰¹å¾ç©ºé—´ä¸­å—ç›Šã€‚</li>
<li>å½“å‰æ¢¯åº¦ä¼°è®¡æŠ€æœ¯ä½¿å¾—è”åˆè®­ç»ƒé‡å­å±‚å’Œæ·±åº¦ç¥ç»ç½‘ç»œä¸åˆ‡å®é™…ã€‚</li>
<li>qtDNNé€šè¿‡ç”¨å°å‹å¯å¾®ç½‘ç»œå±€éƒ¨é€¼è¿‘PQCæ¥è§£å†³è¿™ä¸€é—®é¢˜ã€‚</li>
<li>hDQNN-TD3æ˜¯åŸºäºTD3æ¶æ„çš„æ··åˆæ·±åº¦é‡å­ç¥ç»ç½‘ç»œï¼Œå®ç°äº†åœ¨HumanoidåŸºå‡†æµ‹è¯•ä¸Šçš„è¶…è¶Šç»å…¸æŠ€æœ¯çš„æ€§èƒ½ã€‚</li>
<li>qtDNNè®¾è®¡å…¼å®¹å½“å‰NISQè®¾å¤‡ï¼Œå¹¶æœ‰æœ›é™ä½å¯¹é‡å­ç¡¬ä»¶çš„ä¾èµ–ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.09119">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-b755ffc092053bd685c55a8f1fd2ab2d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-011237890f532ce7dc149cd0fa905b1f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-23d81878b1b2d24a78f70b648bc59f86.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Inst3D-LMM-Instance-Aware-3D-Scene-Understanding-with-Multi-modal-Instruction-Tuning"><a href="#Inst3D-LMM-Instance-Aware-3D-Scene-Understanding-with-Multi-modal-Instruction-Tuning" class="headerlink" title="Inst3D-LMM: Instance-Aware 3D Scene Understanding with Multi-modal   Instruction Tuning"></a>Inst3D-LMM: Instance-Aware 3D Scene Understanding with Multi-modal   Instruction Tuning</h2><p><strong>Authors:Hanxun Yu, Wentong Li, Song Wang, Junbo Chen, Jianke Zhu</strong></p>
<p>Despite encouraging progress in 3D scene understanding, it remains challenging to develop an effective Large Multi-modal Model (LMM) that is capable of understanding and reasoning in complex 3D environments. Most previous methods typically encode 3D point and 2D image features separately, neglecting interactions between 2D semantics and 3D object properties, as well as the spatial relationships within the 3D environment. This limitation not only hinders comprehensive representations of 3D scene, but also compromises training and inference efficiency. To address these challenges, we propose a unified Instance-aware 3D Large Multi-modal Model (Inst3D-LMM) to deal with multiple 3D scene understanding tasks simultaneously. To obtain the fine-grained instance-level visual tokens, we first introduce a novel Multi-view Cross-Modal Fusion (MCMF) module to inject the multi-view 2D semantics into their corresponding 3D geometric features. For scene-level relation-aware tokens, we further present a 3D Instance Spatial Relation (3D-ISR) module to capture the intricate pairwise spatial relationships among objects. Additionally, we perform end-to-end multi-task instruction tuning simultaneously without the subsequent task-specific fine-tuning. Extensive experiments demonstrate that our approach outperforms the state-of-the-art methods across 3D scene understanding, reasoning and grounding tasks. Source code is available at <a target="_blank" rel="noopener" href="https://github.com/hanxunyu/Inst3D-LMM">https://github.com/hanxunyu/Inst3D-LMM</a> </p>
<blockquote>
<p>å°½ç®¡åœ¨ä¸‰ç»´åœºæ™¯ç†è§£æ–¹é¢å–å¾—äº†ä»¤äººé¼“èˆçš„è¿›å±•ï¼Œä½†å¼€å‘èƒ½å¤Ÿåœ¨å¤æ‚ä¸‰ç»´ç¯å¢ƒä¸­è¿›è¡Œç†è§£å’Œæ¨ç†çš„æœ‰æ•ˆå¤§å‹å¤šæ¨¡æ€æ¨¡å‹ï¼ˆLMMï¼‰ä»ç„¶å…·æœ‰æŒ‘æˆ˜æ€§ã€‚å¤§å¤šæ•°ä¹‹å‰çš„æ–¹æ³•é€šå¸¸åˆ†åˆ«ç¼–ç ä¸‰ç»´ç‚¹äº‘å’ŒäºŒç»´å›¾åƒç‰¹å¾ï¼Œå¿½ç•¥äº†äºŒç»´è¯­ä¹‰å’Œä¸‰ç»´ç‰©ä½“å±æ€§ä¹‹é—´çš„äº¤äº’ï¼Œä»¥åŠä¸‰ç»´ç¯å¢ƒå†…çš„ç©ºé—´å…³ç³»ã€‚è¿™ç§å±€é™æ€§ä¸ä»…é˜»ç¢äº†ä¸‰ç»´åœºæ™¯çš„å…¨é¢è¡¨ç¤ºï¼Œè¿˜å½±å“äº†è®­ç»ƒå’Œæ¨ç†çš„æ•ˆç‡ã€‚ä¸ºäº†åº”å¯¹è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ç»Ÿä¸€çš„å®ä¾‹æ„ŸçŸ¥ä¸‰ç»´å¤§å‹å¤šæ¨¡æ€æ¨¡å‹ï¼ˆInst3D-LMMï¼‰ï¼Œä»¥åŒæ—¶å¤„ç†å¤šä¸ªä¸‰ç»´åœºæ™¯ç†è§£ä»»åŠ¡ã€‚ä¸ºäº†è·å¾—ç²¾ç»†çš„å®ä¾‹çº§è§†è§‰æ ‡è®°ï¼Œæˆ‘ä»¬é¦–å…ˆå¼•å…¥äº†ä¸€ç§æ–°å‹çš„å¤šè§†è§’è·¨æ¨¡æ€èåˆï¼ˆMCMFï¼‰æ¨¡å—ï¼Œå°†å¤šè§†è§’çš„äºŒç»´è¯­ä¹‰æ³¨å…¥åˆ°å…¶å¯¹åº”çš„ä¸‰ç»´å‡ ä½•ç‰¹å¾ä¸­ã€‚å¯¹äºåœºæ™¯çº§å…³ç³»æ„ŸçŸ¥æ ‡è®°ï¼Œæˆ‘ä»¬è¿›ä¸€æ­¥æå‡ºäº†ä¸‰ç»´å®ä¾‹ç©ºé—´å…³ç³»ï¼ˆ3D-ISRï¼‰æ¨¡å—ï¼Œä»¥æ•æ‰ç‰©ä½“ä¹‹é—´å¤æ‚çš„ä¸€å¯¹ä¸€ç©ºé—´å…³ç³»ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿›è¡Œäº†ç«¯åˆ°ç«¯çš„å¤šä»»åŠ¡æŒ‡ä»¤è°ƒæ•´ï¼ŒåŒæ—¶æ‰§è¡Œå¤šé¡¹ä»»åŠ¡è€Œæ— éœ€åç»­çš„ä»»åŠ¡ç‰¹å®šå¾®è°ƒã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨ä¸‰ç»´åœºæ™¯ç†è§£ã€æ¨ç†å’Œå®šä½ä»»åŠ¡ä¸Šè¶…è¶Šäº†æœ€å…ˆè¿›çš„æ–¹æ³•ã€‚æºä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/hanxunyu/Inst3D-LMM%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/hanxunyu/Inst3D-LMMæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.00513v2">PDF</a> CVPR2025, Code Link: <a target="_blank" rel="noopener" href="https://github.com/hanxunyu/Inst3D-LMM">https://github.com/hanxunyu/Inst3D-LMM</a></p>
<p><strong>Summary</strong>ï¼š<br>å°½ç®¡åœ¨ä¸‰ç»´åœºæ™¯ç†è§£æ–¹é¢å–å¾—äº†ä»¤äººé¼“èˆçš„è¿›å±•ï¼Œä½†å¼€å‘ä¸€ç§èƒ½å¤Ÿåœ¨å¤æ‚ä¸‰ç»´ç¯å¢ƒä¸­è¿›è¡Œæœ‰æ•ˆç†è§£å’Œæ¨ç†çš„å¤§å‹å¤šæ¨¡æ€æ¨¡å‹ï¼ˆLMMï¼‰ä»ç„¶å…·æœ‰æŒ‘æˆ˜æ€§ã€‚é’ˆå¯¹è¿™ä¸€é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§ç»Ÿä¸€çš„å®ä¾‹æ„ŸçŸ¥ä¸‰ç»´å¤§å‹å¤šæ¨¡æ€æ¨¡å‹ï¼ˆInst3D-LMMï¼‰ï¼Œå¯åŒæ—¶å¤„ç†å¤šä¸ªä¸‰ç»´åœºæ™¯ç†è§£ä»»åŠ¡ã€‚é€šè¿‡å¼•å…¥å¤šè§†å›¾è·¨æ¨¡æ€èåˆï¼ˆMCMFï¼‰æ¨¡å—å’Œä¸‰ç»´å®ä¾‹ç©ºé—´å…³ç³»ï¼ˆ3D-ISRï¼‰æ¨¡å—ï¼Œè¯¥æ¨¡å‹åœ¨ç²¾ç»†ç²’åº¦çš„å®ä¾‹çº§åˆ«å’Œåœºæ™¯çº§åˆ«å…³ç³»æ„ŸçŸ¥æ–¹é¢è¡¨ç°å‡ºè‰²ã€‚å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨ä¸‰ç»´åœºæ™¯ç†è§£ã€æ¨ç†å’Œå®šä½ä»»åŠ¡ä¸Šçš„æ€§èƒ½å‡ä¼˜äºç°æœ‰æŠ€æœ¯ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>å¤§å‹å¤šæ¨¡æ€æ¨¡å‹ï¼ˆLMMï¼‰åœ¨å¤æ‚ä¸‰ç»´ç¯å¢ƒç†è§£å’Œæ¨ç†ä¸Šä»å­˜åœ¨æŒ‘æˆ˜ã€‚</li>
<li>ç°æœ‰æ–¹æ³•é€šå¸¸å°†ä¸‰ç»´ç‚¹ä¿¡æ¯å’ŒäºŒç»´å›¾åƒç‰¹å¾åˆ†åˆ«ç¼–ç ï¼Œå¿½ç•¥äº†äºŒè€…ä¹‹é—´çš„äº¤äº’ä»¥åŠä¸‰ç»´ç©ºé—´å†…çš„å…³ç³»ã€‚</li>
<li>Inst3D-LMMæ¨¡å‹è¢«æå‡ºä»¥è§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œèƒ½åŒæ—¶å¤„ç†å¤šä¸ªä¸‰ç»´åœºæ™¯ç†è§£ä»»åŠ¡ã€‚</li>
<li>MCMFæ¨¡å—å¼•å…¥å¤šè§†å›¾è·¨æ¨¡æ€èåˆï¼Œå°†å¤šè§†å›¾äºŒç»´è¯­ä¹‰æ³¨å…¥åˆ°å¯¹åº”çš„ä¸‰ç»´å‡ ä½•ç‰¹å¾ä¸­ã€‚</li>
<li>3D-ISRæ¨¡å—ç”¨äºæ•æ‰å¯¹è±¡ä¹‹é—´å¤æ‚çš„é…å¯¹ç©ºé—´å…³ç³»ã€‚</li>
<li>è¯¥æ¨¡å‹å¯è¿›è¡Œç«¯åˆ°ç«¯å¤šä»»åŠ¡æŒ‡ä»¤è°ƒæ•´ï¼Œæ— éœ€åç»­çš„ä»»åŠ¡ç‰¹å®šå¾®è°ƒã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.00513">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-7bd5c20b4e9d1046bc9c16a464f22896.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-013e502dbdde407d4c199e6edd2e484b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-08b1d0d8ad78774e002c26662a40445e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-020ecc80c8721387c2ad327205b752ab.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f824df1622d27712f8489f755359a28f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d667e04d5c7aa8dcd60d2ff4b4511da9.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="MAmmoTH-VL-Eliciting-Multimodal-Reasoning-with-Instruction-Tuning-at-Scale"><a href="#MAmmoTH-VL-Eliciting-Multimodal-Reasoning-with-Instruction-Tuning-at-Scale" class="headerlink" title="MAmmoTH-VL: Eliciting Multimodal Reasoning with Instruction Tuning at   Scale"></a>MAmmoTH-VL: Eliciting Multimodal Reasoning with Instruction Tuning at   Scale</h2><p><strong>Authors:Jarvis Guo, Tuney Zheng, Yuelin Bai, Bo Li, Yubo Wang, King Zhu, Yizhi Li, Graham Neubig, Wenhu Chen, Xiang Yue</strong></p>
<p>Open-source multimodal large language models (MLLMs) have shown significant potential in a broad range of multimodal tasks. However, their reasoning capabilities remain constrained by existing instruction-tuning datasets, which were predominately repurposed from academic datasets such as VQA, AI2D, and ChartQA. These datasets target simplistic tasks, and only provide phrase-level answers without any intermediate rationales. To address these challenges, we introduce a scalable and cost-effective method to construct a large-scale multimodal instruction-tuning dataset with rich intermediate rationales designed to elicit CoT reasoning. Using only open models, we create a dataset containing 12M instruction-response pairs to cover diverse, reasoning-intensive tasks with detailed and faithful rationales. Experiments demonstrate that training MLLMs on this dataset significantly improves reasoning capabilities, achieving state-of-the-art performance on benchmarks such as MathVerse (+8.1%), MMMU-Pro (+7%), and MuirBench (+13.3%). Additionally, the model demonstrates notable improvements of up to 4% on non-reasoning-based benchmarks. Ablation studies further highlight the importance of key components, such as rewriting and self-filtering, in the dataset construction process. </p>
<blockquote>
<p>å¼€æºå¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨å¹¿æ³›çš„å¤šæ¨¡æ€ä»»åŠ¡ä¸­æ˜¾ç¤ºå‡ºå·¨å¤§çš„æ½œåŠ›ã€‚ç„¶è€Œï¼Œå®ƒä»¬çš„æ¨ç†èƒ½åŠ›å—åˆ°ç°æœ‰æŒ‡ä»¤è°ƒæ•´æ•°æ®é›†çš„åˆ¶çº¦ï¼Œè¿™äº›æ•°æ®é›†ä¸»è¦ä»VQAã€AI2Då’ŒChartQAç­‰å­¦æœ¯æ•°æ®é›†ä¸­äºŒæ¬¡ä½¿ç”¨ã€‚è¿™äº›æ•°æ®é›†çš„ç›®æ ‡æ˜¯ç®€å•çš„ä»»åŠ¡ï¼Œåªèƒ½æä¾›æ²¡æœ‰ä¸­é—´ç†ç”±çš„çŸ­è¯­çº§ç­”æ¡ˆã€‚ä¸ºäº†è§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬ä»‹ç»äº†ä¸€ç§å¯æ‰©å±•ä¸”æˆæœ¬æ•ˆç›Šé«˜çš„æ–¹æ³•æ¥æ„å»ºå¤§è§„æ¨¡å¤šæ¨¡æ€æŒ‡ä»¤è°ƒæ•´æ•°æ®é›†ï¼Œè¯¥æ•°æ®é›†åŒ…å«ä¸°å¯Œçš„ä¸­é—´ç†ç”±ï¼Œæ—¨åœ¨æ¿€å‘CoTæ¨ç†ã€‚æˆ‘ä»¬ä»…ä½¿ç”¨å…¬å¼€æ¨¡å‹åˆ›å»ºäº†ä¸€ä¸ªåŒ…å«12MæŒ‡ä»¤å“åº”å¯¹çš„æ•°æ®é›†ï¼Œä»¥æ¶µç›–å¤šæ ·åŒ–ä¸”æ³¨é‡æ¨ç†çš„ä»»åŠ¡ï¼Œæä¾›è¯¦ç»†å’ŒçœŸå®åˆç†çš„ç­”æ¡ˆã€‚å®éªŒè¡¨æ˜ï¼Œåœ¨è¿™ä¸ªæ•°æ®é›†ä¸Šè®­ç»ƒMLLMså¯ä»¥æ˜¾è‘—æé«˜æ¨ç†èƒ½åŠ›ï¼Œåœ¨æ•°å­¦é¢†åŸŸï¼ˆMathVerseï¼Œ+8.1%ï¼‰ã€MMMU-Proï¼ˆ+7%ï¼‰å’ŒMuirBenchï¼ˆ+13.3%ï¼‰ç­‰åŸºå‡†æµ‹è¯•ä¸Šè¾¾åˆ°äº†ä¸šç•Œå‰æ²¿æ€§èƒ½ã€‚æ­¤å¤–ï¼Œæ¨¡å‹åœ¨éæ¨ç†åŸºå‡†æµ‹è¯•ä¸­ä¹Ÿè¡¨ç°å‡ºäº†æœ€é«˜å¯è¾¾4%çš„æ˜¾è‘—æ”¹è¿›ã€‚æ¶ˆèç ”ç©¶è¿›ä¸€æ­¥çªå‡ºäº†æ•°æ®æ„å»ºè¿‡ç¨‹ä¸­çš„å…³é”®ç»„ä»¶ï¼ˆå¦‚é‡å†™å’Œè‡ªè¿‡æ»¤ï¼‰çš„é‡è¦æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.05237v2">PDF</a> ACL 2025 Main</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†å¼€æºå¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨å¤šæ¨¡æ€ä»»åŠ¡ä¸­çš„å·¨å¤§æ½œåŠ›ã€‚ç„¶è€Œï¼Œå…¶æ¨ç†èƒ½åŠ›å—åˆ°ç°æœ‰æŒ‡ä»¤è°ƒæ•´æ•°æ®é›†çš„åˆ¶çº¦ï¼Œè¿™äº›æ•°æ®é›†ä¸»è¦æ¥æºäºå¦‚VQAã€AI2Då’ŒChartQAç­‰å­¦æœ¯æ•°æ®é›†ã€‚é’ˆå¯¹è¿™äº›æŒ‘æˆ˜ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§å¯æ‰©å±•ä¸”æˆæœ¬æ•ˆç›Šé«˜çš„æ–¹æ³•ï¼Œæ„å»ºäº†åŒ…å«ä¸°å¯Œä¸­é—´æ¨ç†çš„å¤§å‹å¤šæ¨¡æ€æŒ‡ä»¤è°ƒæ•´æ•°æ®é›†ã€‚å®éªŒè¡¨æ˜ï¼Œåœ¨è¯¥æ•°æ®é›†ä¸Šè®­ç»ƒMLLMsèƒ½æ˜¾è‘—æé«˜æ¨ç†èƒ½åŠ›ï¼Œåœ¨MathVerseã€MMMU-Proå’ŒMuirBenchç­‰åŸºå‡†æµ‹è¯•ä¸Šè¾¾åˆ°æœ€æ–°æŠ€æœ¯æ°´å¹³ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨å¤šæ¨¡æ€ä»»åŠ¡ä¸­å…·æœ‰å·¨å¤§æ½œåŠ›ã€‚</li>
<li>ç°æœ‰æŒ‡ä»¤è°ƒæ•´æ•°æ®é›†ä¸»è¦æ¥æºäºå­¦æœ¯æ•°æ®é›†ï¼Œé™åˆ¶äº†MLLMsçš„æ¨ç†èƒ½åŠ›ã€‚</li>
<li>æå‡ºäº†ä¸€ç§å¯æ‰©å±•ä¸”æˆæœ¬æ•ˆç›Šé«˜çš„æ–¹æ³•ï¼Œæ„å»ºåŒ…å«ä¸°å¯Œä¸­é—´æ¨ç†çš„å¤§å‹å¤šæ¨¡æ€æŒ‡ä»¤è°ƒæ•´æ•°æ®é›†ã€‚</li>
<li>è®­ç»ƒäºè¯¥æ•°æ®é›†ä¸Šçš„MLLMsåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œå¦‚MathVerseã€MMMU-Proå’ŒMuirBenchç­‰ã€‚</li>
<li>æ¨¡å‹åœ¨éæ¨ç†åŸºå‡†æµ‹è¯•ä¸Šä¹Ÿæœ‰æ˜¾è‘—æ”¹å–„ã€‚</li>
<li>æ¶ˆèç ”ç©¶çªå‡ºäº†æ•°æ®é›†æ„å»ºè¿‡ç¨‹ä¸­é‡å†™å’Œè‡ªè¿‡æ»¤ç­‰å…³é”®ç»„ä»¶çš„é‡è¦æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.05237">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-c991591cd1594df10dbb9b9f3749e6a6.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-48331a7147d897c0599a389172ece718.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-aaf951a075892bb8d29ed7cf688cb4fe.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-86325b96350dceba0bfd1fb3ad2803e8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-48fc283a0424105f7cf736f090390e3c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-3613d55f2d2d9c529549aa40c2f2d071.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="YOLO-MARL-You-Only-LLM-Once-for-Multi-Agent-Reinforcement-Learning"><a href="#YOLO-MARL-You-Only-LLM-Once-for-Multi-Agent-Reinforcement-Learning" class="headerlink" title="YOLO-MARL: You Only LLM Once for Multi-Agent Reinforcement Learning"></a>YOLO-MARL: You Only LLM Once for Multi-Agent Reinforcement Learning</h2><p><strong>Authors:Yuan Zhuang, Yi Shen, Zhili Zhang, Yuxiao Chen, Fei Miao</strong></p>
<p>Advancements in deep multi-agent reinforcement learning (MARL) have positioned it as a promising approach for decision-making in cooperative games. However, it still remains challenging for MARL agents to learn cooperative strategies for some game environments. Recently, large language models (LLMs) have demonstrated emergent reasoning capabilities, making them promising candidates for enhancing coordination among the agents. However, due to the model size of LLMs, it can be expensive to frequently infer LLMs for actions that agents can take. In this work, we propose You Only LLM Once for MARL (YOLO-MARL), a novel framework that leverages the high-level task planning capabilities of LLMs to improve the policy learning process of multi-agents in cooperative games. Notably, for each game environment, YOLO-MARL only requires one time interaction with LLMs in the proposed strategy generation, state interpretation and planning function generation modules, before the MARL policy training process. This avoids the ongoing costs and computational time associated with frequent LLMs API calls during training. Moreover, trained decentralized policies based on normal-sized neural networks operate independently of the LLM. We evaluate our method across two different environments and demonstrate that YOLO-MARL outperforms traditional MARL algorithms. </p>
<blockquote>
<p>æ·±åº¦å¤šæ™ºèƒ½ä½“å¼ºåŒ–å­¦ä¹ ï¼ˆMARLï¼‰çš„è¿›æ­¥ä½¿å…¶æˆä¸ºåˆä½œæ¸¸æˆå†³ç­–çš„æœ‰å‰é€”çš„æ–¹æ³•ã€‚ç„¶è€Œï¼Œå¯¹äºæŸäº›æ¸¸æˆç¯å¢ƒï¼ŒMARLæ™ºèƒ½ä½“å­¦ä¹ åˆä½œç­–ç•¥ä»ç„¶å…·æœ‰æŒ‘æˆ˜æ€§ã€‚æœ€è¿‘ï¼Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ˜¾ç¤ºå‡ºæ–°å…´çš„æ¨ç†èƒ½åŠ›ï¼Œä½¿å…¶æˆä¸ºå¢å¼ºæ™ºèƒ½ä½“ä¹‹é—´åè°ƒçš„å€™é€‰è€…ã€‚ç„¶è€Œï¼Œç”±äºLLMçš„æ¨¡å‹è§„æ¨¡ï¼Œé¢‘ç¹å¯¹LLMè¿›è¡Œè¡ŒåŠ¨æ¨ç†ä¼šå¾ˆæ˜‚è´µã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†â€œåªä¸ºMARLä½¿ç”¨ä¸€æ¬¡LLMâ€ï¼ˆYOLO-MARLï¼‰ï¼Œè¿™æ˜¯ä¸€ä¸ªåˆ©ç”¨LLMçš„é«˜çº§ä»»åŠ¡è§„åˆ’èƒ½åŠ›æ¥æ”¹å–„åˆä½œæ¸¸æˆä¸­å¤šæ™ºèƒ½ä½“çš„ç­–ç•¥å­¦ä¹ è¿‡ç¨‹çš„æ–°æ¡†æ¶ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œå¯¹äºæ¯ä¸ªæ¸¸æˆç¯å¢ƒï¼ŒYOLO-MARLä»…åœ¨ç­–ç•¥ç”Ÿæˆã€çŠ¶æ€è§£é‡Šå’Œè§„åˆ’åŠŸèƒ½ç”Ÿæˆæ¨¡å—ä¸­ä¸LLMè¿›è¡Œä¸€æ¬¡äº¤äº’ï¼Œç„¶åæ‰å¼€å§‹MARLç­–ç•¥è®­ç»ƒè¿‡ç¨‹ã€‚è¿™é¿å…äº†è®­ç»ƒè¿‡ç¨‹ä¸­é¢‘ç¹è°ƒç”¨LLM APIæ‰€äº§ç”Ÿçš„æŒç»­æˆæœ¬å’Œè®¡ç®—æ—¶é—´ã€‚æ­¤å¤–ï¼ŒåŸºäºæ­£å¸¸è§„æ¨¡ç¥ç»ç½‘ç»œçš„è®­ç»ƒåˆ†æ•£æ”¿ç­–ç‹¬ç«‹äºLLMè¿è¡Œã€‚æˆ‘ä»¬åœ¨ä¸¤ä¸ªä¸åŒçš„ç¯å¢ƒä¸­è¯„ä¼°äº†æˆ‘ä»¬çš„æ–¹æ³•ï¼Œå¹¶è¯æ˜YOLO-MARLä¼˜äºä¼ ç»Ÿçš„MARLç®—æ³•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2410.03997v2">PDF</a> accepted to International Conference on Intelligent Robots and   Systems (IROS2025)</p>
<p><strong>Summary</strong></p>
<p>å¤šæ™ºèƒ½ä½“æ·±åº¦å¼ºåŒ–å­¦ä¹ ï¼ˆMARLï¼‰åœ¨åˆä½œæ¸¸æˆä¸­å†³ç­–æ–¹é¢å±•ç°å‡ºå·¨å¤§æ½œåŠ›ï¼Œä½†é¢å¯¹æŸäº›æ¸¸æˆç¯å¢ƒå­¦ä¹ åˆä½œç­–ç•¥ä»å…·æŒ‘æˆ˜ã€‚å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å…·å¤‡æ¨ç†èƒ½åŠ›ï¼Œæœ‰æœ›æ”¹å–„æ™ºèƒ½ä½“é—´çš„åè°ƒã€‚ç„¶è€Œï¼ŒLLMsæ¨¡å‹ä½“ç§¯å¤§å¯¼è‡´æ¨ç†æˆæœ¬é«˜æ˜‚ã€‚æœ¬ç ”ç©¶æå‡ºYOLO-MARLæ¡†æ¶ï¼Œåˆ©ç”¨LLMsè¿›è¡Œé«˜çº§ä»»åŠ¡è§„åˆ’ï¼Œæ”¹å–„åˆä½œæ¸¸æˆä¸­çš„å¤šæ™ºèƒ½ä½“ç­–ç•¥å­¦ä¹ è¿‡ç¨‹ã€‚YOLO-MARLä»…éœ€ä¸€æ¬¡ä¸LLMsçš„äº’åŠ¨ï¼Œç”Ÿæˆç­–ç•¥ã€çŠ¶æ€è§£è¯»å’Œè§„åˆ’åŠŸèƒ½æ¨¡å—ï¼Œé™ä½è®­ç»ƒè¿‡ç¨‹ä¸­çš„è®¡ç®—æˆæœ¬å’Œæ—¶é—´ã€‚è¯„ä¼°æ˜¾ç¤ºYOLO-MARLåœ¨ä¸¤ç§ç¯å¢ƒä¸­è¡¨ç°ä¼˜äºä¼ ç»ŸMARLç®—æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤šæ™ºèƒ½ä½“æ·±åº¦å¼ºåŒ–å­¦ä¹ ï¼ˆMARLï¼‰åœ¨åˆä½œæ¸¸æˆä¸­è¡¨ç°è‰¯å¥½ï¼Œä½†å­¦ä¹ åˆä½œç­–ç•¥äºæŸäº›ç¯å¢ƒä»å…·æŒ‘æˆ˜ã€‚</li>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å…·å¤‡æ¨ç†èƒ½åŠ›ï¼Œå¯æ”¹å–„æ™ºèƒ½ä½“é—´çš„åè°ƒã€‚</li>
<li>LLMsæ¨¡å‹ä½“ç§¯å¤§å¯¼è‡´æ¨ç†æˆæœ¬é«˜æ˜‚ã€‚</li>
<li>YOLO-MARLæ¡†æ¶åˆ©ç”¨LLMsè¿›è¡Œé«˜çº§ä»»åŠ¡è§„åˆ’ï¼Œæå‡å¤šæ™ºèƒ½ä½“åœ¨åˆä½œæ¸¸æˆä¸­çš„ç­–ç•¥å­¦ä¹ ã€‚</li>
<li>YOLO-MARLä»…éœ€ä¸€æ¬¡ä¸LLMsäº’åŠ¨ï¼Œç”Ÿæˆç­–ç•¥ã€çŠ¶æ€è§£è¯»å’Œè§„åˆ’æ¨¡å—ã€‚</li>
<li>YOLO-MARLé™ä½è®­ç»ƒè¿‡ç¨‹ä¸­çš„è®¡ç®—æˆæœ¬å’Œæ—¶é—´ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2410.03997">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-bed5b889b4104802e54211d1cfc87911.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-9ca7d565303aba07fc11b08b2a883245.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d1d69046fbd8c49633b1a674242c8886.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f742dcfcedfba99b94540a2bfcc090c5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e9c5c61b8f866302bf17ac6a27ca5901.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="Leopard-A-Vision-Language-Model-For-Text-Rich-Multi-Image-Tasks"><a href="#Leopard-A-Vision-Language-Model-For-Text-Rich-Multi-Image-Tasks" class="headerlink" title="Leopard: A Vision Language Model For Text-Rich Multi-Image Tasks"></a>Leopard: A Vision Language Model For Text-Rich Multi-Image Tasks</h2><p><strong>Authors:Mengzhao Jia, Wenhao Yu, Kaixin Ma, Tianqing Fang, Zhihan Zhang, Siru Ouyang, Hongming Zhang, Dong Yu, Meng Jiang</strong></p>
<p>Text-rich images, where text serves as the central visual element guiding the overall understanding, are prevalent in real-world applications, such as presentation slides, scanned documents, and webpage snapshots. Tasks involving multiple text-rich images are especially challenging, as they require not only understanding the content of individual images but reasoning about inter-relationships and logical flows across multiple visual inputs. Despite the importance of these scenarios, current multimodal large language models (MLLMs) struggle to handle such tasks due to two key challenges: (1) the scarcity of high-quality instruction tuning datasets for text-rich multi-image scenarios, and (2) the difficulty in balancing image resolution with visual feature sequence length. To address these challenges, we propose Leopard, an MLLM tailored for handling vision-language tasks involving multiple text-rich images. First, we curated about one million high-quality multimodal instruction-tuning data, tailored to text-rich, multi-image scenarios. Second, we proposed an adaptive high-resolution multi-image encoding module to dynamically optimize the allocation of visual sequence length based on the original aspect ratios and resolutions of images. Experiments on a diverse set of benchmarks reveal that our model consistently outperforms state-of-the-art systems, such as Llama-3.2 and Qwen2-VL, in challenging text-rich, multi-image evaluations. Remarkably, our approach achieves outstanding performance using only 1.2M training instances, all of which are fully open-sourced, demonstrating both high efficiency and effectiveness compared to models trained on large-scale in-house data. Our code and data are available at <a target="_blank" rel="noopener" href="https://github.com/tencent-ailab/Leopard">https://github.com/tencent-ailab/Leopard</a>. </p>
<blockquote>
<p>æ–‡æœ¬ä¸°å¯Œçš„å›¾åƒï¼Œå…¶ä¸­æ–‡æœ¬ä½œä¸ºå¼•å¯¼æ•´ä½“ç†è§£çš„æ ¸å¿ƒè§†è§‰å…ƒç´ ï¼Œåœ¨ç°å®ä¸–ç•Œåº”ç”¨ä¸­æ™®éå­˜åœ¨ï¼Œä¾‹å¦‚æ¼”ç¤ºå¹»ç¯ç‰‡ã€æ‰«ææ–‡æ¡£å’Œç½‘é¡µå¿«ç…§ã€‚æ¶‰åŠå¤šä¸ªæ–‡æœ¬ä¸°å¯Œçš„å›¾åƒçš„ä»»åŠ¡å°¤å…¶å…·æœ‰æŒ‘æˆ˜æ€§ï¼Œå› ä¸ºå®ƒä»¬ä¸ä»…è¦æ±‚ç†è§£å•ä¸ªå›¾åƒçš„å†…å®¹ï¼Œè¿˜è¦æ±‚æ¨ç†å¤šä¸ªè§†è§‰è¾“å…¥ä¹‹é—´çš„ç›¸äº’ä½œç”¨å’Œé€»è¾‘å…³ç³»ã€‚å°½ç®¡è¿™äº›åœºæ™¯éå¸¸é‡è¦ï¼Œä½†å½“å‰çš„å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨å¤„ç†æ­¤ç±»ä»»åŠ¡æ—¶é¢ä¸´ä¸¤ä¸ªä¸»è¦æŒ‘æˆ˜ï¼š(1) æ–‡æœ¬ä¸°å¯Œå¤šå›¾åƒåœºæ™¯çš„ä¼˜è´¨æŒ‡ä»¤å¾®è°ƒæ•°æ®é›†ç¨€ç¼ºï¼›(2) å¹³è¡¡å›¾åƒåˆ†è¾¨ç‡ä¸è§†è§‰ç‰¹å¾åºåˆ—é•¿åº¦çš„éš¾åº¦ã€‚ä¸ºäº†åº”å¯¹è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†Leopardï¼Œä¸€ä¸ªä¸“ä¸ºå¤„ç†æ¶‰åŠå¤šä¸ªæ–‡æœ¬ä¸°å¯Œå›¾åƒçš„è§†è§‰è¯­è¨€ä»»åŠ¡è€Œå®šåˆ¶çš„å¤§å‹è¯­è¨€æ¨¡å‹ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬ç²¾é€‰äº†å¤§çº¦ä¸€ç™¾ä¸‡ä»½é«˜è´¨é‡çš„å¤šæ¨¡æ€æŒ‡ä»¤å¾®è°ƒæ•°æ®ï¼Œä¸“é—¨é’ˆå¯¹æ–‡æœ¬ä¸°å¯Œå¤šå›¾åƒåœºæ™¯ã€‚å…¶æ¬¡ï¼Œæˆ‘ä»¬æå‡ºäº†è‡ªé€‚åº”é«˜åˆ†è¾¨ç‡å¤šå›¾åƒç¼–ç æ¨¡å—ï¼Œæ ¹æ®å›¾åƒçš„åŸå§‹çºµæ¨ªæ¯”å’Œåˆ†è¾¨ç‡åŠ¨æ€ä¼˜åŒ–è§†è§‰åºåˆ—é•¿åº¦çš„åˆ†é…ã€‚åœ¨å¤šç§åŸºå‡†æµ‹è¯•é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ¨¡å‹åœ¨å…·æœ‰æŒ‘æˆ˜æ€§çš„æ–‡æœ¬ä¸°å¯Œå¤šå›¾åƒè¯„ä¼°ä¸­ä¸€ç›´ä¼˜äºæœ€å…ˆè¿›ç³»ç»Ÿï¼Œå¦‚Llama-3.2å’ŒQwen2-VLã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œæˆ‘ä»¬çš„æ–¹æ³•ä»…ä½¿ç”¨1.2Mè®­ç»ƒå®ä¾‹å°±å®ç°äº†å‡ºè‰²çš„æ€§èƒ½ï¼Œæ‰€æœ‰è¿™äº›å®ä¾‹éƒ½å·²å®Œå…¨å¼€æºï¼Œä¸åœ¨å¤§è§„æ¨¡å†…éƒ¨æ•°æ®ä¸Šè®­ç»ƒçš„æ¨¡å‹ç›¸æ¯”ï¼Œæ˜¾ç¤ºå‡ºé«˜æ•ˆå’Œé«˜æ•ˆèƒ½ã€‚æˆ‘ä»¬çš„ä»£ç å’Œæ•°æ®å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/tencent-ailab/Leopard%E4%B8%8A%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/tencent-ailab/Leopardä¸Šæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2410.01744v3">PDF</a> Our code is available at <a target="_blank" rel="noopener" href="https://github.com/tencent-ailab/Leopard">https://github.com/tencent-ailab/Leopard</a></p>
<p><strong>Summary</strong></p>
<p>æ–‡æœ¬ä¸°å¯Œçš„å›¾åƒåœ¨å¤„ç†å¤šæ¨¡æ€è¯­è¨€æ¨¡å‹ä»»åŠ¡æ—¶å°¤ä¸ºå…³é”®ï¼Œå°¤å…¶åœ¨çœŸå®ä¸–ç•Œåº”ç”¨ä¸­å¦‚æ¼”ç¤ºå¹»ç¯ç‰‡ã€æ‰«ææ–‡æ¡£å’Œç½‘é¡µå¿«ç…§ç­‰åœºæ™¯ã€‚ç„¶è€Œï¼Œå½“å‰çš„å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹åœ¨å¤„ç†æ¶‰åŠå¤šä¸ªæ–‡æœ¬ä¸°å¯Œçš„å›¾åƒçš„ä»»åŠ¡æ—¶é¢ä¸´æŒ‘æˆ˜ï¼Œä¸»è¦æºäºç¼ºä¹é«˜è´¨é‡çš„æ•™å­¦è°ƒä¼˜æ•°æ®é›†å’Œå¹³è¡¡å›¾åƒåˆ†è¾¨ç‡ä¸è§†è§‰ç‰¹å¾åºåˆ—é•¿åº¦çš„éš¾åº¦ã€‚é’ˆå¯¹è¿™äº›æŒ‘æˆ˜ï¼Œæå‡ºäº†åä¸ºLeopardçš„å®šåˆ¶å¤šæ¨¡æ€è¯­è¨€æ¨¡å‹ã€‚é€šè¿‡é‡‡é›†çº¦ä¸€ç™¾ä¸‡é«˜è´¨é‡çš„å¤šæ¨¡æ€æŒ‡ä»¤è°ƒä¼˜æ•°æ®ï¼Œå¹¶å¼•å…¥è‡ªé€‚åº”é«˜åˆ†è¾¨ç‡å¤šå›¾åƒç¼–ç æ¨¡å—ï¼Œå®ç°åŠ¨æ€ä¼˜åŒ–å›¾åƒåºåˆ—é•¿åº¦çš„åˆ†é…ã€‚å®éªŒè¯æ˜ï¼Œè¯¥æ¨¡å‹åœ¨å…·æœ‰æŒ‘æˆ˜æ€§çš„å¤šæ–‡æœ¬å›¾åƒè¯„ä¼°ä¸­è¡¨ç°ä¼˜äºå…¶ä»–å…ˆè¿›ç³»ç»Ÿï¼Œå¦‚Llama-3.2å’ŒQwen2-VLã€‚å…¶ä»…ä½¿ç”¨120ä¸‡è®­ç»ƒå®ä¾‹ä¾¿å–å¾—äº†æ˜¾è‘—æˆæ•ˆï¼Œä¸”æ‰€æœ‰èµ„æºå‡å·²å¼€æºã€‚</p>
<p><strong>Key Takeaways</strong></p>
<p>ä¸€ã€æ–‡æœ¬ä¸°å¯Œçš„å›¾åƒåœ¨çœŸå®ä¸–ç•Œåº”ç”¨ä¸­å¹¿æ³›ä½¿ç”¨ï¼Œæˆä¸ºç ”ç©¶ç„¦ç‚¹ã€‚<br>äºŒã€å½“å‰å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹åœ¨å¤„ç†æ¶‰åŠå¤šä¸ªæ–‡æœ¬ä¸°å¯Œçš„å›¾åƒçš„ä»»åŠ¡æ—¶é¢ä¸´æŒ‘æˆ˜ã€‚<br>ä¸‰ã€ç¼ºä¹é«˜è´¨é‡çš„æ•™å­¦è°ƒä¼˜æ•°æ®é›†å’Œå¹³è¡¡å›¾åƒåˆ†è¾¨ç‡ä¸è§†è§‰ç‰¹å¾åºåˆ—é•¿åº¦çš„éš¾åº¦æ˜¯ä¸¤å¤§ä¸»è¦æŒ‘æˆ˜ã€‚<br>å››ã€æå‡ºçš„Leopardæ¨¡å‹é€šè¿‡å¼•å…¥è‡ªé€‚åº”é«˜åˆ†è¾¨ç‡å¤šå›¾åƒç¼–ç æ¨¡å—è§£å†³ä¸Šè¿°é—®é¢˜ã€‚<br>äº”ã€Leopardæ¨¡å‹è¡¨ç°ä¼˜äºå…¶ä»–å…ˆè¿›ç³»ç»Ÿï¼Œåœ¨å…·æœ‰æŒ‘æˆ˜æ€§çš„å¤šæ–‡æœ¬å›¾åƒè¯„ä¼°ä¸­å–å¾—æ˜¾è‘—æˆæ•ˆã€‚<br>å…­ã€è¯¥æ¨¡å‹ä»…ä½¿ç”¨å°‘é‡è®­ç»ƒå®ä¾‹ä¾¿å–å¾—è‰¯å¥½æ•ˆæœï¼Œæ˜¾ç¤ºå…¶é«˜æ•ˆæ€§ã€‚</p>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2410.01744">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-b3ce89f203937dfd526bc6a85eb81680.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f7c511344e7a626a89adfaad23eb3431.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-da4b2be812b47d3efae2acca3d6a8bdf.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-794b5e542ee17b4ad97cc895ebddc228.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ff841c2ae9ad5b1ab61ada47c0ad31e8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-132ae71424272243dbc92929e9814911.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="Exploring-the-Trade-Offs-Quantization-Methods-Task-Difficulty-and-Model-Size-in-Large-Language-Models-From-Edge-to-Giant"><a href="#Exploring-the-Trade-Offs-Quantization-Methods-Task-Difficulty-and-Model-Size-in-Large-Language-Models-From-Edge-to-Giant" class="headerlink" title="Exploring the Trade-Offs: Quantization Methods, Task Difficulty, and   Model Size in Large Language Models From Edge to Giant"></a>Exploring the Trade-Offs: Quantization Methods, Task Difficulty, and   Model Size in Large Language Models From Edge to Giant</h2><p><strong>Authors:Jemin Lee, Sihyeong Park, Jinse Kwon, Jihun Oh, Yongin Kwon</strong></p>
<p>Quantization has gained attention as a promising solution for the cost-effective deployment of large and small language models. However, most prior work has been limited to perplexity or basic knowledge tasks and lacks a comprehensive evaluation of recent models like Llama-3.3. In this paper, we conduct a comprehensive evaluation of instruction-tuned models spanning 1B to 405B parameters, applying four quantization methods across 13 datasets. Our findings reveal that (1) quantized models generally surpass smaller FP16 baselines, yet they often struggle with instruction-following and hallucination detection; (2) FP8 consistently emerges as the most robust option across tasks, and AWQ tends to outperform GPTQ in weight-only quantization; (3) smaller models can suffer severe accuracy drops at 4-bit quantization, while 70B-scale models maintain stable performance; (4) notably, \textit{hard} tasks do not always experience the largest accuracy losses, indicating that quantization magnifies a modelâ€™s inherent weaknesses rather than simply correlating with task difficulty; and (5) an LLM-based judge (MT-Bench) highlights significant performance declines in Coding and STEM tasks, though it occasionally reports improvements in reasoning. </p>
<blockquote>
<p>é‡åŒ–ä½œä¸ºä¸€ç§æœ‰å‰æ™¯çš„è§£å†³æ–¹æ¡ˆï¼Œå·²ç»å¼•èµ·äº†äººä»¬å¯¹ç»æµé«˜æ•ˆåœ°éƒ¨ç½²å¤§å°è¯­è¨€æ¨¡å‹çš„å…³æ³¨ã€‚ç„¶è€Œï¼Œä¹‹å‰çš„å¤§éƒ¨åˆ†å·¥ä½œéƒ½å±€é™äºå›°æƒ‘åº¦æˆ–åŸºç¡€çŸ¥è¯†ä»»åŠ¡ï¼Œç¼ºä¹å¯¹åƒLlama-3.3è¿™æ ·çš„æœ€æ–°æ¨¡å‹çš„å…¨é¢è¯„ä¼°ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬å¯¹æŒ‡ä»¤ä¼˜åŒ–æ¨¡å‹è¿›è¡Œäº†å…¨é¢è¯„ä¼°ï¼Œè¿™äº›æ¨¡å‹å‚æ•°èŒƒå›´ä»1Båˆ°405Bï¼Œåº”ç”¨äº†å››ç§é‡åŒ–æ–¹æ³•ï¼Œè·¨è¶Š13ä¸ªæ•°æ®é›†ã€‚æˆ‘ä»¬çš„ç ”ç©¶å‘ç°ï¼š</p>
</blockquote>
<p>ï¼ˆ1ï¼‰é‡åŒ–æ¨¡å‹æ€»ä½“ä¸Šè¶…è¿‡äº†è¾ƒå°çš„FP16åŸºçº¿æ¨¡å‹ï¼Œä½†åœ¨éµå¾ªæŒ‡ä»¤å’Œå¹»æƒ³æ£€æµ‹æ–¹é¢ç»å¸¸é‡åˆ°å›°éš¾ï¼›<br>ï¼ˆ2ï¼‰FP8åœ¨å„é¡¹ä»»åŠ¡ä¸­å§‹ç»ˆè¡¨ç°ä¸ºæœ€ç¨³å¥çš„é€‰æ‹©ï¼ŒAWQåœ¨ä»…æƒé‡é‡åŒ–æ–¹é¢å¾€å¾€ä¼˜äºGPTQï¼›<br>ï¼ˆ3ï¼‰è¾ƒå°æ¨¡å‹åœ¨4ä½é‡åŒ–æ—¶å¯èƒ½ä¼šå‡ºç°ä¸¥é‡çš„ç²¾åº¦ä¸‹é™ï¼Œè€Œ70Bè§„æ¨¡çš„æ¨¡å‹åˆ™èƒ½ä¿æŒç¨³å®šçš„æ€§èƒ½ï¼›<br>ï¼ˆ4ï¼‰å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œéš¾åº¦é«˜çš„ä»»åŠ¡å¹¶ä¸æ€»æ˜¯é¢ä¸´æœ€å¤§çš„ç²¾åº¦æŸå¤±ï¼Œè¿™è¡¨æ˜é‡åŒ–ä¼šæ”¾å¤§æ¨¡å‹çš„å›ºæœ‰å¼±ç‚¹ï¼Œè€Œä¸ä»…ä»…æ˜¯ä¸ä»»åŠ¡éš¾åº¦ç›¸å…³ï¼›<br>ï¼ˆ5ï¼‰åŸºäºå¤§å‹è¯­è¨€æ¨¡å‹çš„åˆ¤æ–­ï¼ˆMT-Benchï¼‰å‡¸æ˜¾äº†ç¼–ç å’ŒSTEMä»»åŠ¡çš„æ€§èƒ½æ˜¾è‘—ä¸‹é™ï¼Œå°½ç®¡å®ƒåœ¨æ¨ç†æ–¹é¢å¶å°”ä¼šæœ‰æ”¹è¿›ã€‚</p>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2409.11055v6">PDF</a> Accepted in IJCAI 2025, 21 pages, 2 figure</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡å…¨é¢è¯„ä¼°äº†æŒ‡ä»¤å¾®è°ƒæ¨¡å‹åœ¨é‡åŒ–åçš„è¡¨ç°ï¼Œæ¶‰åŠå‚æ•°èŒƒå›´ä»1Båˆ°405Bï¼Œé‡‡ç”¨å››ç§é‡åŒ–æ–¹æ³•ï¼Œè·¨è¶Š13ä¸ªæ•°æ®é›†ã€‚ç ”ç©¶å‘ç°ï¼Œé‡åŒ–æ¨¡å‹ä¸€èˆ¬è¡¨ç°ä¼˜äºFP16åŸºçº¿æ¨¡å‹ï¼Œä½†åœ¨æŒ‡ä»¤éµå¾ªå’Œå¹»è§‰æ£€æµ‹æ–¹é¢å­˜åœ¨é—®é¢˜ã€‚FP8åœ¨å„é¡¹ä»»åŠ¡ä¸­æœ€å…·ç¨³å¥æ€§ï¼ŒAWQåœ¨ä»…æƒé‡é‡åŒ–æ–¹é¢è¡¨ç°ä¼˜äºGPTQã€‚å°æ¨¡å‹åœ¨4ä½é‡åŒ–æ—¶ç²¾åº¦å¤§å¹…ä¸‹é™ï¼Œè€Œ70Bè§„æ¨¡æ¨¡å‹è¡¨ç°ç¨³å®šã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œé‡åŒ–å¹¶éæ€»æ˜¯å¯¹å›°éš¾ä»»åŠ¡é€ æˆæœ€å¤§ç²¾åº¦æŸå¤±ï¼Œè€Œæ˜¯æ”¾å¤§äº†æ¨¡å‹çš„å›ºæœ‰å¼±ç‚¹ã€‚LLMåˆ¤æ–­ï¼ˆMT-Benchï¼‰åœ¨ç¼–ç å’ŒSTEMä»»åŠ¡ä¸­è¡¨ç°å‡ºæ˜¾è‘—æ€§èƒ½ä¸‹é™ï¼Œä½†æœ‰æ—¶åœ¨æ¨ç†ä»»åŠ¡ä¸­æœ‰æ‰€æ”¹å–„ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>é‡åŒ–æ¨¡å‹æ€»ä½“è¡¨ç°ä¼˜äºFP16åŸºçº¿æ¨¡å‹ï¼Œä½†åœ¨æŒ‡ä»¤éµå¾ªå’Œå¹»è§‰æ£€æµ‹æ–¹é¢å­˜åœ¨é—®é¢˜ã€‚</li>
<li>FP8æ˜¯æœ€ç¨³å¥çš„é‡åŒ–é€‰é¡¹ï¼Œç‰¹åˆ«æ˜¯åœ¨ä¸åŒä»»åŠ¡ä¸­è¡¨ç°ä¸€è‡´ã€‚</li>
<li>AWQåœ¨ä»…æƒé‡é‡åŒ–æ–¹é¢ä¼˜äºGPTQã€‚</li>
<li>å°æ¨¡å‹åœ¨4ä½é‡åŒ–æ—¶ç²¾åº¦æŸå¤±è¾ƒå¤§ï¼Œè€Œè¾ƒå¤§æ¨¡å‹ï¼ˆå¦‚70Bè§„æ¨¡ï¼‰çš„ç²¾åº¦è¡¨ç°ç›¸å¯¹ç¨³å®šã€‚</li>
<li>é‡åŒ–ä¼šæ”¾å¤§æ¨¡å‹çš„å›ºæœ‰å¼±ç‚¹ï¼Œè€Œéä»…ä¸ä»»åŠ¡éš¾åº¦ç›¸å…³ã€‚</li>
<li>LLMåˆ¤æ–­å·¥å…·ï¼ˆå¦‚MT-Benchï¼‰åœ¨æŸäº›ä»»åŠ¡ï¼ˆå¦‚ç¼–ç å’ŒSTEMï¼‰ä¸­æ£€æµ‹åˆ°æ˜¾è‘—æ€§èƒ½ä¸‹é™ã€‚</li>
<li>åœ¨æŸäº›æ¨ç†ä»»åŠ¡ä¸­ï¼Œé‡åŒ–æ¨¡å‹çš„è¡¨ç°æœ‰æ‰€æ”¹å–„ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2409.11055">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-7230159c5d87c4bb90c4d52f4360756e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-36f8be894892e570711864b8b6462dbb.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f76cc5736fac1d2ddc5d3debffb65997.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-6748fa4dc153505de77db20a110c062f.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="TextSquare-Scaling-up-Text-Centric-Visual-Instruction-Tuning"><a href="#TextSquare-Scaling-up-Text-Centric-Visual-Instruction-Tuning" class="headerlink" title="TextSquare: Scaling up Text-Centric Visual Instruction Tuning"></a>TextSquare: Scaling up Text-Centric Visual Instruction Tuning</h2><p><strong>Authors:Jingqun Tang, Chunhui Lin, Zhen Zhao, Shu Wei, Binghong Wu, Qi Liu, Yangfan He, Kuan Lu, Hao Feng, Yang Li, Siqi Wang, Lei Liao, Wei Shi, Yuliang Liu, Hao Liu, Yuan Xie, Xiang Bai, Can Huang</strong></p>
<p>Text-centric visual question answering (VQA) has made great strides with the development of Multimodal Large Language Models (MLLMs), yet open-source models still fall short of leading models like GPT4V and Gemini, partly due to a lack of extensive, high-quality instruction tuning data. To this end, we introduce a new approach for creating a massive, high-quality instruction-tuning dataset, Square-10M, which is generated using closed-source MLLMs. The data construction process, termed Square, consists of four steps: Self-Questioning, Answering, Reasoning, and Evaluation. Our experiments with Square-10M led to three key findings: 1) Our model, TextSquare, considerably surpasses open-source previous state-of-the-art Text-centric MLLMs and sets a new standard on OCRBench(62.2%). It even outperforms top-tier models like GPT4V and Gemini in 6 of 10 text-centric benchmarks. 2) Additionally, we demonstrate the critical role of VQA reasoning data in offering comprehensive contextual insights for specific questions. This not only improves accuracy but also significantly mitigates hallucinations. Specifically, TextSquare scores an average of 75.1% across four general VQA and hallucination evaluation datasets, outperforming previous state-of-the-art models. 3) Notably, the phenomenon observed in scaling text-centric VQA datasets reveals a vivid pattern: the exponential increase of instruction tuning data volume is directly proportional to the improvement in model performance, thereby validating the necessity of the dataset scale and the high quality of Square-10M. </p>
<blockquote>
<p>æ–‡æœ¬ä¸ºä¸­å¿ƒçš„è§†è§‰é—®ç­”ï¼ˆVQAï¼‰éšç€å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰çš„å‘å±•è€Œå–å¾—äº†å·¨å¤§çš„è¿›æ­¥ï¼Œä½†ä»è½åäºGPT4Vå’ŒGeminiç­‰é¢†å…ˆæ¨¡å‹ï¼Œéƒ¨åˆ†åŸå› åœ¨äºç¼ºä¹å¹¿æ³›çš„é«˜è´¨é‡æŒ‡ä»¤å¾®è°ƒæ•°æ®ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§åˆ›å»ºå¤§è§„æ¨¡é«˜è´¨é‡æŒ‡ä»¤å¾®è°ƒæ•°æ®é›†çš„æ–°æ–¹æ³•ï¼Œå³Square-10Mï¼Œå®ƒæ˜¯ä½¿ç”¨å°é—­æºMLLMsç”Ÿæˆçš„ã€‚æ•°æ®æ„å»ºè¿‡ç¨‹ï¼Œå³Squareï¼ŒåŒ…æ‹¬å››ä¸ªæ­¥éª¤ï¼šè‡ªé—®ã€å›ç­”ã€æ¨ç†å’Œè¯„ä¼°ã€‚æˆ‘ä»¬åˆ©ç”¨Square-10Mè¿›è¡Œçš„å®éªŒå¾—å‡ºäº†ä¸‰ä¸ªå…³é”®å‘ç°ï¼š</p>
</blockquote>
<p>ä¸€ã€æˆ‘ä»¬çš„æ¨¡å‹TextSquareæ˜¾è‘—è¶…è¶Šäº†ä¹‹å‰çš„å¼€æºæ–‡æœ¬ä¸ºä¸­å¿ƒçš„æœ€å…ˆè¿›çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼Œå¹¶åœ¨OCRBenchä¸Šè¾¾åˆ°äº†62.2%ï¼Œè®¾å®šäº†æ–°çš„æ ‡å‡†ã€‚åœ¨åä¸ªæ–‡æœ¬ä¸ºä¸­å¿ƒçš„æ ‡å‡†ä¸­ï¼Œå®ƒç”šè‡³åœ¨å…­ä¸ªæ–¹é¢è¶…è¿‡äº†é¡¶å°–æ¨¡å‹å¦‚GPT4Vå’ŒGeminiã€‚</p>
<p>äºŒã€æ­¤å¤–ï¼Œæˆ‘ä»¬è¯æ˜äº†VQAæ¨ç†æ•°æ®åœ¨æä¾›ç‰¹å®šé—®é¢˜çš„å…¨é¢ä¸Šä¸‹æ–‡æ´å¯Ÿä¸­çš„å…³é”®ä½œç”¨ã€‚è¿™ä¸ä»…æé«˜äº†å‡†ç¡®æ€§ï¼Œè€Œä¸”æ˜¾è‘—å‡è½»äº†å¹»è§‰ç°è±¡ã€‚å…·ä½“æ¥è¯´ï¼ŒTextSquareåœ¨å››ä¸ªé€šç”¨VQAå’Œå¹»è§‰è¯„ä¼°æ•°æ®é›†ä¸Šçš„å¹³å‡å¾—åˆ†ä¸º75.1%ï¼Œè¶…è¿‡äº†ä¹‹å‰çš„æœ€å…ˆè¿›æ¨¡å‹ã€‚</p>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2404.12803v3">PDF</a> </p>
<p><strong>Summary</strong><br>     é€šè¿‡åˆ©ç”¨å°é—­å¼æºçš„å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼Œæ¨å‡ºäº†å…¨æ–°çš„å¤§è§„æ¨¡é«˜è´¨é‡æŒ‡ä»¤è°ƒæ•´æ•°æ®é›†Square-10Mï¼ŒåŒ…å«è‡ªæˆ‘æé—®ã€å›ç­”ã€æ¨ç†å’Œè¯„ä¼°å››ä¸ªæ­¥éª¤ã€‚å®éªŒè¡¨æ˜ï¼ŒTextSquareæ¨¡å‹åœ¨OCRBenchä¸Šè¶…è¶Šäº†ç°æœ‰çš„å¼€æºæ–‡æœ¬ä¸­å¿ƒå‹MLLMsæ¨¡å‹ï¼Œå¹¶ä¸”åœ¨å¤§éƒ¨åˆ†æ–‡æœ¬ä¸­å¿ƒåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚ã€‚æ­¤å¤–ï¼Œå¼ºè°ƒäº†VQAæ¨ç†æ•°æ®å¯¹äºæä¾›ç‰¹å®šé—®é¢˜çš„å…¨é¢è¯­å¢ƒæ´å¯Ÿçš„é‡è¦æ€§ï¼Œä¸ä»…æé«˜äº†å‡†ç¡®æ€§ï¼Œè¿˜æ˜¾è‘—å‡å°‘äº†å¹»è§‰ã€‚è¿˜å‘ç°æ–‡æœ¬ä¸­å¿ƒå‹VQAæ•°æ®é›†çš„å¢é•¿è¶‹åŠ¿ä¸æ¨¡å‹æ€§èƒ½æå‡ç›´æ¥ç›¸å…³ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¼•å…¥äº†ä¸€ç§æ–°çš„å¤§è§„æ¨¡é«˜è´¨é‡æŒ‡ä»¤è°ƒæ•´æ•°æ®é›†Square-10Mï¼Œç”¨äºåˆ›å»ºæ–‡æœ¬ä¸­å¿ƒå‹è§†è§‰é—®ç­”ï¼ˆVQAï¼‰æ¨¡å‹ã€‚</li>
<li>Square-10Mæ•°æ®é›†ä½¿ç”¨å°é—­å¼æºçš„å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰ç”Ÿæˆï¼ŒåŒ…å«è‡ªæˆ‘æé—®ã€å›ç­”ã€æ¨ç†å’Œè¯„ä¼°å››ä¸ªæ­¥éª¤ã€‚</li>
<li>TextSquareæ¨¡å‹åœ¨OCRBenchä¸Šçš„è¡¨ç°è¶…è¶Šäº†ç°æœ‰çš„å¼€æºæ–‡æœ¬ä¸­å¿ƒå‹MLLMsæ¨¡å‹ï¼Œå¹¶åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚ã€‚</li>
<li>VQAæ¨ç†æ•°æ®å¯¹äºæé«˜æ¨¡å‹çš„å‡†ç¡®æ€§å’Œå‡å°‘å¹»è§‰è‡³å…³é‡è¦ã€‚</li>
<li>TextSquareæ¨¡å‹åœ¨ä¸€èˆ¬VQAå’Œå¹»è§‰è¯„ä¼°æ•°æ®é›†ä¸Šçš„å¹³å‡å¾—åˆ†é«˜äºä¹‹å‰çš„å…ˆè¿›æ¨¡å‹ã€‚</li>
<li>æŒ‡æ•°å¢é•¿çš„æ–‡æœ¬ä¸­å¿ƒå‹VQAæ•°æ®é›†ä¸æ¨¡å‹æ€§èƒ½çš„æå‡å‘ˆç›´æ¥ç›¸å…³ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2404.12803">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-b71496f5799230982f5f975d4a7a97a8.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-69cc7db9e4dbfe676879e7331e2da6aa.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-096f153eb7d4e01b4053d1e02784d5f5.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-7acb0085b072d6bf157006077fbafa7a.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="VCD-A-Dataset-for-Visual-Commonsense-Discovery-in-Images"><a href="#VCD-A-Dataset-for-Visual-Commonsense-Discovery-in-Images" class="headerlink" title="VCD: A Dataset for Visual Commonsense Discovery in Images"></a>VCD: A Dataset for Visual Commonsense Discovery in Images</h2><p><strong>Authors:Xiangqing Shen, Fanfan Wang, Siwei Wu, Rui Xia</strong></p>
<p>Visual commonsense plays a vital role in understanding and reasoning about the visual world. While commonsense knowledge bases like ConceptNet provide structured collections of general facts, they lack visually grounded representations. Scene graph datasets like Visual Genome, though rich in object-level descriptions, primarily focus on directly observable information and lack systematic categorization of commonsense knowledge. We present Visual Commonsense Dataset (VCD), a large-scale dataset containing over 100,000 images and 14 million object-commonsense pairs that bridges this gap. VCD introduces a novel three-level taxonomy for visual commonsense, integrating both Seen (directly observable) and Unseen (inferrable) commonsense across Property, Action, and Space aspects. Each commonsense is represented as a triple where the head entity is grounded to object bounding boxes in images, enabling scene-dependent and object-specific visual commonsense representation. To demonstrate VCDâ€™s utility, we develop VCM, a generative model that combines a vision-language model with instruction tuning to discover diverse visual commonsense from images. Extensive evaluations demonstrate both the high quality of VCD and its value as a resource for advancing visually grounded commonsense understanding and reasoning. Our dataset and code will be released on <a target="_blank" rel="noopener" href="https://github.com/NUSTM/VCD">https://github.com/NUSTM/VCD</a>. </p>
<blockquote>
<p>è§†è§‰å¸¸è¯†åœ¨ç†è§£å’Œæ¨ç†è§†è§‰ä¸–ç•Œæ–¹é¢èµ·ç€è‡³å…³é‡è¦çš„ä½œç”¨ã€‚è™½ç„¶åƒConceptNetè¿™æ ·çš„å¸¸è¯†çŸ¥è¯†åº“æä¾›äº†é€šç”¨äº‹å®çš„ç»“æ„åŒ–é›†åˆï¼Œä½†å®ƒä»¬ç¼ºä¹è§†è§‰åŸºç¡€è¡¨ç¤ºã€‚åƒVisual Genomeè¿™æ ·çš„åœºæ™¯å›¾æ•°æ®é›†è™½ç„¶å¯Œå«å¯¹è±¡çº§åˆ«çš„æè¿°ï¼Œä½†ä¸»è¦å…³æ³¨å¯ç›´æ¥è§‚å¯Ÿçš„ä¿¡æ¯ï¼Œç¼ºä¹å¸¸è¯†çŸ¥è¯†çš„ç³»ç»Ÿåˆ†ç±»ã€‚æˆ‘ä»¬æ¨å‡ºäº†Visual Commonsense Datasetï¼ˆVCDï¼‰ï¼Œè¿™æ˜¯ä¸€ä¸ªå¤§è§„æ¨¡æ•°æ®é›†ï¼ŒåŒ…å«è¶…è¿‡10ä¸‡å¼ å›¾åƒå’Œ1400ä¸‡ä¸ªå¯¹è±¡å¸¸è¯†å¯¹ï¼Œå¼¥è¡¥äº†è¿™ä¸€ç©ºç™½ã€‚VCDå¼•å…¥äº†ä¸€ç§æ–°é¢–çš„ä¸‰çº§åˆ†ç±»æ³•ï¼Œç”¨äºè§†è§‰å¸¸è¯†ï¼Œèåˆäº†å¯è§ï¼ˆå¯ç›´æ¥è§‚å¯Ÿï¼‰å’Œä¸å¯è§ï¼ˆå¯æ¨æ–­ï¼‰çš„å¸¸è¯†ï¼Œæ¶µç›–å±æ€§ã€åŠ¨ä½œå’Œç©ºé—´æ–¹é¢ã€‚æ¯ä¸ªå¸¸è¯†éƒ½è¡¨ç¤ºä¸ºä¸‰å…ƒç»„ï¼Œå…¶ä¸­å¤´å®ä½“ä¸å›¾åƒä¸­çš„å¯¹è±¡è¾¹ç•Œæ¡†ç›¸ç»“åˆï¼Œå®ç°äº†åœºæ™¯ä¾èµ–å’Œå¯¹è±¡ç‰¹å®šçš„è§†è§‰å¸¸è¯†è¡¨ç¤ºã€‚ä¸ºäº†å±•ç¤ºVCDçš„å®ç”¨æ€§ï¼Œæˆ‘ä»¬å¼€å‘äº†VCMï¼Œè¿™æ˜¯ä¸€ä¸ªç”Ÿæˆæ¨¡å‹ï¼Œå®ƒå°†è§†è§‰è¯­è¨€æ¨¡å‹ä¸æŒ‡ä»¤è°ƒæ•´ç›¸ç»“åˆï¼Œä»å›¾åƒä¸­å‘ç°å¤šæ ·çš„è§†è§‰å¸¸è¯†ã€‚å¹¿æ³›è¯„ä¼°è¡¨æ˜ï¼ŒVCDçš„é«˜è´¨é‡åŠå…¶ä½œä¸ºæ¨è¿›è§†è§‰åŸºç¡€å¸¸è¯†ç†è§£å’Œæ¨ç†çš„èµ„æºä»·å€¼ã€‚æˆ‘ä»¬çš„æ•°æ®é›†å’Œä»£ç å°†åœ¨<a target="_blank" rel="noopener" href="https://github.com/NUSTM/VCD%E4%B8%8A%E5%8F%91%E5%B8%83%E3%80%82">https://github.com/NUSTM/VCDä¸Šå‘å¸ƒã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2402.17213v2">PDF</a> </p>
<p><strong>Summary</strong>ï¼šè§†è§‰å¸¸è¯†åœ¨ç†è§£å’Œæ¨ç†è§†è§‰ä¸–ç•Œæ–¹é¢èµ·ç€é‡è¦ä½œç”¨ã€‚ç°æœ‰çš„å¸¸è¯†çŸ¥è¯†åº“å¦‚ConceptNetç¼ºä¹è§†è§‰åŸºç¡€è¡¨ç¤ºï¼Œè€Œåœºæ™¯å›¾æ•°æ®é›†å¦‚Visual Genomeåˆ™ä¸»è¦å…³æ³¨å¯ç›´æ¥è§‚å¯Ÿçš„ä¿¡æ¯ï¼Œç¼ºä¹ç³»ç»Ÿå¸¸è¯†åˆ†ç±»ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†è§†è§‰å¸¸è¯†æ•°æ®é›†ï¼ˆVCDï¼‰ï¼ŒåŒ…å«è¶…è¿‡100,000å¼ å›¾åƒå’Œ1400ä¸‡å¯¹è±¡-å¸¸è¯†å¯¹ï¼Œå¡«è¡¥äº†è¿™ä¸€ç©ºç™½ã€‚VCDå¼•å…¥äº†ä¸€ç§æ–°å‹ä¸‰çº§åˆ†ç±»æ³•ï¼Œå°†å¯è§ï¼ˆå¯ç›´æ¥è§‚å¯Ÿï¼‰å’Œä¸å¯è§ï¼ˆå¯æ¨æ–­ï¼‰çš„å¸¸è¯†æ•´åˆåˆ°å±æ€§ã€åŠ¨ä½œå’Œç©ºé—´æ–¹é¢ã€‚æ¯ä¸ªå¸¸è¯†éƒ½ä»¥ä¸‰å…ƒç»„çš„å½¢å¼è¡¨ç¤ºï¼Œå¤´éƒ¨å®ä½“ä¸å›¾åƒä¸­çš„å¯¹è±¡è¾¹ç•Œæ¡†ç›¸å¯¹åº”ï¼Œä»è€Œå®ç°åœºæ™¯ç›¸å…³å’Œå¯¹è±¡ç‰¹å®šçš„è§†è§‰å¸¸è¯†è¡¨ç¤ºã€‚ä¸ºäº†å±•ç¤ºVCDçš„å®ç”¨æ€§ï¼Œæˆ‘ä»¬å¼€å‘äº†VCMæ¨¡å‹ï¼Œè¯¥æ¨¡å‹ç»“åˆäº†è§†è§‰è¯­è¨€æ¨¡å‹ä¸æŒ‡ä»¤è°ƒæ•´ï¼Œå¯ä»å›¾åƒä¸­å‘ç°å¤šæ ·çš„è§†è§‰å¸¸è¯†ã€‚å¯¹VCDçš„å¹¿æ³›è¯„ä¼°è¯æ˜äº†å…¶é«˜è´¨é‡å’Œä½œä¸ºæ¨è¿›è§†è§‰å¸¸è¯†ç†è§£å’Œæ¨ç†çš„èµ„æºä»·å€¼ã€‚æˆ‘ä»¬çš„æ•°æ®é›†å’Œä»£ç å°†åœ¨<a target="_blank" rel="noopener" href="https://github.com/NUSTM/VCD">https://github.com/NUSTM/VCD</a>ä¸Šå‘å¸ƒã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>è§†è§‰å¸¸è¯†åœ¨ç†è§£å’Œæ¨ç†è§†è§‰ä¸–ç•Œæ–¹é¢å…·é‡è¦ä½œç”¨ã€‚</li>
<li>å½“å‰çŸ¥è¯†åº“å’Œæ•°æ®é›†å­˜åœ¨å¯¹è§†è§‰åŸºç¡€è¡¨ç¤ºçš„ç¼ºå¤±ã€‚</li>
<li>æå‡ºäº†è§†è§‰å¸¸è¯†æ•°æ®é›†ï¼ˆVCDï¼‰ï¼ŒåŒ…å«å›¾åƒå’Œå¯¹è±¡-å¸¸è¯†å¯¹ï¼Œä»¥å¡«è¡¥è¿™ä¸€ç©ºç™½ã€‚</li>
<li>VCDé‡‡ç”¨æ–°å‹ä¸‰çº§åˆ†ç±»æ³•ï¼Œæ•´åˆå¯è§å’Œä¸å¯è§å¸¸è¯†ï¼Œæ¶‰åŠå±æ€§ã€åŠ¨ä½œå’Œç©ºé—´æ–¹é¢ã€‚</li>
<li>æ¯ä¸ªå¸¸è¯†ä»¥ä¸‰å…ƒç»„å½¢å¼è¡¨ç¤ºï¼Œä¸å›¾åƒä¸­çš„å¯¹è±¡è¾¹ç•Œæ¡†ç›¸å¯¹åº”ã€‚<br>6.å¼€å‘äº†VCMæ¨¡å‹ä»¥å±•ç¤ºVCDçš„å®ç”¨æ€§ï¼Œè¯¥æ¨¡å‹èƒ½ä»å›¾åƒä¸­å‘ç°å¤šæ ·çš„è§†è§‰å¸¸è¯†ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2402.17213">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-04775750c2b7aaa268e08f6d6d42895a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-4703d3091e348bcbf98aa6b35f94e7a4.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-700ab1eb162dbef1b6b81b508cdd1e70.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-47b5d05e9d480e67ac1ce7d86a8db52d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2ccf103f7e301725f53667dd0d4c644d.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="GPT4RoI-Instruction-Tuning-Large-Language-Model-on-Region-of-Interest"><a href="#GPT4RoI-Instruction-Tuning-Large-Language-Model-on-Region-of-Interest" class="headerlink" title="GPT4RoI: Instruction Tuning Large Language Model on Region-of-Interest"></a>GPT4RoI: Instruction Tuning Large Language Model on Region-of-Interest</h2><p><strong>Authors:Shilong Zhang, Peize Sun, Shoufa Chen, Min Xiao, Wenqi Shao, Wenwei Zhang, Yu Liu, Kai Chen, Ping Luo</strong></p>
<p>Visual instruction tuning large language model(LLM) on image-text pairs has achieved general-purpose vision-language abilities. However, the lack of region-text pairs limits their advancements to fine-grained multimodal understanding. In this paper, we propose spatial instruction tuning, which introduces the reference to the region-of-interest(RoI) in the instruction. Before sending to LLM, the reference is replaced by RoI features and interleaved with language embeddings as a sequence. Our model GPT4RoI, trained on 7 region-text pair datasets, brings an unprecedented interactive and conversational experience compared to previous image-level models. (1) Interaction beyond language: Users can interact with our model by both language and drawing bounding boxes to flexibly adjust the referring granularity. (2) Versatile multimodal abilities: A variety of attribute information within each RoI can be mined by GPT4RoI, e.g., color, shape, material, action, etc. Furthermore, it can reason about multiple RoIs based on common sense. On the Visual Commonsense Reasoning(VCR) dataset, GPT4RoI achieves a remarkable accuracy of 81.6%, surpassing all existing models by a significant margin (the second place is 75.6%) and almost reaching human-level performance of 85.0%. The code and model can be found at <a target="_blank" rel="noopener" href="https://github.com/jshilong/GPT4RoI">https://github.com/jshilong/GPT4RoI</a>. </p>
<blockquote>
<p>é€šè¿‡è§†è§‰æŒ‡ä»¤è°ƒæ•´å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨å›¾åƒæ–‡æœ¬å¯¹ä¸Šçš„èƒ½åŠ›å·²ç»å®ç°äº†é€šç”¨è§†è§‰è¯­è¨€åŠŸèƒ½ã€‚ç„¶è€Œï¼Œç¼ºä¹åŒºåŸŸæ–‡æœ¬å¯¹é™åˆ¶äº†å…¶åœ¨ç»†ç²’åº¦å¤šæ¨¡æ€ç†è§£æ–¹é¢çš„è¿›å±•ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ç©ºé—´æŒ‡ä»¤è°ƒæ•´æ–¹æ³•ï¼Œè¯¥æ–¹æ³•åœ¨æŒ‡ä»¤ä¸­å¼•å…¥äº†æ„Ÿå…´è¶£åŒºåŸŸï¼ˆRoIï¼‰çš„å¼•ç”¨ã€‚åœ¨å‘é€åˆ°LLMä¹‹å‰ï¼Œè¯¥å¼•ç”¨è¢«æ›¿æ¢ä¸ºRoIç‰¹å¾ï¼Œå¹¶ä¸è¯­è¨€åµŒå…¥äº¤é”™ä½œä¸ºåºåˆ—ã€‚æˆ‘ä»¬çš„æ¨¡å‹GPT4RoIåœ¨7ä¸ªåŒºåŸŸæ–‡æœ¬å¯¹æ•°æ®é›†ä¸Šè¿›è¡Œè®­ç»ƒï¼Œä¸ä¹‹å‰çš„å›¾åƒçº§åˆ«æ¨¡å‹ç›¸æ¯”ï¼Œå®ƒå¸¦æ¥äº†å‰æ‰€æœªæœ‰çš„äº¤äº’å’Œå¯¹è¯ä½“éªŒã€‚ï¼ˆ1ï¼‰è¶…è¶Šè¯­è¨€çš„äº¤äº’ï¼šç”¨æˆ·å¯ä»¥é€šè¿‡è¯­è¨€å’Œç»˜åˆ¶è¾¹ç•Œæ¡†æ¥ä¸æˆ‘ä»¬çš„æ¨¡å‹è¿›è¡Œäº¤äº’ï¼Œä»¥çµæ´»åœ°è°ƒæ•´æŒ‡ä»£ç²’åº¦ã€‚ï¼ˆ2ï¼‰å¤šåŠŸèƒ½çš„å¤šæ¨¡æ€èƒ½åŠ›ï¼šGPT4RoIå¯ä»¥æŒ–æ˜æ¯ä¸ªRoIå†…çš„å„ç§å±æ€§ä¿¡æ¯ï¼Œä¾‹å¦‚é¢œè‰²ã€å½¢çŠ¶ã€æè´¨ã€åŠ¨ä½œç­‰ã€‚æ­¤å¤–ï¼Œå®ƒè¿˜å¯ä»¥åŸºäºå¸¸è¯†å¯¹å¤šä¸ªRoIè¿›è¡Œæ¨ç†ã€‚åœ¨è§†è§‰å¸¸è¯†æ¨ç†ï¼ˆVCRï¼‰æ•°æ®é›†ä¸Šï¼ŒGPT4RoIå–å¾—äº†81.6%çš„æ˜¾è‘—å‡†ç¡®ç‡ï¼Œæ˜¾è‘—è¶…è¶Šäº†ç°æœ‰æ¨¡å‹ï¼ˆç¬¬äºŒåæ˜¯75.6%ï¼‰ï¼Œå‡ ä¹è¾¾åˆ°äº†äººç±»æ°´å¹³çš„æ€§èƒ½85.0%ã€‚ä»£ç å’Œæ¨¡å‹å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/jshilong/GPT4RoI">https://github.com/jshilong/GPT4RoI</a>æ‰¾åˆ°ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.03601v5">PDF</a> ECCV2024-Workshop, Camera-ready</p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰é€šè¿‡è§†è§‰æŒ‡ä»¤è°ƒæ•´å›¾åƒæ–‡æœ¬å¯¹ï¼Œå·²å…·å¤‡é€šç”¨è§†è§‰è¯­è¨€åŠŸèƒ½ã€‚ç„¶è€Œï¼Œç”±äºç¼ºä¹åŒºåŸŸæ–‡æœ¬å¯¹ï¼Œå…¶ç²¾ç»†ç²’åº¦å¤šæ¨¡å¼ç†è§£çš„å‘å±•å—åˆ°é™åˆ¶ã€‚æœ¬æ–‡æå‡ºç©ºé—´æŒ‡ä»¤è°ƒæ•´ï¼Œå¼•å…¥æŒ‡ä»¤ä¸­çš„æ„Ÿå…´è¶£åŒºåŸŸï¼ˆRoIï¼‰å‚è€ƒã€‚åœ¨å‘é€åˆ°LLMä¹‹å‰ï¼Œé€šè¿‡æ›¿æ¢RoIç‰¹å¾å¹¶å°†å…¶ä¸è¯­è¨€åµŒå…¥ä½œä¸ºåºåˆ—äº¤ç»‡ï¼Œæˆ‘ä»¬çš„æ¨¡å‹GPT4RoIåœ¨7ä¸ªåŒºåŸŸæ–‡æœ¬å¯¹æ•°æ®é›†ä¸Šè¿›è¡Œè®­ç»ƒï¼Œå¸¦æ¥äº†å‰æ‰€æœªæœ‰çš„äº¤äº’å¼ä½“éªŒï¼Œç›¸è¾ƒäºä¹‹å‰çš„å›¾åƒçº§åˆ«æ¨¡å‹æœ‰ç€æ›´é«˜çš„äº’åŠ¨æ€§å’Œä¼šè¯ä½“éªŒã€‚å®ƒè¿˜å¯ä»¥åˆ©ç”¨å¤šç§å±æ€§ä¿¡æ¯ï¼Œå¦‚é¢œè‰²ã€å½¢çŠ¶ã€æè´¨ã€åŠ¨ä½œç­‰ï¼Œå¹¶åŸºäºå¸¸è¯†å¯¹å¤šä¸ªRoIè¿›è¡Œæ¨ç†ã€‚åœ¨è§†è§‰å¸¸è¯†æ¨ç†ï¼ˆVCRï¼‰æ•°æ®é›†ä¸Šï¼ŒGPT4RoIå–å¾—äº†81.6%çš„å‡†ç¡®ç‡ï¼Œæ˜¾è‘—è¶…è¶Šäº†ç°æœ‰æ¨¡å‹ï¼Œå¹¶æ¥è¿‘äººç±»æ°´å¹³çš„æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç©ºé—´æŒ‡ä»¤è°ƒæ•´å¼•å…¥æ„Ÿå…´è¶£åŒºåŸŸï¼ˆRoIï¼‰å‚è€ƒï¼Œæå‡å¤šæ¨¡å¼ç†è§£çš„ç²¾ç»†ç²’åº¦ã€‚</li>
<li>GPT4RoIæ¨¡å‹é€šè¿‡ç»“åˆRoIç‰¹å¾å’Œè¯­è¨€åµŒå…¥ï¼Œæé«˜äº†å¤§å‹è¯­è¨€æ¨¡å‹åœ¨å›¾åƒæ–‡æœ¬å¯¹ä¸Šçš„é€šç”¨è§†è§‰è¯­è¨€åŠŸèƒ½ã€‚</li>
<li>GPT4RoIæ”¯æŒé€šè¿‡è¯­è¨€å’Œç»˜åˆ¶è¾¹ç•Œæ¡†ä¸ç”¨æˆ·äº¤äº’ï¼Œçµæ´»è°ƒæ•´æŒ‡ä»£ç²’åº¦ã€‚</li>
<li>GPT4RoIèƒ½å¤ŸæŒ–æ˜æ¯ä¸ªRoIå†…çš„å¤šç§å±æ€§ä¿¡æ¯ï¼Œå¦‚é¢œè‰²ã€å½¢çŠ¶ã€æè´¨ã€åŠ¨ä½œç­‰ã€‚</li>
<li>GPT4RoIåœ¨è§†è§‰å¸¸è¯†æ¨ç†ï¼ˆVCRï¼‰æ•°æ®é›†ä¸Šå®ç°äº†é«˜å‡†ç¡®ç‡ï¼Œè¾¾åˆ°81.6%ï¼Œæ˜¾è‘—è¶…è¶Šç°æœ‰æ¨¡å‹ã€‚</li>
<li>GPT4RoIæ¨¡å‹å…·æœ‰å¼ºå¤§çš„æ¨ç†èƒ½åŠ›ï¼Œå¯ä»¥åŸºäºå¸¸è¯†å¯¹å¤šä¸ªRoIè¿›è¡Œæ¨ç†ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2307.03601">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-bacceaa5b739488680cd4a31fc507c6f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-944f644a69a7edec1a619783b9e14c67.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c7e00f0f683d10ba6d3bf62809c67610.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-06-21/R1_Reasoning/">https://kedreamix.github.io/Talk2Paper/Paper/2025-06-21/R1_Reasoning/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/R1-Reasoning/">
                                    <span class="chip bg-color">R1_Reasoning</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-06-21/Agent/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-e9aa93264bf1c1aedddcf51b2ad6f237.jpg" class="responsive-img" alt="Agent">
                        
                        <span class="card-title">Agent</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Agent æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-06-21  Learning to Incentivize in Repeated Principal-Agent Problems with Adversarial Agent Arrivals
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-06-21
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Agent/" class="post-category">
                                    Agent
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Agent/">
                        <span class="chip bg-color">Agent</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-06-20/R1_Reasoning/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-dab5064d282966610b2cee845500a27b.jpg" class="responsive-img" alt="R1_Reasoning">
                        
                        <span class="card-title">R1_Reasoning</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            R1_Reasoning æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-06-20  AutoRule Reasoning Chain-of-thought Extracted Rule-based Rewards   Improve Preference Learning
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-06-20
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/R1-Reasoning/" class="post-category">
                                    R1_Reasoning
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/R1-Reasoning/">
                        <span class="chip bg-color">R1_Reasoning</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">26522.9k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
