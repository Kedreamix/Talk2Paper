<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Vision Transformer">
    <meta name="description" content="Vision Transformer 方向最新论文已更新，请持续关注 Update in 2025-06-25  Deep CNN Face Matchers Inherently Support Revocable Biometric Templates">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Vision Transformer | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loader页面消失采用渐隐的方式*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logo出现动画 */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//使用渐隐的方法淡出loading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//强制显示loading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">嘘~  正在从服务器偷取页面 . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>首页</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>标签</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>分类</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>归档</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="搜索" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="深色/浅色模式" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			首页
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			标签
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			分类
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			归档
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-39cb9836b423c7bcf242fca3e66c8c6a.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Vision Transformer</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- 文章内容详情 -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Vision-Transformer/">
                                <span class="chip bg-color">Vision Transformer</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Vision-Transformer/" class="post-category">
                                Vision Transformer
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp;
                    2025-06-25
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>更新日期:&nbsp;&nbsp;
                    2025-07-06
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>文章字数:&nbsp;&nbsp;
                    8.2k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>阅读时长:&nbsp;&nbsp;
                    33 分
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>阅读次数:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>⚠️ 以下所有内容总结都来自于 大语言模型的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p>
</blockquote>
<h1 id="2025-06-25-更新"><a href="#2025-06-25-更新" class="headerlink" title="2025-06-25 更新"></a>2025-06-25 更新</h1><h2 id="Deep-CNN-Face-Matchers-Inherently-Support-Revocable-Biometric-Templates"><a href="#Deep-CNN-Face-Matchers-Inherently-Support-Revocable-Biometric-Templates" class="headerlink" title="Deep CNN Face Matchers Inherently Support Revocable Biometric Templates"></a>Deep CNN Face Matchers Inherently Support Revocable Biometric Templates</h2><p><strong>Authors:Aman Bhatta, Michael C. King, Kevin W. Bowyer</strong></p>
<p>One common critique of biometric authentication is that if an individual’s biometric is compromised, then the individual has no recourse. The concept of revocable biometrics was developed to address this concern. A biometric scheme is revocable if an individual can have their current enrollment in the scheme revoked, so that the compromised biometric template becomes worthless, and the individual can re-enroll with a new template that has similar recognition power. We show that modern deep CNN face matchers inherently allow for a robust revocable biometric scheme. For a given state-of-the-art deep CNN backbone and training set, it is possible to generate an unlimited number of distinct face matcher models that have both (1) equivalent recognition power, and (2) strongly incompatible biometric templates. The equivalent recognition power extends to the point of generating impostor and genuine distributions that have the same shape and placement on the similarity dimension, meaning that the models can share a similarity threshold for a 1-in-10,000 false match rate. The biometric templates from different model instances are so strongly incompatible that the cross-instance similarity score for images of the same person is typically lower than the same-instance similarity score for images of different persons. That is, a stolen biometric template that is revoked is of less value in attempting to match the re-enrolled identity than the average impostor template. We also explore the feasibility of using a Vision Transformer (ViT) backbone-based face matcher in the revocable biometric system proposed in this work and demonstrate that it is less suitable compared to typical ResNet-based deep CNN backbones. </p>
<blockquote>
<p>关于生物认证的常见批评之一是，如果个人的生物识别信息被泄露，那么个人便无计可施。为解决这一担忧，开发了可撤销生物识别技术这一概念。如果一个生物识别方案是可撤销的，那么个人就可以撤销其在该方案中的当前注册，这样泄露的生物识别模板就会变得毫无价值，个人可以重新注册一个新的模板，这个新模板具有类似的识别能力。我们展示了现代深度卷积神经网络面部匹配器本质上允许稳健的可撤销生物识别方案。对于给定的先进深度卷积神经网络主干和训练集，可以生成无限数量的独特面部匹配器模型，这些模型既具有（1）相当的识别能力，又拥有（2）高度不兼容的生物识别模板。相当的识别能力扩展到了生成假冒者和真实者的分布，这些分布在相似度维度上具有相同的形状和位置，这意味着这些模型可以共享一个相似度阈值，以达到万分之一的误匹配率。不同模型实例的生物识别模板之间高度不兼容，以至于同一人的不同图像在不同实例之间的相似度分数通常低于不同人的不同图像的相似度分数。也就是说，被撤销的被盗生物识别模板在尝试匹配重新注册的身份时，其价值通常低于假冒者模板的平均值。我们还探讨了在本工作中提出的可撤销生物识别系统中使用基于视觉转换器（ViT）的面部匹配器的可行性，并证明与典型的基于ResNet的深度卷积神经网络主干相比，它不太适合。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.18731v1">PDF</a> </p>
<p><strong>摘要</strong></p>
<p>关于生物识别认证的常见批评之一是，一旦个体的生物识别信息被泄露，个体将束手无策。为解决这一担忧，发展了可撤销生物识别技术。当个体能撤销其在生物识别方案中的当前注册，使得已泄露的生物识别模板失效并重新注册新模板时，该生物识别方案被认为是可撤销的。新模板应具有相似的识别能力。本文展示了现代深度CNN面部识别器天然支持强大的可撤销生物识别方案。对于给定的先进深度CNN主干和训练集，可以生成无限数量的面部识别器模型实例，这些模型实例不仅具有等同的识别能力，而且其生物识别模板之间具有很强的不兼容性和差异性。这意味着，即使某个人的生物识别模板被盗用并撤销后，其重新注册的新模板的匹配价值低于随机假冒者的匹配价值。此外，本文还探讨了将Vision Transformer（ViT）主干应用于本文提出的可撤销生物识别系统的可行性，并证明相较于典型的基于ResNet的深度CNN主干，ViT的应用表现不太理想。</p>
<p><strong>关键见解</strong></p>
<ol>
<li>可撤销生物识别技术解决了生物识别信息泄露后无解决方案的问题。</li>
<li>现代深度CNN面部识别器允许生成具有等效识别能力的多种模型实例。</li>
<li>这些模型实例的识别模板具有强烈的兼容性和差异性。这意味着撤销的生物识别模板匹配价值较低。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.18731">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-69de9e39b449ad62df6443a99ee75000.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-39cb9836b423c7bcf242fca3e66c8c6a.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-8c3f943d592b397456fd2cbd75e431af.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-1fe18f947502b7e457f27b5c72a17ff2.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Taming-Vision-Language-Models-for-Medical-Image-Analysis-A-Comprehensive-Review"><a href="#Taming-Vision-Language-Models-for-Medical-Image-Analysis-A-Comprehensive-Review" class="headerlink" title="Taming Vision-Language Models for Medical Image Analysis: A   Comprehensive Review"></a>Taming Vision-Language Models for Medical Image Analysis: A   Comprehensive Review</h2><p><strong>Authors:Haoneng Lin, Cheng Xu, Jing Qin</strong></p>
<p>Modern Vision-Language Models (VLMs) exhibit unprecedented capabilities in cross-modal semantic understanding between visual and textual modalities. Given the intrinsic need for multi-modal integration in clinical applications, VLMs have emerged as a promising solution for a wide range of medical image analysis tasks. However, adapting general-purpose VLMs to medical domain poses numerous challenges, such as large domain gaps, complicated pathological variations, and diversity and uniqueness of different tasks. The central purpose of this review is to systematically summarize recent advances in adapting VLMs for medical image analysis, analyzing current challenges, and recommending promising yet urgent directions for further investigations. We begin by introducing core learning strategies for medical VLMs, including pretraining, fine-tuning, and prompt learning. We then categorize five major VLM adaptation strategies for medical image analysis. These strategies are further analyzed across eleven medical imaging tasks to illustrate their current practical implementations. Furthermore, we analyze key challenges that impede the effective adaptation of VLMs to clinical applications and discuss potential directions for future research. We also provide an open-access repository of related literature to facilitate further research, available at <a target="_blank" rel="noopener" href="https://github.com/haonenglin/Awesome-VLM-for-MIA">https://github.com/haonenglin/Awesome-VLM-for-MIA</a>. It is anticipated that this article can help researchers who are interested in harnessing VLMs in medical image analysis tasks have a better understanding on their capabilities and limitations, as well as current technical barriers, to promote their innovative, robust, and safe application in clinical practice. </p>
<blockquote>
<p>现代视觉语言模型（VLMs）在视觉和文本模态之间的跨模态语义理解方面展现出了前所未有的能力。鉴于临床应用中对多模态集成的内在需求，VLMs已成为广泛应用于医疗图像分析任务的有前途的解决方案。然而，将通用VLMs适应于医学领域带来了许多挑战，例如领域差距大、病理变化复杂以及不同任务的多样性和独特性。本文的中心目的是系统地总结近年来将VLMs适应于医疗图像分析的最新进展，分析当前挑战，并为进一步的调查推荐有前途且紧迫的方向。我们首先介绍医学VLMs的核心学习策略，包括预训练、微调和提示学习。然后我们将五大VLM适应策略分类为医疗图像分析。这些策略进一步在十一个医疗成像任务中进行分析，以说明它们当前的实用实现。此外，我们分析了阻碍VLMs有效适应临床应用的关键挑战，并讨论了未来研究的方向。我们还提供了一个开放访问的相关文献仓库，以促进进一步研究，可在<a target="_blank" rel="noopener" href="https://github.com/haonenglin/Awesome-VLM-for-MIA%E8%8E%B7%E5%8F%96%E3%80%82%E9%A2%84%E8%AE%A1%E6%9C%AC%E6%96%87%E8%83%BD%E5%B8%AE%E5%8A%A9%E5%AF%B9%E5%9C%A8%E5%8C%BB%E7%96%97%E5%9B%BE%E5%83%8F%E5%88%86%E6%9E%90%E4%BB%BB%E5%8A%A1%E4%B8%AD%E4%BD%BF%E7%94%A8VLMs%E6%84%9F%E5%85%B4%E8%B6%A3%E7%9A%84%E7%A0%94%E7%A9%B6%E4%BA%BA%E5%91%98%E6%9B%B4%E5%A5%BD%E5%9C%B0%E4%BA%86%E8%A7%A3%E5%AE%83%E4%BB%AC%E7%9A%84%E8%83%BD%E5%8A%9B%E3%80%81%E5%B1%80%E9%99%90%E6%80%A7%E4%BB%A5%E5%8F%8A%E5%BD%93%E5%89%8D%E7%9A%84%E6%8A%80%E6%9C%AF%E9%9A%9C%E7%A2%8D%EF%BC%8C%E4%BB%A5%E4%BF%83%E8%BF%9B%E5%85%B6%E5%9C%A8%E4%B8%B4%E5%BA%8A%E5%AE%9E%E8%B7%B5%E4%B8%AD%E7%9A%84%E5%88%9B%E6%96%B0%E3%80%81%E7%A8%B3%E5%81%A5%E5%92%8C%E5%AE%89%E5%85%A8%E5%BA%94%E7%94%A8%E3%80%82">https://github.com/haonenglin/Awesome-VLM-for-MIA获取。预计本文能帮助对在医疗图像分析任务中使用VLMs感兴趣的研究人员更好地了解它们的能力、局限性以及当前的技术障碍，以促进其在临床实践中的创新、稳健和安全应用。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.18378v1">PDF</a> 34 pages</p>
<p><strong>Summary</strong><br>视觉语言模型（VLMs）在跨模态语义理解方面展现出前所未有的能力，对于医学影像分析任务具有广泛应用前景。然而，将其应用于医学领域面临诸多挑战。本文综述了近期VLMs在医学影像分析中的进展、五大适应性策略及其在十一种医学影像任务中的应用实例。同时，本文分析了关键挑战并探讨了未来研究方向。提供的相关文献库有助于研究人员了解VLMs在医学影像分析中的能力与局限。</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>VLMs展现出跨模态语义理解的强大能力，尤其在医学影像分析领域有广泛应用前景。</li>
<li>VLMs应用于医学领域面临大领域差距、复杂的病理变化和任务多样性等挑战。</li>
<li>本文综述了五大VLM适应性策略，包括核心学习策略如预训练、微调、提示学习等。</li>
<li>在十一种医学影像任务中，详细阐述了这些策略的实际应用实例。</li>
<li>分析了阻碍VLMs有效适应临床应用的挑战，并讨论了未来研究方向。</li>
<li>文章提供了一个开放访问的相关文献库，以促进进一步研究。</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.18378">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-a0376da397e792bb832c73aac3361f73.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-264b884208ed5bdd521bb7d3d447a5a2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-36053664037a7a8e432d0772cd789522.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="HIRE-Lightweight-High-Resolution-Image-Feature-Enrichment-for-Multimodal-LLMs"><a href="#HIRE-Lightweight-High-Resolution-Image-Feature-Enrichment-for-Multimodal-LLMs" class="headerlink" title="HIRE: Lightweight High-Resolution Image Feature Enrichment for   Multimodal LLMs"></a>HIRE: Lightweight High-Resolution Image Feature Enrichment for   Multimodal LLMs</h2><p><strong>Authors:Nikitha SR, Aradhya Neeraj Mathur, Tarun Ram Menta, Rishabh Jain, Mausoom Sarkar</strong></p>
<p>The integration of high-resolution image features in modern multimodal large language models has demonstrated significant improvements in fine-grained visual understanding tasks, achieving high performance across multiple benchmarks. Since these features are obtained from large image encoders like ViT, they come with a significant increase in computational costs due to multiple calls to these encoders. In this work, we first develop an intuition for feature upsampling as a natural extension of high-resolution feature generation. Through extensive experiments and ablations, we demonstrate how a shallow feature enricher can achieve competitive results with tremendous reductions in training and inference time as well as computational cost, with upto 1.5x saving in FLOPs. </p>
<blockquote>
<p>将高分辨率图像特征融入现代多模态大型语言模型，已经在细粒度视觉理解任务中显示出重大改进，并在多个基准测试中实现高性能。由于这些特征来自大型图像编码器（如ViT），因此它们需要通过多次调用这些编码器而获得，从而带来计算成本的显著增加。在这项工作中，我们首先发展了一种特征上采样作为高分辨率特征生成的自然扩展的直觉。通过广泛的实验和消融实验，我们展示了浅层特征丰富器如何在训练和推理时间以及计算成本方面实现巨大减少的同时，取得具有竞争力的结果，并且浮点运算量最多可减少1.5倍。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.17608v1">PDF</a> Accepted in CVPR 2025 Workshop on What’s Next in Multimodal   Foundational Models</p>
<p><strong>Summary</strong></p>
<p>现代多模态大型语言模型中集成了高分辨率图像特征，这在精细粒度的视觉理解任务中取得了显著的提升，并在多个基准测试中实现了高性能。然而，由于需要从大型图像编码器（如ViT）获取这些特征，计算成本显著增加。本文首次提出特征上采样作为高分辨率特征生成的自然扩展。通过广泛的实验和消融研究，我们展示了浅层特征丰富器如何在减少训练和推理时间以及计算成本的同时实现具有竞争力的结果，最多可减少1.5倍的FLOPs。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>高分辨率图像特征在现代多模态大型语言模型中的集成显著提高了精细粒度的视觉理解性能。</li>
<li>这些特征来自大型图像编码器，如ViT，导致计算成本增加。</li>
<li>特征上采样作为高分辨率特征生成的扩展被提出。</li>
<li>通过实验和消融研究，证明了浅层特征丰富器在减少训练和推理时间以及计算成本方面的有效性。</li>
<li>浅层特征丰富器实现了具有竞争力的结果，最多可减少1.5倍的FLOPs。</li>
<li>此方法为提高视觉任务的性能提供了一种新的思路，并可能在未来的研究中得到进一步的应用和发展。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.17608">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-3b456ef7d1832d79df20503d362ff775.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c2f7724543f06be5e049f3f8b6926ea2.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-4bde5d6a2fd901f071df540055869d8b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e20be98369d6f000e63fe27f27d0267a.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="AQUA20-A-Benchmark-Dataset-for-Underwater-Species-Classification-under-Challenging-Conditions"><a href="#AQUA20-A-Benchmark-Dataset-for-Underwater-Species-Classification-under-Challenging-Conditions" class="headerlink" title="AQUA20: A Benchmark Dataset for Underwater Species Classification under   Challenging Conditions"></a>AQUA20: A Benchmark Dataset for Underwater Species Classification under   Challenging Conditions</h2><p><strong>Authors:Taufikur Rahman Fuad, Sabbir Ahmed, Shahriar Ivan</strong></p>
<p>Robust visual recognition in underwater environments remains a significant challenge due to complex distortions such as turbidity, low illumination, and occlusion, which severely degrade the performance of standard vision systems. This paper introduces AQUA20, a comprehensive benchmark dataset comprising 8,171 underwater images across 20 marine species reflecting real-world environmental challenges such as illumination, turbidity, occlusions, etc., providing a valuable resource for underwater visual understanding. Thirteen state-of-the-art deep learning models, including lightweight CNNs (SqueezeNet, MobileNetV2) and transformer-based architectures (ViT, ConvNeXt), were evaluated to benchmark their performance in classifying marine species under challenging conditions. Our experimental results show ConvNeXt achieving the best performance, with a Top-3 accuracy of 98.82% and a Top-1 accuracy of 90.69%, as well as the highest overall F1-score of 88.92% with moderately large parameter size. The results obtained from our other benchmark models also demonstrate trade-offs between complexity and performance. We also provide an extensive explainability analysis using GRAD-CAM and LIME for interpreting the strengths and pitfalls of the models. Our results reveal substantial room for improvement in underwater species recognition and demonstrate the value of AQUA20 as a foundation for future research in this domain. The dataset is publicly available at: <a target="_blank" rel="noopener" href="https://huggingface.co/datasets/taufiktrf/AQUA20">https://huggingface.co/datasets/taufiktrf/AQUA20</a>. </p>
<blockquote>
<p>在水下环境中实现稳健的视觉识别仍然是一个重大挑战，因为诸如浑浊、低光照和遮挡之类的复杂失真会严重降低标准视觉系统的性能。本文介绍了AQUA20，这是一个包含8171张水下图像的综合基准数据集，涵盖了20种海洋物种，反映了现实世界中的环境挑战，如光照、浑浊度、遮挡等，为水下视觉理解提供了宝贵的资源。我们评估了13种最先进的深度学习模型，包括轻量级CNN（SqueezeNet、MobileNetV2）和基于transformer的架构（ViT、ConvNeXt），以基准测试它们在具有挑战性的条件下对海洋物种进行分类的性能。实验结果表明，ConvNeXt表现最佳，前三名准确率达到了98.82%，第一名准确率为90.69%，总体F1分数最高，达到了88.92%，且参数规模适中。其他基准模型的结果也显示了复杂性和性能之间的权衡。我们还使用GRAD-CAM和LIME进行了广泛的解释性分析，以解释模型的优点和缺点。我们的结果揭示了水下物种识别方面仍有很大的改进空间，并表明了AQUA20作为未来该领域研究基础的价值。数据集可在<a target="_blank" rel="noopener" href="https://huggingface.co/datasets/taufiktrf/AQUA20">https://huggingface.co/datasets/taufiktrf/AQUA20</a>公开获取。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.17455v1">PDF</a> Submitted to AJSE Springer</p>
<p><strong>Summary</strong></p>
<p>本文介绍了一个名为AQUA20的水下图像数据集，包含8,171张反映真实水下环境挑战（如光照、浊度、遮挡等）的20种海洋生物图像。文章评估了包括轻量化CNN（如SqueezeNet、MobileNetV2）和基于transformer的架构（如ViT、ConvNeXt）在内的13种最先进的深度学习模型在水下物种识别方面的性能。实验结果显示，ConvNeXt表现最佳，前三准确率达到了98.82%，最高准确率达到了90.69%，总体F1分数最高，为88.92%。同时，文章还通过GRAD-CAM和LIME进行了模型的可解释性分析。该数据集公开可用，为未来的水下视觉研究提供了有价值的资源。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>AQUA20是一个包含多种水下环境挑战的综合基准数据集，涵盖8,171张水下图像，涉及20种海洋生物。</li>
<li>文章评估了13种最先进的深度学习模型在水下物种识别方面的性能。</li>
<li>ConvNeXt在实验中表现最佳，前三准确率达到了98.82%，最高准确率达到了90.69%，总体F1分数最高。</li>
<li>数据集提供了模型可解释性分析，有助于理解模型的优点和缺点。</li>
<li>该数据集公开可用，为未来的水下视觉研究提供了重要资源。</li>
<li>实验结果显示，在复杂的水下环境下，深度学习模型仍然存在性能上的挑战和提升空间。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.17455">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-41767f56929bf8b13c5a4a07e96884e6.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="RadarSeq-A-Temporal-Vision-Framework-for-User-Churn-Prediction-via-Radar-Chart-Sequences"><a href="#RadarSeq-A-Temporal-Vision-Framework-for-User-Churn-Prediction-via-Radar-Chart-Sequences" class="headerlink" title="RadarSeq: A Temporal Vision Framework for User Churn Prediction via   Radar Chart Sequences"></a>RadarSeq: A Temporal Vision Framework for User Churn Prediction via   Radar Chart Sequences</h2><p><strong>Authors:Sina Najafi, M. Hadi Sepanj, Fahimeh Jafari</strong></p>
<p>Predicting user churn in non-subscription gig platforms, where disengagement is implicit, poses unique challenges due to the absence of explicit labels and the dynamic nature of user behavior. Existing methods often rely on aggregated snapshots or static visual representations, which obscure temporal cues critical for early detection. In this work, we propose a temporally-aware computer vision framework that models user behavioral patterns as a sequence of radar chart images, each encoding day-level behavioral features. By integrating a pretrained CNN encoder with a bidirectional LSTM, our architecture captures both spatial and temporal patterns underlying churn behavior. Extensive experiments on a large real-world dataset demonstrate that our method outperforms classical models and ViT-based radar chart baselines, yielding gains of 17.7 in F1 score, 29.4 in precision, and 16.1 in AUC, along with improved interpretability. The framework’s modular design, explainability tools, and efficient deployment characteristics make it suitable for large-scale churn modeling in dynamic gig-economy platforms. </p>
<blockquote>
<p>在非订阅制的零工平台预测用户流失带来了独特的挑战，因为这里不存在明确的标签和用户行为的动态性导致用户离场行为不明显。现有方法往往依赖于聚合快照或静态视觉表示，这会掩盖早期检测的关键时间线索。在这项工作中，我们提出了一种具有时间感知能力的计算机视觉框架，该框架将用户行为模式建模为雷达图图像序列，每个图像都编码日间行为特征。通过将预训练的CNN编码器与双向LSTM集成在一起，我们的架构捕获了用户流失行为背后的时空模式。在大型真实数据集上进行的广泛实验表明，我们的方法在F1得分、精确度和AUC方面分别提高了17.7分、29.4分和16.1分，超过了经典模型和基于ViT的雷达图基线，同时提高了可解释性。该框架的模块化设计、解释工具以及高效的部署特点使其成为动态零工经济平台大规模流失建模的合适选择。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.17325v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>该文本提出了一种针对非订阅制零工平台用户流失预测的计算机视觉框架。该框架通过结合预训练的CNN编码器和双向LSTM，对用户行为模式进行建模，以雷达图序列的形式捕捉空间和时间模式。实验证明，该方法在真实数据集上的表现优于经典模型和基于ViT的雷达图基线，提高了F1分数、精确度和AUC等指标，同时具有较好的可解释性。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>非订阅制零工平台用户流失预测面临独特挑战，如缺乏明确的标签和用户行为的动态性。</li>
<li>现有方法常依赖于聚合快照或静态视觉表示，忽略了早期检测所需的时间线索。</li>
<li>提出的计算机视觉框架采用雷达图序列建模用户行为模式，编码每日行为特征。</li>
<li>结合预训练的CNN编码器和双向LSTM，捕捉用户流失行为的空间和时间模式。</li>
<li>在真实数据集上进行的大量实验证明，该方法在F1分数、精确度和AUC等方面表现优异。</li>
<li>该框架具有模块化设计、可解释性工具和高效部署特点，适合大规模应用于动态零工经济平台用户流失建模。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.17325">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-4a1a5449934925df3f27b0fbef80d0cd.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5f6fd034d33222e09a4e3847c1037bc9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-fe343066c1e05acb3c2063850e1fca0d.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="CLIP-HandID-Vision-Language-Model-for-Hand-Based-Person-Identification"><a href="#CLIP-HandID-Vision-Language-Model-for-Hand-Based-Person-Identification" class="headerlink" title="CLIP-HandID: Vision-Language Model for Hand-Based Person Identification"></a>CLIP-HandID: Vision-Language Model for Hand-Based Person Identification</h2><p><strong>Authors:Nathanael L. Baisa, Babu Pallam, Amudhavel Jayavel</strong></p>
<p>This paper introduces a novel approach to person identification using hand images, designed specifically for criminal investigations. The method is particularly valuable in serious crimes such as sexual abuse, where hand images are often the only identifiable evidence available. Our proposed method, CLIP-HandID, leverages a pre-trained foundational vision-language model - CLIP - to efficiently learn discriminative deep feature representations from hand images (input to CLIP’s image encoder) using textual prompts as semantic guidance. Since hand images are labeled with indexes rather than text descriptions, we employ a textual inversion network to learn pseudo-tokens that encode specific visual contexts or appearance attributes. These learned pseudo-tokens are then incorporated into textual prompts, which are fed into CLIP’s text encoder to leverage its multi-modal reasoning and enhance generalization for identification. Through extensive evaluations on two large, publicly available hand datasets with multi-ethnic representation, we demonstrate that our method significantly outperforms existing approaches. </p>
<blockquote>
<p>本文介绍了一种利用手部图像进行人员识别的新方法，该方法专为刑事侦查设计。该方法在性虐待等严重犯罪中尤其具有价值，在这些情况下，手部图像往往是唯一可用的可识别证据。我们提出的方法CLIP-HandID，利用预训练的通用视觉语言模型CLIP，通过文本提示作为语义指导，有效地从手部图像（输入CLIP图像编码器）中学习判别深度特征表示。由于手部图像用索引而不是文本描述来标记，我们采用文本倒置网络来学习编码特定视觉上下文或外观属性的伪令牌。然后，这些学习到的伪令牌被纳入文本提示中，输入到CLIP的文本编码器中，以利用多模式推理并增强识别推广能力。我们在两个具有多民族代表性的大型公开手部数据集上进行了广泛评估，结果表明我们的方法显著优于现有方法。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.12447v2">PDF</a> </p>
<p><strong>Summary</strong><br>该研究提出了一种利用手部图像进行人员识别的新方法，尤其适用于刑事调查。该方法在性虐待等严重犯罪中尤其有价值，手部图像往往是唯一可用的可识别证据。提出的CLIP-HandID方法利用预训练的视觉语言模型CLIP，通过文本提示作为语义指导，从手部图像中学习辨别深度特征表示。由于手部图像用索引而非文本描述进行标注，因此采用文本倒置网络学习特定视觉上下文或外观属性的伪标记。这些学习到的伪标记融入文本提示中，并输入CLIP的文本编码器，以利用其多模式推理并增强识别效果的泛化能力。在具有多民族代表性的两个大型公开手部数据集上的广泛评估表明，该方法显著优于现有方法。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>该论文介绍了一种利用手部图像进行人员识别的新方法，特别适用于刑事调查中的严重犯罪。</li>
<li>提出了CLIP-HandID方法，利用预训练的视觉语言模型CLIP来学习手部图像的特征表示。</li>
<li>文本提示作为语义指导，用于增强模型对手部图像的识别能力。</li>
<li>由于手部图像使用索引标注，因此采用文本倒置网络学习特定视觉上下文或外观属性的伪标记。</li>
<li>将学习到的伪标记融入文本提示，提高模型的泛化能力。</li>
<li>在具有多民族代表性的大型公开手部数据集上进行了广泛评估。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.12447">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-56069efcc3c8978aced9567d0c791953.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ef06b5e3ecbed6b2d20383e008ef3119.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-933d8bd637d9332ee6482b13b7422f5a.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="SALT-A-Flexible-Semi-Automatic-Labeling-Tool-for-General-LiDAR-Point-Clouds-with-Cross-Scene-Adaptability-and-4D-Consistency"><a href="#SALT-A-Flexible-Semi-Automatic-Labeling-Tool-for-General-LiDAR-Point-Clouds-with-Cross-Scene-Adaptability-and-4D-Consistency" class="headerlink" title="SALT: A Flexible Semi-Automatic Labeling Tool for General LiDAR Point   Clouds with Cross-Scene Adaptability and 4D Consistency"></a>SALT: A Flexible Semi-Automatic Labeling Tool for General LiDAR Point   Clouds with Cross-Scene Adaptability and 4D Consistency</h2><p><strong>Authors:Yanbo Wang, Yongtao Chen, Chuan Cao, Tianchen Deng, Wentao Zhao, Jingchuan Wang, Weidong Chen</strong></p>
<p>We propose a flexible Semi-Automatic Labeling Tool (SALT) for general LiDAR point clouds with cross-scene adaptability and 4D consistency. Unlike recent approaches that rely on camera distillation, SALT operates directly on raw LiDAR data, automatically generating pre-segmentation results. To achieve this, we propose a novel zero-shot learning paradigm, termed data alignment, which transforms LiDAR data into pseudo-images by aligning with the training distribution of vision foundation models. Additionally, we design a 4D-consistent prompting strategy and 4D non-maximum suppression module to enhance SAM2, ensuring high-quality, temporally consistent presegmentation. SALT surpasses the latest zero-shot methods by 18.4% PQ on SemanticKITTI and achieves nearly 40-50% of human annotator performance on our newly collected low-resolution LiDAR data and on combined data from three LiDAR types, significantly boosting annotation efficiency. We anticipate that SALT’s open-sourcing will catalyze substantial expansion of current LiDAR datasets and lay the groundwork for the future development of LiDAR foundation models. Code is available at <a target="_blank" rel="noopener" href="https://github.com/Cavendish518/SALT">https://github.com/Cavendish518/SALT</a>. </p>
<blockquote>
<p>我们提出了一种灵活的半自动标注工具（SALT），适用于一般激光雷达点云，具有跨场景适应性和4D一致性。与最近依赖相机蒸馏的方法不同，SALT直接在原始激光雷达数据上运行，自动生成预分割结果。为实现这一点，我们提出了一种新的零样本学习范式，称为数据对齐，通过对齐激光雷达数据与视觉基础模型的训练分布，将激光雷达数据转换为伪图像。此外，我们设计了4D一致提示策略和4D非最大值抑制模块，以增强SAM2，确保高质量、时间一致的预分割。SALT在SemanticKITTI上的PQ得分超过最新零样本方法18.4%，在我们新收集的低分辨率激光雷达数据和三种激光雷达类型组合的数据上，达到人类标注器性能的近40-50%，显著提高了标注效率。我们预计SALT的开源将极大地推动当前激光雷达数据集的发展，并为未来激光雷达基础模型的开发奠定基础。代码可在<a target="_blank" rel="noopener" href="https://github.com/Cavendish518/SALT%E5%A4%84%E9%9D%A2%E5%8F%96%E3%80%82">https://github.com/Cavendish518/SALT处获取。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.23980v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文提出了一种灵活的半自动标注工具（SALT），适用于一般的激光雷达点云数据，具有跨场景适应性和4D一致性。SALT直接在原始激光雷达数据上操作，自动生成预分割结果，不同于依赖相机蒸馏的现有方法。为实现这一点，本文提出了一种名为数据对齐的新型零样本学习范式，通过将激光雷达数据与视觉基础模型的训练分布对齐，将激光雷达数据转换为伪图像。同时，设计了4D一致的提示策略和4D非最大抑制模块，增强SAM2，确保高质量、时间一致的预分割。SALT在SemanticKITTI上的PQ得分比最新的零样本方法高出18.4%，在新收集的低分辨率激光雷达数据和三种激光雷达类型组合的数据上，达到了人类标注器性能的近40-50%，显著提高了标注效率。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>SALT是一种半自动标注工具，适用于一般激光雷达点云数据，具有跨场景适应性和4D一致性。</li>
<li>SALT直接在原始激光雷达数据上操作，自动生成预分割结果。</li>
<li>数据对齐的零样本学习范式将激光雷达数据转换为伪图像，通过与视觉基础模型的训练分布对齐实现。</li>
<li>4D一致的提示策略和4D非最大抑制模块增强SAM2，确保高质量、时间一致的预分割。</li>
<li>SALT在SemanticKITTI数据集上的性能优于其他零样本方法。</li>
<li>SALT在新收集的低分辨率激光雷达数据和多种激光雷达类型组合的数据上取得了接近人类标注器性能的成果。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.23980">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-fb82127c069da71b8b8ea015fba8cbd0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-79d5371cfeada8bf5f4646f7c5bcde04.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-9d3097c2fb102438b8ecfe6c853269ab.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d88e077cf6c1548c84a50d721b5e0180.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-edb242785650d7312656771c3249c10a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-61bb2172546ea85f6e20be15a5a27fb2.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Transformer-based-RGB-T-Tracking-with-Channel-and-Spatial-Feature-Fusion"><a href="#Transformer-based-RGB-T-Tracking-with-Channel-and-Spatial-Feature-Fusion" class="headerlink" title="Transformer-based RGB-T Tracking with Channel and Spatial Feature Fusion"></a>Transformer-based RGB-T Tracking with Channel and Spatial Feature Fusion</h2><p><strong>Authors:Yunfeng Li, Bo Wang, Ye Li</strong></p>
<p>The main problem in RGB-T tracking is the correct and optimal merging of the cross-modal features of visible and thermal images. Some previous methods either do not fully exploit the potential of RGB and TIR information for channel and spatial feature fusion or lack a direct interaction between the template and the search area, which limits the model’s ability to fully utilize the original semantic information of both modalities. To address these limitations, we investigate how to achieve a direct fusion of cross-modal channels and spatial features in RGB-T tracking and propose CSTNet. It uses the Vision Transformer (ViT) as the backbone and adds a Joint Spatial and Channel Fusion Module (JSCFM) and Spatial Fusion Module (SFM) integrated between the transformer blocks to facilitate cross-modal feature interaction. The JSCFM module achieves joint modeling of channel and multi-level spatial features. The SFM module includes a cross-attention-like architecture for cross modeling and joint learning of RGB and TIR features. Comprehensive experiments show that CSTNet achieves state-of-the-art performance. To enhance practicality, we retrain the model without JSCFM and SFM modules and use CSNet as the pretraining weight, and propose CSTNet-small, which achieves 50% speedup with an average decrease of 1-2% in SR and PR performance. CSTNet and CSTNet-small achieve real-time speeds of 21 fps and 33 fps on the Nvidia Jetson Xavier, meeting actual deployment requirements. Code is available at <a target="_blank" rel="noopener" href="https://github.com/LiYunfengLYF/CSTNet">https://github.com/LiYunfengLYF/CSTNet</a>. </p>
<blockquote>
<p>RGB-T跟踪中的主要问题是正确且最优地融合可见光和热图像的跨模态特征。一些之前的方法要么没有充分利用RGB和TIR信息来进行通道和空间特征融合，要么缺乏模板和搜索区域之间的直接交互，这限制了模型充分利用两种模态的原始语义信息的能力。为了解决这些局限性，我们研究了如何在RGB-T跟踪中实现跨模态通道和空间特征的直接融合，并提出了CSTNet。它使用视觉转换器（ViT）作为主干，并添加了联合空间和通道融合模块（JSCFM）和空间融合模块（SFM），这些模块集成在转换器块之间，以促进跨模态特征交互。JSCFM模块实现了通道和多级空间特征的联合建模。SFM模块包括一种类似交叉注意力的架构，用于RGB和TIR特征的交叉建模和联合学习。综合实验表明，CSTNet达到了最先进的性能。为了提高实用性，我们重新训练了不带JSCFM和SFM模块的模型，并使用CSNet作为预训练权重，提出了CSTNet-small，它实现了50%的加速，在SR和PR性能上平均下降1-2%。CSTNet和CSTNet-small在Nvidia Jetson Xavier上实现实时速度分别为21帧和33帧，满足实际部署要求。代码可在<a target="_blank" rel="noopener" href="https://github.com/LiYunfengLYF/CSTNet%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/LiYunfengLYF/CSTNet找到。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2405.03177v3">PDF</a> This work has been submitted to the IEEE for possible publication</p>
<p><strong>摘要</strong></p>
<p>本文解决RGB-T跟踪中的主要难题——可见光和热成像图像的多模态特征正确和最优融合问题。针对现有方法未充分利用RGB和TIR信息的通道和空间特征融合潜力，以及模板和搜索区域之间缺乏直接交互的问题，本文研究了如何实现RGB-T跟踪中的跨模态通道和空间特征的直接融合，并提出了CSTNet。它以Vision Transformer（ViT）为骨干网，并在transformer块之间添加了联合空间和通道融合模块（JSCFM）和空间融合模块（SFM），以促进跨模态特征交互。JSCFM模块实现了通道和多级空间特征的联合建模。SFM模块采用类似交叉注意力的架构，实现RGB和TIR特征的跨建模和联合学习。实验表明，CSTNet达到了最先进的性能。为提高实用性，我们重新训练了不带JSCFM和SFM模块的模型，以CSNet作为预训练权重，并推出了CSTNet-small，其速度提高了50%，在SR和PR性能方面平均降低了1-2%。CSTNet和CSTNet-small在Nvidia Jetson Xavier上实现了实时速度分别为每秒21帧和33帧，满足实际部署要求。相关代码已发布在GitHub上。代码地址：<a target="_blank" rel="noopener" href="https://github.com/LiYunfengLYF/CSTNet">https://github.com/LiYunfengLYF/CSTNet</a>。</p>
<p><strong>要点</strong></p>
<ol>
<li>RGB-T跟踪中的主要挑战在于正确并最优地融合可见光和热成像图像的跨模态特征。</li>
<li>现有方法在某些方面存在局限，如未充分利用RGB和TIR信息，或在模板和搜索区之间缺乏直接交互。</li>
<li>CSTNet通过引入Vision Transformer（ViT）作为骨干网来解决这些问题，并添加了联合空间和通道融合模块（JSCFM）以及空间融合模块（SFM）。</li>
<li>JSCFM模块实现了通道和多级空间特征的联合建模，而SFM模块采用类似交叉注意力的架构进行特征跨建模和联合学习。</li>
<li>CSTNet在实验中表现出卓越性能，而CSTNet-small版本则更注重实用性，实现了速度的提升并保持了相对的性能。</li>
<li>CSTNet和CSTNet-small满足实时部署要求，并在Nvidia Jetson Xavier上实现了较高的帧率。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2405.03177">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-8b410f111ac5692ce67a75b7a448c331.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-c0da7a7faa54dd71d933ad50d29fa400.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a549d1295ad0f46785402e3a7cf373c0.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b0520084c44735f64e0ea48d3f57b8c8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-20f6ace5c0472ee6ce1c37429506e328.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        文章作者:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        文章链接:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-06-25/Vision%20Transformer/">https://kedreamix.github.io/Talk2Paper/Paper/2025-06-25/Vision%20Transformer/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        版权声明:
                    </i>
                </span>
                <span class="reprint-info">
                    本博客所有文章除特別声明外，均采用
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    许可协议。转载请注明来源
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Vision-Transformer/">
                                    <span class="chip bg-color">Vision Transformer</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;上一篇</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-06-25/%E6%A3%80%E6%B5%8B_%E5%88%86%E5%89%B2_%E8%B7%9F%E8%B8%AA/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-5fdbf250cf0512d5b6f10b8de0def0c5.jpg" class="responsive-img" alt="检测/分割/跟踪">
                        
                        <span class="card-title">检测/分割/跟踪</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            检测/分割/跟踪 方向最新论文已更新，请持续关注 Update in 2025-06-25  Pre-Trained LLM is a Semantic-Aware and Generalizable Segmentation   Booster
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-06-25
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E6%A3%80%E6%B5%8B-%E5%88%86%E5%89%B2-%E8%B7%9F%E8%B8%AA/" class="post-category">
                                    检测/分割/跟踪
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E6%A3%80%E6%B5%8B-%E5%88%86%E5%89%B2-%E8%B7%9F%E8%B8%AA/">
                        <span class="chip bg-color">检测/分割/跟踪</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                下一篇&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-06-25/%E8%A7%86%E9%A2%91%E7%90%86%E8%A7%A3/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-8d844f39af18f665381930b20ae9828b.jpg" class="responsive-img" alt="视频理解">
                        
                        <span class="card-title">视频理解</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            视频理解 方向最新论文已更新，请持续关注 Update in 2025-06-25  VQ-Insight Teaching VLMs for AI-Generated Video Quality Understanding   via Progressive Visual Reinforcement Learning
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-06-25
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E8%A7%86%E9%A2%91%E7%90%86%E8%A7%A3/" class="post-category">
                                    视频理解
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E8%A7%86%E9%A2%91%E7%90%86%E8%A7%A3/">
                        <span class="chip bg-color">视频理解</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- 代码块功能依赖 -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- 代码语言 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- 代码块复制 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- 代码块收缩 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC 悬浮按钮. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* 修复文章卡片 div 的宽度. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // 切换TOC目录展开收缩的相关操作.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;站点总字数:&nbsp;<span
                        class="white-color">23901.5k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- 运行天数提醒. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // 区分是否有年份.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = '本站已运行 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                daysTip = '本站已運行 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = '本站已运行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = '本站已運行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="关注我的知乎: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">知</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- 搜索遮罩框 -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span>
            <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- 白天和黑夜主题 -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- 回到顶部按钮 -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // 只在桌面版网页启用特效
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- 雪花特效 -->
    

    <!-- 鼠标星星特效 -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--腾讯兔小巢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- 动态标签 -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Σ(っ °Д °;)っ诶，页面崩溃了嘛？", clearTimeout(st)) : (document.title = "φ(゜▽゜*)♪咦，又好了！", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
