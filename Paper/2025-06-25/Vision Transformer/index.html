<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Vision Transformer">
    <meta name="description" content="Vision Transformer æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-06-25  Deep CNN Face Matchers Inherently Support Revocable Biometric Templates">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Vision Transformer | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-39cb9836b423c7bcf242fca3e66c8c6a.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Vision Transformer</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Vision-Transformer/">
                                <span class="chip bg-color">Vision Transformer</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Vision-Transformer/" class="post-category">
                                Vision Transformer
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-06-25
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-07-06
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    8.2k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    33 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-06-25-æ›´æ–°"><a href="#2025-06-25-æ›´æ–°" class="headerlink" title="2025-06-25 æ›´æ–°"></a>2025-06-25 æ›´æ–°</h1><h2 id="Deep-CNN-Face-Matchers-Inherently-Support-Revocable-Biometric-Templates"><a href="#Deep-CNN-Face-Matchers-Inherently-Support-Revocable-Biometric-Templates" class="headerlink" title="Deep CNN Face Matchers Inherently Support Revocable Biometric Templates"></a>Deep CNN Face Matchers Inherently Support Revocable Biometric Templates</h2><p><strong>Authors:Aman Bhatta, Michael C. King, Kevin W. Bowyer</strong></p>
<p>One common critique of biometric authentication is that if an individualâ€™s biometric is compromised, then the individual has no recourse. The concept of revocable biometrics was developed to address this concern. A biometric scheme is revocable if an individual can have their current enrollment in the scheme revoked, so that the compromised biometric template becomes worthless, and the individual can re-enroll with a new template that has similar recognition power. We show that modern deep CNN face matchers inherently allow for a robust revocable biometric scheme. For a given state-of-the-art deep CNN backbone and training set, it is possible to generate an unlimited number of distinct face matcher models that have both (1) equivalent recognition power, and (2) strongly incompatible biometric templates. The equivalent recognition power extends to the point of generating impostor and genuine distributions that have the same shape and placement on the similarity dimension, meaning that the models can share a similarity threshold for a 1-in-10,000 false match rate. The biometric templates from different model instances are so strongly incompatible that the cross-instance similarity score for images of the same person is typically lower than the same-instance similarity score for images of different persons. That is, a stolen biometric template that is revoked is of less value in attempting to match the re-enrolled identity than the average impostor template. We also explore the feasibility of using a Vision Transformer (ViT) backbone-based face matcher in the revocable biometric system proposed in this work and demonstrate that it is less suitable compared to typical ResNet-based deep CNN backbones. </p>
<blockquote>
<p>å…³äºç”Ÿç‰©è®¤è¯çš„å¸¸è§æ‰¹è¯„ä¹‹ä¸€æ˜¯ï¼Œå¦‚æœä¸ªäººçš„ç”Ÿç‰©è¯†åˆ«ä¿¡æ¯è¢«æ³„éœ²ï¼Œé‚£ä¹ˆä¸ªäººä¾¿æ— è®¡å¯æ–½ã€‚ä¸ºè§£å†³è¿™ä¸€æ‹…å¿§ï¼Œå¼€å‘äº†å¯æ’¤é”€ç”Ÿç‰©è¯†åˆ«æŠ€æœ¯è¿™ä¸€æ¦‚å¿µã€‚å¦‚æœä¸€ä¸ªç”Ÿç‰©è¯†åˆ«æ–¹æ¡ˆæ˜¯å¯æ’¤é”€çš„ï¼Œé‚£ä¹ˆä¸ªäººå°±å¯ä»¥æ’¤é”€å…¶åœ¨è¯¥æ–¹æ¡ˆä¸­çš„å½“å‰æ³¨å†Œï¼Œè¿™æ ·æ³„éœ²çš„ç”Ÿç‰©è¯†åˆ«æ¨¡æ¿å°±ä¼šå˜å¾—æ¯«æ— ä»·å€¼ï¼Œä¸ªäººå¯ä»¥é‡æ–°æ³¨å†Œä¸€ä¸ªæ–°çš„æ¨¡æ¿ï¼Œè¿™ä¸ªæ–°æ¨¡æ¿å…·æœ‰ç±»ä¼¼çš„è¯†åˆ«èƒ½åŠ›ã€‚æˆ‘ä»¬å±•ç¤ºäº†ç°ä»£æ·±åº¦å·ç§¯ç¥ç»ç½‘ç»œé¢éƒ¨åŒ¹é…å™¨æœ¬è´¨ä¸Šå…è®¸ç¨³å¥çš„å¯æ’¤é”€ç”Ÿç‰©è¯†åˆ«æ–¹æ¡ˆã€‚å¯¹äºç»™å®šçš„å…ˆè¿›æ·±åº¦å·ç§¯ç¥ç»ç½‘ç»œä¸»å¹²å’Œè®­ç»ƒé›†ï¼Œå¯ä»¥ç”Ÿæˆæ— é™æ•°é‡çš„ç‹¬ç‰¹é¢éƒ¨åŒ¹é…å™¨æ¨¡å‹ï¼Œè¿™äº›æ¨¡å‹æ—¢å…·æœ‰ï¼ˆ1ï¼‰ç›¸å½“çš„è¯†åˆ«èƒ½åŠ›ï¼Œåˆæ‹¥æœ‰ï¼ˆ2ï¼‰é«˜åº¦ä¸å…¼å®¹çš„ç”Ÿç‰©è¯†åˆ«æ¨¡æ¿ã€‚ç›¸å½“çš„è¯†åˆ«èƒ½åŠ›æ‰©å±•åˆ°äº†ç”Ÿæˆå‡å†’è€…å’ŒçœŸå®è€…çš„åˆ†å¸ƒï¼Œè¿™äº›åˆ†å¸ƒåœ¨ç›¸ä¼¼åº¦ç»´åº¦ä¸Šå…·æœ‰ç›¸åŒçš„å½¢çŠ¶å’Œä½ç½®ï¼Œè¿™æ„å‘³ç€è¿™äº›æ¨¡å‹å¯ä»¥å…±äº«ä¸€ä¸ªç›¸ä¼¼åº¦é˜ˆå€¼ï¼Œä»¥è¾¾åˆ°ä¸‡åˆ†ä¹‹ä¸€çš„è¯¯åŒ¹é…ç‡ã€‚ä¸åŒæ¨¡å‹å®ä¾‹çš„ç”Ÿç‰©è¯†åˆ«æ¨¡æ¿ä¹‹é—´é«˜åº¦ä¸å…¼å®¹ï¼Œä»¥è‡³äºåŒä¸€äººçš„ä¸åŒå›¾åƒåœ¨ä¸åŒå®ä¾‹ä¹‹é—´çš„ç›¸ä¼¼åº¦åˆ†æ•°é€šå¸¸ä½äºä¸åŒäººçš„ä¸åŒå›¾åƒçš„ç›¸ä¼¼åº¦åˆ†æ•°ã€‚ä¹Ÿå°±æ˜¯è¯´ï¼Œè¢«æ’¤é”€çš„è¢«ç›—ç”Ÿç‰©è¯†åˆ«æ¨¡æ¿åœ¨å°è¯•åŒ¹é…é‡æ–°æ³¨å†Œçš„èº«ä»½æ—¶ï¼Œå…¶ä»·å€¼é€šå¸¸ä½äºå‡å†’è€…æ¨¡æ¿çš„å¹³å‡å€¼ã€‚æˆ‘ä»¬è¿˜æ¢è®¨äº†åœ¨æœ¬å·¥ä½œä¸­æå‡ºçš„å¯æ’¤é”€ç”Ÿç‰©è¯†åˆ«ç³»ç»Ÿä¸­ä½¿ç”¨åŸºäºè§†è§‰è½¬æ¢å™¨ï¼ˆViTï¼‰çš„é¢éƒ¨åŒ¹é…å™¨çš„å¯è¡Œæ€§ï¼Œå¹¶è¯æ˜ä¸å…¸å‹çš„åŸºäºResNetçš„æ·±åº¦å·ç§¯ç¥ç»ç½‘ç»œä¸»å¹²ç›¸æ¯”ï¼Œå®ƒä¸å¤ªé€‚åˆã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.18731v1">PDF</a> </p>
<p><strong>æ‘˜è¦</strong></p>
<p>å…³äºç”Ÿç‰©è¯†åˆ«è®¤è¯çš„å¸¸è§æ‰¹è¯„ä¹‹ä¸€æ˜¯ï¼Œä¸€æ—¦ä¸ªä½“çš„ç”Ÿç‰©è¯†åˆ«ä¿¡æ¯è¢«æ³„éœ²ï¼Œä¸ªä½“å°†æŸæ‰‹æ— ç­–ã€‚ä¸ºè§£å†³è¿™ä¸€æ‹…å¿§ï¼Œå‘å±•äº†å¯æ’¤é”€ç”Ÿç‰©è¯†åˆ«æŠ€æœ¯ã€‚å½“ä¸ªä½“èƒ½æ’¤é”€å…¶åœ¨ç”Ÿç‰©è¯†åˆ«æ–¹æ¡ˆä¸­çš„å½“å‰æ³¨å†Œï¼Œä½¿å¾—å·²æ³„éœ²çš„ç”Ÿç‰©è¯†åˆ«æ¨¡æ¿å¤±æ•ˆå¹¶é‡æ–°æ³¨å†Œæ–°æ¨¡æ¿æ—¶ï¼Œè¯¥ç”Ÿç‰©è¯†åˆ«æ–¹æ¡ˆè¢«è®¤ä¸ºæ˜¯å¯æ’¤é”€çš„ã€‚æ–°æ¨¡æ¿åº”å…·æœ‰ç›¸ä¼¼çš„è¯†åˆ«èƒ½åŠ›ã€‚æœ¬æ–‡å±•ç¤ºäº†ç°ä»£æ·±åº¦CNNé¢éƒ¨è¯†åˆ«å™¨å¤©ç„¶æ”¯æŒå¼ºå¤§çš„å¯æ’¤é”€ç”Ÿç‰©è¯†åˆ«æ–¹æ¡ˆã€‚å¯¹äºç»™å®šçš„å…ˆè¿›æ·±åº¦CNNä¸»å¹²å’Œè®­ç»ƒé›†ï¼Œå¯ä»¥ç”Ÿæˆæ— é™æ•°é‡çš„é¢éƒ¨è¯†åˆ«å™¨æ¨¡å‹å®ä¾‹ï¼Œè¿™äº›æ¨¡å‹å®ä¾‹ä¸ä»…å…·æœ‰ç­‰åŒçš„è¯†åˆ«èƒ½åŠ›ï¼Œè€Œä¸”å…¶ç”Ÿç‰©è¯†åˆ«æ¨¡æ¿ä¹‹é—´å…·æœ‰å¾ˆå¼ºçš„ä¸å…¼å®¹æ€§å’Œå·®å¼‚æ€§ã€‚è¿™æ„å‘³ç€ï¼Œå³ä½¿æŸä¸ªäººçš„ç”Ÿç‰©è¯†åˆ«æ¨¡æ¿è¢«ç›—ç”¨å¹¶æ’¤é”€åï¼Œå…¶é‡æ–°æ³¨å†Œçš„æ–°æ¨¡æ¿çš„åŒ¹é…ä»·å€¼ä½äºéšæœºå‡å†’è€…çš„åŒ¹é…ä»·å€¼ã€‚æ­¤å¤–ï¼Œæœ¬æ–‡è¿˜æ¢è®¨äº†å°†Vision Transformerï¼ˆViTï¼‰ä¸»å¹²åº”ç”¨äºæœ¬æ–‡æå‡ºçš„å¯æ’¤é”€ç”Ÿç‰©è¯†åˆ«ç³»ç»Ÿçš„å¯è¡Œæ€§ï¼Œå¹¶è¯æ˜ç›¸è¾ƒäºå…¸å‹çš„åŸºäºResNetçš„æ·±åº¦CNNä¸»å¹²ï¼ŒViTçš„åº”ç”¨è¡¨ç°ä¸å¤ªç†æƒ³ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>å¯æ’¤é”€ç”Ÿç‰©è¯†åˆ«æŠ€æœ¯è§£å†³äº†ç”Ÿç‰©è¯†åˆ«ä¿¡æ¯æ³„éœ²åæ— è§£å†³æ–¹æ¡ˆçš„é—®é¢˜ã€‚</li>
<li>ç°ä»£æ·±åº¦CNNé¢éƒ¨è¯†åˆ«å™¨å…è®¸ç”Ÿæˆå…·æœ‰ç­‰æ•ˆè¯†åˆ«èƒ½åŠ›çš„å¤šç§æ¨¡å‹å®ä¾‹ã€‚</li>
<li>è¿™äº›æ¨¡å‹å®ä¾‹çš„è¯†åˆ«æ¨¡æ¿å…·æœ‰å¼ºçƒˆçš„å…¼å®¹æ€§å’Œå·®å¼‚æ€§ã€‚è¿™æ„å‘³ç€æ’¤é”€çš„ç”Ÿç‰©è¯†åˆ«æ¨¡æ¿åŒ¹é…ä»·å€¼è¾ƒä½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.18731">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-69de9e39b449ad62df6443a99ee75000.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-39cb9836b423c7bcf242fca3e66c8c6a.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-8c3f943d592b397456fd2cbd75e431af.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-1fe18f947502b7e457f27b5c72a17ff2.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Taming-Vision-Language-Models-for-Medical-Image-Analysis-A-Comprehensive-Review"><a href="#Taming-Vision-Language-Models-for-Medical-Image-Analysis-A-Comprehensive-Review" class="headerlink" title="Taming Vision-Language Models for Medical Image Analysis: A   Comprehensive Review"></a>Taming Vision-Language Models for Medical Image Analysis: A   Comprehensive Review</h2><p><strong>Authors:Haoneng Lin, Cheng Xu, Jing Qin</strong></p>
<p>Modern Vision-Language Models (VLMs) exhibit unprecedented capabilities in cross-modal semantic understanding between visual and textual modalities. Given the intrinsic need for multi-modal integration in clinical applications, VLMs have emerged as a promising solution for a wide range of medical image analysis tasks. However, adapting general-purpose VLMs to medical domain poses numerous challenges, such as large domain gaps, complicated pathological variations, and diversity and uniqueness of different tasks. The central purpose of this review is to systematically summarize recent advances in adapting VLMs for medical image analysis, analyzing current challenges, and recommending promising yet urgent directions for further investigations. We begin by introducing core learning strategies for medical VLMs, including pretraining, fine-tuning, and prompt learning. We then categorize five major VLM adaptation strategies for medical image analysis. These strategies are further analyzed across eleven medical imaging tasks to illustrate their current practical implementations. Furthermore, we analyze key challenges that impede the effective adaptation of VLMs to clinical applications and discuss potential directions for future research. We also provide an open-access repository of related literature to facilitate further research, available at <a target="_blank" rel="noopener" href="https://github.com/haonenglin/Awesome-VLM-for-MIA">https://github.com/haonenglin/Awesome-VLM-for-MIA</a>. It is anticipated that this article can help researchers who are interested in harnessing VLMs in medical image analysis tasks have a better understanding on their capabilities and limitations, as well as current technical barriers, to promote their innovative, robust, and safe application in clinical practice. </p>
<blockquote>
<p>ç°ä»£è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰åœ¨è§†è§‰å’Œæ–‡æœ¬æ¨¡æ€ä¹‹é—´çš„è·¨æ¨¡æ€è¯­ä¹‰ç†è§£æ–¹é¢å±•ç°å‡ºäº†å‰æ‰€æœªæœ‰çš„èƒ½åŠ›ã€‚é‰´äºä¸´åºŠåº”ç”¨ä¸­å¯¹å¤šæ¨¡æ€é›†æˆçš„å†…åœ¨éœ€æ±‚ï¼ŒVLMså·²æˆä¸ºå¹¿æ³›åº”ç”¨äºåŒ»ç–—å›¾åƒåˆ†æä»»åŠ¡çš„æœ‰å‰é€”çš„è§£å†³æ–¹æ¡ˆã€‚ç„¶è€Œï¼Œå°†é€šç”¨VLMsé€‚åº”äºåŒ»å­¦é¢†åŸŸå¸¦æ¥äº†è®¸å¤šæŒ‘æˆ˜ï¼Œä¾‹å¦‚é¢†åŸŸå·®è·å¤§ã€ç—…ç†å˜åŒ–å¤æ‚ä»¥åŠä¸åŒä»»åŠ¡çš„å¤šæ ·æ€§å’Œç‹¬ç‰¹æ€§ã€‚æœ¬æ–‡çš„ä¸­å¿ƒç›®çš„æ˜¯ç³»ç»Ÿåœ°æ€»ç»“è¿‘å¹´æ¥å°†VLMsé€‚åº”äºåŒ»ç–—å›¾åƒåˆ†æçš„æœ€æ–°è¿›å±•ï¼Œåˆ†æå½“å‰æŒ‘æˆ˜ï¼Œå¹¶ä¸ºè¿›ä¸€æ­¥çš„è°ƒæŸ¥æ¨èæœ‰å‰é€”ä¸”ç´§è¿«çš„æ–¹å‘ã€‚æˆ‘ä»¬é¦–å…ˆä»‹ç»åŒ»å­¦VLMsçš„æ ¸å¿ƒå­¦ä¹ ç­–ç•¥ï¼ŒåŒ…æ‹¬é¢„è®­ç»ƒã€å¾®è°ƒå’Œæç¤ºå­¦ä¹ ã€‚ç„¶åæˆ‘ä»¬å°†äº”å¤§VLMé€‚åº”ç­–ç•¥åˆ†ç±»ä¸ºåŒ»ç–—å›¾åƒåˆ†æã€‚è¿™äº›ç­–ç•¥è¿›ä¸€æ­¥åœ¨åä¸€ä¸ªåŒ»ç–—æˆåƒä»»åŠ¡ä¸­è¿›è¡Œåˆ†æï¼Œä»¥è¯´æ˜å®ƒä»¬å½“å‰çš„å®ç”¨å®ç°ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬åˆ†æäº†é˜»ç¢VLMsæœ‰æ•ˆé€‚åº”ä¸´åºŠåº”ç”¨çš„å…³é”®æŒ‘æˆ˜ï¼Œå¹¶è®¨è®ºäº†æœªæ¥ç ”ç©¶çš„æ–¹å‘ã€‚æˆ‘ä»¬è¿˜æä¾›äº†ä¸€ä¸ªå¼€æ”¾è®¿é—®çš„ç›¸å…³æ–‡çŒ®ä»“åº“ï¼Œä»¥ä¿ƒè¿›è¿›ä¸€æ­¥ç ”ç©¶ï¼Œå¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/haonenglin/Awesome-VLM-for-MIA%E8%8E%B7%E5%8F%96%E3%80%82%E9%A2%84%E8%AE%A1%E6%9C%AC%E6%96%87%E8%83%BD%E5%B8%AE%E5%8A%A9%E5%AF%B9%E5%9C%A8%E5%8C%BB%E7%96%97%E5%9B%BE%E5%83%8F%E5%88%86%E6%9E%90%E4%BB%BB%E5%8A%A1%E4%B8%AD%E4%BD%BF%E7%94%A8VLMs%E6%84%9F%E5%85%B4%E8%B6%A3%E7%9A%84%E7%A0%94%E7%A9%B6%E4%BA%BA%E5%91%98%E6%9B%B4%E5%A5%BD%E5%9C%B0%E4%BA%86%E8%A7%A3%E5%AE%83%E4%BB%AC%E7%9A%84%E8%83%BD%E5%8A%9B%E3%80%81%E5%B1%80%E9%99%90%E6%80%A7%E4%BB%A5%E5%8F%8A%E5%BD%93%E5%89%8D%E7%9A%84%E6%8A%80%E6%9C%AF%E9%9A%9C%E7%A2%8D%EF%BC%8C%E4%BB%A5%E4%BF%83%E8%BF%9B%E5%85%B6%E5%9C%A8%E4%B8%B4%E5%BA%8A%E5%AE%9E%E8%B7%B5%E4%B8%AD%E7%9A%84%E5%88%9B%E6%96%B0%E3%80%81%E7%A8%B3%E5%81%A5%E5%92%8C%E5%AE%89%E5%85%A8%E5%BA%94%E7%94%A8%E3%80%82">https://github.com/haonenglin/Awesome-VLM-for-MIAè·å–ã€‚é¢„è®¡æœ¬æ–‡èƒ½å¸®åŠ©å¯¹åœ¨åŒ»ç–—å›¾åƒåˆ†æä»»åŠ¡ä¸­ä½¿ç”¨VLMsæ„Ÿå…´è¶£çš„ç ”ç©¶äººå‘˜æ›´å¥½åœ°äº†è§£å®ƒä»¬çš„èƒ½åŠ›ã€å±€é™æ€§ä»¥åŠå½“å‰çš„æŠ€æœ¯éšœç¢ï¼Œä»¥ä¿ƒè¿›å…¶åœ¨ä¸´åºŠå®è·µä¸­çš„åˆ›æ–°ã€ç¨³å¥å’Œå®‰å…¨åº”ç”¨ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.18378v1">PDF</a> 34 pages</p>
<p><strong>Summary</strong><br>è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰åœ¨è·¨æ¨¡æ€è¯­ä¹‰ç†è§£æ–¹é¢å±•ç°å‡ºå‰æ‰€æœªæœ‰çš„èƒ½åŠ›ï¼Œå¯¹äºåŒ»å­¦å½±åƒåˆ†æä»»åŠ¡å…·æœ‰å¹¿æ³›åº”ç”¨å‰æ™¯ã€‚ç„¶è€Œï¼Œå°†å…¶åº”ç”¨äºåŒ»å­¦é¢†åŸŸé¢ä¸´è¯¸å¤šæŒ‘æˆ˜ã€‚æœ¬æ–‡ç»¼è¿°äº†è¿‘æœŸVLMsåœ¨åŒ»å­¦å½±åƒåˆ†æä¸­çš„è¿›å±•ã€äº”å¤§é€‚åº”æ€§ç­–ç•¥åŠå…¶åœ¨åä¸€ç§åŒ»å­¦å½±åƒä»»åŠ¡ä¸­çš„åº”ç”¨å®ä¾‹ã€‚åŒæ—¶ï¼Œæœ¬æ–‡åˆ†æäº†å…³é”®æŒ‘æˆ˜å¹¶æ¢è®¨äº†æœªæ¥ç ”ç©¶æ–¹å‘ã€‚æä¾›çš„ç›¸å…³æ–‡çŒ®åº“æœ‰åŠ©äºç ”ç©¶äººå‘˜äº†è§£VLMsåœ¨åŒ»å­¦å½±åƒåˆ†æä¸­çš„èƒ½åŠ›ä¸å±€é™ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>VLMså±•ç°å‡ºè·¨æ¨¡æ€è¯­ä¹‰ç†è§£çš„å¼ºå¤§èƒ½åŠ›ï¼Œå°¤å…¶åœ¨åŒ»å­¦å½±åƒåˆ†æé¢†åŸŸæœ‰å¹¿æ³›åº”ç”¨å‰æ™¯ã€‚</li>
<li>VLMsåº”ç”¨äºåŒ»å­¦é¢†åŸŸé¢ä¸´å¤§é¢†åŸŸå·®è·ã€å¤æ‚çš„ç—…ç†å˜åŒ–å’Œä»»åŠ¡å¤šæ ·æ€§ç­‰æŒ‘æˆ˜ã€‚</li>
<li>æœ¬æ–‡ç»¼è¿°äº†äº”å¤§VLMé€‚åº”æ€§ç­–ç•¥ï¼ŒåŒ…æ‹¬æ ¸å¿ƒå­¦ä¹ ç­–ç•¥å¦‚é¢„è®­ç»ƒã€å¾®è°ƒã€æç¤ºå­¦ä¹ ç­‰ã€‚</li>
<li>åœ¨åä¸€ç§åŒ»å­¦å½±åƒä»»åŠ¡ä¸­ï¼Œè¯¦ç»†é˜è¿°äº†è¿™äº›ç­–ç•¥çš„å®é™…åº”ç”¨å®ä¾‹ã€‚</li>
<li>åˆ†æäº†é˜»ç¢VLMsæœ‰æ•ˆé€‚åº”ä¸´åºŠåº”ç”¨çš„æŒ‘æˆ˜ï¼Œå¹¶è®¨è®ºäº†æœªæ¥ç ”ç©¶æ–¹å‘ã€‚</li>
<li>æ–‡ç« æä¾›äº†ä¸€ä¸ªå¼€æ”¾è®¿é—®çš„ç›¸å…³æ–‡çŒ®åº“ï¼Œä»¥ä¿ƒè¿›è¿›ä¸€æ­¥ç ”ç©¶ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.18378">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-a0376da397e792bb832c73aac3361f73.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-264b884208ed5bdd521bb7d3d447a5a2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-36053664037a7a8e432d0772cd789522.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="HIRE-Lightweight-High-Resolution-Image-Feature-Enrichment-for-Multimodal-LLMs"><a href="#HIRE-Lightweight-High-Resolution-Image-Feature-Enrichment-for-Multimodal-LLMs" class="headerlink" title="HIRE: Lightweight High-Resolution Image Feature Enrichment for   Multimodal LLMs"></a>HIRE: Lightweight High-Resolution Image Feature Enrichment for   Multimodal LLMs</h2><p><strong>Authors:Nikitha SR, Aradhya Neeraj Mathur, Tarun Ram Menta, Rishabh Jain, Mausoom Sarkar</strong></p>
<p>The integration of high-resolution image features in modern multimodal large language models has demonstrated significant improvements in fine-grained visual understanding tasks, achieving high performance across multiple benchmarks. Since these features are obtained from large image encoders like ViT, they come with a significant increase in computational costs due to multiple calls to these encoders. In this work, we first develop an intuition for feature upsampling as a natural extension of high-resolution feature generation. Through extensive experiments and ablations, we demonstrate how a shallow feature enricher can achieve competitive results with tremendous reductions in training and inference time as well as computational cost, with upto 1.5x saving in FLOPs. </p>
<blockquote>
<p>å°†é«˜åˆ†è¾¨ç‡å›¾åƒç‰¹å¾èå…¥ç°ä»£å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼Œå·²ç»åœ¨ç»†ç²’åº¦è§†è§‰ç†è§£ä»»åŠ¡ä¸­æ˜¾ç¤ºå‡ºé‡å¤§æ”¹è¿›ï¼Œå¹¶åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­å®ç°é«˜æ€§èƒ½ã€‚ç”±äºè¿™äº›ç‰¹å¾æ¥è‡ªå¤§å‹å›¾åƒç¼–ç å™¨ï¼ˆå¦‚ViTï¼‰ï¼Œå› æ­¤å®ƒä»¬éœ€è¦é€šè¿‡å¤šæ¬¡è°ƒç”¨è¿™äº›ç¼–ç å™¨è€Œè·å¾—ï¼Œä»è€Œå¸¦æ¥è®¡ç®—æˆæœ¬çš„æ˜¾è‘—å¢åŠ ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬é¦–å…ˆå‘å±•äº†ä¸€ç§ç‰¹å¾ä¸Šé‡‡æ ·ä½œä¸ºé«˜åˆ†è¾¨ç‡ç‰¹å¾ç”Ÿæˆçš„è‡ªç„¶æ‰©å±•çš„ç›´è§‰ã€‚é€šè¿‡å¹¿æ³›çš„å®éªŒå’Œæ¶ˆèå®éªŒï¼Œæˆ‘ä»¬å±•ç¤ºäº†æµ…å±‚ç‰¹å¾ä¸°å¯Œå™¨å¦‚ä½•åœ¨è®­ç»ƒå’Œæ¨ç†æ—¶é—´ä»¥åŠè®¡ç®—æˆæœ¬æ–¹é¢å®ç°å·¨å¤§å‡å°‘çš„åŒæ—¶ï¼Œå–å¾—å…·æœ‰ç«äº‰åŠ›çš„ç»“æœï¼Œå¹¶ä¸”æµ®ç‚¹è¿ç®—é‡æœ€å¤šå¯å‡å°‘1.5å€ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.17608v1">PDF</a> Accepted in CVPR 2025 Workshop on Whatâ€™s Next in Multimodal   Foundational Models</p>
<p><strong>Summary</strong></p>
<p>ç°ä»£å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ä¸­é›†æˆäº†é«˜åˆ†è¾¨ç‡å›¾åƒç‰¹å¾ï¼Œè¿™åœ¨ç²¾ç»†ç²’åº¦çš„è§†è§‰ç†è§£ä»»åŠ¡ä¸­å–å¾—äº†æ˜¾è‘—çš„æå‡ï¼Œå¹¶åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­å®ç°äº†é«˜æ€§èƒ½ã€‚ç„¶è€Œï¼Œç”±äºéœ€è¦ä»å¤§å‹å›¾åƒç¼–ç å™¨ï¼ˆå¦‚ViTï¼‰è·å–è¿™äº›ç‰¹å¾ï¼Œè®¡ç®—æˆæœ¬æ˜¾è‘—å¢åŠ ã€‚æœ¬æ–‡é¦–æ¬¡æå‡ºç‰¹å¾ä¸Šé‡‡æ ·ä½œä¸ºé«˜åˆ†è¾¨ç‡ç‰¹å¾ç”Ÿæˆçš„è‡ªç„¶æ‰©å±•ã€‚é€šè¿‡å¹¿æ³›çš„å®éªŒå’Œæ¶ˆèç ”ç©¶ï¼Œæˆ‘ä»¬å±•ç¤ºäº†æµ…å±‚ç‰¹å¾ä¸°å¯Œå™¨å¦‚ä½•åœ¨å‡å°‘è®­ç»ƒå’Œæ¨ç†æ—¶é—´ä»¥åŠè®¡ç®—æˆæœ¬çš„åŒæ—¶å®ç°å…·æœ‰ç«äº‰åŠ›çš„ç»“æœï¼Œæœ€å¤šå¯å‡å°‘1.5å€çš„FLOPsã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>é«˜åˆ†è¾¨ç‡å›¾åƒç‰¹å¾åœ¨ç°ä»£å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ä¸­çš„é›†æˆæ˜¾è‘—æé«˜äº†ç²¾ç»†ç²’åº¦çš„è§†è§‰ç†è§£æ€§èƒ½ã€‚</li>
<li>è¿™äº›ç‰¹å¾æ¥è‡ªå¤§å‹å›¾åƒç¼–ç å™¨ï¼Œå¦‚ViTï¼Œå¯¼è‡´è®¡ç®—æˆæœ¬å¢åŠ ã€‚</li>
<li>ç‰¹å¾ä¸Šé‡‡æ ·ä½œä¸ºé«˜åˆ†è¾¨ç‡ç‰¹å¾ç”Ÿæˆçš„æ‰©å±•è¢«æå‡ºã€‚</li>
<li>é€šè¿‡å®éªŒå’Œæ¶ˆèç ”ç©¶ï¼Œè¯æ˜äº†æµ…å±‚ç‰¹å¾ä¸°å¯Œå™¨åœ¨å‡å°‘è®­ç»ƒå’Œæ¨ç†æ—¶é—´ä»¥åŠè®¡ç®—æˆæœ¬æ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚</li>
<li>æµ…å±‚ç‰¹å¾ä¸°å¯Œå™¨å®ç°äº†å…·æœ‰ç«äº‰åŠ›çš„ç»“æœï¼Œæœ€å¤šå¯å‡å°‘1.5å€çš„FLOPsã€‚</li>
<li>æ­¤æ–¹æ³•ä¸ºæé«˜è§†è§‰ä»»åŠ¡çš„æ€§èƒ½æä¾›äº†ä¸€ç§æ–°çš„æ€è·¯ï¼Œå¹¶å¯èƒ½åœ¨æœªæ¥çš„ç ”ç©¶ä¸­å¾—åˆ°è¿›ä¸€æ­¥çš„åº”ç”¨å’Œå‘å±•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.17608">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-3b456ef7d1832d79df20503d362ff775.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c2f7724543f06be5e049f3f8b6926ea2.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-4bde5d6a2fd901f071df540055869d8b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e20be98369d6f000e63fe27f27d0267a.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="AQUA20-A-Benchmark-Dataset-for-Underwater-Species-Classification-under-Challenging-Conditions"><a href="#AQUA20-A-Benchmark-Dataset-for-Underwater-Species-Classification-under-Challenging-Conditions" class="headerlink" title="AQUA20: A Benchmark Dataset for Underwater Species Classification under   Challenging Conditions"></a>AQUA20: A Benchmark Dataset for Underwater Species Classification under   Challenging Conditions</h2><p><strong>Authors:Taufikur Rahman Fuad, Sabbir Ahmed, Shahriar Ivan</strong></p>
<p>Robust visual recognition in underwater environments remains a significant challenge due to complex distortions such as turbidity, low illumination, and occlusion, which severely degrade the performance of standard vision systems. This paper introduces AQUA20, a comprehensive benchmark dataset comprising 8,171 underwater images across 20 marine species reflecting real-world environmental challenges such as illumination, turbidity, occlusions, etc., providing a valuable resource for underwater visual understanding. Thirteen state-of-the-art deep learning models, including lightweight CNNs (SqueezeNet, MobileNetV2) and transformer-based architectures (ViT, ConvNeXt), were evaluated to benchmark their performance in classifying marine species under challenging conditions. Our experimental results show ConvNeXt achieving the best performance, with a Top-3 accuracy of 98.82% and a Top-1 accuracy of 90.69%, as well as the highest overall F1-score of 88.92% with moderately large parameter size. The results obtained from our other benchmark models also demonstrate trade-offs between complexity and performance. We also provide an extensive explainability analysis using GRAD-CAM and LIME for interpreting the strengths and pitfalls of the models. Our results reveal substantial room for improvement in underwater species recognition and demonstrate the value of AQUA20 as a foundation for future research in this domain. The dataset is publicly available at: <a target="_blank" rel="noopener" href="https://huggingface.co/datasets/taufiktrf/AQUA20">https://huggingface.co/datasets/taufiktrf/AQUA20</a>. </p>
<blockquote>
<p>åœ¨æ°´ä¸‹ç¯å¢ƒä¸­å®ç°ç¨³å¥çš„è§†è§‰è¯†åˆ«ä»ç„¶æ˜¯ä¸€ä¸ªé‡å¤§æŒ‘æˆ˜ï¼Œå› ä¸ºè¯¸å¦‚æµ‘æµŠã€ä½å…‰ç…§å’Œé®æŒ¡ä¹‹ç±»çš„å¤æ‚å¤±çœŸä¼šä¸¥é‡é™ä½æ ‡å‡†è§†è§‰ç³»ç»Ÿçš„æ€§èƒ½ã€‚æœ¬æ–‡ä»‹ç»äº†AQUA20ï¼Œè¿™æ˜¯ä¸€ä¸ªåŒ…å«8171å¼ æ°´ä¸‹å›¾åƒçš„ç»¼åˆåŸºå‡†æ•°æ®é›†ï¼Œæ¶µç›–äº†20ç§æµ·æ´‹ç‰©ç§ï¼Œåæ˜ äº†ç°å®ä¸–ç•Œä¸­çš„ç¯å¢ƒæŒ‘æˆ˜ï¼Œå¦‚å…‰ç…§ã€æµ‘æµŠåº¦ã€é®æŒ¡ç­‰ï¼Œä¸ºæ°´ä¸‹è§†è§‰ç†è§£æä¾›äº†å®è´µçš„èµ„æºã€‚æˆ‘ä»¬è¯„ä¼°äº†13ç§æœ€å…ˆè¿›çš„æ·±åº¦å­¦ä¹ æ¨¡å‹ï¼ŒåŒ…æ‹¬è½»é‡çº§CNNï¼ˆSqueezeNetã€MobileNetV2ï¼‰å’ŒåŸºäºtransformerçš„æ¶æ„ï¼ˆViTã€ConvNeXtï¼‰ï¼Œä»¥åŸºå‡†æµ‹è¯•å®ƒä»¬åœ¨å…·æœ‰æŒ‘æˆ˜æ€§çš„æ¡ä»¶ä¸‹å¯¹æµ·æ´‹ç‰©ç§è¿›è¡Œåˆ†ç±»çš„æ€§èƒ½ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒConvNeXtè¡¨ç°æœ€ä½³ï¼Œå‰ä¸‰åå‡†ç¡®ç‡è¾¾åˆ°äº†98.82%ï¼Œç¬¬ä¸€åå‡†ç¡®ç‡ä¸º90.69%ï¼Œæ€»ä½“F1åˆ†æ•°æœ€é«˜ï¼Œè¾¾åˆ°äº†88.92%ï¼Œä¸”å‚æ•°è§„æ¨¡é€‚ä¸­ã€‚å…¶ä»–åŸºå‡†æ¨¡å‹çš„ç»“æœä¹Ÿæ˜¾ç¤ºäº†å¤æ‚æ€§å’Œæ€§èƒ½ä¹‹é—´çš„æƒè¡¡ã€‚æˆ‘ä»¬è¿˜ä½¿ç”¨GRAD-CAMå’ŒLIMEè¿›è¡Œäº†å¹¿æ³›çš„è§£é‡Šæ€§åˆ†æï¼Œä»¥è§£é‡Šæ¨¡å‹çš„ä¼˜ç‚¹å’Œç¼ºç‚¹ã€‚æˆ‘ä»¬çš„ç»“æœæ­ç¤ºäº†æ°´ä¸‹ç‰©ç§è¯†åˆ«æ–¹é¢ä»æœ‰å¾ˆå¤§çš„æ”¹è¿›ç©ºé—´ï¼Œå¹¶è¡¨æ˜äº†AQUA20ä½œä¸ºæœªæ¥è¯¥é¢†åŸŸç ”ç©¶åŸºç¡€çš„ä»·å€¼ã€‚æ•°æ®é›†å¯åœ¨<a target="_blank" rel="noopener" href="https://huggingface.co/datasets/taufiktrf/AQUA20">https://huggingface.co/datasets/taufiktrf/AQUA20</a>å…¬å¼€è·å–ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.17455v1">PDF</a> Submitted to AJSE Springer</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸€ä¸ªåä¸ºAQUA20çš„æ°´ä¸‹å›¾åƒæ•°æ®é›†ï¼ŒåŒ…å«8,171å¼ åæ˜ çœŸå®æ°´ä¸‹ç¯å¢ƒæŒ‘æˆ˜ï¼ˆå¦‚å…‰ç…§ã€æµŠåº¦ã€é®æŒ¡ç­‰ï¼‰çš„20ç§æµ·æ´‹ç”Ÿç‰©å›¾åƒã€‚æ–‡ç« è¯„ä¼°äº†åŒ…æ‹¬è½»é‡åŒ–CNNï¼ˆå¦‚SqueezeNetã€MobileNetV2ï¼‰å’ŒåŸºäºtransformerçš„æ¶æ„ï¼ˆå¦‚ViTã€ConvNeXtï¼‰åœ¨å†…çš„13ç§æœ€å…ˆè¿›çš„æ·±åº¦å­¦ä¹ æ¨¡å‹åœ¨æ°´ä¸‹ç‰©ç§è¯†åˆ«æ–¹é¢çš„æ€§èƒ½ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒConvNeXtè¡¨ç°æœ€ä½³ï¼Œå‰ä¸‰å‡†ç¡®ç‡è¾¾åˆ°äº†98.82%ï¼Œæœ€é«˜å‡†ç¡®ç‡è¾¾åˆ°äº†90.69%ï¼Œæ€»ä½“F1åˆ†æ•°æœ€é«˜ï¼Œä¸º88.92%ã€‚åŒæ—¶ï¼Œæ–‡ç« è¿˜é€šè¿‡GRAD-CAMå’ŒLIMEè¿›è¡Œäº†æ¨¡å‹çš„å¯è§£é‡Šæ€§åˆ†æã€‚è¯¥æ•°æ®é›†å…¬å¼€å¯ç”¨ï¼Œä¸ºæœªæ¥çš„æ°´ä¸‹è§†è§‰ç ”ç©¶æä¾›äº†æœ‰ä»·å€¼çš„èµ„æºã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>AQUA20æ˜¯ä¸€ä¸ªåŒ…å«å¤šç§æ°´ä¸‹ç¯å¢ƒæŒ‘æˆ˜çš„ç»¼åˆåŸºå‡†æ•°æ®é›†ï¼Œæ¶µç›–8,171å¼ æ°´ä¸‹å›¾åƒï¼Œæ¶‰åŠ20ç§æµ·æ´‹ç”Ÿç‰©ã€‚</li>
<li>æ–‡ç« è¯„ä¼°äº†13ç§æœ€å…ˆè¿›çš„æ·±åº¦å­¦ä¹ æ¨¡å‹åœ¨æ°´ä¸‹ç‰©ç§è¯†åˆ«æ–¹é¢çš„æ€§èƒ½ã€‚</li>
<li>ConvNeXtåœ¨å®éªŒä¸­è¡¨ç°æœ€ä½³ï¼Œå‰ä¸‰å‡†ç¡®ç‡è¾¾åˆ°äº†98.82%ï¼Œæœ€é«˜å‡†ç¡®ç‡è¾¾åˆ°äº†90.69%ï¼Œæ€»ä½“F1åˆ†æ•°æœ€é«˜ã€‚</li>
<li>æ•°æ®é›†æä¾›äº†æ¨¡å‹å¯è§£é‡Šæ€§åˆ†æï¼Œæœ‰åŠ©äºç†è§£æ¨¡å‹çš„ä¼˜ç‚¹å’Œç¼ºç‚¹ã€‚</li>
<li>è¯¥æ•°æ®é›†å…¬å¼€å¯ç”¨ï¼Œä¸ºæœªæ¥çš„æ°´ä¸‹è§†è§‰ç ”ç©¶æä¾›äº†é‡è¦èµ„æºã€‚</li>
<li>å®éªŒç»“æœæ˜¾ç¤ºï¼Œåœ¨å¤æ‚çš„æ°´ä¸‹ç¯å¢ƒä¸‹ï¼Œæ·±åº¦å­¦ä¹ æ¨¡å‹ä»ç„¶å­˜åœ¨æ€§èƒ½ä¸Šçš„æŒ‘æˆ˜å’Œæå‡ç©ºé—´ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.17455">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-41767f56929bf8b13c5a4a07e96884e6.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="RadarSeq-A-Temporal-Vision-Framework-for-User-Churn-Prediction-via-Radar-Chart-Sequences"><a href="#RadarSeq-A-Temporal-Vision-Framework-for-User-Churn-Prediction-via-Radar-Chart-Sequences" class="headerlink" title="RadarSeq: A Temporal Vision Framework for User Churn Prediction via   Radar Chart Sequences"></a>RadarSeq: A Temporal Vision Framework for User Churn Prediction via   Radar Chart Sequences</h2><p><strong>Authors:Sina Najafi, M. Hadi Sepanj, Fahimeh Jafari</strong></p>
<p>Predicting user churn in non-subscription gig platforms, where disengagement is implicit, poses unique challenges due to the absence of explicit labels and the dynamic nature of user behavior. Existing methods often rely on aggregated snapshots or static visual representations, which obscure temporal cues critical for early detection. In this work, we propose a temporally-aware computer vision framework that models user behavioral patterns as a sequence of radar chart images, each encoding day-level behavioral features. By integrating a pretrained CNN encoder with a bidirectional LSTM, our architecture captures both spatial and temporal patterns underlying churn behavior. Extensive experiments on a large real-world dataset demonstrate that our method outperforms classical models and ViT-based radar chart baselines, yielding gains of 17.7 in F1 score, 29.4 in precision, and 16.1 in AUC, along with improved interpretability. The frameworkâ€™s modular design, explainability tools, and efficient deployment characteristics make it suitable for large-scale churn modeling in dynamic gig-economy platforms. </p>
<blockquote>
<p>åœ¨éè®¢é˜…åˆ¶çš„é›¶å·¥å¹³å°é¢„æµ‹ç”¨æˆ·æµå¤±å¸¦æ¥äº†ç‹¬ç‰¹çš„æŒ‘æˆ˜ï¼Œå› ä¸ºè¿™é‡Œä¸å­˜åœ¨æ˜ç¡®çš„æ ‡ç­¾å’Œç”¨æˆ·è¡Œä¸ºçš„åŠ¨æ€æ€§å¯¼è‡´ç”¨æˆ·ç¦»åœºè¡Œä¸ºä¸æ˜æ˜¾ã€‚ç°æœ‰æ–¹æ³•å¾€å¾€ä¾èµ–äºèšåˆå¿«ç…§æˆ–é™æ€è§†è§‰è¡¨ç¤ºï¼Œè¿™ä¼šæ©ç›–æ—©æœŸæ£€æµ‹çš„å…³é”®æ—¶é—´çº¿ç´¢ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§å…·æœ‰æ—¶é—´æ„ŸçŸ¥èƒ½åŠ›çš„è®¡ç®—æœºè§†è§‰æ¡†æ¶ï¼Œè¯¥æ¡†æ¶å°†ç”¨æˆ·è¡Œä¸ºæ¨¡å¼å»ºæ¨¡ä¸ºé›·è¾¾å›¾å›¾åƒåºåˆ—ï¼Œæ¯ä¸ªå›¾åƒéƒ½ç¼–ç æ—¥é—´è¡Œä¸ºç‰¹å¾ã€‚é€šè¿‡å°†é¢„è®­ç»ƒçš„CNNç¼–ç å™¨ä¸åŒå‘LSTMé›†æˆåœ¨ä¸€èµ·ï¼Œæˆ‘ä»¬çš„æ¶æ„æ•è·äº†ç”¨æˆ·æµå¤±è¡Œä¸ºèƒŒåçš„æ—¶ç©ºæ¨¡å¼ã€‚åœ¨å¤§å‹çœŸå®æ•°æ®é›†ä¸Šè¿›è¡Œçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨F1å¾—åˆ†ã€ç²¾ç¡®åº¦å’ŒAUCæ–¹é¢åˆ†åˆ«æé«˜äº†17.7åˆ†ã€29.4åˆ†å’Œ16.1åˆ†ï¼Œè¶…è¿‡äº†ç»å…¸æ¨¡å‹å’ŒåŸºäºViTçš„é›·è¾¾å›¾åŸºçº¿ï¼ŒåŒæ—¶æé«˜äº†å¯è§£é‡Šæ€§ã€‚è¯¥æ¡†æ¶çš„æ¨¡å—åŒ–è®¾è®¡ã€è§£é‡Šå·¥å…·ä»¥åŠé«˜æ•ˆçš„éƒ¨ç½²ç‰¹ç‚¹ä½¿å…¶æˆä¸ºåŠ¨æ€é›¶å·¥ç»æµå¹³å°å¤§è§„æ¨¡æµå¤±å»ºæ¨¡çš„åˆé€‚é€‰æ‹©ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.17325v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>è¯¥æ–‡æœ¬æå‡ºäº†ä¸€ç§é’ˆå¯¹éè®¢é˜…åˆ¶é›¶å·¥å¹³å°ç”¨æˆ·æµå¤±é¢„æµ‹çš„è®¡ç®—æœºè§†è§‰æ¡†æ¶ã€‚è¯¥æ¡†æ¶é€šè¿‡ç»“åˆé¢„è®­ç»ƒçš„CNNç¼–ç å™¨å’ŒåŒå‘LSTMï¼Œå¯¹ç”¨æˆ·è¡Œä¸ºæ¨¡å¼è¿›è¡Œå»ºæ¨¡ï¼Œä»¥é›·è¾¾å›¾åºåˆ—çš„å½¢å¼æ•æ‰ç©ºé—´å’Œæ—¶é—´æ¨¡å¼ã€‚å®éªŒè¯æ˜ï¼Œè¯¥æ–¹æ³•åœ¨çœŸå®æ•°æ®é›†ä¸Šçš„è¡¨ç°ä¼˜äºç»å…¸æ¨¡å‹å’ŒåŸºäºViTçš„é›·è¾¾å›¾åŸºçº¿ï¼Œæé«˜äº†F1åˆ†æ•°ã€ç²¾ç¡®åº¦å’ŒAUCç­‰æŒ‡æ ‡ï¼ŒåŒæ—¶å…·æœ‰è¾ƒå¥½çš„å¯è§£é‡Šæ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>éè®¢é˜…åˆ¶é›¶å·¥å¹³å°ç”¨æˆ·æµå¤±é¢„æµ‹é¢ä¸´ç‹¬ç‰¹æŒ‘æˆ˜ï¼Œå¦‚ç¼ºä¹æ˜ç¡®çš„æ ‡ç­¾å’Œç”¨æˆ·è¡Œä¸ºçš„åŠ¨æ€æ€§ã€‚</li>
<li>ç°æœ‰æ–¹æ³•å¸¸ä¾èµ–äºèšåˆå¿«ç…§æˆ–é™æ€è§†è§‰è¡¨ç¤ºï¼Œå¿½ç•¥äº†æ—©æœŸæ£€æµ‹æ‰€éœ€çš„æ—¶é—´çº¿ç´¢ã€‚</li>
<li>æå‡ºçš„è®¡ç®—æœºè§†è§‰æ¡†æ¶é‡‡ç”¨é›·è¾¾å›¾åºåˆ—å»ºæ¨¡ç”¨æˆ·è¡Œä¸ºæ¨¡å¼ï¼Œç¼–ç æ¯æ—¥è¡Œä¸ºç‰¹å¾ã€‚</li>
<li>ç»“åˆé¢„è®­ç»ƒçš„CNNç¼–ç å™¨å’ŒåŒå‘LSTMï¼Œæ•æ‰ç”¨æˆ·æµå¤±è¡Œä¸ºçš„ç©ºé—´å’Œæ—¶é—´æ¨¡å¼ã€‚</li>
<li>åœ¨çœŸå®æ•°æ®é›†ä¸Šè¿›è¡Œçš„å¤§é‡å®éªŒè¯æ˜ï¼Œè¯¥æ–¹æ³•åœ¨F1åˆ†æ•°ã€ç²¾ç¡®åº¦å’ŒAUCç­‰æ–¹é¢è¡¨ç°ä¼˜å¼‚ã€‚</li>
<li>è¯¥æ¡†æ¶å…·æœ‰æ¨¡å—åŒ–è®¾è®¡ã€å¯è§£é‡Šæ€§å·¥å…·å’Œé«˜æ•ˆéƒ¨ç½²ç‰¹ç‚¹ï¼Œé€‚åˆå¤§è§„æ¨¡åº”ç”¨äºåŠ¨æ€é›¶å·¥ç»æµå¹³å°ç”¨æˆ·æµå¤±å»ºæ¨¡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.17325">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-4a1a5449934925df3f27b0fbef80d0cd.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5f6fd034d33222e09a4e3847c1037bc9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-fe343066c1e05acb3c2063850e1fca0d.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="CLIP-HandID-Vision-Language-Model-for-Hand-Based-Person-Identification"><a href="#CLIP-HandID-Vision-Language-Model-for-Hand-Based-Person-Identification" class="headerlink" title="CLIP-HandID: Vision-Language Model for Hand-Based Person Identification"></a>CLIP-HandID: Vision-Language Model for Hand-Based Person Identification</h2><p><strong>Authors:Nathanael L. Baisa, Babu Pallam, Amudhavel Jayavel</strong></p>
<p>This paper introduces a novel approach to person identification using hand images, designed specifically for criminal investigations. The method is particularly valuable in serious crimes such as sexual abuse, where hand images are often the only identifiable evidence available. Our proposed method, CLIP-HandID, leverages a pre-trained foundational vision-language model - CLIP - to efficiently learn discriminative deep feature representations from hand images (input to CLIPâ€™s image encoder) using textual prompts as semantic guidance. Since hand images are labeled with indexes rather than text descriptions, we employ a textual inversion network to learn pseudo-tokens that encode specific visual contexts or appearance attributes. These learned pseudo-tokens are then incorporated into textual prompts, which are fed into CLIPâ€™s text encoder to leverage its multi-modal reasoning and enhance generalization for identification. Through extensive evaluations on two large, publicly available hand datasets with multi-ethnic representation, we demonstrate that our method significantly outperforms existing approaches. </p>
<blockquote>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åˆ©ç”¨æ‰‹éƒ¨å›¾åƒè¿›è¡Œäººå‘˜è¯†åˆ«çš„æ–°æ–¹æ³•ï¼Œè¯¥æ–¹æ³•ä¸“ä¸ºåˆ‘äº‹ä¾¦æŸ¥è®¾è®¡ã€‚è¯¥æ–¹æ³•åœ¨æ€§è™å¾…ç­‰ä¸¥é‡çŠ¯ç½ªä¸­å°¤å…¶å…·æœ‰ä»·å€¼ï¼Œåœ¨è¿™äº›æƒ…å†µä¸‹ï¼Œæ‰‹éƒ¨å›¾åƒå¾€å¾€æ˜¯å”¯ä¸€å¯ç”¨çš„å¯è¯†åˆ«è¯æ®ã€‚æˆ‘ä»¬æå‡ºçš„æ–¹æ³•CLIP-HandIDï¼Œåˆ©ç”¨é¢„è®­ç»ƒçš„é€šç”¨è§†è§‰è¯­è¨€æ¨¡å‹CLIPï¼Œé€šè¿‡æ–‡æœ¬æç¤ºä½œä¸ºè¯­ä¹‰æŒ‡å¯¼ï¼Œæœ‰æ•ˆåœ°ä»æ‰‹éƒ¨å›¾åƒï¼ˆè¾“å…¥CLIPå›¾åƒç¼–ç å™¨ï¼‰ä¸­å­¦ä¹ åˆ¤åˆ«æ·±åº¦ç‰¹å¾è¡¨ç¤ºã€‚ç”±äºæ‰‹éƒ¨å›¾åƒç”¨ç´¢å¼•è€Œä¸æ˜¯æ–‡æœ¬æè¿°æ¥æ ‡è®°ï¼Œæˆ‘ä»¬é‡‡ç”¨æ–‡æœ¬å€’ç½®ç½‘ç»œæ¥å­¦ä¹ ç¼–ç ç‰¹å®šè§†è§‰ä¸Šä¸‹æ–‡æˆ–å¤–è§‚å±æ€§çš„ä¼ªä»¤ç‰Œã€‚ç„¶åï¼Œè¿™äº›å­¦ä¹ åˆ°çš„ä¼ªä»¤ç‰Œè¢«çº³å…¥æ–‡æœ¬æç¤ºä¸­ï¼Œè¾“å…¥åˆ°CLIPçš„æ–‡æœ¬ç¼–ç å™¨ä¸­ï¼Œä»¥åˆ©ç”¨å¤šæ¨¡å¼æ¨ç†å¹¶å¢å¼ºè¯†åˆ«æ¨å¹¿èƒ½åŠ›ã€‚æˆ‘ä»¬åœ¨ä¸¤ä¸ªå…·æœ‰å¤šæ°‘æ—ä»£è¡¨æ€§çš„å¤§å‹å…¬å¼€æ‰‹éƒ¨æ•°æ®é›†ä¸Šè¿›è¡Œäº†å¹¿æ³›è¯„ä¼°ï¼Œç»“æœè¡¨æ˜æˆ‘ä»¬çš„æ–¹æ³•æ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.12447v2">PDF</a> </p>
<p><strong>Summary</strong><br>è¯¥ç ”ç©¶æå‡ºäº†ä¸€ç§åˆ©ç”¨æ‰‹éƒ¨å›¾åƒè¿›è¡Œäººå‘˜è¯†åˆ«çš„æ–°æ–¹æ³•ï¼Œå°¤å…¶é€‚ç”¨äºåˆ‘äº‹è°ƒæŸ¥ã€‚è¯¥æ–¹æ³•åœ¨æ€§è™å¾…ç­‰ä¸¥é‡çŠ¯ç½ªä¸­å°¤å…¶æœ‰ä»·å€¼ï¼Œæ‰‹éƒ¨å›¾åƒå¾€å¾€æ˜¯å”¯ä¸€å¯ç”¨çš„å¯è¯†åˆ«è¯æ®ã€‚æå‡ºçš„CLIP-HandIDæ–¹æ³•åˆ©ç”¨é¢„è®­ç»ƒçš„è§†è§‰è¯­è¨€æ¨¡å‹CLIPï¼Œé€šè¿‡æ–‡æœ¬æç¤ºä½œä¸ºè¯­ä¹‰æŒ‡å¯¼ï¼Œä»æ‰‹éƒ¨å›¾åƒä¸­å­¦ä¹ è¾¨åˆ«æ·±åº¦ç‰¹å¾è¡¨ç¤ºã€‚ç”±äºæ‰‹éƒ¨å›¾åƒç”¨ç´¢å¼•è€Œéæ–‡æœ¬æè¿°è¿›è¡Œæ ‡æ³¨ï¼Œå› æ­¤é‡‡ç”¨æ–‡æœ¬å€’ç½®ç½‘ç»œå­¦ä¹ ç‰¹å®šè§†è§‰ä¸Šä¸‹æ–‡æˆ–å¤–è§‚å±æ€§çš„ä¼ªæ ‡è®°ã€‚è¿™äº›å­¦ä¹ åˆ°çš„ä¼ªæ ‡è®°èå…¥æ–‡æœ¬æç¤ºä¸­ï¼Œå¹¶è¾“å…¥CLIPçš„æ–‡æœ¬ç¼–ç å™¨ï¼Œä»¥åˆ©ç”¨å…¶å¤šæ¨¡å¼æ¨ç†å¹¶å¢å¼ºè¯†åˆ«æ•ˆæœçš„æ³›åŒ–èƒ½åŠ›ã€‚åœ¨å…·æœ‰å¤šæ°‘æ—ä»£è¡¨æ€§çš„ä¸¤ä¸ªå¤§å‹å…¬å¼€æ‰‹éƒ¨æ•°æ®é›†ä¸Šçš„å¹¿æ³›è¯„ä¼°è¡¨æ˜ï¼Œè¯¥æ–¹æ³•æ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è¯¥è®ºæ–‡ä»‹ç»äº†ä¸€ç§åˆ©ç”¨æ‰‹éƒ¨å›¾åƒè¿›è¡Œäººå‘˜è¯†åˆ«çš„æ–°æ–¹æ³•ï¼Œç‰¹åˆ«é€‚ç”¨äºåˆ‘äº‹è°ƒæŸ¥ä¸­çš„ä¸¥é‡çŠ¯ç½ªã€‚</li>
<li>æå‡ºäº†CLIP-HandIDæ–¹æ³•ï¼Œåˆ©ç”¨é¢„è®­ç»ƒçš„è§†è§‰è¯­è¨€æ¨¡å‹CLIPæ¥å­¦ä¹ æ‰‹éƒ¨å›¾åƒçš„ç‰¹å¾è¡¨ç¤ºã€‚</li>
<li>æ–‡æœ¬æç¤ºä½œä¸ºè¯­ä¹‰æŒ‡å¯¼ï¼Œç”¨äºå¢å¼ºæ¨¡å‹å¯¹æ‰‹éƒ¨å›¾åƒçš„è¯†åˆ«èƒ½åŠ›ã€‚</li>
<li>ç”±äºæ‰‹éƒ¨å›¾åƒä½¿ç”¨ç´¢å¼•æ ‡æ³¨ï¼Œå› æ­¤é‡‡ç”¨æ–‡æœ¬å€’ç½®ç½‘ç»œå­¦ä¹ ç‰¹å®šè§†è§‰ä¸Šä¸‹æ–‡æˆ–å¤–è§‚å±æ€§çš„ä¼ªæ ‡è®°ã€‚</li>
<li>å°†å­¦ä¹ åˆ°çš„ä¼ªæ ‡è®°èå…¥æ–‡æœ¬æç¤ºï¼Œæé«˜æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ã€‚</li>
<li>åœ¨å…·æœ‰å¤šæ°‘æ—ä»£è¡¨æ€§çš„å¤§å‹å…¬å¼€æ‰‹éƒ¨æ•°æ®é›†ä¸Šè¿›è¡Œäº†å¹¿æ³›è¯„ä¼°ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.12447">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-56069efcc3c8978aced9567d0c791953.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ef06b5e3ecbed6b2d20383e008ef3119.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-933d8bd637d9332ee6482b13b7422f5a.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="SALT-A-Flexible-Semi-Automatic-Labeling-Tool-for-General-LiDAR-Point-Clouds-with-Cross-Scene-Adaptability-and-4D-Consistency"><a href="#SALT-A-Flexible-Semi-Automatic-Labeling-Tool-for-General-LiDAR-Point-Clouds-with-Cross-Scene-Adaptability-and-4D-Consistency" class="headerlink" title="SALT: A Flexible Semi-Automatic Labeling Tool for General LiDAR Point   Clouds with Cross-Scene Adaptability and 4D Consistency"></a>SALT: A Flexible Semi-Automatic Labeling Tool for General LiDAR Point   Clouds with Cross-Scene Adaptability and 4D Consistency</h2><p><strong>Authors:Yanbo Wang, Yongtao Chen, Chuan Cao, Tianchen Deng, Wentao Zhao, Jingchuan Wang, Weidong Chen</strong></p>
<p>We propose a flexible Semi-Automatic Labeling Tool (SALT) for general LiDAR point clouds with cross-scene adaptability and 4D consistency. Unlike recent approaches that rely on camera distillation, SALT operates directly on raw LiDAR data, automatically generating pre-segmentation results. To achieve this, we propose a novel zero-shot learning paradigm, termed data alignment, which transforms LiDAR data into pseudo-images by aligning with the training distribution of vision foundation models. Additionally, we design a 4D-consistent prompting strategy and 4D non-maximum suppression module to enhance SAM2, ensuring high-quality, temporally consistent presegmentation. SALT surpasses the latest zero-shot methods by 18.4% PQ on SemanticKITTI and achieves nearly 40-50% of human annotator performance on our newly collected low-resolution LiDAR data and on combined data from three LiDAR types, significantly boosting annotation efficiency. We anticipate that SALTâ€™s open-sourcing will catalyze substantial expansion of current LiDAR datasets and lay the groundwork for the future development of LiDAR foundation models. Code is available at <a target="_blank" rel="noopener" href="https://github.com/Cavendish518/SALT">https://github.com/Cavendish518/SALT</a>. </p>
<blockquote>
<p>æˆ‘ä»¬æå‡ºäº†ä¸€ç§çµæ´»çš„åŠè‡ªåŠ¨æ ‡æ³¨å·¥å…·ï¼ˆSALTï¼‰ï¼Œé€‚ç”¨äºä¸€èˆ¬æ¿€å…‰é›·è¾¾ç‚¹äº‘ï¼Œå…·æœ‰è·¨åœºæ™¯é€‚åº”æ€§å’Œ4Dä¸€è‡´æ€§ã€‚ä¸æœ€è¿‘ä¾èµ–ç›¸æœºè’¸é¦çš„æ–¹æ³•ä¸åŒï¼ŒSALTç›´æ¥åœ¨åŸå§‹æ¿€å…‰é›·è¾¾æ•°æ®ä¸Šè¿è¡Œï¼Œè‡ªåŠ¨ç”Ÿæˆé¢„åˆ†å‰²ç»“æœã€‚ä¸ºå®ç°è¿™ä¸€ç‚¹ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„é›¶æ ·æœ¬å­¦ä¹ èŒƒå¼ï¼Œç§°ä¸ºæ•°æ®å¯¹é½ï¼Œé€šè¿‡å¯¹é½æ¿€å…‰é›·è¾¾æ•°æ®ä¸è§†è§‰åŸºç¡€æ¨¡å‹çš„è®­ç»ƒåˆ†å¸ƒï¼Œå°†æ¿€å…‰é›·è¾¾æ•°æ®è½¬æ¢ä¸ºä¼ªå›¾åƒã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è®¾è®¡äº†4Dä¸€è‡´æç¤ºç­–ç•¥å’Œ4Déæœ€å¤§å€¼æŠ‘åˆ¶æ¨¡å—ï¼Œä»¥å¢å¼ºSAM2ï¼Œç¡®ä¿é«˜è´¨é‡ã€æ—¶é—´ä¸€è‡´çš„é¢„åˆ†å‰²ã€‚SALTåœ¨SemanticKITTIä¸Šçš„PQå¾—åˆ†è¶…è¿‡æœ€æ–°é›¶æ ·æœ¬æ–¹æ³•18.4%ï¼Œåœ¨æˆ‘ä»¬æ–°æ”¶é›†çš„ä½åˆ†è¾¨ç‡æ¿€å…‰é›·è¾¾æ•°æ®å’Œä¸‰ç§æ¿€å…‰é›·è¾¾ç±»å‹ç»„åˆçš„æ•°æ®ä¸Šï¼Œè¾¾åˆ°äººç±»æ ‡æ³¨å™¨æ€§èƒ½çš„è¿‘40-50%ï¼Œæ˜¾è‘—æé«˜äº†æ ‡æ³¨æ•ˆç‡ã€‚æˆ‘ä»¬é¢„è®¡SALTçš„å¼€æºå°†æå¤§åœ°æ¨åŠ¨å½“å‰æ¿€å…‰é›·è¾¾æ•°æ®é›†çš„å‘å±•ï¼Œå¹¶ä¸ºæœªæ¥æ¿€å…‰é›·è¾¾åŸºç¡€æ¨¡å‹çš„å¼€å‘å¥ å®šåŸºç¡€ã€‚ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/Cavendish518/SALT%E5%A4%84%E9%9D%A2%E5%8F%96%E3%80%82">https://github.com/Cavendish518/SALTå¤„è·å–ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.23980v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§çµæ´»çš„åŠè‡ªåŠ¨æ ‡æ³¨å·¥å…·ï¼ˆSALTï¼‰ï¼Œé€‚ç”¨äºä¸€èˆ¬çš„æ¿€å…‰é›·è¾¾ç‚¹äº‘æ•°æ®ï¼Œå…·æœ‰è·¨åœºæ™¯é€‚åº”æ€§å’Œ4Dä¸€è‡´æ€§ã€‚SALTç›´æ¥åœ¨åŸå§‹æ¿€å…‰é›·è¾¾æ•°æ®ä¸Šæ“ä½œï¼Œè‡ªåŠ¨ç”Ÿæˆé¢„åˆ†å‰²ç»“æœï¼Œä¸åŒäºä¾èµ–ç›¸æœºè’¸é¦çš„ç°æœ‰æ–¹æ³•ã€‚ä¸ºå®ç°è¿™ä¸€ç‚¹ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºæ•°æ®å¯¹é½çš„æ–°å‹é›¶æ ·æœ¬å­¦ä¹ èŒƒå¼ï¼Œé€šè¿‡å°†æ¿€å…‰é›·è¾¾æ•°æ®ä¸è§†è§‰åŸºç¡€æ¨¡å‹çš„è®­ç»ƒåˆ†å¸ƒå¯¹é½ï¼Œå°†æ¿€å…‰é›·è¾¾æ•°æ®è½¬æ¢ä¸ºä¼ªå›¾åƒã€‚åŒæ—¶ï¼Œè®¾è®¡äº†4Dä¸€è‡´çš„æç¤ºç­–ç•¥å’Œ4Déæœ€å¤§æŠ‘åˆ¶æ¨¡å—ï¼Œå¢å¼ºSAM2ï¼Œç¡®ä¿é«˜è´¨é‡ã€æ—¶é—´ä¸€è‡´çš„é¢„åˆ†å‰²ã€‚SALTåœ¨SemanticKITTIä¸Šçš„PQå¾—åˆ†æ¯”æœ€æ–°çš„é›¶æ ·æœ¬æ–¹æ³•é«˜å‡º18.4%ï¼Œåœ¨æ–°æ”¶é›†çš„ä½åˆ†è¾¨ç‡æ¿€å…‰é›·è¾¾æ•°æ®å’Œä¸‰ç§æ¿€å…‰é›·è¾¾ç±»å‹ç»„åˆçš„æ•°æ®ä¸Šï¼Œè¾¾åˆ°äº†äººç±»æ ‡æ³¨å™¨æ€§èƒ½çš„è¿‘40-50%ï¼Œæ˜¾è‘—æé«˜äº†æ ‡æ³¨æ•ˆç‡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>SALTæ˜¯ä¸€ç§åŠè‡ªåŠ¨æ ‡æ³¨å·¥å…·ï¼Œé€‚ç”¨äºä¸€èˆ¬æ¿€å…‰é›·è¾¾ç‚¹äº‘æ•°æ®ï¼Œå…·æœ‰è·¨åœºæ™¯é€‚åº”æ€§å’Œ4Dä¸€è‡´æ€§ã€‚</li>
<li>SALTç›´æ¥åœ¨åŸå§‹æ¿€å…‰é›·è¾¾æ•°æ®ä¸Šæ“ä½œï¼Œè‡ªåŠ¨ç”Ÿæˆé¢„åˆ†å‰²ç»“æœã€‚</li>
<li>æ•°æ®å¯¹é½çš„é›¶æ ·æœ¬å­¦ä¹ èŒƒå¼å°†æ¿€å…‰é›·è¾¾æ•°æ®è½¬æ¢ä¸ºä¼ªå›¾åƒï¼Œé€šè¿‡ä¸è§†è§‰åŸºç¡€æ¨¡å‹çš„è®­ç»ƒåˆ†å¸ƒå¯¹é½å®ç°ã€‚</li>
<li>4Dä¸€è‡´çš„æç¤ºç­–ç•¥å’Œ4Déæœ€å¤§æŠ‘åˆ¶æ¨¡å—å¢å¼ºSAM2ï¼Œç¡®ä¿é«˜è´¨é‡ã€æ—¶é—´ä¸€è‡´çš„é¢„åˆ†å‰²ã€‚</li>
<li>SALTåœ¨SemanticKITTIæ•°æ®é›†ä¸Šçš„æ€§èƒ½ä¼˜äºå…¶ä»–é›¶æ ·æœ¬æ–¹æ³•ã€‚</li>
<li>SALTåœ¨æ–°æ”¶é›†çš„ä½åˆ†è¾¨ç‡æ¿€å…‰é›·è¾¾æ•°æ®å’Œå¤šç§æ¿€å…‰é›·è¾¾ç±»å‹ç»„åˆçš„æ•°æ®ä¸Šå–å¾—äº†æ¥è¿‘äººç±»æ ‡æ³¨å™¨æ€§èƒ½çš„æˆæœã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.23980">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-fb82127c069da71b8b8ea015fba8cbd0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-79d5371cfeada8bf5f4646f7c5bcde04.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-9d3097c2fb102438b8ecfe6c853269ab.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d88e077cf6c1548c84a50d721b5e0180.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-edb242785650d7312656771c3249c10a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-61bb2172546ea85f6e20be15a5a27fb2.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Transformer-based-RGB-T-Tracking-with-Channel-and-Spatial-Feature-Fusion"><a href="#Transformer-based-RGB-T-Tracking-with-Channel-and-Spatial-Feature-Fusion" class="headerlink" title="Transformer-based RGB-T Tracking with Channel and Spatial Feature Fusion"></a>Transformer-based RGB-T Tracking with Channel and Spatial Feature Fusion</h2><p><strong>Authors:Yunfeng Li, Bo Wang, Ye Li</strong></p>
<p>The main problem in RGB-T tracking is the correct and optimal merging of the cross-modal features of visible and thermal images. Some previous methods either do not fully exploit the potential of RGB and TIR information for channel and spatial feature fusion or lack a direct interaction between the template and the search area, which limits the modelâ€™s ability to fully utilize the original semantic information of both modalities. To address these limitations, we investigate how to achieve a direct fusion of cross-modal channels and spatial features in RGB-T tracking and propose CSTNet. It uses the Vision Transformer (ViT) as the backbone and adds a Joint Spatial and Channel Fusion Module (JSCFM) and Spatial Fusion Module (SFM) integrated between the transformer blocks to facilitate cross-modal feature interaction. The JSCFM module achieves joint modeling of channel and multi-level spatial features. The SFM module includes a cross-attention-like architecture for cross modeling and joint learning of RGB and TIR features. Comprehensive experiments show that CSTNet achieves state-of-the-art performance. To enhance practicality, we retrain the model without JSCFM and SFM modules and use CSNet as the pretraining weight, and propose CSTNet-small, which achieves 50% speedup with an average decrease of 1-2% in SR and PR performance. CSTNet and CSTNet-small achieve real-time speeds of 21 fps and 33 fps on the Nvidia Jetson Xavier, meeting actual deployment requirements. Code is available at <a target="_blank" rel="noopener" href="https://github.com/LiYunfengLYF/CSTNet">https://github.com/LiYunfengLYF/CSTNet</a>. </p>
<blockquote>
<p>RGB-Tè·Ÿè¸ªä¸­çš„ä¸»è¦é—®é¢˜æ˜¯æ­£ç¡®ä¸”æœ€ä¼˜åœ°èåˆå¯è§å…‰å’Œçƒ­å›¾åƒçš„è·¨æ¨¡æ€ç‰¹å¾ã€‚ä¸€äº›ä¹‹å‰çš„æ–¹æ³•è¦ä¹ˆæ²¡æœ‰å……åˆ†åˆ©ç”¨RGBå’ŒTIRä¿¡æ¯æ¥è¿›è¡Œé€šé“å’Œç©ºé—´ç‰¹å¾èåˆï¼Œè¦ä¹ˆç¼ºä¹æ¨¡æ¿å’Œæœç´¢åŒºåŸŸä¹‹é—´çš„ç›´æ¥äº¤äº’ï¼Œè¿™é™åˆ¶äº†æ¨¡å‹å……åˆ†åˆ©ç”¨ä¸¤ç§æ¨¡æ€çš„åŸå§‹è¯­ä¹‰ä¿¡æ¯çš„èƒ½åŠ›ã€‚ä¸ºäº†è§£å†³è¿™äº›å±€é™æ€§ï¼Œæˆ‘ä»¬ç ”ç©¶äº†å¦‚ä½•åœ¨RGB-Tè·Ÿè¸ªä¸­å®ç°è·¨æ¨¡æ€é€šé“å’Œç©ºé—´ç‰¹å¾çš„ç›´æ¥èåˆï¼Œå¹¶æå‡ºäº†CSTNetã€‚å®ƒä½¿ç”¨è§†è§‰è½¬æ¢å™¨ï¼ˆViTï¼‰ä½œä¸ºä¸»å¹²ï¼Œå¹¶æ·»åŠ äº†è”åˆç©ºé—´å’Œé€šé“èåˆæ¨¡å—ï¼ˆJSCFMï¼‰å’Œç©ºé—´èåˆæ¨¡å—ï¼ˆSFMï¼‰ï¼Œè¿™äº›æ¨¡å—é›†æˆåœ¨è½¬æ¢å™¨å—ä¹‹é—´ï¼Œä»¥ä¿ƒè¿›è·¨æ¨¡æ€ç‰¹å¾äº¤äº’ã€‚JSCFMæ¨¡å—å®ç°äº†é€šé“å’Œå¤šçº§ç©ºé—´ç‰¹å¾çš„è”åˆå»ºæ¨¡ã€‚SFMæ¨¡å—åŒ…æ‹¬ä¸€ç§ç±»ä¼¼äº¤å‰æ³¨æ„åŠ›çš„æ¶æ„ï¼Œç”¨äºRGBå’ŒTIRç‰¹å¾çš„äº¤å‰å»ºæ¨¡å’Œè”åˆå­¦ä¹ ã€‚ç»¼åˆå®éªŒè¡¨æ˜ï¼ŒCSTNetè¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚ä¸ºäº†æé«˜å®ç”¨æ€§ï¼Œæˆ‘ä»¬é‡æ–°è®­ç»ƒäº†ä¸å¸¦JSCFMå’ŒSFMæ¨¡å—çš„æ¨¡å‹ï¼Œå¹¶ä½¿ç”¨CSNetä½œä¸ºé¢„è®­ç»ƒæƒé‡ï¼Œæå‡ºäº†CSTNet-smallï¼Œå®ƒå®ç°äº†50%çš„åŠ é€Ÿï¼Œåœ¨SRå’ŒPRæ€§èƒ½ä¸Šå¹³å‡ä¸‹é™1-2%ã€‚CSTNetå’ŒCSTNet-smallåœ¨Nvidia Jetson Xavierä¸Šå®ç°å®æ—¶é€Ÿåº¦åˆ†åˆ«ä¸º21å¸§å’Œ33å¸§ï¼Œæ»¡è¶³å®é™…éƒ¨ç½²è¦æ±‚ã€‚ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/LiYunfengLYF/CSTNet%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/LiYunfengLYF/CSTNetæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2405.03177v3">PDF</a> This work has been submitted to the IEEE for possible publication</p>
<p><strong>æ‘˜è¦</strong></p>
<p>æœ¬æ–‡è§£å†³RGB-Tè·Ÿè¸ªä¸­çš„ä¸»è¦éš¾é¢˜â€”â€”å¯è§å…‰å’Œçƒ­æˆåƒå›¾åƒçš„å¤šæ¨¡æ€ç‰¹å¾æ­£ç¡®å’Œæœ€ä¼˜èåˆé—®é¢˜ã€‚é’ˆå¯¹ç°æœ‰æ–¹æ³•æœªå……åˆ†åˆ©ç”¨RGBå’ŒTIRä¿¡æ¯çš„é€šé“å’Œç©ºé—´ç‰¹å¾èåˆæ½œåŠ›ï¼Œä»¥åŠæ¨¡æ¿å’Œæœç´¢åŒºåŸŸä¹‹é—´ç¼ºä¹ç›´æ¥äº¤äº’çš„é—®é¢˜ï¼Œæœ¬æ–‡ç ”ç©¶äº†å¦‚ä½•å®ç°RGB-Tè·Ÿè¸ªä¸­çš„è·¨æ¨¡æ€é€šé“å’Œç©ºé—´ç‰¹å¾çš„ç›´æ¥èåˆï¼Œå¹¶æå‡ºäº†CSTNetã€‚å®ƒä»¥Vision Transformerï¼ˆViTï¼‰ä¸ºéª¨å¹²ç½‘ï¼Œå¹¶åœ¨transformerå—ä¹‹é—´æ·»åŠ äº†è”åˆç©ºé—´å’Œé€šé“èåˆæ¨¡å—ï¼ˆJSCFMï¼‰å’Œç©ºé—´èåˆæ¨¡å—ï¼ˆSFMï¼‰ï¼Œä»¥ä¿ƒè¿›è·¨æ¨¡æ€ç‰¹å¾äº¤äº’ã€‚JSCFMæ¨¡å—å®ç°äº†é€šé“å’Œå¤šçº§ç©ºé—´ç‰¹å¾çš„è”åˆå»ºæ¨¡ã€‚SFMæ¨¡å—é‡‡ç”¨ç±»ä¼¼äº¤å‰æ³¨æ„åŠ›çš„æ¶æ„ï¼Œå®ç°RGBå’ŒTIRç‰¹å¾çš„è·¨å»ºæ¨¡å’Œè”åˆå­¦ä¹ ã€‚å®éªŒè¡¨æ˜ï¼ŒCSTNetè¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚ä¸ºæé«˜å®ç”¨æ€§ï¼Œæˆ‘ä»¬é‡æ–°è®­ç»ƒäº†ä¸å¸¦JSCFMå’ŒSFMæ¨¡å—çš„æ¨¡å‹ï¼Œä»¥CSNetä½œä¸ºé¢„è®­ç»ƒæƒé‡ï¼Œå¹¶æ¨å‡ºäº†CSTNet-smallï¼Œå…¶é€Ÿåº¦æé«˜äº†50%ï¼Œåœ¨SRå’ŒPRæ€§èƒ½æ–¹é¢å¹³å‡é™ä½äº†1-2%ã€‚CSTNetå’ŒCSTNet-smallåœ¨Nvidia Jetson Xavierä¸Šå®ç°äº†å®æ—¶é€Ÿåº¦åˆ†åˆ«ä¸ºæ¯ç§’21å¸§å’Œ33å¸§ï¼Œæ»¡è¶³å®é™…éƒ¨ç½²è¦æ±‚ã€‚ç›¸å…³ä»£ç å·²å‘å¸ƒåœ¨GitHubä¸Šã€‚ä»£ç åœ°å€ï¼š<a target="_blank" rel="noopener" href="https://github.com/LiYunfengLYF/CSTNet">https://github.com/LiYunfengLYF/CSTNet</a>ã€‚</p>
<p><strong>è¦ç‚¹</strong></p>
<ol>
<li>RGB-Tè·Ÿè¸ªä¸­çš„ä¸»è¦æŒ‘æˆ˜åœ¨äºæ­£ç¡®å¹¶æœ€ä¼˜åœ°èåˆå¯è§å…‰å’Œçƒ­æˆåƒå›¾åƒçš„è·¨æ¨¡æ€ç‰¹å¾ã€‚</li>
<li>ç°æœ‰æ–¹æ³•åœ¨æŸäº›æ–¹é¢å­˜åœ¨å±€é™ï¼Œå¦‚æœªå……åˆ†åˆ©ç”¨RGBå’ŒTIRä¿¡æ¯ï¼Œæˆ–åœ¨æ¨¡æ¿å’Œæœç´¢åŒºä¹‹é—´ç¼ºä¹ç›´æ¥äº¤äº’ã€‚</li>
<li>CSTNeté€šè¿‡å¼•å…¥Vision Transformerï¼ˆViTï¼‰ä½œä¸ºéª¨å¹²ç½‘æ¥è§£å†³è¿™äº›é—®é¢˜ï¼Œå¹¶æ·»åŠ äº†è”åˆç©ºé—´å’Œé€šé“èåˆæ¨¡å—ï¼ˆJSCFMï¼‰ä»¥åŠç©ºé—´èåˆæ¨¡å—ï¼ˆSFMï¼‰ã€‚</li>
<li>JSCFMæ¨¡å—å®ç°äº†é€šé“å’Œå¤šçº§ç©ºé—´ç‰¹å¾çš„è”åˆå»ºæ¨¡ï¼Œè€ŒSFMæ¨¡å—é‡‡ç”¨ç±»ä¼¼äº¤å‰æ³¨æ„åŠ›çš„æ¶æ„è¿›è¡Œç‰¹å¾è·¨å»ºæ¨¡å’Œè”åˆå­¦ä¹ ã€‚</li>
<li>CSTNetåœ¨å®éªŒä¸­è¡¨ç°å‡ºå“è¶Šæ€§èƒ½ï¼Œè€ŒCSTNet-smallç‰ˆæœ¬åˆ™æ›´æ³¨é‡å®ç”¨æ€§ï¼Œå®ç°äº†é€Ÿåº¦çš„æå‡å¹¶ä¿æŒäº†ç›¸å¯¹çš„æ€§èƒ½ã€‚</li>
<li>CSTNetå’ŒCSTNet-smallæ»¡è¶³å®æ—¶éƒ¨ç½²è¦æ±‚ï¼Œå¹¶åœ¨Nvidia Jetson Xavierä¸Šå®ç°äº†è¾ƒé«˜çš„å¸§ç‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2405.03177">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-8b410f111ac5692ce67a75b7a448c331.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-c0da7a7faa54dd71d933ad50d29fa400.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a549d1295ad0f46785402e3a7cf373c0.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b0520084c44735f64e0ea48d3f57b8c8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-20f6ace5c0472ee6ce1c37429506e328.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-06-25/Vision%20Transformer/">https://kedreamix.github.io/Talk2Paper/Paper/2025-06-25/Vision%20Transformer/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Vision-Transformer/">
                                    <span class="chip bg-color">Vision Transformer</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-06-25/%E6%A3%80%E6%B5%8B_%E5%88%86%E5%89%B2_%E8%B7%9F%E8%B8%AA/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-5fdbf250cf0512d5b6f10b8de0def0c5.jpg" class="responsive-img" alt="æ£€æµ‹/åˆ†å‰²/è·Ÿè¸ª">
                        
                        <span class="card-title">æ£€æµ‹/åˆ†å‰²/è·Ÿè¸ª</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            æ£€æµ‹/åˆ†å‰²/è·Ÿè¸ª æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-06-25  Pre-Trained LLM is a Semantic-Aware and Generalizable Segmentation   Booster
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-06-25
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E6%A3%80%E6%B5%8B-%E5%88%86%E5%89%B2-%E8%B7%9F%E8%B8%AA/" class="post-category">
                                    æ£€æµ‹/åˆ†å‰²/è·Ÿè¸ª
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E6%A3%80%E6%B5%8B-%E5%88%86%E5%89%B2-%E8%B7%9F%E8%B8%AA/">
                        <span class="chip bg-color">æ£€æµ‹/åˆ†å‰²/è·Ÿè¸ª</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-06-25/%E8%A7%86%E9%A2%91%E7%90%86%E8%A7%A3/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-8d844f39af18f665381930b20ae9828b.jpg" class="responsive-img" alt="è§†é¢‘ç†è§£">
                        
                        <span class="card-title">è§†é¢‘ç†è§£</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            è§†é¢‘ç†è§£ æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-06-25  VQ-Insight Teaching VLMs for AI-Generated Video Quality Understanding   via Progressive Visual Reinforcement Learning
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-06-25
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E8%A7%86%E9%A2%91%E7%90%86%E8%A7%A3/" class="post-category">
                                    è§†é¢‘ç†è§£
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E8%A7%86%E9%A2%91%E7%90%86%E8%A7%A3/">
                        <span class="chip bg-color">è§†é¢‘ç†è§£</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">23901.5k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
