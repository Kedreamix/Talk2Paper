<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Agent">
    <meta name="description" content="Agent æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-06-25  Audit &amp; Repair An Agentic Framework for Consistent Story Visualization   in Text-to-Image Diffusion Models">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Agent | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-52b9af6db973294039e24f9bc0227e29.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Agent</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Agent/">
                                <span class="chip bg-color">Agent</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Agent/" class="post-category">
                                Agent
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-06-25
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-07-06
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    16.6k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    67 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-06-25-æ›´æ–°"><a href="#2025-06-25-æ›´æ–°" class="headerlink" title="2025-06-25 æ›´æ–°"></a>2025-06-25 æ›´æ–°</h1><h2 id="Audit-Repair-An-Agentic-Framework-for-Consistent-Story-Visualization-in-Text-to-Image-Diffusion-Models"><a href="#Audit-Repair-An-Agentic-Framework-for-Consistent-Story-Visualization-in-Text-to-Image-Diffusion-Models" class="headerlink" title="Audit &amp; Repair: An Agentic Framework for Consistent Story Visualization   in Text-to-Image Diffusion Models"></a>Audit &amp; Repair: An Agentic Framework for Consistent Story Visualization   in Text-to-Image Diffusion Models</h2><p><strong>Authors:Kiymet Akdemir, Tahira Kazimi, Pinar Yanardag</strong></p>
<p>Story visualization has become a popular task where visual scenes are generated to depict a narrative across multiple panels. A central challenge in this setting is maintaining visual consistency, particularly in how characters and objects persist and evolve throughout the story. Despite recent advances in diffusion models, current approaches often fail to preserve key character attributes, leading to incoherent narratives. In this work, we propose a collaborative multi-agent framework that autonomously identifies, corrects, and refines inconsistencies across multi-panel story visualizations. The agents operate in an iterative loop, enabling fine-grained, panel-level updates without re-generating entire sequences. Our framework is model-agnostic and flexibly integrates with a variety of diffusion models, including rectified flow transformers such as Flux and latent diffusion models such as Stable Diffusion. Quantitative and qualitative experiments show that our method outperforms prior approaches in terms of multi-panel consistency. </p>
<blockquote>
<p>æ•…äº‹å¯è§†åŒ–å·²ç»æˆä¸ºä¸€é¡¹æµè¡Œçš„ä»»åŠ¡ï¼Œé€šè¿‡ç”Ÿæˆè§†è§‰åœºæ™¯æ¥æç»˜è·¨å¤šä¸ªé¢æ¿çš„å™äº‹ã€‚åœ¨è¿™ä¸ªä»»åŠ¡ä¸­çš„ä¸€ä¸ªæ ¸å¿ƒæŒ‘æˆ˜æ˜¯ä¿æŒè§†è§‰ä¸€è‡´æ€§ï¼Œç‰¹åˆ«æ˜¯åœ¨è§’è‰²å’Œç‰©ä½“å¦‚ä½•åœ¨æ•…äº‹ä¸­æŒç»­å’Œæ¼”å˜æ–¹é¢ã€‚å°½ç®¡æ‰©æ•£æ¨¡å‹æœ€è¿‘æœ‰äº†ä¸€äº›è¿›å±•ï¼Œä½†å½“å‰çš„æ–¹æ³•å¾€å¾€æ— æ³•ä¿ç•™å…³é”®çš„è§’è‰²å±æ€§ï¼Œä»è€Œå¯¼è‡´å™äº‹ä¸ä¸€è‡´ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åä½œå¼å¤šæ™ºèƒ½ä½“æ¡†æ¶ï¼Œè¯¥æ¡†æ¶èƒ½å¤Ÿè‡ªä¸»è¯†åˆ«ã€çº æ­£å’Œå®Œå–„è·¨å¤šé¢æ¿æ•…äº‹å¯è§†åŒ–ä¸­çš„ä¸ä¸€è‡´æ€§ã€‚æ™ºèƒ½ä½“åœ¨ä¸€ä¸ªè¿­ä»£å¾ªç¯ä¸­è¿è¡Œï¼Œèƒ½å¤Ÿå®ç°ç²¾ç»†çš„é¢æ¿çº§æ›´æ–°ï¼Œè€Œæ— éœ€é‡æ–°ç”Ÿæˆæ•´ä¸ªåºåˆ—ã€‚æˆ‘ä»¬çš„æ¡†æ¶ä¸å„ç§æ‰©æ•£æ¨¡å‹æ— å…³ï¼Œå¯ä»¥çµæ´»åœ°ä¸å¤šç§æ‰©æ•£æ¨¡å‹é›†æˆï¼ŒåŒ…æ‹¬ç»è¿‡ä¿®æ­£çš„æµå˜å‹å™¨ï¼ˆå¦‚Fluxï¼‰å’Œæ½œåœ¨æ‰©æ•£æ¨¡å‹ï¼ˆå¦‚Stable Diffusionï¼‰ã€‚å®šé‡å’Œå®šæ€§å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨è·¨é¢æ¿ä¸€è‡´æ€§æ–¹é¢ä¼˜äºä»¥å‰çš„æ–¹æ³•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.18900v1">PDF</a> Project webpage: <a target="_blank" rel="noopener" href="https://auditandrepair.github.io/">https://auditandrepair.github.io/</a></p>
<p><strong>Summary</strong></p>
<p>æ•…äº‹å¯è§†åŒ–å·²æˆä¸ºä¸€ä¸ªçƒ­é—¨ä»»åŠ¡ï¼Œæ—¨åœ¨ç”Ÿæˆå¤šä¸ªé¢æ¿æ¥å‘ˆç°å™äº‹ã€‚å…¶ä¸­çš„æ ¸å¿ƒæŒ‘æˆ˜æ˜¯ä¿æŒè§†è§‰ä¸€è‡´æ€§ï¼Œç‰¹åˆ«æ˜¯åœ¨è§’è‰²å’Œç‰©ä½“å¦‚ä½•åœ¨æ•…äº‹ä¸­æŒç»­å’Œæ¼”å˜æ–¹é¢ã€‚å°½ç®¡æ‰©æ•£æ¨¡å‹æœ€è¿‘æœ‰æ‰€è¿›å±•ï¼Œä½†å½“å‰çš„æ–¹æ³•å¾€å¾€æ— æ³•ä¿ç•™å…³é”®è§’è‰²å±æ€§ï¼Œå¯¼è‡´å™äº‹ä¸ä¸€è‡´ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åä½œçš„å¤šæ™ºèƒ½ä½“æ¡†æ¶ï¼Œè¯¥æ¡†æ¶å¯è‡ªä¸»è¯†åˆ«ã€çº æ­£å’Œå®Œå–„å¤šé¢æ¿æ•…äº‹å¯è§†åŒ–ä¸­çš„ä¸ä¸€è‡´æ€§ã€‚æ™ºèƒ½ä½“åœ¨è¿­ä»£å¾ªç¯ä¸­è¿è¡Œï¼Œå¯å®ç°ç²¾ç»†çš„é¢æ¿çº§æ›´æ–°ï¼Œè€Œæ— éœ€é‡æ–°ç”Ÿæˆæ•´ä¸ªåºåˆ—ã€‚æˆ‘ä»¬çš„æ¡†æ¶ä¸å„ç§æ‰©æ•£æ¨¡å‹å…¼å®¹ï¼ŒåŒ…æ‹¬ç»è¿‡ä¿®æ­£çš„æµå˜å‹å™¨ï¼ˆå¦‚Fluxï¼‰å’Œæ½œåœ¨æ‰©æ•£æ¨¡å‹ï¼ˆå¦‚Stable Diffusionï¼‰ã€‚å®šé‡å’Œå®šæ€§å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨è·¨é¢æ¿ä¸€è‡´æ€§æ–¹é¢ä¼˜äºå…ˆå‰çš„æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ•…äº‹å¯è§†åŒ–æ—¨åœ¨é€šè¿‡å¤šä¸ªé¢æ¿å‘ˆç°å™äº‹ï¼Œç»´æŒè§†è§‰ä¸€è‡´æ€§æ˜¯æ ¸å¿ƒæŒ‘æˆ˜ã€‚</li>
<li>å½“å‰æ–¹æ³•éš¾ä»¥ä¿ç•™å…³é”®è§’è‰²å±æ€§ï¼Œå¯¼è‡´å™äº‹ä¸ä¸€è‡´ã€‚</li>
<li>æå‡ºäº†ä¸€ç§åä½œçš„å¤šæ™ºèƒ½ä½“æ¡†æ¶æ¥è¯†åˆ«å’Œçº æ­£å¤šé¢æ¿æ•…äº‹å¯è§†åŒ–ä¸­çš„ä¸ä¸€è‡´æ€§ã€‚</li>
<li>æ™ºèƒ½ä½“å¯åœ¨è¿­ä»£å¾ªç¯ä¸­è¿è¡Œï¼Œå®ç°ç²¾ç»†çš„é¢æ¿çº§æ›´æ–°ã€‚</li>
<li>æ¡†æ¶ä¸å¤šç§æ‰©æ•£æ¨¡å‹å…¼å®¹ï¼ŒåŒ…æ‹¬æµå˜å‹å™¨å’Œæ½œåœ¨æ‰©æ•£æ¨¡å‹ã€‚</li>
<li>å®šé‡å’Œå®šæ€§å®éªŒè¯æ˜è¯¥æ–¹æ³•åœ¨è·¨é¢æ¿ä¸€è‡´æ€§æ–¹é¢ä¼˜äºå…ˆå‰æŠ€æœ¯ã€‚</li>
<li>è¯¥æ–¹æ³•æœ‰åŠ©äºæé«˜æ•…äº‹å¯è§†åŒ–çš„è´¨é‡å’Œè§‚æ„Ÿã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.18900">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-29929cf271d4e60819d0a244f86092f9.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-f2b4cb6d451b978f05f24dfff2a1929e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-af5cb62242c3325bae1406b227fe34c1.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="GRAND-SLAM-Local-Optimization-for-Globally-Consistent-Large-Scale-Multi-Agent-Gaussian-SLAM"><a href="#GRAND-SLAM-Local-Optimization-for-Globally-Consistent-Large-Scale-Multi-Agent-Gaussian-SLAM" class="headerlink" title="GRAND-SLAM: Local Optimization for Globally Consistent Large-Scale   Multi-Agent Gaussian SLAM"></a>GRAND-SLAM: Local Optimization for Globally Consistent Large-Scale   Multi-Agent Gaussian SLAM</h2><p><strong>Authors:Annika Thomas, Aneesa Sonawalla, Alex Rose, Jonathan P. How</strong></p>
<p>3D Gaussian splatting has emerged as an expressive scene representation for RGB-D visual SLAM, but its application to large-scale, multi-agent outdoor environments remains unexplored. Multi-agent Gaussian SLAM is a promising approach to rapid exploration and reconstruction of environments, offering scalable environment representations, but existing approaches are limited to small-scale, indoor environments. To that end, we propose Gaussian Reconstruction via Multi-Agent Dense SLAM, or GRAND-SLAM, a collaborative Gaussian splatting SLAM method that integrates i) an implicit tracking module based on local optimization over submaps and ii) an approach to inter- and intra-robot loop closure integrated into a pose-graph optimization framework. Experiments show that GRAND-SLAM provides state-of-the-art tracking performance and 28% higher PSNR than existing methods on the Replica indoor dataset, as well as 91% lower multi-agent tracking error and improved rendering over existing multi-agent methods on the large-scale, outdoor Kimera-Multi dataset. </p>
<blockquote>
<p>ä¸‰ç»´é«˜æ–¯å–·æº…ï¼ˆGaussian Splattingï¼‰æŠ€æœ¯å·²æˆä¸ºRGB-Dè§†è§‰SLAMï¼ˆSimultaneous Localization and Mappingï¼‰çš„åœºæ™¯è¡¨ç¤ºçš„ä¸€ç§è¡¨è¾¾æ–¹å¼ï¼Œä½†å…¶åœ¨å¤§å‹å¤šæ™ºèƒ½ä½“å®¤å¤–ç¯å¢ƒä¸­çš„åº”ç”¨å°šæœªå¾—åˆ°æ¢ç´¢ã€‚å¤šæ™ºèƒ½ä½“é«˜æ–¯SLAMï¼ˆSimultaneous Localization and Mappingï¼‰æ˜¯ä¸€ç§æœ‰å‰æ™¯çš„æ–¹æ³•ï¼Œå¯ä»¥å¿«é€Ÿæ¢ç´¢å¹¶é‡å»ºç¯å¢ƒï¼Œæä¾›å¯æ‰©å±•çš„ç¯å¢ƒè¡¨ç¤ºï¼Œä½†ç°æœ‰æ–¹æ³•ä»…é™äºå°å‹å®¤å†…ç¯å¢ƒã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†åŸºäºå¤šæ™ºèƒ½ä½“å¯†é›†SLAMçš„é«˜æ–¯é‡å»ºï¼ˆGaussian Reconstruction via Multi-Agent Dense SLAMï¼‰ï¼Œç®€ç§°GRAND-SLAMã€‚è¿™æ˜¯ä¸€ç§åä½œå¼é«˜æ–¯å–·æº…SLAMæ–¹æ³•ï¼Œå®ƒé›†æˆäº†i)åŸºäºå­å›¾å±€éƒ¨ä¼˜åŒ–çš„éšå¼è·Ÿè¸ªæ¨¡å—å’Œii)ä¸€ç§é›†æˆåˆ°å§¿æ€å›¾ä¼˜åŒ–æ¡†æ¶ä¸­çš„æœºå™¨äººå†…å¤–å›è·¯é—­åˆæ–¹æ³•ã€‚å®éªŒè¡¨æ˜ï¼ŒGRAND-SLAMåœ¨Replicaå®¤å†…æ•°æ®é›†ä¸Šæä¾›äº†æœ€å…ˆè¿›çš„è·Ÿè¸ªæ€§èƒ½å’Œæ¯”ç°æœ‰æ–¹æ³•é«˜å‡º28%çš„å³°å€¼ä¿¡å™ªæ¯”ï¼ˆPSNRï¼‰ï¼Œä»¥åŠåœ¨å¤§å‹å®¤å¤–Kimera-Multiæ•°æ®é›†ä¸Šå®ç°äº†91%çš„å¤šæ™ºèƒ½ä½“è·Ÿè¸ªè¯¯å·®é™ä½å’Œæ¸²æŸ“æ€§èƒ½çš„æå‡ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.18885v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ¢è®¨äº†å°†å¤šæ™ºèƒ½ä½“æŠ€æœ¯åº”ç”¨äºå¤§è§„æ¨¡æˆ·å¤–ç¯å¢ƒçš„è§†è§‰å³æ—¶å®šä½ä¸åœ°å›¾æ„å»ºï¼ˆSLAMï¼‰é—®é¢˜ã€‚æå‡ºäº†ä¸€ç§åä¸ºGRAND-SLAMçš„ååŒé«˜æ–¯èåˆSLAMæ–¹æ³•ï¼Œé€šè¿‡å¼•å…¥å±€éƒ¨ä¼˜åŒ–å­å›¾çš„éšå¼è·Ÿè¸ªæ¨¡å—å’Œé›†æˆæœºå™¨äººé—´ä¸æœºå™¨äººå†…éƒ¨çš„é—­ç¯ç­–ç•¥ï¼Œå®ç°äº†åœ¨å¤§å‹ç¯å¢ƒä¸­çš„å“è¶Šæ€§èƒ½ã€‚å®éªŒè¡¨æ˜ï¼ŒGRAND-SLAMåœ¨å¤§å‹å®¤å¤–ç¯å¢ƒä¸­çš„æ€§èƒ½ä¼˜äºç°æœ‰æ–¹æ³•ã€‚ç›¸è¾ƒäºåœ¨å®¤å†…ç¯å¢ƒä¸­å¤åˆ¶çš„ç°æœ‰æ•°æ®é›†ï¼ŒGRAND-SLAMæä¾›å‡ºè‰²çš„è¿½è¸ªæ€§èƒ½å¹¶å®ç°äº†è¾ƒé«˜çš„å³°å€¼ä¿¡å™ªæ¯”ï¼ˆPSNRï¼‰ã€‚åŒæ—¶ï¼Œåœ¨å¤§å‹å®¤å¤–æ•°æ®é›†ä¸Šï¼Œå…¶å¤šæ™ºèƒ½ä½“è¿½è¸ªè¯¯å·®é™ä½äº†çº¦91%ï¼Œæ¸²æŸ“æ•ˆæœä¹Ÿå¾—åˆ°äº†æ”¹å–„ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ä»‹ç»äº†ä¸‰ç»´é«˜æ–¯èåˆåœ¨å¤§è§„æ¨¡å¤šæ™ºèƒ½ä½“æˆ·å¤–ç¯å¢ƒä¸­çš„æ½œåœ¨åº”ç”¨ã€‚</li>
<li>æå‡ºäº†ä¸€ç§æ–°çš„ååŒé«˜æ–¯èåˆSLAMæ–¹æ³•â€”â€”GRAND-SLAMã€‚</li>
<li>GRAND-SLAMå¼•å…¥äº†éšå¼è·Ÿè¸ªæ¨¡å—ï¼ŒåŸºäºå±€éƒ¨ä¼˜åŒ–å­å›¾å®ç°ã€‚</li>
<li>GRAND-SLAMé›†æˆäº†æœºå™¨äººé—´ä¸æœºå™¨äººå†…éƒ¨çš„é—­ç¯ç­–ç•¥ã€‚</li>
<li>å®éªŒè¡¨æ˜GRAND-SLAMåœ¨å¤§å‹å®¤å¤–ç¯å¢ƒä¸­çš„æ€§èƒ½ä¼˜äºç°æœ‰æ–¹æ³•ã€‚ç›¸è¾ƒäºå®¤å†…ç¯å¢ƒæ•°æ®é›†ï¼Œå…¶è¿½è¸ªæ€§èƒ½å’ŒPSNRè¡¨ç°ä¼˜è¶Šã€‚</li>
<li>åœ¨å¤§å‹å®¤å¤–æ•°æ®é›†ä¸Šï¼ŒGRAND-SLAMçš„å¤šæ™ºèƒ½ä½“è¿½è¸ªè¯¯å·®é™ä½äº†çº¦91%ï¼Œæ¸²æŸ“æ•ˆæœå¾—åˆ°æ”¹å–„ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.18885">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-9054e4c345eab596e4d061bbe486775d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-52b9af6db973294039e24f9bc0227e29.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ba88ddff43ef2cbb13b015a13ea93a22.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-6c18cbcbfe3e922ebae7c3b5dfff30d8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-781a0fd4554b18bebacec2c65e9509d2.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Understanding-Software-Engineering-Agents-A-Study-of-Thought-Action-Result-Trajectories"><a href="#Understanding-Software-Engineering-Agents-A-Study-of-Thought-Action-Result-Trajectories" class="headerlink" title="Understanding Software Engineering Agents: A Study of   Thought-Action-Result Trajectories"></a>Understanding Software Engineering Agents: A Study of   Thought-Action-Result Trajectories</h2><p><strong>Authors:Islem Bouzenia, Michael Pradel</strong></p>
<p>Large Language Model (LLM)-based agents are increasingly employed to automate complex software engineering tasks such as program repair and issue resolution. These agents operate by autonomously generating natural language thoughts, invoking external tools, and iteratively refining their solutions. Despite their widespread adoption, the internal decision-making processes of these agents remain largely unexplored, limiting our understanding of their operational dynamics and failure modes. In this paper, we present a large-scale empirical study of the thought-action-result trajectories of three state-of-the-art LLM-based agents: \textsc{RepairAgent}, \textsc{AutoCodeRover}, and \textsc{OpenHands}. We unify their interaction logs into a common format, capturing 120 trajectories and 2822 LLM interactions focused on program repair and issue resolution. Our study combines quantitative analyses of structural properties, action patterns, and token usage with qualitative assessments of reasoning coherence and feedback integration. We identify key trajectory characteristics such as iteration counts and token consumption, recurring action sequences, and the semantic coherence linking thoughts, actions, and their results. Our findings reveal behavioral motifs and anti-patterns that distinguish successful from failed executions, providing actionable insights for improving agent design, including prompting strategies, failure diagnosis, and anti-pattern detection. We release our dataset and annotation framework to support further research on transparent and robust autonomous software engineering agents. </p>
<blockquote>
<p>åŸºäºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„ä»£ç†æ­£è¶Šæ¥è¶Šå¤šåœ°è¢«ç”¨äºè‡ªåŠ¨åŒ–å¤æ‚çš„è½¯ä»¶å·¥ç¨‹ä»»åŠ¡ï¼Œå¦‚ç¨‹åºä¿®å¤å’Œé—®é¢˜è§£æã€‚è¿™äº›ä»£ç†é€šè¿‡è‡ªä¸»ç”Ÿæˆè‡ªç„¶è¯­è¨€æ€æƒ³ã€è°ƒç”¨å¤–éƒ¨å·¥å…·å¹¶è¿­ä»£ä¼˜åŒ–å…¶è§£å†³æ–¹æ¡ˆæ¥è¿è¡Œã€‚å°½ç®¡è¿™äº›ä»£ç†å¾—åˆ°äº†å¹¿æ³›åº”ç”¨ï¼Œä½†å…¶å†…éƒ¨å†³ç­–è¿‡ç¨‹åœ¨å¾ˆå¤§ç¨‹åº¦ä¸Šä»æœªè¢«æ¢ç´¢ï¼Œè¿™é™åˆ¶äº†æˆ‘ä»¬å¯¹å®ƒä»¬è¿è¡ŒåŠ¨æ€å’Œæ•…éšœæ¨¡å¼çš„äº†è§£ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬å¯¹ä¸‰ç§æœ€æ–°åŸºäºLLMçš„ä»£ç†ï¼ˆRepairAgentã€AutoCodeRoverå’ŒOpenHandsï¼‰çš„æ€æƒ³-è¡ŒåŠ¨-ç»“æœè½¨è¿¹è¿›è¡Œäº†å¤§è§„æ¨¡å®è¯ç ”ç©¶ã€‚æˆ‘ä»¬å°†å®ƒä»¬çš„äº¤äº’æ—¥å¿—ç»Ÿä¸€ä¸ºé€šç”¨æ ¼å¼ï¼Œæ•è·äº†120æ¡è½¨è¿¹å’Œ2822æ¬¡ä¸“æ³¨äºç¨‹åºä¿®å¤å’Œé—®é¢˜è§£æçš„LLMäº¤äº’ã€‚æˆ‘ä»¬çš„ç ”ç©¶ç»“åˆäº†ç»“æ„å±æ€§ã€è¡ŒåŠ¨æ¨¡å¼å’Œä»¤ç‰Œä½¿ç”¨é‡çš„å®šé‡åˆ†æï¼Œä»¥åŠå¯¹æ¨ç†è¿è´¯æ€§å’Œåé¦ˆæ•´åˆçš„å®šæ€§è¯„ä¼°ã€‚æˆ‘ä»¬ç¡®å®šäº†å…³é”®è½¨è¿¹ç‰¹å¾ï¼Œå¦‚è¿­ä»£æ¬¡æ•°å’Œä»¤ç‰Œæ¶ˆè€—ã€åå¤å‡ºç°çš„è¡ŒåŠ¨åºåˆ—ï¼Œä»¥åŠè¿æ¥æ€æƒ³ã€è¡ŒåŠ¨å’Œç»“æœä¹‹é—´çš„è¯­ä¹‰è¿è´¯æ€§ã€‚æˆ‘ä»¬çš„ç ”ç©¶ç»“æœæ­ç¤ºäº†åŒºåˆ†æˆåŠŸæ‰§è¡Œå’Œå¤±è´¥æ‰§è¡Œçš„è¡Œä¸ºæ¨¡å¼å’Œåæ¨¡å¼ï¼Œä¸ºæ”¹è¿›ä»£ç†è®¾è®¡æä¾›äº†å¯æ“ä½œçš„è§è§£ï¼ŒåŒ…æ‹¬æç¤ºç­–ç•¥ã€æ•…éšœè¯Šæ–­å’Œåæ¨¡å¼æ£€æµ‹ã€‚æˆ‘ä»¬å‘å¸ƒäº†æˆ‘ä»¬çš„æ•°æ®é›†å’Œæ³¨é‡Šæ¡†æ¶ï¼Œä»¥æ”¯æŒå¯¹é€æ˜å’Œç¨³å¥çš„è‡ªä¸»è½¯ä»¶å·¥ç¨‹ä»£ç†çš„è¿›ä¸€æ­¥ç ”ç©¶ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.18824v1">PDF</a> </p>
<p><strong>Summary</strong>ï¼šåŸºäºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„ä»£ç†è¢«å¹¿æ³›åº”ç”¨äºè‡ªåŠ¨åŒ–è½¯ä»¶å·¥ç¨‹çš„å¤æ‚ä»»åŠ¡ï¼Œå¦‚ç¨‹åºä¿®å¤å’Œé—®é¢˜è§£æã€‚ç„¶è€Œï¼Œå®ƒä»¬çš„å†…éƒ¨å†³ç­–è¿‡ç¨‹å°šå¾…æ¢ç´¢ï¼Œé™åˆ¶äº†æˆ‘ä»¬å¯¹å®ƒä»¬æ“ä½œåŠ¨æ€å’Œå¤±è´¥æ¨¡å¼çš„ç†è§£ã€‚æœ¬ç ”ç©¶å¯¹ä¸‰ç§æœ€æ–°LLMä»£ç†çš„â€œæ€ç»´-è¡ŒåŠ¨-ç»“æœâ€è½¨è¿¹è¿›è¡Œäº†å¤§è§„æ¨¡å®è¯ç ”ç©¶ï¼Œå¹¶æ­ç¤ºäº†å…¶è¡Œä¸ºæ¨¡å¼å’Œå¤±è´¥æ¨¡å¼çš„å…³é”®ç‰¹å¾ã€‚ç ”ç©¶æœ‰åŠ©äºæ”¹è¿›ä»£ç†è®¾è®¡ï¼Œå¹¶æä¾›å…³äºæç¤ºç­–ç•¥ã€æ•…éšœè¯Šæ–­å’Œåæ¨¡å¼æ£€æµ‹çš„è§è§£ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>LLMä»£ç†å¹¿æ³›åº”ç”¨äºè‡ªåŠ¨åŒ–è½¯ä»¶å·¥ç¨‹çš„ç¨‹åºä¿®å¤å’Œé—®é¢˜è§£æä»»åŠ¡ã€‚</li>
<li>LLMä»£ç†çš„å†…éƒ¨å†³ç­–è¿‡ç¨‹å°šå¾…æ¢ç´¢ï¼Œé™åˆ¶äº†å¯¹å…¶æ“ä½œåŠ¨æ€å’Œå¤±è´¥æ¨¡å¼çš„ç†è§£ã€‚</li>
<li>æœ¬ç ”ç©¶å¯¹ä¸‰ç§LLMä»£ç†è¿›è¡Œäº†å¤§è§„æ¨¡å®è¯ç ”ç©¶ï¼Œæ¶‰åŠç¨‹åºä¿®å¤å’Œé—®é¢˜è§£æçš„è½¨è¿¹åˆ†æã€‚</li>
<li>ç ”ç©¶å‘ç°æˆåŠŸä¸å¤±è´¥æ‰§è¡Œçš„å…³é”®ç‰¹å¾åŒ…æ‹¬è¿­ä»£æ¬¡æ•°ã€ä»¤ç‰Œæ¶ˆè€—ã€é‡å¤çš„è¡ŒåŠ¨åºåˆ—ä»¥åŠæ€ç»´ä¸è¡ŒåŠ¨ä¹‹é—´çš„è¯­ä¹‰è¿è´¯æ€§ã€‚</li>
<li>ç ”ç©¶æ­ç¤ºäº†ä»£ç†çš„è¡Œä¸ºæ¨¡å¼å’Œåæ¨¡å¼ï¼Œæœ‰åŠ©äºæ”¹è¿›ä»£ç†è®¾è®¡ã€‚</li>
<li>ç ”ç©¶æä¾›äº†å…³äºæç¤ºç­–ç•¥ã€æ•…éšœè¯Šæ–­å’Œåæ¨¡å¼æ£€æµ‹çš„è§è§£ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.18824">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-e028b99219c1c3ac6f48ab5ae5fa23eb.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-223cf5c87ddbd9b3cd80cf2ceaa5a2c6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-769f0321f2616baa0da60122f6ffaad2.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="MARL-MambaContour-Unleashing-Multi-Agent-Deep-Reinforcement-Learning-for-Active-Contour-Optimization-in-Medical-Image-Segmentation"><a href="#MARL-MambaContour-Unleashing-Multi-Agent-Deep-Reinforcement-Learning-for-Active-Contour-Optimization-in-Medical-Image-Segmentation" class="headerlink" title="MARL-MambaContour: Unleashing Multi-Agent Deep Reinforcement Learning   for Active Contour Optimization in Medical Image Segmentation"></a>MARL-MambaContour: Unleashing Multi-Agent Deep Reinforcement Learning   for Active Contour Optimization in Medical Image Segmentation</h2><p><strong>Authors:Ruicheng Zhang, Yu Sun, Zeyu Zhang, Jinai Li, Xiaofan Liu, Au Hoi Fan, Haowei Guo, Puxin Yan</strong></p>
<p>We introduce MARL-MambaContour, the first contour-based medical image segmentation framework based on Multi-Agent Reinforcement Learning (MARL). Our approach reframes segmentation as a multi-agent cooperation task focused on generate topologically consistent object-level contours, addressing the limitations of traditional pixel-based methods which could lack topological constraints and holistic structural awareness of anatomical regions. Each contour point is modeled as an autonomous agent that iteratively adjusts its position to align precisely with the target boundary, enabling adaptation to blurred edges and intricate morphologies common in medical images. This iterative adjustment process is optimized by a contour-specific Soft Actor-Critic (SAC) algorithm, further enhanced with the Entropy Regularization Adjustment Mechanism (ERAM) which dynamically balance agent exploration with contour smoothness. Furthermore, the framework incorporates a Mamba-based policy network featuring a novel Bidirectional Cross-attention Hidden-state Fusion Mechanism (BCHFM). This mechanism mitigates potential memory confusion limitations associated with long-range modeling in state space models, thereby facilitating more accurate inter-agent information exchange and informed decision-making. Extensive experiments on five diverse medical imaging datasets demonstrate the state-of-the-art performance of MARL-MambaContour, highlighting its potential as an accurate and robust clinical application. </p>
<blockquote>
<p>æˆ‘ä»¬ä»‹ç»äº†MARL-MambaContourï¼Œè¿™æ˜¯åŸºäºå¤šæ™ºèƒ½ä½“å¼ºåŒ–å­¦ä¹ ï¼ˆMARLï¼‰çš„é¦–ä¸ªè½®å»“åŸºåŒ»ç–—å›¾åƒåˆ†å‰²æ¡†æ¶ã€‚æˆ‘ä»¬çš„æ–¹æ³•å°†åˆ†å‰²é‡æ–°æ„å»ºä¸ºä¸€ä¸ªå¤šæ™ºèƒ½ä½“åˆä½œä»»åŠ¡ï¼Œä¸“æ³¨äºç”Ÿæˆæ‹“æ‰‘ä¸€è‡´çš„å¯¹è±¡çº§è½®å»“ï¼Œè§£å†³äº†ä¼ ç»Ÿåƒç´ çº§æ–¹æ³•ç¼ºä¹æ‹“æ‰‘çº¦æŸå’Œæ•´ä½“ç»“æ„æ„ŸçŸ¥è§£å‰–åŒºåŸŸçš„å±€é™æ€§ã€‚æ¯ä¸ªè½®å»“ç‚¹éƒ½è¢«å»ºæ¨¡ä¸ºè‡ªä¸»æ™ºèƒ½ä½“ï¼Œå¯ä»¥è¿­ä»£è°ƒæ•´å…¶ä½ç½®ä»¥ç²¾ç¡®ä¸ç›®æ ‡è¾¹ç•Œå¯¹é½ï¼Œä»è€Œé€‚åº”åŒ»ç–—å›¾åƒä¸­å¸¸è§çš„æ¨¡ç³Šè¾¹ç¼˜å’Œå¤æ‚å½¢æ€ã€‚è¿™ç§è¿­ä»£è°ƒæ•´è¿‡ç¨‹é€šè¿‡è½®å»“ç‰¹å®šçš„æŸ”è½¯æ¼”å‘˜è¯„è®ºå®¶ï¼ˆSACï¼‰ç®—æ³•ä¼˜åŒ–ï¼Œè¿›ä¸€æ­¥å¢å¼ºä¸ç†µæ­£åˆ™åŒ–è°ƒæ•´æœºåˆ¶ï¼ˆERAMï¼‰ï¼ŒåŠ¨æ€å¹³è¡¡æ™ºèƒ½ä½“æ¢ç´¢ä¸è½®å»“å¹³æ»‘ã€‚æ­¤å¤–ï¼Œè¯¥æ¡†æ¶é‡‡ç”¨åŸºäºMambaçš„ç­–ç•¥ç½‘ç»œï¼Œå¹¶å¼•å…¥äº†ä¸€ç§æ–°å‹çš„åŒå‘äº¤å‰æ³¨æ„éšè—çŠ¶æ€èåˆæœºåˆ¶ï¼ˆBCHFMï¼‰ã€‚è¯¥æœºåˆ¶ç¼“è§£äº†çŠ¶æ€ç©ºé—´æ¨¡å‹ä¸­é•¿ç¨‹å»ºæ¨¡å¯èƒ½å¸¦æ¥çš„æ½œåœ¨å†…å­˜æ··æ·†é™åˆ¶ï¼Œä»è€Œä¿ƒè¿›äº†æ›´å‡†ç¡®çš„æ™ºèƒ½ä½“é—´ä¿¡æ¯äº¤æ¢å’Œå†³ç­–ã€‚åœ¨äº”ä¸ªä¸åŒçš„åŒ»å­¦æˆåƒæ•°æ®é›†ä¸Šçš„å¹¿æ³›å®éªŒè¯æ˜äº†MARL-MambaContourçš„å“è¶Šæ€§èƒ½ï¼Œçªæ˜¾äº†å…¶åœ¨ä¸´åºŠåº”ç”¨ä¸­ä½œä¸ºå‡†ç¡®ä¸”ç¨³å¥çš„æ½œåœ¨ä¼˜åŠ¿ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.18679v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>MARL-MambaContouræ˜¯ä¸€ä¸ªåŸºäºå¤šæ™ºèƒ½ä½“å¼ºåŒ–å­¦ä¹ ï¼ˆMARLï¼‰çš„è½®å»“åŸºç¡€åŒ»å­¦å›¾åƒåˆ†å‰²æ¡†æ¶ã€‚å®ƒå°†åˆ†å‰²è§†ä¸ºå¤šæ™ºèƒ½ä½“åˆä½œä»»åŠ¡ï¼Œä¾§é‡äºç”Ÿæˆæ‹“æ‰‘ä¸€è‡´çš„ç‰©ä½“çº§è½®å»“ï¼Œè§£å†³äº†ä¼ ç»Ÿåƒç´ çº§æ–¹æ³•ç¼ºä¹æ‹“æ‰‘çº¦æŸå’Œæ•´ä½“ç»“æ„æ„ŸçŸ¥çš„é—®é¢˜ã€‚è¯¥æ¡†æ¶åˆ©ç”¨è½®å»“ç‰¹å®šçš„Soft Actor-Criticç®—æ³•å’Œç†µæ­£åˆ™åŒ–è°ƒæ•´æœºåˆ¶ï¼Œå¹¶é€šè¿‡Mambaç­–ç•¥ç½‘ç»œå’ŒåŒå‘äº¤å‰å…³æ³¨éšè—çŠ¶æ€èåˆæœºåˆ¶æé«˜å‡†ç¡®æ€§ã€‚åœ¨äº”ä¸ªä¸åŒçš„åŒ»å­¦æˆåƒæ•°æ®é›†ä¸Šçš„å®éªŒè¯æ˜äº†å…¶å“è¶Šæ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>MARL-MambaContouræ˜¯é¦–ä¸ªåŸºäºå¤šæ™ºèƒ½ä½“å¼ºåŒ–å­¦ä¹ ï¼ˆMARLï¼‰çš„è½®å»“åŸºç¡€åŒ»å­¦å›¾åƒåˆ†å‰²æ¡†æ¶ã€‚</li>
<li>è¯¥æ–¹æ³•å°†åŒ»å­¦å›¾åƒåˆ†å‰²è§†ä¸ºå¤šæ™ºèƒ½ä½“åˆä½œä»»åŠ¡ï¼Œä¾§é‡äºç”Ÿæˆæ‹“æ‰‘ä¸€è‡´çš„ç‰©ä½“çº§è½®å»“ã€‚</li>
<li>è§£å†³äº†ä¼ ç»Ÿåƒç´ çº§æ–¹æ³•ç¼ºä¹æ‹“æ‰‘çº¦æŸå’Œæ•´ä½“ç»“æ„æ„ŸçŸ¥çš„é—®é¢˜ã€‚</li>
<li>è½®å»“è°ƒæ•´è¿‡ç¨‹ä¸­å¼•å…¥äº†è½®å»“ç‰¹å®šçš„Soft Actor-Criticç®—æ³•å’Œç†µæ­£åˆ™åŒ–è°ƒæ•´æœºåˆ¶ï¼ˆERAMï¼‰ã€‚</li>
<li>ä½¿ç”¨äº†Mambaç­–ç•¥ç½‘ç»œæ¥æé«˜æ€§èƒ½ï¼ŒåŒ…æ‹¬ä¸€ä¸ªæ–°çš„åŒå‘äº¤å‰å…³æ³¨éšè—çŠ¶æ€èåˆæœºåˆ¶ï¼ˆBCHFMï¼‰ã€‚</li>
<li>è¯¥æ¡†æ¶èƒ½å¤Ÿåœ¨å¤„ç†åŒ»å­¦å›¾åƒä¸­çš„æ¨¡ç³Šè¾¹ç¼˜å’Œå¤æ‚å½¢æ€æ—¶è¡¨ç°å‡ºå¼ºå¤§çš„é€‚åº”æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.18679">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-1160180733a1f15701879c81a783397f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d7b8741f0392cde885ecb4ef733c361b.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="MCN-SLAM-Multi-Agent-Collaborative-Neural-SLAM-with-Hybrid-Implicit-Neural-Scene-Representation"><a href="#MCN-SLAM-Multi-Agent-Collaborative-Neural-SLAM-with-Hybrid-Implicit-Neural-Scene-Representation" class="headerlink" title="MCN-SLAM: Multi-Agent Collaborative Neural SLAM with Hybrid Implicit   Neural Scene Representation"></a>MCN-SLAM: Multi-Agent Collaborative Neural SLAM with Hybrid Implicit   Neural Scene Representation</h2><p><strong>Authors:Tianchen Deng, Guole Shen, Xun Chen, Shenghai Yuan, Hongming Shen, Guohao Peng, Zhenyu Wu, Jingchuan Wang, Lihua Xie, Danwei Wang, Hesheng Wang, Weidong Chen</strong></p>
<p>Neural implicit scene representations have recently shown promising results in dense visual SLAM. However, existing implicit SLAM algorithms are constrained to single-agent scenarios, and fall difficulties in large-scale scenes and long sequences. Existing NeRF-based multi-agent SLAM frameworks cannot meet the constraints of communication bandwidth. To this end, we propose the first distributed multi-agent collaborative neural SLAM framework with hybrid scene representation, distributed camera tracking, intra-to-inter loop closure, and online distillation for multiple submap fusion. A novel triplane-grid joint scene representation method is proposed to improve scene reconstruction. A novel intra-to-inter loop closure method is designed to achieve local (single-agent) and global (multi-agent) consistency. We also design a novel online distillation method to fuse the information of different submaps to achieve global consistency. Furthermore, to the best of our knowledge, there is no real-world dataset for NeRF-based&#x2F;GS-based SLAM that provides both continuous-time trajectories groundtruth and high-accuracy 3D meshes groundtruth. To this end, we propose the first real-world Dense slam (DES) dataset covering both single-agent and multi-agent scenarios, ranging from small rooms to large-scale outdoor scenes, with high-accuracy ground truth for both 3D mesh and continuous-time camera trajectory. This dataset can advance the development of the research in both SLAM, 3D reconstruction, and visual foundation model. Experiments on various datasets demonstrate the superiority of the proposed method in both mapping, tracking, and communication. The dataset and code will open-source on <a target="_blank" rel="noopener" href="https://github.com/dtc111111/mcnslam">https://github.com/dtc111111/mcnslam</a>. </p>
<blockquote>
<p>ç¥ç»éšå¼åœºæ™¯è¡¨ç¤ºåœ¨å¯†é›†è§†è§‰SLAMä¸­æœ€è¿‘æ˜¾ç¤ºå‡ºæœ‰å‰æ™¯çš„ç»“æœã€‚ç„¶è€Œï¼Œç°æœ‰çš„éšå¼SLAMç®—æ³•ä»…é™äºå•æ™ºèƒ½ä½“åœºæ™¯ï¼Œåœ¨å¤§åœºæ™¯å’Œé•¿åºåˆ—ä¸­é‡åˆ°å›°éš¾ã€‚åŸºäºNeRFçš„å¤šæ™ºèƒ½ä½“SLAMæ¡†æ¶æ— æ³•æ»¡è¶³é€šä¿¡å¸¦å®½çš„é™åˆ¶ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†ç¬¬ä¸€ä¸ªåˆ†å¸ƒå¼å¤šæ™ºèƒ½ä½“ååŒç¥ç»SLAMæ¡†æ¶ï¼Œå…·æœ‰æ··åˆåœºæ™¯è¡¨ç¤ºã€åˆ†å¸ƒå¼ç›¸æœºè·Ÿè¸ªã€å†…éƒ¨åˆ°è·¨å¾ªç¯é—­åˆä»¥åŠå¤šå­å›¾èåˆçš„åœ¨çº¿è’¸é¦ã€‚æå‡ºäº†ä¸€ç§æ–°çš„triplaneç½‘æ ¼è”åˆåœºæ™¯è¡¨ç¤ºæ–¹æ³•ï¼Œä»¥æé«˜åœºæ™¯é‡å»ºçš„æ•ˆæœã€‚è®¾è®¡äº†ä¸€ç§æ–°é¢–çš„intra-to-interå¾ªç¯é—­åˆæ–¹æ³•ï¼Œä»¥å®ç°å±€éƒ¨ï¼ˆå•æ™ºèƒ½ä½“ï¼‰å’Œå…¨å±€ï¼ˆå¤šæ™ºèƒ½ä½“ï¼‰çš„ä¸€è‡´æ€§ã€‚æˆ‘ä»¬è¿˜è®¾è®¡äº†ä¸€ç§æ–°é¢–çš„åœ¨çº¿è’¸é¦æ–¹æ³•ï¼Œä»¥èåˆä¸åŒå­å›¾çš„ä¿¡æ¯æ¥å®ç°å…¨å±€ä¸€è‡´æ€§ã€‚æ­¤å¤–ï¼Œæ®æˆ‘ä»¬æ‰€çŸ¥ï¼Œæ²¡æœ‰åŸºäºNeRFæˆ–GSçš„SLAMçœŸå®ä¸–ç•Œæ•°æ®é›†èƒ½å¤Ÿæä¾›è¿ç»­æ—¶é—´è½¨è¿¹çš„åœ°é¢çœŸå®æ•°æ®å’Œé«˜ç²¾åº¦ä¸‰ç»´ç½‘æ ¼åœ°é¢çœŸå®æ•°æ®ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†ç¬¬ä¸€ä¸ªçœŸå®ä¸–ç•Œçš„å¯†é›†SLAMï¼ˆDESï¼‰æ•°æ®é›†ï¼Œæ¶µç›–å•æ™ºèƒ½ä½“å’Œå¤šæ™ºèƒ½ä½“åœºæ™¯ï¼Œä»å°æˆ¿é—´åˆ°å¤§è§„æ¨¡æˆ·å¤–åœºæ™¯ï¼ŒåŒæ—¶æä¾›ä¸‰ç»´ç½‘æ ¼å’Œè¿ç»­æ—¶é—´ç›¸æœºè½¨è¿¹çš„é«˜ç²¾åº¦åœ°é¢çœŸå®æ•°æ®ã€‚è¯¥æ•°æ®é›†å¯ä»¥ä¿ƒè¿›SLAMã€ä¸‰ç»´é‡å»ºå’Œè§†è§‰åŸºç¡€æ¨¡å‹çš„ç ”ç©¶å‘å±•ã€‚åœ¨å„ç§æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œæ‰€æå‡ºçš„æ–¹æ³•åœ¨æ˜ å°„ã€è·Ÿè¸ªå’Œé€šä¿¡æ–¹é¢éƒ½å…·æœ‰ä¼˜è¶Šæ€§ã€‚æ•°æ®é›†å’Œä»£ç å°†åœ¨<a target="_blank" rel="noopener" href="https://github.com/dtc111111/mcnslam%E4%B8%8A%E5%BC%80%E6%BA%90%E3%80%82">https://github.com/dtc111111/mcnslamä¸Šå¼€æºã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.18678v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†é¦–ä¸ªåˆ†å¸ƒå¼å¤šæ™ºèƒ½ä½“ååŒç¥ç»ç½‘ç»œSLAMæ¡†æ¶ï¼Œè¯¥æ¡†æ¶å…·æœ‰æ··åˆåœºæ™¯è¡¨ç¤ºã€åˆ†å¸ƒå¼ç›¸æœºè·Ÿè¸ªã€å†…å¤–ç¯é—­åˆä»¥åŠåœ¨çº¿è’¸é¦å¤šé‡å­å›¾èåˆç­‰ç‰¹ç‚¹ã€‚æå‡ºä¸€ç§æ–°å‹çš„ä¸‰å¹³é¢ç½‘æ ¼è”åˆåœºæ™¯è¡¨ç¤ºæ–¹æ³•ï¼Œæ”¹è¿›äº†åœºæ™¯é‡å»ºã€‚è®¾è®¡äº†ä¸€ç§æ–°å‹çš„å†…å¤–ç¯é—­åˆæ–¹æ³•ï¼Œå®ç°äº†å±€éƒ¨å’Œå…¨å±€çš„ä¸€è‡´æ€§ã€‚æ­¤å¤–ï¼Œè¿˜æå‡ºäº†ä¸€ç§åœ¨çº¿è’¸é¦æ–¹æ³•ï¼Œèåˆä¸åŒå­å›¾çš„ä¿¡æ¯ä»¥å®ç°å…¨å±€ä¸€è‡´æ€§ã€‚ç¼ºä¹é’ˆå¯¹NeRFæˆ–GSçš„SLAMç°å®æ•°æ®é›†ï¼Œä¸ºæ­¤æ¨å‡ºäº†é¦–ä¸ªå¯†é›†slamï¼ˆDESï¼‰æ•°æ®é›†ï¼ŒåŒ…å«å•æ™ºèƒ½ä½“å’Œå¤šæ™ºèƒ½ä½“åœºæ™¯ï¼Œä»å°å‹æˆ¿é—´åˆ°å¤§å‹å®¤å¤–åœºæ™¯ï¼ŒåŒæ—¶æä¾›é«˜ç²¾åº¦çš„ä¸‰ç»´ç½‘æ ¼å’Œè¿ç»­æ—¶é—´ç›¸æœºè½¨è¿¹çš„çœŸå®æ•°æ®ã€‚è¯¥æ•°æ®é›†æ¨åŠ¨äº†SLAMã€ä¸‰ç»´é‡å»ºå’Œè§†è§‰åŸºç¡€æ¨¡å‹çš„ç ”ç©¶å‘å±•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æå‡ºäº†é¦–ä¸ªåˆ†å¸ƒå¼å¤šæ™ºèƒ½ä½“ååŒç¥ç»ç½‘ç»œSLAMæ¡†æ¶ï¼Œé€‚ç”¨äºå¤§è§„æ¨¡åœºæ™¯å’Œé•¿åºåˆ—ã€‚</li>
<li>é‡‡ç”¨äº†æ··åˆåœºæ™¯è¡¨ç¤ºã€åˆ†å¸ƒå¼ç›¸æœºè·Ÿè¸ªã€å†…å¤–ç¯é—­åˆå’Œåœ¨çº¿è’¸é¦æŠ€æœ¯ï¼Œæé«˜äº†åœºæ™¯é‡å»ºçš„ç²¾åº¦å’Œæ•ˆç‡ã€‚</li>
<li>æå‡ºäº†ä¸€ç§æ–°å‹çš„ä¸‰å¹³é¢ç½‘æ ¼è”åˆåœºæ™¯è¡¨ç¤ºæ–¹æ³•ï¼Œæ”¹è¿›äº†åœºæ™¯ç»†èŠ‚å’Œè¿è´¯æ€§çš„è¡¨ç°ã€‚</li>
<li>è®¾è®¡äº†å†…å¤–ç¯é—­åˆæ–¹æ³•ï¼Œå®ç°äº†å±€éƒ¨å’Œå…¨å±€çš„ä¸€è‡´æ€§ï¼Œæé«˜äº†æ™ºèƒ½ä½“ä¹‹é—´çš„åä½œæ•ˆç‡ã€‚</li>
<li>æ¨å‡ºäº†é¦–ä¸ªå¯†é›†slamï¼ˆDESï¼‰æ•°æ®é›†ï¼ŒåŒ…å«å¤šç§åœºæ™¯ï¼Œä»å°å‹æˆ¿é—´åˆ°å¤§å‹å®¤å¤–åœºæ™¯ï¼ŒåŒæ—¶æä¾›é«˜ç²¾åº¦çš„ä¸‰ç»´ç½‘æ ¼å’Œè¿ç»­æ—¶é—´ç›¸æœºè½¨è¿¹çš„çœŸå®æ•°æ®ã€‚</li>
<li>DESæ•°æ®é›†èƒ½æ¨åŠ¨SLAMã€ä¸‰ç»´é‡å»ºå’Œè§†è§‰åŸºç¡€æ¨¡å‹çš„ç ”ç©¶å‘å±•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.18678">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-52a3e1d1cc6e0225e987d15c27bb62e7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-56ca28009d5260476c56072194780a5b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-695d1a91a966a1f33935ee410979fcae.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e62f2974cdf37c29051ef447e844c4db.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Multi-Agent-Reinforcement-Learning-for-Inverse-Design-in-Photonic-Integrated-Circuits"><a href="#Multi-Agent-Reinforcement-Learning-for-Inverse-Design-in-Photonic-Integrated-Circuits" class="headerlink" title="Multi-Agent Reinforcement Learning for Inverse Design in Photonic   Integrated Circuits"></a>Multi-Agent Reinforcement Learning for Inverse Design in Photonic   Integrated Circuits</h2><p><strong>Authors:Yannik Mahlau, Maximilian Schier, Christoph Reinders, Frederik Schubert, Marco BÃ¼gling, Bodo Rosenhahn</strong></p>
<p>Inverse design of photonic integrated circuits (PICs) has traditionally relied on gradientbased optimization. However, this approach is prone to end up in local minima, which results in suboptimal design functionality. As interest in PICs increases due to their potential for addressing modern hardware demands through optical computing, more adaptive optimization algorithms are needed. We present a reinforcement learning (RL) environment as well as multi-agent RL algorithms for the design of PICs. By discretizing the design space into a grid, we formulate the design task as an optimization problem with thousands of binary variables. We consider multiple two- and three-dimensional design tasks that represent PIC components for an optical computing system. By decomposing the design space into thousands of individual agents, our algorithms are able to optimize designs with only a few thousand environment samples. They outperform previous state-of-the-art gradient-based optimization in both twoand three-dimensional design tasks. Our work may also serve as a benchmark for further exploration of sample-efficient RL for inverse design in photonics. </p>
<blockquote>
<p>å…‰å­é›†æˆç”µè·¯ï¼ˆPICï¼‰çš„é€†å‘è®¾è®¡ä¼ ç»Ÿä¸Šä¾èµ–äºåŸºäºæ¢¯åº¦çš„ä¼˜åŒ–ã€‚ç„¶è€Œï¼Œè¿™ç§æ–¹æ³•å®¹æ˜“é™·å…¥å±€éƒ¨æœ€å°å€¼ï¼Œå¯¼è‡´è®¾è®¡åŠŸèƒ½ä¸ä½³ã€‚éšç€å¯¹å…‰å­é›†æˆç”µè·¯è§£å†³ç°ä»£ç¡¬ä»¶éœ€æ±‚æ½œåŠ›çš„å…´è¶£å¢åŠ ï¼Œéœ€è¦é€šè¿‡å…‰å­¦è®¡ç®—æ¥å®ç°æ›´å¤šè‡ªé€‚åº”ä¼˜åŒ–ç®—æ³•çš„éœ€æ±‚ä¹Ÿåœ¨å¢åŠ ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰ç¯å¢ƒä»¥åŠç”¨äºå…‰å­é›†æˆç”µè·¯è®¾è®¡çš„å¤šæ™ºèƒ½ä½“å¼ºåŒ–å­¦ä¹ ç®—æ³•ã€‚é€šè¿‡å°†è®¾è®¡ç©ºé—´ç¦»æ•£åŒ–ä¸ºç½‘æ ¼ï¼Œæˆ‘ä»¬å°†è®¾è®¡ä»»åŠ¡åˆ¶å®šä¸ºä¸€ä¸ªå…·æœ‰æ•°åƒä¸ªäºŒè¿›åˆ¶å˜é‡çš„ä¼˜åŒ–é—®é¢˜ã€‚æˆ‘ä»¬è€ƒè™‘å¤šä¸ªä»£è¡¨å…‰å­¦è®¡ç®—ç³»ç»Ÿå…‰å­é›†æˆç”µè·¯ç»„ä»¶çš„äºŒç»´å’Œä¸‰ç»´è®¾è®¡ä»»åŠ¡ã€‚é€šè¿‡å°†è®¾è®¡ç©ºé—´åˆ†è§£ä¸ºæ•°åƒä¸ªç‹¬ç«‹æ™ºèƒ½ä½“ï¼Œæˆ‘ä»¬çš„ç®—æ³•èƒ½å¤Ÿåœ¨ä»…ä½¿ç”¨æ•°åƒä¸ªç¯å¢ƒæ ·æœ¬çš„æƒ…å†µä¸‹ä¼˜åŒ–è®¾è®¡ã€‚å®ƒä»¬åœ¨äºŒç»´å’Œä¸‰ç»´è®¾è®¡ä»»åŠ¡ä¸­çš„è¡¨ç°å‡ä¼˜äºå½“å‰æœ€å…ˆè¿›çš„åŸºäºæ¢¯åº¦çš„ä¼˜åŒ–æ–¹æ³•ã€‚æˆ‘ä»¬çš„å·¥ä½œä¹Ÿå¯ä»¥ä½œä¸ºè¿›ä¸€æ­¥æ¢ç´¢å…‰å­å­¦é€†å‘è®¾è®¡çš„æ ·æœ¬é«˜æ•ˆå¼ºåŒ–å­¦ä¹ çš„åŸºå‡†ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.18627v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>é€†å‘è®¾è®¡å…‰å­é›†æˆç”µè·¯ï¼ˆPICsï¼‰çš„ä¼ ç»Ÿæ–¹æ³•ä¸»è¦ä¾èµ–äºåŸºäºæ¢¯åº¦çš„ä¼˜åŒ–ã€‚ç„¶è€Œï¼Œè¿™ç§æ–¹æ³•å®¹æ˜“é™·å…¥å±€éƒ¨æœ€å°å€¼ï¼Œå¯¼è‡´è®¾è®¡åŠŸèƒ½æ¬¡ä¼˜ã€‚éšç€å¯¹PICså…´è¶£çš„å¢åŠ ï¼Œä¸ºé€‚åº”ç°ä»£ç¡¬ä»¶éœ€æ±‚å¹¶åº”ç”¨äºå…‰å­¦è®¡ç®—ï¼Œéœ€è¦æ›´å¤šé€‚åº”æ€§ä¼˜åŒ–ç®—æ³•ã€‚æœ¬ç ”ç©¶æå‡ºäº†ç”¨äºPICè®¾è®¡çš„å¼ºåŒ–å­¦ä¹ ç¯å¢ƒåŠå¤šæ™ºèƒ½ä½“å¼ºåŒ–å­¦ä¹ ç®—æ³•ã€‚é€šè¿‡å°†è®¾è®¡ç©ºé—´ç¦»æ•£åŒ–ä¸ºç½‘æ ¼ï¼Œæˆ‘ä»¬å°†è®¾è®¡ä»»åŠ¡å…¬å¼åŒ–ä¸ºå…·æœ‰æ•°åƒä¸ªäºŒè¿›åˆ¶å˜é‡çš„ä¼˜åŒ–é—®é¢˜ã€‚ç ”ç©¶è€ƒè™‘äº†ä»£è¡¨å…‰å­¦è®¡ç®—ç³»ç»Ÿç»„ä»¶çš„å¤šä¸ªäºŒç»´å’Œä¸‰ç»´è®¾è®¡ä»»åŠ¡ã€‚é€šè¿‡åˆ†è§£è®¾è®¡ç©ºé—´ä¸ºæ•°åƒä¸ªç‹¬ç«‹æ™ºèƒ½ä½“ï¼Œæˆ‘ä»¬çš„ç®—æ³•èƒ½å¤Ÿåœ¨æ•°åƒä¸ªç¯å¢ƒæ ·æœ¬ä¸­ä¼˜åŒ–è®¾è®¡ã€‚å®ƒä»¬åœ¨äºŒç»´å’Œä¸‰ç»´è®¾è®¡ä»»åŠ¡ä¸­çš„è¡¨ç°å‡ä¼˜äºå…ˆå‰çš„æœ€å…ˆè¿›çš„åŸºäºæ¢¯åº¦çš„ä¼˜åŒ–ã€‚æœ¬ç ”ç©¶ä¹Ÿå¯ä¸ºæ ·æœ¬é«˜æ•ˆçš„å¼ºåŒ–å­¦ä¹ åœ¨å…‰å­å­¦é€†å‘è®¾è®¡æ–¹é¢çš„è¿›ä¸€æ­¥æ¢ç´¢æä¾›å‚è€ƒã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ä¼ ç»Ÿå…‰å­é›†æˆç”µè·¯çš„é€†å‘è®¾è®¡ä¸»è¦ä¾èµ–åŸºäºæ¢¯åº¦çš„ä¼˜åŒ–æ–¹æ³•ï¼Œä½†æ˜“é™·å…¥å±€éƒ¨æœ€å°å€¼ã€‚</li>
<li>éšç€å…‰å­¦è®¡ç®—çš„éœ€æ±‚å¢é•¿ï¼Œéœ€è¦æ›´çµæ´»çš„ä¼˜åŒ–ç®—æ³•æ¥è®¾è®¡å…‰å­é›†æˆç”µè·¯ã€‚</li>
<li>ç ”ç©¶äººå‘˜æå‡ºäº†ä½¿ç”¨å¼ºåŒ–å­¦ä¹ ç¯å¢ƒåŠå¤šæ™ºèƒ½ä½“å¼ºåŒ–å­¦ä¹ ç®—æ³•è¿›è¡ŒPICè®¾è®¡çš„æ–°æ–¹æ³•ã€‚</li>
<li>è®¾è®¡ç©ºé—´è¢«ç¦»æ•£åŒ–ä¸ºç½‘æ ¼ï¼Œå°†è®¾è®¡ä»»åŠ¡è½¬åŒ–ä¸ºå…·æœ‰æ•°åƒä¸ªäºŒè¿›åˆ¶å˜é‡çš„ä¼˜åŒ–é—®é¢˜ã€‚</li>
<li>è¯¥æ–¹æ³•é€‚ç”¨äºå¤šç§äºŒç»´å’Œä¸‰ç»´è®¾è®¡ä»»åŠ¡ï¼Œä»£è¡¨å…‰å­¦è®¡ç®—ç³»ç»Ÿçš„ç»„ä»¶ã€‚</li>
<li>é€šè¿‡åˆ†è§£è®¾è®¡ç©ºé—´ä¸ºç‹¬ç«‹æ™ºèƒ½ä½“ï¼Œè¯¥æ–¹æ³•èƒ½åœ¨å°‘é‡ç¯å¢ƒæ ·æœ¬ä¸­ä¼˜åŒ–è®¾è®¡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.18627">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-e7db4db2f91eb04ee84668762994a96e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c67fcbc2599b70b21b7eaec2dc3d84b1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2665c8f584c8affb91f134132594c023.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Transformer-World-Model-for-Sample-Efficient-Multi-Agent-Reinforcement-Learning"><a href="#Transformer-World-Model-for-Sample-Efficient-Multi-Agent-Reinforcement-Learning" class="headerlink" title="Transformer World Model for Sample Efficient Multi-Agent Reinforcement   Learning"></a>Transformer World Model for Sample Efficient Multi-Agent Reinforcement   Learning</h2><p><strong>Authors:Azad Deihim, Eduardo Alonso, Dimitra Apostolopoulou</strong></p>
<p>We present the Multi-Agent Transformer World Model (MATWM), a novel transformer-based world model designed for multi-agent reinforcement learning in both vector- and image-based environments. MATWM combines a decentralized imagination framework with a semi-centralized critic and a teammate prediction module, enabling agents to model and anticipate the behavior of others under partial observability. To address non-stationarity, we incorporate a prioritized replay mechanism that trains the world model on recent experiences, allowing it to adapt to agentsâ€™ evolving policies. We evaluated MATWM on a broad suite of benchmarks, including the StarCraft Multi-Agent Challenge, PettingZoo, and MeltingPot. MATWM achieves state-of-the-art performance, outperforming both model-free and prior world model approaches, while demonstrating strong sample efficiency, achieving near-optimal performance in as few as 50K environment interactions. Ablation studies confirm the impact of each component, with substantial gains in coordination-heavy tasks. </p>
<blockquote>
<p>æˆ‘ä»¬æå‡ºäº†å¤šæ™ºèƒ½ä½“Transformerä¸–ç•Œæ¨¡å‹ï¼ˆMATWMï¼‰ï¼Œè¿™æ˜¯ä¸€ç§åŸºäºtransformerçš„æ–°å‹ä¸–ç•Œæ¨¡å‹ï¼Œæ—¨åœ¨æ”¯æŒå‘é‡å’Œå›¾åƒç¯å¢ƒä¸­çš„å¤šæ™ºèƒ½ä½“å¼ºåŒ–å­¦ä¹ ã€‚MATWMç»“åˆäº†åˆ†æ•£å¼æƒ³è±¡æ¡†æ¶ã€åŠé›†ä¸­å¼æ‰¹è¯„å®¶å’Œé˜Ÿå‹é¢„æµ‹æ¨¡å—ï¼Œä½¿æ™ºèƒ½ä½“èƒ½å¤Ÿåœ¨éƒ¨åˆ†å¯è§‚å¯Ÿçš„æƒ…å†µä¸‹å»ºç«‹ä»–äººè¡Œä¸ºçš„æ¨¡å‹å¹¶è¿›è¡Œé¢„æµ‹ã€‚ä¸ºäº†è§£å†³éå¹³ç¨³æ€§é—®é¢˜ï¼Œæˆ‘ä»¬é‡‡ç”¨äº†ä¼˜å…ˆçº§å›æ”¾æœºåˆ¶ï¼Œè¯¥æœºåˆ¶é€šè¿‡æœ€è¿‘çš„ç»éªŒè®­ç»ƒä¸–ç•Œæ¨¡å‹ï¼Œä½¿å…¶èƒ½å¤Ÿé€‚åº”æ™ºèƒ½ä½“ä¸æ–­å˜åŒ–çš„ç­–ç•¥ã€‚æˆ‘ä»¬åœ¨ä¸€ç³»åˆ—åŸºå‡†æµ‹è¯•ä¸Šå¯¹MATWMè¿›è¡Œäº†è¯„ä¼°ï¼ŒåŒ…æ‹¬æ˜Ÿé™…äº‰éœ¸å¤šæ™ºèƒ½ä½“æŒ‘æˆ˜ã€å® ç‰©åŠ¨ç‰©å›­å’Œç†”ç‚‰ã€‚MATWMè¾¾åˆ°äº†æœ€å…ˆè¿›çš„æŠ€æœ¯æ€§èƒ½æ°´å¹³ï¼Œä¸ä»…è¶…è¶Šäº†æ— æ¨¡å‹å’Œé¢„å…ˆçš„ä¸–ç•Œæ¨¡å‹æ–¹æ³•ï¼Œè€Œä¸”è¡¨ç°å‡ºå¼ºå¤§çš„æ ·æœ¬æ•ˆç‡ï¼Œåœ¨ä»…5ä¸‡æ¬¡çš„ç¯å¢ƒäº¤äº’ä¸­å°±èƒ½è¾¾åˆ°æ¥è¿‘æœ€ä¼˜çš„æ€§èƒ½ã€‚æ¶ˆèç ”ç©¶è¯å®äº†æ¯ä¸ªç»„ä»¶çš„å½±å“ï¼Œåœ¨åè°ƒå¯†é›†çš„ä»»åŠ¡ä¸­å–å¾—äº†é‡å¤§è¿›å±•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.18537v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>åŸºäºTransformerçš„ä¸–ç•Œæ¨¡å‹åœ¨å‘é‡å’Œå›¾åƒç¯å¢ƒä¸­çš„å¤šæ™ºèƒ½ä½“å¼ºåŒ–å­¦ä¹ åº”ç”¨æ‘˜è¦ã€‚æå‡ºä¸€ç§æ–°å‹çš„å¤šæ™ºèƒ½ä½“Transformerä¸–ç•Œæ¨¡å‹ï¼ˆMATWMï¼‰ï¼Œç»“åˆäº†åˆ†æ•£å¼æƒ³è±¡æ¡†æ¶ã€åŠé›†ä¸­å¼æ‰¹è¯„å®¶å’Œé˜Ÿå‹é¢„æµ‹æ¨¡å—ï¼Œä»¥åœ¨éƒ¨åˆ†å¯è§‚å¯Ÿçš„æƒ…å†µä¸‹å»ºæ¨¡å’Œé¢„æµ‹å…¶ä»–æ™ºèƒ½ä½“çš„è¡Œä¸ºã€‚é‡‡ç”¨ä¼˜å…ˆå›æ”¾æœºåˆ¶åº”å¯¹éå¹³ç¨³æ€§é—®é¢˜ï¼Œæ ¹æ®æœ€è¿‘ç»éªŒè®­ç»ƒä¸–ç•Œæ¨¡å‹ä»¥é€‚åº”æ™ºèƒ½ä½“ç­–ç•¥çš„å˜åŒ–ã€‚åœ¨StarCraftå¤šæ™ºèƒ½ä½“æŒ‘æˆ˜ã€PettingZooå’ŒMeltingPotç­‰å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œå®ç°äº†ä¼˜äºæ¨¡å‹å…è´¹å’Œå…ˆå‰ä¸–ç•Œæ¨¡å‹çš„æ–¹æ³•çš„å…ˆè¿›æ€§èƒ½ï¼Œå¹¶è¡¨ç°å‡ºäº†å¾ˆå¼ºçš„æ ·æœ¬æ•ˆç‡ï¼Œä»…åœ¨æå°‘æ•°ç¯å¢ƒäº¤äº’ï¼ˆä¾‹å¦‚ä¸åˆ°äº”ä¸‡æ¬¡ï¼‰ä¸­å®ç°è¿‘ä¼˜æ€§èƒ½ã€‚å®éªŒæ˜¾ç¤ºMATWMåœ¨å¤šä»»åŠ¡åè°ƒä¸­å‘æŒ¥å·¨å¤§ä¼˜åŠ¿ã€‚ </p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>MATWMæ˜¯ä¸€ç§åŸºäºTransformerçš„å¤šæ™ºèƒ½ä½“å¼ºåŒ–å­¦ä¹ ä¸–ç•Œæ¨¡å‹ã€‚å®ƒå°†ç”¨äºè§£å†³åŸºäºå‘é‡å’Œå›¾åƒç¯å¢ƒä¸­çš„æ™ºèƒ½ä½“åä½œä»»åŠ¡ã€‚æ­¤æ¨¡å‹å¯æ¨¡æ‹Ÿç¯å¢ƒå¹¶è¿›è¡Œè‡ªé€‚åº”è®­ç»ƒï¼Œä»¥æ»¡è¶³å¿«é€Ÿå˜åŒ–çš„æƒ…å¢ƒå’Œæ–°çš„æŒ‘æˆ˜ã€‚å®ƒç»“åˆäº†å¤šç§æŠ€æœ¯æ¥å¢å¼ºæ€§èƒ½ï¼ŒåŒ…æ‹¬åˆ†æ•£å¼æƒ³è±¡æ¡†æ¶ã€åŠé›†ä¸­å¼æ‰¹è¯„å®¶å’Œé˜Ÿå‹é¢„æµ‹æ¨¡å—ã€‚è¿™äº›æŠ€æœ¯ä½¿å¾—æ™ºèƒ½ä½“èƒ½å¤Ÿåœ¨æœ‰é™çš„è§†é‡ä¸­äº†è§£å’Œç†è§£é˜Ÿå‹è¡Œä¸ºåŠè¡Œä¸ºåŠ¨æœºï¼Œä»è€Œä¿ƒè¿›å¤šæ™ºèƒ½ä½“çš„åè°ƒæ€§å’Œäº¤äº’èƒ½åŠ›ã€‚å¦å¤–å…¶æ”¹è¿›è®­ç»ƒç®—æ³•çš„æ€§èƒ½ä¿è¯æ­¤æ¨¡å‹çš„ä¼˜ç§€è¡¨ç°å¾—ç›Šäºä¼˜å…ˆå›æ”¾æœºåˆ¶çš„åº”ç”¨ï¼Œå®ƒä½¿æ¨¡å‹èƒ½å¤Ÿåº”å¯¹éå¹³ç¨³æ€§é—®é¢˜å¹¶é€‚åº”æ™ºèƒ½ä½“çš„ç­–ç•¥å˜åŒ–ã€‚å…¶åœ¨å¤šç§ä»»åŠ¡æŒ‘æˆ˜ä¸Šå±•ç°å‡ºæé«˜çš„æ€§èƒ½å’Œå‡ºè‰²çš„æ ·æœ¬æ•ˆç‡è¡¨ç°çªå‡ºçš„æ˜¯å…¶å…·å¤‡åœ¨ä¸åŒä»»åŠ¡ä¸Šçš„å¿«é€Ÿå­¦ä¹ å’Œé«˜æ•ˆé€‚åº”èƒ½åŠ›ï¼ŒåŒæ—¶åœ¨åè°ƒå¤æ‚çš„ä»»åŠ¡ä¸­å…·æœ‰è¾ƒå¤§ä¼˜åŠ¿ï¼Œæœ‰æ•ˆæ¨è¿›äº†å¯¹æ–°å‹æ¨¡å‹çš„å‘å±•å’ŒæŒ‘æˆ˜çš„æ€åº¦åœ¨ä¸æ–­åŠ å‰§çš„ä¸–ç•Œè¦æ±‚æ›´ä¸ºç²¾ç»†çš„ä»»åŠ¡æ§åˆ¶æŠ€å·§çš„é—®é¢˜ï¼Œå¹¶å–å¾—ä¸€ç³»åˆ—å…ˆè¿›çš„æµ‹è¯•è¯„ä»·è¡¨ç°ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.18537">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-f5309ebe745518542de8953088995809.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-7bf9d9562fcd5c2b80d30964bf8d8c51.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="GraspMAS-Zero-Shot-Language-driven-Grasp-Detection-with-Multi-Agent-System"><a href="#GraspMAS-Zero-Shot-Language-driven-Grasp-Detection-with-Multi-Agent-System" class="headerlink" title="GraspMAS: Zero-Shot Language-driven Grasp Detection with Multi-Agent   System"></a>GraspMAS: Zero-Shot Language-driven Grasp Detection with Multi-Agent   System</h2><p><strong>Authors:Quang Nguyen, Tri Le, Huy Nguyen, Thieu Vo, Tung D. Ta, Baoru Huang, Minh N. Vu, Anh Nguyen</strong></p>
<p>Language-driven grasp detection has the potential to revolutionize human-robot interaction by allowing robots to understand and execute grasping tasks based on natural language commands. However, existing approaches face two key challenges. First, they often struggle to interpret complex text instructions or operate ineffectively in densely cluttered environments. Second, most methods require a training or finetuning step to adapt to new domains, limiting their generation in real-world applications. In this paper, we introduce GraspMAS, a new multi-agent system framework for language-driven grasp detection. GraspMAS is designed to reason through ambiguities and improve decision-making in real-world scenarios. Our framework consists of three specialized agents: Planner, responsible for strategizing complex queries; Coder, which generates and executes source code; and Observer, which evaluates the outcomes and provides feedback. Intensive experiments on two large-scale datasets demonstrate that our GraspMAS significantly outperforms existing baselines. Additionally, robot experiments conducted in both simulation and real-world settings further validate the effectiveness of our approach. </p>
<blockquote>
<p>è¯­è¨€é©±åŠ¨æŠ“å–æ£€æµ‹å…·æœ‰é€šè¿‡å…è®¸æœºå™¨äººç†è§£å’Œæ‰§è¡ŒåŸºäºè‡ªç„¶è¯­è¨€å‘½ä»¤çš„æŠ“å–ä»»åŠ¡æ¥é©æ–°äººæœºäº¤äº’çš„æ½œåŠ›ã€‚ç„¶è€Œï¼Œç°æœ‰æ–¹æ³•é¢ä¸´ä¸¤å¤§æŒ‘æˆ˜ã€‚é¦–å…ˆï¼Œå®ƒä»¬ç»å¸¸éš¾ä»¥è§£é‡Šå¤æ‚çš„æ–‡æœ¬æŒ‡ä»¤æˆ–åœ¨å¯†é›†æ‚ä¹±çš„ç¯å¢ƒä¸­æ“ä½œæ— æ•ˆã€‚å…¶æ¬¡ï¼Œå¤§å¤šæ•°æ–¹æ³•éœ€è¦ä¸€ä¸ªè®­ç»ƒæˆ–å¾®è°ƒæ­¥éª¤æ¥é€‚åº”æ–°é¢†åŸŸï¼Œè¿™é™åˆ¶äº†å®ƒä»¬åœ¨ç°å®ä¸–ç•Œåº”ç”¨ä¸­çš„ç”Ÿæˆã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†ç”¨äºè¯­è¨€é©±åŠ¨æŠ“å–æ£€æµ‹çš„æ–°å‹å¤šæ™ºèƒ½ä½“ç³»ç»Ÿæ¡†æ¶GraspMASã€‚GraspMASè¢«è®¾è®¡ç”¨äºè§£å†³æ­§ä¹‰å¹¶æ”¹å–„ç°å®åœºæ™¯ä¸­çš„å†³ç­–åˆ¶å®šã€‚æˆ‘ä»¬çš„æ¡†æ¶åŒ…æ‹¬ä¸‰ä¸ªä¸“ä¸šæ™ºèƒ½ä½“ï¼šPlannerè´Ÿè´£ç­–åˆ’å¤æ‚æŸ¥è¯¢ï¼›Coderè´Ÿè´£ç”Ÿæˆå¹¶æ‰§è¡Œæºä»£ç ï¼›Observerè´Ÿè´£è¯„ä¼°ç»“æœå¹¶æä¾›åé¦ˆã€‚åœ¨ä¸¤é¡¹å¤§è§„æ¨¡æ•°æ®é›†ä¸Šçš„å¯†é›†å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„GraspMASæ˜¾è‘—ä¼˜äºç°æœ‰åŸºçº¿ã€‚æ­¤å¤–ï¼Œåœ¨æ¨¡æ‹Ÿå’ŒçœŸå®ç¯å¢ƒä¸­è¿›è¡Œçš„æœºå™¨äººå®éªŒè¿›ä¸€æ­¥éªŒè¯äº†æˆ‘ä»¬çš„æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.18448v1">PDF</a> 8 pages, accepted to IROS 2025</p>
<p><strong>Summary</strong></p>
<p>è¯­è¨€é©±åŠ¨æŠ“å–æ£€æµ‹æœ‰æ½œåŠ›é©æ–°äººæœºäº¤äº’æ–¹å¼ï¼Œè®©æœºå™¨äººé€šè¿‡è‡ªç„¶è¯­è¨€å‘½ä»¤ç†è§£å’Œæ‰§è¡ŒæŠ“å–ä»»åŠ¡ã€‚ç°æœ‰æ–¹æ³•é¢ä¸´ä¸¤å¤§æŒ‘æˆ˜ï¼šä¸€æ˜¯éš¾ä»¥è§£è¯»å¤æ‚æ–‡æœ¬æŒ‡ä»¤æˆ–åœ¨å¯†é›†æ‚ä¹±ç¯å¢ƒä¸­æ“ä½œï¼›äºŒæ˜¯å¤šæ•°æ–¹æ³•éœ€è¦è®­ç»ƒæˆ–å¾®è°ƒä»¥é€‚åº”æ–°é¢†åŸŸï¼Œé™åˆ¶äº†å…¶åœ¨ç°å®ä¸–ç•Œçš„å¹¿æ³›åº”ç”¨ã€‚æœ¬æ–‡ä»‹ç»GraspMASï¼Œä¸€ç§ç”¨äºè¯­è¨€é©±åŠ¨æŠ“å–æ£€æµ‹çš„æ–°å¤šæ™ºèƒ½ä½“ç³»ç»Ÿæ¡†æ¶ã€‚GraspMASæ—¨åœ¨è§£å†³æ­§ä¹‰é—®é¢˜ï¼Œæé«˜ç°å®åœºæ™¯ä¸­çš„å†³ç­–èƒ½åŠ›ã€‚æ¡†æ¶åŒ…å«ä¸‰ä¸ªä¸“ä¸šæ™ºèƒ½ä½“ï¼šè§„åˆ’è€…è´Ÿè´£ç­–åˆ’å¤æ‚æŸ¥è¯¢ï¼›ç¼–ç è€…è´Ÿè´£ç”Ÿæˆå¹¶æ‰§è¡Œæºä»£ç ï¼›è§‚å¯Ÿè€…è´Ÿè´£è¯„ä¼°ç»“æœå¹¶æä¾›åé¦ˆã€‚å¤§è§„æ¨¡æ•°æ®é›†ä¸Šçš„å®éªŒæ˜¾ç¤ºï¼ŒGraspMASæ˜¾è‘—ä¼˜äºç°æœ‰åŸºçº¿æ–¹æ³•ã€‚åœ¨æ¨¡æ‹Ÿå’Œå®é™…æœºå™¨äººå®éªŒä¸­çš„éªŒè¯ä¹Ÿè¿›ä¸€æ­¥è¯æ˜äº†å…¶æœ‰æ•ˆæ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è¯­è¨€é©±åŠ¨æŠ“å–æ£€æµ‹æœ‰æ½œåŠ›é©æ–°äººæœºäº¤äº’æ–¹å¼ã€‚</li>
<li>å½“å‰æ–¹æ³•é¢ä¸´è§£è¯»å¤æ‚æ–‡æœ¬æŒ‡ä»¤å’Œæ‚ä¹±ç¯å¢ƒæ“ä½œä¸¤å¤§æŒ‘æˆ˜ã€‚</li>
<li>GraspMASæ˜¯ä¸€ç§å¤šæ™ºèƒ½ä½“ç³»ç»Ÿæ¡†æ¶ï¼Œç”¨äºè¯­è¨€é©±åŠ¨æŠ“å–æ£€æµ‹ã€‚</li>
<li>GraspMASæ—¨åœ¨è§£å†³æ­§ä¹‰é—®é¢˜å¹¶æé«˜ç°å®åœºæ™¯ä¸­çš„å†³ç­–èƒ½åŠ›ã€‚</li>
<li>GraspMASåŒ…å«è§„åˆ’è€…ã€ç¼–ç è€…å’Œè§‚å¯Ÿè€…ä¸‰ä¸ªä¸“ä¸šæ™ºèƒ½ä½“ã€‚</li>
<li>å®éªŒè¯æ˜GraspMASåœ¨å¤šä¸ªæ•°æ®é›†ä¸Šè¡¨ç°ä¼˜äºç°æœ‰æ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.18448">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-56881adc6768664e224466a13d891700.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a6141aeb608a04e9613d9f3e4c2d88bc.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c58ae9f5e95b57017ada52c3d10357ca.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-38bede20036d497d429f8530d3e1965a.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-e60f95d686d1a74571ce00a246f3e1d7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b67a20a589b3eb47404174b34f5c2eb1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c39a476990e4dd189f11e0b80a6ff320.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b46db713e70c08279333411f63ed8235.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Dynamic-Knowledge-Exchange-and-Dual-diversity-Review-Concisely-Unleashing-the-Potential-of-a-Multi-Agent-Research-Team"><a href="#Dynamic-Knowledge-Exchange-and-Dual-diversity-Review-Concisely-Unleashing-the-Potential-of-a-Multi-Agent-Research-Team" class="headerlink" title="Dynamic Knowledge Exchange and Dual-diversity Review: Concisely   Unleashing the Potential of a Multi-Agent Research Team"></a>Dynamic Knowledge Exchange and Dual-diversity Review: Concisely   Unleashing the Potential of a Multi-Agent Research Team</h2><p><strong>Authors:Weilun Yu, Shixiang Tang, Yonggui Huang, Nanqing Dong, Li Fan, Honggang Qi, Wei Liu, Xiaoli Diao, Xi Chen, Wanli Ouyang</strong></p>
<p>Scientific progress increasingly relies on effective collaboration among researchers, a dynamic that large language models (LLMs) have only begun to emulate. While recent LLM-based scientist agents show promise in autonomous scientific discovery, they often lack the interactive reasoning and evaluation mechanisms essential to real-world research. We propose IDVSCI (Internal Discussion and Vote SCIentists), a multi-agent framework built on LLMs that incorporates two key innovations: a Dynamic Knowledge Exchange mechanism enabling iterative feedback among agents, and a Dual-Diversity Review paradigm that simulates heterogeneous expert evaluation. These components jointly promote deeper reasoning and the generation of more creative and impactful scientific ideas. To evaluate the effectiveness and generalizability of our approach, we conduct experiments on two datasets: a widely used benchmark in computer science and a new dataset we introduce in the health sciences domain. Results show that IDVSCI consistently achieves the best performance across both datasets, outperforming existing systems such as AI Scientist and VIRSCI. These findings highlight the value of modeling interaction and peer review dynamics in LLM-based autonomous research. </p>
<blockquote>
<p>ç§‘æŠ€è¿›æ­¥è¶Šæ¥è¶Šä¾èµ–äºç ”ç©¶è€…ä¹‹é—´çš„æœ‰æ•ˆåä½œï¼Œè¿™ä¸€åŠ¨æ€æ˜¯å¤§è§„æ¨¡è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ‰å¼€å§‹æ¨¡ä»¿çš„ã€‚è™½ç„¶åŸºäºLLMçš„ç§‘å­¦å®¶ä»£ç†äººåœ¨è‡ªä¸»ç§‘å­¦å‘ç°æ–¹é¢æ˜¾ç¤ºå‡ºå‰æ™¯ï¼Œä½†å®ƒä»¬é€šå¸¸ç¼ºä¹ç°å®ä¸–ç•Œç ”ç©¶ä¸­å¿…ä¸å¯å°‘çš„äº¤äº’æ¨ç†å’Œè¯„ä¼°æœºåˆ¶ã€‚æˆ‘ä»¬æå‡ºäº†IDVSCIï¼ˆå†…éƒ¨è®¨è®ºä¸æŠ•ç¥¨ç§‘å­¦å®¶ï¼‰ï¼Œè¿™æ˜¯ä¸€ä¸ªåŸºäºLLMçš„å¤šä»£ç†æ¡†æ¶ï¼ŒåŒ…å«ä¸¤é¡¹å…³é”®åˆ›æ–°ï¼šåŠ¨æ€çŸ¥è¯†äº¤æ¢æœºåˆ¶ï¼Œä¿ƒè¿›ä»£ç†ä¹‹é—´çš„è¿­ä»£åé¦ˆï¼›æ¨¡æ‹Ÿå¼‚è´¨ä¸“å®¶è¯„ä¼°çš„åŒé‡å¤šæ ·æ€§å®¡æŸ¥èŒƒå¼ã€‚è¿™äº›ç»„ä»¶å…±åŒä¿ƒè¿›æ›´æ·±å±‚æ¬¡çš„æ¨ç†å’Œæ›´å…·åˆ›é€ åŠ›å’Œå½±å“åŠ›çš„ç§‘å­¦æƒ³æ³•çš„äº§ç”Ÿã€‚ä¸ºäº†è¯„ä¼°æˆ‘ä»¬æ–¹æ³•çš„æœ‰æ•ˆæ€§å’Œé€šç”¨æ€§ï¼Œæˆ‘ä»¬åœ¨ä¸¤ä¸ªæ•°æ®é›†ä¸Šè¿›è¡Œäº†å®éªŒï¼šä¸€ä¸ªåœ¨è®¡ç®—æœºç§‘å­¦ä¸­å¹¿æ³›ä½¿ç”¨çš„åŸºå‡†æµ‹è¯•å’Œæˆ‘ä»¬å¼•å…¥çš„ä¸€ä¸ªå¥åº·ç§‘å­¦é¢†åŸŸçš„æ–°æ•°æ®é›†ã€‚ç»“æœè¡¨æ˜ï¼ŒIDVSCIåœ¨ä¸¤ä¸ªæ•°æ®é›†ä¸Šå‡è¡¨ç°æœ€ä½³ï¼Œä¼˜äºç°æœ‰çš„AIç§‘å­¦å®¶å’ŒVIRSCIç³»ç»Ÿã€‚è¿™äº›å‘ç°çªæ˜¾äº†åœ¨åŸºäºLLMçš„è‡ªä¸»ç ”ç©¶ä¸­å»ºç«‹äº¤äº’å’ŒåŒè¡Œè¯„å®¡åŠ¨æ€çš„ä»·å€¼ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.18348v1">PDF</a> </p>
<p><strong>æ‘˜è¦</strong></p>
<p>å¤§è¯­è¨€æ¨¡å‹ (LLM) å·²å¼€å§‹æ¨¡æ‹Ÿç§‘å­¦å®¶é—´çš„åä½œæ–¹å¼ä»¥ä¿ƒè¿›ç§‘å­¦è¿›æ­¥ã€‚æ–°æå‡ºçš„åŸºäºLLMçš„ç§‘å­¦å®¶ä»£ç†ç³»ç»Ÿåœ¨è‡ªä¸»ç§‘å­¦å‘ç°æ–¹é¢æ˜¾ç¤ºå‡ºå‰æ™¯ï¼Œä½†å®ƒä»¬é€šå¸¸ç¼ºä¹é‡è¦çš„äº¤äº’æ¨ç†å’Œè¯„ä¼°æœºåˆ¶ã€‚æœ¬ç ”ç©¶æå‡ºIDVSCIï¼ˆå†…éƒ¨è®¨è®ºä¸æŠ•ç¥¨ç§‘å­¦å®¶ï¼‰å¤šä»£ç†æ¡†æ¶ï¼Œè¯¥æ¡†æ¶åœ¨LLMåŸºç¡€ä¸Šæ„å»ºï¼ŒåŒ…å«ä¸¤ä¸ªå…³é”®åˆ›æ–°ç‚¹ï¼šåŠ¨æ€çŸ¥è¯†äº¤æ¢æœºåˆ¶å¯å®ç°ä»£ç†é—´çš„è¿­ä»£åé¦ˆï¼Œä»¥åŠæ¨¡æ‹Ÿä¸åŒä¸“å®¶è¯„ä¼°çš„åŒå…ƒè¯„ä¼°å®¡æŸ¥æ¨¡å¼ã€‚è¿™äº›å…±åŒä¿ƒè¿›äº†æ›´æ·±å…¥çš„æ¨ç†å’Œæ›´å…·åˆ›é€ åŠ›å’Œå½±å“åŠ›çš„ç§‘å­¦æƒ³æ³•çš„äº§ç”Ÿã€‚é€šè¿‡è®¡ç®—æœºç§‘å­¦ä¸å«ç”Ÿç§‘å­¦é¢†åŸŸçš„æ•°æ®é›†è¿›è¡Œçš„å®éªŒè¡¨æ˜ï¼ŒIDVSCIçš„æ€§èƒ½è¡¨ç°ä¼˜äºç°æœ‰ç³»ç»Ÿå¦‚AIç§‘å­¦å®¶å’ŒVIRSCIï¼Œè¯æ˜äº†å»ºæ¨¡äº¤äº’å’ŒåŒè¡Œè¯„å®¡åŠ¨åŠ›å­¦åœ¨åŸºäºLLMçš„è‡ªä¸»ç ”ç©¶ä¸­çš„ä»·å€¼ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ul>
<li>ç§‘å­¦è¿›æ­¥æ„ˆå‘ä¾èµ–äºç ”ç©¶è€…é—´çš„æœ‰æ•ˆåä½œï¼Œå¤§è¯­è¨€æ¨¡å‹å¼€å§‹æ¨¡æ‹Ÿè¿™ç§åä½œæ–¹å¼ã€‚</li>
<li>åŸºäºLLMçš„ç§‘å­¦å®¶ä»£ç†ç³»ç»Ÿåœ¨è‡ªä¸»ç§‘å­¦å‘ç°æ–¹é¢å±•ç°æ½œåŠ›ï¼Œä½†éœ€åŠ å¼ºäº¤äº’æ¨ç†å’Œè¯„ä¼°æœºåˆ¶ã€‚</li>
<li>IDVSCIæ¡†æ¶å¼•å…¥åŠ¨æ€çŸ¥è¯†äº¤æ¢æœºåˆ¶å®ç°ä»£ç†é—´çš„è¿­ä»£åé¦ˆã€‚</li>
<li>IDVSCIé‡‡ç”¨åŒå…ƒè¯„ä¼°å®¡æŸ¥æ¨¡å¼ï¼Œæ¨¡æ‹Ÿå¼‚è´¨ä¸“å®¶è¯„ä¼°ã€‚</li>
<li>å®éªŒç»“æœæ˜¾ç¤ºIDVSCIåœ¨å¤šä¸ªæ•°æ®é›†ä¸Šçš„æ€§èƒ½ä¼˜äºç°æœ‰ç³»ç»Ÿã€‚</li>
<li>IDVSCIå»ºæ¨¡äº¤äº’å’ŒåŒè¡Œè¯„å®¡åŠ¨åŠ›å­¦å¯¹åŸºäºLLMçš„è‡ªä¸»ç ”ç©¶è‡³å…³é‡è¦ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.18348">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-d5ccd22af97efcbad1f4993104b64d33.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4e162a01693d49c0d688ca874d3cf114.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-efa41110fa85270a2f4182c508339648.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-493546ff1e561105c81320731215e461.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f5da51a400590bca905488fb58978ccd.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="From-RAG-to-Agentic-Validating-Islamic-Medicine-Responses-with-LLM-Agents"><a href="#From-RAG-to-Agentic-Validating-Islamic-Medicine-Responses-with-LLM-Agents" class="headerlink" title="From RAG to Agentic: Validating Islamic-Medicine Responses with LLM   Agents"></a>From RAG to Agentic: Validating Islamic-Medicine Responses with LLM   Agents</h2><p><strong>Authors:Mohammad Amaan Sayeed, Mohammed Talha Alam, Raza Imam, Shahab Saquib Sohail, Amir Hussain</strong></p>
<p>Centuries-old Islamic medical texts like Avicennaâ€™s Canon of Medicine and the Prophetic Tibb-e-Nabawi encode a wealth of preventive care, nutrition, and holistic therapies, yet remain inaccessible to many and underutilized in modern AI systems. Existing language-model benchmarks focus narrowly on factual recall or user preference, leaving a gap in validating culturally grounded medical guidance at scale. We propose a unified evaluation pipeline, Tibbe-AG, that aligns 30 carefully curated Prophetic-medicine questions with human-verified remedies and compares three LLMs (LLaMA-3, Mistral-7B, Qwen2-7B) under three configurations: direct generation, retrieval-augmented generation, and a scientific self-critique filter. Each answer is then assessed by a secondary LLM serving as an agentic judge, yielding a single 3C3H quality score. Retrieval improves factual accuracy by 13%, while the agentic prompt adds another 10% improvement through deeper mechanistic insight and safety considerations. Our results demonstrate that blending classical Islamic texts with retrieval and self-evaluation enables reliable, culturally sensitive medical question-answering. </p>
<blockquote>
<p>åƒé˜¿ç»´æ£®çº³çš„ã€ŠåŒ»å…¸ã€‹å’Œã€Šå…ˆçŸ¥åŒ»å­¦ã€‹è¿™æ ·çš„å…·æœ‰æ•°ç™¾å¹´å†å²çš„ä¼Šæ–¯å…°åŒ»å­¦æ–‡æœ¬ï¼Œè•´å«äº†ä¸°å¯Œçš„é¢„é˜²æŠ¤ç†ã€è¥å…»çŸ¥è¯†å’Œæ•´ä½“ç–—æ³•ï¼Œä½†å¯¹è®¸å¤šäººæ¥è¯´ä»ç„¶éš¾ä»¥æ¥è§¦å’Œåˆ©ç”¨ï¼Œåœ¨ç°ä»£äººå·¥æ™ºèƒ½ç³»ç»Ÿä¸­ä¹Ÿæ˜¯å¦‚æ­¤ã€‚ç°æœ‰çš„è¯­è¨€æ¨¡å‹åŸºå‡†æµ‹è¯•ä¸»è¦é›†ä¸­åœ¨äº‹å®å›å¿†æˆ–ç”¨æˆ·åå¥½ä¸Šï¼Œä»è€Œåœ¨éªŒè¯å¤§è§„æ¨¡æ–‡åŒ–åŸºç¡€çš„åŒ»å­¦æŒ‡å¯¼æ–¹é¢å­˜åœ¨å·®è·ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§ç»Ÿä¸€çš„è¯„ä¼°æµç¨‹Tibbe-AGï¼Œå®ƒå°†30ä¸ªç²¾å¿ƒç­–åˆ’çš„å…ˆçŸ¥åŒ»å­¦é—®é¢˜ä¸äººå·¥éªŒè¯çš„è¡¥æ•‘æªæ–½ç›¸ç»“åˆï¼Œå¹¶æ¯”è¾ƒäº†ä¸‰ç§å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLaMA-3ã€Mistral-7Bã€Qwen2-7Bï¼‰åœ¨ä¸‰ç§é…ç½®ä¸‹çš„è¡¨ç°ï¼šç›´æ¥ç”Ÿæˆã€æ£€ç´¢å¢å¼ºç”Ÿæˆå’Œç§‘å­¦è‡ªæˆ‘æ‰¹åˆ¤è¿‡æ»¤ã€‚æ¯ä¸ªç­”æ¡ˆç„¶åç”±ç¬¬äºŒä¸ªå¤§å‹è¯­è¨€æ¨¡å‹ä½œä¸ºè¯„åˆ¤è€…è¿›è¡Œè¯„ä¼°ï¼Œå¾—å‡ºä¸€ä¸ªå•ä¸€çš„3C3Hè´¨é‡åˆ†æ•°ã€‚æ£€ç´¢å¯ä»¥æé«˜äº‹å®å‡†ç¡®æ€§13%ï¼Œè€Œæ™ºèƒ½æç¤ºé€šè¿‡æ›´æ·±å…¥çš„æœºåˆ¶æ´å¯ŸåŠ›å’Œå®‰å…¨è€ƒè™‘å¢åŠ äº†é¢å¤–çš„10%çš„æ”¹è¿›ã€‚æˆ‘ä»¬çš„ç»“æœè¡¨æ˜ï¼Œå°†å¤å…¸ä¼Šæ–¯å…°æ–‡æœ¬ä¸æ£€ç´¢å’Œè‡ªæˆ‘è¯„ä¼°ç›¸ç»“åˆï¼Œå¯å®ç°å¯é ä¸”æ–‡åŒ–æ•æ„Ÿçš„åŒ»ç–—é—®é¢˜å›ç­”ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.15911v2">PDF</a> Published at the 4th Muslims in Machine Learning (MusIML) Workshop   (ICML-25)</p>
<p><strong>Summary</strong></p>
<p>è¿™ç¯‡æ–‡æœ¬è®²è¿°äº†ä¼Šæ–¯å…°åŒ»å­¦å¤ç±å¦‚é˜¿ç»´æ£®çº³çš„ã€ŠåŒ»å…¸ã€‹å’Œå…ˆçŸ¥åŒ»å­¦ä¸­çš„é¢„é˜²ä¿å¥ã€è¥å…»å’Œæ•´ä½“ç–—æ³•ç­‰å†…å®¹ï¼Œå¼ºè°ƒäº†è¿™äº›èµ„æºå¯¹ç°ä»£äººå·¥æ™ºèƒ½ç³»ç»Ÿçš„ä»·å€¼åŠå…¶å¯è®¿é—®æ€§çš„é‡è¦æ€§ã€‚æ–‡ç« æŒ‡å‡ºå½“å‰è¯­è¨€æ¨¡å‹è¯„ä¼°åŸºå‡†åœ¨äº‹å®æ£€ç´¢å’Œç”¨æˆ·åå¥½ä¸Šçš„å±€é™æ€§ï¼Œæå‡ºä¸€ä¸ªç»Ÿä¸€çš„è¯„ä¼°æµç¨‹Tibbe-AGï¼Œå°†å…ˆçŸ¥åŒ»å­¦é—®é¢˜ä¸äººå·¥éªŒè¯çš„ç–—æ³•ç›¸ç»“åˆï¼Œè¯„ä¼°äº†ä¸‰ä¸ªå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLaMA-3ã€Mistral-7Bå’ŒQwen2-7Bï¼‰çš„è¡¨ç°ã€‚é€šè¿‡å¢è®¾æ£€ç´¢å’Œè‡ªæˆ‘æ‰¹åˆ¤è¿‡æ»¤å™¨ï¼Œæé«˜ç­”æ¡ˆçš„äº‹å®å‡†ç¡®æ€§å’Œæ·±åº¦æœºåˆ¶æ´å¯ŸåŠ›ã€‚ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œç»“åˆå¤å…¸ä¼Šæ–¯å…°æ–‡æœ¬ã€æ£€ç´¢å’Œè‡ªæˆ‘è¯„ä¼°å¯å®ç°å¯é ã€æ–‡åŒ–æ•æ„Ÿçš„åŒ»ç–—é—®ç­”ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ä¼Šæ–¯å…°åŒ»å­¦å¤ç±åŒ…å«ä¸°å¯Œçš„é¢„é˜²ä¿å¥ã€è¥å…»å’Œæ•´ä½“ç–—æ³•å†…å®¹ï¼Œä½†å¯¹è®¸å¤šäººæ¥è¯´ä»ç„¶éš¾ä»¥è®¿é—®å¹¶ä¸”åœ¨ç°ä»£AIç³»ç»Ÿä¸­æœªå¾—åˆ°å……åˆ†åˆ©ç”¨ã€‚</li>
<li>å½“å‰è¯­è¨€æ¨¡å‹è¯„ä¼°åŸºå‡†ä¸»è¦å…³æ³¨äº‹å®æ£€ç´¢å’Œç”¨æˆ·åå¥½ï¼Œå¿½è§†äº†æ–‡åŒ–èƒŒæ™¯ä¸‹åŒ»ç–—æŒ‡å¯¼çš„éªŒè¯ã€‚</li>
<li>æå‡ºçš„Tibbe-AGè¯„ä¼°æµç¨‹å°†å…ˆçŸ¥åŒ»å­¦é—®é¢˜ä¸äººå·¥éªŒè¯çš„ç–—æ³•ç›¸ç»“åˆï¼Œè¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹çš„è¡¨ç°ã€‚</li>
<li>æ£€ç´¢åŠŸèƒ½æé«˜äº†ç­”æ¡ˆçš„äº‹å®å‡†ç¡®æ€§ã€‚</li>
<li>é€šè¿‡å¢è®¾è‡ªæˆ‘æ‰¹åˆ¤è¿‡æ»¤å™¨ï¼Œç­”æ¡ˆçš„æ´å¯ŸåŠ›å’Œå®‰å…¨æ€§å¾—åˆ°æé«˜ã€‚</li>
<li>ç»“åˆå¤å…¸ä¼Šæ–¯å…°æ–‡æœ¬ä¸è¯„ä¼°æµç¨‹å¯å®ç°æ–‡åŒ–æ•æ„Ÿçš„åŒ»ç–—é—®ç­”ç³»ç»Ÿã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.15911">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-91d0c311fc75eb9a8ef50d88c3c956b5.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d0e0251b3dadcaf0322e028f8b5b8f09.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ea3db167a7bb55b8f70b096014a38351.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b66b0e5d61030d5860c8b481b8ba5cfe.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-6e5d7c5f035f35f7ffd1f095d368ab5b.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="OAgents-An-Empirical-Study-of-Building-Effective-Agents"><a href="#OAgents-An-Empirical-Study-of-Building-Effective-Agents" class="headerlink" title="OAgents: An Empirical Study of Building Effective Agents"></a>OAgents: An Empirical Study of Building Effective Agents</h2><p><strong>Authors:He Zhu, Tianrui Qin, King Zhu, Heyuan Huang, Yeyi Guan, Jinxiang Xia, Yi Yao, Hanhao Li, Ningning Wang, Pai Liu, Tianhao Peng, Xin Gui, Xiaowan Li, Yuhui Liu, Yuchen Eleanor Jiang, Jun Wang, Changwang Zhang, Xiangru Tang, Ge Zhang, Jian Yang, Minghao Liu, Xitong Gao, Jiaheng Liu, Wangchunshu Zhou</strong></p>
<p>Recently, Agentic AI has become an increasingly popular research field. However, we argue that current agent research practices lack standardization and scientific rigor, making it hard to conduct fair comparisons among methods. As a result, it is still unclear how different design choices in agent frameworks affect effectiveness, and measuring their progress remains challenging. In this work, we conduct a systematic empirical study on GAIA benchmark and BrowseComp to examine the impact of popular design choices in key agent components in a fair and rigorous manner. We find that the lack of a standard evaluation protocol makes previous works, even open-sourced ones, non-reproducible, with significant variance between random runs. Therefore, we introduce a more robust evaluation protocol to stabilize comparisons. Our study reveals which components and designs are crucial for effective agents, while others are redundant, despite seeming logical. Based on our findings, we build and open-source OAgents, a new foundation agent framework that achieves state-of-the-art performance among open-source projects. OAgents offers a modular design for various agent components, promoting future research in Agentic AI. </p>
<blockquote>
<p>è¿‘æœŸï¼ŒAgentic AIå·²æˆä¸ºè¶Šæ¥è¶Šå—æ¬¢è¿çš„ç ”ç©¶é¢†åŸŸã€‚ç„¶è€Œï¼Œæˆ‘ä»¬è®¤ä¸ºå½“å‰çš„agentç ”ç©¶å®è·µç¼ºä¹æ ‡å‡†åŒ–å’Œç§‘å­¦ä¸¥è°¨æ€§ï¼Œä½¿å¾—å„ç§æ–¹æ³•ä¹‹é—´éš¾ä»¥è¿›è¡Œå…¬å¹³çš„æ¯”è¾ƒã€‚å› æ­¤ï¼Œå°šä¸æ¸…æ¥šagentæ¡†æ¶ä¸­çš„ä¸åŒè®¾è®¡é€‰æ‹©å¦‚ä½•å½±å“æ•ˆæœï¼Œè¡¡é‡å…¶è¿›å±•ä»ç„¶å…·æœ‰æŒ‘æˆ˜æ€§ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬å¯¹GAIAåŸºå‡†æµ‹è¯•å’ŒBrowseCompè¿›è¡Œäº†ç³»ç»Ÿçš„å®è¯ç ”ç©¶ï¼Œä»¥å…¬å¹³ä¸¥è°¨çš„æ–¹å¼æ£€æŸ¥äº†å…³é”®agentç»„ä»¶ä¸­æµè¡Œè®¾è®¡é€‰æ‹©çš„å½±å“ã€‚æˆ‘ä»¬å‘ç°ï¼Œç”±äºç¼ºä¹æ ‡å‡†çš„è¯„ä¼°åè®®ï¼Œä»¥å‰çš„å·¥ä½œï¼ˆå³ä½¿æ˜¯å¼€æºçš„ï¼‰ä¹Ÿæ— æ³•é‡ç°ï¼Œéšæœºè¿è¡Œä¹‹é—´å­˜æœ‰æ˜æ˜¾å·®å¼‚ã€‚å› æ­¤ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ªæ›´ç¨³å¥çš„è¯„ä¼°åè®®æ¥ç¨³å®šæ¯”è¾ƒã€‚æˆ‘ä»¬çš„ç ”ç©¶è¡¨æ˜ï¼Œå“ªäº›ç»„ä»¶å’Œè®¾è®¡å¯¹äºæœ‰æ•ˆçš„agentè‡³å…³é‡è¦ï¼Œè€Œå…¶ä»–ç»„ä»¶å°½ç®¡çœ‹ä¼¼é€»è¾‘ä¸Šåˆç†ï¼Œä½†å´æ˜¯å¤šä½™çš„ã€‚åŸºäºæˆ‘ä»¬çš„å‘ç°ï¼Œæˆ‘ä»¬æ„å»ºå¹¶å¼€æºäº†OAgentsï¼Œè¿™æ˜¯ä¸€ä¸ªæ–°çš„åŸºç¡€agentæ¡†æ¶ï¼Œåœ¨å¼€æºé¡¹ç›®ä¸­å®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚OAgentsä¸ºå„ç§agentç»„ä»¶æä¾›äº†æ¨¡å—åŒ–è®¾è®¡ï¼Œä¿ƒè¿›äº†Agentic AIçš„æœªæ¥ç ”ç©¶ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.15741v2">PDF</a> 28 pages</p>
<p><strong>Summary</strong></p>
<p>è¿‘æœŸï¼ŒAgentic AIæˆä¸ºçƒ­é—¨ç ”ç©¶é¢†åŸŸï¼Œä½†å½“å‰ç ”ç©¶å®è·µç¼ºä¹æ ‡å‡†åŒ–å’Œç§‘å­¦ä¸¥è°¨æ€§ï¼Œå¯¼è‡´æ–¹æ³•é—´éš¾ä»¥è¿›è¡Œå…¬å¹³æ¯”è¾ƒã€‚æœ¬ç ”ç©¶é€šè¿‡GAIAåŸºå‡†æµ‹è¯•å’ŒBrowseCompè¿›è¡Œå®è¯ç ”ç©¶ï¼Œæ¢è®¨å…³é”®ä»£ç†ç»„ä»¶ä¸­çš„æµè¡Œè®¾è®¡é€‰æ‹©çš„å½±å“ã€‚ç ”ç©¶å‘ç°ç¼ºä¹æ ‡å‡†è¯„ä¼°åè®®å¯¼è‡´å…ˆå‰å·¥ä½œä¸å¯å¤ç°ï¼Œå­˜åœ¨éšæœºè¿è¡Œä¹‹é—´çš„æ˜¾è‘—å·®å¼‚ã€‚å› æ­¤ï¼Œæˆ‘ä»¬å¼•å…¥æ›´ç¨³å¥çš„è¯„ä¼°åè®®ä»¥ç¨³å®šæ¯”è¾ƒã€‚ç ”ç©¶æ­ç¤ºäº†å“ªäº›ç»„ä»¶å’Œè®¾è®¡å¯¹äºæœ‰æ•ˆä»£ç†è‡³å…³é‡è¦ï¼Œå“ªäº›è™½ç„¶çœ‹ä¼¼åˆç†ä½†å´æ˜¯å†—ä½™çš„ã€‚åŸºäºæˆ‘ä»¬çš„å‘ç°ï¼Œæˆ‘ä»¬æ„å»ºå¹¶å¼€æºäº†OAgentsï¼Œä¸€ä¸ªå…¨æ–°çš„åŸºç¡€ä»£ç†æ¡†æ¶ï¼Œåœ¨å¼€æºé¡¹ç›®ä¸­å®ç°æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚OAgentsä¸ºå„ç§ä»£ç†ç»„ä»¶æä¾›æ¨¡å—åŒ–è®¾è®¡ï¼Œä¿ƒè¿›Agentic AIçš„æœªæ¥ç ”ç©¶ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å½“å‰Agentic AIç ”ç©¶é¢†åŸŸç¼ºä¹æ ‡å‡†åŒ–å’Œç§‘å­¦ä¸¥è°¨æ€§ï¼Œå¯¼è‡´æ–¹æ³•æ¯”è¾ƒå›°éš¾ã€‚</li>
<li>é€šè¿‡GAIAåŸºå‡†æµ‹è¯•å’ŒBrowseCompå®è¯ç ”ç©¶ï¼Œæ¢è®¨äº†å…³é”®ä»£ç†ç»„ä»¶çš„è®¾è®¡é€‰æ‹©çš„å½±å“ã€‚</li>
<li>ç¼ºä¹æ ‡å‡†è¯„ä¼°åè®®å¯¼è‡´å…ˆå‰çš„å·¥ä½œä¸å¯å¤ç°ï¼Œå­˜åœ¨éšæœºè¿è¡Œä¹‹é—´çš„æ˜¾è‘—å·®å¼‚ã€‚</li>
<li>å¼•å…¥æ›´ç¨³å¥çš„è¯„ä¼°åè®®ä»¥ç¨³å®šæ¯”è¾ƒä¸åŒçš„ä»£ç†è®¾è®¡ã€‚</li>
<li>ç ”ç©¶æ­ç¤ºäº†å“ªäº›ä»£ç†ç»„ä»¶å’Œè®¾è®¡è‡³å…³é‡è¦ï¼Œå“ªäº›è®¾è®¡æ˜¯å†—ä½™çš„ã€‚</li>
<li>æ„å»ºå¹¶å¼€æºäº†æ–°çš„åŸºç¡€ä»£ç†æ¡†æ¶OAgentsï¼Œå®ç°å¼€æºé¡¹ç›®ä¸­çš„æœ€å…ˆè¿›æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.15741">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-2b81b342c3259c3c571fa45a7fb2a3f5.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e4f5283171eea9d5000113a6400851bb.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-3382ea0e112d796d68fed973bc80fee6.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="SWE-Dev-Building-Software-Engineering-Agents-with-Training-and-Inference-Scaling"><a href="#SWE-Dev-Building-Software-Engineering-Agents-with-Training-and-Inference-Scaling" class="headerlink" title="SWE-Dev: Building Software Engineering Agents with Training and   Inference Scaling"></a>SWE-Dev: Building Software Engineering Agents with Training and   Inference Scaling</h2><p><strong>Authors:Haoran Wang, Zhenyu Hou, Yao Wei, Jie Tang, Yuxiao Dong</strong></p>
<p>Large language models (LLMs) have advanced rapidly from conversational problem solving to addressing real-world tasks involving tool use, such as software engineering (SWE). Recent LLM-powered toolkits, such as OpenAI Codex and Cursor, have offered end-to-end automation of the software development process. However, building effective SWE agents remains challenging due to the lack of high-quality training data and effective test cases. To address this issue, we present SWE-Dev, an SWE agent built upon open-source LLMs. First, we develop a robust pipeline to synthesize test cases for patch evaluation. Second, we scale up agent trajectories to construct the training data for building SWE-Dev. Experiments on the SWE-bench-Verified benchmark show that the SWE-Dev models can achieve top performance among all open SWE agents. Specifically, the success rates of the SWE-Dev 7B and 32B parameter models reach 23.4% and 36.6%, respectively, outperforming state-of-the-art open-source models. All code, models, and datasets are publicly available at <a target="_blank" rel="noopener" href="https://github.com/THUDM/SWE-Dev">https://github.com/THUDM/SWE-Dev</a>. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å·²ç»ä»è§£å†³å¯¹è¯é—®é¢˜è¿…é€Ÿå‘å±•åˆ°å¤„ç†æ¶‰åŠå·¥å…·ä½¿ç”¨çš„ç°å®ä¸–ç•Œä»»åŠ¡ï¼Œå¦‚è½¯ä»¶å·¥ç¨‹ï¼ˆSWEï¼‰ã€‚æœ€è¿‘çš„LLMé©±åŠ¨çš„å·¥å…·åŒ…ï¼Œå¦‚OpenAIçš„Codexå’ŒCursorï¼Œå·²ç»æä¾›äº†è½¯ä»¶å¼€å‘è¿‡ç¨‹çš„ç«¯åˆ°ç«¯è‡ªåŠ¨åŒ–ã€‚ç„¶è€Œï¼Œç”±äºç¼ºå°‘é«˜è´¨é‡çš„è®­ç»ƒæ•°æ®å’Œæœ‰æ•ˆçš„æµ‹è¯•ç”¨ä¾‹ï¼Œæ„å»ºæœ‰æ•ˆçš„SWEä»£ç†ä»ç„¶å…·æœ‰æŒ‘æˆ˜æ€§ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æ¨å‡ºäº†SWE-Devï¼Œä¸€ä¸ªåŸºäºå¼€æºLLMçš„SWEä»£ç†ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬å¼€å‘äº†ä¸€ä¸ªç¨³å¥çš„ç®¡é“æ¥åˆæˆæµ‹è¯•ç”¨ä¾‹è¿›è¡Œè¯„ä¼°ã€‚å…¶æ¬¡ï¼Œæˆ‘ä»¬æ‰©å¤§äº†ä»£ç†è½¨è¿¹ä»¥æ„å»ºSWE-Devçš„è®­ç»ƒæ•°æ®ã€‚åœ¨SWE-bench-VerifiedåŸºå‡†æµ‹è¯•ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒSWE-Devæ¨¡å‹åœ¨æ‰€æœ‰å…¬å¼€çš„SWEä»£ç†ä¸­éƒ½èƒ½è¾¾åˆ°é¡¶å°–æ€§èƒ½ã€‚å…·ä½“æ¥è¯´ï¼ŒSWE-Dev 7Bå’Œ32Bå‚æ•°æ¨¡å‹çš„æˆåŠŸç‡åˆ†åˆ«è¾¾åˆ°äº†23.4%å’Œ36.6%ï¼Œè¶…è¿‡äº†æœ€æ–°çš„å¼€æºæ¨¡å‹ã€‚æ‰€æœ‰ä»£ç ã€æ¨¡å‹å’Œæ•°æ®é›†å‡å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/THUDM/SWE-Dev%E5%85%AC%E5%BC%80%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/THUDM/SWE-Devå…¬å¼€è·å–ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.07636v2">PDF</a> Accepted to Findings of ACLâ€™25</p>
<p><strong>Summary</strong><br>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å·²ä»å¯¹è¯é—®é¢˜è§£å†³æ–¹æ¡ˆè¿…é€Ÿå‘å±•åˆ°åº”å¯¹æ¶‰åŠå·¥å…·ä½¿ç”¨çš„ç°å®ä¸–ç•Œä»»åŠ¡ï¼Œå¦‚è½¯ä»¶å·¥ç¨‹ï¼ˆSWEï¼‰ã€‚æœ€è¿‘ï¼ŒOpenAI Codexå’ŒCursorç­‰å·¥å…·åŒ…ä¸ºè½¯ä»¶å¼€å‘æµç¨‹æä¾›äº†ç«¯åˆ°ç«¯çš„è‡ªåŠ¨åŒ–ã€‚ç„¶è€Œï¼Œæ„å»ºæœ‰æ•ˆçš„SWEä»£ç†é¢ä¸´ç¼ºä¹é«˜è´¨é‡è®­ç»ƒæ•°æ®å’Œæœ‰æ•ˆæµ‹è¯•ç”¨ä¾‹çš„æŒ‘æˆ˜ã€‚ä¸ºè§£å†³æ­¤é—®é¢˜ï¼Œæˆ‘ä»¬æ¨å‡ºäº†åŸºäºå¼€æºLLMçš„SWE-Devä»£ç†ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬å¼€å‘äº†ä¸€ä¸ªç¨³å¥çš„ç®¡é“æ¥åˆæˆç”¨äºè¡¥ä¸è¯„ä¼°çš„æµ‹è¯•ç”¨ä¾‹ã€‚å…¶æ¬¡ï¼Œæˆ‘ä»¬æ‰©å¤§äº†ä»£ç†è½¨è¿¹ä»¥æ„å»ºSWE-Devçš„è®­ç»ƒæ•°æ®ã€‚åœ¨SWE-bench-VerifiedåŸºå‡†æµ‹è¯•ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒSWE-Devæ¨¡å‹åœ¨æ‰€æœ‰å¼€æºSWEä»£ç†ä¸­ååˆ—å‰èŒ…ã€‚ç‰¹åˆ«æ˜¯ï¼ŒSWE-Dev 7Bå’Œ32Bå‚æ•°æ¨¡å‹çš„æˆåŠŸç‡åˆ†åˆ«è¾¾åˆ°äº†23.4%å’Œ36.6%ï¼Œè¶…è¿‡äº†æœ€æ–°çš„å¼€æºæ¨¡å‹ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å·²æ‰©å±•åˆ°ç°å®ä¸–ç•Œä»»åŠ¡ï¼Œå¦‚è½¯ä»¶å·¥ç¨‹ï¼ˆSWEï¼‰ã€‚</li>
<li>è¿‘æœŸLLMå·¥å…·åŒ…å¦‚OpenAI Codexå’ŒCursoræ¨åŠ¨äº†è½¯ä»¶å¼€å‘çš„è‡ªåŠ¨åŒ–ã€‚</li>
<li>æ„å»ºæœ‰æ•ˆçš„SWEä»£ç†é¢ä¸´ç¼ºä¹é«˜è´¨é‡è®­ç»ƒæ•°æ®å’Œæœ‰æ•ˆæµ‹è¯•ç”¨ä¾‹çš„æŒ‘æˆ˜ã€‚</li>
<li>æå‡ºäº†SWE-Devä»£ç†ä»¥è§£å†³è¿™äº›é—®é¢˜ï¼Œè¯¥ä»£ç†åŸºäºå¼€æºLLMã€‚</li>
<li>SWE-Devé€šè¿‡å¼€å‘ç”¨äºè¡¥ä¸è¯„ä¼°çš„æµ‹è¯•ç”¨ä¾‹çš„ç¨³å¥ç®¡é“æ¥åˆæˆæµ‹è¯•æ¡ˆä¾‹ã€‚</li>
<li>é€šè¿‡æ‰©å¤§ä»£ç†è½¨è¿¹æ„å»ºäº†SWE-Devçš„è®­ç»ƒæ•°æ®ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.07636">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-334527c2866619182819926cc2a7d6cd.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-86f8cfc0fc0c6204d8a5624806f77a3c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-6424d05b0be55d19bdf56335fe924c6a.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-b10278082d8d8096f0de2e4c16ef07f6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-fd5f10a7c59a8fcd57638d57f80fab4e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-96aa4f55130e04329420f7455b24eb97.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c5dd430bb4898c915a36f3958f07845d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-dcc9ac11be647158a2e73142a24fb745.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="A-Survey-on-Large-Language-Model-based-Human-Agent-Systems"><a href="#A-Survey-on-Large-Language-Model-based-Human-Agent-Systems" class="headerlink" title="A Survey on Large Language Model based Human-Agent Systems"></a>A Survey on Large Language Model based Human-Agent Systems</h2><p><strong>Authors:Henry Peng Zou, Wei-Chieh Huang, Yaozu Wu, Yankai Chen, Chunyu Miao, Hoang Nguyen, Yue Zhou, Weizhi Zhang, Liancheng Fang, Langzhou He, Yangning Li, Dongyuan Li, Renhe Jiang, Xue Liu, Philip S. Yu</strong></p>
<p>Recent advances in large language models (LLMs) have sparked growing interest in building fully autonomous agents. However, fully autonomous LLM-based agents still face significant challenges, including limited reliability due to hallucinations, difficulty in handling complex tasks, and substantial safety and ethical risks, all of which limit their feasibility and trustworthiness in real-world applications. To overcome these limitations, LLM-based human-agent systems (LLM-HAS) incorporate human-provided information, feedback, or control into the agent system to enhance system performance, reliability and safety. These human-agent collaboration systems enable humans and LLM-based agents to collaborate effectively by leveraging their complementary strengths. This paper provides the first comprehensive and structured survey of LLM-HAS. It clarifies fundamental concepts, systematically presents core components shaping these systems, including environment &amp; profiling, human feedback, interaction types, orchestration and communication, explores emerging applications, and discusses unique challenges and opportunities arising from human-AI collaboration. By consolidating current knowledge and offering a structured overview, we aim to foster further research and innovation in this rapidly evolving interdisciplinary field. Paper lists and resources are available at <a target="_blank" rel="noopener" href="https://github.com/HenryPengZou/Awesome-LLM-Based-Human-Agent-Systems">https://github.com/HenryPengZou/Awesome-LLM-Based-Human-Agent-Systems</a>. </p>
<blockquote>
<p>æœ€è¿‘å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„è¿›å±•å¼•å‘äº†äººä»¬å¯¹æ„å»ºå®Œå…¨è‡ªä¸»ä»£ç†äººçš„æµ“åšå…´è¶£ã€‚ç„¶è€Œï¼ŒåŸºäºLLMçš„å®Œå…¨è‡ªä¸»ä»£ç†äººä»ç„¶é¢ä¸´é‡å¤§æŒ‘æˆ˜ï¼ŒåŒ…æ‹¬ç”±äºå¹»è§‰å¯¼è‡´çš„å¯é æ€§æœ‰é™ã€å¤„ç†å¤æ‚ä»»åŠ¡çš„å›°éš¾ä»¥åŠå®‰å…¨å’Œä¼¦ç†é£é™©è¾ƒé«˜ï¼Œæ‰€æœ‰è¿™äº›å› ç´ éƒ½é™åˆ¶äº†å®ƒä»¬åœ¨ç°å®ä¸–ç•Œåº”ç”¨ä¸­çš„å¯è¡Œæ€§å’Œå¯ä¿¡åº¦ã€‚ä¸ºäº†å…‹æœè¿™äº›å±€é™æ€§ï¼ŒåŸºäºLLMçš„äººæœºä»£ç†ç³»ç»Ÿï¼ˆLLM-HASï¼‰å°†äººç±»æä¾›çš„ä¿¡æ¯ã€åé¦ˆæˆ–æ§åˆ¶çº³å…¥ä»£ç†ç³»ç»Ÿï¼Œä»¥æé«˜ç³»ç»Ÿæ€§èƒ½ã€å¯é æ€§å’Œå®‰å…¨æ€§ã€‚è¿™äº›äººæœºåä½œç³»ç»Ÿé€šè¿‡åˆ©ç”¨äººç±»å’ŒåŸºäºLLMçš„ä»£ç†äººçš„å„è‡ªä¼˜åŠ¿ï¼Œä½¿äººç±»å’ŒåŸºäºLLMçš„ä»£ç†äººèƒ½å¤Ÿè¿›è¡Œæœ‰æ•ˆçš„åä½œã€‚æœ¬æ–‡å¯¹LLM-HASè¿›è¡Œäº†ç¬¬ä¸€æ¬¡å…¨é¢å’Œç³»ç»Ÿçš„è°ƒæŸ¥ã€‚é˜æ˜äº†åŸºæœ¬æ¦‚å¿µï¼Œç³»ç»Ÿåœ°ä»‹ç»äº†æ„æˆè¿™äº›ç³»ç»Ÿçš„æ ¸å¿ƒç»„ä»¶ï¼ŒåŒ…æ‹¬ç¯å¢ƒåˆ†æã€äººç±»åé¦ˆã€äº¤äº’ç±»å‹ã€ç¼–æ’å’Œé€šä¿¡ï¼Œæ¢è®¨äº†æ–°å…´åº”ç”¨ï¼Œå¹¶è®¨è®ºäº†ç”±äººæœºåä½œäº§ç”Ÿçš„ç‹¬ç‰¹æŒ‘æˆ˜å’Œæœºé‡ã€‚é€šè¿‡æ•´åˆå½“å‰çŸ¥è¯†å¹¶æä¾›ç»“æ„åŒ–æ¦‚è¿°ï¼Œæˆ‘ä»¬çš„ç›®æ ‡æ˜¯ä¿ƒè¿›è¿™ä¸€è¿…é€Ÿå‘å±•çš„è·¨å­¦ç§‘é¢†åŸŸçš„è¿›ä¸€æ­¥ç ”ç©¶å’Œåˆ›æ–°ã€‚è®ºæ–‡åˆ—è¡¨å’Œèµ„æºå¯é€šè¿‡<a target="_blank" rel="noopener" href="https://github.com/HenryPengZou/Awesome-LLM-Based-Human-Agent-Systems%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/HenryPengZou/Awesome-LLM-Based-Human-Agent-Systemsè·å–ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.00753v3">PDF</a> Paper lists and resources are available at   <a target="_blank" rel="noopener" href="https://github.com/HenryPengZou/Awesome-LLM-Based-Human-Agent-Systems">https://github.com/HenryPengZou/Awesome-LLM-Based-Human-Agent-Systems</a></p>
<p><strong>Summary</strong><br>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¸ºåŸºç¡€çš„å…¨è‡ªä¸»ä»£ç†æŠ€æœ¯æ—¥ç›Šå—åˆ°å…³æ³¨ï¼Œä½†ä»é¢ä¸´å¯é æ€§ã€å¤„ç†å¤æ‚ä»»åŠ¡èƒ½åŠ›ã€å®‰å…¨å’Œä¼¦ç†é£é™©ç­‰æ–¹é¢çš„æŒ‘æˆ˜ã€‚ä¸ºäº†å…‹æœè¿™äº›é™åˆ¶ï¼Œç»“åˆäº†äººç±»æä¾›çš„ä¿¡æ¯ã€åé¦ˆæˆ–æ§åˆ¶çš„LLMäººæœºåä½œç³»ç»Ÿï¼ˆLLM-HASï¼‰è¢«æå‡ºï¼Œé€šè¿‡åˆ©ç”¨äººç±»å’ŒLLMä»£ç†çš„äº’è¡¥ä¼˜åŠ¿ä»¥å®ç°æœ‰æ•ˆåä½œã€‚æœ¬æ–‡æ˜¯å¯¹LLM-HASçš„é¦–ä¸ªå…¨é¢ç»“æ„åŒ–è°ƒæŸ¥ï¼Œé˜è¿°äº†åŸºæœ¬æ¦‚å¿µå’Œç³»ç»Ÿæ ¸å¿ƒç»„ä»¶ï¼ŒåŒ…æ‹¬ç¯å¢ƒåˆ†æã€äººç±»åé¦ˆã€äº¤äº’ç±»å‹ã€åè°ƒå’Œé€šä¿¡ç­‰ã€‚æ­¤å¤–ï¼Œæœ¬æ–‡æ¢è®¨äº†äººæœºåä½œå¸¦æ¥çš„ç‹¬ç‰¹æŒ‘æˆ˜å’Œæœºé‡ï¼Œå¹¶æ•´åˆäº†å½“å‰çŸ¥è¯†ï¼Œä¸ºè¿™ä¸€è¿…é€Ÿå‘å±•çš„è·¨å­¦ç§‘é¢†åŸŸçš„ç ”ç©¶å’Œåˆ›æ–°æä¾›äº†åŸºç¡€ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLMsé¢ä¸´å¯é æ€§ã€å¤æ‚ä»»åŠ¡å¤„ç†èƒ½åŠ›åŠå®‰å…¨å’Œä¼¦ç†é£é™©æ–¹é¢çš„æŒ‘æˆ˜ã€‚</li>
<li>LLM-HASç»“åˆäº†äººç±»ä¸LLMä»£ç†çš„ä¿¡æ¯ã€åé¦ˆå’Œæ§åˆ¶ï¼Œä»¥æé«˜ç³»ç»Ÿæ€§èƒ½ã€å¯é æ€§å’Œå®‰å…¨æ€§ã€‚</li>
<li>LLM-HASå®ç°äº†äººç±»å’ŒLLMä»£ç†çš„æœ‰æ•ˆåä½œï¼Œåˆ©ç”¨åŒæ–¹çš„äº’è¡¥ä¼˜åŠ¿ã€‚</li>
<li>æœ¬æ–‡æ˜¯å¯¹LLM-HASçš„å…¨é¢ç»“æ„åŒ–è°ƒæŸ¥ï¼Œé˜è¿°äº†å…¶åŸºæœ¬æ¦‚å¿µå’Œç³»ç»Ÿæ ¸å¿ƒç»„ä»¶ã€‚</li>
<li>LLM-HASçš„ç¯å¢ƒåˆ†æã€äººç±»åé¦ˆã€äº¤äº’ç±»å‹ã€åè°ƒå’Œé€šä¿¡ç­‰è¦ç´ è¢«è¯¦ç»†æ¢è®¨ã€‚</li>
<li>äººæœºåä½œå¸¦æ¥ç‹¬ç‰¹çš„æŒ‘æˆ˜å’Œæœºé‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.00753">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-445f7e17e9c667944d65e0eca0ab8023.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-82cba21d7f7ef039d86770fa7d47057f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9a6ba09dfab9e7e6522ab8bb6ee6d01f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-9887fb5d1c54c35aefa14aa27bc9c479.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-2b804404a395280eb9aa569eae1f75ae.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="Boosting-Virtual-Agent-Learning-and-Reasoning-A-Step-Wise-Multi-Dimensional-and-Generalist-Reward-Model-with-Benchmark"><a href="#Boosting-Virtual-Agent-Learning-and-Reasoning-A-Step-Wise-Multi-Dimensional-and-Generalist-Reward-Model-with-Benchmark" class="headerlink" title="Boosting Virtual Agent Learning and Reasoning: A Step-Wise,   Multi-Dimensional, and Generalist Reward Model with Benchmark"></a>Boosting Virtual Agent Learning and Reasoning: A Step-Wise,   Multi-Dimensional, and Generalist Reward Model with Benchmark</h2><p><strong>Authors:Bingchen Miao, Yang Wu, Minghe Gao, Qifan Yu, Wendong Bu, Wenqiao Zhang, Yunfei Li, Siliang Tang, Tat-Seng Chua, Juncheng Li</strong></p>
<p>The development of Generalist Virtual Agents (GVAs) has shown significant promise in autonomous task execution. However, current training paradigms face critical limitations, including reliance on outcome supervision and labor-intensive human annotations. To address these challenges, we propose Similar, a Step-Wise Multi-Dimensional Generalist Reward Model, which offers fine-grained signals for agent training and can choose better action for inference-time scaling. Specifically, we begin by systematically defining five dimensions for evaluating agent actions. Building on this framework, we design an MCTS-P algorithm to automatically collect and annotate step-wise, five-dimensional agent execution data. Using this data, we train Similar with the Triple-M strategy. Furthermore, we introduce the first benchmark in the virtual agent domain for step-wise, multi-dimensional reward model training and evaluation, named SRM. This benchmark consists of two components: SRMTrain, which serves as the training set for Similar, and SRMEval, a manually selected test set for evaluating the reward model. Experimental results demonstrate that Similar, through its step-wise, multi-dimensional assessment and synergistic gain, provides GVAs with effective intermediate signals during both training and inference-time scaling. The project is available at <a target="_blank" rel="noopener" href="https://github.com/antgroup/Similar">https://github.com/antgroup/Similar</a>. </p>
<blockquote>
<p>é€šç”¨è™šæ‹Ÿæ™ºèƒ½ä½“ï¼ˆGVAsï¼‰çš„å‘å±•åœ¨è‡ªä¸»ä»»åŠ¡æ‰§è¡Œæ–¹é¢å±•ç°å‡ºå·¨å¤§æ½œåŠ›ã€‚ç„¶è€Œï¼Œå½“å‰çš„è®­ç»ƒæ¨¡å¼é¢ä¸´é‡å¤§å±€é™ï¼ŒåŒ…æ‹¬ä¾èµ–ç»“æœç›‘ç£å’ŒåŠ³åŠ¨å¯†é›†å‹äººå·¥æ ‡æ³¨ã€‚ä¸ºäº†åº”å¯¹è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†åä¸ºâ€œSimilarâ€çš„æ­¥çº§å¤šç»´åº¦é€šç”¨å¥–åŠ±æ¨¡å‹ï¼Œè¯¥æ¨¡å‹ä¸ºæ™ºèƒ½ä½“è®­ç»ƒæä¾›ç²¾ç»†çš„ä¿¡å·ï¼Œå¹¶å¯ä»¥åœ¨æ¨ç†æ—¶é€‰æ‹©æ›´å¥½çš„åŠ¨ä½œè¿›è¡Œç¼©æ”¾ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬é¦–å…ˆç³»ç»Ÿåœ°å®šä¹‰äº†äº”ä¸ªç»´åº¦æ¥è¯„ä¼°æ™ºèƒ½ä½“çš„åŠ¨ä½œã€‚åœ¨æ­¤åŸºç¡€ä¸Šï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸€ç§åŸºäºè’™ç‰¹å¡æ´›æ ‘æœç´¢çš„ç®—æ³•ï¼ˆMCTS-Pï¼‰ï¼Œè¯¥ç®—æ³•èƒ½å¤Ÿè‡ªåŠ¨æ”¶é›†å’Œæ ‡æ³¨æ™ºèƒ½ä½“çš„æ­¥çº§ã€äº”ç»´åº¦æ‰§è¡Œæ•°æ®ã€‚ä½¿ç”¨è¿™äº›æ•°æ®ï¼Œæˆ‘ä»¬ç”¨ä¸‰é‡Mç­–ç•¥è®­ç»ƒSimilaræ¨¡å‹ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜å¼•å…¥äº†è™šæ‹Ÿæ™ºèƒ½ä½“é¢†åŸŸé¦–ä¸ªç”¨äºæ­¥çº§å¤šç»´åº¦å¥–åŠ±æ¨¡å‹è®­ç»ƒå’Œè¯„ä»·çš„åŸºå‡†æ•°æ®é›†ï¼Œåä¸ºSRMã€‚è¯¥åŸºå‡†æ•°æ®é›†åŒ…å«ä¸¤ä¸ªç»„ä»¶ï¼šç”¨äºè®­ç»ƒSimilarçš„è®­ç»ƒé›†SRMTrainå’Œç”¨äºè¯„ä¼°å¥–åŠ±æ¨¡å‹çš„SRMEvalæ‰‹åŠ¨é€‰æ‹©æµ‹è¯•é›†ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œé€šè¿‡æ­¥çº§å¤šç»´è¯„ä¼°å’ŒååŒå¢ç›Šï¼ŒSimilarä¸ºGVAsåœ¨è®­ç»ƒå’Œæ¨ç†è¿‡ç¨‹ä¸­æä¾›äº†æœ‰æ•ˆçš„ä¸­é—´ä¿¡å·ã€‚è¯¥é¡¹ç›®å¯é€šè¿‡<a target="_blank" rel="noopener" href="https://github.com/antgroup/Similar%E8%8E%B7%E5%8F%96%E8%AE%BF%E9%97%AE%E3%80%82">https://github.com/antgroup/Similarè·å–è®¿é—®ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.18665v2">PDF</a> Home page is available at <a target="_blank" rel="noopener" href="https://dcd-ant-similar.github.io/">https://dcd-ant-similar.github.io</a></p>
<p><strong>Summary</strong>ï¼šé€šç”¨è™šæ‹Ÿä»£ç†äººï¼ˆGVAsï¼‰åœ¨è‡ªä¸»ä»»åŠ¡æ‰§è¡Œæ–¹é¢å±•ç°å‡ºå·¨å¤§æ½œåŠ›ï¼Œä½†å…¶è®­ç»ƒæ¨¡å¼ä»å­˜åœ¨ä¾èµ–ç»“æœç›‘ç£å’ŒåŠ³åŠ¨å¯†é›†å‹äººå·¥æ ‡æ³¨ç­‰å±€é™æ€§ã€‚ä¸ºæ­¤ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºSimilarçš„é€æ­¥å¤šç»´é€šç”¨å¥–åŠ±æ¨¡å‹ï¼Œä¸ºä»£ç†è®­ç»ƒæä¾›ç²¾ç»†ä¿¡å·ï¼Œå¹¶åœ¨æ¨ç†æ—¶é—´ç¼©æ”¾æ—¶é€‰æ‹©æ›´å¥½çš„è¡ŒåŠ¨ã€‚é€šè¿‡ç³»ç»Ÿåœ°å®šä¹‰äº”ä¸ªç»´åº¦æ¥è¯„ä¼°ä»£ç†è¡ŒåŠ¨ï¼Œè®¾è®¡MCTS-Pç®—æ³•è‡ªåŠ¨æ”¶é›†å’Œæ ‡æ³¨é€æ­¥çš„äº”ç»´ä»£ç†æ‰§è¡Œæ•°æ®ã€‚ä½¿ç”¨è¿™ç§æ•°æ®ï¼Œç”¨Triple-Mç­–ç•¥è®­ç»ƒSimilarã€‚æ­¤å¤–ï¼Œæœ¬æ–‡å¼•å…¥äº†è™šæ‹Ÿä»£ç†é¢†åŸŸé¦–ä¸ªç”¨äºé€æ­¥å¤šç»´å¥–åŠ±æ¨¡å‹è®­ç»ƒå’Œè¯„ä»·çš„åŸºå‡†æµ‹è¯•ï¼Œåä¸ºSRMã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒSimilaré€šè¿‡é€æ­¥å¤šç»´è¯„ä¼°å’ŒååŒå¢ç›Šä¸ºGVAsåœ¨è®­ç»ƒå’Œæ¨ç†æ—¶é—´ç¼©æ”¾æœŸé—´æä¾›æœ‰æ•ˆçš„ä¸­é—´ä¿¡å·ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>Generalist Virtual Agents (GVAs) åœ¨è‡ªä¸»ä»»åŠ¡æ‰§è¡Œæ–¹é¢å±•ç°å‡ºå·¨å¤§æ½œåŠ›ï¼Œä½†ç°æœ‰è®­ç»ƒæ¨¡å¼å­˜åœ¨å±€é™æ€§ã€‚</li>
<li>æå‡ºäº†åä¸ºSimilarçš„é€æ­¥å¤šç»´é€šç”¨å¥–åŠ±æ¨¡å‹ï¼Œä»¥ç²¾ç»†çš„æ–¹å¼ä¸ºä»£ç†è®­ç»ƒæä¾›ä¿¡å·ï¼Œå¹¶åœ¨æ¨ç†æ—¶é€‰æ‹©æœ€ä½³è¡ŒåŠ¨ã€‚</li>
<li>é€šè¿‡ç³»ç»Ÿåœ°å®šä¹‰äº”ä¸ªç»´åº¦æ¥è¯„ä¼°ä»£ç†è¡ŒåŠ¨ï¼Œä¸ºä»£ç†è®­ç»ƒæä¾›æ›´å…¨é¢çš„åé¦ˆã€‚</li>
<li>è®¾è®¡äº†MCTS-Pç®—æ³•æ¥è‡ªåŠ¨æ”¶é›†å’Œæ ‡æ³¨é€æ­¥çš„äº”ç»´ä»£ç†æ‰§è¡Œæ•°æ®ï¼Œå¢å¼ºè®­ç»ƒæ•ˆç‡ã€‚</li>
<li>ä½¿ç”¨Triple-Mç­–ç•¥æ¥è®­ç»ƒSimilaræ¨¡å‹ï¼Œæé«˜äº†æ¨¡å‹çš„æ€§èƒ½ã€‚</li>
<li>å¼•å…¥äº†è™šæ‹Ÿä»£ç†é¢†åŸŸçš„é¦–ä¸ªåŸºå‡†æµ‹è¯•SRMï¼Œç”¨äºé€æ­¥å¤šç»´å¥–åŠ±æ¨¡å‹è®­ç»ƒå’Œè¯„ä¼°ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.18665">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-40ff1c5b339d65b9f973a772f0fd340a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-014d775d8c4cd24dbfb008d22d32be46.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5a375f7c65cb57b10b1ea70814161f63.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-6da3c1a88bf3b89106e38fc6c907708b.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="EmoAgent-A-Multi-Agent-Framework-for-Diverse-Affective-Image-Manipulation"><a href="#EmoAgent-A-Multi-Agent-Framework-for-Diverse-Affective-Image-Manipulation" class="headerlink" title="EmoAgent: A Multi-Agent Framework for Diverse Affective Image   Manipulation"></a>EmoAgent: A Multi-Agent Framework for Diverse Affective Image   Manipulation</h2><p><strong>Authors:Qi Mao, Haobo Hu, Yujie He, Difei Gao, Haokun Chen, Libiao Jin</strong></p>
<p>Affective Image Manipulation (AIM) aims to alter visual elements within an image to evoke specific emotional responses from viewers. However, existing AIM approaches rely on rigid \emph{one-to-one} mappings between emotions and visual cues, making them ill-suited for the inherently subjective and diverse ways in which humans perceive and express emotion.To address this, we introduce a novel task setting termed \emph{Diverse AIM (D-AIM)}, aiming to generate multiple visually distinct yet emotionally consistent image edits from a single source image and target emotion. We propose \emph{EmoAgent}, the first multi-agent framework tailored specifically for D-AIM. EmoAgent explicitly decomposes the manipulation process into three specialized phases executed by collaborative agents: a Planning Agent that generates diverse emotional editing strategies, an Editing Agent that precisely executes these strategies, and a Critic Agent that iteratively refines the results to ensure emotional accuracy. This collaborative design empowers EmoAgent to model \emph{one-to-many} emotion-to-visual mappings, enabling semantically diverse and emotionally faithful edits.Extensive quantitative and qualitative evaluations demonstrate that EmoAgent substantially outperforms state-of-the-art approaches in both emotional fidelity and semantic diversity, effectively generating multiple distinct visual edits that convey the same target emotion. </p>
<blockquote>
<p>æƒ…æ„Ÿå›¾åƒæ“çºµï¼ˆAIMï¼‰æ—¨åœ¨æ”¹å˜å›¾åƒä¸­çš„è§†è§‰å…ƒç´ ï¼Œä»¥æ¿€å‘è§‚çœ‹è€…ç‰¹å®šçš„æƒ…æ„Ÿååº”ã€‚ç„¶è€Œï¼Œç°æœ‰çš„AIMæ–¹æ³•ä¾èµ–äºæƒ…æ„Ÿä¸è§†è§‰çº¿ç´¢ä¹‹é—´çš„åƒµåŒ–â€œä¸€å¯¹ä¸€â€æ˜ å°„ï¼Œè¿™ä½¿å¾—å®ƒä»¬ä¸é€‚åˆäººç±»æ„ŸçŸ¥å’Œè¡¨è¾¾æƒ…æ„Ÿæ‰€å…·æœ‰çš„å›ºæœ‰ä¸»è§‚æ€§å’Œå¤šæ ·æ€§ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§æ–°å‹ä»»åŠ¡è®¾ç½®ï¼Œç§°ä¸ºå¤šæ ·åŒ–AIMï¼ˆD-AIMï¼‰ï¼Œæ—¨åœ¨ä»å•ä¸ªæºå›¾åƒå’Œç›®æ ‡æƒ…æ„Ÿç”Ÿæˆå¤šä¸ªè§†è§‰ä¸Šä¸åŒä½†æƒ…æ„Ÿä¸Šä¸€è‡´çš„å›¾åƒç¼–è¾‘ã€‚æˆ‘ä»¬æå‡ºäº†ä¸“é—¨ä¸ºD-AIMå®šåˆ¶çš„ç¬¬ä¸€ä¸ªå¤šä»£ç†æ¡†æ¶EmoAgentã€‚EmoAgentæ˜ç¡®åœ°å°†æ“çºµè¿‡ç¨‹åˆ†è§£ä¸ºä¸‰ä¸ªé˜¶æ®µï¼Œç”±åä½œä»£ç†æ‰§è¡Œï¼šè§„åˆ’ä»£ç†ç”Ÿæˆå¤šç§æƒ…æ„Ÿç¼–è¾‘ç­–ç•¥ï¼Œç¼–è¾‘ä»£ç†ç²¾ç¡®æ‰§è¡Œè¿™äº›ç­–ç•¥ï¼Œè¯„è®ºå®¶ä»£ç†è¿­ä»£ä¼˜åŒ–ç»“æœä»¥ç¡®ä¿æƒ…æ„Ÿå‡†ç¡®æ€§ã€‚è¿™ç§åä½œè®¾è®¡ä½¿EmoAgentèƒ½å¤Ÿå»ºç«‹â€œä¸€åˆ°å¤šâ€çš„æƒ…æ„Ÿåˆ°è§†è§‰æ˜ å°„ï¼Œä»è€Œå®ç°è¯­ä¹‰å¤šæ ·ä¸”æƒ…æ„ŸçœŸå®çš„ç¼–è¾‘ã€‚å¹¿æ³›å®šé‡å’Œå®šæ€§çš„è¯„ä¼°è¡¨æ˜ï¼ŒEmoAgentåœ¨æƒ…æ„Ÿä¿çœŸåº¦å’Œè¯­ä¹‰å¤šæ ·æ€§æ–¹é¢å¤§å¤§ä¼˜äºç°æœ‰å…ˆè¿›æŠ€æœ¯ï¼Œèƒ½å¤Ÿç”Ÿæˆæœ‰æ•ˆä¼ è¾¾åŒä¸€ç›®æ ‡æƒ…æ„Ÿçš„å¤šä¸ªä¸åŒè§†è§‰ç¼–è¾‘ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.11290v3">PDF</a> </p>
<p><strong>Summary</strong>ï¼š<br>æƒ…æ„Ÿå›¾åƒæ“ä½œï¼ˆAIMï¼‰æ—¨åœ¨æ”¹å˜å›¾åƒçš„è§†è§‰å…ƒç´ ä»¥æ¿€å‘è§‚ä¼—ç‰¹å®šçš„æƒ…æ„Ÿååº”ã€‚ç„¶è€Œï¼Œç°æœ‰çš„AIMæ–¹æ³•ä¾èµ–äºæƒ…ç»ªä¸è§†è§‰çº¿ç´¢ä¹‹é—´çš„åƒµåŒ–çš„ä¸€ä¸€æ˜ å°„ï¼Œè¿™ä½¿å¾—å®ƒä»¬ä¸é€‚åˆäººç±»æ„ŸçŸ¥å’Œè¡¨è¾¾æƒ…ç»ªçš„å›ºæœ‰ä¸»è§‚æ€§å’Œå¤šæ ·æ€§ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ªåä¸ºå¤šæ ·åŒ–AIMï¼ˆD-AIMï¼‰çš„æ–°ä»»åŠ¡è®¾ç½®ï¼Œæ—¨åœ¨ä»å•ä¸ªæºå›¾åƒå’Œç›®æ ‡æƒ…ç»ªç”Ÿæˆå¤šä¸ªè§†è§‰å„å¼‚ä½†æƒ…æ„Ÿä¸€è‡´çš„å›¾åƒç¼–è¾‘ã€‚æˆ‘ä»¬æå‡ºäº†ä¸“é—¨é’ˆå¯¹D-AIMè®¾è®¡çš„ç¬¬ä¸€ä¸ªå¤šä»£ç†æ¡†æ¶EmoAgentã€‚EmoAgentå°†æ“ä½œè¿‡ç¨‹æ˜ç¡®åˆ†è§£ä¸ºä¸‰ä¸ªé˜¶æ®µï¼Œç”±åä½œä»£ç†æ‰§è¡Œï¼šç”Ÿæˆå¤šç§æƒ…æ„Ÿç¼–è¾‘ç­–ç•¥çš„è§„åˆ’ä»£ç†ã€ç²¾ç¡®æ‰§è¡Œè¿™äº›ç­–ç•¥ç¼–è¾‘ä»£ç†å’Œä¸æ–­å¯¹ç»“æœè¿›è¡Œç²¾ç»†åŒ–ä»¥ç¡®ä¿æƒ…æ„Ÿå‡†ç¡®æ€§çš„è¯„è®ºå®¶ä»£ç†ã€‚è¿™ç§åä½œè®¾è®¡ä½¿EmoAgentèƒ½å¤Ÿæ¨¡æ‹Ÿä¸€å¯¹ä¸€çš„æƒ…ç»ªåˆ°è§†è§‰æ˜ å°„ï¼Œå®ç°è¯­ä¹‰å¤šæ ·ä¸”æƒ…æ„ŸçœŸå®çš„ç¼–è¾‘ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>æƒ…æ„Ÿå›¾åƒæ“ä½œï¼ˆAIMï¼‰é€šè¿‡æ”¹å˜å›¾åƒçš„è§†è§‰å…ƒç´ æ¥æ¿€å‘ç‰¹å®šæƒ…æ„Ÿååº”ã€‚</li>
<li>ç°æœ‰æ–¹æ³•ä¾èµ–äºåƒµåŒ–çš„æƒ…ç»ªä¸è§†è§‰çº¿ç´¢çš„ä¸€ä¸€æ˜ å°„ï¼Œç¼ºä¹ä¸»è§‚æ€§å’Œå¤šæ ·æ€§ã€‚</li>
<li>å¼•å…¥å¤šæ ·åŒ–AIMï¼ˆD-AIMï¼‰ä»»åŠ¡è®¾ç½®ï¼Œæ—¨åœ¨ä»å•ä¸ªæºå›¾åƒç”Ÿæˆå¤šä¸ªæƒ…æ„Ÿä¸€è‡´çš„å›¾åƒç¼–è¾‘ã€‚</li>
<li>æå‡ºå¤šä»£ç†æ¡†æ¶EmoAgentï¼ŒåŒ…æ‹¬è§„åˆ’ã€ç¼–è¾‘å’Œè¯„è®ºå®¶ä»£ç†ï¼Œå®ç°è¯­ä¹‰å¤šæ ·ä¸”æƒ…æ„ŸçœŸå®çš„ç¼–è¾‘ã€‚</li>
<li>EmoAgenté€šè¿‡æ¨¡æ‹Ÿä¸€ä¸€å¯¹åº”çš„æƒ…ç»ªåˆ°è§†è§‰æ˜ å°„ï¼Œçªç ´ç°æœ‰æ–¹æ³•é™åˆ¶ã€‚</li>
<li>EmoAgentåœ¨æƒ…æ„Ÿä¿çœŸåº¦å’Œè¯­ä¹‰å¤šæ ·æ€§æ–¹é¢æ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.11290">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-cb7ce0d90ed5868d1cb1d74b7b5a1f36.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-0dcfe6c68c4bd63568fe0370ad6f3e7b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-75f30456eb8e46ed299b9bcea9c3f0ac.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-ad22a24c2ef1673238dac324e8ea23df.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-4f17878047b7fe872f0e69b8ea7650c8.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ef6ecd04a9a6751194831c20dcddb5f2.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="Collaborative-Mean-Estimation-Among-Heterogeneous-Strategic-Agents-Individual-Rationality-Fairness-and-Truthful-Contribution"><a href="#Collaborative-Mean-Estimation-Among-Heterogeneous-Strategic-Agents-Individual-Rationality-Fairness-and-Truthful-Contribution" class="headerlink" title="Collaborative Mean Estimation Among Heterogeneous Strategic Agents:   Individual Rationality, Fairness, and Truthful Contribution"></a>Collaborative Mean Estimation Among Heterogeneous Strategic Agents:   Individual Rationality, Fairness, and Truthful Contribution</h2><p><strong>Authors:Alex Clinton, Yiding Chen, Xiaojin Zhu, Kirthevasan Kandasamy</strong></p>
<p>We study a collaborative learning problem where $m$ agents aim to estimate a vector $\mu &#x3D;(\mu_1,\ldots,\mu_d)\in \mathbb{R}^d$ by sampling from associated univariate normal distributions ${\mathcal{N}(\mu_k, \sigma^2)}<em>{k\in[d]}$. Agent $i$ incurs a cost $c</em>{i,k}$ to sample from $\mathcal{N}(\mu_k, \sigma^2)$. Instead of working independently, agents can exchange data, collecting cheaper samples and sharing them in return for costly data, thereby reducing both costs and estimation error. We design a mechanism to facilitate such collaboration, while addressing two key challenges: ensuring individually rational (IR) and fair outcomes so all agents benefit, and preventing strategic behavior (e.g. non-collection, data fabrication) to avoid socially undesirable outcomes. We design a mechanism and an associated Nash equilibrium (NE) which minimizes the social penalty-sum of agentsâ€™ estimation errors and collection costs-while being IR for all agents. We achieve a $\mathcal{O}(\sqrt{m})$-approximation to the minimum social penalty in the worst case and an $\mathcal{O}(1)$-approximation under favorable conditions. Additionally, we establish three hardness results: no nontrivial mechanism guarantees (i) a dominant strategy equilibrium where agents report truthfully, (ii) is IR for every strategy profile of other agents, (iii) or avoids a worst-case $\Omega(\sqrt{m})$ price of stability in any NE. Finally, by integrating concepts from axiomatic bargaining, we demonstrate that our mechanism supports fairer outcomes than one which minimizes social penalty. </p>
<blockquote>
<p>æˆ‘ä»¬ç ”ç©¶äº†ä¸€ä¸ªåä½œå­¦ä¹ é—®é¢˜ï¼Œå…¶ä¸­mä¸ªä»£ç†æ—¨åœ¨é€šè¿‡ä»ç›¸å…³çš„ä¸€å…ƒæ­£æ€åˆ†å¸ƒä¸­é‡‡æ ·æ¥ä¼°è®¡å‘é‡Î¼&#x3D;(Î¼_1,â€¦,Î¼_d)âˆˆâ„^dã€‚ä»£ç†iä»N(Î¼k,Ïƒ^2)ä¸­é‡‡æ ·ä¼šäº§ç”Ÿæˆæœ¬c_{i,k}ã€‚ä»£ç†ä»¬ä¸å¿…ç‹¬ç«‹å·¥ä½œï¼Œä»–ä»¬å¯ä»¥äº¤æ¢æ•°æ®ï¼Œæ”¶é›†æ›´ä¾¿å®œçš„æ ·æœ¬å¹¶å…±äº«æ˜‚è´µçš„æ ·æœ¬ï¼Œä»è€Œé™ä½æˆæœ¬å’Œä¼°è®¡è¯¯å·®ã€‚æˆ‘ä»¬è®¾è®¡äº†ä¸€ç§æœºåˆ¶æ¥ä¿ƒè¿›è¿™ç§åˆä½œï¼ŒåŒæ—¶è§£å†³ä¸¤ä¸ªå…³é”®æŒ‘æˆ˜ï¼šç¡®ä¿ä¸ªäººç†æ€§å…¬å¹³çš„ç»“æœä½¿æ‰€æœ‰ä»£ç†éƒ½èƒ½å—ç›Šï¼Œå¹¶é˜²æ­¢ç­–ç•¥æ€§è¡Œä¸ºï¼ˆå¦‚ä¸æ”¶é›†ã€åˆ¶é€ è™šå‡æ•°æ®ï¼‰ä»¥é¿å…ç¤¾ä¼šä¸å¸Œæœ›çœ‹åˆ°çš„ç»“æœã€‚æˆ‘ä»¬è®¾è®¡äº†ä¸€ç§æœºåˆ¶ä»¥åŠç›¸å…³çš„çº³ä»€å‡è¡¡ï¼ˆNEï¼‰ï¼Œè¯¥æœºåˆ¶èƒ½åœ¨æœ€å°åŒ–ä»£ç†ä¼°è®¡è¯¯å·®å’Œæ”¶é›†æˆæœ¬çš„ç¤¾ä¼šæƒ©ç½šçš„åŒæ—¶ï¼Œå¯¹æ‰€æœ‰ä»£ç†å®ç°ä¸ªä½“ç†æ€§ã€‚åœ¨æœ€åçš„æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬å®ç°äº†å¯¹æœ€å°ç¤¾ä¼šæƒ©ç½šçš„O(âˆšm)è¿‘ä¼¼å€¼ï¼Œå¹¶åœ¨æœ‰åˆ©çš„æ¡ä»¶ä¸‹å®ç°äº†O(1)è¿‘ä¼¼å€¼ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜ç¡®å®šäº†ä¸‰ä¸ªéš¾åº¦ç»“æœï¼šæ²¡æœ‰ä»»ä½•éå¹³å‡¡æœºåˆ¶èƒ½ä¿è¯ï¼ˆiï¼‰å­˜åœ¨ä¸€ç§ä¼˜åŠ¿ç­–ç•¥å‡è¡¡è®©ä»£ç†äººå¦‚å®æŠ¥å‘Šï¼Œï¼ˆiiï¼‰å¯¹ä»»ä½•å…¶ä»–ä»£ç†çš„ç­–ç•¥ç»„åˆéƒ½æ˜¯ä¸ªä½“ç†æ€§çš„ï¼Œï¼ˆiiiï¼‰åœ¨ä»»ä½•NEä¸­é¿å…æœ€åæƒ…å†µçš„Î©(âˆšm)ç¨³å®šæ€§ä»£ä»·ã€‚æœ€åï¼Œé€šè¿‡æ•´åˆåŸåˆ™æ€§è°ˆåˆ¤çš„æ¦‚å¿µï¼Œæˆ‘ä»¬è¯æ˜äº†æˆ‘ä»¬çš„æœºåˆ¶æ¯”æœ€å°åŒ–ç¤¾ä¼šæƒ©ç½šçš„æœºåˆ¶æ›´èƒ½æ”¯æŒæ›´å…¬å¹³çš„ç»“æœã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2407.15881v2">PDF</a> ICML 2025</p>
<p><strong>Summary</strong><br>     åœ¨åä½œå­¦ä¹ é—®é¢˜ä¸­ï¼Œmä¸ªä»£ç†è¯•å›¾é€šè¿‡ä»ç›¸å…³çš„ä¸€å…ƒæ­£æ€åˆ†å¸ƒä¸­é‡‡æ ·æ¥ä¼°è®¡å‘é‡Î¼ã€‚ä»£ç†é€šè¿‡äº¤æ¢æ•°æ®æ¥é™ä½æˆæœ¬å’Œä¼°è®¡è¯¯å·®ã€‚æœ¬æ–‡è®¾è®¡äº†ä¸€ç§åä½œæœºåˆ¶ï¼Œè§£å†³äº†ä¸¤ä¸ªå…³é”®é—®é¢˜ï¼šç¡®ä¿ä¸ªä½“ç†æ€§å’Œå…¬å¹³çš„ç»“æœï¼Œé˜²æ­¢æˆ˜ç•¥è¡Œä¸ºã€‚è¯¥æœºåˆ¶è¾¾åˆ°äº†æœ€åæƒ…å†µä¸‹ç¤¾ä¼šæƒ©ç½šçš„æœ€å°å€¼ï¼Œå¹¶å»ºç«‹äº†ä¸‰ä¸ªéš¾åº¦ç»“æœã€‚æœ€åï¼Œé€šè¿‡æ•´åˆå…¬ç†è°ˆåˆ¤çš„æ¦‚å¿µï¼Œè¯æ˜äº†è¯¥æœºåˆ¶æ”¯æŒæ›´å…¬å¹³çš„ç»“æœã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç ”ç©¶äº†åä½œå­¦ä¹ é—®é¢˜ä¸­mä¸ªä»£ç†å¦‚ä½•ä¼°è®¡å‘é‡Î¼çš„é—®é¢˜ã€‚</li>
<li>é€šè¿‡æ•°æ®äº¤æ¢é™ä½é‡‡æ ·æˆæœ¬å’Œä¼°è®¡è¯¯å·®ã€‚</li>
<li>è®¾è®¡äº†ä¸€ç§åä½œæœºåˆ¶ä»¥è§£å†³ä¸ªä½“ç†æ€§å’Œå…¬å¹³çš„é—®é¢˜ï¼ŒåŒæ—¶é˜²æ­¢æˆ˜ç•¥è¡Œä¸ºã€‚</li>
<li>è¯¥æœºåˆ¶è¾¾åˆ°äº†æœ€åæƒ…å†µä¸‹ç¤¾ä¼šæƒ©ç½šçš„æœ€å°å€¼çš„è¿‘ä¼¼å€¼ï¼Œå¹¶åœ¨æœ‰åˆ©æ¡ä»¶ä¸‹è¾¾åˆ°äº†è¿‘ä¼¼å€¼ã€‚</li>
<li>å»ºç«‹äº†ä¸‰ä¸ªéš¾åº¦ç»“æœï¼Œè¡¨æ˜ä¸å­˜åœ¨èƒ½ä¿è¯ä¸»å¯¼ç­–ç•¥å‡è¡¡ã€å¯¹æ‰€æœ‰ç­–ç•¥ç»„åˆéƒ½æ˜¯ä¸ªä½“ç†æ€§çš„æœºåˆ¶æˆ–é¿å…æœ€åæƒ…å†µä¸‹çš„ç¨³å®šæ€§ä»·æ ¼çš„æœºåˆ¶ã€‚</li>
<li>é€šè¿‡æ•´åˆå…¬ç†è°ˆåˆ¤çš„æ¦‚å¿µï¼Œè¯æ˜äº†è¯¥æœºåˆ¶æ”¯æŒæ›´å…¬å¹³çš„ç»“æœåˆ†é…ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2407.15881">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-d1c4e5892a29b9b1f26fb3a0b469676e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5cf0e53a79c13f8ec91316e1a0d8e94b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-38f70847f0db78bca5a472b6968b309d.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="Multi-Agent-Soft-Actor-Critic-with-Coordinated-Loss-for-Autonomous-Mobility-on-Demand-Fleet-Control"><a href="#Multi-Agent-Soft-Actor-Critic-with-Coordinated-Loss-for-Autonomous-Mobility-on-Demand-Fleet-Control" class="headerlink" title="Multi-Agent Soft Actor-Critic with Coordinated Loss for Autonomous   Mobility-on-Demand Fleet Control"></a>Multi-Agent Soft Actor-Critic with Coordinated Loss for Autonomous   Mobility-on-Demand Fleet Control</h2><p><strong>Authors:Zeno Woywood, Jasper I. Wiltfang, Julius Luy, Tobias Enders, Maximilian Schiffer</strong></p>
<p>We study a sequential decision-making problem for a profit-maximizing operator of an autonomous mobility-on-demand system. Optimizing a central operatorâ€™s vehicle-to-request dispatching policy requires efficient and effective fleet control strategies. To this end, we employ a multi-agent Soft Actor-Critic algorithm combined with weighted bipartite matching. We propose a novel vehicle-based algorithm architecture and adapt the criticâ€™s loss function to appropriately consider coordinated actions. Furthermore, we extend our algorithm to incorporate rebalancing capabilities. Through numerical experiments, we show that our approach outperforms state-of-the-art benchmarks by up to 12.9% for dispatching and up to 38.9% with integrated rebalancing. </p>
<blockquote>
<p>æˆ‘ä»¬å¯¹è‡ªä¸»æŒ‰éœ€å‡ºè¡Œç³»ç»Ÿä¸­è¿½æ±‚åˆ©æ¶¦æœ€å¤§åŒ–è¿è¥å•†çš„è¿ç»­å†³ç­–é—®é¢˜è¿›è¡Œäº†ç ”ç©¶ã€‚ä¼˜åŒ–ä¸­å¿ƒè¿è¥å•†çš„è½¦è¾†æ´¾é£ç­–ç•¥éœ€è¦é«˜æ•ˆä¸”æœ‰æ•ˆçš„è½¦é˜Ÿæ§åˆ¶ç­–ç•¥ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬é‡‡ç”¨äº†ä¸€ç§åŸºäºå¤šæ™ºèƒ½ä½“çš„Soft Actor-Criticç®—æ³•ï¼Œå¹¶ç»“åˆåŠ æƒäºŒåˆ†åŒ¹é…æ³•ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°å‹çš„è½¦è¾†ç®—æ³•æ¶æ„ï¼Œå¹¶é€‚åº”æ‰¹è¯„è€…çš„æŸå¤±å‡½æ•°ä»¥é€‚å½“è€ƒè™‘åè°ƒè¡ŒåŠ¨ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å°†æˆ‘ä»¬çš„ç®—æ³•æ‰©å±•åˆ°å…·å¤‡å†å¹³è¡¡èƒ½åŠ›çš„æƒ…å†µã€‚é€šè¿‡æ•°å€¼å®éªŒï¼Œæˆ‘ä»¬è¯æ˜äº†æˆ‘ä»¬çš„æ–¹æ³•åœ¨æ´¾é£æ–¹é¢ä¼˜äºæœ€æ–°åŸºå‡†æµ‹è¯•è¾¾12.9%ï¼Œåœ¨é›†æˆå†å¹³è¡¡æ–¹é¢æé«˜äº†é«˜è¾¾38.9%ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2404.06975v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>è¯¥æ–‡æœ¬ç ”ç©¶äº†åˆ©æ¶¦æœ€å¤§åŒ–çš„è‡ªä¸»æŒ‰éœ€å‡ºè¡Œç³»ç»Ÿçš„è¿è¥è€…çš„é¡ºåºå†³ç­–é—®é¢˜ã€‚ä¸ºäº†ä¼˜åŒ–ä¸­å¤®è¿è¥å•†çš„è½¦è¾†å¯¹è¯·æ±‚æ´¾é£æ”¿ç­–ï¼Œéœ€è¦é«˜æ•ˆä¸”æœ‰æ•ˆçš„è½¦é˜Ÿæ§åˆ¶ç­–ç•¥ã€‚ä¸ºæ­¤ï¼Œé‡‡ç”¨äº†ä¸€ç§å¤šä»£ç†Soft Actor-Criticç®—æ³•ä¸åŠ æƒäºŒåˆ†åŒ¹é…ç›¸ç»“åˆçš„æ–¹æ³•ã€‚æå‡ºäº†åŸºäºè½¦è¾†çš„æ–°å‹ç®—æ³•æ¶æ„ï¼Œå¹¶é€‚åº”äº†æ‰¹è¯„è€…çš„æŸå¤±å‡½æ•°ä»¥å……åˆ†è€ƒè™‘åè°ƒè¡ŒåŠ¨ã€‚æ­¤å¤–ï¼Œå°†ç®—æ³•æ‰©å±•åˆ°çº³å…¥å†å¹³è¡¡èƒ½åŠ›ã€‚æ•°å€¼å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨æ´¾é£æ–¹é¢çš„æ€§èƒ½ä¼˜äºç°æœ‰åŸºå‡†æµ‹è¯•é«˜è¾¾12.9%ï¼Œåœ¨é›†æˆå†å¹³è¡¡æ–¹é¢é«˜è¾¾38.9%ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç ”ç©¶äº†è‡ªä¸»æŒ‰éœ€å‡ºè¡Œç³»ç»Ÿçš„è¿è¥è€…çš„é¡ºåºå†³ç­–é—®é¢˜ã€‚</li>
<li>æå‡ºäº†ä¸€ç§å¤šä»£ç†Soft Actor-Criticç®—æ³•ç»“åˆåŠ æƒäºŒåˆ†åŒ¹é…çš„æ–¹æ³•æ¥è§£å†³è½¦è¾†æ´¾é£é—®é¢˜ã€‚</li>
<li>æå‡ºäº†åŸºäºè½¦è¾†çš„æ–°å‹ç®—æ³•æ¶æ„ã€‚</li>
<li>é€‚åº”äº†æ‰¹è¯„è€…çš„æŸå¤±å‡½æ•°ä»¥è€ƒè™‘åè°ƒè¡ŒåŠ¨ã€‚</li>
<li>å°†ç®—æ³•æ‰©å±•åˆ°çº³å…¥å†å¹³è¡¡èƒ½åŠ›ã€‚</li>
<li>æ•°å€¼å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨æ´¾é£å’Œé›†æˆå†å¹³è¡¡æ–¹é¢çš„æ€§èƒ½ä¼˜äºç°æœ‰æ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2404.06975">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-abbacc95583654dbcc32a1fcb2379771.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-0cb2fab8d612fd9d78ae269455d66ec7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6199253b40fb4e5ac5687b6bfa565c38.jpg" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-06-25/Agent/">https://kedreamix.github.io/Talk2Paper/Paper/2025-06-25/Agent/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Agent/">
                                    <span class="chip bg-color">Agent</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-06-25/Few-Shot/">
                    <div class="card-image">
                        
                        <img src="https://pica.zhimg.com/v2-e54bb752f5019594faa254a91ef1aed4.jpg" class="responsive-img" alt="Few-Shot">
                        
                        <span class="card-title">Few-Shot</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Few-Shot æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-06-25  Geometry-aware Distance Measure for Diverse Hierarchical Structures in   Hyperbolic Spaces
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-06-25
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Few-Shot/" class="post-category">
                                    Few-Shot
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Few-Shot/">
                        <span class="chip bg-color">Few-Shot</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-06-25/LLM/">
                    <div class="card-image">
                        
                        <img src="https://pica.zhimg.com/v2-713319f28adfe8dda6d102f914890a01.jpg" class="responsive-img" alt="LLM">
                        
                        <span class="card-title">LLM</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            LLM æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-06-25  Vision as a Dialect Unifying Visual Understanding and Generation via   Text-Aligned Representations
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-06-25
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/LLM/" class="post-category">
                                    LLM
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/LLM/">
                        <span class="chip bg-color">LLM</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">30930.2k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
