<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Agent">
    <meta name="description" content="Agent 方向最新论文已更新，请持续关注 Update in 2025-06-25  Audit &amp; Repair An Agentic Framework for Consistent Story Visualization   in Text-to-Image Diffusion Models">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Agent | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loader页面消失采用渐隐的方式*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logo出现动画 */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//使用渐隐的方法淡出loading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//强制显示loading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">嘘~  正在从服务器偷取页面 . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>首页</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>标签</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>分类</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>归档</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="搜索" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="深色/浅色模式" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			首页
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			标签
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			分类
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			归档
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-52b9af6db973294039e24f9bc0227e29.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Agent</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- 文章内容详情 -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Agent/">
                                <span class="chip bg-color">Agent</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Agent/" class="post-category">
                                Agent
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp;
                    2025-06-25
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>更新日期:&nbsp;&nbsp;
                    2025-07-06
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>文章字数:&nbsp;&nbsp;
                    16.6k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>阅读时长:&nbsp;&nbsp;
                    67 分
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>阅读次数:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>⚠️ 以下所有内容总结都来自于 大语言模型的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p>
</blockquote>
<h1 id="2025-06-25-更新"><a href="#2025-06-25-更新" class="headerlink" title="2025-06-25 更新"></a>2025-06-25 更新</h1><h2 id="Audit-Repair-An-Agentic-Framework-for-Consistent-Story-Visualization-in-Text-to-Image-Diffusion-Models"><a href="#Audit-Repair-An-Agentic-Framework-for-Consistent-Story-Visualization-in-Text-to-Image-Diffusion-Models" class="headerlink" title="Audit &amp; Repair: An Agentic Framework for Consistent Story Visualization   in Text-to-Image Diffusion Models"></a>Audit &amp; Repair: An Agentic Framework for Consistent Story Visualization   in Text-to-Image Diffusion Models</h2><p><strong>Authors:Kiymet Akdemir, Tahira Kazimi, Pinar Yanardag</strong></p>
<p>Story visualization has become a popular task where visual scenes are generated to depict a narrative across multiple panels. A central challenge in this setting is maintaining visual consistency, particularly in how characters and objects persist and evolve throughout the story. Despite recent advances in diffusion models, current approaches often fail to preserve key character attributes, leading to incoherent narratives. In this work, we propose a collaborative multi-agent framework that autonomously identifies, corrects, and refines inconsistencies across multi-panel story visualizations. The agents operate in an iterative loop, enabling fine-grained, panel-level updates without re-generating entire sequences. Our framework is model-agnostic and flexibly integrates with a variety of diffusion models, including rectified flow transformers such as Flux and latent diffusion models such as Stable Diffusion. Quantitative and qualitative experiments show that our method outperforms prior approaches in terms of multi-panel consistency. </p>
<blockquote>
<p>故事可视化已经成为一项流行的任务，通过生成视觉场景来描绘跨多个面板的叙事。在这个任务中的一个核心挑战是保持视觉一致性，特别是在角色和物体如何在故事中持续和演变方面。尽管扩散模型最近有了一些进展，但当前的方法往往无法保留关键的角色属性，从而导致叙事不一致。在这项工作中，我们提出了一种协作式多智能体框架，该框架能够自主识别、纠正和完善跨多面板故事可视化中的不一致性。智能体在一个迭代循环中运行，能够实现精细的面板级更新，而无需重新生成整个序列。我们的框架与各种扩散模型无关，可以灵活地与多种扩散模型集成，包括经过修正的流变压器（如Flux）和潜在扩散模型（如Stable Diffusion）。定量和定性实验表明，我们的方法在跨面板一致性方面优于以前的方法。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.18900v1">PDF</a> Project webpage: <a target="_blank" rel="noopener" href="https://auditandrepair.github.io/">https://auditandrepair.github.io/</a></p>
<p><strong>Summary</strong></p>
<p>故事可视化已成为一个热门任务，旨在生成多个面板来呈现叙事。其中的核心挑战是保持视觉一致性，特别是在角色和物体如何在故事中持续和演变方面。尽管扩散模型最近有所进展，但当前的方法往往无法保留关键角色属性，导致叙事不一致。在这项工作中，我们提出了一种协作的多智能体框架，该框架可自主识别、纠正和完善多面板故事可视化中的不一致性。智能体在迭代循环中运行，可实现精细的面板级更新，而无需重新生成整个序列。我们的框架与各种扩散模型兼容，包括经过修正的流变压器（如Flux）和潜在扩散模型（如Stable Diffusion）。定量和定性实验表明，我们的方法在跨面板一致性方面优于先前的方法。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>故事可视化旨在通过多个面板呈现叙事，维持视觉一致性是核心挑战。</li>
<li>当前方法难以保留关键角色属性，导致叙事不一致。</li>
<li>提出了一种协作的多智能体框架来识别和纠正多面板故事可视化中的不一致性。</li>
<li>智能体可在迭代循环中运行，实现精细的面板级更新。</li>
<li>框架与多种扩散模型兼容，包括流变压器和潜在扩散模型。</li>
<li>定量和定性实验证明该方法在跨面板一致性方面优于先前技术。</li>
<li>该方法有助于提高故事可视化的质量和观感。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.18900">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-29929cf271d4e60819d0a244f86092f9.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-f2b4cb6d451b978f05f24dfff2a1929e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-af5cb62242c3325bae1406b227fe34c1.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="GRAND-SLAM-Local-Optimization-for-Globally-Consistent-Large-Scale-Multi-Agent-Gaussian-SLAM"><a href="#GRAND-SLAM-Local-Optimization-for-Globally-Consistent-Large-Scale-Multi-Agent-Gaussian-SLAM" class="headerlink" title="GRAND-SLAM: Local Optimization for Globally Consistent Large-Scale   Multi-Agent Gaussian SLAM"></a>GRAND-SLAM: Local Optimization for Globally Consistent Large-Scale   Multi-Agent Gaussian SLAM</h2><p><strong>Authors:Annika Thomas, Aneesa Sonawalla, Alex Rose, Jonathan P. How</strong></p>
<p>3D Gaussian splatting has emerged as an expressive scene representation for RGB-D visual SLAM, but its application to large-scale, multi-agent outdoor environments remains unexplored. Multi-agent Gaussian SLAM is a promising approach to rapid exploration and reconstruction of environments, offering scalable environment representations, but existing approaches are limited to small-scale, indoor environments. To that end, we propose Gaussian Reconstruction via Multi-Agent Dense SLAM, or GRAND-SLAM, a collaborative Gaussian splatting SLAM method that integrates i) an implicit tracking module based on local optimization over submaps and ii) an approach to inter- and intra-robot loop closure integrated into a pose-graph optimization framework. Experiments show that GRAND-SLAM provides state-of-the-art tracking performance and 28% higher PSNR than existing methods on the Replica indoor dataset, as well as 91% lower multi-agent tracking error and improved rendering over existing multi-agent methods on the large-scale, outdoor Kimera-Multi dataset. </p>
<blockquote>
<p>三维高斯喷溅（Gaussian Splatting）技术已成为RGB-D视觉SLAM（Simultaneous Localization and Mapping）的场景表示的一种表达方式，但其在大型多智能体室外环境中的应用尚未得到探索。多智能体高斯SLAM（Simultaneous Localization and Mapping）是一种有前景的方法，可以快速探索并重建环境，提供可扩展的环境表示，但现有方法仅限于小型室内环境。为此，我们提出了基于多智能体密集SLAM的高斯重建（Gaussian Reconstruction via Multi-Agent Dense SLAM），简称GRAND-SLAM。这是一种协作式高斯喷溅SLAM方法，它集成了i)基于子图局部优化的隐式跟踪模块和ii)一种集成到姿态图优化框架中的机器人内外回路闭合方法。实验表明，GRAND-SLAM在Replica室内数据集上提供了最先进的跟踪性能和比现有方法高出28%的峰值信噪比（PSNR），以及在大型室外Kimera-Multi数据集上实现了91%的多智能体跟踪误差降低和渲染性能的提升。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.18885v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文探讨了将多智能体技术应用于大规模户外环境的视觉即时定位与地图构建（SLAM）问题。提出了一种名为GRAND-SLAM的协同高斯融合SLAM方法，通过引入局部优化子图的隐式跟踪模块和集成机器人间与机器人内部的闭环策略，实现了在大型环境中的卓越性能。实验表明，GRAND-SLAM在大型室外环境中的性能优于现有方法。相较于在室内环境中复制的现有数据集，GRAND-SLAM提供出色的追踪性能并实现了较高的峰值信噪比（PSNR）。同时，在大型室外数据集上，其多智能体追踪误差降低了约91%，渲染效果也得到了改善。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>介绍了三维高斯融合在大规模多智能体户外环境中的潜在应用。</li>
<li>提出了一种新的协同高斯融合SLAM方法——GRAND-SLAM。</li>
<li>GRAND-SLAM引入了隐式跟踪模块，基于局部优化子图实现。</li>
<li>GRAND-SLAM集成了机器人间与机器人内部的闭环策略。</li>
<li>实验表明GRAND-SLAM在大型室外环境中的性能优于现有方法。相较于室内环境数据集，其追踪性能和PSNR表现优越。</li>
<li>在大型室外数据集上，GRAND-SLAM的多智能体追踪误差降低了约91%，渲染效果得到改善。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.18885">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-9054e4c345eab596e4d061bbe486775d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-52b9af6db973294039e24f9bc0227e29.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ba88ddff43ef2cbb13b015a13ea93a22.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-6c18cbcbfe3e922ebae7c3b5dfff30d8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-781a0fd4554b18bebacec2c65e9509d2.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Understanding-Software-Engineering-Agents-A-Study-of-Thought-Action-Result-Trajectories"><a href="#Understanding-Software-Engineering-Agents-A-Study-of-Thought-Action-Result-Trajectories" class="headerlink" title="Understanding Software Engineering Agents: A Study of   Thought-Action-Result Trajectories"></a>Understanding Software Engineering Agents: A Study of   Thought-Action-Result Trajectories</h2><p><strong>Authors:Islem Bouzenia, Michael Pradel</strong></p>
<p>Large Language Model (LLM)-based agents are increasingly employed to automate complex software engineering tasks such as program repair and issue resolution. These agents operate by autonomously generating natural language thoughts, invoking external tools, and iteratively refining their solutions. Despite their widespread adoption, the internal decision-making processes of these agents remain largely unexplored, limiting our understanding of their operational dynamics and failure modes. In this paper, we present a large-scale empirical study of the thought-action-result trajectories of three state-of-the-art LLM-based agents: \textsc{RepairAgent}, \textsc{AutoCodeRover}, and \textsc{OpenHands}. We unify their interaction logs into a common format, capturing 120 trajectories and 2822 LLM interactions focused on program repair and issue resolution. Our study combines quantitative analyses of structural properties, action patterns, and token usage with qualitative assessments of reasoning coherence and feedback integration. We identify key trajectory characteristics such as iteration counts and token consumption, recurring action sequences, and the semantic coherence linking thoughts, actions, and their results. Our findings reveal behavioral motifs and anti-patterns that distinguish successful from failed executions, providing actionable insights for improving agent design, including prompting strategies, failure diagnosis, and anti-pattern detection. We release our dataset and annotation framework to support further research on transparent and robust autonomous software engineering agents. </p>
<blockquote>
<p>基于大型语言模型（LLM）的代理正越来越多地被用于自动化复杂的软件工程任务，如程序修复和问题解析。这些代理通过自主生成自然语言思想、调用外部工具并迭代优化其解决方案来运行。尽管这些代理得到了广泛应用，但其内部决策过程在很大程度上仍未被探索，这限制了我们对它们运行动态和故障模式的了解。在本文中，我们对三种最新基于LLM的代理（RepairAgent、AutoCodeRover和OpenHands）的思想-行动-结果轨迹进行了大规模实证研究。我们将它们的交互日志统一为通用格式，捕获了120条轨迹和2822次专注于程序修复和问题解析的LLM交互。我们的研究结合了结构属性、行动模式和令牌使用量的定量分析，以及对推理连贯性和反馈整合的定性评估。我们确定了关键轨迹特征，如迭代次数和令牌消耗、反复出现的行动序列，以及连接思想、行动和结果之间的语义连贯性。我们的研究结果揭示了区分成功执行和失败执行的行为模式和反模式，为改进代理设计提供了可操作的见解，包括提示策略、故障诊断和反模式检测。我们发布了我们的数据集和注释框架，以支持对透明和稳健的自主软件工程代理的进一步研究。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.18824v1">PDF</a> </p>
<p><strong>Summary</strong>：基于大型语言模型（LLM）的代理被广泛应用于自动化软件工程的复杂任务，如程序修复和问题解析。然而，它们的内部决策过程尚待探索，限制了我们对它们操作动态和失败模式的理解。本研究对三种最新LLM代理的“思维-行动-结果”轨迹进行了大规模实证研究，并揭示了其行为模式和失败模式的关键特征。研究有助于改进代理设计，并提供关于提示策略、故障诊断和反模式检测的见解。</p>
<p><strong>Key Takeaways</strong>：</p>
<ol>
<li>LLM代理广泛应用于自动化软件工程的程序修复和问题解析任务。</li>
<li>LLM代理的内部决策过程尚待探索，限制了对其操作动态和失败模式的理解。</li>
<li>本研究对三种LLM代理进行了大规模实证研究，涉及程序修复和问题解析的轨迹分析。</li>
<li>研究发现成功与失败执行的关键特征包括迭代次数、令牌消耗、重复的行动序列以及思维与行动之间的语义连贯性。</li>
<li>研究揭示了代理的行为模式和反模式，有助于改进代理设计。</li>
<li>研究提供了关于提示策略、故障诊断和反模式检测的见解。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.18824">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-e028b99219c1c3ac6f48ab5ae5fa23eb.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-223cf5c87ddbd9b3cd80cf2ceaa5a2c6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-769f0321f2616baa0da60122f6ffaad2.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="MARL-MambaContour-Unleashing-Multi-Agent-Deep-Reinforcement-Learning-for-Active-Contour-Optimization-in-Medical-Image-Segmentation"><a href="#MARL-MambaContour-Unleashing-Multi-Agent-Deep-Reinforcement-Learning-for-Active-Contour-Optimization-in-Medical-Image-Segmentation" class="headerlink" title="MARL-MambaContour: Unleashing Multi-Agent Deep Reinforcement Learning   for Active Contour Optimization in Medical Image Segmentation"></a>MARL-MambaContour: Unleashing Multi-Agent Deep Reinforcement Learning   for Active Contour Optimization in Medical Image Segmentation</h2><p><strong>Authors:Ruicheng Zhang, Yu Sun, Zeyu Zhang, Jinai Li, Xiaofan Liu, Au Hoi Fan, Haowei Guo, Puxin Yan</strong></p>
<p>We introduce MARL-MambaContour, the first contour-based medical image segmentation framework based on Multi-Agent Reinforcement Learning (MARL). Our approach reframes segmentation as a multi-agent cooperation task focused on generate topologically consistent object-level contours, addressing the limitations of traditional pixel-based methods which could lack topological constraints and holistic structural awareness of anatomical regions. Each contour point is modeled as an autonomous agent that iteratively adjusts its position to align precisely with the target boundary, enabling adaptation to blurred edges and intricate morphologies common in medical images. This iterative adjustment process is optimized by a contour-specific Soft Actor-Critic (SAC) algorithm, further enhanced with the Entropy Regularization Adjustment Mechanism (ERAM) which dynamically balance agent exploration with contour smoothness. Furthermore, the framework incorporates a Mamba-based policy network featuring a novel Bidirectional Cross-attention Hidden-state Fusion Mechanism (BCHFM). This mechanism mitigates potential memory confusion limitations associated with long-range modeling in state space models, thereby facilitating more accurate inter-agent information exchange and informed decision-making. Extensive experiments on five diverse medical imaging datasets demonstrate the state-of-the-art performance of MARL-MambaContour, highlighting its potential as an accurate and robust clinical application. </p>
<blockquote>
<p>我们介绍了MARL-MambaContour，这是基于多智能体强化学习（MARL）的首个轮廓基医疗图像分割框架。我们的方法将分割重新构建为一个多智能体合作任务，专注于生成拓扑一致的对象级轮廓，解决了传统像素级方法缺乏拓扑约束和整体结构感知解剖区域的局限性。每个轮廓点都被建模为自主智能体，可以迭代调整其位置以精确与目标边界对齐，从而适应医疗图像中常见的模糊边缘和复杂形态。这种迭代调整过程通过轮廓特定的柔软演员评论家（SAC）算法优化，进一步增强与熵正则化调整机制（ERAM），动态平衡智能体探索与轮廓平滑。此外，该框架采用基于Mamba的策略网络，并引入了一种新型的双向交叉注意隐藏状态融合机制（BCHFM）。该机制缓解了状态空间模型中长程建模可能带来的潜在内存混淆限制，从而促进了更准确的智能体间信息交换和决策。在五个不同的医学成像数据集上的广泛实验证明了MARL-MambaContour的卓越性能，突显了其在临床应用中作为准确且稳健的潜在优势。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.18679v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>MARL-MambaContour是一个基于多智能体强化学习（MARL）的轮廓基础医学图像分割框架。它将分割视为多智能体合作任务，侧重于生成拓扑一致的物体级轮廓，解决了传统像素级方法缺乏拓扑约束和整体结构感知的问题。该框架利用轮廓特定的Soft Actor-Critic算法和熵正则化调整机制，并通过Mamba策略网络和双向交叉关注隐藏状态融合机制提高准确性。在五个不同的医学成像数据集上的实验证明了其卓越性能。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>MARL-MambaContour是首个基于多智能体强化学习（MARL）的轮廓基础医学图像分割框架。</li>
<li>该方法将医学图像分割视为多智能体合作任务，侧重于生成拓扑一致的物体级轮廓。</li>
<li>解决了传统像素级方法缺乏拓扑约束和整体结构感知的问题。</li>
<li>轮廓调整过程中引入了轮廓特定的Soft Actor-Critic算法和熵正则化调整机制（ERAM）。</li>
<li>使用了Mamba策略网络来提高性能，包括一个新的双向交叉关注隐藏状态融合机制（BCHFM）。</li>
<li>该框架能够在处理医学图像中的模糊边缘和复杂形态时表现出强大的适应性。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.18679">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-1160180733a1f15701879c81a783397f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d7b8741f0392cde885ecb4ef733c361b.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="MCN-SLAM-Multi-Agent-Collaborative-Neural-SLAM-with-Hybrid-Implicit-Neural-Scene-Representation"><a href="#MCN-SLAM-Multi-Agent-Collaborative-Neural-SLAM-with-Hybrid-Implicit-Neural-Scene-Representation" class="headerlink" title="MCN-SLAM: Multi-Agent Collaborative Neural SLAM with Hybrid Implicit   Neural Scene Representation"></a>MCN-SLAM: Multi-Agent Collaborative Neural SLAM with Hybrid Implicit   Neural Scene Representation</h2><p><strong>Authors:Tianchen Deng, Guole Shen, Xun Chen, Shenghai Yuan, Hongming Shen, Guohao Peng, Zhenyu Wu, Jingchuan Wang, Lihua Xie, Danwei Wang, Hesheng Wang, Weidong Chen</strong></p>
<p>Neural implicit scene representations have recently shown promising results in dense visual SLAM. However, existing implicit SLAM algorithms are constrained to single-agent scenarios, and fall difficulties in large-scale scenes and long sequences. Existing NeRF-based multi-agent SLAM frameworks cannot meet the constraints of communication bandwidth. To this end, we propose the first distributed multi-agent collaborative neural SLAM framework with hybrid scene representation, distributed camera tracking, intra-to-inter loop closure, and online distillation for multiple submap fusion. A novel triplane-grid joint scene representation method is proposed to improve scene reconstruction. A novel intra-to-inter loop closure method is designed to achieve local (single-agent) and global (multi-agent) consistency. We also design a novel online distillation method to fuse the information of different submaps to achieve global consistency. Furthermore, to the best of our knowledge, there is no real-world dataset for NeRF-based&#x2F;GS-based SLAM that provides both continuous-time trajectories groundtruth and high-accuracy 3D meshes groundtruth. To this end, we propose the first real-world Dense slam (DES) dataset covering both single-agent and multi-agent scenarios, ranging from small rooms to large-scale outdoor scenes, with high-accuracy ground truth for both 3D mesh and continuous-time camera trajectory. This dataset can advance the development of the research in both SLAM, 3D reconstruction, and visual foundation model. Experiments on various datasets demonstrate the superiority of the proposed method in both mapping, tracking, and communication. The dataset and code will open-source on <a target="_blank" rel="noopener" href="https://github.com/dtc111111/mcnslam">https://github.com/dtc111111/mcnslam</a>. </p>
<blockquote>
<p>神经隐式场景表示在密集视觉SLAM中最近显示出有前景的结果。然而，现有的隐式SLAM算法仅限于单智能体场景，在大场景和长序列中遇到困难。基于NeRF的多智能体SLAM框架无法满足通信带宽的限制。为此，我们提出了第一个分布式多智能体协同神经SLAM框架，具有混合场景表示、分布式相机跟踪、内部到跨循环闭合以及多子图融合的在线蒸馏。提出了一种新的triplane网格联合场景表示方法，以提高场景重建的效果。设计了一种新颖的intra-to-inter循环闭合方法，以实现局部（单智能体）和全局（多智能体）的一致性。我们还设计了一种新颖的在线蒸馏方法，以融合不同子图的信息来实现全局一致性。此外，据我们所知，没有基于NeRF或GS的SLAM真实世界数据集能够提供连续时间轨迹的地面真实数据和高精度三维网格地面真实数据。为此，我们提出了第一个真实世界的密集SLAM（DES）数据集，涵盖单智能体和多智能体场景，从小房间到大规模户外场景，同时提供三维网格和连续时间相机轨迹的高精度地面真实数据。该数据集可以促进SLAM、三维重建和视觉基础模型的研究发展。在各种数据集上的实验表明，所提出的方法在映射、跟踪和通信方面都具有优越性。数据集和代码将在<a target="_blank" rel="noopener" href="https://github.com/dtc111111/mcnslam%E4%B8%8A%E5%BC%80%E6%BA%90%E3%80%82">https://github.com/dtc111111/mcnslam上开源。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.18678v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文提出了首个分布式多智能体协同神经网络SLAM框架，该框架具有混合场景表示、分布式相机跟踪、内外环闭合以及在线蒸馏多重子图融合等特点。提出一种新型的三平面网格联合场景表示方法，改进了场景重建。设计了一种新型的内外环闭合方法，实现了局部和全局的一致性。此外，还提出了一种在线蒸馏方法，融合不同子图的信息以实现全局一致性。缺乏针对NeRF或GS的SLAM现实数据集，为此推出了首个密集slam（DES）数据集，包含单智能体和多智能体场景，从小型房间到大型室外场景，同时提供高精度的三维网格和连续时间相机轨迹的真实数据。该数据集推动了SLAM、三维重建和视觉基础模型的研究发展。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>提出了首个分布式多智能体协同神经网络SLAM框架，适用于大规模场景和长序列。</li>
<li>采用了混合场景表示、分布式相机跟踪、内外环闭合和在线蒸馏技术，提高了场景重建的精度和效率。</li>
<li>提出了一种新型的三平面网格联合场景表示方法，改进了场景细节和连贯性的表现。</li>
<li>设计了内外环闭合方法，实现了局部和全局的一致性，提高了智能体之间的协作效率。</li>
<li>推出了首个密集slam（DES）数据集，包含多种场景，从小型房间到大型室外场景，同时提供高精度的三维网格和连续时间相机轨迹的真实数据。</li>
<li>DES数据集能推动SLAM、三维重建和视觉基础模型的研究发展。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.18678">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-52a3e1d1cc6e0225e987d15c27bb62e7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-56ca28009d5260476c56072194780a5b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-695d1a91a966a1f33935ee410979fcae.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e62f2974cdf37c29051ef447e844c4db.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Multi-Agent-Reinforcement-Learning-for-Inverse-Design-in-Photonic-Integrated-Circuits"><a href="#Multi-Agent-Reinforcement-Learning-for-Inverse-Design-in-Photonic-Integrated-Circuits" class="headerlink" title="Multi-Agent Reinforcement Learning for Inverse Design in Photonic   Integrated Circuits"></a>Multi-Agent Reinforcement Learning for Inverse Design in Photonic   Integrated Circuits</h2><p><strong>Authors:Yannik Mahlau, Maximilian Schier, Christoph Reinders, Frederik Schubert, Marco Bügling, Bodo Rosenhahn</strong></p>
<p>Inverse design of photonic integrated circuits (PICs) has traditionally relied on gradientbased optimization. However, this approach is prone to end up in local minima, which results in suboptimal design functionality. As interest in PICs increases due to their potential for addressing modern hardware demands through optical computing, more adaptive optimization algorithms are needed. We present a reinforcement learning (RL) environment as well as multi-agent RL algorithms for the design of PICs. By discretizing the design space into a grid, we formulate the design task as an optimization problem with thousands of binary variables. We consider multiple two- and three-dimensional design tasks that represent PIC components for an optical computing system. By decomposing the design space into thousands of individual agents, our algorithms are able to optimize designs with only a few thousand environment samples. They outperform previous state-of-the-art gradient-based optimization in both twoand three-dimensional design tasks. Our work may also serve as a benchmark for further exploration of sample-efficient RL for inverse design in photonics. </p>
<blockquote>
<p>光子集成电路（PIC）的逆向设计传统上依赖于基于梯度的优化。然而，这种方法容易陷入局部最小值，导致设计功能不佳。随着对光子集成电路解决现代硬件需求潜力的兴趣增加，需要通过光学计算来实现更多自适应优化算法的需求也在增加。我们提出了一个强化学习（RL）环境以及用于光子集成电路设计的多智能体强化学习算法。通过将设计空间离散化为网格，我们将设计任务制定为一个具有数千个二进制变量的优化问题。我们考虑多个代表光学计算系统光子集成电路组件的二维和三维设计任务。通过将设计空间分解为数千个独立智能体，我们的算法能够在仅使用数千个环境样本的情况下优化设计。它们在二维和三维设计任务中的表现均优于当前最先进的基于梯度的优化方法。我们的工作也可以作为进一步探索光子学逆向设计的样本高效强化学习的基准。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.18627v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>逆向设计光子集成电路（PICs）的传统方法主要依赖于基于梯度的优化。然而，这种方法容易陷入局部最小值，导致设计功能次优。随着对PICs兴趣的增加，为适应现代硬件需求并应用于光学计算，需要更多适应性优化算法。本研究提出了用于PIC设计的强化学习环境及多智能体强化学习算法。通过将设计空间离散化为网格，我们将设计任务公式化为具有数千个二进制变量的优化问题。研究考虑了代表光学计算系统组件的多个二维和三维设计任务。通过分解设计空间为数千个独立智能体，我们的算法能够在数千个环境样本中优化设计。它们在二维和三维设计任务中的表现均优于先前的最先进的基于梯度的优化。本研究也可为样本高效的强化学习在光子学逆向设计方面的进一步探索提供参考。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>传统光子集成电路的逆向设计主要依赖基于梯度的优化方法，但易陷入局部最小值。</li>
<li>随着光学计算的需求增长，需要更灵活的优化算法来设计光子集成电路。</li>
<li>研究人员提出了使用强化学习环境及多智能体强化学习算法进行PIC设计的新方法。</li>
<li>设计空间被离散化为网格，将设计任务转化为具有数千个二进制变量的优化问题。</li>
<li>该方法适用于多种二维和三维设计任务，代表光学计算系统的组件。</li>
<li>通过分解设计空间为独立智能体，该方法能在少量环境样本中优化设计。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.18627">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-e7db4db2f91eb04ee84668762994a96e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c67fcbc2599b70b21b7eaec2dc3d84b1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2665c8f584c8affb91f134132594c023.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Transformer-World-Model-for-Sample-Efficient-Multi-Agent-Reinforcement-Learning"><a href="#Transformer-World-Model-for-Sample-Efficient-Multi-Agent-Reinforcement-Learning" class="headerlink" title="Transformer World Model for Sample Efficient Multi-Agent Reinforcement   Learning"></a>Transformer World Model for Sample Efficient Multi-Agent Reinforcement   Learning</h2><p><strong>Authors:Azad Deihim, Eduardo Alonso, Dimitra Apostolopoulou</strong></p>
<p>We present the Multi-Agent Transformer World Model (MATWM), a novel transformer-based world model designed for multi-agent reinforcement learning in both vector- and image-based environments. MATWM combines a decentralized imagination framework with a semi-centralized critic and a teammate prediction module, enabling agents to model and anticipate the behavior of others under partial observability. To address non-stationarity, we incorporate a prioritized replay mechanism that trains the world model on recent experiences, allowing it to adapt to agents’ evolving policies. We evaluated MATWM on a broad suite of benchmarks, including the StarCraft Multi-Agent Challenge, PettingZoo, and MeltingPot. MATWM achieves state-of-the-art performance, outperforming both model-free and prior world model approaches, while demonstrating strong sample efficiency, achieving near-optimal performance in as few as 50K environment interactions. Ablation studies confirm the impact of each component, with substantial gains in coordination-heavy tasks. </p>
<blockquote>
<p>我们提出了多智能体Transformer世界模型（MATWM），这是一种基于transformer的新型世界模型，旨在支持向量和图像环境中的多智能体强化学习。MATWM结合了分散式想象框架、半集中式批评家和队友预测模块，使智能体能够在部分可观察的情况下建立他人行为的模型并进行预测。为了解决非平稳性问题，我们采用了优先级回放机制，该机制通过最近的经验训练世界模型，使其能够适应智能体不断变化的策略。我们在一系列基准测试上对MATWM进行了评估，包括星际争霸多智能体挑战、宠物动物园和熔炉。MATWM达到了最先进的技术性能水平，不仅超越了无模型和预先的世界模型方法，而且表现出强大的样本效率，在仅5万次的环境交互中就能达到接近最优的性能。消融研究证实了每个组件的影响，在协调密集的任务中取得了重大进展。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.18537v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>基于Transformer的世界模型在向量和图像环境中的多智能体强化学习应用摘要。提出一种新型的多智能体Transformer世界模型（MATWM），结合了分散式想象框架、半集中式批评家和队友预测模块，以在部分可观察的情况下建模和预测其他智能体的行为。采用优先回放机制应对非平稳性问题，根据最近经验训练世界模型以适应智能体策略的变化。在StarCraft多智能体挑战、PettingZoo和MeltingPot等多个基准测试中表现优异，实现了优于模型免费和先前世界模型的方法的先进性能，并表现出了很强的样本效率，仅在极少数环境交互（例如不到五万次）中实现近优性能。实验显示MATWM在多任务协调中发挥巨大优势。 </p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>MATWM是一种基于Transformer的多智能体强化学习世界模型。它将用于解决基于向量和图像环境中的智能体协作任务。此模型可模拟环境并进行自适应训练，以满足快速变化的情境和新的挑战。它结合了多种技术来增强性能，包括分散式想象框架、半集中式批评家和队友预测模块。这些技术使得智能体能够在有限的视野中了解和理解队友行为及行为动机，从而促进多智能体的协调性和交互能力。另外其改进训练算法的性能保证此模型的优秀表现得益于优先回放机制的应用，它使模型能够应对非平稳性问题并适应智能体的策略变化。其在多种任务挑战上展现出极高的性能和出色的样本效率表现突出的是其具备在不同任务上的快速学习和高效适应能力，同时在协调复杂的任务中具有较大优势，有效推进了对新型模型的发展和挑战的态度在不断加剧的世界要求更为精细的任务控制技巧的问题，并取得一系列先进的测试评价表现。</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.18537">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-f5309ebe745518542de8953088995809.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-7bf9d9562fcd5c2b80d30964bf8d8c51.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="GraspMAS-Zero-Shot-Language-driven-Grasp-Detection-with-Multi-Agent-System"><a href="#GraspMAS-Zero-Shot-Language-driven-Grasp-Detection-with-Multi-Agent-System" class="headerlink" title="GraspMAS: Zero-Shot Language-driven Grasp Detection with Multi-Agent   System"></a>GraspMAS: Zero-Shot Language-driven Grasp Detection with Multi-Agent   System</h2><p><strong>Authors:Quang Nguyen, Tri Le, Huy Nguyen, Thieu Vo, Tung D. Ta, Baoru Huang, Minh N. Vu, Anh Nguyen</strong></p>
<p>Language-driven grasp detection has the potential to revolutionize human-robot interaction by allowing robots to understand and execute grasping tasks based on natural language commands. However, existing approaches face two key challenges. First, they often struggle to interpret complex text instructions or operate ineffectively in densely cluttered environments. Second, most methods require a training or finetuning step to adapt to new domains, limiting their generation in real-world applications. In this paper, we introduce GraspMAS, a new multi-agent system framework for language-driven grasp detection. GraspMAS is designed to reason through ambiguities and improve decision-making in real-world scenarios. Our framework consists of three specialized agents: Planner, responsible for strategizing complex queries; Coder, which generates and executes source code; and Observer, which evaluates the outcomes and provides feedback. Intensive experiments on two large-scale datasets demonstrate that our GraspMAS significantly outperforms existing baselines. Additionally, robot experiments conducted in both simulation and real-world settings further validate the effectiveness of our approach. </p>
<blockquote>
<p>语言驱动抓取检测具有通过允许机器人理解和执行基于自然语言命令的抓取任务来革新人机交互的潜力。然而，现有方法面临两大挑战。首先，它们经常难以解释复杂的文本指令或在密集杂乱的环境中操作无效。其次，大多数方法需要一个训练或微调步骤来适应新领域，这限制了它们在现实世界应用中的生成。在本文中，我们介绍了用于语言驱动抓取检测的新型多智能体系统框架GraspMAS。GraspMAS被设计用于解决歧义并改善现实场景中的决策制定。我们的框架包括三个专业智能体：Planner负责策划复杂查询；Coder负责生成并执行源代码；Observer负责评估结果并提供反馈。在两项大规模数据集上的密集实验表明，我们的GraspMAS显著优于现有基线。此外，在模拟和真实环境中进行的机器人实验进一步验证了我们的方法的有效性。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.18448v1">PDF</a> 8 pages, accepted to IROS 2025</p>
<p><strong>Summary</strong></p>
<p>语言驱动抓取检测有潜力革新人机交互方式，让机器人通过自然语言命令理解和执行抓取任务。现有方法面临两大挑战：一是难以解读复杂文本指令或在密集杂乱环境中操作；二是多数方法需要训练或微调以适应新领域，限制了其在现实世界的广泛应用。本文介绍GraspMAS，一种用于语言驱动抓取检测的新多智能体系统框架。GraspMAS旨在解决歧义问题，提高现实场景中的决策能力。框架包含三个专业智能体：规划者负责策划复杂查询；编码者负责生成并执行源代码；观察者负责评估结果并提供反馈。大规模数据集上的实验显示，GraspMAS显著优于现有基线方法。在模拟和实际机器人实验中的验证也进一步证明了其有效性。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>语言驱动抓取检测有潜力革新人机交互方式。</li>
<li>当前方法面临解读复杂文本指令和杂乱环境操作两大挑战。</li>
<li>GraspMAS是一种多智能体系统框架，用于语言驱动抓取检测。</li>
<li>GraspMAS旨在解决歧义问题并提高现实场景中的决策能力。</li>
<li>GraspMAS包含规划者、编码者和观察者三个专业智能体。</li>
<li>实验证明GraspMAS在多个数据集上表现优于现有方法。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.18448">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-56881adc6768664e224466a13d891700.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a6141aeb608a04e9613d9f3e4c2d88bc.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c58ae9f5e95b57017ada52c3d10357ca.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-38bede20036d497d429f8530d3e1965a.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-e60f95d686d1a74571ce00a246f3e1d7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b67a20a589b3eb47404174b34f5c2eb1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c39a476990e4dd189f11e0b80a6ff320.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b46db713e70c08279333411f63ed8235.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Dynamic-Knowledge-Exchange-and-Dual-diversity-Review-Concisely-Unleashing-the-Potential-of-a-Multi-Agent-Research-Team"><a href="#Dynamic-Knowledge-Exchange-and-Dual-diversity-Review-Concisely-Unleashing-the-Potential-of-a-Multi-Agent-Research-Team" class="headerlink" title="Dynamic Knowledge Exchange and Dual-diversity Review: Concisely   Unleashing the Potential of a Multi-Agent Research Team"></a>Dynamic Knowledge Exchange and Dual-diversity Review: Concisely   Unleashing the Potential of a Multi-Agent Research Team</h2><p><strong>Authors:Weilun Yu, Shixiang Tang, Yonggui Huang, Nanqing Dong, Li Fan, Honggang Qi, Wei Liu, Xiaoli Diao, Xi Chen, Wanli Ouyang</strong></p>
<p>Scientific progress increasingly relies on effective collaboration among researchers, a dynamic that large language models (LLMs) have only begun to emulate. While recent LLM-based scientist agents show promise in autonomous scientific discovery, they often lack the interactive reasoning and evaluation mechanisms essential to real-world research. We propose IDVSCI (Internal Discussion and Vote SCIentists), a multi-agent framework built on LLMs that incorporates two key innovations: a Dynamic Knowledge Exchange mechanism enabling iterative feedback among agents, and a Dual-Diversity Review paradigm that simulates heterogeneous expert evaluation. These components jointly promote deeper reasoning and the generation of more creative and impactful scientific ideas. To evaluate the effectiveness and generalizability of our approach, we conduct experiments on two datasets: a widely used benchmark in computer science and a new dataset we introduce in the health sciences domain. Results show that IDVSCI consistently achieves the best performance across both datasets, outperforming existing systems such as AI Scientist and VIRSCI. These findings highlight the value of modeling interaction and peer review dynamics in LLM-based autonomous research. </p>
<blockquote>
<p>科技进步越来越依赖于研究者之间的有效协作，这一动态是大规模语言模型（LLM）才开始模仿的。虽然基于LLM的科学家代理人在自主科学发现方面显示出前景，但它们通常缺乏现实世界研究中必不可少的交互推理和评估机制。我们提出了IDVSCI（内部讨论与投票科学家），这是一个基于LLM的多代理框架，包含两项关键创新：动态知识交换机制，促进代理之间的迭代反馈；模拟异质专家评估的双重多样性审查范式。这些组件共同促进更深层次的推理和更具创造力和影响力的科学想法的产生。为了评估我们方法的有效性和通用性，我们在两个数据集上进行了实验：一个在计算机科学中广泛使用的基准测试和我们引入的一个健康科学领域的新数据集。结果表明，IDVSCI在两个数据集上均表现最佳，优于现有的AI科学家和VIRSCI系统。这些发现突显了在基于LLM的自主研究中建立交互和同行评审动态的价值。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.18348v1">PDF</a> </p>
<p><strong>摘要</strong></p>
<p>大语言模型 (LLM) 已开始模拟科学家间的协作方式以促进科学进步。新提出的基于LLM的科学家代理系统在自主科学发现方面显示出前景，但它们通常缺乏重要的交互推理和评估机制。本研究提出IDVSCI（内部讨论与投票科学家）多代理框架，该框架在LLM基础上构建，包含两个关键创新点：动态知识交换机制可实现代理间的迭代反馈，以及模拟不同专家评估的双元评估审查模式。这些共同促进了更深入的推理和更具创造力和影响力的科学想法的产生。通过计算机科学与卫生科学领域的数据集进行的实验表明，IDVSCI的性能表现优于现有系统如AI科学家和VIRSCI，证明了建模交互和同行评审动力学在基于LLM的自主研究中的价值。</p>
<p><strong>关键见解</strong></p>
<ul>
<li>科学进步愈发依赖于研究者间的有效协作，大语言模型开始模拟这种协作方式。</li>
<li>基于LLM的科学家代理系统在自主科学发现方面展现潜力，但需加强交互推理和评估机制。</li>
<li>IDVSCI框架引入动态知识交换机制实现代理间的迭代反馈。</li>
<li>IDVSCI采用双元评估审查模式，模拟异质专家评估。</li>
<li>实验结果显示IDVSCI在多个数据集上的性能优于现有系统。</li>
<li>IDVSCI建模交互和同行评审动力学对基于LLM的自主研究至关重要。</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.18348">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-d5ccd22af97efcbad1f4993104b64d33.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4e162a01693d49c0d688ca874d3cf114.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-efa41110fa85270a2f4182c508339648.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-493546ff1e561105c81320731215e461.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f5da51a400590bca905488fb58978ccd.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="From-RAG-to-Agentic-Validating-Islamic-Medicine-Responses-with-LLM-Agents"><a href="#From-RAG-to-Agentic-Validating-Islamic-Medicine-Responses-with-LLM-Agents" class="headerlink" title="From RAG to Agentic: Validating Islamic-Medicine Responses with LLM   Agents"></a>From RAG to Agentic: Validating Islamic-Medicine Responses with LLM   Agents</h2><p><strong>Authors:Mohammad Amaan Sayeed, Mohammed Talha Alam, Raza Imam, Shahab Saquib Sohail, Amir Hussain</strong></p>
<p>Centuries-old Islamic medical texts like Avicenna’s Canon of Medicine and the Prophetic Tibb-e-Nabawi encode a wealth of preventive care, nutrition, and holistic therapies, yet remain inaccessible to many and underutilized in modern AI systems. Existing language-model benchmarks focus narrowly on factual recall or user preference, leaving a gap in validating culturally grounded medical guidance at scale. We propose a unified evaluation pipeline, Tibbe-AG, that aligns 30 carefully curated Prophetic-medicine questions with human-verified remedies and compares three LLMs (LLaMA-3, Mistral-7B, Qwen2-7B) under three configurations: direct generation, retrieval-augmented generation, and a scientific self-critique filter. Each answer is then assessed by a secondary LLM serving as an agentic judge, yielding a single 3C3H quality score. Retrieval improves factual accuracy by 13%, while the agentic prompt adds another 10% improvement through deeper mechanistic insight and safety considerations. Our results demonstrate that blending classical Islamic texts with retrieval and self-evaluation enables reliable, culturally sensitive medical question-answering. </p>
<blockquote>
<p>像阿维森纳的《医典》和《先知医学》这样的具有数百年历史的伊斯兰医学文本，蕴含了丰富的预防护理、营养知识和整体疗法，但对许多人来说仍然难以接触和利用，在现代人工智能系统中也是如此。现有的语言模型基准测试主要集中在事实回忆或用户偏好上，从而在验证大规模文化基础的医学指导方面存在差距。我们提出了一种统一的评估流程Tibbe-AG，它将30个精心策划的先知医学问题与人工验证的补救措施相结合，并比较了三种大型语言模型（LLaMA-3、Mistral-7B、Qwen2-7B）在三种配置下的表现：直接生成、检索增强生成和科学自我批判过滤。每个答案然后由第二个大型语言模型作为评判者进行评估，得出一个单一的3C3H质量分数。检索可以提高事实准确性13%，而智能提示通过更深入的机制洞察力和安全考虑增加了额外的10%的改进。我们的结果表明，将古典伊斯兰文本与检索和自我评估相结合，可实现可靠且文化敏感的医疗问题回答。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.15911v2">PDF</a> Published at the 4th Muslims in Machine Learning (MusIML) Workshop   (ICML-25)</p>
<p><strong>Summary</strong></p>
<p>这篇文本讲述了伊斯兰医学古籍如阿维森纳的《医典》和先知医学中的预防保健、营养和整体疗法等内容，强调了这些资源对现代人工智能系统的价值及其可访问性的重要性。文章指出当前语言模型评估基准在事实检索和用户偏好上的局限性，提出一个统一的评估流程Tibbe-AG，将先知医学问题与人工验证的疗法相结合，评估了三个大型语言模型（LLaMA-3、Mistral-7B和Qwen2-7B）的表现。通过增设检索和自我批判过滤器，提高答案的事实准确性和深度机制洞察力。研究结果表明，结合古典伊斯兰文本、检索和自我评估可实现可靠、文化敏感的医疗问答。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>伊斯兰医学古籍包含丰富的预防保健、营养和整体疗法内容，但对许多人来说仍然难以访问并且在现代AI系统中未得到充分利用。</li>
<li>当前语言模型评估基准主要关注事实检索和用户偏好，忽视了文化背景下医疗指导的验证。</li>
<li>提出的Tibbe-AG评估流程将先知医学问题与人工验证的疗法相结合，评估大型语言模型的表现。</li>
<li>检索功能提高了答案的事实准确性。</li>
<li>通过增设自我批判过滤器，答案的洞察力和安全性得到提高。</li>
<li>结合古典伊斯兰文本与评估流程可实现文化敏感的医疗问答系统。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.15911">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-91d0c311fc75eb9a8ef50d88c3c956b5.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d0e0251b3dadcaf0322e028f8b5b8f09.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ea3db167a7bb55b8f70b096014a38351.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b66b0e5d61030d5860c8b481b8ba5cfe.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-6e5d7c5f035f35f7ffd1f095d368ab5b.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="OAgents-An-Empirical-Study-of-Building-Effective-Agents"><a href="#OAgents-An-Empirical-Study-of-Building-Effective-Agents" class="headerlink" title="OAgents: An Empirical Study of Building Effective Agents"></a>OAgents: An Empirical Study of Building Effective Agents</h2><p><strong>Authors:He Zhu, Tianrui Qin, King Zhu, Heyuan Huang, Yeyi Guan, Jinxiang Xia, Yi Yao, Hanhao Li, Ningning Wang, Pai Liu, Tianhao Peng, Xin Gui, Xiaowan Li, Yuhui Liu, Yuchen Eleanor Jiang, Jun Wang, Changwang Zhang, Xiangru Tang, Ge Zhang, Jian Yang, Minghao Liu, Xitong Gao, Jiaheng Liu, Wangchunshu Zhou</strong></p>
<p>Recently, Agentic AI has become an increasingly popular research field. However, we argue that current agent research practices lack standardization and scientific rigor, making it hard to conduct fair comparisons among methods. As a result, it is still unclear how different design choices in agent frameworks affect effectiveness, and measuring their progress remains challenging. In this work, we conduct a systematic empirical study on GAIA benchmark and BrowseComp to examine the impact of popular design choices in key agent components in a fair and rigorous manner. We find that the lack of a standard evaluation protocol makes previous works, even open-sourced ones, non-reproducible, with significant variance between random runs. Therefore, we introduce a more robust evaluation protocol to stabilize comparisons. Our study reveals which components and designs are crucial for effective agents, while others are redundant, despite seeming logical. Based on our findings, we build and open-source OAgents, a new foundation agent framework that achieves state-of-the-art performance among open-source projects. OAgents offers a modular design for various agent components, promoting future research in Agentic AI. </p>
<blockquote>
<p>近期，Agentic AI已成为越来越受欢迎的研究领域。然而，我们认为当前的agent研究实践缺乏标准化和科学严谨性，使得各种方法之间难以进行公平的比较。因此，尚不清楚agent框架中的不同设计选择如何影响效果，衡量其进展仍然具有挑战性。在这项工作中，我们对GAIA基准测试和BrowseComp进行了系统的实证研究，以公平严谨的方式检查了关键agent组件中流行设计选择的影响。我们发现，由于缺乏标准的评估协议，以前的工作（即使是开源的）也无法重现，随机运行之间存有明显差异。因此，我们引入了一个更稳健的评估协议来稳定比较。我们的研究表明，哪些组件和设计对于有效的agent至关重要，而其他组件尽管看似逻辑上合理，但却是多余的。基于我们的发现，我们构建并开源了OAgents，这是一个新的基础agent框架，在开源项目中实现了最先进的性能。OAgents为各种agent组件提供了模块化设计，促进了Agentic AI的未来研究。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.15741v2">PDF</a> 28 pages</p>
<p><strong>Summary</strong></p>
<p>近期，Agentic AI成为热门研究领域，但当前研究实践缺乏标准化和科学严谨性，导致方法间难以进行公平比较。本研究通过GAIA基准测试和BrowseComp进行实证研究，探讨关键代理组件中的流行设计选择的影响。研究发现缺乏标准评估协议导致先前工作不可复现，存在随机运行之间的显著差异。因此，我们引入更稳健的评估协议以稳定比较。研究揭示了哪些组件和设计对于有效代理至关重要，哪些虽然看似合理但却是冗余的。基于我们的发现，我们构建并开源了OAgents，一个全新的基础代理框架，在开源项目中实现最先进的性能。OAgents为各种代理组件提供模块化设计，促进Agentic AI的未来研究。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>当前Agentic AI研究领域缺乏标准化和科学严谨性，导致方法比较困难。</li>
<li>通过GAIA基准测试和BrowseComp实证研究，探讨了关键代理组件的设计选择的影响。</li>
<li>缺乏标准评估协议导致先前的工作不可复现，存在随机运行之间的显著差异。</li>
<li>引入更稳健的评估协议以稳定比较不同的代理设计。</li>
<li>研究揭示了哪些代理组件和设计至关重要，哪些设计是冗余的。</li>
<li>构建并开源了新的基础代理框架OAgents，实现开源项目中的最先进性能。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.15741">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-2b81b342c3259c3c571fa45a7fb2a3f5.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e4f5283171eea9d5000113a6400851bb.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-3382ea0e112d796d68fed973bc80fee6.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="SWE-Dev-Building-Software-Engineering-Agents-with-Training-and-Inference-Scaling"><a href="#SWE-Dev-Building-Software-Engineering-Agents-with-Training-and-Inference-Scaling" class="headerlink" title="SWE-Dev: Building Software Engineering Agents with Training and   Inference Scaling"></a>SWE-Dev: Building Software Engineering Agents with Training and   Inference Scaling</h2><p><strong>Authors:Haoran Wang, Zhenyu Hou, Yao Wei, Jie Tang, Yuxiao Dong</strong></p>
<p>Large language models (LLMs) have advanced rapidly from conversational problem solving to addressing real-world tasks involving tool use, such as software engineering (SWE). Recent LLM-powered toolkits, such as OpenAI Codex and Cursor, have offered end-to-end automation of the software development process. However, building effective SWE agents remains challenging due to the lack of high-quality training data and effective test cases. To address this issue, we present SWE-Dev, an SWE agent built upon open-source LLMs. First, we develop a robust pipeline to synthesize test cases for patch evaluation. Second, we scale up agent trajectories to construct the training data for building SWE-Dev. Experiments on the SWE-bench-Verified benchmark show that the SWE-Dev models can achieve top performance among all open SWE agents. Specifically, the success rates of the SWE-Dev 7B and 32B parameter models reach 23.4% and 36.6%, respectively, outperforming state-of-the-art open-source models. All code, models, and datasets are publicly available at <a target="_blank" rel="noopener" href="https://github.com/THUDM/SWE-Dev">https://github.com/THUDM/SWE-Dev</a>. </p>
<blockquote>
<p>大型语言模型（LLM）已经从解决对话问题迅速发展到处理涉及工具使用的现实世界任务，如软件工程（SWE）。最近的LLM驱动的工具包，如OpenAI的Codex和Cursor，已经提供了软件开发过程的端到端自动化。然而，由于缺少高质量的训练数据和有效的测试用例，构建有效的SWE代理仍然具有挑战性。为了解决这一问题，我们推出了SWE-Dev，一个基于开源LLM的SWE代理。首先，我们开发了一个稳健的管道来合成测试用例进行评估。其次，我们扩大了代理轨迹以构建SWE-Dev的训练数据。在SWE-bench-Verified基准测试上的实验表明，SWE-Dev模型在所有公开的SWE代理中都能达到顶尖性能。具体来说，SWE-Dev 7B和32B参数模型的成功率分别达到了23.4%和36.6%，超过了最新的开源模型。所有代码、模型和数据集均可在<a target="_blank" rel="noopener" href="https://github.com/THUDM/SWE-Dev%E5%85%AC%E5%BC%80%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/THUDM/SWE-Dev公开获取。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.07636v2">PDF</a> Accepted to Findings of ACL’25</p>
<p><strong>Summary</strong><br>大型语言模型（LLM）已从对话问题解决方案迅速发展到应对涉及工具使用的现实世界任务，如软件工程（SWE）。最近，OpenAI Codex和Cursor等工具包为软件开发流程提供了端到端的自动化。然而，构建有效的SWE代理面临缺乏高质量训练数据和有效测试用例的挑战。为解决此问题，我们推出了基于开源LLM的SWE-Dev代理。首先，我们开发了一个稳健的管道来合成用于补丁评估的测试用例。其次，我们扩大了代理轨迹以构建SWE-Dev的训练数据。在SWE-bench-Verified基准测试上的实验表明，SWE-Dev模型在所有开源SWE代理中名列前茅。特别是，SWE-Dev 7B和32B参数模型的成功率分别达到了23.4%和36.6%，超过了最新的开源模型。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>大型语言模型（LLM）已扩展到现实世界任务，如软件工程（SWE）。</li>
<li>近期LLM工具包如OpenAI Codex和Cursor推动了软件开发的自动化。</li>
<li>构建有效的SWE代理面临缺乏高质量训练数据和有效测试用例的挑战。</li>
<li>提出了SWE-Dev代理以解决这些问题，该代理基于开源LLM。</li>
<li>SWE-Dev通过开发用于补丁评估的测试用例的稳健管道来合成测试案例。</li>
<li>通过扩大代理轨迹构建了SWE-Dev的训练数据。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.07636">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-334527c2866619182819926cc2a7d6cd.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-86f8cfc0fc0c6204d8a5624806f77a3c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-6424d05b0be55d19bdf56335fe924c6a.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-b10278082d8d8096f0de2e4c16ef07f6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-fd5f10a7c59a8fcd57638d57f80fab4e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-96aa4f55130e04329420f7455b24eb97.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c5dd430bb4898c915a36f3958f07845d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-dcc9ac11be647158a2e73142a24fb745.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="A-Survey-on-Large-Language-Model-based-Human-Agent-Systems"><a href="#A-Survey-on-Large-Language-Model-based-Human-Agent-Systems" class="headerlink" title="A Survey on Large Language Model based Human-Agent Systems"></a>A Survey on Large Language Model based Human-Agent Systems</h2><p><strong>Authors:Henry Peng Zou, Wei-Chieh Huang, Yaozu Wu, Yankai Chen, Chunyu Miao, Hoang Nguyen, Yue Zhou, Weizhi Zhang, Liancheng Fang, Langzhou He, Yangning Li, Dongyuan Li, Renhe Jiang, Xue Liu, Philip S. Yu</strong></p>
<p>Recent advances in large language models (LLMs) have sparked growing interest in building fully autonomous agents. However, fully autonomous LLM-based agents still face significant challenges, including limited reliability due to hallucinations, difficulty in handling complex tasks, and substantial safety and ethical risks, all of which limit their feasibility and trustworthiness in real-world applications. To overcome these limitations, LLM-based human-agent systems (LLM-HAS) incorporate human-provided information, feedback, or control into the agent system to enhance system performance, reliability and safety. These human-agent collaboration systems enable humans and LLM-based agents to collaborate effectively by leveraging their complementary strengths. This paper provides the first comprehensive and structured survey of LLM-HAS. It clarifies fundamental concepts, systematically presents core components shaping these systems, including environment &amp; profiling, human feedback, interaction types, orchestration and communication, explores emerging applications, and discusses unique challenges and opportunities arising from human-AI collaboration. By consolidating current knowledge and offering a structured overview, we aim to foster further research and innovation in this rapidly evolving interdisciplinary field. Paper lists and resources are available at <a target="_blank" rel="noopener" href="https://github.com/HenryPengZou/Awesome-LLM-Based-Human-Agent-Systems">https://github.com/HenryPengZou/Awesome-LLM-Based-Human-Agent-Systems</a>. </p>
<blockquote>
<p>最近大型语言模型（LLM）的进展引发了人们对构建完全自主代理人的浓厚兴趣。然而，基于LLM的完全自主代理人仍然面临重大挑战，包括由于幻觉导致的可靠性有限、处理复杂任务的困难以及安全和伦理风险较高，所有这些因素都限制了它们在现实世界应用中的可行性和可信度。为了克服这些局限性，基于LLM的人机代理系统（LLM-HAS）将人类提供的信息、反馈或控制纳入代理系统，以提高系统性能、可靠性和安全性。这些人机协作系统通过利用人类和基于LLM的代理人的各自优势，使人类和基于LLM的代理人能够进行有效的协作。本文对LLM-HAS进行了第一次全面和系统的调查。阐明了基本概念，系统地介绍了构成这些系统的核心组件，包括环境分析、人类反馈、交互类型、编排和通信，探讨了新兴应用，并讨论了由人机协作产生的独特挑战和机遇。通过整合当前知识并提供结构化概述，我们的目标是促进这一迅速发展的跨学科领域的进一步研究和创新。论文列表和资源可通过<a target="_blank" rel="noopener" href="https://github.com/HenryPengZou/Awesome-LLM-Based-Human-Agent-Systems%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/HenryPengZou/Awesome-LLM-Based-Human-Agent-Systems获取。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.00753v3">PDF</a> Paper lists and resources are available at   <a target="_blank" rel="noopener" href="https://github.com/HenryPengZou/Awesome-LLM-Based-Human-Agent-Systems">https://github.com/HenryPengZou/Awesome-LLM-Based-Human-Agent-Systems</a></p>
<p><strong>Summary</strong><br>大型语言模型（LLM）为基础的全自主代理技术日益受到关注，但仍面临可靠性、处理复杂任务能力、安全和伦理风险等方面的挑战。为了克服这些限制，结合了人类提供的信息、反馈或控制的LLM人机协作系统（LLM-HAS）被提出，通过利用人类和LLM代理的互补优势以实现有效协作。本文是对LLM-HAS的首个全面结构化调查，阐述了基本概念和系统核心组件，包括环境分析、人类反馈、交互类型、协调和通信等。此外，本文探讨了人机协作带来的独特挑战和机遇，并整合了当前知识，为这一迅速发展的跨学科领域的研究和创新提供了基础。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLMs面临可靠性、复杂任务处理能力及安全和伦理风险方面的挑战。</li>
<li>LLM-HAS结合了人类与LLM代理的信息、反馈和控制，以提高系统性能、可靠性和安全性。</li>
<li>LLM-HAS实现了人类和LLM代理的有效协作，利用双方的互补优势。</li>
<li>本文是对LLM-HAS的全面结构化调查，阐述了其基本概念和系统核心组件。</li>
<li>LLM-HAS的环境分析、人类反馈、交互类型、协调和通信等要素被详细探讨。</li>
<li>人机协作带来独特的挑战和机遇。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.00753">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-445f7e17e9c667944d65e0eca0ab8023.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-82cba21d7f7ef039d86770fa7d47057f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9a6ba09dfab9e7e6522ab8bb6ee6d01f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-9887fb5d1c54c35aefa14aa27bc9c479.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-2b804404a395280eb9aa569eae1f75ae.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="Boosting-Virtual-Agent-Learning-and-Reasoning-A-Step-Wise-Multi-Dimensional-and-Generalist-Reward-Model-with-Benchmark"><a href="#Boosting-Virtual-Agent-Learning-and-Reasoning-A-Step-Wise-Multi-Dimensional-and-Generalist-Reward-Model-with-Benchmark" class="headerlink" title="Boosting Virtual Agent Learning and Reasoning: A Step-Wise,   Multi-Dimensional, and Generalist Reward Model with Benchmark"></a>Boosting Virtual Agent Learning and Reasoning: A Step-Wise,   Multi-Dimensional, and Generalist Reward Model with Benchmark</h2><p><strong>Authors:Bingchen Miao, Yang Wu, Minghe Gao, Qifan Yu, Wendong Bu, Wenqiao Zhang, Yunfei Li, Siliang Tang, Tat-Seng Chua, Juncheng Li</strong></p>
<p>The development of Generalist Virtual Agents (GVAs) has shown significant promise in autonomous task execution. However, current training paradigms face critical limitations, including reliance on outcome supervision and labor-intensive human annotations. To address these challenges, we propose Similar, a Step-Wise Multi-Dimensional Generalist Reward Model, which offers fine-grained signals for agent training and can choose better action for inference-time scaling. Specifically, we begin by systematically defining five dimensions for evaluating agent actions. Building on this framework, we design an MCTS-P algorithm to automatically collect and annotate step-wise, five-dimensional agent execution data. Using this data, we train Similar with the Triple-M strategy. Furthermore, we introduce the first benchmark in the virtual agent domain for step-wise, multi-dimensional reward model training and evaluation, named SRM. This benchmark consists of two components: SRMTrain, which serves as the training set for Similar, and SRMEval, a manually selected test set for evaluating the reward model. Experimental results demonstrate that Similar, through its step-wise, multi-dimensional assessment and synergistic gain, provides GVAs with effective intermediate signals during both training and inference-time scaling. The project is available at <a target="_blank" rel="noopener" href="https://github.com/antgroup/Similar">https://github.com/antgroup/Similar</a>. </p>
<blockquote>
<p>通用虚拟智能体（GVAs）的发展在自主任务执行方面展现出巨大潜力。然而，当前的训练模式面临重大局限，包括依赖结果监督和劳动密集型人工标注。为了应对这些挑战，我们提出了名为“Similar”的步级多维度通用奖励模型，该模型为智能体训练提供精细的信号，并可以在推理时选择更好的动作进行缩放。具体来说，我们首先系统地定义了五个维度来评估智能体的动作。在此基础上，我们设计了一种基于蒙特卡洛树搜索的算法（MCTS-P），该算法能够自动收集和标注智能体的步级、五维度执行数据。使用这些数据，我们用三重M策略训练Similar模型。此外，我们还引入了虚拟智能体领域首个用于步级多维度奖励模型训练和评价的基准数据集，名为SRM。该基准数据集包含两个组件：用于训练Similar的训练集SRMTrain和用于评估奖励模型的SRMEval手动选择测试集。实验结果表明，通过步级多维评估和协同增益，Similar为GVAs在训练和推理过程中提供了有效的中间信号。该项目可通过<a target="_blank" rel="noopener" href="https://github.com/antgroup/Similar%E8%8E%B7%E5%8F%96%E8%AE%BF%E9%97%AE%E3%80%82">https://github.com/antgroup/Similar获取访问。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.18665v2">PDF</a> Home page is available at <a target="_blank" rel="noopener" href="https://dcd-ant-similar.github.io/">https://dcd-ant-similar.github.io</a></p>
<p><strong>Summary</strong>：通用虚拟代理人（GVAs）在自主任务执行方面展现出巨大潜力，但其训练模式仍存在依赖结果监督和劳动密集型人工标注等局限性。为此，本文提出了一种名为Similar的逐步多维通用奖励模型，为代理训练提供精细信号，并在推理时间缩放时选择更好的行动。通过系统地定义五个维度来评估代理行动，设计MCTS-P算法自动收集和标注逐步的五维代理执行数据。使用这种数据，用Triple-M策略训练Similar。此外，本文引入了虚拟代理领域首个用于逐步多维奖励模型训练和评价的基准测试，名为SRM。实验结果表明，Similar通过逐步多维评估和协同增益为GVAs在训练和推理时间缩放期间提供有效的中间信号。</p>
<p><strong>Key Takeaways</strong>：</p>
<ol>
<li>Generalist Virtual Agents (GVAs) 在自主任务执行方面展现出巨大潜力，但现有训练模式存在局限性。</li>
<li>提出了名为Similar的逐步多维通用奖励模型，以精细的方式为代理训练提供信号，并在推理时选择最佳行动。</li>
<li>通过系统地定义五个维度来评估代理行动，为代理训练提供更全面的反馈。</li>
<li>设计了MCTS-P算法来自动收集和标注逐步的五维代理执行数据，增强训练效率。</li>
<li>使用Triple-M策略来训练Similar模型，提高了模型的性能。</li>
<li>引入了虚拟代理领域的首个基准测试SRM，用于逐步多维奖励模型训练和评估。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.18665">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-40ff1c5b339d65b9f973a772f0fd340a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-014d775d8c4cd24dbfb008d22d32be46.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5a375f7c65cb57b10b1ea70814161f63.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-6da3c1a88bf3b89106e38fc6c907708b.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="EmoAgent-A-Multi-Agent-Framework-for-Diverse-Affective-Image-Manipulation"><a href="#EmoAgent-A-Multi-Agent-Framework-for-Diverse-Affective-Image-Manipulation" class="headerlink" title="EmoAgent: A Multi-Agent Framework for Diverse Affective Image   Manipulation"></a>EmoAgent: A Multi-Agent Framework for Diverse Affective Image   Manipulation</h2><p><strong>Authors:Qi Mao, Haobo Hu, Yujie He, Difei Gao, Haokun Chen, Libiao Jin</strong></p>
<p>Affective Image Manipulation (AIM) aims to alter visual elements within an image to evoke specific emotional responses from viewers. However, existing AIM approaches rely on rigid \emph{one-to-one} mappings between emotions and visual cues, making them ill-suited for the inherently subjective and diverse ways in which humans perceive and express emotion.To address this, we introduce a novel task setting termed \emph{Diverse AIM (D-AIM)}, aiming to generate multiple visually distinct yet emotionally consistent image edits from a single source image and target emotion. We propose \emph{EmoAgent}, the first multi-agent framework tailored specifically for D-AIM. EmoAgent explicitly decomposes the manipulation process into three specialized phases executed by collaborative agents: a Planning Agent that generates diverse emotional editing strategies, an Editing Agent that precisely executes these strategies, and a Critic Agent that iteratively refines the results to ensure emotional accuracy. This collaborative design empowers EmoAgent to model \emph{one-to-many} emotion-to-visual mappings, enabling semantically diverse and emotionally faithful edits.Extensive quantitative and qualitative evaluations demonstrate that EmoAgent substantially outperforms state-of-the-art approaches in both emotional fidelity and semantic diversity, effectively generating multiple distinct visual edits that convey the same target emotion. </p>
<blockquote>
<p>情感图像操纵（AIM）旨在改变图像中的视觉元素，以激发观看者特定的情感反应。然而，现有的AIM方法依赖于情感与视觉线索之间的僵化“一对一”映射，这使得它们不适合人类感知和表达情感所具有的固有主观性和多样性。为了解决这一问题，我们引入了一种新型任务设置，称为多样化AIM（D-AIM），旨在从单个源图像和目标情感生成多个视觉上不同但情感上一致的图像编辑。我们提出了专门为D-AIM定制的第一个多代理框架EmoAgent。EmoAgent明确地将操纵过程分解为三个阶段，由协作代理执行：规划代理生成多种情感编辑策略，编辑代理精确执行这些策略，评论家代理迭代优化结果以确保情感准确性。这种协作设计使EmoAgent能够建立“一到多”的情感到视觉映射，从而实现语义多样且情感真实的编辑。广泛定量和定性的评估表明，EmoAgent在情感保真度和语义多样性方面大大优于现有先进技术，能够生成有效传达同一目标情感的多个不同视觉编辑。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.11290v3">PDF</a> </p>
<p><strong>Summary</strong>：<br>情感图像操作（AIM）旨在改变图像的视觉元素以激发观众特定的情感反应。然而，现有的AIM方法依赖于情绪与视觉线索之间的僵化的一一映射，这使得它们不适合人类感知和表达情绪的固有主观性和多样性。为了解决这个问题，我们引入了一个名为多样化AIM（D-AIM）的新任务设置，旨在从单个源图像和目标情绪生成多个视觉各异但情感一致的图像编辑。我们提出了专门针对D-AIM设计的第一个多代理框架EmoAgent。EmoAgent将操作过程明确分解为三个阶段，由协作代理执行：生成多种情感编辑策略的规划代理、精确执行这些策略编辑代理和不断对结果进行精细化以确保情感准确性的评论家代理。这种协作设计使EmoAgent能够模拟一对一的情绪到视觉映射，实现语义多样且情感真实的编辑。</p>
<p><strong>Key Takeaways</strong>：</p>
<ol>
<li>情感图像操作（AIM）通过改变图像的视觉元素来激发特定情感反应。</li>
<li>现有方法依赖于僵化的情绪与视觉线索的一一映射，缺乏主观性和多样性。</li>
<li>引入多样化AIM（D-AIM）任务设置，旨在从单个源图像生成多个情感一致的图像编辑。</li>
<li>提出多代理框架EmoAgent，包括规划、编辑和评论家代理，实现语义多样且情感真实的编辑。</li>
<li>EmoAgent通过模拟一一对应的情绪到视觉映射，突破现有方法限制。</li>
<li>EmoAgent在情感保真度和语义多样性方面显著优于现有方法。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.11290">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-cb7ce0d90ed5868d1cb1d74b7b5a1f36.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-0dcfe6c68c4bd63568fe0370ad6f3e7b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-75f30456eb8e46ed299b9bcea9c3f0ac.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-ad22a24c2ef1673238dac324e8ea23df.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-4f17878047b7fe872f0e69b8ea7650c8.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ef6ecd04a9a6751194831c20dcddb5f2.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="Collaborative-Mean-Estimation-Among-Heterogeneous-Strategic-Agents-Individual-Rationality-Fairness-and-Truthful-Contribution"><a href="#Collaborative-Mean-Estimation-Among-Heterogeneous-Strategic-Agents-Individual-Rationality-Fairness-and-Truthful-Contribution" class="headerlink" title="Collaborative Mean Estimation Among Heterogeneous Strategic Agents:   Individual Rationality, Fairness, and Truthful Contribution"></a>Collaborative Mean Estimation Among Heterogeneous Strategic Agents:   Individual Rationality, Fairness, and Truthful Contribution</h2><p><strong>Authors:Alex Clinton, Yiding Chen, Xiaojin Zhu, Kirthevasan Kandasamy</strong></p>
<p>We study a collaborative learning problem where $m$ agents aim to estimate a vector $\mu &#x3D;(\mu_1,\ldots,\mu_d)\in \mathbb{R}^d$ by sampling from associated univariate normal distributions ${\mathcal{N}(\mu_k, \sigma^2)}<em>{k\in[d]}$. Agent $i$ incurs a cost $c</em>{i,k}$ to sample from $\mathcal{N}(\mu_k, \sigma^2)$. Instead of working independently, agents can exchange data, collecting cheaper samples and sharing them in return for costly data, thereby reducing both costs and estimation error. We design a mechanism to facilitate such collaboration, while addressing two key challenges: ensuring individually rational (IR) and fair outcomes so all agents benefit, and preventing strategic behavior (e.g. non-collection, data fabrication) to avoid socially undesirable outcomes. We design a mechanism and an associated Nash equilibrium (NE) which minimizes the social penalty-sum of agents’ estimation errors and collection costs-while being IR for all agents. We achieve a $\mathcal{O}(\sqrt{m})$-approximation to the minimum social penalty in the worst case and an $\mathcal{O}(1)$-approximation under favorable conditions. Additionally, we establish three hardness results: no nontrivial mechanism guarantees (i) a dominant strategy equilibrium where agents report truthfully, (ii) is IR for every strategy profile of other agents, (iii) or avoids a worst-case $\Omega(\sqrt{m})$ price of stability in any NE. Finally, by integrating concepts from axiomatic bargaining, we demonstrate that our mechanism supports fairer outcomes than one which minimizes social penalty. </p>
<blockquote>
<p>我们研究了一个协作学习问题，其中m个代理旨在通过从相关的一元正态分布中采样来估计向量μ&#x3D;(μ_1,…,μ_d)∈ℝ^d。代理i从N(μk,σ^2)中采样会产生成本c_{i,k}。代理们不必独立工作，他们可以交换数据，收集更便宜的样本并共享昂贵的样本，从而降低成本和估计误差。我们设计了一种机制来促进这种合作，同时解决两个关键挑战：确保个人理性公平的结果使所有代理都能受益，并防止策略性行为（如不收集、制造虚假数据）以避免社会不希望看到的结果。我们设计了一种机制以及相关的纳什均衡（NE），该机制能在最小化代理估计误差和收集成本的社会惩罚的同时，对所有代理实现个体理性。在最坏的情况下，我们实现了对最小社会惩罚的O(√m)近似值，并在有利的条件下实现了O(1)近似值。此外，我们还确定了三个难度结果：没有任何非平凡机制能保证（i）存在一种优势策略均衡让代理人如实报告，（ii）对任何其他代理的策略组合都是个体理性的，（iii）在任何NE中避免最坏情况的Ω(√m)稳定性代价。最后，通过整合原则性谈判的概念，我们证明了我们的机制比最小化社会惩罚的机制更能支持更公平的结果。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2407.15881v2">PDF</a> ICML 2025</p>
<p><strong>Summary</strong><br>     在协作学习问题中，m个代理试图通过从相关的一元正态分布中采样来估计向量μ。代理通过交换数据来降低成本和估计误差。本文设计了一种协作机制，解决了两个关键问题：确保个体理性和公平的结果，防止战略行为。该机制达到了最坏情况下社会惩罚的最小值，并建立了三个难度结果。最后，通过整合公理谈判的概念，证明了该机制支持更公平的结果。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>研究了协作学习问题中m个代理如何估计向量μ的问题。</li>
<li>通过数据交换降低采样成本和估计误差。</li>
<li>设计了一种协作机制以解决个体理性和公平的问题，同时防止战略行为。</li>
<li>该机制达到了最坏情况下社会惩罚的最小值的近似值，并在有利条件下达到了近似值。</li>
<li>建立了三个难度结果，表明不存在能保证主导策略均衡、对所有策略组合都是个体理性的机制或避免最坏情况下的稳定性价格的机制。</li>
<li>通过整合公理谈判的概念，证明了该机制支持更公平的结果分配。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2407.15881">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-d1c4e5892a29b9b1f26fb3a0b469676e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5cf0e53a79c13f8ec91316e1a0d8e94b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-38f70847f0db78bca5a472b6968b309d.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="Multi-Agent-Soft-Actor-Critic-with-Coordinated-Loss-for-Autonomous-Mobility-on-Demand-Fleet-Control"><a href="#Multi-Agent-Soft-Actor-Critic-with-Coordinated-Loss-for-Autonomous-Mobility-on-Demand-Fleet-Control" class="headerlink" title="Multi-Agent Soft Actor-Critic with Coordinated Loss for Autonomous   Mobility-on-Demand Fleet Control"></a>Multi-Agent Soft Actor-Critic with Coordinated Loss for Autonomous   Mobility-on-Demand Fleet Control</h2><p><strong>Authors:Zeno Woywood, Jasper I. Wiltfang, Julius Luy, Tobias Enders, Maximilian Schiffer</strong></p>
<p>We study a sequential decision-making problem for a profit-maximizing operator of an autonomous mobility-on-demand system. Optimizing a central operator’s vehicle-to-request dispatching policy requires efficient and effective fleet control strategies. To this end, we employ a multi-agent Soft Actor-Critic algorithm combined with weighted bipartite matching. We propose a novel vehicle-based algorithm architecture and adapt the critic’s loss function to appropriately consider coordinated actions. Furthermore, we extend our algorithm to incorporate rebalancing capabilities. Through numerical experiments, we show that our approach outperforms state-of-the-art benchmarks by up to 12.9% for dispatching and up to 38.9% with integrated rebalancing. </p>
<blockquote>
<p>我们对自主按需出行系统中追求利润最大化运营商的连续决策问题进行了研究。优化中心运营商的车辆派遣策略需要高效且有效的车队控制策略。为此，我们采用了一种基于多智能体的Soft Actor-Critic算法，并结合加权二分匹配法。我们提出了一种新型的车辆算法架构，并适应批评者的损失函数以适当考虑协调行动。此外，我们将我们的算法扩展到具备再平衡能力的情况。通过数值实验，我们证明了我们的方法在派遣方面优于最新基准测试达12.9%，在集成再平衡方面提高了高达38.9%。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2404.06975v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>该文本研究了利润最大化的自主按需出行系统的运营者的顺序决策问题。为了优化中央运营商的车辆对请求派遣政策，需要高效且有效的车队控制策略。为此，采用了一种多代理Soft Actor-Critic算法与加权二分匹配相结合的方法。提出了基于车辆的新型算法架构，并适应了批评者的损失函数以充分考虑协调行动。此外，将算法扩展到纳入再平衡能力。数值实验表明，该方法在派遣方面的性能优于现有基准测试高达12.9%，在集成再平衡方面高达38.9%。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>研究了自主按需出行系统的运营者的顺序决策问题。</li>
<li>提出了一种多代理Soft Actor-Critic算法结合加权二分匹配的方法来解决车辆派遣问题。</li>
<li>提出了基于车辆的新型算法架构。</li>
<li>适应了批评者的损失函数以考虑协调行动。</li>
<li>将算法扩展到纳入再平衡能力。</li>
<li>数值实验表明，该方法在派遣和集成再平衡方面的性能优于现有方法。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2404.06975">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-abbacc95583654dbcc32a1fcb2379771.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-0cb2fab8d612fd9d78ae269455d66ec7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6199253b40fb4e5ac5687b6bfa565c38.jpg" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        文章作者:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        文章链接:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-06-25/Agent/">https://kedreamix.github.io/Talk2Paper/Paper/2025-06-25/Agent/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        版权声明:
                    </i>
                </span>
                <span class="reprint-info">
                    本博客所有文章除特別声明外，均采用
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    许可协议。转载请注明来源
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Agent/">
                                    <span class="chip bg-color">Agent</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;上一篇</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-06-25/Few-Shot/">
                    <div class="card-image">
                        
                        <img src="https://pica.zhimg.com/v2-e54bb752f5019594faa254a91ef1aed4.jpg" class="responsive-img" alt="Few-Shot">
                        
                        <span class="card-title">Few-Shot</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Few-Shot 方向最新论文已更新，请持续关注 Update in 2025-06-25  Geometry-aware Distance Measure for Diverse Hierarchical Structures in   Hyperbolic Spaces
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-06-25
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Few-Shot/" class="post-category">
                                    Few-Shot
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Few-Shot/">
                        <span class="chip bg-color">Few-Shot</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                下一篇&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-06-25/LLM/">
                    <div class="card-image">
                        
                        <img src="https://pica.zhimg.com/v2-713319f28adfe8dda6d102f914890a01.jpg" class="responsive-img" alt="LLM">
                        
                        <span class="card-title">LLM</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            LLM 方向最新论文已更新，请持续关注 Update in 2025-06-25  Vision as a Dialect Unifying Visual Understanding and Generation via   Text-Aligned Representations
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-06-25
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/LLM/" class="post-category">
                                    LLM
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/LLM/">
                        <span class="chip bg-color">LLM</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- 代码块功能依赖 -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- 代码语言 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- 代码块复制 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- 代码块收缩 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC 悬浮按钮. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* 修复文章卡片 div 的宽度. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // 切换TOC目录展开收缩的相关操作.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;站点总字数:&nbsp;<span
                        class="white-color">29580.4k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- 运行天数提醒. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // 区分是否有年份.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = '本站已运行 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                daysTip = '本站已運行 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = '本站已运行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = '本站已運行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="关注我的知乎: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">知</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- 搜索遮罩框 -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span>
            <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- 白天和黑夜主题 -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- 回到顶部按钮 -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // 只在桌面版网页启用特效
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- 雪花特效 -->
    

    <!-- 鼠标星星特效 -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--腾讯兔小巢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- 动态标签 -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Σ(っ °Д °;)っ诶，页面崩溃了嘛？", clearTimeout(st)) : (document.title = "φ(゜▽゜*)♪咦，又好了！", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
