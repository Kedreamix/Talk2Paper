<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Few-Shot">
    <meta name="description" content="Few-Shot æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-06-25  Geometry-aware Distance Measure for Diverse Hierarchical Structures in   Hyperbolic Spaces">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Few-Shot | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pica.zhimg.com/v2-e54bb752f5019594faa254a91ef1aed4.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Few-Shot</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Few-Shot/">
                                <span class="chip bg-color">Few-Shot</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Few-Shot/" class="post-category">
                                Few-Shot
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-06-25
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-07-06
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    12.6k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    51 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-06-25-æ›´æ–°"><a href="#2025-06-25-æ›´æ–°" class="headerlink" title="2025-06-25 æ›´æ–°"></a>2025-06-25 æ›´æ–°</h1><h2 id="Geometry-aware-Distance-Measure-for-Diverse-Hierarchical-Structures-in-Hyperbolic-Spaces"><a href="#Geometry-aware-Distance-Measure-for-Diverse-Hierarchical-Structures-in-Hyperbolic-Spaces" class="headerlink" title="Geometry-aware Distance Measure for Diverse Hierarchical Structures in   Hyperbolic Spaces"></a>Geometry-aware Distance Measure for Diverse Hierarchical Structures in   Hyperbolic Spaces</h2><p><strong>Authors:Pengxiang Li, Yuwei Wu, Zhi Gao, Xiaomeng Fan, Wei Wu, Zhipeng Lu, Yunde Jia, Mehrtash Harandi</strong></p>
<p>Learning in hyperbolic spaces has attracted increasing attention due to its superior ability to model hierarchical structures of data. Most existing hyperbolic learning methods use fixed distance measures for all data, assuming a uniform hierarchy across all data points. However, real-world hierarchical structures exhibit significant diversity, making this assumption overly restrictive. In this paper, we propose a geometry-aware distance measure in hyperbolic spaces, which dynamically adapts to varying hierarchical structures. Our approach derives the distance measure by generating tailored projections and curvatures for each pair of data points, effectively mapping them to an appropriate hyperbolic space. We introduce a revised low-rank decomposition scheme and a hard-pair mining mechanism to mitigate the computational cost of pair-wise distance computation without compromising accuracy. We present an upper bound on the low-rank approximation error using Talagrandâ€™s concentration inequality, ensuring theoretical robustness. Extensive experiments on standard image classification (MNIST, CIFAR-10 and CIFAR-100), hierarchical classification (5-level CIFAR-100), and few-shot learning tasks (mini-ImageNet, tiered-ImageNet) demonstrate the effectiveness of our method. Our approach consistently outperforms learning methods that use fixed distance measures, with notable improvements on few-shot learning tasks, where it achieves over 5% gains on mini-ImageNet. The results reveal that adaptive distance measures better capture diverse hierarchical structures, with visualization showing clearer class boundaries and improved prototype separation in hyperbolic spaces. </p>
<blockquote>
<p>åœ¨è¶…å‡ ä½•ç©ºé—´ä¸­çš„å­¦ä¹ å› å…¶å¯¹æ•°æ®ç»“æ„å±‚æ¬¡ç»“æ„çš„å»ºæ¨¡èƒ½åŠ›è€Œå—åˆ°è¶Šæ¥è¶Šå¤šçš„å…³æ³¨ã€‚å¤§å¤šæ•°ç°æœ‰çš„è¶…å‡ ä½•å­¦ä¹ æ–¹æ³•å¯¹æ‰€æœ‰æ•°æ®ä½¿ç”¨å›ºå®šçš„è·ç¦»åº¦é‡ï¼Œå‡è®¾æ‰€æœ‰æ•°æ®ç‚¹çš„å±‚æ¬¡ç»“æ„æ˜¯ç»Ÿä¸€çš„ã€‚ç„¶è€Œï¼Œç°å®ä¸–ç•Œçš„å±‚æ¬¡ç»“æ„è¡¨ç°å‡ºå¾ˆå¤§çš„å¤šæ ·æ€§ï¼Œä½¿å¾—è¿™ä¸ªå‡è®¾è¿‡äºé™åˆ¶ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§è¶…å‡ ä½•ç©ºé—´ä¸­çš„å‡ ä½•æ„ŸçŸ¥è·ç¦»åº¦é‡ï¼Œå®ƒèƒ½åŠ¨æ€é€‚åº”ä¸åŒçš„å±‚æ¬¡ç»“æ„ã€‚æˆ‘ä»¬çš„æ–¹æ³•é€šè¿‡ä¸ºæ¯å¯¹æ•°æ®ç‚¹ç”Ÿæˆå®šåˆ¶çš„æŠ•å½±å’Œæ›²ç‡æ¥æ¨å¯¼è·ç¦»åº¦é‡ï¼Œæœ‰æ•ˆåœ°å°†å®ƒä»¬æ˜ å°„åˆ°é€‚å½“çš„è¶…å‡ ä½•ç©ºé—´ã€‚æˆ‘ä»¬å¼•å…¥äº†ä¸€ç§æ”¹è¿›çš„ä½ç§©åˆ†è§£æ–¹æ¡ˆå’Œç¡¬å¯¹æŒ–æ˜æœºåˆ¶ï¼Œä»¥å‡è½»æˆå¯¹è·ç¦»è®¡ç®—çš„è®¡ç®—æˆæœ¬ï¼ŒåŒæ—¶ä¸æŸå¤±å‡†ç¡®æ€§ã€‚æˆ‘ä»¬ä½¿ç”¨Talagrandçš„æµ“åº¦ä¸ç­‰å¼ç»™å‡ºäº†ä½ç§©é€¼è¿‘è¯¯å·®çš„ä¸Šç•Œï¼Œç¡®ä¿äº†ç†è®ºä¸Šçš„ç¨³å¥æ€§ã€‚åœ¨æ ‡å‡†å›¾åƒåˆ†ç±»ï¼ˆMNISTã€CIFAR-10å’ŒCIFAR-100ï¼‰ã€å±‚æ¬¡åˆ†ç±»ï¼ˆ5çº§CIFAR-100ï¼‰å’Œå°‘æ ·æœ¬å­¦ä¹ ä»»åŠ¡ï¼ˆmini-ImageNetã€tiered-ImageNetï¼‰ä¸Šçš„å¤§é‡å®éªŒè¯æ˜äº†æˆ‘ä»¬çš„æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚æˆ‘ä»¬çš„æ–¹æ³•å§‹ç»ˆä¼˜äºä½¿ç”¨å›ºå®šè·ç¦»åº¦é‡çš„å­¦ä¹ æ–¹æ³•ï¼Œåœ¨å°‘æ ·æœ¬å­¦ä¹ ä»»åŠ¡ä¸Šæœ‰æ˜¾è‘—æ”¹è¿›ï¼Œåœ¨mini-ImageNetä¸Šå®ç°äº†è¶…è¿‡5%çš„å¢ç›Šã€‚ç»“æœè¡¨æ˜ï¼Œè‡ªé€‚åº”è·ç¦»åº¦é‡èƒ½æ›´å¥½åœ°æ•æ‰å¤šæ ·åŒ–çš„å±‚æ¬¡ç»“æ„ï¼Œå¯è§†åŒ–æ˜¾ç¤ºè¶…å‡ ä½•ç©ºé—´ä¸­çš„ç±»è¾¹ç•Œæ›´æ¸…æ™°ï¼ŒåŸå‹åˆ†ç¦»å¾—åˆ°æ”¹è¿›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.18533v1">PDF</a> 24 pages</p>
<p><strong>æ‘˜è¦</strong></p>
<p>å­¦ä¹ åœ¨åŒæ›²ç©ºé—´çš„æ–¹æ³•å› å…¶åœ¨æ¨¡æ‹Ÿæ•°æ®å±‚æ¬¡ç»“æ„æ–¹é¢çš„å“è¶Šèƒ½åŠ›è€Œå¤‡å—å…³æ³¨ã€‚ç„¶è€Œï¼Œå¤§å¤šæ•°ç°æœ‰æ–¹æ³•é‡‡ç”¨å›ºå®šè·ç¦»åº¦é‡æ‰€æœ‰æ•°æ®ï¼Œå‡è®¾æ‰€æœ‰æ•°æ®ç‚¹ä¹‹é—´å­˜åœ¨ç»Ÿä¸€çš„å±‚æ¬¡ç»“æ„ï¼Œè¿™åœ¨ç°å®ä¸–ç•Œä¸­å¹¶ä¸å¸¸è§ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§åŒæ›²ç©ºé—´ä¸­çš„å‡ ä½•æ„ŸçŸ¥è·ç¦»åº¦é‡ï¼Œèƒ½å¤ŸåŠ¨æ€é€‚åº”ä¸åŒçš„å±‚æ¬¡ç»“æ„ã€‚è¯¥æ–¹æ³•é€šè¿‡ä¸ºæ¯å¯¹æ•°æ®ç‚¹ç”Ÿæˆå®šåˆ¶æŠ•å½±å’Œæ›²ç‡æ¥æ¨å¯¼è·ç¦»åº¦é‡ï¼Œæœ‰æ•ˆåœ°å°†å…¶æ˜ å°„åˆ°é€‚å½“çš„åŒæ›²ç©ºé—´ã€‚åŒæ—¶ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§æ”¹è¿›çš„ä½ç§©åˆ†è§£æ–¹æ¡ˆå’Œç¡¬å¯¹æŒ–æ˜æœºåˆ¶ï¼Œä»¥å‡è½»æˆå¯¹è·ç¦»è®¡ç®—çš„è®¡ç®—æˆæœ¬è€Œä¸æŸå¤±å‡†ç¡®æ€§ã€‚åˆ©ç”¨Talagrandçš„æµ“åº¦ä¸ç­‰å¼ï¼Œæˆ‘ä»¬å¯¹ä½ç§©è¿‘ä¼¼è¯¯å·®ç»™å‡ºäº†ä¸Šç•Œï¼Œç¡®ä¿ç†è®ºç¨³å¥æ€§ã€‚åœ¨æ ‡å‡†å›¾åƒåˆ†ç±»ï¼ˆMNISTã€CIFAR-10å’ŒCIFAR-100ï¼‰ã€å±‚æ¬¡åˆ†ç±»ï¼ˆ5çº§CIFAR-100ï¼‰å’Œå°‘é•œå¤´å­¦ä¹ ä»»åŠ¡ï¼ˆmini-ImageNetã€tiered-ImageNetï¼‰ä¸Šçš„å¤§é‡å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•æ•ˆæœæ˜¾è‘—ã€‚ç›¸è¾ƒäºä½¿ç”¨å›ºå®šè·ç¦»åº¦é‡çš„å­¦ä¹ æ–¹æ³•ï¼Œæˆ‘ä»¬çš„æ–¹æ³•å§‹ç»ˆè¡¨ç°å‡ºè‰²ï¼Œç‰¹åˆ«æ˜¯åœ¨å°‘é•œå¤´å­¦ä¹ ä»»åŠ¡ä¸Šå®ç°äº†è¶…è¿‡5%çš„å¢ç›Šã€‚ç»“æœè¡¨æ˜ï¼Œè‡ªé€‚åº”è·ç¦»åº¦é‡èƒ½æ›´å¥½åœ°æ•æ‰å¤šæ ·åŒ–çš„å±‚æ¬¡ç»“æ„ï¼Œå¯è§†åŒ–æ˜¾ç¤ºç±»è¾¹ç•Œæ›´æ¸…æ™°ï¼ŒåŒæ›²ç©ºé—´ä¸­çš„åŸå‹åˆ†ç¦»å¾—åˆ°æ”¹å–„ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>åŒæ›²ç©ºé—´ä¸­çš„å­¦ä¹ å› å…¶å»ºæ¨¡æ•°æ®å±‚æ¬¡ç»“æ„çš„ä¼˜åŠ¿è€Œå—åˆ°å…³æ³¨ã€‚</li>
<li>ç°æœ‰æ–¹æ³•å‡å®šç»Ÿä¸€è·ç¦»åº¦é‡å¯¹æ‰€æœ‰æ•°æ®æœ‰æ•ˆï¼Œè¿™ä¸ç°å®ä¸–ç•Œçš„å¤šæ ·æ€§ä¸ç¬¦ã€‚</li>
<li>æœ¬æ–‡æå‡ºäº†ä¸€ç§å‡ ä½•æ„ŸçŸ¥è·ç¦»åº¦é‡ï¼Œèƒ½åŠ¨æ€é€‚åº”ä¸åŒçš„æ•°æ®å±‚æ¬¡ç»“æ„ã€‚</li>
<li>é€šè¿‡å®šåˆ¶æŠ•å½±å’Œæ›²ç‡ä¸ºæ•°æ®ç‚¹ç”Ÿæˆé€‚å½“çš„åŒæ›²ç©ºé—´æ˜ å°„ã€‚</li>
<li>å¼•å…¥æ”¹è¿›çš„ä½ç§©åˆ†è§£å’Œç¡¬å¯¹æŒ–æ˜æ¥å¹³è¡¡è®¡ç®—æ•ˆç‡å’Œå‡†ç¡®æ€§ã€‚</li>
<li>Talagrandçš„æµ“åº¦ä¸ç­‰å¼ç”¨äºç¡®ä¿ç†è®ºç¨³å¥æ€§å¹¶è®¾å®šä½ç§©è¿‘ä¼¼è¯¯å·®çš„ä¸Šé™ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.18533">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-6fbb6e2362f3be8e85465b27adc66c42.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-0ca71a4b69214a139557f1671e6675d3.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b8ddfde5274be73780fbb177a2c59bb9.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-65831cb642c889a2b1721c23f1712bd1.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="A-Set-to-Set-Distance-Measure-in-Hyperbolic-Space"><a href="#A-Set-to-Set-Distance-Measure-in-Hyperbolic-Space" class="headerlink" title="A Set-to-Set Distance Measure in Hyperbolic Space"></a>A Set-to-Set Distance Measure in Hyperbolic Space</h2><p><strong>Authors:Pengxiang Li, Wei Wu, Zhi Gao, Xiaomeng Fan, Peilin Yu, Yuwei Wu, Zhipeng Lu, Yunde Jia, Mehrtash Harandi</strong></p>
<p>We propose a hyperbolic set-to-set distance measure for computing dissimilarity between sets in hyperbolic space. While point-to-point distances in hyperbolic space effectively capture hierarchical relationships between data points, many real-world applications require comparing sets of hyperbolic data points, where the local structure and the global structure of the sets carry crucial semantic information. The proposed the \underline{h}yperbolic \underline{s}et-\underline{to}-\underline{s}et \underline{d}istance measure (HS2SD) integrates both global and local structural information: global structure through geodesic distances between Einstein midpoints of hyperbolic sets, and local structure through topological characteristics of the two sets. To efficiently compute topological differences, we prove that using a finite Thue-Morse sequence of degree and adjacency matrices can serve as a robust approximation to capture the topological structure of a set. In this case, by considering the topological differences, HS2SD provides a more nuanced understanding of the relationships between two hyperbolic sets. Empirical evaluation on entity matching, standard image classification, and few-shot image classification demonstrates that our distance measure outperforms existing methods by effectively modeling the hierarchical and complex relationships inherent in hyperbolic sets. </p>
<blockquote>
<p>æˆ‘ä»¬æå‡ºäº†ä¸€ç§åŒæ›²é›†è·ç¦»åº¦é‡æ–¹æ³•ï¼Œç”¨äºè®¡ç®—åŒæ›²ç©ºé—´ä¸­é›†åˆä¹‹é—´çš„ä¸ç›¸ä¼¼æ€§ã€‚è™½ç„¶åŒæ›²ç©ºé—´ä¸­çš„ç‚¹-ç‚¹è·ç¦»å¯ä»¥æœ‰æ•ˆåœ°æ•è·æ•°æ®ç‚¹ä¹‹é—´çš„å±‚æ¬¡å…³ç³»ï¼Œä½†åœ¨è®¸å¤šç°å®ä¸–ç•Œçš„åº”ç”¨ä¸­éœ€è¦æ¯”è¾ƒåŒæ›²æ•°æ®ç‚¹çš„é›†åˆï¼Œå…¶ä¸­é›†åˆçš„å±€éƒ¨ç»“æ„å’Œå…¨å±€ç»“æ„æºå¸¦é‡è¦çš„è¯­ä¹‰ä¿¡æ¯ã€‚æå‡ºçš„åŒæ›²é›†è·ç¦»åº¦é‡ï¼ˆHS2SDï¼‰ç»“åˆäº†å…¨å±€å’Œå±€éƒ¨ç»“æ„ä¿¡æ¯ï¼šé€šè¿‡åŒæ›²é›†åˆçš„çˆ±å› æ–¯å¦ä¸­ç‚¹ä¹‹é—´çš„æµ‹åœ°çº¿è·ç¦»æ¥ä½“ç°å…¨å±€ç»“æ„ï¼Œå¹¶é€šè¿‡ä¸¤ä¸ªé›†åˆçš„æ‹“æ‰‘ç‰¹å¾æ¥ä½“ç°å±€éƒ¨ç»“æ„ã€‚ä¸ºäº†æœ‰æ•ˆåœ°è®¡ç®—æ‹“æ‰‘å·®å¼‚ï¼Œæˆ‘ä»¬è¯æ˜ä½¿ç”¨æœ‰é™åº¦çš„Thue-Morseåºåˆ—å’Œåº¦ä¸é‚»æ¥çŸ©é˜µå¯ä»¥ä½œä¸ºä¸€ä¸ªç¨³å¥çš„è¿‘ä¼¼æ¥æ•æ‰é›†åˆçš„æ‹“æ‰‘ç»“æ„ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œé€šè¿‡è€ƒè™‘æ‹“æ‰‘å·®å¼‚ï¼ŒHS2SDå¯ä»¥æ›´æ·±å…¥åœ°ç†è§£ä¸¤ä¸ªåŒæ›²é›†ä¹‹é—´çš„å…³ç³»ã€‚åœ¨å®ä½“åŒ¹é…ã€æ ‡å‡†å›¾åƒåˆ†ç±»å’Œå°‘æ ·æœ¬å›¾åƒåˆ†ç±»ä¸Šçš„å®è¯ç ”ç©¶è¯æ˜ï¼Œæˆ‘ä»¬çš„è·ç¦»åº¦é‡æ–¹æ³•é€šè¿‡æœ‰æ•ˆåœ°å»ºæ¨¡åŒæ›²é›†ä¸­å›ºæœ‰çš„å±‚æ¬¡å’Œå¤æ‚å…³ç³»ï¼Œä¼˜äºç°æœ‰æ–¹æ³•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.18529v1">PDF</a> 24 pages</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºä¸€ç§åŸºäºåŒæ›²ç©ºé—´çš„é›†åˆé—´è·ç¦»åº¦é‡æ–¹æ³•ï¼Œç”¨äºè®¡ç®—é›†åˆä¹‹é—´çš„ä¸ç›¸ä¼¼æ€§ã€‚è¯¥æ–¹æ³•ç»“åˆäº†å…¨å±€å’Œå±€éƒ¨ç»“æ„ä¿¡æ¯ï¼Œé€šè¿‡çˆ±å› æ–¯å¦ä¸­ç‚¹ä¹‹é—´çš„æµ‹åœ°çº¿è·ç¦»æ•æ‰å…¨å±€ç»“æ„ï¼Œé€šè¿‡é›†åˆçš„æ‹“æ‰‘ç‰¹å¾æ•æ‰å±€éƒ¨ç»“æ„ã€‚é€šè¿‡åˆ©ç”¨æœ‰é™åº¦çš„Thue-Morseåºåˆ—å’Œé‚»æ¥çŸ©é˜µï¼Œè¯¥æ–¹æ³•èƒ½å¤Ÿé«˜æ•ˆè®¡ç®—æ‹“æ‰‘å·®å¼‚ï¼Œæ›´æ·±å…¥åœ°ç†è§£ä¸¤ä¸ªåŒæ›²é›†åˆä¹‹é—´çš„å…³ç³»ã€‚ç»éªŒè¯„ä¼°è¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å®ä½“åŒ¹é…ã€æ ‡å‡†å›¾åƒåˆ†ç±»å’Œå°‘æ ·æœ¬å›¾åƒåˆ†ç±»ä¸­è¡¨ç°ä¼˜å¼‚ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æå‡ºäº†ä¸€ç§åŸºäºåŒæ›²ç©ºé—´çš„é›†åˆé—´è·ç¦»åº¦é‡æ–¹æ³•ï¼ˆHS2SDï¼‰ã€‚</li>
<li>HS2SDç»“åˆäº†å…¨å±€å’Œå±€éƒ¨ç»“æ„ä¿¡æ¯ï¼Œé€šè¿‡çˆ±å› æ–¯å¦ä¸­ç‚¹çš„æµ‹åœ°çº¿è·ç¦»å’Œé›†åˆçš„æ‹“æ‰‘ç‰¹å¾æ¥å®ç°ã€‚</li>
<li>åˆ©ç”¨æœ‰é™Thue-Morseåºåˆ—å’Œé‚»æ¥çŸ©é˜µé«˜æ•ˆè®¡ç®—é›†åˆçš„æ‹“æ‰‘å·®å¼‚ã€‚</li>
<li>HS2SDæä¾›äº†å¯¹ä¸¤ä¸ªåŒæ›²é›†åˆä¹‹é—´å…³ç³»çš„æ›´æ·±å…¥ç†è§£ã€‚</li>
<li>åœ¨å®ä½“åŒ¹é…ã€æ ‡å‡†å›¾åƒåˆ†ç±»å’Œå°‘æ ·æœ¬å›¾åƒåˆ†ç±»ä¸­è¿›è¡Œäº†ç»éªŒè¯„ä¼°ã€‚</li>
<li>HS2SDåœ¨å¤æ‚å…³ç³»çš„å»ºæ¨¡ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œå°¤å…¶æ˜¯åœ¨æ•æ‰æ•°æ®çš„å±‚æ¬¡ç»“æ„æ–¹é¢ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.18529">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-78d8567880004118fff06af86cb00516.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3e694011254d8d0b83a97a5ba8616147.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-04dca32970ac1c1467488173b301f27f.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-43c71b8b1eaa4d391d0413ad1db7e2aa.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Fully-Few-shot-Class-incremental-Audio-Classification-Using-Multi-level-Embedding-Extractor-and-Ridge-Regression-Classifier"><a href="#Fully-Few-shot-Class-incremental-Audio-Classification-Using-Multi-level-Embedding-Extractor-and-Ridge-Regression-Classifier" class="headerlink" title="Fully Few-shot Class-incremental Audio Classification Using Multi-level   Embedding Extractor and Ridge Regression Classifier"></a>Fully Few-shot Class-incremental Audio Classification Using Multi-level   Embedding Extractor and Ridge Regression Classifier</h2><p><strong>Authors:Yongjie Si, Yanxiong Li, Jiaxin Tan, Qianhua He, Il-Youp Kwak</strong></p>
<p>In the task of Few-shot Class-incremental Audio Classification (FCAC), training samples of each base class are required to be abundant to train model. However, it is not easy to collect abundant training samples for many base classes due to data scarcity and high collection cost. We discuss a more realistic issue, Fully FCAC (FFCAC), in which training samples of both base and incremental classes are only a few. Furthermore, we propose a FFCAC method using a model which is decoupled into a multi-level embedding extractor and a ridge regression classifier. The embedding extractor consists of an encoder of audio spectrogram Transformer and a fusion module, and is trained in the base session but frozen in all incremental sessions. The classifier is updated continually in each incremental session. Results on three public datasets show that our method exceeds current methods in accuracy, and has advantage over most of them in complexity. The code is at <a target="_blank" rel="noopener" href="https://github.com/YongjieSi/MAR">https://github.com/YongjieSi/MAR</a>. </p>
<blockquote>
<p>åœ¨Few-shot Class-incremental Audio Classificationï¼ˆFCACï¼‰ä»»åŠ¡ä¸­ï¼Œéœ€è¦å„ç±»åŸºç¡€æ¨¡å‹çš„è®­ç»ƒæ ·æœ¬ä¸°å¯Œã€‚ç„¶è€Œï¼Œç”±äºæ•°æ®ç¨€ç¼ºå’Œæ”¶é›†æˆæœ¬é«˜æ˜‚ï¼Œæ”¶é›†è®¸å¤šåŸºç¡€ç±»çš„å¤§é‡è®­ç»ƒæ ·æœ¬å¹¶ä¸å®¹æ˜“ã€‚æˆ‘ä»¬è®¨è®ºäº†ä¸€ä¸ªæ›´å®é™…çš„é—®é¢˜ï¼Œå³å®Œå…¨FCACï¼ˆFFCACï¼‰ï¼Œå…¶ä¸­åŸºç¡€ç±»å’Œå¢é‡ç±»çš„è®­ç»ƒæ ·æœ¬éƒ½å¾ˆå°‘ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§FFCACæ–¹æ³•ï¼Œè¯¥æ–¹æ³•ä½¿ç”¨äº†ä¸€ä¸ªè¢«è§£è€¦ä¸ºå¤šçº§åµŒå…¥æå–å™¨å’Œå²­å›å½’åˆ†ç±»å™¨çš„æ¨¡å‹ã€‚åµŒå…¥æå–å™¨ç”±éŸ³é¢‘å…‰è°±å›¾Transformerçš„ç¼–ç å™¨å’Œèåˆæ¨¡å—ç»„æˆï¼Œå®ƒåœ¨åŸºç¡€ä¼šè¯ä¸­è¿›è¡Œè®­ç»ƒï¼Œä½†åœ¨æ‰€æœ‰å¢é‡ä¼šè¯ä¸­è¢«å†»ç»“ã€‚åˆ†ç±»å™¨ä¼šåœ¨æ¯ä¸ªå¢é‡ä¼šè¯ä¸­ä¸æ–­æ›´æ–°ã€‚åœ¨ä¸‰ä¸ªå…¬å…±æ•°æ®é›†ä¸Šçš„ç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨å‡†ç¡®åº¦ä¸Šè¶…è¿‡äº†å½“å‰çš„æ–¹æ³•ï¼Œåœ¨å¤æ‚æ€§ä¸Šå¤§å¤šæ•°æ–¹æ³•éƒ½æ— æ³•ä¸ä¹‹åŒ¹æ•Œã€‚ä»£ç ä½äº<a target="_blank" rel="noopener" href="https://github.com/YongjieSi">https://github.com/YongjieSi</a> å¯ä»¥é€šè¿‡è¯¥é“¾æ¥è·å–æ›´å¤šä¿¡æ¯ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.18406v1">PDF</a> Accepted for publication on Interspeech 2025. 5 pages, 6 tables, 7   figures</p>
<p><strong>Summary</strong><br>     é’ˆå¯¹å°‘æ ·æœ¬ç±»å¢é‡éŸ³é¢‘åˆ†ç±»ä»»åŠ¡ï¼ˆFCACï¼‰ï¼Œç”±äºæ•°æ®ç¨€ç¼ºå’Œæ”¶é›†æˆæœ¬é«˜æ˜‚ï¼Œéš¾ä»¥æ”¶é›†å„ç±»ä¸°å¯Œè®­ç»ƒæ ·æœ¬ã€‚ç ”ç©¶å›¢é˜Ÿæ¢è®¨æ›´ç°å®çš„é—®é¢˜â€”â€”å®Œå…¨FCACï¼ˆFFCACï¼‰ï¼Œå…¶ä¸­åŸºç¡€ç±»å’Œå¢é‡ç±»çš„è®­ç»ƒæ ·æœ¬éƒ½å¾ˆå°‘ã€‚ä»–ä»¬æå‡ºäº†ä¸€ç§FFCACæ–¹æ³•ï¼Œä½¿ç”¨åˆ†ç¦»æˆå¤šçº§åµŒå…¥æå–å™¨å’Œå²­å›å½’åˆ†ç±»å™¨çš„æ¨¡å‹ã€‚åµŒå…¥æå–å™¨ç”±éŸ³é¢‘é¢‘è°±å›¾Transformerçš„ç¼–ç å™¨å’Œèåˆæ¨¡å—ç»„æˆï¼Œåœ¨åŸºç¡€ä¼šè¯ä¸­è¿›è¡Œè®­ç»ƒï¼Œä½†åœ¨æ‰€æœ‰å¢é‡ä¼šè¯ä¸­å†»ç»“ã€‚åˆ†ç±»å™¨åœ¨æ¯ä¸ªå¢é‡ä¼šè¯ä¸­ä¸æ–­æ›´æ–°ã€‚åœ¨ä¸‰ä¸ªå…¬å…±æ•°æ®é›†ä¸Šçš„ç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨ç²¾åº¦ä¸Šè¶…è¿‡äº†å½“å‰çš„æ–¹æ³•ï¼Œå¹¶åœ¨å¤æ‚æ€§æ–¹é¢çš„å¤§å¤šæ•°æ–¹é¢éƒ½å…·æœ‰ä¼˜åŠ¿ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>FCACä»»åŠ¡éœ€è¦ä¸ºæ¯ä¸ªåŸºç¡€ç±»æä¾›ä¸°å¯Œçš„è®­ç»ƒæ ·æœ¬ï¼Œä½†ç”±äºæ•°æ®ç¨€ç¼ºå’Œæ”¶é›†æˆæœ¬é«˜æ˜‚ï¼Œè¿™ä¸€è¦æ±‚éš¾ä»¥å®ç°ã€‚</li>
<li>ç ”ç©¶å›¢é˜Ÿå¼•å…¥äº†å®Œå…¨FCACï¼ˆFFCACï¼‰é—®é¢˜ï¼Œå…¶ä¸­åŸºç¡€ç±»å’Œå¢é‡ç±»çš„è®­ç»ƒæ ·æœ¬éƒ½æœ‰é™ã€‚</li>
<li>æå‡ºäº†ä¸€ç§FFCACæ–¹æ³•ï¼Œä½¿ç”¨åˆ†ç¦»çš„å¤šçº§åµŒå…¥æå–å™¨å’Œå²­å›å½’åˆ†ç±»å™¨ã€‚</li>
<li>åµŒå…¥æå–å™¨ç”±éŸ³é¢‘é¢‘è°±å›¾Transformerçš„ç¼–ç å™¨å’Œèåˆæ¨¡å—ç»„æˆï¼Œå¹¶åœ¨åŸºç¡€ä¼šè¯ä¸­è®­ç»ƒï¼Œä¹‹åå†»ç»“ã€‚</li>
<li>åˆ†ç±»å™¨ä¼šåœ¨æ¯ä¸ªå¢é‡ä¼šè¯ä¸­æŒç»­æ›´æ–°ã€‚</li>
<li>åœ¨ä¸‰ä¸ªå…¬å…±æ•°æ®é›†ä¸Šçš„å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¯¥æ–¹æ³•åœ¨ç²¾åº¦ä¸Šæœ‰æ‰€è¶…è¶Šï¼Œå¹¶åœ¨å¤æ‚æ€§æ–¹é¢å…·ä¼˜åŠ¿ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.18406">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-5324f6562bf5595f5f6c95f0d9aff408.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-64e827df7ed53d2f4f6b47a535ef84c7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ed6cd290c2b83b8247f6f7b677336334.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2ac0ca256bd610032fc21c7ef2b269fe.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-fc45a70f58749844366b262e4e06c94c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2845bd5f2a6aa13d5ec07a83e36c9075.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-056bda7ec8f9960a6a0beb6bd61183a5.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Spatial-frequency-information-fusion-network-for-few-shot-learning"><a href="#Spatial-frequency-information-fusion-network-for-few-shot-learning" class="headerlink" title="Spatial frequency information fusion network for few-shot learning"></a>Spatial frequency information fusion network for few-shot learning</h2><p><strong>Authors:Wenqing Zhao, Guojia Xie, Han Pan, Biao Yang, Weichuan Zhang</strong></p>
<p>The objective of Few-shot learning is to fully leverage the limited data resources for exploring the latent correlations within the data by applying algorithms and training a model with outstanding performance that can adequately meet the demands of practical applications. In practical applications, the number of images in each category is usually less than that in traditional deep learning, which can lead to over-fitting and poor generalization performance. Currently, many Few-shot classification models pay more attention to spatial domain information while neglecting frequency domain information, which contains more feature information. Ignoring frequency domain information will prevent the model from fully exploiting feature information, which would effect the classification performance. Based on conventional data augmentation, this paper proposes an SFIFNet with innovative data preprocessing. The key of this method is enhancing the accuracy of image feature representation by integrating frequency domain information with spatial domain information. The experimental results demonstrate the effectiveness of this method in enhancing classification performance. </p>
<blockquote>
<p>å°‘æ ·æœ¬å­¦ä¹ çš„ç›®æ ‡æ˜¯å……åˆ†åˆ©ç”¨æœ‰é™çš„èµ„æºæ•°æ®ï¼Œé€šè¿‡åº”ç”¨ç®—æ³•è®­ç»ƒå‡ºæ€§èƒ½ä¼˜å¼‚çš„æ¨¡å‹ï¼Œä»¥æ»¡è¶³å®é™…åº”ç”¨çš„éœ€æ±‚ï¼Œå¹¶æ¢ç´¢æ•°æ®ä¸­çš„æ½œåœ¨å…³è”ã€‚åœ¨å®é™…åº”ç”¨ä¸­ï¼Œæ¯ä¸ªç±»åˆ«çš„å›¾åƒæ•°é‡é€šå¸¸æ¯”ä¼ ç»Ÿçš„æ·±åº¦å­¦ä¹ è¦å°‘ï¼Œè¿™å¯èƒ½å¯¼è‡´è¿‡æ‹Ÿåˆå’Œæ³›åŒ–æ€§èƒ½å·®çš„é—®é¢˜ã€‚ç›®å‰ï¼Œè®¸å¤šå°‘æ ·æœ¬åˆ†ç±»æ¨¡å‹è¿‡äºå…³æ³¨ç©ºé—´åŸŸä¿¡æ¯è€Œå¿½ç•¥äº†é¢‘ç‡åŸŸä¿¡æ¯ï¼Œåè€…åŒ…å«äº†æ›´å¤šçš„ç‰¹å¾ä¿¡æ¯ã€‚å¿½ç•¥é¢‘ç‡åŸŸä¿¡æ¯ä¼šé˜»æ­¢æ¨¡å‹å……åˆ†åˆ©ç”¨ç‰¹å¾ä¿¡æ¯ï¼Œä»è€Œå½±å“åˆ†ç±»æ€§èƒ½ã€‚æœ¬æ–‡åŸºäºä¼ ç»Ÿæ•°æ®å¢å¼ºæŠ€æœ¯ï¼Œæå‡ºäº†ä¸€ç§å…·æœ‰åˆ›æ–°æ•°æ®é¢„å¤„ç†åŠŸèƒ½çš„SFIFNetã€‚è¯¥æ–¹æ³•çš„å…³é”®æ˜¯é€šè¿‡æ•´åˆé¢‘ç‡åŸŸä¿¡æ¯å’Œç©ºé—´åŸŸä¿¡æ¯æ¥æé«˜å›¾åƒç‰¹å¾è¡¨ç¤ºçš„å‡†ç¡®æ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨æé«˜åˆ†ç±»æ€§èƒ½æ–¹é¢æ˜¯æœ‰æ•ˆçš„ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.18364v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†Few-shotå­¦ä¹ çš„ç›®æ ‡ï¼Œå³åˆ©ç”¨æœ‰é™çš„èµ„æºæ•°æ®æ¢ç´¢æ•°æ®ä¸­çš„æ½œåœ¨å…³è”ï¼Œå¹¶è®­ç»ƒå‡ºå…·æœ‰è‰¯å¥½æ€§èƒ½çš„æ¨¡å‹ï¼Œä»¥æ»¡è¶³å®é™…åº”ç”¨çš„éœ€æ±‚ã€‚é’ˆå¯¹å®é™…åº”ç”¨ä¸­æ¯ç±»å›¾åƒæ•°é‡é€šå¸¸è¾ƒå°‘çš„é—®é¢˜ï¼Œè®¸å¤šFew-shotåˆ†ç±»æ¨¡å‹æ›´æ³¨é‡ç©ºé—´åŸŸä¿¡æ¯è€Œå¿½è§†é¢‘ç‡åŸŸä¿¡æ¯ã€‚æœ¬æ–‡åŸºäºä¼ ç»Ÿæ•°æ®å¢å¼ºæ–¹æ³•æå‡ºSFIFNetï¼Œé€šè¿‡æ•´åˆé¢‘ç‡åŸŸä¿¡æ¯å’Œç©ºé—´åŸŸä¿¡æ¯æé«˜å›¾åƒç‰¹å¾è¡¨ç¤ºçš„å‡†ç¡®æ€§ï¼Œä»è€Œæé«˜åˆ†ç±»æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Few-shotå­¦ä¹ çš„ç›®æ ‡æ˜¯åˆ©ç”¨æœ‰é™æ•°æ®èµ„æºæ¢ç´¢æ•°æ®ä¸­çš„æ½œåœ¨å…³è”ï¼Œå¹¶è®­ç»ƒå‡ºé«˜æ€§èƒ½æ¨¡å‹æ»¡è¶³å®é™…åº”ç”¨éœ€æ±‚ã€‚</li>
<li>å®é™…åº”ç”¨ä¸­ï¼Œæ¯ç±»å›¾åƒæ•°é‡é€šå¸¸è¾ƒå°‘ï¼Œå¯èƒ½å¯¼è‡´è¿‡æ‹Ÿåˆå’Œæ³›åŒ–æ€§èƒ½å·®ã€‚</li>
<li>å½“å‰è®¸å¤šFew-shotåˆ†ç±»æ¨¡å‹æ›´æ³¨é‡ç©ºé—´åŸŸä¿¡æ¯ï¼Œè€Œå¿½è§†é¢‘ç‡åŸŸä¿¡æ¯ï¼Œåè€…åŒ…å«æ›´å¤šç‰¹å¾ä¿¡æ¯ã€‚</li>
<li>å¿½è§†é¢‘ç‡åŸŸä¿¡æ¯ä¼šé˜»ç¢æ¨¡å‹å……åˆ†åˆ©ç”¨ç‰¹å¾ä¿¡æ¯ï¼Œå½±å“åˆ†ç±»æ€§èƒ½ã€‚</li>
<li>æœ¬æ–‡æå‡ºSFIFNetï¼Œç»“åˆé¢‘ç‡åŸŸä¿¡æ¯å’Œç©ºé—´åŸŸä¿¡æ¯ï¼Œæé«˜å›¾åƒç‰¹å¾è¡¨ç¤ºçš„å‡†ç¡®æ€§ã€‚</li>
<li>åŸºäºä¼ ç»Ÿæ•°æ®å¢å¼ºæ–¹æ³•ï¼ŒSFIFNetèƒ½æœ‰æ•ˆæé«˜åˆ†ç±»æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.18364">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-10dccd61893a716d61ca4e9170ee8d87.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4f304a013724792ca2a9c11e529a0f2d.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-4aee7834f89b8c010fd51478d7af8651.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b72d02ced1f5edbc4144dd7f51a1490c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-49e597a2a353dff78d564c0a7d1607bd.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Deciphering-Emotions-in-Children-Storybooks-A-Comparative-Analysis-of-Multimodal-LLMs-in-Educational-Applications"><a href="#Deciphering-Emotions-in-Children-Storybooks-A-Comparative-Analysis-of-Multimodal-LLMs-in-Educational-Applications" class="headerlink" title="Deciphering Emotions in Children Storybooks: A Comparative Analysis of   Multimodal LLMs in Educational Applications"></a>Deciphering Emotions in Children Storybooks: A Comparative Analysis of   Multimodal LLMs in Educational Applications</h2><p><strong>Authors:Bushra Asseri, Estabraq Abdelaziz, Maha Al Mogren, Tayef Alhefdhi, Areej Al-Wabil</strong></p>
<p>Emotion recognition capabilities in multimodal AI systems are crucial for developing culturally responsive educational technologies, yet remain underexplored for Arabic language contexts where culturally appropriate learning tools are critically needed. This study evaluates the emotion recognition performance of two advanced multimodal large language models, GPT-4o and Gemini 1.5 Pro, when processing Arabic childrenâ€™s storybook illustrations. We assessed both models across three prompting strategies (zero-shot, few-shot, and chain-of-thought) using 75 images from seven Arabic storybooks, comparing model predictions with human annotations based on Plutchikâ€™s emotional framework. GPT-4o consistently outperformed Gemini across all conditions, achieving the highest macro F1-score of 59% with chain-of-thought prompting compared to Geminiâ€™s best performance of 43%. Error analysis revealed systematic misclassification patterns, with valence inversions accounting for 60.7% of errors, while both models struggled with culturally nuanced emotions and ambiguous narrative contexts. These findings highlight fundamental limitations in current modelsâ€™ cultural understanding and emphasize the need for culturally sensitive training approaches to develop effective emotion-aware educational technologies for Arabic-speaking learners. </p>
<blockquote>
<p>åœ¨å¤šæ¨¡æ€äººå·¥æ™ºèƒ½ç³»ç»Ÿä¸­ï¼Œæƒ…æ„Ÿè¯†åˆ«èƒ½åŠ›å¯¹äºå¼€å‘å…·æœ‰æ–‡åŒ–å›åº”æ€§çš„æ•™è‚²æŠ€æœ¯è‡³å…³é‡è¦ã€‚ç„¶è€Œï¼Œå¯¹äºé˜¿æ‹‰ä¼¯è¯­è¨€æƒ…å¢ƒä¸‹çš„æƒ…æ„Ÿè¯†åˆ«èƒ½åŠ›çš„ç ”ç©¶ä»ç„¶ä¸è¶³ï¼Œè€Œè¿™é‡Œæ°æ°æ€¥éœ€å¼€å‘é€‚åˆçš„æ–‡åŒ–å­¦ä¹ å·¥å…·ã€‚æœ¬ç ”ç©¶æ—¨åœ¨è¯„ä¼°ä¸¤ç§å…ˆè¿›çš„å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹GPT-4oå’ŒGemini 1.5 Proåœ¨å¤„ç†é˜¿æ‹‰ä¼¯å„¿ç«¥æ•…äº‹ä¹¦æ’å›¾æ—¶çš„æƒ…æ„Ÿè¯†åˆ«æ€§èƒ½ã€‚æˆ‘ä»¬ä½¿ç”¨äº†æ¥è‡ªä¸ƒæœ¬é˜¿æ‹‰ä¼¯æ•…äº‹ä¹¦çš„75å¼ å›¾ç‰‡ï¼Œé€šè¿‡ä¸‰ç§æç¤ºç­–ç•¥ï¼ˆé›¶æ¬¡å°„å‡»ã€å°‘æ¬¡å°„å‡»å’Œæ€ç»´é“¾ï¼‰ï¼Œå¯¹è¿™ä¸¤ç§æ¨¡å‹è¿›è¡Œäº†è¯„ä¼°ï¼Œå¹¶å°†æ¨¡å‹é¢„æµ‹ä¸äººç±»åŸºäºæ™®ç‰¹å¥‡æƒ…æ„Ÿæ¡†æ¶çš„æ³¨é‡Šè¿›è¡Œäº†æ¯”è¾ƒã€‚GPT-4oåœ¨æ‰€æœ‰æ¡ä»¶ä¸‹å‡è¡¨ç°ä¼˜äºGeminiï¼Œåœ¨æ€ç»´é“¾æç¤ºä¸‹è¾¾åˆ°æœ€é«˜çš„å®è§‚F1åˆ†æ•°ä¸º59%ï¼Œè€ŒGeminiçš„æœ€ä½³è¡¨ç°ä»…ä¸º43%ã€‚è¯¯å·®åˆ†ææ˜¾ç¤ºï¼Œç³»ç»Ÿå­˜åœ¨è¯¯åˆ†ç±»æ¨¡å¼ï¼Œå…¶ä¸­æƒ…ç»ªæ­£è´Ÿé¢ å€’çš„é”™è¯¯å 60.7%ï¼Œä¸”ä¸¤ä¸ªæ¨¡å‹åœ¨å¤„ç†å…·æœ‰æ–‡åŒ–ç»†å¾®å·®åˆ«çš„æƒ…ç»ªå’Œæ¨¡ç³Šå™äº‹ä¸Šä¸‹æ–‡æ—¶éƒ½é‡åˆ°äº†å›°éš¾ã€‚è¿™äº›å‘ç°çªæ˜¾äº†å½“å‰æ¨¡å‹åœ¨æ–‡åŒ–ç†è§£æ–¹é¢çš„åŸºæœ¬å±€é™æ€§ï¼Œå¹¶å¼ºè°ƒéœ€è¦é‡‡ç”¨æ–‡åŒ–æ•æ„Ÿçš„è®­ç»ƒæ–¹æ³•ï¼Œä»¥å¼€å‘é’ˆå¯¹é˜¿æ‹‰ä¼¯è¯­å­¦ä¹ è€…çš„æœ‰æ•ˆæƒ…æ„Ÿæ„ŸçŸ¥æ•™è‚²æŠ€æœ¯ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.18201v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>è¿™ç¯‡ç ”ç©¶æ¢è®¨äº†å…ˆè¿›çš„åŒæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹GPT-4oå’ŒGemini 1.5 Proåœ¨å¤„ç†é˜¿æ‹‰ä¼¯å„¿ç«¥æ•…äº‹ä¹¦æ’å›¾æ—¶çš„æƒ…ç»ªè¯†åˆ«æ€§èƒ½ã€‚è¯¥ç ”ç©¶åœ¨é›¶æ ·æœ¬ã€å°æ ·æœ¬å’Œæ€ç»´é“¾ä¸‰ç§æç¤ºç­–ç•¥ä¸‹å¯¹è¿™ä¸¤ä¸ªæ¨¡å‹è¿›è¡Œäº†è¯„ä¼°ï¼Œå¹¶ä¸äººç±»åŸºäºæ™®ç‰¹å¥‘å…‹çš„æƒ…æ„Ÿæ¡†æ¶çš„æ³¨é‡Šè¿›è¡Œäº†æ¯”è¾ƒã€‚GPT-4oåœ¨å„ç§æ¡ä»¶ä¸‹å‡è¡¨ç°ä¼˜äºGeminiï¼Œåœ¨æ€ç»´é“¾æç¤ºä¸‹è¾¾åˆ°æœ€é«˜çš„å®è§‚F1åˆ†æ•°ä¸º59%ï¼Œè€ŒGeminiçš„æœ€ä½³è¡¨ç°ä»…ä¸º43%ã€‚åˆ†ææ˜¾ç¤ºï¼Œæ¨¡å‹å­˜åœ¨ç³»ç»Ÿæ€§è¯¯åˆ†ç±»æ¨¡å¼ï¼Œæƒ…æ„Ÿä»·å€¼åè½¬çš„é”™è¯¯å æ¯”è¾ƒé«˜ï¼ŒåŒæ—¶å¯¹äºæ–‡åŒ–æƒ…æ„Ÿå’Œæ¨¡ç³Šå™äº‹ç¯å¢ƒçš„æŠŠæ¡å­˜åœ¨å›°éš¾ã€‚è¿™å‡¸æ˜¾äº†å½“å‰æ¨¡å‹åœ¨æ–‡åŒ–ç†è§£ä¸Šçš„å±€é™æ€§ï¼Œå¹¶å¼ºè°ƒäº†ä¸ºé˜¿æ‹‰ä¼¯è¯­å­¦ä¹ è€…å¼€å‘æƒ…æ„Ÿæ„ŸçŸ¥æ•™è‚²æŠ€æœ¯çš„éœ€æ±‚ï¼Œéœ€è¦é‡‡ç”¨æ–‡åŒ–æ•æ„Ÿçš„è®­ç»ƒæ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å…ˆè¿›çš„å¤šæ¨¡æ€è¯­è¨€æ¨¡å‹åœ¨é˜¿æ‹‰ä¼¯è¯­å¢ƒä¸‹çš„æƒ…ç»ªè¯†åˆ«èƒ½åŠ›ä»å¾…æå‡ã€‚</li>
<li>GPT-4oåœ¨å¤„ç†é˜¿æ‹‰ä¼¯å„¿ç«¥æ•…äº‹ä¹¦æ’å›¾æ—¶çš„æƒ…ç»ªè¯†åˆ«æ€§èƒ½ä¼˜äºGemini 1.5 Proã€‚</li>
<li>æ€ç»´é“¾æç¤ºç­–ç•¥å¯¹äºæé«˜æƒ…ç»ªè¯†åˆ«æ€§èƒ½å…·æœ‰æ˜¾è‘—æ•ˆæœã€‚</li>
<li>æ¨¡å‹å­˜åœ¨ç³»ç»Ÿæ€§çš„è¯¯åˆ†ç±»é—®é¢˜ï¼Œå…¶ä¸­æƒ…æ„Ÿä»·å€¼çš„åè½¬æ˜¯ä¸»è¦é”™è¯¯ä¹‹ä¸€ã€‚</li>
<li>æ¨¡å‹åœ¨ç†è§£å’Œå¤„ç†æ–‡åŒ–æƒ…æ„Ÿå’Œæ¨¡ç³Šå™äº‹ç¯å¢ƒæ–¹é¢å­˜åœ¨æŒ‘æˆ˜ã€‚</li>
<li>å½“å‰æ¨¡å‹åœ¨æ–‡åŒ–ç†è§£ä¸Šå­˜åœ¨å±€é™æ€§ï¼Œéœ€è¦é‡‡ç”¨æ–‡åŒ–æ•æ„Ÿçš„è®­ç»ƒæ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.18201">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-f486714cd9da87d1aa034a5db8ebcf35.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6787d28138e2b1d0195428acbc129d06.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="DExNet-Combining-Observations-of-Domain-Adapted-Critics-for-Leaf-Disease-Classification-with-Limited-Data"><a href="#DExNet-Combining-Observations-of-Domain-Adapted-Critics-for-Leaf-Disease-Classification-with-Limited-Data" class="headerlink" title="DExNet: Combining Observations of Domain Adapted Critics for Leaf   Disease Classification with Limited Data"></a>DExNet: Combining Observations of Domain Adapted Critics for Leaf   Disease Classification with Limited Data</h2><p><strong>Authors:Sabbir Ahmed, Md. Bakhtiar Hasan, Tasnim Ahmed, Md. Hasanul Kabir</strong></p>
<p>While deep learning-based architectures have been widely used for correctly detecting and classifying plant diseases, they require large-scale datasets to learn generalized features and achieve state-of-the-art performance. This poses a challenge for such models to obtain satisfactory performance in classifying leaf diseases with limited samples. This work proposes a few-shot learning framework, Domain-adapted Expert Network (DExNet), for plant disease classification that compensates for the lack of sufficient training data by combining observations of a number of expert critics. It starts with extracting the feature embeddings as â€˜observationsâ€™ from nine â€˜criticsâ€™ that are state-of-the-art pre-trained CNN-based architectures. These critics are â€˜domain adaptedâ€™ using a publicly available leaf disease dataset having no overlapping classes with the specific downstream task of interest. The observations are then passed to the â€˜Feature Fusion Blockâ€™ and finally to a classifier network consisting of Bi-LSTM layers. The proposed pipeline is evaluated on the 10 classes of tomato leaf images from the PlantVillage dataset, achieving promising accuracies of 89.06%, 92.46%, and 94.07%, respectively, for 5-shot, 10-shot, and 15-shot classification. Furthermore, an accuracy of 98.09+-0.7% has been achieved in 80-shot classification, which is only 1.2% less than state-of-the-art, allowing a 94.5% reduction in the training data requirement. The proposed pipeline also outperforms existing works on leaf disease classification with limited data in both laboratory and real-life conditions in single-domain, mixed-domain, and cross-domain scenarios. </p>
<blockquote>
<p>è™½ç„¶åŸºäºæ·±åº¦å­¦ä¹ çš„æ¶æ„å·²å¹¿æ³›åº”ç”¨äºæ¤ç‰©ç—…å®³çš„å‡†ç¡®æ£€æµ‹å’Œåˆ†ç±»ï¼Œä½†å®ƒä»¬éœ€è¦å¤§è§„æ¨¡æ•°æ®é›†æ¥å­¦ä¹ é€šç”¨ç‰¹å¾å¹¶å®ç°æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚è¿™ä¸ºåœ¨æ ·æœ¬æœ‰é™çš„æƒ…å†µä¸‹å¯¹å¶ç‰‡ç—…å®³è¿›è¡Œåˆ†ç±»è·å¾—æ»¡æ„æ€§èƒ½çš„æ¨¡å‹å¸¦æ¥äº†æŒ‘æˆ˜ã€‚è¿™é¡¹å·¥ä½œæå‡ºäº†ä¸€ä¸ªç”¨äºæ¤ç‰©ç—…å®³åˆ†ç±»çš„å°æ ·æœ¬å­¦ä¹ æ¡†æ¶â€”â€”Domain-adapted Expert Networkï¼ˆDExNetï¼‰ã€‚å®ƒé€šè¿‡ç»“åˆå¤šä¸ªä¸“ä¸šè¯„è®ºå®¶ï¼ˆå³ç–¾ç—…åˆ†ç±»é¢†åŸŸçš„ä¸“å®¶ï¼‰çš„è§‚å¯Ÿæ¥å¼¥è¡¥ç¼ºä¹è¶³å¤Ÿè®­ç»ƒæ•°æ®çš„ä¸è¶³ã€‚é¦–å…ˆï¼Œå®ƒä»ä¹ä¸ªæœ€å…ˆè¿›çš„é¢„è®­ç»ƒCNNæ¶æ„ä¸­æå–ç‰¹å¾åµŒå…¥ä½œä¸ºæ¥è‡ªä¸“å®¶çš„â€œè§‚å¯Ÿç»“æœâ€ã€‚è¿™äº›ä¸“å®¶é€šè¿‡ä½¿ç”¨ä¸ç‰¹å®šä¸‹æ¸¸ä»»åŠ¡æ²¡æœ‰é‡å ç±»çš„å…¬å¼€å¯ç”¨çš„å¶ç‰‡ç—…å®³æ•°æ®é›†è¿›è¡Œâ€œåŸŸé€‚åº”â€ã€‚è¿™äº›è§‚å¯Ÿç»“æœç„¶åä¼ é€’ç»™â€œç‰¹å¾èåˆå—â€ï¼Œå¹¶æœ€ç»ˆä¼ é€’ç»™ç”±åŒå‘LSTMå±‚ç»„æˆçš„åˆ†ç±»å™¨ç½‘ç»œã€‚è¯¥ç®¡é“åœ¨PlantVillageæ•°æ®é›†çš„ç•ªèŒ„å¶ç‰‡å›¾åƒä¸­çš„åä¸ªç±»åˆ«ä¸Šè¿›è¡Œäº†è¯„ä¼°ï¼Œåœ¨5é•œå¤´ã€10é•œå¤´å’Œ15é•œå¤´çš„åˆ†ç±»ä¸­åˆ†åˆ«å–å¾—äº†ä»¤äººé¼“èˆçš„89.06ï¼…ã€92.46ï¼…å’Œ94.07ï¼…çš„å‡†ç¡®ç‡ã€‚æ­¤å¤–ï¼Œåœ¨æ‹¥æœ‰æ›´åŠ å¤æ‚å’Œå¤šå…ƒçš„ç–¾ç—…ç±»å‹å’Œæ¡ä»¶çš„å®è·µä¸­æˆ–æ¡ä»¶ç‰¹æ®Šçš„çŠ¶å†µä¸‹ï¼Œé€šè¿‡ç»“åˆå¤šä¸ªé¢†åŸŸçš„æ•°æ®é›†è¿›è¡Œè®­ç»ƒï¼Œè¯¥ç®¡é“åœ¨80é•œå¤´åˆ†ç±»ä¸­å®ç°äº†é«˜è¾¾98.09ï¼…Â±0.7ï¼…çš„å‡†ç¡®ç‡ï¼Œä¸æœ€æ–°æŠ€æœ¯ç›¸æ¯”ä»…é™ä½äº†çº¦ç™¾åˆ†ä¹‹ä¸€ç‚¹äºŒï¼…ï¼Œæ˜¾è‘—å‡å°‘äº†é«˜è¾¾çº¦ç™¾åˆ†ä¹‹ä¹åå››ç‚¹äº”çš„è®­ç»ƒæ•°æ®éœ€æ±‚ã€‚æå‡ºçš„ç®¡é“ä¹Ÿåœ¨å®éªŒå®¤å’Œå®é™…æ¡ä»¶ä¸‹çš„å•åŸŸã€æ··åˆåŸŸå’Œè·¨åŸŸåœºæ™¯ä¸­è¡¨ç°ä¼˜äºç°æœ‰é’ˆå¯¹å¶ç‰‡ç—…å®³åˆ†ç±»æœ‰é™æ•°æ®çš„ç°æœ‰ä½œå“ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.18173v1">PDF</a> Submitted to ACPR Springer, 15 pages, 1 Figure, 7 Tables, and lots of   efforts :)</p>
<p><strong>Summary</strong><br>    æœ¬ç ”ç©¶æå‡ºä¸€ç§åŸºäºå°æ ·æœ¬å­¦ä¹ çš„Domain-adapted Expert Networkï¼ˆDExNetï¼‰æ¡†æ¶ï¼Œç”¨äºæ¤ç‰©ç—…å®³åˆ†ç±»ã€‚è¯¥æ¡†æ¶é€šè¿‡ç»“åˆå¤šä¸ªä¸“å®¶çš„è§‚å¯Ÿæ¥å¼¥è¡¥è®­ç»ƒæ•°æ®ä¸è¶³çš„é—®é¢˜ã€‚å®ƒåœ¨å…¬å¼€çš„æ¤ç‰©å¶ç‰‡ç—…å®³æ•°æ®é›†ä¸Šè¿›è¡ŒåŸŸé€‚åº”ï¼Œå¹¶ä½¿ç”¨ç‰¹å¾èåˆå—å’ŒåŒ…å«Bi-LSTMå±‚çš„åˆ†ç±»å™¨ç½‘ç»œè¿›è¡Œåˆ†ç±»ã€‚åœ¨PlantVillageæ•°æ®é›†çš„ç•ªèŒ„å¶ç‰‡å›¾åƒä¸Šè¯„ä¼°ï¼Œå°æ ·æœ¬åˆ†ç±»è¾¾åˆ°è¾ƒé«˜å‡†ç¡®ç‡ã€‚æ­¤å¤–ï¼Œåœ¨å‡å°‘è®­ç»ƒæ•°æ®éœ€æ±‚çš„æƒ…å†µä¸‹ï¼Œå…¶æ€§èƒ½æ¥è¿‘æœ€æ–°æŠ€æœ¯ã€‚è¯¥æ¡†æ¶åœ¨å•åŸŸã€æ··åˆåŸŸå’Œè·¨åŸŸåœºæ™¯ä¸­å‡ä¼˜äºç°æœ‰å·¥ä½œåœ¨å¶ç‰‡ç—…å®³åˆ†ç±»æ–¹é¢çš„è¡¨ç°ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è¯¥ç ”ç©¶é’ˆå¯¹æ¤ç‰©ç—…å®³åˆ†ç±»æå‡ºä¸€ç§åŸºäºå°æ ·æœ¬å­¦ä¹ çš„DExNetæ¡†æ¶ã€‚</li>
<li>DExNetæ¡†æ¶é€šè¿‡ç»“åˆå¤šä¸ªä¸“å®¶çš„è§‚å¯Ÿæ¥å¼¥è¡¥è®­ç»ƒæ•°æ®ä¸è¶³çš„é—®é¢˜ã€‚</li>
<li>DExNetä½¿ç”¨äº†ç‰¹å¾èåˆå—å’ŒåŒ…å«Bi-LSTMå±‚çš„åˆ†ç±»å™¨ç½‘ç»œè¿›è¡Œåˆ†ç±»ã€‚</li>
<li>åœ¨PlantVillageæ•°æ®é›†çš„ç•ªèŒ„å¶ç‰‡å›¾åƒä¸Šè¯„ä¼°ï¼Œå°æ ·æœ¬åˆ†ç±»è¾¾åˆ°è¾ƒé«˜å‡†ç¡®ç‡ã€‚</li>
<li>è¯¥æ¡†æ¶åœ¨å‡å°‘è®­ç»ƒæ•°æ®éœ€æ±‚çš„æƒ…å†µä¸‹æ€§èƒ½æ¥è¿‘æœ€æ–°æŠ€æœ¯ï¼Œå¹¶å®ç°äº†æ˜¾è‘—çš„æ•°æ®æ•ˆç‡ã€‚</li>
<li>è¯¥æ¡†æ¶åœ¨å•åŸŸã€æ··åˆåŸŸå’Œè·¨åŸŸåœºæ™¯ä¸­çš„å¶ç‰‡ç—…å®³åˆ†ç±»è¡¨ç°å‡ä¼˜äºç°æœ‰å·¥ä½œã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.18173">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-1e695e9d167844f4534a2d7434ca0588.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a0ed53ed1d12e67371cb9c0b78aa445f.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Multimodal-Medical-Image-Binding-via-Shared-Text-Embeddings"><a href="#Multimodal-Medical-Image-Binding-via-Shared-Text-Embeddings" class="headerlink" title="Multimodal Medical Image Binding via Shared Text Embeddings"></a>Multimodal Medical Image Binding via Shared Text Embeddings</h2><p><strong>Authors:Yunhao Liu, Suyang Xi, Shiqi Liu, Hong Ding, Chicheng Jin, Chenxi Yang, Junjun He, Yiqing Shen</strong></p>
<p>Medical image analysis increasingly relies on the integration of multiple imaging modalities to capture complementary anatomical and functional information, enabling more accurate diagnosis and treatment planning. Achieving aligned feature representations across these diverse modalities is therefore important for effective multimodal analysis. While contrastive language-image pre-training (CLIP) and its variant have enabled image-text alignments, they require explicitly paired data between arbitrary two modalities, which is difficult to acquire in medical contexts. To address the gap, we present Multimodal Medical Image Binding with Text (M\textsuperscript{3}Bind), a novel pre-training framework that enables seamless alignment of multiple medical imaging modalities through a shared text representation space without requiring explicit paired data between any two medical image modalities. Specifically, based on the insight that different images can naturally bind with text, M\textsuperscript{3}Bind first fine-tunes pre-trained CLIP-like image-text models to align their modality-specific text embedding space while preserving their original image-text alignments. Subsequently, we distill these modality-specific text encoders into a unified model, creating a shared text embedding space. Experiments on X-ray, CT, retina, ECG, and pathological images on multiple downstream tasks demonstrate that M\textsuperscript{3}Bind achieves state-of-the-art performance in zero-shot, few-shot classification and cross-modal retrieval tasks compared to its CLIP-like counterparts. These results validate M\textsuperscript{3}Bindâ€™s effectiveness in achieving cross-image-modal alignment for medical analysis. </p>
<blockquote>
<p>åŒ»ç–—å›¾åƒåˆ†æè¶Šæ¥è¶Šä¾èµ–äºå¤šç§æˆåƒæ¨¡å¼çš„é›†æˆï¼Œä»¥æ•è·äº’è¡¥çš„è§£å‰–å’ŒåŠŸèƒ½æ€§ä¿¡æ¯ï¼Œä»è€Œå®ç°æ›´å‡†ç¡®çš„è¯Šæ–­å’Œåˆ¶å®šæ²»ç–—æ–¹æ¡ˆã€‚å› æ­¤ï¼Œå®ç°è¿™äº›ä¸åŒæ¨¡å¼ä¹‹é—´çš„å¯¹é½ç‰¹å¾è¡¨ç¤ºå¯¹äºæœ‰æ•ˆçš„å¤šæ¨¡æ€åˆ†æè‡³å…³é‡è¦ã€‚å°½ç®¡å¯¹æ¯”è¯­è¨€å›¾åƒé¢„è®­ç»ƒï¼ˆCLIPï¼‰åŠå…¶å˜ä½“å·²ç»å®ç°äº†å›¾åƒæ–‡æœ¬çš„å¯¹é½ï¼Œä½†å®ƒä»¬éœ€è¦ä»»æ„ä¸¤ä¸ªæ¨¡æ€ä¹‹é—´çš„æ˜ç¡®é…å¯¹æ•°æ®ï¼Œè¿™åœ¨åŒ»å­¦èƒŒæ™¯ä¸‹å¾ˆéš¾è·å–ã€‚ä¸ºäº†è§£å†³è¿™ä¸€å·®è·ï¼Œæˆ‘ä»¬æå‡ºäº†åŸºäºæ–‡æœ¬çš„å¤šæ¨¡æ€åŒ»å­¦å›¾åƒç»‘å®šï¼ˆM\textsuperscript{3}Bindï¼‰ï¼Œè¿™æ˜¯ä¸€ç§æ–°çš„é¢„è®­ç»ƒæ¡†æ¶ï¼Œèƒ½å¤Ÿé€šè¿‡å…±äº«æ–‡æœ¬è¡¨ç¤ºç©ºé—´æ— ç¼å¯¹é½å¤šä¸ªåŒ»å­¦æˆåƒæ¨¡å¼ï¼Œè€Œæ— éœ€ä»»ä½•ä¸¤ä¸ªåŒ»å­¦å›¾åƒæ¨¡å¼ä¹‹é—´çš„æ˜ç¡®é…å¯¹æ•°æ®ã€‚å…·ä½“æ¥è¯´ï¼ŒåŸºäºä¸åŒå›¾åƒå¯ä»¥è‡ªç„¶ç»‘å®šæ–‡æœ¬çš„è§è§£ï¼ŒM\textsuperscript{3}Bindé¦–å…ˆå¾®è°ƒé¢„è®­ç»ƒçš„CLIPç±»ä¼¼çš„å›¾åƒæ–‡æœ¬æ¨¡å‹ï¼Œä»¥å¯¹é½å…¶ç‰¹å®šäºæ¨¡æ€çš„æ–‡æœ¬åµŒå…¥ç©ºé—´ï¼ŒåŒæ—¶ä¿ç•™å…¶åŸå§‹çš„å›¾åƒæ–‡æœ¬å¯¹é½ã€‚éšåï¼Œæˆ‘ä»¬å°†è¿™äº›ç‰¹å®šäºæ¨¡æ€çš„æ–‡æœ¬ç¼–ç å™¨è’¸é¦åˆ°ç»Ÿä¸€æ¨¡å‹ä¸­ï¼Œåˆ›å»ºä¸€ä¸ªå…±äº«çš„æ–‡æœ¬åµŒå…¥ç©ºé—´ã€‚åœ¨Xå…‰ã€CTã€è§†ç½‘è†œã€å¿ƒç”µå›¾å’Œç—…ç†å›¾åƒä¸Šçš„å¤šä¸ªä¸‹æ¸¸ä»»åŠ¡è¿›è¡Œçš„å®éªŒè¡¨æ˜ï¼Œä¸CLIPç±»ä¼¼çš„æ¨¡å‹ç›¸æ¯”ï¼ŒM\textsuperscript{3}Bindåœ¨é›¶æ ·æœ¬ã€å°‘æ ·æœ¬åˆ†ç±»å’Œè·¨æ¨¡æ€æ£€ç´¢ä»»åŠ¡ä¸Šå®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚è¿™äº›ç»“æœéªŒè¯äº†M\textsuperscript{3}Bindåœ¨å®ç°åŒ»å­¦åˆ†æä¸­çš„è·¨å›¾åƒæ¨¡æ€å¯¹é½æ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.18072v1">PDF</a> 10 pages, 3 figures</p>
<p><strong>æ€»ç»“</strong></p>
<p>åŒ»å­¦å›¾åƒåˆ†æè¶Šæ¥è¶Šä¾èµ–äºå¤šç§æˆåƒæ–¹å¼çš„èåˆï¼Œæ•æ‰äº’è¡¥çš„è§£å‰–åŠåŠŸèƒ½ä¿¡æ¯ï¼Œä»¥æä¾›æ›´å‡†ç¡®çš„è¯Šæ–­å’Œæ›´å…¨é¢çš„æ²»ç–—è®¡åˆ’ã€‚å› æ­¤ï¼Œå®ç°åœ¨ä¸åŒæ¨¡æ€é—´çš„å¯¹é½ç‰¹å¾è¡¨ç¤ºå¯¹å¤šæ¨¡æ€åˆ†æè‡³å…³é‡è¦ã€‚è™½ç„¶å¯¹æ¯”è¯­è¨€å›¾åƒé¢„è®­ç»ƒï¼ˆCLIPï¼‰åŠå…¶å˜ä½“èƒ½å¤Ÿå®ç°å›¾åƒæ–‡æœ¬çš„åŒ¹é…ï¼Œä½†å®ƒä»¬éœ€è¦åœ¨ä¸¤ç§ä»»æ„æ¨¡æ€ä¹‹é—´è¿›è¡Œæ˜ç¡®é…å¯¹çš„æ•°æ®ï¼Œè¿™åœ¨åŒ»å­¦ç¯å¢ƒä¸­éš¾ä»¥è·å–ã€‚ä¸ºè§£å†³è¿™ä¸€å·®è·ï¼Œæˆ‘ä»¬æå‡ºäº†å¤šæ¨¡æ€åŒ»å­¦å›¾åƒä¸æ–‡æœ¬ç»‘å®šï¼ˆM\textsuperscript{3}Bindï¼‰çš„é¢„è®­ç»ƒæ¡†æ¶ï¼Œå®ƒèƒ½å¤Ÿé€šè¿‡å…±äº«æ–‡æœ¬è¡¨ç¤ºç©ºé—´æ— ç¼å¯¹é½å¤šä¸ªåŒ»å­¦æˆåƒæ¨¡æ€ï¼Œæ— éœ€ä»»ä½•ä¸¤ç§åŒ»å­¦å›¾åƒæ¨¡æ€ä¹‹é—´çš„æ˜ç¡®é…å¯¹æ•°æ®ã€‚å…·ä½“æ¥è¯´ï¼ŒåŸºäºä¸åŒå›¾åƒå¯ä»¥è‡ªç„¶ç»‘å®šæ–‡æœ¬çš„è§è§£ï¼ŒM\textsuperscript{3}Bindé¦–å…ˆå¾®è°ƒé¢„è®­ç»ƒçš„CLIPç±»å›¾åƒæ–‡æœ¬æ¨¡å‹ï¼Œä»¥å¯¹é½å…¶æ¨¡æ€ç‰¹å®šçš„æ–‡æœ¬åµŒå…¥ç©ºé—´ï¼ŒåŒæ—¶ä¿ç•™å…¶åŸå§‹çš„å›¾åƒæ–‡æœ¬åŒ¹é…ã€‚éšåï¼Œæˆ‘ä»¬å°†è¿™äº›æ¨¡æ€ç‰¹å®šçš„æ–‡æœ¬ç¼–ç å™¨è’¸é¦åˆ°ç»Ÿä¸€æ¨¡å‹ä¸­ï¼Œåˆ›å»ºå…±äº«æ–‡æœ¬åµŒå…¥ç©ºé—´ã€‚åœ¨Xå…‰ã€CTã€è§†ç½‘è†œã€å¿ƒç”µå›¾å’Œç—…ç†å›¾åƒä¸Šçš„å¤šé¡¹ä¸‹æ¸¸ä»»åŠ¡å®éªŒè¡¨æ˜ï¼Œä¸CLIPç±»æ¨¡å‹ç›¸æ¯”ï¼ŒM\textsuperscript{3}Bindåœ¨é›¶æ ·æœ¬ã€å°‘æ ·æœ¬åˆ†ç±»å’Œè·¨æ¨¡æ€æ£€ç´¢ä»»åŠ¡ä¸Šå®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚è¿™äº›ç»“æœéªŒè¯äº†M\textsuperscript{3}Bindåœ¨åŒ»å­¦åˆ†æä¸­å®ç°è·¨å›¾åƒæ¨¡æ€å¯¹é½çš„æœ‰æ•ˆæ€§ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>M\textsuperscript{3}Bindæ˜¯ä¸€ä¸ªç”¨äºå¤šæ¨¡æ€åŒ»å­¦å›¾åƒåˆ†æçš„æ–°é¢„è®­ç»ƒæ¡†æ¶ã€‚</li>
<li>å®ƒé€šè¿‡å…±äº«æ–‡æœ¬è¡¨ç¤ºç©ºé—´å®ç°äº†ä¸åŒåŒ»å­¦æˆåƒæ¨¡æ€çš„æ— ç¼å¯¹é½ã€‚</li>
<li>M\textsuperscript{3}Bindä¸ä¾èµ–ä»»æ„ä¸¤ç§åŒ»å­¦å›¾åƒæ¨¡æ€ä¹‹é—´çš„æ˜ç¡®é…å¯¹æ•°æ®ã€‚</li>
<li>è¯¥æ¡†æ¶é€šè¿‡å¾®è°ƒé¢„è®­ç»ƒçš„CLIPç±»æ¨¡å‹å¹¶è’¸é¦æ¨¡æ€ç‰¹å®šçš„æ–‡æœ¬ç¼–ç å™¨æ¥å®ç°å…¶æ€§èƒ½ã€‚</li>
<li>åœ¨å¤šç§ä¸‹æ¸¸ä»»åŠ¡ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒM\textsuperscript{3}Bindåœ¨é›¶æ ·æœ¬å’Œå°‘æ ·æœ¬åˆ†ç±»ä»¥åŠè·¨æ¨¡æ€æ£€ç´¢ä»»åŠ¡ä¸Šå…·æœ‰å“è¶Šæ€§èƒ½ã€‚</li>
<li>M\textsuperscript{3}Bindçš„æœ‰æ•ˆæ€§åœ¨äºå…¶èƒ½å¤Ÿå®ç°è·¨å›¾åƒæ¨¡æ€çš„å¯¹é½ï¼Œè¿™å¯¹äºåŒ»å­¦åˆ†æè‡³å…³é‡è¦ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.18072">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-16f3e023d8f0abf679272da7ff92e7c3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-33e7d323a8a0c4c759a22550bb7efc4f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-83009d26ceae5f38c7c51a11dbd98686.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Adaptive-Multi-prompt-Contrastive-Network-for-Few-shot-Out-of-distribution-Detection"><a href="#Adaptive-Multi-prompt-Contrastive-Network-for-Few-shot-Out-of-distribution-Detection" class="headerlink" title="Adaptive Multi-prompt Contrastive Network for Few-shot   Out-of-distribution Detection"></a>Adaptive Multi-prompt Contrastive Network for Few-shot   Out-of-distribution Detection</h2><p><strong>Authors:Xiang Fang, Arvind Easwaran, Blaise Genest</strong></p>
<p>Out-of-distribution (OOD) detection attempts to distinguish outlier samples to prevent models trained on the in-distribution (ID) dataset from producing unavailable outputs. Most OOD detection methods require many IID samples for training, which seriously limits their real-world applications. To this end, we target a challenging setting: few-shot OOD detection, where {Only a few {\em labeled ID} samples are available.} Therefore, few-shot OOD detection is much more challenging than the traditional OOD detection setting. Previous few-shot OOD detection works ignore the distinct diversity between different classes. In this paper, we propose a novel network: Adaptive Multi-prompt Contrastive Network (AMCN), which adapts the ID-OOD separation boundary by learning inter- and intra-class distribution. To compensate for the absence of OOD and scarcity of ID {\em image samples}, we leverage CLIP, connecting text with images, engineering learnable ID and OOD {\em textual prompts}. Specifically, we first generate adaptive prompts (learnable ID prompts, label-fixed OOD prompts and label-adaptive OOD prompts). Then, we generate an adaptive class boundary for each class by introducing a class-wise threshold. Finally, we propose a prompt-guided ID-OOD separation module to control the margin between ID and OOD prompts. Experimental results show that AMCN outperforms other state-of-the-art works. </p>
<blockquote>
<p>å¼‚å¸¸æ£€æµ‹ï¼ˆOODï¼‰æ—¨åœ¨åŒºåˆ†å¼‚å¸¸æ ·æœ¬ï¼Œä»¥é˜²æ­¢å¯¹åˆ†å¸ƒå†…ï¼ˆIDï¼‰æ•°æ®é›†è®­ç»ƒçš„æ¨¡å‹äº§ç”Ÿä¸å¯ç”¨è¾“å‡ºã€‚å¤§å¤šæ•°OODæ£€æµ‹æ–¹æ³•éœ€è¦å¤§é‡IIDæ ·æœ¬è¿›è¡Œè®­ç»ƒï¼Œè¿™ä¸¥é‡é™åˆ¶äº†å®ƒä»¬åœ¨ç°å®ä¸–ç•Œä¸­çš„åº”ç”¨ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬é’ˆå¯¹ä¸€ä¸ªå…·æœ‰æŒ‘æˆ˜æ€§çš„åœºæ™¯ï¼šå°‘æ ·æœ¬å¼‚å¸¸æ£€æµ‹ï¼Œå…¶ä¸­åªæœ‰å°‘æ•°å‡ ä¸ªæ ‡è®°çš„IDæ ·æœ¬å¯ç”¨ã€‚å› æ­¤ï¼Œå°‘æ ·æœ¬å¼‚å¸¸æ£€æµ‹æ¯”ä¼ ç»Ÿçš„å¼‚å¸¸æ£€æµ‹è®¾ç½®æ›´å…·æŒ‘æˆ˜æ€§ã€‚ä»¥å‰çš„ç ”ç©¶å¿½ç•¥äº†ä¸åŒç±»åˆ«ä¹‹é—´çš„æ˜æ˜¾å¤šæ ·æ€§ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°å‹ç½‘ç»œï¼šè‡ªé€‚åº”å¤šæç¤ºå¯¹æ¯”ç½‘ç»œï¼ˆAMCNï¼‰ï¼Œå®ƒé€šè¿‡å­¦ä¹ å’Œç±»å†…åˆ†å¸ƒæ¥é€‚åº”ID-OODåˆ†ç¦»è¾¹ç•Œã€‚ä¸ºäº†å¼¥è¡¥ç¼ºå°‘OODå’ŒIDå›¾åƒæ ·æœ¬ç¨€ç¼ºçš„é—®é¢˜ï¼Œæˆ‘ä»¬åˆ©ç”¨CLIPæŠ€æœ¯è¿æ¥æ–‡æœ¬å’Œå›¾åƒï¼Œæ„å»ºå¯å­¦ä¹ çš„IDå’ŒOODæ–‡æœ¬æç¤ºã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬é¦–å…ˆç”Ÿæˆè‡ªé€‚åº”æç¤ºï¼ˆå¯å­¦ä¹ çš„IDæç¤ºã€æ ‡ç­¾å›ºå®šçš„OODæç¤ºå’Œæ ‡ç­¾è‡ªé€‚åº”çš„OODæç¤ºï¼‰ã€‚ç„¶åï¼Œæˆ‘ä»¬é€šè¿‡å¼•å…¥ç±»åˆ«ç‰¹å®šçš„é˜ˆå€¼ä¸ºæ¯ä¸ªç±»åˆ«ç”Ÿæˆè‡ªé€‚åº”çš„ç±»åˆ«è¾¹ç•Œã€‚æœ€åï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªæç¤ºå¼•å¯¼çš„ID-OODåˆ†ç¦»æ¨¡å—æ¥æ§åˆ¶IDå’ŒOODæç¤ºä¹‹é—´çš„é—´éš”ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒAMCNä¼˜äºå…¶ä»–æœ€æ–°æŠ€æœ¯ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.17633v1">PDF</a> ICML 2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†è‡ªé€‚åº”å¤šæç¤ºå¯¹æ¯”ç½‘ç»œï¼ˆAMCNï¼‰ï¼Œè¯¥ç½‘ç»œé’ˆå¯¹å°‘æ ·æœ¬çš„OODæ£€æµ‹é—®é¢˜è¿›è¡Œäº†ä¼˜åŒ–ã€‚é€šè¿‡ç”Ÿæˆè‡ªé€‚åº”æç¤ºå’Œç±»è¾¹ç•Œï¼Œå¹¶åˆ©ç”¨CLIPè¿æ¥æ–‡æœ¬å’Œå›¾åƒï¼Œå·¥ç¨‹å­¦ä¹ IDå’ŒOODçš„æ–‡æœ¬æç¤ºï¼ŒAMCNèƒ½å¤Ÿé€‚åº”ä¸åŒç±»åˆ«ä¹‹é—´çš„ç‹¬ç‰¹å·®å¼‚ï¼Œæœ‰æ•ˆåŒºåˆ†IDå’ŒOODæ ·æœ¬ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒAMCNåœ¨å°‘æ ·æœ¬OODæ£€æµ‹ä»»åŠ¡ä¸Šä¼˜äºå…¶ä»–æœ€æ–°æŠ€æœ¯ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>æœ¬æ–‡é’ˆå¯¹å°‘æ ·æœ¬OODæ£€æµ‹é—®é¢˜æå‡ºäº†ä¸€ç§æ–°çš„ç½‘ç»œç»“æ„â€”â€”è‡ªé€‚åº”å¤šæç¤ºå¯¹æ¯”ç½‘ç»œï¼ˆAMCNï¼‰ã€‚</li>
<li>AMCNé€šè¿‡ç”Ÿæˆè‡ªé€‚åº”æç¤ºæ¥é€‚åº”ä¸åŒç±»åˆ«ä¹‹é—´çš„ç‹¬ç‰¹å·®å¼‚ï¼Œå¹¶åˆ©ç”¨CLIPè¿æ¥æ–‡æœ¬å’Œå›¾åƒï¼Œæ„å»ºå­¦ä¹ IDå’ŒOODçš„æ–‡æœ¬æç¤ºã€‚</li>
<li>ä¸ºäº†å¼¥è¡¥OODæ ·æœ¬çš„ç¼ºä¹å’ŒIDå›¾åƒæ ·æœ¬çš„ç¨€ç¼ºæ€§ï¼ŒAMCNå¼•å…¥äº†å¯å­¦ä¹ çš„IDæç¤ºå’Œæ ‡ç­¾å›ºå®šçš„OODæç¤ºä»¥åŠæ ‡ç­¾è‡ªé€‚åº”çš„OODæç¤ºã€‚</li>
<li>AMC Nå¼•å…¥äº†ä¸ºæ¯ä¸ªç±»åˆ«ç”Ÿæˆçš„ç±»è¾¹ç•Œå’Œç±»åˆ«ç‰¹å®šçš„é˜ˆå€¼æ¥æ§åˆ¶ä¸åŒç±»åˆ«çš„åˆ†å¸ƒè¾¹ç•Œã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.17633">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-8eceda8019d971564c66138dfa6bf806.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-a2f5a4906aba3b04b31ebaea76a8726d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1d6d777e9dd530c4c89e5f8f2c03497f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-117f5883f28987041fc0cf3dbf810c4a.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Trustworthy-Few-Shot-Transfer-of-Medical-VLMs-through-Split-Conformal-Prediction"><a href="#Trustworthy-Few-Shot-Transfer-of-Medical-VLMs-through-Split-Conformal-Prediction" class="headerlink" title="Trustworthy Few-Shot Transfer of Medical VLMs through Split Conformal   Prediction"></a>Trustworthy Few-Shot Transfer of Medical VLMs through Split Conformal   Prediction</h2><p><strong>Authors:Julio Silva-RodrÃ­guez, Ismail Ben Ayed, Jose Dolz</strong></p>
<p>Medical vision-language models (VLMs) have demonstrated unprecedented transfer capabilities and are being increasingly adopted for data-efficient image classification. Despite its growing popularity, its reliability aspect remains largely unexplored. This work explores the split conformal prediction (SCP) framework to provide trustworthiness guarantees when transferring such models based on a small labeled calibration set. Despite its potential, the generalist nature of the VLMsâ€™ pre-training could negatively affect the properties of the predicted conformal sets for specific tasks. While common practice in transfer learning for discriminative purposes involves an adaptation stage, we observe that deploying such a solution for conformal purposes is suboptimal since adapting the model using the available calibration data breaks the rigid exchangeability assumptions for test data in SCP. To address this issue, we propose transductive split conformal adaptation (SCA-T), a novel pipeline for transfer learning on conformal scenarios, which performs an unsupervised transductive adaptation jointly on calibration and test data. We present comprehensive experiments utilizing medical VLMs across various image modalities, transfer tasks, and non-conformity scores. Our framework offers consistent gains in efficiency and conditional coverage compared to SCP, maintaining the same empirical guarantees. </p>
<blockquote>
<p>åŒ»ç–—è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰å·²å±•ç°å‡ºå‰æ‰€æœªæœ‰çš„è¿ç§»èƒ½åŠ›ï¼Œå¹¶è¶Šæ¥è¶Šå¤šåœ°è¢«ç”¨äºé«˜æ•ˆå›¾åƒåˆ†ç±»ã€‚å°½ç®¡å…¶è¶Šæ¥è¶Šå—æ¬¢è¿ï¼Œä½†å…¶å¯é æ€§æ–¹é¢ä»è¢«å¤§é‡å¿½è§†ã€‚æœ¬ç ”ç©¶æ¢è®¨äº†åˆ†è£‚é¡ºåº”æ€§é¢„æµ‹ï¼ˆSCPï¼‰æ¡†æ¶ï¼Œåœ¨åŸºäºä¸€å°éƒ¨åˆ†æ ‡è®°æ ¡å‡†é›†è¿ç§»æ­¤ç±»æ¨¡å‹æ—¶ï¼Œä¸ºå…¶æä¾›å¯é æ€§ä¿è¯ã€‚å°½ç®¡å…·æœ‰æ½œåŠ›ï¼Œä½†VLMsçš„é¢„è®­ç»ƒé€šç”¨æ€§å¯èƒ½ä¼šé’ˆå¯¹ç‰¹å®šä»»åŠ¡å¯¹é¢„æµ‹é¡ºåº”é›†çš„å±æ€§äº§ç”Ÿè´Ÿé¢å½±å“ã€‚åœ¨åˆ¤åˆ«ç›®çš„è¿ç§»å­¦ä¹ ä¸­ï¼Œå¸¸è§åšæ³•æ¶‰åŠé€‚åº”é˜¶æ®µï¼Œæˆ‘ä»¬è§‚å¯Ÿåˆ°é’ˆå¯¹é¡ºåº”ç›®çš„éƒ¨ç½²è¿™æ ·çš„è§£å†³æ–¹æ¡ˆæ˜¯ä¸ç†æƒ³çš„ï¼Œå› ä¸ºä½¿ç”¨å¯ç”¨çš„æ ¡å‡†æ•°æ®é€‚åº”æ¨¡å‹ä¼šç ´åSCPä¸­æµ‹è¯•æ•°æ®çš„åˆšæ€§å¯äº¤æ¢å‡è®¾ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†å½’çº³åˆ†è£‚é¡ºåº”é€‚åº”ï¼ˆSCA-Tï¼‰ï¼Œè¿™æ˜¯ä¸€ä¸ªé’ˆå¯¹é¡ºåº”åœºæ™¯è¿ç§»å­¦ä¹ çš„æ–°å‹ç®¡é“ï¼Œå¯åœ¨æ ¡å‡†å’Œæµ‹è¯•æ•°æ®ä¸Šè”åˆæ‰§è¡Œæ— ç›‘ç£å½’çº³é€‚åº”ã€‚æˆ‘ä»¬è¿›è¡Œäº†å…¨é¢çš„å®éªŒï¼Œåˆ©ç”¨åŒ»ç–—VLMsè·¨å¤šç§å›¾åƒæ¨¡å¼ã€è¿ç§»ä»»åŠ¡å’Œéé¡ºåº”æ€§åˆ†æ•°è¿›è¡Œç ”ç©¶ã€‚æˆ‘ä»¬çš„æ¡†æ¶åœ¨æ•ˆç‡å’Œæœ‰æ¡ä»¶è¦†ç›–ç‡æ–¹é¢ä¸SCPç›¸æ¯”æä¾›äº†ä¸€è‡´çš„æ”¶ç›Šï¼Œå¹¶ä¿æŒç›¸åŒçš„ç»éªŒä¿è¯ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.17503v1">PDF</a> MICCAI 2025. Code: <a target="_blank" rel="noopener" href="https://github.com/jusiro/SCA-T">https://github.com/jusiro/SCA-T</a></p>
<p><strong>Summary</strong><br>åŒ»å­¦è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰çš„è¿ç§»èƒ½åŠ›å¼ºå¤§ï¼Œå¹¿æ³›åº”ç”¨äºå›¾åƒåˆ†ç±»é¢†åŸŸã€‚ä½†å…¶å¯é æ€§æ–¹é¢å°šæœªå¾—åˆ°å……åˆ†æ¢ç´¢ã€‚æœ¬ç ”ç©¶é‡‡ç”¨åˆ†è£‚é¡ºåº”é¢„æµ‹ï¼ˆSCPï¼‰æ¡†æ¶ï¼Œä¸ºåŸºäºå°‘é‡æ ‡æ³¨æ ¡å‡†é›†çš„æ¨¡å‹è¿ç§»æä¾›å¯é æ€§ä¿éšœã€‚ç„¶è€Œï¼ŒVLMsçš„é¢„è®­ç»ƒé€šç”¨æ€§å¯èƒ½å½±å“ç‰¹å®šä»»åŠ¡çš„é¢„æµ‹é¡ºåº”é›†å±æ€§ã€‚è™½ç„¶è¿ç§»å­¦ä¹ åœ¨åˆ¤åˆ«ç›®çš„ä¸­çš„å¸¸è§„åšæ³•åŒ…æ‹¬é€‚åº”é˜¶æ®µï¼Œä½†ç”¨äºé¡ºåº”ç›®çš„æ—¶æ•ˆæœå¹¶ä¸ç†æƒ³ã€‚é’ˆå¯¹è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†è½¬å¯¼åˆ†è£‚é¡ºåº”é€‚åº”ï¼ˆSCA-Tï¼‰è¿™ä¸€æ–°å‹ç®¡é“ï¼Œåœ¨é¡ºåº”åœºæ™¯ä¸‹å®ç°è¿ç§»å­¦ä¹ ï¼Œå¯¹æ ¡å‡†æ•°æ®å’Œæµ‹è¯•æ•°æ®æ‰§è¡Œè”åˆçš„æ— ç›‘ç£è½¬å¯¼é€‚åº”ã€‚ç»è¿‡å¹¿æ³›çš„åŒ»å­¦å›¾åƒæ¨¡æ€å®éªŒéªŒè¯ï¼Œæˆ‘ä»¬çš„æ¡†æ¶åœ¨æ•ˆç‡å’Œæ¡ä»¶è¦†ç›–æ–¹é¢ç›¸è¾ƒäºSCPè¡¨ç°å‡ºä¼˜åŠ¿ï¼Œå¹¶ä¿æŒäº†ç›¸åŒçš„ç»éªŒä¿è¯ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>åŒ»ç–—è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰åœ¨å›¾åƒåˆ†ç±»é¢†åŸŸå…·æœ‰å¼ºå¤§çš„è¿ç§»èƒ½åŠ›ï¼Œä½†å¯é æ€§æ–¹é¢æœ‰å¾…æ¢ç´¢ã€‚</li>
<li>åˆ†è£‚é¡ºåº”é¢„æµ‹ï¼ˆSCPï¼‰æ¡†æ¶ç”¨äºä¸ºåŸºäºå°‘é‡æ ‡æ³¨æ ¡å‡†é›†çš„æ¨¡å‹è¿ç§»æä¾›å¯é æ€§ä¿éšœã€‚</li>
<li>VLMsé¢„è®­ç»ƒçš„é€šç”¨æ€§å¯èƒ½å½±å“ç‰¹å®šä»»åŠ¡çš„é¢„æµ‹é¡ºåº”é›†å±æ€§ã€‚</li>
<li>å¸¸è§„è¿ç§»å­¦ä¹ æ–¹æ³•ç”¨äºé¡ºåº”ç›®çš„æ—¶æ•ˆæœä¸ç†æƒ³ã€‚</li>
<li>æå‡ºè½¬å¯¼åˆ†è£‚é¡ºåº”é€‚åº”ï¼ˆSCA-Tï¼‰è¿™ä¸€æ–°å‹ç®¡é“ï¼Œç”¨äºåœ¨é¡ºåº”åœºæ™¯ä¸‹å®ç°è¿ç§»å­¦ä¹ ï¼Œå¯¹æ ¡å‡†æ•°æ®å’Œæµ‹è¯•æ•°æ®æ‰§è¡Œè”åˆçš„æ— ç›‘ç£è½¬å¯¼é€‚åº”ã€‚</li>
<li>ç»¼åˆå®éªŒéªŒè¯SCA-Tæ¡†æ¶ç›¸è¾ƒäºSCPçš„ä¼˜åŠ¿ï¼Œè¡¨ç°åœ¨æ•ˆç‡å’Œæ¡ä»¶è¦†ç›–æ–¹é¢ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.17503">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-6f412dd9362858c3fcecdf8a923b21a7.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="Few-Shot-Now-for-Real-Medical-VLMs-Adaptation-without-Balanced-Sets-or-Validation"><a href="#Few-Shot-Now-for-Real-Medical-VLMs-Adaptation-without-Balanced-Sets-or-Validation" class="headerlink" title="Few-Shot, Now for Real: Medical VLMs Adaptation without Balanced Sets or   Validation"></a>Few-Shot, Now for Real: Medical VLMs Adaptation without Balanced Sets or   Validation</h2><p><strong>Authors:Julio Silva-RodrÃ­guez, Fereshteh Shakeri, Houda Bahig, Jose Dolz, Ismail Ben Ayed</strong></p>
<p>Vision-language models (VLMs) are gaining attention in medical image analysis. These are pre-trained on large, heterogeneous data sources, yielding rich and transferable representations. Notably, the combination of modality-specialized VLMs with few-shot adaptation has provided fruitful results, enabling the efficient deployment of high-performing solutions. However, previous works on this topic make strong assumptions about the distribution of adaptation data, which are unrealistic in the medical domain. First, prior art assumes access to a balanced support set, a condition that breaks the natural imbalance in disease prevalence found in real-world scenarios. Second, these works typically assume the presence of an additional validation set to fix critical hyper-parameters, which is highly data-inefficient. This work challenges these favorable deployment scenarios and introduces a realistic, imbalanced, validation-free adaptation setting. Our extensive benchmark across various modalities and downstream tasks demonstrates that current methods systematically compromise their performance when operating under realistic conditions, occasionally even performing worse than zero-shot inference. Also, we introduce a training-free linear probe that adaptively blends visual and textual supervision. Detailed studies demonstrate that the proposed solver is a strong, efficient baseline, enabling robust adaptation in challenging scenarios. </p>
<blockquote>
<p>è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰åœ¨åŒ»å­¦å›¾åƒåˆ†æé¢†åŸŸè¶Šæ¥è¶Šå—åˆ°å…³æ³¨ã€‚è¿™äº›æ¨¡å‹åœ¨å¤§å‹ã€å¤šæ ·åŒ–çš„æ•°æ®æºä¸Šè¿›è¡Œé¢„è®­ç»ƒï¼Œäº§ç”Ÿä¸°å¯Œä¸”å¯è¿ç§»çš„è¡¨ç¤ºã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œæ¨¡æ€ä¸“ç”¨VLMsä¸å°‘é‡é€‚åº”çš„ç»“åˆå·²ç»äº§ç”Ÿäº†å¯Œæœ‰æˆæœçš„ç»“æœï¼Œèƒ½å¤Ÿå®ç°é«˜æ€§èƒ½è§£å†³æ–¹æ¡ˆçš„æœ‰æ•ˆéƒ¨ç½²ã€‚ç„¶è€Œï¼Œå…³äºè¯¥ä¸»é¢˜çš„å‰æœŸç ”ç©¶å¯¹é€‚åº”æ•°æ®åˆ†å¸ƒåšå‡ºäº†å¼ºçƒˆçš„å‡è®¾ï¼Œè¿™åœ¨åŒ»å­¦é¢†åŸŸæ˜¯ä¸ç°å®çš„ã€‚é¦–å…ˆï¼Œå…ˆå‰æŠ€æœ¯å‡å®šå¯ä»¥è®¿é—®å¹³è¡¡çš„æ”¯æŒé›†ï¼Œè¿™ä¸€æ¡ä»¶æ‰“ç ´äº†çœŸå®ä¸–ç•Œåœºæ™¯ä¸­ç–¾ç—…å‘ç”Ÿç‡å­˜åœ¨çš„è‡ªç„¶ä¸å¹³è¡¡ã€‚å…¶æ¬¡ï¼Œè¿™äº›å·¥ä½œé€šå¸¸å‡å®šå­˜åœ¨é¢å¤–çš„éªŒè¯é›†æ¥ä¿®æ­£å…³é”®è¶…å‚æ•°ï¼Œè¿™åœ¨æ•°æ®ä½¿ç”¨æ•ˆç‡ä¸Šéå¸¸ä½ã€‚è¿™é¡¹å·¥ä½œå¯¹è¿™äº›æœ‰åˆ©çš„éƒ¨ç½²åœºæ™¯æå‡ºäº†è´¨ç–‘ï¼Œå¹¶å¼•å…¥äº†ä¸€ä¸ªç°å®çš„ã€ä¸å¹³è¡¡çš„ã€æ— éœ€éªŒè¯çš„é€‚åº”ç¯å¢ƒã€‚æˆ‘ä»¬åœ¨å„ç§æ¨¡æ€å’Œä¸‹æ¸¸ä»»åŠ¡ä¸Šçš„å¹¿æ³›åŸºå‡†æµ‹è¯•è¡¨æ˜ï¼Œå½“å‰æ–¹æ³•åœ¨çœŸå®æ¡ä»¶ä¸‹è¿è¡Œæ—¶ï¼Œå…¶æ€§èƒ½ä¼šç³»ç»Ÿæ€§åœ°å—åˆ°æŸå®³ï¼Œæœ‰æ—¶ç”šè‡³è¡¨ç°å¾—æ¯”é›¶å°„å‡»æ¨ç†æ›´å·®ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜ä»‹ç»äº†ä¸€ç§æ— éœ€è®­ç»ƒçš„çº¿æ€§æ¢æµ‹å™¨ï¼Œå®ƒèƒ½è‡ªé€‚åº”åœ°èåˆè§†è§‰å’Œæ–‡æœ¬ç›‘ç£ã€‚è¯¦ç»†ç ”ç©¶è¡¨æ˜ï¼Œæ‰€æå‡ºçš„æ±‚è§£å™¨æ˜¯ä¸€ä¸ªå¼ºå¤§ä¸”é«˜æ•ˆçš„åŸºå‡†ï¼Œèƒ½å¤Ÿåœ¨å…·æœ‰æŒ‘æˆ˜æ€§çš„åœºæ™¯ä¸­å®ç°ç¨³å¥é€‚åº”ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.17500v1">PDF</a> MICCAI 2025. Code: <a target="_blank" rel="noopener" href="https://github.com/jusiro/SS-Text">https://github.com/jusiro/SS-Text</a></p>
<p><strong>Summary</strong><br>    è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰åœ¨åŒ»å­¦å›¾åƒåˆ†æé¢†åŸŸå—åˆ°å…³æ³¨ã€‚è¿™äº›æ¨¡å‹åœ¨å¤§è§„æ¨¡ã€å¤šæ ·åŒ–çš„æ•°æ®æºä¸Šè¿›è¡Œé¢„è®­ç»ƒï¼Œäº§ç”Ÿä¸°å¯Œä¸”å¯è¿ç§»çš„è¡¨ç¤ºã€‚ç»“åˆæ¨¡æ€ä¸“ç”¨VLMsè¿›è¡Œå°æ ·æœ¬é€‚åº”å–å¾—äº†å¯Œæœ‰æˆæœçš„ç»“æœï¼Œå¯å®ç°é«˜æ€§èƒ½è§£å†³æ–¹æ¡ˆçš„é«˜æ•ˆéƒ¨ç½²ã€‚ç„¶è€Œï¼Œä»¥å‰çš„ç ”ç©¶å¯¹è¿™ä¸€ä¸»é¢˜åšå‡ºäº†å…³äºé€‚åº”æ•°æ®åˆ†å¸ƒçš„å¼ºçƒˆå‡è®¾ï¼Œè¿™åœ¨åŒ»å­¦é¢†åŸŸä¸­æ˜¯ä¸åˆ‡å®é™…çš„ã€‚æœ¬ç ”ç©¶æŒ‘æˆ˜äº†è¿™äº›æœ‰åˆ©çš„éƒ¨ç½²åœºæ™¯ï¼Œå¹¶å¼•å…¥äº†æ›´ç°å®çš„ã€ä¸å¹³è¡¡çš„ã€æ— éœ€éªŒè¯çš„é€‚åº”è®¾ç½®ã€‚åœ¨å¤šç§æ¨¡æ€å’Œä¸‹æ¸¸ä»»åŠ¡ä¸Šçš„å¹¿æ³›åŸºå‡†æµ‹è¯•è¡¨æ˜ï¼Œå½“å‰æ–¹æ³•åœ¨ç°å®æ¡ä»¶ä¸‹çš„æ€§èƒ½ä¼šç³»ç»Ÿæ€§åœ°å—åˆ°å½±å“ï¼Œæœ‰æ—¶ç”šè‡³ä¸å¦‚é›¶æ ·æœ¬æ¨ç†ã€‚æ­¤å¤–ï¼Œæœ¬ç ”ç©¶è¿˜æå‡ºäº†ä¸€ç§è®­ç»ƒæœ‰ç´ çš„çº¿æ€§æ¢æµ‹å™¨ï¼Œå¯è‡ªé€‚åº”åœ°èåˆè§†è§‰å’Œæ–‡æœ¬ç›‘ç£ã€‚è¯¦ç»†ç ”ç©¶è¡¨æ˜ï¼Œè¯¥æ±‚è§£å™¨æ˜¯ä¸€ä¸ªå¼ºå¤§ä¸”é«˜æ•ˆçš„åŸºå‡†ï¼Œå¯åœ¨å…·æœ‰æŒ‘æˆ˜æ€§çš„åœºæ™¯ä¸­å®ç°ç¨³å¥é€‚åº”ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰åœ¨åŒ»å­¦å›¾åƒåˆ†æé¢†åŸŸå…·æœ‰åº”ç”¨ä»·å€¼ã€‚</li>
<li>VLMsé€šè¿‡åœ¨å¤§è§„æ¨¡ã€å¤šæ ·åŒ–æ•°æ®æºä¸Šçš„é¢„è®­ç»ƒï¼Œäº§ç”Ÿä¸°å¯Œä¸”å¯è¿ç§»çš„è¡¨ç¤ºã€‚</li>
<li>æ¨¡æ€ä¸“ç”¨VLMsä¸å°æ ·æœ¬é€‚åº”ç›¸ç»“åˆå–å¾—äº†è‰¯å¥½æ•ˆæœã€‚</li>
<li>ä»¥å¾€ç ”ç©¶åœ¨é€‚åº”åŒ»å­¦å›¾åƒåˆ†ææ—¶å­˜åœ¨ä¸åˆ‡å®é™…çš„å‡è®¾ï¼Œå¦‚å¹³è¡¡æ”¯æŒé›†å’Œé¢å¤–çš„éªŒè¯é›†ã€‚</li>
<li>æœ¬ç ”ç©¶æå‡ºäº†æ›´ç°å®çš„ã€æ— éœ€éªŒè¯çš„é€‚åº”è®¾ç½®ï¼Œå¹¶å‘ç°å½“å‰æ–¹æ³•åœ¨ç°å®æ¡ä»¶ä¸‹çš„æ€§èƒ½å—å½±å“ã€‚</li>
<li>å¼•å…¥äº†ä¸€ç§è®­ç»ƒæœ‰ç´ çš„çº¿æ€§æ¢æµ‹å™¨ï¼Œå¯è‡ªé€‚åº”èåˆè§†è§‰å’Œæ–‡æœ¬ç›‘ç£ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.17500">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-c24a17466d45a7fd1612a95ecd402521.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-42293ccd8fb5b12354dcd319d9207bb0.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="Learning-to-Adapt-Frozen-CLIP-for-Few-Shot-Test-Time-Domain-Adaptation"><a href="#Learning-to-Adapt-Frozen-CLIP-for-Few-Shot-Test-Time-Domain-Adaptation" class="headerlink" title="Learning to Adapt Frozen CLIP for Few-Shot Test-Time Domain Adaptation"></a>Learning to Adapt Frozen CLIP for Few-Shot Test-Time Domain Adaptation</h2><p><strong>Authors:Zhixiang Chi, Li Gu, Huan Liu, Ziqiang Wang, Yanan Wu, Yang Wang, Konstantinos N Plataniotis</strong></p>
<p>Few-shot Test-Time Domain Adaptation focuses on adapting a model at test time to a specific domain using only a few unlabeled examples, addressing domain shift. Prior methods leverage CLIPâ€™s strong out-of-distribution (OOD) abilities by generating domain-specific prompts to guide its generalized, frozen features. However, since downstream datasets are not explicitly seen by CLIP, solely depending on the feature space knowledge is constrained by CLIPâ€™s prior knowledge. Notably, when using a less robust backbone like ViT-B&#x2F;16, performance significantly drops on challenging real-world benchmarks. Departing from the state-of-the-art of inheriting the intrinsic OOD capability of CLIP, this work introduces learning directly on the input space to complement the dataset-specific knowledge for frozen CLIP. Specifically, an independent side branch is attached in parallel with CLIP and enforced to learn exclusive knowledge via revert attention. To better capture the dataset-specific label semantics for downstream adaptation, we propose to enhance the inter-dispersion among text features via greedy text ensemble and refinement. The text and visual features are then progressively fused in a domain-aware manner by a generated domain prompt to adapt toward a specific domain. Extensive experiments show our methodâ€™s superiority on 5 large-scale benchmarks (WILDS and DomainNet), notably improving over smaller networks like ViT-B&#x2F;16 with gains of \textbf{+5.1} in F1 for iWildCam and \textbf{+3.1%} in WC Acc for FMoW. </p>
<blockquote>
<p>å°‘æ•°æ ·æœ¬æµ‹è¯•æ—¶åŸŸé€‚åº”ï¼ˆFew-Shot Test-Time Domain Adaptationï¼‰ä¸»è¦å…³æ³¨åœ¨æµ‹è¯•é˜¶æ®µä½¿ç”¨å°‘æ•°æ— æ ‡ç­¾æ ·æœ¬å¯¹æ¨¡å‹è¿›è¡Œç‰¹å®šåŸŸé€‚åº”ï¼Œä»¥è§£å†³åŸŸåç§»é—®é¢˜ã€‚å…ˆå‰çš„æ–¹æ³•é€šè¿‡ç”Ÿæˆç‰¹å®šåŸŸçš„æç¤ºæ¥åˆ©ç”¨CLIPå¼ºå¤§çš„è¶…å‡ºåˆ†å¸ƒï¼ˆOODï¼‰èƒ½åŠ›ï¼Œä»¥æŒ‡å¯¼å…¶é€šç”¨å†»ç»“ç‰¹å¾ã€‚ç„¶è€Œï¼Œç”±äºä¸‹æ¸¸æ•°æ®é›†å¹¶æœªè¢«CLIPæ˜¾å¼åœ°çœ‹åˆ°ï¼Œä»…ä¾èµ–ç‰¹å¾ç©ºé—´çš„çŸ¥è¯†å—åˆ°CLIPå…ˆéªŒçŸ¥è¯†çš„é™åˆ¶ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œå½“ä½¿ç”¨ä¸é‚£ä¹ˆç¨³å¥çš„éª¨å¹²ç½‘ï¼ˆå¦‚ViT-B&#x2F;16ï¼‰æ—¶ï¼Œå…¶åœ¨å…·æœ‰æŒ‘æˆ˜æ€§çš„ç°å®ä¸–ç•ŒåŸºå‡†æµ‹è¯•ä¸Šçš„æ€§èƒ½ä¼šæ˜¾è‘—ä¸‹é™ã€‚è¿™é¡¹å·¥ä½œæ‘’å¼ƒäº†ç»§æ‰¿CLIPçš„å†…åœ¨OODèƒ½åŠ›çš„æœ€æ–°ç†å¿µï¼Œè½¬è€Œç›´æ¥åœ¨è¾“å…¥ç©ºé—´å­¦ä¹ ï¼Œä»¥è¡¥å……é’ˆå¯¹å†»ç»“CLIPçš„ç‰¹å®šæ•°æ®é›†çŸ¥è¯†ã€‚å…·ä½“æ¥è¯´ï¼Œä¸€ä¸ªç‹¬ç«‹çš„ä¾§åˆ†æ”¯ä¸CLIPå¹¶è¡Œè¿æ¥ï¼Œé€šè¿‡åå‘æ³¨æ„åŠ›è¢«å¼ºåˆ¶å­¦ä¹ ä¸“æœ‰çŸ¥è¯†ã€‚ä¸ºäº†æ›´å¥½åœ°æ•è·ä¸‹æ¸¸é€‚åº”çš„ç‰¹å®šæ•°æ®é›†æ ‡ç­¾è¯­ä¹‰ï¼Œæˆ‘ä»¬æå‡ºé€šè¿‡è´ªå©ªæ–‡æœ¬é›†åˆå’Œç»†åŒ–æ¥å¢å¼ºæ–‡æœ¬ç‰¹å¾ä¹‹é—´çš„åˆ†æ•£æ€§ã€‚ç„¶åï¼Œæ–‡æœ¬å’Œè§†è§‰ç‰¹å¾ä»¥é¢†åŸŸæ„ŸçŸ¥çš„æ–¹å¼é€šè¿‡ç”Ÿæˆçš„é¢†åŸŸæç¤ºé€æ­¥èåˆï¼Œä»¥é€‚åº”ç‰¹å®šé¢†åŸŸã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨5ä¸ªå¤§è§„æ¨¡åŸºå‡†æµ‹è¯•ï¼ˆWILDSå’ŒDomainNetï¼‰ä¸Šçš„è¡¨ç°ä¼˜äºå…¶ä»–æ–¹æ³•ï¼Œå°¤å…¶æ˜¯åœ¨è¾ƒå°ç½‘ç»œï¼ˆå¦‚ViT-B&#x2F;16ï¼‰ä¸Šï¼Œåœ¨iWildCamçš„F1å¾—åˆ†ä¸Šæé«˜äº†+5.1ï¼Œåœ¨FMoWçš„WC Accä¸Šæé«˜äº†+3.1%ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.17307v1">PDF</a> ICLR2025,<a target="_blank" rel="noopener" href="https://github.com/chi-chi-zx/L2C">https://github.com/chi-chi-zx/L2C</a></p>
<p><strong>Summary</strong></p>
<p>è¯¥æ–‡æœ¬ä»‹ç»äº†åŸºäºFew-Shotçš„æµ‹è¯•æ—¶é—´åŸŸè‡ªé€‚åº”æŠ€æœ¯ã€‚ä¼ ç»Ÿçš„æ–¹æ³•ä¸»è¦åˆ©ç”¨CLIPå¼ºå¤§çš„è·¨åŸŸèƒ½åŠ›ç”Ÿæˆç‰¹å®šåŸŸæç¤ºï¼Œä½†è¿™ç§æ–¹æ³•çš„æ€§èƒ½å—é™äºCLIPå¯¹ä¸‹æ¸¸æ•°æ®é›†çš„çŸ¥è¯†ç¼ºä¹ã€‚æœ¬æ–‡å¼•å…¥äº†åœ¨è¾“å…¥ç©ºé—´ä¸Šç›´æ¥å­¦ä¹ çš„æ–¹æ³•ï¼Œä»¥è¡¥å……é’ˆå¯¹ç‰¹å®šæ•°æ®é›†çš„çŸ¥è¯†ã€‚é€šè¿‡å¼•å…¥ç‹¬ç«‹ä¾§åˆ†æ”¯å’Œåè½¬æ³¨æ„åŠ›æœºåˆ¶ï¼Œç»“åˆè´ªå©ªæ–‡æœ¬é›†æˆå’Œç»†åŒ–ç­–ç•¥ï¼Œå®ç°äº†å¯¹ç‰¹å®šåŸŸçš„è‡ªé€‚åº”ã€‚å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å¤§å‹åŸºå‡†æµ‹è¯•ä¸Šçš„æ€§èƒ½ä¼˜äºç°æœ‰æŠ€æœ¯ï¼Œç‰¹åˆ«æ˜¯åœ¨è¾ƒå°çš„ç½‘ç»œå¦‚ViT-B&#x2F;16ä¸Šè¡¨ç°æ›´ä¸ºçªå‡ºã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>è¯¥æ–¹æ³•åŸºäºFew-Shotæµ‹è¯•æ—¶é—´åŸŸè‡ªé€‚åº”æŠ€æœ¯ï¼Œé’ˆå¯¹æ¨¡å‹åœ¨ç‰¹å®šåŸŸçš„é€‚åº”é—®é¢˜ï¼Œä»…ä½¿ç”¨å°‘é‡æ— æ ‡ç­¾æ ·æœ¬è¿›è¡Œé€‚åº”ã€‚</li>
<li>ä¼ ç»Ÿæ–¹æ³•ä¸»è¦åˆ©ç”¨CLIPçš„è·¨åŸŸèƒ½åŠ›ç”Ÿæˆç‰¹å®šåŸŸæç¤ºï¼Œä½†å—é™äºCLIPå¯¹ä¸‹æ¸¸æ•°æ®é›†çŸ¥è¯†çš„ç¼ºä¹ã€‚</li>
<li>å¼•å…¥åœ¨è¾“å…¥ç©ºé—´ä¸Šç›´æ¥å­¦ä¹ çš„æ–¹æ³•ï¼Œé€šè¿‡ç‹¬ç«‹ä¾§åˆ†æ”¯å’Œåè½¬æ³¨æ„åŠ›æœºåˆ¶æ¥è¡¥å……é’ˆå¯¹ç‰¹å®šæ•°æ®é›†çš„çŸ¥è¯†ã€‚</li>
<li>å¼•å…¥è´ªå©ªæ–‡æœ¬é›†æˆå’Œç»†åŒ–ç­–ç•¥ï¼Œä»¥æ›´å¥½åœ°æ•æ‰æ•°æ®é›†ç‰¹å®šçš„æ ‡ç­¾è¯­ä¹‰ã€‚</li>
<li>ä½¿ç”¨ç”ŸæˆåŸŸæç¤ºçš„æ–¹å¼ï¼Œå°†æ–‡æœ¬å’Œè§†è§‰ç‰¹å¾é€æ­¥èåˆä»¥é€‚åº”ç‰¹å®šåŸŸã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.17307">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-579b8e5ec7c4a28f2c6829c79af61e8d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-bef80d43cd4816c0eb6095d07442ae55.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-cc2ba195e4b0bb4dffa4b7711700fbbf.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-bcfc3ee3aca027cbc04352cb61baa5b4.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="Leveraging-Model-Guidance-to-Extract-Training-Data-from-Personalized-Diffusion-Models"><a href="#Leveraging-Model-Guidance-to-Extract-Training-Data-from-Personalized-Diffusion-Models" class="headerlink" title="Leveraging Model Guidance to Extract Training Data from Personalized   Diffusion Models"></a>Leveraging Model Guidance to Extract Training Data from Personalized   Diffusion Models</h2><p><strong>Authors:Xiaoyu Wu, Jiaru Zhang, Zhiwei Steven Wu</strong></p>
<p>Diffusion Models (DMs) have become powerful image generation tools, especially for few-shot fine-tuning where a pretrained DM is fine-tuned on a small image set to capture specific styles or objects. Many people upload these personalized checkpoints online, fostering communities such as Civitai and HuggingFace. However, model owners may overlook the data leakage risks when releasing fine-tuned checkpoints. Moreover, concerns regarding copyright violations arise when unauthorized data is used during fine-tuning. In this paper, we ask: â€œCan training data be extracted from these fine-tuned DMs shared online?â€ A successful extraction would present not only data leakage threats but also offer tangible evidence of copyright infringement. To answer this, we propose FineXtract, a framework for extracting fine-tuning data. Our method approximates fine-tuning as a gradual shift in the modelâ€™s learned distribution â€“ from the original pretrained DM toward the fine-tuning data. By extrapolating the models before and after fine-tuning, we guide the generation toward high-probability regions within the fine-tuned data distribution. We then apply a clustering algorithm to extract the most probable images from those generated using this extrapolated guidance. Experiments on DMs fine-tuned with datasets including WikiArt, DreamBooth, and real-world checkpoints posted online validate the effectiveness of our method, extracting about 20% of fine-tuning data in most cases. The code is available <a target="_blank" rel="noopener" href="https://github.com/Nicholas0228/FineXtract">https://github.com/Nicholas0228/FineXtract</a>. </p>
<blockquote>
<p>æ‰©æ•£æ¨¡å‹ï¼ˆDMsï¼‰å·²ç»æˆä¸ºå¼ºå¤§çš„å›¾åƒç”Ÿæˆå·¥å…·ï¼Œç‰¹åˆ«æ˜¯åœ¨å°æ ·æœ¬å¾®è°ƒé¢†åŸŸï¼Œå…¶ä¸­é¢„è®­ç»ƒçš„DMåœ¨å°å‹å›¾åƒé›†ä¸Šè¿›è¡Œå¾®è°ƒä»¥æ•æ‰ç‰¹å®šçš„é£æ ¼æˆ–å¯¹è±¡ã€‚è®¸å¤šäººåœ¨çº¿ä¸Šä¼ è¿™äº›ä¸ªæ€§åŒ–çš„æ£€æŸ¥ç‚¹ï¼Œä¿ƒè¿›äº†å¦‚Civitaiå’ŒHuggingFaceç­‰ç¤¾åŒºçš„å‘å±•ã€‚ç„¶è€Œï¼Œåœ¨å‘å¸ƒå¾®è°ƒæ£€æŸ¥ç‚¹æ—¶ï¼Œæ¨¡å‹æ‰€æœ‰è€…å¯èƒ½ä¼šå¿½ç•¥æ•°æ®æ³„éœ²çš„é£é™©ã€‚æ­¤å¤–ï¼Œå½“æœªç»æˆæƒçš„æ•°æ®ç”¨äºå¾®è°ƒæ—¶ï¼Œä¼šå‡ºç°ä¾µçŠ¯ç‰ˆæƒçš„é—®é¢˜ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºçš„é—®é¢˜æ˜¯ï¼šâ€œè¿™äº›åœ¨çº¿å…±äº«ç»è¿‡å¾®è°ƒåçš„DMsèƒ½å¦æå–å‡ºè®­ç»ƒæ•°æ®ï¼Ÿâ€æˆåŠŸçš„æå–ä¸ä»…ä¼šå¸¦æ¥æ•°æ®æ³„éœ²çš„å¨èƒï¼Œè€Œä¸”ä¹Ÿä¼šæä¾›ç‰ˆæƒä¾µæƒçš„å®é™…è¯æ®ã€‚ä¸ºäº†å›ç­”è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†FineXtractæ¡†æ¶ï¼Œè¯¥æ¡†æ¶ç”¨äºæå–å¾®è°ƒæ•°æ®ã€‚æˆ‘ä»¬çš„æ–¹æ³•å°†å¾®è°ƒè¿‘ä¼¼ä¸ºæ¨¡å‹å­¦ä¹ åˆ†å¸ƒçš„ä¸€ä¸ªé€æ¸å˜åŒ–çš„è¿‡ç¨‹â€”â€”ä»åŸå§‹çš„é¢„è®­ç»ƒDMå‘å¾®è°ƒæ•°æ®è½¬å˜ã€‚é€šè¿‡å¯¹å¾®è°ƒå‰åçš„æ¨¡å‹è¿›è¡Œå¤–æ¨ï¼Œæˆ‘ä»¬å¼•å¯¼ç”Ÿæˆçš„æ–¹å‘æœå‘å¾®è°ƒæ•°æ®åˆ†å¸ƒä¸­çš„é«˜æ¦‚ç‡åŒºåŸŸã€‚ç„¶åï¼Œæˆ‘ä»¬åº”ç”¨èšç±»ç®—æ³•ä»ä½¿ç”¨è¿™ç§å¤–æ¨æŒ‡å¯¼ç”Ÿæˆçš„å›¾åƒä¸­æå–æœ€å¯èƒ½çš„å›¾åƒã€‚åœ¨ä½¿ç”¨WikiArtã€DreamBoothä»¥åŠåœ¨çº¿å‘å¸ƒçš„ç°å®ä¸–ç•Œæ£€æŸ¥ç‚¹å¯¹DMsè¿›è¡Œå¾®è°ƒçš„å®éªŒä¸­ï¼ŒéªŒè¯äº†æˆ‘ä»¬çš„æ–¹æ³•çš„æœ‰æ•ˆæ€§ï¼Œåœ¨å¤§å¤šæ•°æƒ…å†µä¸‹èƒ½å¤Ÿæå–çº¦20%çš„å¾®è°ƒæ•°æ®ã€‚ä»£ç å¯ç”¨<a target="_blank" rel="noopener" href="https://github.com/Nicholas0228/FineXtract%E3%80%82">https://github.com/Nicholas0228/FineXtractã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2410.03039v2">PDF</a> Accepted at the International Conference on Machine Learning (ICML)   2025</p>
<p><strong>Summary</strong><br>     æ‰©æ•£æ¨¡å‹ï¼ˆDMsï¼‰åœ¨å°‘é‡æ ·æœ¬å¾®è°ƒä¸Šå±•ç°å‡ºå¼ºå¤§çš„å›¾åƒç”Ÿæˆèƒ½åŠ›ï¼Œä½†é‡Šæ”¾å¾®è°ƒåçš„æ¨¡å‹æ£€æŸ¥ç‚¹æ—¶å¯èƒ½å­˜åœ¨æ•°æ®æ³„éœ²é£é™©ã€‚æœ¬æ–‡æå‡ºâ€œFineXtractâ€æ¡†æ¶ï¼Œæ—¨åœ¨ä»åœ¨çº¿å…±äº«çš„å¾®è°ƒDMsä¸­æå–è®­ç»ƒæ•°æ®ã€‚é€šè¿‡æ¨¡å‹å¾®è°ƒå‰åçš„å¤–æ¨ï¼Œå¼•å¯¼ç”Ÿæˆå›¾åƒå‘å¾®è°ƒæ•°æ®åˆ†å¸ƒçš„é«˜æ¦‚ç‡åŒºåŸŸé æ‹¢ï¼Œå¹¶åº”ç”¨èšç±»ç®—æ³•ä»ç”Ÿæˆçš„å›¾åƒä¸­æå–æœ€å¯èƒ½çš„å›¾åƒã€‚å®éªŒè¯æ˜è¯¥æ–¹æ³•åœ¨WikiArtã€DreamBoothç­‰æ•°æ®é›†åŠåœ¨çº¿å‘å¸ƒçš„ç°å®æ£€æŸ¥ç‚¹ä¸Šæœ‰æ•ˆï¼Œå¤§å¤šæƒ…å†µä¸‹å¯æå–çº¦20%çš„å¾®è°ƒæ•°æ®ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ‰©æ•£æ¨¡å‹ï¼ˆDMsï¼‰åœ¨å°‘é‡æ ·æœ¬å¾®è°ƒä¸Šå…·æœ‰å¼ºå¤§çš„å›¾åƒç”Ÿæˆèƒ½åŠ›ã€‚</li>
<li>å‘å¸ƒå¾®è°ƒåçš„æ¨¡å‹æ£€æŸ¥ç‚¹æ—¶ï¼Œå­˜åœ¨æ•°æ®æ³„éœ²é£é™©ã€‚</li>
<li>æœ¬æ–‡æå‡ºâ€œFineXtractâ€æ¡†æ¶ï¼Œç”¨äºä»åœ¨çº¿å…±äº«çš„å¾®è°ƒDMsä¸­æå–è®­ç»ƒæ•°æ®ã€‚</li>
<li>é€šè¿‡æ¨¡å‹å¾®è°ƒå‰åçš„å¤–æ¨ï¼Œå¼•å¯¼ç”Ÿæˆå›¾åƒå‘é«˜æ¦‚ç‡åŒºåŸŸé æ‹¢ã€‚</li>
<li>åº”ç”¨èšç±»ç®—æ³•ä»ç”Ÿæˆçš„å›¾åƒä¸­æå–æœ€å¯èƒ½çš„å›¾åƒã€‚</li>
<li>å®éªŒè¯æ˜è¯¥æ–¹æ³•åœ¨å¤šä¸ªæ•°æ®é›†åŠåœ¨çº¿å‘å¸ƒçš„ç°å®æ£€æŸ¥ç‚¹ä¸Šæœ‰æ•ˆã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2410.03039">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-0c0035a8d0c167a3583e03a6343d99e2.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-5ca3fa2a3cea0af4ab46ac95c2941be7.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-e54bb752f5019594faa254a91ef1aed4.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-632725741617ec8f08ed7b8d82652280.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-06-25/Few-Shot/">https://kedreamix.github.io/Talk2Paper/Paper/2025-06-25/Few-Shot/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Few-Shot/">
                                    <span class="chip bg-color">Few-Shot</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-06-25/I2I%20Translation/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-e4d1a94042929f608ea5d7570e031a58.jpg" class="responsive-img" alt="I2I Translation">
                        
                        <span class="card-title">I2I Translation</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            I2I Translation æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-06-25  Enhancing Image Restoration Transformer via Adaptive Translation   Equivariance
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-06-25
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/I2I-Translation/" class="post-category">
                                    I2I Translation
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/I2I-Translation/">
                        <span class="chip bg-color">I2I Translation</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-06-25/Agent/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-52b9af6db973294039e24f9bc0227e29.jpg" class="responsive-img" alt="Agent">
                        
                        <span class="card-title">Agent</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Agent æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-06-25  Audit & Repair An Agentic Framework for Consistent Story Visualization   in Text-to-Image Diffusion Models
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-06-25
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Agent/" class="post-category">
                                    Agent
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Agent/">
                        <span class="chip bg-color">Agent</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">22963.3k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
