<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="åŒ»å­¦å›¾åƒ">
    <meta name="description" content="åŒ»å­¦å›¾åƒ æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-06-25  RAG-6DPose Retrieval-Augmented 6D Pose Estimation via Leveraging CAD as   Knowledge Base">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>åŒ»å­¦å›¾åƒ | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-baa2422c58392beb183525123b6c48ea.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">åŒ»å­¦å›¾åƒ</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                                <span class="chip bg-color">åŒ»å­¦å›¾åƒ</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/" class="post-category">
                                åŒ»å­¦å›¾åƒ
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-06-25
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-07-06
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    19.7k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    81 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-06-25-æ›´æ–°"><a href="#2025-06-25-æ›´æ–°" class="headerlink" title="2025-06-25 æ›´æ–°"></a>2025-06-25 æ›´æ–°</h1><h2 id="RAG-6DPose-Retrieval-Augmented-6D-Pose-Estimation-via-Leveraging-CAD-as-Knowledge-Base"><a href="#RAG-6DPose-Retrieval-Augmented-6D-Pose-Estimation-via-Leveraging-CAD-as-Knowledge-Base" class="headerlink" title="RAG-6DPose: Retrieval-Augmented 6D Pose Estimation via Leveraging CAD as   Knowledge Base"></a>RAG-6DPose: Retrieval-Augmented 6D Pose Estimation via Leveraging CAD as   Knowledge Base</h2><p><strong>Authors:Kuanning Wang, Yuqian Fu, Tianyu Wang, Yanwei Fu, Longfei Liang, Yu-Gang Jiang, Xiangyang Xue</strong></p>
<p>Accurate 6D pose estimation is key for robotic manipulation, enabling precise object localization for tasks like grasping. We present RAG-6DPose, a retrieval-augmented approach that leverages 3D CAD models as a knowledge base by integrating both visual and geometric cues. Our RAG-6DPose roughly contains three stages: 1) Building a Multi-Modal CAD Knowledge Base by extracting 2D visual features from multi-view CAD rendered images and also attaching 3D points; 2) Retrieving relevant CAD features from the knowledge base based on the current query image via our ReSPC module; and 3) Incorporating retrieved CAD information to refine pose predictions via retrieval-augmented decoding. Experimental results on standard benchmarks and real-world robotic tasks demonstrate the effectiveness and robustness of our approach, particularly in handling occlusions and novel viewpoints. Supplementary material is available on our project website: <a target="_blank" rel="noopener" href="https://sressers.github.io/RAG-6DPose">https://sressers.github.io/RAG-6DPose</a> . </p>
<blockquote>
<p>ç²¾ç¡®çš„6Då§¿æ€ä¼°è®¡æ˜¯æœºå™¨äººæ“ä½œçš„å…³é”®ï¼Œå®ƒä¸ºæŠ“å–ç­‰ä»»åŠ¡æä¾›äº†ç²¾ç¡®çš„å¯¹è±¡å®šä½èƒ½åŠ›ã€‚æˆ‘ä»¬æå‡ºäº†RAG-6DPoseï¼Œè¿™æ˜¯ä¸€ç§å¢å¼ºæ£€ç´¢çš„æ–¹æ³•ï¼Œå®ƒåˆ©ç”¨3D CADæ¨¡å‹ä½œä¸ºçŸ¥è¯†åº“ï¼Œé€šè¿‡èåˆè§†è§‰å’Œå‡ ä½•çº¿ç´¢ã€‚æˆ‘ä»¬çš„RAG-6DPoseå¤§è‡´åŒ…å«ä¸‰ä¸ªé˜¶æ®µï¼š1ï¼‰é€šè¿‡å»ºç«‹å¤šæ¨¡æ€CADçŸ¥è¯†åº“ï¼Œä»å¤šè§†å›¾CADæ¸²æŸ“å›¾åƒä¸­æå–2Dè§†è§‰ç‰¹å¾ï¼Œå¹¶é™„åŠ 3Dç‚¹ï¼›2ï¼‰é€šè¿‡æˆ‘ä»¬çš„ReSPCæ¨¡å—ï¼Œæ ¹æ®å½“å‰æŸ¥è¯¢å›¾åƒä»çŸ¥è¯†åº“ä¸­æ£€ç´¢ç›¸å…³çš„CADç‰¹å¾ï¼›3ï¼‰é€šè¿‡å¢å¼ºæ£€ç´¢è§£ç èåˆæ£€ç´¢åˆ°çš„CADä¿¡æ¯æ¥ä¼˜åŒ–å§¿æ€é¢„æµ‹ã€‚åœ¨æ ‡å‡†åŸºå‡†æµ‹è¯•å’ŒçœŸå®ä¸–ç•Œæœºå™¨äººä»»åŠ¡ä¸Šçš„å®éªŒç»“æœè¯æ˜äº†æˆ‘ä»¬çš„æ–¹æ³•çš„æœ‰æ•ˆæ€§å’Œç¨³å¥æ€§ï¼Œç‰¹åˆ«æ˜¯åœ¨å¤„ç†é®æŒ¡å’Œæ–°é¢–è§‚ç‚¹æ—¶ã€‚è¡¥å……ææ–™å¯åœ¨æˆ‘ä»¬çš„é¡¹ç›®ç½‘ç«™ä¸Šæ‰¾åˆ°ï¼š<a target="_blank" rel="noopener" href="https://sressers.github.io/RAG-6DPose%E3%80%82">https://sressers.github.io/RAG-6DPoseã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.18856v1">PDF</a> Accepted by IROS 2025</p>
<p><strong>Summary</strong><br>åŒ»å­¦å›¾åƒé¢†åŸŸçš„ç ”ç©¶äººå‘˜æå‡ºäº†ä¸€ç§åä¸ºRAG-6DPoseçš„æ–°æ–¹æ³•ï¼Œç”¨äºå¢å¼ºæœºå™¨äººæ“ä½œçš„ç²¾ç¡®æ€§ã€‚è¯¥æ–¹æ³•ç»“åˆäº†è§†è§‰å’Œå‡ ä½•çº¿ç´¢ï¼Œåˆ©ç”¨ä¸‰ç»´CADæ¨¡å‹ä½œä¸ºçŸ¥è¯†åº“è¿›è¡Œè¾…åŠ©æ£€ç´¢ï¼ŒåŒ…æ‹¬å»ºç«‹å¤šæ¨¡æ€CADçŸ¥è¯†åº“ã€åŸºäºæŸ¥è¯¢å›¾åƒè¿›è¡ŒCADç‰¹å¾æ£€ç´¢ä»¥åŠåˆ©ç”¨æ£€ç´¢åˆ°çš„CADä¿¡æ¯ä¼˜åŒ–å§¿æ€é¢„æµ‹ä¸‰ä¸ªé˜¶æ®µã€‚è¯¥æ–¹æ³•åœ¨æ ‡å‡†åŸºå‡†å’Œå®é™…æœºå™¨äººä»»åŠ¡ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å¤„ç†é®æŒ¡å’Œæ–°é¢–è§†è§’æ—¶å…·æœ‰æœ‰æ•ˆæ€§å’Œç¨³å¥æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>RAG-6DPoseæ˜¯ä¸€ç§ç”¨äºæœºå™¨äººæ“ä½œçš„å§¿æ€ä¼°è®¡æ–¹æ³•ï¼Œé‡ç‚¹åœ¨äºç²¾ç¡®çš„6Då§¿æ€ä¼°è®¡ã€‚</li>
<li>è¯¥æ–¹æ³•ç»“åˆè§†è§‰å’Œå‡ ä½•çº¿ç´¢ï¼Œåˆ©ç”¨ä¸‰ç»´CADæ¨¡å‹ä½œä¸ºçŸ¥è¯†åº“ã€‚</li>
<li>RAG-6DPoseåŒ…æ‹¬ä¸‰ä¸ªé˜¶æ®µï¼šå»ºç«‹å¤šæ¨¡æ€CADçŸ¥è¯†åº“ã€åŸºäºæŸ¥è¯¢å›¾åƒè¿›è¡ŒCADç‰¹å¾æ£€ç´¢ä»¥åŠåˆ©ç”¨æ£€ç´¢åˆ°çš„CADä¿¡æ¯ä¼˜åŒ–å§¿æ€é¢„æµ‹ã€‚</li>
<li>é€šè¿‡å®éªŒéªŒè¯ï¼ŒRAG-6DPoseåœ¨æ ‡å‡†åŸºå‡†æµ‹è¯•å’Œå®é™…æœºå™¨äººä»»åŠ¡ä¸­è¡¨ç°å‡ºæœ‰æ•ˆæ€§å’Œç¨³å¥æ€§ã€‚</li>
<li>è¯¥æ–¹æ³•ç‰¹åˆ«æ“…é•¿å¤„ç†é®æŒ¡å’Œæ–°é¢–è§†è§’çš„æƒ…å†µã€‚</li>
<li>RAG-6DPoseæ–¹æ³•çš„è¯¦ç»†ä¿¡æ¯å’Œè¡¥å……ææ–™å¯åœ¨é¡¹ç›®ç½‘ç«™ä¸Šæ‰¾åˆ°ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.18856">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-5f9b57b40041288917c42d2e94ff8623.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-755353985679e740ca87261cf1bfadc1.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f020a5e5cfb677d88988f913809e6363.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-4972463c8d4095cfe19bf065c78187b9.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-ae57b1231d3de882de5b0c95f6fa90b7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-955881fdb693a12c47d3118b80b26dda.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a7780e9427808342186f1c830b452503.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Temporal-Neural-Cellular-Automata-Application-to-modeling-of-contrast-enhancement-in-breast-MRI"><a href="#Temporal-Neural-Cellular-Automata-Application-to-modeling-of-contrast-enhancement-in-breast-MRI" class="headerlink" title="Temporal Neural Cellular Automata: Application to modeling of contrast   enhancement in breast MRI"></a>Temporal Neural Cellular Automata: Application to modeling of contrast   enhancement in breast MRI</h2><p><strong>Authors:Daniel M. Lang, Richard Osuala, Veronika Spieker, Karim Lekadir, Rickmer Braren, Julia A. Schnabel</strong></p>
<p>Synthetic contrast enhancement offers fast image acquisition and eliminates the need for intravenous injection of contrast agent. This is particularly beneficial for breast imaging, where long acquisition times and high cost are significantly limiting the applicability of magnetic resonance imaging (MRI) as a widespread screening modality. Recent studies have demonstrated the feasibility of synthetic contrast generation. However, current state-of-the-art (SOTA) methods lack sufficient measures for consistent temporal evolution. Neural cellular automata (NCA) offer a robust and lightweight architecture to model evolving patterns between neighboring cells or pixels. In this work we introduce TeNCA (Temporal Neural Cellular Automata), which extends and further refines NCAs to effectively model temporally sparse, non-uniformly sampled imaging data. To achieve this, we advance the training strategy by enabling adaptive loss computation and define the iterative nature of the method to resemble a physical progression in time. This conditions the model to learn a physiologically plausible evolution of contrast enhancement. We rigorously train and test TeNCA on a diverse breast MRI dataset and demonstrate its effectiveness, surpassing the performance of existing methods in generation of images that align with ground truth post-contrast sequences. </p>
<blockquote>
<p>åˆæˆå¯¹æ¯”åº¦å¢å¼ºæŠ€æœ¯å¯ä»¥å¿«é€Ÿè·å–å›¾åƒï¼Œå¹¶æ¶ˆé™¤äº†å¯¹é™è„‰æ³¨å…¥é€ å½±å‰‚çš„éœ€æ±‚ã€‚è¿™å¯¹äºä¹³è…ºæˆåƒç‰¹åˆ«æœ‰ç›Šï¼Œå› ä¸ºé•¿æ—¶é—´çš„æˆåƒå’Œé«˜æˆæœ¬æå¤§åœ°é™åˆ¶äº†ç£å…±æŒ¯æˆåƒï¼ˆMRIï¼‰ä½œä¸ºå¹¿æ³›ç­›æŸ¥æ–¹å¼çš„é€‚ç”¨æ€§ã€‚æœ€è¿‘çš„ç ”ç©¶å·²ç»è¯æ˜äº†åˆæˆå¯¹æ¯”åº¦ç”Ÿæˆçš„å¯è¡Œæ€§ã€‚ç„¶è€Œï¼Œå½“å‰æœ€å…ˆè¿›çš„æ–¹æ³•ç¼ºä¹ä¸€è‡´çš„ä¸´æ—¶æ¼”å˜çš„è¶³å¤Ÿæªæ–½ã€‚ç¥ç»å…ƒç»†èƒè‡ªåŠ¨æœºï¼ˆNCAï¼‰æä¾›äº†ä¸€ä¸ªç¨³å¥ä¸”è½»é‡çº§çš„æ¶æ„ï¼Œç”¨äºæ¨¡æ‹Ÿç›¸é‚»ç»†èƒæˆ–åƒç´ ä¹‹é—´çš„æ¼”å˜æ¨¡å¼ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬å¼•å…¥äº†TeNCAï¼ˆæ—¶é—´ç¥ç»å…ƒç»†èƒè‡ªåŠ¨æœºï¼‰ï¼Œå®ƒæ‰©å±•å¹¶è¿›ä¸€æ­¥å®Œå–„äº†NCAï¼Œä»¥æœ‰æ•ˆåœ°æ¨¡æ‹Ÿæ—¶é—´ä¸Šç¨€ç–ã€éå‡åŒ€é‡‡æ ·çš„æˆåƒæ•°æ®ã€‚ä¸ºäº†å®ç°è¿™ä¸€ç‚¹ï¼Œæˆ‘ä»¬é€šè¿‡å¯ç”¨è‡ªé€‚åº”æŸå¤±è®¡ç®—æ¥æ”¹è¿›è®­ç»ƒç­–ç•¥ï¼Œå¹¶å®šä¹‰æ–¹æ³•çš„è¿­ä»£æ€§è´¨ä»¥æ¨¡ä»¿æ—¶é—´çš„ç‰©ç†è¿›å±•ã€‚è¿™ä½¿æ¨¡å‹å­¦ä¼šå¯¹æ¯”å¢å¼ºåœ¨ç”Ÿç†ä¸Šçš„å¯èƒ½æ¼”å˜ã€‚æˆ‘ä»¬åœ¨å¤šæ ·åŒ–çš„ä¹³è…ºMRIæ•°æ®é›†ä¸Šå¯¹TeNCAè¿›è¡Œäº†ä¸¥æ ¼çš„è®­ç»ƒå’Œæµ‹è¯•ï¼Œå¹¶è¯æ˜äº†å…¶æœ‰æ•ˆæ€§ï¼Œåœ¨ç”Ÿæˆä¸çœŸå®å¯¹æ¯”åºåˆ—ç›¸ç¬¦çš„å›¾åƒæ–¹é¢è¶…è¿‡äº†ç°æœ‰æ–¹æ³•çš„æ€§èƒ½ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.18720v1">PDF</a> MICCAI 2025</p>
<p><strong>Summary</strong><br>     åˆæˆå¯¹æ¯”å¢å¼ºæŠ€æœ¯èƒ½è¿…é€Ÿè·å–å›¾åƒï¼Œæ— éœ€æ³¨å°„å¯¹æ¯”å‰‚ã€‚å¯¹äºä¹³è…ºæˆåƒè€Œè¨€ï¼Œè¿™ä¸€æŠ€æœ¯å°¤å…¶æœ‰ç›Šï¼Œè§£å†³äº†ç£å…±æŒ¯æˆåƒï¼ˆMRIï¼‰è·å–æ—¶é—´é•¿ã€æˆæœ¬é«˜æ˜‚çš„é—®é¢˜ï¼Œé™åˆ¶äº†å…¶ä½œä¸ºå¹¿æ³›ç­›æŸ¥æ‰‹æ®µçš„åº”ç”¨ã€‚æœ¬ç ”ç©¶å¼•å…¥æ—¶åºç¥ç»ç½‘ç»œç»†èƒè‡ªåŠ¨æœºï¼ˆTeNCAï¼‰ï¼Œç»“åˆç¥ç»ç½‘ç»œç»†èƒè‡ªåŠ¨æœºï¼ˆNCAï¼‰çš„ä¼˜åŠ¿ï¼Œæœ‰æ•ˆæ¨¡æ‹Ÿæ—¶åºç¨€ç–ã€éå‡åŒ€é‡‡æ ·çš„æˆåƒæ•°æ®ã€‚é€šè¿‡æ”¹è¿›è®­ç»ƒç­–ç•¥ã€å®šä¹‰è¿­ä»£æ–¹æ³•çš„ç‰©ç†æ—¶åºæ€§ï¼Œä½¿æ¨¡å‹å­¦ä¹ å¯¹æ¯”å¢å¼ºçš„ç”Ÿç†æ¼”å˜è¿‡ç¨‹ã€‚åœ¨å¤šæ ·çš„ä¹³è…ºMRIæ•°æ®é›†ä¸Šï¼ŒTeNCAè¡¨ç°å“è¶Šï¼Œè¶…è¶Šç°æœ‰æ–¹æ³•çš„å›¾åƒç”Ÿæˆæ•ˆæœï¼Œä¸çœŸå®å¯¹æ¯”åºåˆ—ç›¸ç¬¦ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>åˆæˆå¯¹æ¯”å¢å¼ºæŠ€æœ¯èƒ½è¿…é€Ÿè·å–å›¾åƒï¼Œæ— éœ€æ³¨å°„å¯¹æ¯”å‰‚ï¼Œå¯¹ä¹³è…ºæˆåƒæœ‰é‡è¦æ„ä¹‰ã€‚</li>
<li>ç°æœ‰æ–¹æ³•åœ¨å¤„ç†æ—¶åºç¨€ç–ã€éå‡åŒ€é‡‡æ ·çš„æˆåƒæ•°æ®æ—¶å­˜åœ¨ä¸è¶³ã€‚</li>
<li>æ—¶åºç¥ç»ç½‘ç»œç»†èƒè‡ªåŠ¨æœºï¼ˆTeNCAï¼‰ç»“åˆç¥ç»ç½‘ç»œç»†èƒè‡ªåŠ¨æœºï¼ˆNCAï¼‰çš„ä¼˜åŠ¿ï¼Œæœ‰æ•ˆæ¨¡æ‹Ÿè¿™ç±»æ•°æ®ã€‚</li>
<li>TeNCAé€šè¿‡æ”¹è¿›è®­ç»ƒç­–ç•¥ï¼Œå®ç°è‡ªé€‚åº”æŸå¤±è®¡ç®—ï¼Œå¹¶å®šä¹‰æ–¹æ³•çš„è¿­ä»£æ€§è´¨ä»¥æ¨¡æ‹Ÿç‰©ç†æ—¶åºè¿‡ç¨‹ã€‚</li>
<li>TeNCAæ¨¡å‹èƒ½å¤Ÿå­¦ä¹ å¯¹æ¯”å¢å¼ºçš„ç”Ÿç†æ¼”å˜è¿‡ç¨‹ã€‚</li>
<li>åœ¨ä¹³è…ºMRIæ•°æ®é›†ä¸Šï¼ŒTeNCAè¡¨ç°ä¼˜è¶Šï¼Œè¶…è¶Šç°æœ‰æ–¹æ³•çš„å›¾åƒç”Ÿæˆæ•ˆæœã€‚</li>
<li>TeNCAç”Ÿæˆçš„å›¾åƒä¸çœŸå®å¯¹æ¯”åºåˆ—ç›¸ç¬¦ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.18720">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-fd944fca5c0d209679509e5c68a2074d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-9eab54862b16379d6ec78612deb5f58b.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="MARL-MambaContour-Unleashing-Multi-Agent-Deep-Reinforcement-Learning-for-Active-Contour-Optimization-in-Medical-Image-Segmentation"><a href="#MARL-MambaContour-Unleashing-Multi-Agent-Deep-Reinforcement-Learning-for-Active-Contour-Optimization-in-Medical-Image-Segmentation" class="headerlink" title="MARL-MambaContour: Unleashing Multi-Agent Deep Reinforcement Learning   for Active Contour Optimization in Medical Image Segmentation"></a>MARL-MambaContour: Unleashing Multi-Agent Deep Reinforcement Learning   for Active Contour Optimization in Medical Image Segmentation</h2><p><strong>Authors:Ruicheng Zhang, Yu Sun, Zeyu Zhang, Jinai Li, Xiaofan Liu, Au Hoi Fan, Haowei Guo, Puxin Yan</strong></p>
<p>We introduce MARL-MambaContour, the first contour-based medical image segmentation framework based on Multi-Agent Reinforcement Learning (MARL). Our approach reframes segmentation as a multi-agent cooperation task focused on generate topologically consistent object-level contours, addressing the limitations of traditional pixel-based methods which could lack topological constraints and holistic structural awareness of anatomical regions. Each contour point is modeled as an autonomous agent that iteratively adjusts its position to align precisely with the target boundary, enabling adaptation to blurred edges and intricate morphologies common in medical images. This iterative adjustment process is optimized by a contour-specific Soft Actor-Critic (SAC) algorithm, further enhanced with the Entropy Regularization Adjustment Mechanism (ERAM) which dynamically balance agent exploration with contour smoothness. Furthermore, the framework incorporates a Mamba-based policy network featuring a novel Bidirectional Cross-attention Hidden-state Fusion Mechanism (BCHFM). This mechanism mitigates potential memory confusion limitations associated with long-range modeling in state space models, thereby facilitating more accurate inter-agent information exchange and informed decision-making. Extensive experiments on five diverse medical imaging datasets demonstrate the state-of-the-art performance of MARL-MambaContour, highlighting its potential as an accurate and robust clinical application. </p>
<blockquote>
<p>æˆ‘ä»¬å¼•å…¥äº†åŸºäºå¤šæ™ºèƒ½ä½“å¼ºåŒ–å­¦ä¹ ï¼ˆMARLï¼‰çš„é¦–ä¸ªè½®å»“åŸºç¡€åŒ»å­¦å›¾åƒåˆ†å‰²æ¡†æ¶â€”â€”MARL-MambaContourã€‚æˆ‘ä»¬çš„æ–¹æ³•å°†åˆ†å‰²é‡æ–°æ„å»ºä¸ºä¸€ä¸ªå¤šæ™ºèƒ½ä½“åˆä½œä»»åŠ¡ï¼Œä¾§é‡äºç”Ÿæˆæ‹“æ‰‘ä¸€è‡´çš„ç‰©ä½“çº§è½®å»“ï¼Œè§£å†³äº†ä¼ ç»Ÿåƒç´ çº§æ–¹æ³•å¯èƒ½ç¼ºä¹æ‹“æ‰‘çº¦æŸå’Œæ•´ä½“ç»“æ„æ„ŸçŸ¥è§£å‰–åŒºåŸŸçš„å±€é™æ€§ã€‚æ¯ä¸ªè½®å»“ç‚¹éƒ½è¢«å»ºæ¨¡ä¸ºä¸€ä¸ªè‡ªä¸»æ™ºèƒ½ä½“ï¼Œå¯ä»¥è¿­ä»£è°ƒæ•´å…¶ä½ç½®ä»¥ç²¾ç¡®ä¸ç›®æ ‡è¾¹ç•Œå¯¹é½ï¼Œä»è€Œé€‚åº”åŒ»å­¦å›¾åƒä¸­å¸¸è§çš„æ¨¡ç³Šè¾¹ç¼˜å’Œå¤æ‚å½¢æ€ã€‚è¿™ç§è¿­ä»£è°ƒæ•´è¿‡ç¨‹é€šè¿‡è½®å»“ç‰¹å®šçš„æŸ”æ€§æ¼”å‘˜è¯„è®ºå®¶ï¼ˆSACï¼‰ç®—æ³•è¿›è¡Œä¼˜åŒ–ï¼Œé€šè¿‡ç†µæ­£åˆ™åŒ–è°ƒæ•´æœºåˆ¶ï¼ˆERAMï¼‰è¿›ä¸€æ­¥å¢å¼ºï¼ŒåŠ¨æ€å¹³è¡¡æ™ºèƒ½ä½“çš„æ¢ç´¢ä¸è½®å»“å¹³æ»‘åº¦ã€‚æ­¤å¤–ï¼Œè¯¥æ¡†æ¶é‡‡ç”¨åŸºäºMambaçš„ç­–ç•¥ç½‘ç»œï¼Œå¹¶å¼•å…¥äº†ä¸€ç§æ–°å‹åŒå‘äº¤å‰æ³¨æ„éšè—çŠ¶æ€èåˆæœºåˆ¶ï¼ˆBCHFMï¼‰ã€‚è¯¥æœºåˆ¶ç¼“è§£äº†çŠ¶æ€ç©ºé—´æ¨¡å‹ä¸­é•¿ç¨‹å»ºæ¨¡å¯èƒ½å¸¦æ¥çš„æ½œåœ¨è®°å¿†æ··æ·†é™åˆ¶ï¼Œä»è€Œä¿ƒè¿›äº†æ›´å‡†ç¡®çš„è·¨æ™ºèƒ½ä½“ä¿¡æ¯äº¤æ¢å’Œå†³ç­–ã€‚åœ¨äº”ä¸ªä¸åŒçš„åŒ»å­¦æˆåƒæ•°æ®é›†ä¸Šçš„å¹¿æ³›å®éªŒè¯æ˜äº†MARL-MambaContourçš„å“è¶Šæ€§èƒ½ï¼Œå‡¸æ˜¾äº†å…¶åœ¨ä¸´åºŠåº”ç”¨ä¸­ä½œä¸ºå‡†ç¡®ä¸”ç¨³å¥å·¥å…·çš„æ½œåŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.18679v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>MARL-MambaContouræ˜¯é¦–ä¸ªåŸºäºå¤šæ™ºèƒ½ä½“å¼ºåŒ–å­¦ä¹ ï¼ˆMARLï¼‰çš„è½®å»“åŸºç¡€åŒ»å­¦å›¾åƒåˆ†å‰²æ¡†æ¶ã€‚å®ƒå°†åˆ†å‰²é‡æ–°æ„å»ºä¸ºä¸€ä¸ªå¤šæ™ºèƒ½ä½“åˆä½œä»»åŠ¡ï¼Œä¾§é‡äºç”Ÿæˆæ‹“æ‰‘ä¸€è‡´çš„ç‰©ä½“çº§è½®å»“ï¼Œè§£å†³äº†ä¼ ç»Ÿåƒç´ çº§æ–¹æ³•ç¼ºä¹æ‹“æ‰‘çº¦æŸå’Œæ•´ä½“ç»“æ„æ„ŸçŸ¥åŒºåŸŸçš„é—®é¢˜ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>MARL-MambaContouræ˜¯é¦–ä¸ªåŸºäºMulti-Agent Reinforcement Learning (MARL)çš„è½®å»“åŸºç¡€åŒ»å­¦å›¾åƒåˆ†å‰²æ¡†æ¶ã€‚</li>
<li>è¯¥æ–¹æ³•å°†åŒ»å­¦å›¾åƒåˆ†å‰²é‡æ–°æ„å»ºä¸ºå¤šæ™ºèƒ½ä½“åˆä½œä»»åŠ¡ï¼Œä¾§é‡äºç”Ÿæˆæ‹“æ‰‘ä¸€è‡´çš„ç‰©ä½“è½®å»“ã€‚</li>
<li>ä¼ ç»Ÿåƒç´ çº§æ–¹æ³•çš„å±€é™æ€§åœ¨äºç¼ºä¹æ‹“æ‰‘çº¦æŸå’Œæ•´ä½“ç»“æ„æ„ŸçŸ¥åŒºåŸŸçš„èƒ½åŠ›ã€‚</li>
<li>æ¯ä¸ªè½®å»“ç‚¹è¢«å»ºæ¨¡ä¸ºä¸€ä¸ªè‡ªä¸»æ™ºèƒ½ä½“ï¼Œå¯è¿­ä»£è°ƒæ•´å…¶ä½ç½®ä»¥ç²¾ç¡®å¯¹å‡†ç›®æ ‡è¾¹ç•Œã€‚</li>
<li>é‡‡ç”¨äº†è½®å»“ç‰¹å®šçš„Soft Actor-Critic (SAC)ç®—æ³•ï¼Œå¹¶ç»“åˆäº†ï¿½ï¿½nropy Regularization Adjustment Mechanism (ERAM)æ¥å¹³è¡¡æ™ºèƒ½ä½“çš„æ¢ç´¢ä¸è½®å»“å¹³æ»‘ã€‚</li>
<li>æ¡†æ¶ä¸­èå…¥äº†Mambaç­–ç•¥ç½‘ç»œï¼Œå¹¶å¼•å…¥äº†Bidirectional Cross-attention Hidden-state Fusion Mechanism (BCHFM)æœºåˆ¶æ¥è§£å†³é•¿æœŸå»ºæ¨¡ä¸­çš„æ½œåœ¨è®°å¿†æ··æ·†é—®é¢˜ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.18679">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-1160180733a1f15701879c81a783397f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d7b8741f0392cde885ecb4ef733c361b.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="RL-Driven-Semantic-Compression-Model-Selection-and-Resource-Allocation-in-Semantic-Communication-Systems"><a href="#RL-Driven-Semantic-Compression-Model-Selection-and-Resource-Allocation-in-Semantic-Communication-Systems" class="headerlink" title="RL-Driven Semantic Compression Model Selection and Resource Allocation   in Semantic Communication Systems"></a>RL-Driven Semantic Compression Model Selection and Resource Allocation   in Semantic Communication Systems</h2><p><strong>Authors:Xinyi Lin, Peizheng Li, Adnan Aijaz</strong></p>
<p>Semantic communication (SemCom) is an emerging paradigm that leverages semantic-level understanding to improve communication efficiency, particularly in resource-constrained scenarios. However, existing SemCom systems often overlook diverse computational and communication capabilities and requirements among different users. Motivated by the need to adaptively balance semantic accuracy, latency, and energy consumption, this paper presents a reinforcement learning (RL)-driven framework for semantic compression model (SCM) selection and resource allocation in multi-user SemCom systems. To address the challenges of balancing image reconstruction quality and communication performance, a system-level optimization metric called Rate-Distortion Efficiency (RDE) has been defined. The framework considers multiple SCMs with varying complexity and resource requirements. A proximal policy optimization (PPO)-based RL approach is developed to dynamically select SCMs and allocate bandwidth and power under non-convex constraints. Simulations demonstrate that the proposed method outperforms several baseline strategies. This paper also discusses the generalization ability, computational complexity, scalability, and practical implications of the framework for real-world SemCom systems. </p>
<blockquote>
<p>è¯­ä¹‰é€šä¿¡ï¼ˆSemComï¼‰æ˜¯ä¸€ç§æ–°å…´èŒƒå¼ï¼Œåˆ©ç”¨è¯­ä¹‰çº§ç†è§£æ¥æé«˜é€šä¿¡æ•ˆç‡ï¼Œç‰¹åˆ«æ˜¯åœ¨èµ„æºå—é™çš„åœºæ™¯ä¸­ã€‚ç„¶è€Œï¼Œç°æœ‰çš„SemComç³»ç»Ÿå¾€å¾€å¿½è§†äº†ä¸åŒç”¨æˆ·ä¹‹é—´è®¡ç®—å’Œé€šä¿¡èƒ½åŠ›çš„å¤šæ ·æ€§å’Œè¦æ±‚ã€‚ä¸ºäº†è‡ªé€‚åº”åœ°å¹³è¡¡è¯­ä¹‰å‡†ç¡®æ€§ã€å»¶è¿Ÿå’Œèƒ½è€—çš„éœ€æ±‚ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰çš„è¯­ä¹‰å‹ç¼©æ¨¡å‹ï¼ˆSCMï¼‰é€‰æ‹©åŠå¤šç”¨æˆ·SemComç³»ç»Ÿä¸­èµ„æºåˆ†é…çš„æ¡†æ¶ã€‚ä¸ºäº†è§£å†³å›¾åƒé‡å»ºè´¨é‡ä¸é€šä¿¡æ€§èƒ½ä¹‹é—´çš„å¹³è¡¡æŒ‘æˆ˜ï¼Œå®šä¹‰äº†ä¸€ç§ç³»ç»Ÿçº§ä¼˜åŒ–æŒ‡æ ‡â€”â€”é€Ÿç‡å¤±çœŸæ•ˆç‡ï¼ˆRDEï¼‰ã€‚è¯¥æ¡†æ¶è€ƒè™‘äº†å…·æœ‰ä¸åŒå¤æ‚åº¦å’Œèµ„æºè¦æ±‚çš„å¤šä¸ªSCMã€‚å¼€å‘äº†ä¸€ç§åŸºäºè¿‘ç«¯ç­–ç•¥ä¼˜åŒ–ï¼ˆPPOï¼‰çš„RLæ–¹æ³•ï¼Œä»¥åœ¨éçº¿æ€§çº¦æŸä¸‹åŠ¨æ€é€‰æ‹©SCMå¹¶åˆ†é…å¸¦å®½å’ŒåŠŸç‡ã€‚æ¨¡æ‹Ÿç»“æœè¡¨æ˜ï¼Œæ‰€æå‡ºçš„æ–¹æ³•ä¼˜äºå‡ ç§åŸºçº¿ç­–ç•¥ã€‚æœ¬æ–‡è¿˜è®¨è®ºäº†è¯¥æ¡†æ¶åœ¨ç°å®ä¸–ç•ŒSemComç³»ç»Ÿä¸­çš„æ³›åŒ–èƒ½åŠ›ã€è®¡ç®—å¤æ‚æ€§ã€å¯æ‰©å±•æ€§å’Œå®é™…å½±å“ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.18660v1">PDF</a> Accepted by PIMRC 2025</p>
<p><strong>Summary</strong></p>
<p>è¯­ä¹‰é€šä¿¡ï¼ˆSemComï¼‰åˆ©ç”¨è¯­ä¹‰çº§ç†è§£æé«˜é€šä¿¡æ•ˆç‡ï¼Œå°¤å…¶åœ¨èµ„æºå—é™åœºæ™¯ä¸­ã€‚å½“å‰SemComç³»ç»Ÿå¿½è§†äº†ä¸åŒç”¨æˆ·çš„è®¡ç®—å’Œé€šä¿¡èƒ½åŠ›å·®å¼‚ã€‚ä¸ºå¹³è¡¡è¯­ä¹‰å‡†ç¡®æ€§ã€å»¶è¿Ÿå’Œèƒ½è€—ï¼Œæœ¬æ–‡æå‡ºä¸€ç§åŸºäºå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰çš„è¯­ä¹‰å‹ç¼©æ¨¡å‹ï¼ˆSCMï¼‰é€‰æ‹©åŠèµ„æºåˆ†é…æ¡†æ¶ã€‚ä¸ºè§£å†³å›¾åƒé‡å»ºè´¨é‡ä¸é€šä¿¡æ€§èƒ½å¹³è¡¡é—®é¢˜ï¼Œå®šä¹‰äº†ç³»ç»Ÿçº§ä¼˜åŒ–æŒ‡æ ‡â€”â€”é€Ÿç‡å¤±çœŸæ•ˆç‡ï¼ˆRDEï¼‰ã€‚æ¡†æ¶è€ƒè™‘å¤šç§ä¸åŒå¤æ‚åº¦å’Œèµ„æºéœ€æ±‚çš„SCMã€‚é‡‡ç”¨åŸºäºè¿‘ç«¯ç­–ç•¥ä¼˜åŒ–ï¼ˆPPOï¼‰çš„RLæ–¹æ³•ï¼Œåœ¨éçº¿æ€§çº¦æŸä¸‹åŠ¨æ€é€‰æ‹©SCMå¹¶åˆ†é…å¸¦å®½å’ŒåŠŸç‡ã€‚æ¨¡æ‹Ÿç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•ä¼˜äºå¤šç§åŸºçº¿ç­–ç•¥ï¼Œå¹¶è®¨è®ºäº†æ¡†æ¶çš„é€šç”¨æ€§ã€è®¡ç®—å¤æ‚æ€§ã€å¯æ‰©å±•æ€§å’Œå¯¹å®é™…SemComç³»ç»Ÿçš„å®è·µæ„ä¹‰ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>SemComåˆ©ç”¨è¯­ä¹‰çº§ç†è§£æé«˜é€šä¿¡æ•ˆç‡ï¼Œç‰¹åˆ«æ˜¯åœ¨èµ„æºå—é™ç¯å¢ƒä¸­ã€‚</li>
<li>å½“å‰SemComç³»ç»Ÿå¿½ç•¥äº†ä¸åŒç”¨æˆ·ä¹‹é—´çš„è®¡ç®—å’Œé€šä¿¡èƒ½åŠ›å·®å¼‚ã€‚</li>
<li>ä¸ºå¹³è¡¡è¯­ä¹‰å‡†ç¡®æ€§ã€å»¶è¿Ÿå’Œèƒ½è€—ï¼Œæå‡ºäº†ä¸€ä¸ªåŸºäºå¼ºåŒ–å­¦ä¹ çš„æ¡†æ¶ã€‚</li>
<li>å®šä¹‰äº†ç³»ç»Ÿçº§ä¼˜åŒ–æŒ‡æ ‡â€”â€”é€Ÿç‡å¤±çœŸæ•ˆç‡ï¼ˆRDEï¼‰ä»¥å¹³è¡¡å›¾åƒé‡å»ºè´¨é‡å’Œé€šä¿¡æ€§èƒ½ã€‚</li>
<li>æ¡†æ¶è€ƒè™‘äº†å¤šç§ä¸åŒå¤æ‚åº¦å’Œèµ„æºéœ€æ±‚çš„è¯­ä¹‰å‹ç¼©æ¨¡å‹ï¼ˆSCMï¼‰ã€‚</li>
<li>é‡‡ç”¨åŸºäºè¿‘ç«¯ç­–ç•¥ä¼˜åŒ–ï¼ˆPPOï¼‰çš„RLæ–¹æ³•ï¼ŒåŠ¨æ€é€‰æ‹©SCMå¹¶åˆ†é…èµ„æºã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.18660">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-bab22b03b0eb55d3d5da6f6fce59a23c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d747b696c8b658134f91f3541ebd65fa.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-48f034832b8ba6ccfcd1b7c866d9580d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b17a610e9083249be5332ed0fa2d3579.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-612e77d6bae5287d08fa4abbc7696c78.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7c57c1718e803195b75ca6097af08a35.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="SafeClick-Error-Tolerant-Interactive-Segmentation-of-Any-Medical-Volumes-via-Hierarchical-Expert-Consensus"><a href="#SafeClick-Error-Tolerant-Interactive-Segmentation-of-Any-Medical-Volumes-via-Hierarchical-Expert-Consensus" class="headerlink" title="SafeClick: Error-Tolerant Interactive Segmentation of Any Medical   Volumes via Hierarchical Expert Consensus"></a>SafeClick: Error-Tolerant Interactive Segmentation of Any Medical   Volumes via Hierarchical Expert Consensus</h2><p><strong>Authors:Yifan Gao, Jiaxi Sheng, Wenbin Wu, Haoyue Li, Yaoxian Dong, Chaoyang Ge, Feng Yuan, Xin Gao</strong></p>
<p>Foundation models for volumetric medical image segmentation have emerged as powerful tools in clinical workflows, enabling radiologists to delineate regions of interest through intuitive clicks. While these models demonstrate promising capabilities in segmenting previously unseen anatomical structures, their performance is strongly influenced by prompt quality. In clinical settings, radiologists often provide suboptimal prompts, which affects segmentation reliability and accuracy. To address this limitation, we present SafeClick, an error-tolerant interactive segmentation approach for medical volumes based on hierarchical expert consensus. SafeClick operates as a plug-and-play module compatible with foundation models including SAM 2 and MedSAM 2. The framework consists of two key components: a collaborative expert layer (CEL) that generates diverse feature representations through specialized transformer modules, and a consensus reasoning layer (CRL) that performs cross-referencing and adaptive integration of these features. This architecture transforms the segmentation process from a prompt-dependent operation to a robust framework capable of producing accurate results despite imperfect user inputs. Extensive experiments across 15 public datasets demonstrate that our plug-and-play approach consistently improves the performance of base foundation models, with particularly significant gains when working with imperfect prompts. The source code is available at <a target="_blank" rel="noopener" href="https://github.com/yifangao112/SafeClick">https://github.com/yifangao112/SafeClick</a>. </p>
<blockquote>
<p>åŒ»å­¦å›¾åƒåˆ†å‰²ä¸­çš„åŸºç¡€æ¨¡å‹å·²ç»ä½œä¸ºå¼ºå¤§çš„å·¥å…·åº”ç”¨äºä¸´åºŠå·¥ä½œæµç¨‹ä¸­ï¼Œä½¿æ”¾å°„ç§‘åŒ»ç”Ÿèƒ½å¤Ÿé€šè¿‡ç›´è§‚ç‚¹å‡»æ¥åˆ’åˆ†æ„Ÿå…´è¶£åŒºåŸŸã€‚è™½ç„¶è¿™äº›æ¨¡å‹åœ¨åˆ†å‰²ä»¥å‰æœªè§è¿‡çš„è§£å‰–ç»“æ„ä¸Šæ˜¾ç¤ºå‡ºæœ‰å‰æ™¯çš„èƒ½åŠ›ï¼Œä½†å®ƒä»¬çš„æ€§èƒ½å—åˆ°æç¤ºè´¨é‡çš„å½±å“ã€‚åœ¨ä¸´åºŠç¯å¢ƒä¸­ï¼Œæ”¾å°„ç§‘åŒ»ç”Ÿç»å¸¸æä¾›æ¬¡ä¼˜æç¤ºï¼Œè¿™ä¼šå½±å“åˆ†å‰²çš„å¯é æ€§å’Œå‡†ç¡®æ€§ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†SafeClickï¼Œè¿™æ˜¯ä¸€ç§åŸºäºåˆ†å±‚ä¸“å®¶å…±è¯†çš„å®¹é”™äº¤äº’å¼åˆ†å‰²æ–¹æ³•ï¼Œç”¨äºåŒ»å­¦å›¾åƒå·ç§¯ã€‚SafeClickå¯ä»¥ä½œä¸ºä¸åŸºç¡€æ¨¡å‹ï¼ˆåŒ…æ‹¬SAM 2å’ŒMedSAM 2ï¼‰å…¼å®¹çš„å³æ’å³ç”¨æ¨¡å—è¿è¡Œã€‚è¯¥æ¡†æ¶ç”±ä¸¤ä¸ªå…³é”®ç»„ä»¶ç»„æˆï¼šä¸€ä¸ªåä½œä¸“å®¶å±‚ï¼ˆCELï¼‰ï¼Œå®ƒé€šè¿‡ä¸“ç”¨å˜å‹å™¨æ¨¡å—ç”Ÿæˆå¤šæ ·åŒ–çš„ç‰¹å¾è¡¨ç¤ºï¼›ä¸€ä¸ªå…±è¯†æ¨ç†å±‚ï¼ˆCRLï¼‰ï¼Œå®ƒæ‰§è¡Œè¿™äº›ç‰¹å¾çš„äº¤å‰å¼•ç”¨å’Œè‡ªé€‚åº”é›†æˆã€‚è¿™ç§æ¶æ„å°†åˆ†å‰²è¿‡ç¨‹ä»ä¾èµ–äºæç¤ºçš„æ“ä½œè½¬å˜ä¸ºä¸€ä¸ªç¨³å¥çš„æ¡†æ¶ï¼Œå³ä½¿åœ¨ä¸å®Œç¾çš„ç”¨æˆ·è¾“å…¥æƒ…å†µä¸‹ï¼Œä¹Ÿèƒ½äº§ç”Ÿå‡†ç¡®çš„ç»“æœã€‚åœ¨15ä¸ªå…¬å…±æ•°æ®é›†ä¸Šçš„å¤§é‡å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„å³æ’å³ç”¨æ–¹æ³•ä¸æ–­æ”¹è¿›åŸºç¡€æ¨¡å‹çš„æ€§èƒ½ï¼Œç‰¹åˆ«æ˜¯åœ¨å¤„ç†ä¸å®Œç¾çš„æç¤ºæ—¶è·å¾—äº†ç‰¹åˆ«æ˜¾è‘—çš„æ”¶ç›Šã€‚æºä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/yifangao1d%E5%A4%84%E8%8E%B7%E5%BE%97%E3%80%82">https://github.com/yifangao1då¤„è·å–ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.18404v1">PDF</a> MICCAI 2025</p>
<p><strong>æ‘˜è¦</strong></p>
<p>åŸºäºåˆ†å±‚ä¸“å®¶å…±è¯†çš„SafeClické”™è¯¯å®¹å¿äº¤äº’åˆ†å‰²æ–¹æ³•ï¼Œæå‡äº†åŒ»å­¦ä½“ç§¯å›¾åƒåˆ†å‰²çš„å¯é æ€§åŠå‡†ç¡®æ€§ã€‚SafeClickä½œä¸ºæ¨¡å—å³æ’å³ç”¨ï¼Œå¯ä¸SAM 2å’ŒMedSAM 2ç­‰åŸºå‡†æ¨¡å‹å…¼å®¹ã€‚å…¶åŒ…æ‹¬åä½œä¸“å®¶å±‚ï¼ˆCELï¼‰ä¸å…±è¯†æ¨ç†å±‚ï¼ˆCRLï¼‰ã€‚CELé€šè¿‡ç‰¹æ®Štransformeræ¨¡å—ç”Ÿæˆå¤šæ ·åŒ–ç‰¹å¾è¡¨ç¤ºï¼ŒCRLåˆ™è¿›è¡Œç‰¹å¾äº¤å‰å¼•ç”¨å’Œè‡ªé€‚åº”é›†æˆã€‚SafeClickå°†ä¾èµ–æç¤ºçš„åˆ†å‰²è¿‡ç¨‹è½¬å˜ä¸ºç¨³å¥æ¡†æ¶ï¼Œå¯äº§å‡ºå‡†ç¡®ç»“æœï¼Œå³ä½¿é¢å¯¹ä¸å®Œç¾çš„ç”¨æˆ·è¾“å…¥ã€‚åœ¨15ä¸ªå…¬å¼€æ•°æ®é›†ä¸Šçš„å¹¿æ³›å®éªŒæ˜¾ç¤ºï¼Œè¯¥å³æ’å³ç”¨æ–¹æ³•æŒç»­æå‡äº†åŸºç¡€æ¨¡å‹çš„æ€§èƒ½ï¼Œåœ¨ä¸å®Œç¾æç¤ºä¸‹è¡¨ç°å°¤ä¸ºçªå‡ºã€‚æºç å·²ä¸Šä¼ è‡³<a target="_blank" rel="noopener" href="https://github.com/yifangao112/SafeClick">é“¾æ¥</a>ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ul>
<li>åŒ»å­¦å›¾åƒåˆ†å‰²æ¨¡å‹å·²æˆä¸ºä¸´åºŠæµç¨‹ä¸­çš„å¼ºå¤§å·¥å…·ï¼Œä½¿æ”¾å°„ç§‘åŒ»ç”Ÿèƒ½å¤Ÿé€šè¿‡ç›´è§‚ç‚¹å‡»åˆ’åˆ†æ„Ÿå…´è¶£åŒºåŸŸã€‚</li>
<li>å°½ç®¡è¿™äº›æ¨¡å‹åœ¨åˆ†å‰²æœªè§è§£å‰–ç»“æ„æ–¹é¢æ˜¾ç¤ºå‡ºå·¨å¤§æ½œåŠ›ï¼Œä½†å…¶æ€§èƒ½å¼ºçƒˆå—åˆ°æç¤ºè´¨é‡çš„å½±å“ã€‚</li>
<li>åœ¨ä¸´åºŠç¯å¢ƒä¸­ï¼Œæ”¾å°„ç§‘åŒ»ç”Ÿæä¾›çš„æç¤ºé€šå¸¸ä¸ç†æƒ³ï¼Œå½±å“åˆ†å‰²çš„å¯é æ€§å’Œå‡†ç¡®æ€§ã€‚</li>
<li>SafeClickæ–¹æ³•é€šè¿‡åˆ†å±‚ä¸“å®¶å…±è¯†è§£å†³äº†è¿™ä¸€é—®é¢˜ï¼ŒåŒ…æ‹¬åä½œä¸“å®¶å±‚å’Œå…±è¯†æ¨ç†å±‚ã€‚</li>
<li>SafeClickä½œä¸ºæ¨¡å—å³æ’å³ç”¨ï¼Œä¸ç°æœ‰åŸºå‡†æ¨¡å‹å…¼å®¹ï¼Œå¯ç”Ÿæˆå¤šæ ·åŒ–ç‰¹å¾è¡¨ç¤ºå¹¶è¿›è¡Œäº¤å‰å¼•ç”¨å’Œè‡ªé€‚åº”é›†æˆã€‚</li>
<li>SafeClickå°†ä¾èµ–æç¤ºçš„åˆ†å‰²è¿‡ç¨‹è½¬å˜ä¸ºç¨³å¥æ¡†æ¶ï¼Œå³ä½¿é¢å¯¹ä¸å®Œç¾çš„ç”¨æˆ·è¾“å…¥ä¹Ÿèƒ½äº§ç”Ÿå‡†ç¡®ç»“æœã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.18404">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-75fb2a67b031d99cf2ffd86ee3147f73.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c2eca049847711347d73579972269eee.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-0719f1de554891c9bf907b63fa41adf0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-47cea96887165c76a7278264af698bf3.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-50fdbf74ff05ec4eb3132dea2ffd0161.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Taming-Vision-Language-Models-for-Medical-Image-Analysis-A-Comprehensive-Review"><a href="#Taming-Vision-Language-Models-for-Medical-Image-Analysis-A-Comprehensive-Review" class="headerlink" title="Taming Vision-Language Models for Medical Image Analysis: A   Comprehensive Review"></a>Taming Vision-Language Models for Medical Image Analysis: A   Comprehensive Review</h2><p><strong>Authors:Haoneng Lin, Cheng Xu, Jing Qin</strong></p>
<p>Modern Vision-Language Models (VLMs) exhibit unprecedented capabilities in cross-modal semantic understanding between visual and textual modalities. Given the intrinsic need for multi-modal integration in clinical applications, VLMs have emerged as a promising solution for a wide range of medical image analysis tasks. However, adapting general-purpose VLMs to medical domain poses numerous challenges, such as large domain gaps, complicated pathological variations, and diversity and uniqueness of different tasks. The central purpose of this review is to systematically summarize recent advances in adapting VLMs for medical image analysis, analyzing current challenges, and recommending promising yet urgent directions for further investigations. We begin by introducing core learning strategies for medical VLMs, including pretraining, fine-tuning, and prompt learning. We then categorize five major VLM adaptation strategies for medical image analysis. These strategies are further analyzed across eleven medical imaging tasks to illustrate their current practical implementations. Furthermore, we analyze key challenges that impede the effective adaptation of VLMs to clinical applications and discuss potential directions for future research. We also provide an open-access repository of related literature to facilitate further research, available at <a target="_blank" rel="noopener" href="https://github.com/haonenglin/Awesome-VLM-for-MIA">https://github.com/haonenglin/Awesome-VLM-for-MIA</a>. It is anticipated that this article can help researchers who are interested in harnessing VLMs in medical image analysis tasks have a better understanding on their capabilities and limitations, as well as current technical barriers, to promote their innovative, robust, and safe application in clinical practice. </p>
<blockquote>
<p>ç°ä»£è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰åœ¨è§†è§‰å’Œæ–‡æœ¬æ¨¡æ€ä¹‹é—´çš„è·¨æ¨¡æ€è¯­ä¹‰ç†è§£æ–¹é¢å±•ç°å‡ºäº†å‰æ‰€æœªæœ‰çš„èƒ½åŠ›ã€‚è€ƒè™‘åˆ°ä¸´åºŠåº”ç”¨ä¸­å¯¹å¤šæ¨¡æ€é›†æˆçš„å†…åœ¨éœ€æ±‚ï¼ŒVLMså·²ç»ä½œä¸ºä¸€ç§æœ‰å‰æ™¯çš„è§£å†³æ–¹æ¡ˆå¹¿æ³›åº”ç”¨äºåŒ»å­¦å½±åƒåˆ†æä»»åŠ¡ã€‚ç„¶è€Œï¼Œå°†é€šç”¨VLMsé€‚åº”äºåŒ»å­¦é¢†åŸŸé¢ä¸´ç€è¯¸å¤šæŒ‘æˆ˜ï¼Œå¦‚é¢†åŸŸå·®è·å¤§ã€ç—…ç†å˜åŒ–å¤æ‚ä»¥åŠä¸åŒä»»åŠ¡çš„å¤šæ ·æ€§å’Œç‹¬ç‰¹æ€§ã€‚æœ¬æ–‡çš„ä¸­å¿ƒç›®çš„æ˜¯ç³»ç»Ÿåœ°æ€»ç»“è¿‘æœŸå°†VLMsåº”ç”¨äºåŒ»å­¦å½±åƒåˆ†æçš„è¿›å±•ï¼Œåˆ†æå½“å‰é¢ä¸´çš„æŒ‘æˆ˜ï¼Œå¹¶ä¸ºè¿›ä¸€æ­¥çš„è°ƒæŸ¥æ¨èæœ‰å‰æ™¯ä¸”ç´§è¿«çš„æ–¹å‘ã€‚æˆ‘ä»¬é¦–å…ˆä»‹ç»åŒ»å­¦VLMsçš„æ ¸å¿ƒå­¦ä¹ ç­–ç•¥ï¼ŒåŒ…æ‹¬é¢„è®­ç»ƒã€å¾®è°ƒã€æç¤ºå­¦ä¹ ã€‚ç„¶åæˆ‘ä»¬å°†äº”å¤§VLMé€‚åº”ç­–ç•¥åˆ†ç±»ä¸ºåŒ»å­¦å½±åƒåˆ†æï¼Œå¹¶è¿›ä¸€æ­¥åœ¨åä¸€ä¸ªåŒ»å­¦å½±åƒä»»åŠ¡ä¸­åˆ†æäº†è¿™äº›ç­–ç•¥çš„å®é™…åº”ç”¨æƒ…å†µã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜åˆ†æäº†é˜»ç¢VLMsæœ‰æ•ˆé€‚åº”ä¸´åºŠåº”ç”¨çš„å…³é”®æŒ‘æˆ˜ï¼Œå¹¶è®¨è®ºäº†æœªæ¥ç ”ç©¶çš„æ–¹å‘ã€‚æˆ‘ä»¬è¿˜æä¾›äº†ä¸€ä¸ªå¼€æ”¾è®¿é—®çš„ç›¸å…³æ–‡çŒ®ä»“åº“ï¼Œä»¥ä¿ƒè¿›è¿›ä¸€æ­¥çš„ç ”ç©¶ï¼Œå¯é€šè¿‡<a target="_blank" rel="noopener" href="https://github.com/haonenglin/Awesome-VLM-for-MIA%E8%AE%BF%E9%97%AE%E3%80%82%E9%A2%84%E8%AE%A1%E6%9C%AC%E6%96%87%E8%83%BD%E5%B8%AE%E5%8A%A9%E5%AF%B9%E5%9C%A8%E5%8C%BB%E5%AD%A6%E5%BD%B1%E5%83%8F%E5%88%86%E6%9E%90%E4%BB%BB%E5%8A%A1%E4%B8%AD%E4%BD%BF%E7%94%A8VLMs%E6%84%9F%E5%85%B4%E8%B6%A3%E7%9A%84%E7%A0%94%E7%A9%B6%E4%BA%BA%E5%91%98%E6%9B%B4%E5%A5%BD%E5%9C%B0%E4%BA%86%E8%A7%A3%E5%AE%83%E4%BB%AC%E7%9A%84%E8%83%BD%E5%8A%9B%E3%80%81%E5%B1%80%E9%99%90%E6%80%A7%E4%BB%A5%E5%8F%8A%E5%BD%93%E5%89%8D%E7%9A%84%E6%8A%80%E6%9C%AF%E9%9A%9C%E7%A2%8D%EF%BC%8C%E4%BB%A5%E4%BF%83%E8%BF%9B%E5%85%B6%E5%9C%A8%E4%B8%B4%E5%BA%8A%E5%AE%9E%E8%B7%B5%E4%B8%AD%E7%9A%84%E5%88%9B%E6%96%B0%E3%80%81%E7%A8%B3%E5%81%A5%E5%92%8C%E5%AE%89%E5%85%A8%E5%BA%94%E7%94%A8%E3%80%82">https://github.com/haonenglin/Awesome-VLM-for-MIAè®¿é—®ã€‚é¢„è®¡æœ¬æ–‡èƒ½å¸®åŠ©å¯¹åœ¨åŒ»å­¦å½±åƒåˆ†æä»»åŠ¡ä¸­ä½¿ç”¨VLMsæ„Ÿå…´è¶£çš„ç ”ç©¶äººå‘˜æ›´å¥½åœ°äº†è§£å®ƒä»¬çš„èƒ½åŠ›ã€å±€é™æ€§ä»¥åŠå½“å‰çš„æŠ€æœ¯éšœç¢ï¼Œä»¥ä¿ƒè¿›å…¶åœ¨ä¸´åºŠå®è·µä¸­çš„åˆ›æ–°ã€ç¨³å¥å’Œå®‰å…¨åº”ç”¨ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.18378v1">PDF</a> 34 pages</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ç»¼è¿°äº†ç°ä»£è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰åœ¨åŒ»å­¦å›¾åƒåˆ†æé¢†åŸŸçš„æœ€æ–°è¿›å±•ï¼Œä»‹ç»äº†äº”å¤§åŒ»å­¦å›¾åƒåˆ†æé¢†åŸŸçš„VLMé€‚åº”ç­–ç•¥ï¼Œåˆ†æäº†å½“å‰é¢ä¸´çš„æŒ‘æˆ˜ï¼Œå¹¶æ¢è®¨äº†æœªæ¥ç ”ç©¶çš„æ½œåœ¨æ–¹å‘ã€‚æœ¬æ–‡æ—¨åœ¨å¸®åŠ©ç ”ç©¶äººå‘˜æ›´å¥½åœ°äº†è§£VLMsåœ¨åŒ»å­¦å›¾åƒåˆ†æä»»åŠ¡ä¸­çš„èƒ½åŠ›å’Œå±€é™æ€§ï¼Œä»¥åŠå½“å‰çš„æŠ€æœ¯éšœç¢ï¼Œä»¥ä¿ƒè¿›å…¶åœ¨ä¸´åºŠå®è·µä¸­çš„åˆ›æ–°ã€ç¨³å¥å’Œå®‰å…¨åº”ç”¨ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç°ä»£è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰åœ¨è·¨æ¨¡æ€è¯­ä¹‰ç†è§£æ–¹é¢å…·æœ‰å‰æ‰€æœªæœ‰çš„èƒ½åŠ›ï¼Œä¸ºåŒ»å­¦å›¾åƒåˆ†æä»»åŠ¡æä¾›äº†æœ‰å‰æ™¯çš„è§£å†³æ–¹æ¡ˆã€‚</li>
<li>VLMsåœ¨åŒ»å­¦é¢†åŸŸé¢ä¸´è¯¸å¤šæŒ‘æˆ˜ï¼Œå¦‚é¢†åŸŸå·®è·å¤§ã€ç—…ç†å˜åŒ–å¤æ‚ä»¥åŠä»»åŠ¡çš„å¤šæ ·æ€§å’Œç‹¬ç‰¹æ€§ã€‚</li>
<li>æ ¸å¿ƒå­¦ä¹ ç­–ç•¥åŒ…æ‹¬é¢„è®­ç»ƒã€å¾®è°ƒã€æç¤ºå­¦ä¹ ç­‰ã€‚</li>
<li>VLMé€‚åº”ç­–ç•¥ä¸»è¦åˆ†ä¸ºäº”å¤§ç±»ï¼Œå¹¶åœ¨åä¸€ä¸ªåŒ»å­¦æˆåƒä»»åŠ¡ä¸­è¿›è¡Œäº†å®è·µåº”ç”¨ã€‚</li>
<li>å½“å‰æŒ‘æˆ˜åŒ…æ‹¬å¦‚ä½•æœ‰æ•ˆé€‚åº”åŒ»å­¦åº”ç”¨ï¼Œæœªæ¥çš„ç ”ç©¶æ–¹å‘åŒ…æ‹¬æ”¹è¿›æ¨¡å‹æ€§èƒ½ã€å¢å¼ºå¯è§£é‡Šæ€§å’Œé²æ£’æ€§ç­‰ã€‚</li>
<li>æ–‡çŒ®åº“å¯é€šè¿‡é“¾æ¥è®¿é—®ï¼Œä¾¿äºè¿›ä¸€æ­¥ç ”ç©¶ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.18378">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-a0376da397e792bb832c73aac3361f73.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-264b884208ed5bdd521bb7d3d447a5a2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-36053664037a7a8e432d0772cd789522.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Rethinking-Decoder-Design-Improving-Biomarker-Segmentation-Using-Depth-to-Space-Restoration-and-Residual-Linear-Attention"><a href="#Rethinking-Decoder-Design-Improving-Biomarker-Segmentation-Using-Depth-to-Space-Restoration-and-Residual-Linear-Attention" class="headerlink" title="Rethinking Decoder Design: Improving Biomarker Segmentation Using   Depth-to-Space Restoration and Residual Linear Attention"></a>Rethinking Decoder Design: Improving Biomarker Segmentation Using   Depth-to-Space Restoration and Residual Linear Attention</h2><p><strong>Authors:Saad Wazir, Daeyoung Kim</strong></p>
<p>Segmenting biomarkers in medical images is crucial for various biotech applications. Despite advances, Transformer and CNN based methods often struggle with variations in staining and morphology, limiting feature extraction. In medical image segmentation, where datasets often have limited sample availability, recent state-of-the-art (SOTA) methods achieve higher accuracy by leveraging pre-trained encoders, whereas end-to-end methods tend to underperform. This is due to challenges in effectively transferring rich multiscale features from encoders to decoders, as well as limitations in decoder efficiency. To address these issues, we propose an architecture that captures multi-scale local and global contextual information and a novel decoder design, which effectively integrates features from the encoder, emphasizes important channels and regions, and reconstructs spatial dimensions to enhance segmentation accuracy. Our method, compatible with various encoders, outperforms SOTA methods, as demonstrated by experiments on four datasets and ablation studies. Specifically, our method achieves absolute performance gains of 2.76% on MoNuSeg, 3.12% on DSB, 2.87% on Electron Microscopy, and 4.03% on TNBC datasets compared to existing SOTA methods. Code: <a target="_blank" rel="noopener" href="https://github.com/saadwazir/MCADS-Decoder">https://github.com/saadwazir/MCADS-Decoder</a> </p>
<blockquote>
<p>åœ¨åŒ»å­¦å›¾åƒä¸­åˆ†å‰²ç”Ÿç‰©æ ‡å¿—ç‰©å¯¹äºå„ç§ç”Ÿç‰©æŠ€æœ¯åº”ç”¨è‡³å…³é‡è¦ã€‚å°½ç®¡æœ‰æ‰€è¿›å±•ï¼ŒåŸºäºTransformerå’ŒCNNçš„æ–¹æ³•ä»ç„¶ç»å¸¸é¢ä¸´æŸ“è‰²å’Œå½¢æ€å˜åŒ–å¸¦æ¥çš„æŒ‘æˆ˜ï¼Œé™åˆ¶äº†ç‰¹å¾æå–ã€‚åœ¨åŒ»å­¦å›¾åƒåˆ†å‰²ä¸­ï¼Œç”±äºæ•°æ®é›†é€šå¸¸æ ·æœ¬æœ‰é™ï¼Œæœ€è¿‘çš„æœ€å…ˆè¿›æ–¹æ³•é€šè¿‡åˆ©ç”¨é¢„è®­ç»ƒç¼–ç å™¨å®ç°æ›´é«˜çš„å‡†ç¡®æ€§ï¼Œè€Œç«¯åˆ°ç«¯çš„æ–¹æ³•å¾€å¾€è¡¨ç°ä¸ä½³ã€‚è¿™æ˜¯ç”±äºä»ç¼–ç å™¨åˆ°è§£ç å™¨æœ‰æ•ˆä¼ è¾“ä¸°å¯Œçš„å¤šå°ºåº¦ç‰¹å¾çš„æŒ‘æˆ˜ä»¥åŠè§£ç å™¨æ•ˆç‡çš„å±€é™æ€§ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ¶æ„ï¼Œè¯¥æ¶æ„èƒ½å¤Ÿæ•è·å¤šå°ºåº¦å±€éƒ¨å’Œå…¨å±€ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼Œä»¥åŠä¸€ç§æ–°å‹è§£ç å™¨è®¾è®¡ï¼Œè¯¥è§£ç å™¨èƒ½å¤Ÿæœ‰æ•ˆåœ°æ•´åˆæ¥è‡ªç¼–ç å™¨çš„ç‰¹å¾ï¼Œå¼ºè°ƒé‡è¦çš„é€šé“å’ŒåŒºåŸŸï¼Œå¹¶é‡å»ºç©ºé—´ç»´åº¦ä»¥æé«˜åˆ†å‰²å‡†ç¡®æ€§ã€‚æˆ‘ä»¬çš„æ–¹æ³•ä¸å„ç§ç¼–ç å™¨å…¼å®¹ï¼Œé€šè¿‡å››ä¸ªæ•°æ®é›†å’Œæ¶ˆèç ”ç©¶è¿›è¡Œçš„å®éªŒè¯æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•è¶…è¶Šäº†æœ€å…ˆè¿›çš„æ–¹æ³•ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨MoNuSegæ•°æ®é›†ä¸Šå®ç°äº†å¯¹ç°æœ‰å…ˆè¿›æ–¹æ³•2.76%çš„ç»å¯¹æ€§èƒ½æå‡ï¼Œåœ¨DSBæ•°æ®é›†ä¸Šå®ç°äº†3.12%çš„æå‡ï¼Œåœ¨ç”µå­æ˜¾å¾®é•œæ•°æ®é›†ä¸Šå®ç°äº†2.87%çš„æå‡ï¼Œå¹¶åœ¨TNBCæ•°æ®é›†ä¸Šå®ç°äº†4.03%çš„æå‡ã€‚ä»£ç åœ°å€ï¼š<a target="_blank" rel="noopener" href="https://github.com/saadwazir/MCADS-Decoder">https://github.com/saadwazir/MCADS-Decoder</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.18335v1">PDF</a> Proceedings of the Computer Vision and Pattern Recognition Conference   (CVPR), 2025, pp. 30861-30871</p>
<p><strong>Summary</strong></p>
<p>åŒ»å­¦å›¾åƒä¸­çš„ç”Ÿç‰©æ ‡å¿—ç‰©åˆ†å‰²å¯¹äºå¤šç§ç”Ÿç‰©æŠ€æœ¯åº”ç”¨è‡³å…³é‡è¦ã€‚ç°æœ‰æ–¹æ³•é¢ä¸´æŸ“è‰²å’Œå½¢æ€å˜åŒ–æŒ‘æˆ˜ï¼Œä»¥åŠæ•°æ®é›†æ ·æœ¬æœ‰é™çš„é—®é¢˜ã€‚ä¸ºæå‡åˆ†å‰²ç²¾åº¦ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°å‹æ¶æ„å’Œè§£ç å™¨è®¾è®¡ï¼Œèƒ½æ•æ‰å¤šå°ºåº¦å±€éƒ¨å’Œå…¨å±€ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼Œå¹¶æœ‰æ•ˆæ•´åˆç¼–ç å™¨ç‰¹å¾ã€‚è¯¥æ–¹æ³•å…¼å®¹å¤šç§ç¼–ç å™¨ï¼Œåœ¨å››ä¸ªæ•°æ®é›†ä¸Šçš„è¡¨ç°å‡è¶…è¶Šç°æœ‰æœ€å…ˆè¿›æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>åŒ»å­¦å›¾åƒä¸­çš„ç”Ÿç‰©æ ‡å¿—ç‰©åˆ†å‰²å¯¹ç”Ÿç‰©æŠ€æœ¯åº”ç”¨éå¸¸é‡è¦ã€‚</li>
<li>å½“å‰æ–¹æ³•é¢ä¸´æŸ“è‰²å’Œå½¢æ€å˜åŒ–æŒ‘æˆ˜ä»¥åŠæ•°æ®é›†æ ·æœ¬æœ‰é™çš„é—®é¢˜ã€‚</li>
<li>æå‡ºäº†ä¸€ç§æ–°å‹æ¶æ„å’Œè§£ç å™¨è®¾è®¡ä»¥è§£å†³ä¸Šè¿°é—®é¢˜ã€‚</li>
<li>è¯¥æ–¹æ³•èƒ½æ•æ‰å¤šå°ºåº¦å±€éƒ¨å’Œå…¨å±€ä¸Šä¸‹æ–‡ä¿¡æ¯ã€‚</li>
<li>æœ‰æ•ˆæ•´åˆç¼–ç å™¨ç‰¹å¾ï¼Œæå‡åˆ†å‰²ç²¾åº¦ã€‚</li>
<li>æ–¹æ³•å…¼å®¹å¤šç§ç¼–ç å™¨ï¼Œè¡¨ç°ä¼˜äºç°æœ‰æœ€å…ˆè¿›æ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.18335">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-8c177b86598e871aeca7af7f4d2ff690.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-0c2b83da567ea5058b2047af20f7a16a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-da5ae8963842714f3ba1e6375678edde.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-655a9f2acf5da122411e4184b96bdaec.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="BrainSymphony-A-Transformer-Driven-Fusion-of-fMRI-Time-Series-and-Structural-Connectivity"><a href="#BrainSymphony-A-Transformer-Driven-Fusion-of-fMRI-Time-Series-and-Structural-Connectivity" class="headerlink" title="BrainSymphony: A Transformer-Driven Fusion of fMRI Time Series and   Structural Connectivity"></a>BrainSymphony: A Transformer-Driven Fusion of fMRI Time Series and   Structural Connectivity</h2><p><strong>Authors:Moein Khajehnejad, Forough Habibollahi, Adeel Razi</strong></p>
<p>Existing foundation models for neuroimaging are often prohibitively large and data-intensive. We introduce BrainSymphony, a lightweight, parameter-efficient foundation model that achieves state-of-the-art performance while being pre-trained on significantly smaller public datasets. BrainSymphonyâ€™s strong multimodal architecture processes functional MRI data through parallel spatial and temporal transformer streams, which are then efficiently distilled into a unified representation by a Perceiver module. Concurrently, it models structural connectivity from diffusion MRI using a novel signed graph transformer to encode the brainâ€™s anatomical structure. These powerful, modality-specific representations are then integrated via an adaptive fusion gate. Despite its compact design, our model consistently outperforms larger models on a diverse range of downstream benchmarks, including classification, prediction, and unsupervised network identification tasks. Furthermore, our model revealed novel insights into brain dynamics using attention maps on a unique external psilocybin neuroimaging dataset (pre- and post-administration). BrainSymphony establishes that architecturally-aware, multimodal models can surpass their larger counterparts, paving the way for more accessible and powerful research in computational neuroscience. </p>
<blockquote>
<p>ç°æœ‰çš„ç¥ç»æˆåƒåŸºç¡€æ¨¡å‹é€šå¸¸è§„æ¨¡åºå¤§ä¸”æ•°æ®å¯†é›†ã€‚æˆ‘ä»¬æ¨å‡ºäº†BrainSymphonyï¼Œè¿™æ˜¯ä¸€ä¸ªè½»é‡çº§ã€å‚æ•°é«˜æ•ˆçš„åŸºç¡€æ¨¡å‹ï¼Œå®ƒåœ¨è¾ƒå°çš„å…¬å…±æ•°æ®é›†ä¸Šè¿›è¡Œé¢„è®­ç»ƒå³å¯å®ç°æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚BrainSymphonyå¼ºå¤§çš„å¤šæ¨¡æ€æ¶æ„é€šè¿‡å¹¶è¡Œç©ºé—´å’Œæ—¶é—´è½¬æ¢å™¨æµå¤„ç†åŠŸèƒ½ç£å…±æŒ¯æˆåƒæ•°æ®ï¼Œç„¶åé€šè¿‡æ„ŸçŸ¥å™¨æ¨¡å—æœ‰æ•ˆåœ°è’¸é¦æˆç»Ÿä¸€è¡¨ç¤ºã€‚åŒæ—¶ï¼Œå®ƒä½¿ç”¨æ–°å‹ç¬¦å·å›¾å˜æ¢å™¨å¯¹æ‰©æ•£ç£å…±æŒ¯æˆåƒè¿›è¡Œç»“æ„è¿æ¥å»ºæ¨¡ï¼Œä»¥ç¼–ç å¤§è„‘çš„ç»“æ„ã€‚è¿™äº›å¼ºå¤§ä¸”é’ˆå¯¹ç‰¹å®šæ¨¡æ€çš„è¡¨ç¤ºç„¶åé€šè¿‡è‡ªé€‚åº”èåˆé—¨è¿›è¡Œé›†æˆã€‚å°½ç®¡è®¾è®¡ç´§å‡‘ï¼Œæˆ‘ä»¬çš„æ¨¡å‹åœ¨å¤šç§ä¸‹æ¸¸åŸºå‡†æµ‹è¯•ä¸Šå§‹ç»ˆè¡¨ç°å‡ºè‰²ï¼ŒåŒ…æ‹¬åˆ†ç±»ã€é¢„æµ‹å’Œæ— ç›‘ç£ç½‘ç»œè¯†åˆ«ä»»åŠ¡ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬çš„æ¨¡å‹åˆ©ç”¨ç‹¬ç‰¹çš„å¤–æºæ€§è¿·å¹»è‡ç¥ç»æˆåƒæ•°æ®é›†ï¼ˆç»™è¯å‰åçš„æ•°æ®ï¼‰çš„æ³¨æ„åŠ›å›¾æ­ç¤ºäº†å¤§è„‘åŠ¨æ€çš„å…¨æ–°è§è§£ã€‚BrainSymphonyè¯æ˜ï¼Œç»“æ„æ„ŸçŸ¥çš„å¤šæ¨¡æ€æ¨¡å‹å¯ä»¥è¶…è¶Šå…¶å¤§å‹åŒç±»æ¨¡å‹ï¼Œä¸ºè®¡ç®—ç¥ç»ç§‘å­¦çš„ç ”ç©¶å¼€è¾Ÿæ›´åŠ ä¾¿æ·å’Œå¼ºå¤§çš„é“è·¯ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.18314v1">PDF</a> 21 pages, 8 figures</p>
<p><strong>Summary</strong></p>
<p>BrainSymphonyæ˜¯ä¸€ä¸ªè½»é‡çº§ã€å‚æ•°é«˜æ•ˆçš„ç¥ç»å½±åƒåŸºç¡€æ¨¡å‹ï¼Œå¯åœ¨è¾ƒå°çš„å…¬å…±æ•°æ®é›†ä¸Šè¿›è¡Œé¢„è®­ç»ƒï¼Œå®ç°æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚å®ƒé€šè¿‡å¹¶è¡Œç©ºé—´å’Œæ—¶é—´è½¬æ¢å™¨æµå¤„ç†åŠŸèƒ½ç£å…±æŒ¯æˆåƒæ•°æ®ï¼Œå¹¶ç”±æ„ŸçŸ¥å™¨æ¨¡å—æœ‰æ•ˆåœ°è’¸é¦æˆç»Ÿä¸€è¡¨ç¤ºã€‚åŒæ—¶ï¼Œå®ƒé€šè¿‡ç­¾åçš„å›¾å˜æ¢å™¨å¯¹æ‰©æ•£ç£å…±æŒ¯æˆåƒçš„ç»“æ„è¿é€šæ€§è¿›è¡Œå»ºæ¨¡ï¼Œä»¥ç¼–ç å¤§è„‘çš„è§£å‰–ç»“æ„ã€‚è¿™äº›å¼ºå¤§è€Œç‰¹å®šçš„è¡¨ç¤ºå½¢å¼é€šè¿‡è‡ªé€‚åº”èåˆé—¨è¿›è¡Œé›†æˆã€‚å°½ç®¡è®¾è®¡ç´§å‡‘ï¼Œè¯¥æ¨¡å‹åœ¨åŒ…æ‹¬åˆ†ç±»ã€é¢„æµ‹å’Œæ— ç›‘ç£ç½‘ç»œè¯†åˆ«ä»»åŠ¡åœ¨å†…çš„å„ç§ä¸‹æ¸¸åŸºå‡†æµ‹è¯•ä¸­å§‹ç»ˆè¡¨ç°å‡ºè¶…è¶Šå¤§å‹æ¨¡å‹çš„æ€§èƒ½ã€‚æ­¤å¤–ï¼Œè¯¥æ¨¡å‹åˆ©ç”¨ç‹¬ç‰¹çš„å¤–éƒ¨è£¸ç›–é…°äºŒèƒºé…¸ç¥ç»æˆåƒæ•°æ®é›†ä¸Šçš„æ³¨æ„åŠ›åœ°å›¾æ­ç¤ºäº†å¤§è„‘åŠ¨æ€çš„å…¨æ–°è§è§£ï¼Œä¸ºè®¡ç®—ç¥ç»ç§‘å­¦çš„ç ”ç©¶å¼€è¾Ÿäº†æ›´åŠ ä¾¿æ·å’Œå¼ºå¤§çš„é“è·¯ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>BrainSymphonyæ˜¯ä¸€ä¸ªè½»é‡çº§ã€å‚æ•°é«˜æ•ˆçš„åŸºç¡€æ¨¡å‹ï¼Œé€‚ç”¨äºç¥ç»å½±åƒç ”ç©¶ã€‚</li>
<li>å®ƒé€šè¿‡å¹¶è¡Œç©ºé—´å’Œæ—¶é—´è½¬æ¢å™¨å¤„ç†åŠŸèƒ½ç£å…±æŒ¯æˆåƒæ•°æ®ï¼Œå®ç°é«˜æ•ˆçš„æ•°æ®è¡¨ç¤ºã€‚</li>
<li>æ¨¡å‹é‡‡ç”¨æ„ŸçŸ¥å™¨æ¨¡å—å°†ä¸åŒè¡¨ç¤ºå½¢å¼ç»Ÿä¸€èµ·æ¥ã€‚</li>
<li>é€šè¿‡ç­¾åçš„å›¾å˜æ¢å™¨å¯¹æ‰©æ•£ç£å…±æŒ¯æˆåƒçš„ç»“æ„è¿é€šæ€§è¿›è¡Œå»ºæ¨¡ã€‚</li>
<li>è¯¥æ¨¡å‹é€šè¿‡è‡ªé€‚åº”èåˆé—¨é›†æˆå¤šç§è¡¨ç¤ºå½¢å¼ã€‚</li>
<li>åœ¨å¤šç§ä¸‹æ¸¸ä»»åŠ¡ä¸­è¡¨ç°ä¼˜ç§€ï¼ŒåŒ…æ‹¬åˆ†ç±»ã€é¢„æµ‹å’Œæ— ç›‘ç£ç½‘ç»œè¯†åˆ«ç­‰ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.18314">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-153f2a7ead6f0496dab583fcb0daad08.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-5594ef25df31995edf607f4ba9ba87d3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2832bd75553a40d38e2cdca63690818e.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="NIKA2-Cosmological-Legacy-Survey-Blind-detection-of-galaxy-clusters-in-the-COSMOS-field-via-the-Sunyaev-Zelâ€™dovich-effect"><a href="#NIKA2-Cosmological-Legacy-Survey-Blind-detection-of-galaxy-clusters-in-the-COSMOS-field-via-the-Sunyaev-Zelâ€™dovich-effect" class="headerlink" title="NIKA2 Cosmological Legacy Survey: Blind detection of galaxy clusters in   the COSMOS field via the Sunyaev-Zelâ€™dovich effect"></a>NIKA2 Cosmological Legacy Survey: Blind detection of galaxy clusters in   the COSMOS field via the Sunyaev-Zelâ€™dovich effect</h2><p><strong>Authors:D. ChÃ©rouvrier, J. F. Macias-Perez, F. X. DÃ©sert, R. Adam, P. Ade, H. Ajeddig, S. Amarantidis, P. AndrÃ©, H. Aussel, R. Barrena, A. Beelen, A. Benoit, S. Berta, M. BÃ©thermin, A. Bongiovanni, J. Bounmy, O. Bourrion, L. -J. Bing, M. Calvo, A. Catalano, M. De Petris, S. Doyle, E. F. C. Driessen, G. Ejlali, A. Ferragamo, M. Fernandez-Torreiro, A. Gomez, J. Goupy, C. Hanser, S. Katsioli, F. KÃ©ruzorÃ©, C. Kramer, B. Ladjelate, G. Lagache, S. Leclercq, J. -F. Lestrade, S. C. Madden, A. Maury, F. Mayet, J. -B. Melin, A. Monfardini, A. Moyer-Anin, M. Mu noz-Echeverria, I. Myserlis, R. Neri, A. Paliwal, L. Perotto, G. Pisano, E. Pointecouteau, N. Ponthieu, G. W. Pratt, V. Reveret, A. J. Rigby, A. Ritacco, H. Roussel, F. Ruppin, M. Sanchez-Portal, S. Savorgnano, K. Schuster, A. Sievers, C. Tucker, R. Zylka</strong></p>
<p>(Abridged) Clusters of galaxies, formed in the latest stages of structure formation, are unique cosmological probes. With the advent of large CMB surveys like those from the Planck satellite, the ACT and SPT telescopes, we now have access to a large number of galaxy clusters detected at millimeter wavelengths via the thermal Sunyaev-Zelâ€™dovich (tSZ) effect. Nevertheless, it is interesting to complement them with high-angular-resolution (tens of arcseconds) observations to target the lowest-mass and highest-redshift clusters. This is the case of observations with the NIKA2 camera, which is installed on the IRAM 30â€“m telescope in Pico Veleta, Spain. We used the existing 150 GHz (2 mm) data from the NIKA2 Cosmological Legacy Survey (N2CLS) Large Program to blindly search for galaxy clusters in the well-known COSMOS field, across a 877 arcmin$^2$ region centered on (R.A., Dec.)$_{J2000}$ &#x3D; (10h00m28.81s, +02d17m30.44s). We first developed a dedicated data reduction pipeline to construct NIKA2 maps at 2 mm. We then used a matched-filter algorithm to extract cluster candidates assuming a universal pressure profile to model the expected cluster tSZ signal. We computed the purity and completeness of the sample by applying the previous algorithm to simulated maps of the sky signal in the COSMOS field. We find a total of 16 cluster candidates at S&#x2F;N &gt; 4, from which eight have either an optical or X-ray cluster (or group of galaxies) counterpart. This is the first blind detection of clusters of galaxies at mm wavelengths at 18â€ angular resolution. From this analysis, we confirm that NIKA2 and the IRAM 30â€“m telescope should be sensitive to low-mass clusters at intermediate and high redshift, complementing current and planned large tSZ-based cluster surveys. </p>
<blockquote>
<p>æ˜Ÿç³»å›¢æ˜¯åœ¨ç»“æ„å½¢æˆåæœŸå½¢æˆçš„ç‹¬ç‰¹å®‡å®™å­¦æ¢é’ˆã€‚éšç€åƒæ™®æœ—å…‹å«æ˜Ÿã€ACTå’ŒSPTæœ›è¿œé•œç­‰å¤§å‹å®‡å®™å¾®æ³¢èƒŒæ™¯è¾å°„è°ƒæŸ¥çš„å‡ºç°ï¼Œæˆ‘ä»¬ç°åœ¨å¯ä»¥é€šè¿‡çƒ­Sunyaev-Zelâ€™dovichï¼ˆtSZï¼‰æ•ˆåº”åœ¨æ¯«ç±³æ³¢èŒƒå›´å†…æ£€æµ‹åˆ°å¤§é‡æ˜Ÿç³»å›¢ã€‚ç„¶è€Œï¼Œä¸ºäº†ç„å‡†è´¨é‡æœ€ä½ã€çº¢ç§»æœ€é«˜çš„æ˜Ÿç³»å›¢ï¼Œå¯¹å…¶è¿›è¡Œé«˜åˆ†è¾¨ç‡ï¼ˆå‡ åè§’ç§’ï¼‰çš„è§‚æµ‹æ˜¯æœ‰æ„ä¹‰çš„ã€‚è¿™æ˜¯ä½¿ç”¨å®‰è£…åœ¨è¥¿ç­ç‰™çš®ç§‘éŸ¦è±å¡”IRAM 30ç±³æœ›è¿œé•œä¸Šçš„NIKA2ç›¸æœºè¿›è¡Œçš„è§‚æµ‹çš„æƒ…å†µã€‚æˆ‘ä»¬åœ¨è‘—åçš„å®‡å®™å­¦å¤§å°ºåº¦ç»“æ„è§‚æµ‹åœºï¼ˆCOSMOSåœºï¼‰å†…ï¼Œä»¥ï¼ˆèµ¤ç»ï¼Œèµ¤çº¬ï¼‰J2000&#x3D;ï¼ˆ10æ—¶0åˆ†28.81ç§’ï¼Œ+ 2åº¦17åˆ†30.44ç§’ï¼‰ä¸ºä¸­å¿ƒï¼Œå¯¹é¢ç§¯çº¦ä¸º877å¹³æ–¹è§’åˆ†çš„åŒºåŸŸè¿›è¡Œäº†NIKA2å®‡å®™å­¦é—äº§è°ƒæŸ¥ï¼ˆN2CLSï¼‰å¤§å‹é¡¹ç›®çš„ç°æœ‰150 GHzï¼ˆ2æ¯«ç±³ï¼‰æ•°æ®ï¼Œä»¥ç›²æœæ–¹å¼å¯»æ‰¾æ˜Ÿç³»å›¢ã€‚æˆ‘ä»¬é¦–å…ˆå¼€å‘äº†ä¸€ä¸ªä¸“é—¨çš„æ•°æ®ç¼©å‡ç®¡é“æ¥æ„å»ºNIKA2çš„2æ¯«ç±³åœ°å›¾ã€‚ç„¶åï¼Œæˆ‘ä»¬ä½¿ç”¨åŒ¹é…æ»¤æ³¢å™¨ç®—æ³•ï¼Œå‡è®¾æ™®éçš„å‹åŠ›åˆ†å¸ƒæ¨¡å‹é¢„æœŸçš„æ˜Ÿç³»å›¢çš„tSZä¿¡å·ï¼Œæ¥æå–é›†ç¾¤å€™é€‰è€…ã€‚æˆ‘ä»¬é€šè¿‡å°†ä¸Šè¿°ç®—æ³•åº”ç”¨äºæ¨¡æ‹Ÿçš„å¤©å›¾ä¿¡å·åœ°å›¾æ¥è®¡ç®—æœºæ ·æœ¬çš„çº¯åº¦å’Œå®Œæ•´æ€§ã€‚æˆ‘ä»¬å‘ç°å…±æœ‰ä¿¡å™ªæ¯”å¤§äº4çš„16ä¸ªé›†ç¾¤å€™é€‰è€…ï¼Œå…¶ä¸­8ä¸ªæœ‰å…‰å­¦æˆ–Xå°„çº¿é›†ç¾¤ï¼ˆæˆ–æ˜Ÿç³»å›¢ï¼‰å¯¹åº”ä½“ã€‚è¿™æ˜¯åœ¨æ¯«ç±³æ³¢æ³¢é•¿ä¸‹é¦–æ¬¡ä»¥18è§’ç§’åˆ†è¾¨ç‡è¿›è¡Œç›²æ¢æ¢æµ‹åˆ°çš„æ˜Ÿç³»å›¢ã€‚ä»è¿™é¡¹åˆ†æä¸­ï¼Œæˆ‘ä»¬è¯å®NIKA2å’ŒIRAM 30ç±³æœ›è¿œé•œå¯¹ä¸­é—´å’Œçº¢ç§»çš„ä½è´¨é‡é›†ç¾¤æ•æ„Ÿï¼Œå¯è¡¥å……å½“å‰å’Œè®¡åˆ’ä¸­çš„å¤§å‹åŸºäºtSZçš„é›†ç¾¤æ™®æŸ¥ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.18231v1">PDF</a> </p>
<p><strong>Summary</strong><br>     ä½¿ç”¨æ¯«ç±³æ³¢è§‚æµ‹æŠ€æœ¯ï¼Œé€šè¿‡NIKA2ç›¸æœºåœ¨IRAM 30ç±³æœ›è¿œé•œå¯¹å®‡å®™æ˜Ÿç³»å›¢è¿›è¡Œç›²æ¢æµ‹ã€‚ç ”ç©¶å›¢é˜Ÿå¯¹å®‡å®™ä¸­çš„è‘—ååŒºåŸŸCOSMOSè¿›è¡Œäº†è§‚å¯Ÿï¼Œå¹¶å‘ç°äº†ä½è´¨é‡ã€é«˜çº¢ç§»çš„æ˜Ÿç³»å›¢å€™é€‰è€…ã€‚è¿™æ˜¯é¦–æ¬¡åœ¨æ¯«ç±³æ³¢æ³¢é•¿ä¸‹ä»¥é«˜è§’åº¦åˆ†è¾¨ç‡è¿›è¡Œæ˜Ÿç³»å›¢ç›²æ¢æµ‹ã€‚ç ”ç©¶è¯å®ï¼ŒNIKA2ä¸IRAM 30ç±³æœ›è¿œé•œèƒ½å¤Ÿè¡¥å……å½“å‰åŠè®¡åˆ’çš„å¤§è§„æ¨¡çƒ­Sunyaev-Zelâ€™dovichæ•ˆåº”ä¸ºåŸºç¡€çš„æ˜Ÿç³»å›¢è°ƒæŸ¥ï¼Œå¯¹ä¸­é—´å’Œçº¢ç§»çš„ä½è´¨é‡æ˜Ÿç³»å›¢æ•æ„Ÿã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ˜Ÿç³»å›¢æ˜¯å®‡å®™ç»“æ„å½¢æˆçš„æ™šæœŸé˜¶æ®µçš„ç‹¬ç‰¹å®‡å®™æ¢é’ˆã€‚</li>
<li>å¤§å‹CMBè°ƒæŸ¥å¦‚Planckå«æ˜Ÿã€ACTå’ŒSPTæœ›è¿œé•œæä¾›äº†å¤§é‡é€šè¿‡çƒ­Sunyaev-Zelâ€™dovichæ•ˆåº”åœ¨æ¯«ç±³æ³¢æ³¢é•¿ä¸‹æ£€æµ‹çš„æ˜Ÿç³»å›¢æ•°æ®ã€‚</li>
<li>ä½¿ç”¨IRAM 30ç±³æœ›è¿œé•œä¸Šçš„NIKA2ç›¸æœºè¿›è¡Œé«˜è§’åº¦åˆ†è¾¨ç‡ï¼ˆå‡ åè§’ç§’ï¼‰è§‚æµ‹å¯ä»¥é’ˆå¯¹ä½è´¨é‡å’Œé«˜çº¢ç§»çš„æ˜Ÿç³»å›¢ã€‚</li>
<li>åœ¨è‘—åçš„COSMOSåŒºåŸŸè¿›è¡Œç›²æ¢æµ‹å‘ç°äº†ä½è´¨é‡ã€é«˜çº¢ç§»çš„æ˜Ÿç³»å›¢å€™é€‰è€…ã€‚</li>
<li>è¿™æ˜¯é¦–æ¬¡åœ¨æ¯«ç±³æ³¢æ³¢é•¿ä¸‹è¿›è¡Œçš„é«˜åˆ†è¾¨ç‡æ˜Ÿç³»å›¢ç›²æ¢æµ‹ã€‚</li>
<li>NIKA2å’ŒIRAM 30ç±³æœ›è¿œé•œèƒ½å¤Ÿè¡¥å……å½“å‰å’Œè®¡åˆ’çš„å¤§è§„æ¨¡çƒ­Sunyaev-Zelâ€™dovichæ•ˆåº”ä¸ºåŸºç¡€çš„æ˜Ÿç³»å›¢è°ƒæŸ¥ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.18231">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-48d3b9dc1f3b2c37f18f61d8e68ade6a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-123d346a267f80f952bac763888cae1d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6e70621b500a12953f3c098d69245b44.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f1374e5b9fd84361d96d1c46df4a154c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7f83f86468e3d70ce585e1ac13c40e10.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-fc2e08c5618f0a9f467b8ea22e8fd92b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7ef128e9b5bebc11abd1dd529934f9a2.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="Multimodal-Medical-Image-Binding-via-Shared-Text-Embeddings"><a href="#Multimodal-Medical-Image-Binding-via-Shared-Text-Embeddings" class="headerlink" title="Multimodal Medical Image Binding via Shared Text Embeddings"></a>Multimodal Medical Image Binding via Shared Text Embeddings</h2><p><strong>Authors:Yunhao Liu, Suyang Xi, Shiqi Liu, Hong Ding, Chicheng Jin, Chenxi Yang, Junjun He, Yiqing Shen</strong></p>
<p>Medical image analysis increasingly relies on the integration of multiple imaging modalities to capture complementary anatomical and functional information, enabling more accurate diagnosis and treatment planning. Achieving aligned feature representations across these diverse modalities is therefore important for effective multimodal analysis. While contrastive language-image pre-training (CLIP) and its variant have enabled image-text alignments, they require explicitly paired data between arbitrary two modalities, which is difficult to acquire in medical contexts. To address the gap, we present Multimodal Medical Image Binding with Text (M\textsuperscript{3}Bind), a novel pre-training framework that enables seamless alignment of multiple medical imaging modalities through a shared text representation space without requiring explicit paired data between any two medical image modalities. Specifically, based on the insight that different images can naturally bind with text, M\textsuperscript{3}Bind first fine-tunes pre-trained CLIP-like image-text models to align their modality-specific text embedding space while preserving their original image-text alignments. Subsequently, we distill these modality-specific text encoders into a unified model, creating a shared text embedding space. Experiments on X-ray, CT, retina, ECG, and pathological images on multiple downstream tasks demonstrate that M\textsuperscript{3}Bind achieves state-of-the-art performance in zero-shot, few-shot classification and cross-modal retrieval tasks compared to its CLIP-like counterparts. These results validate M\textsuperscript{3}Bindâ€™s effectiveness in achieving cross-image-modal alignment for medical analysis. </p>
<blockquote>
<p>åŒ»å­¦å›¾åƒåˆ†æè¶Šæ¥è¶Šä¾èµ–äºå¤šç§æˆåƒæ–¹å¼çš„èåˆï¼Œä»¥æ•æ‰äº’è¡¥çš„è§£å‰–å’ŒåŠŸèƒ½æ€§ä¿¡æ¯ï¼Œä»è€Œå®ç°æ›´å‡†ç¡®çš„è¯Šæ–­å’Œåˆ¶å®šæ²»ç–—æ–¹æ¡ˆã€‚å› æ­¤ï¼Œåœ¨è¿™äº›ä¸åŒçš„æ–¹å¼ä¸­å®ç°å¯¹é½çš„ç‰¹å¾è¡¨ç¤ºå¯¹äºæœ‰æ•ˆçš„å¤šæ¨¡å¼åˆ†æè‡³å…³é‡è¦ã€‚å°½ç®¡å¯¹æ¯”è¯­è¨€å›¾åƒé¢„è®­ç»ƒï¼ˆCLIPï¼‰åŠå…¶å˜ä½“å®ç°äº†å›¾åƒæ–‡æœ¬çš„å¯¹é½ï¼Œä½†å®ƒä»¬éœ€è¦åœ¨ä»»æ„ä¸¤ç§æ¨¡å¼ä¹‹é—´æ˜ç¡®é…å¯¹çš„æ•°æ®ï¼Œè¿™åœ¨åŒ»å­¦èƒŒæ™¯ä¸‹å¾ˆéš¾è·å–ã€‚ä¸ºäº†å¼¥è¡¥è¿™ä¸€å·®è·ï¼Œæˆ‘ä»¬æå‡ºäº†åŸºäºæ–‡æœ¬çš„å¤šæ¨¡å¼åŒ»å­¦å›¾åƒç»‘å®šï¼ˆM\textsuperscript{3}Bindï¼‰ï¼Œè¿™æ˜¯ä¸€ç§æ–°çš„é¢„è®­ç»ƒæ¡†æ¶ï¼Œå®ƒå¯ä»¥é€šè¿‡å…±äº«æ–‡æœ¬è¡¨ç¤ºç©ºé—´å®ç°å¤šç§åŒ»å­¦æˆåƒæ–¹å¼çš„æ— ç¼å¯¹é½ï¼Œè€Œæ— éœ€åœ¨ä»»æ„ä¸¤ç§åŒ»å­¦å›¾åƒæ–¹å¼ä¹‹é—´æ˜ç¡®é…å¯¹æ•°æ®ã€‚å…·ä½“æ¥è¯´ï¼ŒåŸºäºä¸åŒå›¾åƒå¯ä»¥è‡ªç„¶ç»‘å®šåˆ°æ–‡æœ¬ä¸Šçš„è§è§£ï¼ŒM\textsuperscript{3}Bindé¦–å…ˆå¾®è°ƒé¢„è®­ç»ƒçš„CLIPç±»å›¾åƒæ–‡æœ¬æ¨¡å‹ï¼Œä»¥å¯¹é½å…¶ç‰¹å®šäºæ–¹å¼çš„æ–‡æœ¬åµŒå…¥ç©ºé—´ï¼ŒåŒæ—¶ä¿ç•™å…¶åŸå§‹çš„å›¾åƒæ–‡æœ¬å¯¹é½ã€‚éšåï¼Œæˆ‘ä»¬å°†è¿™äº›ç‰¹å®šäºæ–¹å¼çš„æ–‡æœ¬ç¼–ç å™¨è’¸é¦åˆ°ç»Ÿä¸€æ¨¡å‹ä¸­ï¼Œåˆ›å»ºä¸€ä¸ªå…±äº«çš„æ–‡æœ¬åµŒå…¥ç©ºé—´ã€‚åœ¨Xå…‰ã€CTã€è§†ç½‘è†œã€å¿ƒç”µå›¾å’Œç—…ç†å›¾åƒç­‰å¤šç§ä¸‹æ¸¸ä»»åŠ¡ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œä¸CLIPç±»æ¨¡å‹ç›¸æ¯”ï¼ŒM\textsuperscript{3}Bindåœ¨é›¶æ ·æœ¬ã€å°‘æ ·æœ¬åˆ†ç±»å’Œè·¨æ¨¡å¼æ£€ç´¢ä»»åŠ¡ä¸Šå®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚è¿™äº›ç»“æœéªŒè¯äº†M\textsuperscript{3}Bindåœ¨å®ç°åŒ»å­¦åˆ†æä¸­çš„è·¨å›¾åƒæ¨¡å¼å¯¹é½æ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.18072v1">PDF</a> 10 pages, 3 figures</p>
<p><strong>Summary</strong><br>     åŒ»å­¦å›¾åƒåˆ†æè¶Šæ¥è¶Šä¾èµ–äºå¤šç§æˆåƒæ¨¡å¼çš„æ•´åˆï¼Œä»¥æ•æ‰äº’è¡¥çš„è§£å‰–å’ŒåŠŸèƒ½æ€§ä¿¡æ¯ï¼Œä»è€Œå®ç°æ›´å‡†ç¡®çš„è¯Šæ–­å’Œåˆ¶å®šæ²»ç–—æ–¹æ¡ˆã€‚å› æ­¤ï¼Œå®ç°è¿™äº›ä¸åŒæ¨¡å¼ä¹‹é—´çš„å¯¹é½ç‰¹å¾è¡¨ç¤ºå¯¹äºæœ‰æ•ˆçš„å¤šæ¨¡å¼åˆ†æè‡³å…³é‡è¦ã€‚æˆ‘ä»¬æå‡ºäº†å¤šæ¨¡å¼åŒ»å­¦å›¾åƒç»‘å®šæ–‡æœ¬ï¼ˆM\textsuperscript{3}Bindï¼‰é¢„è®­ç»ƒæ¡†æ¶ï¼Œé€šè¿‡å…±äº«æ–‡æœ¬è¡¨ç¤ºç©ºé—´ï¼Œæ— éœ€ä»»æ„ä¸¤ç§åŒ»å­¦å›¾åƒæ¨¡å¼ä¹‹é—´çš„æ˜ç¡®é…å¯¹æ•°æ®ï¼Œå³å¯å®ç°æ— ç¼å¯¹é½å¤šä¸ªåŒ»å­¦æˆåƒæ¨¡å¼ã€‚å®éªŒè¡¨æ˜ï¼ŒM\textsuperscript{3}Bindåœ¨é›¶æ ·æœ¬ã€å°‘æ ·æœ¬åˆ†ç±»å’Œè·¨æ¨¡æ€æ£€ç´¢ä»»åŠ¡ä¸Šå®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼ŒéªŒè¯äº†å…¶åœ¨åŒ»å­¦åˆ†æä¸­çš„è·¨å›¾åƒæ¨¡æ€å¯¹é½çš„æœ‰æ•ˆæ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>åŒ»å­¦å›¾åƒåˆ†æä¾èµ–äºå¤šæ¨¡æ€æˆåƒæŠ€æœ¯çš„ç»“åˆï¼Œä»¥è·å–æ›´å…¨é¢çš„è¯Šæ–­ä¿¡æ¯ã€‚</li>
<li>å®ç°ä¸åŒåŒ»å­¦æˆåƒæ¨¡æ€ä¹‹é—´çš„å¯¹é½ç‰¹å¾è¡¨ç¤ºå¯¹äºå¤šæ¨¡æ€åˆ†æè‡³å…³é‡è¦ã€‚</li>
<li>M\textsuperscript{3}Bindæ˜¯ä¸€ç§æ–°çš„é¢„è®­ç»ƒæ¡†æ¶ï¼Œèƒ½åœ¨æ— éœ€æ˜ç¡®é…å¯¹æ•°æ®çš„æƒ…å†µä¸‹ï¼Œå®ç°å¤šç§åŒ»å­¦æˆåƒæ¨¡æ€ä¸æ–‡æœ¬çš„æ— ç¼å¯¹é½ã€‚</li>
<li>M\textsuperscript{3}BindåŸºäºä¸åŒå›¾åƒè‡ªç„¶ç»‘å®šæ–‡æœ¬çš„ç†å¿µã€‚</li>
<li>M\textsuperscript{3}Bindé¦–å…ˆå¾®è°ƒé¢„è®­ç»ƒçš„CLIPç±»å›¾åƒ-æ–‡æœ¬æ¨¡å‹ï¼Œä»¥å¯¹é½æ¨¡æ€ç‰¹å®šçš„æ–‡æœ¬åµŒå…¥ç©ºé—´ï¼ŒåŒæ—¶ä¿ç•™å…¶åŸå§‹çš„å›¾åƒ-æ–‡æœ¬å¯¹é½ã€‚</li>
<li>M\textsuperscript{3}Bindåœ¨å¤šç§ä¸‹æ¸¸ä»»åŠ¡ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼Œå…¶åœ¨é›¶æ ·æœ¬ã€å°‘æ ·æœ¬åˆ†ç±»å’Œè·¨æ¨¡æ€æ£€ç´¢ä»»åŠ¡ä¸Šå®ç°äº†å“è¶Šæ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.18072">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-16f3e023d8f0abf679272da7ff92e7c3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-33e7d323a8a0c4c759a22550bb7efc4f.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-83009d26ceae5f38c7c51a11dbd98686.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="Training-free-Test-time-Improvement-for-Explainable-Medical-Image-Classification"><a href="#Training-free-Test-time-Improvement-for-Explainable-Medical-Image-Classification" class="headerlink" title="Training-free Test-time Improvement for Explainable Medical Image   Classification"></a>Training-free Test-time Improvement for Explainable Medical Image   Classification</h2><p><strong>Authors:Hangzhou He, Jiachen Tang, Lei Zhu, Kaiwen Li, Yanye Lu</strong></p>
<p>Deep learning-based medical image classification techniques are rapidly advancing in medical image analysis, making it crucial to develop accurate and trustworthy models that can be efficiently deployed across diverse clinical scenarios. Concept Bottleneck Models (CBMs), which first predict a set of explainable concepts from images and then perform classification based on these concepts, are increasingly being adopted for explainable medical image classification. However, the inherent explainability of CBMs introduces new challenges when deploying trained models to new environments. Variations in imaging protocols and staining methods may induce concept-level shifts, such as alterations in color distribution and scale. Furthermore, since CBM training requires explicit concept annotations, fine-tuning models solely with image-level labels could compromise concept prediction accuracy and faithfulness - a critical limitation given the high cost of acquiring expert-annotated concept labels in medical domains. To address these challenges, we propose a training-free confusion concept identification strategy. By leveraging minimal new data (e.g., 4 images per class) with only image-level labels, our approach enhances out-of-domain performance without sacrificing source domain accuracy through two key operations: masking misactivated confounding concepts and amplifying under-activated discriminative concepts. The efficacy of our method is validated on both skin and white blood cell images. Our code is available at: <a target="_blank" rel="noopener" href="https://github.com/riverback/TF-TTI-XMed">https://github.com/riverback/TF-TTI-XMed</a>. </p>
<blockquote>
<p>åŸºäºæ·±åº¦å­¦ä¹ çš„åŒ»å­¦å›¾åƒåˆ†ç±»æŠ€æœ¯åœ¨åŒ»å­¦å›¾åƒåˆ†æé¢†åŸŸæ­£å¿«é€Ÿå‘å±•ï¼Œå› æ­¤éœ€è¦å¼€å‘èƒ½å¤Ÿé«˜æ•ˆéƒ¨ç½²åœ¨ä¸åŒä¸´åºŠåœºæ™¯ä¸­çš„å‡†ç¡®ä¸”å¯é çš„æ¨¡å‹ã€‚æ¦‚å¿µç“¶é¢ˆæ¨¡å‹ï¼ˆCBMsï¼‰é¦–å…ˆé€šè¿‡å›¾åƒé¢„æµ‹ä¸€ç»„å¯è§£é‡Šçš„æ¦‚å¿µï¼Œç„¶ååŸºäºè¿™äº›æ¦‚å¿µè¿›è¡Œåˆ†ç±»ï¼Œè¶Šæ¥è¶Šè¢«å¹¿æ³›åœ°åº”ç”¨äºå¯è§£é‡Šçš„åŒ»å­¦å›¾åƒåˆ†ç±»ã€‚ç„¶è€Œï¼ŒCBMsçš„å†…åœ¨å¯è§£é‡Šæ€§åœ¨å‘æ–°ç¯å¢ƒéƒ¨ç½²è®­ç»ƒå¥½çš„æ¨¡å‹æ—¶å¸¦æ¥äº†æ–°çš„æŒ‘æˆ˜ã€‚æˆåƒåè®®å’ŒæŸ“è‰²æ–¹æ³•çš„å·®å¼‚å¯èƒ½å¯¼è‡´æ¦‚å¿µå±‚é¢çš„å˜åŒ–ï¼Œå¦‚é¢œè‰²åˆ†å¸ƒå’Œå°ºåº¦çš„å˜åŒ–ã€‚æ­¤å¤–ï¼Œç”±äºCBMè®­ç»ƒéœ€è¦æ˜ç¡®çš„æ¦‚å¿µæ³¨é‡Šï¼Œä»…ä½¿ç”¨å›¾åƒçº§åˆ«çš„æ ‡ç­¾å¯¹æ¨¡å‹è¿›è¡Œå¾®è°ƒå¯èƒ½ä¼šæŸå®³æ¦‚å¿µé¢„æµ‹å‡†ç¡®æ€§å’Œå¿ å®åº¦â€”â€”è€ƒè™‘åˆ°åœ¨åŒ»å­¦é¢†åŸŸè·å–ä¸“å®¶æ³¨é‡Šçš„æ¦‚å¿µæ ‡ç­¾æˆæœ¬é«˜æ˜‚ï¼Œè¿™æ˜¯ä¸€ä¸ªå…³é”®çš„å±€é™æ€§ã€‚ä¸ºäº†åº”å¯¹è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ— éœ€è®­ç»ƒçš„æ··æ·†æ¦‚å¿µè¯†åˆ«ç­–ç•¥ã€‚é€šè¿‡åˆ©ç”¨å°‘é‡æ–°æ•°æ®ï¼ˆä¾‹å¦‚ï¼Œæ¯ç±»4å¼ å›¾åƒï¼‰å’Œä»…å¸¦æœ‰å›¾åƒçº§åˆ«æ ‡ç­¾çš„æ•°æ®ï¼Œæˆ‘ä»¬çš„æ–¹æ³•é€šè¿‡ä¸¤ä¸ªå…³é”®æ“ä½œâ€”â€”æ©ç›–é”™è¯¯æ¿€æ´»çš„æ··æ·†æ¦‚å¿µå’Œæ”¾å¤§æœªæ¿€æ´»çš„åˆ¤åˆ«æ¦‚å¿µâ€”â€”æé«˜äº†åŸŸå¤–æ€§èƒ½ï¼ŒåŒæ—¶ä¸ç‰ºç‰²æºåŸŸå‡†ç¡®æ€§ã€‚æˆ‘ä»¬çš„æ–¹æ³•åœ¨çš®è‚¤å’Œç™½ç»†èƒå›¾åƒä¸Šçš„æœ‰æ•ˆæ€§å·²ç»å¾—åˆ°éªŒè¯ã€‚æˆ‘ä»¬çš„ä»£ç å¯ç”¨åœ¨ï¼š<a target="_blank" rel="noopener" href="https://github.com/riverback/TF-TTI-XMed%E3%80%82">https://github.com/riverback/TF-TTI-XMedã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.18070v1">PDF</a> This is the initial version of our work accepted by MICCAI 2025.   Weâ€™ll include a link to the version on SpringerLink after this becomes   available</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†åŸºäºæ·±åº¦å­¦ä¹ çš„åŒ»å­¦å›¾åƒåˆ†ç±»æŠ€æœ¯çš„å¿«é€Ÿå‘å±•åŠå…¶åœ¨åŒ»å­¦å›¾åƒåˆ†æä¸­çš„åº”ç”¨ã€‚æ–‡ç« é‡ç‚¹ä»‹ç»äº†æ¦‚å¿µç“¶é¢ˆæ¨¡å‹ï¼ˆCBMsï¼‰åœ¨å¯è§£é‡Šçš„åŒ»å­¦å›¾åƒåˆ†ç±»ä¸­çš„ä½¿ç”¨åŠå…¶é¢ä¸´çš„æŒ‘æˆ˜ã€‚æ–‡ç« æå‡ºä¸€ç§æ— è®­ç»ƒæ··æ·†æ¦‚å¿µè¯†åˆ«ç­–ç•¥ï¼Œé€šè¿‡åˆ©ç”¨å°‘é‡æ–°æ•°æ®ï¼ˆå¦‚æ¯ç±»4å¼ å›¾åƒï¼‰å’Œä»…å›¾åƒçº§åˆ«çš„æ ‡ç­¾ï¼Œå¢å¼ºæ¨¡å‹åœ¨æ–°ç¯å¢ƒä¸‹çš„æ€§èƒ½ï¼ŒåŒæ—¶ä¸ç‰ºç‰²æºåŸŸå‡†ç¡®æ€§ã€‚è¯¥æ–¹æ³•é€šè¿‡æ©ç›–è¯¯æ¿€æ´»çš„æ··æ·†æ¦‚å¿µå’Œæ”¾å¤§æœªæ¿€æ´»çš„åˆ¤åˆ«æ¦‚å¿µæ¥å®ç°æœ‰æ•ˆæ€§ï¼Œå¹¶åœ¨çš®è‚¤å’Œç™½ç»†èƒå›¾åƒä¸Šå¾—åˆ°éªŒè¯ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ·±åº¦å­¦ä¹ æ–¹æ³•åœ¨åŒ»å­¦å›¾åƒåˆ†ç±»ä¸­çš„å¿«é€Ÿå‘å±•ï¼Œéœ€è¦å¼€å‘å‡†ç¡®ã€å¯é çš„æ¨¡å‹ä»¥é€‚åº”ä¸åŒçš„ä¸´åºŠåœºæ™¯ã€‚</li>
<li>æ¦‚å¿µç“¶é¢ˆæ¨¡å‹ï¼ˆCBMsï¼‰æ˜¯ä¸€ç§ç”¨äºè§£é‡ŠåŒ»å­¦å›¾åƒåˆ†ç±»çš„æ–°å…´æŠ€æœ¯ï¼Œå®ƒé€šè¿‡é¢„æµ‹å›¾åƒä¸­çš„ä¸€ç³»åˆ—å¯è§£é‡Šæ¦‚å¿µæ¥è¿›è¡Œåˆ†ç±»ã€‚</li>
<li>CBMsåœ¨éƒ¨ç½²åˆ°æ–°çš„ç¯å¢ƒæ—¶é¢ä¸´æŒ‘æˆ˜ï¼Œå¦‚æˆåƒåè®®å’ŒæŸ“è‰²æ–¹æ³•çš„å·®å¼‚å¯èƒ½å¯¼è‡´æ¦‚å¿µå±‚é¢çš„å˜åŒ–ã€‚</li>
<li>CBMè®­ç»ƒéœ€è¦æ˜ç¡®çš„æ¦‚å¿µæ³¨é‡Šï¼Œä»…ä½¿ç”¨å›¾åƒçº§åˆ«çš„æ ‡ç­¾å¯¹æ¨¡å‹è¿›è¡Œå¾®è°ƒå¯èƒ½ä¼šé™ä½æ¦‚å¿µé¢„æµ‹çš„å‡†ç¡®æ€§ï¼Œè¿™æ˜¯ä¸€ä¸ªå…³é”®çš„å±€é™æ€§ã€‚</li>
<li>é’ˆå¯¹ä»¥ä¸ŠæŒ‘æˆ˜ï¼Œæå‡ºäº†ä¸€ç§æ— è®­ç»ƒæ··æ·†æ¦‚å¿µè¯†åˆ«ç­–ç•¥ã€‚</li>
<li>è¯¥ç­–ç•¥åˆ©ç”¨å°‘é‡æ–°æ•°æ®å’Œä»…å›¾åƒçº§åˆ«çš„æ ‡ç­¾æ¥å¢å¼ºæ¨¡å‹åœ¨æ–°ç¯å¢ƒä¸‹çš„æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.18070">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-0871260607c0f4ddf7a22133aa4f2d04.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a39c72da143d2f43a06c6de91a6609db.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6829cc49f11ee060737b4f816d8aed6d.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-a9b46699c829fcd80642228664e0ee00.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="CmFNet-Cross-modal-Fusion-Network-for-Weakly-supervised-Segmentation-of-Medical-Images"><a href="#CmFNet-Cross-modal-Fusion-Network-for-Weakly-supervised-Segmentation-of-Medical-Images" class="headerlink" title="CmFNet: Cross-modal Fusion Network for Weakly-supervised Segmentation of   Medical Images"></a>CmFNet: Cross-modal Fusion Network for Weakly-supervised Segmentation of   Medical Images</h2><p><strong>Authors:Dongdong Meng, Sheng Li, Hao Wu, Suqing Tian, Wenjun Ma, Guoping Wang, Xueqing Yan</strong></p>
<p>Accurate automatic medical image segmentation relies on high-quality, dense annotations, which are costly and time-consuming. Weakly supervised learning provides a more efficient alternative by leveraging sparse and coarse annotations instead of dense, precise ones. However, segmentation performance degradation and overfitting caused by sparse annotations remain key challenges. To address these issues, we propose CmFNet, a novel 3D weakly supervised cross-modal medical image segmentation approach. CmFNet consists of three main components: a modality-specific feature learning network, a cross-modal feature learning network, and a hybrid-supervised learning strategy. Specifically, the modality-specific feature learning network and the cross-modal feature learning network effectively integrate complementary information from multi-modal images, enhancing shared features across modalities to improve segmentation performance. Additionally, the hybrid-supervised learning strategy guides segmentation through scribble supervision, intra-modal regularization, and inter-modal consistency, modeling spatial and contextual relationships while promoting feature alignment. Our approach effectively mitigates overfitting, delivering robust segmentation results. It excels in segmenting both challenging small tumor regions and common anatomical structures. Extensive experiments on a clinical cross-modal nasopharyngeal carcinoma (NPC) dataset (including CT and MR imaging) and the publicly available CT Whole Abdominal Organ dataset (WORD) show that our approach outperforms state-of-the-art weakly supervised methods. In addition, our approach also outperforms fully supervised methods when full annotation is used. Our approach can facilitate clinical therapy and benefit various specialists, including physicists, radiologists, pathologists, and oncologists. </p>
<blockquote>
<p>å‡†ç¡®çš„è‡ªåŠ¨åŒ»å­¦å›¾åƒåˆ†å‰²ä¾èµ–äºé«˜è´¨é‡ã€å¯†é›†çš„æ³¨é‡Šï¼Œè€Œè¿™äº›æ³¨é‡Šæ˜¯æ˜‚è´µä¸”è€—æ—¶çš„ã€‚å¼±ç›‘ç£å­¦ä¹ é€šè¿‡åˆ©ç”¨ç¨€ç–å’Œç²—ç•¥çš„æ³¨é‡Šè€Œä¸æ˜¯å¯†é›†ã€ç²¾ç¡®çš„æ³¨é‡Šï¼Œæä¾›äº†ä¸€ç§æ›´é«˜æ•ˆçš„æ›¿ä»£æ–¹æ¡ˆã€‚ç„¶è€Œï¼Œç”±ç¨€ç–æ³¨é‡Šå¼•èµ·çš„åˆ†å‰²æ€§èƒ½ä¸‹é™å’Œè¿‡æ‹Ÿåˆä»æ˜¯å…³é”®æŒ‘æˆ˜ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†CmFNetï¼Œè¿™æ˜¯ä¸€ç§æ–°çš„3Då¼±ç›‘ç£è·¨æ¨¡æ€åŒ»å­¦å›¾åƒåˆ†å‰²æ–¹æ³•ã€‚CmFNetç”±ä¸‰ä¸ªä¸»è¦ç»„ä»¶æ„æˆï¼šç‰¹å®šæ¨¡æ€ç‰¹å¾å­¦ä¹ ç½‘ç»œã€è·¨æ¨¡æ€ç‰¹å¾å­¦ä¹ ç½‘ç»œå’Œæ··åˆç›‘ç£å­¦ä¹ ç­–ç•¥ã€‚å…·ä½“è€Œè¨€ï¼Œç‰¹å®šæ¨¡æ€ç‰¹å¾å­¦ä¹ ç½‘ç»œå’Œè·¨æ¨¡æ€ç‰¹å¾å­¦ä¹ ç½‘ç»œæœ‰æ•ˆåœ°èåˆäº†å¤šæ¨¡æ€å›¾åƒçš„äº’è¡¥ä¿¡æ¯ï¼Œå¢å¼ºäº†è·¨æ¨¡æ€çš„å…±äº«ç‰¹å¾ï¼Œæé«˜äº†åˆ†å‰²æ€§èƒ½ã€‚æ­¤å¤–ï¼Œæ··åˆç›‘ç£å­¦ä¹ ç­–ç•¥é€šè¿‡æ¶‚é¸¦ç›‘ç£ã€å†…éƒ¨æ¨¡æ€æ­£åˆ™åŒ–å’Œè·¨æ¨¡æ€ä¸€è‡´æ€§æ¥æŒ‡å¯¼åˆ†å‰²ï¼Œå»ºç«‹ç©ºé—´å…³ç³»å’Œä¸Šä¸‹æ–‡å…³ç³»ï¼ŒåŒæ—¶ä¿ƒè¿›ç‰¹å¾å¯¹é½ã€‚æˆ‘ä»¬çš„æ–¹æ³•æœ‰æ•ˆåœ°å‡è½»äº†è¿‡æ‹Ÿåˆé—®é¢˜ï¼Œæä¾›äº†ç¨³å¥çš„åˆ†å‰²ç»“æœã€‚å®ƒåœ¨åˆ†å‰²å…·æœ‰æŒ‘æˆ˜æ€§çš„å°è‚¿ç˜¤åŒºåŸŸå’Œå¸¸è§çš„è§£å‰–ç»“æ„æ–¹é¢è¡¨ç°å‡ºè‰²ã€‚åœ¨ä¸´åºŠè·¨æ¨¡æ€é¼»å’½ç™Œï¼ˆNPCï¼‰æ•°æ®é›†ï¼ˆåŒ…æ‹¬CTå’ŒMRæˆåƒï¼‰å’Œå…¬å…±å¯ç”¨çš„CTå…¨è…¹éƒ¨å™¨å®˜æ•°æ®é›†ï¼ˆWORDï¼‰ä¸Šçš„å¤§é‡å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•ä¼˜äºæœ€æ–°çš„å¼±ç›‘ç£æ–¹æ³•ã€‚å½“ä½¿ç”¨å®Œæ•´æ³¨é‡Šæ—¶ï¼Œæˆ‘ä»¬çš„æ–¹æ³•ä¹Ÿä¼˜äºå…¨ç›‘ç£æ–¹æ³•ã€‚æˆ‘ä»¬çš„æ–¹æ³•å¯ä»¥è¾…åŠ©ä¸´åºŠæ²»ç–—ï¼Œå¹¶æƒ åŠåŒ…æ‹¬ç‰©ç†å­¦å®¶ã€æ”¾å°„ç§‘åŒ»ç”Ÿã€ç—…ç†åŒ»ç”Ÿå’Œè‚¿ç˜¤å­¦å®¶åœ¨å†…çš„å„ç§ä¸“å®¶ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.18042v1">PDF</a> 10 pages, 6 figures</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºä¸€ç§åä¸ºCmFNetçš„æ–°å‹ä¸‰ç»´å¼±ç›‘ç£è·¨æ¨¡æ€åŒ»å­¦å›¾åƒåˆ†å‰²æ–¹æ³•ï¼Œè¯¥æ–¹æ³•èƒ½æœ‰æ•ˆè§£å†³åŒ»å­¦å›¾åƒåˆ†å‰²ä¸­çš„æŒ‘æˆ˜é—®é¢˜ã€‚é€šè¿‡ç»“åˆç‰¹å®šæ¨¡æ€ç‰¹å¾å­¦ä¹ ç½‘ç»œã€è·¨æ¨¡æ€ç‰¹å¾å­¦ä¹ ç½‘ç»œå’Œæ··åˆç›‘ç£å­¦ä¹ ç­–ç•¥ï¼ŒCmFNetèƒ½æœ‰æ•ˆåˆ©ç”¨ç¨€ç–ã€ç²—ç•¥çš„æ ‡æ³¨æ•°æ®æé«˜åˆ†å‰²æ€§èƒ½ï¼Œå¹¶æˆåŠŸå‡è½»è¿‡æ‹Ÿåˆé—®é¢˜ã€‚è¯¥æ–¹æ³•åœ¨è‚¿ç˜¤åŒºåŸŸå’Œå¸¸è§è§£å‰–ç»“æ„çš„åˆ†å‰²ä¸Šéƒ½è¡¨ç°å‡ºå“è¶Šæ€§èƒ½ï¼Œæ˜¾è‘—æå‡äº†ä¸´åºŠè¯Šç–—çš„æ•ˆç‡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•çš„æ€§èƒ½è¶…è¿‡äº†å½“å‰ä¸»æµçš„å¼±ç›‘ç£æ–¹æ³•å’ŒæŸäº›å…¨ç›‘ç£æ–¹æ³•ã€‚å®ƒæœ‰åŠ©äºç‰©ç†å­¦å®¶ã€æ”¾å°„ç§‘åŒ»ç”Ÿã€ç—…ç†å­¦å®¶å’Œè‚¿ç˜¤å­¦å®¶ç­‰ä¸åŒä¸“ä¸šé¢†åŸŸçš„åŒ»ç”Ÿè¿›è¡Œè¯Šæ–­å’Œæ²»ç–—ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>æå‡ºæ–°å‹åŒ»å­¦å›¾åƒåˆ†å‰²æ–¹æ³•CmFNetï¼Œåˆ©ç”¨å¼±ç›‘ç£å­¦ä¹ å‡å°‘æˆæœ¬å’Œæ—¶é—´ã€‚</li>
<li>æ–¹æ³•åŒ…å«ç‰¹å®šæ¨¡æ€ç‰¹å¾å­¦ä¹ ç½‘ç»œã€è·¨æ¨¡æ€ç‰¹å¾å­¦ä¹ ç½‘ç»œå’Œæ··åˆç›‘ç£å­¦ä¹ ç­–ç•¥ä¸‰ä¸ªä¸»è¦ç»„ä»¶ã€‚</li>
<li>é€šè¿‡é›†æˆå¤šæ¨¡æ€å›¾åƒä¿¡æ¯æé«˜åˆ†å‰²æ€§èƒ½ï¼Œå¹¶æœ‰æ•ˆå‡è½»è¿‡æ‹Ÿåˆé—®é¢˜ã€‚</li>
<li>åœ¨è‚¿ç˜¤åŒºåŸŸå’Œè§£å‰–ç»“æ„åˆ†å‰²ä¸Šè¡¨ç°ä¼˜è¶Šï¼Œå®éªŒç»“æœè¶…è¿‡å…¶ä»–å¼±ç›‘ç£æ–¹æ³•å’Œéƒ¨åˆ†å…¨ç›‘ç£æ–¹æ³•ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.18042">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-3b3f47cc30d04b70050313ab33a22fa6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2d9ba56ff9d4b4561a82fd3fdf885e9d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c3b7c79c42028ee6a110b52b883e0a71.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="Pre-Trained-LLM-is-a-Semantic-Aware-and-Generalizable-Segmentation-Booster"><a href="#Pre-Trained-LLM-is-a-Semantic-Aware-and-Generalizable-Segmentation-Booster" class="headerlink" title="Pre-Trained LLM is a Semantic-Aware and Generalizable Segmentation   Booster"></a>Pre-Trained LLM is a Semantic-Aware and Generalizable Segmentation   Booster</h2><p><strong>Authors:Fenghe Tang, Wenxin Ma, Zhiyang He, Xiaodong Tao, Zihang Jiang, S. Kevin Zhou</strong></p>
<p>With the advancement of Large Language Model (LLM) for natural language processing, this paper presents an intriguing finding: a frozen pre-trained LLM layer can process visual tokens for medical image segmentation tasks. Specifically, we propose a simple hybrid structure that integrates a pre-trained, frozen LLM layer within the CNN encoder-decoder segmentation framework (LLM4Seg). Surprisingly, this design improves segmentation performance with a minimal increase in trainable parameters across various modalities, including ultrasound, dermoscopy, polypscopy, and CT scans. Our in-depth analysis reveals the potential of transferring LLMâ€™s semantic awareness to enhance segmentation tasks, offering both improved global understanding and better local modeling capabilities. The improvement proves robust across different LLMs, validated using LLaMA and DeepSeek. </p>
<blockquote>
<p>éšç€è‡ªç„¶è¯­è¨€å¤„ç†ä¸­å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„å‘å±•ï¼Œæœ¬æ–‡å‘ˆç°äº†ä¸€é¡¹æœ‰è¶£çš„å‘ç°ï¼šå†»ç»“çš„é¢„è®­ç»ƒLLMå±‚å¯ä»¥å¤„ç†åŒ»å­¦å›¾åƒåˆ†å‰²ä»»åŠ¡çš„è§†è§‰æ ‡è®°ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ç®€å•çš„æ··åˆç»“æ„ï¼Œè¯¥ç»“æ„åœ¨CNNç¼–ç å™¨-è§£ç å™¨åˆ†å‰²æ¡†æ¶ä¸­é›†æˆäº†é¢„è®­ç»ƒçš„å†»ç»“LLMå±‚ï¼ˆLLM4Segï¼‰ã€‚ä»¤äººæƒŠè®¶çš„æ˜¯ï¼Œè¿™ç§è®¾è®¡åœ¨å„ç§æ¨¡æ€ï¼ˆåŒ…æ‹¬è¶…å£°ã€çš®è‚¤é•œæ£€æŸ¥ã€ç»“è‚ é•œæ£€æŸ¥å’ŒCTæ‰«æï¼‰çš„åˆ†å‰²æ€§èƒ½ä¸Šéƒ½æœ‰æ‰€æé«˜ï¼ŒåŒæ—¶å¯è®­ç»ƒå‚æ•°åªå¢åŠ äº†å¾ˆå°çš„ä¸€éƒ¨åˆ†ã€‚æˆ‘ä»¬çš„æ·±å…¥åˆ†ææ­ç¤ºäº†å°†LLMçš„è¯­ä¹‰æ„ŸçŸ¥èƒ½åŠ›è½¬ç§»åˆ°å¢å¼ºåˆ†å‰²ä»»åŠ¡ä¸­çš„æ½œåŠ›ï¼Œæä¾›äº†æ›´å¥½çš„å…¨å±€ç†è§£å’Œå±€éƒ¨å»ºæ¨¡èƒ½åŠ›ã€‚æ”¹è¿›åœ¨ä¸åŒçš„å¤§å‹è¯­è¨€æ¨¡å‹ä¸­è¡¨ç°ç¨³å¥ï¼Œé€šè¿‡ä½¿ç”¨LLaMAå’ŒDeepSeekè¿›è¡Œäº†éªŒè¯ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.18034v1">PDF</a> Accepted by MICCAI 2025. Code: <a target="_blank" rel="noopener" href="https://github.com/FengheTan9/LLM4Seg">https://github.com/FengheTan9/LLM4Seg</a></p>
<p><strong>Summary</strong><br>     æœ¬ç ”ç©¶ç»“åˆè‡ªç„¶è¯­è¨€å¤„ç†ä¸­çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ï¼Œæå‡ºä¸€ç§ç”¨äºåŒ»å­¦å›¾åƒåˆ†å‰²ä»»åŠ¡çš„æ–°æ–¹æ³•ã€‚ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œé¢„è®­ç»ƒLLMå±‚çš„å†»ç»“ç»“æ„å¯ç”¨äºå¤„ç†åŒ»å­¦å›¾åƒä¸­çš„è§†è§‰ç¬¦å·ï¼Œç»“åˆå·ç§¯ç¥ç»ç½‘ç»œç¼–ç å™¨è§£ç å™¨åˆ†å‰²æ¡†æ¶ï¼ˆLLM4Segï¼‰ï¼Œèƒ½å¤Ÿæé«˜ä¸åŒæ¨¡æ€ï¼ˆå¦‚è¶…å£°ã€çš®è‚¤é•œã€ç»“è‚ é•œæ£€æŸ¥å’ŒCTæ‰«æï¼‰å›¾åƒçš„åˆ†å‰²æ€§èƒ½ï¼Œä¸”å…·æœ‰æœ€å°‘çš„å‚æ•°å¢é‡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>é¢„è®­ç»ƒçš„LLMå±‚å¯åº”ç”¨äºåŒ»å­¦å›¾åƒåˆ†å‰²ä»»åŠ¡ï¼Œæé«˜æ€§èƒ½ã€‚</li>
<li>æå‡ºçš„æ··åˆç»“æ„LLM4Segç»“åˆäº†é¢„è®­ç»ƒçš„å†»ç»“LLMå±‚ï¼Œå¢å¼ºäº†åˆ†å‰²ä»»åŠ¡çš„æ€§èƒ½ã€‚</li>
<li>åœ¨ä¸åŒæ¨¡æ€çš„åŒ»å­¦å›¾åƒä¸Šå‡å®ç°äº†æ€§èƒ½æå‡ï¼ŒåŒ…æ‹¬è¶…å£°ã€çš®è‚¤é•œã€ç»“è‚ é•œæ£€æŸ¥å’ŒCTæ‰«æã€‚</li>
<li>LLMçš„è¯­ä¹‰æ„è¯†èƒ½å¤Ÿå¢å¼ºåˆ†å‰²ä»»åŠ¡çš„æ€§èƒ½ï¼Œæé«˜äº†å…¨å±€ç†è§£å’Œå±€éƒ¨å»ºæ¨¡èƒ½åŠ›ã€‚</li>
<li>æ€§èƒ½æå‡åœ¨ä¸åŒçš„LLMsä¸­å‡å¾—åˆ°äº†éªŒè¯ï¼ŒåŒ…æ‹¬LLaMAå’ŒDeepSeekã€‚</li>
<li>è¿™ç§æ–°æ–¹æ³•å…·æœ‰æœ€å°çš„å‚æ•°å¢é‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.18034">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-86aec0522162920f038641b53b6672d5.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-3324676610281db545ca74ced5c172d0.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="MiCo-Multiple-Instance-Learning-with-Context-Aware-Clustering-for-Whole-Slide-Image-Analysis"><a href="#MiCo-Multiple-Instance-Learning-with-Context-Aware-Clustering-for-Whole-Slide-Image-Analysis" class="headerlink" title="MiCo: Multiple Instance Learning with Context-Aware Clustering for Whole   Slide Image Analysis"></a>MiCo: Multiple Instance Learning with Context-Aware Clustering for Whole   Slide Image Analysis</h2><p><strong>Authors:Junjian Li, Hulin Kuang, Jin Liu, Hailin Yue, Mengshen He, Jianxin Wang</strong></p>
<p>Multiple instance learning (MIL) has shown significant promise in histopathology whole slide image (WSI) analysis for cancer diagnosis and prognosis. However, the inherent spatial heterogeneity of WSIs presents critical challenges, as morphologically similar tissue types are often dispersed across distant anatomical regions. Conventional MIL methods struggle to model these scattered tissue distributions and capture cross-regional spatial interactions effectively. To address these limitations, we propose a novel Multiple instance learning framework with Context-Aware Clustering (MiCo), designed to enhance cross-regional intra-tissue correlations and strengthen inter-tissue semantic associations in WSIs. MiCo begins by clustering instances to distill discriminative morphological patterns, with cluster centroids serving as semantic anchors. To enhance cross-regional intra-tissue correlations, MiCo employs a Cluster Route module, which dynamically links instances of the same tissue type across distant regions via feature similarity. These semantic anchors act as contextual hubs, propagating semantic relationships to refine instance-level representations. To eliminate semantic fragmentation and strengthen inter-tissue semantic associations, MiCo integrates a Cluster Reducer module, which consolidates redundant anchors while enhancing information exchange between distinct semantic groups. Extensive experiments on two challenging tasks across nine large-scale public cancer datasets demonstrate the effectiveness of MiCo, showcasing its superiority over state-of-the-art methods. The code is available at <a target="_blank" rel="noopener" href="https://github.com/junjianli106/MiCo">https://github.com/junjianli106/MiCo</a>. </p>
<blockquote>
<p>å¤šå®ä¾‹å­¦ä¹ ï¼ˆMILï¼‰åœ¨ç—…ç†å­¦å…¨åˆ‡ç‰‡å›¾åƒï¼ˆWSIï¼‰åˆ†æä¸­ï¼Œå¯¹äºç™Œç—‡è¯Šæ–­å’Œé¢„åæ–¹é¢æ˜¾ç¤ºå‡ºå·¨å¤§æ½œåŠ›ã€‚ç„¶è€Œï¼ŒWSIå›ºæœ‰çš„ç©ºé—´å¼‚è´¨æ€§å¸¦æ¥äº†é‡å¤§æŒ‘æˆ˜ï¼Œå› ä¸ºå½¢æ€ä¸Šç›¸ä¼¼çš„ç»„ç»‡ç±»å‹é€šå¸¸åˆ†å¸ƒåœ¨é¥è¿œçš„è§£å‰–åŒºåŸŸã€‚ä¼ ç»Ÿçš„MILæ–¹æ³•éš¾ä»¥å¯¹è¿™äº›åˆ†æ•£çš„ç»„ç»‡åˆ†å¸ƒè¿›è¡Œå»ºæ¨¡ï¼Œå¹¶æœ‰æ•ˆåœ°æ•è·è·¨åŒºåŸŸçš„ç©ºé—´äº¤äº’ã€‚ä¸ºäº†è§£å†³è¿™äº›å±€é™æ€§ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§å¸¦æœ‰ä¸Šä¸‹æ–‡æ„ŸçŸ¥èšç±»çš„å¤šå®ä¾‹å­¦ä¹ æ¡†æ¶ï¼ˆMiCoï¼‰ï¼Œæ—¨åœ¨å¢å¼ºè·¨åŒºåŸŸçš„ç»„ç»‡å†…ç›¸å…³æ€§å’ŒåŠ å¼ºWSIä¸­çš„ç»„ç»‡é—´è¯­ä¹‰å…³è”ã€‚MiCoé¦–å…ˆé€šè¿‡èšç±»å®ä¾‹æ¥æç‚¼åˆ¤åˆ«æ€§å½¢æ€æ¨¡å¼ï¼Œä»¥èšç±»ä¸­å¿ƒä½œä¸ºè¯­ä¹‰é”šç‚¹ã€‚ä¸ºäº†å¢å¼ºè·¨åŒºåŸŸçš„ç»„ç»‡å†…ç›¸å…³æ€§ï¼ŒMiCoé‡‡ç”¨äº†ä¸€ç§é›†ç¾¤è·¯ç”±æ¨¡å—ï¼Œè¯¥æ¨¡å—é€šè¿‡ç‰¹å¾ç›¸ä¼¼æ€§åŠ¨æ€é“¾æ¥ç›¸åŒç»„ç»‡ç±»å‹çš„å®ä¾‹ï¼Œè·¨è¶Šé¥è¿œåŒºåŸŸã€‚è¿™äº›è¯­ä¹‰é”šç‚¹ä½œä¸ºä¸Šä¸‹æ–‡ä¸­å¿ƒï¼Œä¼ æ’­è¯­ä¹‰å…³ç³»ä»¥ä¼˜åŒ–å®ä¾‹çº§è¡¨ç¤ºã€‚ä¸ºäº†æ¶ˆé™¤è¯­ä¹‰ç¢ç‰‡å¹¶åŠ å¼ºç»„ç»‡é—´çš„è¯­ä¹‰å…³è”ï¼ŒMiCoé›†æˆäº†ä¸€ä¸ªé›†ç¾¤ç¼©å‡æ¨¡å—ï¼Œè¯¥æ¨¡å—æ•´åˆäº†å†—ä½™çš„é”šç‚¹ï¼ŒåŒæ—¶å¢å¼ºäº†ä¸åŒè¯­ä¹‰ç»„ä¹‹é—´çš„ä¿¡æ¯äº¤æ¢ã€‚åœ¨ä¸¤ä¸ªå…·æœ‰æŒ‘æˆ˜æ€§çš„ä»»åŠ¡ä¸Šçš„ä¹ä¸ªå¤§å‹å…¬å¼€ç™Œç—‡æ•°æ®é›†ä¸Šçš„å¹¿æ³›å®éªŒè¯æ˜äº†MiCoçš„æœ‰æ•ˆæ€§ï¼Œå±•ç¤ºäº†å…¶åœ¨æœ€æ–°æŠ€æœ¯ä¸Šçš„ä¼˜è¶Šæ€§ã€‚ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/junjianli106/MiCo%E4%B8%8A%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/junjianli106/MiCoä¸Šæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.18028v1">PDF</a> MICCAI 2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ä¸ªåä¸ºMiCoçš„æ–°å‹å¤šå®ä¾‹å­¦ä¹ æ¡†æ¶ï¼Œç”¨äºå¢å¼ºå…¨æ»‘ç‰‡å›¾åƒï¼ˆWSIï¼‰åˆ†æä¸­è·¨åŒºåŸŸçš„ç»„ç»‡å†…å…³è”æ€§å’Œç»„ç»‡é—´çš„è¯­ä¹‰å…³è”ã€‚é€šè¿‡èšç±»å®ä¾‹å’Œé‡‡ç”¨Cluster Routeæ¨¡å—ï¼ŒMiCoèƒ½å¤ŸåŠ¨æ€é“¾æ¥åŒä¸€ç»„ç»‡ç±»å‹çš„å®ä¾‹ï¼Œå¢å¼ºè·¨åŒºåŸŸçš„ç»„ç»‡å†…å…³è”æ€§ã€‚åŒæ—¶ï¼ŒMiCoçš„Cluster Reduceræ¨¡å—èƒ½å¤Ÿæ•´åˆå†—ä½™é”šç‚¹ï¼Œå¼ºåŒ–ä¸åŒç»„ç»‡é—´çš„è¯­ä¹‰å…³è”ã€‚åœ¨ä¹ä¸ªå¤§å‹å…¬å…±ç™Œç—‡æ•°æ®é›†ä¸Šçš„å®éªŒè¯æ˜ï¼ŒMiCoçš„æœ‰æ•ˆæ€§ä¼˜äºç°æœ‰æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>MiCoæ˜¯ä¸€ä¸ªåŸºäºå¤šå®ä¾‹å­¦ä¹ æ¡†æ¶çš„æ–°å‹æ–¹æ³•ï¼Œç”¨äºè§£å†³å…¨æ»‘ç‰‡å›¾åƒåˆ†æä¸­çš„ç©ºé—´å¼‚è´¨æ€§æŒ‘æˆ˜ã€‚</li>
<li>MiCoé€šè¿‡èšç±»å®ä¾‹æ¥æå–åˆ¤åˆ«æ€§å½¢æ€æ¨¡å¼ï¼Œå¹¶ä½¿ç”¨ç°‡ä¸­å¿ƒä½œä¸ºè¯­ä¹‰é”šç‚¹ã€‚</li>
<li>Cluster Routeæ¨¡å—èƒ½å¤ŸåŠ¨æ€é“¾æ¥åŒä¸€ç»„ç»‡ç±»å‹çš„å®ä¾‹ï¼Œå¢å¼ºè·¨åŒºåŸŸçš„ç»„ç»‡å†…å…³è”æ€§ã€‚</li>
<li>Cluster Reduceræ¨¡å—æ•´åˆå†—ä½™é”šç‚¹ï¼Œå¼ºåŒ–ç»„ç»‡é—´çš„è¯­ä¹‰å…³è”ã€‚</li>
<li>MiCoåœ¨ä¹ä¸ªå¤§å‹å…¬å…±ç™Œç—‡æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨ç°ä¼˜å¼‚ï¼Œè¯æ˜äº†å…¶æœ‰æ•ˆæ€§ã€‚</li>
<li>MiCoé€‚ç”¨äºç™Œç—‡è¯Šæ–­å’Œæ²»ç–—é¢„åä¸­çš„å…¨æ»‘ç‰‡å›¾åƒåˆ†æã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.18028">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-855df2c62a51241b71dfd04174ec3ab1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ebd5a4d37b9ed04d7d5bac543c5bf5ad.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b21192382a4da64fb6c4f43876b3a2bf.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-78d05670bd5b7d86ca88925ccc164b7c.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="LVPNet-A-Latent-variable-based-Prediction-driven-End-to-end-Framework-for-Lossless-Compression-of-Medical-Images"><a href="#LVPNet-A-Latent-variable-based-Prediction-driven-End-to-end-Framework-for-Lossless-Compression-of-Medical-Images" class="headerlink" title="LVPNet: A Latent-variable-based Prediction-driven End-to-end Framework   for Lossless Compression of Medical Images"></a>LVPNet: A Latent-variable-based Prediction-driven End-to-end Framework   for Lossless Compression of Medical Images</h2><p><strong>Authors:Chenyue Song, Chen Hui, Qing Lin, Wei Zhang, Siqiao Li, Shengping Zhang, Haiqi Zhu, Zhixuan Li, Shaohui Liu, Feng Jiang, Xiang Li</strong></p>
<p>Autoregressive Initial Bits is a framework that integrates sub-image autoregression and latent variable modeling, demonstrating its advantages in lossless medical image compression. However, in existing methods, the image segmentation process leads to an even distribution of latent variable information across each sub-image, which in turn causes posterior collapse and inefficient utilization of latent variables. To deal with these issues, we propose a prediction-based end-to-end lossless medical image compression method named LVPNet, leveraging global latent variables to predict pixel values and encoding predicted probabilities for lossless compression. Specifically, we introduce the Global Multi-scale Sensing Module (GMSM), which extracts compact and informative latent representations from the entire image, effectively capturing spatial dependencies within the latent space. Furthermore, to mitigate the information loss introduced during quantization, we propose the Quantization Compensation Module (QCM), which learns the distribution of quantization errors and refines the quantized features to compensate for quantization loss. Extensive experiments on challenging benchmarks demonstrate that our method achieves superior compression efficiency compared to state-of-the-art lossless image compression approaches, while maintaining competitive inference speed. The code is at <a target="_blank" rel="noopener" href="https://github.com/Anonymity00000/Anonymity-repository/">https://github.com/Anonymity00000/Anonymity-repository/</a>. </p>
<blockquote>
<p>â€œAutoregressive Initial Bitsâ€æ˜¯ä¸€ä¸ªèåˆäº†å­å›¾åƒè‡ªå›å½’å’Œæ½œåœ¨å˜é‡å»ºæ¨¡çš„æ¡†æ¶ï¼Œåœ¨æ— æŸåŒ»å­¦å›¾åƒå‹ç¼©ä¸­å±•ç¤ºäº†å…¶ä¼˜åŠ¿ã€‚ç„¶è€Œï¼Œåœ¨ç°æœ‰æ–¹æ³•ä¸­ï¼Œå›¾åƒåˆ†å‰²è¿‡ç¨‹å¯¼è‡´æ½œåœ¨å˜é‡ä¿¡æ¯å‡åŒ€åˆ†å¸ƒåœ¨æ¯ä¸ªå­å›¾åƒä¸­ï¼Œè¿™è¿›è€Œå¯¼è‡´åå´©æºƒå’Œæ½œåœ¨å˜é‡çš„ä½æ•ˆåˆ©ç”¨ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºé¢„æµ‹çš„ç«¯åˆ°ç«¯æ— æŸåŒ»å­¦å›¾åƒå‹ç¼©æ–¹æ³•ï¼Œåä¸ºLVPNetã€‚è¯¥æ–¹æ³•åˆ©ç”¨å…¨å±€æ½œåœ¨å˜é‡æ¥é¢„æµ‹åƒç´ å€¼ï¼Œå¹¶å¯¹é¢„æµ‹æ¦‚ç‡è¿›è¡Œç¼–ç ä»¥å®ç°æ— æŸå‹ç¼©ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬å¼•å…¥äº†å…¨å±€å¤šå°ºåº¦æ„ŸçŸ¥æ¨¡å—ï¼ˆGMSMï¼‰ï¼Œè¯¥æ¨¡å—ä»æ•´ä¸ªå›¾åƒä¸­æå–ç´§å‡‘ä¸”ä¿¡æ¯ä¸°å¯Œçš„æ½œåœ¨è¡¨ç¤ºï¼Œæœ‰æ•ˆåœ°æ•è·æ½œåœ¨ç©ºé—´ä¸­çš„ç©ºé—´ä¾èµ–æ€§ã€‚æ­¤å¤–ï¼Œä¸ºäº†å‡è½»é‡åŒ–è¿‡ç¨‹ä¸­å¼•å…¥çš„ä¿¡æ¯æŸå¤±ï¼Œæˆ‘ä»¬æå‡ºäº†é‡åŒ–è¡¥å¿æ¨¡å—ï¼ˆQCMï¼‰ï¼Œè¯¥æ¨¡å—å­¦ä¹ é‡åŒ–è¯¯å·®çš„åˆ†å¸ƒï¼Œå¹¶ç»†åŒ–é‡åŒ–ç‰¹å¾ä»¥è¡¥å¿é‡åŒ–æŸå¤±ã€‚åœ¨å…·æœ‰æŒ‘æˆ˜æ€§çš„åŸºå‡†æµ‹è¯•ä¸Šçš„å¤§é‡å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•ç›¸è¾ƒäºæœ€æ–°çš„æ— æŸå›¾åƒå‹ç¼©æ–¹æ³•ï¼Œå®ç°äº†æ›´é«˜çš„å‹ç¼©æ•ˆç‡ï¼ŒåŒæ—¶ä¿æŒäº†ç«äº‰æ€§çš„æ¨ç†é€Ÿåº¦ã€‚ä»£ç ä½äº<a target="_blank" rel="noopener" href="https://github.com/Anonymity00000/Anonymity-repository%E3%80%82">https://github.com/Anonymity00000/Anonymity-repository/ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.17983v1">PDF</a> Accepted to MICCAI 2025</p>
<p><strong>Summary</strong></p>
<p>åŒ»å­¦å›¾åƒæ— æŸå‹ç¼©æ–°æŠ€æœ¯â€”â€”LVPNetã€‚è¯¥æ–¹æ³•åˆ©ç”¨å…¨å±€æ½œå˜é‡é¢„æµ‹åƒç´ å€¼ï¼Œå¹¶ç¼–ç é¢„æµ‹æ¦‚ç‡ä»¥å®ç°æ— æŸå‹ç¼©ã€‚é€šè¿‡å¼•å…¥å…¨å±€å¤šå°ºåº¦æ„ŸçŸ¥æ¨¡å—ï¼ˆGMSMï¼‰å’Œé‡åŒ–è¡¥å¿æ¨¡å—ï¼ˆQCMï¼‰ï¼Œæé«˜å‹ç¼©æ•ˆç‡å’Œé‡åŒ–æŸå¤±è¡¥å¿ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LVPNetæ˜¯ä¸€ä¸ªåŸºäºé¢„æµ‹çš„æ— æŸåŒ»å­¦å›¾åƒå‹ç¼©æ–¹æ³•ï¼Œåˆ©ç”¨å…¨å±€æ½œå˜é‡é¢„æµ‹åƒç´ å€¼ã€‚</li>
<li>å¼•å…¥Global Multi-scale Sensing Module (GMSM)ï¼Œä»æ•´ä¸ªå›¾åƒä¸­æå–ç´§å‡‘ä¸”ä¿¡æ¯ä¸°å¯Œçš„æ½œåœ¨è¡¨ç¤ºã€‚</li>
<li>æå‡ºQuantization Compensation Module (QCM)ï¼Œå­¦ä¹ é‡åŒ–è¯¯å·®çš„åˆ†å¸ƒï¼Œå¹¶ä¼˜åŒ–é‡åŒ–ç‰¹å¾ä»¥è¡¥å¿é‡åŒ–æŸå¤±ã€‚</li>
<li>LVPNetåœ¨å…·æœ‰æŒ‘æˆ˜æ€§çš„åŸºå‡†æµ‹è¯•ä¸Šå®ç°äº†é«˜æ•ˆçš„å‹ç¼©æ€§èƒ½ï¼Œä¼˜äºç°æœ‰çš„æ— æŸå›¾åƒå‹ç¼©æ–¹æ³•ã€‚</li>
<li>ä¿æŒäº†è¾ƒé«˜çš„æ¨ç†é€Ÿåº¦ï¼Œå…·æœ‰ç«äº‰æ€§ã€‚</li>
<li>æ–¹æ³•çš„ä»£ç å·²å…¬å¼€ï¼Œå¯ä¾›è¿›ä¸€æ­¥ç ”ç©¶å’Œä½¿ç”¨ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.17983">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-e837720408ff033223a9dafe1a703a4d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-313bfa7785fb8dc3351a909facbeee77.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2051bebfb4a22c53ae19ad6c2ed36275.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-bc70ebf482c2c0bac3e9178b1a1ed081.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="Mobile-Image-Analysis-Application-for-Mantoux-Skin-Test"><a href="#Mobile-Image-Analysis-Application-for-Mantoux-Skin-Test" class="headerlink" title="Mobile Image Analysis Application for Mantoux Skin Test"></a>Mobile Image Analysis Application for Mantoux Skin Test</h2><p><strong>Authors:Liong Gele, Tan Chye Cheah</strong></p>
<p>This paper presents a newly developed mobile application designed to diagnose Latent Tuberculosis Infection (LTBI) using the Mantoux Skin Test (TST). Traditional TST methods often suffer from low follow-up return rates, patient discomfort, and subjective manual interpretation, particularly with the ball-point pen method, leading to misdiagnosis and delayed treatment. Moreover, previous developed mobile applications that used 3D reconstruction, this app utilizes scaling stickers as reference objects for induration measurement. This mobile application integrates advanced image processing technologies, including ARCore, and machine learning algorithms such as DeepLabv3 for robust image segmentation and precise measurement of skin indurations indicative of LTBI. The system employs an edge detection algorithm to enhance accuracy. The application was evaluated against standard clinical practices, demonstrating significant improvements in accuracy and reliability. This innovation is crucial for effective tuberculosis management, especially in resource-limited regions. By automating and standardizing TST evaluations, the application enhances the accessibility and efficiency of TB di-agnostics. Future work will focus on refining machine learning models, optimizing measurement algorithms, expanding functionalities to include comprehensive patient data management, and enhancing ARCoreâ€™s performance across various lighting conditions and operational settings. </p>
<blockquote>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸€æ¬¾æ–°å¼€å‘çš„ç§»åŠ¨åº”ç”¨ç¨‹åºï¼Œè¯¥ç¨‹åºé‡‡ç”¨Mantouxçš®è‚¤è¯•éªŒï¼ˆTSTï¼‰è®¾è®¡æ¥è¯Šæ–­æ½œä¼æ€§ç»“æ ¸æ„ŸæŸ“ï¼ˆLTBIï¼‰ã€‚ä¼ ç»Ÿçš„TSTæ–¹æ³•å¸¸å¸¸å—åˆ°åç»­å›è®¿ç‡ä½ã€æ‚£è€…ä¸é€‚ä»¥åŠæ‰‹åŠ¨è§£è¯»ä¸»è§‚æ€§ï¼ˆç‰¹åˆ«æ˜¯ç”¨åœ†ç ç¬”æ–¹æ³•ï¼‰çš„å½±å“ï¼Œå¯¼è‡´è¯¯è¯Šå’Œå»¶è¿Ÿæ²»ç–—ã€‚æ­¤å¤–ï¼Œä¸ä¹‹å‰ä½¿ç”¨3Dé‡å»ºæŠ€æœ¯çš„ç§»åŠ¨åº”ç”¨ä¸åŒï¼Œæœ¬åº”ç”¨ä½¿ç”¨åˆ»åº¦è´´çº¸ä½œä¸ºæµ‹é‡ç¡¬ç»“æ—¶çš„å‚è€ƒå¯¹è±¡ã€‚è¯¥ç§»åŠ¨åº”ç”¨ç¨‹åºé›†æˆäº†å…ˆè¿›å›¾åƒå¤„ç†æŠ€æœ¯ï¼ŒåŒ…æ‹¬ARCoreï¼Œä»¥åŠæœºå™¨å­¦ä¹ ç®—æ³•ï¼Œå¦‚DeepLabv3ï¼Œç”¨äºç¨³å¥çš„å›¾åƒåˆ†å‰²å’Œç²¾ç¡®æµ‹é‡çš®è‚¤ç¡¬ç»“ï¼Œè¿™æ˜¯LTBIçš„é‡è¦æŒ‡æ ‡ã€‚ç³»ç»Ÿé‡‡ç”¨è¾¹ç¼˜æ£€æµ‹ç®—æ³•æ¥æé«˜å‡†ç¡®æ€§ã€‚è¯¥åº”ç”¨ç¨‹åºä¸æ ‡å‡†ä¸´åºŠå®è·µè¿›è¡Œäº†è¯„ä¼°ï¼Œåœ¨å‡†ç¡®æ€§å’Œå¯é æ€§æ–¹é¢æ˜¾ç¤ºå‡ºæ˜¾ç€æ”¹å–„ã€‚è¿™ä¸€åˆ›æ–°å¯¹äºæœ‰æ•ˆç®¡ç†ç»“æ ¸ç—…ï¼Œç‰¹åˆ«æ˜¯åœ¨èµ„æºæœ‰é™çš„åœ°åŒºï¼Œè‡³å…³é‡è¦ã€‚é€šè¿‡è‡ªåŠ¨åŒ–å’Œæ ‡å‡†åŒ–TSTè¯„ä¼°ï¼Œè¯¥åº”ç”¨ç¨‹åºæé«˜äº†ç»“æ ¸è¯Šæ–­çš„å¯è®¿é—®æ€§å’Œæ•ˆç‡ã€‚æœªæ¥çš„å·¥ä½œå°†ä¾§é‡äºæ”¹è¿›æœºå™¨å­¦ä¹ æ¨¡å‹ã€ä¼˜åŒ–æµ‹é‡ç®—æ³•ã€æ‰©å±•åŠŸèƒ½ä»¥åŒ…æ‹¬å…¨é¢çš„æ‚£è€…æ•°æ®ç®¡ç†ï¼Œä»¥åŠæé«˜ARCoreåœ¨ä¸åŒç…§æ˜æ¡ä»¶å’Œæ“ä½œç¯å¢ƒä¸‹çš„æ€§èƒ½ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.17954v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>è¯¥è®ºæ–‡ä»‹ç»äº†ä¸€æ¬¾æ–°å¼€å‘çš„ç§»åŠ¨åº”ç”¨ç¨‹åºï¼Œç”¨äºé€šè¿‡Mantouxçš®è‚¤è¯•éªŒï¼ˆTSTï¼‰è¯Šæ–­éšæ€§ç»“æ ¸æ„ŸæŸ“ï¼ˆLTBIï¼‰ã€‚è¯¥ç¨‹åºè§£å†³äº†ä¼ ç»ŸTSTæ–¹æ³•é¢ä¸´çš„å›è®¿ç‡ä½ã€æ‚£è€…ä¸é€‚å’Œæ‰‹åŠ¨è§£è¯»ä¸»è§‚ç­‰é—®é¢˜ï¼Œç‰¹åˆ«æ˜¯ä½¿ç”¨åœ†ç ç¬”æ–¹æ³•å¯¼è‡´çš„è¯¯è¯Šå’Œå»¶è¿Ÿæ²»ç–—ã€‚è¯¥ç¨‹åºé‡‡ç”¨å…ˆè¿›çš„å›¾åƒå¤„ç†æŠ€æœ¯ï¼ŒåŒ…æ‹¬ARCoreå’Œæœºå™¨å­¦ä¹ ç®—æ³•ï¼Œå¦‚DeepLabv3ï¼Œè¿›è¡Œçš®è‚¤ç¡¬ç»“æµ‹é‡ï¼Œä»¥å‡†ç¡®è¯Šæ–­LTBIã€‚ç³»ç»Ÿé‡‡ç”¨è¾¹ç¼˜æ£€æµ‹ç®—æ³•æé«˜å‡†ç¡®æ€§ã€‚è¯¥åº”ç”¨ç¨‹åºå·²æŒ‰ç…§æ ‡å‡†ä¸´åºŠå®è·µè¿›è¡Œè¯„ä¼°ï¼Œè¯æ˜å…¶åœ¨å‡†ç¡®æ€§å’Œå¯é æ€§æ–¹é¢æœ‰æ˜æ˜¾æ”¹è¿›ã€‚è¿™ä¸€åˆ›æ–°å¯¹äºæœ‰æ•ˆç®¡ç†ç»“æ ¸ç—…ï¼Œå°¤å…¶æ˜¯åœ¨èµ„æºæœ‰é™çš„åœ°åŒºï¼Œå…·æœ‰é‡è¦æ„ä¹‰ã€‚é€šè¿‡è‡ªåŠ¨åŒ–å’Œæ ‡å‡†åŒ–TSTè¯„ä¼°ï¼Œè¯¥åº”ç”¨ç¨‹åºæé«˜äº†ç»“æ ¸è¯Šæ–­çš„å¯è¾¾æ€§å’Œæ•ˆç‡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ–°ç§»åŠ¨åº”ç”¨ç¨‹åºç”¨äºè¯Šæ–­éšæ€§ç»“æ ¸æ„ŸæŸ“ï¼ˆLTBIï¼‰ï¼ŒåŸºäºMantouxçš®è‚¤è¯•éªŒï¼ˆTSTï¼‰ã€‚</li>
<li>ä¼ ç»ŸTSTæ–¹æ³•å­˜åœ¨å›è®¿ç‡ä½ã€æ‚£è€…ä¸é€‚å’Œä¸»è§‚è§£è¯»ç­‰é—®é¢˜ã€‚</li>
<li>è¯¥åº”ç”¨ç¨‹åºé‡‡ç”¨å…ˆè¿›çš„å›¾åƒå¤„ç†æŠ€æœ¯å’Œæœºå™¨å­¦ä¹ ç®—æ³•è¿›è¡Œçš®è‚¤ç¡¬ç»“æµ‹é‡ã€‚</li>
<li>åº”ç”¨äº†DeepLabv3ç®—æ³•å’ŒARCoreæŠ€æœ¯ï¼Œæé«˜äº†è¯Šæ–­å‡†ç¡®æ€§å’Œå¯é æ€§ã€‚</li>
<li>ç³»ç»Ÿé‡‡ç”¨è¾¹ç¼˜æ£€æµ‹ç®—æ³•å¢å¼ºæµ‹é‡ç²¾åº¦ã€‚</li>
<li>åº”ç”¨ç¨‹åºå·²åœ¨æ ‡å‡†ä¸´åºŠç¯å¢ƒä¸­è¯„ä¼°ï¼Œè¯æ˜å…¶æ•ˆæœã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.17954">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-aadda222acc71b566cec4093b060a1c6.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="Cloud-Aware-SAR-Fusion-for-Enhanced-Optical-Sensing-in-Space-Missions"><a href="#Cloud-Aware-SAR-Fusion-for-Enhanced-Optical-Sensing-in-Space-Missions" class="headerlink" title="Cloud-Aware SAR Fusion for Enhanced Optical Sensing in Space Missions"></a>Cloud-Aware SAR Fusion for Enhanced Optical Sensing in Space Missions</h2><p><strong>Authors:Trong-An Bui, Thanh-Thoai Le</strong></p>
<p>Cloud contamination significantly impairs the usability of optical satellite imagery, affecting critical applications such as environmental monitoring, disaster response, and land-use analysis. This research presents a Cloud-Attentive Reconstruction Framework that integrates SAR-optical feature fusion with deep learning-based image reconstruction to generate cloud-free optical imagery. The proposed framework employs an attention-driven feature fusion mechanism to align complementary structural information from Synthetic Aperture Radar (SAR) with spectral characteristics from optical data. Furthermore, a cloud-aware model update strategy introduces adaptive loss weighting to prioritize cloud-occluded regions, enhancing reconstruction accuracy. Experimental results demonstrate that the proposed method outperforms existing approaches, achieving a PSNR of 31.01 dB, SSIM of 0.918, and MAE of 0.017. These outcomes highlight the frameworkâ€™s effectiveness in producing high-fidelity, spatially and spectrally consistent cloud-free optical images. </p>
<blockquote>
<p>äº‘æ±¡æŸ“ä¸¥é‡æŸå®³äº†å…‰å­¦å«æ˜Ÿå›¾åƒçš„ä½¿ç”¨æ€§èƒ½ï¼Œå¯¹è¯¸å¦‚ç¯å¢ƒç›‘æµ‹ã€ç¾å®³å“åº”å’ŒåœŸåœ°åˆ©ç”¨åˆ†æç­‰å…³é”®åº”ç”¨äº§ç”Ÿäº†å½±å“ã€‚æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§äº‘æ„è¯†é‡å»ºæ¡†æ¶ï¼Œè¯¥æ¡†æ¶èåˆäº†SAR-å…‰å­¦ç‰¹å¾èåˆå’ŒåŸºäºæ·±åº¦å­¦ä¹ çš„å›¾åƒé‡å»ºæŠ€æœ¯ï¼Œä»¥ç”Ÿæˆæ— äº‘å…‰å­¦å›¾åƒã€‚æ‰€æå‡ºçš„æ¡†æ¶é‡‡ç”¨æ³¨æ„åŠ›é©±åŠ¨çš„ç‰¹å¾èåˆæœºåˆ¶ï¼Œå°†åˆæˆå­”å¾„é›·è¾¾ï¼ˆSARï¼‰çš„äº’è¡¥ç»“æ„ä¿¡æ¯ä¸å…‰å­¦æ•°æ®çš„å…‰è°±ç‰¹å¾è¿›è¡Œå¯¹é½ã€‚æ­¤å¤–ï¼Œäº‘æ„ŸçŸ¥æ¨¡å‹æ›´æ–°ç­–ç•¥å¼•å…¥äº†è‡ªé€‚åº”æŸå¤±æƒé‡ï¼Œä»¥ä¼˜å…ˆå¤„ç†äº‘é®æŒ¡åŒºåŸŸï¼Œæé«˜é‡å»ºç²¾åº¦ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œè¾¾åˆ°å³°å€¼ä¿¡å™ªæ¯”ï¼ˆPSNRï¼‰31.01åˆ†è´ï¼Œç»“æ„ç›¸ä¼¼æ€§ï¼ˆSSIMï¼‰0.918ï¼Œå¹³å‡ç»å¯¹è¯¯å·®ï¼ˆMAEï¼‰0.017ã€‚è¿™äº›ç»“æœçªå‡ºäº†è¯¥æ¡†æ¶åœ¨ç”Ÿæˆé«˜ä¿çœŸã€ç©ºé—´å’Œå…‰è°±ä¸€è‡´çš„æ— äº‘å…‰å­¦å›¾åƒæ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.17885v1">PDF</a> </p>
<p><strong>Summary</strong><br>     è¯¥ç ”ç©¶æå‡ºä¸€ç§äº‘æ„ŸçŸ¥é‡å»ºæ¡†æ¶ï¼Œç»“åˆSAR-å…‰å­¦ç‰¹å¾èåˆå’Œæ·±åº¦å­¦ä¹ å›¾åƒé‡å»ºæŠ€æœ¯ï¼Œç”Ÿæˆæ— äº‘å…‰å­¦å½±åƒã€‚è¯¥æ¡†æ¶é‡‡ç”¨æ³¨æ„åŠ›é©±åŠ¨çš„ç‰¹å¾èåˆæœºåˆ¶ï¼Œå°†SARçš„äº’è¡¥ç»“æ„ä¿¡æ¯ä¸å…‰å­¦æ•°æ®çš„å…‰è°±ç‰¹å¾ç›¸ç»“åˆã€‚æ­¤å¤–ï¼Œäº‘æ„ŸçŸ¥æ¨¡å‹æ›´æ–°ç­–ç•¥é€šè¿‡è‡ªé€‚åº”æŸå¤±åŠ æƒæ¥ä¼˜å…ˆå¤„ç†äº‘é®æŒ¡åŒºåŸŸï¼Œæé«˜é‡å»ºç²¾åº¦ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>äº‘æ±¡æŸ“ä¸¥é‡å½±å“å…‰å­¦å«æ˜Ÿå›¾åƒçš„ä½¿ç”¨ï¼Œå½±å“ç¯å¢ƒç›‘æ§ã€ç¾å®³å“åº”å’ŒåœŸåœ°åˆ©ç”¨åˆ†æç­‰å…³é”®åº”ç”¨ã€‚</li>
<li>æå‡ºçš„Cloud-Attentiveé‡å»ºæ¡†æ¶é€šè¿‡ç»“åˆSAR-å…‰å­¦ç‰¹å¾èåˆå’Œæ·±åº¦å­¦ä¹ å›¾åƒé‡å»ºæŠ€æœ¯ï¼Œæœ‰æ•ˆç”Ÿæˆæ— äº‘å…‰å­¦å½±åƒã€‚</li>
<li>æ³¨æ„åŠ›é©±åŠ¨çš„ç‰¹å¾èåˆæœºåˆ¶ç”¨äºç»“åˆSARçš„äº’è¡¥ç»“æ„ä¿¡æ¯å’Œå…‰å­¦æ•°æ®çš„å…‰è°±ç‰¹å¾ã€‚</li>
<li>äº‘æ„ŸçŸ¥æ¨¡å‹æ›´æ–°ç­–ç•¥é€šè¿‡è‡ªé€‚åº”æŸå¤±åŠ æƒä¼˜åŒ–æ¨¡å‹ï¼Œä¼˜å…ˆå¤„ç†äº‘é®æŒ¡åŒºåŸŸï¼Œæé«˜é‡å»ºå‡†ç¡®æ€§ã€‚</li>
<li>å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œè¾¾åˆ°PSNR 31.01 dBï¼ŒSSIM 0.918ï¼ŒMAE 0.017ã€‚</li>
<li>è¿™äº›ç»“æœè¯æ˜äº†æ¡†æ¶åœ¨ç”Ÿæˆé«˜ä¿çœŸã€ç©ºé—´å’Œå…‰è°±ä¸€è‡´çš„æ— äº‘å…‰å­¦å›¾åƒæ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.17885">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-92014c646753152b4d3685d647f9c576.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e46ee12698293583c81f1c2fffe87859.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-ef401500ff1395040acf3fa33daa0af0.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-362fcbaa72fd8cf588995b3671f0c01f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-837818eead8e2d92f2660e3b9f9a3904.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-07093f42051048131abc91138e94a404.jpg" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="3D-Gaussian-Splatting-for-Fine-Detailed-Surface-Reconstruction-in-Large-Scale-Scene"><a href="#3D-Gaussian-Splatting-for-Fine-Detailed-Surface-Reconstruction-in-Large-Scale-Scene" class="headerlink" title="3D Gaussian Splatting for Fine-Detailed Surface Reconstruction in   Large-Scale Scene"></a>3D Gaussian Splatting for Fine-Detailed Surface Reconstruction in   Large-Scale Scene</h2><p><strong>Authors:Shihan Chen, Zhaojin Li, Zeyu Chen, Qingsong Yan, Gaoyang Shen, Ran Duan</strong></p>
<p>Recent developments in 3D Gaussian Splatting have made significant advances in surface reconstruction. However, scaling these methods to large-scale scenes remains challenging due to high computational demands and the complex dynamic appearances typical of outdoor environments. These challenges hinder the application in aerial surveying and autonomous driving. This paper proposes a novel solution to reconstruct large-scale surfaces with fine details, supervised by full-sized images. Firstly, we introduce a coarse-to-fine strategy to reconstruct a coarse model efficiently, followed by adaptive scene partitioning and sub-scene refining from image segments. Additionally, we integrate a decoupling appearance model to capture global appearance variations and a transient mask model to mitigate interference from moving objects. Finally, we expand the multi-view constraint and introduce a single-view regularization for texture-less areas. Our experiments were conducted on the publicly available dataset GauU-Scene V2, which was captured using unmanned aerial vehicles. To the best of our knowledge, our method outperforms existing NeRF-based and Gaussian-based methods, achieving high-fidelity visual results and accurate surface from full-size image optimization. Open-source code will be available on GitHub. </p>
<blockquote>
<p>è¿‘æœŸä¸‰ç»´é«˜æ–¯æ‰©å±•ï¼ˆ3D Gaussian Splattingï¼‰åœ¨è¡¨é¢é‡å»ºæ–¹é¢å–å¾—äº†æ˜¾è‘—è¿›å±•ã€‚ç„¶è€Œï¼Œç”±äºæˆ·å¤–ç¯å¢ƒçš„é«˜è®¡ç®—éœ€æ±‚å’Œå¤æ‚åŠ¨æ€å¤–è§‚ï¼Œå°†è¿™äº›æ–¹æ³•æ‰©å±•åˆ°å¤§è§„æ¨¡åœºæ™¯ä»ç„¶å…·æœ‰æŒ‘æˆ˜æ€§ã€‚è¿™äº›æŒ‘æˆ˜é˜»ç¢äº†å…¶åœ¨èˆªç©ºå‹˜å¯Ÿå’Œè‡ªåŠ¨é©¾é©¶ä¸­çš„åº”ç”¨ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§ç”±å…¨å°ºå¯¸å›¾åƒç›‘ç£çš„é‡å»ºå¤§è§„æ¨¡è¡¨é¢ç»†èŠ‚çš„æ–°è§£å†³æ–¹æ¡ˆã€‚é¦–å…ˆï¼Œæˆ‘ä»¬é‡‡ç”¨ä»ç²—åˆ°ç»†çš„ç­–ç•¥é«˜æ•ˆé‡å»ºç²—ç•¥æ¨¡å‹ï¼Œéšåè¿›è¡Œè‡ªé€‚åº”åœºæ™¯åˆ†å‰²å’Œå›¾åƒæ®µçš„å­åœºæ™¯ç»†åŒ–ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬é›†æˆäº†ä¸€ä¸ªè§£è€¦çš„å¤–è§‚æ¨¡å‹æ¥æ•æ‰å…¨å±€å¤–è§‚å˜åŒ–å’Œä¸€ä¸ªç¬æ€æ©è†œæ¨¡å‹æ¥ç¼“è§£ç§»åŠ¨ç‰©ä½“çš„å¹²æ‰°ã€‚æœ€åï¼Œæˆ‘ä»¬æ‰©å±•äº†å¤šè§†è§’çº¦æŸå¹¶ä¸ºæ— çº¹ç†åŒºåŸŸå¼•å…¥äº†å•è§†è§’æ­£åˆ™åŒ–ã€‚æˆ‘ä»¬åœ¨å…¬å¼€å¯ç”¨çš„GauU-Scene V2æ•°æ®é›†ä¸Šè¿›è¡Œäº†å®éªŒï¼Œè¯¥æ•°æ®é›†æ˜¯é€šè¿‡æ— äººæœºæ•è·çš„ã€‚æ®æˆ‘ä»¬æ‰€çŸ¥ï¼Œæˆ‘ä»¬çš„æ–¹æ³•ä¼˜äºç°æœ‰çš„åŸºäºNeRFå’ŒåŸºäºé«˜æ–¯çš„æ–¹æ³•ï¼Œå®ç°äº†é«˜ä¿çœŸè§†è§‰ç»“æœå’Œé€šè¿‡å…¨å°ºå¯¸å›¾åƒä¼˜åŒ–å¾—åˆ°çš„ç²¾ç¡®è¡¨é¢ã€‚å¼€æºä»£ç å°†åœ¨GitHubä¸Šæä¾›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.17636v1">PDF</a> IROS 2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºä¸€ç§åŸºäºå…¨å°ºå¯¸å›¾åƒç›‘ç£çš„å¤§è§„æ¨¡åœºæ™¯è¡¨é¢é‡å»ºæ–°æ–¹æ³•ï¼Œé‡‡ç”¨ä»ç²—åˆ°ç»†çš„é‡å»ºç­–ç•¥ï¼Œé€šè¿‡è‡ªé€‚åº”åœºæ™¯åˆ†å‰²å’Œå­åœºæ™¯ç»†åŒ–æŠ€æœ¯ï¼Œç»“åˆè§£è€¦å¤–è§‚æ¨¡å‹å’Œç¬æ€æ©æ¨¡æ¨¡å‹ï¼Œå®ç°é«˜ä¿çœŸè§†è§‰æ•ˆæœå’Œç²¾ç¡®çš„è¡¨é¢é‡å»ºã€‚è¯¥æ–¹æ³•åœ¨å…¬å¼€æ•°æ®é›†GauU-Scene V2ä¸Šè¿›è¡Œäº†å®éªŒéªŒè¯ï¼Œè¡¨ç°å‡ºä¼˜å¼‚æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¼•å…¥äº†ä¸€ç§åŸºäºå…¨å°ºå¯¸å›¾åƒç›‘ç£çš„å¤§è§„æ¨¡åœºæ™¯è¡¨é¢é‡å»ºæ–¹æ³•ã€‚</li>
<li>é‡‡ç”¨ä»ç²—åˆ°ç»†çš„é‡å»ºç­–ç•¥ï¼Œå…ˆæ„å»ºç²—ç•¥æ¨¡å‹ï¼Œå†è¿›è¡Œç»†åŒ–ã€‚</li>
<li>é€šè¿‡è‡ªé€‚åº”åœºæ™¯åˆ†å‰²å’Œå­åœºæ™¯ç»†åŒ–æŠ€æœ¯ï¼Œæé«˜é‡å»ºç²¾åº¦ã€‚</li>
<li>ç»“åˆè§£è€¦å¤–è§‚æ¨¡å‹å’Œç¬æ€æ©æ¨¡æ¨¡å‹ï¼Œæ•æ‰å…¨å±€å¤–è§‚å˜åŒ–å’Œå‡è½»ç§»åŠ¨ç‰©ä½“çš„å¹²æ‰°ã€‚</li>
<li>æ‹“å±•äº†å¤šè§†è§’çº¦æŸï¼Œå¼•å…¥äº†å•è§†è§’æ­£åˆ™åŒ–ï¼Œç”¨äºå¤„ç†çº¹ç†ç¼ºå¤±åŒºåŸŸã€‚</li>
<li>åœ¨å…¬å¼€æ•°æ®é›†GauU-Scene V2ä¸Šè¿›è¡Œäº†å®éªŒéªŒè¯ï¼Œè¯¥æ–¹æ³•ä¼˜äºç°æœ‰çš„NeRFå’ŒGaussianæ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.17636">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-917d91332db2f4f889f2b653ab3fdaa4.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-281b760caaf47a0df09c8ae2a3256d5e.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-51ecf064cf59084f294efe5d69b55db7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-bec757883c541dfa018962c7a9b610f0.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-fe541c80d2da629920a802aebb8ca679.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3d9045b2c3dc1e9e80afdb668c6a9b41.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-4eac93e27e3554d76e18e7836c6895f7.jpg" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="DRIMV-TSK-An-Interpretable-Surgical-Evaluation-Model-for-Incomplete-Multi-View-Rectal-Cancer-Data"><a href="#DRIMV-TSK-An-Interpretable-Surgical-Evaluation-Model-for-Incomplete-Multi-View-Rectal-Cancer-Data" class="headerlink" title="DRIMV_TSK: An Interpretable Surgical Evaluation Model for Incomplete   Multi-View Rectal Cancer Data"></a>DRIMV_TSK: An Interpretable Surgical Evaluation Model for Incomplete   Multi-View Rectal Cancer Data</h2><p><strong>Authors:Wei Zhang, Zi Wang, Hanwen Zhou, Zhaohong Deng, Weiping Ding, Yuxi Ge, Te Zhang, Yuanpeng Zhang, Kup-Sze Choi, Shitong Wang, Shudong Hu</strong></p>
<p>A reliable evaluation of surgical difficulty can improve the success of the treatment for rectal cancer and the current evaluation method is based on clinical data. However, more data about rectal cancer can be collected with the development of technology. Meanwhile, with the development of artificial intelligence, its application in rectal cancer treatment is becoming possible. In this paper, a multi-view rectal cancer dataset is first constructed to give a more comprehensive view of patients, including the high-resolution MRI image view, pressed-fat MRI image view, and clinical data view. Then, an interpretable incomplete multi-view surgical evaluation model is proposed, considering that it is hard to obtain extensive and complete patient data in real application scenarios. Specifically, a dual representation incomplete multi-view learning model is first proposed to extract the common information between views and specific information in each view. In this model, the missing view imputation is integrated into representation learning, and second-order similarity constraint is also introduced to improve the cooperative learning between these two parts. Then, based on the imputed multi-view data and the learned dual representation, a multi-view surgical evaluation model with the TSK fuzzy system is proposed. In the proposed model, a cooperative learning mechanism is constructed to explore the consistent information between views, and Shannon entropy is also introduced to adapt the view weight. On the MVRC dataset, we compared it with several advanced algorithms and DRIMV_TSK obtained the best results. </p>
<blockquote>
<p>å¯¹æ‰‹æœ¯éš¾åº¦çš„å¯é è¯„ä¼°å¯ä»¥æé«˜ç›´è‚ ç™Œæ²»ç–—æˆåŠŸç‡ï¼Œç›®å‰çš„è¯„ä¼°æ–¹æ³•ä¸»è¦åŸºäºä¸´åºŠæ•°æ®ã€‚ç„¶è€Œï¼Œéšç€ç§‘æŠ€çš„å‘å±•ï¼Œå¯ä»¥æ”¶é›†åˆ°æ›´å¤šå…³äºç›´è‚ ç™Œçš„æ•°æ®ã€‚åŒæ—¶ï¼Œéšç€äººå·¥æ™ºèƒ½çš„å‘å±•ï¼Œå…¶åœ¨ç›´è‚ ç™Œæ²»ç–—ä¸­çš„åº”ç”¨ä¹Ÿæˆä¸ºå¯èƒ½ã€‚æœ¬æ–‡é¦–å…ˆæ„å»ºäº†ä¸€ä¸ªå¤šè§†è§’ç›´è‚ ç™Œæ•°æ®é›†ï¼Œä¸ºæ‚£è€…æä¾›æ›´å…¨é¢çš„è§†è§’ï¼ŒåŒ…æ‹¬é«˜åˆ†è¾¨ç‡MRIå›¾åƒè§†è§’ã€å—å‹è„‚è‚ªMRIå›¾åƒè§†è§’å’Œä¸´åºŠæ•°æ®è§†è§’ã€‚ç„¶åï¼Œæå‡ºäº†ä¸€ç§å¯è§£é‡Šçš„ã€ä¸å®Œå…¨çš„å¤šè§†è§’æ‰‹æœ¯è¯„ä¼°æ¨¡å‹ï¼Œè€ƒè™‘åˆ°åœ¨å®é™…åº”ç”¨åœºæ™¯ä¸­å¾ˆéš¾è·å¾—å…¨é¢å®Œæ•´çš„ç—…äººæ•°æ®ã€‚å…·ä½“æ¥è¯´ï¼Œé¦–å…ˆæå‡ºäº†ä¸€ç§åŒé‡è¡¨ç¤ºçš„ä¸å®Œå…¨å¤šè§†è§’å­¦ä¹ æ¨¡å‹ï¼Œä»¥æå–å„è§†è§’é—´çš„å…±åŒä¿¡æ¯å’Œæ¯ä¸ªè§†è§’çš„ç‰¹å®šä¿¡æ¯ã€‚åœ¨è¯¥æ¨¡å‹ä¸­ï¼Œå°†ç¼ºå¤±è§†è§’çš„å¡«è¡¥æ•´åˆåˆ°è¡¨ç¤ºå­¦ä¹ ä¸­ï¼Œå¹¶å¼•å…¥äº†äºŒé˜¶ç›¸ä¼¼æ€§çº¦æŸï¼Œä»¥æé«˜è¿™ä¸¤éƒ¨åˆ†çš„ååŒå­¦ä¹ ã€‚ç„¶åï¼ŒåŸºäºè¡¥é½çš„å¤šè§†è§’æ•°æ®å’Œå­¦åˆ°çš„åŒé‡è¡¨ç¤ºï¼Œç»“åˆTSKæ¨¡ç³Šç³»ç»Ÿï¼Œæå‡ºäº†ä¸€ç§å¤šè§†è§’æ‰‹æœ¯è¯„ä¼°æ¨¡å‹ã€‚åœ¨è¯¥æ¨¡å‹ä¸­ï¼Œæ„å»ºäº†ä¸€ç§ååŒå­¦ä¹ æœºåˆ¶æ¥æ¢ç´¢å„è§†è§’é—´çš„å…±åŒä¿¡æ¯ï¼Œå¹¶å¼•å…¥äº†é¦™å†œç†µæ¥é€‚åº”è§†è§’æƒé‡ã€‚åœ¨MVRCæ•°æ®é›†ä¸Šï¼Œä¸å‡ ç§å…ˆè¿›ç®—æ³•ç›¸æ¯”ï¼ŒDRIMV_TSKå–å¾—äº†æœ€ä½³ç»“æœã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.17552v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>è¯¥æ–‡æœ¬ä¸»è¦æ„å»ºäº†ä¸€ä¸ªå¤šè§†è§’çš„ç›´è‚ ç™Œæ•°æ®é›†ï¼Œå¹¶åŸºäºè¯¥æ•°æ®é›†æå‡ºäº†ä¸€ç§å¯è§£é‡Šçš„ä¸å®Œå…¨å¤šè§†è§’æ‰‹æœ¯è¯„ä¼°æ¨¡å‹ã€‚é€šè¿‡æ•´åˆç¼ºå¤±è§†è§’çš„æ’è¡¥å’Œè¡¨ç¤ºå­¦ä¹ ï¼Œè¯¥æ¨¡å‹èƒ½å¤Ÿæå–ä¸åŒè§†è§’é—´çš„å…±åŒä¿¡æ¯å’Œç‰¹å®šä¿¡æ¯ã€‚å¼•å…¥äºŒé˜¶ç›¸ä¼¼åº¦çº¦æŸä»¥æ”¹å–„è¿™ä¸¤éƒ¨åˆ†ä¹‹é—´çš„ååŒå­¦ä¹ ã€‚å¹¶ç»“åˆæ’è¡¥çš„å¤šè§†è§’æ•°æ®å’Œå­¦ä¹ çš„åŒé‡è¡¨ç¤ºï¼Œæå‡ºäº†ä¸€ç§åŸºäºTSKæ¨¡ç³Šç³»ç»Ÿçš„å¤šè§†è§’æ‰‹æœ¯è¯„ä¼°æ¨¡å‹ã€‚è¯¥æ¨¡å‹é€šè¿‡åˆä½œæœºåˆ¶æ¢ç´¢è§†è§’é—´çš„ä¸€è‡´æ€§ä¿¡æ¯ï¼Œå¹¶å¼•å…¥é¦™å†œç†µæ¥é€‚åº”è§†è§’æƒé‡ã€‚åœ¨MVRCæ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼ŒDRIMV_TSKè·å¾—äº†æœ€ä½³ç»“æœã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ–‡æœ¬æå‡ºäº†æ„å»ºå¤šè§†è§’çš„ç›´è‚ ç™Œæ•°æ®é›†çš„é‡è¦æ€§ï¼Œæ—¨åœ¨æä¾›æ›´å…¨é¢çš„æ‚£è€…ä¿¡æ¯ï¼ŒåŒ…æ‹¬é«˜åˆ†è¾¨ç‡MRIå›¾åƒã€å—å‹è„‚è‚ªMRIå›¾åƒå’Œä¸´åºŠæ•°æ®ã€‚</li>
<li>é’ˆå¯¹å®é™…åº”ç”¨åœºæ™¯ä¸­æ‚£è€…æ•°æ®ä¸å…¨é¢å’Œç¼ºå¤±çš„é—®é¢˜ï¼Œæ–‡æœ¬æå‡ºäº†ä¸€ç§å¯è§£é‡Šçš„ä¸å®Œå…¨å¤šè§†è§’æ‰‹æœ¯è¯„ä¼°æ¨¡å‹ã€‚</li>
<li>è¯¥æ¨¡å‹ç»“åˆäº†ç¼ºå¤±è§†è§’æ’è¡¥å’Œè¡¨ç¤ºå­¦ä¹ ï¼Œä»¥æå–ä¸åŒè§†è§’é—´çš„å…±åŒä¿¡æ¯å’Œç‰¹å®šä¿¡æ¯ã€‚</li>
<li>ä¸ºäº†æé«˜æ¨¡å‹æ€§èƒ½ï¼Œå¼•å…¥äº†äºŒé˜¶ç›¸ä¼¼åº¦çº¦æŸï¼Œæ”¹å–„äº†ä¸åŒéƒ¨åˆ†ä¹‹é—´çš„ååŒå­¦ä¹ ã€‚</li>
<li>ç»“åˆæ’è¡¥çš„å¤šè§†è§’æ•°æ®å’Œå­¦ä¹ çš„åŒé‡è¡¨ç¤ºï¼Œæ–‡æœ¬è¿˜æå‡ºäº†ä¸€ç§åŸºäºTSKæ¨¡ç³Šç³»ç»Ÿçš„å¤šè§†è§’æ‰‹æœ¯è¯„ä¼°æ¨¡å‹ã€‚</li>
<li>è¯¥æ¨¡å‹é€šè¿‡åˆä½œæœºåˆ¶æ¢ç´¢è§†è§’é—´çš„ä¸€è‡´æ€§ä¿¡æ¯ï¼Œå¢å¼ºäº†æ¨¡å‹çš„å†³ç­–èƒ½åŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.17552">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-6b19ca514bf540b2dbd3295b7445f5e3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e49e1dba934200adea2cdb9eedf13f54.jpg" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1><h2 id="Exploring-Strategies-for-Personalized-Radiation-Therapy-Part-II-Predicting-Tumor-Drift-Patterns-with-Diffusion-Models"><a href="#Exploring-Strategies-for-Personalized-Radiation-Therapy-Part-II-Predicting-Tumor-Drift-Patterns-with-Diffusion-Models" class="headerlink" title="Exploring Strategies for Personalized Radiation Therapy Part II   Predicting Tumor Drift Patterns with Diffusion Models"></a>Exploring Strategies for Personalized Radiation Therapy Part II   Predicting Tumor Drift Patterns with Diffusion Models</h2><p><strong>Authors:Hao Peng, Steve Jiang, Robert Timmerman</strong></p>
<p>Radiation therapy outcomes are decided by two key parameters, dose and timing, whose best values vary substantially across patients. This variability is especially critical in the treatment of brain cancer, where fractionated or staged stereotactic radiosurgery improves safety compared to single fraction approaches, but complicates the ability to predict treatment response. To address this challenge, we employ Personalized Ultra-fractionated Stereotactic Adaptive Radiotherapy (PULSAR), a strategy that dynamically adjusts treatment based on how each tumor evolves over time. However, the success of PULSAR and other adaptive approaches depends on predictive tools that can guide early treatment decisions and avoid both overtreatment and undertreatment. However, current radiomics and dosiomics models offer limited insight into the evolving spatial and temporal patterns of tumor response. To overcome these limitations, we propose a novel framework using Denoising Diffusion Implicit Models (DDIM), which learns data-driven mappings from pre to post treatment imaging. In this study, we developed single step and iterative denoising strategies and compared their performance. The results show that diffusion models can effectively simulate patient specific tumor evolution and localize regions associated with treatment response. The proposed strategy provides a promising foundation for modeling heterogeneous treatment response and enabling early, adaptive interventions, paving the way toward more personalized and biologically informed radiotherapy. </p>
<blockquote>
<p>æ”¾ç–—æ•ˆæœç”±å‰‚é‡å’Œæ—¶é—´è¿™ä¸¤ä¸ªå…³é”®å‚æ•°å†³å®šï¼Œè€Œæœ€ä½³å€¼åœ¨ä¸åŒæ‚£è€…ä¹‹é—´æœ‰å¾ˆå¤§çš„å·®å¼‚ã€‚è¿™ç§å·®å¼‚åœ¨è„‘ç™Œçš„æ²»ç–—ä¸­å°¤å…¶å…³é”®ï¼Œåˆ†æ®µæˆ–åˆ†æœŸç«‹ä½“å®šå‘æ”¾å°„æ‰‹æœ¯ä¸å•æ¬¡æ‰‹æœ¯ç›¸æ¯”æé«˜äº†å®‰å…¨æ€§ï¼Œä½†å¢åŠ äº†é¢„æµ‹æ²»ç–—ååº”çš„èƒ½åŠ›çš„éš¾åº¦ã€‚ä¸ºäº†åº”å¯¹è¿™ä¸€æŒ‘æˆ˜ï¼Œæˆ‘ä»¬é‡‡ç”¨äº†ä¸ªæ€§åŒ–è¶…åˆ†æ®µç«‹ä½“å®šå‘è‡ªé€‚åº”æ”¾ç–—ï¼ˆPULSARï¼‰ç­–ç•¥ï¼Œè¯¥ç­–ç•¥æ ¹æ®æ¯ä¸ªè‚¿ç˜¤çš„éšæ—¶é—´æ¼”å˜æƒ…å†µåŠ¨æ€è°ƒæ•´æ²»ç–—ã€‚ç„¶è€Œï¼ŒPULSARå’Œå…¶ä»–è‡ªé€‚åº”æ–¹æ³•çš„æˆåŠŸå–å†³äºèƒ½å¤ŸæŒ‡å¯¼æ—©æœŸæ²»ç–—å†³ç­–å¹¶é¿å…è¿‡åº¦æ²»ç–—å’Œä¸è¶³æ²»ç–—çš„é¢„æµ‹å·¥å…·ã€‚ç„¶è€Œï¼Œå½“å‰çš„æ”¾å°„å­¦ç»„å’Œå‰‚é‡å­¦æ¨¡å‹å¯¹äºè‚¿ç˜¤ååº”çš„ç©ºé—´å’Œæ—¶é—´æ¨¡å¼çš„æ¼”å˜æä¾›äº†æœ‰é™çš„è§è§£ã€‚ä¸ºäº†å…‹æœè¿™äº›é™åˆ¶ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ä½¿ç”¨å»å™ªæ‰©æ•£éšå¼æ¨¡å‹ï¼ˆDDIMï¼‰çš„æ–°æ¡†æ¶ï¼Œè¯¥æ¨¡å‹å¯ä»¥ä»æ²»ç–—å‰åˆ°æ²»ç–—åçš„æˆåƒä¸­å­¦ä¹ æ•°æ®é©±åŠ¨æ˜ å°„ã€‚åœ¨è¿™é¡¹ç ”ç©¶ä¸­ï¼Œæˆ‘ä»¬å¼€å‘äº†å•æ­¥å’Œè¿­ä»£å»å™ªç­–ç•¥å¹¶æ¯”è¾ƒäº†å®ƒä»¬çš„æ€§èƒ½ã€‚ç»“æœè¡¨æ˜ï¼Œæ‰©æ•£æ¨¡å‹å¯ä»¥æœ‰æ•ˆåœ°æ¨¡æ‹Ÿæ‚£è€…ç‰¹å®šçš„è‚¿ç˜¤æ¼”å˜ï¼Œå¹¶å®šä½ä¸æ²»ç–—ååº”ç›¸å…³çš„åŒºåŸŸã€‚æ‰€æå‡ºçš„ç­–ç•¥ä¸ºæ¨¡æ‹Ÿå¼‚è´¨æ²»ç–—ååº”å’Œæ—©æœŸè‡ªé€‚åº”å¹²é¢„æä¾›äº†æœ‰å‰é€”çš„åŸºç¡€ï¼Œä¸ºæ›´ä¸ªæ€§åŒ–å’Œç”Ÿç‰©å­¦ä¿¡æ¯é©±åŠ¨çš„æ”¾å°„æ²»ç–—é“ºå¹³äº†é“è·¯ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.17491v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ¢è®¨äº†æ”¾å°„æ²»ç–—ç»“æœç”±å‰‚é‡å’Œæ—¶é—´ä¸¤ä¸ªå…³é”®å‚æ•°å†³å®šï¼Œä¸åŒæ‚£è€…æœ€ä½³å€¼å·®å¼‚è¾ƒå¤§ã€‚åœ¨æ²»ç–—è„‘ç™Œæ—¶ï¼Œåˆ†æ¬¡æˆ–åˆ†æœŸç«‹ä½“å®šå‘æ”¾å°„æ‰‹æœ¯ç›¸è¾ƒäºå•æ¬¡æ”¾å°„æ²»ç–—æ›´ä¸ºå®‰å…¨ï¼Œä½†é¢„æµ‹æ²»ç–—ååº”çš„èƒ½åŠ›å´å¤æ‚åŒ–ã€‚ä¸ºåº”å¯¹è¿™ä¸€æŒ‘æˆ˜ï¼Œæœ¬æ–‡é‡‡ç”¨ä¸ªæ€§åŒ–è¶…åˆ†æ¬¡ç«‹ä½“å®šå‘è‡ªé€‚åº”æ”¾å°„æ²»ç–—ï¼ˆPULSARï¼‰ç­–ç•¥ï¼Œæ ¹æ®è‚¿ç˜¤éšæ—¶é—´çš„å˜åŒ–åŠ¨æ€è°ƒæ•´æ²»ç–—ã€‚ä½†PULSARç­‰è‡ªé€‚åº”ç­–ç•¥çš„æˆåŠŸå–å†³äºèƒ½å¤ŸæŒ‡å¯¼æ—©æœŸæ²»ç–—å†³ç­–å¹¶é¿å…è¿‡åº¦æˆ–ä¸è¶³æ²»ç–—çš„é¢„æµ‹å·¥å…·ã€‚å½“å‰æ”¾å°„å­¦å’Œå‰‚é‡å­¦æ¨¡å‹å¯¹è‚¿ç˜¤ååº”çš„ç©ºé—´å’Œæ—¶é—´æ¨¡å¼æ¼”å˜äº†è§£æœ‰é™ã€‚æœ¬æ–‡æå‡ºä¸€ç§æ–°å‹æ¡†æ¶ï¼Œé‡‡ç”¨å»å™ªæ‰©æ•£éšæ¨¡å‹ï¼ˆDDIMï¼‰ï¼Œä»æ²»ç–—å‰åˆ°æ²»ç–—åçš„æˆåƒå­¦ä¹ æ•°æ®é©±åŠ¨æ˜ å°„ã€‚æœ¬ç ”ç©¶å¼€å‘äº†å•æ­¥å’Œè¿­ä»£å»å™ªç­–ç•¥ï¼Œå¹¶æ¯”è¾ƒäº†å®ƒä»¬çš„æ€§èƒ½ã€‚ç»“æœè¡¨æ˜ï¼Œæ‰©æ•£æ¨¡å‹å¯æœ‰æ•ˆæ¨¡æ‹Ÿæ‚£è€…ç‰¹å®šçš„è‚¿ç˜¤æ¼”å˜ï¼Œå¹¶å®šä½ä¸æ²»ç–—ååº”ç›¸å…³çš„åŒºåŸŸã€‚æ‰€æç­–ç•¥ä¸ºæ¨¡æ‹Ÿå¼‚è´¨æ€§æ²»ç–—ååº”å’Œæ—©æœŸè‡ªé€‚åº”å¹²é¢„æä¾›äº†æœ‰å‰é€”çš„åŸºç¡€ï¼Œä¸ºæ›´ä¸ªæ€§åŒ–å’Œç”Ÿç‰©å­¦ä¿¡æ¯é©±åŠ¨çš„æ”¾å°„æ²»ç–—é“ºå¹³äº†é“è·¯ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ”¾å°„æ²»ç–—ç»“æœå—å‰‚é‡å’Œæ—¶é—´çš„åŒé‡å½±å“ï¼Œä¸åŒæ‚£è€…çš„æœ€ä½³å€¼å·®å¼‚æ˜¾è‘—ã€‚</li>
<li>åœ¨è„‘ç™Œæ²»ç–—ä¸­ï¼Œåˆ†æ¬¡æˆ–åˆ†æœŸç«‹ä½“å®šå‘æ”¾å°„æ‰‹æœ¯èƒ½æé«˜å®‰å…¨æ€§ï¼Œä½†é¢„æµ‹æ²»ç–—ååº”çš„éš¾åº¦å¢åŠ ã€‚</li>
<li>PULSARç­–ç•¥èƒ½æ ¹æ®è‚¿ç˜¤å˜åŒ–åŠ¨æ€è°ƒæ•´æ²»ç–—ï¼Œä½†æˆåŠŸä¾èµ–äºé¢„æµ‹å·¥å…·ã€‚</li>
<li>å½“å‰æ”¾å°„å­¦å’Œå‰‚é‡å­¦æ¨¡å‹å¯¹è‚¿ç˜¤ååº”çš„ç©ºé—´å’Œæ—¶é—´æ¨¡å¼æ¼”å˜äº†è§£æœ‰é™ã€‚</li>
<li>æ–°å‹å»å™ªæ‰©æ•£éšæ¨¡å‹ï¼ˆDDIMï¼‰æ¡†æ¶è¢«æå‡ºï¼Œä»¥ä»æ²»ç–—å‰åˆ°æ²»ç–—åçš„æˆåƒä¸­å­¦ä¹ æ•°æ®é©±åŠ¨æ˜ å°„ã€‚</li>
<li>æ‰©æ•£æ¨¡å‹èƒ½æœ‰æ•ˆæ¨¡æ‹Ÿæ‚£è€…ç‰¹å®šè‚¿ç˜¤æ¼”å˜ï¼Œå¹¶å®šä½ä¸æ²»ç–—ååº”ç›¸å…³åŒºåŸŸã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.17491">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-baa2422c58392beb183525123b6c48ea.jpg" align="middle">
</details>


<h1 id="-19"><a href="#-19" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-06-25/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">https://kedreamix.github.io/Talk2Paper/Paper/2025-06-25/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                                    <span class="chip bg-color">åŒ»å­¦å›¾åƒ</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-06-25/TTS/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-b030629238cfa23ed174c5da4e1f06be.jpg" class="responsive-img" alt="TTS">
                        
                        <span class="card-title">TTS</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            TTS æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-06-25  Selecting N-lowest scores for training MOS prediction models
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-06-25
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/TTS/" class="post-category">
                                    TTS
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/TTS/">
                        <span class="chip bg-color">TTS</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-06-25/Diffusion%20Models/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-cd51a0c4d9d03867bc364d3da271f7b6.jpg" class="responsive-img" alt="Diffusion Models">
                        
                        <span class="card-title">Diffusion Models</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Diffusion Models æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-06-25  DIP Unsupervised Dense In-Context Post-training of Visual   Representations
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-06-25
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Diffusion-Models/" class="post-category">
                                    Diffusion Models
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Diffusion-Models/">
                        <span class="chip bg-color">Diffusion Models</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">32714.1k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
