<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="NeRF">
    <meta name="description" content="NeRF æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-06-25  MCN-SLAM Multi-Agent Collaborative Neural SLAM with Hybrid Implicit   Neural Scene Representation">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>NeRF | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pic1.zhimg.com/v2-56ca28009d5260476c56072194780a5b.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">NeRF</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/NeRF/">
                                <span class="chip bg-color">NeRF</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/NeRF/" class="post-category">
                                NeRF
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-06-25
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-07-06
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    12.1k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    49 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-06-25-æ›´æ–°"><a href="#2025-06-25-æ›´æ–°" class="headerlink" title="2025-06-25 æ›´æ–°"></a>2025-06-25 æ›´æ–°</h1><h2 id="MCN-SLAM-Multi-Agent-Collaborative-Neural-SLAM-with-Hybrid-Implicit-Neural-Scene-Representation"><a href="#MCN-SLAM-Multi-Agent-Collaborative-Neural-SLAM-with-Hybrid-Implicit-Neural-Scene-Representation" class="headerlink" title="MCN-SLAM: Multi-Agent Collaborative Neural SLAM with Hybrid Implicit   Neural Scene Representation"></a>MCN-SLAM: Multi-Agent Collaborative Neural SLAM with Hybrid Implicit   Neural Scene Representation</h2><p><strong>Authors:Tianchen Deng, Guole Shen, Xun Chen, Shenghai Yuan, Hongming Shen, Guohao Peng, Zhenyu Wu, Jingchuan Wang, Lihua Xie, Danwei Wang, Hesheng Wang, Weidong Chen</strong></p>
<p>Neural implicit scene representations have recently shown promising results in dense visual SLAM. However, existing implicit SLAM algorithms are constrained to single-agent scenarios, and fall difficulties in large-scale scenes and long sequences. Existing NeRF-based multi-agent SLAM frameworks cannot meet the constraints of communication bandwidth. To this end, we propose the first distributed multi-agent collaborative neural SLAM framework with hybrid scene representation, distributed camera tracking, intra-to-inter loop closure, and online distillation for multiple submap fusion. A novel triplane-grid joint scene representation method is proposed to improve scene reconstruction. A novel intra-to-inter loop closure method is designed to achieve local (single-agent) and global (multi-agent) consistency. We also design a novel online distillation method to fuse the information of different submaps to achieve global consistency. Furthermore, to the best of our knowledge, there is no real-world dataset for NeRF-based&#x2F;GS-based SLAM that provides both continuous-time trajectories groundtruth and high-accuracy 3D meshes groundtruth. To this end, we propose the first real-world Dense slam (DES) dataset covering both single-agent and multi-agent scenarios, ranging from small rooms to large-scale outdoor scenes, with high-accuracy ground truth for both 3D mesh and continuous-time camera trajectory. This dataset can advance the development of the research in both SLAM, 3D reconstruction, and visual foundation model. Experiments on various datasets demonstrate the superiority of the proposed method in both mapping, tracking, and communication. The dataset and code will open-source on <a target="_blank" rel="noopener" href="https://github.com/dtc111111/mcnslam">https://github.com/dtc111111/mcnslam</a>. </p>
<blockquote>
<p>ç¥ç»éšå¼åœºæ™¯è¡¨ç¤ºåœ¨å¯†é›†è§†è§‰SLAMä¸­æœ€è¿‘æ˜¾ç¤ºå‡ºæœ‰å‰é€”çš„ç»“æœã€‚ç„¶è€Œï¼Œç°æœ‰çš„éšå¼SLAMç®—æ³•ä»…é™äºå•æ™ºèƒ½ä½“åœºæ™¯ï¼Œåœ¨å¤§åœºæ™¯å’Œé•¿åºåˆ—ä¸­é¢ä¸´å›°éš¾ã€‚åŸºäºNeRFçš„å¤šæ™ºèƒ½ä½“SLAMæ¡†æ¶æ— æ³•æ»¡è¶³é€šä¿¡å¸¦å®½çš„é™åˆ¶ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†ç¬¬ä¸€ä¸ªåˆ†å¸ƒå¼å¤šæ™ºèƒ½ä½“ååŒç¥ç»SLAMæ¡†æ¶ï¼Œå…·æœ‰æ··åˆåœºæ™¯è¡¨ç¤ºã€åˆ†å¸ƒå¼ç›¸æœºè·Ÿè¸ªã€å†…å¤–ç¯é—­åˆä»¥åŠç”¨äºå¤šä¸ªå­å›¾èåˆçš„åœ¨çº¿è’¸é¦ã€‚æå‡ºäº†ä¸€ç§æ–°å‹çš„ä¸‰å¹³é¢ç½‘æ ¼è”åˆåœºæ™¯è¡¨ç¤ºæ–¹æ³•ï¼Œä»¥æ”¹è¿›åœºæ™¯é‡å»ºã€‚è®¾è®¡äº†ä¸€ç§æ–°å‹çš„å†…å¤–ç¯é—­åˆæ–¹æ³•ï¼Œä»¥å®ç°å±€éƒ¨ï¼ˆå•æ™ºèƒ½ä½“ï¼‰å’Œå…¨å±€ï¼ˆå¤šæ™ºèƒ½ä½“ï¼‰çš„ä¸€è‡´æ€§ã€‚æˆ‘ä»¬è¿˜è®¾è®¡äº†ä¸€ç§æ–°çš„åœ¨çº¿è’¸é¦æ–¹æ³•ï¼Œä»¥èåˆä¸åŒå­å›¾çš„ä¿¡æ¯ï¼Œå®ç°å…¨å±€ä¸€è‡´æ€§ã€‚æ­¤å¤–ï¼Œæ®æˆ‘ä»¬æ‰€çŸ¥ï¼Œæ²¡æœ‰åŸºäºNeRFæˆ–GSçš„SLAMçœŸå®ä¸–ç•Œæ•°æ®é›†èƒ½å¤Ÿæä¾›è¿ç»­æ—¶é—´è½¨è¿¹çš„é«˜ç²¾åº¦3Dç½‘æ ¼åœ°é¢çœŸå®æƒ…å†µã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†ç¬¬ä¸€ä¸ªçœŸå®ä¸–ç•Œçš„Dense slamï¼ˆDESï¼‰æ•°æ®é›†ï¼Œæ¶µç›–å•æ™ºèƒ½ä½“å’Œå¤šæ™ºèƒ½ä½“åœºæ™¯ï¼Œä»å°æˆ¿é—´åˆ°å¤§å‹å®¤å¤–åœºæ™¯ï¼ŒåŒæ—¶æä¾›3Dç½‘æ ¼å’Œè¿ç»­æ—¶é—´ç›¸æœºè½¨è¿¹çš„é«˜ç²¾åº¦åœ°é¢çœŸå®æƒ…å†µã€‚è¯¥æ•°æ®é›†å¯ä»¥ä¿ƒè¿›SLAMã€3Dé‡å»ºå’Œè§†è§‰åŸºç¡€æ¨¡å‹çš„ç ”ç©¶å‘å±•ã€‚åœ¨å„ç§æ•°æ®é›†ä¸Šçš„å®éªŒè¯æ˜äº†æ‰€æå‡ºæ–¹æ³•åœ¨æ˜ å°„ã€è·Ÿè¸ªå’Œé€šä¿¡æ–¹é¢çš„ä¼˜è¶Šæ€§ã€‚æ•°æ®é›†å’Œä»£ç å°†åœ¨<a target="_blank" rel="noopener" href="https://github.com/dtc111111/mcnslam">https://github.com/dtc111111/mcnslam</a>ä¸Šå¼€æºã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.18678v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºé¦–ä¸ªåˆ†å¸ƒå¼å¤šæ™ºèƒ½ä½“ååŒç¥ç»ç½‘ç»œSLAMæ¡†æ¶ï¼Œè§£å†³ç°æœ‰éšå¼SLAMç®—æ³•åœ¨å¤§å‹åœºæ™¯å’Œé•¿åºåˆ—ä¸­çš„å›°éš¾ï¼Œä»¥åŠNeRFåŸºå¤šæ™ºèƒ½ä½“SLAMæ¡†æ¶é€šä¿¡å¸¦å®½çš„é™åˆ¶ã€‚æå‡ºæ–°å‹åœºæ™¯è¡¨ç¤ºæ–¹æ³•ã€åˆ†å¸ƒå¼ç›¸æœºè·Ÿè¸ªã€ç¯è·¯é—­åˆæŠ€æœ¯åŠåœ¨çº¿è’¸é¦èåˆå¤šå­å›¾ä¿¡æ¯ã€‚åŒæ—¶ï¼Œå‘å¸ƒé¦–ä¸ªçœŸå®ä¸–ç•ŒDense slamï¼ˆDESï¼‰æ•°æ®é›†ï¼Œæ¨åŠ¨SLAMã€3Dé‡å»ºå’Œè§†è§‰åŸºç¡€æ¨¡å‹çš„å‘å±•ã€‚å®éªŒè¯æ˜è¯¥æ–¹æ³•å’Œæ•°æ®é›†åœ¨æ˜ å°„ã€è¿½è¸ªå’Œé€šä¿¡æ–¹é¢çš„ä¼˜è¶Šæ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æå‡ºé¦–ä¸ªåˆ†å¸ƒå¼å¤šæ™ºèƒ½ä½“ååŒç¥ç»ç½‘ç»œSLAMæ¡†æ¶ï¼Œé€‚ç”¨äºå¤§å‹åœºæ™¯å’Œé•¿åºåˆ—ã€‚</li>
<li>å¼•å…¥æ–°å‹åœºæ™¯è¡¨ç¤ºæ–¹æ³•ï¼Œå¦‚triplane-gridè”åˆåœºæ™¯è¡¨ç¤ºæ³•ï¼Œæ”¹å–„åœºæ™¯é‡å»ºã€‚</li>
<li>è®¾è®¡åˆ†å¸ƒå¼ç›¸æœºè·Ÿè¸ªã€ç¯è·¯é—­åˆæ–¹æ³•ï¼Œå®ç°å±€éƒ¨å’Œå…¨å±€ä¸€è‡´æ€§ã€‚</li>
<li>åˆ›æ–°åœ¨çº¿è’¸é¦æ–¹æ³•èåˆä¸åŒå­å›¾ä¿¡æ¯ï¼Œä¿éšœå…¨å±€ä¸€è‡´æ€§ã€‚</li>
<li>å…¬å¼€é¦–ä¸ªçœŸå®ä¸–ç•Œçš„Dense slamï¼ˆDESï¼‰æ•°æ®é›†ï¼ŒåŒ…å«å•æ™ºèƒ½ä½“å’Œå¤šæ™ºèƒ½ä½“åœºæ™¯ï¼Œæä¾›é«˜ç²¾åº¦3Dç½‘æ ¼å’Œè¿ç»­æ—¶é—´ç›¸æœºè½¨è¿¹çš„åœ°é¢çœŸå®æ•°æ®ã€‚</li>
<li>è¯¥æ•°æ®é›†æ¨åŠ¨SLAMã€3Dé‡å»ºå’Œè§†è§‰åŸºç¡€æ¨¡å‹çš„ç ”ç©¶å‘å±•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.18678">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-52a3e1d1cc6e0225e987d15c27bb62e7.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-56ca28009d5260476c56072194780a5b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-695d1a91a966a1f33935ee410979fcae.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e62f2974cdf37c29051ef447e844c4db.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="2D-Triangle-Splatting-for-Direct-Differentiable-Mesh-Training"><a href="#2D-Triangle-Splatting-for-Direct-Differentiable-Mesh-Training" class="headerlink" title="2D Triangle Splatting for Direct Differentiable Mesh Training"></a>2D Triangle Splatting for Direct Differentiable Mesh Training</h2><p><strong>Authors:Kaifeng Sheng, Zheng Zhou, Yingliang Peng, Qianwei Wang</strong></p>
<p>Differentiable rendering with 3D Gaussian primitives has emerged as a powerful method for reconstructing high-fidelity 3D scenes from multi-view images. While it offers improvements over NeRF-based methods, this representation still encounters challenges with rendering speed and advanced rendering effects, such as relighting and shadow rendering, compared to mesh-based models. In this paper, we propose 2D Triangle Splatting (2DTS), a novel method that replaces 3D Gaussian primitives with 2D triangle facelets. This representation naturally forms a discrete mesh-like structure while retaining the benefits of continuous volumetric modeling. By incorporating a compactness parameter into the triangle primitives, we enable direct training of photorealistic meshes. Our experimental results demonstrate that our triangle-based method, in its vanilla version (without compactness tuning), achieves higher fidelity compared to state-of-the-art Gaussian-based methods. Furthermore, our approach produces reconstructed meshes with superior visual quality compared to existing mesh reconstruction methods. </p>
<blockquote>
<p>ä½¿ç”¨3Dé«˜æ–¯åŸå§‹æ•°æ®çš„å¯å¾®æ¸²æŸ“å·²ç»æˆä¸ºä¸€ç§å¼ºå¤§çš„æ–¹æ³•ï¼Œå¯ä»¥ä»å¤šè§†è§’å›¾åƒé‡å»ºé«˜ä¿çœŸåº¦çš„3Dåœºæ™¯ã€‚è™½ç„¶ç›¸è¾ƒäºåŸºäºNeRFçš„æ–¹æ³•æœ‰æ‰€æå‡ï¼Œä½†è¿™ç§è¡¨ç¤ºå½¢å¼åœ¨æ¸²æŸ“é€Ÿåº¦å’Œé«˜çº§æ¸²æŸ“æ•ˆæœï¼ˆå¦‚é‡æ–°ç…§æ˜å’Œé˜´å½±æ¸²æŸ“ï¼‰æ–¹é¢ä»ç„¶é¢ä¸´æŒ‘æˆ˜ï¼Œç‰¹åˆ«æ˜¯åœ¨ä¸åŸºäºç½‘æ ¼çš„æ¨¡å‹æ¯”è¾ƒæ—¶ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†äºŒç»´ä¸‰è§’æ‹¼è´´ï¼ˆ2DTSï¼‰è¿™ç§æ–°å‹æ–¹æ³•ï¼Œå®ƒç”¨äºŒç»´ä¸‰è§’å½¢é¢ç‰‡æ›¿ä»£äº†åŸå§‹çš„3Dé«˜æ–¯æ•°æ®ã€‚è¿™ç§è¡¨ç¤ºå½¢å¼è‡ªç„¶åœ°å½¢æˆäº†ä¸€ä¸ªç¦»æ•£ç½‘æ ¼çŠ¶ç»“æ„ï¼ŒåŒæ—¶ä¿ç•™äº†è¿ç»­ä½“ç§¯å»ºæ¨¡çš„ä¼˜ç‚¹ã€‚é€šè¿‡å°†ç´§å‡‘å‚æ•°èå…¥ä¸‰è§’å½¢åŸå§‹æ•°æ®ä¸­ï¼Œæˆ‘ä»¬å¯ä»¥ç›´æ¥è®­ç»ƒé€¼çœŸçš„ç½‘æ ¼ã€‚æˆ‘ä»¬çš„å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬åŸºäºä¸‰è§’å½¢çš„åŸç”Ÿç‰ˆæœ¬æ–¹æ³•ï¼ˆæ— éœ€ç´§å‡‘åº¦è°ƒæ•´ï¼‰ç›¸è¾ƒäºæœ€å…ˆè¿›çš„åŸºäºé«˜æ–¯çš„æ–¹æ³•è¾¾åˆ°äº†æ›´é«˜çš„ä¿çœŸåº¦ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬çš„æ–¹æ³•äº§ç”Ÿçš„é‡å»ºç½‘æ ¼åœ¨è§†è§‰è´¨é‡ä¸Šç›¸è¾ƒäºç°æœ‰çš„ç½‘æ ¼é‡å»ºæ–¹æ³•æ›´ä¸ºä¼˜è¶Šã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.18575v1">PDF</a> 13 pages, 8 figures</p>
<p><strong>æ‘˜è¦</strong></p>
<p>ä»¥äºŒç»´ä¸‰è§’å‰–åˆ†ï¼ˆ2DTSï¼‰ä¸ºè¡¨ç°æ–¹æ³•çš„å¯å¾®åˆ†æ¸²æŸ“æŠ€æœ¯ï¼Œä¸ºä»å¤šè§†è§’å›¾åƒé‡å»ºé«˜ä¿çœŸä¸‰ç»´åœºæ™¯æä¾›äº†ä¸€ç§æ–°çš„æœ‰åŠ›æ‰‹æ®µã€‚ç›¸è¾ƒäºåŸºäºNeRFçš„æ–¹æ³•ï¼Œæ­¤æ–¹æ³•è™½æœ‰æ‰€æ”¹è¿›ï¼Œä½†åœ¨æ¸²æŸ“é€Ÿåº¦åŠé«˜çº§æ¸²æŸ“æ•ˆæœï¼ˆå¦‚é‡æ–°ç…§æ˜å’Œé˜´å½±æ¸²æŸ“ï¼‰æ–¹é¢ä»é¢ä¸´æŒ‘æˆ˜ã€‚æœ¬æ–‡æå‡ºä¸€ç§æ–°å‹æ–¹æ³•ï¼Œä»¥äºŒç»´ä¸‰è§’é¢ç‰‡æ›¿ä»£ä¸‰ç»´é«˜æ–¯åŸå§‹å•å…ƒã€‚è¿™ç§è¡¨ç°æ–¹æ³•æ—¢èƒ½å½¢æˆç¦»æ•£ç½‘æ ¼ç»“æ„ï¼ŒåŒæ—¶ä¿ç•™è¿ç»­ä½“ç§¯å»ºæ¨¡çš„ä¼˜åŠ¿ã€‚é€šè¿‡å¼•å…¥ç´§å‡‘å‚æ•°ï¼Œæˆ‘ä»¬èƒ½ç›´æ¥è®­ç»ƒå…·æœ‰é€¼çœŸæ„Ÿçš„ç½‘æ ¼æ¨¡å‹ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œåœ¨æœªç»ç´§å‡‘åº¦è°ƒæ•´çš„åŸºç¡€ä¸‰è§’å‰–åˆ†æ–¹æ³•ï¼Œå…¶ä¿çœŸåº¦å·²è¶…è¶Šå½“å‰ä¸»æµçš„é«˜æ–¯åŸºç¡€æ–¹æ³•ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬çš„æ–¹æ³•ç”Ÿæˆçš„é‡å»ºç½‘æ ¼åœ¨è§†è§‰è´¨é‡ä¸Šä¼˜äºç°æœ‰çš„ç½‘æ ¼é‡å»ºæ–¹æ³•ã€‚</p>
<p><strong>è¦ç‚¹æç‚¼</strong></p>
<ol>
<li>è®ºæ–‡æå‡ºäº†äºŒç»´ä¸‰è§’å‰–åˆ†ï¼ˆ2DTSï¼‰æŠ€æœ¯ï¼Œè¯¥æŠ€æœ¯ä½¿ç”¨äºŒç»´ä¸‰è§’é¢ç‰‡æ›¿ä»£ä¸‰ç»´é«˜æ–¯åŸå§‹å•å…ƒè¿›è¡Œå¯å¾®åˆ†æ¸²æŸ“ã€‚</li>
<li>ç›¸è¾ƒäºåŸºäºNeRFçš„æ–¹æ³•ï¼Œè™½ç„¶æœ‰æ‰€æ”¹è¿›ï¼Œä½†åœ¨æ¸²æŸ“é€Ÿåº¦å’Œé«˜çº§æ¸²æŸ“æ•ˆæœæ–¹é¢ä»å­˜åœ¨æŒ‘æˆ˜ã€‚</li>
<li>äºŒç»´ä¸‰è§’å‰–åˆ†æŠ€æœ¯èƒ½å¤Ÿå½¢æˆç¦»æ•£ç½‘æ ¼ç»“æ„ï¼ŒåŒæ—¶ä¿ç•™è¿ç»­ä½“ç§¯å»ºæ¨¡çš„ä¼˜åŠ¿ã€‚</li>
<li>é€šè¿‡å¼•å…¥ç´§å‡‘å‚æ•°ï¼Œè¯¥æ–¹æ³•çš„ä¸‰è§’åŸå§‹å•å…ƒèƒ½ç”¨äºç›´æ¥è®­ç»ƒå…·æœ‰é€¼çœŸæ„Ÿçš„ç½‘æ ¼æ¨¡å‹ã€‚</li>
<li>å®éªŒç»“æœæ˜¾ç¤ºï¼ŒåŸºç¡€ä¸‰è§’å‰–åˆ†æ–¹æ³•åœ¨æœªç»ç´§å‡‘åº¦è°ƒæ•´çš„æƒ…å†µä¸‹å·²è¶…è¶Šä¸»æµçš„é«˜æ–¯åŸºç¡€æ–¹æ³•ã€‚</li>
<li>è¯¥æ–¹æ³•ç”Ÿæˆçš„é‡å»ºç½‘æ ¼åœ¨è§†è§‰è´¨é‡ä¸Šä¼˜äºç°æœ‰çš„ç½‘æ ¼é‡å»ºæ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.18575">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-67c31ca5a958f891df0a70b10d1af191.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3c622d0cf74a34f589b40161f87f0f35.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-edd1485fbf7329d6942e201c5f8d0f62.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ef027bfadb5dc3fb90e68352ae1d44d3.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="GANs-vs-Diffusion-Models-for-virtual-staining-with-the-HER2match-dataset"><a href="#GANs-vs-Diffusion-Models-for-virtual-staining-with-the-HER2match-dataset" class="headerlink" title="GANs vs. Diffusion Models for virtual staining with the HER2match   dataset"></a>GANs vs. Diffusion Models for virtual staining with the HER2match   dataset</h2><p><strong>Authors:Pascal KlÃ¶ckner, JosÃ© Teixeira, Diana Montezuma, Jaime S. Cardoso, Hugo M. Horlings, Sara P. Oliveira</strong></p>
<p>Virtual staining is a promising technique that uses deep generative models to recreate histological stains, providing a faster and more cost-effective alternative to traditional tissue chemical staining. Specifically for H&amp;E-HER2 staining transfer, despite a rising trend in publications, the lack of sufficient public datasets has hindered progress in the topic. Additionally, it is currently unclear which model frameworks perform best for this particular task. In this paper, we introduce the HER2match dataset, the first publicly available dataset with the same breast cancer tissue sections stained with both H&amp;E and HER2. Furthermore, we compare the performance of several Generative Adversarial Networks (GANs) and Diffusion Models (DMs), and implement a novel Brownian Bridge Diffusion Model for H&amp;E-HER2 translation. Our findings indicate that, overall, GANs perform better than DMs, with only the BBDM achieving comparable results. Furthermore, we emphasize the importance of data alignment, as all models trained on HER2match produced vastly improved visuals compared to the widely used consecutive-slide BCI dataset. This research provides a new high-quality dataset ([available upon publication acceptance]), improving both model training and evaluation. In addition, our comparison of frameworks offers valuable guidance for researchers working on the topic. </p>
<blockquote>
<p>è™šæ‹ŸæŸ“è‰²æ˜¯ä¸€ç§æœ‰å‰é€”çš„æŠ€æœ¯ï¼Œå®ƒåˆ©ç”¨æ·±åº¦ç”Ÿæˆæ¨¡å‹é‡æ–°åˆ›å»ºç»„ç»‡æŸ“è‰²ï¼Œä¸ºä¼ ç»Ÿçš„ç»„ç»‡åŒ–å­¦æŸ“è‰²æä¾›äº†æ›´å¿«ã€æ›´ç»æµçš„æ›¿ä»£æ–¹æ¡ˆã€‚å¯¹äºH&amp;E-HER2æŸ“è‰²è½¬ç§»è€Œè¨€ï¼Œå°½ç®¡ç›¸å…³æ–‡çŒ®å‘ˆä¸Šå‡è¶‹åŠ¿ï¼Œä½†ç¼ºä¹è¶³å¤Ÿçš„å…¬å…±æ•°æ®é›†é˜»ç¢äº†è¯¥ä¸»é¢˜çš„ç ”ç©¶è¿›å±•ã€‚æ­¤å¤–ï¼Œç›®å‰å°šä¸æ¸…æ¥šå“ªäº›æ¨¡å‹æ¡†æ¶ç‰¹åˆ«é€‚ç”¨äºè¿™ä¸€ä»»åŠ¡ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†HER2matchæ•°æ®é›†ï¼Œè¿™æ˜¯ç¬¬ä¸€ä¸ªå…¬å¼€å¯ç”¨çš„åŒæ—¶å…·æœ‰H&amp;Eå’ŒHER2æŸ“è‰²çš„ä¹³è…ºç™Œç»„ç»‡åˆ‡ç‰‡çš„å…¬å…±æ•°æ®é›†ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æ¯”è¾ƒäº†å‡ ç§ç”Ÿæˆå¯¹æŠ—ç½‘ç»œï¼ˆGANsï¼‰å’Œæ‰©æ•£æ¨¡å‹ï¼ˆDMsï¼‰çš„æ€§èƒ½ï¼Œå¹¶å®ç°äº†ç”¨äºH&amp;E-HER2ç¿»è¯‘çš„æ–°å‹å¸ƒæœ—æ¡¥æ‰©æ•£æ¨¡å‹ï¼ˆBBDMï¼‰ã€‚æˆ‘ä»¬çš„ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œæ€»ä½“è€Œè¨€ï¼ŒGANsçš„è¡¨ç°ä¼˜äºDMsï¼Œåªæœ‰BBDMå–å¾—äº†ç›¸å½“çš„ç»“æœã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å¼ºè°ƒäº†æ•°æ®å¯¹é½çš„é‡è¦æ€§ï¼Œå› ä¸ºåœ¨HER2matchæ•°æ®é›†ä¸Šè®­ç»ƒçš„æ‰€æœ‰æ¨¡å‹äº§ç”Ÿçš„è§†è§‰æ•ˆæœéƒ½å¤§å¤§ä¼˜äºå¹¿æ³›ä½¿ç”¨çš„è¿ç»­åˆ‡ç‰‡BCIæ•°æ®é›†ã€‚è¯¥ç ”ç©¶æä¾›äº†ä¸€ä¸ªæ–°çš„é«˜è´¨é‡æ•°æ®é›†ï¼ˆåœ¨è®ºæ–‡è¢«æ¥å—åå³å¯è·å¾—ï¼‰ï¼Œæé«˜äº†æ¨¡å‹è®­ç»ƒå’Œè¯„ä¼°çš„æ•ˆæœã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å¯¹æ¡†æ¶çš„æ¯”è¾ƒä¸ºä»äº‹è¿™ä¸€é¢†åŸŸçš„ç ”ç©¶äººå‘˜æä¾›äº†å®è´µçš„æŒ‡å¯¼ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.18484v1">PDF</a> </p>
<p><strong>Summary</strong><br>     è¯¥è®ºæ–‡æå‡ºå¹¶å¼•å…¥äº†HER2matchæ•°æ®é›†ï¼Œå®ƒæ˜¯ç¬¬ä¸€ä¸ªå…¬å¼€å¯ç”¨çš„ã€åŒæ—¶å«æœ‰H&amp;Eå’ŒHER2æŸ“è‰²çš„ä¹³è…ºç™Œç»„ç»‡åˆ‡ç‰‡æ•°æ®é›†ã€‚è®ºæ–‡æ¯”è¾ƒäº†ç”Ÿæˆå¯¹æŠ—ç½‘ç»œï¼ˆGANsï¼‰å’Œæ‰©æ•£æ¨¡å‹ï¼ˆDMsï¼‰çš„æ€§èƒ½ï¼Œå¹¶å®ç°äº†é’ˆå¯¹H&amp;E-HER2ç¿»è¯‘çš„å¸ƒæœ—æ¡¥æ‰©æ•£æ¨¡å‹ï¼ˆBBDMï¼‰ã€‚ç ”ç©¶ç»“æœæ˜¾ç¤ºï¼ŒGANsæ•´ä½“è¡¨ç°ä¼˜äºDMsï¼Œå…¶ä¸­BBDMå–å¾—äº†ç›¸å½“çš„ç»“æœã€‚æ­¤å¤–ï¼Œè®ºæ–‡å¼ºè°ƒäº†æ•°æ®å¯¹é½çš„é‡è¦æ€§ï¼Œæ‰€æœ‰åœ¨HER2matchä¸Šè®­ç»ƒçš„æ¨¡å‹éƒ½æ¯”å¹¿æ³›ä½¿ç”¨çš„è¿ç»­åˆ‡ç‰‡BCIæ•°æ®é›†äº§ç”Ÿäº†æ›´å¥½çš„è§†è§‰æ•ˆæœã€‚è¯¥ç ”ç©¶æä¾›äº†é«˜è´¨é‡çš„æ–°æ•°æ®é›†ï¼Œæ”¹è¿›äº†æ¨¡å‹è®­ç»ƒå’Œè¯„ä¼°ï¼Œå¹¶ä¸ºç›¸å…³ç ”ç©¶æä¾›äº†æœ‰ä»·å€¼çš„æŒ‡å¯¼ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è™šæ‹ŸæŸ“è‰²æŠ€æœ¯åˆ©ç”¨æ·±åº¦ç”Ÿæˆæ¨¡å‹é‡æ–°åˆ›å»ºç»„ç»‡åŒ–å­¦æŸ“è‰²çš„è¿‡ç¨‹ï¼Œä¸ºæ›´å¿«ã€æ›´ç»æµçš„æŸ“è‰²æ–¹å¼å¸¦æ¥å¸Œæœ›ã€‚</li>
<li>å¯¹äºH&amp;E-HER2æŸ“è‰²è½¬ç§»ä»»åŠ¡ï¼Œç¼ºä¹å…¬å…±æ•°æ®é›†é™åˆ¶äº†ç ”ç©¶è¿›å±•ã€‚</li>
<li>è®ºæ–‡é¦–æ¬¡å¼•å…¥äº†HER2matchæ•°æ®é›†ï¼ŒåŒ…å«ç›¸åŒçš„ä¹³è…ºç™Œç»„ç»‡åˆ‡ç‰‡H&amp;Eå’ŒHER2æŸ“è‰²ç»“æœã€‚</li>
<li>æ¯”è¾ƒäº†å¤šç§ç”Ÿæˆå¯¹æŠ—ç½‘ç»œï¼ˆGANsï¼‰å’Œæ‰©æ•£æ¨¡å‹ï¼ˆDMsï¼‰çš„æ€§èƒ½ã€‚</li>
<li>æ–°æå‡ºçš„å¸ƒæœ—æ¡¥æ‰©æ•£æ¨¡å‹ï¼ˆBBDMï¼‰åœ¨H&amp;E-HER2ç¿»è¯‘ä»»åŠ¡ä¸Šå–å¾—äº†è‰¯å¥½æ•ˆæœã€‚</li>
<li>GANsåœ¨æ•´ä½“æ€§èƒ½ä¸Šä¼˜äºDMsï¼Œå…¶ä¸­BBDMè¡¨ç°å°¤ä¸ºçªå‡ºã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.18484">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-927b299321ead6c71a4d3ed4b629e3ba.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-251c6eea13f4cd23303d821dfaf0f16b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d7a133b61358f7900cad1a6035bb428c.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Transforming-H-E-images-into-IHC-A-Variance-Penalized-GAN-for-Precision-Oncology"><a href="#Transforming-H-E-images-into-IHC-A-Variance-Penalized-GAN-for-Precision-Oncology" class="headerlink" title="Transforming H&amp;E images into IHC: A Variance-Penalized GAN for Precision   Oncology"></a>Transforming H&amp;E images into IHC: A Variance-Penalized GAN for Precision   Oncology</h2><p><strong>Authors:Sara Rehmat, Hafeez Ur Rehman</strong></p>
<p>The overexpression of the human epidermal growth factor receptor 2 (HER2) in breast cells is a key driver of HER2-positive breast cancer, a highly aggressive subtype requiring precise diagnosis and targeted therapy. Immunohistochemistry (IHC) is the standard technique for HER2 assessment but is costly, labor-intensive, and highly dependent on antibody selection. In contrast, hematoxylin and eosin (H&amp;E) staining, a routine histopathological procedure, offers broader accessibility but lacks HER2 specificity. This study proposes an advanced deep learning-based image translation framework to generate highfidelity IHC images from H&amp;E-stained tissue samples, enabling cost-effective and scalable HER2 assessment. By modifying the loss function of pyramid pix2pix, we mitigate mode collapse, a fundamental limitation in generative adversarial networks (GANs), and introduce a novel variance-based penalty that enforces structural diversity in generated images. Our model particularly excels in translating HER2-positive (IHC 3+) images, which have remained challenging for existing methods due to their complex morphological variations. Extensive evaluations on the BCI histopathological dataset demonstrate that our model surpasses state-of-the-art methods in terms of peak signal-tonoise ratio (PSNR), structural similarity index (SSIM), and Frechet Inception Distance (FID), particularly in accurately translating HER2-positive (IHC 3+) images. Beyond medical imaging, our model exhibits superior performance in general image-to-image translation tasks, showcasing its potential across multiple domains. This work marks a significant step toward AI-driven precision oncology, offering a reliable and efficient alternative to traditional HER2 diagnostics. </p>
<blockquote>
<p>äººç±»è¡¨çš®ç”Ÿé•¿å› å­å—ä½“2ï¼ˆHER2ï¼‰åœ¨ä¹³è…ºç»†èƒä¸­çš„è¿‡åº¦è¡¨è¾¾æ˜¯HER2é˜³æ€§ä¹³è…ºç™Œçš„å…³é”®é©±åŠ¨å› ç´ ï¼Œè¿™æ˜¯ä¸€ç§é«˜åº¦ä¾µè¢­æ€§çš„äºšå‹ï¼Œéœ€è¦ç²¾ç¡®è¯Šæ–­å’Œæ²»ç–—ã€‚å…ç–«ç»„ç»‡åŒ–å­¦ï¼ˆIHCï¼‰æ˜¯è¯„ä¼°HER2çš„æ ‡å‡†æŠ€æœ¯ï¼Œä½†æˆæœ¬é«˜æ˜‚ã€åŠ³åŠ¨å¼ºåº¦å¤§ï¼Œä¸”é«˜åº¦ä¾èµ–äºæŠ—ä½“é€‰æ‹©ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼Œè‹æœ¨ç²¾å’Œä¼Šçº¢ï¼ˆH&amp;Eï¼‰æŸ“è‰²æ˜¯ä¸€ç§å¸¸è§„çš„ç—…ç†ç»„ç»‡å­¦ç¨‹åºï¼Œå…·æœ‰æ›´å¹¿æ³›çš„å¯åŠæ€§ï¼Œä½†ç¼ºä¹HER2ç‰¹å¼‚æ€§ã€‚æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§å…ˆè¿›çš„åŸºäºæ·±åº¦å­¦ä¹ çš„å›¾åƒç¿»è¯‘æ¡†æ¶ï¼Œèƒ½å¤Ÿä»H&amp;EæŸ“è‰²ç»„ç»‡æ ·æœ¬ç”Ÿæˆé«˜ä¿çœŸIHCå›¾åƒï¼Œä»è€Œå®ç°ç»æµé«˜æ•ˆçš„HER2è¯„ä¼°ã€‚é€šè¿‡ä¿®æ”¹é‡‘å­—å¡”pix2pixçš„æŸå¤±å‡½æ•°ï¼Œæˆ‘ä»¬å‡è½»äº†æ¨¡å¼å´©æºƒè¿™ä¸€ç”Ÿæˆå¯¹æŠ—ç½‘ç»œï¼ˆGANsï¼‰çš„åŸºæœ¬å±€é™æ€§ï¼Œå¹¶å¼•å…¥äº†ä¸€ç§åŸºäºæ–¹å·®çš„æ–°å‹æƒ©ç½šæœºåˆ¶ï¼Œä»¥åœ¨ç”Ÿæˆçš„å›¾åƒä¸­å®æ–½ç»“æ„å¤šæ ·æ€§ã€‚æˆ‘ä»¬çš„æ¨¡å‹åœ¨ç¿»è¯‘HER2é˜³æ€§ï¼ˆIHC 3+ï¼‰å›¾åƒæ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œç”±äºå¤æ‚çš„å½¢æ€å˜åŒ–ï¼Œè¿™äº›å›¾åƒå¯¹äºç°æœ‰æ–¹æ³•æ¥è¯´ä¸€ç›´å…·æœ‰æŒ‘æˆ˜æ€§ã€‚åœ¨BCIç—…ç†æ•°æ®é›†ä¸Šçš„å¹¿æ³›è¯„ä¼°è¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ¨¡å‹åœ¨å³°å€¼ä¿¡å™ªæ¯”ï¼ˆPSNRï¼‰ã€ç»“æ„ç›¸ä¼¼æ€§æŒ‡æ•°ï¼ˆSSIMï¼‰å’Œå¼—é›·æ­‡ç‰¹Inceptionè·ç¦»ï¼ˆFIDï¼‰ç­‰æ–¹é¢è¶…è¿‡äº†æœ€å…ˆè¿›çš„æ–¹æ³•ï¼Œç‰¹åˆ«æ˜¯åœ¨å‡†ç¡®ç¿»è¯‘HER2é˜³æ€§ï¼ˆIHC 3+ï¼‰å›¾åƒæ–¹é¢ã€‚é™¤äº†åŒ»å­¦æˆåƒå¤–ï¼Œæˆ‘ä»¬çš„æ¨¡å‹åœ¨ä¸€èˆ¬çš„å›¾åƒåˆ°å›¾åƒç¿»è¯‘ä»»åŠ¡ä¸­ä¹Ÿè¡¨ç°å‡ºå“è¶Šçš„æ€§èƒ½ï¼Œå±•ç¤ºäº†å…¶åœ¨å¤šä¸ªé¢†åŸŸçš„æ½œåŠ›ã€‚è¿™é¡¹å·¥ä½œæ ‡å¿—ç€äººå·¥æ™ºèƒ½é©±åŠ¨ç²¾å‡†è‚¿ç˜¤å­¦çš„é‡å¤§è¿›å±•ï¼Œä¸ºä¼ ç»Ÿçš„HER2è¯Šæ–­æä¾›äº†å¯é é«˜æ•ˆçš„æ›¿ä»£æ–¹æ¡ˆã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.18371v1">PDF</a> </p>
<p><strong>æ‘˜è¦</strong><br>æœ¬æ–‡å…³æ³¨äººè¡¨çš®ç”Ÿé•¿å› å­å—ä½“2ï¼ˆHER2ï¼‰åœ¨ä¹³è…ºç™Œç»†èƒä¸­çš„è¿‡åº¦è¡¨è¾¾ï¼Œè¿™æ˜¯HER2é˜³æ€§ä¹³è…ºç™Œçš„å…³é”®é©±åŠ¨å› ç´ ã€‚è¯¥ç ”ç©¶æå‡ºäº†ä¸€ç§åŸºäºæ·±åº¦å­¦ä¹ çš„å›¾åƒç¿»è¯‘æ¡†æ¶ï¼Œèƒ½å¤Ÿä»H&amp;EæŸ“è‰²ç»„ç»‡æ ·æœ¬ç”Ÿæˆé«˜ä¿çœŸIHCå›¾åƒï¼Œå®ç°äº†æˆæœ¬æ•ˆç›Šé«˜ä¸”å¯æ‰©å±•çš„HER2è¯„ä¼°ã€‚è¯¥ç ”ç©¶é€šè¿‡æ”¹è¿›é‡‘å­—å¡”pix2pixçš„æŸå¤±å‡½æ•°ï¼Œç¼“è§£äº†ç”Ÿæˆå¯¹æŠ—ç½‘ç»œï¼ˆGANsï¼‰çš„æ ¹æœ¬å±€é™æ€§â€”â€”æ¨¡å¼å´©æºƒï¼Œå¹¶å¼•å…¥äº†ä¸€ç§æ–°å‹æ–¹å·®æƒ©ç½šï¼Œä»¥åŠ å¼ºç”Ÿæˆå›¾åƒçš„ç»“æ„å¤šæ ·æ€§ã€‚åœ¨BCIç—…ç†æ•°æ®é›†ä¸Šçš„å…¨é¢è¯„ä¼°è¡¨æ˜ï¼Œè¯¥ç ”ç©¶æå‡ºçš„æ¨¡å‹åœ¨å³°å€¼ä¿¡å™ªæ¯”ï¼ˆPSNRï¼‰ã€ç»“æ„ç›¸ä¼¼æ€§æŒ‡æ•°ï¼ˆSSIMï¼‰å’ŒFrechet Inception Distanceï¼ˆFIDï¼‰ç­‰æ–¹é¢è¶…è¶Šäº†ç°æœ‰æœ€å…ˆè¿›çš„æ–¹æ³•ï¼Œç‰¹åˆ«æ˜¯åœ¨ç¿»è¯‘HER2é˜³æ€§ï¼ˆIHC 3+ï¼‰å›¾åƒæ–¹é¢ã€‚æ­¤å¤–ï¼Œè¯¥æ¨¡å‹åœ¨ä¸€èˆ¬å›¾åƒåˆ°å›¾åƒçš„ç¿»è¯‘ä»»åŠ¡ä¸­ä¹Ÿè¡¨ç°å‡ºå“è¶Šæ€§èƒ½ï¼Œå±•ç¤ºäº†å…¶åœ¨å¤šä¸ªé¢†åŸŸçš„æ½œåŠ›ã€‚è¿™é¡¹ç ”ç©¶æ˜¯è¿ˆå‘äººå·¥æ™ºèƒ½é©±åŠ¨çš„ç²¾å‡†è‚¿ç˜¤å­¦çš„é‡å¤§ä¸€æ­¥ï¼Œä¸ºä¼ ç»Ÿçš„HER2è¯Šæ–­æä¾›äº†å¯é é«˜æ•ˆçš„æ›¿ä»£æ–¹æ¡ˆã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>HER2åœ¨ä¹³è…ºç™Œç»†èƒä¸­çš„è¿‡åº¦è¡¨è¾¾æ˜¯HER2é˜³æ€§ä¹³è…ºç™Œçš„å…³é”®é©±åŠ¨å› ç´ ï¼Œéœ€è¦ç²¾ç¡®è¯Šæ–­å’Œæ²»ç–—ã€‚</li>
<li>å…ç–«ç»„ç»‡åŒ–å­¦ï¼ˆIHCï¼‰æ˜¯è¯„ä¼°HER2çš„æ ‡å‡†æŠ€æœ¯ï¼Œä½†æˆæœ¬é«˜ã€åŠ³åŠ¨å¯†é›†ä¸”ä¾èµ–äºæŠ—ä½“é€‰æ‹©ã€‚</li>
<li>å¸¸è§„ç—…ç†ç¨‹åºH&amp;EæŸ“è‰²æ–¹æ³•æ›´å¹¿æ³›å¯ç”¨ï¼Œä½†ç¼ºä¹HER2ç‰¹å¼‚æ€§ã€‚</li>
<li>æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§åŸºäºæ·±åº¦å­¦ä¹ çš„å›¾åƒç¿»è¯‘æ¡†æ¶ï¼Œèƒ½å¤Ÿä»H&amp;EæŸ“è‰²ç”Ÿæˆé«˜ä¿çœŸIHCå›¾åƒï¼Œä»¥å®ç°æˆæœ¬æ•ˆç›Šé«˜ä¸”å¯æ‰©å±•çš„HER2è¯„ä¼°ã€‚</li>
<li>é€šè¿‡æ”¹è¿›æŸå¤±å‡½æ•°å’Œå¼•å…¥æ–¹å·®æƒ©ç½šï¼Œè¯¥æ¨¡å‹åœ¨BCIæ•°æ®é›†ä¸Šè¶…è¶Šäº†ç°æœ‰æ–¹æ³•ï¼Œç‰¹åˆ«æ˜¯åœ¨ç¿»è¯‘HER2é˜³æ€§å›¾åƒæ–¹é¢ã€‚</li>
<li>è¯¥æ¨¡å‹åœ¨ä¸€èˆ¬å›¾åƒåˆ°å›¾åƒçš„ç¿»è¯‘ä»»åŠ¡ä¸­ä¹Ÿè¡¨ç°å‡ºå“è¶Šæ€§èƒ½ï¼Œå…·æœ‰è·¨å¤šä¸ªé¢†åŸŸçš„æ½œåŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.18371">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-dc534c2c193337db405b96e28df6bb5c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-fa190c919a0d09a772138943fbefd34b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3448e7287556247c0a78e137dea916c5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c5e00b3ae849ed48cc7eb580e5455981.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-a6eb60938732a39594176002167b54bd.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ee3d00950d84533350ffd3b9f5ccb785.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c04dce8c4c9f5c5ae1b6d3ba245a97f9.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="3D-Gaussian-Splatting-for-Fine-Detailed-Surface-Reconstruction-in-Large-Scale-Scene"><a href="#3D-Gaussian-Splatting-for-Fine-Detailed-Surface-Reconstruction-in-Large-Scale-Scene" class="headerlink" title="3D Gaussian Splatting for Fine-Detailed Surface Reconstruction in   Large-Scale Scene"></a>3D Gaussian Splatting for Fine-Detailed Surface Reconstruction in   Large-Scale Scene</h2><p><strong>Authors:Shihan Chen, Zhaojin Li, Zeyu Chen, Qingsong Yan, Gaoyang Shen, Ran Duan</strong></p>
<p>Recent developments in 3D Gaussian Splatting have made significant advances in surface reconstruction. However, scaling these methods to large-scale scenes remains challenging due to high computational demands and the complex dynamic appearances typical of outdoor environments. These challenges hinder the application in aerial surveying and autonomous driving. This paper proposes a novel solution to reconstruct large-scale surfaces with fine details, supervised by full-sized images. Firstly, we introduce a coarse-to-fine strategy to reconstruct a coarse model efficiently, followed by adaptive scene partitioning and sub-scene refining from image segments. Additionally, we integrate a decoupling appearance model to capture global appearance variations and a transient mask model to mitigate interference from moving objects. Finally, we expand the multi-view constraint and introduce a single-view regularization for texture-less areas. Our experiments were conducted on the publicly available dataset GauU-Scene V2, which was captured using unmanned aerial vehicles. To the best of our knowledge, our method outperforms existing NeRF-based and Gaussian-based methods, achieving high-fidelity visual results and accurate surface from full-size image optimization. Open-source code will be available on GitHub. </p>
<blockquote>
<p>è¿‘æœŸ3Dé«˜æ–¯è´´å›¾æŠ€æœ¯çš„è¿›å±•åœ¨è¡¨é¢é‡å»ºæ–¹é¢å–å¾—äº†é‡å¤§çªç ´ã€‚ç„¶è€Œï¼Œç”±äºé«˜è®¡ç®—éœ€æ±‚å’Œæˆ·å¤–ç¯å¢ƒçš„å¤æ‚åŠ¨æ€å¤–è§‚ï¼Œå°†è¿™äº›æ–¹æ³•æ‰©å±•åˆ°å¤§è§„æ¨¡åœºæ™¯ä»ç„¶å…·æœ‰æŒ‘æˆ˜æ€§ã€‚è¿™äº›æŒ‘æˆ˜é˜»ç¢äº†å…¶åœ¨èˆªç©ºæµ‹é‡å’Œè‡ªåŠ¨é©¾é©¶ä¸­çš„åº”ç”¨ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§ç”±å…¨å°ºå¯¸å›¾åƒç›‘ç£çš„ï¼Œå¸¦æœ‰ç²¾ç»†ç»†èŠ‚çš„å¤§è§„æ¨¡è¡¨é¢é‡å»ºçš„æ–°è§£å†³æ–¹æ¡ˆã€‚é¦–å…ˆï¼Œæˆ‘ä»¬é‡‡ç”¨ä»ç²—åˆ°ç»†çš„ç­–ç•¥é«˜æ•ˆé‡å»ºç²—æ¨¡å‹ï¼Œéšåè¿›è¡Œè‡ªé€‚åº”åœºæ™¯åˆ†å‰²å’Œæ¥è‡ªå›¾åƒæ®µçš„å­åœºæ™¯ç»†åŒ–ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬é›†æˆäº†ä¸€ä¸ªåˆ†ç¦»çš„å¤–è§‚æ¨¡å‹æ¥æ•æ‰å…¨å±€å¤–è§‚å˜åŒ–ï¼Œå¹¶å¼•å…¥äº†ä¸€ä¸ªç¬æ€æ©è†œæ¨¡å‹æ¥å‡è½»ç§»åŠ¨ç‰©ä½“çš„å¹²æ‰°ã€‚æœ€åï¼Œæˆ‘ä»¬æ‰©å±•äº†å¤šè§†è§’çº¦æŸï¼Œå¹¶ä¸ºæ— çº¹ç†åŒºåŸŸå¼•å…¥äº†å•è§†è§’æ­£åˆ™åŒ–ã€‚æˆ‘ä»¬çš„å®éªŒæ˜¯åœ¨å…¬å¼€å¯ç”¨çš„GauU-Scene V2æ•°æ®é›†ä¸Šè¿›è¡Œçš„ï¼Œè¯¥æ•°æ®é›†æ˜¯ä½¿ç”¨æ— äººæœºæ•è·çš„ã€‚æ®æˆ‘ä»¬æ‰€çŸ¥ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨NeRFåŸºå’ŒåŸºäºé«˜æ–¯çš„æ–¹æ³•ä¸­è¡¨ç°æœ€ä½³ï¼Œé€šè¿‡å…¨å°ºå¯¸å›¾åƒä¼˜åŒ–å®ç°äº†é«˜ä¿çœŸè§†è§‰ç»“æœå’Œç²¾ç¡®çš„è¡¨é¢é‡å»ºã€‚å¼€æºä»£ç å°†åœ¨GitHubä¸Šæä¾›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.17636v1">PDF</a> IROS 2025</p>
<p><strong>Summary</strong><br>å¤§è§„æ¨¡åœºæ™¯çš„ä¸‰ç»´é«˜æ–¯èåˆæŠ€æœ¯è™½ç„¶å–å¾—äº†è¿›å±•ï¼Œä½†å…¶åœ¨å¤æ‚åŠ¨æ€å®¤å¤–ç¯å¢ƒä¸­çš„åº”ç”¨ä»å­˜åœ¨è®¡ç®—é‡å¤§å’Œç»†èŠ‚ç¼ºå¤±çš„æŒ‘æˆ˜ã€‚æœ¬æ–‡æå‡ºä¸€ç§æ–°å‹æ–¹æ³•ï¼Œåˆ©ç”¨å…¨å°ºå¯¸å›¾åƒç›‘ç£é‡å»ºå…·æœ‰ç²¾ç»†ç»†èŠ‚çš„å¤§è§„æ¨¡åœºæ™¯è¡¨é¢ã€‚é€šè¿‡ç”±ç²—åˆ°ç»†çš„é‡å»ºç­–ç•¥ï¼Œç»“åˆè‡ªé€‚åº”åœºæ™¯åˆ†å‰²å’Œå­åœºæ™¯ç»†åŒ–ï¼Œä»¥åŠè§£è€¦å¤–è§‚æ¨¡å‹å’Œç¬æ€æ©æ¨¡æ¨¡å‹ï¼Œè¯¥æ–¹æ³•å®ç°äº†é«˜æ•ˆå‡†ç¡®çš„é‡å»ºã€‚åœ¨å…¬å¼€æ•°æ®é›†GauU-Scene V2ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•ä¼˜äºç°æœ‰çš„NeRFå’Œé«˜æ–¯æ–¹æ³•ï¼Œå®ç°äº†é«˜ä¿çœŸè§†è§‰ç»“æœå’Œä»å…¨å°ºå¯¸å›¾åƒä¼˜åŒ–çš„å‡†ç¡®è¡¨é¢é‡å»ºã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§è§„æ¨¡åœºæ™¯çš„ä¸‰ç»´é‡å»ºåœ¨å¤æ‚åŠ¨æ€å®¤å¤–ç¯å¢ƒä¸­å­˜åœ¨è®¡ç®—é‡å¤§å’Œç»†èŠ‚ç¼ºå¤±çš„æŒ‘æˆ˜ã€‚</li>
<li>æå‡ºäº†ä¸€ç§æ–°çš„é‡å»ºæ–¹æ³•ï¼Œé€šè¿‡å…¨å°ºå¯¸å›¾åƒç›‘ç£å®ç°å¤§è§„æ¨¡åœºæ™¯çš„ç²¾ç»†ç»†èŠ‚é‡å»ºã€‚</li>
<li>é‡‡ç”¨ç”±ç²—åˆ°ç»†çš„é‡å»ºç­–ç•¥ï¼Œé¦–å…ˆæ„å»ºç²—ç•¥æ¨¡å‹ï¼Œå†è¿›è¡Œç»†åŒ–ã€‚</li>
<li>åˆ©ç”¨è‡ªé€‚åº”åœºæ™¯åˆ†å‰²å’Œå­åœºæ™¯ç»†åŒ–æŠ€æœ¯ï¼Œæé«˜é‡å»ºæ•ˆç‡ã€‚</li>
<li>å¼•å…¥è§£è€¦å¤–è§‚æ¨¡å‹å’Œç¬æ€æ©æ¨¡æ¨¡å‹ï¼Œä»¥æ•æ‰å…¨å±€å¤–è§‚å˜åŒ–å’Œå‡è½»ç§»åŠ¨ç‰©ä½“çš„å¹²æ‰°ã€‚</li>
<li>æ‰©å±•äº†å¤šè§†è§’çº¦æŸï¼Œå¹¶ä¸ºæ— çº¹ç†åŒºåŸŸå¼•å…¥äº†å•è§†è§’æ­£åˆ™åŒ–ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.17636">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-917d91332db2f4f889f2b653ab3fdaa4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-281b760caaf47a0df09c8ae2a3256d5e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-51ecf064cf59084f294efe5d69b55db7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-bec757883c541dfa018962c7a9b610f0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-fe541c80d2da629920a802aebb8ca679.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3d9045b2c3dc1e9e80afdb668c6a9b41.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4eac93e27e3554d76e18e7836c6895f7.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="MTSIC-Multi-stage-Transformer-based-GAN-for-Spectral-Infrared-Image-Colorization"><a href="#MTSIC-Multi-stage-Transformer-based-GAN-for-Spectral-Infrared-Image-Colorization" class="headerlink" title="MTSIC: Multi-stage Transformer-based GAN for Spectral Infrared Image   Colorization"></a>MTSIC: Multi-stage Transformer-based GAN for Spectral Infrared Image   Colorization</h2><p><strong>Authors:Tingting Liu, Yuan Liu, Jinhui Tang, Liyin Yuan, Chengyu Liu, Chunlai Li, Xiubao Sui, Qian Chen</strong></p>
<p>Thermal infrared (TIR) images, acquired through thermal radiation imaging, are unaffected by variations in lighting conditions and atmospheric haze. However, TIR images inherently lack color and texture information, limiting downstream tasks and potentially causing visual fatigue. Existing colorization methods primarily rely on single-band images with limited spectral information and insufficient feature extraction capabilities, which often result in image distortion and semantic ambiguity. In contrast, multiband infrared imagery provides richer spectral data, facilitating the preservation of finer details and enhancing semantic accuracy. In this paper, we propose a generative adversarial network (GAN)-based framework designed to integrate spectral information to enhance the colorization of infrared images. The framework employs a multi-stage spectral self-attention Transformer network (MTSIC) as the generator. Each spectral feature is treated as a token for self-attention computation, and a multi-head self-attention mechanism forms a spatial-spectral attention residual block (SARB), achieving multi-band feature mapping and reducing semantic confusion. Multiple SARB units are integrated into a Transformer-based single-stage network (STformer), which uses a U-shaped architecture to extract contextual information, combined with multi-scale wavelet blocks (MSWB) to align semantic information in the spatial-frequency dual domain. Multiple STformer modules are cascaded to form MTSIC, progressively optimizing the reconstruction quality. Experimental results demonstrate that the proposed method significantly outperforms traditional techniques and effectively enhances the visual quality of infrared images. </p>
<blockquote>
<p>çƒ­çº¢å¤–ï¼ˆTIRï¼‰å›¾åƒæ˜¯é€šè¿‡çƒ­è¾å°„æˆåƒè·å¾—çš„ï¼Œä¸å—å…‰ç…§æ¡ä»¶å’Œå¤§æ°”é›¾å½±å“ã€‚ç„¶è€Œï¼Œç”±äºæœ¬èº«ç¼ºä¹è‰²å½©å’Œçº¹ç†ä¿¡æ¯ï¼Œè¿™äº›å›¾åƒä¼šç»™åç»­ä»»åŠ¡å¸¦æ¥æ½œåœ¨é—®é¢˜ï¼Œå¹¶ä¸”å¯èƒ½ä¼šå¯¼è‡´è§†è§‰ç–²åŠ³ã€‚ç°æœ‰çš„å½©è‰²åŒ–æ–¹æ³•ä¸»è¦ä¾èµ–äºå…·æœ‰æœ‰é™å…‰è°±ä¿¡æ¯çš„å•æ³¢æ®µå›¾åƒï¼Œå¹¶ä¸”ç‰¹å¾æå–èƒ½åŠ›ä¸è¶³ä»¥æ»¡è¶³éœ€æ±‚ï¼Œè¿™å¾€å¾€ä¼šå¯¼è‡´å›¾åƒå¤±çœŸå’Œè¯­ä¹‰æ¨¡ç³Šã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼Œå¤šæ³¢æ®µçº¢å¤–å›¾åƒæä¾›äº†æ›´ä¸°å¯Œå…‰è°±æ•°æ®ï¼Œæœ‰åŠ©äºä¿ç•™æ›´ç²¾ç»†çš„ç»†èŠ‚å¹¶æé«˜è¯­ä¹‰å‡†ç¡®æ€§ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºç”Ÿæˆå¯¹æŠ—ç½‘ç»œï¼ˆGANï¼‰çš„æ¡†æ¶ï¼Œæ—¨åœ¨èåˆå…‰è°±ä¿¡æ¯æ¥æé«˜çº¢å¤–å›¾åƒçš„å½©è‰²åŒ–æ•ˆæœã€‚è¯¥æ¡†æ¶é‡‡ç”¨å¤šé˜¶æ®µå…‰è°±è‡ªæ³¨æ„åŠ›Transformerç½‘ç»œï¼ˆMTSICï¼‰ä½œä¸ºç”Ÿæˆå™¨ã€‚å°†æ¯ä¸ªå…‰è°±ç‰¹å¾è§†ä¸ºè‡ªæ³¨æ„åŠ›è®¡ç®—çš„æ ‡è®°ï¼Œåˆ©ç”¨å¤šå¤´è‡ªæ³¨æ„åŠ›æœºåˆ¶æ„å»ºç©ºé—´å…‰è°±æ³¨æ„åŠ›æ®‹å·®å—ï¼ˆSARBï¼‰ï¼Œå®ç°å¤šæ³¢æ®µç‰¹å¾æ˜ å°„å¹¶å‡å°‘è¯­ä¹‰æ··æ·†ã€‚å¤šä¸ªSARBå•å…ƒè¢«é›†æˆåˆ°åŸºäºTransformerçš„å•é˜¶æ®µç½‘ç»œï¼ˆSTformerï¼‰ä¸­ï¼Œè¯¥ç½‘ç»œé‡‡ç”¨Uå‹ç»“æ„æ¥æå–ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼Œå¹¶ç»“åˆå¤šå°ºåº¦å°æ³¢å—ï¼ˆMSWBï¼‰åœ¨ç©ºé—´é¢‘ç‡åŒåŸŸä¸­å¯¹é½è¯­ä¹‰ä¿¡æ¯ã€‚å¤šä¸ªSTformeræ¨¡å—çº§è”å½¢æˆMTSICï¼Œé€æ­¥ä¼˜åŒ–é‡å»ºè´¨é‡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæ‰€æå‡ºçš„æ–¹æ³•åœ¨æ€§èƒ½ä¸Šå¤§å¤§ä¼˜äºä¼ ç»ŸæŠ€æœ¯ï¼Œæœ‰æ•ˆåœ°æé«˜äº†çº¢å¤–å›¾åƒçš„å¯è§†è´¨é‡ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.17540v1">PDF</a> </p>
<p><strong>æ‘˜è¦</strong></p>
<p>æœ¬æ–‡æå‡ºä¸€ç§åŸºäºç”Ÿæˆå¯¹æŠ—ç½‘ç»œï¼ˆGANï¼‰çš„æ¡†æ¶ï¼Œæ—¨åœ¨æ•´åˆå…‰è°±ä¿¡æ¯æå‡çº¢å¤–å›¾åƒçš„å½©è‰²åŒ–ã€‚é‡‡ç”¨å¤šé˜¶æ®µå…‰è°±è‡ªæ³¨æ„åŠ›Transformerç½‘ç»œï¼ˆMTSICï¼‰ä½œä¸ºç”Ÿæˆå™¨ï¼Œå°†æ¯ä¸ªå…‰è°±ç‰¹å¾è§†ä¸ºè‡ªæ³¨æ„åŠ›çš„æ ‡è®°ï¼Œå¹¶é€šè¿‡å¤šå¤´è‡ªæ³¨æ„åŠ›æœºåˆ¶å½¢æˆç©ºé—´å…‰è°±æ³¨æ„åŠ›æ®‹å·®å—ï¼ˆSARBï¼‰ï¼Œå®ç°å¤šé¢‘æ®µç‰¹å¾æ˜ å°„ï¼Œå‡å°‘è¯­ä¹‰æ··æ·†ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•æ˜¾è‘—ä¼˜äºä¼ ç»ŸæŠ€æœ¯ï¼Œæœ‰æ•ˆæé«˜çº¢å¤–å›¾åƒè§†è§‰è´¨é‡ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>çº¢å¤–å›¾åƒåœ¨å¤šç§å…‰ç…§å’Œå¤§æ°”æ¡ä»¶ä¸‹è¡¨ç°ç¨³å®šï¼Œä½†å…¶æœ¬èº«ç¼ºä¹é¢œè‰²å’Œçº¹ç†ä¿¡æ¯ï¼Œé™åˆ¶åç»­ä»»åŠ¡å’Œè§†è§‰æ„Ÿå—ã€‚</li>
<li>ä¼ ç»Ÿé¢œè‰²åŒ–æ–¹æ³•ä¾èµ–å•ä¸€é¢‘å¸¦å›¾åƒï¼Œå­˜åœ¨ä¿¡æ¯æå–èƒ½åŠ›ä¸è¶³çš„é—®é¢˜ï¼Œå¯èƒ½å¯¼è‡´å›¾åƒå¤±çœŸå’Œè¯­ä¹‰æ¨¡ç³Šã€‚</li>
<li>å¤šé¢‘å¸¦çº¢å¤–æˆåƒæä¾›ä¸°å¯Œçš„å…‰è°±æ•°æ®ï¼Œæœ‰åŠ©äºä¿ç•™æ›´ç²¾ç»†çš„ç»†èŠ‚å¹¶å¢å¼ºè¯­ä¹‰å‡†ç¡®æ€§ã€‚</li>
<li>æœ¬æ–‡æå‡ºä¸€ç§åŸºäºGANçš„æ¡†æ¶ï¼Œæ•´åˆå…‰è°±ä¿¡æ¯ä»¥æå‡çº¢å¤–å›¾åƒçš„é¢œè‰²åŒ–è´¨é‡ã€‚</li>
<li>é‡‡ç”¨å¤šå¤´è‡ªæ³¨æ„åŠ›æœºåˆ¶å½¢æˆSARBï¼Œå®ç°å¤šé¢‘æ®µç‰¹å¾æ˜ å°„ï¼Œå‡å°‘è¯­ä¹‰æ··æ·†ã€‚</li>
<li>é€šè¿‡ç»“åˆå¤šä¸ªSTformeræ¨¡å—å½¢æˆMTSICï¼Œé€æ­¥ä¼˜åŒ–é‡å»ºè´¨é‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.17540">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-7a8ddb5e1b48f6533e3c21661fc7547e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-00f6159c96800f743e8de34785413d8f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4a533c47f8fd489cc16f047411f457d6.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-559e0332c4304dec3f76c7ece944d70e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ba635e74065e3b0146f53a5b8a410eee.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="R3eVision-A-Survey-on-Robust-Rendering-Restoration-and-Enhancement-for-3D-Low-Level-Vision"><a href="#R3eVision-A-Survey-on-Robust-Rendering-Restoration-and-Enhancement-for-3D-Low-Level-Vision" class="headerlink" title="R3eVision: A Survey on Robust Rendering, Restoration, and Enhancement   for 3D Low-Level Vision"></a>R3eVision: A Survey on Robust Rendering, Restoration, and Enhancement   for 3D Low-Level Vision</h2><p><strong>Authors:Weeyoung Kwon, Jeahun Sung, Minkyu Jeon, Chanho Eom, Jihyong Oh</strong></p>
<p>Neural rendering methods such as Neural Radiance Fields (NeRF) and 3D Gaussian Splatting (3DGS) have achieved significant progress in photorealistic 3D scene reconstruction and novel view synthesis. However, most existing models assume clean and high-resolution (HR) multi-view inputs, which limits their robustness under real-world degradations such as noise, blur, low-resolution (LR), and weather-induced artifacts. To address these limitations, the emerging field of 3D Low-Level Vision (3D LLV) extends classical 2D Low-Level Vision tasks including super-resolution (SR), deblurring, weather degradation removal, restoration, and enhancement into the 3D spatial domain. This survey, referred to as R\textsuperscript{3}eVision, provides a comprehensive overview of robust rendering, restoration, and enhancement for 3D LLV by formalizing the degradation-aware rendering problem and identifying key challenges related to spatio-temporal consistency and ill-posed optimization. Recent methods that integrate LLV into neural rendering frameworks are categorized to illustrate how they enable high-fidelity 3D reconstruction under adverse conditions. Application domains such as autonomous driving, AR&#x2F;VR, and robotics are also discussed, where reliable 3D perception from degraded inputs is critical. By reviewing representative methods, datasets, and evaluation protocols, this work positions 3D LLV as a fundamental direction for robust 3D content generation and scene-level reconstruction in real-world environments. </p>
<blockquote>
<p>ç¥ç»æ¸²æŸ“æ–¹æ³•ï¼Œä¾‹å¦‚ç¥ç»è¾å°„åœºï¼ˆNeRFï¼‰å’Œ3Dé«˜æ–¯å–·ç»˜ï¼ˆ3DGSï¼‰ï¼Œåœ¨çœŸå®æ„Ÿ3Dåœºæ™¯é‡å»ºå’Œæ–°å‹è§†å›¾åˆæˆæ–¹é¢å–å¾—äº†æ˜¾è‘—è¿›å±•ã€‚ç„¶è€Œï¼Œå¤§å¤šæ•°ç°æœ‰æ¨¡å‹å‡è®¾æ¸…æ™°ä¸”é«˜åˆ†è¾¨ç‡ï¼ˆHRï¼‰çš„å¤šè§†è§’è¾“å…¥ï¼Œè¿™é™åˆ¶äº†å®ƒä»¬åœ¨ç°å®ä¸–ç•Œé€€åŒ–ï¼ˆä¾‹å¦‚å™ªå£°ã€æ¨¡ç³Šã€ä½åˆ†è¾¨ç‡ï¼ˆLRï¼‰å’Œå¤©æ°”å¼•èµ·çš„ä¼ªå½±ï¼‰å½±å“ä¸‹çš„ç¨³å¥æ€§ã€‚ä¸ºäº†è§£å†³è¿™äº›å±€é™æ€§ï¼Œæ–°å…´çš„3Dä½çº§è§†è§‰ï¼ˆ3D LLVï¼‰é¢†åŸŸå°†ä¼ ç»Ÿçš„2Dä½çº§è§†è§‰ä»»åŠ¡ï¼ˆåŒ…æ‹¬è¶…åˆ†è¾¨ç‡ï¼ˆSRï¼‰ã€å»æ¨¡ç³Šã€å»é™¤å¤©æ°”é€€åŒ–ã€æ¢å¤å’Œå¢å¼ºï¼‰æ‰©å±•åˆ°3Dç©ºé—´åŸŸã€‚è¿™ç¯‡ç»¼è¿°è¢«ç§°ä¸ºR\textsuperscript{3}eVisionï¼Œé€šè¿‡å¯¹é€€åŒ–æ„ŸçŸ¥æ¸²æŸ“é—®é¢˜è¿›è¡Œå½¢å¼åŒ–å¹¶ç¡®å®šä¸æ—¶ç©ºä¸€è‡´æ€§å’Œä¸é€‚å®šä¼˜åŒ–ç›¸å…³çš„å…³é”®æŒ‘æˆ˜ï¼Œå…¨é¢æ¦‚è¿°äº†3D LLVçš„ç¨³å¥æ¸²æŸ“ã€æ¢å¤å’Œå¢å¼ºã€‚å°†LLVé›†æˆåˆ°ç¥ç»æ¸²æŸ“æ¡†æ¶ä¸­çš„æœ€è¿‘æ–¹æ³•è¢«åˆ†ç±»ï¼Œä»¥è¯´æ˜å®ƒä»¬åœ¨ä¸åˆ©æ¡ä»¶ä¸‹å®ç°é«˜ä¿çœŸ3Dé‡å»ºçš„èƒ½åŠ›ã€‚è¿˜è®¨è®ºäº†è‡ªåŠ¨é©¾é©¶ã€AR&#x2F;VRå’Œæœºå™¨äººç­‰é¢†åŸŸï¼Œåœ¨è¿™äº›é¢†åŸŸä¸­ï¼Œä»é€€åŒ–è¾“å…¥ä¸­å¯é åœ°è·å–3Dæ„ŸçŸ¥è‡³å…³é‡è¦ã€‚é€šè¿‡å›é¡¾ä»£è¡¨æ€§æ–¹æ³•ã€æ•°æ®é›†å’Œè¯„ä¼°åè®®ï¼Œè¿™é¡¹å·¥ä½œå°†3D LLVå®šä½ä¸ºåœ¨çœŸå®ä¸–ç•Œç¯å¢ƒä¸­è¿›è¡Œç¨³å¥çš„3Då†…å®¹ç”Ÿæˆå’Œåœºæ™¯çº§é‡å»ºçš„åŸºæœ¬æ–¹å‘ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.16262v2">PDF</a> Please visit our project page at   <a target="_blank" rel="noopener" href="https://github.com/CMLab-Korea/Awesome-3D-Low-Level-Vision">https://github.com/CMLab-Korea/Awesome-3D-Low-Level-Vision</a></p>
<p><strong>æ‘˜è¦</strong><br>    NeRFä¸ç›¸å…³æŠ€æœ¯å¦‚3DGSåœ¨ä¸‰ç»´åœºæ™¯é‡å»ºå’Œæ–°é¢–è§†è§’åˆæˆæ–¹é¢å–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œä½†ç°æœ‰æ¨¡å‹å¤§å¤šä¾èµ–å¹²å‡€ã€é«˜æ¸…çš„å¤šè§†è§’è¾“å…¥ï¼Œè¿™åœ¨ç°å®ä¸–ç•Œä¸­å®¹æ˜“å—åˆ°å™ªå£°ã€æ¨¡ç³Šã€ä½åˆ†è¾¨ç‡å’Œå¤©æ°”å½±å“ã€‚ä¸ºè§£å†³è¿™äº›é—®é¢˜ï¼Œ3Dä½å±‚æ¬¡è§†è§‰ï¼ˆLLVï¼‰é¢†åŸŸå°†è¶…åˆ†è¾¨ç‡ã€å»æ¨¡ç³Šç­‰ä»»åŠ¡æ‰©å±•åˆ°ä¸‰ç»´ç©ºé—´åŸŸã€‚è¿™ç¯‡ç»¼è¿°ä»‹ç»äº†é¢å‘æ¶åŠ£ç¯å¢ƒçš„ç¨³å¥æ¸²æŸ“ã€æ¢å¤å’Œå¢å¼ºæŠ€æœ¯ï¼Œå¼ºè°ƒå¯¹æ„ŸçŸ¥é€€åŒ–æ„ŸçŸ¥çš„æ¸²æŸ“é—®é¢˜çš„é‡è§†å¹¶æ€»ç»“äº†ç›¸å…³çš„æŒ‘æˆ˜ã€‚é›†æˆLLVåˆ°ç¥ç»æ¸²æŸ“æ¡†æ¶çš„æ–¹æ³•ä¹Ÿè¢«åˆ†ç±»è®¨è®ºï¼Œå±•ç¤ºäº†å®ƒä»¬åœ¨æ¶åŠ£æ¡ä»¶ä¸‹å®ç°é«˜è´¨é‡ä¸‰ç»´é‡å»ºçš„èƒ½åŠ›ã€‚åº”ç”¨é¢†åŸŸå¦‚è‡ªåŠ¨é©¾é©¶ã€AR&#x2F;VRå’Œæœºå™¨äººæŠ€æœ¯ä¹Ÿå—åˆ°äº†å…³æ³¨ï¼Œå…¶ä¸­ä»é€€åŒ–è¾“å…¥ä¸­è¿›è¡Œå¯é çš„3Dæ„ŸçŸ¥è‡³å…³é‡è¦ã€‚æœ¬æ–‡æ€»ç»“äº†ä»£è¡¨æ€§æ–¹æ³•ã€æ•°æ®é›†å’Œè¯„ä¼°åè®®ï¼Œå°†3D LLVå®šä½ä¸ºç°å®ç¯å¢ƒä¸­é²æ£’ä¸‰ç»´å†…å®¹ç”Ÿæˆå’Œåœºæ™¯çº§é‡å»ºçš„åŸºæœ¬æ–¹å‘ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>NeRFç­‰ç¥ç»æ¸²æŸ“æŠ€æœ¯åœ¨ä¸‰ç»´åœºæ™¯é‡å»ºå’Œæ–°é¢–è§†è§’åˆæˆæ–¹é¢å–å¾—æ˜¾è‘—è¿›å±•ï¼Œä½†å¯¹è¾“å…¥æ•°æ®çš„è´¨é‡å’Œæ•°é‡æœ‰è¾ƒé«˜è¦æ±‚ã€‚</li>
<li>ç°æœ‰æ¨¡å‹åœ¨é¢ä¸´ç°å®ä¸–ç•Œä¸­çš„å™ªå£°ã€æ¨¡ç³Šã€ä½åˆ†è¾¨ç‡å’Œå¤©æ°”å½±å“æ—¶è¡¨ç°å—é™ã€‚</li>
<li>3Dä½å±‚æ¬¡è§†è§‰ï¼ˆLLVï¼‰é¢†åŸŸæ‰©å±•äº†ä¼ ç»Ÿçš„äºŒç»´ä½å±‚æ¬¡è§†è§‰ä»»åŠ¡åˆ°ä¸‰ç»´ç©ºé—´åŸŸï¼Œä»¥è§£å†³ä¸Šè¿°é—®é¢˜ã€‚</li>
<li>R\textsuperscript{3}eVisionç»¼è¿°ä»‹ç»äº†é¢å‘æ¶åŠ£ç¯å¢ƒçš„ç¨³å¥æ¸²æŸ“ã€æ¢å¤å’Œå¢å¼ºæŠ€æœ¯ï¼Œå¹¶å¼ºè°ƒäº†æ„ŸçŸ¥é€€åŒ–æ„ŸçŸ¥çš„æ¸²æŸ“é—®é¢˜çš„ä¸¥é‡æ€§ã€‚</li>
<li>æ­¤é¢†åŸŸé¢ä¸´çš„å…³é”®æŒ‘æˆ˜åŒ…æ‹¬æ—¶ç©ºä¸€è‡´æ€§å’Œç—…æ€ä¼˜åŒ–é—®é¢˜ã€‚</li>
<li>é›†æˆLLVåˆ°ç¥ç»æ¸²æŸ“æ¡†æ¶çš„æ–¹æ³•èƒ½å¤Ÿåœ¨æ¶åŠ£æ¡ä»¶ä¸‹å®ç°é«˜è´¨é‡çš„ä¸‰ç»´é‡å»ºã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.16262">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-7aab4fb76393877bbfeedeb7236a410c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f539f679a475ae8cddb515af1915a28a.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-d98e96726900ad5db41bf549eec201c4.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-9024d639736efa6c12b2da7ef0a6467c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-bd54095a3a0f6c964810393a28b3eeac.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-9fbd4da6357b9b3969feb4164ed3bb70.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Layered-Motion-Fusion-Lifting-Motion-Segmentation-to-3D-in-Egocentric-Videos"><a href="#Layered-Motion-Fusion-Lifting-Motion-Segmentation-to-3D-in-Egocentric-Videos" class="headerlink" title="Layered Motion Fusion: Lifting Motion Segmentation to 3D in Egocentric   Videos"></a>Layered Motion Fusion: Lifting Motion Segmentation to 3D in Egocentric   Videos</h2><p><strong>Authors:Vadim Tschernezki, Diane Larlus, Iro Laina, Andrea Vedaldi</strong></p>
<p>Computer vision is largely based on 2D techniques, with 3D vision still relegated to a relatively narrow subset of applications. However, by building on recent advances in 3D models such as neural radiance fields, some authors have shown that 3D techniques can at last improve outputs extracted from independent 2D views, by fusing them into 3D and denoising them. This is particularly helpful in egocentric videos, where the camera motion is significant, but only under the assumption that the scene itself is static. In fact, as shown in the recent analysis conducted by EPIC Fields, 3D techniques are ineffective when it comes to studying dynamic phenomena, and, in particular, when segmenting moving objects. In this paper, we look into this issue in more detail. First, we propose to improve dynamic segmentation in 3D by fusing motion segmentation predictions from a 2D-based model into layered radiance fields (Layered Motion Fusion). However, the high complexity of long, dynamic videos makes it challenging to capture the underlying geometric structure, and, as a result, hinders the fusion of motion cues into the (incomplete) scene geometry. We address this issue through test-time refinement, which helps the model to focus on specific frames, thereby reducing the data complexity. This results in a synergy between motion fusion and the refinement, and in turn leads to segmentation predictions of the 3D model that surpass the 2D baseline by a large margin. This demonstrates that 3D techniques can enhance 2D analysis even for dynamic phenomena in a challenging and realistic setting. </p>
<blockquote>
<p>è®¡ç®—æœºè§†è§‰åœ¨å¾ˆå¤§ç¨‹åº¦ä¸ŠåŸºäºäºŒç»´æŠ€æœ¯ï¼Œè€Œä¸‰ç»´è§†è§‰ä»ç„¶å±€é™äºç›¸å¯¹è¾ƒå°‘çš„åº”ç”¨é¢†åŸŸã€‚ç„¶è€Œï¼Œä¸€äº›ä½œè€…é€šè¿‡å»ºç«‹åŸºäºç¥ç»ç½‘ç»œè¾å°„åœºç­‰ä¸‰ç»´æ¨¡å‹çš„æœ€æ–°è¿›å±•ï¼Œå±•ç¤ºäº†ä¸‰ç»´æŠ€æœ¯æœ€ç»ˆå¯ä»¥é€šè¿‡èåˆç‹¬ç«‹äºŒç»´è§†å›¾å¹¶å¯¹å…¶è¿›è¡Œå»å™ªï¼Œæ¥æ”¹å–„æå–çš„è¾“å‡ºã€‚è¿™åœ¨ä»¥è‡ªæˆ‘ä¸ºä¸­å¿ƒçš„è§†é¢‘ä¸­ç‰¹åˆ«æœ‰å¸®åŠ©ï¼Œå…¶ä¸­ç›¸æœºè¿åŠ¨æ˜¾è‘—ï¼Œä½†å‡è®¾åœºæ™¯æœ¬èº«æ˜¯é™æ€çš„ã€‚äº‹å®ä¸Šï¼Œæ­£å¦‚EPIC Fieldsçš„æœ€æ–°åˆ†ææ‰€ç¤ºï¼Œå½“æ¶‰åŠåˆ°åŠ¨æ€ç°è±¡ï¼Œç‰¹åˆ«æ˜¯åˆ†å‰²ç§»åŠ¨ç‰©ä½“æ—¶ï¼Œä¸‰ç»´æŠ€æœ¯å¹¶ä¸æœ‰æ•ˆã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬å°†æ›´è¯¦ç»†åœ°æ¢è®¨è¿™ä¸ªé—®é¢˜ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬æå‡ºé€šè¿‡èåˆåŸºäºäºŒç»´æ¨¡å‹çš„åŠ¨æ€åˆ†å‰²é¢„æµ‹ç»“æœåˆ°åˆ†å±‚è¾å°„åœºï¼ˆLayered Motion Fusionï¼‰æ¥æ”¹å–„ä¸‰ç»´ä¸­çš„åŠ¨æ€åˆ†å‰²ã€‚ç„¶è€Œï¼Œé•¿åŠ¨æ€è§†é¢‘çš„å¤æ‚æ€§ä½¿å¾—æ•æ‰å…¶åº•å±‚å‡ ä½•ç»“æ„å…·æœ‰æŒ‘æˆ˜æ€§ï¼Œå¹¶å› æ­¤é˜»ç¢äº†å°†è¿åŠ¨çº¿ç´¢èåˆåˆ°ï¼ˆä¸å®Œæ•´ï¼‰åœºæ™¯å‡ ä½•ä¸­ã€‚æˆ‘ä»¬é€šè¿‡æµ‹è¯•æ—¶æ”¹è¿›æ¥è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œè¿™æœ‰åŠ©äºæ¨¡å‹ä¸“æ³¨äºç‰¹å®šå¸§ï¼Œä»è€Œé™ä½æ•°æ®å¤æ‚æ€§ã€‚è¿™å¯¼è‡´äº†è¿åŠ¨èåˆå’Œç»†åŒ–ä¹‹é—´çš„ååŒä½œç”¨ï¼Œå¹¶è¿›è€Œå®ç°äº†å¯¹ä¸‰ç»´æ¨¡å‹çš„åˆ†å‰²é¢„æµ‹ï¼Œå¤§å¹…è¶…è¶Šäº†äºŒç»´åŸºå‡†æµ‹è¯•ã€‚è¿™è¡¨æ˜ï¼Œå³ä½¿åœ¨å…·æœ‰æŒ‘æˆ˜æ€§å’Œç°å®æ€§çš„è®¾ç½®ä¸­ï¼Œä¸‰ç»´æŠ€æœ¯ä¹Ÿå¯ä»¥å¢å¼ºäºŒç»´åˆ†æï¼Œç”šè‡³ç”¨äºåŠ¨æ€ç°è±¡ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.05546v2">PDF</a> Camera-ready for CVPR25</p>
<p><strong>æ‘˜è¦</strong><br>     æœ¬æ–‡æ¢è®¨äº†è®¡ç®—æœºè§†è§‰é¢†åŸŸä¸­3DæŠ€æœ¯ä¸åŠ¨æ€ç°è±¡åˆ†æçš„é—®é¢˜ã€‚è™½ç„¶è®¡ç®—æœºè§†è§‰ä¸»è¦åŸºäº2DæŠ€æœ¯ï¼Œä½†è¿‘å¹´æ¥åœ¨3Dæ¨¡å‹ä¸Šçš„è¿›å±•è¡¨æ˜ï¼Œå°†ç‹¬ç«‹2Dè§†è§’èåˆåˆ°3Då¹¶è¿›è¡Œå»å™ªå¯ä»¥æé«˜è¾“å‡ºè´¨é‡ã€‚ç‰¹åˆ«æ˜¯åœ¨ä»¥è‡ªæˆ‘ä¸ºä¸­å¿ƒçš„è§†é¢‘ä¸­ï¼Œå½“åœºæ™¯æœ¬èº«é™æ€æ—¶ï¼Œè¿™ç§æŠ€æœ¯å°¤å…¶æœ‰ç”¨ã€‚ç„¶è€Œï¼ŒEPIC Fieldsçš„è¿‘æœŸåˆ†ææ˜¾ç¤ºï¼Œå¯¹äºåŠ¨æ€ç°è±¡çš„ç ”ç©¶ï¼Œå°¤å…¶æ˜¯åˆ†å‰²ç§»åŠ¨ç‰©ä½“æ—¶ï¼Œ3DæŠ€æœ¯å¹¶ä¸æœ‰æ•ˆã€‚æœ¬æ–‡æ—¨åœ¨è§£å†³è¿™ä¸€é—®é¢˜ï¼Œé€šè¿‡æå‡ºå°†åŸºäºäºŒç»´æ¨¡å‹çš„åŠ¨æ€åˆ†å‰²é¢„æµ‹èåˆåˆ°åˆ†å±‚è¾å°„åœºï¼ˆLayered Motion Fusionï¼‰ä¸­æ”¹è¿›åŠ¨æ€åœºæ™¯çš„åˆ†å‰²é—®é¢˜ã€‚ç„¶è€Œï¼Œå¤„ç†é•¿åŠ¨æ€è§†é¢‘çš„é«˜å¤æ‚æ€§ä½¿å…¶éš¾ä»¥æ•æ‰åº•å±‚å‡ ä½•ç»“æ„ï¼Œé˜»ç¢äº†è¿åŠ¨çº¿ç´¢èå…¥ï¼ˆä¸å®Œæ•´ï¼‰åœºæ™¯å‡ ä½•çš„èåˆã€‚æœ¬æ–‡é€šè¿‡æµ‹è¯•æ—¶ä¿®æ­£è§£å†³äº†è¿™ä¸€é—®é¢˜ï¼Œå¸®åŠ©æ¨¡å‹ä¸“æ³¨äºç‰¹å®šå¸§ï¼Œé™ä½æ•°æ®å¤æ‚æ€§ï¼Œå®ç°è¿åŠ¨èåˆä¸ä¿®æ­£ä¹‹é—´çš„ååŒä½œç”¨ï¼Œè¿›è€Œæé«˜ä¸‰ç»´æ¨¡å‹çš„åˆ†å‰²é¢„æµ‹ç»“æœï¼Œå¤§å¹…è¶…è¶ŠäºŒç»´åŸºçº¿ï¼Œè¯æ˜äº†ä¸‰ç»´æŠ€æœ¯å³ä½¿åœ¨æŒ‘æˆ˜æ€§å’Œç°å®åœºæ™¯ä¸­ä¹Ÿèƒ½æé«˜äºŒç»´åˆ†æå¯¹åŠ¨æ€ç°è±¡çš„åˆ†ææ•ˆæœã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>è®¡ç®—æœºè§†è§‰ä¸»è¦ä¾èµ–2DæŠ€æœ¯ï¼Œä½†3DæŠ€æœ¯åœ¨ç‰¹å®šé¢†åŸŸå¦‚è§†é¢‘å¤„ç†ä¸­å±•ç°å‡ºä¼˜åŠ¿ã€‚</li>
<li>èåˆç‹¬ç«‹2Dè§†è§’åˆ°3Då¹¶å»å™ªå¯ä»¥æé«˜è¾“å‡ºè´¨é‡ï¼Œå°¤å…¶åœ¨åœºæ™¯é™æ€æ—¶æ•ˆæœæ˜¾è‘—ã€‚</li>
<li>å¯¹äºåŠ¨æ€ç°è±¡å’Œç§»åŠ¨ç‰©ä½“åˆ†å‰²ï¼Œå°¤å…¶æ˜¯é•¿è§†é¢‘ï¼Œä¼ ç»Ÿçš„3DæŠ€æœ¯å­˜åœ¨å±€é™æ€§ã€‚</li>
<li>æœ¬ç ”ç©¶æ—¨åœ¨é€šè¿‡Layered Motion Fusionæ–¹æ³•æ”¹è¿›åŠ¨æ€åœºæ™¯çš„åˆ†å‰²é—®é¢˜ã€‚</li>
<li>å¤„ç†å¤æ‚åŠ¨æ€è§†é¢‘æ—¶é¢ä¸´åº•å±‚å‡ ä½•ç»“æ„æ•æ‰çš„æŒ‘æˆ˜ã€‚</li>
<li>æµ‹è¯•æ—¶ä¿®æ­£æ–¹æ³•å¸®åŠ©æ¨¡å‹ä¸“æ³¨äºç‰¹å®šå¸§ï¼Œæé«˜è¿åŠ¨èåˆä¸ä¿®æ­£ä¹‹é—´çš„ååŒä½œç”¨ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.05546">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-3b90f44019d88deb3a74cf238ad29567.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e0b9d56cd7384029bf46fee6024b2b9d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c75524e314100eceed629639500fefc8.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="PhysicsNeRF-Physics-Guided-3D-Reconstruction-from-Sparse-Views"><a href="#PhysicsNeRF-Physics-Guided-3D-Reconstruction-from-Sparse-Views" class="headerlink" title="PhysicsNeRF: Physics-Guided 3D Reconstruction from Sparse Views"></a>PhysicsNeRF: Physics-Guided 3D Reconstruction from Sparse Views</h2><p><strong>Authors:Mohamed Rayan Barhdadi, Hasan Kurban, Hussein Alnuweiri</strong></p>
<p>PhysicsNeRF is a physically grounded framework for 3D reconstruction from sparse views, extending Neural Radiance Fields with four complementary constraints: depth ranking, RegNeRF-style consistency, sparsity priors, and cross-view alignment. While standard NeRFs fail under sparse supervision, PhysicsNeRF employs a compact 0.67M-parameter architecture and achieves 21.4 dB average PSNR using only 8 views, outperforming prior methods. A generalization gap of 5.7-6.2 dB is consistently observed and analyzed, revealing fundamental limitations of sparse-view reconstruction. PhysicsNeRF enables physically consistent, generalizable 3D representations for agent interaction and simulation, and clarifies the expressiveness-generalization trade-off in constrained NeRF models. </p>
<blockquote>
<p>PhysicsNeRFæ˜¯ä¸€ä¸ªåŸºäºç‰©ç†çš„æ¡†æ¶ï¼Œç”¨äºä»ç¨€ç–è§†è§’è¿›è¡Œ3Dé‡å»ºï¼Œå®ƒæ‰©å±•äº†Neural Radiance Fieldsï¼ŒåŒ…å«å››ç§äº’è¡¥çº¦æŸï¼šæ·±åº¦æ’åºã€RegNeRFé£æ ¼çš„ä¸€è‡´æ€§ã€ç¨€ç–å…ˆéªŒå’Œè·¨è§†å›¾å¯¹é½ã€‚è™½ç„¶æ ‡å‡†NeRFåœ¨ç¨€ç–ç›‘ç£ä¸‹ä¼šå¤±æ•ˆï¼Œä½†PhysicsNeRFé‡‡ç”¨ç´§å‡‘çš„0.67Må‚æ•°æ¶æ„ï¼Œä»…ä½¿ç”¨8ä¸ªè§†è§’å°±å®ç°äº†21.4 dBçš„å¹³å‡PSNRï¼Œä¼˜äºå…ˆå‰çš„æ–¹æ³•ã€‚è§‚å¯Ÿåˆ°å¹¶åˆ†æäº†5.7-6.2 dBçš„æ³›åŒ–å·®è·ï¼Œæ­ç¤ºäº†ç¨€ç–è§†å›¾é‡å»ºçš„æ ¹æœ¬å±€é™æ€§ã€‚PhysicsNeRFèƒ½å¤Ÿå®ç°ç‰©ç†ä¸Šä¸€è‡´çš„ã€å¯æ³›åŒ–çš„3Dè¡¨ç¤ºï¼Œç”¨äºä»£ç†äº¤äº’å’Œæ¨¡æ‹Ÿï¼Œå¹¶æ˜ç¡®äº†å—é™NeRFæ¨¡å‹çš„è¡¨è¾¾åŠ›-æ³›åŒ–èƒ½åŠ›ä¹‹é—´çš„æƒè¡¡ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.23481v2">PDF</a> 4 pages, 2 figures, 2 tables. Appearing in Building Physically   Plausible World Models at the 42nd International Conference on Machine   Learning (ICML 2025), Vancouver, Canada</p>
<p><strong>Summary</strong></p>
<p>PhysicsNeRFæ˜¯ä¸€ç§åŸºäºç‰©ç†çš„ä¸‰ç»´é‡å»ºæ¡†æ¶ï¼Œå®ƒé€šè¿‡å››ä¸ªäº’è¡¥çº¦æŸæ‰©å±•äº†ç¥ç»è¾å°„åœºï¼ŒåŒ…æ‹¬æ·±åº¦æ’åºã€RegNeRFé£æ ¼çš„ä¸€è‡´æ€§ã€ç¨€ç–å…ˆéªŒå’Œè·¨è§†å›¾å¯¹é½ã€‚åœ¨ç¨€ç–ç›‘ç£ä¸‹ï¼Œæ ‡å‡†NeRFè¡¨ç°ä¸ä½³ï¼Œè€ŒPhysicsNeRFé‡‡ç”¨ç´§å‡‘çš„0.67Må‚æ•°æ¶æ„ï¼Œä»…ä½¿ç”¨8ä¸ªè§†å›¾å°±å®ç°äº†å¹³å‡å³°å€¼ä¿¡å™ªæ¯”ï¼ˆPSNRï¼‰ä¸º21.4 dBçš„é«˜æ€§èƒ½è¡¨ç°ï¼Œä¼˜äºç°æœ‰æ–¹æ³•ã€‚æ­¤å¤–ï¼Œç ”ç©¶å›¢é˜Ÿå‘ç°äº†ä¸€è‡´çš„æ³›åŒ–å·®è·ä¸º5.7~6.2 dBï¼Œæ­ç¤ºäº†ç¨€ç–è§†å›¾é‡å»ºçš„æ ¹æœ¬å±€é™æ€§ã€‚PhysicsNeRFå¯å®ç°ç‰©ç†ä¸€è‡´æ€§å¼ºçš„é€šç”¨ä¸‰ç»´è¡¨ç¤ºï¼Œé€‚ç”¨äºä»£ç†äº¤äº’å’Œæ¨¡æ‹Ÿï¼Œå¹¶æ˜ç¡®äº†çº¦æŸNeRFæ¨¡å‹çš„è¡¨è¾¾æ€§ä¸æ³›åŒ–ä¹‹é—´çš„æƒè¡¡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>PhysicsNeRFæ˜¯ä¸€ç§åŸºäºç‰©ç†çš„ä¸‰ç»´é‡å»ºæ¡†æ¶ï¼Œç”¨äºè§£å†³ç¨€ç–è§†å›¾ä¸‹çš„é‡å»ºé—®é¢˜ã€‚</li>
<li>å®ƒé€šè¿‡å››ä¸ªäº’è¡¥çº¦æŸæ‰©å±•äº†ç¥ç»è¾å°„åœºï¼šæ·±åº¦æ’åºã€RegNeRFé£æ ¼çš„ä¸€è‡´æ€§ã€ç¨€ç–å…ˆéªŒå’Œè·¨è§†å›¾å¯¹é½ã€‚</li>
<li>ä¸æ ‡å‡†NeRFç›¸æ¯”ï¼ŒPhysicsNeRFåœ¨ç¨€ç–ç›‘ç£ä¸‹è¡¨ç°æ›´ä¼˜ç§€ï¼Œä½¿ç”¨ç´§å‡‘çš„æ¶æ„å®ç°äº†é«˜PSNRå€¼ã€‚</li>
<li>ç ”ç©¶å›¢é˜Ÿå‘ç°PhysicsNeRFç›¸è¾ƒäºå…¶ä»–æ–¹æ³•çš„å¹³å‡æ³›åŒ–å·®è·ä¸º5~6dBã€‚</li>
<li>PhysicsNeRFå¯å®ç°ç‰©ç†ä¸€è‡´æ€§å¼ºçš„é€šç”¨ä¸‰ç»´è¡¨ç¤ºï¼Œé€‚ç”¨äºå¤šç§åº”ç”¨åœºæ™¯å¦‚ä»£ç†äº¤äº’å’Œæ¨¡æ‹Ÿã€‚</li>
<li>PhysicsNeRFå¼ºè°ƒäº†çº¦æŸNeRFæ¨¡å‹çš„è¡¨è¾¾æ€§ä¸æ³›åŒ–ä¹‹é—´çš„æƒè¡¡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.23481">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-f3463c5d1916385c6cd59c6537ed18c6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-01a3779da9beccfad961e082d216fea9.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-458c1f8383a55bdd24252a45c8e9576f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f4387bd3284df2c5b2776cf1db7a6085.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="CGS-GAN-3D-Consistent-Gaussian-Splatting-GANs-for-High-Resolution-Human-Head-Synthesis"><a href="#CGS-GAN-3D-Consistent-Gaussian-Splatting-GANs-for-High-Resolution-Human-Head-Synthesis" class="headerlink" title="CGS-GAN: 3D Consistent Gaussian Splatting GANs for High Resolution Human   Head Synthesis"></a>CGS-GAN: 3D Consistent Gaussian Splatting GANs for High Resolution Human   Head Synthesis</h2><p><strong>Authors:Florian Barthel, Wieland Morgenstern, Paul Hinzer, Anna Hilsmann, Peter Eisert</strong></p>
<p>Recently, 3D GANs based on 3D Gaussian splatting have been proposed for high quality synthesis of human heads. However, existing methods stabilize training and enhance rendering quality from steep viewpoints by conditioning the random latent vector on the current camera position. This compromises 3D consistency, as we observe significant identity changes when re-synthesizing the 3D head with each camera shift. Conversely, fixing the camera to a single viewpoint yields high-quality renderings for that perspective but results in poor performance for novel views. Removing view-conditioning typically destabilizes GAN training, often causing the training to collapse. In response to these challenges, we introduce CGS-GAN, a novel 3D Gaussian Splatting GAN framework that enables stable training and high-quality 3D-consistent synthesis of human heads without relying on view-conditioning. To ensure training stability, we introduce a multi-view regularization technique that enhances generator convergence with minimal computational overhead. Additionally, we adapt the conditional loss used in existing 3D Gaussian splatting GANs and propose a generator architecture designed to not only stabilize training but also facilitate efficient rendering and straightforward scaling, enabling output resolutions up to $2048^2$. To evaluate the capabilities of CGS-GAN, we curate a new dataset derived from FFHQ. This dataset enables very high resolutions, focuses on larger portions of the human head, reduces view-dependent artifacts for improved 3D consistency, and excludes images where subjects are obscured by hands or other objects. As a result, our approach achieves very high rendering quality, supported by competitive FID scores, while ensuring consistent 3D scene generation. Check our our project page here: <a target="_blank" rel="noopener" href="https://fraunhoferhhi.github.io/cgs-gan/">https://fraunhoferhhi.github.io/cgs-gan/</a> </p>
<blockquote>
<p>æœ€è¿‘ï¼ŒåŸºäºä¸‰ç»´é«˜æ–¯æ‹¼è´´æŠ€æœ¯çš„ä¸‰ç»´GANå·²è¢«æå‡ºç”¨äºé«˜è´¨é‡çš„äººå¤´åˆæˆã€‚ç„¶è€Œï¼Œç°æœ‰æ–¹æ³•é€šè¿‡æ ¹æ®å½“å‰ç›¸æœºä½ç½®å¯¹éšæœºæ½œåœ¨å‘é‡è¿›è¡Œæ¡ä»¶å¤„ç†ï¼Œä»è€Œç¨³å®šè®­ç»ƒå¹¶ä»é™¡å³­è§†è§’å¢å¼ºæ¸²æŸ“è´¨é‡ã€‚è¿™ä¼šå½±å“ä¸‰ç»´ä¸€è‡´æ€§ï¼Œå› ä¸ºæˆ‘ä»¬åœ¨é‡æ–°åˆæˆä¸‰ç»´å¤´éƒ¨æ—¶è§‚å¯Ÿåˆ°èº«ä»½çš„é‡å¤§å˜åŒ–ä¼šéšç€æ¯æ¬¡ç›¸æœºç§»åŠ¨è€Œæ”¹å˜ã€‚ç›¸åï¼Œå°†ç›¸æœºå›ºå®šåœ¨å•ä¸€è§†è§’ä¼šäº§ç”Ÿé«˜è´¨é‡çš„è¯¥è§†è§’çš„æ¸²æŸ“æ•ˆæœï¼Œä½†å¯¹äºæ–°é¢–è§†è§’çš„æ•ˆæœä¸ä½³ã€‚ç§»é™¤è§†è§’æ¡ä»¶é€šå¸¸ä¼šç ´åGANçš„è®­ç»ƒç¨³å®šæ€§ï¼Œç»å¸¸å¯¼è‡´è®­ç»ƒå´©æºƒã€‚é’ˆå¯¹è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†CGS-GANï¼Œè¿™æ˜¯ä¸€ç§æ–°å‹çš„ä¸‰ç»´é«˜æ–¯æ‹¼è´´GANæ¡†æ¶ï¼Œå¯åœ¨ä¸ä¾èµ–è§†è§’æ¡ä»¶çš„æƒ…å†µä¸‹å®ç°ç¨³å®šè®­ç»ƒå’Œé«˜è´¨é‡çš„ä¸‰ç»´ä¸€è‡´äººå¤´åˆæˆã€‚ä¸ºç¡®ä¿è®­ç»ƒç¨³å®šæ€§ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§å¤šè§†è§’æ­£åˆ™åŒ–æŠ€æœ¯ï¼Œè¯¥æŠ€æœ¯å¯åœ¨æœ€å°è®¡ç®—å¼€é”€çš„æƒ…å†µä¸‹å¢å¼ºç”Ÿæˆå™¨çš„æ”¶æ•›æ€§ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬é€‚åº”äº†ç°æœ‰ä¸‰ç»´é«˜æ–¯æ‹¼è´´GANä¸­çš„æ¡ä»¶æŸå¤±ï¼Œå¹¶è®¾è®¡äº†ä¸€ç§æ—¨åœ¨ç¨³å®šè®­ç»ƒå¹¶ä¿ƒè¿›é«˜æ•ˆæ¸²æŸ“å’Œç®€ä¾¿æ‰©å±•çš„ç”Ÿæˆå™¨æ¶æ„ï¼Œå¯å®ç°é«˜è¾¾$ 2048^2 $çš„è¾“å‡ºåˆ†è¾¨ç‡ã€‚ä¸ºäº†è¯„ä¼°CGS-GANçš„èƒ½åŠ›ï¼Œæˆ‘ä»¬ä»FFHQä¸­ç­›é€‰å‡ºäº†ä¸€ä¸ªæ–°çš„æ•°æ®é›†ã€‚è¯¥æ•°æ®é›†å¯å®ç°æé«˜åˆ†è¾¨ç‡ï¼Œä¾§é‡äºäººç±»å¤´éƒ¨çš„å¤§éƒ¨åˆ†åŒºåŸŸï¼Œå‡å°‘äº†è§†è§’ç›¸å…³çš„ä¼ªå½±ä»¥æ”¹å–„ä¸‰ç»´ä¸€è‡´æ€§ï¼Œå¹¶æ’é™¤äº†ä¸»ä½“è¢«æ‰‹æˆ–å…¶ä»–ç‰©ä½“é®æŒ¡çš„å›¾åƒã€‚å› æ­¤ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨ä¿è¯ä¸‰ç»´åœºæ™¯ä¸€è‡´æ€§ç”Ÿæˆçš„åŒæ—¶ï¼Œå®ç°äº†éå¸¸é«˜çš„æ¸²æŸ“è´¨é‡å¹¶å¾—åˆ°æœ‰ç«äº‰åŠ›çš„FIDåˆ†æ•°ã€‚è¯·è®¿é—®æˆ‘ä»¬çš„é¡¹ç›®é¡µé¢äº†è§£æ›´å¤šä¿¡æ¯ï¼š<a target="_blank" rel="noopener" href="https://fraunhoferhhi.github.io/cgs-gan/">https://fraunhoferhhi.github.io/cgs-gan/</a> ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.17590v2">PDF</a> Main paper 12 pages, supplementary materials 8 pages</p>
<p><strong>Summary</strong><br>     è¿‘æ—¥ï¼Œæå‡ºäº†åŸºäº3Dé«˜æ–¯è´´å›¾çš„3D GANsç”¨äºé«˜è´¨é‡çš„äººå¤´åˆæˆã€‚ç°æœ‰æ–¹æ³•é€šè¿‡å½“å‰ç›¸æœºä½ç½®å¯¹éšæœºæ½œåœ¨å‘é‡è¿›è¡Œæ¡ä»¶åŒ–ï¼Œä»¥å®ç°è®­ç»ƒç¨³å®šæ€§å’Œä»é™¡å³­è§†è§’æé«˜æ¸²æŸ“è´¨é‡ï¼Œä½†è¿™ä¼šæŸå®³3Dä¸€è‡´æ€§ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œæå‡ºCGS-GANæ¡†æ¶ï¼Œæ— éœ€ä¾èµ–è§†è§’æ¡ä»¶åŒ–å³å¯å®ç°ç¨³å®šè®­ç»ƒå’Œé«˜è´¨é‡ã€ä¸€è‡´çš„3Däººå¤´åˆæˆã€‚é€šè¿‡å¤šè§†è§’æ­£åˆ™åŒ–æŠ€æœ¯ç¡®ä¿è®­ç»ƒç¨³å®šæ€§ï¼Œå¹¶æ”¹è¿›ç°æœ‰æ¡ä»¶æŸå¤±å’Œç”Ÿæˆå™¨æ¶æ„ï¼Œä»¥æé«˜æ¸²æŸ“æ•ˆç‡å’Œåˆ†è¾¨ç‡ã€‚è¯„ä¼°CGS-GANèƒ½åŠ›çš„æ–°æ•°æ®é›†å·²é—®ä¸–ï¼Œæ›´ä¸“æ³¨äºå¤§å¤´éƒ¨åˆ†çš„é«˜åˆ†è¾¨ç‡ï¼Œå‡å°‘è§†è§’ç›¸å…³çš„ä¼ªå½±å’Œé®æŒ¡ç‰©ã€‚è¯¥æ–¹æ³•çš„æ¸²æŸ“è´¨é‡é«˜ï¼Œä¿è¯ä¸€è‡´çš„3Dåœºæ™¯ç”Ÿæˆã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç°æœ‰åŸºäº3Dé«˜æ–¯è´´å›¾çš„3D GANsæ–¹æ³•åœ¨åˆæˆé«˜è´¨é‡çš„äººå¤´æ—¶é¢ä¸´æŒ‘æˆ˜ï¼Œå¦‚è®­ç»ƒç¨³å®šæ€§å’Œè§†è§’ç›¸å…³çš„æ¸²æŸ“è´¨é‡ã€‚</li>
<li>CGS-GANæ¡†æ¶è¢«å¼•å…¥ä»¥è§£å†³è¿™äº›é—®é¢˜ï¼Œæ— éœ€ä¾èµ–è§†è§’æ¡ä»¶åŒ–å³å¯å®ç°ç¨³å®šè®­ç»ƒå’Œé«˜è´¨é‡ã€ä¸€è‡´çš„3Däººå¤´åˆæˆã€‚</li>
<li>é€šè¿‡å¤šè§†è§’æ­£åˆ™åŒ–æŠ€æœ¯ç¡®ä¿è®­ç»ƒç¨³å®šæ€§ï¼Œæé«˜ç”Ÿæˆå™¨æ”¶æ•›æ€§ã€‚</li>
<li>æ”¹è¿›äº†ç°æœ‰æ¡ä»¶æŸå¤±å’Œç”Ÿæˆå™¨æ¶æ„ï¼Œä»¥æé«˜æ¸²æŸ“æ•ˆç‡å’Œåˆ†è¾¨ç‡ï¼Œæ”¯æŒé«˜è¾¾$2048^2$çš„è¾“å‡ºåˆ†è¾¨ç‡ã€‚</li>
<li>æ–°æ•°æ®é›†é—®ä¸–ä»¥è¯„ä¼°CGS-GANçš„èƒ½åŠ›ï¼Œä¸“æ³¨äºå¤§å¤´éƒ¨åˆ†çš„é«˜åˆ†è¾¨ç‡ï¼Œå‡å°‘è§†è§’ç›¸å…³çš„ä¼ªå½±å’Œé®æŒ¡ç‰©çš„å½±å“ã€‚</li>
<li>CGS-GANå®ç°äº†é«˜æ¸²æŸ“è´¨é‡å’Œä¸€è‡´çš„3Dåœºæ™¯ç”Ÿæˆï¼Œå…·æœ‰ç«äº‰åŠ›çš„FIDåˆ†æ•°ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.17590">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-11fa15bca380aa3bf5fc8d1e0c8a8007.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-08124573fc86493b9dfb2d48d5d3917b.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-d53008d8bcac44d52ab6807cbf170cf5.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="Direct-Discriminative-Optimization-Your-Likelihood-Based-Visual-Generative-Model-is-Secretly-a-GAN-Discriminator"><a href="#Direct-Discriminative-Optimization-Your-Likelihood-Based-Visual-Generative-Model-is-Secretly-a-GAN-Discriminator" class="headerlink" title="Direct Discriminative Optimization: Your Likelihood-Based Visual   Generative Model is Secretly a GAN Discriminator"></a>Direct Discriminative Optimization: Your Likelihood-Based Visual   Generative Model is Secretly a GAN Discriminator</h2><p><strong>Authors:Kaiwen Zheng, Yongxin Chen, Huayu Chen, Guande He, Ming-Yu Liu, Jun Zhu, Qinsheng Zhang</strong></p>
<p>While likelihood-based generative models, particularly diffusion and autoregressive models, have achieved remarkable fidelity in visual generation, the maximum likelihood estimation (MLE) objective, which minimizes the forward KL divergence, inherently suffers from a mode-covering tendency that limits the generation quality under limited model capacity. In this work, we propose Direct Discriminative Optimization (DDO) as a unified framework that integrates likelihood-based generative training and GAN-type discrimination to bypass this fundamental constraint by exploiting reverse KL and self-generated negative signals. Our key insight is to parameterize a discriminator implicitly using the likelihood ratio between a learnable target model and a fixed reference model, drawing parallels with the philosophy of Direct Preference Optimization (DPO). Unlike GANs, this parameterization eliminates the need for joint training of generator and discriminator networks, allowing for direct, efficient, and effective finetuning of a well-trained model to its full potential beyond the limits of MLE. DDO can be performed iteratively in a self-play manner for progressive model refinement, with each round requiring less than 1% of pretraining epochs. Our experiments demonstrate the effectiveness of DDO by significantly advancing the previous SOTA diffusion model EDM, reducing FID scores from 1.79&#x2F;1.58&#x2F;1.96 to new records of 1.30&#x2F;0.97&#x2F;1.26 on CIFAR-10&#x2F;ImageNet-64&#x2F;ImageNet 512x512 datasets without any guidance mechanisms, and by consistently improving both guidance-free and CFG-enhanced FIDs of visual autoregressive models on ImageNet 256x256. </p>
<blockquote>
<p>åŸºäºæ¦‚ç‡çš„ç”Ÿæˆæ¨¡å‹ï¼Œç‰¹åˆ«æ˜¯æ‰©æ•£å’Œè‡ªå›å½’æ¨¡å‹ï¼Œåœ¨è§†è§‰ç”Ÿæˆæ–¹é¢å–å¾—äº†æ˜¾è‘—çš„ä¿çœŸåº¦ã€‚æœ€å¤§ä¼¼ç„¶ä¼°è®¡ï¼ˆMLEï¼‰ç›®æ ‡é€šè¿‡æœ€å°åŒ–æ­£å‘KLæ•£åº¦ï¼Œå›ºæœ‰åœ°é­å—æ¨¡å¼è¦†ç›–çš„å€¾å‘ï¼Œè¿™é™åˆ¶äº†æœ‰é™çš„æ¨¡å‹å®¹é‡ä¸‹çš„ç”Ÿæˆè´¨é‡ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ç›´æ¥åˆ¤åˆ«ä¼˜åŒ–ï¼ˆDDOï¼‰ä½œä¸ºä¸€ä¸ªç»Ÿä¸€çš„æ¡†æ¶ï¼Œå®ƒå°†åŸºäºæ¦‚ç‡çš„ç”Ÿæˆè®­ç»ƒå’ŒGANç±»å‹çš„åˆ¤åˆ«ç»“åˆèµ·æ¥ï¼Œé€šè¿‡åˆ©ç”¨åå‘KLå’Œè‡ªæˆ‘ç”Ÿæˆçš„è´Ÿä¿¡å·ç»•è¿‡è¿™ä¸ªåŸºæœ¬çº¦æŸã€‚æˆ‘ä»¬çš„å…³é”®è§è§£æ˜¯ï¼Œä½¿ç”¨ä¸€ä¸ªåˆ¤åˆ«å™¨éšå¼åœ°å‚æ•°åŒ–ç›®æ ‡æ¨¡å‹å’Œä¸€ä¸ªå›ºå®šå‚è€ƒæ¨¡å‹ä¹‹é—´çš„æ¦‚ç‡æ¯”ç‡ï¼Œè¿™ä¸ç›´æ¥åå¥½ä¼˜åŒ–ï¼ˆDPOï¼‰çš„ç†å¿µç›¸å¹³è¡Œã€‚ä¸åŒäºç”Ÿæˆå¯¹æŠ—ç½‘ç»œï¼ˆGANsï¼‰ï¼Œè¿™ç§å‚æ•°åŒ–æ¶ˆé™¤äº†ç”Ÿæˆå™¨å’Œåˆ¤åˆ«å™¨ç½‘ç»œè”åˆè®­ç»ƒçš„éœ€è¦ï¼Œå…è®¸å¯¹è‰¯å¥½è®­ç»ƒçš„æ¨¡å‹è¿›è¡Œç›´æ¥ã€é«˜æ•ˆå’Œæœ‰æ•ˆçš„å¾®è°ƒï¼Œå……åˆ†å‘æŒ¥å…¶æ½œåŠ›ï¼Œè¶…è¶ŠMLEçš„é™åˆ¶ã€‚DDOå¯ä»¥ä»¥ä¸€ç§è‡ªæˆ‘å¯¹æŠ—çš„æ–¹å¼è¿­ä»£è¿›è¡Œæ¨¡å‹çš„æ¸è¿›ä¼˜åŒ–ï¼Œæ¯ä¸€è½®æ‰€éœ€çš„é¢„è®­ç»ƒå‘¨æœŸä¸åˆ°1%ã€‚æˆ‘ä»¬çš„å®éªŒé€šè¿‡æ˜¾è‘—åœ°æå‡å…ˆå‰çš„æœ€ä½³æ‰©æ•£æ¨¡å‹EDMï¼Œåœ¨æ— éœ€ä»»ä½•æŒ‡å¯¼æœºåˆ¶çš„æƒ…å†µä¸‹ï¼Œåœ¨CIFAR-10ã€ImageNet-64å’ŒImageNet 512x512æ•°æ®é›†ä¸Šå°†FIDå¾—åˆ†ä»1.79&#x2F;1.58&#x2F;1.96é™ä½åˆ°æ–°çš„è®°å½•1.30&#x2F;0.97&#x2F;1.26ï¼Œè¯æ˜äº†DDOçš„æœ‰æ•ˆæ€§ã€‚åŒæ—¶ï¼Œå®ƒåœ¨æ— éœ€æŒ‡å¯¼å’ŒCFGå¢å¼ºçš„æ¡ä»¶ä¸‹ï¼Œå¯¹ImageNet 256x256çš„è§†è§‰è‡ªå›å½’æ¨¡å‹çš„FIDè¿›è¡Œäº†æŒç»­ä¸æ–­çš„æ”¹è¿›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.01103v3">PDF</a> ICML 2025 Spotlight Project Page:   <a target="_blank" rel="noopener" href="https://research.nvidia.com/labs/dir/ddo/">https://research.nvidia.com/labs/dir/ddo/</a> Code: <a target="_blank" rel="noopener" href="https://github.com/NVlabs/DDO">https://github.com/NVlabs/DDO</a></p>
<p><strong>æ‘˜è¦</strong></p>
<p>åŸºäºä¼¼ç„¶ç”Ÿæˆæ¨¡å‹ï¼Œç‰¹åˆ«æ˜¯æ‰©æ•£å’Œè‡ªå›å½’æ¨¡å‹ï¼Œåœ¨è§†è§‰ç”Ÿæˆæ–¹é¢å–å¾—äº†æ˜¾è‘—çš„ä¿çœŸåº¦ã€‚ç„¶è€Œï¼Œæœ€å¤§ä¼¼ç„¶ä¼°è®¡ï¼ˆMLEï¼‰ç›®æ ‡é€šè¿‡æœ€å°åŒ–æ­£å‘KLæ•£åº¦ï¼Œå­˜åœ¨æ¨¡å¼è¦†ç›–å€¾å‘ï¼Œé™åˆ¶äº†æœ‰é™æ¨¡å‹å®¹é‡ä¸‹çš„ç”Ÿæˆè´¨é‡ã€‚æœ¬ç ”ç©¶æå‡ºç›´æ¥åˆ¤åˆ«ä¼˜åŒ–ï¼ˆDDOï¼‰ä½œä¸ºç»Ÿä¸€æ¡†æ¶ï¼ŒèåˆåŸºäºä¼¼ç„¶çš„ç”Ÿæˆè®­ç»ƒå’ŒGANå‹åˆ¤åˆ«ï¼Œé€šè¿‡åå‘KLå’Œè‡ªæˆ‘ç”Ÿæˆçš„è´Ÿä¿¡å·ç»•è¿‡è¿™ä¸€åŸºæœ¬çº¦æŸã€‚æˆ‘ä»¬çš„å…³é”®è§è§£æ˜¯åˆ©ç”¨å¯å­¦ä¹ çš„ç›®æ ‡æ¨¡å‹å’Œå›ºå®šå‚è€ƒæ¨¡å‹ä¹‹é—´çš„ä¼¼ç„¶æ¯”æ¥éšå«åœ°å‚æ•°åŒ–åˆ¤åˆ«å™¨ï¼Œè¿™ä¸ç›´æ¥åå¥½ä¼˜åŒ–ï¼ˆDPOï¼‰çš„ç†å¿µç›¸å¥‘åˆã€‚ä¸åŒäºGANsï¼Œè¿™ç§å‚æ•°åŒ–æ–¹å¼æ— éœ€ç”Ÿæˆå™¨å’Œåˆ¤åˆ«å™¨çš„è”åˆè®­ç»ƒï¼Œèƒ½å¤Ÿç›´æ¥ã€é«˜æ•ˆä¸”æœ‰æ•ˆåœ°å¯¹é¢„è®­ç»ƒè‰¯å¥½çš„æ¨¡å‹è¿›è¡Œå¾®è°ƒï¼Œå……åˆ†å‘æŒ¥å…¶æ½œåŠ›ï¼Œçªç ´MLEçš„é™åˆ¶ã€‚DDOå¯ä»¥è‡ªæˆ‘è¿­ä»£çš„æ–¹å¼è¿›è¡Œæ¸è¿›æ¨¡å‹ä¼˜åŒ–ï¼Œæ¯è½®æ‰€éœ€é¢„è®­ç»ƒå‘¨æœŸä¸åˆ°1%ã€‚å®éªŒè¯æ˜DDOçš„æœ‰æ•ˆæ€§ï¼Œæ˜¾è‘—æå‡äº†ä¹‹å‰çš„SOTAæ‰©æ•£æ¨¡å‹EDMï¼Œåœ¨æ— éœ€ä»»ä½•å¼•å¯¼æœºåˆ¶çš„æƒ…å†µä¸‹ï¼Œå°†CIFAR-10&#x2F;ImageNet-64&#x2F;ImageNet 512x512æ•°æ®é›†çš„FIDåˆ†æ•°ä»1.79&#x2F;1.58&#x2F;1.96é™è‡³æ–°çºªå½•çš„1.30&#x2F;0.97&#x2F;1.26ã€‚åŒæ—¶ï¼Œå¯¹äºImageNet 256x256çš„è§†è§‰è‡ªå›å½’æ¨¡å‹ï¼Œåœ¨æ— å¼•å¯¼å’ŒCFGå¢å¼ºçš„æƒ…å†µä¸‹ï¼Œä¹Ÿå®ç°äº†FIDçš„æŒç»­æ”¹å–„ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>åŸºäºä¼¼ç„¶ç”Ÿæˆæ¨¡å‹åœ¨è§†è§‰ç”Ÿæˆæ–¹é¢çš„ä¼˜å¼‚è¡¨ç°ï¼Œä½†å­˜åœ¨æ¨¡å¼è¦†ç›–çš„å›ºæœ‰ç¼ºé™·ã€‚</li>
<li>æå‡ºç›´æ¥åˆ¤åˆ«ä¼˜åŒ–ï¼ˆDDOï¼‰æ¡†æ¶ï¼Œç»“åˆä¼¼ç„¶ç”Ÿæˆè®­ç»ƒå’ŒGANå‹åˆ¤åˆ«ï¼Œç»•è¿‡æ¨¡å¼è¦†ç›–é—®é¢˜ã€‚</li>
<li>åˆ©ç”¨å¯å­¦ä¹ çš„ç›®æ ‡æ¨¡å‹å’Œå›ºå®šå‚è€ƒæ¨¡å‹ä¹‹é—´çš„ä¼¼ç„¶æ¯”å‚æ•°åŒ–åˆ¤åˆ«å™¨ï¼Œä¸ç›´æ¥åå¥½ä¼˜åŒ–ï¼ˆDPOï¼‰ç†å¿µç›¸ä¼¼ã€‚</li>
<li>DDOä¸éœ€è¦ç”Ÿæˆå™¨å’Œåˆ¤åˆ«å™¨çš„è”åˆè®­ç»ƒï¼Œèƒ½å¤Ÿç›´æ¥å¾®è°ƒé¢„è®­ç»ƒæ¨¡å‹ã€‚</li>
<li>DDOå¯å®ç°æ¸è¿›æ¨¡å‹ä¼˜åŒ–ï¼Œæ¯è½®é¢„è®­ç»ƒå‘¨æœŸå°‘ã€‚</li>
<li>DDOæ˜¾è‘—æå‡äº†æ‰©æ•£æ¨¡å‹çš„æ€§èƒ½ï¼Œé™ä½äº†FIDåˆ†æ•°ã€‚</li>
<li>DDOå¯¹æ— å¼•å¯¼å’Œå¸¦CFGå¢å¼ºçš„è§†è§‰è‡ªå›å½’æ¨¡å‹ä¹Ÿæœ‰æ”¹å–„æ•ˆæœã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.01103">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-bbdc4b98217532d4a2d01ba910caaac0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-eddac7b84b8b22297c5f1258d379eebd.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-8c21d725625523af412647cbffa3b415.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-408f70df8664d0234d35fe0fb9aa99fd.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-06-25/NeRF/">https://kedreamix.github.io/Talk2Paper/Paper/2025-06-25/NeRF/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/NeRF/">
                                    <span class="chip bg-color">NeRF</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-06-25/Diffusion%20Models/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-cd51a0c4d9d03867bc364d3da271f7b6.jpg" class="responsive-img" alt="Diffusion Models">
                        
                        <span class="card-title">Diffusion Models</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Diffusion Models æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-06-25  DIP Unsupervised Dense In-Context Post-training of Visual   Representations
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-06-25
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Diffusion-Models/" class="post-category">
                                    Diffusion Models
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Diffusion-Models/">
                        <span class="chip bg-color">Diffusion Models</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-06-25/3DGS/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-d90e373654d245c35acece531a9185b7.jpg" class="responsive-img" alt="3DGS">
                        
                        <span class="card-title">3DGS</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            3DGS æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-06-25  GRAND-SLAM Local Optimization for Globally Consistent Large-Scale   Multi-Agent Gaussian SLAM
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-06-25
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/3DGS/" class="post-category">
                                    3DGS
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/3DGS/">
                        <span class="chip bg-color">3DGS</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">27685.3k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
