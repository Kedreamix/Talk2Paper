<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Speech">
    <meta name="description" content="Speech 方向最新论文已更新，请持续关注 Update in 2025-06-25  Context Biasing for Pronunciations-Orthography Mismatch in Automatic   Speech Recognition">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Speech | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loader页面消失采用渐隐的方式*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logo出现动画 */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//使用渐隐的方法淡出loading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//强制显示loading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">嘘~  正在从服务器偷取页面 . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>首页</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>标签</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>分类</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>归档</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="搜索" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="深色/浅色模式" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			首页
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			标签
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			分类
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			归档
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-c20f6db03d3cb4937f7653b40c8f63c6.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Speech</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- 文章内容详情 -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Speech/">
                                <span class="chip bg-color">Speech</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Speech/" class="post-category">
                                Speech
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp;
                    2025-06-25
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>更新日期:&nbsp;&nbsp;
                    2025-07-06
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>文章字数:&nbsp;&nbsp;
                    8.6k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>阅读时长:&nbsp;&nbsp;
                    34 分
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>阅读次数:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>⚠️ 以下所有内容总结都来自于 大语言模型的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p>
</blockquote>
<h1 id="2025-06-25-更新"><a href="#2025-06-25-更新" class="headerlink" title="2025-06-25 更新"></a>2025-06-25 更新</h1><h2 id="Context-Biasing-for-Pronunciations-Orthography-Mismatch-in-Automatic-Speech-Recognition"><a href="#Context-Biasing-for-Pronunciations-Orthography-Mismatch-in-Automatic-Speech-Recognition" class="headerlink" title="Context Biasing for Pronunciations-Orthography Mismatch in Automatic   Speech Recognition"></a>Context Biasing for Pronunciations-Orthography Mismatch in Automatic   Speech Recognition</h2><p><strong>Authors:Christian Huber, Alexander Waibel</strong></p>
<p>Neural sequence-to-sequence systems deliver state-of-the-art performance for automatic speech recognition. When using appropriate modeling units, e.g., byte-pair encoded characters, these systems are in principal open vocabulary systems. In practice, however, they often fail to recognize words not seen during training, e.g., named entities, acronyms, or domain-specific special words. To address this problem, many context biasing methods have been proposed; however, for words with a pronunciation-orthography mismatch, these methods may still struggle. We propose a method which allows corrections of substitution errors to improve the recognition accuracy of such challenging words. Users can add corrections on the fly during inference. We show that with this method we get a relative improvement in biased word error rate of up to 11%, while maintaining a competitive overall word error rate. </p>
<blockquote>
<p>神经网络序列到序列系统在自动语音识别方面达到了最先进的性能。在使用适当的建模单元（例如，字节对编码字符）时，这些系统在理论上都是开放词汇系统。然而，在实践中，它们往往无法识别在训练期间未见过的词汇，例如，实体名称、缩写或特定领域的特殊词汇。为了解决这一问题，已经提出了许多上下文偏向方法；但对于发音和书写不匹配的词汇，这些方法可能仍然面临挑战。我们提出了一种方法，可以纠正替换错误，以提高此类具有挑战性词汇的识别准确率。用户可以在推理过程中即时添加修正。我们证明，使用这种方法，在偏向词汇错误率方面实现了高达11%的相对改进，同时保持了具有竞争力的总体词汇错误率。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.18703v1">PDF</a> </p>
<p><strong>Summary</strong>：神经网络序列到序列系统在自动语音识别方面表现出卓越性能。通过使用适当的建模单元（如字节对编码字符），这些系统原则上具有开放词汇表。但在实践中，它们往往无法识别训练期间未见过的单词，如命名实体、缩写或特定领域的特殊词汇。为解决这一问题，我们提出了一种方法，允许在推理过程中即时纠正替代错误，以提高此类具有挑战性单词的识别准确性。实验表明，使用此方法可将偏向性单词错误率相对提高高达11%，同时保持整体单词错误率的竞争力。</p>
<p><strong>Key Takeaways</strong>：</p>
<ol>
<li>神经网络序列到序列系统在自动语音识别方面具有卓越性能。</li>
<li>这些系统原则上具有开放词汇表，但实践中对未见过的单词识别能力有限。</li>
<li>现有方法对于存在语音和书写不一致的单词可能仍有困难。</li>
<li>提出了一种允许即时纠正替代错误的方法，以提高具有挑战性单词的识别准确性。</li>
<li>用户可以在推理过程中进行实时修正。</li>
<li>此方法可将偏向性单词错误率相对提高高达11%。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.18703">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-7aee237b44db3811c10250aa2e54b5b7.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-4d7ed34434e84ddae7bdb4c4baad5ffd.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-1abcf73e2cc7ed28d63368583ea04c4c.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="AI-Generated-Song-Detection-via-Lyrics-Transcripts"><a href="#AI-Generated-Song-Detection-via-Lyrics-Transcripts" class="headerlink" title="AI-Generated Song Detection via Lyrics Transcripts"></a>AI-Generated Song Detection via Lyrics Transcripts</h2><p><strong>Authors:Markus Frohmann, Elena V. Epure, Gabriel Meseguer-Brocal, Markus Schedl, Romain Hennequin</strong></p>
<p>The recent rise in capabilities of AI-based music generation tools has created an upheaval in the music industry, necessitating the creation of accurate methods to detect such AI-generated content. This can be done using audio-based detectors; however, it has been shown that they struggle to generalize to unseen generators or when the audio is perturbed. Furthermore, recent work used accurate and cleanly formatted lyrics sourced from a lyrics provider database to detect AI-generated music. However, in practice, such perfect lyrics are not available (only the audio is); this leaves a substantial gap in applicability in real-life use cases. In this work, we instead propose solving this gap by transcribing songs using general automatic speech recognition (ASR) models. We do this using several detectors. The results on diverse, multi-genre, and multi-lingual lyrics show generally strong detection performance across languages and genres, particularly for our best-performing model using Whisper large-v2 and LLM2Vec embeddings. In addition, we show that our method is more robust than state-of-the-art audio-based ones when the audio is perturbed in different ways and when evaluated on different music generators. Our code is available at <a target="_blank" rel="noopener" href="https://github.com/deezer/robust-AI-lyrics-detection">https://github.com/deezer/robust-AI-lyrics-detection</a>. </p>
<blockquote>
<p>近期AI音乐生成工具的能提升对引起了音乐产业的动荡，因此急需创造准确的方法来检测这类AI生成的内容。这可以通过基于音频的检测器来实现；然而，它们在新生成的音乐或者音频发生干扰的情况下通常无法进行有效的检测。另外，最近的研究工作采用了来自歌词提供数据库的准确和格式良好的歌词来检测AI生成的音乐。但在实际应用中，完美的歌词通常无法获取（只有音频），这使得在现实应用场景中的适用性存在很大差距。在这项工作中，我们提议通过采用通用的语音识别（ASR）模型进行歌词转录来解决这一差距。我们使用多种检测器实现这一点。在不同风格和多语言的歌词上的结果表明，我们的模型在不同语言和风格上的检测性能普遍强大，尤其是使用Whisper large-v2和LLM2Vec嵌入的最佳性能模型。此外，我们证明了当音频以不同方式受到干扰以及在不同的音乐生成器上评估时，我们的方法比最新的音频检测方法更为稳健。我们的代码可在<a target="_blank" rel="noopener" href="https://github.com/deezer/robust-AI-lyrics-detection%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/deezer/robust-AI-lyrics-detection找到。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.18488v1">PDF</a> Accepted to ISMIR 2025</p>
<p><strong>Summary</strong><br>     最近人工智能音乐生成工具的进步对音乐产业产生了巨大影响，需要创建准确的方法来检测AI生成的内容。尽管音频检测器可用于此目的，但它们对新生成器或扰动音频的泛化能力有限。为此，本研究提出利用通用语音识别（ASR）模型进行歌词转录来解决实际应用中的问题。通过使用多个检测器，该模型在跨语言和多风格的音乐上表现优异，尤其是使用Whisper large-v2和LLM2Vec嵌入的最佳模型。此外，当音频以不同方式受到干扰或在不同的音乐生成器上评估时，该方法比最新的音频方法更稳健。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>AI音乐生成工具的进步对音乐产业产生了深远影响。</li>
<li>音频检测器在泛化新生成器和扰动音频方面存在局限性。</li>
<li>本研究通过利用通用语音识别（ASR）模型进行歌词转录来解决实际应用中的问题。</li>
<li>该模型使用多个检测器在跨语言和多风格的音乐上表现良好。</li>
<li>最佳模型使用Whisper large-v2和LLM2Vec嵌入技术。</li>
<li>该方法比现有的音频方法更稳健，特别是在处理扰动音频和不同音乐生成器时。</li>
<li>研究的代码已公开在GitHub上。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.18488">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-e95de3e6e02272e04c6c9d81027637d6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-bcc5c2b57fc1803f3eddbf4349a0fc8b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ed095d8a11541221e508c2162ea3dbac.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ea722d81e536beecaf13627941a77019.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d97d311c6beb6632a08b8f2a690e77c3.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-dd89225e8e7e04a2b3dcf7c915937310.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c20f6db03d3cb4937f7653b40c8f63c6.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Rethinking-Mean-Opinion-Scores-in-Speech-Quality-Assessment-Aggregation-through-Quantized-Distribution-Fitting"><a href="#Rethinking-Mean-Opinion-Scores-in-Speech-Quality-Assessment-Aggregation-through-Quantized-Distribution-Fitting" class="headerlink" title="Rethinking Mean Opinion Scores in Speech Quality Assessment: Aggregation   through Quantized Distribution Fitting"></a>Rethinking Mean Opinion Scores in Speech Quality Assessment: Aggregation   through Quantized Distribution Fitting</h2><p><strong>Authors:Yuto Kondo, Hirokazu Kameoka, Kou Tanaka, Takuhiro Kaneko</strong></p>
<p>Speech quality assessment (SQA) aims to evaluate the quality of speech samples without relying on time-consuming listener questionnaires. Recent efforts have focused on training neural-based SQA models to predict the mean opinion score (MOS) of speech samples produced by text-to-speech or voice conversion systems. This paper targets the enhancement of MOS prediction models’ performance. We propose a novel score aggregation method to address the limitations of conventional annotations for MOS, which typically involve ratings on a scale from 1 to 5. Our method is based on the hypothesis that annotators internally consider continuous scores and then choose the nearest discrete rating. By modeling this process, we approximate the generative distribution of ratings by quantizing the latent continuous distribution. We then use the peak of this latent distribution, estimated through the loss between the quantized distribution and annotated ratings, as a new representative value instead of MOS. Experimental results demonstrate that substituting MOSNet’s predicted target with this proposed value improves prediction performance. </p>
<blockquote>
<p>语音质量评估（SQA）旨在评估语音样本的质量，而不依赖于耗时的听众问卷调查。近期的研究努力集中在训练基于神经网络的SQA模型上，以预测文本到语音或语音转换系统产生的语音样本的平均意见得分（MOS）。本文针对提高MOS预测模型的性能。我们提出了一种新的评分聚合方法，以解决传统MOS标注的局限性，传统标注通常涉及从1到5的评分范围。我们的方法基于这样的假设：注释者在内部考虑连续分数，然后选择最近的离散评分。通过对这一过程进行建模，我们通过量化潜在的连续分布来近似评分的生成分布。然后，我们通过量化分布与注释评分之间的损失来估计潜在分布的峰值，并将其作为新的代表值来代替MOS。实验结果表明，用该提议的值替换MOSNet的预测目标可以提高预测性能。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.18307v1">PDF</a> Accepted on ICASSP 2025</p>
<p><strong>总结</strong></p>
<p>本文关注语音质量评估（SQA）中均值意见得分（MOS）预测模型的性能提升。针对传统MOS标注方法的局限性，提出了一种新的评分聚合方法。该方法基于标注者实际上考虑的是连续分数，然后选择最接近的离散评分这一假设。通过量化潜在的连续分布来近似评分的生成分布，并使用此潜在分布的峰值（通过量化分布与标注评分之间的损失来估计）作为新的代表性值，以改进MOS预测模型的性能。</p>
<p><strong>关键见解</strong></p>
<ol>
<li>语音质量评估（SQA）旨在评估语音样本的质量，且不依赖耗时的人力调查问卷。</li>
<li>近期研究致力于训练基于神经网络的SQA模型，以预测文本到语音或语音转换系统产生的语音样本的均值意见得分（MOS）。</li>
<li>本文提出了一种新的评分聚合方法，以解决传统MOS标注方法的局限性。</li>
<li>该方法基于标注者实际上在考虑连续分数然后选择最接近的离散评分的假设。</li>
<li>通过量化潜在的连续分布来近似评分的生成分布。</li>
<li>使用潜在分布的峰值作为新的代表性值，通过损失函数估计这一峰值。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.18307">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-2f8c9dff4eaee3a6e7c92349a82ab85f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a2cae77620a35907ccf3524eb30b62e1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-49421c9d1faa570108dca21ec6af0171.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Breaking-the-Transcription-Bottleneck-Fine-tuning-ASR-Models-for-Extremely-Low-Resource-Fieldwork-Languages"><a href="#Breaking-the-Transcription-Bottleneck-Fine-tuning-ASR-Models-for-Extremely-Low-Resource-Fieldwork-Languages" class="headerlink" title="Breaking the Transcription Bottleneck: Fine-tuning ASR Models for   Extremely Low-Resource Fieldwork Languages"></a>Breaking the Transcription Bottleneck: Fine-tuning ASR Models for   Extremely Low-Resource Fieldwork Languages</h2><p><strong>Authors:Siyu Liang, Gina-Anne Levow</strong></p>
<p>Automatic Speech Recognition (ASR) has reached impressive accuracy for high-resource languages, yet its utility in linguistic fieldwork remains limited. Recordings collected in fieldwork contexts present unique challenges, including spontaneous speech, environmental noise, and severely constrained datasets from under-documented languages. In this paper, we benchmark the performance of two fine-tuned multilingual ASR models, MMS and XLS-R, on five typologically diverse low-resource languages with control of training data duration. Our findings show that MMS is best suited when extremely small amounts of training data are available, whereas XLS-R shows parity performance once training data exceed one hour. We provide linguistically grounded analysis for further provide insights towards practical guidelines for field linguists, highlighting reproducible ASR adaptation approaches to mitigate the transcription bottleneck in language documentation. </p>
<blockquote>
<p>自动语音识别（ASR）对于资源丰富的语言已经达到了令人印象深刻的准确性，然而其在语言领域工作中的实用性仍然有限。在现场工作环境中收集的录音呈现出独特的挑战，包括自然口语、环境噪声以及来自记录不全的语言的严重受限数据集。在本文中，我们在控制训练数据时间长度的情况下，对两种经过微调的多语种ASR模型MMS和XLS-R进行了五种类型丰富且资源匮乏的语言的基准测试。我们的研究发现，当可用训练数据量极小的时候，MMS最为合适；而当训练数据超过一个小时时，XLS-R表现出平等的性能。我们提供了基于语言学的分析，为进一步为语言学家提供实际指导的见解，强调可复制的ASR适应方法来减轻语言记录中的转录瓶颈问题。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.17459v1">PDF</a> </p>
<p><strong>Summary</strong>：<br>自动语音识别（ASR）在高资源语言上的准确性令人印象深刻，但在语言领域中的应用仍然有限。特别是在语言田野工作中收集的录音面临诸多挑战，如自发语音、环境噪声以及来自未记录语言的有限数据集。本文对比了两种经过微调的多语种ASR模型MMS和XLS-R，在五种类型多样但资源稀缺的语言环境下的性能。结果表明，当可用训练数据极为有限时，MMS表现最佳，当训练数据超过一小时时，XLS-R表现出同等性能。此外，我们还提供了语言学的见解和实践指南，帮助语言工作者更好地理解如何解决实际应用中的转录瓶颈问题。总体而言，这些建议将极大地促进语言的记录和保护。通过重新建立数据集和其他ASR适应性策略的实施策略的运用有效减少转录的瓶颈问题。我们强调了实际应用中可再生ASR适应性方法的优点，以及它在解决转录瓶颈方面的作用，这将有助于改善未来ASR模型的性能和效率。通过适当的改进和调整策略的运用能够进一步推动ASR技术在语言田野工作中发挥更大的作用。 </p>
<p><strong>Key Takeaways</strong>:</p>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.17459">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-bf27e5e8ba41a30d81d751ca702f6f67.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4a7eb23a2b02142ff71c1419ba381ddb.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-255454f3b185607a4ce02f06a77ca8b3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-36ca7664213779f4f18bac2f011b2395.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-054d595a6d2e157ce0c4b9fed212997a.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="SuPseudo-A-Pseudo-supervised-Learning-Method-for-Neural-Speech-Enhancement-in-Far-field-Speech-Recognition"><a href="#SuPseudo-A-Pseudo-supervised-Learning-Method-for-Neural-Speech-Enhancement-in-Far-field-Speech-Recognition" class="headerlink" title="SuPseudo: A Pseudo-supervised Learning Method for Neural Speech   Enhancement in Far-field Speech Recognition"></a>SuPseudo: A Pseudo-supervised Learning Method for Neural Speech   Enhancement in Far-field Speech Recognition</h2><p><strong>Authors:Longjie Luo, Lin Li, Qingyang Hong</strong></p>
<p>Due to the lack of target speech annotations in real-recorded far-field conversational datasets, speech enhancement (SE) models are typically trained on simulated data. However, the trained models often perform poorly in real-world conditions, hindering their application in far-field speech recognition. To address the issue, we (a) propose direct sound estimation (DSE) to estimate the oracle direct sound of real-recorded data for SE; and (b) present a novel pseudo-supervised learning method, SuPseudo, which leverages DSE-estimates as pseudo-labels and enables SE models to directly learn from and adapt to real-recorded data, thereby improving their generalization capability. Furthermore, an SE model called FARNET is designed to fully utilize SuPseudo. Experiments on the MISP2023 corpus demonstrate the effectiveness of SuPseudo, and our system significantly outperforms the previous state-of-the-art. A demo of our method can be found at <a target="_blank" rel="noopener" href="https://eellj.github.io/SuPseudo/">https://EeLLJ.github.io/SuPseudo/</a>. </p>
<blockquote>
<p>由于真实录制的远距离对话数据集中缺乏目标语音注释，语音增强（SE）模型通常是在模拟数据上进行训练的。然而，在真实世界条件下，训练好的模型的性能往往不佳，阻碍了它们在远距离语音识别中的应用。为了解决这一问题，我们（a）提出了直接声音估计（DSE），以估计真实录制数据的理想直达声音，用于语音增强；（b）提出了一种新型伪监督学习方法SuPseudo，它利用DSE估计作为伪标签，使SE模型能够直接从真实数据中学习和适应，从而提高其泛化能力。此外，还设计了一个名为FARNET的SE模型，以充分利用SuPseudo。在MISP2023语料库上的实验证明了SuPseudo的有效性，我们的系统显著优于之前的最先进水平。有关我们方法的演示可在<a target="_blank" rel="noopener" href="https://eellj.github.io/SuPseudo/">https://EeLLJ.github.io/SuPseudo/</a>找到。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.24450v2">PDF</a> Accepted by InterSpeech 2025</p>
<p><strong>Summary</strong></p>
<p>针对远场语音识别中真实录音数据集目标语音注释缺失的问题，现有的语音增强（SE）模型通常在模拟数据上进行训练，但在现实环境中的表现往往不佳。本文提出了两种解决方案：（a）提出直接声音估计（DSE），用于估计真实录音数据的理想直接声音，用于语音增强；（b）提出了一种新的伪监督学习方法SuPseudo，它利用DSE估计作为伪标签，使SE模型能够直接从真实数据中学习和适应，从而提高其泛化能力。实验表明，SuPseudo方法有效，使用其设计的FARNET模型在MISP2023语料库上的表现优于现有技术。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>真实录音的远场语音识别数据集缺乏目标语音注释，导致语音增强（SE）模型在模拟数据上训练后，现实环境中的表现不佳。</li>
<li>提出直接声音估计（DSE）方法，用以估计真实录音数据的理想直接声音，为解决此问题打下基础。</li>
<li>介绍了新的伪监督学习方法SuPseudo，它利用DSE的估计结果作为伪标签，使SE模型能够直接从真实数据中学习和适应。</li>
<li>SuPseudo方法提高了SE模型的泛化能力。</li>
<li>设计了名为FARNET的SE模型，以充分利用SuPseudo方法。</li>
<li>在MISP2023语料库上的实验证明了SuPseudo方法的有效性，且其表现优于现有技术。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.24450">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-583ba48e097f009ffff8dad37dd4cb99.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-efc60436bae0475679a8ca26eaeee1da.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e5ee31987b8703bd3aea1af33be50171.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-19307c4333638b41bcbba90f5d8cb337.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Pseudo-Labels-based-Neural-Speech-Enhancement-for-the-AVSR-Task-in-the-MISP-Meeting-Challenge"><a href="#Pseudo-Labels-based-Neural-Speech-Enhancement-for-the-AVSR-Task-in-the-MISP-Meeting-Challenge" class="headerlink" title="Pseudo Labels-based Neural Speech Enhancement for the AVSR Task in the   MISP-Meeting Challenge"></a>Pseudo Labels-based Neural Speech Enhancement for the AVSR Task in the   MISP-Meeting Challenge</h2><p><strong>Authors:Longjie Luo, Shenghui Lu, Lin Li, Qingyang Hong</strong></p>
<p>This paper presents our system for the MISP-Meeting Challenge Track 2. The primary difficulty lies in the dataset, which contains strong background noise, reverberation, overlapping speech, and diverse meeting topics. To address these issues, we (a) designed G-SpatialNet, a speech enhancement (SE) model to improve Guided Source Separation (GSS) signals; (b) proposed TLS, a framework comprising time alignment, level alignment, and signal-to-noise ratio filtering, to generate signal-level pseudo labels for real-recorded far-field audio data, thereby facilitating SE models’ training; and (c) explored fine-tuning strategies, data augmentation, and multimodal information to enhance the performance of pre-trained Automatic Speech Recognition (ASR) models in meeting scenarios. Finally, our system achieved character error rates (CERs) of 5.44% and 9.52% on the Dev and Eval sets, respectively, with relative improvements of 64.8% and 52.6% over the baseline, securing second place. </p>
<blockquote>
<p>本文介绍了我们在MISP-Meeting挑战赛轨道2中的系统。主要难点在于数据集，它包含强烈的背景噪声、回声、语音重叠和多样的会议主题。为了解决这些问题，我们（a）设计了G-SpatialNet，这是一个语音增强（SE）模型，用于改进导向源分离（GSS）信号；（b）提出TLS，一个包含时间对齐、水平对齐和信噪比过滤的框架，用于为真实录制的远距离音频数据生成信号级伪标签，从而帮助训练SE模型；（c）探索了微调策略、数据增强和多模态信息，以提高预训练的自动语音识别（ASR）模型在会议场景中的性能。最终，我们的系统在开发集和评估集上分别实现了5.44%和9.52%的字符错误率（CERs），相对于基线有64.8%和52.6%的相对改进，获得了第二名。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.24446v2">PDF</a> Accepted by InterSpeech 2025</p>
<p><strong>Summary</strong>：</p>
<p>本文介绍了针对MISP-Meeting挑战赛道2的系统。主要难点在于数据集存在强背景噪声、回声、语音重叠以及会议主题多样等问题。为解决这些问题，提出了G-SpatialNet模型以提高引导源分离信号的语音增强效果，并提出了TLS框架以生成信号级伪标签用于真实远距离音频数据的训练，同时探索了微调策略、数据增强和多模态信息以增强会议场景下预训练自动语音识别模型的性能。最终，系统实现了开发集和评估集上的字符错误率分别为5.44%和9.52%，相对于基线有显著改善，获得第二名。</p>
<p><strong>Key Takeaways</strong>：</p>
<ol>
<li>系统面临的主要难点在于数据集存在多种语音干扰和多样的会议主题。</li>
<li>提出了G-SpatialNet模型，用于提高引导源分离信号的语音增强效果。</li>
<li>提出了TLS框架，包括时间对齐、级别对齐和信噪比过滤，生成信号级伪标签用于训练语音增强模型。</li>
<li>通过探索微调策略、数据增强和多模态信息，增强了预训练自动语音识别模型在会议场景下的性能。</li>
<li>系统在开发集和评估集上实现了较低的字符错误率。</li>
<li>与基线相比，系统性能有显著改善，获得了第二名的好成绩。</li>
<li>文中涉及的技术包括语音增强、自动语音识别、信号处理和深度学习等。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.24446">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-9c5064f1546d36213a254a86c71e5126.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-150d47cb3482ccfe59aff819c48e2a48.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-96f4b540d6eddfd296bdb56b0aa80e67.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9e3a4aaf3397ad6b1235b13ebc340afa.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Analysis-and-Evaluation-of-Synthetic-Data-Generation-in-Speech-Dysfluency-Detection"><a href="#Analysis-and-Evaluation-of-Synthetic-Data-Generation-in-Speech-Dysfluency-Detection" class="headerlink" title="Analysis and Evaluation of Synthetic Data Generation in Speech   Dysfluency Detection"></a>Analysis and Evaluation of Synthetic Data Generation in Speech   Dysfluency Detection</h2><p><strong>Authors:Jinming Zhang, Xuanru Zhou, Jiachen Lian, Shuhe Li, William Li, Zoe Ezzes, Rian Bogley, Lisa Wauters, Zachary Miller, Jet Vonk, Brittany Morin, Maria Gorno-Tempini, Gopala Anumanchipalli</strong></p>
<p>Speech dysfluency detection is crucial for clinical diagnosis and language assessment, but existing methods are limited by the scarcity of high-quality annotated data. Although recent advances in TTS model have enabled synthetic dysfluency generation, existing synthetic datasets suffer from unnatural prosody and limited contextual diversity. To address these limitations, we propose LLM-Dys – the most comprehensive dysfluent speech corpus with LLM-enhanced dysfluency simulation. This dataset captures 11 dysfluency categories spanning both word and phoneme levels. Building upon this resource, we improve an end-to-end dysfluency detection framework. Experimental validation demonstrates state-of-the-art performance. All data, models, and code are open-sourced at <a target="_blank" rel="noopener" href="https://github.com/Berkeley-Speech-Group/LLM-Dys">https://github.com/Berkeley-Speech-Group/LLM-Dys</a>. </p>
<blockquote>
<p>语音流畅性检测对于临床诊断和治疗语言评估至关重要，但现有方法受到高质量注释数据稀缺的限制。尽管最近文本到语音（TTS）模型的进步已经能够实现合成流畅性生成，但现有合成数据集存在语调不自然和上下文多样性有限的问题。为了解决这些局限性，我们提出了LLM-Dys——一个由大型语言模型增强流畅性模拟的最全面的流畅性语音语料库。该数据集涵盖了跨越单词和音素级别的11个流畅性问题类别。基于这一资源，我们改进了一个端到端的流畅性检测框架。实验验证证明了其卓越的性能。所有数据、模型和代码均公开开源于<a target="_blank" rel="noopener" href="https://github.com/Berkeley-Speech-Group/LLM-Dys%E3%80%82">https://github.com/Berkeley-Speech-Group/LLM-Dys。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.22029v2">PDF</a> Accepted by Interspeech 2025</p>
<p><strong>Summary</strong></p>
<p>本文介绍了语音流畅性检测在临床诊断和治疗语言评估中的重要性。现有方法受限于高质量标注数据的稀缺性。尽管最近文本转语音模型的进步使得可以生成合成流畅性问题，但现有合成数据集存在韵律不自然和上下文多样性有限的问题。为解决这些问题，我们提出了LLM-Dys——最全面的流畅性语音语料库，通过大型语言模型增强了流畅性模拟。该数据集涵盖了词汇和音素级别的11个流畅性问题类别。基于该资源，我们改进了端到端的流畅性检测框架。实验验证达到了先进水平的效果。所有相关数据、模型和代码已开源分享在：<a target="_blank" rel="noopener" href="https://github.com/Berkeley-Speech-Group/LLM-Dys%E3%80%82">https://github.com/Berkeley-Speech-Group/LLM-Dys。</a></p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>现有语音流畅性检测方法受限于高质量标注数据的稀缺性，对于临床诊断和语言评估具有重要意义。</li>
<li>TTS模型虽可实现合成流畅性的生成，但存在韵律不自然和上下文多样性不足的问题。</li>
<li>提出LLM-Dys作为最全面的流畅性语音语料库，通过大型语言模型增强流畅性模拟。</li>
<li>LLM-Dys涵盖了词汇和音素级别的多种流畅性问题类别。</li>
<li>基于LLM-Dys数据集改进了端到端的流畅性检测框架。</li>
<li>实验验证显示改进后的框架性能达到先进水平。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.22029">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-0f3a82ffc0080873fb3281bd5781c0ce.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f09ea430ad1b7c0b1ad598e4f7cab2fa.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-b030629238cfa23ed174c5da4e1f06be.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-eae4225d2805104617350cc554123266.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-d4c856b39137d632c5e621835da585e0.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-40f8c15c1bcb1f5c3aa3c4628cc2c9b9.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Introducing-voice-timbre-attribute-detection"><a href="#Introducing-voice-timbre-attribute-detection" class="headerlink" title="Introducing voice timbre attribute detection"></a>Introducing voice timbre attribute detection</h2><p><strong>Authors:Jinghao He, Zhengyan Sheng, Liping Chen, Kong Aik Lee, Zhen-Hua Ling</strong></p>
<p>This paper focuses on explaining the timbre conveyed by speech signals and introduces a task termed voice timbre attribute detection (vTAD). In this task, voice timbre is explained with a set of sensory attributes describing its human perception. A pair of speech utterances is processed, and their intensity is compared in a designated timbre descriptor. Moreover, a framework is proposed, which is built upon the speaker embeddings extracted from the speech utterances. The investigation is conducted on the VCTK-RVA dataset. Experimental examinations on the ECAPA-TDNN and FACodec speaker encoders demonstrated that: 1) the ECAPA-TDNN speaker encoder was more capable in the seen scenario, where the testing speakers were included in the training set; 2) the FACodec speaker encoder was superior in the unseen scenario, where the testing speakers were not part of the training, indicating enhanced generalization capability. The VCTK-RVA dataset and open-source code are available on the website <a target="_blank" rel="noopener" href="https://github.com/vTAD2025-Challenge/vTAD">https://github.com/vTAD2025-Challenge/vTAD</a>. </p>
<blockquote>
<p>本文重点解释语音信号所传递的音色，并介绍了一项称为音色属性检测（vTAD）的任务。在该任务中，音色通过一系列描述人类感知的感官属性来解释。对一对语音片段进行处理，并在指定的音色描述符中比较它们的强度。此外，提出了一个基于从语音片段中提取的说话者嵌入的框架。该研究在VCTK-RVA数据集上进行。对ECAPA-TDNN和FACodec说话人编码器的实验表明：1）在可见场景下，ECAPA-TDNN说话人编码器更具能力，其中测试说话人包含在训练集中；2）FACodec说话人编码器在未见过的情况下表现更好，其中测试说话人不是训练的一部分，表明其增强了泛化能力。VCTK-RVA数据集和开源代码可在网站<a target="_blank" rel="noopener" href="https://github.com/vTAD2025-Challenge/vTAD%E4%B8%8A%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/vTAD2025-Challenge/vTAD上找到。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.09661v2">PDF</a> arXiv admin note: substantial text overlap with arXiv:2505.09382</p>
<p><strong>总结</strong><br>    本文介绍了语音信号的音色特征，并定义了一个名为声音音色属性检测（vTAD）的任务。在该任务中，通过一系列描述音色的感知属性来解释音色。处理一段语音信号后，对比两段语音的音色强度在一个特定的音色描述符上。此外，提出了一种基于从语音中提取的说话者嵌入信息的框架，该研究在VCTK-RVA数据集上进行验证。通过ECAPA-TDNN和FACodec两种说话者编码器的实验表明：ECAPA-TDNN在测试说话者包含在训练集中的情况下表现更好；FACodec在测试说话者未参与训练的情况下表现出较强的泛化能力。相关数据集和开源代码可在<a target="_blank" rel="noopener" href="https://github.com/vTAD2025-Challenge/vTAD">网站链接</a>上找到。</p>
<p><strong>关键见解</strong></p>
<ol>
<li>文章介绍了语音信号的音色特征，并定义了声音音色属性检测（vTAD）的任务。这是一个新颖的角度来探讨语音信号处理。</li>
<li>提出一种对比两段语音信号在特定音色描述符上的音色强度的对比方法。这是vTAD任务的一个重要组成部分。</li>
<li>利用基于说话者嵌入信息的框架进行研究，为语音信号的处理和分析提供了新思路。</li>
<li>在VCTK-RVA数据集上进行了实验验证，该数据集可用于训练和测试vTAD相关的算法。</li>
<li>实验结果显示，ECAPA-TDNN在已知说话者场景下表现较好，而FACodec在未知说话者场景中具有更强的泛化能力。这对于选择适合的编码器提供了参考依据。</li>
<li>文章提供的开源代码和数据集有助于推动vTAD领域的研究进展。这对于语音信号处理领域的研究者是一大福音。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.09661">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-2159d5803006082d307d96e8fbbc1a16.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-8c5c45aac246baa2e6663d290a914b96.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1c773031d1d203f73e7ddefe11f68c8a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d70cf26033dbb1bafbc7379a8780f861.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-537c4bf826ec6929e8bd35313852d4d9.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Protecting-Your-Voice-Temporal-aware-Robust-Watermarking"><a href="#Protecting-Your-Voice-Temporal-aware-Robust-Watermarking" class="headerlink" title="Protecting Your Voice: Temporal-aware Robust Watermarking"></a>Protecting Your Voice: Temporal-aware Robust Watermarking</h2><p><strong>Authors:Yue Li, Weizhi Liu, Dongdong Lin, Hui Tian, Hongxia Wang</strong></p>
<p>The rapid advancement of generative models has led to the synthesis of real-fake ambiguous voices. To erase the ambiguity, embedding watermarks into the frequency-domain features of synthesized voices has become a common routine. However, the robustness achieved by choosing the frequency domain often comes at the expense of fine-grained voice features, leading to a loss of fidelity. Maximizing the comprehensive learning of time-domain features to enhance fidelity while maintaining robustness, we pioneer a \textbf{\underline{t}}emporal-aware \textbf{\underline{r}}ob\textbf{\underline{u}}st wat\textbf{\underline{e}}rmarking (\emph{True}) method for protecting the speech and singing voice. For this purpose, the integrated content-driven encoder is designed for watermarked waveform reconstruction, which is structurally lightweight. Additionally, the temporal-aware gated convolutional network is meticulously designed to bit-wise recover the watermark. Comprehensive experiments and comparisons with existing state-of-the-art methods have demonstrated the superior fidelity and vigorous robustness of the proposed \textit{True} achieving an average PESQ score of 4.63. </p>
<blockquote>
<p>生成模型的快速发展导致了真实与虚假模糊语音的合成。为了消除这种模糊性，将水印嵌入合成语音的频率域特征已成为一种常见做法。然而，通过选择频率域实现的稳健性往往以牺牲精细的语音特征为代价，导致保真度损失。我们最大化对时域特征的全面学习，以提高保真度并保持稳健性，开创了一种保护语音和歌声的时间感知稳健水印方法（True）。为此，设计了一个集成的内容驱动编码器进行水印波形重建，该编码器结构轻巧。此外，精心设计了时间感知门控卷积网络以位恢复水印。综合实验与现有先进方法的比较表明，所提出的True方法的保真度更高、稳健性更强，平均PESQ得分达到4.63。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.14832v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>随着生成模型的快速发展，合成语音的真实性和伪造性变得模糊。为了在频率域特征中为合成语音嵌入水印以消除歧义，同时保持稳健性并最大化时间域特征的全面学习以提高保真度，我们首创了一种称为“True”的时间感知稳健水印方法，用于保护语音和歌声。该方法设计了集成的内容驱动编码器和时间感知门控卷积网络，以进行水印嵌入和提取。实验证明，该方法在保真度和稳健性方面均优于现有技术，平均PESQ得分达到4.63。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>生成模型的快速发展导致合成语音的真实性和伪造性变得模糊。</li>
<li>在频率域特征中为合成语音嵌入水印是消除歧义的一种常见方法，但可能导致语音特征的损失。</li>
<li>为了提高保真度并维持稳健性，需要最大化时间域特征的全面学习。</li>
<li>提出了一种新的时间感知稳健水印方法“True”，用于保护语音和歌声。</li>
<li>“True”方法包括一个集成的内容驱动编码器和一个时间感知门控卷积网络。</li>
<li>实验证明，“True”方法在保真度和稳健性方面优于现有技术。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.14832">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-3447f83fabdc25babd1604a269b04d19.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1f072090c4dbcd947311e965fb165aaa.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a1035712b8ba8e877eb3ed6f8839e1f2.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b761cdcdebbe8a0cf3ad7b0890c05bc7.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-46ba874f769d21bfbfe4affac6dc059d.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="AnyEnhance-A-Unified-Generative-Model-with-Prompt-Guidance-and-Self-Critic-for-Voice-Enhancement"><a href="#AnyEnhance-A-Unified-Generative-Model-with-Prompt-Guidance-and-Self-Critic-for-Voice-Enhancement" class="headerlink" title="AnyEnhance: A Unified Generative Model with Prompt-Guidance and   Self-Critic for Voice Enhancement"></a>AnyEnhance: A Unified Generative Model with Prompt-Guidance and   Self-Critic for Voice Enhancement</h2><p><strong>Authors:Junan Zhang, Jing Yang, Zihao Fang, Yuancheng Wang, Zehua Zhang, Zhuo Wang, Fan Fan, Zhizheng Wu</strong></p>
<p>We introduce AnyEnhance, a unified generative model for voice enhancement that processes both speech and singing voices. Based on a masked generative model, AnyEnhance is capable of handling both speech and singing voices, supporting a wide range of enhancement tasks including denoising, dereverberation, declipping, super-resolution, and target speaker extraction, all simultaneously and without fine-tuning. AnyEnhance introduces a prompt-guidance mechanism for in-context learning, which allows the model to natively accept a reference speaker’s timbre. In this way, it could boost enhancement performance when a reference audio is available and enable the target speaker extraction task without altering the underlying architecture. Moreover, we also introduce a self-critic mechanism into the generative process for masked generative models, yielding higher-quality outputs through iterative self-assessment and refinement. Extensive experiments on various enhancement tasks demonstrate AnyEnhance outperforms existing methods in terms of both objective metrics and subjective listening tests. Demo audios are publicly available at <a target="_blank" rel="noopener" href="https://amphionspace.github.io/anyenhance/">https://amphionspace.github.io/anyenhance/</a>. </p>
<blockquote>
<p>我们介绍了AnyEnhance，这是一个统一的生成模型，用于处理语音和歌唱声音的声音增强。基于掩模生成模型，AnyEnhance能够同时处理语音和歌唱声音，支持广泛的增强任务，包括去噪、去混响、去剪辑、超分辨率和目标说话人提取，而无需微调。AnyEnhance引入了一种上下文学习中的提示引导机制，允许模型直接接受参考说话人的音色。这样，在有参考音频可用的情况下，它可以提高增强性能，并在不改变底层架构的情况下实现目标说话人提取任务。此外，我们还引入了生成过程中的自我批判机制，通过迭代自我评估和修正，产生更高质量的输出。在各种增强任务上的大量实验表明，AnyEnhance在客观指标和主观听觉测试方面优于现有方法。演示音频可在<a target="_blank" rel="noopener" href="https://amphionspace.github.io/anyenhance/%E5%85%AC%E5%BC%80%E8%AE%BF%E9%97%AE%E3%80%82">https://amphionspace.github.io/anyenhance/公开访问。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.15417v2">PDF</a> Accepted by IEEE&#x2F;ACM Transactions on Audio, Speech, and Language   Processing (TASLP) 2025</p>
<p><strong>Summary</strong></p>
<p>AnyEnhance是一个统一的生成模型，用于增强语音和歌声。它基于掩膜生成模型，能够同时处理各种增强任务，如去噪、去混响、去剪辑、超分辨率和目标语音提取，而无需微调。AnyEnhance引入了一种基于提示的引导机制，用于上下文学习，使模型能够自然地接受参考说话人的音色。此外，还引入了自我批判机制，通过迭代自我评估和细化，产生更高质量的输出。实验表明，AnyEnhance在客观指标和主观听觉测试方面均优于现有方法。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>AnyEnhance是一个统一的生成模型，适用于语音和歌声增强。</li>
<li>它基于掩膜生成模型，能够处理多种增强任务，包括去噪、去混响、去剪辑、超分辨率和目标语音提取。</li>
<li>AnyEnhance引入了基于提示的引导机制，允许模型接受参考说话人的音色，提升在有参考音频时的增强性能。</li>
<li>模型能够通过自我批判机制产生更高质量的输出，通过迭代自我评估和细化。</li>
<li>AnyEnhance在客观指标和主观听觉测试方面均表现出优异性能。</li>
<li>该模型的应用演示音频可在公众平台获取。</li>
<li>AnyEnhance的设计使其具有广泛的应用前景，包括语音识别、语音助手、音频编辑等领域。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.15417">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-ec634d16045dc1c317ca6beb1cdb6195.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-8e95577bf3e9620907dc0181b9691eb7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ee041506c94ef3ebea08ccc76a45c42a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-31d5eabdbfcfc60654fbe97864284362.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-9aa9519f11d537c0162b2d5eff58903a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-653cbcecd11f55fe2a7f78087d311a37.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        文章作者:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        文章链接:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-06-25/Speech/">https://kedreamix.github.io/Talk2Paper/Paper/2025-06-25/Speech/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        版权声明:
                    </i>
                </span>
                <span class="reprint-info">
                    本博客所有文章除特別声明外，均采用
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    许可协议。转载请注明来源
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Speech/">
                                    <span class="chip bg-color">Speech</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;上一篇</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-06-25/Face%20Swapping/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-a6e837ca3e62d58b0e338cae02b84733.jpg" class="responsive-img" alt="Face Swapping">
                        
                        <span class="card-title">Face Swapping</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Face Swapping 方向最新论文已更新，请持续关注 Update in 2025-06-25  TCDiff++ An End-to-end Trajectory-Controllable Diffusion Model for   Harmonious Music-Driven Group Choreography
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-06-25
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Face-Swapping/" class="post-category">
                                    Face Swapping
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Face-Swapping/">
                        <span class="chip bg-color">Face Swapping</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                下一篇&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-06-25/%E6%97%A0%E7%9B%91%E7%9D%A3_%E5%8D%8A%E7%9B%91%E7%9D%A3_%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-117f5883f28987041fc0cf3dbf810c4a.jpg" class="responsive-img" alt="无监督/半监督/对比学习">
                        
                        <span class="card-title">无监督/半监督/对比学习</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            无监督/半监督/对比学习 方向最新论文已更新，请持续关注 Update in 2025-06-25  Temporal Neural Cellular Automata Application to modeling of contrast   enhancement in breast MRI
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-06-25
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E6%97%A0%E7%9B%91%E7%9D%A3-%E5%8D%8A%E7%9B%91%E7%9D%A3-%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0/" class="post-category">
                                    无监督/半监督/对比学习
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E6%97%A0%E7%9B%91%E7%9D%A3-%E5%8D%8A%E7%9B%91%E7%9D%A3-%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0/">
                        <span class="chip bg-color">无监督/半监督/对比学习</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- 代码块功能依赖 -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- 代码语言 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- 代码块复制 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- 代码块收缩 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC 悬浮按钮. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* 修复文章卡片 div 的宽度. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // 切换TOC目录展开收缩的相关操作.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;站点总字数:&nbsp;<span
                        class="white-color">23523.1k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- 运行天数提醒. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // 区分是否有年份.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = '本站已运行 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                daysTip = '本站已運行 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = '本站已运行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = '本站已運行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="关注我的知乎: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">知</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- 搜索遮罩框 -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span>
            <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- 白天和黑夜主题 -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- 回到顶部按钮 -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // 只在桌面版网页启用特效
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- 雪花特效 -->
    

    <!-- 鼠标星星特效 -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--腾讯兔小巢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- 动态标签 -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Σ(っ °Д °;)っ诶，页面崩溃了嘛？", clearTimeout(st)) : (document.title = "φ(゜▽゜*)♪咦，又好了！", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
