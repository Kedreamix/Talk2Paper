<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Speech">
    <meta name="description" content="Speech æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-06-25  Context Biasing for Pronunciations-Orthography Mismatch in Automatic   Speech Recognition">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Speech | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-c20f6db03d3cb4937f7653b40c8f63c6.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Speech</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Speech/">
                                <span class="chip bg-color">Speech</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Speech/" class="post-category">
                                Speech
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-06-25
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-07-06
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    8.6k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    34 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-06-25-æ›´æ–°"><a href="#2025-06-25-æ›´æ–°" class="headerlink" title="2025-06-25 æ›´æ–°"></a>2025-06-25 æ›´æ–°</h1><h2 id="Context-Biasing-for-Pronunciations-Orthography-Mismatch-in-Automatic-Speech-Recognition"><a href="#Context-Biasing-for-Pronunciations-Orthography-Mismatch-in-Automatic-Speech-Recognition" class="headerlink" title="Context Biasing for Pronunciations-Orthography Mismatch in Automatic   Speech Recognition"></a>Context Biasing for Pronunciations-Orthography Mismatch in Automatic   Speech Recognition</h2><p><strong>Authors:Christian Huber, Alexander Waibel</strong></p>
<p>Neural sequence-to-sequence systems deliver state-of-the-art performance for automatic speech recognition. When using appropriate modeling units, e.g., byte-pair encoded characters, these systems are in principal open vocabulary systems. In practice, however, they often fail to recognize words not seen during training, e.g., named entities, acronyms, or domain-specific special words. To address this problem, many context biasing methods have been proposed; however, for words with a pronunciation-orthography mismatch, these methods may still struggle. We propose a method which allows corrections of substitution errors to improve the recognition accuracy of such challenging words. Users can add corrections on the fly during inference. We show that with this method we get a relative improvement in biased word error rate of up to 11%, while maintaining a competitive overall word error rate. </p>
<blockquote>
<p>ç¥ç»ç½‘ç»œåºåˆ—åˆ°åºåˆ—ç³»ç»Ÿåœ¨è‡ªåŠ¨è¯­éŸ³è¯†åˆ«æ–¹é¢è¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚åœ¨ä½¿ç”¨é€‚å½“çš„å»ºæ¨¡å•å…ƒï¼ˆä¾‹å¦‚ï¼Œå­—èŠ‚å¯¹ç¼–ç å­—ç¬¦ï¼‰æ—¶ï¼Œè¿™äº›ç³»ç»Ÿåœ¨ç†è®ºä¸Šéƒ½æ˜¯å¼€æ”¾è¯æ±‡ç³»ç»Ÿã€‚ç„¶è€Œï¼Œåœ¨å®è·µä¸­ï¼Œå®ƒä»¬å¾€å¾€æ— æ³•è¯†åˆ«åœ¨è®­ç»ƒæœŸé—´æœªè§è¿‡çš„è¯æ±‡ï¼Œä¾‹å¦‚ï¼Œå®ä½“åç§°ã€ç¼©å†™æˆ–ç‰¹å®šé¢†åŸŸçš„ç‰¹æ®Šè¯æ±‡ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œå·²ç»æå‡ºäº†è®¸å¤šä¸Šä¸‹æ–‡åå‘æ–¹æ³•ï¼›ä½†å¯¹äºå‘éŸ³å’Œä¹¦å†™ä¸åŒ¹é…çš„è¯æ±‡ï¼Œè¿™äº›æ–¹æ³•å¯èƒ½ä»ç„¶é¢ä¸´æŒ‘æˆ˜ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–¹æ³•ï¼Œå¯ä»¥çº æ­£æ›¿æ¢é”™è¯¯ï¼Œä»¥æé«˜æ­¤ç±»å…·æœ‰æŒ‘æˆ˜æ€§è¯æ±‡çš„è¯†åˆ«å‡†ç¡®ç‡ã€‚ç”¨æˆ·å¯ä»¥åœ¨æ¨ç†è¿‡ç¨‹ä¸­å³æ—¶æ·»åŠ ä¿®æ­£ã€‚æˆ‘ä»¬è¯æ˜ï¼Œä½¿ç”¨è¿™ç§æ–¹æ³•ï¼Œåœ¨åå‘è¯æ±‡é”™è¯¯ç‡æ–¹é¢å®ç°äº†é«˜è¾¾11%çš„ç›¸å¯¹æ”¹è¿›ï¼ŒåŒæ—¶ä¿æŒäº†å…·æœ‰ç«äº‰åŠ›çš„æ€»ä½“è¯æ±‡é”™è¯¯ç‡ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.18703v1">PDF</a> </p>
<p><strong>Summary</strong>ï¼šç¥ç»ç½‘ç»œåºåˆ—åˆ°åºåˆ—ç³»ç»Ÿåœ¨è‡ªåŠ¨è¯­éŸ³è¯†åˆ«æ–¹é¢è¡¨ç°å‡ºå“è¶Šæ€§èƒ½ã€‚é€šè¿‡ä½¿ç”¨é€‚å½“çš„å»ºæ¨¡å•å…ƒï¼ˆå¦‚å­—èŠ‚å¯¹ç¼–ç å­—ç¬¦ï¼‰ï¼Œè¿™äº›ç³»ç»ŸåŸåˆ™ä¸Šå…·æœ‰å¼€æ”¾è¯æ±‡è¡¨ã€‚ä½†åœ¨å®è·µä¸­ï¼Œå®ƒä»¬å¾€å¾€æ— æ³•è¯†åˆ«è®­ç»ƒæœŸé—´æœªè§è¿‡çš„å•è¯ï¼Œå¦‚å‘½åå®ä½“ã€ç¼©å†™æˆ–ç‰¹å®šé¢†åŸŸçš„ç‰¹æ®Šè¯æ±‡ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–¹æ³•ï¼Œå…è®¸åœ¨æ¨ç†è¿‡ç¨‹ä¸­å³æ—¶çº æ­£æ›¿ä»£é”™è¯¯ï¼Œä»¥æé«˜æ­¤ç±»å…·æœ‰æŒ‘æˆ˜æ€§å•è¯çš„è¯†åˆ«å‡†ç¡®æ€§ã€‚å®éªŒè¡¨æ˜ï¼Œä½¿ç”¨æ­¤æ–¹æ³•å¯å°†åå‘æ€§å•è¯é”™è¯¯ç‡ç›¸å¯¹æé«˜é«˜è¾¾11%ï¼ŒåŒæ—¶ä¿æŒæ•´ä½“å•è¯é”™è¯¯ç‡çš„ç«äº‰åŠ›ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>ç¥ç»ç½‘ç»œåºåˆ—åˆ°åºåˆ—ç³»ç»Ÿåœ¨è‡ªåŠ¨è¯­éŸ³è¯†åˆ«æ–¹é¢å…·æœ‰å“è¶Šæ€§èƒ½ã€‚</li>
<li>è¿™äº›ç³»ç»ŸåŸåˆ™ä¸Šå…·æœ‰å¼€æ”¾è¯æ±‡è¡¨ï¼Œä½†å®è·µä¸­å¯¹æœªè§è¿‡çš„å•è¯è¯†åˆ«èƒ½åŠ›æœ‰é™ã€‚</li>
<li>ç°æœ‰æ–¹æ³•å¯¹äºå­˜åœ¨è¯­éŸ³å’Œä¹¦å†™ä¸ä¸€è‡´çš„å•è¯å¯èƒ½ä»æœ‰å›°éš¾ã€‚</li>
<li>æå‡ºäº†ä¸€ç§å…è®¸å³æ—¶çº æ­£æ›¿ä»£é”™è¯¯çš„æ–¹æ³•ï¼Œä»¥æé«˜å…·æœ‰æŒ‘æˆ˜æ€§å•è¯çš„è¯†åˆ«å‡†ç¡®æ€§ã€‚</li>
<li>ç”¨æˆ·å¯ä»¥åœ¨æ¨ç†è¿‡ç¨‹ä¸­è¿›è¡Œå®æ—¶ä¿®æ­£ã€‚</li>
<li>æ­¤æ–¹æ³•å¯å°†åå‘æ€§å•è¯é”™è¯¯ç‡ç›¸å¯¹æé«˜é«˜è¾¾11%ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.18703">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-7aee237b44db3811c10250aa2e54b5b7.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-4d7ed34434e84ddae7bdb4c4baad5ffd.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-1abcf73e2cc7ed28d63368583ea04c4c.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="AI-Generated-Song-Detection-via-Lyrics-Transcripts"><a href="#AI-Generated-Song-Detection-via-Lyrics-Transcripts" class="headerlink" title="AI-Generated Song Detection via Lyrics Transcripts"></a>AI-Generated Song Detection via Lyrics Transcripts</h2><p><strong>Authors:Markus Frohmann, Elena V. Epure, Gabriel Meseguer-Brocal, Markus Schedl, Romain Hennequin</strong></p>
<p>The recent rise in capabilities of AI-based music generation tools has created an upheaval in the music industry, necessitating the creation of accurate methods to detect such AI-generated content. This can be done using audio-based detectors; however, it has been shown that they struggle to generalize to unseen generators or when the audio is perturbed. Furthermore, recent work used accurate and cleanly formatted lyrics sourced from a lyrics provider database to detect AI-generated music. However, in practice, such perfect lyrics are not available (only the audio is); this leaves a substantial gap in applicability in real-life use cases. In this work, we instead propose solving this gap by transcribing songs using general automatic speech recognition (ASR) models. We do this using several detectors. The results on diverse, multi-genre, and multi-lingual lyrics show generally strong detection performance across languages and genres, particularly for our best-performing model using Whisper large-v2 and LLM2Vec embeddings. In addition, we show that our method is more robust than state-of-the-art audio-based ones when the audio is perturbed in different ways and when evaluated on different music generators. Our code is available at <a target="_blank" rel="noopener" href="https://github.com/deezer/robust-AI-lyrics-detection">https://github.com/deezer/robust-AI-lyrics-detection</a>. </p>
<blockquote>
<p>è¿‘æœŸAIéŸ³ä¹ç”Ÿæˆå·¥å…·çš„èƒ½æå‡å¯¹å¼•èµ·äº†éŸ³ä¹äº§ä¸šçš„åŠ¨è¡ï¼Œå› æ­¤æ€¥éœ€åˆ›é€ å‡†ç¡®çš„æ–¹æ³•æ¥æ£€æµ‹è¿™ç±»AIç”Ÿæˆçš„å†…å®¹ã€‚è¿™å¯ä»¥é€šè¿‡åŸºäºéŸ³é¢‘çš„æ£€æµ‹å™¨æ¥å®ç°ï¼›ç„¶è€Œï¼Œå®ƒä»¬åœ¨æ–°ç”Ÿæˆçš„éŸ³ä¹æˆ–è€…éŸ³é¢‘å‘ç”Ÿå¹²æ‰°çš„æƒ…å†µä¸‹é€šå¸¸æ— æ³•è¿›è¡Œæœ‰æ•ˆçš„æ£€æµ‹ã€‚å¦å¤–ï¼Œæœ€è¿‘çš„ç ”ç©¶å·¥ä½œé‡‡ç”¨äº†æ¥è‡ªæ­Œè¯æä¾›æ•°æ®åº“çš„å‡†ç¡®å’Œæ ¼å¼è‰¯å¥½çš„æ­Œè¯æ¥æ£€æµ‹AIç”Ÿæˆçš„éŸ³ä¹ã€‚ä½†åœ¨å®é™…åº”ç”¨ä¸­ï¼Œå®Œç¾çš„æ­Œè¯é€šå¸¸æ— æ³•è·å–ï¼ˆåªæœ‰éŸ³é¢‘ï¼‰ï¼Œè¿™ä½¿å¾—åœ¨ç°å®åº”ç”¨åœºæ™¯ä¸­çš„é€‚ç”¨æ€§å­˜åœ¨å¾ˆå¤§å·®è·ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æè®®é€šè¿‡é‡‡ç”¨é€šç”¨çš„è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰æ¨¡å‹è¿›è¡Œæ­Œè¯è½¬å½•æ¥è§£å†³è¿™ä¸€å·®è·ã€‚æˆ‘ä»¬ä½¿ç”¨å¤šç§æ£€æµ‹å™¨å®ç°è¿™ä¸€ç‚¹ã€‚åœ¨ä¸åŒé£æ ¼å’Œå¤šè¯­è¨€çš„æ­Œè¯ä¸Šçš„ç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ¨¡å‹åœ¨ä¸åŒè¯­è¨€å’Œé£æ ¼ä¸Šçš„æ£€æµ‹æ€§èƒ½æ™®éå¼ºå¤§ï¼Œå°¤å…¶æ˜¯ä½¿ç”¨Whisper large-v2å’ŒLLM2VecåµŒå…¥çš„æœ€ä½³æ€§èƒ½æ¨¡å‹ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¯æ˜äº†å½“éŸ³é¢‘ä»¥ä¸åŒæ–¹å¼å—åˆ°å¹²æ‰°ä»¥åŠåœ¨ä¸åŒçš„éŸ³ä¹ç”Ÿæˆå™¨ä¸Šè¯„ä¼°æ—¶ï¼Œæˆ‘ä»¬çš„æ–¹æ³•æ¯”æœ€æ–°çš„éŸ³é¢‘æ£€æµ‹æ–¹æ³•æ›´ä¸ºç¨³å¥ã€‚æˆ‘ä»¬çš„ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/deezer/robust-AI-lyrics-detection%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/deezer/robust-AI-lyrics-detectionæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.18488v1">PDF</a> Accepted to ISMIR 2025</p>
<p><strong>Summary</strong><br>     æœ€è¿‘äººå·¥æ™ºèƒ½éŸ³ä¹ç”Ÿæˆå·¥å…·çš„è¿›æ­¥å¯¹éŸ³ä¹äº§ä¸šäº§ç”Ÿäº†å·¨å¤§å½±å“ï¼Œéœ€è¦åˆ›å»ºå‡†ç¡®çš„æ–¹æ³•æ¥æ£€æµ‹AIç”Ÿæˆçš„å†…å®¹ã€‚å°½ç®¡éŸ³é¢‘æ£€æµ‹å™¨å¯ç”¨äºæ­¤ç›®çš„ï¼Œä½†å®ƒä»¬å¯¹æ–°ç”Ÿæˆå™¨æˆ–æ‰°åŠ¨éŸ³é¢‘çš„æ³›åŒ–èƒ½åŠ›æœ‰é™ã€‚ä¸ºæ­¤ï¼Œæœ¬ç ”ç©¶æå‡ºåˆ©ç”¨é€šç”¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰æ¨¡å‹è¿›è¡Œæ­Œè¯è½¬å½•æ¥è§£å†³å®é™…åº”ç”¨ä¸­çš„é—®é¢˜ã€‚é€šè¿‡ä½¿ç”¨å¤šä¸ªæ£€æµ‹å™¨ï¼Œè¯¥æ¨¡å‹åœ¨è·¨è¯­è¨€å’Œå¤šé£æ ¼çš„éŸ³ä¹ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œå°¤å…¶æ˜¯ä½¿ç”¨Whisper large-v2å’ŒLLM2VecåµŒå…¥çš„æœ€ä½³æ¨¡å‹ã€‚æ­¤å¤–ï¼Œå½“éŸ³é¢‘ä»¥ä¸åŒæ–¹å¼å—åˆ°å¹²æ‰°æˆ–åœ¨ä¸åŒçš„éŸ³ä¹ç”Ÿæˆå™¨ä¸Šè¯„ä¼°æ—¶ï¼Œè¯¥æ–¹æ³•æ¯”æœ€æ–°çš„éŸ³é¢‘æ–¹æ³•æ›´ç¨³å¥ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>AIéŸ³ä¹ç”Ÿæˆå·¥å…·çš„è¿›æ­¥å¯¹éŸ³ä¹äº§ä¸šäº§ç”Ÿäº†æ·±è¿œå½±å“ã€‚</li>
<li>éŸ³é¢‘æ£€æµ‹å™¨åœ¨æ³›åŒ–æ–°ç”Ÿæˆå™¨å’Œæ‰°åŠ¨éŸ³é¢‘æ–¹é¢å­˜åœ¨å±€é™æ€§ã€‚</li>
<li>æœ¬ç ”ç©¶é€šè¿‡åˆ©ç”¨é€šç”¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰æ¨¡å‹è¿›è¡Œæ­Œè¯è½¬å½•æ¥è§£å†³å®é™…åº”ç”¨ä¸­çš„é—®é¢˜ã€‚</li>
<li>è¯¥æ¨¡å‹ä½¿ç”¨å¤šä¸ªæ£€æµ‹å™¨åœ¨è·¨è¯­è¨€å’Œå¤šé£æ ¼çš„éŸ³ä¹ä¸Šè¡¨ç°è‰¯å¥½ã€‚</li>
<li>æœ€ä½³æ¨¡å‹ä½¿ç”¨Whisper large-v2å’ŒLLM2VecåµŒå…¥æŠ€æœ¯ã€‚</li>
<li>è¯¥æ–¹æ³•æ¯”ç°æœ‰çš„éŸ³é¢‘æ–¹æ³•æ›´ç¨³å¥ï¼Œç‰¹åˆ«æ˜¯åœ¨å¤„ç†æ‰°åŠ¨éŸ³é¢‘å’Œä¸åŒéŸ³ä¹ç”Ÿæˆå™¨æ—¶ã€‚</li>
<li>ç ”ç©¶çš„ä»£ç å·²å…¬å¼€åœ¨GitHubä¸Šã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.18488">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-e95de3e6e02272e04c6c9d81027637d6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-bcc5c2b57fc1803f3eddbf4349a0fc8b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ed095d8a11541221e508c2162ea3dbac.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ea722d81e536beecaf13627941a77019.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d97d311c6beb6632a08b8f2a690e77c3.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-dd89225e8e7e04a2b3dcf7c915937310.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c20f6db03d3cb4937f7653b40c8f63c6.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Rethinking-Mean-Opinion-Scores-in-Speech-Quality-Assessment-Aggregation-through-Quantized-Distribution-Fitting"><a href="#Rethinking-Mean-Opinion-Scores-in-Speech-Quality-Assessment-Aggregation-through-Quantized-Distribution-Fitting" class="headerlink" title="Rethinking Mean Opinion Scores in Speech Quality Assessment: Aggregation   through Quantized Distribution Fitting"></a>Rethinking Mean Opinion Scores in Speech Quality Assessment: Aggregation   through Quantized Distribution Fitting</h2><p><strong>Authors:Yuto Kondo, Hirokazu Kameoka, Kou Tanaka, Takuhiro Kaneko</strong></p>
<p>Speech quality assessment (SQA) aims to evaluate the quality of speech samples without relying on time-consuming listener questionnaires. Recent efforts have focused on training neural-based SQA models to predict the mean opinion score (MOS) of speech samples produced by text-to-speech or voice conversion systems. This paper targets the enhancement of MOS prediction modelsâ€™ performance. We propose a novel score aggregation method to address the limitations of conventional annotations for MOS, which typically involve ratings on a scale from 1 to 5. Our method is based on the hypothesis that annotators internally consider continuous scores and then choose the nearest discrete rating. By modeling this process, we approximate the generative distribution of ratings by quantizing the latent continuous distribution. We then use the peak of this latent distribution, estimated through the loss between the quantized distribution and annotated ratings, as a new representative value instead of MOS. Experimental results demonstrate that substituting MOSNetâ€™s predicted target with this proposed value improves prediction performance. </p>
<blockquote>
<p>è¯­éŸ³è´¨é‡è¯„ä¼°ï¼ˆSQAï¼‰æ—¨åœ¨è¯„ä¼°è¯­éŸ³æ ·æœ¬çš„è´¨é‡ï¼Œè€Œä¸ä¾èµ–äºè€—æ—¶çš„å¬ä¼—é—®å·è°ƒæŸ¥ã€‚è¿‘æœŸçš„ç ”ç©¶åŠªåŠ›é›†ä¸­åœ¨è®­ç»ƒåŸºäºç¥ç»ç½‘ç»œçš„SQAæ¨¡å‹ä¸Šï¼Œä»¥é¢„æµ‹æ–‡æœ¬åˆ°è¯­éŸ³æˆ–è¯­éŸ³è½¬æ¢ç³»ç»Ÿäº§ç”Ÿçš„è¯­éŸ³æ ·æœ¬çš„å¹³å‡æ„è§å¾—åˆ†ï¼ˆMOSï¼‰ã€‚æœ¬æ–‡é’ˆå¯¹æé«˜MOSé¢„æµ‹æ¨¡å‹çš„æ€§èƒ½ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„è¯„åˆ†èšåˆæ–¹æ³•ï¼Œä»¥è§£å†³ä¼ ç»ŸMOSæ ‡æ³¨çš„å±€é™æ€§ï¼Œä¼ ç»Ÿæ ‡æ³¨é€šå¸¸æ¶‰åŠä»1åˆ°5çš„è¯„åˆ†èŒƒå›´ã€‚æˆ‘ä»¬çš„æ–¹æ³•åŸºäºè¿™æ ·çš„å‡è®¾ï¼šæ³¨é‡Šè€…åœ¨å†…éƒ¨è€ƒè™‘è¿ç»­åˆ†æ•°ï¼Œç„¶åé€‰æ‹©æœ€è¿‘çš„ç¦»æ•£è¯„åˆ†ã€‚é€šè¿‡å¯¹è¿™ä¸€è¿‡ç¨‹è¿›è¡Œå»ºæ¨¡ï¼Œæˆ‘ä»¬é€šè¿‡é‡åŒ–æ½œåœ¨çš„è¿ç»­åˆ†å¸ƒæ¥è¿‘ä¼¼è¯„åˆ†çš„ç”Ÿæˆåˆ†å¸ƒã€‚ç„¶åï¼Œæˆ‘ä»¬é€šè¿‡é‡åŒ–åˆ†å¸ƒä¸æ³¨é‡Šè¯„åˆ†ä¹‹é—´çš„æŸå¤±æ¥ä¼°è®¡æ½œåœ¨åˆ†å¸ƒçš„å³°å€¼ï¼Œå¹¶å°†å…¶ä½œä¸ºæ–°çš„ä»£è¡¨å€¼æ¥ä»£æ›¿MOSã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œç”¨è¯¥æè®®çš„å€¼æ›¿æ¢MOSNetçš„é¢„æµ‹ç›®æ ‡å¯ä»¥æé«˜é¢„æµ‹æ€§èƒ½ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.18307v1">PDF</a> Accepted on ICASSP 2025</p>
<p><strong>æ€»ç»“</strong></p>
<p>æœ¬æ–‡å…³æ³¨è¯­éŸ³è´¨é‡è¯„ä¼°ï¼ˆSQAï¼‰ä¸­å‡å€¼æ„è§å¾—åˆ†ï¼ˆMOSï¼‰é¢„æµ‹æ¨¡å‹çš„æ€§èƒ½æå‡ã€‚é’ˆå¯¹ä¼ ç»ŸMOSæ ‡æ³¨æ–¹æ³•çš„å±€é™æ€§ï¼Œæå‡ºäº†ä¸€ç§æ–°çš„è¯„åˆ†èšåˆæ–¹æ³•ã€‚è¯¥æ–¹æ³•åŸºäºæ ‡æ³¨è€…å®é™…ä¸Šè€ƒè™‘çš„æ˜¯è¿ç»­åˆ†æ•°ï¼Œç„¶åé€‰æ‹©æœ€æ¥è¿‘çš„ç¦»æ•£è¯„åˆ†è¿™ä¸€å‡è®¾ã€‚é€šè¿‡é‡åŒ–æ½œåœ¨çš„è¿ç»­åˆ†å¸ƒæ¥è¿‘ä¼¼è¯„åˆ†çš„ç”Ÿæˆåˆ†å¸ƒï¼Œå¹¶ä½¿ç”¨æ­¤æ½œåœ¨åˆ†å¸ƒçš„å³°å€¼ï¼ˆé€šè¿‡é‡åŒ–åˆ†å¸ƒä¸æ ‡æ³¨è¯„åˆ†ä¹‹é—´çš„æŸå¤±æ¥ä¼°è®¡ï¼‰ä½œä¸ºæ–°çš„ä»£è¡¨æ€§å€¼ï¼Œä»¥æ”¹è¿›MOSé¢„æµ‹æ¨¡å‹çš„æ€§èƒ½ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>è¯­éŸ³è´¨é‡è¯„ä¼°ï¼ˆSQAï¼‰æ—¨åœ¨è¯„ä¼°è¯­éŸ³æ ·æœ¬çš„è´¨é‡ï¼Œä¸”ä¸ä¾èµ–è€—æ—¶çš„äººåŠ›è°ƒæŸ¥é—®å·ã€‚</li>
<li>è¿‘æœŸç ”ç©¶è‡´åŠ›äºè®­ç»ƒåŸºäºç¥ç»ç½‘ç»œçš„SQAæ¨¡å‹ï¼Œä»¥é¢„æµ‹æ–‡æœ¬åˆ°è¯­éŸ³æˆ–è¯­éŸ³è½¬æ¢ç³»ç»Ÿäº§ç”Ÿçš„è¯­éŸ³æ ·æœ¬çš„å‡å€¼æ„è§å¾—åˆ†ï¼ˆMOSï¼‰ã€‚</li>
<li>æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„è¯„åˆ†èšåˆæ–¹æ³•ï¼Œä»¥è§£å†³ä¼ ç»ŸMOSæ ‡æ³¨æ–¹æ³•çš„å±€é™æ€§ã€‚</li>
<li>è¯¥æ–¹æ³•åŸºäºæ ‡æ³¨è€…å®é™…ä¸Šåœ¨è€ƒè™‘è¿ç»­åˆ†æ•°ç„¶åé€‰æ‹©æœ€æ¥è¿‘çš„ç¦»æ•£è¯„åˆ†çš„å‡è®¾ã€‚</li>
<li>é€šè¿‡é‡åŒ–æ½œåœ¨çš„è¿ç»­åˆ†å¸ƒæ¥è¿‘ä¼¼è¯„åˆ†çš„ç”Ÿæˆåˆ†å¸ƒã€‚</li>
<li>ä½¿ç”¨æ½œåœ¨åˆ†å¸ƒçš„å³°å€¼ä½œä¸ºæ–°çš„ä»£è¡¨æ€§å€¼ï¼Œé€šè¿‡æŸå¤±å‡½æ•°ä¼°è®¡è¿™ä¸€å³°å€¼ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.18307">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-2f8c9dff4eaee3a6e7c92349a82ab85f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a2cae77620a35907ccf3524eb30b62e1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-49421c9d1faa570108dca21ec6af0171.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Breaking-the-Transcription-Bottleneck-Fine-tuning-ASR-Models-for-Extremely-Low-Resource-Fieldwork-Languages"><a href="#Breaking-the-Transcription-Bottleneck-Fine-tuning-ASR-Models-for-Extremely-Low-Resource-Fieldwork-Languages" class="headerlink" title="Breaking the Transcription Bottleneck: Fine-tuning ASR Models for   Extremely Low-Resource Fieldwork Languages"></a>Breaking the Transcription Bottleneck: Fine-tuning ASR Models for   Extremely Low-Resource Fieldwork Languages</h2><p><strong>Authors:Siyu Liang, Gina-Anne Levow</strong></p>
<p>Automatic Speech Recognition (ASR) has reached impressive accuracy for high-resource languages, yet its utility in linguistic fieldwork remains limited. Recordings collected in fieldwork contexts present unique challenges, including spontaneous speech, environmental noise, and severely constrained datasets from under-documented languages. In this paper, we benchmark the performance of two fine-tuned multilingual ASR models, MMS and XLS-R, on five typologically diverse low-resource languages with control of training data duration. Our findings show that MMS is best suited when extremely small amounts of training data are available, whereas XLS-R shows parity performance once training data exceed one hour. We provide linguistically grounded analysis for further provide insights towards practical guidelines for field linguists, highlighting reproducible ASR adaptation approaches to mitigate the transcription bottleneck in language documentation. </p>
<blockquote>
<p>è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰å¯¹äºèµ„æºä¸°å¯Œçš„è¯­è¨€å·²ç»è¾¾åˆ°äº†ä»¤äººå°è±¡æ·±åˆ»çš„å‡†ç¡®æ€§ï¼Œç„¶è€Œå…¶åœ¨è¯­è¨€é¢†åŸŸå·¥ä½œä¸­çš„å®ç”¨æ€§ä»ç„¶æœ‰é™ã€‚åœ¨ç°åœºå·¥ä½œç¯å¢ƒä¸­æ”¶é›†çš„å½•éŸ³å‘ˆç°å‡ºç‹¬ç‰¹çš„æŒ‘æˆ˜ï¼ŒåŒ…æ‹¬è‡ªç„¶å£è¯­ã€ç¯å¢ƒå™ªå£°ä»¥åŠæ¥è‡ªè®°å½•ä¸å…¨çš„è¯­è¨€çš„ä¸¥é‡å—é™æ•°æ®é›†ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬åœ¨æ§åˆ¶è®­ç»ƒæ•°æ®æ—¶é—´é•¿åº¦çš„æƒ…å†µä¸‹ï¼Œå¯¹ä¸¤ç§ç»è¿‡å¾®è°ƒçš„å¤šè¯­ç§ASRæ¨¡å‹MMSå’ŒXLS-Rè¿›è¡Œäº†äº”ç§ç±»å‹ä¸°å¯Œä¸”èµ„æºåŒ®ä¹çš„è¯­è¨€çš„åŸºå‡†æµ‹è¯•ã€‚æˆ‘ä»¬çš„ç ”ç©¶å‘ç°ï¼Œå½“å¯ç”¨è®­ç»ƒæ•°æ®é‡æå°çš„æ—¶å€™ï¼ŒMMSæœ€ä¸ºåˆé€‚ï¼›è€Œå½“è®­ç»ƒæ•°æ®è¶…è¿‡ä¸€ä¸ªå°æ—¶æ—¶ï¼ŒXLS-Rè¡¨ç°å‡ºå¹³ç­‰çš„æ€§èƒ½ã€‚æˆ‘ä»¬æä¾›äº†åŸºäºè¯­è¨€å­¦çš„åˆ†æï¼Œä¸ºè¿›ä¸€æ­¥ä¸ºè¯­è¨€å­¦å®¶æä¾›å®é™…æŒ‡å¯¼çš„è§è§£ï¼Œå¼ºè°ƒå¯å¤åˆ¶çš„ASRé€‚åº”æ–¹æ³•æ¥å‡è½»è¯­è¨€è®°å½•ä¸­çš„è½¬å½•ç“¶é¢ˆé—®é¢˜ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.17459v1">PDF</a> </p>
<p><strong>Summary</strong>ï¼š<br>è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰åœ¨é«˜èµ„æºè¯­è¨€ä¸Šçš„å‡†ç¡®æ€§ä»¤äººå°è±¡æ·±åˆ»ï¼Œä½†åœ¨è¯­è¨€é¢†åŸŸä¸­çš„åº”ç”¨ä»ç„¶æœ‰é™ã€‚ç‰¹åˆ«æ˜¯åœ¨è¯­è¨€ç”°é‡å·¥ä½œä¸­æ”¶é›†çš„å½•éŸ³é¢ä¸´è¯¸å¤šæŒ‘æˆ˜ï¼Œå¦‚è‡ªå‘è¯­éŸ³ã€ç¯å¢ƒå™ªå£°ä»¥åŠæ¥è‡ªæœªè®°å½•è¯­è¨€çš„æœ‰é™æ•°æ®é›†ã€‚æœ¬æ–‡å¯¹æ¯”äº†ä¸¤ç§ç»è¿‡å¾®è°ƒçš„å¤šè¯­ç§ASRæ¨¡å‹MMSå’ŒXLS-Rï¼Œåœ¨äº”ç§ç±»å‹å¤šæ ·ä½†èµ„æºç¨€ç¼ºçš„è¯­è¨€ç¯å¢ƒä¸‹çš„æ€§èƒ½ã€‚ç»“æœè¡¨æ˜ï¼Œå½“å¯ç”¨è®­ç»ƒæ•°æ®æä¸ºæœ‰é™æ—¶ï¼ŒMMSè¡¨ç°æœ€ä½³ï¼Œå½“è®­ç»ƒæ•°æ®è¶…è¿‡ä¸€å°æ—¶æ—¶ï¼ŒXLS-Rè¡¨ç°å‡ºåŒç­‰æ€§èƒ½ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜æä¾›äº†è¯­è¨€å­¦çš„è§è§£å’Œå®è·µæŒ‡å—ï¼Œå¸®åŠ©è¯­è¨€å·¥ä½œè€…æ›´å¥½åœ°ç†è§£å¦‚ä½•è§£å†³å®é™…åº”ç”¨ä¸­çš„è½¬å½•ç“¶é¢ˆé—®é¢˜ã€‚æ€»ä½“è€Œè¨€ï¼Œè¿™äº›å»ºè®®å°†æå¤§åœ°ä¿ƒè¿›è¯­è¨€çš„è®°å½•å’Œä¿æŠ¤ã€‚é€šè¿‡é‡æ–°å»ºç«‹æ•°æ®é›†å’Œå…¶ä»–ASRé€‚åº”æ€§ç­–ç•¥çš„å®æ–½ç­–ç•¥çš„è¿ç”¨æœ‰æ•ˆå‡å°‘è½¬å½•çš„ç“¶é¢ˆé—®é¢˜ã€‚æˆ‘ä»¬å¼ºè°ƒäº†å®é™…åº”ç”¨ä¸­å¯å†ç”ŸASRé€‚åº”æ€§æ–¹æ³•çš„ä¼˜ç‚¹ï¼Œä»¥åŠå®ƒåœ¨è§£å†³è½¬å½•ç“¶é¢ˆæ–¹é¢çš„ä½œç”¨ï¼Œè¿™å°†æœ‰åŠ©äºæ”¹å–„æœªæ¥ASRæ¨¡å‹çš„æ€§èƒ½å’Œæ•ˆç‡ã€‚é€šè¿‡é€‚å½“çš„æ”¹è¿›å’Œè°ƒæ•´ç­–ç•¥çš„è¿ç”¨èƒ½å¤Ÿè¿›ä¸€æ­¥æ¨åŠ¨ASRæŠ€æœ¯åœ¨è¯­è¨€ç”°é‡å·¥ä½œä¸­å‘æŒ¥æ›´å¤§çš„ä½œç”¨ã€‚ </p>
<p><strong>Key Takeaways</strong>:</p>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.17459">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-bf27e5e8ba41a30d81d751ca702f6f67.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4a7eb23a2b02142ff71c1419ba381ddb.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-255454f3b185607a4ce02f06a77ca8b3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-36ca7664213779f4f18bac2f011b2395.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-054d595a6d2e157ce0c4b9fed212997a.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="SuPseudo-A-Pseudo-supervised-Learning-Method-for-Neural-Speech-Enhancement-in-Far-field-Speech-Recognition"><a href="#SuPseudo-A-Pseudo-supervised-Learning-Method-for-Neural-Speech-Enhancement-in-Far-field-Speech-Recognition" class="headerlink" title="SuPseudo: A Pseudo-supervised Learning Method for Neural Speech   Enhancement in Far-field Speech Recognition"></a>SuPseudo: A Pseudo-supervised Learning Method for Neural Speech   Enhancement in Far-field Speech Recognition</h2><p><strong>Authors:Longjie Luo, Lin Li, Qingyang Hong</strong></p>
<p>Due to the lack of target speech annotations in real-recorded far-field conversational datasets, speech enhancement (SE) models are typically trained on simulated data. However, the trained models often perform poorly in real-world conditions, hindering their application in far-field speech recognition. To address the issue, we (a) propose direct sound estimation (DSE) to estimate the oracle direct sound of real-recorded data for SE; and (b) present a novel pseudo-supervised learning method, SuPseudo, which leverages DSE-estimates as pseudo-labels and enables SE models to directly learn from and adapt to real-recorded data, thereby improving their generalization capability. Furthermore, an SE model called FARNET is designed to fully utilize SuPseudo. Experiments on the MISP2023 corpus demonstrate the effectiveness of SuPseudo, and our system significantly outperforms the previous state-of-the-art. A demo of our method can be found at <a target="_blank" rel="noopener" href="https://eellj.github.io/SuPseudo/">https://EeLLJ.github.io/SuPseudo/</a>. </p>
<blockquote>
<p>ç”±äºçœŸå®å½•åˆ¶çš„è¿œè·ç¦»å¯¹è¯æ•°æ®é›†ä¸­ç¼ºä¹ç›®æ ‡è¯­éŸ³æ³¨é‡Šï¼Œè¯­éŸ³å¢å¼ºï¼ˆSEï¼‰æ¨¡å‹é€šå¸¸æ˜¯åœ¨æ¨¡æ‹Ÿæ•°æ®ä¸Šè¿›è¡Œè®­ç»ƒçš„ã€‚ç„¶è€Œï¼Œåœ¨çœŸå®ä¸–ç•Œæ¡ä»¶ä¸‹ï¼Œè®­ç»ƒå¥½çš„æ¨¡å‹çš„æ€§èƒ½å¾€å¾€ä¸ä½³ï¼Œé˜»ç¢äº†å®ƒä»¬åœ¨è¿œè·ç¦»è¯­éŸ³è¯†åˆ«ä¸­çš„åº”ç”¨ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬ï¼ˆaï¼‰æå‡ºäº†ç›´æ¥å£°éŸ³ä¼°è®¡ï¼ˆDSEï¼‰ï¼Œä»¥ä¼°è®¡çœŸå®å½•åˆ¶æ•°æ®çš„ç†æƒ³ç›´è¾¾å£°éŸ³ï¼Œç”¨äºè¯­éŸ³å¢å¼ºï¼›ï¼ˆbï¼‰æå‡ºäº†ä¸€ç§æ–°å‹ä¼ªç›‘ç£å­¦ä¹ æ–¹æ³•SuPseudoï¼Œå®ƒåˆ©ç”¨DSEä¼°è®¡ä½œä¸ºä¼ªæ ‡ç­¾ï¼Œä½¿SEæ¨¡å‹èƒ½å¤Ÿç›´æ¥ä»çœŸå®æ•°æ®ä¸­å­¦ä¹ å’Œé€‚åº”ï¼Œä»è€Œæé«˜å…¶æ³›åŒ–èƒ½åŠ›ã€‚æ­¤å¤–ï¼Œè¿˜è®¾è®¡äº†ä¸€ä¸ªåä¸ºFARNETçš„SEæ¨¡å‹ï¼Œä»¥å……åˆ†åˆ©ç”¨SuPseudoã€‚åœ¨MISP2023è¯­æ–™åº“ä¸Šçš„å®éªŒè¯æ˜äº†SuPseudoçš„æœ‰æ•ˆæ€§ï¼Œæˆ‘ä»¬çš„ç³»ç»Ÿæ˜¾è‘—ä¼˜äºä¹‹å‰çš„æœ€å…ˆè¿›æ°´å¹³ã€‚æœ‰å…³æˆ‘ä»¬æ–¹æ³•çš„æ¼”ç¤ºå¯åœ¨<a target="_blank" rel="noopener" href="https://eellj.github.io/SuPseudo/">https://EeLLJ.github.io/SuPseudo/</a>æ‰¾åˆ°ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.24450v2">PDF</a> Accepted by InterSpeech 2025</p>
<p><strong>Summary</strong></p>
<p>é’ˆå¯¹è¿œåœºè¯­éŸ³è¯†åˆ«ä¸­çœŸå®å½•éŸ³æ•°æ®é›†ç›®æ ‡è¯­éŸ³æ³¨é‡Šç¼ºå¤±çš„é—®é¢˜ï¼Œç°æœ‰çš„è¯­éŸ³å¢å¼ºï¼ˆSEï¼‰æ¨¡å‹é€šå¸¸åœ¨æ¨¡æ‹Ÿæ•°æ®ä¸Šè¿›è¡Œè®­ç»ƒï¼Œä½†åœ¨ç°å®ç¯å¢ƒä¸­çš„è¡¨ç°å¾€å¾€ä¸ä½³ã€‚æœ¬æ–‡æå‡ºäº†ä¸¤ç§è§£å†³æ–¹æ¡ˆï¼šï¼ˆaï¼‰æå‡ºç›´æ¥å£°éŸ³ä¼°è®¡ï¼ˆDSEï¼‰ï¼Œç”¨äºä¼°è®¡çœŸå®å½•éŸ³æ•°æ®çš„ç†æƒ³ç›´æ¥å£°éŸ³ï¼Œç”¨äºè¯­éŸ³å¢å¼ºï¼›ï¼ˆbï¼‰æå‡ºäº†ä¸€ç§æ–°çš„ä¼ªç›‘ç£å­¦ä¹ æ–¹æ³•SuPseudoï¼Œå®ƒåˆ©ç”¨DSEä¼°è®¡ä½œä¸ºä¼ªæ ‡ç­¾ï¼Œä½¿SEæ¨¡å‹èƒ½å¤Ÿç›´æ¥ä»çœŸå®æ•°æ®ä¸­å­¦ä¹ å’Œé€‚åº”ï¼Œä»è€Œæé«˜å…¶æ³›åŒ–èƒ½åŠ›ã€‚å®éªŒè¡¨æ˜ï¼ŒSuPseudoæ–¹æ³•æœ‰æ•ˆï¼Œä½¿ç”¨å…¶è®¾è®¡çš„FARNETæ¨¡å‹åœ¨MISP2023è¯­æ–™åº“ä¸Šçš„è¡¨ç°ä¼˜äºç°æœ‰æŠ€æœ¯ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>çœŸå®å½•éŸ³çš„è¿œåœºè¯­éŸ³è¯†åˆ«æ•°æ®é›†ç¼ºä¹ç›®æ ‡è¯­éŸ³æ³¨é‡Šï¼Œå¯¼è‡´è¯­éŸ³å¢å¼ºï¼ˆSEï¼‰æ¨¡å‹åœ¨æ¨¡æ‹Ÿæ•°æ®ä¸Šè®­ç»ƒåï¼Œç°å®ç¯å¢ƒä¸­çš„è¡¨ç°ä¸ä½³ã€‚</li>
<li>æå‡ºç›´æ¥å£°éŸ³ä¼°è®¡ï¼ˆDSEï¼‰æ–¹æ³•ï¼Œç”¨ä»¥ä¼°è®¡çœŸå®å½•éŸ³æ•°æ®çš„ç†æƒ³ç›´æ¥å£°éŸ³ï¼Œä¸ºè§£å†³æ­¤é—®é¢˜æ‰“ä¸‹åŸºç¡€ã€‚</li>
<li>ä»‹ç»äº†æ–°çš„ä¼ªç›‘ç£å­¦ä¹ æ–¹æ³•SuPseudoï¼Œå®ƒåˆ©ç”¨DSEçš„ä¼°è®¡ç»“æœä½œä¸ºä¼ªæ ‡ç­¾ï¼Œä½¿SEæ¨¡å‹èƒ½å¤Ÿç›´æ¥ä»çœŸå®æ•°æ®ä¸­å­¦ä¹ å’Œé€‚åº”ã€‚</li>
<li>SuPseudoæ–¹æ³•æé«˜äº†SEæ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ã€‚</li>
<li>è®¾è®¡äº†åä¸ºFARNETçš„SEæ¨¡å‹ï¼Œä»¥å……åˆ†åˆ©ç”¨SuPseudoæ–¹æ³•ã€‚</li>
<li>åœ¨MISP2023è¯­æ–™åº“ä¸Šçš„å®éªŒè¯æ˜äº†SuPseudoæ–¹æ³•çš„æœ‰æ•ˆæ€§ï¼Œä¸”å…¶è¡¨ç°ä¼˜äºç°æœ‰æŠ€æœ¯ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.24450">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-583ba48e097f009ffff8dad37dd4cb99.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-efc60436bae0475679a8ca26eaeee1da.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e5ee31987b8703bd3aea1af33be50171.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-19307c4333638b41bcbba90f5d8cb337.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Pseudo-Labels-based-Neural-Speech-Enhancement-for-the-AVSR-Task-in-the-MISP-Meeting-Challenge"><a href="#Pseudo-Labels-based-Neural-Speech-Enhancement-for-the-AVSR-Task-in-the-MISP-Meeting-Challenge" class="headerlink" title="Pseudo Labels-based Neural Speech Enhancement for the AVSR Task in the   MISP-Meeting Challenge"></a>Pseudo Labels-based Neural Speech Enhancement for the AVSR Task in the   MISP-Meeting Challenge</h2><p><strong>Authors:Longjie Luo, Shenghui Lu, Lin Li, Qingyang Hong</strong></p>
<p>This paper presents our system for the MISP-Meeting Challenge Track 2. The primary difficulty lies in the dataset, which contains strong background noise, reverberation, overlapping speech, and diverse meeting topics. To address these issues, we (a) designed G-SpatialNet, a speech enhancement (SE) model to improve Guided Source Separation (GSS) signals; (b) proposed TLS, a framework comprising time alignment, level alignment, and signal-to-noise ratio filtering, to generate signal-level pseudo labels for real-recorded far-field audio data, thereby facilitating SE modelsâ€™ training; and (c) explored fine-tuning strategies, data augmentation, and multimodal information to enhance the performance of pre-trained Automatic Speech Recognition (ASR) models in meeting scenarios. Finally, our system achieved character error rates (CERs) of 5.44% and 9.52% on the Dev and Eval sets, respectively, with relative improvements of 64.8% and 52.6% over the baseline, securing second place. </p>
<blockquote>
<p>æœ¬æ–‡ä»‹ç»äº†æˆ‘ä»¬åœ¨MISP-MeetingæŒ‘æˆ˜èµ›è½¨é“2ä¸­çš„ç³»ç»Ÿã€‚ä¸»è¦éš¾ç‚¹åœ¨äºæ•°æ®é›†ï¼Œå®ƒåŒ…å«å¼ºçƒˆçš„èƒŒæ™¯å™ªå£°ã€å›å£°ã€è¯­éŸ³é‡å å’Œå¤šæ ·çš„ä¼šè®®ä¸»é¢˜ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬ï¼ˆaï¼‰è®¾è®¡äº†G-SpatialNetï¼Œè¿™æ˜¯ä¸€ä¸ªè¯­éŸ³å¢å¼ºï¼ˆSEï¼‰æ¨¡å‹ï¼Œç”¨äºæ”¹è¿›å¯¼å‘æºåˆ†ç¦»ï¼ˆGSSï¼‰ä¿¡å·ï¼›ï¼ˆbï¼‰æå‡ºTLSï¼Œä¸€ä¸ªåŒ…å«æ—¶é—´å¯¹é½ã€æ°´å¹³å¯¹é½å’Œä¿¡å™ªæ¯”è¿‡æ»¤çš„æ¡†æ¶ï¼Œç”¨äºä¸ºçœŸå®å½•åˆ¶çš„è¿œè·ç¦»éŸ³é¢‘æ•°æ®ç”Ÿæˆä¿¡å·çº§ä¼ªæ ‡ç­¾ï¼Œä»è€Œå¸®åŠ©è®­ç»ƒSEæ¨¡å‹ï¼›ï¼ˆcï¼‰æ¢ç´¢äº†å¾®è°ƒç­–ç•¥ã€æ•°æ®å¢å¼ºå’Œå¤šæ¨¡æ€ä¿¡æ¯ï¼Œä»¥æé«˜é¢„è®­ç»ƒçš„è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰æ¨¡å‹åœ¨ä¼šè®®åœºæ™¯ä¸­çš„æ€§èƒ½ã€‚æœ€ç»ˆï¼Œæˆ‘ä»¬çš„ç³»ç»Ÿåœ¨å¼€å‘é›†å’Œè¯„ä¼°é›†ä¸Šåˆ†åˆ«å®ç°äº†5.44%å’Œ9.52%çš„å­—ç¬¦é”™è¯¯ç‡ï¼ˆCERsï¼‰ï¼Œç›¸å¯¹äºåŸºçº¿æœ‰64.8%å’Œ52.6%çš„ç›¸å¯¹æ”¹è¿›ï¼Œè·å¾—äº†ç¬¬äºŒåã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.24446v2">PDF</a> Accepted by InterSpeech 2025</p>
<p><strong>Summary</strong>ï¼š</p>
<p>æœ¬æ–‡ä»‹ç»äº†é’ˆå¯¹MISP-MeetingæŒ‘æˆ˜èµ›é“2çš„ç³»ç»Ÿã€‚ä¸»è¦éš¾ç‚¹åœ¨äºæ•°æ®é›†å­˜åœ¨å¼ºèƒŒæ™¯å™ªå£°ã€å›å£°ã€è¯­éŸ³é‡å ä»¥åŠä¼šè®®ä¸»é¢˜å¤šæ ·ç­‰é—®é¢˜ã€‚ä¸ºè§£å†³è¿™äº›é—®é¢˜ï¼Œæå‡ºäº†G-SpatialNetæ¨¡å‹ä»¥æé«˜å¼•å¯¼æºåˆ†ç¦»ä¿¡å·çš„è¯­éŸ³å¢å¼ºæ•ˆæœï¼Œå¹¶æå‡ºäº†TLSæ¡†æ¶ä»¥ç”Ÿæˆä¿¡å·çº§ä¼ªæ ‡ç­¾ç”¨äºçœŸå®è¿œè·ç¦»éŸ³é¢‘æ•°æ®çš„è®­ç»ƒï¼ŒåŒæ—¶æ¢ç´¢äº†å¾®è°ƒç­–ç•¥ã€æ•°æ®å¢å¼ºå’Œå¤šæ¨¡æ€ä¿¡æ¯ä»¥å¢å¼ºä¼šè®®åœºæ™¯ä¸‹é¢„è®­ç»ƒè‡ªåŠ¨è¯­éŸ³è¯†åˆ«æ¨¡å‹çš„æ€§èƒ½ã€‚æœ€ç»ˆï¼Œç³»ç»Ÿå®ç°äº†å¼€å‘é›†å’Œè¯„ä¼°é›†ä¸Šçš„å­—ç¬¦é”™è¯¯ç‡åˆ†åˆ«ä¸º5.44%å’Œ9.52%ï¼Œç›¸å¯¹äºåŸºçº¿æœ‰æ˜¾è‘—æ”¹å–„ï¼Œè·å¾—ç¬¬äºŒåã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>ç³»ç»Ÿé¢ä¸´çš„ä¸»è¦éš¾ç‚¹åœ¨äºæ•°æ®é›†å­˜åœ¨å¤šç§è¯­éŸ³å¹²æ‰°å’Œå¤šæ ·çš„ä¼šè®®ä¸»é¢˜ã€‚</li>
<li>æå‡ºäº†G-SpatialNetæ¨¡å‹ï¼Œç”¨äºæé«˜å¼•å¯¼æºåˆ†ç¦»ä¿¡å·çš„è¯­éŸ³å¢å¼ºæ•ˆæœã€‚</li>
<li>æå‡ºäº†TLSæ¡†æ¶ï¼ŒåŒ…æ‹¬æ—¶é—´å¯¹é½ã€çº§åˆ«å¯¹é½å’Œä¿¡å™ªæ¯”è¿‡æ»¤ï¼Œç”Ÿæˆä¿¡å·çº§ä¼ªæ ‡ç­¾ç”¨äºè®­ç»ƒè¯­éŸ³å¢å¼ºæ¨¡å‹ã€‚</li>
<li>é€šè¿‡æ¢ç´¢å¾®è°ƒç­–ç•¥ã€æ•°æ®å¢å¼ºå’Œå¤šæ¨¡æ€ä¿¡æ¯ï¼Œå¢å¼ºäº†é¢„è®­ç»ƒè‡ªåŠ¨è¯­éŸ³è¯†åˆ«æ¨¡å‹åœ¨ä¼šè®®åœºæ™¯ä¸‹çš„æ€§èƒ½ã€‚</li>
<li>ç³»ç»Ÿåœ¨å¼€å‘é›†å’Œè¯„ä¼°é›†ä¸Šå®ç°äº†è¾ƒä½çš„å­—ç¬¦é”™è¯¯ç‡ã€‚</li>
<li>ä¸åŸºçº¿ç›¸æ¯”ï¼Œç³»ç»Ÿæ€§èƒ½æœ‰æ˜¾è‘—æ”¹å–„ï¼Œè·å¾—äº†ç¬¬äºŒåçš„å¥½æˆç»©ã€‚</li>
<li>æ–‡ä¸­æ¶‰åŠçš„æŠ€æœ¯åŒ…æ‹¬è¯­éŸ³å¢å¼ºã€è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ã€ä¿¡å·å¤„ç†å’Œæ·±åº¦å­¦ä¹ ç­‰ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.24446">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-9c5064f1546d36213a254a86c71e5126.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-150d47cb3482ccfe59aff819c48e2a48.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-96f4b540d6eddfd296bdb56b0aa80e67.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9e3a4aaf3397ad6b1235b13ebc340afa.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Analysis-and-Evaluation-of-Synthetic-Data-Generation-in-Speech-Dysfluency-Detection"><a href="#Analysis-and-Evaluation-of-Synthetic-Data-Generation-in-Speech-Dysfluency-Detection" class="headerlink" title="Analysis and Evaluation of Synthetic Data Generation in Speech   Dysfluency Detection"></a>Analysis and Evaluation of Synthetic Data Generation in Speech   Dysfluency Detection</h2><p><strong>Authors:Jinming Zhang, Xuanru Zhou, Jiachen Lian, Shuhe Li, William Li, Zoe Ezzes, Rian Bogley, Lisa Wauters, Zachary Miller, Jet Vonk, Brittany Morin, Maria Gorno-Tempini, Gopala Anumanchipalli</strong></p>
<p>Speech dysfluency detection is crucial for clinical diagnosis and language assessment, but existing methods are limited by the scarcity of high-quality annotated data. Although recent advances in TTS model have enabled synthetic dysfluency generation, existing synthetic datasets suffer from unnatural prosody and limited contextual diversity. To address these limitations, we propose LLM-Dys â€“ the most comprehensive dysfluent speech corpus with LLM-enhanced dysfluency simulation. This dataset captures 11 dysfluency categories spanning both word and phoneme levels. Building upon this resource, we improve an end-to-end dysfluency detection framework. Experimental validation demonstrates state-of-the-art performance. All data, models, and code are open-sourced at <a target="_blank" rel="noopener" href="https://github.com/Berkeley-Speech-Group/LLM-Dys">https://github.com/Berkeley-Speech-Group/LLM-Dys</a>. </p>
<blockquote>
<p>è¯­éŸ³æµç•…æ€§æ£€æµ‹å¯¹äºä¸´åºŠè¯Šæ–­å’Œæ²»ç–—è¯­è¨€è¯„ä¼°è‡³å…³é‡è¦ï¼Œä½†ç°æœ‰æ–¹æ³•å—åˆ°é«˜è´¨é‡æ³¨é‡Šæ•°æ®ç¨€ç¼ºçš„é™åˆ¶ã€‚å°½ç®¡æœ€è¿‘æ–‡æœ¬åˆ°è¯­éŸ³ï¼ˆTTSï¼‰æ¨¡å‹çš„è¿›æ­¥å·²ç»èƒ½å¤Ÿå®ç°åˆæˆæµç•…æ€§ç”Ÿæˆï¼Œä½†ç°æœ‰åˆæˆæ•°æ®é›†å­˜åœ¨è¯­è°ƒä¸è‡ªç„¶å’Œä¸Šä¸‹æ–‡å¤šæ ·æ€§æœ‰é™çš„é—®é¢˜ã€‚ä¸ºäº†è§£å†³è¿™äº›å±€é™æ€§ï¼Œæˆ‘ä»¬æå‡ºäº†LLM-Dysâ€”â€”ä¸€ä¸ªç”±å¤§å‹è¯­è¨€æ¨¡å‹å¢å¼ºæµç•…æ€§æ¨¡æ‹Ÿçš„æœ€å…¨é¢çš„æµç•…æ€§è¯­éŸ³è¯­æ–™åº“ã€‚è¯¥æ•°æ®é›†æ¶µç›–äº†è·¨è¶Šå•è¯å’ŒéŸ³ç´ çº§åˆ«çš„11ä¸ªæµç•…æ€§é—®é¢˜ç±»åˆ«ã€‚åŸºäºè¿™ä¸€èµ„æºï¼Œæˆ‘ä»¬æ”¹è¿›äº†ä¸€ä¸ªç«¯åˆ°ç«¯çš„æµç•…æ€§æ£€æµ‹æ¡†æ¶ã€‚å®éªŒéªŒè¯è¯æ˜äº†å…¶å“è¶Šçš„æ€§èƒ½ã€‚æ‰€æœ‰æ•°æ®ã€æ¨¡å‹å’Œä»£ç å‡å…¬å¼€å¼€æºäº<a target="_blank" rel="noopener" href="https://github.com/Berkeley-Speech-Group/LLM-Dys%E3%80%82">https://github.com/Berkeley-Speech-Group/LLM-Dysã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.22029v2">PDF</a> Accepted by Interspeech 2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†è¯­éŸ³æµç•…æ€§æ£€æµ‹åœ¨ä¸´åºŠè¯Šæ–­å’Œæ²»ç–—è¯­è¨€è¯„ä¼°ä¸­çš„é‡è¦æ€§ã€‚ç°æœ‰æ–¹æ³•å—é™äºé«˜è´¨é‡æ ‡æ³¨æ•°æ®çš„ç¨€ç¼ºæ€§ã€‚å°½ç®¡æœ€è¿‘æ–‡æœ¬è½¬è¯­éŸ³æ¨¡å‹çš„è¿›æ­¥ä½¿å¾—å¯ä»¥ç”Ÿæˆåˆæˆæµç•…æ€§é—®é¢˜ï¼Œä½†ç°æœ‰åˆæˆæ•°æ®é›†å­˜åœ¨éŸµå¾‹ä¸è‡ªç„¶å’Œä¸Šä¸‹æ–‡å¤šæ ·æ€§æœ‰é™çš„é—®é¢˜ã€‚ä¸ºè§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†LLM-Dysâ€”â€”æœ€å…¨é¢çš„æµç•…æ€§è¯­éŸ³è¯­æ–™åº“ï¼Œé€šè¿‡å¤§å‹è¯­è¨€æ¨¡å‹å¢å¼ºäº†æµç•…æ€§æ¨¡æ‹Ÿã€‚è¯¥æ•°æ®é›†æ¶µç›–äº†è¯æ±‡å’ŒéŸ³ç´ çº§åˆ«çš„11ä¸ªæµç•…æ€§é—®é¢˜ç±»åˆ«ã€‚åŸºäºè¯¥èµ„æºï¼Œæˆ‘ä»¬æ”¹è¿›äº†ç«¯åˆ°ç«¯çš„æµç•…æ€§æ£€æµ‹æ¡†æ¶ã€‚å®éªŒéªŒè¯è¾¾åˆ°äº†å…ˆè¿›æ°´å¹³çš„æ•ˆæœã€‚æ‰€æœ‰ç›¸å…³æ•°æ®ã€æ¨¡å‹å’Œä»£ç å·²å¼€æºåˆ†äº«åœ¨ï¼š<a target="_blank" rel="noopener" href="https://github.com/Berkeley-Speech-Group/LLM-Dys%E3%80%82">https://github.com/Berkeley-Speech-Group/LLM-Dysã€‚</a></p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç°æœ‰è¯­éŸ³æµç•…æ€§æ£€æµ‹æ–¹æ³•å—é™äºé«˜è´¨é‡æ ‡æ³¨æ•°æ®çš„ç¨€ç¼ºæ€§ï¼Œå¯¹äºä¸´åºŠè¯Šæ–­å’Œè¯­è¨€è¯„ä¼°å…·æœ‰é‡è¦æ„ä¹‰ã€‚</li>
<li>TTSæ¨¡å‹è™½å¯å®ç°åˆæˆæµç•…æ€§çš„ç”Ÿæˆï¼Œä½†å­˜åœ¨éŸµå¾‹ä¸è‡ªç„¶å’Œä¸Šä¸‹æ–‡å¤šæ ·æ€§ä¸è¶³çš„é—®é¢˜ã€‚</li>
<li>æå‡ºLLM-Dysä½œä¸ºæœ€å…¨é¢çš„æµç•…æ€§è¯­éŸ³è¯­æ–™åº“ï¼Œé€šè¿‡å¤§å‹è¯­è¨€æ¨¡å‹å¢å¼ºæµç•…æ€§æ¨¡æ‹Ÿã€‚</li>
<li>LLM-Dysæ¶µç›–äº†è¯æ±‡å’ŒéŸ³ç´ çº§åˆ«çš„å¤šç§æµç•…æ€§é—®é¢˜ç±»åˆ«ã€‚</li>
<li>åŸºäºLLM-Dysæ•°æ®é›†æ”¹è¿›äº†ç«¯åˆ°ç«¯çš„æµç•…æ€§æ£€æµ‹æ¡†æ¶ã€‚</li>
<li>å®éªŒéªŒè¯æ˜¾ç¤ºæ”¹è¿›åçš„æ¡†æ¶æ€§èƒ½è¾¾åˆ°å…ˆè¿›æ°´å¹³ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.22029">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-0f3a82ffc0080873fb3281bd5781c0ce.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f09ea430ad1b7c0b1ad598e4f7cab2fa.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-b030629238cfa23ed174c5da4e1f06be.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-eae4225d2805104617350cc554123266.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-d4c856b39137d632c5e621835da585e0.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-40f8c15c1bcb1f5c3aa3c4628cc2c9b9.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Introducing-voice-timbre-attribute-detection"><a href="#Introducing-voice-timbre-attribute-detection" class="headerlink" title="Introducing voice timbre attribute detection"></a>Introducing voice timbre attribute detection</h2><p><strong>Authors:Jinghao He, Zhengyan Sheng, Liping Chen, Kong Aik Lee, Zhen-Hua Ling</strong></p>
<p>This paper focuses on explaining the timbre conveyed by speech signals and introduces a task termed voice timbre attribute detection (vTAD). In this task, voice timbre is explained with a set of sensory attributes describing its human perception. A pair of speech utterances is processed, and their intensity is compared in a designated timbre descriptor. Moreover, a framework is proposed, which is built upon the speaker embeddings extracted from the speech utterances. The investigation is conducted on the VCTK-RVA dataset. Experimental examinations on the ECAPA-TDNN and FACodec speaker encoders demonstrated that: 1) the ECAPA-TDNN speaker encoder was more capable in the seen scenario, where the testing speakers were included in the training set; 2) the FACodec speaker encoder was superior in the unseen scenario, where the testing speakers were not part of the training, indicating enhanced generalization capability. The VCTK-RVA dataset and open-source code are available on the website <a target="_blank" rel="noopener" href="https://github.com/vTAD2025-Challenge/vTAD">https://github.com/vTAD2025-Challenge/vTAD</a>. </p>
<blockquote>
<p>æœ¬æ–‡é‡ç‚¹è§£é‡Šè¯­éŸ³ä¿¡å·æ‰€ä¼ é€’çš„éŸ³è‰²ï¼Œå¹¶ä»‹ç»äº†ä¸€é¡¹ç§°ä¸ºéŸ³è‰²å±æ€§æ£€æµ‹ï¼ˆvTADï¼‰çš„ä»»åŠ¡ã€‚åœ¨è¯¥ä»»åŠ¡ä¸­ï¼ŒéŸ³è‰²é€šè¿‡ä¸€ç³»åˆ—æè¿°äººç±»æ„ŸçŸ¥çš„æ„Ÿå®˜å±æ€§æ¥è§£é‡Šã€‚å¯¹ä¸€å¯¹è¯­éŸ³ç‰‡æ®µè¿›è¡Œå¤„ç†ï¼Œå¹¶åœ¨æŒ‡å®šçš„éŸ³è‰²æè¿°ç¬¦ä¸­æ¯”è¾ƒå®ƒä»¬çš„å¼ºåº¦ã€‚æ­¤å¤–ï¼Œæå‡ºäº†ä¸€ä¸ªåŸºäºä»è¯­éŸ³ç‰‡æ®µä¸­æå–çš„è¯´è¯è€…åµŒå…¥çš„æ¡†æ¶ã€‚è¯¥ç ”ç©¶åœ¨VCTK-RVAæ•°æ®é›†ä¸Šè¿›è¡Œã€‚å¯¹ECAPA-TDNNå’ŒFACodecè¯´è¯äººç¼–ç å™¨çš„å®éªŒè¡¨æ˜ï¼š1ï¼‰åœ¨å¯è§åœºæ™¯ä¸‹ï¼ŒECAPA-TDNNè¯´è¯äººç¼–ç å™¨æ›´å…·èƒ½åŠ›ï¼Œå…¶ä¸­æµ‹è¯•è¯´è¯äººåŒ…å«åœ¨è®­ç»ƒé›†ä¸­ï¼›2ï¼‰FACodecè¯´è¯äººç¼–ç å™¨åœ¨æœªè§è¿‡çš„æƒ…å†µä¸‹è¡¨ç°æ›´å¥½ï¼Œå…¶ä¸­æµ‹è¯•è¯´è¯äººä¸æ˜¯è®­ç»ƒçš„ä¸€éƒ¨åˆ†ï¼Œè¡¨æ˜å…¶å¢å¼ºäº†æ³›åŒ–èƒ½åŠ›ã€‚VCTK-RVAæ•°æ®é›†å’Œå¼€æºä»£ç å¯åœ¨ç½‘ç«™<a target="_blank" rel="noopener" href="https://github.com/vTAD2025-Challenge/vTAD%E4%B8%8A%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/vTAD2025-Challenge/vTADä¸Šæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.09661v2">PDF</a> arXiv admin note: substantial text overlap with arXiv:2505.09382</p>
<p><strong>æ€»ç»“</strong><br>    æœ¬æ–‡ä»‹ç»äº†è¯­éŸ³ä¿¡å·çš„éŸ³è‰²ç‰¹å¾ï¼Œå¹¶å®šä¹‰äº†ä¸€ä¸ªåä¸ºå£°éŸ³éŸ³è‰²å±æ€§æ£€æµ‹ï¼ˆvTADï¼‰çš„ä»»åŠ¡ã€‚åœ¨è¯¥ä»»åŠ¡ä¸­ï¼Œé€šè¿‡ä¸€ç³»åˆ—æè¿°éŸ³è‰²çš„æ„ŸçŸ¥å±æ€§æ¥è§£é‡ŠéŸ³è‰²ã€‚å¤„ç†ä¸€æ®µè¯­éŸ³ä¿¡å·åï¼Œå¯¹æ¯”ä¸¤æ®µè¯­éŸ³çš„éŸ³è‰²å¼ºåº¦åœ¨ä¸€ä¸ªç‰¹å®šçš„éŸ³è‰²æè¿°ç¬¦ä¸Šã€‚æ­¤å¤–ï¼Œæå‡ºäº†ä¸€ç§åŸºäºä»è¯­éŸ³ä¸­æå–çš„è¯´è¯è€…åµŒå…¥ä¿¡æ¯çš„æ¡†æ¶ï¼Œè¯¥ç ”ç©¶åœ¨VCTK-RVAæ•°æ®é›†ä¸Šè¿›è¡ŒéªŒè¯ã€‚é€šè¿‡ECAPA-TDNNå’ŒFACodecä¸¤ç§è¯´è¯è€…ç¼–ç å™¨çš„å®éªŒè¡¨æ˜ï¼šECAPA-TDNNåœ¨æµ‹è¯•è¯´è¯è€…åŒ…å«åœ¨è®­ç»ƒé›†ä¸­çš„æƒ…å†µä¸‹è¡¨ç°æ›´å¥½ï¼›FACodecåœ¨æµ‹è¯•è¯´è¯è€…æœªå‚ä¸è®­ç»ƒçš„æƒ…å†µä¸‹è¡¨ç°å‡ºè¾ƒå¼ºçš„æ³›åŒ–èƒ½åŠ›ã€‚ç›¸å…³æ•°æ®é›†å’Œå¼€æºä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/vTAD2025-Challenge/vTAD">ç½‘ç«™é“¾æ¥</a>ä¸Šæ‰¾åˆ°ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>æ–‡ç« ä»‹ç»äº†è¯­éŸ³ä¿¡å·çš„éŸ³è‰²ç‰¹å¾ï¼Œå¹¶å®šä¹‰äº†å£°éŸ³éŸ³è‰²å±æ€§æ£€æµ‹ï¼ˆvTADï¼‰çš„ä»»åŠ¡ã€‚è¿™æ˜¯ä¸€ä¸ªæ–°é¢–çš„è§’åº¦æ¥æ¢è®¨è¯­éŸ³ä¿¡å·å¤„ç†ã€‚</li>
<li>æå‡ºä¸€ç§å¯¹æ¯”ä¸¤æ®µè¯­éŸ³ä¿¡å·åœ¨ç‰¹å®šéŸ³è‰²æè¿°ç¬¦ä¸Šçš„éŸ³è‰²å¼ºåº¦çš„å¯¹æ¯”æ–¹æ³•ã€‚è¿™æ˜¯vTADä»»åŠ¡çš„ä¸€ä¸ªé‡è¦ç»„æˆéƒ¨åˆ†ã€‚</li>
<li>åˆ©ç”¨åŸºäºè¯´è¯è€…åµŒå…¥ä¿¡æ¯çš„æ¡†æ¶è¿›è¡Œç ”ç©¶ï¼Œä¸ºè¯­éŸ³ä¿¡å·çš„å¤„ç†å’Œåˆ†ææä¾›äº†æ–°æ€è·¯ã€‚</li>
<li>åœ¨VCTK-RVAæ•°æ®é›†ä¸Šè¿›è¡Œäº†å®éªŒéªŒè¯ï¼Œè¯¥æ•°æ®é›†å¯ç”¨äºè®­ç»ƒå’Œæµ‹è¯•vTADç›¸å…³çš„ç®—æ³•ã€‚</li>
<li>å®éªŒç»“æœæ˜¾ç¤ºï¼ŒECAPA-TDNNåœ¨å·²çŸ¥è¯´è¯è€…åœºæ™¯ä¸‹è¡¨ç°è¾ƒå¥½ï¼Œè€ŒFACodecåœ¨æœªçŸ¥è¯´è¯è€…åœºæ™¯ä¸­å…·æœ‰æ›´å¼ºçš„æ³›åŒ–èƒ½åŠ›ã€‚è¿™å¯¹äºé€‰æ‹©é€‚åˆçš„ç¼–ç å™¨æä¾›äº†å‚è€ƒä¾æ®ã€‚</li>
<li>æ–‡ç« æä¾›çš„å¼€æºä»£ç å’Œæ•°æ®é›†æœ‰åŠ©äºæ¨åŠ¨vTADé¢†åŸŸçš„ç ”ç©¶è¿›å±•ã€‚è¿™å¯¹äºè¯­éŸ³ä¿¡å·å¤„ç†é¢†åŸŸçš„ç ”ç©¶è€…æ˜¯ä¸€å¤§ç¦éŸ³ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.09661">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-2159d5803006082d307d96e8fbbc1a16.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-8c5c45aac246baa2e6663d290a914b96.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1c773031d1d203f73e7ddefe11f68c8a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d70cf26033dbb1bafbc7379a8780f861.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-537c4bf826ec6929e8bd35313852d4d9.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Protecting-Your-Voice-Temporal-aware-Robust-Watermarking"><a href="#Protecting-Your-Voice-Temporal-aware-Robust-Watermarking" class="headerlink" title="Protecting Your Voice: Temporal-aware Robust Watermarking"></a>Protecting Your Voice: Temporal-aware Robust Watermarking</h2><p><strong>Authors:Yue Li, Weizhi Liu, Dongdong Lin, Hui Tian, Hongxia Wang</strong></p>
<p>The rapid advancement of generative models has led to the synthesis of real-fake ambiguous voices. To erase the ambiguity, embedding watermarks into the frequency-domain features of synthesized voices has become a common routine. However, the robustness achieved by choosing the frequency domain often comes at the expense of fine-grained voice features, leading to a loss of fidelity. Maximizing the comprehensive learning of time-domain features to enhance fidelity while maintaining robustness, we pioneer a \textbf{\underline{t}}emporal-aware \textbf{\underline{r}}ob\textbf{\underline{u}}st wat\textbf{\underline{e}}rmarking (\emph{True}) method for protecting the speech and singing voice. For this purpose, the integrated content-driven encoder is designed for watermarked waveform reconstruction, which is structurally lightweight. Additionally, the temporal-aware gated convolutional network is meticulously designed to bit-wise recover the watermark. Comprehensive experiments and comparisons with existing state-of-the-art methods have demonstrated the superior fidelity and vigorous robustness of the proposed \textit{True} achieving an average PESQ score of 4.63. </p>
<blockquote>
<p>ç”Ÿæˆæ¨¡å‹çš„å¿«é€Ÿå‘å±•å¯¼è‡´äº†çœŸå®ä¸è™šå‡æ¨¡ç³Šè¯­éŸ³çš„åˆæˆã€‚ä¸ºäº†æ¶ˆé™¤è¿™ç§æ¨¡ç³Šæ€§ï¼Œå°†æ°´å°åµŒå…¥åˆæˆè¯­éŸ³çš„é¢‘ç‡åŸŸç‰¹å¾å·²æˆä¸ºä¸€ç§å¸¸è§åšæ³•ã€‚ç„¶è€Œï¼Œé€šè¿‡é€‰æ‹©é¢‘ç‡åŸŸå®ç°çš„ç¨³å¥æ€§å¾€å¾€ä»¥ç‰ºç‰²ç²¾ç»†çš„è¯­éŸ³ç‰¹å¾ä¸ºä»£ä»·ï¼Œå¯¼è‡´ä¿çœŸåº¦æŸå¤±ã€‚æˆ‘ä»¬æœ€å¤§åŒ–å¯¹æ—¶åŸŸç‰¹å¾çš„å…¨é¢å­¦ä¹ ï¼Œä»¥æé«˜ä¿çœŸåº¦å¹¶ä¿æŒç¨³å¥æ€§ï¼Œå¼€åˆ›äº†ä¸€ç§ä¿æŠ¤è¯­éŸ³å’Œæ­Œå£°çš„æ—¶é—´æ„ŸçŸ¥ç¨³å¥æ°´å°æ–¹æ³•ï¼ˆTrueï¼‰ã€‚ä¸ºæ­¤ï¼Œè®¾è®¡äº†ä¸€ä¸ªé›†æˆçš„å†…å®¹é©±åŠ¨ç¼–ç å™¨è¿›è¡Œæ°´å°æ³¢å½¢é‡å»ºï¼Œè¯¥ç¼–ç å™¨ç»“æ„è½»å·§ã€‚æ­¤å¤–ï¼Œç²¾å¿ƒè®¾è®¡äº†æ—¶é—´æ„ŸçŸ¥é—¨æ§å·ç§¯ç½‘ç»œä»¥ä½æ¢å¤æ°´å°ã€‚ç»¼åˆå®éªŒä¸ç°æœ‰å…ˆè¿›æ–¹æ³•çš„æ¯”è¾ƒè¡¨æ˜ï¼Œæ‰€æå‡ºçš„Trueæ–¹æ³•çš„ä¿çœŸåº¦æ›´é«˜ã€ç¨³å¥æ€§æ›´å¼ºï¼Œå¹³å‡PESQå¾—åˆ†è¾¾åˆ°4.63ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.14832v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>éšç€ç”Ÿæˆæ¨¡å‹çš„å¿«é€Ÿå‘å±•ï¼Œåˆæˆè¯­éŸ³çš„çœŸå®æ€§å’Œä¼ªé€ æ€§å˜å¾—æ¨¡ç³Šã€‚ä¸ºäº†åœ¨é¢‘ç‡åŸŸç‰¹å¾ä¸­ä¸ºåˆæˆè¯­éŸ³åµŒå…¥æ°´å°ä»¥æ¶ˆé™¤æ­§ä¹‰ï¼ŒåŒæ—¶ä¿æŒç¨³å¥æ€§å¹¶æœ€å¤§åŒ–æ—¶é—´åŸŸç‰¹å¾çš„å…¨é¢å­¦ä¹ ä»¥æé«˜ä¿çœŸåº¦ï¼Œæˆ‘ä»¬é¦–åˆ›äº†ä¸€ç§ç§°ä¸ºâ€œTrueâ€çš„æ—¶é—´æ„ŸçŸ¥ç¨³å¥æ°´å°æ–¹æ³•ï¼Œç”¨äºä¿æŠ¤è¯­éŸ³å’Œæ­Œå£°ã€‚è¯¥æ–¹æ³•è®¾è®¡äº†é›†æˆçš„å†…å®¹é©±åŠ¨ç¼–ç å™¨å’Œæ—¶é—´æ„ŸçŸ¥é—¨æ§å·ç§¯ç½‘ç»œï¼Œä»¥è¿›è¡Œæ°´å°åµŒå…¥å’Œæå–ã€‚å®éªŒè¯æ˜ï¼Œè¯¥æ–¹æ³•åœ¨ä¿çœŸåº¦å’Œç¨³å¥æ€§æ–¹é¢å‡ä¼˜äºç°æœ‰æŠ€æœ¯ï¼Œå¹³å‡PESQå¾—åˆ†è¾¾åˆ°4.63ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç”Ÿæˆæ¨¡å‹çš„å¿«é€Ÿå‘å±•å¯¼è‡´åˆæˆè¯­éŸ³çš„çœŸå®æ€§å’Œä¼ªé€ æ€§å˜å¾—æ¨¡ç³Šã€‚</li>
<li>åœ¨é¢‘ç‡åŸŸç‰¹å¾ä¸­ä¸ºåˆæˆè¯­éŸ³åµŒå…¥æ°´å°æ˜¯æ¶ˆé™¤æ­§ä¹‰çš„ä¸€ç§å¸¸è§æ–¹æ³•ï¼Œä½†å¯èƒ½å¯¼è‡´è¯­éŸ³ç‰¹å¾çš„æŸå¤±ã€‚</li>
<li>ä¸ºäº†æé«˜ä¿çœŸåº¦å¹¶ç»´æŒç¨³å¥æ€§ï¼Œéœ€è¦æœ€å¤§åŒ–æ—¶é—´åŸŸç‰¹å¾çš„å…¨é¢å­¦ä¹ ã€‚</li>
<li>æå‡ºäº†ä¸€ç§æ–°çš„æ—¶é—´æ„ŸçŸ¥ç¨³å¥æ°´å°æ–¹æ³•â€œTrueâ€ï¼Œç”¨äºä¿æŠ¤è¯­éŸ³å’Œæ­Œå£°ã€‚</li>
<li>â€œTrueâ€æ–¹æ³•åŒ…æ‹¬ä¸€ä¸ªé›†æˆçš„å†…å®¹é©±åŠ¨ç¼–ç å™¨å’Œä¸€ä¸ªæ—¶é—´æ„ŸçŸ¥é—¨æ§å·ç§¯ç½‘ç»œã€‚</li>
<li>å®éªŒè¯æ˜ï¼Œâ€œTrueâ€æ–¹æ³•åœ¨ä¿çœŸåº¦å’Œç¨³å¥æ€§æ–¹é¢ä¼˜äºç°æœ‰æŠ€æœ¯ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.14832">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-3447f83fabdc25babd1604a269b04d19.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1f072090c4dbcd947311e965fb165aaa.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a1035712b8ba8e877eb3ed6f8839e1f2.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b761cdcdebbe8a0cf3ad7b0890c05bc7.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-46ba874f769d21bfbfe4affac6dc059d.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="AnyEnhance-A-Unified-Generative-Model-with-Prompt-Guidance-and-Self-Critic-for-Voice-Enhancement"><a href="#AnyEnhance-A-Unified-Generative-Model-with-Prompt-Guidance-and-Self-Critic-for-Voice-Enhancement" class="headerlink" title="AnyEnhance: A Unified Generative Model with Prompt-Guidance and   Self-Critic for Voice Enhancement"></a>AnyEnhance: A Unified Generative Model with Prompt-Guidance and   Self-Critic for Voice Enhancement</h2><p><strong>Authors:Junan Zhang, Jing Yang, Zihao Fang, Yuancheng Wang, Zehua Zhang, Zhuo Wang, Fan Fan, Zhizheng Wu</strong></p>
<p>We introduce AnyEnhance, a unified generative model for voice enhancement that processes both speech and singing voices. Based on a masked generative model, AnyEnhance is capable of handling both speech and singing voices, supporting a wide range of enhancement tasks including denoising, dereverberation, declipping, super-resolution, and target speaker extraction, all simultaneously and without fine-tuning. AnyEnhance introduces a prompt-guidance mechanism for in-context learning, which allows the model to natively accept a reference speakerâ€™s timbre. In this way, it could boost enhancement performance when a reference audio is available and enable the target speaker extraction task without altering the underlying architecture. Moreover, we also introduce a self-critic mechanism into the generative process for masked generative models, yielding higher-quality outputs through iterative self-assessment and refinement. Extensive experiments on various enhancement tasks demonstrate AnyEnhance outperforms existing methods in terms of both objective metrics and subjective listening tests. Demo audios are publicly available at <a target="_blank" rel="noopener" href="https://amphionspace.github.io/anyenhance/">https://amphionspace.github.io/anyenhance/</a>. </p>
<blockquote>
<p>æˆ‘ä»¬ä»‹ç»äº†AnyEnhanceï¼Œè¿™æ˜¯ä¸€ä¸ªç»Ÿä¸€çš„ç”Ÿæˆæ¨¡å‹ï¼Œç”¨äºå¤„ç†è¯­éŸ³å’Œæ­Œå”±å£°éŸ³çš„å£°éŸ³å¢å¼ºã€‚åŸºäºæ©æ¨¡ç”Ÿæˆæ¨¡å‹ï¼ŒAnyEnhanceèƒ½å¤ŸåŒæ—¶å¤„ç†è¯­éŸ³å’Œæ­Œå”±å£°éŸ³ï¼Œæ”¯æŒå¹¿æ³›çš„å¢å¼ºä»»åŠ¡ï¼ŒåŒ…æ‹¬å»å™ªã€å»æ··å“ã€å»å‰ªè¾‘ã€è¶…åˆ†è¾¨ç‡å’Œç›®æ ‡è¯´è¯äººæå–ï¼Œè€Œæ— éœ€å¾®è°ƒã€‚AnyEnhanceå¼•å…¥äº†ä¸€ç§ä¸Šä¸‹æ–‡å­¦ä¹ ä¸­çš„æç¤ºå¼•å¯¼æœºåˆ¶ï¼Œå…è®¸æ¨¡å‹ç›´æ¥æ¥å—å‚è€ƒè¯´è¯äººçš„éŸ³è‰²ã€‚è¿™æ ·ï¼Œåœ¨æœ‰å‚è€ƒéŸ³é¢‘å¯ç”¨çš„æƒ…å†µä¸‹ï¼Œå®ƒå¯ä»¥æé«˜å¢å¼ºæ€§èƒ½ï¼Œå¹¶åœ¨ä¸æ”¹å˜åº•å±‚æ¶æ„çš„æƒ…å†µä¸‹å®ç°ç›®æ ‡è¯´è¯äººæå–ä»»åŠ¡ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜å¼•å…¥äº†ç”Ÿæˆè¿‡ç¨‹ä¸­çš„è‡ªæˆ‘æ‰¹åˆ¤æœºåˆ¶ï¼Œé€šè¿‡è¿­ä»£è‡ªæˆ‘è¯„ä¼°å’Œä¿®æ­£ï¼Œäº§ç”Ÿæ›´é«˜è´¨é‡çš„è¾“å‡ºã€‚åœ¨å„ç§å¢å¼ºä»»åŠ¡ä¸Šçš„å¤§é‡å®éªŒè¡¨æ˜ï¼ŒAnyEnhanceåœ¨å®¢è§‚æŒ‡æ ‡å’Œä¸»è§‚å¬è§‰æµ‹è¯•æ–¹é¢ä¼˜äºç°æœ‰æ–¹æ³•ã€‚æ¼”ç¤ºéŸ³é¢‘å¯åœ¨<a target="_blank" rel="noopener" href="https://amphionspace.github.io/anyenhance/%E5%85%AC%E5%BC%80%E8%AE%BF%E9%97%AE%E3%80%82">https://amphionspace.github.io/anyenhance/å…¬å¼€è®¿é—®ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.15417v2">PDF</a> Accepted by IEEE&#x2F;ACM Transactions on Audio, Speech, and Language   Processing (TASLP) 2025</p>
<p><strong>Summary</strong></p>
<p>AnyEnhanceæ˜¯ä¸€ä¸ªç»Ÿä¸€çš„ç”Ÿæˆæ¨¡å‹ï¼Œç”¨äºå¢å¼ºè¯­éŸ³å’Œæ­Œå£°ã€‚å®ƒåŸºäºæ©è†œç”Ÿæˆæ¨¡å‹ï¼Œèƒ½å¤ŸåŒæ—¶å¤„ç†å„ç§å¢å¼ºä»»åŠ¡ï¼Œå¦‚å»å™ªã€å»æ··å“ã€å»å‰ªè¾‘ã€è¶…åˆ†è¾¨ç‡å’Œç›®æ ‡è¯­éŸ³æå–ï¼Œè€Œæ— éœ€å¾®è°ƒã€‚AnyEnhanceå¼•å…¥äº†ä¸€ç§åŸºäºæç¤ºçš„å¼•å¯¼æœºåˆ¶ï¼Œç”¨äºä¸Šä¸‹æ–‡å­¦ä¹ ï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿè‡ªç„¶åœ°æ¥å—å‚è€ƒè¯´è¯äººçš„éŸ³è‰²ã€‚æ­¤å¤–ï¼Œè¿˜å¼•å…¥äº†è‡ªæˆ‘æ‰¹åˆ¤æœºåˆ¶ï¼Œé€šè¿‡è¿­ä»£è‡ªæˆ‘è¯„ä¼°å’Œç»†åŒ–ï¼Œäº§ç”Ÿæ›´é«˜è´¨é‡çš„è¾“å‡ºã€‚å®éªŒè¡¨æ˜ï¼ŒAnyEnhanceåœ¨å®¢è§‚æŒ‡æ ‡å’Œä¸»è§‚å¬è§‰æµ‹è¯•æ–¹é¢å‡ä¼˜äºç°æœ‰æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>AnyEnhanceæ˜¯ä¸€ä¸ªç»Ÿä¸€çš„ç”Ÿæˆæ¨¡å‹ï¼Œé€‚ç”¨äºè¯­éŸ³å’Œæ­Œå£°å¢å¼ºã€‚</li>
<li>å®ƒåŸºäºæ©è†œç”Ÿæˆæ¨¡å‹ï¼Œèƒ½å¤Ÿå¤„ç†å¤šç§å¢å¼ºä»»åŠ¡ï¼ŒåŒ…æ‹¬å»å™ªã€å»æ··å“ã€å»å‰ªè¾‘ã€è¶…åˆ†è¾¨ç‡å’Œç›®æ ‡è¯­éŸ³æå–ã€‚</li>
<li>AnyEnhanceå¼•å…¥äº†åŸºäºæç¤ºçš„å¼•å¯¼æœºåˆ¶ï¼Œå…è®¸æ¨¡å‹æ¥å—å‚è€ƒè¯´è¯äººçš„éŸ³è‰²ï¼Œæå‡åœ¨æœ‰å‚è€ƒéŸ³é¢‘æ—¶çš„å¢å¼ºæ€§èƒ½ã€‚</li>
<li>æ¨¡å‹èƒ½å¤Ÿé€šè¿‡è‡ªæˆ‘æ‰¹åˆ¤æœºåˆ¶äº§ç”Ÿæ›´é«˜è´¨é‡çš„è¾“å‡ºï¼Œé€šè¿‡è¿­ä»£è‡ªæˆ‘è¯„ä¼°å’Œç»†åŒ–ã€‚</li>
<li>AnyEnhanceåœ¨å®¢è§‚æŒ‡æ ‡å’Œä¸»è§‚å¬è§‰æµ‹è¯•æ–¹é¢å‡è¡¨ç°å‡ºä¼˜å¼‚æ€§èƒ½ã€‚</li>
<li>è¯¥æ¨¡å‹çš„åº”ç”¨æ¼”ç¤ºéŸ³é¢‘å¯åœ¨å…¬ä¼—å¹³å°è·å–ã€‚</li>
<li>AnyEnhanceçš„è®¾è®¡ä½¿å…¶å…·æœ‰å¹¿æ³›çš„åº”ç”¨å‰æ™¯ï¼ŒåŒ…æ‹¬è¯­éŸ³è¯†åˆ«ã€è¯­éŸ³åŠ©æ‰‹ã€éŸ³é¢‘ç¼–è¾‘ç­‰é¢†åŸŸã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.15417">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-ec634d16045dc1c317ca6beb1cdb6195.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-8e95577bf3e9620907dc0181b9691eb7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ee041506c94ef3ebea08ccc76a45c42a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-31d5eabdbfcfc60654fbe97864284362.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-9aa9519f11d537c0162b2d5eff58903a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-653cbcecd11f55fe2a7f78087d311a37.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-06-25/Speech/">https://kedreamix.github.io/Talk2Paper/Paper/2025-06-25/Speech/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Speech/">
                                    <span class="chip bg-color">Speech</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-06-25/Face%20Swapping/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-a6e837ca3e62d58b0e338cae02b84733.jpg" class="responsive-img" alt="Face Swapping">
                        
                        <span class="card-title">Face Swapping</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Face Swapping æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-06-25  TCDiff++ An End-to-end Trajectory-Controllable Diffusion Model for   Harmonious Music-Driven Group Choreography
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-06-25
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Face-Swapping/" class="post-category">
                                    Face Swapping
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Face-Swapping/">
                        <span class="chip bg-color">Face Swapping</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-06-25/%E6%97%A0%E7%9B%91%E7%9D%A3_%E5%8D%8A%E7%9B%91%E7%9D%A3_%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-117f5883f28987041fc0cf3dbf810c4a.jpg" class="responsive-img" alt="æ— ç›‘ç£/åŠç›‘ç£/å¯¹æ¯”å­¦ä¹ ">
                        
                        <span class="card-title">æ— ç›‘ç£/åŠç›‘ç£/å¯¹æ¯”å­¦ä¹ </span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            æ— ç›‘ç£/åŠç›‘ç£/å¯¹æ¯”å­¦ä¹  æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-06-25  Temporal Neural Cellular Automata Application to modeling of contrast   enhancement in breast MRI
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-06-25
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E6%97%A0%E7%9B%91%E7%9D%A3-%E5%8D%8A%E7%9B%91%E7%9D%A3-%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0/" class="post-category">
                                    æ— ç›‘ç£/åŠç›‘ç£/å¯¹æ¯”å­¦ä¹ 
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E6%97%A0%E7%9B%91%E7%9D%A3-%E5%8D%8A%E7%9B%91%E7%9D%A3-%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0/">
                        <span class="chip bg-color">æ— ç›‘ç£/åŠç›‘ç£/å¯¹æ¯”å­¦ä¹ </span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">33125.9k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
