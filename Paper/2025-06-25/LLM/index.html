<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="LLM">
    <meta name="description" content="LLM æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-06-25  Vision as a Dialect Unifying Visual Understanding and Generation via   Text-Aligned Representations">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>LLM | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pica.zhimg.com/v2-713319f28adfe8dda6d102f914890a01.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">LLM</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/LLM/">
                                <span class="chip bg-color">LLM</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/LLM/" class="post-category">
                                LLM
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-06-25
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-07-06
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    21k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    85 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-06-25-æ›´æ–°"><a href="#2025-06-25-æ›´æ–°" class="headerlink" title="2025-06-25 æ›´æ–°"></a>2025-06-25 æ›´æ–°</h1><h2 id="Vision-as-a-Dialect-Unifying-Visual-Understanding-and-Generation-via-Text-Aligned-Representations"><a href="#Vision-as-a-Dialect-Unifying-Visual-Understanding-and-Generation-via-Text-Aligned-Representations" class="headerlink" title="Vision as a Dialect: Unifying Visual Understanding and Generation via   Text-Aligned Representations"></a>Vision as a Dialect: Unifying Visual Understanding and Generation via   Text-Aligned Representations</h2><p><strong>Authors:Jiaming Han, Hao Chen, Yang Zhao, Hanyu Wang, Qi Zhao, Ziyan Yang, Hao He, Xiangyu Yue, Lu Jiang</strong></p>
<p>This paper presents a multimodal framework that attempts to unify visual understanding and generation within a shared discrete semantic representation. At its core is the Text-Aligned Tokenizer (TA-Tok), which converts images into discrete tokens using a text-aligned codebook projected from a large language modelâ€™s (LLM) vocabulary. By integrating vision and text into a unified space with an expanded vocabulary, our multimodal LLM, Tar, enables cross-modal input and output through a shared interface, without the need for modality-specific designs. Additionally, we propose scale-adaptive encoding and decoding to balance efficiency and visual detail, along with a generative de-tokenizer to produce high-fidelity visual outputs. To address diverse decoding needs, we utilize two complementary de-tokenizers: a fast autoregressive model and a diffusion-based model. To enhance modality fusion, we investigate advanced pre-training tasks, demonstrating improvements in both visual understanding and generation. Experiments across benchmarks show that Tar matches or surpasses existing multimodal LLM methods, achieving faster convergence and greater training efficiency. Code, models, and data are available at <a target="_blank" rel="noopener" href="https://tar.csuhan.com/">https://tar.csuhan.com</a> </p>
<blockquote>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸€ä¸ªå¤šæ¨¡æ€æ¡†æ¶ï¼Œè¯¥æ¡†æ¶è¯•å›¾åœ¨å…±äº«ç¦»æ•£è¯­ä¹‰è¡¨ç¤ºä¸­ç»Ÿä¸€è§†è§‰ç†è§£å’Œç”Ÿæˆã€‚å…¶æ ¸å¿ƒæ˜¯æ–‡æœ¬å¯¹é½åˆ†è¯å™¨ï¼ˆTA-Tokï¼‰ï¼Œå®ƒä½¿ç”¨ä»å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è¯æ±‡è¡¨ä¸­æŠ•å½±çš„æ–‡æœ¬å¯¹é½ä»£ç æœ¬å°†å›¾åƒè½¬æ¢ä¸ºç¦»æ•£ä»¤ç‰Œã€‚é€šè¿‡å°†è§†è§‰å’Œæ–‡æœ¬é›†æˆåˆ°ä¸€ä¸ªå…·æœ‰æ‰©å±•è¯æ±‡è¡¨çš„ç»Ÿä¸€ç©ºé—´ä¸­ï¼Œæˆ‘ä»¬çš„å¤šæ¨¡æ€LLM Tarèƒ½å¤Ÿé€šè¿‡å…±äº«æ¥å£å®ç°è·¨æ¨¡æ€è¾“å…¥å’Œè¾“å‡ºï¼Œè€Œæ— éœ€è¿›è¡Œç‰¹å®šäºæ¨¡æ€çš„è®¾è®¡ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æå‡ºäº†è§„æ¨¡è‡ªé€‚åº”ç¼–ç å’Œè§£ç ä»¥å¹³è¡¡æ•ˆç‡å’Œè§†è§‰ç»†èŠ‚ï¼Œä»¥åŠç”Ÿæˆå»ä»¤ç‰ŒåŒ–å™¨ä»¥äº§ç”Ÿé«˜ä¿çœŸè§†è§‰è¾“å‡ºã€‚ä¸ºäº†è§£å†³ä¸åŒçš„è§£ç éœ€æ±‚ï¼Œæˆ‘ä»¬ä½¿ç”¨äº†ä¸¤ç§äº’è¡¥çš„å»ä»¤ç‰ŒåŒ–å™¨ï¼šå¿«é€Ÿè‡ªå›å½’æ¨¡å‹å’ŒåŸºäºæ‰©æ•£çš„æ¨¡å‹ã€‚ä¸ºäº†æé«˜æ¨¡æ€èåˆï¼Œæˆ‘ä»¬ç ”ç©¶äº†å…ˆè¿›çš„é¢„è®­ç»ƒä»»åŠ¡ï¼Œåœ¨è§†è§‰ç†è§£å’Œç”Ÿæˆæ–¹é¢éƒ½è¯æ˜äº†æ”¹è¿›ã€‚è·¨åŸºå‡†å®éªŒè¡¨æ˜ï¼ŒTaråŒ¹é…æˆ–è¶…è¶Šäº†ç°æœ‰çš„å¤šæ¨¡æ€LLMæ–¹æ³•ï¼Œå®ç°äº†æ›´å¿«çš„æ”¶æ•›å’Œæ›´é«˜çš„è®­ç»ƒæ•ˆç‡ã€‚ä»£ç ã€æ¨¡å‹å’Œæ•°æ®å¯åœ¨<a target="_blank" rel="noopener" href="https://tar.csuhan.comä¸Šæ‰¾åˆ°./">https://tar.csuhan.comä¸Šæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.18898v1">PDF</a> Project page: <a target="_blank" rel="noopener" href="https://tar.csuhan.com/">https://tar.csuhan.com</a></p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§å¤šæ¨¡æ€æ¡†æ¶ï¼Œæ—¨åœ¨åœ¨ä¸€ä¸ªå…±äº«ç¦»æ•£è¯­ä¹‰è¡¨ç¤ºä¸­ç»Ÿä¸€è§†è§‰ç†è§£å’Œç”Ÿæˆã€‚å…¶æ ¸å¿ƒæ˜¯æ–‡æœ¬å¯¹é½åˆ†è¯å™¨ï¼ˆTA-Tokï¼‰ï¼Œå®ƒä½¿ç”¨ä»å¤§è¯­è¨€æ¨¡å‹è¯æ±‡è¡¨ä¸­æŠ•å½±çš„æ–‡æœ¬å¯¹é½ä»£ç æœ¬å°†å›¾åƒè½¬æ¢ä¸ºç¦»æ•£ä»¤ç‰Œã€‚é€šè¿‡æ•´åˆè§†è§‰å’Œæ–‡æœ¬åˆ°ä¸€ä¸ªç»Ÿä¸€çš„ç©ºé—´å¹¶æ‰©å±•è¯æ±‡é‡ï¼Œæˆ‘ä»¬çš„å¤šæ¨¡æ€LLM Tarèƒ½å¤Ÿé€šè¿‡å…±äº«æ¥å£å®ç°è·¨æ¨¡æ€è¾“å…¥å’Œè¾“å‡ºï¼Œæ— éœ€ç‰¹å®šæ¨¡æ€çš„è®¾è®¡ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æå‡ºäº†è§„æ¨¡è‡ªé€‚åº”ç¼–ç å’Œè§£ç ä»¥å¹³è¡¡æ•ˆç‡å’Œè§†è§‰ç»†èŠ‚ï¼Œä»¥åŠç”Ÿæˆæ€§å»åˆ†è¯å™¨ä»¥äº§ç”Ÿé«˜ä¿çœŸè§†è§‰è¾“å‡ºã€‚å®éªŒè¡¨æ˜ï¼ŒTaråœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­åŒ¹é…æˆ–è¶…è¶Šäº†ç°æœ‰çš„å¤šæ¨¡æ€LLMæ–¹æ³•ï¼Œå®ç°äº†æ›´å¿«çš„æ”¶æ•›å’Œæ›´é«˜çš„è®­ç»ƒæ•ˆç‡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è®ºæ–‡æå‡ºäº†ä¸€ç§å¤šæ¨¡æ€æ¡†æ¶ï¼Œæ—¨åœ¨åœ¨å…±äº«ç¦»æ•£è¯­ä¹‰è¡¨ç¤ºä¸­ç»Ÿä¸€è§†è§‰ç†è§£å’Œç”Ÿæˆã€‚</li>
<li>æ ¸å¿ƒç»„ä»¶æ˜¯æ–‡æœ¬å¯¹é½åˆ†è¯å™¨ï¼ˆTA-Tokï¼‰ï¼Œèƒ½å°†å›¾åƒè½¬æ¢ä¸ºç¦»æ•£ä»¤ç‰Œã€‚</li>
<li>é€šè¿‡æ•´åˆè§†è§‰å’Œæ–‡æœ¬ï¼ŒTarå®ç°äº†è·¨æ¨¡æ€è¾“å…¥å’Œè¾“å‡ºï¼Œæ— éœ€ç‰¹å®šæ¨¡æ€çš„è®¾è®¡ã€‚</li>
<li>è§„æ¨¡è‡ªé€‚åº”ç¼–ç å’Œè§£ç ä»¥å¹³è¡¡æ•ˆç‡å’Œè§†è§‰ç»†èŠ‚ã€‚</li>
<li>è®ºæ–‡æå‡ºäº†ä¸¤ç§äº’è¡¥çš„è§£ç å™¨ï¼šå¿«é€Ÿè‡ªå›å½’æ¨¡å‹å’ŒåŸºäºæ‰©æ•£çš„æ¨¡å‹ã€‚</li>
<li>é€šè¿‡é«˜çº§é¢„è®­ç»ƒä»»åŠ¡å¢å¼ºæ¨¡æ€èåˆã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.18898">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-45a35d95faec56f795f287b4e15ec8ae.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-aaa419cad28e4dadf6378eadbe28e0c8.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-7a6edfc6d43b230c24de369a72f3afa1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-30ac4a2e82ef5417653b4238d4f79be2.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="ReasonFlux-PRM-Trajectory-Aware-PRMs-for-Long-Chain-of-Thought-Reasoning-in-LLMs"><a href="#ReasonFlux-PRM-Trajectory-Aware-PRMs-for-Long-Chain-of-Thought-Reasoning-in-LLMs" class="headerlink" title="ReasonFlux-PRM: Trajectory-Aware PRMs for Long Chain-of-Thought   Reasoning in LLMs"></a>ReasonFlux-PRM: Trajectory-Aware PRMs for Long Chain-of-Thought   Reasoning in LLMs</h2><p><strong>Authors:Jiaru Zou, Ling Yang, Jingwen Gu, Jiahao Qiu, Ke Shen, Jingrui He, Mengdi Wang</strong></p>
<p>Process Reward Models (PRMs) have recently emerged as a powerful framework for supervising intermediate reasoning steps in large language models (LLMs). Previous PRMs are primarily trained on model final output responses and struggle to evaluate intermediate thinking trajectories robustly, especially in the emerging setting of trajectory-response outputs generated by frontier reasoning models like Deepseek-R1. In this work, we introduce ReasonFlux-PRM, a novel trajectory-aware PRM explicitly designed to evaluate the trajectory-response type of reasoning traces. ReasonFlux-PRM incorporates both step-level and trajectory-level supervision, enabling fine-grained reward assignment aligned with structured chain-of-thought data. We adapt ReasonFlux-PRM to support reward supervision under both offline and online settings, including (i) selecting high-quality model distillation data for downstream supervised fine-tuning of smaller models, (ii) providing dense process-level rewards for policy optimization during reinforcement learning, and (iii) enabling reward-guided Best-of-N test-time scaling. Empirical results on challenging downstream benchmarks such as AIME, MATH500, and GPQA-Diamond demonstrate that ReasonFlux-PRM-7B selects higher quality data than strong PRMs (e.g., Qwen2.5-Math-PRM-72B) and human-curated baselines. Furthermore, our derived ReasonFlux-PRM-7B yields consistent performance improvements, achieving average gains of 12.1% in supervised fine-tuning, 4.5% in reinforcement learning, and 6.3% in test-time scaling. We also release our efficient ReasonFlux-PRM-1.5B for resource-constrained applications and edge deployment. Projects: <a target="_blank" rel="noopener" href="https://github.com/Gen-Verse/ReasonFlux">https://github.com/Gen-Verse/ReasonFlux</a> </p>
<blockquote>
<p>è¿‡ç¨‹å¥–åŠ±æ¨¡å‹ï¼ˆPRMsï¼‰æœ€è¿‘ä½œä¸ºç›‘ç£å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¸­çš„ä¸­é—´æ¨ç†æ­¥éª¤çš„å¼ºå¤§æ¡†æ¶è€Œå‡ºç°ã€‚ä»¥å‰çš„PRMä¸»è¦åŸºäºæ¨¡å‹çš„æœ€ç»ˆè¾“å‡ºå“åº”è¿›è¡Œè®­ç»ƒï¼Œåœ¨è¯„ä¼°å‰æ²¿æ¨ç†æ¨¡å‹ï¼ˆå¦‚Deepseek-R1ï¼‰ç”Ÿæˆçš„è½¨è¿¹å“åº”è¾“å‡ºæ—¶ï¼Œéš¾ä»¥ç¨³å¥åœ°è¯„ä¼°ä¸­é—´æ€ç»´è½¨è¿¹ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬å¼•å…¥äº†ReasonFlux-PRMï¼Œè¿™æ˜¯ä¸€ç§æ˜¾å¼è®¾è®¡ç”¨äºè¯„ä¼°è½¨è¿¹å“åº”ç±»å‹æ¨ç†ç—•è¿¹çš„æ–°å‹è½¨è¿¹æ„ŸçŸ¥PRMã€‚ReasonFlux-PRMç»“åˆäº†æ­¥éª¤çº§å’Œè½¨è¿¹çº§çš„ç›‘ç£ï¼Œèƒ½å¤Ÿå®ç°ä¸ç»“æ„åŒ–æ€ç»´é“¾æ•°æ®å¯¹é½çš„ç²¾ç»†å¥–åŠ±åˆ†é…ã€‚æˆ‘ä»¬ä½¿ReasonFlux-PRMé€‚åº”ç¦»çº¿å’Œåœ¨çº¿è®¾ç½®ä¸‹çš„å¥–åŠ±ç›‘ç£ï¼ŒåŒ…æ‹¬ï¼ˆiï¼‰é€‰æ‹©é«˜è´¨é‡æ¨¡å‹è’¸é¦æ•°æ®ï¼Œä»¥ç”¨äºä¸‹æ¸¸è¾ƒå°æ¨¡å‹çš„ç›‘ç£å¾®è°ƒï¼Œï¼ˆiiï¼‰åœ¨å¼ºåŒ–å­¦ä¹ ä¸­ä¸ºç­–ç•¥ä¼˜åŒ–æä¾›å¯†é›†çš„æµç¨‹çº§å¥–åŠ±ï¼Œä»¥åŠï¼ˆiiiï¼‰å®ç°å¥–åŠ±å¼•å¯¼çš„Best-of-Næµ‹è¯•æ—¶é—´ç¼©æ”¾ã€‚åœ¨å…·æœ‰æŒ‘æˆ˜æ€§çš„ä¸‹æ¸¸åŸºå‡†æµ‹è¯•ï¼ˆä¾‹å¦‚AIMEã€MATH500å’ŒGPQA-Diamondï¼‰ä¸Šçš„å®è¯ç»“æœè¡¨æ˜ï¼ŒReasonFlux-PRM-7Bæ¯”å¼ºå¤§çš„PRMï¼ˆä¾‹å¦‚Qwen2.5-Math-PRM-72Bï¼‰å’Œäººç±»ç²¾é€‰çš„åŸºçº¿æ›´èƒ½é€‰æ‹©é«˜è´¨é‡çš„æ•°æ®ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æ´¾ç”Ÿçš„ReasonFlux-PRM-7Båœ¨ç›‘ç£å¾®è°ƒã€å¼ºåŒ–å­¦ä¹ å’Œæµ‹è¯•æ—¶é—´ç¼©æ”¾æ–¹é¢å–å¾—äº†æŒç»­çš„æ€§èƒ½æ”¹è¿›ï¼Œå¹³å‡å¢ç›Šåˆ†åˆ«ä¸º12.1%ã€4.5%å’Œ6.3%ã€‚æˆ‘ä»¬è¿˜å‘å¸ƒäº†ç”¨äºèµ„æºå—é™åº”ç”¨å’Œè¾¹ç¼˜éƒ¨ç½²çš„é«˜æ•ˆReasonFlux-PRM-1.5Bã€‚é¡¹ç›®åœ°å€ï¼š<a target="_blank" rel="noopener" href="https://github.com/Gen-Verse/ReasonFlux">https://github.com/Gen-Verse/ReasonFlux</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.18896v1">PDF</a> Codes and Models: <a target="_blank" rel="noopener" href="https://github.com/Gen-Verse/ReasonFlux">https://github.com/Gen-Verse/ReasonFlux</a></p>
<p><strong>Summary</strong></p>
<p>åŸºäºè¿‡ç¨‹å¥–åŠ±æ¨¡å‹ï¼ˆPRMsï¼‰åœ¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ä¸­çš„ç›‘ç£ä½œç”¨ï¼Œæœ¬æ–‡æå‡ºäº†ReasonFlux-PRMï¼Œè¿™æ˜¯ä¸€ç§ä¸“é—¨ç”¨äºè¯„ä¼°æ¨ç†è½¨è¿¹çš„è½¨è¿¹æ„ŸçŸ¥PRMã€‚å®ƒç»“åˆäº†æ­¥éª¤çº§å’Œè½¨è¿¹çº§çš„ç›‘ç£ï¼Œä½¿å¥–åŠ±åˆ†é…ä¸ç»“æ„åŒ–æ€ç»´æ•°æ®ç²¾ç»†å¯¹é½ã€‚ReasonFlux-PRMèƒ½é€‚åº”ç¦»çº¿ä¸åœ¨çº¿ç¯å¢ƒä¸‹çš„å¥–åŠ±ç›‘ç£ï¼ŒåŒ…æ‹¬é€‰æ‹©é«˜è´¨é‡æ¨¡å‹è’¸é¦æ•°æ®ã€ä¸ºå¼ºåŒ–å­¦ä¹ æä¾›å¯†é›†çš„è¿‡ç¨‹çº§å¥–åŠ±ä»¥åŠå®ç°å¥–åŠ±å¼•å¯¼çš„æœ€ä½³Næµ‹è¯•æ—¶é—´ç¼©æ”¾ã€‚åœ¨AIMEã€MATH500å’ŒGPQA-Diamondç­‰åŸºå‡†æµ‹è¯•ä¸­ï¼ŒReasonFlux-PRMè¡¨ç°å‡ºå“è¶Šæ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ReasonFlux-PRMæ˜¯ä¸€ç§ä¸“é—¨è¯„ä¼°æ¨ç†è½¨è¿¹çš„è½¨è¿¹æ„ŸçŸ¥è¿‡ç¨‹å¥–åŠ±æ¨¡å‹ï¼ˆPRMï¼‰ã€‚</li>
<li>è¯¥æ¨¡å‹ç»“åˆäº†æ­¥éª¤çº§å’Œè½¨è¿¹çº§çš„ç›‘ç£ï¼Œä»¥ç²¾ç»†çš„æ–¹å¼ä¸ç»“æ„åŒ–æ€ç»´æ•°æ®å¯¹é½ã€‚</li>
<li>ReasonFlux-PRMèƒ½é€‚åº”ç¦»çº¿ä¸åœ¨çº¿ç¯å¢ƒçš„å¥–åŠ±ç›‘ç£ï¼ŒåŒ…æ‹¬é€‰æ‹©é«˜è´¨é‡æ¨¡å‹è’¸é¦æ•°æ®ã€å¼ºåŒ–å­¦ä¹ çš„æ”¿ç­–ä¼˜åŒ–å’Œæµ‹è¯•æ—¶é—´ç¼©æ”¾ã€‚</li>
<li>åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­ï¼ŒReasonFlux-PRMè¡¨ç°å‡ºå“è¶Šæ€§èƒ½ï¼Œå¦‚AIMEã€MATH500å’ŒGPQA-Diamondç­‰ã€‚</li>
<li>ä¸å…¶ä»–å¼ºå¤§çš„PRMså’Œäººç±»åŸºå‡†ç›¸æ¯”ï¼ŒReasonFlux-PRM-7Bèƒ½é€‰æ‹©æ›´é«˜è´¨é‡çš„æ•°æ®ã€‚</li>
<li>ReasonFlux-PRM-7Båœ¨ç›‘ç£å¾®è°ƒã€å¼ºåŒ–å­¦ä¹ å’Œæµ‹è¯•æ—¶é—´ç¼©æ”¾æ–¹é¢å‡å®ç°äº†æ€§èƒ½æ”¹è¿›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.18896">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-aab61dc221956decece14c17a6303868.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-dcdcb2df77765b595c28533b1ff54fac.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-56b25cd6183bd35508c70de51de3ce24.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-22863899044a8cd20252d7070a1ec8de.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-713319f28adfe8dda6d102f914890a01.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Steering-Conceptual-Bias-via-Transformer-Latent-Subspace-Activation"><a href="#Steering-Conceptual-Bias-via-Transformer-Latent-Subspace-Activation" class="headerlink" title="Steering Conceptual Bias via Transformer Latent-Subspace Activation"></a>Steering Conceptual Bias via Transformer Latent-Subspace Activation</h2><p><strong>Authors:Vansh Sharma, Venkat Raman</strong></p>
<p>This work examines whether activating latent subspaces in language models (LLMs) can steer scientific code generation toward a specific programming language. Five causal LLMs were first evaluated on scientific coding prompts to quantify their baseline bias among four programming languages. A static neuron-attribution method, perturbing the highest activated MLP weight for a C++ or CPP token, proved brittle and exhibited limited generalization across prompt styles and model scales. To address these limitations, a gradient-refined adaptive activation steering framework (G-ACT) was developed: per-prompt activation differences are clustered into a small set of steering directions, and lightweight per-layer probes are trained and refined online to select the appropriate steering vector. In LLaMA-3.2 3B, this approach reliably biases generation towards the CPP language by increasing the average probe classification accuracy by 15% and the early layers (0-6) improving the probe classification accuracy by 61.5% compared to the standard ACT framework. For LLaMA-3.3 70B, where attention-head signals become more diffuse, targeted injections at key layers still improve language selection. Although per-layer probing introduces a modest inference overhead, it remains practical by steering only a subset of layers and enables reproducible model behavior. These results demonstrate a scalable, interpretable and efficient mechanism for concept-level control for practical agentic systems. </p>
<blockquote>
<p>æœ¬æ–‡æ—¨åœ¨æ¢è®¨æ¿€æ´»è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¸­çš„æ½œåœ¨å­ç©ºé—´æ˜¯å¦èƒ½å¼•å¯¼ç§‘å­¦ä»£ç ç”Ÿæˆæœå‘ç‰¹å®šçš„ç¼–ç¨‹è¯­è¨€ã€‚é¦–å…ˆï¼Œå¯¹äº”ç§å› æœLLMè¿›è¡Œç§‘å­¦ç¼–ç æç¤ºè¯„ä¼°ï¼Œä»¥é‡åŒ–å®ƒä»¬åœ¨å››ç§ç¼–ç¨‹è¯­è¨€ä¹‹é—´çš„åŸºçº¿åè§ã€‚ä¸€ç§é™æ€ç¥ç»å…ƒå½’å› æ–¹æ³•ï¼Œé€šè¿‡æ‰°åŠ¨C++æˆ–CPPæ ‡è®°çš„æœ€é«˜æ¿€æ´»MLPæƒé‡ï¼Œè¢«è¯æ˜æ˜¯è„†å¼±çš„ï¼Œå¹¶ä¸”åœ¨æç¤ºé£æ ¼å’Œæ¨¡å‹è§„æ¨¡æ–¹é¢è¡¨ç°å‡ºæœ‰é™çš„æ³›åŒ–èƒ½åŠ›ã€‚ä¸ºäº†è§£å†³è¿™äº›å±€é™æ€§ï¼Œå¼€å‘äº†ä¸€ä¸ªåŸºäºæ¢¯åº¦ç²¾åŒ–çš„è‡ªé€‚åº”æ¿€æ´»å¼•å¯¼æ¡†æ¶ï¼ˆG-ACTï¼‰ï¼šå°†æ¯ä¸ªæç¤ºçš„æ¿€æ´»å·®å¼‚èšé›†æˆä¸€å°éƒ¨åˆ†å¼•å¯¼æ–¹å‘é›†ï¼Œå¹¶åœ¨çº¿è®­ç»ƒå’Œç²¾åŒ–è½»é‡çº§çš„é€å±‚æ¢é’ˆï¼Œä»¥é€‰æ‹©é€‚å½“çš„å¼•å¯¼å‘é‡ã€‚åœ¨LLaMA-3.2 3Bä¸­ï¼Œè¿™ç§æ–¹æ³•é€šè¿‡æé«˜æ¢é’ˆåˆ†ç±»å‡†ç¡®ç‡15%ï¼Œå¹¶ä¸”åœ¨æ—©æœŸå±‚ï¼ˆ0-6å±‚ï¼‰ä¸æ ‡å‡†ACTæ¡†æ¶ç›¸æ¯”ï¼Œæ¢é’ˆåˆ†ç±»å‡†ç¡®ç‡æé«˜äº†61.5%ï¼Œä»è€Œå¯é åœ°å°†ç”Ÿæˆåå‘CPPè¯­è¨€ã€‚å¯¹äºæ³¨æ„åŠ›å¤´ä¿¡å·æ›´åŠ åˆ†æ•£çš„LLaMA-3.3 70Bï¼Œå¯¹å…³é”®å±‚çš„é’ˆå¯¹æ€§æ³¨å…¥ä»ç„¶èƒ½æ”¹å–„è¯­è¨€é€‰æ‹©ã€‚å°½ç®¡é€å±‚æ¢æµ‹å¼•å…¥äº†é€‚ä¸­çš„æ¨ç†å¼€é”€ï¼Œä½†å®ƒåªå¼•å¯¼éƒ¨åˆ†å±‚ï¼Œä»ç„¶å…·æœ‰å®ç”¨æ€§ï¼Œå¹¶èƒ½å®ç°å¯é‡å¤æ¨¡å‹è¡Œä¸ºã€‚è¿™äº›ç»“æœå±•ç¤ºäº†ä¸€ç§å¯è§„æ¨¡åŒ–ã€å¯è§£é‡Šå’Œé«˜æ•ˆçš„æœºåˆ¶ï¼Œç”¨äºå®ç°å®é™…ä»£ç†ç³»ç»Ÿçš„æ¦‚å¿µçº§æ§åˆ¶ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.18887v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ¢è®¨äº†æ¿€æ´»è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¸­çš„æ½œåœ¨å­ç©ºé—´æ˜¯å¦å¯ä»¥å¼•å¯¼ç§‘å­¦ä»£ç ç”Ÿæˆç‰¹å®šç¼–ç¨‹è¯­è¨€çš„é—®é¢˜ã€‚é€šè¿‡å¯¹äº”ä¸ªå› æœLLMåœ¨ç§‘æŠ€ç¼–ç æç¤ºä¸Šçš„è¯„ä¼°ï¼Œå‘ç°å®ƒä»¬åœ¨å››ç§ç¼–ç¨‹è¯­è¨€ä¸­çš„åŸºçº¿åè§ã€‚ç ”ç©¶æå‡ºä¸€ç§æ¢¯åº¦ä¼˜åŒ–è‡ªé€‚åº”æ¿€æ´»å¼•å¯¼æ¡†æ¶ï¼ˆG-ACTï¼‰ï¼Œé€šè¿‡åœ¨æ—©æœŸå±‚çº§ä¸­é€‰æ‹©é€‚å½“çš„å¼•å¯¼å‘é‡æ¥å¼•å¯¼ä»£ç ç”Ÿæˆæœç€ç‰¹å®šè¯­è¨€ï¼ˆå¦‚CPPï¼‰å‘å±•ã€‚è¯¥æ¡†æ¶èƒ½æé«˜æ¢æµ‹å™¨çš„åˆ†ç±»å‡†ç¡®åº¦ï¼Œå±•ç°å‡ºå¼ºå¤§çš„å®ç”¨ä»·å€¼ã€‚å°½ç®¡æœ‰ä¸€å®šæ¨ç†æ—¶é—´å¼€é”€ï¼Œä½†é€šè¿‡ä»…å¼•å¯¼éƒ¨åˆ†å±‚çº§ä»å¯å®ç°é«˜æ•ˆã€å¯è§£é‡Šçš„æ¨¡å‹æ§åˆ¶æœºåˆ¶ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLMæ¨¡å‹åœ¨ç§‘å­¦ä»£ç ç”Ÿæˆä¸­å±•ç°å¯¹ç‰¹å®šç¼–ç¨‹è¯­è¨€çš„åŸºçº¿åè§ã€‚</li>
<li>é€šè¿‡æ¿€æ´»è¯­è¨€æ¨¡å‹ä¸­çš„ç‰¹å®šç¥ç»å…ƒæ¥å¼•å¯¼ä»£ç ç”Ÿæˆæœå‘ç‰¹å®šè¯­è¨€æˆä¸ºå¯èƒ½ã€‚</li>
<li>é™æ€ç¥ç»å…ƒå½’å› æ–¹æ³•å¯¹äºè·¨æç¤ºé£æ ¼å’Œæ¨¡å‹è§„æ¨¡å­˜åœ¨å±€é™æ€§ã€‚</li>
<li>æå‡ºä¸€ç§æ¢¯åº¦ä¼˜åŒ–è‡ªé€‚åº”æ¿€æ´»å¼•å¯¼æ¡†æ¶ï¼ˆG-ACTï¼‰ï¼Œèƒ½æœ‰æ•ˆæé«˜æ¢æµ‹å™¨çš„åˆ†ç±»å‡†ç¡®åº¦ã€‚</li>
<li>åœ¨æ—©æœŸå±‚çº§ä½¿ç”¨G-ACTæ¡†æ¶èƒ½æ˜¾è‘—æé«˜åˆ†ç±»å‡†ç¡®åº¦ã€‚</li>
<li>åœ¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆå¦‚LLaMA-3.3 70Bï¼‰ä¸­ï¼Œå…³é”®å±‚çº§çš„é’ˆå¯¹æ€§æ³¨å…¥ä»èƒ½æé«˜è¯­è¨€é€‰æ‹©èƒ½åŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.18887">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-d4455c08b684d7577c10ebdd963c91c1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d25307bfe798585971269871d70afbec.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Universal-Video-Temporal-Grounding-with-Generative-Multi-modal-Large-Language-Models"><a href="#Universal-Video-Temporal-Grounding-with-Generative-Multi-modal-Large-Language-Models" class="headerlink" title="Universal Video Temporal Grounding with Generative Multi-modal Large   Language Models"></a>Universal Video Temporal Grounding with Generative Multi-modal Large   Language Models</h2><p><strong>Authors:Zeqian Li, Shangzhe Di, Zhonghua Zhai, Weilin Huang, Yanfeng Wang, Weidi Xie</strong></p>
<p>This paper presents a computational model for universal video temporal grounding, which accurately localizes temporal moments in videos based on natural language queries (e.g., questions or descriptions). Unlike existing methods that are often limited to specific video domains or durations, we propose UniTime, a robust and universal video grounding model leveraging the strong vision-language understanding capabilities of generative Multi-modal Large Language Models (MLLMs). Our model effectively handles videos of diverse views, genres, and lengths while comprehending complex language queries. The key contributions include: (i) We consider steering strong MLLMs for temporal grounding in videos. To enable precise timestamp outputs, we incorporate temporal information by interleaving timestamp tokens with video tokens. (ii) By training the model to handle videos with different input granularities through adaptive frame scaling, our approach achieves robust temporal grounding for both short and long videos. (iii) Comprehensive experiments show that UniTime outperforms state-of-the-art approaches in both zero-shot and dataset-specific finetuned settings across five public temporal grounding benchmarks. (iv) When employed as a preliminary moment retriever for long-form video question-answering (VideoQA), UniTime significantly improves VideoQA accuracy, highlighting its value for complex video understanding tasks. </p>
<blockquote>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§ç”¨äºé€šç”¨è§†é¢‘æ—¶é—´å®šä½çš„è®¡ç®—æ¨¡å‹ã€‚è¯¥æ¨¡å‹èƒ½å¤ŸåŸºäºè‡ªç„¶è¯­è¨€æŸ¥è¯¢ï¼ˆå¦‚é—®é¢˜æˆ–æè¿°ï¼‰å‡†ç¡®åœ°åœ¨è§†é¢‘ä¸­å®šä½æ—¶é—´æ—¶åˆ»ã€‚ä¸ç°æœ‰æ–¹æ³•å¸¸å¸¸å±€é™äºç‰¹å®šè§†é¢‘é¢†åŸŸæˆ–æ—¶é•¿ä¸åŒï¼Œæˆ‘ä»¬æå‡ºäº†UniTimeï¼Œä¸€ä¸ªç¨³å¥ä¸”é€šç”¨çš„è§†é¢‘å®šä½æ¨¡å‹ï¼Œå®ƒåˆ©ç”¨ç”Ÿæˆå¼å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰çš„å¼ºå¤§è§†è§‰è¯­è¨€ç†è§£èƒ½åŠ›ã€‚æˆ‘ä»¬çš„æ¨¡å‹èƒ½å¤Ÿæœ‰æ•ˆåœ°å¤„ç†å…·æœ‰å¤šç§è§†è§’ã€ç±»å‹å’Œæ—¶é•¿çš„è§†é¢‘ï¼ŒåŒæ—¶ç†è§£å¤æ‚çš„è¯­è¨€æŸ¥è¯¢ã€‚ä¸»è¦è´¡çŒ®åŒ…æ‹¬ï¼šï¼ˆiï¼‰æˆ‘ä»¬è€ƒè™‘åˆ©ç”¨å¼ºå¤§çš„MLLMsè¿›è¡Œè§†é¢‘æ—¶é—´å®šä½ã€‚ä¸ºäº†å®ç°ç²¾ç¡®çš„æ—¶é—´æˆ³è¾“å‡ºï¼Œæˆ‘ä»¬é€šè¿‡å°†æ—¶é—´æˆ³ä»¤ç‰Œä¸è§†é¢‘ä»¤ç‰Œäº¤é”™ï¼Œæ¥èå…¥æ—¶é—´ä¿¡æ¯ã€‚ï¼ˆiiï¼‰é€šè¿‡è®­ç»ƒæ¨¡å‹ä»¥ä¸åŒçš„è¾“å…¥ç²’åº¦å¤„ç†è§†é¢‘ï¼Œé€šè¿‡è‡ªé€‚åº”å¸§ç¼©æ”¾ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨å®ç°çŸ­è§†é¢‘å’Œé•¿è§†é¢‘çš„ç¨³å¥æ—¶é—´å®šä½ã€‚ï¼ˆiiiï¼‰ç»¼åˆå®éªŒè¡¨æ˜ï¼ŒUniTimeåœ¨äº”ä¸ªå…¬å…±æ—¶é—´å®šä½åŸºå‡†æµ‹è¯•ä¸Šï¼Œæ— è®ºæ˜¯é›¶æ ·æœ¬è¿˜æ˜¯ç‰¹å®šæ•°æ®é›†å¾®è°ƒè®¾ç½®ï¼Œå‡ä¼˜äºæœ€æ–°æ–¹æ³•ã€‚ï¼ˆivï¼‰å½“ä½œä¸ºé•¿å½¢å¼è§†é¢‘é—®ç­”ï¼ˆVideoQAï¼‰çš„åˆæ­¥æ—¶åˆ»æ£€ç´¢å™¨æ—¶ï¼ŒUniTimeæ˜¾è‘—æé«˜äº†VideoQAçš„å‡†ç¡®æ€§ï¼Œçªæ˜¾å…¶åœ¨å¤æ‚è§†é¢‘ç†è§£ä»»åŠ¡ä¸­çš„ä»·å€¼ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.18883v1">PDF</a> </p>
<p><strong>Summary</strong>ï¼š<br>æœ¬æ–‡æå‡ºäº†ä¸€ç§ç”¨äºé€šç”¨è§†é¢‘æ—¶é—´å®šä½çš„è®¡ç®—æ¨¡å‹ï¼Œèƒ½å¤ŸåŸºäºè‡ªç„¶è¯­è¨€æŸ¥è¯¢å‡†ç¡®åœ°å¯¹è§†é¢‘ä¸­çš„æ—¶é—´ç‰‡æ®µè¿›è¡Œå®šä½ã€‚æ¨¡å‹å€ŸåŠ©ç”Ÿæˆå¼å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰çš„è§†å¬ç†è§£èƒ½åŠ›ï¼Œå®ç°äº†ç¨³å¥å’Œé€šç”¨çš„è§†é¢‘å®šä½ã€‚é€šè¿‡èå…¥æ—¶é—´ä¿¡æ¯å¹¶è®­ç»ƒæ¨¡å‹é€‚åº”ä¸åŒç²’åº¦çš„è§†é¢‘è¾“å…¥ï¼Œè¯¥æ¨¡å‹èƒ½å¤Ÿç²¾ç¡®å¤„ç†é•¿çŸ­ä¸ä¸€çš„è§†é¢‘ï¼Œå¹¶åœ¨å¤šä¸ªå…¬å¼€æ—¶é—´å®šä½åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºè¶…è¶Šç°æœ‰æ–¹æ³•çš„æ•ˆæœã€‚æ­¤å¤–ï¼Œå°†å…¶åº”ç”¨äºé•¿è§†é¢‘é—®ç­”ä»»åŠ¡æ—¶ï¼Œèƒ½æ˜¾è‘—æé«˜å‡†ç¡®ç‡ï¼Œè¯æ˜äº†å…¶åœ¨å¤æ‚è§†é¢‘ç†è§£ä»»åŠ¡ä¸­çš„ä»·å€¼ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>è¯¥è®ºæ–‡æå‡ºäº†ä¸€ç§è®¡ç®—æ¨¡å‹UniTimeï¼Œç”¨äºåŸºäºè‡ªç„¶è¯­è¨€æŸ¥è¯¢çš„è§†é¢‘æ—¶é—´å®šä½ã€‚</li>
<li>UniTimeåˆ©ç”¨ç”Ÿæˆå¼å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰è¿›è¡Œç¨³å¥çš„è§†é¢‘å®šä½ã€‚</li>
<li>æ¨¡å‹é€šè¿‡èå…¥æ—¶é—´ä¿¡æ¯å®ç°äº†ç²¾ç¡®çš„æ—¶é—´å®šä½ã€‚</li>
<li>UniTimeèƒ½å¤Ÿé€‚åº”ä¸åŒç²’åº¦çš„è§†é¢‘è¾“å…¥ï¼Œå®ç°é•¿çŸ­è§†é¢‘çš„ç¨³å¥æ—¶é—´å®šä½ã€‚</li>
<li>åœ¨å¤šä¸ªå…¬å¼€åŸºå‡†æµ‹è¯•ä¸­ï¼ŒUniTimeè¡¨ç°å‡ºè¶…è¶Šç°æœ‰æ–¹æ³•çš„æ•ˆæœã€‚</li>
<li>UniTimeå¯ä½œä¸ºé•¿è§†é¢‘é—®ç­”ä»»åŠ¡çš„åˆæ­¥æ—¶åˆ»æ£€ç´¢å™¨ï¼Œæ˜¾è‘—æé«˜å‡†ç¡®ç‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.18883">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-5b7913dbda4ce5f6600e059ce178aa96.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-42c4d8f7a78098e60859b291495d9319.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-52a8840640b370a987d55231f4c3b637.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-333e03c8bf46894882680d8b09273118.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-acaeffb61f8e20097cf229143f2510be.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="CommVQ-Commutative-Vector-Quantization-for-KV-Cache-Compression"><a href="#CommVQ-Commutative-Vector-Quantization-for-KV-Cache-Compression" class="headerlink" title="CommVQ: Commutative Vector Quantization for KV Cache Compression"></a>CommVQ: Commutative Vector Quantization for KV Cache Compression</h2><p><strong>Authors:Junyan Li, Yang Zhang, Muhammad Yusuf Hassan, Talha Chafekar, Tianle Cai, Zhile Ren, Pengsheng Guo, Foroozan Karimzadeh, Colorado Reed, Chong Wang, Chuang Gan</strong></p>
<p>Large Language Models (LLMs) are increasingly used in applications requiring long context lengths, but the key-value (KV) cache often becomes a memory bottleneck on GPUs as context grows. To address this, we propose Commutative Vector Quantization (CommVQ) to significantly reduce memory usage for long-context LLM inference. We first introduce additive quantization with a lightweight encoder and codebook to compress the KV cache, which can be decoded via simple matrix multiplication. To further reduce computational costs during decoding, we design the codebook to be commutative with Rotary Position Embedding (RoPE) and train it using an Expectation-Maximization (EM) algorithm. This enables efficient integration of decoding into the self-attention mechanism. Our approach achieves high accuracy with additive quantization and low overhead via the RoPE-commutative codebook. Experiments on long-context benchmarks and GSM8K show that our method reduces FP16 KV cache size by 87.5% with 2-bit quantization, while outperforming state-of-the-art KV cache quantization methods. Notably, it enables 1-bit KV cache quantization with minimal accuracy loss, allowing a LLaMA-3.1 8B model to run with a 128K context length on a single RTX 4090 GPU. The source code is available at: <a target="_blank" rel="noopener" href="https://github.com/UMass-Embodied-AGI/CommVQ">https://github.com/UMass-Embodied-AGI/CommVQ</a>. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨éœ€è¦é•¿ä¸Šä¸‹æ–‡çš„åº”ç”¨ä¸­è¶Šæ¥è¶Šå—æ¬¢è¿ï¼Œä½†é”®å€¼ï¼ˆKVï¼‰ç¼“å­˜éšç€ä¸Šä¸‹æ–‡çš„å¢é•¿å¾€å¾€æˆä¸ºGPUä¸Šçš„å†…å­˜ç“¶é¢ˆã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†å¯äº¤æ¢å‘é‡é‡åŒ–ï¼ˆCommVQï¼‰æ¥æ˜¾è‘—å‡å°‘é•¿ä¸Šä¸‹æ–‡LLMæ¨ç†ä¸­çš„å†…å­˜ä½¿ç”¨ã€‚æˆ‘ä»¬é¦–å…ˆä½¿ç”¨è½»é‡çº§ç¼–ç å™¨å’Œä»£ç æœ¬å¼•å…¥åŠ æ³•é‡åŒ–æ¥å‹ç¼©KVç¼“å­˜ï¼Œå®ƒå¯ä»¥é€šè¿‡ç®€å•çš„çŸ©é˜µä¹˜æ³•è¿›è¡Œè§£ç ã€‚ä¸ºäº†è¿›ä¸€æ­¥é™ä½è§£ç è¿‡ç¨‹ä¸­çš„è®¡ç®—æˆæœ¬ï¼Œæˆ‘ä»¬è®¾è®¡ä»£ç æœ¬ä¸æ—‹è½¬ä½ç½®åµŒå…¥ï¼ˆRoPEï¼‰å¯äº¤æ¢ï¼Œå¹¶ä½¿ç”¨æœŸæœ›æœ€å¤§åŒ–ï¼ˆEMï¼‰ç®—æ³•è¿›è¡Œè®­ç»ƒã€‚è¿™ä½¿å¾—è§£ç èƒ½å¤Ÿé«˜æ•ˆåœ°é›†æˆåˆ°è‡ªæ³¨æ„åŠ›æœºåˆ¶ä¸­ã€‚æˆ‘ä»¬çš„æ–¹æ³•é€šè¿‡RoPEå¯äº¤æ¢ä»£ç æœ¬å®ç°äº†åŠ æ³•é‡åŒ–çš„é«˜å‡†ç¡®æ€§å’Œä½å¼€é”€ã€‚åœ¨é•¿ä¸Šä¸‹æ–‡åŸºå‡†æµ‹è¯•å’ŒGSM8Kä¸Šçš„å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨2ä½é‡åŒ–çš„æƒ…å†µä¸‹å°†FP16 KVç¼“å­˜å¤§å°å‡å°‘äº†87.5%ï¼ŒåŒæ—¶ä¼˜äºæœ€æ–°çš„KVç¼“å­˜é‡åŒ–æ–¹æ³•ã€‚å°¤å…¶å€¼å¾—ä¸€æçš„æ˜¯ï¼Œå®ƒå®ç°äº†å…·æœ‰æå°ç²¾åº¦æŸå¤±çš„1ä½KVç¼“å­˜é‡åŒ–ï¼Œå…è®¸åœ¨å•ä¸ªRTX 4090 GPUä¸Šè¿è¡ŒLLaMA-3.1 8Bæ¨¡å‹çš„128Kä¸Šä¸‹æ–‡é•¿åº¦ã€‚æºä»£ç å¯åœ¨ï¼š<a target="_blank" rel="noopener" href="https://github.com/UMass-Embodied-AGI/CommVQ%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/UMass-Embodied-AGI/CommVQè·å–ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.18879v1">PDF</a> ICML 2025 poster</p>
<p><strong>æ‘˜è¦</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨å¤„ç†éœ€è¦é•¿è¯­å¢ƒçš„åº”ç”¨æ—¶ï¼Œé”®å€¼ï¼ˆKVï¼‰ç¼“å­˜æˆä¸ºGPUå†…å­˜ç“¶é¢ˆçš„é—®é¢˜æ—¥ç›Šçªå‡ºã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºä½¿ç”¨å¯äº¤æ¢å‘é‡é‡åŒ–ï¼ˆCommVQï¼‰æ¥æ˜¾è‘—å‡å°‘é•¿è¯­å¢ƒLLMæ¨ç†çš„å†…å­˜ä½¿ç”¨ã€‚æˆ‘ä»¬é¦–æ¬¡å¼•å…¥åŠ æ³•é‡åŒ–ï¼Œé€šè¿‡è½»é‡çº§ç¼–ç å™¨å’Œä»£ç æœ¬å‹ç¼©KVç¼“å­˜ï¼Œè§£ç æ—¶åªéœ€è¿›è¡Œç®€å•çš„çŸ©é˜µä¹˜æ³•ã€‚ä¸ºè¿›ä¸€æ­¥é™ä½è§£ç è¿‡ç¨‹ä¸­çš„è®¡ç®—æˆæœ¬ï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸€ä¸ªä¸æ—‹è½¬ä½ç½®åµŒå…¥ï¼ˆRoPEï¼‰å¯äº¤æ¢çš„ä»£ç æœ¬ï¼Œå¹¶ä½¿ç”¨æœŸæœ›æœ€å¤§åŒ–ï¼ˆEMï¼‰ç®—æ³•è¿›è¡Œè®­ç»ƒã€‚è¿™èƒ½å¤Ÿå®ç°é«˜æ•ˆåœ°å°†è§£ç é›†æˆåˆ°è‡ªæ³¨æ„åŠ›æœºåˆ¶ä¸­ã€‚æˆ‘ä»¬çš„æ–¹æ³•é€šè¿‡åŠ æ³•é‡åŒ–å®ç°äº†é«˜å‡†ç¡®ç‡ï¼Œå¹¶é€šè¿‡RoPEå¯äº¤æ¢ä»£ç æœ¬å®ç°äº†ä½å¼€é”€ã€‚åœ¨é•¿è¯­å¢ƒåŸºå‡†æµ‹è¯•å’ŒGSM8Kä¸Šçš„å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•å°†FP16 KVç¼“å­˜å¤§å°å‡å°‘äº†87.5%ï¼ŒåŒæ—¶å®ç°äº†2ä½é‡åŒ–ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œå®ƒå®ç°äº†1ä½KVç¼“å­˜é‡åŒ–ï¼Œå¯¹ç²¾åº¦æŸå¤±æå°ï¼Œå…è®¸LLaMA-3.1 8Bæ¨¡å‹åœ¨å•ä¸ªRTX 4090 GPUä¸Šè¿è¡Œï¼Œè¯­å¢ƒé•¿åº¦è¾¾åˆ°128Kã€‚æºä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/UMass-Embodied-AGI/CommVQ%E8%AE%BF%E9%97%AE%E3%80%82">https://github.com/UMass-Embodied-AGI/CommVQè®¿é—®ã€‚</a></p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹åœ¨å¤„ç†é•¿è¯­å¢ƒåº”ç”¨æ—¶é¢ä¸´KVç¼“å­˜å†…å­˜ç“¶é¢ˆé—®é¢˜ã€‚</li>
<li>CommutVQé€šè¿‡åŠ æ³•é‡åŒ–æ˜¾è‘—å‡å°‘KVç¼“å­˜å†…å­˜ä½¿ç”¨ã€‚</li>
<li>å¼•å…¥è½»é‡çº§ç¼–ç å™¨å’Œä»£ç æœ¬è¿›è¡ŒKVç¼“å­˜å‹ç¼©ï¼Œè§£ç è¿‡ç¨‹é«˜æ•ˆã€‚</li>
<li>åˆ©ç”¨å¯äº¤æ¢ä»£ç æœ¬å’ŒRoPEæŠ€æœ¯é™ä½è§£ç è®¡ç®—æˆæœ¬ã€‚</li>
<li>CommutVQå®ç°é«˜å‡†ç¡®ç‡å¹¶é™ä½å†…å­˜å¼€é”€ã€‚</li>
<li>å®éªŒæ˜¾ç¤ºï¼Œè¯¥æ–¹æ³•æ˜¾è‘—å‡å°‘KVç¼“å­˜å¤§å°ï¼Œå®ç°é«˜æ•ˆé•¿è¯­å¢ƒLLMæ¨ç†ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.18879">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-455078ef0f846c1a313096e1e330a534.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1056607aef8b56a37db50e6755f0ad78.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="TAMMs-Temporal-Aware-Multimodal-Model-for-Satellite-Image-Change-Understanding-and-Forecasting"><a href="#TAMMs-Temporal-Aware-Multimodal-Model-for-Satellite-Image-Change-Understanding-and-Forecasting" class="headerlink" title="TAMMs: Temporal-Aware Multimodal Model for Satellite Image Change   Understanding and Forecasting"></a>TAMMs: Temporal-Aware Multimodal Model for Satellite Image Change   Understanding and Forecasting</h2><p><strong>Authors:Zhongbin Guo, Yuhao Wang, Ping Jian, Xinyue Chen, Wei Peng, Ertai E</strong></p>
<p>Satellite image time-series analysis demands fine-grained spatial-temporal reasoning, which remains a challenge for existing multimodal large language models (MLLMs). In this work, we study the capabilities of MLLMs on a novel task that jointly targets temporal change understanding and future scene generation, aiming to assess their potential for modeling complex multimodal dynamics over time. We propose TAMMs, a Temporal-Aware Multimodal Model for satellite image change understanding and forecasting, which enhances frozen MLLMs with lightweight temporal modules for structured sequence encoding and contextual prompting. To guide future image generation, TAMMs introduces a Semantic-Fused Control Injection (SFCI) mechanism that adaptively combines high-level semantic reasoning and structural priors within an enhanced ControlNet. This dual-path conditioning enables temporally consistent and semantically grounded image synthesis. Experiments demonstrate that TAMMs outperforms strong MLLM baselines in both temporal change understanding and future image forecasting tasks, highlighting how carefully designed temporal reasoning and semantic fusion can unlock the full potential of MLLMs for spatio-temporal understanding. </p>
<blockquote>
<p>å«æ˜Ÿå›¾åƒæ—¶é—´åºåˆ—åˆ†æéœ€è¦ç²¾ç»†çš„æ—¶ç©ºæ¨ç†ï¼Œè¿™å¯¹ç°æœ‰çš„å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰æ¥è¯´ä»ç„¶æ˜¯ä¸€ä¸ªæŒ‘æˆ˜ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬ç ”ç©¶MLLMsåœ¨ä¸€ä¸ªæ–°ä»»åŠ¡ä¸Šçš„èƒ½åŠ›ï¼Œè¯¥ä»»åŠ¡æ—¨åœ¨ç†è§£æ—¶é—´å˜åŒ–å’Œæœªæ¥åœºæ™¯ç”Ÿæˆï¼Œä»¥è¯„ä¼°å®ƒä»¬åœ¨å»ºæ¨¡éšæ—¶é—´å˜åŒ–çš„å¤æ‚å¤šæ¨¡æ€åŠ¨æ€æ–¹é¢çš„æ½œåŠ›ã€‚æˆ‘ä»¬æå‡ºäº†TAMMsï¼Œè¿™æ˜¯ä¸€ä¸ªç”¨äºå«æ˜Ÿå›¾åƒå˜åŒ–ç†è§£å’Œé¢„æµ‹çš„æ—¶é—´æ„ŸçŸ¥å¤šæ¨¡æ€æ¨¡å‹ï¼Œå®ƒé€šè¿‡è½»é‡çº§çš„ä¸´æ—¶æ¨¡å—å’Œç»“æ„åŒ–åºåˆ—ç¼–ç ä»¥åŠä¸Šä¸‹æ–‡æç¤ºæ¥å¢å¼ºå†»ç»“çš„MLLMsã€‚ä¸ºäº†æŒ‡å¯¼æœªæ¥å›¾åƒçš„ç”Ÿæˆï¼ŒTAMMså¼•å…¥äº†ä¸€ç§è¯­ä¹‰èåˆæ§åˆ¶æ³¨å…¥ï¼ˆSFCIï¼‰æœºåˆ¶ï¼Œè¯¥æœºåˆ¶åœ¨ä¸€ä¸ªå¢å¼ºçš„ControlNetä¸­è‡ªé€‚åº”åœ°ç»“åˆäº†é«˜çº§è¯­ä¹‰æ¨ç†å’Œç»“æ„å…ˆéªŒã€‚è¿™ç§åŒè·¯å¾„æ¡ä»¶ä½¿å¾—å›¾åƒåˆæˆåœ¨æ—¶é—´ä¸Šä¿æŒä¸€è‡´ï¼Œå¹¶åœ¨è¯­ä¹‰ä¸Šå¾—ä»¥éªŒè¯ã€‚å®éªŒè¡¨æ˜ï¼ŒTAMMsåœ¨ç†è§£æ—¶é—´å˜åŒ–å’Œé¢„æµ‹æœªæ¥å›¾åƒçš„ä»»åŠ¡ä¸Šå‡ä¼˜äºå¼ºå¤§çš„MLLMåŸºå‡†æ¨¡å‹ï¼Œè¿™çªæ˜¾äº†ç²¾å¿ƒè®¾è®¡çš„ä¸´æ—¶æ¨ç†å’Œè¯­ä¹‰èåˆå¦‚ä½•è§£é”MLLMsåœ¨æ—¶ç©ºç†è§£æ–¹é¢çš„å…¨éƒ¨æ½œåŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.18862v1">PDF</a> Submitted to the 33rd ACM International Conference on Multimedia. Our   dataset can be found at <a target="_blank" rel="noopener" href="https://huggingface.co/datasets/IceInPot/TAMMs">https://huggingface.co/datasets/IceInPot/TAMMs</a></p>
<p><strong>Summary</strong></p>
<p>å«æ˜Ÿå›¾åƒæ—¶é—´åºåˆ—åˆ†æéœ€è¦ç²¾ç»†çš„ç©ºé—´æ—¶é—´æ¨ç†èƒ½åŠ›ï¼Œè¿™å¯¹ç°æœ‰çš„å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰æ¥è¯´ä»ç„¶æ˜¯ä¸€ä¸ªæŒ‘æˆ˜ã€‚æœ¬ç ”ç©¶æ—¨åœ¨è¯„ä¼°MLLMsåœ¨ç†è§£æ—¶é—´å˜åŒ–å’Œé¢„æµ‹æœªæ¥åœºæ™¯æ–¹é¢çš„æ½œåŠ›ã€‚æˆ‘ä»¬æå‡ºäº†TAMMsæ¨¡å‹ï¼Œè¿™æ˜¯ä¸€ç§ç”¨äºå«æ˜Ÿå›¾åƒå˜åŒ–ç†è§£å’Œé¢„æµ‹çš„æ—¶é—´æ„ŸçŸ¥å¤šæ¨¡æ€æ¨¡å‹ã€‚å®ƒé€šè¿‡æ·»åŠ è½»é‡çº§çš„æ—¶é—´æ¨¡å—è¿›è¡Œç»“æ„åŒ–åºåˆ—ç¼–ç å’Œä¸Šä¸‹æ–‡æç¤ºï¼Œå¢å¼ºäº†å†»ç»“çš„MLLMsçš„åŠŸèƒ½ã€‚ä¸ºäº†æŒ‡å¯¼æœªæ¥å›¾åƒçš„ç”Ÿæˆï¼ŒTAMMså¼•å…¥äº†è¯­ä¹‰èåˆæ§åˆ¶æ³¨å…¥ï¼ˆSFCIï¼‰æœºåˆ¶ï¼Œè¯¥æœºåˆ¶è‡ªé€‚åº”åœ°ç»“åˆäº†é«˜çº§è¯­ä¹‰æ¨ç†å’Œç»“æ„å…ˆéªŒçŸ¥è¯†åœ¨ä¸€ä¸ªå¢å¼ºçš„ControlNetä¸­ã€‚è¿™ç§åŒè·¯å¾„æ¡ä»¶ä½¿å¾—å›¾åƒåˆæˆåœ¨æ—¶é—´ä¸Šæ˜¯è¿è´¯çš„ä¸”åœ¨è¯­ä¹‰ä¸Šæ˜¯æœ‰æ ¹æ®çš„ã€‚å®éªŒè¡¨æ˜ï¼ŒTAMMsåœ¨ç†è§£æ—¶é—´å˜åŒ–å’Œé¢„æµ‹æœªæ¥å›¾åƒçš„ä»»åŠ¡ä¸Šéƒ½è¶…è¶Šäº†å¼ºå¤§çš„MLLMåŸºå‡†æµ‹è¯•ï¼Œå±•ç°äº†ç²¾å¿ƒè®¾è®¡çš„æ—¶é—´æ¨ç†å’Œè¯­ä¹‰èåˆå¦‚ä½•è§£é”MLLMsåœ¨æ—¶ç©ºç†è§£æ–¹é¢çš„æ½œåŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å«æ˜Ÿå›¾åƒæ—¶é—´åºåˆ—åˆ†æéœ€è¦å¼ºå¤§çš„ç©ºé—´æ—¶é—´æ¨ç†èƒ½åŠ›ï¼Œè¿™ä»æ˜¯å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹çš„æŒ‘æˆ˜ã€‚</li>
<li>æœ¬ç ”ç©¶è¯„ä¼°äº†å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹åœ¨ç†è§£æ—¶é—´å˜åŒ–å’Œé¢„æµ‹æœªæ¥åœºæ™¯æ–¹é¢çš„æ½œåŠ›ã€‚<br>3.TAMMsæ¨¡å‹æ˜¯ä¸€ç§ç”¨äºå«æ˜Ÿå›¾åƒå˜åŒ–ç†è§£å’Œé¢„æµ‹çš„æ—¶é—´æ„ŸçŸ¥å¤šæ¨¡æ€æ¨¡å‹ï¼Œå®ƒé€šè¿‡æ·»åŠ è½»é‡çº§çš„æ—¶é—´æ¨¡å—è¿›è¡Œç»“æ„åŒ–åºåˆ—ç¼–ç å’Œä¸Šä¸‹æ–‡æç¤ºæ¥å¢å¼ºæ€§èƒ½ã€‚<br>4.TAMMsé€šè¿‡è¯­ä¹‰èåˆæ§åˆ¶æ³¨å…¥æœºåˆ¶ç»“åˆé«˜çº§è¯­ä¹‰æ¨ç†å’Œç»“æ„å…ˆéªŒçŸ¥è¯†ï¼Œå®ç°æ—¶ç©ºè¿è´¯çš„å›¾åƒåˆæˆã€‚</li>
<li>å®éªŒè¡¨æ˜ï¼ŒTAMMsåœ¨ç†è§£æ—¶é—´å˜åŒ–å’Œé¢„æµ‹æœªæ¥å›¾åƒçš„ä»»åŠ¡ä¸Šè¶…è¶Šäº†ç°æœ‰çš„å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ã€‚</li>
<li>ç²¾å¿ƒè®¾è®¡çš„æ—¶é—´æ¨ç†å’Œè¯­ä¹‰èåˆå¯¹äºè§£é”å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹åœ¨æ—¶ç©ºç†è§£æ–¹é¢çš„æ½œåŠ›è‡³å…³é‡è¦ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.18862">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-94593ef82be91a87f99e685b30b68d06.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7a04f77638d9d0db4ccdf0df76722f0c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ab375183c976154aefe12c542510f396.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-eb2c74fb629237dea856aeaf19eef3bf.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="LongWriter-Zero-Mastering-Ultra-Long-Text-Generation-via-Reinforcement-Learning"><a href="#LongWriter-Zero-Mastering-Ultra-Long-Text-Generation-via-Reinforcement-Learning" class="headerlink" title="LongWriter-Zero: Mastering Ultra-Long Text Generation via Reinforcement   Learning"></a>LongWriter-Zero: Mastering Ultra-Long Text Generation via Reinforcement   Learning</h2><p><strong>Authors:Yuhao Wu, Yushi Bai, Zhiqiang Hu, Roy Ka-Wei Lee, Juanzi Li</strong></p>
<p>Ultra-long generation by large language models (LLMs) is a widely demanded scenario, yet it remains a significant challenge due to their maximum generation length limit and overall quality degradation as sequence length increases. Previous approaches, exemplified by LongWriter, typically rely on â€˜â€™teachingâ€™â€™, which involves supervised fine-tuning (SFT) on synthetic long-form outputs. However, this strategy heavily depends on synthetic SFT data, which is difficult and costly to construct, often lacks coherence and consistency, and tends to be overly artificial and structurally monotonous. In this work, we propose an incentivization-based approach that, starting entirely from scratch and without relying on any annotated or synthetic data, leverages reinforcement learning (RL) to foster the emergence of ultra-long, high-quality text generation capabilities in LLMs. We perform RL training starting from a base model, similar to R1-Zero, guiding it to engage in reasoning that facilitates planning and refinement during the writing process. To support this, we employ specialized reward models that steer the LLM towards improved length control, writing quality, and structural formatting. Experimental evaluations show that our LongWriter-Zero model, trained from Qwen2.5-32B, consistently outperforms traditional SFT methods on long-form writing tasks, achieving state-of-the-art results across all metrics on WritingBench and Arena-Write, and even surpassing 100B+ models such as DeepSeek R1 and Qwen3-235B. We open-source our data and model checkpoints under <a target="_blank" rel="noopener" href="https://huggingface.co/THU-KEG/LongWriter-Zero-32B">https://huggingface.co/THU-KEG/LongWriter-Zero-32B</a> </p>
<blockquote>
<p>è¶…é•¿æ–‡æœ¬ç”Ÿæˆæ˜¯å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¸­å¹¿æ³›éœ€æ±‚çš„åœºæ™¯ï¼Œä½†ç”±äºå…¶æœ€å¤§ç”Ÿæˆé•¿åº¦é™åˆ¶ä»¥åŠåºåˆ—é•¿åº¦å¢åŠ å¯¼è‡´çš„æ•´ä½“è´¨é‡ä¸‹é™ï¼Œè¿™ä»ç„¶æ˜¯ä¸€ä¸ªå·¨å¤§çš„æŒ‘æˆ˜ã€‚ä»¥å¾€çš„æ–¹æ³•ï¼Œä»¥LongWriterä¸ºä»£è¡¨ï¼Œé€šå¸¸ä¾èµ–äºâ€œæ•™å­¦â€æ–¹å¼ï¼Œæ¶‰åŠåœ¨åˆæˆé•¿ç¯‡è¾“å‡ºä¸Šè¿›è¡Œç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰ã€‚ç„¶è€Œï¼Œè¿™ç§ç­–ç•¥ä¸¥é‡ä¾èµ–äºåˆæˆçš„SFTæ•°æ®ï¼Œè¿™äº›æ•°æ®æ„å»ºå›°éš¾ä¸”æˆæœ¬é«˜æ˜‚ï¼Œå¾€å¾€ç¼ºä¹è¿è´¯æ€§å’Œä¸€è‡´æ€§ï¼Œä¸”å€¾å‘äºè¿‡äºäººå·¥åŒ–å’Œç»“æ„å•è°ƒã€‚</p>
</blockquote>
<p>åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºæ¿€åŠ±çš„æ–¹æ³•ï¼Œè¯¥æ–¹æ³•å®Œå…¨ä»å¤´å¼€å§‹ï¼Œä¸ä¾èµ–ä»»ä½•æ³¨é‡Šæˆ–åˆæˆæ•°æ®ï¼Œåˆ©ç”¨å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰æ¥åŸ¹å…»LLMä¸­äº§ç”Ÿè¶…é•¿ã€é«˜è´¨é‡æ–‡æœ¬ç”Ÿæˆçš„èƒ½åŠ›ã€‚æˆ‘ä»¬ä»åŸºç¡€æ¨¡å‹å¼€å§‹è¿›è¡ŒRLè®­ç»ƒï¼Œç±»ä¼¼äºR1-Zeroï¼ŒæŒ‡å¯¼å®ƒåœ¨è¿›è¡Œå†™ä½œè¿‡ç¨‹ä¸­çš„æ¨ç†ã€è§„åˆ’å’Œç»†åŒ–ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬é‡‡ç”¨ä¸“é—¨çš„å¥–åŠ±æ¨¡å‹æ¥å¼•å¯¼LLMæ”¹è¿›é•¿åº¦æ§åˆ¶ã€å†™ä½œè´¨é‡å’Œç»“æ„æ ¼å¼åŒ–ã€‚</p>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.18841v1">PDF</a> </p>
<p><strong>æ‘˜è¦</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„è¶…é•¿ç”Ÿæˆæ˜¯ä¸€ä¸ªå¹¿æ³›éœ€æ±‚ä½†å…·æœ‰æŒ‘æˆ˜æ€§çš„ä»»åŠ¡ï¼Œå—åˆ°ç”Ÿæˆé•¿åº¦é™åˆ¶å’Œåºåˆ—é•¿åº¦å¢åŠ å¯¼è‡´çš„è´¨é‡ä¸‹é™çš„å½±å“ã€‚ä¹‹å‰çš„æ–¹æ³•å¦‚LongWriterä¸»è¦ä¾èµ–â€œæ•™å­¦â€æ–¹å¼ï¼Œå³åœ¨åˆæˆé•¿å½¢å¼è¾“å‡ºä¸Šè¿›è¡Œç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰ã€‚ç„¶è€Œï¼Œè¿™ç§æ–¹æ³•ä¸¥é‡ä¾èµ–äºåˆæˆSFTæ•°æ®ï¼Œè¿™äº›æ•°æ®æ„å»ºå›°éš¾ã€æˆæœ¬é«˜æ˜‚ï¼Œä¸”å¸¸å¸¸ç¼ºä¹è¿è´¯æ€§å’Œä¸€è‡´æ€§ï¼Œè¿‡äºäººå·¥åŒ–ä¸”ç»“æ„å•è°ƒã€‚æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§åŸºäºæ¿€åŠ±çš„æ–¹æ³•ï¼Œè¯¥æ–¹æ³•å®Œå…¨ä»å¤´å¼€å§‹ï¼Œä¸ä¾èµ–ä»»ä½•æ³¨é‡Šæˆ–åˆæˆæ•°æ®ï¼Œåˆ©ç”¨å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰ä¿ƒè¿›LLMä¸­è¶…é•¿ã€é«˜è´¨é‡æ–‡æœ¬ç”Ÿæˆèƒ½åŠ›çš„å‡ºç°ã€‚æˆ‘ä»¬ä»åŸºç¡€æ¨¡å‹å¼€å§‹è¿›è¡ŒRLè®­ç»ƒï¼Œç±»ä¼¼äºR1-Zeroï¼Œå¼•å¯¼å…¶åœ¨è¿›è¡Œå†™ä½œè¿‡ç¨‹æ—¶è¿›è¡Œæ¨ç†ï¼Œä¿ƒè¿›è§„åˆ’å’Œç»†åŒ–ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬é‡‡ç”¨ä¸“é—¨çš„å¥–åŠ±æ¨¡å‹ï¼Œå¼•å¯¼LLMæ”¹è¿›é•¿åº¦æ§åˆ¶ã€å†™ä½œè´¨é‡å’Œç»“æ„æ ¼å¼åŒ–ã€‚å®éªŒè¯„ä¼°è¡¨æ˜ï¼Œæˆ‘ä»¬çš„LongWriter-Zeroæ¨¡å‹ä»Qwen2.5-32Bè®­ç»ƒè€Œæ¥ï¼Œåœ¨é•¿ç¯‡å†™ä½œä»»åŠ¡ä¸Šå§‹ç»ˆä¼˜äºä¼ ç»ŸSFTæ–¹æ³•ï¼Œåœ¨WritingBenchå’ŒArena-Writeä¸Šçš„å„é¡¹æŒ‡æ ‡å‡è¾¾åˆ°æœ€æ–°æ°´å¹³ï¼Œç”šè‡³è¶…è¶Šäº†DeepSeek R1å’ŒQwen3-235Bç­‰100B+æ¨¡å‹ã€‚æˆ‘ä»¬çš„æ•°æ®å’Œæ¨¡å‹æ£€æŸ¥ç‚¹å·²å…¬å¼€åœ¨<a target="_blank" rel="noopener" href="https://huggingface.co/THU-KEG/LongWriter-Zero-32B">https://huggingface.co/THU-KEG/LongWriter-Zero-32B</a>ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„è¶…é•¿æ–‡æœ¬ç”Ÿæˆæ˜¯ä¸€é¡¹å…·æœ‰æŒ‘æˆ˜æ€§çš„ä»»åŠ¡ï¼Œå› ä¸ºå­˜åœ¨ç”Ÿæˆé•¿åº¦é™åˆ¶å’Œåºåˆ—é•¿åº¦å¢åŠ å¯¼è‡´çš„è´¨é‡ä¸‹é™é—®é¢˜ã€‚</li>
<li>æ­¤å‰çš„æ–¹æ³•å¦‚LongWriterä¸»è¦ä¾èµ–ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰è¿›è¡Œâ€œæ•™å­¦â€ï¼Œä½†è¿™ä¾èµ–äºéš¾ä»¥æ„å»ºä¸”æˆæœ¬é«˜æ˜‚çš„åˆæˆæ•°æ®ã€‚</li>
<li>æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§åŸºäºæ¿€åŠ±çš„æ–¹æ³•ï¼Œåˆ©ç”¨å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰ä¿ƒè¿›LLMçš„è¶…é•¿æ–‡æœ¬ç”Ÿæˆèƒ½åŠ›ï¼Œæ— éœ€ä¾èµ–ä»»ä½•æ³¨é‡Šæˆ–åˆæˆæ•°æ®ã€‚</li>
<li>é‡‡ç”¨äº†ç±»ä¼¼äºR1-Zeroçš„RLè®­ç»ƒæ–¹å¼ï¼Œç»“åˆä¸“é—¨çš„å¥–åŠ±æ¨¡å‹ï¼Œæ”¹è¿›äº†é•¿åº¦æ§åˆ¶ã€å†™ä½œè´¨é‡å’Œç»“æ„æ ¼å¼åŒ–ã€‚</li>
<li>å®éªŒè¡¨æ˜ï¼ŒLongWriter-Zeroæ¨¡å‹åœ¨é•¿ç¯‡å†™ä½œä»»åŠ¡ä¸Šçš„è¡¨ç°ä¼˜äºä¼ ç»ŸSFTæ–¹æ³•ï¼Œè¾¾åˆ°æœ€æ–°æ°´å¹³ã€‚</li>
<li>LongWriter-Zeroæ¨¡å‹å·²è¶…è¶ŠæŸäº›å¤§å‹æ¨¡å‹ï¼Œå¦‚DeepSeek R1å’ŒQwen3-235Bç­‰ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.18841">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-3c2fba3801aba0ddb41c9f8b07a2d3fd.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-77c224b16368e01693b8bad8317d5864.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a9359518a63d016798dd3bb2aa619b2c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-08a96e2d43f91c548aec8d43686be894.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Understanding-Software-Engineering-Agents-A-Study-of-Thought-Action-Result-Trajectories"><a href="#Understanding-Software-Engineering-Agents-A-Study-of-Thought-Action-Result-Trajectories" class="headerlink" title="Understanding Software Engineering Agents: A Study of   Thought-Action-Result Trajectories"></a>Understanding Software Engineering Agents: A Study of   Thought-Action-Result Trajectories</h2><p><strong>Authors:Islem Bouzenia, Michael Pradel</strong></p>
<p>Large Language Model (LLM)-based agents are increasingly employed to automate complex software engineering tasks such as program repair and issue resolution. These agents operate by autonomously generating natural language thoughts, invoking external tools, and iteratively refining their solutions. Despite their widespread adoption, the internal decision-making processes of these agents remain largely unexplored, limiting our understanding of their operational dynamics and failure modes. In this paper, we present a large-scale empirical study of the thought-action-result trajectories of three state-of-the-art LLM-based agents: \textsc{RepairAgent}, \textsc{AutoCodeRover}, and \textsc{OpenHands}. We unify their interaction logs into a common format, capturing 120 trajectories and 2822 LLM interactions focused on program repair and issue resolution. Our study combines quantitative analyses of structural properties, action patterns, and token usage with qualitative assessments of reasoning coherence and feedback integration. We identify key trajectory characteristics such as iteration counts and token consumption, recurring action sequences, and the semantic coherence linking thoughts, actions, and their results. Our findings reveal behavioral motifs and anti-patterns that distinguish successful from failed executions, providing actionable insights for improving agent design, including prompting strategies, failure diagnosis, and anti-pattern detection. We release our dataset and annotation framework to support further research on transparent and robust autonomous software engineering agents. </p>
<blockquote>
<p>åŸºäºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„ä»£ç†æ­£è¶Šæ¥è¶Šå¤šåœ°è¢«ç”¨äºè‡ªåŠ¨åŒ–å¤æ‚çš„è½¯ä»¶å·¥ç¨‹ä»»åŠ¡ï¼Œå¦‚ç¨‹åºä¿®å¤å’Œé—®é¢˜è§£æã€‚è¿™äº›ä»£ç†é€šè¿‡è‡ªä¸»ç”Ÿæˆè‡ªç„¶è¯­è¨€æ€æƒ³ã€è°ƒç”¨å¤–éƒ¨å·¥å…·å¹¶è¿­ä»£ä¼˜åŒ–å…¶è§£å†³æ–¹æ¡ˆæ¥è¿è¡Œã€‚å°½ç®¡è¿™äº›ä»£ç†å·²å¾—åˆ°å¹¿æ³›åº”ç”¨ï¼Œä½†å…¶å†…éƒ¨å†³ç­–è¿‡ç¨‹ä»å¤§å¤šæœªè¢«æ¢ç´¢ï¼Œè¿™é™åˆ¶äº†æˆ‘ä»¬å¯¹å®ƒä»¬è¿è¡ŒåŠ¨æ€å’Œæ•…éšœæ¨¡å¼çš„ç†è§£ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬å¯¹ä¸‰ç§æœ€æ–°LLMä»£ç†çš„æ€æƒ³è¡ŒåŠ¨ç»“æœè½¨è¿¹è¿›è¡Œäº†å¤§è§„æ¨¡å®è¯ç ”ç©¶ï¼šRepairAgentã€AutoCodeRoverå’ŒOpenHandsã€‚æˆ‘ä»¬å°†å®ƒä»¬çš„äº¤äº’æ—¥å¿—ç»Ÿä¸€ä¸ºé€šç”¨æ ¼å¼ï¼Œæ•è·äº†120æ¡è½¨è¿¹å’Œ2822æ¬¡ä»¥ç¨‹åºä¿®å¤å’Œé—®é¢˜è§£æä¸ºé‡ç‚¹çš„LLMäº¤äº’ã€‚æˆ‘ä»¬çš„ç ”ç©¶ç»“åˆäº†ç»“æ„æ€§å±æ€§ã€è¡ŒåŠ¨æ¨¡å¼å’Œä»¤ç‰Œä½¿ç”¨é‡çš„å®šé‡åˆ†æï¼Œä»¥åŠä¸æ¨ç†è¿è´¯æ€§å’Œåé¦ˆæ•´åˆçš„å®šæ€§è¯„ä¼°ã€‚æˆ‘ä»¬ç¡®å®šäº†å…³é”®è½¨è¿¹ç‰¹å¾ï¼Œå¦‚è¿­ä»£æ¬¡æ•°å’Œä»¤ç‰Œæ¶ˆè€—ã€é‡å¤çš„è¡ŒåŠ¨åºåˆ—ï¼Œä»¥åŠé“¾æ¥æ€æƒ³ã€è¡ŒåŠ¨å’Œç»“æœä¹‹é—´çš„è¯­ä¹‰è¿è´¯æ€§ã€‚æˆ‘ä»¬çš„ç ”ç©¶ç»“æœæ­ç¤ºäº†åŒºåˆ†æˆåŠŸæ‰§è¡Œå’Œå¤±è´¥æ‰§è¡Œçš„è¡Œä¸ºæ¨¡å¼å’Œåæ¨¡å¼ï¼Œä¸ºæ”¹è¿›ä»£ç†è®¾è®¡æä¾›äº†å¯æ“ä½œçš„è§è§£ï¼ŒåŒ…æ‹¬æç¤ºç­–ç•¥ã€æ•…éšœè¯Šæ–­å’Œåæ¨¡å¼æ£€æµ‹ã€‚æˆ‘ä»¬å‘å¸ƒæˆ‘ä»¬çš„æ•°æ®é›†å’Œæ³¨é‡Šæ¡†æ¶ï¼Œä»¥æ”¯æŒå¯¹é€æ˜å’Œç¨³å¥çš„è‡ªä¸»è½¯ä»¶å·¥ç¨‹ä»£ç†çš„è¿›ä¸€æ­¥ç ”ç©¶ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.18824v1">PDF</a> </p>
<p><strong>Summary</strong>ï¼š<br>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰é©±åŠ¨çš„ä»£ç†è¢«å¹¿æ³›åº”ç”¨äºè‡ªåŠ¨åŒ–å¤æ‚çš„è½¯ä»¶å·¥ç¨‹ä»»åŠ¡ï¼Œå¦‚ç¨‹åºä¿®å¤å’Œé—®é¢˜è§£æã€‚æœ¬æ–‡è¿›è¡Œäº†å¤§è§„æ¨¡å®è¯ç ”ç©¶ï¼Œå¯¹ä¸‰ç§æœ€æ–°LLMä»£ç†ï¼ˆRepairAgentã€AutoCodeRoverå’ŒOpenHandsï¼‰çš„æ€æƒ³è¡ŒåŠ¨ç»“æœè½¨è¿¹è¿›è¡Œç ”ç©¶ã€‚é€šè¿‡å¯¹ç»“æ„æ€§è´¨ã€è¡ŒåŠ¨æ¨¡å¼ã€ä»¤ç‰Œä½¿ç”¨é‡çš„å®šé‡åˆ†æï¼Œä»¥åŠæ¨ç†è¿è´¯æ€§å’Œåé¦ˆæ•´åˆçš„å®šæ€§è¯„ä¼°ï¼Œç ”ç©¶æ­ç¤ºäº†å…³é”®è½¨è¿¹ç‰¹å¾ï¼ŒåŒ…æ‹¬è¿­ä»£æ¬¡æ•°å’Œä»¤ç‰Œæ¶ˆè€—ã€é‡å¤çš„è¡ŒåŠ¨åºåˆ—ä»¥åŠæ€æƒ³å’Œè¡ŒåŠ¨ç»“æœçš„è¯­ä¹‰è¿è´¯æ€§ã€‚è¿™äº›å‘ç°æ­ç¤ºäº†æˆåŠŸä¸å¤±è´¥æ‰§è¡Œçš„åŒºåˆ«ï¼Œä¸ºæ”¹è¿›ä»£ç†è®¾è®¡æä¾›äº†è¡ŒåŠ¨æŒ‡å¯¼ï¼ŒåŒ…æ‹¬æç¤ºç­–ç•¥ã€æ•…éšœè¯Šæ–­å’Œåæ¨¡å¼æ£€æµ‹ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>LLMä»£ç†è¢«å¹¿æ³›åº”ç”¨äºè‡ªåŠ¨åŒ–å¤æ‚çš„è½¯ä»¶å·¥ç¨‹ä»»åŠ¡ã€‚</li>
<li>ä¸‰ç§LLMä»£ç†ï¼ˆRepairAgentã€AutoCodeRoverå’ŒOpenHandsï¼‰çš„æ€æƒ³è¡ŒåŠ¨ç»“æœè½¨è¿¹è¿›è¡Œäº†ç ”ç©¶ã€‚</li>
<li>ç ”ç©¶ç»“åˆäº†å®šé‡å’Œå®šæ€§çš„åˆ†ææ–¹æ³•ï¼ŒåŒ…æ‹¬ç»“æ„æ€§è´¨ã€è¡ŒåŠ¨æ¨¡å¼å’Œä»¤ç‰Œä½¿ç”¨é‡çš„åˆ†æä»¥åŠæ¨ç†è¿è´¯æ€§å’Œåé¦ˆæ•´åˆçš„è¯„ä¼°ã€‚</li>
<li>ç ”ç©¶æ­ç¤ºäº†å…³é”®è½¨è¿¹ç‰¹å¾ï¼ŒåŒ…æ‹¬è¿­ä»£æ¬¡æ•°ã€ä»¤ç‰Œæ¶ˆè€—å’Œè¡ŒåŠ¨åºåˆ—ã€‚</li>
<li>å‘ç°äº†æˆåŠŸä¸å¤±è´¥æ‰§è¡Œçš„åŒºåˆ«ï¼ŒåŒ…æ‹¬è¡Œä¸ºæ¨¡å¼å’Œåæ¨¡å¼ã€‚</li>
<li>ç ”ç©¶ç»“æœæä¾›äº†æ”¹è¿›ä»£ç†è®¾è®¡çš„è¡ŒåŠ¨æŒ‡å¯¼ï¼ŒåŒ…æ‹¬æç¤ºç­–ç•¥ã€æ•…éšœè¯Šæ–­å’Œåæ¨¡å¼æ£€æµ‹ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.18824">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-e028b99219c1c3ac6f48ab5ae5fa23eb.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-223cf5c87ddbd9b3cd80cf2ceaa5a2c6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-769f0321f2616baa0da60122f6ffaad2.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="RWESummary-A-Framework-and-Test-for-Choosing-Large-Language-Models-to-Summarize-Real-World-Evidence-RWE-Studies"><a href="#RWESummary-A-Framework-and-Test-for-Choosing-Large-Language-Models-to-Summarize-Real-World-Evidence-RWE-Studies" class="headerlink" title="RWESummary: A Framework and Test for Choosing Large Language Models to   Summarize Real-World Evidence (RWE) Studies"></a>RWESummary: A Framework and Test for Choosing Large Language Models to   Summarize Real-World Evidence (RWE) Studies</h2><p><strong>Authors:Arjun Mukerji, Michael L. Jackson, Jason Jones, Neil Sanghavi</strong></p>
<p>Large Language Models (LLMs) have been extensively evaluated for general summarization tasks as well as medical research assistance, but they have not been specifically evaluated for the task of summarizing real-world evidence (RWE) from structured output of RWE studies. We introduce RWESummary, a proposed addition to the MedHELM framework (Bedi, Cui, Fuentes, Unell et al., 2025) to enable benchmarking of LLMs for this task. RWESummary includes one scenario and three evaluations covering major types of errors observed in summarization of medical research studies and was developed using Atropos Health proprietary data. Additionally, we use RWESummary to compare the performance of different LLMs in our internal RWE summarization tool. At the time of publication, with 13 distinct RWE studies, we found the Gemini 2.5 models performed best overall (both Flash and Pro). We suggest RWESummary as a novel and useful foundation model benchmark for real-world evidence study summarization. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å·²ç»å¹¿æ³›åº”ç”¨äºä¸€èˆ¬çš„æ‘˜è¦ä»»åŠ¡ä»¥åŠåŒ»å­¦ç ”ç©¶è¾…åŠ©å·¥ä½œï¼Œä½†å®ƒä»¬å°šæœªé’ˆå¯¹ä»ç»“æ„åŒ–è¾“å‡ºä¸­å¯¹ç°å®ä¸–ç•Œçš„è¯æ®ï¼ˆRWEï¼‰è¿›è¡Œæ‘˜è¦çš„ä»»åŠ¡è¿›è¡Œä¸“é—¨è¯„ä¼°ã€‚æˆ‘ä»¬å¼•å…¥äº†RWESummaryï¼Œè¿™æ˜¯MedHELMæ¡†æ¶ï¼ˆBediã€Cuiã€Fuentesã€Unellç­‰äººï¼Œ2025å¹´ï¼‰çš„ä¸€ä¸ªæ‹Ÿè®®è¡¥å……ï¼Œä»¥å®ç°å¯¹è¿™é¡¹ä»»åŠ¡çš„LLMåŸºå‡†æµ‹è¯•ã€‚RWESummaryåŒ…æ‹¬ä¸€ä¸ªåœºæ™¯å’Œä¸‰ç§è¯„ä¼°ï¼Œæ¶µç›–äº†åŒ»å­¦ç ”ç©¶ä¸­æ‘˜è¦ä¸»è¦ç±»å‹çš„é”™è¯¯ï¼Œå¹¶ä½¿ç”¨äº†Atropos Healthä¸“æœ‰æ•°æ®å¼€å‘ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬ä½¿ç”¨RWESummaryæ¥æ¯”è¾ƒå†…éƒ¨RWEæ‘˜è¦å·¥å…·ä¸­ä¸åŒLLMçš„æ€§èƒ½ã€‚åœ¨å‘å¸ƒæ—¶ï¼Œæˆ‘ä»¬åˆ†æäº†1beçš„å·®å¼‚ç°å®ä¸–ç•Œç ”ç©¶æ•°æ®æˆ‘ä»¬å‘ç°Gemini 2.5æ¨¡å‹è¡¨ç°æœ€å¥½ï¼ˆFlashå’ŒProä¸¤è€…éƒ½æ˜¯ï¼‰ã€‚æˆ‘ä»¬æè®®å°†RWESummaryä½œä¸ºæ‘˜è¦ç°å®è¯æ®ç ”ç©¶çš„æ–°å‹ä¸”æœ‰ç”¨çš„åŸºå‡†æ¨¡å‹ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.18819v1">PDF</a> 24 pages, 2 figures</p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨é€šç”¨æ‘˜è¦ä»»åŠ¡å’ŒåŒ»ç–—ç ”ç©¶è¾…åŠ©æ–¹é¢å·²æœ‰å¹¿æ³›è¯„ä¼°ï¼Œä½†æœªé’ˆå¯¹ä»ç»“æ„åŒ–è¾“å‡ºå¯¹çœŸå®ä¸–ç•Œè¯æ®ï¼ˆRWEï¼‰çš„æ‘˜è¦ä»»åŠ¡è¿›è¡Œä¸“é—¨è¯„ä¼°ã€‚æœ¬æ–‡å¼•å…¥RWESummaryï¼Œä½œä¸ºMedHELMæ¡†æ¶çš„è¡¥å……ï¼Œä»¥å®ç°å¯¹è¯¥ä»»åŠ¡çš„LLMåŸºå‡†æµ‹è¯•ã€‚RWESummaryåŒ…å«ä¸€ç§æƒ…æ™¯å’Œä¸‰ç§è¯„ä¼°ï¼Œæ¶µç›–åœ¨åŒ»ç–—ç ”ç©¶æ‘˜è¦ä¸­è§‚å¯Ÿåˆ°çš„ä¸»è¦ç±»å‹é”™è¯¯ï¼Œå¹¶åˆ©ç”¨Atropos Healthä¸“æœ‰æ•°æ®å¼€å‘ã€‚æ­¤å¤–ï¼Œä½œè€…ä½¿ç”¨RWESummaryæ¯”è¾ƒäº†å†…éƒ¨RWEæ‘˜è¦å·¥å…·ä¸­ä¸åŒLLMçš„æ€§èƒ½ã€‚æˆªè‡³å‡ºç‰ˆæ—¶ï¼ŒåŸºäº13é¡¹ä¸åŒçš„RWEç ”ç©¶ï¼Œæˆ‘ä»¬å‘ç°Gemini 2.5æ¨¡å‹ï¼ˆFlashå’ŒProï¼‰æ€»ä½“è¡¨ç°æœ€ä½³ã€‚æˆ‘ä»¬å»ºè®®RWESummaryä½œä¸ºçœŸå®ä¸–ç•Œè¯æ®ç ”ç©¶æ‘˜è¦çš„æ–°å‹æœ‰ç”¨åŸºå‡†æ¨¡å‹ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨çœŸå®ä¸–ç•Œè¯æ®ï¼ˆRWEï¼‰æ‘˜è¦ä»»åŠ¡ä¸Šçš„è¯„ä¼°å°šå±ç©ºç™½ã€‚</li>
<li>æœ¬æ–‡å¼•å…¥RWESummaryï¼Œä½œä¸ºMedHELMæ¡†æ¶çš„è¡¥å……ï¼Œä»¥å®ç°å¯¹RWEæ‘˜è¦ä»»åŠ¡çš„LLMåŸºå‡†æµ‹è¯•ã€‚</li>
<li>RWESummaryæ¶µç›–äº†ä¸€ç§æƒ…æ™¯å’Œä¸‰ç§è¯„ä¼°ï¼Œæ—¨åœ¨æ•æ‰åœ¨åŒ»ç–—ç ”ç©¶æ‘˜è¦ä¸­å¸¸è§çš„ä¸»è¦é”™è¯¯ç±»å‹ã€‚</li>
<li>Atropos Healthçš„ä¸“æœ‰æ•°æ®è¢«ç”¨äºå¼€å‘RWESummaryã€‚</li>
<li>ä¸åŒLLMåœ¨å†…éƒ¨RWEæ‘˜è¦å·¥å…·ä¸­çš„æ€§èƒ½è¡¨ç°å­˜åœ¨å·®å¼‚ã€‚</li>
<li>åœ¨13é¡¹ä¸åŒçš„RWEç ”ç©¶ä¸­ï¼ŒGemini 2.5æ¨¡å‹ï¼ˆFlashå’ŒProï¼‰æ€»ä½“è¡¨ç°æœ€ä½³ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.18819">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-4299eb03b1c37417ed2abe0ff8b5a8c5.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c6b36dc1f2bc97703ec4843870df1953.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a711255927c5883d7722905cee01a070.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="Context-Aware-CodeLLM-Eviction-for-AI-assisted-Coding"><a href="#Context-Aware-CodeLLM-Eviction-for-AI-assisted-Coding" class="headerlink" title="Context-Aware CodeLLM Eviction for AI-assisted Coding"></a>Context-Aware CodeLLM Eviction for AI-assisted Coding</h2><p><strong>Authors:Kishanthan Thangarajah, Boyuan Chen, Shi Chang, Ahmed E. Hassan</strong></p>
<p>AI-assisted coding tools powered by Code Large Language Models (CodeLLMs) are increasingly integrated into modern software development workflows. To address concerns around privacy, latency, and model customization, many enterprises opt to self-host these models. However, the diversity and growing number of CodeLLMs, coupled with limited accelerator memory, introduce practical challenges in model management and serving efficiency. This paper presents CACE, a novel context-aware model eviction strategy designed specifically to optimize self-hosted CodeLLM serving under resource constraints. Unlike traditional eviction strategies based solely on recency (e.g., Least Recently Used), CACE leverages multiple context-aware factors, including model load time, task-specific latency sensitivity, expected output length, and recent usage and future demand tracked through a sliding window. We evaluate CACE using realistic workloads that include both latency-sensitive code completion and throughput-intensive code reasoning tasks. Our experiments show that CACE reduces Time-to-First-Token (TTFT) and end-to-end (E2E) latency, while significantly lowering the number of model evictions compared to state-of-the-art systems. Ablation studies further demonstrate the importance of multi-factor eviction in balancing responsiveness and resource efficiency. This work contributes practical strategies for deploying scalable, low-latency AI coding assistants in real-world software engineering environments. </p>
<blockquote>
<p>ç”±å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆCodeLLMï¼‰é©±åŠ¨çš„AIè¾…åŠ©ç¼–ç å·¥å…·è¶Šæ¥è¶Šå¤šåœ°è¢«é›†æˆåˆ°ç°ä»£è½¯ä»¶å¼€å‘å·¥ä½œæµç¨‹ä¸­ã€‚ä¸ºäº†è§£å†³å…³äºéšç§ã€å»¶è¿Ÿå’Œæ¨¡å‹å®šåˆ¶çš„æ‹…å¿§ï¼Œè®¸å¤šä¼ä¸šé€‰æ‹©è‡ªä¸»æ‰˜ç®¡è¿™äº›æ¨¡å‹ã€‚ç„¶è€Œï¼ŒCodeLLMçš„å¤šæ ·æ€§å’Œæ•°é‡ä¸æ–­å¢é•¿ï¼ŒåŠ ä¸ŠåŠ é€Ÿå™¨å†…å­˜æœ‰é™ï¼Œç»™æ¨¡å‹ç®¡ç†å’ŒæœåŠ¡æ•ˆç‡å¸¦æ¥äº†å®é™…æŒ‘æˆ˜ã€‚æœ¬æ–‡é’ˆå¯¹èµ„æºå—é™æƒ…å†µä¸‹è‡ªä¸»æ‰˜ç®¡çš„CodeLLMæœåŠ¡ä¼˜åŒ–é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§æ–°å‹ä¸Šä¸‹æ–‡æ„ŸçŸ¥æ¨¡å‹é©±é€ç­–ç•¥CACEã€‚ä¸ä¼ ç»Ÿçš„ä»…åŸºäºæ—¶æ•ˆæ€§çš„é©±é€ç­–ç•¥ï¼ˆå¦‚æœ€è¿‘æœ€å°‘ä½¿ç”¨ï¼‰ä¸åŒï¼ŒCACEåˆ©ç”¨å¤šä¸ªä¸Šä¸‹æ–‡æ„ŸçŸ¥å› ç´ ï¼ŒåŒ…æ‹¬æ¨¡å‹åŠ è½½æ—¶é—´ã€é’ˆå¯¹ä»»åŠ¡çš„å»¶è¿Ÿæ•æ„Ÿæ€§ã€é¢„æœŸè¾“å‡ºé•¿åº¦ä»¥åŠé€šè¿‡æ»‘åŠ¨çª—å£è·Ÿè¸ªçš„è¿‘æœŸä½¿ç”¨æƒ…å†µå’Œæœªæ¥éœ€æ±‚ã€‚æˆ‘ä»¬ä½¿ç”¨åŒ…å«å»¶è¿Ÿæ•æ„Ÿçš„ä»£ç è¡¥å…¨å’Œååé‡å¯†é›†çš„ä»£ç æ¨ç†ä»»åŠ¡çš„å®é™…å·¥ä½œè´Ÿè½½å¯¹CACEè¿›è¡Œäº†è¯„ä¼°ã€‚å®éªŒè¡¨æ˜ï¼ŒCACEå‡å°‘äº†é¦–æ¬¡ä»¤ç‰Œæ—¶é—´ï¼ˆTTFTï¼‰å’Œç«¯åˆ°ç«¯ï¼ˆE2Eï¼‰å»¶è¿Ÿï¼ŒåŒæ—¶ä¸æœ€å…ˆè¿›çš„ç³»ç»Ÿç›¸æ¯”ï¼Œå¤§å¤§é™ä½äº†æ¨¡å‹è¢«é©±é€çš„æ¬¡æ•°ã€‚æ¶ˆèç ”ç©¶è¿›ä¸€æ­¥è¯æ˜äº†å¤šå› ç´ é©±é€åœ¨å¹³è¡¡å“åº”æ€§å’Œèµ„æºæ•ˆç‡ä¸­çš„é‡è¦æ€§ã€‚è¿™é¡¹å·¥ä½œæå‡ºäº†åœ¨ç°å®ä¸–ç•Œè½¯ä»¶å·¥ç¨‹ç¯å¢ƒä¸­éƒ¨ç½²å¯æ‰©å±•ã€ä½å»¶è¿Ÿçš„AIç¼–ç åŠ©ç†çš„å®é™…ç­–ç•¥ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.18796v1">PDF</a> 12 pages, 6 figures</p>
<p><strong>æ‘˜è¦</strong></p>
<p>åŸºäºCode Large Language Modelsï¼ˆCodeLLMï¼‰çš„AIè¾…åŠ©ç¼–ç å·¥å…·æ­£æ—¥ç›Šèå…¥ç°ä»£è½¯ä»¶å¼€å‘æµç¨‹ã€‚ä¸ºäº†åº”å¯¹éšç§ã€å»¶è¿Ÿå’Œæ¨¡å‹å®šåˆ¶ç­‰å…³æ³¨ç‚¹ï¼Œè®¸å¤šä¼ä¸šé€‰æ‹©è‡ªä¸»æ‰˜ç®¡è¿™äº›æ¨¡å‹ã€‚ç„¶è€Œï¼ŒCodeLLMçš„å¤šæ ·æ€§å’Œæ•°é‡å¢é•¿ï¼Œä»¥åŠåŠ é€Ÿå™¨å†…å­˜çš„é™åˆ¶ï¼Œç»™æ¨¡å‹ç®¡ç†å’ŒæœåŠ¡æ•ˆç‡å¸¦æ¥äº†å®é™…æŒ‘æˆ˜ã€‚æœ¬æ–‡æå‡ºäº†CACEï¼Œä¸€ç§æ–°å‹çš„ä¸Šä¸‹æ–‡æ„ŸçŸ¥æ¨¡å‹é€å‡ºç­–ç•¥ï¼Œæ—¨åœ¨ä¼˜åŒ–èµ„æºå—é™ä¸‹çš„è‡ªä¸»æ‰˜ç®¡CodeLLMæœåŠ¡ã€‚ä¸ä¼ ç»Ÿçš„ä»…åŸºäºæœ€è¿‘ä½¿ç”¨æƒ…å†µçš„é€å‡ºç­–ç•¥ï¼ˆå¦‚æœ€è¿‘æœ€å°‘ä½¿ç”¨ï¼‰ä¸åŒï¼ŒCACEåˆ©ç”¨å¤šä¸ªä¸Šä¸‹æ–‡æ„ŸçŸ¥å› ç´ ï¼ŒåŒ…æ‹¬æ¨¡å‹åŠ è½½æ—¶é—´ã€ä»»åŠ¡ç‰¹å®šçš„å»¶è¿Ÿæ•æ„Ÿæ€§ã€é¢„æœŸè¾“å‡ºé•¿åº¦ä»¥åŠé€šè¿‡æ»‘åŠ¨çª—å£è¿½è¸ªçš„æœ€è¿‘ä½¿ç”¨æƒ…å†µå’Œæœªæ¥éœ€æ±‚ã€‚æˆ‘ä»¬ä½¿ç”¨åŒ…æ‹¬å»¶è¿Ÿæ•æ„Ÿçš„ä»£ç è¡¥å…¨å’Œååé‡å¯†é›†çš„ä»£ç æ¨ç†ä»»åŠ¡çš„å®é™…å·¥ä½œè´Ÿè½½æ¥è¯„ä¼°CACEã€‚å®éªŒè¡¨æ˜ï¼ŒCACEå‡å°‘äº†æ—¶é—´è‡³ç¬¬ä¸€ä»¤ç‰Œï¼ˆTTFTï¼‰å’Œç«¯åˆ°ç«¯ï¼ˆE2Eï¼‰å»¶è¿Ÿï¼Œå¹¶ä¸”ä¸æœ€å…ˆè¿›çš„ç³»ç»Ÿç›¸æ¯”ï¼Œæ˜¾è‘—é™ä½äº†æ¨¡å‹é€å‡ºçš„æ¬¡æ•°ã€‚æ¶ˆèç ”ç©¶è¿›ä¸€æ­¥è¯æ˜äº†å¤šå› ç´ é€å‡ºåœ¨å¹³è¡¡å“åº”æ€§å’Œèµ„æºæ•ˆç‡ä¸­çš„é‡è¦æ€§ã€‚æœ¬ç ”ç©¶ä¸ºåœ¨ç°å®ä¸–ç•Œçš„è½¯ä»¶å·¥ç¨‹ç¯å¢ƒä¸­éƒ¨ç½²å¯æ‰©å±•ã€ä½å»¶è¿Ÿçš„AIç¼–ç åŠ©æ‰‹æä¾›äº†å®ç”¨ç­–ç•¥ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>AIè¾…åŠ©ç¼–ç å·¥å…·é›†æˆåˆ°ç°ä»£è½¯ä»¶å¼€å‘æµç¨‹ä¸­ã€‚</li>
<li>ä¼ä¸šä¸ºéšç§ã€å»¶è¿Ÿå’Œæ¨¡å‹å®šåˆ¶é€‰æ‹©è‡ªä¸»æ‰˜ç®¡CodeLLMã€‚</li>
<li>CodeLLMçš„å¤šæ ·æ€§å’Œæ•°é‡å¢é•¿ä»¥åŠåŠ é€Ÿå™¨å†…å­˜é™åˆ¶å¸¦æ¥æ¨¡å‹ç®¡ç†å’ŒæœåŠ¡æ•ˆç‡æŒ‘æˆ˜ã€‚</li>
<li>æå‡ºäº†CACEä¸Šä¸‹æ–‡æ„ŸçŸ¥æ¨¡å‹é€å‡ºç­–ç•¥ï¼Œä¼˜åŒ–èµ„æºå—é™ä¸‹çš„CodeLLMæœåŠ¡ã€‚</li>
<li>CACEåˆ©ç”¨ä¸Šä¸‹æ–‡æ„ŸçŸ¥å› ç´ åŒ…æ‹¬æ¨¡å‹åŠ è½½æ—¶é—´ã€ä»»åŠ¡å»¶è¿Ÿæ•æ„Ÿæ€§ç­‰ã€‚</li>
<li>CACEåœ¨å»¶è¿Ÿæ•æ„Ÿçš„ä»£ç è¡¥å…¨å’Œååé‡å¯†é›†çš„ä»£ç æ¨ç†ä»»åŠ¡çš„å®é™…å·¥ä½œè´Ÿè½½è¯„ä¼°ä¸­è¡¨ç°ä¼˜è¶Šã€‚</li>
<li>CACEå‡å°‘äº†æ—¶é—´è‡³ç¬¬ä¸€ä»¤ç‰Œå’Œç«¯åˆ°ç«¯å»¶è¿Ÿï¼Œå¹¶æ˜¾è‘—é™ä½äº†æ¨¡å‹é€å‡ºçš„æ¬¡æ•°ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.18796">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-299c438c168e1b80e1a35329a5e2643e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-cebb565559d9f75bb8b91d0035dbd1b6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8a9cabab3a1e581fcab015d4cd62f455.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3ace927b4eef3a1fc981f30ce6f89d22.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b94f4282e040d939b30e1e58905f0673.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="Existing-LLMs-Are-Not-Self-Consistent-For-Simple-Tasks"><a href="#Existing-LLMs-Are-Not-Self-Consistent-For-Simple-Tasks" class="headerlink" title="Existing LLMs Are Not Self-Consistent For Simple Tasks"></a>Existing LLMs Are Not Self-Consistent For Simple Tasks</h2><p><strong>Authors:Zhenru Lin, Jiawen Tao, Yang Yuan, Andrew Chi-Chih Yao</strong></p>
<p>Large Language Models (LLMs) have grown increasingly powerful, yet ensuring their decisions remain transparent and trustworthy requires self-consistency â€“ no contradictions in their internal reasoning. Our study reveals that even on simple tasks, such as comparing points on a line or a plane, or reasoning in a family tree, all smaller models are highly inconsistent, and even state-of-the-art models like DeepSeek-R1 and GPT-o4-mini are not fully self-consistent. To quantify and mitigate these inconsistencies, we introduce inconsistency metrics and propose two automated methods â€“ a graph-based and an energy-based approach. While these fixes provide partial improvements, they also highlight the complexity and importance of self-consistency in building more reliable and interpretable AI. The code and data are available at <a target="_blank" rel="noopener" href="https://github.com/scorpio-nova/llm-self-consistency">https://github.com/scorpio-nova/llm-self-consistency</a>. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„èƒ½åŠ›æ—¥ç›Šå¢å¼ºï¼Œä½†ç¡®ä¿å®ƒä»¬çš„å†³ç­–ä¿æŒé€æ˜å’Œå¯ä¿¡éœ€è¦ä¸€è‡´æ€§â€”â€”åœ¨å…¶å†…éƒ¨æ¨ç†ä¸­æ²¡æœ‰çŸ›ç›¾ã€‚æˆ‘ä»¬çš„ç ”ç©¶è¡¨æ˜ï¼Œå³ä½¿åœ¨ç®€å•çš„ä»»åŠ¡ï¼Œå¦‚åœ¨çº¿æˆ–å¹³é¢ä¸Šçš„ç‚¹æ¯”è¾ƒï¼Œæˆ–åœ¨å®¶æ—æ ‘ä¸­çš„æ¨ç†ï¼Œæ‰€æœ‰è¾ƒå°æ¨¡å‹éƒ½å­˜åœ¨é«˜åº¦ä¸ä¸€è‡´æ€§ï¼Œç”šè‡³æœ€å…ˆè¿›çš„æ¨¡å‹å¦‚DeepSeek-R1å’ŒGPT-o4-miniä¹Ÿä¸å®Œå…¨å…·æœ‰è‡ªèº«ä¸€è‡´æ€§ã€‚ä¸ºäº†é‡åŒ–å’Œå‡è½»è¿™äº›ä¸ä¸€è‡´æ€§ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸ä¸€è‡´æ€§åº¦é‡æ ‡å‡†ï¼Œå¹¶æå‡ºäº†ä¸¤ç§è‡ªåŠ¨åŒ–æ–¹æ³•â€”â€”åŸºäºå›¾å’ŒåŸºäºèƒ½é‡çš„æ–¹æ³•ã€‚è™½ç„¶è¿™äº›ä¿®å¤æä¾›äº†éƒ¨åˆ†æ”¹è¿›ï¼Œä½†ä¹Ÿçªå‡ºäº†åœ¨å»ºè®¾æ›´å¯é å’Œå¯è§£é‡Šçš„AIè¿‡ç¨‹ä¸­è‡ªæˆ‘ä¸€è‡´æ€§å¤æ‚æ€§å’Œé‡è¦æ€§ã€‚ç›¸å…³ä»£ç å’Œæ•°æ®å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/scorpio-nova/llm-self-consistency%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/scorpio-nova/llm-self-consistencyè·å–ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.18781v1">PDF</a> 10 pages, 6 figures</p>
<p><strong>æ€»ç»“</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„å¼ºå¤§æ€§ä¸æ–­æå‡ï¼Œä½†åœ¨ç¡®ä¿å®ƒä»¬å†³ç­–çš„é€æ˜åº¦å’Œå¯ä¿¡åº¦æ–¹é¢ï¼Œéœ€è¦å®ƒä»¬å†…éƒ¨æ¨ç†çš„ä¸€è‡´æ€§ï¼Œå³è‡ªæˆ‘ä¸€è‡´æ€§ã€‚æœ¬ç ”ç©¶å‘ç°ï¼Œå³ä½¿åœ¨ç®€å•çš„ä»»åŠ¡ä¸­ï¼Œå¦‚åœ¨çº¿æˆ–å¹³é¢ä¸Šçš„ç‚¹æ¯”è¾ƒï¼Œæˆ–åœ¨å®¶æ—æ ‘ä¸­è¿›è¡Œæ¨ç†ï¼Œæ‰€æœ‰å°å‹æ¨¡å‹éƒ½å­˜åœ¨é«˜åº¦ä¸ä¸€è‡´æ€§ï¼Œç”šè‡³æœ€å…ˆè¿›çš„æ¨¡å‹å¦‚DeepSeek-R1å’ŒGPT-o4-miniä¹Ÿæ— æ³•å®Œå…¨å®ç°è‡ªæˆ‘ä¸€è‡´æ€§ã€‚ä¸ºäº†é‡åŒ–å’Œç¼“è§£è¿™äº›ä¸ä¸€è‡´æ€§ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸ä¸€è‡´æ€§æŒ‡æ ‡ï¼Œå¹¶æå‡ºäº†ä¸¤ç§è‡ªåŠ¨åŒ–æ–¹æ³•â€”â€”åŸºäºå›¾å’ŒåŸºäºèƒ½é‡çš„æ–¹æ³•ã€‚è™½ç„¶è¿™äº›ä¿®å¤æä¾›äº†éƒ¨åˆ†æ”¹è¿›ï¼Œä½†ä¹Ÿçªæ˜¾äº†è‡ªæˆ‘ä¸€è‡´æ€§åœ¨æ„å»ºæ›´å¯é å’Œå¯è§£é‡Šçš„AIä¸­çš„é‡è¦æ€§åŠå¤æ‚æ€§ã€‚ç›¸å…³ä»£ç å’Œæ•°æ®å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/scorpio-nova/llm-self-consistency%E4%B8%8A%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/scorpio-nova/llm-self-consistencyä¸Šæ‰¾åˆ°ã€‚</a></p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨ç®€å•ä»»åŠ¡ä¸­ä¹Ÿå­˜åœ¨è‡ªæˆ‘ä¸€è‡´æ€§æ–¹é¢çš„é—®é¢˜ã€‚</li>
<li>æœ€å…ˆè¿›çš„æ¨¡å‹å¦‚DeepSeek-R1å’ŒGPT-o4-miniå¹¶ä¸èƒ½å®Œå…¨å®ç°è‡ªæˆ‘ä¸€è‡´æ€§ã€‚</li>
<li>ä¸ºè§£å†³è¿™äº›ä¸ä¸€è‡´æ€§ï¼Œç ”ç©¶å¼•å…¥äº†ä¸ä¸€è‡´æ€§æŒ‡æ ‡ã€‚</li>
<li>æå‡ºä¸¤ç§è‡ªåŠ¨åŒ–æ–¹æ³•ï¼šåŸºäºå›¾çš„æ–¹æ³•å’ŒåŸºäºèƒ½é‡çš„æ–¹æ³•ã€‚</li>
<li>è¿™äº›æ–¹æ³•è™½èƒ½éƒ¨åˆ†æ”¹å–„ä¸ä¸€è‡´æ€§é—®é¢˜ï¼Œä½†è‡ªæˆ‘ä¸€è‡´æ€§ä»æ˜¯æ„å»ºæ›´å¯é å’Œå¯è§£é‡Šçš„AIçš„å…³é”®æŒ‘æˆ˜ã€‚</li>
<li>è‡ªæˆ‘ä¸€è‡´æ€§çš„é‡è¦æ€§åœ¨äºå®ƒèƒ½æé«˜AIçš„é€æ˜åº¦å’Œå¯ä¿¡åº¦ã€‚</li>
<li>ç›¸å…³ä»£ç å’Œæ•°æ®å·²åœ¨æŒ‡å®šå¹³å°ä¸Šå…¬å¼€ï¼Œä¾›è¿›ä¸€æ­¥ç ”ç©¶ä½¿ç”¨ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.18781">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-209f32125c2a3719d05a46dc4a5fb900.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2e6ea1c837a06d14378eeb4885266cdd.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-0e7da8d184dfb98be0f325dd712f1cee.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ffad4b5502c0fe915c73b1151ddaadfa.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-abaf4f46050e0b8264d0f1b99aff06de.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="Benchmarking-the-Pedagogical-Knowledge-of-Large-Language-Models"><a href="#Benchmarking-the-Pedagogical-Knowledge-of-Large-Language-Models" class="headerlink" title="Benchmarking the Pedagogical Knowledge of Large Language Models"></a>Benchmarking the Pedagogical Knowledge of Large Language Models</h2><p><strong>Authors:Maxime LeliÃ¨vre, Amy Waldock, Meng Liu, Natalia ValdÃ©s Aspillaga, Alasdair Mackintosh, MarÃ­a JosÃ© Ogando Portelo, Jared Lee, Paul Atherton, Robin A. A. Ince, Oliver G. B. Garrod</strong></p>
<p>Benchmarks like Massive Multitask Language Understanding (MMLU) have played a pivotal role in evaluating AIâ€™s knowledge and abilities across diverse domains. However, existing benchmarks predominantly focus on content knowledge, leaving a critical gap in assessing modelsâ€™ understanding of pedagogy - the method and practice of teaching. This paper introduces The Pedagogy Benchmark, a novel dataset designed to evaluate large language models on their Cross-Domain Pedagogical Knowledge (CDPK) and Special Education Needs and Disability (SEND) pedagogical knowledge. These benchmarks are built on a carefully curated set of questions sourced from professional development exams for teachers, which cover a range of pedagogical subdomains such as teaching strategies and assessment methods. Here we outline the methodology and development of these benchmarks. We report results for 97 models, with accuracies spanning a range from 28% to 89% on the pedagogical knowledge questions. We consider the relationship between cost and accuracy and chart the progression of the Pareto value frontier over time. We provide online leaderboards at <a target="_blank" rel="noopener" href="https://rebrand.ly/pedagogy">https://rebrand.ly/pedagogy</a> which are updated with new models and allow interactive exploration and filtering based on various model properties, such as cost per token and open-vs-closed weights, as well as looking at performance in different subjects. LLMs and generative AI have tremendous potential to influence education and help to address the global learning crisis. Education-focused benchmarks are crucial to measure modelsâ€™ capacities to understand pedagogical concepts, respond appropriately to learnersâ€™ needs, and support effective teaching practices across diverse contexts. They are needed for informing the responsible and evidence-based deployment of LLMs and LLM-based tools in educational settings, and for guiding both development and policy decisions. </p>
<blockquote>
<p>åƒå¤§è§„æ¨¡å¤šä»»åŠ¡è¯­è¨€ç†è§£ï¼ˆMMLUï¼‰è¿™æ ·çš„åŸºå‡†æµ‹è¯•åœ¨è¯„ä¼°äººå·¥æ™ºèƒ½åœ¨ä¸åŒé¢†åŸŸçš„çŸ¥è¯†å’Œèƒ½åŠ›æ–¹é¢å‘æŒ¥äº†è‡³å…³é‡è¦çš„ä½œç”¨ã€‚ç„¶è€Œï¼Œç°æœ‰çš„åŸºå‡†æµ‹è¯•ä¸»è¦é›†ä¸­åœ¨å†…å®¹çŸ¥è¯†ä¸Šï¼Œåœ¨è¯„ä¼°æ¨¡å‹å¯¹æ•™å­¦æ–¹æ³•çš„ç†è§£æ–¹é¢å­˜åœ¨é‡å¤§ç©ºç™½â€”â€”æ•™å­¦æ–¹æ³•å’Œå®è·µã€‚æœ¬æ–‡ä»‹ç»äº†ã€Šæ•™å­¦åŸºå‡†æµ‹è¯•ã€‹ï¼Œè¿™æ˜¯ä¸€ä¸ªæ–°å‹æ•°æ®é›†ï¼Œæ—¨åœ¨è¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹åœ¨è·¨åŸŸæ•™å­¦çŸ¥è¯†ï¼ˆCDPKï¼‰å’Œç‰¹æ®Šæ•™è‚²éœ€æ±‚ä¸æ®‹ç–¾ï¼ˆSENDï¼‰æ•™å­¦çŸ¥è¯†æ–¹é¢çš„èƒ½åŠ›ã€‚è¿™äº›åŸºå‡†æµ‹è¯•å»ºç«‹åœ¨ä»æ•™å¸ˆèŒä¸šå‘å±•è€ƒè¯•ä¸­ç²¾å¿ƒæŒ‘é€‰çš„é—®é¢˜é›†ä¸Šï¼Œæ¶µç›–äº†æ•™å­¦ç­–ç•¥å’Œè¯„ä¼°æ–¹æ³•ç­‰ä¸€ç³»åˆ—æ•™å­¦å­é¢†åŸŸã€‚åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬æ¦‚è¿°äº†è¿™äº›åŸºå‡†æµ‹è¯•çš„æ–¹æ³•è®ºå’Œå‘å±•ã€‚æˆ‘ä»¬æŠ¥å‘Šäº†97ä¸ªæ¨¡å‹çš„ç»“æœï¼Œåœ¨æ•™å­¦çŸ¥è¯†é—®é¢˜ä¸Šçš„å‡†ç¡®ç‡ä»28%åˆ°89%ä¸ç­‰ã€‚æˆ‘ä»¬è€ƒè™‘äº†æˆæœ¬ä¸å‡†ç¡®ç‡ä¹‹é—´çš„å…³ç³»ï¼Œå¹¶ç»˜åˆ¶äº†éšæ—¶é—´æ¨ç§»çš„å¸•ç´¯æ‰˜ä»·å€¼å‰æ²¿çš„è¿›å±•ã€‚æˆ‘ä»¬æä¾›äº†åœ¨çº¿æ’è¡Œæ¦œ<a target="_blank" rel="noopener" href="https://rebrand.ly/pedagogy%EF%BC%8C%E8%AF%A5%E6%8E%92%E8%A1%8C%E6%A6%9C%E4%BC%9A%E9%9A%8F%E6%96%B0%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%8A%A0%E5%85%A5%E8%80%8C%E6%9B%B4%E6%96%B0%EF%BC%8C%E5%B9%B6%E5%8F%AF%E6%A0%B9%E6%8D%AE%E5%90%84%E7%A7%8D%E6%A8%A1%E5%9E%8B%E5%B1%9E%E6%80%A7%E8%BF%9B%E8%A1%8C%E4%BA%92%E5%8A%A8%E6%8E%A2%E7%B4%A2%E5%92%8C%E8%BF%87%E6%BB%A4%EF%BC%8C%E5%A6%82%E6%AF%8F%E4%BB%A4%E7%89%8C%E7%9A%84%E6%88%90%E6%9C%AC%E5%92%8C%E5%BC%80%E6%94%BE%E4%B8%8E%E5%B0%81%E9%97%AD%E6%9D%83%E9%87%8D%EF%BC%8C%E4%BB%A5%E5%8F%8A%E5%9C%A8%E4%B8%8D%E5%90%8C%E7%A7%91%E7%9B%AE%E4%B8%AD%E7%9A%84%E8%A1%A8%E7%8E%B0%E3%80%82%E5%A4%A7%E5%9E%8B%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E5%92%8C%E7%94%9F%E6%88%90%E5%BC%8F%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E5%AF%B9%E6%95%99%E8%82%B2%E5%92%8C%E5%B8%AE%E5%8A%A9%E5%BA%94%E5%AF%B9%E5%85%A8%E7%90%83%E5%AD%A6%E4%B9%A0%E5%8D%B1%E6%9C%BA%E5%85%B7%E6%9C%89%E5%B7%A8%E5%A4%A7%E6%BD%9C%E5%8A%9B%E3%80%82%E4%BB%A5%E6%95%99%E8%82%B2%E4%B8%BA%E9%87%8D%E7%82%B9%E7%9A%84%E5%9F%BA%E5%87%86%E6%B5%8B%E8%AF%95%E5%AF%B9%E4%BA%8E%E8%A1%A1%E9%87%8F%E6%A8%A1%E5%9E%8B%E7%90%86%E8%A7%A3%E6%95%99%E5%AD%A6%E6%A6%82%E5%BF%B5%E7%9A%84%E8%83%BD%E5%8A%9B%E3%80%81%E9%80%82%E5%BD%93%E5%93%8D%E5%BA%94%E5%AD%A6%E4%B9%A0%E8%80%85%E7%9A%84%E9%9C%80%E6%B1%82%E4%BB%A5%E5%8F%8A%E5%9C%A8%E5%90%84%E7%A7%8D%E8%83%8C%E6%99%AF%E4%B8%8B%E6%94%AF%E6%8C%81%E6%9C%89%E6%95%88%E6%95%99%E5%AD%A6%E5%AE%9E%E8%B7%B5%E8%87%B3%E5%85%B3%E9%87%8D%E8%A6%81%E3%80%82%E5%AE%83%E4%BB%AC%E5%AF%B9%E4%BA%8E%E5%9C%A8%E6%95%99%E8%82%B2%E7%8E%AF%E5%A2%83%E4%B8%AD%E8%B4%9F%E8%B4%A3%E4%BB%BB%E5%92%8C%E5%9F%BA%E4%BA%8E%E8%AF%81%E6%8D%AE%E5%9C%B0%E9%83%A8%E7%BD%B2%E5%A4%A7%E5%9E%8B%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E5%92%8C%E5%A4%A7%E5%9E%8B%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E5%B7%A5%E5%85%B7%EF%BC%8C%E4%BB%A5%E5%8F%8A%E6%8C%87%E5%AF%BC%E5%BC%80%E5%8F%91%E5%92%8C%E6%94%BF%E7%AD%96%E5%86%B3%E7%AD%96%E9%83%BD%E8%87%B3%E5%85%B3%E9%87%8D%E8%A6%81%E3%80%82">https://rebrand.ly/pedagogyï¼Œè¯¥æ’è¡Œæ¦œä¼šéšæ–°æ¨¡å‹çš„åŠ å…¥è€Œæ›´æ–°ï¼Œå¹¶å¯æ ¹æ®å„ç§æ¨¡å‹å±æ€§è¿›è¡Œäº’åŠ¨æ¢ç´¢å’Œè¿‡æ»¤ï¼Œå¦‚æ¯ä»¤ç‰Œçš„æˆæœ¬å’Œå¼€æ”¾ä¸å°é—­æƒé‡ï¼Œä»¥åŠåœ¨ä¸åŒç§‘ç›®ä¸­çš„è¡¨ç°ã€‚å¤§å‹è¯­è¨€æ¨¡å‹å’Œç”Ÿæˆå¼äººå·¥æ™ºèƒ½å¯¹æ•™è‚²å’Œå¸®åŠ©åº”å¯¹å…¨çƒå­¦ä¹ å±æœºå…·æœ‰å·¨å¤§æ½œåŠ›ã€‚ä»¥æ•™è‚²ä¸ºé‡ç‚¹çš„åŸºå‡†æµ‹è¯•å¯¹äºè¡¡é‡æ¨¡å‹ç†è§£æ•™å­¦æ¦‚å¿µçš„èƒ½åŠ›ã€é€‚å½“å“åº”å­¦ä¹ è€…çš„éœ€æ±‚ä»¥åŠåœ¨å„ç§èƒŒæ™¯ä¸‹æ”¯æŒæœ‰æ•ˆæ•™å­¦å®è·µè‡³å…³é‡è¦ã€‚å®ƒä»¬å¯¹äºåœ¨æ•™è‚²ç¯å¢ƒä¸­è´Ÿè´£ä»»å’ŒåŸºäºè¯æ®åœ°éƒ¨ç½²å¤§å‹è¯­è¨€æ¨¡å‹å’Œå¤§å‹è¯­è¨€æ¨¡å‹å·¥å…·ï¼Œä»¥åŠæŒ‡å¯¼å¼€å‘å’Œæ”¿ç­–å†³ç­–éƒ½è‡³å…³é‡è¦ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.18710v1">PDF</a> </p>
<p><strong>æ‘˜è¦</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†æ–°çš„åŸºå‡†æµ‹è¯•â€”â€”æ•™å­¦åŸºå‡†æµ‹è¯•ï¼Œè¯¥åŸºå‡†æµ‹è¯•æ˜¯é€šè¿‡ä¸€ç»„ä¸“ä¸šæ•™å¸ˆå‘å±•è€ƒè¯•çš„é—®é¢˜æ„å»ºçš„ï¼Œæ—¨åœ¨è¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹åœ¨è·¨åŸŸæ•™å­¦çŸ¥è¯†ï¼ˆCDPKï¼‰å’Œç‰¹æ®Šæ•™è‚²ä¸æ®‹ç–¾ï¼ˆSENDï¼‰æ•™å­¦çŸ¥è¯†æ–¹é¢çš„èƒ½åŠ›ã€‚æ–‡ç« æ¦‚è¿°äº†è¯¥æ–¹æ³•è®ºå’Œè¿™äº›åŸºå‡†æµ‹è¯•çš„å¼€å‘è¿‡ç¨‹ï¼Œå¹¶æŠ¥å‘Šäº†97ä¸ªæ¨¡å‹åœ¨æ•™å­¦çŸ¥è¯†é—®é¢˜ä¸Šçš„å‡†ç¡®ç‡ï¼ŒèŒƒå›´ä»28%åˆ°89%ã€‚æ–‡ç« è¿˜è€ƒè™‘äº†æˆæœ¬ä¸å‡†ç¡®ç‡ä¹‹é—´çš„å…³ç³»ï¼Œå¹¶éšç€æ—¶é—´çš„æ¨ç§»è®°å½•äº†å¸•ç´¯æ‰˜ä»·å€¼å‰æ²¿çš„è¿›å±•ã€‚è¯„ä¼°å’Œæ’åå¯ä»¥åœ¨åœ¨çº¿æ’è¡Œæ¦œæ‰¾åˆ°ã€‚æ•™è‚²é¢†åŸŸçš„åŸºå‡†æµ‹è¯•å¯¹äºè¡¡é‡å¤§å‹è¯­è¨€æ¨¡å‹ç†è§£æ•™å­¦æ¦‚å¿µçš„èƒ½åŠ›ã€é€‚å½“å“åº”å­¦ä¹ è€…éœ€æ±‚çš„èƒ½åŠ›ä»¥åŠæ”¯æŒä¸åŒèƒŒæ™¯ä¸‹çš„æœ‰æ•ˆæ•™å­¦å®è·µçš„èƒ½åŠ›è‡³å…³é‡è¦ã€‚å®ƒä»¬å¯¹äºåœ¨æ•™è‚²ç¯å¢ƒä¸­è´Ÿè´£ä»»å’Œå¾ªè¯åœ°éƒ¨ç½²å¤§å‹è¯­è¨€æ¨¡å‹ä»¥åŠæŒ‡å¯¼å¼€å‘å’Œæ”¿ç­–å†³ç­–æ˜¯å¿…è¦çš„ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹åœ¨æ•™å­¦é¢†åŸŸçš„çŸ¥è¯†ç†è§£è¯„ä¼°ä¸­å­˜åœ¨ä¸€ä¸ªé‡å¤§å·®è·ã€‚ç°æœ‰åŸºå‡†æµ‹è¯•ä¸»è¦å…³æ³¨å†…å®¹çŸ¥è¯†ï¼Œè€Œå¿½è§†äº†å¯¹æ•™å­¦æ–¹æ³•çš„è¯„ä¼°ã€‚ä¸ºæ­¤æœ¬æ–‡å¼•å…¥æ–°çš„æ•™å­¦åŸºå‡†æµ‹è¯•è¿›è¡Œè¯„ä¼°ã€‚è¯¥åŸºå‡†æµ‹è¯•æ¶‰åŠå¤šä¸ªé¢†åŸŸçš„æ•™å­¦çŸ¥è¯†ï¼Œå¦‚æ•™å­¦æ–¹æ³•å’Œè¯„ä¼°æ–¹æ³•ç­‰ã€‚</li>
<li>æ•™å­¦åŸºå‡†æµ‹è¯•æ˜¯åŸºäºæ•™å¸ˆèŒä¸šå‘å±•è€ƒè¯•çš„é—®é¢˜æ„å»ºçš„ï¼Œè¿™äº›é—®é¢˜è¦†ç›–äº†å¹¿æ³›çš„æ•™å­¦é¢†åŸŸï¼ŒåŒ…æ‹¬æ•™å­¦ç­–ç•¥ã€è¯„ä¼°æ–¹æ³•ç­‰ã€‚è¿™ä¸ºè¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹åœ¨æ•™å­¦çŸ¥è¯†æ–¹é¢çš„èƒ½åŠ›æä¾›äº†ä¾æ®ã€‚</li>
<li>åœ¨æ•™å­¦çŸ¥è¯†é—®é¢˜çš„æµ‹è¯•ä¸­ï¼Œæ¨¡å‹çš„å‡†ç¡®ç‡å·®å¼‚å¾ˆå¤§ï¼ŒèŒƒå›´ä»28%åˆ°89%ï¼Œè¿™è¡¨æ˜å¤§å‹è¯­è¨€æ¨¡å‹åœ¨æ•™å­¦é¢†åŸŸçš„ç†è§£å’Œè¡¨ç°æ°´å¹³æœ‰å¾ˆå¤§å·®å¼‚ã€‚è¿™ä¸ºç ”ç©¶è€…æä¾›äº†æ¨¡å‹æ€§èƒ½çš„æ¯”è¾ƒä¾æ®ã€‚æ­¤å¤–è¿˜è€ƒè™‘äº†æˆæœ¬å’Œå‡†ç¡®ç‡ä¹‹é—´çš„å…³ç³»ï¼Œä»¥åŠå¸•ç´¯æ‰˜ä»·å€¼å‰æ²¿çš„è¿›å±•è¶‹åŠ¿åˆ†æã€‚è¿™ä¸ºæ¨¡å‹å¼€å‘è€…æä¾›äº†é‡è¦çš„å†³ç­–ä¾æ®ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.18710">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-6108ff582c9d60850da39e17184fdabf.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-15e1813648a35e69eafbf500776edd07.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-93cf348b124c0af8bc92c811d4a7c3dd.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-af413a311489c9357a17bfe9e51cfa95.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="Floating-Point-Data-Transformation-for-Lossless-Compression"><a href="#Floating-Point-Data-Transformation-for-Lossless-Compression" class="headerlink" title="Floating-Point Data Transformation for Lossless Compression"></a>Floating-Point Data Transformation for Lossless Compression</h2><p><strong>Authors:Samirasadat Jamalidinan, Kazem Cheshmi</strong></p>
<p>Floating-point data is widely used across various domains. Depending on the required precision, each floating-point value can occupy several bytes. Lossless storage of this information is crucial due to its critical accuracy, as seen in applications such as medical imaging and language model weights. In these cases, data size is often significant, making lossless compression essential. Previous approaches either treat this data as raw byte streams for compression or fail to leverage all patterns within the dataset. However, because multiple bytes represent a single value and due to inherent patterns in floating-point representations, some of these bytes are correlated. To leverage this property, we propose a novel data transformation method called Typed Data Transformation (\DTT{}) that groups related bytes together to improve compression. We implemented and tested our approach on various datasets across both CPU and GPU. \DTT{} achieves a geometric mean compression ratio improvement of 1.16$\times$ over state-of-the-art compression tools such as zstd, while also improving both compression and decompression throughput by 1.18â€“3.79$\times$. </p>
<blockquote>
<p>æµ®ç‚¹æ•°æ®åœ¨å„ä¸ªé¢†åŸŸéƒ½æœ‰å¹¿æ³›åº”ç”¨ã€‚æ ¹æ®æ‰€éœ€çš„ç²¾åº¦ï¼Œæ¯ä¸ªæµ®ç‚¹å€¼å¯èƒ½ä¼šå ç”¨å¤šä¸ªå­—èŠ‚ã€‚ç”±äºå…¶åœ¨åŒ»ç–—æˆåƒå’Œè¯­è¨€æ¨¡å‹æƒé‡ç­‰åº”ç”¨ä¸­çš„å…³é”®å‡†ç¡®æ€§ï¼Œè¿™ç§ä¿¡æ¯çš„æ— æŸå­˜å‚¨è‡³å…³é‡è¦ã€‚åœ¨è¿™äº›æƒ…å†µä¸‹ï¼Œæ•°æ®å¤§å°å¾€å¾€å¾ˆå¤§ï¼Œå› æ­¤æ— æŸå‹ç¼©å˜å¾—è‡³å…³é‡è¦ã€‚ä»¥å‰çš„æ–¹æ³•è¦ä¹ˆå°†è¿™ç§æ•°æ®è§†ä¸ºåŸå§‹å­—èŠ‚æµè¿›è¡Œå‹ç¼©ï¼Œè¦ä¹ˆæœªèƒ½åˆ©ç”¨æ•°æ®é›†ä¸­çš„æ‰€æœ‰æ¨¡å¼ã€‚ç„¶è€Œï¼Œç”±äºå¤šä¸ªå­—èŠ‚ä»£è¡¨ä¸€ä¸ªå€¼ï¼Œå¹¶ä¸”ç”±äºæµ®ç‚¹è¡¨ç¤ºä¸­çš„å›ºæœ‰æ¨¡å¼ï¼Œè¿™äº›å­—èŠ‚ä¸­çš„æŸäº›æ˜¯ç›¸äº’å…³è”çš„ã€‚ä¸ºäº†åˆ©ç”¨è¿™ä¸€ç‰¹æ€§ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°å‹æ•°æ®è½¬æ¢æ–¹æ³•ï¼Œç§°ä¸ºTyped Data Transformationï¼ˆDTTï¼‰ï¼Œå®ƒå°†ç›¸å…³å­—èŠ‚ç»„åˆåœ¨ä¸€èµ·ä»¥æé«˜å‹ç¼©æ•ˆæœã€‚æˆ‘ä»¬åœ¨CPUå’ŒGPUä¸Šçš„å„ç§æ•°æ®é›†ä¸Šå®ç°äº†å¹¶æµ‹è¯•äº†æˆ‘ä»¬çš„æ–¹æ³•ã€‚DTTä¸æœ€æ–°çš„å‹ç¼©å·¥å…·ï¼ˆå¦‚zstdï¼‰ç›¸æ¯”ï¼Œå®ç°äº†å‡ ä½•å¹³å‡å‹ç¼©æ¯”æé«˜1.16å€ï¼ŒåŒæ—¶æé«˜äº†å‹ç¼©å’Œè§£å‹ç¼©ååé‡çš„1.18-3.79å€ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.18062v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†æµ®ç‚¹æ•°æ•°æ®åœ¨å„ä¸ªé¢†åŸŸä¸­çš„å¹¿æ³›åº”ç”¨ï¼Œä»¥åŠå¯¹å…¶è¿›è¡Œæ— æŸå­˜å‚¨å’Œå‹ç¼©çš„é‡è¦æ€§ã€‚é’ˆå¯¹æµ®ç‚¹æ•°æ•°æ®çš„ç‰¹ç‚¹ï¼Œæå‡ºäº†ä¸€ç§æ–°å‹çš„æ•°æ®è½¬æ¢æ–¹æ³•â€”â€”Typed Data Transformationï¼ˆDTTï¼‰ï¼Œè¯¥æ–¹æ³•èƒ½å¤Ÿåˆ©ç”¨æµ®ç‚¹æ•°å†…åœ¨çš„æ¨¡å¼å’Œç›¸å…³æ€§ï¼Œå°†ç›¸å…³å­—èŠ‚åˆ†ç»„ä»¥æé«˜å‹ç¼©æ•ˆç‡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒDTTæ–¹æ³•åœ¨CPUå’ŒGPUä¸Šçš„æ•°æ®é›†å‹ç¼©æ•ˆç‡è¾ƒç°æœ‰å·¥å…·ï¼ˆå¦‚zstdï¼‰å¹³å‡æé«˜äº†1.16å€ï¼Œå¹¶æå‡äº†å‹ç¼©å’Œè§£å‹ç¼©çš„é€Ÿåº¦ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æµ®ç‚¹æ•°æ•°æ®åœ¨åŒ»ç–—æˆåƒå’Œè¯­è¨€æ¨¡å‹æƒé‡ç­‰åº”ç”¨ä¸­ï¼Œç”±äºå…¶ç²¾ç¡®åº¦è¦æ±‚ï¼Œæ— æŸå­˜å‚¨éå¸¸é‡è¦ã€‚</li>
<li>ç°æœ‰æ–¹æ³•åœ¨å¤„ç†æµ®ç‚¹æ•°æ•°æ®æ—¶å­˜åœ¨å±€é™æ€§ï¼Œæ— æ³•å……åˆ†åˆ©ç”¨æ•°æ®ä¸­çš„æ¨¡å¼ã€‚</li>
<li>æµ®ç‚¹æ•°æ•°æ®ä¸­çš„å¤šä¸ªå­—èŠ‚ä»£è¡¨ä¸€ä¸ªå€¼ï¼Œè¿™äº›å­—èŠ‚ä¹‹é—´å­˜åœ¨ç›¸å…³æ€§ã€‚</li>
<li>æå‡ºäº†ä¸€ç§æ–°çš„æ•°æ®è½¬æ¢æ–¹æ³•â€”â€”Typed Data Transformationï¼ˆDTTï¼‰ï¼Œåˆ©ç”¨æµ®ç‚¹æ•°æ•°æ®ä¸­çš„ç›¸å…³å­—èŠ‚ä»¥æé«˜å‹ç¼©æ•ˆç‡ã€‚</li>
<li>DTTæ–¹æ³•å®ç°äº†å¯¹æµ®ç‚¹æ•°æ•°æ®çš„æ— æŸå‹ç¼©ï¼Œå¹¶æé«˜äº†å‹ç¼©å’Œè§£å‹ç¼©çš„é€Ÿåº¦ã€‚</li>
<li>DTTæ–¹æ³•åœ¨CPUå’ŒGPUä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼Œå…¶å‹ç¼©æ•ˆç‡è¾ƒç°æœ‰å·¥å…·å¹³å‡æé«˜äº†1.16å€ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.18062">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-ce4e86f05586dea78e0b16cabb19002b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1cbc0c726c5b9aa54c31a71b1ce373e6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b1dd9ca559f952dffe7917cb0b9174c0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-185071418550b682e8d6ffcec7b82f92.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-7c21976891ac445f346a985196f75adc.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f0fa4d680d19a58c6e43235e459a5015.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="TPTT-Transforming-Pretrained-Transformer-into-Titans"><a href="#TPTT-Transforming-Pretrained-Transformer-into-Titans" class="headerlink" title="TPTT: Transforming Pretrained Transformer into Titans"></a>TPTT: Transforming Pretrained Transformer into Titans</h2><p><strong>Authors:Fabien Furfaro</strong></p>
<p>Recent advances in large language models (LLMs) have led to remarkable progress in natural language processing, but their computational and memory demands remain a significant challenge, particularly for long-context inference. We introduce TPTT (Transforming Pretrained Transformer into Titans), a novel framework for enhancing pretrained Transformer models with efficient linearized attention mechanisms and advanced memory management. TPTT employs techniques such as Memory as Gate (MaG) and mixed linearized attention (LiZA). It is fully compatible with the Hugging Face Transformers library, enabling seamless adaptation of any causal LLM through parameter-efficient fine-tuning (LoRA) without full retraining. We show the effectiveness of TPTT on the MMLU benchmark with models of approximately 1 billion parameters, observing substantial improvements in both efficiency and accuracy. For instance, Titans-Llama-3.2-1B achieves a 20% increase in Exact Match (EM) over its baseline. Statistical analyses and comparisons with recent state-of-the-art methods confirm the practical scalability and robustness of TPTT. Code is available at <a target="_blank" rel="noopener" href="https://github.com/fabienfrfr/tptt">https://github.com/fabienfrfr/tptt</a> . Python package at <a target="_blank" rel="noopener" href="https://pypi.org/project/tptt/">https://pypi.org/project/tptt/</a> . </p>
<blockquote>
<p>è¿‘æœŸå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„è¿›æ­¥åœ¨è‡ªç„¶è¯­è¨€å¤„ç†æ–¹é¢å–å¾—äº†æ˜¾è‘—æˆæ•ˆï¼Œä½†å…¶è®¡ç®—å’Œå†…å­˜éœ€æ±‚ä»ç„¶æ˜¯ä¸€ä¸ªå·¨å¤§æŒ‘æˆ˜ï¼Œå°¤å…¶æ˜¯å¯¹é•¿æ–‡æœ¬ä¸Šä¸‹æ–‡æ¨ç†è€Œè¨€ã€‚æˆ‘ä»¬ä»‹ç»äº†TPTTï¼ˆå°†é¢„è®­ç»ƒTransformerè½¬åŒ–ä¸ºå·¨äººï¼‰è¿™ä¸€æ–°å‹æ¡†æ¶ï¼Œå®ƒé€šè¿‡é«˜æ•ˆçš„çº¿æ€§åŒ–æ³¨æ„åŠ›æœºåˆ¶å’Œå…ˆè¿›çš„å†…å­˜ç®¡ç†æ¥å¢å¼ºé¢„è®­ç»ƒçš„Transformeræ¨¡å‹ã€‚TPTTé‡‡ç”¨è¯¸å¦‚Memory as Gateï¼ˆMaGï¼‰å’Œæ··åˆçº¿æ€§åŒ–æ³¨æ„åŠ›ï¼ˆLiZAï¼‰ç­‰æŠ€æœ¯ã€‚å®ƒå®Œå…¨å…¼å®¹Hugging Face Transformersåº“ï¼Œé€šè¿‡å‚æ•°é«˜æ•ˆçš„å¾®è°ƒï¼ˆLoRAï¼‰æ— ç¼é€‚åº”ä»»ä½•å› æœLLMï¼Œæ— éœ€è¿›è¡Œå…¨é¢å†è®­ç»ƒã€‚æˆ‘ä»¬åœ¨MMLUåŸºå‡†æµ‹è¯•ä¸Šå±•ç¤ºäº†TPTTçš„æœ‰æ•ˆæ€§ï¼Œè¯¥æµ‹è¯•ä½¿ç”¨çš„æ¨¡å‹å‚æ•°å¤§çº¦1äº¿ï¼Œåœ¨æ•ˆç‡å’Œå‡†ç¡®æ€§æ–¹é¢éƒ½å–å¾—äº†æ˜¾è‘—æ”¹è¿›ã€‚ä¾‹å¦‚ï¼ŒTitans-Llama-3.2-1Båœ¨ç²¾ç¡®åŒ¹é…ï¼ˆEMï¼‰æ–¹é¢å®ç°äº†æ¯”å…¶åŸºå‡†æ¨¡å‹é«˜å‡º20%çš„æ”¹è¿›ã€‚ä¸æœ€æ–°æœ€å…ˆè¿›çš„æ–¹æ³•çš„ç»Ÿè®¡åˆ†æå’Œæ¯”è¾ƒï¼Œè¯å®äº†TPTTçš„å®é™…å¯æ‰©å±•æ€§å’Œç¨³å¥æ€§ã€‚ä»£ç å¯åœ¨[<a target="_blank" rel="noopener" href="https://github.com/fabienfrfr/tptt%E6%89%BE%E5%88%B0%E3%80%82Python%E8%BD%AF%E4%BB%B6%E5%8C%85%E5%8F%AF%E5%9C%A8https://pypi.org/project/tptt/%E3%80%82]">https://github.com/fabienfrfr/tpttæ‰¾åˆ°ã€‚Pythonè½¯ä»¶åŒ…å¯åœ¨https://pypi.org/project/tptt/ã€‚]</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.17671v1">PDF</a> 6 pages, 1 figure</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æœ€æ–°è¿›å±•ï¼Œè™½ç„¶è‡ªç„¶è¯­è¨€å¤„ç†å–å¾—äº†æ˜¾è‘—è¿›æ­¥ï¼Œä½†è®¡ç®—å’Œå†…å­˜éœ€æ±‚ä»ç„¶æ˜¯ä¸€ä¸ªæŒ‘æˆ˜ï¼Œç‰¹åˆ«æ˜¯å¯¹äºé•¿æ–‡æœ¬æ¨ç†ã€‚ä¸ºæ­¤ï¼Œæ–‡ç« æå‡ºäº†ä¸€ç§æ–°å‹æ¡†æ¶TPTTï¼ˆTransforming Pretrained Transformer into Titansï¼‰ï¼Œé€šè¿‡æœ‰æ•ˆçš„çº¿æ€§åŒ–æ³¨æ„åŠ›æœºåˆ¶å’Œå…ˆè¿›çš„å†…å­˜ç®¡ç†å¢å¼ºé¢„è®­ç»ƒTransformeræ¨¡å‹ã€‚TPTTé‡‡ç”¨Memory as Gateï¼ˆMaGï¼‰å’Œæ··åˆçº¿æ€§åŒ–æ³¨æ„åŠ›ï¼ˆLiZAï¼‰ç­‰æŠ€æœ¯ï¼Œä¸Hugging Face Transformersåº“å®Œå…¨å…¼å®¹ï¼Œå¯é€šè¿‡å‚æ•°é«˜æ•ˆå¾®è°ƒï¼ˆLoRAï¼‰æ— ç¼é€‚åº”ä»»ä½•å› æœLLMï¼Œæ— éœ€å…¨é¢é‡æ–°è®­ç»ƒã€‚åœ¨MMLUåŸºå‡†æµ‹è¯•ä¸Šï¼ŒTPTTè¡¨ç°å‡ºå“è¶Šçš„æ•ˆæœï¼Œå°¤å…¶æ˜¯æ¨¡å‹å‚æ•°çº¦ä¸ºåäº¿çš„å¤§å‹æ¨¡å‹ï¼Œåœ¨æ•ˆç‡å’Œå‡†ç¡®æ€§æ–¹é¢å‡æœ‰æ˜¾è‘—æé«˜ã€‚ä¾‹å¦‚ï¼Œâ€œTitans-Llama-3.2-1Bâ€æ¨¡å‹çš„ç²¾ç¡®åŒ¹é…ï¼ˆEMï¼‰æé«˜äº†20%ã€‚ç»Ÿè®¡åˆ†æå’Œä¸æœ€æ–°æœ€å…ˆè¿›çš„æŠ€æœ¯çš„æ¯”è¾ƒè¯æ˜äº†TPTTçš„å®é™…å¯æ‰©å±•æ€§å’Œç¨³å¥æ€§ã€‚ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/fabienfrfr/tptt%E6%89%BE%E5%88%B0%E3%80%82Python%E5%8C%85%E5%8F%AF%E5%9C%A8https://pypi.org/project/tptt/%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/fabienfrfr/tpttæ‰¾åˆ°ã€‚PythonåŒ…å¯åœ¨https://pypi.org/project/tptt/è·å–ã€‚</a></p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>TPTTæ¡†æ¶ç”¨äºå¢å¼ºé¢„è®­ç»ƒTransformeræ¨¡å‹ï¼Œé€šè¿‡æœ‰æ•ˆçš„çº¿æ€§åŒ–æ³¨æ„åŠ›æœºåˆ¶å’Œå…ˆè¿›çš„å†…å­˜ç®¡ç†æé«˜æ€§èƒ½ã€‚</li>
<li>TPTTé‡‡ç”¨Memory as Gate (MaG) å’Œæ··åˆçº¿æ€§åŒ–æ³¨æ„åŠ›ï¼ˆLiZAï¼‰æŠ€æœ¯ã€‚</li>
<li>TPTTä¸Hugging Face Transformersåº“å…¼å®¹ï¼Œå¯æ— ç¼é€‚åº”ä»»ä½•å› æœLLMã€‚</li>
<li>TPTTé€šè¿‡å‚æ•°é«˜æ•ˆå¾®è°ƒï¼ˆLoRAï¼‰å®ç°ï¼Œæ— éœ€å…¨é¢é‡æ–°è®­ç»ƒæ¨¡å‹ã€‚</li>
<li>åœ¨MMLUåŸºå‡†æµ‹è¯•ä¸­ï¼ŒTPTTè¡¨ç°å“è¶Šï¼Œå°¤å…¶åœ¨å¤§å‹æ¨¡å‹ä¸Šæ˜¾è‘—æé«˜äº†æ•ˆç‡å’Œå‡†ç¡®æ€§ã€‚</li>
<li>â€œTitans-Llama-3.2-1Bâ€æ¨¡å‹çš„ç²¾ç¡®åŒ¹é…åº¦è¾ƒåŸºçº¿æé«˜äº†20%ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.17671">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-1b153ab3b5fc9806cef5d5808de7de96.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-9199c1ab4e28c92520bd6a7c725fc77c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-267b36b29686bd5b617cb7258ff07fa9.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="TyphoFormer-Language-Augmented-Transformer-for-Accurate-Typhoon-Track-Forecasting"><a href="#TyphoFormer-Language-Augmented-Transformer-for-Accurate-Typhoon-Track-Forecasting" class="headerlink" title="TyphoFormer: Language-Augmented Transformer for Accurate Typhoon Track   Forecasting"></a>TyphoFormer: Language-Augmented Transformer for Accurate Typhoon Track   Forecasting</h2><p><strong>Authors:Lincan Li, Eren Erman Ozguven, Yue Zhao, Guang Wang, Yiqun Xie, Yushun Dong</strong></p>
<p>Accurate typhoon track forecasting is crucial for early system warning and disaster response. While Transformer-based models have demonstrated strong performance in modeling the temporal dynamics of dense trajectories of humans and vehicles in smart cities, they usually lack access to broader contextual knowledge that enhances the forecasting reliability of sparse meteorological trajectories, such as typhoon tracks. To address this challenge, we propose TyphoFormer, a novel framework that incorporates natural language descriptions as auxiliary prompts to improve typhoon trajectory forecasting. For each time step, we use Large Language Model (LLM) to generate concise textual descriptions based on the numerical attributes recorded in the North Atlantic hurricane database. The language descriptions capture high-level meteorological semantics and are embedded as auxiliary special tokens prepended to the numerical time series input. By integrating both textual and sequential information within a unified Transformer encoder, TyphoFormer enables the model to leverage contextual cues that are otherwise inaccessible through numerical features alone. Extensive experiments are conducted on HURDAT2 benchmark, results show that TyphoFormer consistently outperforms other state-of-the-art baseline methods, particularly under challenging scenarios involving nonlinear path shifts and limited historical observations. </p>
<blockquote>
<p>ç²¾ç¡®é¢„æµ‹å°é£è·¯å¾„å¯¹äºæ—©æœŸç³»ç»Ÿé¢„è­¦å’Œç¾å®³åº”å¯¹è‡³å…³é‡è¦ã€‚è™½ç„¶åŸºäºTransformerçš„æ¨¡å‹åœ¨æ¨¡æ‹Ÿæ™ºèƒ½åŸå¸‚ä¸­äººç±»å’Œè½¦è¾†çš„å¯†é›†è½¨è¿¹çš„æ—¶é—´åŠ¨æ€æ–¹é¢è¡¨ç°å‡ºå¼ºå¤§çš„æ€§èƒ½ï¼Œä½†å®ƒä»¬é€šå¸¸æ— æ³•è·å–æ›´å¹¿æ³›çš„ä¸Šä¸‹æ–‡çŸ¥è¯†ï¼Œè¿™æœ‰åŠ©äºæé«˜ç¨€ç–æ°”è±¡è½¨è¿¹çš„é¢„æµ‹å¯é æ€§ï¼Œä¾‹å¦‚å°é£è·¯å¾„ã€‚ä¸ºäº†è§£å†³è¿™ä¸€æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†TyphFormerï¼Œè¿™æ˜¯ä¸€ä¸ªæ–°çš„æ¡†æ¶ï¼Œå®ƒåˆ©ç”¨è‡ªç„¶è¯­è¨€æè¿°ä½œä¸ºè¾…åŠ©æç¤ºæ¥æé«˜å°é£è½¨è¿¹çš„é¢„æµ‹èƒ½åŠ›ã€‚å¯¹äºæ¯ä¸ªæ—¶é—´ç‚¹ï¼Œæˆ‘ä»¬ä½¿ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ ¹æ®åŒ—å¤§è¥¿æ´‹é£“é£æ•°æ®åº“è®°å½•çš„æ•°å€¼å±æ€§ç”Ÿæˆç®€æ´çš„æ–‡æœ¬æè¿°ã€‚è¿™äº›è¯­è¨€æè¿°æ•æ‰äº†é«˜çº§æ°”è±¡è¯­ä¹‰ï¼Œå¹¶è¢«åµŒå…¥ä½œä¸ºè¾…åŠ©ç‰¹æ®Šä»¤ç‰Œé™„åŠ åˆ°æ•°å€¼æ—¶é—´åºåˆ—è¾“å…¥ä¸­ã€‚é€šè¿‡åœ¨ç»Ÿä¸€çš„Transformerç¼–ç å™¨å†…æ•´åˆæ–‡æœ¬å’Œæ—¶åºä¿¡æ¯ï¼ŒTyphFormerä½¿æ¨¡å‹èƒ½å¤Ÿåˆ©ç”¨æ— æ³•é€šè¿‡æ•°å€¼ç‰¹å¾å•ç‹¬è®¿é—®çš„ä¸Šä¸‹æ–‡çº¿ç´¢ã€‚åœ¨HURDAT2åŸºå‡†æµ‹è¯•ä¸Šè¿›è¡Œäº†å¤§é‡å®éªŒï¼Œç»“æœè¡¨æ˜ï¼ŒTyphFormeræŒç»­ä¼˜äºå…¶ä»–æœ€å…ˆè¿›çš„åŸºç¡€æ–¹æ³•ï¼Œç‰¹åˆ«æ˜¯åœ¨æ¶‰åŠéçº¿æ€§è·¯å¾„å˜åŒ–å’Œæœ‰é™å†å²è§‚æµ‹çš„å…·æœ‰æŒ‘æˆ˜æ€§çš„åœºæ™¯ä¸­è¡¨ç°å°¤å…¶å‡ºè‰²ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.17609v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºä¸€ç§åä¸ºTyphoFormerçš„æ–°å‹æ¡†æ¶ï¼Œç”¨äºç»“åˆè‡ªç„¶è¯­è¨€æè¿°ä½œä¸ºè¾…åŠ©æç¤ºï¼Œä»¥æé«˜å°é£è½¨è¿¹é¢„æµ‹çš„å‡†ç¡®æ€§ã€‚è¯¥æ¡†æ¶åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ç”ŸæˆåŸºäºåŒ—å¤§è¥¿æ´‹é£“é£æ•°æ®åº“ä¸­æ•°å€¼å±æ€§çš„ç®€æ´æ–‡æœ¬æè¿°ï¼Œæ•æ‰é«˜çº§æ°”è±¡è¯­ä¹‰ï¼Œå¹¶å°†å…¶åµŒå…¥ä½œä¸ºè¾…åŠ©ç‰¹æ®Šä»¤ç‰Œé™„åŠ åˆ°æ•°å€¼æ—¶é—´åºåˆ—è¾“å…¥ä¸­ã€‚é€šè¿‡åœ¨ä¸€ä¸ªç»Ÿä¸€çš„Transformerç¼–ç å™¨å†…æ•´åˆæ–‡æœ¬å’Œæ—¶åºä¿¡æ¯ï¼ŒTyphoFormerä½¿æ¨¡å‹èƒ½å¤Ÿåˆ©ç”¨ä»…é€šè¿‡æ•°å€¼ç‰¹å¾æ— æ³•è·å–çš„ä¸Šä¸‹æ–‡çº¿ç´¢ã€‚åœ¨HURDAT2åŸºå‡†æµ‹è¯•ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼ŒTyphoFormerå§‹ç»ˆä¼˜äºå…¶ä»–å…ˆè¿›çš„åŸºç¡€æ–¹æ³•ï¼Œç‰¹åˆ«æ˜¯åœ¨æ¶‰åŠéçº¿æ€§è·¯å¾„å˜åŒ–å’Œæœ‰é™å†å²è§‚æµ‹çš„å…·æœ‰æŒ‘æˆ˜æ€§çš„åœºæ™¯ä¸­ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å°é£è½¨è¿¹å‡†ç¡®é¢„æµ‹å¯¹æ—©æœŸç³»ç»Ÿé¢„è­¦å’Œç¾å®³åº”å¯¹è‡³å…³é‡è¦ã€‚</li>
<li>Transformeræ¨¡å‹åœ¨æ™ºèƒ½åŸå¸‚ä¸­å¯¹äººå’Œè½¦è¾†çš„å¯†é›†è½¨è¿¹å»ºæ¨¡è¡¨ç°å‡ºå¼ºå¤§çš„æ€§èƒ½ï¼Œä½†åœ¨ç¼ºä¹å¹¿æ³›æ°”è±¡èƒŒæ™¯çŸ¥è¯†çš„æƒ…å¢ƒä¸‹ï¼Œå¯¹äºç¨€ç–æ°”è±¡è½¨è¿¹çš„é¢„æµ‹å¯é æ€§æœ‰å¾…æé«˜ã€‚</li>
<li>TyphoFormeræ¡†æ¶ç»“åˆäº†è‡ªç„¶è¯­è¨€æè¿°ä½œä¸ºè¾…åŠ©æç¤ºï¼Œæ—¨åœ¨æé«˜å°é£è½¨è¿¹é¢„æµ‹çš„å‡†ç¡®æ€§ã€‚</li>
<li>åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ç”ŸæˆåŸºäºæ•°å€¼å±æ€§çš„æ–‡æœ¬æè¿°ï¼Œæ•æ‰é«˜çº§æ°”è±¡è¯­ä¹‰ä¿¡æ¯ã€‚</li>
<li>é€šè¿‡åµŒå…¥è¾…åŠ©ç‰¹æ®Šä»¤ç‰Œåˆ°æ•°å€¼æ—¶é—´åºåˆ—è¾“å…¥ä¸­ï¼ŒTyphoFormeræ•´åˆäº†æ–‡æœ¬å’Œæ—¶åºä¿¡æ¯ã€‚</li>
<li>å®éªŒç»“æœè¡¨æ˜ï¼ŒTyphoFormeråœ¨HURDAT2åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œç‰¹åˆ«æ˜¯åœ¨å¤„ç†éçº¿æ€§è·¯å¾„å˜åŒ–å’Œæœ‰é™å†å²è§‚æµ‹ç­‰æŒ‘æˆ˜åœºæ™¯æ—¶ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.17609">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-901aa6814749a1ccd74629cd8d9255c6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6c11c1aaf8b90dba71b6ac0fabd4a50d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-714e62f07666d8d1294f4e1a190fe3a3.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="Cite-Pretrain-Retrieval-Free-Knowledge-Attribution-for-Large-Language-Models"><a href="#Cite-Pretrain-Retrieval-Free-Knowledge-Attribution-for-Large-Language-Models" class="headerlink" title="Cite Pretrain: Retrieval-Free Knowledge Attribution for Large Language   Models"></a>Cite Pretrain: Retrieval-Free Knowledge Attribution for Large Language   Models</h2><p><strong>Authors:Yukun Huang, Sanxing Chen, Jian Pei, Manzil Zaheer, Bhuwan Dhingra</strong></p>
<p>Trustworthy language models should provide both correct and verifiable answers. While language models can sometimes attribute their outputs to pretraining data, their citations are often unreliable due to hallucination. As a result, current systems insert citations by querying an external retriever at inference time, introducing latency, infrastructure dependence, and vulnerability to retrieval noise. We explore whether LLMs can be made to reliably attribute to the documents seen during (continual) pretrainingâ€“without test-time retrievalâ€“by revising the training process. To evaluate this, we release CitePretrainBench, a benchmark that mixes real-world corpora (Wikipedia, Common Crawl, arXiv) with novel, unseen documents and probes both short-form (single fact) and long-form (multi-fact) citation tasks. Our approach follows a two-stage process: (1) continual pretraining to bind facts to persistent document identifiers, and (2) instruction tuning to elicit citation behavior. We find that simple Passive Indexing, which appends an identifier to each document, helps memorize verbatim text but fails on paraphrased or compositional facts. Instead, we propose Active Indexing, which continually pretrains on synthetic QA pairs that (1) restate each fact in diverse compositional forms, and (2) require bidirectional source-to-fact and fact-to-source generation, jointly teaching the model to generate content from a cited source and to attribute its own answers. Experiments with Qwen2.5-7B and 3B show that Active Indexing consistently outperforms Passive Indexing across all tasks and models, with citation precision gains up to 30.2 percent. Our ablation studies reveal that performance continues to improve as we scale the amount of augmented data, showing a clear upward trend even at 16 times the original token count. </p>
<blockquote>
<p>å¯ä¿¡èµ–çš„è¯­è¨€æ¨¡å‹åº”è¯¥æä¾›æ­£ç¡®ä¸”å¯éªŒè¯çš„ç­”æ¡ˆã€‚è™½ç„¶è¯­è¨€æ¨¡å‹æœ‰æ—¶ä¼šå°†å®ƒä»¬çš„è¾“å‡ºå½’åŠŸäºé¢„è®­ç»ƒæ•°æ®ï¼Œä½†ç”±äºå¹»è§‰ï¼Œå®ƒä»¬çš„å¼•ç”¨é€šå¸¸ä¸å¯é ã€‚å› æ­¤ï¼Œå½“å‰çš„ç³»ç»Ÿä¼šåœ¨æ¨ç†æ—¶æŸ¥è¯¢å¤–éƒ¨æ£€ç´¢å™¨æ¥æ’å…¥å¼•ç”¨ï¼Œè¿™å¼•å…¥äº†å»¶è¿Ÿã€å¯¹åŸºç¡€è®¾æ–½çš„ä¾èµ–ä»¥åŠæ˜“å—æ£€ç´¢å™ªå£°å½±å“çš„é—®é¢˜ã€‚æˆ‘ä»¬é€šè¿‡ä¿®æ”¹è®­ç»ƒè¿‡ç¨‹ï¼Œæ¢ç´¢æ˜¯å¦èƒ½è®©å¤§å‹è¯­è¨€æ¨¡å‹åœ¨ï¼ˆæŒç»­ï¼‰é¢„è®­ç»ƒæœŸé—´å¯é åœ°å¼•ç”¨æ‰€çœ‹åˆ°çš„æ–‡æ¡£ï¼Œè€Œæ— éœ€åœ¨æµ‹è¯•æ—¶è¿›è¡Œæ£€ç´¢ã€‚ä¸ºäº†è¯„ä¼°è¿™ä¸€ç‚¹ï¼Œæˆ‘ä»¬å‘å¸ƒäº†CitePretrainBenchåŸºå‡†æµ‹è¯•ï¼Œå®ƒæ··åˆäº†çœŸå®è¯­æ–™åº“ï¼ˆWikipediaã€Common Crawlã€arXivï¼‰ä¸æ–°é¢–ä¸”æœªè§è¿‡çš„æ–‡æ¡£ï¼Œå¹¶æ¢è®¨äº†çŸ­å½¢å¼ï¼ˆå•äº‹å®ï¼‰å’Œé•¿å½¢å¼ï¼ˆå¤šäº‹å®ï¼‰çš„å¼•ç”¨ä»»åŠ¡ã€‚æˆ‘ä»¬çš„æ–¹æ³•éµå¾ªä¸¤é˜¶æ®µè¿‡ç¨‹ï¼šï¼ˆ1ï¼‰æŒç»­é¢„è®­ç»ƒï¼Œä»¥å°†äº‹å®ä¸æŒä¹…çš„æ–‡æ¡£æ ‡è¯†ç¬¦ç»‘å®šï¼›ï¼ˆ2ï¼‰æŒ‡ä»¤å¾®è°ƒä»¥æ¿€å‘å¼•ç”¨è¡Œä¸ºã€‚æˆ‘ä»¬å‘ç°ç®€å•çš„è¢«åŠ¨ç´¢å¼•ï¼ˆåªæ˜¯åœ¨æ¯ä¸ªæ–‡æ¡£åé™„åŠ ä¸€ä¸ªæ ‡è¯†ç¬¦ï¼‰æœ‰åŠ©äºè®°å¿†é€å­—æ–‡æœ¬ï¼Œä½†åœ¨å¤è¿°æˆ–ç»„åˆäº‹å®ä¸Šä¼šå¤±è´¥ã€‚å› æ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸»åŠ¨ç´¢å¼•ï¼Œå®ƒä¼šåœ¨åˆæˆé—®ç­”å¯¹ä¸ŠæŒç»­è¿›è¡Œé¢„è®­ç»ƒï¼Œè¿™äº›é—®ç­”å¯¹ï¼ˆ1ï¼‰ä»¥å¤šæ ·çš„ç»„åˆå½¢å¼é‡è¿°æ¯ä¸ªäº‹å®ï¼›ï¼ˆ2ï¼‰è¦æ±‚è¿›è¡ŒåŒå‘çš„æºåˆ°äº‹å®ã€äº‹å®åˆ°æºçš„ç”Ÿæˆï¼Œå…±åŒæ•™å¯¼æ¨¡å‹ä»å¼•ç”¨çš„æºç”Ÿæˆå†…å®¹å¹¶å½’å› å…¶ç­”æ¡ˆã€‚ä½¿ç”¨Qwen2.5-7Bå’Œ3Bçš„å®éªŒè¡¨æ˜ï¼Œä¸»åŠ¨ç´¢å¼•åœ¨æ‰€æœ‰ä»»åŠ¡å’Œæ¨¡å‹ä¸Šå§‹ç»ˆä¼˜äºè¢«åŠ¨ç´¢å¼•ï¼Œå¼•ç”¨ç²¾åº¦æé«˜é«˜è¾¾30.2%ã€‚æˆ‘ä»¬çš„æ¶ˆèç ”ç©¶è¡¨æ˜ï¼Œéšç€å¢åŠ æ‰©å……æ•°æ®é‡çš„è§„æ¨¡ï¼Œæ€§èƒ½ç»§ç»­æé«˜ï¼Œå³ä½¿åœ¨åŸå§‹æ ‡è®°è®¡æ•°çš„16å€æ—¶ï¼Œä¹Ÿæ˜¾ç¤ºå‡ºæ˜æ˜¾çš„ä¸Šå‡è¶‹åŠ¿ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.17585v1">PDF</a> </p>
<p><strong>æ‘˜è¦</strong></p>
<p>å¯ä¿¡çš„è¯­è¨€æ¨¡å‹åº”èƒ½æä¾›æ­£ç¡®ä¸”å¯éªŒè¯çš„ç­”æ¡ˆã€‚è¯­è¨€æ¨¡å‹æœ‰æ—¶ä¼šå°†å…¶è¾“å‡ºå½’å› äºé¢„è®­ç»ƒæ•°æ®ï¼Œä½†ç”±äºå¹»è§‰ï¼Œå…¶å¼•ç”¨å¾€å¾€ä¸å¯é ã€‚å› æ­¤ï¼Œå½“å‰ç³»ç»Ÿä¼šåœ¨æ¨ç†æ—¶æŸ¥è¯¢å¤–éƒ¨æ£€ç´¢å™¨æ¥æ’å…¥å¼•ç”¨ï¼Œè¿™å¼•å…¥äº†å»¶è¿Ÿã€å¯¹åŸºç¡€è®¾æ–½çš„ä¾èµ–ä»¥åŠæ˜“å—æ£€ç´¢å™ªå£°å½±å“çš„é—®é¢˜ã€‚æœ¬æ–‡æ¢ç´¢äº†é€šè¿‡ä¿®æ”¹è®­ç»ƒè¿‡ç¨‹ï¼Œæ˜¯å¦å¯ä»¥è®©LLMså¯é åœ°å¼•ç”¨åœ¨ï¼ˆæŒç»­ï¼‰é¢„è®­ç»ƒæœŸé—´æ‰€è§çš„æ–‡æ¡£ï¼Œè€Œæ— éœ€è¿›è¡Œæµ‹è¯•æ—¶æ£€ç´¢ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æ¨å‡ºäº†CitePretrainBenchåŸºå‡†æµ‹è¯•ï¼Œå®ƒå°†çœŸå®ä¸–ç•Œè¯­æ–™åº“ï¼ˆå¦‚Wikipediaã€Common Crawlã€arXivï¼‰ä¸æœªè§çš„æ–°æ–‡æ¡£æ··åˆï¼Œå¹¶æ¢è®¨äº†çŸ­å½¢å¼ï¼ˆå•äº‹å®ï¼‰å’Œé•¿å½¢å¼ï¼ˆå¤šäº‹å®ï¼‰çš„å¼•ç”¨ä»»åŠ¡ã€‚æˆ‘ä»¬çš„æ–¹æ³•éµå¾ªä¸¤ä¸ªé˜¶æ®µçš„è¿‡ç¨‹ï¼šï¼ˆ1ï¼‰æŒç»­é¢„è®­ç»ƒï¼Œå°†äº‹å®ç»‘å®šåˆ°æŒä¹…çš„æ–‡æ¡£æ ‡è¯†ç¬¦ä¸Šï¼›ï¼ˆ2ï¼‰æŒ‡ä»¤è°ƒæ•´ï¼Œä»¥æ¿€å‘å¼•ç”¨è¡Œä¸ºã€‚æˆ‘ä»¬å‘ç°ç®€å•çš„è¢«åŠ¨ç´¢å¼•ï¼ˆå³åœ¨æ¯ä¸ªæ–‡æ¡£åé™„åŠ ä¸€ä¸ªæ ‡è¯†ç¬¦ï¼‰æœ‰åŠ©äºè®°å¿†é€å­—æ–‡æœ¬ï¼Œä½†åœ¨è½¬è¿°æˆ–ç»„åˆäº‹å®ä¸Šä¼šå¤±è´¥ã€‚ç›¸åï¼Œæˆ‘ä»¬æå‡ºäº†ä¸»åŠ¨ç´¢å¼•ï¼Œå®ƒæŒç»­åœ°åœ¨åˆæˆçš„é—®ç­”å¯¹ä¸Šè¿›è¡Œé¢„è®­ç»ƒï¼Œè¿™äº›é—®ç­”å¯¹ï¼ˆ1ï¼‰ä»¥å¤šæ ·çš„ç»„åˆå½¢å¼é‡è¿°æ¯ä¸ªäº‹å®ï¼Œï¼ˆ2ï¼‰éœ€è¦åŒå‘çš„æºåˆ°äº‹å®å’Œäº‹å®åˆ°æºçš„ç”Ÿæˆï¼Œå…±åŒæ•™å¯¼æ¨¡å‹ä»å¼•ç”¨çš„æºç”Ÿæˆå†…å®¹å¹¶ä¸ºå…¶ç­”æ¡ˆæä¾›å±æ€§ã€‚ä½¿ç”¨Qwen2.5-7Bå’Œ3Bçš„å®éªŒè¡¨æ˜ï¼Œä¸»åŠ¨ç´¢å¼•åœ¨æ‰€æœ‰ä»»åŠ¡å’Œæ¨¡å‹ä¸Šçš„è¡¨ç°éƒ½ä¸€ç›´ä¼˜äºè¢«åŠ¨ç´¢å¼•ï¼Œå¼•ç”¨çš„ç²¾ç¡®åº¦æé«˜äº†é«˜è¾¾30.2%ã€‚æˆ‘ä»¬çš„æ¶ˆèç ”ç©¶è¿˜è¡¨æ˜ï¼Œéšç€æˆ‘ä»¬å¢åŠ æ‰©å……çš„æ•°æ®é‡ï¼Œæ€§èƒ½ä¼šç»§ç»­æé«˜ï¼Œå³ä½¿åœ¨åŸå§‹æ ‡è®°è®¡æ•°çš„16å€æ—¶ï¼Œä¹Ÿæ˜¾ç¤ºå‡ºæ˜æ˜¾çš„ä¸Šå‡è¶‹åŠ¿ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è¯­è¨€æ¨¡å‹åœ¨æä¾›å¼•ç”¨æ—¶å­˜åœ¨å¯é æ€§é—®é¢˜ï¼Œéœ€è¦æ”¹è¿›è®­ç»ƒè¿‡ç¨‹ä»¥æé«˜å¼•ç”¨çš„å‡†ç¡®æ€§ã€‚</li>
<li>æå‡ºäº†CitePretrainBenchåŸºå‡†æµ‹è¯•ï¼Œç”¨äºè¯„ä¼°è¯­è¨€æ¨¡å‹åœ¨å¼•ç”¨ä»»åŠ¡ä¸Šçš„æ€§èƒ½ã€‚</li>
<li>è¢«åŠ¨ç´¢å¼•æ–¹æ³•æœ‰åŠ©äºè®°å¿†é€å­—æ–‡æœ¬ï¼Œä½†åœ¨å¤„ç†è½¬è¿°æˆ–ç»„åˆäº‹å®æ—¶æ•ˆæœä¸ä½³ã€‚</li>
<li>ä¸»åŠ¨ç´¢å¼•æ–¹æ³•é€šè¿‡æŒç»­åœ¨åˆæˆçš„é—®ç­”å¯¹ä¸Šé¢„è®­ç»ƒï¼Œæé«˜è¯­è¨€æ¨¡å‹åœ¨å¼•ç”¨ä»»åŠ¡ä¸Šçš„æ€§èƒ½ã€‚</li>
<li>ä¸»åŠ¨ç´¢å¼•åœ¨æ‰€æœ‰ä»»åŠ¡å’Œæ¨¡å‹ä¸Šçš„è¡¨ç°å‡ä¼˜äºè¢«åŠ¨ç´¢å¼•ï¼Œå¼•ç”¨ç²¾ç¡®åº¦æœ‰æ˜¾è‘—æé«˜ã€‚</li>
<li>éšç€ä½¿ç”¨æ‰©å……çš„æ•°æ®é‡çš„å¢åŠ ï¼Œè¯­è¨€æ¨¡å‹åœ¨å¼•ç”¨ä»»åŠ¡ä¸Šçš„æ€§èƒ½ä¼šæŒç»­æé«˜ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.17585">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-b63ea645cffae3fd5093f29f04848696.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0d4db3d0dc24eba6f29d160a96ccc36a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-bece76ba9014a56de2003372729cede1.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="LLM-Driven-APT-Detection-for-6G-Wireless-Networks-A-Systematic-Review-and-Taxonomy"><a href="#LLM-Driven-APT-Detection-for-6G-Wireless-Networks-A-Systematic-Review-and-Taxonomy" class="headerlink" title="LLM-Driven APT Detection for 6G Wireless Networks: A Systematic Review   and Taxonomy"></a>LLM-Driven APT Detection for 6G Wireless Networks: A Systematic Review   and Taxonomy</h2><p><strong>Authors:Muhammed Golec, Yaser Khamayseh, Suhib Bani Melhem, Abdulmalik Alwarafy</strong></p>
<p>Sixth Generation (6G) wireless networks, which are expected to be deployed in the 2030s, have already created great excitement in academia and the private sector with their extremely high communication speed and low latency rates. However, despite the ultra-low latency, high throughput, and AI-assisted orchestration capabilities they promise, they are vulnerable to stealthy and long-term Advanced Persistent Threats (APTs). Large Language Models (LLMs) stand out as an ideal candidate to fill this gap with their high success in semantic reasoning and threat intelligence. In this paper, we present a comprehensive systematic review and taxonomy study for LLM-assisted APT detection in 6G networks. We address five research questions, namely, semantic merging of fragmented logs, encrypted traffic analysis, edge distribution constraints, dataset&#x2F;modeling techniques, and reproducibility trends, by leveraging most recent studies on the intersection of LLMs, APTs, and 6G wireless networks. We identify open challenges such as explainability gaps, data scarcity, edge hardware limitations, and the need for real-time slicing-aware adaptation by presenting various taxonomies such as granularity, deployment models, and kill chain stages. We then conclude the paper by providing several research gaps in 6G infrastructures for future researchers. To the best of our knowledge, this paper is the first comprehensive systematic review and classification study on LLM-based APT detection in 6G networks. </p>
<blockquote>
<p>å…³äºé¢„è®¡å°†äºæœ¬ä¸–çºª30å¹´ä»£éƒ¨ç½²çš„ç¬¬å…­ä»£ï¼ˆ6Gï¼‰æ— çº¿ç½‘ç»œï¼Œå­¦æœ¯ç•Œå’Œç§è¥éƒ¨é—¨å·²ç»ä¸ºä¹‹æ¿€åŠ¨ä¸å·²ã€‚è¿™ç§ç½‘ç»œå…·æœ‰æé«˜çš„é€šä¿¡é€Ÿåº¦å’Œæä½çš„å»¶è¿Ÿç‡ã€‚ç„¶è€Œï¼Œå°½ç®¡å®ƒä»¬å…·æœ‰è¶…ä½å»¶è¿Ÿã€é«˜ååé‡å’Œäººå·¥æ™ºèƒ½è¾…åŠ©ç¼–æ’èƒ½åŠ›ï¼Œä½†å®ƒä»¬ä»ç„¶å®¹æ˜“å—åˆ°éšè”½ä¸”é•¿æœŸå­˜åœ¨çš„æŒä¹…å¨èƒï¼ˆAPTsï¼‰ã€‚å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å‡­å€Ÿå…¶åœ¨è¯­ä¹‰æ¨ç†å’Œå¨èƒæƒ…æŠ¥æ–¹é¢çš„å‡ºè‰²è¡¨ç°ï¼Œæˆä¸ºäº†å¡«è¡¥è¿™ä¸€ç©ºç™½çš„ç†æƒ³é€‰æ‹©ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬å¯¹LLMè¾…åŠ©çš„APTæ£€æµ‹åœ¨6Gç½‘ç»œä¸­çš„ç ”ç©¶è¿›è¡Œäº†å…¨é¢çš„ç³»ç»Ÿå®¡æŸ¥å’Œåˆ†ç±»ç ”ç©¶ã€‚é€šè¿‡åˆ©ç”¨å…³äºLLMã€APTå’Œ6Gæ— çº¿ç½‘ç»œäº¤å‰é¢†åŸŸçš„æœ€æ–°ç ”ç©¶ï¼Œæˆ‘ä»¬å›ç­”äº†äº”ä¸ªç ”ç©¶é—®é¢˜ï¼Œå³ç¢ç‰‡åŒ–æ—¥å¿—çš„è¯­ä¹‰åˆå¹¶ã€åŠ å¯†æµé‡åˆ†æã€è¾¹ç¼˜åˆ†å¸ƒçº¦æŸã€æ•°æ®é›†&#x2F;å»ºæ¨¡æŠ€æœ¯å’Œå¯é‡å¤æ€§è¶‹åŠ¿ã€‚æˆ‘ä»¬ç¡®å®šäº†å¼€æ”¾æ€§çš„æŒ‘æˆ˜ï¼Œå¦‚è§£é‡Šæ€§å·®è·ã€æ•°æ®ç¨€ç¼ºã€è¾¹ç¼˜ç¡¬ä»¶é™åˆ¶å’Œå®æ—¶åˆ‡ç‰‡æ„ŸçŸ¥é€‚åº”çš„éœ€æ±‚ç­‰ï¼Œå¹¶æå‡ºäº†å„ç§åˆ†ç±»æ–¹æ³•ï¼Œå¦‚ç²’åº¦ã€éƒ¨ç½²æ¨¡å‹å’Œæ€ä¼¤é“¾é˜¶æ®µã€‚æœ€åï¼Œæœ¬æ–‡æ€»ç»“äº†æœªæ¥ç ”ç©¶äººå‘˜åœ¨6GåŸºç¡€è®¾æ–½ä¸­çš„ç ”ç©¶ç©ºç™½ã€‚æ®æˆ‘ä»¬æ‰€çŸ¥ï¼Œæœ¬æ–‡æ˜¯å…³äºåŸºäºLLMçš„APTæ£€æµ‹åœ¨6Gç½‘ç»œä¸­çš„é¦–ä¸ªå…¨é¢ç³»ç»Ÿå®¡æŸ¥å’Œåˆ†ç±»ç ”ç©¶ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.18846v2">PDF</a> 22 pages, 11 figures, 8 tables. Submitted to Computer Science Review   (Elsevier), May 2025</p>
<p><strong>æ‘˜è¦</strong><br>LLMæ¨¡å‹å¯å¡«è¡¥å½“å‰æŠ€æœ¯çš„ç¼ºé™·å¹¶åŠ å¼ºå®‰å…¨ç­–ç•¥éƒ¨ç½²äºåŠ¨æ€å¤šå˜çš„ç½‘ç»œç¯å¢ƒä¸­ã€‚æœ¬æ–‡å¯¹LLMè¾…åŠ©APTæ£€æµ‹åœ¨6Gç½‘ç»œä¸­çš„ç»¼åˆç³»ç»Ÿå®¡æŸ¥ä¸åˆ†ç±»è¿›è¡Œç ”ç©¶ï¼Œæ¢è®¨äº†äº”å¤§ç ”ç©¶é—®é¢˜ï¼Œå¹¶æŒ‡å‡ºäº†å¤šé¡¹å¼€æ”¾æŒ‘æˆ˜å’Œæœªæ¥çš„ç ”ç©¶ç©ºç™½ã€‚è¿™ä¸€ç ”ç©¶çš„ç›®çš„æ˜¯ä¸ºäº†å¼€åˆ›æ›´ç¨³å¥ä¸”å¯æŒç»­çš„ç½‘ç»œæŠ€æœ¯æ—¶ä»£ï¼Œå¼ºåŒ–APTæ”»å‡»çš„å¨èƒç®¡ç†å¹¶å‡å°‘å…¶å¯¹ç³»ç»Ÿçš„å¨èƒå½±å“ã€‚æ­¤é¡¹ç ”ç©¶æå…·åˆ›æ–°æ€§å’Œå®ç”¨æ€§ä»·å€¼ã€‚é€šè¿‡æœ¬ç ”ç©¶å¯ä¸ºæœªæ¥ç ”ç©¶äººå‘˜åœ¨ç›¸å…³æŠ€æœ¯çš„ç ”ç©¶ä¸­æä¾›æœ‰ç›Šçš„å‚è€ƒã€‚è¯¥è®ºæ–‡å¡«è¡¥äº†ç›¸å…³é¢†åŸŸç ”ç©¶çš„ç©ºç™½ï¼Œæ˜¯ç›®å‰å¯¹LLMåœ¨æ£€æµ‹åŸºäºAPTå¨èƒä¸­çš„é¦–æ¬¡ç»¼åˆç³»ç»Ÿå®¡æŸ¥ä¸åˆ†ç±»ç ”ç©¶ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ul>
<li>LLMæ¨¡å‹åœ¨è¯­ä¹‰æ¨ç†å’Œå¨èƒæƒ…æŠ¥æ–¹é¢è¡¨ç°å‡ºå“è¶Šçš„èƒ½åŠ›ï¼Œæˆä¸ºåº”å¯¹APTæ”»å‡»çš„ç†æƒ³é€‰æ‹©ã€‚è¿™äº›æ¨¡å‹å¯¹äºæé«˜6Gç½‘ç»œçš„å®‰å…¨æ€§è‡³å…³é‡è¦ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.18846">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-aa2c247b258e03fe3a47c73c20452631.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-8d437ec68505e715048e0ed3ff632d81.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f0d42621287112aa3439856ee7bdbbc1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2bfd61f36fd8c3b9e02ace13a6f8b326.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-25a56cb50c51407864591a3c184cfa58.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b95f85c6a71828f531458456bf4b7c67.jpg" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="InstructAttribute-Fine-grained-Object-Attributes-editing-with-Instruction"><a href="#InstructAttribute-Fine-grained-Object-Attributes-editing-with-Instruction" class="headerlink" title="InstructAttribute: Fine-grained Object Attributes editing with   Instruction"></a>InstructAttribute: Fine-grained Object Attributes editing with   Instruction</h2><p><strong>Authors:Xingxi Yin, Jingfeng Zhang, Yue Deng, Zhi Li, Yicheng Li, Yin Zhang</strong></p>
<p>Text-to-image (T2I) diffusion models are widely used in image editing due to their powerful generative capabilities. However, achieving fine-grained control over specific object attributes, such as color and material, remains a considerable challenge. Existing methods often fail to accurately modify these attributes or compromise structural integrity and overall image consistency. To fill this gap, we introduce Structure Preservation and Attribute Amplification (SPAA), a novel training-free framework that enables precise generation of color and material attributes for the same object by intelligently manipulating self-attention maps and cross-attention values within diffusion models. Building on SPAA, we integrate multi-modal large language models (MLLMs) to automate data curation and instruction generation. Leveraging this object attribute data collection engine, we construct the Attribute Dataset, encompassing a comprehensive range of colors and materials across diverse object categories. Using this generated dataset, we propose InstructAttribute, an instruction-tuned model that enables fine-grained and object-level attribute editing through natural language prompts. This capability holds significant practical implications for diverse fields, from accelerating product design and e-commerce visualization to enhancing virtual try-on experiences. Extensive experiments demonstrate that InstructAttribute outperforms existing instruction-based baselines, achieving a superior balance between attribute modification accuracy and structural preservation. </p>
<blockquote>
<p>æ–‡æœ¬åˆ°å›¾åƒï¼ˆT2Iï¼‰æ‰©æ•£æ¨¡å‹å› å…¶å¼ºå¤§çš„ç”Ÿæˆèƒ½åŠ›è€Œå¹¿æ³›åº”ç”¨äºå›¾åƒç¼–è¾‘ã€‚ç„¶è€Œï¼Œå®ç°å¯¹ç‰¹å®šå¯¹è±¡å±æ€§ï¼ˆå¦‚é¢œè‰²å’Œæè´¨ï¼‰çš„ç²¾ç»†æ§åˆ¶ä»ç„¶å­˜åœ¨ç›¸å½“å¤§çš„æŒ‘æˆ˜ã€‚ç°æœ‰æ–¹æ³•å¾€å¾€æ— æ³•å‡†ç¡®ä¿®æ”¹è¿™äº›å±æ€§ï¼Œæˆ–è€…ä¼šæŸå®³ç»“æ„å®Œæ•´æ€§å’Œæ•´ä½“å›¾åƒä¸€è‡´æ€§ã€‚ä¸ºäº†å¡«è¡¥è¿™ä¸€ç©ºç™½ï¼Œæˆ‘ä»¬å¼•å…¥äº†æ— è®­ç»ƒæ¡†æ¶â€”â€”ç»“æ„ä¿ç•™ä¸å±æ€§æ”¾å¤§ï¼ˆSPAAï¼‰ï¼Œé€šè¿‡æ™ºèƒ½æ“ä½œæ‰©æ•£æ¨¡å‹ä¸­çš„è‡ªæ³¨æ„åœ°å›¾å’Œè·¨æ³¨æ„å€¼ï¼Œå®ç°å¯¹åŒä¸€å¯¹è±¡çš„é¢œè‰²å’Œæè´¨å±æ€§çš„ç²¾ç¡®ç”Ÿæˆã€‚åŸºäºSPAAï¼Œæˆ‘ä»¬é›†æˆäº†å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰ï¼Œä»¥è‡ªåŠ¨åŒ–æ•°æ®æ•´ç†å’ŒæŒ‡ä»¤ç”Ÿæˆã€‚åˆ©ç”¨è¿™ä¸€å¯¹è±¡å±æ€§æ•°æ®é‡‡é›†å¼•æ“ï¼Œæˆ‘ä»¬æ„å»ºäº†å±æ€§æ•°æ®é›†ï¼Œæ¶µç›–äº†å„ç±»å¯¹è±¡ä¸­å¹¿æ³›çš„é¢œè‰²ä¸æè´¨ã€‚ä½¿ç”¨è¿™ä¸ªç”Ÿæˆçš„æ•°æ®é›†ï¼Œæˆ‘ä»¬æ¨å‡ºäº†æŒ‡ä»¤è°ƒæ•´æ¨¡å‹â€”â€”InstructAttributeï¼Œé€šè¿‡è‡ªç„¶è¯­è¨€æç¤ºå®ç°ç²¾ç»†å’Œå¯¹è±¡çº§åˆ«çš„å±æ€§ç¼–è¾‘ã€‚è¿™ä¸€èƒ½åŠ›åœ¨å¤šä¸ªé¢†åŸŸå…·æœ‰å®é™…åº”ç”¨ä»·å€¼ï¼Œå¦‚åŠ é€Ÿäº§å“è®¾è®¡ã€ç”µå­å•†åŠ¡å¯è§†åŒ–ä»¥åŠæå‡è™šæ‹Ÿè¯•ç©¿ä½“éªŒç­‰ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼ŒInstructAttributeåœ¨å±æ€§ä¿®æ”¹ç²¾åº¦å’Œç»“æ„ä¿ç•™ä¹‹é—´è¾¾åˆ°äº†å‡ºè‰²çš„å¹³è¡¡ï¼Œè¶…è¶Šäº†ç°æœ‰çš„æŒ‡ä»¤åŸºå‡†æµ‹è¯•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.00751v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æ–‡æœ¬ä»‹ç»äº†é’ˆå¯¹æ–‡æœ¬åˆ°å›¾åƒï¼ˆT2Iï¼‰æ‰©æ•£æ¨¡å‹çš„æ–°æ¡†æ¶Structure Preservation and Attribute Amplificationï¼ˆSPAAï¼‰ã€‚è¯¥æ¡†æ¶æ— éœ€è®­ç»ƒå³å¯å®ç°å¯¹åŒä¸€å¯¹è±¡çš„é¢œè‰²å’Œææ–™ç­‰å±æ€§çš„ç²¾ç¡®ç”Ÿæˆï¼Œé€šè¿‡æ™ºèƒ½æ“ä½œæ‰©æ•£æ¨¡å‹ä¸­çš„è‡ªæ³¨æ„åŠ›å›¾å’Œè·¨æ³¨æ„åŠ›å€¼æ¥å®ç°ã€‚åŸºäºSPAAï¼Œé›†æˆäº†å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰ä»¥è‡ªåŠ¨åŒ–æ•°æ®æ•´ç†å’ŒæŒ‡ä»¤ç”Ÿæˆã€‚åˆ©ç”¨å¯¹è±¡å±æ€§æ•°æ®æ”¶é›†å¼•æ“ï¼Œæ„å»ºäº†åŒ…å«å„ç§å¯¹è±¡ç±»åˆ«çš„é¢œè‰²å’Œææ–™çš„Attributeæ•°æ®é›†ã€‚ä½¿ç”¨ç”Ÿæˆçš„æ•°æ®é›†ï¼Œæå‡ºäº†åŸºäºæŒ‡ä»¤çš„InstructAttributeæ¨¡å‹ï¼Œé€šè¿‡è‡ªç„¶è¯­è¨€æç¤ºå®ç°ç²¾ç»†å’Œå¯¹è±¡çº§çš„å±æ€§ç¼–è¾‘ã€‚è¯¥èƒ½åŠ›å¯¹äºåŠ é€Ÿäº§å“è®¾è®¡ã€ç”µå­å•†åŠ¡å¯è§†åŒ–ä»¥åŠå¢å¼ºè™šæ‹Ÿè¯•ç©¿ä½“éªŒç­‰é¢†åŸŸå…·æœ‰å®é™…æ„ä¹‰ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ–‡æœ¬åˆ°å›¾åƒï¼ˆT2Iï¼‰æ‰©æ•£æ¨¡å‹å¹¿æ³›åº”ç”¨äºå›¾åƒç¼–è¾‘ï¼Œä½†åœ¨æ§åˆ¶ç‰¹å®šå¯¹è±¡å±æ€§æ–¹é¢å­˜åœ¨æŒ‘æˆ˜ã€‚</li>
<li>ç°æœ‰æ–¹æ³•å¸¸å¸¸æ— æ³•å‡†ç¡®ä¿®æ”¹å±æ€§æˆ–ä¼šæŸå®³å›¾åƒçš„ç»“æ„å®Œæ•´æ€§å’Œä¸€è‡´æ€§ã€‚</li>
<li>SPAAæ¡†æ¶é€šè¿‡æ™ºèƒ½æ“ä½œè‡ªæ³¨æ„åŠ›å›¾å’Œè·¨æ³¨æ„åŠ›å€¼ï¼Œå®ç°åŒä¸€å¯¹è±¡çš„é¢œè‰²å’Œææ–™ç­‰å±æ€§çš„ç²¾ç¡®ç”Ÿæˆï¼Œä¸”æ— éœ€è®­ç»ƒã€‚</li>
<li>ç»“åˆå¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰ï¼Œè‡ªåŠ¨åŒ–æ•°æ®æ•´ç†å’ŒæŒ‡ä»¤ç”Ÿæˆã€‚</li>
<li>åˆ©ç”¨å¯¹è±¡å±æ€§æ•°æ®æ”¶é›†å¼•æ“ï¼Œæ„å»ºäº†Attributeæ•°æ®é›†ï¼ŒåŒ…å«å„ç§å¯¹è±¡ç±»åˆ«çš„é¢œè‰²å’Œææ–™ã€‚</li>
<li>æå‡ºåŸºäºæŒ‡ä»¤çš„InstructAttributeæ¨¡å‹ï¼Œé€šè¿‡è‡ªç„¶è¯­è¨€æç¤ºå®ç°ç²¾ç»†å’Œå¯¹è±¡çº§çš„å±æ€§ç¼–è¾‘ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.00751">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-d71edee45541a3c5266dc5b93d026f26.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b955e4ee4e9e6adc35cce6d758e83a4d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-269319fa013775b4924b05fa8ed39030.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-57605603ab8bebde709309f0c1dd4742.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f5cef4073e32a7d768575249995abf59.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-beae8194358085e26d4d8c3ff9f26b7d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5b06582b7db4887a343daf19d43cffd4.jpg" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="Context-Aware-Human-Behavior-Prediction-Using-Multimodal-Large-Language-Models-Challenges-and-Insights"><a href="#Context-Aware-Human-Behavior-Prediction-Using-Multimodal-Large-Language-Models-Challenges-and-Insights" class="headerlink" title="Context-Aware Human Behavior Prediction Using Multimodal Large Language   Models: Challenges and Insights"></a>Context-Aware Human Behavior Prediction Using Multimodal Large Language   Models: Challenges and Insights</h2><p><strong>Authors:Yuchen Liu, Lino Lerch, Luigi Palmieri, Andrey Rudenko, Sebastian Koch, Timo Ropinski, Marco Aiello</strong></p>
<p>Predicting human behavior in shared environments is crucial for safe and efficient human-robot interaction. Traditional data-driven methods to that end are pre-trained on domain-specific datasets, activity types, and prediction horizons. In contrast, the recent breakthroughs in Large Language Models (LLMs) promise open-ended cross-domain generalization to describe various human activities and make predictions in any context. In particular, Multimodal LLMs (MLLMs) are able to integrate information from various sources, achieving more contextual awareness and improved scene understanding. The difficulty in applying general-purpose MLLMs directly for prediction stems from their limited capacity for processing large input sequences, sensitivity to prompt design, and expensive fine-tuning. In this paper, we present a systematic analysis of applying pre-trained MLLMs for context-aware human behavior prediction. To this end, we introduce a modular multimodal human activity prediction framework that allows us to benchmark various MLLMs, input variations, In-Context Learning (ICL), and autoregressive techniques. Our evaluation indicates that the best-performing framework configuration is able to reach 92.8% semantic similarity and 66.1% exact label accuracy in predicting human behaviors in the target frame. </p>
<blockquote>
<p>é¢„æµ‹å…±äº«ç¯å¢ƒä¸­çš„äººç±»è¡Œä¸ºå¯¹äºå®‰å…¨é«˜æ•ˆçš„äººæœºäº¤äº’è‡³å…³é‡è¦ã€‚ä¼ ç»Ÿçš„ä»¥æ•°æ®é©±åŠ¨çš„æ–¹æ³•ä¸ºæ­¤ç›®çš„è€Œåœ¨ç‰¹å®šé¢†åŸŸæ•°æ®é›†ã€æ´»åŠ¨ç±»å‹å’Œé¢„æµ‹èŒƒå›´ä¸Šè¿›è¡Œé¢„è®­ç»ƒã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼Œæœ€è¿‘å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„çªç ´æœ‰æœ›å®ç°å¯¹å„ç§äººç±»æ´»åŠ¨çš„å¼€æ”¾å¼è·¨åŸŸæè¿°å’Œä»»ä½•ä¸Šä¸‹æ–‡ä¸­çš„é¢„æµ‹ã€‚ç‰¹åˆ«æ˜¯å¤šæ¨¡æ€LLMï¼ˆMLLMï¼‰èƒ½å¤Ÿæ•´åˆæ¥è‡ªå„ç§æ¥æºçš„ä¿¡æ¯ï¼Œå®ç°æ›´ä¸Šä¸‹æ–‡æ„ŸçŸ¥å’Œæ”¹è¿›çš„åœºæ™¯ç†è§£ã€‚å°†é€šç”¨MLLMç›´æ¥åº”ç”¨äºé¢„æµ‹çš„å›°éš¾æºäºå…¶å¤„ç†å¤§è¾“å…¥åºåˆ—çš„èƒ½åŠ›æœ‰é™ã€å¯¹æç¤ºè®¾è®¡çš„æ•æ„Ÿæ€§ä»¥åŠæ˜‚è´µçš„å¾®è°ƒã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬å¯¹å°†é¢„è®­ç»ƒçš„MLLMåº”ç”¨äºä¸Šä¸‹æ–‡æ„ŸçŸ¥çš„äººç±»è¡Œä¸ºé¢„æµ‹è¿›è¡Œäº†ç³»ç»Ÿåˆ†æã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ªæ¨¡å—åŒ–å¤šæ¨¡æ€äººç±»æ´»åŠ¨é¢„æµ‹æ¡†æ¶ï¼Œè¯¥æ¡†æ¶ä½¿æˆ‘ä»¬èƒ½å¤Ÿè¯„ä¼°å„ç§MLLMã€è¾“å…¥å˜åŒ–ã€ä¸Šä¸‹æ–‡å­¦ä¹ ï¼ˆICLï¼‰å’Œè‡ªå›å½’æŠ€æœ¯ã€‚æˆ‘ä»¬çš„è¯„ä¼°è¡¨æ˜ï¼Œè¡¨ç°æœ€ä½³çš„æ¡†æ¶é…ç½®åœ¨é¢„æµ‹ç›®æ ‡æ¡†æ¶ä¸­çš„äººç±»è¡Œä¸ºæ—¶ï¼Œèƒ½å¤Ÿè¾¾åˆ°92.8%çš„è¯­ä¹‰ç›¸ä¼¼åº¦å’Œ66.1%çš„æ ‡ç­¾å‡†ç¡®æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.00839v2">PDF</a> Accepted at IEEE International Conference on Robot and Human   Interactive Communication (RO-MAN), 2025</p>
<p><strong>Summary</strong></p>
<p>åŸºäºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„å¤šæ¨¡æ€æ–¹æ³•åœ¨é¢„æµ‹äººç±»è¡Œä¸ºæ–¹é¢å±•ç°å‡ºå·¨å¤§æ½œåŠ›ã€‚ä¼ ç»Ÿçš„æ•°æ®é©±åŠ¨æ–¹æ³•é€šå¸¸å±€é™äºç‰¹å®šé¢†åŸŸçš„æ•°æ®é›†å’Œæ´»åŠ¨ç±»å‹é¢„æµ‹ã€‚ç„¶è€Œï¼ŒLLMçš„å¤šæ¨¡æ€èƒ½åŠ›å¯ä»¥å®ç°è·¨é¢†åŸŸæè¿°å„ç§äººç±»æ´»åŠ¨å¹¶è¿›è¡Œé¢„æµ‹ã€‚å°½ç®¡å­˜åœ¨å¤„ç†å¤§è¾“å…¥åºåˆ—çš„å±€é™æ€§ä»¥åŠå¯¹æç¤ºè®¾è®¡å’Œç²¾ç»†è°ƒæ•´çš„æ•æ„Ÿæ€§ç­‰é—®é¢˜ï¼Œä½†æœ¬æ–‡æå‡ºäº†ä¸€ç§æ¨¡å—åŒ–å¤šæ¨¡æ€äººç±»æ´»åŠ¨é¢„æµ‹æ¡†æ¶ï¼Œè¯¥æ¡†æ¶å…è®¸å¯¹ä¸åŒçš„LLMã€è¾“å…¥å˜åŒ–ã€ä¸Šä¸‹æ–‡å­¦ä¹ å’Œè‡ªå›å½’æŠ€æœ¯è¿›è¡Œè¯„ä¼°ã€‚æœ€ä½³é…ç½®çš„æ¡†æ¶èƒ½å¤Ÿè¾¾åˆ°ç›®æ ‡å¸§çš„é¢„æµ‹è¡Œä¸ºè¯­ä¹‰ç›¸ä¼¼æ€§è¾¾åˆ°92.8%ï¼Œç²¾ç¡®æ ‡ç­¾å‡†ç¡®ç‡è¾¾åˆ°äº†66.1%ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨é¢„æµ‹äººç±»è¡Œä¸ºæ–¹é¢å±•ç°å‡ºæ½œåŠ›ã€‚</li>
<li>ä¼ ç»Ÿæ•°æ®é©±åŠ¨æ–¹æ³•å—é™äºç‰¹å®šé¢†åŸŸï¼Œè€ŒLLMå¯å®ç°è·¨é¢†åŸŸæè¿°å’Œé¢„æµ‹ã€‚</li>
<li>å¤šæ¨¡æ€LLMï¼ˆMLLMï¼‰èƒ½æ•´åˆä¸åŒæ¥æºçš„ä¿¡æ¯ï¼Œæå‡ä¸Šä¸‹æ–‡æ„ŸçŸ¥å’Œåœºæ™¯ç†è§£ã€‚</li>
<li>åº”ç”¨MLLMsè¿›è¡Œé¢„æµ‹å­˜åœ¨å¤„ç†å¤§è¾“å…¥åºåˆ—çš„å±€é™æ€§åŠå¯¹æç¤ºè®¾è®¡å’Œç²¾ç»†è°ƒæ•´çš„æ•æ„Ÿæ€§ã€‚</li>
<li>è®ºæ–‡æå‡ºäº†ä¸€ç§æ¨¡å—åŒ–å¤šæ¨¡æ€äººç±»æ´»åŠ¨é¢„æµ‹æ¡†æ¶ï¼Œå¯ä»¥è¯„ä¼°ä¸åŒçš„MLLMsã€è¾“å…¥å˜åŒ–ç­‰ã€‚</li>
<li>æœ€ä½³é…ç½®çš„æ¡†æ¶åœ¨é¢„æµ‹äººç±»è¡Œä¸ºæ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œè¯­ä¹‰ç›¸ä¼¼æ€§å’Œç²¾ç¡®æ ‡ç­¾å‡†ç¡®ç‡åˆ†åˆ«è¾¾åˆ°äº†92.8%å’Œ66.1%ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.00839">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-7c0940b1c649514a7c736f745117d242.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c4ed4c347ab6056f930a656b12cf1203.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c797e2f930eb90d2b91478fb4dd89d3c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d5aeadc636221bd6f9a914d47ef9195a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-26abf8405866c3f12b1c6a19aa77cc59.jpg" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1><h2 id="Talking-to-GDELT-Through-Knowledge-Graphs"><a href="#Talking-to-GDELT-Through-Knowledge-Graphs" class="headerlink" title="Talking to GDELT Through Knowledge Graphs"></a>Talking to GDELT Through Knowledge Graphs</h2><p><strong>Authors:Audun Myers, Max Vargas, Sinan G. Aksoy, Cliff Joslyn, Benjamin Wilson, Lee Burke, Tom Grimes</strong></p>
<p>In this work we study various Retrieval Augmented Regeneration (RAG) approaches to gain an understanding of the strengths and weaknesses of each approach in a question-answering analysis. To gain this understanding we use a case-study subset of the Global Database of Events, Language, and Tone (GDELT) dataset as well as a corpus of raw text scraped from the online news articles. To retrieve information from the text corpus we implement a traditional vector store RAG as well as state-of-the-art large language model (LLM) based approaches for automatically constructing KGs and retrieving the relevant subgraphs. In addition to these corpus approaches, we develop a novel ontology-based framework for constructing knowledge graphs (KGs) from GDELT directly which leverages the underlying schema of GDELT to create structured representations of global events. For retrieving relevant information from the ontology-based KGs we implement both direct graph queries and state-of-the-art graph retrieval approaches. We compare the performance of each method in a question-answering task. We find that while our ontology-based KGs are valuable for question-answering, automated extraction of the relevant subgraphs is challenging. Conversely, LLM-generated KGs, while capturing event summaries, often lack consistency and interpretability. Our findings suggest benefits of a synergistic approach between ontology and LLM-based KG construction, with proposed avenues toward that end. </p>
<blockquote>
<p>åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬ç ”ç©¶äº†å„ç§åŸºäºæ£€ç´¢å¢å¼ºçš„å†ç”Ÿï¼ˆRAGï¼‰æ–¹æ³•ï¼Œä»¥äº†è§£æ¯ç§æ–¹æ³•åœ¨å¤„ç†é—®ç­”åˆ†ææ—¶çš„ä¼˜ç¼ºç‚¹ã€‚ä¸ºäº†æ·±å…¥äº†è§£ï¼Œæˆ‘ä»¬ä½¿ç”¨äº†å…¨çƒäº‹ä»¶ã€è¯­è¨€å’Œè¯­è°ƒæ•°æ®åº“çš„æ¡ˆä¾‹ç ”ç©¶å­é›†ä»¥åŠä¸ç½‘ç»œæ–°é—»æŠ¥é“ä¸€åŒæ”¶é›†çš„ç”Ÿè¯­æ–™åº“æ–‡æœ¬æ•°æ®ã€‚ä¸ºäº†ä»æ–‡æœ¬è¯­æ–™åº“ä¸­æ£€ç´¢ä¿¡æ¯ï¼Œæˆ‘ä»¬å®æ–½äº†ä¸€ç§ä¼ ç»Ÿçš„å‘é‡å­˜å‚¨RAGä»¥åŠåŸºäºæœ€æ–°å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ–¹æ³•ï¼Œè¿™äº›æ–¹æ³•å¯ä»¥è‡ªåŠ¨æ„å»ºçŸ¥è¯†å›¾è°±å¹¶æ£€ç´¢ç›¸å…³çš„å­å›¾ã€‚é™¤äº†è¿™äº›è¯­æ–™åº“æ–¹æ³•å¤–ï¼Œæˆ‘ä»¬è¿˜å¼€å‘äº†ä¸€ç§åŸºäºæœ¬ä½“æ„å»ºçŸ¥è¯†å›¾è°±ï¼ˆKGsï¼‰çš„æ–°æ¡†æ¶ï¼Œè¯¥æ¡†æ¶ç›´æ¥ä»GDELTä¸­æå–ä¿¡æ¯å¹¶åˆ©ç”¨å…¶åº•å±‚æ¶æ„æ¥åˆ›å»ºå…¨çƒäº‹ä»¶çš„ç»“æ„åŒ–è¡¨ç¤ºã€‚ä¸ºäº†ä»åŸºäºæœ¬ä½“çš„çŸ¥è¯†å›¾è°±ä¸­æ£€ç´¢ç›¸å…³ä¿¡æ¯ï¼Œæˆ‘ä»¬å®æ–½äº†ç›´æ¥å›¾å½¢æŸ¥è¯¢å’Œæœ€æ–°çš„å›¾å½¢æ£€ç´¢æ–¹æ³•ã€‚æˆ‘ä»¬åœ¨é—®ç­”ä»»åŠ¡ä¸­æ¯”è¾ƒäº†æ¯ç§æ–¹æ³•çš„æ€§èƒ½ã€‚æˆ‘ä»¬å‘ç°ï¼Œè™½ç„¶åŸºäºæœ¬ä½“çš„çŸ¥è¯†å›¾è°±å¯¹äºé—®ç­”éå¸¸æœ‰ä»·å€¼ï¼Œä½†è‡ªåŠ¨æå–ç›¸å…³å­å›¾å´å…·æœ‰æŒ‘æˆ˜æ€§ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼Œè™½ç„¶LLMç”Ÿæˆçš„çŸ¥è¯†å›¾è°±èƒ½å¤Ÿæ•æ‰äº‹ä»¶æ‘˜è¦ï¼Œä½†å®ƒä»¬å¾€å¾€ç¼ºä¹ä¸€è‡´æ€§å’Œå¯è§£é‡Šæ€§ã€‚æˆ‘ä»¬çš„ç ”ç©¶ç»“æœè¡¨æ˜äº†æœ¬ä½“å’ŒåŸºäºLLMçš„çŸ¥è¯†å›¾è°±æ„å»ºä¹‹é—´çš„ååŒæ–¹æ³•çš„ä¼˜åŠ¿ï¼Œå¹¶ä¸ºæ­¤æå‡ºäº†å¯èƒ½çš„é€”å¾„ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.07584v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ç ”ç©¶äº†å¤šç§åŸºäºçŸ¥è¯†å›¾è°±çš„æ£€ç´¢å¢å¼ºå†ç”Ÿï¼ˆRAGï¼‰æ–¹æ³•ï¼Œä»¥äº†è§£å…¶åœ¨é—®ç­”åˆ†æä¸­çš„ä¼˜ç¼ºç‚¹ã€‚ç ”ç©¶ä½¿ç”¨äº†å…¨çƒäº‹ä»¶ã€è¯­è¨€å’Œæƒ…æ„Ÿæ•°æ®åº“çš„å­é›†å’Œåœ¨çº¿æ–°é—»æ–‡ç« çš„è¯­æ–™åº“ä½œä¸ºå®éªŒæ•°æ®é›†ã€‚å®éªŒæ¯”è¾ƒäº†åŸºäºä¼ ç»Ÿå‘é‡å­˜å‚¨ã€å¤§å‹è¯­è¨€æ¨¡å‹è‡ªåŠ¨æ„å»ºçŸ¥è¯†å›¾è°±çš„æ–¹æ³•ä»¥åŠæ–°å‹åŸºäºæœ¬ä½“æ„å»ºçŸ¥è¯†å›¾è°±çš„æ–¹æ³•ã€‚ç ”ç©¶å‘ç°åœ¨é—®ç­”ä»»åŠ¡ä¸­ï¼ŒåŸºäºæœ¬ä½“çš„çŸ¥è¯†å›¾è°±å¯¹äºå›ç­”é—®é¢˜å¾ˆæœ‰ä»·å€¼ï¼Œä½†è‡ªåŠ¨æå–ç›¸å…³å­å›¾å…·æœ‰æŒ‘æˆ˜æ€§ï¼›è€ŒåŸºäºå¤§å‹è¯­è¨€æ¨¡å‹æ„å»ºçš„çŸ¥è¯†å›¾è°±è™½ç„¶èƒ½å¤Ÿæ•æ‰äº‹ä»¶æ‘˜è¦ï¼Œä½†å¾€å¾€ç¼ºä¹ä¸€è‡´æ€§å’Œå¯è§£é‡Šæ€§ã€‚å› æ­¤ï¼Œç ”ç©¶æå‡ºäº†æœ¬ä½“ä¸å¤§å‹è¯­è¨€æ¨¡å‹ç›¸ç»“åˆæ„å»ºçŸ¥è¯†å›¾è°±çš„ååŒæ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>ç ”ç©¶é‡‡ç”¨å¤šç§æ–¹æ³•æ„å»ºçŸ¥è¯†å›¾è°±å¹¶è¿›è¡Œå¯¹æ¯”è¯„ä¼°ã€‚</li>
<li>é‡‡ç”¨ä¼ ç»Ÿå‘é‡å­˜å‚¨æ–¹æ³•ä»¥åŠå…ˆè¿›çš„å¤§å‹è¯­è¨€æ¨¡å‹è¿›è¡Œæ–‡æœ¬ä¿¡æ¯æ£€ç´¢ã€‚</li>
<li>æå‡ºæ–°å‹åŸºäºæœ¬ä½“æ„å»ºçŸ¥è¯†å›¾è°±çš„æ–¹æ³•ï¼Œåˆ©ç”¨å…¨çƒäº‹ä»¶æ•°æ®åº“çš„ç»“æ„åŒ–æ•°æ®ã€‚</li>
<li>åŸºäºæœ¬ä½“æ„å»ºçš„çŸ¥è¯†å›¾è°±åœ¨é—®ç­”ä»»åŠ¡ä¸­è¡¨ç°å‡ºä»·å€¼ï¼Œä½†è‡ªåŠ¨æå–ç›¸å…³å­å›¾å…·æœ‰æŒ‘æˆ˜æ€§ã€‚</li>
<li>åŸºäºå¤§å‹è¯­è¨€æ¨¡å‹æ„å»ºçš„çŸ¥è¯†å›¾è°±èƒ½å¤Ÿæ•æ‰äº‹ä»¶æ‘˜è¦ï¼Œä½†ç¼ºä¹ä¸€è‡´æ€§å’Œå¯è§£é‡Šæ€§ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.07584">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-543da7e4810e02bbf8f19bcfe8a1eb6c.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-d42577c0c4c8ea2870eb55585bfd9fe2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-806b50bbafc70883f2222147b6a435d9.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f2f97c913999000bc1074598e271f675.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-0fd0df0cf7d1459cbf09f0e84aa042be.jpg" align="middle">
</details>


<h1 id="-19"><a href="#-19" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-06-25/LLM/">https://kedreamix.github.io/Talk2Paper/Paper/2025-06-25/LLM/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/LLM/">
                                    <span class="chip bg-color">LLM</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-06-25/Agent/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-52b9af6db973294039e24f9bc0227e29.jpg" class="responsive-img" alt="Agent">
                        
                        <span class="card-title">Agent</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Agent æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-06-25  Audit & Repair An Agentic Framework for Consistent Story Visualization   in Text-to-Image Diffusion Models
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-06-25
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Agent/" class="post-category">
                                    Agent
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Agent/">
                        <span class="chip bg-color">Agent</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-06-25/R1_Reasoning/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-2cf24f70b89b4a2c22b43150cb9ce55c.jpg" class="responsive-img" alt="R1_Reasoning">
                        
                        <span class="card-title">R1_Reasoning</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            R1_Reasoning æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-06-25  ReasonFlux-PRM Trajectory-Aware PRMs for Long Chain-of-Thought   Reasoning in LLMs
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-06-25
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/R1-Reasoning/" class="post-category">
                                    R1_Reasoning
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/R1-Reasoning/">
                        <span class="chip bg-color">R1_Reasoning</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">32127.8k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
