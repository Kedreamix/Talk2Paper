<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Diffusion Models">
    <meta name="description" content="Diffusion Models æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-06-25  DIP Unsupervised Dense In-Context Post-training of Visual   Representations">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Diffusion Models | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pic1.zhimg.com/v2-cd51a0c4d9d03867bc364d3da271f7b6.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Diffusion Models</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Diffusion-Models/">
                                <span class="chip bg-color">Diffusion Models</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Diffusion-Models/" class="post-category">
                                Diffusion Models
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-06-25
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-07-06
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    20k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    81 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-06-25-æ›´æ–°"><a href="#2025-06-25-æ›´æ–°" class="headerlink" title="2025-06-25 æ›´æ–°"></a>2025-06-25 æ›´æ–°</h1><h2 id="DIP-Unsupervised-Dense-In-Context-Post-training-of-Visual-Representations"><a href="#DIP-Unsupervised-Dense-In-Context-Post-training-of-Visual-Representations" class="headerlink" title="DIP: Unsupervised Dense In-Context Post-training of Visual   Representations"></a>DIP: Unsupervised Dense In-Context Post-training of Visual   Representations</h2><p><strong>Authors:Sophia Sirko-Galouchenko, Spyros Gidaris, Antonin Vobecky, Andrei Bursuc, Nicolas Thome</strong></p>
<p>We introduce DIP, a novel unsupervised post-training method designed to enhance dense image representations in large-scale pretrained vision encoders for in-context scene understanding. Unlike prior approaches that rely on complex self-distillation architectures, our method trains the vision encoder using pseudo-tasks that explicitly simulate downstream in-context scenarios, inspired by meta-learning principles. To enable post-training on unlabeled data, we propose an automatic mechanism for generating in-context tasks that combines a pretrained diffusion model and the vision encoder itself. DIP is simple, unsupervised, and computationally efficient, requiring less than 9 hours on a single A100 GPU. By learning dense representations through pseudo in-context tasks, it achieves strong performance across a wide variety of downstream real-world in-context scene understanding tasks. It outperforms both the initial vision encoder and prior methods, offering a practical and effective solution for improving dense representations. Code available here: <a target="_blank" rel="noopener" href="https://github.com/sirkosophia/DIP">https://github.com/sirkosophia/DIP</a> </p>
<blockquote>
<p>æˆ‘ä»¬ä»‹ç»äº†DIPï¼Œè¿™æ˜¯ä¸€ç§æ–°å‹çš„æ— ç›‘ç£åè®­ç»ƒæ³•ï¼Œæ—¨åœ¨å¢å¼ºå¤§è§„æ¨¡é¢„è®­ç»ƒè§†è§‰ç¼–ç å™¨çš„å›¾åƒå¯†é›†è¡¨ç¤ºï¼Œä»¥å®ç°å¯¹ä¸Šä¸‹æ–‡åœºæ™¯çš„ç†è§£ã€‚ä¸åŒäºä¾èµ–å¤æ‚è‡ªæˆ‘è’¸é¦æ¶æ„çš„å…ˆå‰æ–¹æ³•ï¼Œæˆ‘ä»¬çš„æ–¹æ³•ä½¿ç”¨ä¼ªä»»åŠ¡æ¥æ˜ç¡®æ¨¡æ‹Ÿä¸‹æ¸¸ä¸Šä¸‹æ–‡åœºæ™¯ï¼Œè¿™æ˜¯å—å…ƒå­¦ä¹ åŸç†çš„å¯å‘ã€‚ä¸ºäº†åœ¨æ— æ ‡ç­¾æ•°æ®ä¸Šè¿›è¡Œåè®­ç»ƒï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§è‡ªåŠ¨ç”Ÿæˆä¸Šä¸‹æ–‡ä»»åŠ¡çš„æœºåˆ¶ï¼Œå®ƒç»“åˆäº†é¢„è®­ç»ƒçš„æ‰©æ•£æ¨¡å‹å’Œè§†è§‰ç¼–ç å™¨æœ¬èº«ã€‚DIPç®€å•ã€æ— ç›‘ç£ã€è®¡ç®—é«˜æ•ˆï¼Œåœ¨å•ä¸ªA100 GPUä¸Šè€—æ—¶ä¸åˆ°9å°æ—¶ã€‚å®ƒé€šè¿‡ä¼ªä¸Šä¸‹æ–‡ä»»åŠ¡å­¦ä¹ å¯†é›†è¡¨ç¤ºï¼Œåœ¨å¤šç§ä¸‹æ¸¸ç°å®åœºæ™¯ç†è§£ä»»åŠ¡ä¸­è¡¨ç°å‡ºå¼ºå¤§çš„æ€§èƒ½ã€‚å®ƒä¼˜äºåˆå§‹çš„è§†è§‰ç¼–ç å™¨å’Œå…ˆå‰çš„æ–¹æ³•ï¼Œä¸ºæ”¹è¿›å¯†é›†è¡¨ç¤ºæä¾›äº†å®ç”¨æœ‰æ•ˆçš„è§£å†³æ–¹æ¡ˆã€‚ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/sirkosophia/DIP%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/sirkosophia/DIPæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.18463v1">PDF</a> </p>
<p><strong>Summary</strong><br>     æå‡ºä¸€ç§æ–°å‹æ— ç›‘ç£åè®­ç»ƒæ³•DIPï¼Œæ—¨åœ¨å¢å¼ºå¤§è§„æ¨¡é¢„è®­ç»ƒè§†è§‰ç¼–ç å™¨çš„å¯†é›†å›¾åƒè¡¨ç¤ºï¼Œä»¥æå‡å¯¹ä¸Šä¸‹æ–‡åœºæ™¯çš„ç†è§£ã€‚è¯¥æ–¹æ³•é€šè¿‡æ¨¡æ‹Ÿä¸‹æ¸¸ä¸Šä¸‹æ–‡åœºæ™¯ç”Ÿæˆä¼ªä»»åŠ¡ï¼Œå—å…ƒå­¦ä¹ åŸç†å¯å‘ï¼Œè®­ç»ƒè§†è§‰ç¼–ç å™¨ã€‚ä½¿ç”¨é¢„è®­ç»ƒçš„æ‰©æ•£æ¨¡å‹å’Œè§†è§‰ç¼–ç å™¨æœ¬èº«ç”Ÿæˆä¸Šä¸‹æ–‡ä»»åŠ¡ï¼Œå®ç°æ— ç›‘ç£çš„åè®­ç»ƒã€‚DIPæ–¹æ³•ç®€å•ã€é«˜æ•ˆï¼Œåœ¨å•ä¸ªA100 GPUä¸Šä¸åˆ°9å°æ—¶å³å¯å®Œæˆã€‚é€šè¿‡å­¦ä¹ å¯†é›†è¡¨ç¤ºï¼Œåœ¨å¤šç§ä¸‹æ¸¸ç°å®åœºæ™¯ç†è§£ä»»åŠ¡ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œä¼˜äºåˆå§‹è§†è§‰ç¼–ç å™¨å’Œå…ˆå‰æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>DIPæ˜¯ä¸€ç§æ–°å‹æ— ç›‘ç£åè®­ç»ƒæ³•ï¼Œæ—¨åœ¨å¢å¼ºè§†è§‰ç¼–ç å™¨çš„å¯†é›†å›¾åƒè¡¨ç¤ºï¼Œæå‡å¯¹ä¸Šä¸‹æ–‡åœºæ™¯çš„ç†è§£ã€‚</li>
<li>è¯¥æ–¹æ³•é€šè¿‡æ¨¡æ‹Ÿä¸‹æ¸¸ä¸Šä¸‹æ–‡åœºæ™¯ç”Ÿæˆä¼ªä»»åŠ¡è¿›è¡Œè®­ç»ƒï¼Œå—å…ƒå­¦ä¹ åŸç†å¯å‘ã€‚</li>
<li>DIPä½¿ç”¨é¢„è®­ç»ƒçš„æ‰©æ•£æ¨¡å‹å’Œè§†è§‰ç¼–ç å™¨æœ¬èº«ç»“åˆï¼Œè‡ªåŠ¨ç”Ÿæˆä¸Šä¸‹æ–‡ä»»åŠ¡ï¼Œå®ç°æ— ç›‘ç£çš„åè®­ç»ƒã€‚</li>
<li>DIPå…·æœ‰ç®€å•ã€é«˜æ•ˆçš„ç‰¹ç‚¹ï¼Œè®¡ç®—æˆæœ¬ä½ï¼Œé€‚ç”¨äºå¤§è§„æ¨¡å›¾åƒæ•°æ®å¤„ç†ã€‚</li>
<li>DIPåœ¨å¤šç§ä¸‹æ¸¸ç°å®åœºæ™¯ç†è§£ä»»åŠ¡ä¸­è¡¨ç°ä¼˜å¼‚ï¼ŒåŒ…æ‹¬å›¾åƒåˆ†ç±»ã€ç›®æ ‡æ£€æµ‹ç­‰ã€‚</li>
<li>DIPæ–¹æ³•ä¼˜äºåˆå§‹è§†è§‰ç¼–ç å™¨å’Œå…ˆå‰æ–¹æ³•ï¼Œä¸ºæ”¹å–„å¯†é›†è¡¨ç¤ºæä¾›äº†å®ç”¨æœ‰æ•ˆçš„è§£å†³æ–¹æ¡ˆã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.18463">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-47810542c001af041a5b8a141272fe3e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c39b485894042ea91ba05e70dd94c087.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-793dd8b24adfef17d0068d69d2f18b3c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c8fdc7df2e6fadf603e42bd81f8c3860.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="CPAM-Context-Preserving-Adaptive-Manipulation-for-Zero-Shot-Real-Image-Editing"><a href="#CPAM-Context-Preserving-Adaptive-Manipulation-for-Zero-Shot-Real-Image-Editing" class="headerlink" title="CPAM: Context-Preserving Adaptive Manipulation for Zero-Shot Real Image   Editing"></a>CPAM: Context-Preserving Adaptive Manipulation for Zero-Shot Real Image   Editing</h2><p><strong>Authors:Dinh-Khoi Vo, Thanh-Toan Do, Tam V. Nguyen, Minh-Triet Tran, Trung-Nghia Le</strong></p>
<p>Editing natural images using textual descriptions in text-to-image diffusion models remains a significant challenge, particularly in achieving consistent generation and handling complex, non-rigid objects. Existing methods often struggle to preserve textures and identity, require extensive fine-tuning, and exhibit limitations in editing specific spatial regions or objects while retaining background details. This paper proposes Context-Preserving Adaptive Manipulation (CPAM), a novel zero-shot framework for complicated, non-rigid real image editing. Specifically, we propose a preservation adaptation module that adjusts self-attention mechanisms to preserve and independently control the object and background effectively. This ensures that the objectsâ€™ shapes, textures, and identities are maintained while keeping the background undistorted during the editing process using the mask guidance technique. Additionally, we develop a localized extraction module to mitigate the interference with the non-desired modified regions during conditioning in cross-attention mechanisms. We also introduce various mask-guidance strategies to facilitate diverse image manipulation tasks in a simple manner. Extensive experiments on our newly constructed Image Manipulation BenchmArk (IMBA), a robust benchmark dataset specifically designed for real image editing, demonstrate that our proposed method is the preferred choice among human raters, outperforming existing state-of-the-art editing techniques. </p>
<blockquote>
<p>åœ¨æ–‡æœ¬åˆ°å›¾åƒçš„æ‰©æ•£æ¨¡å‹ä¸­ï¼Œä½¿ç”¨æ–‡æœ¬æè¿°ç¼–è¾‘è‡ªç„¶å›¾åƒä»ç„¶æ˜¯ä¸€ä¸ªé‡å¤§æŒ‘æˆ˜ï¼Œç‰¹åˆ«æ˜¯åœ¨å®ç°ä¸€è‡´ç”Ÿæˆå’Œå¤„ç†å¤æ‚ã€éåˆšæ€§ç‰©ä½“æ–¹é¢ã€‚ç°æœ‰æ–¹æ³•å¾€å¾€éš¾ä»¥ä¿ç•™çº¹ç†å’Œèº«ä»½ï¼Œéœ€è¦å¤§é‡å¾®è°ƒï¼Œå¹¶ä¸”åœ¨ç¼–è¾‘ç‰¹å®šç©ºé—´åŒºåŸŸæˆ–ç‰©ä½“æ—¶ä¿ç•™èƒŒæ™¯ç»†èŠ‚æ–¹é¢å­˜åœ¨å±€é™æ€§ã€‚æœ¬æ–‡æå‡ºäº†ä¸Šä¸‹æ–‡ä¿ç•™è‡ªé€‚åº”æ“ä½œï¼ˆCPAMï¼‰ï¼Œè¿™æ˜¯ä¸€ç§ç”¨äºå¤æ‚ã€éåˆšæ€§çš„çœŸå®å›¾åƒç¼–è¾‘çš„æ–°å‹é›¶æ ·æœ¬æ¡†æ¶ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªä¿ç•™é€‚åº”æ¨¡å—ï¼Œè¯¥æ¨¡å—è°ƒæ•´è‡ªæ³¨æ„åŠ›æœºåˆ¶ï¼Œä»¥æœ‰æ•ˆåœ°ä¿ç•™å¹¶ç‹¬ç«‹æ§åˆ¶ç‰©ä½“å’ŒèƒŒæ™¯ã€‚è¿™ç¡®ä¿äº†ç‰©ä½“çš„å½¢çŠ¶ã€çº¹ç†å’Œèº«ä»½å¾—ä»¥ä¿æŒï¼ŒåŒæ—¶åœ¨ç¼–è¾‘è¿‡ç¨‹ä¸­ä½¿ç”¨é®ç½©æŒ‡å¯¼æŠ€æœ¯ä¿æŒèƒŒæ™¯ä¸å˜å½¢ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å¼€å‘äº†ä¸€ä¸ªå±€éƒ¨æå–æ¨¡å—ï¼Œä»¥å‡è½»äº¤å‰æ³¨æ„åŠ›æœºåˆ¶ä¸­æ¡ä»¶è®¾ç½®æ—¶éæœŸæœ›ä¿®æ”¹åŒºåŸŸçš„å¹²æ‰°ã€‚æˆ‘ä»¬è¿˜ä»‹ç»äº†å„ç§é®ç½©æŒ‡å¯¼ç­–ç•¥ï¼Œä»¥ç®€å•çš„æ–¹å¼ä¿ƒè¿›å„ç§å›¾åƒæ“ä½œä»»åŠ¡ã€‚åœ¨æˆ‘ä»¬æ–°æ„å»ºçš„å›¾åƒæ“ä½œåŸºå‡†æµ‹è¯•ï¼ˆIMBAï¼‰ä¸Šçš„å¤§é‡å®éªŒè¡¨æ˜ï¼Œä¸äººç±»è¯„ä¼°è€…ç›¸æ¯”ï¼Œæˆ‘ä»¬æå‡ºçš„æ–¹æ³•æ˜¯é¦–é€‰ï¼Œè¶…è¶Šäº†ç°æœ‰çš„å…ˆè¿›ç¼–è¾‘æŠ€æœ¯ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.18438v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºCPAMçš„é›¶æ ·æœ¬æ¡†æ¶ï¼Œç”¨äºå¤æ‚ã€éåˆšæ€§çš„çœŸå®å›¾åƒç¼–è¾‘ã€‚è¯¥æ¡†æ¶é€šè¿‡è°ƒæ•´è‡ªæˆ‘å…³æ³¨æœºåˆ¶ï¼Œå®ç°äº†å¯¹è±¡ä¸èƒŒæ™¯çš„æœ‰æ•ˆç‹¬ç«‹æ§åˆ¶ï¼Œä»è€Œåœ¨ç¼–è¾‘è¿‡ç¨‹ä¸­ä¿æŒå¯¹è±¡å’ŒèƒŒæ™¯çš„ä¸€è‡´æ€§ã€‚ä½¿ç”¨maskæŒ‡å¯¼æŠ€æœ¯ï¼Œèƒ½å¤Ÿåœ¨ä¿æŒèƒŒæ™¯ä¸å˜çš„åŒæ—¶ï¼Œä¿æŒå¯¹è±¡çš„å½¢çŠ¶ã€çº¹ç†å’Œèº«ä»½ã€‚æ­¤å¤–ï¼Œè¿˜å¼•å…¥äº†å¤šç§maskæŒ‡å¯¼ç­–ç•¥ï¼Œä»¥ç®€åŒ–å„ç§å›¾åƒæ“ä½œä»»åŠ¡ã€‚åœ¨ä¸“é—¨è®¾è®¡çš„çœŸå®å›¾åƒç¼–è¾‘åŸºå‡†æ•°æ®é›†IMBAä¸Šè¿›è¡Œçš„å¤§é‡å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨äººç±»è¯„ä¼°ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œè¶…è¶Šäº†ç°æœ‰çš„å…ˆè¿›ç¼–è¾‘æŠ€æœ¯ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>CPAMæ˜¯ä¸€ä¸ªç”¨äºå¤æ‚ã€éåˆšæ€§çœŸå®å›¾åƒç¼–è¾‘çš„é›¶æ ·æœ¬æ¡†æ¶ã€‚</li>
<li>æ¡†æ¶é€šè¿‡è°ƒæ•´è‡ªæˆ‘å…³æ³¨æœºåˆ¶ï¼Œå®ç°äº†å¯¹è±¡å’ŒèƒŒæ™¯çš„æœ‰æ•ˆç‹¬ç«‹æ§åˆ¶ã€‚</li>
<li>ä½¿ç”¨maskæŒ‡å¯¼æŠ€æœ¯ï¼Œèƒ½å¤Ÿåœ¨ç¼–è¾‘è¿‡ç¨‹ä¸­ä¿æŒå¯¹è±¡çš„å½¢çŠ¶ã€çº¹ç†å’Œèº«ä»½ã€‚</li>
<li>å¼•å…¥äº†å¤šç§maskæŒ‡å¯¼ç­–ç•¥ï¼Œä»¥ç®€åŒ–å›¾åƒæ“ä½œä»»åŠ¡ã€‚</li>
<li>æå‡ºäº†ä¸€ä¸ªæ–°å»ºçš„çœŸå®å›¾åƒç¼–è¾‘åŸºå‡†æ•°æ®é›†IMBAã€‚</li>
<li>å®éªŒç»“æœè¡¨æ˜ï¼ŒCPAMåœ¨äººç±»è¯„ä¼°ä¸­è¡¨ç°ä¼˜å¼‚ã€‚</li>
<li>CPAMè¶…è¶Šäº†ç°æœ‰çš„å…ˆè¿›ç¼–è¾‘æŠ€æœ¯ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.18438">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-637c31656e40e1f44ae7d183dbe90d51.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-bda7de7f5458ae08eac80388cbe50568.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ca61544aac989e6612155ce0a0faa1d1.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Morse-Dual-Sampling-for-Lossless-Acceleration-of-Diffusion-Models"><a href="#Morse-Dual-Sampling-for-Lossless-Acceleration-of-Diffusion-Models" class="headerlink" title="Morse: Dual-Sampling for Lossless Acceleration of Diffusion Models"></a>Morse: Dual-Sampling for Lossless Acceleration of Diffusion Models</h2><p><strong>Authors:Chao Li, Jiawei Fan, Anbang Yao</strong></p>
<p>In this paper, we present Morse, a simple dual-sampling framework for accelerating diffusion models losslessly. The key insight of Morse is to reformulate the iterative generation (from noise to data) process via taking advantage of fast jump sampling and adaptive residual feedback strategies. Specifically, Morse involves two models called Dash and Dot that interact with each other. The Dash model is just the pre-trained diffusion model of any type, but operates in a jump sampling regime, creating sufficient space for sampling efficiency improvement. The Dot model is significantly faster than the Dash model, which is learnt to generate residual feedback conditioned on the observations at the current jump sampling point on the trajectory of the Dash model, lifting the noise estimate to easily match the next-step estimate of the Dash model without jump sampling. By chaining the outputs of the Dash and Dot models run in a time-interleaved fashion, Morse exhibits the merit of flexibly attaining desired image generation performance while improving overall runtime efficiency. With our proposed weight sharing strategy between the Dash and Dot models, Morse is efficient for training and inference. Our method shows a lossless speedup of 1.78X to 3.31X on average over a wide range of sampling step budgets relative to 9 baseline diffusion models on 6 image generation tasks. Furthermore, we show that our method can be also generalized to improve the Latent Consistency Model (LCM-SDXL, which is already accelerated with consistency distillation technique) tailored for few-step text-to-image synthesis. The code and models are available at <a target="_blank" rel="noopener" href="https://github.com/deep-optimization/Morse">https://github.com/deep-optimization/Morse</a>. </p>
<blockquote>
<p>æœ¬æ–‡ä»‹ç»äº†Morseï¼Œè¿™æ˜¯ä¸€ä¸ªç®€å•çš„é«˜æ•ˆæ— æŸæ‰©æ•£æ¨¡å‹åŒé‡‡æ ·æ¡†æ¶ã€‚Morseçš„å…³é”®æ´å¯ŸåŠ›æ˜¯é€šè¿‡åˆ©ç”¨å¿«é€Ÿè·³è·ƒé‡‡æ ·å’Œè‡ªé€‚åº”æ®‹å·®åé¦ˆç­–ç•¥æ¥é‡æ–°åˆ¶å®šè¿­ä»£ç”Ÿæˆï¼ˆä»å™ªå£°åˆ°æ•°æ®ï¼‰è¿‡ç¨‹ã€‚å…·ä½“æ¥è¯´ï¼ŒMorseåŒ…å«ä¸¤ä¸ªç›¸äº’ä½œç”¨çš„æ¨¡å‹ï¼Œç§°ä¸ºDashå’ŒDotã€‚Dashæ¨¡å‹åªæ˜¯ä»»ä½•ç±»å‹çš„é¢„è®­ç»ƒæ‰©æ•£æ¨¡å‹ï¼Œä½†åœ¨è·³è·ƒé‡‡æ ·æœºåˆ¶ä¸‹è¿è¡Œï¼Œä¸ºé‡‡æ ·æ•ˆç‡æ”¹è¿›åˆ›é€ äº†è¶³å¤Ÿç©ºé—´ã€‚Dotæ¨¡å‹æ˜¾è‘—å¿«äºDashæ¨¡å‹ï¼Œå®ƒå­¦ä¼šæ ¹æ®Dashæ¨¡å‹è½¨è¿¹å½“å‰è·³è·ƒé‡‡æ ·ç‚¹çš„è§‚å¯Ÿç”Ÿæˆæ®‹å·®åé¦ˆï¼Œå°†å™ªå£°ä¼°è®¡æå‡åˆ°æ— éœ€è·³è·ƒé‡‡æ ·çš„ä¸‹ä¸€æ­¥ä¼°è®¡ï¼Œä¸Dashæ¨¡å‹çš„ä¼°è®¡ç›¸åŒ¹é…ã€‚é€šè¿‡ä»¥æ—¶é—´äº¤é”™æ–¹å¼é“¾æ¥Dashå’ŒDotæ¨¡å‹çš„è¾“å‡ºï¼ŒMorseåœ¨çµæ´»å®ç°æ‰€éœ€çš„å›¾åƒç”Ÿæˆæ€§èƒ½çš„åŒæ—¶ï¼Œæé«˜äº†æ€»ä½“è¿è¡Œæ•ˆç‡ã€‚é€šè¿‡æˆ‘ä»¬åœ¨Dashå’ŒDotæ¨¡å‹ä¹‹é—´æå‡ºçš„æƒé‡å…±äº«ç­–ç•¥ï¼ŒMorseåœ¨è®­ç»ƒå’Œæ¨ç†æ–¹é¢éƒ½å¾ˆé«˜æ•ˆã€‚æˆ‘ä»¬çš„æ–¹æ³•åœ¨é‡‡æ ·æ­¥éª¤é¢„ç®—æ–¹é¢ç›¸å¯¹äº9ä¸ªåŸºçº¿æ‰©æ•£æ¨¡å‹åœ¨6ä¸ªå›¾åƒç”Ÿæˆä»»åŠ¡ä¸Šå¹³å‡æ— æŸåŠ é€Ÿ1.78Xè‡³3.31Xã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜è¯æ˜äº†æˆ‘ä»¬æ–¹æ³•å¯ä»¥æ¨å¹¿åˆ°æ”¹è¿›é’ˆå¯¹å°‘æ­¥éª¤æ–‡æœ¬åˆ°å›¾åƒåˆæˆçš„Latent Consistency Modelï¼ˆLCM-SDXLï¼Œå·²ç»ä½¿ç”¨ä¸€è‡´æ€§è’¸é¦æŠ€æœ¯åŠ é€Ÿï¼‰ã€‚ä»£ç å’Œæ¨¡å‹å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/deep-optimization/Morse%E8%8E%B7%E5%BE%97%E3%80%82">https://github.com/deep-optimization/Morseè·å¾—ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.18251v1">PDF</a> This work is accepted to ICML 2025. The project page:   <a target="_blank" rel="noopener" href="https://github.com/deep-optimization/Morse">https://github.com/deep-optimization/Morse</a></p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†åä¸ºMorseçš„åŠ é€Ÿæ‰©æ•£æ¨¡å‹æ¡†æ¶ï¼Œå®ƒé€šè¿‡é‡‡ç”¨åŒé‡‡æ ·ç­–ç•¥å’Œè‡ªé€‚åº”æ®‹å·®åé¦ˆç­–ç•¥æ¥ä¼˜åŒ–è¿­ä»£ç”Ÿæˆè¿‡ç¨‹ã€‚MorseåŒ…å«ä¸¤ä¸ªç›¸äº’ä½œç”¨çš„æ¨¡å‹ï¼šDashå’ŒDotã€‚Dashæ¨¡å‹æ˜¯ä»»ä½•ç±»å‹çš„é¢„è®­ç»ƒæ‰©æ•£æ¨¡å‹ï¼Œé‡‡ç”¨è·³è·ƒé‡‡æ ·æœºåˆ¶ä»¥æé«˜é‡‡æ ·æ•ˆç‡ã€‚Dotæ¨¡å‹æ¯”Dashæ¨¡å‹æ›´å¿«ï¼Œå­¦ä¼šæ ¹æ®Dashæ¨¡å‹çš„å½“å‰è·³è·ƒé‡‡æ ·ç‚¹ç”Ÿæˆæ®‹å·®åé¦ˆï¼Œä»è€Œæ›´å®¹æ˜“åŒ¹é…ä¸‹ä¸€æ­¥ä¼°è®¡ã€‚é€šè¿‡äº¤æ›¿ä½¿ç”¨Dashå’ŒDotæ¨¡å‹çš„è¾“å‡ºï¼ŒMorseåœ¨æé«˜è¿è¡Œæ•ˆç‡çš„åŒæ—¶çµæ´»å®ç°å›¾åƒç”Ÿæˆæ€§èƒ½ã€‚æ­¤å¤–ï¼ŒMorseè¿˜é‡‡ç”¨æƒé‡å…±äº«ç­–ç•¥ï¼Œæé«˜äº†è®­ç»ƒå’Œæ¨ç†æ•ˆç‡ã€‚åœ¨å¹¿æ³›çš„é‡‡æ ·æ­¥é•¿é¢„ç®—èŒƒå›´å†…ï¼Œç›¸å¯¹äº9ç§åŸºçº¿æ‰©æ•£æ¨¡å‹ï¼ŒMorseåœ¨6é¡¹å›¾åƒç”Ÿæˆä»»åŠ¡ä¸Šçš„å¹³å‡æ— æŸåŠ é€Ÿè¾¾åˆ°1.78Xè‡³3.31Xã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ³•è¿˜å¯ä»¥æ¨å¹¿åˆ°ç”¨äºå°‘æ­¥éª¤æ–‡æœ¬åˆ°å›¾åƒåˆæˆçš„LCM-SDXLæ¨¡å‹çš„åŠ é€Ÿã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Morseæ˜¯ä¸€ä¸ªç”¨äºåŠ é€Ÿæ‰©æ•£æ¨¡å‹çš„ç®€å•åŒé‡‡æ ·æ¡†æ¶ï¼Œèƒ½æ— æŸåœ°æé«˜å›¾åƒç”Ÿæˆæ•ˆç‡ã€‚</li>
<li>MorseåŒ…å«ä¸¤ä¸ªæ¨¡å‹ï¼šDashå’ŒDotï¼Œåˆ†åˆ«è´Ÿè´£ä¸åŒçš„é‡‡æ ·å’Œåé¦ˆç”Ÿæˆä»»åŠ¡ã€‚</li>
<li>Morseåˆ©ç”¨è·³è·ƒé‡‡æ ·å’Œè‡ªé€‚åº”æ®‹å·®åé¦ˆç­–ç•¥ä¼˜åŒ–è¿­ä»£ç”Ÿæˆè¿‡ç¨‹ã€‚</li>
<li>Morseé€šè¿‡äº¤æ›¿ä½¿ç”¨Dashå’ŒDotæ¨¡å‹çš„è¾“å‡ºï¼Œå®ç°äº†å›¾åƒç”Ÿæˆæ€§èƒ½å’Œè¿è¡Œæ•ˆç‡çš„æå‡ã€‚</li>
<li>Morseé‡‡ç”¨æƒé‡å…±äº«ç­–ç•¥ï¼Œæé«˜äº†è®­ç»ƒå’Œæ¨ç†æ•ˆç‡ã€‚</li>
<li>Morseç›¸å¯¹äºå¤šç§åŸºçº¿æ‰©æ•£æ¨¡å‹ï¼Œåœ¨å¤šé¡¹å›¾åƒç”Ÿæˆä»»åŠ¡ä¸Šå®ç°äº†å¹³å‡æ— æŸåŠ é€Ÿã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.18251">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-b03ea832be05d042ddcbe61acbc687c4.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-05708ab567db0ad895ccfcbee4651323.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5dd16ba0285d3520134e85faa72004be.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-474832705962235fd7685e55a9e41d9e.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-558f34ecd7ec39ab0e0fbb70c269d726.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="CDG-MAE-Learning-Correspondences-from-Diffusion-Generated-Views"><a href="#CDG-MAE-Learning-Correspondences-from-Diffusion-Generated-Views" class="headerlink" title="CDG-MAE: Learning Correspondences from Diffusion Generated Views"></a>CDG-MAE: Learning Correspondences from Diffusion Generated Views</h2><p><strong>Authors:Varun Belagali, Pierre Marza, Srikar Yellapragada, Zilinghan Li, Tarak Nath Nandi, Ravi K Madduri, Joel Saltz, Stergios Christodoulidis, Maria Vakalopoulou, Dimitris Samaras</strong></p>
<p>Learning dense correspondences, critical for application such as video label propagation, is hindered by tedious and unscalable manual annotation. Self-supervised methods address this by using a cross-view pretext task, often modeled with a masked autoencoder, where a masked target view is reconstructed from an anchor view. However, acquiring effective training data remains a challenge - collecting diverse video datasets is difficult and costly, while simple image crops lack necessary pose variations. This paper introduces CDG-MAE, a novel MAE-based self-supervised method that uses diverse synthetic views generated from static images via an image-conditioned diffusion model. These generated views exhibit substantial changes in pose and perspective, providing a rich training signal that overcomes the limitations of video and crop-based anchors. We present a quantitative method to evaluate local and global consistency of generated images, discussing their use for cross-view self-supervised pretraining. Furthermore, we enhance the standard single-anchor MAE setting to a multi-anchor strategy to effectively modulate the difficulty of pretext task. CDG-MAE significantly outperforms state-of-the-art MAE methods reliant only on images and substantially narrows the performance gap to video-based approaches. </p>
<blockquote>
<p>å­¦ä¹ å¯†é›†å¯¹åº”å…³ç³»å¯¹äºè§†é¢‘æ ‡ç­¾ä¼ æ’­ç­‰åº”ç”¨è‡³å…³é‡è¦ï¼Œä½†ç¹çä¸”ä¸å¯æ‰©å±•çš„æ‰‹åŠ¨æ³¨é‡Šé˜»ç¢äº†å…¶å­¦ä¹ ã€‚è‡ªç›‘ç£æ–¹æ³•é€šè¿‡ä½¿ç”¨è·¨è§†å›¾é¢„è®­ç»ƒä»»åŠ¡æ¥è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œé€šå¸¸ä½¿ç”¨æ©ç è‡ªåŠ¨ç¼–ç å™¨è¿›è¡Œå»ºæ¨¡ï¼Œå…¶ä¸­ä»é”šè§†å›¾é‡å»ºæ©ç çš„ç›®æ ‡è§†å›¾ã€‚ç„¶è€Œï¼Œè·å–æœ‰æ•ˆçš„è®­ç»ƒæ•°æ®ä»ç„¶æ˜¯ä¸€ä¸ªæŒ‘æˆ˜â€”â€”æ”¶é›†å¤šæ ·åŒ–çš„è§†é¢‘æ•°æ®é›†æ—¢å›°éš¾åˆæˆæœ¬é«˜æ˜‚ï¼Œè€Œç®€å•çš„å›¾åƒè£å‰ªç¼ºä¹å¿…è¦çš„å§¿åŠ¿å˜åŒ–ã€‚æœ¬æ–‡ä»‹ç»äº†CDG-MAEï¼Œè¿™æ˜¯ä¸€ç§åŸºäºMAEçš„æ–°å‹è‡ªç›‘ç£æ–¹æ³•ï¼Œå®ƒä½¿ç”¨å›¾åƒæ¡ä»¶æ‰©æ•£æ¨¡å‹ä»é™æ€å›¾åƒç”Ÿæˆå¤šæ ·åŒ–çš„åˆæˆè§†å›¾ã€‚è¿™äº›ç”Ÿæˆçš„è§†å›¾åœ¨å§¿åŠ¿å’Œè§†è§’ä¸Šè¡¨ç°å‡ºé‡å¤§å˜åŒ–ï¼Œæä¾›äº†ä¸°å¯Œçš„è®­ç»ƒä¿¡å·ï¼Œå…‹æœäº†åŸºäºè§†é¢‘å’Œè£å‰ªé”šç‚¹çš„å±€é™æ€§ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§å®šé‡æ–¹æ³•ï¼Œä»¥è¯„ä¼°ç”Ÿæˆå›¾åƒçš„åœ°æ–¹å’Œå…¨çƒä¸€è‡´æ€§ï¼Œå¹¶è®¨è®ºå®ƒä»¬åœ¨è·¨è§†å›¾è‡ªç›‘ç£é¢„è®­ç»ƒä¸­çš„ä½¿ç”¨ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å°†æ ‡å‡†å•é”šç‚¹MAEè®¾ç½®å¢å¼ºä¸ºå¤šé”šç‚¹ç­–ç•¥ï¼Œä»¥æœ‰æ•ˆåœ°è°ƒèŠ‚é¢„è®­ç»ƒä»»åŠ¡çš„éš¾åº¦ã€‚CDG-MAEæ˜¾è‘—ä¼˜äºä»…ä¾èµ–å›¾åƒçš„æœ€æ–°MAEæ–¹æ³•ï¼Œå¹¶å¤§å¹…ç¼©å°äº†ä¸åŸºäºè§†é¢‘çš„æ–¹æ³•çš„æ€§èƒ½å·®è·ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.18164v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºä¸€ç§åŸºäºè‡ªç›‘ç£å­¦ä¹ çš„å¯†é›†å¯¹åº”å­¦ä¹ æ–¹æ³•CDG-MAEï¼Œé€šè¿‡å›¾åƒæ¡ä»¶æ‰©æ•£æ¨¡å‹ç”Ÿæˆå¤šæ ·çš„åˆæˆè§†å›¾ï¼Œè§£å†³äº†è§†é¢‘æ ‡ç­¾ä¼ æ’­ç­‰åº”ç”¨ä¸­æ‰‹åŠ¨æ ‡æ³¨çš„ç¹çå’Œä¸å¯æ‰©å±•çš„é—®é¢˜ã€‚è¯¥æ–¹æ³•åˆ©ç”¨é™æ€å›¾åƒç”Ÿæˆå…·æœ‰æ˜¾è‘—å§¿åŠ¿å’Œè§†è§’å˜åŒ–çš„è§†å›¾ï¼Œæä¾›ä¸°å¯Œçš„è®­ç»ƒä¿¡å·ï¼Œå…‹æœäº†è§†é¢‘å’ŒåŸºäºè£å‰ªçš„é”šç‚¹æ–¹æ³•çš„å±€é™æ€§ã€‚åŒæ—¶ï¼Œæœ¬æ–‡è¿˜æå‡ºäº†ä¸€ç§è¯„ä¼°ç”Ÿæˆå›¾åƒå±€éƒ¨å’Œå…¨å±€ä¸€è‡´æ€§çš„å®šé‡æ–¹æ³•ï¼Œå¹¶è®¨è®ºäº†å®ƒä»¬åœ¨è·¨è§†å›¾è‡ªç›‘ç£é¢„è®­ç»ƒä¸­çš„åº”ç”¨ã€‚æ­¤å¤–ï¼Œè¿˜å¢å¼ºäº†æ ‡å‡†å•é”šç‚¹MAEè®¾ç½®ï¼Œé‡‡ç”¨å¤šé”šç‚¹ç­–ç•¥ï¼Œä»¥æœ‰æ•ˆè°ƒæ•´å…ˆéªŒä»»åŠ¡çš„éš¾åº¦ã€‚CDG-MAEæ˜¾è‘—ä¼˜äºä»…ä¾èµ–å›¾åƒçš„æœ€æ–°MAEæ–¹æ³•ï¼Œå¹¶å¤§å¤§ç¼©å°äº†ä¸åŸºäºè§†é¢‘çš„æ–¹æ³•ä¹‹é—´çš„æ€§èƒ½å·®è·ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>CDG-MAEæ˜¯ä¸€ç§åŸºäºè‡ªç›‘ç£å­¦ä¹ çš„å¯†é›†å¯¹åº”å­¦ä¹ æ–¹æ³•ï¼Œè§£å†³äº†è§†é¢‘æ ‡ç­¾ä¼ æ’­ä¸­çš„ç¹çæ‰‹åŠ¨æ ‡æ³¨é—®é¢˜ã€‚</li>
<li>åˆ©ç”¨å›¾åƒæ¡ä»¶æ‰©æ•£æ¨¡å‹ç”Ÿæˆå¤šæ ·åˆæˆè§†å›¾ï¼Œè¿™äº›è§†å›¾å…·æœ‰æ˜¾è‘—å˜åŒ–çš„å§¿åŠ¿å’Œè§†è§’ã€‚</li>
<li>ç”Ÿæˆå›¾åƒå…·æœ‰ä¸°å¯Œè®­ç»ƒä¿¡å·ï¼Œå…‹æœè§†é¢‘å’ŒåŸºäºè£å‰ªé”šç‚¹çš„å±€é™æ€§ã€‚</li>
<li>æå‡ºä¸€ç§å®šé‡è¯„ä¼°ç”Ÿæˆå›¾åƒå±€éƒ¨å’Œå…¨å±€ä¸€è‡´æ€§çš„æ–¹æ³•ã€‚</li>
<li>è®¨è®ºäº†ç”Ÿæˆå›¾åƒåœ¨è·¨è§†å›¾è‡ªç›‘ç£é¢„è®­ç»ƒä¸­çš„åº”ç”¨ã€‚</li>
<li>å¢å¼ºäº†æ ‡å‡†å•é”šç‚¹MAEè®¾ç½®ï¼Œé‡‡ç”¨å¤šé”šç‚¹ç­–ç•¥ä»¥æœ‰æ•ˆè°ƒæ•´å…ˆéªŒä»»åŠ¡éš¾åº¦ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.18164">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-0bc9640c3f8e61a92187b8a964ccffbb.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-fb7e98f665e0203aa786aaccfe4bdb42.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d0a4b992d5cea92474d7f37bb2797711.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-73dcfcdd5403da5b0667fbff652953e9.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Enabling-PSO-Secure-Synthetic-Data-Sharing-Using-Diversity-Aware-Diffusion-Models"><a href="#Enabling-PSO-Secure-Synthetic-Data-Sharing-Using-Diversity-Aware-Diffusion-Models" class="headerlink" title="Enabling PSO-Secure Synthetic Data Sharing Using Diversity-Aware   Diffusion Models"></a>Enabling PSO-Secure Synthetic Data Sharing Using Diversity-Aware   Diffusion Models</h2><p><strong>Authors:Mischa Dombrowski, Bernhard Kainz</strong></p>
<p>Synthetic data has recently reached a level of visual fidelity that makes it nearly indistinguishable from real data, offering great promise for privacy-preserving data sharing in medical imaging. However, fully synthetic datasets still suffer from significant limitations: First and foremost, the legal aspect of sharing synthetic data is often neglected and data regulations, such as the GDPR, are largley ignored. Secondly, synthetic models fall short of matching the performance of real data, even for in-domain downstream applications. Recent methods for image generation have focused on maximising image diversity instead of fidelity solely to improve the mode coverage and therefore the downstream performance of synthetic data. In this work, we shift perspective and highlight how maximizing diversity can also be interpreted as protecting natural persons from being singled out, which leads to predicate singling-out (PSO) secure synthetic datasets. Specifically, we propose a generalisable framework for training diffusion models on personal data which leads to unpersonal synthetic datasets achieving performance within one percentage point of real-data models while significantly outperforming state-of-the-art methods that do not ensure privacy. Our code is available at <a target="_blank" rel="noopener" href="https://github.com/MischaD/Trichotomy">https://github.com/MischaD/Trichotomy</a>. </p>
<blockquote>
<p>åˆæˆæ•°æ®æœ€è¿‘å·²ç»è¾¾åˆ°äº†è§†è§‰ä¿çœŸåº¦çš„æ°´å¹³ï¼Œä½¿å…¶å‡ ä¹æ— æ³•ä¸çœŸå®æ•°æ®åŒºåˆ†å¼€ï¼Œè¿™åœ¨åŒ»å­¦å½±åƒçš„éšç§ä¿æŠ¤æ•°æ®å…±äº«æ–¹é¢æ˜¾ç¤ºå‡ºå·¨å¤§çš„æ½œåŠ›ã€‚ç„¶è€Œï¼Œå®Œå…¨åˆæˆæ•°æ®é›†ä»å­˜åœ¨é‡å¤§å±€é™æ€§ï¼šé¦–å…ˆï¼Œåˆæˆæ•°æ®çš„å…±äº«æ–¹é¢çš„æ³•å¾‹å¸¸å¸¸è¢«å¿½è§†ï¼Œæ•°æ®æ³•è§„ï¼ˆå¦‚GDPRï¼‰åœ¨å¾ˆå¤§ç¨‹åº¦ä¸Šè¢«å¿½ç•¥ã€‚å…¶æ¬¡ï¼Œåˆæˆæ¨¡å‹åœ¨åŒ¹é…çœŸå®æ•°æ®æ€§èƒ½æ–¹é¢è¿˜å­˜åœ¨ä¸è¶³ï¼Œå³ä½¿åœ¨åŸŸå†…ä¸‹æ¸¸åº”ç”¨ä¸­ä¹Ÿæ˜¯å¦‚æ­¤ã€‚æœ€è¿‘çš„å›¾åƒç”Ÿæˆæ–¹æ³•ä¸»è¦å…³æ³¨æœ€å¤§åŒ–å›¾åƒå¤šæ ·æ€§è€Œéä»…ä»…æé«˜ä¿çœŸåº¦ï¼Œä»¥æ­¤æ¥æé«˜åˆæˆæ•°æ®çš„æ¨¡å¼è¦†ç›–ç‡å’Œä¸‹æ¸¸æ€§èƒ½ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æ”¹å˜äº†è§†è§’å¹¶å¼ºè°ƒäº†å¦‚ä½•æœ€å¤§åŒ–å¤šæ ·æ€§ä¹Ÿå¯ä»¥è¢«è§£é‡Šä¸ºä¿æŠ¤è‡ªç„¶äººå…å—ä¸ªåˆ«å¯¹å¾…ï¼Œä»è€Œå¯¼è‡´è°“è¯­å•ä¸€åŒ–ï¼ˆPSOï¼‰å®‰å…¨çš„åˆæˆæ•°æ®é›†ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªé€šç”¨çš„è®­ç»ƒæ‰©æ•£æ¨¡å‹ä¸ªäººæ•°æ®çš„æ¡†æ¶ï¼Œè¯¥æ¡†æ¶ç”Ÿæˆçš„æ— ä¸ªäººä¿¡æ¯çš„åˆæˆæ•°æ®é›†çš„æ€§èƒ½ä¸çœŸå®æ•°æ®æ¨¡å‹ç›¸å·®ä¸åˆ°ä¸€ä¸ªç™¾åˆ†ç‚¹ï¼ŒåŒæ—¶æ˜¾è‘—ä¼˜äºä¸èƒ½ä¿è¯éšç§çš„æœ€æ–°æ–¹æ³•ã€‚æˆ‘ä»¬çš„ä»£ç å¯åœ¨ <a target="_blank" rel="noopener" href="https://github.com/MischaD/Trichotomy">https://github.com/MischaD/Trichotomy</a> æ‰¾åˆ°ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.17975v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æŒ‡å‡ºåˆæˆæ•°æ®åœ¨è§†è§‰ä¿çœŸåº¦æ–¹é¢å·²ç»è¾¾åˆ°è¿‘ä¹æ— æ³•åˆ†è¾¨çœŸä¼ªçš„æ°´å¹³ï¼Œå¯¹äºåŒ»å­¦å½±åƒç­‰éšç§ä¿æŠ¤é¢†åŸŸçš„æ•°æ®å…±äº«å…·æœ‰å·¨å¤§æ½œåŠ›ã€‚ç„¶è€Œï¼Œåˆæˆæ•°æ®é›†ä»å­˜åœ¨æ³•å¾‹å¿½è§†å’Œæ•°æ®æ€§èƒ½åŒ¹é…ä¸è¶³ç­‰å±€é™æ€§ã€‚ç ”ç©¶å›¢é˜Ÿæå‡ºäº†ä¸€ç§åŸºäºæ‰©æ•£æ¨¡å‹çš„é€šç”¨æ¡†æ¶ï¼Œæ—¨åœ¨è®­ç»ƒä¸ªäººæ•°æ®ä¸Šçš„åˆæˆæ•°æ®é›†ï¼Œåœ¨ä¿æŠ¤ä¸ªäººéšç§çš„åŒæ—¶å®ç°ä¸çœŸå®æ•°æ®æ¨¡å‹ç›¸è¿‘çš„æ€§èƒ½è¡¨ç°ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>åˆæˆæ•°æ®åœ¨è§†è§‰ä¿çœŸåº¦ä¸Šå·²æ¥è¿‘çœŸå®æ•°æ®æ°´å¹³ï¼Œä¸ºéšç§ä¿æŠ¤é¢†åŸŸçš„æ•°æ®å…±äº«å¸¦æ¥å·¨å¤§æ½œåŠ›ã€‚</li>
<li>ç›®å‰åˆæˆæ•°æ®é›†é¢ä¸´æ³•å¾‹å’Œæ€§èƒ½åŒ¹é…é—®é¢˜ï¼Œä¾‹å¦‚å¿½è§†æ•°æ®æ³•è§„ï¼ˆå¦‚GDPRï¼‰ä»¥åŠæ¨¡å‹æ€§èƒ½ä¸çœŸå®æ•°æ®å­˜åœ¨å·®è·ã€‚</li>
<li>è¿‘æœŸå›¾åƒç”Ÿæˆæ–¹æ³•ä¾§é‡äºæœ€å¤§åŒ–å›¾åƒå¤šæ ·æ€§ä»¥æé«˜åˆæˆæ•°æ®çš„ä¸‹æ¸¸æ€§èƒ½ï¼Œä½†å¯èƒ½å¿½è§†äº†ä¿çœŸåº¦çš„é‡è¦æ€§ã€‚</li>
<li>ç ”ç©¶å›¢é˜Ÿæå‡ºäº†ä¸€ä¸ªåŸºäºæ‰©æ•£æ¨¡å‹çš„é€šç”¨æ¡†æ¶ï¼Œè¯¥æ¡†æ¶æ—¨åœ¨è®­ç»ƒåˆæˆæ•°æ®é›†ä»¥å®ç°éšç§ä¿æŠ¤ã€‚</li>
<li>è¯¥æ¡†æ¶åœ¨ä¿æŠ¤ä¸ªäººéšç§çš„åŒæ—¶ï¼Œå®ç°äº†ä¸çœŸå®æ•°æ®æ¨¡å‹ç›¸è¿‘çš„æ€§èƒ½è¡¨ç°ï¼Œä¸”åœ¨æ€§èƒ½ä¸Šæ˜¾è‘—ä¼˜äºæœªç¡®ä¿éšç§çš„ç°æœ‰æ–¹æ³•ã€‚</li>
<li>åˆæˆæ•°æ®åœ¨ç¡®ä¿éšç§çš„åŒæ—¶å¯ä»¥æå‡æ¨¡å‹æ€§èƒ½ï¼Œå¯¹åŒ»å­¦å½±åƒç­‰é¢†åŸŸçš„æ•°æ®å…±äº«å…·æœ‰é‡è¦æ„ä¹‰ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.17975">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-5af4954560077d553135c547a8b81591.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f61ef801244680859ae71b2489a6ce62.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-27c3cc84d7b5007ec2670e9595bf29d4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c27d96314c1811e2fa12cfb385384903.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Programmable-Room-Interactive-Textured-3D-Room-Meshes-Generation-Empowered-by-Large-Language-Models"><a href="#Programmable-Room-Interactive-Textured-3D-Room-Meshes-Generation-Empowered-by-Large-Language-Models" class="headerlink" title="Programmable-Room: Interactive Textured 3D Room Meshes Generation   Empowered by Large Language Models"></a>Programmable-Room: Interactive Textured 3D Room Meshes Generation   Empowered by Large Language Models</h2><p><strong>Authors:Jihyun Kim, Junho Park, Kyeongbo Kong, Suk-Ju Kang</strong></p>
<p>We present Programmable-Room, a framework which interactively generates and edits a 3D room mesh, given natural language instructions. For precise control of a roomâ€™s each attribute, we decompose the challenging task into simpler steps such as creating plausible 3D coordinates for room meshes, generating panorama images for the texture, constructing 3D meshes by integrating the coordinates and panorama texture images, and arranging furniture. To support the various decomposed tasks with a unified framework, we incorporate visual programming (VP). VP is a method that utilizes a large language model (LLM) to write a Python-like program which is an ordered list of necessary modules for the various tasks given in natural language. We develop most of the modules. Especially, for the texture generating module, we utilize a pretrained large-scale diffusion model to generate panorama images conditioned on text and visual prompts (i.e., layout, depth, and semantic map) simultaneously. Specifically, we enhance the panorama image generation quality by optimizing the training objective with a 1D representation of a panorama scene obtained from bidirectional LSTM. We demonstrate Programmable-Roomâ€™s flexibility in generating and editing 3D room meshes, and prove our frameworkâ€™s superiority to an existing model quantitatively and qualitatively. Project page is available in <a target="_blank" rel="noopener" href="https://jihyun0510.github.io/Programmable_Room_Page/">https://jihyun0510.github.io/Programmable_Room_Page/</a>. </p>
<blockquote>
<p>æˆ‘ä»¬æ¨å‡ºäº†Programmable-Roomï¼Œè¿™æ˜¯ä¸€ä¸ªæ¡†æ¶ï¼Œå¯ä»¥æ ¹æ®è‡ªç„¶è¯­è¨€æŒ‡ä»¤äº¤äº’åœ°ç”Ÿæˆå’Œç¼–è¾‘3Dæˆ¿é—´ç½‘æ ¼ã€‚ä¸ºäº†ç²¾ç¡®æ§åˆ¶æˆ¿é—´çš„æ¯ä¸ªå±æ€§ï¼Œæˆ‘ä»¬å°†å¤æ‚çš„ä»»åŠ¡åˆ†è§£ä¸ºæ›´ç®€å•çš„æ­¥éª¤ï¼Œä¾‹å¦‚ä¸ºæˆ¿é—´ç½‘æ ¼åˆ›å»ºåˆç†çš„3Dåæ ‡ã€ä¸ºçº¹ç†ç”Ÿæˆå…¨æ™¯å›¾åƒã€é€šè¿‡æ•´åˆåæ ‡å’Œå…¨æ™¯çº¹ç†å›¾åƒæ„å»º3Dç½‘æ ¼ï¼Œä»¥åŠå¸ƒç½®å®¶å…·ã€‚ä¸ºäº†ç”¨ä¸€ä¸ªç»Ÿä¸€çš„æ¡†æ¶æ¥æ”¯æŒå„ç§åˆ†è§£ä»»åŠ¡ï¼Œæˆ‘ä»¬å¼•å…¥äº†å¯è§†åŒ–ç¼–ç¨‹ï¼ˆVPï¼‰ã€‚VPæ˜¯ä¸€ç§åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ¥ç¼–å†™ç±»ä¼¼äºPythonçš„ç¨‹åºçš„æ–¹æ³•ï¼Œè¯¥ç¨‹åºæ˜¯æ ¹æ®è‡ªç„¶è¯­è¨€ç»™å‡ºçš„å„ç§ä»»åŠ¡æ‰€éœ€çš„å¿…è¦æ¨¡å—çš„æœ‰åºåˆ—è¡¨ã€‚æˆ‘ä»¬å¼€å‘äº†å¤§éƒ¨åˆ†çš„æ¨¡å—ã€‚ç‰¹åˆ«æ˜¯çº¹ç†ç”Ÿæˆæ¨¡å—ï¼Œæˆ‘ä»¬åˆ©ç”¨é¢„è®­ç»ƒçš„å¤§å‹æ‰©æ•£æ¨¡å‹æ¥æ ¹æ®æ–‡æœ¬å’Œè§†è§‰æç¤ºï¼ˆå³å¸ƒå±€ã€æ·±åº¦å’Œè¯­ä¹‰åœ°å›¾ï¼‰åŒæ—¶ç”Ÿæˆå…¨æ™¯å›¾åƒã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬é€šè¿‡ä¼˜åŒ–è®­ç»ƒç›®æ ‡ï¼Œä½¿ç”¨ä¸€ä¸ªä»åŒå‘LSTMè·å¾—çš„å…¨æ™¯åœºæ™¯çš„ä¸€ç»´è¡¨ç¤ºï¼Œæé«˜äº†å…¨æ™¯å›¾åƒç”Ÿæˆçš„å“è´¨ã€‚æˆ‘ä»¬å±•ç¤ºäº†Programmable-Roomåœ¨ç”Ÿæˆå’Œç¼–è¾‘3Dæˆ¿é—´ç½‘æ ¼æ–¹é¢çš„çµæ´»æ€§ï¼Œå¹¶ä»å®šé‡å’Œå®šæ€§çš„è§’åº¦è¯æ˜äº†æˆ‘ä»¬çš„æ¡†æ¶å¯¹ç°æœ‰æ¨¡å‹çš„ä¼˜è¶Šæ€§ã€‚é¡¹ç›®é¡µé¢å¯åœ¨<a target="_blank" rel="noopener" href="https://jihyun0510.github.io/Programmable_Room_Page/%E8%AE%BF%E9%97%AE%E3%80%82">https://jihyun0510.github.io/Programmable_Room_Page&#x2F;è®¿é—®ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.17707v1">PDF</a> Accepted by IEEE Transactions on Multimedia</p>
<p><strong>Summary</strong></p>
<p>è¯¥ç¨‹åºæå‡ºäº†ä¸€ä¸ªåä¸ºProgrammable-Roomçš„æ¡†æ¶ï¼Œè¯¥æ¡†æ¶èƒ½å¤Ÿæ ¹æ®è‡ªç„¶è¯­è¨€æŒ‡ä»¤äº¤äº’å¼åœ°ç”Ÿæˆå’Œç¼–è¾‘3Dæˆ¿é—´ç½‘æ ¼ã€‚é€šè¿‡åˆ†è§£ä»»åŠ¡ï¼Œå®ç°å¯¹æˆ¿é—´æ¯ä¸ªå±æ€§çš„ç²¾ç¡®æ§åˆ¶ï¼Œå¦‚ç”Ÿæˆæˆ¿é—´ç½‘æ ¼çš„3Dåæ ‡ã€çº¹ç†çš„å…¨æ™¯å›¾åƒç”Ÿæˆã€ç»“åˆåæ ‡å’Œå…¨æ™¯çº¹ç†å›¾åƒæ„å»º3Dç½‘æ ¼ä»¥åŠå®¶å…·å¸ƒç½®ç­‰ã€‚ä¸ºæ”¯æŒå„é¡¹åˆ†è§£ä»»åŠ¡ï¼Œè¯¥ç¨‹åºç»“åˆäº†å¯è§†åŒ–ç¼–ç¨‹ï¼ˆVPï¼‰æŠ€æœ¯ã€‚VPåˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ç¼–å†™Pythonç±»ç¨‹åºï¼Œä»¥æœ‰åºåˆ—è¡¨çš„å½¢å¼å‘ˆç°å„ç§ä»»åŠ¡æ‰€éœ€çš„æ¨¡å—ï¼Œå¹¶æ ¹æ®è‡ªç„¶è¯­è¨€è¿›è¡Œåˆ’åˆ†ã€‚è¯¥ç¨‹åºå¤§éƒ¨åˆ†æ¨¡å—ä¸ºè‡ªä¸»å¼€å‘ï¼Œå…¶ä¸­çº¹ç†ç”Ÿæˆæ¨¡å—åˆ©ç”¨é¢„è®­ç»ƒçš„å¤§å‹æ‰©æ•£æ¨¡å‹ï¼Œæ ¹æ®æ–‡æœ¬å’Œè§†è§‰æç¤ºï¼ˆå¦‚å¸ƒå±€ã€æ·±åº¦å’Œè¯­ä¹‰åœ°å›¾ï¼‰ç”Ÿæˆå…¨æ™¯å›¾åƒã€‚é€šè¿‡ä¼˜åŒ–è®­ç»ƒç›®æ ‡ï¼Œæé«˜äº†å…¨æ™¯å›¾åƒç”Ÿæˆè´¨é‡ã€‚Programmable-Roomå±•ç°äº†ç”Ÿæˆå’Œç¼–è¾‘3Dæˆ¿é—´ç½‘æ ¼çš„çµæ´»æ€§ï¼Œå¹¶åœ¨å®šé‡å’Œå®šæ€§æ–¹é¢è¯æ˜äº†å…¶æ¡†æ¶çš„ä¼˜è¶Šæ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Programmable-Roomæ¡†æ¶èƒ½åŸºäºè‡ªç„¶è¯­è¨€æŒ‡ä»¤ç”Ÿæˆå’Œç¼–è¾‘3Dæˆ¿é—´ç½‘æ ¼ã€‚</li>
<li>é€šè¿‡åˆ†è§£ä»»åŠ¡ï¼Œæ¡†æ¶èƒ½ç²¾ç¡®æ§åˆ¶æˆ¿é—´çš„æ¯ä¸ªå±æ€§ï¼ŒåŒ…æ‹¬3Dåæ ‡ã€çº¹ç†å…¨æ™¯å›¾åƒã€3Dç½‘æ ¼æ„å»ºå’Œå®¶å…·å¸ƒå±€ã€‚</li>
<li>æ¡†æ¶ç»“åˆäº†å¯è§†åŒ–ç¼–ç¨‹æŠ€æœ¯ï¼Œåˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹å¤„ç†è‡ªç„¶è¯­è¨€æŒ‡ä»¤ã€‚</li>
<li>çº¹ç†ç”Ÿæˆæ¨¡å—é‡‡ç”¨é¢„è®­ç»ƒçš„å¤§å‹æ‰©æ•£æ¨¡å‹ï¼Œèƒ½åŸºäºæ–‡æœ¬å’Œè§†è§‰æç¤ºç”Ÿæˆå…¨æ™¯å›¾åƒã€‚</li>
<li>é€šè¿‡ä¼˜åŒ–è®­ç»ƒç›®æ ‡ï¼Œæé«˜äº†å…¨æ™¯å›¾åƒç”Ÿæˆçš„å“è´¨ã€‚</li>
<li>Programmable-Roomå±•ç°äº†ç”Ÿæˆå’Œç¼–è¾‘3Dæˆ¿é—´ç½‘æ ¼çš„çµæ´»æ€§ã€‚</li>
<li>ä¸ç°æœ‰æ¨¡å‹ç›¸æ¯”ï¼ŒProgrammable-Roomæ¡†æ¶åœ¨å®šé‡å’Œå®šæ€§æ–¹é¢è¡¨ç°å‡ºä¼˜è¶Šæ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.17707">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-d0e2abd2543a6f6a300e18632be1442b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a16e81e3f814239ee777d4a2764af714.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-5c81cd3e186c4be2de7d124013f8093f.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-13958b7a1b2b043a9733be7c4461dac6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-941e380c9e8f6cccf14f098a342543ee.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-14c30665d0e6ee88633a4080ff700f83.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="DreamJourney-Perpetual-View-Generation-with-Video-Diffusion-Models"><a href="#DreamJourney-Perpetual-View-Generation-with-Video-Diffusion-Models" class="headerlink" title="DreamJourney: Perpetual View Generation with Video Diffusion Models"></a>DreamJourney: Perpetual View Generation with Video Diffusion Models</h2><p><strong>Authors:Bo Pan, Yang Chen, Yingwei Pan, Ting Yao, Wei Chen, Tao Mei</strong></p>
<p>Perpetual view generation aims to synthesize a long-term video corresponding to an arbitrary camera trajectory solely from a single input image. Recent methods commonly utilize a pre-trained text-to-image diffusion model to synthesize new content of previously unseen regions along camera movement. However, the underlying 2D diffusion model lacks 3D awareness and results in distorted artifacts. Moreover, they are limited to generating views of static 3D scenes, neglecting to capture object movements within the dynamic 4D world. To alleviate these issues, we present DreamJourney, a two-stage framework that leverages the world simulation capacity of video diffusion models to trigger a new perpetual scene view generation task with both camera movements and object dynamics. Specifically, in stage I, DreamJourney first lifts the input image to 3D point cloud and renders a sequence of partial images from a specific camera trajectory. A video diffusion model is then utilized as generative prior to complete the missing regions and enhance visual coherence across the sequence, producing a cross-view consistent video adheres to the 3D scene and camera trajectory. Meanwhile, we introduce two simple yet effective strategies (early stopping and view padding) to further stabilize the generation process and improve visual quality. Next, in stage II, DreamJourney leverages a multimodal large language model to produce a text prompt describing object movements in current view, and uses video diffusion model to animate current view with object movements. Stage I and II are repeated recurrently, enabling perpetual dynamic scene view generation. Extensive experiments demonstrate the superiority of our DreamJourney over state-of-the-art methods both quantitatively and qualitatively. Our project page: <a target="_blank" rel="noopener" href="https://dream-journey.vercel.app/">https://dream-journey.vercel.app</a>. </p>
<blockquote>
<p>æŒç»­è§†å›¾ç”Ÿæˆæ—¨åœ¨ä»å•ä¸ªè¾“å…¥å›¾åƒåˆæˆä¸ä»»æ„ç›¸æœºè½¨è¿¹ç›¸å¯¹åº”çš„é•¿è§†é¢‘ã€‚æœ€è¿‘çš„æ–¹æ³•é€šå¸¸åˆ©ç”¨é¢„å…ˆè®­ç»ƒçš„æ–‡æœ¬åˆ°å›¾åƒçš„æ‰©æ•£æ¨¡å‹ï¼Œæ¥åˆæˆç›¸æœºç§»åŠ¨æ—¶å…ˆå‰æœªè§åŒºåŸŸçš„æ–°å†…å®¹ã€‚ç„¶è€Œï¼ŒåŸºæœ¬çš„äºŒç»´æ‰©æ•£æ¨¡å‹ç¼ºä¹ä¸‰ç»´æ„è¯†ï¼Œå¯¼è‡´å¤±çœŸä¼ªå½±ã€‚æ­¤å¤–ï¼Œå®ƒä»¬ä»…é™äºç”Ÿæˆé™æ€ä¸‰ç»´åœºæ™¯çš„è§‚ç‚¹ï¼Œå¿½ç•¥äº†æ•æ‰åŠ¨æ€å››ç»´ä¸–ç•Œä¸­çš„ç‰©ä½“è¿åŠ¨ã€‚ä¸ºäº†ç¼“è§£è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æ¨å‡ºäº†DreamJourneyï¼Œè¿™æ˜¯ä¸€ä¸ªä¸¤é˜¶æ®µæ¡†æ¶ï¼Œåˆ©ç”¨è§†é¢‘æ‰©æ•£æ¨¡å‹çš„ä¸–ç•Œæ¨¡æ‹Ÿèƒ½åŠ›ï¼Œè§¦å‘æ–°çš„æŒç»­åœºæ™¯è§†å›¾ç”Ÿæˆä»»åŠ¡ï¼ŒåŒ…æ‹¬ç›¸æœºè¿åŠ¨å’Œç‰©ä½“åŠ¨åŠ›å­¦ã€‚å…·ä½“æ¥è¯´ï¼Œåœ¨ç¬¬ä¸€é˜¶æ®µï¼ŒDreamJourneyé¦–å…ˆå°†è¾“å…¥å›¾åƒæå‡åˆ°ä¸‰ç»´ç‚¹äº‘ï¼Œå¹¶ä»ç‰¹å®šçš„ç›¸æœºè½¨è¿¹æ¸²æŸ“ä¸€ç³»åˆ—éƒ¨åˆ†å›¾åƒã€‚è§†é¢‘æ‰©æ•£æ¨¡å‹è¢«ç”¨ä½œç”Ÿæˆå…ˆéªŒï¼Œä»¥å®Œæˆç¼ºå¤±çš„åŒºåŸŸï¼Œå¢å¼ºåºåˆ—ä¸­çš„è§†è§‰è¿è´¯æ€§ï¼Œäº§ç”Ÿç¬¦åˆä¸‰ç»´åœºæ™¯å’Œç›¸æœºè½¨è¿¹çš„è·¨è§†å›¾ä¸€è‡´è§†é¢‘ã€‚åŒæ—¶ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸¤ä¸ªç®€å•æœ‰æ•ˆçš„ç­–ç•¥ï¼ˆæ—©æœŸåœæ­¢å’Œè§†å›¾å¡«å……ï¼‰æ¥è¿›ä¸€æ­¥ç¨³å®šç”Ÿæˆè¿‡ç¨‹ï¼Œæé«˜è§†è§‰è´¨é‡ã€‚æ¥ä¸‹æ¥ï¼Œåœ¨ç¬¬äºŒé˜¶æ®µï¼ŒDreamJourneyåˆ©ç”¨å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ç”Ÿæˆæè¿°å½“å‰è§†å›¾ä¸­ç‰©ä½“è¿åŠ¨çš„æ–‡æœ¬æç¤ºï¼Œå¹¶ä½¿ç”¨è§†é¢‘æ‰©æ•£æ¨¡å‹ä½¿å½“å‰è§†å›¾ä¸ç‰©ä½“è¿åŠ¨ç›¸ç»“åˆã€‚é˜¶æ®µä¸€å’Œé˜¶æ®µäºŒåå¤è¿›è¡Œï¼Œå®ç°äº†æŒç»­çš„åŠ¨æ€åœºæ™¯è§†å›¾ç”Ÿæˆã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„DreamJourneyåœ¨å®šé‡å’Œå®šæ€§ä¸Šå‡ä¼˜äºæœ€å…ˆè¿›çš„æ–¹æ³•ã€‚æˆ‘ä»¬çš„é¡¹ç›®é¡µé¢ï¼š<a target="_blank" rel="noopener" href="https://dream-journey.vercel.app./">https://dream-journey.vercel.appã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.17705v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>è¯¥æ–‡ä»‹ç»äº†åˆ©ç”¨è§†é¢‘æ‰©æ•£æ¨¡å‹è¿›è¡Œæ— é™åŠ¨æ€åœºæ™¯è§†å›¾ç”Ÿæˆçš„æŠ€æœ¯ã€‚é¦–å…ˆé€šè¿‡é˜¶æ®µä¸€å°†è¾“å…¥å›¾åƒæå‡åˆ°ä¸‰ç»´ç‚¹äº‘ï¼Œå¹¶ä»ç‰¹å®šç›¸æœºè½¨è¿¹æ¸²æŸ“ä¸€ç³»åˆ—éƒ¨åˆ†å›¾åƒï¼Œä½¿ç”¨è§†é¢‘æ‰©æ•£æ¨¡å‹å®Œæˆç¼ºå¤±åŒºåŸŸå¹¶å¢å¼ºåºåˆ—çš„è§†è§‰è¿è´¯æ€§ï¼Œäº§ç”Ÿéµå¾ªä¸‰ç»´åœºæ™¯å’Œç›¸æœºè½¨è¿¹çš„è·¨è§†å›¾ä¸€è‡´è§†é¢‘ã€‚é˜¶æ®µäºŒåˆ™åˆ©ç”¨å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ç”Ÿæˆæè¿°å½“å‰è§†å›¾å¯¹è±¡è¿åŠ¨çš„æ–‡æœ¬æç¤ºï¼Œå¹¶ä½¿ç”¨è§†é¢‘æ‰©æ•£æ¨¡å‹è¿›è¡ŒåŠ¨ç”»ç”Ÿæˆã€‚ä¸¤ä¸ªé˜¶æ®µçš„é‡å¤è¿›è¡Œï¼Œå®ç°äº†æŒç»­çš„åŠ¨æ€åœºæ™¯è§†å›¾ç”Ÿæˆã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è¯¥æŠ€æœ¯åˆ©ç”¨è§†é¢‘æ‰©æ•£æ¨¡å‹åˆæˆä¸ç›¸æœºè½¨è¿¹ç›¸å¯¹åº”çš„é•¿è§†é¢‘ï¼Œä»…ä»å•å¼ è¾“å…¥å›¾åƒå¼€å§‹ã€‚</li>
<li>ç°æœ‰æ–¹æ³•ä¸»è¦ä½¿ç”¨é¢„è®­ç»ƒçš„æ–‡æœ¬åˆ°å›¾åƒçš„æ‰©æ•£æ¨¡å‹æ¥åˆæˆæ–°çš„å†…å®¹ï¼Œä½†ç¼ºä¹ä¸‰ç»´æ„è¯†ï¼Œå¯¼è‡´å¤±çœŸå’Œå¿½ç•¥å¯¹è±¡åŠ¨æ€çš„é—®é¢˜ã€‚</li>
<li>æå‡ºçš„DreamJourneyæ¡†æ¶åŒ…å«ä¸¤ä¸ªé˜¶æ®µï¼Œç¬¬ä¸€é˜¶æ®µå°†è¾“å…¥å›¾åƒæå‡åˆ°ä¸‰ç»´ç‚¹äº‘å¹¶æ¸²æŸ“ä¸€ç³»åˆ—éƒ¨åˆ†å›¾åƒï¼Œç„¶åä½¿ç”¨è§†é¢‘æ‰©æ•£æ¨¡å‹å®Œæˆç¼ºå¤±åŒºåŸŸå¹¶å¢å¼ºè§†è§‰è¿è´¯æ€§ã€‚</li>
<li>ç¬¬äºŒé˜¶æ®µåˆ©ç”¨å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹å’Œè§†é¢‘æ‰©æ•£æ¨¡å‹æ¥ç”Ÿæˆå’ŒåŠ¨ç”»åŒ–å½“å‰è§†å›¾çš„å¯¹è±¡è¿åŠ¨ã€‚</li>
<li>é€šè¿‡é‡å¤ä¸¤ä¸ªé˜¶æ®µï¼ŒDreamJourneyèƒ½å®ç°æŒç»­çš„åŠ¨æ€åœºæ™¯è§†å›¾ç”Ÿæˆã€‚</li>
<li>å¼•å…¥ä¸¤ç§ç­–ç•¥ï¼ˆæ—©æœŸåœæ­¢å’Œè§†å›¾å¡«å……ï¼‰æ¥è¿›ä¸€æ­¥ç¨³å®šç”Ÿæˆè¿‡ç¨‹å¹¶æé«˜è§†è§‰è´¨é‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.17705">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-6c2919b830ab2a8caa12b74ce28ca9f8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1e35109d87cff1f0d26dd0ad24e3ecb3.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-cd8bb0c953271c1e1cc62ec179c25dff.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d40e4897bbc16bb580930975b4a567a5.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Exploring-Strategies-for-Personalized-Radiation-Therapy-Part-II-Predicting-Tumor-Drift-Patterns-with-Diffusion-Models"><a href="#Exploring-Strategies-for-Personalized-Radiation-Therapy-Part-II-Predicting-Tumor-Drift-Patterns-with-Diffusion-Models" class="headerlink" title="Exploring Strategies for Personalized Radiation Therapy Part II   Predicting Tumor Drift Patterns with Diffusion Models"></a>Exploring Strategies for Personalized Radiation Therapy Part II   Predicting Tumor Drift Patterns with Diffusion Models</h2><p><strong>Authors:Hao Peng, Steve Jiang, Robert Timmerman</strong></p>
<p>Radiation therapy outcomes are decided by two key parameters, dose and timing, whose best values vary substantially across patients. This variability is especially critical in the treatment of brain cancer, where fractionated or staged stereotactic radiosurgery improves safety compared to single fraction approaches, but complicates the ability to predict treatment response. To address this challenge, we employ Personalized Ultra-fractionated Stereotactic Adaptive Radiotherapy (PULSAR), a strategy that dynamically adjusts treatment based on how each tumor evolves over time. However, the success of PULSAR and other adaptive approaches depends on predictive tools that can guide early treatment decisions and avoid both overtreatment and undertreatment. However, current radiomics and dosiomics models offer limited insight into the evolving spatial and temporal patterns of tumor response. To overcome these limitations, we propose a novel framework using Denoising Diffusion Implicit Models (DDIM), which learns data-driven mappings from pre to post treatment imaging. In this study, we developed single step and iterative denoising strategies and compared their performance. The results show that diffusion models can effectively simulate patient specific tumor evolution and localize regions associated with treatment response. The proposed strategy provides a promising foundation for modeling heterogeneous treatment response and enabling early, adaptive interventions, paving the way toward more personalized and biologically informed radiotherapy. </p>
<blockquote>
<p>æ”¾ç–—æ•ˆæœç”±å‰‚é‡å’Œæ—¶é—´ä¸¤ä¸ªå…³é”®å‚æ•°å†³å®šï¼Œæœ€ä½³å€¼åœ¨ä¸åŒæ‚£è€…ä¹‹é—´æœ‰å¾ˆå¤§çš„å·®å¼‚ã€‚è¿™ç§å·®å¼‚åœ¨è„‘ç™Œçš„æ²»ç–—ä¸­å°¤å…¶å…³é”®ï¼Œåˆ†æ®µæˆ–åˆ†æœŸç«‹ä½“å®šå‘æ”¾å°„æ‰‹æœ¯ä¸å•æ¬¡åˆ†å‰²æ–¹æ³•ç›¸æ¯”æé«˜äº†å®‰å…¨æ€§ï¼Œä½†é¢„æµ‹æ²»ç–—ååº”çš„éš¾åº¦å¢åŠ ã€‚ä¸ºäº†åº”å¯¹è¿™ä¸€æŒ‘æˆ˜ï¼Œæˆ‘ä»¬é‡‡ç”¨äº†ä¸ªæ€§åŒ–è¶…åˆ†æ®µç«‹ä½“å®šå‘è‡ªé€‚åº”æ”¾ç–—ï¼ˆPULSARï¼‰ç­–ç•¥ï¼Œè¯¥ç­–ç•¥æ ¹æ®æ¯ä¸ªè‚¿ç˜¤çš„éšæ—¶é—´æ¼”å˜æƒ…å†µåŠ¨æ€è°ƒæ•´æ²»ç–—ã€‚ç„¶è€Œï¼ŒPULSARå’Œå…¶ä»–è‡ªé€‚åº”æ–¹æ³•çš„æˆåŠŸå–å†³äºèƒ½å¤ŸæŒ‡å¯¼æ—©æœŸæ²»ç–—å†³ç­–å¹¶é¿å…è¿‡åº¦æ²»ç–—å’Œä¸è¶³æ²»ç–—çš„é¢„æµ‹å·¥å…·ã€‚ç„¶è€Œï¼Œå½“å‰çš„æ”¾å°„ç»„å­¦å’Œå‰‚é‡ç»„å­¦æ¨¡å‹å¯¹è‚¿ç˜¤ååº”ä¸æ–­å˜åŒ–çš„æ—¶ç©ºæ¨¡å¼æä¾›çš„æ´å¯Ÿæœ‰é™ã€‚ä¸ºäº†å…‹æœè¿™äº›é™åˆ¶ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ä½¿ç”¨å»å™ªæ‰©æ•£éšå¼æ¨¡å‹ï¼ˆDDIMï¼‰çš„æ–°æ¡†æ¶ï¼Œè¯¥æ¡†æ¶å­¦ä¹ ä»æ²»ç–—å‰åˆ°æ²»ç–—åçš„æˆåƒæ•°æ®é©±åŠ¨æ˜ å°„ã€‚åœ¨è¿™é¡¹ç ”ç©¶ä¸­ï¼Œæˆ‘ä»¬å¼€å‘äº†å•æ­¥å’Œè¿­ä»£å»å™ªç­–ç•¥ï¼Œå¹¶æ¯”è¾ƒäº†å®ƒä»¬çš„æ€§èƒ½ã€‚ç»“æœè¡¨æ˜ï¼Œæ‰©æ•£æ¨¡å‹å¯ä»¥æœ‰æ•ˆåœ°æ¨¡æ‹Ÿæ‚£è€…ç‰¹å®šçš„è‚¿ç˜¤æ¼”å˜ï¼Œå¹¶å®šä½ä¸æ²»ç–—ååº”ç›¸å…³çš„åŒºåŸŸã€‚æ‰€æå‡ºçš„ç­–ç•¥ä¸ºæ¨¡æ‹Ÿå¼‚è´¨æ²»ç–—ååº”å’Œå®ç°æ—©æœŸè‡ªé€‚åº”å¹²é¢„æä¾›äº†æœ‰å‰é€”çš„åŸºç¡€ï¼Œä¸ºæ›´ä¸ªæ€§åŒ–å’Œç”Ÿç‰©å­¦ä¿¡æ¯é©±åŠ¨çš„æ”¾å°„æ²»ç–—é“ºå¹³äº†é“è·¯ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.17491v1">PDF</a> </p>
<p><strong>Summary</strong>ï¼š<br>è¾å°„æ²»ç–—ç»“æœå–å†³äºå‰‚é‡å’Œæ—¶é—´ä¸¤ä¸ªå…³é”®å‚æ•°ï¼Œå…¶æœ€ä½³å€¼åœ¨ä¸åŒæ‚£è€…é—´å·®å¼‚æ˜¾è‘—ã€‚åœ¨æ²»ç–—è„‘ç™Œæ—¶ï¼Œè¿™ç§å·®å¼‚æ€§å°¤ä¸ºå…³é”®ã€‚ä¸ºåº”å¯¹æŒ‘æˆ˜ï¼Œæˆ‘ä»¬é‡‡ç”¨ä¸ªæ€§åŒ–è¶…åˆ†å‰²ç«‹ä½“å®šå‘è‡ªé€‚åº”æ”¾ç–—ï¼ˆPULSARï¼‰ç­–ç•¥ï¼Œæ ¹æ®è‚¿ç˜¤éšæ—¶é—´çš„å˜åŒ–åŠ¨æ€è°ƒæ•´æ²»ç–—ã€‚ä½†PULSARç­‰è‡ªé€‚åº”ç­–ç•¥çš„æˆåŠŸå–å†³äºèƒ½æŒ‡å¯¼æ—©æœŸæ²»ç–—å†³ç­–ã€é¿å…è¿‡åº¦æˆ–ä¸è¶³æ²»ç–—çš„é¢„æµ‹å·¥å…·ã€‚å½“å‰æ”¾å°„ç»„å­¦å’Œå‰‚é‡ç»„å­¦æ¨¡å‹å¯¹è‚¿ç˜¤ååº”çš„ç©ºé—´å’Œæ—¶é—´æ¨¡å¼äº†è§£æœ‰é™ã€‚ä¸ºå…‹æœè¿™äº›å±€é™ï¼Œæˆ‘ä»¬æå‡ºä½¿ç”¨å»å™ªæ‰©æ•£éšæ¨¡å‹ï¼ˆDDIMï¼‰çš„æ–°æ¡†æ¶ï¼Œå­¦ä¹ ä»æ²»ç–—å‰åˆ°æ²»ç–—åçš„æˆåƒæ•°æ®é©±åŠ¨æ˜ å°„ã€‚ç ”ç©¶ç»“æœæ˜¾ç¤ºï¼Œæ‰©æ•£æ¨¡å‹èƒ½æœ‰æ•ˆæ¨¡æ‹Ÿæ‚£è€…ç‰¹å®šè‚¿ç˜¤æ¼”å˜ï¼Œå®šä½ä¸æ²»ç–—ååº”ç›¸å…³çš„åŒºåŸŸã€‚è¯¥ç­–ç•¥ä¸ºå®ç°ä¸ªæ€§åŒ–ã€ç”Ÿç‰©å­¦ä¿¡æ¯æ”¯æŒçš„æ”¾å°„æ²»ç–—é“ºå¹³äº†é“è·¯ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>è¾å°„æ²»ç–—ç»“æœå—å‰‚é‡å’Œæ—¶é—´å‚æ•°å½±å“ï¼Œæœ€ä½³å€¼å› æ‚£è€…è€Œå¼‚ã€‚</li>
<li>åœ¨è„‘ç™Œæ²»ç–—ä¸­ï¼Œåˆ†å‰²æˆ–åˆ†æœŸç«‹ä½“å®šå‘æ”¾å°„æ‰‹æœ¯æé«˜äº†å®‰å…¨æ€§ï¼Œä½†é¢„æµ‹æ²»ç–—ååº”çš„èƒ½åŠ›å¤æ‚åŒ–ã€‚</li>
<li>PULSARç­–ç•¥æ ¹æ®è‚¿ç˜¤å˜åŒ–åŠ¨æ€è°ƒæ•´æ²»ç–—ã€‚</li>
<li>å½“å‰æ”¾å°„ç»„å­¦å’Œå‰‚é‡ç»„å­¦æ¨¡å‹å¯¹è‚¿ç˜¤ååº”çš„ç©ºé—´å’Œæ—¶é—´æ¨¡å¼äº†è§£æœ‰é™ã€‚</li>
<li>å»å™ªæ‰©æ•£éšæ¨¡å‹ï¼ˆDDIMï¼‰èƒ½æœ‰æ•ˆæ¨¡æ‹Ÿæ‚£è€…ç‰¹å®šè‚¿ç˜¤æ¼”å˜ã€‚</li>
<li>æ‰©æ•£æ¨¡å‹èƒ½å®šä½ä¸æ²»ç–—ååº”ç›¸å…³çš„åŒºåŸŸã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.17491">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-baa2422c58392beb183525123b6c48ea.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="A-Prior-Guided-Joint-Diffusion-Model-in-Projection-Domain-for-PET-Tracer-Conversion"><a href="#A-Prior-Guided-Joint-Diffusion-Model-in-Projection-Domain-for-PET-Tracer-Conversion" class="headerlink" title="A Prior-Guided Joint Diffusion Model in Projection Domain for PET Tracer   Conversion"></a>A Prior-Guided Joint Diffusion Model in Projection Domain for PET Tracer   Conversion</h2><p><strong>Authors:Fang Chen, Weifeng Zhang, Xingyu Ai, BingXuan Li, An Li, Qiegen Liu</strong></p>
<p>Positron emission tomography (PET) is widely used to assess metabolic activity, but its application is limited by the availability of radiotracers. 18F-labeled fluorodeoxyglucose (18F-FDG) is the most commonly used tracer but shows limited effectiveness for certain tumors. In contrast, 6-18F-fluoro-3,4-dihydroxy-L-phenylalanine (18F-DOPA) offers higher specificity for neuroendocrine tumors and neurological disorders. However, the complexity of its synthesis process and constraints on transportation time have limited its clinical application. Among different forms of raw data acquired by the scanner, sinogram is a commonly used representation in PET imaging. Therefore, modeling in projection domain enables more direct utilization of the original information, potentially reducing the accumulation errors during the image reconstruction process. Inspired by these factors, this study proposes a prior-guided joint diffusion model (PJDM) for transforming 18F-FDG PET sinograms into 18F-DOPA PET sinograms. During inference, an initial synthetic 18F-DOPA PET sinogram is first generated using a higher-order hybrid sampler. This sinogram is then degraded and serves as an additional condition to guide the iterative refinement process. Experimental results demonstrated that PJDM effectively improved both sinogram quality and the final synthetic outcomes. The code is available at: <a target="_blank" rel="noopener" href="https://github.com/yqx7150/PJDM">https://github.com/yqx7150/PJDM</a>. </p>
<blockquote>
<p>æ­£ç”µå­å‘å°„æ–­å±‚æ‰«æï¼ˆPETï¼‰å¹¿æ³›ç”¨äºè¯„ä¼°ä»£è°¢æ´»æ€§ï¼Œä½†å…¶åº”ç”¨å—åˆ°æ”¾å°„æ€§ç¤ºè¸ªå‰‚å¯ç”¨æ€§çš„é™åˆ¶ã€‚18Fæ ‡è®°çš„æ°Ÿè„±æ°§è‘¡è„ç³–ï¼ˆ18F-FDGï¼‰æ˜¯æœ€å¸¸ç”¨çš„ç¤ºè¸ªå‰‚ï¼Œä½†å¯¹äºæŸäº›è‚¿ç˜¤ï¼Œå…¶æ•ˆæœæœ‰é™ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼Œ6-18F-æ°Ÿ-3,4-äºŒç¾ŸåŸº-L-è‹¯ä¸™æ°¨é…¸ï¼ˆ18F-DOPAï¼‰å¯¹ç¥ç»å†…åˆ†æ³Œè‚¿ç˜¤å’Œç¥ç»æ€§ç–¾ç—…å…·æœ‰æ›´é«˜çš„ç‰¹å¼‚æ€§ã€‚ç„¶è€Œï¼Œå…¶åˆæˆè¿‡ç¨‹çš„å¤æ‚æ€§å’Œè¿è¾“æ—¶é—´çš„é™åˆ¶é˜»ç¢äº†å…¶ä¸´åºŠåº”ç”¨çš„å¹¿æ³›å®æ–½ã€‚åœ¨ä¼—å¤šç”±æ‰«æä»ªè·å¾—çš„ä¸åŒå½¢å¼çš„åŸå§‹æ•°æ®ä¸­ï¼Œè¾›æ ¼æ‹‰å§†ï¼ˆsinogramï¼‰æ˜¯PETæˆåƒä¸­å¸¸ç”¨çš„è¡¨ç¤ºæ–¹æ³•ã€‚å› æ­¤ï¼ŒæŠ•å½±åŸŸçš„å»ºæ¨¡èƒ½å¤Ÿæ›´ç›´æ¥åœ°åˆ©ç”¨åŸå§‹ä¿¡æ¯ï¼Œä»è€Œå¯èƒ½å‡å°‘å›¾åƒé‡å»ºè¿‡ç¨‹ä¸­çš„ç´¯ç§¯è¯¯å·®ã€‚å—è¿™äº›å› ç´ å¯å‘ï¼Œæœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§å…ˆéªŒå¼•å¯¼è”åˆæ‰©æ•£æ¨¡å‹ï¼ˆPJDMï¼‰ï¼Œç”¨äºå°†18F-FDG PETè¾›æ ¼æ‹‰å§†è½¬æ¢ä¸º18F-DOPA PETè¾›æ ¼æ‹‰å§†ã€‚åœ¨æ¨ç†è¿‡ç¨‹ä¸­ï¼Œé¦–å…ˆä½¿ç”¨é«˜é˜¶æ··åˆé‡‡æ ·å™¨ç”Ÿæˆåˆå§‹åˆæˆ18F-DOPA PETè¾›æ ¼æ‹‰å§†ã€‚ç„¶åå°†å…¶é€€åŒ–å¹¶ä½œä¸ºé™„åŠ æ¡ä»¶æ¥å¼•å¯¼è¿­ä»£ä¼˜åŒ–è¿‡ç¨‹ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒPJDMæœ‰æ•ˆæé«˜è¾›æ ¼æ‹‰å§†çš„è´¨é‡å’Œæœ€ç»ˆåˆæˆç»“æœã€‚ç›¸å…³ä»£ç å¯é€šè¿‡ä»¥ä¸‹ç½‘å€è·å–ï¼š<a target="_blank" rel="noopener" href="https://github.com/yqx7150/PJDM">https://github.com/yqx7150/PJDM</a> ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.16733v2">PDF</a> </p>
<p><strong>æ‘˜è¦</strong><br>è¯¥æ–‡æœ¬ä»‹ç»äº†ä¸€ç§åŸºäºå…ˆéªŒçŸ¥è¯†çš„è”åˆæ‰©æ•£æ¨¡å‹ï¼ˆPJDMï¼‰ï¼Œç”¨äºå°†18F-FDG PETæ­£å¼¦å›¾è½¬æ¢ä¸º18F-DOPA PETæ­£å¼¦å›¾ã€‚PJDMåœ¨æŠ•å½±åŸŸè¿›è¡Œå»ºæ¨¡ï¼Œç›´æ¥åˆ©ç”¨åŸå§‹ä¿¡æ¯ï¼Œå‡å°‘å›¾åƒé‡å»ºè¿‡ç¨‹ä¸­çš„ç´¯ç§¯è¯¯å·®ã€‚é€šè¿‡åˆå§‹åˆæˆ18F-DOPA PETæ­£å¼¦å›¾ï¼Œç„¶åå°†å…¶é€€åŒ–å¹¶ä½œä¸ºè¿­ä»£ä¼˜åŒ–è¿‡ç¨‹çš„é™„åŠ æ¡ä»¶è¿›è¡Œå¼•å¯¼ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒPJDMèƒ½æœ‰æ•ˆæé«˜æ­£å¼¦å›¾å’Œæœ€ç»ˆåˆæˆç»“æœçš„è´¨é‡ã€‚ç›¸å…³ä»£ç å·²å…¬å¼€äºGitHubä¸Šã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>PETåœ¨è¯„ä¼°ä»£è°¢æ´»æ€§æ–¹é¢åº”ç”¨å¹¿æ³›ï¼Œä½†å—é™äºæ”¾å°„æ€§ç¤ºè¸ªå‰‚çš„å¯ç”¨æ€§ã€‚</li>
<li>18F-FDGæ˜¯æœ€å¸¸ç”¨çš„ç¤ºè¸ªå‰‚ï¼Œä½†å¯¹æŸäº›è‚¿ç˜¤çš„æ•ˆç”¨æœ‰é™ã€‚</li>
<li>ç›¸æ¯”ä¹‹ä¸‹ï¼Œ18F-DOPAå¯¹ç¥ç»å†…åˆ†æ³Œè‚¿ç˜¤å’Œç¥ç»ç³»ç»Ÿç–¾ç—…å…·æœ‰æ›´é«˜çš„ç‰¹å¼‚æ€§ï¼Œä½†ç”±äºåˆæˆè¿‡ç¨‹å¤æ‚åŠè¿è¾“æ—¶é—´é™åˆ¶ï¼Œå…¶ä¸´åºŠåº”ç”¨å—é™ã€‚</li>
<li>ç ”ç©¶æå‡ºäº†ä¸€ç§åŸºäºå…ˆéªŒçŸ¥è¯†çš„è”åˆæ‰©æ•£æ¨¡å‹ï¼ˆPJDMï¼‰ï¼Œç”¨äºè½¬æ¢PETæ­£å¼¦å›¾æ•°æ®ï¼Œä»18F-FDGè½¬æ¢è‡³18F-DOPAã€‚</li>
<li>è¯¥æ¨¡å‹ç›´æ¥åœ¨æŠ•å½±åŸŸå»ºæ¨¡ï¼Œæœ‰æ•ˆåˆ©ç”¨åŸå§‹ä¿¡æ¯ï¼Œå‡å°‘å›¾åƒé‡å»ºè¿‡ç¨‹ä¸­çš„è¯¯å·®ç§¯ç´¯ã€‚</li>
<li>é€šè¿‡åˆå§‹åˆæˆå¹¶é€€åŒ–æ­£å¼¦å›¾æ¥å¼•å¯¼è¿­ä»£ä¼˜åŒ–è¿‡ç¨‹ã€‚</li>
<li>å®éªŒç»“æœè¡¨æ˜PJDMå¯¹æé«˜æ­£å¼¦å›¾å’Œæœ€ç»ˆåˆæˆå›¾åƒè´¨é‡æœ‰æ•ˆï¼Œç›¸å…³ä»£ç å·²å…¬å¼€äºGitHubä¸Šã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.16733">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-78c7bfea79c9107f8a5c0bfe289d7f2e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ba8d250fc2424d48be3a9ab765321bb5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a5b4566556c3c0ebdb91c114b2c2ea67.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c2be39376a8108fc618f8703c6ffca8d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-646ff2a2d0c0164821e1ce7a928e0aa8.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="Joint-Reconstruction-of-Activity-and-Attenuation-in-PET-by-Diffusion-Posterior-Sampling-in-Wavelet-Coefficient-Space"><a href="#Joint-Reconstruction-of-Activity-and-Attenuation-in-PET-by-Diffusion-Posterior-Sampling-in-Wavelet-Coefficient-Space" class="headerlink" title="Joint Reconstruction of Activity and Attenuation in PET by Diffusion   Posterior Sampling in Wavelet Coefficient Space"></a>Joint Reconstruction of Activity and Attenuation in PET by Diffusion   Posterior Sampling in Wavelet Coefficient Space</h2><p><strong>Authors:ClÃ©mentine Phung-Ngoc, Alexandre Bousse, Antoine De Paepe, Hong-Phuong Dang, Olivier Saut, Dimitris Visvikis</strong></p>
<p>Attenuation correction (AC) is necessary for accurate activity quantification in positron emission tomography (PET). Conventional reconstruction methods typically rely on attenuation maps derived from a co-registered computed tomography (CT) or magnetic resonance imaging scan. However, this additional scan may complicate the imaging workflow, introduce misalignment artifacts and increase radiation exposure. In this paper, we propose a joint reconstruction of activity and attenuation (JRAA) approach that eliminates the need for auxiliary anatomical imaging by relying solely on emission data. This framework combines wavelet diffusion model (WDM) and diffusion posterior sampling (DPS) to reconstruct fully three-dimensional (3-D) data. Experimental results show our method outperforms maximum likelihood activity and attenuation (MLAA) and MLAA with UNet-based post processing, and yields high-quality noise-free reconstructions across various count settings when time-of-flight (TOF) information is available. It is also able to reconstruct non-TOF data, although the reconstruction quality significantly degrades in low-count (LC) conditions, limiting its practical effectiveness in such settings. This approach represents a step towards stand-alone PET imaging by reducing the dependence on anatomical modalities while maintaining quantification accuracy, even in low-count scenarios when TOF information is available. </p>
<blockquote>
<p>è¡°å‡æ ¡æ­£ï¼ˆACï¼‰åœ¨æ­£ç”µå­å‘å°„æ–­å±‚æ‰«æï¼ˆPETï¼‰çš„å‡†ç¡®æ´»æ€§å®šé‡ä¸­å¿…ä¸å¯å°‘ã€‚ä¼ ç»Ÿé‡å»ºæ–¹æ³•é€šå¸¸ä¾èµ–äºç”±å…±æ³¨å†Œçš„è®¡ç®—æœºæ–­å±‚æ‰«æï¼ˆCTï¼‰æˆ–ç£å…±æŒ¯æˆåƒæ‰«æå¾—åˆ°çš„è¡°å‡å›¾ã€‚ç„¶è€Œï¼Œè¿™ç§é¢å¤–çš„æ‰«æå¯èƒ½ä¼šä½¿æˆåƒå·¥ä½œæµç¨‹å¤æ‚åŒ–ï¼Œå¼•å…¥é”™ä½ä¼ªå½±å¹¶å¢åŠ è¾å°„æš´éœ²ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§è”åˆé‡å»ºæ´»åŠ¨å’Œè¡°å‡ï¼ˆJRAAï¼‰çš„æ–¹æ³•ï¼Œè¯¥æ–¹æ³•ä»…ä¾é å‘å°„æ•°æ®ï¼Œæ— éœ€è¾…åŠ©è§£å‰–æˆåƒã€‚è¯¥æ¡†æ¶ç»“åˆäº†å°æ³¢æ‰©æ•£æ¨¡å‹ï¼ˆWDMï¼‰å’Œæ‰©æ•£åé‡‡æ ·ï¼ˆDPSï¼‰æ¥é‡å»ºå…¨ä¸‰ç»´ï¼ˆ3-Dï¼‰æ•°æ®ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨æœ‰æ—¶é—´é£è¡Œï¼ˆTOFï¼‰ä¿¡æ¯çš„æƒ…å†µä¸‹ï¼Œæ€§èƒ½ä¼˜äºæœ€å¤§ä¼¼ç„¶æ´»åŠ¨å’Œè¡°å‡ï¼ˆMLAAï¼‰ä»¥åŠåŸºäºUNetçš„åå¤„ç†çš„MLAAï¼Œå¹¶åœ¨å„ç§è®¡æ•°è®¾ç½®ä¸‹å®ç°äº†é«˜è´¨é‡çš„æ— å™ªå£°é‡å»ºã€‚å°½ç®¡åœ¨ä½è®¡æ•°æ¡ä»¶ä¸‹é‡å»ºè´¨é‡æœ‰æ‰€é™ä½ï¼Œä½†è¯¥æ–¹æ³•åœ¨æ— TOFæ•°æ®çš„æƒ…å†µä¸‹ä¹Ÿèƒ½è¿›è¡Œé‡å»ºï¼Œè¿™åœ¨å®è·µä¸­å…·æœ‰ä¸€å®šçš„æœ‰æ•ˆæ€§ã€‚è¿™ç§æ–¹æ³•é€šè¿‡å‡å°‘å¯¹è§£å‰–æ¨¡æ€çš„ä¾èµ–ï¼Œå³ä½¿åœ¨æœ‰æ—¶é—´é£è¡Œä¿¡æ¯çš„æƒ…å†µä¸‹ä½è®¡æ•°åœºæ™¯ä¸­ä¹Ÿèƒ½ä¿æŒé‡åŒ–ç²¾åº¦ï¼Œä»£è¡¨äº†å®ç°ç‹¬ç«‹PETæˆåƒçš„ä¸€æ­¥ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.18782v3">PDF</a> 10 pages, 9 figures, 1 table</p>
<p><strong>Summary</strong><br>     æœ¬æ–‡æå‡ºä¸€ç§è”åˆé‡å»ºæ´»åŠ¨åº¦å’Œè¡°å‡ï¼ˆJRAAï¼‰çš„æ–¹æ³•ï¼Œè¯¥æ–¹æ³•åˆ©ç”¨å‘å°„æ•°æ®ï¼Œæ— éœ€è¾…åŠ©çš„è§£å‰–å­¦æˆåƒã€‚é€šè¿‡ç»“åˆå°æ³¢æ‰©æ•£æ¨¡å‹ï¼ˆWDMï¼‰å’Œæ‰©æ•£åé‡‡æ ·ï¼ˆDPSï¼‰ï¼Œé‡å»ºå‡ºå…¨ä¸‰ç»´æ•°æ®ã€‚å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨æœ‰æ—¶é—´é£è¡Œï¼ˆTOFï¼‰ä¿¡æ¯çš„æƒ…å†µä¸‹ï¼Œåœ¨å¤šç§è®¡æ•°è®¾ç½®ä¸‹è¡¨ç°å‡ºä¼˜äºæœ€å¤§ä¼¼ç„¶æ´»åŠ¨åº¦å’Œè¡°å‡ï¼ˆMLAAï¼‰åŠå…¶UNetåå¤„ç†çš„æ–¹æ³•çš„æ€§èƒ½ï¼Œä¸”èƒ½å¾—åˆ°é«˜è´¨é‡çš„æ— å™ªå£°é‡å»ºã€‚å°½ç®¡è¯¥æ–¹æ³•èƒ½é‡å»ºéTOFæ•°æ®ï¼Œä½†åœ¨ä½è®¡æ•°æ¡ä»¶ä¸‹é‡å»ºè´¨é‡æ˜¾è‘—é™ä½ï¼Œé™åˆ¶äº†å…¶åœ¨è¿™äº›åœºæ™¯ä¸­çš„å®é™…åº”ç”¨æ•ˆæœã€‚æ­¤æ–¹æ³•ä¸ºå®ç°ç‹¬ç«‹PETæˆåƒæä¾›äº†å¯èƒ½æ€§ï¼Œé™ä½äº†å¯¹è§£å‰–å­¦æ¨¡æ€çš„ä¾èµ–ï¼ŒåŒæ—¶ä¿æŒäº†é‡åŒ–å‡†ç¡®æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è¡°å‡æ ¡æ­£ï¼ˆACï¼‰å¯¹äºæ­£ç”µå­å‘å°„æ–­å±‚æ‰«æï¼ˆPETï¼‰ä¸­çš„æ´»åŠ¨é‡åŒ–è‡³å…³é‡è¦ã€‚</li>
<li>ä¼ ç»Ÿé‡å»ºæ–¹æ³•ä¾èµ–äºä»åˆå¹¶çš„è®¡ç®—æœºæ–­å±‚æ‰«æï¼ˆCTï¼‰æˆ–ç£å…±æŒ¯æˆåƒæ‰«æå¾—åˆ°çš„è¡°å‡å›¾ã€‚</li>
<li>æå‡ºçš„è”åˆé‡å»ºæ´»åŠ¨åº¦å’Œè¡°å‡ï¼ˆJRAAï¼‰æ–¹æ³•æ¶ˆé™¤äº†å¯¹è¾…åŠ©è§£å‰–å­¦æˆåƒçš„éœ€æ±‚ï¼Œä»…ä¾èµ–å‘å°„æ•°æ®ã€‚</li>
<li>JRAAæ–¹æ³•ç»“åˆå°æ³¢æ‰©æ•£æ¨¡å‹å’Œæ‰©æ•£åé‡‡æ ·è¿›è¡Œå…¨ä¸‰ç»´æ•°æ®é‡å»ºã€‚</li>
<li>åœ¨æœ‰æ—¶é—´é£è¡Œï¼ˆTOFï¼‰ä¿¡æ¯çš„æƒ…å†µä¸‹ï¼ŒJRAAæ–¹æ³•åœ¨å„ç§è®¡æ•°è®¾ç½®ä¸‹è¡¨ç°å‡ºä¼˜è¶Šæ€§èƒ½ã€‚</li>
<li>JRAAæ–¹æ³•åœ¨éTOFæ•°æ®çš„é‡å»ºä¸­ä¹Ÿèƒ½åº”ç”¨ï¼Œä½†åœ¨ä½è®¡æ•°æ¡ä»¶ä¸‹é‡å»ºè´¨é‡ä¸‹é™ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.18782">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-b6ea26bc591260eb3608f7723c3c5179.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-44ede7683cddb537d59457ccedbd19ae.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d656d0ab49e0b73aebe3246052e0a482.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="MDAA-Diff-CT-Guided-Multi-Dose-Adaptive-Attention-Diffusion-Model-for-PET-Denoising"><a href="#MDAA-Diff-CT-Guided-Multi-Dose-Adaptive-Attention-Diffusion-Model-for-PET-Denoising" class="headerlink" title="MDAA-Diff: CT-Guided Multi-Dose Adaptive Attention Diffusion Model for   PET Denoising"></a>MDAA-Diff: CT-Guided Multi-Dose Adaptive Attention Diffusion Model for   PET Denoising</h2><p><strong>Authors:Xiaolong Niu, Zanting Ye, Xu Han, Yanchao Huang, Hao Sun, Hubing Wu, Lijun Lu</strong></p>
<p>Acquiring high-quality Positron Emission Tomography (PET) images requires administering high-dose radiotracers, which increases radiation exposure risks. Generating standard-dose PET (SPET) from low-dose PET (LPET) has become a potential solution. However, previous studies have primarily focused on single low-dose PET denoising, neglecting two critical factors: discrepancies in dose response caused by inter-patient variability, and complementary anatomical constraints derived from CT images. In this work, we propose a novel CT-Guided Multi-dose Adaptive Attention Denoising Diffusion Model (MDAA-Diff) for multi-dose PET denoising. Our approach integrates anatomical guidance and dose-level adaptation to achieve superior denoising performance under low-dose conditions. Specifically, this approach incorporates a CT-Guided High-frequency Wavelet Attention (HWA) module, which uses wavelet transforms to separate high-frequency anatomical boundary features from CT images. These extracted features are then incorporated into PET imaging through an adaptive weighted fusion mechanism to enhance edge details. Additionally, we propose the Dose-Adaptive Attention (DAA) module, a dose-conditioned enhancement mechanism that dynamically integrates dose levels into channel-spatial attention weight calculation. Extensive experiments on 18F-FDG and 68Ga-FAPI datasets demonstrate that MDAA-Diff outperforms state-of-the-art approaches in preserving diagnostic quality under reduced-dose conditions. Our code is publicly available. </p>
<blockquote>
<p>è·å–é«˜è´¨é‡çš„æ­£ç”µå­å‘å°„æ–­å±‚æ‰«æï¼ˆPETï¼‰å›¾åƒéœ€è¦ç»™äºˆé«˜å‰‚é‡æ”¾å°„æ€§ç¤ºè¸ªå‰‚ï¼Œè¿™å¢åŠ äº†è¾å°„æš´éœ²é£é™©ã€‚ä»ä½å‰‚é‡PETï¼ˆLPETï¼‰ç”Ÿæˆæ ‡å‡†å‰‚é‡PETï¼ˆSPETï¼‰å·²æˆä¸ºä¸€ç§å¯èƒ½çš„è§£å†³æ–¹æ¡ˆã€‚ç„¶è€Œï¼Œä»¥å¾€çš„ç ”ç©¶ä¸»è¦é›†ä¸­åœ¨å•ä¸€ä½å‰‚é‡PETå»å™ªä¸Šï¼Œå¿½ç•¥äº†ä¸¤ä¸ªå…³é”®å› ç´ ï¼šç”±æ‚£è€…é—´å·®å¼‚å¼•èµ·çš„å‰‚é‡ååº”å·®å¼‚ï¼Œä»¥åŠä»CTå›¾åƒä¸­å¾—åˆ°çš„äº’è¡¥è§£å‰–çº¦æŸã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°å‹çš„CTå¼•å¯¼å¤šå‰‚é‡è‡ªé€‚åº”æ³¨æ„åŠ›å»å™ªæ‰©æ•£æ¨¡å‹ï¼ˆMDAA-Diffï¼‰ç”¨äºå¤šå‰‚é‡PETå»å™ªã€‚æˆ‘ä»¬çš„æ–¹æ³•ç»“åˆäº†è§£å‰–æŒ‡å¯¼å’Œå‰‚é‡æ°´å¹³é€‚åº”ï¼Œä»¥åœ¨ä½å‰‚é‡æ¡ä»¶ä¸‹å®ç°å“è¶Šçš„å»å™ªæ€§èƒ½ã€‚å…·ä½“æ¥è¯´ï¼Œè¯¥æ–¹æ³•é‡‡ç”¨CTå¼•å¯¼çš„é«˜é¢‘å°æ³¢æ³¨æ„åŠ›ï¼ˆHWAï¼‰æ¨¡å—ï¼Œåˆ©ç”¨å°æ³¢å˜æ¢ä»CTå›¾åƒä¸­åˆ†ç¦»å‡ºé«˜é¢‘è§£å‰–è¾¹ç•Œç‰¹å¾ã€‚è¿™äº›æå–çš„ç‰¹å¾ç„¶åé€šè¿‡è‡ªé€‚åº”åŠ æƒèåˆæœºåˆ¶èå…¥åˆ°PETæˆåƒä¸­ï¼Œä»¥å¢å¼ºè¾¹ç¼˜ç»†èŠ‚ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æå‡ºäº†å‰‚é‡è‡ªé€‚åº”æ³¨æ„åŠ›ï¼ˆDAAï¼‰æ¨¡å—ï¼Œè¿™æ˜¯ä¸€ç§å‰‚é‡è°ƒèŠ‚å¢å¼ºæœºåˆ¶ï¼ŒåŠ¨æ€åœ°å°†å‰‚é‡æ°´å¹³é›†æˆåˆ°é€šé“ç©ºé—´æ³¨æ„åŠ›æƒé‡è®¡ç®—ä¸­ã€‚åœ¨18F-FDGå’Œ68Ga-FAPIæ•°æ®é›†ä¸Šçš„å¤§é‡å®éªŒè¡¨æ˜ï¼ŒMDAA-Diffåœ¨é™ä½å‰‚é‡æ¡ä»¶ä¸‹ä¿æŒè¯Šæ–­è´¨é‡æ–¹é¢ä¼˜äºæœ€å…ˆè¿›çš„æ–¹æ³•ã€‚æˆ‘ä»¬çš„ä»£ç å…¬å¼€å¯ç”¨ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.05112v2">PDF</a> </p>
<p><strong>æ‘˜è¦</strong></p>
<p>è·å–é«˜è´¨é‡çš„å‘å°„å‹è®¡ç®—æœºæ–­å±‚æ‰«æï¼ˆPETï¼‰å›¾åƒéœ€è¦ç»™äºˆå¤§å‰‚é‡æ”¾å°„æ€§ç¤ºè¸ªå‰‚ï¼Œä»è€Œå¢åŠ äº†è¾å°„æš´éœ²é£é™©ã€‚ä½å‰‚é‡PETï¼ˆLPETï¼‰ç”Ÿæˆæ ‡å‡†å‰‚é‡PETï¼ˆSPETï¼‰å·²æˆä¸ºä¸€ç§å¯èƒ½çš„è§£å†³æ–¹æ¡ˆã€‚ç„¶è€Œï¼Œä»¥å‰çš„ç ”ç©¶ä¸»è¦é›†ä¸­åœ¨å•å‰‚é‡PETå»å™ªä¸Šï¼Œå¿½ç•¥äº†ç”±æ‚£è€…é—´å·®å¼‚å¼•èµ·çš„å‰‚é‡ååº”å·®å¼‚ä»¥åŠæ¥è‡ªCTå›¾åƒçš„äº’è¡¥è§£å‰–çº¦æŸè¿™ä¸¤ä¸ªå…³é”®å› ç´ ã€‚åœ¨è¿™é¡¹ç ”ç©¶ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°å‹çš„CTå¼•å¯¼å¤šå‰‚é‡è‡ªé€‚åº”æ³¨æ„åŠ›å»å™ªæ‰©æ•£æ¨¡å‹ï¼ˆMDAA-Diffï¼‰ï¼Œç”¨äºå¤šå‰‚é‡PETå»å™ªã€‚æˆ‘ä»¬çš„æ–¹æ³•ç»“åˆäº†è§£å‰–æŒ‡å¯¼å’Œå‰‚é‡æ°´å¹³è‡ªé€‚åº”ï¼Œä»¥å®ç°ä½å‰‚é‡æ¡ä»¶ä¸‹çš„å“è¶Šå»å™ªæ€§èƒ½ã€‚å…·ä½“æ¥è¯´ï¼Œè¯¥æ–¹æ³•é‡‡ç”¨CTå¼•å¯¼çš„é«˜é¢‘å°æ³¢æ³¨æ„åŠ›ï¼ˆHWAï¼‰æ¨¡å—ï¼Œåˆ©ç”¨å°æ³¢å˜æ¢ä»CTå›¾åƒä¸­åˆ†ç¦»å‡ºé«˜é¢‘è§£å‰–è¾¹ç•Œç‰¹å¾ã€‚è¿™äº›æå–çš„ç‰¹å¾é€šè¿‡è‡ªé€‚åº”åŠ æƒèåˆæœºåˆ¶èå…¥åˆ°PETæˆåƒä¸­ï¼Œä»¥å¢å¼ºè¾¹ç¼˜ç»†èŠ‚ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜æå‡ºäº†å‰‚é‡è‡ªé€‚åº”æ³¨æ„åŠ›ï¼ˆDAAï¼‰æ¨¡å—ï¼Œè¿™æ˜¯ä¸€ç§æ ¹æ®å‰‚é‡æ¡ä»¶å¢å¼ºçš„æœºåˆ¶ï¼ŒåŠ¨æ€åœ°å°†å‰‚é‡æ°´å¹³çº³å…¥é€šé“ç©ºé—´æ³¨æ„åŠ›æƒé‡è®¡ç®—ã€‚åœ¨18F-FDGå’Œ68Ga-FAPIæ•°æ®é›†ä¸Šçš„å¤§é‡å®éªŒè¡¨æ˜ï¼ŒMDAA-Diffåœ¨é™ä½å‰‚é‡æ¡ä»¶ä¸‹ä¿ç•™è¯Šæ–­è´¨é‡æ–¹é¢ä¼˜äºç°æœ‰æŠ€æœ¯ã€‚æˆ‘ä»¬çš„ä»£ç å·²å…¬å¼€å¯ç”¨ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>é«˜å‰‚é‡æ”¾å°„æ€§ç¤ºè¸ªå‰‚åœ¨PETæˆåƒä¸­çš„ä½¿ç”¨å¢åŠ äº†è¾å°„æš´éœ²é£é™©ï¼Œæå‡ºé€šè¿‡ç”Ÿæˆæ ‡å‡†å‰‚é‡PETï¼ˆSPETï¼‰æ¥è§£å†³è¿™ä¸€é—®é¢˜ã€‚</li>
<li>ç°æœ‰ç ”ç©¶ä¸»è¦å…³æ³¨å•ä¸€ä½å‰‚é‡PETå»å™ªï¼Œå¿½ç•¥äº†æ‚£è€…é—´å·®å¼‚å¼•èµ·çš„å‰‚é‡ååº”å·®å¼‚å’ŒCTå›¾åƒçš„è§£å‰–çº¦æŸã€‚</li>
<li>æå‡ºäº†ä¸€ç§æ–°å‹çš„CTå¼•å¯¼å¤šå‰‚é‡è‡ªé€‚åº”æ³¨æ„åŠ›å»å™ªæ‰©æ•£æ¨¡å‹ï¼ˆMDAA-Diffï¼‰ï¼Œç»“åˆäº†è§£å‰–æŒ‡å¯¼å’Œå‰‚é‡æ°´å¹³è‡ªé€‚åº”ã€‚</li>
<li>é‡‡ç”¨CTå¼•å¯¼çš„é«˜é¢‘å°æ³¢æ³¨æ„åŠ›ï¼ˆHWAï¼‰æ¨¡å—ï¼Œé€šè¿‡å°æ³¢å˜æ¢æå–CTå›¾åƒä¸­çš„é«˜é¢‘è§£å‰–è¾¹ç•Œç‰¹å¾ï¼Œå¹¶å°†å…¶èå…¥PETæˆåƒä¸­å¢å¼ºè¾¹ç¼˜ç»†èŠ‚ã€‚</li>
<li>æå‡ºå‰‚é‡è‡ªé€‚åº”æ³¨æ„åŠ›ï¼ˆDAAï¼‰æ¨¡å—ï¼ŒåŠ¨æ€ç»“åˆå‰‚é‡æ°´å¹³è¿›è¡Œè®¡ç®—ï¼Œå¢å¼ºå»å™ªæ•ˆæœã€‚</li>
<li>åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒMDAA-Diffåœ¨é™ä½å‰‚é‡æ¡ä»¶ä¸‹èƒ½ä¿ç•™è¯Šæ–­è´¨é‡ï¼Œä¼˜äºç°æœ‰æŠ€æœ¯ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.05112">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-33da8d114b4aeb60ab3d1ec7cffd13b4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-72fb0ab82d2eb4551957a3c0446571de.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-4c8b8e643f982798fdc7076b26c61567.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="InstructAttribute-Fine-grained-Object-Attributes-editing-with-Instruction"><a href="#InstructAttribute-Fine-grained-Object-Attributes-editing-with-Instruction" class="headerlink" title="InstructAttribute: Fine-grained Object Attributes editing with   Instruction"></a>InstructAttribute: Fine-grained Object Attributes editing with   Instruction</h2><p><strong>Authors:Xingxi Yin, Jingfeng Zhang, Yue Deng, Zhi Li, Yicheng Li, Yin Zhang</strong></p>
<p>Text-to-image (T2I) diffusion models are widely used in image editing due to their powerful generative capabilities. However, achieving fine-grained control over specific object attributes, such as color and material, remains a considerable challenge. Existing methods often fail to accurately modify these attributes or compromise structural integrity and overall image consistency. To fill this gap, we introduce Structure Preservation and Attribute Amplification (SPAA), a novel training-free framework that enables precise generation of color and material attributes for the same object by intelligently manipulating self-attention maps and cross-attention values within diffusion models. Building on SPAA, we integrate multi-modal large language models (MLLMs) to automate data curation and instruction generation. Leveraging this object attribute data collection engine, we construct the Attribute Dataset, encompassing a comprehensive range of colors and materials across diverse object categories. Using this generated dataset, we propose InstructAttribute, an instruction-tuned model that enables fine-grained and object-level attribute editing through natural language prompts. This capability holds significant practical implications for diverse fields, from accelerating product design and e-commerce visualization to enhancing virtual try-on experiences. Extensive experiments demonstrate that InstructAttribute outperforms existing instruction-based baselines, achieving a superior balance between attribute modification accuracy and structural preservation. </p>
<blockquote>
<p>æ–‡æœ¬åˆ°å›¾åƒï¼ˆT2Iï¼‰æ‰©æ•£æ¨¡å‹ç”±äºå…¶å¼ºå¤§çš„ç”Ÿæˆèƒ½åŠ›è€Œåœ¨å›¾åƒç¼–è¾‘ä¸­å¾—åˆ°äº†å¹¿æ³›åº”ç”¨ã€‚ç„¶è€Œï¼Œå®ç°å¯¹ç‰¹å®šå¯¹è±¡å±æ€§ï¼ˆå¦‚é¢œè‰²å’Œæè´¨ï¼‰çš„ç²¾ç»†æ§åˆ¶ä»ç„¶æ˜¯ä¸€ä¸ªå·¨å¤§çš„æŒ‘æˆ˜ã€‚ç°æœ‰æ–¹æ³•å¾€å¾€æ— æ³•å‡†ç¡®ä¿®æ”¹è¿™äº›å±æ€§ï¼Œæˆ–è€…ä¼šæŸå®³ç»“æ„å®Œæ•´æ€§å’Œæ•´ä½“å›¾åƒä¸€è‡´æ€§ã€‚ä¸ºäº†å¡«è¡¥è¿™ä¸€ç©ºç™½ï¼Œæˆ‘ä»¬å¼•å…¥äº†æ— è®­ç»ƒæ¡†æ¶â€”â€”ç»“æ„ä¿ç•™ä¸å±æ€§æ”¾å¤§ï¼ˆSPAAï¼‰ï¼Œé€šè¿‡æ™ºèƒ½æ“ä½œæ‰©æ•£æ¨¡å‹ä¸­çš„è‡ªæ³¨æ„åŠ›å›¾å’Œè·¨æ³¨æ„åŠ›å€¼ï¼Œå®ç°å¯¹åŒä¸€å¯¹è±¡çš„é¢œè‰²å’Œæè´¨å±æ€§çš„ç²¾ç¡®ç”Ÿæˆã€‚åŸºäºSPAAï¼Œæˆ‘ä»¬é›†æˆäº†å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰æ¥è‡ªåŠ¨è¿›è¡Œæ•°æ®æ•´ç†å’ŒæŒ‡ä»¤ç”Ÿæˆã€‚åˆ©ç”¨è¯¥å¯¹è±¡å±æ€§æ•°æ®æ”¶é›†å¼•æ“ï¼Œæˆ‘ä»¬æ„å»ºäº†å±æ€§æ•°æ®é›†ï¼Œæ¶µç›–äº†å„ç±»å¯¹è±¡ä¸­çš„å¤šç§é¢œè‰²å’Œæè´¨ã€‚ä½¿ç”¨è¿™ä¸ªç”Ÿæˆçš„æ•°æ®é›†ï¼Œæˆ‘ä»¬æå‡ºäº†æŒ‡ä»¤å±æ€§æ¨¡å‹InstructAttributeï¼Œè¿™æ˜¯ä¸€ä¸ªé€šè¿‡è‡ªç„¶è¯­è¨€æç¤ºå®ç°ç²¾ç»†ç²’åº¦å’Œå¯¹è±¡çº§åˆ«çš„å±æ€§ç¼–è¾‘çš„æŒ‡ä»¤è°ƒä¼˜æ¨¡å‹ã€‚è¿™ç§èƒ½åŠ›å¯¹äºåŠ é€Ÿäº§å“è®¾è®¡ã€ç”µå­å•†åŠ¡å¯è§†åŒ–ä»¥åŠå¢å¼ºè™šæ‹Ÿè¯•ç©¿ä½“éªŒç­‰å¤šä¸ªé¢†åŸŸéƒ½å…·æœ‰é‡è¦çš„å®é™…æ„ä¹‰ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼ŒInstructAttributeåœ¨å±æ€§ä¿®æ”¹ç²¾åº¦å’Œç»“æ„ä¿ç•™ä¹‹é—´å–å¾—äº†å‡ºè‰²çš„å¹³è¡¡ï¼Œè¶…è¶Šäº†ç°æœ‰çš„æŒ‡ä»¤åŸºå‡†æ¨¡å‹ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.00751v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æ–‡æœ¬ä»‹ç»äº†é’ˆå¯¹æ–‡æœ¬è½¬å›¾åƒï¼ˆT2Iï¼‰æ‰©æ•£æ¨¡å‹çš„ä¸€é¡¹æ–°æŠ€æœ¯â€”â€”ç»“æ„ä¿ç•™ä¸å±æ€§æ”¾å¤§ï¼ˆSPAAï¼‰ã€‚è¯¥æŠ€æœ¯è§£å†³äº†ç°æœ‰æ–¹æ³•åœ¨ä¿®æ”¹ç‰¹å®šå¯¹è±¡å±æ€§ï¼ˆå¦‚é¢œè‰²å’Œæè´¨ï¼‰æ—¶é¢ä¸´çš„æŒ‘æˆ˜ï¼Œèƒ½å¤Ÿåœ¨ä¸æŸå®³ç»“æ„å®Œæ•´æ€§å’Œæ•´ä½“å›¾åƒä¸€è‡´æ€§çš„æƒ…å†µä¸‹ï¼Œç²¾ç¡®ç”Ÿæˆé¢œè‰²å’Œæè´¨å±æ€§ã€‚æ­¤å¤–ï¼Œè¯¥ç ”ç©¶è¿˜ç»“åˆäº†å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰æ„å»ºäº†å±æ€§æ•°æ®é›†ï¼Œå¹¶æå‡ºäº†åŸºäºæŒ‡ä»¤è°ƒæ ¡çš„æ¨¡å‹InstructAttributeï¼Œå¯å®ç°é€šè¿‡è‡ªç„¶è¯­è¨€æç¤ºè¿›è¡Œç²¾ç»†åŒ–å’Œå¯¹è±¡çº§çš„å±æ€§ç¼–è¾‘ã€‚è¿™é¡¹æŠ€æœ¯åœ¨äº§å“è®¾è®¡ã€ç”µå­å•†åŠ¡å¯è§†åŒ–ä»¥åŠè™šæ‹Ÿè¯•ç©¿ä½“éªŒç­‰é¢†åŸŸå…·æœ‰å®é™…æ„ä¹‰ï¼Œå¹¶åœ¨å®éªŒä¸­è¡¨ç°å‡ºä¼˜äºç°æœ‰æŒ‡ä»¤åŸºå‡†çš„æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>T2Iæ‰©æ•£æ¨¡å‹å¹¿æ³›åº”ç”¨äºå›¾åƒç¼–è¾‘ï¼Œä½†åœ¨ç²¾ç»†æ§åˆ¶ç‰¹å®šå¯¹è±¡å±æ€§æ–¹é¢å­˜åœ¨æŒ‘æˆ˜ã€‚</li>
<li>SPAAæ¡†æ¶è§£å†³äº†ç°æœ‰æ–¹æ³•åœ¨ä¿®æ”¹é¢œè‰²å’Œæè´¨å±æ€§æ–¹é¢çš„ä¸è¶³ï¼Œé€šè¿‡æ™ºèƒ½æ“ä½œæ‰©æ•£æ¨¡å‹ä¸­çš„è‡ªæ³¨æ„åŠ›å›¾å’Œè·¨æ³¨æ„åŠ›å€¼ï¼Œå®ç°ç²¾ç¡®ç”Ÿæˆå±æ€§ã€‚</li>
<li>ç»“åˆMLLMsæ„å»ºå±æ€§æ•°æ®é›†ï¼Œç”¨äºè‡ªåŠ¨åŒ–æ•°æ®æ•´ç†å’ŒæŒ‡ä»¤ç”Ÿæˆã€‚</li>
<li>æå‡ºInstructAttributeæ¨¡å‹ï¼Œé€šè¿‡è‡ªç„¶è¯­è¨€æç¤ºå®ç°ç²¾ç»†åŒ–å’Œå¯¹è±¡çº§çš„å±æ€§ç¼–è¾‘ã€‚</li>
<li>InstructAttributeåœ¨å¹³è¡¡å±æ€§ä¿®æ”¹å‡†ç¡®æ€§å’Œç»“æ„ä¿ç•™æ–¹é¢è¡¨ç°å‡ºä¼˜å¼‚æ€§èƒ½ã€‚</li>
<li>è¯¥æŠ€æœ¯å¯¹äºäº§å“è®¾è®¡ã€ç”µå­å•†åŠ¡å¯è§†åŒ–åŠè™šæ‹Ÿè¯•ç©¿ä½“éªŒç­‰é¢†åŸŸå…·æœ‰å®é™…æ„ä¹‰ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.00751">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-d71edee45541a3c5266dc5b93d026f26.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b955e4ee4e9e6adc35cce6d758e83a4d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-269319fa013775b4924b05fa8ed39030.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-57605603ab8bebde709309f0c1dd4742.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f5cef4073e32a7d768575249995abf59.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-beae8194358085e26d4d8c3ff9f26b7d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5b06582b7db4887a343daf19d43cffd4.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="From-Easy-to-Hard-Building-a-Shortcut-for-Differentially-Private-Image-Synthesis"><a href="#From-Easy-to-Hard-Building-a-Shortcut-for-Differentially-Private-Image-Synthesis" class="headerlink" title="From Easy to Hard: Building a Shortcut for Differentially Private Image   Synthesis"></a>From Easy to Hard: Building a Shortcut for Differentially Private Image   Synthesis</h2><p><strong>Authors:Kecen Li, Chen Gong, Xiaochen Li, Yuzhong Zhao, Xinwen Hou, Tianhao Wang</strong></p>
<p>Differentially private (DP) image synthesis aims to generate synthetic images from a sensitive dataset, alleviating the privacy leakage concerns of organizations sharing and utilizing synthetic images. Although previous methods have significantly progressed, especially in training diffusion models on sensitive images with DP Stochastic Gradient Descent (DP-SGD), they still suffer from unsatisfactory performance. In this work, inspired by curriculum learning, we propose a two-stage DP image synthesis framework, where diffusion models learn to generate DP synthetic images from easy to hard. Unlike existing methods that directly use DP-SGD to train diffusion models, we propose an easy stage in the beginning, where diffusion models learn simple features of the sensitive images. To facilitate this easy stage, we propose to use &#96;central imagesâ€™, simply aggregations of random samples of the sensitive dataset. Intuitively, although those central images do not show details, they demonstrate useful characteristics of all images and only incur minimal privacy costs, thus helping early-phase model training. We conduct experiments to present that on the average of four investigated image datasets, the fidelity and utility metrics of our synthetic images are 33.1% and 2.1% better than the state-of-the-art method. </p>
<blockquote>
<p>å·®åˆ†éšç§ï¼ˆDPï¼‰å›¾åƒåˆæˆæ—¨åœ¨ä»æ•æ„Ÿæ•°æ®é›†ä¸­ç”Ÿæˆåˆæˆå›¾åƒï¼Œä»è€Œå‡è½»ç»„ç»‡å’Œæœºæ„å…±äº«å’Œåˆ©ç”¨åˆæˆå›¾åƒæ—¶çš„éšç§æ³„éœ²æ‹…å¿§ã€‚å°½ç®¡ä¹‹å‰çš„æ–¹æ³•å·²ç»æœ‰äº†æ˜¾è‘—è¿›å±•ï¼Œç‰¹åˆ«æ˜¯åœ¨ä½¿ç”¨å·®åˆ†éšç§éšæœºæ¢¯åº¦ä¸‹é™æ³•ï¼ˆDP-SGDï¼‰å¯¹æ•æ„Ÿå›¾åƒè¿›è¡Œæ‰©æ•£æ¨¡å‹è®­ç»ƒæ–¹é¢ï¼Œä½†å®ƒä»¬ä»ç„¶å­˜åœ¨æ€§èƒ½ä¸ä½³çš„é—®é¢˜ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬å—åˆ°è¯¾ç¨‹å­¦ä¹ çš„å¯å‘ï¼Œæå‡ºäº†ä¸€ä¸ªä¸¤é˜¶æ®µçš„å·®åˆ†éšç§å›¾åƒåˆæˆæ¡†æ¶ï¼Œå…¶ä¸­æ‰©æ•£æ¨¡å‹ä»æ˜“åˆ°éš¾å­¦ä¹ ç”Ÿæˆå·®åˆ†éšç§åˆæˆå›¾åƒã€‚ä¸ç°æœ‰æ–¹æ³•ç›´æ¥ä½¿ç”¨DP-SGDè®­ç»ƒæ‰©æ•£æ¨¡å‹ä¸åŒï¼Œæˆ‘ä»¬åœ¨å¼€å§‹æ—¶æå‡ºäº†ä¸€ä¸ªç®€å•é˜¶æ®µï¼Œè®©æ‰©æ•£æ¨¡å‹å­¦ä¹ æ•æ„Ÿå›¾åƒçš„ç®€å•ç‰¹å¾ã€‚ä¸ºäº†ä¿ƒè¿›è¿™ä¸€ç®€å•é˜¶æ®µï¼Œæˆ‘ä»¬å»ºè®®ä½¿ç”¨â€œä¸­å¿ƒå›¾åƒâ€ï¼Œå³æ•æ„Ÿæ•°æ®é›†éšæœºæ ·æœ¬çš„ç®€å•é›†åˆã€‚ç›´è§‚åœ°çœ‹ï¼Œè™½ç„¶è¿™äº›ä¸­å¿ƒå›¾åƒä¸æ˜¾ç¤ºç»†èŠ‚ï¼Œä½†å®ƒä»¬å±•ç¤ºäº†æ‰€æœ‰å›¾åƒçš„æœ‰ç”¨ç‰¹å¾å¹¶ä¸”åªäº§ç”Ÿå¾®å°çš„éšç§æˆæœ¬ï¼Œä»è€Œæœ‰åŠ©äºæ—©æœŸæ¨¡å‹è®­ç»ƒã€‚æˆ‘ä»¬è¿›è¡Œå®éªŒè¡¨æ˜ï¼Œåœ¨è°ƒæŸ¥çš„å››ä¸ªå›¾åƒæ•°æ®é›†çš„å¹³å‡å€¼ä¸Šï¼Œæˆ‘ä»¬çš„åˆæˆå›¾åƒçš„ä¿çœŸåº¦å’Œæ•ˆç”¨æŒ‡æ ‡æ¯”ç°æœ‰æœ€å…ˆè¿›çš„æ–¹æ³•åˆ†åˆ«æé«˜äº†33.1%å’Œ2.1%ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.01395v2">PDF</a> Accepted at IEEE S&amp;P (Oakland) 2025; code available at   <a target="_blank" rel="noopener" href="https://github.com/SunnierLee/DP-FETA">https://github.com/SunnierLee/DP-FETA</a>; revised proofs in App.A</p>
<p><strong>Summary</strong><br>å·®åˆ†éšç§ï¼ˆDPï¼‰å›¾åƒåˆæˆæ—¨åœ¨ä»æ•æ„Ÿæ•°æ®é›†ä¸­ç”Ÿæˆåˆæˆå›¾åƒï¼Œä»¥ç¼“è§£ç»„ç»‡å’Œå…¬ä¼—å¯¹å…±äº«å’Œåˆ©ç”¨åˆæˆå›¾åƒæ—¶çš„éšç§æ³„éœ²æ‹…å¿§ã€‚å°½ç®¡ä¹‹å‰çš„æ–¹æ³•ï¼Œç‰¹åˆ«æ˜¯åœ¨ä½¿ç”¨DPéšæœºæ¢¯åº¦ä¸‹é™ï¼ˆDP-SGDï¼‰è®­ç»ƒæ‰©æ•£æ¨¡å‹æ–¹é¢å–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œä½†å®ƒä»¬ä»ç„¶è¡¨ç°ä¸ä½³ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œå—åˆ°è¯¾ç¨‹å­¦ä¹ çš„å¯å‘ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªä¸¤é˜¶æ®µçš„DPå›¾åƒåˆæˆæ¡†æ¶ï¼Œå…¶ä¸­æ‰©æ•£æ¨¡å‹ä»æ˜“åˆ°éš¾å­¦ä¹ ç”ŸæˆDPåˆæˆå›¾åƒã€‚ä¸ç°æœ‰æ–¹æ³•ç›´æ¥ä½¿ç”¨DP-SGDè®­ç»ƒæ‰©æ•£æ¨¡å‹ä¸åŒï¼Œæˆ‘ä»¬åœ¨å¼€å§‹æ—¶åŠ å…¥äº†ä¸€ä¸ªç®€å•çš„é˜¶æ®µï¼Œè®©æ‰©æ•£æ¨¡å‹å­¦ä¹ æ•æ„Ÿå›¾åƒçš„ç®€å•ç‰¹å¾ã€‚ä¸ºäº†ä¿ƒè¿›è¿™ä¸€ç®€å•é˜¶æ®µï¼Œæˆ‘ä»¬å»ºè®®ä½¿ç”¨â€œä¸­å¿ƒå›¾åƒâ€ï¼Œå³æ•æ„Ÿæ•°æ®é›†çš„éšæœºæ ·æœ¬çš„é›†åˆã€‚å°½ç®¡è¿™äº›ä¸­å¿ƒå›¾åƒä¸æ˜¾ç¤ºç»†èŠ‚ï¼Œä½†å®ƒä»¬å±•ç¤ºäº†æ‰€æœ‰å›¾åƒçš„æœ‰ç”¨ç‰¹å¾å¹¶ä¸”åªäº§ç”Ÿæœ€å°çš„éšç§æˆæœ¬ï¼Œä»è€Œæœ‰åŠ©äºæ—©æœŸé˜¶æ®µçš„æ¨¡å‹è®­ç»ƒã€‚å®éªŒè¡¨æ˜ï¼Œåœ¨è°ƒæŸ¥çš„å››ä¸ªå›¾åƒæ•°æ®é›†ä¸Šï¼Œæˆ‘ä»¬ç”Ÿæˆçš„åˆæˆå›¾åƒçš„ä¿çœŸåº¦å’Œæ•ˆç”¨æŒ‡æ ‡å‡ä¼˜äºç°æœ‰æœ€ä½³æ–¹æ³•ï¼Œåˆ†åˆ«æé«˜äº†33.1%å’Œ2.1%ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å·®åˆ†éšç§ï¼ˆDPï¼‰å›¾åƒåˆæˆæ—¨åœ¨è§£å†³å…±äº«å’Œåˆ©ç”¨åˆæˆå›¾åƒæ—¶çš„éšç§æ³„éœ²é—®é¢˜ã€‚</li>
<li>ç°æœ‰æ–¹æ³•åœ¨DPå›¾åƒåˆæˆæ–¹é¢çš„è¡¨ç°ä»æœ‰å¾…æå‡ã€‚</li>
<li>æå‡ºäº†ä¸€ä¸ªä¸¤é˜¶æ®µçš„DPå›¾åƒåˆæˆæ¡†æ¶ï¼Œä»ç®€å•åˆ°å¤æ‚è®­ç»ƒæ‰©æ•£æ¨¡å‹ã€‚</li>
<li>åœ¨æ—©æœŸé˜¶æ®µï¼Œä½¿ç”¨â€œä¸­å¿ƒå›¾åƒâ€ä¿ƒè¿›æ¨¡å‹å­¦ä¹ æ•æ„Ÿå›¾åƒçš„ç®€å•ç‰¹å¾ã€‚</li>
<li>ä¸­å¿ƒå›¾åƒèƒ½å¤Ÿå±•ç¤ºæ•°æ®é›†çš„æœ‰ç”¨ç‰¹å¾ä¸”å¯¹éšç§å½±å“è¾ƒå°ã€‚</li>
<li>å®éªŒç»“æœæ˜¾ç¤ºï¼Œåœ¨å¤šä¸ªå›¾åƒæ•°æ®é›†ä¸Šï¼Œè¯¥æ–¹æ³•ç”Ÿæˆçš„åˆæˆå›¾åƒä¼˜äºç°æœ‰æœ€ä½³æ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.01395">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-73d28dd1194b4c75f39845c82c6de24e.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-057fbc3069c27194f932c6d9bf2e86e8.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-056f1fa86a5dc437744152f00acbd9d9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4f5727ea66791288146d0419b49be13d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-37d2c4519a6cc14fb1017219f4ed3e59.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e7218dc417e1af5c4e584533c7c216eb.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="MIFNet-Learning-Modality-Invariant-Features-for-Generalizable-Multimodal-Image-Matching"><a href="#MIFNet-Learning-Modality-Invariant-Features-for-Generalizable-Multimodal-Image-Matching" class="headerlink" title="MIFNet: Learning Modality-Invariant Features for Generalizable   Multimodal Image Matching"></a>MIFNet: Learning Modality-Invariant Features for Generalizable   Multimodal Image Matching</h2><p><strong>Authors:Yepeng Liu, Zhichao Sun, Baosheng Yu, Yitian Zhao, Bo Du, Yongchao Xu, Jun Cheng</strong></p>
<p>Many keypoint detection and description methods have been proposed for image matching or registration. While these methods demonstrate promising performance for single-modality image matching, they often struggle with multimodal data because the descriptors trained on single-modality data tend to lack robustness against the non-linear variations present in multimodal data. Extending such methods to multimodal image matching often requires well-aligned multimodal data to learn modality-invariant descriptors. However, acquiring such data is often costly and impractical in many real-world scenarios. To address this challenge, we propose a modality-invariant feature learning network (MIFNet) to compute modality-invariant features for keypoint descriptions in multimodal image matching using only single-modality training data. Specifically, we propose a novel latent feature aggregation module and a cumulative hybrid aggregation module to enhance the base keypoint descriptors trained on single-modality data by leveraging pre-trained features from Stable Diffusion models. %, our approach generates robust and invariant features across diverse and unknown modalities. We validate our method with recent keypoint detection and description methods in three multimodal retinal image datasets (CF-FA, CF-OCT, EMA-OCTA) and two remote sensing datasets (Optical-SAR and Optical-NIR). Extensive experiments demonstrate that the proposed MIFNet is able to learn modality-invariant feature for multimodal image matching without accessing the targeted modality and has good zero-shot generalization ability. The code will be released at <a target="_blank" rel="noopener" href="https://github.com/lyp-deeplearning/MIFNet">https://github.com/lyp-deeplearning/MIFNet</a>. </p>
<blockquote>
<p>é’ˆå¯¹å›¾åƒåŒ¹é…æˆ–æ³¨å†Œï¼Œå·²ç»æå‡ºäº†è®¸å¤šå…³é”®ç‚¹æ£€æµ‹ä¸æè¿°æ–¹æ³•ã€‚è™½ç„¶è¿™äº›æ–¹æ³•åœ¨å•æ¨¡æ€å›¾åƒåŒ¹é…ä¸Šè¡¨ç°å‡ºæœ‰å‰æ™¯çš„æ€§èƒ½ï¼Œä½†å®ƒä»¬é€šå¸¸åœ¨å¤šæ¨¡æ€æ•°æ®ä¸Šè¡¨ç°æŒ£æ‰ï¼Œå› ä¸ºé‚£äº›åœ¨å•æ¨¡æ€æ•°æ®ä¸Šè®­ç»ƒçš„æè¿°ç¬¦å¾€å¾€ç¼ºä¹å¯¹å¤šæ¨¡æ€æ•°æ®ä¸­å­˜åœ¨çš„éçº¿æ€§å˜å¼‚çš„ç¨³å¥æ€§ã€‚å°†è¿™äº›æ–¹æ³•æ‰©å±•åˆ°å¤šæ¨¡æ€å›¾åƒåŒ¹é…é€šå¸¸éœ€è¦ç²¾å‡†å¯¹é½çš„å¤šæ¨¡æ€æ•°æ®æ¥å­¦ä¹ æ¨¡æ€ä¸å˜æè¿°ç¬¦ã€‚ç„¶è€Œï¼Œåœ¨ç°å®çš„è®¸å¤šåœºæ™¯ä¸­ï¼Œè·å–è¿™æ ·çš„æ•°æ®å¾€å¾€æˆæœ¬é«˜æ˜‚ä¸”ä¸åˆ‡å®é™…ã€‚ä¸ºäº†åº”å¯¹è¿™ä¸€æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ¨¡æ€ä¸å˜ç‰¹å¾å­¦ä¹ ç½‘ç»œï¼ˆMIFNetï¼‰ï¼Œä»…ä½¿ç”¨å•æ¨¡æ€è®­ç»ƒæ•°æ®ï¼Œä¸ºå¤šæ¨¡æ€å›¾åƒåŒ¹é…ä¸­çš„å…³é”®ç‚¹æè¿°è®¡ç®—æ¨¡æ€ä¸å˜ç‰¹å¾ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°å‹æ½œåœ¨ç‰¹å¾èšåˆæ¨¡å—å’Œç´¯ç§¯æ··åˆèšåˆæ¨¡å—ï¼Œä»¥åˆ©ç”¨æ¥è‡ªStable Diffusionæ¨¡å‹çš„é¢„è®­ç»ƒç‰¹å¾ï¼Œå¢å¼ºåœ¨å•æ¨¡æ€æ•°æ®ä¸Šè®­ç»ƒçš„åŸºå‡†å…³é”®ç‚¹æè¿°ç¬¦ã€‚æˆ‘ä»¬çš„æ–¹æ³•èƒ½å¤Ÿåœ¨å¤šæ ·ä¸”æœªçŸ¥çš„æ¨¡å¼ä¸‹ç”Ÿæˆç¨³å¥å’Œä¸å˜çš„ç‰¹å¾ã€‚æˆ‘ä»¬åœ¨ä¸‰ä¸ªå¤šæ¨¡æ€è§†ç½‘è†œå›¾åƒæ•°æ®é›†ï¼ˆCF-FAã€CF-OCTã€EMA-OCTAï¼‰å’Œä¸¤ä¸ªé¥æ„Ÿæ•°æ®é›†ï¼ˆOptical-SARå’ŒOptical-NIRï¼‰ä¸Šä¸æœ€è¿‘çš„å…³é”®ç‚¹æ£€æµ‹å’Œæè¿°æ–¹æ³•è¿›è¡ŒéªŒè¯ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼Œæ‰€æå‡ºçš„MIFNetèƒ½å¤Ÿåœ¨ä¸æ¥è§¦ç›®æ ‡æ¨¡æ€çš„æƒ…å†µä¸‹å­¦ä¹ å¤šæ¨¡æ€å›¾åƒåŒ¹é…çš„æ¨¡æ€ä¸å˜ç‰¹å¾ï¼Œå¹¶å…·æœ‰è‰¯å¥½çš„é›¶æ ·æœ¬æ³›åŒ–èƒ½åŠ›ã€‚ä»£ç å°†å‘å¸ƒåœ¨<a target="_blank" rel="noopener" href="https://github.com/lyp-deeplearning/MIFNet%E3%80%82">https://github.com/lyp-deeplearning/MIFNetã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.11299v2">PDF</a> Accept by IEEE TIP 2025</p>
<p><strong>Summary</strong><br>     é’ˆå¯¹å¤šæ¨¡æ€å›¾åƒåŒ¹é…ä¸­çš„æŒ‘æˆ˜ï¼Œæå‡ºä¸€ç§æ¨¡æ€ä¸å˜ç‰¹å¾å­¦ä¹ ç½‘ç»œï¼ˆMIFNetï¼‰ï¼Œåˆ©ç”¨å•æ¨¡æ€è®­ç»ƒæ•°æ®è®¡ç®—å…³é”®ç‚¹çš„æ¨¡æ€ä¸å˜ç‰¹å¾ã€‚é€šè¿‡æ–°å‹æ½œåœ¨ç‰¹å¾èšåˆæ¨¡å—å’Œç´¯ç§¯æ··åˆèšåˆæ¨¡å—å¢å¼ºåŸºäºå•æ¨¡æ€æ•°æ®è®­ç»ƒçš„åŸºç¡€å…³é”®æè¿°ç¬¦ï¼Œå¹¶å€ŸåŠ©Stable Diffusionæ¨¡å‹çš„é¢„è®­ç»ƒç‰¹å¾ã€‚å®éªŒéªŒè¯ï¼ŒMIFNetåœ¨å¤šç§æ¨¡æ€è§†ç½‘è†œå›¾åƒæ•°æ®é›†å’Œé¥æ„Ÿæ•°æ®é›†ä¸­è¡¨ç°å‡ºè‰¯å¥½çš„é›¶æ ·æœ¬æ³›åŒ–èƒ½åŠ›å’Œæ¨¡æ€ä¸å˜ç‰¹å¾å­¦ä¹ èƒ½åŠ›ï¼Œæ— éœ€è®¿é—®ç›®æ ‡æ¨¡æ€ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤šæ¨¡æ€å›¾åƒåŒ¹é…é¢ä¸´æŒ‘æˆ˜ï¼šç°æœ‰æ–¹æ³•åœ¨å•æ¨¡æ€å›¾åƒåŒ¹é…ä¸Šè¡¨ç°è‰¯å¥½ï¼Œä½†åœ¨å¤šæ¨¡æ€æ•°æ®ä¸Šç¼ºä¹é²æ£’æ€§ã€‚</li>
<li>MIFNetè¢«æå‡ºä»¥è§£å†³æ­¤æŒ‘æˆ˜ï¼šè®¡ç®—å…³é”®ç‚¹çš„æ¨¡æ€ä¸å˜ç‰¹å¾ï¼Œé€‚ç”¨äºå¤šæ¨¡æ€å›¾åƒåŒ¹é…ã€‚</li>
<li>MIFNetåˆ©ç”¨å•æ¨¡æ€è®­ç»ƒæ•°æ®ï¼šä¸éœ€è¦å¤šæ¨¡æ€å¯¹é½æ•°æ®ï¼Œé™ä½äº†è·å–æˆæœ¬ã€‚</li>
<li>æ–°å‹æ½œåœ¨ç‰¹å¾èšåˆæ¨¡å—å’Œç´¯ç§¯æ··åˆèšåˆæ¨¡å—ï¼šå¢å¼ºåŸºäºå•æ¨¡æ€æ•°æ®è®­ç»ƒçš„åŸºç¡€å…³é”®æè¿°ç¬¦ã€‚</li>
<li>åˆ©ç”¨Stable Diffusionæ¨¡å‹çš„é¢„è®­ç»ƒç‰¹å¾ï¼šæå‡ç‰¹å¾çš„è´¨é‡å’Œæ³›åŒ–èƒ½åŠ›ã€‚</li>
<li>åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šçš„å®éªŒéªŒè¯ï¼šæ˜¾ç¤ºMIFNetçš„é›¶æ ·æœ¬æ³›åŒ–èƒ½åŠ›å’Œæ¨¡æ€ä¸å˜ç‰¹å¾å­¦ä¹ èƒ½åŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.11299">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-f11bc00029b21dc77a003d01fd22e422.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-76b23b0417fbc9c7d57258fc22b14d9c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9301b36cd8ac02cc2062bcc8adb298ea.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3a83d029a462456056eed4b9a0746dcc.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="CDI-Copyrighted-Data-Identification-in-Diffusion-Models"><a href="#CDI-Copyrighted-Data-Identification-in-Diffusion-Models" class="headerlink" title="CDI: Copyrighted Data Identification in Diffusion Models"></a>CDI: Copyrighted Data Identification in Diffusion Models</h2><p><strong>Authors:Jan DubiÅ„ski, Antoni Kowalczuk, Franziska Boenisch, Adam Dziedzic</strong></p>
<p>Diffusion Models (DMs) benefit from large and diverse datasets for their training. Since this data is often scraped from the Internet without permission from the data owners, this raises concerns about copyright and intellectual property protections. While (illicit) use of data is easily detected for training samples perfectly re-created by a DM at inference time, it is much harder for data owners to verify if their data was used for training when the outputs from the suspect DM are not close replicas. Conceptually, membership inference attacks (MIAs), which detect if a given data point was used during training, present themselves as a suitable tool to address this challenge. However, we demonstrate that existing MIAs are not strong enough to reliably determine the membership of individual images in large, state-of-the-art DMs. To overcome this limitation, we propose CDI, a framework for data owners to identify whether their dataset was used to train a given DM. CDI relies on dataset inference techniques, i.e., instead of using the membership signal from a single data point, CDI leverages the fact that most data owners, such as providers of stock photography, visual media companies, or even individual artists, own datasets with multiple publicly exposed data points which might all be included in the training of a given DM. By selectively aggregating signals from existing MIAs and using new handcrafted methods to extract features for these datasets, feeding them to a scoring model, and applying rigorous statistical testing, CDI allows data owners with as little as 70 data points to identify with a confidence of more than 99% whether their data was used to train a given DM. Thereby, CDI represents a valuable tool for data owners to claim illegitimate use of their copyrighted data. We make the code available at <a target="_blank" rel="noopener" href="https://github.com/sprintml/copyrighted_data_identification">https://github.com/sprintml/copyrighted_data_identification</a> </p>
<blockquote>
<p>æ‰©æ•£æ¨¡å‹ï¼ˆDMsï¼‰å—ç›Šäºå¤§è§„æ¨¡ã€å¤šæ ·åŒ–çš„æ•°æ®é›†è¿›è¡Œè®­ç»ƒã€‚ç”±äºè¿™äº›æ•°æ®é€šå¸¸æœªç»æ•°æ®æ‰€æœ‰è€…è®¸å¯å°±ä»äº’è”ç½‘ä¸ŠæŠ“å–ï¼Œè¿™å¼•å‘äº†å…³äºç‰ˆæƒå’ŒçŸ¥è¯†äº§æƒä¿æŠ¤çš„æ‹…å¿§ã€‚è™½ç„¶ï¼ˆéæ³•ï¼‰ä½¿ç”¨æ•°æ®å¾ˆå®¹æ˜“æ£€æµ‹åˆ°è®­ç»ƒæ ·æœ¬è¢«æ‰©æ•£æ¨¡å‹åœ¨æ¨ç†æ—¶å®Œç¾é‡å»ºçš„æƒ…å†µï¼Œä½†å½“æ€€ç–‘çš„æ‰©æ•£æ¨¡å‹è¾“å‡ºä¸æ˜¯ç´§å¯†å‰¯æœ¬æ—¶ï¼Œæ•°æ®æ‰€æœ‰è€…å¾ˆéš¾éªŒè¯å…¶æ•°æ®æ˜¯å¦ç”¨äºè®­ç»ƒã€‚ä»æ¦‚å¿µä¸Šè®²ï¼Œæˆå‘˜æ¨ç†æ”»å‡»ï¼ˆMIAsï¼‰èƒ½å¤Ÿæ£€æµ‹ç»™å®šæ•°æ®ç‚¹æ˜¯å¦ç”¨äºè®­ç»ƒï¼Œä¼¼ä¹æ˜¯ä¸€ä¸ªè§£å†³è¿™ä¸€æŒ‘æˆ˜çš„åˆç†å·¥å…·ã€‚ç„¶è€Œï¼Œæˆ‘ä»¬è¯æ˜ç°æœ‰çš„MIAsä¸è¶³ä»¥å¯é åœ°ç¡®å®šå¤§å‹ã€æœ€å…ˆè¿›çš„DMä¸­ä¸ªåˆ«å›¾åƒçš„ä¼šå‘˜èº«ä»½ã€‚ä¸ºäº†å…‹æœè¿™ä¸€å±€é™æ€§ï¼Œæˆ‘ä»¬æå‡ºäº†CDIï¼Œä¸€ä¸ªæ•°æ®æ‰€æœ‰è€…ç”¨æ¥åˆ¤æ–­å…¶æ•°æ®é›†æ˜¯å¦ç”¨äºè®­ç»ƒç‰¹å®šæ‰©æ•£æ¨¡å‹çš„æ¡†æ¶ã€‚CDIä¾èµ–äºæ•°æ®é›†æ¨ç†æŠ€æœ¯ï¼Œå³ä¸æ˜¯ä½¿ç”¨å•ä¸ªæ•°æ®ç‚¹çš„æˆå‘˜ä¿¡å·ï¼Œè€Œæ˜¯åˆ©ç”¨å¤§å¤šæ•°æ•°æ®æ‰€æœ‰è€…ï¼ˆå¦‚åº“å­˜ç…§ç‰‡æä¾›å•†ã€è§†è§‰åª’ä½“å…¬å¸ç”šè‡³ä¸ªäººè‰ºæœ¯å®¶ï¼‰æ‹¥æœ‰å¤šä¸ªå¯èƒ½åŒ…å«åœ¨ç‰¹å®šæ‰©æ•£æ¨¡å‹è®­ç»ƒä¸­çš„å…¬å¼€æ•°æ®ç‚¹è¿™ä¸€äº‹å®ã€‚é€šè¿‡æœ‰é€‰æ‹©åœ°èšåˆç°æœ‰MIAsçš„ä¿¡å·ï¼Œå¹¶ä½¿ç”¨æ–°çš„æ‰‹å·¥æ–¹æ³•æ¥æå–è¿™äº›æ•°æ®é›†çš„ç‰¹å¾ï¼Œå°†å…¶è¾“å…¥è¯„åˆ†æ¨¡å‹å¹¶è¿›è¡Œä¸¥æ ¼çš„ç»Ÿè®¡æµ‹è¯•ï¼ŒCDIå…è®¸æ•°æ®æ‰€æœ‰è€…ä½¿ç”¨è‡³å°‘70ä¸ªæ•°æ®ç‚¹ä»¥è¶…è¿‡99%çš„ç½®ä¿¡åº¦æ¥è¯†åˆ«ä»–ä»¬çš„æ•°æ®æ˜¯å¦ç”¨äºè®­ç»ƒç‰¹å®šçš„æ‰©æ•£æ¨¡å‹ã€‚å› æ­¤ï¼ŒCDIæ˜¯æ•°æ®æ‰€æœ‰è€…è¦æ±‚åˆ¶æ­¢å¯¹å…¶ç‰ˆæƒæ•°æ®çš„éæ³•ä½¿ç”¨çš„ä¸€ç§æœ‰ä»·å€¼å·¥å…·ã€‚æˆ‘ä»¬åœ¨<a target="_blank" rel="noopener" href="https://github.com/sprintml/copyrighted_data_identification%E4%B8%8A%E6%8F%90%E4%BE%9B%E4%BA%86%E4%BB%A3%E7%A0%81%E3%80%82">https://github.com/sprintml/copyrighted_data_identificationä¸Šæä¾›äº†ä»£ç ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2411.12858v3">PDF</a> Accepted at CVPR2025 (Conference on Computer Vision and Pattern   Recognition) Code available at   <a target="_blank" rel="noopener" href="https://github.com/sprintml/copyrighted_data_identification">https://github.com/sprintml/copyrighted_data_identification</a></p>
<p><strong>æ‘˜è¦</strong></p>
<p>æ‰©æ•£æ¨¡å‹ï¼ˆDMsï¼‰è®­ç»ƒå—ç›Šäºå¤§è§„æ¨¡ã€å¤šæ ·åŒ–çš„æ•°æ®é›†ï¼Œä½†è¿™äº›æ•°æ®å¾€å¾€æœªç»æ•°æ®æ‰€æœ‰è€…è®¸å¯ä»äº’è”ç½‘æŠ“å–ï¼Œå¼•å‘ç‰ˆæƒå’ŒçŸ¥è¯†äº§æƒä¿æŠ¤çš„æ‹…å¿§ã€‚å¯¹äºè®­ç»ƒæ ·æœ¬çš„éæ³•ä½¿ç”¨ï¼Œå¦‚æœDMåœ¨æ¨ç†é˜¶æ®µå®Œç¾å¤ç°è¿™äº›æ ·æœ¬ï¼Œåˆ™å®¹æ˜“æ£€æµ‹ã€‚ç„¶è€Œï¼Œå½“è¾“å‡ºå¹¶éç²¾ç¡®å¤åˆ¶æ—¶ï¼Œæ•°æ®æ‰€æœ‰è€…éš¾ä»¥è¯æ˜å…¶æ•°æ®æ˜¯å¦è¢«ç”¨äºè®­ç»ƒã€‚æˆå‘˜æ¨ç†æ”»å‡»ï¼ˆMIAsï¼‰ä½œä¸ºä¸€ç§æ£€æµ‹ç»™å®šæ•°æ®ç‚¹æ˜¯å¦ç”¨äºè®­ç»ƒçš„å·¥å…·ï¼Œå¯¹æ­¤æŒ‘æˆ˜å…·æœ‰æ¦‚å¿µä¸Šçš„é€‚ç”¨æ€§ã€‚ä½†ç ”ç©¶æŒ‡å‡ºç°æœ‰MIAsä¸è¶³ä»¥å¯é åˆ¤æ–­å¤§å‹å‰æ²¿DMä¸­å•ä¸ªå›¾åƒçš„æˆå‘˜èº«ä»½ã€‚ä¸ºè§£å†³æ­¤å±€é™æ€§ï¼Œæˆ‘ä»¬æå‡ºCDIæ¡†æ¶ï¼Œå¸®åŠ©æ•°æ®æ‰€æœ‰è€…åˆ¤æ–­å…¶æ•°æ®é›†æ˜¯å¦è¢«ç”¨äºè®­ç»ƒç‰¹å®šDMã€‚CDIä¾èµ–æ•°æ®é›†æ¨ç†æŠ€æœ¯ï¼Œå³å¹¶éä½¿ç”¨å•ä¸ªæ•°æ®ç‚¹çš„æˆå‘˜ä¿¡å·ï¼Œè€Œæ˜¯åˆ©ç”¨å¤§å¤šæ•°æ•°æ®æ‰€æœ‰è€…ï¼ˆå¦‚åº“å­˜ç…§ç‰‡æä¾›å•†ã€è§†è§‰åª’ä½“å…¬å¸æˆ–ä¸ªäººè‰ºæœ¯å®¶ï¼‰æ‹¥æœ‰å¤šä¸ªå¯èƒ½åŒ…å«åœ¨ç‰¹å®šDMè®­ç»ƒä¸­çš„å…¬å¼€æ•°æ®ç‚¹è¿™ä¸€äº‹å®ã€‚é€šè¿‡æœ‰é€‰æ‹©åœ°èšåˆæ¥è‡ªç°æœ‰MIAsçš„ä¿¡å·ï¼Œå¹¶ä½¿ç”¨æ–°çš„æ‰‹å·¥æ–¹æ³•æå–è¿™äº›æ•°æ®é›†çš„ç‰¹å¾ï¼Œå°†å…¶è¾“å…¥è¯„åˆ†æ¨¡å‹å¹¶è¿›è¡Œä¸¥æ ¼çš„ç»Ÿè®¡æµ‹è¯•ï¼ŒCDIå…è®¸æ‹¥æœ‰ä»…70ä¸ªæ•°æ®ç‚¹çš„æ•°æ®æ‰€æœ‰è€…ä»¥è¶…è¿‡99%çš„ç½®ä¿¡åº¦åˆ¤æ–­å…¶æ•°æ®æ˜¯å¦è¢«ç”¨äºè®­ç»ƒç‰¹å®šDMã€‚å› æ­¤ï¼ŒCDIæ˜¯æ•°æ®æ‰€æœ‰è€…ä¸»å¼ å…¶ç‰ˆæƒæ•°æ®è¢«éæ³•ä½¿ç”¨æ—¶çš„å®è´µå·¥å…·ã€‚æˆ‘ä»¬å·²å°†ä»£ç å…¬å¼€åœ¨<a target="_blank" rel="noopener" href="https://github.com/sprintml/copyrighted_data_identification%E3%80%82">https://github.com/sprintml/copyrighted_data_identificationã€‚</a></p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>æ‰©æ•£æ¨¡å‹ï¼ˆDMsï¼‰ç»å¸¸ä»äº’è”ç½‘æŠ“å–å¤§è§„æ¨¡ã€å¤šæ ·åŒ–çš„æ•°æ®é›†è¿›è¡Œè®­ç»ƒï¼Œå¼•å‘ç‰ˆæƒé—®é¢˜ã€‚</li>
<li>å½“DMè¾“å‡ºå¹¶éç²¾ç¡®å¤åˆ¶è®­ç»ƒæ ·æœ¬æ—¶ï¼Œæ•°æ®æ‰€æœ‰è€…éš¾ä»¥è¯æ˜å…¶æ•°æ®æ˜¯å¦è¢«ç”¨äºè®­ç»ƒDMã€‚</li>
<li>æˆå‘˜æ¨ç†æ”»å‡»ï¼ˆMIAsï¼‰åœ¨åˆ¤æ–­DMæ˜¯å¦ä½¿ç”¨äº†ç‰¹å®šæ•°æ®æ–¹é¢å…·æœ‰ä¸€å®šä»·å€¼ï¼Œä½†ç°æœ‰MIAsä¸è¶³ä»¥è¿›è¡Œå¯é åˆ¤æ–­ã€‚</li>
<li>æå‡ºçš„CDIæ¡†æ¶åŸºäºæ•°æ®é›†æ¨ç†æŠ€æœ¯ï¼Œåˆ©ç”¨æ•°æ®æ‰€æœ‰è€…çš„å¤šä¸ªå…¬å¼€æ•°æ®ç‚¹æ¥è¯†åˆ«å…¶æ•°æ®é›†æ˜¯å¦è¢«ç”¨äºè®­ç»ƒDMã€‚</li>
<li>CDIé€šè¿‡ç»“åˆç°æœ‰MIAsçš„ä¿¡å·å’Œæ–°çš„æ‰‹å·¥æ–¹æ³•æå–æ•°æ®é›†ç‰¹å¾ï¼Œè¿›è¡Œä¸¥æ ¼çš„ç»Ÿè®¡æµ‹è¯•ã€‚</li>
<li>ä»…éœ€70ä¸ªæ•°æ®ç‚¹ï¼ŒCDIä¾¿èƒ½ä»¥è¶…è¿‡99%çš„ç½®ä¿¡åº¦åˆ¤æ–­æ•°æ®æ˜¯å¦è¢«ç”¨äºè®­ç»ƒç‰¹å®šDMã€‚</li>
<li>CDIå·¥å…·å¯¹äºæ•°æ®æ‰€æœ‰è€…ä¸»å¼ å…¶ç‰ˆæƒæ•°æ®è¢«éæ³•ä½¿ç”¨å…·æœ‰é‡è¦æ„ä¹‰ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2411.12858">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-17ab5960dae55dd2a8455b195093fb8f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9e240e888a7f18fd52399f4ff537f0cf.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-738e8aeb0108b3ce8663f2b34aba62ca.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-72b15d678625686eb0ce0c6155205ab9.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="Leveraging-Model-Guidance-to-Extract-Training-Data-from-Personalized-Diffusion-Models"><a href="#Leveraging-Model-Guidance-to-Extract-Training-Data-from-Personalized-Diffusion-Models" class="headerlink" title="Leveraging Model Guidance to Extract Training Data from Personalized   Diffusion Models"></a>Leveraging Model Guidance to Extract Training Data from Personalized   Diffusion Models</h2><p><strong>Authors:Xiaoyu Wu, Jiaru Zhang, Zhiwei Steven Wu</strong></p>
<p>Diffusion Models (DMs) have become powerful image generation tools, especially for few-shot fine-tuning where a pretrained DM is fine-tuned on a small image set to capture specific styles or objects. Many people upload these personalized checkpoints online, fostering communities such as Civitai and HuggingFace. However, model owners may overlook the data leakage risks when releasing fine-tuned checkpoints. Moreover, concerns regarding copyright violations arise when unauthorized data is used during fine-tuning. In this paper, we ask: â€œCan training data be extracted from these fine-tuned DMs shared online?â€ A successful extraction would present not only data leakage threats but also offer tangible evidence of copyright infringement. To answer this, we propose FineXtract, a framework for extracting fine-tuning data. Our method approximates fine-tuning as a gradual shift in the modelâ€™s learned distribution â€“ from the original pretrained DM toward the fine-tuning data. By extrapolating the models before and after fine-tuning, we guide the generation toward high-probability regions within the fine-tuned data distribution. We then apply a clustering algorithm to extract the most probable images from those generated using this extrapolated guidance. Experiments on DMs fine-tuned with datasets including WikiArt, DreamBooth, and real-world checkpoints posted online validate the effectiveness of our method, extracting about 20% of fine-tuning data in most cases. The code is available <a target="_blank" rel="noopener" href="https://github.com/Nicholas0228/FineXtract">https://github.com/Nicholas0228/FineXtract</a>. </p>
<blockquote>
<p>æ‰©æ•£æ¨¡å‹ï¼ˆDMsï¼‰å·²æˆä¸ºå¼ºå¤§çš„å›¾åƒç”Ÿæˆå·¥å…·ï¼Œç‰¹åˆ«æ˜¯åœ¨å°æ ·æœ¬å¾®è°ƒä¸­ï¼Œé¢„è®­ç»ƒçš„DMè¢«å¾®è°ƒä»¥æ•æ‰ç‰¹å®šé£æ ¼æˆ–å¯¹è±¡çš„å°å›¾åƒé›†ã€‚è®¸å¤šäººåœ¨çº¿ä¸Šä¼ è¿™äº›ä¸ªæ€§åŒ–çš„æ£€æŸ¥ç‚¹ï¼Œä¿ƒè¿›äº†å¦‚Civitaiå’ŒHuggingFaceç­‰ç¤¾åŒºçš„å‘å±•ã€‚ç„¶è€Œï¼Œåœ¨å‘å¸ƒå¾®è°ƒæ£€æŸ¥ç‚¹æ—¶ï¼Œæ¨¡å‹æ‰€æœ‰è€…å¯èƒ½ä¼šå¿½ç•¥æ•°æ®æ³„éœ²çš„é£é™©ã€‚æ­¤å¤–ï¼Œå½“å¾®è°ƒè¿‡ç¨‹ä¸­ä½¿ç”¨æœªç»æˆæƒçš„æ•°æ®æ—¶ï¼Œä¼šå‡ºç°ç‰ˆæƒè¿è§„çš„æ‹…å¿§ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºçš„é—®é¢˜æ˜¯ï¼šâ€œå¯ä»¥ä»åœ¨çº¿å…±äº«çš„è¿™äº›å¾®è°ƒåçš„DMsä¸­æå–è®­ç»ƒæ•°æ®å—ï¼Ÿâ€æˆåŠŸçš„æå–ä¸ä»…ä¼šå¸¦æ¥æ•°æ®æ³„éœ²çš„å¨èƒï¼Œè€Œä¸”æä¾›äº†ç‰ˆæƒä¾µæƒçš„åˆ‡å®è¯æ®ã€‚ä¸ºäº†å›ç­”è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†FineXtractï¼Œä¸€ä¸ªç”¨äºæå–å¾®è°ƒæ•°æ®çš„æ¡†æ¶ã€‚æˆ‘ä»¬çš„æ–¹æ³•å°†å¾®è°ƒè¿‘ä¼¼ä¸ºæ¨¡å‹å­¦ä¹ åˆ†å¸ƒçš„ä¸€ä¸ªé€æ¸å˜åŒ–â€”â€”ä»åŸå§‹çš„é¢„è®­ç»ƒDMå‘å¾®è°ƒæ•°æ®å˜åŒ–ã€‚é€šè¿‡å¤–æ¨å¾®è°ƒå‰åçš„æ¨¡å‹ï¼Œæˆ‘ä»¬å¼•å¯¼ç”Ÿæˆè¿›å…¥å¾®è°ƒæ•°æ®åˆ†å¸ƒå†…çš„æ¦‚ç‡è¾ƒé«˜çš„åŒºåŸŸã€‚ç„¶åï¼Œæˆ‘ä»¬åº”ç”¨èšç±»ç®—æ³•ä»ä½¿ç”¨è¿™ç§å¤–æ¨æŒ‡å¯¼ç”Ÿæˆçš„å›¾åƒä¸­æå–æœ€å¯èƒ½çš„å›¾åƒã€‚åœ¨ä½¿ç”¨WikiArtã€DreamBoothå’Œåœ¨çº¿å‘å¸ƒçš„ç°å®ä¸–ç•Œæ£€æŸ¥ç‚¹å¯¹DMsè¿›è¡Œå¾®è°ƒçš„å®éªŒéªŒè¯äº†æˆ‘ä»¬çš„æ–¹æ³•çš„æœ‰æ•ˆæ€§ï¼Œåœ¨å¤§å¤šæ•°æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬èƒ½å¤Ÿæå–çº¦20%çš„å¾®è°ƒæ•°æ®ã€‚ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/Nicholas0228/FineXtract">æ­¤é“¾æ¥</a>æ‰¾åˆ°ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2410.03039v2">PDF</a> Accepted at the International Conference on Machine Learning (ICML)   2025</p>
<p><strong>æ‘˜è¦</strong></p>
<p>æ‰©æ•£æ¨¡å‹ï¼ˆDMsï¼‰å·²æˆä¸ºå¼ºå¤§çš„å›¾åƒç”Ÿæˆå·¥å…·ï¼Œç‰¹åˆ«æ˜¯åœ¨å°æ ·æ•°æ®å¾®è°ƒä¸­ï¼Œé¢„è®­ç»ƒçš„DMé€šè¿‡å¾®è°ƒå°‘é‡çš„å›¾åƒé›†æ¥æ•æ‰ç‰¹å®šçš„é£æ ¼æˆ–å¯¹è±¡ã€‚æœ¬æ–‡æ¢è®¨äº†ä»åœ¨çº¿å…±äº«çš„å¾®è°ƒæ‰©æ•£æ¨¡å‹ä¸­æå–è®­ç»ƒæ•°æ®çš„é—®é¢˜ã€‚æˆ‘ä»¬æå‡ºFineXtractæ¡†æ¶æ¥æå–å¾®è°ƒæ•°æ®ï¼Œé€šè¿‡å¤–æ¨æ¨¡å‹åœ¨å¾®è°ƒå‰åçš„å˜åŒ–ï¼Œå¼•å¯¼ç”Ÿæˆå‘å¾®è°ƒæ•°æ®åˆ†å¸ƒçš„é«˜æ¦‚ç‡åŒºåŸŸï¼Œå¹¶åº”ç”¨èšç±»ç®—æ³•ä»ç”Ÿæˆçš„å›¾åƒä¸­æå–æœ€å¯èƒ½çš„å›¾åƒã€‚å®éªŒè¯æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å¤§å¤šæ•°æƒ…å†µä¸‹å¯æœ‰æ•ˆæå–çº¦20%çš„å¾®è°ƒæ•°æ®ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>æ‰©æ•£æ¨¡å‹ï¼ˆDMsï¼‰åœ¨å›¾åƒç”Ÿæˆé¢†åŸŸè¡¨ç°å‡ºå¼ºå¤§çš„èƒ½åŠ›ï¼Œå°¤å…¶åœ¨å°æ ·æ•°æ®å¾®è°ƒæ–¹é¢ã€‚</li>
<li>äººä»¬ä¼šåœ¨ç½‘ä¸Šä¸Šä¼ ä¸ªæ€§åŒ–çš„æ£€æŸ¥ç‚¹ï¼Œä»è€Œä¿ƒè¿›äº†ç¤¾åŒºçš„å‘å±•ï¼Œä½†ä¹Ÿå¸¦æ¥äº†æ•°æ®æ³„éœ²é£é™©ã€‚</li>
<li>å­˜åœ¨å¯¹ç‰ˆæƒä¾µçŠ¯çš„æ‹…å¿§ï¼Œå½“æœªç»æˆæƒçš„æ•°æ®ç”¨äºå¾®è°ƒæ—¶ã€‚</li>
<li>FineXtractæ¡†æ¶è¢«æå‡ºæ¥æå–å¾®è°ƒæ•°æ®ï¼Œé€šè¿‡å°†å¾®è°ƒè§†ä¸ºæ¨¡å‹å­¦ä¹ åˆ†å¸ƒçš„ä¸€ä¸ªæ¸å˜å˜åŒ–æ¥å®ç°ã€‚</li>
<li>é€šè¿‡å¤–æ¨æ¨¡å‹åœ¨å¾®è°ƒå‰åçš„å˜åŒ–ï¼Œå¯ä»¥å¼•å¯¼ç”Ÿæˆå‘å¾®è°ƒæ•°æ®åˆ†å¸ƒçš„é«˜æ¦‚ç‡åŒºåŸŸã€‚</li>
<li>åº”ç”¨èšç±»ç®—æ³•å¯ä»¥ä»ç”Ÿæˆçš„å›¾åƒä¸­æå–æœ€å¯èƒ½çš„å›¾åƒã€‚</li>
<li>å®éªŒè¯æ˜ï¼Œè¯¥æ–¹æ³•åœ¨æå–åœ¨çº¿å…±äº«çš„å¾®è°ƒæ‰©æ•£æ¨¡å‹çš„è®­ç»ƒæ•°æ®æ–¹é¢æ˜¯æœ‰æ•ˆçš„ï¼Œæå–çš„æ•°æ®é‡çº¦ä¸º20%ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2410.03039">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-0c0035a8d0c167a3583e03a6343d99e2.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-5ca3fa2a3cea0af4ab46ac95c2941be7.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e54bb752f5019594faa254a91ef1aed4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-632725741617ec8f08ed7b8d82652280.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="Bridging-Geometric-Diffusion-and-Energy-Minimization-A-Unified-Framework-for-Neural-Message-Passing"><a href="#Bridging-Geometric-Diffusion-and-Energy-Minimization-A-Unified-Framework-for-Neural-Message-Passing" class="headerlink" title="Bridging Geometric Diffusion and Energy Minimization: A Unified   Framework for Neural Message Passing"></a>Bridging Geometric Diffusion and Energy Minimization: A Unified   Framework for Neural Message Passing</h2><p><strong>Authors:Qitian Wu, David Wipf, Junchi Yan</strong></p>
<p>Learning representations for structured data with certain geometries (e.g., observed or unobserved) is a fundamental challenge, wherein message passing neural networks (MPNNs) have become a de facto class of model solutions. In this paper, we propose an energy-constrained diffusion model as a principled mathematical framework for understanding the mechanism of MPNNs and navigating novel architectural designs. Inspired by physical systems, the model combines the inductive bias of diffusion on manifolds with layer-wise constraints of energy minimization. We identify that the diffusion operators have a one-to-one correspondence with the energy functions implicitly descended by the diffusion process, and the finite-difference iteration for solving the energy-constrained diffusion system induces the propagation layers of various types of MPNNs operating on observed or latent structures. This leads to a unified perspective on common neural architectures whose computational flows can be cast as message passing (or its special case), including MLP, GCN, GIN, APPNP, GCNII, GAT, and Transformers. Building on these insights, we devise a new class of neural message passing models, dubbed diffusion-inspired Transformers, whose global attention layers are derived from the principled energy-constrained diffusion framework. Across diverse datasets, ranging from real-world networks to images, texts, and physical particles, we demonstrate that the new model achieves promising performance in scenarios where the data structures are observed (as a graph), partially observed, or entirely unobserved. </p>
<blockquote>
<p>å¯¹äºå…·æœ‰ç‰¹å®šå‡ ä½•ç»“æ„ï¼ˆæ— è®ºæ˜¯è§‚å¯Ÿåˆ°çš„è¿˜æ˜¯æœªè§‚å¯Ÿåˆ°çš„ï¼‰çš„ç»“æ„åŒ–æ•°æ®çš„å­¦ä¹ è¡¨ç¤ºæ˜¯ä¸€ä¸ªåŸºæœ¬æŒ‘æˆ˜ï¼Œå…¶ä¸­æ¶ˆæ¯ä¼ é€’ç¥ç»ç½‘ç»œï¼ˆMPNNsï¼‰å·²æˆä¸ºä¸€ç§æ¨¡å‹è§£å†³æ–¹æ¡ˆçš„å®é™…ç±»åˆ«ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§èƒ½é‡çº¦æŸæ‰©æ•£æ¨¡å‹ï¼Œä½œä¸ºç†è§£MPNNæœºåˆ¶å¹¶æ¢ç´¢æ–°å‹æ¶æ„è®¾è®¡çš„åŸºæœ¬åŸåˆ™æ•°å­¦æ¡†æ¶ã€‚è¯¥æ¨¡å‹å—åˆ°ç‰©ç†ç³»ç»Ÿçš„å¯å‘ï¼Œç»“åˆäº†æµå½¢ä¸Šæ‰©æ•£çš„å½’çº³åè§å’Œèƒ½é‡æœ€å°åŒ–çš„é€å±‚çº¦æŸã€‚æˆ‘ä»¬å‘ç°æ‰©æ•£ç®—å­ä¸æ‰©æ•£è¿‡ç¨‹éšå«ä¸‹é™çš„èƒ½å‡½æ•°ä¹‹é—´å­˜åœ¨ä¸€ä¸€å¯¹åº”å…³ç³»ï¼Œè§£å†³èƒ½é‡çº¦æŸæ‰©æ•£ç³»ç»Ÿçš„æœ‰é™å·®åˆ†è¿­ä»£å¼•å‘äº†å„ç§ç±»å‹çš„MPNNä¼ æ’­å±‚ï¼Œè¿™äº›å±‚åœ¨è§‚å¯Ÿåˆ°çš„æˆ–æ½œåœ¨çš„ç»“æ„ä¸Šè¿è¡Œã€‚è¿™ä¸ºå¸¸è§çš„ç¥ç»ç½‘ç»œæ¶æ„æä¾›äº†ä¸€ä¸ªç»Ÿä¸€çš„è§†è§’ï¼Œå…¶è®¡ç®—æµç¨‹å¯ä»¥è½¬æ¢ä¸ºæ¶ˆæ¯ä¼ é€’ï¼ˆæˆ–å…¶ç‰¹æ®Šæƒ…å†µï¼‰ï¼ŒåŒ…æ‹¬MLPã€GCNã€GINã€APPNPã€GCNIIã€GATå’ŒTransformerã€‚åŸºäºè¿™äº›è§è§£ï¼Œæˆ‘ä»¬å¼€å‘äº†ä¸€ç±»æ–°çš„ç¥ç»ç½‘ç»œæ¶ˆæ¯ä¼ é€’æ¨¡å‹ï¼Œç§°ä¸ºâ€œæ‰©æ•£å¯å‘å¼Transformerâ€ï¼Œå…¶å…¨å±€æ³¨æ„åŠ›å±‚æºäºæœ‰åŸåˆ™çš„èƒ½é‡çº¦æŸæ‰©æ•£æ¡†æ¶ã€‚åœ¨çœŸå®ä¸–ç•Œç½‘ç»œã€å›¾åƒã€æ–‡æœ¬å’Œç‰©ç†ç²’å­ç­‰å¤šç§æ•°æ®é›†ä¸Šï¼Œæˆ‘ä»¬è¯æ˜äº†æ–°æ¨¡å‹åœ¨å¤„ç†è§‚å¯Ÿåˆ°çš„ï¼ˆä½œä¸ºå›¾å½¢ï¼‰ã€éƒ¨åˆ†è§‚å¯Ÿåˆ°çš„æˆ–å®Œå…¨æœªè§‚å¯Ÿåˆ°çš„æ•°æ®ç»“æ„åœºæ™¯ä¸­å‡å–å¾—äº†æœ‰å‰æ™¯çš„æ€§èƒ½ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2409.09111v2">PDF</a> Accepted to Journal of Machine Learning Research (JMLR). Extended   version from DIFFormer in ICLR 2023</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºä¸€ç§èƒ½é‡çº¦æŸæ‰©æ•£æ¨¡å‹ï¼Œä½œä¸ºç†è§£æ¶ˆæ¯ä¼ é€’ç¥ç»ç½‘ç»œï¼ˆMPNNsï¼‰æœºåˆ¶çš„åŸåˆ™æ€§æ•°å­¦æ¡†æ¶ï¼Œå¹¶æ¢ç´¢æ–°å‹æ¶æ„è®¾è®¡ã€‚æ¨¡å‹ç»“åˆæµå½¢ä¸Šçš„æ‰©æ•£å½’çº³åè§ä¸èƒ½é‡æœ€å°åŒ–çš„é€å±‚çº¦æŸã€‚æ‰©æ•£ç®—å­ä¸èƒ½é‡å‡½æ•°æœ‰ä¸€ä¸€å¯¹åº”å…³ç³»ï¼Œé€šè¿‡æ±‚è§£èƒ½é‡çº¦æŸæ‰©æ•£ç³»ç»Ÿçš„æœ‰é™å·®åˆ†è¿­ä»£ï¼Œè¯±å¯¼å„ç§MPNNsçš„ä¼ æ’­å±‚åœ¨è§‚æµ‹æˆ–æ½œåœ¨ç»“æ„ä¸Šè¿›è¡Œæ“ä½œã€‚è¿™æä¾›äº†ä¸€ä¸ªç»Ÿä¸€è§†è§’ï¼Œé‡æ–°å®¡è§†è®¡ç®—æµå¯è½¬æ¢ä¸ºæ¶ˆæ¯ä¼ é€’æˆ–å…¶ç‰¹æ®Šæƒ…å†µçš„å¸¸è§ç¥ç»ç½‘ç»œæ¶æ„ã€‚åŸºäºè¿™äº›è§è§£ï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸€ç§æ–°å‹çš„ç¥ç»ç½‘ç»œæ¶ˆæ¯ä¼ é€’æ¨¡å‹â€”â€”æ‰©æ•£å¯å‘å¼Transformerï¼Œå…¶å…¨å±€æ³¨æ„åŠ›å±‚æºäºåŸåˆ™æ€§çš„èƒ½é‡çº¦æŸæ‰©æ•£æ¡†æ¶ã€‚åœ¨å¤šç§æ•°æ®é›†ä¸Šï¼Œæ— è®ºæ˜¯ç½‘ç»œã€å›¾åƒã€æ–‡æœ¬è¿˜æ˜¯ç‰©ç†ç²’å­ï¼Œæ–°æ¨¡å‹åœ¨æ•°æ®ç»“æ„è¢«è§‚æµ‹ã€éƒ¨åˆ†è§‚æµ‹æˆ–å®Œå…¨æœªè¢«è§‚æµ‹çš„åœºæ™¯ä¸­å‡è¡¨ç°å‡ºè‰¯å¥½æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è®ºæ–‡æå‡ºä¸€ç§åŸºäºèƒ½é‡çº¦æŸæ‰©æ•£æ¨¡å‹çš„æ•°å­¦æ¡†æ¶ï¼Œç”¨ä»¥ç†è§£æ¶ˆæ¯ä¼ é€’ç¥ç»ç½‘ç»œï¼ˆMPNNsï¼‰çš„å·¥ä½œåŸç†ã€‚</li>
<li>æ¨¡å‹ç»“åˆäº†æµå½¢ä¸Šçš„æ‰©æ•£å½’çº³åè§å’Œèƒ½é‡æœ€å°åŒ–çš„é€å±‚çº¦æŸï¼Œä¸ºæ–°å‹ç¥ç»ç½‘ç»œæ¶æ„è®¾è®¡æä¾›çµæ„Ÿã€‚</li>
<li>è®ºæ–‡æ­ç¤ºäº†æ‰©æ•£ç®—å­ä¸èƒ½é‡å‡½æ•°ä¹‹é—´çš„å¯¹åº”å…³ç³»ï¼Œå¹¶é€šè¿‡æœ‰é™å·®åˆ†è¿­ä»£æ¨¡æ‹Ÿæ‰©æ•£è¿‡ç¨‹ï¼Œä»è€Œç†è§£ä¸åŒç±»å‹çš„MPNNsä¼ æ’­å±‚ã€‚</li>
<li>æ¨¡å‹æä¾›äº†ä¸€ä¸ªç»Ÿä¸€è§†è§’ï¼Œæ¶µç›–å¤šç§è®¡ç®—æµå¯è½¬æ¢ä¸ºæ¶ˆæ¯ä¼ é€’æˆ–å…¶ç‰¹æ®Šæƒ…å†µçš„ç¥ç»ç½‘ç»œæ¶æ„ã€‚</li>
<li>åŸºäºè¯¥æ¡†æ¶ï¼Œè®ºæ–‡è®¾è®¡äº†ä¸€ç§æ–°å‹çš„ç¥ç»ç½‘ç»œæ¶ˆæ¯ä¼ é€’æ¨¡å‹â€”â€”æ‰©æ•£å¯å‘å¼Transformerï¼Œå…¶å…¨å±€æ³¨æ„åŠ›å±‚æºäºèƒ½é‡çº¦æŸæ‰©æ•£åŸç†ã€‚</li>
<li>æ–°æ¨¡å‹åœ¨å¤šç§æ•°æ®é›†ä¸Šè¡¨ç°å‡ºè‰¯å¥½æ€§èƒ½ï¼ŒåŒ…æ‹¬ç½‘ç»œã€å›¾åƒã€æ–‡æœ¬å’Œç‰©ç†ç²’å­ç­‰ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2409.09111">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-84fc55b59892ca0edc99f9047c6d1bef.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-cd51a0c4d9d03867bc364d3da271f7b6.jpg" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="EDA-DM-Enhanced-Distribution-Alignment-for-Post-Training-Quantization-of-Diffusion-Models"><a href="#EDA-DM-Enhanced-Distribution-Alignment-for-Post-Training-Quantization-of-Diffusion-Models" class="headerlink" title="EDA-DM: Enhanced Distribution Alignment for Post-Training Quantization   of Diffusion Models"></a>EDA-DM: Enhanced Distribution Alignment for Post-Training Quantization   of Diffusion Models</h2><p><strong>Authors:Xuewen Liu, Zhikai Li, Junrui Xiao, Mengjuan Chen, Jianquan Li, Qingyi Gu</strong></p>
<p>Diffusion models have achieved great success in image generation tasks. However, the lengthy denoising process and complex neural networks hinder their low-latency applications in real-world scenarios. Quantization can effectively reduce model complexity, and post-training quantization (PTQ), which does not require fine-tuning, is highly promising for compressing and accelerating diffusion models. Unfortunately, we find that due to the highly dynamic activations, existing PTQ methods suffer from distribution mismatch issues at both calibration sample level and reconstruction output level, which makes the performance far from satisfactory. In this paper, we propose EDA-DM, a standardized PTQ method that efficiently addresses the above issues. Specifically, at the calibration sample level, we extract information from the density and diversity of latent space feature maps, which guides the selection of calibration samples to align with the overall sample distribution; and at the reconstruction output level, we theoretically analyze the reasons for previous reconstruction failures and, based on this insight, optimize block reconstruction using the Hessian loss of layers, aligning the outputs of quantized model and full-precision model at different network granularity. Extensive experiments demonstrate that EDA-DM significantly outperforms the existing PTQ methods across various models and datasets. Our method achieves a 1.83 times speedup and 4 times compression for the popular Stable-Diffusion on MS-COCO, with only a 0.05 loss in CLIP score. Code is available at <a target="_blank" rel="noopener" href="http://github.com/BienLuky/EDA-DM">http://github.com/BienLuky/EDA-DM</a> . </p>
<blockquote>
<p>æ‰©æ•£æ¨¡å‹åœ¨å›¾åƒç”Ÿæˆä»»åŠ¡ä¸­å–å¾—äº†å·¨å¤§æˆåŠŸã€‚ç„¶è€Œï¼Œæ¼«é•¿çš„å»å™ªè¿‡ç¨‹å’Œå¤æ‚çš„ç¥ç»ç½‘ç»œé˜»ç¢äº†å…¶åœ¨ç°å®åœºæ™¯ä¸­çš„ä½å»¶è¿Ÿåº”ç”¨ã€‚é‡åŒ–å¯ä»¥æœ‰æ•ˆåœ°é™ä½æ¨¡å‹å¤æ‚åº¦ï¼Œè€Œä¸éœ€è¦å¾®è°ƒçš„åè®­ç»ƒé‡åŒ–ï¼ˆPTQï¼‰åœ¨å‹ç¼©å’ŒåŠ é€Ÿæ‰©æ•£æ¨¡å‹æ–¹é¢å‰æ™¯å¹¿é˜”ã€‚ç„¶è€Œï¼Œæˆ‘ä»¬å‘ç°ç”±äºæ¿€æ´»é«˜åº¦åŠ¨æ€ï¼Œç°æœ‰çš„PTQæ–¹æ³•åœ¨æ ¡å‡†æ ·æœ¬çº§åˆ«å’Œé‡å»ºè¾“å‡ºçº§åˆ«éƒ½å­˜åœ¨åˆ†å¸ƒä¸åŒ¹é…é—®é¢˜ï¼Œè¿™ä½¿å¾—æ€§èƒ½è¿œä¸èƒ½ä»¤äººæ»¡æ„ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†EDA-DMï¼Œä¸€ç§æ ‡å‡†åŒ–çš„PTQæ–¹æ³•ï¼Œæœ‰æ•ˆåœ°è§£å†³äº†ä¸Šè¿°é—®é¢˜ã€‚å…·ä½“è€Œè¨€ï¼Œåœ¨æ ¡å‡†æ ·æœ¬çº§åˆ«ï¼Œæˆ‘ä»¬ä»æ½œåœ¨ç©ºé—´ç‰¹å¾å›¾çš„å¯†åº¦å’Œå¤šæ ·æ€§ä¸­æå–ä¿¡æ¯ï¼Œè¿™æœ‰åŠ©äºé€‰æ‹©æ ¡å‡†æ ·æœ¬ä»¥ç¬¦åˆæ€»ä½“æ ·æœ¬åˆ†å¸ƒï¼›åœ¨é‡å»ºè¾“å‡ºçº§åˆ«ï¼Œæˆ‘ä»¬åˆ†æäº†ä¹‹å‰é‡å»ºå¤±è´¥çš„åŸå› ï¼Œå¹¶åœ¨æ­¤åŸºç¡€ä¸Šï¼Œåˆ©ç”¨å±‚çš„é«˜æ–¯æŸå¤±ä¼˜åŒ–å—é‡å»ºï¼Œä½¿é‡åŒ–æ¨¡å‹å’Œå…¨ç²¾åº¦æ¨¡å‹åœ¨ä¸åŒç½‘ç»œç²’åº¦ä¸Šçš„è¾“å‡ºä¿æŒä¸€è‡´ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼Œåœ¨å„ç§æ¨¡å‹å’Œæ•°æ®é›†ä¸Šï¼ŒEDA-DMæ˜¾è‘—ä¼˜äºç°æœ‰çš„PTQæ–¹æ³•ã€‚æˆ‘ä»¬çš„æ–¹æ³•åœ¨MS-COCOä¸Šçš„æµè¡ŒStable-Diffusionä¸Šå®ç°äº†1.83å€çš„é€Ÿåº¦æå‡å’Œ4å€çš„å‹ç¼©ï¼ŒåŒæ—¶CLIPåˆ†æ•°åªæŸå¤±äº†0.05ã€‚ç›¸å…³ä»£ç å¯è®¿é—®<a target="_blank" rel="noopener" href="http://github.com/BienLuky/EDA-DM%E8%8E%B7%E5%8F%96%E3%80%82">http://github.com/BienLuky/EDA-DMè·å–ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2401.04585v3">PDF</a> Code: <a target="_blank" rel="noopener" href="http://github.com/BienLuky/EDA-DM">http://github.com/BienLuky/EDA-DM</a></p>
<p><strong>æ‘˜è¦</strong></p>
<p>æ‰©æ•£æ¨¡å‹åœ¨å›¾åƒç”Ÿæˆä»»åŠ¡ä¸­å–å¾—äº†å·¨å¤§æˆåŠŸï¼Œä½†å…¶å»å™ªè¿‡ç¨‹å†—é•¿ã€ç¥ç»ç½‘ç»œå¤æ‚ï¼Œé™åˆ¶äº†å…¶åœ¨ç°å®åœºæ™¯ä¸­çš„ä½å»¶è¿Ÿåº”ç”¨ã€‚é‡åŒ–å¯ä»¥æœ‰æ•ˆé™ä½æ¨¡å‹å¤æ‚åº¦ï¼Œåè®­ç»ƒé‡åŒ–ï¼ˆPTQï¼‰åœ¨ä¸éœ€å¾®è°ƒçš„æƒ…å†µä¸‹å±•ç°å‡ºå¯¹å‹ç¼©å’ŒåŠ é€Ÿæ‰©æ•£æ¨¡å‹çš„å·¨å¤§æ½œåŠ›ã€‚ç„¶è€Œï¼Œç”±äºæ¿€æ´»é«˜åº¦åŠ¨æ€ï¼Œç°æœ‰PTQæ–¹æ³•åœ¨æ ¡å‡†æ ·æœ¬çº§å’Œé‡å»ºè¾“å‡ºçº§å­˜åœ¨åˆ†å¸ƒä¸åŒ¹é…é—®é¢˜ï¼Œæ€§èƒ½éš¾ä»¥ä»¤äººæ»¡æ„ã€‚æœ¬æ–‡æå‡ºEDA-DMï¼Œä¸€ç§æ ‡å‡†åŒ–çš„PTQæ–¹æ³•ï¼Œæœ‰æ•ˆè§£å†³ä¸Šè¿°é—®é¢˜ã€‚åœ¨æ ¡å‡†æ ·æœ¬çº§ï¼Œæˆ‘ä»¬ä»æ½œåœ¨ç©ºé—´ç‰¹å¾å›¾çš„å¯†åº¦å’Œå¤šæ ·æ€§ä¸­æå–ä¿¡æ¯ï¼ŒæŒ‡å¯¼æ ¡å‡†æ ·æœ¬çš„é€‰æ‹©ï¼Œä»¥ä¸æ•´ä½“æ ·æœ¬åˆ†å¸ƒå¯¹é½ï¼›åœ¨é‡å»ºè¾“å‡ºçº§ï¼Œæˆ‘ä»¬åˆ†æä»¥å¾€é‡å»ºå¤±è´¥çš„åŸå› ï¼ŒåŸºäºæ­¤ä¼˜åŒ–å—é‡å»ºï¼Œä½¿ç”¨å±‚çš„é«˜æ–¯æŸå¤±ï¼Œåœ¨ä¸åŒç½‘ç»œç²’åº¦ä¸Šå¯¹é½é‡åŒ–æ¨¡å‹å’Œå…¨ç²¾åº¦æ¨¡å‹çš„è¾“å‡ºã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼ŒEDA-DMåœ¨å¤šç§æ¨¡å‹å’Œæ•°æ®é›†ä¸Šæ˜¾è‘—ä¼˜äºç°æœ‰PTQæ–¹æ³•ã€‚æˆ‘ä»¬çš„æ–¹æ³•ä¸ºæµè¡Œçš„Stable-Diffusionåœ¨MS-COCOä¸Šå®ç°äº†1.83å€åŠ é€Ÿå’Œ4å€å‹ç¼©ï¼ŒCLIPåˆ†æ•°ä»…ä¸‹é™0.05ã€‚ç›¸å…³ä»£ç å¯é€šè¿‡<a target="_blank" rel="noopener" href="http://github.com/BienLuky/EDA-DM%E8%8E%B7%E5%8F%96%E3%80%82">http://github.com/BienLuky/EDA-DMè·å–ã€‚</a></p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>æ‰©æ•£æ¨¡å‹åœ¨å›¾åƒç”Ÿæˆä¸­è¡¨ç°ä¼˜ç§€ï¼Œä½†åœ¨å®é™…ä½å»¶è¿Ÿåº”ç”¨ä¸­å› è¿‡ç¨‹å†—é•¿å’Œç¥ç»ç½‘ç»œå¤æ‚è€Œå—åˆ°é™åˆ¶ã€‚</li>
<li>é‡åŒ–æ˜¯é™ä½æ¨¡å‹å¤æ‚åº¦çš„ä¸€ç§æœ‰æ•ˆæ–¹æ³•ï¼Œåè®­ç»ƒé‡åŒ–ï¼ˆPTQï¼‰ä¸ºæ‰©æ•£æ¨¡å‹çš„å‹ç¼©å’ŒåŠ é€Ÿæä¾›å·¨å¤§æ½œåŠ›ã€‚</li>
<li>ç°æœ‰PTQæ–¹æ³•é¢ä¸´æ ¡å‡†æ ·æœ¬çº§å’Œé‡å»ºè¾“å‡ºçº§çš„åˆ†å¸ƒä¸åŒ¹é…é—®é¢˜ã€‚</li>
<li>EDA-DMæ˜¯ä¸€ç§æ ‡å‡†åŒ–çš„PTQæ–¹æ³•ï¼Œé€šè¿‡æå–æ½œåœ¨ç©ºé—´ç‰¹å¾å›¾çš„ä¿¡æ¯æ¥è§£å†³åˆ†å¸ƒä¸åŒ¹é…é—®é¢˜ï¼Œå¹¶åœ¨é‡å»ºè¾“å‡ºçº§è¿›è¡Œä¼˜åŒ–ã€‚</li>
<li>EDA-DMåœ¨å¤šç§æ¨¡å‹å’Œæ•°æ®é›†ä¸Šè¡¨ç°ä¼˜äºå…¶ä»–PTQæ–¹æ³•ã€‚</li>
<li>åœ¨MS-COCOä¸Šçš„Stable-Diffusionå®éªŒè¡¨æ˜ï¼ŒEDA-DMå®ç°äº†åŠ é€Ÿå’Œå‹ç¼©çš„åŒæ—¶ï¼Œä»…ç‰ºç‰²äº†å¾®å°çš„CLIPåˆ†æ•°ã€‚</li>
<li>ç›¸å…³ç ”ç©¶ä»£ç å·²å…¬å¼€å¯è®¿é—®ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2401.04585">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-8debd6f060b745ad809f948626979bcf.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-dc47b6e79dbab89a7b5ddb4db83adbdd.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-364df4a10ea6c94030cfceed0bc1efed.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e1aa5d3321ab8d370bed0fdd79f0c375.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e551bed7695698713efbc14c2b430fc8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-19bbc781fa77d41749c606ded0e1eb54.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-dee974f3939dd76c5a4f660cd5633e55.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d2ad4f8265e489e3e5ae3beec40f4751.jpg" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-06-25/Diffusion%20Models/">https://kedreamix.github.io/Talk2Paper/Paper/2025-06-25/Diffusion%20Models/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Diffusion-Models/">
                                    <span class="chip bg-color">Diffusion Models</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-06-25/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-baa2422c58392beb183525123b6c48ea.jpg" class="responsive-img" alt="åŒ»å­¦å›¾åƒ">
                        
                        <span class="card-title">åŒ»å­¦å›¾åƒ</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            åŒ»å­¦å›¾åƒ æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-06-25  RAG-6DPose Retrieval-Augmented 6D Pose Estimation via Leveraging CAD as   Knowledge Base
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-06-25
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/" class="post-category">
                                    åŒ»å­¦å›¾åƒ
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                        <span class="chip bg-color">åŒ»å­¦å›¾åƒ</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-06-25/NeRF/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-56ca28009d5260476c56072194780a5b.jpg" class="responsive-img" alt="NeRF">
                        
                        <span class="card-title">NeRF</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            NeRF æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-06-25  MCN-SLAM Multi-Agent Collaborative Neural SLAM with Hybrid Implicit   Neural Scene Representation
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-06-25
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/NeRF/" class="post-category">
                                    NeRF
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/NeRF/">
                        <span class="chip bg-color">NeRF</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">32306k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
