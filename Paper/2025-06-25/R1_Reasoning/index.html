<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="R1_Reasoning">
    <meta name="description" content="R1_Reasoning æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-06-25  ReasonFlux-PRM Trajectory-Aware PRMs for Long Chain-of-Thought   Reasoning in LLMs">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>R1_Reasoning | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pic1.zhimg.com/v2-2cf24f70b89b4a2c22b43150cb9ce55c.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">R1_Reasoning</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/R1-Reasoning/">
                                <span class="chip bg-color">R1_Reasoning</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/R1-Reasoning/" class="post-category">
                                R1_Reasoning
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-06-25
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-07-06
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    21.1k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    86 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-06-25-æ›´æ–°"><a href="#2025-06-25-æ›´æ–°" class="headerlink" title="2025-06-25 æ›´æ–°"></a>2025-06-25 æ›´æ–°</h1><h2 id="ReasonFlux-PRM-Trajectory-Aware-PRMs-for-Long-Chain-of-Thought-Reasoning-in-LLMs"><a href="#ReasonFlux-PRM-Trajectory-Aware-PRMs-for-Long-Chain-of-Thought-Reasoning-in-LLMs" class="headerlink" title="ReasonFlux-PRM: Trajectory-Aware PRMs for Long Chain-of-Thought   Reasoning in LLMs"></a>ReasonFlux-PRM: Trajectory-Aware PRMs for Long Chain-of-Thought   Reasoning in LLMs</h2><p><strong>Authors:Jiaru Zou, Ling Yang, Jingwen Gu, Jiahao Qiu, Ke Shen, Jingrui He, Mengdi Wang</strong></p>
<p>Process Reward Models (PRMs) have recently emerged as a powerful framework for supervising intermediate reasoning steps in large language models (LLMs). Previous PRMs are primarily trained on model final output responses and struggle to evaluate intermediate thinking trajectories robustly, especially in the emerging setting of trajectory-response outputs generated by frontier reasoning models like Deepseek-R1. In this work, we introduce ReasonFlux-PRM, a novel trajectory-aware PRM explicitly designed to evaluate the trajectory-response type of reasoning traces. ReasonFlux-PRM incorporates both step-level and trajectory-level supervision, enabling fine-grained reward assignment aligned with structured chain-of-thought data. We adapt ReasonFlux-PRM to support reward supervision under both offline and online settings, including (i) selecting high-quality model distillation data for downstream supervised fine-tuning of smaller models, (ii) providing dense process-level rewards for policy optimization during reinforcement learning, and (iii) enabling reward-guided Best-of-N test-time scaling. Empirical results on challenging downstream benchmarks such as AIME, MATH500, and GPQA-Diamond demonstrate that ReasonFlux-PRM-7B selects higher quality data than strong PRMs (e.g., Qwen2.5-Math-PRM-72B) and human-curated baselines. Furthermore, our derived ReasonFlux-PRM-7B yields consistent performance improvements, achieving average gains of 12.1% in supervised fine-tuning, 4.5% in reinforcement learning, and 6.3% in test-time scaling. We also release our efficient ReasonFlux-PRM-1.5B for resource-constrained applications and edge deployment. Projects: <a target="_blank" rel="noopener" href="https://github.com/Gen-Verse/ReasonFlux">https://github.com/Gen-Verse/ReasonFlux</a> </p>
<blockquote>
<p>è¿‡ç¨‹å¥–åŠ±æ¨¡å‹ï¼ˆPRMsï¼‰æœ€è¿‘ä½œä¸ºç›‘ç£å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ä¸­çš„ä¸­é—´æ¨ç†æ­¥éª¤çš„å¼ºå¤§æ¡†æ¶è€Œå‡ºç°ã€‚ä»¥å‰çš„PRMä¸»è¦åŸºäºæ¨¡å‹çš„æœ€ç»ˆè¾“å‡ºå“åº”è¿›è¡Œè®­ç»ƒï¼Œåœ¨è¯„ä¼°å‰æ²¿æ¨ç†æ¨¡å‹ï¼ˆå¦‚Deepseek-R1ï¼‰ç”Ÿæˆçš„è½¨è¿¹å“åº”è¾“å‡ºæ—¶ï¼Œéš¾ä»¥ç¨³å¥åœ°è¯„ä¼°ä¸­é—´æ€ç»´è½¨è¿¹ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬å¼•å…¥äº†ReasonFlux-PRMï¼Œè¿™æ˜¯ä¸€ç§ä¸“é—¨è®¾è®¡ç”¨äºè¯„ä¼°è½¨è¿¹å“åº”ç±»å‹æ¨ç†ç—•è¿¹çš„æ–°å‹è½¨è¿¹æ„ŸçŸ¥PRMã€‚ReasonFlux-PRMç»“åˆäº†æ­¥éª¤çº§å’Œè½¨è¿¹çº§çš„ç›‘ç£ï¼Œèƒ½å¤Ÿå®ç°ä¸ç»“æ„åŒ–æ€ç»´é“¾æ•°æ®ç²¾ç»†å¯¹é½çš„å¥–åŠ±åˆ†é…ã€‚æˆ‘ä»¬ä½¿ReasonFlux-PRMé€‚åº”ç¦»çº¿åœ¨çº¿è®¾ç½®ä¸‹çš„å¥–åŠ±ç›‘ç£ï¼ŒåŒ…æ‹¬ï¼ˆiï¼‰ä¸ºä¸‹æ¸¸ç›‘ç£å¾®è°ƒè¾ƒå°æ¨¡å‹é€‰æ‹©é«˜è´¨é‡æ¨¡å‹è’¸é¦æ•°æ®ï¼Œï¼ˆiiï¼‰åœ¨å¼ºåŒ–å­¦ä¹ ä¸­ä¸ºç­–ç•¥ä¼˜åŒ–æä¾›å¯†é›†çš„æµç¨‹çº§å¥–åŠ±ï¼Œä»¥åŠï¼ˆiiiï¼‰å®ç°å¥–åŠ±å¼•å¯¼çš„Best-of-Næµ‹è¯•æ—¶é—´ç¼©æ”¾ã€‚åœ¨AIMEã€MATH500å’ŒGPQA-Diamondç­‰å…·æœ‰æŒ‘æˆ˜æ€§çš„ä¸‹æ¸¸åŸºå‡†æµ‹è¯•ä¸Šçš„å®è¯ç»“æœè¡¨æ˜ï¼ŒReasonFlux-PRM-7Bé€‰æ‹©çš„æ•°æ®è´¨é‡é«˜äºå¼ºå¤§çš„PRMï¼ˆå¦‚Qwen2.5-Math-PRM-72Bï¼‰å’Œäººç±»ç­–åˆ’çš„åŸºçº¿ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¡ç”Ÿçš„ReasonFlux-PRM-7Båœ¨å„æ–¹é¢æ€§èƒ½å‡æœ‰æ‰€æ”¹è¿›ï¼Œç›‘ç£å¾®è°ƒå¹³å‡æå‡12.1%ï¼Œå¼ºåŒ–å­¦ä¹ æå‡4.5%ï¼Œæµ‹è¯•æ—¶é—´ç¼©æ”¾æå‡6.3%ã€‚æˆ‘ä»¬è¿˜å‘å¸ƒäº†é€‚ç”¨äºèµ„æºå—é™åº”ç”¨å’Œè¾¹ç¼˜éƒ¨ç½²çš„é«˜æ•ˆReasonFlux-PRM-1.5Bã€‚é¡¹ç›®åœ°å€ï¼š<a target="_blank" rel="noopener" href="https://github.com/Gen-Verse/ReasonFlux">https://github.com/Gen-Verse/ReasonFlux</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.18896v1">PDF</a> Codes and Models: <a target="_blank" rel="noopener" href="https://github.com/Gen-Verse/ReasonFlux">https://github.com/Gen-Verse/ReasonFlux</a></p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ReasonFlux-PRMï¼Œè¿™æ˜¯ä¸€ç§æ–°å‹çš„è½¨è¿¹æ„ŸçŸ¥è¿‡ç¨‹å¥–åŠ±æ¨¡å‹ï¼ˆPRMï¼‰ï¼Œä¸“ä¸ºè¯„ä¼°å‰æ²¿æ¨ç†æ¨¡å‹å¦‚Deepseek-R1ç”Ÿæˆçš„è½¨è¿¹å“åº”ç±»å‹æ¨ç†è½¨è¿¹è€Œè®¾è®¡ã€‚è¯¥æ¨¡å‹ç»“åˆäº†æ­¥éª¤çº§å’Œè½¨è¿¹çº§çš„ç›‘ç£ï¼Œå®ç°äº†ä¸ç»“æ„åŒ–æ€ç»´æ•°æ®ç›¸åŒ¹é…çš„ç²¾ç»†å¥–åŠ±åˆ†é…ã€‚ReasonFlux-PRMæ”¯æŒåœ¨çº¿å’Œç¦»çº¿ç¯å¢ƒä¸‹çš„å¥–åŠ±ç›‘ç£ï¼Œå¯ç”¨äºé€‰æ‹©é«˜è´¨é‡æ¨¡å‹è’¸é¦æ•°æ®ã€ä¸ºå¼ºåŒ–å­¦ä¹ æä¾›å¯†é›†çš„è¿‡ç¨‹çº§å¥–åŠ±ï¼Œä»¥åŠå®ç°å¥–åŠ±å¼•å¯¼çš„æœ€ä½³Næµ‹è¯•æ—¶é—´ç¼©æ”¾ã€‚åœ¨AIMEã€MATH500å’ŒGPQA-Diamondç­‰åŸºå‡†æµ‹è¯•ä¸­ï¼ŒReasonFlux-PRM-7Bè¡¨ç°å‡ºè‰²ï¼Œä¼˜äºå…¶ä»–å¼ºå¤§çš„PRMå’Œäººå·¥åŸºå‡†çº¿ã€‚æ­¤å¤–ï¼Œè¡ç”Ÿå‡ºçš„ReasonFlux-PRM-7Båœ¨ç›‘ç£å¾®è°ƒã€å¼ºåŒ–å­¦ä¹ å’Œæµ‹è¯•æ—¶é—´ç¼©æ”¾æ–¹é¢å‡å®ç°äº†æ€§èƒ½æ”¹è¿›ã€‚æˆ‘ä»¬è¿˜æ¨å‡ºäº†é€‚ç”¨äºèµ„æºå—é™åº”ç”¨å’Œè¾¹ç¼˜éƒ¨ç½²çš„é«˜æ•ˆReasonFlux-PRM-1.5Bã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ReasonFlux-PRMæ˜¯ä¸€ç§è½¨è¿¹æ„ŸçŸ¥è¿‡ç¨‹å¥–åŠ±æ¨¡å‹ï¼ˆPRMï¼‰ï¼Œç”¨äºè¯„ä¼°å‰æ²¿æ¨ç†æ¨¡å‹çš„è½¨è¿¹å“åº”ã€‚</li>
<li>è¯¥æ¨¡å‹ç»“åˆæ­¥éª¤çº§å’Œè½¨è¿¹çº§çš„ç›‘ç£ï¼Œå®ç°ç²¾ç»†å¥–åŠ±åˆ†é…ï¼Œä¸ç»“æ„åŒ–æ€ç»´æ•°æ®å¯¹é½ã€‚</li>
<li>ReasonFlux-PRMæ”¯æŒåœ¨çº¿å’Œç¦»çº¿ç¯å¢ƒä¸‹çš„å¥–åŠ±ç›‘ç£ï¼ŒåŒ…æ‹¬é€‰æ‹©é«˜è´¨é‡æ¨¡å‹è’¸é¦æ•°æ®ã€å¼ºåŒ–å­¦ä¹ çš„å¯†é›†è¿‡ç¨‹çº§å¥–åŠ±ï¼Œä»¥åŠæµ‹è¯•æ—¶é—´ç¼©æ”¾ã€‚</li>
<li>åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­ï¼ŒReasonFlux-PRMè¡¨ç°å‡ºè‰²ï¼Œä¼˜äºå…¶ä»–å¼ºå¤§çš„PRMå’Œäººå·¥åŸºå‡†çº¿ã€‚</li>
<li>ReasonFlux-PRM-7Båœ¨ç›‘ç£å¾®è°ƒã€å¼ºåŒ–å­¦ä¹ å’Œæµ‹è¯•æ—¶é—´ç¼©æ”¾æ–¹é¢å®ç°äº†æ€§èƒ½æ”¹è¿›ã€‚</li>
<li>æ¨å‡ºäº†é€‚ç”¨äºèµ„æºå—é™åº”ç”¨å’Œè¾¹ç¼˜éƒ¨ç½²çš„é«˜æ•ˆReasonFlux-PRM-1.5Bã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.18896">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-aab61dc221956decece14c17a6303868.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-dcdcb2df77765b595c28533b1ff54fac.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-56b25cd6183bd35508c70de51de3ce24.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-22863899044a8cd20252d7070a1ec8de.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-713319f28adfe8dda6d102f914890a01.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="LongWriter-Zero-Mastering-Ultra-Long-Text-Generation-via-Reinforcement-Learning"><a href="#LongWriter-Zero-Mastering-Ultra-Long-Text-Generation-via-Reinforcement-Learning" class="headerlink" title="LongWriter-Zero: Mastering Ultra-Long Text Generation via Reinforcement   Learning"></a>LongWriter-Zero: Mastering Ultra-Long Text Generation via Reinforcement   Learning</h2><p><strong>Authors:Yuhao Wu, Yushi Bai, Zhiqiang Hu, Roy Ka-Wei Lee, Juanzi Li</strong></p>
<p>Ultra-long generation by large language models (LLMs) is a widely demanded scenario, yet it remains a significant challenge due to their maximum generation length limit and overall quality degradation as sequence length increases. Previous approaches, exemplified by LongWriter, typically rely on â€˜â€™teachingâ€™â€™, which involves supervised fine-tuning (SFT) on synthetic long-form outputs. However, this strategy heavily depends on synthetic SFT data, which is difficult and costly to construct, often lacks coherence and consistency, and tends to be overly artificial and structurally monotonous. In this work, we propose an incentivization-based approach that, starting entirely from scratch and without relying on any annotated or synthetic data, leverages reinforcement learning (RL) to foster the emergence of ultra-long, high-quality text generation capabilities in LLMs. We perform RL training starting from a base model, similar to R1-Zero, guiding it to engage in reasoning that facilitates planning and refinement during the writing process. To support this, we employ specialized reward models that steer the LLM towards improved length control, writing quality, and structural formatting. Experimental evaluations show that our LongWriter-Zero model, trained from Qwen2.5-32B, consistently outperforms traditional SFT methods on long-form writing tasks, achieving state-of-the-art results across all metrics on WritingBench and Arena-Write, and even surpassing 100B+ models such as DeepSeek R1 and Qwen3-235B. We open-source our data and model checkpoints under <a target="_blank" rel="noopener" href="https://huggingface.co/THU-KEG/LongWriter-Zero-32B">https://huggingface.co/THU-KEG/LongWriter-Zero-32B</a> </p>
<blockquote>
<p>è¶…é•¿æ–‡æœ¬ç”Ÿæˆæ˜¯å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å¹¿æ³›éœ€æ±‚çš„åœºæ™¯ï¼Œä½†ç”±äºå…¶æœ€å¤§ç”Ÿæˆé•¿åº¦é™åˆ¶ä»¥åŠéšç€åºåˆ—é•¿åº¦å¢åŠ çš„æ•´ä½“è´¨é‡ä¸‹é™ï¼Œè¿™ä»ç„¶æ˜¯ä¸€ä¸ªå·¨å¤§çš„æŒ‘æˆ˜ã€‚ä»¥å‰çš„æ–¹æ³•ï¼Œä»¥LongWriterä¸ºä»£è¡¨ï¼Œé€šå¸¸ä¾èµ–äºâ€œæ•™å­¦â€ï¼Œè¿™æ¶‰åŠåœ¨åˆæˆé•¿å½¢å¼è¾“å‡ºä¸Šçš„æœ‰ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰ã€‚ç„¶è€Œï¼Œè¿™ç§ç­–ç•¥ä¸¥é‡ä¾èµ–äºåˆæˆçš„SFTæ•°æ®ï¼Œè¿™äº›æ•°æ®æ„å»ºå›°éš¾ä¸”æˆæœ¬é«˜æ˜‚ï¼Œå¾€å¾€ç¼ºä¹è¿è´¯æ€§å’Œä¸€è‡´æ€§ï¼Œä¸”å€¾å‘äºè¿‡äºäººå·¥å’Œç»“æ„å•è°ƒã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºæ¿€åŠ±çš„æ–¹æ³•ï¼Œå®ƒä»é›¶å¼€å§‹ï¼Œä¸ä¾èµ–ä»»ä½•æ³¨é‡Šæˆ–åˆæˆæ•°æ®ï¼Œåˆ©ç”¨å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰æ¥ä¿ƒè¿›å¤§è¯­è¨€æ¨¡å‹ä¸­è¶…é•¿é«˜è´¨é‡æ–‡æœ¬ç”Ÿæˆèƒ½åŠ›çš„å‡ºç°ã€‚æˆ‘ä»¬ä»åŸºç¡€æ¨¡å‹å¼€å§‹è¿›è¡ŒRLè®­ç»ƒï¼Œç±»ä¼¼äºR1-Zeroï¼ŒæŒ‡å¯¼å®ƒåœ¨è¿›è¡Œå†™ä½œè¿‡ç¨‹ä¸­çš„æ¨ç†ï¼Œä¿ƒè¿›è§„åˆ’å’Œæ”¹è¿›ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬é‡‡ç”¨ä¸“é—¨çš„å¥–åŠ±æ¨¡å‹æ¥å¼•å¯¼LLMæ”¹è¿›é•¿åº¦æ§åˆ¶ã€å†™ä½œè´¨é‡å’Œç»“æ„æ ¼å¼åŒ–ã€‚å®éªŒè¯„ä¼°è¡¨æ˜ï¼Œæˆ‘ä»¬çš„LongWriter-Zeroæ¨¡å‹ä»Qwen2.5-32Bå¼€å§‹è®­ç»ƒï¼Œåœ¨é•¿æœŸå†™ä½œä»»åŠ¡ä¸Šå§‹ç»ˆä¼˜äºä¼ ç»ŸSFTæ–¹æ³•ï¼Œåœ¨WritingBenchå’ŒArena-Writeçš„æ‰€æœ‰æŒ‡æ ‡ä¸Šè¾¾åˆ°æœ€æ–°æ°´å¹³ï¼Œç”šè‡³è¶…è¶Šäº†DeepSeek R1å’ŒQwen3-235Bç­‰ç™¾äº¿çº§æ¨¡å‹ã€‚æˆ‘ä»¬å°†åœ¨<a target="_blank" rel="noopener" href="https://huggingface.co/THU-KEG/LongWriter-Zero-32B">https://huggingface.co/THU-KEG/LongWriter-Zero-32B</a>ä¸‹å¼€æºæˆ‘ä»¬çš„æ•°æ®å’Œæ¨¡å‹æ£€æŸ¥ç‚¹ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.18841v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†åˆ©ç”¨åŸºäºå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰çš„æ–¹æ³•è®­ç»ƒå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è¿›è¡Œè¶…é•¿æ–‡æœ¬ç”Ÿæˆçš„ä»»åŠ¡ã€‚è¯¥ç ”ç©¶æå‡ºäº†ä¸€ç§ä¸ä¾èµ–ä»»ä½•æ ‡æ³¨æˆ–åˆæˆæ•°æ®çš„æ¿€åŠ±åŒ–æ–¹æ³•ï¼Œä»åŸºç¡€æ¨¡å‹å¼€å§‹ï¼Œé€šè¿‡RLè®­ç»ƒä¿ƒè¿›LLMç”Ÿæˆè¶…é•¿ã€é«˜è´¨é‡çš„æ–‡æœ¬ã€‚é€šè¿‡ä¸“ä¸šåŒ–çš„å¥–åŠ±æ¨¡å‹ï¼Œå¼•å¯¼LLMæ”¹å–„é•¿åº¦æ§åˆ¶ã€å†™ä½œè´¨é‡å’Œç»“æ„æ ¼å¼åŒ–ã€‚å®éªŒè¯„ä¼°è¡¨æ˜ï¼ŒLongWriter-Zeroæ¨¡å‹åœ¨é•¿ç¯‡å†™ä½œä»»åŠ¡ä¸ŠæŒç»­ä¼˜äºä¼ ç»Ÿçš„ç›‘ç£å¾®è°ƒæ–¹æ³•ï¼Œå¹¶åœ¨WritingBenchå’ŒArena-Writeä¸Šè¾¾åˆ°é¢†å…ˆæ°´å¹³ï¼Œç”šè‡³è¶…è¶Šäº†DeepSeek R1å’ŒQwen3ç­‰å¤§å‹æ¨¡å‹ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨è¶…é•¿æ–‡æœ¬ç”Ÿæˆæ–¹é¢å­˜åœ¨æŒ‘æˆ˜ï¼ŒåŒ…æ‹¬æœ€å¤§ç”Ÿæˆé•¿åº¦é™åˆ¶å’Œåºåˆ—é•¿åº¦å¢åŠ å¯¼è‡´çš„æ•´ä½“è´¨é‡ä¸‹é™ã€‚</li>
<li>æ­¤å‰çš„ç ”ç©¶å¦‚LongWriterä¸»è¦ä¾èµ–ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰åœ¨åˆæˆé•¿æ–‡è¾“å‡ºä¸Šçš„ç­–ç•¥ï¼Œä½†è¿™ç§æ–¹æ³•ä¾èµ–äºéš¾ä»¥æ„å»ºä¸”æˆæœ¬é«˜æ˜‚çš„åˆæˆSFTæ•°æ®ã€‚</li>
<li>æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºæ¿€åŠ±çš„æ–¹æ³•ï¼Œä¸ä¾èµ–ä»»ä½•æ ‡æ³¨æˆ–åˆæˆæ•°æ®ï¼Œåˆ©ç”¨å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰ä¿ƒè¿›è¶…é•¿ã€é«˜è´¨é‡çš„æ–‡æœ¬ç”Ÿæˆèƒ½åŠ›åœ¨LLMsä¸­çš„å‡ºç°ã€‚</li>
<li>é€šè¿‡ä¸“ä¸šåŒ–çš„å¥–åŠ±æ¨¡å‹ï¼Œæ”¹å–„LLMçš„é•¿åº¦æ§åˆ¶ã€å†™ä½œè´¨é‡å’Œç»“æ„æ ¼å¼åŒ–ã€‚</li>
<li>å®éªŒè¡¨æ˜ï¼ŒLongWriter-Zeroæ¨¡å‹åœ¨é•¿ç¯‡å†™ä½œä»»åŠ¡ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œè¾¾åˆ°ç”šè‡³è¶…è¶Šäº†ä¸€äº›å¤§å‹æ¨¡å‹çš„æ°´å¹³ã€‚</li>
<li>è¯¥ç ”ç©¶å¼€æ”¾æºä»£ç å’Œæ•°æ®ï¼Œä¾¿äºå…¶ä»–ç ”ç©¶è€…ä½¿ç”¨å’Œè¿›ä¸€æ­¥æ¢ç´¢ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.18841">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-3c2fba3801aba0ddb41c9f8b07a2d3fd.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-77c224b16368e01693b8bad8317d5864.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a9359518a63d016798dd3bb2aa619b2c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-08a96e2d43f91c548aec8d43686be894.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="ConciseHint-Boosting-Efficient-Reasoning-via-Continuous-Concise-Hints-during-Generation"><a href="#ConciseHint-Boosting-Efficient-Reasoning-via-Continuous-Concise-Hints-during-Generation" class="headerlink" title="ConciseHint: Boosting Efficient Reasoning via Continuous Concise Hints   during Generation"></a>ConciseHint: Boosting Efficient Reasoning via Continuous Concise Hints   during Generation</h2><p><strong>Authors:Siao Tang, Xinyin Ma, Gongfan Fang, Xinchao Wang</strong></p>
<p>Recent advancements in large reasoning models (LRMs) like DeepSeek-R1 and OpenAI o1 series have achieved notable performance enhancements on complex reasoning tasks by scaling up the generation length by Chain-of-Thought (CoT). However, an emerging issue is their inclination to produce excessively verbose reasoning processes, leading to the inefficiency problem. Existing literature on improving efficiency mainly adheres to the before-reasoning paradigms such as prompting and reasoning or fine-tuning and reasoning, but ignores the promising direction of directly encouraging the model to speak concisely by intervening during the generation of reasoning. In order to fill the blank, we propose a framework dubbed ConciseHint, which continuously encourages the reasoning model to speak concisely by injecting the textual hint (manually designed or trained on the concise data) during the token generation of the reasoning process. Besides, ConciseHint is adaptive to the complexity of the query by adaptively adjusting the hint intensity, which ensures it will not undermine model performance. Experiments on the state-of-the-art LRMs, including DeepSeek-R1 and Qwen-3 series, demonstrate that our method can effectively produce concise reasoning processes while maintaining performance well. For instance, we achieve a reduction ratio of 65% for the reasoning length on GSM8K benchmark with Qwen-3 4B with nearly no accuracy loss. </p>
<blockquote>
<p>æœ€è¿‘ï¼Œå¦‚DeepSeek-R1å’ŒOpenAI o1ç³»åˆ—ç­‰å¤§å‹æ¨ç†æ¨¡å‹ï¼ˆLRMsï¼‰é€šè¿‡æ€ç»´é“¾ï¼ˆChain-of-Thoughtï¼‰ï¼ˆCoTï¼‰æ‰©å±•ç”Ÿæˆé•¿åº¦ï¼Œåœ¨å¤æ‚æ¨ç†ä»»åŠ¡ä¸Šå®ç°äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚ç„¶è€Œï¼Œä¸€ä¸ªæ–°å…´çš„é—®é¢˜æ˜¯ä»–ä»¬å€¾å‘äºäº§ç”Ÿè¿‡äºå†—é•¿çš„æ¨ç†è¿‡ç¨‹ï¼Œå¯¼è‡´äº†æ•ˆç‡é—®é¢˜ã€‚å…³äºæé«˜æ•ˆç‡çš„ç°æœ‰æ–‡çŒ®ä¸»è¦éµå¾ªé¢„å…ˆæ¨ç†çš„æ¨¡å¼ï¼Œå¦‚æç¤ºå’Œæ¨ç†æˆ–å¾®è°ƒå†æ¨ç†ï¼Œä½†å¿½ç•¥äº†é€šè¿‡å¹²é¢„æ¨ç†ç”Ÿæˆè¿‡ç¨‹ä¸­ç›´æ¥é¼“åŠ±æ¨¡å‹ç®€æ´è¡¨è¾¾çš„æ½œåœ¨æ–¹å‘ã€‚ä¸ºäº†å¡«è¡¥è¿™ä¸€ç©ºç™½ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åä¸ºConciseHintçš„æ¡†æ¶ï¼Œå®ƒé€šè¿‡æ³¨å…¥æ–‡æœ¬æç¤ºï¼ˆäººä¸ºè®¾è®¡æˆ–åœ¨ç®€æ´æ•°æ®ä¸Šè®­ç»ƒï¼‰åœ¨æ¨ç†è¿‡ç¨‹çš„ä»¤ç‰Œç”Ÿæˆè¿‡ç¨‹ä¸­ä¸æ–­é¼“åŠ±æ¨ç†æ¨¡å‹ç®€æ´è¡¨è¾¾ã€‚æ­¤å¤–ï¼ŒConciseHintå¯ä»¥é€‚åº”æŸ¥è¯¢çš„å¤æ‚æ€§ï¼Œé€šè¿‡è‡ªé€‚åº”è°ƒæ•´æç¤ºå¼ºåº¦ï¼Œç¡®ä¿ä¸ä¼šæŸå®³æ¨¡å‹æ€§èƒ½ã€‚åœ¨åŒ…æ‹¬DeepSeek-R1å’ŒQwen-3ç³»åˆ—ç­‰åœ¨å†…çš„æœ€å…ˆè¿›LRMsä¸Šçš„å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•å¯ä»¥åœ¨ä¿æŒæ€§èƒ½çš„åŒæ—¶æœ‰æ•ˆåœ°äº§ç”Ÿç®€æ´çš„æ¨ç†è¿‡ç¨‹ã€‚ä¾‹å¦‚ï¼Œåœ¨GSM8KåŸºå‡†æµ‹è¯•ä¸Šï¼Œä½¿ç”¨Qwen-3 4Bè¿›è¡Œæ¨ç†é•¿åº¦ç¼©å‡äº†65%ï¼ŒåŒæ—¶å‡ ä¹æ²¡æœ‰æŸå¤±å‡†ç¡®æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.18810v1">PDF</a> Codes are available at <a target="_blank" rel="noopener" href="https://github.com/tsa18/ConciseHint">https://github.com/tsa18/ConciseHint</a></p>
<p><strong>Summary</strong></p>
<p>å¤§å‹æ¨ç†æ¨¡å‹ï¼ˆå¦‚DeepSeek-R1å’ŒOpenAI o1ç³»åˆ—ï¼‰åœ¨å¤æ‚çš„æ¨ç†ä»»åŠ¡ä¸Šå–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ï¼Œä½†å®ƒä»¬å€¾å‘ç”Ÿæˆå†—é•¿çš„æ¨ç†è¿‡ç¨‹ï¼Œå¯¼è‡´æ•ˆç‡é—®é¢˜ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§åä¸ºConciseHintçš„æ¡†æ¶ï¼Œé€šè¿‡åœ¨æ¨ç†è¿‡ç¨‹ä¸­æ³¨å…¥ç®€æ´çš„æ–‡æœ¬æç¤ºæ¥é¼“åŠ±æ¨¡å‹ç®€æ´è¡¨è¾¾ã€‚è¯¥æ¡†æ¶èƒ½å¤Ÿé€‚åº”æŸ¥è¯¢çš„å¤æ‚æ€§ï¼Œå¹¶èƒ½åœ¨ä¿æŒæ¨¡å‹æ€§èƒ½çš„åŒæ—¶ï¼Œæœ‰æ•ˆç”Ÿæˆç®€æ´çš„æ¨ç†è¿‡ç¨‹ã€‚åœ¨å¤šä¸ªå…ˆè¿›çš„å¤§å‹æ¨ç†æ¨¡å‹ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨ä¿æŒæ€§èƒ½çš„åŒæ—¶ï¼Œèƒ½å¤Ÿæ˜¾è‘—é™ä½æ¨ç†è¿‡ç¨‹çš„é•¿åº¦ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>å¤§å‹æ¨ç†æ¨¡å‹åœ¨å¤æ‚ä»»åŠ¡ä¸Šæ€§èƒ½æ˜¾è‘—æå‡ï¼Œä½†å­˜åœ¨ç”Ÿæˆå†—é•¿æ¨ç†è¿‡ç¨‹çš„é—®é¢˜ã€‚</li>
<li>ç°æœ‰æ–‡çŒ®ä¸»è¦å…³æ³¨æé«˜æ•ˆç‡çš„é¢„æ¨ç†æ–¹æ³•ï¼Œä½†ç¼ºä¹ç›´æ¥é¼“åŠ±æ¨¡å‹ç®€æ´è¡¨è¾¾çš„æ–¹æ³•ã€‚</li>
<li>æå‡ºäº†ä¸€ç§åä¸ºConciseHintçš„æ¡†æ¶ï¼Œé€šè¿‡æ³¨å…¥æ–‡æœ¬æç¤ºæ¥é¼“åŠ±æ¨¡å‹ç®€æ´è¡¨è¾¾ã€‚</li>
<li>ConciseHintèƒ½å¤Ÿé€‚åº”æŸ¥è¯¢çš„å¤æ‚æ€§ï¼Œç¡®ä¿åœ¨æé«˜æ•ˆç‡çš„åŒæ—¶ä¸æŸå®³æ¨¡å‹æ€§èƒ½ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.18810">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-7290828db52325588637a26b5d3fa8aa.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-dab27b579484f26a10f29913787ad90c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-526868ff0426ba729b4ca09ee19ecc20.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a44687f4dddaecaf13e73bf0ac478f88.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Harnessing-the-Power-of-Reinforcement-Learning-for-Language-Model-Based-Information-Retriever-via-Query-Document-Co-Augmentation"><a href="#Harnessing-the-Power-of-Reinforcement-Learning-for-Language-Model-Based-Information-Retriever-via-Query-Document-Co-Augmentation" class="headerlink" title="Harnessing the Power of Reinforcement Learning for Language-Model-Based   Information Retriever via Query-Document Co-Augmentation"></a>Harnessing the Power of Reinforcement Learning for Language-Model-Based   Information Retriever via Query-Document Co-Augmentation</h2><p><strong>Authors:Jingming Liu, Yumeng Li, Wei Shi, Yao-Xiang Ding, Hui Su, Kun Zhou</strong></p>
<p>Recent studies have proposed leveraging Large Language Models (LLMs) as information retrievers through query rewriting. However, for challenging corpora, we argue that enhancing queries alone is insufficient for robust semantic matching; the LLM should also have sufficient understanding of the corpus by directly handling and augmenting the documents themselves. To this end, we present an LLM-based retriever empowered to augment both user queries and corpus documents, with its policy fully explored via reinforcement learning (RL) and minimal human inductive bias. Notably, we find that simply allowing the LLM to modify documents yields little benefit unless paired with our carefully designed bidirectional RL framework, which enables the LLM to simultaneously learn and collaborate on both query and document augmentation policies. A key technical challenge in realizing such a framework lies in jointly updating both policies during training, where the rewards for the two directions depend on each other, making their entangled reward intractable. Our approach addresses this by introducing a reward sampling strategy and a specifically designed RL algorithm that enables effective training with these sampled rewards. Experimental results demonstrate that our approach significantly enhances LLM-based retrieval performance in both sparse and dense settings, particularly in difficult retrieval domains, and achieves strong cross-benchmark generalization. Our code is released at <a target="_blank" rel="noopener" href="https://github.com/liujm2001/CoAugRetriever">https://github.com/liujm2001/CoAugRetriever</a>. </p>
<blockquote>
<p>æœ€è¿‘çš„ç ”ç©¶æè®®åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰é€šè¿‡æŸ¥è¯¢é‡å†™æ¥ä½œä¸ºä¿¡æ¯æ£€ç´¢å™¨ã€‚ç„¶è€Œï¼Œå¯¹äºå…·æœ‰æŒ‘æˆ˜æ€§çš„è¯­æ–™åº“ï¼Œæˆ‘ä»¬è®¤ä¸ºä»…ä»…å¢å¼ºæŸ¥è¯¢å¯¹äºå®ç°ç¨³å¥çš„è¯­ä¹‰åŒ¹é…æ˜¯ä¸å¤Ÿçš„ï¼›LLMè¿˜åº”å¯¹è¯­æ–™åº“æœ¬èº«æœ‰è¶³å¤Ÿçš„ç†è§£ï¼Œé€šè¿‡ç›´æ¥å¤„ç†å’Œæ‰©å……æ–‡æ¡£æœ¬èº«ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºLLMçš„æ£€ç´¢å™¨ï¼Œè¯¥æ£€ç´¢å™¨èƒ½å¤ŸåŒæ—¶å¢å¼ºç”¨æˆ·æŸ¥è¯¢å’Œè¯­æ–™åº“æ–‡æ¡£ï¼Œå…¶ç­–ç•¥å®Œå…¨é€šè¿‡å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰è¿›è¡Œæ¢ç´¢ï¼Œå¹¶ä¸”å…·æœ‰æœ€å°çš„äººå·¥è¯±å¯¼åè§ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œæˆ‘ä»¬å‘ç°ï¼Œé™¤éä¸æˆ‘ä»¬çš„ç²¾å¿ƒè®¾è®¡åŒå‘RLæ¡†æ¶é…å¯¹ï¼Œå¦åˆ™ä»…ä»…å…è®¸LLMä¿®æ”¹æ–‡æ¡£å¹¶ä¸ä¼šå¸¦æ¥å¤šå°‘å¥½å¤„ã€‚è¯¥æ¡†æ¶ä½¿LLMèƒ½å¤Ÿåœ¨æŸ¥è¯¢å’Œæ–‡æ¡£æ‰©å……ç­–ç•¥ä¸ŠåŒæ—¶å­¦ä¹ å’Œåä½œã€‚åœ¨å®ç°è¿™æ ·ä¸€ä¸ªæ¡†æ¶çš„å…³é”®æŠ€æœ¯æŒ‘æˆ˜åœ¨äºåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­è”åˆæ›´æ–°è¿™ä¸¤ç§ç­–ç•¥ï¼Œå…¶ä¸­ä¸¤ä¸ªæ–¹å‘çš„å¥–åŠ±æ˜¯ç›¸äº’ä¾èµ–çš„ï¼Œä½¿å¾—å®ƒä»¬çº ç¼ çš„å¥–åŠ±éš¾ä»¥å¤„ç†ã€‚æˆ‘ä»¬çš„æ–¹æ³•é€šè¿‡å¼•å…¥å¥–åŠ±é‡‡æ ·ç­–ç•¥å’Œä¸“é—¨è®¾è®¡çš„RLç®—æ³•æ¥è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œè¿™äº›ç®—æ³•å¯ä»¥åˆ©ç”¨è¿™äº›é‡‡æ ·å¥–åŠ±è¿›è¡Œæœ‰æ•ˆçš„è®­ç»ƒã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨ç¨€ç–å’Œå¯†é›†çš„ç¯å¢ƒä¸­éƒ½èƒ½æ˜¾è‘—æé«˜åŸºäºLLMçš„æ£€ç´¢æ€§èƒ½ï¼Œç‰¹åˆ«æ˜¯åœ¨å›°éš¾çš„æ£€ç´¢é¢†åŸŸï¼Œå¹¶ä¸”å®ç°äº†è·¨åŸºå‡†æµ‹è¯•çš„å¼ºå¤§æ³›åŒ–èƒ½åŠ›ã€‚æˆ‘ä»¬çš„ä»£ç å·²å‘å¸ƒåœ¨<a target="_blank" rel="noopener" href="https://github.com/liujm2001/CoAugRetriever%E3%80%82">https://github.com/liujm2001/CoAugRetrieverã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.18670v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ¢è®¨äº†åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä½œä¸ºä¿¡æ¯æ£€ç´¢å™¨çš„é—®é¢˜ã€‚ä½œè€…è®¤ä¸ºï¼Œå¯¹äºå…·æœ‰æŒ‘æˆ˜æ€§çš„è¯­æ–™åº“ï¼Œä»…ä»…ä¼˜åŒ–æŸ¥è¯¢æ˜¯ä¸å¤Ÿçš„ï¼ŒLLMè¿˜éœ€è¦ç›´æ¥å¤„ç†å’Œæ‰©å……æ–‡æ¡£ä»¥å……åˆ†ç†è§£è¯­æ–™åº“ã€‚ä¸ºæ­¤ï¼Œæå‡ºäº†ä¸€ç§åŸºäºLLMçš„æ£€ç´¢å™¨ï¼Œè¯¥æ£€ç´¢å™¨èƒ½å¤ŸåŒæ—¶æ‰©å……ç”¨æˆ·æŸ¥è¯¢å’Œè¯­æ–™åº“æ–‡æ¡£ï¼Œå…¶ç­–ç•¥é€šè¿‡å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰è¿›è¡Œæ¢ç´¢ï¼Œå¹¶å…·æœ‰æœ€å°çš„äººç±»è¯±å¯¼åè§ã€‚ç ”ç©¶å‘ç°ï¼Œé™¤éä¸ç²¾å¿ƒè®¾è®¡çš„åŒå‘RLæ¡†æ¶é…å¯¹ï¼Œå¦åˆ™å•çº¯å…è®¸LLMä¿®æ”¹æ–‡æ¡£å¹¶ä¸ä¼šå¸¦æ¥æ˜æ˜¾å¥½å¤„ã€‚è¯¥æ¡†æ¶çš„å…³é”®æŠ€æœ¯æŒ‘æˆ˜åœ¨äºè®­ç»ƒè¿‡ç¨‹ä¸­çš„ç­–ç•¥è”åˆæ›´æ–°é—®é¢˜ï¼Œä¸¤ä¸ªæ–¹å‘çš„å¥–åŠ±ç›¸äº’ä¾èµ–ï¼Œä½¿å¾—å®ƒä»¬çš„å¥–åŠ±çº ç¼ éš¾ä»¥è§£å†³ã€‚ä½œè€…é€šè¿‡å¼•å…¥å¥–åŠ±é‡‡æ ·ç­–ç•¥å’Œä¸“é—¨è®¾è®¡çš„RLç®—æ³•æ¥è§£å†³è¿™ä¸€é—®é¢˜ï¼Œå®ç°äº†æœ‰æ•ˆçš„è®­ç»ƒã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨ç¨€ç–å’Œå¯†é›†ç¯å¢ƒä¸­éƒ½èƒ½æ˜¾è‘—æé«˜LLMçš„æ£€ç´¢æ€§èƒ½ï¼Œç‰¹åˆ«æ˜¯åœ¨å›°éš¾æ£€ç´¢é¢†åŸŸå…·æœ‰å¾ˆå¼ºçš„è·¨åŸºå‡†æµ‹è¯•æ³›åŒ–èƒ½åŠ›ã€‚ç›¸å…³ä»£ç å·²å‘å¸ƒåœ¨æŒ‡å®šçš„GitHubä»“åº“ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å¯ä½œä¸ºä¿¡æ¯æ£€ç´¢å™¨ï¼Œä½†éœ€å¢å¼ºå…¶åœ¨æŒ‘æˆ˜æ€§è¯­æ–™åº“ä¸Šçš„è¡¨ç°ã€‚</li>
<li>å•çº¯çš„æŸ¥è¯¢ä¼˜åŒ–ä¸è¶³ä»¥å®ç°ç¨³å¥çš„è¯­ä¹‰åŒ¹é…ï¼ŒLLMéœ€è¦ç›´æ¥å¤„ç†å’Œæ‰©å……æ–‡æ¡£ä»¥ç†è§£è¯­æ–™åº“ã€‚</li>
<li>æå‡ºäº†ä¸€ç§åŸºäºLLMçš„æ£€ç´¢å™¨ï¼Œå…·å¤‡å¯¹ç”¨æˆ·æŸ¥è¯¢å’Œè¯­æ–™åº“æ–‡æ¡£çš„åŒå‘æ‰©å……èƒ½åŠ›ã€‚</li>
<li>é‡‡ç”¨å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰æ¢ç´¢æ‰©å……ç­–ç•¥ï¼Œå‡å°‘äººä¸ºå¹²é¢„å’Œåè§ã€‚</li>
<li>åŒå‘RLæ¡†æ¶çš„è®¾è®¡æ˜¯å…³é”®ï¼Œèƒ½åŒæ—¶å­¦ä¹ å’Œåä½œå¤„ç†æŸ¥è¯¢å’Œæ–‡æ¡£æ‰©å……ç­–ç•¥ã€‚</li>
<li>è®­ç»ƒè¿‡ç¨‹ä¸­çš„ç­–ç•¥è”åˆæ›´æ–°æ˜¯æŠ€æœ¯æŒ‘æˆ˜ï¼Œä½œè€…é€šè¿‡å¼•å…¥å¥–åŠ±é‡‡æ ·ç­–ç•¥å’Œç‰¹å®šçš„RLç®—æ³•è§£å†³äº†è¿™ä¸€é—®é¢˜ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.18670">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-4a9749187a0d8ce9083ec59b03c300ed.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-710f8d6d5e281a4b86447b39544dd4c5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d286e10a69fe146738e710d5d7de020e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-40df5da984197e979a4eb0bc43ae3846.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-951ad6d2626222d860828f761c2ec5be.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="ReDit-Reward-Dithering-for-Improved-LLM-Policy-Optimization"><a href="#ReDit-Reward-Dithering-for-Improved-LLM-Policy-Optimization" class="headerlink" title="ReDit: Reward Dithering for Improved LLM Policy Optimization"></a>ReDit: Reward Dithering for Improved LLM Policy Optimization</h2><p><strong>Authors:Chenxing Wei, Jiarui Yu, Ying Tiffany He, Hande Dong, Yao Shu, Fei Yu</strong></p>
<p>DeepSeek-R1 has successfully enhanced Large Language Model (LLM) reasoning capabilities through its rule-based reward system. While itâ€™s a â€˜â€™perfectâ€™â€™ reward system that effectively mitigates reward hacking, such reward functions are often discrete. Our experimental observations suggest that discrete rewards can lead to gradient anomaly, unstable optimization, and slow convergence. To address this issue, we propose ReDit (Reward Dithering), a method that dithers the discrete reward signal by adding simple random noise. With this perturbed reward, exploratory gradients are continuously provided throughout the learning process, enabling smoother gradient updates and accelerating convergence. The injected noise also introduces stochasticity into flat reward regions, encouraging the model to explore novel policies and escape local optima. Experiments across diverse tasks demonstrate the effectiveness and efficiency of ReDit. On average, ReDit achieves performance comparable to vanilla GRPO with only approximately 10% the training steps, and furthermore, still exhibits a 4% performance improvement over vanilla GRPO when trained for a similar duration. Visualizations confirm significant mitigation of gradient issues with ReDit. Moreover, theoretical analyses are provided to further validate these advantages. </p>
<blockquote>
<p>DeepSeek-R1å·²æˆåŠŸé€šè¿‡å…¶åŸºäºè§„åˆ™çš„å¥–åŠ±ç³»ç»Ÿå¢å¼ºäº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ¨ç†èƒ½åŠ›ã€‚è™½ç„¶å®ƒæ˜¯ä¸€ä¸ªâ€œå®Œç¾â€çš„å¥–åŠ±ç³»ç»Ÿï¼Œèƒ½å¤Ÿæœ‰æ•ˆåœ°éåˆ¶å¥–åŠ±é»‘å®¢è¡Œä¸ºï¼Œä½†è¿™æ ·çš„å¥–åŠ±åŠŸèƒ½æ˜¯ç¦»æ•£çš„ã€‚æˆ‘ä»¬çš„å®éªŒè§‚å¯Ÿè¡¨æ˜ï¼Œç¦»æ•£å¥–åŠ±å¯èƒ½å¯¼è‡´æ¢¯åº¦å¼‚å¸¸ã€ä¼˜åŒ–ä¸ç¨³å®šå’Œæ”¶æ•›ç¼“æ…¢ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ReDitï¼ˆå¥–åŠ±æŠ–åŠ¨ï¼‰æ–¹æ³•ï¼Œå®ƒé€šè¿‡æ·»åŠ ç®€å•çš„éšæœºå™ªå£°æ¥æŠ–åŠ¨ç¦»æ•£å¥–åŠ±ä¿¡å·ã€‚é€šè¿‡æ‰°åŠ¨å¥–åŠ±ï¼Œå¯ä»¥åœ¨æ•´ä¸ªå­¦ä¹ è¿‡ç¨‹ä¸­æŒç»­æä¾›æ¢ç´¢æ€§æ¢¯åº¦ï¼Œä»è€Œå®ç°æ›´å¹³æ»‘çš„æ¢¯åº¦æ›´æ–°å¹¶åŠ é€Ÿæ”¶æ•›ã€‚æ³¨å…¥çš„å™ªå£°è¿˜ä¸ºå¹³å¦å¥–åŠ±åŒºåŸŸå¼•å…¥äº†éšæœºæ€§ï¼Œé¼“åŠ±æ¨¡å‹æ¢ç´¢æ–°ç­–ç•¥å¹¶é€ƒç¦»å±€éƒ¨æœ€ä¼˜è§£ã€‚åœ¨ä¸åŒä»»åŠ¡ä¸Šçš„å®éªŒè¯æ˜äº†ReDitçš„æœ‰æ•ˆæ€§å’Œæ•ˆç‡ã€‚å¹³å‡è€Œè¨€ï¼ŒReDitåœ¨ä»…ä½¿ç”¨å¤§çº¦10%çš„è®­ç»ƒæ­¥éª¤çš„æƒ…å†µä¸‹å®ç°äº†ä¸å¸¸è§„GRPOç›¸å½“çš„æ€§èƒ½ï¼Œå¹¶ä¸”åœ¨ç»è¿‡ç±»ä¼¼æ—¶é—´çš„è®­ç»ƒåï¼Œå…¶æ€§èƒ½è¿˜æé«˜äº†4%ã€‚å¯è§†åŒ–ç»“æœè¯å®äº†ReDitåœ¨è§£å†³æ¢¯åº¦é—®é¢˜æ–¹é¢çš„æ˜¾è‘—ä½œç”¨ã€‚æ­¤å¤–ï¼Œè¿˜æä¾›äº†ç†è®ºåˆ†æä»¥è¿›ä¸€æ­¥éªŒè¯è¿™äº›ä¼˜åŠ¿ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.18631v1">PDF</a> 10 pages, 15 figures</p>
<p><strong>Summary</strong><br>æ·±åº¦å­¦ä¹ æ¨¡å‹DeepSeek-R1é€šè¿‡åŸºäºè§„åˆ™çš„å¥–åŠ±ç³»ç»Ÿå¢å¼ºäº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ¨ç†èƒ½åŠ›ã€‚ç„¶è€Œï¼Œå°½ç®¡å®ƒæ˜¯ä¸€ä¸ªå®Œç¾çš„å¥–åŠ±ç³»ç»Ÿï¼Œèƒ½æœ‰æ•ˆéåˆ¶å¥–åŠ±ä½œå¼Šï¼Œä½†è¿™ç§å¥–åŠ±åŠŸèƒ½é€šå¸¸æ˜¯ç¦»æ•£çš„ã€‚å®éªŒè§‚å¯Ÿè¡¨æ˜ï¼Œç¦»æ•£å¥–åŠ±å¯èƒ½å¯¼è‡´æ¢¯åº¦å¼‚å¸¸ã€ä¼˜åŒ–ä¸ç¨³å®šå’Œæ”¶æ•›ç¼“æ…¢ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºReDitï¼ˆå¥–åŠ±æŠ–åŠ¨ï¼‰çš„æ–¹æ³•ï¼Œé€šè¿‡æ·»åŠ ç®€å•éšæœºå™ªå£°æ¥æŠ–åŠ¨ç¦»æ•£å¥–åŠ±ä¿¡å·ã€‚è¿™ç§æ‰°åŠ¨å¥–åŠ±ä¸ºå­¦ä¹ è¿‡ç¨‹æä¾›äº†æŒç»­çš„æ¢ç´¢æ€§æ¢¯åº¦ï¼Œä½¿æ¢¯åº¦æ›´æ–°æ›´åŠ å¹³æ»‘ï¼ŒåŠ é€Ÿæ”¶æ•›ã€‚æ³¨å…¥çš„å™ªå£°è¿˜ä¸ºå¹³å¦å¥–åŠ±åŒºåŸŸå¼•å…¥äº†éšæœºæ€§ï¼Œé¼“åŠ±æ¨¡å‹æ¢ç´¢æ–°ç­–ç•¥å¹¶é€ƒç¦»å±€éƒ¨æœ€ä¼˜ã€‚åœ¨å¤šç§ä»»åŠ¡ä¸Šçš„å®éªŒè¯æ˜äº†ReDitçš„æœ‰æ•ˆæ€§å’Œæ•ˆç‡ã€‚ReDitåœ¨å¹³å‡æƒ…å†µä¸‹å®ç°äº†ä¸åŸºç¡€GRPOç›¸å½“çš„æ€§èƒ½ï¼Œä½†ä»…ä½¿ç”¨äº†çº¦10%çš„è®­ç»ƒæ­¥éª¤ã€‚å½“è®­ç»ƒæ—¶é—´ç›¸ä¼¼æ—¶ï¼ŒReDitçš„æ€§èƒ½è¿˜æé«˜äº†4%ã€‚å¯è§†åŒ–ç»“æœè¯å®äº†ReDitå¯¹æ¢¯åº¦é—®é¢˜çš„æ˜¾è‘—ç¼“è§£ã€‚æ­¤å¤–ï¼Œè¿˜æä¾›äº†ç†è®ºåˆ†æä»¥è¿›ä¸€æ­¥éªŒè¯è¿™äº›ä¼˜åŠ¿ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>DeepSeek-R1é€šè¿‡åŸºäºè§„åˆ™çš„å¥–åŠ±ç³»ç»Ÿå¢å¼ºäº†LLMçš„æ¨ç†èƒ½åŠ›ï¼Œä½†å­˜åœ¨ç¦»æ•£å¥–åŠ±å¯¼è‡´çš„é—®é¢˜ã€‚</li>
<li>ç¦»æ•£å¥–åŠ±å¯èƒ½å¯¼è‡´æ¢¯åº¦å¼‚å¸¸ã€ä¼˜åŒ–ä¸ç¨³å®šå’Œæ”¶æ•›ç¼“æ…¢ã€‚</li>
<li>ReDitï¼ˆå¥–åŠ±æŠ–åŠ¨ï¼‰æ–¹æ³•é€šè¿‡æ·»åŠ éšæœºå™ªå£°æ¥è§£å†³ç¦»æ•£å¥–åŠ±çš„é—®é¢˜ã€‚</li>
<li>æ‰°åŠ¨å¥–åŠ±æœ‰åŠ©äºæä¾›æŒç»­çš„æ¢ç´¢æ€§æ¢¯åº¦ï¼ŒåŠ é€Ÿæ”¶æ•›å¹¶æ”¹å–„æ¨¡å‹æ€§èƒ½ã€‚</li>
<li>ReDitåœ¨å¤šç§ä»»åŠ¡ä¸Šè¡¨ç°å‡ºé«˜æ•ˆæ€§å’Œæœ‰æ•ˆæ€§ï¼Œå®ç°äº†ä¸åŸºç¡€GRPOç›¸å½“çš„æ€§èƒ½ï¼Œå¹¶å‡å°‘äº†è®­ç»ƒæ­¥éª¤ã€‚</li>
<li>ReDitåœ¨ç›¸ä¼¼è®­ç»ƒæ—¶é—´å†…ç›¸æ¯”åŸºç¡€GRPOæœ‰4%çš„æ€§èƒ½æå‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.18631">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-41e5ef6bc055d3f9fe8873b27276c2f1.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-871e6579002ad96f9e267e9a4f33a987.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-13f09a2a5b3a09b6554df387d0c5618e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c42f709fd6a2347188a67a06be7245e3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-31aa52130c5ba863cc1fd7475dce2b66.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-259e0feedb5b0c9439c1e5ca8fde4ce5.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="MedTVT-R1-A-Multimodal-LLM-Empowering-Medical-Reasoning-and-Diagnosis"><a href="#MedTVT-R1-A-Multimodal-LLM-Empowering-Medical-Reasoning-and-Diagnosis" class="headerlink" title="MedTVT-R1: A Multimodal LLM Empowering Medical Reasoning and Diagnosis"></a>MedTVT-R1: A Multimodal LLM Empowering Medical Reasoning and Diagnosis</h2><p><strong>Authors:Yuting Zhang, Kaishen Yuan, Hao Lu, Yutao Yue, Jintai Chen, Kaishun Wu</strong></p>
<p>Accurate and interpretable multi-disease diagnosis remains a critical challenge in medical research, particularly when leveraging heterogeneous multimodal medical data. Current approaches often rely on single-modal data, limiting their ability to comprehensively understand complex diseases. To address this, we propose MedTVT-R1, a novel Multimodal Large Language Model (MLLM) framework designed to integrate clinical multimodal data for reasoning and diagnosing multiple diseases. We construct MedTVT-QA, a curated instruction dataset that provides question-answer pairs for physiological-level interpretations and disease-level diagnoses with a Chain of Evidence approach. MedTVT-R1 incorporates a modality perception layer to capture inter-modal dependencies and adaptively weight modality contributions. Additionally, we employ Group Relative Policy Optimization (GRPO)-based Reinforcement Fine-Tuning with a Jaccard Reward function to enhance diagnostic reasoning. Experimental results demonstrate MedTVT-R1â€™s superiority in multimodal feature utilization and multi-disease diagnosis, offering significant potential for clinical applications such as diagnostic report generation and comorbidity reasoning. The dataset and code are available at <a target="_blank" rel="noopener" href="https://github.com/keke-nice/MedTVT-R1">https://github.com/keke-nice/MedTVT-R1</a>. </p>
<blockquote>
<p>åœ¨åŒ»å­¦ç ”ç©¶ä¸­ï¼Œå‡†ç¡®ä¸”å¯è§£é‡Šçš„å¤šç–¾ç—…è¯Šæ–­ä»ç„¶æ˜¯ä¸€ä¸ªå…³é”®æŒ‘æˆ˜ï¼Œå°¤å…¶æ˜¯åœ¨åˆ©ç”¨å¼‚æ„å¤šæ¨¡æ€åŒ»å­¦æ•°æ®æ—¶ã€‚å½“å‰çš„æ–¹æ³•å¾€å¾€ä¾èµ–äºå•æ¨¡æ€æ•°æ®ï¼Œé™åˆ¶äº†å®ƒä»¬å…¨é¢ç†è§£å¤æ‚ç–¾ç—…çš„èƒ½åŠ›ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†MedTVT-R1ï¼Œè¿™æ˜¯ä¸€ç§æ–°å‹çš„å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰æ¡†æ¶ï¼Œæ—¨åœ¨æ•´åˆä¸´åºŠå¤šæ¨¡æ€æ•°æ®è¿›è¡Œæ¨ç†å’Œå¤šç§ç–¾ç—…çš„è¯Šæ–­ã€‚æˆ‘ä»¬æ„å»ºäº†MedTVT-QAï¼Œè¿™æ˜¯ä¸€ä¸ªç²¾é€‰çš„æŒ‡ä»¤æ•°æ®é›†ï¼Œæä¾›ç”¨äºç”Ÿç†æ°´å¹³è§£é‡Šå’Œç–¾ç—…æ°´å¹³è¯Šæ–­çš„é—®é¢˜ç­”æ¡ˆå¯¹ï¼Œé‡‡ç”¨è¯æ®é“¾æ–¹æ³•ã€‚MedTVT-R1èå…¥äº†ä¸€ä¸ªæ¨¡æ€æ„ŸçŸ¥å±‚ï¼Œä»¥æ•æ‰è·¨æ¨¡æ€çš„ä¾èµ–å…³ç³»å¹¶è‡ªé€‚åº”åœ°åŠ æƒæ¨¡æ€è´¡çŒ®ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬é‡‡ç”¨åŸºäºç¾¤ä½“ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆGRPOï¼‰çš„å¼ºåŒ–å¾®è°ƒæ–¹æ³•ï¼Œä½¿ç”¨Jaccardå¥–åŠ±å‡½æ•°æ¥æé«˜è¯Šæ–­æ¨ç†èƒ½åŠ›ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒMedTVT-R1åœ¨å¤šæ¨¡æ€ç‰¹å¾åˆ©ç”¨å’Œå¤šç–¾ç—…è¯Šæ–­æ–¹é¢å…·æœ‰ä¼˜åŠ¿ï¼Œåœ¨ä¸´åºŠåº”ç”¨å¦‚ç”Ÿæˆè¯Šæ–­æŠ¥å‘Šå’Œåˆå¹¶ç—‡æ¨ç†ç­‰æ–¹é¢å…·æœ‰å·¨å¤§æ½œåŠ›ã€‚æ•°æ®é›†å’Œä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/keke-nice/MedTVT-R1%E8%8E%B7%E5%BE%97%E3%80%82">https://github.com/keke-nice/MedTVT-R1è·å¾—ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.18512v1">PDF</a> </p>
<p><strong>Summary</strong><br>åŒ»å­¦ç ”ç©¶ä¸­ï¼Œå¤šç–¾ç—…å‡†ç¡®ä¸”å¯è§£é‡Šçš„è¯Šæ–­ä»ç„¶æ˜¯ä¸€ä¸ªé‡å¤§æŒ‘æˆ˜ï¼Œç‰¹åˆ«æ˜¯åœ¨åˆ©ç”¨å¼‚è´¨å¤šæ¨¡æ€åŒ»ç–—æ•°æ®æ–¹é¢ã€‚å½“å‰æ–¹æ³•å¸¸ä¾èµ–äºå•ä¸€æ¨¡æ€æ•°æ®ï¼Œæ— æ³•å…¨é¢ç†è§£å¤æ‚ç–¾ç—…ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºMedTVT-R1ï¼Œä¸€ç§æ–°å‹å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰æ¡†æ¶ï¼Œæ—¨åœ¨æ•´åˆä¸´åºŠå¤šæ¨¡æ€æ•°æ®è¿›è¡Œæ¨ç†å’Œå¤šç§ç–¾ç—…çš„è¯Šæ–­ã€‚æˆ‘ä»¬æ„å»ºäº†MedTVT-QAæ•°æ®é›†ï¼Œæä¾›ç”Ÿç†æ°´å¹³è§£è¯»å’Œç–¾ç—…æ°´å¹³è¯Šæ–­çš„é—®é¢˜ç­”æ¡ˆå¯¹ï¼Œé‡‡ç”¨è¯æ®é“¾æ–¹æ³•ã€‚MedTVT-R1èå…¥æ¨¡æ€æ„ŸçŸ¥å±‚ä»¥æ•æ‰è·¨æ¨¡æ€ä¾èµ–å…³ç³»å¹¶è‡ªé€‚åº”åœ°åŠ æƒæ¨¡æ€è´¡çŒ®ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬é‡‡ç”¨åŸºäºç»„ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆGRPOï¼‰çš„å¼ºåŒ–ç²¾ç»†è°ƒæ•´æ–¹æ³•ï¼Œç»“åˆJaccardå¥–åŠ±å‡½æ•°ï¼Œæå‡è¯Šæ–­æ¨ç†èƒ½åŠ›ã€‚å®éªŒç»“æœè¯æ˜MedTVT-R1åœ¨å¤šæ¨¡æ€ç‰¹å¾åˆ©ç”¨å’Œå¤šç–¾ç—…è¯Šæ–­æ–¹é¢çš„ä¼˜è¶Šæ€§ï¼Œåœ¨ç”Ÿæˆè¯Šæ–­æŠ¥å‘Šå’Œåˆå¹¶ç—‡æ¨ç†ç­‰ä¸´åºŠåº”ç”¨æ–¹é¢å±•ç°å‡ºå·¨å¤§æ½œåŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤šç–¾ç—…è¯Šæ–­é¢ä¸´æŒ‘æˆ˜ï¼šå½“å‰æ–¹æ³•ä¸»è¦ä¾èµ–å•ä¸€æ¨¡æ€æ•°æ®ï¼Œéš¾ä»¥å…¨é¢ç†è§£å¤æ‚ç–¾ç—…ã€‚</li>
<li>MedTVT-R1æ¡†æ¶ä»‹ç»ï¼šè¿™æ˜¯ä¸€ç§æ–°å‹çš„å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰ï¼Œæ—¨åœ¨æ•´åˆä¸´åºŠå¤šæ¨¡æ€æ•°æ®è¿›è¡Œæ¨ç†å’Œè¯Šæ–­ã€‚</li>
<li>MedTVT-QAæ•°æ®é›†ï¼šè¯¥æ•°æ®é›†åŒ…å«é—®é¢˜ç­”æ¡ˆå¯¹ï¼Œç”¨äºç”Ÿç†æ°´å¹³è§£è¯»å’Œç–¾ç—…æ°´å¹³è¯Šæ–­ï¼Œé‡‡ç”¨è¯æ®é“¾æ–¹æ³•ã€‚</li>
<li>MedTVT-R1ä¸­çš„æ¨¡æ€æ„ŸçŸ¥å±‚ï¼šè¿™ä¸€å±‚å¯ä»¥æ•æ‰è·¨æ¨¡æ€ä¾èµ–å…³ç³»ï¼Œå¹¶è‡ªé€‚åº”åœ°åŠ æƒä¸åŒæ¨¡æ€çš„è´¡çŒ®ã€‚</li>
<li>å¼ºåŒ–ç²¾ç»†è°ƒæ•´æ–¹æ³•ï¼šä½¿ç”¨åŸºäºç»„ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆGRPOï¼‰å’ŒJaccardå¥–åŠ±å‡½æ•°çš„å¼ºåŒ–ç²¾ç»†è°ƒæ•´ï¼Œæå‡è¯Šæ–­æ¨ç†èƒ½åŠ›ã€‚</li>
<li>å®éªŒç»“æœï¼šMedTVT-R1åœ¨å¤šæ¨¡æ€ç‰¹å¾åˆ©ç”¨å’Œå¤šç–¾ç—…è¯Šæ–­æ–¹é¢è¡¨ç°å‡ºä¼˜è¶Šæ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.18512">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-8853ca80eeb07680851d149e3d2ca7ed.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a5c68be5610034d9a45649aa3bb3d283.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-697d40b30cc27e37aba63b5e3e8a8d8e.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-e045ff7c6923bcfcf3292d2165b1748e.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="TReB-A-Comprehensive-Benchmark-for-Evaluating-Table-Reasoning-Capabilities-of-Large-Language-Models"><a href="#TReB-A-Comprehensive-Benchmark-for-Evaluating-Table-Reasoning-Capabilities-of-Large-Language-Models" class="headerlink" title="TReB: A Comprehensive Benchmark for Evaluating Table Reasoning   Capabilities of Large Language Models"></a>TReB: A Comprehensive Benchmark for Evaluating Table Reasoning   Capabilities of Large Language Models</h2><p><strong>Authors:Ce Li, Xiaofan Liu, Zhiyan Song, Ce Chi, Chen Zhao, Jingjing Yang, Zhendong Wang, Kexin Yang, Boshen Shi, Xing Wang, Chao Deng, Junlan Feng</strong></p>
<p>The majority of data in businesses and industries is stored in tables, databases, and data warehouses. Reasoning with table-structured data poses significant challenges for large language models (LLMs) due to its hidden semantics, inherent complexity, and structured nature. One of these challenges is lacking an effective evaluation benchmark fairly reflecting the performances of LLMs on broad table reasoning abilities. In this paper, we fill in this gap, presenting a comprehensive table reasoning evolution benchmark, TReB, which measures both shallow table understanding abilities and deep table reasoning abilities, a total of 26 sub-tasks. We construct a high quality dataset through an iterative data processing procedure. We create an evaluation framework to robustly measure table reasoning capabilities with three distinct inference modes, TCoT, PoT and ICoT. Further, we benchmark over 20 state-of-the-art LLMs using this frame work and prove its effectiveness. Experimental results reveal that existing LLMs still have significant room for improvement in addressing the complex and real world Table related tasks. Both the dataset and evaluation framework are publicly available, with the dataset hosted on [HuggingFace] and the framework on [GitHub]. </p>
<blockquote>
<p>åœ¨å•†ä¸šå’Œå·¥ä¸šé¢†åŸŸï¼Œå¤§éƒ¨åˆ†æ•°æ®éƒ½å­˜å‚¨åœ¨è¡¨æ ¼ã€æ•°æ®åº“å’Œæ•°æ®ä»“åº“ä¸­ã€‚ç”±äºè¡¨æ ¼ç»“æ„æ•°æ®çš„éšè—è¯­ä¹‰ã€å›ºæœ‰å¤æ‚æ€§å’Œç»“æ„æ€§è´¨ï¼Œå¯¹äºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰æ¥è¯´ï¼Œå¯¹å…¶è¿›è¡Œæ¨ç†æ„æˆäº†é‡å¤§æŒ‘æˆ˜ã€‚è¿™äº›æŒ‘æˆ˜ä¹‹ä¸€æ˜¯ç¼ºä¹æœ‰æ•ˆè¯„ä¼°åŸºå‡†ï¼Œæ— æ³•å…¬æ­£åœ°åæ˜ LLMåœ¨å¹¿æ³›è¡¨æ ¼æ¨ç†èƒ½åŠ›æ–¹é¢çš„è¡¨ç°ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬å¼¥è¡¥äº†è¿™ä¸€ç©ºç™½ï¼Œæå‡ºäº†ä¸€ä¸ªå…¨é¢çš„è¡¨æ ¼æ¨ç†è¿›åŒ–åŸºå‡†æµ‹è¯•ï¼ˆTReBï¼‰ï¼Œè¯¥åŸºå‡†æµ‹è¯•è¡¡é‡äº†æµ…å±‚æ¬¡çš„è¡¨æ ¼ç†è§£èƒ½åŠ›å’Œæ·±å±‚æ¬¡çš„è¡¨æ ¼æ¨ç†èƒ½åŠ›ï¼Œå…±æœ‰26ä¸ªå­ä»»åŠ¡ã€‚æˆ‘ä»¬é€šè¿‡è¿­ä»£æ•°æ®å¤„ç†ç¨‹åºæ„å»ºäº†é«˜è´¨é‡çš„æ•°æ®é›†ã€‚æˆ‘ä»¬åˆ›å»ºäº†ä¸€ä¸ªè¯„ä¼°æ¡†æ¶ï¼Œé€šè¿‡ä¸‰ç§ä¸åŒçš„æ¨ç†æ¨¡å¼ï¼ˆTCoTã€PoTå’ŒICoTï¼‰æ¥ç¨³å¥åœ°è¡¡é‡è¡¨æ ¼æ¨ç†èƒ½åŠ›ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬ä½¿ç”¨æ­¤æ¡†æ¶å¯¹20å¤šé¡¹æœ€æ–°LLMè¿›è¡Œäº†åŸºå‡†æµ‹è¯•ï¼Œå¹¶è¯æ˜äº†å…¶æœ‰æ•ˆæ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œç°æœ‰çš„LLMåœ¨è§£å†³å¤æ‚å’Œç°å®ä¸–ç•Œä¸­çš„è¡¨æ ¼ç›¸å…³ä»»åŠ¡æ–¹é¢ä»æœ‰å¾ˆå¤§çš„æ”¹è¿›ç©ºé—´ã€‚æ•°æ®é›†å’Œè¯„ä¼°æ¡†æ¶å‡å…¬å¼€å¯ç”¨ï¼Œæ•°æ®é›†æ‰˜ç®¡åœ¨[HuggingFace]ï¼Œæ¡†æ¶æ‰˜ç®¡åœ¨[GitHub]ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.18421v1">PDF</a> Benmark report v1.0</p>
<p><strong>Summary</strong><br>æ–‡æœ¬ä»‹ç»äº†åœ¨å•†ä¸šå’Œå·¥ä¸šé¢†åŸŸä¸­ï¼Œå¤§éƒ¨åˆ†æ•°æ®ä»¥è¡¨æ ¼å½¢å¼å­˜å‚¨åœ¨æ•°æ®åº“å’Œæ•°æ®ä»“åº“ä¸­ã€‚ç”±äºè¡¨æ ¼ç»“æ„çš„éšè—è¯­ä¹‰ã€å†…åœ¨å¤æ‚æ€§å’Œç»“æ„åŒ–ç‰¹æ€§ï¼Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨å¤„ç†è¡¨æ ¼æ•°æ®æ—¶é¢ä¸´æŒ‘æˆ˜ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§å…¨é¢çš„è¡¨æ ¼æ¨ç†è¯„ä¼°åŸºå‡†ï¼ˆTReBï¼‰ï¼Œæ—¨åœ¨è¡¡é‡LLMsåœ¨è¡¨æ ¼ç†è§£æ–¹é¢çš„èƒ½åŠ›ï¼ŒåŒ…æ‹¬æµ…å±‚æ¬¡ç†è§£å’Œæ·±å±‚æ¬¡æ¨ç†èƒ½åŠ›ï¼Œå…±æ¶‰åŠ26ä¸ªå­ä»»åŠ¡ã€‚é€šè¿‡è¿­ä»£æ•°æ®å¤„ç†ç¨‹åºæ„å»ºé«˜è´¨é‡æ•°æ®é›†ï¼Œå¹¶åˆ›å»ºä¸€ä¸ªè¯„ä¼°æ¡†æ¶æ¥ç¨³å¥åœ°è¡¡é‡è¡¨æ ¼æ¨ç†èƒ½åŠ›ï¼ŒåŒ…æ‹¬ä¸‰ç§ä¸åŒçš„æ¨ç†æ¨¡å¼ã€‚æœ¬æ–‡å¯¹20å¤šä¸ªæœ€æ–°LLMsè¿›è¡Œäº†åŸºå‡†æµ‹è¯•ï¼Œè¯æ˜äº†å…¶æœ‰æ•ˆæ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œç°æœ‰LLMsåœ¨å¤„ç†å¤æ‚å’Œç°å®ä¸–ç•Œçš„è¡¨æ ¼ç›¸å…³ä»»åŠ¡æ–¹é¢ä»æœ‰å¾ˆå¤§æå‡ç©ºé—´ã€‚æ•°æ®é›†å’Œè¯„ä¼°æ¡†æ¶å‡å…¬å¼€å¯ç”¨ï¼Œæ•°æ®é›†æ‰˜ç®¡åœ¨HuggingFaceä¸Šï¼Œæ¡†æ¶æ‰˜ç®¡åœ¨GitHubä¸Šã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ•°æ®åœ¨ä¼ä¸šå’Œè¡Œä¸šä¸­ä¸»è¦ä»¥è¡¨æ ¼å½¢å¼å­˜å‚¨ï¼Œå¦‚æ•°æ®åº“å’Œæ•°æ®ä»“åº“ã€‚</li>
<li>è¡¨æ ¼ç»“æ„çš„éšè—è¯­ä¹‰ã€å†…åœ¨å¤æ‚æ€§å’Œç»“æ„åŒ–ç‰¹æ€§ä½¿å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å¤„ç†è¡¨æ ¼æ•°æ®æ—¶é¢ä¸´æŒ‘æˆ˜ã€‚</li>
<li>ç°æœ‰çš„LLMsåœ¨è¡¨æ ¼ç†è§£æ–¹é¢ä»å­˜åœ¨æ˜¾è‘—ä¸è¶³ï¼Œå°¤å…¶æ˜¯åœ¨å¤„ç†å¤æ‚å’Œç°å®ä¸–ç•Œçš„è¡¨æ ¼ä»»åŠ¡æ—¶ã€‚</li>
<li>æœ¬æ–‡æå‡ºäº†ä¸€ç§å…¨é¢çš„è¡¨æ ¼æ¨ç†è¯„ä¼°åŸºå‡†ï¼ˆTReBï¼‰ï¼ŒåŒ…å«26ä¸ªå­ä»»åŠ¡ï¼Œæ—¨åœ¨è¡¡é‡LLMsçš„è¡¨æ ¼ç†è§£èƒ½åŠ›å’Œæ¨ç†èƒ½åŠ›ã€‚</li>
<li>TReBåŒ…å«ä¸‰ç§ä¸åŒçš„æ¨ç†æ¨¡å¼ï¼Œä»¥ç¨³å¥åœ°è¡¡é‡è¡¨æ ¼æ¨ç†èƒ½åŠ›ã€‚</li>
<li>æœ¬æ–‡é€šè¿‡è¿­ä»£æ•°æ®å¤„ç†ç¨‹åºæ„å»ºäº†ä¸€ä¸ªé«˜è´¨é‡çš„æ•°æ®é›†ï¼Œå¹¶å…¬å¼€æä¾›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.18421">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-78ee90de730675b35c7f74262906a3f1.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-5210ff3b7740d0f113408dbfa61178be.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="LOGICPO-Efficient-Translation-of-NL-based-Logical-Problems-to-FOL-using-LLMs-and-Preference-Optimization"><a href="#LOGICPO-Efficient-Translation-of-NL-based-Logical-Problems-to-FOL-using-LLMs-and-Preference-Optimization" class="headerlink" title="LOGICPO: Efficient Translation of NL-based Logical Problems to FOL using   LLMs and Preference Optimization"></a>LOGICPO: Efficient Translation of NL-based Logical Problems to FOL using   LLMs and Preference Optimization</h2><p><strong>Authors:Koushik Viswanadha, Deepanway Ghosal, Somak Aditya</strong></p>
<p>Logical reasoning is a key task for artificial intelligence due to itâ€™s role in major downstream tasks such as Question Answering, Summarization. Recent methods in improving the reasoning ability of LLMs fall short in correctly converting a natural language reasoning problem to an equivalent logical formulation, which hinders the frameworkâ€™s overall ability to reason. Towards this, we propose to use finetuning on a preference optimization dataset to learn to parse and represent a natural language problem as a whole to a consistent logical program by 1) introducing a new supervised and preference optimization dataset LogicPO, and 2) adopting popular techniques such as Direct Preference Optimization (DPO), Kahneman-Tversky optimization (KTO) to finetune open-source LLMs. Our best model with Phi-3.5 consistently outperforms GPT-3.5-turboâ€™s (8-shot) by producing 10% more logically correct and with 14% less syntax errors. Through the framework and our improved evaluation metrics, we offer a promising direction in improving the logical reasoning of LLMs by better representing them in their logical formulations. </p>
<blockquote>
<p>é€»è¾‘æ¨ç†æ˜¯äººå·¥æ™ºèƒ½çš„ä¸€é¡¹å…³é”®ä»»åŠ¡ï¼Œå› ä¸ºå®ƒåœ¨é—®ç­”ã€æ‘˜è¦ç­‰ä¸‹æ¸¸ä¸»è¦ä»»åŠ¡ä¸­å‘æŒ¥ç€é‡è¦ä½œç”¨ã€‚è¿‘å¹´æ¥ï¼Œåœ¨æå‡å¤§å‹è¯­è¨€æ¨¡å‹çš„æ¨ç†èƒ½åŠ›æ–¹é¢ï¼Œç°æœ‰æ–¹æ³•éš¾ä»¥å°†è‡ªç„¶è¯­è¨€æ¨ç†é—®é¢˜æ­£ç¡®è½¬æ¢ä¸ºç­‰æ•ˆçš„é€»è¾‘å½¢å¼ï¼Œè¿™é˜»ç¢äº†æ¡†æ¶æ•´ä½“çš„æ¨ç†èƒ½åŠ›ã€‚é’ˆå¯¹è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºé€šè¿‡åœ¨ä¸€ä¸ªåå¥½ä¼˜åŒ–æ•°æ®é›†ä¸Šè¿›è¡Œå¾®è°ƒæ¥å­¦ä¹ å°†æ•´ä¸ªè‡ªç„¶è¯­è¨€é—®é¢˜è§£æå¹¶è¡¨ç¤ºä¸ºä¸€è‡´çš„é€»è¾‘ç¨‹åºã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬1ï¼‰å¼•å…¥äº†ä¸€ä¸ªæ–°çš„æœ‰ç›‘ç£å­¦ä¹ å’Œåå¥½ä¼˜åŒ–æ•°æ®é›†LogicPOï¼Œä»¥åŠ2ï¼‰é‡‡ç”¨æµè¡Œçš„æŠ€æœ¯ï¼Œå¦‚ç›´æ¥åå¥½ä¼˜åŒ–ï¼ˆDPOï¼‰ã€å¡å†…æ›¼-ç‰¹ç»´å°”æ–¯åŸºä¼˜åŒ–ï¼ˆKTOï¼‰æ¥å¾®è°ƒå¼€æºçš„å¤§å‹è¯­è¨€æ¨¡å‹ã€‚æˆ‘ä»¬çš„æœ€ä½³æ¨¡å‹Phi-3.5åœ¨é€»è¾‘ä¸ŠæŒç»­è¶…è¶Šäº†GPT-3.5 turboï¼ˆ8æ¬¡æ‹æ‘„ï¼‰ï¼Œäº§ç”Ÿæ›´é€»è¾‘æ­£ç¡®çš„ç»“æœè¾¾10%ï¼Œè¯­æ³•é”™è¯¯å‡å°‘äº†14%ã€‚é€šè¿‡æˆ‘ä»¬çš„æ¡†æ¶å’Œæ”¹è¿›çš„è¯„ä»·æŒ‡æ ‡ï¼Œæˆ‘ä»¬åœ¨é€šè¿‡é€»è¾‘è¡¨è¿°æ›´å¥½åœ°è¡¨ç¤ºå¤§å‹è¯­è¨€æ¨¡å‹çš„é€»è¾‘æ–¹é¢ï¼Œæä¾›äº†ä¸€ä¸ªæœ‰å‰é€”çš„æ–¹å‘ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.18383v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>è¿‘æœŸå¯¹äºæå‡å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„æ¨ç†èƒ½åŠ›çš„æ–¹æ³•åœ¨å¤„ç†è‡ªç„¶è¯­è¨€æ¨ç†é—®é¢˜æ—¶ï¼Œéš¾ä»¥æ­£ç¡®è½¬æ¢ä¸ºé€»è¾‘å½¢å¼ã€‚ä¸ºæ­¤ï¼Œè¯¥ç ”ç©¶é€šè¿‡å¾®è°ƒåå¥½ä¼˜åŒ–æ•°æ®é›†çš„æ–¹å¼å­¦ä¹ è§£æå’Œè¡¨ç¤ºè‡ªç„¶è¯­è¨€é—®é¢˜ä¸ºä¸€ä¸ªè¿è´¯çš„é€»è¾‘ç¨‹åºã€‚ç ”ç©¶å¼•å…¥äº†æ–°çš„ç›‘ç£å­¦ä¹ å’Œåå¥½ä¼˜åŒ–æ•°æ®é›†LogicPOï¼Œå¹¶é‡‡ç”¨Direct Preference Optimization (DPO)å’ŒKahneman-Tverskyä¼˜åŒ–ï¼ˆKTOï¼‰ç­‰æŠ€æœ¯å¯¹å¼€æºLLMsè¿›è¡Œå¾®è°ƒã€‚æœ€ä½³æ¨¡å‹Phi-3.5åœ¨é€»è¾‘æ­£ç¡®æ€§ä¸Šè¾ƒGPT-3.5-turboé«˜å‡º10%ï¼Œè¯­æ³•é”™è¯¯å‡å°‘äº†14%ã€‚è¯¥ç ”ç©¶ä¸ºæ”¹è¿›LLMsçš„é€»è¾‘æ¨ç†èƒ½åŠ›æä¾›äº†æ–¹å‘ï¼Œå³é€šè¿‡æ›´å¥½åœ°åœ¨é€»è¾‘å½¢å¼ä¸­è¡¨ç¤ºå®ƒä»¬ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è‡ªç„¶è¯­è¨€æ¨ç†æ˜¯äººå·¥æ™ºèƒ½çš„æ ¸å¿ƒä»»åŠ¡ä¹‹ä¸€ï¼Œå¯¹äºä¸‹æ¸¸ä»»åŠ¡å¦‚é—®ç­”ã€æ‘˜è¦ç­‰å…·æœ‰é‡è¦æ„ä¹‰ã€‚</li>
<li>ç°æœ‰æå‡LLMsæ¨ç†èƒ½åŠ›çš„æ–¹æ³•åœ¨å°†è‡ªç„¶è¯­è¨€é—®é¢˜è½¬æ¢ä¸ºé€»è¾‘å½¢å¼æ—¶å­˜åœ¨ç¼ºé™·ã€‚</li>
<li>ç ”ç©¶é€šè¿‡å¾®è°ƒåå¥½ä¼˜åŒ–æ•°æ®é›†çš„æ–¹å¼ï¼Œå­¦ä¹ å°†è‡ªç„¶è¯­è¨€é—®é¢˜è§£æå’Œè¡¨ç¤ºä¸ºä¸€ä¸ªè¿è´¯çš„é€»è¾‘ç¨‹åºã€‚</li>
<li>å¼•å…¥äº†æ–°çš„ç›‘ç£å­¦ä¹ å’Œåå¥½ä¼˜åŒ–æ•°æ®é›†LogicPOã€‚</li>
<li>é‡‡ç”¨DPOå’ŒKTOç­‰æŠ€æœ¯å¯¹å¼€æºLLMsè¿›è¡Œå¾®è°ƒï¼Œä»¥æé«˜å…¶é€»è¾‘æ¨ç†èƒ½åŠ›ã€‚</li>
<li>æœ€ä½³æ¨¡å‹Phi-3.5åœ¨é€»è¾‘æ­£ç¡®æ€§å’Œè¯­æ³•é”™è¯¯æ–¹é¢è¾ƒGPT-3.5-turboæœ‰æ˜¾è‘—æ”¹å–„ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.18383">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-79e10cdace975e674f85361acb2a849e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-9729469b5496465b05bc5e7e31cb2643.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-8c625485e2da27fcd99065ded133106b.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="RePIC-Reinforced-Post-Training-for-Personalizing-Multi-Modal-Language-Models"><a href="#RePIC-Reinforced-Post-Training-for-Personalizing-Multi-Modal-Language-Models" class="headerlink" title="RePIC: Reinforced Post-Training for Personalizing Multi-Modal Language   Models"></a>RePIC: Reinforced Post-Training for Personalizing Multi-Modal Language   Models</h2><p><strong>Authors:Yeongtak Oh, Jisoo Mok, Dohyun Chung, Juhyeon Shin, Sangha Park, Johan Barthelemy, Sungroh Yoon</strong></p>
<p>Recent multi-modal large language models (MLLMs) often struggle to generate personalized image captions, even when trained on high-quality captions. In this work, we observe that such limitations persist in existing post-training-based MLLM personalization methods. Specifically, despite being post-tuned with large-scale caption data through supervised fine-tuning (SFT), these models frequently fail to produce faithful descriptions in real-world scenarios, such as multi-concept image captioning. However, acquiring large-scale, high-quality captions for such complex settings is both costly and difficult. To address the data-centric nature of SFT, we propose a reinforcement learning (RL)-based post-training framework. To the best of our knowledge, this is the first RL-based approach to post-train MLLMs for personalized image captioning. Our method significantly enhances both visual recognition and personalized generation capabilities of MLLMs, and consistently outperforms existing SFT-based baselines, especially in the challenging multi-concept image captioning task. </p>
<blockquote>
<p>æœ€è¿‘çš„å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰å³ä½¿åœ¨é«˜è´¨é‡çš„å›¾ç‰‡æè¿°ä¸Šè®­ç»ƒï¼Œä¹Ÿå¾€å¾€éš¾ä»¥ç”Ÿæˆä¸ªæ€§åŒ–çš„å›¾åƒæ ‡é¢˜ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬è§‚å¯Ÿåˆ°ç°æœ‰çš„åŸºäºåè®­ç»ƒçš„MLLMä¸ªæ€§åŒ–æ–¹æ³•å­˜åœ¨è¿™æ ·çš„å±€é™æ€§ã€‚å…·ä½“æ¥è¯´ï¼Œå°½ç®¡é€šè¿‡æœ‰ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰åœ¨å¤§è§„æ¨¡å›¾ç‰‡æè¿°æ•°æ®ä¸Šè¿›è¡Œåè°ƒæ•´ï¼Œè¿™äº›æ¨¡å‹åœ¨çœŸå®åœºæ™¯ï¼ˆå¦‚å¤šæ¦‚å¿µå›¾åƒæ ‡é¢˜ç”Ÿæˆï¼‰ä¸­ä»ç„¶æ— æ³•äº§ç”Ÿå¿ å®çš„æè¿°ã€‚ç„¶è€Œï¼Œä¸ºè¿™ç§å¤æ‚ç¯å¢ƒè·å–å¤§è§„æ¨¡çš„é«˜è´¨é‡å›¾ç‰‡æè¿°æ—¢æ˜‚è´µåˆå›°éš¾ã€‚ä¸ºäº†è§£å†³SFTä»¥æ•°æ®ä¸ºä¸­å¿ƒçš„ç‰¹æ€§ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰çš„åè®­ç»ƒæ¡†æ¶ã€‚æ®æˆ‘ä»¬æ‰€çŸ¥ï¼Œè¿™æ˜¯ç¬¬ä¸€ä¸ªç”¨äºä¸ªæ€§åŒ–å›¾åƒæ ‡é¢˜ç”Ÿæˆçš„åè®­ç»ƒMLLMçš„RLæ–¹æ³•ã€‚æˆ‘ä»¬çš„æ–¹æ³•æ˜¾è‘—æé«˜äº†MLLMçš„è§†è§‰è¯†åˆ«èƒ½åŠ›å’Œä¸ªæ€§åŒ–ç”Ÿæˆèƒ½åŠ›ï¼Œå¹¶ä¸”åœ¨å…·æœ‰æŒ‘æˆ˜æ€§çš„å¤šæ¦‚å¿µå›¾åƒæ ‡é¢˜ä»»åŠ¡ä¸Šï¼Œè¡¨ç°å‡ä¼˜äºç°æœ‰çš„åŸºäºSFTçš„åŸºçº¿ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.18369v1">PDF</a> Project Page: <a target="_blank" rel="noopener" href="https://github.com/oyt9306/RePIC">https://github.com/oyt9306/RePIC</a></p>
<p><strong>Summary</strong></p>
<p>å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨ç”Ÿæˆä¸ªæ€§åŒ–å›¾åƒæè¿°æ—¶å­˜åœ¨å±€é™ï¼Œå³ä½¿ç»è¿‡é«˜è´¨é‡æè¿°è®­ç»ƒï¼Œä»éš¾ä»¥åœ¨çœŸå®åœºæ™¯ä¸­ç”Ÿæˆå¿ å®æè¿°ã€‚æœ¬æ–‡è§‚å¯Ÿåˆ°ç°æœ‰åŸºäºåè®­ç»ƒçš„MLLMä¸ªæ€§åŒ–æ–¹æ³•å­˜åœ¨æ­¤ç±»é™åˆ¶ã€‚å°½ç®¡é€šè¿‡ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰è¿›è¡Œå¤§è§„æ¨¡å›¾åƒæè¿°æ•°æ®åè°ƒï¼Œä½†è¿™äº›æ¨¡å‹åœ¨å¤šæ¦‚å¿µå›¾åƒæè¿°ç­‰å¤æ‚åœºæ™¯ä¸­ä»æ— æ³•äº§ç”Ÿå¿ å®æè¿°ã€‚ä¸ºè§£å†³SFTçš„æ•°æ®ä¸­å¿ƒæ€§è´¨ï¼Œæœ¬æ–‡æå‡ºä¸€ç§åŸºäºå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰çš„åè®­ç»ƒæ¡†æ¶ã€‚æ®æˆ‘ä»¬æ‰€çŸ¥ï¼Œè¿™æ˜¯é¦–æ¬¡ä½¿ç”¨RLå¯¹MLLMè¿›è¡Œä¸ªæ€§åŒ–å›¾åƒæè¿°çš„åè®­ç»ƒã€‚è¯¥æ–¹æ³•æ˜¾è‘—æé«˜MLLMçš„è§†è§‰è¯†åˆ«å’Œä¸ªæ€§åŒ–ç”Ÿæˆèƒ½åŠ›ï¼Œå°¤å…¶åœ¨å¤šæ¦‚å¿µå›¾åƒæè¿°ç­‰æŒ‘æˆ˜æ€§ä»»åŠ¡ä¸­ï¼Œä¼˜äºç°æœ‰çš„åŸºäºSFTçš„åŸºçº¿æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨ç”Ÿæˆä¸ªæ€§åŒ–å›¾åƒæè¿°æ—¶å­˜åœ¨æŒ‘æˆ˜ã€‚</li>
<li>å³ä½¿ç»è¿‡é«˜è´¨é‡æè¿°è®­ç»ƒï¼Œç°æœ‰åŸºäºåè®­ç»ƒçš„MLLMä¸ªæ€§åŒ–æ–¹æ³•ä»é¢ä¸´é™åˆ¶ã€‚</li>
<li>ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰éš¾ä»¥å¤„ç†å¤æ‚åœºæ™¯ä¸­çš„å¤šæ¦‚å¿µå›¾åƒæè¿°ã€‚</li>
<li>å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰è¢«é¦–æ¬¡ç”¨äºMLLMçš„åè®­ç»ƒï¼Œä»¥æ”¹å–„ä¸ªæ€§åŒ–å›¾åƒæè¿°ç”Ÿæˆã€‚</li>
<li>æå‡ºä¸€ç§åŸºäºRLçš„åè®­ç»ƒæ¡†æ¶ï¼Œæ˜¾è‘—æé«˜MLLMçš„è§†è§‰è¯†åˆ«å’Œä¸ªæ€§åŒ–ç”Ÿæˆèƒ½åŠ›ã€‚</li>
<li>è¯¥æ–¹æ³•åœ¨æŒ‘æˆ˜æ€§ä»»åŠ¡å¦‚å¤šæ¦‚å¿µå›¾åƒæè¿°ä¸­ï¼Œè¡¨ç°ä¼˜äºåŸºäºSFTçš„åŸºçº¿æ–¹æ³•ã€‚</li>
<li>æ•°æ®æ”¶é›†å’Œå‡†å¤‡æ˜¯æ”¹å–„æ¨¡å‹æ€§èƒ½çš„å…³é”®ï¼Œå°¤å…¶æ˜¯åœ¨å¤æ‚åœºæ™¯ä¸­ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.18369">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-5471f03898635eeb293eb5d67bc875ee.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f194d31b940cc2323d86ee9fc836807c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-782c893141cb94a016c2d944c0be5854.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7180929f477707d60608b550883f53ab.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b0b73aea860171ef8c9a4e7c0dd20573.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="Confucius3-Math-A-Lightweight-High-Performance-Reasoning-LLM-for-Chinese-K-12-Mathematics-Learning"><a href="#Confucius3-Math-A-Lightweight-High-Performance-Reasoning-LLM-for-Chinese-K-12-Mathematics-Learning" class="headerlink" title="Confucius3-Math: A Lightweight High-Performance Reasoning LLM for   Chinese K-12 Mathematics Learning"></a>Confucius3-Math: A Lightweight High-Performance Reasoning LLM for   Chinese K-12 Mathematics Learning</h2><p><strong>Authors:Lixin Wu, Na Cai, Qiao Cheng, Jiachen Wang, Yitao Duan</strong></p>
<p>We introduce Confucius3-Math, an open-source large language model with 14B parameters that (1) runs efficiently on a single consumer-grade GPU; (2) achieves SOTA performances on a range of mathematical reasoning tasks, outperforming many models with significantly larger sizes. In particular, as part of our mission to enhancing education and knowledge dissemination with AI, Confucius3-Math is specifically committed to mathematics learning for Chinese K-12 students and educators. Built via post-training with large-scale reinforcement learning (RL), Confucius3-Math aligns with national curriculum and excels at solving main-stream Chinese K-12 mathematical problems with low cost. In this report we share our development recipe, the challenges we encounter and the techniques we develop to overcome them. In particular, we introduce three technical innovations: Targeted Entropy Regularization, Recent Sample Recovery and Policy-Specific Hardness Weighting. These innovations encompass a new entropy regularization, a novel data scheduling policy, and an improved group-relative advantage estimator. Collectively, they significantly stabilize the RL training, improve data efficiency, and boost performance. Our work demonstrates the feasibility of building strong reasoning models in a particular domain at low cost. We open-source our model and code at <a target="_blank" rel="noopener" href="https://github.com/netease-youdao/Confucius3-Math">https://github.com/netease-youdao/Confucius3-Math</a>. </p>
<blockquote>
<p>æˆ‘ä»¬æ¨å‡ºConfucius3-Mathï¼Œè¿™æ˜¯ä¸€æ¬¾æ‹¥æœ‰14äº¿å‚æ•°çš„å¼€æºå¤§å‹è¯­è¨€æ¨¡å‹ï¼Œå®ƒå…·å¤‡ä»¥ä¸‹ç‰¹ç‚¹ï¼šï¼ˆ1ï¼‰åœ¨å•ä¸ªæ¶ˆè´¹çº§GPUä¸Šè¿è¡Œé«˜æ•ˆï¼›ï¼ˆ2ï¼‰åœ¨å„ç§æ•°å­¦æ¨ç†ä»»åŠ¡ä¸Šè¾¾åˆ°äº†æœ€æ–°æŠ€æœ¯æ°´å¹³ï¼Œè¶…è¶Šäº†è®¸å¤šè§„æ¨¡æ›´å¤§çš„æ¨¡å‹ã€‚ç‰¹åˆ«æ˜¯ï¼Œä½œä¸ºæˆ‘ä»¬ç”¨AIåŠ å¼ºæ•™è‚²å’ŒçŸ¥è¯†æ™®åŠçš„ä½¿å‘½çš„ä¸€éƒ¨åˆ†ï¼ŒConfucius3-Mathä¸“æ³¨äºä¸ºä¸­å›½çš„K-12å­¦ç”Ÿå’Œæ•™è‚²å·¥ä½œè€…æä¾›æ•°å­¦å­¦ä¹ å¸®åŠ©ã€‚é€šè¿‡è®­ç»ƒåçš„å¤§è§„æ¨¡å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰æ„å»ºï¼ŒConfucius3-Mathä¸å›½å®¶è¯¾ç¨‹ç›¸å¥‘åˆï¼Œæ“…é•¿è§£å†³ä¸»æµçš„ä¸­å›½K-12æ•°å­¦é—®é¢˜ä¸”æˆæœ¬ä½å»‰ã€‚åœ¨è¿™ä»½æŠ¥å‘Šä¸­ï¼Œæˆ‘ä»¬åˆ†äº«äº†æˆ‘ä»¬çš„å¼€å‘é…æ–¹ã€é‡åˆ°çš„æŒ‘æˆ˜ä»¥åŠä¸ºå…‹æœè¿™äº›æŒ‘æˆ˜è€Œå¼€å‘çš„æŠ€æœ¯ã€‚ç‰¹åˆ«åœ°ï¼Œæˆ‘ä»¬ä»‹ç»äº†ä¸‰é¡¹æŠ€æœ¯åˆ›æ–°ï¼šç›®æ ‡ç†µæ­£åˆ™åŒ–ã€æœ€æ–°æ ·æœ¬æ¢å¤å’Œç­–ç•¥ç‰¹å®šç¡¬åº¦åŠ æƒã€‚è¿™äº›åˆ›æ–°åŒ…æ‹¬ä¸€ç§æ–°çš„ç†µæ­£åˆ™åŒ–ã€ä¸€ç§æ–°çš„æ•°æ®è°ƒåº¦ç­–ç•¥ä»¥åŠä¸€ä¸ªæ”¹è¿›çš„å°ç»„ç›¸å¯¹ä¼˜åŠ¿ä¼°è®¡å™¨ã€‚æ€»ä½“ä¸Šï¼Œå®ƒä»¬æ˜¾è‘—ç¨³å®šäº†RLè®­ç»ƒï¼Œæé«˜äº†æ•°æ®æ•ˆç‡å¹¶æå‡äº†æ€§èƒ½ã€‚æˆ‘ä»¬çš„å·¥ä½œè¯æ˜äº†åœ¨ç‰¹å®šé¢†åŸŸæ„å»ºå¼ºå¤§çš„æ¨ç†æ¨¡å‹çš„å¯è¡Œæ€§ï¼Œå¹¶ä¸”æˆæœ¬ä½å»‰ã€‚æˆ‘ä»¬åœ¨<a target="_blank" rel="noopener" href="https://github.com/netease-youdao/Confucius3-Math">https://github.com/netease-youdao/Confucius3-Math</a>å¼€æºæˆ‘ä»¬çš„æ¨¡å‹å’Œä»£ç ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.18330v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å­”å­ä¸‰å·æ•°å­¦æ¨¡å‹æ˜¯ä¸€ä¸ªé¢å‘ä¸­æ–‡K-12å­¦ç”Ÿå’Œæ•™è‚²å·¥ä½œè€…çš„æ•°å­¦æ¨ç†å¤§å‹è¯­è¨€æ¨¡å‹ã€‚è¯¥æ¨¡å‹å…·æœ‰å¼€æºã€è¿è¡Œæ•ˆç‡é«˜ã€æ€§èƒ½å“è¶Šç­‰ç‰¹ç‚¹ï¼Œèƒ½å¤Ÿåœ¨å•ä¸ªæ¶ˆè´¹çº§GPUä¸Šè¿è¡Œï¼Œå¹¶åœ¨ä¸€ç³»åˆ—æ•°å­¦æ¨ç†ä»»åŠ¡ä¸Šå–å¾—äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚é€šè¿‡å¤§è§„æ¨¡å¼ºåŒ–å­¦ä¹ è¿›è¡Œè®­ç»ƒï¼Œè¯¥æ¨¡å‹ç¬¦åˆå›½å®¶è¯¾ç¨‹æ ‡å‡†ï¼Œæ“…é•¿è§£å†³ä¸»æµçš„æ•°å­¦é—®é¢˜ï¼Œæˆæœ¬ä½å»‰ã€‚æœ¬æ–‡ä»‹ç»äº†æ¨¡å‹çš„å¼€å‘è¿‡ç¨‹ã€é¢ä¸´çš„æŒ‘æˆ˜ä»¥åŠä¸ºå…‹æœæŒ‘æˆ˜è€Œå¼€å‘çš„æŠ€æœ¯ã€‚è¯¥æ¨¡å‹çš„åˆ›æ–°æŠ€æœ¯åŒ…æ‹¬ç›®æ ‡ç†µæ­£åˆ™åŒ–ã€æœ€æ–°æ ·æœ¬æ¢å¤å’Œç­–ç•¥ç‰¹å®šç¡¬åº¦åŠ æƒç­‰ã€‚è¿™äº›åˆ›æ–°æŠ€æœ¯æ˜¾è‘—ç¨³å®šå¼ºåŒ–å­¦ä¹ è®­ç»ƒï¼Œæé«˜æ•°æ®æ•ˆç‡å¹¶æå‡æ€§èƒ½ã€‚æˆ‘ä»¬å…¬å¼€äº†æ¨¡å‹å’Œä»£ç ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å­”å­ä¸‰å·æ•°å­¦æ˜¯ä¸€ä¸ªé¢å‘ä¸­æ–‡K-12å­¦ç”Ÿå’Œæ•™è‚²å·¥ä½œè€…çš„å¼€æºå¤§å‹è¯­è¨€æ¨¡å‹ã€‚</li>
<li>è¯¥æ¨¡å‹èƒ½åœ¨å•ä¸ªæ¶ˆè´¹çº§GPUä¸Šé«˜æ•ˆè¿è¡Œï¼Œå…·æœ‰å“è¶Šæ€§èƒ½ã€‚</li>
<li>é€šè¿‡å¤§è§„æ¨¡å¼ºåŒ–å­¦ä¹ è®­ç»ƒï¼Œå­”å­ä¸‰å·æ•°å­¦ç¬¦åˆå›½å®¶è¯¾ç¨‹æ ‡å‡†ï¼Œæ“…é•¿è§£å†³ä¸»æµæ•°å­¦é—®é¢˜ã€‚</li>
<li>æ¨¡å‹å®ç°ä¸‰å¤§æŠ€æœ¯åˆ›æ–°ï¼šç›®æ ‡ç†µæ­£åˆ™åŒ–ã€æœ€æ–°æ ·æœ¬æ¢å¤å’Œç­–ç•¥ç‰¹å®šç¡¬åº¦åŠ æƒã€‚</li>
<li>è¿™äº›åˆ›æ–°æŠ€æœ¯æ˜¾è‘—ç¨³å®šå¼ºåŒ–å­¦ä¹ è®­ç»ƒè¿‡ç¨‹ï¼Œæé«˜æ•°æ®æ•ˆç‡å¹¶æå‡æ¨¡å‹æ€§èƒ½ã€‚</li>
<li>è¯¥æ¨¡å‹å…¬å¼€æºä»£ç ä¾›å…¬ä¼—ä½¿ç”¨å’Œå­¦ä¹ ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.18330">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-c26914852901ce1d0ff238a03316ab50.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-06c51b587016c21202cde20c202d6649.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7a6d0d2362c8b4b0bbc3e60364a7d079.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="RLPR-Extrapolating-RLVR-to-General-Domains-without-Verifiers"><a href="#RLPR-Extrapolating-RLVR-to-General-Domains-without-Verifiers" class="headerlink" title="RLPR: Extrapolating RLVR to General Domains without Verifiers"></a>RLPR: Extrapolating RLVR to General Domains without Verifiers</h2><p><strong>Authors:Tianyu Yu, Bo Ji, Shouli Wang, Shu Yao, Zefan Wang, Ganqu Cui, Lifan Yuan, Ning Ding, Yuan Yao, Zhiyuan Liu, Maosong Sun, Tat-Seng Chua</strong></p>
<p>Reinforcement Learning with Verifiable Rewards (RLVR) demonstrates promising potential in advancing the reasoning capabilities of LLMs. However, its success remains largely confined to mathematical and code domains. This primary limitation stems from the heavy reliance on domain-specific verifiers, which results in prohibitive complexity and limited scalability. To address the challenge, our key observation is that LLMâ€™s intrinsic probability of generating a correct free-form answer directly indicates its own evaluation of the reasoning reward (i.e., how well the reasoning process leads to the correct answer). Building on this insight, we propose RLPR, a simple verifier-free framework that extrapolates RLVR to broader general domains. RLPR uses the LLMâ€™s own token probability scores for reference answers as the reward signal and maximizes the expected reward during training. We find that addressing the high variance of this noisy probability reward is crucial to make it work, and propose prob-to-reward and stabilizing methods to ensure a precise and stable reward from LLM intrinsic probabilities. Comprehensive experiments in four general-domain benchmarks and three mathematical benchmarks show that RLPR consistently improves reasoning capabilities in both areas for Gemma, Llama, and Qwen based models. Notably, RLPR outperforms concurrent VeriFree by 7.6 points on TheoremQA and 7.5 points on Minerva, and even surpasses strong verifier-model-dependent approaches General-Reasoner by 1.6 average points across seven benchmarks. </p>
<blockquote>
<p>å¼ºåŒ–å­¦ä¹ ä¸å¯éªŒè¯å¥–åŠ±ï¼ˆRLVRï¼‰åœ¨æå‡å¤§å‹è¯­è¨€æ¨¡å‹çš„æ¨ç†èƒ½åŠ›æ–¹é¢æ˜¾ç¤ºå‡ºå·¨å¤§çš„æ½œåŠ›ã€‚ç„¶è€Œï¼Œå…¶æˆåŠŸä¸»è¦å±€é™äºæ•°å­¦å’Œä»£ç é¢†åŸŸã€‚è¿™ç§ä¸»è¦å±€é™æºäºå¯¹ç‰¹å®šé¢†åŸŸéªŒè¯å™¨çš„ä¸¥é‡ä¾èµ–ï¼Œè¿™å¯¼è‡´äº†ç¦æ­¢çš„å¤æ‚æ€§å’Œæœ‰é™çš„æ‰©å±•æ€§ã€‚ä¸ºäº†åº”å¯¹è¿™ä¸€æŒ‘æˆ˜ï¼Œæˆ‘ä»¬çš„ä¸»è¦è§‚å¯Ÿç»“æœæ˜¯ï¼Œå¤§å‹è¯­è¨€æ¨¡å‹ç”Ÿæˆæ­£ç¡®è‡ªç”±å½¢å¼ç­”æ¡ˆçš„å†…åœ¨æ¦‚ç‡ç›´æ¥æŒ‡ç¤ºå…¶å¯¹æ¨ç†å¥–åŠ±çš„è¯„ä¼°ï¼ˆå³æ¨ç†è¿‡ç¨‹å¦‚ä½•å¯¼è‡´æ­£ç¡®ç­”æ¡ˆï¼‰ã€‚åŸºäºè¿™ä¸€è§è§£ï¼Œæˆ‘ä»¬æå‡ºäº†æ— éªŒè¯å™¨å‚ä¸çš„RLPRæ¡†æ¶ï¼Œè¯¥æ¡†æ¶å°†RLVRæ¨å¹¿åˆ°æ›´å¹¿æ³›çš„é€šç”¨é¢†åŸŸã€‚RLPRä½¿ç”¨å¤§å‹è¯­è¨€æ¨¡å‹å¯¹å‚è€ƒç­”æ¡ˆçš„è‡ªèº«ä»¤ç‰Œæ¦‚ç‡åˆ†æ•°ä½œä¸ºå¥–åŠ±ä¿¡å·ï¼Œå¹¶åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­æœ€å¤§åŒ–é¢„æœŸå¥–åŠ±ã€‚æˆ‘ä»¬å‘ç°è§£å†³è¿™ç§å˜ˆæ‚æ¦‚ç‡å¥–åŠ±çš„é«˜æ–¹å·®æ˜¯ä½¿å…¶å‘æŒ¥ä½œç”¨çš„å…³é”®ï¼Œå¹¶æå‡ºæ¦‚ç‡åˆ°å¥–åŠ±å’Œç¨³å®šæ–¹æ³•ï¼Œä»¥ç¡®ä¿ä»å¤§å‹è¯­è¨€æ¨¡å‹å†…åœ¨æ¦‚ç‡ä¸­è·å¾—ç²¾ç¡®ç¨³å®šçš„å¥–åŠ±ã€‚åœ¨å››ä¸ªé€šç”¨é¢†åŸŸåŸºå‡†æµ‹è¯•å’Œä¸‰ä¸ªæ•°å­¦åŸºå‡†æµ‹è¯•çš„ç»¼åˆå®éªŒè¡¨æ˜ï¼ŒRLPRåœ¨è¿™ä¸¤ä¸ªé¢†åŸŸçš„æ¨ç†èƒ½åŠ›å‡æœ‰æ‰€æé«˜ï¼Œé€‚ç”¨äºGemmaã€Llamaå’ŒQwenæ¨¡å‹ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼ŒRLPRåœ¨å®šç†é—®ç­”ï¼ˆTheoremQAï¼‰å’ŒMinervaä¸Šçš„å¾—åˆ†åˆ†åˆ«æ¯”åŒæœŸéªŒè¯å™¨VeriFreeé«˜å‡º7.6åˆ†å’Œ7.5åˆ†ï¼Œç”šè‡³åœ¨ä¸ƒä¸ªåŸºå‡†æµ‹è¯•ä¸­å¹³å‡é«˜å‡ºéªŒè¯å™¨ä¾èµ–æ–¹æ³•General-Reasoner 1.6åˆ†ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.18254v1">PDF</a> Project Website: <a target="_blank" rel="noopener" href="https://github.com/openbmb/RLPR">https://github.com/openbmb/RLPR</a></p>
<p><strong>Summary</strong></p>
<p>å¼ºåŒ–å­¦ä¹ ä¸å¯éªŒè¯å¥–åŠ±ï¼ˆRLVRï¼‰åœ¨æå‡å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ¨ç†èƒ½åŠ›æ–¹é¢å±•ç°å‡ºå·¨å¤§æ½œåŠ›ï¼Œä½†å…¶åº”ç”¨ä¸»è¦å±€é™äºæ•°å­¦å’Œä»£ç é¢†åŸŸã€‚è¿™ä¸»è¦æ˜¯å› ä¸ºå…¶è¿‡äºä¾èµ–ç‰¹å®šé¢†åŸŸçš„éªŒè¯å™¨ï¼Œå¯¼è‡´å¤æ‚æ€§å¢åŠ å’Œå¯æ‰©å±•æ€§æœ‰é™ã€‚åŸºäºLLMç”Ÿæˆæ­£ç¡®è‡ªç”±å½¢å¼ç­”æ¡ˆçš„å›ºæœ‰æ¦‚ç‡å¯ä»¥ç›´æ¥åæ˜ å…¶å¯¹æ¨ç†å¥–åŠ±çš„è¯„ä¼°è¿™ä¸€è§‚å¯Ÿï¼Œæå‡ºäº†æ— éœ€éªŒè¯å™¨çš„RLPRæ¡†æ¶ï¼Œå°†RLVRæ¨å¹¿è‡³æ›´å¹¿æ³›çš„é€šç”¨é¢†åŸŸã€‚RLPRåˆ©ç”¨LLMå¯¹å‚è€ƒç­”æ¡ˆçš„è‡ªèº«ä»¤ç‰Œæ¦‚ç‡åˆ†æ•°ä½œä¸ºå¥–åŠ±ä¿¡å·ï¼Œå¹¶åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­æœ€å¤§åŒ–é¢„æœŸå¥–åŠ±ã€‚ç ”ç©¶å‘ç°ï¼Œè§£å†³è¿™ç§æ¦‚ç‡å¥–åŠ±çš„é«˜æ–¹å·®è‡³å…³é‡è¦ï¼Œå¹¶æå‡ºäº†prob-to-rewardå’Œç¨³å®šæ–¹æ³•ï¼Œä»¥ç¡®ä¿ä»LLMå†…åœ¨æ¦‚ç‡ä¸­è·å¾—ç²¾ç¡®ç¨³å®šçš„å¥–åŠ±ã€‚å®éªŒè¡¨æ˜ï¼ŒRLPRåœ¨é€šç”¨é¢†åŸŸå’Œæ•°å­¦é¢†åŸŸçš„å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­ï¼Œå‡æé«˜äº†Gemmaã€Llamaå’ŒQwenç­‰æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ã€‚ç‰¹åˆ«æ˜¯åœ¨å®šç†QAå’ŒMinervaä¸Šï¼ŒRLPRåˆ†åˆ«ä¼˜äºå¹¶å‘éªŒè¯å™¨VeriFree 7.6ç‚¹å’Œ7.5ç‚¹ï¼Œç”šè‡³åœ¨ä¸ƒä¸ªåŸºå‡†æµ‹è¯•ä¸­å¹³å‡ä¼˜äºå¼ºéªŒè¯å™¨ä¾èµ–çš„æ–¹æ³•General-Reasoner 1.6ç‚¹ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>RLVRåœ¨æå‡LLMæ¨ç†èƒ½åŠ›æ–¹é¢å±•ç°æ½œåŠ›ï¼Œä½†ä¸»è¦å±€é™äºæ•°å­¦å’Œä»£ç é¢†åŸŸã€‚</li>
<li>ä¾èµ–ç‰¹å®šé¢†åŸŸçš„éªŒè¯å™¨å¯¼è‡´å¤æ‚æ€§å’Œæ‰©å±•æ€§é™åˆ¶ã€‚</li>
<li>LLMç”Ÿæˆæ­£ç¡®ç­”æ¡ˆçš„å›ºæœ‰æ¦‚ç‡å¯ä»¥ä½œä¸ºè¯„ä¼°æ¨ç†å¥–åŠ±çš„æŒ‡ç¤ºã€‚</li>
<li>æå‡ºæ— éœ€éªŒè¯å™¨çš„RLPRæ¡†æ¶ï¼Œé€‚ç”¨äºæ›´å¹¿æ³›çš„é€šç”¨é¢†åŸŸã€‚</li>
<li>RLPRåˆ©ç”¨LLMçš„ä»¤ç‰Œæ¦‚ç‡åˆ†æ•°ä½œä¸ºå¥–åŠ±ä¿¡å·ï¼Œå¹¶åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­æœ€å¤§åŒ–é¢„æœŸå¥–åŠ±ã€‚</li>
<li>è§£å†³æ¦‚ç‡å¥–åŠ±çš„é«˜æ–¹å·®é—®é¢˜å¯¹äºRLPRçš„æˆåŠŸè‡³å…³é‡è¦ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.18254">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-e67da2d31b79c1241459106e53220942.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3a476fdaff9f8862befe4f3ce4ff89c5.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e1fc27fa4e3a3eca3289468c1f1beac7.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="Smart-LLaMA-DPO-Reinforced-Large-Language-Model-for-Explainable-Smart-Contract-Vulnerability-Detection"><a href="#Smart-LLaMA-DPO-Reinforced-Large-Language-Model-for-Explainable-Smart-Contract-Vulnerability-Detection" class="headerlink" title="Smart-LLaMA-DPO: Reinforced Large Language Model for Explainable Smart   Contract Vulnerability Detection"></a>Smart-LLaMA-DPO: Reinforced Large Language Model for Explainable Smart   Contract Vulnerability Detection</h2><p><strong>Authors:Lei Yu, Zhirong Huang, Hang Yuan, Shiqi Cheng, Li Yang, Fengjun Zhang, Chenjie Shen, Jiajia Ma, Jingyuan Zhang, Junyi Lu, Chun Zuo</strong></p>
<p>Smart contract vulnerability detection remains a major challenge in blockchain security. Existing vulnerability detection methods face two main issues: (1) Existing datasets lack comprehensive coverage and high-quality explanations for preference learning. (2) Large language models (LLMs) often struggle with accurately interpreting specific concepts in smart contract security. Empirical analysis shows that even after continual pre-training (CPT) and supervised fine-tuning (SFT), LLMs may misinterpret the execution order of state changes, resulting in incorrect explanations despite making correct detection decisions. To address these challenges, we propose Smart-LLaMA-DPO based on LLaMA-3.1-8B. We construct a comprehensive dataset covering four major vulnerability types and machine-unauditable vulnerabilities, including precise labels, explanations, and locations for SFT, as well as high-quality and low-quality output pairs for Direct Preference Optimization (DPO). Second, we perform CPT using large-scale smart contract to enhance the LLMâ€™s understanding of specific security practices in smart contracts. Futhermore, we conduct SFT with our comprehensive dataset. Finally, we apply DPO, leveraging human feedback and a specially designed loss function that increases the probability of preferred explanations while reducing the likelihood of non-preferred outputs. We evaluate Smart-LLaMA-DPO on four major vulnerability types: reentrancy, timestamp dependence, integer overflow&#x2F;underflow, and delegatecall, as well as machine-unauditable vulnerabilities. Our method significantly outperforms state-of-the-art baselines, with average improvements of 10.43% in F1 score and 7.87% in accuracy. Moreover, both LLM evaluation and human evaluation confirm that our method generates more correct, thorough, and clear explanations. </p>
<blockquote>
<p>æ™ºèƒ½åˆçº¦æ¼æ´æ£€æµ‹ä»æ˜¯åŒºå—é“¾å®‰å…¨é¢†åŸŸçš„ä¸»è¦æŒ‘æˆ˜ã€‚ç°æœ‰çš„æ¼æ´æ£€æµ‹æ–¹æ³•é¢ä¸´ä¸¤å¤§é—®é¢˜ï¼šï¼ˆ1ï¼‰ç°æœ‰æ•°æ®é›†åœ¨åå¥½å­¦ä¹ æ–¹é¢ç¼ºä¹å…¨é¢è¦†ç›–å’Œé«˜è´¨é‡è§£é‡Šã€‚ï¼ˆ2ï¼‰å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨å‡†ç¡®è§£é‡Šæ™ºèƒ½åˆçº¦å®‰å…¨ä¸­çš„ç‰¹å®šæ¦‚å¿µæ—¶ç»å¸¸é‡åˆ°å›°éš¾ã€‚å®è¯åˆ†æè¡¨æ˜ï¼Œå³ä½¿ç»è¿‡æŒç»­é¢„è®­ç»ƒï¼ˆCPTï¼‰å’Œç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰ï¼ŒLLMsä»å¯èƒ½è¯¯è§£çŠ¶æ€å˜æ›´çš„æ‰§è¡Œé¡ºåºï¼Œå¯¼è‡´å³ä½¿åšå‡ºæ­£ç¡®çš„æ£€æµ‹å†³ç­–ï¼Œè§£é‡Šä¹Ÿæ˜¯é”™è¯¯çš„ã€‚ä¸ºäº†è§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬åŸºäºLLaMA-3.1-8Bæå‡ºSmart-LLaMA-DPOã€‚æˆ‘ä»¬æ„å»ºäº†ä¸€ä¸ªç»¼åˆæ•°æ®é›†ï¼Œè¦†ç›–å››ç§ä¸»è¦æ¼æ´ç±»å‹å’Œæœºå™¨æ— æ³•å®¡æ ¸çš„æ¼æ´ï¼ŒåŒ…æ‹¬ç²¾ç¡®æ ‡ç­¾ã€è§£é‡Šã€ç”¨äºSFTçš„ä½ç½®ï¼Œä»¥åŠç”¨äºç›´æ¥åå¥½ä¼˜åŒ–ï¼ˆDPOï¼‰çš„é«˜è´¨é‡å’Œä½è´¨é‡è¾“å‡ºå¯¹ã€‚å…¶æ¬¡ï¼Œæˆ‘ä»¬ä½¿ç”¨å¤§è§„æ¨¡æ™ºèƒ½åˆçº¦è¿›è¡ŒCPTï¼Œä»¥å¢å¼ºLLMå¯¹æ™ºèƒ½åˆçº¦ä¸­ç‰¹å®šå®‰å…¨å®è·µçš„ç†è§£ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬ç”¨æˆ‘ä»¬çš„ç»¼åˆæ•°æ®é›†è¿›è¡ŒSFTã€‚æœ€åï¼Œæˆ‘ä»¬åº”ç”¨DPOï¼Œåˆ©ç”¨äººç±»åé¦ˆå’Œä¸“é—¨è®¾è®¡çš„æŸå¤±å‡½æ•°ï¼Œå¢åŠ é¦–é€‰è§£é‡Šçš„æ¦‚ç‡ï¼ŒåŒæ—¶å‡å°‘éé¦–é€‰è¾“å‡ºçš„å¯èƒ½æ€§ã€‚æˆ‘ä»¬åœ¨å››ç§ä¸»è¦æ¼æ´ç±»å‹ï¼šé‡å…¥ã€æ—¶é—´æˆ³ä¾èµ–ã€æ•´æ•°æº¢å‡º&#x2F;ä¸‹æº¢å’Œdelegatecallï¼Œä»¥åŠæœºå™¨æ— æ³•å®¡æ ¸çš„æ¼æ´ä¸Šè¯„ä¼°äº†Smart-LLaMA-DPOã€‚æˆ‘ä»¬çš„æ–¹æ³•æ˜¾è‘—ä¼˜äºæœ€æ–°åŸºçº¿ï¼ŒF1åˆ†æ•°å¹³å‡æé«˜10.43%ï¼Œå‡†ç¡®ç‡æé«˜7.87%ã€‚è€Œä¸”ï¼ŒLLMè¯„ä¼°å’Œäººç±»è¯„ä¼°éƒ½è¯å®ï¼Œæˆ‘ä»¬çš„æ–¹æ³•ç”Ÿæˆçš„è§£é‡Šæ›´åŠ æ­£ç¡®ã€å…¨é¢å’Œæ¸…æ™°ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.18245v1">PDF</a> Accepted to ISSTA 2025</p>
<p><strong>Summary</strong><br>æ™ºèƒ½åˆçº¦æ¼æ´æ£€æµ‹æ˜¯åŒºå—é“¾å®‰å…¨ä¸­çš„ä¸€å¤§æŒ‘æˆ˜ã€‚ç°æœ‰æ–¹æ³•é¢ä¸´æ•°æ®é›†è¦†ç›–ä¸å…¨ã€è§£é‡Šè´¨é‡ä¸é«˜ä»¥åŠå¤§å‹è¯­è¨€æ¨¡å‹å¯¹æ™ºèƒ½åˆçº¦å®‰å…¨ç‰¹å®šæ¦‚å¿µè§£è¯»ä¸ç²¾å‡†çš„é—®é¢˜ã€‚ä¸ºè§£å†³è¿™äº›é—®é¢˜ï¼Œæå‡ºåŸºäºLLaMA-3.1-8Bçš„Smart-LLaMA-DPOæ–¹æ³•ã€‚æ„å»ºå…¨é¢æ•°æ®é›†ï¼Œæ¶µç›–å››å¤§æ¼æ´ç±»å‹å’Œæœºå™¨æ— æ³•å®¡æ ¸çš„æ¼æ´ï¼Œå¢å¼ºLLMå¯¹æ™ºèƒ½åˆçº¦å®‰å…¨å®è·µçš„ç†è§£ï¼Œå¹¶è¿›è¡Œç›´æ¥åå¥½ä¼˜åŒ–ã€‚è¯„ä¼°æ˜¾ç¤ºï¼Œè¯¥æ–¹æ³•åœ¨ä¸»è¦æ¼æ´ç±»å‹ä¸Šæ˜¾è‘—ä¼˜äºç°æœ‰åŸºçº¿ï¼ŒF1åˆ†æ•°æé«˜10.43%ï¼Œå‡†ç¡®ç‡æé«˜7.87%ï¼Œç”Ÿæˆçš„è§£é‡Šæ›´å‡†ç¡®ã€å…¨é¢ã€æ¸…æ™°ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ™ºèƒ½åˆçº¦æ¼æ´æ£€æµ‹æ˜¯åŒºå—é“¾å®‰å…¨çš„é‡è¦æŒ‘æˆ˜ã€‚</li>
<li>ç°æœ‰æ–¹æ³•é¢ä¸´æ•°æ®é›†è¦†ç›–ä¸å…¨å’Œé«˜è´¨é‡è§£é‡Šç¼ºå¤±çš„é—®é¢˜ã€‚</li>
<li>å¤§å‹è¯­è¨€æ¨¡å‹åœ¨è§£è¯»æ™ºèƒ½åˆçº¦å®‰å…¨ç‰¹å®šæ¦‚å¿µæ—¶å­˜åœ¨å›°éš¾ã€‚</li>
<li>æå‡ºSmart-LLaMA-DPOæ–¹æ³•ï¼ŒåŸºäºLLaMA-3.1-8Bï¼Œæ„å»ºå…¨é¢æ•°æ®é›†ä»¥è§£å†³ä¸Šè¿°é—®é¢˜ã€‚</li>
<li>æ–¹æ³•åŒ…æ‹¬æ„å»ºæ•°æ®é›†ã€å¢å¼ºLLMç†è§£ã€è¿›è¡Œç›´æ¥åå¥½ä¼˜åŒ–ç­‰æ­¥éª¤ã€‚</li>
<li>è¯„ä¼°æ˜¾ç¤ºï¼ŒSmart-LLaMA-DPOåœ¨ä¸»è¦æ¼æ´ç±»å‹ä¸Šæ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.18245">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-032153fd995bd9a50080e998f50bf0ac.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6cb6bc88b0770badb459f2b07ff3d509.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-e39ead151db9b8bf10b26d20d1f7b5bb.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="Drive-R1-Bridging-Reasoning-and-Planning-in-VLMs-for-Autonomous-Driving-with-Reinforcement-Learning"><a href="#Drive-R1-Bridging-Reasoning-and-Planning-in-VLMs-for-Autonomous-Driving-with-Reinforcement-Learning" class="headerlink" title="Drive-R1: Bridging Reasoning and Planning in VLMs for Autonomous Driving   with Reinforcement Learning"></a>Drive-R1: Bridging Reasoning and Planning in VLMs for Autonomous Driving   with Reinforcement Learning</h2><p><strong>Authors:Yue Li, Meng Tian, Dechang Zhu, Jiangtong Zhu, Zhenyu Lin, Zhiwei Xiong, Xinhai Zhao</strong></p>
<p>Large vision-language models (VLMs) for autonomous driving (AD) are evolving beyond perception and cognition tasks toward motion planning. However, we identify two critical challenges in this direction: (1) VLMs tend to learn shortcuts by relying heavily on history input information, achieving seemingly strong planning results without genuinely understanding the visual inputs; and (2) the chain-ofthought (COT) reasoning processes are always misaligned with the motion planning outcomes, and how to effectively leverage the complex reasoning capability to enhance planning remains largely underexplored. In this paper, we start from a small-scale domain-specific VLM and propose Drive-R1 designed to bridges the scenario reasoning and motion planning for AD. Drive-R1 first undergoes the supervised finetuning on a elaborate dataset containing both long and short COT data. Drive-R1 is encouraged to reason step-by-step from visual input to final planning decisions. Subsequently, Drive-R1 is trained within a reinforcement learning framework that incentivizes the discovery of reasoning paths that are more informative for planning, guided by rewards based on predicted trajectories and meta actions. Experimental evaluations on the nuScenes and DriveLM-nuScenes benchmarks demonstrate that Drive-R1 achieves superior performance compared to existing state-of-the-art VLMs. We believe that Drive-R1 presents a promising direction for bridging reasoning and planning in AD, offering methodological insights for future research and applications. </p>
<blockquote>
<p>é’ˆå¯¹è‡ªåŠ¨é©¾é©¶ï¼ˆADï¼‰çš„å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰æ­£åœ¨ä»æ„ŸçŸ¥å’Œè®¤çŸ¥ä»»åŠ¡å‘è¿åŠ¨è§„åˆ’å‘å±•ã€‚ç„¶è€Œï¼Œæˆ‘ä»¬åœ¨è¿™ä¸ªæ–¹å‘ä¸Šé¢ä¸´ä¸¤ä¸ªå…³é”®æŒ‘æˆ˜ï¼šï¼ˆ1ï¼‰VLMå€¾å‘äºé€šè¿‡ä¾èµ–å¤§é‡çš„å†å²è¾“å…¥ä¿¡æ¯æ¥å­¦ä¼šèµ°æ·å¾„ï¼Œè€Œå¹¶éçœŸæ­£ç†è§£è§†è§‰è¾“å…¥ä¾¿èƒ½å¤Ÿè¾¾åˆ°çœ‹ä¼¼å¼ºå¤§çš„è§„åˆ’ç»“æœï¼›ï¼ˆ2ï¼‰é“¾å¼æ€ç»´ï¼ˆCOTï¼‰æ¨ç†è¿‡ç¨‹æ€»æ˜¯ä¸è¿åŠ¨è§„åˆ’ç»“æœä¸ä¸€è‡´ï¼Œå¦‚ä½•æœ‰æ•ˆåˆ©ç”¨å¤æ‚çš„æ¨ç†èƒ½åŠ›æ¥æå‡è§„åˆ’ä»ç„¶æ˜¯ä¸€ä¸ªæœªè¢«å……åˆ†ç ”ç©¶çš„é—®é¢˜ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬ä»ä¸€ä¸ªå°è§„æ¨¡çš„ç‰¹å®šé¢†åŸŸVLMå‡ºå‘ï¼Œæå‡ºäº†æ—¨åœ¨å¼¥åˆåœºæ™¯æ¨ç†å’Œè‡ªåŠ¨é©¾é©¶è¿åŠ¨è§„åˆ’çš„Drive-R1ã€‚Drive-R1é¦–å…ˆåœ¨åŒ…å«é•¿çŸ­é“¾æ€ç»´æ•°æ®çš„ç²¾è‡´æ•°æ®é›†ä¸Šè¿›è¡Œç›‘ç£å¾®è°ƒã€‚å®ƒé¼“åŠ±ä»è§†è§‰è¾“å…¥åˆ°æœ€ç»ˆè§„åˆ’å†³ç­–è¿›è¡Œé€æ­¥æ¨ç†ã€‚éšåï¼Œæˆ‘ä»¬åœ¨å¼ºåŒ–å­¦ä¹ æ¡†æ¶ä¸‹è®­ç»ƒDrive-R1ï¼Œæ¿€åŠ±å‘ç°æ›´æœ‰ç›Šäºè§„åˆ’çš„æ¨ç†è·¯å¾„ï¼Œå¹¶æ ¹æ®é¢„æµ‹çš„è½¨è¿¹å’Œå…ƒåŠ¨ä½œæä¾›å¥–åŠ±ä½œä¸ºæŒ‡å¯¼ã€‚åœ¨nuSceneså’ŒDriveLM-nuScenesåŸºå‡†æµ‹è¯•ä¸Šçš„å®éªŒè¯„ä¼°è¡¨æ˜ï¼Œä¸ç°æœ‰çš„æœ€å…ˆè¿›çš„VLMç›¸æ¯”ï¼ŒDrive-R1å®ç°äº†å“è¶Šçš„æ€§èƒ½ã€‚æˆ‘ä»¬ç›¸ä¿¡ï¼ŒDrive-R1ä¸ºå¼¥åˆè‡ªåŠ¨é©¾é©¶ä¸­çš„æ¨ç†å’Œè§„åˆ’æä¾›äº†ä¸€ä¸ªæœ‰å‰æ™¯çš„æ–¹å‘ï¼Œä¸ºæœªæ¥ç ”ç©¶ä¸åº”ç”¨æä¾›äº†æ–¹æ³•è®ºä¸Šçš„å¯ç¤ºã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.18234v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰åœ¨è‡ªåŠ¨é©¾é©¶ï¼ˆADï¼‰é¢†åŸŸé¢ä¸´çš„æŒ‘æˆ˜ã€‚ä¸ºè§£å†³è¿™äº›é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§è®¾è®¡ç”¨äºè¿æ¥åœºæ™¯æ¨ç†å’Œè‡ªåŠ¨é©¾é©¶è¿åŠ¨è§„åˆ’çš„ç‰¹å®šåŸŸå°å°ºåº¦æ¨¡å‹Drive-R1ã€‚é€šè¿‡é‡‡ç”¨ç»“åˆé•¿æœŸå’ŒçŸ­æœŸæ€ç»´è¿‡ç¨‹çš„å¼ºåŒ–å­¦ä¹ è®­ç»ƒæ–¹æ³•ï¼ŒDrive-R1èƒ½å¤Ÿå®ç°ä»è§†è§‰è¾“å…¥åˆ°æœ€ç»ˆè§„åˆ’å†³ç­–çš„é€æ­¥æ¨ç†ï¼Œä¸”åœ¨nuSceneså’ŒDriveLM-nuScenesåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºå“è¶Šæ€§èƒ½ã€‚è¿™ä¸ºæœªæ¥çš„ç ”ç©¶å’Œåº”ç”¨æä¾›äº†æ–¹æ³•è®ºä¸Šçš„å¯ç¤ºã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹åœ¨è‡ªåŠ¨é©¾é©¶é¢†åŸŸæ­£é¢ä¸´ä»æ„ŸçŸ¥å’Œè®¤çŸ¥ä»»åŠ¡å‘è¿åŠ¨è§„åˆ’å‘å±•çš„æŒ‘æˆ˜ã€‚</li>
<li>å­˜åœ¨ä¸¤ä¸ªå…³é”®æŒ‘æˆ˜ï¼šæ¨¡å‹ä¾èµ–å†å²ä¿¡æ¯å¯¼è‡´çš„â€œæ·å¾„â€å­¦ä¹ å’Œæ€ç»´è¿‡ç¨‹ä¸è¿åŠ¨è§„åˆ’ç»“æœçš„ä¸å¯¹é½ã€‚</li>
<li>æå‡ºäº†ä¸€ç§ç‰¹å®šåŸŸå°å°ºåº¦æ¨¡å‹Drive-R1ï¼Œæ—¨åœ¨è¿æ¥åœºæ™¯æ¨ç†å’Œè‡ªåŠ¨é©¾é©¶è¿åŠ¨è§„åˆ’ã€‚</li>
<li>Drive-R1é€šè¿‡ç»“åˆé•¿æœŸå’ŒçŸ­æœŸæ€ç»´è¿‡ç¨‹çš„å¼ºåŒ–å­¦ä¹ è®­ç»ƒï¼Œå®ç°ä»è§†è§‰è¾“å…¥åˆ°æœ€ç»ˆè§„åˆ’å†³ç­–çš„é€æ­¥æ¨ç†ã€‚</li>
<li>æ¨¡å‹åœ¨nuSceneså’ŒDriveLM-nuScenesåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜è¶Šã€‚</li>
<li>Drive-R1ä¸ºæœªæ¥çš„ç ”ç©¶å’Œåº”ç”¨æä¾›äº†æ–¹æ³•è®ºä¸Šçš„å¯ç¤ºã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.18234">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-b8a51ba5a80d8c3c50af3e4012a794b1.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-2e486f6642fc5c212c5b761058cd79c6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-89390160552661fbde0c6908a1cc2dbe.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-da0f6117e1e0ed6dcc0b9096fda30ead.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="Reasoning-about-Uncertainty-Do-Reasoning-Models-Know-When-They-Donâ€™t-Know"><a href="#Reasoning-about-Uncertainty-Do-Reasoning-Models-Know-When-They-Donâ€™t-Know" class="headerlink" title="Reasoning about Uncertainty: Do Reasoning Models Know When They Donâ€™t   Know?"></a>Reasoning about Uncertainty: Do Reasoning Models Know When They Donâ€™t   Know?</h2><p><strong>Authors:Zhiting Mei, Christina Zhang, Tenny Yin, Justin Lidard, Ola Shorinwa, Anirudha Majumdar</strong></p>
<p>Reasoning language models have set state-of-the-art (SOTA) records on many challenging benchmarks, enabled by multi-step reasoning induced using reinforcement learning. However, like previous language models, reasoning models are prone to generating confident, plausible responses that are incorrect (hallucinations). Knowing when and how much to trust these models is critical to the safe deployment of reasoning models in real-world applications. To this end, we explore uncertainty quantification of reasoning models in this work. Specifically, we ask three fundamental questions: First, are reasoning models well-calibrated? Second, does deeper reasoning improve model calibration? Finally, inspired by humansâ€™ innate ability to double-check their thought processes to verify the validity of their answers and their confidence, we ask: can reasoning models improve their calibration by explicitly reasoning about their chain-of-thought traces? We introduce introspective uncertainty quantification (UQ) to explore this direction. In extensive evaluations on SOTA reasoning models across a broad range of benchmarks, we find that reasoning models: (i) are typically overconfident, with self-verbalized confidence estimates often greater than 85% particularly for incorrect responses, (ii) become even more overconfident with deeper reasoning, and (iii) can become better calibrated through introspection (e.g., o3-Mini and DeepSeek R1) but not uniformly (e.g., Claude 3.7 Sonnet becomes more poorly calibrated). Lastly, we conclude with important research directions to design necessary UQ benchmarks and improve the calibration of reasoning models. </p>
<blockquote>
<p>æ¨ç†è¯­è¨€æ¨¡å‹å·²åœ¨è®¸å¤šå…·æœ‰æŒ‘æˆ˜æ€§çš„åŸºå‡†æµ‹è¯•ä¸Šè¾¾åˆ°äº†æœ€æ–°æŠ€æœ¯è®°å½•ï¼Œè¿™æ˜¯é€šè¿‡ä½¿ç”¨å¼ºåŒ–å­¦ä¹ å¼•å‘çš„å¤šæ­¥æ¨ç†å®ç°çš„ã€‚ç„¶è€Œï¼Œä¸ä¹‹å‰çš„è¯­è¨€æ¨¡å‹ä¸€æ ·ï¼Œæ¨ç†æ¨¡å‹ä¹Ÿå®¹æ˜“äº§ç”Ÿè‡ªä¿¡ä¸”çœ‹ä¼¼åˆç†çš„é”™è¯¯ç­”æ¡ˆï¼ˆå¹»è§‰ï¼‰ã€‚çŸ¥é“åœ¨ä½•æ—¶ä»¥åŠå¤šä¿¡ä»»è¿™äº›æ¨¡å‹å¯¹äºåœ¨çœŸå®ä¸–ç•Œåº”ç”¨ä¸­å®‰å…¨éƒ¨ç½²æ¨ç†æ¨¡å‹è‡³å…³é‡è¦ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬åœ¨æ­¤å·¥ä½œä¸­æ¢ç´¢äº†æ¨ç†æ¨¡å‹çš„ä¸ç¡®å®šæ€§é‡åŒ–ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸‰ä¸ªåŸºæœ¬é—®é¢˜ï¼šé¦–å…ˆï¼Œæ¨ç†æ¨¡å‹æ˜¯å¦ç»è¿‡è‰¯å¥½æ ¡å‡†ï¼Ÿå…¶æ¬¡ï¼Œæ›´æ·±å…¥çš„æ¨ç†èƒ½å¦æé«˜æ¨¡å‹çš„æ ¡å‡†åº¦ï¼Ÿæœ€åï¼Œå—äººç±»å¤©ç”Ÿèƒ½å¤Ÿæ£€æŸ¥è‡ªå·±çš„æ€ç»´è¿‡ç¨‹ä»¥éªŒè¯ç­”æ¡ˆçš„æœ‰æ•ˆæ€§å’Œè‡ªä¿¡å¿ƒçš„å¯å‘ï¼Œæˆ‘ä»¬é—®ï¼šæ¨ç†æ¨¡å‹æ˜¯å¦å¯ä»¥é€šè¿‡æ˜ç¡®æ¨ç†å…¶æ€ç»´è½¨è¿¹æ¥æé«˜å…¶æ ¡å‡†åº¦ï¼Ÿæˆ‘ä»¬å¼•å…¥å†…çœä¸ç¡®å®šæ€§é‡åŒ–ï¼ˆUQï¼‰æ¥æ¢ç´¢è¿™ä¸ªæ–¹å‘ã€‚åœ¨å¯¹ä¸€ç³»åˆ—æœ€æ–°æŠ€æœ¯æ¨ç†æ¨¡å‹è¿›è¡Œå¹¿æ³›çš„åŸºå‡†æµ‹è¯•è¯„ä¼°åï¼Œæˆ‘ä»¬å‘ç°æ¨ç†æ¨¡å‹é€šå¸¸è¿‡äºè‡ªä¿¡ï¼Œè‡ªæˆ‘è¡¨è¿°çš„ç½®ä¿¡åº¦ä¼°è®¡å°¤å…¶æ˜¯é’ˆå¯¹é”™è¯¯ç­”æ¡ˆæ—¶ç»å¸¸è¶…è¿‡85%ï¼Œéšç€æ¨ç†çš„æ·±å…¥ï¼Œå®ƒä»¬ç”šè‡³å˜å¾—æ›´åŠ è‡ªä¿¡ï¼Œä½†é€šè¿‡å†…çœï¼ˆä¾‹å¦‚o3-Miniå’ŒDeepSeek R1ï¼‰å¯ä»¥æ›´å¥½åœ°æ ¡å‡†ï¼Œä½†å¹¶éæ‰€æœ‰æ¨¡å‹éƒ½å¦‚æ­¤ï¼ˆä¾‹å¦‚Claude 3.7 Sonnetçš„æ ¡å‡†åº¦å˜å¾—æ›´å·®ï¼‰ã€‚æœ€åï¼Œæˆ‘ä»¬å¾—å‡ºäº†é‡è¦ç ”ç©¶æ–¹å‘ï¼Œä»¥è®¾è®¡å¿…è¦çš„ä¸ç¡®å®šæ€§é‡åŒ–åŸºå‡†å¹¶æ”¹å–„æ¨ç†æ¨¡å‹çš„æ ¡å‡†ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.18183v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ¢è®¨äº†åŸºäºå¼ºåŒ–å­¦ä¹ è¯±å¯¼çš„å¤šæ­¥æ¨ç†è¯­è¨€æ¨¡å‹çš„ä¸ç¡®å®šæ€§é‡åŒ–é—®é¢˜ã€‚æ–‡ç« æå‡ºäº†ä¸‰ä¸ªå…³é”®é—®é¢˜ï¼šæ¨ç†æ¨¡å‹æ˜¯å¦æ ¡å‡†è‰¯å¥½ï¼Ÿæ›´æ·±çš„æ¨ç†æ˜¯å¦æ”¹å–„æ¨¡å‹æ ¡å‡†ï¼Ÿä»¥åŠæ¨ç†æ¨¡å‹æ˜¯å¦å¯ä»¥é€šè¿‡å¯¹å…¶æ€ç»´è¿‡ç¨‹è¿›è¡Œå†…çœæ¥æé«˜æ ¡å‡†ï¼Ÿç ”ç©¶å‘ç°åœ¨ä¸€äº›åŸºå‡†æµ‹è¯•ä¸­ï¼Œæ¨ç†æ¨¡å‹æ™®éå­˜åœ¨è¿‡åº¦è‡ªä¿¡é—®é¢˜ï¼Œæ›´æ·±å±‚æ¬¡çš„æ¨ç†å¯èƒ½ä¼šåŠ å‰§è¿™ç§è¿‡åº¦è‡ªä¿¡ï¼Œè€Œé€šè¿‡å†…çœçš„æ–¹å¼å¯ä»¥æ”¹å–„æ¨¡å‹çš„æ ¡å‡†æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ¨ç†è¯­è¨€æ¨¡å‹åœ¨è®¸å¤šæŒ‘æˆ˜æ€§åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºå“è¶Šæ€§èƒ½ï¼Œä½†ä»å­˜åœ¨ç”Ÿæˆè‡ªä¿¡ä½†é”™è¯¯çš„å›ç­”ï¼ˆhallucinationsï¼‰çš„é—®é¢˜ã€‚</li>
<li>æ¨ç†æ¨¡å‹çš„ä¸ç¡®å®šæ€§é‡åŒ–å¯¹äºå…¶åœ¨ç°å®ä¸–ç•Œåº”ç”¨ä¸­çš„å®‰å…¨éƒ¨ç½²è‡³å…³é‡è¦ã€‚</li>
<li>æ¨ç†æ¨¡å‹æ™®éå­˜åœ¨è¿‡åº¦è‡ªä¿¡é—®é¢˜ï¼Œç‰¹åˆ«æ˜¯åœ¨é”™è¯¯çš„å›ç­”ä¸Šï¼Œè‡ªæˆ‘è¡¨è¿°çš„ä¿¡å¿ƒä¼°è®¡é€šå¸¸è¶…è¿‡85%ã€‚</li>
<li>æ›´æ·±çš„æ¨ç†å¯èƒ½ä¼šåŠ å‰§æ¨¡å‹çš„è¿‡åº¦è‡ªä¿¡ã€‚</li>
<li>é€šè¿‡å†…çœçš„æ–¹å¼ï¼Œéƒ¨åˆ†æ¨ç†æ¨¡å‹å¯ä»¥æ”¹å–„å…¶æ ¡å‡†æ€§èƒ½ï¼Œä½†ä¸æ˜¯æ‰€æœ‰æ¨¡å‹éƒ½èƒ½å¾—åˆ°ç»Ÿä¸€æ”¹å–„ã€‚</li>
<li>éœ€è¦è®¾è®¡å¿…è¦çš„ä¸ç¡®å®šæ€§é‡åŒ–åŸºå‡†æµ‹è¯•æ¥æ”¹å–„æ¨ç†æ¨¡å‹çš„æ ¡å‡†ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.18183">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-af1ed632fbe9555047e580c499296f6d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-85108b4cc391ab764cd59473a6e97f5f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-6de958e088c45ab303d691ad724ffd40.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="InspireDebate-Multi-Dimensional-Subjective-Objective-Evaluation-Guided-Reasoning-and-Optimization-for-Debating"><a href="#InspireDebate-Multi-Dimensional-Subjective-Objective-Evaluation-Guided-Reasoning-and-Optimization-for-Debating" class="headerlink" title="InspireDebate: Multi-Dimensional Subjective-Objective Evaluation-Guided   Reasoning and Optimization for Debating"></a>InspireDebate: Multi-Dimensional Subjective-Objective Evaluation-Guided   Reasoning and Optimization for Debating</h2><p><strong>Authors:Fuyu Wang, Jiangtong Li, Kun Zhu, Changjun Jiang</strong></p>
<p>With the rapid advancements in large language models (LLMs), debating tasks, such as argument quality assessment and debate process simulation, have made significant progress. However, existing LLM-based debating systems focus on responding to specific arguments while neglecting objective assessments such as authenticity and logical validity. Furthermore, these systems lack a structured approach to optimize across various dimensions$-$including evaluation metrics, chain-of-thought (CoT) reasoning, and multi-turn debate refinement$-$thereby limiting their effectiveness. To address these interconnected challenges, we propose a dual-component framework: (1) $\textbf{InspireScore}$, a novel evaluation system that establishes a multi-dimensional assessment architecture incorporating four subjective criteria (emotional appeal, argument clarity, argument arrangement, and topic relevance) alongside two objective metrics (fact authenticity and logical validity); and (2) $\textbf{InspireDebate}$, an optimized debating framework employing a phased optimization approach through CoT reasoning enhancement, multi-dimensional Direct Preference Optimization (DPO), and real-time knowledge grounding via web-based Retrieval Augmented Generation (Web-RAG). Empirical evaluations demonstrate that $\textbf{InspireScore}$ achieves 44$%$ higher correlation with expert judgments compared to existing methods, while $\textbf{InspireDebate}$ shows significant improvements, outperforming baseline models by 57$%$. Source code is available at <a target="_blank" rel="noopener" href="https://github.com/fywang12/InspireDebate">https://github.com/fywang12/InspireDebate</a>. </p>
<blockquote>
<p>éšç€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„å¿«é€Ÿå‘å±•ï¼Œè¾©è®ºä»»åŠ¡ï¼Œå¦‚è®ºè¯è´¨é‡è¯„ä¼°å’Œè¾©è®ºè¿‡ç¨‹æ¨¡æ‹Ÿï¼Œéƒ½å–å¾—äº†æ˜¾è‘—çš„è¿›æ­¥ã€‚ç„¶è€Œï¼Œç°æœ‰çš„åŸºäºLLMçš„è¾©è®ºç³»ç»Ÿä¸»è¦å…³æ³¨å¯¹ç‰¹å®šè®ºç‚¹çš„å›åº”ï¼Œè€Œå¿½è§†äº†å®¢è§‚æ€§è¯„ä¼°ï¼Œå¦‚çœŸå®æ€§å’Œé€»è¾‘æœ‰æ•ˆæ€§ã€‚æ­¤å¤–ï¼Œè¿™äº›ç³»ç»Ÿç¼ºä¹ä¸€ç§ç»“æ„åŒ–æ–¹æ³•æ¥ä¼˜åŒ–å„ä¸ªç»´åº¦ï¼ŒåŒ…æ‹¬è¯„ä¼°æŒ‡æ ‡ã€æ€ç»´é“¾ï¼ˆCoTï¼‰æ¨ç†å’Œå¤šè½®è¾©è®ºç»†åŒ–ç­‰ï¼Œä»è€Œé™åˆ¶äº†å…¶æœ‰æ•ˆæ€§ã€‚ä¸ºäº†è§£å†³è¿™äº›ç›¸äº’å…³è”çš„æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªåŒç»„ä»¶æ¡†æ¶ï¼šï¼ˆ1ï¼‰<strong>InspireScore</strong>ï¼Œè¿™æ˜¯ä¸€ä¸ªæ–°çš„è¯„ä¼°ç³»ç»Ÿï¼Œå»ºç«‹äº†ä¸€ä¸ªå¤šç»´è¯„ä¼°æ¶æ„ï¼Œç»“åˆäº†å››ä¸ªä¸»è§‚æ ‡å‡†ï¼ˆæƒ…æ„Ÿå¸å¼•åŠ›ã€è®ºè¯æ¸…æ™°åº¦ã€è®ºè¯ç»„ç»‡å’Œè¯é¢˜ç›¸å…³æ€§ï¼‰ï¼Œä»¥åŠä¸¤ä¸ªå®¢è§‚æŒ‡æ ‡ï¼ˆäº‹å®çœŸå®æ€§å’Œé€»è¾‘æœ‰æ•ˆæ€§ï¼‰ï¼›ï¼ˆ2ï¼‰<strong>InspireDebate</strong>ï¼Œè¿™æ˜¯ä¸€ä¸ªä¼˜åŒ–çš„è¾©è®ºæ¡†æ¶ï¼Œé€šè¿‡æ€ç»´é“¾æ¨ç†å¢å¼ºã€å¤šç»´ç›´æ¥åå¥½ä¼˜åŒ–ï¼ˆDPOï¼‰å’ŒåŸºäºWebçš„æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆWeb-RAGï¼‰è¿›è¡Œå®æ—¶çŸ¥è¯†å®šä½ï¼Œé‡‡ç”¨åˆ†é˜¶æ®µä¼˜åŒ–æ–¹æ³•ã€‚å®è¯è¯„ä¼°è¡¨æ˜ï¼Œ<strong>InspireScore</strong>ä¸ä¸“å®¶åˆ¤æ–­çš„ç›¸å…³æ€§æ¯”ç°æœ‰æ–¹æ³•é«˜å‡º44%ï¼Œè€Œ<strong>InspireDebate</strong>æ˜¾ç¤ºå‡ºæ˜¾è‘—æ”¹è¿›ï¼Œè¾ƒåŸºçº¿æ¨¡å‹æé«˜äº†57%ã€‚æºä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/fywang12/InspireDebate">https://github.com/fywang12/InspireDebate</a>è·å–ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.18102v1">PDF</a> 20 pages; Accepted to ACL 2025 Main</p>
<p><strong>Summary</strong></p>
<p>éšç€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„å¿«é€Ÿå‘å±•ï¼Œè¾©è®ºä»»åŠ¡ï¼Œå¦‚è®ºè¯è´¨é‡è¯„ä¼°ä¸è¾©è®ºè¿‡ç¨‹æ¨¡æ‹Ÿï¼Œå·²å–å¾—äº†æ˜¾è‘—è¿›æ­¥ã€‚ç„¶è€Œï¼Œç°æœ‰çš„LLMè¾©è®ºç³»ç»Ÿä¸»è¦å…³æ³¨å¯¹ç‰¹å®šè®ºè¯çš„å›åº”ï¼Œå¿½è§†äº†çœŸå®æ€§ã€é€»è¾‘æœ‰æ•ˆæ€§ç­‰å®¢è§‚è¯„ä¼°ã€‚æ­¤å¤–ï¼Œè¿™äº›ç³»ç»Ÿç¼ºä¹è·¨å¤šä¸ªç»´åº¦çš„ä¼˜åŒ–æ–¹æ³•ï¼ŒåŒ…æ‹¬è¯„ä¼°æŒ‡æ ‡ã€æ€ç»´é“¾æ¨ç†å’Œå¤šè½®è¾©è®ºä¼˜åŒ–ç­‰ï¼Œä»è€Œé™åˆ¶äº†å…¶æ•ˆæœã€‚ä¸ºè§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæå‡ºäº†ä¸€ä¸ªåŒç»„åˆ†æ¡†æ¶ï¼ŒåŒ…æ‹¬InspireScoreè¯„ä»·ç³»ç»Ÿå’ŒInspireDebateè¾©è®ºæ¡†æ¶ã€‚InspireScoreå»ºç«‹äº†ä¸€ä¸ªå¤šç»´åº¦è¯„ä¼°æ¶æ„ï¼Œç»“åˆå››ä¸ªä¸»è§‚æ ‡å‡†å’Œä¸¤ä¸ªå®¢è§‚æŒ‡æ ‡ï¼›InspireDebateåˆ™é‡‡ç”¨åˆ†é˜¶æ®µä¼˜åŒ–æ–¹æ³•ï¼Œé€šè¿‡å¢å¼ºæ€ç»´é“¾æ¨ç†ã€å¤šç»´ç›´æ¥åå¥½ä¼˜åŒ–å’Œå®æ—¶çŸ¥è¯†æ¥åœ°ç­‰æŠ€æœ¯æé«˜è¾©è®ºè´¨é‡ã€‚ç»éªŒè¯„ä¼°æ˜¾ç¤ºï¼ŒInspireScoreä¸ä¸“å®¶åˆ¤æ–­çš„ç›¸å…³æ€§æ¯”ç°æœ‰æ–¹æ³•é«˜å‡º44%ï¼Œè€ŒInspireDebateä¹Ÿæ˜¾ç¤ºå‡ºæ˜¾è‘—æ”¹è¿›ï¼Œè¾ƒåŸºçº¿æ¨¡å‹æé«˜äº†57%ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLMsåœ¨è¾©è®ºä»»åŠ¡ä¸Šå–å¾—äº†æ˜¾è‘—è¿›æ­¥ï¼Œä½†ä»å­˜åœ¨å¯¹å®¢è§‚è¯„ä¼°çš„å¿½è§†å’Œç¼ºä¹å¤šç»´åº¦ä¼˜åŒ–æ–¹æ³•çš„æŒ‘æˆ˜ã€‚</li>
<li>æå‡ºçš„InspireScoreè¯„ä»·ç³»ç»Ÿå»ºç«‹äº†ä¸€ä¸ªå¤šç»´åº¦è¯„ä¼°æ¶æ„ï¼Œç»“åˆä¸»è§‚å’Œå®¢è§‚æŒ‡æ ‡ï¼Œæ›´å…¨é¢åœ°è¯„ä¼°è®ºè¯ã€‚</li>
<li>InspireDebateè¾©è®ºæ¡†æ¶é‡‡ç”¨åˆ†é˜¶æ®µä¼˜åŒ–æ–¹æ³•ï¼ŒåŒ…æ‹¬æ€ç»´é“¾æ¨ç†å¢å¼ºã€å¤šç»´ç›´æ¥åå¥½ä¼˜åŒ–å’Œå®æ—¶çŸ¥è¯†æ¥åœ°ç­‰æŠ€æœ¯ã€‚</li>
<li>InspireScoreä¸ä¸“å®¶åˆ¤æ–­çš„ç›¸å…³æ€§è¾ƒé«˜ï¼Œè¾¾åˆ°44%çš„æå‡ã€‚</li>
<li>InspireDebateåœ¨æ€§èƒ½ä¸Šè¾ƒåŸºçº¿æ¨¡å‹æœ‰æ˜¾è‘—æ”¹å–„ï¼Œæé«˜äº†57%ã€‚</li>
<li>å…¬å¼€çš„ä»£ç èµ„æºå¯ç”¨äºè¿›ä¸€æ­¥ç ”ç©¶å’Œå¼€å‘ã€‚</li>
<li>è¯¥æ¡†æ¶å…·æœ‰æ½œåŠ›è§£å†³ç°æœ‰LLMè¾©è®ºç³»ç»Ÿçš„æ ¸å¿ƒé—®é¢˜ï¼Œä¸ºè¾©è®ºä»»åŠ¡æä¾›æ›´æœ‰æ•ˆçš„æ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.18102">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-d1fde41187b1a3590f8d727d17ee5631.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-bfd3a47443a6288393eda5ce1bf3c97b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d1f0d17ff048cc38c1ca8039dc0750f7.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="SegChange-R1-Augmented-Reasoning-for-Remote-Sensing-Change-Detection-via-Large-Language-Models"><a href="#SegChange-R1-Augmented-Reasoning-for-Remote-Sensing-Change-Detection-via-Large-Language-Models" class="headerlink" title="SegChange-R1:Augmented Reasoning for Remote Sensing Change Detection via   Large Language Models"></a>SegChange-R1:Augmented Reasoning for Remote Sensing Change Detection via   Large Language Models</h2><p><strong>Authors:Fei Zhou</strong></p>
<p>Remote sensing change detection is widely used in a variety of fields such as urban planning, terrain and geomorphology analysis, and environmental monitoring, mainly by analyzing the significant change differences of features (e.g., building changes) in the same spatial region at different time phases. In this paper, we propose a large language model (LLM) augmented inference approach (SegChange-R1), which enhances the detection capability by integrating textual descriptive information and aims at guiding the model to segment the more interested change regions, thus accelerating the convergence speed. Moreover, we design a spatial transformation module (BEV) based on linear attention, which solves the problem of modal misalignment in change detection by unifying features from different temporal perspectives onto the BEV space. In addition, we construct the first dataset for building change detection from UAV viewpoints (DVCD ), and our experiments on four widely-used change detection datasets show a significant improvement over existing methods. The code and pre-trained models are available in <a target="_blank" rel="noopener" href="https://github.com/Yu-Zhouz/SegChange-R1">https://github.com/Yu-Zhouz/SegChange-R1</a>. </p>
<blockquote>
<p>é¥æ„Ÿå˜åŒ–æ£€æµ‹å¹¿æ³›åº”ç”¨äºåŸå¸‚è§„åˆ’ã€åœ°å½¢åœ°è²Œåˆ†æã€ç¯å¢ƒç›‘æµ‹ç­‰å¤šä¸ªé¢†åŸŸï¼Œä¸»è¦é€šè¿‡åˆ†æä¸åŒæ—¶é—´é˜¶æ®µåŒä¸€ç©ºé—´åŒºåŸŸå†…ç‰¹å¾ï¼ˆå¦‚å»ºç­‘å˜åŒ–ï¼‰çš„æ˜¾è‘—å˜åŒ–å·®å¼‚æ¥è¿›è¡Œæ£€æµ‹ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºå¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„å¢å¼ºæ¨ç†æ–¹æ³•ï¼ˆSegChange-R1ï¼‰ï¼Œé€šè¿‡é›†æˆæ–‡æœ¬æè¿°ä¿¡æ¯æ¥æé«˜æ£€æµ‹èƒ½åŠ›ï¼Œæ—¨åœ¨å¼•å¯¼æ¨¡å‹åˆ†å‰²æ›´æ„Ÿå…´è¶£çš„å˜åŒ–åŒºåŸŸï¼Œä»è€ŒåŠ å¿«æ”¶æ•›é€Ÿåº¦ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸€ä¸ªåŸºäºçº¿æ€§æ³¨æ„åŠ›çš„ç©ºé—´å˜æ¢æ¨¡å—ï¼ˆBEVï¼‰ï¼Œé€šè¿‡å°†ä¸åŒæ—¶é—´ç‚¹çš„ç‰¹å¾ç»Ÿä¸€åˆ°BEVç©ºé—´æ¥è§£å†³å˜åŒ–æ£€æµ‹ä¸­çš„æ¨¡æ€ä¸åŒ¹é…é—®é¢˜ã€‚å¦å¤–ï¼Œæˆ‘ä»¬è¿˜æ„å»ºäº†é¦–ä¸ªç”¨äºæ— äººæœºè§†è§’çš„å»ºç­‘å˜åŒ–æ£€æµ‹æ•°æ®é›†ï¼ˆDVCDï¼‰ï¼Œåœ¨å››ä¸ªå¹¿æ³›ä½¿ç”¨çš„å˜åŒ–æ£€æµ‹æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼Œæˆ‘ä»¬çš„æ–¹æ³•å–å¾—äº†æ˜¾è‘—æ”¹è¿›ã€‚ç›¸å…³ä»£ç å’Œé¢„è®­ç»ƒæ¨¡å‹å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/Yu-Zhouz/SegChange-R1%E4%B8%AD%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/Yu-Zhouz/SegChange-R1ä¸­æ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.17944v1">PDF</a> </p>
<p><strong>Summary</strong><br>é¥æ„Ÿå˜åŒ–æ£€æµ‹å¹¿æ³›åº”ç”¨äºåŸå¸‚è§„åˆ’ã€åœ°å½¢åœ°è²Œåˆ†æã€ç¯å¢ƒç›‘æµ‹ç­‰é¢†åŸŸï¼Œä¸»è¦é€šè¿‡å¯¹åŒä¸€ç©ºé—´åŒºåŸŸä¸åŒæ—¶é—´é˜¶æ®µç‰¹å¾å˜åŒ–å·®å¼‚çš„åˆ†ææ¥å®ç°ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºå¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ¨ç†æ–¹æ³•ï¼ˆSegChange-R1ï¼‰ï¼Œé€šè¿‡é›†æˆæ–‡æœ¬æè¿°ä¿¡æ¯æé«˜æ£€æµ‹èƒ½åŠ›ï¼Œå¹¶è®¾è®¡äº†ä¸€ä¸ªåŸºäºçº¿æ€§æ³¨æ„åŠ›çš„ç©ºé—´å˜æ¢æ¨¡å—ï¼ˆBEVï¼‰ï¼Œè§£å†³äº†å˜åŒ–æ£€æµ‹ä¸­çš„æ¨¡æ€ä¸åŒ¹é…é—®é¢˜ã€‚æ­¤å¤–ï¼Œæœ¬æ–‡è¿˜æ„å»ºäº†é¦–ä¸ªæ— äººæœºè§†è§’çš„å»ºç­‘å˜åŒ–æ£€æµ‹æ•°æ®é›†ï¼ˆDVCDï¼‰ï¼Œå¹¶åœ¨å››ä¸ªå¹¿æ³›ä½¿ç”¨çš„å˜åŒ–æ£€æµ‹æ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨ç°æœ‰æ–¹æ³•çš„åŸºç¡€ä¸Šæœ‰æ˜¾è‘—æ”¹è¿›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>é¥æ„Ÿå˜åŒ–æ£€æµ‹å¹¿æ³›åº”ç”¨äºåŸå¸‚è§„åˆ’ã€åœ°å½¢åœ°è²Œåˆ†æã€ç¯å¢ƒç›‘æµ‹ç­‰é¢†åŸŸã€‚</li>
<li>æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºå¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ¨ç†æ–¹æ³•ï¼ˆSegChange-R1ï¼‰ï¼Œç”¨äºå¢å¼ºå˜åŒ–æ£€æµ‹èƒ½åŠ›ã€‚</li>
<li>SegChange-R1é€šè¿‡é›†æˆæ–‡æœ¬æè¿°ä¿¡æ¯ï¼Œèƒ½å¤ŸæŒ‡å¯¼æ¨¡å‹æ›´å…³æ³¨æ„Ÿå…´è¶£çš„å˜åŒ–åŒºåŸŸï¼Œå¹¶åŠ é€Ÿæ”¶æ•›é€Ÿåº¦ã€‚</li>
<li>è®¾è®¡äº†ä¸€ä¸ªåŸºäºçº¿æ€§æ³¨æ„åŠ›çš„ç©ºé—´å˜æ¢æ¨¡å—ï¼ˆBEVï¼‰ï¼Œè§£å†³äº†å˜åŒ–æ£€æµ‹ä¸­çš„æ¨¡æ€ä¸åŒ¹é…é—®é¢˜ã€‚</li>
<li>æ„å»ºäº†é¦–ä¸ªæ— äººæœºè§†è§’çš„å»ºç­‘å˜åŒ–æ£€æµ‹æ•°æ®é›†ï¼ˆDVCDï¼‰ã€‚</li>
<li>åœ¨å››ä¸ªå¹¿æ³›ä½¿ç”¨çš„å˜åŒ–æ£€æµ‹æ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼ŒSegChange-R1æ–¹æ³•è¾ƒç°æœ‰æ–¹æ³•æœ‰æ˜æ˜¾æ”¹è¿›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.17944">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-a385aba48c62e8c263c433625b9dd255.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c35b3343d4735cffa9f0ca24e497c5a1.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-b86c50109494b5d534c061b790f6d0ad.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-2cf24f70b89b4a2c22b43150cb9ce55c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-9665dbc1348a5f121b1d21d873fdcff5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0cb4f2b645a9041c8d4fa204993646b3.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-205e23f94dab19dfb0fa6844d38e1457.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="GEMeX-ThinkVG-Towards-Thinking-with-Visual-Grounding-in-Medical-VQA-via-Reinforcement-Learning"><a href="#GEMeX-ThinkVG-Towards-Thinking-with-Visual-Grounding-in-Medical-VQA-via-Reinforcement-Learning" class="headerlink" title="GEMeX-ThinkVG: Towards Thinking with Visual Grounding in Medical VQA via   Reinforcement Learning"></a>GEMeX-ThinkVG: Towards Thinking with Visual Grounding in Medical VQA via   Reinforcement Learning</h2><p><strong>Authors:Bo Liu, Xiangyu Zhao, Along He, Yidi Chen, Huazhu Fu, Xiao-Ming Wu</strong></p>
<p>Medical visual question answering aims to support clinical decision-making by enabling models to answer natural language questions based on medical images. While recent advances in multi-modal learning have significantly improved performance, current methods still suffer from limited answer reliability and poor interpretability, impairing the ability of clinicians and patients to understand and trust model-generated answers. To address this, this work first proposes a Thinking with Visual Grounding (ThinkVG) dataset wherein the answer generation is decomposed into intermediate reasoning steps that explicitly ground relevant visual regions of the medical image, thereby providing fine-grained explainability. Furthermore, we introduce a novel verifiable reward mechanism for reinforcement learning to guide post-training, improving the alignment between the modelâ€™s reasoning process and its final answer. Remarkably, our method achieves comparable performance using only one-eighth of the training data, demonstrating the efficiency and effectiveness of the proposal. The dataset is available at <a target="_blank" rel="noopener" href="https://huggingface.co/datasets/BoKelvin/GEMeX-ThinkVG">https://huggingface.co/datasets/BoKelvin/GEMeX-ThinkVG</a>. </p>
<blockquote>
<p>åŒ»ç–—è§†è§‰é—®ç­”æ—¨åœ¨é€šè¿‡ä½¿æ¨¡å‹èƒ½å¤Ÿæ ¹æ®åŒ»å­¦å›¾åƒå›ç­”è‡ªç„¶è¯­è¨€é—®é¢˜æ¥æ”¯æŒä¸´åºŠå†³ç­–ã€‚è™½ç„¶å¤šæ¨¡æ€å­¦ä¹ çš„æœ€æ–°è¿›å±•å·²ç»å¤§å¤§æé«˜äº†æ€§èƒ½ï¼Œä½†å½“å‰çš„æ–¹æ³•ä»ç„¶å­˜åœ¨ç€ç­”æ¡ˆå¯é æ€§æœ‰é™å’Œè§£é‡Šæ€§è¾ƒå·®çš„é—®é¢˜ï¼Œå½±å“ä¸´åºŠåŒ»ç”Ÿå’Œæ‚£è€…ç†è§£å’Œä¿¡ä»»æ¨¡å‹ç”Ÿæˆçš„ç­”æ¡ˆã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œè¿™é¡¹å·¥ä½œé¦–å…ˆæå‡ºäº†ä¸€ä¸ªåŸºäºè§†è§‰å®šä½çš„æ€è€ƒï¼ˆThinkVGï¼‰æ•°æ®é›†ï¼Œå…¶ä¸­ç­”æ¡ˆç”Ÿæˆè¢«åˆ†è§£ä¸ºæ˜ç¡®çš„ä¸­é—´æ¨ç†æ­¥éª¤ï¼Œè¿™äº›æ­¥éª¤æ˜ç¡®åœ°å°†åŒ»å­¦å›¾åƒçš„ç›¸å…³è§†è§‰åŒºåŸŸä½œä¸ºä¾æ®ï¼Œä»è€Œæä¾›ç²¾ç»†çš„è§£é‡Šæ€§ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§æ–°å‹çš„å¯éªŒè¯å¥–åŠ±æœºåˆ¶ï¼Œç”¨äºå¼ºåŒ–å­¦ä¹ ï¼Œä»¥æŒ‡å¯¼è®­ç»ƒåçš„è¿‡ç¨‹ï¼Œæé«˜æ¨¡å‹æ¨ç†è¿‡ç¨‹ä¸å…¶æœ€ç»ˆç­”æ¡ˆä¹‹é—´çš„å¯¹é½ç¨‹åº¦ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œæˆ‘ä»¬çš„æ–¹æ³•ä»…ä½¿ç”¨å…«åˆ†ä¹‹ä¸€çš„è®­ç»ƒæ•°æ®å°±è¾¾åˆ°äº†ç›¸å½“çš„æ€§èƒ½ï¼Œè¯æ˜äº†è¯¥ææ¡ˆçš„æ•ˆç‡å’Œæœ‰æ•ˆæ€§ã€‚æ•°æ®é›†å¯åœ¨<a target="_blank" rel="noopener" href="https://huggingface.co/datasets/BoKelvin/GEMeX-ThinkVG">https://huggingface.co/datasets/BoKelvin/GEMeX-ThinkVG</a>è·å¾—ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.17939v1">PDF</a> Work in Progress</p>
<p><strong>Summary</strong><br>åŒ»ç–—è§†è§‰é—®ç­”æ—¨åœ¨é€šè¿‡ä½¿æ¨¡å‹èƒ½å¤Ÿåœ¨åŒ»ç–—å›¾åƒä¸Šå›ç­”è‡ªç„¶è¯­è¨€é—®é¢˜æ¥æ”¯æŒä¸´åºŠå†³ç­–ã€‚å°½ç®¡å¤šæ¨¡æ€å­¦ä¹ çš„æœ€æ–°è¿›å±•å¤§å¤§æé«˜äº†æ€§èƒ½ï¼Œä½†å½“å‰çš„æ–¹æ³•ä»å­˜åœ¨ç­”æ¡ˆå¯é æ€§æœ‰é™å’Œè§£é‡Šæ€§å·®çš„ç¼ºç‚¹ï¼Œå½±å“ä¸´åºŠåŒ»ç”Ÿå’Œæ‚£è€…ç†è§£å’Œä¿¡ä»»æ¨¡å‹ç”Ÿæˆçš„ç­”æ¡ˆã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œæœ¬å·¥ä½œé¦–å…ˆæå‡ºä¸€ä¸ªåä¸ºâ€œè§†è§‰æ¨ç†æ€è€ƒâ€ï¼ˆThinkVGï¼‰çš„æ•°æ®é›†ï¼Œè¯¥æ•°æ®é›†ä¸­çš„ç­”æ¡ˆç”Ÿæˆè¢«åˆ†è§£ä¸ºä»¥åŒ»ç–—å›¾åƒçš„ç›¸å…³è§†è§‰åŒºåŸŸä¸ºåŸºç¡€çš„ä¸­é—´æ¨ç†æ­¥éª¤ï¼Œä»è€Œæä¾›äº†ç²¾ç»†çš„è§£é‡Šæ€§ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜å¼•å…¥äº†ä¸€ç§æ–°å‹çš„å¯éªŒè¯å¥–åŠ±æœºåˆ¶ç”¨äºå¼ºåŒ–å­¦ä¹ ï¼Œä»¥æŒ‡å¯¼è®­ç»ƒåè¿‡ç¨‹ï¼Œæé«˜æ¨¡å‹æ¨ç†è¿‡ç¨‹å’Œæœ€ç»ˆç­”æ¡ˆä¹‹é—´çš„å¯¹é½ç¨‹åº¦ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œæˆ‘ä»¬çš„æ–¹æ³•ä»…ä½¿ç”¨å…«åˆ†ä¹‹ä¸€çš„è®­ç»ƒæ•°æ®å°±è¾¾åˆ°äº†ç›¸å½“çš„æ€§èƒ½ï¼Œè¯æ˜äº†ææ¡ˆçš„æ•ˆç‡å’Œæœ‰æ•ˆæ€§ã€‚è¯¥æ•°æ®é›†å¯åœ¨<a target="_blank" rel="noopener" href="https://huggingface.co/datasets/BoKelvin/GEMex-ThinkVG%E8%8E%B7%E5%8F%96%E3%80%82">https://huggingface.co/datasets/BoKelvin/GEMex-ThinkVGè·å–ã€‚</a></p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>åŒ»ç–—è§†è§‰é—®ç­”æ—¨åœ¨æ”¯æŒä¸´åºŠå†³ç­–ï¼Œé€šè¿‡æ¨¡å‹å›ç­”åŸºäºåŒ»ç–—å›¾åƒçš„è‡ªç„¶è¯­è¨€é—®é¢˜ã€‚</li>
<li>å½“å‰æ–¹æ³•å­˜åœ¨ç­”æ¡ˆå¯é æ€§åŠè§£é‡Šæ€§ä¸è¶³çš„é—®é¢˜ã€‚</li>
<li>æå‡ºäº†â€œè§†è§‰æ¨ç†æ€è€ƒâ€ï¼ˆThinkVGï¼‰æ•°æ®é›†ï¼Œé€šè¿‡åˆ†è§£ç­”æ¡ˆç”Ÿæˆè¿‡ç¨‹ï¼Œæä¾›åŒ»ç–—å›¾åƒç›¸å…³è§†è§‰åŒºåŸŸçš„ç²¾ç»†è§£é‡Šã€‚</li>
<li>å¼•å…¥æ–°å‹å¯éªŒè¯å¥–åŠ±æœºåˆ¶ç”¨äºå¼ºåŒ–å­¦ä¹ ï¼Œæé«˜æ¨¡å‹æ¨ç†ä¸ç­”æ¡ˆçš„å¯¹é½ç¨‹åº¦ã€‚</li>
<li>æ–¹æ³•åœ¨ä»…ä½¿ç”¨å°‘é‡è®­ç»ƒæ•°æ®çš„æƒ…å†µä¸‹å®ç°äº†è‰¯å¥½çš„æ€§èƒ½ï¼Œæ˜¾ç¤ºå‡ºææ¡ˆçš„æ•ˆç‡å’Œæœ‰æ•ˆæ€§ã€‚</li>
<li>è¯¥æ•°æ®é›†å¯åœ¨ç‰¹å®šç½‘å€è·å–ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.17939">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-225c16183b8185ee2dfc9a8d6abb7957.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-f96a94050c1455a0f994e44daed8c1f0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f2b45c0e3f732096a88ade378a46d716.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b33591b32e3328ba1a50d9c4c3fa9fe9.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-08ac5f911831cf33ee028725828c971b.jpg" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="Learning-Reasoning-Refinement-A-Framework-for-Kahnemanâ€™s-Dual-System-Intelligence-in-GUI-Agents"><a href="#Learning-Reasoning-Refinement-A-Framework-for-Kahnemanâ€™s-Dual-System-Intelligence-in-GUI-Agents" class="headerlink" title="Learning, Reasoning, Refinement: A Framework for Kahnemanâ€™s Dual-System   Intelligence in GUI Agents"></a>Learning, Reasoning, Refinement: A Framework for Kahnemanâ€™s Dual-System   Intelligence in GUI Agents</h2><p><strong>Authors:Jinjie Wei, Jiyao Liu, Lihao Liu, Ming Hu, Junzhi Ning, Mingcheng Li, Weijie Yin, Junjun He, Xiao Liang, Chao Feng, Dingkang Yang</strong></p>
<p>Graphical User Interface (GUI) agents have made significant progress in automating digital tasks through the utilization of computer vision and language models. Nevertheless, existing agent systems encounter notable limitations. Firstly, they predominantly depend on trial and error decision making rather than progressive reasoning, thereby lacking the capability to learn and adapt from interactive encounters. Secondly, these systems are assessed using overly simplistic single step accuracy metrics, which do not adequately reflect the intricate nature of real world GUI interactions. In this paper, we present CogniGUI, a cognitive framework developed to overcome these limitations by enabling adaptive learning for GUI automation resembling human-like behavior. Inspired by Kahnemanâ€™s Dual Process Theory, our approach combines two main components: (1) an omni parser engine that conducts immediate hierarchical parsing of GUI elements through quick visual semantic analysis to identify actionable components, and (2) a Group based Relative Policy Optimization (GRPO) grounding agent that assesses multiple interaction paths using a unique relative reward system, promoting minimal and efficient operational routes. This dual-system design facilitates iterative â€˜â€™exploration learning masteryâ€™â€™ cycles, enabling the agent to enhance its strategies over time based on accumulated experience. Moreover, to assess the generalization and adaptability of agent systems, we introduce ScreenSeek, a comprehensive benchmark that includes multi application navigation, dynamic state transitions, and cross interface coherence, which are often overlooked challenges in current benchmarks. Experimental results demonstrate that CogniGUI surpasses state-of-the-art methods in both the current GUI grounding benchmarks and our newly proposed benchmark. </p>
<blockquote>
<p>å›¾å½¢ç”¨æˆ·ç•Œé¢ï¼ˆGUIï¼‰ä»£ç†åœ¨åˆ©ç”¨è®¡ç®—æœºè§†è§‰å’Œè¯­è¨€æ¨¡å‹è‡ªåŠ¨åŒ–æ•°å­—ä»»åŠ¡æ–¹é¢å–å¾—äº†æ˜¾è‘—è¿›å±•ã€‚ç„¶è€Œï¼Œç°æœ‰çš„ä»£ç†ç³»ç»Ÿä»ç„¶é¢ä¸´æ˜¾è‘—å±€é™ã€‚é¦–å…ˆï¼Œå®ƒä»¬ä¸»è¦ä¾èµ–äºè¯•é”™å†³ç­–åˆ¶å®šï¼Œè€Œéæ¸è¿›æ¨ç†ï¼Œå› æ­¤ç¼ºä¹ä»äº’åŠ¨ä¸­å­¦ä¹ å’Œé€‚åº”çš„èƒ½åŠ›ã€‚å…¶æ¬¡ï¼Œè¿™äº›ç³»ç»Ÿçš„è¯„ä¼°ä½¿ç”¨çš„æ˜¯è¿‡äºç®€å•çš„å•æ­¥ç²¾åº¦æŒ‡æ ‡ï¼Œå¹¶ä¸èƒ½å……åˆ†åæ˜ ç°å®ä¸–ç•ŒGUIäº¤äº’çš„å¤æ‚æ€§ã€‚</p>
</blockquote>
<p>åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†CogniGUIï¼Œè¿™æ˜¯ä¸€ä¸ªè®¤çŸ¥æ¡†æ¶ï¼Œæ—¨åœ¨é€šè¿‡å®ç°GUIè‡ªåŠ¨åŒ–çš„è‡ªé€‚åº”å­¦ä¹ æ¥å…‹æœè¿™äº›é™åˆ¶ï¼Œæ¨¡ä»¿äººç±»è¡Œä¸ºã€‚å—åˆ°Kahnemançš„åŒé‡è¿‡ç¨‹ç†è®ºçš„å¯å‘ï¼Œæˆ‘ä»¬çš„æ–¹æ³•ç»“åˆäº†ä¸¤ä¸ªä¸»è¦ç»„æˆéƒ¨åˆ†ï¼šï¼ˆ1ï¼‰ä¸€ä¸ªå…¨èƒ½è§£æå¼•æ“ï¼Œå®ƒé€šè¿‡å¯¹GUIå…ƒç´ è¿›è¡Œå³æ—¶åˆ†å±‚è§£æï¼Œå¿«é€Ÿè¿›è¡Œè§†è§‰è¯­ä¹‰åˆ†æï¼Œä»¥è¯†åˆ«å¯æ“ä½œç»„ä»¶ï¼›ï¼ˆ2ï¼‰ä¸€ä¸ªåŸºäºGroupçš„ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆGRPOï¼‰æ¥åœ°ä»£ç†ï¼Œå®ƒä½¿ç”¨ç‹¬ç‰¹çš„ç›¸å¯¹å¥–åŠ±ç³»ç»Ÿè¯„ä¼°å¤šä¸ªäº¤äº’è·¯å¾„ï¼Œä¿ƒè¿›æœ€å°åŒ–å’Œé«˜æ•ˆçš„æ“ä½œè·¯çº¿ã€‚è¿™ç§åŒç³»ç»Ÿè®¾è®¡ä¿ƒè¿›äº†è¿­ä»£å¼çš„â€œæ¢ç´¢å­¦ä¹ æŒæ¡â€å¾ªç¯ï¼Œä½¿ä»£ç†èƒ½å¤Ÿæ ¹æ®ç§¯ç´¯çš„ç»éªŒéšç€æ—¶é—´çš„æ¨ç§»å¢å¼ºå…¶ç­–ç•¥ã€‚</p>
<p>æ­¤å¤–ï¼Œä¸ºäº†è¯„ä¼°ä»£ç†ç³»ç»Ÿçš„é€šç”¨æ€§å’Œé€‚åº”æ€§ï¼Œæˆ‘ä»¬æ¨å‡ºäº†ScreenSeekï¼Œè¿™æ˜¯ä¸€ä¸ªå…¨é¢çš„åŸºå‡†æµ‹è¯•ï¼ŒåŒ…æ‹¬è·¨åº”ç”¨ç¨‹åºå¯¼èˆªã€åŠ¨æ€çŠ¶æ€è½¬æ¢å’Œè·¨ç•Œé¢ä¸€è‡´æ€§ï¼Œè¿™äº›éƒ½æ˜¯å½“å‰åŸºå‡†æµ‹è¯•ä¸­ç»å¸¸è¢«å¿½è§†çš„æŒ‘æˆ˜ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒCogniGUIåœ¨å½“å‰çš„GUIæ¥åœ°åŸºå‡†æµ‹è¯•å’Œæˆ‘ä»¬æ–°æå‡ºçš„åŸºå‡†æµ‹è¯•ä¸­å‡è¶…è¶Šäº†ç°æœ‰æ–¹æ³•ã€‚</p>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.17913v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>GUIè‡ªåŠ¨åŒ–ä»£ç†åœ¨åˆ©ç”¨è®¡ç®—æœºè§†è§‰å’Œè¯­è¨€æ¨¡å‹æ–¹é¢å–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œä½†ä»å­˜åœ¨ä¾èµ–è¯•é”™å†³ç­–è€Œéæ¨ç†èƒ½åŠ›çš„é—®é¢˜ï¼Œä»¥åŠè¯„ä¼°æŒ‡æ ‡è¿‡äºç®€å•çš„é—®é¢˜ã€‚æœ¬æ–‡æå‡ºCogniGUIè®¤çŸ¥æ¡†æ¶ï¼Œæ—¨åœ¨é€šè¿‡èå…¥è‡ªé€‚åº”å­¦ä¹ ï¼Œå…‹æœä¸Šè¿°å±€é™æ€§ï¼Œæ¨¡æ‹Ÿäººç±»è¡Œä¸ºï¼Œå¹¶å¯¹GUIè‡ªåŠ¨åŒ–è¿›è¡Œæ“ä½œä¼˜åŒ–ã€‚å®ƒé‡‡ç”¨åŸºäºåŒé‡è¿›ç¨‹ç†è®ºçš„åŒç³»ç»Ÿæœºåˆ¶ï¼Œé€šè¿‡å³æ—¶å±‚æ¬¡è§£æGUIå…ƒç´ ä¸ç›¸å¯¹å¥–åŠ±ç³»ç»Ÿè¯„ä¼°äº¤äº’è·¯å¾„ï¼Œä¿ƒè¿›ç­–ç•¥è¿­ä»£ä¼˜åŒ–ã€‚æ­¤å¤–ï¼Œæœ¬æ–‡è¿˜å¼•å…¥ScreenSeekç»¼åˆåŸºå‡†æµ‹è¯•å¹³å°ï¼Œä»¥è¯„ä¼°ä»£ç†ç³»ç»Ÿçš„æ³›åŒ–ä¸é€‚åº”æ€§ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒCogniGUIåœ¨ç°æœ‰GUIåŸºå‡†æµ‹è¯•åŠæ–°æå‡ºçš„æµ‹è¯•ä¸­å‡è¶…è¶Šç°æœ‰æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>GUIè‡ªåŠ¨åŒ–ä»£ç†è™½ç„¶æœ‰æ‰€è¿›æ­¥ï¼Œä½†ä»å—é™äºè¯•é”™å†³ç­–è€Œéæ¨ç†èƒ½åŠ›ã€‚</li>
<li>å½“å‰è¯„ä¼°æŒ‡æ ‡è¿‡äºç®€åŒ–ï¼Œæ— æ³•åæ˜ çœŸå®ä¸–ç•ŒGUIäº¤äº’çš„å¤æ‚æ€§ã€‚</li>
<li>CogniGUIè®¤çŸ¥æ¡†æ¶é€šè¿‡è‡ªé€‚åº”å­¦ä¹ æ¨¡æ‹Ÿäººç±»è¡Œä¸ºï¼Œä¼˜åŒ–GUIè‡ªåŠ¨åŒ–æ“ä½œã€‚</li>
<li>é‡‡ç”¨åŒé‡è¿›ç¨‹ç†è®ºçš„åŒç³»ç»Ÿæœºåˆ¶å®ç°å¿«é€Ÿè§†è§‰è¯­ä¹‰åˆ†æå’Œç›¸å¯¹å¥–åŠ±ç³»ç»Ÿè¯„ä¼°äº¤äº’è·¯å¾„ã€‚</li>
<li>CogniGUIå®ç°ç­–ç•¥è¿­ä»£ä¼˜åŒ–ï¼Œæé«˜ä»£ç†éšæ—¶é—´æ¨ç§»çš„ç­–ç•¥æ•ˆèƒ½ã€‚</li>
<li>ScreenSeekåŸºå‡†æµ‹è¯•å¹³å°æ¶µç›–å¤šåº”ç”¨å¯¼èˆªã€åŠ¨æ€çŠ¶æ€è½¬æ¢å’Œè·¨ç•Œé¢ä¸€è‡´æ€§ç­‰æŒ‘æˆ˜ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.17913">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-19bdf39e2286e3972f4287c51aaeaf88.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-99751cd8ca1a7e9b161f20c253a5c23a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f9ab0afc0f1aa754772c7678eae8d8ac.jpg" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="PhysUniBench-An-Undergraduate-Level-Physics-Reasoning-Benchmark-for-Multimodal-Models"><a href="#PhysUniBench-An-Undergraduate-Level-Physics-Reasoning-Benchmark-for-Multimodal-Models" class="headerlink" title="PhysUniBench: An Undergraduate-Level Physics Reasoning Benchmark for   Multimodal Models"></a>PhysUniBench: An Undergraduate-Level Physics Reasoning Benchmark for   Multimodal Models</h2><p><strong>Authors:Lintao Wang, Encheng Su, Jiaqi Liu, Pengze Li, Peng Xia, Jiabei Xiao, Wenlong Zhang, Xinnan Dai, Xi Chen, Yuan Meng, Mingyu Ding, Lei Bai, Wanli Ouyang, Shixiang Tang, Aoran Wang, Xinzhu Ma</strong></p>
<p>Physics problem-solving is a challenging domain for large AI models, requiring integration of conceptual understanding, mathematical reasoning, and interpretation of physical diagrams. Current evaluation methodologies show notable limitations in capturing the breadth and complexity of undergraduate-level physics, underscoring the need for more rigorous assessments. To this end, we present PhysUniBench, a large-scale multimodal benchmark designed to evaluate and improve the reasoning capabilities of multimodal large language models (MLLMs) specifically on undergraduate-level physics problems. PhysUniBench consists of 3,304 physics questions spanning 8 major sub-disciplines of physics, each accompanied by one visual diagrams. The benchmark includes both open-ended and multiple-choice questions, systematically curated and difficulty-rated through an iterative model-in-the-loop process. The benchmarkâ€™s construction involved a rigorous multi-stage process, including multiple roll-outs, expert-level evaluation, automated filtering of easily solved problems, and a nuanced difficulty grading system with five levels. Through extensive experiments, we observe that current state-of-the-art models encounter substantial challenges in physics reasoning. For example, GPT-4o mini achieves only about 34.2% accuracy in the proposed PhysUniBench. These results highlight that current MLLMs struggle with advanced physics reasoning, especially on multi-step problems and those requiring precise diagram interpretation. By providing a broad and rigorous assessment tool, PhysUniBench aims to drive progress in AI for Science, encouraging the development of models with stronger physical reasoning, problem-solving skills, and multimodal understanding. The benchmark and evaluation scripts are available at <a target="_blank" rel="noopener" href="https://prismax-team.github.io/PhysUniBenchmark/">https://prismax-team.github.io/PhysUniBenchmark/</a>. </p>
<blockquote>
<p>ç‰©ç†é—®é¢˜è§£å†³æ˜¯ä¸€ä¸ªå¯¹å¤§å‹AIæ¨¡å‹å…·æœ‰æŒ‘æˆ˜æ€§çš„é¢†åŸŸï¼Œè¦æ±‚èåˆæ¦‚å¿µç†è§£ã€æ•°å­¦æ¨ç†å’Œç‰©ç†å›¾è¡¨è§£è¯»ã€‚ç°æœ‰çš„è¯„ä¼°æ–¹æ³•åœ¨æ•æ‰æœ¬ç§‘ç‰©ç†çš„å¹¿åº¦å’Œå¤æ‚æ€§æ–¹é¢å­˜åœ¨æ˜æ˜¾å±€é™ï¼Œè¿™å¼ºè°ƒäº†éœ€è¦è¿›è¡Œæ›´ä¸¥æ ¼è¯„ä¼°çš„å¿…è¦æ€§ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æ¨å‡ºäº†PhysUniBenchï¼Œè¿™æ˜¯ä¸€ä¸ªå¤§è§„æ¨¡çš„å¤šæ¨¡å¼åŸºå‡†æµ‹è¯•ï¼Œæ—¨åœ¨è¯„ä¼°å’Œæé«˜å¤šæ¨¡å¼å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨æœ¬ç§‘ç‰©ç†é—®é¢˜ä¸Šçš„æ¨ç†èƒ½åŠ›ã€‚PhysUniBenchåŒ…å«3304ä¸ªç‰©ç†é—®é¢˜ï¼Œæ¶µç›–ç‰©ç†å­¦çš„8ä¸ªä¸»è¦å­å­¦ç§‘ï¼Œæ¯ä¸ªé—®é¢˜éƒ½é…æœ‰ä¸€ä¸ªè§†è§‰å›¾è¡¨ã€‚è¯¥åŸºå‡†æµ‹è¯•åŒ…æ‹¬å¼€æ”¾å¼å’Œé€‰æ‹©é¢˜ï¼Œé€šè¿‡å¾ªç¯æ¨¡å‹è¿‡ç¨‹è¿›è¡Œç³»ç»Ÿæ€§ç­›é€‰å’Œéš¾åº¦è¯„çº§ã€‚åŸºå‡†æµ‹è¯•çš„æ„å»ºæ¶‰åŠä¸€ä¸ªä¸¥æ ¼çš„å¤šé˜¶æ®µè¿‡ç¨‹ï¼ŒåŒ…æ‹¬å¤šæ¬¡æ¨å‡ºã€ä¸“å®¶çº§è¯„ä¼°ã€è‡ªåŠ¨è¿‡æ»¤å®¹æ˜“è§£å†³çš„é—®é¢˜ä»¥åŠä¸€ä¸ªäº”çº§ç²¾ç»†çš„éš¾åº¦åˆ†çº§ç³»ç»Ÿã€‚é€šè¿‡å¹¿æ³›å®éªŒï¼Œæˆ‘ä»¬å‘ç°æœ€å…ˆè¿›æ¨¡å‹åœ¨ç‰©ç†æ¨ç†æ–¹é¢é‡åˆ°ç›¸å½“å¤§çš„æŒ‘æˆ˜ã€‚ä¾‹å¦‚ï¼ŒGPT-4o miniåœ¨æå‡ºçš„PhysUniBenchä¸Šä»…è¾¾åˆ°çº¦34.2%çš„å‡†ç¡®ç‡ã€‚è¿™äº›ç»“æœå¼ºè°ƒï¼Œå½“å‰MLLMåœ¨å¤„ç†é«˜çº§ç‰©ç†æ¨ç†æ–¹é¢å­˜åœ¨å›°éš¾ï¼Œå°¤å…¶æ˜¯åœ¨å¤šæ­¥éª¤é—®é¢˜å’Œéœ€è¦ç²¾ç¡®å›¾è¡¨è§£è¯»çš„é—®é¢˜ä¸Šã€‚é€šè¿‡æä¾›å¹¿æ³›è€Œä¸¥æ ¼çš„è¯„ä¼°å·¥å…·ï¼ŒPhysUniBenchæ—¨åœ¨æ¨åŠ¨AIç§‘å­¦çš„å‘å±•ï¼Œé¼“åŠ±å¼€å‘å…·æœ‰æ›´å¼ºç‰©ç†æ¨ç†ã€é—®é¢˜è§£å†³èƒ½åŠ›å’Œå¤šæ¨¡å¼ç†è§£èƒ½åŠ›çš„æ¨¡å‹ã€‚åŸºå‡†æµ‹è¯•å’Œè¯„ä¼°è„šæœ¬å¯åœ¨<a target="_blank" rel="noopener" href="https://prismax-team.github.io/PhysUniBenchmark/%E8%8E%B7%E5%BE%97%E3%80%82">https://prismax-team.github.io/PhysUniBenchmark/è·å–ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.17667v1">PDF</a> </p>
<p><strong>Summary</strong><br>ç‰©ç†å­¦é¢†åŸŸçš„é—®é¢˜è§£å†³å¯¹äºå¤§å‹AIæ¨¡å‹æ¥è¯´æ˜¯ä¸€é¡¹æŒ‘æˆ˜ï¼Œéœ€è¦æ•´åˆæ¦‚å¿µç†è§£ã€æ•°å­¦æ¨ç†å’Œç‰©ç†å›¾è¡¨è§£è¯»èƒ½åŠ›ã€‚å½“å‰è¯„ä¼°æ–¹æ³•å­˜åœ¨æ˜¾è‘—å±€é™æ€§ï¼Œæ— æ³•å…¨é¢æ•æ‰æœ¬ç§‘ç‰©ç†å­¦çŸ¥è¯†çš„æ·±åº¦å’Œå¹¿åº¦ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æ¨å‡ºPhysUniBenchï¼Œä¸€ä¸ªä¸“é—¨è¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹åœ¨æœ¬ç§‘ç‰©ç†å­¦é—®é¢˜ä¸Šçš„æ¨ç†èƒ½åŠ›çš„å¤§è§„æ¨¡å¤šæ¨¡å¼åŸºå‡†æµ‹è¯•ã€‚PhysUniBenchåŒ…å«3304ä¸ªç‰©ç†é—®é¢˜ï¼Œæ¶µç›–ç‰©ç†å­¦çš„å…«å¤§å­å­¦ç§‘ï¼Œæ¯ä¸ªé—®é¢˜éƒ½é™„æœ‰å¯è§†åŒ–å›¾è¡¨ã€‚è¯¥åŸºå‡†æµ‹è¯•åŒ…å«å¼€æ”¾å¼å’Œå¤šé¡¹é€‰æ‹©é¢˜ï¼Œé€šè¿‡è¿­ä»£æ¨¡å‹å¾ªç¯è¿‡ç¨‹è¿›è¡Œç³»ç»Ÿæ€§ç­›é€‰å’Œéš¾åº¦è¯„çº§ã€‚å½“å‰æœ€å…ˆè¿›çš„æ¨¡å‹åœ¨ç‰©ç†æ¨ç†æ–¹é¢é¢ä¸´å·¨å¤§æŒ‘æˆ˜ï¼Œä¾‹å¦‚åœ¨PhysUniBenchä¸ŠGPT-4o miniçš„å‡†ç¡®ç‡ä»…ä¸ºçº¦34.2%ã€‚è¿™çªæ˜¾å‡ºå½“å‰çš„å¤§å‹è¯­è¨€æ¨¡å‹åœ¨é«˜çº§ç‰©ç†æ¨ç†æ–¹é¢çš„ä¸è¶³ï¼Œå°¤å…¶æ˜¯åœ¨å¤šæ­¥éª¤é—®é¢˜å’Œç²¾ç¡®å›¾è¡¨è§£è¯»æ–¹é¢çš„æŒ‘æˆ˜ã€‚é€šè¿‡æä¾›å…¨é¢ä¸¥è°¨çš„è¯„ä¼°å·¥å…·ï¼ŒPhysUniBenchæ—¨åœ¨æ¨åŠ¨AIç§‘å­¦å‘å±•ï¼Œé¼“åŠ±å¼€å‘å…·æœ‰æ›´å¼ºç‰©ç†æ¨ç†èƒ½åŠ›ã€é—®é¢˜è§£å†³èƒ½åŠ›å’Œå¤šæ¨¡å¼ç†è§£èƒ½åŠ›çš„æ¨¡å‹ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç‰©ç†å­¦é—®é¢˜è§£å†³å¯¹äºå¤§å‹AIæ¨¡å‹æ˜¯ä¸€ä¸ªæŒ‘æˆ˜ï¼Œéœ€è¦æ•´åˆå¤šç§èƒ½åŠ›ã€‚</li>
<li>å½“å‰è¯„ä¼°æ–¹æ³•åœ¨æ•æ‰æœ¬ç§‘ç‰©ç†å­¦çŸ¥è¯†çš„æ·±åº¦å’Œå¹¿åº¦ä¸Šå­˜åœ¨å±€é™æ€§ã€‚</li>
<li>PhysUniBenchæ˜¯ä¸€ä¸ªå¤§è§„æ¨¡å¤šæ¨¡å¼åŸºå‡†æµ‹è¯•ï¼Œæ—¨åœ¨è¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹åœ¨æœ¬ç§‘ç‰©ç†å­¦é—®é¢˜ä¸Šçš„æ¨ç†èƒ½åŠ›ã€‚</li>
<li>PhysUniBenchåŒ…å«å¤šæ ·åŒ–ç‰©ç†é—®é¢˜ï¼Œæ¶µç›–å¤šä¸ªç‰©ç†å­å­¦ç§‘ï¼Œå¹¶é…æœ‰å¯è§†åŒ–å›¾è¡¨ã€‚</li>
<li>åŸºå‡†æµ‹è¯•åŒ…å«å¼€æ”¾å¼å’Œå¤šé¡¹é€‰æ‹©é¢˜ï¼Œç»è¿‡ç³»ç»Ÿæ€§ç­›é€‰å’Œéš¾åº¦è¯„çº§ã€‚</li>
<li>å½“å‰æœ€å…ˆè¿›çš„æ¨¡å‹åœ¨ç‰©ç†æ¨ç†æ–¹é¢é¢ä¸´æŒ‘æˆ˜ï¼Œå°¤å…¶åœ¨å¤šæ­¥éª¤å’Œå›¾è¡¨è§£è¯»æ–¹é¢ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.17667">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-fb77e3c66c0c774b12819f79e563f8c1.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-eb7340026ac7bbb2f2a6cb27ee24038a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-949a8efd8599be9582d46baa06cb3c44.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-d07ec680c46599e63294869925c8bbf6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-337334350740ea769d3d918c58b2e936.jpg" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1><h2 id="Answer-Centric-or-Reasoning-Driven-Uncovering-the-Latent-Memory-Anchor-in-LLMs"><a href="#Answer-Centric-or-Reasoning-Driven-Uncovering-the-Latent-Memory-Anchor-in-LLMs" class="headerlink" title="Answer-Centric or Reasoning-Driven? Uncovering the Latent Memory Anchor   in LLMs"></a>Answer-Centric or Reasoning-Driven? Uncovering the Latent Memory Anchor   in LLMs</h2><p><strong>Authors:Yang Wu, Yifan Zhang, Yiwei Wang, Yujun Cai, Yurong Wu, Yuran Wang, Ning Xu, Jian Cheng</strong></p>
<p>While Large Language Models (LLMs) demonstrate impressive reasoning capabilities, growing evidence suggests much of their success stems from memorized answer-reasoning patterns rather than genuine inference. In this work, we investigate a central question: are LLMs primarily anchored to final answers or to the textual pattern of reasoning chains? We propose a five-level answer-visibility prompt framework that systematically manipulates answer cues and probes model behavior through indirect, behavioral analysis. Experiments across state-of-the-art LLMs reveal a strong and consistent reliance on explicit answers. The performance drops by 26.90% when answer cues are masked, even with complete reasoning chains. These findings suggest that much of the reasoning exhibited by LLMs may reflect post-hoc rationalization rather than true inference, calling into question their inferential depth. Our study uncovers the answer-anchoring phenomenon with rigorous empirical validation and underscores the need for a more nuanced understanding of what constitutes reasoning in LLMs. </p>
<blockquote>
<p>è™½ç„¶å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å±•ç°å‡ºä»¤äººå°è±¡æ·±åˆ»çš„æ¨ç†èƒ½åŠ›ï¼Œä½†è¶Šæ¥è¶Šå¤šçš„è¯æ®è¡¨æ˜ï¼Œå®ƒä»¬çš„æˆåŠŸå¾ˆå¤§ç¨‹åº¦ä¸Šæºäºè®°å¿†ç­”æ¡ˆçš„æ¨ç†æ¨¡å¼ï¼Œè€ŒéçœŸæ­£çš„æ¨ç†ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æ¢è®¨äº†ä¸€ä¸ªæ ¸å¿ƒé—®é¢˜ï¼šLLMsä¸»è¦ä¾èµ–äºæœ€ç»ˆç­”æ¡ˆè¿˜æ˜¯æ¨ç†é“¾çš„æ–‡æœ¬æ¨¡å¼ï¼Ÿæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªäº”çº§çš„ç­”æ¡ˆå¯è§æ€§æç¤ºæ¡†æ¶ï¼Œè¯¥æ¡†æ¶ç³»ç»Ÿåœ°æ“ä½œç­”æ¡ˆçº¿ç´¢ï¼Œå¹¶é€šè¿‡é—´æ¥çš„è¡Œä¸ºåˆ†ææ¥æ¢æµ‹æ¨¡å‹è¡Œä¸ºã€‚åœ¨å°–ç«¯LLMsä¸Šè¿›è¡Œçš„å®éªŒæ˜¾ç¤ºäº†å¯¹æ˜ç¡®ç­”æ¡ˆçš„å¼ºçƒˆä¸”æŒç»­çš„ä¾èµ–ã€‚å½“ç­”æ¡ˆçº¿ç´¢è¢«æ©ç›–æ—¶ï¼Œå³ä½¿æ‹¥æœ‰å®Œæ•´çš„æ¨ç†é“¾ï¼Œæ€§èƒ½ä¹Ÿä¼šä¸‹é™26.90%ã€‚è¿™äº›å‘ç°è¡¨æ˜ï¼ŒLLMsæ‰€å±•ç°çš„æ¨ç†å¾ˆå¤§ä¸€éƒ¨åˆ†å¯èƒ½æ˜¯äº‹åæ¨ç†ï¼Œè€ŒéçœŸæ­£çš„æ¨ç†ï¼Œè¿™å¯¹å…¶æ¨ç†æ·±åº¦æå‡ºäº†è´¨ç–‘ã€‚æˆ‘ä»¬çš„ç ”ç©¶é€šè¿‡ä¸¥æ ¼çš„å®è¯éªŒè¯æ­ç¤ºäº†ç­”æ¡ˆé”šå®šç°è±¡ï¼Œå¹¶å¼ºè°ƒäº†å¯¹LLMsä¸­æ¨ç†æ„æˆçš„ç†è§£éœ€è¦æ›´åŠ å¾®å¦™ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.17630v1">PDF</a> 14 pages, 8 figures</p>
<p><strong>æ‘˜è¦</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å±•ç°å‡ºä»¤äººå°è±¡æ·±åˆ»çš„æ¨ç†èƒ½åŠ›ï¼Œä½†æœ‰è¶Šæ¥è¶Šå¤šçš„è¯æ®è¡¨æ˜ï¼Œå®ƒä»¬çš„æˆåŠŸå¾ˆå¤§ç¨‹åº¦ä¸Šæºäºè®°å¿†åŒ–çš„ç­”æ¡ˆæ¨ç†æ¨¡å¼ï¼Œè€ŒéçœŸæ­£çš„æ¨ç†ã€‚æœ¬ç ”ç©¶æ—¨åœ¨æ¢ç©¶ä¸€ä¸ªæ ¸å¿ƒé—®é¢˜ï¼šLLMsä¸»è¦ä¾èµ–äºæœ€ç»ˆç­”æ¡ˆè¿˜æ˜¯æ¨ç†é“¾çš„æ–‡æœ¬æ¨¡å¼ï¼Ÿæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªäº”å±‚æ¬¡çš„ç­”æ¡ˆå¯è§æ€§æç¤ºæ¡†æ¶ï¼Œé€šè¿‡é—´æ¥ã€è¡Œä¸ºåˆ†æçš„æ–¹å¼ï¼Œç³»ç»Ÿåœ°æ“ä½œç­”æ¡ˆçº¿ç´¢å¹¶æ¢ç©¶æ¨¡å‹è¡Œä¸ºã€‚å¯¹æœ€å…ˆè¿›çš„å¤§å‹è¯­è¨€æ¨¡å‹çš„å®éªŒè¡¨æ˜ï¼Œå®ƒä»¬å¯¹æ˜ç¡®ç­”æ¡ˆçš„ä¾èµ–æ€§å¼ºä¸”æŒç»­ç¨³å®šã€‚å½“ç­”æ¡ˆçº¿ç´¢è¢«æ©ç›–æ—¶ï¼Œå³ä½¿æ‹¥æœ‰å®Œæ•´çš„æ¨ç†é“¾ï¼Œå…¶æ€§èƒ½ä¹Ÿä¼šä¸‹é™26.90%ã€‚è¿™äº›å‘ç°è¡¨æ˜ï¼Œå¤§å‹è¯­è¨€æ¨¡å‹æ‰€å±•ç°çš„æ¨ç†èƒ½åŠ›å¯èƒ½æ›´å¤šæ˜¯å¯¹åéªŒç†æ€§çš„åæ˜ ï¼Œè€ŒéçœŸæ­£çš„æ¨ç†ï¼Œè¿™å¯¹å…¶æ¨ç†æ·±åº¦çš„è´¨ç–‘æå‡ºäº†è­¦ç¤ºã€‚æœ¬ç ”ç©¶é€šè¿‡ä¸¥æ ¼çš„å®è¯éªŒè¯æ­ç¤ºäº†ç­”æ¡ˆé”šå®šç°è±¡ï¼Œå¹¶å¼ºè°ƒäº†æ›´æ·±åˆ»ã€æ›´ç²¾ç»†åœ°ç†è§£å¤§å‹è¯­è¨€æ¨¡å‹ä¸­æ¨ç†çš„æ„æˆçš„å¿…è¦æ€§ã€‚</p>
<p><strong>è¦ç‚¹æ‘˜è¦</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰æˆåŠŸèƒŒåçš„å¾ˆå¤§ä¸€éƒ¨åˆ†æ¥è‡ªäºè®°å¿†åŒ–çš„ç­”æ¡ˆæ¨ç†æ¨¡å¼ï¼Œè€ŒéçœŸæ­£çš„æ¨ç†ã€‚</li>
<li>LLMså¯¹æ˜ç¡®ç­”æ¡ˆçš„ä¾èµ–æ€§å¼ºï¼Œå³ä½¿æ‹¥æœ‰å®Œæ•´çš„æ¨ç†é“¾ï¼Œå½“ç­”æ¡ˆçº¿ç´¢è¢«æ©ç›–æ—¶ï¼Œæ€§èƒ½ä¼šæ˜¾è‘—ä¸‹é™ã€‚</li>
<li>LLMsçš„æ¨ç†èƒ½åŠ›å¯èƒ½æ›´å¤šæ˜¯å¯¹åéªŒç†æ€§çš„åæ˜ ï¼Œè€ŒéçœŸæ­£çš„æ¨ç†ï¼Œè¿™å¯¹å…¶æ¨ç†æ·±åº¦æå‡ºäº†è´¨ç–‘ã€‚</li>
<li>æå‡ºäº”å±‚æ¬¡çš„ç­”æ¡ˆå¯è§æ€§æç¤ºæ¡†æ¶æ¥ç³»ç»Ÿåœ°æ¢ç©¶å¤§å‹è¯­è¨€æ¨¡å‹çš„è¡Œä¸ºã€‚</li>
<li>ç ”ç©¶é€šè¿‡ä¸¥æ ¼çš„å®è¯éªŒè¯æ­ç¤ºäº†ç­”æ¡ˆé”šå®šç°è±¡ã€‚</li>
<li>éœ€è¦æ›´æ·±åˆ»ã€æ›´ç²¾ç»†åœ°ç†è§£å¤§å‹è¯­è¨€æ¨¡å‹ä¸­æ¨ç†çš„æ„æˆã€‚</li>
<li>æœ¬ç ”ç©¶å¼ºè°ƒäº†ç†è§£å¤§å‹è¯­è¨€æ¨¡å‹å¦‚ä½•ç†è§£å’Œç”Ÿæˆæ–‡æœ¬çš„é‡è¦æ€§ï¼Œä»¥åŠå…¶åœ¨è‡ªç„¶è¯­è¨€å¤„ç†é¢†åŸŸä¸­çš„æ½œåœ¨åº”ç”¨ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.17630">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-cae8e581bcff46f2cf20aaa694e7f635.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-2945d1d2306f62e2cd4f596fe7eefcbf.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-b0ceca67e02499e8f73f97681bbad25f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9c05cd7b9af00a6a0f2a167d6a10f15b.jpg" align="middle">
</details>


<h1 id="-19"><a href="#-19" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-06-25/R1_Reasoning/">https://kedreamix.github.io/Talk2Paper/Paper/2025-06-25/R1_Reasoning/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/R1-Reasoning/">
                                    <span class="chip bg-color">R1_Reasoning</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-06-25/LLM/">
                    <div class="card-image">
                        
                        <img src="https://pica.zhimg.com/v2-713319f28adfe8dda6d102f914890a01.jpg" class="responsive-img" alt="LLM">
                        
                        <span class="card-title">LLM</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            LLM æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-06-25  Vision as a Dialect Unifying Visual Understanding and Generation via   Text-Aligned Representations
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-06-25
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/LLM/" class="post-category">
                                    LLM
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/LLM/">
                        <span class="chip bg-color">LLM</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-06-24/Talking%20Head%20Generation/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-6fbd86d57734cfefe8b9f85530004f6d.jpg" class="responsive-img" alt="Talking Head Generation">
                        
                        <span class="card-title">Talking Head Generation</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Talking Head Generation æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-06-24  Probe before You Talk Towards Black-box Defense against Backdoor   Unalignment for Large Language Models
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-06-24
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Talking-Head-Generation/" class="post-category">
                                    Talking Head Generation
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Talking-Head-Generation/">
                        <span class="chip bg-color">Talking Head Generation</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">27927k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
