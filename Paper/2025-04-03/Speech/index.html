<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Speech">
    <meta name="description" content="Speech æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-04-04  Foundations and Evaluations in NLP">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Speech | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-e24b088cf8479fb0ba985763cd641b37.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Speech</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Speech/">
                                <span class="chip bg-color">Speech</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Speech/" class="post-category">
                                Speech
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-04-04
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-05-14
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    18k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    74 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-04-04-æ›´æ–°"><a href="#2025-04-04-æ›´æ–°" class="headerlink" title="2025-04-04 æ›´æ–°"></a>2025-04-04 æ›´æ–°</h1><h2 id="Foundations-and-Evaluations-in-NLP"><a href="#Foundations-and-Evaluations-in-NLP" class="headerlink" title="Foundations and Evaluations in NLP"></a>Foundations and Evaluations in NLP</h2><p><strong>Authors:Jungyeul Park</strong></p>
<p>This memoir explores two fundamental aspects of Natural Language Processing (NLP): the creation of linguistic resources and the evaluation of NLP system performance. Over the past decade, my work has focused on developing a morpheme-based annotation scheme for the Korean language that captures linguistic properties from morphology to semantics. This approach has achieved state-of-the-art results in various NLP tasks, including part-of-speech tagging, dependency parsing, and named entity recognition. Additionally, this work provides a comprehensive analysis of segmentation granularity and its critical impact on NLP system performance. In parallel with linguistic resource development, I have proposed a novel evaluation framework, the jp-algorithm, which introduces an alignment-based method to address challenges in preprocessing tasks like tokenization and sentence boundary detection (SBD). Traditional evaluation methods assume identical tokenization and sentence lengths between gold standards and system outputs, limiting their applicability to real-world data. The jp-algorithm overcomes these limitations, enabling robust end-to-end evaluations across a variety of NLP tasks. It enhances accuracy and flexibility by incorporating linear-time alignment while preserving the complexity of traditional evaluation metrics. This memoir provides key insights into the processing of morphologically rich languages, such as Korean, while offering a generalizable framework for evaluating diverse end-to-end NLP systems. My contributions lay the foundation for future developments, with broader implications for multilingual resource development and system evaluation. </p>
<blockquote>
<p>è¿™ç¯‡å›å¿†å½•æ¢è®¨äº†è‡ªç„¶è¯­è¨€å¤„ç†ï¼ˆNLPï¼‰çš„ä¸¤ä¸ªåŸºæœ¬æ–¹é¢ï¼šè¯­è¨€èµ„æºçš„åˆ›å»ºå’ŒNLPç³»ç»Ÿæ€§èƒ½çš„è¯„ä»·ã€‚è¿‡å»åå¹´ï¼Œæˆ‘çš„å·¥ä½œä¸»è¦é›†ä¸­åœ¨ä¸ºéŸ©è¯­å¼€å‘ä¸€ç§åŸºäºè¯ç´ çš„æ ‡æ³¨æ–¹æ¡ˆï¼Œè¯¥æ–¹æ¡ˆèƒ½å¤Ÿæ•æ‰ä»å½¢æ€åˆ°è¯­ä¹‰çš„è¯­è¨€ç‰¹æ€§ã€‚è¯¥æ–¹æ³•åœ¨å„ç§NLPä»»åŠ¡ä¸­è¾¾åˆ°äº†æœ€æ–°æ°´å¹³ï¼ŒåŒ…æ‹¬è¯æ€§æ ‡æ³¨ã€ä¾å­˜è§£æå’Œå‘½åå®ä½“è¯†åˆ«ã€‚æ­¤å¤–ï¼Œè¿™é¡¹å·¥ä½œè¿˜å…¨é¢åˆ†æäº†åˆ†è¯ç²’åº¦åŠå…¶å¯¹NLPç³»ç»Ÿæ€§èƒ½çš„å…³é”®å½±å“ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.01342v1">PDF</a> </p>
<p><strong>Summary</strong>ï¼š<br>æœ¬å›å¿†å½•æ¢è®¨äº†è‡ªç„¶è¯­è¨€å¤„ç†ï¼ˆNLPï¼‰çš„ä¸¤ä¸ªåŸºæœ¬æ–¹é¢ï¼šè¯­è¨€èµ„æºçš„åˆ›å»ºå’ŒNLPç³»ç»Ÿæ€§èƒ½çš„è¯„ä»·ã€‚ä½œè€…ä»¥è¿‡å»åå¹´çš„å·¥ä½œç»éªŒä¸ºåŸºç¡€ï¼Œä»‹ç»äº†ä¸ºéŸ©è¯­å¼€å‘åŸºäºè¯­ç´ çš„æ³¨é‡Šæ–¹æ¡ˆï¼Œè¯¥æ–¹æ¡ˆåœ¨å¤šç§NLPä»»åŠ¡ä¸­å–å¾—äº†æœ€å…ˆè¿›çš„æˆæœï¼ŒåŒ…æ‹¬è¯æ€§æ ‡æ³¨ã€ä¾å­˜è§£æå’Œå‘½åå®ä½“è¯†åˆ«ã€‚åŒæ—¶ï¼Œä½œè€…è¿˜å…¨é¢åˆ†æäº†åˆ†è¯ç²’åº¦å¯¹NLPç³»ç»Ÿæ€§èƒ½çš„å…³é”®å½±å“ã€‚æ­¤å¤–ï¼Œä½œè€…æå‡ºäº†ä¸€ä¸ªæ–°é¢–çš„è¯„ä»·æ¡†æ¶jp-algorithmï¼Œè¯¥æ¡†æ¶å¼•å…¥äº†å¯¹é½æ–¹æ³•ï¼Œè§£å†³äº†é¢„å¤„ç†ä»»åŠ¡ä¸­çš„æŒ‘æˆ˜ï¼Œå¦‚åˆ†è¯å’Œå¥å­è¾¹ç•Œæ£€æµ‹ã€‚æ­¤å›å¿†å½•ä¸ºå¤„ç†å½¢æ€ä¸°å¯Œçš„è¯­è¨€ï¼ˆå¦‚éŸ©è¯­ï¼‰æä¾›äº†å…³é”®è§è§£ï¼Œå¹¶ä¸ºè¯„ä¼°å¤šç§ç«¯åˆ°ç«¯çš„NLPç³»ç»Ÿæä¾›äº†å¯æ¨å¹¿çš„æ¡†æ¶ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>ä½œè€…ä»‹ç»äº†å…¶éŸ©è¯­è¯­ç´ æ³¨é‡Šæ–¹æ¡ˆåŠå…¶åœ¨å¤šç§NLPä»»åŠ¡ä¸­çš„å“è¶Šè¡¨ç°ã€‚</li>
<li>åˆ†è¯ç²’åº¦å¯¹NLPç³»ç»Ÿæ€§èƒ½å…·æœ‰é‡è¦å½±å“ã€‚</li>
<li>ä½œè€…æå‡ºäº†jp-algorithmè¯„ä»·æ¡†æ¶ï¼Œè§£å†³äº†ä¼ ç»Ÿè¯„ä»·æ–¹æ³•çš„å±€é™æ€§ã€‚</li>
<li>jp-algorithmé€šè¿‡å¼•å…¥å¯¹é½æ–¹æ³•ï¼Œæé«˜äº†è¯„ä¼°çš„å‡†ç¡®æ€§å’Œçµæ´»æ€§ã€‚</li>
<li>æ­¤å›å¿†å½•ä¸ºå¤„ç†å½¢æ€ä¸°å¯Œçš„è¯­è¨€æä¾›äº†é‡è¦è§è§£ã€‚</li>
<li>ä½œè€…çš„è´¡çŒ®ä¸ºæœªæ¥å¤šè¯­è¨€èµ„æºå¼€å‘å’Œç³»ç»Ÿè¯„ä»·å¥ å®šäº†åŸºç¡€ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.01342">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-108fb837911aab3d37f497065f1e3ce7.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="SViQA-A-Unified-Speech-Vision-Multimodal-Model-for-Textless-Visual-Question-Answering"><a href="#SViQA-A-Unified-Speech-Vision-Multimodal-Model-for-Textless-Visual-Question-Answering" class="headerlink" title="SViQA: A Unified Speech-Vision Multimodal Model for Textless Visual   Question Answering"></a>SViQA: A Unified Speech-Vision Multimodal Model for Textless Visual   Question Answering</h2><p><strong>Authors:Bingxin Li</strong></p>
<p>Multimodal models integrating speech and vision hold significant potential for advancing human-computer interaction, particularly in Speech-Based Visual Question Answering (SBVQA) where spoken questions about images require direct audio-visual understanding. Existing approaches predominantly focus on text-visual integration, leaving speech-visual modality gaps underexplored due to their inherent heterogeneity. To this end, we introduce SViQA, a unified speech-vision model that directly processes spoken questions without text transcription. Building upon the LLaVA architecture, our framework bridges auditory and visual modalities through two key innovations: (1) end-to-end speech feature extraction eliminating intermediate text conversion, and (2) cross-modal alignment optimization enabling effective fusion of speech signals with visual content. Extensive experimental results on the SBVQA benchmark demonstrate the proposed SViQAâ€™s state-of-the-art performance, achieving 75.62% accuracy, and competitive multimodal generalization. Leveraging speech-text mixed input boosts performance to 78.85%, a 3.23% improvement over pure speech input, highlighting SViQAâ€™s enhanced robustness and effective cross-modal attention alignment. </p>
<blockquote>
<p>å¤šæ¨¡æ€æ¨¡å‹é›†æˆäº†è¯­éŸ³å’Œè§†è§‰ï¼Œåœ¨æ¨è¿›äººæœºäº¤äº’æ–¹é¢æ‹¥æœ‰å·¨å¤§æ½œåŠ›ï¼Œç‰¹åˆ«æ˜¯åœ¨åŸºäºè¯­éŸ³çš„è§†è§‰é—®ç­”ï¼ˆSBVQAï¼‰ä¸­ï¼Œå…³äºå›¾åƒçš„å£è¯­é—®é¢˜éœ€è¦ç›´æ¥éŸ³è§†é¢‘çš„æ„ŸçŸ¥ã€‚ç°æœ‰æ–¹æ³•ä¸»è¦é›†ä¸­åœ¨æ–‡æœ¬è§†è§‰é›†æˆä¸Šï¼Œç”±äºå†…åœ¨å¼‚è´¨æ€§å¯¼è‡´è¯­éŸ³è§†è§‰æ¨¡æ€å·®è·å°šæœªå¾—åˆ°å……åˆ†æ¢ç´¢ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬å¼•å…¥äº†SViQAï¼Œè¿™æ˜¯ä¸€ä¸ªç»Ÿä¸€çš„è¯­éŸ³è§†è§‰æ¨¡å‹ï¼Œå¯ä»¥ç›´æ¥å¤„ç†å£è¯­é—®é¢˜è€Œæ— éœ€æ–‡æœ¬è½¬å½•ã€‚æˆ‘ä»¬çš„æ¡†æ¶å»ºç«‹åœ¨LLaVAæ¶æ„ä¹‹ä¸Šï¼Œé€šè¿‡ä¸¤ä¸ªå…³é”®åˆ›æ–°ç‚¹æ¡¥æ¢å¬è§‰å’Œè§†è§‰æ¨¡å¼ï¼šï¼ˆ1ï¼‰ç«¯åˆ°ç«¯çš„è¯­éŸ³ç‰¹å¾æå–æ¶ˆé™¤äº†ä¸­é—´æ–‡æœ¬è½¬æ¢ï¼Œä»¥åŠï¼ˆ2ï¼‰è·¨æ¨¡æ€å¯¹é½ä¼˜åŒ–ï¼Œä½¿è¯­éŸ³ä¿¡å·ä¸è§†è§‰å†…å®¹çš„èåˆæœ‰æ•ˆã€‚åœ¨SBVQAåŸºå‡†æµ‹è¯•ä¸Šçš„å¤§é‡å®éªŒç»“æœè¡¨æ˜ï¼Œæ‰€æå‡ºçš„SViQAå…·æœ‰æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œè¾¾åˆ°äº†75.62ï¼…çš„å‡†ç¡®ç‡ï¼Œå¹¶ä¸”åœ¨å¤šæ¨¡æ€æ¨å¹¿æ–¹é¢å…·æœ‰ç«äº‰åŠ›ã€‚åˆ©ç”¨è¯­éŸ³æ–‡æœ¬æ··åˆè¾“å…¥å°†æ€§èƒ½æé«˜åˆ°78.85ï¼…ï¼Œç›¸è¾ƒäºçº¯è¯­éŸ³è¾“å…¥æé«˜äº†3.23ï¼…ï¼Œçªæ˜¾äº†SViQAå¢å¼ºçš„ç¨³å¥æ€§å’Œæœ‰æ•ˆçš„è·¨æ¨¡æ€æ³¨æ„åŠ›å¯¹é½ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.01049v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†SViQAæ¨¡å‹åœ¨å¤šæ¨¡æ€è¯­éŸ³è¯†åˆ«å’Œè§†è§‰é—®ç­”ï¼ˆSBVQAï¼‰æ–¹é¢çš„åº”ç”¨ã€‚è¯¥æ¨¡å‹ç›´æ¥å¤„ç†è¯­éŸ³é—®é¢˜ï¼Œæ— éœ€æ–‡æœ¬è½¬å½•ï¼Œé€šè¿‡ä¸¤ä¸ªå…³é”®åˆ›æ–°ç‚¹â€”â€”ç«¯åˆ°ç«¯çš„è¯­éŸ³ç‰¹å¾æå–å’Œè·¨æ¨¡æ€å¯¹é½ä¼˜åŒ–ï¼Œå®ç°äº†å¯¹è¯­éŸ³ä¿¡å·ä¸è§†è§‰å†…å®¹çš„èåˆã€‚å®éªŒç»“æœè¯æ˜äº†SViQAæ¨¡å‹åœ¨SBVQAä¸Šçš„å…ˆè¿›æ€§èƒ½å’Œå¯¹å¤šæ¨¡æ€æ¨å¹¿çš„ç«äº‰åŠ›ã€‚æ··åˆä½¿ç”¨è¯­éŸ³å’Œæ–‡å­—è¾“å…¥èƒ½è¿›ä¸€æ­¥æé«˜æ¨¡å‹æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>SViQAæ¨¡å‹å°†è¯­éŸ³è¯†åˆ«ä¸è§†è§‰æ•´åˆåœ¨ä¸€ä¸ªç»Ÿä¸€æ¡†æ¶å†…ï¼Œå¯ç›´æ¥å¤„ç†è¯­éŸ³é—®é¢˜è€Œæ— éœ€æ–‡æœ¬è½¬å½•ã€‚</li>
<li>é€šè¿‡ä¸¤ä¸ªå…³é”®åˆ›æ–°å®ç°äº†è¯­éŸ³ä¿¡å·ä¸è§†è§‰å†…å®¹çš„èåˆï¼šç«¯åˆ°ç«¯çš„è¯­éŸ³ç‰¹å¾æå–å’Œè·¨æ¨¡æ€å¯¹é½ä¼˜åŒ–ã€‚</li>
<li>å®éªŒç»“æœå±•ç¤ºäº†SViQAåœ¨SBVQAä¸Šçš„å“è¶Šæ€§èƒ½ï¼Œè¾¾åˆ°äº†75.62%çš„å‡†ç¡®ç‡ã€‚</li>
<li>å½“ä½¿ç”¨æ··åˆè¯­éŸ³å’Œæ–‡å­—è¾“å…¥æ—¶ï¼ŒSViQAçš„æ€§èƒ½æå‡åˆ°78.85%ï¼Œæ˜¾ç¤ºå‡ºå…¶åœ¨è·¨æ¨¡æ€æ³¨æ„åŠ›å¯¹é½æ–¹é¢çš„ä¼˜åŠ¿ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.01049">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-62c9aad66da13d70bac555643b32f4f8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-73e12d169cb6e03a83d5e9c041f18c99.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b05c027e650dd9890941ef645b6bf7e7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d401569fefb4da81f70bb4680fbf8509.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Whispering-Under-the-Eaves-Protecting-User-Privacy-Against-Commercial-and-LLM-powered-Automatic-Speech-Recognition-Systems"><a href="#Whispering-Under-the-Eaves-Protecting-User-Privacy-Against-Commercial-and-LLM-powered-Automatic-Speech-Recognition-Systems" class="headerlink" title="Whispering Under the Eaves: Protecting User Privacy Against Commercial   and LLM-powered Automatic Speech Recognition Systems"></a>Whispering Under the Eaves: Protecting User Privacy Against Commercial   and LLM-powered Automatic Speech Recognition Systems</h2><p><strong>Authors:Weifei Jin, Yuxin Cao, Junjie Su, Derui Wang, Yedi Zhang, Minhui Xue, Jie Hao, Jin Song Dong, Yixian Yang</strong></p>
<p>The widespread application of automatic speech recognition (ASR) supports large-scale voice surveillance, raising concerns about privacy among users. In this paper, we concentrate on using adversarial examples to mitigate unauthorized disclosure of speech privacy thwarted by potential eavesdroppers in speech communications. While audio adversarial examples have demonstrated the capability to mislead ASR models or evade ASR surveillance, they are typically constructed through time-intensive offline optimization, restricting their practicality in real-time voice communication. Recent work overcame this limitation by generating universal adversarial perturbations (UAPs) and enhancing their transferability for black-box scenarios. However, they introduced excessive noise that significantly degrades audio quality and affects human perception, thereby limiting their effectiveness in practical scenarios. To address this limitation and protect live usersâ€™ speech against ASR systems, we propose a novel framework, AudioShield. Central to this framework is the concept of Transferable Universal Adversarial Perturbations in the Latent Space (LS-TUAP). By transferring the perturbations to the latent space, the audio quality is preserved to a large extent. Additionally, we propose target feature adaptation to enhance the transferability of UAPs by embedding target text features into the perturbations. Comprehensive evaluation on four commercial ASR APIs (Google, Amazon, iFlytek, and Alibaba), three voice assistants, two LLM-powered ASR and one NN-based ASR demonstrates the protection superiority of AudioShield over existing competitors, and both objective and subjective evaluations indicate that AudioShield significantly improves the audio quality. Moreover, AudioShield also shows high effectiveness in real-time end-to-end scenarios, and demonstrates strong resilience against adaptive countermeasures. </p>
<blockquote>
<p>è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰çš„å¹¿æ³›åº”ç”¨æ”¯æŒå¤§è§„æ¨¡è¯­éŸ³ç›‘æ§ï¼Œå¼•å‘äº†ç”¨æˆ·å¯¹äºéšç§çš„æ‹…å¿§ã€‚æœ¬æ–‡ä¸“æ³¨äºä½¿ç”¨å¯¹æŠ—æ€§æ ·æœ¬å‡è½»è¯­éŸ³é€šä¿¡ä¸­æ½œåœ¨çªƒå¬è€…æ“…è‡ªæ³„éœ²è¯­éŸ³éšç§çš„é—®é¢˜ã€‚è™½ç„¶éŸ³é¢‘å¯¹æŠ—æ€§æ ·æœ¬å·²å±•ç°å‡ºå¹²æ‰°ASRæ¨¡å‹æˆ–èº²é¿ASRç›‘æ§çš„èƒ½åŠ›ï¼Œä½†å®ƒä»¬é€šå¸¸æ˜¯é€šè¿‡è€—æ—¶çš„ç¦»çº¿ä¼˜åŒ–æ„å»ºçš„ï¼Œè¿™é™åˆ¶äº†å®ƒä»¬åœ¨å®æ—¶è¯­éŸ³é€šä¿¡ä¸­çš„å®ç”¨æ€§ã€‚æœ€è¿‘çš„å·¥ä½œé€šè¿‡ç”Ÿæˆé€šç”¨å¯¹æŠ—æ€§æ‰°åŠ¨ï¼ˆUAPsï¼‰å¹¶æé«˜å…¶é’ˆå¯¹é»‘ç®±åœºæ™¯çš„è¿ç§»æ€§æ¥å…‹æœè¿™ä¸€é™åˆ¶ã€‚ç„¶è€Œï¼Œå®ƒä»¬å¼•å…¥äº†è¿‡å¤šçš„å™ªå£°ï¼Œæ˜¾è‘—é™ä½äº†éŸ³é¢‘è´¨é‡å¹¶å½±å“äº†äººç±»æ„ŸçŸ¥ï¼Œä»è€Œé™åˆ¶äº†å®ƒä»¬åœ¨å®é™…æƒ…å†µä¸­çš„æœ‰æ•ˆæ€§ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é™åˆ¶å¹¶ä¿æŠ¤å®æ—¶ç”¨æˆ·çš„è¯­éŸ³å…å—ASRç³»ç»Ÿçš„å½±å“ï¼Œæˆ‘ä»¬æå‡ºäº†æ–°å‹æ¡†æ¶AudioShieldã€‚è¯¥æ¡†æ¶çš„æ ¸å¿ƒæ¦‚å¿µæ˜¯æ½œåœ¨ç©ºé—´ä¸­çš„å¯è¿ç§»é€šç”¨å¯¹æŠ—æ€§æ‰°åŠ¨ï¼ˆLS-TUAPï¼‰ã€‚é€šè¿‡å°†æ‰°åŠ¨è½¬ç§»åˆ°æ½œåœ¨ç©ºé—´ï¼Œå¯ä»¥åœ¨å¾ˆå¤§ç¨‹åº¦ä¸Šä¿ç•™éŸ³é¢‘è´¨é‡ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬é€šè¿‡å°†ç›®æ ‡æ–‡æœ¬ç‰¹å¾åµŒå…¥åˆ°æ‰°åŠ¨ä¸­ï¼Œæå‡ºäº†ç›®æ ‡ç‰¹å¾é€‚åº”æ–¹æ³•æ¥å¢å¼ºUAPsçš„è¿ç§»æ€§ã€‚å¯¹å››ä¸ªå•†ä¸šASR APIï¼ˆGoogleã€Amazonã€iFlytekå’ŒAlibabaï¼‰ã€ä¸‰ä¸ªè¯­éŸ³åŠ©æ‰‹ã€ä¸¤ä¸ªåŸºäºå¤§å‹è¯­è¨€æ¨¡å‹çš„ASRå’Œä¸€ä¸ªåŸºäºç¥ç»ç½‘ç»œçš„ASRçš„å…¨é¢è¯„ä¼°è¡¨æ˜ï¼ŒAudioShieldçš„ä¿æŠ¤æ€§èƒ½ä¼˜äºç°æœ‰ç«äº‰å¯¹æ‰‹ã€‚å®¢è§‚å’Œä¸»è§‚è¯„ä¼°å‡è¡¨æ˜ï¼ŒAudioShieldæ˜¾è‘—æé«˜äº†éŸ³é¢‘è´¨é‡ã€‚æ­¤å¤–ï¼ŒAudioShieldåœ¨å®æ—¶ç«¯åˆ°ç«¯åœºæ™¯ä¸­è¡¨ç°å‡ºé«˜æœ‰æ•ˆæ€§ï¼Œå¹¶å¯¹è‡ªé€‚åº”å¯¹ç­–è¡¨ç°å‡ºå¼ºçƒˆæŠ—æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.00858v1">PDF</a> Accept to USENIX Security 2025</p>
<p><strong>æ‘˜è¦</strong></p>
<p>æœ¬æ–‡ä¸»è¦å…³æ³¨è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰åœ¨è¯­éŸ³é€šä¿¡ä¸­å¯¹éšç§çš„æŒ‘æˆ˜ï¼Œå¹¶æå‡ºäº†ä¸€ç§æ–°å‹æ¡†æ¶AudioShieldï¼Œé€šè¿‡åˆ©ç”¨æ½œä¼ç©ºé—´ä¸­çš„å¯è½¬ç§»é€šç”¨å¯¹æŠ—æ‰°åŠ¨ï¼ˆLS-TUAPï¼‰æ¥ç”ŸæˆéŸ³é¢‘å¯¹æŠ—å®ä¾‹ä»¥ä¿æŠ¤ç”¨æˆ·å®æ—¶è¯­éŸ³å…å—ASRç³»ç»Ÿçš„ä¾µçŠ¯ã€‚è¯¥æ–¹æ³•ä¸ä»…ä¿æŠ¤äº†éŸ³é¢‘è´¨é‡ï¼Œè¿˜æé«˜äº†UAPçš„è¿ç§»æ€§ã€‚é€šè¿‡åµŒå…¥ç›®æ ‡æ–‡æœ¬ç‰¹å¾åˆ°æ‰°åŠ¨ä¸­ï¼Œè¿›ä¸€æ­¥å¢å¼ºäº†UAPçš„è¿ç§»æ€§ã€‚å¯¹ç°æœ‰å•†ä¸šASR APIå’Œè¯­éŸ³åŠ©æ‰‹ç³»ç»Ÿçš„å…¨é¢è¯„ä¼°è¡¨æ˜ï¼ŒAudioShieldçš„ä¿æŠ¤æ€§èƒ½ä¼˜äºç°æœ‰ç«å“ï¼Œå¹¶åœ¨éŸ³é¢‘è´¨é‡å’Œä¿æŠ¤èƒ½åŠ›æ–¹é¢å–å¾—æ˜¾è‘—è¿›æ­¥ã€‚åŒæ—¶ï¼ŒAudioShieldåœ¨å®é™…åœºæ™¯ä¸­è¡¨ç°è‰¯å¥½ï¼Œå¯¹è‡ªé€‚åº”å¯¹ç­–å…·æœ‰å¼ºå¤§çš„éŸ§æ€§ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰çš„å¤§è§„æ¨¡åº”ç”¨å¼•å‘äº†å…³äºè¯­éŸ³éšç§æ³„éœ²çš„æ‹…å¿§ã€‚</li>
<li>å¯¹æŠ—å®ä¾‹å¯ç”¨äºé˜²æ­¢æœªç»æˆæƒçš„è¯­éŸ³éšç§æ³„éœ²ã€‚</li>
<li>AudioShieldæ¡†æ¶ä½¿ç”¨æ½œä¼ç©ºé—´ä¸­çš„å¯è½¬ç§»é€šç”¨å¯¹æŠ—æ‰°åŠ¨ï¼ˆLS-TUAPï¼‰æ¥ä¿æŠ¤å®æ—¶è¯­éŸ³éšç§ã€‚</li>
<li>AudioShieldé€šè¿‡ä¿ç•™éŸ³é¢‘è´¨é‡å¹¶åµŒå…¥ç›®æ ‡æ–‡æœ¬ç‰¹å¾æ¥æé«˜UAPçš„è¿ç§»æ€§ã€‚</li>
<li>AudioShieldåœ¨å¤šä¸ªASRç³»ç»Ÿå’Œè¯­éŸ³åŠ©æ‰‹ä¸Šçš„è¯„ä¼°è¡¨ç°ä¼˜äºå…¶ä»–ç«äº‰å¯¹æ‰‹ã€‚</li>
<li>AudioShieldæ—¢æœ‰æ•ˆä¿æŠ¤äº†éŸ³é¢‘è´¨é‡ï¼Œä¹Ÿæé«˜äº†è¯­éŸ³éšç§ä¿æŠ¤èƒ½åŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.00858">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-ea065013ad624b3da560c292cfe65216.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2581b32977ebc1474dd259929a729cb7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-cc5236c833ac2961a59ffd9739a65332.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-51bb3a0bfe95c0aa95bac531c130c767.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-6987eaae202a39206274a80e650c8fae.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8443b3cfa60159c9df8aedc66037207b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3f97eae96a7ea56a4443c0b91c97c018.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="TeleAntiFraud-28k-An-Audio-Text-Slow-Thinking-Dataset-for-Telecom-Fraud-Detection"><a href="#TeleAntiFraud-28k-An-Audio-Text-Slow-Thinking-Dataset-for-Telecom-Fraud-Detection" class="headerlink" title="TeleAntiFraud-28k: An Audio-Text Slow-Thinking Dataset for Telecom Fraud   Detection"></a>TeleAntiFraud-28k: An Audio-Text Slow-Thinking Dataset for Telecom Fraud   Detection</h2><p><strong>Authors:Zhiming Ma, Peidong Wang, Minhua Huang, Jingpeng Wang, Kai Wu, Xiangzhao Lv, Yachun Pang, Yin Yang, Wenjie Tang, Yuchen Kang</strong></p>
<p>The detection of telecom fraud faces significant challenges due to the lack of high-quality multimodal training data that integrates audio signals with reasoning-oriented textual analysis. To address this gap, we present TeleAntiFraud-28k, the first open-source audio-text slow-thinking dataset specifically designed for automated telecom fraud analysis. Our dataset is constructed through three strategies: (1) Privacy-preserved text-truth sample generation using automatically speech recognition (ASR)-transcribed call recordings (with anonymized original audio), ensuring real-world consistency through text-to-speech (TTS) model regeneration; (2) Semantic enhancement via large language model (LLM)-based self-instruction sampling on authentic ASR outputs to expand scenario coverage; (3) Multi-agent adversarial synthesis that simulates emerging fraud tactics through predefined communication scenarios and fraud typologies. The generated dataset contains 28,511 rigorously processed speech-text pairs, complete with detailed annotations for fraud reasoning. The dataset is divided into three tasks: scenario classification, fraud detection, fraud type classification. Furthermore, we construct TeleAntiFraud-Bench, a standardized evaluation benchmark comprising proportionally sampled instances from the dataset, to facilitate systematic testing of model performance on telecom fraud detection tasks. We also contribute a production-optimized supervised fine-tuning (SFT) model trained on hybrid real&#x2F;synthetic data, while open-sourcing the data processing framework to enable community-driven dataset expansion. This work establishes a foundational framework for multimodal anti-fraud research while addressing critical challenges in data privacy and scenario diversity. The project will be released at <a target="_blank" rel="noopener" href="https://github.com/JimmyMa99/TeleAntiFraud">https://github.com/JimmyMa99/TeleAntiFraud</a>. </p>
<blockquote>
<p>ç”µä¿¡æ¬ºè¯ˆæ£€æµ‹é¢ä¸´ç€å·¨å¤§çš„æŒ‘æˆ˜ï¼Œä¸»è¦æ˜¯ç”±äºç¼ºä¹é«˜è´¨é‡çš„å¤šæ¨¡å¼è®­ç»ƒæ•°æ®ï¼Œæ— æ³•å°†éŸ³é¢‘ä¿¡å·ä¸é¢å‘æ¨ç†çš„æ–‡æœ¬åˆ†æç›¸ç»“åˆã€‚ä¸ºäº†è§£å†³è¿™ä¸€ç©ºç™½ï¼Œæˆ‘ä»¬æ¨å‡ºäº†TeleAntiFraud-28kï¼Œè¿™æ˜¯ä¸“é—¨ä¸ºç”µä¿¡æ¬ºè¯ˆè‡ªåŠ¨åŒ–åˆ†æè®¾è®¡çš„ç¬¬ä¸€ä¸ªå¼€æºéŸ³é¢‘æ–‡æœ¬æ…¢æ€è€ƒæ•°æ®é›†ã€‚æˆ‘ä»¬çš„æ•°æ®é›†é€šè¿‡ä»¥ä¸‹ä¸‰ç§ç­–ç•¥æ„å»ºï¼šï¼ˆ1ï¼‰ä½¿ç”¨è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰è½¬å½•çš„é€šè¯å½•éŸ³ç”Ÿæˆéšç§ä¿æŠ¤æ–‡æœ¬çœŸå®æ ·æœ¬ï¼ˆå¸¦æœ‰åŒ¿ååŸå§‹éŸ³é¢‘ï¼‰ï¼Œå¹¶é€šè¿‡æ–‡æœ¬åˆ°è¯­éŸ³ï¼ˆTTSï¼‰æ¨¡å‹å†ç”Ÿç¡®ä¿ç°å®ä¸–ç•Œçš„ä¸€è‡´æ€§ï¼›ï¼ˆ2ï¼‰é€šè¿‡åŸºäºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„è‡ªæˆ‘æŒ‡ä»¤é‡‡æ ·å¯¹çœŸå®çš„ASRè¾“å‡ºè¿›è¡Œè¯­ä¹‰å¢å¼ºï¼Œä»¥æ‰©å¤§åœºæ™¯è¦†ç›–ï¼›ï¼ˆ3ï¼‰æ¨¡æ‹Ÿæ–°å…´æ¬ºè¯ˆç­–ç•¥çš„å¤šä»£ç†å¯¹æŠ—åˆæˆï¼Œé€šè¿‡é¢„å®šçš„é€šä¿¡åœºæ™¯å’Œæ¬ºè¯ˆç±»å‹ã€‚ç”Ÿæˆçš„æ•°æ®é›†åŒ…å«ç»è¿‡ä¸¥æ ¼å¤„ç†çš„28511ä¸ªè¯­éŸ³æ–‡æœ¬å¯¹ï¼Œå¸¦æœ‰è¯¦ç»†çš„æ¬ºè¯ˆæ¨ç†æ³¨é‡Šã€‚æ•°æ®é›†åˆ†ä¸ºä¸‰ä¸ªä»»åŠ¡ï¼šåœºæ™¯åˆ†ç±»ã€æ¬ºè¯ˆæ£€æµ‹ã€æ¬ºè¯ˆç±»å‹åˆ†ç±»ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æ„å»ºäº†TeleAntiFraud-Benchï¼Œä¸€ä¸ªæ ‡å‡†åŒ–çš„è¯„ä¼°åŸºå‡†ï¼Œå…¶ä¸­åŒ…å«ä»æ•°æ®é›†ä¸­æŒ‰æ¯”ä¾‹é‡‡æ ·çš„å®ä¾‹ï¼Œä»¥ä¿ƒè¿›ç”µä¿¡æ¬ºè¯ˆæ£€æµ‹ä»»åŠ¡ä¸Šæ¨¡å‹æ€§èƒ½çš„ç³»ç»Ÿæµ‹è¯•ã€‚æˆ‘ä»¬è¿˜ä¸ºæ··åˆçœŸå®&#x2F;åˆæˆæ•°æ®è®­ç»ƒçš„ç”Ÿäº§ä¼˜åŒ–ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰æ¨¡å‹åšå‡ºäº†è´¡çŒ®ï¼ŒåŒæ—¶å¼€æºæ•°æ®å¤„ç†æ¡†æ¶ï¼Œä»¥å®ç°ç¤¾åŒºé©±åŠ¨çš„æ•°æ®é›†æ‰©å±•ã€‚è¿™é¡¹å·¥ä½œä¸ºå¤šåª’ä½“åæ¬ºè¯ˆç ”ç©¶å»ºç«‹äº†åŸºç¡€æ¡†æ¶ï¼ŒåŒæ—¶è§£å†³äº†æ•°æ®éšç§å’Œåœºæ™¯å¤šæ ·æ€§æ–¹é¢çš„å…³é”®æŒ‘æˆ˜ã€‚è¯¥é¡¹ç›®å°†åœ¨<a target="_blank" rel="noopener" href="https://github.com/JimmyMa99/TeleAntiFraud">https://github.com/JimmyMa99/TeleAntiFraud</a>å‘å¸ƒã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.24115v3">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>è¯¥æ–‡æœ¬ä»‹ç»äº†é’ˆå¯¹ç”µä¿¡æ¬ºè¯ˆæ£€æµ‹é¢†åŸŸæ‰€é¢ä¸´çš„æŒ‘æˆ˜ï¼Œæå‡ºäº†ä¸€ç§æ–°çš„å¼€æºéŸ³é¢‘æ–‡æœ¬æ…¢æ€è€ƒæ•°æ®é›†TeleAntiFraud-28kã€‚è¯¥æ•°æ®é›†é€šè¿‡ä¸‰ç§ç­–ç•¥æ„å»ºï¼ŒåŒ…æ‹¬éšç§ä¿æŠ¤æ–‡æœ¬çœŸå®æ ·æœ¬ç”Ÿæˆã€è¯­ä¹‰å¢å¼ºä»¥åŠå¤šä»£ç†å¯¹æŠ—åˆæˆã€‚æ•°æ®é›†åŒ…å«28,511ä¸ªç»è¿‡ä¸¥æ ¼å¤„ç†çš„è¯­éŸ³æ–‡æœ¬å¯¹ï¼Œå¸¦æœ‰è¯¦ç»†çš„æ¬ºè¯ˆæ¨ç†æ³¨é‡Šï¼Œåˆ†ä¸ºåœºæ™¯åˆ†ç±»ã€æ¬ºè¯ˆæ£€æµ‹å’Œæ¬ºè¯ˆç±»å‹åˆ†ç±»ä¸‰ä¸ªä»»åŠ¡ã€‚åŒæ—¶ï¼Œæ–‡ç« è¿˜ä»‹ç»äº†TeleAntiFraud-Benchè¯„ä¼°åŸºå‡†çš„æ„å»ºï¼Œä»¥åŠä½¿ç”¨æ··åˆçœŸå®&#x2F;åˆæˆæ•°æ®è®­ç»ƒçš„ä¼˜åŒ–ç›‘ç£å¾®è°ƒæ¨¡å‹ã€‚è¯¥ç ”ç©¶ä¸ºå¤šåª’ä½“æŠ—æ¬ºè¯ˆç ”ç©¶æä¾›äº†åŸºç¡€æ¡†æ¶ï¼Œå¹¶è§£å†³äº†æ•°æ®éšç§å’Œåœºæ™¯å¤šæ ·æ€§ç­‰å…³é”®æŒ‘æˆ˜ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>TeleAntiFraud-28kæ˜¯ä¸“é—¨ä¸ºç”µä¿¡æ¬ºè¯ˆåˆ†æè®¾è®¡çš„é¦–ä¸ªå¼€æºéŸ³é¢‘æ–‡æœ¬æ…¢æ€è€ƒæ•°æ®é›†ã€‚</li>
<li>æ•°æ®é›†é€šè¿‡éšç§ä¿æŠ¤çš„æ–‡æœ¬çœŸå®æ ·æœ¬ç”Ÿæˆã€è¯­ä¹‰å¢å¼ºå’Œå¤šä»£ç†å¯¹æŠ—åˆæˆä¸‰ç§ç­–ç•¥æ„å»ºã€‚</li>
<li>æ•°æ®é›†åŒ…å«28,511ä¸ªè¯­éŸ³æ–‡æœ¬å¯¹ï¼Œå¸¦æœ‰è¯¦ç»†çš„æ¬ºè¯ˆæ¨ç†æ³¨é‡Šï¼Œåˆ†ä¸ºåœºæ™¯åˆ†ç±»ã€æ¬ºè¯ˆæ£€æµ‹å’Œæ¬ºè¯ˆç±»å‹åˆ†ç±»ä¸‰ä¸ªä»»åŠ¡ã€‚</li>
<li>ä»‹ç»äº†TeleAntiFraud-Benchè¯„ä¼°åŸºå‡†çš„æ„å»ºï¼Œä»¥ç³»ç»ŸåŒ–æµ‹è¯•ç”µä¿¡æ¬ºè¯ˆæ£€æµ‹ä»»åŠ¡çš„æ¨¡å‹æ€§èƒ½ã€‚</li>
<li>å…¬å¼€äº†ä¸€ä¸ªä¼˜åŒ–ç›‘ç£å¾®è°ƒæ¨¡å‹ï¼Œè¯¥æ¨¡å‹ä½¿ç”¨æ··åˆçœŸå®&#x2F;åˆæˆæ•°æ®è¿›è¡Œè®­ç»ƒã€‚</li>
<li>ç ”ç©¶ä¸ºå¤šåª’ä½“æŠ—æ¬ºè¯ˆç ”ç©¶æä¾›äº†åŸºç¡€æ¡†æ¶ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.24115">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-e04604273ac37876ed94a141c9dfb05a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4ab7ba5582a063e68f5fe266c9f2818e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2ea373be85c803e7f199329c6e1006ce.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-82a5dba04352ae797e591d0d181ab5d8.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="UniSep-Universal-Target-Audio-Separation-with-Language-Models-at-Scale"><a href="#UniSep-Universal-Target-Audio-Separation-with-Language-Models-at-Scale" class="headerlink" title="UniSep: Universal Target Audio Separation with Language Models at Scale"></a>UniSep: Universal Target Audio Separation with Language Models at Scale</h2><p><strong>Authors:Yuanyuan Wang, Hangting Chen, Dongchao Yang, Weiqin Li, Dan Luo, Guangzhi Li, Shan Yang, Zhiyong Wu, Helen Meng, Xixin Wu</strong></p>
<p>We propose Universal target audio Separation (UniSep), addressing the separation task on arbitrary mixtures of different types of audio. Distinguished from previous studies, UniSep is performed on unlimited source domains and unlimited source numbers. We formulate the separation task as a sequence-to-sequence problem, and a large language model (LLM) is used to model the audio sequence in the discrete latent space, leveraging the power of LLM in handling complex mixture audios with large-scale data. Moreover, a novel pre-training strategy is proposed to utilize audio-only data, which reduces the efforts of large-scale data simulation and enhances the ability of LLMs to understand the consistency and correlation of information within audio sequences. We also demonstrate the effectiveness of scaling datasets in an audio separation task: we use large-scale data (36.5k hours), including speech, music, and sound, to train a universal target audio separation model that is not limited to a specific domain. Experiments show that UniSep achieves competitive subjective and objective evaluation results compared with single-task models. </p>
<blockquote>
<p>æˆ‘ä»¬æå‡ºäº†é€šç”¨ç›®æ ‡éŸ³é¢‘åˆ†ç¦»ï¼ˆUniSepï¼‰ï¼Œè§£å†³äº†ä¸åŒç±»å‹éŸ³é¢‘çš„ä»»æ„æ··åˆåˆ†ç¦»ä»»åŠ¡ã€‚ä¸ä»¥å‰çš„ç ”ç©¶ä¸åŒï¼ŒUniSepé€‚ç”¨äºæ— é™çš„æºåŸŸå’Œæ— é™çš„æºæ•°é‡ã€‚æˆ‘ä»¬å°†åˆ†ç¦»ä»»åŠ¡åˆ¶å®šä¸ºåºåˆ—åˆ°åºåˆ—çš„é—®é¢˜ï¼Œå¹¶ä½¿ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å¯¹ç¦»æ•£æ½œåœ¨ç©ºé—´ä¸­çš„éŸ³é¢‘åºåˆ—è¿›è¡Œå»ºæ¨¡ï¼Œåˆ©ç”¨LLMåœ¨å¤„ç†å¤æ‚æ··åˆéŸ³é¢‘å’Œå¤§è§„æ¨¡æ•°æ®æ–¹é¢çš„å¼ºå¤§åŠŸèƒ½ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„é¢„è®­ç»ƒç­–ç•¥ï¼Œåªä½¿ç”¨éŸ³é¢‘æ•°æ®ï¼Œè¿™å‡å°‘äº†å¤§è§„æ¨¡æ•°æ®æ¨¡æ‹Ÿçš„åŠªåŠ›ï¼Œå¢å¼ºäº†LLMç†è§£éŸ³é¢‘åºåˆ—å†…ä¿¡æ¯ä¸€è‡´æ€§å’Œå…³è”æ€§çš„èƒ½åŠ›ã€‚æˆ‘ä»¬è¿˜å±•ç¤ºäº†åœ¨éŸ³é¢‘åˆ†ç¦»ä»»åŠ¡ä¸­æ‰©å¤§æ•°æ®é›†çš„æœ‰æ•ˆæ€§ï¼šæˆ‘ä»¬ä½¿ç”¨å¤§è§„æ¨¡æ•°æ®ï¼ˆ36.5kå°æ—¶ï¼‰ï¼ŒåŒ…æ‹¬è¯­éŸ³ã€éŸ³ä¹å’Œå£°éŸ³ï¼Œæ¥è®­ç»ƒä¸€ä¸ªé€šç”¨çš„ç›®æ ‡éŸ³é¢‘åˆ†ç¦»æ¨¡å‹ï¼Œè¯¥æ¨¡å‹ä¸å—ç‰¹å®šé¢†åŸŸçš„é™åˆ¶ã€‚å®éªŒè¡¨æ˜ï¼Œä¸å•ä»»åŠ¡æ¨¡å‹ç›¸æ¯”ï¼ŒUniSepåœ¨ä¸»è§‚å’Œå®¢è§‚è¯„ä¼°æ–¹é¢éƒ½å–å¾—äº†å…·æœ‰ç«äº‰åŠ›çš„ç»“æœã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.23762v1">PDF</a> Accepted by ICME 2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†Universalç›®æ ‡éŸ³é¢‘åˆ†ç¦»ï¼ˆUniSepï¼‰æ–¹æ³•ï¼Œè§£å†³äº†ä¸åŒç±»å‹éŸ³é¢‘çš„ä»»æ„æ··åˆåˆ†ç¦»ä»»åŠ¡ã€‚è¯¥æ–¹æ³•åœ¨ä¸é™æºåŸŸå’Œä¸é™æºæ•°é‡çš„æƒ…å†µä¸‹æ‰§è¡Œåˆ†ç¦»ä»»åŠ¡ã€‚å°†åˆ†ç¦»ä»»åŠ¡å…¬å¼åŒ–ä¸ºåºåˆ—åˆ°åºåˆ—çš„é—®é¢˜ï¼Œå¹¶ä½¿ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨ç¦»æ•£æ½œåœ¨ç©ºé—´ä¸­å¯¹éŸ³é¢‘åºåˆ—è¿›è¡Œå»ºæ¨¡ã€‚æ­¤å¤–ï¼Œæå‡ºäº†ä¸€ç§æ–°çš„é¢„è®­ç»ƒç­–ç•¥ï¼Œåˆ©ç”¨çº¯éŸ³é¢‘æ•°æ®æ¥é™ä½å¤§è§„æ¨¡æ•°æ®æ¨¡æ‹Ÿçš„åŠªåŠ›å¹¶å¢å¼ºLLMå¯¹éŸ³é¢‘åºåˆ—ä¸­ä¿¡æ¯çš„ä¸€è‡´æ€§å’Œå…³è”æ€§çš„ç†è§£ã€‚å®éªŒè¯æ˜ï¼ŒUniSepåœ¨å¤šæ•°æ®é›†è®­ç»ƒä¸‹çš„æ•ˆæœå‡ºè‰²ï¼Œä¸å•ä»»åŠ¡æ¨¡å‹ç›¸æ¯”å–å¾—äº†æœ‰ç«äº‰åŠ›çš„ä¸»è§‚å’Œå®¢è§‚è¯„ä»·ç»“æœã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>UniSepè§£å†³äº†ä¸åŒç±»å‹éŸ³é¢‘çš„ä»»æ„æ··åˆåˆ†ç¦»ä»»åŠ¡ï¼Œé€‚ç”¨äºæ— é™æºåŸŸå’Œæ— é™æºæ•°é‡ã€‚</li>
<li>é‡‡ç”¨åºåˆ—åˆ°åºåˆ—çš„å…¬å¼åŒ–æ–¹æ³•å¤„ç†éŸ³é¢‘åˆ†ç¦»ä»»åŠ¡ã€‚</li>
<li>åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨ç¦»æ•£æ½œåœ¨ç©ºé—´ä¸­å¯¹éŸ³é¢‘åºåˆ—å»ºæ¨¡ã€‚</li>
<li>æå‡ºäº†ä¸€ç§æ–°çš„é¢„è®­ç»ƒç­–ç•¥ï¼Œåˆ©ç”¨çº¯éŸ³é¢‘æ•°æ®å¢å¼ºLLMå¯¹éŸ³é¢‘ä¿¡æ¯çš„ç†è§£ã€‚</li>
<li>é€šè¿‡ä½¿ç”¨å¤§è§„æ¨¡æ•°æ®ï¼ˆåŒ…æ‹¬è¯­éŸ³ã€éŸ³ä¹å’Œå£°éŸ³ï¼‰è®­ç»ƒé€šç”¨ç›®æ ‡éŸ³é¢‘åˆ†ç¦»æ¨¡å‹ï¼Œä¸å±€é™äºç‰¹å®šé¢†åŸŸã€‚</li>
<li>å®éªŒè¡¨æ˜ï¼ŒUniSepåœ¨ä¸»è§‚å’Œå®¢è§‚è¯„ä»·æ–¹é¢è¡¨ç°å‡ºç«äº‰åŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.23762">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-906a33dd3c5dc25e404638af57533e89.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-aad09c1c94dc6b847e1e242723e3ac6d.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-60a0885d2537774f98aca4a066a582eb.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-94d4ee35bc03aba88889cd755af37fd5.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-5e0ad4bedbcf20a2d78ecd6ae45acb69.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e07f5d7b540a65c37977d64698342755.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-83890fea6d048db7080ac35591cc57b8.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Whisper-LM-Improving-ASR-Models-with-Language-Models-for-Low-Resource-Languages"><a href="#Whisper-LM-Improving-ASR-Models-with-Language-Models-for-Low-Resource-Languages" class="headerlink" title="Whisper-LM: Improving ASR Models with Language Models for Low-Resource   Languages"></a>Whisper-LM: Improving ASR Models with Language Models for Low-Resource   Languages</h2><p><strong>Authors:Xabier de Zuazo, Eva Navas, Ibon Saratxaga, Inma HernÃ¡ez Rioja</strong></p>
<p>Automatic speech recognition systems have undoubtedly advanced with the integration of multilingual and multitask models such as Whisper, which have shown a promising ability to understand and process speech across a wide range of languages. Despite their robustness, these models often fall short in handling the linguistic distinctions of minority languages. This study addresses this gap by integrating traditional and novel language models with fine-tuned Whisper models to raise their performance in less commonly studied languages. Through rigorous fine-tuning and evaluation across multiple datasets, we demonstrate substantial improvements in word error rate, particularly in low-resource scenarios. Our approach not only does take advantage of the extensive data Whisper was pre-trained on, but also complements its linguistic adaptability by incorporating language models. We obtained improvements up to 51% for in-distribution datasets and up to 34% for out-of-distribution sentences using statistical language models, while large language models provided moderate but consistently robust improvement across diverse linguistic contexts. The findings reveal that, while the integration reliably benefits all model sizes, the extent of improvement varies, highlighting the importance of optimized language model parameters. Finally, we emphasize the importance of selecting appropriate evaluation parameters when reporting the results using transformer-based ASR models. In summary, this research clears the way for more inclusive ASR technologies that perform better across languages by enriching their linguistic knowledge. For further implementation details of this study, the technical documentation and source code are available at <a target="_blank" rel="noopener" href="http://www.github.com/hitz-zentroa/whisper-lm">http://www.github.com/hitz-zentroa/whisper-lm</a>. </p>
<blockquote>
<p>è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ç³»ç»Ÿæ— ç–‘å·²é€šè¿‡æ•´åˆå¦‚whisperç­‰å¤šè¯­è¨€å’Œå¤šä»»åŠ¡æ¨¡å‹ï¼Œåœ¨ç†è§£å’Œå¤„ç†å¹¿æ³›è¯­è¨€æ–¹é¢è¡¨ç°å‡ºäº†ä»¤äººç©ç›®çš„èƒ½åŠ›ã€‚å°½ç®¡è¿™äº›æ¨¡å‹å…·æœ‰ç¨³å¥æ€§ï¼Œä½†åœ¨å¤„ç†å°‘æ•°è¯­è¨€çš„è¯­è¨€å·®å¼‚æ–¹é¢å¾€å¾€è¡¨ç°ä¸è¶³ã€‚æœ¬ç ”ç©¶é€šè¿‡æ•´åˆä¼ ç»Ÿå’Œæ–°å‹è¯­è¨€æ¨¡å‹ä¸å¾®è°ƒè¿‡çš„whisperæ¨¡å‹ï¼Œæ¥è§£å†³è¿™ä¸€å·®è·ï¼Œä»¥æé«˜å…¶åœ¨è¾ƒå°‘ç ”ç©¶çš„è¯­è¨€ä¸­çš„æ€§èƒ½ã€‚é€šè¿‡å¤šä¸ªæ•°æ®é›†çš„ä¸¥æ ¼å¾®è°ƒä¸è¯„ä¼°ï¼Œæˆ‘ä»¬è¯æ˜äº†åœ¨å•è¯é”™è¯¯ç‡æ–¹é¢çš„æ˜¾è‘—æ”¹è¿›ï¼Œç‰¹åˆ«æ˜¯åœ¨èµ„æºåŒ®ä¹çš„åœºæ™¯ä¸‹ã€‚æˆ‘ä»¬çš„æ–¹æ³•ä¸ä»…åˆ©ç”¨äº†whisperé¢„è®­ç»ƒæ‰€ä¾èµ–çš„å¤§é‡æ•°æ®ï¼Œè€Œä¸”é€šè¿‡èå…¥è¯­è¨€æ¨¡å‹æ¥è¡¥å……å…¶è¯­è¨€é€‚åº”æ€§ã€‚æˆ‘ä»¬åœ¨ä½¿ç”¨ç»Ÿè®¡è¯­è¨€æ¨¡å‹çš„æƒ…å†µä¸‹ï¼Œå¯¹å†…éƒ¨æ•°æ®é›†æ”¹è¿›äº†é«˜è¾¾51%ï¼Œå¯¹å¤–éƒ¨å¥å­æ”¹è¿›äº†é«˜è¾¾32%ï¼Œè€Œå¤§å‹è¯­è¨€æ¨¡å‹åœ¨ä¸åŒçš„è¯­è¨€ç¯å¢ƒä¸­æä¾›äº†é€‚åº¦çš„ä½†å§‹ç»ˆç¨³å®šçš„æ”¹è¿›ã€‚ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œè™½ç„¶æ•´åˆå¯¹æ‰€æœ‰æ¨¡å‹è§„æ¨¡éƒ½æœ‰å¯é çš„å¥½å¤„ï¼Œä½†æ”¹è¿›ç¨‹åº¦æœ‰æ‰€ä¸åŒï¼Œè¿™å¼ºè°ƒäº†ä¼˜åŒ–è¯­è¨€æ¨¡å‹å‚æ•°çš„é‡è¦æ€§ã€‚æœ€åï¼Œæˆ‘ä»¬å¼ºè°ƒäº†åœ¨ä½¿ç”¨åŸºäºå˜å‹å™¨çš„ASRæ¨¡å‹æŠ¥å‘Šç»“æœæ—¶ï¼Œé€‰æ‹©é€‚å½“çš„è¯„ä¼°å‚æ•°çš„é‡è¦æ€§ã€‚æ€»ä¹‹ï¼Œæœ¬ç ”ç©¶ä¸ºå¼€å‘æ›´å…·åŒ…å®¹æ€§çš„ASRæŠ€æœ¯é“ºå¹³äº†é“è·¯ï¼Œè¿™äº›æŠ€æœ¯é€šè¿‡ä¸°å¯Œå…¶è¯­è¨€çŸ¥è¯†ï¼Œåœ¨è·¨è¯­è¨€æ–¹é¢è¡¨ç°æ›´å¥½ã€‚æœ‰å…³æœ¬ç ”ç©¶çš„è¿›ä¸€æ­¥å®æ–½ç»†èŠ‚ï¼ŒæŠ€æœ¯æ–‡æ¡£å’Œæºä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="http://www.github.com/hitz-zentroa/whisper-lm%E5%A4%84%E8%8E%B7%E5%BE%97%E3%80%82">http://www.github.com/hitz-zentroa/whisper-lmå¤„è·å¾—ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.23542v1">PDF</a> 26 pages, 6 figures, includes supplementary materials. Will be   submitted to IEEE&#x2F;ACM Transactions on Audio, Speech, and Language Processing</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ç ”ç©¶äº†è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ç³»ç»Ÿï¼ˆASRï¼‰åœ¨å¤„ç†å°‘æ•°è¯­è¨€æ—¶çš„å±€é™æ€§ï¼Œå¹¶æ¢è®¨äº†é€šè¿‡é›†æˆä¼ ç»Ÿå’Œæ–°å‹è¯­è¨€æ¨¡å‹æ¥æé«˜é¢„è®­ç»ƒwhisperæ¨¡å‹æ€§èƒ½çš„å¯è¡Œæ€§ã€‚ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œæ•´åˆæ¨¡å‹æ˜¾è‘—æé«˜äº†å•è¯é”™è¯¯ç‡ï¼Œç‰¹åˆ«æ˜¯åœ¨èµ„æºæœ‰é™çš„æƒ…å†µä¸‹ã€‚è¯¥ç ”ç©¶ä¸ºå¢å¼ºASRæŠ€æœ¯çš„è¯­è¨€é€‚åº”æ€§å¹¶æé«˜å…¶è·¨è¯­è¨€çš„æ€§èƒ½å¼€è¾Ÿäº†é“è·¯ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤šè¯­è¨€å¤šä»»åŠ¡æ¨¡å‹å¦‚Whisperèƒ½ç†è§£å’Œå¤„ç†å¤šç§è¯­è¨€çš„è¯­éŸ³ã€‚</li>
<li>åœ¨å¤„ç†å°‘æ•°è¯­è¨€æ—¶ï¼Œç°æœ‰æ¨¡å‹å¦‚Whisperå¯èƒ½å­˜åœ¨å±€é™æ€§ã€‚</li>
<li>é€šè¿‡æ•´åˆä¼ ç»Ÿå’Œæ–°å‹è¯­è¨€æ¨¡å‹ï¼Œå¯ä»¥æé«˜whisperæ¨¡å‹çš„æ€§èƒ½ã€‚</li>
<li>æ•´åˆæ¨¡å‹åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šçš„ä¸¥æ ¼ç²¾ç»†è°ƒæ•´å’Œè¯„ä»·æ˜¾ç¤ºï¼Œå•è¯é”™è¯¯ç‡æ˜¾è‘—æé«˜ã€‚</li>
<li>åœ¨èµ„æºæœ‰é™çš„æƒ…å†µä¸‹ï¼Œæ”¹è¿›å°¤ä¸ºæ˜¾è‘—ã€‚</li>
<li>å¤§å‹è¯­è¨€æ¨¡å‹åœ¨å¤šç§è¯­è¨€ç¯å¢ƒä¸‹æä¾›äº†ç¨³å¥ä½†é€‚ä¸­çš„æ”¹è¿›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.23542">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-6cda4dd8ac862a846cfde9d2ea59e7e8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4a25d179e6eccfb5a16eed4cb9d6f1f6.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Dual-Audio-Centric-Modality-Coupling-for-Talking-Head-Generation"><a href="#Dual-Audio-Centric-Modality-Coupling-for-Talking-Head-Generation" class="headerlink" title="Dual Audio-Centric Modality Coupling for Talking Head Generation"></a>Dual Audio-Centric Modality Coupling for Talking Head Generation</h2><p><strong>Authors:Ao Fu, Ziqi Ni, Yi Zhou</strong></p>
<p>The generation of audio-driven talking head videos is a key challenge in computer vision and graphics, with applications in virtual avatars and digital media. Traditional approaches often struggle with capturing the complex interaction between audio and facial dynamics, leading to lip synchronization and visual quality issues. In this paper, we propose a novel NeRF-based framework, Dual Audio-Centric Modality Coupling (DAMC), which effectively integrates content and dynamic features from audio inputs. By leveraging a dual encoder structure, DAMC captures semantic content through the Content-Aware Encoder and ensures precise visual synchronization through the Dynamic-Sync Encoder. These features are fused using a Cross-Synchronized Fusion Module (CSFM), enhancing content representation and lip synchronization. Extensive experiments show that our method outperforms existing state-of-the-art approaches in key metrics such as lip synchronization accuracy and image quality, demonstrating robust generalization across various audio inputs, including synthetic speech from text-to-speech (TTS) systems. Our results provide a promising solution for high-quality, audio-driven talking head generation and present a scalable approach for creating realistic talking heads. </p>
<blockquote>
<p>éŸ³é¢‘é©±åŠ¨çš„è¯´è¯äººå¤´éƒ¨è§†é¢‘ç”Ÿæˆæ˜¯è®¡ç®—æœºè§†è§‰å’Œå›¾å½¢å­¦é¢†åŸŸçš„å…³é”®æŒ‘æˆ˜ï¼Œåœ¨è™šæ‹ŸåŒ–èº«å’Œæ•°å­—åª’ä½“ä¸­æœ‰å¹¿æ³›çš„åº”ç”¨ã€‚ä¼ ç»Ÿçš„æ–¹æ³•å¾€å¾€éš¾ä»¥æ•æ‰éŸ³é¢‘å’Œé¢éƒ¨åŠ¨æ€ä¹‹é—´çš„å¤æ‚äº¤äº’ï¼Œå¯¼è‡´å”‡åŒæ­¥å’Œè§†è§‰è´¨é‡é—®é¢˜ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºNeRFçš„æ–°å‹æ¡†æ¶â€”â€”åŒéŸ³é¢‘ä¸­å¿ƒæ¨¡æ€è€¦åˆï¼ˆDAMCï¼‰ï¼Œå®ƒæœ‰æ•ˆåœ°æ•´åˆäº†æ¥è‡ªéŸ³é¢‘è¾“å…¥çš„å†…å®¹å’ŒåŠ¨æ€ç‰¹å¾ã€‚é€šè¿‡åˆ©ç”¨åŒç¼–ç å™¨ç»“æ„ï¼ŒDAMCé€šè¿‡å†…å®¹æ„ŸçŸ¥ç¼–ç å™¨æ•è·è¯­ä¹‰å†…å®¹ï¼Œå¹¶é€šè¿‡åŠ¨æ€åŒæ­¥ç¼–ç å™¨ç¡®ä¿ç²¾ç¡®çš„è§†è§‰åŒæ­¥ã€‚è¿™äº›ç‰¹å¾ä½¿ç”¨è·¨åŒæ­¥èåˆæ¨¡å—ï¼ˆCSFMï¼‰è¿›è¡Œèåˆï¼Œå¢å¼ºäº†å†…å®¹è¡¨ç¤ºå’Œå”‡åŒæ­¥ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨å”‡åŒæ­¥å‡†ç¡®æ€§å’Œå›¾åƒè´¨é‡ç­‰å…³é”®æŒ‡æ ‡ä¸Šä¼˜äºç°æœ‰çš„æœ€å…ˆè¿›çš„æ–¹æ³•ï¼Œå±•ç¤ºäº†åœ¨å„ç§éŸ³é¢‘è¾“å…¥ä¸Šçš„ç¨³å¥æ³›åŒ–èƒ½åŠ›ï¼ŒåŒ…æ‹¬æ¥è‡ªæ–‡æœ¬åˆ°è¯­éŸ³ï¼ˆTTSï¼‰ç³»ç»Ÿçš„åˆæˆè¯­éŸ³ã€‚æˆ‘ä»¬çš„ç»“æœä¸ºé«˜è´¨é‡éŸ³é¢‘é©±åŠ¨çš„è¯´è¯äººå¤´éƒ¨ç”Ÿæˆæä¾›äº†æœ‰å‰æ™¯çš„è§£å†³æ–¹æ¡ˆï¼Œå¹¶å±•ç¤ºäº†ä¸€ç§åˆ›å»ºé€¼çœŸè¯´è¯äººå¤´éƒ¨çš„å¯æ‰©å±•æ–¹æ³•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.22728v1">PDF</a> 9 pages, 4 figures</p>
<p><strong>Summary</strong><br>éŸ³é¢‘é©±åŠ¨çš„äººå¤´åŠ¨ç”»ç”Ÿæˆæ˜¯è®¡ç®—æœºè§†è§‰å’Œå›¾å½¢å­¦é¢†åŸŸçš„å…³é”®æŒ‘æˆ˜ï¼Œåº”ç”¨äºè™šæ‹Ÿè§’è‰²å’Œæ•°å­—åª’ä½“ã€‚ä¼ ç»Ÿæ–¹æ³•éš¾ä»¥æ•æ‰éŸ³é¢‘ä¸é¢éƒ¨åŠ¨æ€çš„å¤æ‚äº¤äº’ï¼Œå¯¼è‡´å”‡éƒ¨åŒæ­¥å’Œè§†è§‰è´¨é‡é—®é¢˜ã€‚æœ¬æ–‡æå‡ºåŸºäºNeRFçš„åŒéŸ³é¢‘ä¸­å¿ƒæ¨¡æ€è€¦åˆï¼ˆDAMCï¼‰æ¡†æ¶ï¼Œæœ‰æ•ˆæ•´åˆéŸ³é¢‘è¾“å…¥çš„å†…å®¹å’ŒåŠ¨æ€ç‰¹å¾ã€‚é€šè¿‡åŒç¼–ç å™¨ç»“æ„ï¼ŒDAMCé€šè¿‡å†…å®¹æ„ŸçŸ¥ç¼–ç å™¨æ•æ‰è¯­ä¹‰å†…å®¹ï¼Œå¹¶é€šè¿‡åŠ¨æ€åŒæ­¥ç¼–ç å™¨ç¡®ä¿ç²¾ç¡®è§†è§‰åŒæ­¥ã€‚è¿™äº›ç‰¹å¾é€šè¿‡äº¤å‰åŒæ­¥èåˆæ¨¡å—ï¼ˆCSFMï¼‰èåˆï¼Œæé«˜å†…å®¹è¡¨è¾¾å’Œå”‡éƒ¨åŒæ­¥ã€‚å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å”‡éƒ¨åŒæ­¥å‡†ç¡®æ€§å’Œå›¾åƒè´¨é‡ç­‰å…³é”®æŒ‡æ ‡ä¸Šä¼˜äºç°æœ‰å…ˆè¿›æ–¹æ³•ï¼Œå¹¶åœ¨å„ç§éŸ³é¢‘è¾“å…¥ä¸Šè¡¨ç°å‡ºå¼ºå¤§çš„æ³›åŒ–èƒ½åŠ›ï¼ŒåŒ…æ‹¬æ–‡æœ¬åˆ°è¯­éŸ³ï¼ˆTTSï¼‰ç³»ç»Ÿçš„åˆæˆè¯­éŸ³ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>éŸ³é¢‘é©±åŠ¨çš„äººå¤´åŠ¨ç”»ç”Ÿæˆæ˜¯è®¡ç®—æœºè§†è§‰å’Œå›¾å½¢é¢†åŸŸçš„é‡è¦æŒ‘æˆ˜ï¼Œå…·æœ‰å¹¿æ³›çš„åº”ç”¨äºè™šæ‹Ÿè§’è‰²å’Œæ•°å­—åª’ä½“ã€‚</li>
<li>ä¼ ç»Ÿæ–¹æ³•éš¾ä»¥æ•æ‰éŸ³é¢‘ä¸é¢éƒ¨åŠ¨æ€çš„å¤æ‚äº¤äº’ï¼Œå¯¼è‡´å”‡éƒ¨åŒæ­¥å’Œè§†è§‰è´¨é‡é—®é¢˜ã€‚</li>
<li>æœ¬æ–‡æå‡ºäº†ä¸€ä¸ªåŸºäºNeRFçš„åŒéŸ³é¢‘ä¸­å¿ƒæ¨¡æ€è€¦åˆï¼ˆDAMCï¼‰æ¡†æ¶æ¥è§£å†³è¿™ä¸ªé—®é¢˜ã€‚</li>
<li>DAMCé€šè¿‡å†…å®¹æ„ŸçŸ¥ç¼–ç å™¨å’ŒåŠ¨æ€åŒæ­¥ç¼–ç å™¨æœ‰æ•ˆæ•´åˆéŸ³é¢‘è¾“å…¥çš„å†…å®¹å’ŒåŠ¨æ€ç‰¹å¾ã€‚</li>
<li>é€šè¿‡äº¤å‰åŒæ­¥èåˆæ¨¡å—ï¼ˆCSFMï¼‰èåˆå†…å®¹å’ŒåŠ¨æ€ç‰¹å¾ï¼Œæé«˜å†…å®¹è¡¨è¾¾å’Œå”‡éƒ¨åŒæ­¥ã€‚</li>
<li>å®éªŒè¡¨æ˜ï¼ŒDAMCåœ¨å…³é”®æŒ‡æ ‡ä¸Šä¼˜äºç°æœ‰æ–¹æ³•ï¼Œå¦‚å”‡éƒ¨åŒæ­¥å‡†ç¡®æ€§å’Œå›¾åƒè´¨é‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.22728">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-2b2c7d49850f18f4d83bef5aebdce69d.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-f14fbcc16baec813cf008cbb3379f343.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-d792a51b6c87be14443f6a1fbc972941.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-df38b1b97f3c37aa076c7f6fdd8e6fd1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2563f0e3ff5ac02b7aacf1380dc9a5c9.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-10b2b6a0f229fd4764b744d91119afd6.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Audio-Plane-Audio-Factorization-Plane-Gaussian-Splatting-for-Real-Time-Talking-Head-Synthesis"><a href="#Audio-Plane-Audio-Factorization-Plane-Gaussian-Splatting-for-Real-Time-Talking-Head-Synthesis" class="headerlink" title="Audio-Plane: Audio Factorization Plane Gaussian Splatting for Real-Time   Talking Head Synthesis"></a>Audio-Plane: Audio Factorization Plane Gaussian Splatting for Real-Time   Talking Head Synthesis</h2><p><strong>Authors:Shuai Shen, Wanhua Li, Yunpeng Zhang, Weipeng Hu, Yap-Peng Tan</strong></p>
<p>Talking head synthesis has become a key research area in computer graphics and multimedia, yet most existing methods often struggle to balance generation quality with computational efficiency. In this paper, we present a novel approach that leverages an Audio Factorization Plane (Audio-Plane) based Gaussian Splatting for high-quality and real-time talking head generation. For modeling a dynamic talking head, 4D volume representation is needed. However, directly storing a dense 4D grid is impractical due to the high cost and lack of scalability for longer durations. We overcome this challenge with the proposed Audio-Plane, where the 4D volume representation is decomposed into audio-independent space planes and audio-dependent planes. This provides a compact and interpretable feature representation for talking head, facilitating more precise audio-aware spatial encoding and enhanced audio-driven lip dynamic modeling. To further improve speech dynamics, we develop a dynamic splatting method that helps the network more effectively focus on modeling the dynamics of the mouth region. Extensive experiments demonstrate that by integrating these innovations with the powerful Gaussian Splatting, our method is capable of synthesizing highly realistic talking videos in real time while ensuring precise audio-lip synchronization. Synthesized results are available in <a target="_blank" rel="noopener" href="https://sstzal.github.io/Audio-Plane/">https://sstzal.github.io/Audio-Plane/</a>. </p>
<blockquote>
<p>å¤´éƒ¨è¯´è¯åˆæˆå·²æˆä¸ºè®¡ç®—æœºå›¾å½¢å­¦å’Œå¤šåª’ä½“é¢†åŸŸçš„ä¸€ä¸ªå…³é”®ç ”ç©¶æ–¹å‘ï¼Œç„¶è€Œï¼Œå¤§å¤šæ•°ç°æœ‰æ–¹æ³•å¾€å¾€éš¾ä»¥åœ¨ç”Ÿæˆè´¨é‡å’Œè®¡ç®—æ•ˆç‡ä¹‹é—´å–å¾—å¹³è¡¡ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºéŸ³é¢‘åˆ†è§£å¹³é¢ï¼ˆAudio-Planeï¼‰çš„é«˜æ–¯æ¶‚æŠ¹æŠ€æœ¯çš„æ–°æ–¹æ³•ï¼Œç”¨äºé«˜è´¨é‡å®æ—¶å¤´éƒ¨è¯´è¯ç”Ÿæˆã€‚ä¸ºäº†æ¨¡æ‹ŸåŠ¨æ€çš„è¯´è¯å¤´éƒ¨ï¼Œéœ€è¦4Dä½“ç§¯è¡¨ç¤ºã€‚ç„¶è€Œï¼Œç”±äºæˆæœ¬é«˜æ˜‚å’Œé•¿æœŸç¼ºä¹å¯æ‰©å±•æ€§ï¼Œç›´æ¥å­˜å‚¨å¯†é›†çš„4Dç½‘æ ¼å¹¶ä¸å®ç”¨ã€‚æˆ‘ä»¬å…‹æœäº†è¿™ä¸€æŒ‘æˆ˜ï¼Œæå‡ºäº†éŸ³é¢‘å¹³é¢ï¼ˆAudio-Planeï¼‰ï¼Œå…¶ä¸­å°†4Dä½“ç§¯è¡¨ç¤ºåˆ†è§£ä¸ºç‹¬ç«‹äºéŸ³é¢‘çš„ç©ºé—´å¹³é¢å’Œä¾èµ–äºéŸ³é¢‘çš„å¹³é¢ã€‚è¿™ä¸ºè¯´è¯çš„å¤´éƒ¨æä¾›äº†ä¸€ä¸ªç´§å‡‘ä¸”å¯è§£é‡Šçš„ç‰¹å¾è¡¨ç¤ºï¼Œä¿ƒè¿›äº†æ›´ç²¾ç¡®çš„å£°éŸ³æ„ŸçŸ¥ç©ºé—´ç¼–ç å’Œå¢å¼ºçš„éŸ³é¢‘é©±åŠ¨å”‡éƒ¨åŠ¨æ€å»ºæ¨¡ã€‚ä¸ºäº†è¿›ä¸€æ­¥æé«˜è¯­éŸ³åŠ¨æ€æ•ˆæœï¼Œæˆ‘ä»¬å¼€å‘äº†ä¸€ç§åŠ¨æ€æ¶‚æŠ¹æ–¹æ³•ï¼Œå¸®åŠ©ç½‘ç»œæ›´æœ‰æ•ˆåœ°ä¸“æ³¨äºå”‡éƒ¨åŒºåŸŸçš„åŠ¨æ€å»ºæ¨¡ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼Œé€šè¿‡å°†è¿™äº›åˆ›æ–°ä¸å¼ºå¤§çš„é«˜æ–¯æ¶‚æŠ¹æŠ€æœ¯ç›¸ç»“åˆï¼Œæˆ‘ä»¬çš„æ–¹æ³•èƒ½å¤Ÿåœ¨ä¿è¯ç²¾ç¡®éŸ³é¢‘å”‡éƒ¨åŒæ­¥çš„æƒ…å†µä¸‹ï¼Œå®æ—¶åˆæˆé«˜åº¦é€¼çœŸçš„è¯´è¯è§†é¢‘ã€‚åˆæˆç»“æœå¯åœ¨<a target="_blank" rel="noopener" href="https://sstzal.github.io/Audio-Plane/%E6%9F%A5%E7%9C%8B%E3%80%82">https://sstzal.github.io/Audio-Plane/æŸ¥çœ‹ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.22605v1">PDF</a> </p>
<p><strong>Summary</strong><br>æœ¬è®ºæ–‡æå‡ºäº†ä¸€ç§åˆ©ç”¨éŸ³é¢‘åˆ†è§£å¹³é¢ï¼ˆAudio-Planeï¼‰é«˜æ–¯æ¨¡ç³ŠæŠ€æœ¯å®ç°é«˜è´¨é‡å®æ—¶åŠ¨æ€è°ˆè¯å¤´ç”Ÿæˆçš„æ–°æ–¹æ³•ã€‚é€šè¿‡åˆ†è§£å››ç»´ä½“ç§¯è¡¨ç¤ºæ³•ä¸ºéŸ³é¢‘ç‹¬ç«‹ç©ºé—´å¹³é¢å’ŒéŸ³é¢‘ä¾èµ–å¹³é¢ï¼Œè§£å†³äº†ç›´æ¥å­˜å‚¨å¯†é›†å››ç»´ç½‘æ ¼çš„ä¸åˆ‡å®é™…å’Œç¼ºä¹å¯æ‰©å±•æ€§çš„é—®é¢˜ã€‚è¯¥æ–¹æ³•ä¸ºè°ˆè¯å¤´æä¾›äº†ç´§å‡‘ä¸”å¯è§£é‡Šçš„ç‰¹å¾è¡¨ç¤ºï¼Œä¿ƒè¿›äº†æ›´ç²¾ç¡®çš„éŸ³é¢‘æ„ŸçŸ¥ç©ºé—´ç¼–ç å’Œå¢å¼ºçš„éŸ³é¢‘é©±åŠ¨å”‡åŠ¨æ€å»ºæ¨¡ã€‚ä¸ºäº†è¿›ä¸€æ­¥æé«˜è¯­éŸ³åŠ¨æ€ï¼Œå¼€å‘äº†ä¸€ç§åŠ¨æ€æ¨¡ç³Šæ–¹æ³•ï¼Œå¸®åŠ©ç½‘ç»œæ›´æœ‰æ•ˆåœ°ä¸“æ³¨äºå”‡éƒ¨åŒºåŸŸçš„åŠ¨æ€å»ºæ¨¡ã€‚å®éªŒè¯æ˜ï¼Œé€šè¿‡å°†è¿™äº›åˆ›æ–°ä¸å¼ºå¤§çš„é«˜æ–¯æ¨¡ç³ŠæŠ€æœ¯ç›¸ç»“åˆï¼Œè¯¥æ–¹æ³•èƒ½å¤Ÿå®æ—¶åˆæˆé«˜åº¦é€¼çœŸçš„è°ˆè¯è§†é¢‘ï¼ŒåŒæ—¶ç¡®ä¿ç²¾ç¡®çš„éŸ³é¢‘-å”‡éƒ¨åŒæ­¥ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æœ¬è®ºæ–‡æå‡ºäº†åˆ©ç”¨éŸ³é¢‘åˆ†è§£å¹³é¢ï¼ˆAudio-Planeï¼‰æŠ€æœ¯å®ç°é«˜è´¨é‡å®æ—¶è°ˆè¯å¤´ç”Ÿæˆçš„æ–¹æ³•ã€‚</li>
<li>é€šè¿‡å°†å››ç»´ä½“ç§¯è¡¨ç¤ºåˆ†è§£ä¸ºéŸ³é¢‘ç‹¬ç«‹å’Œä¾èµ–çš„å¹³é¢ï¼Œè§£å†³äº†å­˜å‚¨å¯†é›†å››ç»´ç½‘æ ¼çš„é—®é¢˜ã€‚</li>
<li>Audio-PlaneæŠ€æœ¯æä¾›äº†ç´§å‡‘ä¸”å¯è§£é‡Šçš„è°ˆè¯å¤´ç‰¹å¾è¡¨ç¤ºã€‚</li>
<li>è¯¥æ–¹æ³•ä¿ƒè¿›äº†æ›´ç²¾ç¡®çš„éŸ³é¢‘æ„ŸçŸ¥ç©ºé—´ç¼–ç å’Œå¢å¼ºçš„éŸ³é¢‘é©±åŠ¨å”‡åŠ¨æ€å»ºæ¨¡ã€‚</li>
<li>é€šè¿‡åŠ¨æ€æ¨¡ç³Šæ–¹æ³•ï¼Œæé«˜äº†è¯­éŸ³åŠ¨æ€å»ºæ¨¡çš„æ•ˆæœã€‚</li>
<li>å®éªŒè¯æ˜è¯¥æ–¹æ³•èƒ½å¤Ÿåˆæˆé«˜åº¦é€¼çœŸçš„è°ˆè¯è§†é¢‘ï¼Œå¹¶ç¡®ä¿éŸ³é¢‘ä¸å”‡éƒ¨çš„ç²¾ç¡®åŒæ­¥ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.22605">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-dc3945e146a1125a5d769517b491305e.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-3c343fd56b0ea0f2ccfabdd9058dcf16.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-fb17023d93340211b982eb56e0a42e41.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-4c0d22507b0d089e075c77b6ec79cfee.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-14c4fa59a61d1b3a409ba585990402d3.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-08c3de0ff764017706147587374a97e4.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="ReCoM-Realistic-Co-Speech-Motion-Generation-with-Recurrent-Embedded-Transformer"><a href="#ReCoM-Realistic-Co-Speech-Motion-Generation-with-Recurrent-Embedded-Transformer" class="headerlink" title="ReCoM: Realistic Co-Speech Motion Generation with Recurrent Embedded   Transformer"></a>ReCoM: Realistic Co-Speech Motion Generation with Recurrent Embedded   Transformer</h2><p><strong>Authors:Yong Xie, Yunlian Sun, Hongwen Zhang, Yebin Liu, Jinhui Tang</strong></p>
<p>We present ReCoM, an efficient framework for generating high-fidelity and generalizable human body motions synchronized with speech. The core innovation lies in the Recurrent Embedded Transformer (RET), which integrates Dynamic Embedding Regularization (DER) into a Vision Transformer (ViT) core architecture to explicitly model co-speech motion dynamics. This architecture enables joint spatial-temporal dependency modeling, thereby enhancing gesture naturalness and fidelity through coherent motion synthesis. To enhance model robustness, we incorporate the proposed DER strategy, which equips the model with dual capabilities of noise resistance and cross-domain generalization, thereby improving the naturalness and fluency of zero-shot motion generation for unseen speech inputs. To mitigate inherent limitations of autoregressive inference, including error accumulation and limited self-correction, we propose an iterative reconstruction inference (IRI) strategy. IRI refines motion sequences via cyclic pose reconstruction, driven by two key components: (1) classifier-free guidance improves distribution alignment between generated and real gestures without auxiliary supervision, and (2) a temporal smoothing process eliminates abrupt inter-frame transitions while ensuring kinematic continuity. Extensive experiments on benchmark datasets validate ReCoMâ€™s effectiveness, achieving state-of-the-art performance across metrics. Notably, it reduces the Fr&#39;echet Gesture Distance (FGD) from 18.70 to 2.48, demonstrating an 86.7% improvement in motion realism. Our project page is <a target="_blank" rel="noopener" href="https://yong-xie-xy.github.io/ReCoM/">https://yong-xie-xy.github.io/ReCoM/</a>. </p>
<blockquote>
<p>æˆ‘ä»¬æå‡ºäº†ReCoMï¼Œè¿™æ˜¯ä¸€ä¸ªé«˜æ•ˆæ¡†æ¶ï¼Œç”¨äºç”Ÿæˆä¸è¯­éŸ³åŒæ­¥çš„é«˜ä¿çœŸå’Œå¯æ¨å¹¿çš„äººä½“åŠ¨ä½œã€‚æ ¸å¿ƒåˆ›æ–°ç‚¹åœ¨äºå¾ªç¯åµŒå…¥è½¬æ¢å™¨ï¼ˆRETï¼‰ï¼Œå®ƒå°†åŠ¨æ€åµŒå…¥æ­£åˆ™åŒ–ï¼ˆDERï¼‰é›†æˆåˆ°è§†è§‰è½¬æ¢å™¨ï¼ˆViTï¼‰æ ¸å¿ƒæ¶æ„ä¸­ï¼Œä»¥æ˜¾å¼åœ°å»ºæ¨¡å…±è¯­éŸ³è¿åŠ¨åŠ¨åŠ›å­¦ã€‚è¯¥æ¶æ„èƒ½å¤Ÿè”åˆå»ºæ¨¡æ—¶ç©ºä¾èµ–æ€§ï¼Œä»è€Œé€šè¿‡è¿è´¯çš„è¿åŠ¨åˆæˆå¢å¼ºæ‰‹åŠ¿çš„è‡ªç„¶æ€§å’Œä¿çœŸåº¦ã€‚ä¸ºäº†æé«˜æ¨¡å‹çš„ç¨³å¥æ€§ï¼Œæˆ‘ä»¬é‡‡ç”¨äº†æå‡ºçš„DERç­–ç•¥ï¼Œè¯¥ç­–ç•¥ä½¿æ¨¡å‹å…·å¤‡æŠ—å™ªå£°å’Œè·¨åŸŸæ¨å¹¿çš„åŒé‡èƒ½åŠ›ï¼Œä»è€Œæé«˜äº†æœªè§è¯­éŸ³è¾“å…¥çš„é›¶æ ·æœ¬è¿åŠ¨ç”Ÿæˆçš„è‡ªç„¶æ€§å’Œæµç•…æ€§ã€‚ä¸ºäº†å‡è½»è‡ªå›å½’æ¨ç†çš„å›ºæœ‰å±€é™æ€§ï¼ŒåŒ…æ‹¬è¯¯å·®ç´¯ç§¯å’Œæœ‰é™çš„è‡ªæˆ‘æ ¡æ­£èƒ½åŠ›ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§è¿­ä»£é‡å»ºæ¨ç†ï¼ˆIRIï¼‰ç­–ç•¥ã€‚IRIé€šè¿‡å¾ªç¯å§¿åŠ¿é‡å»ºæ¥ä¼˜åŒ–è¿åŠ¨åºåˆ—ï¼Œè¿™ç”±ä¸¤ä¸ªå…³é”®ç»„ä»¶é©±åŠ¨ï¼šï¼ˆ1ï¼‰æ— åˆ†ç±»å™¨æŒ‡å¯¼æ”¹å–„äº†ç”Ÿæˆæ‰‹åŠ¿å’ŒçœŸå®æ‰‹åŠ¿ä¹‹é—´çš„åˆ†å¸ƒå¯¹é½ï¼Œæ— éœ€è¾…åŠ©ç›‘ç£ï¼›ï¼ˆ2ï¼‰æ—¶é—´å¹³æ»‘è¿‡ç¨‹æ¶ˆé™¤äº†å¸§é—´çªå…€çš„è¿‡æ¸¡ï¼ŒåŒæ—¶ç¡®ä¿äº†è¿åŠ¨å­¦è¿ç»­æ€§ã€‚åœ¨åŸºå‡†æ•°æ®é›†ä¸Šçš„å¹¿æ³›å®éªŒéªŒè¯äº†ReCoMçš„æœ‰æ•ˆæ€§ï¼Œå…¶åœ¨å„é¡¹æŒ‡æ ‡ä¸Šå‡è¾¾åˆ°äº†æœ€æ–°æŠ€æœ¯æ°´å¹³ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œå®ƒå°†FrÃ©chetæ‰‹åŠ¿è·ç¦»ï¼ˆFGDï¼‰ä»18.70å‡å°‘åˆ°2.48ï¼Œæ˜¾ç¤ºå‡ºè¿åŠ¨çœŸå®æ€§çš„86.7%æ”¹è¿›ã€‚æˆ‘ä»¬çš„é¡¹ç›®é¡µé¢æ˜¯<a target="_blank" rel="noopener" href="https://yong-xie-xy.github.io/ReCoM%E3%80%82">https://yong-xie-xy.github.io/ReCoM/ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.21847v1">PDF</a> 8 pages, 6 figures, Project Page:   <a target="_blank" rel="noopener" href="https://yong-xie-xy.github.io/ReCoM/">https://yong-xie-xy.github.io/ReCoM/</a></p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ReCoMæ¡†æ¶ï¼Œå®ƒèƒ½ç”Ÿæˆä¸è¯­éŸ³åŒæ­¥çš„é«˜ä¿çœŸã€å¯æ¨å¹¿çš„äººä½“åŠ¨ä½œã€‚æ ¸å¿ƒåˆ›æ–°åœ¨äºRecurrent Embedded Transformerï¼ˆRETï¼‰ï¼Œå®ƒå°†Dynamic Embedding Regularizationï¼ˆDERï¼‰é›†æˆåˆ°Vision Transformerï¼ˆViTï¼‰æ ¸å¿ƒæ¶æ„ä¸­ï¼Œä»¥æ˜¾å¼åœ°æ¨¡æ‹ŸåŠ¨ä½œä¸è¯­éŸ³çš„ååŒåŠ¨æ€ã€‚é€šè¿‡è”åˆç©ºé—´æ—¶é—´ä¾èµ–æ€§å»ºæ¨¡ï¼Œæé«˜äº†æ‰‹åŠ¿çš„è‡ªç„¶æ€§å’Œä¿çœŸæ€§ï¼Œå¹¶é€šè¿‡è¿è´¯çš„åŠ¨ä½œåˆæˆå®ç°äº†é€¼çœŸè¿åŠ¨ã€‚ä¸ºæé«˜æ¨¡å‹ç¨³å¥æ€§ï¼Œç»“åˆäº†DERç­–ç•¥ï¼Œä½¿æ¨¡å‹å…·æœ‰æŠ—å™ªå£°å’Œè·¨åŸŸæ¨å¹¿èƒ½åŠ›ï¼Œæé«˜äº†æœªè§è¯­éŸ³è¾“å…¥çš„é›¶æ ·æœ¬è¿åŠ¨ç”Ÿæˆçš„æµç•…æ€§å’Œè‡ªç„¶æ€§ã€‚ä¸ºç¼“è§£è‡ªå›å½’æ¨ç†çš„å›ºæœ‰å±€é™æ€§ï¼Œå¦‚è¯¯å·®ç´¯ç§¯å’Œæœ‰é™çš„è‡ªæˆ‘æ ¡æ­£èƒ½åŠ›ï¼Œæå‡ºäº†è¿­ä»£é‡å»ºæ¨ç†ï¼ˆIRIï¼‰ç­–ç•¥ã€‚é€šè¿‡å¾ªç¯å§¿åŠ¿é‡å»ºä¼˜åŒ–åŠ¨ä½œåºåˆ—ï¼Œç”±æ— ç›‘ç£åˆ†ç±»å™¨å¼•å¯¼å’Œä¸´æ—¶å¹³æ»‘è¿‡ç¨‹é©±åŠ¨ï¼Œæ¶ˆé™¤äº†å¸§é—´çªå…€è¿‡æ¸¡ï¼Œç¡®ä¿äº†è¿åŠ¨å­¦è¿ç»­æ€§ã€‚åœ¨åŸºå‡†æ•°æ®é›†ä¸Šçš„å¹¿æ³›å®éªŒéªŒè¯äº†ReCoMçš„æœ‰æ•ˆæ€§ï¼Œå®ç°äº†è·¨æŒ‡æ ‡çš„å…ˆè¿›æ€§èƒ½ã€‚æ˜¾è‘—åœ°ï¼Œå®ƒå°†Frâ€™echet Gesture Distanceï¼ˆFGDï¼‰ä»18.70é™ä½åˆ°2.48ï¼Œæ˜¾ç¤ºå‡ºåŠ¨ä½œçœŸå®æ€§çš„86.7%æå‡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ReCoMæ˜¯ä¸€ä¸ªé«˜æ•ˆæ¡†æ¶ï¼Œç”¨äºç”Ÿæˆä¸è¯­éŸ³åŒæ­¥çš„é«˜ä¿çœŸå’Œå¯æ¨å¹¿çš„äººä½“åŠ¨ä½œã€‚</li>
<li>æ ¸å¿ƒåˆ›æ–°æ˜¯Recurrent Embedded Transformerï¼ˆRETï¼‰ï¼Œé›†æˆäº†Dynamic Embedding Regularizationï¼ˆDERï¼‰å’ŒVision Transformerï¼ˆViTï¼‰ã€‚</li>
<li>RETèƒ½æ˜¾å¼åœ°æ¨¡æ‹ŸåŠ¨ä½œä¸è¯­éŸ³çš„ååŒåŠ¨æ€ï¼Œé€šè¿‡è”åˆç©ºé—´æ—¶é—´ä¾èµ–æ€§å»ºæ¨¡æé«˜æ‰‹åŠ¿çš„è‡ªç„¶æ€§å’Œä¿çœŸæ€§ã€‚</li>
<li>DERç­–ç•¥æé«˜äº†æ¨¡å‹çš„ç¨³å¥æ€§ï¼Œå…·æœ‰æŠ—å™ªå£°å’Œè·¨åŸŸæ¨å¹¿èƒ½åŠ›ã€‚</li>
<li>è¿­ä»£é‡å»ºæ¨ç†ï¼ˆIRIï¼‰ç­–ç•¥ç¼“è§£è‡ªå›å½’æ¨ç†çš„å±€é™æ€§ï¼Œå¦‚è¯¯å·®ç´¯ç§¯å’Œæœ‰é™çš„è‡ªæˆ‘æ ¡æ­£èƒ½åŠ›ã€‚</li>
<li>IRIé€šè¿‡å¾ªç¯å§¿åŠ¿é‡å»ºä¼˜åŒ–åŠ¨ä½œåºåˆ—ï¼Œç”±æ— ç›‘ç£åˆ†ç±»å™¨å¼•å¯¼å’Œä¸´æ—¶å¹³æ»‘è¿‡ç¨‹é©±åŠ¨ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.21847">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-7dcd56a9686962fe72ae2b6f38b5209e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-970b5b00da0172e59efc09926c363490.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-50f2dd645cb22ef8c8e5b9d767e9cdcd.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-058866dff95090e67b360030a97b60db.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-cd883c5e6a1487049bc61b16de22a1f8.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="QualiSpeech-A-Speech-Quality-Assessment-Dataset-with-Natural-Language-Reasoning-and-Descriptions"><a href="#QualiSpeech-A-Speech-Quality-Assessment-Dataset-with-Natural-Language-Reasoning-and-Descriptions" class="headerlink" title="QualiSpeech: A Speech Quality Assessment Dataset with Natural Language   Reasoning and Descriptions"></a>QualiSpeech: A Speech Quality Assessment Dataset with Natural Language   Reasoning and Descriptions</h2><p><strong>Authors:Siyin Wang, Wenyi Yu, Xianzhao Chen, Xiaohai Tian, Jun Zhang, Lu Lu, Yu Tsao, Junichi Yamagishi, Yuxuan Wang, Chao Zhang</strong></p>
<p>This paper explores a novel perspective to speech quality assessment by leveraging natural language descriptions, offering richer, more nuanced insights than traditional numerical scoring methods. Natural language feedback provides instructive recommendations and detailed evaluations, yet existing datasets lack the comprehensive annotations needed for this approach. To bridge this gap, we introduce QualiSpeech, a comprehensive low-level speech quality assessment dataset encompassing 11 key aspects and detailed natural language comments that include reasoning and contextual insights. Additionally, we propose the QualiSpeech Benchmark to evaluate the low-level speech understanding capabilities of auditory large language models (LLMs). Experimental results demonstrate that finetuned auditory LLMs can reliably generate detailed descriptions of noise and distortion, effectively identifying their types and temporal characteristics. The results further highlight the potential for incorporating reasoning to enhance the accuracy and reliability of quality assessments. The dataset will be released at <a target="_blank" rel="noopener" href="https://huggingface.co/datasets/tsinghua-ee/QualiSpeech">https://huggingface.co/datasets/tsinghua-ee/QualiSpeech</a>. </p>
<blockquote>
<p>æœ¬æ–‡æ¢ç´¢äº†ä¸€ç§åˆ©ç”¨è‡ªç„¶è¯­è¨€æè¿°è¿›è¡Œè¯­éŸ³è´¨é‡è¯„ä¼°çš„æ–°è§†è§’ï¼Œæä¾›äº†æ¯”ä¼ ç»Ÿæ•°å­—è¯„åˆ†æ–¹æ³•æ›´ä¸°å¯Œã€æ›´ç»†å¾®çš„è§è§£ã€‚è‡ªç„¶è¯­è¨€åé¦ˆæä¾›äº†æŒ‡å¯¼æ€§çš„å»ºè®®å’Œè¯¦ç»†çš„è¯„ä»·ï¼Œä½†ç°æœ‰æ•°æ®é›†ç¼ºä¹è¿™ç§è¯„ä¼°æ–¹æ³•æ‰€éœ€çš„å…¨é¢æ³¨é‡Šã€‚ä¸ºäº†å¼¥è¡¥è¿™ä¸€å·®è·ï¼Œæˆ‘ä»¬æ¨å‡ºäº†QualiSpeechæ•°æ®é›†ï¼Œè¿™æ˜¯ä¸€ä¸ªå…¨é¢çš„ä½çº§åˆ«è¯­éŸ³è´¨é‡è¯„ä¼°æ•°æ®é›†ï¼Œæ¶µç›–äº†11ä¸ªå…³é”®æ–¹é¢å’ŒåŒ…å«æ¨ç†å’Œä¸Šä¸‹æ–‡æ´å¯Ÿçš„è‡ªç„¶è¯­è¨€è¯¦ç»†æ³¨é‡Šã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æå‡ºäº†QualiSpeechåŸºå‡†æµ‹è¯•ï¼Œä»¥è¯„ä¼°å¬è§‰å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å¯¹ä½çº§åˆ«è¯­éŸ³çš„ç†è§£èƒ½åŠ›ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œç»è¿‡å¾®è°ƒåçš„å¬è§‰LLMå¯ä»¥å¯é åœ°æè¿°å™ªå£°å’Œå¤±çœŸçš„ç»†èŠ‚ï¼Œæœ‰æ•ˆåœ°è¯†åˆ«å®ƒä»¬çš„ç±»å‹å’Œæ—¶é—´ç‰¹å¾ã€‚ç»“æœè¿˜å¼ºè°ƒäº†ç»“åˆæ¨ç†æé«˜è´¨é‡è¯„ä¼°å‡†ç¡®æ€§å’Œå¯é æ€§çš„æ½œåŠ›ã€‚è¯¥æ•°æ®é›†å°†åœ¨<a target="_blank" rel="noopener" href="https://huggingface.co/datasets/tsinghua-ee/QualiSpeech%E5%8F%91%E5%B8%83%E3%80%82">https://huggingface.co/datasets/tsinghua-ee/QualiSpeechå‘å¸ƒã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.20290v2">PDF</a> 23 pages, 16 figures</p>
<p><strong>æ‘˜è¦</strong></p>
<p>æœ¬æ–‡åˆ©ç”¨è‡ªç„¶è¯­è¨€æè¿°ï¼Œä»æ–°çš„è§’åº¦æ¢è®¨è¯­éŸ³è´¨é‡è¯„ä¼°ï¼Œæä¾›æ¯”ä¼ ç»Ÿæ•°å­—è¯„åˆ†æ–¹æ³•æ›´ä¸°å¯Œã€æ›´ç»†å¾®çš„è§è§£ã€‚è‡ªç„¶è¯­è¨€åé¦ˆæä¾›æŒ‡å¯¼æ€§çš„å»ºè®®å’Œè¯¦ç»†çš„è¯„ä»·ï¼Œä½†ç°æœ‰æ•°æ®é›†ç¼ºä¹å…¨é¢æ³¨é‡Šï¼Œæ— æ³•æ”¯æŒæ­¤æ–¹æ³•ã€‚ä¸ºäº†å¼¥è¡¥è¿™ä¸€å·®è·ï¼Œæˆ‘ä»¬æ¨å‡ºäº†QualiSpeechæ•°æ®é›†ï¼Œå®ƒæ¶µç›–äº†ä½å±‚æ¬¡çš„è¯­éŸ³è´¨é‡è¯„ä¼°çš„11ä¸ªå…³é”®æ–¹é¢ï¼ŒåŒ…å«æ¨ç†å’Œä¸Šä¸‹æ–‡æ´å¯Ÿçš„è‡ªç„¶è¯­è¨€è¯„è®ºã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æå‡ºäº†QualiSpeechåŸºå‡†æµ‹è¯•ï¼Œä»¥è¯„ä¼°å¬è§‰å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å¯¹ä½å±‚æ¬¡è¯­éŸ³çš„ç†è§£èƒ½åŠ›ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œç»è¿‡å¾®è°ƒçš„å¤§å‹è¯­è¨€æ¨¡å‹èƒ½å¤Ÿå¯é åœ°æè¿°å™ªå£°å’Œå¤±çœŸç»†èŠ‚ï¼Œæœ‰æ•ˆè¯†åˆ«å®ƒä»¬çš„ç±»å‹å’Œæ—¶é—´ç‰¹å¾ã€‚ç»“æœè¿˜è¡¨æ˜ï¼Œç»“åˆæ¨ç†å¯ä»¥æé«˜è´¨é‡è¯„ä¼°çš„å‡†ç¡®æ€§å’Œå¯é æ€§ã€‚è¯¥æ•°æ®é›†å°†åœ¨<a target="_blank" rel="noopener" href="https://huggingface.co/datasets/tsinghua-ee/QualiSpeech%E4%B8%8A%E5%8F%91%E6%98%BE%E3%80%82">https://huggingface.co/datasets/tsinghua-ee/QualiSpeechä¸Šå‘å¸ƒã€‚</a></p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>åˆ©ç”¨è‡ªç„¶è¯­è¨€æè¿°æä¾›äº†ä¸€ä¸ªæ–°çš„è¯­éŸ³è´¨é‡è¯„ä¼°è§†è§’ï¼Œæ›´ä¸°å¯Œä¸”ç»†å¾®ã€‚</li>
<li>è‡ªç„¶è¯­è¨€åé¦ˆèƒ½æä¾›æŒ‡å¯¼æ€§çš„å»ºè®®å’Œè¯¦ç»†çš„è¯„ä»·ã€‚</li>
<li>ç°æœ‰æ•°æ®é›†ç¼ºä¹å…¨é¢æ³¨é‡Šï¼Œæ— æ³•æ”¯æŒåŸºäºè‡ªç„¶è¯­è¨€æè¿°çš„è¯­éŸ³è´¨é‡è¯„ä¼°æ–¹æ³•ã€‚</li>
<li>æ¨å‡ºQualiSpeechæ•°æ®é›†ï¼Œæ¶µç›–ä½å±‚æ¬¡è¯­éŸ³è´¨é‡è¯„ä¼°çš„å¤šä¸ªæ–¹é¢å’Œè¯¦ç»†è‡ªç„¶è¯­è¨€è¯„è®ºã€‚</li>
<li>æå‡ºQualiSpeechåŸºå‡†æµ‹è¯•ä»¥è¯„ä¼°å¬è§‰å¤§å‹è¯­è¨€æ¨¡å‹å¯¹ä½å±‚æ¬¡è¯­éŸ³çš„ç†è§£èƒ½åŠ›ã€‚</li>
<li>å®éªŒè¡¨æ˜ï¼Œç»è¿‡è®­ç»ƒçš„å¤§å‹è¯­è¨€æ¨¡å‹èƒ½å¤Ÿè¯¦ç»†æè¿°è¯­éŸ³ä¸­çš„å™ªå£°å’Œå¤±çœŸã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.20290">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-33d85e460b85196fd81bcaffef10a739.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-91978749794794dfc280d966b6381780.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-954ad441dd32fc16510f9c15b46f45e5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-92f61a13cedc03a3f0f000df2f33e779.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9b4cb303644bc2e13b7ba7798e2e41c6.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="Whispering-in-Amharic-Fine-tuning-Whisper-for-Low-resource-Language"><a href="#Whispering-in-Amharic-Fine-tuning-Whisper-for-Low-resource-Language" class="headerlink" title="Whispering in Amharic: Fine-tuning Whisper for Low-resource Language"></a>Whispering in Amharic: Fine-tuning Whisper for Low-resource Language</h2><p><strong>Authors:Dawit Ketema Gete, Bedru Yimam Ahmed, Tadesse Destaw Belay, Yohannes Ayana Ejigu, Sukairaj Hafiz Imam, Alemu Belay Tessema, Mohammed Oumer Adem, Tadesse Amare Belay, Robert Geislinger, Umma Aliyu Musa, Martin Semmann, Shamsuddeen Hassan Muhammad, Henning Schreiber, Seid Muhie Yimam</strong></p>
<p>This work explores fine-tuning OpenAIâ€™s Whisper automatic speech recognition (ASR) model for Amharic, a low-resource language, to improve transcription accuracy. While the foundational Whisper model struggles with Amharic due to limited representation in its training data, we fine-tune it using datasets like Mozilla Common Voice, FLEURS, and the BDU-speech dataset. The best-performing model, Whispersmall-am, significantly improves when finetuned on a mix of existing FLEURS data and new, unseen Amharic datasets. Training solely on new data leads to poor performance, but combining it with FLEURS data reinforces the model, enabling better specialization in Amharic. We also demonstrate that normalizing Amharic homophones significantly enhances Word Error Rate (WER) and Bilingual Evaluation Understudy (BLEU) scores. This study underscores the importance of fine-tuning strategies and dataset composition for improving ASR in low-resource languages, providing insights for future Amharic speech recognition research. </p>
<blockquote>
<p>è¿™é¡¹å·¥ä½œæ¢è®¨äº†é’ˆå¯¹é˜¿å§†å“ˆæ‹‰è¯­ï¼ˆä¸€ç§ä½èµ„æºè¯­è¨€ï¼‰å¯¹OpenAIçš„è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰æ¨¡å‹Whisperè¿›è¡Œå¾®è°ƒï¼Œä»¥æé«˜è½¬å½•å‡†ç¡®æ€§ã€‚å°½ç®¡åŸºç¡€Whisperæ¨¡å‹åœ¨é˜¿å§†å“ˆæ‹‰è¯­æ–¹é¢ç”±äºå…¶è®­ç»ƒæ•°æ®ä¸­çš„æœ‰é™è¡¨ç¤ºè€Œé¢ä¸´æŒ‘æˆ˜ï¼Œä½†æˆ‘ä»¬ä½¿ç”¨Mozilla Common Voiceã€FLEURSå’ŒBDU-speechç­‰æ•°æ®é›†å¯¹å…¶è¿›è¡Œå¾®è°ƒã€‚è¡¨ç°æœ€ä½³çš„æ¨¡å‹Whispersmall-amï¼Œåœ¨ç°æœ‰çš„FLEURSæ•°æ®å’Œæ–°çš„æœªè§è¿‡çš„é˜¿å§†å“ˆæ‹‰è¯­æ•°æ®é›†çš„æ··åˆä¸Šå¾®è°ƒæ—¶ï¼Œæ€§èƒ½å¾—åˆ°äº†æ˜¾è‘—æå‡ã€‚ä»…åœ¨æ–°æ•°æ®ä¸Šè¿›è¡Œè®­ç»ƒä¼šå¯¼è‡´æ€§èƒ½ä¸ä½³ï¼Œä½†å°†å…¶ä¸FLEURSæ•°æ®ç›¸ç»“åˆå¯ä»¥åŠ å¼ºæ¨¡å‹ï¼Œä½¿é˜¿å§†å“ˆæ‹‰è¯­çš„ä¸“ä¸šåŒ–èƒ½åŠ›æ›´å¼ºã€‚æˆ‘ä»¬è¿˜è¯æ˜ï¼Œå¯¹é˜¿å§†å“ˆæ‹‰è¯­çš„åŒéŸ³å­—è¿›è¡Œå½’ä¸€åŒ–å¤„ç†å¯ä»¥æ˜¾è‘—æé«˜å•è¯é”™è¯¯ç‡ï¼ˆWERï¼‰å’ŒåŒè¯­è¯„ä¼°ä¸‹ç ”ç©¶ï¼ˆBLEUï¼‰çš„å¾—åˆ†ã€‚è¿™é¡¹ç ”ç©¶å¼ºè°ƒäº†å¾®è°ƒç­–ç•¥å’Œæ•°æ®é›†ç»„æˆåœ¨æé«˜ä½èµ„æºè¯­è¨€çš„è¯­éŸ³è¯†åˆ«ä¸­çš„é‡è¦æ€§ï¼Œä¸ºæœªæ¥é˜¿å§†å“ˆæ‹‰è¯­è¯­éŸ³è¯†åˆ«ç ”ç©¶æä¾›äº†è§è§£ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.18485v2">PDF</a> </p>
<p><strong>æ€»ç»“</strong><br>    æœ¬æ–‡æ¢è®¨äº†é’ˆå¯¹é˜¿å§†å“ˆæ‹‰è¯­è¿™ç§èµ„æºè´«ä¹çš„è¯­è¨€ï¼Œå¦‚ä½•å¯¹OpenAIçš„Whisperè‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰æ¨¡å‹è¿›è¡Œå¾®è°ƒï¼Œä»¥æé«˜è½¬å½•å‡†ç¡®æ€§ã€‚ä½¿ç”¨Mozilla Common Voiceã€FLEURSå’ŒBDU-speechç­‰æ•°æ®é›†å¯¹åŸºç¡€Whisperæ¨¡å‹è¿›è¡Œå¾®è°ƒï¼Œè§£å†³äº†æ¨¡å‹åœ¨é˜¿å§†å“ˆæ‹‰è¯­æ–¹é¢çš„è¯†åˆ«å›°éš¾ã€‚åœ¨ç°æœ‰FLEURSæ•°æ®å’Œæ–°ã€æœªè§è¿‡çš„é˜¿å§†å“ˆæ‹‰è¯­æ•°æ®é›†çš„æ··åˆä¸Šè®­ç»ƒçš„Whispersmall-amæ¨¡å‹è¡¨ç°æœ€ä½³ã€‚ä»…åœ¨æ–°æ•°æ®ä¸Šè®­ç»ƒä¼šå¯¼è‡´æ€§èƒ½ä¸ä½³ï¼Œä½†ä¸FLEURSæ•°æ®ç›¸ç»“åˆï¼Œå¼ºåŒ–äº†æ¨¡å‹çš„æ€§èƒ½ï¼Œä½¿å…¶åœ¨é˜¿å§†å“ˆæ‹‰è¯­æ–¹é¢æ›´åŠ ä¸“ä¸šåŒ–ã€‚åŒæ—¶è¯æ˜ï¼Œå¯¹é˜¿å§†å“ˆæ‹‰è¯­çš„åŒéŸ³å­—è¿›è¡Œå½’ä¸€åŒ–å¤„ç†å¯ä»¥æ˜¾è‘—æé«˜å•è¯é”™è¯¯ç‡å’ŒåŒè¯­è¯„ä¼°ç ”ç©¶å¾—åˆ†ã€‚æœ¬ç ”ç©¶å¼ºè°ƒäº†å¾®è°ƒç­–ç•¥å’Œæ•°æ®é›†ç»„æˆåœ¨æ”¹è¿›ä½èµ„æºè¯­è¨€çš„è¯­éŸ³è¯†åˆ«ä¸­çš„é‡è¦æ€§ï¼Œä¸ºæœªæ¥é˜¿å§†å“ˆæ‹‰è¯­è¯­éŸ³è¯†åˆ«ç ”ç©¶æä¾›äº†å¯ç¤ºã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>OpenAIçš„Whisperæ¨¡å‹åœ¨èµ„æºè´«ä¹çš„é˜¿å§†å“ˆæ‹‰è¯­ä¸Šçš„è¡¨ç°æœ‰å¾…æé«˜ã€‚</li>
<li>ä½¿ç”¨Mozilla Common Voiceã€FLEURSå’ŒBDU-speechç­‰æ•°æ®é›†çš„å¾®è°ƒç­–ç•¥èƒ½æœ‰æ•ˆæå‡æ¨¡å‹åœ¨é˜¿å§†å“ˆæ‹‰è¯­ä¸Šçš„æ€§èƒ½ã€‚</li>
<li>æ··åˆä½¿ç”¨ç°æœ‰å’Œæ–°æ•°æ®é›†è¿›è¡Œå¾®è°ƒï¼Œç‰¹åˆ«æ˜¯FLEURSæ•°æ®ä¸æ–°æ•°æ®çš„ç»“åˆï¼Œèƒ½æ˜¾è‘—æé«˜æ¨¡å‹çš„æ€§èƒ½ã€‚</li>
<li>ä»…åœ¨æ–°æ•°æ®é›†ä¸Šè®­ç»ƒASRæ¨¡å‹å¯èƒ½å¯¼è‡´æ€§èƒ½ä¸ä½³ã€‚</li>
<li>å¯¹é˜¿å§†å“ˆæ‹‰è¯­çš„åŒéŸ³å­—è¿›è¡Œå½’ä¸€åŒ–å¤„ç†èƒ½æ”¹å–„è¯­éŸ³è¯†åˆ«ç»“æœçš„å‡†ç¡®æ€§ã€‚</li>
<li>æœ‰æ•ˆçš„å¾®è°ƒç­–ç•¥å’Œé€‰æ‹©åˆé€‚çš„æ•°æ®é›†ç»„æˆå¯¹äºæé«˜ä½èµ„æºè¯­è¨€çš„è¯­éŸ³è¯†åˆ«è‡³å…³é‡è¦ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.18485">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-3fb7edaea3e316e67e15eb208457d489.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e3849c2b4fcd9ee28e1b6c1206ac7c37.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b690b59afd52e0322e37dfc7b2eea545.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9b06f5e26ca6b4048b8446abac6c61c1.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="Automatic-Speech-Recognition-for-Non-Native-English-Accuracy-and-Disfluency-Handling"><a href="#Automatic-Speech-Recognition-for-Non-Native-English-Accuracy-and-Disfluency-Handling" class="headerlink" title="Automatic Speech Recognition for Non-Native English: Accuracy and   Disfluency Handling"></a>Automatic Speech Recognition for Non-Native English: Accuracy and   Disfluency Handling</h2><p><strong>Authors:Michael McGuire</strong></p>
<p>Automatic speech recognition (ASR) has been an essential component of computer assisted language learning (CALL) and computer assisted language testing (CALT) for many years. As this technology continues to develop rapidly, it is important to evaluate the accuracy of current ASR systems for language learning applications. This study assesses five cutting-edge ASR systemsâ€™ recognition of non-native accented English speech using recordings from the L2-ARCTIC corpus, featuring speakers from six different L1 backgrounds (Arabic, Chinese, Hindi, Korean, Spanish, and Vietnamese), in the form of both read and spontaneous speech. The read speech consisted of 2,400 single sentence recordings from 24 speakers, while the spontaneous speech included narrative recordings from 22 speakers. Results showed that for read speech, Whisper and AssemblyAI achieved the best accuracy with mean Match Error Rates (MER) of 0.054 and 0.056 respectively, approaching human-level accuracy. For spontaneous speech, RevAI performed best with a mean MER of 0.063. The study also examined how each system handled disfluencies such as filler words, repetitions, and revisions, finding significant variation in performance across systems and disfluency types. While processing speed varied considerably between systems, longer processing times did not necessarily correlate with better accuracy. By detailing the performance of several of the most recent, widely-available ASR systems on non-native English speech, this study aims to help language instructors and researchers understand the strengths and weaknesses of each system and identify which may be suitable for specific use cases. </p>
<blockquote>
<p>å¤šå¹´æ¥ï¼Œè¯­éŸ³è¯†åˆ«æŠ€æœ¯ï¼ˆASRï¼‰ä¸€ç›´æ˜¯è®¡ç®—æœºè¾…åŠ©è¯­è¨€å­¦ä¹ ï¼ˆCALLï¼‰å’Œè®¡ç®—æœºè¾…åŠ©è¯­è¨€æµ‹è¯•ï¼ˆCALTï¼‰çš„é‡è¦ç»„æˆéƒ¨åˆ†ã€‚éšç€è¿™é¡¹æŠ€æœ¯çš„å¿«é€Ÿå‘å±•ï¼Œè¯„ä¼°å½“å‰è¯­éŸ³è¯†åˆ«ç³»ç»Ÿåœ¨è¯­è¨€å­¦ä¹ åº”ç”¨ä¸­çš„å‡†ç¡®æ€§è‡³å…³é‡è¦ã€‚æœ¬ç ”ç©¶è¯„ä¼°äº†äº”ç§å‰æ²¿çš„è¯­éŸ³è¯†åˆ«ç³»ç»Ÿå¯¹éæ¯è¯­è‹±è¯­å£éŸ³çš„è¯†åˆ«èƒ½åŠ›ï¼Œä½¿ç”¨äº†L2-ARCTICè¯­æ–™åº“ä¸­çš„å½•éŸ³ï¼Œè¯¥è¯­æ–™åº“åŒ…å«äº†æ¥è‡ªå…­ä¸ªä¸åŒæ¯è¯­èƒŒæ™¯ï¼ˆé˜¿æ‹‰ä¼¯è¯­ã€ä¸­æ–‡ã€å°åœ°è¯­ã€éŸ©è¯­ã€è¥¿ç­ç‰™è¯­å’Œè¶Šå—è¯­ï¼‰çš„å‘éŸ³äººçš„æœ—è¯»å’Œå³å…´æ¼”è®²ã€‚æœ—è¯»éƒ¨åˆ†ç”±24åå‘éŸ³äººçš„2400ä¸ªå•å¥å½•éŸ³ç»„æˆï¼Œè€Œå³å…´æ¼”è®²éƒ¨åˆ†åˆ™åŒ…å«æ¥è‡ª22åå‘éŸ³äººçš„å™è¿°å½•éŸ³ã€‚ç»“æœè¡¨æ˜ï¼Œåœ¨æœ—è¯»éƒ¨åˆ†ï¼Œwhisperå’ŒAssemblyAIå–å¾—äº†æœ€ä½³å‡†ç¡®æ€§ï¼Œå¹³å‡åŒ¹é…é”™è¯¯ç‡ï¼ˆMERï¼‰åˆ†åˆ«ä¸º0.054å’Œ0.056ï¼Œæ¥è¿‘äººç±»æ°´å¹³çš„å‡†ç¡®æ€§ã€‚åœ¨å³å…´æ¼”è®²éƒ¨åˆ†ï¼ŒRevAIè¡¨ç°æœ€ä½³ï¼Œå¹³å‡MERä¸º0.063ã€‚è¯¥ç ”ç©¶è¿˜æ¢è®¨äº†å„ç³»ç»Ÿå¦‚ä½•å¤„ç†ä¸æµç•…ç°è±¡ï¼Œå¦‚å¡«å……è¯ã€é‡å¤å’Œä¿®æ­£ç­‰ï¼Œå‘ç°ç³»ç»Ÿä¹‹é—´çš„æ€§èƒ½å·®å¼‚å¾ˆå¤§ã€‚è™½ç„¶å¤„ç†é€Ÿåº¦åœ¨ä¸åŒç³»ç»Ÿä¹‹é—´å­˜åœ¨æ˜¾è‘—å·®å¼‚ï¼Œä½†å¤„ç†æ—¶é—´çš„é•¿çŸ­å¹¶ä¸ä¸€å®šä¸å‡†ç¡®æ€§ç›¸å…³ã€‚é€šè¿‡å¯¹å‡ ä¸ªæœ€æ–°ä¸”å¹¿æ³›å¯ç”¨çš„è¯­éŸ³è¯†åˆ«ç³»ç»Ÿåœ¨éæ¯è¯­è‹±è¯­å‘éŸ³ä¸Šçš„è¡¨ç°è¿›è¡Œè¯¦ç»†åˆ†æï¼Œæœ¬ç ”ç©¶æ—¨åœ¨å¸®åŠ©è¯­è¨€æ•™å¸ˆå’Œç ”ç©¶è€…äº†è§£å„ç³»ç»Ÿçš„ä¼˜ç¼ºç‚¹ï¼Œå¹¶ç¡®å®šå“ªäº›ç³»ç»Ÿé€‚ç”¨äºç‰¹å®šçš„ç”¨ä¾‹ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.06924v2">PDF</a> 26 pages, 10 figures</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä¸»è¦ç ”ç©¶äº†äº”ç§å‰æ²¿çš„è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ç³»ç»Ÿåœ¨éæ¯è¯­è‹±è¯­å£éŸ³è¯†åˆ«æ–¹é¢çš„æ€§èƒ½ã€‚é€šè¿‡é‡‡ç”¨L2-ARCTICè¯­æ–™åº“ä¸­çš„å½•éŸ³ï¼Œç ”ç©¶è¯„ä¼°äº†è¿™äº›ç³»ç»Ÿå¯¹ä¸åŒæ¯è¯­èƒŒæ™¯çš„è¯´è¯è€…çš„è¯†åˆ«èƒ½åŠ›ï¼ŒåŒ…æ‹¬é˜…è¯»è¯­å’Œè‡ªå‘æ€§å£è¯­ã€‚å„ç³»ç»Ÿåœ¨é˜…è¯»è¯­å’Œè‡ªå‘æ€§å£è¯­çš„è¯†åˆ«å‡†ç¡®ç‡ä¸Šå­˜åœ¨å·®å¼‚ï¼Œå…¶ä¸­æŸäº›ç³»ç»Ÿè¡¨ç°å‡ºæ¥è¿‘äººç±»æ°´å¹³çš„æ€§èƒ½ã€‚æ­¤å¤–ï¼Œç ”ç©¶è¿˜æ¢è®¨äº†å„ç³»ç»Ÿå¯¹å£è¯­ä¸æµç•…ç°è±¡çš„å¤„ç†èƒ½åŠ›ï¼Œå¦‚å¡«å……è¯ã€é‡å¤å’Œä¿®è®¢ç­‰ã€‚ç»“æœæ˜¾ç¤ºï¼Œå„ç³»ç»Ÿåœ¨å¤„ç†ä¸åŒä¸æµç•…ç°è±¡æ–¹é¢çš„è¡¨ç°å·®å¼‚æ˜¾è‘—ã€‚åŒæ—¶ï¼Œç³»ç»Ÿå¤„ç†é€Ÿåº¦ä¸è¯†åˆ«å‡†ç¡®ç‡å¹¶ä¸å®Œå…¨ç›¸å…³ã€‚æœ¬ç ”ç©¶æ—¨åœ¨ä¸ºè¯­è¨€æ•™å¸ˆå’Œç ”ç©¶äººå‘˜äº†è§£å„ç³»ç»Ÿçš„ä¼˜ç¼ºç‚¹ä»¥åŠé€‰æ‹©åˆé€‚ç³»ç»Ÿæä¾›å‚è€ƒã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>äº”ç§å‰æ²¿çš„ASRç³»ç»Ÿè¢«è¯„ä¼°åœ¨éæ¯è¯­è‹±è¯­å£éŸ³è¯†åˆ«ä¸Šçš„æ€§èƒ½ã€‚</li>
<li>è¿™äº›ç³»ç»Ÿåœ¨é˜…è¯»è¯­å’Œè‡ªå‘æ€§å£è¯­çš„è¯†åˆ«ä¸Šéƒ½è¡¨ç°å‡ºå·®å¼‚ã€‚</li>
<li>Whisperå’ŒAssemblyAIåœ¨é˜…è¯»è¯­çš„è¯†åˆ«ä¸Šè¡¨ç°æœ€ä½³ï¼Œæ¥è¿‘äººç±»æ°´å¹³ã€‚</li>
<li>RevAIåœ¨è‡ªå‘æ€§å£è¯­çš„è¯†åˆ«ä¸Šè¡¨ç°æœ€å¥½ã€‚</li>
<li>å„ç³»ç»Ÿåœ¨å¤„ç†å£è¯­ä¸æµç•…ç°è±¡ï¼ˆå¦‚å¡«å……è¯ã€é‡å¤å’Œä¿®è®¢ï¼‰æ–¹é¢çš„è¡¨ç°å·®å¼‚æ˜¾è‘—ã€‚</li>
<li>ASRç³»ç»Ÿçš„å¤„ç†é€Ÿåº¦å¹¶ä¸ç›´æ¥å…³è”å…¶è¯†åˆ«å‡†ç¡®ç‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.06924">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-68a60f9e0ddeb91bfe94df5b5523f9ff.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="MegaTTS-3-Sparse-Alignment-Enhanced-Latent-Diffusion-Transformer-for-Zero-Shot-Speech-Synthesis"><a href="#MegaTTS-3-Sparse-Alignment-Enhanced-Latent-Diffusion-Transformer-for-Zero-Shot-Speech-Synthesis" class="headerlink" title="MegaTTS 3: Sparse Alignment Enhanced Latent Diffusion Transformer for   Zero-Shot Speech Synthesis"></a>MegaTTS 3: Sparse Alignment Enhanced Latent Diffusion Transformer for   Zero-Shot Speech Synthesis</h2><p><strong>Authors:Ziyue Jiang, Yi Ren, Ruiqi Li, Shengpeng Ji, Boyang Zhang, Zhenhui Ye, Chen Zhang, Bai Jionghao, Xiaoda Yang, Jialong Zuo, Yu Zhang, Rui Liu, Xiang Yin, Zhou Zhao</strong></p>
<p>While recent zero-shot text-to-speech (TTS) models have significantly improved speech quality and expressiveness, mainstream systems still suffer from issues related to speech-text alignment modeling: 1) models without explicit speech-text alignment modeling exhibit less robustness, especially for hard sentences in practical applications; 2) predefined alignment-based models suffer from naturalness constraints of forced alignments. This paper introduces \textit{MegaTTS 3}, a TTS system featuring an innovative sparse alignment algorithm that guides the latent diffusion transformer (DiT). Specifically, we provide sparse alignment boundaries to MegaTTS 3 to reduce the difficulty of alignment without limiting the search space, thereby achieving high naturalness. Moreover, we employ a multi-condition classifier-free guidance strategy for accent intensity adjustment and adopt the piecewise rectified flow technique to accelerate the generation process. Experiments demonstrate that MegaTTS 3 achieves state-of-the-art zero-shot TTS speech quality and supports highly flexible control over accent intensity. Notably, our system can generate high-quality one-minute speech with only 8 sampling steps. Audio samples are available at <a target="_blank" rel="noopener" href="https://sditdemo.github.io/sditdemo/">https://sditdemo.github.io/sditdemo/</a>. </p>
<blockquote>
<p>è¿‘æœŸï¼Œé›¶æ ·æœ¬æ–‡æœ¬åˆ°è¯­éŸ³ï¼ˆTTSï¼‰æ¨¡å‹åœ¨è¯­éŸ³è´¨é‡å’Œè¡¨è¾¾åŠ›æ–¹é¢æœ‰äº†æ˜¾è‘—çš„æå‡ï¼Œä½†ä¸»æµç³»ç»Ÿä»ç„¶é¢ä¸´ä¸è¯­éŸ³æ–‡æœ¬å¯¹é½å»ºæ¨¡ç›¸å…³çš„é—®é¢˜ï¼š1ï¼‰æ²¡æœ‰æ˜ç¡®çš„è¯­éŸ³æ–‡æœ¬å¯¹é½å»ºæ¨¡çš„æ¨¡å‹è¡¨ç°å‡ºè¾ƒä½çš„ç¨³å¥æ€§ï¼Œç‰¹åˆ«æ˜¯åœ¨å®é™…åº”ç”¨ä¸­çš„éš¾å¥ï¼›2ï¼‰åŸºäºé¢„å®šä¹‰å¯¹é½çš„æ¨¡å‹å—åˆ°å¼ºåˆ¶å¯¹é½çš„è‡ªç„¶æ€§çº¦æŸã€‚æœ¬æ–‡ä»‹ç»äº†<em>MegaTTS 3</em>ï¼Œè¿™æ˜¯ä¸€ä¸ªå…·æœ‰åˆ›æ–°ç¨€ç–å¯¹é½ç®—æ³•çš„TTSç³»ç»Ÿï¼Œç”¨äºæŒ‡å¯¼æ½œåœ¨æ‰©æ•£å˜å‹å™¨ï¼ˆDiTï¼‰ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬ä¸ºMegaTTS 3æä¾›ç¨€ç–å¯¹é½è¾¹ç•Œï¼Œä»¥å‡å°‘å¯¹é½çš„éš¾åº¦ï¼ŒåŒæ—¶ä¸é™åˆ¶æœç´¢ç©ºé—´ï¼Œä»è€Œå®ç°é«˜è‡ªç„¶åº¦ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬é‡‡ç”¨å¤šæ¡ä»¶æ— åˆ†ç±»æŒ‡å¯¼ç­–ç•¥æ¥è°ƒæ•´å£éŸ³å¼ºåº¦ï¼Œå¹¶é‡‡ç”¨åˆ†æ®µæ•´æµæµæŠ€æœ¯æ¥åŠ é€Ÿç”Ÿæˆè¿‡ç¨‹ã€‚å®éªŒè¡¨æ˜ï¼ŒMegaTTS 3è¾¾åˆ°äº†æœ€å…ˆè¿›çš„é›¶æ ·æœ¬TTSè¯­éŸ³è´¨é‡ï¼Œå¹¶å¯¹å£éŸ³å¼ºåº¦å®ç°äº†é«˜åº¦çµæ´»çš„æ§åˆ¶ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œæˆ‘ä»¬çš„ç³»ç»Ÿåªéœ€8ä¸ªé‡‡æ ·æ­¥éª¤å°±èƒ½ç”Ÿæˆé«˜è´¨é‡çš„ä¸€åˆ†é’Ÿè¯­éŸ³ã€‚éŸ³é¢‘æ ·æœ¬å¯åœ¨<a target="_blank" rel="noopener" href="https://sditdemo.github.io/sditdemo/%E6%89%BE%E5%88%B0%E3%80%82">https://sditdemo.github.io/sditdemo/æ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.18924v4">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†æ–°ä¸€ä»£çš„æ–‡æœ¬è½¬è¯­éŸ³ï¼ˆTTSï¼‰ç³»ç»Ÿâ€”â€”MegaTTS 3ã€‚è¯¥ç³»ç»Ÿé‡‡ç”¨åˆ›æ–°çš„ç¨€ç–å¯¹é½ç®—æ³•ï¼ŒæŒ‡å¯¼æ½œåœ¨æ‰©æ•£å˜å‹å™¨ï¼ˆDiTï¼‰è¿›è¡Œè¯­éŸ³å’Œæ–‡æœ¬çš„å¯¹é½ã€‚é€šè¿‡æä¾›ç¨€ç–å¯¹é½è¾¹ç•Œï¼ŒMegaTTS 3åœ¨é™ä½å¯¹é½éš¾åº¦çš„åŒæ—¶ä¸é™åˆ¶æœç´¢ç©ºé—´ï¼Œå®ç°äº†é«˜åº¦çš„è‡ªç„¶æ€§ã€‚æ­¤å¤–ï¼Œè¿˜é‡‡ç”¨äº†å¤šæ¡ä»¶æ— åˆ†ç±»æŒ‡å¯¼ç­–ç•¥æ¥è°ƒæ•´å£éŸ³å¼ºåº¦ï¼Œå¹¶é‡‡ç”¨åˆ†æ®µæ•´æµæµæŠ€æœ¯åŠ é€Ÿç”Ÿæˆè¿‡ç¨‹ã€‚å®éªŒè¡¨æ˜ï¼ŒMegaTTS 3è¾¾åˆ°äº†é›¶æ ·æœ¬TTSçš„å…ˆè¿›è¯­éŸ³è´¨é‡ï¼Œå¹¶é«˜åº¦çµæ´»æ§åˆ¶å£éŸ³å¼ºåº¦ã€‚å°¤å…¶å€¼å¾—å…³æ³¨çš„æ˜¯ï¼Œè¯¥ç³»ç»Ÿå¯ä»¥åœ¨ä»…8ä¸ªé‡‡æ ·æ­¥éª¤å†…ç”Ÿæˆé«˜è´¨é‡çš„ä¸€åˆ†é’Ÿè¯­éŸ³ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>MegaTTS 3æ˜¯ä¸€ä¸ªæ–°å‹çš„æ–‡æœ¬è½¬è¯­éŸ³ï¼ˆTTSï¼‰ç³»ç»Ÿï¼Œä¸»è¦è§£å†³ç°æœ‰TTSç³»ç»Ÿè¯­éŸ³å’Œæ–‡æœ¬å¯¹é½çš„é—®é¢˜ã€‚</li>
<li>é€šè¿‡å¼•å…¥åˆ›æ–°çš„ç¨€ç–å¯¹é½ç®—æ³•ï¼ŒMegaTTS 3èƒ½å¤Ÿæ›´æœ‰æ•ˆåœ°è¿›è¡Œè¯­éŸ³å’Œæ–‡æœ¬çš„å¯¹é½ï¼Œæé«˜äº†ç³»ç»Ÿçš„é²æ£’æ€§ã€‚</li>
<li>ç³»ç»Ÿé‡‡ç”¨å¤šæ¡ä»¶æ— åˆ†ç±»æŒ‡å¯¼ç­–ç•¥ï¼Œå…è®¸çµæ´»è°ƒæ•´å£éŸ³å¼ºåº¦ã€‚</li>
<li>åˆ†æ®µæ•´æµæµæŠ€æœ¯çš„è¿ç”¨åŠ é€Ÿäº†è¯­éŸ³ç”Ÿæˆçš„æµç¨‹ã€‚</li>
<li>å®éªŒç»“æœè¡¨æ˜ï¼ŒMegaTTS 3çš„è¯­éŸ³è´¨é‡è¾¾åˆ°äº†é›¶æ ·æœ¬TTSçš„å…ˆè¿›æ°´å¹³ã€‚</li>
<li>MegaTTS 3èƒ½å¤Ÿåœ¨çŸ­æ—¶é—´å†…ç”Ÿæˆé«˜è´¨é‡çš„è¯­éŸ³ï¼Œä¾‹å¦‚ä»…8ä¸ªé‡‡æ ·æ­¥éª¤ç”Ÿæˆä¸€åˆ†é’Ÿçš„è¯­éŸ³ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.18924">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-6a4848ec4bc65716ed58ab74f09839dd.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f63c305d51b567c1b33e75f11ea31ced.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-799c5f97dff2642f57db63aacabdbaa7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d4dc21c79b3bf4b146770c255229fcba.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="DGSNA-prompt-based-Dynamic-Generative-Scene-based-Noise-Addition-method"><a href="#DGSNA-prompt-based-Dynamic-Generative-Scene-based-Noise-Addition-method" class="headerlink" title="DGSNA: prompt-based Dynamic Generative Scene-based Noise Addition method"></a>DGSNA: prompt-based Dynamic Generative Scene-based Noise Addition method</h2><p><strong>Authors:Zihao Chen, Zhentao Lin, Bi Zeng, Linyi Huang, Zhi Li, Jia Cai</strong></p>
<p>To ensure the reliable operation of speech systems across diverse environments, noise addition methods have emerged as the prevailing solution. However, existing methods offer limited coverage of real-world noisy scenes and depend on pre-existing scene-based information and noise. This paper presents prompt-based Dynamic Generative Scene-based Noise Addition (DGSNA), a novel noise addition methodology that integrates Dynamic Generation of Scene-based Information (DGSI) with Scene-based Noise Addition for Speech (SNAS). This integration facilitates automated scene-based noise addition by transforming clean speech into various noise environments, thereby providing a more comprehensive and realistic simulation of diverse noise conditions. Experimental results demonstrate that DGSNA significantly enhances the robustness of speech recognition and keyword spotting models across various noise conditions, achieving a relative improvement of up to 11.21%. Furthermore, DGSNA can be effectively integrated with other noise addition methods to enhance performance. Our implementation and demonstrations are available at <a target="_blank" rel="noopener" href="https://dgsna.github.io/">https://dgsna.github.io</a>. </p>
<blockquote>
<p>ä¸ºäº†ç¡®ä¿è·¨ä¸åŒç¯å¢ƒä¸­çš„è¯­éŸ³ç³»ç»Ÿå¯é è¿è¡Œï¼Œå™ªå£°æ·»åŠ æ–¹æ³•å·²ç»æˆä¸ºäº†ä¸€ç§ä¸»æµçš„è§£å†³æ–¹æ¡ˆã€‚ç„¶è€Œï¼Œç°æœ‰æ–¹æ³•æä¾›çš„çœŸå®ä¸–ç•Œå™ªå£°åœºæ™¯çš„è¦†ç›–èŒƒå›´æœ‰é™ï¼Œå¹¶ä¾èµ–äºåŸºäºåœºæ™¯çš„ä¿¡æ¯å’Œå™ªå£°ã€‚æœ¬æ–‡æå‡ºäº†åŸºäºæç¤ºçš„åŠ¨æ€ç”Ÿæˆåœºæ™¯å™ªå£°æ·»åŠ ï¼ˆDGSNAï¼‰ï¼Œè¿™æ˜¯ä¸€ç§æ–°å‹å™ªå£°æ·»åŠ æ–¹æ³•ï¼Œå®ƒå°†åŸºäºåœºæ™¯çš„åŠ¨æ€ä¿¡æ¯ç”Ÿæˆï¼ˆDGSIï¼‰ä¸åŸºäºåœºæ™¯çš„è¯­éŸ³å™ªå£°æ·»åŠ ï¼ˆSNASï¼‰ç›¸ç»“åˆã€‚è¿™ç§ç»“åˆé€šè¿‡è½¬æ¢å¹²å‡€è¯­éŸ³ä¸ºå„ç§å™ªå£°ç¯å¢ƒï¼Œå®ç°äº†åŸºäºåœºæ™¯çš„è‡ªåŠ¨å™ªå£°æ·»åŠ ï¼Œä»è€Œæä¾›äº†æ›´å…¨é¢å’Œç°å®çš„å¤šç§å™ªå£°æ¡ä»¶æ¨¡æ‹Ÿã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒDGSNAæ˜¾è‘—æé«˜äº†å„ç§å™ªå£°æ¡ä»¶ä¸‹çš„è¯­éŸ³è¯†åˆ«å’Œå…³é”®è¯è¯†åˆ«æ¨¡å‹çš„ç¨³å¥æ€§ï¼Œç›¸å¯¹æ”¹è¿›ç‡æœ€é«˜å¯è¾¾11.21%ã€‚æ­¤å¤–ï¼ŒDGSNAå¯ä»¥ä¸å…¶ä»–å™ªå£°æ·»åŠ æ–¹æ³•æœ‰æ•ˆç»“åˆä»¥æé«˜æ€§èƒ½ã€‚æˆ‘ä»¬çš„å®ç°å’Œæ¼”ç¤ºå¯åœ¨<a target="_blank" rel="noopener" href="https://dgsna.github.io/">https://dgsna.github.io</a>æŸ¥çœ‹ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2411.12363v2">PDF</a> </p>
<p><strong>Summary</strong><br>æ–‡æœ¬æå‡ºäº†ä¸€ç§åŸºäºæç¤ºçš„åŠ¨æ€ç”Ÿæˆåœºæ™¯å™ªå£°æ·»åŠ æ–¹æ³•ï¼ˆDGSNAï¼‰ï¼Œè¯¥æ–¹æ³•ç»“åˆäº†åœºæ™¯ä¿¡æ¯çš„åŠ¨æ€ç”Ÿæˆï¼ˆDGSIï¼‰å’ŒåŸºäºåœºæ™¯çš„å™ªå£°æ·»åŠ æŠ€æœ¯ï¼ˆSNASï¼‰ã€‚DGSNAå¯è‡ªåŠ¨å°†å¹²å‡€è¯­éŸ³è½¬æ¢ä¸ºå„ç§å™ªå£°ç¯å¢ƒï¼Œæä¾›æ›´å…¨é¢å’Œç°å®çš„å™ªå£°æ¡ä»¶æ¨¡æ‹Ÿã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒDGSNAå¯æ˜¾è‘—æé«˜è¯­éŸ³è¯†åˆ«å’Œå…³é”®è¯è¯†åˆ«æ¨¡å‹åœ¨å„ç§å™ªå£°æ¡ä»¶ä¸‹çš„ç¨³å¥æ€§ï¼Œç›¸å¯¹æ”¹è¿›ç‡æœ€é«˜å¯è¾¾11.21%ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>DGSNAæ˜¯ä¸€ç§æ–°çš„å™ªå£°æ·»åŠ æ–¹æ³•ï¼Œå®ƒç»“åˆäº†DGSIå’ŒSNASæŠ€æœ¯ã€‚</li>
<li>DGSNAèƒ½è‡ªåŠ¨å°†å¹²å‡€è¯­éŸ³è½¬åŒ–ä¸ºå„ç§å™ªå£°ç¯å¢ƒï¼Œå®ç°æ›´å…¨é¢å’Œç°å®çš„å™ªå£°æ¡ä»¶æ¨¡æ‹Ÿã€‚</li>
<li>å®éªŒç»“æœè¡¨æ˜ï¼ŒDGSNAèƒ½æ˜¾è‘—æé«˜è¯­éŸ³è¯†åˆ«å’Œå…³é”®è¯è¯†åˆ«æ¨¡å‹åœ¨å¤šç§å™ªå£°æ¡ä»¶ä¸‹çš„ç¨³å¥æ€§ã€‚</li>
<li>DGSNAçš„ç›¸å¯¹æ”¹è¿›ç‡æœ€é«˜å¯è¾¾11.21%ã€‚</li>
<li>DGSNAä¸å…¶ä»–å™ªå£°æ·»åŠ æ–¹æ³•ç»“åˆä½¿ç”¨ï¼Œå¯è¿›ä¸€æ­¥æå‡æ€§èƒ½ã€‚</li>
<li>DGSNAçš„å®æ–½å’Œæ¼”ç¤ºå¯åœ¨æŒ‡å®šç½‘ç«™æ‰¾åˆ°ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2411.12363">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-e22de12632ce3c2dfdafa47b86bc4e4c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9f4ca47be4f7ec2cb4c5fddbde0ac754.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="SAV-SE-Scene-aware-Audio-Visual-Speech-Enhancement-with-Selective-State-Space-Model"><a href="#SAV-SE-Scene-aware-Audio-Visual-Speech-Enhancement-with-Selective-State-Space-Model" class="headerlink" title="SAV-SE: Scene-aware Audio-Visual Speech Enhancement with Selective State   Space Model"></a>SAV-SE: Scene-aware Audio-Visual Speech Enhancement with Selective State   Space Model</h2><p><strong>Authors:Xinyuan Qian, Jiaran Gao, Yaodan Zhang, Qiquan Zhang, Hexin Liu, Leibny Paola Garcia, Haizhou Li</strong></p>
<p>Speech enhancement plays an essential role in various applications, and the integration of visual information has been demonstrated to bring substantial advantages. However, the majority of current research concentrates on the examination of facial and lip movements, which can be compromised or entirely inaccessible in scenarios where occlusions occur or when the camera view is distant. Whereas contextual visual cues from the surrounding environment have been overlooked: for example, when we see a dog bark, our brain has the innate ability to discern and filter out the barking noise. To this end, in this paper, we introduce a novel task, i.e. SAV-SE. To our best knowledge, this is the first proposal to use rich contextual information from synchronized video as auxiliary cues to indicate the type of noise, which eventually improves the speech enhancement performance. Specifically, we propose the VC-S$^2$E method, which incorporates the Conformer and Mamba modules for their complementary strengths. Extensive experiments are conducted on public MUSIC, AVSpeech and AudioSet datasets, where the results demonstrate the superiority of VC-S$^2$E over other competitive methods. We will make the source code publicly available. Project demo page: <a target="_blank" rel="noopener" href="https://avsepage.github.io/">https://AVSEPage.github.io/</a> </p>
<blockquote>
<p>è¯­éŸ³å¢å¼ºåœ¨å„ç±»åº”ç”¨ä¸­æ‰®æ¼”ç€é‡è¦è§’è‰²ï¼Œè€Œè§†è§‰ä¿¡æ¯çš„èåˆå·²è¯æ˜å¯ä»¥å¸¦æ¥å·¨å¤§çš„ä¼˜åŠ¿ã€‚ç„¶è€Œï¼Œå½“å‰å¤§å¤šæ•°ç ”ç©¶ä¸»è¦å…³æ³¨é¢éƒ¨å’Œå˜´å”‡åŠ¨ä½œçš„æ£€æŸ¥ï¼Œè¿™äº›åœ¨é®æŒ¡å‘ç”Ÿæˆ–æ‘„åƒå¤´è§†è§’è¾ƒè¿œçš„æƒ…å†µä¸‹å¯èƒ½ä¼šå—åˆ°æŸå®³æˆ–å®Œå…¨æ— æ³•è·å–ã€‚è€Œå‘¨å›´ç¯å¢ƒä¸­çš„ä¸Šä¸‹æ–‡è§†è§‰çº¿ç´¢å´è¢«å¿½è§†äº†ï¼šä¾‹å¦‚ï¼Œå½“æˆ‘ä»¬çœ‹åˆ°ç‹—å«æ—¶ï¼Œæˆ‘ä»¬çš„å¤§è„‘å°±æœ‰å¤©ç”Ÿçš„èƒ½åŠ›å»è¾¨åˆ«å’Œè¿‡æ»¤æ‰å å«å™ªéŸ³ã€‚å› æ­¤ï¼Œæœ¬æ–‡ä»‹ç»äº†ä¸€é¡¹æ–°ä»»åŠ¡ï¼Œå³SAV-SEã€‚æ®æˆ‘ä»¬æ‰€çŸ¥ï¼Œè¿™æ˜¯é¦–æ¬¡æå‡ºåˆ©ç”¨æ¥è‡ªåŒæ­¥è§†é¢‘ä¸­çš„ä¸°å¯Œä¸Šä¸‹æ–‡ä¿¡æ¯ä½œä¸ºè¾…åŠ©çº¿ç´¢ï¼Œä»¥æŒ‡ç¤ºå™ªå£°ç±»å‹ï¼Œæœ€ç»ˆæé«˜è¯­éŸ³å¢å¼ºæ€§èƒ½ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬æå‡ºäº†VC-S$^2$Eæ–¹æ³•ï¼Œè¯¥æ–¹æ³•ç»“åˆäº†Conformerå’ŒMambaæ¨¡å—ï¼Œä»¥å‘æŒ¥å®ƒä»¬çš„äº’è¡¥ä¼˜åŠ¿ã€‚åœ¨å…¬å…±MUSICã€AVSpeechå’ŒAudioSetæ•°æ®é›†ä¸Šè¿›è¡Œäº†å¤§é‡å®éªŒï¼Œç»“æœè¡¨æ˜VC-S$^2$Eä¼˜äºå…¶ä»–ç«äº‰æ–¹æ³•ã€‚æˆ‘ä»¬å°†å…¬å¼€æºä»£ç ã€‚é¡¹ç›®æ¼”ç¤ºé¡µé¢ï¼š<a target="_blank" rel="noopener" href="https://avsepage.github.io/">https://AVSEPage.github.io/</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2411.07751v2">PDF</a> accepted by IEEE Journal of Selected Topics in Signal Processing</p>
<p><strong>Summary</strong><br>è¯­éŸ³å¢å¼ºåœ¨å„ç§åº”ç”¨ä¸­å‘æŒ¥ç€é‡è¦ä½œç”¨ï¼Œé›†æˆè§†è§‰ä¿¡æ¯å¯ä»¥å¸¦æ¥æ˜æ˜¾çš„ä¼˜åŠ¿ã€‚å½“å‰ç ”ç©¶å¤šé›†ä¸­åœ¨é¢éƒ¨å’Œå˜´å”‡åŠ¨ä½œçš„è§‚å¯Ÿä¸Šï¼Œä½†åœ¨é®æŒ¡æˆ–è¿œè·ç¦»æ‹æ‘„æ—¶ï¼Œè¿™äº›ä¿¡æ¯å¯èƒ½å—é™æˆ–æ— æ³•è·å–ã€‚ç›¸è¾ƒä¹‹ä¸‹ï¼Œç¯å¢ƒä¸­çš„ä¸Šä¸‹æ–‡è§†è§‰çº¿ç´¢å¸¸è¢«å¿½è§†ã€‚æœ¬æ–‡ä»‹ç»äº†ä¸€ç§æ–°å‹ä»»åŠ¡â€”â€”SAV-SEï¼Œæ—¨åœ¨åˆ©ç”¨åŒæ­¥è§†é¢‘ä¸­çš„ä¸°å¯Œä¸Šä¸‹æ–‡ä¿¡æ¯ä½œä¸ºè¾…åŠ©çº¿ç´¢ï¼Œä»¥è¯†åˆ«å™ªå£°ç±»å‹ï¼Œä»è€Œæé«˜è¯­éŸ³å¢å¼ºçš„æ€§èƒ½ã€‚å…·ä½“æå‡ºäº†VC-S$^2$Eæ–¹æ³•ï¼Œç»“åˆäº†Conformerå’ŒMambaæ¨¡å—çš„ä¼˜åŠ¿ã€‚åœ¨å…¬å…±MUSICã€AVSpeechå’ŒAudioSetæ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼ŒVC-S$^2$Eä¼˜äºå…¶ä»–ç«äº‰æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è¯­éŸ³å¢å¼ºé›†æˆè§†è§‰ä¿¡æ¯è‡³å…³é‡è¦ï¼Œèƒ½å¸¦æ¥æ˜¾è‘—ä¼˜åŠ¿ã€‚</li>
<li>å½“å‰ç ”ç©¶ä¸»è¦å…³æ³¨é¢éƒ¨å’Œå˜´å”‡åŠ¨ä½œï¼Œä½†åœ¨ç‰¹å®šæƒ…å¢ƒä¸‹å­˜åœ¨å±€é™æ€§ã€‚</li>
<li>ä¸Šä¸‹æ–‡è§†è§‰çº¿ç´¢ï¼ˆå¦‚ç¯å¢ƒä¸­çš„å£°éŸ³ä¸è§†è§‰ä¿¡å·çš„å…³è”ï¼‰åœ¨è¯­éŸ³å¢å¼ºä¸­æ‰®æ¼”é‡è¦è§’è‰²ã€‚</li>
<li>SAV-SEæ—¨åœ¨åˆ©ç”¨åŒæ­¥è§†é¢‘çš„ä¸°å¯Œä¸Šä¸‹æ–‡ä¿¡æ¯è¯†åˆ«å™ªå£°ç±»å‹ï¼Œæé«˜è¯­éŸ³å¢å¼ºæ€§èƒ½ã€‚</li>
<li>VC-S$^2$Eæ–¹æ³•ç»“åˆäº†Conformerå’ŒMambaæ¨¡å—çš„ä¼˜åŠ¿ã€‚</li>
<li>åœ¨å¤šä¸ªå…¬å…±æ•°æ®é›†ä¸Šçš„å®éªŒè¯æ˜äº†VC-S$^2$Eæ–¹æ³•çš„ä¼˜è¶Šæ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2411.07751">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-d515743992559c37f0284db4da56b9cb.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b98d23ad8d59f497d03efc55fef69103.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-fcb71963e58fe8c6ff639227b3287cec.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="Continuous-Speech-Tokenizer-in-Text-To-Speech"><a href="#Continuous-Speech-Tokenizer-in-Text-To-Speech" class="headerlink" title="Continuous Speech Tokenizer in Text To Speech"></a>Continuous Speech Tokenizer in Text To Speech</h2><p><strong>Authors:Yixing Li, Ruobing Xie, Xingwu Sun, Yu Cheng, Zhanhui Kang</strong></p>
<p>The fusion of speech and language in the era of large language models has garnered significant attention. Discrete speech token is often utilized in text-to-speech tasks for speech compression and portability, which is convenient for joint training with text and have good compression efficiency. However, we found that the discrete speech tokenizer still suffers from information loss. Therefore, we propose a simple yet effective continuous speech tokenizer named Cont-SPT, and a text-to-speech model based on continuous speech tokens. Our results show that the speech language model based on the continuous speech tokenizer has better continuity and higher estimated Mean Opinion Scores (MoS). This enhancement is attributed to better information preservation rate of the continuous speech tokenizer across both low and high frequencies in the frequency domain. The code and resources for Cont-SPT can be found in <a target="_blank" rel="noopener" href="https://github.com/Yixing-Li/Continuous-Speech-Tokenizer">https://github.com/Yixing-Li/Continuous-Speech-Tokenizer</a> </p>
<blockquote>
<p>åœ¨å¤§å‹è¯­è¨€æ¨¡å‹æ—¶ä»£ï¼Œè¯­éŸ³å’Œè¯­è¨€çš„èåˆå¼•èµ·äº†äººä»¬çš„å¹¿æ³›å…³æ³¨ã€‚ç¦»æ•£è¯­éŸ³ä»¤ç‰Œé€šå¸¸ç”¨äºæ–‡æœ¬åˆ°è¯­éŸ³çš„ä»»åŠ¡ï¼Œä»¥å®ç°è¯­éŸ³å‹ç¼©å’Œä¾¿æºæ€§ï¼Œè¿™ä¾¿äºä¸æ–‡æœ¬è¿›è¡Œè”åˆè®­ç»ƒå¹¶å…·æœ‰è‰¯å¥½çš„å‹ç¼©æ•ˆç‡ã€‚ç„¶è€Œï¼Œæˆ‘ä»¬å‘ç°ç¦»æ•£è¯­éŸ³ä»¤ç‰ŒåŒ–ä»ç„¶å­˜åœ¨ä¿¡æ¯ä¸¢å¤±çš„é—®é¢˜ã€‚å› æ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ç®€å•æœ‰æ•ˆçš„è¿ç»­è¯­éŸ³ä»¤ç‰ŒåŒ–æ–¹æ³•ï¼Œå‘½åä¸ºCont-SPTï¼Œä»¥åŠä¸€ç§åŸºäºè¿ç»­è¯­éŸ³ä»¤ç‰Œçš„æ–‡æœ¬åˆ°è¯­éŸ³æ¨¡å‹ã€‚ç»“æœè¡¨æ˜ï¼ŒåŸºäºè¿ç»­è¯­éŸ³ä»¤ç‰Œçš„è¯­éŸ³è¯­è¨€æ¨¡å‹å…·æœ‰æ›´å¥½çš„è¿ç»­æ€§å’Œæ›´é«˜çš„é¢„ä¼°å¹³å‡æ„è§å¾—åˆ†ï¼ˆMoSï¼‰ã€‚è¿™ç§å¢å¼ºåŠŸèƒ½å½’å› äºè¿ç»­è¯­éŸ³ä»¤ç‰ŒåŒ–åœ¨ä½é¢‘å’Œé«˜é¢‘åŸŸéƒ½èƒ½æ›´å¥½åœ°ä¿æŒä¿¡æ¯ã€‚Cont-SPTçš„ä»£ç å’Œèµ„æºå¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/Yixing-Li/Continuous-Speech-Tokenizer%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/Yixing-Li/Continuous-Speech-Tokenizeræ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2410.17081v2">PDF</a> NAACL 2025 Findings Poster</p>
<p><strong>æ€»ç»“</strong></p>
<p>éšç€å¤§å‹è¯­è¨€æ¨¡å‹çš„å…´èµ·ï¼Œè¯­éŸ³å’Œè¯­è¨€çš„èåˆå¼•èµ·äº†äººä»¬çš„å¹¿æ³›å…³æ³¨ã€‚ç¦»æ•£è¯­éŸ³ä»¤ç‰Œåœ¨æ–‡æœ¬åˆ°è¯­éŸ³ä»»åŠ¡ä¸­å¸¸ç”¨äºè¯­éŸ³å‹ç¼©å’Œä¾¿æºæ€§ï¼Œä¾¿äºä¸æ–‡æœ¬è¿›è¡Œè”åˆè®­ç»ƒï¼Œå¹¶å…·æœ‰è‰¯å¥½çš„å‹ç¼©æ•ˆç‡ã€‚ç„¶è€Œï¼Œç ”ç©¶å‘ç°ç¦»æ•£è¯­éŸ³åˆ†è¯å™¨ä»å­˜åœ¨ä¿¡æ¯ä¸¢å¤±çš„é—®é¢˜ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ç®€å•æœ‰æ•ˆçš„è¿ç»­è¯­éŸ³åˆ†è¯å™¨Cont-SPTï¼Œä»¥åŠåŸºäºè¿ç»­è¯­éŸ³ä»¤ç‰Œçš„æ–‡æœ¬åˆ°è¯­éŸ³æ¨¡å‹ã€‚ç»“æœè¡¨æ˜ï¼ŒåŸºäºè¿ç»­è¯­éŸ³åˆ†è¯å™¨çš„è¯­éŸ³è¯­è¨€æ¨¡å‹å…·æœ‰æ›´å¥½çš„è¿ç»­æ€§å’Œæ›´é«˜çš„é¢„ä¼°å¹³å‡æ„è§å¾—åˆ†ï¼ˆMoSï¼‰ã€‚è¿™ä¸€æ”¹è¿›å½’å› äºè¿ç»­è¯­éŸ³åˆ†è¯å™¨åœ¨é¢‘åŸŸä¸­ä½é¢‘å’Œé«˜é¢‘æ®µçš„ä¿¡æ¯ä¿æŒç‡æ›´é«˜ã€‚Cont-SPTçš„ä»£ç å’Œèµ„æºå¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/Yixing-Li/Continuous-Speech-Tokenizer%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/Yixing-Li/Continuous-Speech-Tokenizeræ‰¾åˆ°ã€‚</a></p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>è¯­éŸ³å’Œè¯­è¨€èåˆåœ¨å¤§å‹è¯­è¨€æ¨¡å‹æ—¶ä»£å¤‡å—å…³æ³¨ã€‚</li>
<li>ç¦»æ•£è¯­éŸ³ä»¤ç‰Œåœ¨æ–‡æœ¬åˆ°è¯­éŸ³ä»»åŠ¡ä¸­ç”¨äºè¯­éŸ³å‹ç¼©å’Œä¾¿æºæ€§ã€‚</li>
<li>ç¦»æ•£è¯­éŸ³åˆ†è¯å™¨å­˜åœ¨ä¿¡æ¯ä¸¢å¤±çš„é—®é¢˜ã€‚</li>
<li>æå‡ºäº†åä¸ºCont-SPTçš„è¿ç»­è¯­éŸ³åˆ†è¯å™¨ã€‚</li>
<li>åŸºäºè¿ç»­è¯­éŸ³ä»¤ç‰Œçš„æ–‡æœ¬åˆ°è¯­éŸ³æ¨¡å‹è¡¨ç°å‡ºæ›´å¥½çš„è¿ç»­æ€§å’Œæ›´é«˜çš„é¢„ä¼°å¹³å‡æ„è§å¾—åˆ†ï¼ˆMoSï¼‰ã€‚</li>
<li>è¿ç»­è¯­éŸ³åˆ†è¯å™¨åœ¨é¢‘åŸŸä¸­å…·æœ‰è¾ƒé«˜çš„ä¿¡æ¯ä¿æŒç‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2410.17081">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-a2f6d4234ae6ceb8eced875223976719.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ac1c91fd3522e27a7a088ee987913ee4.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a486d8f9668db1bd0fc9ec5cec8797ae.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7521a02d93a4930b4722ab2d9a8daff9.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="Multi-modal-Speech-Transformer-Decoders-When-Do-Multiple-Modalities-Improve-Accuracy"><a href="#Multi-modal-Speech-Transformer-Decoders-When-Do-Multiple-Modalities-Improve-Accuracy" class="headerlink" title="Multi-modal Speech Transformer Decoders: When Do Multiple Modalities   Improve Accuracy?"></a>Multi-modal Speech Transformer Decoders: When Do Multiple Modalities   Improve Accuracy?</h2><p><strong>Authors:Yiwen Guan, Viet Anh Trinh, Vivek Voleti, Jacob Whitehill</strong></p>
<p>Decoder-only discrete-token language models have recently achieved significant success in automatic speech recognition. However, systematic analyses of how different modalities impact performance in specific scenarios remain limited. In this paper, we investigate the effects of multiple modalities on recognition accuracy on both synthetic and real-world datasets. Our experiments suggest that: (1) Integrating more modalities can increase accuracy; in particular, our paper is, to our best knowledge, the first to show the benefit of combining audio, image context, and lip information; (2) Images as a supplementary modality for speech recognition provide the greatest benefit at moderate noise levels, moreover, they exhibit a different trend compared to inherently synchronized modalities like lip movements; (3) Performance improves on both synthetic and real-world datasets when the most relevant visual information is filtered as a preprocessing step. </p>
<blockquote>
<p>åœ¨è‡ªåŠ¨è¯­éŸ³è¯†åˆ«é¢†åŸŸï¼Œä»…ä½¿ç”¨è§£ç å™¨çš„ç¦»æ•£ä»¤ç‰Œè¯­è¨€æ¨¡å‹æœ€è¿‘å–å¾—äº†é‡å¤§æˆåŠŸã€‚ç„¶è€Œï¼Œå…³äºä¸åŒæ¨¡æ€å¦‚ä½•åœ¨ç‰¹å®šåœºæ™¯ä¸­å½±å“æ€§èƒ½çš„ç³»ç»Ÿæ€§åˆ†æä»ç„¶æœ‰é™ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬ç ”ç©¶äº†å¤šç§æ¨¡æ€å¯¹åˆæˆå’Œç°å®ä¸–ç•Œæ•°æ®é›†ä¸Šè¯†åˆ«å‡†ç¡®æ€§çš„å½±å“ã€‚æˆ‘ä»¬çš„å®éªŒè¡¨æ˜ï¼šï¼ˆ1ï¼‰é›†æˆæ›´å¤šæ¨¡æ€å¯ä»¥æé«˜å‡†ç¡®æ€§ï¼›ç‰¹åˆ«æ˜¯ï¼Œæ®æˆ‘ä»¬æ‰€çŸ¥ï¼Œæœ¬æ–‡é¦–æ¬¡å±•ç¤ºäº†ç»“åˆéŸ³é¢‘ã€å›¾åƒä¸Šä¸‹æ–‡å’Œå˜´å”‡ä¿¡æ¯çš„ç›Šå¤„ï¼›ï¼ˆ2ï¼‰å›¾åƒä½œä¸ºè¯­éŸ³è¯†åˆ«çš„è¾…åŠ©æ¨¡æ€ï¼Œåœ¨ä¸­ç­‰å™ªå£°æ°´å¹³ä¸‹æä¾›çš„æ•ˆç›Šæœ€å¤§ï¼Œè€Œä¸”ä¸å˜´å”‡åŠ¨ä½œç­‰å†…åœ¨åŒæ­¥æ¨¡æ€ç›¸æ¯”ï¼Œå®ƒä»¬è¡¨ç°å‡ºä¸åŒçš„è¶‹åŠ¿ï¼›ï¼ˆ3ï¼‰å½“æœ€ç›¸å…³çš„è§†è§‰ä¿¡æ¯ä½œä¸ºé¢„å¤„ç†æ­¥éª¤è¿›è¡Œè¿‡æ»¤æ—¶ï¼Œåˆæˆå’Œç°å®ä¸–ç•Œæ•°æ®é›†ä¸Šçš„æ€§èƒ½éƒ½ä¼šæé«˜ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2409.09221v2">PDF</a> </p>
<p><strong>Summary</strong><br>     æœ¬æ–‡æ¢è®¨äº†å¤šæ¨¡æ€åœ¨åˆæˆå’ŒçœŸå®ä¸–ç•Œæ•°æ®é›†ä¸Šè¯­éŸ³è¯†åˆ«å‡†ç¡®æ€§çš„å½±å“ã€‚ç ”ç©¶å‘ç°ï¼Œæ•´åˆå¤šæ¨¡æ€ä¿¡æ¯èƒ½æé«˜è¯†åˆ«å‡†ç¡®æ€§ï¼Œç‰¹åˆ«æ˜¯ç»“åˆéŸ³é¢‘ã€å›¾åƒèƒŒæ™¯å’Œå”‡è¯­ä¿¡æ¯ï¼›å›¾åƒä½œä¸ºè¯­éŸ³è¯†åˆ«çš„è¾…åŠ©æ¨¡æ€åœ¨ä¸­ç­‰å™ªå£°æ°´å¹³ä¸‹æä¾›æœ€å¤§å¸®åŠ©ï¼Œå¹¶ä¸”ä¸å”‡åŠ¨ç­‰å†…åœ¨åŒæ­¥æ¨¡æ€å±•ç°å‡ºä¸åŒè¶‹åŠ¿ï¼›åœ¨è¿‡æ»¤æœ€ç›¸å…³çš„è§†è§‰ä¿¡æ¯ä½œä¸ºé¢„å¤„ç†æ­¥éª¤æ—¶ï¼Œåˆæˆå’ŒçœŸå®ä¸–ç•Œæ•°æ®é›†ä¸Šçš„æ€§èƒ½å‡æœ‰æ‰€æé«˜ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤šæ¨¡æ€ä¿¡æ¯çš„æ•´åˆèƒ½æé«˜è¯­éŸ³è¯†åˆ«å‡†ç¡®æ€§ã€‚</li>
<li>ç»“åˆéŸ³é¢‘ã€å›¾åƒèƒŒæ™¯å’Œå”‡è¯­ä¿¡æ¯èƒ½å¸¦æ¥æœ€ä½³è¯†åˆ«æ•ˆæœã€‚</li>
<li>å›¾åƒä½œä¸ºè¯­éŸ³è¯†åˆ«è¾…åŠ©æ¨¡æ€åœ¨ä¸­ç­‰å™ªå£°ç¯å¢ƒä¸‹æ•ˆç›Šæœ€å¤§ã€‚</li>
<li>ä¸å†…åœ¨åŒæ­¥æ¨¡æ€å¦‚å”‡åŠ¨ç›¸æ¯”ï¼Œå›¾åƒæ¨¡æ€å±•ç°å‡ºä¸åŒè¶‹åŠ¿ã€‚</li>
<li>è¿‡æ»¤æœ€ç›¸å…³çš„è§†è§‰ä¿¡æ¯ä½œä¸ºé¢„å¤„ç†èƒ½æé«˜è¯†åˆ«æ€§èƒ½ã€‚</li>
<li>ä¸Šè¿°å‘ç°åœ¨åˆæˆå’ŒçœŸå®ä¸–ç•Œæ•°æ®é›†ä¸Šéƒ½å¾—åˆ°äº†éªŒè¯ã€‚</li>
<li>è¯¥ç ”ç©¶ä¸ºè¯­éŸ³è¯†åˆ«ä¸­çš„å¤šæ¨¡æ€èåˆæä¾›äº†æ–°è§è§£ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2409.09221">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-b0bafdeb6780cbecb1d6946db37408f0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-24bd934af7376e7ba1d87df26c16c8fc.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-064b23bff11e7ddbdea0ae473c99fbe7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c3ddb7e89b731e981061fc5224f1f57d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d0cd001349ee2c56f86b5ce767c93af8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8823ea5bbfb3feab68473343aed2150f.jpg" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="Large-Language-Model-Can-Transcribe-Speech-in-Multi-Talker-Scenarios-with-Versatile-Instructions"><a href="#Large-Language-Model-Can-Transcribe-Speech-in-Multi-Talker-Scenarios-with-Versatile-Instructions" class="headerlink" title="Large Language Model Can Transcribe Speech in Multi-Talker Scenarios   with Versatile Instructions"></a>Large Language Model Can Transcribe Speech in Multi-Talker Scenarios   with Versatile Instructions</h2><p><strong>Authors:Lingwei Meng, Shujie Hu, Jiawen Kang, Zhaoqing Li, Yuejiao Wang, Wenxuan Wu, Xixin Wu, Xunying Liu, Helen Meng</strong></p>
<p>Recent advancements in large language models (LLMs) have revolutionized various domains, bringing significant progress and new opportunities. Despite progress in speech-related tasks, LLMs have not been sufficiently explored in multi-talker scenarios. In this work, we present a pioneering effort to investigate the capability of LLMs in transcribing speech in multi-talker environments, following versatile instructions related to multi-talker automatic speech recognition (ASR), target talker ASR, and ASR based on specific talker attributes such as sex, occurrence order, language, and keyword spoken. Our approach utilizes WavLM and Whisper encoder to extract multi-faceted speech representations that are sensitive to speaker characteristics and semantic context. These representations are then fed into an LLM fine-tuned using LoRA, enabling the capabilities for speech comprehension and transcription. Comprehensive experiments reveal the promising performance of our proposed system, MT-LLM, in cocktail party scenarios, highlighting the potential of LLM to handle speech-related tasks based on user instructions in such complex settings. The code, model, and samples are available at <a target="_blank" rel="noopener" href="https://github.com/cuhealthybrains/MT-LLM">https://github.com/cuhealthybrains/MT-LLM</a>. </p>
<blockquote>
<p>è¿‘æœŸå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„è¿›æ­¥å·²ç»é¢ è¦†äº†å¤šä¸ªé¢†åŸŸï¼Œå¸¦æ¥äº†æ˜¾è‘—çš„è¿›æ­¥å’Œæ–°æœºé‡ã€‚å°½ç®¡åœ¨è¯­éŸ³ç›¸å…³ä»»åŠ¡ä¸Šæœ‰æ‰€è¿›å±•ï¼Œä½†LLMåœ¨å¤šè¯´è¯äººåœºæ™¯ä¸­çš„åº”ç”¨å°šæœªå¾—åˆ°å……åˆ†æ¢ç´¢ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬ç‡å…ˆç ”ç©¶äº†LLMåœ¨å¤šè¯´è¯äººç¯å¢ƒä¸‹çš„è¯­éŸ³è¯†åˆ«èƒ½åŠ›ã€‚æˆ‘ä»¬éµå¾ªä¸å¤šè¯´è¯äººè‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰ã€ç›®æ ‡è¯´è¯äººASRä»¥åŠåŸºäºç‰¹å®šè¯´è¯äººå±æ€§ï¼ˆå¦‚æ€§åˆ«ã€å‘è¨€é¡ºåºã€è¯­è¨€å’Œå…³é”®è¯ï¼‰çš„ASRç›¸å…³çš„é€šç”¨æŒ‡ä»¤ã€‚æˆ‘ä»¬çš„æ–¹æ³•åˆ©ç”¨WavLMå’ŒWhisperç¼–ç å™¨æå–å¤šé¢è¯­éŸ³è¡¨ç¤ºï¼Œè¿™äº›è¡¨ç¤ºå¯¹è¯´è¯äººç‰¹æ€§å’Œè¯­ä¹‰ä¸Šä¸‹æ–‡æ•æ„Ÿã€‚ç„¶åï¼Œè¿™äº›è¡¨ç¤ºè¢«è¾“å…¥åˆ°ä½¿ç”¨LoRAå¾®è°ƒè¿‡çš„LLMä¸­ï¼Œä»¥å®ç°è¯­éŸ³ç†è§£å’Œè½¬å½•åŠŸèƒ½ã€‚ç»¼åˆå®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬æå‡ºçš„MT-LLMç³»ç»Ÿåœ¨é¸¡å°¾é…’ä¼šåœºæ™¯ä¸­è¡¨ç°å‡ºæœ‰å‰æ™¯çš„æ€§èƒ½ï¼Œçªæ˜¾äº†LLMåœ¨å¤æ‚ç¯å¢ƒä¸­æ ¹æ®ç”¨æˆ·æŒ‡ä»¤å¤„ç†è¯­éŸ³ç›¸å…³ä»»åŠ¡çš„æ½œåŠ›ã€‚ä»£ç ã€æ¨¡å‹å’Œæ ·æœ¬å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/cuhealthybrains/MT-LLM%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/cuhealthybrains/MT-LLMæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2409.08596v2">PDF</a> Accepted to IEEE ICASSP 2025. Update code link</p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„è¿‘æœŸè¿›å±•ä¸ºå„é¢†åŸŸå¸¦æ¥äº†é©å‘½æ€§çš„å˜é©ï¼Œä¿ƒè¿›äº†æ˜¾è‘—è¿›æ­¥å¹¶ä¸ºæ–°çš„æœºä¼šæ•å¼€äº†å¤§é—¨ã€‚å°½ç®¡åœ¨è¯­éŸ³ç›¸å…³ä»»åŠ¡æ–¹é¢æœ‰æ‰€è¿›å±•ï¼Œä½†LLMåœ¨å¤šè¯´è¯äººåœºæ™¯ä¸­çš„è½¬å½•åº”ç”¨å°šæœªå¾—åˆ°å……åˆ†æ¢ç´¢ã€‚æœ¬ç ”ç©¶è‡´åŠ›äºæ¢ç´¢LLMåœ¨å¤šå˜ç¯å¢ƒä¸‹çš„è¯­éŸ³è½¬å½•èƒ½åŠ›ï¼ŒåŒ…æ‹¬ä¸å¤šè¯´è¯äººè‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰ã€ç›®æ ‡è¯´è¯äººASRä»¥åŠåŸºäºç‰¹å®šè¯´è¯äººå±æ€§ï¼ˆå¦‚æ€§åˆ«ã€å‘ç”Ÿé¡ºåºã€è¯­è¨€å’Œå…³é”®è¯ï¼‰çš„ASRç›¸å…³çš„çµæ´»æŒ‡ä»¤ã€‚æˆ‘ä»¬çš„æ–¹æ³•åˆ©ç”¨WavLMå’ŒWhisperç¼–ç å™¨æå–å¤šé¢è¯­éŸ³è¡¨ç¤ºï¼Œè¿™äº›è¡¨ç¤ºå¯¹å‘è¨€è€…çš„ç‰¹æ€§å’Œè¯­ä¹‰ä¸Šä¸‹æ–‡éƒ½å¾ˆæ•æ„Ÿã€‚ç„¶åï¼Œé€šè¿‡LoRAå¾®è°ƒè¿™äº›è¡¨ç¤ºï¼Œä½¿å…¶å…·å¤‡ç†è§£å¹¶è½¬å½•è¯­éŸ³çš„èƒ½åŠ›ã€‚ç»¼åˆå®éªŒæ˜¾ç¤ºï¼Œæˆ‘ä»¬çš„ç³»ç»ŸMT-LLMåœ¨é¸¡å°¾é…’ä¼šåœºæ™¯ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œçªæ˜¾äº†LLMåœ¨å¤æ‚ç¯å¢ƒä¸­å¤„ç†åŸºäºç”¨æˆ·æŒ‡ä»¤çš„è¯­éŸ³ç›¸å…³ä»»åŠ¡çš„æ½œåŠ›ã€‚ä»£ç ã€æ¨¡å‹å’Œæ ·æœ¬å¯åœ¨GitHubä¸Šè·å–ï¼š<a target="_blank" rel="noopener" href="https://github.com/cuhealthybrains/MT-LLM">https://github.com/cuhealthybrains/MT-LLM</a>ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨è¯­éŸ³ç›¸å…³ä»»åŠ¡ä¸­çš„åº”ç”¨æ­£åœ¨å¿«é€Ÿå‘å±•ã€‚</li>
<li>LLMåœ¨å¤šè¯´è¯äººåœºæ™¯ä¸­çš„è¯­éŸ³è½¬å½•å°šæœªå……åˆ†æ¢ç´¢ã€‚</li>
<li>æœ¬ç ”ç©¶æ¢ç´¢äº†LLMåœ¨å¤šè¯´è¯äººç¯å¢ƒä¸­çš„è¯­éŸ³è½¬å½•èƒ½åŠ›ã€‚</li>
<li>åˆ©ç”¨WavLMå’ŒWhisperç¼–ç å™¨æå–å¤šé¢è¯­éŸ³è¡¨ç¤ºä»¥åŒºåˆ†ä¸åŒè¯´è¯è€…ã€‚</li>
<li>åˆ©ç”¨LoRAå¾®è°ƒLLMä»¥ç†è§£å¹¶è½¬å½•è¯­éŸ³ã€‚</li>
<li>ç»¼åˆå®éªŒæ˜¾ç¤ºMT-LLMç³»ç»Ÿåœ¨å¤æ‚åœºæ™¯ä¸‹çš„å‡ºè‰²è¡¨ç°ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2409.08596">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-90769faa15e8000ac109981a8ea4355e.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-c0e6a17397d641a1f25d7fda434b6901.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-c27bd9d1095cbd407afa90e10b1aaa80.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e24b088cf8479fb0ba985763cd641b37.jpg" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="Medical-Spoken-Named-Entity-Recognition"><a href="#Medical-Spoken-Named-Entity-Recognition" class="headerlink" title="Medical Spoken Named Entity Recognition"></a>Medical Spoken Named Entity Recognition</h2><p><strong>Authors:Khai Le-Duc, David Thulke, Hung-Phong Tran, Long Vo-Dang, Khai-Nguyen Nguyen, Truong-Son Hy, Ralf SchlÃ¼ter</strong></p>
<p>Spoken Named Entity Recognition (NER) aims to extract named entities from speech and categorise them into types like person, location, organization, etc. In this work, we present VietMed-NER - the first spoken NER dataset in the medical domain. To our knowledge, our Vietnamese real-world dataset is the largest spoken NER dataset in the world regarding the number of entity types, featuring 18 distinct types. Furthermore, we present baseline results using various state-of-the-art pre-trained models: encoder-only and sequence-to-sequence; and conduct quantitative and qualitative error analysis. We found that pre-trained multilingual models generally outperform monolingual models on reference text and ASR output and encoders outperform sequence-to-sequence models in NER tasks. By translating the transcripts, the dataset can also be utilised for text NER in the medical domain in other languages than Vietnamese. All code, data and models are publicly available: <a target="_blank" rel="noopener" href="https://github.com/leduckhai/MultiMed/tree/master/VietMed-NER">https://github.com/leduckhai/MultiMed/tree/master/VietMed-NER</a>. </p>
<blockquote>
<p>è¯­éŸ³å‘½åå®ä½“è¯†åˆ«ï¼ˆNERï¼‰æ—¨åœ¨ä»è¯­éŸ³ä¸­æå–å‘½åå®ä½“ï¼Œå¹¶å°†å…¶åˆ†ç±»ä¸ºäººç‰©ã€åœ°ç‚¹ã€ç»„ç»‡ç­‰ç±»å‹ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æ¨å‡ºäº†VietMed-NERâ€”â€”åŒ»ç–—é¢†åŸŸçš„é¦–ä¸ªè¯­éŸ³NERæ•°æ®é›†ã€‚æ®æˆ‘ä»¬æ‰€çŸ¥ï¼Œæˆ‘ä»¬çš„è¶Šå—ç°å®æ•°æ®é›†æ˜¯ä¸–ç•Œä¸Šå®ä½“ç±»å‹æ•°é‡æœ€å¤šçš„è¯­éŸ³NERæ•°æ®é›†ï¼Œå…·æœ‰18ç§ä¸åŒçš„ç±»å‹ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬ä½¿ç”¨å„ç§æœ€å…ˆè¿›çš„é¢„è®­ç»ƒæ¨¡å‹å±•ç¤ºäº†åŸºçº¿ç»“æœï¼šä»…ç¼–ç å™¨æ¨¡å‹å’Œåºåˆ—åˆ°åºåˆ—æ¨¡å‹ï¼›å¹¶è¿›è¡Œå®šé‡å’Œå®šæ€§è¯¯å·®åˆ†æã€‚æˆ‘ä»¬å‘ç°ï¼Œåœ¨å‚è€ƒæ–‡æœ¬å’ŒASRè¾“å‡ºä¸Šï¼Œé¢„è®­ç»ƒçš„å¤šè¯­è¨€æ¨¡å‹é€šå¸¸ä¼˜äºå•è¯­è¨€æ¨¡å‹ï¼Œå¹¶ä¸”åœ¨NERä»»åŠ¡ä¸­ï¼Œç¼–ç å™¨æ¨¡å‹çš„æ€§èƒ½ä¼˜äºåºåˆ—åˆ°åºåˆ—æ¨¡å‹ã€‚é€šè¿‡ç¿»è¯‘å½•éŸ³ï¼Œè¯¥æ•°æ®é›†ä¹Ÿå¯ç”¨äºé™¤è¶Šå—è¯­ä»¥å¤–çš„å…¶ä»–è¯­è¨€çš„åŒ»ç–—é¢†åŸŸæ–‡æœ¬NERã€‚æ‰€æœ‰ä»£ç ã€æ•°æ®å’Œæ¨¡å‹å¯å…¬å¼€è®¿é—®ï¼š<a target="_blank" rel="noopener" href="https://github.com/leduckhai/MultiMed/tree/master/VietMed-NER%E3%80%82">https://github.com/leduckhai/MultiMed/tree/master/VietMed-NERã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2406.13337v3">PDF</a> NAACL 2025, 60 pages</p>
<p><strong>Summary</strong></p>
<p>è¶Šå—è¯­åŒ»ç–—é¢†åŸŸå£è¯­å‘½åå®ä½“è¯†åˆ«æ•°æ®é›†VietMed-NERé¢ä¸–ï¼ŒåŒ…å«18ç§ä¸åŒå®ä½“ç±»å‹ï¼Œä¸ºä¸–ç•Œä¸Šæœ€å¤§çš„åŒ»ç–—é¢†åŸŸå£è¯­NERæ•°æ®é›†ã€‚ç ”ç©¶é‡‡ç”¨æœ€æ–°é¢„è®­ç»ƒæ¨¡å‹ï¼Œå‘ç°å¤šè¯­è¨€é¢„è®­ç»ƒæ¨¡å‹åœ¨å‚è€ƒæ–‡æœ¬å’Œè‡ªåŠ¨è¯­éŸ³è¯†åˆ«è¾“å‡ºä¸Šé€šå¸¸ä¼˜äºå•è¯­è¨€æ¨¡å‹ï¼Œç¼–ç å™¨åœ¨å‘½åå®ä½“è¯†åˆ«ä»»åŠ¡ä¸Šè¡¨ç°ä¼˜äºåºåˆ—åˆ°åºåˆ—æ¨¡å‹ã€‚æ•°æ®é›†å¯é€šè¿‡ç¿»è¯‘è½¬å½•æœ¬ç”¨äºå…¶ä»–è¯­è¨€åŒ»ç–—æ–‡æœ¬å‘½åå®ä½“è¯†åˆ«ç ”ç©¶ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<p>1.VietMed-NERæ˜¯åŒ»ç–—é¢†åŸŸçš„é¦–ä¸ªè¶Šå—è¯­å£è¯­å‘½åå®ä½“è¯†åˆ«æ•°æ®é›†ã€‚<br>2.VietMed-NERåŒ…å«18ç§ä¸åŒçš„å®ä½“ç±»å‹ï¼Œä¸ºä¸–ç•Œä¸Šæœ€å¤§çš„åŒ»ç–—é¢†åŸŸå£è¯­NERæ•°æ®é›†ã€‚<br>3.ç ”ç©¶ä½¿ç”¨äº†æœ€æ–°çš„é¢„è®­ç»ƒæ¨¡å‹ï¼ŒåŒ…æ‹¬ç¼–ç å™¨æ¨¡å‹å’Œåºåˆ—åˆ°åºåˆ—æ¨¡å‹ã€‚<br>4.å¤šè¯­è¨€é¢„è®­ç»ƒæ¨¡å‹åœ¨å‚è€ƒæ–‡æœ¬å’Œè‡ªåŠ¨è¯­éŸ³è¯†åˆ«è¾“å‡ºä¸Šçš„è¡¨ç°é€šå¸¸ä¼˜äºå•è¯­è¨€æ¨¡å‹ã€‚<br>5.ç¼–ç å™¨åœ¨å‘½åå®ä½“è¯†åˆ«ä»»åŠ¡ä¸Šçš„è¡¨ç°ä¼˜äºåºåˆ—åˆ°åºåˆ—æ¨¡å‹ã€‚<br>6.VietMed-NERæ•°æ®é›†é€šè¿‡ç¿»è¯‘è½¬å½•æœ¬ä¹Ÿå¯ç”¨äºå…¶ä»–è¯­è¨€çš„åŒ»ç–—æ–‡æœ¬å‘½åå®ä½“è¯†åˆ«ç ”ç©¶ã€‚</p>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2406.13337">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-2bad8508ccfe6e2a372ece2b0198a298.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6a9b6c4f7017c58154cd697b22256afe.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-77eeb057efe6addf845dff7b975f5f4a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-49ca730068c82d726fea60e139aa1ad7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7116f9fab3f17e9f75b75e7e01daeed8.jpg" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-04-03/Speech/">https://kedreamix.github.io/Talk2Paper/Paper/2025-04-03/Speech/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Speech/">
                                    <span class="chip bg-color">Speech</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-04-04/Face%20Swapping/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-fd07040bae99422e0e799efb3713571e.jpg" class="responsive-img" alt="Face Swapping">
                        
                        <span class="card-title">Face Swapping</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Face Swapping æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-04-04  High-Fidelity Diffusion Face Swapping with ID-Constrained Facial   Conditioning
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-04-04
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Face-Swapping/" class="post-category">
                                    Face Swapping
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Face-Swapping/">
                        <span class="chip bg-color">Face Swapping</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-04-04/%E6%A3%80%E6%B5%8B_%E5%88%86%E5%89%B2_%E8%B7%9F%E8%B8%AA/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-14422b131c837ebcd7e30e889d0cab24.jpg" class="responsive-img" alt="æ£€æµ‹/åˆ†å‰²/è·Ÿè¸ª">
                        
                        <span class="card-title">æ£€æµ‹/åˆ†å‰²/è·Ÿè¸ª</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            æ£€æµ‹/åˆ†å‰²/è·Ÿè¸ª æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-04-04  View-Invariant Pixelwise Anomaly Detection in Multi-object Scenes with   Adaptive View Synthesis
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-04-04
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E6%A3%80%E6%B5%8B-%E5%88%86%E5%89%B2-%E8%B7%9F%E8%B8%AA/" class="post-category">
                                    æ£€æµ‹/åˆ†å‰²/è·Ÿè¸ª
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E6%A3%80%E6%B5%8B-%E5%88%86%E5%89%B2-%E8%B7%9F%E8%B8%AA/">
                        <span class="chip bg-color">æ£€æµ‹/åˆ†å‰²/è·Ÿè¸ª</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">25156.8k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
