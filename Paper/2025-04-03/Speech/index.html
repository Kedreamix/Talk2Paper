<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Speech">
    <meta name="description" content="Speech 方向最新论文已更新，请持续关注 Update in 2025-04-04  Foundations and Evaluations in NLP">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Speech | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loader页面消失采用渐隐的方式*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logo出现动画 */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//使用渐隐的方法淡出loading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//强制显示loading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">嘘~  正在从服务器偷取页面 . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>首页</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>标签</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>分类</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>归档</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="搜索" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="深色/浅色模式" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			首页
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			标签
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			分类
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			归档
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-e24b088cf8479fb0ba985763cd641b37.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Speech</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- 文章内容详情 -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Speech/">
                                <span class="chip bg-color">Speech</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Speech/" class="post-category">
                                Speech
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp;
                    2025-04-04
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>更新日期:&nbsp;&nbsp;
                    2025-05-14
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>文章字数:&nbsp;&nbsp;
                    18k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>阅读时长:&nbsp;&nbsp;
                    74 分
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>阅读次数:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>⚠️ 以下所有内容总结都来自于 大语言模型的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p>
</blockquote>
<h1 id="2025-04-04-更新"><a href="#2025-04-04-更新" class="headerlink" title="2025-04-04 更新"></a>2025-04-04 更新</h1><h2 id="Foundations-and-Evaluations-in-NLP"><a href="#Foundations-and-Evaluations-in-NLP" class="headerlink" title="Foundations and Evaluations in NLP"></a>Foundations and Evaluations in NLP</h2><p><strong>Authors:Jungyeul Park</strong></p>
<p>This memoir explores two fundamental aspects of Natural Language Processing (NLP): the creation of linguistic resources and the evaluation of NLP system performance. Over the past decade, my work has focused on developing a morpheme-based annotation scheme for the Korean language that captures linguistic properties from morphology to semantics. This approach has achieved state-of-the-art results in various NLP tasks, including part-of-speech tagging, dependency parsing, and named entity recognition. Additionally, this work provides a comprehensive analysis of segmentation granularity and its critical impact on NLP system performance. In parallel with linguistic resource development, I have proposed a novel evaluation framework, the jp-algorithm, which introduces an alignment-based method to address challenges in preprocessing tasks like tokenization and sentence boundary detection (SBD). Traditional evaluation methods assume identical tokenization and sentence lengths between gold standards and system outputs, limiting their applicability to real-world data. The jp-algorithm overcomes these limitations, enabling robust end-to-end evaluations across a variety of NLP tasks. It enhances accuracy and flexibility by incorporating linear-time alignment while preserving the complexity of traditional evaluation metrics. This memoir provides key insights into the processing of morphologically rich languages, such as Korean, while offering a generalizable framework for evaluating diverse end-to-end NLP systems. My contributions lay the foundation for future developments, with broader implications for multilingual resource development and system evaluation. </p>
<blockquote>
<p>这篇回忆录探讨了自然语言处理（NLP）的两个基本方面：语言资源的创建和NLP系统性能的评价。过去十年，我的工作主要集中在为韩语开发一种基于词素的标注方案，该方案能够捕捉从形态到语义的语言特性。该方法在各种NLP任务中达到了最新水平，包括词性标注、依存解析和命名实体识别。此外，这项工作还全面分析了分词粒度及其对NLP系统性能的关键影响。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.01342v1">PDF</a> </p>
<p><strong>Summary</strong>：<br>本回忆录探讨了自然语言处理（NLP）的两个基本方面：语言资源的创建和NLP系统性能的评价。作者以过去十年的工作经验为基础，介绍了为韩语开发基于语素的注释方案，该方案在多种NLP任务中取得了最先进的成果，包括词性标注、依存解析和命名实体识别。同时，作者还全面分析了分词粒度对NLP系统性能的关键影响。此外，作者提出了一个新颖的评价框架jp-algorithm，该框架引入了对齐方法，解决了预处理任务中的挑战，如分词和句子边界检测。此回忆录为处理形态丰富的语言（如韩语）提供了关键见解，并为评估多种端到端的NLP系统提供了可推广的框架。</p>
<p><strong>Key Takeaways</strong>：</p>
<ol>
<li>作者介绍了其韩语语素注释方案及其在多种NLP任务中的卓越表现。</li>
<li>分词粒度对NLP系统性能具有重要影响。</li>
<li>作者提出了jp-algorithm评价框架，解决了传统评价方法的局限性。</li>
<li>jp-algorithm通过引入对齐方法，提高了评估的准确性和灵活性。</li>
<li>此回忆录为处理形态丰富的语言提供了重要见解。</li>
<li>作者的贡献为未来多语言资源开发和系统评价奠定了基础。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.01342">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-108fb837911aab3d37f497065f1e3ce7.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="SViQA-A-Unified-Speech-Vision-Multimodal-Model-for-Textless-Visual-Question-Answering"><a href="#SViQA-A-Unified-Speech-Vision-Multimodal-Model-for-Textless-Visual-Question-Answering" class="headerlink" title="SViQA: A Unified Speech-Vision Multimodal Model for Textless Visual   Question Answering"></a>SViQA: A Unified Speech-Vision Multimodal Model for Textless Visual   Question Answering</h2><p><strong>Authors:Bingxin Li</strong></p>
<p>Multimodal models integrating speech and vision hold significant potential for advancing human-computer interaction, particularly in Speech-Based Visual Question Answering (SBVQA) where spoken questions about images require direct audio-visual understanding. Existing approaches predominantly focus on text-visual integration, leaving speech-visual modality gaps underexplored due to their inherent heterogeneity. To this end, we introduce SViQA, a unified speech-vision model that directly processes spoken questions without text transcription. Building upon the LLaVA architecture, our framework bridges auditory and visual modalities through two key innovations: (1) end-to-end speech feature extraction eliminating intermediate text conversion, and (2) cross-modal alignment optimization enabling effective fusion of speech signals with visual content. Extensive experimental results on the SBVQA benchmark demonstrate the proposed SViQA’s state-of-the-art performance, achieving 75.62% accuracy, and competitive multimodal generalization. Leveraging speech-text mixed input boosts performance to 78.85%, a 3.23% improvement over pure speech input, highlighting SViQA’s enhanced robustness and effective cross-modal attention alignment. </p>
<blockquote>
<p>多模态模型集成了语音和视觉，在推进人机交互方面拥有巨大潜力，特别是在基于语音的视觉问答（SBVQA）中，关于图像的口语问题需要直接音视频的感知。现有方法主要集中在文本视觉集成上，由于内在异质性导致语音视觉模态差距尚未得到充分探索。为此，我们引入了SViQA，这是一个统一的语音视觉模型，可以直接处理口语问题而无需文本转录。我们的框架建立在LLaVA架构之上，通过两个关键创新点桥梁听觉和视觉模式：（1）端到端的语音特征提取消除了中间文本转换，以及（2）跨模态对齐优化，使语音信号与视觉内容的融合有效。在SBVQA基准测试上的大量实验结果表明，所提出的SViQA具有最先进的性能，达到了75.62％的准确率，并且在多模态推广方面具有竞争力。利用语音文本混合输入将性能提高到78.85％，相较于纯语音输入提高了3.23％，突显了SViQA增强的稳健性和有效的跨模态注意力对齐。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.01049v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文介绍了SViQA模型在多模态语音识别和视觉问答（SBVQA）方面的应用。该模型直接处理语音问题，无需文本转录，通过两个关键创新点——端到端的语音特征提取和跨模态对齐优化，实现了对语音信号与视觉内容的融合。实验结果证明了SViQA模型在SBVQA上的先进性能和对多模态推广的竞争力。混合使用语音和文字输入能进一步提高模型性能。</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>SViQA模型将语音识别与视觉整合在一个统一框架内，可直接处理语音问题而无需文本转录。</li>
<li>通过两个关键创新实现了语音信号与视觉内容的融合：端到端的语音特征提取和跨模态对齐优化。</li>
<li>实验结果展示了SViQA在SBVQA上的卓越性能，达到了75.62%的准确率。</li>
<li>当使用混合语音和文字输入时，SViQA的性能提升到78.85%，显示出其在跨模态注意力对齐方面的优势。</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.01049">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-62c9aad66da13d70bac555643b32f4f8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-73e12d169cb6e03a83d5e9c041f18c99.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b05c027e650dd9890941ef645b6bf7e7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d401569fefb4da81f70bb4680fbf8509.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Whispering-Under-the-Eaves-Protecting-User-Privacy-Against-Commercial-and-LLM-powered-Automatic-Speech-Recognition-Systems"><a href="#Whispering-Under-the-Eaves-Protecting-User-Privacy-Against-Commercial-and-LLM-powered-Automatic-Speech-Recognition-Systems" class="headerlink" title="Whispering Under the Eaves: Protecting User Privacy Against Commercial   and LLM-powered Automatic Speech Recognition Systems"></a>Whispering Under the Eaves: Protecting User Privacy Against Commercial   and LLM-powered Automatic Speech Recognition Systems</h2><p><strong>Authors:Weifei Jin, Yuxin Cao, Junjie Su, Derui Wang, Yedi Zhang, Minhui Xue, Jie Hao, Jin Song Dong, Yixian Yang</strong></p>
<p>The widespread application of automatic speech recognition (ASR) supports large-scale voice surveillance, raising concerns about privacy among users. In this paper, we concentrate on using adversarial examples to mitigate unauthorized disclosure of speech privacy thwarted by potential eavesdroppers in speech communications. While audio adversarial examples have demonstrated the capability to mislead ASR models or evade ASR surveillance, they are typically constructed through time-intensive offline optimization, restricting their practicality in real-time voice communication. Recent work overcame this limitation by generating universal adversarial perturbations (UAPs) and enhancing their transferability for black-box scenarios. However, they introduced excessive noise that significantly degrades audio quality and affects human perception, thereby limiting their effectiveness in practical scenarios. To address this limitation and protect live users’ speech against ASR systems, we propose a novel framework, AudioShield. Central to this framework is the concept of Transferable Universal Adversarial Perturbations in the Latent Space (LS-TUAP). By transferring the perturbations to the latent space, the audio quality is preserved to a large extent. Additionally, we propose target feature adaptation to enhance the transferability of UAPs by embedding target text features into the perturbations. Comprehensive evaluation on four commercial ASR APIs (Google, Amazon, iFlytek, and Alibaba), three voice assistants, two LLM-powered ASR and one NN-based ASR demonstrates the protection superiority of AudioShield over existing competitors, and both objective and subjective evaluations indicate that AudioShield significantly improves the audio quality. Moreover, AudioShield also shows high effectiveness in real-time end-to-end scenarios, and demonstrates strong resilience against adaptive countermeasures. </p>
<blockquote>
<p>自动语音识别（ASR）的广泛应用支持大规模语音监控，引发了用户对于隐私的担忧。本文专注于使用对抗性样本减轻语音通信中潜在窃听者擅自泄露语音隐私的问题。虽然音频对抗性样本已展现出干扰ASR模型或躲避ASR监控的能力，但它们通常是通过耗时的离线优化构建的，这限制了它们在实时语音通信中的实用性。最近的工作通过生成通用对抗性扰动（UAPs）并提高其针对黑箱场景的迁移性来克服这一限制。然而，它们引入了过多的噪声，显著降低了音频质量并影响了人类感知，从而限制了它们在实际情况中的有效性。为了解决这一限制并保护实时用户的语音免受ASR系统的影响，我们提出了新型框架AudioShield。该框架的核心概念是潜在空间中的可迁移通用对抗性扰动（LS-TUAP）。通过将扰动转移到潜在空间，可以在很大程度上保留音频质量。此外，我们通过将目标文本特征嵌入到扰动中，提出了目标特征适应方法来增强UAPs的迁移性。对四个商业ASR API（Google、Amazon、iFlytek和Alibaba）、三个语音助手、两个基于大型语言模型的ASR和一个基于神经网络的ASR的全面评估表明，AudioShield的保护性能优于现有竞争对手。客观和主观评估均表明，AudioShield显著提高了音频质量。此外，AudioShield在实时端到端场景中表现出高有效性，并对自适应对策表现出强烈抗性。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.00858v1">PDF</a> Accept to USENIX Security 2025</p>
<p><strong>摘要</strong></p>
<p>本文主要关注自动语音识别（ASR）在语音通信中对隐私的挑战，并提出了一种新型框架AudioShield，通过利用潜伏空间中的可转移通用对抗扰动（LS-TUAP）来生成音频对抗实例以保护用户实时语音免受ASR系统的侵犯。该方法不仅保护了音频质量，还提高了UAP的迁移性。通过嵌入目标文本特征到扰动中，进一步增强了UAP的迁移性。对现有商业ASR API和语音助手系统的全面评估表明，AudioShield的保护性能优于现有竞品，并在音频质量和保护能力方面取得显著进步。同时，AudioShield在实际场景中表现良好，对自适应对策具有强大的韧性。</p>
<p><strong>关键见解</strong></p>
<ol>
<li>自动语音识别（ASR）的大规模应用引发了关于语音隐私泄露的担忧。</li>
<li>对抗实例可用于防止未经授权的语音隐私泄露。</li>
<li>AudioShield框架使用潜伏空间中的可转移通用对抗扰动（LS-TUAP）来保护实时语音隐私。</li>
<li>AudioShield通过保留音频质量并嵌入目标文本特征来提高UAP的迁移性。</li>
<li>AudioShield在多个ASR系统和语音助手上的评估表现优于其他竞争对手。</li>
<li>AudioShield既有效保护了音频质量，也提高了语音隐私保护能力。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.00858">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-ea065013ad624b3da560c292cfe65216.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2581b32977ebc1474dd259929a729cb7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-cc5236c833ac2961a59ffd9739a65332.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-51bb3a0bfe95c0aa95bac531c130c767.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-6987eaae202a39206274a80e650c8fae.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8443b3cfa60159c9df8aedc66037207b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3f97eae96a7ea56a4443c0b91c97c018.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="TeleAntiFraud-28k-An-Audio-Text-Slow-Thinking-Dataset-for-Telecom-Fraud-Detection"><a href="#TeleAntiFraud-28k-An-Audio-Text-Slow-Thinking-Dataset-for-Telecom-Fraud-Detection" class="headerlink" title="TeleAntiFraud-28k: An Audio-Text Slow-Thinking Dataset for Telecom Fraud   Detection"></a>TeleAntiFraud-28k: An Audio-Text Slow-Thinking Dataset for Telecom Fraud   Detection</h2><p><strong>Authors:Zhiming Ma, Peidong Wang, Minhua Huang, Jingpeng Wang, Kai Wu, Xiangzhao Lv, Yachun Pang, Yin Yang, Wenjie Tang, Yuchen Kang</strong></p>
<p>The detection of telecom fraud faces significant challenges due to the lack of high-quality multimodal training data that integrates audio signals with reasoning-oriented textual analysis. To address this gap, we present TeleAntiFraud-28k, the first open-source audio-text slow-thinking dataset specifically designed for automated telecom fraud analysis. Our dataset is constructed through three strategies: (1) Privacy-preserved text-truth sample generation using automatically speech recognition (ASR)-transcribed call recordings (with anonymized original audio), ensuring real-world consistency through text-to-speech (TTS) model regeneration; (2) Semantic enhancement via large language model (LLM)-based self-instruction sampling on authentic ASR outputs to expand scenario coverage; (3) Multi-agent adversarial synthesis that simulates emerging fraud tactics through predefined communication scenarios and fraud typologies. The generated dataset contains 28,511 rigorously processed speech-text pairs, complete with detailed annotations for fraud reasoning. The dataset is divided into three tasks: scenario classification, fraud detection, fraud type classification. Furthermore, we construct TeleAntiFraud-Bench, a standardized evaluation benchmark comprising proportionally sampled instances from the dataset, to facilitate systematic testing of model performance on telecom fraud detection tasks. We also contribute a production-optimized supervised fine-tuning (SFT) model trained on hybrid real&#x2F;synthetic data, while open-sourcing the data processing framework to enable community-driven dataset expansion. This work establishes a foundational framework for multimodal anti-fraud research while addressing critical challenges in data privacy and scenario diversity. The project will be released at <a target="_blank" rel="noopener" href="https://github.com/JimmyMa99/TeleAntiFraud">https://github.com/JimmyMa99/TeleAntiFraud</a>. </p>
<blockquote>
<p>电信欺诈检测面临着巨大的挑战，主要是由于缺乏高质量的多模式训练数据，无法将音频信号与面向推理的文本分析相结合。为了解决这一空白，我们推出了TeleAntiFraud-28k，这是专门为电信欺诈自动化分析设计的第一个开源音频文本慢思考数据集。我们的数据集通过以下三种策略构建：（1）使用自动语音识别（ASR）转录的通话录音生成隐私保护文本真实样本（带有匿名原始音频），并通过文本到语音（TTS）模型再生确保现实世界的一致性；（2）通过基于大型语言模型（LLM）的自我指令采样对真实的ASR输出进行语义增强，以扩大场景覆盖；（3）模拟新兴欺诈策略的多代理对抗合成，通过预定的通信场景和欺诈类型。生成的数据集包含经过严格处理的28511个语音文本对，带有详细的欺诈推理注释。数据集分为三个任务：场景分类、欺诈检测、欺诈类型分类。此外，我们构建了TeleAntiFraud-Bench，一个标准化的评估基准，其中包含从数据集中按比例采样的实例，以促进电信欺诈检测任务上模型性能的系统测试。我们还为混合真实&#x2F;合成数据训练的生产优化监督微调（SFT）模型做出了贡献，同时开源数据处理框架，以实现社区驱动的数据集扩展。这项工作为多媒体反欺诈研究建立了基础框架，同时解决了数据隐私和场景多样性方面的关键挑战。该项目将在<a target="_blank" rel="noopener" href="https://github.com/JimmyMa99/TeleAntiFraud">https://github.com/JimmyMa99/TeleAntiFraud</a>发布。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.24115v3">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>该文本介绍了针对电信欺诈检测领域所面临的挑战，提出了一种新的开源音频文本慢思考数据集TeleAntiFraud-28k。该数据集通过三种策略构建，包括隐私保护文本真实样本生成、语义增强以及多代理对抗合成。数据集包含28,511个经过严格处理的语音文本对，带有详细的欺诈推理注释，分为场景分类、欺诈检测和欺诈类型分类三个任务。同时，文章还介绍了TeleAntiFraud-Bench评估基准的构建，以及使用混合真实&#x2F;合成数据训练的优化监督微调模型。该研究为多媒体抗欺诈研究提供了基础框架，并解决了数据隐私和场景多样性等关键挑战。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>TeleAntiFraud-28k是专门为电信欺诈分析设计的首个开源音频文本慢思考数据集。</li>
<li>数据集通过隐私保护的文本真实样本生成、语义增强和多代理对抗合成三种策略构建。</li>
<li>数据集包含28,511个语音文本对，带有详细的欺诈推理注释，分为场景分类、欺诈检测和欺诈类型分类三个任务。</li>
<li>介绍了TeleAntiFraud-Bench评估基准的构建，以系统化测试电信欺诈检测任务的模型性能。</li>
<li>公开了一个优化监督微调模型，该模型使用混合真实&#x2F;合成数据进行训练。</li>
<li>研究为多媒体抗欺诈研究提供了基础框架。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.24115">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-e04604273ac37876ed94a141c9dfb05a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4ab7ba5582a063e68f5fe266c9f2818e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2ea373be85c803e7f199329c6e1006ce.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-82a5dba04352ae797e591d0d181ab5d8.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="UniSep-Universal-Target-Audio-Separation-with-Language-Models-at-Scale"><a href="#UniSep-Universal-Target-Audio-Separation-with-Language-Models-at-Scale" class="headerlink" title="UniSep: Universal Target Audio Separation with Language Models at Scale"></a>UniSep: Universal Target Audio Separation with Language Models at Scale</h2><p><strong>Authors:Yuanyuan Wang, Hangting Chen, Dongchao Yang, Weiqin Li, Dan Luo, Guangzhi Li, Shan Yang, Zhiyong Wu, Helen Meng, Xixin Wu</strong></p>
<p>We propose Universal target audio Separation (UniSep), addressing the separation task on arbitrary mixtures of different types of audio. Distinguished from previous studies, UniSep is performed on unlimited source domains and unlimited source numbers. We formulate the separation task as a sequence-to-sequence problem, and a large language model (LLM) is used to model the audio sequence in the discrete latent space, leveraging the power of LLM in handling complex mixture audios with large-scale data. Moreover, a novel pre-training strategy is proposed to utilize audio-only data, which reduces the efforts of large-scale data simulation and enhances the ability of LLMs to understand the consistency and correlation of information within audio sequences. We also demonstrate the effectiveness of scaling datasets in an audio separation task: we use large-scale data (36.5k hours), including speech, music, and sound, to train a universal target audio separation model that is not limited to a specific domain. Experiments show that UniSep achieves competitive subjective and objective evaluation results compared with single-task models. </p>
<blockquote>
<p>我们提出了通用目标音频分离（UniSep），解决了不同类型音频的任意混合分离任务。与以前的研究不同，UniSep适用于无限的源域和无限的源数量。我们将分离任务制定为序列到序列的问题，并使用大型语言模型（LLM）对离散潜在空间中的音频序列进行建模，利用LLM在处理复杂混合音频和大规模数据方面的强大功能。此外，我们提出了一种新的预训练策略，只使用音频数据，这减少了大规模数据模拟的努力，增强了LLM理解音频序列内信息一致性和关联性的能力。我们还展示了在音频分离任务中扩大数据集的有效性：我们使用大规模数据（36.5k小时），包括语音、音乐和声音，来训练一个通用的目标音频分离模型，该模型不受特定领域的限制。实验表明，与单任务模型相比，UniSep在主观和客观评估方面都取得了具有竞争力的结果。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.23762v1">PDF</a> Accepted by ICME 2025</p>
<p><strong>Summary</strong></p>
<p>本文提出了Universal目标音频分离（UniSep）方法，解决了不同类型音频的任意混合分离任务。该方法在不限源域和不限源数量的情况下执行分离任务。将分离任务公式化为序列到序列的问题，并使用大型语言模型（LLM）在离散潜在空间中对音频序列进行建模。此外，提出了一种新的预训练策略，利用纯音频数据来降低大规模数据模拟的努力并增强LLM对音频序列中信息的一致性和关联性的理解。实验证明，UniSep在多数据集训练下的效果出色，与单任务模型相比取得了有竞争力的主观和客观评价结果。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>UniSep解决了不同类型音频的任意混合分离任务，适用于无限源域和无限源数量。</li>
<li>采用序列到序列的公式化方法处理音频分离任务。</li>
<li>利用大型语言模型（LLM）在离散潜在空间中对音频序列建模。</li>
<li>提出了一种新的预训练策略，利用纯音频数据增强LLM对音频信息的理解。</li>
<li>通过使用大规模数据（包括语音、音乐和声音）训练通用目标音频分离模型，不局限于特定领域。</li>
<li>实验表明，UniSep在主观和客观评价方面表现出竞争力。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.23762">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-906a33dd3c5dc25e404638af57533e89.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-aad09c1c94dc6b847e1e242723e3ac6d.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-60a0885d2537774f98aca4a066a582eb.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-94d4ee35bc03aba88889cd755af37fd5.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-5e0ad4bedbcf20a2d78ecd6ae45acb69.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e07f5d7b540a65c37977d64698342755.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-83890fea6d048db7080ac35591cc57b8.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Whisper-LM-Improving-ASR-Models-with-Language-Models-for-Low-Resource-Languages"><a href="#Whisper-LM-Improving-ASR-Models-with-Language-Models-for-Low-Resource-Languages" class="headerlink" title="Whisper-LM: Improving ASR Models with Language Models for Low-Resource   Languages"></a>Whisper-LM: Improving ASR Models with Language Models for Low-Resource   Languages</h2><p><strong>Authors:Xabier de Zuazo, Eva Navas, Ibon Saratxaga, Inma Hernáez Rioja</strong></p>
<p>Automatic speech recognition systems have undoubtedly advanced with the integration of multilingual and multitask models such as Whisper, which have shown a promising ability to understand and process speech across a wide range of languages. Despite their robustness, these models often fall short in handling the linguistic distinctions of minority languages. This study addresses this gap by integrating traditional and novel language models with fine-tuned Whisper models to raise their performance in less commonly studied languages. Through rigorous fine-tuning and evaluation across multiple datasets, we demonstrate substantial improvements in word error rate, particularly in low-resource scenarios. Our approach not only does take advantage of the extensive data Whisper was pre-trained on, but also complements its linguistic adaptability by incorporating language models. We obtained improvements up to 51% for in-distribution datasets and up to 34% for out-of-distribution sentences using statistical language models, while large language models provided moderate but consistently robust improvement across diverse linguistic contexts. The findings reveal that, while the integration reliably benefits all model sizes, the extent of improvement varies, highlighting the importance of optimized language model parameters. Finally, we emphasize the importance of selecting appropriate evaluation parameters when reporting the results using transformer-based ASR models. In summary, this research clears the way for more inclusive ASR technologies that perform better across languages by enriching their linguistic knowledge. For further implementation details of this study, the technical documentation and source code are available at <a target="_blank" rel="noopener" href="http://www.github.com/hitz-zentroa/whisper-lm">http://www.github.com/hitz-zentroa/whisper-lm</a>. </p>
<blockquote>
<p>自动语音识别系统无疑已通过整合如whisper等多语言和多任务模型，在理解和处理广泛语言方面表现出了令人瞩目的能力。尽管这些模型具有稳健性，但在处理少数语言的语言差异方面往往表现不足。本研究通过整合传统和新型语言模型与微调过的whisper模型，来解决这一差距，以提高其在较少研究的语言中的性能。通过多个数据集的严格微调与评估，我们证明了在单词错误率方面的显著改进，特别是在资源匮乏的场景下。我们的方法不仅利用了whisper预训练所依赖的大量数据，而且通过融入语言模型来补充其语言适应性。我们在使用统计语言模型的情况下，对内部数据集改进了高达51%，对外部句子改进了高达32%，而大型语言模型在不同的语言环境中提供了适度的但始终稳定的改进。研究结果表明，虽然整合对所有模型规模都有可靠的好处，但改进程度有所不同，这强调了优化语言模型参数的重要性。最后，我们强调了在使用基于变压器的ASR模型报告结果时，选择适当的评估参数的重要性。总之，本研究为开发更具包容性的ASR技术铺平了道路，这些技术通过丰富其语言知识，在跨语言方面表现更好。有关本研究的进一步实施细节，技术文档和源代码可在<a target="_blank" rel="noopener" href="http://www.github.com/hitz-zentroa/whisper-lm%E5%A4%84%E8%8E%B7%E5%BE%97%E3%80%82">http://www.github.com/hitz-zentroa/whisper-lm处获得。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.23542v1">PDF</a> 26 pages, 6 figures, includes supplementary materials. Will be   submitted to IEEE&#x2F;ACM Transactions on Audio, Speech, and Language Processing</p>
<p><strong>Summary</strong></p>
<p>本文研究了自动语音识别系统（ASR）在处理少数语言时的局限性，并探讨了通过集成传统和新型语言模型来提高预训练whisper模型性能的可行性。研究结果表明，整合模型显著提高了单词错误率，特别是在资源有限的情况下。该研究为增强ASR技术的语言适应性并提高其跨语言的性能开辟了道路。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>多语言多任务模型如Whisper能理解和处理多种语言的语音。</li>
<li>在处理少数语言时，现有模型如Whisper可能存在局限性。</li>
<li>通过整合传统和新型语言模型，可以提高whisper模型的性能。</li>
<li>整合模型在多个数据集上的严格精细调整和评价显示，单词错误率显著提高。</li>
<li>在资源有限的情况下，改进尤为显著。</li>
<li>大型语言模型在多种语言环境下提供了稳健但适中的改进。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.23542">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-6cda4dd8ac862a846cfde9d2ea59e7e8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4a25d179e6eccfb5a16eed4cb9d6f1f6.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Dual-Audio-Centric-Modality-Coupling-for-Talking-Head-Generation"><a href="#Dual-Audio-Centric-Modality-Coupling-for-Talking-Head-Generation" class="headerlink" title="Dual Audio-Centric Modality Coupling for Talking Head Generation"></a>Dual Audio-Centric Modality Coupling for Talking Head Generation</h2><p><strong>Authors:Ao Fu, Ziqi Ni, Yi Zhou</strong></p>
<p>The generation of audio-driven talking head videos is a key challenge in computer vision and graphics, with applications in virtual avatars and digital media. Traditional approaches often struggle with capturing the complex interaction between audio and facial dynamics, leading to lip synchronization and visual quality issues. In this paper, we propose a novel NeRF-based framework, Dual Audio-Centric Modality Coupling (DAMC), which effectively integrates content and dynamic features from audio inputs. By leveraging a dual encoder structure, DAMC captures semantic content through the Content-Aware Encoder and ensures precise visual synchronization through the Dynamic-Sync Encoder. These features are fused using a Cross-Synchronized Fusion Module (CSFM), enhancing content representation and lip synchronization. Extensive experiments show that our method outperforms existing state-of-the-art approaches in key metrics such as lip synchronization accuracy and image quality, demonstrating robust generalization across various audio inputs, including synthetic speech from text-to-speech (TTS) systems. Our results provide a promising solution for high-quality, audio-driven talking head generation and present a scalable approach for creating realistic talking heads. </p>
<blockquote>
<p>音频驱动的说话人头部视频生成是计算机视觉和图形学领域的关键挑战，在虚拟化身和数字媒体中有广泛的应用。传统的方法往往难以捕捉音频和面部动态之间的复杂交互，导致唇同步和视觉质量问题。在本文中，我们提出了一种基于NeRF的新型框架——双音频中心模态耦合（DAMC），它有效地整合了来自音频输入的内容和动态特征。通过利用双编码器结构，DAMC通过内容感知编码器捕获语义内容，并通过动态同步编码器确保精确的视觉同步。这些特征使用跨同步融合模块（CSFM）进行融合，增强了内容表示和唇同步。大量实验表明，我们的方法在唇同步准确性和图像质量等关键指标上优于现有的最先进的方法，展示了在各种音频输入上的稳健泛化能力，包括来自文本到语音（TTS）系统的合成语音。我们的结果为高质量音频驱动的说话人头部生成提供了有前景的解决方案，并展示了一种创建逼真说话人头部的可扩展方法。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.22728v1">PDF</a> 9 pages, 4 figures</p>
<p><strong>Summary</strong><br>音频驱动的人头动画生成是计算机视觉和图形学领域的关键挑战，应用于虚拟角色和数字媒体。传统方法难以捕捉音频与面部动态的复杂交互，导致唇部同步和视觉质量问题。本文提出基于NeRF的双音频中心模态耦合（DAMC）框架，有效整合音频输入的内容和动态特征。通过双编码器结构，DAMC通过内容感知编码器捕捉语义内容，并通过动态同步编码器确保精确视觉同步。这些特征通过交叉同步融合模块（CSFM）融合，提高内容表达和唇部同步。实验表明，该方法在唇部同步准确性和图像质量等关键指标上优于现有先进方法，并在各种音频输入上表现出强大的泛化能力，包括文本到语音（TTS）系统的合成语音。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>音频驱动的人头动画生成是计算机视觉和图形领域的重要挑战，具有广泛的应用于虚拟角色和数字媒体。</li>
<li>传统方法难以捕捉音频与面部动态的复杂交互，导致唇部同步和视觉质量问题。</li>
<li>本文提出了一个基于NeRF的双音频中心模态耦合（DAMC）框架来解决这个问题。</li>
<li>DAMC通过内容感知编码器和动态同步编码器有效整合音频输入的内容和动态特征。</li>
<li>通过交叉同步融合模块（CSFM）融合内容和动态特征，提高内容表达和唇部同步。</li>
<li>实验表明，DAMC在关键指标上优于现有方法，如唇部同步准确性和图像质量。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.22728">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-2b2c7d49850f18f4d83bef5aebdce69d.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-f14fbcc16baec813cf008cbb3379f343.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-d792a51b6c87be14443f6a1fbc972941.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-df38b1b97f3c37aa076c7f6fdd8e6fd1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2563f0e3ff5ac02b7aacf1380dc9a5c9.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-10b2b6a0f229fd4764b744d91119afd6.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Audio-Plane-Audio-Factorization-Plane-Gaussian-Splatting-for-Real-Time-Talking-Head-Synthesis"><a href="#Audio-Plane-Audio-Factorization-Plane-Gaussian-Splatting-for-Real-Time-Talking-Head-Synthesis" class="headerlink" title="Audio-Plane: Audio Factorization Plane Gaussian Splatting for Real-Time   Talking Head Synthesis"></a>Audio-Plane: Audio Factorization Plane Gaussian Splatting for Real-Time   Talking Head Synthesis</h2><p><strong>Authors:Shuai Shen, Wanhua Li, Yunpeng Zhang, Weipeng Hu, Yap-Peng Tan</strong></p>
<p>Talking head synthesis has become a key research area in computer graphics and multimedia, yet most existing methods often struggle to balance generation quality with computational efficiency. In this paper, we present a novel approach that leverages an Audio Factorization Plane (Audio-Plane) based Gaussian Splatting for high-quality and real-time talking head generation. For modeling a dynamic talking head, 4D volume representation is needed. However, directly storing a dense 4D grid is impractical due to the high cost and lack of scalability for longer durations. We overcome this challenge with the proposed Audio-Plane, where the 4D volume representation is decomposed into audio-independent space planes and audio-dependent planes. This provides a compact and interpretable feature representation for talking head, facilitating more precise audio-aware spatial encoding and enhanced audio-driven lip dynamic modeling. To further improve speech dynamics, we develop a dynamic splatting method that helps the network more effectively focus on modeling the dynamics of the mouth region. Extensive experiments demonstrate that by integrating these innovations with the powerful Gaussian Splatting, our method is capable of synthesizing highly realistic talking videos in real time while ensuring precise audio-lip synchronization. Synthesized results are available in <a target="_blank" rel="noopener" href="https://sstzal.github.io/Audio-Plane/">https://sstzal.github.io/Audio-Plane/</a>. </p>
<blockquote>
<p>头部说话合成已成为计算机图形学和多媒体领域的一个关键研究方向，然而，大多数现有方法往往难以在生成质量和计算效率之间取得平衡。在本文中，我们提出了一种基于音频分解平面（Audio-Plane）的高斯涂抹技术的新方法，用于高质量实时头部说话生成。为了模拟动态的说话头部，需要4D体积表示。然而，由于成本高昂和长期缺乏可扩展性，直接存储密集的4D网格并不实用。我们克服了这一挑战，提出了音频平面（Audio-Plane），其中将4D体积表示分解为独立于音频的空间平面和依赖于音频的平面。这为说话的头部提供了一个紧凑且可解释的特征表示，促进了更精确的声音感知空间编码和增强的音频驱动唇部动态建模。为了进一步提高语音动态效果，我们开发了一种动态涂抹方法，帮助网络更有效地专注于唇部区域的动态建模。大量实验表明，通过将这些创新与强大的高斯涂抹技术相结合，我们的方法能够在保证精确音频唇部同步的情况下，实时合成高度逼真的说话视频。合成结果可在<a target="_blank" rel="noopener" href="https://sstzal.github.io/Audio-Plane/%E6%9F%A5%E7%9C%8B%E3%80%82">https://sstzal.github.io/Audio-Plane/查看。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.22605v1">PDF</a> </p>
<p><strong>Summary</strong><br>本论文提出了一种利用音频分解平面（Audio-Plane）高斯模糊技术实现高质量实时动态谈话头生成的新方法。通过分解四维体积表示法为音频独立空间平面和音频依赖平面，解决了直接存储密集四维网格的不切实际和缺乏可扩展性的问题。该方法为谈话头提供了紧凑且可解释的特征表示，促进了更精确的音频感知空间编码和增强的音频驱动唇动态建模。为了进一步提高语音动态，开发了一种动态模糊方法，帮助网络更有效地专注于唇部区域的动态建模。实验证明，通过将这些创新与强大的高斯模糊技术相结合，该方法能够实时合成高度逼真的谈话视频，同时确保精确的音频-唇部同步。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>本论文提出了利用音频分解平面（Audio-Plane）技术实现高质量实时谈话头生成的方法。</li>
<li>通过将四维体积表示分解为音频独立和依赖的平面，解决了存储密集四维网格的问题。</li>
<li>Audio-Plane技术提供了紧凑且可解释的谈话头特征表示。</li>
<li>该方法促进了更精确的音频感知空间编码和增强的音频驱动唇动态建模。</li>
<li>通过动态模糊方法，提高了语音动态建模的效果。</li>
<li>实验证明该方法能够合成高度逼真的谈话视频，并确保音频与唇部的精确同步。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.22605">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-dc3945e146a1125a5d769517b491305e.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-3c343fd56b0ea0f2ccfabdd9058dcf16.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-fb17023d93340211b982eb56e0a42e41.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-4c0d22507b0d089e075c77b6ec79cfee.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-14c4fa59a61d1b3a409ba585990402d3.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-08c3de0ff764017706147587374a97e4.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="ReCoM-Realistic-Co-Speech-Motion-Generation-with-Recurrent-Embedded-Transformer"><a href="#ReCoM-Realistic-Co-Speech-Motion-Generation-with-Recurrent-Embedded-Transformer" class="headerlink" title="ReCoM: Realistic Co-Speech Motion Generation with Recurrent Embedded   Transformer"></a>ReCoM: Realistic Co-Speech Motion Generation with Recurrent Embedded   Transformer</h2><p><strong>Authors:Yong Xie, Yunlian Sun, Hongwen Zhang, Yebin Liu, Jinhui Tang</strong></p>
<p>We present ReCoM, an efficient framework for generating high-fidelity and generalizable human body motions synchronized with speech. The core innovation lies in the Recurrent Embedded Transformer (RET), which integrates Dynamic Embedding Regularization (DER) into a Vision Transformer (ViT) core architecture to explicitly model co-speech motion dynamics. This architecture enables joint spatial-temporal dependency modeling, thereby enhancing gesture naturalness and fidelity through coherent motion synthesis. To enhance model robustness, we incorporate the proposed DER strategy, which equips the model with dual capabilities of noise resistance and cross-domain generalization, thereby improving the naturalness and fluency of zero-shot motion generation for unseen speech inputs. To mitigate inherent limitations of autoregressive inference, including error accumulation and limited self-correction, we propose an iterative reconstruction inference (IRI) strategy. IRI refines motion sequences via cyclic pose reconstruction, driven by two key components: (1) classifier-free guidance improves distribution alignment between generated and real gestures without auxiliary supervision, and (2) a temporal smoothing process eliminates abrupt inter-frame transitions while ensuring kinematic continuity. Extensive experiments on benchmark datasets validate ReCoM’s effectiveness, achieving state-of-the-art performance across metrics. Notably, it reduces the Fr&#39;echet Gesture Distance (FGD) from 18.70 to 2.48, demonstrating an 86.7% improvement in motion realism. Our project page is <a target="_blank" rel="noopener" href="https://yong-xie-xy.github.io/ReCoM/">https://yong-xie-xy.github.io/ReCoM/</a>. </p>
<blockquote>
<p>我们提出了ReCoM，这是一个高效框架，用于生成与语音同步的高保真和可推广的人体动作。核心创新点在于循环嵌入转换器（RET），它将动态嵌入正则化（DER）集成到视觉转换器（ViT）核心架构中，以显式地建模共语音运动动力学。该架构能够联合建模时空依赖性，从而通过连贯的运动合成增强手势的自然性和保真度。为了提高模型的稳健性，我们采用了提出的DER策略，该策略使模型具备抗噪声和跨域推广的双重能力，从而提高了未见语音输入的零样本运动生成的自然性和流畅性。为了减轻自回归推理的固有局限性，包括误差累积和有限的自我校正能力，我们提出了一种迭代重建推理（IRI）策略。IRI通过循环姿势重建来优化运动序列，这由两个关键组件驱动：（1）无分类器指导改善了生成手势和真实手势之间的分布对齐，无需辅助监督；（2）时间平滑过程消除了帧间突兀的过渡，同时确保了运动学连续性。在基准数据集上的广泛实验验证了ReCoM的有效性，其在各项指标上均达到了最新技术水平。值得注意的是，它将Fréchet手势距离（FGD）从18.70减少到2.48，显示出运动真实性的86.7%改进。我们的项目页面是<a target="_blank" rel="noopener" href="https://yong-xie-xy.github.io/ReCoM%E3%80%82">https://yong-xie-xy.github.io/ReCoM/。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.21847v1">PDF</a> 8 pages, 6 figures, Project Page:   <a target="_blank" rel="noopener" href="https://yong-xie-xy.github.io/ReCoM/">https://yong-xie-xy.github.io/ReCoM/</a></p>
<p><strong>Summary</strong></p>
<p>本文介绍了ReCoM框架，它能生成与语音同步的高保真、可推广的人体动作。核心创新在于Recurrent Embedded Transformer（RET），它将Dynamic Embedding Regularization（DER）集成到Vision Transformer（ViT）核心架构中，以显式地模拟动作与语音的协同动态。通过联合空间时间依赖性建模，提高了手势的自然性和保真性，并通过连贯的动作合成实现了逼真运动。为提高模型稳健性，结合了DER策略，使模型具有抗噪声和跨域推广能力，提高了未见语音输入的零样本运动生成的流畅性和自然性。为缓解自回归推理的固有局限性，如误差累积和有限的自我校正能力，提出了迭代重建推理（IRI）策略。通过循环姿势重建优化动作序列，由无监督分类器引导和临时平滑过程驱动，消除了帧间突兀过渡，确保了运动学连续性。在基准数据集上的广泛实验验证了ReCoM的有效性，实现了跨指标的先进性能。显著地，它将Fr’echet Gesture Distance（FGD）从18.70降低到2.48，显示出动作真实性的86.7%提升。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ReCoM是一个高效框架，用于生成与语音同步的高保真和可推广的人体动作。</li>
<li>核心创新是Recurrent Embedded Transformer（RET），集成了Dynamic Embedding Regularization（DER）和Vision Transformer（ViT）。</li>
<li>RET能显式地模拟动作与语音的协同动态，通过联合空间时间依赖性建模提高手势的自然性和保真性。</li>
<li>DER策略提高了模型的稳健性，具有抗噪声和跨域推广能力。</li>
<li>迭代重建推理（IRI）策略缓解自回归推理的局限性，如误差累积和有限的自我校正能力。</li>
<li>IRI通过循环姿势重建优化动作序列，由无监督分类器引导和临时平滑过程驱动。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.21847">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-7dcd56a9686962fe72ae2b6f38b5209e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-970b5b00da0172e59efc09926c363490.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-50f2dd645cb22ef8c8e5b9d767e9cdcd.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-058866dff95090e67b360030a97b60db.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-cd883c5e6a1487049bc61b16de22a1f8.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="QualiSpeech-A-Speech-Quality-Assessment-Dataset-with-Natural-Language-Reasoning-and-Descriptions"><a href="#QualiSpeech-A-Speech-Quality-Assessment-Dataset-with-Natural-Language-Reasoning-and-Descriptions" class="headerlink" title="QualiSpeech: A Speech Quality Assessment Dataset with Natural Language   Reasoning and Descriptions"></a>QualiSpeech: A Speech Quality Assessment Dataset with Natural Language   Reasoning and Descriptions</h2><p><strong>Authors:Siyin Wang, Wenyi Yu, Xianzhao Chen, Xiaohai Tian, Jun Zhang, Lu Lu, Yu Tsao, Junichi Yamagishi, Yuxuan Wang, Chao Zhang</strong></p>
<p>This paper explores a novel perspective to speech quality assessment by leveraging natural language descriptions, offering richer, more nuanced insights than traditional numerical scoring methods. Natural language feedback provides instructive recommendations and detailed evaluations, yet existing datasets lack the comprehensive annotations needed for this approach. To bridge this gap, we introduce QualiSpeech, a comprehensive low-level speech quality assessment dataset encompassing 11 key aspects and detailed natural language comments that include reasoning and contextual insights. Additionally, we propose the QualiSpeech Benchmark to evaluate the low-level speech understanding capabilities of auditory large language models (LLMs). Experimental results demonstrate that finetuned auditory LLMs can reliably generate detailed descriptions of noise and distortion, effectively identifying their types and temporal characteristics. The results further highlight the potential for incorporating reasoning to enhance the accuracy and reliability of quality assessments. The dataset will be released at <a target="_blank" rel="noopener" href="https://huggingface.co/datasets/tsinghua-ee/QualiSpeech">https://huggingface.co/datasets/tsinghua-ee/QualiSpeech</a>. </p>
<blockquote>
<p>本文探索了一种利用自然语言描述进行语音质量评估的新视角，提供了比传统数字评分方法更丰富、更细微的见解。自然语言反馈提供了指导性的建议和详细的评价，但现有数据集缺乏这种评估方法所需的全面注释。为了弥补这一差距，我们推出了QualiSpeech数据集，这是一个全面的低级别语音质量评估数据集，涵盖了11个关键方面和包含推理和上下文洞察的自然语言详细注释。此外，我们提出了QualiSpeech基准测试，以评估听觉大型语言模型（LLM）对低级别语音的理解能力。实验结果表明，经过微调后的听觉LLM可以可靠地描述噪声和失真的细节，有效地识别它们的类型和时间特征。结果还强调了结合推理提高质量评估准确性和可靠性的潜力。该数据集将在<a target="_blank" rel="noopener" href="https://huggingface.co/datasets/tsinghua-ee/QualiSpeech%E5%8F%91%E5%B8%83%E3%80%82">https://huggingface.co/datasets/tsinghua-ee/QualiSpeech发布。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.20290v2">PDF</a> 23 pages, 16 figures</p>
<p><strong>摘要</strong></p>
<p>本文利用自然语言描述，从新的角度探讨语音质量评估，提供比传统数字评分方法更丰富、更细微的见解。自然语言反馈提供指导性的建议和详细的评价，但现有数据集缺乏全面注释，无法支持此方法。为了弥补这一差距，我们推出了QualiSpeech数据集，它涵盖了低层次的语音质量评估的11个关键方面，包含推理和上下文洞察的自然语言评论。此外，我们提出了QualiSpeech基准测试，以评估听觉大型语言模型（LLM）对低层次语音的理解能力。实验结果表明，经过微调的大型语言模型能够可靠地描述噪声和失真细节，有效识别它们的类型和时间特征。结果还表明，结合推理可以提高质量评估的准确性和可靠性。该数据集将在<a target="_blank" rel="noopener" href="https://huggingface.co/datasets/tsinghua-ee/QualiSpeech%E4%B8%8A%E5%8F%91%E6%98%BE%E3%80%82">https://huggingface.co/datasets/tsinghua-ee/QualiSpeech上发布。</a></p>
<p><strong>关键见解</strong></p>
<ol>
<li>利用自然语言描述提供了一个新的语音质量评估视角，更丰富且细微。</li>
<li>自然语言反馈能提供指导性的建议和详细的评价。</li>
<li>现有数据集缺乏全面注释，无法支持基于自然语言描述的语音质量评估方法。</li>
<li>推出QualiSpeech数据集，涵盖低层次语音质量评估的多个方面和详细自然语言评论。</li>
<li>提出QualiSpeech基准测试以评估听觉大型语言模型对低层次语音的理解能力。</li>
<li>实验表明，经过训练的大型语言模型能够详细描述语音中的噪声和失真。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.20290">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-33d85e460b85196fd81bcaffef10a739.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-91978749794794dfc280d966b6381780.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-954ad441dd32fc16510f9c15b46f45e5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-92f61a13cedc03a3f0f000df2f33e779.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9b4cb303644bc2e13b7ba7798e2e41c6.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="Whispering-in-Amharic-Fine-tuning-Whisper-for-Low-resource-Language"><a href="#Whispering-in-Amharic-Fine-tuning-Whisper-for-Low-resource-Language" class="headerlink" title="Whispering in Amharic: Fine-tuning Whisper for Low-resource Language"></a>Whispering in Amharic: Fine-tuning Whisper for Low-resource Language</h2><p><strong>Authors:Dawit Ketema Gete, Bedru Yimam Ahmed, Tadesse Destaw Belay, Yohannes Ayana Ejigu, Sukairaj Hafiz Imam, Alemu Belay Tessema, Mohammed Oumer Adem, Tadesse Amare Belay, Robert Geislinger, Umma Aliyu Musa, Martin Semmann, Shamsuddeen Hassan Muhammad, Henning Schreiber, Seid Muhie Yimam</strong></p>
<p>This work explores fine-tuning OpenAI’s Whisper automatic speech recognition (ASR) model for Amharic, a low-resource language, to improve transcription accuracy. While the foundational Whisper model struggles with Amharic due to limited representation in its training data, we fine-tune it using datasets like Mozilla Common Voice, FLEURS, and the BDU-speech dataset. The best-performing model, Whispersmall-am, significantly improves when finetuned on a mix of existing FLEURS data and new, unseen Amharic datasets. Training solely on new data leads to poor performance, but combining it with FLEURS data reinforces the model, enabling better specialization in Amharic. We also demonstrate that normalizing Amharic homophones significantly enhances Word Error Rate (WER) and Bilingual Evaluation Understudy (BLEU) scores. This study underscores the importance of fine-tuning strategies and dataset composition for improving ASR in low-resource languages, providing insights for future Amharic speech recognition research. </p>
<blockquote>
<p>这项工作探讨了针对阿姆哈拉语（一种低资源语言）对OpenAI的自动语音识别（ASR）模型Whisper进行微调，以提高转录准确性。尽管基础Whisper模型在阿姆哈拉语方面由于其训练数据中的有限表示而面临挑战，但我们使用Mozilla Common Voice、FLEURS和BDU-speech等数据集对其进行微调。表现最佳的模型Whispersmall-am，在现有的FLEURS数据和新的未见过的阿姆哈拉语数据集的混合上微调时，性能得到了显著提升。仅在新数据上进行训练会导致性能不佳，但将其与FLEURS数据相结合可以加强模型，使阿姆哈拉语的专业化能力更强。我们还证明，对阿姆哈拉语的同音字进行归一化处理可以显著提高单词错误率（WER）和双语评估下研究（BLEU）的得分。这项研究强调了微调策略和数据集组成在提高低资源语言的语音识别中的重要性，为未来阿姆哈拉语语音识别研究提供了见解。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.18485v2">PDF</a> </p>
<p><strong>总结</strong><br>    本文探讨了针对阿姆哈拉语这种资源贫乏的语言，如何对OpenAI的Whisper自动语音识别（ASR）模型进行微调，以提高转录准确性。使用Mozilla Common Voice、FLEURS和BDU-speech等数据集对基础Whisper模型进行微调，解决了模型在阿姆哈拉语方面的识别困难。在现有FLEURS数据和新、未见过的阿姆哈拉语数据集的混合上训练的Whispersmall-am模型表现最佳。仅在新数据上训练会导致性能不佳，但与FLEURS数据相结合，强化了模型的性能，使其在阿姆哈拉语方面更加专业化。同时证明，对阿姆哈拉语的同音字进行归一化处理可以显著提高单词错误率和双语评估研究得分。本研究强调了微调策略和数据集组成在改进低资源语言的语音识别中的重要性，为未来阿姆哈拉语语音识别研究提供了启示。</p>
<p><strong>关键见解</strong></p>
<ol>
<li>OpenAI的Whisper模型在资源贫乏的阿姆哈拉语上的表现有待提高。</li>
<li>使用Mozilla Common Voice、FLEURS和BDU-speech等数据集的微调策略能有效提升模型在阿姆哈拉语上的性能。</li>
<li>混合使用现有和新数据集进行微调，特别是FLEURS数据与新数据的结合，能显著提高模型的性能。</li>
<li>仅在新数据集上训练ASR模型可能导致性能不佳。</li>
<li>对阿姆哈拉语的同音字进行归一化处理能改善语音识别结果的准确性。</li>
<li>有效的微调策略和选择合适的数据集组成对于提高低资源语言的语音识别至关重要。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.18485">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-3fb7edaea3e316e67e15eb208457d489.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e3849c2b4fcd9ee28e1b6c1206ac7c37.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b690b59afd52e0322e37dfc7b2eea545.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9b06f5e26ca6b4048b8446abac6c61c1.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="Automatic-Speech-Recognition-for-Non-Native-English-Accuracy-and-Disfluency-Handling"><a href="#Automatic-Speech-Recognition-for-Non-Native-English-Accuracy-and-Disfluency-Handling" class="headerlink" title="Automatic Speech Recognition for Non-Native English: Accuracy and   Disfluency Handling"></a>Automatic Speech Recognition for Non-Native English: Accuracy and   Disfluency Handling</h2><p><strong>Authors:Michael McGuire</strong></p>
<p>Automatic speech recognition (ASR) has been an essential component of computer assisted language learning (CALL) and computer assisted language testing (CALT) for many years. As this technology continues to develop rapidly, it is important to evaluate the accuracy of current ASR systems for language learning applications. This study assesses five cutting-edge ASR systems’ recognition of non-native accented English speech using recordings from the L2-ARCTIC corpus, featuring speakers from six different L1 backgrounds (Arabic, Chinese, Hindi, Korean, Spanish, and Vietnamese), in the form of both read and spontaneous speech. The read speech consisted of 2,400 single sentence recordings from 24 speakers, while the spontaneous speech included narrative recordings from 22 speakers. Results showed that for read speech, Whisper and AssemblyAI achieved the best accuracy with mean Match Error Rates (MER) of 0.054 and 0.056 respectively, approaching human-level accuracy. For spontaneous speech, RevAI performed best with a mean MER of 0.063. The study also examined how each system handled disfluencies such as filler words, repetitions, and revisions, finding significant variation in performance across systems and disfluency types. While processing speed varied considerably between systems, longer processing times did not necessarily correlate with better accuracy. By detailing the performance of several of the most recent, widely-available ASR systems on non-native English speech, this study aims to help language instructors and researchers understand the strengths and weaknesses of each system and identify which may be suitable for specific use cases. </p>
<blockquote>
<p>多年来，语音识别技术（ASR）一直是计算机辅助语言学习（CALL）和计算机辅助语言测试（CALT）的重要组成部分。随着这项技术的快速发展，评估当前语音识别系统在语言学习应用中的准确性至关重要。本研究评估了五种前沿的语音识别系统对非母语英语口音的识别能力，使用了L2-ARCTIC语料库中的录音，该语料库包含了来自六个不同母语背景（阿拉伯语、中文、印地语、韩语、西班牙语和越南语）的发音人的朗读和即兴演讲。朗读部分由24名发音人的2400个单句录音组成，而即兴演讲部分则包含来自22名发音人的叙述录音。结果表明，在朗读部分，whisper和AssemblyAI取得了最佳准确性，平均匹配错误率（MER）分别为0.054和0.056，接近人类水平的准确性。在即兴演讲部分，RevAI表现最佳，平均MER为0.063。该研究还探讨了各系统如何处理不流畅现象，如填充词、重复和修正等，发现系统之间的性能差异很大。虽然处理速度在不同系统之间存在显著差异，但处理时间的长短并不一定与准确性相关。通过对几个最新且广泛可用的语音识别系统在非母语英语发音上的表现进行详细分析，本研究旨在帮助语言教师和研究者了解各系统的优缺点，并确定哪些系统适用于特定的用例。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.06924v2">PDF</a> 26 pages, 10 figures</p>
<p><strong>Summary</strong></p>
<p>本文主要研究了五种前沿的自动语音识别系统在非母语英语口音识别方面的性能。通过采用L2-ARCTIC语料库中的录音，研究评估了这些系统对不同母语背景的说话者的识别能力，包括阅读语和自发性口语。各系统在阅读语和自发性口语的识别准确率上存在差异，其中某些系统表现出接近人类水平的性能。此外，研究还探讨了各系统对口语不流畅现象的处理能力，如填充词、重复和修订等。结果显示，各系统在处理不同不流畅现象方面的表现差异显著。同时，系统处理速度与识别准确率并不完全相关。本研究旨在为语言教师和研究人员了解各系统的优缺点以及选择合适系统提供参考。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>五种前沿的ASR系统被评估在非母语英语口音识别上的性能。</li>
<li>这些系统在阅读语和自发性口语的识别上都表现出差异。</li>
<li>Whisper和AssemblyAI在阅读语的识别上表现最佳，接近人类水平。</li>
<li>RevAI在自发性口语的识别上表现最好。</li>
<li>各系统在处理口语不流畅现象（如填充词、重复和修订）方面的表现差异显著。</li>
<li>ASR系统的处理速度并不直接关联其识别准确率。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.06924">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-68a60f9e0ddeb91bfe94df5b5523f9ff.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="MegaTTS-3-Sparse-Alignment-Enhanced-Latent-Diffusion-Transformer-for-Zero-Shot-Speech-Synthesis"><a href="#MegaTTS-3-Sparse-Alignment-Enhanced-Latent-Diffusion-Transformer-for-Zero-Shot-Speech-Synthesis" class="headerlink" title="MegaTTS 3: Sparse Alignment Enhanced Latent Diffusion Transformer for   Zero-Shot Speech Synthesis"></a>MegaTTS 3: Sparse Alignment Enhanced Latent Diffusion Transformer for   Zero-Shot Speech Synthesis</h2><p><strong>Authors:Ziyue Jiang, Yi Ren, Ruiqi Li, Shengpeng Ji, Boyang Zhang, Zhenhui Ye, Chen Zhang, Bai Jionghao, Xiaoda Yang, Jialong Zuo, Yu Zhang, Rui Liu, Xiang Yin, Zhou Zhao</strong></p>
<p>While recent zero-shot text-to-speech (TTS) models have significantly improved speech quality and expressiveness, mainstream systems still suffer from issues related to speech-text alignment modeling: 1) models without explicit speech-text alignment modeling exhibit less robustness, especially for hard sentences in practical applications; 2) predefined alignment-based models suffer from naturalness constraints of forced alignments. This paper introduces \textit{MegaTTS 3}, a TTS system featuring an innovative sparse alignment algorithm that guides the latent diffusion transformer (DiT). Specifically, we provide sparse alignment boundaries to MegaTTS 3 to reduce the difficulty of alignment without limiting the search space, thereby achieving high naturalness. Moreover, we employ a multi-condition classifier-free guidance strategy for accent intensity adjustment and adopt the piecewise rectified flow technique to accelerate the generation process. Experiments demonstrate that MegaTTS 3 achieves state-of-the-art zero-shot TTS speech quality and supports highly flexible control over accent intensity. Notably, our system can generate high-quality one-minute speech with only 8 sampling steps. Audio samples are available at <a target="_blank" rel="noopener" href="https://sditdemo.github.io/sditdemo/">https://sditdemo.github.io/sditdemo/</a>. </p>
<blockquote>
<p>近期，零样本文本到语音（TTS）模型在语音质量和表达力方面有了显著的提升，但主流系统仍然面临与语音文本对齐建模相关的问题：1）没有明确的语音文本对齐建模的模型表现出较低的稳健性，特别是在实际应用中的难句；2）基于预定义对齐的模型受到强制对齐的自然性约束。本文介绍了<em>MegaTTS 3</em>，这是一个具有创新稀疏对齐算法的TTS系统，用于指导潜在扩散变压器（DiT）。具体来说，我们为MegaTTS 3提供稀疏对齐边界，以减少对齐的难度，同时不限制搜索空间，从而实现高自然度。此外，我们采用多条件无分类指导策略来调整口音强度，并采用分段整流流技术来加速生成过程。实验表明，MegaTTS 3达到了最先进的零样本TTS语音质量，并对口音强度实现了高度灵活的控制。值得注意的是，我们的系统只需8个采样步骤就能生成高质量的一分钟语音。音频样本可在<a target="_blank" rel="noopener" href="https://sditdemo.github.io/sditdemo/%E6%89%BE%E5%88%B0%E3%80%82">https://sditdemo.github.io/sditdemo/找到。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.18924v4">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文介绍了新一代的文本转语音（TTS）系统——MegaTTS 3。该系统采用创新的稀疏对齐算法，指导潜在扩散变压器（DiT）进行语音和文本的对齐。通过提供稀疏对齐边界，MegaTTS 3在降低对齐难度的同时不限制搜索空间，实现了高度的自然性。此外，还采用了多条件无分类指导策略来调整口音强度，并采用分段整流流技术加速生成过程。实验表明，MegaTTS 3达到了零样本TTS的先进语音质量，并高度灵活控制口音强度。尤其值得关注的是，该系统可以在仅8个采样步骤内生成高质量的一分钟语音。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>MegaTTS 3是一个新型的文本转语音（TTS）系统，主要解决现有TTS系统语音和文本对齐的问题。</li>
<li>通过引入创新的稀疏对齐算法，MegaTTS 3能够更有效地进行语音和文本的对齐，提高了系统的鲁棒性。</li>
<li>系统采用多条件无分类指导策略，允许灵活调整口音强度。</li>
<li>分段整流流技术的运用加速了语音生成的流程。</li>
<li>实验结果表明，MegaTTS 3的语音质量达到了零样本TTS的先进水平。</li>
<li>MegaTTS 3能够在短时间内生成高质量的语音，例如仅8个采样步骤生成一分钟的语音。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.18924">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-6a4848ec4bc65716ed58ab74f09839dd.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f63c305d51b567c1b33e75f11ea31ced.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-799c5f97dff2642f57db63aacabdbaa7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d4dc21c79b3bf4b146770c255229fcba.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="DGSNA-prompt-based-Dynamic-Generative-Scene-based-Noise-Addition-method"><a href="#DGSNA-prompt-based-Dynamic-Generative-Scene-based-Noise-Addition-method" class="headerlink" title="DGSNA: prompt-based Dynamic Generative Scene-based Noise Addition method"></a>DGSNA: prompt-based Dynamic Generative Scene-based Noise Addition method</h2><p><strong>Authors:Zihao Chen, Zhentao Lin, Bi Zeng, Linyi Huang, Zhi Li, Jia Cai</strong></p>
<p>To ensure the reliable operation of speech systems across diverse environments, noise addition methods have emerged as the prevailing solution. However, existing methods offer limited coverage of real-world noisy scenes and depend on pre-existing scene-based information and noise. This paper presents prompt-based Dynamic Generative Scene-based Noise Addition (DGSNA), a novel noise addition methodology that integrates Dynamic Generation of Scene-based Information (DGSI) with Scene-based Noise Addition for Speech (SNAS). This integration facilitates automated scene-based noise addition by transforming clean speech into various noise environments, thereby providing a more comprehensive and realistic simulation of diverse noise conditions. Experimental results demonstrate that DGSNA significantly enhances the robustness of speech recognition and keyword spotting models across various noise conditions, achieving a relative improvement of up to 11.21%. Furthermore, DGSNA can be effectively integrated with other noise addition methods to enhance performance. Our implementation and demonstrations are available at <a target="_blank" rel="noopener" href="https://dgsna.github.io/">https://dgsna.github.io</a>. </p>
<blockquote>
<p>为了确保跨不同环境中的语音系统可靠运行，噪声添加方法已经成为了一种主流的解决方案。然而，现有方法提供的真实世界噪声场景的覆盖范围有限，并依赖于基于场景的信息和噪声。本文提出了基于提示的动态生成场景噪声添加（DGSNA），这是一种新型噪声添加方法，它将基于场景的动态信息生成（DGSI）与基于场景的语音噪声添加（SNAS）相结合。这种结合通过转换干净语音为各种噪声环境，实现了基于场景的自动噪声添加，从而提供了更全面和现实的多种噪声条件模拟。实验结果表明，DGSNA显著提高了各种噪声条件下的语音识别和关键词识别模型的稳健性，相对改进率最高可达11.21%。此外，DGSNA可以与其他噪声添加方法有效结合以提高性能。我们的实现和演示可在<a target="_blank" rel="noopener" href="https://dgsna.github.io/">https://dgsna.github.io</a>查看。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2411.12363v2">PDF</a> </p>
<p><strong>Summary</strong><br>文本提出了一种基于提示的动态生成场景噪声添加方法（DGSNA），该方法结合了场景信息的动态生成（DGSI）和基于场景的噪声添加技术（SNAS）。DGSNA可自动将干净语音转换为各种噪声环境，提供更全面和现实的噪声条件模拟。实验结果表明，DGSNA可显著提高语音识别和关键词识别模型在各种噪声条件下的稳健性，相对改进率最高可达11.21%。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>DGSNA是一种新的噪声添加方法，它结合了DGSI和SNAS技术。</li>
<li>DGSNA能自动将干净语音转化为各种噪声环境，实现更全面和现实的噪声条件模拟。</li>
<li>实验结果表明，DGSNA能显著提高语音识别和关键词识别模型在多种噪声条件下的稳健性。</li>
<li>DGSNA的相对改进率最高可达11.21%。</li>
<li>DGSNA与其他噪声添加方法结合使用，可进一步提升性能。</li>
<li>DGSNA的实施和演示可在指定网站找到。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2411.12363">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-e22de12632ce3c2dfdafa47b86bc4e4c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9f4ca47be4f7ec2cb4c5fddbde0ac754.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="SAV-SE-Scene-aware-Audio-Visual-Speech-Enhancement-with-Selective-State-Space-Model"><a href="#SAV-SE-Scene-aware-Audio-Visual-Speech-Enhancement-with-Selective-State-Space-Model" class="headerlink" title="SAV-SE: Scene-aware Audio-Visual Speech Enhancement with Selective State   Space Model"></a>SAV-SE: Scene-aware Audio-Visual Speech Enhancement with Selective State   Space Model</h2><p><strong>Authors:Xinyuan Qian, Jiaran Gao, Yaodan Zhang, Qiquan Zhang, Hexin Liu, Leibny Paola Garcia, Haizhou Li</strong></p>
<p>Speech enhancement plays an essential role in various applications, and the integration of visual information has been demonstrated to bring substantial advantages. However, the majority of current research concentrates on the examination of facial and lip movements, which can be compromised or entirely inaccessible in scenarios where occlusions occur or when the camera view is distant. Whereas contextual visual cues from the surrounding environment have been overlooked: for example, when we see a dog bark, our brain has the innate ability to discern and filter out the barking noise. To this end, in this paper, we introduce a novel task, i.e. SAV-SE. To our best knowledge, this is the first proposal to use rich contextual information from synchronized video as auxiliary cues to indicate the type of noise, which eventually improves the speech enhancement performance. Specifically, we propose the VC-S$^2$E method, which incorporates the Conformer and Mamba modules for their complementary strengths. Extensive experiments are conducted on public MUSIC, AVSpeech and AudioSet datasets, where the results demonstrate the superiority of VC-S$^2$E over other competitive methods. We will make the source code publicly available. Project demo page: <a target="_blank" rel="noopener" href="https://avsepage.github.io/">https://AVSEPage.github.io/</a> </p>
<blockquote>
<p>语音增强在各类应用中扮演着重要角色，而视觉信息的融合已证明可以带来巨大的优势。然而，当前大多数研究主要关注面部和嘴唇动作的检查，这些在遮挡发生或摄像头视角较远的情况下可能会受到损害或完全无法获取。而周围环境中的上下文视觉线索却被忽视了：例如，当我们看到狗叫时，我们的大脑就有天生的能力去辨别和过滤掉吠叫噪音。因此，本文介绍了一项新任务，即SAV-SE。据我们所知，这是首次提出利用来自同步视频中的丰富上下文信息作为辅助线索，以指示噪声类型，最终提高语音增强性能。具体来说，我们提出了VC-S$^2$E方法，该方法结合了Conformer和Mamba模块，以发挥它们的互补优势。在公共MUSIC、AVSpeech和AudioSet数据集上进行了大量实验，结果表明VC-S$^2$E优于其他竞争方法。我们将公开源代码。项目演示页面：<a target="_blank" rel="noopener" href="https://avsepage.github.io/">https://AVSEPage.github.io/</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2411.07751v2">PDF</a> accepted by IEEE Journal of Selected Topics in Signal Processing</p>
<p><strong>Summary</strong><br>语音增强在各种应用中发挥着重要作用，集成视觉信息可以带来明显的优势。当前研究多集中在面部和嘴唇动作的观察上，但在遮挡或远距离拍摄时，这些信息可能受限或无法获取。相较之下，环境中的上下文视觉线索常被忽视。本文介绍了一种新型任务——SAV-SE，旨在利用同步视频中的丰富上下文信息作为辅助线索，以识别噪声类型，从而提高语音增强的性能。具体提出了VC-S$^2$E方法，结合了Conformer和Mamba模块的优势。在公共MUSIC、AVSpeech和AudioSet数据集上的实验结果表明，VC-S$^2$E优于其他竞争方法。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>语音增强集成视觉信息至关重要，能带来显著优势。</li>
<li>当前研究主要关注面部和嘴唇动作，但在特定情境下存在局限性。</li>
<li>上下文视觉线索（如环境中的声音与视觉信号的关联）在语音增强中扮演重要角色。</li>
<li>SAV-SE旨在利用同步视频的丰富上下文信息识别噪声类型，提高语音增强性能。</li>
<li>VC-S$^2$E方法结合了Conformer和Mamba模块的优势。</li>
<li>在多个公共数据集上的实验证明了VC-S$^2$E方法的优越性。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2411.07751">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-d515743992559c37f0284db4da56b9cb.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b98d23ad8d59f497d03efc55fef69103.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-fcb71963e58fe8c6ff639227b3287cec.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="Continuous-Speech-Tokenizer-in-Text-To-Speech"><a href="#Continuous-Speech-Tokenizer-in-Text-To-Speech" class="headerlink" title="Continuous Speech Tokenizer in Text To Speech"></a>Continuous Speech Tokenizer in Text To Speech</h2><p><strong>Authors:Yixing Li, Ruobing Xie, Xingwu Sun, Yu Cheng, Zhanhui Kang</strong></p>
<p>The fusion of speech and language in the era of large language models has garnered significant attention. Discrete speech token is often utilized in text-to-speech tasks for speech compression and portability, which is convenient for joint training with text and have good compression efficiency. However, we found that the discrete speech tokenizer still suffers from information loss. Therefore, we propose a simple yet effective continuous speech tokenizer named Cont-SPT, and a text-to-speech model based on continuous speech tokens. Our results show that the speech language model based on the continuous speech tokenizer has better continuity and higher estimated Mean Opinion Scores (MoS). This enhancement is attributed to better information preservation rate of the continuous speech tokenizer across both low and high frequencies in the frequency domain. The code and resources for Cont-SPT can be found in <a target="_blank" rel="noopener" href="https://github.com/Yixing-Li/Continuous-Speech-Tokenizer">https://github.com/Yixing-Li/Continuous-Speech-Tokenizer</a> </p>
<blockquote>
<p>在大型语言模型时代，语音和语言的融合引起了人们的广泛关注。离散语音令牌通常用于文本到语音的任务，以实现语音压缩和便携性，这便于与文本进行联合训练并具有良好的压缩效率。然而，我们发现离散语音令牌化仍然存在信息丢失的问题。因此，我们提出了一种简单有效的连续语音令牌化方法，命名为Cont-SPT，以及一种基于连续语音令牌的文本到语音模型。结果表明，基于连续语音令牌的语音语言模型具有更好的连续性和更高的预估平均意见得分（MoS）。这种增强功能归因于连续语音令牌化在低频和高频域都能更好地保持信息。Cont-SPT的代码和资源可在<a target="_blank" rel="noopener" href="https://github.com/Yixing-Li/Continuous-Speech-Tokenizer%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/Yixing-Li/Continuous-Speech-Tokenizer找到。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2410.17081v2">PDF</a> NAACL 2025 Findings Poster</p>
<p><strong>总结</strong></p>
<p>随着大型语言模型的兴起，语音和语言的融合引起了人们的广泛关注。离散语音令牌在文本到语音任务中常用于语音压缩和便携性，便于与文本进行联合训练，并具有良好的压缩效率。然而，研究发现离散语音分词器仍存在信息丢失的问题。为此，我们提出了一种简单有效的连续语音分词器Cont-SPT，以及基于连续语音令牌的文本到语音模型。结果表明，基于连续语音分词器的语音语言模型具有更好的连续性和更高的预估平均意见得分（MoS）。这一改进归因于连续语音分词器在频域中低频和高频段的信息保持率更高。Cont-SPT的代码和资源可在<a target="_blank" rel="noopener" href="https://github.com/Yixing-Li/Continuous-Speech-Tokenizer%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/Yixing-Li/Continuous-Speech-Tokenizer找到。</a></p>
<p><strong>关键见解</strong></p>
<ol>
<li>语音和语言融合在大型语言模型时代备受关注。</li>
<li>离散语音令牌在文本到语音任务中用于语音压缩和便携性。</li>
<li>离散语音分词器存在信息丢失的问题。</li>
<li>提出了名为Cont-SPT的连续语音分词器。</li>
<li>基于连续语音令牌的文本到语音模型表现出更好的连续性和更高的预估平均意见得分（MoS）。</li>
<li>连续语音分词器在频域中具有较高的信息保持率。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2410.17081">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-a2f6d4234ae6ceb8eced875223976719.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ac1c91fd3522e27a7a088ee987913ee4.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a486d8f9668db1bd0fc9ec5cec8797ae.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7521a02d93a4930b4722ab2d9a8daff9.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="Multi-modal-Speech-Transformer-Decoders-When-Do-Multiple-Modalities-Improve-Accuracy"><a href="#Multi-modal-Speech-Transformer-Decoders-When-Do-Multiple-Modalities-Improve-Accuracy" class="headerlink" title="Multi-modal Speech Transformer Decoders: When Do Multiple Modalities   Improve Accuracy?"></a>Multi-modal Speech Transformer Decoders: When Do Multiple Modalities   Improve Accuracy?</h2><p><strong>Authors:Yiwen Guan, Viet Anh Trinh, Vivek Voleti, Jacob Whitehill</strong></p>
<p>Decoder-only discrete-token language models have recently achieved significant success in automatic speech recognition. However, systematic analyses of how different modalities impact performance in specific scenarios remain limited. In this paper, we investigate the effects of multiple modalities on recognition accuracy on both synthetic and real-world datasets. Our experiments suggest that: (1) Integrating more modalities can increase accuracy; in particular, our paper is, to our best knowledge, the first to show the benefit of combining audio, image context, and lip information; (2) Images as a supplementary modality for speech recognition provide the greatest benefit at moderate noise levels, moreover, they exhibit a different trend compared to inherently synchronized modalities like lip movements; (3) Performance improves on both synthetic and real-world datasets when the most relevant visual information is filtered as a preprocessing step. </p>
<blockquote>
<p>在自动语音识别领域，仅使用解码器的离散令牌语言模型最近取得了重大成功。然而，关于不同模态如何在特定场景中影响性能的系统性分析仍然有限。在本文中，我们研究了多种模态对合成和现实世界数据集上识别准确性的影响。我们的实验表明：（1）集成更多模态可以提高准确性；特别是，据我们所知，本文首次展示了结合音频、图像上下文和嘴唇信息的益处；（2）图像作为语音识别的辅助模态，在中等噪声水平下提供的效益最大，而且与嘴唇动作等内在同步模态相比，它们表现出不同的趋势；（3）当最相关的视觉信息作为预处理步骤进行过滤时，合成和现实世界数据集上的性能都会提高。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2409.09221v2">PDF</a> </p>
<p><strong>Summary</strong><br>     本文探讨了多模态在合成和真实世界数据集上语音识别准确性的影响。研究发现，整合多模态信息能提高识别准确性，特别是结合音频、图像背景和唇语信息；图像作为语音识别的辅助模态在中等噪声水平下提供最大帮助，并且与唇动等内在同步模态展现出不同趋势；在过滤最相关的视觉信息作为预处理步骤时，合成和真实世界数据集上的性能均有所提高。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>多模态信息的整合能提高语音识别准确性。</li>
<li>结合音频、图像背景和唇语信息能带来最佳识别效果。</li>
<li>图像作为语音识别辅助模态在中等噪声环境下效益最大。</li>
<li>与内在同步模态如唇动相比，图像模态展现出不同趋势。</li>
<li>过滤最相关的视觉信息作为预处理能提高识别性能。</li>
<li>上述发现在合成和真实世界数据集上都得到了验证。</li>
<li>该研究为语音识别中的多模态融合提供了新见解。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2409.09221">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-b0bafdeb6780cbecb1d6946db37408f0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-24bd934af7376e7ba1d87df26c16c8fc.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-064b23bff11e7ddbdea0ae473c99fbe7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c3ddb7e89b731e981061fc5224f1f57d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d0cd001349ee2c56f86b5ce767c93af8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8823ea5bbfb3feab68473343aed2150f.jpg" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="Large-Language-Model-Can-Transcribe-Speech-in-Multi-Talker-Scenarios-with-Versatile-Instructions"><a href="#Large-Language-Model-Can-Transcribe-Speech-in-Multi-Talker-Scenarios-with-Versatile-Instructions" class="headerlink" title="Large Language Model Can Transcribe Speech in Multi-Talker Scenarios   with Versatile Instructions"></a>Large Language Model Can Transcribe Speech in Multi-Talker Scenarios   with Versatile Instructions</h2><p><strong>Authors:Lingwei Meng, Shujie Hu, Jiawen Kang, Zhaoqing Li, Yuejiao Wang, Wenxuan Wu, Xixin Wu, Xunying Liu, Helen Meng</strong></p>
<p>Recent advancements in large language models (LLMs) have revolutionized various domains, bringing significant progress and new opportunities. Despite progress in speech-related tasks, LLMs have not been sufficiently explored in multi-talker scenarios. In this work, we present a pioneering effort to investigate the capability of LLMs in transcribing speech in multi-talker environments, following versatile instructions related to multi-talker automatic speech recognition (ASR), target talker ASR, and ASR based on specific talker attributes such as sex, occurrence order, language, and keyword spoken. Our approach utilizes WavLM and Whisper encoder to extract multi-faceted speech representations that are sensitive to speaker characteristics and semantic context. These representations are then fed into an LLM fine-tuned using LoRA, enabling the capabilities for speech comprehension and transcription. Comprehensive experiments reveal the promising performance of our proposed system, MT-LLM, in cocktail party scenarios, highlighting the potential of LLM to handle speech-related tasks based on user instructions in such complex settings. The code, model, and samples are available at <a target="_blank" rel="noopener" href="https://github.com/cuhealthybrains/MT-LLM">https://github.com/cuhealthybrains/MT-LLM</a>. </p>
<blockquote>
<p>近期大型语言模型（LLM）的进步已经颠覆了多个领域，带来了显著的进步和新机遇。尽管在语音相关任务上有所进展，但LLM在多说话人场景中的应用尚未得到充分探索。在这项工作中，我们率先研究了LLM在多说话人环境下的语音识别能力。我们遵循与多说话人自动语音识别（ASR）、目标说话人ASR以及基于特定说话人属性（如性别、发言顺序、语言和关键词）的ASR相关的通用指令。我们的方法利用WavLM和Whisper编码器提取多面语音表示，这些表示对说话人特性和语义上下文敏感。然后，这些表示被输入到使用LoRA微调过的LLM中，以实现语音理解和转录功能。综合实验表明，我们提出的MT-LLM系统在鸡尾酒会场景中表现出有前景的性能，突显了LLM在复杂环境中根据用户指令处理语音相关任务的潜力。代码、模型和样本可在<a target="_blank" rel="noopener" href="https://github.com/cuhealthybrains/MT-LLM%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/cuhealthybrains/MT-LLM找到。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2409.08596v2">PDF</a> Accepted to IEEE ICASSP 2025. Update code link</p>
<p><strong>Summary</strong></p>
<p>大型语言模型（LLM）的近期进展为各领域带来了革命性的变革，促进了显著进步并为新的机会敞开了大门。尽管在语音相关任务方面有所进展，但LLM在多说话人场景中的转录应用尚未得到充分探索。本研究致力于探索LLM在多变环境下的语音转录能力，包括与多说话人自动语音识别（ASR）、目标说话人ASR以及基于特定说话人属性（如性别、发生顺序、语言和关键词）的ASR相关的灵活指令。我们的方法利用WavLM和Whisper编码器提取多面语音表示，这些表示对发言者的特性和语义上下文都很敏感。然后，通过LoRA微调这些表示，使其具备理解并转录语音的能力。综合实验显示，我们的系统MT-LLM在鸡尾酒会场景中表现优异，突显了LLM在复杂环境中处理基于用户指令的语音相关任务的潜力。代码、模型和样本可在GitHub上获取：<a target="_blank" rel="noopener" href="https://github.com/cuhealthybrains/MT-LLM">https://github.com/cuhealthybrains/MT-LLM</a>。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>大型语言模型（LLM）在语音相关任务中的应用正在快速发展。</li>
<li>LLM在多说话人场景中的语音转录尚未充分探索。</li>
<li>本研究探索了LLM在多说话人环境中的语音转录能力。</li>
<li>利用WavLM和Whisper编码器提取多面语音表示以区分不同说话者。</li>
<li>利用LoRA微调LLM以理解并转录语音。</li>
<li>综合实验显示MT-LLM系统在复杂场景下的出色表现。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2409.08596">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-90769faa15e8000ac109981a8ea4355e.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-c0e6a17397d641a1f25d7fda434b6901.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-c27bd9d1095cbd407afa90e10b1aaa80.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e24b088cf8479fb0ba985763cd641b37.jpg" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="Medical-Spoken-Named-Entity-Recognition"><a href="#Medical-Spoken-Named-Entity-Recognition" class="headerlink" title="Medical Spoken Named Entity Recognition"></a>Medical Spoken Named Entity Recognition</h2><p><strong>Authors:Khai Le-Duc, David Thulke, Hung-Phong Tran, Long Vo-Dang, Khai-Nguyen Nguyen, Truong-Son Hy, Ralf Schlüter</strong></p>
<p>Spoken Named Entity Recognition (NER) aims to extract named entities from speech and categorise them into types like person, location, organization, etc. In this work, we present VietMed-NER - the first spoken NER dataset in the medical domain. To our knowledge, our Vietnamese real-world dataset is the largest spoken NER dataset in the world regarding the number of entity types, featuring 18 distinct types. Furthermore, we present baseline results using various state-of-the-art pre-trained models: encoder-only and sequence-to-sequence; and conduct quantitative and qualitative error analysis. We found that pre-trained multilingual models generally outperform monolingual models on reference text and ASR output and encoders outperform sequence-to-sequence models in NER tasks. By translating the transcripts, the dataset can also be utilised for text NER in the medical domain in other languages than Vietnamese. All code, data and models are publicly available: <a target="_blank" rel="noopener" href="https://github.com/leduckhai/MultiMed/tree/master/VietMed-NER">https://github.com/leduckhai/MultiMed/tree/master/VietMed-NER</a>. </p>
<blockquote>
<p>语音命名实体识别（NER）旨在从语音中提取命名实体，并将其分类为人物、地点、组织等类型。在这项工作中，我们推出了VietMed-NER——医疗领域的首个语音NER数据集。据我们所知，我们的越南现实数据集是世界上实体类型数量最多的语音NER数据集，具有18种不同的类型。此外，我们使用各种最先进的预训练模型展示了基线结果：仅编码器模型和序列到序列模型；并进行定量和定性误差分析。我们发现，在参考文本和ASR输出上，预训练的多语言模型通常优于单语言模型，并且在NER任务中，编码器模型的性能优于序列到序列模型。通过翻译录音，该数据集也可用于除越南语以外的其他语言的医疗领域文本NER。所有代码、数据和模型可公开访问：<a target="_blank" rel="noopener" href="https://github.com/leduckhai/MultiMed/tree/master/VietMed-NER%E3%80%82">https://github.com/leduckhai/MultiMed/tree/master/VietMed-NER。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2406.13337v3">PDF</a> NAACL 2025, 60 pages</p>
<p><strong>Summary</strong></p>
<p>越南语医疗领域口语命名实体识别数据集VietMed-NER面世，包含18种不同实体类型，为世界上最大的医疗领域口语NER数据集。研究采用最新预训练模型，发现多语言预训练模型在参考文本和自动语音识别输出上通常优于单语言模型，编码器在命名实体识别任务上表现优于序列到序列模型。数据集可通过翻译转录本用于其他语言医疗文本命名实体识别研究。</p>
<p><strong>Key Takeaways</strong></p>
<p>1.VietMed-NER是医疗领域的首个越南语口语命名实体识别数据集。<br>2.VietMed-NER包含18种不同的实体类型，为世界上最大的医疗领域口语NER数据集。<br>3.研究使用了最新的预训练模型，包括编码器模型和序列到序列模型。<br>4.多语言预训练模型在参考文本和自动语音识别输出上的表现通常优于单语言模型。<br>5.编码器在命名实体识别任务上的表现优于序列到序列模型。<br>6.VietMed-NER数据集通过翻译转录本也可用于其他语言的医疗文本命名实体识别研究。</p>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2406.13337">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-2bad8508ccfe6e2a372ece2b0198a298.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6a9b6c4f7017c58154cd697b22256afe.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-77eeb057efe6addf845dff7b975f5f4a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-49ca730068c82d726fea60e139aa1ad7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7116f9fab3f17e9f75b75e7e01daeed8.jpg" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        文章作者:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        文章链接:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-04-03/Speech/">https://kedreamix.github.io/Talk2Paper/Paper/2025-04-03/Speech/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        版权声明:
                    </i>
                </span>
                <span class="reprint-info">
                    本博客所有文章除特別声明外，均采用
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    许可协议。转载请注明来源
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Speech/">
                                    <span class="chip bg-color">Speech</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;上一篇</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-04-04/Face%20Swapping/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-fd07040bae99422e0e799efb3713571e.jpg" class="responsive-img" alt="Face Swapping">
                        
                        <span class="card-title">Face Swapping</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Face Swapping 方向最新论文已更新，请持续关注 Update in 2025-04-04  High-Fidelity Diffusion Face Swapping with ID-Constrained Facial   Conditioning
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-04-04
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Face-Swapping/" class="post-category">
                                    Face Swapping
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Face-Swapping/">
                        <span class="chip bg-color">Face Swapping</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                下一篇&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-04-04/%E6%A3%80%E6%B5%8B_%E5%88%86%E5%89%B2_%E8%B7%9F%E8%B8%AA/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-14422b131c837ebcd7e30e889d0cab24.jpg" class="responsive-img" alt="检测/分割/跟踪">
                        
                        <span class="card-title">检测/分割/跟踪</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            检测/分割/跟踪 方向最新论文已更新，请持续关注 Update in 2025-04-04  View-Invariant Pixelwise Anomaly Detection in Multi-object Scenes with   Adaptive View Synthesis
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-04-04
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E6%A3%80%E6%B5%8B-%E5%88%86%E5%89%B2-%E8%B7%9F%E8%B8%AA/" class="post-category">
                                    检测/分割/跟踪
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E6%A3%80%E6%B5%8B-%E5%88%86%E5%89%B2-%E8%B7%9F%E8%B8%AA/">
                        <span class="chip bg-color">检测/分割/跟踪</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- 代码块功能依赖 -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- 代码语言 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- 代码块复制 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- 代码块收缩 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC 悬浮按钮. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* 修复文章卡片 div 的宽度. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // 切换TOC目录展开收缩的相关操作.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;站点总字数:&nbsp;<span
                        class="white-color">25156.8k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- 运行天数提醒. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // 区分是否有年份.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = '本站已运行 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                daysTip = '本站已運行 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = '本站已运行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = '本站已運行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="关注我的知乎: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">知</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- 搜索遮罩框 -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span>
            <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- 白天和黑夜主题 -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- 回到顶部按钮 -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // 只在桌面版网页启用特效
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- 雪花特效 -->
    

    <!-- 鼠标星星特效 -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--腾讯兔小巢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- 动态标签 -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Σ(っ °Д °;)っ诶，页面崩溃了嘛？", clearTimeout(st)) : (document.title = "φ(゜▽゜*)♪咦，又好了！", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
