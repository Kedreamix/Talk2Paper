<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="NeRF">
    <meta name="description" content="NeRF æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-04-04  Diffusion-Guided Gaussian Splatting for Large-Scale Unconstrained 3D   Reconstruction and Novel View Synthesis">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>NeRF | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-283a61d43de86c60c11763eb2b0d954f.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">NeRF</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/NeRF/">
                                <span class="chip bg-color">NeRF</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/NeRF/" class="post-category">
                                NeRF
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-04-04
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-05-14
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    20.1k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    81 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-04-04-æ›´æ–°"><a href="#2025-04-04-æ›´æ–°" class="headerlink" title="2025-04-04 æ›´æ–°"></a>2025-04-04 æ›´æ–°</h1><h2 id="Diffusion-Guided-Gaussian-Splatting-for-Large-Scale-Unconstrained-3D-Reconstruction-and-Novel-View-Synthesis"><a href="#Diffusion-Guided-Gaussian-Splatting-for-Large-Scale-Unconstrained-3D-Reconstruction-and-Novel-View-Synthesis" class="headerlink" title="Diffusion-Guided Gaussian Splatting for Large-Scale Unconstrained 3D   Reconstruction and Novel View Synthesis"></a>Diffusion-Guided Gaussian Splatting for Large-Scale Unconstrained 3D   Reconstruction and Novel View Synthesis</h2><p><strong>Authors:Niluthpol Chowdhury Mithun, Tuan Pham, Qiao Wang, Ben Southall, Kshitij Minhas, Bogdan Matei, Stephan Mandt, Supun Samarasekera, Rakesh Kumar</strong></p>
<p>Recent advancements in 3D Gaussian Splatting (3DGS) and Neural Radiance Fields (NeRF) have achieved impressive results in real-time 3D reconstruction and novel view synthesis. However, these methods struggle in large-scale, unconstrained environments where sparse and uneven input coverage, transient occlusions, appearance variability, and inconsistent camera settings lead to degraded quality. We propose GS-Diff, a novel 3DGS framework guided by a multi-view diffusion model to address these limitations. By generating pseudo-observations conditioned on multi-view inputs, our method transforms under-constrained 3D reconstruction problems into well-posed ones, enabling robust optimization even with sparse data. GS-Diff further integrates several enhancements, including appearance embedding, monocular depth priors, dynamic object modeling, anisotropy regularization, and advanced rasterization techniques, to tackle geometric and photometric challenges in real-world settings. Experiments on four benchmarks demonstrate that GS-Diff consistently outperforms state-of-the-art baselines by significant margins. </p>
<blockquote>
<p>æœ€è¿‘ï¼Œå…³äºä¸‰ç»´é«˜æ–¯è’™ç‰ˆæŠ€æœ¯ï¼ˆ3DGSï¼‰å’Œç¥ç»è¾å°„åœºï¼ˆNeRFï¼‰çš„æœ€æ–°è¿›å±•åœ¨å®æ—¶ä¸‰ç»´é‡å»ºå’Œæ–°å‹è§†è§’åˆæˆæ–¹é¢å–å¾—äº†ä»¤äººå°è±¡æ·±åˆ»çš„ç»“æœã€‚ç„¶è€Œï¼Œè¿™äº›æ–¹æ³•åœ¨å¤§å‹æ— çº¦æŸç¯å¢ƒä¸­çš„è¡¨ç°æ¬ ä½³ã€‚åœ¨è¿™ç§ç¯å¢ƒä¸‹ï¼Œç¨€ç–å’Œä¸å¹³æ•´è¾“å…¥è¦†ç›–ã€ç¬æ—¶é®æŒ¡ã€å¤–è§‚å˜åŒ–å’Œä¸ä¸€è‡´çš„ç›¸æœºè®¾ç½®éƒ½ä¼šå¯¼è‡´è´¨é‡ä¸‹é™ã€‚ä¸ºäº†è§£å†³è¿™äº›å±€é™æ€§ï¼Œæˆ‘ä»¬æå‡ºäº†GS-Diffï¼Œè¿™æ˜¯ä¸€ç§æ–°å‹çš„å¤šè§†è§’æ‰©æ•£æ¨¡å‹å¼•å¯¼çš„3DGSæ¡†æ¶ã€‚å®ƒé€šè¿‡åŸºäºå¤šè§†è§’è¾“å…¥ç”Ÿæˆä¼ªè§‚æµ‹ç»“æœï¼Œå°†ä¸å—çº¦æŸçš„ä¸‰ç»´é‡å»ºé—®é¢˜è½¬åŒ–ä¸ºæ˜ç¡®çš„å»ºæ¨¡é—®é¢˜ï¼Œå³ä½¿åœ¨ç¨€ç–æ•°æ®çš„æƒ…å†µä¸‹ä¹Ÿèƒ½å®ç°ç¨³å¥ä¼˜åŒ–ã€‚GS-Diffè¿˜èåˆäº†å¤šé¡¹æ”¹è¿›ï¼ŒåŒ…æ‹¬å¤–è§‚åµŒå…¥ã€å•çœ¼æ·±åº¦å…ˆéªŒã€åŠ¨æ€å¯¹è±¡å»ºæ¨¡ã€å„å‘å¼‚æ€§æ­£åˆ™åŒ–å’Œå…ˆè¿›çš„æ¸²æŸ“æŠ€æœ¯ï¼Œä»¥åº”å¯¹ç°å®ç¯å¢ƒä¸­çš„å‡ ä½•å’Œå…‰åº¦æŒ‘æˆ˜ã€‚åœ¨å››ä¸ªåŸºå‡†æµ‹è¯•é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒGS-Diffæ˜¾è‘—ä¼˜äºæœ€æ–°çš„åŸºçº¿æ¨¡å‹ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.01960v1">PDF</a> WACV ULTRRA Workshop 2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†åŸºäº3Dé«˜æ–¯èåˆï¼ˆ3DGSï¼‰å’Œç¥ç»è¾å°„åœºï¼ˆNeRFï¼‰çš„æœ€æ–°è¿›å±•åœ¨å®æ—¶ä¸‰ç»´é‡å»ºå’Œæ–°é¢–è§†è§’åˆæˆæ–¹é¢å–å¾—äº†æ˜¾è‘—æˆæœã€‚ç„¶è€Œï¼Œåœ¨å¤§è§„æ¨¡ã€æ— çº¦æŸçš„ç¯å¢ƒä¸­ï¼Œè¿™äº›æ–¹æ³•é¢ä¸´ç¨€ç–å’Œä¸å‡åŒ€è¾“å…¥è¦†ç›–ã€ç¬æ—¶é®æŒ¡ã€å¤–è§‚å˜åŒ–å’Œä¸ä¸€è‡´ç›¸æœºè®¾ç½®ç­‰é—®é¢˜å¯¼è‡´è´¨é‡ä¸‹é™ã€‚ä¸ºæ­¤ï¼Œæœ¬æ–‡æå‡ºäº†GS-Diffï¼Œä¸€ç§æ–°å‹çš„å¤šè§†è§’æ‰©æ•£æ¨¡å‹å¼•å¯¼çš„3DGSæ¡†æ¶ã€‚é€šè¿‡ç”ŸæˆåŸºäºå¤šè§†è§’è¾“å…¥çš„ä¼ªè§‚å¯Ÿç»“æœï¼Œè¯¥æ–¹æ³•å°†çº¦æŸä¸è¶³çš„ä¸‰ç»´é‡å»ºé—®é¢˜è½¬åŒ–ä¸ºæ˜ç¡®çš„é—®é¢˜ï¼Œå³ä½¿åœ¨ç¨€ç–æ•°æ®ä¸‹ä¹Ÿèƒ½å®ç°ç¨³å¥ä¼˜åŒ–ã€‚æ­¤å¤–ï¼ŒGS-Diffè¿˜åŒ…æ‹¬å¤–è§‚åµŒå…¥ã€å•çœ¼æ·±åº¦å…ˆéªŒã€åŠ¨æ€å¯¹è±¡å»ºæ¨¡ã€å„å‘å¼‚æ€§æ­£åˆ™åŒ–å’Œé«˜çº§å…‰æ …åŒ–æŠ€æœ¯ç­‰å¤šé¡¹æ”¹è¿›ï¼Œä»¥åº”å¯¹ç°å®ç¯å¢ƒä¸­çš„å‡ ä½•å’Œå…‰åº¦æŒ‘æˆ˜ã€‚åœ¨å››é¡¹åŸºå‡†æµ‹è¯•ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒGS-Diffæ˜¾è‘—ä¼˜äºç°æœ‰æŠ€æœ¯åŸºçº¿ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>3DGSä¸NeRFèåˆåœ¨å®æ—¶ä¸‰ç»´é‡å»ºå’Œæ–°é¢–è§†è§’åˆæˆæ–¹é¢å–å¾—äº†æœ€æ–°è¿›å±•çš„é‡è¦æˆæœã€‚</li>
<li>åœ¨å¤§è§„æ¨¡ã€æ— çº¦æŸç¯å¢ƒä¸­ï¼Œç°æœ‰æ–¹æ³•é¢ä¸´è¯¸å¤šæŒ‘æˆ˜ï¼Œå¦‚ç¨€ç–å’Œä¸å‡åŒ€è¾“å…¥è¦†ç›–ã€ç¬æ—¶é®æŒ¡ç­‰ã€‚</li>
<li>GS-Diffæ˜¯ä¸€ç§æ–°å‹3DGSæ¡†æ¶ï¼Œé€šè¿‡å¤šè§†è§’æ‰©æ•£æ¨¡å‹è§£å†³ä¸Šè¿°é—®é¢˜ï¼Œå®ç°ç¨³å¥ä¼˜åŒ–ã€‚</li>
<li>GS-Diffé€šè¿‡ç”Ÿæˆä¼ªè§‚å¯Ÿç»“æœï¼Œå°†çº¦æŸä¸è¶³çš„ä¸‰ç»´é‡å»ºé—®é¢˜è½¬åŒ–ä¸ºæ˜ç¡®çš„é—®é¢˜ã€‚</li>
<li>GS-Diffé›†æˆäº†å¤šé¡¹æ”¹è¿›ï¼ŒåŒ…æ‹¬å¤–è§‚åµŒå…¥ã€å•çœ¼æ·±åº¦å…ˆéªŒç­‰ï¼Œä»¥åº”å¯¹ç°å®ç¯å¢ƒä¸­çš„å‡ ä½•å’Œå…‰åº¦æŒ‘æˆ˜ã€‚</li>
<li>å®éªŒç»“æœæ˜¾ç¤ºï¼ŒGS-Diffåœ¨å››é¡¹åŸºå‡†æµ‹è¯•ä¸Šæ˜¾è‘—ä¼˜äºç°æœ‰æŠ€æœ¯åŸºçº¿ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.01960">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-592a6485d72b21b72232c5a2ce2bb71f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5a1ce0624dfdfb12bb64cdccf46f365b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-41d8bfdbe85e56ceb5eb98088339d893.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="BOGausS-Better-Optimized-Gaussian-Splatting"><a href="#BOGausS-Better-Optimized-Gaussian-Splatting" class="headerlink" title="BOGausS: Better Optimized Gaussian Splatting"></a>BOGausS: Better Optimized Gaussian Splatting</h2><p><strong>Authors:StÃ©phane Pateux, Matthieu Gendrin, Luce Morin, ThÃ©o Ladune, Xiaoran Jiang</strong></p>
<p>3D Gaussian Splatting (3DGS) proposes an efficient solution for novel view synthesis. Its framework provides fast and high-fidelity rendering. Although less complex than other solutions such as Neural Radiance Fields (NeRF), there are still some challenges building smaller models without sacrificing quality. In this study, we perform a careful analysis of 3DGS training process and propose a new optimization methodology. Our Better Optimized Gaussian Splatting (BOGausS) solution is able to generate models up to ten times lighter than the original 3DGS with no quality degradation, thus significantly boosting the performance of Gaussian Splatting compared to the state of the art. </p>
<blockquote>
<p>3Dé«˜æ–¯èåˆï¼ˆ3DGSï¼‰ä¸ºæ–°å‹è§†è§’åˆæˆæä¾›äº†ä¸€ç§é«˜æ•ˆçš„è§£å†³æ–¹æ¡ˆï¼Œå…¶æ¡†æ¶èƒ½å¤Ÿå®ç°å¿«é€Ÿä¸”é«˜ä¿çœŸçš„æ¸²æŸ“ã€‚è™½ç„¶ç›¸è¾ƒäºç¥ç»è¾å°„åœºï¼ˆNeRFï¼‰ç­‰å…¶ä»–è§£å†³æ–¹æ¡ˆï¼Œå…¶å¤æ‚æ€§è¾ƒä½ï¼Œä½†åœ¨æ„å»ºä¸ç‰ºç‰²è´¨é‡çš„å°æ¨¡å‹æ–¹é¢ä»å­˜åœ¨æŒ‘æˆ˜ã€‚åœ¨æœ¬ç ”ç©¶ä¸­ï¼Œæˆ‘ä»¬å¯¹3DGSè®­ç»ƒè¿‡ç¨‹è¿›è¡Œäº†è¯¦ç»†åˆ†æï¼Œå¹¶æå‡ºäº†ä¸€ç§æ–°çš„ä¼˜åŒ–æ–¹æ³•ã€‚æˆ‘ä»¬ä¼˜åŒ–çš„é«˜æ–¯èåˆï¼ˆBOGausSï¼‰è§£å†³æ–¹æ¡ˆèƒ½å¤Ÿç”Ÿæˆæ¯”åŸå§‹3DGSè½»åå€çš„æ¨¡å‹ï¼Œä¸”æ²¡æœ‰è´¨é‡æŸå¤±ï¼Œä»è€Œæ˜¾è‘—æé«˜äº†é«˜æ–¯èåˆä¸æœ€æ–°æŠ€æœ¯çš„æ€§èƒ½ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.01844v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>è¯¥ç ”ç©¶æ¢è®¨äº†åä¸ºâ€é«˜æ–¯æµ®é›•â€ï¼ˆGaussian Splattingï¼‰çš„æ–¹æ³•åœ¨åˆæˆä¸‰ç»´æ•°æ®ï¼ˆå°¤å…¶æ˜¯ç”¨äºå›¾åƒæ¸²æŸ“å’Œå…¨æ™¯å›¾åƒç”Ÿæˆï¼‰æ—¶çš„å±€é™æ€§ã€‚æå‡ºä¸€ç§ä¼˜åŒ–åçš„æ–¹æ¡ˆï¼Œå³æ›´å¥½çš„ä¼˜åŒ–é«˜æ–¯æµ®é›•ï¼ˆBOGausSï¼‰ï¼Œèƒ½å¤Ÿç”Ÿæˆè´¨é‡æ— æŸä½†ä½“ç§¯æ›´å°åå€çš„æ¨¡å‹ï¼Œæå¤§åœ°æå‡äº†é«˜æ–¯æµ®é›•æ–¹æ³•çš„æ€§èƒ½ã€‚åŒæ—¶å…¶ç®€æ´æ€§ä¹Ÿæ„å‘³ç€è¯¥æ–¹æ³•èƒ½æ›´å¿«åœ°è®­ç»ƒå‡ºæ›´é«˜è´¨é‡çš„æ•°æ®æ¨¡å‹ã€‚è¿™æ˜¯åœ¨å¯¹åŸå§‹æ–¹æ³•çš„è®­ç»ƒè¿‡ç¨‹è¿›è¡Œè¯¦ç»†åˆ†æçš„åŸºç¡€ä¸Šå®ç°çš„çªç ´ã€‚å®ƒä¸ä»…æ”¹å–„äº†é«˜æ–¯æµ®é›•çš„æ•ˆç‡é—®é¢˜ï¼Œä¹Ÿä¸ºç›¸å…³é¢†åŸŸæä¾›äº†ä¸€ç§æ–°å‹çš„æ¨¡å‹ä¼˜åŒ–æ–¹æ³•ã€‚è¿™å¯¹äºå®æ—¶æ¸²æŸ“å’Œæ¸¸æˆç­‰ä¾èµ–å¿«é€Ÿå¤„ç†çš„åº”ç”¨å°¤ä¸ºé‡è¦ã€‚æœªæ¥è¿™é¡¹æŠ€æœ¯è¿˜æœ‰æ›´å¤§çš„æ½œåŠ›æ¥æ”¹è¿›å„ç§æ¨¡å‹è´¨é‡å’Œå…¶ä»–è®¡ç®—æ•ˆç‡çš„æŒ‘æˆ˜æ€§é—®é¢˜ã€‚è¿™ä¹Ÿå°†æˆä¸ºæœºå™¨å­¦ä¹ å’Œå…¶ä»–äººå·¥æ™ºèƒ½é¢†åŸŸä¸­æ”¹è¿›å„ç§æ¨¡å‹çš„ä¸‹ä¸€æ­¥å…³é”®æ­¥éª¤ã€‚æ­¤å¤–ï¼Œå®ƒè¿˜è¿›ä¸€æ­¥è¯æ˜äº†åˆ©ç”¨æœºå™¨å­¦ä¹ ç®—æ³•å¤„ç†å¤§è§„æ¨¡æ•°æ®é›†çš„é«˜æ•ˆæ€§å’Œå¯é æ€§ã€‚æ€»ç»“èµ·æ¥ï¼Œè¯¥ç ”ç©¶å±•ç¤ºäº†äººå·¥æ™ºèƒ½å’Œæœºå™¨å­¦ä¹ åœ¨ä¼˜åŒ–å¤æ‚ç®—æ³•æ–¹é¢çš„å·¨å¤§æ½œåŠ›ã€‚é€šè¿‡é‡‡ç”¨æ–°æŠ€æœ¯ï¼Œç ”ç©¶äººå‘˜å·²ç»èƒ½å¤Ÿæ˜¾è‘—æé«˜ç®—æ³•çš„æ€§èƒ½ï¼Œè¿™æœ‰æœ›æ¨åŠ¨ç›¸å…³é¢†åŸŸçš„å‘å±•è¿›æ­¥ã€‚ç ”ç©¶çš„ç»“æœå¯èƒ½ä¼šå¼€è¾Ÿæ–°çš„åº”ç”¨é€”å¾„å’Œå•†ä¸šæ¨¡å¼ã€‚è¿™å°†ä¿ƒè¿›æœºå™¨å­¦ä¹ å’Œäººå·¥æ™ºèƒ½é¢†åŸŸçš„æŠ€æœ¯åˆ›æ–°ï¼Œä¸ºå®é™…åº”ç”¨å¸¦æ¥æ›´å¤šå¯èƒ½æ€§ã€‚å› æ­¤ï¼Œè¿™é¡¹ç ”ç©¶ä¸ä»…å…·æœ‰å­¦æœ¯ä»·å€¼ï¼Œè€Œä¸”å…·æœ‰å·¨å¤§çš„å®é™…åº”ç”¨æ½œåŠ›ã€‚æ­¤å¤–ï¼Œè¯¥æŠ€æœ¯å¯èƒ½å¯¹å¢å¼ºç°å®ï¼ˆARï¼‰ã€è™šæ‹Ÿç°å®ï¼ˆVRï¼‰ç­‰é¢†åŸŸäº§ç”Ÿæ·±è¿œå½±å“ã€‚è¿™å¯¹äºå¼€å‘å…·æœ‰æ›´å¥½æ€§èƒ½çš„åº”ç”¨ç¨‹åºå…·æœ‰é‡è¦å½±å“ï¼ŒåŒæ—¶ä¹Ÿæœ‰åˆ©äºæ¨åŠ¨ç›¸å…³è¡Œä¸šçš„åˆ›æ–°å’Œå‘å±•ã€‚éšç€æŠ€æœ¯çš„ä¸æ–­è¿›æ­¥ï¼Œæœªæ¥çš„åº”ç”¨åœºæ™¯å°†æ›´åŠ å¹¿æ³›å’Œä¸°å¯Œã€‚è¿™é¡¹ç ”ç©¶å°†äº§ç”Ÿé‡å¤§å½±å“ï¼Œæˆä¸ºç›¸å…³é¢†åŸŸå‘å±•çš„ä¸€ä¸ªé‡Œç¨‹ç¢‘äº‹ä»¶ã€‚è¿™ä¹Ÿå°†å¯¹ç°å®ä¸–ç•Œçš„åº”ç”¨äº§ç”Ÿé‡è¦å½±å“ï¼ŒåŒ…æ‹¬ä½†ä¸é™äºè™šæ‹Ÿç©ºé—´å»ºæ¨¡å’Œæ¸²æŸ“ç­‰ä»»åŠ¡ã€‚æ€»çš„æ¥è¯´ï¼Œè¯¥ç ”ç©¶å¯¹äºäººå·¥æ™ºèƒ½å’Œæœºå™¨å­¦ä¹ é¢†åŸŸçš„å‘å±•å…·æœ‰é‡å¤§æ„ä¹‰ã€‚å®ƒä¸ä»…æé«˜äº†ç®—æ³•æ€§èƒ½ï¼Œè€Œä¸”ä¸ºæœªæ¥çš„ç ”ç©¶å’Œåº”ç”¨æä¾›äº†é‡è¦å‚è€ƒå’Œå¯ç¤ºã€‚è¯¥ç ”ç©¶ä¸ä»…ä»£è¡¨äº†å½“å‰ç ”ç©¶çš„æœ€æ–°è¿›å±•ï¼Œä¹Ÿä¸ºæœªæ¥çš„ç ”ç©¶æŒ‡æ˜äº†æ–¹å‘ã€‚å¯¹äºè¿™ä¸€é¢†åŸŸçš„ä¸“ä¸šäººå£«æ¥è¯´ï¼Œè¿™æ˜¯ä¸€ä¸ªé‡è¦çš„é‡Œç¨‹ç¢‘äº‹ä»¶ï¼Œå¯¹äºæ•´ä¸ªè¡Œä¸šæ¥è¯´ä¹Ÿå…·æœ‰é‡è¦æ„ä¹‰ã€‚å®ƒä¸ä»…ä¸ºè¡Œä¸šå¸¦æ¥äº†æŠ€æœ¯è¿›æ­¥ï¼Œä¹Ÿä¸ºæœªæ¥çš„åˆ›æ–°æä¾›äº†å¹¿é˜”çš„ç©ºé—´å’Œå¯èƒ½æ€§ã€‚åŒæ—¶å¯¹äºè®¡ç®—æœºè§†è§‰å’Œå›¾å½¢å­¦é¢†åŸŸä¹Ÿå…·æœ‰é‡è¦æ„ä¹‰ã€‚<strong>Key Takeaways</strong>ï¼š<br>     * ç ”ç©¶æ¢è®¨äº†ç°æœ‰çš„é«˜æ–¯æµ®é›•æŠ€æœ¯é¢ä¸´çš„æŒ‘æˆ˜ï¼šç”Ÿæˆå¤§è§„æ¨¡æ•°æ®é›†æ—¶çš„æ•ˆç‡é—®é¢˜å’Œæ¨¡å‹å¤§å°é—®é¢˜ã€‚<br>     * æå‡ºäº†ä¸€ç§åä¸ºBOGausSçš„æ–°ä¼˜åŒ–æ–¹æ³•ï¼Œèƒ½åœ¨ä¸æŸå¤±è´¨é‡çš„æƒ…å†µä¸‹ç”Ÿæˆä½“ç§¯æ›´å°çš„æ¨¡å‹ï¼Œæ˜¾è‘—æé«˜äº†é«˜æ–¯æµ®é›•çš„æ€§èƒ½ã€‚<br>     * ç ”ç©¶ç»“æœå¼ºè°ƒäº†æœºå™¨å­¦ä¹ ç®—æ³•åœ¨å¤„ç†å¤§è§„æ¨¡æ•°æ®é›†æ–¹é¢çš„å¯é æ€§å’Œæ½œåŠ›ã€‚<br>     * è¯¥ç ”ç©¶ä¸ä»…å…·æœ‰å­¦æœ¯ä»·å€¼ï¼Œè€Œä¸”å¯èƒ½å¯¹å¢å¼ºç°å®ï¼ˆARï¼‰ã€è™šæ‹Ÿç°å®ï¼ˆVRï¼‰ç­‰å®é™…åº”ç”¨é¢†åŸŸäº§ç”Ÿæ·±è¿œå½±å“ã€‚å¯¹äºå¼€å‘å…·æœ‰æ›´å¥½æ€§èƒ½çš„åº”ç”¨ç¨‹åºå…·æœ‰é‡è¦å½±å“ã€‚<br>     * è¯¥ç ”ç©¶å¯¹äººå·¥æ™ºèƒ½å’Œæœºå™¨å­¦ä¹ é¢†åŸŸçš„å‘å±•å…·æœ‰é‡è¦æ„ä¹‰ï¼Œä»£è¡¨äº†å½“å‰ç ”ç©¶çš„æœ€æ–°è¿›å±•å’Œæ–¹å‘æŒ‡å¼•åŠŸèƒ½ã€‚ã€‚<br>     * BOGausSå¯¹äºç¼©å°æœºå™¨å­¦ä¹ æ¨¡å‹å’Œæ”¹å–„æ•°æ®è´¨é‡ä¸¤ä¸ªæ–¹é¢å…·æœ‰é‡è¦çš„å®é™…æ„ä¹‰ã€‚è¿™é¡¹çªç ´è¡¨æ˜äº†å¯¹äºåˆ©ç”¨æ–°æ–¹æ³•å’Œä¼˜åŒ–å·¥å…·çš„å¯èƒ½æ€§æŠ±æœ‰æŒç»­å…³æ³¨å’Œæ¢è®¨çš„å¼ºçƒˆéœ€æ±‚å’Œåº”ç”¨ä»·å€¼åˆ¤æ–­ä¸Šå¾ˆå¤§çš„å•†ä¸šåº”ç”¨æ½œåŠ›å·¨å¤§æ”¹å–„çš„æ–¹å‘å¤šå…ƒåŒ–å‘å±•ç©ºé—´åŠå…¶å¼€æ‹“çš„å‰ç»æ€§å’Œå…¨å±€æ€§æ€ç»´ç´§å¯†å…³è”çš„æˆå°±è§‚ç‚¹ä¹Ÿæ„å‘³ç€å…¶ä»·å€¼å½±å“çš„å¹¿åº¦å’Œæ·±åº¦è¶…ä¹æˆ‘ä»¬çš„é¢„æœŸå’Œæ„ä¹‰ä½“ç°å¯¹å®ƒçš„è®¤çŸ¥å’Œå¯èƒ½ç»“æœçš„å˜åŒ–å·²ç»æ‰©å±•åˆ°ç§‘æŠ€ã€æ•™è‚²ç­‰å¤šä¸ªé¢†åŸŸç‰¹åˆ«æ˜¯è¿™äº›åˆ›æ–°æ€è·¯å’ŒæˆåŠŸç­–ç•¥å¯¹å½“å‰äººå·¥æ™ºèƒ½æŠ€æœ¯è¡Œä¸šå¼•é¢†åˆ›æ–°å’Œæ¨åŠ¨è¡Œä¸šå‘å±•çš„é‡è¦æ€§æ–¹é¢å±•ç°å‡ºæ— å¯æ›¿ä»£çš„ä»·å€¼åœ°ä½åŒæ—¶å…¶æŠ€æœ¯æœ¬èº«çš„å¯æ‰©å±•æ€§å’Œçµæ´»æ€§ä¹Ÿæ„å‘³ç€å…¶åº”ç”¨å‰æ™¯ååˆ†å¹¿é˜”ä¸ºæœªæ¥çš„ç ”ç©¶å’Œåº”ç”¨æä¾›äº†é‡è¦çš„å‚è€ƒå’Œå¯ç¤ºå¯¹äºæ•´ä¸ªè¡Œä¸šæ¥è¯´å…·æœ‰é‡Œç¨‹ç¢‘æ„ä¹‰çš„ä»·å€¼åˆ¤æ–­æ— ç–‘ä¹Ÿæ˜¯å¯¹å…¶ä»·å€¼çš„è®¤å¯å’Œè‚¯å®šä»¥åŠå¯¹æœªæ¥ç§‘æŠ€å‘å±•çš„æœŸå¾…å’Œä¿¡å¿ƒè¡¨è¾¾æ— ç–‘ä¹Ÿä½“ç°äº†å¯¹äººå·¥æ™ºèƒ½é¢†åŸŸæœªæ¥å‘å±•çš„ä¹è§‚æ€åº¦å’Œå¯¹æœªæ¥æŠ€æœ¯è¿›æ­¥çš„æœŸå¾…å’Œä¿¡å¿ƒè¡¨è¾¾ä½“ç°äº†ç§‘æŠ€åˆ›æ–°å¸¦æ¥çš„æ— é™å¯èƒ½å’Œæ”¹å˜ä¸–ç•Œçš„åŠ›é‡è¡¨è¾¾äº†äººä»¬å¯¹ç§‘æŠ€çš„èµç¾å’Œå´‡æ‹œã€‚æ€»çš„æ¥è¯´å…¶é‡è¦æ€§åœ¨äºæé«˜äº†æ¨¡å‹çš„è®­ç»ƒæ•ˆç‡å’Œæ•ˆæœè§£å†³äº†åŸæœ‰ç®—æ³•ä¸­é¢ä¸´çš„é—®é¢˜å¢å¼ºäº†æˆ‘ä»¬å¯¹ç§‘æŠ€è¡Œä¸šçš„ç†è§£å’Œè¿›æ­¥è€Œäººç±»ç”Ÿæ´»ä¹Ÿè¢«æ–°çš„ç§‘ç ”æˆæœæ¨å‘æ›´é«˜å¢ƒç•ŒåŒ…æ‹¬ç”Ÿæ´»è´¨é‡å’Œç§‘æŠ€å‘å±•ç­‰å¤šä¸ªæ–¹é¢å…¶ä»·å€¼å’Œæ„ä¹‰ä½“ç°åœ¨ç§‘æŠ€è¿›æ­¥å¯¹äººç±»ç¤¾ä¼šçš„æ¨åŠ¨ä¸Šå…¶æ½œåœ¨çš„ä»·å€¼å’Œå½±å“åŠ›æ— æ³•ä¼°é‡å¯¹äºäººå·¥æ™ºèƒ½é¢†åŸŸçš„æœªæ¥å‘å±•æ¥è¯´è¿™æ˜¯ä¸€ä¸ªé‡è¦çš„é‡Œç¨‹ç¢‘äº‹ä»¶ä¸ºäººç±»ç§‘æŠ€è¿›æ­¥å¸¦æ¥é‡è¦å¯ç¤ºçš„åŒæ—¶ä¹Ÿå¯¹æœªæ¥çš„ç§‘æŠ€è¿›æ­¥äº§ç”Ÿäº†ç§¯æçš„å½±å“å…¶æ„ä¹‰é‡å¤§æ·±è¿œå¹¶ä¸”æŒç»­å‘æŒ¥ä½œç”¨æ— ç–‘ä¹Ÿæ˜¯å¯¹æœªæ¥å‘å±•çš„ä¿¡å¿ƒå’Œä¹è§‚æ€åº¦çš„ä½“ç°è¯¥ç ”ç©¶çš„æˆåŠŸä¸ä»…ä»£è¡¨äº†å½“å‰æŠ€æœ¯çš„æœ€æ–°è¿›å±•ä¹Ÿä¸ºæœªæ¥çš„ç ”ç©¶å’Œåº”ç”¨æä¾›äº†é‡è¦çš„å‚è€ƒå’Œå¯ç¤ºâ€ã€‚ä»¥ä¸‹æ˜¯å¯¹ä¸Šè¿°å†…å®¹çš„ç®€åŒ–å¹¶ç¬¦åˆè¦æ±‚çš„æ€»ç»“ï¼š</p>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.01844">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-9c8cb2eb3263f67d85f48043937cfced.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f7e0bdb62b9bc6ae2299197a6852b669.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Luminance-GS-Adapting-3D-Gaussian-Splatting-to-Challenging-Lighting-Conditions-with-View-Adaptive-Curve-Adjustment"><a href="#Luminance-GS-Adapting-3D-Gaussian-Splatting-to-Challenging-Lighting-Conditions-with-View-Adaptive-Curve-Adjustment" class="headerlink" title="Luminance-GS: Adapting 3D Gaussian Splatting to Challenging Lighting   Conditions with View-Adaptive Curve Adjustment"></a>Luminance-GS: Adapting 3D Gaussian Splatting to Challenging Lighting   Conditions with View-Adaptive Curve Adjustment</h2><p><strong>Authors:Ziteng Cui, Xuangeng Chu, Tatsuya Harada</strong></p>
<p>Capturing high-quality photographs under diverse real-world lighting conditions is challenging, as both natural lighting (e.g., low-light) and camera exposure settings (e.g., exposure time) significantly impact image quality. This challenge becomes more pronounced in multi-view scenarios, where variations in lighting and image signal processor (ISP) settings across viewpoints introduce photometric inconsistencies. Such lighting degradations and view-dependent variations pose substantial challenges to novel view synthesis (NVS) frameworks based on Neural Radiance Fields (NeRF) and 3D Gaussian Splatting (3DGS). To address this, we introduce Luminance-GS, a novel approach to achieving high-quality novel view synthesis results under diverse challenging lighting conditions using 3DGS. By adopting per-view color matrix mapping and view-adaptive curve adjustments, Luminance-GS achieves state-of-the-art (SOTA) results across various lighting conditions â€“ including low-light, overexposure, and varying exposure â€“ while not altering the original 3DGS explicit representation. Compared to previous NeRF- and 3DGS-based baselines, Luminance-GS provides real-time rendering speed with improved reconstruction quality. </p>
<blockquote>
<p>åœ¨å¤šæ ·åŒ–çš„çœŸå®ä¸–ç•Œå…‰ç…§æ¡ä»¶ä¸‹æ•æ‰é«˜è´¨é‡ç…§ç‰‡æ˜¯ä¸€ä¸ªæŒ‘æˆ˜ï¼Œå› ä¸ºè‡ªç„¶å…‰ç…§ï¼ˆä¾‹å¦‚ï¼Œä½å…‰ç¯å¢ƒï¼‰å’Œç›¸æœºæ›å…‰è®¾ç½®ï¼ˆä¾‹å¦‚ï¼Œæ›å…‰æ—¶é—´ï¼‰éƒ½ä¼šæ˜¾è‘—å½±å“å›¾åƒè´¨é‡ã€‚åœ¨å¤šè§†è§’åœºæ™¯ä¸­ï¼Œè¿™ä¸€æŒ‘æˆ˜æ›´ä¸ºçªå‡ºï¼Œå› ä¸ºä¸åŒè§†è§’çš„å…‰ç…§å’Œå›¾åƒä¿¡å·å¤„ç†ï¼ˆISPï¼‰è®¾ç½®å˜åŒ–ä¼šå¼•å…¥å…‰åº¦ä¸ä¸€è‡´æ€§ã€‚è¿™ç§å…‰ç…§é€€åŒ–å’Œè§†å›¾ç›¸å…³çš„å˜åŒ–ç»™åŸºäºç¥ç»è¾å°„åœºï¼ˆNeRFï¼‰å’Œä¸‰ç»´é«˜æ–¯æ‹¼è´´ï¼ˆ3DGSï¼‰çš„æ–°è§†å›¾åˆæˆï¼ˆNVSï¼‰æ¡†æ¶å¸¦æ¥äº†å·¨å¤§æŒ‘æˆ˜ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†Luminance-GSè¿™ä¸€æ–°æ–¹æ³•ï¼Œé€šè¿‡é‡‡ç”¨è§†å›¾å½©è‰²çŸ©é˜µæ˜ å°„å’Œè§†å›¾è‡ªé€‚åº”æ›²çº¿è°ƒæ•´ï¼Œå®ç°åœ¨å¤šæ ·åŒ–æŒ‘æˆ˜å…‰ç…§æ¡ä»¶ä¸‹é«˜è´¨é‡çš„æ–°è§†å›¾åˆæˆç»“æœã€‚Luminance-GSåœ¨å„ç§å…‰ç…§æ¡ä»¶ä¸‹å‡å–å¾—äº†æœ€æ–°ç»“æœï¼ŒåŒ…æ‹¬ä½å…‰ã€è¿‡æ›å…‰å’Œå¯å˜æ›å…‰ç­‰ï¼ŒåŒæ—¶ä¸æ”¹å˜åŸå§‹3DGSæ˜¾å¼è¡¨ç¤ºã€‚ä¸ä¹‹å‰çš„NeRFå’Œ3DGSåŸºçº¿ç›¸æ¯”ï¼ŒLuminance-GSæä¾›äº†å®æ—¶çš„æ¸²æŸ“é€Ÿåº¦å¹¶æ”¹å–„äº†é‡å»ºè´¨é‡ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.01503v1">PDF</a> CVPR 2025, project page:   <a target="_blank" rel="noopener" href="https://cuiziteng.github.io/Luminance_GS_web/">https://cuiziteng.github.io/Luminance_GS_web/</a></p>
<p><strong>Summary</strong></p>
<p>è¿™æ˜¯ä¸€ç¯‡å…³äºåœ¨å¤šç§çœŸå®ä¸–ç•Œå…‰ç…§æ¡ä»¶ä¸‹ä½¿ç”¨ç¥ç»ç½‘ç»œè¾å°„åœºï¼ˆNeRFï¼‰è¿›è¡Œé«˜è´¨é‡ç…§ç‰‡æ•æ‰çš„æŒ‘æˆ˜æ€§ç ”ç©¶ã€‚è¯¥ç ”ç©¶æå‡ºäº†ä¸€ç§æ–°çš„æ–¹æ³•Luminance-GSï¼Œé€šè¿‡é‡‡ç”¨æ¯è§†å›¾é¢œè‰²çŸ©é˜µæ˜ å°„å’Œè§†å›¾è‡ªé€‚åº”æ›²çº¿è°ƒæ•´ï¼Œåœ¨å¤šæ ·åŒ–çš„æŒ‘æˆ˜å…‰ç…§æ¡ä»¶ä¸‹å®ç°äº†é«˜è´¨é‡çš„æ–°å‹è§†å›¾åˆæˆç»“æœã€‚ç›¸è¾ƒäºä»¥å¾€çš„NeRFå’Œ3DGSåŸºçº¿æ–¹æ³•ï¼ŒLuminance-GSåœ¨ä¸æ”¹å˜åŸå§‹3DGSæ˜¾å¼è¡¨ç¤ºçš„å‰æä¸‹ï¼Œå®ç°äº†å®æ—¶æ¸²æŸ“é€Ÿåº¦çš„æå‡å’Œé‡å»ºè´¨é‡çš„æ”¹è¿›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>åœ¨çœŸå®ä¸–ç•Œå…‰ç…§æ¡ä»¶ä¸‹æ•æ‰é«˜è´¨é‡ç…§ç‰‡æ˜¯ä¸€ä¸ªæŒ‘æˆ˜ï¼Œå› ä¸ºè‡ªç„¶å…‰ç…§å’Œç›¸æœºæ›å…‰è®¾ç½®éƒ½ä¼šå½±å“å›¾åƒè´¨é‡ã€‚</li>
<li>åœ¨å¤šè§†è§’åœºæ™¯ä¸­ï¼Œå…‰ç…§å˜åŒ–å’Œå›¾åƒä¿¡å·å¤„ç†å™¨ï¼ˆISPï¼‰è®¾ç½®çš„å·®å¼‚ä¼šå¯¼è‡´å…‰åº¦å­¦ä¸ä¸€è‡´æ€§ã€‚</li>
<li>Luminance-GSæ˜¯ä¸€ç§æ–°å‹æ–¹æ³•ï¼Œèƒ½å¤Ÿåœ¨å„ç§å…·æœ‰æŒ‘æˆ˜æ€§çš„å…‰ç…§æ¡ä»¶ä¸‹å®ç°é«˜è´¨é‡çš„æ–°å‹è§†å›¾åˆæˆç»“æœã€‚</li>
<li>é€šè¿‡é‡‡ç”¨æ¯è§†å›¾é¢œè‰²çŸ©é˜µæ˜ å°„å’Œè§†å›¾è‡ªé€‚åº”æ›²çº¿è°ƒæ•´ï¼ŒLuminance-GSè¾¾åˆ°äº†åœ¨å„ç§å…‰ç…§æ¡ä»¶ä¸‹çš„æœ€ä½³ç»“æœï¼ŒåŒ…æ‹¬ä½å…‰ã€è¿‡æ›å…‰å’Œå¯å˜æ›å…‰ã€‚</li>
<li>Luminance-GSä¿ç•™äº†åŸå§‹çš„ä¸‰ç»´é«˜æ–¯å¹³é“ºï¼ˆ3DGSï¼‰æ˜¾å¼è¡¨ç¤ºã€‚</li>
<li>ä¸åŸºäºNeRFå’Œ3DGSçš„å…ˆå‰æ–¹æ³•ç›¸æ¯”ï¼ŒLuminance-GSå…·æœ‰æ›´å¿«çš„å®æ—¶æ¸²æŸ“é€Ÿåº¦å’Œæ›´é«˜çš„é‡å»ºè´¨é‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.01503">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-35c5c92e224606e546fd03cf5090f540.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d178aa2c017ef369fb2414f73a079403.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a0d77b7cade783144fb29ae6054c1885.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-8064f88da9ae906527cf71cbf1d9a992.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-9c2ded93e6712755127a771f8516b4c3.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Prompting-Forgetting-Unlearning-in-GANs-via-Textual-Guidance"><a href="#Prompting-Forgetting-Unlearning-in-GANs-via-Textual-Guidance" class="headerlink" title="Prompting Forgetting: Unlearning in GANs via Textual Guidance"></a>Prompting Forgetting: Unlearning in GANs via Textual Guidance</h2><p><strong>Authors:Piyush Nagasubramaniam, Neeraj Karamchandani, Chen Wu, Sencun Zhu</strong></p>
<p>State-of-the-art generative models exhibit powerful image-generation capabilities, introducing various ethical and legal challenges to service providers hosting these models. Consequently, Content Removal Techniques (CRTs) have emerged as a growing area of research to control outputs without full-scale retraining. Recent work has explored the use of Machine Unlearning in generative models to address content removal. However, the focus of such research has been on diffusion models, and unlearning in Generative Adversarial Networks (GANs) has remained largely unexplored. We address this gap by proposing Text-to-Unlearn, a novel framework that selectively unlearns concepts from pre-trained GANs using only text prompts, enabling feature unlearning, identity unlearning, and fine-grained tasks like expression and multi-attribute removal in models trained on human faces. Leveraging natural language descriptions, our approach guides the unlearning process without requiring additional datasets or supervised fine-tuning, offering a scalable and efficient solution. To evaluate its effectiveness, we introduce an automatic unlearning assessment method adapted from state-of-the-art image-text alignment metrics, providing a comprehensive analysis of the unlearning methodology. To our knowledge, Text-to-Unlearn is the first cross-modal unlearning framework for GANs, representing a flexible and efficient advancement in managing generative model behavior. </p>
<blockquote>
<p>å½“å‰æœ€å…ˆè¿›çš„ç”Ÿæˆæ¨¡å‹å±•ç°å‡ºå¼ºå¤§çš„å›¾åƒç”Ÿæˆèƒ½åŠ›ï¼Œç»™æ‰˜ç®¡è¿™äº›æ¨¡å‹çš„æœåŠ¡æä¾›å•†å¸¦æ¥äº†å„ç§ä¼¦ç†å’Œæ³•å¾‹æŒ‘æˆ˜ã€‚å› æ­¤ï¼Œå†…å®¹åˆ é™¤æŠ€æœ¯ï¼ˆCRTsï¼‰ä½œä¸ºæ§åˆ¶è¾“å‡ºè€Œæ— éœ€å…¨é¢å†è®­ç»ƒçš„ç ”ç©¶é¢†åŸŸæ­£æ—¥ç›Šå…´èµ·ã€‚è¿‘æœŸçš„å·¥ä½œæ¢ç´¢äº†åœ¨ç”Ÿæˆæ¨¡å‹ä¸­ä½¿ç”¨æœºå™¨é—å¿˜æ¥åº”å¯¹å†…å®¹åˆ é™¤ã€‚ç„¶è€Œï¼Œè¿™ç±»ç ”ç©¶çš„é‡ç‚¹ä¸»è¦é›†ä¸­åœ¨æ‰©æ•£æ¨¡å‹ä¸Šï¼Œç”Ÿæˆå¯¹æŠ—ç½‘ç»œï¼ˆGANsï¼‰ä¸­çš„é—å¿˜å´è¢«å¤§å¤§å¿½è§†äº†ã€‚æˆ‘ä»¬é€šè¿‡æå‡ºText-to-Unlearnæ¥è§£å†³è¿™ä¸€ç©ºç™½ï¼Œè¿™æ˜¯ä¸€ä¸ªæ–°é¢–çš„æ¡†æ¶ï¼Œèƒ½å¤Ÿä»…ä½¿ç”¨æ–‡æœ¬æç¤ºä»é¢„è®­ç»ƒçš„GANsä¸­é€‰æ‹©æ€§é—å¿˜æ¦‚å¿µï¼Œå®ç°ç‰¹å¾é—å¿˜ã€èº«ä»½é—å¿˜ä»¥åŠåœ¨äººè„¸è®­ç»ƒæ¨¡å‹ä¸­çš„è¡¨æƒ…å’Œå¤šå±æ€§åˆ é™¤ç­‰ç²¾ç»†ä»»åŠ¡ã€‚æˆ‘ä»¬çš„æ–¹æ³•åˆ©ç”¨è‡ªç„¶è¯­è¨€æè¿°æ¥å¼•å¯¼é—å¿˜è¿‡ç¨‹ï¼Œæ— éœ€é¢å¤–çš„æ•°æ®é›†æˆ–ç›‘ç£å¾®è°ƒï¼Œæä¾›å¯ä¼¸ç¼©å’Œé«˜æ•ˆçš„è§£å†³æ–¹æ¡ˆã€‚ä¸ºäº†è¯„ä¼°å…¶æœ‰æ•ˆæ€§ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§è‡ªåŠ¨é—å¿˜è¯„ä¼°æ–¹æ³•ï¼Œè¯¥æ–¹æ³•æ”¹ç¼–è‡ªæœ€å…ˆè¿›çš„å›¾åƒæ–‡æœ¬å¯¹é½æŒ‡æ ‡ï¼Œå¯¹é—å¿˜æ–¹æ³•è¿›è¡Œå…¨é¢åˆ†æã€‚æ®æˆ‘ä»¬æ‰€çŸ¥ï¼ŒText-to-Unlearnæ˜¯é¦–ä¸ªç”¨äºGANsçš„è·¨æ¨¡æ€é—å¿˜æ¡†æ¶ï¼Œä»£è¡¨ç€åœ¨ç®¡ç†ç”Ÿæˆæ¨¡å‹è¡Œä¸ºæ–¹é¢çµæ´»é«˜æ•ˆçš„è¿›æ­¥ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.01218v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºText-to-Unlearnçš„æ–°å‹æ¡†æ¶ï¼Œè¯¥æ¡†æ¶èƒ½å¤Ÿé€šè¿‡æ–‡æœ¬æç¤ºé€‰æ‹©æ€§é—å¿˜é¢„è®­ç»ƒç”Ÿæˆå¯¹æŠ—ç½‘ç»œï¼ˆGANsï¼‰ä¸­çš„æ¦‚å¿µï¼Œä»è€Œå®ç°ç‰¹å¾é—å¿˜ã€èº«ä»½é—å¿˜ä»¥åŠé¢éƒ¨è¡¨æƒ…å’Œå¤šå±æ€§é—å¿˜ç­‰ç²¾ç»†ä»»åŠ¡ã€‚è¯¥æ¡†æ¶åˆ©ç”¨è‡ªç„¶è¯­è¨€æè¿°å¼•å¯¼é—å¿˜è¿‡ç¨‹ï¼Œæ— éœ€é¢å¤–æ•°æ®é›†æˆ–ç›‘ç£å¾®è°ƒï¼Œæä¾›å¯ä¼¸ç¼©å’Œé«˜æ•ˆçš„è§£å†³æ–¹æ¡ˆã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Text-to-Unlearnæ˜¯ä¸€ä¸ªæ–°å‹æ¡†æ¶ï¼Œèƒ½å¤Ÿé€‰æ‹©æ€§é—å¿˜é¢„è®­ç»ƒç”Ÿæˆå¯¹æŠ—ç½‘ç»œï¼ˆGANsï¼‰ä¸­çš„ç‰¹å®šæ¦‚å¿µã€‚</li>
<li>è¯¥æ¡†æ¶é€šè¿‡æ–‡æœ¬æç¤ºå®ç°ç‰¹å¾é—å¿˜ã€èº«ä»½é—å¿˜ä»¥åŠé¢éƒ¨è¡¨æƒ…å’Œå¤šå±æ€§é—å¿˜ç­‰ç²¾ç»†ä»»åŠ¡ã€‚</li>
<li>Text-to-Unlearnåˆ©ç”¨è‡ªç„¶è¯­è¨€æè¿°æ¥å¼•å¯¼é—å¿˜è¿‡ç¨‹ï¼Œæ— éœ€é¢å¤–çš„æ•°æ®é›†æˆ–ç›‘ç£å¾®è°ƒã€‚</li>
<li>è¯¥æ¡†æ¶æä¾›äº†ä¸€ç§å¯ä¼¸ç¼©å’Œé«˜æ•ˆçš„è§£å†³æ–¹æ¡ˆï¼Œç”¨äºç®¡ç†ç”Ÿæˆæ¨¡å‹çš„è¡Œä¸ºã€‚</li>
<li>Text-to-Unlearné¦–æ¬¡å®ç°äº†è·¨æ¨¡æ€çš„GANsé—å¿˜æ¡†æ¶ã€‚</li>
<li>è¯¥æ¡†æ¶çš„è¯„ä»·æ–¹æ³•æ˜¯åŸºäºå…ˆè¿›çš„å›¾åƒæ–‡æœ¬å¯¹é½åº¦é‡æŒ‡æ ‡è¿›è¡Œè‡ªåŠ¨è¯„ä¼°çš„ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.01218">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-ce7efed644f29823794ac569b2343bc0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-314f1d89fb12216a96e360c7bfb5fa7e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5fe9c1683527b12a7c4c12aeb2691c2b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-7a3ef7b00893ade8c040e80318e82c73.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c9292e701a6915ddc2aca931bd67329a.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-422d801be0aac01f8bb39eb64f2e6abf.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-28943ea8a45a22e30fa7a6639aa783f0.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Neural-Pruning-for-3D-Scene-Reconstruction-Efficient-NeRF-Acceleration"><a href="#Neural-Pruning-for-3D-Scene-Reconstruction-Efficient-NeRF-Acceleration" class="headerlink" title="Neural Pruning for 3D Scene Reconstruction: Efficient NeRF Acceleration"></a>Neural Pruning for 3D Scene Reconstruction: Efficient NeRF Acceleration</h2><p><strong>Authors:Tianqi Ding, Dawei Xiang, Pablo Rivas, Liang Dong</strong></p>
<p>Neural Radiance Fields (NeRF) have become a popular 3D reconstruction approach in recent years. While they produce high-quality results, they also demand lengthy training times, often spanning days. This paper studies neural pruning as a strategy to address these concerns. We compare pruning approaches, including uniform sampling, importance-based methods, and coreset-based techniques, to reduce the model size and speed up training. Our findings show that coreset-driven pruning can achieve a 50% reduction in model size and a 35% speedup in training, with only a slight decrease in accuracy. These results suggest that pruning can be an effective method for improving the efficiency of NeRF models in resource-limited settings. </p>
<blockquote>
<p>ç¥ç»è¾å°„åœºï¼ˆNeRFï¼‰è¿‘å¹´æ¥å·²æˆä¸ºæµè¡Œçš„3Dé‡å»ºæ–¹æ³•ã€‚è™½ç„¶å®ƒä»¬èƒ½äº§ç”Ÿé«˜è´¨é‡çš„ç»“æœï¼Œä½†ä¹Ÿéœ€è¦é•¿æ—¶é—´çš„è®­ç»ƒï¼Œé€šå¸¸æŒç»­æ•°å¤©ã€‚æœ¬æ–‡ç ”ç©¶äº†ç¥ç»ç½‘ç»œä¿®å‰ªä½œä¸ºä¸€ç§è§£å†³è¿™äº›é—®é¢˜çš„ç­–ç•¥ã€‚æˆ‘ä»¬æ¯”è¾ƒäº†ä¿®å‰ªæ–¹æ³•ï¼ŒåŒ…æ‹¬å‡åŒ€é‡‡æ ·ã€åŸºäºé‡è¦æ€§æ–¹æ³•å’ŒåŸºäºæ ¸å¿ƒé›†çš„æŠ€æœ¯ï¼Œä»¥å‡å°æ¨¡å‹å¤§å°å¹¶åŠ å¿«è®­ç»ƒé€Ÿåº¦ã€‚æˆ‘ä»¬çš„ç ”ç©¶ç»“æœè¡¨æ˜ï¼ŒåŸºäºæ ¸å¿ƒé›†çš„ä¿®å‰ªå¯ä»¥å®ç°æ¨¡å‹å¤§å°å‡å°‘50%ï¼Œè®­ç»ƒé€Ÿåº¦æé«˜35%ï¼ŒåŒæ—¶ç²¾åº¦ç•¥æœ‰ä¸‹é™ã€‚è¿™äº›ç»“æœè¡¨æ˜ï¼Œåœ¨èµ„æºå—é™çš„ç¯å¢ƒä¸­ï¼Œä¿®å‰ªæ˜¯æé«˜NeRFæ¨¡å‹æ•ˆç‡çš„æœ‰æ•ˆæ–¹æ³•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.00950v1">PDF</a> 12 pages, 4 figures, accepted by International Conference on the AI   Revolution: Research, Ethics, and Society (AIR-RES 2025)</p>
<p><strong>Summary</strong></p>
<p>ç¥ç»ç½‘ç»œè¾å°„åœºï¼ˆNeRFï¼‰æ˜¯è¿‘å¹´æ¥æµè¡Œçš„3Dé‡å»ºæ–¹æ³•ï¼Œè™½ç„¶èƒ½ç”Ÿæˆé«˜è´¨é‡ç»“æœï¼Œä½†è®­ç»ƒæ—¶é—´é•¿ã€‚æœ¬æ–‡ç ”ç©¶ç¥ç»ç½‘ç»œä¿®å‰ªç­–ç•¥ä»¥è§£å†³é—®é¢˜ã€‚å¯¹æ¯”äº†å‡åŒ€é‡‡æ ·ã€åŸºäºé‡è¦æ€§å’ŒåŸºäºæ ¸å¿ƒé›†çš„ä¿®å‰ªæ–¹æ³•ï¼Œä»¥å‡å°æ¨¡å‹å°ºå¯¸å¹¶åŠ é€Ÿè®­ç»ƒã€‚ç ”ç©¶å‘ç°ï¼ŒåŸºäºæ ¸å¿ƒé›†çš„ä¿®å‰ªæ–¹æ³•èƒ½åœ¨ä»…è½»å¾®é™ä½å‡†ç¡®æ€§çš„æƒ…å†µä¸‹ï¼Œå®ç°æ¨¡å‹å¤§å°å‡å°‘50%ï¼Œè®­ç»ƒé€Ÿåº¦æé«˜35%ã€‚è¡¨æ˜ä¿®å‰ªæ˜¯æ”¹è¿›èµ„æºå—é™ç¯å¢ƒä¸­NeRFæ¨¡å‹æ•ˆç‡çš„æœ‰æ•ˆæ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>NeRFä½œä¸ºä¸€ç§æµè¡Œçš„3Dé‡å»ºæ–¹æ³•ï¼Œè™½ç„¶èƒ½äº§ç”Ÿé«˜è´¨é‡ç»“æœï¼Œä½†è®­ç»ƒæ—¶é—´é•¿ã€‚</li>
<li>ç¥ç»ç½‘ç»œä¿®å‰ªæ˜¯è§£å†³NeRFè®­ç»ƒæ—¶é—´é•¿çš„ä¸€ç§ç­–ç•¥ã€‚</li>
<li>å¯¹æ¯”äº†å¤šç§ä¿®å‰ªæ–¹æ³•ï¼ŒåŒ…æ‹¬å‡åŒ€é‡‡æ ·ã€åŸºäºé‡è¦æ€§å’ŒåŸºäºæ ¸å¿ƒé›†çš„ä¿®å‰ªã€‚</li>
<li>åŸºäºæ ¸å¿ƒé›†çš„ä¿®å‰ªæ–¹æ³•èƒ½æ˜¾è‘—å‡å°æ¨¡å‹å°ºå¯¸å¹¶åŠ é€Ÿè®­ç»ƒã€‚</li>
<li>æ ¸å¿ƒé›†é©±åŠ¨çš„ä¿®å‰ªèƒ½å®ç°æ¨¡å‹å¤§å°å‡å°‘50%ï¼Œè®­ç»ƒé€Ÿåº¦æé«˜35%ã€‚</li>
<li>ä¿®å‰ªæ–¹æ³•å¯¹å‡†ç¡®æ€§çš„å½±å“è¾ƒå°ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.00950">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-03357584e64e5f750a3549f963e69108.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e0a0c65908f5341c99362f273efc7be2.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-0a6e94fe72a3744b86645df943d43711.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-cbab8306837a8bb33c92d1d3b371e755.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Data-Cleansing-for-GANs"><a href="#Data-Cleansing-for-GANs" class="headerlink" title="Data Cleansing for GANs"></a>Data Cleansing for GANs</h2><p><strong>Authors:Naoyuki Terashita, Hiroki Ohashi, Satoshi Hara</strong></p>
<p>As the application of generative adversarial networks (GANs) expands, it becomes increasingly critical to develop a unified approach that improves performance across various generative tasks. One effective strategy that applies to any machine learning task is identifying harmful instances, whose removal improves the performance. While previous studies have successfully estimated these harmful training instances in supervised settings, their approaches are not easily applicable to GANs. The challenge lies in two requirements of the previous approaches that do not apply to GANs. First, previous approaches require that the absence of a training instance directly affects the parameters. However, in the training for GANs, the instances do not directly affect the generatorâ€™s parameters since they are only fed into the discriminator. Second, previous approaches assume that the change in loss directly quantifies the harmfulness of the instance to a modelâ€™s performance, while common types of GAN losses do not always reflect the generative performance. To overcome the first challenge, we propose influence estimation methods that use the Jacobian of the generatorâ€™s gradient with respect to the discriminatorâ€™s parameters (and vice versa). Such a Jacobian represents the indirect effect between two models: how removing an instance from the discriminatorâ€™s training changes the generatorâ€™s parameters. Second, we propose an instance evaluation scheme that measures the harmfulness of each training instance based on how a GAN evaluation metric (e.g., Inception score) is expected to change by the instanceâ€™s removal. Furthermore, we demonstrate that removing the identified harmful instances significantly improves the generative performance on various GAN evaluation metrics. </p>
<blockquote>
<p>éšç€ç”Ÿæˆå¯¹æŠ—ç½‘ç»œï¼ˆGANsï¼‰çš„åº”ç”¨ä¸æ–­æ‰©å¤§ï¼Œå¼€å‘ä¸€ç§é€‚ç”¨äºå„ç§ç”Ÿæˆä»»åŠ¡çš„ç»Ÿä¸€æ–¹æ³•å˜å¾—è¶Šæ¥è¶Šå…³é”®ã€‚å¯¹äºä»»ä½•æœºå™¨å­¦ä¹ ä»»åŠ¡éƒ½é€‚ç”¨çš„ä¸€ä¸ªæœ‰æ•ˆç­–ç•¥æ˜¯è¯†åˆ«æœ‰å®³å®ä¾‹ï¼Œå»é™¤è¿™äº›å®ä¾‹å¯ä»¥æé«˜æ€§èƒ½ã€‚è™½ç„¶ä»¥å‰çš„ç ”ç©¶å·²ç»åœ¨æœ‰ç›‘ç£ç¯å¢ƒä¸­æˆåŠŸåœ°ä¼°è®¡äº†è¿™äº›æœ‰å®³çš„è®­ç»ƒå®ä¾‹ï¼Œä½†å®ƒä»¬çš„æ–¹æ³•å¹¶ä¸æ˜“äºåº”ç”¨äºGANsã€‚æŒ‘æˆ˜åœ¨äºä¹‹å‰çš„æ–¹æ³•æœ‰ä¸¤ä¸ªä¸é€‚ç”¨äºGANsçš„è¦æ±‚ã€‚é¦–å…ˆï¼Œä¹‹å‰çš„æ–¹æ³•è¦æ±‚è®­ç»ƒå®ä¾‹çš„ç¼ºå¤±ç›´æ¥å½±å“å‚æ•°ã€‚ç„¶è€Œï¼Œåœ¨GANsçš„è®­ç»ƒä¸­ï¼Œå®ä¾‹å¹¶ä¸ä¼šç›´æ¥å½±å“ç”Ÿæˆå™¨çš„å‚æ•°ï¼Œå› ä¸ºå®ƒä»¬åªè¾“å…¥åˆ°åˆ¤åˆ«å™¨ä¸­ã€‚å…¶æ¬¡ï¼Œä¹‹å‰çš„æ–¹æ³•å‡è®¾æŸå¤±çš„å˜åŒ–ç›´æ¥é‡åŒ–å®ä¾‹å¯¹æ¨¡å‹æ€§èƒ½çš„æŸå®³ç¨‹åº¦ï¼Œè€Œå¸¸è§çš„GANæŸå¤±å¹¶ä¸æ€»æ˜¯åæ˜ ç”Ÿæˆæ€§èƒ½ã€‚ä¸ºäº†å…‹æœç¬¬ä¸€ä¸ªæŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä½¿ç”¨ç”Ÿæˆå™¨æ¢¯åº¦ç›¸å¯¹äºåˆ¤åˆ«å™¨å‚æ•°çš„é›…å¯æ¯”ï¼ˆä»¥åŠåä¹‹ï¼‰è¿›è¡Œå½±å“ä¼°è®¡çš„æ–¹æ³•ã€‚è¿™æ ·çš„é›…å¯æ¯”ä»£è¡¨äº†ä¸¤ä¸ªæ¨¡å‹ä¹‹é—´çš„é—´æ¥å½±å“ï¼šå³ç§»é™¤åˆ¤åˆ«å™¨è®­ç»ƒä¸­çš„ä¸€ä¸ªå®ä¾‹å¦‚ä½•æ”¹å˜ç”Ÿæˆå™¨çš„å‚æ•°ã€‚å…¶æ¬¡ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§å®ä¾‹è¯„ä¼°æ–¹æ¡ˆï¼Œè¯¥æ–¹æ¡ˆåŸºäºGANè¯„ä¼°æŒ‡æ ‡ï¼ˆä¾‹å¦‚Inceptionåˆ†æ•°ï¼‰é¢„æœŸä¼šå› å®ä¾‹çš„ç§»é™¤è€Œå¦‚ä½•å˜åŒ–æ¥æµ‹é‡æ¯ä¸ªè®­ç»ƒå®ä¾‹çš„æœ‰å®³æ€§ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¯æ˜ï¼Œå»é™¤å·²è¯†åˆ«å‡ºçš„æœ‰å®³å®ä¾‹å¯ä»¥æ˜¾è‘—æé«˜å„ç§GANè¯„ä¼°æŒ‡æ ‡çš„ç”Ÿæˆæ€§èƒ½ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.00603v1">PDF</a> Accepted for IEEE Transactions on Neural Networks and Learning   Systems (TNNLS, 2025). Journal extention of   <a target="_blank" rel="noopener" href="https://openreview.net/forum?id=opHLcXxYTC">https://openreview.net/forum?id=opHLcXxYTC</a>_</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§é’ˆå¯¹ç”Ÿæˆå¯¹æŠ—ç½‘ç»œï¼ˆGANsï¼‰çš„æœ‰å®³å®ä¾‹è¯†åˆ«æ–¹æ³•ã€‚è¯¥æ–¹æ³•é€šè¿‡è¯„ä¼°ç§»é™¤æŸä¸ªè®­ç»ƒå®ä¾‹åå¯¹ç”Ÿæˆå™¨å‚æ•°çš„å½±å“ï¼Œæ¥é‡åŒ–å®ä¾‹çš„â€œå±å®³æ€§â€ã€‚é‡‡ç”¨JacobiançŸ©é˜µè¡¡é‡ç”Ÿæˆå™¨æ¢¯åº¦ä¸åˆ¤åˆ«å™¨å‚æ•°ä¹‹é—´çš„é—´æ¥å½±å“ï¼Œå¹¶æå‡ºä¸€ç§åŸºäºGANè¯„ä»·æŒ‡æ ‡ï¼ˆå¦‚Inceptionåˆ†æ•°ï¼‰å˜åŒ–æ¥è¯„ä¼°å®ä¾‹å±å®³æ€§çš„æ–¹æ¡ˆã€‚å®éªŒè¯æ˜ï¼Œç§»é™¤è¯†åˆ«å‡ºçš„æœ‰å®³å®ä¾‹èƒ½æ˜¾è‘—æé«˜GANåœ¨å„ç§è¯„ä»·æŒ‡æ ‡ä¸Šçš„ç”Ÿæˆæ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æœ¬æ–‡æå‡ºäº†ä¸€ç§é’ˆå¯¹GANsçš„æœ‰å®³å®ä¾‹è¯†åˆ«æ–¹æ³•ï¼Œæ—¨åœ¨æé«˜å…¶åœ¨å„ç§ç”Ÿæˆä»»åŠ¡ä¸­çš„æ€§èƒ½ã€‚</li>
<li>ç°æœ‰æ–¹æ³•éš¾ä»¥ç›´æ¥åº”ç”¨äºGANsï¼Œå› ä¸ºGANsä¸­è®­ç»ƒå®ä¾‹å¹¶ä¸ç›´æ¥å½±å“ç”Ÿæˆå™¨å‚æ•°ã€‚</li>
<li>é€šè¿‡ä½¿ç”¨JacobiançŸ©é˜µè¡¡é‡ç”Ÿæˆå™¨å’Œåˆ¤åˆ«å™¨å‚æ•°é—´çš„é—´æ¥å½±å“ï¼Œæ¥å…‹æœè¿™ä¸€æŒ‘æˆ˜ã€‚</li>
<li>æå‡ºäº†ä¸€ç§å®ä¾‹è¯„ä¼°æ–¹æ¡ˆï¼Œè¯¥æ–¹æ¡ˆåŸºäºGANè¯„ä»·æŒ‡æ ‡ï¼ˆå¦‚Inceptionåˆ†æ•°ï¼‰çš„é¢„æœŸå˜åŒ–æ¥è¯„ä¼°æ¯ä¸ªè®­ç»ƒå®ä¾‹çš„â€œå±å®³æ€§â€ã€‚</li>
<li>è¿™ç§æ–¹æ³•ä¸ä»…ç†è®ºä¸Šæœ‰ä¾æ®ï¼Œè€Œä¸”å®éªŒç»“æœæ˜¾ç¤ºï¼Œç§»é™¤è¯†åˆ«å‡ºçš„æœ‰å®³å®ä¾‹èƒ½æ˜¾è‘—æé«˜GANçš„ç”Ÿæˆæ€§èƒ½ã€‚</li>
<li>è¯¥æ–¹æ³•å…·æœ‰å¹¿æ³›çš„åº”ç”¨å‰æ™¯ï¼Œå¯åº”ç”¨äºå…¶ä»–æœºå™¨å­¦ä¹ ä»»åŠ¡ä¸­æœ‰å®³å®ä¾‹çš„è¯†åˆ«ä¸ç§»é™¤ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.00603">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-2e44131fb54901bb97be4d234ea012a0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7400537dee6e46df021ca29ff2e23dfe.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="LITA-GS-Illumination-Agnostic-Novel-View-Synthesis-via-Reference-Free-3D-Gaussian-Splatting-and-Physical-Priors"><a href="#LITA-GS-Illumination-Agnostic-Novel-View-Synthesis-via-Reference-Free-3D-Gaussian-Splatting-and-Physical-Priors" class="headerlink" title="LITA-GS: Illumination-Agnostic Novel View Synthesis via Reference-Free   3D Gaussian Splatting and Physical Priors"></a>LITA-GS: Illumination-Agnostic Novel View Synthesis via Reference-Free   3D Gaussian Splatting and Physical Priors</h2><p><strong>Authors:Han Zhou, Wei Dong, Jun Chen</strong></p>
<p>Directly employing 3D Gaussian Splatting (3DGS) on images with adverse illumination conditions exhibits considerable difficulty in achieving high-quality, normally-exposed representations due to: (1) The limited Structure from Motion (SfM) points estimated in adverse illumination scenarios fail to capture sufficient scene details; (2) Without ground-truth references, the intensive information loss, significant noise, and color distortion pose substantial challenges for 3DGS to produce high-quality results; (3) Combining existing exposure correction methods with 3DGS does not achieve satisfactory performance due to their individual enhancement processes, which lead to the illumination inconsistency between enhanced images from different viewpoints. To address these issues, we propose LITA-GS, a novel illumination-agnostic novel view synthesis method via reference-free 3DGS and physical priors. Firstly, we introduce an illumination-invariant physical prior extraction pipeline. Secondly, based on the extracted robust spatial structure prior, we develop the lighting-agnostic structure rendering strategy, which facilitates the optimization of the scene structure and object appearance. Moreover, a progressive denoising module is introduced to effectively mitigate the noise within the light-invariant representation. We adopt the unsupervised strategy for the training of LITA-GS and extensive experiments demonstrate that LITA-GS surpasses the state-of-the-art (SOTA) NeRF-based method while enjoying faster inference speed and costing reduced training time. The code is released at <a target="_blank" rel="noopener" href="https://github.com/LowLevelAI/LITA-GS">https://github.com/LowLevelAI/LITA-GS</a>. </p>
<blockquote>
<p>ç›´æ¥å¯¹ä¸è‰¯ç…§æ˜æ¡ä»¶ä¸‹çš„å›¾åƒåº”ç”¨ä¸‰ç»´é«˜æ–¯å¹³é“ºï¼ˆ3DGSï¼‰åœ¨å®ç°é«˜è´¨é‡çš„æ­£å¸¸æ›å…‰è¡¨ç¤ºæ–¹é¢å­˜åœ¨ç›¸å½“å¤§çš„å›°éš¾ï¼ŒåŸå› å¦‚ä¸‹ï¼šï¼ˆ1ï¼‰åœ¨ä¸è‰¯ç…§æ˜åœºæ™¯ä¸­ä¼°è®¡çš„è¿åŠ¨ç»“æ„ï¼ˆSfMï¼‰ç‚¹æœ‰é™ï¼Œæ— æ³•æ•è·è¶³å¤Ÿçš„åœºæ™¯ç»†èŠ‚ï¼›ï¼ˆ2ï¼‰æ²¡æœ‰çœŸå®å‚è€ƒï¼Œå¯†é›†çš„ä¿¡æ¯ä¸¢å¤±ã€æ˜¾è‘—çš„å™ªå£°å’Œè‰²å½©å¤±çœŸç»™3DGSå¸¦æ¥å·¨å¤§æŒ‘æˆ˜ï¼Œéš¾ä»¥äº§ç”Ÿé«˜è´¨é‡çš„ç»“æœï¼›ï¼ˆ3ï¼‰å°†ç°æœ‰çš„æ›å…‰æ ¡æ­£æ–¹æ³•ä¸3DGSç›¸ç»“åˆå¹¶æœªå®ç°ä»¤äººæ»¡æ„çš„æ€§èƒ½ï¼Œå› ä¸ºå®ƒä»¬å„è‡ªçš„å¢å¼ºå¤„ç†è¿‡ç¨‹å¯¼è‡´ä»ä¸åŒè§†ç‚¹å¢å¼ºçš„å›¾åƒä¹‹é—´ç…§æ˜ä¸ä¸€è‡´ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†LITA-GSï¼Œè¿™æ˜¯ä¸€ç§æ–°çš„ç…§æ˜æ— å…³çš„æ–°è§†å›¾åˆæˆæ–¹æ³•ï¼Œé€šè¿‡æ— å‚è€ƒçš„3DGSå’Œç‰©ç†å…ˆéªŒæ¥å®ç°ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ªå…‰ç…§ä¸å˜ç‰©ç†å…ˆéªŒæå–ç®¡é“ã€‚å…¶æ¬¡ï¼ŒåŸºäºæå–çš„ç¨³å¥ç©ºé—´ç»“æ„å…ˆéªŒï¼Œæˆ‘ä»¬å¼€å‘äº†å…‰ç…§æ— å…³çš„ç»“æ„æ¸²æŸ“ç­–ç•¥ï¼Œè¿™æœ‰åŠ©äºä¼˜åŒ–åœºæ™¯ç»“æ„å’Œå¯¹è±¡å¤–è§‚ã€‚æ­¤å¤–ï¼Œå¼•å…¥äº†ä¸€ä¸ªæ¸è¿›çš„é™å™ªæ¨¡å—ï¼Œä»¥æœ‰æ•ˆåœ°å‡è½»å…‰ä¸å˜è¡¨ç¤ºä¸­çš„å™ªå£°ã€‚æˆ‘ä»¬é‡‡ç”¨æ— ç›‘ç£ç­–ç•¥å¯¹LITA-GSè¿›è¡Œè®­ç»ƒï¼Œå¤§é‡å®éªŒè¡¨æ˜ï¼ŒLITA-GSè¶…è¶Šäº†åŸºäºNeRFçš„æœ€å…ˆè¿›æ–¹æ³•ï¼ŒåŒæ—¶æ‹¥æœ‰æ›´å¿«çš„æ¨ç†é€Ÿåº¦å’Œæ›´çŸ­çš„è®­ç»ƒæ—¶é—´ã€‚ä»£ç å·²å‘å¸ƒåœ¨<a target="_blank" rel="noopener" href="https://github.com/LowLevelAI/LITA-GS%E3%80%82">https://github.com/LowLevelAI/LITA-GSã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.00219v1">PDF</a> Accepted by CVPR 2025. 3DGS, Adverse illumination conditions,   Reference-free, Physical priors</p>
<p><strong>Summary</strong></p>
<p>è¯¥æ–‡æœ¬ä¸»è¦ä»‹ç»äº†åœ¨ä¸è‰¯å…‰ç…§æ¡ä»¶ä¸‹ç›´æ¥åº”ç”¨3Dé«˜æ–¯è´´å›¾ï¼ˆ3DGSï¼‰é¢ä¸´çš„æŒ‘æˆ˜ï¼Œå¹¶æå‡ºäº†è§£å†³æ–¹æ¡ˆã€‚é€šè¿‡å¼•å…¥å…‰ç…§ä¸å˜ç‰©ç†å…ˆéªŒæå–ç®¡é“å’Œå…‰ç…§æ— å…³çš„ç»“æ„æ¸²æŸ“ç­–ç•¥ï¼Œä»¥åŠæ¸è¿›å¼é™å™ªæ¨¡å—ï¼ŒæˆåŠŸè§£å†³äº†å› å…‰ç…§ä¸ä½³å¯¼è‡´çš„ç»“æ„ç»†èŠ‚ç¼ºå¤±ã€ä¿¡æ¯ä¸¢å¤±ã€å™ªå£°å’Œè‰²å½©å¤±çœŸç­‰é—®é¢˜ã€‚æ‰€æå‡ºçš„æ–¹æ³•åœ¨è®­ç»ƒå’Œæ¨ç†é€Ÿåº¦ä¸Šä¼˜äºç°æœ‰çš„NeRFæ–¹æ³•ï¼Œå¹¶å·²åœ¨GitHubä¸Šå‘å¸ƒç›¸å…³ä»£ç ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>åœ¨ä¸è‰¯å…‰ç…§æ¡ä»¶ä¸‹ç›´æ¥åº”ç”¨3Dé«˜æ–¯è´´å›¾ï¼ˆ3DGSï¼‰é¢ä¸´æŒ‘æˆ˜ï¼ŒåŒ…æ‹¬ç»“æ„ç»†èŠ‚ç¼ºå¤±ã€ä¿¡æ¯ä¸¢å¤±ã€å™ªå£°å’Œè‰²å½©å¤±çœŸç­‰é—®é¢˜ã€‚</li>
<li>æå‡ºäº†ä¸€ç§æ–°çš„å…‰ç…§æ— å…³çš„æ–°è§†è§’åˆæˆæ–¹æ³•LITA-GSï¼Œè¯¥æ–¹æ³•ç»“åˆäº†æ— å‚è€ƒçš„3DGSå’Œç‰©ç†å…ˆéªŒæŠ€æœ¯æ¥è§£å†³ä¸Šè¿°é—®é¢˜ã€‚</li>
<li>LITA-GSå¼•å…¥äº†å…‰ç…§ä¸å˜ç‰©ç†å…ˆéªŒæå–ç®¡é“ï¼Œä»¥æå–ç¨³å¥çš„ç©ºé—´ç»“æ„å…ˆéªŒã€‚</li>
<li>åŸºäºæå–çš„ç¨³å¥ç©ºé—´ç»“æ„å…ˆéªŒï¼Œå¼€å‘äº†å…‰ç…§æ— å…³çš„ç»“æ„æ¸²æŸ“ç­–ç•¥ï¼Œä¼˜åŒ–äº†åœºæ™¯ç»“æ„å’Œç‰©ä½“å¤–è§‚ã€‚</li>
<li>LITA-GSè¿˜å¼•å…¥äº†æ¸è¿›å¼é™å™ªæ¨¡å—ï¼Œæœ‰æ•ˆå‡è½»äº†å…‰ç…§ä¸å˜è¡¨ç¤ºä¸­çš„å™ªå£°ã€‚</li>
<li>LITA-GSé‡‡ç”¨æ— ç›‘ç£ç­–ç•¥è¿›è¡Œè®­ç»ƒï¼Œå¹¶åœ¨å®éªŒä¸­è¡¨ç°å‡ºä¼˜äºç°æœ‰NeRFæ–¹æ³•çš„æ€§èƒ½ï¼ŒåŒæ—¶æ‹¥æœ‰æ›´å¿«çš„æ¨ç†é€Ÿåº¦å’Œå‡å°‘çš„è®­ç»ƒæ—¶é—´ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.00219">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-724b60f30c6aa43bb38a6f183871b706.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-17aa639b3cdddc86604f9ed53f258f16.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c37b58252ecdf8d6303c5f74971ec484.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-4f92eeb53810a3b1e93299a7488f9dee.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="ERUPT-Efficient-Rendering-with-Unposed-Patch-Transformer"><a href="#ERUPT-Efficient-Rendering-with-Unposed-Patch-Transformer" class="headerlink" title="ERUPT: Efficient Rendering with Unposed Patch Transformer"></a>ERUPT: Efficient Rendering with Unposed Patch Transformer</h2><p><strong>Authors:Maxim V. Shugaev, Vincent Chen, Maxim Karrenbach, Kyle Ashley, Bridget Kennedy, Naresh P. Cuntoor</strong></p>
<p>This work addresses the problem of novel view synthesis in diverse scenes from small collections of RGB images. We propose ERUPT (Efficient Rendering with Unposed Patch Transformer) a state-of-the-art scene reconstruction model capable of efficient scene rendering using unposed imagery. We introduce patch-based querying, in contrast to existing pixel-based queries, to reduce the compute required to render a target view. This makes our model highly efficient both during training and at inference, capable of rendering at 600 fps on commercial hardware. Notably, our model is designed to use a learned latent camera pose which allows for training using unposed targets in datasets with sparse or inaccurate ground truth camera pose. We show that our approach can generalize on large real-world data and introduce a new benchmark dataset (MSVS-1M) for latent view synthesis using street-view imagery collected from Mapillary. In contrast to NeRF and Gaussian Splatting, which require dense imagery and precise metadata, ERUPT can render novel views of arbitrary scenes with as few as five unposed input images. ERUPT achieves better rendered image quality than current state-of-the-art methods for unposed image synthesis tasks, reduces labeled data requirements by ~95% and decreases computational requirements by an order of magnitude, providing efficient novel view synthesis for diverse real-world scenes. </p>
<blockquote>
<p>æœ¬æ–‡è§£å†³äº†ä»å°‘é‡RGBå›¾åƒé›†åˆä¸­åˆæˆå¤šæ ·åœºæ™¯çš„æ–°è§†è§’çš„é—®é¢˜ã€‚æˆ‘ä»¬æå‡ºäº†ERUPTï¼ˆæ— é¢„è®¾è¡¥ä¸è½¬æ¢å™¨çš„æœ‰æ•ˆæ¸²æŸ“ï¼‰è¿™ä¸€å…ˆè¿›çš„åœºæ™¯é‡å»ºæ¨¡å‹ï¼Œå®ƒèƒ½å¤Ÿåˆ©ç”¨æ— é¢„è®¾çš„å›¾åƒè¿›è¡Œé«˜æ•ˆåœºæ™¯æ¸²æŸ“ã€‚æˆ‘ä»¬å¼•å…¥äº†åŸºäºè¡¥ä¸çš„æŸ¥è¯¢ï¼Œä¸ç°æœ‰çš„åŸºäºåƒç´ çš„æŸ¥è¯¢ç›¸æ¯”ï¼Œå‡å°‘äº†æ¸²æŸ“ç›®æ ‡è§†å›¾æ‰€éœ€çš„è®¡ç®—é‡ã€‚è¿™ä½¿å¾—æˆ‘ä»¬çš„æ¨¡å‹åœ¨è®­ç»ƒå’Œæ¨ç†è¿‡ç¨‹ä¸­éƒ½éå¸¸é«˜æ•ˆï¼Œèƒ½å¤Ÿåœ¨å•†ç”¨ç¡¬ä»¶ä¸Šä»¥600å¸§&#x2F;ç§’çš„é€Ÿåº¦è¿›è¡Œæ¸²æŸ“ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œæˆ‘ä»¬çš„æ¨¡å‹é‡‡ç”¨å­¦ä¹ æ½œåœ¨çš„ç›¸æœºå§¿æ€è®¾è®¡ï¼Œè¿™å…è®¸åœ¨æ•°æ®é›†ä¸Šä½¿ç”¨æ— é¢„è®¾ç›®æ ‡è¿›è¡Œè®­ç»ƒï¼Œå³ä½¿æ•°æ®é›†å…·æœ‰ç¨€ç–æˆ–ä¸å‡†ç¡®çš„çœŸå®ç›¸æœºå§¿æ€ã€‚æˆ‘ä»¬è¯æ˜äº†æˆ‘ä»¬çš„æ–¹æ³•å¯ä»¥åœ¨å¤§å‹çœŸå®ä¸–ç•Œæ•°æ®ä¸Šè¿›è¡Œæ¨å¹¿ï¼Œå¹¶ä½¿ç”¨Mapillaryæ”¶é›†çš„è¡—æ™¯å›¾åƒå¼•å…¥äº†ç”¨äºæ½œåœ¨è§†å›¾åˆæˆçš„æ–°åŸºå‡†æ•°æ®é›†ï¼ˆMSVS-1Mï¼‰ã€‚ä¸éœ€è¦å¯†é›†å›¾åƒå’Œç²¾ç¡®å…ƒæ•°æ®çš„NeRFå’Œé«˜æ–¯æ‹¼è´´ä¸åŒï¼ŒERUPTä»…ä½¿ç”¨äº”å¼ æ— é¢„è®¾çš„è¾“å…¥å›¾åƒå°±å¯ä»¥æ¸²æŸ“ä»»æ„åœºæ™¯çš„æ–°è§†è§’ã€‚ERUPTåœ¨æ— é¢„è®¾å›¾åƒåˆæˆä»»åŠ¡ä¸Šå®ç°äº†æ¯”å½“å‰æœ€å…ˆè¿›æ–¹æ³•æ›´å¥½çš„æ¸²æŸ“å›¾åƒè´¨é‡ï¼Œå°†æ ‡è®°æ•°æ®è¦æ±‚é™ä½äº†çº¦95ï¼…ï¼Œå¹¶å°†è®¡ç®—è¦æ±‚é™ä½äº†æ•°é‡çº§ï¼Œä¸ºå¤šæ ·åŒ–çš„çœŸå®ä¸–ç•Œåœºæ™¯æä¾›äº†é«˜æ•ˆçš„æ–°è§†è§’åˆæˆã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.24374v1">PDF</a> Accepted to CVPR 2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºä¸€ç§ç§°ä¸ºERUPTï¼ˆåŸºäºæœªæ ‡å®šå›¾åƒå—çš„é«˜æ•ˆæ¸²æŸ“ï¼‰çš„åœºæ™¯é‡å»ºæ¨¡å‹ï¼Œç”¨äºè§£å†³ä»å°‘é‡RGBå›¾åƒä¸­åˆæˆæ–°é¢–è§†è§’çš„é—®é¢˜ã€‚é€šè¿‡å¼•å…¥åŸºäºå›¾åƒå—çš„æŸ¥è¯¢æ–¹æ³•ï¼Œé™ä½äº†æ¸²æŸ“ç›®æ ‡è§†è§’æ‰€éœ€çš„è®¡ç®—é‡ï¼Œä½¿æ¨¡å‹åœ¨è®­ç»ƒå’Œæ¨ç†æ—¶éƒ½å…·å¤‡é«˜æ•ˆç‡ï¼Œå¯åœ¨å•†ç”¨ç¡¬ä»¶ä¸Šä»¥600å¸§&#x2F;ç§’çš„é€Ÿåº¦è¿›è¡Œæ¸²æŸ“ã€‚è¯¥æ¨¡å‹è®¾è®¡ç”¨äºå­¦ä¹ æ½œåœ¨ç›¸æœºå§¿æ€ï¼Œå¯åœ¨æ•°æ®é›†ä¸Šä½¿ç”¨æœªæ ‡å®šçš„ç›®æ ‡è¿›è¡Œè®­ç»ƒï¼Œå³ä½¿æ•°æ®é›†å…·æœ‰ç¨€ç–æˆ–ä¸å‡†ç¡®çš„çœŸå®ç›¸æœºå§¿æ€ã€‚å®éªŒè¡¨æ˜ï¼ŒERUPTæ–¹æ³•èƒ½å¤Ÿåœ¨å¤§å‹çœŸå®ä¸–ç•Œæ•°æ®ä¸Šè¿›è¡Œæ¨å¹¿ï¼Œå¹¶å¼•å…¥äº†ä¸€ä¸ªæ–°çš„åŸºå‡†æ•°æ®é›†MSVS-1Mï¼Œç”¨äºä½¿ç”¨æ¥è‡ªMapillaryçš„è¡—é“è§†å›¾å›¾åƒè¿›è¡Œæ½œåœ¨è§†å›¾åˆæˆã€‚ä¸éœ€è¦å¯†é›†å›¾åƒå’Œç²¾ç¡®å…ƒæ•°æ®çš„NeRFå’Œé«˜æ–¯æ‹¼è´´ç›¸æ¯”ï¼ŒERUPTä»…éœ€å°‘æ•°æœªæ ‡å®šçš„è¾“å…¥å›¾åƒå³å¯åˆæˆæ–°é¢–è§†è§’ã€‚ERUPTåœ¨æœªç»æ ‡å®šçš„å›¾åƒåˆæˆä»»åŠ¡ä¸Šå®ç°äº†æ¯”å½“å‰å…ˆè¿›æŠ€æœ¯æ›´å¥½çš„æ¸²æŸ“å›¾åƒè´¨é‡ï¼Œå°†æ ‡è®°æ•°æ®éœ€æ±‚å‡å°‘äº†çº¦95ï¼…ï¼Œå¹¶å°†è®¡ç®—éœ€æ±‚é™ä½äº†ä¸€ä¸ªæ•°é‡çº§ï¼Œä¸ºå¤šæ ·åŒ–çš„çœŸå®ä¸–ç•Œåœºæ™¯æä¾›äº†é«˜æ•ˆçš„æ–°é¢–è§†è§’åˆæˆã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ERUPTæ¨¡å‹è§£å†³äº†ä»å°‘é‡RGBå›¾åƒä¸­è¿›è¡Œæ–°é¢–è§†è§’åˆæˆçš„é—®é¢˜ã€‚</li>
<li>é€šè¿‡å¼•å…¥åŸºäºå›¾åƒå—çš„æŸ¥è¯¢æ–¹æ³•ï¼Œæé«˜äº†æ¸²æŸ“æ•ˆç‡ã€‚</li>
<li>æ¨¡å‹å…·å¤‡é«˜æ•ˆçš„å­¦ä¹ èƒ½åŠ›ï¼Œå¯åœ¨è®­ç»ƒå’Œæ¨ç†æ—¶å¿«é€Ÿæ¸²æŸ“ã€‚</li>
<li>ERUPTå¯ä»¥åœ¨å•†ä¸šç¡¬ä»¶ä¸Šä»¥é«˜è¾¾600å¸§&#x2F;ç§’çš„é€Ÿåº¦è¿›è¡Œæ¸²æŸ“ã€‚</li>
<li>æ¨¡å‹è®¾è®¡ç”¨äºå­¦ä¹ æ½œåœ¨ç›¸æœºå§¿æ€ï¼Œé€‚åº”äºæ•°æ®é›†ä¸Šæœªæ ‡å®šçš„ç›®æ ‡ã€‚</li>
<li>å¼•å…¥æ–°çš„åŸºå‡†æ•°æ®é›†MSVS-1Mï¼Œç”¨äºæ½œåœ¨è§†å›¾åˆæˆã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.24374">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-ca11a43b27495e28893890b44f8abb7e.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-f5bda767db22f2952901ef41d0f05e35.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-81bcaa3ca5eff325183266b91206f977.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-c2449e77485b4db1e24f8a3a2589fa36.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Style-Quantization-for-Data-Efficient-GAN-Training"><a href="#Style-Quantization-for-Data-Efficient-GAN-Training" class="headerlink" title="Style Quantization for Data-Efficient GAN Training"></a>Style Quantization for Data-Efficient GAN Training</h2><p><strong>Authors:Jian Wang, Xin Lan, Jizhe Zhou, Yuxin Tian, Jiancheng Lv</strong></p>
<p>Under limited data setting, GANs often struggle to navigate and effectively exploit the input latent space. Consequently, images generated from adjacent variables in a sparse input latent space may exhibit significant discrepancies in realism, leading to suboptimal consistency regularization (CR) outcomes. To address this, we propose \textit{SQ-GAN}, a novel approach that enhances CR by introducing a style space quantization scheme. This method transforms the sparse, continuous input latent space into a compact, structured discrete proxy space, allowing each element to correspond to a specific real data point, thereby improving CR performance. Instead of direct quantization, we first map the input latent variables into a less entangled &#96;&#96;styleâ€™â€™ space and apply quantization using a learnable codebook. This enables each quantized code to control distinct factors of variation. Additionally, we optimize the optimal transport distance to align the codebook codes with features extracted from the training data by a foundation model, embedding external knowledge into the codebook and establishing a semantically rich vocabulary that properly describes the training dataset. Extensive experiments demonstrate significant improvements in both discriminator robustness and generation quality with our method. </p>
<blockquote>
<p>åœ¨æœ‰é™æ•°æ®è®¾ç½®ä¸‹ï¼Œç”Ÿæˆå¯¹æŠ—ç½‘ç»œï¼ˆGANsï¼‰é€šå¸¸éš¾ä»¥å¯¼èˆªå¹¶æœ‰æ•ˆåœ°åˆ©ç”¨è¾“å…¥æ½œåœ¨ç©ºé—´ã€‚å› æ­¤ï¼Œåœ¨ç¨€ç–è¾“å…¥æ½œåœ¨ç©ºé—´ä¸­ä»ç›¸é‚»å˜é‡ç”Ÿæˆçš„å›¾åƒåœ¨é€¼çœŸåº¦æ–¹é¢å¯èƒ½å­˜åœ¨æ˜¾è‘—å·®å¼‚ï¼Œå¯¼è‡´æ¬¡ä¼˜çš„ä¸€è‡´æ€§æ­£åˆ™åŒ–ï¼ˆCRï¼‰ç»“æœã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†SQ-GANï¼Œè¿™æ˜¯ä¸€ç§é€šè¿‡å¼•å…¥é£æ ¼ç©ºé—´é‡åŒ–æ–¹æ¡ˆæ¥å¢å¼ºCRçš„æ–°æ–¹æ³•ã€‚è¯¥æ–¹æ³•å°†ç¨€ç–ã€è¿ç»­çš„è¾“å…¥æ½œåœ¨ç©ºé—´è½¬æ¢ä¸ºä¸€ä¸ªç´§å‡‘ã€ç»“æ„åŒ–çš„ç¦»æ•£ä»£ç†ç©ºé—´ï¼Œä½¿æ¯ä¸ªå…ƒç´ éƒ½èƒ½å¯¹åº”ä¸€ä¸ªç‰¹å®šçš„çœŸå®æ•°æ®ç‚¹ï¼Œä»è€Œæé«˜CRæ€§èƒ½ã€‚æˆ‘ä»¬ä¸æ˜¯ç›´æ¥è¿›è¡Œé‡åŒ–ï¼Œè€Œæ˜¯é¦–å…ˆå°†è¾“å…¥æ½œåœ¨å˜é‡æ˜ å°„åˆ°ä¸€ä¸ªä¸é‚£ä¹ˆçº ç¼ çš„â€œé£æ ¼â€ç©ºé—´ï¼Œå¹¶ä½¿ç”¨å¯å­¦ä¹ çš„ä»£ç æœ¬è¿›è¡Œé‡åŒ–ã€‚è¿™ä½¿å¾—æ¯ä¸ªé‡åŒ–ä»£ç èƒ½å¤Ÿæ§åˆ¶ä¸åŒçš„å˜å¼‚å› ç´ ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬ä¼˜åŒ–äº†æœ€ä¼˜ä¼ è¾“è·ç¦»ï¼Œå°†ä»£ç æœ¬ä»£ç ä¸åŸºç¡€æ¨¡å‹ä»è®­ç»ƒæ•°æ®ä¸­æå–çš„ç‰¹å¾è¿›è¡Œå¯¹é½ï¼Œå°†å¤–éƒ¨çŸ¥è¯†åµŒå…¥åˆ°ä»£ç æœ¬ä¸­ï¼Œå¹¶å»ºç«‹è¯­ä¹‰ä¸°å¯Œçš„è¯æ±‡è¡¨ï¼Œæ°å½“åœ°æè¿°è®­ç»ƒæ•°æ®é›†ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨åˆ¤åˆ«å™¨é²æ£’æ€§å’Œç”Ÿæˆè´¨é‡æ–¹é¢éƒ½æœ‰æ˜¾è‘—æé«˜ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.24282v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºä¸€ç§åä¸ºSQ-GANçš„æ–°æ–¹æ³•ï¼Œé€šè¿‡å¼•å…¥é£æ ¼ç©ºé—´é‡åŒ–æ–¹æ¡ˆæ”¹è¿›ä¸€è‡´æ€§æ­£åˆ™åŒ–ï¼ˆCRï¼‰ã€‚è¯¥æ–¹æ³•å°†ç¨€ç–ã€è¿ç»­çš„è¾“å…¥æ½œåœ¨ç©ºé—´è½¬åŒ–ä¸ºç´§å‡‘ã€ç»“æ„åŒ–çš„ç¦»æ•£ä»£ç†ç©ºé—´ï¼Œä½¿æ¯ä¸ªå…ƒç´ å¯¹åº”ä¸€ä¸ªçœŸå®æ•°æ®ç‚¹ï¼Œä»è€Œæé«˜CRæ€§èƒ½ã€‚å®éªŒè¯æ˜ï¼Œè¯¥æ–¹æ³•åœ¨åˆ¤åˆ«å™¨ç¨³å¥æ€§å’Œç”Ÿæˆè´¨é‡æ–¹é¢æœ‰æ˜æ˜¾æ”¹è¿›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>GANsåœ¨æœ‰é™æ•°æ®è®¾ç½®ä¸‹é¢ä¸´å¯¼èˆªå’Œæœ‰æ•ˆæ¢ç´¢è¾“å…¥æ½œåœ¨ç©ºé—´çš„æŒ‘æˆ˜ã€‚</li>
<li>ç¨€ç–è¾“å…¥æ½œåœ¨ç©ºé—´ä¸­ï¼Œç›¸é‚»å˜é‡ç”Ÿæˆçš„å›¾åƒåœ¨çœŸå®æ€§ä¸Šå¯èƒ½å­˜åœ¨æ˜¾è‘—å·®å¼‚ï¼Œå¯¼è‡´CRç»“æœä¸ç†æƒ³ã€‚</li>
<li>SQ-GANé€šè¿‡å¼•å…¥é£æ ¼ç©ºé—´é‡åŒ–æ–¹æ¡ˆæ”¹è¿›CRæ€§èƒ½ã€‚</li>
<li>è¯¥æ–¹æ³•å°†ç¨€ç–ã€è¿ç»­çš„è¾“å…¥æ½œåœ¨ç©ºé—´è½¬åŒ–ä¸ºç´§å‡‘ã€ç»“æ„åŒ–çš„ç¦»æ•£ä»£ç†ç©ºé—´ï¼Œæ¯ä¸ªå…ƒç´ å¯¹åº”ä¸€ä¸ªçœŸå®æ•°æ®ç‚¹ã€‚</li>
<li>ä½¿ç”¨å¯å­¦ä¹ çš„ä»£ç æœ¬è¿›è¡Œé‡åŒ–ï¼Œè€Œéç›´æ¥é‡åŒ–ï¼Œä½¿æ¯ä¸ªé‡åŒ–ä»£ç æ§åˆ¶ä¸åŒçš„å˜é‡å˜åŒ–å› ç´ ã€‚</li>
<li>é€šè¿‡ä¼˜åŒ–ä¼ è¾“è·ç¦»ï¼Œå°†ä»£ç æœ¬ä»£ç ä¸åŸºç¡€æ¨¡å‹ä»è®­ç»ƒæ•°æ®ä¸­æå–çš„ç‰¹å¾å¯¹é½ï¼Œèå…¥å¤–éƒ¨çŸ¥è¯†ï¼Œå»ºç«‹è¯­ä¹‰ä¸°å¯Œçš„è¯æ±‡è¡¨ï¼Œæè¿°è®­ç»ƒæ•°æ®é›†ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.24282">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-20f7a357be35bf0e24ef0897b3f51b03.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6b991e157764a9b779fa1b915f2277ca.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="Strain-distribution-in-GaN-AlN-superlattices-grown-on-AlN-sapphire-templates-comparison-of-X-ray-diffraction-and-photoluminescence-studies"><a href="#Strain-distribution-in-GaN-AlN-superlattices-grown-on-AlN-sapphire-templates-comparison-of-X-ray-diffraction-and-photoluminescence-studies" class="headerlink" title="Strain distribution in GaN&#x2F;AlN superlattices grown on AlN&#x2F;sapphire   templates: comparison of X-ray diffraction and photoluminescence studies"></a>Strain distribution in GaN&#x2F;AlN superlattices grown on AlN&#x2F;sapphire   templates: comparison of X-ray diffraction and photoluminescence studies</h2><p><strong>Authors:Aleksandra Wierzbicka, Agata Kaminska, Kamil Sobczak, Dawid Jankowski, Kamil Koronski, Pawel Strak, Marta Sobanska, Zbigniew R. Zytkiewicz</strong></p>
<p>Series of GaN&#x2F;AlN superlattices (SLs) with various periods and the same thicknesses of GaN quantum wells and AlN barriers have been investigated. X-ray diffraction, photoluminescence (PL) and transmission electron microscopy (TEM) techniques were used to study the influence of thickness of AlN and GaN sublayers on strain distribution in GaN&#x2F;AlN SL structures. Detailed X-ray diffraction measurements demonstrate that the strain occurring in SLs generally decreases with an increase of well&#x2F;barrier thickness. Fitting of X-ray diffraction curves allowed determining the real thicknesses of the GaN wells and AlN barriers. Since blurring of the interfaces causes deviation of calculated data from experimental results the quality of the interfaces has been evaluated as well and compared with results of TEM measurements. For the samples with thinner wells&#x2F;barriers the presence of pin-holes and threading dislocations has been observed in TEM measurements. The best quality of interfaces has been found for the sample with a well&#x2F;barrier thickness of 3 nm. Finally, PL spectra showed that due to Quantum-Confined Stark Effect the PL peak energies of the SLs decreased with increasing the width of the GaN quantum wells and AlN barriers. The effect is well modelled by ab initio calculations based on the density functional theory applied for tetragonally strained structures of the same geometry using a full tensorial representation of the strain in the SLs. </p>
<blockquote>
<p>å·²ç»ç ”ç©¶äº†å…·æœ‰ä¸åŒå‘¨æœŸå’Œç›¸åŒGaNé‡å­é˜±å’ŒAlNåŠ¿å’åšåº¦çš„GaN&#x2F;AlNè¶…æ™¶æ ¼ï¼ˆSLï¼‰ç³»åˆ—ã€‚ä½¿ç”¨Xå°„çº¿è¡å°„ã€å…‰è‡´å‘å…‰ï¼ˆPLï¼‰å’Œé€å°„ç”µå­æ˜¾å¾®é•œï¼ˆTEMï¼‰æŠ€æœ¯ï¼Œç ”ç©¶äº†AlNå’ŒGaNäºšå±‚åšåº¦å¯¹GaN&#x2F;AlN SLç»“æ„ä¸­åº”å˜åˆ†å¸ƒçš„å½±å“ã€‚è¯¦ç»†çš„Xå°„çº¿è¡å°„æµ‹é‡è¡¨æ˜ï¼ŒSLsä¸­çš„åº”å˜é€šå¸¸éšç€é˜±&#x2F;åŠ¿å’åšåº¦çš„å¢åŠ è€Œå‡å°ã€‚é€šè¿‡æ‹ŸåˆXå°„çº¿è¡å°„æ›²çº¿ï¼Œå¯ä»¥ç¡®å®šGaNé˜±å’ŒAlNåŠ¿å’çš„å®é™…åšåº¦ã€‚ç”±äºç•Œé¢æ¨¡ç³Šå¯¼è‡´è®¡ç®—æ•°æ®ä¸å®éªŒç»“æœå­˜åœ¨åå·®ï¼Œå› æ­¤è¿˜å¯¹ç•Œé¢è´¨é‡è¿›è¡Œäº†è¯„ä¼°ï¼Œå¹¶ä¸TEMæµ‹é‡ç»“æœè¿›è¡Œäº†æ¯”è¾ƒã€‚åœ¨è¾ƒè–„é˜±&#x2F;åŠ¿å’çš„æ ·å“ä¸­ï¼Œé€šè¿‡TEMæµ‹é‡è§‚å¯Ÿåˆ°å­˜åœ¨é’ˆå­”å’Œç©¿çº¿é”™ä½ã€‚åœ¨é˜±&#x2F;åŠ¿å’åšåº¦ä¸º3nmçš„æ ·å“ä¸­å‘ç°äº†æœ€ä½³ç•Œé¢è´¨é‡ã€‚æœ€åï¼ŒPLå…‰è°±æ˜¾ç¤ºï¼Œç”±äºé‡å­é™åˆ¶æ–¯å¡”å…‹æ•ˆåº”ï¼ŒSLsçš„PLå³°èƒ½é‡éšç€GaNé‡å­é˜±å’ŒAlNåŠ¿å’å®½åº¦çš„å¢åŠ è€Œé™ä½ã€‚è¯¥æ•ˆåº”é€šè¿‡åŸºäºå¯†åº¦æ³›å‡½ç†è®ºçš„ç¬¬ä¸€æ€§åŸç†è®¡ç®—å¾—åˆ°äº†å¾ˆå¥½çš„æ¨¡æ‹Ÿï¼Œè¯¥ç†è®ºåº”ç”¨äºå…·æœ‰ç›¸åŒå‡ ä½•å½¢çŠ¶çš„å››è¾¹å½¢åº”å˜ç»“æ„ï¼Œå¹¶ä½¿ç”¨SLsä¸­åº”å˜çš„å…¨å¼ é‡è¡¨ç¤ºã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.22294v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>è¯¥æ–‡ç ”ç©¶äº†ä¸åŒå‘¨æœŸçš„GaN&#x2F;AlNè¶…æ™¶æ ¼ï¼ˆSLsï¼‰ç»“æ„ï¼Œå¹¶é‡‡ç”¨äº†Xå°„çº¿è¡å°„ã€å…‰è‡´å‘å…‰å’Œé€å°„ç”µå­æ˜¾å¾®é•œç­‰æŠ€æœ¯ï¼Œæ¢è®¨äº†GaNå’ŒAlNå­å±‚åšåº¦å¯¹GaN&#x2F;AlNè¶…æ™¶æ ¼åº”å˜åˆ†å¸ƒçš„å½±å“ã€‚ç ”ç©¶å‘ç°ï¼Œéšç€é˜±&#x2F;åŠ¿å’åšåº¦çš„å¢åŠ ï¼Œè¶…æ™¶æ ¼ä¸­çš„åº”å˜ä¸€èˆ¬ä¼šå‡å°ã€‚æ­¤å¤–ï¼Œé€šè¿‡Xå°„çº¿è¡å°„æ›²çº¿çš„æ‹Ÿåˆç¡®å®šäº†GaNé˜±å’ŒAlNåŠ¿å’çš„å®é™…åšåº¦ã€‚åŒæ—¶ï¼Œå¯¹ç•Œé¢è´¨é‡è¿›è¡Œäº†è¯„ä¼°å¹¶ä¸é€å°„ç”µå­æ˜¾å¾®é•œçš„æµ‹é‡ç»“æœè¿›è¡Œäº†æ¯”è¾ƒã€‚å¯¹äºè¾ƒè–„çš„é˜±&#x2F;åŠ¿å’æ ·å“ï¼Œé€å°„ç”µå­æ˜¾å¾®é•œè§‚å¯Ÿåˆ°å­˜åœ¨é’ˆå­”å’Œè´¯ç©¿ä½é”™ã€‚åœ¨é˜±&#x2F;åŠ¿å’åšåº¦ä¸º3nmçš„æ ·å“ä¸­è·å¾—äº†æœ€ä½³ç•Œé¢è´¨é‡ã€‚æœ€åï¼Œå…‰è‡´å‘å…‰å…‰è°±è¡¨æ˜ï¼Œç”±äºé‡å­å—é™æ–¯å¡”å…‹æ•ˆåº”ï¼Œéšç€GaNé‡å­é˜±å’ŒAlNåŠ¿å’å®½åº¦çš„å¢åŠ ï¼ŒSLsçš„PLå³°èƒ½é‡é™ä½ã€‚è¿™ä¸€æ•ˆåº”é€šè¿‡åŸºäºå¯†åº¦æ³›å‡½ç†è®ºçš„ä»å¤´ç®—æ³•å¯¹å…·æœ‰ç›¸åŒå‡ ä½•å½¢çŠ¶çš„å››è¾¹å½¢åº”å˜ç»“æ„è¿›è¡Œæ¨¡æ‹Ÿå¾—åˆ°äº†å¾ˆå¥½çš„éªŒè¯ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>GaN&#x2F;AlNè¶…æ™¶æ ¼ï¼ˆSLsï¼‰çš„åº”å˜åˆ†å¸ƒå—åˆ°å­å±‚åšåº¦çš„å½±å“ã€‚</li>
<li>éšç€é˜±&#x2F;åŠ¿å’åšåº¦çš„å¢åŠ ï¼Œè¶…æ™¶æ ¼ä¸­çš„åº”å˜ä¸€èˆ¬ä¼šå‡å°ã€‚</li>
<li>Xå°„çº¿è¡å°„è¢«ç”¨äºç¡®å®šGaNé˜±å’ŒAlNåŠ¿å’çš„å®é™…åšåº¦ã€‚</li>
<li>ç•Œé¢è´¨é‡æ˜¯å½±å“è¶…æ™¶æ ¼æ€§èƒ½çš„é‡è¦å› ç´ ï¼Œå·²å¯¹å…¶è¿›è¡Œäº†è¯„ä¼°å’Œæ¯”è¾ƒã€‚</li>
<li>è¾ƒè–„çš„é˜±&#x2F;åŠ¿å’æ ·å“ä¸­è§‚å¯Ÿåˆ°é’ˆå­”å’Œè´¯ç©¿ä½é”™ã€‚</li>
<li>æœ€ä½³ç•Œé¢è´¨é‡æ˜¯åœ¨é˜±&#x2F;åŠ¿å’åšåº¦ä¸º3nmçš„æ ·å“ä¸­å‘ç°çš„ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.22294">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-17d727948c34a80e619067de4b3d016b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b8583095db588e171c5422232ba83dc1.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="ABC-GS-Alignment-Based-Controllable-Style-Transfer-for-3D-Gaussian-Splatting"><a href="#ABC-GS-Alignment-Based-Controllable-Style-Transfer-for-3D-Gaussian-Splatting" class="headerlink" title="ABC-GS: Alignment-Based Controllable Style Transfer for 3D Gaussian   Splatting"></a>ABC-GS: Alignment-Based Controllable Style Transfer for 3D Gaussian   Splatting</h2><p><strong>Authors:Wenjie Liu, Zhongliang Liu, Xiaoyan Yang, Man Sha, Yang Li</strong></p>
<p>3D scene stylization approaches based on Neural Radiance Fields (NeRF) achieve promising results by optimizing with Nearest Neighbor Feature Matching (NNFM) loss. However, NNFM loss does not consider global style information. In addition, the implicit representation of NeRF limits their fine-grained control over the resulting scenes. In this paper, we introduce ABC-GS, a novel framework based on 3D Gaussian Splatting to achieve high-quality 3D style transfer. To this end, a controllable matching stage is designed to achieve precise alignment between scene content and style features through segmentation masks. Moreover, a style transfer loss function based on feature alignment is proposed to ensure that the outcomes of style transfer accurately reflect the global style of the reference image. Furthermore, the original geometric information of the scene is preserved with the depth loss and Gaussian regularization terms. Extensive experiments show that our ABC-GS provides controllability of style transfer and achieves stylization results that are more faithfully aligned with the global style of the chosen artistic reference. Our homepage is available at <a target="_blank" rel="noopener" href="https://vpx-ecnu.github.io/ABC-GS-website">https://vpx-ecnu.github.io/ABC-GS-website</a>. </p>
<blockquote>
<p>åŸºäºç¥ç»è¾å°„åœºï¼ˆNeRFï¼‰çš„3Dåœºæ™¯é£æ ¼åŒ–æ–¹æ³•é€šè¿‡æœ€è¿‘é‚»ç‰¹å¾åŒ¹é…ï¼ˆNNFMï¼‰æŸå¤±è¿›è¡Œä¼˜åŒ–ï¼Œå–å¾—äº†æœ‰å‰æ™¯çš„ç»“æœã€‚ç„¶è€Œï¼ŒNNFMæŸå¤±å¹¶æ²¡æœ‰è€ƒè™‘å…¨å±€é£æ ¼ä¿¡æ¯ã€‚æ­¤å¤–ï¼ŒNeRFçš„éšå¼è¡¨ç¤ºé™åˆ¶äº†å…¶å¯¹ç»“æœåœºæ™¯çš„ç»†ç²’åº¦æ§åˆ¶ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†ABC-GSï¼Œä¸€ä¸ªåŸºäº3Dé«˜æ–¯å–·æ¶‚çš„æ–°å‹æ¡†æ¶ï¼Œä»¥å®ç°é«˜è´¨é‡çš„ä¸‰ç»´é£æ ¼è½¬ç§»ã€‚ä¸ºæ­¤ï¼Œè®¾è®¡äº†ä¸€ä¸ªå¯æ§çš„åŒ¹é…é˜¶æ®µï¼Œé€šè¿‡åˆ†å‰²æ©è†œå®ç°åœºæ™¯å†…å®¹ä¸é£æ ¼ç‰¹å¾çš„ç²¾ç¡®å¯¹é½ã€‚æ­¤å¤–ï¼Œæå‡ºäº†ä¸€ç§åŸºäºç‰¹å¾å¯¹é½çš„é£æ ¼è½¬ç§»æŸå¤±å‡½æ•°ï¼Œä»¥ç¡®ä¿é£æ ¼è½¬ç§»çš„ç»“æœå‡†ç¡®åæ˜ å‚è€ƒå›¾åƒçš„å…¨å±€é£æ ¼ã€‚æ­¤å¤–ï¼Œé€šè¿‡æ·±åº¦æŸå¤±å’Œé«˜æ–¯æ­£åˆ™åŒ–é¡¹ä¿ç•™äº†åœºæ™¯çš„åŸå‡ ä½•ä¿¡æ¯ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„ABC-GSæä¾›äº†é£æ ¼è½¬ç§»çš„æ§åˆ¶èƒ½åŠ›ï¼Œå¹¶å®ç°äº†ä¸æ‰€é€‰è‰ºæœ¯å‚è€ƒçš„å…¨å±€é£æ ¼æ›´å¿ å®å¯¹é½çš„é£æ ¼åŒ–ç»“æœã€‚æˆ‘ä»¬çš„ä¸»é¡µå¯åœ¨<a target="_blank" rel="noopener" href="https://vpx-ecnu.github.io/ABC-GS-website%E8%AE%BF%E9%97%AE%E3%80%82">https://vpx-ecnu.github.io/ABC-GS-websiteè®¿é—®ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.22218v1">PDF</a> 10 pages, 14 figures</p>
<p><strong>Summary</strong><br>åŸºäºNeRFæŠ€æœ¯çš„ä¸‰ç»´åœºæ™¯é£æ ¼åŒ–æ–¹æ³•å¼•å…¥äº†ä¸€ç§æ–°å‹æ¡†æ¶ABC-GSï¼Œç»“åˆä¸‰ç»´é«˜æ–¯æå›¾å®ç°é«˜è´¨é‡çš„ä¸‰ç»´é£æ ¼è½¬ç§»ã€‚è¯¥æ¡†æ¶è®¾è®¡äº†ä¸€ä¸ªå¯æ§åŒ¹é…é˜¶æ®µï¼Œé€šè¿‡åˆ†å‰²æ©è†œå®ç°åœºæ™¯å†…å®¹ä¸é£æ ¼ç‰¹å¾çš„ç²¾ç¡®å¯¹é½ã€‚åŒæ—¶ï¼Œæå‡ºäº†ä¸€ç§åŸºäºç‰¹å¾å¯¹é½çš„é£æ ¼è½¬ç§»æŸå¤±å‡½æ•°ï¼Œç¡®ä¿é£æ ¼è½¬ç§»ç»“æœå‡†ç¡®åæ˜ å‚è€ƒå›¾åƒçš„æ•´ä½“é£æ ¼ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>åˆ©ç”¨NeRFæŠ€æœ¯çš„ä¸‰ç»´åœºæ™¯é£æ ¼åŒ–æ–¹æ³•å–å¾—æ˜¾è‘—æˆæœã€‚</li>
<li>NNFMæŸå¤±ä¸è€ƒè™‘å…¨å±€é£æ ¼ä¿¡æ¯ï¼Œæ–°å‹æ¡†æ¶ABC-GSè§£å†³æ­¤é—®é¢˜ã€‚</li>
<li>ABC-GSé‡‡ç”¨ä¸‰ç»´é«˜æ–¯æå›¾æŠ€æœ¯å®ç°é«˜è´¨é‡ä¸‰ç»´é£æ ¼è½¬ç§»ã€‚</li>
<li>å¯æ§åŒ¹é…é˜¶æ®µè®¾è®¡ç”¨äºç²¾ç¡®å¯¹é½åœºæ™¯å†…å®¹ä¸é£æ ¼ç‰¹å¾ã€‚</li>
<li>åŸºäºç‰¹å¾å¯¹é½çš„é£æ ¼è½¬ç§»æŸå¤±å‡½æ•°ç¡®ä¿åæ˜ å‚è€ƒå›¾åƒçš„æ•´ä½“é£æ ¼ã€‚</li>
<li>ä¿ç•™åŸå§‹åœºæ™¯å‡ ä½•ä¿¡æ¯ï¼Œé€šè¿‡æ·±åº¦æŸå¤±å’Œé«˜æ–¯æ­£åˆ™åŒ–é¡¹å®ç°ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.22218">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-c150b593acd3ad8af8bbbc67d19774d6.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-3f0a34dcf472a776172956adfe1b8f8f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-283a61d43de86c60c11763eb2b0d954f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b80b6c3426d1ed93e72af69a2318f832.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7f89d45b2378711221b582477857f78d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a9fc0b4d14604ddcbec4769021841d33.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="RainyGS-Efficient-Rain-Synthesis-with-Physically-Based-Gaussian-Splatting"><a href="#RainyGS-Efficient-Rain-Synthesis-with-Physically-Based-Gaussian-Splatting" class="headerlink" title="RainyGS: Efficient Rain Synthesis with Physically-Based Gaussian   Splatting"></a>RainyGS: Efficient Rain Synthesis with Physically-Based Gaussian   Splatting</h2><p><strong>Authors:Qiyu Dai, Xingyu Ni, Qianfan Shen, Wenzheng Chen, Baoquan Chen, Mengyu Chu</strong></p>
<p>We consider the problem of adding dynamic rain effects to in-the-wild scenes in a physically-correct manner. Recent advances in scene modeling have made significant progress, with NeRF and 3DGS techniques emerging as powerful tools for reconstructing complex scenes. However, while effective for novel view synthesis, these methods typically struggle with challenging scene editing tasks, such as physics-based rain simulation. In contrast, traditional physics-based simulations can generate realistic rain effects, such as raindrops and splashes, but they often rely on skilled artists to carefully set up high-fidelity scenes. This process lacks flexibility and scalability, limiting its applicability to broader, open-world environments. In this work, we introduce RainyGS, a novel approach that leverages the strengths of both physics-based modeling and 3DGS to generate photorealistic, dynamic rain effects in open-world scenes with physical accuracy. At the core of our method is the integration of physically-based raindrop and shallow water simulation techniques within the fast 3DGS rendering framework, enabling realistic and efficient simulations of raindrop behavior, splashes, and reflections. Our method supports synthesizing rain effects at over 30 fps, offering users flexible control over rain intensity â€“ from light drizzles to heavy downpours. We demonstrate that RainyGS performs effectively for both real-world outdoor scenes and large-scale driving scenarios, delivering more photorealistic and physically-accurate rain effects compared to state-of-the-art methods. Project page can be found at <a target="_blank" rel="noopener" href="https://pku-vcl-geometry.github.io/RainyGS/">https://pku-vcl-geometry.github.io/RainyGS/</a> </p>
<blockquote>
<p>æˆ‘ä»¬è€ƒè™‘ä»¥ç‰©ç†æ­£ç¡®çš„æ–¹å¼ç»™è‡ªç„¶åœºæ™¯æ·»åŠ åŠ¨æ€é›¨æ°´æ•ˆæœçš„é—®é¢˜ã€‚æœ€è¿‘åœºæ™¯å»ºæ¨¡æ–¹é¢çš„è¿›å±•å·²ç»å–å¾—äº†é‡å¤§çªç ´ï¼ŒNeRFå’Œ3DGSæŠ€æœ¯ä½œä¸ºé‡å»ºå¤æ‚åœºæ™¯çš„å¼ºå¤§å·¥å…·è€Œå´­éœ²å¤´è§’ã€‚ç„¶è€Œï¼Œè™½ç„¶è¿™äº›æ–¹æ³•åœ¨åˆæˆæ–°è§†è§’æ–¹é¢å¾ˆæœ‰æ•ˆï¼Œä½†å®ƒä»¬é€šå¸¸é¢ä¸´å…·æœ‰æŒ‘æˆ˜æ€§çš„åœºæ™¯ç¼–è¾‘ä»»åŠ¡ï¼Œå¦‚åŸºäºç‰©ç†çš„é›¨æ°´æ¨¡æ‹Ÿã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼Œä¼ ç»Ÿçš„åŸºäºç‰©ç†çš„æ¨¡æ‹Ÿå¯ä»¥äº§ç”Ÿé€¼çœŸçš„é›¨æ°´æ•ˆæœï¼Œå¦‚é›¨æ»´å’Œé£æº…ï¼Œä½†å®ƒä»¬é€šå¸¸ä¾èµ–äºç†Ÿç»ƒçš„è‰ºæœ¯å®¶ä»”ç»†è®¾ç½®é«˜ä¿çœŸåœºæ™¯ã€‚è¿™ä¸ªè¿‡ç¨‹ç¼ºä¹çµæ´»æ€§å’Œå¯æ‰©å±•æ€§ï¼Œé™åˆ¶äº†å…¶åœ¨æ›´å¹¿æ³›ã€å¼€æ”¾ä¸–ç•Œç¯å¢ƒä¸­çš„é€‚ç”¨æ€§ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†RainyGSï¼Œè¿™æ˜¯ä¸€ç§æ–°çš„æ–¹æ³•ï¼Œå®ƒç»“åˆäº†åŸºäºç‰©ç†çš„å»ºæ¨¡å’Œ3DGSçš„ä¼˜åŠ¿ï¼Œåœ¨å¼€æ”¾ä¸–ç•Œåœºæ™¯ä¸­ç”Ÿæˆå…·æœ‰ç‰©ç†ç²¾åº¦çš„é€¼çœŸåŠ¨æ€é›¨æ°´æ•ˆæœã€‚æˆ‘ä»¬æ–¹æ³•çš„æ ¸å¿ƒæ˜¯åœ¨å¿«é€Ÿçš„3DGSæ¸²æŸ“æ¡†æ¶å†…æ•´åˆåŸºäºç‰©ç†çš„é›¨æ»´å’Œæµ…æ°´æ¨¡æ‹ŸæŠ€æœ¯ï¼Œèƒ½å¤Ÿé€¼çœŸã€é«˜æ•ˆåœ°æ¨¡æ‹Ÿé›¨æ»´è¡Œä¸ºã€é£æº…å’Œåå°„ã€‚æˆ‘ä»¬çš„æ–¹æ³•æ”¯æŒä»¥è¶…è¿‡30å¸§&#x2F;ç§’çš„é€Ÿåº¦åˆæˆé›¨æ°´æ•ˆæœï¼Œä¸ºç”¨æˆ·æä¾›çµæ´»çš„é›¨æ°´å¼ºåº¦æ§åˆ¶â€”â€”ä»è½»å¾®çš„å°é›¨åˆ°å€¾ç›†å¤§é›¨ã€‚æˆ‘ä»¬è¯æ˜RainyGSåœ¨çœŸå®æˆ·å¤–åœºæ™¯å’Œå¤§è§„æ¨¡é©¾é©¶åœºæ™¯ä¸­éƒ½èƒ½æœ‰æ•ˆè¡¨ç°ï¼Œä¸æœ€æ–°æ–¹æ³•ç›¸æ¯”ï¼Œå®ƒæä¾›äº†æ›´é€¼çœŸå’Œæ›´ç‰©ç†å‡†ç¡®çš„é›¨æ°´æ•ˆæœã€‚é¡¹ç›®é¡µé¢å¯åœ¨<a target="_blank" rel="noopener" href="https://pku-vcl-geometry.github.io/RainyGS/%E6%89%BE%E5%88%B0%E3%80%82">https://pku-vcl-geometry.github.io/RainyGS/æ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.21442v2">PDF</a> CVPR 2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºä¸€ç§åä¸ºRainyGSçš„æ–°æ–¹æ³•ï¼Œç»“åˆäº†ç‰©ç†å»ºæ¨¡å’Œ3DGSæŠ€æœ¯ï¼Œå¯åœ¨å¼€æ”¾ä¸–ç•Œåœºæ™¯ä¸­ç”Ÿæˆå…·æœ‰ç‰©ç†å‡†ç¡®æ€§çš„é«˜é€¼çœŸçš„åŠ¨æ€é›¨æ°´æ•ˆæœã€‚è¯¥æ–¹æ³•èåˆäº†åŸºäºç‰©ç†çš„é›¨æ»´å’Œæµ…æ°´æ¨¡æ‹ŸæŠ€æœ¯ï¼Œåœ¨å¿«é€Ÿçš„3DGSæ¸²æŸ“æ¡†æ¶å†…è¿›è¡Œï¼Œèƒ½æ¨¡æ‹Ÿé›¨æ»´è¡Œä¸ºã€æº…æ°´å’Œåå°„ç­‰ã€‚RainyGSæ”¯æŒä»¥è¶…è¿‡30å¸§çš„é€Ÿåº¦åˆæˆé›¨æ°´æ•ˆæœï¼Œå¹¶å…è®¸ç”¨æˆ·çµæ´»æ§åˆ¶é›¨æ°´å¼ºåº¦ã€‚ç›¸è¾ƒäºç°æœ‰æ–¹æ³•ï¼ŒRainyGSåœ¨çœŸå®æˆ·å¤–åœºæ™¯å’Œå¤§è§„æ¨¡é©¾é©¶åœºæ™¯ä¸­æœ‰æ›´é€¼çœŸå’Œç‰©ç†å‡†ç¡®çš„é›¨æ°´æ•ˆæœã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æœ¬æ–‡ä»‹ç»äº†åœ¨é‡å¤–åœºæ™¯ä¸­æ·»åŠ åŠ¨æ€é›¨æ°´æ•ˆæœçš„éš¾é¢˜ï¼Œå¹¶æŒ‡å‡ºä¼ ç»Ÿæ–¹æ³•ä¾èµ–äºç†Ÿç»ƒè‰ºæœ¯å®¶è¿›è¡Œé«˜ä¿çœŸåœºæ™¯çš„ç»†è‡´è®¾ç½®ï¼Œç¼ºä¹çµæ´»æ€§å’Œå¯æ‰©å±•æ€§ã€‚</li>
<li>æå‡ºäº†åä¸ºRainyGSçš„æ–°æ–¹æ³•ï¼Œç»“åˆäº†ç‰©ç†å»ºæ¨¡å’Œ3DGSæŠ€æœ¯æ¥ç”Ÿæˆé€¼çœŸçš„åŠ¨æ€é›¨æ°´æ•ˆæœã€‚</li>
<li>RainyGSé›†æˆäº†åŸºäºç‰©ç†çš„é›¨æ»´å’Œæµ…æ°´æ¨¡æ‹ŸæŠ€æœ¯ï¼Œåœ¨å¿«é€Ÿ3DGSæ¸²æŸ“æ¡†æ¶å†…è¿›è¡Œï¼Œå®ç°çœŸå®ä¸”é«˜æ•ˆçš„é›¨æ°´æ¨¡æ‹Ÿã€‚</li>
<li>RainyGSæ”¯æŒé«˜å¸§ç‡ï¼ˆè¶…è¿‡30fpsï¼‰çš„é›¨æ°´æ•ˆæœåˆæˆï¼Œç”¨æˆ·å¯çµæ´»æ§åˆ¶é›¨æ°´å¼ºåº¦ã€‚</li>
<li>RainyGSåœ¨çœŸå®æˆ·å¤–åœºæ™¯å’Œå¤§è§„æ¨¡é©¾é©¶åœºæ™¯ä¸­è¡¨ç°å‡ºæ›´é€¼çœŸå’Œç‰©ç†å‡†ç¡®çš„é›¨æ°´æ•ˆæœã€‚</li>
<li>RainyGSæ–¹æ³•é€‚ç”¨äºå¼€æ”¾ä¸–ç•Œç¯å¢ƒï¼Œå…·æœ‰å¹¿æ³›çš„åº”ç”¨æ½œåŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.21442">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-0c3b19f41ae58db25446cd92dae747c1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d92736acb1d72a0482a1e4739ae0fadb.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2540fe3d87f579b141ef35602ce18620.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="LandMarkSystem-Technical-Report"><a href="#LandMarkSystem-Technical-Report" class="headerlink" title="LandMarkSystem Technical Report"></a>LandMarkSystem Technical Report</h2><p><strong>Authors:Zhenxiang Ma, Zhenyu Yang, Miao Tao, Yuanzhen Zhou, Zeyu He, Yuchang Zhang, Rong Fu, Hengjie Li</strong></p>
<p>3D reconstruction is vital for applications in autonomous driving, virtual reality, augmented reality, and the metaverse. Recent advancements such as Neural Radiance Fields(NeRF) and 3D Gaussian Splatting (3DGS) have transformed the field, yet traditional deep learning frameworks struggle to meet the increasing demands for scene quality and scale. This paper introduces LandMarkSystem, a novel computing framework designed to enhance multi-scale scene reconstruction and rendering. By leveraging a componentized model adaptation layer, LandMarkSystem supports various NeRF and 3DGS structures while optimizing computational efficiency through distributed parallel computing and model parameter offloading. Our system addresses the limitations of existing frameworks, providing dedicated operators for complex 3D sparse computations, thus facilitating efficient training and rapid inference over extensive scenes. Key contributions include a modular architecture, a dynamic loading strategy for limited resources, and proven capabilities across multiple representative algorithms.This comprehensive solution aims to advance the efficiency and effectiveness of 3D reconstruction tasks.To facilitate further research and collaboration, the source code and documentation for the LandMarkSystem project are publicly available in an open-source repository, accessing the repository at: <a target="_blank" rel="noopener" href="https://github.com/InternLandMark/LandMarkSystem">https://github.com/InternLandMark/LandMarkSystem</a>. </p>
<blockquote>
<p>ä¸‰ç»´é‡å»ºåœ¨è‡ªåŠ¨é©¾é©¶ã€è™šæ‹Ÿç°å®ã€å¢å¼ºç°å®å’Œå…ƒå®‡å®™ç­‰åº”ç”¨ä¸­å…·æœ‰é‡è¦æ„ä¹‰ã€‚æœ€è¿‘çš„è¿›å±•ï¼Œå¦‚ç¥ç»ç½‘ç»œè¾å°„åœºï¼ˆNeRFï¼‰å’Œä¸‰ç»´é«˜æ–¯å–·æ¶‚ï¼ˆ3DGSï¼‰ï¼Œå·²ç»æ”¹å˜äº†è¿™ä¸€é¢†åŸŸï¼Œä½†ä¼ ç»Ÿçš„æ·±åº¦å­¦ä¹ æ¡†æ¶éš¾ä»¥æ»¡è¶³å¯¹åœºæ™¯è´¨é‡å’Œè§„æ¨¡æ—¥ç›Šå¢é•¿çš„éœ€æ±‚ã€‚æœ¬æ–‡ä»‹ç»äº†LandMarkSystemï¼Œè¿™æ˜¯ä¸€ç§æ–°å‹è®¡ç®—æ¡†æ¶ï¼Œæ—¨åœ¨æé«˜å¤šå°ºåº¦åœºæ™¯é‡å»ºå’Œæ¸²æŸ“ã€‚é€šè¿‡åˆ©ç”¨ç»„ä»¶åŒ–æ¨¡å‹é€‚é…å±‚ï¼ŒLandMarkSystemæ”¯æŒå„ç§NeRFå’Œ3DGSç»“æ„ï¼ŒåŒæ—¶é€šè¿‡åˆ†å¸ƒå¼å¹¶è¡Œè®¡ç®—å’Œæ¨¡å‹å‚æ•°å¸è½½ä¼˜åŒ–è®¡ç®—æ•ˆç‡ã€‚æˆ‘ä»¬çš„ç³»ç»Ÿè§£å†³äº†ç°æœ‰æ¡†æ¶çš„å±€é™æ€§ï¼Œä¸ºå¤æ‚çš„ä¸‰ç»´ç¨€ç–è®¡ç®—æä¾›äº†ä¸“ç”¨æ“ä½œç¬¦ï¼Œä»è€Œå®ç°äº†å¤§è§„æ¨¡åœºæ™¯çš„é«˜æ•ˆè®­ç»ƒå’Œå¿«é€Ÿæ¨ç†ã€‚ä¸»è¦è´¡çŒ®åŒ…æ‹¬æ¨¡å—åŒ–æ¶æ„ã€æœ‰é™èµ„æºçš„åŠ¨æ€åŠ è½½ç­–ç•¥ä»¥åŠåœ¨å¤šä¸ªä»£è¡¨æ€§ç®—æ³•ä¸­çš„è¯æ˜èƒ½åŠ›ã€‚è¿™ä¸ªå…¨é¢çš„è§£å†³æ–¹æ¡ˆæ—¨åœ¨æé«˜ä¸‰ç»´é‡å»ºä»»åŠ¡çš„æ•ˆç‡å’Œæ•ˆæœã€‚ä¸ºä¿ƒè¿›è¿›ä¸€æ­¥çš„ç ”ç©¶å’Œåˆä½œï¼ŒLandMarkSystemé¡¹ç›®çš„æºä»£ç å’Œæ–‡æ¡£å·²åœ¨å¼€æºä»“åº“ä¸­æä¾›ï¼Œå¯é€šè¿‡ä»¥ä¸‹é“¾æ¥è®¿é—®ï¼š<a target="_blank" rel="noopener" href="https://github.com/InternLandMark/LandMarkSystem%E3%80%82">https://github.com/InternLandMark/LandMarkSystemã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.21364v2">PDF</a> </p>
<p><strong>Summary</strong><br>åŸºäºNeRFæŠ€æœ¯å’Œç»„ä»¶åŒ–æ¨¡å‹é€‚é…å±‚ï¼ŒLandMarkSystemæ¡†æ¶æ—¨åœ¨æé«˜å¤šå°ºåº¦åœºæ™¯é‡å»ºå’Œæ¸²æŸ“çš„æ•ˆç‡å’Œè´¨é‡ã€‚é€šè¿‡åˆ†å¸ƒå¼å¹¶è¡Œè®¡ç®—å’Œæ¨¡å‹å‚æ•°å¸è½½ä¼˜åŒ–è®¡ç®—æ•ˆç‡ï¼Œè§£å†³äº†ç°æœ‰æ¡†æ¶çš„é™åˆ¶ï¼Œä¸ºå¤æ‚çš„ä¸‰ç»´ç¨€ç–è®¡ç®—æä¾›ä¸“ç”¨æ“ä½œç¬¦ï¼Œæ—¨åœ¨æ¨è¿›ä¸‰ç»´é‡å»ºä»»åŠ¡çš„æ•ˆç‡å’Œæ•ˆæœã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LandMarkSystemåˆ©ç”¨NeRFæŠ€æœ¯å’Œç»„ä»¶åŒ–æ¨¡å‹é€‚é…å±‚æ¥å¢å¼ºå¤šå°ºåº¦åœºæ™¯é‡å»ºã€‚</li>
<li>è¯¥æ¡†æ¶é€šè¿‡åˆ†å¸ƒå¼å¹¶è¡Œè®¡ç®—å’Œæ¨¡å‹å‚æ•°å¸è½½ä¼˜åŒ–è®¡ç®—æ•ˆç‡ã€‚</li>
<li>LandMarkSystemè§£å†³äº†ç°æœ‰æ¡†æ¶åœ¨å¤„ç†å¤æ‚ä¸‰ç»´ç¨€ç–è®¡ç®—æ–¹é¢çš„é™åˆ¶ã€‚</li>
<li>å®ƒæä¾›äº†ä¸€ä¸ªæ¨¡å—åŒ–æ¶æ„å’ŒåŠ¨æ€åŠ è½½ç­–ç•¥ä»¥åº”å¯¹æœ‰é™çš„èµ„æºã€‚</li>
<li>LandMarkSystemå·²åœ¨å¤šä¸ªä»£è¡¨æ€§ç®—æ³•ä¸­è¯æ˜äº†å…¶èƒ½åŠ›ã€‚</li>
<li>è¯¥æ¡†æ¶æ—¨åœ¨æé«˜ä¸‰ç»´é‡å»ºä»»åŠ¡çš„æ•ˆç‡å’Œæ•ˆæœã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.21364">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-6cab89666ecae86575af634424164eee.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="UGNA-VPR-A-Novel-Training-Paradigm-for-Visual-Place-Recognition-Based-on-Uncertainty-Guided-NeRF-Augmentation"><a href="#UGNA-VPR-A-Novel-Training-Paradigm-for-Visual-Place-Recognition-Based-on-Uncertainty-Guided-NeRF-Augmentation" class="headerlink" title="UGNA-VPR: A Novel Training Paradigm for Visual Place Recognition Based   on Uncertainty-Guided NeRF Augmentation"></a>UGNA-VPR: A Novel Training Paradigm for Visual Place Recognition Based   on Uncertainty-Guided NeRF Augmentation</h2><p><strong>Authors:Yehui Shen, Lei Zhang, Qingqiu Li, Xiongwei Zhao, Yue Wang, Huimin Lu, Xieyuanli Chen</strong></p>
<p>Visual place recognition (VPR) is crucial for robots to identify previously visited locations, playing an important role in autonomous navigation in both indoor and outdoor environments. However, most existing VPR datasets are limited to single-viewpoint scenarios, leading to reduced recognition accuracy, particularly in multi-directional driving or feature-sparse scenes. Moreover, obtaining additional data to mitigate these limitations is often expensive. This paper introduces a novel training paradigm to improve the performance of existing VPR networks by enhancing multi-view diversity within current datasets through uncertainty estimation and NeRF-based data augmentation. Specifically, we initially train NeRF using the existing VPR dataset. Then, our devised self-supervised uncertainty estimation network identifies places with high uncertainty. The poses of these uncertain places are input into NeRF to generate new synthetic observations for further training of VPR networks. Additionally, we propose an improved storage method for efficient organization of augmented and original training data. We conducted extensive experiments on three datasets and tested three different VPR backbone networks. The results demonstrate that our proposed training paradigm significantly improves VPR performance by fully utilizing existing data, outperforming other training approaches. We further validated the effectiveness of our approach on self-recorded indoor and outdoor datasets, consistently demonstrating superior results. Our dataset and code have been released at \href{<a target="_blank" rel="noopener" href="https://github.com/nubot-nudt/UGNA-VPR%7D%7Bhttps://github.com/nubot-nudt/UGNA-VPR%7D">https://github.com/nubot-nudt/UGNA-VPR}{https://github.com/nubot-nudt/UGNA-VPR}</a>. </p>
<blockquote>
<p>è§†è§‰ä½ç½®è¯†åˆ«ï¼ˆVPRï¼‰å¯¹äºæœºå™¨äººè¯†åˆ«å…ˆå‰è®¿é—®è¿‡çš„ä½ç½®è‡³å…³é‡è¦ï¼Œåœ¨å®¤å†…å’Œå®¤å¤–ç¯å¢ƒçš„è‡ªä¸»å¯¼èˆªä¸­æ‰®æ¼”ç€é‡è¦è§’è‰²ã€‚ç„¶è€Œï¼Œå¤§å¤šæ•°ç°æœ‰çš„VPRæ•°æ®é›†ä»…é™äºå•è§†ç‚¹åœºæ™¯ï¼Œå¯¼è‡´è¯†åˆ«ç²¾åº¦é™ä½ï¼Œç‰¹åˆ«æ˜¯åœ¨å¤šæ–¹å‘é©¾é©¶æˆ–ç‰¹å¾ç¨€ç–çš„åœºæ™¯ä¸­ã€‚æ­¤å¤–ï¼Œè·å–é¢å¤–æ•°æ®ä»¥ç¼“è§£è¿™äº›é™åˆ¶é€šå¸¸æˆæœ¬é«˜æ˜‚ã€‚æœ¬æ–‡å¼•å…¥äº†ä¸€ç§æ–°çš„è®­ç»ƒèŒƒå¼ï¼Œä»¥æé«˜ç°æœ‰VPRç½‘ç»œçš„æ€§èƒ½ã€‚æˆ‘ä»¬é€šè¿‡ä¸ç¡®å®šæ€§ä¼°è®¡å’ŒåŸºäºNeRFçš„æ•°æ®å¢å¼ºï¼Œå¢å¼ºå½“å‰æ•°æ®é›†å†…çš„å¤šè§†å›¾å¤šæ ·æ€§ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬æœ€åˆä½¿ç”¨ç°æœ‰çš„VPRæ•°æ®é›†è®­ç»ƒNeRFã€‚ç„¶åï¼Œæˆ‘ä»¬è®¾è®¡çš„è‡ªç›‘ç£ä¸ç¡®å®šæ€§ä¼°è®¡ç½‘ç»œè¯†åˆ«å‡ºä¸ç¡®å®šæ€§è¾ƒé«˜çš„ä½ç½®ã€‚è¿™äº›ä¸ç¡®å®šä½ç½®çš„å§¿æ€è¢«è¾“å…¥åˆ°NeRFä¸­ï¼Œä»¥ç”Ÿæˆæ–°çš„åˆæˆè§‚å¯Ÿç»“æœï¼Œç”¨äºè¿›ä¸€æ­¥è®­ç»ƒVPRç½‘ç»œã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜æå‡ºäº†ä¸€ç§æ”¹è¿›çš„æ•°æ®å­˜å‚¨æ–¹æ³•ï¼Œä»¥æœ‰æ•ˆåœ°ç»„ç»‡å¢å¼ºå’ŒåŸå§‹è®­ç»ƒæ•°æ®ã€‚æˆ‘ä»¬åœ¨ä¸‰ä¸ªæ•°æ®é›†ä¸Šè¿›è¡Œäº†å¹¿æ³›çš„å®éªŒï¼Œå¹¶æµ‹è¯•äº†ä¸‰ç§ä¸åŒçš„VPRéª¨å¹²ç½‘ç»œã€‚ç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬æå‡ºçš„è®­ç»ƒèŒƒå¼å……åˆ†åˆ©ç”¨äº†ç°æœ‰æ•°æ®ï¼Œæ˜¾è‘—æé«˜äº†VPRçš„æ€§èƒ½ï¼Œè¶…è¶Šäº†å…¶ä»–è®­ç»ƒæ–¹æ³•ã€‚æˆ‘ä»¬åœ¨è‡ªåˆ¶çš„å®¤å†…å’Œå®¤å¤–æ•°æ®é›†ä¸Šè¿›ä¸€æ­¥éªŒè¯äº†æˆ‘ä»¬çš„æ–¹æ³•çš„æœ‰æ•ˆæ€§ï¼Œå§‹ç»ˆè¡¨ç°å‡ºå“è¶Šçš„ç»“æœã€‚æˆ‘ä»¬çš„æ•°æ®é›†å’Œä»£ç å·²å‘å¸ƒåœ¨<a target="_blank" rel="noopener" href="https://github.com/nubot-nudt/UGNA-VPR">https://github.com/nubot-nudt/UGNA-VPR</a>ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.21338v1">PDF</a> Accepted to IEEE Robotics and Automation Letters (RA-L)</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„è®­ç»ƒèŒƒå¼ï¼Œé€šè¿‡åˆ©ç”¨ä¸ç¡®å®šåº¦ä¼°è®¡å’ŒåŸºäºNeRFçš„æ•°æ®å¢å¼ºæŠ€æœ¯ï¼Œå¢å¼ºç°æœ‰VPRç½‘ç»œçš„å¤šè§†è§’å¤šæ ·æ€§ï¼Œæé«˜å…¶åœ¨å¤šæ–¹å‘é©¾é©¶å’Œç‰¹å¾ç¨€ç–åœºæ™¯ä¸­çš„è¯†åˆ«å‡†ç¡®ç‡ã€‚é€šè¿‡NeRFä½¿ç”¨ç°æœ‰VPRæ•°æ®é›†è¿›è¡Œåˆå§‹è®­ç»ƒï¼Œç„¶ååˆ©ç”¨è‡ªä¸»è®¾è®¡çš„è‡ªç›‘ç£ä¸ç¡®å®šåº¦ä¼°è®¡ç½‘ç»œè¯†åˆ«é«˜ä¸ç¡®å®šåº¦åœ°ç‚¹ï¼Œå°†è¿™äº›åœ°ç‚¹çš„å§¿æ€è¾“å…¥NeRFç”Ÿæˆæ–°çš„åˆæˆè§‚å¯Ÿç»“æœï¼Œç”¨äºè¿›ä¸€æ­¥è®­ç»ƒVPRç½‘ç»œã€‚åŒæ—¶ï¼Œæå‡ºäº†ä¸€ç§æ”¹è¿›çš„æ•°æ®å­˜å‚¨æ–¹æ³•ï¼Œä»¥æ›´æœ‰æ•ˆåœ°ç»„ç»‡å¢å¼ºå’ŒåŸå§‹è®­ç»ƒæ•°æ®ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥è®­ç»ƒèŒƒå¼èƒ½å……åˆ†åˆ©ç”¨ç°æœ‰æ•°æ®ï¼Œæ˜¾è‘—æé«˜VPRæ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¼•å…¥äº†ä¸€ç§æ–°çš„è®­ç»ƒèŒƒå¼ï¼Œæ—¨åœ¨æé«˜VPRç½‘ç»œåœ¨å¤šç§åœºæ™¯ä¸­çš„è¯†åˆ«æ€§èƒ½ã€‚</li>
<li>åˆ©ç”¨ä¸ç¡®å®šåº¦ä¼°è®¡æŠ€æœ¯è¯†åˆ«é«˜ä¸ç¡®å®šåº¦åœ°ç‚¹ï¼Œé€šè¿‡NeRFç”Ÿæˆæ–°çš„åˆæˆè§‚å¯Ÿç»“æœã€‚</li>
<li>æå‡ºäº†ä¸€ä¸ªè‡ªç›‘ç£ä¸ç¡®å®šåº¦ä¼°è®¡ç½‘ç»œï¼Œç”¨äºè¾…åŠ©ç”Ÿæˆåˆæˆè§‚å¯Ÿæ•°æ®ã€‚</li>
<li>é‡‡ç”¨äº†ä¸€ç§æ”¹è¿›çš„æ•°æ®å­˜å‚¨æ–¹æ³•ï¼Œä»¥ä¾¿æ›´æœ‰æ•ˆåœ°ç»„ç»‡å¢å¼ºå’ŒåŸå§‹è®­ç»ƒæ•°æ®ã€‚</li>
<li>é€šè¿‡ä¸‰ä¸ªæ•°æ®é›†å’Œä¸‰ç§ä¸åŒçš„VPRéª¨å¹²ç½‘ç»œè¿›è¡Œçš„å®éªŒè¡¨æ˜ï¼Œæ–°è®­ç»ƒèŒƒå¼æ˜¾è‘—æé«˜äº†VPRæ€§èƒ½ã€‚</li>
<li>åœ¨è‡ªä¸»å½•åˆ¶çš„å®¤å†…å’Œå®¤å¤–æ•°æ®é›†ä¸ŠéªŒè¯äº†æ–¹æ³•çš„æœ‰æ•ˆæ€§ï¼Œå‡å–å¾—äº†ä¼˜å¼‚ç»“æœã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.21338">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-2d64fae156815e3a3aeec368229be729.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d535b9c8da77d7048c470fa9913f5b84.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-45e66ba7990fba5569f0f1451df6dfa0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-bad45da56d9377a5101018b25da7ffb3.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="NeRFPrior-Learning-Neural-Radiance-Field-as-a-Prior-for-Indoor-Scene-Reconstruction"><a href="#NeRFPrior-Learning-Neural-Radiance-Field-as-a-Prior-for-Indoor-Scene-Reconstruction" class="headerlink" title="NeRFPrior: Learning Neural Radiance Field as a Prior for Indoor Scene   Reconstruction"></a>NeRFPrior: Learning Neural Radiance Field as a Prior for Indoor Scene   Reconstruction</h2><p><strong>Authors:Wenyuan Zhang, Emily Yue-ting Jia, Junsheng Zhou, Baorui Ma, Kanle Shi, Yu-Shen Liu, Zhizhong Han</strong></p>
<p>Recently, it has shown that priors are vital for neural implicit functions to reconstruct high-quality surfaces from multi-view RGB images. However, current priors require large-scale pre-training, and merely provide geometric clues without considering the importance of color. In this paper, we present NeRFPrior, which adopts a neural radiance field as a prior to learn signed distance fields using volume rendering for surface reconstruction. Our NeRF prior can provide both geometric and color clues, and also get trained fast under the same scene without additional data. Based on the NeRF prior, we are enabled to learn a signed distance function (SDF) by explicitly imposing a multi-view consistency constraint on each ray intersection for surface inference. Specifically, at each ray intersection, we use the density in the prior as a coarse geometry estimation, while using the color near the surface as a clue to check its visibility from another view angle. For the textureless areas where the multi-view consistency constraint does not work well, we further introduce a depth consistency loss with confidence weights to infer the SDF. Our experimental results outperform the state-of-the-art methods under the widely used benchmarks. </p>
<blockquote>
<p>æœ€è¿‘ï¼Œç ”ç©¶æ˜¾ç¤ºå…ˆéªŒçŸ¥è¯†å¯¹äºç¥ç»éšå¼å‡½æ•°ä»å¤šè§†è§’RGBå›¾åƒé‡å»ºé«˜è´¨é‡è¡¨é¢è‡³å…³é‡è¦ã€‚ç„¶è€Œï¼Œå½“å‰å…ˆéªŒçŸ¥è¯†éœ€è¦å¤§é‡é¢„è®­ç»ƒï¼Œå¹¶ä¸”ä»…æä¾›å‡ ä½•çº¿ç´¢ï¼Œè€Œæœªè€ƒè™‘é¢œè‰²çš„é‡è¦æ€§ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†NeRFPriorï¼Œå®ƒé‡‡ç”¨ç¥ç»è¾å°„åœºä½œä¸ºå…ˆéªŒçŸ¥è¯†ï¼Œä½¿ç”¨ä½“ç§¯æ¸²æŸ“æ¥å­¦ä¹ ç¬¦å·è·ç¦»åœºè¿›è¡Œè¡¨é¢é‡å»ºã€‚æˆ‘ä»¬çš„NeRFå…ˆéªŒå¯ä»¥æä¾›å‡ ä½•å’Œé¢œè‰²çº¿ç´¢ï¼Œå¹¶åœ¨åŒä¸€åœºæ™¯ä¸‹æ— éœ€é¢å¤–æ•°æ®è¿›è¡Œå¿«é€Ÿè®­ç»ƒã€‚åŸºäºNeRFå…ˆéªŒï¼Œæˆ‘ä»¬èƒ½å¤Ÿé€šè¿‡åœ¨æ¯æ¡å°„çº¿äº¤ç‚¹ä¸Šæ˜ç¡®æ–½åŠ å¤šè§†è§’ä¸€è‡´æ€§çº¦æŸæ¥å­¦ä¹ ç¬¦å·è·ç¦»å‡½æ•°ï¼ˆSDFï¼‰ï¼Œä»¥å®ç°è¡¨é¢æ¨æ–­ã€‚å…·ä½“æ¥è¯´ï¼Œåœ¨æ¯æ¡å°„çº¿äº¤ç‚¹ä¸Šï¼Œæˆ‘ä»¬ä½¿ç”¨å…ˆéªŒä¸­çš„å¯†åº¦ä½œä¸ºç²—ç•¥çš„å‡ ä½•ä¼°è®¡ï¼ŒåŒæ—¶ä½¿ç”¨æ¥è¿‘è¡¨é¢çš„é¢œè‰²ä½œä¸ºä»å¦ä¸€ä¸ªè§†è§’æ£€æŸ¥å…¶å¯è§æ€§çš„çº¿ç´¢ã€‚å¯¹äºçº¹ç†ç¼ºå¤±çš„åŒºåŸŸï¼Œå¤šè§†è§’ä¸€è‡´æ€§çº¦æŸæ— æ³•å¾ˆå¥½åœ°å·¥ä½œï¼Œæˆ‘ä»¬è¿›ä¸€æ­¥å¼•å…¥å¸¦æœ‰ç½®ä¿¡æƒé‡çš„æ·±åº¦ä¸€è‡´æ€§æŸå¤±æ¥æ¨æ–­SDFã€‚æˆ‘ä»¬çš„å®éªŒç»“æœä¼˜äºå¹¿æ³›ä½¿ç”¨çš„åŸºå‡†æµ‹è¯•ä¸­çš„æœ€æ–°æ–¹æ³•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.18361v2">PDF</a> Accepted by CVPR 2025. Project page:   <a target="_blank" rel="noopener" href="https://wen-yuan-zhang.github.io/NeRFPrior/">https://wen-yuan-zhang.github.io/NeRFPrior/</a></p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†NeRFPrioræ–¹æ³•ï¼Œåˆ©ç”¨ç¥ç»è¾å°„åœºä½œä¸ºå…ˆéªŒæ¥å­¦ä¹ ä½¿ç”¨ä½“ç§¯æ¸²æŸ“çš„è¡¨é¢é‡å»ºä¸­çš„ç¬¦å·è·ç¦»åœºã€‚NeRFPriorä¸ä»…æä¾›å‡ ä½•çº¿ç´¢ï¼Œè¿˜è€ƒè™‘é¢œè‰²é‡è¦æ€§ï¼Œæ— éœ€é¢å¤–æ•°æ®å³å¯åœ¨åŒä¸€åœºæ™¯ä¸‹è¿›è¡Œå¿«é€Ÿè®­ç»ƒã€‚é€šè¿‡æ˜ç¡®å¯¹æ¯æ¡å°„çº¿äº¤ç‚¹æ–½åŠ å¤šè§†è§’ä¸€è‡´æ€§çº¦æŸï¼Œå®ç°è¡¨é¢æ¨æ–­çš„ç¬¦å·è·ç¦»å‡½æ•°ï¼ˆSDFï¼‰å­¦ä¹ ã€‚åœ¨çº¹ç†ç¼ºå¤±åŒºåŸŸï¼Œå¼•å…¥å¸¦æœ‰ç½®ä¿¡æƒé‡çš„æ·±åº¦ä¸€è‡´æ€§æŸå¤±æ¥æ¨æ–­SDFï¼Œå®éªŒç»“æœä¼˜äºå¹¿æ³›ä½¿ç”¨çš„åŸºå‡†æµ‹è¯•ä¸­çš„æœ€æ–°æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>NeRFPrioråˆ©ç”¨ç¥ç»è¾å°„åœºä½œä¸ºå…ˆéªŒï¼Œç”¨äºè¡¨é¢é‡å»ºä¸­çš„ç¬¦å·è·ç¦»åœºå­¦ä¹ ã€‚</li>
<li>è¯¥æ–¹æ³•ä¸ä»…èƒ½æä¾›å‡ ä½•çº¿ç´¢ï¼Œè¿˜è€ƒè™‘é¢œè‰²çš„é‡è¦æ€§ã€‚</li>
<li>NeRFPrioråœ¨åŒä¸€åœºæ™¯ä¸‹è¿›è¡Œå¿«é€Ÿè®­ç»ƒï¼Œæ— éœ€é¢å¤–æ•°æ®ã€‚</li>
<li>é€šè¿‡æ˜ç¡®å¯¹æ¯æ¡å°„çº¿äº¤ç‚¹æ–½åŠ å¤šè§†è§’ä¸€è‡´æ€§çº¦æŸï¼Œå®ç°ç¬¦å·è·ç¦»å‡½æ•°ï¼ˆSDFï¼‰å­¦ä¹ ã€‚</li>
<li>åœ¨çº¹ç†ç¼ºå¤±åŒºåŸŸï¼Œå¼•å…¥æ·±åº¦ä¸€è‡´æ€§æŸå¤±ä»¥è¾…åŠ©è¡¨é¢æ¨æ–­ã€‚</li>
<li>NeRFPriorçš„å®éªŒç»“æœä¼˜äºå¹¿æ³›ä½¿ç”¨çš„åŸºå‡†æµ‹è¯•ä¸­çš„æœ€æ–°æ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.18361">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-dced1b075b30f7831c4352987fd1efeb.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-329c32e673c33ffb395103bf6fad45f9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-529d9fe25e668a950d63460ca52e07fd.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a736b9c84b1f2095dd212bde8fde2f75.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2894203a097369e7df754db7fa9dca19.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-22dcbf7ed51c78afe9d29df43db16aba.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="Scalable-Real2Sim-Physics-Aware-Asset-Generation-Via-Robotic-Pick-and-Place-Setups"><a href="#Scalable-Real2Sim-Physics-Aware-Asset-Generation-Via-Robotic-Pick-and-Place-Setups" class="headerlink" title="Scalable Real2Sim: Physics-Aware Asset Generation Via Robotic   Pick-and-Place Setups"></a>Scalable Real2Sim: Physics-Aware Asset Generation Via Robotic   Pick-and-Place Setups</h2><p><strong>Authors:Nicholas Pfaff, Evelyn Fu, Jeremy Binagia, Phillip Isola, Russ Tedrake</strong></p>
<p>Simulating object dynamics from real-world perception shows great promise for digital twins and robotic manipulation but often demands labor-intensive measurements and expertise. We present a fully automated Real2Sim pipeline that generates simulation-ready assets for real-world objects through robotic interaction. Using only a robotâ€™s joint torque sensors and an external camera, the pipeline identifies visual geometry, collision geometry, and physical properties such as inertial parameters. Our approach introduces a general method for extracting high-quality, object-centric meshes from photometric reconstruction techniques (e.g., NeRF, Gaussian Splatting) by employing alpha-transparent training while explicitly distinguishing foreground occlusions from background subtraction. We validate the full pipeline through extensive experiments, demonstrating its effectiveness across diverse objects. By eliminating the need for manual intervention or environment modifications, our pipeline can be integrated directly into existing pick-and-place setups, enabling scalable and efficient dataset creation. Project page (with code and data): <a target="_blank" rel="noopener" href="https://scalable-real2sim.github.io/">https://scalable-real2sim.github.io/</a>. </p>
<blockquote>
<p>é€šè¿‡ä»çœŸå®ä¸–ç•Œæ„ŸçŸ¥æ¨¡æ‹Ÿç‰©ä½“åŠ¨æ€ï¼Œæ•°å­—å­ªç”Ÿå’Œæœºå™¨äººæ“ä½œå±•ç°å‡ºäº†å·¨å¤§çš„æ½œåŠ›ï¼Œä½†è¿™é€šå¸¸éœ€è¦å¯†é›†åŠ³åŠ¨çš„æµ‹é‡å’Œä¸“ä¸šçŸ¥è¯†ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§å…¨è‡ªåŠ¨çš„Real2Simç®¡é“ï¼Œé€šè¿‡æœºå™¨äººäº¤äº’ä¸ºçœŸå®ä¸–ç•Œå¯¹è±¡ç”Ÿæˆæ¨¡æ‹Ÿå°±ç»ªèµ„äº§ã€‚ä»…ä½¿ç”¨æœºå™¨äººçš„å…³èŠ‚æ‰­çŸ©ä¼ æ„Ÿå™¨å’Œå¤–éƒ¨æ‘„åƒå¤´ï¼Œè¯¥ç®¡é“å°±èƒ½è¯†åˆ«è§†è§‰å‡ ä½•ã€ç¢°æ’å‡ ä½•ä»¥åŠç‰©ç†å±æ€§ï¼Œå¦‚æƒ¯æ€§å‚æ•°ã€‚æˆ‘ä»¬çš„æ–¹æ³•é€šè¿‡é‡‡ç”¨alphaé€æ˜è®­ç»ƒï¼Œå¼•å…¥äº†ä¸€ç§ä»å…‰åº¦é‡å»ºæŠ€æœ¯ï¼ˆä¾‹å¦‚NeRFã€é«˜æ–¯å¹³æ¿å°åˆ·ï¼‰ä¸­æå–é«˜è´¨é‡ã€ä»¥ç‰©ä½“ä¸ºä¸­å¿ƒçš„ç½‘æ ¼çš„ä¸€èˆ¬æ–¹æ³•ï¼ŒåŒæ—¶æ˜ç¡®åŒºåˆ†å‰æ™¯é®æŒ¡å’ŒèƒŒæ™¯å‡æ³•ã€‚æˆ‘ä»¬é€šè¿‡å¹¿æ³›çš„å®éªŒéªŒè¯äº†æ•´ä¸ªç®¡é“çš„æœ‰æ•ˆæ€§ï¼Œè¯æ˜äº†å®ƒåœ¨å„ç§å¯¹è±¡ä¸Šçš„æœ‰æ•ˆæ€§ã€‚é€šè¿‡æ¶ˆé™¤å¯¹äººå·¥å¹²é¢„æˆ–ç¯å¢ƒä¿®æ”¹çš„éœ€æ±‚ï¼Œæˆ‘ä»¬çš„ç®¡é“å¯ä»¥ç›´æ¥é›†æˆåˆ°ç°æœ‰çš„æ‹¾å–å’Œæ”¾ç½®è®¾ç½®ä¸­ï¼Œä»è€Œå®ç°å¯æ‰©å±•å’Œé«˜æ•ˆçš„æ•°æ®åº“åˆ›å»ºã€‚é¡¹ç›®é¡µé¢ï¼ˆåŒ…å«ä»£ç å’Œæ•°æ®ï¼‰ï¼š<a target="_blank" rel="noopener" href="https://scalable-real2sim.github.io/%E3%80%82">https://scalable-real2sim.github.io/ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.00370v2">PDF</a> Website: <a target="_blank" rel="noopener" href="https://scalable-real2sim.github.io/">https://scalable-real2sim.github.io/</a></p>
<p><strong>Summary</strong></p>
<p>æœ¬æ‘˜è¦ä»‹ç»äº†å…¨è‡ªåŠ¨åŒ–çš„Real2Simç®¡é“æŠ€æœ¯ï¼Œè¯¥æŠ€æœ¯é€šè¿‡æœºå™¨äººäº¤äº’ä¸ºçœŸå®ä¸–ç•Œç‰©ä½“ç”Ÿæˆæ¨¡æ‹Ÿä»¿çœŸèµ„äº§ã€‚ä»…ä½¿ç”¨æœºå™¨äººçš„å…³èŠ‚æ‰­çŸ©ä¼ æ„Ÿå™¨å’Œå¤–éƒ¨ç›¸æœºï¼Œè¯¥æŠ€æœ¯å°±èƒ½è¯†åˆ«ç‰©ä½“çš„è§†è§‰å‡ ä½•ã€ç¢°æ’å‡ ä½•å’Œç‰©ç†å±æ€§ï¼Œå¦‚æƒ¯æ€§å‚æ•°ã€‚è¯¥æ–¹æ³•é‡‡ç”¨alphaé€æ˜è®­ç»ƒæ³•ï¼Œç»“åˆNeRFç­‰å…‰åº¦é‡å»ºæŠ€æœ¯ï¼Œæå–é«˜è´¨é‡çš„å¯¹è±¡ä¸­å¿ƒç½‘æ ¼ï¼Œå¹¶é€šè¿‡å‰æ™¯é®æŒ¡ä¸èƒŒæ™¯å‡å½±çš„æ˜ç¡®åŒºåˆ†æ¥å®ç°ã€‚å®éªŒéªŒè¯è¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å¤šç§å¯¹è±¡ä¸Šå‡æœ‰æ•ˆã€‚å®ƒèƒ½ç›´æ¥é›†æˆç°æœ‰çš„æ‹¾å–å’Œæ”¾ç½®è®¾ç½®ï¼Œæ— éœ€äººå·¥å¹²é¢„æˆ–ç¯å¢ƒä¿®æ”¹ï¼Œä»è€Œå®ç°å¯æ‰©å±•å’Œé«˜æ•ˆçš„æ•°æ®åº“åˆ›å»ºã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ä»‹ç»äº†ä¸€ç§å…¨è‡ªåŠ¨åŒ–çš„Real2Simç®¡é“æŠ€æœ¯ï¼Œç”¨äºç”Ÿæˆæ¨¡æ‹Ÿä»¿çœŸèµ„äº§ï¼Œç”¨äºæ•°å­—åŒèƒèƒå’Œæœºå™¨äººæ“ä½œã€‚</li>
<li>ä»…ä½¿ç”¨æœºå™¨äººçš„å…³èŠ‚æ‰­çŸ©ä¼ æ„Ÿå™¨å’Œå¤–éƒ¨ç›¸æœºï¼Œå°±èƒ½è¯†åˆ«ç‰©ä½“çš„è§†è§‰å‡ ä½•ã€ç¢°æ’å‡ ä½•å’Œç‰©ç†å±æ€§ã€‚</li>
<li>é‡‡ç”¨alphaé€æ˜è®­ç»ƒæ³•ç»“åˆNeRFç­‰å…‰åº¦é‡å»ºæŠ€æœ¯ï¼Œå®ç°é«˜è´¨é‡çš„å¯¹è±¡ä¸­å¿ƒç½‘æ ¼æå–ã€‚</li>
<li>æ˜ç¡®åŒºåˆ†å‰æ™¯é®æŒ¡å’ŒèƒŒæ™¯å‡å½±ï¼Œæé«˜äº†æŠ€æœ¯çš„æ•ˆèƒ½ã€‚</li>
<li>å®éªŒéªŒè¯è¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å¤šç§å¯¹è±¡ä¸Šå‡æœ‰æ•ˆã€‚</li>
<li>è¯¥æŠ€æœ¯èƒ½ç›´æ¥é›†æˆç°æœ‰çš„æ‹¾å–å’Œæ”¾ç½®è®¾ç½®ï¼Œæ— éœ€äººå·¥å¹²é¢„æˆ–ç¯å¢ƒä¿®æ”¹ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.00370">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-2035f6c4be743de83b6b6c3884690b27.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-ba7a1307374e31e9533da7b441b4c2b4.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ffc53775a0f4dbe213849bd232a6e089.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-8bf64c8b4433f1f326ffa94b2421a4db.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ca287e62ba891a0564adb80d7d49c127.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="ActiveGAMER-Active-GAussian-Mapping-through-Efficient-Rendering"><a href="#ActiveGAMER-Active-GAussian-Mapping-through-Efficient-Rendering" class="headerlink" title="ActiveGAMER: Active GAussian Mapping through Efficient Rendering"></a>ActiveGAMER: Active GAussian Mapping through Efficient Rendering</h2><p><strong>Authors:Liyan Chen, Huangying Zhan, Kevin Chen, Xiangyu Xu, Qingan Yan, Changjiang Cai, Yi Xu</strong></p>
<p>We introduce ActiveGAMER, an active mapping system that utilizes 3D Gaussian Splatting (3DGS) to achieve high-quality, real-time scene mapping and exploration. Unlike traditional NeRF-based methods, which are computationally demanding and restrict active mapping performance, our approach leverages the efficient rendering capabilities of 3DGS, allowing effective and efficient exploration in complex environments. The core of our system is a rendering-based information gain module that dynamically identifies the most informative viewpoints for next-best-view planning, enhancing both geometric and photometric reconstruction accuracy. ActiveGAMER also integrates a carefully balanced framework, combining coarse-to-fine exploration, post-refinement, and a global-local keyframe selection strategy to maximize reconstruction completeness and fidelity. Our system autonomously explores and reconstructs environments with state-of-the-art geometric and photometric accuracy and completeness, significantly surpassing existing approaches in both aspects. Extensive evaluations on benchmark datasets such as Replica and MP3D highlight ActiveGAMERâ€™s effectiveness in active mapping tasks. </p>
<blockquote>
<p>æˆ‘ä»¬ä»‹ç»äº†ActiveGAMERï¼Œè¿™æ˜¯ä¸€ä¸ªåˆ©ç”¨3Dé«˜æ–¯æ‹¼æ¥ï¼ˆ3DGSï¼‰æŠ€æœ¯çš„ä¸»åŠ¨æ˜ å°„ç³»ç»Ÿï¼Œå¯å®ç°é«˜è´¨é‡ã€å®æ—¶çš„åœºæ™¯æ˜ å°„å’Œæ¢ç´¢ã€‚ä¸ä¼ ç»Ÿçš„åŸºäºNeRFçš„æ–¹æ³•ä¸åŒï¼Œè¿™äº›æ–¹æ³•è®¡ç®—é‡å¤§ï¼Œé™åˆ¶äº†ä¸»åŠ¨æ˜ å°„çš„æ€§èƒ½ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åˆ©ç”¨3DGSçš„é«˜æ•ˆæ¸²æŸ“èƒ½åŠ›ï¼Œèƒ½å¤Ÿåœ¨å¤æ‚ç¯å¢ƒä¸­å®ç°æœ‰æ•ˆä¸”é«˜æ•ˆçš„æ¢ç´¢ã€‚æˆ‘ä»¬çš„ç³»ç»Ÿçš„æ ¸å¿ƒæ˜¯åŸºäºæ¸²æŸ“çš„ä¿¡æ¯å¢ç›Šæ¨¡å—ï¼Œè¯¥æ¨¡å—èƒ½å¤ŸåŠ¨æ€è¯†åˆ«æœ€å…·ä¿¡æ¯é‡çš„è§‚ç‚¹ï¼Œç”¨äºè¿›è¡Œä¸‹ä¸€æ¬¡æœ€ä½³è§†è§’è§„åˆ’ï¼Œæé«˜å‡ ä½•å’Œå…‰åº¦é‡å»ºçš„ç²¾åº¦ã€‚ActiveGAMERè¿˜æ•´åˆäº†ä¸€ä¸ªç²¾å¿ƒè®¾è®¡çš„æ¡†æ¶ï¼Œç»“åˆäº†ä»ç²—åˆ°ç»†çš„æ¢ç´¢ã€åå¤„ç†å’Œå…¨å±€å±€éƒ¨å…³é”®å¸§é€‰æ‹©ç­–ç•¥ï¼Œä»¥æœ€å¤§åŒ–é‡å»ºçš„å®Œæ•´æ€§å’Œé€¼çœŸåº¦ã€‚æˆ‘ä»¬çš„ç³»ç»Ÿä»¥æœ€å…ˆè¿›çš„å‡ ä½•å’Œå…‰åº¦å‡†ç¡®æ€§å’Œå®Œæ•´æ€§è‡ªä¸»æ¢ç´¢å’Œé‡å»ºç¯å¢ƒï¼Œåœ¨å„æ–¹é¢éƒ½å¤§å¤§è¶…è¶Šäº†ç°æœ‰æ–¹æ³•ã€‚åœ¨Replicaå’ŒMP3Dç­‰åŸºå‡†æ•°æ®é›†ä¸Šçš„å¹¿æ³›è¯„ä¼°å‡¸æ˜¾äº†ActiveGAMERåœ¨ä¸»åŠ¨æ˜ å°„ä»»åŠ¡ä¸­çš„æœ‰æ•ˆæ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.06897v2">PDF</a> Accepted to CVPR2025</p>
<p><strong>Summary</strong><br>ActiveGAMERæ˜¯ä¸€ç§åˆ©ç”¨3Dé«˜æ–¯æ‹¼è´´æŠ€æœ¯å®ç°é«˜è´¨é‡å®æ—¶åœºæ™¯æ˜ å°„å’Œæ¢ç´¢çš„ä¸»åŠ¨æ˜ å°„ç³»ç»Ÿã€‚å®ƒé‡‡ç”¨é«˜æ•ˆæ¸²æŸ“æŠ€æœ¯ï¼Œå…‹æœä¼ ç»ŸNeRFæ–¹æ³•è®¡ç®—é‡å¤§ã€å½±å“ä¸»åŠ¨æ˜ å°„æ€§èƒ½çš„ç¼ºç‚¹ï¼Œåœ¨å¤æ‚ç¯å¢ƒä¸­å®ç°é«˜æ•ˆæ¢ç´¢ã€‚å…¶æ ¸å¿ƒæ˜¯ä¸€ä¸ªåŸºäºæ¸²æŸ“çš„ä¿¡æ¯å¢ç›Šæ¨¡å—ï¼Œèƒ½åŠ¨æ€è¯†åˆ«æœ€å…·ä¿¡æ¯é‡çš„è§†è§’è¿›è¡Œä¸‹ä¸€æ­¥æœ€ä½³è§†è§’è§„åˆ’ï¼Œæé«˜å‡ ä½•å’Œå…‰åº¦é‡å»ºç²¾åº¦ã€‚ActiveGAMERç»“åˆç²—åˆ°ç»†æ¢ç´¢ã€åä¼˜åŒ–å’Œå…¨å±€å±€éƒ¨å…³é”®å¸§é€‰æ‹©ç­–ç•¥ï¼Œæœ€å¤§åŒ–é‡å»ºçš„å®Œæ•´æ€§å’Œä¿çœŸåº¦ã€‚ç³»ç»Ÿåœ¨å‡ ä½•å’Œå…‰åº¦å‡†ç¡®æ€§å’Œå®Œæ•´æ€§æ–¹é¢è¾¾åˆ°é¢†å…ˆæ°´å¹³ï¼Œæ˜¾è‘—è¶…è¶Šç°æœ‰æ–¹æ³•ã€‚åœ¨å¦‚Replicaå’ŒMP3Dç­‰åŸºå‡†æ•°æ®é›†ä¸Šçš„è¯„ä¼°è¯æ˜äº†ActiveGAMERåœ¨ä¸»åŠ¨æ˜ å°„ä»»åŠ¡ä¸­çš„æœ‰æ•ˆæ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ActiveGAMERç³»ç»Ÿåˆ©ç”¨3Dé«˜æ–¯æ‹¼è´´æŠ€æœ¯å®ç°é«˜æ•ˆæ¸²æŸ“ï¼Œæé«˜åœºæ™¯æ˜ å°„å’Œæ¢ç´¢çš„è´¨é‡ã€‚</li>
<li>ä¸ä¼ ç»ŸNeRFæ–¹æ³•ç›¸æ¯”ï¼ŒActiveGAMERå…·æœ‰æ›´é«˜çš„è®¡ç®—æ•ˆç‡ï¼Œèƒ½å¤Ÿå…‹æœè®¡ç®—é‡å¤§å¯¹ä¸»åŠ¨æ˜ å°„æ€§èƒ½çš„å½±å“ã€‚</li>
<li>ActiveGAMERé€šè¿‡åŠ¨æ€è¯†åˆ«æœ€å…·ä¿¡æ¯é‡çš„è§†è§’ï¼Œæé«˜å‡ ä½•å’Œå…‰åº¦é‡å»ºçš„ç²¾åº¦ã€‚</li>
<li>ç³»ç»Ÿç»“åˆç²—åˆ°ç»†æ¢ç´¢ã€åä¼˜åŒ–å’Œå…¨å±€å±€éƒ¨å…³é”®å¸§é€‰æ‹©ç­–ç•¥ï¼Œä»¥æœ€å¤§åŒ–é‡å»ºçš„å®Œæ•´æ€§å’Œä¿çœŸåº¦ã€‚</li>
<li>ActiveGAMERåœ¨å‡ ä½•å’Œå…‰åº¦å‡†ç¡®æ€§å’Œå®Œæ•´æ€§æ–¹é¢è¾¾åˆ°é¢†å…ˆæ°´å¹³ã€‚</li>
<li>åœ¨åŸºå‡†æ•°æ®é›†ä¸Šçš„è¯„ä¼°è¯æ˜äº†ActiveGAMERåœ¨ä¸»åŠ¨æ˜ å°„ä»»åŠ¡ä¸­çš„æœ‰æ•ˆæ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.06897">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-3ee2068cd514667fdf8590d3278fd8d4.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-6ec14eb4cb933ec03aae06000294635e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-50bac8affe1e91cfbfd2b6b121faa689.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-39039f9725c9caaa76b25907768bed3e.jpg" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="Light-Transport-aware-Diffusion-Posterior-Sampling-for-Single-View-Reconstruction-of-3D-Volumes"><a href="#Light-Transport-aware-Diffusion-Posterior-Sampling-for-Single-View-Reconstruction-of-3D-Volumes" class="headerlink" title="Light Transport-aware Diffusion Posterior Sampling for Single-View   Reconstruction of 3D Volumes"></a>Light Transport-aware Diffusion Posterior Sampling for Single-View   Reconstruction of 3D Volumes</h2><p><strong>Authors:Ludwic Leonard, Nils Thuerey, Ruediger Westermann</strong></p>
<p>We introduce a single-view reconstruction technique of volumetric fields in which multiple light scattering effects are omnipresent, such as in clouds. We model the unknown distribution of volumetric fields using an unconditional diffusion model trained on a novel benchmark dataset comprising 1,000 synthetically simulated volumetric density fields. The neural diffusion model is trained on the latent codes of a novel, diffusion-friendly, monoplanar representation. The generative model is used to incorporate a tailored parametric diffusion posterior sampling technique into different reconstruction tasks. A physically-based differentiable volume renderer is employed to provide gradients with respect to light transport in the latent space. This stands in contrast to classic NeRF approaches and makes the reconstructions better aligned with observed data. Through various experiments, we demonstrate single-view reconstruction of volumetric clouds at a previously unattainable quality. </p>
<blockquote>
<p>æˆ‘ä»¬ä»‹ç»äº†ä¸€ç§ä½“ç§¯åœºå•è§†å›¾é‡å»ºæŠ€æœ¯ï¼Œè¯¥æŠ€æœ¯æ™®éå­˜åœ¨å¤šæ¬¡æ•£å°„æ•ˆåº”ï¼Œä¾‹å¦‚åœ¨äº‘ä¸­ã€‚æˆ‘ä»¬ä½¿ç”¨æ— æ¡ä»¶æ‰©æ•£æ¨¡å‹å¯¹ä½“ç§¯åœºçš„æœªçŸ¥åˆ†å¸ƒè¿›è¡Œå»ºæ¨¡ï¼Œè¯¥æ¨¡å‹æ˜¯åœ¨ç”±1000ä¸ªåˆæˆæ¨¡æ‹Ÿä½“ç§¯å¯†åº¦åœºç»„æˆçš„æ–°å‹åŸºå‡†æ•°æ®é›†ä¸Šè¿›è¡Œè®­ç»ƒçš„ã€‚ç¥ç»æ‰©æ•£æ¨¡å‹æ˜¯åœ¨ä¸€ç§æ–°å‹çš„ã€é€‚åˆæ‰©æ•£çš„å•å¹³é¢è¡¨ç¤ºçš„æ½œåœ¨ä»£ç ä¸Šè¿›è¡Œè®­ç»ƒçš„ã€‚ç”Ÿæˆæ¨¡å‹è¢«ç”¨äºå°†å®šåˆ¶çš„å‚æ•°åŒ–æ‰©æ•£åé‡‡æ ·æŠ€æœ¯èå…¥ä¸åŒçš„é‡å»ºä»»åŠ¡ä¸­ã€‚é‡‡ç”¨åŸºäºç‰©ç†çš„å¯å¾®ä½“ç§¯æ¸²æŸ“å™¨ï¼Œä»¥æä¾›å…³äºæ½œåœ¨ç©ºé—´ä¸­å…‰ä¼ è¾“çš„æ¢¯åº¦ã€‚è¿™ä¸ç»å…¸çš„NeRFæ–¹æ³•å½¢æˆå¯¹æ¯”ï¼Œä½¿é‡å»ºä¸è§‚æµ‹æ•°æ®æ›´å¥½åœ°å¯¹é½ã€‚é€šè¿‡ä¸€ç³»åˆ—å®éªŒï¼Œæˆ‘ä»¬å±•ç¤ºäº†ä½“ç§¯äº‘çš„å•è§†å›¾é‡å»ºè¾¾åˆ°äº†å‰æ‰€æœªæœ‰çš„è´¨é‡ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.05226v3">PDF</a> CVPR 2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸€ç§å•è§†å›¾é‡å»ºæŠ€æœ¯ï¼Œè¯¥æŠ€æœ¯èƒ½å¤Ÿæ¨¡æ‹Ÿåœ¨äº‘ç­‰åœºæ™¯ä¸­çš„å¤šæ¬¡æ•£å°„æ•ˆåº”ã€‚é€šè¿‡é‡‡ç”¨æ–°å‹åŸºå‡†æ•°æ®é›†ï¼Œå¯¹ä½“ç§¯åœºåˆ†å¸ƒè¿›è¡Œå»ºæ¨¡å¹¶ä½¿ç”¨æ— æ¡ä»¶æ‰©æ•£æ¨¡å‹è¿›è¡Œè®­ç»ƒã€‚è¯¥æ¨¡å‹åŸºäºä¸€ç§æ–°å‹çš„æ‰©æ•£å‹å¥½å‹å•å¹³é¢è¡¨ç¤ºæ–¹æ³•çš„æ½œåœ¨ä»£ç è¿›è¡Œè®­ç»ƒã€‚é‡‡ç”¨å®šåˆ¶çš„å‚æ•°åŒ–æ‰©æ•£åé‡‡æ ·æŠ€æœ¯èå…¥ä¸åŒçš„é‡å»ºä»»åŠ¡ä¸­ï¼Œå¹¶é‡‡ç”¨åŸºäºç‰©ç†çš„å¯å¾®åˆ†ä½“ç§¯æ¸²æŸ“å™¨æ¥æä¾›å…³äºæ½œåœ¨ç©ºé—´çš„å…‰ä¼ è¾“æ¢¯åº¦ã€‚ä¸ç»å…¸NeRFæ–¹æ³•ç›¸æ¯”ï¼Œè¿™ç§æ–¹æ³•ä½¿é‡å»ºç»“æœæ›´åŠ ç¬¦åˆè§‚æµ‹æ•°æ®ï¼Œå¹¶é€šè¿‡å®éªŒå®ç°äº†ä½“ç§¯äº‘çš„å•è§†å›¾é‡å»ºï¼Œè¾¾åˆ°äº†å‰æ‰€æœªæœ‰çš„è´¨é‡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¼•å…¥äº†ä¸€ç§å•è§†å›¾é‡å»ºæŠ€æœ¯ï¼Œèƒ½å¤Ÿæ¨¡æ‹Ÿå¤šæ¬¡æ•£å°„æ•ˆåº”ï¼Œé€‚ç”¨äºäº‘ç­‰åœºæ™¯ã€‚</li>
<li>é‡‡ç”¨æ–°å‹åŸºå‡†æ•°æ®é›†å¯¹ä½“ç§¯åœºåˆ†å¸ƒè¿›è¡Œå»ºæ¨¡ï¼Œå¹¶ä½¿ç”¨æ— æ¡ä»¶æ‰©æ•£æ¨¡å‹è®­ç»ƒã€‚</li>
<li>åˆ©ç”¨æ‰©æ•£å‹å¥½å‹å•å¹³é¢è¡¨ç¤ºæ–¹æ³•çš„æ½œåœ¨ä»£ç è¿›è¡Œè®­ç»ƒã€‚</li>
<li>é‡‡ç”¨å®šåˆ¶çš„å‚æ•°åŒ–æ‰©æ•£åé‡‡æ ·æŠ€æœ¯èå…¥é‡å»ºä»»åŠ¡ä¸­ã€‚</li>
<li>é‡‡ç”¨åŸºäºç‰©ç†çš„å¯å¾®åˆ†ä½“ç§¯æ¸²æŸ“å™¨æä¾›å…³äºæ½œåœ¨ç©ºé—´çš„å…‰ä¼ è¾“æ¢¯åº¦ã€‚</li>
<li>è¯¥æ–¹æ³•ä½¿é‡å»ºç»“æœæ›´ç¬¦åˆè§‚æµ‹æ•°æ®ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.05226">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-11566dd50cc743473d60ef3912138912.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a06d3ba44d73061de32f149c95dd8d86.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-7503e4f9aa5d5234ea882157ec7087e5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-765075193dfa447cd0ad9bd7a3954830.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b9be618087546de4ff8ec70027a91ab5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-43b74ad375be4fbaa283ef2166a6bff1.jpg" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="Self-Calibrating-4D-Novel-View-Synthesis-from-Monocular-Videos-Using-Gaussian-Splatting"><a href="#Self-Calibrating-4D-Novel-View-Synthesis-from-Monocular-Videos-Using-Gaussian-Splatting" class="headerlink" title="Self-Calibrating 4D Novel View Synthesis from Monocular Videos Using   Gaussian Splatting"></a>Self-Calibrating 4D Novel View Synthesis from Monocular Videos Using   Gaussian Splatting</h2><p><strong>Authors:Fang Li, Hao Zhang, Narendra Ahuja</strong></p>
<p>Gaussian Splatting (GS) has significantly elevated scene reconstruction efficiency and novel view synthesis (NVS) accuracy compared to Neural Radiance Fields (NeRF), particularly for dynamic scenes. However, current 4D NVS methods, whether based on GS or NeRF, primarily rely on camera parameters provided by COLMAP and even utilize sparse point clouds generated by COLMAP for initialization, which lack accuracy as well are time-consuming. This sometimes results in poor dynamic scene representation, especially in scenes with large object movements, or extreme camera conditions e.g. small translations combined with large rotations. Some studies simultaneously optimize the estimation of camera parameters and scenes, supervised by additional information like depth, optical flow, etc. obtained from off-the-shelf models. Using this unverified information as ground truth can reduce robustness and accuracy, which does frequently occur for long monocular videos (with e.g. &gt; hundreds of frames). We propose a novel approach that learns a high-fidelity 4D GS scene representation with self-calibration of camera parameters. It includes the extraction of 2D point features that robustly represent 3D structure, and their use for subsequent joint optimization of camera parameters and 3D structure towards overall 4D scene optimization. We demonstrate the accuracy and time efficiency of our method through extensive quantitative and qualitative experimental results on several standard benchmarks. The results show significant improvements over state-of-the-art methods for 4D novel view synthesis. The source code will be released soon at <a target="_blank" rel="noopener" href="https://github.com/fangli333/SC-4DGS">https://github.com/fangli333/SC-4DGS</a>. </p>
<blockquote>
<p>åŸºäºé«˜æ–¯ç»˜åˆ¶æŠ€æœ¯ï¼ˆGSï¼‰çš„åœºæ™¯é‡å»ºæ•ˆç‡å’Œæ–°é¢–è§†è§’åˆæˆï¼ˆNVSï¼‰ç²¾åº¦ç›¸è¾ƒäºç¥ç»ç½‘ç»œè¾å°„åœºï¼ˆNeRFï¼‰æœ‰æ˜¾è‘—æå‡ï¼Œç‰¹åˆ«æ˜¯å¯¹äºåŠ¨æ€åœºæ™¯è€Œè¨€ã€‚ç„¶è€Œï¼Œå½“å‰çš„å››ç»´NVSæ–¹æ³•ï¼Œæ— è®ºæ˜¯åŸºäºGSè¿˜æ˜¯NeRFï¼Œä¸»è¦ä¾èµ–äºCOLMAPæä¾›çš„ç›¸æœºå‚æ•°ï¼Œç”šè‡³ä½¿ç”¨COLMAPç”Ÿæˆçš„ç¨€ç–ç‚¹äº‘è¿›è¡Œåˆå§‹åŒ–ï¼Œè¿™æ—¢ç¼ºä¹å‡†ç¡®æ€§åˆè€—æ—¶ã€‚è¿™æœ‰æ—¶ä¼šå¯¼è‡´å¯¹åŠ¨æ€åœºæ™¯çš„è¡¨ç¤ºä¸ä½³ï¼Œç‰¹åˆ«æ˜¯åœ¨ç‰©ä½“ç§»åŠ¨è¾ƒå¤§çš„åœºæ™¯æˆ–æç«¯ç›¸æœºæ¡ä»¶ä¸‹ï¼ˆä¾‹å¦‚å°å¹³ç§»ä¸å¤§æ—‹è½¬ç»“åˆçš„æƒ…å†µï¼‰ã€‚ä¸€äº›ç ”ç©¶åŒæ—¶ä¼˜åŒ–ç›¸æœºå‚æ•°çš„ä¼°è®¡å’Œåœºæ™¯ï¼Œé€šè¿‡ç°æˆçš„æ¨¡å‹è·å–æ·±åº¦ã€å…‰æµç­‰é¢å¤–ä¿¡æ¯è¿›è¡Œç›‘ç£ã€‚ä½¿ç”¨æœªç»éªŒè¯çš„ä¿¡æ¯ä½œä¸ºçœŸå®æ•°æ®å¯èƒ½ä¼šé™ä½ç¨³å¥æ€§å’Œå‡†ç¡®æ€§ï¼Œè¿™åœ¨é’ˆå¯¹å…·æœ‰æ•°ç™¾å¸§çš„é•¿å•çœ¼è§†é¢‘æ—¶ç»å¸¸å‘ç”Ÿã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°å‹æ–¹æ³•ï¼Œè¯¥æ–¹æ³•é‡‡ç”¨è‡ªæ ¡å‡†ç›¸æœºå‚æ•°çš„æ–¹å¼å­¦ä¹ é«˜ä¿çœŸå››ç»´GSåœºæ™¯è¡¨ç¤ºã€‚å®ƒåŒ…å«æå–ç¨³å¥ä»£è¡¨ä¸‰ç»´ç»“æ„çš„äºŒç»´ç‚¹ç‰¹å¾ï¼Œå¹¶å°†å…¶ç”¨äºè”åˆä¼˜åŒ–ç›¸æœºå‚æ•°å’Œä¸‰ç»´ç»“æ„ï¼Œä»¥å®ç°æ•´ä½“çš„å››ç»´åœºæ™¯ä¼˜åŒ–ã€‚æˆ‘ä»¬é€šè¿‡å¤šä¸ªæ ‡å‡†åŸºå‡†çš„å®šé‡å’Œå®šæ€§å®éªŒç»“æœï¼Œå±•ç¤ºäº†è¯¥æ–¹æ³•çš„å‡†ç¡®æ€§å’Œæ—¶é—´æ•ˆç‡ã€‚ç»“æœæ˜¾ç¤ºï¼Œç›¸è¾ƒäºå››ç»´æ–°é¢–è§†è§’åˆæˆé¢†åŸŸçš„æœ€å…ˆè¿›æ–¹æ³•æœ‰æ˜æ˜¾çš„æ”¹è¿›ã€‚æºä»£ç å¾ˆå¿«å°†åœ¨[<a target="_blank" rel="noopener" href="https://github.com/fangli333/SC-4DGS%E5%8F%91%E5%B8%83%E3%80%82]">https://github.com/fangli333/SC-4DGSå‘å¸ƒã€‚]</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2406.01042v3">PDF</a> GitHub Page: <a target="_blank" rel="noopener" href="https://github.com/fangli333/SC-4DGS">https://github.com/fangli333/SC-4DGS</a></p>
<p><strong>Summary</strong><br>     æœ¬æ–‡æå‡ºä¸€ç§åŸºäºé«˜æ–¯æ··æˆæŠ€æœ¯ï¼ˆGaussian Splatting, GSï¼‰çš„è‡ªæˆ‘æ ¡å‡†4Dåœºæ™¯è¡¨ç¤ºæ–¹æ³•ã€‚è¯¥æ–¹æ³•èƒ½é«˜æ•ˆå‡†ç¡®åœ°é‡å»ºåœºæ™¯å¹¶åˆæˆæ–°è§†è§’ï¼Œå°¤å…¶é€‚ç”¨äºåŠ¨æ€åœºæ™¯ã€‚é€šè¿‡æå–ç¨³å¥çš„2Dç‰¹å¾ç‚¹ï¼Œè”åˆä¼˜åŒ–ç›¸æœºå‚æ•°å’Œ3Dç»“æ„ï¼Œè¾¾åˆ°æ•´ä½“çš„4Dåœºæ™¯ä¼˜åŒ–ã€‚åœ¨å¤šä¸ªæ ‡å‡†åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜è¶Šã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>é«˜æ–¯æ··æˆæŠ€æœ¯ï¼ˆGSï¼‰ç›¸è¾ƒäºç¥ç»ç½‘ç»œè¾å°„åœºï¼ˆNeRFï¼‰æå‡äº†åœºæ™¯é‡å»ºæ•ˆç‡å’Œæ–°å‹è§†è§’åˆæˆï¼ˆNVSï¼‰çš„å‡†ç¡®æ€§ï¼Œç‰¹åˆ«æ˜¯åœ¨åŠ¨æ€åœºæ™¯ä¸­ã€‚</li>
<li>å½“å‰4D NVSæ–¹æ³•ä¸»è¦ä¾èµ–COLMAPæä¾›çš„ç›¸æœºå‚æ•°ï¼Œå¹¶ç”¨äºåˆå§‹åŒ–ç¨€ç–ç‚¹äº‘ï¼Œä½†è¿™ç§æ–¹æ³•å‡†ç¡®æ€§å’Œæ•ˆç‡è¾ƒä½ã€‚</li>
<li>æ–‡ä¸­æåˆ°çš„æ–¹æ³•åˆ©ç”¨æå–çš„2Dç‰¹å¾ç‚¹æ¥ç¨³å¥åœ°è¡¨ç¤º3Dç»“æ„ï¼Œéšåå¯¹ç›¸æœºå‚æ•°å’Œ3Dç»“æ„è¿›è¡Œè”åˆä¼˜åŒ–ï¼Œå®ç°è‡ªæˆ‘æ ¡å‡†çš„4Dåœºæ™¯è¡¨ç¤ºã€‚</li>
<li>æ–‡ä¸­æ–¹æ³•æ˜¾è‘—æé«˜äº†4Dæ–°å‹è§†è§’åˆæˆçš„å‡†ç¡®æ€§ï¼Œå¹¶åœ¨å¤šä¸ªæ ‡å‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜è¶Šã€‚</li>
<li>æ‰€æå‡ºçš„æ–¹æ³•é€‚ç”¨äºå¤§å‹å¯¹è±¡ç§»åŠ¨ã€æç«¯ç›¸æœºæ¡ä»¶ï¼ˆå¦‚å°å¹³ç§»ä¸å¤§æ—‹è½¬ç»“åˆï¼‰ç­‰å¤æ‚åœºæ™¯ã€‚</li>
<li>ç›®å‰çš„æ–¹æ³•ä¾èµ–äºé¢å¤–çš„ä¿¡æ¯ï¼ˆå¦‚æ·±åº¦ã€å…‰æµç­‰ï¼‰æ¥ç›‘ç£ç›¸æœºå‚æ•°å’Œåœºæ™¯çš„ä¼°è®¡ï¼Œè¿™å¯èƒ½é™ä½ç¨³å¥æ€§å’Œå‡†ç¡®æ€§ï¼Œç‰¹åˆ«æ˜¯åœ¨é•¿å•ç›®è§†é¢‘ï¼ˆå¦‚åŒ…å«æ•°ç™¾å¸§ï¼‰ä¸­ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2406.01042">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-aeb8b6be9ed822de16b6ab6a254689b4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-943f2448f91afb60a72f6a93466398e1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-46743dd4fd6de272e99deea3ad94b4ac.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-cb752928248c98233389d6a68f5cbb84.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-94b1a6297fd9bb1be9cdc3fbe53fc163.jpg" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1><h2 id="MixRT-Mixed-Neural-Representations-For-Real-Time-NeRF-Rendering"><a href="#MixRT-Mixed-Neural-Representations-For-Real-Time-NeRF-Rendering" class="headerlink" title="MixRT: Mixed Neural Representations For Real-Time NeRF Rendering"></a>MixRT: Mixed Neural Representations For Real-Time NeRF Rendering</h2><p><strong>Authors:Chaojian Li, Bichen Wu, Peter Vajda, Yingyan Celine Lin</strong></p>
<p>Neural Radiance Field (NeRF) has emerged as a leading technique for novel view synthesis, owing to its impressive photorealistic reconstruction and rendering capability. Nevertheless, achieving real-time NeRF rendering in large-scale scenes has presented challenges, often leading to the adoption of either intricate baked mesh representations with a substantial number of triangles or resource-intensive ray marching in baked representations. We challenge these conventions, observing that high-quality geometry, represented by meshes with substantial triangles, is not necessary for achieving photorealistic rendering quality. Consequently, we propose MixRT, a novel NeRF representation that includes a low-quality mesh, a view-dependent displacement map, and a compressed NeRF model. This design effectively harnesses the capabilities of existing graphics hardware, thus enabling real-time NeRF rendering on edge devices. Leveraging a highly-optimized WebGL-based rendering framework, our proposed MixRT attains real-time rendering speeds on edge devices (over 30 FPS at a resolution of 1280 x 720 on a MacBook M1 Pro laptop), better rendering quality (0.2 PSNR higher in indoor scenes of the Unbounded-360 datasets), and a smaller storage size (less than 80% compared to state-of-the-art methods). </p>
<blockquote>
<p>ç¥ç»è¾å°„åœºï¼ˆNeRFï¼‰å·²ç»æˆä¸ºæ–°å‹è§†è§’åˆæˆçš„ä¸»æµæŠ€æœ¯ï¼Œå› å…¶ä»¤äººå°è±¡æ·±åˆ»çš„é€¼çœŸé‡å»ºå’Œæ¸²æŸ“èƒ½åŠ›ã€‚ç„¶è€Œï¼Œåœ¨å¤§è§„æ¨¡åœºæ™¯ä¸­å®ç°å®æ—¶NeRFæ¸²æŸ“æå‡ºäº†æŒ‘æˆ˜ï¼Œé€šå¸¸å¯¼è‡´é‡‡ç”¨å¤æ‚çš„é¢„çƒ˜ç„™ç½‘æ ¼è¡¨ç¤ºæ³•ï¼Œå…¶ä¸­åŒ…å«å¤§é‡çš„ä¸‰è§’å½¢ï¼Œæˆ–è€…é‡‡ç”¨é¢„çƒ˜ç„™è¡¨ç¤ºæ³•ä¸­çš„èµ„æºå¯†é›†å‹å…‰çº¿è¿½è¸ªã€‚æˆ‘ä»¬è´¨ç–‘è¿™äº›ä¼ ç»Ÿè§‚å¿µï¼Œè§‚å¯Ÿåˆ°é«˜è´¨é‡å‡ ä½•ä½“ï¼ˆç”±å¤§é‡ä¸‰è§’å½¢è¡¨ç¤ºçš„ç½‘æ ¼ï¼‰å¯¹äºå®ç°é€¼çœŸæ¸²æŸ“è´¨é‡å¹¶éå¿…è¦ã€‚å› æ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†MixRTï¼Œè¿™æ˜¯ä¸€ç§æ–°å‹çš„NeRFè¡¨ç¤ºæ–¹æ³•ï¼Œå®ƒåŒ…å«ä½è´¨é‡ç½‘æ ¼ã€è§†å·®ç›¸å…³ä½ç§»å›¾å’Œå‹ç¼©NeRFæ¨¡å‹ã€‚è¿™ç§è®¾è®¡æœ‰æ•ˆåœ°åˆ©ç”¨äº†ç°æœ‰å›¾å½¢ç¡¬ä»¶çš„åŠŸèƒ½ï¼Œä»è€Œåœ¨è¾¹ç¼˜è®¾å¤‡ä¸Šå®ç°äº†å®æ—¶NeRFæ¸²æŸ“ã€‚é€šè¿‡åŸºäºé«˜åº¦ä¼˜åŒ–çš„WebGLæ¸²æŸ“æ¡†æ¶ï¼Œæˆ‘ä»¬æå‡ºçš„MixRTåœ¨è¾¹ç¼˜è®¾å¤‡ä¸Šå®ç°äº†å®æ—¶æ¸²æŸ“é€Ÿåº¦ï¼ˆåœ¨MacBook M1 Proç¬”è®°æœ¬ç”µè„‘ä¸Šä»¥1280 x 720çš„åˆ†è¾¨ç‡è¶…è¿‡30 FPSï¼‰ï¼Œæ›´å¥½çš„æ¸²æŸ“è´¨é‡ï¼ˆåœ¨Unbounded-360æ•°æ®é›†å®¤å†…åœºæ™¯ä¸­é«˜å‡º0.2 PSNRï¼‰ï¼Œå¹¶ä¸”å­˜å‚¨å¤§å°æ›´å°ï¼ˆä¸æœ€å…ˆè¿›çš„æ–¹æ³•ç›¸æ¯”å‡å°‘äº†ä¸åˆ°80%ï¼‰ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2312.11841v5">PDF</a> Accepted by 3DVâ€™24. Project Page: <a target="_blank" rel="noopener" href="https://licj15.github.io/MixRT/">https://licj15.github.io/MixRT/</a></p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†NeRFæŠ€æœ¯åœ¨æ–°å‹è§†å›¾åˆæˆæ–¹é¢çš„å‰æ²¿åº”ç”¨ï¼Œæå‡ºä¸€ç§åä¸ºMixRTçš„NeRFè¡¨ç¤ºæ–¹æ³•ï¼Œé€šè¿‡ç»“åˆä½è´¨é‡ç½‘æ ¼ã€è§†è§’ç›¸å…³ä½ç§»å›¾å’Œå‹ç¼©NeRFæ¨¡å‹ï¼Œå®ç°äº†å¯¹å¤§è§„æ¨¡åœºæ™¯çš„å®æ—¶æ¸²æŸ“ã€‚åˆ©ç”¨ä¼˜åŒ–çš„WebGLæ¸²æŸ“æ¡†æ¶ï¼ŒMixRTåœ¨è¾¹ç¼˜è®¾å¤‡ä¸Šå®ç°äº†å®æ—¶æ¸²æŸ“é€Ÿåº¦ï¼ŒåŒæ—¶æé«˜äº†æ¸²æŸ“è´¨é‡å’Œå­˜å‚¨æ•ˆç‡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>NeRFæŠ€æœ¯å·²æˆä¸ºæ–°å‹è§†å›¾åˆæˆçš„é¢†å…ˆæŠ€æœ¯ï¼Œå…·æœ‰æƒŠäººçš„ç…§ç‰‡çº§é‡å»ºå’Œæ¸²æŸ“èƒ½åŠ›ã€‚</li>
<li>å®ç°å¤§è§„æ¨¡åœºæ™¯å®æ—¶NeRFæ¸²æŸ“å…·æœ‰æŒ‘æˆ˜ï¼Œé€šå¸¸é‡‡ç”¨å¤æ‚çš„çƒ˜ç„™ç½‘æ ¼è¡¨ç¤ºæˆ–èµ„æºå¯†é›†å‹çš„å°„çº¿è¿½è¸ªæ–¹æ³•ã€‚</li>
<li>æœ¬æ–‡æŒ‘æˆ˜äº†ä¸Šè¿°å¸¸è§„æ–¹æ³•ï¼ŒæŒ‡å‡ºé«˜è´¨é‡å‡ ä½•ç½‘æ ¼å¹¶éå®ç°ç…§ç‰‡çº§æ¸²æŸ“è´¨é‡çš„å¿…è¦æ¡ä»¶ã€‚</li>
<li>æå‡ºäº†MixRTï¼Œä¸€ç§ç»“åˆä½è´¨é‡ç½‘æ ¼ã€è§†è§’ç›¸å…³ä½ç§»å›¾å’Œå‹ç¼©NeRFæ¨¡å‹çš„NeRFè¡¨ç¤ºæ–¹æ³•ã€‚</li>
<li>MixRTæœ‰æ•ˆåˆ©ç”¨ç°æœ‰å›¾å½¢ç¡¬ä»¶ï¼Œä½¿å®æ—¶NeRFæ¸²æŸ“åœ¨è¾¹ç¼˜è®¾å¤‡ä¸Šæˆä¸ºå¯èƒ½ã€‚</li>
<li>åˆ©ç”¨é«˜åº¦ä¼˜åŒ–çš„WebGLæ¸²æŸ“æ¡†æ¶ï¼ŒMixRTå®ç°äº†è¾¹ç¼˜è®¾å¤‡ä¸Šçš„å®æ—¶æ¸²æŸ“é€Ÿåº¦ï¼Œå¹¶åœ¨Unbound-360æ•°æ®é›†å®¤å†…åœºæ™¯ä¸­è·å¾—æ›´é«˜çš„æ¸²æŸ“è´¨é‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2312.11841">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-c34dfa8d80e73c73f189bda7be9230e0.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-44d66c65781dd7094b22b4ed21552bd3.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-16ddd4478152e5c6a4f02bc6b86875f0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-729c2c4e7c400fc17f0e1faacd580d64.jpg" align="middle">
</details>


<h1 id="-19"><a href="#-19" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-04-03/NeRF/">https://kedreamix.github.io/Talk2Paper/Paper/2025-04-03/NeRF/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/NeRF/">
                                    <span class="chip bg-color">NeRF</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-04-04/NeRF/">
                    <div class="card-image">
                        
                        <img src="https://pica.zhimg.com/v2-592a6485d72b21b72232c5a2ce2bb71f.jpg" class="responsive-img" alt="NeRF">
                        
                        <span class="card-title">NeRF</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            NeRF æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-04-04  Diffusion-Guided Gaussian Splatting for Large-Scale Unconstrained 3D   Reconstruction and Novel View Synthesis
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-04-04
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/NeRF/" class="post-category">
                                    NeRF
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/NeRF/">
                        <span class="chip bg-color">NeRF</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-04-04/3DGS/">
                    <div class="card-image">
                        
                        <img src="https://pica.zhimg.com/v2-0cb003298d1d12e0a7c9c740f48cff60.jpg" class="responsive-img" alt="3DGS">
                        
                        <span class="card-title">3DGS</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            3DGS æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-04-04  Diffusion-Guided Gaussian Splatting for Large-Scale Unconstrained 3D   Reconstruction and Novel View Synthesis
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-04-04
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/3DGS/" class="post-category">
                                    3DGS
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/3DGS/">
                        <span class="chip bg-color">3DGS</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">26254.1k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
