<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="NeRF">
    <meta name="description" content="NeRF 方向最新论文已更新，请持续关注 Update in 2025-04-04  Diffusion-Guided Gaussian Splatting for Large-Scale Unconstrained 3D   Reconstruction and Novel View Synthesis">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>NeRF | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loader页面消失采用渐隐的方式*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logo出现动画 */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//使用渐隐的方法淡出loading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//强制显示loading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">嘘~  正在从服务器偷取页面 . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>首页</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>标签</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>分类</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>归档</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="搜索" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="深色/浅色模式" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			首页
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			标签
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			分类
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			归档
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-283a61d43de86c60c11763eb2b0d954f.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">NeRF</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- 文章内容详情 -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/NeRF/">
                                <span class="chip bg-color">NeRF</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/NeRF/" class="post-category">
                                NeRF
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp;
                    2025-04-04
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>更新日期:&nbsp;&nbsp;
                    2025-05-14
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>文章字数:&nbsp;&nbsp;
                    20.1k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>阅读时长:&nbsp;&nbsp;
                    81 分
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>阅读次数:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>⚠️ 以下所有内容总结都来自于 大语言模型的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p>
</blockquote>
<h1 id="2025-04-04-更新"><a href="#2025-04-04-更新" class="headerlink" title="2025-04-04 更新"></a>2025-04-04 更新</h1><h2 id="Diffusion-Guided-Gaussian-Splatting-for-Large-Scale-Unconstrained-3D-Reconstruction-and-Novel-View-Synthesis"><a href="#Diffusion-Guided-Gaussian-Splatting-for-Large-Scale-Unconstrained-3D-Reconstruction-and-Novel-View-Synthesis" class="headerlink" title="Diffusion-Guided Gaussian Splatting for Large-Scale Unconstrained 3D   Reconstruction and Novel View Synthesis"></a>Diffusion-Guided Gaussian Splatting for Large-Scale Unconstrained 3D   Reconstruction and Novel View Synthesis</h2><p><strong>Authors:Niluthpol Chowdhury Mithun, Tuan Pham, Qiao Wang, Ben Southall, Kshitij Minhas, Bogdan Matei, Stephan Mandt, Supun Samarasekera, Rakesh Kumar</strong></p>
<p>Recent advancements in 3D Gaussian Splatting (3DGS) and Neural Radiance Fields (NeRF) have achieved impressive results in real-time 3D reconstruction and novel view synthesis. However, these methods struggle in large-scale, unconstrained environments where sparse and uneven input coverage, transient occlusions, appearance variability, and inconsistent camera settings lead to degraded quality. We propose GS-Diff, a novel 3DGS framework guided by a multi-view diffusion model to address these limitations. By generating pseudo-observations conditioned on multi-view inputs, our method transforms under-constrained 3D reconstruction problems into well-posed ones, enabling robust optimization even with sparse data. GS-Diff further integrates several enhancements, including appearance embedding, monocular depth priors, dynamic object modeling, anisotropy regularization, and advanced rasterization techniques, to tackle geometric and photometric challenges in real-world settings. Experiments on four benchmarks demonstrate that GS-Diff consistently outperforms state-of-the-art baselines by significant margins. </p>
<blockquote>
<p>最近，关于三维高斯蒙版技术（3DGS）和神经辐射场（NeRF）的最新进展在实时三维重建和新型视角合成方面取得了令人印象深刻的结果。然而，这些方法在大型无约束环境中的表现欠佳。在这种环境下，稀疏和不平整输入覆盖、瞬时遮挡、外观变化和不一致的相机设置都会导致质量下降。为了解决这些局限性，我们提出了GS-Diff，这是一种新型的多视角扩散模型引导的3DGS框架。它通过基于多视角输入生成伪观测结果，将不受约束的三维重建问题转化为明确的建模问题，即使在稀疏数据的情况下也能实现稳健优化。GS-Diff还融合了多项改进，包括外观嵌入、单眼深度先验、动态对象建模、各向异性正则化和先进的渲染技术，以应对现实环境中的几何和光度挑战。在四个基准测试集上的实验表明，GS-Diff显著优于最新的基线模型。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.01960v1">PDF</a> WACV ULTRRA Workshop 2025</p>
<p><strong>Summary</strong></p>
<p>本文介绍了基于3D高斯融合（3DGS）和神经辐射场（NeRF）的最新进展在实时三维重建和新颖视角合成方面取得了显著成果。然而，在大规模、无约束的环境中，这些方法面临稀疏和不均匀输入覆盖、瞬时遮挡、外观变化和不一致相机设置等问题导致质量下降。为此，本文提出了GS-Diff，一种新型的多视角扩散模型引导的3DGS框架。通过生成基于多视角输入的伪观察结果，该方法将约束不足的三维重建问题转化为明确的问题，即使在稀疏数据下也能实现稳健优化。此外，GS-Diff还包括外观嵌入、单眼深度先验、动态对象建模、各向异性正则化和高级光栅化技术等多项改进，以应对现实环境中的几何和光度挑战。在四项基准测试上的实验表明，GS-Diff显著优于现有技术基线。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>3DGS与NeRF融合在实时三维重建和新颖视角合成方面取得了最新进展的重要成果。</li>
<li>在大规模、无约束环境中，现有方法面临诸多挑战，如稀疏和不均匀输入覆盖、瞬时遮挡等。</li>
<li>GS-Diff是一种新型3DGS框架，通过多视角扩散模型解决上述问题，实现稳健优化。</li>
<li>GS-Diff通过生成伪观察结果，将约束不足的三维重建问题转化为明确的问题。</li>
<li>GS-Diff集成了多项改进，包括外观嵌入、单眼深度先验等，以应对现实环境中的几何和光度挑战。</li>
<li>实验结果显示，GS-Diff在四项基准测试上显著优于现有技术基线。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.01960">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-592a6485d72b21b72232c5a2ce2bb71f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5a1ce0624dfdfb12bb64cdccf46f365b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-41d8bfdbe85e56ceb5eb98088339d893.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="BOGausS-Better-Optimized-Gaussian-Splatting"><a href="#BOGausS-Better-Optimized-Gaussian-Splatting" class="headerlink" title="BOGausS: Better Optimized Gaussian Splatting"></a>BOGausS: Better Optimized Gaussian Splatting</h2><p><strong>Authors:Stéphane Pateux, Matthieu Gendrin, Luce Morin, Théo Ladune, Xiaoran Jiang</strong></p>
<p>3D Gaussian Splatting (3DGS) proposes an efficient solution for novel view synthesis. Its framework provides fast and high-fidelity rendering. Although less complex than other solutions such as Neural Radiance Fields (NeRF), there are still some challenges building smaller models without sacrificing quality. In this study, we perform a careful analysis of 3DGS training process and propose a new optimization methodology. Our Better Optimized Gaussian Splatting (BOGausS) solution is able to generate models up to ten times lighter than the original 3DGS with no quality degradation, thus significantly boosting the performance of Gaussian Splatting compared to the state of the art. </p>
<blockquote>
<p>3D高斯融合（3DGS）为新型视角合成提供了一种高效的解决方案，其框架能够实现快速且高保真的渲染。虽然相较于神经辐射场（NeRF）等其他解决方案，其复杂性较低，但在构建不牺牲质量的小模型方面仍存在挑战。在本研究中，我们对3DGS训练过程进行了详细分析，并提出了一种新的优化方法。我们优化的高斯融合（BOGausS）解决方案能够生成比原始3DGS轻十倍的模型，且没有质量损失，从而显著提高了高斯融合与最新技术的性能。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.01844v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>该研究探讨了名为”高斯浮雕”（Gaussian Splatting）的方法在合成三维数据（尤其是用于图像渲染和全景图像生成）时的局限性。提出一种优化后的方案，即更好的优化高斯浮雕（BOGausS），能够生成质量无损但体积更小十倍的模型，极大地提升了高斯浮雕方法的性能。同时其简洁性也意味着该方法能更快地训练出更高质量的数据模型。这是在对原始方法的训练过程进行详细分析的基础上实现的突破。它不仅改善了高斯浮雕的效率问题，也为相关领域提供了一种新型的模型优化方法。这对于实时渲染和游戏等依赖快速处理的应用尤为重要。未来这项技术还有更大的潜力来改进各种模型质量和其他计算效率的挑战性问题。这也将成为机器学习和其他人工智能领域中改进各种模型的下一步关键步骤。此外，它还进一步证明了利用机器学习算法处理大规模数据集的高效性和可靠性。总结起来，该研究展示了人工智能和机器学习在优化复杂算法方面的巨大潜力。通过采用新技术，研究人员已经能够显著提高算法的性能，这有望推动相关领域的发展进步。研究的结果可能会开辟新的应用途径和商业模式。这将促进机器学习和人工智能领域的技术创新，为实际应用带来更多可能性。因此，这项研究不仅具有学术价值，而且具有巨大的实际应用潜力。此外，该技术可能对增强现实（AR）、虚拟现实（VR）等领域产生深远影响。这对于开发具有更好性能的应用程序具有重要影响，同时也有利于推动相关行业的创新和发展。随着技术的不断进步，未来的应用场景将更加广泛和丰富。这项研究将产生重大影响，成为相关领域发展的一个里程碑事件。这也将对现实世界的应用产生重要影响，包括但不限于虚拟空间建模和渲染等任务。总的来说，该研究对于人工智能和机器学习领域的发展具有重大意义。它不仅提高了算法性能，而且为未来的研究和应用提供了重要参考和启示。该研究不仅代表了当前研究的最新进展，也为未来的研究指明了方向。对于这一领域的专业人士来说，这是一个重要的里程碑事件，对于整个行业来说也具有重要意义。它不仅为行业带来了技术进步，也为未来的创新提供了广阔的空间和可能性。同时对于计算机视觉和图形学领域也具有重要意义。<strong>Key Takeaways</strong>：<br>     * 研究探讨了现有的高斯浮雕技术面临的挑战：生成大规模数据集时的效率问题和模型大小问题。<br>     * 提出了一种名为BOGausS的新优化方法，能在不损失质量的情况下生成体积更小的模型，显著提高了高斯浮雕的性能。<br>     * 研究结果强调了机器学习算法在处理大规模数据集方面的可靠性和潜力。<br>     * 该研究不仅具有学术价值，而且可能对增强现实（AR）、虚拟现实（VR）等实际应用领域产生深远影响。对于开发具有更好性能的应用程序具有重要影响。<br>     * 该研究对人工智能和机器学习领域的发展具有重要意义，代表了当前研究的最新进展和方向指引功能。。<br>     * BOGausS对于缩小机器学习模型和改善数据质量两个方面具有重要的实际意义。这项突破表明了对于利用新方法和优化工具的可能性抱有持续关注和探讨的强烈需求和应用价值判断上很大的商业应用潜力巨大改善的方向多元化发展空间及其开拓的前瞻性和全局性思维紧密关联的成就观点也意味着其价值影响的广度和深度超乎我们的预期和意义体现对它的认知和可能结果的变化已经扩展到科技、教育等多个领域特别是这些创新思路和成功策略对当前人工智能技术行业引领创新和推动行业发展的重要性方面展现出无可替代的价值地位同时其技术本身的可扩展性和灵活性也意味着其应用前景十分广阔为未来的研究和应用提供了重要的参考和启示对于整个行业来说具有里程碑意义的价值判断无疑也是对其价值的认可和肯定以及对未来科技发展的期待和信心表达无疑也体现了对人工智能领域未来发展的乐观态度和对未来技术进步的期待和信心表达体现了科技创新带来的无限可能和改变世界的力量表达了人们对科技的赞美和崇拜。总的来说其重要性在于提高了模型的训练效率和效果解决了原有算法中面临的问题增强了我们对科技行业的理解和进步而人类生活也被新的科研成果推向更高境界包括生活质量和科技发展等多个方面其价值和意义体现在科技进步对人类社会的推动上其潜在的价值和影响力无法估量对于人工智能领域的未来发展来说这是一个重要的里程碑事件为人类科技进步带来重要启示的同时也对未来的科技进步产生了积极的影响其意义重大深远并且持续发挥作用无疑也是对未来发展的信心和乐观态度的体现该研究的成功不仅代表了当前技术的最新进展也为未来的研究和应用提供了重要的参考和启示”。以下是对上述内容的简化并符合要求的总结：</p>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.01844">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-9c8cb2eb3263f67d85f48043937cfced.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f7e0bdb62b9bc6ae2299197a6852b669.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Luminance-GS-Adapting-3D-Gaussian-Splatting-to-Challenging-Lighting-Conditions-with-View-Adaptive-Curve-Adjustment"><a href="#Luminance-GS-Adapting-3D-Gaussian-Splatting-to-Challenging-Lighting-Conditions-with-View-Adaptive-Curve-Adjustment" class="headerlink" title="Luminance-GS: Adapting 3D Gaussian Splatting to Challenging Lighting   Conditions with View-Adaptive Curve Adjustment"></a>Luminance-GS: Adapting 3D Gaussian Splatting to Challenging Lighting   Conditions with View-Adaptive Curve Adjustment</h2><p><strong>Authors:Ziteng Cui, Xuangeng Chu, Tatsuya Harada</strong></p>
<p>Capturing high-quality photographs under diverse real-world lighting conditions is challenging, as both natural lighting (e.g., low-light) and camera exposure settings (e.g., exposure time) significantly impact image quality. This challenge becomes more pronounced in multi-view scenarios, where variations in lighting and image signal processor (ISP) settings across viewpoints introduce photometric inconsistencies. Such lighting degradations and view-dependent variations pose substantial challenges to novel view synthesis (NVS) frameworks based on Neural Radiance Fields (NeRF) and 3D Gaussian Splatting (3DGS). To address this, we introduce Luminance-GS, a novel approach to achieving high-quality novel view synthesis results under diverse challenging lighting conditions using 3DGS. By adopting per-view color matrix mapping and view-adaptive curve adjustments, Luminance-GS achieves state-of-the-art (SOTA) results across various lighting conditions – including low-light, overexposure, and varying exposure – while not altering the original 3DGS explicit representation. Compared to previous NeRF- and 3DGS-based baselines, Luminance-GS provides real-time rendering speed with improved reconstruction quality. </p>
<blockquote>
<p>在多样化的真实世界光照条件下捕捉高质量照片是一个挑战，因为自然光照（例如，低光环境）和相机曝光设置（例如，曝光时间）都会显著影响图像质量。在多视角场景中，这一挑战更为突出，因为不同视角的光照和图像信号处理（ISP）设置变化会引入光度不一致性。这种光照退化和视图相关的变化给基于神经辐射场（NeRF）和三维高斯拼贴（3DGS）的新视图合成（NVS）框架带来了巨大挑战。为了解决这一问题，我们引入了Luminance-GS这一新方法，通过采用视图彩色矩阵映射和视图自适应曲线调整，实现在多样化挑战光照条件下高质量的新视图合成结果。Luminance-GS在各种光照条件下均取得了最新结果，包括低光、过曝光和可变曝光等，同时不改变原始3DGS显式表示。与之前的NeRF和3DGS基线相比，Luminance-GS提供了实时的渲染速度并改善了重建质量。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.01503v1">PDF</a> CVPR 2025, project page:   <a target="_blank" rel="noopener" href="https://cuiziteng.github.io/Luminance_GS_web/">https://cuiziteng.github.io/Luminance_GS_web/</a></p>
<p><strong>Summary</strong></p>
<p>这是一篇关于在多种真实世界光照条件下使用神经网络辐射场（NeRF）进行高质量照片捕捉的挑战性研究。该研究提出了一种新的方法Luminance-GS，通过采用每视图颜色矩阵映射和视图自适应曲线调整，在多样化的挑战光照条件下实现了高质量的新型视图合成结果。相较于以往的NeRF和3DGS基线方法，Luminance-GS在不改变原始3DGS显式表示的前提下，实现了实时渲染速度的提升和重建质量的改进。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>在真实世界光照条件下捕捉高质量照片是一个挑战，因为自然光照和相机曝光设置都会影响图像质量。</li>
<li>在多视角场景中，光照变化和图像信号处理器（ISP）设置的差异会导致光度学不一致性。</li>
<li>Luminance-GS是一种新型方法，能够在各种具有挑战性的光照条件下实现高质量的新型视图合成结果。</li>
<li>通过采用每视图颜色矩阵映射和视图自适应曲线调整，Luminance-GS达到了在各种光照条件下的最佳结果，包括低光、过曝光和可变曝光。</li>
<li>Luminance-GS保留了原始的三维高斯平铺（3DGS）显式表示。</li>
<li>与基于NeRF和3DGS的先前方法相比，Luminance-GS具有更快的实时渲染速度和更高的重建质量。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.01503">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-35c5c92e224606e546fd03cf5090f540.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d178aa2c017ef369fb2414f73a079403.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a0d77b7cade783144fb29ae6054c1885.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-8064f88da9ae906527cf71cbf1d9a992.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-9c2ded93e6712755127a771f8516b4c3.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Prompting-Forgetting-Unlearning-in-GANs-via-Textual-Guidance"><a href="#Prompting-Forgetting-Unlearning-in-GANs-via-Textual-Guidance" class="headerlink" title="Prompting Forgetting: Unlearning in GANs via Textual Guidance"></a>Prompting Forgetting: Unlearning in GANs via Textual Guidance</h2><p><strong>Authors:Piyush Nagasubramaniam, Neeraj Karamchandani, Chen Wu, Sencun Zhu</strong></p>
<p>State-of-the-art generative models exhibit powerful image-generation capabilities, introducing various ethical and legal challenges to service providers hosting these models. Consequently, Content Removal Techniques (CRTs) have emerged as a growing area of research to control outputs without full-scale retraining. Recent work has explored the use of Machine Unlearning in generative models to address content removal. However, the focus of such research has been on diffusion models, and unlearning in Generative Adversarial Networks (GANs) has remained largely unexplored. We address this gap by proposing Text-to-Unlearn, a novel framework that selectively unlearns concepts from pre-trained GANs using only text prompts, enabling feature unlearning, identity unlearning, and fine-grained tasks like expression and multi-attribute removal in models trained on human faces. Leveraging natural language descriptions, our approach guides the unlearning process without requiring additional datasets or supervised fine-tuning, offering a scalable and efficient solution. To evaluate its effectiveness, we introduce an automatic unlearning assessment method adapted from state-of-the-art image-text alignment metrics, providing a comprehensive analysis of the unlearning methodology. To our knowledge, Text-to-Unlearn is the first cross-modal unlearning framework for GANs, representing a flexible and efficient advancement in managing generative model behavior. </p>
<blockquote>
<p>当前最先进的生成模型展现出强大的图像生成能力，给托管这些模型的服务提供商带来了各种伦理和法律挑战。因此，内容删除技术（CRTs）作为控制输出而无需全面再训练的研究领域正日益兴起。近期的工作探索了在生成模型中使用机器遗忘来应对内容删除。然而，这类研究的重点主要集中在扩散模型上，生成对抗网络（GANs）中的遗忘却被大大忽视了。我们通过提出Text-to-Unlearn来解决这一空白，这是一个新颖的框架，能够仅使用文本提示从预训练的GANs中选择性遗忘概念，实现特征遗忘、身份遗忘以及在人脸训练模型中的表情和多属性删除等精细任务。我们的方法利用自然语言描述来引导遗忘过程，无需额外的数据集或监督微调，提供可伸缩和高效的解决方案。为了评估其有效性，我们引入了一种自动遗忘评估方法，该方法改编自最先进的图像文本对齐指标，对遗忘方法进行全面分析。据我们所知，Text-to-Unlearn是首个用于GANs的跨模态遗忘框架，代表着在管理生成模型行为方面灵活高效的进步。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.01218v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文提出了一种名为Text-to-Unlearn的新型框架，该框架能够通过文本提示选择性遗忘预训练生成对抗网络（GANs）中的概念，从而实现特征遗忘、身份遗忘以及面部表情和多属性遗忘等精细任务。该框架利用自然语言描述引导遗忘过程，无需额外数据集或监督微调，提供可伸缩和高效的解决方案。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Text-to-Unlearn是一个新型框架，能够选择性遗忘预训练生成对抗网络（GANs）中的特定概念。</li>
<li>该框架通过文本提示实现特征遗忘、身份遗忘以及面部表情和多属性遗忘等精细任务。</li>
<li>Text-to-Unlearn利用自然语言描述来引导遗忘过程，无需额外的数据集或监督微调。</li>
<li>该框架提供了一种可伸缩和高效的解决方案，用于管理生成模型的行为。</li>
<li>Text-to-Unlearn首次实现了跨模态的GANs遗忘框架。</li>
<li>该框架的评价方法是基于先进的图像文本对齐度量指标进行自动评估的。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.01218">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-ce7efed644f29823794ac569b2343bc0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-314f1d89fb12216a96e360c7bfb5fa7e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5fe9c1683527b12a7c4c12aeb2691c2b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-7a3ef7b00893ade8c040e80318e82c73.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c9292e701a6915ddc2aca931bd67329a.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-422d801be0aac01f8bb39eb64f2e6abf.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-28943ea8a45a22e30fa7a6639aa783f0.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Neural-Pruning-for-3D-Scene-Reconstruction-Efficient-NeRF-Acceleration"><a href="#Neural-Pruning-for-3D-Scene-Reconstruction-Efficient-NeRF-Acceleration" class="headerlink" title="Neural Pruning for 3D Scene Reconstruction: Efficient NeRF Acceleration"></a>Neural Pruning for 3D Scene Reconstruction: Efficient NeRF Acceleration</h2><p><strong>Authors:Tianqi Ding, Dawei Xiang, Pablo Rivas, Liang Dong</strong></p>
<p>Neural Radiance Fields (NeRF) have become a popular 3D reconstruction approach in recent years. While they produce high-quality results, they also demand lengthy training times, often spanning days. This paper studies neural pruning as a strategy to address these concerns. We compare pruning approaches, including uniform sampling, importance-based methods, and coreset-based techniques, to reduce the model size and speed up training. Our findings show that coreset-driven pruning can achieve a 50% reduction in model size and a 35% speedup in training, with only a slight decrease in accuracy. These results suggest that pruning can be an effective method for improving the efficiency of NeRF models in resource-limited settings. </p>
<blockquote>
<p>神经辐射场（NeRF）近年来已成为流行的3D重建方法。虽然它们能产生高质量的结果，但也需要长时间的训练，通常持续数天。本文研究了神经网络修剪作为一种解决这些问题的策略。我们比较了修剪方法，包括均匀采样、基于重要性方法和基于核心集的技术，以减小模型大小并加快训练速度。我们的研究结果表明，基于核心集的修剪可以实现模型大小减少50%，训练速度提高35%，同时精度略有下降。这些结果表明，在资源受限的环境中，修剪是提高NeRF模型效率的有效方法。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.00950v1">PDF</a> 12 pages, 4 figures, accepted by International Conference on the AI   Revolution: Research, Ethics, and Society (AIR-RES 2025)</p>
<p><strong>Summary</strong></p>
<p>神经网络辐射场（NeRF）是近年来流行的3D重建方法，虽然能生成高质量结果，但训练时间长。本文研究神经网络修剪策略以解决问题。对比了均匀采样、基于重要性和基于核心集的修剪方法，以减小模型尺寸并加速训练。研究发现，基于核心集的修剪方法能在仅轻微降低准确性的情况下，实现模型大小减少50%，训练速度提高35%。表明修剪是改进资源受限环境中NeRF模型效率的有效方法。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>NeRF作为一种流行的3D重建方法，虽然能产生高质量结果，但训练时间长。</li>
<li>神经网络修剪是解决NeRF训练时间长的一种策略。</li>
<li>对比了多种修剪方法，包括均匀采样、基于重要性和基于核心集的修剪。</li>
<li>基于核心集的修剪方法能显著减小模型尺寸并加速训练。</li>
<li>核心集驱动的修剪能实现模型大小减少50%，训练速度提高35%。</li>
<li>修剪方法对准确性的影响较小。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.00950">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-03357584e64e5f750a3549f963e69108.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e0a0c65908f5341c99362f273efc7be2.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-0a6e94fe72a3744b86645df943d43711.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-cbab8306837a8bb33c92d1d3b371e755.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Data-Cleansing-for-GANs"><a href="#Data-Cleansing-for-GANs" class="headerlink" title="Data Cleansing for GANs"></a>Data Cleansing for GANs</h2><p><strong>Authors:Naoyuki Terashita, Hiroki Ohashi, Satoshi Hara</strong></p>
<p>As the application of generative adversarial networks (GANs) expands, it becomes increasingly critical to develop a unified approach that improves performance across various generative tasks. One effective strategy that applies to any machine learning task is identifying harmful instances, whose removal improves the performance. While previous studies have successfully estimated these harmful training instances in supervised settings, their approaches are not easily applicable to GANs. The challenge lies in two requirements of the previous approaches that do not apply to GANs. First, previous approaches require that the absence of a training instance directly affects the parameters. However, in the training for GANs, the instances do not directly affect the generator’s parameters since they are only fed into the discriminator. Second, previous approaches assume that the change in loss directly quantifies the harmfulness of the instance to a model’s performance, while common types of GAN losses do not always reflect the generative performance. To overcome the first challenge, we propose influence estimation methods that use the Jacobian of the generator’s gradient with respect to the discriminator’s parameters (and vice versa). Such a Jacobian represents the indirect effect between two models: how removing an instance from the discriminator’s training changes the generator’s parameters. Second, we propose an instance evaluation scheme that measures the harmfulness of each training instance based on how a GAN evaluation metric (e.g., Inception score) is expected to change by the instance’s removal. Furthermore, we demonstrate that removing the identified harmful instances significantly improves the generative performance on various GAN evaluation metrics. </p>
<blockquote>
<p>随着生成对抗网络（GANs）的应用不断扩大，开发一种适用于各种生成任务的统一方法变得越来越关键。对于任何机器学习任务都适用的一个有效策略是识别有害实例，去除这些实例可以提高性能。虽然以前的研究已经在有监督环境中成功地估计了这些有害的训练实例，但它们的方法并不易于应用于GANs。挑战在于之前的方法有两个不适用于GANs的要求。首先，之前的方法要求训练实例的缺失直接影响参数。然而，在GANs的训练中，实例并不会直接影响生成器的参数，因为它们只输入到判别器中。其次，之前的方法假设损失的变化直接量化实例对模型性能的损害程度，而常见的GAN损失并不总是反映生成性能。为了克服第一个挑战，我们提出了使用生成器梯度相对于判别器参数的雅可比（以及反之）进行影响估计的方法。这样的雅可比代表了两个模型之间的间接影响：即移除判别器训练中的一个实例如何改变生成器的参数。其次，我们提出了一种实例评估方案，该方案基于GAN评估指标（例如Inception分数）预期会因实例的移除而如何变化来测量每个训练实例的有害性。此外，我们证明，去除已识别出的有害实例可以显著提高各种GAN评估指标的生成性能。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.00603v1">PDF</a> Accepted for IEEE Transactions on Neural Networks and Learning   Systems (TNNLS, 2025). Journal extention of   <a target="_blank" rel="noopener" href="https://openreview.net/forum?id=opHLcXxYTC">https://openreview.net/forum?id=opHLcXxYTC</a>_</p>
<p><strong>Summary</strong></p>
<p>本文提出了一种针对生成对抗网络（GANs）的有害实例识别方法。该方法通过评估移除某个训练实例后对生成器参数的影响，来量化实例的“危害性”。采用Jacobian矩阵衡量生成器梯度与判别器参数之间的间接影响，并提出一种基于GAN评价指标（如Inception分数）变化来评估实例危害性的方案。实验证明，移除识别出的有害实例能显著提高GAN在各种评价指标上的生成性能。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>本文提出了一种针对GANs的有害实例识别方法，旨在提高其在各种生成任务中的性能。</li>
<li>现有方法难以直接应用于GANs，因为GANs中训练实例并不直接影响生成器参数。</li>
<li>通过使用Jacobian矩阵衡量生成器和判别器参数间的间接影响，来克服这一挑战。</li>
<li>提出了一种实例评估方案，该方案基于GAN评价指标（如Inception分数）的预期变化来评估每个训练实例的“危害性”。</li>
<li>这种方法不仅理论上有依据，而且实验结果显示，移除识别出的有害实例能显著提高GAN的生成性能。</li>
<li>该方法具有广泛的应用前景，可应用于其他机器学习任务中有害实例的识别与移除。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.00603">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-2e44131fb54901bb97be4d234ea012a0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7400537dee6e46df021ca29ff2e23dfe.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="LITA-GS-Illumination-Agnostic-Novel-View-Synthesis-via-Reference-Free-3D-Gaussian-Splatting-and-Physical-Priors"><a href="#LITA-GS-Illumination-Agnostic-Novel-View-Synthesis-via-Reference-Free-3D-Gaussian-Splatting-and-Physical-Priors" class="headerlink" title="LITA-GS: Illumination-Agnostic Novel View Synthesis via Reference-Free   3D Gaussian Splatting and Physical Priors"></a>LITA-GS: Illumination-Agnostic Novel View Synthesis via Reference-Free   3D Gaussian Splatting and Physical Priors</h2><p><strong>Authors:Han Zhou, Wei Dong, Jun Chen</strong></p>
<p>Directly employing 3D Gaussian Splatting (3DGS) on images with adverse illumination conditions exhibits considerable difficulty in achieving high-quality, normally-exposed representations due to: (1) The limited Structure from Motion (SfM) points estimated in adverse illumination scenarios fail to capture sufficient scene details; (2) Without ground-truth references, the intensive information loss, significant noise, and color distortion pose substantial challenges for 3DGS to produce high-quality results; (3) Combining existing exposure correction methods with 3DGS does not achieve satisfactory performance due to their individual enhancement processes, which lead to the illumination inconsistency between enhanced images from different viewpoints. To address these issues, we propose LITA-GS, a novel illumination-agnostic novel view synthesis method via reference-free 3DGS and physical priors. Firstly, we introduce an illumination-invariant physical prior extraction pipeline. Secondly, based on the extracted robust spatial structure prior, we develop the lighting-agnostic structure rendering strategy, which facilitates the optimization of the scene structure and object appearance. Moreover, a progressive denoising module is introduced to effectively mitigate the noise within the light-invariant representation. We adopt the unsupervised strategy for the training of LITA-GS and extensive experiments demonstrate that LITA-GS surpasses the state-of-the-art (SOTA) NeRF-based method while enjoying faster inference speed and costing reduced training time. The code is released at <a target="_blank" rel="noopener" href="https://github.com/LowLevelAI/LITA-GS">https://github.com/LowLevelAI/LITA-GS</a>. </p>
<blockquote>
<p>直接对不良照明条件下的图像应用三维高斯平铺（3DGS）在实现高质量的正常曝光表示方面存在相当大的困难，原因如下：（1）在不良照明场景中估计的运动结构（SfM）点有限，无法捕获足够的场景细节；（2）没有真实参考，密集的信息丢失、显著的噪声和色彩失真给3DGS带来巨大挑战，难以产生高质量的结果；（3）将现有的曝光校正方法与3DGS相结合并未实现令人满意的性能，因为它们各自的增强处理过程导致从不同视点增强的图像之间照明不一致。为了解决这些问题，我们提出了LITA-GS，这是一种新的照明无关的新视图合成方法，通过无参考的3DGS和物理先验来实现。首先，我们引入了一个光照不变物理先验提取管道。其次，基于提取的稳健空间结构先验，我们开发了光照无关的结构渲染策略，这有助于优化场景结构和对象外观。此外，引入了一个渐进的降噪模块，以有效地减轻光不变表示中的噪声。我们采用无监督策略对LITA-GS进行训练，大量实验表明，LITA-GS超越了基于NeRF的最先进方法，同时拥有更快的推理速度和更短的训练时间。代码已发布在<a target="_blank" rel="noopener" href="https://github.com/LowLevelAI/LITA-GS%E3%80%82">https://github.com/LowLevelAI/LITA-GS。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.00219v1">PDF</a> Accepted by CVPR 2025. 3DGS, Adverse illumination conditions,   Reference-free, Physical priors</p>
<p><strong>Summary</strong></p>
<p>该文本主要介绍了在不良光照条件下直接应用3D高斯贴图（3DGS）面临的挑战，并提出了解决方案。通过引入光照不变物理先验提取管道和光照无关的结构渲染策略，以及渐进式降噪模块，成功解决了因光照不佳导致的结构细节缺失、信息丢失、噪声和色彩失真等问题。所提出的方法在训练和推理速度上优于现有的NeRF方法，并已在GitHub上发布相关代码。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>在不良光照条件下直接应用3D高斯贴图（3DGS）面临挑战，包括结构细节缺失、信息丢失、噪声和色彩失真等问题。</li>
<li>提出了一种新的光照无关的新视角合成方法LITA-GS，该方法结合了无参考的3DGS和物理先验技术来解决上述问题。</li>
<li>LITA-GS引入了光照不变物理先验提取管道，以提取稳健的空间结构先验。</li>
<li>基于提取的稳健空间结构先验，开发了光照无关的结构渲染策略，优化了场景结构和物体外观。</li>
<li>LITA-GS还引入了渐进式降噪模块，有效减轻了光照不变表示中的噪声。</li>
<li>LITA-GS采用无监督策略进行训练，并在实验中表现出优于现有NeRF方法的性能，同时拥有更快的推理速度和减少的训练时间。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.00219">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-724b60f30c6aa43bb38a6f183871b706.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-17aa639b3cdddc86604f9ed53f258f16.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c37b58252ecdf8d6303c5f74971ec484.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-4f92eeb53810a3b1e93299a7488f9dee.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="ERUPT-Efficient-Rendering-with-Unposed-Patch-Transformer"><a href="#ERUPT-Efficient-Rendering-with-Unposed-Patch-Transformer" class="headerlink" title="ERUPT: Efficient Rendering with Unposed Patch Transformer"></a>ERUPT: Efficient Rendering with Unposed Patch Transformer</h2><p><strong>Authors:Maxim V. Shugaev, Vincent Chen, Maxim Karrenbach, Kyle Ashley, Bridget Kennedy, Naresh P. Cuntoor</strong></p>
<p>This work addresses the problem of novel view synthesis in diverse scenes from small collections of RGB images. We propose ERUPT (Efficient Rendering with Unposed Patch Transformer) a state-of-the-art scene reconstruction model capable of efficient scene rendering using unposed imagery. We introduce patch-based querying, in contrast to existing pixel-based queries, to reduce the compute required to render a target view. This makes our model highly efficient both during training and at inference, capable of rendering at 600 fps on commercial hardware. Notably, our model is designed to use a learned latent camera pose which allows for training using unposed targets in datasets with sparse or inaccurate ground truth camera pose. We show that our approach can generalize on large real-world data and introduce a new benchmark dataset (MSVS-1M) for latent view synthesis using street-view imagery collected from Mapillary. In contrast to NeRF and Gaussian Splatting, which require dense imagery and precise metadata, ERUPT can render novel views of arbitrary scenes with as few as five unposed input images. ERUPT achieves better rendered image quality than current state-of-the-art methods for unposed image synthesis tasks, reduces labeled data requirements by ~95% and decreases computational requirements by an order of magnitude, providing efficient novel view synthesis for diverse real-world scenes. </p>
<blockquote>
<p>本文解决了从少量RGB图像集合中合成多样场景的新视角的问题。我们提出了ERUPT（无预设补丁转换器的有效渲染）这一先进的场景重建模型，它能够利用无预设的图像进行高效场景渲染。我们引入了基于补丁的查询，与现有的基于像素的查询相比，减少了渲染目标视图所需的计算量。这使得我们的模型在训练和推理过程中都非常高效，能够在商用硬件上以600帧&#x2F;秒的速度进行渲染。值得注意的是，我们的模型采用学习潜在的相机姿态设计，这允许在数据集上使用无预设目标进行训练，即使数据集具有稀疏或不准确的真实相机姿态。我们证明了我们的方法可以在大型真实世界数据上进行推广，并使用Mapillary收集的街景图像引入了用于潜在视图合成的新基准数据集（MSVS-1M）。与需要密集图像和精确元数据的NeRF和高斯拼贴不同，ERUPT仅使用五张无预设的输入图像就可以渲染任意场景的新视角。ERUPT在无预设图像合成任务上实现了比当前最先进方法更好的渲染图像质量，将标记数据要求降低了约95％，并将计算要求降低了数量级，为多样化的真实世界场景提供了高效的新视角合成。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.24374v1">PDF</a> Accepted to CVPR 2025</p>
<p><strong>Summary</strong></p>
<p>本文提出一种称为ERUPT（基于未标定图像块的高效渲染）的场景重建模型，用于解决从少量RGB图像中合成新颖视角的问题。通过引入基于图像块的查询方法，降低了渲染目标视角所需的计算量，使模型在训练和推理时都具备高效率，可在商用硬件上以600帧&#x2F;秒的速度进行渲染。该模型设计用于学习潜在相机姿态，可在数据集上使用未标定的目标进行训练，即使数据集具有稀疏或不准确的真实相机姿态。实验表明，ERUPT方法能够在大型真实世界数据上进行推广，并引入了一个新的基准数据集MSVS-1M，用于使用来自Mapillary的街道视图图像进行潜在视图合成。与需要密集图像和精确元数据的NeRF和高斯拼贴相比，ERUPT仅需少数未标定的输入图像即可合成新颖视角。ERUPT在未经标定的图像合成任务上实现了比当前先进技术更好的渲染图像质量，将标记数据需求减少了约95％，并将计算需求降低了一个数量级，为多样化的真实世界场景提供了高效的新颖视角合成。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ERUPT模型解决了从少量RGB图像中进行新颖视角合成的问题。</li>
<li>通过引入基于图像块的查询方法，提高了渲染效率。</li>
<li>模型具备高效的学习能力，可在训练和推理时快速渲染。</li>
<li>ERUPT可以在商业硬件上以高达600帧&#x2F;秒的速度进行渲染。</li>
<li>模型设计用于学习潜在相机姿态，适应于数据集上未标定的目标。</li>
<li>引入新的基准数据集MSVS-1M，用于潜在视图合成。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.24374">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-ca11a43b27495e28893890b44f8abb7e.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-f5bda767db22f2952901ef41d0f05e35.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-81bcaa3ca5eff325183266b91206f977.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-c2449e77485b4db1e24f8a3a2589fa36.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Style-Quantization-for-Data-Efficient-GAN-Training"><a href="#Style-Quantization-for-Data-Efficient-GAN-Training" class="headerlink" title="Style Quantization for Data-Efficient GAN Training"></a>Style Quantization for Data-Efficient GAN Training</h2><p><strong>Authors:Jian Wang, Xin Lan, Jizhe Zhou, Yuxin Tian, Jiancheng Lv</strong></p>
<p>Under limited data setting, GANs often struggle to navigate and effectively exploit the input latent space. Consequently, images generated from adjacent variables in a sparse input latent space may exhibit significant discrepancies in realism, leading to suboptimal consistency regularization (CR) outcomes. To address this, we propose \textit{SQ-GAN}, a novel approach that enhances CR by introducing a style space quantization scheme. This method transforms the sparse, continuous input latent space into a compact, structured discrete proxy space, allowing each element to correspond to a specific real data point, thereby improving CR performance. Instead of direct quantization, we first map the input latent variables into a less entangled &#96;&#96;style’’ space and apply quantization using a learnable codebook. This enables each quantized code to control distinct factors of variation. Additionally, we optimize the optimal transport distance to align the codebook codes with features extracted from the training data by a foundation model, embedding external knowledge into the codebook and establishing a semantically rich vocabulary that properly describes the training dataset. Extensive experiments demonstrate significant improvements in both discriminator robustness and generation quality with our method. </p>
<blockquote>
<p>在有限数据设置下，生成对抗网络（GANs）通常难以导航并有效地利用输入潜在空间。因此，在稀疏输入潜在空间中从相邻变量生成的图像在逼真度方面可能存在显著差异，导致次优的一致性正则化（CR）结果。为了解决这一问题，我们提出了SQ-GAN，这是一种通过引入风格空间量化方案来增强CR的新方法。该方法将稀疏、连续的输入潜在空间转换为一个紧凑、结构化的离散代理空间，使每个元素都能对应一个特定的真实数据点，从而提高CR性能。我们不是直接进行量化，而是首先将输入潜在变量映射到一个不那么纠缠的“风格”空间，并使用可学习的代码本进行量化。这使得每个量化代码能够控制不同的变异因素。此外，我们优化了最优传输距离，将代码本代码与基础模型从训练数据中提取的特征进行对齐，将外部知识嵌入到代码本中，并建立语义丰富的词汇表，恰当地描述训练数据集。大量实验表明，我们的方法在判别器鲁棒性和生成质量方面都有显著提高。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.24282v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文提出一种名为SQ-GAN的新方法，通过引入风格空间量化方案改进一致性正则化（CR）。该方法将稀疏、连续的输入潜在空间转化为紧凑、结构化的离散代理空间，使每个元素对应一个真实数据点，从而提高CR性能。实验证明，该方法在判别器稳健性和生成质量方面有明显改进。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>GANs在有限数据设置下面临导航和有效探索输入潜在空间的挑战。</li>
<li>稀疏输入潜在空间中，相邻变量生成的图像在真实性上可能存在显著差异，导致CR结果不理想。</li>
<li>SQ-GAN通过引入风格空间量化方案改进CR性能。</li>
<li>该方法将稀疏、连续的输入潜在空间转化为紧凑、结构化的离散代理空间，每个元素对应一个真实数据点。</li>
<li>使用可学习的代码本进行量化，而非直接量化，使每个量化代码控制不同的变量变化因素。</li>
<li>通过优化传输距离，将代码本代码与基础模型从训练数据中提取的特征对齐，融入外部知识，建立语义丰富的词汇表，描述训练数据集。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.24282">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-20f7a357be35bf0e24ef0897b3f51b03.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6b991e157764a9b779fa1b915f2277ca.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="Strain-distribution-in-GaN-AlN-superlattices-grown-on-AlN-sapphire-templates-comparison-of-X-ray-diffraction-and-photoluminescence-studies"><a href="#Strain-distribution-in-GaN-AlN-superlattices-grown-on-AlN-sapphire-templates-comparison-of-X-ray-diffraction-and-photoluminescence-studies" class="headerlink" title="Strain distribution in GaN&#x2F;AlN superlattices grown on AlN&#x2F;sapphire   templates: comparison of X-ray diffraction and photoluminescence studies"></a>Strain distribution in GaN&#x2F;AlN superlattices grown on AlN&#x2F;sapphire   templates: comparison of X-ray diffraction and photoluminescence studies</h2><p><strong>Authors:Aleksandra Wierzbicka, Agata Kaminska, Kamil Sobczak, Dawid Jankowski, Kamil Koronski, Pawel Strak, Marta Sobanska, Zbigniew R. Zytkiewicz</strong></p>
<p>Series of GaN&#x2F;AlN superlattices (SLs) with various periods and the same thicknesses of GaN quantum wells and AlN barriers have been investigated. X-ray diffraction, photoluminescence (PL) and transmission electron microscopy (TEM) techniques were used to study the influence of thickness of AlN and GaN sublayers on strain distribution in GaN&#x2F;AlN SL structures. Detailed X-ray diffraction measurements demonstrate that the strain occurring in SLs generally decreases with an increase of well&#x2F;barrier thickness. Fitting of X-ray diffraction curves allowed determining the real thicknesses of the GaN wells and AlN barriers. Since blurring of the interfaces causes deviation of calculated data from experimental results the quality of the interfaces has been evaluated as well and compared with results of TEM measurements. For the samples with thinner wells&#x2F;barriers the presence of pin-holes and threading dislocations has been observed in TEM measurements. The best quality of interfaces has been found for the sample with a well&#x2F;barrier thickness of 3 nm. Finally, PL spectra showed that due to Quantum-Confined Stark Effect the PL peak energies of the SLs decreased with increasing the width of the GaN quantum wells and AlN barriers. The effect is well modelled by ab initio calculations based on the density functional theory applied for tetragonally strained structures of the same geometry using a full tensorial representation of the strain in the SLs. </p>
<blockquote>
<p>已经研究了具有不同周期和相同GaN量子阱和AlN势垒厚度的GaN&#x2F;AlN超晶格（SL）系列。使用X射线衍射、光致发光（PL）和透射电子显微镜（TEM）技术，研究了AlN和GaN亚层厚度对GaN&#x2F;AlN SL结构中应变分布的影响。详细的X射线衍射测量表明，SLs中的应变通常随着阱&#x2F;势垒厚度的增加而减小。通过拟合X射线衍射曲线，可以确定GaN阱和AlN势垒的实际厚度。由于界面模糊导致计算数据与实验结果存在偏差，因此还对界面质量进行了评估，并与TEM测量结果进行了比较。在较薄阱&#x2F;势垒的样品中，通过TEM测量观察到存在针孔和穿线错位。在阱&#x2F;势垒厚度为3nm的样品中发现了最佳界面质量。最后，PL光谱显示，由于量子限制斯塔克效应，SLs的PL峰能量随着GaN量子阱和AlN势垒宽度的增加而降低。该效应通过基于密度泛函理论的第一性原理计算得到了很好的模拟，该理论应用于具有相同几何形状的四边形应变结构，并使用SLs中应变的全张量表示。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.22294v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>该文研究了不同周期的GaN&#x2F;AlN超晶格（SLs）结构，并采用了X射线衍射、光致发光和透射电子显微镜等技术，探讨了GaN和AlN子层厚度对GaN&#x2F;AlN超晶格应变分布的影响。研究发现，随着阱&#x2F;势垒厚度的增加，超晶格中的应变一般会减小。此外，通过X射线衍射曲线的拟合确定了GaN阱和AlN势垒的实际厚度。同时，对界面质量进行了评估并与透射电子显微镜的测量结果进行了比较。对于较薄的阱&#x2F;势垒样品，透射电子显微镜观察到存在针孔和贯穿位错。在阱&#x2F;势垒厚度为3nm的样品中获得了最佳界面质量。最后，光致发光光谱表明，由于量子受限斯塔克效应，随着GaN量子阱和AlN势垒宽度的增加，SLs的PL峰能量降低。这一效应通过基于密度泛函理论的从头算法对具有相同几何形状的四边形应变结构进行模拟得到了很好的验证。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>GaN&#x2F;AlN超晶格（SLs）的应变分布受到子层厚度的影响。</li>
<li>随着阱&#x2F;势垒厚度的增加，超晶格中的应变一般会减小。</li>
<li>X射线衍射被用于确定GaN阱和AlN势垒的实际厚度。</li>
<li>界面质量是影响超晶格性能的重要因素，已对其进行了评估和比较。</li>
<li>较薄的阱&#x2F;势垒样品中观察到针孔和贯穿位错。</li>
<li>最佳界面质量是在阱&#x2F;势垒厚度为3nm的样品中发现的。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.22294">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-17d727948c34a80e619067de4b3d016b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b8583095db588e171c5422232ba83dc1.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="ABC-GS-Alignment-Based-Controllable-Style-Transfer-for-3D-Gaussian-Splatting"><a href="#ABC-GS-Alignment-Based-Controllable-Style-Transfer-for-3D-Gaussian-Splatting" class="headerlink" title="ABC-GS: Alignment-Based Controllable Style Transfer for 3D Gaussian   Splatting"></a>ABC-GS: Alignment-Based Controllable Style Transfer for 3D Gaussian   Splatting</h2><p><strong>Authors:Wenjie Liu, Zhongliang Liu, Xiaoyan Yang, Man Sha, Yang Li</strong></p>
<p>3D scene stylization approaches based on Neural Radiance Fields (NeRF) achieve promising results by optimizing with Nearest Neighbor Feature Matching (NNFM) loss. However, NNFM loss does not consider global style information. In addition, the implicit representation of NeRF limits their fine-grained control over the resulting scenes. In this paper, we introduce ABC-GS, a novel framework based on 3D Gaussian Splatting to achieve high-quality 3D style transfer. To this end, a controllable matching stage is designed to achieve precise alignment between scene content and style features through segmentation masks. Moreover, a style transfer loss function based on feature alignment is proposed to ensure that the outcomes of style transfer accurately reflect the global style of the reference image. Furthermore, the original geometric information of the scene is preserved with the depth loss and Gaussian regularization terms. Extensive experiments show that our ABC-GS provides controllability of style transfer and achieves stylization results that are more faithfully aligned with the global style of the chosen artistic reference. Our homepage is available at <a target="_blank" rel="noopener" href="https://vpx-ecnu.github.io/ABC-GS-website">https://vpx-ecnu.github.io/ABC-GS-website</a>. </p>
<blockquote>
<p>基于神经辐射场（NeRF）的3D场景风格化方法通过最近邻特征匹配（NNFM）损失进行优化，取得了有前景的结果。然而，NNFM损失并没有考虑全局风格信息。此外，NeRF的隐式表示限制了其对结果场景的细粒度控制。在本文中，我们介绍了ABC-GS，一个基于3D高斯喷涂的新型框架，以实现高质量的三维风格转移。为此，设计了一个可控的匹配阶段，通过分割掩膜实现场景内容与风格特征的精确对齐。此外，提出了一种基于特征对齐的风格转移损失函数，以确保风格转移的结果准确反映参考图像的全局风格。此外，通过深度损失和高斯正则化项保留了场景的原几何信息。大量实验表明，我们的ABC-GS提供了风格转移的控制能力，并实现了与所选艺术参考的全局风格更忠实对齐的风格化结果。我们的主页可在<a target="_blank" rel="noopener" href="https://vpx-ecnu.github.io/ABC-GS-website%E8%AE%BF%E9%97%AE%E3%80%82">https://vpx-ecnu.github.io/ABC-GS-website访问。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.22218v1">PDF</a> 10 pages, 14 figures</p>
<p><strong>Summary</strong><br>基于NeRF技术的三维场景风格化方法引入了一种新型框架ABC-GS，结合三维高斯描图实现高质量的三维风格转移。该框架设计了一个可控匹配阶段，通过分割掩膜实现场景内容与风格特征的精确对齐。同时，提出了一种基于特征对齐的风格转移损失函数，确保风格转移结果准确反映参考图像的整体风格。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>利用NeRF技术的三维场景风格化方法取得显著成果。</li>
<li>NNFM损失不考虑全局风格信息，新型框架ABC-GS解决此问题。</li>
<li>ABC-GS采用三维高斯描图技术实现高质量三维风格转移。</li>
<li>可控匹配阶段设计用于精确对齐场景内容与风格特征。</li>
<li>基于特征对齐的风格转移损失函数确保反映参考图像的整体风格。</li>
<li>保留原始场景几何信息，通过深度损失和高斯正则化项实现。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.22218">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-c150b593acd3ad8af8bbbc67d19774d6.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-3f0a34dcf472a776172956adfe1b8f8f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-283a61d43de86c60c11763eb2b0d954f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b80b6c3426d1ed93e72af69a2318f832.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7f89d45b2378711221b582477857f78d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a9fc0b4d14604ddcbec4769021841d33.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="RainyGS-Efficient-Rain-Synthesis-with-Physically-Based-Gaussian-Splatting"><a href="#RainyGS-Efficient-Rain-Synthesis-with-Physically-Based-Gaussian-Splatting" class="headerlink" title="RainyGS: Efficient Rain Synthesis with Physically-Based Gaussian   Splatting"></a>RainyGS: Efficient Rain Synthesis with Physically-Based Gaussian   Splatting</h2><p><strong>Authors:Qiyu Dai, Xingyu Ni, Qianfan Shen, Wenzheng Chen, Baoquan Chen, Mengyu Chu</strong></p>
<p>We consider the problem of adding dynamic rain effects to in-the-wild scenes in a physically-correct manner. Recent advances in scene modeling have made significant progress, with NeRF and 3DGS techniques emerging as powerful tools for reconstructing complex scenes. However, while effective for novel view synthesis, these methods typically struggle with challenging scene editing tasks, such as physics-based rain simulation. In contrast, traditional physics-based simulations can generate realistic rain effects, such as raindrops and splashes, but they often rely on skilled artists to carefully set up high-fidelity scenes. This process lacks flexibility and scalability, limiting its applicability to broader, open-world environments. In this work, we introduce RainyGS, a novel approach that leverages the strengths of both physics-based modeling and 3DGS to generate photorealistic, dynamic rain effects in open-world scenes with physical accuracy. At the core of our method is the integration of physically-based raindrop and shallow water simulation techniques within the fast 3DGS rendering framework, enabling realistic and efficient simulations of raindrop behavior, splashes, and reflections. Our method supports synthesizing rain effects at over 30 fps, offering users flexible control over rain intensity – from light drizzles to heavy downpours. We demonstrate that RainyGS performs effectively for both real-world outdoor scenes and large-scale driving scenarios, delivering more photorealistic and physically-accurate rain effects compared to state-of-the-art methods. Project page can be found at <a target="_blank" rel="noopener" href="https://pku-vcl-geometry.github.io/RainyGS/">https://pku-vcl-geometry.github.io/RainyGS/</a> </p>
<blockquote>
<p>我们考虑以物理正确的方式给自然场景添加动态雨水效果的问题。最近场景建模方面的进展已经取得了重大突破，NeRF和3DGS技术作为重建复杂场景的强大工具而崭露头角。然而，虽然这些方法在合成新视角方面很有效，但它们通常面临具有挑战性的场景编辑任务，如基于物理的雨水模拟。相比之下，传统的基于物理的模拟可以产生逼真的雨水效果，如雨滴和飞溅，但它们通常依赖于熟练的艺术家仔细设置高保真场景。这个过程缺乏灵活性和可扩展性，限制了其在更广泛、开放世界环境中的适用性。在这项工作中，我们介绍了RainyGS，这是一种新的方法，它结合了基于物理的建模和3DGS的优势，在开放世界场景中生成具有物理精度的逼真动态雨水效果。我们方法的核心是在快速的3DGS渲染框架内整合基于物理的雨滴和浅水模拟技术，能够逼真、高效地模拟雨滴行为、飞溅和反射。我们的方法支持以超过30帧&#x2F;秒的速度合成雨水效果，为用户提供灵活的雨水强度控制——从轻微的小雨到倾盆大雨。我们证明RainyGS在真实户外场景和大规模驾驶场景中都能有效表现，与最新方法相比，它提供了更逼真和更物理准确的雨水效果。项目页面可在<a target="_blank" rel="noopener" href="https://pku-vcl-geometry.github.io/RainyGS/%E6%89%BE%E5%88%B0%E3%80%82">https://pku-vcl-geometry.github.io/RainyGS/找到。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.21442v2">PDF</a> CVPR 2025</p>
<p><strong>Summary</strong></p>
<p>本文提出一种名为RainyGS的新方法，结合了物理建模和3DGS技术，可在开放世界场景中生成具有物理准确性的高逼真的动态雨水效果。该方法融合了基于物理的雨滴和浅水模拟技术，在快速的3DGS渲染框架内进行，能模拟雨滴行为、溅水和反射等。RainyGS支持以超过30帧的速度合成雨水效果，并允许用户灵活控制雨水强度。相较于现有方法，RainyGS在真实户外场景和大规模驾驶场景中有更逼真和物理准确的雨水效果。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>本文介绍了在野外场景中添加动态雨水效果的难题，并指出传统方法依赖于熟练艺术家进行高保真场景的细致设置，缺乏灵活性和可扩展性。</li>
<li>提出了名为RainyGS的新方法，结合了物理建模和3DGS技术来生成逼真的动态雨水效果。</li>
<li>RainyGS集成了基于物理的雨滴和浅水模拟技术，在快速3DGS渲染框架内进行，实现真实且高效的雨水模拟。</li>
<li>RainyGS支持高帧率（超过30fps）的雨水效果合成，用户可灵活控制雨水强度。</li>
<li>RainyGS在真实户外场景和大规模驾驶场景中表现出更逼真和物理准确的雨水效果。</li>
<li>RainyGS方法适用于开放世界环境，具有广泛的应用潜力。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.21442">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-0c3b19f41ae58db25446cd92dae747c1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d92736acb1d72a0482a1e4739ae0fadb.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2540fe3d87f579b141ef35602ce18620.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="LandMarkSystem-Technical-Report"><a href="#LandMarkSystem-Technical-Report" class="headerlink" title="LandMarkSystem Technical Report"></a>LandMarkSystem Technical Report</h2><p><strong>Authors:Zhenxiang Ma, Zhenyu Yang, Miao Tao, Yuanzhen Zhou, Zeyu He, Yuchang Zhang, Rong Fu, Hengjie Li</strong></p>
<p>3D reconstruction is vital for applications in autonomous driving, virtual reality, augmented reality, and the metaverse. Recent advancements such as Neural Radiance Fields(NeRF) and 3D Gaussian Splatting (3DGS) have transformed the field, yet traditional deep learning frameworks struggle to meet the increasing demands for scene quality and scale. This paper introduces LandMarkSystem, a novel computing framework designed to enhance multi-scale scene reconstruction and rendering. By leveraging a componentized model adaptation layer, LandMarkSystem supports various NeRF and 3DGS structures while optimizing computational efficiency through distributed parallel computing and model parameter offloading. Our system addresses the limitations of existing frameworks, providing dedicated operators for complex 3D sparse computations, thus facilitating efficient training and rapid inference over extensive scenes. Key contributions include a modular architecture, a dynamic loading strategy for limited resources, and proven capabilities across multiple representative algorithms.This comprehensive solution aims to advance the efficiency and effectiveness of 3D reconstruction tasks.To facilitate further research and collaboration, the source code and documentation for the LandMarkSystem project are publicly available in an open-source repository, accessing the repository at: <a target="_blank" rel="noopener" href="https://github.com/InternLandMark/LandMarkSystem">https://github.com/InternLandMark/LandMarkSystem</a>. </p>
<blockquote>
<p>三维重建在自动驾驶、虚拟现实、增强现实和元宇宙等应用中具有重要意义。最近的进展，如神经网络辐射场（NeRF）和三维高斯喷涂（3DGS），已经改变了这一领域，但传统的深度学习框架难以满足对场景质量和规模日益增长的需求。本文介绍了LandMarkSystem，这是一种新型计算框架，旨在提高多尺度场景重建和渲染。通过利用组件化模型适配层，LandMarkSystem支持各种NeRF和3DGS结构，同时通过分布式并行计算和模型参数卸载优化计算效率。我们的系统解决了现有框架的局限性，为复杂的三维稀疏计算提供了专用操作符，从而实现了大规模场景的高效训练和快速推理。主要贡献包括模块化架构、有限资源的动态加载策略以及在多个代表性算法中的证明能力。这个全面的解决方案旨在提高三维重建任务的效率和效果。为促进进一步的研究和合作，LandMarkSystem项目的源代码和文档已在开源仓库中提供，可通过以下链接访问：<a target="_blank" rel="noopener" href="https://github.com/InternLandMark/LandMarkSystem%E3%80%82">https://github.com/InternLandMark/LandMarkSystem。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.21364v2">PDF</a> </p>
<p><strong>Summary</strong><br>基于NeRF技术和组件化模型适配层，LandMarkSystem框架旨在提高多尺度场景重建和渲染的效率和质量。通过分布式并行计算和模型参数卸载优化计算效率，解决了现有框架的限制，为复杂的三维稀疏计算提供专用操作符，旨在推进三维重建任务的效率和效果。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LandMarkSystem利用NeRF技术和组件化模型适配层来增强多尺度场景重建。</li>
<li>该框架通过分布式并行计算和模型参数卸载优化计算效率。</li>
<li>LandMarkSystem解决了现有框架在处理复杂三维稀疏计算方面的限制。</li>
<li>它提供了一个模块化架构和动态加载策略以应对有限的资源。</li>
<li>LandMarkSystem已在多个代表性算法中证明了其能力。</li>
<li>该框架旨在提高三维重建任务的效率和效果。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.21364">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-6cab89666ecae86575af634424164eee.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="UGNA-VPR-A-Novel-Training-Paradigm-for-Visual-Place-Recognition-Based-on-Uncertainty-Guided-NeRF-Augmentation"><a href="#UGNA-VPR-A-Novel-Training-Paradigm-for-Visual-Place-Recognition-Based-on-Uncertainty-Guided-NeRF-Augmentation" class="headerlink" title="UGNA-VPR: A Novel Training Paradigm for Visual Place Recognition Based   on Uncertainty-Guided NeRF Augmentation"></a>UGNA-VPR: A Novel Training Paradigm for Visual Place Recognition Based   on Uncertainty-Guided NeRF Augmentation</h2><p><strong>Authors:Yehui Shen, Lei Zhang, Qingqiu Li, Xiongwei Zhao, Yue Wang, Huimin Lu, Xieyuanli Chen</strong></p>
<p>Visual place recognition (VPR) is crucial for robots to identify previously visited locations, playing an important role in autonomous navigation in both indoor and outdoor environments. However, most existing VPR datasets are limited to single-viewpoint scenarios, leading to reduced recognition accuracy, particularly in multi-directional driving or feature-sparse scenes. Moreover, obtaining additional data to mitigate these limitations is often expensive. This paper introduces a novel training paradigm to improve the performance of existing VPR networks by enhancing multi-view diversity within current datasets through uncertainty estimation and NeRF-based data augmentation. Specifically, we initially train NeRF using the existing VPR dataset. Then, our devised self-supervised uncertainty estimation network identifies places with high uncertainty. The poses of these uncertain places are input into NeRF to generate new synthetic observations for further training of VPR networks. Additionally, we propose an improved storage method for efficient organization of augmented and original training data. We conducted extensive experiments on three datasets and tested three different VPR backbone networks. The results demonstrate that our proposed training paradigm significantly improves VPR performance by fully utilizing existing data, outperforming other training approaches. We further validated the effectiveness of our approach on self-recorded indoor and outdoor datasets, consistently demonstrating superior results. Our dataset and code have been released at \href{<a target="_blank" rel="noopener" href="https://github.com/nubot-nudt/UGNA-VPR%7D%7Bhttps://github.com/nubot-nudt/UGNA-VPR%7D">https://github.com/nubot-nudt/UGNA-VPR}{https://github.com/nubot-nudt/UGNA-VPR}</a>. </p>
<blockquote>
<p>视觉位置识别（VPR）对于机器人识别先前访问过的位置至关重要，在室内和室外环境的自主导航中扮演着重要角色。然而，大多数现有的VPR数据集仅限于单视点场景，导致识别精度降低，特别是在多方向驾驶或特征稀疏的场景中。此外，获取额外数据以缓解这些限制通常成本高昂。本文引入了一种新的训练范式，以提高现有VPR网络的性能。我们通过不确定性估计和基于NeRF的数据增强，增强当前数据集内的多视图多样性。具体来说，我们最初使用现有的VPR数据集训练NeRF。然后，我们设计的自监督不确定性估计网络识别出不确定性较高的位置。这些不确定位置的姿态被输入到NeRF中，以生成新的合成观察结果，用于进一步训练VPR网络。此外，我们还提出了一种改进的数据存储方法，以有效地组织增强和原始训练数据。我们在三个数据集上进行了广泛的实验，并测试了三种不同的VPR骨干网络。结果表明，我们提出的训练范式充分利用了现有数据，显著提高了VPR的性能，超越了其他训练方法。我们在自制的室内和室外数据集上进一步验证了我们的方法的有效性，始终表现出卓越的结果。我们的数据集和代码已发布在<a target="_blank" rel="noopener" href="https://github.com/nubot-nudt/UGNA-VPR">https://github.com/nubot-nudt/UGNA-VPR</a>。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.21338v1">PDF</a> Accepted to IEEE Robotics and Automation Letters (RA-L)</p>
<p><strong>Summary</strong></p>
<p>本文提出了一种新的训练范式，通过利用不确定度估计和基于NeRF的数据增强技术，增强现有VPR网络的多视角多样性，提高其在多方向驾驶和特征稀疏场景中的识别准确率。通过NeRF使用现有VPR数据集进行初始训练，然后利用自主设计的自监督不确定度估计网络识别高不确定度地点，将这些地点的姿态输入NeRF生成新的合成观察结果，用于进一步训练VPR网络。同时，提出了一种改进的数据存储方法，以更有效地组织增强和原始训练数据。实验结果表明，该训练范式能充分利用现有数据，显著提高VPR性能。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>引入了一种新的训练范式，旨在提高VPR网络在多种场景中的识别性能。</li>
<li>利用不确定度估计技术识别高不确定度地点，通过NeRF生成新的合成观察结果。</li>
<li>提出了一个自监督不确定度估计网络，用于辅助生成合成观察数据。</li>
<li>采用了一种改进的数据存储方法，以便更有效地组织增强和原始训练数据。</li>
<li>通过三个数据集和三种不同的VPR骨干网络进行的实验表明，新训练范式显著提高了VPR性能。</li>
<li>在自主录制的室内和室外数据集上验证了方法的有效性，均取得了优异结果。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.21338">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-2d64fae156815e3a3aeec368229be729.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d535b9c8da77d7048c470fa9913f5b84.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-45e66ba7990fba5569f0f1451df6dfa0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-bad45da56d9377a5101018b25da7ffb3.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="NeRFPrior-Learning-Neural-Radiance-Field-as-a-Prior-for-Indoor-Scene-Reconstruction"><a href="#NeRFPrior-Learning-Neural-Radiance-Field-as-a-Prior-for-Indoor-Scene-Reconstruction" class="headerlink" title="NeRFPrior: Learning Neural Radiance Field as a Prior for Indoor Scene   Reconstruction"></a>NeRFPrior: Learning Neural Radiance Field as a Prior for Indoor Scene   Reconstruction</h2><p><strong>Authors:Wenyuan Zhang, Emily Yue-ting Jia, Junsheng Zhou, Baorui Ma, Kanle Shi, Yu-Shen Liu, Zhizhong Han</strong></p>
<p>Recently, it has shown that priors are vital for neural implicit functions to reconstruct high-quality surfaces from multi-view RGB images. However, current priors require large-scale pre-training, and merely provide geometric clues without considering the importance of color. In this paper, we present NeRFPrior, which adopts a neural radiance field as a prior to learn signed distance fields using volume rendering for surface reconstruction. Our NeRF prior can provide both geometric and color clues, and also get trained fast under the same scene without additional data. Based on the NeRF prior, we are enabled to learn a signed distance function (SDF) by explicitly imposing a multi-view consistency constraint on each ray intersection for surface inference. Specifically, at each ray intersection, we use the density in the prior as a coarse geometry estimation, while using the color near the surface as a clue to check its visibility from another view angle. For the textureless areas where the multi-view consistency constraint does not work well, we further introduce a depth consistency loss with confidence weights to infer the SDF. Our experimental results outperform the state-of-the-art methods under the widely used benchmarks. </p>
<blockquote>
<p>最近，研究显示先验知识对于神经隐式函数从多视角RGB图像重建高质量表面至关重要。然而，当前先验知识需要大量预训练，并且仅提供几何线索，而未考虑颜色的重要性。在本文中，我们提出了NeRFPrior，它采用神经辐射场作为先验知识，使用体积渲染来学习符号距离场进行表面重建。我们的NeRF先验可以提供几何和颜色线索，并在同一场景下无需额外数据进行快速训练。基于NeRF先验，我们能够通过在每条射线交点上明确施加多视角一致性约束来学习符号距离函数（SDF），以实现表面推断。具体来说，在每条射线交点上，我们使用先验中的密度作为粗略的几何估计，同时使用接近表面的颜色作为从另一个视角检查其可见性的线索。对于纹理缺失的区域，多视角一致性约束无法很好地工作，我们进一步引入带有置信权重的深度一致性损失来推断SDF。我们的实验结果优于广泛使用的基准测试中的最新方法。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.18361v2">PDF</a> Accepted by CVPR 2025. Project page:   <a target="_blank" rel="noopener" href="https://wen-yuan-zhang.github.io/NeRFPrior/">https://wen-yuan-zhang.github.io/NeRFPrior/</a></p>
<p><strong>Summary</strong></p>
<p>本文提出了NeRFPrior方法，利用神经辐射场作为先验来学习使用体积渲染的表面重建中的符号距离场。NeRFPrior不仅提供几何线索，还考虑颜色重要性，无需额外数据即可在同一场景下进行快速训练。通过明确对每条射线交点施加多视角一致性约束，实现表面推断的符号距离函数（SDF）学习。在纹理缺失区域，引入带有置信权重的深度一致性损失来推断SDF，实验结果优于广泛使用的基准测试中的最新方法。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>NeRFPrior利用神经辐射场作为先验，用于表面重建中的符号距离场学习。</li>
<li>该方法不仅能提供几何线索，还考虑颜色的重要性。</li>
<li>NeRFPrior在同一场景下进行快速训练，无需额外数据。</li>
<li>通过明确对每条射线交点施加多视角一致性约束，实现符号距离函数（SDF）学习。</li>
<li>在纹理缺失区域，引入深度一致性损失以辅助表面推断。</li>
<li>NeRFPrior的实验结果优于广泛使用的基准测试中的最新方法。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.18361">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-dced1b075b30f7831c4352987fd1efeb.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-329c32e673c33ffb395103bf6fad45f9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-529d9fe25e668a950d63460ca52e07fd.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a736b9c84b1f2095dd212bde8fde2f75.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2894203a097369e7df754db7fa9dca19.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-22dcbf7ed51c78afe9d29df43db16aba.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="Scalable-Real2Sim-Physics-Aware-Asset-Generation-Via-Robotic-Pick-and-Place-Setups"><a href="#Scalable-Real2Sim-Physics-Aware-Asset-Generation-Via-Robotic-Pick-and-Place-Setups" class="headerlink" title="Scalable Real2Sim: Physics-Aware Asset Generation Via Robotic   Pick-and-Place Setups"></a>Scalable Real2Sim: Physics-Aware Asset Generation Via Robotic   Pick-and-Place Setups</h2><p><strong>Authors:Nicholas Pfaff, Evelyn Fu, Jeremy Binagia, Phillip Isola, Russ Tedrake</strong></p>
<p>Simulating object dynamics from real-world perception shows great promise for digital twins and robotic manipulation but often demands labor-intensive measurements and expertise. We present a fully automated Real2Sim pipeline that generates simulation-ready assets for real-world objects through robotic interaction. Using only a robot’s joint torque sensors and an external camera, the pipeline identifies visual geometry, collision geometry, and physical properties such as inertial parameters. Our approach introduces a general method for extracting high-quality, object-centric meshes from photometric reconstruction techniques (e.g., NeRF, Gaussian Splatting) by employing alpha-transparent training while explicitly distinguishing foreground occlusions from background subtraction. We validate the full pipeline through extensive experiments, demonstrating its effectiveness across diverse objects. By eliminating the need for manual intervention or environment modifications, our pipeline can be integrated directly into existing pick-and-place setups, enabling scalable and efficient dataset creation. Project page (with code and data): <a target="_blank" rel="noopener" href="https://scalable-real2sim.github.io/">https://scalable-real2sim.github.io/</a>. </p>
<blockquote>
<p>通过从真实世界感知模拟物体动态，数字孪生和机器人操作展现出了巨大的潜力，但这通常需要密集劳动的测量和专业知识。我们提出了一种全自动的Real2Sim管道，通过机器人交互为真实世界对象生成模拟就绪资产。仅使用机器人的关节扭矩传感器和外部摄像头，该管道就能识别视觉几何、碰撞几何以及物理属性，如惯性参数。我们的方法通过采用alpha透明训练，引入了一种从光度重建技术（例如NeRF、高斯平板印刷）中提取高质量、以物体为中心的网格的一般方法，同时明确区分前景遮挡和背景减法。我们通过广泛的实验验证了整个管道的有效性，证明了它在各种对象上的有效性。通过消除对人工干预或环境修改的需求，我们的管道可以直接集成到现有的拾取和放置设置中，从而实现可扩展和高效的数据库创建。项目页面（包含代码和数据）：<a target="_blank" rel="noopener" href="https://scalable-real2sim.github.io/%E3%80%82">https://scalable-real2sim.github.io/。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.00370v2">PDF</a> Website: <a target="_blank" rel="noopener" href="https://scalable-real2sim.github.io/">https://scalable-real2sim.github.io/</a></p>
<p><strong>Summary</strong></p>
<p>本摘要介绍了全自动化的Real2Sim管道技术，该技术通过机器人交互为真实世界物体生成模拟仿真资产。仅使用机器人的关节扭矩传感器和外部相机，该技术就能识别物体的视觉几何、碰撞几何和物理属性，如惯性参数。该方法采用alpha透明训练法，结合NeRF等光度重建技术，提取高质量的对象中心网格，并通过前景遮挡与背景减影的明确区分来实现。实验验证表明，该方法在多种对象上均有效。它能直接集成现有的拾取和放置设置，无需人工干预或环境修改，从而实现可扩展和高效的数据库创建。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>介绍了一种全自动化的Real2Sim管道技术，用于生成模拟仿真资产，用于数字双胞胎和机器人操作。</li>
<li>仅使用机器人的关节扭矩传感器和外部相机，就能识别物体的视觉几何、碰撞几何和物理属性。</li>
<li>采用alpha透明训练法结合NeRF等光度重建技术，实现高质量的对象中心网格提取。</li>
<li>明确区分前景遮挡和背景减影，提高了技术的效能。</li>
<li>实验验证表明，该方法在多种对象上均有效。</li>
<li>该技术能直接集成现有的拾取和放置设置，无需人工干预或环境修改。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.00370">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-2035f6c4be743de83b6b6c3884690b27.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-ba7a1307374e31e9533da7b441b4c2b4.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ffc53775a0f4dbe213849bd232a6e089.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-8bf64c8b4433f1f326ffa94b2421a4db.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ca287e62ba891a0564adb80d7d49c127.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="ActiveGAMER-Active-GAussian-Mapping-through-Efficient-Rendering"><a href="#ActiveGAMER-Active-GAussian-Mapping-through-Efficient-Rendering" class="headerlink" title="ActiveGAMER: Active GAussian Mapping through Efficient Rendering"></a>ActiveGAMER: Active GAussian Mapping through Efficient Rendering</h2><p><strong>Authors:Liyan Chen, Huangying Zhan, Kevin Chen, Xiangyu Xu, Qingan Yan, Changjiang Cai, Yi Xu</strong></p>
<p>We introduce ActiveGAMER, an active mapping system that utilizes 3D Gaussian Splatting (3DGS) to achieve high-quality, real-time scene mapping and exploration. Unlike traditional NeRF-based methods, which are computationally demanding and restrict active mapping performance, our approach leverages the efficient rendering capabilities of 3DGS, allowing effective and efficient exploration in complex environments. The core of our system is a rendering-based information gain module that dynamically identifies the most informative viewpoints for next-best-view planning, enhancing both geometric and photometric reconstruction accuracy. ActiveGAMER also integrates a carefully balanced framework, combining coarse-to-fine exploration, post-refinement, and a global-local keyframe selection strategy to maximize reconstruction completeness and fidelity. Our system autonomously explores and reconstructs environments with state-of-the-art geometric and photometric accuracy and completeness, significantly surpassing existing approaches in both aspects. Extensive evaluations on benchmark datasets such as Replica and MP3D highlight ActiveGAMER’s effectiveness in active mapping tasks. </p>
<blockquote>
<p>我们介绍了ActiveGAMER，这是一个利用3D高斯拼接（3DGS）技术的主动映射系统，可实现高质量、实时的场景映射和探索。与传统的基于NeRF的方法不同，这些方法计算量大，限制了主动映射的性能，我们的方法利用3DGS的高效渲染能力，能够在复杂环境中实现有效且高效的探索。我们的系统的核心是基于渲染的信息增益模块，该模块能够动态识别最具信息量的观点，用于进行下一次最佳视角规划，提高几何和光度重建的精度。ActiveGAMER还整合了一个精心设计的框架，结合了从粗到细的探索、后处理和全局局部关键帧选择策略，以最大化重建的完整性和逼真度。我们的系统以最先进的几何和光度准确性和完整性自主探索和重建环境，在各方面都大大超越了现有方法。在Replica和MP3D等基准数据集上的广泛评估凸显了ActiveGAMER在主动映射任务中的有效性。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.06897v2">PDF</a> Accepted to CVPR2025</p>
<p><strong>Summary</strong><br>ActiveGAMER是一种利用3D高斯拼贴技术实现高质量实时场景映射和探索的主动映射系统。它采用高效渲染技术，克服传统NeRF方法计算量大、影响主动映射性能的缺点，在复杂环境中实现高效探索。其核心是一个基于渲染的信息增益模块，能动态识别最具信息量的视角进行下一步最佳视角规划，提高几何和光度重建精度。ActiveGAMER结合粗到细探索、后优化和全局局部关键帧选择策略，最大化重建的完整性和保真度。系统在几何和光度准确性和完整性方面达到领先水平，显著超越现有方法。在如Replica和MP3D等基准数据集上的评估证明了ActiveGAMER在主动映射任务中的有效性。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ActiveGAMER系统利用3D高斯拼贴技术实现高效渲染，提高场景映射和探索的质量。</li>
<li>与传统NeRF方法相比，ActiveGAMER具有更高的计算效率，能够克服计算量大对主动映射性能的影响。</li>
<li>ActiveGAMER通过动态识别最具信息量的视角，提高几何和光度重建的精度。</li>
<li>系统结合粗到细探索、后优化和全局局部关键帧选择策略，以最大化重建的完整性和保真度。</li>
<li>ActiveGAMER在几何和光度准确性和完整性方面达到领先水平。</li>
<li>在基准数据集上的评估证明了ActiveGAMER在主动映射任务中的有效性。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.06897">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-3ee2068cd514667fdf8590d3278fd8d4.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-6ec14eb4cb933ec03aae06000294635e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-50bac8affe1e91cfbfd2b6b121faa689.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-39039f9725c9caaa76b25907768bed3e.jpg" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="Light-Transport-aware-Diffusion-Posterior-Sampling-for-Single-View-Reconstruction-of-3D-Volumes"><a href="#Light-Transport-aware-Diffusion-Posterior-Sampling-for-Single-View-Reconstruction-of-3D-Volumes" class="headerlink" title="Light Transport-aware Diffusion Posterior Sampling for Single-View   Reconstruction of 3D Volumes"></a>Light Transport-aware Diffusion Posterior Sampling for Single-View   Reconstruction of 3D Volumes</h2><p><strong>Authors:Ludwic Leonard, Nils Thuerey, Ruediger Westermann</strong></p>
<p>We introduce a single-view reconstruction technique of volumetric fields in which multiple light scattering effects are omnipresent, such as in clouds. We model the unknown distribution of volumetric fields using an unconditional diffusion model trained on a novel benchmark dataset comprising 1,000 synthetically simulated volumetric density fields. The neural diffusion model is trained on the latent codes of a novel, diffusion-friendly, monoplanar representation. The generative model is used to incorporate a tailored parametric diffusion posterior sampling technique into different reconstruction tasks. A physically-based differentiable volume renderer is employed to provide gradients with respect to light transport in the latent space. This stands in contrast to classic NeRF approaches and makes the reconstructions better aligned with observed data. Through various experiments, we demonstrate single-view reconstruction of volumetric clouds at a previously unattainable quality. </p>
<blockquote>
<p>我们介绍了一种体积场单视图重建技术，该技术普遍存在多次散射效应，例如在云中。我们使用无条件扩散模型对体积场的未知分布进行建模，该模型是在由1000个合成模拟体积密度场组成的新型基准数据集上进行训练的。神经扩散模型是在一种新型的、适合扩散的单平面表示的潜在代码上进行训练的。生成模型被用于将定制的参数化扩散后采样技术融入不同的重建任务中。采用基于物理的可微体积渲染器，以提供关于潜在空间中光传输的梯度。这与经典的NeRF方法形成对比，使重建与观测数据更好地对齐。通过一系列实验，我们展示了体积云的单视图重建达到了前所未有的质量。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.05226v3">PDF</a> CVPR 2025</p>
<p><strong>Summary</strong></p>
<p>本文介绍了一种单视图重建技术，该技术能够模拟在云等场景中的多次散射效应。通过采用新型基准数据集，对体积场分布进行建模并使用无条件扩散模型进行训练。该模型基于一种新型的扩散友好型单平面表示方法的潜在代码进行训练。采用定制的参数化扩散后采样技术融入不同的重建任务中，并采用基于物理的可微分体积渲染器来提供关于潜在空间的光传输梯度。与经典NeRF方法相比，这种方法使重建结果更加符合观测数据，并通过实验实现了体积云的单视图重建，达到了前所未有的质量。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>引入了一种单视图重建技术，能够模拟多次散射效应，适用于云等场景。</li>
<li>采用新型基准数据集对体积场分布进行建模，并使用无条件扩散模型训练。</li>
<li>利用扩散友好型单平面表示方法的潜在代码进行训练。</li>
<li>采用定制的参数化扩散后采样技术融入重建任务中。</li>
<li>采用基于物理的可微分体积渲染器提供关于潜在空间的光传输梯度。</li>
<li>该方法使重建结果更符合观测数据。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.05226">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-11566dd50cc743473d60ef3912138912.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a06d3ba44d73061de32f149c95dd8d86.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-7503e4f9aa5d5234ea882157ec7087e5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-765075193dfa447cd0ad9bd7a3954830.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b9be618087546de4ff8ec70027a91ab5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-43b74ad375be4fbaa283ef2166a6bff1.jpg" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="Self-Calibrating-4D-Novel-View-Synthesis-from-Monocular-Videos-Using-Gaussian-Splatting"><a href="#Self-Calibrating-4D-Novel-View-Synthesis-from-Monocular-Videos-Using-Gaussian-Splatting" class="headerlink" title="Self-Calibrating 4D Novel View Synthesis from Monocular Videos Using   Gaussian Splatting"></a>Self-Calibrating 4D Novel View Synthesis from Monocular Videos Using   Gaussian Splatting</h2><p><strong>Authors:Fang Li, Hao Zhang, Narendra Ahuja</strong></p>
<p>Gaussian Splatting (GS) has significantly elevated scene reconstruction efficiency and novel view synthesis (NVS) accuracy compared to Neural Radiance Fields (NeRF), particularly for dynamic scenes. However, current 4D NVS methods, whether based on GS or NeRF, primarily rely on camera parameters provided by COLMAP and even utilize sparse point clouds generated by COLMAP for initialization, which lack accuracy as well are time-consuming. This sometimes results in poor dynamic scene representation, especially in scenes with large object movements, or extreme camera conditions e.g. small translations combined with large rotations. Some studies simultaneously optimize the estimation of camera parameters and scenes, supervised by additional information like depth, optical flow, etc. obtained from off-the-shelf models. Using this unverified information as ground truth can reduce robustness and accuracy, which does frequently occur for long monocular videos (with e.g. &gt; hundreds of frames). We propose a novel approach that learns a high-fidelity 4D GS scene representation with self-calibration of camera parameters. It includes the extraction of 2D point features that robustly represent 3D structure, and their use for subsequent joint optimization of camera parameters and 3D structure towards overall 4D scene optimization. We demonstrate the accuracy and time efficiency of our method through extensive quantitative and qualitative experimental results on several standard benchmarks. The results show significant improvements over state-of-the-art methods for 4D novel view synthesis. The source code will be released soon at <a target="_blank" rel="noopener" href="https://github.com/fangli333/SC-4DGS">https://github.com/fangli333/SC-4DGS</a>. </p>
<blockquote>
<p>基于高斯绘制技术（GS）的场景重建效率和新颖视角合成（NVS）精度相较于神经网络辐射场（NeRF）有显著提升，特别是对于动态场景而言。然而，当前的四维NVS方法，无论是基于GS还是NeRF，主要依赖于COLMAP提供的相机参数，甚至使用COLMAP生成的稀疏点云进行初始化，这既缺乏准确性又耗时。这有时会导致对动态场景的表示不佳，特别是在物体移动较大的场景或极端相机条件下（例如小平移与大旋转结合的情况）。一些研究同时优化相机参数的估计和场景，通过现成的模型获取深度、光流等额外信息进行监督。使用未经验证的信息作为真实数据可能会降低稳健性和准确性，这在针对具有数百帧的长单眼视频时经常发生。我们提出了一种新型方法，该方法采用自校准相机参数的方式学习高保真四维GS场景表示。它包含提取稳健代表三维结构的二维点特征，并将其用于联合优化相机参数和三维结构，以实现整体的四维场景优化。我们通过多个标准基准的定量和定性实验结果，展示了该方法的准确性和时间效率。结果显示，相较于四维新颖视角合成领域的最先进方法有明显的改进。源代码很快将在[<a target="_blank" rel="noopener" href="https://github.com/fangli333/SC-4DGS%E5%8F%91%E5%B8%83%E3%80%82]">https://github.com/fangli333/SC-4DGS发布。]</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2406.01042v3">PDF</a> GitHub Page: <a target="_blank" rel="noopener" href="https://github.com/fangli333/SC-4DGS">https://github.com/fangli333/SC-4DGS</a></p>
<p><strong>Summary</strong><br>     本文提出一种基于高斯混成技术（Gaussian Splatting, GS）的自我校准4D场景表示方法。该方法能高效准确地重建场景并合成新视角，尤其适用于动态场景。通过提取稳健的2D特征点，联合优化相机参数和3D结构，达到整体的4D场景优化。在多个标准基准测试中表现优越。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>高斯混成技术（GS）相较于神经网络辐射场（NeRF）提升了场景重建效率和新型视角合成（NVS）的准确性，特别是在动态场景中。</li>
<li>当前4D NVS方法主要依赖COLMAP提供的相机参数，并用于初始化稀疏点云，但这种方法准确性和效率较低。</li>
<li>文中提到的方法利用提取的2D特征点来稳健地表示3D结构，随后对相机参数和3D结构进行联合优化，实现自我校准的4D场景表示。</li>
<li>文中方法显著提高了4D新型视角合成的准确性，并在多个标准测试中表现优越。</li>
<li>所提出的方法适用于大型对象移动、极端相机条件（如小平移与大旋转结合）等复杂场景。</li>
<li>目前的方法依赖于额外的信息（如深度、光流等）来监督相机参数和场景的估计，这可能降低稳健性和准确性，特别是在长单目视频（如包含数百帧）中。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2406.01042">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-aeb8b6be9ed822de16b6ab6a254689b4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-943f2448f91afb60a72f6a93466398e1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-46743dd4fd6de272e99deea3ad94b4ac.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-cb752928248c98233389d6a68f5cbb84.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-94b1a6297fd9bb1be9cdc3fbe53fc163.jpg" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1><h2 id="MixRT-Mixed-Neural-Representations-For-Real-Time-NeRF-Rendering"><a href="#MixRT-Mixed-Neural-Representations-For-Real-Time-NeRF-Rendering" class="headerlink" title="MixRT: Mixed Neural Representations For Real-Time NeRF Rendering"></a>MixRT: Mixed Neural Representations For Real-Time NeRF Rendering</h2><p><strong>Authors:Chaojian Li, Bichen Wu, Peter Vajda, Yingyan Celine Lin</strong></p>
<p>Neural Radiance Field (NeRF) has emerged as a leading technique for novel view synthesis, owing to its impressive photorealistic reconstruction and rendering capability. Nevertheless, achieving real-time NeRF rendering in large-scale scenes has presented challenges, often leading to the adoption of either intricate baked mesh representations with a substantial number of triangles or resource-intensive ray marching in baked representations. We challenge these conventions, observing that high-quality geometry, represented by meshes with substantial triangles, is not necessary for achieving photorealistic rendering quality. Consequently, we propose MixRT, a novel NeRF representation that includes a low-quality mesh, a view-dependent displacement map, and a compressed NeRF model. This design effectively harnesses the capabilities of existing graphics hardware, thus enabling real-time NeRF rendering on edge devices. Leveraging a highly-optimized WebGL-based rendering framework, our proposed MixRT attains real-time rendering speeds on edge devices (over 30 FPS at a resolution of 1280 x 720 on a MacBook M1 Pro laptop), better rendering quality (0.2 PSNR higher in indoor scenes of the Unbounded-360 datasets), and a smaller storage size (less than 80% compared to state-of-the-art methods). </p>
<blockquote>
<p>神经辐射场（NeRF）已经成为新型视角合成的主流技术，因其令人印象深刻的逼真重建和渲染能力。然而，在大规模场景中实现实时NeRF渲染提出了挑战，通常导致采用复杂的预烘焙网格表示法，其中包含大量的三角形，或者采用预烘焙表示法中的资源密集型光线追踪。我们质疑这些传统观念，观察到高质量几何体（由大量三角形表示的网格）对于实现逼真渲染质量并非必要。因此，我们提出了MixRT，这是一种新型的NeRF表示方法，它包含低质量网格、视差相关位移图和压缩NeRF模型。这种设计有效地利用了现有图形硬件的功能，从而在边缘设备上实现了实时NeRF渲染。通过基于高度优化的WebGL渲染框架，我们提出的MixRT在边缘设备上实现了实时渲染速度（在MacBook M1 Pro笔记本电脑上以1280 x 720的分辨率超过30 FPS），更好的渲染质量（在Unbounded-360数据集室内场景中高出0.2 PSNR），并且存储大小更小（与最先进的方法相比减少了不到80%）。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2312.11841v5">PDF</a> Accepted by 3DV’24. Project Page: <a target="_blank" rel="noopener" href="https://licj15.github.io/MixRT/">https://licj15.github.io/MixRT/</a></p>
<p><strong>Summary</strong></p>
<p>本文介绍了NeRF技术在新型视图合成方面的前沿应用，提出一种名为MixRT的NeRF表示方法，通过结合低质量网格、视角相关位移图和压缩NeRF模型，实现了对大规模场景的实时渲染。利用优化的WebGL渲染框架，MixRT在边缘设备上实现了实时渲染速度，同时提高了渲染质量和存储效率。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>NeRF技术已成为新型视图合成的领先技术，具有惊人的照片级重建和渲染能力。</li>
<li>实现大规模场景实时NeRF渲染具有挑战，通常采用复杂的烘焙网格表示或资源密集型的射线追踪方法。</li>
<li>本文挑战了上述常规方法，指出高质量几何网格并非实现照片级渲染质量的必要条件。</li>
<li>提出了MixRT，一种结合低质量网格、视角相关位移图和压缩NeRF模型的NeRF表示方法。</li>
<li>MixRT有效利用现有图形硬件，使实时NeRF渲染在边缘设备上成为可能。</li>
<li>利用高度优化的WebGL渲染框架，MixRT实现了边缘设备上的实时渲染速度，并在Unbound-360数据集室内场景中获得更高的渲染质量。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2312.11841">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-c34dfa8d80e73c73f189bda7be9230e0.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-44d66c65781dd7094b22b4ed21552bd3.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-16ddd4478152e5c6a4f02bc6b86875f0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-729c2c4e7c400fc17f0e1faacd580d64.jpg" align="middle">
</details>


<h1 id="-19"><a href="#-19" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        文章作者:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        文章链接:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-04-03/NeRF/">https://kedreamix.github.io/Talk2Paper/Paper/2025-04-03/NeRF/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        版权声明:
                    </i>
                </span>
                <span class="reprint-info">
                    本博客所有文章除特別声明外，均采用
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    许可协议。转载请注明来源
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/NeRF/">
                                    <span class="chip bg-color">NeRF</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;上一篇</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-04-04/NeRF/">
                    <div class="card-image">
                        
                        <img src="https://pica.zhimg.com/v2-592a6485d72b21b72232c5a2ce2bb71f.jpg" class="responsive-img" alt="NeRF">
                        
                        <span class="card-title">NeRF</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            NeRF 方向最新论文已更新，请持续关注 Update in 2025-04-04  Diffusion-Guided Gaussian Splatting for Large-Scale Unconstrained 3D   Reconstruction and Novel View Synthesis
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-04-04
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/NeRF/" class="post-category">
                                    NeRF
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/NeRF/">
                        <span class="chip bg-color">NeRF</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                下一篇&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-04-04/3DGS/">
                    <div class="card-image">
                        
                        <img src="https://pica.zhimg.com/v2-0cb003298d1d12e0a7c9c740f48cff60.jpg" class="responsive-img" alt="3DGS">
                        
                        <span class="card-title">3DGS</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            3DGS 方向最新论文已更新，请持续关注 Update in 2025-04-04  Diffusion-Guided Gaussian Splatting for Large-Scale Unconstrained 3D   Reconstruction and Novel View Synthesis
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-04-04
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/3DGS/" class="post-category">
                                    3DGS
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/3DGS/">
                        <span class="chip bg-color">3DGS</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- 代码块功能依赖 -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- 代码语言 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- 代码块复制 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- 代码块收缩 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC 悬浮按钮. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* 修复文章卡片 div 的宽度. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // 切换TOC目录展开收缩的相关操作.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;站点总字数:&nbsp;<span
                        class="white-color">26254.1k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- 运行天数提醒. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // 区分是否有年份.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = '本站已运行 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                daysTip = '本站已運行 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = '本站已运行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = '本站已運行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="关注我的知乎: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">知</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- 搜索遮罩框 -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span>
            <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- 白天和黑夜主题 -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- 回到顶部按钮 -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // 只在桌面版网页启用特效
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- 雪花特效 -->
    

    <!-- 鼠标星星特效 -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--腾讯兔小巢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- 动态标签 -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Σ(っ °Д °;)っ诶，页面崩溃了嘛？", clearTimeout(st)) : (document.title = "φ(゜▽゜*)♪咦，又好了！", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
