<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="æ£€æµ‹/åˆ†å‰²/è·Ÿè¸ª">
    <meta name="description" content="æ£€æµ‹/åˆ†å‰²/è·Ÿè¸ª æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-04-04  v-CLR View-Consistent Learning for Open-World Instance Segmentation">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>æ£€æµ‹/åˆ†å‰²/è·Ÿè¸ª | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-95b2d0fb29b18389b1c180b7a3c48137.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">æ£€æµ‹/åˆ†å‰²/è·Ÿè¸ª</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/%E6%A3%80%E6%B5%8B-%E5%88%86%E5%89%B2-%E8%B7%9F%E8%B8%AA/">
                                <span class="chip bg-color">æ£€æµ‹/åˆ†å‰²/è·Ÿè¸ª</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/%E6%A3%80%E6%B5%8B-%E5%88%86%E5%89%B2-%E8%B7%9F%E8%B8%AA/" class="post-category">
                                æ£€æµ‹/åˆ†å‰²/è·Ÿè¸ª
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-04-04
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-04-04
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    19.9k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    82 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-04-04-æ›´æ–°"><a href="#2025-04-04-æ›´æ–°" class="headerlink" title="2025-04-04 æ›´æ–°"></a>2025-04-04 æ›´æ–°</h1><h2 id="v-CLR-View-Consistent-Learning-for-Open-World-Instance-Segmentation"><a href="#v-CLR-View-Consistent-Learning-for-Open-World-Instance-Segmentation" class="headerlink" title="v-CLR: View-Consistent Learning for Open-World Instance Segmentation"></a>v-CLR: View-Consistent Learning for Open-World Instance Segmentation</h2><p><strong>Authors:Chang-Bin Zhang, Jinhong Ni, Yujie Zhong, Kai Han</strong></p>
<p>In this paper, we address the challenging problem of open-world instance segmentation. Existing works have shown that vanilla visual networks are biased toward learning appearance information, \eg texture, to recognize objects. This implicit bias causes the model to fail in detecting novel objects with unseen textures in the open-world setting. To address this challenge, we propose a learning framework, called view-Consistent LeaRning (v-CLR), which aims to enforce the model to learn appearance-invariant representations for robust instance segmentation. In v-CLR, we first introduce additional views for each image, where the texture undergoes significant alterations while preserving the imageâ€™s underlying structure. We then encourage the model to learn the appearance-invariant representation by enforcing the consistency between object features across different views, for which we obtain class-agnostic object proposals using off-the-shelf unsupervised models that possess strong object-awareness. These proposals enable cross-view object feature matching, greatly reducing the appearance dependency while enhancing the object-awareness. We thoroughly evaluate our method on public benchmarks under both cross-class and cross-dataset settings, achieving state-of-the-art performance. Project page: <a target="_blank" rel="noopener" href="https://visual-ai.github.io/vclr">https://visual-ai.github.io/vclr</a> </p>
<blockquote>
<p>æœ¬æ–‡è§£å†³äº†å¼€æ”¾ä¸–ç•Œå®ä¾‹åˆ†å‰²çš„æŒ‘æˆ˜æ€§é—®é¢˜ã€‚ç°æœ‰å·¥ä½œè¡¨æ˜ï¼Œæ™®é€šè§†è§‰ç½‘ç»œåœ¨å­¦ä¹ è¯†åˆ«ç‰©ä½“æ—¶åå‘äºå¤–è§‚ä¿¡æ¯ï¼Œä¾‹å¦‚çº¹ç†ã€‚è¿™ç§éšæ€§åè§å¯¼è‡´æ¨¡å‹åœ¨å¼€æ”¾ä¸–ç•Œç¯å¢ƒä¸­æ£€æµ‹å…·æœ‰æœªè§çº¹ç†çš„æ–°ç‰©ä½“æ—¶å¤±è´¥ã€‚ä¸ºäº†è§£å†³è¿™ä¸€æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§å­¦ä¹ æ¡†æ¶ï¼Œç§°ä¸ºè§†å›¾ä¸€è‡´æ€§å­¦ä¹ ï¼ˆv-CLRï¼‰ï¼Œæ—¨åœ¨å¼ºåˆ¶æ¨¡å‹å­¦ä¹ é²æ£’çš„å¤–è§‚ä¸å˜è¡¨ç¤ºæ¥è¿›è¡Œç¨³å¥çš„å®ä¾‹åˆ†å‰²ã€‚åœ¨v-CLRä¸­ï¼Œæˆ‘ä»¬é¦–å…ˆä¸ºæ¯å¼ å›¾åƒå¼•å…¥é¢å¤–çš„è§†å›¾ï¼Œå…¶ä¸­çº¹ç†ç»å†äº†æ˜¾è‘—çš„å˜åŒ–ï¼ŒåŒæ—¶ä¿ç•™äº†å›¾åƒçš„åŸºæœ¬ç»“æ„ã€‚ç„¶åï¼Œæˆ‘ä»¬é€šè¿‡å¼ºåˆ¶ä¸åŒè§†å›¾ä¹‹é—´å¯¹è±¡ç‰¹å¾çš„ä¸€è‡´æ€§ï¼Œé¼“åŠ±æ¨¡å‹å­¦ä¹ å¤–è§‚ä¸å˜è¡¨ç¤ºã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬ä½¿ç”¨ç°æˆçš„å…·æœ‰å¼ºçƒˆå¯¹è±¡æ„è¯†çš„éç›‘ç£æ¨¡å‹è·å¾—ç±»æ— å…³çš„å¯¹è±¡ææ¡ˆã€‚è¿™äº›ææ¡ˆå®ç°äº†è·¨è§†å›¾å¯¹è±¡ç‰¹å¾åŒ¹é…ï¼Œå¤§å¤§é™ä½äº†å¤–è§‚ä¾èµ–æ€§ï¼ŒåŒæ—¶å¢å¼ºäº†å¯¹è±¡æ„è¯†ã€‚æˆ‘ä»¬åœ¨å…¬å…±åŸºå‡†æµ‹è¯•ä¸Šå½»åº•è¯„ä¼°äº†æˆ‘ä»¬çš„æ–¹æ³•ï¼Œåœ¨è·¨ç±»å’Œè·¨æ•°æ®é›†è®¾ç½®ä¸‹å‡å®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚é¡¹ç›®é¡µé¢ï¼š<a target="_blank" rel="noopener" href="https://visual-ai.github.io/vclr">https://visual-ai.github.io/vclr</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.01383v1">PDF</a> Accepted by CVPR 2025, Project page:   <a target="_blank" rel="noopener" href="https://visual-ai.github.io/vclr">https://visual-ai.github.io/vclr</a>, Code: <a target="_blank" rel="noopener" href="https://github.com/Visual-AI/vCLR">https://github.com/Visual-AI/vCLR</a></p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡è§£å†³å¼€æ”¾ä¸–ç•Œå®ä¾‹åˆ†å‰²çš„æŒ‘æˆ˜æ€§é—®é¢˜ã€‚ç°æœ‰è§†è§‰ç½‘ç»œå€¾å‘äºå­¦ä¹ å¤–è§‚ä¿¡æ¯ï¼ˆå¦‚çº¹ç†ï¼‰æ¥è¯†åˆ«ç‰©ä½“ï¼Œå¯¼è‡´æ¨¡å‹åœ¨æ£€æµ‹å¼€æ”¾ä¸–ç•Œä¸­æœªè§çº¹ç†çš„æ–°ç‰©ä½“æ—¶å¤±è´¥ã€‚ä¸ºæ­¤ï¼Œæœ¬æ–‡æå‡ºä¸€ç§å­¦ä¹ æ¡†æ¶â€”â€”è§†å›¾ä¸€è‡´æ€§å­¦ä¹ ï¼ˆv-CLRï¼‰ï¼Œæ—¨åœ¨å¼ºåˆ¶æ¨¡å‹å­¦ä¹ é²æ£’çš„ã€å¯¹å¤–è§‚å˜åŒ–çš„è¡¨ç¤ºï¼Œä»¥å®ç°ç¨³å¥çš„å®ä¾‹åˆ†å‰²ã€‚v-CLRé€šè¿‡å¼•å…¥å›¾åƒçš„ä¸åŒè§†å›¾ï¼Œåœ¨ä¿æŒå›¾åƒåº•å±‚ç»“æ„çš„åŒæ—¶ï¼Œä½¿çº¹ç†å‘ç”Ÿæ˜¾è‘—å˜åŒ–ã€‚ç„¶åï¼Œé€šè¿‡åœ¨ä¸åŒè§†å›¾é—´ä¿æŒå¯¹è±¡ç‰¹å¾çš„ä¸€è‡´æ€§ï¼Œé¼“åŠ±æ¨¡å‹å­¦ä¹ å¤–è§‚ä¸å˜çš„è¡¨ç¤ºã€‚åˆ©ç”¨é€šç”¨çš„æ— ç›‘ç£æ¨¡å‹ç”Ÿæˆç±»æ— å…³çš„å¯¹è±¡ææ¡ˆï¼Œè¿™äº›ææ¡ˆå…·æœ‰å¼ºçƒˆçš„å¯¹è±¡æ„è¯†ï¼Œèƒ½å¤Ÿå®ç°è·¨è§†å›¾çš„å¯¹è±¡ç‰¹å¾åŒ¹é…ï¼Œä»è€Œæå¤§åœ°å‡å°‘äº†å¤–è§‚ä¾èµ–æ€§ï¼Œæé«˜äº†å¯¹è±¡æ„è¯†ã€‚åœ¨å…¬å…±åŸºå‡†æµ‹è¯•ä¸Šçš„è¯„ä¼°ç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨è·¨ç±»å’Œè·¨æ•°æ®é›†è®¾ç½®ä¸‹å‡è¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æœ¬æ–‡è§£å†³å¼€æ”¾ä¸–ç•Œå®ä¾‹åˆ†å‰²ä¸­çš„æŒ‘æˆ˜ï¼ŒæŒ‡å‡ºç°æœ‰è§†è§‰ç½‘ç»œå€¾å‘äºå­¦ä¹ å¤–è§‚ä¿¡æ¯ï¼Œå¯¼è‡´åœ¨æ–°ç¯å¢ƒä¸‹è¯†åˆ«å¤±è´¥ã€‚</li>
<li>æå‡ºä¸€ç§åä¸ºv-CLRçš„å­¦ä¹ æ¡†æ¶ï¼Œé€šè¿‡å¼•å…¥ä¸åŒè§†å›¾æ¥å­¦ä¹ å¤–è§‚ä¸å˜çš„è¡¨ç¤ºï¼Œä»¥æé«˜æ¨¡å‹çš„é²æ£’æ€§ã€‚</li>
<li>v-CLRåœ¨ä¿æŒå›¾åƒåº•å±‚ç»“æ„çš„åŒæ—¶ï¼Œä½¿çº¹ç†å‘ç”Ÿæ˜¾è‘—å˜åŒ–ã€‚</li>
<li>åˆ©ç”¨æ— ç›‘ç£æ¨¡å‹ç”Ÿæˆç±»æ— å…³çš„å¯¹è±¡ææ¡ˆï¼Œå®ç°è·¨è§†å›¾çš„å¯¹è±¡ç‰¹å¾åŒ¹é…ã€‚</li>
<li>è¯¥æ–¹æ³•å‡å°‘äº†å¤–è§‚ä¾èµ–æ€§ï¼Œæé«˜äº†å¯¹è±¡æ„è¯†ã€‚</li>
<li>åœ¨å…¬å…±åŸºå‡†æµ‹è¯•ä¸Šçš„è¯„ä¼°ç»“æœè¡¨æ˜ï¼Œv-CLRåœ¨è·¨ç±»å’Œè·¨æ•°æ®é›†è®¾ç½®ä¸‹å‡è¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.01383">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-991106673539b8e67c536577fb6c01d2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9addfe67db8b11369a2151e9a8e0b6ea.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-8edd2ef9599d2178a983f2d2cb0ec396.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1455938aed866e7293785cb348ad9f17.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="CFMD-Dynamic-Cross-layer-Feature-Fusion-for-Salient-Object-Detection"><a href="#CFMD-Dynamic-Cross-layer-Feature-Fusion-for-Salient-Object-Detection" class="headerlink" title="CFMD: Dynamic Cross-layer Feature Fusion for Salient Object Detection"></a>CFMD: Dynamic Cross-layer Feature Fusion for Salient Object Detection</h2><p><strong>Authors:Jin Lian, Zhongyu Wan, Ming Gao, JunFeng Chen</strong></p>
<p>Cross-layer feature pyramid networks (CFPNs) have achieved notable progress in multi-scale feature fusion and boundary detail preservation for salient object detection. However, traditional CFPNs still suffer from two core limitations: (1) a computational bottleneck caused by complex feature weighting operations, and (2) degraded boundary accuracy due to feature blurring in the upsampling process. To address these challenges, we propose CFMD, a novel cross-layer feature pyramid network that introduces two key innovations. First, we design a context-aware feature aggregation module (CFLMA), which incorporates the state-of-the-art Mamba architecture to construct a dynamic weight distribution mechanism. This module adaptively adjusts feature importance based on image context, significantly improving both representation efficiency and generalization. Second, we introduce an adaptive dynamic upsampling unit (CFLMD) that preserves spatial details during resolution recovery. By adjusting the upsampling range dynamically and initializing with a bilinear strategy, the module effectively reduces feature overlap and maintains fine-grained boundary structures. Extensive experiments on three standard benchmarks using three mainstream backbone networks demonstrate that CFMD achieves substantial improvements in pixel-level accuracy and boundary segmentation quality, especially in complex scenes. The results validate the effectiveness of CFMD in jointly enhancing computational efficiency and segmentation performance, highlighting its strong potential in salient object detection tasks. </p>
<blockquote>
<p>è·¨å±‚ç‰¹å¾é‡‘å­—å¡”ç½‘ç»œï¼ˆCFPNsï¼‰åœ¨æ˜¾è‘—ç›®æ ‡æ£€æµ‹çš„å°ºåº¦ç‰¹å¾èåˆå’Œè¾¹ç•Œç»†èŠ‚ä¿ç•™æ–¹é¢å–å¾—äº†æ˜¾è‘—çš„è¿›å±•ã€‚ç„¶è€Œï¼Œä¼ ç»Ÿçš„CFPNsä»ç„¶å­˜åœ¨ä¸¤ä¸ªæ ¸å¿ƒå±€é™æ€§ï¼šï¼ˆ1ï¼‰ç”±äºå¤æ‚çš„ç‰¹å¾åŠ æƒæ“ä½œå¯¼è‡´çš„è®¡ç®—ç“¶é¢ˆï¼›ï¼ˆ2ï¼‰ç”±äºä¸Šé‡‡æ ·è¿‡ç¨‹ä¸­çš„ç‰¹å¾æ¨¡ç³Šè€Œå¯¼è‡´çš„è¾¹ç•Œç²¾åº¦ä¸‹é™ã€‚ä¸ºäº†è§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†CFMDï¼Œè¿™æ˜¯ä¸€ç§æ–°å‹è·¨å±‚ç‰¹å¾é‡‘å­—å¡”ç½‘ç»œï¼Œå®ƒå¼•å…¥äº†ä¸¤ä¸ªå…³é”®åˆ›æ–°ç‚¹ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸Šä¸‹æ–‡æ„ŸçŸ¥ç‰¹å¾èšåˆæ¨¡å—ï¼ˆCFLMAï¼‰ï¼Œå®ƒç»“åˆäº†æœ€å…ˆè¿›çš„Mambaæ¶æ„æ¥æ„å»ºåŠ¨æ€æƒé‡åˆ†å¸ƒæœºåˆ¶ã€‚è¯¥æ¨¡å—æ ¹æ®å›¾åƒä¸Šä¸‹æ–‡è‡ªé€‚åº”åœ°è°ƒæ•´ç‰¹å¾é‡è¦æ€§ï¼Œæ˜¾è‘—æé«˜äº†è¡¨å¾æ•ˆç‡å’Œæ³›åŒ–èƒ½åŠ›ã€‚å…¶æ¬¡ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ªè‡ªé€‚åº”åŠ¨æ€ä¸Šé‡‡æ ·å•å…ƒï¼ˆCFLMDï¼‰ï¼Œåœ¨åˆ†è¾¨ç‡æ¢å¤è¿‡ç¨‹ä¸­ä¿ç•™ç©ºé—´ç»†èŠ‚ã€‚é€šè¿‡åŠ¨æ€è°ƒæ•´ä¸Šé‡‡æ ·èŒƒå›´å¹¶ä½¿ç”¨åŒçº¿æ€§ç­–ç•¥è¿›è¡Œåˆå§‹åŒ–ï¼Œè¯¥æ¨¡å—æœ‰æ•ˆåœ°å‡å°‘äº†ç‰¹å¾é‡å å¹¶ä¿æŒç²¾ç»†çš„è¾¹ç•Œç»“æ„ã€‚åœ¨ä¸‰ä¸ªæ ‡å‡†åŸºå‡†æµ‹è¯•ä¸Šå¯¹ä¸‰ç§ä¸»æµéª¨å¹²ç½‘ç»œçš„å¤§é‡å®éªŒè¡¨æ˜ï¼ŒCFMDåœ¨åƒç´ çº§ç²¾åº¦å’Œè¾¹ç•Œåˆ†å‰²è´¨é‡æ–¹é¢å–å¾—äº†æ˜¾è‘—æé«˜ï¼Œå°¤å…¶æ˜¯åœ¨å¤æ‚åœºæ™¯ä¸­ã€‚ç»“æœéªŒè¯äº†CFMDåœ¨æé«˜è®¡ç®—æ•ˆç‡å’Œåˆ†å‰²æ€§èƒ½æ–¹é¢çš„æœ‰æ•ˆæ€§ï¼Œçªæ˜¾äº†å…¶åœ¨æ˜¾è‘—ç›®æ ‡æ£€æµ‹ä»»åŠ¡ä¸­çš„å¼ºå¤§æ½œåŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.01326v1">PDF</a> </p>
<p><strong>Summary</strong>ï¼š</p>
<p>CFMDç½‘ç»œè§£å†³äº†ä¼ ç»Ÿç‰¹å¾é‡‘å­—å¡”ç½‘ç»œåœ¨è®¡ç®—ç“¶é¢ˆå’Œè¾¹ç•Œç²¾åº¦ä¸‹é™çš„é—®é¢˜ã€‚é€šè¿‡å¼•å…¥ä¸Šä¸‹æ–‡æ„ŸçŸ¥ç‰¹å¾èšåˆæ¨¡å—å’Œè‡ªé€‚åº”åŠ¨æ€ä¸Šé‡‡æ ·å•å…ƒï¼ŒCFMDç½‘ç»œæé«˜äº†å¤šå°ºåº¦ç‰¹å¾èåˆå’Œè¾¹ç•Œç»†èŠ‚ä¿ç•™çš„æ•ˆæœï¼Œæ˜¾è‘—æå‡äº†æ˜¾è‘—æ€§ç›®æ ‡æ£€æµ‹çš„åƒç´ çº§ç²¾åº¦å’Œè¾¹ç•Œåˆ†å‰²è´¨é‡ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>CFMDç½‘ç»œé’ˆå¯¹ä¼ ç»Ÿç‰¹å¾é‡‘å­—å¡”ç½‘ç»œçš„ä¸¤ä¸ªæ ¸å¿ƒé—®é¢˜è¿›è¡Œäº†æ”¹è¿›ï¼šè®¡ç®—ç“¶é¢ˆå’Œè¾¹ç•Œç²¾åº¦ä¸‹é™ã€‚</li>
<li>CFMDç½‘ç»œå¼•å…¥äº†ä¸Šä¸‹æ–‡æ„ŸçŸ¥ç‰¹å¾èšåˆæ¨¡å—ï¼ˆCFLMAï¼‰ï¼Œç»“åˆMambaæ¶æ„æ„å»ºåŠ¨æ€æƒé‡åˆ†å¸ƒæœºåˆ¶ï¼Œè‡ªé€‚åº”è°ƒæ•´ç‰¹å¾é‡è¦æ€§ã€‚</li>
<li>CFMDç½‘ç»œé€šè¿‡è‡ªé€‚åº”åŠ¨æ€ä¸Šé‡‡æ ·å•å…ƒï¼ˆCFLMDï¼‰åœ¨åˆ†è¾¨ç‡æ¢å¤è¿‡ç¨‹ä¸­ä¿ç•™ç©ºé—´ç»†èŠ‚ï¼Œæœ‰æ•ˆå‡å°‘ç‰¹å¾é‡å å¹¶ä¿æŒç²¾ç»†çš„è¾¹ç•Œç»“æ„ã€‚</li>
<li>CFMDç½‘ç»œåœ¨ä¸‰ä¸ªæ ‡å‡†åŸºå‡†æµ‹è¯•ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼Œå…¶åƒç´ çº§ç²¾åº¦å’Œè¾¹ç•Œåˆ†å‰²è´¨é‡æœ‰æ˜¾è‘—æé«˜ï¼Œå°¤å…¶åœ¨å¤æ‚åœºæ™¯ä¸­è¡¨ç°æ›´ä¼˜ç§€ã€‚</li>
<li>CFMDç½‘ç»œè”åˆæé«˜äº†è®¡ç®—æ•ˆç‡å’Œåˆ†å‰²æ€§èƒ½ï¼Œå‡¸æ˜¾äº†å…¶åœ¨æ˜¾è‘—æ€§ç›®æ ‡æ£€æµ‹ä»»åŠ¡ä¸­çš„å¼ºå¤§æ½œåŠ›ã€‚</li>
<li>å®éªŒç»“æœéªŒè¯äº†CFMDç½‘ç»œçš„æœ‰æ•ˆæ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.01326">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-67086a79a7838a875e9c231e3c7f2eba.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1c6c560fafd169858a85396f2886ce1d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c66c3d794a38f15bc0341fffd9f827fc.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3cd28bd9c2a2d8222f6ca5dcddb6f478.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="FSSUWNet-Mitigating-the-Fragility-of-Pre-trained-Models-with-Feature-Enhancement-for-Few-Shot-Semantic-Segmentation-in-Underwater-Images"><a href="#FSSUWNet-Mitigating-the-Fragility-of-Pre-trained-Models-with-Feature-Enhancement-for-Few-Shot-Semantic-Segmentation-in-Underwater-Images" class="headerlink" title="FSSUWNet: Mitigating the Fragility of Pre-trained Models with Feature   Enhancement for Few-Shot Semantic Segmentation in Underwater Images"></a>FSSUWNet: Mitigating the Fragility of Pre-trained Models with Feature   Enhancement for Few-Shot Semantic Segmentation in Underwater Images</h2><p><strong>Authors:Zhuohao Li, Zhicheng Huang, Wenchao Liu, Zhuxing Zhang, Jianming Miao</strong></p>
<p>Few-Shot Semantic Segmentation (FSS), which focuses on segmenting new classes in images using only a limited number of annotated examples, has recently progressed in data-scarce domains. However, in this work, we show that the existing FSS methods often struggle to generalize to underwater environments. Specifically, the prior features extracted by pre-trained models used as feature extractors are fragile due to the unique challenges of underwater images. To address this, we propose FSSUWNet, a tailored FSS framework for underwater images with feature enhancement. FSSUWNet exploits the integration of complementary features, emphasizing both low-level and high-level image characteristics. In addition to employing a pre-trained model as the primary encoder, we propose an auxiliary encoder called Feature Enhanced Encoder which extracts complementary features to better adapt to underwater scene characteristics. Furthermore, a simple and effective Feature Alignment Module aims to provide global prior knowledge and align low-level features with high-level features in dimensions. Given the scarcity of underwater images, we introduce a cross-validation dataset version based on the Segmentation of Underwater Imagery dataset. Extensive experiments on public underwater segmentation datasets demonstrate that our approach achieves state-of-the-art performance. For example, our method outperforms the previous best method by 2.8% and 2.6% in terms of the mean Intersection over Union metric for 1-shot and 5-shot scenarios in the datasets, respectively. Our implementation is available at <a target="_blank" rel="noopener" href="https://github.com/lizhh268/FSSUWNet">https://github.com/lizhh268/FSSUWNet</a>. </p>
<blockquote>
<p>å°‘é‡è¯­ä¹‰åˆ†å‰²ï¼ˆFSSï¼‰æŠ€æœ¯ä¸“æ³¨äºä½¿ç”¨æœ‰é™çš„æ ‡æ³¨æ ·æœ¬å¯¹å›¾åƒä¸­çš„æ–°ç±»åˆ«è¿›è¡Œåˆ†å‰²ï¼Œæœ€è¿‘åœ¨æ•°æ®ç¨€ç¼ºé¢†åŸŸå–å¾—äº†è¿›å±•ã€‚ç„¶è€Œï¼Œåœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬å±•ç¤ºäº†ç°æœ‰çš„FSSæ–¹æ³•å¾€å¾€éš¾ä»¥æ¨å¹¿åˆ°æ°´ä¸‹ç¯å¢ƒã€‚å…·ä½“æ¥è¯´ï¼Œç”±äºæ°´ä¸‹å›¾åƒçš„ç‹¬ç‰¹æŒ‘æˆ˜ï¼Œé¢„è®­ç»ƒæ¨¡å‹æå–çš„å…ˆéªŒç‰¹å¾æ˜¯è„†å¼±çš„ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†FSSUWNetï¼Œè¿™æ˜¯ä¸€ä¸ªé’ˆå¯¹æ°´ä¸‹å›¾åƒè¿›è¡Œç‰¹å¾å¢å¼ºçš„FSSæ¡†æ¶ã€‚FSSUWNetåˆ©ç”¨äº’è¡¥ç‰¹å¾çš„é›†æˆï¼Œå¼ºè°ƒå›¾åƒçš„ä½çº§å’Œé«˜çº§ç‰¹å¾ã€‚é™¤äº†ä½¿ç”¨é¢„è®­ç»ƒæ¨¡å‹ä½œä¸ºä¸»ç¼–ç å™¨å¤–ï¼Œæˆ‘ä»¬è¿˜æå‡ºäº†ä¸€ä¸ªåä¸ºç‰¹å¾å¢å¼ºç¼–ç å™¨çš„è¾…åŠ©ç¼–ç å™¨ï¼Œä»¥æå–äº’è¡¥ç‰¹å¾ï¼Œæ›´å¥½åœ°é€‚åº”æ°´ä¸‹åœºæ™¯ç‰¹å¾ã€‚æ­¤å¤–ï¼Œç®€å•æœ‰æ•ˆçš„ç‰¹å¾å¯¹é½æ¨¡å—æ—¨åœ¨æä¾›å…¨å±€å…ˆéªŒçŸ¥è¯†ï¼Œå¹¶å¯¹ä½çº§ç‰¹å¾ä¸é«˜çº§ç‰¹å¾è¿›è¡Œç»´åº¦å¯¹é½ã€‚é‰´äºæ°´ä¸‹å›¾åƒçš„ç¨€ç¼ºæ€§ï¼Œæˆ‘ä»¬åŸºäºæ°´ä¸‹å›¾åƒåˆ†å‰²æ•°æ®é›†å¼•å…¥äº†ä¸€ä¸ªäº¤å‰éªŒè¯æ•°æ®é›†ç‰ˆæœ¬ã€‚åœ¨å…¬å¼€çš„æ°´ä¸‹åˆ†å‰²æ•°æ®é›†ä¸Šçš„å¤§é‡å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•è¾¾åˆ°äº†æœ€æ–°æ€§èƒ½æ°´å¹³ã€‚ä¾‹å¦‚ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨æ•°æ®é›†çš„å•æ ·æœ¬å’Œäº”æ ·æœ¬åœºæ™¯ä¸‹çš„å¹³å‡äº¤å¹¶æ¯”æŒ‡æ ‡ä¸Šåˆ†åˆ«æ¯”ä¹‹å‰çš„æœ€ä½³æ–¹æ³•é«˜å‡º2.8%å’Œ2.6%ã€‚æˆ‘ä»¬çš„å®ç°å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/lizhh268/FSSUWNet%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/lizhh268/FSSUWNetæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.00478v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æ°´ä¸‹ç¯å¢ƒå¯¹äºç°æœ‰å°‘æ ·æœ¬è¯­ä¹‰åˆ†å‰²ï¼ˆFSSï¼‰æ–¹æ³•æ¥è¯´æ˜¯ä¸€å¤§æŒ‘æˆ˜ã€‚ä¸ºåº”å¯¹è¿™ä¸€éš¾é¢˜ï¼Œæå‡ºä¸€ç§é’ˆå¯¹æ°´ä¸‹å›¾åƒçš„FSSæ¡†æ¶â€”â€”FSSUWNetï¼Œé€šè¿‡ç‰¹å¾å¢å¼ºæ¥åº”å¯¹æ°´ä¸‹ç¯å¢ƒçš„ç‹¬ç‰¹æŒ‘æˆ˜ã€‚FSSUWNeté›†æˆäº†ä¸»è¦ç¼–ç å™¨å’Œè¾…åŠ©ç¼–ç å™¨æå–çš„äº’è¡¥ç‰¹å¾ï¼ŒåŒæ—¶å¼ºè°ƒä½çº§åˆ«å’Œé«˜çº§åˆ«å›¾åƒç‰¹æ€§ã€‚å¦å¤–ï¼Œè¿˜å¼•å…¥äº†ä¸€ä¸ªç‰¹å¾å¯¹é½æ¨¡å—ï¼Œæ—¨åœ¨æä¾›å…¨å±€å…ˆéªŒçŸ¥è¯†å¹¶åœ¨ç»´åº¦ä¸Šå¯¹é½ä½çº§åˆ«ä¸é«˜çº§åˆ«ç‰¹å¾ã€‚åœ¨å…¬å¼€çš„æ°´ä¸‹åˆ†å‰²æ•°æ®é›†ä¸Šè¿›è¡Œçš„å¤§é‡å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•å–å¾—äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>FSSåœ¨æ°´ä¸‹ç¯å¢ƒé¢ä¸´æŒ‘æˆ˜ï¼Œç°æœ‰æ–¹æ³•éš¾ä»¥æ¨å¹¿ã€‚</li>
<li>FSSUWNetæ˜¯ä¸€ä¸ªé’ˆå¯¹æ°´ä¸‹å›¾åƒçš„FSSæ¡†æ¶ï¼Œå…·å¤‡ç‰¹å¾å¢å¼ºåŠŸèƒ½ã€‚</li>
<li>FSSUWNeté›†æˆäº†ä¸»è¦ç¼–ç å™¨å’Œè¾…åŠ©ç¼–ç å™¨æå–çš„äº’è¡¥ç‰¹å¾ï¼Œé€‚åº”æ°´ä¸‹åœºæ™¯ç‰¹æ€§ã€‚</li>
<li>ç‰¹å¾å¯¹é½æ¨¡å—æä¾›å…¨å±€å…ˆéªŒçŸ¥è¯†ï¼Œå¯¹é½ä½çº§åˆ«å’Œé«˜çº§åˆ«ç‰¹å¾ã€‚</li>
<li>å¼•å…¥åŸºäºæ°´ä¸‹å›¾åƒé›†çš„äº¤å‰éªŒè¯æ•°æ®é›†ç‰ˆæœ¬ã€‚</li>
<li>åœ¨å…¬å¼€çš„æ°´ä¸‹åˆ†å‰²æ•°æ®é›†ä¸Šï¼ŒFSSUWNetå–å¾—äº†æœ€å…ˆè¿›çš„æ€§èƒ½è¡¨ç°ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.00478">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-563cf2feb5a978df25548882fd3a0301.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-f2f1d2b24a32e63ede0ecc80777d62d4.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-8b5b14126cc4ec0a90ddaf0013d5f8fe.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-15a04d5cbbddeee6ad30a6b0f76f8162.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-18f6163170c5b6aedaf3322eabbb2673.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-217ca76dc735c5e0fe0a90313ddc4416.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="SU-YOLO-Spiking-Neural-Network-for-Efficient-Underwater-Object-Detection"><a href="#SU-YOLO-Spiking-Neural-Network-for-Efficient-Underwater-Object-Detection" class="headerlink" title="SU-YOLO: Spiking Neural Network for Efficient Underwater Object   Detection"></a>SU-YOLO: Spiking Neural Network for Efficient Underwater Object   Detection</h2><p><strong>Authors:Chenyang Li, Wenxuan Liu, Guoqiang Gong, Xiaobo Ding, Xian Zhong</strong></p>
<p>Underwater object detection is critical for oceanic research and industrial safety inspections. However, the complex optical environment and the limited resources of underwater equipment pose significant challenges to achieving high accuracy and low power consumption. To address these issues, we propose Spiking Underwater YOLO (SU-YOLO), a Spiking Neural Network (SNN) model. Leveraging the lightweight and energy-efficient properties of SNNs, SU-YOLO incorporates a novel spike-based underwater image denoising method based solely on integer addition, which enhances the quality of feature maps with minimal computational overhead. In addition, we introduce Separated Batch Normalization (SeBN), a technique that normalizes feature maps independently across multiple time steps and is optimized for integration with residual structures to capture the temporal dynamics of SNNs more effectively. The redesigned spiking residual blocks integrate the Cross Stage Partial Network (CSPNet) with the YOLO architecture to mitigate spike degradation and enhance the modelâ€™s feature extraction capabilities. Experimental results on URPC2019 underwater dataset demonstrate that SU-YOLO achieves mAP of 78.8% with 6.97M parameters and an energy consumption of 2.98 mJ, surpassing mainstream SNN models in both detection accuracy and computational efficiency. These results underscore the potential of SNNs for engineering applications. The code is available in <a target="_blank" rel="noopener" href="https://github.com/lwxfight/snn-underwater">https://github.com/lwxfight/snn-underwater</a>. </p>
<blockquote>
<p>æ°´ä¸‹ç›®æ ‡æ£€æµ‹å¯¹äºæµ·æ´‹ç ”ç©¶å’Œå·¥ä¸šå®‰å…¨æ£€æŸ¥è‡³å…³é‡è¦ã€‚ç„¶è€Œï¼Œå¤æ‚çš„å…‰å­¦ç¯å¢ƒå’Œæ°´ä¸‹è®¾å¤‡çš„æœ‰é™èµ„æºç»™å®ç°é«˜ç²¾åº¦å’Œä½åŠŸè€—å¸¦æ¥äº†é‡å¤§æŒ‘æˆ˜ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†Spiking Underwater YOLOï¼ˆSU-YOLOï¼‰ï¼Œè¿™æ˜¯ä¸€ç§è„‰å†²ç¥ç»ç½‘ç»œï¼ˆSNNï¼‰æ¨¡å‹ã€‚SU-YOLOåˆ©ç”¨SNNsçš„è½»é‡çº§å’ŒèŠ‚èƒ½ç‰¹æ€§ï¼Œç»“åˆäº†ä¸€ç§åŸºäºè„‰å†²çš„æ–°å‹æ°´ä¸‹å›¾åƒå»å™ªæ–¹æ³•ï¼Œè¯¥æ–¹æ³•ä»…ä½¿ç”¨æ•´æ•°åŠ æ³•ï¼Œå¯åœ¨å‡ ä¹ä¸å¢åŠ è®¡ç®—å¼€é”€çš„æƒ…å†µä¸‹æé«˜ç‰¹å¾å›¾çš„è´¨é‡ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å¼•å…¥äº†åˆ†ç¦»æ‰¹å½’ä¸€åŒ–ï¼ˆSeBNï¼‰æŠ€æœ¯ï¼Œè¯¥æŠ€æœ¯èƒ½å¤Ÿç‹¬ç«‹åœ°å¯¹å¤šä¸ªæ—¶é—´æ­¥é•¿çš„ç‰¹å¾å›¾è¿›è¡Œå½’ä¸€åŒ–ï¼Œå¹¶é’ˆå¯¹ä¸æ®‹å·®ç»“æ„çš„é›†æˆè¿›è¡Œä¼˜åŒ–ï¼Œä»¥æ›´æœ‰æ•ˆåœ°æ•æ‰SNNsçš„æ—¶ç©ºåŠ¨æ€ã€‚é‡æ–°è®¾è®¡çš„è„‰å†²æ®‹å·®å—å°†è·¨é˜¶æ®µéƒ¨åˆ†ç½‘ç»œï¼ˆCSPNetï¼‰ä¸YOLOæ¶æ„ç›¸ç»“åˆï¼Œå‡è½»äº†è„‰å†²é€€åŒ–ç°è±¡ï¼Œæé«˜äº†æ¨¡å‹çš„ç‰¹å¾æå–èƒ½åŠ›ã€‚åœ¨URPC2019æ°´ä¸‹æ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼ŒSU-YOLOè¾¾åˆ°äº†78.8%çš„mAPï¼Œå…·æœ‰6.97Mçš„å‚æ•°å’Œ2.98mJçš„èƒ½è€—ï¼Œåœ¨æ£€æµ‹ç²¾åº¦å’Œè®¡ç®—æ•ˆç‡æ–¹é¢éƒ½è¶…è¶Šäº†ä¸»æµçš„SNNæ¨¡å‹ã€‚è¿™äº›ç»“æœå‡¸æ˜¾äº†SNNåœ¨å·¥ç¨‹åº”ç”¨ä¸­çš„æ½œåŠ›ã€‚ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/lwxfight/snn-underwater">https://github.com/lwxfight/snn-underwater</a>ä¸­è·å–ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.24389v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºä¸€ç§åä¸ºSpiking Underwater YOLOï¼ˆSU-YOLOï¼‰çš„è„‰å†²ç¥ç»ç½‘ç»œæ¨¡å‹ï¼Œç”¨äºæ°´ä¸‹ç›®æ ‡æ£€æµ‹ã€‚è¯¥æ¨¡å‹ç»“åˆè„‰å†²ç¥ç»ç½‘ç»œçš„è½»é‡çº§å’Œèƒ½æ•ˆä¼˜åŠ¿ï¼Œé‡‡ç”¨åŸºäºæ•´æ•°æ·»åŠ çš„è„‰å†²æ°´ä¸‹å›¾åƒå»å™ªæ–¹æ³•ï¼Œæé«˜ç‰¹å¾æ˜ å°„è´¨é‡ä¸”è®¡ç®—å¼€é”€å°ã€‚åŒæ—¶å¼•å…¥åˆ†ç¦»æ‰¹å½’ä¸€åŒ–ï¼ˆSeBNï¼‰æŠ€æœ¯ï¼Œç‹¬ç«‹åœ°å¯¹å¤šä¸ªæ—¶é—´æ­¥é•¿çš„ç‰¹å¾æ˜ å°„è¿›è¡Œå½’ä¸€åŒ–ï¼Œä¸æ®‹å·®ç»“æ„ç»“åˆï¼Œæ›´æœ‰æ•ˆåœ°æ•æ‰SNNçš„æ—¶ç©ºåŠ¨æ€ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒSU-YOLOåœ¨URPC2019æ°´ä¸‹æ•°æ®é›†ä¸Šè¾¾åˆ°78.8%çš„mAPï¼Œå‚æ•°ä»…6.97Mï¼Œèƒ½è€—ä¸º2.98 mJï¼Œåœ¨æ£€æµ‹ç²¾åº¦å’Œè®¡ç®—æ•ˆç‡ä¸Šå‡è¶…è¶Šä¸»æµSNNæ¨¡å‹ï¼Œå‡¸æ˜¾äº†SNNåœ¨å·¥ç¨‹åº”ç”¨ä¸­çš„æ½œåŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ°´ä¸‹ç›®æ ‡æ£€æµ‹é¢ä¸´å¤æ‚å…‰å­¦ç¯å¢ƒå’Œè®¾å¤‡èµ„æºé™åˆ¶çš„æŒ‘æˆ˜ã€‚</li>
<li>æå‡ºSpiking Underwater YOLOï¼ˆSU-YOLOï¼‰æ¨¡å‹ï¼Œç»“åˆè„‰å†²ç¥ç»ç½‘ç»œçš„ä¼˜ç‚¹ã€‚</li>
<li>é‡‡ç”¨åŸºäºæ•´æ•°æ·»åŠ çš„è„‰å†²æ°´ä¸‹å›¾åƒå»å™ªæ–¹æ³•ï¼Œæé«˜ç‰¹å¾æ˜ å°„è´¨é‡ã€‚</li>
<li>å¼•å…¥åˆ†ç¦»æ‰¹å½’ä¸€åŒ–ï¼ˆSeBNï¼‰æŠ€æœ¯ï¼Œæœ‰æ•ˆæ•æ‰SNNçš„æ—¶ç©ºåŠ¨æ€ã€‚</li>
<li>èåˆCross Stage Partial Networkï¼ˆCSPNetï¼‰å’ŒYOLOæ¶æ„ï¼Œå¢å¼ºç‰¹å¾æå–èƒ½åŠ›ã€‚</li>
<li>åœ¨URPC2019æ°´ä¸‹æ•°æ®é›†ä¸Šå®ç°é«˜æ£€æµ‹ç²¾åº¦å’Œè®¡ç®—æ•ˆç‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.24389">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-c7fc1e271f602aecd4463a4aef1c751a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-4a1d0b897c7329fed393b37393dee54f.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Improving-underwater-semantic-segmentation-with-underwater-image-quality-attention-and-muti-scale-aggregation-attention"><a href="#Improving-underwater-semantic-segmentation-with-underwater-image-quality-attention-and-muti-scale-aggregation-attention" class="headerlink" title="Improving underwater semantic segmentation with underwater image quality   attention and muti-scale aggregation attention"></a>Improving underwater semantic segmentation with underwater image quality   attention and muti-scale aggregation attention</h2><p><strong>Authors:Xin Zuo, Jiaran Jiang, Jifeng Shen, Wankou Yang</strong></p>
<p>Underwater image understanding is crucial for both submarine navigation and seabed exploration. However, the low illumination in underwater environments degrades the imaging quality, which in turn seriously deteriorates the performance of underwater semantic segmentation, particularly for outlining the object region boundaries. To tackle this issue, we present UnderWater SegFormer (UWSegFormer), a transformer-based framework for semantic segmentation of low-quality underwater images. Firstly, we propose the Underwater Image Quality Attention (UIQA) module. This module enhances the representation of highquality semantic information in underwater image feature channels through a channel self-attention mechanism. In order to address the issue of loss of imaging details due to the underwater environment, the Multi-scale Aggregation Attention(MAA) module is proposed. This module aggregates sets of semantic features at different scales by extracting discriminative information from high-level features,thus compensating for the semantic loss of detail in underwater objects. Finally, during training, we introduce Edge Learning Loss (ELL) in order to enhance the modelâ€™s learning of underwater object edges and improve the modelâ€™s prediction accuracy. Experiments conducted on the SUIM and DUT-USEG (DUT) datasets have demonstrated that the proposed method has advantages in terms of segmentation completeness, boundary clarity, and subjective perceptual details when compared to SOTA methods. In addition, the proposed method achieves the highest mIoU of 82.12 and 71.41 on the SUIM and DUT datasets, respectively. Code will be available at <a target="_blank" rel="noopener" href="https://github.com/SAWRJJ/UWSegFormer">https://github.com/SAWRJJ/UWSegFormer</a>. </p>
<blockquote>
<p>æ°´ä¸‹å›¾åƒç†è§£å¯¹äºæ½œè‰‡å¯¼èˆªå’Œæµ·åº•æ¢ç´¢éƒ½è‡³å…³é‡è¦ã€‚ç„¶è€Œï¼Œæ°´ä¸‹ç¯å¢ƒçš„ä½ç…§æ˜æ¡ä»¶ä¼šé™ä½å›¾åƒè´¨é‡ï¼Œè¿›è€Œä¸¥é‡æŸå®³æ°´ä¸‹è¯­ä¹‰åˆ†å‰²çš„æ€§èƒ½ï¼Œç‰¹åˆ«æ˜¯åœ¨æç»˜ç‰©ä½“åŒºåŸŸè¾¹ç•Œæ—¶ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†UnderWater SegFormerï¼ˆUWSegFormerï¼‰ï¼Œè¿™æ˜¯ä¸€ä¸ªåŸºäºå˜å‹å™¨çš„æ¡†æ¶ï¼Œç”¨äºå¯¹ä½è´¨é‡çš„æ°´ä¸‹å›¾åƒè¿›è¡Œè¯­ä¹‰åˆ†å‰²ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬æå‡ºäº†æ°´ä¸‹å›¾åƒè´¨é‡æ³¨æ„åŠ›ï¼ˆUIQAï¼‰æ¨¡å—ã€‚è¯¥æ¨¡å—é€šè¿‡é€šé“è‡ªæ³¨æ„åŠ›æœºåˆ¶ï¼Œå¢å¼ºæ°´ä¸‹å›¾åƒç‰¹å¾é€šé“ä¸­é«˜è´¨é‡è¯­ä¹‰ä¿¡æ¯çš„è¡¨ç¤ºã€‚ä¸ºäº†è§£å†³æ°´ä¸‹ç¯å¢ƒå¯¼è‡´çš„æˆåƒç»†èŠ‚ä¸¢å¤±é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†å¤šå°ºåº¦èšåˆæ³¨æ„åŠ›ï¼ˆMAAï¼‰æ¨¡å—ã€‚è¯¥æ¨¡å—é€šè¿‡ä»é«˜çº§ç‰¹å¾ä¸­æå–åˆ¤åˆ«ä¿¡æ¯æ¥èšåˆä¸åŒå°ºåº¦çš„è¯­ä¹‰ç‰¹å¾é›†ï¼Œä»è€Œå¼¥è¡¥æ°´ä¸‹ç‰©ä½“è¯­ä¹‰ç»†èŠ‚çš„æŸå¤±ã€‚æœ€åï¼Œåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œæˆ‘ä»¬å¼•å…¥äº†è¾¹ç¼˜å­¦ä¹ æŸå¤±ï¼ˆELLï¼‰ï¼Œä»¥æé«˜æ¨¡å‹å¯¹æ°´ä¸‹ç‰©ä½“è¾¹ç¼˜çš„å­¦ä¹ èƒ½åŠ›ï¼Œæé«˜æ¨¡å‹çš„é¢„æµ‹ç²¾åº¦ã€‚åœ¨SUIMå’ŒDUT-USEGï¼ˆDUTï¼‰æ•°æ®é›†ä¸Šè¿›è¡Œçš„å®éªŒè¡¨æ˜ï¼Œä¸æœ€æ–°æ–¹æ³•ç›¸æ¯”ï¼Œæ‰€ææ–¹æ³•åœ¨åˆ†å‰²å®Œæ•´æ€§ã€è¾¹ç•Œæ¸…æ™°åº¦å’Œä¸»è§‚æ„ŸçŸ¥ç»†èŠ‚æ–¹é¢å…·ä¼˜åŠ¿ã€‚æ­¤å¤–ï¼Œæ‰€ææ–¹æ³•åœ¨SUIMå’ŒDUTæ•°æ®é›†ä¸Šåˆ†åˆ«è¾¾åˆ°äº†æœ€é«˜çš„mIoUå€¼ï¼Œåˆ†åˆ«ä¸º82.12å’Œ71.41ã€‚ä»£ç å°†å‘å¸ƒåœ¨<a target="_blank" rel="noopener" href="https://github.com/SAWRJJ/UWSegFormer%E3%80%82">https://github.com/SAWRJJ/UWSegFormerã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.23422v1">PDF</a> Accepted by Pattern Analysis and Applications</p>
<p><strong>Summary</strong><br>    é’ˆå¯¹æ°´ä¸‹å›¾åƒè´¨é‡ä½å½±å“è¯­ä¹‰åˆ†å‰²çš„é—®é¢˜ï¼Œæå‡ºäº†åŸºäºTransformerçš„UnderWater SegFormeræ¡†æ¶ã€‚é€šè¿‡UIQAæ¨¡å—å¢å¼ºé«˜è´¨é‡è¯­ä¹‰ä¿¡æ¯è¡¨ç¤ºï¼Œé€šè¿‡MAAæ¨¡å—è¡¥å¿æ°´ä¸‹ç‰©ä½“ç»†èŠ‚è¯­ä¹‰æŸå¤±ï¼Œå¼•å…¥Edge Learning Lossæé«˜æ¨¡å‹å¯¹æ°´ä¸‹ç‰©ä½“è¾¹ç¼˜çš„å­¦ä¹ èƒ½åŠ›ã€‚å®éªŒè¯æ˜ï¼Œè¯¥æ–¹æ³•åœ¨åˆ†å‰²å®Œæ•´æ€§ã€è¾¹ç•Œæ¸…æ™°åº¦å’Œä¸»è§‚æ„ŸçŸ¥ç»†èŠ‚æ–¹é¢ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œå¹¶åœ¨SUIMå’ŒDUTæ•°æ®é›†ä¸Šè¾¾åˆ°æœ€é«˜mIoUå€¼ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ°´ä¸‹å›¾åƒç†è§£å¯¹æ½œè‰‡å¯¼èˆªå’Œæµ·åº•æ¢ç´¢è‡³å…³é‡è¦ã€‚</li>
<li>ä½ç…§æ˜ç¯å¢ƒå¯¼è‡´æ°´ä¸‹å›¾åƒè´¨é‡ä¸‹é™ï¼Œå½±å“è¯­ä¹‰åˆ†å‰²æ€§èƒ½ï¼Œç‰¹åˆ«æ˜¯åœ¨æç»˜ç‰©ä½“åŒºåŸŸè¾¹ç•Œæ–¹é¢ã€‚</li>
<li>æå‡ºçš„UnderWater SegFormeræ˜¯ä¸€ä¸ªåŸºäºTransformerçš„æ¡†æ¶ï¼Œç”¨äºå¯¹ä½è´¨é‡æ°´ä¸‹å›¾åƒè¿›è¡Œè¯­ä¹‰åˆ†å‰²ã€‚</li>
<li>UIQAæ¨¡å—å¢å¼ºæ°´ä¸‹å›¾åƒç‰¹å¾é€šé“çš„é«˜è´¨é‡è¯­ä¹‰ä¿¡æ¯è¡¨ç¤ºã€‚</li>
<li>MAAæ¨¡å—é€šè¿‡èšåˆä¸åŒå°ºåº¦çš„è¯­ä¹‰ç‰¹å¾æ¥è¡¥å¿æ°´ä¸‹ç‰©ä½“ç»†èŠ‚è¯­ä¹‰çš„æŸå¤±ã€‚</li>
<li>å¼•å…¥Edge Learning Lossä»¥æé«˜æ¨¡å‹å¯¹æ°´ä¸‹ç‰©ä½“è¾¹ç¼˜çš„å­¦ä¹ èƒ½åŠ›ï¼Œæé«˜é¢„æµ‹å‡†ç¡®æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.23422">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-15809d14ffd2e07d9dc99d7d7b788970.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Large-Self-Supervised-Models-Bridge-the-Gap-in-Domain-Adaptive-Object-Detection"><a href="#Large-Self-Supervised-Models-Bridge-the-Gap-in-Domain-Adaptive-Object-Detection" class="headerlink" title="Large Self-Supervised Models Bridge the Gap in Domain Adaptive Object   Detection"></a>Large Self-Supervised Models Bridge the Gap in Domain Adaptive Object   Detection</h2><p><strong>Authors:Marc-Antoine Lavoie, Anas Mahmoud, Steven L. Waslander</strong></p>
<p>The current state-of-the-art methods in domain adaptive object detection (DAOD) use Mean Teacher self-labelling, where a teacher model, directly derived as an exponential moving average of the student model, is used to generate labels on the target domain which are then used to improve both models in a positive loop. This couples learning and generating labels on the target domain, and other recent works also leverage the generated labels to add additional domain alignment losses. We believe this coupling is brittle and excessively constrained: there is no guarantee that a student trained only on source data can generate accurate target domain labels and initiate the positive feedback loop, and much better target domain labels can likely be generated by using a large pretrained network that has been exposed to much more data. Vision foundational models are exactly such models, and they have shown impressive task generalization capabilities even when frozen. We want to leverage these models for DAOD and introduce DINO Teacher, which consists of two components. First, we train a new labeller on source data only using a large frozen DINOv2 backbone and show it generates more accurate labels than Mean Teacher. Next, we align the studentâ€™s source and target image patch features with those from a DINO encoder, driving source and target representations closer to the generalizable DINO representation. We obtain state-of-the-art performance on multiple DAOD datasets. Code available at <a target="_blank" rel="noopener" href="https://github.com/TRAILab/DINO_Teacher">https://github.com/TRAILab/DINO_Teacher</a> </p>
<blockquote>
<p>å½“å‰æœ€å…ˆè¿›çš„é¢†åŸŸè‡ªé€‚åº”ç›®æ ‡æ£€æµ‹ï¼ˆDAODï¼‰æ–¹æ³•é‡‡ç”¨Mean Teacherè‡ªæ ‡æ³¨æŠ€æœ¯ã€‚å…¶ä¸­ï¼Œæ•™å¸ˆæ¨¡å‹è¢«ç›´æ¥ç”¨ä½œå­¦ç”Ÿæ¨¡å‹çš„æŒ‡æ•°ç§»åŠ¨å¹³å‡æ¨¡å‹ï¼Œåœ¨ç›®æ ‡åŸŸä¸Šç”Ÿæˆæ ‡ç­¾ï¼Œå¹¶ç”¨äºåœ¨æ­£å‘å¾ªç¯ä¸­æ”¹è¿›è¿™ä¸¤ä¸ªæ¨¡å‹ã€‚è¿™ç§æŠ€æœ¯å°†å­¦ä¹ å’Œåœ¨ç›®æ ‡åŸŸä¸Šç”Ÿæˆæ ‡ç­¾ç›¸ç»“åˆï¼Œå…¶ä»–è¿‘æœŸçš„ç ”ç©¶ä¹Ÿåˆ©ç”¨ç”Ÿæˆçš„æ ‡ç­¾æ¥å¢åŠ é¢å¤–çš„åŸŸå¯¹é½æŸå¤±ã€‚æˆ‘ä»¬è®¤ä¸ºè¿™ç§ç»“åˆæ–¹å¼è„†å¼±ä¸”å—åˆ°è¿‡åº¦çº¦æŸï¼šæ²¡æœ‰ä¿è¯ä»…é€šè¿‡æºæ•°æ®è®­ç»ƒçš„å­¦ç”Ÿæ¨¡å‹èƒ½å¤Ÿåœ¨ç›®æ ‡åŸŸä¸Šç”Ÿæˆå‡†ç¡®çš„æ ‡ç­¾å¹¶å¯åŠ¨æ­£å‘åé¦ˆå¾ªç¯ï¼›é€šè¿‡ä½¿ç”¨å·²æ¥è§¦åˆ°æ›´å¤šæ•°æ®çš„å¤§å‹é¢„è®­ç»ƒç½‘ç»œï¼Œå¾ˆå¯èƒ½ä¼šç”Ÿæˆæ›´å¥½çš„ç›®æ ‡åŸŸæ ‡ç­¾ã€‚è§†è§‰åŸºç¡€æ¨¡å‹æ­£æ˜¯è¿™æ ·çš„æ¨¡å‹ï¼Œå³ä½¿åœ¨å†»ç»“çŠ¶æ€ä¸‹ï¼Œå®ƒä»¬ä¹Ÿè¡¨ç°å‡ºäº†ä»¤äººå°è±¡æ·±åˆ»çš„ä»»åŠ¡æ³›åŒ–èƒ½åŠ›ã€‚æˆ‘ä»¬å¸Œæœ›åˆ©ç”¨è¿™äº›æ¨¡å‹è¿›è¡ŒDAODï¼Œå¹¶å¼•å…¥DINO Teacherï¼Œå®ƒåŒ…å«ä¸¤ä¸ªç»„ä»¶ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬ä»…ä½¿ç”¨æºæ•°æ®è®­ç»ƒä¸€ä¸ªæ–°çš„æ ‡æ³¨å™¨ï¼Œé‡‡ç”¨å¤§å‹å†»ç»“çš„DINOv2éª¨å¹²ç½‘ï¼Œå¹¶è¯æ˜å…¶ç”Ÿæˆçš„æ ‡ç­¾æ¯”Mean Teacheræ›´å‡†ç¡®ã€‚æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬å°†å­¦ç”Ÿæ¨¡å‹çš„æºå›¾åƒå’Œç›®æ ‡å›¾åƒè¡¥ä¸ç‰¹å¾ä¸DINOç¼–ç å™¨çš„ç‰¹å¾è¿›è¡Œå¯¹é½ï¼Œä½¿æºå’Œç›®æ ‡è¡¨ç¤ºæ›´æ¥è¿‘å¯æ³›åŒ–çš„DINOè¡¨ç¤ºã€‚æˆ‘ä»¬åœ¨å¤šä¸ªDAODæ•°æ®é›†ä¸Šè·å¾—äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚ä»£ç å¯è®¿é—®ï¼š<a target="_blank" rel="noopener" href="https://github.com/TRAILab/DINO_Teacher">https://github.com/TRAILab/DINO_Teacher</a> ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.23220v1">PDF</a> 16 pages (8 main), 5 figures, accepted at CVPR 2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†åœ¨è‡ªé€‚åº”ç›®æ ‡æ£€æµ‹é¢†åŸŸçš„æ–°æ–¹æ³•ï¼Œæå‡ºä½¿ç”¨DINO Teacherä»£æ›¿ä¼ ç»Ÿçš„Mean Teacherè‡ªæ ‡æ³¨æŠ€æœ¯ã€‚DINO Teacheråˆ©ç”¨å¤§å‹é¢„è®­ç»ƒæ¨¡å‹ç”Ÿæˆæ›´å‡†ç¡®çš„ç›®æ ‡åŸŸæ ‡ç­¾ï¼Œå¹¶é€šè¿‡ä¸å­¦ç”Ÿæ¨¡å‹çš„æºå›¾åƒå’Œç›®æ ‡å›¾åƒå—ç‰¹å¾å¯¹é½ï¼Œæ‹‰è¿‘æºå’Œç›®æ ‡è¡¨ç¤ºçš„å·®è·ï¼Œå®ç°è·¨é¢†åŸŸè‡ªé€‚åº”æ€§èƒ½çš„æå‡ã€‚æ­¤ç ”ç©¶è·å¾—å¤šä¸ªè‡ªé€‚åº”ç›®æ ‡æ£€æµ‹æ•°æ®é›†çš„æœ€ä½³è¡¨ç°ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å½“å‰è‡ªé€‚åº”ç›®æ ‡æ£€æµ‹çš„ä¸»æµæ–¹æ³•ä½¿ç”¨Mean Teacherè‡ªæ ‡æ³¨æŠ€æœ¯ï¼Œé€šè¿‡æ•™å¸ˆæ¨¡å‹ç”Ÿæˆç›®æ ‡åŸŸæ ‡ç­¾æ¥æ”¹å–„å­¦ç”Ÿå’Œæ•™å¸ˆæ¨¡å‹ã€‚</li>
<li>æœ¬æ–‡è®¤ä¸ºè¿™ç§æ–¹æ³•çš„è€¦åˆæ€§å¼ºä¸”è¿‡äºå—é™ï¼Œå› ä¸ºä»…ä¾èµ–æºæ•°æ®è®­ç»ƒçš„å­¦ç”Ÿæ¨¡å‹å¯èƒ½æ— æ³•ç”Ÿæˆå‡†ç¡®çš„ç›®æ ‡åŸŸæ ‡ç­¾ã€‚</li>
<li>å¤§å‹é¢„è®­ç»ƒæ¨¡å‹ï¼ˆå¦‚Vision foundational modelsï¼‰åœ¨ä»»åŠ¡æ³›åŒ–æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œæœ¬æ–‡æ—¨åœ¨åˆ©ç”¨è¿™ç±»æ¨¡å‹è¿›è¡Œè‡ªé€‚åº”ç›®æ ‡æ£€æµ‹ã€‚</li>
<li>å¼•å…¥DINO Teacherï¼Œå…¶åŒ…å«ä¸¤ä¸ªç»„æˆéƒ¨åˆ†ï¼šä¸€æ˜¯ä½¿ç”¨å¤§å‹å†»ç»“çš„DINOv2éª¨å¹²ç½‘åœ¨æºæ•°æ®ä¸Šè®­ç»ƒæ–°çš„æ ‡æ³¨å™¨ï¼Œç”Ÿæˆæ›´å‡†ç¡®çš„æ ‡ç­¾ï¼›äºŒæ˜¯é€šè¿‡ä¸å­¦ç”Ÿæ¨¡å‹çš„æºå›¾åƒå’Œç›®æ ‡å›¾åƒå—ç‰¹å¾å¯¹é½ï¼Œæ‹‰è¿‘æºå’Œç›®æ ‡è¡¨ç¤ºçš„å·®è·ã€‚</li>
<li>DINO Teacheråœ¨å¤šä¸ªè‡ªé€‚åº”ç›®æ ‡æ£€æµ‹æ•°æ®é›†ä¸Šè·å¾—æœ€ä½³æ€§èƒ½ã€‚</li>
<li>ç ”ç©¶è€…æä¾›äº†ä»£ç å®ç°ï¼Œå¯ä¾›å…¬ä¼—è®¿é—®ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.23220">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-01999a2042c7d7c5fb807c441b319a64.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8484c78e8b775fed882147b76a46f8db.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-964ad0d71fe2e7408f253d6b21010426.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="RUNA-Object-level-Out-of-Distribution-Detection-via-Regional-Uncertainty-Alignment-of-Multimodal-Representations"><a href="#RUNA-Object-level-Out-of-Distribution-Detection-via-Regional-Uncertainty-Alignment-of-Multimodal-Representations" class="headerlink" title="RUNA: Object-level Out-of-Distribution Detection via Regional   Uncertainty Alignment of Multimodal Representations"></a>RUNA: Object-level Out-of-Distribution Detection via Regional   Uncertainty Alignment of Multimodal Representations</h2><p><strong>Authors:Bin Zhang, Jinggang Chen, Xiaoyang Qu, Guokuan Li, Kai Lu, Jiguang Wan, Jing Xiao, Jianzong Wang</strong></p>
<p>Enabling object detectors to recognize out-of-distribution (OOD) objects is vital for building reliable systems. A primary obstacle stems from the fact that models frequently do not receive supervisory signals from unfamiliar data, leading to overly confident predictions regarding OOD objects. Despite previous progress that estimates OOD uncertainty based on the detection model and in-distribution (ID) samples, we explore using pre-trained vision-language representations for object-level OOD detection. We first discuss the limitations of applying image-level CLIP-based OOD detection methods to object-level scenarios. Building upon these insights, we propose RUNA, a novel framework that leverages a dual encoder architecture to capture rich contextual information and employs a regional uncertainty alignment mechanism to distinguish ID from OOD objects effectively. We introduce a few-shot fine-tuning approach that aligns region-level semantic representations to further improve the modelâ€™s capability to discriminate between similar objects. Our experiments show that RUNA substantially surpasses state-of-the-art methods in object-level OOD detection, particularly in challenging scenarios with diverse and complex object instances. </p>
<blockquote>
<p>å®ç°å¯¹è±¡æ£€æµ‹å™¨å¯¹ç¦»ç¾¤åˆ†å¸ƒï¼ˆOODï¼‰å¯¹è±¡çš„è¯†åˆ«å¯¹äºæ„å»ºå¯é ç³»ç»Ÿè‡³å…³é‡è¦ã€‚ä¸»è¦çš„éšœç¢åœ¨äºæ¨¡å‹ç»å¸¸æ— æ³•ä»æœªçŸ¥æ•°æ®ä¸­è·å–ç›‘ç£ä¿¡å·ï¼Œå¯¼è‡´å¯¹OODå¯¹è±¡çš„é¢„æµ‹è¿‡äºè‡ªä¿¡ã€‚å°½ç®¡å…ˆå‰çš„å·¥ä½œå·²ç»åŸºäºæ£€æµ‹æ¨¡å‹å’Œå†…éƒ¨åˆ†å¸ƒï¼ˆIDï¼‰æ ·æœ¬ä¼°è®¡äº†OODä¸ç¡®å®šæ€§ï¼Œä½†æˆ‘ä»¬åœ¨å¯¹è±¡çº§åˆ«çš„OODæ£€æµ‹ä¸­æ¢ç´¢äº†ä½¿ç”¨é¢„è®­ç»ƒçš„è§†è§‰è¯­è¨€è¡¨ç¤ºã€‚æˆ‘ä»¬é¦–å…ˆè®¨è®ºäº†å°†åŸºäºå›¾åƒçº§åˆ«çš„CLIPçš„OODæ£€æµ‹æ–¹æ³•åº”ç”¨äºå¯¹è±¡çº§åˆ«åœºæ™¯çš„å±€é™æ€§ã€‚åŸºäºè¿™äº›è§è§£ï¼Œæˆ‘ä»¬æå‡ºäº†RUNAè¿™ä¸€æ–°å‹æ¡†æ¶ï¼Œå®ƒé‡‡ç”¨åŒç¼–ç å™¨æ¶æ„æ¥æ•è·ä¸°å¯Œçš„ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼Œå¹¶é‡‡ç”¨äº†åŒºåŸŸä¸ç¡®å®šæ€§å¯¹é½æœºåˆ¶æ¥æœ‰æ•ˆåœ°åŒºåˆ†IDå’ŒOODå¯¹è±¡ã€‚æˆ‘ä»¬å¼•å…¥äº†ä¸€ç§å°æ ·æœ¬å¾®è°ƒæ–¹æ³•ï¼Œé€šè¿‡è°ƒæ•´åŒºåŸŸçº§åˆ«çš„è¯­ä¹‰è¡¨ç¤ºæ¥è¿›ä¸€æ­¥æé«˜æ¨¡å‹åŒºåˆ†ç›¸ä¼¼å¯¹è±¡çš„èƒ½åŠ›ã€‚å®éªŒè¡¨æ˜ï¼ŒRUNAåœ¨å¯¹è±¡çº§åˆ«çš„OODæ£€æµ‹æ–¹é¢æ˜¾è‘—è¶…è¶Šäº†ç°æœ‰å…ˆè¿›æŠ€æœ¯ï¼Œç‰¹åˆ«æ˜¯åœ¨å…·æœ‰å¤šæ ·åŒ–å’Œå¤æ‚å¯¹è±¡å®ä¾‹çš„å…·æœ‰æŒ‘æˆ˜æ€§çš„åœºæ™¯ä¸­ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.22285v1">PDF</a> 9 pages, 5 figures</p>
<p><strong>Summary</strong></p>
<p>å¯¹è±¡æ£€æµ‹å™¨è¯†åˆ«å‡ºåˆ†å¸ƒå¤–ï¼ˆOODï¼‰å¯¹è±¡çš„èƒ½åŠ›å¯¹äºæ„å»ºå¯é ç³»ç»Ÿè‡³å…³é‡è¦ã€‚æœ¬æ–‡æ¢è®¨äº†å°†é¢„è®­ç»ƒçš„è§†è§‰è¯­è¨€è¡¨ç¤ºåº”ç”¨äºå¯¹è±¡çº§åˆ«çš„OODæ£€æµ‹ã€‚é’ˆå¯¹å›¾åƒçº§åˆ«çš„CLIPåŸºOODæ£€æµ‹æ–¹æ³•åœ¨å¯¹è±¡çº§åˆ«åœºæ™¯ä¸­çš„åº”ç”¨å±€é™æ€§ï¼Œæˆ‘ä»¬æå‡ºäº†RUNAæ¡†æ¶ï¼Œè¯¥æ¡†æ¶é‡‡ç”¨åŒç¼–ç å™¨æ¶æ„æ•æ‰ä¸°å¯Œçš„ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼Œå¹¶åˆ©ç”¨åŒºåŸŸä¸ç¡®å®šæ€§å¯¹é½æœºåˆ¶æœ‰æ•ˆåŒºåˆ†IDå’ŒOODå¯¹è±¡ã€‚é€šè¿‡å¼•å…¥å°‘é‡æ ·æœ¬å¾®è°ƒæ–¹æ³•ï¼Œè¿›ä¸€æ­¥æé«˜äº†æ¨¡å‹åŒºåˆ†ç›¸ä¼¼å¯¹è±¡çš„èƒ½åŠ›ã€‚å®éªŒè¡¨æ˜ï¼ŒRUNAåœ¨å¯¹è±¡çº§åˆ«çš„OODæ£€æµ‹ä¸­æ˜¾è‘—è¶…è¶Šäº†ç°æœ‰æ–¹æ³•ï¼Œç‰¹åˆ«æ˜¯åœ¨å…·æœ‰å¤šæ ·æ€§å’Œå¤æ‚æ€§å¯¹è±¡å®ä¾‹çš„æŒ‘æˆ˜åœºæ™¯ä¸­ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¯¹è±¡æ£€æµ‹å™¨è¯†åˆ«åˆ†å¸ƒå¤–ï¼ˆOODï¼‰å¯¹è±¡çš„é‡è¦æ€§ã€‚</li>
<li>æ¨¡å‹åœ¨é¢å¯¹ä¸ç†Ÿæ‚‰æ•°æ®æ—¶ç¼ºä¹ç›‘ç£ä¿¡å·ï¼Œå¯¼è‡´å¯¹OODå¯¹è±¡çš„é¢„æµ‹è¿‡äºè‡ªä¿¡ï¼Œè¿™æ˜¯ä¸»è¦éšœç¢ä¹‹ä¸€ã€‚</li>
<li>å€ŸåŠ©é¢„è®­ç»ƒçš„è§†è§‰è¯­è¨€è¡¨ç¤ºè¿›è¡Œå¯¹è±¡çº§åˆ«çš„OODæ£€æµ‹æ˜¯ä¸€ç§æ–°æ¢ç´¢ã€‚</li>
<li>RUNAæ¡†æ¶é‡‡ç”¨åŒç¼–ç å™¨æ¶æ„å’ŒåŒºåŸŸä¸ç¡®å®šæ€§å¯¹é½æœºåˆ¶æ¥åŒºåˆ†IDå’ŒOODå¯¹è±¡ã€‚</li>
<li>RUNAæ¡†æ¶é€šè¿‡å¼•å…¥å°‘é‡æ ·æœ¬å¾®è°ƒæ–¹æ³•æ¥æé«˜æ¨¡å‹åŒºåˆ†ç›¸ä¼¼å¯¹è±¡çš„èƒ½åŠ›ã€‚</li>
<li>å®éªŒæ˜¾ç¤ºRUNAåœ¨å¯¹è±¡çº§åˆ«çš„OODæ£€æµ‹ä¸­è¡¨ç°ä¼˜è¶Šï¼Œå°¤å…¶æ˜¯å¤„ç†å¤šæ ·æ€§å’Œå¤æ‚æ€§å¯¹è±¡å®ä¾‹çš„æŒ‘æˆ˜åœºæ™¯ã€‚</li>
<li>è¯¥ç ”ç©¶ä¸ºæ„å»ºæ›´å¯é çš„å¯¹è±¡æ£€æµ‹å™¨æä¾›äº†æ–°çš„æ€è·¯å’Œæ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.22285">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-3148fdfbc140dad9f1afa9c7e92f80c2.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-43398acaa3b3fd4ae19accd39a8f0994.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6b7e036d3b8dc94bceb088f00f8b6671.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-9ff9f6c0aec462954974a348d2338bc5.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-2d694516f43e8daea7ecab8908d7e8d0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-fef36782ae60f260c395c5f88a9fad22.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-fa37a1787daff2aa7c50747565a282e7.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Hyperspectral-Adapter-for-Object-Tracking-based-on-Hyperspectral-Video"><a href="#Hyperspectral-Adapter-for-Object-Tracking-based-on-Hyperspectral-Video" class="headerlink" title="Hyperspectral Adapter for Object Tracking based on Hyperspectral Video"></a>Hyperspectral Adapter for Object Tracking based on Hyperspectral Video</h2><p><strong>Authors:Long Gao, Yunhe Zhang, Langkun Chen, Yan Jiang, Weiying Xie, Yunsong Li</strong></p>
<p>Object tracking based on hyperspectral video attracts increasing attention to the rich material and motion information in the hyperspectral videos. The prevailing hyperspectral methods adapt pretrained RGB-based object tracking networks for hyperspectral tasks by fine-tuning the entire network on hyperspectral datasets, which achieves impressive results in challenging scenarios. However, the performance of hyperspectral trackers is limited by the loss of spectral information during the transformation, and fine-tuning the entire pretrained network is inefficient for practical applications. To address the issues, a new hyperspectral object tracking method, hyperspectral adapter for tracking (HyA-T), is proposed in this work. The hyperspectral adapter for the self-attention (HAS) and the hyperspectral adapter for the multilayer perceptron (HAM) are proposed to generate the adaption information and to transfer the multi-head self-attention (MSA) module and the multilayer perceptron (MLP) in pretrained network for the hyperspectral object tracking task by augmenting the adaption information into the calculation of the MSA and MLP. Additionally, the hyperspectral enhancement of input (HEI) is proposed to augment the original spectral information into the input of the tracking network. The proposed methods extract spectral information directly from the hyperspectral images, which prevent the loss of the spectral information. Moreover, only the parameters in the proposed methods are fine-tuned, which is more efficient than the existing methods. Extensive experiments were conducted on four datasets with various spectral bands, verifing the effectiveness of the proposed methods. The HyA-T achieves state-of-the-art performance on all the datasets. </p>
<blockquote>
<p>åŸºäºé«˜å…‰è°±è§†é¢‘çš„ç‰©ä½“è·Ÿè¸ªå¸å¼•äº†äººä»¬å¯¹é«˜å…‰è°±è§†é¢‘ä¸­ä¸°å¯Œçš„ææ–™å’Œè¿åŠ¨ä¿¡æ¯çš„å…³æ³¨ã€‚ç›®å‰æµè¡Œçš„é«˜å…‰è°±æ–¹æ³•é€šè¿‡åœ¨é«˜å…‰è°±æ•°æ®é›†ä¸Šå¾®è°ƒæ•´ä¸ªç½‘ç»œï¼Œå°†é¢„è®­ç»ƒçš„åŸºäºRGBçš„ç‰©ä½“è·Ÿè¸ªç½‘ç»œé€‚åº”äºé«˜å…‰è°±ä»»åŠ¡ï¼Œè¿™åœ¨å…·æœ‰æŒ‘æˆ˜æ€§çš„åœºæ™¯ä¸­å–å¾—äº†ä»¤äººå°è±¡æ·±åˆ»çš„ç»“æœã€‚ç„¶è€Œï¼Œé«˜å…‰è°±è·Ÿè¸ªå™¨çš„æ€§èƒ½å—åˆ°è½¬æ¢è¿‡ç¨‹ä¸­å…‰è°±ä¿¡æ¯æŸå¤±çš„é™åˆ¶ï¼Œå¹¶ä¸”å¾®è°ƒæ•´ä¸ªé¢„è®­ç»ƒç½‘ç»œå¯¹äºå®é™…åº”ç”¨æ˜¯ä¸é«˜æ•ˆçš„ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„é«˜å…‰è°±ç‰©ä½“è·Ÿè¸ªæ–¹æ³•ï¼Œå³ç”¨äºè·Ÿè¸ªçš„é«˜å…‰è°±é€‚é…å™¨ï¼ˆHyA-Tï¼‰ã€‚æå‡ºäº†ç”¨äºè‡ªæ³¨æ„åŠ›æœºåˆ¶çš„é«˜å…‰è°±é€‚é…å™¨ï¼ˆHASï¼‰å’Œç”¨äºå¤šå±‚æ„ŸçŸ¥å™¨çš„é«˜å…‰è°±é€‚é…å™¨ï¼ˆHAMï¼‰ï¼Œä»¥ç”Ÿæˆé€‚åº”ä¿¡æ¯ï¼Œå¹¶å°†å¤šå¤´è‡ªæ³¨æ„åŠ›ï¼ˆMSAï¼‰æ¨¡å—å’Œå¤šå±‚æ„ŸçŸ¥å™¨ï¼ˆMLPï¼‰è½¬ç§»åˆ°é«˜å…‰è°±ç‰©ä½“è·Ÿè¸ªä»»åŠ¡ä¸­ï¼Œé€šè¿‡å°†é€‚åº”ä¿¡æ¯å¢åŠ åˆ°MSAå’ŒMLPçš„è®¡ç®—ä¸­ã€‚æ­¤å¤–ï¼Œè¿˜æå‡ºäº†é«˜å…‰è°±å¢å¼ºè¾“å…¥ï¼ˆHEIï¼‰çš„æ–¹æ³•ï¼Œå°†åŸå§‹å…‰è°±ä¿¡æ¯å¢å¼ºåˆ°è·Ÿè¸ªç½‘ç»œçš„è¾“å…¥ä¸­ã€‚æ‰€æå‡ºçš„æ–¹æ³•ç›´æ¥ä»é«˜å…‰è°±å›¾åƒä¸­æå–å…‰è°±ä¿¡æ¯ï¼Œé˜²æ­¢äº†å…‰è°±ä¿¡æ¯çš„æŸå¤±ã€‚è€Œä¸”ï¼Œåªæœ‰æ‰€æå‡ºçš„æ–¹æ³•ä¸­çš„å‚æ•°è¿›è¡Œäº†å¾®è°ƒï¼Œè¿™æ¯”ç°æœ‰æ–¹æ³•æ›´æœ‰æ•ˆç‡ã€‚åœ¨å››ä¸ªå…·æœ‰ä¸åŒå…‰è°±æ³¢æ®µçš„æ•°æ®åº“ä¸Šè¿›è¡Œäº†å¤§é‡å®éªŒï¼ŒéªŒè¯äº†æ‰€æå‡ºæ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚HyA-Tåœ¨æ‰€æœ‰æ•°æ®é›†ä¸Šå‡è¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.22199v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡å…³æ³¨åŸºäºé«˜å…‰è°±è§†é¢‘çš„ç‰©ä½“è·Ÿè¸ªæŠ€æœ¯ã€‚ç°æœ‰çš„é«˜å…‰è°±æ–¹æ³•é€šè¿‡å¾®è°ƒæ•´ä¸ªç½‘ç»œåœ¨é«˜å…‰è°±æ•°æ®é›†ä¸Šè¿›è¡ŒRGBåŸºç¡€ç‰©ä½“è·Ÿè¸ªç½‘ç»œçš„é¢„è®­ç»ƒï¼Œè™½ç„¶åœ¨å…·æœ‰æŒ‘æˆ˜æ€§çš„åœºæ™¯ä¸­è¡¨ç°æƒŠäººï¼Œä½†ä»é¢ä¸´ä¿¡æ¯è½¬æ¢ä¸­çš„å…‰è°±ä¿¡æ¯æŸå¤±ä»¥åŠå®é™…åº”ç”¨ä¸­çš„æ•ˆç‡ä½ä¸‹çš„é—®é¢˜ã€‚ä¸ºè§£å†³è¿™äº›é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„é«˜å…‰è°±ç‰©ä½“è·Ÿè¸ªæ–¹æ³•â€”â€”é«˜å…‰è°±é€‚é…å™¨è·Ÿè¸ªï¼ˆHyA-Tï¼‰ã€‚è¯¥æ–¹æ³•åŒ…æ‹¬é«˜å…‰è°±è‡ªæ³¨æ„åŠ›é€‚é…å™¨ï¼ˆHASï¼‰å’Œé«˜å…‰è°±å¤šå±‚æ„ŸçŸ¥å™¨é€‚é…å™¨ï¼ˆHAMï¼‰ï¼Œæ—¨åœ¨ç”Ÿæˆé€‚åº”ä¿¡æ¯å¹¶å°†å¤šå¤´è‡ªæ³¨æ„åŠ›æ¨¡å—å’Œå¤šå±‚æ„ŸçŸ¥å™¨è½¬ç§»åˆ°é«˜å…‰è°±ç‰©ä½“è·Ÿè¸ªä»»åŠ¡ä¸­ã€‚åŒæ—¶æå‡ºäº†é«˜å…‰è°±å¢å¼ºè¾“å…¥ï¼ˆHEIï¼‰ï¼Œä»¥å°†åŸå§‹å…‰è°±ä¿¡æ¯æ·»åŠ åˆ°è·Ÿè¸ªç½‘ç»œçš„è¾“å…¥ä¸­ã€‚è¯¥æ–¹æ³•ç›´æ¥ä»é«˜å…‰è°±å›¾åƒä¸­æå–å…‰è°±ä¿¡æ¯ï¼Œé¿å…äº†ä¿¡æ¯æŸå¤±ï¼Œå¹¶ä¸”ä»…å¾®è°ƒè¯¥æ–¹æ³•çš„å‚æ•°ï¼Œä½¿å¾—å…¶åœ¨æ•ˆç‡å’Œæ•ˆæœä¸Šéƒ½è¶…è¶Šäº†ç°æœ‰æ–¹æ³•ã€‚ç»è¿‡å››ä¸ªæ•°æ®é›†ä¸Šçš„å¹¿æ³›å®éªŒéªŒè¯äº†è¯¥æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚HyA-Tåœ¨æ‰€æœ‰æ•°æ®é›†ä¸Šè¾¾åˆ°äº†æœ€å…ˆè¿›çš„è¡¨ç°ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>é«˜å…‰è°±è§†é¢‘ç‰©ä½“è·Ÿè¸ªé‡è§†ä¸°å¯Œçš„ææ–™å’Œè¿åŠ¨ä¿¡æ¯ã€‚</li>
<li>ç°æœ‰æ–¹æ³•é€šè¿‡å¾®è°ƒæ•´ä¸ªé¢„è®­ç»ƒç½‘ç»œä»¥é€‚åº”é«˜å…‰è°±ä»»åŠ¡ï¼Œè™½æ•ˆæœè‰¯å¥½ä½†å­˜åœ¨æ•ˆç‡å’Œä¿¡æ¯æŸå¤±é—®é¢˜ã€‚</li>
<li>æ–°æ–¹æ³•HyA-TåŒ…æ‹¬HASã€HAMå’ŒHEIï¼Œæ—¨åœ¨è§£å†³ä¿¡æ¯æŸå¤±å’Œæé«˜æ•ˆç‡é—®é¢˜ã€‚</li>
<li>HASå’ŒHAMç”Ÿæˆé€‚åº”ä¿¡æ¯å¹¶è½¬ç§»é¢„è®­ç»ƒç½‘ç»œä¸­çš„ç‰¹å®šæ¨¡å—åˆ°é«˜å…‰è°±ç‰©ä½“è·Ÿè¸ªä»»åŠ¡ã€‚</li>
<li>HEIå°†åŸå§‹å…‰è°±ä¿¡æ¯æ·»åŠ åˆ°è·Ÿè¸ªç½‘ç»œè¾“å…¥ä¸­ã€‚</li>
<li>æ–¹æ³•ç›´æ¥ä»é«˜å…‰è°±å›¾åƒæå–ä¿¡æ¯ï¼Œé¿å…äº†ä¿¡æ¯æŸå¤±ã€‚</li>
<li>ä»…å¾®è°ƒæ–°æ–¹æ³•å‚æ•°ï¼Œæé«˜äº†æ•ˆç‡ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.22199">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-ba5d3e1480edbfbc99588533976019cd.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-bbfe904f66cc8faa099981cae769faa0.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-64b71e23205080a93d48273b5b299c15.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8cf3288471fb7cbf78b459f2532a6c25.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Knowledge-Rectification-for-Camouflaged-Object-Detection-Unlocking-Insights-from-Low-Quality-Data"><a href="#Knowledge-Rectification-for-Camouflaged-Object-Detection-Unlocking-Insights-from-Low-Quality-Data" class="headerlink" title="Knowledge Rectification for Camouflaged Object Detection: Unlocking   Insights from Low-Quality Data"></a>Knowledge Rectification for Camouflaged Object Detection: Unlocking   Insights from Low-Quality Data</h2><p><strong>Authors:Juwei Guan, Xiaolin Fang, Donghyun Kim, Haotian Gong, Tongxin Zhu, Zhen Ling, Ming Yang</strong></p>
<p>Low-quality data often suffer from insufficient image details, introducing an extra implicit aspect of camouflage that complicates camouflaged object detection (COD). Existing COD methods focus primarily on high-quality data, overlooking the challenges posed by low-quality data, which leads to significant performance degradation. Therefore, we propose KRNet, the first framework explicitly designed for COD on low-quality data. KRNet presents a Leader-Follower framework where the Leader extracts dual gold-standard distributions: conditional and hybrid, from high-quality data to drive the Follower in rectifying knowledge learned from low-quality data. The framework further benefits from a cross-consistency strategy that improves the rectification of these distributions and a time-dependent conditional encoder that enriches the distribution diversity. Extensive experiments on benchmark datasets demonstrate that KRNet outperforms state-of-the-art COD methods and super-resolution-assisted COD approaches, proving its effectiveness in tackling the challenges of low-quality data in COD. </p>
<blockquote>
<p>ä½è´¨é‡æ•°æ®é€šå¸¸å­˜åœ¨å›¾åƒç»†èŠ‚ä¸è¶³çš„é—®é¢˜ï¼Œå¼•å…¥äº†ä¸€ç§é¢å¤–çš„éšæ€§ä¼ªè£…æ–¹é¢ï¼Œè¿™å¢åŠ äº†å¯¹éšè”½ç›®æ ‡æ£€æµ‹ï¼ˆCODï¼‰çš„å¤æ‚æ€§ã€‚ç°æœ‰çš„CODæ–¹æ³•ä¸»è¦å…³æ³¨é«˜è´¨é‡æ•°æ®ï¼Œå¿½è§†äº†ä½è´¨é‡æ•°æ®å¸¦æ¥çš„æŒ‘æˆ˜ï¼Œè¿™å¯¼è‡´äº†æ€§èƒ½çš„å¤§å¹…ä¸‹é™ã€‚å› æ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†KRNetï¼Œè¿™æ˜¯ç¬¬ä¸€ä¸ªä¸“ä¸ºä½è´¨é‡æ•°æ®è¿›è¡ŒCODè®¾è®¡çš„æ¡†æ¶ã€‚KRNeté‡‡ç”¨é¢†å¯¼è€…-è¿½éšè€…æ¡†æ¶ï¼Œå…¶ä¸­é¢†å¯¼è€…ä»é«˜è´¨é‡æ•°æ®ä¸­æå–åŒé‡‘æ ‡å‡†åˆ†å¸ƒï¼šæ¡ä»¶åˆ†å¸ƒå’Œæ··åˆåˆ†å¸ƒï¼Œä»¥é©±åŠ¨è¿½éšè€…çº æ­£ä»ä½è´¨é‡æ•°æ®ä¸­å­¦ä¹ çš„çŸ¥è¯†ã€‚è¯¥æ¡†æ¶è¿˜å—ç›Šäºäº¤å‰ä¸€è‡´æ€§ç­–ç•¥ï¼Œæé«˜äº†è¿™äº›åˆ†å¸ƒçš„æ ¡æ­£æ•ˆæœï¼Œä»¥åŠæ—¶é—´ç›¸å…³çš„æ¡ä»¶ç¼–ç å™¨ï¼Œä¸°å¯Œäº†åˆ†å¸ƒçš„å¤šæ ·æ€§ã€‚åœ¨åŸºå‡†æ•°æ®é›†ä¸Šçš„å¤§é‡å®éªŒè¡¨æ˜ï¼ŒKRNetä¼˜äºæœ€å…ˆè¿›çš„CODæ–¹æ³•å’Œè¶…åˆ†è¾¨ç‡è¾…åŠ©çš„CODæ–¹æ³•ï¼Œè¯æ˜äº†å…¶åœ¨è§£å†³CODä¸­ä½è´¨é‡æ•°æ®æŒ‘æˆ˜æ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.22180v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä¸»è¦é’ˆå¯¹ä½è´¨é‡æ•°æ®åœ¨éšè”½ç›®æ ‡æ£€æµ‹ä¸­çš„æŒ‘æˆ˜ï¼Œæå‡ºäº†KRNetæ¡†æ¶ã€‚è¯¥æ¡†æ¶é€šè¿‡é¢†å¯¼è€…ï¼ˆLeaderï¼‰ä»é«˜è´¨é‡æ•°æ®ä¸­æå–ä¸¤ç§é»„é‡‘æ ‡å‡†åˆ†å¸ƒï¼ˆæ¡ä»¶åˆ†å¸ƒå’Œæ··åˆåˆ†å¸ƒï¼‰ï¼Œé©±åŠ¨è·Ÿéšè€…ï¼ˆFollowerï¼‰çº æ­£ä»ä½è´¨é‡æ•°æ®ä¸­å­¦ä¹ çš„çŸ¥è¯†ã€‚æ­¤å¤–ï¼Œè¿˜é‡‡ç”¨äº†è·¨ä¸€è‡´æ€§ç­–ç•¥å’Œæ—¶é—´ä¾èµ–æ¡ä»¶ç¼–ç å™¨ï¼Œä»¥æé«˜åˆ†å¸ƒçš„ä¿®æ­£å’Œä¸°å¯Œåˆ†å¸ƒå¤šæ ·æ€§ã€‚å®éªŒè¯æ˜ï¼ŒKRNetåœ¨ä½è´¨é‡æ•°æ®çš„éšè”½ç›®æ ‡æ£€æµ‹æ–¹é¢ä¼˜äºæœ€æ–°çš„æŠ€æœ¯æ–¹æ³•å’Œè¶…åˆ†è¾¨ç‡è¾…åŠ©éšè”½ç›®æ ‡æ£€æµ‹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ä½è´¨é‡æ•°æ®å› å…¶å›¾åƒç»†èŠ‚ä¸è¶³è€Œå…·æœ‰éšè”½æ€§ï¼Œå¢åŠ äº†éšè”½ç›®æ ‡æ£€æµ‹çš„å¤æ‚æ€§ã€‚</li>
<li>å½“å‰éšè”½ç›®æ ‡æ£€æµ‹æ–¹æ³•ä¸»è¦å…³æ³¨é«˜è´¨é‡æ•°æ®ï¼Œå¿½è§†äº†ä½è´¨é‡æ•°æ®çš„æŒ‘æˆ˜ï¼Œå¯¼è‡´æ€§èƒ½ä¸‹é™ã€‚</li>
<li>KRNetæ˜¯é¦–ä¸ªä¸“ä¸ºä½è´¨é‡æ•°æ®éšè”½ç›®æ ‡æ£€æµ‹è®¾è®¡çš„æ¡†æ¶ã€‚</li>
<li>KRNeté‡‡ç”¨é¢†å¯¼è€…-è·Ÿéšè€…æ¡†æ¶ï¼Œé¢†å¯¼è€…ä»é«˜è´¨é‡æ•°æ®ä¸­æå–ä¸¤ç§é»„é‡‘æ ‡å‡†åˆ†å¸ƒæ¥æŒ‡å¯¼è·Ÿéšè€…çº æ­£ä»ä½è´¨é‡æ•°æ®ä¸­å­¦ä¹ çš„çŸ¥è¯†ã€‚</li>
<li>é‡‡ç”¨è·¨ä¸€è‡´æ€§ç­–ç•¥æ”¹å–„åˆ†å¸ƒçš„ä¿®æ­£è¿‡ç¨‹ã€‚</li>
<li>æ—¶é—´ä¾èµ–æ¡ä»¶ç¼–ç å™¨ä¸°å¯Œåˆ†å¸ƒå¤šæ ·æ€§ï¼Œè¿›ä¸€æ­¥æé«˜æ£€æµ‹æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.22180">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-3e0f704ad40b62748b6e04dafdbb7f75.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-cf34843cf916e2e34845d9706449d352.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-1212cadf99c82c93c87d058852523789.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-640bf3a75e98df5163ea0e54593ed140.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-b8b8cc0aea25592ff82a33f1d6fa4f3a.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="Beyond-Background-Shift-Rethinking-Instance-Replay-in-Continual-Semantic-Segmentation"><a href="#Beyond-Background-Shift-Rethinking-Instance-Replay-in-Continual-Semantic-Segmentation" class="headerlink" title="Beyond Background Shift: Rethinking Instance Replay in Continual   Semantic Segmentation"></a>Beyond Background Shift: Rethinking Instance Replay in Continual   Semantic Segmentation</h2><p><strong>Authors:Hongmei Yin, Tingliang Feng, Fan Lyu, Fanhua Shang, Hongying Liu, Wei Feng, Liang Wan</strong></p>
<p>In this work, we focus on continual semantic segmentation (CSS), where segmentation networks are required to continuously learn new classes without erasing knowledge of previously learned ones. Although storing images of old classes and directly incorporating them into the training of new models has proven effective in mitigating catastrophic forgetting in classification tasks, this strategy presents notable limitations in CSS. Specifically, the stored and new images with partial category annotations leads to confusion between unannotated categories and the background, complicating model fitting. To tackle this issue, this paper proposes a novel Enhanced Instance Replay (EIR) method, which not only preserves knowledge of old classes while simultaneously eliminating background confusion by instance storage of old classes, but also mitigates background shifts in the new images by integrating stored instances with new images. By effectively resolving background shifts in both stored and new images, EIR alleviates catastrophic forgetting in the CSS task, thereby enhancing the modelâ€™s capacity for CSS. Experimental results validate the efficacy of our approach, which significantly outperforms state-of-the-art CSS methods. </p>
<blockquote>
<p>åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬ä¸»è¦å…³æ³¨æŒç»­è¯­ä¹‰åˆ†å‰²ï¼ˆCSSï¼‰ï¼Œè¿™éœ€è¦åˆ†å‰²ç½‘ç»œèƒ½å¤Ÿåœ¨ä¸æ–­å­¦ä¹ æ–°ç±»åˆ«çš„åŒæ—¶ï¼Œä¸ä¼šé—å¿˜ä¹‹å‰å·²ç»å­¦è¿‡çš„çŸ¥è¯†ã€‚è™½ç„¶å­˜å‚¨æ—§ç±»åˆ«çš„å›¾åƒå¹¶å°†å…¶ç›´æ¥çº³å…¥æ–°æ¨¡å‹çš„è®­ç»ƒä¸­è¢«è¯æ˜å¯ä»¥æœ‰æ•ˆç¼“è§£åˆ†ç±»ä»»åŠ¡ä¸­çš„ç¾éš¾æ€§é—å¿˜é—®é¢˜ï¼Œä½†è¿™ç§ç­–ç•¥åœ¨CSSä¸­å´å­˜åœ¨æ˜æ˜¾çš„å±€é™æ€§ã€‚å…·ä½“æ¥è¯´ï¼Œå¸¦æœ‰éƒ¨åˆ†ç±»åˆ«æ³¨é‡Šçš„å­˜å‚¨å’Œæ–°å›¾åƒä¼šå¯¼è‡´æœªæ ‡æ³¨ç±»åˆ«ä¸èƒŒæ™¯ä¹‹é—´çš„æ··æ·†ï¼Œä½¿æ¨¡å‹æ‹Ÿåˆå¤æ‚åŒ–ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°å‹çš„å¢å¼ºå®ä¾‹å›æ”¾ï¼ˆEIRï¼‰æ–¹æ³•ï¼Œå®ƒä¸ä»…é€šè¿‡å­˜å‚¨æ—§ç±»çš„å®ä¾‹æ¥ä¿ç•™å¯¹æ—§ç±»çš„çŸ¥è¯†ï¼ŒåŒæ—¶æ¶ˆé™¤èƒŒæ™¯æ··æ·†ï¼Œè€Œä¸”è¿˜é€šè¿‡æ•´åˆå­˜å‚¨çš„å®ä¾‹ä¸æ–°å›¾åƒæ¥ç¼“è§£æ–°å›¾åƒä¸­çš„èƒŒæ™¯å˜åŒ–ã€‚é€šè¿‡æœ‰æ•ˆè§£å†³å­˜å‚¨å’Œæ–°å›¾åƒä¸­çš„èƒŒæ™¯å˜åŒ–é—®é¢˜ï¼ŒEIRå‡è½»äº†CSSä»»åŠ¡ä¸­çš„ç¾éš¾æ€§é—å¿˜é—®é¢˜ï¼Œä»è€Œå¢å¼ºäº†æ¨¡å‹è¿›è¡ŒCSSçš„èƒ½åŠ›ã€‚å®éªŒç»“æœéªŒè¯äº†æˆ‘ä»¬çš„æ–¹æ³•çš„æœ‰æ•ˆæ€§ï¼Œè¯¥æ–¹æ³•æ˜¾è‘—ä¼˜äºæœ€å…ˆè¿›çš„CSSæ–¹æ³•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.22136v1">PDF</a> </p>
<p><strong>Summary</strong><br>è¯¥è®ºæ–‡å…³æ³¨æŒç»­è¯­ä¹‰åˆ†å‰²ï¼ˆCSSï¼‰é—®é¢˜ï¼Œå…¶ä¸­è¦æ±‚åˆ†å‰²ç½‘ç»œèƒ½å¤Ÿè¿ç»­å­¦ä¹ æ–°ç±»åˆ«ï¼ŒåŒæ—¶ä¸é—å¿˜å·²å­¦çŸ¥è¯†ã€‚é’ˆå¯¹åˆ†ç±»ä»»åŠ¡ä¸­é€šè¿‡å­˜å‚¨æ—§ç±»åˆ«å›¾åƒå¹¶ç›´æ¥å°†å…¶çº³å…¥æ–°æ¨¡å‹è®­ç»ƒæ¥å‡è½»ç¾éš¾æ€§é—å¿˜çš„ç­–ç•¥åœ¨CSSä¸­çš„å±€é™æ€§ï¼Œè®ºæ–‡æå‡ºäº†ä¸€ç§æ–°å‹çš„å¢å¼ºå®ä¾‹å›æ”¾ï¼ˆEIRï¼‰æ–¹æ³•ã€‚EIRä¸ä»…é€šè¿‡å®ä¾‹å­˜å‚¨æ—§ç±»åˆ«æ¥ä¿ç•™å¯¹æ—§çŸ¥è¯†çš„äº†è§£ï¼ŒåŒæ—¶æ¶ˆé™¤èƒŒæ™¯æ··æ·†ï¼Œè¿˜é€šè¿‡æ•´åˆå­˜å‚¨çš„å®ä¾‹ä¸æ–°å›¾åƒæ¥å‡è½»æ–°å›¾åƒä¸­çš„èƒŒæ™¯åç§»ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒEIRæ–¹æ³•åœ¨CSSä»»åŠ¡ä¸Šæœ‰æ•ˆç¼“è§£ç¾éš¾æ€§é—å¿˜ï¼Œæ˜¾è‘—ä¼˜äºç°æœ‰CSSæ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è®ºæ–‡å…³æ³¨æŒç»­è¯­ä¹‰åˆ†å‰²ï¼ˆCSSï¼‰é—®é¢˜ï¼Œè¦æ±‚æ¨¡å‹åœ¨è¿ç»­å­¦ä¹ ä¸­èƒ½å¤Ÿè¯†åˆ«æ–°ç±»åˆ«å¹¶ä¿ç•™å¯¹æ—§çŸ¥è¯†çš„äº†è§£ã€‚</li>
<li>å­˜å‚¨æ—§ç±»åˆ«å›¾åƒå¹¶ç›´æ¥ç”¨äºæ–°æ¨¡å‹è®­ç»ƒçš„ç­–ç•¥åœ¨CSSä¸­å­˜åœ¨å±€é™æ€§ï¼Œä¼šå¼•èµ·ç±»åˆ«å’ŒèƒŒæ™¯ä¹‹é—´çš„æ··æ·†ã€‚</li>
<li>æå‡ºçš„å¢å¼ºå®ä¾‹å›æ”¾ï¼ˆEIRï¼‰æ–¹æ³•é€šè¿‡å®ä¾‹å­˜å‚¨æ—§ç±»åˆ«ï¼Œä»¥æ¶ˆé™¤èƒŒæ™¯æ··æ·†å¹¶ä¿ç•™å¯¹æ—§çŸ¥è¯†çš„äº†è§£ã€‚</li>
<li>EIRæ–¹æ³•è¿˜é€šè¿‡æ•´åˆå­˜å‚¨çš„å®ä¾‹ä¸æ–°å›¾åƒï¼Œå‡è½»æ–°å›¾åƒä¸­çš„èƒŒæ™¯åç§»é—®é¢˜ã€‚</li>
<li>EIRæ–¹æ³•æœ‰æ•ˆç¼“è§£CSSä»»åŠ¡ä¸­çš„ç¾éš¾æ€§é—å¿˜é—®é¢˜ã€‚</li>
<li>å®éªŒç»“æœè¡¨æ˜ï¼ŒEIRæ–¹æ³•åœ¨CSSä»»åŠ¡ä¸Šçš„è¡¨ç°æ˜¾è‘—ä¼˜äºç°æœ‰çš„æ–¹æ³•ã€‚</li>
<li>EIRæ–¹æ³•ä¸ºæŒç»­è¯­ä¹‰åˆ†å‰²æä¾›äº†ä¸€ä¸ªæœ‰æ•ˆçš„è§£å†³æ–¹æ¡ˆã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.22136">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-8b7d39460062fc58c052bd9e231c35c8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b92c99d0896783a36338a8b3ce8e6084.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6a34e0d2746c84352fdde50ba5c74ef3.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-4116d9683fbfe83a80f09b66277378ca.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="Semantic-segmentation-for-building-houses-from-wooden-cubes"><a href="#Semantic-segmentation-for-building-houses-from-wooden-cubes" class="headerlink" title="Semantic segmentation for building houses from wooden cubes"></a>Semantic segmentation for building houses from wooden cubes</h2><p><strong>Authors:Ivan Beleacov</strong></p>
<p>Automated construction is one of the most promising areas that can improve efficiency, reduce costs and minimize errors in the process of building construction. In this paper, a comparative analysis of three neural network models for semantic segmentation, U-Net(light), LinkNet and PSPNet, is performed. Two specialized datasets with images of houses built from wooden cubes were created for the experiments. The first dataset contains 4 classes (background, foundation, walls, roof ) and is designed for basic model evaluation, while the second dataset includes 44 classes where each cube is labeled as a separate object. The models were trained with the same hyperparameters and their accuracy was evaluated using MeanIoU and F1 Score metrics. According to the results obtained, U-Net(light) showed the best performance with 78% MeanIoU and 87% F1 Score on the first dataset and 17% and 25% respectively on the second dataset. The poor results on the second dataset are due to the limited amount of data, the complexity of the partitioning and the imbalance of classes, making it difficult to accurately select individual cubes. In addition, overtraining was observed in all experiments, manifested by high accuracy on the training dataset and its significant decrease on the validation dataset. The present work is the basis for the development of algorithms for automatic generation of staged building plans, which can be further scaled to design complete buildings. Future research is planned to extend the datasets and apply methods to combat overfitting (L1&#x2F;L2 regularization, Early Stopping). The next stage of work will be the development of algorithms for automatic generation of a step-by-step plan for building houses from cubes using manipulators. Index Terms-Deep Learning, Computer vision, CNN, Semantic segmentation, Construction materials. </p>
<blockquote>
<p>è‡ªåŠ¨åŒ–å»ºé€ æ˜¯ä¸€ä¸ªèƒ½å¤Ÿæé«˜å»ºé€ æ•ˆç‡ã€é™ä½æˆæœ¬å¹¶å‡å°‘é”™è¯¯çš„é¢†åŸŸä¹‹ä¸€ã€‚æœ¬æ–‡æ¯”è¾ƒåˆ†æäº†ä¸‰ç§ç”¨äºè¯­ä¹‰åˆ†å‰²çš„ç¥ç»ç½‘ç»œæ¨¡å‹ï¼Œå³U-Netï¼ˆè½»é‡çº§ï¼‰ã€LinkNetå’ŒPSPNetã€‚ä¸ºäº†å®éªŒï¼Œåˆ›å»ºäº†ä¸¤ç»„åŒ…å«ç”±æœ¨æ–¹å—å»ºé€ çš„æˆ¿å±‹çš„å›¾ç‰‡æ•°æ®é›†ã€‚ç¬¬ä¸€ç»„æ•°æ®é›†åŒ…å«å››ç±»ï¼ˆèƒŒæ™¯ã€åœ°åŸºã€å¢™å£ã€å±‹é¡¶ï¼‰ï¼Œç”¨äºåŸºç¡€æ¨¡å‹è¯„ä¼°ï¼›è€Œç¬¬äºŒç»„æ•°æ®é›†åˆ™åŒ…å«44ç±»ï¼Œå…¶ä¸­æ¯ä¸ªæœ¨æ–¹å—éƒ½è¢«æ ‡è®°ä¸ºä¸€ä¸ªå•ç‹¬çš„å¯¹è±¡ã€‚è¿™äº›æ¨¡å‹é‡‡ç”¨ç›¸åŒçš„è¶…å‚æ•°è¿›è¡Œè®­ç»ƒï¼Œå¹¶ä½¿ç”¨MeanIoUå’ŒF1åˆ†æ•°æŒ‡æ ‡å¯¹å…¶å‡†ç¡®æ€§è¿›è¡Œè¯„ä¼°ã€‚æ ¹æ®ç»“æœï¼ŒU-Netï¼ˆè½»é‡çº§ï¼‰è¡¨ç°æœ€ä½³ï¼Œåœ¨ç¬¬ä¸€ç»„æ•°æ®é›†ä¸Šè¾¾åˆ°78%çš„MeanIoUå’Œ87%çš„F1åˆ†æ•°ï¼Œè€Œåœ¨ç¬¬äºŒç»„æ•°æ®é›†ä¸Šåˆ†åˆ«ä¸º17%å’Œ25%ã€‚ç¬¬äºŒç»„æ•°æ®çš„ç»“æœä¸ä½³æ˜¯ç”±äºæ•°æ®é‡æœ‰é™ã€åˆ†åŒºå¤æ‚ä»¥åŠç±»åˆ«ä¸å¹³è¡¡ï¼Œå¯¼è‡´éš¾ä»¥å‡†ç¡®é€‰æ‹©å•ä¸ªæœ¨æ–¹å—ã€‚æ­¤å¤–ï¼Œæ‰€æœ‰å®éªŒä¸­å‡è§‚å¯Ÿåˆ°è¿‡æ‹Ÿåˆç°è±¡ï¼Œè¡¨ç°ä¸ºè®­ç»ƒé›†ä¸Šçš„å‡†ç¡®æ€§å¾ˆé«˜ï¼Œè€Œåœ¨éªŒè¯é›†ä¸Šçš„å‡†ç¡®æ€§å´å¤§å¹…ä¸‹é™ã€‚ç›®å‰çš„å·¥ä½œæ˜¯å¼€å‘ç”¨äºè‡ªåŠ¨ç”Ÿæˆåˆ†é˜¶æ®µå»ºç­‘è®¡åˆ’çš„ç®—æ³•çš„åŸºç¡€ï¼Œè¿™äº›ç®—æ³•å¯ä»¥è¿›ä¸€æ­¥æ‰©å±•ä»¥è®¾è®¡å®Œæ•´çš„å»ºç­‘ç‰©ã€‚æœªæ¥çš„ç ”ç©¶è®¡åˆ’æ˜¯æ‰©å±•æ•°æ®é›†å¹¶åº”ç”¨æ–¹æ³•æ¥å¯¹æŠ—è¿‡æ‹Ÿåˆï¼ˆL1&#x2F;L2æ­£åˆ™åŒ–ã€æ—©æœŸåœæ­¢ï¼‰ã€‚ä¸‹ä¸€é˜¶æ®µçš„å·¥ä½œå°†æ˜¯å¼€å‘ç®—æ³•ï¼Œä»¥è‡ªåŠ¨ç”Ÿæˆä½¿ç”¨æ“çºµå™¨ä»æœ¨æ–¹å—å»ºé€ æˆ¿å±‹çš„é€æ­¥è®¡åˆ’ã€‚å…³é”®è¯ï¼šæ·±åº¦å­¦ä¹ ã€è®¡ç®—æœºè§†è§‰ã€å·ç§¯ç¥ç»ç½‘ç»œã€è¯­ä¹‰åˆ†å‰²ã€å»ºç­‘ææ–™ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.22125v1">PDF</a> 10 pages, 6 figures, 2 tables</p>
<p><strong>Summary</strong><br>     æœ¬æ–‡æ¯”è¾ƒåˆ†æäº†ä¸‰ç§ç¥ç»ç½‘ç»œæ¨¡å‹ï¼ˆU-Net(light)ã€LinkNetå’ŒPSPNetï¼‰åœ¨è¯­ä¹‰åˆ†å‰²æ–¹é¢çš„è¡¨ç°ï¼Œåº”ç”¨äºè‡ªåŠ¨åŒ–å»ºç­‘é¢†åŸŸã€‚å®éªŒé‡‡ç”¨åŒ…å«æœ¨åˆ¶ç«‹æ–¹ä½“å»ºé€ æˆ¿å±‹å›¾åƒçš„ä¸¤ä¸ªä¸“ç”¨æ•°æ®é›†ï¼Œç»“æœU-Net(light)è¡¨ç°æœ€ä½³ã€‚è¿™ä¸ºå¼€å‘ç”¨äºè‡ªåŠ¨ç”Ÿæˆåˆ†é˜¶æ®µå»ºç­‘è®¡åˆ’çš„ç®—æ³•å¥ å®šäº†åŸºç¡€ï¼Œæœªæ¥ç ”ç©¶å°†æ‰©å±•æ•°æ®é›†å¹¶åº”ç”¨æ–¹æ³•å¯¹æŠ—è¿‡æ‹Ÿåˆé—®é¢˜ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è‡ªåŠ¨åŒ–å»ºç­‘é¢†åŸŸå…·æœ‰æé«˜æ•ˆç‡ã€é™ä½æˆæœ¬å’Œå‡å°‘é”™è¯¯çš„æ½œåŠ›ã€‚</li>
<li>ä¸‰ç§ç¥ç»ç½‘ç»œæ¨¡å‹ï¼ˆU-Net(light)ã€LinkNetå’ŒPSPNetï¼‰åœ¨è¯­ä¹‰åˆ†å‰²æ–¹é¢è¿›è¡Œæ¯”è¾ƒåˆ†æã€‚</li>
<li>é‡‡ç”¨åŒ…å«æœ¨åˆ¶ç«‹æ–¹ä½“å»ºé€ æˆ¿å±‹å›¾åƒçš„ä¸¤ä¸ªä¸“ç”¨æ•°æ®é›†è¿›è¡Œå®éªŒã€‚</li>
<li>U-Net(light)åœ¨å®éªŒä¸­è¡¨ç°æœ€ä½³ï¼Œåœ¨åŸºæœ¬æ•°æ®é›†ä¸Šè¾¾åˆ°78%çš„MeanIoUå’Œ87%çš„F1åˆ†æ•°ã€‚</li>
<li>ç¬¬äºŒæ•°æ®é›†çš„è¾ƒå·®ç»“æœæ˜¯ç”±äºæ•°æ®é‡æœ‰é™ã€åˆ†åŒºå¤æ‚ä»¥åŠç±»åˆ«ä¸å¹³è¡¡æ‰€å¯¼è‡´çš„ã€‚</li>
<li>æ‰€æœ‰å®éªŒä¸­è§‚å¯Ÿåˆ°è¿‡æ‹Ÿåˆç°è±¡ï¼Œæœªæ¥ç ”ç©¶è®¡åˆ’åŒ…æ‹¬æ‰©å±•æ•°æ®é›†å¹¶åº”ç”¨æ–¹æ³•å¯¹æŠ—è¿‡æ‹Ÿåˆï¼ˆå¦‚L1&#x2F;L2æ­£åˆ™åŒ–ã€æ—©æœŸåœæ­¢ï¼‰ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.22125">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-e0a84eeb7316c1d72754841099c194ef.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-613a6d5f8f4f3263d441726e3b950fc6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-042b429d757b2089a899a8b55f80b960.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d177301a17dd9639489ebf7236d62890.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-67b866c09ac906311b61da365d6a34ff.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="A-Deep-Learning-Framework-for-Boundary-Aware-Semantic-Segmentation"><a href="#A-Deep-Learning-Framework-for-Boundary-Aware-Semantic-Segmentation" class="headerlink" title="A Deep Learning Framework for Boundary-Aware Semantic Segmentation"></a>A Deep Learning Framework for Boundary-Aware Semantic Segmentation</h2><p><strong>Authors:Tai An, Weiqiang Huang, Da Xu, Qingyuan He, Jiacheng Hu, Yujia Lou</strong></p>
<p>As a fundamental task in computer vision, semantic segmentation is widely applied in fields such as autonomous driving, remote sensing image analysis, and medical image processing. In recent years, Transformer-based segmentation methods have demonstrated strong performance in global feature modeling. However, they still struggle with blurred target boundaries and insufficient recognition of small targets. To address these issues, this study proposes a Mask2Former-based semantic segmentation algorithm incorporating a boundary enhancement feature bridging module (BEFBM). The goal is to improve target boundary accuracy and segmentation consistency. Built upon the Mask2Former framework, this method constructs a boundary-aware feature map and introduces a feature bridging mechanism. This enables effective cross-scale feature fusion, enhancing the modelâ€™s ability to focus on target boundaries. Experiments on the Cityscapes dataset demonstrate that, compared to mainstream segmentation methods, the proposed approach achieves significant improvements in metrics such as mIOU, mDICE, and mRecall. It also exhibits superior boundary retention in complex scenes. Visual analysis further confirms the modelâ€™s advantages in fine-grained regions. Future research will focus on optimizing computational efficiency and exploring its potential in other high-precision segmentation tasks. </p>
<blockquote>
<p>è¯­ä¹‰åˆ†å‰²æ˜¯è®¡ç®—æœºè§†è§‰ä¸­çš„ä¸€é¡¹åŸºæœ¬ä»»åŠ¡ï¼Œå¹¿æ³›åº”ç”¨äºè‡ªåŠ¨é©¾é©¶ã€é¥æ„Ÿå›¾åƒåˆ†æå’ŒåŒ»å­¦å›¾åƒå¤„ç†ç­‰é¢†åŸŸã€‚è¿‘å¹´æ¥ï¼ŒåŸºäºTransformerçš„åˆ†å‰²æ–¹æ³•åœ¨å…¨å±€ç‰¹å¾å»ºæ¨¡æ–¹é¢è¡¨ç°å‡ºå¼ºå¤§çš„æ€§èƒ½ã€‚ç„¶è€Œï¼Œå®ƒä»¬ä»ç„¶é¢ä¸´ç›®æ ‡è¾¹ç•Œæ¨¡ç³Šä»¥åŠå°ç›®æ ‡è¯†åˆ«ä¸è¶³çš„é—®é¢˜ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§åŸºäºMask2Formerçš„è¯­ä¹‰åˆ†å‰²ç®—æ³•ï¼Œè¯¥ç®—æ³•èå…¥äº†ä¸€ä¸ªè¾¹ç•Œå¢å¼ºç‰¹å¾æ¡¥æ¥æ¨¡å—ï¼ˆBEFBMï¼‰ï¼Œæ—¨åœ¨æé«˜ç›®æ ‡è¾¹ç•Œçš„å‡†ç¡®æ€§å’Œåˆ†å‰²çš„ä¸€è‡´æ€§ã€‚è¯¥æ–¹æ³•åŸºäºMask2Formeræ¡†æ¶ï¼Œæ„å»ºäº†ä¸€ä¸ªè¾¹ç•Œæ„ŸçŸ¥ç‰¹å¾å›¾ï¼Œå¹¶å¼•å…¥äº†ä¸€ç§ç‰¹å¾æ¡¥æ¥æœºåˆ¶ï¼Œèƒ½å¤Ÿå®ç°æœ‰æ•ˆçš„è·¨å°ºåº¦ç‰¹å¾èåˆï¼Œå¢å¼ºæ¨¡å‹å¯¹ç›®æ ‡è¾¹ç•Œçš„å…³æ³¨åº¦ã€‚åœ¨åŸå¸‚æ™¯è§‚æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œä¸ä¸»æµåˆ†å‰²æ–¹æ³•ç›¸æ¯”ï¼Œè¯¥æ–¹æ³•åœ¨mIOUã€mDICEå’ŒmRecallç­‰æŒ‡æ ‡ä¸Šå–å¾—äº†æ˜¾è‘—çš„æ”¹è¿›ï¼Œå¹¶ä¸”åœ¨å¤æ‚åœºæ™¯ä¸­è¡¨ç°å‡ºä¼˜è¶Šçš„è¾¹ç•Œä¿æŒèƒ½åŠ›ã€‚å¯è§†åŒ–åˆ†æè¿›ä¸€æ­¥è¯å®äº†è¯¥æ¨¡å‹åœ¨ç»†èŠ‚åŒºåŸŸçš„ä¼˜åŠ¿ã€‚æœªæ¥çš„ç ”ç©¶å°†é‡ç‚¹å…³æ³¨è®¡ç®—æ•ˆç‡çš„ä¼˜åŒ–ï¼Œå¹¶æ¢ç´¢å…¶åœ¨å…¶ä»–é«˜ç²¾åº¦åˆ†å‰²ä»»åŠ¡ä¸­çš„æ½œåŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.22050v1">PDF</a> </p>
<p><strong>Summary</strong><br>åŸºäºTransformerçš„è¯­ä¹‰åˆ†å‰²æ–¹æ³•å·²å¹¿æ³›åº”ç”¨äºè‡ªåŠ¨é©¾é©¶ã€é¥æ„Ÿå›¾åƒåˆ†æå’ŒåŒ»ç–—å›¾åƒå¤„ç†ç­‰é¢†åŸŸã€‚ä¸ºè§£å†³ç›®æ ‡è¾¹ç•Œæ¨¡ç³Šå’Œå°ç›®æ ‡è¯†åˆ«ä¸è¶³çš„é—®é¢˜ï¼Œæœ¬ç ”ç©¶æå‡ºä¸€ç§åŸºäºMask2Formerçš„è¯­ä¹‰åˆ†å‰²ç®—æ³•ï¼Œèå…¥è¾¹ç•Œå¢å¼ºç‰¹å¾æ¡¥æ¥æ¨¡å—ï¼ˆBEFBMï¼‰ï¼Œæ—¨åœ¨æé«˜ç›®æ ‡è¾¹ç•Œç²¾åº¦å’Œåˆ†å‰²ä¸€è‡´æ€§ã€‚å®éªŒè¯æ˜ï¼Œè¯¥æ–¹æ³•åœ¨Cityscapesæ•°æ®é›†ä¸Šè¾ƒä¸»æµåˆ†å‰²æ–¹æ³•æœ‰æ˜æ˜¾æå‡ï¼Œå°¤å…¶åœ¨mIOUã€mDICEå’ŒmRecallç­‰æŒ‡æ ‡ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œå¤æ‚åœºæ™¯ä¸‹çš„è¾¹ç•Œä¿ç•™èƒ½åŠ›ä¹Ÿæ›´å¼ºã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>è¯­ä¹‰åˆ†å‰²æ˜¯è®¡ç®—æœºè§†è§‰ä¸­çš„åŸºç¡€ä»»åŠ¡ï¼Œå¹¿æ³›åº”ç”¨äºå¤šä¸ªé¢†åŸŸã€‚</li>
<li>åŸºäºTransformerçš„åˆ†å‰²æ–¹æ³•å·²å±•ç°å‡ºè‰²çš„å…¨å±€ç‰¹å¾å»ºæ¨¡èƒ½åŠ›ã€‚</li>
<li>è¾¹ç•Œå¢å¼ºç‰¹å¾æ¡¥æ¥æ¨¡å—ï¼ˆBEFBMï¼‰è¢«å¼•å…¥ä»¥è§£å†³ç›®æ ‡è¾¹ç•Œæ¨¡ç³Šå’Œå°ç›®æ ‡è¯†åˆ«é—®é¢˜ã€‚</li>
<li>Mask2Formeræ¡†æ¶è¢«ç”¨æ¥æ„å»ºè¾¹ç•Œæ„ŸçŸ¥ç‰¹å¾å›¾ï¼Œå¹¶å¼•å…¥ç‰¹å¾æ¡¥æ¥æœºåˆ¶ï¼Œå®ç°è·¨å°ºåº¦ç‰¹å¾èåˆã€‚</li>
<li>åœ¨Cityscapesæ•°æ®é›†ä¸Šçš„å®éªŒè¯æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å¤šä¸ªæŒ‡æ ‡ä¸Šè¾ƒä¸»æµåˆ†å‰²æ–¹æ³•æœ‰æ˜¾è‘—æå‡ã€‚</li>
<li>è¯¥æ–¹æ³•åœ¨å¤æ‚åœºæ™¯ä¸‹çš„è¾¹ç•Œä¿ç•™èƒ½åŠ›è¾ƒå¼ºã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.22050">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-6c995351648a322289f1745d53ad5b7c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3893e0aa513138268d4d169e2200db81.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-88bdb2ce58fa6789749714ce25a7449b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-4d45cbe611cae52e058c0548d80c1b6e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-623b2a46a23b379a4ee66f42c7716b22.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="BOOTPLACE-Bootstrapped-Object-Placement-with-Detection-Transformers"><a href="#BOOTPLACE-Bootstrapped-Object-Placement-with-Detection-Transformers" class="headerlink" title="BOOTPLACE: Bootstrapped Object Placement with Detection Transformers"></a>BOOTPLACE: Bootstrapped Object Placement with Detection Transformers</h2><p><strong>Authors:Hang Zhou, Xinxin Zuo, Rui Ma, Li Cheng</strong></p>
<p>In this paper, we tackle the copy-paste image-to-image composition problem with a focus on object placement learning. Prior methods have leveraged generative models to reduce the reliance for dense supervision. However, this often limits their capacity to model complex data distributions. Alternatively, transformer networks with a sparse contrastive loss have been explored, but their over-relaxed regularization often leads to imprecise object placement. We introduce BOOTPLACE, a novel paradigm that formulates object placement as a placement-by-detection problem. Our approach begins by identifying suitable regions of interest for object placement. This is achieved by training a specialized detection transformer on object-subtracted backgrounds, enhanced with multi-object supervisions. It then semantically associates each target compositing object with detected regions based on their complementary characteristics. Through a boostrapped training approach applied to randomly object-subtracted images, our model enforces meaningful placements through extensive paired data augmentation. Experimental results on established benchmarks demonstrate BOOTPLACEâ€™s superior performance in object repositioning, markedly surpassing state-of-the-art baselines on Cityscapes and OPA datasets with notable improvements in IOU scores. Additional ablation studies further showcase the compositionality and generalizability of our approach, supported by user study evaluations. </p>
<blockquote>
<p>æœ¬æ–‡ç€é‡è§£å†³å›¾åƒå¤åˆ¶ç²˜è´´çš„å›¾åƒç»„åˆé—®é¢˜ï¼Œé‡ç‚¹å…³æ³¨å¯¹è±¡æ”¾ç½®å­¦ä¹ ã€‚å…ˆå‰çš„æ–¹æ³•å·²ç»åˆ©ç”¨ç”Ÿæˆæ¨¡å‹æ¥å‡å°‘å¯¹å¯†é›†ç›‘ç£çš„ä¾èµ–ï¼Œä½†è¿™å¾€å¾€é™åˆ¶äº†å®ƒä»¬å¯¹å¤æ‚æ•°æ®åˆ†å¸ƒçš„å»ºæ¨¡èƒ½åŠ›ã€‚è™½ç„¶ä¹Ÿæ¢ç´¢äº†å¸¦æœ‰ç¨€ç–å¯¹æ¯”æŸå¤±çš„è½¬æ¢å™¨ç½‘ç»œï¼Œä½†å®ƒä»¬è¿‡äºå®½æ¾çš„è§„åˆ™åŒ–å¸¸å¸¸å¯¼è‡´å¯¹è±¡æ”¾ç½®ä¸å‡†ç¡®ã€‚æˆ‘ä»¬å¼•å…¥äº†BOOTPLACEï¼Œè¿™æ˜¯ä¸€ç§å°†å¯¹è±¡æ”¾ç½®åˆ¶å®šä¸ºæ£€æµ‹å®šä½é—®é¢˜çš„æ–°å‹èŒƒå¼ã€‚æˆ‘ä»¬çš„æ–¹æ³•é¦–å…ˆè¯†åˆ«é€‚åˆå¯¹è±¡æ”¾ç½®çš„å…´è¶£åŒºåŸŸã€‚è¿™æ˜¯åœ¨å‡å»èƒŒæ™¯å¯¹è±¡çš„å›¾åƒä¸Šè®­ç»ƒä¸“ç”¨æ£€æµ‹è½¬æ¢å™¨å®ç°çš„ï¼Œå¹¶ç”¨å¤šç›®æ ‡ç›‘ç£å¢å¼ºã€‚ç„¶åï¼Œå®ƒæ ¹æ®ç›®æ ‡å¤åˆå¯¹è±¡çš„è¡¥å……ç‰¹å¾ï¼Œå°†è¿™äº›ç›®æ ‡ä¸æ£€æµ‹åˆ°çš„åŒºåŸŸè¿›è¡Œè¯­ä¹‰å…³è”ã€‚é€šè¿‡å¯¹éšæœºå‡å»å¯¹è±¡çš„å›¾åƒåº”ç”¨å¼•å¯¼è®­ç»ƒæ³•ï¼Œæˆ‘ä»¬çš„æ¨¡å‹é€šè¿‡å¤§é‡çš„é…å¯¹æ•°æ®å¢å¼ºæ¥æ‰§è¡Œæœ‰æ„ä¹‰çš„æ”¾ç½®ã€‚åœ¨åŸºå‡†æµ‹è¯•ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼ŒBOOTPLACEåœ¨å¯¹è±¡é‡æ–°å®šä½æ–¹é¢è¡¨ç°å‡ºå“è¶Šæ€§èƒ½ï¼Œåœ¨Cityscapeså’ŒOPAæ•°æ®é›†ä¸Šæ˜¾è‘—è¶…è¶Šäº†æœ€æ–°åŸºçº¿æŠ€æœ¯ï¼Œå¹¶åœ¨IOUå¾—åˆ†ä¸Šå–å¾—äº†æ˜¾è‘—çš„æ”¹è¿›ã€‚é¢å¤–çš„æ¶ˆèç ”ç©¶è¿›ä¸€æ­¥å±•ç¤ºäº†æˆ‘ä»¬çš„æ–¹æ³•çš„ç»„åˆæ€§å’Œé€šç”¨æ€§ï¼Œå¹¶å¾—åˆ°ç”¨æˆ·ç ”ç©¶è¯„ä¼°çš„æ”¯æŒã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.21991v1">PDF</a> CVPR 2025. Project page: <a target="_blank" rel="noopener" href="https://ryanhangzhou.github.io/bootplace/">https://ryanhangzhou.github.io/bootplace/</a> ,   code: <a target="_blank" rel="noopener" href="https://github.com/RyanHangZhou/BOOTPLACE">https://github.com/RyanHangZhou/BOOTPLACE</a></p>
<p><strong>Summary</strong><br>æœ¬æ–‡è§£å†³å¤åˆ¶ç²˜è´´å›¾åƒç»„åˆé—®é¢˜ï¼Œé‡ç‚¹å…³æ³¨å¯¹è±¡æ”¾ç½®å­¦ä¹ ã€‚å¼•å…¥BOOTPLACEæ–¹æ³•ï¼Œå°†å¯¹è±¡æ”¾ç½®é—®é¢˜è½¬åŒ–ä¸ºæ£€æµ‹é—®é¢˜ã€‚é€šè¿‡è®­ç»ƒç‰¹å®šæ£€æµ‹å™¨åœ¨ç§»é™¤å¯¹è±¡çš„èƒŒæ™¯ä¸Šè¯†åˆ«åˆé€‚çš„æ„Ÿå…´è¶£åŒºåŸŸï¼Œå®ç°ç›®æ ‡åˆæˆå¯¹è±¡çš„è¯­ä¹‰å…³è”ã€‚é‡‡ç”¨éšæœºç§»é™¤å¯¹è±¡çš„å›¾åƒè¿›è¡Œå¼•å¯¼è®­ç»ƒï¼Œé€šè¿‡å¹¿æ³›çš„æ•°æ®å¢å¼ºå®ç°æœ‰æ„ä¹‰æ”¾ç½®ã€‚åœ¨Cityscapeså’ŒOPAæ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼ŒBOOTPLACEåœ¨å¯¹è±¡é‡æ–°å®šä½æ–¹é¢è¡¨ç°å‡ºå“è¶Šæ€§èƒ½ï¼Œæ˜¾è‘—ä¼˜äºç°æœ‰æŠ€æœ¯åŸºçº¿ï¼Œå¹¶å¾—åˆ°äº†ç”¨æˆ·ç ”ç©¶çš„æ”¯æŒã€‚</p>
<p><strong>Key Takeaways</strong></p>
<p>ä»¥ä¸‹æ˜¯æ–‡æœ¬çš„ä¸»è¦è§‚ç‚¹ä¸è§è§£ï¼š</p>
<ul>
<li>æœ¬æ–‡ä¸»è¦è§£å†³å¤åˆ¶ç²˜è´´å›¾åƒç»„åˆé—®é¢˜ï¼Œä¾§é‡äºå¯¹è±¡æ”¾ç½®å­¦ä¹ ã€‚</li>
<li>å¼•å…¥äº†ä¸€ç§æ–°å‹æ–¹æ³•BOOTPLACEï¼Œå°†å¯¹è±¡æ”¾ç½®é—®é¢˜è½¬åŒ–ä¸ºæ£€æµ‹é—®é¢˜ã€‚</li>
<li>é€šè¿‡è®­ç»ƒç‰¹å®šæ£€æµ‹å™¨è¯†åˆ«æ„Ÿå…´è¶£åŒºåŸŸï¼Œå®ç°ç›®æ ‡åˆæˆå¯¹è±¡çš„è¯­ä¹‰å…³è”ã€‚</li>
<li>é‡‡ç”¨éšæœºç§»é™¤å¯¹è±¡çš„å›¾åƒè¿›è¡Œå¼•å¯¼è®­ç»ƒï¼Œå¢å¼ºæ¨¡å‹å¯¹æœ‰æ„ä¹‰æ”¾ç½®çš„ç†è§£ã€‚</li>
<li>åœ¨Cityscapeså’ŒOPAæ•°æ®é›†ä¸Šçš„å®éªŒéªŒè¯äº†BOOTPLACEåœ¨å¯¹è±¡é‡æ–°å®šä½æ–¹é¢çš„å“è¶Šæ€§èƒ½ã€‚</li>
<li>BOOTPLACEæ˜¾è‘—ä¼˜äºç°æœ‰æŠ€æœ¯åŸºçº¿ï¼Œæé«˜äº†IOUåˆ†æ•°ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.21991">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-816c6c1c89891425eeff717fb5fa7e11.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d948d33954f5af3b580e4e2ce304de66.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-dc97229bd9cafbf95cbee4be5aba59bf.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-26b8b0b967991a7c11e9b8f838906992.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-5ddd67c7db95bc3fb6410cdc7454c97f.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="Exponentially-Weighted-Instance-Aware-Repeat-Factor-Sampling-for-Long-Tailed-Object-Detection-Model-Training-in-Unmanned-Aerial-Vehicles-Surveillance-Scenarios"><a href="#Exponentially-Weighted-Instance-Aware-Repeat-Factor-Sampling-for-Long-Tailed-Object-Detection-Model-Training-in-Unmanned-Aerial-Vehicles-Surveillance-Scenarios" class="headerlink" title="Exponentially Weighted Instance-Aware Repeat Factor Sampling for   Long-Tailed Object Detection Model Training in Unmanned Aerial Vehicles   Surveillance Scenarios"></a>Exponentially Weighted Instance-Aware Repeat Factor Sampling for   Long-Tailed Object Detection Model Training in Unmanned Aerial Vehicles   Surveillance Scenarios</h2><p><strong>Authors:Taufiq Ahmed, Abhishek Kumar, Constantino Ãlvarez Casado, Anlan Zhang, Tuomo HÃ¤nninen, Lauri Loven, Miguel Bordallo LÃ³pez, Sasu Tarkoma</strong></p>
<p>Object detection models often struggle with class imbalance, where rare categories appear significantly less frequently than common ones. Existing sampling-based rebalancing strategies, such as Repeat Factor Sampling (RFS) and Instance-Aware Repeat Factor Sampling (IRFS), mitigate this issue by adjusting sample frequencies based on image and instance counts. However, these methods are based on linear adjustments, which limit their effectiveness in long-tailed distributions. This work introduces Exponentially Weighted Instance-Aware Repeat Factor Sampling (E-IRFS), an extension of IRFS that applies exponential scaling to better differentiate between rare and frequent classes. E-IRFS adjusts sampling probabilities using an exponential function applied to the geometric mean of image and instance frequencies, ensuring a more adaptive rebalancing strategy. We evaluate E-IRFS on a dataset derived from the Fireman-UAV-RGBT Dataset and four additional public datasets, using YOLOv11 object detection models to identify fire, smoke, people and lakes in emergency scenarios. The results show that E-IRFS improves detection performance by 22% over the baseline and outperforms RFS and IRFS, particularly for rare categories. The analysis also highlights that E-IRFS has a stronger effect on lightweight models with limited capacity, as these models rely more on data sampling strategies to address class imbalance. The findings demonstrate that E-IRFS improves rare object detection in resource-constrained environments, making it a suitable solution for real-time applications such as UAV-based emergency monitoring. </p>
<blockquote>
<p>å¯¹è±¡æ£€æµ‹æ¨¡å‹ç»å¸¸é¢ä¸´ç±»åˆ«ä¸å¹³è¡¡çš„é—®é¢˜ï¼Œå…¶ä¸­ç¨€æœ‰ç±»åˆ«çš„å‡ºç°é¢‘ç‡è¿œè¿œä½äºå¸¸è§ç±»åˆ«ã€‚ç°æœ‰çš„åŸºäºé‡‡æ ·çš„å†å¹³è¡¡ç­–ç•¥ï¼Œå¦‚é‡å¤å› å­é‡‡æ ·ï¼ˆRFSï¼‰å’Œå®ä¾‹æ„ŸçŸ¥é‡å¤å› å­é‡‡æ ·ï¼ˆIRFSï¼‰ï¼Œé€šè¿‡æ ¹æ®å›¾åƒå’Œå®ä¾‹è®¡æ•°è°ƒæ•´æ ·æœ¬é¢‘ç‡æ¥ç¼“è§£è¿™ä¸ªé—®é¢˜ã€‚ç„¶è€Œï¼Œè¿™äº›æ–¹æ³•åŸºäºçº¿æ€§è°ƒæ•´ï¼Œåœ¨é•¿å°¾åˆ†å¸ƒä¸­é™åˆ¶äº†å…¶æœ‰æ•ˆæ€§ã€‚æœ¬æ–‡å¼•å…¥äº†åŠ æƒå®ä¾‹æ„ŸçŸ¥é‡å¤å› å­é‡‡æ ·ï¼ˆE-IRFSï¼‰ï¼Œè¿™æ˜¯IRFSçš„æ‰©å±•ï¼Œåº”ç”¨æŒ‡æ•°ç¼©æ”¾æ¥æ›´å¥½åœ°åŒºåˆ†ç¨€æœ‰ç±»åˆ«å’Œå¸¸è§ç±»åˆ«ã€‚E-IRFSä½¿ç”¨åº”ç”¨äºå›¾åƒå’Œå®ä¾‹é¢‘ç‡å‡ ä½•å‡å€¼çš„æŒ‡æ•°å‡½æ•°æ¥è°ƒæ•´é‡‡æ ·æ¦‚ç‡ï¼Œç¡®ä¿æ›´è‡ªé€‚åº”çš„å†å¹³è¡¡ç­–ç•¥ã€‚æˆ‘ä»¬åœ¨ä»æ¶ˆé˜²æ— äººæœºRGBTæ•°æ®é›†æ´¾ç”Ÿçš„æ•°æ®é›†å’Œå››å¥—å…¬å¼€æ•°æ®é›†ä¸Šè¯„ä¼°äº†E-IRFSï¼Œä½¿ç”¨YOLOv11ç›®æ ‡æ£€æµ‹æ¨¡å‹æ¥è¯†åˆ«ç´§æ€¥åœºæ™¯ä¸­çš„ç«ç¾ã€çƒŸé›¾ã€äººå‘˜å’Œæ¹–æ³Šã€‚ç»“æœè¡¨æ˜ï¼ŒE-IRFSåœ¨åŸºçº¿åŸºç¡€ä¸Šæé«˜äº†22%çš„æ£€æµ‹æ€§èƒ½ï¼Œå¹¶ä¸”åœ¨ç¨€æœ‰ç±»åˆ«ä¸Šä¼˜äºRFSå’ŒIRFSã€‚åˆ†æè¿˜è¡¨æ˜ï¼ŒE-IRFSå¯¹å®¹é‡æœ‰é™çš„è½»é‡çº§æ¨¡å‹å½±å“æ›´å¤§ï¼Œå› ä¸ºè¿™äº›æ¨¡å‹æ›´ä¾èµ–æ•°æ®é‡‡æ ·ç­–ç•¥æ¥è§£å†³ç±»åˆ«ä¸å¹³è¡¡é—®é¢˜ã€‚ç ”ç©¶ç»“æœè¡¨æ˜ï¼ŒE-IRFSæ”¹è¿›äº†åœ¨èµ„æºå—é™ç¯å¢ƒä¸­ç¨€æœ‰å¯¹è±¡çš„æ£€æµ‹ï¼Œä½¿å…¶æˆä¸ºé€‚ç”¨äºå®æ—¶åº”ç”¨ï¼ˆå¦‚æ— äººæœºåº”æ€¥ç›‘æµ‹ï¼‰çš„åˆé€‚è§£å†³æ–¹æ¡ˆã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.21893v1">PDF</a> 6 pages, 2 figures, 9 tables, 6 formulas, conference paper</p>
<p><strong>Summary</strong>ï¼šè¯¥æ–‡æœ¬ä»‹ç»äº†ä¸€ç§è§£å†³ç›®æ ‡æ£€æµ‹æ¨¡å‹ä¸­ç±»åˆ«ä¸å¹³è¡¡é—®é¢˜çš„æ–¹æ³•â€”â€”æŒ‡æ•°åŠ æƒå®ä¾‹æ„ŸçŸ¥é‡å¤å› å­é‡‡æ ·ï¼ˆE-IRFSï¼‰ã€‚E-IRFSé€šè¿‡åº”ç”¨æŒ‡æ•°ç¼©æ”¾æ¥è°ƒæ•´æ ·æœ¬æ¦‚ç‡ï¼Œæ›´å¥½åœ°åŒºåˆ†ç¨€æœ‰ç±»å’Œå¸¸è§ç±»ã€‚åœ¨ç´§æ€¥åœºæ™¯ä¸‹çš„æ•°æ®é›†æµ‹è¯•ä¸­ï¼ŒE-IRFSæé«˜äº†ç›®æ ‡æ£€æµ‹æ€§èƒ½ï¼Œç‰¹åˆ«æ˜¯åœ¨è¯†åˆ«ç«æƒ…ã€çƒŸé›¾ã€äººå‘˜å’Œæ¹–æ³Šç­‰ç¨€æœ‰ç±»åˆ«æ—¶è¡¨ç°æ›´ä½³ã€‚åˆ†ææ˜¾ç¤ºï¼ŒE-IRFSå¯¹å®¹é‡æœ‰é™çš„è½»é‡çº§æ¨¡å‹æ•ˆæœæ›´å¼ºï¼Œé€‚ç”¨äºèµ„æºå—é™ç¯å¢ƒä¸­çš„å®æ—¶åº”ç”¨ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>ç›®æ ‡æ£€æµ‹æ¨¡å‹é¢ä¸´ç±»åˆ«ä¸å¹³è¡¡é—®é¢˜ï¼Œç¨€æœ‰ç±»åˆ«çš„å‡ºç°é¢‘ç‡è¿œä½äºå¸¸è§ç±»åˆ«ã€‚</li>
<li>ç°æœ‰é‡‡æ ·é‡å¹³è¡¡ç­–ç•¥å¦‚RFSå’ŒIRFSé€šè¿‡è°ƒæ•´æ ·æœ¬é¢‘ç‡æ¥è§£å†³æ­¤é—®é¢˜ã€‚</li>
<li>E-IRFSæ˜¯IRFSçš„æ‰©å±•ï¼Œåº”ç”¨æŒ‡æ•°ç¼©æ”¾ä»¥æ›´å¥½åœ°åŒºåˆ†ç¨€æœ‰å’Œå¸¸è§ç±»åˆ«ã€‚</li>
<li>E-IRFSé€šè¿‡è°ƒæ•´é‡‡æ ·æ¦‚ç‡ï¼Œä½¿ç”¨å›¾åƒå’Œå®ä¾‹é¢‘ç‡çš„å‡ ä½•å¹³å‡å€¼è¿›è¡ŒæŒ‡æ•°å‡½æ•°åº”ç”¨ã€‚</li>
<li>E-IRFSåœ¨å¤šä¸ªæ•°æ®é›†ä¸Šçš„è¯„ä¼°è¡¨æ˜ï¼Œä¸åŸºå‡†æ–¹æ³•ç›¸æ¯”ï¼Œå®ƒæé«˜äº†ç›®æ ‡æ£€æµ‹æ€§èƒ½22%ã€‚</li>
<li>E-IRFSåœ¨è¯†åˆ«ç¨€æœ‰ç±»åˆ«æ—¶è¡¨ç°ä¼˜å¼‚ï¼Œç‰¹åˆ«æ˜¯åœ¨ç´§æ€¥åœºæ™¯ä¸‹çš„æ•°æ®é›†æµ‹è¯•ä¸­ã€‚</li>
<li>åˆ†ææ˜¾ç¤ºï¼ŒE-IRFSå¯¹è½»é‡çº§æ¨¡å‹çš„å½±å“æ›´å¤§ï¼Œè¿™äº›æ¨¡å‹æ›´ä¾èµ–äºæ•°æ®é‡‡æ ·ç­–ç•¥æ¥è§£å†³ç±»åˆ«ä¸å¹³è¡¡é—®é¢˜ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.21893">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-8637ed433cc0bd19ce9c09e7f4f7aab4.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-1c7aaca286e8f1a8f1674d4554efbe54.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b55651f340ce55a383d82a3a1bd0b0be.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-42b2ff30e8fe270f3ec14ee25272bc11.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-75ff8b70f7cd67f52b8ec73204bdeeb9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ab8dcccd61d247491960c02b4a45b645.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9424ddd42d742c628e8bab08d4b88023.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a4f00907528ce8223493c4dd54edbf67.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-510c5ca606aaee0030c4682ca5609f36.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="Cyclic-Contrastive-Knowledge-Transfer-for-Open-Vocabulary-Object-Detection"><a href="#Cyclic-Contrastive-Knowledge-Transfer-for-Open-Vocabulary-Object-Detection" class="headerlink" title="Cyclic Contrastive Knowledge Transfer for Open-Vocabulary Object   Detection"></a>Cyclic Contrastive Knowledge Transfer for Open-Vocabulary Object   Detection</h2><p><strong>Authors:Chuhan Zhang, Chaoyang Zhu, Pingcheng Dong, Long Chen, Dong Zhang</strong></p>
<p>In pursuit of detecting unstinted objects that extend beyond predefined categories, prior arts of open-vocabulary object detection (OVD) typically resort to pretrained vision-language models (VLMs) for base-to-novel category generalization. However, to mitigate the misalignment between upstream image-text pretraining and downstream region-level perception, additional supervisions are indispensable, eg, image-text pairs or pseudo annotations generated via self-training strategies. In this work, we propose CCKT-Det trained without any extra supervision. The proposed framework constructs a cyclic and dynamic knowledge transfer from language queries and visual region features extracted from VLMs, which forces the detector to closely align with the visual-semantic space of VLMs. Specifically, 1) we prefilter and inject semantic priors to guide the learning of queries, and 2) introduce a regional contrastive loss to improve the awareness of queries on novel objects. CCKT-Det can consistently improve performance as the scale of VLMs increases, all while requiring the detector at a moderate level of computation overhead. Comprehensive experimental results demonstrate that our method achieves performance gain of +2.9% and +10.2% AP50 over previous state-of-the-arts on the challenging COCO benchmark, both without and with a stronger teacher model. </p>
<blockquote>
<p>åœ¨è¿½æ±‚æ£€æµ‹ä¸å—é™åˆ¶çš„ã€è¶…å‡ºé¢„å®šç±»åˆ«çš„ç‰©ä½“æ—¶ï¼Œå¼€æ”¾è¯æ±‡è¡¨å¯¹è±¡æ£€æµ‹ï¼ˆOVDï¼‰çš„ç°æœ‰æŠ€æœ¯é€šå¸¸ä¼šå€ŸåŠ©é¢„è®­ç»ƒçš„è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰æ¥å®ç°åŸºç¡€åˆ°æ–°é¢–ç±»åˆ«çš„æ¨å¹¿ã€‚ç„¶è€Œï¼Œä¸ºäº†å‡è½»ä¸Šæ¸¸å›¾åƒæ–‡æœ¬é¢„è®­ç»ƒå’Œä¸‹æ¸¸åŒºåŸŸçº§åˆ«æ„ŸçŸ¥ä¹‹é—´çš„ä¸ä¸€è‡´æ€§ï¼Œé¢å¤–çš„ç›‘ç£æ˜¯å¿…ä¸å¯å°‘çš„ï¼Œä¾‹å¦‚é€šè¿‡è‡ªè®­ç»ƒç­–ç•¥ç”Ÿæˆçš„å›¾åƒæ–‡æœ¬å¯¹æˆ–ä¼ªæ³¨é‡Šã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†æ— éœ€ä»»ä½•é¢å¤–ç›‘ç£çš„CCKT-Detã€‚æ‰€æå‡ºçš„æ¡†æ¶æ„å»ºäº†ä»è¯­è¨€æŸ¥è¯¢å’Œä»VLMsæå–çš„è§†è§‰åŒºåŸŸç‰¹å¾ä¹‹é—´çš„å¾ªç¯å’ŒåŠ¨æ€çŸ¥è¯†è½¬ç§»ï¼Œè¿™è¿«ä½¿æ£€æµ‹å™¨ç´§å¯†åœ°å¯¹åº”äºVLMsçš„è§†è§‰è¯­ä¹‰ç©ºé—´ã€‚å…·ä½“æ¥è¯´ï¼Œ1ï¼‰æˆ‘ä»¬ä¼˜å…ˆè¿‡æ»¤å¹¶æ³¨å…¥è¯­ä¹‰å…ˆéªŒæ¥æŒ‡å¯¼æŸ¥è¯¢å­¦ä¹ ï¼Œ2ï¼‰å¼•å…¥åŒºåŸŸå¯¹æ¯”æŸå¤±æ¥æé«˜æŸ¥è¯¢å¯¹æ–°ç‰©ä½“çš„æ„è¯†ã€‚éšç€VLMsè§„æ¨¡çš„å¢åŠ ï¼ŒCCKT-Detå¯ä»¥æŒç»­æé«˜æ€§èƒ½ï¼ŒåŒæ—¶åªéœ€é€‚åº¦å¢åŠ æ£€æµ‹å™¨çš„è®¡ç®—å¼€é”€ã€‚ç»¼åˆå®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨å…·æœ‰æŒ‘æˆ˜æ€§çš„COCOåŸºå‡†æµ‹è¯•ä¸Šï¼Œç›¸è¾ƒäºä¹‹å‰çš„æœ€ä¼˜æ–¹æ³•ï¼Œå®ç°äº†+2.9%å’Œ+10.2%çš„AP50æ€§èƒ½æå‡ï¼Œæ— è®ºæ˜¯å¦ä½¿ç”¨æ›´å¼ºçš„æ•™å¸ˆæ¨¡å‹ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.11005v2">PDF</a> 10 pages, 5 figures, Published as a conference paper at ICLR 2025</p>
<p><strong>Summary</strong>ï¼šä¸ºæé«˜å¼€æ”¾è¯æ±‡è¡¨å¯¹è±¡æ£€æµ‹çš„å‡†ç¡®ç‡ï¼Œå…ˆå‰æŠ€æœ¯ä¸»è¦å€ŸåŠ©é¢„è®­ç»ƒçš„è·¨åª’ä½“æ¨¡å‹ï¼ˆVLMsï¼‰è¿›è¡ŒåŸºç¡€åˆ°æ–°é¢–ç±»åˆ«æ¨å¹¿ã€‚æœ¬ç ”ç©¶æå‡ºæ— éœ€é¢å¤–ç›‘ç£çš„CCKT-Detæ¡†æ¶ï¼Œé€šè¿‡æ„å»ºå¾ªç¯åŠ¨æ€çŸ¥è¯†è½¬ç§»æœºåˆ¶ï¼Œä»è¯­è¨€æŸ¥è¯¢å’Œè§†è§‰åŒºåŸŸç‰¹å¾ä¸­å­¦ä¹ ã€‚é€šè¿‡é¢„ç­›é€‰å’Œæ³¨å…¥è¯­ä¹‰å…ˆéªŒæ¥å¼•å¯¼æŸ¥è¯¢å­¦ä¹ ï¼Œå¼•å…¥åŒºåŸŸå¯¹æ¯”æŸå¤±æé«˜æŸ¥è¯¢å¯¹æ–°é¢–å¯¹è±¡çš„æ„ŸçŸ¥èƒ½åŠ›ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¯¥æ–¹æ³•åœ¨COCOåŸºå‡†æµ‹è¯•ä¸­æ€§èƒ½æå‡æ˜¾è‘—ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>å¼€æ”¾è¯æ±‡è¡¨å¯¹è±¡æ£€æµ‹ï¼ˆOVDï¼‰é¢ä¸´ä»è¯­è¨€åˆ°è§†è§‰çš„è·¨åª’ä½“å¯¹é½æŒ‘æˆ˜ã€‚</li>
<li>ç°æœ‰æ–¹æ³•ä¾èµ–é¢„è®­ç»ƒçš„è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰è¿›è¡Œç±»åˆ«æ¨å¹¿ã€‚</li>
<li>CCKT-Detæ¡†æ¶é€šè¿‡å¾ªç¯åŠ¨æ€çŸ¥è¯†è½¬ç§»æé«˜æ£€æµ‹æ€§èƒ½ï¼Œæ— éœ€é¢å¤–ç›‘ç£ã€‚<br>4.CCKT-Detåˆ©ç”¨è¯­ä¹‰å…ˆéªŒå¼•å¯¼æŸ¥è¯¢å­¦ä¹ ï¼Œæé«˜æŸ¥è¯¢å¯¹æ–°é¢–å¯¹è±¡çš„æ„ŸçŸ¥èƒ½åŠ›ã€‚</li>
<li>é€šè¿‡å¼•å…¥åŒºåŸŸå¯¹æ¯”æŸå¤±æ¥å¼ºåŒ–æ¨¡å‹çš„è¾¨åˆ«èƒ½åŠ›ã€‚</li>
<li>éšç€VLMè§„æ¨¡çš„æ‰©å¤§ï¼ŒCCKT-Detçš„æ€§èƒ½å¯æŒç»­æå‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.11005">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-f11a0cf70369ea35fba688dc75a791b6.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b53f7289550afffe64a2fbaffc09ba83.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ae6dad6363a1ebbfb2f9eb47258ffab1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5f35ec1971db6907184083f874be5745.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="Improving-SAM-for-Camouflaged-Object-Detection-via-Dual-Stream-Adapters"><a href="#Improving-SAM-for-Camouflaged-Object-Detection-via-Dual-Stream-Adapters" class="headerlink" title="Improving SAM for Camouflaged Object Detection via Dual Stream Adapters"></a>Improving SAM for Camouflaged Object Detection via Dual Stream Adapters</h2><p><strong>Authors:Jiaming Liu, Linghe Kong, Guihai Chen</strong></p>
<p>Segment anything model (SAM) has shown impressive general-purpose segmentation performance on natural images, but its performance on camouflaged object detection (COD) is unsatisfactory. In this paper, we propose SAM-COD that performs camouflaged object detection for RGB-D inputs. While keeping the SAM architecture intact, dual stream adapters are expanded on the image encoder to learn potential complementary information from RGB images and depth images, and fine-tune the mask decoder and its depth replica to perform dual-stream mask prediction. In practice, the dual stream adapters are embedded into the attention block of the image encoder in a parallel manner to facilitate the refinement and correction of the two types of image embeddings. To mitigate channel discrepancies arising from dual stream embeddings that do not directly interact with each other, we augment the association of dual stream embeddings using bidirectional knowledge distillation including a model distiller and a modal distiller. In addition, to predict the masks for RGB and depth attention maps, we hybridize the two types of image embeddings which are jointly learned with the prompt embeddings to update the initial prompt, and then feed them into the mask decoders to synchronize the consistency of image embeddings and prompt embeddings. Experimental results on four COD benchmarks show that our SAM-COD achieves excellent detection performance gains over SAM and achieves state-of-the-art results with a given fine-tuning paradigm. </p>
<blockquote>
<p>åˆ†æ®µä»»ä½•æ¨¡å‹ï¼ˆSAMï¼‰åœ¨è‡ªç„¶å›¾åƒä¸Šçš„é€šç”¨åˆ†å‰²æ€§èƒ½ä»¤äººå°è±¡æ·±åˆ»ï¼Œä½†åœ¨ä¼ªè£…ç›®æ ‡æ£€æµ‹ï¼ˆCODï¼‰æ–¹é¢çš„è¡¨ç°å´ä¸ä»¤äººæ»¡æ„ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†SAM-CODï¼Œè¯¥æ¨¡å‹é’ˆå¯¹RGB-Dè¾“å…¥æ‰§è¡Œä¼ªè£…ç›®æ ‡æ£€æµ‹ã€‚åœ¨ä¿æŒSAMæ¶æ„å®Œæ•´æ€§çš„åŒæ—¶ï¼Œåœ¨å›¾åƒç¼–ç å™¨ä¸Šæ‰©å±•äº†åŒæµé€‚é…å™¨ï¼Œä»¥ä»RGBå›¾åƒå’Œæ·±åº¦å›¾åƒä¸­å­¦ä¹ æ½œåœ¨çš„äº’è¡¥ä¿¡æ¯ï¼Œå¹¶å¾®è°ƒæ©è†œè§£ç å™¨åŠå…¶æ·±åº¦å‰¯æœ¬ä»¥æ‰§è¡ŒåŒæµæ©è†œé¢„æµ‹ã€‚åœ¨å®è·µä¸­ï¼ŒåŒæµé€‚é…å™¨ä»¥å¹¶è¡Œæ–¹å¼åµŒå…¥åˆ°å›¾åƒç¼–ç å™¨çš„æ³¨æ„åŠ›å—ä¸­ï¼Œä»¥ä¿ƒè¿›ä¸¤ç§å›¾åƒåµŒå…¥çš„ç»†åŒ–å’Œæ ¡æ­£ã€‚ä¸ºäº†å‡è½»ç”±äºåŒæµåµŒå…¥ä¸ç›´æ¥ç›¸äº’äº¤äº’è€Œäº§ç”Ÿçš„é€šé“å·®å¼‚ï¼Œæˆ‘ä»¬ä½¿ç”¨åŒå‘çŸ¥è¯†è’¸é¦å¢å¼ºåŒæµåµŒå…¥çš„å…³è”ï¼ŒåŒ…æ‹¬æ¨¡å‹è’¸é¦å™¨å’Œæ¨¡æ€è’¸é¦å™¨ã€‚æ­¤å¤–ï¼Œä¸ºäº†é¢„æµ‹RGBå’Œæ·±åº¦æ³¨æ„åŠ›å›¾çš„æ©è†œï¼Œæˆ‘ä»¬æ··åˆäº†ä¸¤ç§ç±»å‹çš„å›¾åƒåµŒå…¥ï¼Œè¿™äº›åµŒå…¥ä¸æç¤ºåµŒå…¥ä¸€èµ·è”åˆå­¦ä¹ ä»¥æ›´æ–°åˆå§‹æç¤ºï¼Œç„¶åå°†å…¶è¾“å…¥åˆ°æ©è†œè§£ç å™¨ä¸­ï¼Œä»¥åŒæ­¥å›¾åƒåµŒå…¥å’Œæç¤ºåµŒå…¥çš„ä¸€è‡´æ€§ã€‚åœ¨å››ä¸ªCODåŸºå‡†æµ‹è¯•ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„SAM-CODåœ¨SAMä¸Šå®ç°äº†å‡ºè‰²çš„æ£€æµ‹æ€§èƒ½æå‡ï¼Œå¹¶åœ¨ç»™å®šçš„å¾®è°ƒèŒƒå¼ä¸‹è¾¾åˆ°äº†æœ€å…ˆè¿›çš„æˆæœã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.06042v2">PDF</a> </p>
<p><strong>Summary</strong>ï¼šSAMæ¨¡å‹åœ¨è‡ªç„¶å›¾åƒåˆ†å‰²æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œä½†åœ¨ä¼ªè£…ç›®æ ‡æ£€æµ‹ï¼ˆCODï¼‰ä¸Šçš„è¡¨ç°ä¸ä½³ã€‚æœ¬æ–‡æå‡ºSAM-CODæ¨¡å‹ï¼Œè¯¥æ¨¡å‹åœ¨RGB-Dè¾“å…¥ä¸Šæ‰§è¡Œä¼ªè£…ç›®æ ‡æ£€æµ‹ã€‚é€šè¿‡æ‰©å±•SAMæ¶æ„ï¼Œå¼•å…¥åŒæµé€‚é…å™¨å­¦ä¹ RGBå›¾åƒå’Œæ·±åº¦å›¾åƒä¹‹é—´çš„æ½œåœ¨äº’è¡¥ä¿¡æ¯ï¼Œå¹¶å¾®è°ƒæ©è†œè§£ç å™¨åŠå…¶æ·±åº¦å‰¯æœ¬ä»¥æ‰§è¡ŒåŒæµæ©è†œé¢„æµ‹ã€‚å®è·µä¸­ï¼Œé‡‡ç”¨åŒå‘çŸ¥è¯†è’¸é¦æŠ€æœ¯å…³è”åŒæµåµŒå…¥ä¿¡æ¯ä»¥æé«˜å‡†ç¡®æ€§ï¼ŒåŒæ—¶ä½¿ç”¨ä¸¤ç§ä¸åŒçš„æ©è†œç¼–ç å™¨ç”ŸæˆRGBå’Œæ·±åº¦æ³¨æ„åŠ›å›¾ã€‚å®éªŒç»“æœè¯æ˜ï¼ŒSAM-CODåœ¨ä¼ªè£…ç›®æ ‡æ£€æµ‹æ–¹é¢å–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ï¼Œå¹¶è¾¾åˆ°äº†ç»™å®šå¾®è°ƒæ¨¡å¼çš„æœ€ä½³çŠ¶æ€ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>SAMæ¨¡å‹åœ¨è‡ªç„¶å›¾åƒåˆ†å‰²ä¸Šè¡¨ç°è‰¯å¥½ï¼Œä½†åœ¨ä¼ªè£…ç›®æ ‡æ£€æµ‹ï¼ˆCODï¼‰ä¸Šçš„æ€§èƒ½ä¸ä½³ã€‚</li>
<li>æå‡ºSAM-CODæ¨¡å‹ä»¥æ”¹è¿›åœ¨CODä¸Šçš„æ€§èƒ½ã€‚</li>
<li>é€šè¿‡æ‰©å±•SAMæ¶æ„å¹¶å¼•å…¥åŒæµé€‚é…å™¨å­¦ä¹ RGBå’Œæ·±åº¦å›¾åƒçš„äº’è¡¥ä¿¡æ¯ã€‚</li>
<li>é‡‡ç”¨åŒå‘çŸ¥è¯†è’¸é¦æŠ€æœ¯å…³è”åŒæµåµŒå…¥ä¿¡æ¯ï¼Œä»¥æé«˜æ£€æµ‹çš„å‡†ç¡®æ€§ã€‚</li>
<li>åŒæµé€‚é…å™¨åµŒå…¥åˆ°å›¾åƒç¼–ç å™¨çš„æ³¨æ„åŠ›å—ä¸­ï¼Œä»¥ä¼˜åŒ–å’Œæ ¡æ­£ä¸¤ç§ç±»å‹çš„å›¾åƒåµŒå…¥ä¿¡æ¯ã€‚</li>
<li>ä½¿ç”¨ä¸¤ç§æ©è†œç¼–ç å™¨ç”ŸæˆRGBå’Œæ·±åº¦æ³¨æ„åŠ›å›¾ä»¥è¿›è¡Œæ©è†œé¢„æµ‹ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.06042">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-f4a64b31f9a975d12a6e798398a8ba7c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-479e5b1bde5db53fb39683e4e19e6919.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ae5efcc20a8ed57ea31ab1cdf8148495.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-17c794269379ec97436955f10cc75d30.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="Generative-Semantic-Communication-for-Joint-Image-Transmission-and-Segmentation"><a href="#Generative-Semantic-Communication-for-Joint-Image-Transmission-and-Segmentation" class="headerlink" title="Generative Semantic Communication for Joint Image Transmission and   Segmentation"></a>Generative Semantic Communication for Joint Image Transmission and   Segmentation</h2><p><strong>Authors:Weiwen Yuan, Jinke Ren, Chongjie Wang, Ruichen Zhang, Jun Wei, Dong In Kim, Shuguang Cui</strong></p>
<p>Semantic communication has emerged as a promising technology for enhancing communication efficiency. However, most existing research emphasizes single-task reconstruction, neglecting model adaptability and generalization across multi-task systems. In this paper, we propose a novel generative semantic communication system that supports both image reconstruction and segmentation tasks. Our approach builds upon semantic knowledge bases (KBs) at both the transmitter and receiver, with each semantic KB comprising a source KB and a task KB. The source KB at the transmitter leverages a hierarchical Swin-Transformer, a generative AI scheme, to extract multi-level features from the input image. Concurrently, the counterpart source KB at the receiver utilizes hierarchical residual blocks to generate task-specific knowledge. Furthermore, the task KBs adopt a semantic similarity model to map different task requirements into pre-defined task instructions, thereby facilitating the feature selection of the source KBs. Additionally, we develop a unified residual block-based joint source and channel (JSCC) encoder and two task-specific JSCC decoders to achieve the two image tasks. In particular, a generative diffusion model is adopted to construct the JSCC decoder for the image reconstruction task. Experimental results show that our multi-task generative semantic communication system outperforms previous single-task communication systems in terms of peak signal-to-noise ratio and segmentation accuracy. </p>
<blockquote>
<p>è¯­ä¹‰é€šä¿¡ä½œä¸ºä¸€ç§æé«˜é€šä¿¡æ•ˆç‡çš„æœ‰å‰æ™¯çš„æŠ€æœ¯å·²ç»å´­éœ²å¤´è§’ã€‚ç„¶è€Œï¼Œç°æœ‰çš„å¤§å¤šæ•°ç ”ç©¶å¼ºè°ƒå•ä¸€ä»»åŠ¡çš„é‡å»ºï¼Œå¿½è§†äº†æ¨¡å‹åœ¨å¤šä»»åŠ¡ç³»ç»Ÿä¸­çš„é€‚åº”æ€§å’Œæ³›åŒ–èƒ½åŠ›ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ”¯æŒå›¾åƒé‡å»ºå’Œåˆ†å‰²ä»»åŠ¡çš„æ–°å‹ç”Ÿæˆå¼è¯­ä¹‰é€šä¿¡ç³»ç»Ÿã€‚æˆ‘ä»¬çš„æ–¹æ³•å»ºç«‹åœ¨å‘å°„å™¨å’Œæ¥æ”¶å™¨åŒæ–¹çš„è¯­ä¹‰çŸ¥è¯†åº“ï¼ˆKBsï¼‰ä¹‹ä¸Šï¼Œæ¯ä¸ªè¯­ä¹‰çŸ¥è¯†åº“éƒ½ç”±æºçŸ¥è¯†åº“å’Œä»»åŠ¡çŸ¥è¯†åº“ç»„æˆã€‚å‘å°„å™¨çš„æºçŸ¥è¯†åº“åˆ©ç”¨åˆ†å±‚Swin-Transformerï¼ˆä¸€ç§ç”Ÿæˆå¼äººå·¥æ™ºèƒ½æ–¹æ¡ˆï¼‰ä»è¾“å…¥å›¾åƒä¸­æå–å¤šå±‚æ¬¡ç‰¹å¾ã€‚åŒæ—¶ï¼Œæ¥æ”¶å™¨çš„å¯¹åº”æºçŸ¥è¯†åº“åˆ©ç”¨åˆ†å±‚æ®‹å·®å—ç”Ÿæˆç‰¹å®šä»»åŠ¡çš„çŸ¥è¯†ã€‚æ­¤å¤–ï¼Œä»»åŠ¡çŸ¥è¯†åº“é‡‡ç”¨è¯­ä¹‰ç›¸ä¼¼æ€§æ¨¡å‹ï¼Œå°†ä¸åŒçš„ä»»åŠ¡è¦æ±‚æ˜ å°„åˆ°é¢„å®šä¹‰çš„ä»»åŠ¡æŒ‡ä»¤ä¸­ï¼Œä»è€Œä¿ƒè¿›äº†æºçŸ¥è¯†åº“çš„ç‰¹å¾é€‰æ‹©ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å¼€å‘äº†ä¸€ç§åŸºäºç»Ÿä¸€æ®‹å·®å—çš„è”åˆæºä¿¡é“ï¼ˆJSCCï¼‰ç¼–ç å™¨ï¼Œä»¥åŠä¸¤ä¸ªé’ˆå¯¹ç‰¹å®šä»»åŠ¡çš„JSCCè§£ç å™¨ï¼Œä»¥å®ç°ä¸¤ä¸ªå›¾åƒä»»åŠ¡ã€‚ç‰¹åˆ«æ˜¯ï¼Œé‡‡ç”¨ç”Ÿæˆæ‰©æ•£æ¨¡å‹æ„å»ºå›¾åƒé‡å»ºä»»åŠ¡çš„JSCCè§£ç å™¨ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„å¤šä»»åŠ¡ç”Ÿæˆè¯­ä¹‰é€šä¿¡ç³»ç»Ÿç›¸è¾ƒäºä¹‹å‰çš„å•ä»»åŠ¡é€šä¿¡ç³»ç»Ÿï¼Œåœ¨å³°å€¼ä¿¡å™ªæ¯”å’Œåˆ†å‰²å‡†ç¡®åº¦æ–¹é¢è¡¨ç°å‡ºæ›´ä¼˜çš„æ€§èƒ½ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2411.18005v2">PDF</a> This paper has been accepted by the 2025 IEEE International   Conference on Communications Workshops and is scheduled for publication</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºä¸€ç§æ”¯æŒå›¾åƒé‡å»ºå’Œåˆ†å‰²ä»»åŠ¡çš„å¤šä»»åŠ¡ç”Ÿæˆè¯­ä¹‰é€šä¿¡ç³»ç»Ÿã€‚è¯¥ç³»ç»ŸåŸºäºå‘é€å™¨å’Œæ¥æ”¶å™¨ç«¯çš„è¯­ä¹‰çŸ¥è¯†åº“ï¼Œåˆ©ç”¨åˆ†å±‚Swin-Transformerå’Œç”ŸæˆAIæ–¹æ¡ˆæå–è¾“å…¥å›¾åƒçš„å¤šå±‚æ¬¡ç‰¹å¾ï¼Œå¹¶é€šè¿‡ä»»åŠ¡ç‰¹å®šçš„çŸ¥è¯†åº“å®ç°ä»»åŠ¡æ˜ å°„å’Œç‰¹å¾é€‰æ‹©ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥ç³»ç»Ÿåœ¨å³°å€¼ä¿¡å™ªæ¯”å’Œåˆ†å‰²ç²¾åº¦æ–¹é¢ä¼˜äºå•ä¸€ä»»åŠ¡çš„é€šä¿¡ç³»ç»Ÿã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æå‡ºäº†ä¸€ç§å¤šä»»åŠ¡ç”Ÿæˆè¯­ä¹‰é€šä¿¡ç³»ç»Ÿï¼Œæ”¯æŒå›¾åƒé‡å»ºå’Œåˆ†å‰²ä»»åŠ¡ã€‚</li>
<li>ç³»ç»ŸåŸºäºè¯­ä¹‰çŸ¥è¯†åº“ï¼ŒåŒ…æ‹¬æºçŸ¥è¯†åº“å’Œä»»åŠ¡çŸ¥è¯†åº“ã€‚</li>
<li>å‘é€ç«¯çš„æºçŸ¥è¯†åº“åˆ©ç”¨åˆ†å±‚Swin-Transformeræå–å¤šå±‚æ¬¡ç‰¹å¾ï¼Œç”ŸæˆAIæ–¹æ¡ˆç”¨äºå¤„ç†ã€‚</li>
<li>æ¥æ”¶ç«¯çš„æºçŸ¥è¯†åº“åˆ©ç”¨å±‚æ¬¡æ®‹å·®å—ç”Ÿæˆä»»åŠ¡ç‰¹å®šçŸ¥è¯†ã€‚</li>
<li>ä»»åŠ¡çŸ¥è¯†åº“é‡‡ç”¨è¯­ä¹‰ç›¸ä¼¼æ€§æ¨¡å‹å°†ä¸åŒä»»åŠ¡è¦æ±‚æ˜ å°„åˆ°é¢„å®šä¹‰çš„ä»»åŠ¡æŒ‡ä»¤ä¸­ã€‚</li>
<li>å¼€å‘äº†åŸºäºç»Ÿä¸€æ®‹å·®å—çš„è”åˆæºå’Œä¿¡é“ç¼–ç å™¨ä»¥åŠä¸¤ä¸ªä»»åŠ¡ç‰¹å®šçš„è§£ç å™¨ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2411.18005">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-9954e5e590104bf9e3b226a82767f13c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ceda23b32157ea9af0c70f52db91f9e6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-95b2d0fb29b18389b1c180b7a3c48137.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-39a70f5911a0556872659f7ee9a02171.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-448d441d154f05e8cb03795f5cda9c1f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-8da7749eb87e468e6736dd20015fc896.jpg" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="Effective-SAM-Combination-for-Open-Vocabulary-Semantic-Segmentation"><a href="#Effective-SAM-Combination-for-Open-Vocabulary-Semantic-Segmentation" class="headerlink" title="Effective SAM Combination for Open-Vocabulary Semantic Segmentation"></a>Effective SAM Combination for Open-Vocabulary Semantic Segmentation</h2><p><strong>Authors:Minhyeok Lee, Suhwan Cho, Jungho Lee, Sunghun Yang, Heeseung Choi, Ig-Jae Kim, Sangyoun Lee</strong></p>
<p>Open-vocabulary semantic segmentation aims to assign pixel-level labels to images across an unlimited range of classes. Traditional methods address this by sequentially connecting a powerful mask proposal generator, such as the Segment Anything Model (SAM), with a pre-trained vision-language model like CLIP. But these two-stage approaches often suffer from high computational costs, memory inefficiencies. In this paper, we propose ESC-Net, a novel one-stage open-vocabulary segmentation model that leverages the SAM decoder blocks for class-agnostic segmentation within an efficient inference framework. By embedding pseudo prompts generated from image-text correlations into SAMâ€™s promptable segmentation framework, ESC-Net achieves refined spatial aggregation for accurate mask predictions. ESC-Net achieves superior performance on standard benchmarks, including ADE20K, PASCAL-VOC, and PASCAL-Context, outperforming prior methods in both efficiency and accuracy. Comprehensive ablation studies further demonstrate its robustness across challenging conditions. </p>
<blockquote>
<p>å¼€æ”¾è¯æ±‡è¯­ä¹‰åˆ†å‰²æ—¨åœ¨ç»™æ— é™ç±»åˆ«çš„å›¾åƒåˆ†é…åƒç´ çº§æ ‡ç­¾ã€‚ä¼ ç»Ÿæ–¹æ³•é€šè¿‡æŒ‰é¡ºåºè¿æ¥å¼ºå¤§çš„æ©è†œææ¡ˆç”Ÿæˆå™¨ï¼ˆå¦‚Segment Anything Model (SAM)ï¼‰å’Œé¢„è®­ç»ƒçš„è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆå¦‚CLIPï¼‰æ¥è§£å†³è¿™ä¸ªé—®é¢˜ã€‚ä½†è¿™äº›ä¸¤é˜¶æ®µæ–¹æ³•é€šå¸¸é¢ä¸´é«˜è®¡ç®—æˆæœ¬å’Œå†…å­˜æ•ˆç‡ä½çš„é—®é¢˜ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ESC-Netï¼Œè¿™æ˜¯ä¸€ç§æ–°å‹çš„ä¸€é˜¶æ®µå¼€æ”¾è¯æ±‡åˆ†å‰²æ¨¡å‹ï¼Œå®ƒåˆ©ç”¨SAMè§£ç å™¨å—åœ¨æœ‰æ•ˆçš„æ¨ç†æ¡†æ¶å†…è¿›è¡Œç±»åˆ«æ— å…³çš„åˆ†å‰²ã€‚é€šè¿‡å°†ç”±å›¾åƒæ–‡æœ¬ç›¸å…³æ€§ç”Ÿæˆçš„ä¼ªæç¤ºåµŒå…¥åˆ°SAMçš„å¯æç¤ºåˆ†å‰²æ¡†æ¶ä¸­ï¼ŒESC-Netå®ç°äº†ç²¾ç»†çš„ç©ºé—´èšåˆï¼Œä»¥è·å¾—å‡†ç¡®çš„æ©è†œé¢„æµ‹ã€‚ESC-Netåœ¨åŒ…æ‹¬ADE20Kã€PASCAL-VOCå’ŒPASCAL-Contextåœ¨å†…çš„æ ‡å‡†åŸºå‡†æµ‹è¯•ä¸Šè¡¨ç°å‡ºå“è¶Šçš„æ€§èƒ½ï¼Œåœ¨æ•ˆç‡å’Œå‡†ç¡®æ€§æ–¹é¢å‡ä¼˜äºä»¥å‰çš„æ–¹æ³•ã€‚å…¨é¢çš„æ¶ˆèç ”ç©¶è¿›ä¸€æ­¥è¯æ˜äº†å…¶åœ¨å„ç§æŒ‘æˆ˜æ¡ä»¶ä¸‹çš„ç¨³å¥æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2411.14723v2">PDF</a> Accepted to CVPR 2025</p>
<p><strong>Summary</strong>ï¼š</p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°å‹çš„å¼€æ”¾è¯æ±‡è¯­ä¹‰åˆ†å‰²æ¨¡å‹ESC-Netï¼Œè¯¥æ¨¡å‹é‡‡ç”¨å•é˜¶æ®µè®¾è®¡ï¼Œåˆ©ç”¨SAMè§£ç å™¨å—è¿›è¡Œç±»æ— å…³åˆ†å‰²ï¼Œå¹¶é€šè¿‡åµŒå…¥ä¼ªæç¤ºå®ç°ç²¾ç¡®æ©è†œé¢„æµ‹ã€‚ESC-Netåœ¨ADE20Kã€PASCAL-VOCå’ŒPASCAL-Contextç­‰æ ‡å‡†æ•°æ®é›†ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œç›¸è¾ƒäºä¹‹å‰çš„æ–¹æ³•åœ¨æ•ˆç‡å’Œå‡†ç¡®æ€§ä¸Šéƒ½æœ‰æ˜¾è‘—æå‡ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>å¼€æ”¾è¯æ±‡è¯­ä¹‰åˆ†å‰²æ—¨åœ¨ç»™å›¾åƒä¸­çš„æ¯ä¸ªåƒç´ åˆ†é…æ ‡ç­¾ï¼Œæ¶‰åŠæ— é™ç±»åˆ«ã€‚</li>
<li>ä¼ ç»Ÿæ–¹æ³•é€šå¸¸é‡‡ç”¨ä¸¤é˜¶æ®µæ–¹æ³•ï¼Œç»“åˆæ©è†œææ¡ˆç”Ÿæˆå™¨å’Œé¢„è®­ç»ƒçš„è¯­è¨€è§†è§‰æ¨¡å‹ï¼Œä½†å­˜åœ¨è®¡ç®—æˆæœ¬é«˜å’Œå†…å­˜æ•ˆç‡ä½çš„é—®é¢˜ã€‚</li>
<li>ESC-Netæ˜¯ä¸€ä¸ªæ–°å‹å•é˜¶æ®µå¼€æ”¾è¯æ±‡åˆ†å‰²æ¨¡å‹ï¼Œåˆ©ç”¨SAMè§£ç å™¨å—è¿›è¡Œç±»æ— å…³åˆ†å‰²ã€‚</li>
<li>ESC-Neté€šè¿‡å°†å›¾åƒæ–‡æœ¬ç›¸å…³æ€§ç”Ÿæˆçš„ä¼ªæç¤ºåµŒå…¥SAMçš„æç¤ºåˆ†å‰²æ¡†æ¶ï¼Œå®ç°äº†ç²¾ç¡®æ©è†œé¢„æµ‹ã€‚</li>
<li>ESC-Netåœ¨å¤šä¸ªæ ‡å‡†æ•°æ®é›†ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œè¯æ˜å…¶é«˜æ•ˆå’Œå‡†ç¡®ã€‚</li>
<li>ç»¼åˆæ¶ˆèç ”ç©¶è¿›ä¸€æ­¥å±•ç¤ºäº†ESC-Netåœ¨æŒ‘æˆ˜æ¡ä»¶ä¸‹çš„ç¨³å¥æ€§ã€‚</li>
<li>ESC-Netçš„è®¾è®¡ä¸ºè§£å†³å¼€æ”¾è¯æ±‡è¯­ä¹‰åˆ†å‰²é—®é¢˜æä¾›äº†ä¸€ç§æœ‰æ•ˆçš„æ–°é€”å¾„ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2411.14723">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-c096903ed540421304da388c61cec2b7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-31cd9e3384d6a84abdf267d8636798eb.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d97301f4453c061d76b50be6ae62dbb2.jpg" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="Multimodal-Object-Detection-using-Depth-and-Image-Data-for-Manufacturing-Parts"><a href="#Multimodal-Object-Detection-using-Depth-and-Image-Data-for-Manufacturing-Parts" class="headerlink" title="Multimodal Object Detection using Depth and Image Data for Manufacturing   Parts"></a>Multimodal Object Detection using Depth and Image Data for Manufacturing   Parts</h2><p><strong>Authors:Nazanin Mahjourian, Vinh Nguyen</strong></p>
<p>Manufacturing requires reliable object detection methods for precise picking and handling of diverse types of manufacturing parts and components. Traditional object detection methods utilize either only 2D images from cameras or 3D data from lidars or similar 3D sensors. However, each of these sensors have weaknesses and limitations. Cameras do not have depth perception and 3D sensors typically do not carry color information. These weaknesses can undermine the reliability and robustness of industrial manufacturing systems. To address these challenges, this work proposes a multi-sensor system combining an red-green-blue (RGB) camera and a 3D point cloud sensor. The two sensors are calibrated for precise alignment of the multimodal data captured from the two hardware devices. A novel multimodal object detection method is developed to process both RGB and depth data. This object detector is based on the Faster R-CNN baseline that was originally designed to process only camera images. The results show that the multimodal model significantly outperforms the depth-only and RGB-only baselines on established object detection metrics. More specifically, the multimodal model improves mAP by 13% and raises Mean Precision by 11.8% in comparison to the RGB-only baseline. Compared to the depth-only baseline, it improves mAP by 78% and raises Mean Precision by 57%. Hence, this method facilitates more reliable and robust object detection in service to smart manufacturing applications. </p>
<blockquote>
<p>åˆ¶é€ éœ€è¦å¯é çš„ç‰©ä½“æ£€æµ‹æ–¹æ³•ï¼Œä»¥ä¾¿å¯¹å„ç§ç±»å‹çš„åˆ¶é€ é›¶éƒ¨ä»¶è¿›è¡Œç²¾ç¡®æŒ‘é€‰å’Œå¤„ç†ã€‚ä¼ ç»Ÿçš„ç‰©ä½“æ£€æµ‹æ–¹æ³•è¦ä¹ˆåªä½¿ç”¨ç›¸æœºçš„äºŒç»´å›¾åƒï¼Œè¦ä¹ˆä½¿ç”¨æ¿€å…‰é›·è¾¾æˆ–ç±»ä¼¼çš„ä¸‰ç»´ä¼ æ„Ÿå™¨çš„ä¸‰ç»´æ•°æ®ã€‚ç„¶è€Œï¼Œæ¯ç§ä¼ æ„Ÿå™¨éƒ½æœ‰å…¶å¼±ç‚¹å’Œå±€é™æ€§ã€‚ç›¸æœºæ²¡æœ‰æ·±åº¦æ„ŸçŸ¥èƒ½åŠ›ï¼Œè€Œä¸‰ç»´ä¼ æ„Ÿå™¨é€šå¸¸ä¸æºå¸¦é¢œè‰²ä¿¡æ¯ã€‚è¿™äº›å¼±ç‚¹å¯èƒ½ä¼šç ´åå·¥ä¸šåˆ¶é€ ç³»ç»Ÿçš„å¯é æ€§å’Œç¨³å¥æ€§ã€‚ä¸ºäº†åº”å¯¹è¿™äº›æŒ‘æˆ˜ï¼Œè¿™é¡¹å·¥ä½œæå‡ºäº†ä¸€ç§å¤šä¼ æ„Ÿå™¨ç³»ç»Ÿï¼Œè¯¥ç³»ç»Ÿç»“åˆäº†çº¢ç»¿è“ï¼ˆRGBï¼‰ç›¸æœºå’Œä¸‰ç»´ç‚¹äº‘ä¼ æ„Ÿå™¨ã€‚ä¸¤ä¸ªä¼ æ„Ÿå™¨ç»è¿‡æ ¡å‡†ï¼Œä»¥ç²¾ç¡®å¯¹é½ä»ä¸¤ä¸ªç¡¬ä»¶è®¾å¤‡æ•è·çš„å¤šæ¨¡æ€æ•°æ®ã€‚å¼€å‘äº†ä¸€ç§æ–°å‹çš„å¤šæ¨¡æ€ç‰©ä½“æ£€æµ‹æ–¹æ³•ï¼Œå¯åŒæ—¶å¤„ç†RGBå’Œæ·±åº¦æ•°æ®ã€‚è¯¥ç‰©ä½“æ£€æµ‹å™¨åŸºäºFaster R-CNNåŸºçº¿æ„å»ºï¼Œåè€…æœ€åˆæ˜¯ä¸ºä»…å¤„ç†ç›¸æœºå›¾åƒè€Œè®¾è®¡çš„ã€‚ç»“æœè¡¨æ˜ï¼Œå¤šæ¨¡æ€æ¨¡å‹åœ¨æ—¢å®šçš„ç‰©ä½“æ£€æµ‹æŒ‡æ ‡ä¸Šæ˜¾è‘—ä¼˜äºä»…ä½¿ç”¨æ·±åº¦å’Œä»…ä½¿ç”¨RGBçš„åŸºçº¿ã€‚å…·ä½“æ¥è¯´ï¼Œä¸ä»…ä½¿ç”¨RGBçš„åŸºçº¿ç›¸æ¯”ï¼Œå¤šæ¨¡æ€æ¨¡å‹æé«˜äº†13%çš„mAPå¹¶æé«˜äº†11.8%çš„å¹³å‡ç²¾åº¦ã€‚ä¸ä»…ä½¿ç”¨æ·±åº¦çš„åŸºçº¿ç›¸æ¯”ï¼Œå®ƒæé«˜äº†78%çš„mAPå¹¶æé«˜äº†57%çš„å¹³å‡ç²¾åº¦ã€‚å› æ­¤ï¼Œè¯¥æ–¹æ³•æœ‰åŠ©äºæ›´å¯é å’Œç¨³å¥çš„ç‰©ä½“æ£€æµ‹ï¼Œä¸ºæ™ºèƒ½åˆ¶é€ åº”ç”¨æä¾›æœåŠ¡ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2411.09062v2">PDF</a> </p>
<p><strong>æ‘˜è¦</strong><br>åœ¨åˆ¶é€ è¿‡ç¨‹ä¸­ï¼Œå¯é çš„ç›®æ ‡æ£€æµ‹æ–¹æ³•æ˜¯ç²¾ç¡®æ‹¾å–å’Œå¤„ç†å„ç§åˆ¶é€ é›¶éƒ¨ä»¶çš„å…³é”®ã€‚ä¼ ç»Ÿæ–¹æ³•ä¸»è¦ä¾èµ–2Då›¾åƒæˆ–3Dæ•°æ®ï¼Œä½†å­˜åœ¨å±€é™æ€§ã€‚ä¸ºåº”å¯¹æŒ‘æˆ˜ï¼Œæœ¬æ–‡æå‡ºä¸€ç§èåˆRGBç›¸æœºå’Œ3Dç‚¹äº‘ä¼ æ„Ÿå™¨çš„å¤šä¼ æ„Ÿå™¨ç³»ç»Ÿã€‚ä¸¤è€…æ ¡å‡†å¯å®ç°å¤šæ¨¡å¼æ•°æ®çš„ç²¾ç¡®å¯¹é½ã€‚å¼€å‘çš„æ–°å‹å¤šæ¨¡å¼ç›®æ ‡æ£€æµ‹æ³•å¯å¤„ç†RGBå’Œæ·±åº¦æ•°æ®ï¼ŒåŸºäºFaster R-CNNåŸºçº¿æŠ€æœ¯æ„å»ºï¼ŒåŸæœ¬ä»…ç”¨äºå¤„ç†ç›¸æœºå›¾åƒã€‚ç»“æœæ˜¾ç¤ºï¼Œå¤šæ¨¡å¼æ¨¡å‹åœ¨ç›®æ ‡æ£€æµ‹æŒ‡æ ‡ä¸Šæ˜¾è‘—ä¼˜äºä»…ä½¿ç”¨æ·±åº¦æˆ–RGBæ•°æ®çš„æ¨¡å‹ï¼ŒmAPæé«˜13%ï¼Œå¹³å‡ç²¾åº¦æé«˜11.8%ã€‚ä¸ä»…ä½¿ç”¨æ·±åº¦æ•°æ®çš„åŸºçº¿ç›¸æ¯”ï¼ŒmAPæé«˜78%ï¼Œå¹³å‡ç²¾åº¦æé«˜57%ã€‚å› æ­¤ï¼Œæ­¤æ–¹æ³•æœ‰åŠ©äºæ›´å¯é ã€æ›´ç¨³å¥çš„ç›®æ ‡æ£€æµ‹ï¼ŒæœåŠ¡äºæ™ºèƒ½åˆ¶é€ åº”ç”¨ã€‚</p>
<p><strong>è¦ç‚¹æç‚¼</strong></p>
<ol>
<li>åˆ¶é€ ä¸­ç›®æ ‡æ£€æµ‹çš„å¯é æ€§å¯¹äºç²¾ç¡®æ‹¾å–å’Œå¤„ç†é›¶éƒ¨ä»¶è‡³å…³é‡è¦ã€‚</li>
<li>ä¼ ç»Ÿæ–¹æ³•ä¾èµ–å•ä¸€ä¼ æ„Ÿå™¨ï¼ˆå¦‚ç›¸æœºæˆ–3Dä¼ æ„Ÿå™¨ï¼‰ï¼Œå­˜åœ¨å±€é™æ€§ã€‚</li>
<li>æœ¬æ–‡æå‡ºèåˆRGBç›¸æœºå’Œ3Dç‚¹äº‘ä¼ æ„Ÿå™¨çš„å¤šä¼ æ„Ÿå™¨ç³»ç»Ÿï¼Œä»¥å…‹æœå•ä¸€ä¼ æ„Ÿå™¨çš„å±€é™æ€§ã€‚</li>
<li>é€šè¿‡æ ¡å‡†ä¸¤ç§ä¼ æ„Ÿå™¨ï¼Œå¯å®ç°å¤šæ¨¡å¼æ•°æ®çš„ç²¾ç¡®å¯¹é½ã€‚</li>
<li>æ–°å‹å¤šæ¨¡å¼ç›®æ ‡æ£€æµ‹æ³•ç»“åˆRGBå’Œæ·±åº¦æ•°æ®ï¼ŒåŸºäºFaster R-CNNåŸºçº¿æŠ€æœ¯æ„å»ºã€‚</li>
<li>å¤šæ¨¡å¼æ¨¡å‹åœ¨ç›®æ ‡æ£€æµ‹æ–¹é¢è¡¨ç°ä¼˜äºä»…ä½¿ç”¨æ·±åº¦æˆ–RGBæ•°æ®çš„æ¨¡å‹ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2411.09062">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-031c48e49077b765dc65f4ff49d1a373.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-729938fdb06e8d40c2f26909cae06f9f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c03ab947f6f228be03254a9afff54289.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4c8b026f40ca96dfb03e1c17accfb6b3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f71943568030cc041305953bb9c5abbf.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-3af85886a92d0b81738e9bf40245b253.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-515e1bc82e2581ec0b59868457608425.jpg" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1><h2 id="Vocabulary-Free-3D-Instance-Segmentation-with-Vision-and-Language-Assistant"><a href="#Vocabulary-Free-3D-Instance-Segmentation-with-Vision-and-Language-Assistant" class="headerlink" title="Vocabulary-Free 3D Instance Segmentation with Vision and Language   Assistant"></a>Vocabulary-Free 3D Instance Segmentation with Vision and Language   Assistant</h2><p><strong>Authors:Guofeng Mei, Luigi Riz, Yiming Wang, Fabio Poiesi</strong></p>
<p>Most recent 3D instance segmentation methods are open vocabulary, offering a greater flexibility than closed-vocabulary methods. Yet, they are limited to reasoning within a specific set of concepts, \ie the vocabulary, prompted by the user at test time. In essence, these models cannot reason in an open-ended fashion, i.e., answering â€œList the objects in the scene.â€™â€™. We introduce the first method to address 3D instance segmentation in a setting that is void of any vocabulary prior, namely a vocabulary-free setting. We leverage a large vision-language assistant and an open-vocabulary 2D instance segmenter to discover and ground semantic categories on the posed images. To form 3D instance mask, we first partition the input point cloud into dense superpoints, which are then merged into 3D instance masks. We propose a novel superpoint merging strategy via spectral clustering, accounting for both mask coherence and semantic coherence that are estimated from the 2D object instance masks. We evaluate our method using ScanNet200 and Replica, outperforming existing methods in both vocabulary-free and open-vocabulary settings. Code will be made available. Project page: <a target="_blank" rel="noopener" href="https://gfmei.github.io/PoVo">https://gfmei.github.io/PoVo</a> </p>
<blockquote>
<p>æœ€æ–°çš„ä¸‰ç»´å®ä¾‹åˆ†å‰²æ–¹æ³•å±äºå¼€æ”¾è¯æ±‡æ–¹æ³•ï¼Œç›¸æ¯”äºå°é—­è¯æ±‡æ–¹æ³•æä¾›äº†æ›´å¤§çš„çµæ´»æ€§ã€‚ç„¶è€Œï¼Œå®ƒä»¬å—é™äºåœ¨ç‰¹å®šæ¦‚å¿µé›†ï¼ˆå³è¯æ±‡ï¼‰å†…çš„æ¨ç†ï¼Œè¿™äº›è¯æ±‡ç”±ç”¨æˆ·åœ¨æµ‹è¯•æ—¶æä¾›ã€‚æœ¬è´¨ä¸Šï¼Œè¿™äº›æ¨¡å‹æ— æ³•è¿›è¡Œå¼€æ”¾å¼æ¨ç†ï¼Œå³å›ç­”â€œåˆ—å‡ºåœºæ™¯ä¸­çš„ç‰©ä½“â€ã€‚æˆ‘ä»¬é¦–æ¬¡å¼•å…¥äº†ä¸€ç§è§£å†³æ— è¯æ±‡å…ˆéªŒè®¾ç½®ä¸‹çš„ä¸‰ç»´å®ä¾‹åˆ†å‰²é—®é¢˜çš„æ–¹æ³•ï¼Œå³æ— è¯æ±‡è®¾ç½®ã€‚æˆ‘ä»¬åˆ©ç”¨å¤§å‹è§†è§‰è¯­è¨€åŠ©ç†å’Œä¸€ä¸ªå¼€æ”¾è¯æ±‡çš„äºŒç»´å®ä¾‹åˆ†å‰²å™¨æ¥å‘ç°å’Œå®šä½å§¿æ€å›¾åƒä¸Šçš„è¯­ä¹‰ç±»åˆ«ã€‚ä¸ºäº†å½¢æˆä¸‰ç»´å®ä¾‹æ©è†œï¼Œæˆ‘ä»¬é¦–å…ˆè¾“å…¥ç‚¹äº‘åˆ†å‰²æˆå¯†é›†çš„è¶…ç‚¹ï¼Œç„¶åå°†è¿™äº›è¶…ç‚¹åˆå¹¶ä¸ºä¸‰ç»´å®ä¾‹æ©è†œã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„è¶…ç‚¹åˆå¹¶ç­–ç•¥ï¼Œé€šè¿‡è°±èšç±»æ³•ï¼Œè€ƒè™‘äº†æ©è†œçš„ä¸€è‡´æ€§å’Œä»äºŒç»´ç›®æ ‡å®ä¾‹æ©è†œä¸­ä¼°è®¡å‡ºçš„è¯­ä¹‰ä¸€è‡´æ€§ã€‚æˆ‘ä»¬åœ¨ScanNet200å’ŒReplicaæ•°æ®é›†ä¸Šè¯„ä¼°äº†æˆ‘ä»¬çš„æ–¹æ³•ï¼Œåœ¨æ— è¯æ±‡å’Œå¼€æ”¾è¯æ±‡è®¾ç½®ä¸­å‡ä¼˜äºç°æœ‰æ–¹æ³•ã€‚ä»£ç å°†å…¬å¼€æä¾›ã€‚é¡¹ç›®é¡µé¢ï¼š<a target="_blank" rel="noopener" href="https://gfmei.github.io/PoVo">é“¾æ¥åœ°å€</a>ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2408.10652v2">PDF</a> Accepted by 3DV</p>
<p><strong>Summary</strong></p>
<p>æœ€æ–°3Då®ä¾‹åˆ†å‰²æ–¹æ³•é‡‡ç”¨å¼€æ”¾è¯æ±‡è¡¨ï¼Œè¾ƒå°é—­è¯æ±‡è¡¨æ–¹æ³•æ›´çµæ´»ã€‚ç„¶è€Œï¼Œå®ƒä»¬å—é™äºåœ¨ç‰¹å®šæ¦‚å¿µé›†ï¼ˆå³è¯æ±‡è¡¨ï¼‰å†…çš„æ¨ç†ï¼Œè¿™äº›è¯æ±‡è¡¨åœ¨æµ‹è¯•æ—¶ç”±ç”¨æˆ·æç¤ºã€‚æœ¬è´¨ä¸Šï¼Œè¿™äº›æ–¹æ³•æ— æ³•ä»¥å¼€æ”¾å¼æ–¹å¼è¿›è¡Œæ¨ç†ï¼Œå³å›ç­”â€œåˆ—å‡ºåœºæ™¯ä¸­çš„ç‰©ä½“â€ã€‚æˆ‘ä»¬é¦–æ¬¡æå‡ºè§£å†³åœ¨æ²¡æœ‰ä»»ä½•è¯æ±‡å…ˆéªŒæ¡ä»¶ä¸‹çš„ä¸‰ç»´å®ä¾‹åˆ†å‰²é—®é¢˜çš„æ–¹æ³•ï¼Œå³æ— è¯æ±‡è®¾ç½®ã€‚æˆ‘ä»¬åˆ©ç”¨å¤§å‹è§†è§‰è¯­è¨€åŠ©ç†å’Œå¼€æ”¾è¯æ±‡çš„äºŒç»´å®ä¾‹åˆ†å‰²å™¨æ¥å‘ç°å’Œå®šä½å›¾åƒä¸Šçš„è¯­ä¹‰ç±»åˆ«ã€‚ä¸ºäº†å½¢æˆä¸‰ç»´å®ä¾‹æ©è†œï¼Œæˆ‘ä»¬é¦–å…ˆå¯¹è¾“å…¥çš„ç‚¹äº‘è¿›è¡Œå¯†é›†è¶…ç‚¹åˆ’åˆ†ï¼Œç„¶åå°†è¿™äº›è¶…ç‚¹åˆå¹¶ä¸ºä¸‰ç»´å®ä¾‹æ©è†œã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„è¶…ç‚¹åˆå¹¶ç­–ç•¥ï¼Œé€šè¿‡è°±èšç±»è€ƒè™‘æ©è†œçš„ä¸€è‡´æ€§å’Œè¯­ä¹‰ä¸€è‡´æ€§ï¼Œè¿™äº›ä¸€è‡´æ€§æ˜¯æ ¹æ®äºŒç»´å¯¹è±¡å®ä¾‹æ©è†œä¼°è®¡çš„ã€‚æˆ‘ä»¬åœ¨ScanNet200å’ŒReplicaä¸Šè¯„ä¼°äº†æˆ‘ä»¬çš„æ–¹æ³•ï¼Œåœ¨æ— è¯æ±‡è¡¨å’Œå¼€æ”¾è¯æ±‡è¡¨è®¾ç½®ä¸­å‡ä¼˜äºç°æœ‰æ–¹æ³•ã€‚ä»£ç å°†å…¬å¼€æä¾›ã€‚é¡¹ç›®é¡µé¢ï¼š<a target="_blank" rel="noopener" href="https://gfmei.github.io/PoVo">https://gfmei.github.io/PoVo</a>ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æœ€æ–°3Då®ä¾‹åˆ†å‰²æ–¹æ³•å…·æœ‰å¼€æ”¾è¯æ±‡ç‰¹æ€§ï¼Œæä¾›æ›´å¤§çµæ´»æ€§ã€‚</li>
<li>å½“å‰æ–¹æ³•å—é™äºåœ¨ç‰¹å®šæ¦‚å¿µé›†ï¼ˆè¯æ±‡è¡¨ï¼‰å†…çš„æ¨ç†ã€‚</li>
<li>æå‡ºä¸€ç§æ— è¯æ±‡è®¾ç½®ä¸‹çš„ä¸‰ç»´å®ä¾‹åˆ†å‰²æ–¹æ³•ã€‚</li>
<li>åˆ©ç”¨è§†è§‰è¯­è¨€åŠ©ç†å’Œå¼€æ”¾è¯æ±‡çš„äºŒç»´å®ä¾‹åˆ†å‰²å™¨æ¥è¯†åˆ«å’Œå®šä½è¯­ä¹‰ç±»åˆ«ã€‚</li>
<li>é€šè¿‡è°±èšç±»å®ç°è¶…ç‚¹åˆå¹¶ç­–ç•¥ï¼Œè€ƒè™‘æ©è†œçš„ä¸€è‡´æ€§å’Œè¯­ä¹‰ä¸€è‡´æ€§ã€‚</li>
<li>åœ¨ScanNet200å’ŒReplicaä¸Šçš„æ€§èƒ½è¶…è¶Šäº†ç°æœ‰æ–¹æ³•ï¼Œç‰¹åˆ«æ˜¯åœ¨æ— è¯æ±‡è¡¨å’Œå¼€æ”¾è¯æ±‡è¡¨è®¾ç½®ä¸­ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2408.10652">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-04-03\./crop_æ£€æµ‹_åˆ†å‰²_è·Ÿè¸ª/2408.10652v2/page_0_0.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-01f532b24316159a83b4e6e273097b6e.jpg" align="middle">
</details>


<h1 id="-19"><a href="#-19" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-04-03/%E6%A3%80%E6%B5%8B_%E5%88%86%E5%89%B2_%E8%B7%9F%E8%B8%AA/">https://kedreamix.github.io/Talk2Paper/Paper/2025-04-03/%E6%A3%80%E6%B5%8B_%E5%88%86%E5%89%B2_%E8%B7%9F%E8%B8%AA/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/%E6%A3%80%E6%B5%8B-%E5%88%86%E5%89%B2-%E8%B7%9F%E8%B8%AA/">
                                    <span class="chip bg-color">æ£€æµ‹/åˆ†å‰²/è·Ÿè¸ª</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-04-03/%E4%BA%BA%E8%84%B8%E7%9B%B8%E5%85%B3/">
                    <div class="card-image">
                        
                        <img src="https://pica.zhimg.com/v2-f9c35fe17853a93d766adaedcd07165b.jpg" class="responsive-img" alt="äººè„¸ç›¸å…³">
                        
                        <span class="card-title">äººè„¸ç›¸å…³</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            äººè„¸ç›¸å…³ æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-04-04  Data Synthesis with Diverse Styles for Face Recognition via 3DMM-Guided   Diffusion
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-04-04
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E4%BA%BA%E8%84%B8%E7%9B%B8%E5%85%B3/" class="post-category">
                                    äººè„¸ç›¸å…³
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E4%BA%BA%E8%84%B8%E7%9B%B8%E5%85%B3/">
                        <span class="chip bg-color">äººè„¸ç›¸å…³</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-04-04/LLM/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-c4198d82bea3dd405c405082c9f941d8.jpg" class="responsive-img" alt="LLM">
                        
                        <span class="card-title">LLM</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            LLM æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-04-04  CADFormer Fine-Grained Cross-modal Alignment and Decoding Transformer   for Referring Remote Sensing Image Segmentation
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-04-04
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/LLM/" class="post-category">
                                    LLM
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/LLM/">
                        <span class="chip bg-color">LLM</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">14643.7k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
