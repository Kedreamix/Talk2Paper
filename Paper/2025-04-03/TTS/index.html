<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="TTS">
    <meta name="description" content="TTS æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-04-04  LEP3 A High-Luminosity e+e- Higgs and ElectroweakFactory in the LHC   Tunnel">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>TTS | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pic1.zhimg.com/v2-a486d8f9668db1bd0fc9ec5cec8797ae.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">TTS</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/TTS/">
                                <span class="chip bg-color">TTS</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/TTS/" class="post-category">
                                TTS
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-04-04
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-05-14
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    14.1k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    57 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-04-04-æ›´æ–°"><a href="#2025-04-04-æ›´æ–°" class="headerlink" title="2025-04-04 æ›´æ–°"></a>2025-04-04 æ›´æ–°</h1><h2 id="LEP3-A-High-Luminosity-e-e-Higgs-and-ElectroweakFactory-in-the-LHC-Tunnel"><a href="#LEP3-A-High-Luminosity-e-e-Higgs-and-ElectroweakFactory-in-the-LHC-Tunnel" class="headerlink" title="LEP3: A High-Luminosity e+e- Higgs and ElectroweakFactory in the LHC   Tunnel"></a>LEP3: A High-Luminosity e+e- Higgs and ElectroweakFactory in the LHC   Tunnel</h2><p><strong>Authors:C. Anastopoulos, R. Assmann, A. Ball, O. Bruning, O. Buchmueller, T. Camporesi, P. Collier, J Dainton, G. Davies, J. R. Ellis, B. Goddard, L. Gouskos, M. Klute, M. Koratzinos, G. Landsberg, K. Long, L. Malgeri, F. Maltoni, F. Moortgat, C. Mariotti, S. Myers, J. A. Osborne, M. Pierini, D. R. Tovey, D. Treille, T. S. Virdee, N. Wardle, M. Zanetti</strong></p>
<p>As stated in the 2019 European Strategy for Particle Physics (ESPP), it is of the utmost importance that the HL-LHC upgrade of the accelerator and the experiments be successfully completed in a timely manner. All necessary efforts should be devoted to achieving this goal. We also recall two of the principal recommendations of the 2019 ESPP for future accelerator initiatives, namely that 1) An electron-positron Higgs factory is the highest priority for the next collider (Rec. c). 2) Europe, together with its international partners, should investigate the technical and financial feasibility of a future hadron collider at CERN with a centre-of-mass energy of at least 100 TeV and with an electron-positron Higgs and electroweak factory as a possible first stage (Rec. e). A major objective in particle physics is always to operate an accelerator that allows a leap of an order of magnitude in the constituent centre-of-mass energy with respect to the previous one. We support FCC-ee and FCC-hh as the preferred option for CERN future, as it addresses both of the above recommendations.   The guidance for the 2025 ESPP requests, in addition to the preferred option, the inclusion of &#96;&#96;prioritised alternatives to be pursued if the chosen preferred option turns out not to be feasible or competitiveâ€™â€™. Proposed alternatives to the preferred FCC option include linear, muon colliders and LHeC accelerators. In response to this request we propose reusing the existing LHC tunnel for an electron-positron collider, called LEP3, as a back-up alternative if the FCC cannot proceed. LEP3 leverages much of the R&amp;D conducted for FCC-ee, offers high-precision studies of Z, W, and Higgs bosons below the tt threshold, and offers potential physics performance comparable or superior to other fallback options at a lower cost while supporting continued R&amp;D towards a next-generation energy frontier machine. </p>
<blockquote>
<p>å¦‚2019å¹´æ¬§æ´²ç²’å­ç‰©ç†ç­–ç•¥ï¼ˆESPPï¼‰æ‰€è¿°ï¼ŒåŠæ—¶æˆåŠŸå®ŒæˆHL-LHCåŠ é€Ÿå™¨åŠå®éªŒçš„å‡çº§è‡³å…³é‡è¦ã€‚æ‰€æœ‰å¿…è¦çš„åŠªåŠ›éƒ½åº”è‡´åŠ›äºå®ç°è¿™ä¸€ç›®æ ‡ã€‚æˆ‘ä»¬è¿˜å›é¡¾äº†2019å¹´ESPPé’ˆå¯¹æœªæ¥åŠ é€Ÿå™¨å€¡è®®çš„ä¸¤é¡¹ä¸»è¦å»ºè®®ï¼Œå³ï¼š1ï¼‰ç”µå­-æ­£ç”µå­å¸Œæ ¼æ–¯å·¥å‚æ˜¯ä¸‹ä¸€ä¸ªå¯¹æ’æœºçš„æœ€é«˜ä¼˜å…ˆçº§ï¼ˆå»ºè®®cï¼‰ã€‚2ï¼‰æ¬§æ´²åº”ä¸å…¶å›½é™…åˆä½œä¼™ä¼´å…±åŒç ”ç©¶åœ¨CERNå»ºé€ è‡³å°‘100TeVè´¨å¿ƒèƒ½é‡çš„æœªæ¥å¼ºå­å¯¹æ’æœºçš„æŠ€æœ¯å’Œç»æµå¯è¡Œæ€§ï¼Œå¹¶å°†ç”µå­-æ­£ç”µå­å¸Œæ ¼æ–¯å·¥å‚å’Œå¼±ç”µå·¥å‚ä½œä¸ºå¯èƒ½çš„ç¬¬ä¸€é˜¶æ®µï¼ˆå»ºè®®eï¼‰ã€‚ç²’å­ç‰©ç†å­¦çš„ä¸€ä¸ªä¸»è¦ç›®æ ‡å§‹ç»ˆæ˜¯è¿è¡Œä¸€ä¸ªåŠ é€Ÿå™¨ï¼Œè¯¥åŠ é€Ÿå™¨åœ¨è´¨å¿ƒèƒ½é‡æ–¹é¢ç›¸æ¯”ä¹‹å‰çš„æœ‰ä¸€ä¸ªæ•°é‡çº§çš„é£è·ƒã€‚æˆ‘ä»¬æ”¯æŒFCC-eeå’ŒFCC-hhä½œä¸ºCERNæœªæ¥çš„é¦–é€‰æ–¹æ¡ˆï¼Œå› ä¸ºå®ƒç¬¦åˆä¸Šè¿°ä¸¤é¡¹å»ºè®®ã€‚æ­¤å¤–ï¼Œæ ¹æ®2025å¹´ESPPçš„è¦æ±‚ï¼Œé™¤äº†é¦–é€‰æ–¹æ¡ˆå¤–ï¼Œè¿˜éœ€åŒ…æ‹¬â€œå¦‚æœé¦–é€‰æ–¹æ¡ˆä¸å¯è¡Œæˆ–ç¼ºä¹ç«äº‰åŠ›æ—¶ä¼˜å…ˆè€ƒè™‘çš„æ›¿ä»£æ–¹æ¡ˆâ€ã€‚ä½œä¸ºé¦–é€‰FCCæ–¹æ¡ˆçš„æ›¿ä»£æ–¹æ¡ˆï¼Œæå‡ºäº†çº¿æ€§åŠ é€Ÿå™¨ã€ç¼ªå­å¯¹æ’æœºå’ŒLHeCåŠ é€Ÿå™¨ç­‰æè®®ã€‚å¯¹æ­¤è¦æ±‚ï¼Œæˆ‘ä»¬æè®®åœ¨FCCæ— æ³•å®æ–½çš„æƒ…å†µä¸‹ï¼Œåˆ©ç”¨ç°æœ‰çš„LHCéš§é“è¿›è¡Œç”µå­-æ­£ç”µå­å¯¹æ’æœºï¼ˆç§°ä¸ºLEP3ï¼‰ä½œä¸ºåå¤‡æ›¿ä»£æ–¹æ¡ˆã€‚LEP3å……åˆ†åˆ©ç”¨äº†ä¸ºFCC-eeå¼€å±•çš„ç ”ç©¶ä¸å¼€å‘å·¥ä½œï¼Œå¯ä»¥è¿›è¡Œä½äºtté˜ˆå€¼çš„Zã€Wå’Œå¸Œæ ¼æ–¯ç»è‰²å­çš„é«˜ç²¾åº¦ç ”ç©¶ï¼Œå¹¶åœ¨è¾ƒä½çš„æˆæœ¬ä¸‹æä¾›ä¸å…¶ä»–å¤‡é€‰æ–¹æ¡ˆç›¸å½“æˆ–æ›´å¥½çš„ç‰©ç†æ€§èƒ½è¡¨ç°ï¼ŒåŒæ—¶æ”¯æŒä¸‹ä¸€ä»£èƒ½æºå‰æ²¿æœºå™¨çš„ç ”ç©¶ä¸å¼€å‘ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.00541v1">PDF</a> 11 pages, 3 tables</p>
<p><strong>æ‘˜è¦</strong><br>     æ ¹æ® 2019 å¹´æ¬§æ´²ç²’å­ç‰©ç†æˆ˜ç•¥ (ESPP)ï¼Œå¤§å‹å¼ºå­å¯¹æ’æœº (HL-LHC) çš„åŠ é€Ÿå™¨åŠå…¶å®éªŒçš„å‡çº§å·¥ä½œè‡³å…³é‡è¦ï¼Œå¿…é¡»åŠæ—¶æˆåŠŸå®Œæˆã€‚æ¬§æ´²åº”ä¸å…¶å›½é™…åˆä½œä¼™ä¼´å…±åŒç ”ç©¶æ¬§æ´²æ ¸ç ”ç©¶ç»„ç»‡æœªæ¥å»ºé€ è‡³å°‘ 100 TeV è´¨å¿ƒèƒ½é‡çš„å¼ºå­å¯¹æ’æœºçš„æŠ€æœ¯å’Œç»æµå¯è¡Œæ€§ï¼Œå¹¶å°†ç”µå­æ­£è´Ÿç”µå­å¸Œæ ¼æ–¯å·¥å‚ä½œä¸ºå¯èƒ½çš„ç¬¬ä¸€ä¸ªé˜¶æ®µã€‚åŒæ—¶æ”¯æŒæœªæ¥ç¯å½¢å¯¹æ’æœº (FCC) ä½œä¸ºé¦–é€‰æ–¹æ¡ˆï¼Œè¯¥æ–¹æ¡ˆç¬¦åˆä¸Šè¿°ä¸¤ä¸ªå»ºè®®ã€‚å¯¹äº 2025 å¹´ ESPP çš„æŒ‡å¯¼è¯·æ±‚ï¼Œé™¤äº†é¦–é€‰æ–¹æ¡ˆå¤–ï¼Œè¿˜åº”è€ƒè™‘â€œå¦‚æœé¦–é€‰æ–¹æ¡ˆä¸å¯è¡Œæˆ–ä¸å…·ç«äº‰åŠ›æ—¶ï¼Œä¼˜å…ˆè¿½æ±‚çš„å¤‡é€‰æ–¹æ¡ˆâ€ã€‚å› æ­¤æå‡ºåˆ©ç”¨ç°æœ‰å¤§å‹å¼ºå­å¯¹æ’æœºéš§é“å»ºç«‹ç”µå­æ­£è´Ÿç”µå­å¯¹æ’æœºï¼ˆLEP3ï¼‰ä½œä¸ºå¤‡é€‰æ–¹æ¡ˆã€‚LEP3 å¯ä»¥åˆ©ç”¨å¤§é‡ä¸ºæœªæ¥ç¯å½¢å¯¹æ’æœºå¼€å±•çš„ç ”å‘å·¥ä½œæˆæœï¼Œåœ¨é«˜ç²¾åº¦ç ”ç©¶ Z æ³¢ã€W æ³¢å’Œå¸Œæ ¼æ–¯ç»è‰²å­æ–¹é¢è¡¨ç°å‡ºä¼˜å¼‚çš„æ€§èƒ½ã€‚è¯¥æ–¹æ¡ˆå…·æœ‰æˆæœ¬ä½çš„ä¼˜åŠ¿ï¼Œå¹¶æ”¯æŒä¸‹ä¸€ä»£èƒ½é‡å‰æ²¿æœºå™¨çš„ç ”å‘å·¥ä½œã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>å®Œæˆ HL-LHC åŠ é€Ÿå™¨å’Œå®éªŒçš„å‡çº§è‡³å…³é‡è¦ã€‚éœ€è¦æ‰€æœ‰å¿…è¦çš„åŠªåŠ›æ¥å®ç°è¿™ä¸€ç›®æ ‡ã€‚</li>
<li>ESPP ä¸»å¼ å»ºè®¾ç”µå­æ­£è´Ÿç”µå­å¸Œæ ¼æ–¯å·¥å‚ä½œä¸ºæœªæ¥é¦–é€‰å¯¹æ’æœºé¡¹ç›®ã€‚æ¬§æ´²åŠå…¶å›½é™…åˆä½œä¼™ä¼´æ­£åœ¨æ¢ç´¢æœªæ¥å¼ºå­å¯¹æ’æœºçš„æŠ€æœ¯å’Œç»æµå¯è¡Œæ€§ã€‚</li>
<li>æ”¯æŒ FCC ä½œä¸ºé¦–é€‰æ–¹æ¡ˆï¼Œå®ƒç¬¦åˆ ESPP çš„ä¸¤ä¸ªä¸»è¦å»ºè®®ï¼šä¼˜å…ˆå»ºè®¾ç”µå­æ­£è´Ÿç”µå­å¸Œæ ¼æ–¯å·¥å‚ä½œä¸ºåˆæ­¥é˜¶æ®µå’Œå¯¹æ›´é«˜è´¨å¿ƒèƒ½é‡çš„æ¢ç´¢ã€‚</li>
<li>å¯¹äºå¯èƒ½çš„å¤‡é€‰æ–¹æ¡ˆï¼Œæå‡ºåˆ©ç”¨ç°æœ‰çš„å¤§å‹å¼ºå­å¯¹æ’æœºéš§é“å»ºç«‹ LEP3 ä½œä¸ºå¤‡ç”¨é€‰é¡¹æ–¹æ¡ˆï¼Œåˆ©ç”¨å·²æœ‰çš„ç ”å‘æˆæœä»¥å®ç°ä½æˆæœ¬é«˜æ€§èƒ½çš„ç‰©ç†ç ”ç©¶ã€‚æ­¤å¤–è¿˜åŒ…æ‹¬çº¿æ€§åŠ é€Ÿå™¨ã€Î¼å­å’Œ LHeC åŠ é€Ÿå™¨ç­‰æ›¿ä»£æ–¹æ¡ˆã€‚</li>
<li>LEP3 å…·æœ‰é«˜çµæ•åº¦ç ”ç©¶ç‰¹å®šç²’å­çš„æ½œåŠ›ï¼Œè¿™å¯èƒ½ä¸ºå°†æ¥çš„èƒ½æºå‰æ²¿ç ”ç©¶å¼€è¾Ÿé“è·¯ï¼Œä½œä¸ºä¸€ç§ç›¸å¯¹ä½æˆæœ¬çš„æœ‰æ•ˆç­–ç•¥è¢«ä¼˜å…ˆè€ƒè™‘é‡‡ç”¨è¿™ç§æŠ€æœ¯æ–¹æ¡ˆå¯¹äºå»¶ç»­æœªæ¥çš„ç²’å­ç‰©ç†ç ”å‘ä¹Ÿæœ‰ç€é‡è¦ä½œç”¨ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.00541">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-1fbaabe0822176a369625404e53039b0.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="TeleAntiFraud-28k-An-Audio-Text-Slow-Thinking-Dataset-for-Telecom-Fraud-Detection"><a href="#TeleAntiFraud-28k-An-Audio-Text-Slow-Thinking-Dataset-for-Telecom-Fraud-Detection" class="headerlink" title="TeleAntiFraud-28k: An Audio-Text Slow-Thinking Dataset for Telecom Fraud   Detection"></a>TeleAntiFraud-28k: An Audio-Text Slow-Thinking Dataset for Telecom Fraud   Detection</h2><p><strong>Authors:Zhiming Ma, Peidong Wang, Minhua Huang, Jingpeng Wang, Kai Wu, Xiangzhao Lv, Yachun Pang, Yin Yang, Wenjie Tang, Yuchen Kang</strong></p>
<p>The detection of telecom fraud faces significant challenges due to the lack of high-quality multimodal training data that integrates audio signals with reasoning-oriented textual analysis. To address this gap, we present TeleAntiFraud-28k, the first open-source audio-text slow-thinking dataset specifically designed for automated telecom fraud analysis. Our dataset is constructed through three strategies: (1) Privacy-preserved text-truth sample generation using automatically speech recognition (ASR)-transcribed call recordings (with anonymized original audio), ensuring real-world consistency through text-to-speech (TTS) model regeneration; (2) Semantic enhancement via large language model (LLM)-based self-instruction sampling on authentic ASR outputs to expand scenario coverage; (3) Multi-agent adversarial synthesis that simulates emerging fraud tactics through predefined communication scenarios and fraud typologies. The generated dataset contains 28,511 rigorously processed speech-text pairs, complete with detailed annotations for fraud reasoning. The dataset is divided into three tasks: scenario classification, fraud detection, fraud type classification. Furthermore, we construct TeleAntiFraud-Bench, a standardized evaluation benchmark comprising proportionally sampled instances from the dataset, to facilitate systematic testing of model performance on telecom fraud detection tasks. We also contribute a production-optimized supervised fine-tuning (SFT) model trained on hybrid real&#x2F;synthetic data, while open-sourcing the data processing framework to enable community-driven dataset expansion. This work establishes a foundational framework for multimodal anti-fraud research while addressing critical challenges in data privacy and scenario diversity. The project will be released at <a target="_blank" rel="noopener" href="https://github.com/JimmyMa99/TeleAntiFraud">https://github.com/JimmyMa99/TeleAntiFraud</a>. </p>
<blockquote>
<p>ç”µä¿¡æ¬ºè¯ˆæ£€æµ‹é¢ä¸´ç€å·¨å¤§çš„æŒ‘æˆ˜ï¼Œå› ä¸ºç¼ºä¹é«˜è´¨é‡çš„å¤šæ¨¡å¼è®­ç»ƒæ•°æ®ï¼Œæ— æ³•å°†éŸ³é¢‘ä¿¡å·ä¸é¢å‘æ¨ç†çš„æ–‡æœ¬åˆ†æç›¸ç»“åˆã€‚ä¸ºäº†è§£å†³è¿™ä¸€ç©ºç™½ï¼Œæˆ‘ä»¬æ¨å‡ºäº†TeleAntiFraud-28kï¼Œè¿™æ˜¯ä¸“é—¨ä¸ºç”µä¿¡æ¬ºè¯ˆè‡ªåŠ¨åŒ–åˆ†æè®¾è®¡çš„é¦–ä¸ªå¼€æºéŸ³é¢‘æ–‡æœ¬æ…¢æ€è€ƒæ•°æ®é›†ã€‚æˆ‘ä»¬çš„æ•°æ®é›†é€šè¿‡ä¸‰ç§ç­–ç•¥æ„å»ºï¼šï¼ˆ1ï¼‰ä½¿ç”¨è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰è½¬å½•çš„å½•éŸ³ç”Ÿæˆéšç§ä¿æŠ¤æ–‡æœ¬çœŸå®æ ·æœ¬ï¼ˆå¸¦æœ‰åŒ¿ååŸå§‹éŸ³é¢‘ï¼‰ï¼Œå¹¶é€šè¿‡æ–‡æœ¬åˆ°è¯­éŸ³ï¼ˆTTSï¼‰æ¨¡å‹å†ç”Ÿç¡®ä¿ç°å®ä¸–ç•Œçš„ä¸€è‡´æ€§ï¼›ï¼ˆ2ï¼‰é€šè¿‡åŸºäºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„è‡ªæˆ‘æŒ‡ä»¤é‡‡æ ·å¯¹çœŸå®çš„ASRè¾“å‡ºè¿›è¡Œè¯­ä¹‰å¢å¼ºï¼Œä»¥æ‰©å¤§åœºæ™¯è¦†ç›–èŒƒå›´ï¼›ï¼ˆ3ï¼‰æ¨¡æ‹Ÿæ–°å…´æ¬ºè¯ˆç­–ç•¥çš„å¤šä»£ç†å¯¹æŠ—åˆæˆé€šè¿‡é¢„è®¾çš„é€šä¿¡åœºæ™¯å’Œæ¬ºè¯ˆç±»å‹ã€‚ç”Ÿæˆçš„æ•°æ®é›†åŒ…å«ç»è¿‡ä¸¥æ ¼å¤„ç†çš„28,511ä¸ªè¯­éŸ³æ–‡æœ¬å¯¹ï¼Œå¸¦æœ‰è¯¦ç»†çš„æ¬ºè¯ˆæ¨ç†æ³¨é‡Šã€‚æ•°æ®é›†åˆ†ä¸ºä¸‰ä¸ªä»»åŠ¡ï¼šåœºæ™¯åˆ†ç±»ã€æ¬ºè¯ˆæ£€æµ‹ã€æ¬ºè¯ˆç±»å‹åˆ†ç±»ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æ„å»ºäº†TeleAntiFraud-Benchï¼Œè¿™æ˜¯ä¸€ä¸ªæ ‡å‡†åŒ–çš„è¯„ä¼°åŸºå‡†ï¼Œç”±æ•°æ®é›†ä¸­æŒ‰æ¯”ä¾‹é‡‡æ ·çš„å®ä¾‹ç»„æˆï¼Œä¾¿äºå¯¹ç”µä¿¡æ¬ºè¯ˆæ£€æµ‹ä»»åŠ¡ä¸Šçš„æ¨¡å‹æ€§èƒ½è¿›è¡Œç³»ç»Ÿæ€§æµ‹è¯•ã€‚æˆ‘ä»¬è¿˜ä¸ºæ··åˆçœŸå®&#x2F;åˆæˆæ•°æ®è®­ç»ƒçš„ç”Ÿäº§ä¼˜åŒ–ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰æ¨¡å‹åšå‡ºäº†è´¡çŒ®ï¼ŒåŒæ—¶å¼€æºæ•°æ®å¤„ç†æ¡†æ¶ä»¥æ¨åŠ¨æ•°æ®é›†æ‰©å±•ã€‚è¿™é¡¹å·¥ä½œä¸ºè·¨æ¨¡å¼åæ¬ºè¯ˆç ”ç©¶å»ºç«‹äº†åŸºç¡€æ¡†æ¶ï¼Œå¹¶è§£å†³äº†æ•°æ®éšç§å’Œåœºæ™¯å¤šæ ·æ€§æ–¹é¢çš„å…³é”®æŒ‘æˆ˜ã€‚è¯¥é¡¹ç›®å°†åœ¨<a target="_blank" rel="noopener" href="https://github.com/JimmyMa99/TeleAntiFraud%E5%B8%B3%E5%B9%BF%E3%80%82">https://github.com/JimmyMa99/TeleAntiFraudå‘å¸ƒã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.24115v3">PDF</a> </p>
<p><strong>æ‘˜è¦</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†é’ˆå¯¹ç”µä¿¡æ¬ºè¯ˆæ£€æµ‹æ‰€é¢ä¸´çš„æŒ‘æˆ˜ï¼Œæå‡ºäº†ä¸€ä¸ªå¼€æ”¾æºçš„å¤šæ¨¡æ€æ•°æ®é›†TeleAntiFraud-28kã€‚è¯¥æ•°æ®é›†æ•´åˆäº†éŸ³é¢‘ä¿¡å·å’Œé¢å‘æ¨ç†çš„æ–‡æœ¬åˆ†æï¼Œä¸“é—¨ç”¨äºç”µä¿¡æ¬ºè¯ˆåˆ†æã€‚æ•°æ®é›†é€šè¿‡ä¸‰ç§ç­–ç•¥æ„å»ºï¼šéšç§ä¿æŠ¤çš„æ–‡æœ¬çœŸå®æ ·æœ¬ç”Ÿæˆã€åŸºäºå¤§å‹è¯­è¨€æ¨¡å‹çš„è¯­ä¹‰å¢å¼ºè‡ªæˆ‘æŒ‡ä»¤é‡‡æ ·å’Œå¤šä»£ç†å¯¹æŠ—åˆæˆã€‚æ•°æ®é›†åŒ…å«ç»è¿‡ä¸¥æ ¼å¤„ç†çš„è¯­éŸ³æ–‡æœ¬å¯¹ï¼Œä»¥åŠé’ˆå¯¹æ¬ºè¯ˆæ¨ç†çš„è¯¦ç»†æ³¨é‡Šã€‚æ­¤å¤–ï¼Œè¿˜æ„å»ºäº†æ ‡å‡†åŒ–è¯„ä¼°åŸºå‡†TeleAntiFraud-Benchï¼Œä»¥ä¿ƒè¿›ç”µä¿¡æ¬ºè¯ˆæ£€æµ‹ä»»åŠ¡çš„æ¨¡å‹æ€§èƒ½çš„ç³»ç»Ÿæµ‹è¯•ã€‚æœ¬æ–‡å»ºç«‹äº†å¤šæ¨¡æ€åæ¬ºè¯ˆç ”ç©¶çš„åŸºç¡€æ¡†æ¶ï¼ŒåŒæ—¶è§£å†³äº†æ•°æ®éšç§å’Œåœºæ™¯å¤šæ ·æ€§ç­‰å…³é”®æŒ‘æˆ˜ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>ç”µä¿¡æ¬ºè¯ˆæ£€æµ‹é¢ä¸´ç¼ºä¹é«˜è´¨é‡å¤šæ¨¡æ€è®­ç»ƒæ•°æ®çš„æŒ‘æˆ˜ã€‚</li>
<li>ä»‹ç»äº†é¦–ä¸ªå¼€æ”¾æºçš„å¤šæ¨¡æ€éŸ³é¢‘æ–‡æœ¬æ•°æ®é›†TeleAntiFraud-28kï¼Œä¸“é—¨ç”¨äºç”µä¿¡æ¬ºè¯ˆåˆ†æã€‚</li>
<li>æ•°æ®é›†é€šè¿‡éšç§ä¿æŠ¤çš„æ–‡æœ¬çœŸå®æ ·æœ¬ç”Ÿæˆã€è¯­ä¹‰å¢å¼ºå’Œè‡ªæˆ‘æŒ‡ä»¤é‡‡æ ·ã€å¤šä»£ç†å¯¹æŠ—åˆæˆä¸‰ç§ç­–ç•¥æ„å»ºã€‚</li>
<li>æ•°æ®é›†åŒ…å«ç»è¿‡å¤„ç†çš„è¯­éŸ³æ–‡æœ¬å¯¹å’Œè¯¦ç»†çš„æ¬ºè¯ˆæ³¨é‡Šã€‚</li>
<li>æ„å»ºäº†æ ‡å‡†åŒ–è¯„ä¼°åŸºå‡†TeleAntiFraud-Benchï¼Œä¿ƒè¿›ç”µä¿¡æ¬ºè¯ˆæ£€æµ‹ä»»åŠ¡çš„æ¨¡å‹æ€§èƒ½çš„ç³»ç»Ÿæµ‹è¯•ã€‚</li>
<li>å¼€æ”¾æºçš„æ•°æ®å¤„ç†æ¡†æ¶ä½¿å¾—ç¤¾åŒºèƒ½å¤Ÿè¿›è¡Œæ•°æ®é›†æ‰©å±•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.24115">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-e04604273ac37876ed94a141c9dfb05a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-4ab7ba5582a063e68f5fe266c9f2818e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-2ea373be85c803e7f199329c6e1006ce.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-82a5dba04352ae797e591d0d181ab5d8.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="SupertonicTTS-Towards-Highly-Scalable-and-Efficient-Text-to-Speech-System"><a href="#SupertonicTTS-Towards-Highly-Scalable-and-Efficient-Text-to-Speech-System" class="headerlink" title="SupertonicTTS: Towards Highly Scalable and Efficient Text-to-Speech   System"></a>SupertonicTTS: Towards Highly Scalable and Efficient Text-to-Speech   System</h2><p><strong>Authors:Hyeongju Kim, Jinhyeok Yang, Yechan Yu, Seunghun Ji, Jacob Morton, Frederik Bous, Joon Byun, Juheon Lee</strong></p>
<p>We present a novel text-to-speech (TTS) system, namely SupertonicTTS, for improved scalability and efficiency in speech synthesis. SupertonicTTS is comprised of three components: a speech autoencoder for continuous latent representation, a text-to-latent module leveraging flow-matching for text-to-latent mapping, and an utterance-level duration predictor. To enable a lightweight architecture, we employ a low-dimensional latent space, temporal compression of latents, and ConvNeXt blocks. We further simplify the TTS pipeline by operating directly on raw character-level text and employing cross-attention for text-speech alignment, thus eliminating the need for grapheme-to-phoneme (G2P) modules and external aligners. In addition, we introduce context-sharing batch expansion that accelerates loss convergence and stabilizes text-speech alignment. Experimental results demonstrate that SupertonicTTS achieves competitive performance while significantly reducing architectural complexity and computational overhead compared to contemporary TTS models. Audio samples demonstrating the capabilities of SupertonicTTS are available at: <a target="_blank" rel="noopener" href="https://supertonictts.github.io/">https://supertonictts.github.io/</a>. </p>
<blockquote>
<p>æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°å‹æ–‡æœ¬åˆ°è¯­éŸ³ï¼ˆTTSï¼‰ç³»ç»Ÿï¼Œå³SupertonicTTSï¼Œä»¥æé«˜è¯­éŸ³åˆæˆçš„å¯æ‰©å±•æ€§å’Œæ•ˆç‡ã€‚SupertonicTTSç”±ä¸‰ä¸ªç»„ä»¶ç»„æˆï¼šç”¨äºè¿ç»­æ½œåœ¨è¡¨ç¤ºçš„è¯­éŸ³è‡ªåŠ¨ç¼–ç å™¨ï¼Œåˆ©ç”¨æµåŒ¹é…è¿›è¡Œæ–‡æœ¬åˆ°æ½œåœ¨æ˜ å°„çš„æ–‡æœ¬åˆ°æ½œåœ¨æ¨¡å—ï¼Œä»¥åŠå¥å­çº§åˆ«çš„æŒç»­æ—¶é—´é¢„æµ‹å™¨ã€‚ä¸ºäº†å®ç°è½»é‡çº§æ¶æ„ï¼Œæˆ‘ä»¬é‡‡ç”¨äº†ä½ç»´æ½œåœ¨ç©ºé—´ã€æ½œåœ¨æ—¶é—´çš„å‹ç¼©å’ŒConvNeXtå—ã€‚æˆ‘ä»¬é€šè¿‡å¯¹åŸå§‹å­—ç¬¦çº§æ–‡æœ¬è¿›è¡Œç›´æ¥æ“ä½œï¼Œå¹¶é‡‡ç”¨è·¨æ³¨æ„åŠ›è¿›è¡Œæ–‡æœ¬è¯­éŸ³å¯¹é½ï¼Œä»è€Œç®€åŒ–äº†TTSç®¡é“ï¼Œä»è€Œæ¶ˆé™¤äº†å¯¹å­—æ¯åˆ°éŸ³ç´ ï¼ˆG2Pï¼‰æ¨¡å—å’Œå¤–éƒ¨å¯¹é½å™¨çš„éœ€æ±‚ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸Šä¸‹æ–‡å…±äº«æ‰¹é‡æ‰©å±•ï¼Œè¿™åŠ é€Ÿäº†æŸå¤±æ”¶æ•›å¹¶ç¨³å®šäº†æ–‡æœ¬è¯­éŸ³å¯¹é½ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒSupertonicTTSåœ¨é™ä½æ¶æ„å¤æ‚æ€§å’Œè®¡ç®—å¼€é”€çš„åŒæ—¶ï¼Œä¸å½“ä»£TTSæ¨¡å‹ç›¸æ¯”å…·æœ‰ç«äº‰åŠ›ã€‚æœ‰å…³å±•ç¤ºSupertonicTTSåŠŸèƒ½çš„éŸ³é¢‘æ ·æœ¬ï¼Œè¯·è®¿é—®ï¼š<a target="_blank" rel="noopener" href="https://supertonictts.github.io/%E3%80%82">https://supertonictts.github.io/ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.23108v1">PDF</a> 19 pages, preprint</p>
<p><strong>Summary</strong><br>     æå‡ºä¸€ç§æ–°å‹æ–‡æœ¬åˆ°è¯­éŸ³ï¼ˆTTSï¼‰ç³»ç»Ÿâ€”â€”SupertonicTTSï¼Œæ—¨åœ¨æé«˜è¯­éŸ³åˆæˆçš„å¯æ‰©å±•æ€§å’Œæ•ˆç‡ã€‚è¯¥ç³»ç»ŸåŒ…å«ä¸‰ä¸ªç»„ä»¶ï¼šç”¨äºè¿ç»­æ½œåœ¨è¡¨ç¤ºçš„è¯­éŸ³è‡ªåŠ¨ç¼–ç å™¨ã€åˆ©ç”¨æµåŒ¹é…è¿›è¡Œæ–‡æœ¬åˆ°æ½œåœ¨æ˜ å°„çš„æ–‡æœ¬åˆ°æ½œåœ¨æ¨¡å—ï¼Œä»¥åŠå¥å­çº§åˆ«çš„æŒç»­æ—¶é—´é¢„æµ‹å™¨ã€‚é€šè¿‡é‡‡ç”¨ä½ç»´æ½œåœ¨ç©ºé—´ã€æ½œåœ¨æ—¶æ€å‹ç¼©å’ŒConvNeXtå—ï¼Œå®ç°äº†è½»é‡åŒ–æ¶æ„ã€‚è¿›ä¸€æ­¥ç®€åŒ–TTSç®¡é“ï¼Œç›´æ¥åœ¨å­—ç¬¦çº§æ–‡æœ¬ä¸Šæ“ä½œï¼Œé‡‡ç”¨è·¨æ³¨æ„åŠ›è¿›è¡Œæ–‡æœ¬è¯­éŸ³å¯¹é½ï¼Œä»è€Œæ¶ˆé™¤äº†å¯¹å­—æ¯åˆ°è¯­éŸ³ï¼ˆG2Pï¼‰æ¨¡å—å’Œå¤–éƒ¨å¯¹é½å™¨çš„éœ€æ±‚ã€‚æ­¤å¤–ï¼Œè¿˜å¼•å…¥äº†ä¸Šä¸‹æ–‡å…±äº«æ‰¹é‡æ‰©å±•ï¼Œä»¥åŠ é€ŸæŸå¤±æ”¶æ•›å¹¶ç¨³å®šæ–‡æœ¬è¯­éŸ³å¯¹é½ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒSupertonicTTSåœ¨é™ä½æ¶æ„å¤æ‚æ€§å’Œè®¡ç®—å¼€é”€çš„åŒæ—¶ï¼Œå®ç°äº†ä¸å½“ä»£TTSæ¨¡å‹ç›¸å½“çš„æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>SupertonicTTSæ˜¯ä¸€ä¸ªæ–°å‹çš„æ–‡æœ¬åˆ°è¯­éŸ³ï¼ˆTTSï¼‰ç³»ç»Ÿï¼Œæ—¨åœ¨æé«˜è¯­éŸ³åˆæˆçš„å¯æ‰©å±•æ€§å’Œæ•ˆç‡ã€‚</li>
<li>ç³»ç»ŸåŒ…å«ä¸‰ä¸ªä¸»è¦ç»„ä»¶ï¼šè¯­éŸ³è‡ªåŠ¨ç¼–ç å™¨ã€æ–‡æœ¬åˆ°æ½œåœ¨æ¨¡å—å’ŒæŒç»­æ—¶é—´é¢„æµ‹å™¨ã€‚</li>
<li>é€šè¿‡é‡‡ç”¨ä½ç»´æ½œåœ¨ç©ºé—´ã€æ½œåœ¨æ—¶æ€å‹ç¼©å’ŒConvNeXtå—ï¼Œå®ç°äº†è½»é‡åŒ–æ¶æ„ã€‚</li>
<li>ç›´æ¥åœ¨å­—ç¬¦çº§æ–‡æœ¬ä¸Šæ“ä½œï¼Œé‡‡ç”¨è·¨æ³¨æ„åŠ›å¯¹é½ï¼Œç®€åŒ–äº†TTSç®¡é“ï¼Œæ¶ˆé™¤äº†å¯¹G2Pæ¨¡å—å’Œå¤–éƒ¨å¯¹é½å™¨çš„éœ€æ±‚ã€‚</li>
<li>å¼•å…¥äº†ä¸Šä¸‹æ–‡å…±äº«æ‰¹é‡æ‰©å±•ï¼Œä»¥åŠ é€ŸæŸå¤±æ”¶æ•›å¹¶ç¨³å®šæ–‡æœ¬è¯­éŸ³å¯¹é½ã€‚</li>
<li>å®éªŒç»“æœè¡¨æ˜ï¼ŒSupertonicTTSåœ¨é™ä½è®¡ç®—å¼€é”€çš„åŒæ—¶ï¼Œå®ç°äº†ä¸å½“ä»£TTSæ¨¡å‹ç›¸å½“çš„æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.23108">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-fee073788125b542056d93c1e00317cf.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4aa87ee10749829c8848a685c2248f4c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e33ecec007f9e4564e4a174ccd351b45.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Dual-Audio-Centric-Modality-Coupling-for-Talking-Head-Generation"><a href="#Dual-Audio-Centric-Modality-Coupling-for-Talking-Head-Generation" class="headerlink" title="Dual Audio-Centric Modality Coupling for Talking Head Generation"></a>Dual Audio-Centric Modality Coupling for Talking Head Generation</h2><p><strong>Authors:Ao Fu, Ziqi Ni, Yi Zhou</strong></p>
<p>The generation of audio-driven talking head videos is a key challenge in computer vision and graphics, with applications in virtual avatars and digital media. Traditional approaches often struggle with capturing the complex interaction between audio and facial dynamics, leading to lip synchronization and visual quality issues. In this paper, we propose a novel NeRF-based framework, Dual Audio-Centric Modality Coupling (DAMC), which effectively integrates content and dynamic features from audio inputs. By leveraging a dual encoder structure, DAMC captures semantic content through the Content-Aware Encoder and ensures precise visual synchronization through the Dynamic-Sync Encoder. These features are fused using a Cross-Synchronized Fusion Module (CSFM), enhancing content representation and lip synchronization. Extensive experiments show that our method outperforms existing state-of-the-art approaches in key metrics such as lip synchronization accuracy and image quality, demonstrating robust generalization across various audio inputs, including synthetic speech from text-to-speech (TTS) systems. Our results provide a promising solution for high-quality, audio-driven talking head generation and present a scalable approach for creating realistic talking heads. </p>
<blockquote>
<p>éŸ³é¢‘é©±åŠ¨çš„è¯´è¯äººå¤´éƒ¨çš„è§†é¢‘ç”Ÿæˆæ˜¯è®¡ç®—æœºè§†è§‰å’Œå›¾å½¢é¢†åŸŸçš„ä¸€ä¸ªå…³é”®æŒ‘æˆ˜ï¼Œåœ¨è™šæ‹ŸåŒ–èº«å’Œæ•°å­—åª’ä½“ä¸­æœ‰å¹¿æ³›çš„åº”ç”¨ã€‚ä¼ ç»Ÿçš„æ–¹æ³•åœ¨æ•æ‰éŸ³é¢‘å’Œé¢éƒ¨åŠ¨æ€ä¹‹é—´çš„å¤æ‚äº¤äº’æ—¶å¾€å¾€ä¼šé‡åˆ°å›°éš¾ï¼Œä»è€Œå¯¼è‡´å”‡éƒ¨åŒæ­¥å’Œè§†è§‰è´¨é‡é—®é¢˜ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºNeRFçš„æ–°å‹æ¡†æ¶â€”â€”åŒéŸ³é¢‘ä¸­å¿ƒæ¨¡æ€è€¦åˆï¼ˆDAMCï¼‰ï¼Œå®ƒèƒ½å¤Ÿæœ‰æ•ˆåœ°æ•´åˆéŸ³é¢‘è¾“å…¥çš„å†…å®¹å’ŒåŠ¨æ€ç‰¹å¾ã€‚é€šè¿‡åˆ©ç”¨åŒç¼–ç å™¨ç»“æ„ï¼ŒDAMCé€šè¿‡å†…å®¹æ„ŸçŸ¥ç¼–ç å™¨æ•è·è¯­ä¹‰å†…å®¹ï¼Œå¹¶é€šè¿‡åŠ¨æ€åŒæ­¥ç¼–ç å™¨ç¡®ä¿ç²¾ç¡®çš„è§†è§‰åŒæ­¥ã€‚è¿™äº›ç‰¹å¾ä½¿ç”¨è·¨åŒæ­¥èåˆæ¨¡å—ï¼ˆCSFMï¼‰è¿›è¡Œèåˆï¼Œå¢å¼ºäº†å†…å®¹è¡¨ç¤ºå’Œå”‡éƒ¨åŒæ­¥ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨å”‡åŒæ­¥å‡†ç¡®æ€§å’Œå›¾åƒè´¨é‡ç­‰å…³é”®æŒ‡æ ‡ä¸Šä¼˜äºç°æœ‰çš„æœ€å…ˆè¿›çš„æ–¹æ³•ï¼Œè¯æ˜äº†åœ¨å„ç§éŸ³é¢‘è¾“å…¥ä¸Šçš„ç¨³å¥æ³›åŒ–èƒ½åŠ›ï¼ŒåŒ…æ‹¬æ¥è‡ªæ–‡æœ¬åˆ°è¯­éŸ³ï¼ˆTTSï¼‰ç³»ç»Ÿçš„åˆæˆè¯­éŸ³ã€‚æˆ‘ä»¬çš„ç»“æœæä¾›äº†é«˜è´¨é‡çš„éŸ³é¢‘é©±åŠ¨çš„è¯´è¯äººå¤´éƒ¨çš„ç”Ÿæˆçš„ä¸€ç§æœ‰å‰é€”çš„è§£å†³æ–¹æ¡ˆï¼Œå¹¶æå‡ºäº†ä¸€ç§åˆ›å»ºé€¼çœŸè¯´è¯å¤´éƒ¨çš„æ–¹æ³•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.22728v1">PDF</a> 9 pages, 4 figures</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºNeRFçš„åŒéŸ³é¢‘ä¸­å¿ƒæ¨¡æ€è€¦åˆï¼ˆDAMCï¼‰æ¡†æ¶ï¼Œç”¨äºç”ŸæˆéŸ³é¢‘é©±åŠ¨çš„è¯´è¯äººå¤´è§†é¢‘ã€‚è¯¥æ¡†æ¶é€šè¿‡å†…å®¹æ„ŸçŸ¥ç¼–ç å™¨å’ŒåŠ¨æ€åŒæ­¥ç¼–ç å™¨ï¼Œæœ‰æ•ˆæ•´åˆéŸ³é¢‘è¾“å…¥çš„å†…å®¹ä¸åŠ¨æ€ç‰¹å¾ï¼Œæé«˜å†…å®¹è¡¨è¾¾å’Œå”‡åŒæ­¥ç²¾åº¦ã€‚å®éªŒç»“æœè¯æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å”‡åŒæ­¥å‡†ç¡®æ€§å’Œå›¾åƒè´¨é‡ç­‰æ–¹é¢ä¼˜äºç°æœ‰å…ˆè¿›æŠ€æœ¯ï¼Œä¸ºé«˜è´¨é‡éŸ³é¢‘é©±åŠ¨çš„è¯´è¯äººå¤´ç”Ÿæˆæä¾›äº†æœ‰å‰é€”çš„è§£å†³æ–¹æ¡ˆã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æœ¬æ–‡è§£å†³äº†è®¡ç®—æœºè§†è§‰å’Œå›¾å½¢å­¦ä¸­çš„éŸ³é¢‘é©±åŠ¨è¯´è¯äººå¤´è§†é¢‘ç”Ÿæˆçš„å…³é”®æŒ‘æˆ˜ã€‚</li>
<li>æå‡ºäº†åŸºäºNeRFçš„åŒéŸ³é¢‘ä¸­å¿ƒæ¨¡æ€è€¦åˆï¼ˆDAMCï¼‰æ¡†æ¶ï¼Œæœ‰æ•ˆæ•´åˆéŸ³é¢‘è¾“å…¥çš„å†…å®¹ä¸åŠ¨æ€ç‰¹å¾ã€‚</li>
<li>é€šè¿‡å†…å®¹æ„ŸçŸ¥ç¼–ç å™¨å’ŒåŠ¨æ€åŒæ­¥ç¼–ç å™¨ï¼ŒDAMCæé«˜äº†å†…å®¹è¡¨è¾¾å’Œå”‡åŒæ­¥ç²¾åº¦ã€‚</li>
<li>æ¡†æ¶åŒ…å«äº¤å‰åŒæ­¥èåˆæ¨¡å—ï¼ˆCSFMï¼‰ï¼Œç”¨äºå¢å¼ºå†…å®¹è¡¨ç¤ºå’Œå”‡åŒæ­¥ã€‚</li>
<li>å®éªŒè¯æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å”‡åŒæ­¥å‡†ç¡®æ€§å’Œå›¾åƒè´¨é‡ç­‰æ–¹é¢ä¼˜äºç°æœ‰æŠ€æœ¯ã€‚</li>
<li>è¯¥æ–¹æ³•èƒ½å¤Ÿå¤„ç†å„ç§éŸ³é¢‘è¾“å…¥ï¼ŒåŒ…æ‹¬æ–‡æœ¬åˆ°è¯­éŸ³ï¼ˆTTSï¼‰ç³»ç»Ÿç”Ÿæˆçš„åˆæˆè¯­éŸ³ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.22728">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-2b2c7d49850f18f4d83bef5aebdce69d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f14fbcc16baec813cf008cbb3379f343.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d792a51b6c87be14443f6a1fbc972941.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-df38b1b97f3c37aa076c7f6fdd8e6fd1.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-2563f0e3ff5ac02b7aacf1380dc9a5c9.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-10b2b6a0f229fd4764b744d91119afd6.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="DeepAudio-V1-Towards-Multi-Modal-Multi-Stage-End-to-End-Video-to-Speech-and-Audio-Generation"><a href="#DeepAudio-V1-Towards-Multi-Modal-Multi-Stage-End-to-End-Video-to-Speech-and-Audio-Generation" class="headerlink" title="DeepAudio-V1:Towards Multi-Modal Multi-Stage End-to-End Video to Speech   and Audio Generation"></a>DeepAudio-V1:Towards Multi-Modal Multi-Stage End-to-End Video to Speech   and Audio Generation</h2><p><strong>Authors:Haomin Zhang, Chang Liu, Junjie Zheng, Zihao Chen, Chaofan Ding, Xinhan Di</strong></p>
<p>Currently, high-quality, synchronized audio is synthesized using various multi-modal joint learning frameworks, leveraging video and optional text inputs. In the video-to-audio benchmarks, video-to-audio quality, semantic alignment, and audio-visual synchronization are effectively achieved. However, in real-world scenarios, speech and audio often coexist in videos simultaneously, and the end-to-end generation of synchronous speech and audio given video and text conditions are not well studied. Therefore, we propose an end-to-end multi-modal generation framework that simultaneously produces speech and audio based on video and text conditions. Furthermore, the advantages of video-to-audio (V2A) models for generating speech from videos remain unclear. The proposed framework, DeepAudio, consists of a video-to-audio (V2A) module, a text-to-speech (TTS) module, and a dynamic mixture of modality fusion (MoF) module. In the evaluation, the proposed end-to-end framework achieves state-of-the-art performance on the video-audio benchmark, video-speech benchmark, and text-speech benchmark. In detail, our framework achieves comparable results in the comparison with state-of-the-art models for the video-audio and text-speech benchmarks, and surpassing state-of-the-art models in the video-speech benchmark, with WER 16.57% to 3.15% (+80.99%), SPK-SIM 78.30% to 89.38% (+14.15%), EMO-SIM 66.24% to 75.56% (+14.07%), MCD 8.59 to 7.98 (+7.10%), MCD SL 11.05 to 9.40 (+14.93%) across a variety of dubbing settings. </p>
<blockquote>
<p>ç›®å‰ï¼Œé«˜è´¨é‡çš„åŒæ­¥éŸ³é¢‘æ˜¯ä½¿ç”¨å„ç§å¤šæ¨¡æ€è”åˆå­¦ä¹ æ¡†æ¶è¿›è¡Œåˆæˆçš„ï¼Œè¿™äº›æ¡†æ¶åˆ©ç”¨è§†é¢‘å’Œå¯é€‰çš„æ–‡æœ¬è¾“å…¥ã€‚åœ¨è§†é¢‘åˆ°éŸ³é¢‘çš„åŸºå‡†æµ‹è¯•ä¸­ï¼Œè§†é¢‘åˆ°éŸ³é¢‘çš„è´¨é‡ã€è¯­ä¹‰å¯¹é½å’Œè§†å¬åŒæ­¥éƒ½å¾—åˆ°äº†æœ‰æ•ˆå®ç°ã€‚ç„¶è€Œï¼Œåœ¨çœŸå®åœºæ™¯ä¸­ï¼Œè¯­éŸ³å’ŒéŸ³é¢‘ç»å¸¸åŒæ—¶å­˜åœ¨äºè§†é¢‘ä¸­ï¼Œå¯¹äºç»™å®šè§†é¢‘å’Œæ–‡æœ¬æ¡ä»¶çš„ç«¯åˆ°ç«¯çš„åŒæ­¥è¯­éŸ³å’ŒéŸ³é¢‘ç”Ÿæˆç ”ç©¶è¿˜ä¸å¤Ÿå……åˆ†ã€‚å› æ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ç«¯åˆ°ç«¯çš„å¤šæ¨¡æ€ç”Ÿæˆæ¡†æ¶ï¼Œè¯¥æ¡†æ¶å¯ä»¥åŒæ—¶æ ¹æ®è§†é¢‘å’Œæ–‡æœ¬æ¡ä»¶ç”Ÿæˆè¯­éŸ³å’ŒéŸ³é¢‘ã€‚æ­¤å¤–ï¼Œè§†é¢‘åˆ°éŸ³é¢‘ï¼ˆV2Aï¼‰æ¨¡å‹åœ¨ç”Ÿæˆè§†é¢‘è¯­éŸ³çš„ä¼˜åŠ¿å°šä¸æ¸…æ¥šã€‚æ‰€æå‡ºçš„æ¡†æ¶DeepAudioç”±è§†é¢‘åˆ°éŸ³é¢‘ï¼ˆV2Aï¼‰æ¨¡å—ã€æ–‡æœ¬åˆ°è¯­éŸ³ï¼ˆTTSï¼‰æ¨¡å—å’ŒåŠ¨æ€æ¨¡æ€èåˆï¼ˆMoFï¼‰æ¨¡å—ç»„æˆã€‚åœ¨è¯„ä¼°ä¸­ï¼Œæ‰€æå‡ºç«¯åˆ°ç«¯æ¡†æ¶åœ¨è§†é¢‘éŸ³é¢‘åŸºå‡†æµ‹è¯•ã€è§†é¢‘è¯­éŸ³åŸºå‡†æµ‹è¯•å’Œæ–‡æœ¬è¯­éŸ³åŸºå‡†æµ‹è¯•ä¸Šå‡è¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬çš„æ¡†æ¶åœ¨ä¸è§†é¢‘éŸ³é¢‘å’Œæ–‡æœ¬è¯­éŸ³åŸºå‡†æµ‹è¯•çš„æœ€å…ˆè¿›æ¨¡å‹çš„æ¯”è¾ƒä¸­å–å¾—äº†ç›¸å½“çš„ç»“æœï¼Œå¹¶åœ¨è§†é¢‘è¯­éŸ³åŸºå‡†æµ‹è¯•ä¸­è¶…è¶Šäº†æœ€å…ˆè¿›çš„æ¨¡å‹ï¼Œå…¶ä¸­å­—è¯é”™è¯¯ç‡ä»16.57%é™ä½åˆ°3.15%ï¼ˆ+80.99%ï¼‰ï¼Œè¯­éŸ³ç›¸ä¼¼æ€§ä»78.30%æé«˜åˆ°89.38%ï¼ˆ+14.15%ï¼‰ï¼Œæƒ…ç»ªç›¸ä¼¼æ€§ä»66.24%æé«˜åˆ°75.56%ï¼ˆ+14.07%ï¼‰ï¼ŒMCDä»8.59é™è‡³7.98ï¼ˆ+7.10%ï¼‰ï¼ŒMCD SLä»11.05é™è‡³9.40ï¼ˆ+14.93%ï¼‰ï¼Œè·¨è¶Šäº†å„ç§é…éŸ³è®¾ç½®ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.22265v1">PDF</a> 11 pages, 5 figures</p>
<p><strong>Summary</strong>:<br>åœ¨æ·±å±‚æ¬¡çš„å¤šåª’ä½“è”åˆå­¦ä¹ æ¡†æ¶ä¸‹ï¼Œåˆ©ç”¨è§†é¢‘å’Œæ–‡æœ¬è¾“å…¥ç”Ÿæˆé«˜è´¨é‡åŒæ­¥éŸ³é¢‘æ˜¯å½“å‰ç ”ç©¶çš„çƒ­ç‚¹ã€‚é’ˆå¯¹è§†é¢‘ä¸­çš„è¯­éŸ³å’ŒéŸ³é¢‘åŒæ—¶ç”Ÿæˆçš„é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§ç«¯å¯¹ç«¯çš„å¤šæ¨¡å¼ç”Ÿæˆæ¡†æ¶ï¼Œè¯¥æ¡†æ¶èƒ½åŸºäºè§†é¢‘å’Œæ–‡æœ¬æ¡ä»¶åŒæ—¶äº§ç”Ÿè¯­éŸ³å’ŒéŸ³é¢‘ã€‚æ­¤å¤–ï¼Œè¯¥æ¡†æ¶è¿˜åŒ…æ‹¬è§†é¢‘åˆ°éŸ³é¢‘æ¨¡å—ã€æ–‡æœ¬åˆ°è¯­éŸ³æ¨¡å—ä»¥åŠåŠ¨æ€æ¨¡æ€èåˆæ¨¡å—ï¼Œå¹¶åœ¨è§†é¢‘éŸ³é¢‘ã€è§†é¢‘è¯­éŸ³å’Œæ–‡æœ¬è¯­éŸ³çš„åŸºå‡†æµ‹è¯•ä¸­å–å¾—äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong>:</p>
<ol>
<li>å½“å‰é«˜è´¨é‡åŒæ­¥éŸ³é¢‘çš„åˆæˆé‡‡ç”¨å¤šæ¨¡æ€è”åˆå­¦ä¹ æ¡†æ¶ï¼Œåˆ©ç”¨è§†é¢‘å’Œå¯é€‰æ–‡æœ¬è¾“å…¥ã€‚</li>
<li>è§†é¢‘ä¸­çš„è¯­éŸ³å’ŒéŸ³é¢‘åœ¨ç°å®ä¸­å¸¸åŒæ—¶å­˜åœ¨ï¼Œç«¯å¯¹ç«¯çš„åŒæ­¥è¯­éŸ³å’ŒéŸ³é¢‘ç”Ÿæˆç ”ç©¶ä¸è¶³ã€‚</li>
<li>æå‡ºäº†ä¸€ç§ç«¯å¯¹ç«¯çš„å¤šæ¨¡å¼ç”Ÿæˆæ¡†æ¶ï¼Œèƒ½åŸºäºè§†é¢‘å’Œæ–‡æœ¬æ¡ä»¶åŒæ—¶äº§ç”Ÿè¯­éŸ³å’ŒéŸ³é¢‘ã€‚</li>
<li>æ¡†æ¶åŒ…æ‹¬è§†é¢‘åˆ°éŸ³é¢‘æ¨¡å—ã€æ–‡æœ¬åˆ°è¯­éŸ³æ¨¡å—ä»¥åŠåŠ¨æ€æ¨¡æ€èåˆæ¨¡å—ã€‚</li>
<li>è¯¥æ¡†æ¶åœ¨è§†é¢‘éŸ³é¢‘ã€è§†é¢‘è¯­éŸ³å’Œæ–‡æœ¬è¯­éŸ³çš„åŸºå‡†æµ‹è¯•ä¸­å®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚</li>
<li>ä¸å…¶ä»–æ¨¡å‹ç›¸æ¯”ï¼Œè¯¥æ¡†æ¶åœ¨è§†é¢‘è¯­éŸ³åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å°¤ä¸ºçªå‡ºã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.22265">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-e745f63970742adbd3d5084a59108c61.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-17b3820bbce588d949bbce154145bbc1.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b79bfc85872940a52048ed7dbc2f52a7.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="F-INR-Functional-Tensor-Decomposition-for-Implicit-Neural-Representations"><a href="#F-INR-Functional-Tensor-Decomposition-for-Implicit-Neural-Representations" class="headerlink" title="F-INR: Functional Tensor Decomposition for Implicit Neural   Representations"></a>F-INR: Functional Tensor Decomposition for Implicit Neural   Representations</h2><p><strong>Authors:Sai Karthikeya Vemuri, Tim BÃ¼chner, Joachim Denzler</strong></p>
<p>Implicit Neural Representation (INR) has emerged as a powerful tool for encoding discrete signals into continuous, differentiable functions using neural networks. However, these models often have an unfortunate reliance on monolithic architectures to represent high-dimensional data, leading to prohibitive computational costs as dimensionality grows. We propose F-INR, a framework that reformulates INR learning through functional tensor decomposition, breaking down high-dimensional tasks into lightweight, axis-specific sub-networks. Each sub-network learns a low-dimensional data component (e.g., spatial or temporal). Then, we combine these components via tensor operations, reducing forward pass complexity while improving accuracy through specialized learning. F-INR is modular and, therefore, architecture-agnostic, compatible with MLPs, SIREN, WIRE, or other state-of-the-art INR architecture. It is also decomposition-agnostic, supporting CP, TT, and Tucker modes with user-defined rank for speed-accuracy control. In our experiments, F-INR trains $100\times$ faster than existing approaches on video tasks while achieving higher fidelity (+3.4 dB PSNR). Similar gains hold for image compression, physics simulations, and 3D geometry reconstruction. Through this, F-INR offers a new scalable, flexible solution for high-dimensional signal modeling. </p>
<blockquote>
<p>éšå¼ç¥ç»ç½‘ç»œè¡¨ç¤ºï¼ˆINRï¼‰å·²ç»æˆä¸ºä¸€ç§å¼ºå¤§çš„å·¥å…·ï¼Œèƒ½å¤Ÿåˆ©ç”¨ç¥ç»ç½‘ç»œå°†ç¦»æ•£ä¿¡å·ç¼–ç ä¸ºè¿ç»­ã€å¯å¾®åˆ†çš„å‡½æ•°ã€‚ç„¶è€Œï¼Œè¿™äº›æ¨¡å‹å¸¸å¸¸ä¾èµ–å•ä¸€çš„æ¶æ„æ¥è¡¨ç¤ºé«˜ç»´æ•°æ®ï¼Œéšç€ç»´åº¦çš„å¢é•¿ï¼Œè®¡ç®—æˆæœ¬ä¹Ÿéšä¹‹å¢åŠ ã€‚æˆ‘ä»¬æå‡ºäº†F-INRæ¡†æ¶ï¼Œå®ƒé€šè¿‡å‡½æ•°å¼ é‡åˆ†è§£é‡æ–°æ„å»ºäº†INRå­¦ä¹ ï¼Œå°†é«˜ç»´ä»»åŠ¡åˆ†è§£ä¸ºè½»é‡ã€é’ˆå¯¹è½´çš„å­ç½‘ç»œã€‚æ¯ä¸ªå­ç½‘ç»œå­¦ä¹ ä½ç»´æ•°æ®ç»„ä»¶ï¼ˆä¾‹å¦‚ï¼Œç©ºé—´æˆ–æ—¶é—´ï¼‰ã€‚ç„¶åï¼Œæˆ‘ä»¬é€šè¿‡å¼ é‡è¿ç®—å°†è¿™äº›ç»„ä»¶ç»„åˆèµ·æ¥ï¼Œé™ä½äº†å‰å‘ä¼ æ’­çš„å¤æ‚æ€§ï¼ŒåŒæ—¶é€šè¿‡ä¸“é¡¹å­¦ä¹ æé«˜äº†å‡†ç¡®æ€§ã€‚F-INRå…·æœ‰æ¨¡å—åŒ–ç‰¹æ€§ï¼Œå› æ­¤å®ƒæ˜¯ä¸æ¶æ„æ— å…³çš„ï¼Œå¯ä»¥ä¸MLPã€SIRENã€WIREæˆ–å…¶ä»–æœ€å…ˆè¿›çš„INRæ¶æ„å…¼å®¹ã€‚å®ƒä¹Ÿæ˜¯ä¸åˆ†è§£æ–¹å¼æ— å…³çš„ï¼Œæ”¯æŒCPã€TTå’ŒTuckeræ¨¡å¼ï¼Œç”¨æˆ·å®šä¹‰çš„ç­‰çº§å¯ç”¨äºæ§åˆ¶é€Ÿåº¦å’Œå‡†ç¡®æ€§ã€‚åœ¨æˆ‘ä»¬çš„å®éªŒä¸­ï¼ŒF-INRåœ¨è§†é¢‘ä»»åŠ¡ä¸Šçš„è®­ç»ƒé€Ÿåº¦æ˜¯ç°æœ‰æ–¹æ³•çš„100å€ï¼ŒåŒæ—¶å®ç°äº†æ›´é«˜çš„ä¿çœŸåº¦ï¼ˆ+3.4åˆ†è´PSNRï¼‰ã€‚ç±»ä¼¼çš„å¢ç›Šåœ¨å›¾åƒå‹ç¼©ã€ç‰©ç†æ¨¡æ‹Ÿå’Œ3Då‡ ä½•é‡å»ºä¸­åŒæ ·å­˜åœ¨ã€‚å› æ­¤ï¼ŒF-INRä¸ºé«˜æ€§èƒ½ä¿¡å·å»ºæ¨¡æä¾›äº†æ–°çš„å¯æ‰©å±•å’Œçµæ´»è§£å†³æ–¹æ¡ˆã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.21507v1">PDF</a> 26 pages, 33 figures, 12 tables</p>
<p><strong>Summary</strong></p>
<p>åŸºäºéšç¥ç»è¡¨ç¤ºï¼ˆINRï¼‰çš„å¼ºå¤§åŠŸèƒ½ï¼Œæå‡ºä¸€ç§æ–°å‹æ¡†æ¶F-INRï¼Œé€šè¿‡åŠŸèƒ½å¼ é‡åˆ†è§£é‡æ„INRå­¦ä¹ ã€‚å®ƒå°†é«˜ç»´ä»»åŠ¡åˆ†è§£ä¸ºè½»é‡çº§çš„ã€é’ˆå¯¹è½´çš„å­ç½‘ç»œï¼Œæ¯ä¸ªå­ç½‘ç»œå­¦ä¹ ä½ç»´æ•°æ®ç»„ä»¶ï¼ˆå¦‚ç©ºé—´æˆ–æ—¶é—´ï¼‰ã€‚é€šè¿‡å¼ é‡æ“ä½œç»„åˆè¿™äº›ç»„ä»¶ï¼Œé™ä½å‰å‘ä¼ æ’­å¤æ‚æ€§ï¼ŒåŒæ—¶é€šè¿‡ä¸“é¡¹å­¦ä¹ æé«˜å‡†ç¡®æ€§ã€‚F-INRå…·æœ‰æ¨¡å—åŒ–ã€é€šç”¨æ€§å’Œå¯ä¼¸ç¼©æ€§ï¼Œä¸å¤šç§æœ€æ–°INRæ¶æ„å…¼å®¹ï¼Œå¹¶æ”¯æŒå¤šç§åˆ†è§£æ¨¡å¼ã€‚å®éªŒè¡¨æ˜ï¼ŒF-INRåœ¨è§†é¢‘ä»»åŠ¡ä¸Šçš„è®­ç»ƒé€Ÿåº¦æ˜¯ç°æœ‰æ–¹æ³•çš„100å€ï¼ŒåŒæ—¶å®ç°äº†æ›´é«˜çš„ä¿çœŸåº¦ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>F-INRåˆ©ç”¨éšç¥ç»è¡¨ç¤ºï¼ˆINRï¼‰çš„å¼ºå¤§åŠŸèƒ½ï¼Œé’ˆå¯¹é«˜ç»´æ•°æ®è¡¨ç¤ºè¿›è¡Œäº†ä¼˜åŒ–ã€‚</li>
<li>é€šè¿‡åŠŸèƒ½å¼ é‡åˆ†è§£é‡æ„INRå­¦ä¹ ï¼Œåˆ†è§£é«˜ç»´ä»»åŠ¡ä¸ºè½´ç‰¹å®šçš„å­ç½‘ç»œã€‚</li>
<li>å­ç½‘ç»œå­¦ä¹ ä½ç»´æ•°æ®ç»„ä»¶ï¼ˆå¦‚ç©ºé—´æˆ–æ—¶é—´ï¼‰ï¼Œç„¶åé€šè¿‡å¼ é‡æ“ä½œç»„åˆå®ƒä»¬ã€‚</li>
<li>F-INRæé«˜äº†å‡†ç¡®æ€§ï¼ŒåŒæ—¶é™ä½äº†å‰å‘ä¼ æ’­çš„å¤æ‚æ€§ã€‚</li>
<li>F-INRå…·æœ‰æ¨¡å—åŒ–ã€é€šç”¨æ€§å’Œå¯ä¼¸ç¼©æ€§ï¼Œä¸å¤šç§æœ€æ–°INRæ¶æ„å…¼å®¹ã€‚</li>
<li>F-INRæ”¯æŒå¤šç§åˆ†è§£æ¨¡å¼ï¼ŒåŒ…æ‹¬CPã€TTå’ŒTuckeræ¨¡å¼ï¼Œå¹¶æä¾›ç”¨æˆ·å®šä¹‰çš„ç­‰çº§ä»¥å®ç°é€Ÿåº¦å’Œå‡†ç¡®æ€§çš„æ§åˆ¶ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.21507">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-6ee9105b0361c08ef318683d8a16808e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7c1e5cbab226bb3e111af4c010451ed7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a13eec11f8fda88d7e9b8d338f708251.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-b6fb51be1cb821a52c031e1edebf53ce.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-a782cc9b06bbca4537087d00987f1851.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-307a18f3245bea30d1aae78b002fffa0.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Text-Driven-Voice-Conversion-via-Latent-State-Space-Modeling"><a href="#Text-Driven-Voice-Conversion-via-Latent-State-Space-Modeling" class="headerlink" title="Text-Driven Voice Conversion via Latent State-Space Modeling"></a>Text-Driven Voice Conversion via Latent State-Space Modeling</h2><p><strong>Authors:Wen Li, Sofia Martinez, Priyanka Shah</strong></p>
<p>Text-driven voice conversion allows customization of speaker characteristics and prosodic elements using textual descriptions. However, most existing methods rely heavily on direct text-to-speech training, limiting their flexibility in controlling nuanced style elements or timbral features. In this paper, we propose a novel \textbf{Latent State-Space} approach for text-driven voice conversion (\textbf{LSS-VC}). Our method treats each utterance as an evolving dynamical system in a continuous latent space. Drawing inspiration from mamba, which introduced a state-space model for efficient text-driven \emph{image} style transfer, we adapt a loosely related methodology for \emph{voice} style transformation. Specifically, we learn a voice latent manifold where style and content can be manipulated independently by textual style prompts. We propose an adaptive cross-modal fusion mechanism to inject style information into the voice latent representation, enabling interpretable and fine-grained control over speaker identity, speaking rate, and emphasis. Extensive experiments show that our approach significantly outperforms recent baselines in both subjective and objective quality metrics, while offering smoother transitions between styles, reduced artifacts, and more precise text-based style control. </p>
<blockquote>
<p>æ–‡æœ¬é©±åŠ¨çš„å£°éŸ³è½¬æ¢å…è®¸ä½¿ç”¨æ–‡æœ¬æè¿°æ¥å®šåˆ¶è¯´è¯äººçš„ç‰¹æ€§å’ŒéŸµå¾‹å…ƒç´ ã€‚ç„¶è€Œï¼Œå¤§å¤šæ•°ç°æœ‰æ–¹æ³•ä¸¥é‡ä¾èµ–äºç›´æ¥çš„æ–‡æœ¬åˆ°è¯­éŸ³è®­ç»ƒï¼Œè¿™åœ¨æ§åˆ¶å¾®å¦™çš„é£æ ¼å…ƒç´ æˆ–éŸ³è‰²ç‰¹å¾æ–¹é¢çš„çµæ´»æ€§æœ‰é™ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°å‹çš„<strong>æ½œåœ¨çŠ¶æ€ç©ºé—´</strong>æ–¹æ³•ï¼Œç”¨äºæ–‡æœ¬é©±åŠ¨çš„å£°éŸ³è½¬æ¢ï¼ˆ<strong>LSS-VC</strong>ï¼‰ã€‚æˆ‘ä»¬çš„æ–¹æ³•å°†æ¯ä¸ªè¯­å¥è§†ä¸ºä¸€ä¸ªä¸æ–­å˜åŒ–çš„æ½œåœ¨ç©ºé—´ä¸­çš„åŠ¨æ€ç³»ç»Ÿã€‚æˆ‘ä»¬ä»å¼•å…¥äº†çŠ¶æ€ç©ºé—´æ¨¡å‹çš„mambaä¸­æ±²å–çµæ„Ÿï¼Œç”¨äºé«˜æ•ˆçš„æ–‡æœ¬é©±åŠ¨å›¾åƒé£æ ¼è½¬æ¢ï¼Œæˆ‘ä»¬é‡‡ç”¨äº†ä¸€ç§æ¾æ•£ç›¸å…³çš„æ–¹æ³•æ¥è¿›è¡Œå£°éŸ³é£æ ¼è½¬æ¢ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬å­¦ä¹ ä¸€ä¸ªå¯ä»¥ç‹¬ç«‹æ“çºµé£æ ¼å’Œå†…å®¹çš„è¯­éŸ³æ½œåœ¨æµå½¢ï¼Œé€šè¿‡æ–‡æœ¬é£æ ¼æç¤ºæ¥å®ç°ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§è‡ªé€‚åº”çš„è·¨æ¨¡æ€èåˆæœºåˆ¶ï¼Œå°†é£æ ¼ä¿¡æ¯æ³¨å…¥è¯­éŸ³æ½œåœ¨è¡¨ç¤ºä¸­ï¼Œå®ç°å¯¹è¯´è¯äººèº«ä»½ã€è¯­é€Ÿå’Œé‡ç‚¹çš„å¯è§£é‡Šå’Œç²¾ç»†æ§åˆ¶ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨ä¸»è§‚å’Œå®¢è§‚è´¨é‡æŒ‡æ ‡ä¸Šéƒ½æ˜¾è‘—ä¼˜äºæœ€è¿‘çš„åŸºçº¿æ–¹æ³•ï¼ŒåŒæ—¶æä¾›äº†æ›´å¹³æ»‘çš„é£æ ¼è¿‡æ¸¡ã€å‡å°‘äº†ä¼ªå½±ï¼Œå¹¶æä¾›äº†æ›´ç²¾ç¡®çš„æ–‡æœ¬åŸºäºé£æ ¼æ§åˆ¶ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.20999v1">PDF</a> </p>
<p><strong>Summary</strong><br>æ–‡æœ¬é©±åŠ¨çš„å£°éŸ³è½¬æ¢æ–¹æ³•é€šè¿‡ä½¿ç”¨æ–‡æœ¬æè¿°å®šåˆ¶è¯´è¯äººçš„ç‰¹æ€§å’ŒéŸµå¾‹å…ƒç´ ã€‚ç„¶è€Œï¼Œå¤§å¤šæ•°ç°æœ‰æ–¹æ³•è¿‡äºä¾èµ–ç›´æ¥çš„æ–‡æœ¬åˆ°è¯­éŸ³è®­ç»ƒï¼Œé™åˆ¶äº†å®ƒä»¬åœ¨æ§åˆ¶å¾®å¦™çš„é£æ ¼å…ƒç´ æˆ–éŸ³è‰²ç‰¹å¾æ–¹é¢çš„çµæ´»æ€§ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°é¢–çš„æ½œåœ¨çŠ¶æ€ç©ºé—´æ–¹æ³•ç”¨äºæ–‡æœ¬é©±åŠ¨çš„å£°éŸ³è½¬æ¢ï¼ˆLSS-VCï¼‰ã€‚æˆ‘ä»¬çš„æ–¹æ³•å°†æ¯ä¸ªè¯è¯­è§†ä¸ºä¸€ä¸ªåœ¨ä¸æ–­å˜åŒ–çš„æ½œåœ¨ç©ºé—´ä¸­çš„åŠ¨æ€ç³»ç»Ÿã€‚æˆ‘ä»¬ä»ä¸ºé«˜æ•ˆçš„æ–‡æœ¬é©±åŠ¨å›¾åƒé£æ ¼è½¬ç§»å¼•å…¥çŠ¶æ€ç©ºé—´æ¨¡å‹çš„mambaä¸­æ±²å–çµæ„Ÿï¼Œå¹¶é€‚åº”äº†ä¸€ç§ä¸å£°éŸ³é£æ ¼è½¬æ¢æ¾æ•£ç›¸å…³çš„æ–¹æ³•è®ºã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬å­¦ä¹ äº†ä¸€ä¸ªå¯ä»¥ç‹¬ç«‹æ“ä½œé£æ ¼å’Œå†…å®¹çš„è¯­éŸ³æ½œåœ¨æµå½¢ï¼Œé€šè¿‡æ–‡æœ¬é£æ ¼æç¤ºæ¥æ“çºµå®ƒä»¬ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§è‡ªé€‚åº”çš„è·¨æ¨¡æ€èåˆæœºåˆ¶ï¼Œå°†é£æ ¼ä¿¡æ¯æ³¨å…¥è¯­éŸ³æ½œåœ¨è¡¨ç¤ºä¸­ï¼Œå®ç°å¯¹è¯´è¯äººèº«ä»½ã€è¯­é€Ÿå’Œå¼ºè°ƒçš„ç›´è§‚å’Œç²¾ç»†æ§åˆ¶ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨ä¸»è§‚å’Œå®¢è§‚è´¨é‡æŒ‡æ ‡ä¸Šéƒ½æ˜¾è‘—ä¼˜äºæœ€æ–°åŸºçº¿ï¼ŒåŒæ—¶æä¾›äº†æ›´å¹³æ»‘çš„é£æ ¼è¿‡æ¸¡ã€å‡å°‘äº†ä¼ªè¿¹ï¼Œå¹¶ä¸”åŸºäºæ–‡æœ¬çš„æ ·å¼æ§åˆ¶æ›´ä¸ºç²¾ç¡®ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ–‡æœ¬é©±åŠ¨çš„å£°éŸ³è½¬æ¢èƒ½å¤Ÿåˆ©ç”¨æ–‡æœ¬æè¿°å®šåˆ¶è¯´è¯äººçš„ç‰¹æ€§å’ŒéŸµå¾‹å…ƒç´ ã€‚</li>
<li>ç°æœ‰æ–¹æ³•è¿‡äºä¾èµ–ç›´æ¥çš„æ–‡æœ¬åˆ°è¯­éŸ³è®­ç»ƒï¼Œç¼ºä¹çµæ´»æ€§ã€‚</li>
<li>LSS-VCæ–¹æ³•å°†æ¯ä¸ªè¯è¯­è§†ä¸ºä¸€ä¸ªåœ¨æ½œåœ¨ç©ºé—´ä¸­çš„åŠ¨æ€ç³»ç»Ÿã€‚</li>
<li>ä»mambaä¸­æ±²å–çµæ„Ÿï¼Œå¼•å…¥äº†çŠ¶æ€ç©ºé—´æ¨¡å‹ç”¨äºå£°éŸ³è½¬æ¢ã€‚</li>
<li>é€šè¿‡å­¦ä¹ è¯­éŸ³æ½œåœ¨æµå½¢ï¼Œå®ç°é£æ ¼å’Œå†…å®¹çš„ç‹¬ç«‹æ“ä½œã€‚</li>
<li>æå‡ºçš„è‡ªé€‚åº”è·¨æ¨¡æ€èåˆæœºåˆ¶èƒ½å¤Ÿæ³¨å…¥é£æ ¼ä¿¡æ¯åˆ°è¯­éŸ³è¡¨ç¤ºä¸­ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.20999">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-2cea6677e118f6282f5504c4c696fa03.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-4dd01a165128fd70fd5cd0ce86dd5f14.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c7e8f263843248ad96418f6eb6716f9e.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Video-T1-Test-Time-Scaling-for-Video-Generation"><a href="#Video-T1-Test-Time-Scaling-for-Video-Generation" class="headerlink" title="Video-T1: Test-Time Scaling for Video Generation"></a>Video-T1: Test-Time Scaling for Video Generation</h2><p><strong>Authors:Fangfu Liu, Hanyang Wang, Yimo Cai, Kaiyan Zhang, Xiaohang Zhan, Yueqi Duan</strong></p>
<p>With the scale capability of increasing training data, model size, and computational cost, video generation has achieved impressive results in digital creation, enabling users to express creativity across various domains. Recently, researchers in Large Language Models (LLMs) have expanded the scaling to test-time, which can significantly improve LLM performance by using more inference-time computation. Instead of scaling up video foundation models through expensive training costs, we explore the power of Test-Time Scaling (TTS) in video generation, aiming to answer the question: if a video generation model is allowed to use non-trivial amount of inference-time compute, how much can it improve generation quality given a challenging text prompt. In this work, we reinterpret the test-time scaling of video generation as a searching problem to sample better trajectories from Gaussian noise space to the target video distribution. Specifically, we build the search space with test-time verifiers to provide feedback and heuristic algorithms to guide searching process. Given a text prompt, we first explore an intuitive linear search strategy by increasing noise candidates at inference time. As full-step denoising all frames simultaneously requires heavy test-time computation costs, we further design a more efficient TTS method for video generation called Tree-of-Frames (ToF) that adaptively expands and prunes video branches in an autoregressive manner. Extensive experiments on text-conditioned video generation benchmarks demonstrate that increasing test-time compute consistently leads to significant improvements in the quality of videos. Project page: <a target="_blank" rel="noopener" href="https://liuff19.github.io/Video-T1">https://liuff19.github.io/Video-T1</a> </p>
<blockquote>
<p>éšç€è®­ç»ƒæ•°æ®ã€æ¨¡å‹è§„æ¨¡å’Œè®¡ç®—æˆæœ¬çš„è§„æ¨¡èƒ½åŠ›ä¸æ–­æå‡ï¼Œè§†é¢‘ç”Ÿæˆåœ¨æ•°å­—åˆ›ä½œä¸­å–å¾—äº†ä»¤äººå°è±¡æ·±åˆ»çš„ç»“æœï¼Œä½¿ç”¨æˆ·èƒ½å¤Ÿåœ¨å„ç§é¢†åŸŸè¡¨è¾¾åˆ›é€ åŠ›ã€‚æœ€è¿‘ï¼Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„ç ”ç©¶äººå‘˜å°†è§„æ¨¡æ‰©å±•åˆ°äº†æµ‹è¯•é˜¶æ®µï¼Œé€šè¿‡å¢åŠ æ¨ç†æ—¶é—´è®¡ç®—æ¥æ˜¾è‘—æé«˜LLMçš„æ€§èƒ½ã€‚æˆ‘ä»¬å¹¶ä¸é€šè¿‡æ˜‚è´µçš„è®­ç»ƒæˆæœ¬æ¥æ‰©å¤§è§†é¢‘åŸºç¡€æ¨¡å‹çš„è§„æ¨¡ï¼Œè€Œæ˜¯æ¢ç´¢è§†é¢‘ç”Ÿæˆä¸­æµ‹è¯•æ—¶é—´ç¼©æ”¾ï¼ˆTTSï¼‰çš„å¨åŠ›ï¼Œæ—¨åœ¨å›ç­”ä»¥ä¸‹é—®é¢˜ï¼šå¦‚æœè§†é¢‘ç”Ÿæˆæ¨¡å‹è¢«å…è®¸ä½¿ç”¨éå¾®ä¸è¶³é“çš„æ¨ç†æ—¶é—´è®¡ç®—é‡ï¼Œé‚£ä¹ˆåœ¨å…·æœ‰æŒ‘æˆ˜æ€§çš„æ–‡æœ¬æç¤ºä¸‹ï¼Œå®ƒèƒ½åœ¨å¤šå¤§ç¨‹åº¦ä¸Šæé«˜ç”Ÿæˆè§†é¢‘çš„è´¨é‡ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬å°†è§†é¢‘ç”Ÿæˆçš„æµ‹è¯•æ—¶é—´ç¼©æ”¾é‡æ–°è§£é‡Šä¸ºæœç´¢é—®é¢˜ï¼Œä»é«˜æ–¯å™ªå£°ç©ºé—´é‡‡æ ·æ›´å¥½çš„è½¨è¿¹ä»¥æ¥è¿‘ç›®æ ‡è§†é¢‘åˆ†å¸ƒã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬æ„å»ºäº†ä¸€ä¸ªæœç´¢ç©ºé—´ï¼Œåˆ©ç”¨æµ‹è¯•æ—¶é—´éªŒè¯å™¨æä¾›åé¦ˆå’Œå¯å‘å¼ç®—æ³•æ¥æŒ‡å¯¼æœç´¢è¿‡ç¨‹ã€‚ç»™å®šä¸€ä¸ªæ–‡æœ¬æç¤ºï¼Œæˆ‘ä»¬é¦–å…ˆæ¢ç´¢ä¸€ç§ç›´è§‚çš„çº¿æ€§æœç´¢ç­–ç•¥ï¼Œé€šè¿‡åœ¨æ¨ç†æ—¶é—´å¢åŠ å™ªå£°å€™é€‰è€…æ¥è¿›è¡Œã€‚ç”±äºåŒæ—¶å®Œå…¨å»å™ªæ‰€æœ‰å¸§éœ€è¦å¤§é‡çš„æµ‹è¯•æ—¶é—´è®¡ç®—æˆæœ¬ï¼Œæˆ‘ä»¬è¿›ä¸€æ­¥ä¸ºè§†é¢‘ç”Ÿæˆè®¾è®¡äº†ä¸€ç§æ›´æœ‰æ•ˆçš„TTSæ–¹æ³•ï¼Œç§°ä¸ºâ€œå¸§æ ‘â€ï¼ˆToFï¼‰ï¼Œè¯¥æ–¹æ³•ä»¥è‡ªå›å½’çš„æ–¹å¼è‡ªé€‚åº”åœ°æ‰©å±•å’Œä¿®å‰ªè§†é¢‘åˆ†æ”¯ã€‚åœ¨æ–‡æœ¬æ¡ä»¶è§†é¢‘ç”ŸæˆåŸºå‡†æµ‹è¯•çš„å¤§é‡å®éªŒè¡¨æ˜ï¼Œå¢åŠ æµ‹è¯•æ—¶é—´è®¡ç®—é‡å§‹ç»ˆæœ‰åŠ©äºæé«˜è§†é¢‘è´¨é‡ã€‚é¡¹ç›®é¡µé¢ï¼š<a target="_blank" rel="noopener" href="https://liuff19.github.io/Video-T1">https://liuff19.github.io/Video-T1</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.18942v2">PDF</a> Project page: <a target="_blank" rel="noopener" href="https://liuff19.github.io/Video-T1">https://liuff19.github.io/Video-T1</a></p>
<p><strong>Summary</strong><br>     è§†é¢‘ç”Ÿæˆé¢†åŸŸé€šè¿‡æ‰©å¤§è®­ç»ƒæ•°æ®ã€æ¨¡å‹è§„æ¨¡å’Œè®¡ç®—æˆæœ¬ï¼Œå®ç°äº†ä»¤äººå°è±¡æ·±åˆ»çš„æˆæœã€‚è¿‘æœŸï¼Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„ç ”ç©¶è€…å°†è§„æ¨¡æ‰©å±•è‡³æµ‹è¯•é˜¶æ®µï¼Œé€šè¿‡ä½¿ç”¨æ›´å¤šçš„æ¨ç†æ—¶é—´è®¡ç®—æ¥æ˜¾è‘—æé«˜LLMæ€§èƒ½ã€‚æœ¬ç ”ç©¶æ¢ç´¢äº†è§†é¢‘ç”Ÿæˆä¸­çš„æµ‹è¯•æ—¶é—´ç¼©æ”¾ï¼ˆTTSï¼‰èƒ½åŠ›ï¼Œå°†è§†é¢‘ç”Ÿæˆçš„æµ‹è¯•æ—¶é—´ç¼©æ”¾é‡æ–°è§£é‡Šä¸ºæœç´¢é—®é¢˜ï¼Œä»é«˜æ–¯å™ªå£°ç©ºé—´é‡‡æ ·æ›´å¥½çš„è½¨è¿¹æ¥é€¼è¿‘ç›®æ ‡è§†é¢‘åˆ†å¸ƒã€‚æœ¬ç ”ç©¶è®¾è®¡äº†é«˜æ•ˆçš„TTSæ–¹æ³•â€”â€”Tree-of-Framesï¼ˆToFï¼‰ï¼Œåœ¨ç»™å®šæ–‡æœ¬æç¤ºçš„æƒ…å†µä¸‹ï¼Œä»¥è‡ªé€‚åº”çš„æ–¹å¼æ‰©å±•å’Œä¿®å‰ªè§†é¢‘åˆ†æ”¯ã€‚å®éªŒè¯æ˜ï¼Œå¢åŠ æµ‹è¯•æ—¶é—´çš„è®¡ç®—é‡å¯ä»¥æ˜¾è‘—æé«˜è§†é¢‘è´¨é‡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è§†é¢‘ç”Ÿæˆé€šè¿‡æ‰©å¤§è®­ç»ƒæ•°æ®ã€æ¨¡å‹è§„æ¨¡å’Œè®¡ç®—æˆæœ¬å–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œä½¿åˆ›æ„è¡¨è¾¾è·¨è¶Šå¤šä¸ªé¢†åŸŸæˆä¸ºå¯èƒ½ã€‚</li>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æµ‹è¯•æ—¶é—´ç¼©æ”¾ï¼ˆTTSï¼‰å¯ä»¥æé«˜æ¨¡å‹æ€§èƒ½ã€‚</li>
<li>ç ”ç©¶è€…å°†è§†é¢‘ç”Ÿæˆçš„æµ‹è¯•æ—¶é—´ç¼©æ”¾è§£é‡Šä¸ºæœç´¢é—®é¢˜ï¼Œä»é«˜æ–¯å™ªå£°ç©ºé—´é‡‡æ ·è½¨è¿¹ä»¥é€¼è¿‘ç›®æ ‡è§†é¢‘åˆ†å¸ƒã€‚</li>
<li>é€šè¿‡æ„å»ºæµ‹è¯•æ—¶é—´éªŒè¯å™¨å’Œå¯å‘å¼ç®—æ³•æ¥æŒ‡å¯¼æœç´¢è¿‡ç¨‹ã€‚</li>
<li>ç»™å®šæ–‡æœ¬æç¤ºï¼Œæœ¬ç ”ç©¶é‡‡ç”¨äº†ä¸€ç§ç›´è§‚çš„çº¿æ€§æœç´¢ç­–ç•¥ï¼Œé€šè¿‡å¢åŠ æ¨ç†æ—¶é—´çš„å™ªå£°å€™é€‰æ¥æé«˜è§†é¢‘è´¨é‡ã€‚</li>
<li>ç”±äºåŒæ—¶å»å™ªæ‰€æœ‰å¸§éœ€è¦å·¨å¤§çš„è®¡ç®—é‡ï¼Œç ”ç©¶è€…è¿›ä¸€æ­¥è®¾è®¡äº†ä¸€ç§æ›´æœ‰æ•ˆçš„è§†é¢‘ç”ŸæˆTTSæ–¹æ³•â€”â€”Tree-of-Framesï¼ˆToFï¼‰ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.18942">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-4e41aa767734ab722920bc9dfcd1461d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-28be37f548465dcd999e72bc35a1c779.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2a96efe176a78a126c8be4a2df8ee355.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ac5a102c6d5c62082016c1b081527d95.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="MegaTTS-3-Sparse-Alignment-Enhanced-Latent-Diffusion-Transformer-for-Zero-Shot-Speech-Synthesis"><a href="#MegaTTS-3-Sparse-Alignment-Enhanced-Latent-Diffusion-Transformer-for-Zero-Shot-Speech-Synthesis" class="headerlink" title="MegaTTS 3: Sparse Alignment Enhanced Latent Diffusion Transformer for   Zero-Shot Speech Synthesis"></a>MegaTTS 3: Sparse Alignment Enhanced Latent Diffusion Transformer for   Zero-Shot Speech Synthesis</h2><p><strong>Authors:Ziyue Jiang, Yi Ren, Ruiqi Li, Shengpeng Ji, Boyang Zhang, Zhenhui Ye, Chen Zhang, Bai Jionghao, Xiaoda Yang, Jialong Zuo, Yu Zhang, Rui Liu, Xiang Yin, Zhou Zhao</strong></p>
<p>While recent zero-shot text-to-speech (TTS) models have significantly improved speech quality and expressiveness, mainstream systems still suffer from issues related to speech-text alignment modeling: 1) models without explicit speech-text alignment modeling exhibit less robustness, especially for hard sentences in practical applications; 2) predefined alignment-based models suffer from naturalness constraints of forced alignments. This paper introduces \textit{MegaTTS 3}, a TTS system featuring an innovative sparse alignment algorithm that guides the latent diffusion transformer (DiT). Specifically, we provide sparse alignment boundaries to MegaTTS 3 to reduce the difficulty of alignment without limiting the search space, thereby achieving high naturalness. Moreover, we employ a multi-condition classifier-free guidance strategy for accent intensity adjustment and adopt the piecewise rectified flow technique to accelerate the generation process. Experiments demonstrate that MegaTTS 3 achieves state-of-the-art zero-shot TTS speech quality and supports highly flexible control over accent intensity. Notably, our system can generate high-quality one-minute speech with only 8 sampling steps. Audio samples are available at <a target="_blank" rel="noopener" href="https://sditdemo.github.io/sditdemo/">https://sditdemo.github.io/sditdemo/</a>. </p>
<blockquote>
<p>è¿‘å¹´æ¥ï¼Œé›¶æ ·æœ¬æ–‡æœ¬åˆ°è¯­éŸ³ï¼ˆTTSï¼‰æ¨¡å‹åœ¨è¯­éŸ³è´¨é‡å’Œè¡¨ç°åŠ›æ–¹é¢å–å¾—äº†æ˜¾è‘—æ”¹è¿›ï¼Œä½†ä¸»æµç³»ç»Ÿä»ç„¶é¢ä¸´ä¸è¯­éŸ³æ–‡æœ¬å¯¹é½å»ºæ¨¡ç›¸å…³çš„é—®é¢˜ï¼š1ï¼‰æ²¡æœ‰æ˜ç¡®çš„è¯­éŸ³æ–‡æœ¬å¯¹é½å»ºæ¨¡çš„æ¨¡å‹è¡¨ç°å‡ºè¾ƒä½çš„ç¨³å¥æ€§ï¼Œç‰¹åˆ«æ˜¯åœ¨å®é™…åº”ç”¨ä¸­çš„éš¾å¥ï¼›2ï¼‰åŸºäºé¢„å®šä¹‰å¯¹é½çš„æ¨¡å‹å—åˆ°å¼ºåˆ¶å¯¹é½çš„è‡ªç„¶æ€§çº¦æŸã€‚æœ¬æ–‡ä»‹ç»äº†<em>MegaTTS 3</em>ï¼Œè¿™æ˜¯ä¸€ä¸ªTTSç³»ç»Ÿï¼Œé‡‡ç”¨åˆ›æ–°ç¨€ç–å¯¹é½ç®—æ³•ï¼Œå¼•å¯¼æ½œåœ¨æ‰©æ•£å˜å‹å™¨ï¼ˆDiTï¼‰ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬ä¸ºMegaTTS 3æä¾›ç¨€ç–å¯¹é½è¾¹ç•Œï¼Œä»¥å‡å°‘å¯¹é½éš¾åº¦è€Œä¸é™åˆ¶æœç´¢ç©ºé—´ï¼Œä»è€Œå®ç°é«˜è‡ªç„¶åº¦ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬é‡‡ç”¨å¤šæ¡ä»¶æ— åˆ†ç±»æŒ‡å¯¼ç­–ç•¥è¿›è¡Œå£éŸ³å¼ºåº¦è°ƒæ•´ï¼Œå¹¶é‡‡ç”¨åˆ†æ®µæ•´æµæµæŠ€æœ¯åŠ å¿«ç”Ÿæˆè¿‡ç¨‹ã€‚å®éªŒè¡¨æ˜ï¼ŒMegaTTS 3è¾¾åˆ°äº†æœ€å…ˆè¿›çš„é›¶æ ·æœ¬TTSè¯­éŸ³è´¨é‡ï¼Œå¹¶æ”¯æŒå¯¹å£éŸ³å¼ºåº¦è¿›è¡Œé«˜åº¦çµæ´»çš„æ§åˆ¶ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œæˆ‘ä»¬çš„ç³»ç»Ÿåªéœ€8ä¸ªé‡‡æ ·æ­¥éª¤å°±èƒ½ç”Ÿæˆé«˜è´¨é‡çš„ä¸€åˆ†é’Ÿè¯­éŸ³ã€‚éŸ³é¢‘æ ·æœ¬å¯åœ¨<a target="_blank" rel="noopener" href="https://sditdemo.github.io/sditdemo/%E6%89%BE%E5%88%B0%E3%80%82">https://sditdemo.github.io/sditdemo/æ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.18924v4">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†æœ€æ–°æ¨å‡ºçš„MegaTTS 3æ–‡æœ¬è½¬è¯­éŸ³ï¼ˆTTSï¼‰ç³»ç»Ÿï¼Œå…¶ç‰¹ç‚¹åœ¨äºé‡‡ç”¨åˆ›æ–°ç¨€ç–å¯¹é½ç®—æ³•å¼•å¯¼æ½œåœ¨æ‰©æ•£è½¬æ¢å™¨ï¼ˆDiTï¼‰ã€‚è¯¥ç³»ç»Ÿé€šè¿‡æä¾›ç¨€ç–å¯¹é½è¾¹ç•Œï¼Œé™ä½å¯¹é½éš¾åº¦ä¸”ä¸é™åˆ¶æœç´¢ç©ºé—´ï¼Œä»è€Œå®ç°é«˜è‡ªç„¶åº¦ã€‚æ­¤å¤–ï¼Œå®ƒé‡‡ç”¨å¤šæ¡ä»¶æ— åˆ†ç±»å¼•å¯¼ç­–ç•¥è°ƒæ•´å£éŸ³å¼ºåº¦ï¼Œå¹¶ä½¿ç”¨åˆ†æ®µæ•´æµæŠ€æœ¯åŠ é€Ÿç”Ÿæˆè¿‡ç¨‹ã€‚å®éªŒè¯æ˜ï¼ŒMegaTTS 3å®ç°äº†æœ€å…ˆè¿›çš„é›¶æ ·æœ¬TTSè¯­éŸ³è´¨é‡ï¼Œå¹¶æ”¯æŒé«˜åº¦çµæ´»çš„å£éŸ³å¼ºåº¦æ§åˆ¶ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>MegaTTS 3æ˜¯ä¸€ä¸ªæ–°çš„æ–‡æœ¬è½¬è¯­éŸ³ï¼ˆTTSï¼‰ç³»ç»Ÿï¼Œå…·æœ‰å…ˆè¿›çš„ç¨€ç–å¯¹é½ç®—æ³•ã€‚</li>
<li>è¯¥ç³»ç»Ÿé€šè¿‡æä¾›ç¨€ç–å¯¹é½è¾¹ç•Œï¼Œé™ä½å¯¹é½éš¾åº¦ï¼ŒåŒæ—¶ä¿æŒæœç´¢ç©ºé—´çš„é«˜çµæ´»æ€§ã€‚</li>
<li>MegaTTS 3é‡‡ç”¨å¤šæ¡ä»¶æ— åˆ†ç±»å¼•å¯¼ç­–ç•¥ï¼Œç”¨äºè°ƒæ•´å£éŸ³å¼ºåº¦ã€‚</li>
<li>ç³»ç»Ÿé‡‡ç”¨åˆ†æ®µæ•´æµæŠ€æœ¯æ¥åŠ é€Ÿè¯­éŸ³ç”Ÿæˆè¿‡ç¨‹ã€‚</li>
<li>å®éªŒè¯æ˜MegaTTS 3åœ¨é›¶æ ·æœ¬TTSè¯­éŸ³è´¨é‡æ–¹é¢è¾¾åˆ°æœ€æ–°æ°´å¹³ã€‚</li>
<li>ç³»ç»Ÿæ”¯æŒé«˜åº¦çµæ´»çš„å£éŸ³å¼ºåº¦æ§åˆ¶ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.18924">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-6a4848ec4bc65716ed58ab74f09839dd.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f63c305d51b567c1b33e75f11ea31ced.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-799c5f97dff2642f57db63aacabdbaa7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d4dc21c79b3bf4b146770c255229fcba.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="VERSA-A-Versatile-Evaluation-Toolkit-for-Speech-Audio-and-Music"><a href="#VERSA-A-Versatile-Evaluation-Toolkit-for-Speech-Audio-and-Music" class="headerlink" title="VERSA: A Versatile Evaluation Toolkit for Speech, Audio, and Music"></a>VERSA: A Versatile Evaluation Toolkit for Speech, Audio, and Music</h2><p><strong>Authors:Jiatong Shi, Hye-jin Shim, Jinchuan Tian, Siddhant Arora, Haibin Wu, Darius Petermann, Jia Qi Yip, You Zhang, Yuxun Tang, Wangyou Zhang, Dareen Safar Alharthi, Yichen Huang, Koichi Saito, Jionghao Han, Yiwen Zhao, Chris Donahue, Shinji Watanabe</strong></p>
<p>In this work, we introduce VERSA, a unified and standardized evaluation toolkit designed for various speech, audio, and music signals. The toolkit features a Pythonic interface with flexible configuration and dependency control, making it user-friendly and efficient. With full installation, VERSA offers 65 metrics with 729 metric variations based on different configurations. These metrics encompass evaluations utilizing diverse external resources, including matching and non-matching reference audio, text transcriptions, and text captions. As a lightweight yet comprehensive toolkit, VERSA is versatile to support the evaluation of a wide range of downstream scenarios. To demonstrate its capabilities, this work highlights example use cases for VERSA, including audio coding, speech synthesis, speech enhancement, singing synthesis, and music generation. The toolkit is available at <a target="_blank" rel="noopener" href="https://github.com/wavlab-speech/versa">https://github.com/wavlab-speech/versa</a>. </p>
<blockquote>
<p>åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†VERSAï¼Œè¿™æ˜¯ä¸€ä¸ªä¸ºè¯­éŸ³ã€éŸ³é¢‘å’ŒéŸ³ä¹ä¿¡å·è®¾è®¡çš„ç»Ÿä¸€ã€æ ‡å‡†åŒ–çš„è¯„ä¼°å·¥å…·åŒ…ã€‚è¯¥å·¥å…·åŒ…å…·æœ‰Pythoné£æ ¼çš„æ¥å£ï¼Œå…·æœ‰çµæ´»çš„é…ç½®å’Œä¾èµ–æ§åˆ¶ï¼Œä½¿å…¶å‹å¥½ä¸”é«˜æ•ˆã€‚å®Œæˆå®‰è£…åï¼ŒVERSAæä¾›äº†åŸºäºä¸åŒé…ç½®çš„65ç§æŒ‡æ ‡å’Œ729ç§æŒ‡æ ‡å˜ä½“ã€‚è¿™äº›æŒ‡æ ‡åŒ…æ‹¬åˆ©ç”¨å¤šç§å¤–éƒ¨èµ„æºçš„è¯„ä¼°ï¼ŒåŒ…æ‹¬åŒ¹é…å’ŒéåŒ¹é…çš„å‚è€ƒéŸ³é¢‘ã€æ–‡æœ¬è½¬å½•å’Œæ–‡æœ¬æ ‡é¢˜ã€‚ä½œä¸ºä¸€ä¸ªè½»ä¾¿è€Œå…¨é¢çš„å·¥å…·åŒ…ï¼ŒVERSAæ”¯æŒå¯¹å„ç§ä¸‹æ¸¸åœºæ™¯çš„è¯„ä¼°ã€‚ä¸ºäº†è¯æ˜å…¶èƒ½åŠ›ï¼Œè¿™é¡¹å·¥ä½œé‡ç‚¹ä»‹ç»äº†VERSAçš„ç¤ºä¾‹ç”¨ä¾‹ï¼ŒåŒ…æ‹¬éŸ³é¢‘ç¼–ç ã€è¯­éŸ³åˆæˆã€è¯­éŸ³å¢å¼ºã€æ­Œå”±åˆæˆå’ŒéŸ³ä¹ç”Ÿæˆã€‚è¯¥å·¥å…·åŒ…å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/wavlab-speech/versa%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/wavlab-speech/versaæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.17667v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>VERSAæ˜¯ä¸€ä¸ªç»Ÿä¸€ã€æ ‡å‡†åŒ–çš„è¯­éŸ³ã€éŸ³é¢‘å’ŒéŸ³ä¹ä¿¡å·è¯„ä¼°å·¥å…·åŒ…ï¼Œå…·æœ‰Pythonicæ¥å£ã€çµæ´»çš„é…ç½®å’Œä¾èµ–æ§åˆ¶ï¼Œæä¾›å¤šç§è¯„ä¼°æŒ‡æ ‡ï¼Œæ”¯æŒå¤šç§ä¸‹æ¸¸åœºæ™¯çš„è¯„ä»·ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>VERSAæ˜¯ä¸€ä¸ªé’ˆå¯¹è¯­éŸ³ã€éŸ³é¢‘å’ŒéŸ³ä¹ä¿¡å·è¯„ä¼°çš„å·¥å…·åŒ…ã€‚</li>
<li>å®ƒæä¾›äº†ä¸€ä¸ªPythonicæ¥å£ï¼Œå…·æœ‰çµæ´»çš„é…ç½®å’Œä¾èµ–æ§åˆ¶ã€‚</li>
<li>VERSAåŒ…å«65ç§è¯„ä¼°æŒ‡æ ‡ï¼Œä»¥åŠåŸºäºä¸åŒé…ç½®çš„729ç§æŒ‡æ ‡å˜åŒ–ã€‚</li>
<li>è¯¥å·¥å…·åŒ…å¯ä»¥åˆ©ç”¨å„ç§å¤–éƒ¨èµ„æºè¿›è¡Œè¯„ä¼°ï¼ŒåŒ…æ‹¬åŒ¹é…å’ŒéåŒ¹é…çš„å‚è€ƒéŸ³é¢‘ã€æ–‡æœ¬è½¬å½•å’Œæ–‡æœ¬æ ‡é¢˜ã€‚</li>
<li>VERSAæ”¯æŒå¤šç§ä¸‹æ¸¸åœºæ™¯çš„è¯„ä»·ï¼Œå¦‚éŸ³é¢‘ç¼–ç ã€è¯­éŸ³åˆæˆã€è¯­éŸ³å¢å¼ºã€æ­Œå”±åˆæˆå’ŒéŸ³ä¹ç”Ÿæˆç­‰ã€‚</li>
<li>VERSAåœ¨GitHubä¸Šå¯ç”¨ï¼Œç”¨æˆ·å¯ä»¥é€šè¿‡é“¾æ¥è·å–æ›´å¤šè¯¦ç»†ä¿¡æ¯å’Œç¤ºä¾‹ç”¨ä¾‹ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.17667">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-49dd14a08aac38b6c02b55a3089ebb77.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ef56415eb65cfb8300e0a1913078f61c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7f8e0e799de48fe8975d415cbff636bd.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-11c60c703837d6d4aba282bfeb1cd3b0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6be0d11933d9ac99b6081aaa2e0804c5.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="Towards-Controllable-Speech-Synthesis-in-the-Era-of-Large-Language-Models-A-Survey"><a href="#Towards-Controllable-Speech-Synthesis-in-the-Era-of-Large-Language-Models-A-Survey" class="headerlink" title="Towards Controllable Speech Synthesis in the Era of Large Language   Models: A Survey"></a>Towards Controllable Speech Synthesis in the Era of Large Language   Models: A Survey</h2><p><strong>Authors:Tianxin Xie, Yan Rong, Pengfei Zhang, Wenwu Wang, Li Liu</strong></p>
<p>Text-to-speech (TTS), also known as speech synthesis, is a prominent research area that aims to generate natural-sounding human speech from text. Recently, with the increasing industrial demand, TTS technologies have evolved beyond synthesizing human-like speech to enabling controllable speech generation. This includes fine-grained control over various attributes of synthesized speech such as emotion, prosody, timbre, and duration. In addition, advancements in deep learning, such as diffusion and large language models, have significantly enhanced controllable TTS over the past several years. In this work, we conduct a comprehensive survey of controllable TTS, covering approaches ranging from basic control techniques to methods utilizing natural language prompts, aiming to provide a clear understanding of the current state of research. We examine the general controllable TTS pipeline, challenges, model architectures, and control strategies, offering a comprehensive and clear taxonomy of existing methods. Additionally, we provide a detailed summary of datasets and evaluation metrics and shed some light on the applications and future directions of controllable TTS. To the best of our knowledge, this survey paper provides the first comprehensive review of emerging controllable TTS methods, which can serve as a beneficial resource for both academic researchers and industrial practitioners. </p>
<blockquote>
<p>æ–‡æœ¬è½¬è¯­éŸ³ï¼ˆTTSï¼‰ï¼Œä¹Ÿè¢«ç§°ä¸ºè¯­éŸ³åˆæˆï¼Œæ˜¯ä¸€ä¸ªçªå‡ºçš„ç ”ç©¶é¢†åŸŸï¼Œæ—¨åœ¨ä»æ–‡æœ¬ç”Ÿæˆå¬èµ·æ¥å¾ˆè‡ªç„¶çš„äººç±»è¯­éŸ³ã€‚æœ€è¿‘ï¼Œéšç€å·¥ä¸šéœ€æ±‚çš„å¢åŠ ï¼ŒTTSæŠ€æœ¯å·²ç»è¶…è¶Šäº†åˆæˆç±»ä¼¼äººç±»çš„è¯­éŸ³ï¼Œå‘å±•åˆ°äº†èƒ½å¤Ÿå®ç°å¯æ§çš„è¯­éŸ³ç”Ÿæˆã€‚è¿™åŒ…æ‹¬åˆæˆè¯­éŸ³çš„å„ç§å±æ€§çš„ç²¾ç»†æ§åˆ¶ï¼Œå¦‚æƒ…æ„Ÿã€è¯­è°ƒã€éŸ³è´¨å’ŒæŒç»­æ—¶é—´ã€‚æ­¤å¤–ï¼Œæ·±åº¦å­¦ä¹ ä¸­çš„æ‰©æ•£å’Œå¤§å‹è¯­è¨€æ¨¡å‹ç­‰æŠ€æœ¯çš„è¿›æ­¥ï¼Œåœ¨è¿‡å»å‡ å¹´ä¸­æ˜¾è‘—å¢å¼ºäº†å¯æ§TTSçš„æ€§èƒ½ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬å¯¹å¯æ§TTSè¿›è¡Œäº†å…¨é¢çš„è°ƒæŸ¥ï¼Œæ¶µç›–äº†ä»åŸºæœ¬æ§åˆ¶æŠ€æœ¯åˆ°åˆ©ç”¨è‡ªç„¶è¯­è¨€æç¤ºçš„æ–¹æ³•ç­‰å¤šç§æ–¹æ³•ï¼Œæ—¨åœ¨æä¾›å¯¹å½“å‰ç ”ç©¶çŠ¶æ€çš„æ¸…æ™°ç†è§£ã€‚æˆ‘ä»¬ç ”ç©¶äº†é€šç”¨çš„å¯æ§TTSç®¡é“ã€æŒ‘æˆ˜ã€æ¨¡å‹æ¶æ„å’Œæ§åˆ¶ç­–ç•¥ï¼Œæä¾›äº†ç°æœ‰æ–¹æ³•çš„å…¨é¢è€Œæ¸…æ™°çš„åˆ†ç±»ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜è¯¦ç»†æ€»ç»“äº†æ•°æ®é›†å’Œè¯„ä»·æŒ‡æ ‡ï¼Œå¹¶å¯¹å¯æ§TTSçš„åº”ç”¨å’Œæœªæ¥å‘å±•æ–¹å‘è¿›è¡Œäº†ä¸€äº›é˜è¿°ã€‚æ®æˆ‘ä»¬æ‰€çŸ¥ï¼Œè¿™ç¯‡ç»¼è¿°è®ºæ–‡é¦–æ¬¡å…¨é¢å›é¡¾äº†æ–°å…´çš„å¯æ§TTSæ–¹æ³•ï¼Œå¯¹å­¦æœ¯ç ”ç©¶äººå‘˜å’Œå·¥ä¸šä»ä¸šè€…éƒ½æœ‰å¾ˆå¤§çš„å‚è€ƒä»·å€¼ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.06602v2">PDF</a> A comprehensive survey on controllable TTS, 26 pages, 7 tables, 6   figures, 317 references. Under review</p>
<p><strong>Summary</strong><br>æ–‡æœ¬è½¬è¯­éŸ³ï¼ˆTTSï¼‰æ˜¯ç”Ÿæˆè‡ªç„¶è¯­éŸ³çš„å…³é”®ç ”ç©¶é¢†åŸŸã€‚è¿‘å¹´æ¥ï¼Œéšç€å·¥ä¸šéœ€æ±‚çš„å¢é•¿ï¼ŒTTSæŠ€æœ¯ä¸ä»…åˆæˆäººç±»è¯­éŸ³ï¼Œè¿˜èƒ½å®ç°å¯æ§çš„è¯­éŸ³ç”Ÿæˆã€‚æ­¤æŠ€æœ¯æ¶µç›–å¯¹åˆæˆè¯­éŸ³çš„å„ç§å±æ€§çš„ç²¾ç»†æ§åˆ¶ï¼Œå¦‚æƒ…æ„Ÿã€è¯­è°ƒã€éŸ³è´¨å’Œæ—¶é•¿ç­‰ã€‚æ·±åº¦å­¦ä¹ ä¸­çš„æ‰©æ•£æ¨¡å‹å’Œå¤§å‹è¯­è¨€æ¨¡å‹ç­‰è¿›æ­¥ä¸ºå¯æ§TTSå¸¦æ¥äº†æ˜¾è‘—çš„æå‡ã€‚æœ¬æ–‡å…¨é¢ç»¼è¿°äº†å¯æ§TTSçš„ç ”ç©¶ç°çŠ¶ï¼Œä»åŸºæœ¬æ§åˆ¶æŠ€å·§åˆ°åˆ©ç”¨è‡ªç„¶è¯­è¨€æç¤ºçš„æ–¹æ³•éƒ½æœ‰æ‰€æ¶‰åŠï¼Œæ—¨åœ¨ä¸ºå­¦æœ¯ç•Œå’Œå·¥ä¸šç•Œæä¾›æœ‰ç›Šçš„å‚è€ƒã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>TTSç ”ç©¶çš„ç›®çš„æ˜¯ä»æ–‡æœ¬ç”Ÿæˆè‡ªç„¶è¯­éŸ³ã€‚</li>
<li>è¿‘æœŸTTSæŠ€æœ¯å·²ä»åˆæˆäººç±»è¯­éŸ³æ‰©å±•åˆ°å¯æ§çš„è¯­éŸ³ç”Ÿæˆã€‚</li>
<li>å¯æ§TTSæ¶µç›–å¯¹åˆæˆè¯­éŸ³çš„å„ç§å±æ€§çš„ç²¾ç»†æ§åˆ¶ï¼Œå¦‚æƒ…æ„Ÿã€è¯­è°ƒã€éŸ³è´¨å’Œæ—¶é•¿ã€‚</li>
<li>æ·±åº¦å­¦ä¹ é¢†åŸŸçš„è¿›æ­¥ï¼Œå¦‚æ‰©æ•£æ¨¡å‹å’Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼Œæ˜¾è‘—æå‡äº†å¯æ§TTSçš„æ€§èƒ½ã€‚</li>
<li>æœ¬æ–‡å…¨é¢æ¦‚è¿°äº†å¯æ§TTSçš„ç ”ç©¶ç°çŠ¶ï¼ŒåŒ…æ‹¬é€šç”¨ç®¡é“ã€æŒ‘æˆ˜ã€æ¨¡å‹æ¶æ„å’Œæ§åˆ¶ç­–ç•¥ã€‚</li>
<li>æ–‡ç« è¿˜è¯¦ç»†ä»‹ç»äº†æ•°æ®é›†å’Œè¯„ä¼°æŒ‡æ ‡ã€‚</li>
<li>æ­¤ç»¼è¿°æ—¨åœ¨ä¸ºå­¦æœ¯ç•Œå’Œå·¥ä¸šç•Œæä¾›å…³äºå¯æ§TTSçš„å®è´µèµ„æºã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.06602">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-77b9d1b4adf952efd84a9a0f647cb16e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-6793a177689270ec51d736f2677c63a6.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-914ceb695b61c14f39efea85c78546a6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-53c531461a37a5313b4c529845b753e2.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="Continuous-Speech-Tokenizer-in-Text-To-Speech"><a href="#Continuous-Speech-Tokenizer-in-Text-To-Speech" class="headerlink" title="Continuous Speech Tokenizer in Text To Speech"></a>Continuous Speech Tokenizer in Text To Speech</h2><p><strong>Authors:Yixing Li, Ruobing Xie, Xingwu Sun, Yu Cheng, Zhanhui Kang</strong></p>
<p>The fusion of speech and language in the era of large language models has garnered significant attention. Discrete speech token is often utilized in text-to-speech tasks for speech compression and portability, which is convenient for joint training with text and have good compression efficiency. However, we found that the discrete speech tokenizer still suffers from information loss. Therefore, we propose a simple yet effective continuous speech tokenizer named Cont-SPT, and a text-to-speech model based on continuous speech tokens. Our results show that the speech language model based on the continuous speech tokenizer has better continuity and higher estimated Mean Opinion Scores (MoS). This enhancement is attributed to better information preservation rate of the continuous speech tokenizer across both low and high frequencies in the frequency domain. The code and resources for Cont-SPT can be found in <a target="_blank" rel="noopener" href="https://github.com/Yixing-Li/Continuous-Speech-Tokenizer">https://github.com/Yixing-Li/Continuous-Speech-Tokenizer</a> </p>
<blockquote>
<p>åœ¨å¤§å‹è¯­è¨€æ¨¡å‹æ—¶ä»£ï¼Œè¯­éŸ³å’Œè¯­è¨€çš„èåˆå¼•èµ·äº†äººä»¬çš„å¹¿æ³›å…³æ³¨ã€‚åœ¨æ–‡æœ¬åˆ°è¯­éŸ³çš„ä»»åŠ¡ä¸­ï¼Œç¦»æ•£è¯­éŸ³æ ‡è®°é€šå¸¸ç”¨äºè¯­éŸ³å‹ç¼©å’Œä¾¿æºæ€§ï¼Œè¿™ä¾¿äºä¸æ–‡æœ¬è¿›è¡Œè”åˆè®­ç»ƒå¹¶å…·æœ‰è‰¯å¥½çš„å‹ç¼©æ•ˆç‡ã€‚ç„¶è€Œï¼Œæˆ‘ä»¬å‘ç°ç¦»æ•£è¯­éŸ³æ ‡è®°å™¨ä»ç„¶å­˜åœ¨ä¿¡æ¯ä¸¢å¤±çš„é—®é¢˜ã€‚å› æ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ç®€å•æœ‰æ•ˆçš„è¿ç»­è¯­éŸ³æ ‡è®°å™¨ï¼ˆCont-SPTï¼‰å’ŒåŸºäºè¿ç»­è¯­éŸ³æ ‡è®°çš„æ–‡æœ¬åˆ°è¯­éŸ³æ¨¡å‹ã€‚ç»“æœè¡¨æ˜ï¼ŒåŸºäºè¿ç»­è¯­éŸ³æ ‡è®°å™¨çš„è¯­éŸ³è¯­è¨€æ¨¡å‹å…·æœ‰æ›´å¥½çš„è¿ç»­æ€§å’Œæ›´é«˜çš„é¢„ä¼°å¹³å‡æ„è§å¾—åˆ†ï¼ˆMoSï¼‰ã€‚è¿™ç§æ”¹è¿›å½’å› äºè¿ç»­è¯­éŸ³æ ‡è®°å™¨åœ¨ä½é¢‘å’Œé«˜é¢‘é¢‘ç‡åŸŸä¸­å…·æœ‰æ›´å¥½çš„ä¿¡æ¯ä¿æŒç‡ã€‚Cont-SPTçš„ä»£ç å’Œèµ„æºå¯ä»¥åœ¨<a target="_blank" rel="noopener" href="https://github.com/Yixing-Li/Continuous-Speech-Tokenizer%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/Yixing-Li/Continuous-Speech-Tokenizeræ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2410.17081v2">PDF</a> NAACL 2025 Findings Poster</p>
<p><strong>æ€»ç»“</strong><br>    åœ¨å¤§å‹è¯­è¨€æ¨¡å‹æ—¶ä»£ï¼Œè¯­éŸ³å’Œè¯­è¨€çš„èåˆå¼•èµ·äº†å¹¿æ³›å…³æ³¨ã€‚ç¦»æ•£è¯­éŸ³æ ‡è®°å¸¸ç”¨äºæ–‡æœ¬åˆ°è¯­éŸ³çš„ä»»åŠ¡ï¼Œä»¥å®ç°è¯­éŸ³å‹ç¼©å’Œä¾¿æºæ€§ï¼Œä¾¿äºä¸æ–‡æœ¬è¿›è¡Œè”åˆè®­ç»ƒå¹¶å…·æœ‰é«˜æ•ˆçš„å‹ç¼©æ€§èƒ½ã€‚ç„¶è€Œï¼Œç ”ç©¶å‘ç°ç¦»æ•£è¯­éŸ³åˆ†è¯å™¨ä»å­˜åœ¨ä¿¡æ¯ä¸¢å¤±çš„é—®é¢˜ã€‚å› æ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ç®€å•æœ‰æ•ˆçš„è¿ç»­è¯­éŸ³åˆ†è¯å™¨Cont-SPTå’ŒåŸºäºè¿ç»­è¯­éŸ³æ ‡è®°çš„æ–‡æœ¬åˆ°è¯­éŸ³æ¨¡å‹ã€‚ç»“æœè¡¨æ˜ï¼ŒåŸºäºè¿ç»­è¯­éŸ³åˆ†è¯å™¨çš„è¯­éŸ³è¯­è¨€æ¨¡å‹å…·æœ‰æ›´å¥½çš„è¿ç»­æ€§å’Œæ›´é«˜çš„é¢„ä¼°å¹³å‡æ„è§å¾—åˆ†ï¼ˆMoSï¼‰ã€‚è¿™ç§æ”¹è¿›å½’å› äºè¿ç»­è¯­éŸ³åˆ†è¯å™¨åœ¨é¢‘åŸŸä¸­ä½é¢‘å’Œé«˜é¢‘çš„æ›´å¥½ä¿¡æ¯ä¿æŒç‡ã€‚Cont-SPTçš„ä»£ç å’Œèµ„æºå¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/Yixing-Li/Continuous-Speech-Tokenizer%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/Yixing-Li/Continuous-Speech-Tokenizeræ‰¾åˆ°ã€‚</a></p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>è¯­éŸ³å’Œè¯­è¨€èåˆåœ¨å¤§å‹è¯­è¨€æ¨¡å‹æ—¶ä»£å—åˆ°å…³æ³¨ã€‚</li>
<li>ç¦»æ•£è¯­éŸ³æ ‡è®°ç”¨äºæ–‡æœ¬åˆ°è¯­éŸ³ä»»åŠ¡ä»¥å®ç°è¯­éŸ³å‹ç¼©å’Œä¾¿æºæ€§ã€‚</li>
<li>ç¦»æ•£è¯­éŸ³åˆ†è¯å™¨å­˜åœ¨ä¿¡æ¯ä¸¢å¤±çš„é—®é¢˜ã€‚</li>
<li>æå‡ºäº†è¿ç»­è¯­éŸ³åˆ†è¯å™¨Cont-SPTã€‚</li>
<li>åŸºäºè¿ç»­è¯­éŸ³æ ‡è®°çš„æ–‡æœ¬åˆ°è¯­éŸ³æ¨¡å‹è¡¨ç°å‡ºæ›´å¥½çš„è¿ç»­æ€§å’Œæ›´é«˜çš„é¢„ä¼°å¹³å‡æ„è§å¾—åˆ†ï¼ˆMoSï¼‰ã€‚</li>
<li>è¿ç»­è¯­éŸ³åˆ†è¯å™¨åœ¨é¢‘åŸŸä¸­ä½é¢‘å’Œé«˜é¢‘çš„ä¿¡æ¯ä¿æŒç‡æ›´é«˜ã€‚</li>
<li>Cont-SPTçš„ä»£ç å’Œèµ„æºå¯é€šè¿‡ç‰¹å®šé“¾æ¥è·å–ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2410.17081">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-a2f6d4234ae6ceb8eced875223976719.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ac1c91fd3522e27a7a088ee987913ee4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a486d8f9668db1bd0fc9ec5cec8797ae.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-7521a02d93a4930b4722ab2d9a8daff9.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="SF-Speech-Straightened-Flow-for-Zero-Shot-Voice-Clone"><a href="#SF-Speech-Straightened-Flow-for-Zero-Shot-Voice-Clone" class="headerlink" title="SF-Speech: Straightened Flow for Zero-Shot Voice Clone"></a>SF-Speech: Straightened Flow for Zero-Shot Voice Clone</h2><p><strong>Authors:Xuyuan Li, Zengqiang Shang, Hua Hua, Peiyang Shi, Chen Yang, Li Wang, Pengyuan Zhang</strong></p>
<p>Recently, neural ordinary differential equations (ODE) models trained with flow matching have achieved impressive performance on the zero-shot voice clone task. Nevertheless, postulating standard Gaussian noise as the initial distribution of ODE gives rise to numerous intersections within the fitted targets of flow matching, which presents challenges to model training and enhances the curvature of the learned generated trajectories. These curved trajectories restrict the capacity of ODE models for generating desirable samples with a few steps. This paper proposes SF-Speech, a novel voice clone model based on ODE and in-context learning. Unlike the previous works, SF-Speech adopts a lightweight multi-stage module to generate a more deterministic initial distribution for ODE. Without introducing any additional loss function, we effectively straighten the curved reverse trajectories of the ODE model by jointly training it with the proposed module. Experiment results on datasets of various scales show that SF-Speech outperforms the state-of-the-art zero-shot TTS methods and requires only a quarter of the solver steps, resulting in a generation speed approximately 3.7 times that of Voicebox and E2 TTS. Audio samples are available at the demo page\footnote{[Online] Available: <a target="_blank" rel="noopener" href="https://lixuyuan102.github.io/Demo/%7D">https://lixuyuan102.github.io/Demo/}</a>. </p>
<blockquote>
<p>æœ€è¿‘ï¼Œä½¿ç”¨æµåŒ¹é…è®­ç»ƒçš„ç¥ç»å¸¸å¾®åˆ†æ–¹ç¨‹ï¼ˆODEï¼‰æ¨¡å‹åœ¨é›¶æ ·æœ¬è¯­éŸ³å…‹éš†ä»»åŠ¡ä¸Šå–å¾—äº†ä»¤äººå°è±¡æ·±åˆ»çš„è¡¨ç°ã€‚ç„¶è€Œï¼Œå‡è®¾æ ‡å‡†é«˜æ–¯å™ªå£°ä¸ºODEçš„åˆå§‹åˆ†å¸ƒä¼šå¯¼è‡´æµåŒ¹é…çš„æ‹Ÿåˆç›®æ ‡å†…éƒ¨å‡ºç°è®¸å¤šäº¤é›†ï¼Œè¿™ç»™æ¨¡å‹è®­ç»ƒå¸¦æ¥äº†æŒ‘æˆ˜ï¼Œå¹¶å¢å¼ºäº†å­¦ä¹ ç”Ÿæˆè½¨è¿¹çš„æ›²ç‡ã€‚è¿™äº›å¼¯æ›²çš„è½¨è¿¹é™åˆ¶äº†ODEæ¨¡å‹åœ¨å‡ æ­¥å†…ç”Ÿæˆç†æƒ³æ ·æœ¬çš„èƒ½åŠ›ã€‚æœ¬æ–‡æå‡ºäº†SF-Speechï¼Œè¿™æ˜¯ä¸€ç§åŸºäºODEå’Œä¸Šä¸‹æ–‡å­¦ä¹ çš„æ–°å‹è¯­éŸ³å…‹éš†æ¨¡å‹ã€‚ä¸åŒäºä»¥å‰çš„å·¥ä½œï¼ŒSF-Speeché‡‡ç”¨è½»é‡çº§çš„å¤šé˜¶æ®µæ¨¡å—æ¥ä¸ºODEç”Ÿæˆæ›´ç¡®å®šçš„åˆå§‹åˆ†å¸ƒã€‚æˆ‘ä»¬æ²¡æœ‰å¼•å…¥ä»»ä½•é¢å¤–çš„æŸå¤±å‡½æ•°ï¼Œè€Œæ˜¯é€šè¿‡ä¸æ‰€æå‡ºæ¨¡å—çš„å…±åŒè®­ç»ƒï¼Œæœ‰æ•ˆåœ°æ ¡æ­£äº†ODEæ¨¡å‹çš„å¼¯æ›²åå‘è½¨è¿¹ã€‚åœ¨å„ç§è§„æ¨¡æ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼ŒSF-Speechä¼˜äºæœ€å…ˆè¿›çš„é›¶æ ·æœ¬TTSæ–¹æ³•ï¼Œå¹¶ä¸”ä»…éœ€å››åˆ†ä¹‹ä¸€çš„æ±‚è§£å™¨æ­¥éª¤ï¼Œç”Ÿæˆé€Ÿåº¦å¤§çº¦æ˜¯Voiceboxå’ŒE2 TTSçš„3.7å€ã€‚éŸ³é¢‘æ ·æœ¬å¯åœ¨æ¼”ç¤ºé¡µé¢è·å¾—^[åœ¨çº¿å¯ç”¨ï¼š<a target="_blank" rel="noopener" href="https://lixuyuan102.github.io/Demo/]%E3%80%82">https://lixuyuan102.github.io/Demo/]ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2410.12399v2">PDF</a> Accepted by IEEE Transactions on Audio, Speech and Language   Processing</p>
<p><strong>Summary</strong></p>
<p>ç¥ç»ç½‘ç»œå¸¸å¾®åˆ†æ–¹ç¨‹ï¼ˆODEï¼‰æ¨¡å‹åœ¨é›¶æ ·æœ¬è¯­éŸ³å…‹éš†ä»»åŠ¡ä¸Šå–å¾—äº†ä»¤äººå°è±¡æ·±åˆ»çš„æ€§èƒ½è¡¨ç°ã€‚ç„¶è€Œï¼Œä»¥æ ‡å‡†é«˜æ–¯å™ªå£°ä½œä¸ºODEçš„åˆå§‹åˆ†å¸ƒä¼šå¯¼è‡´æµåŒ¹é…çš„ç›®æ ‡æ‹Ÿåˆä¸­å‡ºç°è®¸å¤šäº¤é›†ï¼Œè¿™ç»™æ¨¡å‹è®­ç»ƒå¸¦æ¥æŒ‘æˆ˜ï¼Œå¹¶å¢å¼ºäº†å­¦ä¹ ç”Ÿæˆè½¨è¿¹çš„æ›²ç‡ã€‚æœ¬æ–‡æå‡ºäº†åŸºäºODEå’Œä¸Šä¸‹æ–‡å­¦ä¹ çš„SF-Speechæ–°å‹è¯­éŸ³å…‹éš†æ¨¡å‹ã€‚ä¸åŒäºä»¥å‰çš„å·¥ä½œï¼ŒSF-Speeché‡‡ç”¨è½»é‡çº§çš„å¤šé˜¶æ®µæ¨¡å—æ¥ä¸ºODEç”Ÿæˆæ›´ç¡®å®šçš„åˆå§‹åˆ†å¸ƒã€‚é€šè¿‡è”åˆè®­ç»ƒè¯¥æ¨¡å—ï¼Œæˆ‘ä»¬æ— éœ€å¼•å…¥ä»»ä½•é¢å¤–çš„æŸå¤±å‡½æ•°å³å¯æœ‰æ•ˆåœ°æ ¡æ­£ODEæ¨¡å‹çš„å¼¯æ›²åå‘è½¨è¿¹ã€‚åœ¨å¤šç§è§„æ¨¡æ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼ŒSF-Speechåœ¨é›¶æ ·æœ¬æ–‡æœ¬åˆ°è¯­éŸ³è½¬æ¢æ–¹é¢çš„æ€§èƒ½ä¼˜äºæœ€æ–°æŠ€æœ¯ï¼Œä¸”ä»…éœ€å››åˆ†ä¹‹ä¸€çš„æ±‚è§£å™¨æ­¥éª¤ï¼Œç”Ÿæˆé€Ÿåº¦å¤§çº¦æ˜¯Voiceboxå’ŒE2 TTSçš„3.7å€ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç¥ç»ç½‘ç»œå¸¸å¾®åˆ†æ–¹ç¨‹ï¼ˆODEï¼‰æ¨¡å‹åœ¨é›¶æ ·æœ¬è¯­éŸ³å…‹éš†ä»»åŠ¡ä¸Šå…·æœ‰å“è¶Šæ€§èƒ½ã€‚</li>
<li>ä»¥æ ‡å‡†é«˜æ–¯å™ªå£°ä½œä¸ºODEåˆå§‹åˆ†å¸ƒä¼šå¯¼è‡´æµåŒ¹é…ä¸­çš„ç›®æ ‡æ‹Ÿåˆäº¤é›†å’Œè½¨è¿¹æ›²ç‡å¢å¼ºã€‚</li>
<li>SF-Speechæ˜¯ä¸€ç§æ–°å‹çš„è¯­éŸ³å…‹éš†æ¨¡å‹ï¼ŒåŸºäºODEå’Œä¸Šä¸‹æ–‡å­¦ä¹ ã€‚</li>
<li>SF-Speeché‡‡ç”¨è½»é‡çº§å¤šé˜¶æ®µæ¨¡å—ç”Ÿæˆæ›´ç¡®å®šçš„ODEåˆå§‹åˆ†å¸ƒã€‚</li>
<li>é€šè¿‡è”åˆè®­ç»ƒï¼ŒSF-Speechèƒ½æœ‰æ•ˆæ ¡æ­£ODEæ¨¡å‹çš„å¼¯æ›²åå‘è½¨è¿¹ï¼Œæ— éœ€é¢å¤–çš„æŸå¤±å‡½æ•°ã€‚</li>
<li>SF-Speechåœ¨å¤šç§æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨ç°ä¼˜äºç°æœ‰æŠ€æœ¯ï¼Œä¸”å‡å°‘äº†æ±‚è§£å™¨æ­¥éª¤å’Œæé«˜äº†ç”Ÿæˆé€Ÿåº¦ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2410.12399">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-3f54ed6c1c036704cf78583341190999.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c42ed00e1a01701ddea1b8235a065904.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-4806d14cab1e4eda416bd5e6b2c0f646.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-bda72226910d0d2c9b20440eecf366b2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-051e36ed8c714e7c8b117910852132c8.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="Enabling-Auditory-Large-Language-Models-for-Automatic-Speech-Quality-Evaluation"><a href="#Enabling-Auditory-Large-Language-Models-for-Automatic-Speech-Quality-Evaluation" class="headerlink" title="Enabling Auditory Large Language Models for Automatic Speech Quality   Evaluation"></a>Enabling Auditory Large Language Models for Automatic Speech Quality   Evaluation</h2><p><strong>Authors:Siyin Wang, Wenyi Yu, Yudong Yang, Changli Tang, Yixuan Li, Jimin Zhuang, Xianzhao Chen, Xiaohai Tian, Jun Zhang, Guangzhi Sun, Lu Lu, Yuxuan Wang, Chao Zhang</strong></p>
<p>Speech quality assessment typically requires evaluating audio from multiple aspects, such as mean opinion score (MOS) and speaker similarity (SIM) \etc., which can be challenging to cover using one small model designed for a single task. In this paper, we propose leveraging recently introduced auditory large language models (LLMs) for automatic speech quality assessment. By employing task-specific prompts, auditory LLMs are finetuned to predict MOS, SIM and A&#x2F;B testing results, which are commonly used for evaluating text-to-speech systems. Additionally, the finetuned auditory LLM is able to generate natural language descriptions assessing aspects like noisiness, distortion, discontinuity, and overall quality, providing more interpretable outputs. Extensive experiments have been performed on the NISQA, BVCC, SOMOS and VoxSim speech quality datasets, using open-source auditory LLMs such as SALMONN, Qwen-Audio, and Qwen2-Audio. For the natural language descriptions task, a commercial model Google Gemini 1.5 Pro is also evaluated. The results demonstrate that auditory LLMs achieve competitive performance compared to state-of-the-art task-specific small models in predicting MOS and SIM, while also delivering promising results in A&#x2F;B testing and natural language descriptions. Our data processing scripts and finetuned model checkpoints can be found at <a target="_blank" rel="noopener" href="https://github.com/bytedance/SALMONN">https://github.com/bytedance/SALMONN</a>. </p>
<blockquote>
<p>è¯­éŸ³è´¨é‡è¯„ä¼°é€šå¸¸éœ€è¦ä»å¤šä¸ªæ–¹é¢å¯¹éŸ³é¢‘è¿›è¡Œè¯„ä¼°ï¼Œå¦‚å¹³å‡æ„è§å¾—åˆ†ï¼ˆMOSï¼‰å’Œè¯´è¯äººç›¸ä¼¼æ€§ï¼ˆSIMï¼‰ç­‰ã€‚ä½¿ç”¨ä¸“ä¸ºå•ä¸€ä»»åŠ¡è®¾è®¡çš„å°å‹æ¨¡å‹æ¥è¦†ç›–è¿™äº›æ–¹é¢å¯èƒ½å…·æœ‰æŒ‘æˆ˜æ€§ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºåˆ©ç”¨æœ€è¿‘å¼•å…¥çš„è‡ªåŠ¨è¯­éŸ³è´¨é‡è¯„ä¼°çš„å¬è§‰å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ã€‚é€šè¿‡é‡‡ç”¨ç‰¹å®šä»»åŠ¡çš„æç¤ºï¼Œå¬è§‰LLMç»è¿‡å¾®è°ƒä»¥é¢„æµ‹MOSã€SIMå’ŒA&#x2F;Bæµ‹è¯•ç»“æœï¼Œè¿™äº›ç»“æœé€šå¸¸ç”¨äºè¯„ä¼°æ–‡æœ¬åˆ°è¯­éŸ³ç³»ç»Ÿã€‚æ­¤å¤–ï¼Œç»è¿‡å¾®è°ƒåçš„å¬è§‰LLMèƒ½å¤Ÿç”Ÿæˆè‡ªç„¶è¯­è¨€æè¿°ï¼Œè¯„ä¼°è¯¸å¦‚å™ªéŸ³ã€å¤±çœŸã€æ–­ç»­å’Œæ€»ä½“è´¨é‡ç­‰æ–¹é¢çš„å†…å®¹ï¼Œä»è€Œæä¾›æ›´å¯è§£é‡Šçš„è¾“å‡ºã€‚åœ¨NISQAã€BVCCã€SOMOSå’ŒVoxSimè¯­éŸ³è´¨é‡æ•°æ®é›†ä¸Šè¿›è¡Œäº†å¤§é‡å®éªŒï¼Œä½¿ç”¨äº†å¼€æºçš„å¬è§‰LLMï¼Œå¦‚SALMONNã€Qwen-Audioå’ŒQwen2-Audioã€‚å¯¹äºè‡ªç„¶è¯­è¨€æè¿°ä»»åŠ¡ï¼Œè¿˜è¯„ä¼°äº†å•†ä¸šæ¨¡å‹Google Gemini 1.5 Proã€‚ç»“æœè¡¨æ˜ï¼Œå¬è§‰LLMåœ¨é¢„æµ‹MOSå’ŒSIMæ–¹é¢ä¸æœ€å…ˆè¿›çš„ç‰¹å®šä»»åŠ¡å°å‹æ¨¡å‹ç›¸æ¯”å…·æœ‰ç«äº‰åŠ›ï¼ŒåŒæ—¶åœ¨A&#x2F;Bæµ‹è¯•å’Œè‡ªç„¶è¯­è¨€æè¿°æ–¹é¢ä¹Ÿè¡¨ç°å‡ºæœ‰å¸Œæœ›çš„ç»“æœã€‚æˆ‘ä»¬çš„æ•°æ®å¤„ç†è„šæœ¬å’Œå¾®è°ƒåçš„æ¨¡å‹æ£€æŸ¥ç‚¹å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/bytedance/SALMONN">https://github.com/bytedance/SALMONN</a>æ‰¾åˆ°ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2409.16644v3">PDF</a> Accepted by ICASSP 2025</p>
<p><strong>Summary</strong><br>     æœ¬æ–‡æå‡ºåˆ©ç”¨æœ€è¿‘å¼•å…¥çš„å¬è§‰å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰è¿›è¡Œè‡ªåŠ¨è¯­éŸ³è´¨é‡è¯„ä¼°ã€‚é€šè¿‡ç‰¹å®šä»»åŠ¡çš„æç¤ºï¼Œå¯¹å¬è§‰LLMsè¿›è¡Œå¾®è°ƒä»¥é¢„æµ‹å¹³å‡æ„è§å¾—åˆ†ï¼ˆMOSï¼‰ã€ç›¸ä¼¼æ€§ï¼ˆSIMï¼‰ç­‰ï¼Œå¹¶å¯¹è¯­éŸ³è´¨é‡æ•°æ®é›†è¿›è¡Œäº†å¤§é‡å®éªŒéªŒè¯ã€‚ç»“æœè¡¨æ˜ï¼Œå¬è§‰LLMsä¸æœ€å…ˆè¿›çš„ç‰¹å®šä»»åŠ¡å°å‹æ¨¡å‹ç›¸æ¯”å…·æœ‰ç«äº‰åŠ›ï¼ŒåŒæ—¶è¿˜åœ¨A&#x2F;Bæµ‹è¯•å’Œè‡ªç„¶è¯­è¨€æè¿°æ–¹é¢è¡¨ç°å‡ºè‰¯å¥½çš„æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è¯­éŸ³è´¨é‡è¯„ä¼°æ¶‰åŠå¤šä¸ªæ–¹é¢ï¼Œå¦‚å¹³å‡æ„è§å¾—åˆ†ï¼ˆMOSï¼‰å’Œè¯´è¯äººç›¸ä¼¼æ€§ï¼ˆSIMï¼‰ã€‚</li>
<li>å¬è§‰å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰è¢«å¼•å…¥ç”¨äºè‡ªåŠ¨è¯­éŸ³è´¨é‡è¯„ä¼°ã€‚</li>
<li>é€šè¿‡ç‰¹å®šä»»åŠ¡çš„æç¤ºï¼Œå¬è§‰LLMså¯é¢„æµ‹MOSã€SIMä»¥åŠA&#x2F;Bæµ‹è¯•ç»“æœã€‚</li>
<li>å¬è§‰LLMså¯ä»¥ç”Ÿæˆè¯„ä¼°è¯­éŸ³è´¨é‡å„æ–¹é¢çš„è‡ªç„¶è¯­è¨€æè¿°ï¼Œå¦‚å™ªéŸ³ã€å¤±çœŸã€æ–­ç»­å’Œæ€»ä½“è´¨é‡ã€‚</li>
<li>å¹¿æ³›å®éªŒè¡¨æ˜ï¼Œå¬è§‰LLMsåœ¨é¢„æµ‹MOSå’ŒSIMæ–¹é¢è¡¨ç°å‡ºç«äº‰åŠ›ï¼Œå¹¶ä¸å…ˆè¿›çš„å°å‹æ¨¡å‹ç›¸æ¯”å…·æœ‰æ½œåŠ›ã€‚</li>
<li>å¬è§‰LLMsåœ¨A&#x2F;Bæµ‹è¯•ä»¥åŠè‡ªç„¶è¯­è¨€æè¿°æ–¹é¢ä¹Ÿæ˜¾ç¤ºå‡ºè‰¯å¥½çš„æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2409.16644">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-d49ecc20f884af03000e63af5f867530.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-0fc8eaf8d5cded4d6217ebce5f6378e3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4678db138eb072cda7b99840e513682e.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-cd3fd21a2b0a8f926cc644b8382796f8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f559231a629b22b8761c535d2998852b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-81fefc28d16dda07b1a120dad72ab9ce.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-04-03/TTS/">https://kedreamix.github.io/Talk2Paper/Paper/2025-04-03/TTS/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/TTS/">
                                    <span class="chip bg-color">TTS</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-04-04/TTS/">
                    <div class="card-image">
                        
                        <img src="https://pica.zhimg.com/v2-ef56415eb65cfb8300e0a1913078f61c.jpg" class="responsive-img" alt="TTS">
                        
                        <span class="card-title">TTS</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            TTS æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-04-04  LEP3 A High-Luminosity e+e- Higgs and ElectroweakFactory in the LHC   Tunnel
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-04-04
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/TTS/" class="post-category">
                                    TTS
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/TTS/">
                        <span class="chip bg-color">TTS</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-04-04/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-6e7076a4f8670e51111dc0215d03e1a0.jpg" class="responsive-img" alt="åŒ»å­¦å›¾åƒ">
                        
                        <span class="card-title">åŒ»å­¦å›¾åƒ</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            åŒ»å­¦å›¾åƒ æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-04-04  BioAtt Anatomical Prior Driven Low-Dose CT Denoising
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-04-04
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/" class="post-category">
                                    åŒ»å­¦å›¾åƒ
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                        <span class="chip bg-color">åŒ»å­¦å›¾åƒ</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">25156.8k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
