<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="TTS">
    <meta name="description" content="TTS 方向最新论文已更新，请持续关注 Update in 2025-04-04  LEP3 A High-Luminosity e+e- Higgs and ElectroweakFactory in the LHC   Tunnel">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>TTS | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loader页面消失采用渐隐的方式*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logo出现动画 */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//使用渐隐的方法淡出loading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//强制显示loading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">嘘~  正在从服务器偷取页面 . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>首页</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>标签</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>分类</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>归档</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="搜索" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="深色/浅色模式" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			首页
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			标签
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			分类
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			归档
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pic1.zhimg.com/v2-a486d8f9668db1bd0fc9ec5cec8797ae.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">TTS</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- 文章内容详情 -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/TTS/">
                                <span class="chip bg-color">TTS</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/TTS/" class="post-category">
                                TTS
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp;
                    2025-04-04
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>更新日期:&nbsp;&nbsp;
                    2025-05-14
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>文章字数:&nbsp;&nbsp;
                    14.1k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>阅读时长:&nbsp;&nbsp;
                    57 分
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>阅读次数:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>⚠️ 以下所有内容总结都来自于 大语言模型的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p>
</blockquote>
<h1 id="2025-04-04-更新"><a href="#2025-04-04-更新" class="headerlink" title="2025-04-04 更新"></a>2025-04-04 更新</h1><h2 id="LEP3-A-High-Luminosity-e-e-Higgs-and-ElectroweakFactory-in-the-LHC-Tunnel"><a href="#LEP3-A-High-Luminosity-e-e-Higgs-and-ElectroweakFactory-in-the-LHC-Tunnel" class="headerlink" title="LEP3: A High-Luminosity e+e- Higgs and ElectroweakFactory in the LHC   Tunnel"></a>LEP3: A High-Luminosity e+e- Higgs and ElectroweakFactory in the LHC   Tunnel</h2><p><strong>Authors:C. Anastopoulos, R. Assmann, A. Ball, O. Bruning, O. Buchmueller, T. Camporesi, P. Collier, J Dainton, G. Davies, J. R. Ellis, B. Goddard, L. Gouskos, M. Klute, M. Koratzinos, G. Landsberg, K. Long, L. Malgeri, F. Maltoni, F. Moortgat, C. Mariotti, S. Myers, J. A. Osborne, M. Pierini, D. R. Tovey, D. Treille, T. S. Virdee, N. Wardle, M. Zanetti</strong></p>
<p>As stated in the 2019 European Strategy for Particle Physics (ESPP), it is of the utmost importance that the HL-LHC upgrade of the accelerator and the experiments be successfully completed in a timely manner. All necessary efforts should be devoted to achieving this goal. We also recall two of the principal recommendations of the 2019 ESPP for future accelerator initiatives, namely that 1) An electron-positron Higgs factory is the highest priority for the next collider (Rec. c). 2) Europe, together with its international partners, should investigate the technical and financial feasibility of a future hadron collider at CERN with a centre-of-mass energy of at least 100 TeV and with an electron-positron Higgs and electroweak factory as a possible first stage (Rec. e). A major objective in particle physics is always to operate an accelerator that allows a leap of an order of magnitude in the constituent centre-of-mass energy with respect to the previous one. We support FCC-ee and FCC-hh as the preferred option for CERN future, as it addresses both of the above recommendations.   The guidance for the 2025 ESPP requests, in addition to the preferred option, the inclusion of &#96;&#96;prioritised alternatives to be pursued if the chosen preferred option turns out not to be feasible or competitive’’. Proposed alternatives to the preferred FCC option include linear, muon colliders and LHeC accelerators. In response to this request we propose reusing the existing LHC tunnel for an electron-positron collider, called LEP3, as a back-up alternative if the FCC cannot proceed. LEP3 leverages much of the R&amp;D conducted for FCC-ee, offers high-precision studies of Z, W, and Higgs bosons below the tt threshold, and offers potential physics performance comparable or superior to other fallback options at a lower cost while supporting continued R&amp;D towards a next-generation energy frontier machine. </p>
<blockquote>
<p>如2019年欧洲粒子物理策略（ESPP）所述，及时成功完成HL-LHC加速器及实验的升级至关重要。所有必要的努力都应致力于实现这一目标。我们还回顾了2019年ESPP针对未来加速器倡议的两项主要建议，即：1）电子-正电子希格斯工厂是下一个对撞机的最高优先级（建议c）。2）欧洲应与其国际合作伙伴共同研究在CERN建造至少100TeV质心能量的未来强子对撞机的技术和经济可行性，并将电子-正电子希格斯工厂和弱电工厂作为可能的第一阶段（建议e）。粒子物理学的一个主要目标始终是运行一个加速器，该加速器在质心能量方面相比之前的有一个数量级的飞跃。我们支持FCC-ee和FCC-hh作为CERN未来的首选方案，因为它符合上述两项建议。此外，根据2025年ESPP的要求，除了首选方案外，还需包括“如果首选方案不可行或缺乏竞争力时优先考虑的替代方案”。作为首选FCC方案的替代方案，提出了线性加速器、缪子对撞机和LHeC加速器等提议。对此要求，我们提议在FCC无法实施的情况下，利用现有的LHC隧道进行电子-正电子对撞机（称为LEP3）作为后备替代方案。LEP3充分利用了为FCC-ee开展的研究与开发工作，可以进行低于tt阈值的Z、W和希格斯玻色子的高精度研究，并在较低的成本下提供与其他备选方案相当或更好的物理性能表现，同时支持下一代能源前沿机器的研究与开发。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.00541v1">PDF</a> 11 pages, 3 tables</p>
<p><strong>摘要</strong><br>     根据 2019 年欧洲粒子物理战略 (ESPP)，大型强子对撞机 (HL-LHC) 的加速器及其实验的升级工作至关重要，必须及时成功完成。欧洲应与其国际合作伙伴共同研究欧洲核研究组织未来建造至少 100 TeV 质心能量的强子对撞机的技术和经济可行性，并将电子正负电子希格斯工厂作为可能的第一个阶段。同时支持未来环形对撞机 (FCC) 作为首选方案，该方案符合上述两个建议。对于 2025 年 ESPP 的指导请求，除了首选方案外，还应考虑“如果首选方案不可行或不具竞争力时，优先追求的备选方案”。因此提出利用现有大型强子对撞机隧道建立电子正负电子对撞机（LEP3）作为备选方案。LEP3 可以利用大量为未来环形对撞机开展的研发工作成果，在高精度研究 Z 波、W 波和希格斯玻色子方面表现出优异的性能。该方案具有成本低的优势，并支持下一代能量前沿机器的研发工作。</p>
<p><strong>关键见解</strong></p>
<ol>
<li>完成 HL-LHC 加速器和实验的升级至关重要。需要所有必要的努力来实现这一目标。</li>
<li>ESPP 主张建设电子正负电子希格斯工厂作为未来首选对撞机项目。欧洲及其国际合作伙伴正在探索未来强子对撞机的技术和经济可行性。</li>
<li>支持 FCC 作为首选方案，它符合 ESPP 的两个主要建议：优先建设电子正负电子希格斯工厂作为初步阶段和对更高质心能量的探索。</li>
<li>对于可能的备选方案，提出利用现有的大型强子对撞机隧道建立 LEP3 作为备用选项方案，利用已有的研发成果以实现低成本高性能的物理研究。此外还包括线性加速器、μ子和 LHeC 加速器等替代方案。</li>
<li>LEP3 具有高灵敏度研究特定粒子的潜力，这可能为将来的能源前沿研究开辟道路，作为一种相对低成本的有效策略被优先考虑采用这种技术方案对于延续未来的粒子物理研发也有着重要作用。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.00541">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-1fbaabe0822176a369625404e53039b0.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="TeleAntiFraud-28k-An-Audio-Text-Slow-Thinking-Dataset-for-Telecom-Fraud-Detection"><a href="#TeleAntiFraud-28k-An-Audio-Text-Slow-Thinking-Dataset-for-Telecom-Fraud-Detection" class="headerlink" title="TeleAntiFraud-28k: An Audio-Text Slow-Thinking Dataset for Telecom Fraud   Detection"></a>TeleAntiFraud-28k: An Audio-Text Slow-Thinking Dataset for Telecom Fraud   Detection</h2><p><strong>Authors:Zhiming Ma, Peidong Wang, Minhua Huang, Jingpeng Wang, Kai Wu, Xiangzhao Lv, Yachun Pang, Yin Yang, Wenjie Tang, Yuchen Kang</strong></p>
<p>The detection of telecom fraud faces significant challenges due to the lack of high-quality multimodal training data that integrates audio signals with reasoning-oriented textual analysis. To address this gap, we present TeleAntiFraud-28k, the first open-source audio-text slow-thinking dataset specifically designed for automated telecom fraud analysis. Our dataset is constructed through three strategies: (1) Privacy-preserved text-truth sample generation using automatically speech recognition (ASR)-transcribed call recordings (with anonymized original audio), ensuring real-world consistency through text-to-speech (TTS) model regeneration; (2) Semantic enhancement via large language model (LLM)-based self-instruction sampling on authentic ASR outputs to expand scenario coverage; (3) Multi-agent adversarial synthesis that simulates emerging fraud tactics through predefined communication scenarios and fraud typologies. The generated dataset contains 28,511 rigorously processed speech-text pairs, complete with detailed annotations for fraud reasoning. The dataset is divided into three tasks: scenario classification, fraud detection, fraud type classification. Furthermore, we construct TeleAntiFraud-Bench, a standardized evaluation benchmark comprising proportionally sampled instances from the dataset, to facilitate systematic testing of model performance on telecom fraud detection tasks. We also contribute a production-optimized supervised fine-tuning (SFT) model trained on hybrid real&#x2F;synthetic data, while open-sourcing the data processing framework to enable community-driven dataset expansion. This work establishes a foundational framework for multimodal anti-fraud research while addressing critical challenges in data privacy and scenario diversity. The project will be released at <a target="_blank" rel="noopener" href="https://github.com/JimmyMa99/TeleAntiFraud">https://github.com/JimmyMa99/TeleAntiFraud</a>. </p>
<blockquote>
<p>电信欺诈检测面临着巨大的挑战，因为缺乏高质量的多模式训练数据，无法将音频信号与面向推理的文本分析相结合。为了解决这一空白，我们推出了TeleAntiFraud-28k，这是专门为电信欺诈自动化分析设计的首个开源音频文本慢思考数据集。我们的数据集通过三种策略构建：（1）使用自动语音识别（ASR）转录的录音生成隐私保护文本真实样本（带有匿名原始音频），并通过文本到语音（TTS）模型再生确保现实世界的一致性；（2）通过基于大型语言模型（LLM）的自我指令采样对真实的ASR输出进行语义增强，以扩大场景覆盖范围；（3）模拟新兴欺诈策略的多代理对抗合成通过预设的通信场景和欺诈类型。生成的数据集包含经过严格处理的28,511个语音文本对，带有详细的欺诈推理注释。数据集分为三个任务：场景分类、欺诈检测、欺诈类型分类。此外，我们构建了TeleAntiFraud-Bench，这是一个标准化的评估基准，由数据集中按比例采样的实例组成，便于对电信欺诈检测任务上的模型性能进行系统性测试。我们还为混合真实&#x2F;合成数据训练的生产优化监督微调（SFT）模型做出了贡献，同时开源数据处理框架以推动数据集扩展。这项工作为跨模式反欺诈研究建立了基础框架，并解决了数据隐私和场景多样性方面的关键挑战。该项目将在<a target="_blank" rel="noopener" href="https://github.com/JimmyMa99/TeleAntiFraud%E5%B8%B3%E5%B9%BF%E3%80%82">https://github.com/JimmyMa99/TeleAntiFraud发布。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.24115v3">PDF</a> </p>
<p><strong>摘要</strong></p>
<p>本文介绍了针对电信欺诈检测所面临的挑战，提出了一个开放源的多模态数据集TeleAntiFraud-28k。该数据集整合了音频信号和面向推理的文本分析，专门用于电信欺诈分析。数据集通过三种策略构建：隐私保护的文本真实样本生成、基于大型语言模型的语义增强自我指令采样和多代理对抗合成。数据集包含经过严格处理的语音文本对，以及针对欺诈推理的详细注释。此外，还构建了标准化评估基准TeleAntiFraud-Bench，以促进电信欺诈检测任务的模型性能的系统测试。本文建立了多模态反欺诈研究的基础框架，同时解决了数据隐私和场景多样性等关键挑战。</p>
<p><strong>关键见解</strong></p>
<ol>
<li>电信欺诈检测面临缺乏高质量多模态训练数据的挑战。</li>
<li>介绍了首个开放源的多模态音频文本数据集TeleAntiFraud-28k，专门用于电信欺诈分析。</li>
<li>数据集通过隐私保护的文本真实样本生成、语义增强和自我指令采样、多代理对抗合成三种策略构建。</li>
<li>数据集包含经过处理的语音文本对和详细的欺诈注释。</li>
<li>构建了标准化评估基准TeleAntiFraud-Bench，促进电信欺诈检测任务的模型性能的系统测试。</li>
<li>开放源的数据处理框架使得社区能够进行数据集扩展。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.24115">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-e04604273ac37876ed94a141c9dfb05a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-4ab7ba5582a063e68f5fe266c9f2818e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-2ea373be85c803e7f199329c6e1006ce.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-82a5dba04352ae797e591d0d181ab5d8.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="SupertonicTTS-Towards-Highly-Scalable-and-Efficient-Text-to-Speech-System"><a href="#SupertonicTTS-Towards-Highly-Scalable-and-Efficient-Text-to-Speech-System" class="headerlink" title="SupertonicTTS: Towards Highly Scalable and Efficient Text-to-Speech   System"></a>SupertonicTTS: Towards Highly Scalable and Efficient Text-to-Speech   System</h2><p><strong>Authors:Hyeongju Kim, Jinhyeok Yang, Yechan Yu, Seunghun Ji, Jacob Morton, Frederik Bous, Joon Byun, Juheon Lee</strong></p>
<p>We present a novel text-to-speech (TTS) system, namely SupertonicTTS, for improved scalability and efficiency in speech synthesis. SupertonicTTS is comprised of three components: a speech autoencoder for continuous latent representation, a text-to-latent module leveraging flow-matching for text-to-latent mapping, and an utterance-level duration predictor. To enable a lightweight architecture, we employ a low-dimensional latent space, temporal compression of latents, and ConvNeXt blocks. We further simplify the TTS pipeline by operating directly on raw character-level text and employing cross-attention for text-speech alignment, thus eliminating the need for grapheme-to-phoneme (G2P) modules and external aligners. In addition, we introduce context-sharing batch expansion that accelerates loss convergence and stabilizes text-speech alignment. Experimental results demonstrate that SupertonicTTS achieves competitive performance while significantly reducing architectural complexity and computational overhead compared to contemporary TTS models. Audio samples demonstrating the capabilities of SupertonicTTS are available at: <a target="_blank" rel="noopener" href="https://supertonictts.github.io/">https://supertonictts.github.io/</a>. </p>
<blockquote>
<p>我们提出了一种新型文本到语音（TTS）系统，即SupertonicTTS，以提高语音合成的可扩展性和效率。SupertonicTTS由三个组件组成：用于连续潜在表示的语音自动编码器，利用流匹配进行文本到潜在映射的文本到潜在模块，以及句子级别的持续时间预测器。为了实现轻量级架构，我们采用了低维潜在空间、潜在时间的压缩和ConvNeXt块。我们通过对原始字符级文本进行直接操作，并采用跨注意力进行文本语音对齐，从而简化了TTS管道，从而消除了对字母到音素（G2P）模块和外部对齐器的需求。此外，我们引入了上下文共享批量扩展，这加速了损失收敛并稳定了文本语音对齐。实验结果表明，SupertonicTTS在降低架构复杂性和计算开销的同时，与当代TTS模型相比具有竞争力。有关展示SupertonicTTS功能的音频样本，请访问：<a target="_blank" rel="noopener" href="https://supertonictts.github.io/%E3%80%82">https://supertonictts.github.io/。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.23108v1">PDF</a> 19 pages, preprint</p>
<p><strong>Summary</strong><br>     提出一种新型文本到语音（TTS）系统——SupertonicTTS，旨在提高语音合成的可扩展性和效率。该系统包含三个组件：用于连续潜在表示的语音自动编码器、利用流匹配进行文本到潜在映射的文本到潜在模块，以及句子级别的持续时间预测器。通过采用低维潜在空间、潜在时态压缩和ConvNeXt块，实现了轻量化架构。进一步简化TTS管道，直接在字符级文本上操作，采用跨注意力进行文本语音对齐，从而消除了对字母到语音（G2P）模块和外部对齐器的需求。此外，还引入了上下文共享批量扩展，以加速损失收敛并稳定文本语音对齐。实验结果表明，SupertonicTTS在降低架构复杂性和计算开销的同时，实现了与当代TTS模型相当的性能。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>SupertonicTTS是一个新型的文本到语音（TTS）系统，旨在提高语音合成的可扩展性和效率。</li>
<li>系统包含三个主要组件：语音自动编码器、文本到潜在模块和持续时间预测器。</li>
<li>通过采用低维潜在空间、潜在时态压缩和ConvNeXt块，实现了轻量化架构。</li>
<li>直接在字符级文本上操作，采用跨注意力对齐，简化了TTS管道，消除了对G2P模块和外部对齐器的需求。</li>
<li>引入了上下文共享批量扩展，以加速损失收敛并稳定文本语音对齐。</li>
<li>实验结果表明，SupertonicTTS在降低计算开销的同时，实现了与当代TTS模型相当的性能。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.23108">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-fee073788125b542056d93c1e00317cf.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4aa87ee10749829c8848a685c2248f4c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e33ecec007f9e4564e4a174ccd351b45.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Dual-Audio-Centric-Modality-Coupling-for-Talking-Head-Generation"><a href="#Dual-Audio-Centric-Modality-Coupling-for-Talking-Head-Generation" class="headerlink" title="Dual Audio-Centric Modality Coupling for Talking Head Generation"></a>Dual Audio-Centric Modality Coupling for Talking Head Generation</h2><p><strong>Authors:Ao Fu, Ziqi Ni, Yi Zhou</strong></p>
<p>The generation of audio-driven talking head videos is a key challenge in computer vision and graphics, with applications in virtual avatars and digital media. Traditional approaches often struggle with capturing the complex interaction between audio and facial dynamics, leading to lip synchronization and visual quality issues. In this paper, we propose a novel NeRF-based framework, Dual Audio-Centric Modality Coupling (DAMC), which effectively integrates content and dynamic features from audio inputs. By leveraging a dual encoder structure, DAMC captures semantic content through the Content-Aware Encoder and ensures precise visual synchronization through the Dynamic-Sync Encoder. These features are fused using a Cross-Synchronized Fusion Module (CSFM), enhancing content representation and lip synchronization. Extensive experiments show that our method outperforms existing state-of-the-art approaches in key metrics such as lip synchronization accuracy and image quality, demonstrating robust generalization across various audio inputs, including synthetic speech from text-to-speech (TTS) systems. Our results provide a promising solution for high-quality, audio-driven talking head generation and present a scalable approach for creating realistic talking heads. </p>
<blockquote>
<p>音频驱动的说话人头部的视频生成是计算机视觉和图形领域的一个关键挑战，在虚拟化身和数字媒体中有广泛的应用。传统的方法在捕捉音频和面部动态之间的复杂交互时往往会遇到困难，从而导致唇部同步和视觉质量问题。在本文中，我们提出了一种基于NeRF的新型框架——双音频中心模态耦合（DAMC），它能够有效地整合音频输入的内容和动态特征。通过利用双编码器结构，DAMC通过内容感知编码器捕获语义内容，并通过动态同步编码器确保精确的视觉同步。这些特征使用跨同步融合模块（CSFM）进行融合，增强了内容表示和唇部同步。大量实验表明，我们的方法在唇同步准确性和图像质量等关键指标上优于现有的最先进的方法，证明了在各种音频输入上的稳健泛化能力，包括来自文本到语音（TTS）系统的合成语音。我们的结果提供了高质量的音频驱动的说话人头部的生成的一种有前途的解决方案，并提出了一种创建逼真说话头部的方法。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.22728v1">PDF</a> 9 pages, 4 figures</p>
<p><strong>Summary</strong></p>
<p>本文提出了一种基于NeRF的双音频中心模态耦合（DAMC）框架，用于生成音频驱动的说话人头视频。该框架通过内容感知编码器和动态同步编码器，有效整合音频输入的内容与动态特征，提高内容表达和唇同步精度。实验结果证明，该方法在唇同步准确性和图像质量等方面优于现有先进技术，为高质量音频驱动的说话人头生成提供了有前途的解决方案。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>本文解决了计算机视觉和图形学中的音频驱动说话人头视频生成的关键挑战。</li>
<li>提出了基于NeRF的双音频中心模态耦合（DAMC）框架，有效整合音频输入的内容与动态特征。</li>
<li>通过内容感知编码器和动态同步编码器，DAMC提高了内容表达和唇同步精度。</li>
<li>框架包含交叉同步融合模块（CSFM），用于增强内容表示和唇同步。</li>
<li>实验证明，该方法在唇同步准确性和图像质量等方面优于现有技术。</li>
<li>该方法能够处理各种音频输入，包括文本到语音（TTS）系统生成的合成语音。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.22728">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-2b2c7d49850f18f4d83bef5aebdce69d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f14fbcc16baec813cf008cbb3379f343.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d792a51b6c87be14443f6a1fbc972941.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-df38b1b97f3c37aa076c7f6fdd8e6fd1.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-2563f0e3ff5ac02b7aacf1380dc9a5c9.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-10b2b6a0f229fd4764b744d91119afd6.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="DeepAudio-V1-Towards-Multi-Modal-Multi-Stage-End-to-End-Video-to-Speech-and-Audio-Generation"><a href="#DeepAudio-V1-Towards-Multi-Modal-Multi-Stage-End-to-End-Video-to-Speech-and-Audio-Generation" class="headerlink" title="DeepAudio-V1:Towards Multi-Modal Multi-Stage End-to-End Video to Speech   and Audio Generation"></a>DeepAudio-V1:Towards Multi-Modal Multi-Stage End-to-End Video to Speech   and Audio Generation</h2><p><strong>Authors:Haomin Zhang, Chang Liu, Junjie Zheng, Zihao Chen, Chaofan Ding, Xinhan Di</strong></p>
<p>Currently, high-quality, synchronized audio is synthesized using various multi-modal joint learning frameworks, leveraging video and optional text inputs. In the video-to-audio benchmarks, video-to-audio quality, semantic alignment, and audio-visual synchronization are effectively achieved. However, in real-world scenarios, speech and audio often coexist in videos simultaneously, and the end-to-end generation of synchronous speech and audio given video and text conditions are not well studied. Therefore, we propose an end-to-end multi-modal generation framework that simultaneously produces speech and audio based on video and text conditions. Furthermore, the advantages of video-to-audio (V2A) models for generating speech from videos remain unclear. The proposed framework, DeepAudio, consists of a video-to-audio (V2A) module, a text-to-speech (TTS) module, and a dynamic mixture of modality fusion (MoF) module. In the evaluation, the proposed end-to-end framework achieves state-of-the-art performance on the video-audio benchmark, video-speech benchmark, and text-speech benchmark. In detail, our framework achieves comparable results in the comparison with state-of-the-art models for the video-audio and text-speech benchmarks, and surpassing state-of-the-art models in the video-speech benchmark, with WER 16.57% to 3.15% (+80.99%), SPK-SIM 78.30% to 89.38% (+14.15%), EMO-SIM 66.24% to 75.56% (+14.07%), MCD 8.59 to 7.98 (+7.10%), MCD SL 11.05 to 9.40 (+14.93%) across a variety of dubbing settings. </p>
<blockquote>
<p>目前，高质量的同步音频是使用各种多模态联合学习框架进行合成的，这些框架利用视频和可选的文本输入。在视频到音频的基准测试中，视频到音频的质量、语义对齐和视听同步都得到了有效实现。然而，在真实场景中，语音和音频经常同时存在于视频中，对于给定视频和文本条件的端到端的同步语音和音频生成研究还不够充分。因此，我们提出了一种端到端的多模态生成框架，该框架可以同时根据视频和文本条件生成语音和音频。此外，视频到音频（V2A）模型在生成视频语音的优势尚不清楚。所提出的框架DeepAudio由视频到音频（V2A）模块、文本到语音（TTS）模块和动态模态融合（MoF）模块组成。在评估中，所提出端到端框架在视频音频基准测试、视频语音基准测试和文本语音基准测试上均达到了最先进的性能。具体来说，我们的框架在与视频音频和文本语音基准测试的最先进模型的比较中取得了相当的结果，并在视频语音基准测试中超越了最先进的模型，其中字词错误率从16.57%降低到3.15%（+80.99%），语音相似性从78.30%提高到89.38%（+14.15%），情绪相似性从66.24%提高到75.56%（+14.07%），MCD从8.59降至7.98（+7.10%），MCD SL从11.05降至9.40（+14.93%），跨越了各种配音设置。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.22265v1">PDF</a> 11 pages, 5 figures</p>
<p><strong>Summary</strong>:<br>在深层次的多媒体联合学习框架下，利用视频和文本输入生成高质量同步音频是当前研究的热点。针对视频中的语音和音频同时生成的问题，提出了一种端对端的多模式生成框架，该框架能基于视频和文本条件同时产生语音和音频。此外，该框架还包括视频到音频模块、文本到语音模块以及动态模态融合模块，并在视频音频、视频语音和文本语音的基准测试中取得了最先进的性能。</p>
<p><strong>Key Takeaways</strong>:</p>
<ol>
<li>当前高质量同步音频的合成采用多模态联合学习框架，利用视频和可选文本输入。</li>
<li>视频中的语音和音频在现实中常同时存在，端对端的同步语音和音频生成研究不足。</li>
<li>提出了一种端对端的多模式生成框架，能基于视频和文本条件同时产生语音和音频。</li>
<li>框架包括视频到音频模块、文本到语音模块以及动态模态融合模块。</li>
<li>该框架在视频音频、视频语音和文本语音的基准测试中实现了最先进的性能。</li>
<li>与其他模型相比，该框架在视频语音基准测试中表现尤为突出。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.22265">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-e745f63970742adbd3d5084a59108c61.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-17b3820bbce588d949bbce154145bbc1.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b79bfc85872940a52048ed7dbc2f52a7.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="F-INR-Functional-Tensor-Decomposition-for-Implicit-Neural-Representations"><a href="#F-INR-Functional-Tensor-Decomposition-for-Implicit-Neural-Representations" class="headerlink" title="F-INR: Functional Tensor Decomposition for Implicit Neural   Representations"></a>F-INR: Functional Tensor Decomposition for Implicit Neural   Representations</h2><p><strong>Authors:Sai Karthikeya Vemuri, Tim Büchner, Joachim Denzler</strong></p>
<p>Implicit Neural Representation (INR) has emerged as a powerful tool for encoding discrete signals into continuous, differentiable functions using neural networks. However, these models often have an unfortunate reliance on monolithic architectures to represent high-dimensional data, leading to prohibitive computational costs as dimensionality grows. We propose F-INR, a framework that reformulates INR learning through functional tensor decomposition, breaking down high-dimensional tasks into lightweight, axis-specific sub-networks. Each sub-network learns a low-dimensional data component (e.g., spatial or temporal). Then, we combine these components via tensor operations, reducing forward pass complexity while improving accuracy through specialized learning. F-INR is modular and, therefore, architecture-agnostic, compatible with MLPs, SIREN, WIRE, or other state-of-the-art INR architecture. It is also decomposition-agnostic, supporting CP, TT, and Tucker modes with user-defined rank for speed-accuracy control. In our experiments, F-INR trains $100\times$ faster than existing approaches on video tasks while achieving higher fidelity (+3.4 dB PSNR). Similar gains hold for image compression, physics simulations, and 3D geometry reconstruction. Through this, F-INR offers a new scalable, flexible solution for high-dimensional signal modeling. </p>
<blockquote>
<p>隐式神经网络表示（INR）已经成为一种强大的工具，能够利用神经网络将离散信号编码为连续、可微分的函数。然而，这些模型常常依赖单一的架构来表示高维数据，随着维度的增长，计算成本也随之增加。我们提出了F-INR框架，它通过函数张量分解重新构建了INR学习，将高维任务分解为轻量、针对轴的子网络。每个子网络学习低维数据组件（例如，空间或时间）。然后，我们通过张量运算将这些组件组合起来，降低了前向传播的复杂性，同时通过专项学习提高了准确性。F-INR具有模块化特性，因此它是与架构无关的，可以与MLP、SIREN、WIRE或其他最先进的INR架构兼容。它也是与分解方式无关的，支持CP、TT和Tucker模式，用户定义的等级可用于控制速度和准确性。在我们的实验中，F-INR在视频任务上的训练速度是现有方法的100倍，同时实现了更高的保真度（+3.4分贝PSNR）。类似的增益在图像压缩、物理模拟和3D几何重建中同样存在。因此，F-INR为高性能信号建模提供了新的可扩展和灵活解决方案。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.21507v1">PDF</a> 26 pages, 33 figures, 12 tables</p>
<p><strong>Summary</strong></p>
<p>基于隐神经表示（INR）的强大功能，提出一种新型框架F-INR，通过功能张量分解重构INR学习。它将高维任务分解为轻量级的、针对轴的子网络，每个子网络学习低维数据组件（如空间或时间）。通过张量操作组合这些组件，降低前向传播复杂性，同时通过专项学习提高准确性。F-INR具有模块化、通用性和可伸缩性，与多种最新INR架构兼容，并支持多种分解模式。实验表明，F-INR在视频任务上的训练速度是现有方法的100倍，同时实现了更高的保真度。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>F-INR利用隐神经表示（INR）的强大功能，针对高维数据表示进行了优化。</li>
<li>通过功能张量分解重构INR学习，分解高维任务为轴特定的子网络。</li>
<li>子网络学习低维数据组件（如空间或时间），然后通过张量操作组合它们。</li>
<li>F-INR提高了准确性，同时降低了前向传播的复杂性。</li>
<li>F-INR具有模块化、通用性和可伸缩性，与多种最新INR架构兼容。</li>
<li>F-INR支持多种分解模式，包括CP、TT和Tucker模式，并提供用户定义的等级以实现速度和准确性的控制。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.21507">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-6ee9105b0361c08ef318683d8a16808e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7c1e5cbab226bb3e111af4c010451ed7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a13eec11f8fda88d7e9b8d338f708251.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-b6fb51be1cb821a52c031e1edebf53ce.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-a782cc9b06bbca4537087d00987f1851.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-307a18f3245bea30d1aae78b002fffa0.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Text-Driven-Voice-Conversion-via-Latent-State-Space-Modeling"><a href="#Text-Driven-Voice-Conversion-via-Latent-State-Space-Modeling" class="headerlink" title="Text-Driven Voice Conversion via Latent State-Space Modeling"></a>Text-Driven Voice Conversion via Latent State-Space Modeling</h2><p><strong>Authors:Wen Li, Sofia Martinez, Priyanka Shah</strong></p>
<p>Text-driven voice conversion allows customization of speaker characteristics and prosodic elements using textual descriptions. However, most existing methods rely heavily on direct text-to-speech training, limiting their flexibility in controlling nuanced style elements or timbral features. In this paper, we propose a novel \textbf{Latent State-Space} approach for text-driven voice conversion (\textbf{LSS-VC}). Our method treats each utterance as an evolving dynamical system in a continuous latent space. Drawing inspiration from mamba, which introduced a state-space model for efficient text-driven \emph{image} style transfer, we adapt a loosely related methodology for \emph{voice} style transformation. Specifically, we learn a voice latent manifold where style and content can be manipulated independently by textual style prompts. We propose an adaptive cross-modal fusion mechanism to inject style information into the voice latent representation, enabling interpretable and fine-grained control over speaker identity, speaking rate, and emphasis. Extensive experiments show that our approach significantly outperforms recent baselines in both subjective and objective quality metrics, while offering smoother transitions between styles, reduced artifacts, and more precise text-based style control. </p>
<blockquote>
<p>文本驱动的声音转换允许使用文本描述来定制说话人的特性和韵律元素。然而，大多数现有方法严重依赖于直接的文本到语音训练，这在控制微妙的风格元素或音色特征方面的灵活性有限。在本文中，我们提出了一种新型的<strong>潜在状态空间</strong>方法，用于文本驱动的声音转换（<strong>LSS-VC</strong>）。我们的方法将每个语句视为一个不断变化的潜在空间中的动态系统。我们从引入了状态空间模型的mamba中汲取灵感，用于高效的文本驱动图像风格转换，我们采用了一种松散相关的方法来进行声音风格转换。具体来说，我们学习一个可以独立操纵风格和内容的语音潜在流形，通过文本风格提示来实现。我们提出了一种自适应的跨模态融合机制，将风格信息注入语音潜在表示中，实现对说话人身份、语速和重点的可解释和精细控制。大量实验表明，我们的方法在主观和客观质量指标上都显著优于最近的基线方法，同时提供了更平滑的风格过渡、减少了伪影，并提供了更精确的文本基于风格控制。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.20999v1">PDF</a> </p>
<p><strong>Summary</strong><br>文本驱动的声音转换方法通过使用文本描述定制说话人的特性和韵律元素。然而，大多数现有方法过于依赖直接的文本到语音训练，限制了它们在控制微妙的风格元素或音色特征方面的灵活性。本文提出了一种新颖的潜在状态空间方法用于文本驱动的声音转换（LSS-VC）。我们的方法将每个话语视为一个在不断变化的潜在空间中的动态系统。我们从为高效的文本驱动图像风格转移引入状态空间模型的mamba中汲取灵感，并适应了一种与声音风格转换松散相关的方法论。具体来说，我们学习了一个可以独立操作风格和内容的语音潜在流形，通过文本风格提示来操纵它们。我们提出了一种自适应的跨模态融合机制，将风格信息注入语音潜在表示中，实现对说话人身份、语速和强调的直观和精细控制。大量实验表明，我们的方法在主观和客观质量指标上都显著优于最新基线，同时提供了更平滑的风格过渡、减少了伪迹，并且基于文本的样式控制更为精确。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>文本驱动的声音转换能够利用文本描述定制说话人的特性和韵律元素。</li>
<li>现有方法过于依赖直接的文本到语音训练，缺乏灵活性。</li>
<li>LSS-VC方法将每个话语视为一个在潜在空间中的动态系统。</li>
<li>从mamba中汲取灵感，引入了状态空间模型用于声音转换。</li>
<li>通过学习语音潜在流形，实现风格和内容的独立操作。</li>
<li>提出的自适应跨模态融合机制能够注入风格信息到语音表示中。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.20999">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-2cea6677e118f6282f5504c4c696fa03.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-4dd01a165128fd70fd5cd0ce86dd5f14.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c7e8f263843248ad96418f6eb6716f9e.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Video-T1-Test-Time-Scaling-for-Video-Generation"><a href="#Video-T1-Test-Time-Scaling-for-Video-Generation" class="headerlink" title="Video-T1: Test-Time Scaling for Video Generation"></a>Video-T1: Test-Time Scaling for Video Generation</h2><p><strong>Authors:Fangfu Liu, Hanyang Wang, Yimo Cai, Kaiyan Zhang, Xiaohang Zhan, Yueqi Duan</strong></p>
<p>With the scale capability of increasing training data, model size, and computational cost, video generation has achieved impressive results in digital creation, enabling users to express creativity across various domains. Recently, researchers in Large Language Models (LLMs) have expanded the scaling to test-time, which can significantly improve LLM performance by using more inference-time computation. Instead of scaling up video foundation models through expensive training costs, we explore the power of Test-Time Scaling (TTS) in video generation, aiming to answer the question: if a video generation model is allowed to use non-trivial amount of inference-time compute, how much can it improve generation quality given a challenging text prompt. In this work, we reinterpret the test-time scaling of video generation as a searching problem to sample better trajectories from Gaussian noise space to the target video distribution. Specifically, we build the search space with test-time verifiers to provide feedback and heuristic algorithms to guide searching process. Given a text prompt, we first explore an intuitive linear search strategy by increasing noise candidates at inference time. As full-step denoising all frames simultaneously requires heavy test-time computation costs, we further design a more efficient TTS method for video generation called Tree-of-Frames (ToF) that adaptively expands and prunes video branches in an autoregressive manner. Extensive experiments on text-conditioned video generation benchmarks demonstrate that increasing test-time compute consistently leads to significant improvements in the quality of videos. Project page: <a target="_blank" rel="noopener" href="https://liuff19.github.io/Video-T1">https://liuff19.github.io/Video-T1</a> </p>
<blockquote>
<p>随着训练数据、模型规模和计算成本的规模能力不断提升，视频生成在数字创作中取得了令人印象深刻的结果，使用户能够在各种领域表达创造力。最近，大型语言模型（LLM）的研究人员将规模扩展到了测试阶段，通过增加推理时间计算来显著提高LLM的性能。我们并不通过昂贵的训练成本来扩大视频基础模型的规模，而是探索视频生成中测试时间缩放（TTS）的威力，旨在回答以下问题：如果视频生成模型被允许使用非微不足道的推理时间计算量，那么在具有挑战性的文本提示下，它能在多大程度上提高生成视频的质量。在这项工作中，我们将视频生成的测试时间缩放重新解释为搜索问题，从高斯噪声空间采样更好的轨迹以接近目标视频分布。具体来说，我们构建了一个搜索空间，利用测试时间验证器提供反馈和启发式算法来指导搜索过程。给定一个文本提示，我们首先探索一种直观的线性搜索策略，通过在推理时间增加噪声候选者来进行。由于同时完全去噪所有帧需要大量的测试时间计算成本，我们进一步为视频生成设计了一种更有效的TTS方法，称为“帧树”（ToF），该方法以自回归的方式自适应地扩展和修剪视频分支。在文本条件视频生成基准测试的大量实验表明，增加测试时间计算量始终有助于提高视频质量。项目页面：<a target="_blank" rel="noopener" href="https://liuff19.github.io/Video-T1">https://liuff19.github.io/Video-T1</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.18942v2">PDF</a> Project page: <a target="_blank" rel="noopener" href="https://liuff19.github.io/Video-T1">https://liuff19.github.io/Video-T1</a></p>
<p><strong>Summary</strong><br>     视频生成领域通过扩大训练数据、模型规模和计算成本，实现了令人印象深刻的成果。近期，大型语言模型（LLM）的研究者将规模扩展至测试阶段，通过使用更多的推理时间计算来显著提高LLM性能。本研究探索了视频生成中的测试时间缩放（TTS）能力，将视频生成的测试时间缩放重新解释为搜索问题，从高斯噪声空间采样更好的轨迹来逼近目标视频分布。本研究设计了高效的TTS方法——Tree-of-Frames（ToF），在给定文本提示的情况下，以自适应的方式扩展和修剪视频分支。实验证明，增加测试时间的计算量可以显著提高视频质量。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>视频生成通过扩大训练数据、模型规模和计算成本取得了显著进展，使创意表达跨越多个领域成为可能。</li>
<li>大型语言模型（LLM）的测试时间缩放（TTS）可以提高模型性能。</li>
<li>研究者将视频生成的测试时间缩放解释为搜索问题，从高斯噪声空间采样轨迹以逼近目标视频分布。</li>
<li>通过构建测试时间验证器和启发式算法来指导搜索过程。</li>
<li>给定文本提示，本研究采用了一种直观的线性搜索策略，通过增加推理时间的噪声候选来提高视频质量。</li>
<li>由于同时去噪所有帧需要巨大的计算量，研究者进一步设计了一种更有效的视频生成TTS方法——Tree-of-Frames（ToF）。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.18942">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-4e41aa767734ab722920bc9dfcd1461d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-28be37f548465dcd999e72bc35a1c779.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2a96efe176a78a126c8be4a2df8ee355.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ac5a102c6d5c62082016c1b081527d95.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="MegaTTS-3-Sparse-Alignment-Enhanced-Latent-Diffusion-Transformer-for-Zero-Shot-Speech-Synthesis"><a href="#MegaTTS-3-Sparse-Alignment-Enhanced-Latent-Diffusion-Transformer-for-Zero-Shot-Speech-Synthesis" class="headerlink" title="MegaTTS 3: Sparse Alignment Enhanced Latent Diffusion Transformer for   Zero-Shot Speech Synthesis"></a>MegaTTS 3: Sparse Alignment Enhanced Latent Diffusion Transformer for   Zero-Shot Speech Synthesis</h2><p><strong>Authors:Ziyue Jiang, Yi Ren, Ruiqi Li, Shengpeng Ji, Boyang Zhang, Zhenhui Ye, Chen Zhang, Bai Jionghao, Xiaoda Yang, Jialong Zuo, Yu Zhang, Rui Liu, Xiang Yin, Zhou Zhao</strong></p>
<p>While recent zero-shot text-to-speech (TTS) models have significantly improved speech quality and expressiveness, mainstream systems still suffer from issues related to speech-text alignment modeling: 1) models without explicit speech-text alignment modeling exhibit less robustness, especially for hard sentences in practical applications; 2) predefined alignment-based models suffer from naturalness constraints of forced alignments. This paper introduces \textit{MegaTTS 3}, a TTS system featuring an innovative sparse alignment algorithm that guides the latent diffusion transformer (DiT). Specifically, we provide sparse alignment boundaries to MegaTTS 3 to reduce the difficulty of alignment without limiting the search space, thereby achieving high naturalness. Moreover, we employ a multi-condition classifier-free guidance strategy for accent intensity adjustment and adopt the piecewise rectified flow technique to accelerate the generation process. Experiments demonstrate that MegaTTS 3 achieves state-of-the-art zero-shot TTS speech quality and supports highly flexible control over accent intensity. Notably, our system can generate high-quality one-minute speech with only 8 sampling steps. Audio samples are available at <a target="_blank" rel="noopener" href="https://sditdemo.github.io/sditdemo/">https://sditdemo.github.io/sditdemo/</a>. </p>
<blockquote>
<p>近年来，零样本文本到语音（TTS）模型在语音质量和表现力方面取得了显著改进，但主流系统仍然面临与语音文本对齐建模相关的问题：1）没有明确的语音文本对齐建模的模型表现出较低的稳健性，特别是在实际应用中的难句；2）基于预定义对齐的模型受到强制对齐的自然性约束。本文介绍了<em>MegaTTS 3</em>，这是一个TTS系统，采用创新稀疏对齐算法，引导潜在扩散变压器（DiT）。具体来说，我们为MegaTTS 3提供稀疏对齐边界，以减少对齐难度而不限制搜索空间，从而实现高自然度。此外，我们采用多条件无分类指导策略进行口音强度调整，并采用分段整流流技术加快生成过程。实验表明，MegaTTS 3达到了最先进的零样本TTS语音质量，并支持对口音强度进行高度灵活的控制。值得注意的是，我们的系统只需8个采样步骤就能生成高质量的一分钟语音。音频样本可在<a target="_blank" rel="noopener" href="https://sditdemo.github.io/sditdemo/%E6%89%BE%E5%88%B0%E3%80%82">https://sditdemo.github.io/sditdemo/找到。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.18924v4">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文介绍了最新推出的MegaTTS 3文本转语音（TTS）系统，其特点在于采用创新稀疏对齐算法引导潜在扩散转换器（DiT）。该系统通过提供稀疏对齐边界，降低对齐难度且不限制搜索空间，从而实现高自然度。此外，它采用多条件无分类引导策略调整口音强度，并使用分段整流技术加速生成过程。实验证明，MegaTTS 3实现了最先进的零样本TTS语音质量，并支持高度灵活的口音强度控制。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>MegaTTS 3是一个新的文本转语音（TTS）系统，具有先进的稀疏对齐算法。</li>
<li>该系统通过提供稀疏对齐边界，降低对齐难度，同时保持搜索空间的高灵活性。</li>
<li>MegaTTS 3采用多条件无分类引导策略，用于调整口音强度。</li>
<li>系统采用分段整流技术来加速语音生成过程。</li>
<li>实验证明MegaTTS 3在零样本TTS语音质量方面达到最新水平。</li>
<li>系统支持高度灵活的口音强度控制。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.18924">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-6a4848ec4bc65716ed58ab74f09839dd.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f63c305d51b567c1b33e75f11ea31ced.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-799c5f97dff2642f57db63aacabdbaa7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d4dc21c79b3bf4b146770c255229fcba.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="VERSA-A-Versatile-Evaluation-Toolkit-for-Speech-Audio-and-Music"><a href="#VERSA-A-Versatile-Evaluation-Toolkit-for-Speech-Audio-and-Music" class="headerlink" title="VERSA: A Versatile Evaluation Toolkit for Speech, Audio, and Music"></a>VERSA: A Versatile Evaluation Toolkit for Speech, Audio, and Music</h2><p><strong>Authors:Jiatong Shi, Hye-jin Shim, Jinchuan Tian, Siddhant Arora, Haibin Wu, Darius Petermann, Jia Qi Yip, You Zhang, Yuxun Tang, Wangyou Zhang, Dareen Safar Alharthi, Yichen Huang, Koichi Saito, Jionghao Han, Yiwen Zhao, Chris Donahue, Shinji Watanabe</strong></p>
<p>In this work, we introduce VERSA, a unified and standardized evaluation toolkit designed for various speech, audio, and music signals. The toolkit features a Pythonic interface with flexible configuration and dependency control, making it user-friendly and efficient. With full installation, VERSA offers 65 metrics with 729 metric variations based on different configurations. These metrics encompass evaluations utilizing diverse external resources, including matching and non-matching reference audio, text transcriptions, and text captions. As a lightweight yet comprehensive toolkit, VERSA is versatile to support the evaluation of a wide range of downstream scenarios. To demonstrate its capabilities, this work highlights example use cases for VERSA, including audio coding, speech synthesis, speech enhancement, singing synthesis, and music generation. The toolkit is available at <a target="_blank" rel="noopener" href="https://github.com/wavlab-speech/versa">https://github.com/wavlab-speech/versa</a>. </p>
<blockquote>
<p>在这项工作中，我们介绍了VERSA，这是一个为语音、音频和音乐信号设计的统一、标准化的评估工具包。该工具包具有Python风格的接口，具有灵活的配置和依赖控制，使其友好且高效。完成安装后，VERSA提供了基于不同配置的65种指标和729种指标变体。这些指标包括利用多种外部资源的评估，包括匹配和非匹配的参考音频、文本转录和文本标题。作为一个轻便而全面的工具包，VERSA支持对各种下游场景的评估。为了证明其能力，这项工作重点介绍了VERSA的示例用例，包括音频编码、语音合成、语音增强、歌唱合成和音乐生成。该工具包可在<a target="_blank" rel="noopener" href="https://github.com/wavlab-speech/versa%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/wavlab-speech/versa找到。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.17667v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>VERSA是一个统一、标准化的语音、音频和音乐信号评估工具包，具有Pythonic接口、灵活的配置和依赖控制，提供多种评估指标，支持多种下游场景的评价。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>VERSA是一个针对语音、音频和音乐信号评估的工具包。</li>
<li>它提供了一个Pythonic接口，具有灵活的配置和依赖控制。</li>
<li>VERSA包含65种评估指标，以及基于不同配置的729种指标变化。</li>
<li>该工具包可以利用各种外部资源进行评估，包括匹配和非匹配的参考音频、文本转录和文本标题。</li>
<li>VERSA支持多种下游场景的评价，如音频编码、语音合成、语音增强、歌唱合成和音乐生成等。</li>
<li>VERSA在GitHub上可用，用户可以通过链接获取更多详细信息和示例用例。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.17667">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-49dd14a08aac38b6c02b55a3089ebb77.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ef56415eb65cfb8300e0a1913078f61c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7f8e0e799de48fe8975d415cbff636bd.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-11c60c703837d6d4aba282bfeb1cd3b0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6be0d11933d9ac99b6081aaa2e0804c5.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="Towards-Controllable-Speech-Synthesis-in-the-Era-of-Large-Language-Models-A-Survey"><a href="#Towards-Controllable-Speech-Synthesis-in-the-Era-of-Large-Language-Models-A-Survey" class="headerlink" title="Towards Controllable Speech Synthesis in the Era of Large Language   Models: A Survey"></a>Towards Controllable Speech Synthesis in the Era of Large Language   Models: A Survey</h2><p><strong>Authors:Tianxin Xie, Yan Rong, Pengfei Zhang, Wenwu Wang, Li Liu</strong></p>
<p>Text-to-speech (TTS), also known as speech synthesis, is a prominent research area that aims to generate natural-sounding human speech from text. Recently, with the increasing industrial demand, TTS technologies have evolved beyond synthesizing human-like speech to enabling controllable speech generation. This includes fine-grained control over various attributes of synthesized speech such as emotion, prosody, timbre, and duration. In addition, advancements in deep learning, such as diffusion and large language models, have significantly enhanced controllable TTS over the past several years. In this work, we conduct a comprehensive survey of controllable TTS, covering approaches ranging from basic control techniques to methods utilizing natural language prompts, aiming to provide a clear understanding of the current state of research. We examine the general controllable TTS pipeline, challenges, model architectures, and control strategies, offering a comprehensive and clear taxonomy of existing methods. Additionally, we provide a detailed summary of datasets and evaluation metrics and shed some light on the applications and future directions of controllable TTS. To the best of our knowledge, this survey paper provides the first comprehensive review of emerging controllable TTS methods, which can serve as a beneficial resource for both academic researchers and industrial practitioners. </p>
<blockquote>
<p>文本转语音（TTS），也被称为语音合成，是一个突出的研究领域，旨在从文本生成听起来很自然的人类语音。最近，随着工业需求的增加，TTS技术已经超越了合成类似人类的语音，发展到了能够实现可控的语音生成。这包括合成语音的各种属性的精细控制，如情感、语调、音质和持续时间。此外，深度学习中的扩散和大型语言模型等技术的进步，在过去几年中显著增强了可控TTS的性能。在这项工作中，我们对可控TTS进行了全面的调查，涵盖了从基本控制技术到利用自然语言提示的方法等多种方法，旨在提供对当前研究状态的清晰理解。我们研究了通用的可控TTS管道、挑战、模型架构和控制策略，提供了现有方法的全面而清晰的分类。此外，我们还详细总结了数据集和评价指标，并对可控TTS的应用和未来发展方向进行了一些阐述。据我们所知，这篇综述论文首次全面回顾了新兴的可控TTS方法，对学术研究人员和工业从业者都有很大的参考价值。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.06602v2">PDF</a> A comprehensive survey on controllable TTS, 26 pages, 7 tables, 6   figures, 317 references. Under review</p>
<p><strong>Summary</strong><br>文本转语音（TTS）是生成自然语音的关键研究领域。近年来，随着工业需求的增长，TTS技术不仅合成人类语音，还能实现可控的语音生成。此技术涵盖对合成语音的各种属性的精细控制，如情感、语调、音质和时长等。深度学习中的扩散模型和大型语言模型等进步为可控TTS带来了显著的提升。本文全面综述了可控TTS的研究现状，从基本控制技巧到利用自然语言提示的方法都有所涉及，旨在为学术界和工业界提供有益的参考。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>TTS研究的目的是从文本生成自然语音。</li>
<li>近期TTS技术已从合成人类语音扩展到可控的语音生成。</li>
<li>可控TTS涵盖对合成语音的各种属性的精细控制，如情感、语调、音质和时长。</li>
<li>深度学习领域的进步，如扩散模型和大型语言模型，显著提升了可控TTS的性能。</li>
<li>本文全面概述了可控TTS的研究现状，包括通用管道、挑战、模型架构和控制策略。</li>
<li>文章还详细介绍了数据集和评估指标。</li>
<li>此综述旨在为学术界和工业界提供关于可控TTS的宝贵资源。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.06602">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-77b9d1b4adf952efd84a9a0f647cb16e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-6793a177689270ec51d736f2677c63a6.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-914ceb695b61c14f39efea85c78546a6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-53c531461a37a5313b4c529845b753e2.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="Continuous-Speech-Tokenizer-in-Text-To-Speech"><a href="#Continuous-Speech-Tokenizer-in-Text-To-Speech" class="headerlink" title="Continuous Speech Tokenizer in Text To Speech"></a>Continuous Speech Tokenizer in Text To Speech</h2><p><strong>Authors:Yixing Li, Ruobing Xie, Xingwu Sun, Yu Cheng, Zhanhui Kang</strong></p>
<p>The fusion of speech and language in the era of large language models has garnered significant attention. Discrete speech token is often utilized in text-to-speech tasks for speech compression and portability, which is convenient for joint training with text and have good compression efficiency. However, we found that the discrete speech tokenizer still suffers from information loss. Therefore, we propose a simple yet effective continuous speech tokenizer named Cont-SPT, and a text-to-speech model based on continuous speech tokens. Our results show that the speech language model based on the continuous speech tokenizer has better continuity and higher estimated Mean Opinion Scores (MoS). This enhancement is attributed to better information preservation rate of the continuous speech tokenizer across both low and high frequencies in the frequency domain. The code and resources for Cont-SPT can be found in <a target="_blank" rel="noopener" href="https://github.com/Yixing-Li/Continuous-Speech-Tokenizer">https://github.com/Yixing-Li/Continuous-Speech-Tokenizer</a> </p>
<blockquote>
<p>在大型语言模型时代，语音和语言的融合引起了人们的广泛关注。在文本到语音的任务中，离散语音标记通常用于语音压缩和便携性，这便于与文本进行联合训练并具有良好的压缩效率。然而，我们发现离散语音标记器仍然存在信息丢失的问题。因此，我们提出了一种简单有效的连续语音标记器（Cont-SPT）和基于连续语音标记的文本到语音模型。结果表明，基于连续语音标记器的语音语言模型具有更好的连续性和更高的预估平均意见得分（MoS）。这种改进归因于连续语音标记器在低频和高频频率域中具有更好的信息保持率。Cont-SPT的代码和资源可以在<a target="_blank" rel="noopener" href="https://github.com/Yixing-Li/Continuous-Speech-Tokenizer%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/Yixing-Li/Continuous-Speech-Tokenizer找到。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2410.17081v2">PDF</a> NAACL 2025 Findings Poster</p>
<p><strong>总结</strong><br>    在大型语言模型时代，语音和语言的融合引起了广泛关注。离散语音标记常用于文本到语音的任务，以实现语音压缩和便携性，便于与文本进行联合训练并具有高效的压缩性能。然而，研究发现离散语音分词器仍存在信息丢失的问题。因此，我们提出了一种简单有效的连续语音分词器Cont-SPT和基于连续语音标记的文本到语音模型。结果表明，基于连续语音分词器的语音语言模型具有更好的连续性和更高的预估平均意见得分（MoS）。这种改进归因于连续语音分词器在频域中低频和高频的更好信息保持率。Cont-SPT的代码和资源可在<a target="_blank" rel="noopener" href="https://github.com/Yixing-Li/Continuous-Speech-Tokenizer%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/Yixing-Li/Continuous-Speech-Tokenizer找到。</a></p>
<p><strong>关键见解</strong></p>
<ol>
<li>语音和语言融合在大型语言模型时代受到关注。</li>
<li>离散语音标记用于文本到语音任务以实现语音压缩和便携性。</li>
<li>离散语音分词器存在信息丢失的问题。</li>
<li>提出了连续语音分词器Cont-SPT。</li>
<li>基于连续语音标记的文本到语音模型表现出更好的连续性和更高的预估平均意见得分（MoS）。</li>
<li>连续语音分词器在频域中低频和高频的信息保持率更高。</li>
<li>Cont-SPT的代码和资源可通过特定链接获取。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2410.17081">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-a2f6d4234ae6ceb8eced875223976719.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ac1c91fd3522e27a7a088ee987913ee4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a486d8f9668db1bd0fc9ec5cec8797ae.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-7521a02d93a4930b4722ab2d9a8daff9.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="SF-Speech-Straightened-Flow-for-Zero-Shot-Voice-Clone"><a href="#SF-Speech-Straightened-Flow-for-Zero-Shot-Voice-Clone" class="headerlink" title="SF-Speech: Straightened Flow for Zero-Shot Voice Clone"></a>SF-Speech: Straightened Flow for Zero-Shot Voice Clone</h2><p><strong>Authors:Xuyuan Li, Zengqiang Shang, Hua Hua, Peiyang Shi, Chen Yang, Li Wang, Pengyuan Zhang</strong></p>
<p>Recently, neural ordinary differential equations (ODE) models trained with flow matching have achieved impressive performance on the zero-shot voice clone task. Nevertheless, postulating standard Gaussian noise as the initial distribution of ODE gives rise to numerous intersections within the fitted targets of flow matching, which presents challenges to model training and enhances the curvature of the learned generated trajectories. These curved trajectories restrict the capacity of ODE models for generating desirable samples with a few steps. This paper proposes SF-Speech, a novel voice clone model based on ODE and in-context learning. Unlike the previous works, SF-Speech adopts a lightweight multi-stage module to generate a more deterministic initial distribution for ODE. Without introducing any additional loss function, we effectively straighten the curved reverse trajectories of the ODE model by jointly training it with the proposed module. Experiment results on datasets of various scales show that SF-Speech outperforms the state-of-the-art zero-shot TTS methods and requires only a quarter of the solver steps, resulting in a generation speed approximately 3.7 times that of Voicebox and E2 TTS. Audio samples are available at the demo page\footnote{[Online] Available: <a target="_blank" rel="noopener" href="https://lixuyuan102.github.io/Demo/%7D">https://lixuyuan102.github.io/Demo/}</a>. </p>
<blockquote>
<p>最近，使用流匹配训练的神经常微分方程（ODE）模型在零样本语音克隆任务上取得了令人印象深刻的表现。然而，假设标准高斯噪声为ODE的初始分布会导致流匹配的拟合目标内部出现许多交集，这给模型训练带来了挑战，并增强了学习生成轨迹的曲率。这些弯曲的轨迹限制了ODE模型在几步内生成理想样本的能力。本文提出了SF-Speech，这是一种基于ODE和上下文学习的新型语音克隆模型。不同于以前的工作，SF-Speech采用轻量级的多阶段模块来为ODE生成更确定的初始分布。我们没有引入任何额外的损失函数，而是通过与所提出模块的共同训练，有效地校正了ODE模型的弯曲反向轨迹。在各种规模数据集上的实验结果表明，SF-Speech优于最先进的零样本TTS方法，并且仅需四分之一的求解器步骤，生成速度大约是Voicebox和E2 TTS的3.7倍。音频样本可在演示页面获得^[在线可用：<a target="_blank" rel="noopener" href="https://lixuyuan102.github.io/Demo/]%E3%80%82">https://lixuyuan102.github.io/Demo/]。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2410.12399v2">PDF</a> Accepted by IEEE Transactions on Audio, Speech and Language   Processing</p>
<p><strong>Summary</strong></p>
<p>神经网络常微分方程（ODE）模型在零样本语音克隆任务上取得了令人印象深刻的性能表现。然而，以标准高斯噪声作为ODE的初始分布会导致流匹配的目标拟合中出现许多交集，这给模型训练带来挑战，并增强了学习生成轨迹的曲率。本文提出了基于ODE和上下文学习的SF-Speech新型语音克隆模型。不同于以前的工作，SF-Speech采用轻量级的多阶段模块来为ODE生成更确定的初始分布。通过联合训练该模块，我们无需引入任何额外的损失函数即可有效地校正ODE模型的弯曲反向轨迹。在多种规模数据集上的实验结果表明，SF-Speech在零样本文本到语音转换方面的性能优于最新技术，且仅需四分之一的求解器步骤，生成速度大约是Voicebox和E2 TTS的3.7倍。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>神经网络常微分方程（ODE）模型在零样本语音克隆任务上具有卓越性能。</li>
<li>以标准高斯噪声作为ODE初始分布会导致流匹配中的目标拟合交集和轨迹曲率增强。</li>
<li>SF-Speech是一种新型的语音克隆模型，基于ODE和上下文学习。</li>
<li>SF-Speech采用轻量级多阶段模块生成更确定的ODE初始分布。</li>
<li>通过联合训练，SF-Speech能有效校正ODE模型的弯曲反向轨迹，无需额外的损失函数。</li>
<li>SF-Speech在多种数据集上的实验表现优于现有技术，且减少了求解器步骤和提高了生成速度。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2410.12399">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-3f54ed6c1c036704cf78583341190999.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c42ed00e1a01701ddea1b8235a065904.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-4806d14cab1e4eda416bd5e6b2c0f646.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-bda72226910d0d2c9b20440eecf366b2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-051e36ed8c714e7c8b117910852132c8.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="Enabling-Auditory-Large-Language-Models-for-Automatic-Speech-Quality-Evaluation"><a href="#Enabling-Auditory-Large-Language-Models-for-Automatic-Speech-Quality-Evaluation" class="headerlink" title="Enabling Auditory Large Language Models for Automatic Speech Quality   Evaluation"></a>Enabling Auditory Large Language Models for Automatic Speech Quality   Evaluation</h2><p><strong>Authors:Siyin Wang, Wenyi Yu, Yudong Yang, Changli Tang, Yixuan Li, Jimin Zhuang, Xianzhao Chen, Xiaohai Tian, Jun Zhang, Guangzhi Sun, Lu Lu, Yuxuan Wang, Chao Zhang</strong></p>
<p>Speech quality assessment typically requires evaluating audio from multiple aspects, such as mean opinion score (MOS) and speaker similarity (SIM) \etc., which can be challenging to cover using one small model designed for a single task. In this paper, we propose leveraging recently introduced auditory large language models (LLMs) for automatic speech quality assessment. By employing task-specific prompts, auditory LLMs are finetuned to predict MOS, SIM and A&#x2F;B testing results, which are commonly used for evaluating text-to-speech systems. Additionally, the finetuned auditory LLM is able to generate natural language descriptions assessing aspects like noisiness, distortion, discontinuity, and overall quality, providing more interpretable outputs. Extensive experiments have been performed on the NISQA, BVCC, SOMOS and VoxSim speech quality datasets, using open-source auditory LLMs such as SALMONN, Qwen-Audio, and Qwen2-Audio. For the natural language descriptions task, a commercial model Google Gemini 1.5 Pro is also evaluated. The results demonstrate that auditory LLMs achieve competitive performance compared to state-of-the-art task-specific small models in predicting MOS and SIM, while also delivering promising results in A&#x2F;B testing and natural language descriptions. Our data processing scripts and finetuned model checkpoints can be found at <a target="_blank" rel="noopener" href="https://github.com/bytedance/SALMONN">https://github.com/bytedance/SALMONN</a>. </p>
<blockquote>
<p>语音质量评估通常需要从多个方面对音频进行评估，如平均意见得分（MOS）和说话人相似性（SIM）等。使用专为单一任务设计的小型模型来覆盖这些方面可能具有挑战性。在本文中，我们提出利用最近引入的自动语音质量评估的听觉大型语言模型（LLM）。通过采用特定任务的提示，听觉LLM经过微调以预测MOS、SIM和A&#x2F;B测试结果，这些结果通常用于评估文本到语音系统。此外，经过微调后的听觉LLM能够生成自然语言描述，评估诸如噪音、失真、断续和总体质量等方面的内容，从而提供更可解释的输出。在NISQA、BVCC、SOMOS和VoxSim语音质量数据集上进行了大量实验，使用了开源的听觉LLM，如SALMONN、Qwen-Audio和Qwen2-Audio。对于自然语言描述任务，还评估了商业模型Google Gemini 1.5 Pro。结果表明，听觉LLM在预测MOS和SIM方面与最先进的特定任务小型模型相比具有竞争力，同时在A&#x2F;B测试和自然语言描述方面也表现出有希望的结果。我们的数据处理脚本和微调后的模型检查点可在<a target="_blank" rel="noopener" href="https://github.com/bytedance/SALMONN">https://github.com/bytedance/SALMONN</a>找到。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2409.16644v3">PDF</a> Accepted by ICASSP 2025</p>
<p><strong>Summary</strong><br>     本文提出利用最近引入的听觉大型语言模型（LLMs）进行自动语音质量评估。通过特定任务的提示，对听觉LLMs进行微调以预测平均意见得分（MOS）、相似性（SIM）等，并对语音质量数据集进行了大量实验验证。结果表明，听觉LLMs与最先进的特定任务小型模型相比具有竞争力，同时还在A&#x2F;B测试和自然语言描述方面表现出良好的性能。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>语音质量评估涉及多个方面，如平均意见得分（MOS）和说话人相似性（SIM）。</li>
<li>听觉大型语言模型（LLMs）被引入用于自动语音质量评估。</li>
<li>通过特定任务的提示，听觉LLMs可预测MOS、SIM以及A&#x2F;B测试结果。</li>
<li>听觉LLMs可以生成评估语音质量各方面的自然语言描述，如噪音、失真、断续和总体质量。</li>
<li>广泛实验表明，听觉LLMs在预测MOS和SIM方面表现出竞争力，并与先进的小型模型相比具有潜力。</li>
<li>听觉LLMs在A&#x2F;B测试以及自然语言描述方面也显示出良好的性能。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2409.16644">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-d49ecc20f884af03000e63af5f867530.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-0fc8eaf8d5cded4d6217ebce5f6378e3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4678db138eb072cda7b99840e513682e.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-cd3fd21a2b0a8f926cc644b8382796f8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f559231a629b22b8761c535d2998852b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-81fefc28d16dda07b1a120dad72ab9ce.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        文章作者:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        文章链接:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-04-03/TTS/">https://kedreamix.github.io/Talk2Paper/Paper/2025-04-03/TTS/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        版权声明:
                    </i>
                </span>
                <span class="reprint-info">
                    本博客所有文章除特別声明外，均采用
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    许可协议。转载请注明来源
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/TTS/">
                                    <span class="chip bg-color">TTS</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;上一篇</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-04-04/TTS/">
                    <div class="card-image">
                        
                        <img src="https://pica.zhimg.com/v2-ef56415eb65cfb8300e0a1913078f61c.jpg" class="responsive-img" alt="TTS">
                        
                        <span class="card-title">TTS</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            TTS 方向最新论文已更新，请持续关注 Update in 2025-04-04  LEP3 A High-Luminosity e+e- Higgs and ElectroweakFactory in the LHC   Tunnel
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-04-04
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/TTS/" class="post-category">
                                    TTS
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/TTS/">
                        <span class="chip bg-color">TTS</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                下一篇&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-04-04/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-6e7076a4f8670e51111dc0215d03e1a0.jpg" class="responsive-img" alt="医学图像">
                        
                        <span class="card-title">医学图像</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            医学图像 方向最新论文已更新，请持续关注 Update in 2025-04-04  BioAtt Anatomical Prior Driven Low-Dose CT Denoising
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-04-04
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/" class="post-category">
                                    医学图像
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                        <span class="chip bg-color">医学图像</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- 代码块功能依赖 -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- 代码语言 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- 代码块复制 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- 代码块收缩 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC 悬浮按钮. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* 修复文章卡片 div 的宽度. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // 切换TOC目录展开收缩的相关操作.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;站点总字数:&nbsp;<span
                        class="white-color">25156.8k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- 运行天数提醒. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // 区分是否有年份.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = '本站已运行 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                daysTip = '本站已運行 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = '本站已运行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = '本站已運行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="关注我的知乎: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">知</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- 搜索遮罩框 -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span>
            <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- 白天和黑夜主题 -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- 回到顶部按钮 -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // 只在桌面版网页启用特效
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- 雪花特效 -->
    

    <!-- 鼠标星星特效 -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--腾讯兔小巢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- 动态标签 -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Σ(っ °Д °;)っ诶，页面崩溃了嘛？", clearTimeout(st)) : (document.title = "φ(゜▽゜*)♪咦，又好了！", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
