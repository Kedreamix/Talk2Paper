<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="I2I Translation">
    <meta name="description" content="I2I Translation 方向最新论文已更新，请持续关注 Update in 2025-04-04  Leveraging Generalizability of Image-to-Image Translation for Enhanced   Adversarial Defense">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>I2I Translation | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loader页面消失采用渐隐的方式*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logo出现动画 */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//使用渐隐的方法淡出loading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//强制显示loading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">嘘~  正在从服务器偷取页面 . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>首页</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>标签</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>分类</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>归档</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="搜索" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="深色/浅色模式" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			首页
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			标签
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			分类
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			归档
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pica.zhimg.com/v2-56ba19ee851fd0b044661cc31ed9a0b2.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">I2I Translation</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- 文章内容详情 -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/I2I-Translation/">
                                <span class="chip bg-color">I2I Translation</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/I2I-Translation/" class="post-category">
                                I2I Translation
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp;
                    2025-04-04
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>更新日期:&nbsp;&nbsp;
                    2025-05-14
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>文章字数:&nbsp;&nbsp;
                    16k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>阅读时长:&nbsp;&nbsp;
                    65 分
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>阅读次数:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>⚠️ 以下所有内容总结都来自于 大语言模型的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p>
</blockquote>
<h1 id="2025-04-04-更新"><a href="#2025-04-04-更新" class="headerlink" title="2025-04-04 更新"></a>2025-04-04 更新</h1><h2 id="Leveraging-Generalizability-of-Image-to-Image-Translation-for-Enhanced-Adversarial-Defense"><a href="#Leveraging-Generalizability-of-Image-to-Image-Translation-for-Enhanced-Adversarial-Defense" class="headerlink" title="Leveraging Generalizability of Image-to-Image Translation for Enhanced   Adversarial Defense"></a>Leveraging Generalizability of Image-to-Image Translation for Enhanced   Adversarial Defense</h2><p><strong>Authors:Haibo Zhang, Zhihua Yao, Kouichi Sakurai, Takeshi Saitoh</strong></p>
<p>In the rapidly evolving field of artificial intelligence, machine learning emerges as a key technology characterized by its vast potential and inherent risks. The stability and reliability of these models are important, as they are frequent targets of security threats. Adversarial attacks, first rigorously defined by Ian Goodfellow et al. in 2013, highlight a critical vulnerability: they can trick machine learning models into making incorrect predictions by applying nearly invisible perturbations to images. Although many studies have focused on constructing sophisticated defensive mechanisms to mitigate such attacks, they often overlook the substantial time and computational costs of training and maintaining these models. Ideally, a defense method should be able to generalize across various, even unseen, adversarial attacks with minimal overhead. Building on our previous work on image-to-image translation-based defenses, this study introduces an improved model that incorporates residual blocks to enhance generalizability. The proposed method requires training only a single model, effectively defends against diverse attack types, and is well-transferable between different target models. Experiments show that our model can restore the classification accuracy from near zero to an average of 72% while maintaining competitive performance compared to state-of-the-art methods. </p>
<blockquote>
<p>在人工智能这个快速发展的领域里，机器学习作为一种具有巨大潜力和固有风险的关键技术而崭露头角。这些模型的稳定性和可靠性非常重要，因为它们经常受到安全威胁的侵袭。Ian Goodfellow等人在2013年首次严格定义的对抗性攻击，凸显了一个关键漏洞：他们可以通过对图像施加几乎不可见的扰动，欺骗机器学习模型做出错误的预测。尽管许多研究致力于构建复杂的防御机制来减轻这类攻击，但它们往往忽视了训练和维持这些模型的巨大时间和计算成本。理想的防御方法应该能够在各种甚至未知的对抗性攻击中具有通用性，并且具有最小的额外开销。本研究基于我们之前在基于图像到图像翻译的防御方面的工作，引入了一个改进模型，该模型结合了残差块以增强通用性。所提出的方法只需要训练一个单一模型，就能有效地防御多种攻击类型，并且在不同的目标模型之间具有良好的可迁移性。实验表明，我们的模型可以将分类准确率从接近零恢复到平均72%，同时与最新方法相比保持竞争力。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.01399v1">PDF</a> </p>
<p><strong>Summary</strong><br>     机器学习模型在人工智能的快速发展中展现出巨大的潜力和固有的风险。模型的稳定性和可靠性尤为重要，因为它们经常面临安全威胁。对抗性攻击是机器学习模型的关键漏洞之一，可以通过对图像施加几乎不可见的扰动来误导模型做出错误的预测。尽管许多研究致力于构建复杂的防御机制来减轻这些攻击，但它们往往忽略了训练和维持这些模型的巨大时间和计算成本。本研究基于我们在图像到图像翻译防御方面的工作，提出了一种改进的模型，该模型结合了残差块以增强通用性。所提出的方法只需要训练一个模型，可以有效地防御多种攻击类型，并且在不同的目标模型之间具有良好的可转移性。实验表明，我们的模型可以将分类准确率从接近于零恢复到平均72%，同时与最先进的方法相比保持竞争力。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>机器学习在人工智能领域展现出巨大潜力，但存在对抗性攻击的安全风险。</li>
<li>对抗性攻击能够通过几乎不可见的扰动误导机器学习模型做出错误预测。</li>
<li>许多防御机制的研究忽略了训练和维持防御模型的巨大时间和计算成本。</li>
<li>本研究基于图像到图像翻译防御工作，提出了一个改进的模型。</li>
<li>改进模型结合了残差块以增强通用性，能防御多种攻击类型，并在不同目标模型间具有良好的可转移性。</li>
<li>实验结果显示，该模型能将分类准确率从近乎零恢复到平均72%。</li>
<li>该模型的性能与当前最先进的方法相比具有竞争力。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.01399">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-879568d77047b4f188a94666dd0c6288.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-56ba19ee851fd0b044661cc31ed9a0b2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-793722898546799bbf07ae7bea65fae8.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="UniFault-A-Fault-Diagnosis-Foundation-Model-from-Bearing-Data"><a href="#UniFault-A-Fault-Diagnosis-Foundation-Model-from-Bearing-Data" class="headerlink" title="UniFault: A Fault Diagnosis Foundation Model from Bearing Data"></a>UniFault: A Fault Diagnosis Foundation Model from Bearing Data</h2><p><strong>Authors:Emadeldeen Eldele, Mohamed Ragab, Xu Qing,  Edward, Zhenghua Chen, Min Wu, Xiaoli Li, Jay Lee</strong></p>
<p>Machine fault diagnosis (FD) is a critical task for predictive maintenance, enabling early fault detection and preventing unexpected failures. Despite its importance, existing FD models are operation-specific with limited generalization across diverse datasets. Foundation models (FM) have demonstrated remarkable potential in both visual and language domains, achieving impressive generalization capabilities even with minimal data through few-shot or zero-shot learning. However, translating these advances to FD presents unique hurdles. Unlike the large-scale, cohesive datasets available for images and text, FD datasets are typically smaller and more heterogeneous, with significant variations in sampling frequencies and the number of channels across different systems and applications. This heterogeneity complicates the design of a universal architecture capable of effectively processing such diverse data while maintaining robust feature extraction and learning capabilities. In this paper, we introduce UniFault, a foundation model for fault diagnosis that systematically addresses these issues. Specifically, the model incorporates a comprehensive data harmonization pipeline featuring two key innovations. First, a unification scheme transforms multivariate inputs into standardized univariate sequences while retaining local inter-channel relationships. Second, a novel cross-domain temporal fusion strategy mitigates distribution shifts and enriches sample diversity and count, improving the model generalization across varying conditions. UniFault is pretrained on over 9 billion data points spanning diverse FD datasets, enabling superior few-shot performance. Extensive experiments on real-world FD datasets demonstrate that UniFault achieves SoTA performance, setting a new benchmark for fault diagnosis models and paving the way for more scalable and robust predictive maintenance solutions. </p>
<blockquote>
<p>机器故障诊断（FD）是预测性维护中的一项关键任务，能够实现早期故障检测并防止意外故障。尽管其重要性不言而喻，但现有的FD模型都是针对特定操作的，在多种数据集之间的泛化能力有限。基础模型（FM）在视觉和语言领域都表现出了显著的优势，即使在少量数据的情况下，也能通过小样本或零样本学习实现令人印象深刻的泛化能力。然而，将这些进展应用于FD却面临独特的障碍。与可用于图像和文本的规模庞大、连贯的数据集不同，FD数据集通常较小且更异质，不同系统和应用之间的采样频率和数据通道数量存在重大差异。这种异质性使得设计一个能够处理如此多样数据的同时保持稳健的特征提取和学习能力的通用架构变得复杂。在本文中，我们引入了UniFault，这是一个用于故障诊断的基础模型，系统地解决了这些问题。具体来说，该模型结合了一个全面的数据调和管道，包含两个关键的创新点。首先，一个统一方案将多元输入转换为标准化的单变量序列，同时保留局部通道间的关系。其次，一种新的跨域时间融合策略缓解了分布偏移问题并丰富了样本的多样性和数量，提高了模型在不同条件下的泛化能力。UniFault在跨越多种FD数据集的超过9亿个数据点上进行预训练，实现了卓越的小样本性能。在真实世界的FD数据集上的广泛实验表明，UniFault达到了最新的性能水平，为故障诊断模型设定了新的基准，并为更可扩展和稳健的预测性维护解决方案铺平了道路。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.01373v1">PDF</a> </p>
<p><strong>摘要</strong></p>
<p>机器故障诊断（FD）是预测性维护中的一项关键任务，能够实现对早期故障的及时检测，避免意外停机情况的出现。虽然故障诊断模型的发展非常重要，但目前存在大量的操作特定模型，这些模型的跨数据集泛化能力受限。基础模型（FM）在视觉和语言领域已经展现出令人瞩目的潜力，通过少量样本学习或零样本学习即可实现令人印象深刻的泛化能力。然而，将这一进展转化为故障诊断面临着独特的挑战。与可用于图像和文本的大规模、连贯数据集相比，故障诊断数据集通常更小、更异质，不同系统和应用之间的采样频率和数据通道数量存在显著差异。这种异质性使得设计一种能够处理此类多样数据同时保持稳健的特征提取和学习能力的通用架构变得复杂。本文介绍了用于故障诊断的基础模型UniFault，该模型系统地解决了这些问题。具体来说，该模型包含全面的数据调和管道，具有两大创新之处。首先，统一方案将多元输入转换为标准化的单变量序列，同时保留局部通道间关系。其次，一种新的跨域时间融合策略缓解了分布偏移问题并丰富了样本多样性和计数，提高了模型在不同条件下的泛化能力。UniFault在跨越多个故障诊断数据集超过9亿个数据点上进行预训练，实现了出色的少量样本性能。在真实世界的故障诊断数据集上进行的大量实验表明，UniFault达到了最先进的性能水平，为故障诊断模型设定了新的基准线，并为更可扩展和稳健的预测性维护解决方案铺平了道路。</p>
<p><strong>要点提炼</strong></p>
<ol>
<li>机器故障诊断（FD）是预测性维护中的核心任务，要求早期故障检测并避免意外停机。</li>
<li>当前FD模型操作特定性强，泛化能力受限。</li>
<li>基础模型（FM）在视觉和语言领域具有出色的泛化能力。</li>
<li>将FM应用于FD面临数据集小、异质性的挑战。</li>
<li>UniFault模型通过数据调和管道解决这些问题，包括输入数据的统一转换和跨域时间融合策略。</li>
<li>UniFault模型在大量FD数据集上进行预训练，实现优秀少量样本性能。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.01373">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-ebf09123c7ce6dcde9c49d0f45ea7984.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-cb2f867e8850b4fa95c5225151462878.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-da8a894b53c9c78b2a1e2f39065388cd.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Direction-Aware-Hybrid-Representation-Learning-for-3D-Hand-Pose-and-Shape-Estimation"><a href="#Direction-Aware-Hybrid-Representation-Learning-for-3D-Hand-Pose-and-Shape-Estimation" class="headerlink" title="Direction-Aware Hybrid Representation Learning for 3D Hand Pose and   Shape Estimation"></a>Direction-Aware Hybrid Representation Learning for 3D Hand Pose and   Shape Estimation</h2><p><strong>Authors:Shiyong Liu, Zhihao Li, Xiao Tang, Jianzhuang Liu</strong></p>
<p>Most model-based 3D hand pose and shape estimation methods directly regress the parametric model parameters from an image to obtain 3D joints under weak supervision. However, these methods involve solving a complex optimization problem with many local minima, making training difficult. To address this challenge, we propose learning direction-aware hybrid features (DaHyF) that fuse implicit image features and explicit 2D joint coordinate features. This fusion is enhanced by the pixel direction information in the camera coordinate system to estimate pose, shape, and camera viewpoint. Our method directly predicts 3D hand poses with DaHyF representation and reduces jittering during motion capture using prediction confidence based on contrastive learning. We evaluate our method on the FreiHAND dataset and show that it outperforms existing state-of-the-art methods by more than 33% in accuracy. DaHyF also achieves the top ranking on both the HO3Dv2 and HO3Dv3 leaderboards for the metric of Mean Joint Error (after scale and translation alignment). Compared to the second-best results, the largest improvement observed is 10%. We also demonstrate its effectiveness in real-time motion capture scenarios with hand position variability, occlusion, and motion blur. </p>
<blockquote>
<p>大多数基于模型的3D手势姿态和形状估计方法直接通过图像回归参数模型参数，以在弱监督下获得3D关节。然而，这些方法需要解决具有多个局部最小值的复杂优化问题，使得训练变得困难。为了应对这一挑战，我们提出了学习方向感知混合特征（DaHyF），该特征融合了隐式图像特征和显式的2D关节坐标特征。这种融合通过相机坐标系中的像素方向信息来增强，以估计姿态、形状和相机视点。我们的方法直接使用DaHyF表示预测3D手势姿态，并利用基于对比学习的预测置信度减少运动捕捉过程中的抖动。我们在FreiHAND数据集上评估了我们的方法，结果表明，在准确性方面，它比现有最先进的方法高出33%以上。DaHyF在HO3Dv2和HO3Dv3排行榜上的平均关节误差指标方面也获得了排名第一。与第二好的结果相比，观察到的最大改进是10%。我们还证明了它在具有手部位置变化、遮挡和运动模糊的实时运动捕捉场景中的有效性。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.01298v1">PDF</a> Accepted to CVPR 2025 workshop</p>
<p><strong>Summary</strong><br>手姿三维姿态及形状估算中，许多模型基于回归图像参数化模型参数来得到三维关节点，但存在训练困难的问题。为解决此问题，我们提出学习方向感知混合特征（DaHyF），融合隐性图像特征和显性二维关节坐标特征，并结合相机坐标系中的像素方向信息来估算姿态、形状和相机视角。该方法可直接预测三维手姿态，通过对比学习减少运动捕捉时的抖动。在FreiHAND数据集上的实验表明，其准确度高于现有先进技术超过33%，在HO3Dv2和HO3Dv3排行榜上的平均关节误差排名也位居前列。对实时运动捕捉场景下的手部位置变化、遮挡和运动模糊等情况进行了验证。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>提出学习方向感知混合特征（DaHyF）以融合隐性图像特征和显性二维关节坐标特征。</li>
<li>DaHyF结合像素方向信息在相机坐标系中估算姿态、形状和相机视角。</li>
<li>方法能减少运动捕捉时的抖动问题，增强预测的准确性。</li>
<li>在FreiHAND数据集上的准确度显著提高，准确率超越现有技术至少33%。</li>
<li>DaHyF在HO3Dv2和HO3Dv3排行榜上的平均关节误差排名名列前茅，相对第二好的结果最高提升了10%。</li>
<li>DaHyF适用于多种真实场景，包括手部位置变化、遮挡和运动模糊等情况。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.01298">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-c9d3c69d0ba4984ca90510dedc43e0e6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-413cd2fdc4ae6524fd5c6829490622a5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-54f1f1ae4405acc071da7fc23b96ad01.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8dc32e49673f5cac92e700ce91690395.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-52159398b69a3ded68affc4d62ee80d8.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="AnimeGamer-Infinite-Anime-Life-Simulation-with-Next-Game-State-Prediction"><a href="#AnimeGamer-Infinite-Anime-Life-Simulation-with-Next-Game-State-Prediction" class="headerlink" title="AnimeGamer: Infinite Anime Life Simulation with Next Game State   Prediction"></a>AnimeGamer: Infinite Anime Life Simulation with Next Game State   Prediction</h2><p><strong>Authors:Junhao Cheng, Yuying Ge, Yixiao Ge, Jing Liao, Ying Shan</strong></p>
<p>Recent advancements in image and video synthesis have opened up new promise in generative games. One particularly intriguing application is transforming characters from anime films into interactive, playable entities. This allows players to immerse themselves in the dynamic anime world as their favorite characters for life simulation through language instructions. Such games are defined as infinite game since they eliminate predetermined boundaries and fixed gameplay rules, where players can interact with the game world through open-ended language and experience ever-evolving storylines and environments. Recently, a pioneering approach for infinite anime life simulation employs large language models (LLMs) to translate multi-turn text dialogues into language instructions for image generation. However, it neglects historical visual context, leading to inconsistent gameplay. Furthermore, it only generates static images, failing to incorporate the dynamics necessary for an engaging gaming experience. In this work, we propose AnimeGamer, which is built upon Multimodal Large Language Models (MLLMs) to generate each game state, including dynamic animation shots that depict character movements and updates to character states, as illustrated in Figure 1. We introduce novel action-aware multimodal representations to represent animation shots, which can be decoded into high-quality video clips using a video diffusion model. By taking historical animation shot representations as context and predicting subsequent representations, AnimeGamer can generate games with contextual consistency and satisfactory dynamics. Extensive evaluations using both automated metrics and human evaluations demonstrate that AnimeGamer outperforms existing methods in various aspects of the gaming experience. Codes and checkpoints are available at <a target="_blank" rel="noopener" href="https://github.com/TencentARC/AnimeGamer">https://github.com/TencentARC/AnimeGamer</a>. </p>
<blockquote>
<p>最近图像和视频合成的进展为生成游戏带来了新的前景。一个特别吸引人的应用是将动漫电影中的角色转变为可互动的游戏实体。这允许玩家通过语言指令沉浸在动态的动漫世界中，扮演他们最喜欢的角色进行生活模拟。这类游戏被定义为无限游戏，因为它们消除了预定的边界和固定的游戏规则，玩家可以通过开放式的语言与游戏世界互动，体验不断演变的故事情节和环境。最近，一种用于无限动漫生活模拟的开创性方法采用大型语言模型（LLM）将多轮文本对话翻译为图像生成的语言指令。然而，它忽略了历史视觉上下文，导致游戏体验不一致。此外，它只能生成静态图像，无法融入动态元素，无法提供引人入胜的游戏体验。在这项工作中，我们提出了AnimeGamer，它基于多模态大型语言模型（MLLM）来生成每种游戏状态，包括描绘角色动作和状态更新的动态动画镜头，如图1所示。我们引入了新型的动作感知多模态表示法来表示动画镜头，可以使用视频扩散模型将其解码为高质量的视频片段。通过获取历史动画镜头表示作为上下文并预测随后的表示，AnimeGamer可以生成具有上下文一致性和令人满意的动力学特性的游戏。使用自动化指标和人类评估的广泛评估表明，AnimeGamer在游戏体验的各个方面都优于现有方法。代码和检查点可用于<a target="_blank" rel="noopener" href="https://github.com/TencentARC/AnimeGamer%E3%80%82">https://github.com/TencentARC/AnimeGamer。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.01014v1">PDF</a> Project released at: <a target="_blank" rel="noopener" href="https://howe125.github.io/AnimeGamer.github.io/">https://howe125.github.io/AnimeGamer.github.io/</a></p>
<p><strong>Summary</strong></p>
<p>本文介绍了最新图像和视频合成技术在生成游戏领域的应用，特别是在将动漫角色转化为可互动实体方面的应用。游戏玩家可以通过语言指令沉浸于动态的动漫世界，并扮演他们最喜欢的角色进行生活模拟。提出了一种基于多模态大型语言模型（MLLMs）构建的AnimeGamer系统，能够生成游戏状态，包括动态动画镜头和角色状态更新。通过引入新型的动作感知多模态表示方法，并结合视频扩散模型，生成的游戏具有上下文一致性和满意的动态效果。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>最新图像和视频合成技术为生成游戏领域带来新希望。</li>
<li>将动漫角色转化为可互动实体是一种引人入胜的应用。</li>
<li>玩家可通过语言指令沉浸于动漫世界，扮演喜欢的角色进行生活模拟。</li>
<li>提出了一种基于多模态大型语言模型（MLLMs）的AnimeGamer系统。</li>
<li>AnimeGamer能生成包括动态动画镜头的游戏状态。</li>
<li>通过引入动作感知多模态表示方法，结合视频扩散模型，实现游戏的上下文一致性和动态效果。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.01014">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-cd0ebf6fe3158be837dc673128def6b7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8fd29e104e47e0ff54d6807a15db67d9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-696e28d03c6550c4c5c82861b2ad2d4e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-27966b67be617eeb22bb1d6a7c5065e3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3f3546a6de895403e043b8e44e47447f.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-2cef586b07bbc87c8a676a1da28206c4.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="SCFANet-Style-Distribution-Constraint-Feature-Alignment-Network-For-Pathological-Staining-Translation"><a href="#SCFANet-Style-Distribution-Constraint-Feature-Alignment-Network-For-Pathological-Staining-Translation" class="headerlink" title="SCFANet: Style Distribution Constraint Feature Alignment Network For   Pathological Staining Translation"></a>SCFANet: Style Distribution Constraint Feature Alignment Network For   Pathological Staining Translation</h2><p><strong>Authors:Zetong Chen, Yuzhuo Chen, Hai Zhong, Xu Qiao</strong></p>
<p>Immunohistochemical (IHC) staining serves as a valuable technique for detecting specific antigens or proteins through antibody-mediated visualization. However, the IHC staining process is both time-consuming and costly. To address these limitations, the application of deep learning models for direct translation of cost-effective Hematoxylin and Eosin (H&amp;E) stained images into IHC stained images has emerged as an efficient solution. Nevertheless, the conversion from H&amp;E to IHC images presents significant challenges, primarily due to alignment discrepancies between image pairs and the inherent diversity in IHC staining style patterns. To overcome these challenges, we propose the Style Distribution Constraint Feature Alignment Network (SCFANet), which incorporates two innovative modules: the Style Distribution Constrainer (SDC) and Feature Alignment Learning (FAL). The SDC ensures consistency between the generated and target images’ style distributions while integrating cycle consistency loss to maintain structural consistency. To mitigate the complexity of direct image-to-image translation, the FAL module decomposes the end-to-end translation task into two subtasks: image reconstruction and feature alignment. Furthermore, we ensure pathological consistency between generated and target images by maintaining pathological pattern consistency and Optical Density (OD) uniformity. Extensive experiments conducted on the Breast Cancer Immunohistochemical (BCI) dataset demonstrate that our SCFANet model outperforms existing methods, achieving precise transformation of H&amp;E-stained images into their IHC-stained counterparts. The proposed approach not only addresses the technical challenges in H&amp;E to IHC image translation but also provides a robust framework for accurate and efficient stain conversion in pathological analysis. </p>
<blockquote>
<p>免疫组织化学（IHC）染色作为一种通过抗体介导的可视化检测特定抗原或蛋白质的有价值的技术。然而，IHC染色过程既耗时又成本高昂。为了解决这些局限性，应用深度学习模型将成本效益高的苏木精和伊红（H&amp;E）染色图像直接翻译成IHC染色图像已成为一种有效的解决方案。然而，从H&amp;E到IHC图像的转换存在重大挑战，主要是由于图像对之间的对齐差异和IHC染色风格模式的固有多样性。为了克服这些挑战，我们提出了风格分布约束特征对齐网络（SCFANet），它包含两个创新模块：风格分布约束器（SDC）和特征对齐学习（FAL）。SDC确保生成图像和目标图像之间风格分布的一致性，同时结合循环一致性损失以保持结构一致性。为了减轻直接图像到图像翻译的复杂性，FAL模块将端到端的翻译任务分解为两个子任务：图像重建和特征对齐。此外，我们通过保持病理模式一致性和光学密度（OD）均匀性，确保生成图像和目标图像之间的病理一致性。在乳腺癌免疫组织化学（BCI）数据集上进行的广泛实验表明，我们的SCFANet模型优于现有方法，实现了H&amp;E染色图像到IHC染色图像的精确转换。该方法不仅解决了H&amp;E到IHC图像转换的技术挑战，而且为病理分析中的准确高效染色转换提供了稳健的框架。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.00490v1">PDF</a> </p>
<p><strong>Summary</strong><br>IHC染色技术用于检测特定抗原或蛋白质，但过程耗时且成本高昂。为解决这些问题，研究者利用深度学习模型将经济高效的H&amp;E染色图像直接转换为IHC染色图像。然而，转换过程中存在对齐差异和IHC染色风格模式多样性等挑战。为此，提出SCFANet模型，包括SDC和FAL两个创新模块，确保风格分布和结构性一致性，并分解为图像重建和特征对齐两个子任务。在乳腺癌免疫组织化学数据集上的实验表明，SCFANet模型精确转换H&amp;E染色图像为IHC染色图像，提供稳健的框架进行病理分析中的染色转换。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>IHC染色是检测特定抗原或蛋白质的重要技术，但存在时间和成本问题。</li>
<li>深度学习模型可用于将H&amp;E染色图像转换为IHC染色图像，提高效率。</li>
<li>转换过程中面临图像对齐和IHC染色风格多样性的挑战。</li>
<li>SCFANet模型通过SDC和FAL模块克服这些挑战，确保风格分布和结构性一致性。</li>
<li>SCFANet将转换任务分解为图像重建和特征对齐两个子任务。</li>
<li>通过维持病理性模式一致性和光学密度均匀性，确保病理一致性。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.00490">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-a6c77991d1297560af1521e98891059e.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Multiview-Image-Based-Localization"><a href="#Multiview-Image-Based-Localization" class="headerlink" title="Multiview Image-Based Localization"></a>Multiview Image-Based Localization</h2><p><strong>Authors:Cameron Fiore, Hongyi Fan, Benjamin Kimia</strong></p>
<p>The image retrieval (IR) approach to image localization has distinct advantages to the 3D and the deep learning (DNN) approaches: it is seen-agnostic, simpler to implement and use, has no privacy issues, and is computationally efficient. The main drawback of this approach is relatively poor localization in both position and orientation of the query camera when compared to the competing approaches. This paper represents a hybrid approach that stores only image features in the database like some IR methods, but relies on a latent 3D reconstruction, like 3D methods but without retaining a 3D scene reconstruction. The approach is based on two ideas: {\em (i)} a novel proposal where query camera center estimation relies only on relative translation estimates but not relative rotation estimates through a decoupling of the two, and {\em (ii)} a shift from computing optimal pose from estimated relative pose to computing optimal pose from multiview correspondences, thus cutting out the &#96;&#96;middle-man’’. Our approach shows improved performance on the 7-Scenes and Cambridge Landmarks datasets while also improving on timing and memory footprint as compared to state-of-the-art. </p>
<blockquote>
<p>图像检索（IR）方法在图像定位方面具有与3D和深度学习（DNN）方法不同的明显优势：它不受视觉影响，更容易实施和使用，没有隐私问题，计算效率高。这种方法的主要缺点是与竞争方法相比，在查询相机的位置和方位定位方面相对较差。本文代表了一种混合方法，该方法仅将图像特征存储在数据库中，类似于某些IR方法，但依赖于潜在的3D重建，类似于3D方法，但不保留3D场景重建。该方法基于两个思想：（i）一种新型提案，其中查询相机中心估计仅依赖于相对平移估计，而不依赖于通过解耦两者得到的相对旋转估计；（ii）从通过估计的相对姿势计算最佳姿势转向通过多视图对应关系计算最佳姿势，从而省略了“中间人”。我们的方法在7场景和剑桥地标数据集上显示出性能改进，同时在时间和内存占用方面与最新技术相比也有所改进。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.23577v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>该文提出一种图像检索（IR）与深度学习和三维重建技术结合的混合方法，用于图像定位。该方法具有视觉无关性、易于实现和使用、无隐私问题和计算效率高等优点。其主要缺点是与其他方法相比，查询相机的位置和方位定位相对较差。新方法仅在数据库中存储图像特征，依靠潜在的三维重建技术但不保留整个三维场景重建。它通过相对平移估计而非相对旋转估计来估算查询相机中心，并通过从估计的相对姿态计算最优姿态转向多视角对应关系的最优姿态计算，实现了性能提升。在7场景和剑桥地标数据集上，该方法在时间和内存占用方面均优于现有技术。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>图像检索（IR）方法具有视觉无关性、易于实现和使用、无隐私问题和计算效率高等优点。</li>
<li>混合方法结合IR和深度学习（DNN）以及三维重建技术用于图像定位。</li>
<li>该方法主要缺点在于查询相机的定位和方位定位相对较差。</li>
<li>该方法在数据库中仅存储图像特征，依赖潜在的三维重建技术。</li>
<li>方法通过相对平移估计而非相对旋转估计来估算查询相机中心。</li>
<li>方法从估计的相对姿态计算最优姿态转向多视角对应关系的最优姿态计算。</li>
<li>在数据集上，该方法相较于现有技术具有更好的性能和较低的时间和内存占用。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.23577">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-5d56cbb7bfa9ddf48568ce8523033a55.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-99bd1ac8e4fb36f051e7d62052e9e193.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ec466288461d27ef71eee3e6db803a10.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6deb20a1f06321adb6155309281cec19.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-51745f577ba0ad41bd09d9533ba84221.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-60a025f6726849a65d2fc547691901f7.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Deterministic-Medical-Image-Translation-via-High-fidelity-Brownian-Bridges"><a href="#Deterministic-Medical-Image-Translation-via-High-fidelity-Brownian-Bridges" class="headerlink" title="Deterministic Medical Image Translation via High-fidelity Brownian   Bridges"></a>Deterministic Medical Image Translation via High-fidelity Brownian   Bridges</h2><p><strong>Authors:Qisheng He, Nicholas Summerfield, Peiyong Wang, Carri Glide-Hurst, Ming Dong</strong></p>
<p>Recent studies have shown that diffusion models produce superior synthetic images when compared to Generative Adversarial Networks (GANs). However, their outputs are often non-deterministic and lack high fidelity to the ground truth due to the inherent randomness. In this paper, we propose a novel High-fidelity Brownian bridge model (HiFi-BBrg) for deterministic medical image translations. Our model comprises two distinct yet mutually beneficial mappings: a generation mapping and a reconstruction mapping. The Brownian bridge training process is guided by the fidelity loss and adversarial training in the reconstruction mapping. This ensures that translated images can be accurately reversed to their original forms, thereby achieving consistent translations with high fidelity to the ground truth. Our extensive experiments on multiple datasets show HiFi-BBrg outperforms state-of-the-art methods in multi-modal image translation and multi-image super-resolution. </p>
<blockquote>
<p>最近的研究表明，与生成对抗网络（GANs）相比，扩散模型在生成合成图像方面表现出更优越的性能。然而，它们的输出通常是非确定的，并且由于固有的随机性，对真实数据的保真度不高。在本文中，我们提出了一种用于确定性医学图像翻译的高保真布朗桥模型（HiFi-BBrg）。我们的模型包括两个不同但相互有益的映射：生成映射和重建映射。布朗桥训练过程由重建映射中的保真度损失和对抗性训练引导。这确保了翻译的图像可以准确地恢复到其原始形式，从而实现与真实数据高度一致的翻译。我们在多个数据集上的广泛实验表明，HiFi-BBrg在跨模态图像翻译和多图像超分辨率方面优于最先进的方法。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.22531v1">PDF</a> </p>
<p><strong>Summary</strong><br>     最近的研究表明，扩散模型在生成合成图像方面相较于生成对抗网络（GANs）有优势。然而，其输出通常具有非确定性，并且由于固有的随机性，对真实数据的保真度不高。本文提出了一种新型的用于确定性医学图像翻译的高保真布朗桥模型（HiFi-BBrg）。该模型包含两个独特且相辅相成的映射：生成映射和重建映射。布朗桥训练过程由重建映射中的保真损失和对抗训练引导，确保翻译后的图像可以准确还原为原始形式，从而实现具有与真实数据高保真的一致翻译。在多个数据集上的广泛实验表明，HiFi-BBrg在跨模态图像翻译和多图像超分辨率方面优于现有先进技术。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>扩散模型在合成图像生成方面相较于GANs有优势。</li>
<li>扩散模型的输出常具有非确定性，并且对真实数据的保真度不高。</li>
<li>提出的HiFi-BBrg模型用于确定性医学图像翻译。</li>
<li>HiFi-BBrg模型包含生成映射和重建映射两个独特且相辅相成的部分。</li>
<li>布朗桥训练过程确保翻译后的图像可以准确还原。</li>
<li>HiFi-BBrg模型实现了高保真的一致翻译。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.22531">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-89b720f2289ff49b5d424b1d1bb39466.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-42c63a9a154920b748723ea6f60d03e9.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-10b4c05035d61db74240beef8ac3ded3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-74dd698f33980edc4ea7e2552f99da80.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-6c4519fed2f33f938908a378ab11a3e6.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-d5c85f0176bb823177a0938160c03c26.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Time-resolved-dynamic-CBCT-reconstruction-using-prior-model-free-spatiotemporal-Gaussian-representation-PMF-STGR"><a href="#Time-resolved-dynamic-CBCT-reconstruction-using-prior-model-free-spatiotemporal-Gaussian-representation-PMF-STGR" class="headerlink" title="Time-resolved dynamic CBCT reconstruction using prior-model-free   spatiotemporal Gaussian representation (PMF-STGR)"></a>Time-resolved dynamic CBCT reconstruction using prior-model-free   spatiotemporal Gaussian representation (PMF-STGR)</h2><p><strong>Authors:Jiacheng Xie, Hua-Chieh Shao, You Zhang</strong></p>
<p>Time-resolved CBCT imaging, which reconstructs a dynamic sequence of CBCTs reflecting intra-scan motion (one CBCT per x-ray projection without phase sorting or binning), is highly desired for regular and irregular motion characterization, patient setup, and motion-adapted radiotherapy. Representing patient anatomy and associated motion fields as 3D Gaussians, we developed a Gaussian representation-based framework (PMF-STGR) for fast and accurate dynamic CBCT reconstruction. PMF-STGR comprises three major components: a dense set of 3D Gaussians to reconstruct a reference-frame CBCT for the dynamic sequence; another 3D Gaussian set to capture three-level, coarse-to-fine motion-basis-components (MBCs) to model the intra-scan motion; and a CNN-based motion encoder to solve projection-specific temporal coefficients for the MBCs. Scaled by the temporal coefficients, the learned MBCs will combine into deformation vector fields to deform the reference CBCT into projection-specific, time-resolved CBCTs to capture the dynamic motion. Due to the strong representation power of 3D Gaussians, PMF-STGR can reconstruct dynamic CBCTs in a ‘one-shot’ training fashion from a standard 3D CBCT scan, without using any prior anatomical or motion model. We evaluated PMF-STGR using XCAT phantom simulations and real patient scans. Metrics including the image relative error, structural-similarity-index-measure, tumor center-of-mass-error, and landmark localization error were used to evaluate the accuracy of solved dynamic CBCTs and motion. PMF-STGR shows clear advantages over a state-of-the-art, INR-based approach, PMF-STINR. Compared with PMF-STINR, PMF-STGR reduces reconstruction time by 50% while reconstructing less blurred images with better motion accuracy. With improved efficiency and accuracy, PMF-STGR enhances the applicability of dynamic CBCT imaging for potential clinical translation. </p>
<blockquote>
<p>时间解析CBCT成像技术重构反映扫描内动态的一系列CBCT图像（每次X射线投影都有一张CBCT，无需相位排序或分组），对于常规和不规则运动特征、患者设置和适应性放疗中的运动需求具有重要意义。我们用三维高斯函数来表示患者解剖结构和相关运动场，并开发了一种基于高斯表示的框架（PMF-STGR），用于快速准确地重建动态CBCT。PMF-STGR主要包括三个组成部分：使用密集的三维高斯集重构动态序列的参考帧CBCT；使用另一个三维高斯集捕捉三级粗细运动基础成分（MBCs），以模拟扫描内的运动；基于CNN的运动编码器用于解决与MBC投影相关的特定时间系数。根据时间系数，学习到的MBC将组合成变形矢量场，将参考CBCT变形为与投影相关的、时间解析的CBCT，以捕捉动态运动。由于三维高斯函数具有很强的表示能力，PMF-STGR可以从标准的3D CBCT扫描中通过“单次”训练的方式重建动态CBCT，无需使用任何先验解剖或运动模型。我们使用XCAT幻影模拟和实际患者扫描对PMF-STGR进行了评估。使用图像相对误差、结构相似性指数度量、肿瘤质量中心误差和地标定位误差等指标来评估求解的动态CBCT和运动准确性。PMF-STGR相比于目前最先进的方法——基于INR的方法PMF-STINR具有明显的优势。与PMF-STINR相比，PMF-STGR在重建时间减少50%的同时，生成了模糊度更低的图像，并提高了运动准确性。凭借更高的效率和准确性，PMF-STGR增强了动态CBCT成像的适用性，具有潜在的临床应用价值。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.22139v1">PDF</a> 25 pages, 5 figures</p>
<p><strong>Summary</strong></p>
<p>动态CBCT成像能够反映无相位排序或无分组的单次扫描中的运动，这对于常规和不规则的运动特性分析、患者设置和自适应放疗具有极大价值。为快速准确地重建动态CBCT序列，研究团队建立了基于高斯表示的框架（PMF-STGR）。该框架通过构建密集的三维高斯集来重建动态序列的参考框架CBCT，并引入另一组三维高斯集捕捉三个级别的运动基础分量（MBCs），从而模拟扫描期间的运动。同时，采用基于卷积神经网络（CNN）的运动编码器计算针对MBCs的投影特定时间系数。基于这些时间系数学习的MBCs会转化为变形向量场，以将参考CBCT变形为适应投影特定时间的动态CBCT图像序列，实现动态运动捕捉。该方法可在一次扫描中使用单个CBCT训练出动态CBCT图像序列，无需使用任何先验解剖或运动模型。通过XCAT幻影模拟和真实患者扫描验证了其性能。评估指标包括图像相对误差、结构相似性指数度量、肿瘤质心误差和地标定位误差等。相较于目前的主流方法PMF-STINR，PMF-STGR具有明显优势，其重建时间缩短了50%，同时生成的图像更清晰且运动准确性更高。这提高了动态CBCT成像在临床应用中的效率和准确性。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>时间解析CBCT成像对于运动表征、患者设置和自适应放疗至关重要。</li>
<li>研究团队提出了基于高斯表示的框架（PMF-STGR）进行动态CBCT重建。</li>
<li>PMF-STGR利用三维高斯集捕捉参考帧和运动中细节。</li>
<li>引入CNN解决运动编码问题，生成动态运动的变形向量场。</li>
<li>PMF-STGR在一次扫描中即可训练出动态CBCT图像序列，无需先验模型。</li>
<li>与现有方法相比，PMF-STGR在图像质量和运动准确性方面表现出优势。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.22139">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-bfd8e9728794a4da0f9548e5ba18d0ea.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Image-as-an-IMU-Estimating-Camera-Motion-from-a-Single-Motion-Blurred-Image"><a href="#Image-as-an-IMU-Estimating-Camera-Motion-from-a-Single-Motion-Blurred-Image" class="headerlink" title="Image as an IMU: Estimating Camera Motion from a Single Motion-Blurred   Image"></a>Image as an IMU: Estimating Camera Motion from a Single Motion-Blurred   Image</h2><p><strong>Authors:Jerred Chen, Ronald Clark</strong></p>
<p>In many robotics and VR&#x2F;AR applications, fast camera motions cause a high level of motion blur, causing existing camera pose estimation methods to fail. In this work, we propose a novel framework that leverages motion blur as a rich cue for motion estimation rather than treating it as an unwanted artifact. Our approach works by predicting a dense motion flow field and a monocular depth map directly from a single motion-blurred image. We then recover the instantaneous camera velocity by solving a linear least squares problem under the small motion assumption. In essence, our method produces an IMU-like measurement that robustly captures fast and aggressive camera movements. To train our model, we construct a large-scale dataset with realistic synthetic motion blur derived from ScanNet++v2 and further refine our model by training end-to-end on real data using our fully differentiable pipeline. Extensive evaluations on real-world benchmarks demonstrate that our method achieves state-of-the-art angular and translational velocity estimates, outperforming current methods like MASt3R and COLMAP. </p>
<blockquote>
<p>在机器人技术和虚拟现实&#x2F;增强现实应用中，快速相机运动会产生高水平的运动模糊，导致现有的相机姿态估计方法失效。在这项工作中，我们提出了一种新型框架，该框架利用运动模糊作为运动估计的丰富线索，而不是将其视为不受欢迎的伪像。我们的方法通过直接从单张运动模糊图像预测密集运动流场和单眼深度图来工作。然后，我们在小运动假设下通过解决线性最小二乘问题来恢复瞬时相机速度。本质上，我们的方法产生了一种类似于IMU的测量值，能够稳健地捕获快速且猛烈的相机运动。为了训练我们的模型，我们使用ScanNet++v2构建了具有现实合成运动模糊的大规模数据集，并通过在我们的完全可微管道上对真实数据进行端到端的训练来进一步完善我们的模型。在真实世界基准测试上的广泛评估表明，我们的方法达到了最先进的角速度和线速度估计，优于MASt3R和COLMAP等方法。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.17358v3">PDF</a> Project page: <a target="_blank" rel="noopener" href="https://jerredchen.github.io/image-as-imu/">https://jerredchen.github.io/image-as-imu/</a></p>
<p><strong>Summary</strong></p>
<p>该文本提出了一种利用运动模糊作为运动估计的丰富线索的新框架，而不是将其视为不需要的伪影。该框架通过预测从单个运动模糊图像中的密集运动流场和单眼深度图来工作，然后通过解决小运动假设下的线性最小二乘问题来恢复瞬时相机速度。该方法实质上产生了IMU式的测量，能够稳健地捕获快速和激烈的相机运动。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>现有方法在快速相机运动下会出现运动模糊问题，导致姿态估计失败。</li>
<li>提出了一种新的框架，利用运动模糊作为运动估计的丰富线索。</li>
<li>通过预测密集运动流场和单眼深度图来估计相机运动。</li>
<li>通过解决线性最小二乘问题来恢复瞬时相机速度。</li>
<li>方法实质上产生了IMU式的测量，稳健地捕获快速和激烈的相机运动。</li>
<li>使用ScanNet++v2构建的大规模数据集进行模型训练，并利用完全可微分的管道进行真实数据的端到端训练来进一步优化模型。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.17358">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-1777a89b737824da0e9215fe6ae9626b.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-3fb73a1c56093ba9736e574607743b9d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-26d18fa7bf9cd68e578ba48ef497162e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-fd72da0aaee36d32b4b7d37e7f10969a.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-e14172ed08be1736a14fbf020ba7306c.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="PTDiffusion-Free-Lunch-for-Generating-Optical-Illusion-Hidden-Pictures-with-Phase-Transferred-Diffusion-Model"><a href="#PTDiffusion-Free-Lunch-for-Generating-Optical-Illusion-Hidden-Pictures-with-Phase-Transferred-Diffusion-Model" class="headerlink" title="PTDiffusion: Free Lunch for Generating Optical Illusion Hidden Pictures   with Phase-Transferred Diffusion Model"></a>PTDiffusion: Free Lunch for Generating Optical Illusion Hidden Pictures   with Phase-Transferred Diffusion Model</h2><p><strong>Authors:Xiang Gao, Shuai Yang, Jiaying Liu</strong></p>
<p>Optical illusion hidden picture is an interesting visual perceptual phenomenon where an image is cleverly integrated into another picture in a way that is not immediately obvious to the viewer. Established on the off-the-shelf text-to-image (T2I) diffusion model, we propose a novel training-free text-guided image-to-image (I2I) translation framework dubbed as \textbf{P}hase-\textbf{T}ransferred \textbf{Diffusion} Model (PTDiffusion) for hidden art syntheses. PTDiffusion harmoniously embeds an input reference image into arbitrary scenes described by the text prompts, producing illusion images exhibiting hidden visual cues of the reference image. At the heart of our method is a plug-and-play phase transfer mechanism that dynamically and progressively transplants diffusion features’ phase spectrum from the denoising process to reconstruct the reference image into the one to sample the generated illusion image, realizing deep fusion of the reference structural information and the textual semantic information in the diffusion model latent space. Furthermore, we propose asynchronous phase transfer to enable flexible control to the degree of hidden content discernability. Our method bypasses any model training and fine-tuning process, all while substantially outperforming related text-guided I2I methods in image generation quality, text fidelity, visual discernibility, and contextual naturalness for illusion picture synthesis, as demonstrated by extensive qualitative and quantitative experiments. Our project is publically available at \href{<a target="_blank" rel="noopener" href="https://xianggao1102.github.io/PTDiffusion_webpage/%7D%7Bthis">https://xianggao1102.github.io/PTDiffusion_webpage/}{this</a> web page}. </p>
<blockquote>
<p>光学错觉隐藏图像是一种有趣的视觉感知现象，其中图像被巧妙地集成到另一幅图像中，以至于观众无法立即察觉。我们基于现成的文本到图像（T2I）扩散模型，提出了一种无需训练的文字引导图像到图像（I2I）转换框架，名为阶段转移扩散模型（PTDiffusion），用于合成隐藏艺术。PTDiffusion和谐地将输入参考图像嵌入到文本提示描述的任意场景中，生成显示参考图像的隐藏视觉线索的错觉图像。我们方法的核心是一个即插即用的相位转移机制，该机制动态且渐进地从去噪过程中移植扩散特征的相位谱，将参考图像重建到采样生成的错觉图像中，实现在扩散模型潜在空间中的参考结构信息和文本语义信息的深度融合。此外，我们提出了异步相位转移，以实现灵活控制隐藏内容的可识别程度。我们的方法避开了任何模型训练和微调过程，同时在图像生成质量、文本忠实度、视觉可辨识度和上下文自然度等方面大大优于相关的文本引导I2I方法，用于错觉图像合成。我们的项目已在以下网页公开：<a target="_blank" rel="noopener" href="https://xianggao1102.github.io/PTDiffusion_webpage/">网页链接</a>。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.06186v4">PDF</a> Proceedings of the IEEE&#x2F;CVF Conference on Computer Vision and Pattern   Recognition (CVPR 2025)</p>
<p><strong>Summary</strong></p>
<p>文本描述了一个名为PTDiffusion的新方法，它是一种无需训练的文本引导的图像到图像（I2I）翻译框架，用于合成隐藏艺术图像。该方法通过将输入参考图像嵌入到由文本提示描述的任意场景中，生成隐藏视觉线索的错觉图像。其核心技术是插播式相位转移机制，实现参考图像与文本语义信息的深度融合。此外，还提出了异步相位转移，以实现灵活控制隐藏内容的识别程度。该方法无需任何模型训练和微调过程，在图像生成质量、文本忠实度、视觉辨识度和上下文自然性方面均大大优于相关文本引导的I2I方法。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>PTDiffusion是一种新的文本引导的图像到图像（I2I）翻译框架，用于合成隐藏艺术图像。</li>
<li>它通过将参考图像巧妙地嵌入到由文本描述的场景中，生成具有隐藏视觉线索的错觉图像。</li>
<li>插播式相位转移机制是PTDiffusion的核心技术，实现了参考图像与文本信息的深度融合。</li>
<li>异步相位转移允许灵活控制隐藏内容的识别程度。</li>
<li>PTDiffusion无需任何模型训练和微调，具有出色的图像生成质量、文本忠实度、视觉辨识度和上下文自然性。</li>
<li>该方法对于创建视觉错觉图像具有重要的应用价值。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.06186">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-889d3d3cf3e78ee16cb22a8e5afcee2e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d546ccc0900e834495405121f3861e10.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a6a32b6a8b666e971777898d004c84fa.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-a2e7ca8dc8a3c6bf59c59e13a1e6479a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1414e64ebf8d7c7960f3cd0f6426a083.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="Motion-Prompting-Controlling-Video-Generation-with-Motion-Trajectories"><a href="#Motion-Prompting-Controlling-Video-Generation-with-Motion-Trajectories" class="headerlink" title="Motion Prompting: Controlling Video Generation with Motion Trajectories"></a>Motion Prompting: Controlling Video Generation with Motion Trajectories</h2><p><strong>Authors:Daniel Geng, Charles Herrmann, Junhwa Hur, Forrester Cole, Serena Zhang, Tobias Pfaff, Tatiana Lopez-Guevara, Carl Doersch, Yusuf Aytar, Michael Rubinstein, Chen Sun, Oliver Wang, Andrew Owens, Deqing Sun</strong></p>
<p>Motion control is crucial for generating expressive and compelling video content; however, most existing video generation models rely mainly on text prompts for control, which struggle to capture the nuances of dynamic actions and temporal compositions. To this end, we train a video generation model conditioned on spatio-temporally sparse or dense motion trajectories. In contrast to prior motion conditioning work, this flexible representation can encode any number of trajectories, object-specific or global scene motion, and temporally sparse motion; due to its flexibility we refer to this conditioning as motion prompts. While users may directly specify sparse trajectories, we also show how to translate high-level user requests into detailed, semi-dense motion prompts, a process we term motion prompt expansion. We demonstrate the versatility of our approach through various applications, including camera and object motion control, “interacting” with an image, motion transfer, and image editing. Our results showcase emergent behaviors, such as realistic physics, suggesting the potential of motion prompts for probing video models and interacting with future generative world models. Finally, we evaluate quantitatively, conduct a human study, and demonstrate strong performance. Video results are available on our webpage: <a target="_blank" rel="noopener" href="https://motion-prompting.github.io/">https://motion-prompting.github.io/</a> </p>
<blockquote>
<p>动作控制在生成富有表现力和引人入胜的视频内容方面起着至关重要的作用。然而，现有的大多数视频生成模型主要依赖于文本提示来进行控制，这很难捕捉到动态动作和时序组合的细微差别。为此，我们训练了一种基于时空稀疏或密集运动轨迹的视频生成模型。与之前的工作相比，这种灵活的表示可以编码任意数量的轨迹、特定对象的运动或全局场景的运动，以及时序稀疏的运动。由于其灵活性，我们将这种条件称为运动提示。虽然用户可以直接指定稀疏轨迹，我们还展示了如何将高级用户请求翻译成详细、半密集的运动提示，我们称这个过程为运动提示扩展。我们通过各种应用展示了我们的方法的通用性，包括相机和对象运动控制、“与图像互动”、运动转移和图像编辑等。我们的结果展示了新兴的行为，如逼真的物理效果，这表明运动提示在探查视频模型以及与未来的生成世界模型互动方面具有潜力。最后，我们进行了定量评估、人类研究，并展示了强大的性能。视频结果可在我们的网页上看到：<a target="_blank" rel="noopener" href="https://motion-prompting.github.io/%E3%80%82">https://motion-prompting.github.io/。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.02700v2">PDF</a> CVPR 2025 camera ready. Project page:   <a target="_blank" rel="noopener" href="https://motion-prompting.github.io/">https://motion-prompting.github.io/</a></p>
<p><strong>Summary</strong></p>
<p>本文主要探讨了动作控制在生成具有表现力和吸引力的视频内容中的重要性。传统的视频生成模型主要依赖于文本提示进行动作控制，难以捕捉动态行为和时序组合的细节。为此，本文训练了一种基于时空稀疏或密集运动轨迹的视频生成模型，并引入了一种灵活的运动提示条件表达。这种表达方式能够编码任意数量的轨迹、物体特定的或全局场景的运动，以及时序稀疏的运动。用户可以直接指定稀疏轨迹，本文还展示了如何将高级用户请求转化为详细的半密集运动提示，称为运动提示扩展。本文通过各种应用展示了该方法的通用性，包括相机和物体运动控制、“与图像互动”、运动转移和图像编辑等。结果展现了新兴的行为，如现实物理现象，显示了运动提示在探测视频模型和与未来生成世界模型互动中的潜力。最后，本文进行了定量评估、人类研究，并展示了强大的性能表现。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>动作控制在生成视频内容中的重要性。</li>
<li>现有视频生成模型主要依赖文本提示进行动作控制，存在难以捕捉动态行为和时序组合的细微之处的缺陷。</li>
<li>引入了一种基于时空稀疏或密集运动轨迹的视频生成模型，并称之为“运动提示”。</li>
<li>运动提示可以编码任意数量的轨迹、物体特定的或全局场景的运动，以及时序稀疏的运动，具有灵活性。</li>
<li>用户可以直接指定稀疏轨迹，同时本文还展示了如何将高级用户请求转化为详细的半密集运动提示（运动提示扩展）。</li>
<li>该方法具有广泛的应用性，包括相机和物体运动控制、与图像的互动、运动转移和图像编辑等。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.02700">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-ba5400b4ccad7078f3fcd89bccd68bd9.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-f412082d9bdeceb09aafff53399ecf72.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a1e8e03529298f81ab365b5d7d7091df.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3161894bd42dbfb38da86c504923149a.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-4f555e8e65db76eea91e285ef4e29c10.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="A-Unified-Framework-for-Forward-and-Inverse-Problems-in-Subsurface-Imaging-using-Latent-Space-Translations"><a href="#A-Unified-Framework-for-Forward-and-Inverse-Problems-in-Subsurface-Imaging-using-Latent-Space-Translations" class="headerlink" title="A Unified Framework for Forward and Inverse Problems in Subsurface   Imaging using Latent Space Translations"></a>A Unified Framework for Forward and Inverse Problems in Subsurface   Imaging using Latent Space Translations</h2><p><strong>Authors:Naveen Gupta, Medha Sawhney, Arka Daw, Youzuo Lin, Anuj Karpatne</strong></p>
<p>In subsurface imaging, learning the mapping from velocity maps to seismic waveforms (forward problem) and waveforms to velocity (inverse problem) is important for several applications. While traditional techniques for solving forward and inverse problems are computationally prohibitive, there is a growing interest in leveraging recent advances in deep learning to learn the mapping between velocity maps and seismic waveform images directly from data. Despite the variety of architectures explored in previous works, several open questions still remain unanswered such as the effect of latent space sizes, the importance of manifold learning, the complexity of translation models, and the value of jointly solving forward and inverse problems. We propose a unified framework to systematically characterize prior research in this area termed the Generalized Forward-Inverse (GFI) framework, building on the assumption of manifolds and latent space translations. We show that GFI encompasses previous works in deep learning for subsurface imaging, which can be viewed as specific instantiations of GFI. We also propose two new model architectures within the framework of GFI: Latent U-Net and Invertible X-Net, leveraging the power of U-Nets for domain translation and the ability of IU-Nets to simultaneously learn forward and inverse translations, respectively. We show that our proposed models achieve state-of-the-art (SOTA) performance for forward and inverse problems on a wide range of synthetic datasets, and also investigate their zero-shot effectiveness on two real-world-like datasets. Our code is available at <a target="_blank" rel="noopener" href="https://github.com/KGML-lab/Generalized-Forward-Inverse-Framework-for-DL4SI">https://github.com/KGML-lab/Generalized-Forward-Inverse-Framework-for-DL4SI</a> </p>
<blockquote>
<p>在地下成像中，学习从速度图到地震波形（正问题）和从波形到速度（反问题）的映射对于多个应用非常重要。虽然传统解决正问题和反问题的技术在计算上是禁止的，但越来越多的兴趣在于利用深度学习领域的最新进展直接从数据中学习速度图与地震波形图像之间的映射。尽管以前的研究中探索了多种架构，但仍有许多公开问题尚未解决，例如潜在空间大小的影响、流形学习的重要性、翻译模型的复杂性以及联合解决正问题和反问题的价值。我们提出了一个统一框架，旨在系统地刻画这一领域的先前研究，我们称之为广义正反（GFI）框架，它建立在流形和潜在空间翻译假设的基础上。我们表明，GFI涵盖了地下成像深度学习领域的先前研究，这些研究可以看作是GFI的特定实例。在GFI框架下，我们还提出了两种新型模型架构：潜在U-Net和可逆X-Net，它们利用U-Net在领域翻译方面的力量以及IU-Net同时学习正向和逆向翻译的能力。我们展示了我们提出的模型在合成数据集上的正反问题上实现了最先进的性能，并调查了它们在两个类似现实世界数据集上的零样本有效性。我们的代码可在 <a target="_blank" rel="noopener" href="https://github.com/KGML-lab/Generalized-Forward-Inverse-Framework-for-DL4SI">https://github.com/KGML-lab/Generalized-Forward-Inverse-Framework-for-DL4SI</a> 找到。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2410.11247v3">PDF</a> Accepted at ICLR 2025</p>
<p><strong>Summary</strong></p>
<p>本文探讨地下成像领域中速度图与地震波形之间的映射问题，包括正问题和反问题。传统解决正反问题的方法计算量大，现多采用深度学习技术直接从数据中学习速度图与地震波形图像之间的映射关系。文章提出一种广义正逆（GFI）框架来系统分析该领域的前期研究，并在此框架下提出两种新型模型架构：潜在U-Net和可逆X-Net。这些模型在合成数据集上表现优异，并在两种模拟现实世界数据集上实现了零样本有效性。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>地下成像中，正问题和反问题的研究重要，涉及速度图到地震波形的映射以及反向过程。</li>
<li>传统方法计算量大，现有研究倾向于利用深度学习技术来学习速度图与地震波形之间的直接映射。</li>
<li>提出广义正逆（GFI）框架，用于系统分析该领域的前期研究。</li>
<li>在GFI框架下，提出两种新型模型架构：潜在U-Net和可逆X-Net。</li>
<li>模型在合成数据集上表现优越，达到目前最佳水平（SOTA）。</li>
<li>模型在模拟现实世界数据集上实现零样本有效性。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2410.11247">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-8c52507908c7162d13d840091c84584c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8217b968921eeb42dbb6465291ec312a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4c9816ff7e7322bebc490d7c206a8843.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-a364660b40c10c7ad69133118967ee7c.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="Improving-Neural-Optimal-Transport-via-Displacement-Interpolation"><a href="#Improving-Neural-Optimal-Transport-via-Displacement-Interpolation" class="headerlink" title="Improving Neural Optimal Transport via Displacement Interpolation"></a>Improving Neural Optimal Transport via Displacement Interpolation</h2><p><strong>Authors:Jaemoo Choi, Yongxin Chen, Jaewoong Choi</strong></p>
<p>Optimal Transport (OT) theory investigates the cost-minimizing transport map that moves a source distribution to a target distribution. Recently, several approaches have emerged for learning the optimal transport map for a given cost function using neural networks. We refer to these approaches as the OT Map. OT Map provides a powerful tool for diverse machine learning tasks, such as generative modeling and unpaired image-to-image translation. However, existing methods that utilize max-min optimization often experience training instability and sensitivity to hyperparameters. In this paper, we propose a novel method to improve stability and achieve a better approximation of the OT Map by exploiting displacement interpolation, dubbed Displacement Interpolation Optimal Transport Model (DIOTM). We derive the dual formulation of displacement interpolation at specific time $t$ and prove how these dual problems are related across time. This result allows us to utilize the entire trajectory of displacement interpolation in learning the OT Map. Our method improves the training stability and achieves superior results in estimating optimal transport maps. We demonstrate that DIOTM outperforms existing OT-based models on image-to-image translation tasks. </p>
<blockquote>
<p>最优传输（OT）理论探究的是将源分布转移到目标分布的成本最小化传输映射。最近，已经出现了几种使用神经网络为给定成本函数学习最优传输映射的方法。我们将这些方法称为OT Map。OT Map为多样的机器学习任务（如生成建模和未配对的图像到图像翻译）提供了强大的工具。然而，现有的使用最大-最小优化方法的方法通常存在训练不稳定和对超参数敏感的问题。在本文中，我们提出了一种新的方法，通过利用位移插值来改善稳定性，并更好地逼近OT Map，被称为位移插值最优传输模型（DIOTM）。我们推导出位移插值在特定时间t的对偶公式，并证明这些对偶问题随时间推移是如何相互关联的。这一结果允许我们利用位移插值的整个轨迹来学习OT Map。我们的方法提高了训练稳定性，并在估计最优传输映射方面取得了更好的结果。我们证明DIOTM在图像到图像翻译任务上优于现有的基于OT的模型。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2410.03783v3">PDF</a> 20 pages</p>
<p><strong>Summary</strong></p>
<p>本文介绍了最优传输（OT）理论及其在计算机学习中的应用，特别是在生成建模和未配对图像转换等领域的应用。文章提出了一种新的方法，即位移插值最优传输模型（DIOTM），以提高训练稳定性和对最优传输地图的近似精度。该方法通过利用位移插值的整个轨迹来学习最优传输地图，并在图像转换任务上表现出优异性能。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>最优传输（OT）理论是研究如何将源分布转移到目标分布的成本最小化传输图。</li>
<li>神经网络已用于学习给定成本函数的最优传输图，但现有方法存在训练不稳定和对超参数敏感的问题。</li>
<li>本文提出了一种新的方法，即位移插值最优传输模型（DIOTM），以提高稳定性和对最优传输图的近似精度。</li>
<li>DIOTM通过推导位移插值在特定时间t的对偶公式，并利用整个位移插值轨迹来学习最优传输图。</li>
<li>DIOTM方法提高了训练稳定性。</li>
<li>在图像转换任务上，DIOTM表现出优于现有基于OT的模型的性能。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2410.03783">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-3083ba409ae8da6d3439c077d6c46a20.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="AI-in-radiological-imaging-of-soft-tissue-and-bone-tumours-a-systematic-review-evaluating-against-CLAIM-and-FUTURE-AI-guidelines"><a href="#AI-in-radiological-imaging-of-soft-tissue-and-bone-tumours-a-systematic-review-evaluating-against-CLAIM-and-FUTURE-AI-guidelines" class="headerlink" title="AI in radiological imaging of soft-tissue and bone tumours: a systematic   review evaluating against CLAIM and FUTURE-AI guidelines"></a>AI in radiological imaging of soft-tissue and bone tumours: a systematic   review evaluating against CLAIM and FUTURE-AI guidelines</h2><p><strong>Authors:Douwe J. Spaanderman, Matthew Marzetti, Xinyi Wan, Andrew F. Scarsbrook, Philip Robinson, Edwin H. G. Oei, Jacob J. Visser, Robert Hemke, Kirsten van Langevelde, David F. Hanff, Geert J. L. H. van Leenders, Cornelis Verhoef, Dirk J. Gruühagen, Wiro J. Niessen, Stefan Klein, Martijn P. A. Starmans</strong></p>
<p>Soft-tissue and bone tumours (STBT) are rare, diagnostically challenging lesions with variable clinical behaviours and treatment approaches. This systematic review provides an overview of Artificial Intelligence (AI) methods using radiological imaging for diagnosis and prognosis of these tumours, highlighting challenges in clinical translation, and evaluating study alignment with the Checklist for AI in Medical Imaging (CLAIM) and the FUTURE-AI international consensus guidelines for trustworthy and deployable AI to promote the clinical translation of AI methods. The review covered literature from several bibliographic databases, including papers published before 17&#x2F;07&#x2F;2024. Original research in peer-reviewed journals focused on radiology-based AI for diagnosing or prognosing primary STBT was included. Exclusion criteria were animal, cadaveric, or laboratory studies, and non-English papers. Abstracts were screened by two of three independent reviewers for eligibility. Eligible papers were assessed against guidelines by one of three independent reviewers. The search identified 15,015 abstracts, from which 325 articles were included for evaluation. Most studies performed moderately on CLAIM, averaging a score of 28.9$\pm$7.5 out of 53, but poorly on FUTURE-AI, averaging 5.1$\pm$2.1 out of 30. Imaging-AI tools for STBT remain at the proof-of-concept stage, indicating significant room for improvement. Future efforts by AI developers should focus on design (e.g. define unmet clinical need, intended clinical setting and how AI would be integrated in clinical workflow), development (e.g. build on previous work, explainability), evaluation (e.g. evaluating and addressing biases, evaluating AI against best practices), and data reproducibility and availability (making documented code and data publicly available). Following these recommendations could improve clinical translation of AI methods. </p>
<blockquote>
<p>软组织及骨肿瘤（STBT）是罕见的、诊断具有挑战性的病变，其临床行为和治疗方式各异。这篇系统性综述概述了使用放射影像学进行诊断和预后的软组织及骨肿瘤的人工智能（AI）方法，强调了临床翻译中的挑战，并评估了研究是否符合医学影像学人工智能（CLAIM）清单以及可信且可部署的人工智能的国际共识指南（FUTURE-AI），以促进AI方法的临床翻译。综述涵盖了多个文献数据库的文献，包括在2024年7月17日之前发表的文章。纳入的原始研究是对基于放射学的AI用于诊断或预测原发性STBT的研究。排除标准是动物、尸体或实验室研究，以及非英语论文。摘要由三名独立评审员中的两名进行资格筛选。合格论文由三名独立评审员之一根据指南进行评估。搜索确定了15015篇摘要，其中325篇文章纳入评估。大多数研究在CLAIM上的表现中等，平均得分为28.9±7.5（满分为53分），但在FUTURE-AI上的表现较差，平均得分为5.1±2.1（满分为30分）。针对STBT的成像AI工具仍处于概念验证阶段，表明还有很大的改进空间。未来人工智能开发者应重点关注设计（如确定未满足的临床需求、预期的临床环境以及AI如何融入临床工作流程）、开发（如基于以往工作进行建设、解释性）、评估（如评估和解决偏见、评估AI是否符合最佳实践）、数据可重复性和可用性（公开提供有记录的代码和数据）。遵循这些建议可能有助于改善AI方法的临床翻译。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2408.12491v2">PDF</a> 25 pages, 6 figures, 8 supplementary figures</p>
<p><strong>Summary</strong><br>     软组织及骨肿瘤诊断与预后的人工智能影像技术系统综述，探讨临床转化挑战，并对照医疗影像人工智能清单（CLAIM）及国际可靠的未来人工智能（FUTURE-AI）指南进行评价。综述涵盖多个文献数据库的研究，重点讨论基于放射学的AI技术在诊断原发性软组织及骨肿瘤（STBT）方面的进展。现有研究在CLAIM上的表现中等，但在FUTURE-AI上的表现不佳。未来人工智能开发者应在设计、开发、评估和数据可重复性方面努力改进。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>AI在STBT诊断和预后中的应用得到了系统性回顾。</li>
<li>AI技术在放射学影像诊断方面的进展受到关注。</li>
<li>综述探讨了临床转化中的挑战，并对照医疗影像人工智能清单（CLAIM）和国际指南进行评价。</li>
<li>研究在CLAIME表现中等，但在FUTURE-AI指南上表现欠佳。</li>
<li>当前影像人工智能工具仍处于概念验证阶段，有巨大的改进空间。</li>
<li>AI开发者需要在设计、开发、评估和数据的可重复性方面做出努力。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2408.12491">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-c6d1d90463beb6ebf6b91fc61e8d7f8d.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="Style-transfer-between-Microscopy-and-Magnetic-Resonance-Imaging-via-Generative-Adversarial-Network-in-small-sample-size-settings"><a href="#Style-transfer-between-Microscopy-and-Magnetic-Resonance-Imaging-via-Generative-Adversarial-Network-in-small-sample-size-settings" class="headerlink" title="Style transfer between Microscopy and Magnetic Resonance Imaging via   Generative Adversarial Network in small sample size settings"></a>Style transfer between Microscopy and Magnetic Resonance Imaging via   Generative Adversarial Network in small sample size settings</h2><p><strong>Authors:Monika Pytlarz, Adrian Onicas, Alessandro Crimi</strong></p>
<p>Cross-modal augmentation of Magnetic Resonance Imaging (MRI) and microscopic imaging based on the same tissue samples is promising because it can allow histopathological analysis in the absence of an underlying invasive biopsy procedure. Here, we tested a method for generating microscopic histological images from MRI scans of the corpus callosum using conditional generative adversarial network (cGAN) architecture. To our knowledge, this is the first multimodal translation of the brain MRI to histological volumetric representation of the same sample. The technique was assessed by training paired image translation models taking sets of images from MRI scans and microscopy. The use of cGAN for this purpose is challenging because microscopy images are large in size and typically have low sample availability. The current work demonstrates that the framework reliably synthesizes histology images from MRI scans of corpus callosum, emphasizing the network’s ability to train on high resolution histologies paired with relatively lower-resolution MRI scans. With the ultimate goal of avoiding biopsies, the proposed tool can be used for educational purposes. </p>
<blockquote>
<p>基于同一组织样本的磁共振成像（MRI）和显微镜成像的跨模态增强方法前景广阔，因为它可以在没有基本的侵入性活检程序的情况下进行组织病理学分析。在这里，我们测试了一种方法，利用条件生成对抗网络（cGAN）架构，从胼胝体的MRI扫描生成显微镜组织学图像。据我们所知，这是首次将大脑MRI转换为同一样本的组织学体积表示的多模式转换。该技术通过训练配对图像翻译模型来评估，这些模型从MRI扫描和显微镜图像中获取信息。使用cGAN进行此目的具有挑战性，因为显微镜图像尺寸较大且样本通常可用量较少。当前的工作证明该框架可以可靠地从胼胝体的MRI扫描中合成组织学图像，突出了网络在相对较低的分辨率MRI扫描与较高的分辨率组织学配对训练的能力。以最终避免活检为目标，所提议的工具可用于教学目的。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10414v2">PDF</a> 2023 IEEE International Conference on Image Processing (ICIP)</p>
<p><strong>Summary</strong>：利用条件生成对抗网络（cGAN）架构，首次实现了基于MRI的微观组织学图像生成，展示了从MRI扫描到同一样本组织学体积表示的跨模态转换。尽管面临样本可用性低和图像分辨率高的挑战，但该框架仍成功合成出可靠的组织学图像，可为教育和临床实践提供支持。</p>
<p><strong>Key Takeaways</strong>：</p>
<ol>
<li>利用条件生成对抗网络（cGAN）实现了MRI与微观成像的跨模态增强。</li>
<li>成功实现从MRI扫描到同一样本组织学体积表示的转变。</li>
<li>框架能够在低分辨率MRI扫描上训练，并成功合成高分辨率组织学图像。</li>
<li>该技术可应用于教育目的，模拟真实微观环境下的病理变化，以辅助学习和研究。</li>
<li>尽管面临样本可用性和图像分辨率的挑战，但该技术仍显示出巨大的潜力。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2310.10414">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-fc0c12523c4b36dcaabf21ab502af2d0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4eb529541b04553defce20fdfac56c89.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ef9a59282640004234cddaa5201a1bf9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-100d26d07b2e1186e5a84cb61a73f34d.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        文章作者:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        文章链接:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-04-03/I2I%20Translation/">https://kedreamix.github.io/Talk2Paper/Paper/2025-04-03/I2I%20Translation/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        版权声明:
                    </i>
                </span>
                <span class="reprint-info">
                    本博客所有文章除特別声明外，均采用
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    许可协议。转载请注明来源
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/I2I-Translation/">
                                    <span class="chip bg-color">I2I Translation</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;上一篇</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-04-03/%E8%A7%86%E9%A2%91%E7%90%86%E8%A7%A3/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-ec05533b496a742a58c549d69e4bd8a0.jpg" class="responsive-img" alt="视频理解">
                        
                        <span class="card-title">视频理解</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            视频理解 方向最新论文已更新，请持续关注 Update in 2025-04-04  TimeSearch Hierarchical Video Search with Spotlight and Reflection for   Human-like Long Video Understanding
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-04-04
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E8%A7%86%E9%A2%91%E7%90%86%E8%A7%A3/" class="post-category">
                                    视频理解
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E8%A7%86%E9%A2%91%E7%90%86%E8%A7%A3/">
                        <span class="chip bg-color">视频理解</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                下一篇&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-04-03/Few-Shot/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-f811b76b1be7649f806abbab165d8628.jpg" class="responsive-img" alt="Few-Shot">
                        
                        <span class="card-title">Few-Shot</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Few-Shot 方向最新论文已更新，请持续关注 Update in 2025-04-03  Is Temporal Prompting All We Need For Limited Labeled Action   Recognition?
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-04-03
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Few-Shot/" class="post-category">
                                    Few-Shot
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Few-Shot/">
                        <span class="chip bg-color">Few-Shot</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- 代码块功能依赖 -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- 代码语言 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- 代码块复制 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- 代码块收缩 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC 悬浮按钮. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* 修复文章卡片 div 的宽度. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // 切换TOC目录展开收缩的相关操作.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;站点总字数:&nbsp;<span
                        class="white-color">29301k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- 运行天数提醒. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // 区分是否有年份.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = '本站已运行 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                daysTip = '本站已運行 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = '本站已运行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = '本站已運行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="关注我的知乎: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">知</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- 搜索遮罩框 -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span>
            <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- 白天和黑夜主题 -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- 回到顶部按钮 -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // 只在桌面版网页启用特效
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- 雪花特效 -->
    

    <!-- 鼠标星星特效 -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--腾讯兔小巢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- 动态标签 -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Σ(っ °Д °;)っ诶，页面崩溃了嘛？", clearTimeout(st)) : (document.title = "φ(゜▽゜*)♪咦，又好了！", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
