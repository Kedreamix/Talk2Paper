<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="I2I Translation">
    <meta name="description" content="I2I Translation æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-04-04  Leveraging Generalizability of Image-to-Image Translation for Enhanced   Adversarial Defense">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>I2I Translation | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pica.zhimg.com/v2-56ba19ee851fd0b044661cc31ed9a0b2.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">I2I Translation</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/I2I-Translation/">
                                <span class="chip bg-color">I2I Translation</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/I2I-Translation/" class="post-category">
                                I2I Translation
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-04-04
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-05-14
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    16k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    65 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-04-04-æ›´æ–°"><a href="#2025-04-04-æ›´æ–°" class="headerlink" title="2025-04-04 æ›´æ–°"></a>2025-04-04 æ›´æ–°</h1><h2 id="Leveraging-Generalizability-of-Image-to-Image-Translation-for-Enhanced-Adversarial-Defense"><a href="#Leveraging-Generalizability-of-Image-to-Image-Translation-for-Enhanced-Adversarial-Defense" class="headerlink" title="Leveraging Generalizability of Image-to-Image Translation for Enhanced   Adversarial Defense"></a>Leveraging Generalizability of Image-to-Image Translation for Enhanced   Adversarial Defense</h2><p><strong>Authors:Haibo Zhang, Zhihua Yao, Kouichi Sakurai, Takeshi Saitoh</strong></p>
<p>In the rapidly evolving field of artificial intelligence, machine learning emerges as a key technology characterized by its vast potential and inherent risks. The stability and reliability of these models are important, as they are frequent targets of security threats. Adversarial attacks, first rigorously defined by Ian Goodfellow et al. in 2013, highlight a critical vulnerability: they can trick machine learning models into making incorrect predictions by applying nearly invisible perturbations to images. Although many studies have focused on constructing sophisticated defensive mechanisms to mitigate such attacks, they often overlook the substantial time and computational costs of training and maintaining these models. Ideally, a defense method should be able to generalize across various, even unseen, adversarial attacks with minimal overhead. Building on our previous work on image-to-image translation-based defenses, this study introduces an improved model that incorporates residual blocks to enhance generalizability. The proposed method requires training only a single model, effectively defends against diverse attack types, and is well-transferable between different target models. Experiments show that our model can restore the classification accuracy from near zero to an average of 72% while maintaining competitive performance compared to state-of-the-art methods. </p>
<blockquote>
<p>åœ¨äººå·¥æ™ºèƒ½è¿™ä¸ªå¿«é€Ÿå‘å±•çš„é¢†åŸŸé‡Œï¼Œæœºå™¨å­¦ä¹ ä½œä¸ºä¸€ç§å…·æœ‰å·¨å¤§æ½œåŠ›å’Œå›ºæœ‰é£é™©çš„å…³é”®æŠ€æœ¯è€Œå´­éœ²å¤´è§’ã€‚è¿™äº›æ¨¡å‹çš„ç¨³å®šæ€§å’Œå¯é æ€§éå¸¸é‡è¦ï¼Œå› ä¸ºå®ƒä»¬ç»å¸¸å—åˆ°å®‰å…¨å¨èƒçš„ä¾µè¢­ã€‚Ian Goodfellowç­‰äººåœ¨2013å¹´é¦–æ¬¡ä¸¥æ ¼å®šä¹‰çš„å¯¹æŠ—æ€§æ”»å‡»ï¼Œå‡¸æ˜¾äº†ä¸€ä¸ªå…³é”®æ¼æ´ï¼šä»–ä»¬å¯ä»¥é€šè¿‡å¯¹å›¾åƒæ–½åŠ å‡ ä¹ä¸å¯è§çš„æ‰°åŠ¨ï¼Œæ¬ºéª—æœºå™¨å­¦ä¹ æ¨¡å‹åšå‡ºé”™è¯¯çš„é¢„æµ‹ã€‚å°½ç®¡è®¸å¤šç ”ç©¶è‡´åŠ›äºæ„å»ºå¤æ‚çš„é˜²å¾¡æœºåˆ¶æ¥å‡è½»è¿™ç±»æ”»å‡»ï¼Œä½†å®ƒä»¬å¾€å¾€å¿½è§†äº†è®­ç»ƒå’Œç»´æŒè¿™äº›æ¨¡å‹çš„å·¨å¤§æ—¶é—´å’Œè®¡ç®—æˆæœ¬ã€‚ç†æƒ³çš„é˜²å¾¡æ–¹æ³•åº”è¯¥èƒ½å¤Ÿåœ¨å„ç§ç”šè‡³æœªçŸ¥çš„å¯¹æŠ—æ€§æ”»å‡»ä¸­å…·æœ‰é€šç”¨æ€§ï¼Œå¹¶ä¸”å…·æœ‰æœ€å°çš„é¢å¤–å¼€é”€ã€‚æœ¬ç ”ç©¶åŸºäºæˆ‘ä»¬ä¹‹å‰åœ¨åŸºäºå›¾åƒåˆ°å›¾åƒç¿»è¯‘çš„é˜²å¾¡æ–¹é¢çš„å·¥ä½œï¼Œå¼•å…¥äº†ä¸€ä¸ªæ”¹è¿›æ¨¡å‹ï¼Œè¯¥æ¨¡å‹ç»“åˆäº†æ®‹å·®å—ä»¥å¢å¼ºé€šç”¨æ€§ã€‚æ‰€æå‡ºçš„æ–¹æ³•åªéœ€è¦è®­ç»ƒä¸€ä¸ªå•ä¸€æ¨¡å‹ï¼Œå°±èƒ½æœ‰æ•ˆåœ°é˜²å¾¡å¤šç§æ”»å‡»ç±»å‹ï¼Œå¹¶ä¸”åœ¨ä¸åŒçš„ç›®æ ‡æ¨¡å‹ä¹‹é—´å…·æœ‰è‰¯å¥½çš„å¯è¿ç§»æ€§ã€‚å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ¨¡å‹å¯ä»¥å°†åˆ†ç±»å‡†ç¡®ç‡ä»æ¥è¿‘é›¶æ¢å¤åˆ°å¹³å‡72%ï¼ŒåŒæ—¶ä¸æœ€æ–°æ–¹æ³•ç›¸æ¯”ä¿æŒç«äº‰åŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.01399v1">PDF</a> </p>
<p><strong>Summary</strong><br>     æœºå™¨å­¦ä¹ æ¨¡å‹åœ¨äººå·¥æ™ºèƒ½çš„å¿«é€Ÿå‘å±•ä¸­å±•ç°å‡ºå·¨å¤§çš„æ½œåŠ›å’Œå›ºæœ‰çš„é£é™©ã€‚æ¨¡å‹çš„ç¨³å®šæ€§å’Œå¯é æ€§å°¤ä¸ºé‡è¦ï¼Œå› ä¸ºå®ƒä»¬ç»å¸¸é¢ä¸´å®‰å…¨å¨èƒã€‚å¯¹æŠ—æ€§æ”»å‡»æ˜¯æœºå™¨å­¦ä¹ æ¨¡å‹çš„å…³é”®æ¼æ´ä¹‹ä¸€ï¼Œå¯ä»¥é€šè¿‡å¯¹å›¾åƒæ–½åŠ å‡ ä¹ä¸å¯è§çš„æ‰°åŠ¨æ¥è¯¯å¯¼æ¨¡å‹åšå‡ºé”™è¯¯çš„é¢„æµ‹ã€‚å°½ç®¡è®¸å¤šç ”ç©¶è‡´åŠ›äºæ„å»ºå¤æ‚çš„é˜²å¾¡æœºåˆ¶æ¥å‡è½»è¿™äº›æ”»å‡»ï¼Œä½†å®ƒä»¬å¾€å¾€å¿½ç•¥äº†è®­ç»ƒå’Œç»´æŒè¿™äº›æ¨¡å‹çš„å·¨å¤§æ—¶é—´å’Œè®¡ç®—æˆæœ¬ã€‚æœ¬ç ”ç©¶åŸºäºæˆ‘ä»¬åœ¨å›¾åƒåˆ°å›¾åƒç¿»è¯‘é˜²å¾¡æ–¹é¢çš„å·¥ä½œï¼Œæå‡ºäº†ä¸€ç§æ”¹è¿›çš„æ¨¡å‹ï¼Œè¯¥æ¨¡å‹ç»“åˆäº†æ®‹å·®å—ä»¥å¢å¼ºé€šç”¨æ€§ã€‚æ‰€æå‡ºçš„æ–¹æ³•åªéœ€è¦è®­ç»ƒä¸€ä¸ªæ¨¡å‹ï¼Œå¯ä»¥æœ‰æ•ˆåœ°é˜²å¾¡å¤šç§æ”»å‡»ç±»å‹ï¼Œå¹¶ä¸”åœ¨ä¸åŒçš„ç›®æ ‡æ¨¡å‹ä¹‹é—´å…·æœ‰è‰¯å¥½çš„å¯è½¬ç§»æ€§ã€‚å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ¨¡å‹å¯ä»¥å°†åˆ†ç±»å‡†ç¡®ç‡ä»æ¥è¿‘äºé›¶æ¢å¤åˆ°å¹³å‡72%ï¼ŒåŒæ—¶ä¸æœ€å…ˆè¿›çš„æ–¹æ³•ç›¸æ¯”ä¿æŒç«äº‰åŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æœºå™¨å­¦ä¹ åœ¨äººå·¥æ™ºèƒ½é¢†åŸŸå±•ç°å‡ºå·¨å¤§æ½œåŠ›ï¼Œä½†å­˜åœ¨å¯¹æŠ—æ€§æ”»å‡»çš„å®‰å…¨é£é™©ã€‚</li>
<li>å¯¹æŠ—æ€§æ”»å‡»èƒ½å¤Ÿé€šè¿‡å‡ ä¹ä¸å¯è§çš„æ‰°åŠ¨è¯¯å¯¼æœºå™¨å­¦ä¹ æ¨¡å‹åšå‡ºé”™è¯¯é¢„æµ‹ã€‚</li>
<li>è®¸å¤šé˜²å¾¡æœºåˆ¶çš„ç ”ç©¶å¿½ç•¥äº†è®­ç»ƒå’Œç»´æŒé˜²å¾¡æ¨¡å‹çš„å·¨å¤§æ—¶é—´å’Œè®¡ç®—æˆæœ¬ã€‚</li>
<li>æœ¬ç ”ç©¶åŸºäºå›¾åƒåˆ°å›¾åƒç¿»è¯‘é˜²å¾¡å·¥ä½œï¼Œæå‡ºäº†ä¸€ä¸ªæ”¹è¿›çš„æ¨¡å‹ã€‚</li>
<li>æ”¹è¿›æ¨¡å‹ç»“åˆäº†æ®‹å·®å—ä»¥å¢å¼ºé€šç”¨æ€§ï¼Œèƒ½é˜²å¾¡å¤šç§æ”»å‡»ç±»å‹ï¼Œå¹¶åœ¨ä¸åŒç›®æ ‡æ¨¡å‹é—´å…·æœ‰è‰¯å¥½çš„å¯è½¬ç§»æ€§ã€‚</li>
<li>å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¯¥æ¨¡å‹èƒ½å°†åˆ†ç±»å‡†ç¡®ç‡ä»è¿‘ä¹é›¶æ¢å¤åˆ°å¹³å‡72%ã€‚</li>
<li>è¯¥æ¨¡å‹çš„æ€§èƒ½ä¸å½“å‰æœ€å…ˆè¿›çš„æ–¹æ³•ç›¸æ¯”å…·æœ‰ç«äº‰åŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.01399">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-879568d77047b4f188a94666dd0c6288.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-56ba19ee851fd0b044661cc31ed9a0b2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-793722898546799bbf07ae7bea65fae8.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="UniFault-A-Fault-Diagnosis-Foundation-Model-from-Bearing-Data"><a href="#UniFault-A-Fault-Diagnosis-Foundation-Model-from-Bearing-Data" class="headerlink" title="UniFault: A Fault Diagnosis Foundation Model from Bearing Data"></a>UniFault: A Fault Diagnosis Foundation Model from Bearing Data</h2><p><strong>Authors:Emadeldeen Eldele, Mohamed Ragab, Xu Qing,  Edward, Zhenghua Chen, Min Wu, Xiaoli Li, Jay Lee</strong></p>
<p>Machine fault diagnosis (FD) is a critical task for predictive maintenance, enabling early fault detection and preventing unexpected failures. Despite its importance, existing FD models are operation-specific with limited generalization across diverse datasets. Foundation models (FM) have demonstrated remarkable potential in both visual and language domains, achieving impressive generalization capabilities even with minimal data through few-shot or zero-shot learning. However, translating these advances to FD presents unique hurdles. Unlike the large-scale, cohesive datasets available for images and text, FD datasets are typically smaller and more heterogeneous, with significant variations in sampling frequencies and the number of channels across different systems and applications. This heterogeneity complicates the design of a universal architecture capable of effectively processing such diverse data while maintaining robust feature extraction and learning capabilities. In this paper, we introduce UniFault, a foundation model for fault diagnosis that systematically addresses these issues. Specifically, the model incorporates a comprehensive data harmonization pipeline featuring two key innovations. First, a unification scheme transforms multivariate inputs into standardized univariate sequences while retaining local inter-channel relationships. Second, a novel cross-domain temporal fusion strategy mitigates distribution shifts and enriches sample diversity and count, improving the model generalization across varying conditions. UniFault is pretrained on over 9 billion data points spanning diverse FD datasets, enabling superior few-shot performance. Extensive experiments on real-world FD datasets demonstrate that UniFault achieves SoTA performance, setting a new benchmark for fault diagnosis models and paving the way for more scalable and robust predictive maintenance solutions. </p>
<blockquote>
<p>æœºå™¨æ•…éšœè¯Šæ–­ï¼ˆFDï¼‰æ˜¯é¢„æµ‹æ€§ç»´æŠ¤ä¸­çš„ä¸€é¡¹å…³é”®ä»»åŠ¡ï¼Œèƒ½å¤Ÿå®ç°æ—©æœŸæ•…éšœæ£€æµ‹å¹¶é˜²æ­¢æ„å¤–æ•…éšœã€‚å°½ç®¡å…¶é‡è¦æ€§ä¸è¨€è€Œå–»ï¼Œä½†ç°æœ‰çš„FDæ¨¡å‹éƒ½æ˜¯é’ˆå¯¹ç‰¹å®šæ“ä½œçš„ï¼Œåœ¨å¤šç§æ•°æ®é›†ä¹‹é—´çš„æ³›åŒ–èƒ½åŠ›æœ‰é™ã€‚åŸºç¡€æ¨¡å‹ï¼ˆFMï¼‰åœ¨è§†è§‰å’Œè¯­è¨€é¢†åŸŸéƒ½è¡¨ç°å‡ºäº†æ˜¾è‘—çš„ä¼˜åŠ¿ï¼Œå³ä½¿åœ¨å°‘é‡æ•°æ®çš„æƒ…å†µä¸‹ï¼Œä¹Ÿèƒ½é€šè¿‡å°æ ·æœ¬æˆ–é›¶æ ·æœ¬å­¦ä¹ å®ç°ä»¤äººå°è±¡æ·±åˆ»çš„æ³›åŒ–èƒ½åŠ›ã€‚ç„¶è€Œï¼Œå°†è¿™äº›è¿›å±•åº”ç”¨äºFDå´é¢ä¸´ç‹¬ç‰¹çš„éšœç¢ã€‚ä¸å¯ç”¨äºå›¾åƒå’Œæ–‡æœ¬çš„è§„æ¨¡åºå¤§ã€è¿è´¯çš„æ•°æ®é›†ä¸åŒï¼ŒFDæ•°æ®é›†é€šå¸¸è¾ƒå°ä¸”æ›´å¼‚è´¨ï¼Œä¸åŒç³»ç»Ÿå’Œåº”ç”¨ä¹‹é—´çš„é‡‡æ ·é¢‘ç‡å’Œæ•°æ®é€šé“æ•°é‡å­˜åœ¨é‡å¤§å·®å¼‚ã€‚è¿™ç§å¼‚è´¨æ€§ä½¿å¾—è®¾è®¡ä¸€ä¸ªèƒ½å¤Ÿå¤„ç†å¦‚æ­¤å¤šæ ·æ•°æ®çš„åŒæ—¶ä¿æŒç¨³å¥çš„ç‰¹å¾æå–å’Œå­¦ä¹ èƒ½åŠ›çš„é€šç”¨æ¶æ„å˜å¾—å¤æ‚ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬å¼•å…¥äº†UniFaultï¼Œè¿™æ˜¯ä¸€ä¸ªç”¨äºæ•…éšœè¯Šæ–­çš„åŸºç¡€æ¨¡å‹ï¼Œç³»ç»Ÿåœ°è§£å†³äº†è¿™äº›é—®é¢˜ã€‚å…·ä½“æ¥è¯´ï¼Œè¯¥æ¨¡å‹ç»“åˆäº†ä¸€ä¸ªå…¨é¢çš„æ•°æ®è°ƒå’Œç®¡é“ï¼ŒåŒ…å«ä¸¤ä¸ªå…³é”®çš„åˆ›æ–°ç‚¹ã€‚é¦–å…ˆï¼Œä¸€ä¸ªç»Ÿä¸€æ–¹æ¡ˆå°†å¤šå…ƒè¾“å…¥è½¬æ¢ä¸ºæ ‡å‡†åŒ–çš„å•å˜é‡åºåˆ—ï¼ŒåŒæ—¶ä¿ç•™å±€éƒ¨é€šé“é—´çš„å…³ç³»ã€‚å…¶æ¬¡ï¼Œä¸€ç§æ–°çš„è·¨åŸŸæ—¶é—´èåˆç­–ç•¥ç¼“è§£äº†åˆ†å¸ƒåç§»é—®é¢˜å¹¶ä¸°å¯Œäº†æ ·æœ¬çš„å¤šæ ·æ€§å’Œæ•°é‡ï¼Œæé«˜äº†æ¨¡å‹åœ¨ä¸åŒæ¡ä»¶ä¸‹çš„æ³›åŒ–èƒ½åŠ›ã€‚UniFaultåœ¨è·¨è¶Šå¤šç§FDæ•°æ®é›†çš„è¶…è¿‡9äº¿ä¸ªæ•°æ®ç‚¹ä¸Šè¿›è¡Œé¢„è®­ç»ƒï¼Œå®ç°äº†å“è¶Šçš„å°æ ·æœ¬æ€§èƒ½ã€‚åœ¨çœŸå®ä¸–ç•Œçš„FDæ•°æ®é›†ä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼ŒUniFaultè¾¾åˆ°äº†æœ€æ–°çš„æ€§èƒ½æ°´å¹³ï¼Œä¸ºæ•…éšœè¯Šæ–­æ¨¡å‹è®¾å®šäº†æ–°çš„åŸºå‡†ï¼Œå¹¶ä¸ºæ›´å¯æ‰©å±•å’Œç¨³å¥çš„é¢„æµ‹æ€§ç»´æŠ¤è§£å†³æ–¹æ¡ˆé“ºå¹³äº†é“è·¯ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.01373v1">PDF</a> </p>
<p><strong>æ‘˜è¦</strong></p>
<p>æœºå™¨æ•…éšœè¯Šæ–­ï¼ˆFDï¼‰æ˜¯é¢„æµ‹æ€§ç»´æŠ¤ä¸­çš„ä¸€é¡¹å…³é”®ä»»åŠ¡ï¼Œèƒ½å¤Ÿå®ç°å¯¹æ—©æœŸæ•…éšœçš„åŠæ—¶æ£€æµ‹ï¼Œé¿å…æ„å¤–åœæœºæƒ…å†µçš„å‡ºç°ã€‚è™½ç„¶æ•…éšœè¯Šæ–­æ¨¡å‹çš„å‘å±•éå¸¸é‡è¦ï¼Œä½†ç›®å‰å­˜åœ¨å¤§é‡çš„æ“ä½œç‰¹å®šæ¨¡å‹ï¼Œè¿™äº›æ¨¡å‹çš„è·¨æ•°æ®é›†æ³›åŒ–èƒ½åŠ›å—é™ã€‚åŸºç¡€æ¨¡å‹ï¼ˆFMï¼‰åœ¨è§†è§‰å’Œè¯­è¨€é¢†åŸŸå·²ç»å±•ç°å‡ºä»¤äººç©ç›®çš„æ½œåŠ›ï¼Œé€šè¿‡å°‘é‡æ ·æœ¬å­¦ä¹ æˆ–é›¶æ ·æœ¬å­¦ä¹ å³å¯å®ç°ä»¤äººå°è±¡æ·±åˆ»çš„æ³›åŒ–èƒ½åŠ›ã€‚ç„¶è€Œï¼Œå°†è¿™ä¸€è¿›å±•è½¬åŒ–ä¸ºæ•…éšœè¯Šæ–­é¢ä¸´ç€ç‹¬ç‰¹çš„æŒ‘æˆ˜ã€‚ä¸å¯ç”¨äºå›¾åƒå’Œæ–‡æœ¬çš„å¤§è§„æ¨¡ã€è¿è´¯æ•°æ®é›†ç›¸æ¯”ï¼Œæ•…éšœè¯Šæ–­æ•°æ®é›†é€šå¸¸æ›´å°ã€æ›´å¼‚è´¨ï¼Œä¸åŒç³»ç»Ÿå’Œåº”ç”¨ä¹‹é—´çš„é‡‡æ ·é¢‘ç‡å’Œæ•°æ®é€šé“æ•°é‡å­˜åœ¨æ˜¾è‘—å·®å¼‚ã€‚è¿™ç§å¼‚è´¨æ€§ä½¿å¾—è®¾è®¡ä¸€ç§èƒ½å¤Ÿå¤„ç†æ­¤ç±»å¤šæ ·æ•°æ®åŒæ—¶ä¿æŒç¨³å¥çš„ç‰¹å¾æå–å’Œå­¦ä¹ èƒ½åŠ›çš„é€šç”¨æ¶æ„å˜å¾—å¤æ‚ã€‚æœ¬æ–‡ä»‹ç»äº†ç”¨äºæ•…éšœè¯Šæ–­çš„åŸºç¡€æ¨¡å‹UniFaultï¼Œè¯¥æ¨¡å‹ç³»ç»Ÿåœ°è§£å†³äº†è¿™äº›é—®é¢˜ã€‚å…·ä½“æ¥è¯´ï¼Œè¯¥æ¨¡å‹åŒ…å«å…¨é¢çš„æ•°æ®è°ƒå’Œç®¡é“ï¼Œå…·æœ‰ä¸¤å¤§åˆ›æ–°ä¹‹å¤„ã€‚é¦–å…ˆï¼Œç»Ÿä¸€æ–¹æ¡ˆå°†å¤šå…ƒè¾“å…¥è½¬æ¢ä¸ºæ ‡å‡†åŒ–çš„å•å˜é‡åºåˆ—ï¼ŒåŒæ—¶ä¿ç•™å±€éƒ¨é€šé“é—´å…³ç³»ã€‚å…¶æ¬¡ï¼Œä¸€ç§æ–°çš„è·¨åŸŸæ—¶é—´èåˆç­–ç•¥ç¼“è§£äº†åˆ†å¸ƒåç§»é—®é¢˜å¹¶ä¸°å¯Œäº†æ ·æœ¬å¤šæ ·æ€§å’Œè®¡æ•°ï¼Œæé«˜äº†æ¨¡å‹åœ¨ä¸åŒæ¡ä»¶ä¸‹çš„æ³›åŒ–èƒ½åŠ›ã€‚UniFaultåœ¨è·¨è¶Šå¤šä¸ªæ•…éšœè¯Šæ–­æ•°æ®é›†è¶…è¿‡9äº¿ä¸ªæ•°æ®ç‚¹ä¸Šè¿›è¡Œé¢„è®­ç»ƒï¼Œå®ç°äº†å‡ºè‰²çš„å°‘é‡æ ·æœ¬æ€§èƒ½ã€‚åœ¨çœŸå®ä¸–ç•Œçš„æ•…éšœè¯Šæ–­æ•°æ®é›†ä¸Šè¿›è¡Œçš„å¤§é‡å®éªŒè¡¨æ˜ï¼ŒUniFaultè¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½æ°´å¹³ï¼Œä¸ºæ•…éšœè¯Šæ–­æ¨¡å‹è®¾å®šäº†æ–°çš„åŸºå‡†çº¿ï¼Œå¹¶ä¸ºæ›´å¯æ‰©å±•å’Œç¨³å¥çš„é¢„æµ‹æ€§ç»´æŠ¤è§£å†³æ–¹æ¡ˆé“ºå¹³äº†é“è·¯ã€‚</p>
<p><strong>è¦ç‚¹æç‚¼</strong></p>
<ol>
<li>æœºå™¨æ•…éšœè¯Šæ–­ï¼ˆFDï¼‰æ˜¯é¢„æµ‹æ€§ç»´æŠ¤ä¸­çš„æ ¸å¿ƒä»»åŠ¡ï¼Œè¦æ±‚æ—©æœŸæ•…éšœæ£€æµ‹å¹¶é¿å…æ„å¤–åœæœºã€‚</li>
<li>å½“å‰FDæ¨¡å‹æ“ä½œç‰¹å®šæ€§å¼ºï¼Œæ³›åŒ–èƒ½åŠ›å—é™ã€‚</li>
<li>åŸºç¡€æ¨¡å‹ï¼ˆFMï¼‰åœ¨è§†è§‰å’Œè¯­è¨€é¢†åŸŸå…·æœ‰å‡ºè‰²çš„æ³›åŒ–èƒ½åŠ›ã€‚</li>
<li>å°†FMåº”ç”¨äºFDé¢ä¸´æ•°æ®é›†å°ã€å¼‚è´¨æ€§çš„æŒ‘æˆ˜ã€‚</li>
<li>UniFaultæ¨¡å‹é€šè¿‡æ•°æ®è°ƒå’Œç®¡é“è§£å†³è¿™äº›é—®é¢˜ï¼ŒåŒ…æ‹¬è¾“å…¥æ•°æ®çš„ç»Ÿä¸€è½¬æ¢å’Œè·¨åŸŸæ—¶é—´èåˆç­–ç•¥ã€‚</li>
<li>UniFaultæ¨¡å‹åœ¨å¤§é‡FDæ•°æ®é›†ä¸Šè¿›è¡Œé¢„è®­ç»ƒï¼Œå®ç°ä¼˜ç§€å°‘é‡æ ·æœ¬æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.01373">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-ebf09123c7ce6dcde9c49d0f45ea7984.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-cb2f867e8850b4fa95c5225151462878.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-da8a894b53c9c78b2a1e2f39065388cd.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Direction-Aware-Hybrid-Representation-Learning-for-3D-Hand-Pose-and-Shape-Estimation"><a href="#Direction-Aware-Hybrid-Representation-Learning-for-3D-Hand-Pose-and-Shape-Estimation" class="headerlink" title="Direction-Aware Hybrid Representation Learning for 3D Hand Pose and   Shape Estimation"></a>Direction-Aware Hybrid Representation Learning for 3D Hand Pose and   Shape Estimation</h2><p><strong>Authors:Shiyong Liu, Zhihao Li, Xiao Tang, Jianzhuang Liu</strong></p>
<p>Most model-based 3D hand pose and shape estimation methods directly regress the parametric model parameters from an image to obtain 3D joints under weak supervision. However, these methods involve solving a complex optimization problem with many local minima, making training difficult. To address this challenge, we propose learning direction-aware hybrid features (DaHyF) that fuse implicit image features and explicit 2D joint coordinate features. This fusion is enhanced by the pixel direction information in the camera coordinate system to estimate pose, shape, and camera viewpoint. Our method directly predicts 3D hand poses with DaHyF representation and reduces jittering during motion capture using prediction confidence based on contrastive learning. We evaluate our method on the FreiHAND dataset and show that it outperforms existing state-of-the-art methods by more than 33% in accuracy. DaHyF also achieves the top ranking on both the HO3Dv2 and HO3Dv3 leaderboards for the metric of Mean Joint Error (after scale and translation alignment). Compared to the second-best results, the largest improvement observed is 10%. We also demonstrate its effectiveness in real-time motion capture scenarios with hand position variability, occlusion, and motion blur. </p>
<blockquote>
<p>å¤§å¤šæ•°åŸºäºæ¨¡å‹çš„3Dæ‰‹åŠ¿å§¿æ€å’Œå½¢çŠ¶ä¼°è®¡æ–¹æ³•ç›´æ¥é€šè¿‡å›¾åƒå›å½’å‚æ•°æ¨¡å‹å‚æ•°ï¼Œä»¥åœ¨å¼±ç›‘ç£ä¸‹è·å¾—3Då…³èŠ‚ã€‚ç„¶è€Œï¼Œè¿™äº›æ–¹æ³•éœ€è¦è§£å†³å…·æœ‰å¤šä¸ªå±€éƒ¨æœ€å°å€¼çš„å¤æ‚ä¼˜åŒ–é—®é¢˜ï¼Œä½¿å¾—è®­ç»ƒå˜å¾—å›°éš¾ã€‚ä¸ºäº†åº”å¯¹è¿™ä¸€æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†å­¦ä¹ æ–¹å‘æ„ŸçŸ¥æ··åˆç‰¹å¾ï¼ˆDaHyFï¼‰ï¼Œè¯¥ç‰¹å¾èåˆäº†éšå¼å›¾åƒç‰¹å¾å’Œæ˜¾å¼çš„2Då…³èŠ‚åæ ‡ç‰¹å¾ã€‚è¿™ç§èåˆé€šè¿‡ç›¸æœºåæ ‡ç³»ä¸­çš„åƒç´ æ–¹å‘ä¿¡æ¯æ¥å¢å¼ºï¼Œä»¥ä¼°è®¡å§¿æ€ã€å½¢çŠ¶å’Œç›¸æœºè§†ç‚¹ã€‚æˆ‘ä»¬çš„æ–¹æ³•ç›´æ¥ä½¿ç”¨DaHyFè¡¨ç¤ºé¢„æµ‹3Dæ‰‹åŠ¿å§¿æ€ï¼Œå¹¶åˆ©ç”¨åŸºäºå¯¹æ¯”å­¦ä¹ çš„é¢„æµ‹ç½®ä¿¡åº¦å‡å°‘è¿åŠ¨æ•æ‰è¿‡ç¨‹ä¸­çš„æŠ–åŠ¨ã€‚æˆ‘ä»¬åœ¨FreiHANDæ•°æ®é›†ä¸Šè¯„ä¼°äº†æˆ‘ä»¬çš„æ–¹æ³•ï¼Œç»“æœè¡¨æ˜ï¼Œåœ¨å‡†ç¡®æ€§æ–¹é¢ï¼Œå®ƒæ¯”ç°æœ‰æœ€å…ˆè¿›çš„æ–¹æ³•é«˜å‡º33%ä»¥ä¸Šã€‚DaHyFåœ¨HO3Dv2å’ŒHO3Dv3æ’è¡Œæ¦œä¸Šçš„å¹³å‡å…³èŠ‚è¯¯å·®æŒ‡æ ‡æ–¹é¢ä¹Ÿè·å¾—äº†æ’åç¬¬ä¸€ã€‚ä¸ç¬¬äºŒå¥½çš„ç»“æœç›¸æ¯”ï¼Œè§‚å¯Ÿåˆ°çš„æœ€å¤§æ”¹è¿›æ˜¯10%ã€‚æˆ‘ä»¬è¿˜è¯æ˜äº†å®ƒåœ¨å…·æœ‰æ‰‹éƒ¨ä½ç½®å˜åŒ–ã€é®æŒ¡å’Œè¿åŠ¨æ¨¡ç³Šçš„å®æ—¶è¿åŠ¨æ•æ‰åœºæ™¯ä¸­çš„æœ‰æ•ˆæ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.01298v1">PDF</a> Accepted to CVPR 2025 workshop</p>
<p><strong>Summary</strong><br>æ‰‹å§¿ä¸‰ç»´å§¿æ€åŠå½¢çŠ¶ä¼°ç®—ä¸­ï¼Œè®¸å¤šæ¨¡å‹åŸºäºå›å½’å›¾åƒå‚æ•°åŒ–æ¨¡å‹å‚æ•°æ¥å¾—åˆ°ä¸‰ç»´å…³èŠ‚ç‚¹ï¼Œä½†å­˜åœ¨è®­ç»ƒå›°éš¾çš„é—®é¢˜ã€‚ä¸ºè§£å†³æ­¤é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºå­¦ä¹ æ–¹å‘æ„ŸçŸ¥æ··åˆç‰¹å¾ï¼ˆDaHyFï¼‰ï¼Œèåˆéšæ€§å›¾åƒç‰¹å¾å’Œæ˜¾æ€§äºŒç»´å…³èŠ‚åæ ‡ç‰¹å¾ï¼Œå¹¶ç»“åˆç›¸æœºåæ ‡ç³»ä¸­çš„åƒç´ æ–¹å‘ä¿¡æ¯æ¥ä¼°ç®—å§¿æ€ã€å½¢çŠ¶å’Œç›¸æœºè§†è§’ã€‚è¯¥æ–¹æ³•å¯ç›´æ¥é¢„æµ‹ä¸‰ç»´æ‰‹å§¿æ€ï¼Œé€šè¿‡å¯¹æ¯”å­¦ä¹ å‡å°‘è¿åŠ¨æ•æ‰æ—¶çš„æŠ–åŠ¨ã€‚åœ¨FreiHANDæ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œå…¶å‡†ç¡®åº¦é«˜äºç°æœ‰å…ˆè¿›æŠ€æœ¯è¶…è¿‡33%ï¼Œåœ¨HO3Dv2å’ŒHO3Dv3æ’è¡Œæ¦œä¸Šçš„å¹³å‡å…³èŠ‚è¯¯å·®æ’åä¹Ÿä½å±…å‰åˆ—ã€‚å¯¹å®æ—¶è¿åŠ¨æ•æ‰åœºæ™¯ä¸‹çš„æ‰‹éƒ¨ä½ç½®å˜åŒ–ã€é®æŒ¡å’Œè¿åŠ¨æ¨¡ç³Šç­‰æƒ…å†µè¿›è¡Œäº†éªŒè¯ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æå‡ºå­¦ä¹ æ–¹å‘æ„ŸçŸ¥æ··åˆç‰¹å¾ï¼ˆDaHyFï¼‰ä»¥èåˆéšæ€§å›¾åƒç‰¹å¾å’Œæ˜¾æ€§äºŒç»´å…³èŠ‚åæ ‡ç‰¹å¾ã€‚</li>
<li>DaHyFç»“åˆåƒç´ æ–¹å‘ä¿¡æ¯åœ¨ç›¸æœºåæ ‡ç³»ä¸­ä¼°ç®—å§¿æ€ã€å½¢çŠ¶å’Œç›¸æœºè§†è§’ã€‚</li>
<li>æ–¹æ³•èƒ½å‡å°‘è¿åŠ¨æ•æ‰æ—¶çš„æŠ–åŠ¨é—®é¢˜ï¼Œå¢å¼ºé¢„æµ‹çš„å‡†ç¡®æ€§ã€‚</li>
<li>åœ¨FreiHANDæ•°æ®é›†ä¸Šçš„å‡†ç¡®åº¦æ˜¾è‘—æé«˜ï¼Œå‡†ç¡®ç‡è¶…è¶Šç°æœ‰æŠ€æœ¯è‡³å°‘33%ã€‚</li>
<li>DaHyFåœ¨HO3Dv2å’ŒHO3Dv3æ’è¡Œæ¦œä¸Šçš„å¹³å‡å…³èŠ‚è¯¯å·®æ’åååˆ—å‰èŒ…ï¼Œç›¸å¯¹ç¬¬äºŒå¥½çš„ç»“æœæœ€é«˜æå‡äº†10%ã€‚</li>
<li>DaHyFé€‚ç”¨äºå¤šç§çœŸå®åœºæ™¯ï¼ŒåŒ…æ‹¬æ‰‹éƒ¨ä½ç½®å˜åŒ–ã€é®æŒ¡å’Œè¿åŠ¨æ¨¡ç³Šç­‰æƒ…å†µã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.01298">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-c9d3c69d0ba4984ca90510dedc43e0e6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-413cd2fdc4ae6524fd5c6829490622a5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-54f1f1ae4405acc071da7fc23b96ad01.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8dc32e49673f5cac92e700ce91690395.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-52159398b69a3ded68affc4d62ee80d8.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="AnimeGamer-Infinite-Anime-Life-Simulation-with-Next-Game-State-Prediction"><a href="#AnimeGamer-Infinite-Anime-Life-Simulation-with-Next-Game-State-Prediction" class="headerlink" title="AnimeGamer: Infinite Anime Life Simulation with Next Game State   Prediction"></a>AnimeGamer: Infinite Anime Life Simulation with Next Game State   Prediction</h2><p><strong>Authors:Junhao Cheng, Yuying Ge, Yixiao Ge, Jing Liao, Ying Shan</strong></p>
<p>Recent advancements in image and video synthesis have opened up new promise in generative games. One particularly intriguing application is transforming characters from anime films into interactive, playable entities. This allows players to immerse themselves in the dynamic anime world as their favorite characters for life simulation through language instructions. Such games are defined as infinite game since they eliminate predetermined boundaries and fixed gameplay rules, where players can interact with the game world through open-ended language and experience ever-evolving storylines and environments. Recently, a pioneering approach for infinite anime life simulation employs large language models (LLMs) to translate multi-turn text dialogues into language instructions for image generation. However, it neglects historical visual context, leading to inconsistent gameplay. Furthermore, it only generates static images, failing to incorporate the dynamics necessary for an engaging gaming experience. In this work, we propose AnimeGamer, which is built upon Multimodal Large Language Models (MLLMs) to generate each game state, including dynamic animation shots that depict character movements and updates to character states, as illustrated in Figure 1. We introduce novel action-aware multimodal representations to represent animation shots, which can be decoded into high-quality video clips using a video diffusion model. By taking historical animation shot representations as context and predicting subsequent representations, AnimeGamer can generate games with contextual consistency and satisfactory dynamics. Extensive evaluations using both automated metrics and human evaluations demonstrate that AnimeGamer outperforms existing methods in various aspects of the gaming experience. Codes and checkpoints are available at <a target="_blank" rel="noopener" href="https://github.com/TencentARC/AnimeGamer">https://github.com/TencentARC/AnimeGamer</a>. </p>
<blockquote>
<p>æœ€è¿‘å›¾åƒå’Œè§†é¢‘åˆæˆçš„è¿›å±•ä¸ºç”Ÿæˆæ¸¸æˆå¸¦æ¥äº†æ–°çš„å‰æ™¯ã€‚ä¸€ä¸ªç‰¹åˆ«å¸å¼•äººçš„åº”ç”¨æ˜¯å°†åŠ¨æ¼«ç”µå½±ä¸­çš„è§’è‰²è½¬å˜ä¸ºå¯äº’åŠ¨çš„æ¸¸æˆå®ä½“ã€‚è¿™å…è®¸ç©å®¶é€šè¿‡è¯­è¨€æŒ‡ä»¤æ²‰æµ¸åœ¨åŠ¨æ€çš„åŠ¨æ¼«ä¸–ç•Œä¸­ï¼Œæ‰®æ¼”ä»–ä»¬æœ€å–œæ¬¢çš„è§’è‰²è¿›è¡Œç”Ÿæ´»æ¨¡æ‹Ÿã€‚è¿™ç±»æ¸¸æˆè¢«å®šä¹‰ä¸ºæ— é™æ¸¸æˆï¼Œå› ä¸ºå®ƒä»¬æ¶ˆé™¤äº†é¢„å®šçš„è¾¹ç•Œå’Œå›ºå®šçš„æ¸¸æˆè§„åˆ™ï¼Œç©å®¶å¯ä»¥é€šè¿‡å¼€æ”¾å¼çš„è¯­è¨€ä¸æ¸¸æˆä¸–ç•Œäº’åŠ¨ï¼Œä½“éªŒä¸æ–­æ¼”å˜çš„æ•…äº‹æƒ…èŠ‚å’Œç¯å¢ƒã€‚æœ€è¿‘ï¼Œä¸€ç§ç”¨äºæ— é™åŠ¨æ¼«ç”Ÿæ´»æ¨¡æ‹Ÿçš„å¼€åˆ›æ€§æ–¹æ³•é‡‡ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å°†å¤šè½®æ–‡æœ¬å¯¹è¯ç¿»è¯‘ä¸ºå›¾åƒç”Ÿæˆçš„è¯­è¨€æŒ‡ä»¤ã€‚ç„¶è€Œï¼Œå®ƒå¿½ç•¥äº†å†å²è§†è§‰ä¸Šä¸‹æ–‡ï¼Œå¯¼è‡´æ¸¸æˆä½“éªŒä¸ä¸€è‡´ã€‚æ­¤å¤–ï¼Œå®ƒåªèƒ½ç”Ÿæˆé™æ€å›¾åƒï¼Œæ— æ³•èå…¥åŠ¨æ€å…ƒç´ ï¼Œæ— æ³•æä¾›å¼•äººå…¥èƒœçš„æ¸¸æˆä½“éªŒã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†AnimeGamerï¼Œå®ƒåŸºäºå¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰æ¥ç”Ÿæˆæ¯ç§æ¸¸æˆçŠ¶æ€ï¼ŒåŒ…æ‹¬æç»˜è§’è‰²åŠ¨ä½œå’ŒçŠ¶æ€æ›´æ–°çš„åŠ¨æ€åŠ¨ç”»é•œå¤´ï¼Œå¦‚å›¾1æ‰€ç¤ºã€‚æˆ‘ä»¬å¼•å…¥äº†æ–°å‹çš„åŠ¨ä½œæ„ŸçŸ¥å¤šæ¨¡æ€è¡¨ç¤ºæ³•æ¥è¡¨ç¤ºåŠ¨ç”»é•œå¤´ï¼Œå¯ä»¥ä½¿ç”¨è§†é¢‘æ‰©æ•£æ¨¡å‹å°†å…¶è§£ç ä¸ºé«˜è´¨é‡çš„è§†é¢‘ç‰‡æ®µã€‚é€šè¿‡è·å–å†å²åŠ¨ç”»é•œå¤´è¡¨ç¤ºä½œä¸ºä¸Šä¸‹æ–‡å¹¶é¢„æµ‹éšåçš„è¡¨ç¤ºï¼ŒAnimeGamerå¯ä»¥ç”Ÿæˆå…·æœ‰ä¸Šä¸‹æ–‡ä¸€è‡´æ€§å’Œä»¤äººæ»¡æ„çš„åŠ¨åŠ›å­¦ç‰¹æ€§çš„æ¸¸æˆã€‚ä½¿ç”¨è‡ªåŠ¨åŒ–æŒ‡æ ‡å’Œäººç±»è¯„ä¼°çš„å¹¿æ³›è¯„ä¼°è¡¨æ˜ï¼ŒAnimeGameråœ¨æ¸¸æˆä½“éªŒçš„å„ä¸ªæ–¹é¢éƒ½ä¼˜äºç°æœ‰æ–¹æ³•ã€‚ä»£ç å’Œæ£€æŸ¥ç‚¹å¯ç”¨äº<a target="_blank" rel="noopener" href="https://github.com/TencentARC/AnimeGamer%E3%80%82">https://github.com/TencentARC/AnimeGamerã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.01014v1">PDF</a> Project released at: <a target="_blank" rel="noopener" href="https://howe125.github.io/AnimeGamer.github.io/">https://howe125.github.io/AnimeGamer.github.io/</a></p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†æœ€æ–°å›¾åƒå’Œè§†é¢‘åˆæˆæŠ€æœ¯åœ¨ç”Ÿæˆæ¸¸æˆé¢†åŸŸçš„åº”ç”¨ï¼Œç‰¹åˆ«æ˜¯åœ¨å°†åŠ¨æ¼«è§’è‰²è½¬åŒ–ä¸ºå¯äº’åŠ¨å®ä½“æ–¹é¢çš„åº”ç”¨ã€‚æ¸¸æˆç©å®¶å¯ä»¥é€šè¿‡è¯­è¨€æŒ‡ä»¤æ²‰æµ¸äºåŠ¨æ€çš„åŠ¨æ¼«ä¸–ç•Œï¼Œå¹¶æ‰®æ¼”ä»–ä»¬æœ€å–œæ¬¢çš„è§’è‰²è¿›è¡Œç”Ÿæ´»æ¨¡æ‹Ÿã€‚æå‡ºäº†ä¸€ç§åŸºäºå¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰æ„å»ºçš„AnimeGamerç³»ç»Ÿï¼Œèƒ½å¤Ÿç”Ÿæˆæ¸¸æˆçŠ¶æ€ï¼ŒåŒ…æ‹¬åŠ¨æ€åŠ¨ç”»é•œå¤´å’Œè§’è‰²çŠ¶æ€æ›´æ–°ã€‚é€šè¿‡å¼•å…¥æ–°å‹çš„åŠ¨ä½œæ„ŸçŸ¥å¤šæ¨¡æ€è¡¨ç¤ºæ–¹æ³•ï¼Œå¹¶ç»“åˆè§†é¢‘æ‰©æ•£æ¨¡å‹ï¼Œç”Ÿæˆçš„æ¸¸æˆå…·æœ‰ä¸Šä¸‹æ–‡ä¸€è‡´æ€§å’Œæ»¡æ„çš„åŠ¨æ€æ•ˆæœã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æœ€æ–°å›¾åƒå’Œè§†é¢‘åˆæˆæŠ€æœ¯ä¸ºç”Ÿæˆæ¸¸æˆé¢†åŸŸå¸¦æ¥æ–°å¸Œæœ›ã€‚</li>
<li>å°†åŠ¨æ¼«è§’è‰²è½¬åŒ–ä¸ºå¯äº’åŠ¨å®ä½“æ˜¯ä¸€ç§å¼•äººå…¥èƒœçš„åº”ç”¨ã€‚</li>
<li>ç©å®¶å¯é€šè¿‡è¯­è¨€æŒ‡ä»¤æ²‰æµ¸äºåŠ¨æ¼«ä¸–ç•Œï¼Œæ‰®æ¼”å–œæ¬¢çš„è§’è‰²è¿›è¡Œç”Ÿæ´»æ¨¡æ‹Ÿã€‚</li>
<li>æå‡ºäº†ä¸€ç§åŸºäºå¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰çš„AnimeGamerç³»ç»Ÿã€‚</li>
<li>AnimeGamerèƒ½ç”ŸæˆåŒ…æ‹¬åŠ¨æ€åŠ¨ç”»é•œå¤´çš„æ¸¸æˆçŠ¶æ€ã€‚</li>
<li>é€šè¿‡å¼•å…¥åŠ¨ä½œæ„ŸçŸ¥å¤šæ¨¡æ€è¡¨ç¤ºæ–¹æ³•ï¼Œç»“åˆè§†é¢‘æ‰©æ•£æ¨¡å‹ï¼Œå®ç°æ¸¸æˆçš„ä¸Šä¸‹æ–‡ä¸€è‡´æ€§å’ŒåŠ¨æ€æ•ˆæœã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.01014">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-cd0ebf6fe3158be837dc673128def6b7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8fd29e104e47e0ff54d6807a15db67d9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-696e28d03c6550c4c5c82861b2ad2d4e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-27966b67be617eeb22bb1d6a7c5065e3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3f3546a6de895403e043b8e44e47447f.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-2cef586b07bbc87c8a676a1da28206c4.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="SCFANet-Style-Distribution-Constraint-Feature-Alignment-Network-For-Pathological-Staining-Translation"><a href="#SCFANet-Style-Distribution-Constraint-Feature-Alignment-Network-For-Pathological-Staining-Translation" class="headerlink" title="SCFANet: Style Distribution Constraint Feature Alignment Network For   Pathological Staining Translation"></a>SCFANet: Style Distribution Constraint Feature Alignment Network For   Pathological Staining Translation</h2><p><strong>Authors:Zetong Chen, Yuzhuo Chen, Hai Zhong, Xu Qiao</strong></p>
<p>Immunohistochemical (IHC) staining serves as a valuable technique for detecting specific antigens or proteins through antibody-mediated visualization. However, the IHC staining process is both time-consuming and costly. To address these limitations, the application of deep learning models for direct translation of cost-effective Hematoxylin and Eosin (H&amp;E) stained images into IHC stained images has emerged as an efficient solution. Nevertheless, the conversion from H&amp;E to IHC images presents significant challenges, primarily due to alignment discrepancies between image pairs and the inherent diversity in IHC staining style patterns. To overcome these challenges, we propose the Style Distribution Constraint Feature Alignment Network (SCFANet), which incorporates two innovative modules: the Style Distribution Constrainer (SDC) and Feature Alignment Learning (FAL). The SDC ensures consistency between the generated and target imagesâ€™ style distributions while integrating cycle consistency loss to maintain structural consistency. To mitigate the complexity of direct image-to-image translation, the FAL module decomposes the end-to-end translation task into two subtasks: image reconstruction and feature alignment. Furthermore, we ensure pathological consistency between generated and target images by maintaining pathological pattern consistency and Optical Density (OD) uniformity. Extensive experiments conducted on the Breast Cancer Immunohistochemical (BCI) dataset demonstrate that our SCFANet model outperforms existing methods, achieving precise transformation of H&amp;E-stained images into their IHC-stained counterparts. The proposed approach not only addresses the technical challenges in H&amp;E to IHC image translation but also provides a robust framework for accurate and efficient stain conversion in pathological analysis. </p>
<blockquote>
<p>å…ç–«ç»„ç»‡åŒ–å­¦ï¼ˆIHCï¼‰æŸ“è‰²ä½œä¸ºä¸€ç§é€šè¿‡æŠ—ä½“ä»‹å¯¼çš„å¯è§†åŒ–æ£€æµ‹ç‰¹å®šæŠ—åŸæˆ–è›‹ç™½è´¨çš„æœ‰ä»·å€¼çš„æŠ€æœ¯ã€‚ç„¶è€Œï¼ŒIHCæŸ“è‰²è¿‡ç¨‹æ—¢è€—æ—¶åˆæˆæœ¬é«˜æ˜‚ã€‚ä¸ºäº†è§£å†³è¿™äº›å±€é™æ€§ï¼Œåº”ç”¨æ·±åº¦å­¦ä¹ æ¨¡å‹å°†æˆæœ¬æ•ˆç›Šé«˜çš„è‹æœ¨ç²¾å’Œä¼Šçº¢ï¼ˆH&amp;Eï¼‰æŸ“è‰²å›¾åƒç›´æ¥ç¿»è¯‘æˆIHCæŸ“è‰²å›¾åƒå·²æˆä¸ºä¸€ç§æœ‰æ•ˆçš„è§£å†³æ–¹æ¡ˆã€‚ç„¶è€Œï¼Œä»H&amp;Eåˆ°IHCå›¾åƒçš„è½¬æ¢å­˜åœ¨é‡å¤§æŒ‘æˆ˜ï¼Œä¸»è¦æ˜¯ç”±äºå›¾åƒå¯¹ä¹‹é—´çš„å¯¹é½å·®å¼‚å’ŒIHCæŸ“è‰²é£æ ¼æ¨¡å¼çš„å›ºæœ‰å¤šæ ·æ€§ã€‚ä¸ºäº†å…‹æœè¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†é£æ ¼åˆ†å¸ƒçº¦æŸç‰¹å¾å¯¹é½ç½‘ç»œï¼ˆSCFANetï¼‰ï¼Œå®ƒåŒ…å«ä¸¤ä¸ªåˆ›æ–°æ¨¡å—ï¼šé£æ ¼åˆ†å¸ƒçº¦æŸå™¨ï¼ˆSDCï¼‰å’Œç‰¹å¾å¯¹é½å­¦ä¹ ï¼ˆFALï¼‰ã€‚SDCç¡®ä¿ç”Ÿæˆå›¾åƒå’Œç›®æ ‡å›¾åƒä¹‹é—´é£æ ¼åˆ†å¸ƒçš„ä¸€è‡´æ€§ï¼ŒåŒæ—¶ç»“åˆå¾ªç¯ä¸€è‡´æ€§æŸå¤±ä»¥ä¿æŒç»“æ„ä¸€è‡´æ€§ã€‚ä¸ºäº†å‡è½»ç›´æ¥å›¾åƒåˆ°å›¾åƒç¿»è¯‘çš„å¤æ‚æ€§ï¼ŒFALæ¨¡å—å°†ç«¯åˆ°ç«¯çš„ç¿»è¯‘ä»»åŠ¡åˆ†è§£ä¸ºä¸¤ä¸ªå­ä»»åŠ¡ï¼šå›¾åƒé‡å»ºå’Œç‰¹å¾å¯¹é½ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬é€šè¿‡ä¿æŒç—…ç†æ¨¡å¼ä¸€è‡´æ€§å’Œå…‰å­¦å¯†åº¦ï¼ˆODï¼‰å‡åŒ€æ€§ï¼Œç¡®ä¿ç”Ÿæˆå›¾åƒå’Œç›®æ ‡å›¾åƒä¹‹é—´çš„ç—…ç†ä¸€è‡´æ€§ã€‚åœ¨ä¹³è…ºç™Œå…ç–«ç»„ç»‡åŒ–å­¦ï¼ˆBCIï¼‰æ•°æ®é›†ä¸Šè¿›è¡Œçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„SCFANetæ¨¡å‹ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œå®ç°äº†H&amp;EæŸ“è‰²å›¾åƒåˆ°IHCæŸ“è‰²å›¾åƒçš„ç²¾ç¡®è½¬æ¢ã€‚è¯¥æ–¹æ³•ä¸ä»…è§£å†³äº†H&amp;Eåˆ°IHCå›¾åƒè½¬æ¢çš„æŠ€æœ¯æŒ‘æˆ˜ï¼Œè€Œä¸”ä¸ºç—…ç†åˆ†æä¸­çš„å‡†ç¡®é«˜æ•ˆæŸ“è‰²è½¬æ¢æä¾›äº†ç¨³å¥çš„æ¡†æ¶ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.00490v1">PDF</a> </p>
<p><strong>Summary</strong><br>IHCæŸ“è‰²æŠ€æœ¯ç”¨äºæ£€æµ‹ç‰¹å®šæŠ—åŸæˆ–è›‹ç™½è´¨ï¼Œä½†è¿‡ç¨‹è€—æ—¶ä¸”æˆæœ¬é«˜æ˜‚ã€‚ä¸ºè§£å†³è¿™äº›é—®é¢˜ï¼Œç ”ç©¶è€…åˆ©ç”¨æ·±åº¦å­¦ä¹ æ¨¡å‹å°†ç»æµé«˜æ•ˆçš„H&amp;EæŸ“è‰²å›¾åƒç›´æ¥è½¬æ¢ä¸ºIHCæŸ“è‰²å›¾åƒã€‚ç„¶è€Œï¼Œè½¬æ¢è¿‡ç¨‹ä¸­å­˜åœ¨å¯¹é½å·®å¼‚å’ŒIHCæŸ“è‰²é£æ ¼æ¨¡å¼å¤šæ ·æ€§ç­‰æŒ‘æˆ˜ã€‚ä¸ºæ­¤ï¼Œæå‡ºSCFANetæ¨¡å‹ï¼ŒåŒ…æ‹¬SDCå’ŒFALä¸¤ä¸ªåˆ›æ–°æ¨¡å—ï¼Œç¡®ä¿é£æ ¼åˆ†å¸ƒå’Œç»“æ„æ€§ä¸€è‡´æ€§ï¼Œå¹¶åˆ†è§£ä¸ºå›¾åƒé‡å»ºå’Œç‰¹å¾å¯¹é½ä¸¤ä¸ªå­ä»»åŠ¡ã€‚åœ¨ä¹³è…ºç™Œå…ç–«ç»„ç»‡åŒ–å­¦æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒSCFANetæ¨¡å‹ç²¾ç¡®è½¬æ¢H&amp;EæŸ“è‰²å›¾åƒä¸ºIHCæŸ“è‰²å›¾åƒï¼Œæä¾›ç¨³å¥çš„æ¡†æ¶è¿›è¡Œç—…ç†åˆ†æä¸­çš„æŸ“è‰²è½¬æ¢ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>IHCæŸ“è‰²æ˜¯æ£€æµ‹ç‰¹å®šæŠ—åŸæˆ–è›‹ç™½è´¨çš„é‡è¦æŠ€æœ¯ï¼Œä½†å­˜åœ¨æ—¶é—´å’Œæˆæœ¬é—®é¢˜ã€‚</li>
<li>æ·±åº¦å­¦ä¹ æ¨¡å‹å¯ç”¨äºå°†H&amp;EæŸ“è‰²å›¾åƒè½¬æ¢ä¸ºIHCæŸ“è‰²å›¾åƒï¼Œæé«˜æ•ˆç‡ã€‚</li>
<li>è½¬æ¢è¿‡ç¨‹ä¸­é¢ä¸´å›¾åƒå¯¹é½å’ŒIHCæŸ“è‰²é£æ ¼å¤šæ ·æ€§çš„æŒ‘æˆ˜ã€‚</li>
<li>SCFANetæ¨¡å‹é€šè¿‡SDCå’ŒFALæ¨¡å—å…‹æœè¿™äº›æŒ‘æˆ˜ï¼Œç¡®ä¿é£æ ¼åˆ†å¸ƒå’Œç»“æ„æ€§ä¸€è‡´æ€§ã€‚</li>
<li>SCFANetå°†è½¬æ¢ä»»åŠ¡åˆ†è§£ä¸ºå›¾åƒé‡å»ºå’Œç‰¹å¾å¯¹é½ä¸¤ä¸ªå­ä»»åŠ¡ã€‚</li>
<li>é€šè¿‡ç»´æŒç—…ç†æ€§æ¨¡å¼ä¸€è‡´æ€§å’Œå…‰å­¦å¯†åº¦å‡åŒ€æ€§ï¼Œç¡®ä¿ç—…ç†ä¸€è‡´æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.00490">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-a6c77991d1297560af1521e98891059e.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Multiview-Image-Based-Localization"><a href="#Multiview-Image-Based-Localization" class="headerlink" title="Multiview Image-Based Localization"></a>Multiview Image-Based Localization</h2><p><strong>Authors:Cameron Fiore, Hongyi Fan, Benjamin Kimia</strong></p>
<p>The image retrieval (IR) approach to image localization has distinct advantages to the 3D and the deep learning (DNN) approaches: it is seen-agnostic, simpler to implement and use, has no privacy issues, and is computationally efficient. The main drawback of this approach is relatively poor localization in both position and orientation of the query camera when compared to the competing approaches. This paper represents a hybrid approach that stores only image features in the database like some IR methods, but relies on a latent 3D reconstruction, like 3D methods but without retaining a 3D scene reconstruction. The approach is based on two ideas: {\em (i)} a novel proposal where query camera center estimation relies only on relative translation estimates but not relative rotation estimates through a decoupling of the two, and {\em (ii)} a shift from computing optimal pose from estimated relative pose to computing optimal pose from multiview correspondences, thus cutting out the &#96;&#96;middle-manâ€™â€™. Our approach shows improved performance on the 7-Scenes and Cambridge Landmarks datasets while also improving on timing and memory footprint as compared to state-of-the-art. </p>
<blockquote>
<p>å›¾åƒæ£€ç´¢ï¼ˆIRï¼‰æ–¹æ³•åœ¨å›¾åƒå®šä½æ–¹é¢å…·æœ‰ä¸3Då’Œæ·±åº¦å­¦ä¹ ï¼ˆDNNï¼‰æ–¹æ³•ä¸åŒçš„æ˜æ˜¾ä¼˜åŠ¿ï¼šå®ƒä¸å—è§†è§‰å½±å“ï¼Œæ›´å®¹æ˜“å®æ–½å’Œä½¿ç”¨ï¼Œæ²¡æœ‰éšç§é—®é¢˜ï¼Œè®¡ç®—æ•ˆç‡é«˜ã€‚è¿™ç§æ–¹æ³•çš„ä¸»è¦ç¼ºç‚¹æ˜¯ä¸ç«äº‰æ–¹æ³•ç›¸æ¯”ï¼Œåœ¨æŸ¥è¯¢ç›¸æœºçš„ä½ç½®å’Œæ–¹ä½å®šä½æ–¹é¢ç›¸å¯¹è¾ƒå·®ã€‚æœ¬æ–‡ä»£è¡¨äº†ä¸€ç§æ··åˆæ–¹æ³•ï¼Œè¯¥æ–¹æ³•ä»…å°†å›¾åƒç‰¹å¾å­˜å‚¨åœ¨æ•°æ®åº“ä¸­ï¼Œç±»ä¼¼äºæŸäº›IRæ–¹æ³•ï¼Œä½†ä¾èµ–äºæ½œåœ¨çš„3Dé‡å»ºï¼Œç±»ä¼¼äº3Dæ–¹æ³•ï¼Œä½†ä¸ä¿ç•™3Dåœºæ™¯é‡å»ºã€‚è¯¥æ–¹æ³•åŸºäºä¸¤ä¸ªæ€æƒ³ï¼šï¼ˆiï¼‰ä¸€ç§æ–°å‹ææ¡ˆï¼Œå…¶ä¸­æŸ¥è¯¢ç›¸æœºä¸­å¿ƒä¼°è®¡ä»…ä¾èµ–äºç›¸å¯¹å¹³ç§»ä¼°è®¡ï¼Œè€Œä¸ä¾èµ–äºé€šè¿‡è§£è€¦ä¸¤è€…å¾—åˆ°çš„ç›¸å¯¹æ—‹è½¬ä¼°è®¡ï¼›ï¼ˆiiï¼‰ä»é€šè¿‡ä¼°è®¡çš„ç›¸å¯¹å§¿åŠ¿è®¡ç®—æœ€ä½³å§¿åŠ¿è½¬å‘é€šè¿‡å¤šè§†å›¾å¯¹åº”å…³ç³»è®¡ç®—æœ€ä½³å§¿åŠ¿ï¼Œä»è€Œçœç•¥äº†â€œä¸­é—´äººâ€ã€‚æˆ‘ä»¬çš„æ–¹æ³•åœ¨7åœºæ™¯å’Œå‰‘æ¡¥åœ°æ ‡æ•°æ®é›†ä¸Šæ˜¾ç¤ºå‡ºæ€§èƒ½æ”¹è¿›ï¼ŒåŒæ—¶åœ¨æ—¶é—´å’Œå†…å­˜å ç”¨æ–¹é¢ä¸æœ€æ–°æŠ€æœ¯ç›¸æ¯”ä¹Ÿæœ‰æ‰€æ”¹è¿›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.23577v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>è¯¥æ–‡æå‡ºä¸€ç§å›¾åƒæ£€ç´¢ï¼ˆIRï¼‰ä¸æ·±åº¦å­¦ä¹ å’Œä¸‰ç»´é‡å»ºæŠ€æœ¯ç»“åˆçš„æ··åˆæ–¹æ³•ï¼Œç”¨äºå›¾åƒå®šä½ã€‚è¯¥æ–¹æ³•å…·æœ‰è§†è§‰æ— å…³æ€§ã€æ˜“äºå®ç°å’Œä½¿ç”¨ã€æ— éšç§é—®é¢˜å’Œè®¡ç®—æ•ˆç‡é«˜ç­‰ä¼˜ç‚¹ã€‚å…¶ä¸»è¦ç¼ºç‚¹æ˜¯ä¸å…¶ä»–æ–¹æ³•ç›¸æ¯”ï¼ŒæŸ¥è¯¢ç›¸æœºçš„ä½ç½®å’Œæ–¹ä½å®šä½ç›¸å¯¹è¾ƒå·®ã€‚æ–°æ–¹æ³•ä»…åœ¨æ•°æ®åº“ä¸­å­˜å‚¨å›¾åƒç‰¹å¾ï¼Œä¾é æ½œåœ¨çš„ä¸‰ç»´é‡å»ºæŠ€æœ¯ä½†ä¸ä¿ç•™æ•´ä¸ªä¸‰ç»´åœºæ™¯é‡å»ºã€‚å®ƒé€šè¿‡ç›¸å¯¹å¹³ç§»ä¼°è®¡è€Œéç›¸å¯¹æ—‹è½¬ä¼°è®¡æ¥ä¼°ç®—æŸ¥è¯¢ç›¸æœºä¸­å¿ƒï¼Œå¹¶é€šè¿‡ä»ä¼°è®¡çš„ç›¸å¯¹å§¿æ€è®¡ç®—æœ€ä¼˜å§¿æ€è½¬å‘å¤šè§†è§’å¯¹åº”å…³ç³»çš„æœ€ä¼˜å§¿æ€è®¡ç®—ï¼Œå®ç°äº†æ€§èƒ½æå‡ã€‚åœ¨7åœºæ™¯å’Œå‰‘æ¡¥åœ°æ ‡æ•°æ®é›†ä¸Šï¼Œè¯¥æ–¹æ³•åœ¨æ—¶é—´å’Œå†…å­˜å ç”¨æ–¹é¢å‡ä¼˜äºç°æœ‰æŠ€æœ¯ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å›¾åƒæ£€ç´¢ï¼ˆIRï¼‰æ–¹æ³•å…·æœ‰è§†è§‰æ— å…³æ€§ã€æ˜“äºå®ç°å’Œä½¿ç”¨ã€æ— éšç§é—®é¢˜å’Œè®¡ç®—æ•ˆç‡é«˜ç­‰ä¼˜ç‚¹ã€‚</li>
<li>æ··åˆæ–¹æ³•ç»“åˆIRå’Œæ·±åº¦å­¦ä¹ ï¼ˆDNNï¼‰ä»¥åŠä¸‰ç»´é‡å»ºæŠ€æœ¯ç”¨äºå›¾åƒå®šä½ã€‚</li>
<li>è¯¥æ–¹æ³•ä¸»è¦ç¼ºç‚¹åœ¨äºæŸ¥è¯¢ç›¸æœºçš„å®šä½å’Œæ–¹ä½å®šä½ç›¸å¯¹è¾ƒå·®ã€‚</li>
<li>è¯¥æ–¹æ³•åœ¨æ•°æ®åº“ä¸­ä»…å­˜å‚¨å›¾åƒç‰¹å¾ï¼Œä¾èµ–æ½œåœ¨çš„ä¸‰ç»´é‡å»ºæŠ€æœ¯ã€‚</li>
<li>æ–¹æ³•é€šè¿‡ç›¸å¯¹å¹³ç§»ä¼°è®¡è€Œéç›¸å¯¹æ—‹è½¬ä¼°è®¡æ¥ä¼°ç®—æŸ¥è¯¢ç›¸æœºä¸­å¿ƒã€‚</li>
<li>æ–¹æ³•ä»ä¼°è®¡çš„ç›¸å¯¹å§¿æ€è®¡ç®—æœ€ä¼˜å§¿æ€è½¬å‘å¤šè§†è§’å¯¹åº”å…³ç³»çš„æœ€ä¼˜å§¿æ€è®¡ç®—ã€‚</li>
<li>åœ¨æ•°æ®é›†ä¸Šï¼Œè¯¥æ–¹æ³•ç›¸è¾ƒäºç°æœ‰æŠ€æœ¯å…·æœ‰æ›´å¥½çš„æ€§èƒ½å’Œè¾ƒä½çš„æ—¶é—´å’Œå†…å­˜å ç”¨ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.23577">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-5d56cbb7bfa9ddf48568ce8523033a55.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-99bd1ac8e4fb36f051e7d62052e9e193.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ec466288461d27ef71eee3e6db803a10.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6deb20a1f06321adb6155309281cec19.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-51745f577ba0ad41bd09d9533ba84221.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-60a025f6726849a65d2fc547691901f7.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Deterministic-Medical-Image-Translation-via-High-fidelity-Brownian-Bridges"><a href="#Deterministic-Medical-Image-Translation-via-High-fidelity-Brownian-Bridges" class="headerlink" title="Deterministic Medical Image Translation via High-fidelity Brownian   Bridges"></a>Deterministic Medical Image Translation via High-fidelity Brownian   Bridges</h2><p><strong>Authors:Qisheng He, Nicholas Summerfield, Peiyong Wang, Carri Glide-Hurst, Ming Dong</strong></p>
<p>Recent studies have shown that diffusion models produce superior synthetic images when compared to Generative Adversarial Networks (GANs). However, their outputs are often non-deterministic and lack high fidelity to the ground truth due to the inherent randomness. In this paper, we propose a novel High-fidelity Brownian bridge model (HiFi-BBrg) for deterministic medical image translations. Our model comprises two distinct yet mutually beneficial mappings: a generation mapping and a reconstruction mapping. The Brownian bridge training process is guided by the fidelity loss and adversarial training in the reconstruction mapping. This ensures that translated images can be accurately reversed to their original forms, thereby achieving consistent translations with high fidelity to the ground truth. Our extensive experiments on multiple datasets show HiFi-BBrg outperforms state-of-the-art methods in multi-modal image translation and multi-image super-resolution. </p>
<blockquote>
<p>æœ€è¿‘çš„ç ”ç©¶è¡¨æ˜ï¼Œä¸ç”Ÿæˆå¯¹æŠ—ç½‘ç»œï¼ˆGANsï¼‰ç›¸æ¯”ï¼Œæ‰©æ•£æ¨¡å‹åœ¨ç”Ÿæˆåˆæˆå›¾åƒæ–¹é¢è¡¨ç°å‡ºæ›´ä¼˜è¶Šçš„æ€§èƒ½ã€‚ç„¶è€Œï¼Œå®ƒä»¬çš„è¾“å‡ºé€šå¸¸æ˜¯éç¡®å®šçš„ï¼Œå¹¶ä¸”ç”±äºå›ºæœ‰çš„éšæœºæ€§ï¼Œå¯¹çœŸå®æ•°æ®çš„ä¿çœŸåº¦ä¸é«˜ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ç”¨äºç¡®å®šæ€§åŒ»å­¦å›¾åƒç¿»è¯‘çš„é«˜ä¿çœŸå¸ƒæœ—æ¡¥æ¨¡å‹ï¼ˆHiFi-BBrgï¼‰ã€‚æˆ‘ä»¬çš„æ¨¡å‹åŒ…æ‹¬ä¸¤ä¸ªä¸åŒä½†ç›¸äº’æœ‰ç›Šçš„æ˜ å°„ï¼šç”Ÿæˆæ˜ å°„å’Œé‡å»ºæ˜ å°„ã€‚å¸ƒæœ—æ¡¥è®­ç»ƒè¿‡ç¨‹ç”±é‡å»ºæ˜ å°„ä¸­çš„ä¿çœŸåº¦æŸå¤±å’Œå¯¹æŠ—æ€§è®­ç»ƒå¼•å¯¼ã€‚è¿™ç¡®ä¿äº†ç¿»è¯‘çš„å›¾åƒå¯ä»¥å‡†ç¡®åœ°æ¢å¤åˆ°å…¶åŸå§‹å½¢å¼ï¼Œä»è€Œå®ç°ä¸çœŸå®æ•°æ®é«˜åº¦ä¸€è‡´çš„ç¿»è¯‘ã€‚æˆ‘ä»¬åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼ŒHiFi-BBrgåœ¨è·¨æ¨¡æ€å›¾åƒç¿»è¯‘å’Œå¤šå›¾åƒè¶…åˆ†è¾¨ç‡æ–¹é¢ä¼˜äºæœ€å…ˆè¿›çš„æ–¹æ³•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.22531v1">PDF</a> </p>
<p><strong>Summary</strong><br>     æœ€è¿‘çš„ç ”ç©¶è¡¨æ˜ï¼Œæ‰©æ•£æ¨¡å‹åœ¨ç”Ÿæˆåˆæˆå›¾åƒæ–¹é¢ç›¸è¾ƒäºç”Ÿæˆå¯¹æŠ—ç½‘ç»œï¼ˆGANsï¼‰æœ‰ä¼˜åŠ¿ã€‚ç„¶è€Œï¼Œå…¶è¾“å‡ºé€šå¸¸å…·æœ‰éç¡®å®šæ€§ï¼Œå¹¶ä¸”ç”±äºå›ºæœ‰çš„éšæœºæ€§ï¼Œå¯¹çœŸå®æ•°æ®çš„ä¿çœŸåº¦ä¸é«˜ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°å‹çš„ç”¨äºç¡®å®šæ€§åŒ»å­¦å›¾åƒç¿»è¯‘çš„é«˜ä¿çœŸå¸ƒæœ—æ¡¥æ¨¡å‹ï¼ˆHiFi-BBrgï¼‰ã€‚è¯¥æ¨¡å‹åŒ…å«ä¸¤ä¸ªç‹¬ç‰¹ä¸”ç›¸è¾…ç›¸æˆçš„æ˜ å°„ï¼šç”Ÿæˆæ˜ å°„å’Œé‡å»ºæ˜ å°„ã€‚å¸ƒæœ—æ¡¥è®­ç»ƒè¿‡ç¨‹ç”±é‡å»ºæ˜ å°„ä¸­çš„ä¿çœŸæŸå¤±å’Œå¯¹æŠ—è®­ç»ƒå¼•å¯¼ï¼Œç¡®ä¿ç¿»è¯‘åçš„å›¾åƒå¯ä»¥å‡†ç¡®è¿˜åŸä¸ºåŸå§‹å½¢å¼ï¼Œä»è€Œå®ç°å…·æœ‰ä¸çœŸå®æ•°æ®é«˜ä¿çœŸçš„ä¸€è‡´ç¿»è¯‘ã€‚åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼ŒHiFi-BBrgåœ¨è·¨æ¨¡æ€å›¾åƒç¿»è¯‘å’Œå¤šå›¾åƒè¶…åˆ†è¾¨ç‡æ–¹é¢ä¼˜äºç°æœ‰å…ˆè¿›æŠ€æœ¯ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ‰©æ•£æ¨¡å‹åœ¨åˆæˆå›¾åƒç”Ÿæˆæ–¹é¢ç›¸è¾ƒäºGANsæœ‰ä¼˜åŠ¿ã€‚</li>
<li>æ‰©æ•£æ¨¡å‹çš„è¾“å‡ºå¸¸å…·æœ‰éç¡®å®šæ€§ï¼Œå¹¶ä¸”å¯¹çœŸå®æ•°æ®çš„ä¿çœŸåº¦ä¸é«˜ã€‚</li>
<li>æå‡ºçš„HiFi-BBrgæ¨¡å‹ç”¨äºç¡®å®šæ€§åŒ»å­¦å›¾åƒç¿»è¯‘ã€‚</li>
<li>HiFi-BBrgæ¨¡å‹åŒ…å«ç”Ÿæˆæ˜ å°„å’Œé‡å»ºæ˜ å°„ä¸¤ä¸ªç‹¬ç‰¹ä¸”ç›¸è¾…ç›¸æˆçš„éƒ¨åˆ†ã€‚</li>
<li>å¸ƒæœ—æ¡¥è®­ç»ƒè¿‡ç¨‹ç¡®ä¿ç¿»è¯‘åçš„å›¾åƒå¯ä»¥å‡†ç¡®è¿˜åŸã€‚</li>
<li>HiFi-BBrgæ¨¡å‹å®ç°äº†é«˜ä¿çœŸçš„ä¸€è‡´ç¿»è¯‘ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.22531">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-89b720f2289ff49b5d424b1d1bb39466.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-42c63a9a154920b748723ea6f60d03e9.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-10b4c05035d61db74240beef8ac3ded3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-74dd698f33980edc4ea7e2552f99da80.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-6c4519fed2f33f938908a378ab11a3e6.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-d5c85f0176bb823177a0938160c03c26.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Time-resolved-dynamic-CBCT-reconstruction-using-prior-model-free-spatiotemporal-Gaussian-representation-PMF-STGR"><a href="#Time-resolved-dynamic-CBCT-reconstruction-using-prior-model-free-spatiotemporal-Gaussian-representation-PMF-STGR" class="headerlink" title="Time-resolved dynamic CBCT reconstruction using prior-model-free   spatiotemporal Gaussian representation (PMF-STGR)"></a>Time-resolved dynamic CBCT reconstruction using prior-model-free   spatiotemporal Gaussian representation (PMF-STGR)</h2><p><strong>Authors:Jiacheng Xie, Hua-Chieh Shao, You Zhang</strong></p>
<p>Time-resolved CBCT imaging, which reconstructs a dynamic sequence of CBCTs reflecting intra-scan motion (one CBCT per x-ray projection without phase sorting or binning), is highly desired for regular and irregular motion characterization, patient setup, and motion-adapted radiotherapy. Representing patient anatomy and associated motion fields as 3D Gaussians, we developed a Gaussian representation-based framework (PMF-STGR) for fast and accurate dynamic CBCT reconstruction. PMF-STGR comprises three major components: a dense set of 3D Gaussians to reconstruct a reference-frame CBCT for the dynamic sequence; another 3D Gaussian set to capture three-level, coarse-to-fine motion-basis-components (MBCs) to model the intra-scan motion; and a CNN-based motion encoder to solve projection-specific temporal coefficients for the MBCs. Scaled by the temporal coefficients, the learned MBCs will combine into deformation vector fields to deform the reference CBCT into projection-specific, time-resolved CBCTs to capture the dynamic motion. Due to the strong representation power of 3D Gaussians, PMF-STGR can reconstruct dynamic CBCTs in a â€˜one-shotâ€™ training fashion from a standard 3D CBCT scan, without using any prior anatomical or motion model. We evaluated PMF-STGR using XCAT phantom simulations and real patient scans. Metrics including the image relative error, structural-similarity-index-measure, tumor center-of-mass-error, and landmark localization error were used to evaluate the accuracy of solved dynamic CBCTs and motion. PMF-STGR shows clear advantages over a state-of-the-art, INR-based approach, PMF-STINR. Compared with PMF-STINR, PMF-STGR reduces reconstruction time by 50% while reconstructing less blurred images with better motion accuracy. With improved efficiency and accuracy, PMF-STGR enhances the applicability of dynamic CBCT imaging for potential clinical translation. </p>
<blockquote>
<p>æ—¶é—´è§£æCBCTæˆåƒæŠ€æœ¯é‡æ„åæ˜ æ‰«æå†…åŠ¨æ€çš„ä¸€ç³»åˆ—CBCTå›¾åƒï¼ˆæ¯æ¬¡Xå°„çº¿æŠ•å½±éƒ½æœ‰ä¸€å¼ CBCTï¼Œæ— éœ€ç›¸ä½æ’åºæˆ–åˆ†ç»„ï¼‰ï¼Œå¯¹äºå¸¸è§„å’Œä¸è§„åˆ™è¿åŠ¨ç‰¹å¾ã€æ‚£è€…è®¾ç½®å’Œé€‚åº”æ€§æ”¾ç–—ä¸­çš„è¿åŠ¨éœ€æ±‚å…·æœ‰é‡è¦æ„ä¹‰ã€‚æˆ‘ä»¬ç”¨ä¸‰ç»´é«˜æ–¯å‡½æ•°æ¥è¡¨ç¤ºæ‚£è€…è§£å‰–ç»“æ„å’Œç›¸å…³è¿åŠ¨åœºï¼Œå¹¶å¼€å‘äº†ä¸€ç§åŸºäºé«˜æ–¯è¡¨ç¤ºçš„æ¡†æ¶ï¼ˆPMF-STGRï¼‰ï¼Œç”¨äºå¿«é€Ÿå‡†ç¡®åœ°é‡å»ºåŠ¨æ€CBCTã€‚PMF-STGRä¸»è¦åŒ…æ‹¬ä¸‰ä¸ªç»„æˆéƒ¨åˆ†ï¼šä½¿ç”¨å¯†é›†çš„ä¸‰ç»´é«˜æ–¯é›†é‡æ„åŠ¨æ€åºåˆ—çš„å‚è€ƒå¸§CBCTï¼›ä½¿ç”¨å¦ä¸€ä¸ªä¸‰ç»´é«˜æ–¯é›†æ•æ‰ä¸‰çº§ç²—ç»†è¿åŠ¨åŸºç¡€æˆåˆ†ï¼ˆMBCsï¼‰ï¼Œä»¥æ¨¡æ‹Ÿæ‰«æå†…çš„è¿åŠ¨ï¼›åŸºäºCNNçš„è¿åŠ¨ç¼–ç å™¨ç”¨äºè§£å†³ä¸MBCæŠ•å½±ç›¸å…³çš„ç‰¹å®šæ—¶é—´ç³»æ•°ã€‚æ ¹æ®æ—¶é—´ç³»æ•°ï¼Œå­¦ä¹ åˆ°çš„MBCå°†ç»„åˆæˆå˜å½¢çŸ¢é‡åœºï¼Œå°†å‚è€ƒCBCTå˜å½¢ä¸ºä¸æŠ•å½±ç›¸å…³çš„ã€æ—¶é—´è§£æçš„CBCTï¼Œä»¥æ•æ‰åŠ¨æ€è¿åŠ¨ã€‚ç”±äºä¸‰ç»´é«˜æ–¯å‡½æ•°å…·æœ‰å¾ˆå¼ºçš„è¡¨ç¤ºèƒ½åŠ›ï¼ŒPMF-STGRå¯ä»¥ä»æ ‡å‡†çš„3D CBCTæ‰«æä¸­é€šè¿‡â€œå•æ¬¡â€è®­ç»ƒçš„æ–¹å¼é‡å»ºåŠ¨æ€CBCTï¼Œæ— éœ€ä½¿ç”¨ä»»ä½•å…ˆéªŒè§£å‰–æˆ–è¿åŠ¨æ¨¡å‹ã€‚æˆ‘ä»¬ä½¿ç”¨XCATå¹»å½±æ¨¡æ‹Ÿå’Œå®é™…æ‚£è€…æ‰«æå¯¹PMF-STGRè¿›è¡Œäº†è¯„ä¼°ã€‚ä½¿ç”¨å›¾åƒç›¸å¯¹è¯¯å·®ã€ç»“æ„ç›¸ä¼¼æ€§æŒ‡æ•°åº¦é‡ã€è‚¿ç˜¤è´¨é‡ä¸­å¿ƒè¯¯å·®å’Œåœ°æ ‡å®šä½è¯¯å·®ç­‰æŒ‡æ ‡æ¥è¯„ä¼°æ±‚è§£çš„åŠ¨æ€CBCTå’Œè¿åŠ¨å‡†ç¡®æ€§ã€‚PMF-STGRç›¸æ¯”äºç›®å‰æœ€å…ˆè¿›çš„æ–¹æ³•â€”â€”åŸºäºINRçš„æ–¹æ³•PMF-STINRå…·æœ‰æ˜æ˜¾çš„ä¼˜åŠ¿ã€‚ä¸PMF-STINRç›¸æ¯”ï¼ŒPMF-STGRåœ¨é‡å»ºæ—¶é—´å‡å°‘50%çš„åŒæ—¶ï¼Œç”Ÿæˆäº†æ¨¡ç³Šåº¦æ›´ä½çš„å›¾åƒï¼Œå¹¶æé«˜äº†è¿åŠ¨å‡†ç¡®æ€§ã€‚å‡­å€Ÿæ›´é«˜çš„æ•ˆç‡å’Œå‡†ç¡®æ€§ï¼ŒPMF-STGRå¢å¼ºäº†åŠ¨æ€CBCTæˆåƒçš„é€‚ç”¨æ€§ï¼Œå…·æœ‰æ½œåœ¨çš„ä¸´åºŠåº”ç”¨ä»·å€¼ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.22139v1">PDF</a> 25 pages, 5 figures</p>
<p><strong>Summary</strong></p>
<p>åŠ¨æ€CBCTæˆåƒèƒ½å¤Ÿåæ˜ æ— ç›¸ä½æ’åºæˆ–æ— åˆ†ç»„çš„å•æ¬¡æ‰«æä¸­çš„è¿åŠ¨ï¼Œè¿™å¯¹äºå¸¸è§„å’Œä¸è§„åˆ™çš„è¿åŠ¨ç‰¹æ€§åˆ†æã€æ‚£è€…è®¾ç½®å’Œè‡ªé€‚åº”æ”¾ç–—å…·æœ‰æå¤§ä»·å€¼ã€‚ä¸ºå¿«é€Ÿå‡†ç¡®åœ°é‡å»ºåŠ¨æ€CBCTåºåˆ—ï¼Œç ”ç©¶å›¢é˜Ÿå»ºç«‹äº†åŸºäºé«˜æ–¯è¡¨ç¤ºçš„æ¡†æ¶ï¼ˆPMF-STGRï¼‰ã€‚è¯¥æ¡†æ¶é€šè¿‡æ„å»ºå¯†é›†çš„ä¸‰ç»´é«˜æ–¯é›†æ¥é‡å»ºåŠ¨æ€åºåˆ—çš„å‚è€ƒæ¡†æ¶CBCTï¼Œå¹¶å¼•å…¥å¦ä¸€ç»„ä¸‰ç»´é«˜æ–¯é›†æ•æ‰ä¸‰ä¸ªçº§åˆ«çš„è¿åŠ¨åŸºç¡€åˆ†é‡ï¼ˆMBCsï¼‰ï¼Œä»è€Œæ¨¡æ‹Ÿæ‰«ææœŸé—´çš„è¿åŠ¨ã€‚åŒæ—¶ï¼Œé‡‡ç”¨åŸºäºå·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNï¼‰çš„è¿åŠ¨ç¼–ç å™¨è®¡ç®—é’ˆå¯¹MBCsçš„æŠ•å½±ç‰¹å®šæ—¶é—´ç³»æ•°ã€‚åŸºäºè¿™äº›æ—¶é—´ç³»æ•°å­¦ä¹ çš„MBCsä¼šè½¬åŒ–ä¸ºå˜å½¢å‘é‡åœºï¼Œä»¥å°†å‚è€ƒCBCTå˜å½¢ä¸ºé€‚åº”æŠ•å½±ç‰¹å®šæ—¶é—´çš„åŠ¨æ€CBCTå›¾åƒåºåˆ—ï¼Œå®ç°åŠ¨æ€è¿åŠ¨æ•æ‰ã€‚è¯¥æ–¹æ³•å¯åœ¨ä¸€æ¬¡æ‰«æä¸­ä½¿ç”¨å•ä¸ªCBCTè®­ç»ƒå‡ºåŠ¨æ€CBCTå›¾åƒåºåˆ—ï¼Œæ— éœ€ä½¿ç”¨ä»»ä½•å…ˆéªŒè§£å‰–æˆ–è¿åŠ¨æ¨¡å‹ã€‚é€šè¿‡XCATå¹»å½±æ¨¡æ‹Ÿå’ŒçœŸå®æ‚£è€…æ‰«æéªŒè¯äº†å…¶æ€§èƒ½ã€‚è¯„ä¼°æŒ‡æ ‡åŒ…æ‹¬å›¾åƒç›¸å¯¹è¯¯å·®ã€ç»“æ„ç›¸ä¼¼æ€§æŒ‡æ•°åº¦é‡ã€è‚¿ç˜¤è´¨å¿ƒè¯¯å·®å’Œåœ°æ ‡å®šä½è¯¯å·®ç­‰ã€‚ç›¸è¾ƒäºç›®å‰çš„ä¸»æµæ–¹æ³•PMF-STINRï¼ŒPMF-STGRå…·æœ‰æ˜æ˜¾ä¼˜åŠ¿ï¼Œå…¶é‡å»ºæ—¶é—´ç¼©çŸ­äº†50%ï¼ŒåŒæ—¶ç”Ÿæˆçš„å›¾åƒæ›´æ¸…æ™°ä¸”è¿åŠ¨å‡†ç¡®æ€§æ›´é«˜ã€‚è¿™æé«˜äº†åŠ¨æ€CBCTæˆåƒåœ¨ä¸´åºŠåº”ç”¨ä¸­çš„æ•ˆç‡å’Œå‡†ç¡®æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ—¶é—´è§£æCBCTæˆåƒå¯¹äºè¿åŠ¨è¡¨å¾ã€æ‚£è€…è®¾ç½®å’Œè‡ªé€‚åº”æ”¾ç–—è‡³å…³é‡è¦ã€‚</li>
<li>ç ”ç©¶å›¢é˜Ÿæå‡ºäº†åŸºäºé«˜æ–¯è¡¨ç¤ºçš„æ¡†æ¶ï¼ˆPMF-STGRï¼‰è¿›è¡ŒåŠ¨æ€CBCTé‡å»ºã€‚</li>
<li>PMF-STGRåˆ©ç”¨ä¸‰ç»´é«˜æ–¯é›†æ•æ‰å‚è€ƒå¸§å’Œè¿åŠ¨ä¸­ç»†èŠ‚ã€‚</li>
<li>å¼•å…¥CNNè§£å†³è¿åŠ¨ç¼–ç é—®é¢˜ï¼Œç”ŸæˆåŠ¨æ€è¿åŠ¨çš„å˜å½¢å‘é‡åœºã€‚</li>
<li>PMF-STGRåœ¨ä¸€æ¬¡æ‰«æä¸­å³å¯è®­ç»ƒå‡ºåŠ¨æ€CBCTå›¾åƒåºåˆ—ï¼Œæ— éœ€å…ˆéªŒæ¨¡å‹ã€‚</li>
<li>ä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼ŒPMF-STGRåœ¨å›¾åƒè´¨é‡å’Œè¿åŠ¨å‡†ç¡®æ€§æ–¹é¢è¡¨ç°å‡ºä¼˜åŠ¿ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.22139">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-bfd8e9728794a4da0f9548e5ba18d0ea.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Image-as-an-IMU-Estimating-Camera-Motion-from-a-Single-Motion-Blurred-Image"><a href="#Image-as-an-IMU-Estimating-Camera-Motion-from-a-Single-Motion-Blurred-Image" class="headerlink" title="Image as an IMU: Estimating Camera Motion from a Single Motion-Blurred   Image"></a>Image as an IMU: Estimating Camera Motion from a Single Motion-Blurred   Image</h2><p><strong>Authors:Jerred Chen, Ronald Clark</strong></p>
<p>In many robotics and VR&#x2F;AR applications, fast camera motions cause a high level of motion blur, causing existing camera pose estimation methods to fail. In this work, we propose a novel framework that leverages motion blur as a rich cue for motion estimation rather than treating it as an unwanted artifact. Our approach works by predicting a dense motion flow field and a monocular depth map directly from a single motion-blurred image. We then recover the instantaneous camera velocity by solving a linear least squares problem under the small motion assumption. In essence, our method produces an IMU-like measurement that robustly captures fast and aggressive camera movements. To train our model, we construct a large-scale dataset with realistic synthetic motion blur derived from ScanNet++v2 and further refine our model by training end-to-end on real data using our fully differentiable pipeline. Extensive evaluations on real-world benchmarks demonstrate that our method achieves state-of-the-art angular and translational velocity estimates, outperforming current methods like MASt3R and COLMAP. </p>
<blockquote>
<p>åœ¨æœºå™¨äººæŠ€æœ¯å’Œè™šæ‹Ÿç°å®&#x2F;å¢å¼ºç°å®åº”ç”¨ä¸­ï¼Œå¿«é€Ÿç›¸æœºè¿åŠ¨ä¼šäº§ç”Ÿé«˜æ°´å¹³çš„è¿åŠ¨æ¨¡ç³Šï¼Œå¯¼è‡´ç°æœ‰çš„ç›¸æœºå§¿æ€ä¼°è®¡æ–¹æ³•å¤±æ•ˆã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°å‹æ¡†æ¶ï¼Œè¯¥æ¡†æ¶åˆ©ç”¨è¿åŠ¨æ¨¡ç³Šä½œä¸ºè¿åŠ¨ä¼°è®¡çš„ä¸°å¯Œçº¿ç´¢ï¼Œè€Œä¸æ˜¯å°†å…¶è§†ä¸ºä¸å—æ¬¢è¿çš„ä¼ªåƒã€‚æˆ‘ä»¬çš„æ–¹æ³•é€šè¿‡ç›´æ¥ä»å•å¼ è¿åŠ¨æ¨¡ç³Šå›¾åƒé¢„æµ‹å¯†é›†è¿åŠ¨æµåœºå’Œå•çœ¼æ·±åº¦å›¾æ¥å·¥ä½œã€‚ç„¶åï¼Œæˆ‘ä»¬åœ¨å°è¿åŠ¨å‡è®¾ä¸‹é€šè¿‡è§£å†³çº¿æ€§æœ€å°äºŒä¹˜é—®é¢˜æ¥æ¢å¤ç¬æ—¶ç›¸æœºé€Ÿåº¦ã€‚æœ¬è´¨ä¸Šï¼Œæˆ‘ä»¬çš„æ–¹æ³•äº§ç”Ÿäº†ä¸€ç§ç±»ä¼¼äºIMUçš„æµ‹é‡å€¼ï¼Œèƒ½å¤Ÿç¨³å¥åœ°æ•è·å¿«é€Ÿä¸”çŒ›çƒˆçš„ç›¸æœºè¿åŠ¨ã€‚ä¸ºäº†è®­ç»ƒæˆ‘ä»¬çš„æ¨¡å‹ï¼Œæˆ‘ä»¬ä½¿ç”¨ScanNet++v2æ„å»ºäº†å…·æœ‰ç°å®åˆæˆè¿åŠ¨æ¨¡ç³Šçš„å¤§è§„æ¨¡æ•°æ®é›†ï¼Œå¹¶é€šè¿‡åœ¨æˆ‘ä»¬çš„å®Œå…¨å¯å¾®ç®¡é“ä¸Šå¯¹çœŸå®æ•°æ®è¿›è¡Œç«¯åˆ°ç«¯çš„è®­ç»ƒæ¥è¿›ä¸€æ­¥å®Œå–„æˆ‘ä»¬çš„æ¨¡å‹ã€‚åœ¨çœŸå®ä¸–ç•ŒåŸºå‡†æµ‹è¯•ä¸Šçš„å¹¿æ³›è¯„ä¼°è¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•è¾¾åˆ°äº†æœ€å…ˆè¿›çš„è§’é€Ÿåº¦å’Œçº¿é€Ÿåº¦ä¼°è®¡ï¼Œä¼˜äºMASt3Rå’ŒCOLMAPç­‰æ–¹æ³•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.17358v3">PDF</a> Project page: <a target="_blank" rel="noopener" href="https://jerredchen.github.io/image-as-imu/">https://jerredchen.github.io/image-as-imu/</a></p>
<p><strong>Summary</strong></p>
<p>è¯¥æ–‡æœ¬æå‡ºäº†ä¸€ç§åˆ©ç”¨è¿åŠ¨æ¨¡ç³Šä½œä¸ºè¿åŠ¨ä¼°è®¡çš„ä¸°å¯Œçº¿ç´¢çš„æ–°æ¡†æ¶ï¼Œè€Œä¸æ˜¯å°†å…¶è§†ä¸ºä¸éœ€è¦çš„ä¼ªå½±ã€‚è¯¥æ¡†æ¶é€šè¿‡é¢„æµ‹ä»å•ä¸ªè¿åŠ¨æ¨¡ç³Šå›¾åƒä¸­çš„å¯†é›†è¿åŠ¨æµåœºå’Œå•çœ¼æ·±åº¦å›¾æ¥å·¥ä½œï¼Œç„¶åé€šè¿‡è§£å†³å°è¿åŠ¨å‡è®¾ä¸‹çš„çº¿æ€§æœ€å°äºŒä¹˜é—®é¢˜æ¥æ¢å¤ç¬æ—¶ç›¸æœºé€Ÿåº¦ã€‚è¯¥æ–¹æ³•å®è´¨ä¸Šäº§ç”Ÿäº†IMUå¼çš„æµ‹é‡ï¼Œèƒ½å¤Ÿç¨³å¥åœ°æ•è·å¿«é€Ÿå’Œæ¿€çƒˆçš„ç›¸æœºè¿åŠ¨ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç°æœ‰æ–¹æ³•åœ¨å¿«é€Ÿç›¸æœºè¿åŠ¨ä¸‹ä¼šå‡ºç°è¿åŠ¨æ¨¡ç³Šé—®é¢˜ï¼Œå¯¼è‡´å§¿æ€ä¼°è®¡å¤±è´¥ã€‚</li>
<li>æå‡ºäº†ä¸€ç§æ–°çš„æ¡†æ¶ï¼Œåˆ©ç”¨è¿åŠ¨æ¨¡ç³Šä½œä¸ºè¿åŠ¨ä¼°è®¡çš„ä¸°å¯Œçº¿ç´¢ã€‚</li>
<li>é€šè¿‡é¢„æµ‹å¯†é›†è¿åŠ¨æµåœºå’Œå•çœ¼æ·±åº¦å›¾æ¥ä¼°è®¡ç›¸æœºè¿åŠ¨ã€‚</li>
<li>é€šè¿‡è§£å†³çº¿æ€§æœ€å°äºŒä¹˜é—®é¢˜æ¥æ¢å¤ç¬æ—¶ç›¸æœºé€Ÿåº¦ã€‚</li>
<li>æ–¹æ³•å®è´¨ä¸Šäº§ç”Ÿäº†IMUå¼çš„æµ‹é‡ï¼Œç¨³å¥åœ°æ•è·å¿«é€Ÿå’Œæ¿€çƒˆçš„ç›¸æœºè¿åŠ¨ã€‚</li>
<li>ä½¿ç”¨ScanNet++v2æ„å»ºçš„å¤§è§„æ¨¡æ•°æ®é›†è¿›è¡Œæ¨¡å‹è®­ç»ƒï¼Œå¹¶åˆ©ç”¨å®Œå…¨å¯å¾®åˆ†çš„ç®¡é“è¿›è¡ŒçœŸå®æ•°æ®çš„ç«¯åˆ°ç«¯è®­ç»ƒæ¥è¿›ä¸€æ­¥ä¼˜åŒ–æ¨¡å‹ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.17358">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-1777a89b737824da0e9215fe6ae9626b.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-3fb73a1c56093ba9736e574607743b9d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-26d18fa7bf9cd68e578ba48ef497162e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-fd72da0aaee36d32b4b7d37e7f10969a.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-e14172ed08be1736a14fbf020ba7306c.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="PTDiffusion-Free-Lunch-for-Generating-Optical-Illusion-Hidden-Pictures-with-Phase-Transferred-Diffusion-Model"><a href="#PTDiffusion-Free-Lunch-for-Generating-Optical-Illusion-Hidden-Pictures-with-Phase-Transferred-Diffusion-Model" class="headerlink" title="PTDiffusion: Free Lunch for Generating Optical Illusion Hidden Pictures   with Phase-Transferred Diffusion Model"></a>PTDiffusion: Free Lunch for Generating Optical Illusion Hidden Pictures   with Phase-Transferred Diffusion Model</h2><p><strong>Authors:Xiang Gao, Shuai Yang, Jiaying Liu</strong></p>
<p>Optical illusion hidden picture is an interesting visual perceptual phenomenon where an image is cleverly integrated into another picture in a way that is not immediately obvious to the viewer. Established on the off-the-shelf text-to-image (T2I) diffusion model, we propose a novel training-free text-guided image-to-image (I2I) translation framework dubbed as \textbf{P}hase-\textbf{T}ransferred \textbf{Diffusion} Model (PTDiffusion) for hidden art syntheses. PTDiffusion harmoniously embeds an input reference image into arbitrary scenes described by the text prompts, producing illusion images exhibiting hidden visual cues of the reference image. At the heart of our method is a plug-and-play phase transfer mechanism that dynamically and progressively transplants diffusion featuresâ€™ phase spectrum from the denoising process to reconstruct the reference image into the one to sample the generated illusion image, realizing deep fusion of the reference structural information and the textual semantic information in the diffusion model latent space. Furthermore, we propose asynchronous phase transfer to enable flexible control to the degree of hidden content discernability. Our method bypasses any model training and fine-tuning process, all while substantially outperforming related text-guided I2I methods in image generation quality, text fidelity, visual discernibility, and contextual naturalness for illusion picture synthesis, as demonstrated by extensive qualitative and quantitative experiments. Our project is publically available at \href{<a target="_blank" rel="noopener" href="https://xianggao1102.github.io/PTDiffusion_webpage/%7D%7Bthis">https://xianggao1102.github.io/PTDiffusion_webpage/}{this</a> web page}. </p>
<blockquote>
<p>å…‰å­¦é”™è§‰éšè—å›¾åƒæ˜¯ä¸€ç§æœ‰è¶£çš„è§†è§‰æ„ŸçŸ¥ç°è±¡ï¼Œå…¶ä¸­å›¾åƒè¢«å·§å¦™åœ°é›†æˆåˆ°å¦ä¸€å¹…å›¾åƒä¸­ï¼Œä»¥è‡³äºè§‚ä¼—æ— æ³•ç«‹å³å¯Ÿè§‰ã€‚æˆ‘ä»¬åŸºäºç°æˆçš„æ–‡æœ¬åˆ°å›¾åƒï¼ˆT2Iï¼‰æ‰©æ•£æ¨¡å‹ï¼Œæå‡ºäº†ä¸€ç§æ— éœ€è®­ç»ƒçš„æ–‡å­—å¼•å¯¼å›¾åƒåˆ°å›¾åƒï¼ˆI2Iï¼‰è½¬æ¢æ¡†æ¶ï¼Œåä¸ºé˜¶æ®µè½¬ç§»æ‰©æ•£æ¨¡å‹ï¼ˆPTDiffusionï¼‰ï¼Œç”¨äºåˆæˆéšè—è‰ºæœ¯ã€‚PTDiffusionå’Œè°åœ°å°†è¾“å…¥å‚è€ƒå›¾åƒåµŒå…¥åˆ°æ–‡æœ¬æç¤ºæè¿°çš„ä»»æ„åœºæ™¯ä¸­ï¼Œç”Ÿæˆæ˜¾ç¤ºå‚è€ƒå›¾åƒçš„éšè—è§†è§‰çº¿ç´¢çš„é”™è§‰å›¾åƒã€‚æˆ‘ä»¬æ–¹æ³•çš„æ ¸å¿ƒæ˜¯ä¸€ä¸ªå³æ’å³ç”¨çš„ç›¸ä½è½¬ç§»æœºåˆ¶ï¼Œè¯¥æœºåˆ¶åŠ¨æ€ä¸”æ¸è¿›åœ°ä»å»å™ªè¿‡ç¨‹ä¸­ç§»æ¤æ‰©æ•£ç‰¹å¾çš„ç›¸ä½è°±ï¼Œå°†å‚è€ƒå›¾åƒé‡å»ºåˆ°é‡‡æ ·ç”Ÿæˆçš„é”™è§‰å›¾åƒä¸­ï¼Œå®ç°åœ¨æ‰©æ•£æ¨¡å‹æ½œåœ¨ç©ºé—´ä¸­çš„å‚è€ƒç»“æ„ä¿¡æ¯å’Œæ–‡æœ¬è¯­ä¹‰ä¿¡æ¯çš„æ·±åº¦èåˆã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æå‡ºäº†å¼‚æ­¥ç›¸ä½è½¬ç§»ï¼Œä»¥å®ç°çµæ´»æ§åˆ¶éšè—å†…å®¹çš„å¯è¯†åˆ«ç¨‹åº¦ã€‚æˆ‘ä»¬çš„æ–¹æ³•é¿å¼€äº†ä»»ä½•æ¨¡å‹è®­ç»ƒå’Œå¾®è°ƒè¿‡ç¨‹ï¼ŒåŒæ—¶åœ¨å›¾åƒç”Ÿæˆè´¨é‡ã€æ–‡æœ¬å¿ å®åº¦ã€è§†è§‰å¯è¾¨è¯†åº¦å’Œä¸Šä¸‹æ–‡è‡ªç„¶åº¦ç­‰æ–¹é¢å¤§å¤§ä¼˜äºç›¸å…³çš„æ–‡æœ¬å¼•å¯¼I2Iæ–¹æ³•ï¼Œç”¨äºé”™è§‰å›¾åƒåˆæˆã€‚æˆ‘ä»¬çš„é¡¹ç›®å·²åœ¨ä»¥ä¸‹ç½‘é¡µå…¬å¼€ï¼š<a target="_blank" rel="noopener" href="https://xianggao1102.github.io/PTDiffusion_webpage/">ç½‘é¡µé“¾æ¥</a>ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.06186v4">PDF</a> Proceedings of the IEEE&#x2F;CVF Conference on Computer Vision and Pattern   Recognition (CVPR 2025)</p>
<p><strong>Summary</strong></p>
<p>æ–‡æœ¬æè¿°äº†ä¸€ä¸ªåä¸ºPTDiffusionçš„æ–°æ–¹æ³•ï¼Œå®ƒæ˜¯ä¸€ç§æ— éœ€è®­ç»ƒçš„æ–‡æœ¬å¼•å¯¼çš„å›¾åƒåˆ°å›¾åƒï¼ˆI2Iï¼‰ç¿»è¯‘æ¡†æ¶ï¼Œç”¨äºåˆæˆéšè—è‰ºæœ¯å›¾åƒã€‚è¯¥æ–¹æ³•é€šè¿‡å°†è¾“å…¥å‚è€ƒå›¾åƒåµŒå…¥åˆ°ç”±æ–‡æœ¬æç¤ºæè¿°çš„ä»»æ„åœºæ™¯ä¸­ï¼Œç”Ÿæˆéšè—è§†è§‰çº¿ç´¢çš„é”™è§‰å›¾åƒã€‚å…¶æ ¸å¿ƒæŠ€æœ¯æ˜¯æ’æ’­å¼ç›¸ä½è½¬ç§»æœºåˆ¶ï¼Œå®ç°å‚è€ƒå›¾åƒä¸æ–‡æœ¬è¯­ä¹‰ä¿¡æ¯çš„æ·±åº¦èåˆã€‚æ­¤å¤–ï¼Œè¿˜æå‡ºäº†å¼‚æ­¥ç›¸ä½è½¬ç§»ï¼Œä»¥å®ç°çµæ´»æ§åˆ¶éšè—å†…å®¹çš„è¯†åˆ«ç¨‹åº¦ã€‚è¯¥æ–¹æ³•æ— éœ€ä»»ä½•æ¨¡å‹è®­ç»ƒå’Œå¾®è°ƒè¿‡ç¨‹ï¼Œåœ¨å›¾åƒç”Ÿæˆè´¨é‡ã€æ–‡æœ¬å¿ å®åº¦ã€è§†è§‰è¾¨è¯†åº¦å’Œä¸Šä¸‹æ–‡è‡ªç„¶æ€§æ–¹é¢å‡å¤§å¤§ä¼˜äºç›¸å…³æ–‡æœ¬å¼•å¯¼çš„I2Iæ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>PTDiffusionæ˜¯ä¸€ç§æ–°çš„æ–‡æœ¬å¼•å¯¼çš„å›¾åƒåˆ°å›¾åƒï¼ˆI2Iï¼‰ç¿»è¯‘æ¡†æ¶ï¼Œç”¨äºåˆæˆéšè—è‰ºæœ¯å›¾åƒã€‚</li>
<li>å®ƒé€šè¿‡å°†å‚è€ƒå›¾åƒå·§å¦™åœ°åµŒå…¥åˆ°ç”±æ–‡æœ¬æè¿°çš„åœºæ™¯ä¸­ï¼Œç”Ÿæˆå…·æœ‰éšè—è§†è§‰çº¿ç´¢çš„é”™è§‰å›¾åƒã€‚</li>
<li>æ’æ’­å¼ç›¸ä½è½¬ç§»æœºåˆ¶æ˜¯PTDiffusionçš„æ ¸å¿ƒæŠ€æœ¯ï¼Œå®ç°äº†å‚è€ƒå›¾åƒä¸æ–‡æœ¬ä¿¡æ¯çš„æ·±åº¦èåˆã€‚</li>
<li>å¼‚æ­¥ç›¸ä½è½¬ç§»å…è®¸çµæ´»æ§åˆ¶éšè—å†…å®¹çš„è¯†åˆ«ç¨‹åº¦ã€‚</li>
<li>PTDiffusionæ— éœ€ä»»ä½•æ¨¡å‹è®­ç»ƒå’Œå¾®è°ƒï¼Œå…·æœ‰å‡ºè‰²çš„å›¾åƒç”Ÿæˆè´¨é‡ã€æ–‡æœ¬å¿ å®åº¦ã€è§†è§‰è¾¨è¯†åº¦å’Œä¸Šä¸‹æ–‡è‡ªç„¶æ€§ã€‚</li>
<li>è¯¥æ–¹æ³•å¯¹äºåˆ›å»ºè§†è§‰é”™è§‰å›¾åƒå…·æœ‰é‡è¦çš„åº”ç”¨ä»·å€¼ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.06186">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-889d3d3cf3e78ee16cb22a8e5afcee2e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d546ccc0900e834495405121f3861e10.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a6a32b6a8b666e971777898d004c84fa.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-a2e7ca8dc8a3c6bf59c59e13a1e6479a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1414e64ebf8d7c7960f3cd0f6426a083.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="Motion-Prompting-Controlling-Video-Generation-with-Motion-Trajectories"><a href="#Motion-Prompting-Controlling-Video-Generation-with-Motion-Trajectories" class="headerlink" title="Motion Prompting: Controlling Video Generation with Motion Trajectories"></a>Motion Prompting: Controlling Video Generation with Motion Trajectories</h2><p><strong>Authors:Daniel Geng, Charles Herrmann, Junhwa Hur, Forrester Cole, Serena Zhang, Tobias Pfaff, Tatiana Lopez-Guevara, Carl Doersch, Yusuf Aytar, Michael Rubinstein, Chen Sun, Oliver Wang, Andrew Owens, Deqing Sun</strong></p>
<p>Motion control is crucial for generating expressive and compelling video content; however, most existing video generation models rely mainly on text prompts for control, which struggle to capture the nuances of dynamic actions and temporal compositions. To this end, we train a video generation model conditioned on spatio-temporally sparse or dense motion trajectories. In contrast to prior motion conditioning work, this flexible representation can encode any number of trajectories, object-specific or global scene motion, and temporally sparse motion; due to its flexibility we refer to this conditioning as motion prompts. While users may directly specify sparse trajectories, we also show how to translate high-level user requests into detailed, semi-dense motion prompts, a process we term motion prompt expansion. We demonstrate the versatility of our approach through various applications, including camera and object motion control, â€œinteractingâ€ with an image, motion transfer, and image editing. Our results showcase emergent behaviors, such as realistic physics, suggesting the potential of motion prompts for probing video models and interacting with future generative world models. Finally, we evaluate quantitatively, conduct a human study, and demonstrate strong performance. Video results are available on our webpage: <a target="_blank" rel="noopener" href="https://motion-prompting.github.io/">https://motion-prompting.github.io/</a> </p>
<blockquote>
<p>åŠ¨ä½œæ§åˆ¶åœ¨ç”Ÿæˆå¯Œæœ‰è¡¨ç°åŠ›å’Œå¼•äººå…¥èƒœçš„è§†é¢‘å†…å®¹æ–¹é¢èµ·ç€è‡³å…³é‡è¦çš„ä½œç”¨ã€‚ç„¶è€Œï¼Œç°æœ‰çš„å¤§å¤šæ•°è§†é¢‘ç”Ÿæˆæ¨¡å‹ä¸»è¦ä¾èµ–äºæ–‡æœ¬æç¤ºæ¥è¿›è¡Œæ§åˆ¶ï¼Œè¿™å¾ˆéš¾æ•æ‰åˆ°åŠ¨æ€åŠ¨ä½œå’Œæ—¶åºç»„åˆçš„ç»†å¾®å·®åˆ«ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬è®­ç»ƒäº†ä¸€ç§åŸºäºæ—¶ç©ºç¨€ç–æˆ–å¯†é›†è¿åŠ¨è½¨è¿¹çš„è§†é¢‘ç”Ÿæˆæ¨¡å‹ã€‚ä¸ä¹‹å‰çš„å·¥ä½œç›¸æ¯”ï¼Œè¿™ç§çµæ´»çš„è¡¨ç¤ºå¯ä»¥ç¼–ç ä»»æ„æ•°é‡çš„è½¨è¿¹ã€ç‰¹å®šå¯¹è±¡çš„è¿åŠ¨æˆ–å…¨å±€åœºæ™¯çš„è¿åŠ¨ï¼Œä»¥åŠæ—¶åºç¨€ç–çš„è¿åŠ¨ã€‚ç”±äºå…¶çµæ´»æ€§ï¼Œæˆ‘ä»¬å°†è¿™ç§æ¡ä»¶ç§°ä¸ºè¿åŠ¨æç¤ºã€‚è™½ç„¶ç”¨æˆ·å¯ä»¥ç›´æ¥æŒ‡å®šç¨€ç–è½¨è¿¹ï¼Œæˆ‘ä»¬è¿˜å±•ç¤ºäº†å¦‚ä½•å°†é«˜çº§ç”¨æˆ·è¯·æ±‚ç¿»è¯‘æˆè¯¦ç»†ã€åŠå¯†é›†çš„è¿åŠ¨æç¤ºï¼Œæˆ‘ä»¬ç§°è¿™ä¸ªè¿‡ç¨‹ä¸ºè¿åŠ¨æç¤ºæ‰©å±•ã€‚æˆ‘ä»¬é€šè¿‡å„ç§åº”ç”¨å±•ç¤ºäº†æˆ‘ä»¬çš„æ–¹æ³•çš„é€šç”¨æ€§ï¼ŒåŒ…æ‹¬ç›¸æœºå’Œå¯¹è±¡è¿åŠ¨æ§åˆ¶ã€â€œä¸å›¾åƒäº’åŠ¨â€ã€è¿åŠ¨è½¬ç§»å’Œå›¾åƒç¼–è¾‘ç­‰ã€‚æˆ‘ä»¬çš„ç»“æœå±•ç¤ºäº†æ–°å…´çš„è¡Œä¸ºï¼Œå¦‚é€¼çœŸçš„ç‰©ç†æ•ˆæœï¼Œè¿™è¡¨æ˜è¿åŠ¨æç¤ºåœ¨æ¢æŸ¥è§†é¢‘æ¨¡å‹ä»¥åŠä¸æœªæ¥çš„ç”Ÿæˆä¸–ç•Œæ¨¡å‹äº’åŠ¨æ–¹é¢å…·æœ‰æ½œåŠ›ã€‚æœ€åï¼Œæˆ‘ä»¬è¿›è¡Œäº†å®šé‡è¯„ä¼°ã€äººç±»ç ”ç©¶ï¼Œå¹¶å±•ç¤ºäº†å¼ºå¤§çš„æ€§èƒ½ã€‚è§†é¢‘ç»“æœå¯åœ¨æˆ‘ä»¬çš„ç½‘é¡µä¸Šçœ‹åˆ°ï¼š<a target="_blank" rel="noopener" href="https://motion-prompting.github.io/%E3%80%82">https://motion-prompting.github.io/ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.02700v2">PDF</a> CVPR 2025 camera ready. Project page:   <a target="_blank" rel="noopener" href="https://motion-prompting.github.io/">https://motion-prompting.github.io/</a></p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä¸»è¦æ¢è®¨äº†åŠ¨ä½œæ§åˆ¶åœ¨ç”Ÿæˆå…·æœ‰è¡¨ç°åŠ›å’Œå¸å¼•åŠ›çš„è§†é¢‘å†…å®¹ä¸­çš„é‡è¦æ€§ã€‚ä¼ ç»Ÿçš„è§†é¢‘ç”Ÿæˆæ¨¡å‹ä¸»è¦ä¾èµ–äºæ–‡æœ¬æç¤ºè¿›è¡ŒåŠ¨ä½œæ§åˆ¶ï¼Œéš¾ä»¥æ•æ‰åŠ¨æ€è¡Œä¸ºå’Œæ—¶åºç»„åˆçš„ç»†èŠ‚ã€‚ä¸ºæ­¤ï¼Œæœ¬æ–‡è®­ç»ƒäº†ä¸€ç§åŸºäºæ—¶ç©ºç¨€ç–æˆ–å¯†é›†è¿åŠ¨è½¨è¿¹çš„è§†é¢‘ç”Ÿæˆæ¨¡å‹ï¼Œå¹¶å¼•å…¥äº†ä¸€ç§çµæ´»çš„è¿åŠ¨æç¤ºæ¡ä»¶è¡¨è¾¾ã€‚è¿™ç§è¡¨è¾¾æ–¹å¼èƒ½å¤Ÿç¼–ç ä»»æ„æ•°é‡çš„è½¨è¿¹ã€ç‰©ä½“ç‰¹å®šçš„æˆ–å…¨å±€åœºæ™¯çš„è¿åŠ¨ï¼Œä»¥åŠæ—¶åºç¨€ç–çš„è¿åŠ¨ã€‚ç”¨æˆ·å¯ä»¥ç›´æ¥æŒ‡å®šç¨€ç–è½¨è¿¹ï¼Œæœ¬æ–‡è¿˜å±•ç¤ºäº†å¦‚ä½•å°†é«˜çº§ç”¨æˆ·è¯·æ±‚è½¬åŒ–ä¸ºè¯¦ç»†çš„åŠå¯†é›†è¿åŠ¨æç¤ºï¼Œç§°ä¸ºè¿åŠ¨æç¤ºæ‰©å±•ã€‚æœ¬æ–‡é€šè¿‡å„ç§åº”ç”¨å±•ç¤ºäº†è¯¥æ–¹æ³•çš„é€šç”¨æ€§ï¼ŒåŒ…æ‹¬ç›¸æœºå’Œç‰©ä½“è¿åŠ¨æ§åˆ¶ã€â€œä¸å›¾åƒäº’åŠ¨â€ã€è¿åŠ¨è½¬ç§»å’Œå›¾åƒç¼–è¾‘ç­‰ã€‚ç»“æœå±•ç°äº†æ–°å…´çš„è¡Œä¸ºï¼Œå¦‚ç°å®ç‰©ç†ç°è±¡ï¼Œæ˜¾ç¤ºäº†è¿åŠ¨æç¤ºåœ¨æ¢æµ‹è§†é¢‘æ¨¡å‹å’Œä¸æœªæ¥ç”Ÿæˆä¸–ç•Œæ¨¡å‹äº’åŠ¨ä¸­çš„æ½œåŠ›ã€‚æœ€åï¼Œæœ¬æ–‡è¿›è¡Œäº†å®šé‡è¯„ä¼°ã€äººç±»ç ”ç©¶ï¼Œå¹¶å±•ç¤ºäº†å¼ºå¤§çš„æ€§èƒ½è¡¨ç°ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>åŠ¨ä½œæ§åˆ¶åœ¨ç”Ÿæˆè§†é¢‘å†…å®¹ä¸­çš„é‡è¦æ€§ã€‚</li>
<li>ç°æœ‰è§†é¢‘ç”Ÿæˆæ¨¡å‹ä¸»è¦ä¾èµ–æ–‡æœ¬æç¤ºè¿›è¡ŒåŠ¨ä½œæ§åˆ¶ï¼Œå­˜åœ¨éš¾ä»¥æ•æ‰åŠ¨æ€è¡Œä¸ºå’Œæ—¶åºç»„åˆçš„ç»†å¾®ä¹‹å¤„çš„ç¼ºé™·ã€‚</li>
<li>å¼•å…¥äº†ä¸€ç§åŸºäºæ—¶ç©ºç¨€ç–æˆ–å¯†é›†è¿åŠ¨è½¨è¿¹çš„è§†é¢‘ç”Ÿæˆæ¨¡å‹ï¼Œå¹¶ç§°ä¹‹ä¸ºâ€œè¿åŠ¨æç¤ºâ€ã€‚</li>
<li>è¿åŠ¨æç¤ºå¯ä»¥ç¼–ç ä»»æ„æ•°é‡çš„è½¨è¿¹ã€ç‰©ä½“ç‰¹å®šçš„æˆ–å…¨å±€åœºæ™¯çš„è¿åŠ¨ï¼Œä»¥åŠæ—¶åºç¨€ç–çš„è¿åŠ¨ï¼Œå…·æœ‰çµæ´»æ€§ã€‚</li>
<li>ç”¨æˆ·å¯ä»¥ç›´æ¥æŒ‡å®šç¨€ç–è½¨è¿¹ï¼ŒåŒæ—¶æœ¬æ–‡è¿˜å±•ç¤ºäº†å¦‚ä½•å°†é«˜çº§ç”¨æˆ·è¯·æ±‚è½¬åŒ–ä¸ºè¯¦ç»†çš„åŠå¯†é›†è¿åŠ¨æç¤ºï¼ˆè¿åŠ¨æç¤ºæ‰©å±•ï¼‰ã€‚</li>
<li>è¯¥æ–¹æ³•å…·æœ‰å¹¿æ³›çš„åº”ç”¨æ€§ï¼ŒåŒ…æ‹¬ç›¸æœºå’Œç‰©ä½“è¿åŠ¨æ§åˆ¶ã€ä¸å›¾åƒçš„äº’åŠ¨ã€è¿åŠ¨è½¬ç§»å’Œå›¾åƒç¼–è¾‘ç­‰ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.02700">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-ba5400b4ccad7078f3fcd89bccd68bd9.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-f412082d9bdeceb09aafff53399ecf72.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a1e8e03529298f81ab365b5d7d7091df.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3161894bd42dbfb38da86c504923149a.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-4f555e8e65db76eea91e285ef4e29c10.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="A-Unified-Framework-for-Forward-and-Inverse-Problems-in-Subsurface-Imaging-using-Latent-Space-Translations"><a href="#A-Unified-Framework-for-Forward-and-Inverse-Problems-in-Subsurface-Imaging-using-Latent-Space-Translations" class="headerlink" title="A Unified Framework for Forward and Inverse Problems in Subsurface   Imaging using Latent Space Translations"></a>A Unified Framework for Forward and Inverse Problems in Subsurface   Imaging using Latent Space Translations</h2><p><strong>Authors:Naveen Gupta, Medha Sawhney, Arka Daw, Youzuo Lin, Anuj Karpatne</strong></p>
<p>In subsurface imaging, learning the mapping from velocity maps to seismic waveforms (forward problem) and waveforms to velocity (inverse problem) is important for several applications. While traditional techniques for solving forward and inverse problems are computationally prohibitive, there is a growing interest in leveraging recent advances in deep learning to learn the mapping between velocity maps and seismic waveform images directly from data. Despite the variety of architectures explored in previous works, several open questions still remain unanswered such as the effect of latent space sizes, the importance of manifold learning, the complexity of translation models, and the value of jointly solving forward and inverse problems. We propose a unified framework to systematically characterize prior research in this area termed the Generalized Forward-Inverse (GFI) framework, building on the assumption of manifolds and latent space translations. We show that GFI encompasses previous works in deep learning for subsurface imaging, which can be viewed as specific instantiations of GFI. We also propose two new model architectures within the framework of GFI: Latent U-Net and Invertible X-Net, leveraging the power of U-Nets for domain translation and the ability of IU-Nets to simultaneously learn forward and inverse translations, respectively. We show that our proposed models achieve state-of-the-art (SOTA) performance for forward and inverse problems on a wide range of synthetic datasets, and also investigate their zero-shot effectiveness on two real-world-like datasets. Our code is available at <a target="_blank" rel="noopener" href="https://github.com/KGML-lab/Generalized-Forward-Inverse-Framework-for-DL4SI">https://github.com/KGML-lab/Generalized-Forward-Inverse-Framework-for-DL4SI</a> </p>
<blockquote>
<p>åœ¨åœ°ä¸‹æˆåƒä¸­ï¼Œå­¦ä¹ ä»é€Ÿåº¦å›¾åˆ°åœ°éœ‡æ³¢å½¢ï¼ˆæ­£é—®é¢˜ï¼‰å’Œä»æ³¢å½¢åˆ°é€Ÿåº¦ï¼ˆåé—®é¢˜ï¼‰çš„æ˜ å°„å¯¹äºå¤šä¸ªåº”ç”¨éå¸¸é‡è¦ã€‚è™½ç„¶ä¼ ç»Ÿè§£å†³æ­£é—®é¢˜å’Œåé—®é¢˜çš„æŠ€æœ¯åœ¨è®¡ç®—ä¸Šæ˜¯ç¦æ­¢çš„ï¼Œä½†è¶Šæ¥è¶Šå¤šçš„å…´è¶£åœ¨äºåˆ©ç”¨æ·±åº¦å­¦ä¹ é¢†åŸŸçš„æœ€æ–°è¿›å±•ç›´æ¥ä»æ•°æ®ä¸­å­¦ä¹ é€Ÿåº¦å›¾ä¸åœ°éœ‡æ³¢å½¢å›¾åƒä¹‹é—´çš„æ˜ å°„ã€‚å°½ç®¡ä»¥å‰çš„ç ”ç©¶ä¸­æ¢ç´¢äº†å¤šç§æ¶æ„ï¼Œä½†ä»æœ‰è®¸å¤šå…¬å¼€é—®é¢˜å°šæœªè§£å†³ï¼Œä¾‹å¦‚æ½œåœ¨ç©ºé—´å¤§å°çš„å½±å“ã€æµå½¢å­¦ä¹ çš„é‡è¦æ€§ã€ç¿»è¯‘æ¨¡å‹çš„å¤æ‚æ€§ä»¥åŠè”åˆè§£å†³æ­£é—®é¢˜å’Œåé—®é¢˜çš„ä»·å€¼ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªç»Ÿä¸€æ¡†æ¶ï¼Œæ—¨åœ¨ç³»ç»Ÿåœ°åˆ»ç”»è¿™ä¸€é¢†åŸŸçš„å…ˆå‰ç ”ç©¶ï¼Œæˆ‘ä»¬ç§°ä¹‹ä¸ºå¹¿ä¹‰æ­£åï¼ˆGFIï¼‰æ¡†æ¶ï¼Œå®ƒå»ºç«‹åœ¨æµå½¢å’Œæ½œåœ¨ç©ºé—´ç¿»è¯‘å‡è®¾çš„åŸºç¡€ä¸Šã€‚æˆ‘ä»¬è¡¨æ˜ï¼ŒGFIæ¶µç›–äº†åœ°ä¸‹æˆåƒæ·±åº¦å­¦ä¹ é¢†åŸŸçš„å…ˆå‰ç ”ç©¶ï¼Œè¿™äº›ç ”ç©¶å¯ä»¥çœ‹ä½œæ˜¯GFIçš„ç‰¹å®šå®ä¾‹ã€‚åœ¨GFIæ¡†æ¶ä¸‹ï¼Œæˆ‘ä»¬è¿˜æå‡ºäº†ä¸¤ç§æ–°å‹æ¨¡å‹æ¶æ„ï¼šæ½œåœ¨U-Netå’Œå¯é€†X-Netï¼Œå®ƒä»¬åˆ©ç”¨U-Netåœ¨é¢†åŸŸç¿»è¯‘æ–¹é¢çš„åŠ›é‡ä»¥åŠIU-NetåŒæ—¶å­¦ä¹ æ­£å‘å’Œé€†å‘ç¿»è¯‘çš„èƒ½åŠ›ã€‚æˆ‘ä»¬å±•ç¤ºäº†æˆ‘ä»¬æå‡ºçš„æ¨¡å‹åœ¨åˆæˆæ•°æ®é›†ä¸Šçš„æ­£åé—®é¢˜ä¸Šå®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œå¹¶è°ƒæŸ¥äº†å®ƒä»¬åœ¨ä¸¤ä¸ªç±»ä¼¼ç°å®ä¸–ç•Œæ•°æ®é›†ä¸Šçš„é›¶æ ·æœ¬æœ‰æ•ˆæ€§ã€‚æˆ‘ä»¬çš„ä»£ç å¯åœ¨ <a target="_blank" rel="noopener" href="https://github.com/KGML-lab/Generalized-Forward-Inverse-Framework-for-DL4SI">https://github.com/KGML-lab/Generalized-Forward-Inverse-Framework-for-DL4SI</a> æ‰¾åˆ°ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2410.11247v3">PDF</a> Accepted at ICLR 2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ¢è®¨åœ°ä¸‹æˆåƒé¢†åŸŸä¸­é€Ÿåº¦å›¾ä¸åœ°éœ‡æ³¢å½¢ä¹‹é—´çš„æ˜ å°„é—®é¢˜ï¼ŒåŒ…æ‹¬æ­£é—®é¢˜å’Œåé—®é¢˜ã€‚ä¼ ç»Ÿè§£å†³æ­£åé—®é¢˜çš„æ–¹æ³•è®¡ç®—é‡å¤§ï¼Œç°å¤šé‡‡ç”¨æ·±åº¦å­¦ä¹ æŠ€æœ¯ç›´æ¥ä»æ•°æ®ä¸­å­¦ä¹ é€Ÿåº¦å›¾ä¸åœ°éœ‡æ³¢å½¢å›¾åƒä¹‹é—´çš„æ˜ å°„å…³ç³»ã€‚æ–‡ç« æå‡ºä¸€ç§å¹¿ä¹‰æ­£é€†ï¼ˆGFIï¼‰æ¡†æ¶æ¥ç³»ç»Ÿåˆ†æè¯¥é¢†åŸŸçš„å‰æœŸç ”ç©¶ï¼Œå¹¶åœ¨æ­¤æ¡†æ¶ä¸‹æå‡ºä¸¤ç§æ–°å‹æ¨¡å‹æ¶æ„ï¼šæ½œåœ¨U-Netå’Œå¯é€†X-Netã€‚è¿™äº›æ¨¡å‹åœ¨åˆæˆæ•°æ®é›†ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œå¹¶åœ¨ä¸¤ç§æ¨¡æ‹Ÿç°å®ä¸–ç•Œæ•°æ®é›†ä¸Šå®ç°äº†é›¶æ ·æœ¬æœ‰æ•ˆæ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>åœ°ä¸‹æˆåƒä¸­ï¼Œæ­£é—®é¢˜å’Œåé—®é¢˜çš„ç ”ç©¶é‡è¦ï¼Œæ¶‰åŠé€Ÿåº¦å›¾åˆ°åœ°éœ‡æ³¢å½¢çš„æ˜ å°„ä»¥åŠåå‘è¿‡ç¨‹ã€‚</li>
<li>ä¼ ç»Ÿæ–¹æ³•è®¡ç®—é‡å¤§ï¼Œç°æœ‰ç ”ç©¶å€¾å‘äºåˆ©ç”¨æ·±åº¦å­¦ä¹ æŠ€æœ¯æ¥å­¦ä¹ é€Ÿåº¦å›¾ä¸åœ°éœ‡æ³¢å½¢ä¹‹é—´çš„ç›´æ¥æ˜ å°„ã€‚</li>
<li>æå‡ºå¹¿ä¹‰æ­£é€†ï¼ˆGFIï¼‰æ¡†æ¶ï¼Œç”¨äºç³»ç»Ÿåˆ†æè¯¥é¢†åŸŸçš„å‰æœŸç ”ç©¶ã€‚</li>
<li>åœ¨GFIæ¡†æ¶ä¸‹ï¼Œæå‡ºä¸¤ç§æ–°å‹æ¨¡å‹æ¶æ„ï¼šæ½œåœ¨U-Netå’Œå¯é€†X-Netã€‚</li>
<li>æ¨¡å‹åœ¨åˆæˆæ•°æ®é›†ä¸Šè¡¨ç°ä¼˜è¶Šï¼Œè¾¾åˆ°ç›®å‰æœ€ä½³æ°´å¹³ï¼ˆSOTAï¼‰ã€‚</li>
<li>æ¨¡å‹åœ¨æ¨¡æ‹Ÿç°å®ä¸–ç•Œæ•°æ®é›†ä¸Šå®ç°é›¶æ ·æœ¬æœ‰æ•ˆæ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2410.11247">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-8c52507908c7162d13d840091c84584c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8217b968921eeb42dbb6465291ec312a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4c9816ff7e7322bebc490d7c206a8843.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-a364660b40c10c7ad69133118967ee7c.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="Improving-Neural-Optimal-Transport-via-Displacement-Interpolation"><a href="#Improving-Neural-Optimal-Transport-via-Displacement-Interpolation" class="headerlink" title="Improving Neural Optimal Transport via Displacement Interpolation"></a>Improving Neural Optimal Transport via Displacement Interpolation</h2><p><strong>Authors:Jaemoo Choi, Yongxin Chen, Jaewoong Choi</strong></p>
<p>Optimal Transport (OT) theory investigates the cost-minimizing transport map that moves a source distribution to a target distribution. Recently, several approaches have emerged for learning the optimal transport map for a given cost function using neural networks. We refer to these approaches as the OT Map. OT Map provides a powerful tool for diverse machine learning tasks, such as generative modeling and unpaired image-to-image translation. However, existing methods that utilize max-min optimization often experience training instability and sensitivity to hyperparameters. In this paper, we propose a novel method to improve stability and achieve a better approximation of the OT Map by exploiting displacement interpolation, dubbed Displacement Interpolation Optimal Transport Model (DIOTM). We derive the dual formulation of displacement interpolation at specific time $t$ and prove how these dual problems are related across time. This result allows us to utilize the entire trajectory of displacement interpolation in learning the OT Map. Our method improves the training stability and achieves superior results in estimating optimal transport maps. We demonstrate that DIOTM outperforms existing OT-based models on image-to-image translation tasks. </p>
<blockquote>
<p>æœ€ä¼˜ä¼ è¾“ï¼ˆOTï¼‰ç†è®ºæ¢ç©¶çš„æ˜¯å°†æºåˆ†å¸ƒè½¬ç§»åˆ°ç›®æ ‡åˆ†å¸ƒçš„æˆæœ¬æœ€å°åŒ–ä¼ è¾“æ˜ å°„ã€‚æœ€è¿‘ï¼Œå·²ç»å‡ºç°äº†å‡ ç§ä½¿ç”¨ç¥ç»ç½‘ç»œä¸ºç»™å®šæˆæœ¬å‡½æ•°å­¦ä¹ æœ€ä¼˜ä¼ è¾“æ˜ å°„çš„æ–¹æ³•ã€‚æˆ‘ä»¬å°†è¿™äº›æ–¹æ³•ç§°ä¸ºOT Mapã€‚OT Mapä¸ºå¤šæ ·çš„æœºå™¨å­¦ä¹ ä»»åŠ¡ï¼ˆå¦‚ç”Ÿæˆå»ºæ¨¡å’Œæœªé…å¯¹çš„å›¾åƒåˆ°å›¾åƒç¿»è¯‘ï¼‰æä¾›äº†å¼ºå¤§çš„å·¥å…·ã€‚ç„¶è€Œï¼Œç°æœ‰çš„ä½¿ç”¨æœ€å¤§-æœ€å°ä¼˜åŒ–æ–¹æ³•çš„æ–¹æ³•é€šå¸¸å­˜åœ¨è®­ç»ƒä¸ç¨³å®šå’Œå¯¹è¶…å‚æ•°æ•æ„Ÿçš„é—®é¢˜ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„æ–¹æ³•ï¼Œé€šè¿‡åˆ©ç”¨ä½ç§»æ’å€¼æ¥æ”¹å–„ç¨³å®šæ€§ï¼Œå¹¶æ›´å¥½åœ°é€¼è¿‘OT Mapï¼Œè¢«ç§°ä¸ºä½ç§»æ’å€¼æœ€ä¼˜ä¼ è¾“æ¨¡å‹ï¼ˆDIOTMï¼‰ã€‚æˆ‘ä»¬æ¨å¯¼å‡ºä½ç§»æ’å€¼åœ¨ç‰¹å®šæ—¶é—´tçš„å¯¹å¶å…¬å¼ï¼Œå¹¶è¯æ˜è¿™äº›å¯¹å¶é—®é¢˜éšæ—¶é—´æ¨ç§»æ˜¯å¦‚ä½•ç›¸äº’å…³è”çš„ã€‚è¿™ä¸€ç»“æœå…è®¸æˆ‘ä»¬åˆ©ç”¨ä½ç§»æ’å€¼çš„æ•´ä¸ªè½¨è¿¹æ¥å­¦ä¹ OT Mapã€‚æˆ‘ä»¬çš„æ–¹æ³•æé«˜äº†è®­ç»ƒç¨³å®šæ€§ï¼Œå¹¶åœ¨ä¼°è®¡æœ€ä¼˜ä¼ è¾“æ˜ å°„æ–¹é¢å–å¾—äº†æ›´å¥½çš„ç»“æœã€‚æˆ‘ä»¬è¯æ˜DIOTMåœ¨å›¾åƒåˆ°å›¾åƒç¿»è¯‘ä»»åŠ¡ä¸Šä¼˜äºç°æœ‰çš„åŸºäºOTçš„æ¨¡å‹ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2410.03783v3">PDF</a> 20 pages</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†æœ€ä¼˜ä¼ è¾“ï¼ˆOTï¼‰ç†è®ºåŠå…¶åœ¨è®¡ç®—æœºå­¦ä¹ ä¸­çš„åº”ç”¨ï¼Œç‰¹åˆ«æ˜¯åœ¨ç”Ÿæˆå»ºæ¨¡å’Œæœªé…å¯¹å›¾åƒè½¬æ¢ç­‰é¢†åŸŸçš„åº”ç”¨ã€‚æ–‡ç« æå‡ºäº†ä¸€ç§æ–°çš„æ–¹æ³•ï¼Œå³ä½ç§»æ’å€¼æœ€ä¼˜ä¼ è¾“æ¨¡å‹ï¼ˆDIOTMï¼‰ï¼Œä»¥æé«˜è®­ç»ƒç¨³å®šæ€§å’Œå¯¹æœ€ä¼˜ä¼ è¾“åœ°å›¾çš„è¿‘ä¼¼ç²¾åº¦ã€‚è¯¥æ–¹æ³•é€šè¿‡åˆ©ç”¨ä½ç§»æ’å€¼çš„æ•´ä¸ªè½¨è¿¹æ¥å­¦ä¹ æœ€ä¼˜ä¼ è¾“åœ°å›¾ï¼Œå¹¶åœ¨å›¾åƒè½¬æ¢ä»»åŠ¡ä¸Šè¡¨ç°å‡ºä¼˜å¼‚æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æœ€ä¼˜ä¼ è¾“ï¼ˆOTï¼‰ç†è®ºæ˜¯ç ”ç©¶å¦‚ä½•å°†æºåˆ†å¸ƒè½¬ç§»åˆ°ç›®æ ‡åˆ†å¸ƒçš„æˆæœ¬æœ€å°åŒ–ä¼ è¾“å›¾ã€‚</li>
<li>ç¥ç»ç½‘ç»œå·²ç”¨äºå­¦ä¹ ç»™å®šæˆæœ¬å‡½æ•°çš„æœ€ä¼˜ä¼ è¾“å›¾ï¼Œä½†ç°æœ‰æ–¹æ³•å­˜åœ¨è®­ç»ƒä¸ç¨³å®šå’Œå¯¹è¶…å‚æ•°æ•æ„Ÿçš„é—®é¢˜ã€‚</li>
<li>æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„æ–¹æ³•ï¼Œå³ä½ç§»æ’å€¼æœ€ä¼˜ä¼ è¾“æ¨¡å‹ï¼ˆDIOTMï¼‰ï¼Œä»¥æé«˜ç¨³å®šæ€§å’Œå¯¹æœ€ä¼˜ä¼ è¾“å›¾çš„è¿‘ä¼¼ç²¾åº¦ã€‚</li>
<li>DIOTMé€šè¿‡æ¨å¯¼ä½ç§»æ’å€¼åœ¨ç‰¹å®šæ—¶é—´tçš„å¯¹å¶å…¬å¼ï¼Œå¹¶åˆ©ç”¨æ•´ä¸ªä½ç§»æ’å€¼è½¨è¿¹æ¥å­¦ä¹ æœ€ä¼˜ä¼ è¾“å›¾ã€‚</li>
<li>DIOTMæ–¹æ³•æé«˜äº†è®­ç»ƒç¨³å®šæ€§ã€‚</li>
<li>åœ¨å›¾åƒè½¬æ¢ä»»åŠ¡ä¸Šï¼ŒDIOTMè¡¨ç°å‡ºä¼˜äºç°æœ‰åŸºäºOTçš„æ¨¡å‹çš„æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2410.03783">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-3083ba409ae8da6d3439c077d6c46a20.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="AI-in-radiological-imaging-of-soft-tissue-and-bone-tumours-a-systematic-review-evaluating-against-CLAIM-and-FUTURE-AI-guidelines"><a href="#AI-in-radiological-imaging-of-soft-tissue-and-bone-tumours-a-systematic-review-evaluating-against-CLAIM-and-FUTURE-AI-guidelines" class="headerlink" title="AI in radiological imaging of soft-tissue and bone tumours: a systematic   review evaluating against CLAIM and FUTURE-AI guidelines"></a>AI in radiological imaging of soft-tissue and bone tumours: a systematic   review evaluating against CLAIM and FUTURE-AI guidelines</h2><p><strong>Authors:Douwe J. Spaanderman, Matthew Marzetti, Xinyi Wan, Andrew F. Scarsbrook, Philip Robinson, Edwin H. G. Oei, Jacob J. Visser, Robert Hemke, Kirsten van Langevelde, David F. Hanff, Geert J. L. H. van Leenders, Cornelis Verhoef, Dirk J. GruÃ¼hagen, Wiro J. Niessen, Stefan Klein, Martijn P. A. Starmans</strong></p>
<p>Soft-tissue and bone tumours (STBT) are rare, diagnostically challenging lesions with variable clinical behaviours and treatment approaches. This systematic review provides an overview of Artificial Intelligence (AI) methods using radiological imaging for diagnosis and prognosis of these tumours, highlighting challenges in clinical translation, and evaluating study alignment with the Checklist for AI in Medical Imaging (CLAIM) and the FUTURE-AI international consensus guidelines for trustworthy and deployable AI to promote the clinical translation of AI methods. The review covered literature from several bibliographic databases, including papers published before 17&#x2F;07&#x2F;2024. Original research in peer-reviewed journals focused on radiology-based AI for diagnosing or prognosing primary STBT was included. Exclusion criteria were animal, cadaveric, or laboratory studies, and non-English papers. Abstracts were screened by two of three independent reviewers for eligibility. Eligible papers were assessed against guidelines by one of three independent reviewers. The search identified 15,015 abstracts, from which 325 articles were included for evaluation. Most studies performed moderately on CLAIM, averaging a score of 28.9$\pm$7.5 out of 53, but poorly on FUTURE-AI, averaging 5.1$\pm$2.1 out of 30. Imaging-AI tools for STBT remain at the proof-of-concept stage, indicating significant room for improvement. Future efforts by AI developers should focus on design (e.g. define unmet clinical need, intended clinical setting and how AI would be integrated in clinical workflow), development (e.g. build on previous work, explainability), evaluation (e.g. evaluating and addressing biases, evaluating AI against best practices), and data reproducibility and availability (making documented code and data publicly available). Following these recommendations could improve clinical translation of AI methods. </p>
<blockquote>
<p>è½¯ç»„ç»‡åŠéª¨è‚¿ç˜¤ï¼ˆSTBTï¼‰æ˜¯ç½•è§çš„ã€è¯Šæ–­å…·æœ‰æŒ‘æˆ˜æ€§çš„ç—…å˜ï¼Œå…¶ä¸´åºŠè¡Œä¸ºå’Œæ²»ç–—æ–¹å¼å„å¼‚ã€‚è¿™ç¯‡ç³»ç»Ÿæ€§ç»¼è¿°æ¦‚è¿°äº†ä½¿ç”¨æ”¾å°„å½±åƒå­¦è¿›è¡Œè¯Šæ–­å’Œé¢„åçš„è½¯ç»„ç»‡åŠéª¨è‚¿ç˜¤çš„äººå·¥æ™ºèƒ½ï¼ˆAIï¼‰æ–¹æ³•ï¼Œå¼ºè°ƒäº†ä¸´åºŠç¿»è¯‘ä¸­çš„æŒ‘æˆ˜ï¼Œå¹¶è¯„ä¼°äº†ç ”ç©¶æ˜¯å¦ç¬¦åˆåŒ»å­¦å½±åƒå­¦äººå·¥æ™ºèƒ½ï¼ˆCLAIMï¼‰æ¸…å•ä»¥åŠå¯ä¿¡ä¸”å¯éƒ¨ç½²çš„äººå·¥æ™ºèƒ½çš„å›½é™…å…±è¯†æŒ‡å—ï¼ˆFUTURE-AIï¼‰ï¼Œä»¥ä¿ƒè¿›AIæ–¹æ³•çš„ä¸´åºŠç¿»è¯‘ã€‚ç»¼è¿°æ¶µç›–äº†å¤šä¸ªæ–‡çŒ®æ•°æ®åº“çš„æ–‡çŒ®ï¼ŒåŒ…æ‹¬åœ¨2024å¹´7æœˆ17æ—¥ä¹‹å‰å‘è¡¨çš„æ–‡ç« ã€‚çº³å…¥çš„åŸå§‹ç ”ç©¶æ˜¯å¯¹åŸºäºæ”¾å°„å­¦çš„AIç”¨äºè¯Šæ–­æˆ–é¢„æµ‹åŸå‘æ€§STBTçš„ç ”ç©¶ã€‚æ’é™¤æ ‡å‡†æ˜¯åŠ¨ç‰©ã€å°¸ä½“æˆ–å®éªŒå®¤ç ”ç©¶ï¼Œä»¥åŠéè‹±è¯­è®ºæ–‡ã€‚æ‘˜è¦ç”±ä¸‰åç‹¬ç«‹è¯„å®¡å‘˜ä¸­çš„ä¸¤åè¿›è¡Œèµ„æ ¼ç­›é€‰ã€‚åˆæ ¼è®ºæ–‡ç”±ä¸‰åç‹¬ç«‹è¯„å®¡å‘˜ä¹‹ä¸€æ ¹æ®æŒ‡å—è¿›è¡Œè¯„ä¼°ã€‚æœç´¢ç¡®å®šäº†15015ç¯‡æ‘˜è¦ï¼Œå…¶ä¸­325ç¯‡æ–‡ç« çº³å…¥è¯„ä¼°ã€‚å¤§å¤šæ•°ç ”ç©¶åœ¨CLAIMä¸Šçš„è¡¨ç°ä¸­ç­‰ï¼Œå¹³å‡å¾—åˆ†ä¸º28.9Â±7.5ï¼ˆæ»¡åˆ†ä¸º53åˆ†ï¼‰ï¼Œä½†åœ¨FUTURE-AIä¸Šçš„è¡¨ç°è¾ƒå·®ï¼Œå¹³å‡å¾—åˆ†ä¸º5.1Â±2.1ï¼ˆæ»¡åˆ†ä¸º30åˆ†ï¼‰ã€‚é’ˆå¯¹STBTçš„æˆåƒAIå·¥å…·ä»å¤„äºæ¦‚å¿µéªŒè¯é˜¶æ®µï¼Œè¡¨æ˜è¿˜æœ‰å¾ˆå¤§çš„æ”¹è¿›ç©ºé—´ã€‚æœªæ¥äººå·¥æ™ºèƒ½å¼€å‘è€…åº”é‡ç‚¹å…³æ³¨è®¾è®¡ï¼ˆå¦‚ç¡®å®šæœªæ»¡è¶³çš„ä¸´åºŠéœ€æ±‚ã€é¢„æœŸçš„ä¸´åºŠç¯å¢ƒä»¥åŠAIå¦‚ä½•èå…¥ä¸´åºŠå·¥ä½œæµç¨‹ï¼‰ã€å¼€å‘ï¼ˆå¦‚åŸºäºä»¥å¾€å·¥ä½œè¿›è¡Œå»ºè®¾ã€è§£é‡Šæ€§ï¼‰ã€è¯„ä¼°ï¼ˆå¦‚è¯„ä¼°å’Œè§£å†³åè§ã€è¯„ä¼°AIæ˜¯å¦ç¬¦åˆæœ€ä½³å®è·µï¼‰ã€æ•°æ®å¯é‡å¤æ€§å’Œå¯ç”¨æ€§ï¼ˆå…¬å¼€æä¾›æœ‰è®°å½•çš„ä»£ç å’Œæ•°æ®ï¼‰ã€‚éµå¾ªè¿™äº›å»ºè®®å¯èƒ½æœ‰åŠ©äºæ”¹å–„AIæ–¹æ³•çš„ä¸´åºŠç¿»è¯‘ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2408.12491v2">PDF</a> 25 pages, 6 figures, 8 supplementary figures</p>
<p><strong>Summary</strong><br>     è½¯ç»„ç»‡åŠéª¨è‚¿ç˜¤è¯Šæ–­ä¸é¢„åçš„äººå·¥æ™ºèƒ½å½±åƒæŠ€æœ¯ç³»ç»Ÿç»¼è¿°ï¼Œæ¢è®¨ä¸´åºŠè½¬åŒ–æŒ‘æˆ˜ï¼Œå¹¶å¯¹ç…§åŒ»ç–—å½±åƒäººå·¥æ™ºèƒ½æ¸…å•ï¼ˆCLAIMï¼‰åŠå›½é™…å¯é çš„æœªæ¥äººå·¥æ™ºèƒ½ï¼ˆFUTURE-AIï¼‰æŒ‡å—è¿›è¡Œè¯„ä»·ã€‚ç»¼è¿°æ¶µç›–å¤šä¸ªæ–‡çŒ®æ•°æ®åº“çš„ç ”ç©¶ï¼Œé‡ç‚¹è®¨è®ºåŸºäºæ”¾å°„å­¦çš„AIæŠ€æœ¯åœ¨è¯Šæ–­åŸå‘æ€§è½¯ç»„ç»‡åŠéª¨è‚¿ç˜¤ï¼ˆSTBTï¼‰æ–¹é¢çš„è¿›å±•ã€‚ç°æœ‰ç ”ç©¶åœ¨CLAIMä¸Šçš„è¡¨ç°ä¸­ç­‰ï¼Œä½†åœ¨FUTURE-AIä¸Šçš„è¡¨ç°ä¸ä½³ã€‚æœªæ¥äººå·¥æ™ºèƒ½å¼€å‘è€…åº”åœ¨è®¾è®¡ã€å¼€å‘ã€è¯„ä¼°å’Œæ•°æ®å¯é‡å¤æ€§æ–¹é¢åŠªåŠ›æ”¹è¿›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>AIåœ¨STBTè¯Šæ–­å’Œé¢„åä¸­çš„åº”ç”¨å¾—åˆ°äº†ç³»ç»Ÿæ€§å›é¡¾ã€‚</li>
<li>AIæŠ€æœ¯åœ¨æ”¾å°„å­¦å½±åƒè¯Šæ–­æ–¹é¢çš„è¿›å±•å—åˆ°å…³æ³¨ã€‚</li>
<li>ç»¼è¿°æ¢è®¨äº†ä¸´åºŠè½¬åŒ–ä¸­çš„æŒ‘æˆ˜ï¼Œå¹¶å¯¹ç…§åŒ»ç–—å½±åƒäººå·¥æ™ºèƒ½æ¸…å•ï¼ˆCLAIMï¼‰å’Œå›½é™…æŒ‡å—è¿›è¡Œè¯„ä»·ã€‚</li>
<li>ç ”ç©¶åœ¨CLAIMEè¡¨ç°ä¸­ç­‰ï¼Œä½†åœ¨FUTURE-AIæŒ‡å—ä¸Šè¡¨ç°æ¬ ä½³ã€‚</li>
<li>å½“å‰å½±åƒäººå·¥æ™ºèƒ½å·¥å…·ä»å¤„äºæ¦‚å¿µéªŒè¯é˜¶æ®µï¼Œæœ‰å·¨å¤§çš„æ”¹è¿›ç©ºé—´ã€‚</li>
<li>AIå¼€å‘è€…éœ€è¦åœ¨è®¾è®¡ã€å¼€å‘ã€è¯„ä¼°å’Œæ•°æ®çš„å¯é‡å¤æ€§æ–¹é¢åšå‡ºåŠªåŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2408.12491">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-c6d1d90463beb6ebf6b91fc61e8d7f8d.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="Style-transfer-between-Microscopy-and-Magnetic-Resonance-Imaging-via-Generative-Adversarial-Network-in-small-sample-size-settings"><a href="#Style-transfer-between-Microscopy-and-Magnetic-Resonance-Imaging-via-Generative-Adversarial-Network-in-small-sample-size-settings" class="headerlink" title="Style transfer between Microscopy and Magnetic Resonance Imaging via   Generative Adversarial Network in small sample size settings"></a>Style transfer between Microscopy and Magnetic Resonance Imaging via   Generative Adversarial Network in small sample size settings</h2><p><strong>Authors:Monika Pytlarz, Adrian Onicas, Alessandro Crimi</strong></p>
<p>Cross-modal augmentation of Magnetic Resonance Imaging (MRI) and microscopic imaging based on the same tissue samples is promising because it can allow histopathological analysis in the absence of an underlying invasive biopsy procedure. Here, we tested a method for generating microscopic histological images from MRI scans of the corpus callosum using conditional generative adversarial network (cGAN) architecture. To our knowledge, this is the first multimodal translation of the brain MRI to histological volumetric representation of the same sample. The technique was assessed by training paired image translation models taking sets of images from MRI scans and microscopy. The use of cGAN for this purpose is challenging because microscopy images are large in size and typically have low sample availability. The current work demonstrates that the framework reliably synthesizes histology images from MRI scans of corpus callosum, emphasizing the networkâ€™s ability to train on high resolution histologies paired with relatively lower-resolution MRI scans. With the ultimate goal of avoiding biopsies, the proposed tool can be used for educational purposes. </p>
<blockquote>
<p>åŸºäºåŒä¸€ç»„ç»‡æ ·æœ¬çš„ç£å…±æŒ¯æˆåƒï¼ˆMRIï¼‰å’Œæ˜¾å¾®é•œæˆåƒçš„è·¨æ¨¡æ€å¢å¼ºæ–¹æ³•å‰æ™¯å¹¿é˜”ï¼Œå› ä¸ºå®ƒå¯ä»¥åœ¨æ²¡æœ‰åŸºæœ¬çš„ä¾µå…¥æ€§æ´»æ£€ç¨‹åºçš„æƒ…å†µä¸‹è¿›è¡Œç»„ç»‡ç—…ç†å­¦åˆ†æã€‚åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬æµ‹è¯•äº†ä¸€ç§æ–¹æ³•ï¼Œåˆ©ç”¨æ¡ä»¶ç”Ÿæˆå¯¹æŠ—ç½‘ç»œï¼ˆcGANï¼‰æ¶æ„ï¼Œä»èƒ¼èƒä½“çš„MRIæ‰«æç”Ÿæˆæ˜¾å¾®é•œç»„ç»‡å­¦å›¾åƒã€‚æ®æˆ‘ä»¬æ‰€çŸ¥ï¼Œè¿™æ˜¯é¦–æ¬¡å°†å¤§è„‘MRIè½¬æ¢ä¸ºåŒä¸€æ ·æœ¬çš„ç»„ç»‡å­¦ä½“ç§¯è¡¨ç¤ºçš„å¤šæ¨¡å¼è½¬æ¢ã€‚è¯¥æŠ€æœ¯é€šè¿‡è®­ç»ƒé…å¯¹å›¾åƒç¿»è¯‘æ¨¡å‹æ¥è¯„ä¼°ï¼Œè¿™äº›æ¨¡å‹ä»MRIæ‰«æå’Œæ˜¾å¾®é•œå›¾åƒä¸­è·å–ä¿¡æ¯ã€‚ä½¿ç”¨cGANè¿›è¡Œæ­¤ç›®çš„å…·æœ‰æŒ‘æˆ˜æ€§ï¼Œå› ä¸ºæ˜¾å¾®é•œå›¾åƒå°ºå¯¸è¾ƒå¤§ä¸”æ ·æœ¬é€šå¸¸å¯ç”¨é‡è¾ƒå°‘ã€‚å½“å‰çš„å·¥ä½œè¯æ˜è¯¥æ¡†æ¶å¯ä»¥å¯é åœ°ä»èƒ¼èƒä½“çš„MRIæ‰«æä¸­åˆæˆç»„ç»‡å­¦å›¾åƒï¼Œçªå‡ºäº†ç½‘ç»œåœ¨ç›¸å¯¹è¾ƒä½çš„åˆ†è¾¨ç‡MRIæ‰«æä¸è¾ƒé«˜çš„åˆ†è¾¨ç‡ç»„ç»‡å­¦é…å¯¹è®­ç»ƒçš„èƒ½åŠ›ã€‚ä»¥æœ€ç»ˆé¿å…æ´»æ£€ä¸ºç›®æ ‡ï¼Œæ‰€æè®®çš„å·¥å…·å¯ç”¨äºæ•™å­¦ç›®çš„ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10414v2">PDF</a> 2023 IEEE International Conference on Image Processing (ICIP)</p>
<p><strong>Summary</strong>ï¼šåˆ©ç”¨æ¡ä»¶ç”Ÿæˆå¯¹æŠ—ç½‘ç»œï¼ˆcGANï¼‰æ¶æ„ï¼Œé¦–æ¬¡å®ç°äº†åŸºäºMRIçš„å¾®è§‚ç»„ç»‡å­¦å›¾åƒç”Ÿæˆï¼Œå±•ç¤ºäº†ä»MRIæ‰«æåˆ°åŒä¸€æ ·æœ¬ç»„ç»‡å­¦ä½“ç§¯è¡¨ç¤ºçš„è·¨æ¨¡æ€è½¬æ¢ã€‚å°½ç®¡é¢ä¸´æ ·æœ¬å¯ç”¨æ€§ä½å’Œå›¾åƒåˆ†è¾¨ç‡é«˜çš„æŒ‘æˆ˜ï¼Œä½†è¯¥æ¡†æ¶ä»æˆåŠŸåˆæˆå‡ºå¯é çš„ç»„ç»‡å­¦å›¾åƒï¼Œå¯ä¸ºæ•™è‚²å’Œä¸´åºŠå®è·µæä¾›æ”¯æŒã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>åˆ©ç”¨æ¡ä»¶ç”Ÿæˆå¯¹æŠ—ç½‘ç»œï¼ˆcGANï¼‰å®ç°äº†MRIä¸å¾®è§‚æˆåƒçš„è·¨æ¨¡æ€å¢å¼ºã€‚</li>
<li>æˆåŠŸå®ç°ä»MRIæ‰«æåˆ°åŒä¸€æ ·æœ¬ç»„ç»‡å­¦ä½“ç§¯è¡¨ç¤ºçš„è½¬å˜ã€‚</li>
<li>æ¡†æ¶èƒ½å¤Ÿåœ¨ä½åˆ†è¾¨ç‡MRIæ‰«æä¸Šè®­ç»ƒï¼Œå¹¶æˆåŠŸåˆæˆé«˜åˆ†è¾¨ç‡ç»„ç»‡å­¦å›¾åƒã€‚</li>
<li>è¯¥æŠ€æœ¯å¯åº”ç”¨äºæ•™è‚²ç›®çš„ï¼Œæ¨¡æ‹ŸçœŸå®å¾®è§‚ç¯å¢ƒä¸‹çš„ç—…ç†å˜åŒ–ï¼Œä»¥è¾…åŠ©å­¦ä¹ å’Œç ”ç©¶ã€‚</li>
<li>å°½ç®¡é¢ä¸´æ ·æœ¬å¯ç”¨æ€§å’Œå›¾åƒåˆ†è¾¨ç‡çš„æŒ‘æˆ˜ï¼Œä½†è¯¥æŠ€æœ¯ä»æ˜¾ç¤ºå‡ºå·¨å¤§çš„æ½œåŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2310.10414">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-fc0c12523c4b36dcaabf21ab502af2d0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4eb529541b04553defce20fdfac56c89.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ef9a59282640004234cddaa5201a1bf9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-100d26d07b2e1186e5a84cb61a73f34d.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-04-03/I2I%20Translation/">https://kedreamix.github.io/Talk2Paper/Paper/2025-04-03/I2I%20Translation/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/I2I-Translation/">
                                    <span class="chip bg-color">I2I Translation</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-04-03/%E8%A7%86%E9%A2%91%E7%90%86%E8%A7%A3/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-ec05533b496a742a58c549d69e4bd8a0.jpg" class="responsive-img" alt="è§†é¢‘ç†è§£">
                        
                        <span class="card-title">è§†é¢‘ç†è§£</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            è§†é¢‘ç†è§£ æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-04-04  TimeSearch Hierarchical Video Search with Spotlight and Reflection for   Human-like Long Video Understanding
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-04-04
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E8%A7%86%E9%A2%91%E7%90%86%E8%A7%A3/" class="post-category">
                                    è§†é¢‘ç†è§£
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E8%A7%86%E9%A2%91%E7%90%86%E8%A7%A3/">
                        <span class="chip bg-color">è§†é¢‘ç†è§£</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-04-03/Few-Shot/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-f811b76b1be7649f806abbab165d8628.jpg" class="responsive-img" alt="Few-Shot">
                        
                        <span class="card-title">Few-Shot</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Few-Shot æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-04-03  Is Temporal Prompting All We Need For Limited Labeled Action   Recognition?
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-04-03
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Few-Shot/" class="post-category">
                                    Few-Shot
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Few-Shot/">
                        <span class="chip bg-color">Few-Shot</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">32714.1k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
