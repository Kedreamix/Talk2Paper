<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="LLM">
    <meta name="description" content="LLM æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-04-03  Towards Unified Referring Expression Segmentation Across Omni-Level   Visual Target Granularities">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>LLM | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pic1.zhimg.com/v2-e673eac368ee8357e01903864dfaf4db.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">LLM</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/LLM/">
                                <span class="chip bg-color">LLM</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/LLM/" class="post-category">
                                LLM
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-04-03
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-05-14
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    19.9k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    81 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-04-03-æ›´æ–°"><a href="#2025-04-03-æ›´æ–°" class="headerlink" title="2025-04-03 æ›´æ–°"></a>2025-04-03 æ›´æ–°</h1><h2 id="Towards-Unified-Referring-Expression-Segmentation-Across-Omni-Level-Visual-Target-Granularities"><a href="#Towards-Unified-Referring-Expression-Segmentation-Across-Omni-Level-Visual-Target-Granularities" class="headerlink" title="Towards Unified Referring Expression Segmentation Across Omni-Level   Visual Target Granularities"></a>Towards Unified Referring Expression Segmentation Across Omni-Level   Visual Target Granularities</h2><p><strong>Authors:Jing Liu, Wenxuan Wang, Yisi Zhang, Yepeng Tang, Xingjian He, Longteng Guo, Tongtian Yue, Xinlong Wang</strong></p>
<p>Referring expression segmentation (RES) aims at segmenting the entitiesâ€™ masks that match the descriptive language expression. While traditional RES methods primarily address object-level grounding, real-world scenarios demand a more versatile framework that can handle multiple levels of target granularity, such as multi-object, single object or part-level references. This introduces great challenges due to the diverse and nuanced ways users describe targets. However, existing datasets and models mainly focus on designing grounding specialists for object-level target localization, lacking the necessary data resources and unified frameworks for the more practical multi-grained RES. In this paper, we take a step further towards visual granularity unified RES task. To overcome the limitation of data scarcity, we introduce a new multi-granularity referring expression segmentation (MRES) task, alongside the RefCOCOm benchmark, which includes part-level annotations for advancing finer-grained visual understanding. In addition, we create MRES-32M, the largest visual grounding dataset, comprising over 32.2M masks and captions across 1M images, specifically designed for part-level vision-language grounding. To tackle the challenges of multi-granularity RES, we propose UniRES++, a unified multimodal large language model that integrates object-level and part-level RES tasks. UniRES++ incorporates targeted designs for fine-grained visual feature exploration. With the joint model architecture and parameters, UniRES++ achieves state-of-the-art performance across multiple benchmarks, including RefCOCOm for MRES, gRefCOCO for generalized RES, and RefCOCO, RefCOCO+, RefCOCOg for classic RES. To foster future research into multi-grained visual grounding, our RefCOCOm benchmark, MRES-32M dataset and model UniRES++ will be publicly available at <a target="_blank" rel="noopener" href="https://github.com/Rubics-Xuan/MRES">https://github.com/Rubics-Xuan/MRES</a>. </p>
<blockquote>
<p>æŒ‡ä»£è¡¨è¾¾å¼åˆ†å‰²ï¼ˆRESï¼‰æ—¨åœ¨åˆ†å‰²ä¸æè¿°æ€§è¯­è¨€è¡¨è¾¾ç›¸åŒ¹é…çš„å®ä½“æ©è†œã€‚è™½ç„¶ä¼ ç»Ÿçš„RESæ–¹æ³•ä¸»è¦è§£å†³å¯¹è±¡çº§åˆ«çš„å®šä½é—®é¢˜ï¼Œä½†ç°å®åœºæ™¯éœ€è¦æ›´é€šç”¨çš„æ¡†æ¶ï¼Œèƒ½å¤Ÿå¤„ç†å¤šç§ç›®æ ‡ç²’åº¦çº§åˆ«ï¼Œä¾‹å¦‚å¤šç›®æ ‡ã€å•ç›®æ ‡æˆ–éƒ¨ä»¶çº§åˆ«çš„å¼•ç”¨ã€‚è¿™ç”±äºç”¨æˆ·æè¿°ç›®æ ‡çš„å¤šæ ·æ€§å’Œå¾®å¦™å·®å¼‚è€Œå¸¦æ¥äº†æå¤§çš„æŒ‘æˆ˜ã€‚ç„¶è€Œï¼Œç°æœ‰æ•°æ®é›†å’Œæ¨¡å‹ä¸»è¦å…³æ³¨ä¸ºå¯¹è±¡çº§åˆ«çš„ç›®æ ‡å®šä½è®¾è®¡å®šä½ä¸“å®¶ï¼Œç¼ºä¹æ›´å®ç”¨çš„å¤šç²’åº¦RESæ‰€éœ€çš„æ•°æ®èµ„æºå’Œç»Ÿä¸€æ¡†æ¶ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æœç€ç»Ÿä¸€è§†è§‰ç²’åº¦çš„RESä»»åŠ¡è¿ˆè¿›äº†ä¸€æ­¥ã€‚ä¸ºäº†å…‹æœæ•°æ®ç¨€ç¼ºçš„é™åˆ¶ï¼Œæˆ‘ä»¬å¼•å…¥äº†æ–°çš„å¤šç²’åº¦æŒ‡ä»£è¡¨è¾¾å¼åˆ†å‰²ï¼ˆMRESï¼‰ä»»åŠ¡ï¼Œä»¥åŠRefCOCOmåŸºå‡†æµ‹è¯•ï¼Œè¯¥åŸºå‡†æµ‹è¯•åŒ…å«éƒ¨ä»¶çº§åˆ«çš„æ³¨é‡Šï¼Œä»¥ä¿ƒè¿›æ›´ç²¾ç»†çš„è§†è§‰ç†è§£ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬åˆ›å»ºäº†MRES-32Mï¼Œè¿™æ˜¯æœ€å¤§çš„è§†è§‰å®šä½æ•°æ®é›†ï¼ŒåŒ…å«è¶…è¿‡3220ä¸‡ä¸ªæ©è†œå’Œæ ‡é¢˜ï¼Œæ¶µç›–100ä¸‡å¼ å›¾åƒï¼Œä¸“é—¨ç”¨äºéƒ¨ä»¶çº§åˆ«çš„è§†è§‰è¯­è¨€å®šä½ã€‚ä¸ºäº†è§£å†³å¤šç²’åº¦RESçš„æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†UniRES++ï¼Œè¿™æ˜¯ä¸€ä¸ªç»Ÿä¸€çš„å¤šæ¨¡å¼å¤§å‹è¯­è¨€æ¨¡å‹ï¼Œé›†æˆäº†å¯¹è±¡çº§åˆ«å’Œéƒ¨ä»¶çº§åˆ«çš„RESä»»åŠ¡ã€‚UniRES++é’ˆå¯¹ç²¾ç»†ç²’åº¦çš„è§†è§‰ç‰¹å¾æ¢ç´¢è¿›è¡Œäº†æœ‰é’ˆå¯¹æ€§çš„è®¾è®¡ã€‚é€šè¿‡è”åˆæ¨¡å‹ç»“æ„å’Œå‚æ•°ï¼ŒUniRES++åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­å®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼ŒåŒ…æ‹¬ç”¨äºMRESçš„RefCOCOmã€ç”¨äºå¹¿ä¹‰RESçš„gRefCOCOä»¥åŠç”¨äºç»å…¸RESçš„RefCOCOã€RefCOCO+å’ŒRefCOCOgã€‚ä¸ºäº†ä¿ƒè¿›æœªæ¥å¯¹å¤šç²’åº¦è§†è§‰å®šä½çš„ç ”ç©¶ï¼Œæˆ‘ä»¬çš„RefCOCOmåŸºå‡†æµ‹è¯•ã€MRES-32Mæ•°æ®é›†å’ŒUniRES++æ¨¡å‹å°†åœ¨<a target="_blank" rel="noopener" href="https://github.com/Rubics-Xuan/MRES">https://github.com/Rubics-Xuan/MRES</a>ä¸Šå…¬å¼€æä¾›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.01954v1">PDF</a> </p>
<p><strong>æ‘˜è¦</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†å¤šç²’åº¦å¼•ç”¨è¡¨è¾¾å¼åˆ†å‰²ï¼ˆMRESï¼‰ä»»åŠ¡åŠå…¶æŒ‘æˆ˜ã€‚ä¸ºè§£å†³æ•°æ®ç¨€ç¼ºçš„é—®é¢˜ï¼Œä½œè€…å¼•å…¥äº†RefCOCOmåŸºå‡†æµ‹è¯•é›†å’ŒMRES-32Mæ•°æ®é›†ï¼Œç”¨äºæ¨è¿›ç»†ç²’åº¦è§†è§‰ç†è§£ã€‚ä¸ºåº”å¯¹å¤šç²’åº¦RESçš„æŒ‘æˆ˜ï¼Œä½œè€…æå‡ºäº†UniRES++ç»Ÿä¸€å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼Œè¯¥æ¨¡å‹æ•´åˆäº†å¯¹è±¡çº§åˆ«å’Œéƒ¨åˆ†çº§åˆ«çš„RESä»»åŠ¡ï¼Œå¹¶å®ç°äº†å¤šä¸ªåŸºå‡†æµ‹è¯•é›†çš„æœ€ä½³æ€§èƒ½ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>å¼•ç”¨è¡¨è¾¾å¼åˆ†å‰²ï¼ˆRESï¼‰æ—¨åœ¨åˆ†å‰²ä¸æè¿°æ€§è¯­è¨€è¡¨è¾¾ç›¸åŒ¹é…çš„å®ä½“æ©è†œã€‚</li>
<li>ä¼ ç»ŸRESæ–¹æ³•ä¸»è¦è§£å†³å¯¹è±¡çº§åˆ«çš„å®šä½ï¼Œä½†ç°å®åœºæ™¯éœ€è¦æ›´çµæ´»çš„å¤„ç†å¤šç§ç²’åº¦ç›®æ ‡ã€‚</li>
<li>ç”¨æˆ·æè¿°ç›®æ ‡çš„å¤šæ ·æ€§å’Œç»†å¾®å·®åˆ«ç»™RESå¸¦æ¥äº†å¾ˆå¤§çš„æŒ‘æˆ˜ã€‚</li>
<li>æ•°æ®ç¨€ç¼ºæ˜¯é™åˆ¶å¤šç²’åº¦RESå‘å±•çš„ä¸€ä¸ªä¸»è¦é—®é¢˜ã€‚</li>
<li>ä½œè€…å¼•å…¥äº†æ–°çš„å¤šç²’åº¦å¼•ç”¨è¡¨è¾¾å¼åˆ†å‰²ï¼ˆMRESï¼‰ä»»åŠ¡å’ŒRefCOCOmåŸºå‡†æµ‹è¯•é›†æ¥è§£å†³è¿™ä¸€é—®é¢˜ã€‚</li>
<li>MRES-32Mæ•°æ®é›†çš„åˆ›å»ºæ˜¯ä¸ºäº†æ¨è¿›ç»†ç²’åº¦è§†è§‰ç†è§£ï¼ŒåŒ…å«è¶…è¿‡32.2Mçš„æ©è†œå’Œæ ‡é¢˜ã€‚</li>
<li>UniRES++æ¨¡å‹æ˜¯ç¬¬ä¸€ä¸ªç»Ÿä¸€çš„å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼Œç”¨äºå¤„ç†å¯¹è±¡çº§åˆ«å’Œéƒ¨åˆ†çº§åˆ«çš„RESä»»åŠ¡ï¼Œå¹¶åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•é›†ä¸Šå®ç°äº†æœ€ä½³æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.01954">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-935f87848d7b498eccf29fb7d7a2c45f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-59f9f116af0f86d86a506ae8512f5f76.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-480c638e94fdc4e1e2f8a69c33591a4e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c999c00260917a8f66034b1ae980ccbb.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-32388cd402944d484cb88f0be43c351d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-57e8546915c8432e910b1c2b853425b5.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-7893fb39ca0d8aa3590b9f672bda352d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-190c45dcf2ccef334328cac3b8a07945.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="OpenCodeReasoning-Advancing-Data-Distillation-for-Competitive-Coding"><a href="#OpenCodeReasoning-Advancing-Data-Distillation-for-Competitive-Coding" class="headerlink" title="OpenCodeReasoning: Advancing Data Distillation for Competitive Coding"></a>OpenCodeReasoning: Advancing Data Distillation for Competitive Coding</h2><p><strong>Authors:Wasi Uddin Ahmad, Sean Narenthiran, Somshubra Majumdar, Aleksander Ficek, Siddhartha Jain, Jocelyn Huang, Vahid Noroozi, Boris Ginsburg</strong></p>
<p>Since the advent of reasoning-based large language models, many have found great success from distilling reasoning capabilities into student models. Such techniques have significantly bridged the gap between reasoning and standard LLMs on coding tasks. Despite this, much of the progress on distilling reasoning models remains locked behind proprietary datasets or lacks details on data curation, filtering and subsequent training. To address this, we construct a superior supervised fine-tuning (SFT) dataset that we use to achieve state-of-the-art coding capability results in models of various sizes. Our distilled models use only SFT to achieve 61.8% on LiveCodeBench and 24.6% on CodeContests, surpassing alternatives trained with reinforcement learning. We then perform analysis on the data sources used to construct our dataset, the impact of code execution filtering, and the importance of instruction&#x2F;solution diversity. We observe that execution filtering negatively affected benchmark accuracy, leading us to prioritize instruction diversity over solution correctness. Finally, we also analyze the token efficiency and reasoning patterns utilized by these models. We will open-source these datasets and distilled models to the community. </p>
<blockquote>
<p>è‡ªä»åŸºäºæ¨ç†çš„å¤§å‹è¯­è¨€æ¨¡å‹å‡ºç°ä»¥æ¥ï¼Œè®¸å¤šäººé€šè¿‡å°†æ¨ç†èƒ½åŠ›è’¸é¦åˆ°å­¦ç”Ÿæ¨¡å‹ä¸­å–å¾—äº†å·¨å¤§æˆåŠŸã€‚è¿™ç±»æŠ€æœ¯åœ¨ç¼–ç ä»»åŠ¡ä¸Šæ˜¾è‘—ç¼©å°äº†æ¨ç†å’Œæ ‡å‡†å¤§å‹è¯­è¨€æ¨¡å‹ä¹‹é—´çš„å·®è·ã€‚å°½ç®¡å¦‚æ­¤ï¼Œå…³äºè’¸é¦æ¨ç†æ¨¡å‹çš„è®¸å¤šè¿›å±•ä»ç„¶å±€é™äºä¸“æœ‰æ•°æ®é›†ï¼Œæˆ–è€…ç¼ºä¹å…³äºæ•°æ®æ”¶é›†ã€è¿‡æ»¤å’Œåç»­è®­ç»ƒæ–¹é¢çš„è¯¦ç»†ä¿¡æ¯ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æ„å»ºäº†ä¸€ä¸ªå‡ºè‰²çš„ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰æ•°æ®é›†ï¼Œæˆ‘ä»¬ç”¨å®ƒåœ¨å„ç§è§„æ¨¡çš„æ¨¡å‹ä¸­å®ç°äº†æœ€å…ˆè¿›çš„ç¼–ç èƒ½åŠ›ç»“æœã€‚æˆ‘ä»¬çš„è’¸é¦æ¨¡å‹ä»…ä½¿ç”¨SFTåœ¨LiveCodeBenchä¸Šè¾¾åˆ°61.8%ï¼Œåœ¨CodeContestsä¸Šè¾¾åˆ°24.6%ï¼Œè¶…è¿‡äº†ä½¿ç”¨å¼ºåŒ–å­¦ä¹ è®­ç»ƒçš„æ›¿ä»£æ¨¡å‹ã€‚ç„¶åï¼Œæˆ‘ä»¬å¯¹æ„å»ºæˆ‘ä»¬æ•°æ®é›†æ‰€ä½¿ç”¨çš„æ•°æ®æ¥æºã€ä»£ç æ‰§è¡Œè¿‡æ»¤çš„å½±å“ä»¥åŠæŒ‡ä»¤&#x2F;è§£å†³æ–¹æ¡ˆå¤šæ ·æ€§çš„é‡è¦æ€§è¿›è¡Œäº†åˆ†æã€‚æˆ‘ä»¬å‘ç°æ‰§è¡Œè¿‡æ»¤å¯¹åŸºå‡†æµ‹è¯•å‡†ç¡®æ€§äº§ç”Ÿäº†è´Ÿé¢å½±å“ï¼Œå› æ­¤æˆ‘ä»¬ä¼˜å…ˆé‡è§†æŒ‡ä»¤çš„å¤šæ ·æ€§è€Œéè§£å†³æ–¹æ¡ˆçš„æ­£ç¡®æ€§ã€‚æœ€åï¼Œæˆ‘ä»¬è¿˜åˆ†æäº†è¿™äº›æ¨¡å‹çš„ä»¤ç‰Œæ•ˆç‡å’Œæ¨ç†æ¨¡å¼ã€‚æˆ‘ä»¬å°†å‘ç¤¾åŒºå¼€æºè¿™äº›æ•°æ®é›†å’Œè’¸é¦æ¨¡å‹ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.01943v1">PDF</a> Work in progress</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†é€šè¿‡æ„å»ºä¼˜è´¨ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰æ•°æ®é›†ï¼Œå°†æ¨ç†èƒ½åŠ›è’¸é¦åˆ°å­¦ç”Ÿæ¨¡å‹ä¸­çš„æ–¹æ³•ï¼Œå¹¶åœ¨å„ç§è§„æ¨¡çš„æ¨¡å‹ä¸Šå®ç°äº†æœ€å…ˆè¿›çš„ç¼–ç èƒ½åŠ›ç»“æœã€‚é€šè¿‡ä½¿ç”¨ä»…SFTï¼Œæˆ‘ä»¬çš„è’¸é¦æ¨¡å‹åœ¨LiveCodeBenchä¸Šè¾¾åˆ°61.8%ï¼Œåœ¨CodeContestsä¸Šè¾¾åˆ°24.6%ï¼Œè¶…è¶Šäº†ä½¿ç”¨å¼ºåŒ–å­¦ä¹ è®­ç»ƒçš„æ›¿ä»£æ¨¡å‹ã€‚æœ¬æ–‡å¯¹æ„å»ºæ•°æ®é›†ä½¿ç”¨çš„æ•°æ®æ¥æºã€ä»£ç æ‰§è¡Œè¿‡æ»¤çš„å½±å“ä»¥åŠæŒ‡ä»¤&#x2F;è§£å†³æ–¹æ¡ˆå¤šæ ·æ€§çš„é‡è¦æ€§è¿›è¡Œäº†åˆ†æã€‚å‘ç°æ‰§è¡Œè¿‡æ»¤ä¼šå¯¹åŸºå‡†æµ‹è¯•å‡†ç¡®æ€§äº§ç”Ÿè´Ÿé¢å½±å“ï¼Œå› æ­¤æŒ‡ä»¤å¤šæ ·æ€§ä¼˜å…ˆäºè§£å†³æ–¹æ¡ˆçš„æ­£ç¡®æ€§ã€‚æœ€åï¼Œè¿˜åˆ†æäº†è¿™äº›æ¨¡å‹çš„ä»¤ç‰Œæ•ˆç‡å’Œæ¨ç†æ¨¡å¼ã€‚æˆ‘ä»¬å°†è¿™äº›æ•°æ®é›†å’Œè’¸é¦æ¨¡å‹å¼€æºç»™ç¤¾åŒºä½¿ç”¨ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>é€šè¿‡æ„å»ºä¼˜è´¨ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰æ•°æ®é›†ï¼ŒæˆåŠŸå°†æ¨ç†èƒ½åŠ›è’¸é¦åˆ°å­¦ç”Ÿæ¨¡å‹ä¸­ï¼Œå®ç°å…ˆè¿›ç¼–ç èƒ½åŠ›ã€‚</li>
<li>ä½¿ç”¨ä»…SFTçš„è’¸é¦æ¨¡å‹åœ¨LiveCodeBenchå’ŒCodeContestsä¸Šå–å¾—è¾ƒé«˜æˆç»©ã€‚</li>
<li>å¯¹æ¯”å¼ºåŒ–å­¦ä¹ è®­ç»ƒçš„æ¨¡å‹ï¼ŒSFTæ–¹æ³•è¡¨ç°æ›´ä¼˜ã€‚</li>
<li>åˆ†æå‘ç°æ‰§è¡Œè¿‡æ»¤å¯¹åŸºå‡†æµ‹è¯•å‡†ç¡®æ€§æœ‰è´Ÿé¢å½±å“ï¼Œå› æ­¤é‡è§†æŒ‡ä»¤å¤šæ ·æ€§ã€‚</li>
<li>å¼€æºæ•°æ®é›†å’Œè’¸é¦æ¨¡å‹ï¼Œä¾¿äºç¤¾åŒºä½¿ç”¨ã€‚</li>
<li>åˆ†æäº†æ¨¡å‹çš„ä»¤ç‰Œæ•ˆç‡å’Œæ¨ç†æ¨¡å¼ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.01943">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-d4380a2da8480d4b9e1fcb7c28f44a90.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a0bb60cea00b040106932355d54f872f.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-6bd0fdea622909f4419f65eb13767bb7.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-003554efde4e4b026d13dedecf6f056d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8561d6b1998c13873167749f11edc13d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1170c2a7a6ec92d93d72822e29ec3937.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="A-thorough-benchmark-of-automatic-text-classification-From-traditional-approaches-to-large-language-models"><a href="#A-thorough-benchmark-of-automatic-text-classification-From-traditional-approaches-to-large-language-models" class="headerlink" title="A thorough benchmark of automatic text classification: From traditional   approaches to large language models"></a>A thorough benchmark of automatic text classification: From traditional   approaches to large language models</h2><p><strong>Authors:Washington Cunha, Leonardo Rocha, Marcos AndrÃ© GonÃ§alves</strong></p>
<p>Automatic text classification (ATC) has experienced remarkable advancements in the past decade, best exemplified by recent small and large language models (SLMs and LLMs), leveraged by Transformer architectures. Despite recent effectiveness improvements, a comprehensive cost-benefit analysis investigating whether the effectiveness gains of these recent approaches compensate their much higher costs when compared to more traditional text classification approaches such as SVMs and Logistic Regression is still missing in the literature. In this context, this workâ€™s main contributions are twofold: (i) we provide a scientifically sound comparative analysis of the cost-benefit of twelve traditional and recent ATC solutions including five open LLMs, and (ii) a large benchmark comprising {22 datasets}, including sentiment analysis and topic classification, with their (train-validation-test) partitions based on folded cross-validation procedures, along with documentation, and code. The release of code, data, and documentation enables the community to replicate experiments and advance the field in a more scientifically sound manner. Our comparative experimental results indicate that LLMs outperform traditional approaches (up to 26%-7.1% on average) and SLMs (up to 4.9%-1.9% on average) in terms of effectiveness. However, LLMs incur significantly higher computational costs due to fine-tuning, being, on average 590x and 8.5x slower than traditional methods and SLMs, respectively. Results suggests the following recommendations: (1) LLMs for applications that require the best possible effectiveness and can afford the costs; (2) traditional methods such as Logistic Regression and SVM for resource-limited applications or those that cannot afford the cost of tuning large LLMs; and (3) SLMs like Roberta for near-optimal effectiveness-efficiency trade-off. </p>
<blockquote>
<p>æ–‡æœ¬åˆ†ç±»ï¼ˆATCï¼‰åœ¨è¿‡å»çš„åå¹´ä¸­å–å¾—äº†æ˜¾è‘—çš„è¿›æ­¥ï¼Œæœ€è¿‘çš„å°å‹å’Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆSLMå’ŒLLMï¼‰å°±æ˜¯æœ€å¥½çš„ä¾‹è¯ï¼Œå®ƒä»¬åˆ©ç”¨äº†Transformeræ¶æ„ã€‚å°½ç®¡è¿‘æœŸçš„æœ‰æ•ˆæ€§æ”¹è¿›å¾—åˆ°äº†å¾ˆå¥½çš„éªŒè¯ï¼Œä½†åœ¨æ–‡çŒ®ä¸­ä»ç¼ºä¹å…³äºè¿™äº›æ–°æ–¹æ³•çš„æœ‰æ•ˆæ€§æ”¶ç›Šæ˜¯å¦å¼¥è¡¥äº†å…¶ä¸ä¼ ç»Ÿæ–‡æœ¬åˆ†ç±»æ–¹æ³•ï¼ˆå¦‚SVMå’Œé€»è¾‘å›å½’ï¼‰ç›¸æ¯”çš„é«˜æ˜‚æˆæœ¬çš„å…¨é¢æˆæœ¬æ•ˆç›Šåˆ†æã€‚åœ¨æ­¤èƒŒæ™¯ä¸‹ï¼Œè¿™é¡¹å·¥ä½œçš„ä¸»è¦è´¡çŒ®æœ‰ä¸¤ç‚¹ï¼šé¦–å…ˆï¼Œæˆ‘ä»¬å¯¹åŒ…æ‹¬äº”ç§å¼€æºå¤§å‹è¯­è¨€æ¨¡å‹åœ¨å†…çš„åäºŒç§ä¼ ç»Ÿå’Œæœ€æ–°çš„è‡ªåŠ¨æ–‡æœ¬åˆ†ç±»è§£å†³æ–¹æ¡ˆçš„æˆæœ¬æ•ˆç›Šè¿›è¡Œäº†ç§‘å­¦çš„æ¯”è¾ƒåˆ†æï¼›å…¶æ¬¡ï¼Œæˆ‘ä»¬æ„å»ºäº†ä¸€ä¸ªåŒ…å«æƒ…æ„Ÿåˆ†æå’Œä¸»é¢˜åˆ†ç±»çš„{22ä¸ªæ•°æ®é›†}çš„å¤§å‹åŸºå‡†æµ‹è¯•é›†ï¼ŒåŸºäºæŠ˜å äº¤å‰éªŒè¯ç¨‹åºå¯¹å…¶è¿›è¡Œäº†ï¼ˆè®­ç»ƒ-éªŒè¯-æµ‹è¯•ï¼‰åˆ†åŒºï¼Œå¹¶é™„å¸¦äº†æ–‡æ¡£å’Œä»£ç ã€‚ä»£ç çš„å‘å¸ƒã€æ•°æ®å’Œæ–‡æ¡£èµ„æ–™ä½¿ç¤¾åŒºèƒ½å¤Ÿå¤åˆ¶å®éªŒå¹¶ä»¥æ›´ç§‘å­¦çš„æ–¹å¼æ¨åŠ¨è¯¥é¢†åŸŸçš„å‘å±•ã€‚æˆ‘ä»¬çš„æ¯”è¾ƒå®éªŒç»“æœæŒ‡å‡ºï¼Œå¤§å‹è¯­è¨€æ¨¡å‹åœ¨æœ‰æ•ˆæ€§æ–¹é¢ä¼˜äºä¼ ç»Ÿæ–¹æ³•ï¼ˆå¹³å‡é«˜å‡º26%~7.1%ï¼‰å’Œå°å‹è¯­è¨€æ¨¡å‹ï¼ˆå¹³å‡é«˜å‡º4.9%~1.9%ï¼‰ã€‚ç„¶è€Œï¼Œç”±äºå¾®è°ƒçš„éœ€è¦ï¼Œå¤§å‹è¯­è¨€æ¨¡å‹çš„è®¡ç®—æˆæœ¬æ˜¾è‘—æ›´é«˜ï¼Œå¹³å‡è€Œè¨€ï¼Œå®ƒä»¬æ¯”ä¼ ç»Ÿæ–¹æ³•å’Œå°å‹è¯­è¨€æ¨¡å‹æ…¢590å€å’Œ8.5å€ã€‚æ ¹æ®ç ”ç©¶ç»“æœï¼Œæˆ‘ä»¬æå‡ºä»¥ä¸‹å»ºè®®ï¼š1ï¼‰å¯¹äºè¦æ±‚æœ€ä½³æ•ˆæœå¹¶èƒ½æ‰¿å—æˆæœ¬çš„åº”ç”¨ç¨‹åºï¼Œä½¿ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼›2ï¼‰å¯¹äºèµ„æºæœ‰é™æˆ–æ— æ³•æ‰¿å—å¤§å‹è¯­è¨€æ¨¡å‹è°ƒæ•´æˆæœ¬çš„åº”ç”¨ç¨‹åºï¼Œä½¿ç”¨ä¼ ç»Ÿæ–¹æ³•ï¼Œå¦‚é€»è¾‘å›å½’å’ŒSVMï¼›3ï¼‰å¯¹äºå¯»æ±‚è¿‘ä¹æœ€ä¼˜çš„æœ‰æ•ˆæ€§å’Œæ•ˆç‡å¹³è¡¡çš„åº”ç”¨ç¨‹åºï¼Œä½¿ç”¨å°å‹è¯­è¨€æ¨¡å‹ï¼ˆå¦‚Robertaï¼‰ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.01930v1">PDF</a> 7 pages, 2 figures, 3 tables</p>
<p><strong>Summary</strong></p>
<p>è¯¥æ–‡å¯¹æ¯”åˆ†æäº†ä¼ ç»Ÿä¸æœ€æ–°çš„è‡ªåŠ¨æ–‡æœ¬åˆ†ç±»ï¼ˆATCï¼‰è§£å†³æ–¹æ¡ˆçš„æˆæœ¬æ•ˆç›Šï¼ŒåŒ…æ‹¬äº”ç§å¼€æºçš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ã€‚ç ”ç©¶å‘ç°ï¼ŒLLMsåœ¨æ•ˆæœä¸Šä¼˜äºä¼ ç»Ÿæ–¹æ³•å’Œå°å‹è¯­è¨€æ¨¡å‹ï¼ˆSLMsï¼‰ï¼Œä½†è®¡ç®—æˆæœ¬æ›´é«˜ã€‚å› æ­¤ï¼Œæ ¹æ®åº”ç”¨éœ€æ±‚å’Œèµ„æºé™åˆ¶ï¼Œå¯é€‰æ‹©ä¸åŒçš„æ–¹æ³•ã€‚å¯¹äºè¿½æ±‚æœ€ä½³æ•ˆæœä¸”èƒ½æ‰¿å—æˆæœ¬çš„åº”ç”¨ï¼Œæ¨èä½¿ç”¨LLMsï¼›å¯¹äºèµ„æºæœ‰é™æˆ–æ— æ³•æ‰¿æ‹…å¤§å‹è¯­è¨€æ¨¡å‹è°ƒæ•´æˆæœ¬çš„åº”ç”¨ï¼Œæ¨èä½¿ç”¨ä¼ ç»Ÿæ–¹æ³•ï¼Œå¦‚é€»è¾‘å›å½’å’Œæ”¯æŒå‘é‡æœºï¼›å¯¹äºå¯»æ±‚è¿‘ä¼¼çš„æœ€ä¼˜æ•ˆç›Šæ•ˆç‡æƒè¡¡çš„ï¼Œå»ºè®®ä½¿ç”¨SLMsï¼Œå¦‚Robertaã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLMsåœ¨è‡ªåŠ¨æ–‡æœ¬åˆ†ç±»ä¸­è¡¨ç°å‡ºä¼˜å¼‚çš„æ•ˆæœï¼Œç›¸æ¯”ä¼ ç»Ÿæ–¹æ³•å’ŒSLMsæœ‰æ›´é«˜çš„åˆ†ç±»å‡†ç¡®æ€§ã€‚</li>
<li>LLMsçš„è®¡ç®—æˆæœ¬æ˜¾è‘—è¾ƒé«˜ï¼Œéœ€è¦è¿›è¡Œç²¾ç»†è°ƒæ•´ã€‚</li>
<li>LLMsã€ä¼ ç»Ÿæ–¹æ³•å’ŒSLMsä¹‹é—´å­˜åœ¨çš„æ•ˆç›Šæ•ˆç‡æƒè¡¡éœ€æ ¹æ®å…·ä½“åº”ç”¨åœºæ™¯é€‰æ‹©ã€‚</li>
<li>å¯¹äºèµ„æºå—é™æˆ–æˆæœ¬æ•æ„Ÿçš„åº”ç”¨ï¼Œæ¨èä½¿ç”¨ä¼ ç»Ÿæ–¹æ³•å¦‚é€»è¾‘å›å½’å’Œæ”¯æŒå‘é‡æœºã€‚</li>
<li>å¯¹äºè¿½æ±‚æœ€ä½³æ•ˆæœçš„åœºæ™¯ï¼ŒLLMsæ˜¯ç†æƒ³é€‰æ‹©ã€‚</li>
<li>SLMså¦‚Robertaæä¾›äº†è¿‘ä¼¼çš„æœ€ä¼˜æ•ˆç›Šæ•ˆç‡æƒè¡¡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.01930">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-ba42484bb44a8039dda8e0fdbe5cb485.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-10f73cfd0b46a3130c060a1a0abb54bc.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c82471e7dabb031a45468af7a7aab20d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a93a690b84867ee53fe8054c6435058a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f0f3952e97b50d4d4cc2fd903f21d91e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b51abde8e40dfcd2b42dab547f9072d9.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="STAR-1-Safer-Alignment-of-Reasoning-LLMs-with-1K-Data"><a href="#STAR-1-Safer-Alignment-of-Reasoning-LLMs-with-1K-Data" class="headerlink" title="STAR-1: Safer Alignment of Reasoning LLMs with 1K Data"></a>STAR-1: Safer Alignment of Reasoning LLMs with 1K Data</h2><p><strong>Authors:Zijun Wang, Haoqin Tu, Yuhan Wang, Juncheng Wu, Jieru Mei, Brian R. Bartoldson, Bhavya Kailkhura, Cihang Xie</strong></p>
<p>This paper introduces STAR-1, a high-quality, just-1k-scale safety dataset specifically designed for large reasoning models (LRMs) like DeepSeek-R1. Built on three core principles â€“ diversity, deliberative reasoning, and rigorous filtering â€“ STAR-1 aims to address the critical needs for safety alignment in LRMs. Specifically, we begin by integrating existing open-source safety datasets from diverse sources. Then, we curate safety policies to generate policy-grounded deliberative reasoning samples. Lastly, we apply a GPT-4o-based safety scoring system to select training examples aligned with best practices. Experimental results show that fine-tuning LRMs with STAR-1 leads to an average 40% improvement in safety performance across four benchmarks, while only incurring a marginal decrease (e.g., an average of 1.1%) in reasoning ability measured across five reasoning tasks. Extensive ablation studies further validate the importance of our design principles in constructing STAR-1 and analyze its efficacy across both LRMs and traditional LLMs. Our project page is <a target="_blank" rel="noopener" href="https://ucsc-vlaa.github.io/STAR-1">https://ucsc-vlaa.github.io/STAR-1</a>. </p>
<blockquote>
<p>æœ¬æ–‡ä»‹ç»äº†STAR-1ï¼Œè¿™æ˜¯ä¸€ä¸ªä¸“ä¸ºå¤§å‹æ¨ç†æ¨¡å‹ï¼ˆå¦‚DeepSeek-R1ï¼‰è®¾è®¡çš„é«˜è´¨é‡ã€ä»…åŒ…å«ä¸€åƒä¸ªæ ·æœ¬çš„å®‰å…¨æ•°æ®é›†ã€‚åŸºäºå¤šæ ·æ€§ã€å®¡æ…æ¨ç†å’Œä¸¥æ ¼ç­›é€‰ä¸‰å¤§æ ¸å¿ƒåŸåˆ™ï¼ŒSTAR-1æ—¨åœ¨è§£å†³å¤§å‹æ¨ç†æ¨¡å‹åœ¨å®‰å…¨å¯¹é½æ–¹é¢çš„å…³é”®éœ€æ±‚ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬é¦–å…ˆæ•´åˆæ¥è‡ªä¸åŒæ¥æºçš„ç°æœ‰å¼€æºå®‰å…¨æ•°æ®é›†ã€‚ç„¶åï¼Œæˆ‘ä»¬åˆ¶å®šå®‰å…¨ç­–ç•¥ä»¥ç”ŸæˆåŸºäºç­–ç•¥å®¡æ…æ¨ç†æ ·æœ¬ã€‚æœ€åï¼Œæˆ‘ä»¬åº”ç”¨åŸºäºGPT-4çš„å®‰å…¨è¯„åˆ†ç³»ç»Ÿæ¥é€‰æ‹©ä¸æœ€ä½³å®è·µå¯¹é½çš„è®­ç»ƒæ ·æœ¬ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œä½¿ç”¨STAR-1å¯¹å¤§å‹æ¨ç†æ¨¡å‹è¿›è¡Œå¾®è°ƒï¼Œåœ¨å››ä¸ªåŸºå‡†æµ‹è¯•ä¸Šçš„å®‰å…¨æ€§èƒ½å¹³å‡æé«˜äº†4. æå‡å¹…åº¦è¾¾ ç™¾åˆ†ä¹‹å››åï¼ŒåŒæ—¶ä»…åœ¨äº”ä¸ªæ¨ç†ä»»åŠ¡ä¸­ç•¥å¾®é™ä½æ¨ç†èƒ½åŠ›ï¼ˆå¹³å‡ä¸‹é™ç™¾åˆ†ä¹‹ä¸€ç‚¹ä¸€ï¼‰ã€‚å¹¿æ³›çš„æ¶ˆèç ”ç©¶è¿›ä¸€æ­¥éªŒè¯äº†æˆ‘ä»¬çš„è®¾è®¡åŸåˆ™åœ¨æ„å»ºSTAR-1ä¸­çš„é‡è¦æ€§ï¼Œå¹¶åˆ†æäº†å…¶åœ¨å¤§å‹æ¨ç†æ¨¡å‹å’Œä¼ ç»Ÿå¤§å‹è¯­è¨€æ¨¡å‹ä¸­çš„æœ‰æ•ˆæ€§ã€‚æˆ‘ä»¬çš„é¡¹ç›®é¡µé¢æ˜¯ï¼š<a target="_blank" rel="noopener" href="https://ucsc-vlaa.github.io/STAR-1%E3%80%82">https://ucsc-vlaa.github.io/STAR-1ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.01903v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>STAR-1æ•°æ®é›†æ—¨åœ¨æ»¡è¶³å¤§å‹æ¨ç†æ¨¡å‹ï¼ˆLRMsï¼‰åœ¨å®‰å…¨æ€§æ–¹é¢çš„éœ€æ±‚ã€‚å®ƒé€šè¿‡é›†æˆå¤šç§å¼€æºæ•°æ®é›†ã€åˆ¶å®šå®‰å…¨ç­–ç•¥ï¼Œå¹¶åŸºäºGPT-4oçš„å®‰å…¨è¯„åˆ†ç³»ç»Ÿç­›é€‰è®­ç»ƒæ ·æœ¬ï¼Œä»è€Œç¡®ä¿æ•°æ®é›†çš„å¤šæ ·æ€§å’Œå®¡æ…æ¨ç†èƒ½åŠ›ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œä½¿ç”¨STAR-1å¾®è°ƒçš„LRMåœ¨å››ä¸ªåŸºå‡†æµ‹è¯•ä¸­å®‰å…¨æ€§èƒ½å¹³å‡æå‡40%ï¼ŒåŒæ—¶æ¨ç†èƒ½åŠ›ä»…å°å¹…ä¸‹é™ï¼ˆå¹³å‡ä¸‹é™ç‡ä¸º1.1%ï¼‰ã€‚ç ”ç©¶è¿˜é€šè¿‡æ¶ˆèç ”ç©¶éªŒè¯äº†æ•°æ®é›†è®¾è®¡çš„ä¸‰ä¸ªæ ¸å¿ƒåŸåˆ™çš„é‡è¦æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>STAR-1æ˜¯ä¸€ä¸ªä¸“ä¸ºå¤§å‹æ¨ç†æ¨¡å‹è®¾è®¡çš„é«˜è´¨é‡å®‰å…¨æ•°æ®é›†ã€‚</li>
<li>æ•°æ®é›†æ„å»ºåŸºäºå¤šæ ·æ€§ã€å®¡æ…æ¨ç†å’Œä¸¥æ ¼ç­›é€‰ä¸‰å¤§åŸåˆ™ã€‚</li>
<li>é€šè¿‡é›†æˆå¤šç§å¼€æºæ•°æ®é›†æ¥å¢å¼ºæ•°æ®å¤šæ ·æ€§ã€‚</li>
<li>åˆ¶å®šå®‰å…¨ç­–ç•¥ï¼Œç”ŸæˆåŸºäºç­–ç•¥çš„å®¡æ…æ¨ç†æ ·æœ¬ã€‚</li>
<li>ä½¿ç”¨GPT-4oçš„å®‰å…¨è¯„åˆ†ç³»ç»Ÿç­›é€‰è®­ç»ƒæ ·æœ¬ï¼Œä¸æœ€ä½³å®è·µå¯¹é½ã€‚</li>
<li>å®éªŒç»“æœæ˜¾ç¤ºï¼Œä½¿ç”¨STAR-1å¾®è°ƒçš„LRMå®‰å…¨æ€§èƒ½å¤§å¹…æå‡ï¼ŒåŒæ—¶æ¨ç†èƒ½åŠ›å½±å“è¾ƒå°ã€‚</li>
<li>æ¶ˆèç ”ç©¶éªŒè¯äº†æ•°æ®é›†è®¾è®¡çš„æ ¸å¿ƒåŸåˆ™çš„é‡è¦æ€§ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.01903">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-eacf55114141fbf8489fd896f6fca4ae.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a3bce844203c398839f3e89586195374.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-40d99782dafeba56ee8265365c9370be.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-832ebff3e52aed24090d3653c44e6f77.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-85fc740539de2c3066cdb182445fa637.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Ross3D-Reconstructive-Visual-Instruction-Tuning-with-3D-Awareness"><a href="#Ross3D-Reconstructive-Visual-Instruction-Tuning-with-3D-Awareness" class="headerlink" title="Ross3D: Reconstructive Visual Instruction Tuning with 3D-Awareness"></a>Ross3D: Reconstructive Visual Instruction Tuning with 3D-Awareness</h2><p><strong>Authors:Haochen Wang, Yucheng Zhao, Tiancai Wang, Haoqiang Fan, Xiangyu Zhang, Zhaoxiang Zhang</strong></p>
<p>The rapid development of Large Multimodal Models (LMMs) for 2D images and videos has spurred efforts to adapt these models for interpreting 3D scenes. However, the absence of large-scale 3D vision-language datasets has posed a significant obstacle. To address this issue, typical approaches focus on injecting 3D awareness into 2D LMMs by designing 3D input-level scene representations. This work provides a new perspective. We introduce reconstructive visual instruction tuning with 3D-awareness (Ross3D), which integrates 3D-aware visual supervision into the training procedure. Specifically, it incorporates cross-view and global-view reconstruction. The former requires reconstructing masked views by aggregating overlapping information from other views. The latter aims to aggregate information from all available views to recover Birdâ€™s-Eye-View images, contributing to a comprehensive overview of the entire scene. Empirically, Ross3D achieves state-of-the-art performance across various 3D scene understanding benchmarks. More importantly, our semi-supervised experiments demonstrate significant potential in leveraging large amounts of unlabeled 3D vision-only data. </p>
<blockquote>
<p>å¤§å‹å¤šæ¨¡æ€æ¨¡å‹ï¼ˆLMMï¼‰åœ¨2Då›¾åƒå’Œè§†é¢‘é¢†åŸŸçš„å¿«é€Ÿå‘å±•ï¼Œæ¿€å‘äº†å°†è¿™äº›æ¨¡å‹åº”ç”¨äºè§£é‡Š3Dåœºæ™¯çš„åŠªåŠ›ã€‚ç„¶è€Œï¼Œç¼ºä¹å¤§è§„æ¨¡3Dè§†è§‰è¯­è¨€æ•°æ®é›†æˆä¸ºäº†å·¨å¤§çš„éšœç¢ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œå…¸å‹çš„æ–¹æ³•æ˜¯é€šè¿‡è®¾è®¡3Dè¾“å…¥çº§åœºæ™¯è¡¨ç¤ºï¼Œå°†3Dæ„è¯†æ³¨å…¥åˆ°2D LMMä¸­ã€‚è¿™é¡¹å·¥ä½œæä¾›äº†ä¸€ä¸ªæ–°çš„è§†è§’ã€‚æˆ‘ä»¬å¼•å…¥äº†å…·æœ‰3Dæ„è¯†çš„é‡å»ºè§†è§‰æŒ‡ä»¤è°ƒæ•´ï¼ˆRoss3Dï¼‰ï¼Œå®ƒå°†3Dæ„è¯†è§†è§‰ç›‘ç£é›†æˆåˆ°è®­ç»ƒè¿‡ç¨‹ä¸­ã€‚å…·ä½“æ¥è¯´ï¼Œå®ƒç»“åˆäº†è·¨è§†å›¾å’Œå…¨å±€è§†å›¾é‡å»ºã€‚å‰è€…éœ€è¦é€šè¿‡èšåˆå…¶ä»–è§†å›¾çš„é‡å ä¿¡æ¯æ¥é‡å»ºè¢«é®æŒ¡çš„è§†å›¾ã€‚åè€…æ—¨åœ¨ä»æ‰€æœ‰å¯ç”¨çš„è§†å›¾ä¸­èšåˆä¿¡æ¯ï¼Œä»¥æ¢å¤é¸Ÿç°å›¾åƒï¼Œä¸ºæ•´ä¸ªåœºæ™¯æä¾›å…¨é¢çš„æ¦‚è¿°ã€‚ç»éªŒä¸Šï¼ŒRoss3Dåœ¨å„ç§3Dåœºæ™¯ç†è§£åŸºå‡†æµ‹è¯•ä¸­è¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚æ›´é‡è¦çš„æ˜¯ï¼Œæˆ‘ä»¬çš„åŠç›‘ç£å®éªŒè¡¨æ˜ï¼Œåˆ©ç”¨å¤§é‡æœªæ ‡è®°çš„3Dè§†è§‰æ•°æ®å…·æœ‰å·¨å¤§çš„æ½œåŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.01901v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤§å‹å¤šæ¨¡æ€æ¨¡å‹ï¼ˆLMMsï¼‰åœ¨è§£è¯»äºŒç»´å›¾åƒå’Œè§†é¢‘æ–¹é¢å‘å±•è¿…é€Ÿï¼Œä½†åœ¨è§£è¯»ä¸‰ç»´åœºæ™¯æ—¶ä»é¢ä¸´æŒ‘æˆ˜ã€‚ç¼ºä¹å¤§è§„æ¨¡çš„ä¸‰ç»´è§†è§‰è¯­è¨€æ•°æ®é›†æ˜¯ä¸»è¦éšœç¢ä¹‹ä¸€ã€‚æœ¬æ–‡æå‡ºä¸€ç§æ–°å‹æ–¹æ³•Ross3Dï¼Œé€šè¿‡å°†ä¸‰ç»´æ„ŸçŸ¥è§†è§‰ç›‘ç£èå…¥è®­ç»ƒè¿‡ç¨‹ï¼Œå°†ä¸‰ç»´æ„è¯†æ³¨å…¥äºŒç»´LMMsã€‚å®ƒåŒ…å«è·¨è§†å›¾é‡å»ºå’Œå…¨å±€è§†å›¾é‡å»ºï¼Œåˆ†åˆ«é€šè¿‡èšåˆå…¶ä»–è§†å›¾çš„é‡å ä¿¡æ¯å’Œæ‰€æœ‰å¯ç”¨è§†å›¾çš„ä¿¡æ¯æ¥é‡å»ºé®æŒ¡è§†å›¾å’Œè·å–å¯¹æ•´ä¸ªåœºæ™¯çš„å…¨é¢æ¦‚è¿°ã€‚å®è¯ç»“æœæ˜¾ç¤ºï¼ŒRoss3Dåœ¨å¤šç§ä¸‰ç»´åœºæ™¯ç†è§£åŸºå‡†æµ‹è¯•ä¸­è¾¾åˆ°æœ€ä½³æ€§èƒ½ï¼Œå¹¶ä¸”åœ¨åˆ©ç”¨å¤§é‡æœªæ ‡è®°çš„ä¸‰ç»´è§†è§‰æ•°æ®æ–¹é¢å±•ç°å‡ºå·¨å¤§æ½œåŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹å¤šæ¨¡æ€æ¨¡å‹ï¼ˆLMMsï¼‰åœ¨è§£è¯»ä¸‰ç»´åœºæ™¯æ—¶é¢ä¸´æŒ‘æˆ˜ã€‚</li>
<li>ç¼ºä¹å¤§è§„æ¨¡ä¸‰ç»´è§†è§‰è¯­è¨€æ•°æ®é›†æ˜¯ä¸»è¦éšœç¢ä¹‹ä¸€ã€‚</li>
<li>Ross3Dæ–¹æ³•é€šè¿‡å°†ä¸‰ç»´æ„ŸçŸ¥è§†è§‰ç›‘ç£èå…¥è®­ç»ƒè¿‡ç¨‹æ¥è§£å†³è¿™ä¸€é—®é¢˜ã€‚</li>
<li>Ross3DåŒ…å«è·¨è§†å›¾é‡å»ºå’Œå…¨å±€è§†å›¾é‡å»ºã€‚</li>
<li>è·¨è§†å›¾é‡å»ºé€šè¿‡èšåˆå…¶ä»–è§†å›¾çš„ä¿¡æ¯æ¥é‡å»ºé®æŒ¡è§†å›¾ã€‚</li>
<li>å…¨å±€è§†å›¾é‡å»ºæ—¨åœ¨è·å–å¯¹æ•´ä¸ªåœºæ™¯çš„å…¨é¢æ¦‚è¿°ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.01901">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-02cdd63d51c21c4ee60b1aa2fa23109f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-47f832d9af9584b479dced5d24c96c31.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-6653a1d0ced561d38145f3ddb3eb57bb.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-80ca58f098bce993744b0042eee99101.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="TransientTables-Evaluating-LLMsâ€™-Reasoning-on-Temporally-Evolving-Semi-structured-Tables"><a href="#TransientTables-Evaluating-LLMsâ€™-Reasoning-on-Temporally-Evolving-Semi-structured-Tables" class="headerlink" title="TransientTables: Evaluating LLMsâ€™ Reasoning on Temporally Evolving   Semi-structured Tables"></a>TransientTables: Evaluating LLMsâ€™ Reasoning on Temporally Evolving   Semi-structured Tables</h2><p><strong>Authors:Abhilash Shankarampeta, Harsh Mahajan, Tushar Kataria, Dan Roth, Vivek Gupta</strong></p>
<p>Humans continuously make new discoveries, and understanding temporal sequence of events leading to these breakthroughs is essential for advancing science and society. This ability to reason over time allows us to identify future steps and understand the effects of financial and political decisions on our lives. However, large language models (LLMs) are typically trained on static datasets, limiting their ability to perform effective temporal reasoning. To assess the temporal reasoning capabilities of LLMs, we present the TRANSIENTTABLES dataset, which comprises 3,971 questions derived from over 14,000 tables, spanning 1,238 entities across multiple time periods. We introduce a template-based question-generation pipeline that harnesses LLMs to refine both templates and questions. Additionally, we establish baseline results using state-of-the-art LLMs to create a benchmark. We also introduce novel modeling strategies centered around task decomposition, enhancing LLM performance. </p>
<blockquote>
<p>äººç±»ä¸æ–­æœ‰æ–°çš„å‘ç°ï¼Œäº†è§£å¯¼è‡´è¿™äº›çªç ´çš„äº‹ä»¶çš„æ—¶é—´é¡ºåºå¯¹äºæ¨åŠ¨ç§‘å­¦å’Œç¤¾ä¼šè¿›æ­¥è‡³å…³é‡è¦ã€‚è¿™ç§éšæ—¶é—´æ¨ç†çš„èƒ½åŠ›ä½¿æˆ‘ä»¬èƒ½å¤Ÿç¡®å®šæœªæ¥çš„æ­¥éª¤ï¼Œå¹¶äº†è§£é‡‘èå’Œæ”¿æ²»å†³ç­–å¯¹æˆ‘ä»¬çš„ç”Ÿæ´»çš„å½±å“ã€‚ç„¶è€Œï¼Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰é€šå¸¸æ˜¯åœ¨é™æ€æ•°æ®é›†ä¸Šè¿›è¡Œè®­ç»ƒçš„ï¼Œè¿™é™åˆ¶äº†å®ƒä»¬è¿›è¡Œæœ‰æ•ˆçš„æ—¶é—´æ¨ç†çš„èƒ½åŠ›ã€‚ä¸ºäº†è¯„ä¼°LLMçš„æ—¶é—´æ¨ç†èƒ½åŠ›ï¼Œæˆ‘ä»¬æ¨å‡ºäº†TRANSIENTTABLESæ•°æ®é›†ï¼Œè¯¥æ•°æ®é›†åŒ…å«3971ä¸ªé—®é¢˜ï¼Œè¿™äº›é—®é¢˜æºäºè¶…è¿‡14000ä¸ªè¡¨æ ¼ï¼Œæ¶‰åŠå¤šä¸ªæ—¶é—´æ®µçš„1238ä¸ªå®ä½“ã€‚æˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ªåŸºäºæ¨¡æ¿çš„é—®é¢˜ç”Ÿæˆç®¡é“ï¼Œåˆ©ç”¨LLMæ¥å®Œå–„æ¨¡æ¿å’Œé—®é¢˜ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜ä½¿ç”¨æœ€å…ˆè¿›çš„LLMå»ºç«‹åŸºçº¿ç»“æœï¼Œä»¥å»ºç«‹åŸºå‡†æµ‹è¯•ï¼Œå¹¶å›´ç»•ä»»åŠ¡åˆ†è§£å¼•å…¥æ–°å‹å»ºæ¨¡ç­–ç•¥ï¼Œä»¥æé«˜LLMçš„æ€§èƒ½ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.01879v1">PDF</a> 19 Pages. 21 Tables, 1 figure</p>
<p><strong>Summary</strong>ï¼šäººç±»ä¸æ–­æœ‰æ–°çš„å‘ç°ï¼Œäº†è§£è¿™äº›çªç ´äº‹ä»¶çš„æ—¶åºå¯¹äºæ¨è¿›ç§‘å­¦å’Œç¤¾ä¼šè‡³å…³é‡è¦ã€‚å¯¹æ—¶é—´çš„æ¨ç†èƒ½åŠ›æœ‰åŠ©äºæˆ‘ä»¬é¢„æµ‹æœªæ¥å¹¶é‡‡å–å½±å“ç”Ÿæ´»å’Œç¤¾ä¼šçš„å†³ç­–ã€‚ç„¶è€Œï¼Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰é€šå¸¸åœ¨é™æ€æ•°æ®é›†ä¸Šè®­ç»ƒï¼Œæ— æ³•æœ‰æ•ˆåœ°è¿›è¡Œæ—¶é—´æ¨ç†ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬å¼•å…¥äº†TRANSIENTTABLESæ•°æ®é›†ï¼Œå…¶ä¸­åŒ…å«ä»è¶…è¿‡ä¸€ä¸‡å››åƒå¼ è¡¨æ ¼è¡ç”Ÿå‡ºçš„ä¸‰åƒä¹ç™¾ä¸ƒåä¸€ä¸ªé—®é¢˜ï¼Œæ¶µç›–äº†è·¨è¶Šå¤šä¸ªæ—¶é—´æ®µçš„å®ä½“ã€‚æˆ‘ä»¬è¿˜ä»‹ç»äº†ä¸€ç§åŸºäºæ¨¡æ¿çš„é—®é¢˜ç”Ÿæˆç®¡é“ï¼Œå¹¶åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹å¯¹æ¨¡æ¿å’Œé—®é¢˜è¿›è¡Œäº†æ”¹è¿›ã€‚åŒæ—¶ï¼Œæˆ‘ä»¬å»ºç«‹äº†ä½¿ç”¨æœ€æ–°å¤§å‹è¯­è¨€æ¨¡å‹çš„åŸºå‡†ç»“æœï¼Œå¹¶æå‡ºäº†ä»¥ä»»åŠ¡åˆ†è§£ä¸ºä¸­å¿ƒçš„æ–°å‹å»ºæ¨¡ç­–ç•¥æ¥æé«˜å¤§å‹è¯­è¨€æ¨¡å‹çš„æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨æ—¶åºæ¨ç†æ–¹é¢å­˜åœ¨å±€é™æ€§ã€‚</li>
<li>TRANSIENTTABLESæ•°æ®é›†åŒ…å«ä»å¤§é‡è¡¨æ ¼è¡ç”Ÿçš„é—®é¢˜ï¼Œç”¨äºè¯„ä¼°LLMçš„æ—¶åºæ¨ç†èƒ½åŠ›ã€‚</li>
<li>è¯¥æ•°æ®é›†æ¶µç›–äº†å¤šä¸ªæ—¶é—´æ®µçš„å®ä½“ï¼Œå¯¹LLMæå‡ºäº†æŒ‘æˆ˜ã€‚</li>
<li>å¼•å…¥äº†ä¸€ç§åŸºäºæ¨¡æ¿çš„é—®é¢˜ç”Ÿæˆç®¡é“ï¼Œæ”¹è¿›äº†å¤§å‹è¯­è¨€æ¨¡å‹çš„æ€§èƒ½ã€‚</li>
<li>ä½¿ç”¨æœ€æ–°çš„å¤§å‹è¯­è¨€æ¨¡å‹å»ºç«‹äº†åŸºå‡†ç»“æœã€‚</li>
<li>æå‡ºäº†æ–°å‹å»ºæ¨¡ç­–ç•¥ï¼Œå¦‚ä»»åŠ¡åˆ†è§£ï¼Œä»¥æé«˜LLMçš„æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.01879">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-89af2b462028452ff5332f4254508e05.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-1a5bebf66d486a17704a2234c9c22dbc.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Cross-Lingual-Consistency-A-Novel-Inference-Framework-for-Advancing-Reasoning-in-Large-Language-Models"><a href="#Cross-Lingual-Consistency-A-Novel-Inference-Framework-for-Advancing-Reasoning-in-Large-Language-Models" class="headerlink" title="Cross-Lingual Consistency: A Novel Inference Framework for Advancing   Reasoning in Large Language Models"></a>Cross-Lingual Consistency: A Novel Inference Framework for Advancing   Reasoning in Large Language Models</h2><p><strong>Authors:Zhiwei Yu, Tuo Li, Changhong Wang, Hui Chen, Lang Zhou</strong></p>
<p>Chain-of-thought (CoT) has emerged as a critical mechanism for enhancing reasoning capabilities in large language models (LLMs), with self-consistency demonstrating notable promise in boosting performance. However, inherent linguistic biases in multilingual training corpora frequently cause semantic drift and logical inconsistencies, especially in sub-10B parameter LLMs handling complex inference tasks. To overcome these constraints, we propose the Cross-Lingual Consistency (CLC) framework, an innovative inference paradigm that integrates multilingual reasoning paths through majority voting to elevate LLMsâ€™ reasoning capabilities. Empirical evaluations on the CMATH dataset reveal CLCâ€™s superiority over the conventional self-consistency method, delivering 9.5%, 6.5%, and 6.0% absolute accuracy gains for DeepSeek-Math-7B-Instruct, Qwen2.5-Math-7B-Instruct, and Gemma2-9B-Instruct respectively. Expanding CLCâ€™s linguistic scope to 11 diverse languages implies two synergistic benefits: 1) neutralizing linguistic biases in multilingual training corpora through multilingual ensemble voting, 2) escaping monolingual reasoning traps by exploring the broader multilingual solution space. This dual benefits empirically enables more globally optimal reasoning paths compared to monolingual self-consistency baselines, as evidenced by the 4.1%-18.5% accuracy gains using Gemma2-9B-Instruct on the MGSM dataset. </p>
<blockquote>
<p>æ€ç»´é“¾ï¼ˆCoTï¼‰ä½œä¸ºæå‡å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ¨ç†èƒ½åŠ›çš„é‡è¦æœºåˆ¶å·²ç»å´­éœ²å¤´è§’ï¼Œå…¶ä¸­è‡ªæˆ‘ä¸€è‡´æ€§åœ¨æå‡æ€§èƒ½ä¸Šæ˜¾ç¤ºå‡ºå·¨å¤§æ½œåŠ›ã€‚ç„¶è€Œï¼Œå¤šè¯­ç§è®­ç»ƒè¯­æ–™åº“ä¸­çš„å›ºæœ‰è¯­è¨€åè§ç»å¸¸å¯¼è‡´è¯­ä¹‰æ¼‚ç§»å’Œé€»è¾‘ä¸ä¸€è‡´ï¼Œç‰¹åˆ«æ˜¯åœ¨å¤„ç†å¤æ‚æ¨ç†ä»»åŠ¡çš„å‚æ•°å°äº10Bçš„LLMsä¸­æ›´ä¸ºæ˜æ˜¾ã€‚ä¸ºäº†å…‹æœè¿™äº›é™åˆ¶ï¼Œæˆ‘ä»¬æå‡ºäº†è·¨è¯­è¨€ä¸€è‡´æ€§ï¼ˆCLCï¼‰æ¡†æ¶ï¼Œè¿™æ˜¯ä¸€ç§åˆ›æ–°æ€§çš„æ¨ç†èŒƒå¼ï¼Œå®ƒé€šè¿‡å¤šæ•°æŠ•ç¥¨çš„æ–¹å¼æ•´åˆå¤šè¯­è¨€æ¨ç†è·¯å¾„ï¼Œä»¥æå‡LLMsçš„æ¨ç†èƒ½åŠ›ã€‚åœ¨CMATHæ•°æ®é›†ä¸Šçš„å®è¯è¯„ä¼°æ˜¾ç¤ºï¼ŒCLCç›¸è¾ƒäºä¼ ç»Ÿçš„è‡ªæˆ‘ä¸€è‡´æ€§æ–¹æ³•å…·æœ‰ä¼˜è¶Šæ€§ï¼Œåˆ†åˆ«ä¸ºDeepSeek-Math-7B-Instructã€Qwen2.5-Math-7B-Instructå’ŒGemma2-9B-Instructå¸¦æ¥äº†9.5%ã€6.5%å’Œ6.0%çš„ç»å¯¹ç²¾åº¦æå‡ã€‚å°†CLCçš„è¯­è¨€èŒƒå›´æ‰©å±•åˆ°11ç§ä¸åŒçš„è¯­è¨€ï¼Œå¸¦æ¥äº†ä¸¤ä¸ªååŒæ•ˆç›Šï¼š1ï¼‰é€šè¿‡å¤šè¯­è¨€é›†æˆæŠ•ç¥¨ä¸­å’Œå¤šè¯­ç§è®­ç»ƒè¯­æ–™åº“ä¸­çš„è¯­è¨€åè§ï¼›2ï¼‰é€šè¿‡æ¢ç´¢æ›´å¹¿æ³›çš„å¤šè¯­è¨€è§£å†³æ–¹æ¡ˆç©ºé—´ï¼Œé¿å…å•è¯­æ¨ç†é™·é˜±ã€‚è¿™ä¸¤ç§æ•ˆç›Šçš„ç»“åˆï¼Œä½¿å¾—ä¸å•è¯­è‡ªæˆ‘ä¸€è‡´æ€§åŸºå‡†ç›¸æ¯”ï¼Œèƒ½å¤Ÿæ‰¾åˆ°æ›´å¤šå…¨å±€æœ€ä¼˜çš„æ¨ç†è·¯å¾„ã€‚å¦‚åœ¨MGSMæ•°æ®é›†ä¸Šï¼Œä½¿ç”¨Gemma2-9B-Instructçš„ç²¾åº¦æå‡äº†4.1%~18.5%ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.01857v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>åŸºäºChain-of-thoughtï¼ˆCoTï¼‰æœºåˆ¶ï¼Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ¨ç†èƒ½åŠ›å¾—åˆ°äº†å¢å¼ºã€‚ç„¶è€Œï¼Œåœ¨å¤šè¯­è¨€è®­ç»ƒè¯­æ–™åº“ä¸­å­˜åœ¨çš„å›ºæœ‰è¯­è¨€åè§ä¼šå¯¼è‡´è¯­ä¹‰æ¼‚ç§»å’Œé€»è¾‘ä¸ä¸€è‡´ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œæå‡ºäº†Cross-Lingual Consistencyï¼ˆCLCï¼‰æ¡†æ¶ï¼Œé€šè¿‡å¤šæ•°æŠ•ç¥¨çš„æ–¹å¼æ•´åˆå¤šè¯­è¨€æ¨ç†è·¯å¾„ï¼Œæå‡LLMçš„æ¨ç†èƒ½åŠ›ã€‚åœ¨CMATHæ•°æ®é›†ä¸Šçš„å®è¯è¯„ä¼°è¡¨æ˜ï¼Œç›¸è¾ƒäºä¼ ç»Ÿçš„è‡ªä¸€è‡´æ€§æ–¹æ³•ï¼ŒCLCæ¡†æ¶åˆ†åˆ«å®ç°äº†DeepSeek-Math-7B-Instructã€Qwen2.5-Math-7B-Instructå’ŒGemma2-9B-Instructæ¨¡å‹ç»å¯¹ç²¾åº¦åˆ†åˆ«æå‡9.5%ã€6.5%å’Œ6.0%ã€‚æ­¤å¤–ï¼Œå°†CLCæ¡†æ¶çš„è¯­è¨€èŒƒå›´æ‰©å±•åˆ°11ç§è¯­è¨€å¯ä»¥ä¸­å’Œå¤šè¯­è¨€è®­ç»ƒè¯­æ–™åº“ä¸­çš„è¯­è¨€åè§ï¼ŒåŒæ—¶é€šè¿‡æ¢ç´¢æ›´å¹¿æ³›çš„å¤šè¯­è¨€è§£å†³æ–¹æ¡ˆç©ºé—´ï¼Œé¿å…äº†å•ä¸€è¯­è¨€çš„æ¨ç†é™·é˜±ã€‚è¿™æœ‰åŠ©äºæ‰¾åˆ°ç›¸æ¯”å•ä¸€è¯­è¨€è‡ªä¸€è‡´æ€§åŸºå‡†æµ‹è¯•æ›´ä¸ºå…¨å±€ä¼˜åŒ–çš„æ¨ç†è·¯å¾„ï¼Œå¦‚åœ¨MGSMæ•°æ®é›†ä¸Šï¼ŒGemma2-9B-Instructæ¨¡å‹çš„ç²¾åº¦æé«˜äº†4.1%~18.5%ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>Chain-of-thought (CoT)å¢å¼ºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ¨ç†èƒ½åŠ›ã€‚</li>
<li>å¤šè¯­è¨€è®­ç»ƒè¯­æ–™åº“ä¸­çš„è¯­è¨€åè§ä¼šå¯¼è‡´è¯­ä¹‰æ¼‚ç§»å’Œé€»è¾‘ä¸ä¸€è‡´ã€‚</li>
<li>Cross-Lingual Consistency (CLC)æ¡†æ¶é€šè¿‡å¤šæ•°æŠ•ç¥¨æ•´åˆå¤šè¯­è¨€æ¨ç†è·¯å¾„ï¼Œæå‡LLMçš„æ¨ç†èƒ½åŠ›ã€‚</li>
<li>CLCæ¡†æ¶åœ¨CMATHæ•°æ®é›†ä¸Šçš„å®è¯è¯„ä¼°è¡¨ç°ä¼˜è¶Šï¼Œå¸¦æ¥æ˜æ˜¾çš„ç²¾åº¦æå‡ã€‚</li>
<li>CLCæ¡†æ¶æ‰©å±•åˆ°11ç§è¯­è¨€å¯ä»¥ä¸­å’Œè¯­è¨€åè§ï¼Œé¿å…å•ä¸€è¯­è¨€çš„æ¨ç†é™·é˜±ã€‚</li>
<li>å¤šè¯­è¨€è§£å†³æ–¹æ¡ˆç©ºé—´æ¢ç´¢æœ‰åŠ©äºæ‰¾åˆ°æ›´ä¸ºå…¨å±€ä¼˜åŒ–çš„æ¨ç†è·¯å¾„ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.01857">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-ae9bdf4b23091f2c7b4ca0b9c70d22c3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8b817bac65b02e0cf5debe8e6595637d.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="LARGE-Legal-Retrieval-Augmented-Generation-Evaluation-Tool"><a href="#LARGE-Legal-Retrieval-Augmented-Generation-Evaluation-Tool" class="headerlink" title="LARGE: Legal Retrieval Augmented Generation Evaluation Tool"></a>LARGE: Legal Retrieval Augmented Generation Evaluation Tool</h2><p><strong>Authors:Minhu Park, Hongseok Oh, Eunkyung Choi, Wonseok Hwang</strong></p>
<p>Recently, building retrieval-augmented generation (RAG) systems to enhance the capability of large language models (LLMs) has become a common practice. Especially in the legal domain, previous judicial decisions play a significant role under the doctrine of stare decisis which emphasizes the importance of making decisions based on (retrieved) prior documents. However, the overall performance of RAG system depends on many components: (1) retrieval corpora, (2) retrieval algorithms, (3) rerankers, (4) LLM backbones, and (5) evaluation metrics. Here we propose LRAGE, an open-source tool for holistic evaluation of RAG systems focusing on the legal domain. LRAGE provides GUI and CLI interfaces to facilitate seamless experiments and investigate how changes in the aforementioned five components affect the overall accuracy. We validated LRAGE using multilingual legal benches including Korean (KBL), English (LegalBench), and Chinese (LawBench) by demonstrating how the overall accuracy changes when varying the five components mentioned above. The source code is available at <a target="_blank" rel="noopener" href="https://github.com/hoorangyee/LRAGE">https://github.com/hoorangyee/LRAGE</a>. </p>
<blockquote>
<p>æœ€è¿‘ï¼Œä¸ºäº†å¢å¼ºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„èƒ½åŠ›ï¼Œæ„å»ºå¢å¼ºæ£€ç´¢ç”Ÿæˆï¼ˆRAGï¼‰ç³»ç»Ÿå·²æˆä¸ºä¸€ç§å¸¸è§åšæ³•ã€‚ç‰¹åˆ«æ˜¯åœ¨æ³•å¾‹é¢†åŸŸï¼Œæ ¹æ®éµå¾ªå…ˆä¾‹åŸåˆ™ï¼Œå…ˆå‰çš„å¸æ³•åˆ¤å†³åœ¨å†³ç­–ä¸­æ‰®æ¼”ç€é‡è¦è§’è‰²ï¼Œè¯¥åŸåˆ™å¼ºè°ƒåŸºäºï¼ˆæ£€ç´¢åˆ°çš„ï¼‰å…ˆå‰æ–‡æ¡£è¿›è¡Œå†³ç­–çš„é‡è¦æ€§ã€‚ç„¶è€Œï¼ŒRAGç³»ç»Ÿçš„æ•´ä½“æ€§èƒ½å–å†³äºå¤šä¸ªç»„ä»¶ï¼šï¼ˆ1ï¼‰æ£€ç´¢è¯­æ–™åº“ã€ï¼ˆ2ï¼‰æ£€ç´¢ç®—æ³•ã€ï¼ˆ3ï¼‰é‡æ–°æ’åºå™¨ã€ï¼ˆ4ï¼‰LLMä¸»å¹²å’Œï¼ˆ5ï¼‰è¯„ä¼°æŒ‡æ ‡ã€‚åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬æå‡ºäº†LRAGEï¼Œè¿™æ˜¯ä¸€ä¸ªä¸“æ³¨äºæ³•å¾‹é¢†åŸŸçš„RAGç³»ç»Ÿçš„æ•´ä½“è¯„ä¼°çš„å¼€æºå·¥å…·ã€‚LRAGEæä¾›äº†GUIå’ŒCLIæ¥å£ï¼Œä»¥è¿›è¡Œæ— ç¼å®éªŒå¹¶ç ”ç©¶ä¸Šè¿°äº”ä¸ªç»„ä»¶çš„å˜åŒ–å¦‚ä½•å½±å“æ€»ä½“å‡†ç¡®æ€§ã€‚æˆ‘ä»¬ä½¿ç”¨åŒ…æ‹¬éŸ©è¯­ï¼ˆKBLï¼‰ã€è‹±è¯­ï¼ˆLegalBenchï¼‰å’Œä¸­æ–‡ï¼ˆLawBenchï¼‰åœ¨å†…çš„å¤šè¯­è¨€æ³•å¾‹åŸºå‡†æµ‹è¯•éªŒè¯äº†LRAGEï¼Œé€šè¿‡å±•ç¤ºä¸Šè¿°äº”ä¸ªç»„ä»¶å˜åŒ–æ—¶æ€»ä½“å‡†ç¡®æ€§çš„å˜åŒ–æ¥éªŒè¯ã€‚æºä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/hoorangyee/LRAGE">https://github.com/hoorangyee/LRAGE</a>è·å¾—ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.01840v1">PDF</a> 12 pages</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†å¦‚ä½•æ„å»ºå¢å¼ºæ£€ç´¢ç”Ÿæˆï¼ˆRAGï¼‰ç³»ç»Ÿä»¥æå‡å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„èƒ½åŠ›ï¼Œç‰¹åˆ«æ˜¯åœ¨æ³•å¾‹é¢†åŸŸä¸­ï¼Œä»¥å¾€å¸æ³•å†³ç­–åœ¨éµå¾ªå…ˆä¾‹åŸåˆ™ä¸‹å…·æœ‰é‡è¦æ„ä¹‰ã€‚æ–‡ç« æå‡ºäº†LRAGEè¿™ä¸€å¼€æºå·¥å…·ï¼Œç”¨äºå…¨é¢è¯„ä¼°RAGç³»ç»Ÿï¼Œå¹¶å…³æ³¨æ³•å¾‹é¢†åŸŸã€‚LRAGEæä¾›GUIå’ŒCLIæ¥å£ï¼Œæ–¹ä¾¿è¿›è¡Œå®éªŒå¹¶ç ”ç©¶äº”ä¸ªå…³é”®ç»„ä»¶çš„å˜åŒ–å¯¹æ•´ä½“å‡†ç¡®ç‡çš„å½±å“ã€‚ç»è¿‡åŒ…æ‹¬éŸ©è¯­ï¼ˆKBLï¼‰ã€è‹±è¯­ï¼ˆLegalBenchï¼‰å’Œä¸­æ–‡ï¼ˆLawBenchï¼‰åœ¨å†…çš„å¤šè¯­ç§æ³•å¾‹åŸºå‡†æµ‹è¯•éªŒè¯ï¼Œå±•ç¤ºäº†LRAGEçš„æœ‰æ•ˆæ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>RAGç³»ç»Ÿé€šè¿‡å¢å¼ºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„èƒ½åŠ›ï¼Œåœ¨æ³•å¾‹é¢†åŸŸä¸­å°¤ä¸ºé‡è¦ï¼Œéµå¾ªå…ˆä¾‹åŸåˆ™ä½¿å¾—ä»¥å¾€å¸æ³•å†³ç­–å…·æœ‰å…³é”®å‚è€ƒä»·å€¼ã€‚</li>
<li>LRAGEæ˜¯ä¸€ä¸ªç”¨äºå…¨é¢è¯„ä¼°RAGç³»ç»Ÿçš„å¼€æºå·¥å…·ï¼Œæä¾›GUIå’ŒCLIæ¥å£ä»¥æ–¹ä¾¿å®éªŒå’Œæ€§èƒ½åˆ†æã€‚</li>
<li>LRAGEè¯„ä¼°é‡ç‚¹åŒ…æ‹¬äº”ä¸ªå…³é”®ç»„ä»¶ï¼šæ£€ç´¢è¯­æ–™åº“ã€æ£€ç´¢ç®—æ³•ã€æ’åºå™¨ã€LLMä¸»å¹²å’Œè¯„ä¼°æŒ‡æ ‡ã€‚</li>
<li>é€šè¿‡å¤šè¯­ç§æ³•å¾‹åŸºå‡†æµ‹è¯•éªŒè¯ï¼ŒLRAGEå±•ç¤ºäº†å…¶æœ‰æ•ˆæ€§å’Œå®ç”¨æ€§ã€‚</li>
<li>LRAGEèƒ½å±•ç¤ºä¸åŒç»„ä»¶å˜åŒ–å¯¹æ•´ä½“å‡†ç¡®ç‡çš„å½±å“ï¼Œä¸ºä¼˜åŒ–RAGç³»ç»Ÿæä¾›æœ‰åŠ›æ”¯æŒã€‚</li>
<li>è¯¥å·¥å…·æºä»£ç å·²å…¬å¼€ï¼Œæ–¹ä¾¿è¿›ä¸€æ­¥å¼€å‘å’Œæ”¹è¿›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.01840">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-542ac136d895ae0f32b5875f8f282612.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-43a323bd9fe17304d80782c925c25a49.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-173bf64026ff1d75fdfec6f50aecb04b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3747e55b40f40efb0afb6a456be918ba.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ec6d5ef03ca30b015b16e9527f598277.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="YourBench-Easy-Custom-Evaluation-Sets-for-Everyone"><a href="#YourBench-Easy-Custom-Evaluation-Sets-for-Everyone" class="headerlink" title="YourBench: Easy Custom Evaluation Sets for Everyone"></a>YourBench: Easy Custom Evaluation Sets for Everyone</h2><p><strong>Authors:Sumuk Shashidhar, ClÃ©mentine Fourrier, Alina Lozovskia, Thomas Wolf, Gokhan Tur, Dilek Hakkani-TÃ¼r</strong></p>
<p>Evaluating large language models (LLMs) effectively remains a critical bottleneck, as traditional static benchmarks suffer from saturation and contamination, while human evaluations are costly and slow. This hinders timely or domain-specific assessment, crucial for real-world applications. We introduce YourBench, a novel, open-source framework that addresses these limitations by enabling dynamic, automated generation of reliable, up-to-date, and domain-tailored benchmarks cheaply and without manual annotation, directly from user-provided documents. We demonstrate its efficacy by replicating 7 diverse MMLU subsets using minimal source text, achieving this for under 15 USD in total inference costs while perfectly preserving the relative model performance rankings (Spearman Rho &#x3D; 1) observed on the original benchmark. To ensure that YourBench generates data grounded in provided input instead of relying on posterior parametric knowledge in models, we also introduce Tempora-0325, a novel dataset of over 7K diverse documents, published exclusively after March 2025. Our comprehensive analysis spans 26 SoTA models from 7 major families across varying scales (3-671B parameters) to validate the quality of generated evaluations through rigorous algorithmic checks (e.g., citation grounding) and human assessments. We release the YourBench library, the Tempora-0325 dataset, 150k+ question answer pairs based on Tempora and all evaluation and inference traces to facilitate reproducible research and empower the community to generate bespoke benchmarks on demand, fostering more relevant and trustworthy LLM evaluation. </p>
<blockquote>
<p>è¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä»ç„¶æ˜¯ä¸€ä¸ªå…³é”®çš„ç“¶é¢ˆï¼Œå› ä¸ºä¼ ç»Ÿçš„é™æ€åŸºå‡†æµ‹è¯•é¥±å—é¥±å’Œå’Œæ±¡æŸ“ä¹‹è‹¦ï¼Œè€Œäººå·¥è¯„ä¼°åˆ™æˆæœ¬é«˜æ˜‚ä¸”é€Ÿåº¦æ…¢ã€‚è¿™é˜»ç¢äº†é’ˆå¯¹ç‰¹å®šæ—¶é—´æˆ–ç‰¹å®šé¢†åŸŸçš„è¯„ä¼°ï¼Œå¯¹äºç°å®ä¸–ç•Œçš„åº”ç”¨è‡³å…³é‡è¦ã€‚æˆ‘ä»¬æ¨å‡ºäº†YourBenchï¼Œè¿™æ˜¯ä¸€ä¸ªæ–°é¢–çš„å¼€æºæ¡†æ¶ï¼Œé€šè¿‡åŠ¨æ€è‡ªåŠ¨åŒ–ç”Ÿæˆå¯é ã€æœ€æ–°ä¸”é’ˆå¯¹ç‰¹å®šé¢†åŸŸçš„åŸºå‡†æµ‹è¯•ï¼Œä»¥ä½å»‰çš„æˆæœ¬è€Œæ— éœ€æ‰‹åŠ¨æ³¨é‡Šï¼Œç›´æ¥ä»ç”¨æˆ·æä¾›çš„æ–‡æ¡£ä¸­è¿›è¡Œè¯„ä¼°ï¼Œä»è€Œè§£å†³äº†è¿™äº›é™åˆ¶ã€‚æˆ‘ä»¬é€šè¿‡å¤åˆ¶ä½¿ç”¨æœ€å°‘æºæ–‡æœ¬çš„7ä¸ªå¤šæ ·åŒ–çš„MMLUå­é›†æ¥å±•ç¤ºå…¶æœ‰æ•ˆæ€§ï¼Œåœ¨æ€»å…±çš„æ¨ç†æˆæœ¬ä¸åˆ°15ç¾å…ƒçš„æƒ…å†µä¸‹ï¼Œå®Œç¾åœ°ä¿æŒäº†åŸå§‹åŸºå‡†æµ‹è¯•ä¸Šè§‚å¯Ÿåˆ°çš„ç›¸å¯¹æ¨¡å‹æ€§èƒ½æ’åï¼ˆæ–¯çš®å°”æ›¼ç­‰çº§Ï&#x3D;1ï¼‰ã€‚ä¸ºäº†ç¡®ä¿YourBenchç”Ÿæˆçš„æ•°æ®åŸºäºæä¾›çš„è¾“å…¥è€Œä¸æ˜¯ä¾èµ–æ¨¡å‹ä¸­çš„äº‹åå‚æ•°çŸ¥è¯†ï¼Œæˆ‘ä»¬è¿˜æ¨å‡ºäº†Tempora-0325æ•°æ®é›†ï¼Œè¿™æ˜¯ä¸€ç»„è¶…è¿‡7,000ä¸ªå¤šæ ·åŒ–çš„æ–‡æ¡£çš„æ–°æ•°æ®é›†ï¼Œä»…åœ¨2025å¹´3æœˆä¹‹åå‘å¸ƒã€‚æˆ‘ä»¬çš„ç»¼åˆåˆ†ææ¶µç›–äº†æ¥è‡ª7ä¸ªä¸»è¦å®¶æ—çš„26ä¸ªæœ€æ–°æ¨¡å‹ï¼Œè·¨è¶Šäº†ä¸åŒçš„è§„æ¨¡ï¼ˆ3äº¿è‡³671äº¿å‚æ•°ï¼‰ï¼Œä»¥éªŒè¯ç”Ÿæˆè¯„ä¼°çš„è´¨é‡é€šè¿‡ä¸¥æ ¼çš„ç®—æ³•æ£€æŸ¥ï¼ˆä¾‹å¦‚å¼•æ–‡å®šä½ï¼‰å’Œäººå·¥è¯„ä¼°ã€‚æˆ‘ä»¬å‘å¸ƒäº†YourBenchåº“ã€Tempora-0325æ•°æ®é›†ã€åŸºäºTemporaçš„è¶…è¿‡15ä¸‡ä¸ªé—®é¢˜ç­”æ¡ˆå¯¹ä»¥åŠæ‰€æœ‰è¯„ä¼°å’Œæ¨ç†ç—•è¿¹ï¼Œä»¥ä¿ƒè¿›å¯é‡å¤çš„ç ”ç©¶å¹¶èµ‹äºˆç¤¾åŒºæŒ‰éœ€ç”Ÿæˆå®šåˆ¶åŸºå‡†æµ‹è¯•çš„èƒ½åŠ›ï¼Œä»è€Œæ¨åŠ¨æ›´ç›¸å…³å’Œå¯é çš„å¤§å‹è¯­è¨€æ¨¡å‹è¯„ä¼°ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.01833v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†å¯¹ä¼ ç»Ÿè¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ–¹æ³•å­˜åœ¨çš„é—®é¢˜ï¼ŒåŒ…æ‹¬é¥±å’Œå’Œæ±¡æŸ“ç­‰é—®é¢˜ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œä½œè€…æå‡ºäº†ä¸€ç§æ–°å‹çš„å¼€æºæ¡†æ¶YourBenchï¼Œè¯¥æ¡†æ¶å¯ä»¥åŠ¨æ€åœ°è‡ªåŠ¨ç”Ÿæˆå¯é ã€æœ€æ–°ã€é’ˆå¯¹ç‰¹å®šé¢†åŸŸçš„åŸºå‡†æµ‹è¯•ï¼Œå¹¶ä¸”å…·æœ‰æˆæœ¬ä½ã€æ— éœ€äººå·¥æ ‡æ³¨ç­‰ä¼˜ç‚¹ã€‚ä½œè€…é€šè¿‡å¤åˆ¶å¤šä¸ªMMLUå­é›†éªŒè¯äº†å…¶æœ‰æ•ˆæ€§ï¼ŒåŒæ—¶å¼•å…¥äº†ä¸€ä¸ªæ–°å‹æ•°æ®é›†Tempora-0325æ¥ç¡®ä¿ç”Ÿæˆçš„åŸºå‡†æµ‹è¯•å…·æœ‰å®é™…åº”ç”¨ä»·å€¼ã€‚æ­¤å¤–ï¼Œä½œè€…è¿˜å¯¹å¤šä¸ªæœ€æ–°çš„å¤§å‹è¯­è¨€æ¨¡å‹è¿›è¡Œäº†å…¨é¢çš„è¯„ä¼°ï¼Œå¹¶é€šè¿‡ä¸¥æ ¼çš„ç®—æ³•æ£€æŸ¥å’Œäººå·¥è¯„ä¼°éªŒè¯äº†ç”Ÿæˆè¯„ä¼°çš„è´¨é‡ã€‚æœ€åï¼Œä½œè€…é‡Šæ”¾äº†YourBenchåº“ã€Tempora-0325æ•°æ®é›†å’Œå…¶ä»–èµ„æºï¼Œä»¥ä¿ƒè¿›ç ”ç©¶äººå‘˜çš„å¯é‡å¤æ€§ç ”ç©¶å’Œç¤¾åŒºç”Ÿæˆå®šåˆ¶åŒ–çš„åŸºå‡†æµ‹è¯•éœ€æ±‚ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ä¼ ç»Ÿçš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è¯„ä¼°æ–¹æ³•å­˜åœ¨é¥±å’Œå’Œæ±¡æŸ“é—®é¢˜ã€‚</li>
<li>YourBenchæ˜¯ä¸€ä¸ªæ–°å‹çš„å¼€æºæ¡†æ¶ï¼Œå¯ä»¥åŠ¨æ€åœ°è‡ªåŠ¨ç”Ÿæˆå¯é ã€æœ€æ–°ã€é’ˆå¯¹ç‰¹å®šé¢†åŸŸçš„åŸºå‡†æµ‹è¯•ã€‚</li>
<li>YourBenchèƒ½å¤Ÿé™ä½æˆæœ¬å¹¶åŠ é€Ÿè¯„ä¼°è¿‡ç¨‹ï¼ŒåŒæ—¶æ— éœ€äººå·¥æ ‡æ³¨ã€‚</li>
<li>é€šè¿‡å¤åˆ¶MMLUå­é›†éªŒè¯äº†YourBenchçš„æœ‰æ•ˆæ€§ã€‚</li>
<li>Tempora-0325æ•°æ®é›†ç”¨äºç¡®ä¿ç”Ÿæˆçš„åŸºå‡†æµ‹è¯•å…·æœ‰å®é™…åº”ç”¨ä»·å€¼ã€‚</li>
<li>ä½œè€…å¯¹å¤šä¸ªæœ€æ–°çš„å¤§å‹è¯­è¨€æ¨¡å‹è¿›è¡Œäº†å…¨é¢çš„è¯„ä¼°ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.01833">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-7cfe6cc91d997a1052ed939bc8a54bca.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b4bbea2b75ec3d981ad5ea45b64b8005.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="Leveraging-Embedding-Techniques-in-Multimodal-Machine-Learning-for-Mental-Illness-Assessment"><a href="#Leveraging-Embedding-Techniques-in-Multimodal-Machine-Learning-for-Mental-Illness-Assessment" class="headerlink" title="Leveraging Embedding Techniques in Multimodal Machine Learning for   Mental Illness Assessment"></a>Leveraging Embedding Techniques in Multimodal Machine Learning for   Mental Illness Assessment</h2><p><strong>Authors:Abdelrahaman A. Hassan, Abdelrahman A. Ali, Aya E. Fouda, Radwa J. Hanafy, Mohammed E. Fouda</strong></p>
<p>The increasing global prevalence of mental disorders, such as depression and PTSD, requires objective and scalable diagnostic tools. Traditional clinical assessments often face limitations in accessibility, objectivity, and consistency. This paper investigates the potential of multimodal machine learning to address these challenges, leveraging the complementary information available in text, audio, and video data. Our approach involves a comprehensive analysis of various data preprocessing techniques, including novel chunking and utterance-based formatting strategies. We systematically evaluate a range of state-of-the-art embedding models for each modality and employ Convolutional Neural Networks (CNNs) and Bidirectional LSTM Networks (BiLSTMs) for feature extraction. We explore data-level, feature-level, and decision-level fusion techniques, including a novel integration of Large Language Model (LLM) predictions. We also investigate the impact of replacing Multilayer Perceptron classifiers with Support Vector Machines. We extend our analysis to severity prediction using PHQ-8 and PCL-C scores and multi-class classification (considering co-occurring conditions). Our results demonstrate that utterance-based chunking significantly improves performance, particularly for text and audio modalities. Decision-level fusion, incorporating LLM predictions, achieves the highest accuracy, with a balanced accuracy of 94.8% for depression and 96.2% for PTSD detection. The combination of CNN-BiLSTM architectures with utterance-level chunking, coupled with the integration of external LLM, provides a powerful and nuanced approach to the detection and assessment of mental health conditions. Our findings highlight the potential of MMML for developing more accurate, accessible, and personalized mental healthcare tools. </p>
<blockquote>
<p>éšç€æŠ‘éƒç—‡å’Œåˆ›ä¼¤ååº”æ¿€éšœç¢ç­‰ç²¾ç¥ç–¾ç—…çš„å…¨çƒå‘ç—…ç‡ä¸æ–­å¢åŠ ï¼Œæˆ‘ä»¬éœ€è¦å®¢è§‚ä¸”å¯è§„æ¨¡åŒ–çš„è¯Šæ–­å·¥å…·ã€‚ä¼ ç»Ÿçš„ä¸´åºŠè¯„ä¼°åœ¨å¯åŠæ€§ã€å®¢è§‚æ€§å’Œä¸€è‡´æ€§æ–¹é¢ç»å¸¸é¢ä¸´å±€é™æ€§ã€‚æœ¬æ–‡ç ”ç©¶äº†å¤šæ¨¡æ€æœºå™¨å­¦ä¹ åœ¨åº”å¯¹è¿™äº›æŒ‘æˆ˜æ–¹é¢çš„æ½œåŠ›ï¼Œåˆ©ç”¨æ–‡æœ¬ã€éŸ³é¢‘å’Œè§†é¢‘æ•°æ®ä¸­çš„äº’è¡¥ä¿¡æ¯ã€‚æˆ‘ä»¬çš„æ–¹æ³•æ¶‰åŠå¯¹å„ç§æ•°æ®é¢„å¤„ç†æŠ€æœ¯çš„ç»¼åˆåˆ†æï¼ŒåŒ…æ‹¬æ–°é¢–çš„åŸºäºåˆ†å—å’Œå‘è¨€çš„æ ¼å¼åŒ–ç­–ç•¥ã€‚æˆ‘ä»¬ç³»ç»Ÿåœ°è¯„ä¼°äº†æ¯ç§æ¨¡æ€çš„ä¸€ç³»åˆ—æœ€æ–°åµŒå…¥æ¨¡å‹ï¼Œå¹¶ä½¿ç”¨å·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNï¼‰å’ŒåŒå‘LSTMç½‘ç»œï¼ˆBiLSTMsï¼‰è¿›è¡Œç‰¹å¾æå–ã€‚æˆ‘ä»¬æ¢ç´¢äº†æ•°æ®å±‚ã€ç‰¹å¾å±‚å’Œå†³ç­–å±‚çš„èåˆæŠ€æœ¯ï¼ŒåŒ…æ‹¬å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰é¢„æµ‹çš„æ–°é¢–é›†æˆã€‚æˆ‘ä»¬è¿˜ç ”ç©¶äº†å°†å¤šå±‚æ„ŸçŸ¥å™¨åˆ†ç±»å™¨æ›¿æ¢ä¸ºæ”¯æŒå‘é‡æœºçš„å½±å“ã€‚æˆ‘ä»¬å°†åˆ†ææ‰©å±•åˆ°ä¸¥é‡æ€§é¢„æµ‹ï¼Œä½¿ç”¨PHQ-8å’ŒPCL-Cåˆ†æ•°å’Œå¤šç±»åˆ†ç±»ï¼ˆè€ƒè™‘å…±ç—…æƒ…å†µï¼‰ã€‚æˆ‘ä»¬çš„ç»“æœè¡¨æ˜ï¼ŒåŸºäºå‘è¨€çš„åˆ†å—æ˜¾è‘—æ”¹å–„æ€§èƒ½ï¼Œç‰¹åˆ«æ˜¯å¯¹æ–‡æœ¬å’ŒéŸ³é¢‘æ¨¡æ€è€Œè¨€ã€‚å†³ç­–çº§èåˆï¼Œç»“åˆLLMé¢„æµ‹ï¼Œè¾¾åˆ°äº†æœ€é«˜ç²¾åº¦ï¼ŒæŠ‘éƒç—‡å’Œåˆ›ä¼¤ååº”æ¿€éšœç¢æ£€æµ‹çš„å¹³è¡¡ç²¾åº¦åˆ†åˆ«ä¸º94.8%å’Œ96.2%ã€‚ç»“åˆCNN-BiLSTMæ¶æ„çš„å‘è¨€çº§åˆ†å—ï¼Œä»¥åŠLLMçš„é›†æˆï¼Œä¸ºç²¾ç¥å¥åº·æ¡ä»¶çš„æ£€æµ‹å’Œè¯„ä¼°æä¾›äº†ä¸€ç§å¼ºå¤§è€Œç»†è‡´çš„æ–¹æ³•ã€‚æˆ‘ä»¬çš„ç ”ç©¶ç»“æœè¡¨æ˜å¤šæ¨¡æ€æœºå™¨å­¦ä¹ åœ¨å¼€å‘æ›´å‡†ç¡®ã€æ›´å¯åŠã€æ›´ä¸ªæ€§åŒ–çš„ç²¾ç¥å«ç”Ÿå·¥å…·æ–¹é¢çš„æ½œåŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.01767v1">PDF</a> </p>
<p><strong>Summary</strong><br>    æœ¬æ–‡æ¢è®¨äº†å¤šæ¨¡æ€æœºå™¨å­¦ä¹ ï¼ˆMMMLï¼‰åœ¨ç²¾ç¥å¥åº·è¯Šæ–­ä¸­çš„åº”ç”¨æ½œåŠ›ï¼Œç‰¹åˆ«æ˜¯é’ˆå¯¹æŠ‘éƒç—‡å’ŒPTSDç­‰ç²¾ç¥ç–¾ç—…çš„è¯Šæ–­ã€‚ç ”ç©¶é€šè¿‡æ–‡æœ¬ã€éŸ³é¢‘å’Œè§†é¢‘æ•°æ®çš„å¤šæ¨¡æ€èåˆï¼Œé‡‡ç”¨å¤šç§æ•°æ®é¢„å¤„ç†æŠ€æœ¯ã€åµŒå…¥æ¨¡å‹ã€å·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNï¼‰ã€åŒå‘LSTMç½‘ç»œï¼ˆBiLSTMsï¼‰ç­‰ï¼Œè¿›è¡Œç³»ç»Ÿæ€§çš„å®éªŒè¯„ä¼°ã€‚å†³ç­–çº§èåˆç»“åˆLLMé¢„æµ‹ï¼Œå®ç°äº†é«˜å‡†ç¡®ç‡çš„ç²¾ç¥ç–¾ç—…æ£€æµ‹ä¸è¯„ä¼°ã€‚è¯¥ç ”ç©¶ä¸ºå¼€å‘æ›´å‡†ç¡®ã€å¯è®¿é—®å’Œä¸ªæ€§åŒ–çš„ç²¾ç¥å«ç”Ÿå·¥å…·æä¾›äº†æœ‰åŠ›æ”¯æŒã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å…¨çƒç²¾ç¥ç–¾ç—…çš„æ™®åŠç‡å¢é•¿ï¼Œéœ€è¦å®¢è§‚å’Œå¯è§„æ¨¡åŒ–çš„è¯Šæ–­å·¥å…·ã€‚</li>
<li>ä¼ ç»Ÿä¸´åºŠè¯„ä¼°åœ¨å¯åŠæ€§ã€å®¢è§‚æ€§å’Œä¸€è‡´æ€§æ–¹é¢å­˜åœ¨å±€é™æ€§ã€‚</li>
<li>å¤šæ¨¡æ€æœºå™¨å­¦ä¹ ï¼ˆMMMLï¼‰å…·æœ‰è§£å†³è¿™äº›æŒ‘æˆ˜çš„å·¨å¤§æ½œåŠ›ã€‚</li>
<li>æ–‡æœ¬ã€éŸ³é¢‘å’Œè§†é¢‘æ•°æ®çš„èåˆæœ‰åŠ©äºæé«˜è¯Šæ–­å‡†ç¡®æ€§ã€‚</li>
<li>utterance-based chunkingæŠ€æœ¯åœ¨æ–‡æœ¬å’ŒéŸ³é¢‘æ¨¡æ€ä¸Šè¡¨ç°å°¤ä¸ºå‡ºè‰²ã€‚</li>
<li>ç»“åˆCNN-BiLSTMæ¶æ„ä¸LLMçš„å†³ç­–çº§èåˆè¾¾åˆ°æœ€é«˜å‡†ç¡®ç‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.01767">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-de1284a8b579cffc56eefa6c95d2ea71.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="TransforMerger-Transformer-based-Voice-Gesture-Fusion-for-Robust-Human-Robot-Communication"><a href="#TransforMerger-Transformer-based-Voice-Gesture-Fusion-for-Robust-Human-Robot-Communication" class="headerlink" title="TransforMerger: Transformer-based Voice-Gesture Fusion for Robust   Human-Robot Communication"></a>TransforMerger: Transformer-based Voice-Gesture Fusion for Robust   Human-Robot Communication</h2><p><strong>Authors:Petr Vanc, Karla Stepanova</strong></p>
<p>As human-robot collaboration advances, natural and flexible communication methods are essential for effective robot control. Traditional methods relying on a single modality or rigid rules struggle with noisy or misaligned data as well as with object descriptions that do not perfectly fit the predefined object names (e.g. â€˜Pick that red objectâ€™). We introduce TransforMerger, a transformer-based reasoning model that infers a structured action command for robotic manipulation based on fused voice and gesture inputs. Our approach merges multimodal data into a single unified sentence, which is then processed by the language model. We employ probabilistic embeddings to handle uncertainty and we integrate contextual scene understanding to resolve ambiguous references (e.g., gestures pointing to multiple objects or vague verbal cues like â€œthisâ€). We evaluate TransforMerger in simulated and real-world experiments, demonstrating its robustness to noise, misalignment, and missing information. Our results show that TransforMerger outperforms deterministic baselines, especially in scenarios requiring more contextual knowledge, enabling more robust and flexible human-robot communication. Code and datasets are available at: <a target="_blank" rel="noopener" href="http://imitrob.ciirc.cvut.cz/publications/transformerger">http://imitrob.ciirc.cvut.cz/publications/transformerger</a>. </p>
<blockquote>
<p>éšç€äººæœºåä½œçš„ä¸æ–­å‘å±•ï¼Œè‡ªç„¶ä¸”çµæ´»çš„æ²Ÿé€šæ–¹å¼å¯¹äºå®ç°æœ‰æ•ˆçš„æœºå™¨äººæ§åˆ¶è‡³å…³é‡è¦ã€‚ä¾èµ–å•ä¸€æ¨¡å¼æˆ–åƒµåŒ–è§„åˆ™çš„ä¼ ç»Ÿæ–¹æ³•åœ¨å¤„ç†å˜ˆæ‚æˆ–é”™ä½çš„æ•°æ®ä»¥åŠä¸ç¬¦åˆé¢„å®šä¹‰å¯¹è±¡åç§°çš„å¯¹è±¡æè¿°ï¼ˆä¾‹å¦‚â€œæ‹¿èµ·é‚£ä¸ªçº¢è‰²çš„ç‰©ä½“â€ï¼‰æ—¶é‡åˆ°äº†å›°éš¾ã€‚æˆ‘ä»¬æ¨å‡ºäº†TransforMergerï¼Œè¿™æ˜¯ä¸€æ¬¾åŸºäºå˜å‹å™¨çš„æ¨ç†æ¨¡å‹ï¼Œå®ƒå¯ä»¥æ ¹æ®èåˆçš„è¯­éŸ³å’Œæ‰‹åŠ¿è¾“å…¥æ¨æ–­å‡ºç”¨äºæœºå™¨äººæ“ä½œçš„ç»“æ„åŒ–åŠ¨ä½œå‘½ä»¤ã€‚æˆ‘ä»¬çš„æ–¹æ³•å°†å¤šæ¨¡å¼æ•°æ®åˆå¹¶æˆå•ä¸ªç»Ÿä¸€çš„å¥å­ï¼Œç„¶åç”±è¯­è¨€æ¨¡å‹è¿›è¡Œå¤„ç†ã€‚æˆ‘ä»¬é‡‡ç”¨æ¦‚ç‡åµŒå…¥æ¥å¤„ç†ä¸ç¡®å®šæ€§ï¼Œå¹¶æ•´åˆåœºæ™¯ä¸Šä¸‹æ–‡ç†è§£æ¥è§£å†³æ¨¡ç³Šå¼•ç”¨ï¼ˆä¾‹å¦‚ï¼ŒæŒ‡å‘å¤šä¸ªå¯¹è±¡çš„æ‰‹åŠ¿æˆ–æ¨¡ç³Šçš„å£å¤´çº¿ç´¢ï¼Œå¦‚â€œè¿™ä¸ªâ€ï¼‰ã€‚æˆ‘ä»¬åœ¨æ¨¡æ‹Ÿå’ŒçœŸå®ä¸–ç•Œçš„å®éªŒä¸­è¯„ä¼°äº†TransforMergerï¼Œè¯æ˜äº†å®ƒå¯¹å™ªå£°ã€é”™ä½å’Œç¼ºå¤±ä¿¡æ¯çš„ç¨³å¥æ€§ã€‚æˆ‘ä»¬çš„ç»“æœè¡¨æ˜ï¼ŒTransforMergeråœ¨éœ€è¦æ›´å¤šä¸Šä¸‹æ–‡çŸ¥è¯†çš„åœºæ™¯ä¸­è¡¨ç°ä¼˜äºç¡®å®šæ€§åŸºçº¿ï¼Œèƒ½å¤Ÿå®ç°æ›´ç¨³å¥å’Œçµæ´»çš„äººæœºæ²Ÿé€šã€‚ç›¸å…³ä»£ç å’Œæ•°æ®é›†å¯é€šè¿‡ä»¥ä¸‹ç½‘å€è·å–ï¼š<a target="_blank" rel="noopener" href="http://imitrob.ciirc.cvut.cz/publications/transformerger">http://imitrob.ciirc.cvut.cz/publications/transformerger</a> ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.01708v1">PDF</a> 8 pages, 7 figures</p>
<p><strong>Summary</strong><br>åœ¨äººæœºåä½œæ—¥ç›Šå‘å±•çš„èƒŒæ™¯ä¸‹ï¼Œè‡ªç„¶çµæ´»çš„äº¤æµæ–¹å¼å¯¹æœºå™¨äººæ§åˆ¶è‡³å…³é‡è¦ã€‚ä¼ ç»Ÿæ–¹æ³•åœ¨å¤„ç†å™ªå£°æˆ–é”™ä½æ•°æ®ä»¥åŠå¯¹è±¡æè¿°ä¸é¢„è®¾åç§°ä¸åŒ¹é…çš„é—®é¢˜æ—¶æ˜¾å¾—æ‰è¥Ÿè§è‚˜ã€‚æˆ‘ä»¬æ¨å‡ºäº†åŸºäºTransformerçš„æ¨ç†æ¨¡å‹TransforMergerï¼Œé€šè¿‡èåˆè¯­éŸ³å’Œæ‰‹åŠ¿è¾“å…¥ï¼Œæ¥æ¨æ–­ç”¨äºæœºå™¨äººæ“æ§çš„ç»“æ„åŒ–åŠ¨ä½œæŒ‡ä»¤ã€‚è¯¥æ–¹æ³•å°†å¤šæ¨¡å¼æ•°æ®åˆå¹¶æˆç»Ÿä¸€è¯­å¥ï¼Œå¹¶ç”±è¯­è¨€æ¨¡å‹å¤„ç†ã€‚æˆ‘ä»¬é‡‡ç”¨æ¦‚ç‡åµŒå…¥æ¥å¤„ç†ä¸ç¡®å®šæ€§ï¼Œå¹¶æ•´åˆåœºæ™¯ä¸Šä¸‹æ–‡ç†è§£æ¥è§£å†³æ¨¡ç³Šå‚ç…§é—®é¢˜ã€‚åœ¨æ¨¡æ‹Ÿå’ŒçœŸå®ç¯å¢ƒä¸­ï¼ŒTransforMergerå±•ç°å‡ºå¯¹å™ªå£°ã€é”™ä½å’Œç¼ºå¤±ä¿¡æ¯çš„ç¨³å¥æ€§ï¼Œå¹¶åœ¨éœ€è¦æ›´å¤šä¸Šä¸‹æ–‡çŸ¥è¯†çš„åœºæ™¯ä¸­è¡¨ç°å‡ºè¶…è¶Šç¡®å®šæ€§åŸºå‡†çš„æ€§èƒ½ï¼Œä»è€Œå®ç°æ›´ç¨³å¥å’Œçµæ´»çš„äººæœºäº¤æµã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>éšç€äººæœºåä½œçš„è¿›æ­¥ï¼Œè‡ªç„¶çµæ´»çš„äº¤æµæ–¹å¼å¯¹äºæœºå™¨äººæ§åˆ¶è‡³å…³é‡è¦ã€‚</li>
<li>ä¼ ç»Ÿæ–¹æ³•åœ¨å¤„ç†å™ªå£°æˆ–é”™ä½æ•°æ®ä»¥åŠå¯¹è±¡æè¿°ä¸é¢„è®¾åç§°ä¸åŒ¹é…æ—¶å­˜åœ¨æŒ‘æˆ˜ã€‚</li>
<li>TransforMergeræ˜¯ä¸€ä¸ªåŸºäºTransformerçš„æ¨ç†æ¨¡å‹ï¼Œèƒ½èåˆè¯­éŸ³å’Œæ‰‹åŠ¿è¾“å…¥æ¥æ¨æ–­ç»“æ„åŒ–åŠ¨ä½œæŒ‡ä»¤ã€‚</li>
<li>TransforMergerå°†å¤šæ¨¡å¼æ•°æ®åˆå¹¶æˆç»Ÿä¸€è¯­å¥ï¼Œå¹¶ç”±è¯­è¨€æ¨¡å‹å¤„ç†ã€‚</li>
<li>è¯¥æ¨¡å‹é‡‡ç”¨æ¦‚ç‡åµŒå…¥å¤„ç†ä¸ç¡®å®šæ€§ï¼Œå¹¶æ•´åˆåœºæ™¯ä¸Šä¸‹æ–‡ç†è§£ä»¥è§£å†³æ¨¡ç³Šå‚ç…§é—®é¢˜ã€‚</li>
<li>TransforMergeråœ¨æ¨¡æ‹Ÿå’ŒçœŸå®ç¯å¢ƒä¸­å±•ç°å‡ºå¯¹å™ªå£°ã€é”™ä½å’Œç¼ºå¤±ä¿¡æ¯çš„ç¨³å¥æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.01708">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-7d9ba578142da10ab1bae8ed4f7cb36f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-67c87a7977d617801b4a2ad163fae0a6.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-f091bb220138be6d742d3c7d57ebd5e9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b1ea0a2e0fe7ccb62d3a1083b786ed38.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="Q-Adapt-Adapting-LMM-for-Visual-Quality-Assessment-with-Progressive-Instruction-Tuning"><a href="#Q-Adapt-Adapting-LMM-for-Visual-Quality-Assessment-with-Progressive-Instruction-Tuning" class="headerlink" title="Q-Adapt: Adapting LMM for Visual Quality Assessment with Progressive   Instruction Tuning"></a>Q-Adapt: Adapting LMM for Visual Quality Assessment with Progressive   Instruction Tuning</h2><p><strong>Authors:Yiting Lu, Xin Li, Haoning Wu, Bingchen Li, Weisi Lin, Zhibo Chen</strong></p>
<p>The rapid advancement of Large Multi-modal Foundation Models (LMM) has paved the way for the possible Explainable Image Quality Assessment (EIQA) with instruction tuning from two perspectives: overall quality explanation, and attribute-wise perception answering. However, existing works usually overlooked the conflicts between these two types of perception explanations during joint instruction tuning, leading to insufficient perception understanding. To mitigate this, we propose a new paradigm for perception-oriented instruction tuning, i.e., Q-Adapt, which aims to eliminate the conflicts and achieve the synergy between these two EIQA tasks when adapting LMM, resulting in enhanced multi-faceted explanations of IQA. Particularly, we propose a progressive instruction tuning strategy by dividing the adaption process of LMM for EIQA into two stages, where the first stage empowers the LMM with universal perception knowledge tailored for two tasks using an efficient transfer learning strategy, i.e., LoRA, and the second stage introduces the instruction-adaptive visual prompt tuning to dynamically adapt visual features for the different instructions from two tasks. In this way, our proposed Q-Adapt can achieve a lightweight visual quality evaluator, demonstrating comparable performance and, in some instances, superior results across perceptual-related benchmarks and commonly-used IQA databases. The source code is publicly available at <a target="_blank" rel="noopener" href="https://github.com/yeppp27/Q-Adapt">https://github.com/yeppp27/Q-Adapt</a>. </p>
<blockquote>
<p>å¤§å‹å¤šæ¨¡æ€åŸºç¡€æ¨¡å‹ï¼ˆLMMï¼‰çš„å¿«é€Ÿå‘å±•ä¸ºå¯èƒ½çš„å¯è§£é‡Šæ€§å›¾åƒè´¨é‡è¯„ä¼°ï¼ˆEIQAï¼‰é“ºå¹³äº†é“è·¯ï¼Œé€šè¿‡ä¸¤ä¸ªæ–¹é¢çš„æŒ‡ä»¤è°ƒæ•´æ¥å®ç°æ•´ä½“è´¨é‡è§£é‡Šå’Œå±æ€§æ„ŸçŸ¥å›ç­”ã€‚ç„¶è€Œï¼Œç°æœ‰å·¥ä½œé€šå¸¸åœ¨è”åˆæŒ‡ä»¤è°ƒæ•´æ—¶å¿½ç•¥äº†è¿™ä¸¤ç§æ„ŸçŸ¥è§£é‡Šä¹‹é—´çš„å†²çªï¼Œå¯¼è‡´æ„ŸçŸ¥ç†è§£ä¸è¶³ã€‚ä¸ºäº†ç¼“è§£è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§é¢å‘æ„ŸçŸ¥çš„æŒ‡ä»¤è°ƒæ•´æ–°èŒƒå¼ï¼Œå³Q-Adaptã€‚å…¶ç›®çš„æ˜¯åœ¨é€‚åº”LMMæ—¶æ¶ˆé™¤è¿™ä¸¤ç§EIQAä»»åŠ¡ä¹‹é—´çš„å†²çªï¼Œå®ç°ååŒä½œç”¨ï¼Œä»è€Œå¢å¼ºIQAçš„å¤šæ–¹é¢è§£é‡Šã€‚ç‰¹åˆ«æ˜¯ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ¸è¿›çš„æŒ‡ä»¤è°ƒæ•´ç­–ç•¥ï¼Œå°†LMMå¯¹EIQAçš„é€‚åº”è¿‡ç¨‹åˆ†ä¸ºä¸¤ä¸ªé˜¶æ®µã€‚åœ¨ç¬¬ä¸€é˜¶æ®µï¼Œæˆ‘ä»¬ä½¿ç”¨é«˜æ•ˆçš„è¿ç§»å­¦ä¹ ç­–ç•¥LoRAï¼Œèµ‹äºˆLMMé’ˆå¯¹è¿™ä¸¤ä¸ªä»»åŠ¡çš„é€šç”¨æ„ŸçŸ¥çŸ¥è¯†ï¼›åœ¨ç¬¬äºŒé˜¶æ®µï¼Œæˆ‘ä»¬å¼•å…¥æŒ‡ä»¤é€‚åº”æ€§è§†è§‰æç¤ºè°ƒæ•´ï¼Œä»¥æ ¹æ®ä¸åŒä»»åŠ¡çš„æŒ‡ä»¤åŠ¨æ€é€‚åº”è§†è§‰ç‰¹å¾ã€‚é€šè¿‡è¿™ç§æ–¹å¼ï¼Œæˆ‘ä»¬æå‡ºçš„Q-Adaptå¯ä»¥å®ç°è½»é‡çº§è§†è§‰è´¨é‡è¯„ä¼°å™¨ï¼Œåœ¨æ„ŸçŸ¥ç›¸å…³åŸºå‡†æµ‹è¯•å’Œå¸¸ç”¨çš„IQAæ•°æ®åº“ä¸Šè¡¨ç°å‡ºç›¸å½“çš„æ€§èƒ½ï¼Œå¹¶åœ¨æŸäº›æƒ…å†µä¸‹è¡¨ç°æ›´ä¼˜ç§€ã€‚æºä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/yeppp27/Q-Adapt%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/yeppp27/Q-Adaptæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.01655v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤§å‹å¤šæ¨¡æ€åŸºç¡€æ¨¡å‹çš„å¿«é€Ÿå‘å±•ï¼Œä¸ºåŸºäºæŒ‡ä»¤è°ƒå‚çš„å¯è§£é‡Šå›¾åƒè´¨é‡è¯„ä¼°ï¼ˆEIQAï¼‰æä¾›äº†å¯èƒ½ã€‚ç°æœ‰æ–¹æ³•åœ¨è”åˆæŒ‡ä»¤è°ƒå‚æ—¶å¿½è§†äº†æ•´ä½“è´¨é‡ä¸å±æ€§æ„ŸçŸ¥è§£é‡Šä¹‹é—´çš„å†²çªï¼Œå¯¼è‡´æ„ŸçŸ¥ç†è§£ä¸è¶³ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†é¢å‘æ„ŸçŸ¥çš„æŒ‡ä»¤è°ƒå‚æ–°æ¨¡å¼â€”â€”Q-Adaptï¼Œæ—¨åœ¨æ¶ˆé™¤è¿™ä¸¤ç§EIQAä»»åŠ¡é—´çš„å†²çªï¼Œå®ç°ååŒé€‚åº”LMMï¼Œå¢å¼ºå›¾åƒè´¨é‡è¯„ä¼°çš„å¤šæ–¹é¢è§£é‡Šã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹å¤šæ¨¡æ€åŸºç¡€æ¨¡å‹çš„è¿›æ­¥ä¸ºEIQAçš„æŒ‡ä»¤è°ƒå‚æä¾›äº†æœºä¼šã€‚</li>
<li>ç°æœ‰æ–¹æ³•åœ¨è”åˆæŒ‡ä»¤è°ƒå‚æ—¶å¿½è§†æ•´ä½“è´¨é‡ä¸å±æ€§æ„ŸçŸ¥è§£é‡Šä¹‹é—´çš„å†²çªã€‚</li>
<li>Q-Adaptæ—¨åœ¨æ¶ˆé™¤è¿™ç§å†²çªï¼Œå®ç°ä¸¤ç§EIQAä»»åŠ¡çš„ååŒé€‚åº”ã€‚</li>
<li>Q-Adapté‡‡ç”¨åˆ†é˜¶æ®µæŒ‡ä»¤è°ƒå‚ç­–ç•¥ï¼Œç¬¬ä¸€é˜¶æ®µèµ‹äºˆLMMé’ˆå¯¹ä¸¤ä¸ªä»»åŠ¡é€šç”¨æ„ŸçŸ¥çŸ¥è¯†ï¼Œç¬¬äºŒé˜¶æ®µæ ¹æ®ä¸åŒæŒ‡ä»¤åŠ¨æ€è°ƒæ•´è§†è§‰ç‰¹å¾ã€‚</li>
<li>Q-Adaptèƒ½å®ç°è½»é‡çº§è§†è§‰è´¨é‡è¯„ä¼°ï¼Œåœ¨æ„ŸçŸ¥ç›¸å…³åŸºå‡†æµ‹è¯•å’Œå¸¸ç”¨IQAæ•°æ®åº“ä¸Šè¡¨ç°ä¼˜å¼‚ã€‚</li>
<li>Q-Adaptçš„æºä»£ç å·²å…¬å¼€åœ¨<a target="_blank" rel="noopener" href="https://github.com/yeppp27/Q-Adapt%E3%80%82">https://github.com/yeppp27/Q-Adaptã€‚</a></li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.01655">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-e673eac368ee8357e01903864dfaf4db.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0f187acbe4cc287850a959b55d3d6ccf.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-91681af32fe8059583475d81ab36a862.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6e8c91388a63167f835f4420666613dd.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="COST-Contrastive-One-Stage-Transformer-for-Vision-Language-Small-Object-Tracking"><a href="#COST-Contrastive-One-Stage-Transformer-for-Vision-Language-Small-Object-Tracking" class="headerlink" title="COST: Contrastive One-Stage Transformer for Vision-Language Small Object   Tracking"></a>COST: Contrastive One-Stage Transformer for Vision-Language Small Object   Tracking</h2><p><strong>Authors:Chunhui Zhang, Li Liu, Jialin Gao, Xin Sun, Hao Wen, Xi Zhou, Shiming Ge, Yanfeng Wang</strong></p>
<p>Transformer has recently demonstrated great potential in improving vision-language (VL) tracking algorithms. However, most of the existing VL trackers rely on carefully designed mechanisms to perform the multi-stage multi-modal fusion. Additionally, direct multi-modal fusion without alignment ignores distribution discrepancy between modalities in feature space, potentially leading to suboptimal representations. In this work, we propose COST, a contrastive one-stage transformer fusion framework for VL tracking, aiming to learn semantically consistent and unified VL representations. Specifically, we introduce a contrastive alignment strategy that maximizes mutual information (MI) between a video and its corresponding language description. This enables effective cross-modal alignment, yielding semantically consistent features in the representation space. By leveraging a visual-linguistic transformer, we establish an efficient multi-modal fusion and reasoning mechanism, empirically demonstrating that a simple stack of transformer encoders effectively enables unified VL representations. Moreover, we contribute a newly collected VL tracking benchmark dataset for small object tracking, named VL-SOT500, with bounding boxes and language descriptions. Our dataset comprises two challenging subsets, VL-SOT230 and VL-SOT270, dedicated to evaluating generic and high-speed small object tracking, respectively. Small object tracking is notoriously challenging due to weak appearance and limited features, and this dataset is, to the best of our knowledge, the first to explore the usage of language cues to enhance visual representation for small object tracking. Extensive experiments demonstrate that COST achieves state-of-the-art performance on five existing VL tracking datasets, as well as on our proposed VL-SOT500 dataset. Source codes and dataset will be made publicly available. </p>
<blockquote>
<p>Transformeråœ¨æ”¹è¿›è§†è§‰è¯­è¨€ï¼ˆVLï¼‰è·Ÿè¸ªç®—æ³•æ–¹é¢è¡¨ç°å‡ºäº†å·¨å¤§çš„æ½œåŠ›ã€‚ç„¶è€Œï¼Œå¤§å¤šæ•°ç°æœ‰çš„VLè·Ÿè¸ªå™¨éƒ½ä¾èµ–äºç²¾å¿ƒè®¾è®¡çš„æœºåˆ¶æ¥æ‰§è¡Œå¤šé˜¶æ®µå¤šæ¨¡å¼èåˆã€‚å¦å¤–ï¼Œæ²¡æœ‰å¯¹é½çš„ç›´æ¥å¤šæ¨¡å¼èåˆå¿½ç•¥äº†ä¸åŒæ¨¡å¼ä¹‹é—´ç‰¹å¾ç©ºé—´çš„åˆ†å¸ƒå·®å¼‚ï¼Œå¯èƒ½å¯¼è‡´è¡¨ç¤ºä¸ä½³ã€‚åœ¨æœ¬ç ”ç©¶ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†é’ˆå¯¹VLè·Ÿè¸ªçš„å¯¹æ¯”ä¸€é˜¶Transformerèåˆæ¡†æ¶COSTï¼Œæ—¨åœ¨å­¦ä¹ è¯­ä¹‰ä¸€è‡´ä¸”ç»Ÿä¸€çš„VLè¡¨ç¤ºã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§å¯¹æ¯”å¯¹é½ç­–ç•¥ï¼Œä»¥æœ€å¤§åŒ–è§†é¢‘ä¸å…¶ç›¸åº”çš„è¯­è¨€æè¿°ä¹‹é—´çš„äº’ä¿¡æ¯ï¼ˆMIï¼‰ã€‚è¿™å®ç°äº†æœ‰æ•ˆçš„è·¨æ¨¡å¼å¯¹é½ï¼Œåœ¨è¡¨ç¤ºç©ºé—´ä¸­äº§ç”Ÿè¯­ä¹‰ä¸€è‡´çš„ç‰¹å¾ã€‚é€šè¿‡åˆ©ç”¨è§†è§‰è¯­è¨€Transformerï¼Œæˆ‘ä»¬å»ºç«‹äº†é«˜æ•ˆçš„å¤šæ¨¡å¼èåˆå’Œæ¨ç†æœºåˆ¶ï¼Œå®è¯è¡¨æ˜ï¼Œç®€å•çš„Transformerç¼–ç å™¨å †æ ˆå¯ä»¥æœ‰æ•ˆåœ°å®ç°ç»Ÿä¸€çš„VLè¡¨ç¤ºã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æ”¶é›†äº†ä¸€ä¸ªåä¸ºVL-SOT500çš„VLè·Ÿè¸ªåŸºå‡†æ•°æ®é›†ï¼Œç”¨äºå°ç›®æ ‡è·Ÿè¸ªï¼Œå…¶ä¸­åŒ…å«è¾¹ç•Œæ¡†å’Œè¯­è¨€æè¿°ã€‚æˆ‘ä»¬çš„æ•°æ®é›†åŒ…æ‹¬ä¸¤ä¸ªå…·æœ‰æŒ‘æˆ˜æ€§çš„å­é›†ï¼Œå³VL-SOT230å’ŒVL-SOT270ï¼Œä¸“é—¨ç”¨äºè¯„ä¼°é€šç”¨å’Œé«˜é€Ÿå°ç›®æ ‡è·Ÿè¸ªã€‚ç”±äºå°ç›®æ ‡çš„å¤–è²Œè¾ƒå¼±ä¸”ç‰¹å¾æœ‰é™ï¼Œå°ç›®æ ‡è·Ÿè¸ªæ˜¯ä¼—æ‰€å‘¨çŸ¥çš„æŒ‘æˆ˜ï¼Œæ®æˆ‘ä»¬æ‰€çŸ¥ï¼Œè¯¥æ•°æ®é›†æ˜¯ç¬¬ä¸€ä¸ªæ¢ç´¢ä½¿ç”¨è¯­è¨€çº¿ç´¢æ¥æé«˜å°ç›®æ ‡è·Ÿè¸ªçš„è§†è§‰è¡¨ç¤ºçš„æ•°æ®é›†ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼ŒCOSTåœ¨äº”ä¸ªç°æœ‰çš„VLè·Ÿè¸ªæ•°æ®é›†ä»¥åŠæˆ‘ä»¬æå‡ºçš„VL-SOT500æ•°æ®é›†ä¸Šè¾¾åˆ°äº†æœ€æ–°æ€§èƒ½ã€‚æºä»£ç å’Œæ•°æ®é›†å°†å…¬å¼€å‘å¸ƒã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.01321v1">PDF</a> Preprint submitted to Elsevier.   <a target="_blank" rel="noopener" href="https://github.com/983632847/Awesome-Multimodal-Object-Tracking">https://github.com/983632847/Awesome-Multimodal-Object-Tracking</a></p>
<p><strong>æ‘˜è¦</strong></p>
<p>æœ¬æ–‡æå‡ºä¸€ç§åŸºäºå¯¹æ¯”å­¦ä¹ çš„ä¸€ç«™å¼Transformerèåˆæ¡†æ¶ï¼ˆCOSTï¼‰ï¼Œç”¨äºè§†è§‰è¯­è¨€ï¼ˆVLï¼‰è·Ÿè¸ªä»»åŠ¡ã€‚è¯¥æ¡†æ¶æ—¨åœ¨å­¦ä¹ è¯­ä¹‰ä¸€è‡´ä¸”ç»Ÿä¸€çš„VLè¡¨ç¤ºï¼Œå¼•å…¥å¯¹æ¯”å¯¹é½ç­–ç•¥æ¥æœ€å¤§åŒ–è§†é¢‘ä¸å…¶å¯¹åº”çš„è¯­è¨€æè¿°ä¹‹é—´çš„äº’ä¿¡æ¯ï¼ˆMIï¼‰ï¼Œä»è€Œå®ç°æœ‰æ•ˆçš„è·¨æ¨¡æ€å¯¹é½ï¼Œåœ¨è¡¨ç¤ºç©ºé—´ä¸­äº§ç”Ÿè¯­ä¹‰ä¸€è‡´çš„ç‰¹å¾ã€‚åˆ©ç”¨è§†è§‰è¯­è¨€Transformerï¼Œå»ºç«‹äº†é«˜æ•ˆçš„å¤šæ¨¡æ€èåˆå’Œæ¨ç†æœºåˆ¶ï¼Œå®è¯è¡¨æ˜ç®€å•çš„Transformerç¼–ç å™¨å †å å¯æœ‰æ•ˆå®ç°ç»Ÿä¸€çš„VLè¡¨ç¤ºã€‚æ­¤å¤–ï¼Œæœ¬æ–‡è´¡çŒ®äº†ä¸€ä¸ªæ–°æ”¶é›†çš„VLè·Ÿè¸ªåŸºå‡†æ•°æ®é›†VL-SOT500ï¼Œç”¨äºå°ç›®æ ‡è·Ÿè¸ªï¼ŒåŒ…æ‹¬è¾¹ç•Œæ¡†å’Œè¯­è¨€æè¿°ã€‚è¯¥æ•°æ®é›†åŒ…å«ä¸¤ä¸ªå…·æœ‰æŒ‘æˆ˜æ€§çš„å­é›†VL-SOT230å’ŒVL-SOT270ï¼Œåˆ†åˆ«ç”¨äºè¯„ä¼°é€šç”¨é«˜é€Ÿå°ç›®æ ‡è·Ÿè¸ªã€‚å¹¿æ³›å®éªŒè¡¨æ˜ï¼ŒCOSTåœ¨äº”ä¸ªç°æœ‰çš„VLè·Ÿè¸ªæ•°æ®é›†ä»¥åŠæ‰€æå‡ºçš„VL-SOT500æ•°æ®é›†ä¸Šå‡è¾¾åˆ°æœ€æ–°æ€§èƒ½ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>æå‡ºäº†åŸºäºå¯¹æ¯”å­¦ä¹ çš„ä¸€ç«™å¼Transformerèåˆæ¡†æ¶ï¼ˆCOSTï¼‰ç”¨äºè§†è§‰è¯­è¨€è·Ÿè¸ªä»»åŠ¡ã€‚</li>
<li>å¼•å…¥å¯¹æ¯”å¯¹é½ç­–ç•¥æ¥æœ€å¤§åŒ–è§†é¢‘ä¸è¯­è¨€æè¿°ä¹‹é—´çš„äº’ä¿¡æ¯ï¼Œå®ç°è·¨æ¨¡æ€æœ‰æ•ˆå¯¹é½ã€‚</li>
<li>åˆ©ç”¨è§†è§‰è¯­è¨€Transformerå®ç°å¤šæ¨¡æ€èåˆå’Œæ¨ç†æœºåˆ¶ã€‚</li>
<li>æå‡ºä¸€ä¸ªæ–°çš„VLè·Ÿè¸ªåŸºå‡†æ•°æ®é›†VL-SOT500ï¼Œä¸“æ³¨äºå°ç›®æ ‡è·Ÿè¸ªã€‚</li>
<li>æ•°æ®é›†åŒ…å«VL-SOT230å’ŒVL-SOT270ä¸¤ä¸ªå…·æœ‰æŒ‘æˆ˜æ€§çš„å­é›†ï¼Œåˆ†åˆ«ç”¨äºè¯„ä¼°é€šç”¨å’Œé«˜é€Ÿå°ç›®æ ‡è·Ÿè¸ªã€‚</li>
<li>COSTæ¡†æ¶åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šå®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.01321">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-2464c33d2475767ad1eb3661aced83c5.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-704574aae0f1bf20472bf105f8ad2a65.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ec10c6ed22bbb1303f949cd58fad5e14.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-db451990559e00474daa9cd61e85e404.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ba2725e8dcff19721c2fbc476b656596.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-e6cf49eed838c4565423b06664e6ffa2.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d6d7f6361951adf776b1afd931d1c9bd.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="Bi-LAT-Bilateral-Control-Based-Imitation-Learning-via-Natural-Language-and-Action-Chunking-with-Transformers"><a href="#Bi-LAT-Bilateral-Control-Based-Imitation-Learning-via-Natural-Language-and-Action-Chunking-with-Transformers" class="headerlink" title="Bi-LAT: Bilateral Control-Based Imitation Learning via Natural Language   and Action Chunking with Transformers"></a>Bi-LAT: Bilateral Control-Based Imitation Learning via Natural Language   and Action Chunking with Transformers</h2><p><strong>Authors:Takumi Kobayashi, Masato Kobayashi, Thanpimon Buamanee, Yuki Uranishi</strong></p>
<p>We present Bi-LAT, a novel imitation learning framework that unifies bilateral control with natural language processing to achieve precise force modulation in robotic manipulation. Bi-LAT leverages joint position, velocity, and torque data from leader-follower teleoperation while also integrating visual and linguistic cues to dynamically adjust applied force. By encoding human instructions such as â€œsoftly grasp the cupâ€ or â€œstrongly twist the spongeâ€ through a multimodal Transformer-based model, Bi-LAT learns to distinguish nuanced force requirements in real-world tasks. We demonstrate Bi-LATâ€™s performance in (1) unimanual cup-stacking scenario where the robot accurately modulates grasp force based on language commands, and (2) bimanual sponge-twisting task that requires coordinated force control. Experimental results show that Bi-LAT effectively reproduces the instructed force levels, particularly when incorporating SigLIP among tested language encoders. Our findings demonstrate the potential of integrating natural language cues into imitation learning, paving the way for more intuitive and adaptive human-robot interaction. For additional material, please visit: <a target="_blank" rel="noopener" href="https://mertcookimg.github.io/bi-lat/">https://mertcookimg.github.io/bi-lat/</a> </p>
<blockquote>
<p>æˆ‘ä»¬æå‡ºäº†Bi-LATï¼Œè¿™æ˜¯ä¸€ç§æ–°çš„æ¨¡ä»¿å­¦ä¹ æ¡†æ¶ï¼Œå®ƒå°†åŒè¾¹æ§åˆ¶ä¸è‡ªç„¶è¯­è¨€å¤„ç†ç›¸ç»“åˆï¼Œå®ç°æœºå™¨äººæ“ä½œä¸­çš„ç²¾ç¡®åŠ›åº¦è°ƒèŠ‚ã€‚Bi-LATåˆ©ç”¨é¢†å¯¼è€…-è·Ÿéšè€…é¥æ“ä½œä¸­çš„å…³èŠ‚ä½ç½®ã€é€Ÿåº¦å’Œæ‰­çŸ©æ•°æ®ï¼ŒåŒæ—¶ç»“åˆè§†è§‰å’Œè¯­è¨€çº¿ç´¢æ¥åŠ¨æ€è°ƒæ•´åº”ç”¨åŠ›åº¦ã€‚é€šè¿‡å¤šæ¨¡æ€Transformeræ¨¡å‹ç¼–ç äººç±»æŒ‡ä»¤ï¼Œå¦‚â€œè½»è½»æ¡ä½æ¯å­â€æˆ–â€œç”¨åŠ›æ‹§æµ·ç»µâ€ï¼ŒBi-LATå­¦ä¼šäº†åŒºåˆ†ç°å®ä¸–ç•Œä»»åŠ¡ä¸­çš„ç»†å¾®åŠ›åº¦è¦æ±‚ã€‚æˆ‘ä»¬åœ¨ï¼ˆ1ï¼‰å•æ‰‹æ³•å †å æ¯å­çš„åœºæ™¯ä¸­å±•ç¤ºäº†Bi-LATçš„æ€§èƒ½ï¼Œæœºå™¨äººèƒ½å¤Ÿæ ¹æ®è¯­è¨€å‘½ä»¤å‡†ç¡®è°ƒèŠ‚æ¡åŠ›ï¼Œä»¥åŠåœ¨ï¼ˆ2ï¼‰éœ€è¦åè°ƒåŠ›åº¦æ§åˆ¶çš„åŒæ‰‹æ‹§æµ·ç»µä»»åŠ¡ä¸­å±•ç¤ºäº†å…¶æ€§èƒ½ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒBi-LATåœ¨æµ‹è¯•çš„è¯­è¨€ç¼–ç å™¨ä¸­èå…¥SigLIPåï¼Œèƒ½å¤Ÿæœ‰æ•ˆåœ°å†ç°æŒ‡ç¤ºçš„åŠ›åº¦æ°´å¹³ã€‚æˆ‘ä»¬çš„ç ”ç©¶å±•ç¤ºäº†å°†è‡ªç„¶è¯­è¨€çº¿ç´¢èå…¥æ¨¡ä»¿å­¦ä¹ çš„æ½œåŠ›ï¼Œä¸ºæ›´ç›´è§‚å’Œé€‚åº”æ€§çš„äººæœºäº¤äº’é“ºå¹³äº†é“è·¯ã€‚æ›´å¤šèµ„æ–™è¯·è®¿é—®ï¼š[<a target="_blank" rel="noopener" href="https://mertcookimg.github.io/bi-lat/]">https://mertcookimg.github.io/bi-lat/]</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.01301v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†Bi-LATè¿™ä¸€æ–°å‹æ¨¡ä»¿å­¦ä¹ æ¡†æ¶ï¼Œå®ƒå°†åŒè¾¹æ§åˆ¶ä¸è‡ªç„¶è¯­è¨€å¤„ç†ç›¸ç»“åˆï¼Œå®ç°äº†æœºå™¨äººæ“ä½œä¸­çš„ç²¾ç¡®åŠ›åº¦è°ƒèŠ‚ã€‚Bi-LATåˆ©ç”¨é¢†å¯¼è€…è·Ÿéšè€…çš„é¥æ“ä½œä¸­çš„å…³èŠ‚ä½ç½®ã€é€Ÿåº¦å’Œæ‰­çŸ©æ•°æ®ï¼ŒåŒæ—¶æ•´åˆè§†è§‰å’Œè¯­è¨€çº¿ç´¢æ¥åŠ¨æ€è°ƒæ•´åº”ç”¨åŠ›åº¦ã€‚é€šè¿‡å¤šæ¨¡æ€Transformeræ¨¡å‹ç¼–ç äººç±»æŒ‡ä»¤ï¼ŒBi-LATèƒ½å¤ŸåŒºåˆ†ç°å®ä»»åŠ¡ä¸­çš„ç»†å¾®åŠ›åº¦è¦æ±‚ã€‚åœ¨å•æ‰‹å æ¯å’ŒåŒæ‰‹æ‰­æµ·ç»µçš„å®éªŒåœºæ™¯ä¸­ï¼ŒBi-LATè¡¨ç°å‡ºè‰¯å¥½çš„æ€§èƒ½ï¼Œæœ‰æ•ˆå¤åˆ¶äº†æŒ‡ä»¤åŠ›åº¦ã€‚è¯¥ç ”ç©¶å±•ç¤ºäº†è‡ªç„¶è¯­è¨€çº¿ç´¢èå…¥æ¨¡ä»¿å­¦ä¹ çš„æ½œåŠ›ï¼Œä¸ºæ›´ç›´è§‚å’Œé€‚åº”æ€§çš„äººæœºäº¤äº’é“ºå¹³äº†é“è·¯ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Bi-LATæ˜¯ä¸€ä¸ªç»“åˆåŒè¾¹æ§åˆ¶ä¸è‡ªç„¶è¯­è¨€å¤„ç†çš„æ¨¡ä»¿å­¦ä¹ æ¡†æ¶ï¼Œç”¨äºæœºå™¨äººæ“ä½œçš„ç²¾ç¡®åŠ›åº¦è°ƒèŠ‚ã€‚</li>
<li>Bi-LATåˆ©ç”¨é¢†å¯¼è€…è·Ÿéšè€…çš„é¥æ“ä½œæ•°æ®ï¼Œå¹¶æ•´åˆè§†è§‰å’Œè¯­è¨€çº¿ç´¢ä»¥åŠ¨æ€è°ƒæ•´åŠ›åº¦ã€‚</li>
<li>é€šè¿‡å¤šæ¨¡æ€Transformeræ¨¡å‹ç¼–ç äººç±»æŒ‡ä»¤ï¼ŒBi-LATèƒ½åŒºåˆ†ç°å®ä»»åŠ¡ä¸­çš„ç»†å¾®åŠ›åº¦è¦æ±‚ã€‚</li>
<li>åœ¨å æ¯å’Œæ‰­æµ·ç»µçš„å®éªŒåœºæ™¯ä¸­ï¼ŒBi-LATè¡¨ç°å‡ºè‰¯å¥½çš„æ€§èƒ½ã€‚</li>
<li>Bi-LATåœ¨ç»“åˆSigLIPè¯­è¨€ç¼–ç å™¨æ—¶ï¼Œèƒ½æ›´æœ‰æ•ˆåœ°å¤åˆ¶æŒ‡ä»¤åŠ›åº¦ã€‚</li>
<li>ç ”ç©¶è¡¨æ˜ï¼Œè‡ªç„¶è¯­è¨€çº¿ç´¢èå…¥æ¨¡ä»¿å­¦ä¹ å…·æœ‰æ½œåŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.01301">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-b1f77c2725378d59bb268753d972d384.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0abc12fe263921a77ef0c5f5a655d811.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1f98433984e7328ce74b520b785abcde.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-001017459d7afd5367f0a488807bc7ec.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-aee2cd05500d450884f2f16731da17d9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-711a90776358682f1b05a8a5dca5d3e1.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-21e224135314776ef587b2431d61775e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-2e368b57faedfe1f1cc6fbef11b4b8b2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0a2ac4b1bc1b1f3e0ee90bc65e708afc.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="Open-Qwen2VL-Compute-Efficient-Pre-Training-of-Fully-Open-Multimodal-LLMs-on-Academic-Resources"><a href="#Open-Qwen2VL-Compute-Efficient-Pre-Training-of-Fully-Open-Multimodal-LLMs-on-Academic-Resources" class="headerlink" title="Open-Qwen2VL: Compute-Efficient Pre-Training of Fully-Open Multimodal   LLMs on Academic Resources"></a>Open-Qwen2VL: Compute-Efficient Pre-Training of Fully-Open Multimodal   LLMs on Academic Resources</h2><p><strong>Authors:Weizhi Wang, Yu Tian, Linjie Yang, Heng Wang, Xifeng Yan</strong></p>
<p>The reproduction of state-of-the-art multimodal LLM pre-training faces barriers at every stage of the pipeline, including high-quality data filtering, multimodal data mixture strategies, sequence packing techniques, and training frameworks. We introduce Open-Qwen2VL, a fully open-source 2B-parameter Multimodal Large Language Model pre-trained efficiently on 29M image-text pairs using only 220 A100-40G GPU hours. Our approach employs low-to-high dynamic image resolution and multimodal sequence packing to significantly enhance pre-training efficiency. The training dataset was carefully curated using both MLLM-based filtering techniques (e.g., MLM-Filter) and conventional CLIP-based filtering methods, substantially improving data quality and training efficiency. The Open-Qwen2VL pre-training is conducted on academic level 8xA100-40G GPUs at UCSB on 5B packed multimodal tokens, which is 0.36% of 1.4T multimodal pre-training tokens of Qwen2-VL. The final instruction-tuned Open-Qwen2VL outperforms partially-open state-of-the-art MLLM Qwen2-VL-2B on various multimodal benchmarks of MMBench, SEEDBench, MMstar, and MathVista, indicating the remarkable training efficiency of Open-Qwen2VL. We open-source all aspects of our work, including compute-efficient and data-efficient training details, data filtering methods, sequence packing scripts, pre-training data in WebDataset format, FSDP-based training codebase, and both base and instruction-tuned model checkpoints. We redefine â€œfully openâ€ for multimodal LLMs as the complete release of: 1) the training codebase, 2) detailed data filtering techniques, and 3) all pre-training and supervised fine-tuning data used to develop the model. </p>
<blockquote>
<p>æ¨¡æ€å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹é¢„è®­ç»ƒçš„é‡ç°é¢ä¸´ç®¡é“æ¯ä¸ªé˜¶æ®µçš„éšœç¢ï¼ŒåŒ…æ‹¬é«˜è´¨é‡æ•°æ®è¿‡æ»¤ã€å¤šæ¨¡æ€æ•°æ®æ··åˆç­–ç•¥ã€åºåˆ—æ‰“åŒ…æŠ€æœ¯å’Œè®­ç»ƒæ¡†æ¶ã€‚æˆ‘ä»¬å¼•å…¥Open-Qwen2VLï¼Œè¿™æ˜¯ä¸€ä¸ªå®Œå…¨å¼€æºçš„2Bå‚æ•°å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼Œåœ¨ä»…ä½¿ç”¨29Må›¾åƒæ–‡æœ¬å¯¹çš„æƒ…å†µä¸‹ï¼Œä½¿ç”¨é«˜æ•ˆçš„é¢„è®­ç»ƒæ–¹æ³•ã€‚æˆ‘ä»¬çš„æ–¹æ³•é‡‡ç”¨ä»ä½åˆ°é«˜çš„åŠ¨æ€å›¾åƒåˆ†è¾¨ç‡å’Œå¤šæ¨¡æ€åºåˆ—æ‰“åŒ…ï¼Œä»¥æ˜¾è‘—æé«˜é¢„è®­ç»ƒæ•ˆç‡ã€‚è®­ç»ƒæ•°æ®é›†ç»è¿‡ç²¾å¿ƒç­›é€‰ï¼Œæ—¢ä½¿ç”¨åŸºäºMLLMçš„è¿‡æ»¤æŠ€æœ¯ï¼ˆå¦‚MLM-Filterï¼‰ï¼Œä¹Ÿä½¿ç”¨ä¼ ç»Ÿçš„åŸºäºCLIPçš„è¿‡æ»¤æ–¹æ³•ï¼Œå¤§å¤§æé«˜äº†æ•°æ®è´¨é‡å’Œè®­ç»ƒæ•ˆç‡ã€‚Open-Qwen2VLçš„é¢„è®­ç»ƒæ˜¯åœ¨UCSBçš„å­¦æœ¯çº§8xA100-40G GPUä¸Šè¿›è¡Œçš„ï¼Œå¤„ç†5Bä¸ªæ‰“åŒ…çš„å¤šæ¨¡æ€ä»¤ç‰Œï¼Œè¿™æ˜¯Qwen2VLçš„1.4Tå¤šæ¨¡æ€é¢„è®­ç»ƒä»¤ç‰Œçš„0.36%ã€‚æœ€ç»ˆæŒ‡ä»¤è°ƒæ•´çš„Open-Qwen2VLåœ¨å„ç§å¤šæ¨¡æ€åŸºå‡†æµ‹è¯•ï¼ˆå¦‚MMBenchã€SEEDBenchã€MMstarå’ŒMathVistaï¼‰ä¸Šä¼˜äºéƒ¨åˆ†å¼€æºçš„å…ˆè¿›MLLM Qwen2-VL-2Bï¼Œè¯æ˜äº†Open-Qwen2VLçš„æ˜¾è‘—è®­ç»ƒæ•ˆç‡ã€‚æˆ‘ä»¬å¼€æºäº†å·¥ä½œçš„å„ä¸ªæ–¹é¢ï¼ŒåŒ…æ‹¬è®¡ç®—é«˜æ•ˆå’Œæ•°æ®é«˜æ•ˆçš„è®­ç»ƒç»†èŠ‚ã€æ•°æ®è¿‡æ»¤æ–¹æ³•ã€åºåˆ—æ‰“åŒ…è„šæœ¬ã€WebDatasetæ ¼å¼çš„é¢„è®­ç»ƒæ•°æ®ã€åŸºäºFSDPçš„è®­ç»ƒä»£ç åº“ä»¥åŠåŸºç¡€æŒ‡ä»¤å’Œç»è¿‡è°ƒæ•´çš„æ¨¡å‹æ£€æŸ¥ç‚¹ã€‚æˆ‘ä»¬é‡æ–°å®šä¹‰äº†å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹çš„â€œå®Œå…¨å¼€æ”¾â€ï¼Œå³å®Œå…¨å‘å¸ƒï¼š1ï¼‰è®­ç»ƒä»£ç åº“ï¼Œ2ï¼‰è¯¦ç»†çš„æ•°æ®è¿‡æ»¤æŠ€æœ¯ï¼Œä»¥åŠ3ï¼‰ç”¨äºå¼€å‘æ¨¡å‹çš„æ‰€æœ‰é¢„è®­ç»ƒå’Œç›‘ç£å¾®è°ƒæ•°æ®ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.00595v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤§è§„æ¨¡å¤šæ¨¡æ€é¢„è®­ç»ƒè¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„å¤åˆ¶é¢ä¸´è¯¸å¤šæŒ‘æˆ˜ï¼ŒåŒ…æ‹¬é«˜è´¨é‡æ•°æ®è¿‡æ»¤ã€å¤šæ¨¡æ€æ•°æ®æ··åˆç­–ç•¥ã€åºåˆ—æ‰“åŒ…æŠ€æœ¯å’Œè®­ç»ƒæ¡†æ¶ç­‰ã€‚æˆ‘ä»¬æ¨å‡ºOpen-Qwen2VLï¼Œè¿™æ˜¯ä¸€ä¸ªå®Œå…¨å¼€æºçš„2Bå‚æ•°å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼Œåœ¨åªæœ‰220ä¸ªA100-40G GPUå°æ—¶çš„æƒ…å†µä¸‹ï¼Œåœ¨ç”±ç²¾é€‰å›¾åƒå’Œæ–‡æœ¬ç»„æˆçš„å¤æ‚è®­ç»ƒé›†ä¸Šè¿›è¡Œé«˜æ•ˆè®­ç»ƒã€‚é€šè¿‡ä½¿ç”¨ä»ä½åˆ°é«˜çš„åŠ¨æ€å›¾åƒåˆ†è¾¨ç‡å’Œå¤šæ¨¡æ€åºåˆ—æ‰“åŒ…æ–¹æ³•ï¼Œæˆ‘ä»¬æ˜¾è‘—æé«˜äº†é¢„è®­ç»ƒæ•ˆç‡ã€‚Open-Qwen2VLåœ¨å„ç§å¤šæ¨¡æ€åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œå¹¶ä¸”æˆ‘ä»¬å…¬å¼€äº†æ‰€æœ‰ç›¸å…³çš„è®­ç»ƒç»†èŠ‚å’Œæ•°æ®é›†ç­‰ã€‚æˆ‘ä»¬é‡æ–°å®šä¹‰äº†å¤šæ¨¡æ€LLMçš„â€œå®Œå…¨å¼€æ”¾â€ï¼ŒåŒ…æ‹¬å…¬å¼€è®­ç»ƒä»£ç åº“ã€è¯¦ç»†çš„æ•°æ®è¿‡æ»¤æŠ€æœ¯å’Œæ‰€æœ‰ç”¨äºå¼€å‘æ¨¡å‹çš„é¢„è®­ç»ƒå’Œç›‘ç£å¾®è°ƒæ•°æ®ã€‚æ•´ä½“æ¥è¯´ï¼Œè¿™æ˜¯ä¸€ä¸ªæ•ˆç‡å‡ºä¼—ã€å¼€æ”¾æ€§é«˜çš„å¤šæ¨¡æ€è¯­è¨€æ¨¡å‹ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Open-Qwen2VLæ˜¯ä¸€ä¸ªå¼€æºçš„å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼Œèƒ½å¤Ÿåœ¨æœ‰é™çš„GPUèµ„æºä¸‹é«˜æ•ˆè®­ç»ƒã€‚</li>
<li>è¯¥æ¨¡å‹é‡‡ç”¨äº†å…ˆè¿›çš„å›¾åƒåˆ†è¾¨ç‡ç­–ç•¥å’Œåºåˆ—æ‰“åŒ…æŠ€æœ¯æ¥æå‡è®­ç»ƒæ•ˆç‡ã€‚</li>
<li>Open-Qwen2VLé‡‡ç”¨äº†ä¸¥æ ¼çš„è®­ç»ƒæ•°æ®é›†ç­›é€‰è¿‡ç¨‹ï¼Œç»“åˆäº†MLLMè¿‡æ»¤æŠ€æœ¯å’ŒCLIPè¿‡æ»¤æ–¹æ³•ï¼Œæå‡äº†æ•°æ®è´¨é‡å’Œè®­ç»ƒæ•ˆç‡ã€‚</li>
<li>è¯¥æ¨¡å‹åœ¨å„ç§å¤šæ¨¡æ€åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œä¼˜äºéƒ¨åˆ†å¼€æºçš„å¤šæ¨¡æ€LLM Qwen2-VL-2Bã€‚</li>
<li>Open-Qwen2VLå®ç°äº†çœŸæ­£æ„ä¹‰ä¸Šçš„å¼€æºï¼ŒåŒ…æ‹¬å…¬å¼€è®­ç»ƒä»£ç åº“ã€æ•°æ®è¿‡æ»¤æŠ€æœ¯ã€é¢„è®­ç»ƒæ•°æ®å’Œè®­ç»ƒè¿‡ç¨‹ç­‰ç»†èŠ‚ã€‚è¿™ä¸ºè¯¥é¢†åŸŸçš„æœªæ¥ç ”ç©¶æä¾›äº†é‡è¦å‚è€ƒå’ŒåŸºç¡€ã€‚ </li>
<li>é«˜æ•ˆçš„è®¡ç®—å’Œæ•°æ®åˆ©ç”¨ä½¿å…¶æˆä¸ºç ”ç©¶è€…å’Œå¼€å‘è€…çš„å®è´µèµ„æºã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.00595">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-cbc553f14bd356fe5d6ce4548da2cbcd.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3b3e617cdd4eb7e3dfe366eca1cdc5a7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-461103848bc31481129d7aeec36ed8b8.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-ecdb73abe5e9b6b46cffed27e3af2fcc.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-8d0e0a61563d61365b765858d101420b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e72c847c62cfed80798a0b83a7f189fb.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="Any2Caption-Interpreting-Any-Condition-to-Caption-for-Controllable-Video-Generation"><a href="#Any2Caption-Interpreting-Any-Condition-to-Caption-for-Controllable-Video-Generation" class="headerlink" title="Any2Caption:Interpreting Any Condition to Caption for Controllable Video   Generation"></a>Any2Caption:Interpreting Any Condition to Caption for Controllable Video   Generation</h2><p><strong>Authors:Shengqiong Wu, Weicai Ye, Jiahao Wang, Quande Liu, Xintao Wang, Pengfei Wan, Di Zhang, Kun Gai, Shuicheng Yan, Hao Fei, Tat-Seng Chua</strong></p>
<p>To address the bottleneck of accurate user intent interpretation within the current video generation community, we present Any2Caption, a novel framework for controllable video generation under any condition. The key idea is to decouple various condition interpretation steps from the video synthesis step. By leveraging modern multimodal large language models (MLLMs), Any2Caption interprets diverse inputsâ€“text, images, videos, and specialized cues such as region, motion, and camera posesâ€“into dense, structured captions that offer backbone video generators with better guidance. We also introduce Any2CapIns, a large-scale dataset with 337K instances and 407K conditions for any-condition-to-caption instruction tuning. Comprehensive evaluations demonstrate significant improvements of our system in controllability and video quality across various aspects of existing video generation models. Project Page: <a target="_blank" rel="noopener" href="https://sqwu.top/Any2Cap/">https://sqwu.top/Any2Cap/</a> </p>
<blockquote>
<p>é’ˆå¯¹å½“å‰è§†é¢‘ç”Ÿæˆç¤¾åŒºä¸­å‡†ç¡®ç†è§£ç”¨æˆ·æ„å›¾çš„ç“¶é¢ˆé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†Any2Captionï¼Œè¿™æ˜¯ä¸€ä¸ªæ–°å‹çš„å¯æ§è§†é¢‘ç”Ÿæˆæ¡†æ¶ï¼Œå¯åœ¨ä»»ä½•æ¡ä»¶ä¸‹è¿›è¡Œè§†é¢‘ç”Ÿæˆã€‚å…³é”®æ€æƒ³æ˜¯å°†å„ç§æ¡ä»¶è§£è¯»æ­¥éª¤ä¸è§†é¢‘åˆæˆæ­¥éª¤è§£è€¦ã€‚é€šè¿‡åˆ©ç”¨ç°ä»£çš„å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰ï¼ŒAny2Captionèƒ½å¤Ÿè§£é‡Šå„ç§è¾“å…¥ï¼ŒåŒ…æ‹¬æ–‡æœ¬ã€å›¾åƒã€è§†é¢‘ä»¥åŠä¸“ä¸šæç¤ºï¼ˆå¦‚åŒºåŸŸã€è¿åŠ¨å’Œç›¸æœºå§¿æ€ï¼‰ï¼Œå¹¶å°†å…¶è½¬åŒ–ä¸ºå¯†é›†çš„ç»“æ„åŒ–å­—å¹•ï¼Œä¸ºè§†é¢‘ç”Ÿæˆå™¨æä¾›æ›´å¥½çš„æŒ‡å¯¼ã€‚æˆ‘ä»¬è¿˜ä»‹ç»äº†Any2CapInsï¼Œè¿™æ˜¯ä¸€ä¸ªå¤§è§„æ¨¡æ•°æ®é›†ï¼ŒåŒ…å«337Kä¸ªå®ä¾‹å’Œ407Kä¸ªæ¡ä»¶ï¼Œç”¨äºä»»ä½•æ¡ä»¶åˆ°å­—å¹•çš„æŒ‡ä»¤è°ƒæ•´ã€‚ç»¼åˆè¯„ä¼°è¡¨æ˜ï¼Œæˆ‘ä»¬çš„ç³»ç»Ÿåœ¨å¯æ§æ€§å’Œè§†é¢‘è´¨é‡æ–¹é¢å¯¹ç°æœ‰è§†é¢‘ç”Ÿæˆæ¨¡å‹çš„å„ç§æ–¹é¢éƒ½æœ‰æ˜¾è‘—æé«˜ã€‚é¡¹ç›®é¡µé¢ï¼š<a target="_blank" rel="noopener" href="https://sqwu.top/Any2Cap/">https://sqwu.top/Any2Cap/</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.24379v1">PDF</a> Project Page: <a target="_blank" rel="noopener" href="https://sqwu.top/Any2Cap/">https://sqwu.top/Any2Cap/</a></p>
<p><strong>Summary</strong></p>
<p>Any2Captionæ˜¯ä¸€ä¸ªé’ˆå¯¹ä»»ä½•æ¡ä»¶ä¸‹çš„å¯æ§è§†é¢‘ç”Ÿæˆçš„æ–°å‹æ¡†æ¶ï¼Œå®ƒé€šè¿‡è§£è€¦å„ç§æ¡ä»¶è§£è¯»æ­¥éª¤ä¸è§†é¢‘åˆæˆæ­¥éª¤ï¼Œåˆ©ç”¨ç°ä»£çš„å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰æ¥è§£è¯»å¤šæ ·åŒ–çš„è¾“å…¥ï¼Œå¦‚æ–‡æœ¬ã€å›¾åƒã€è§†é¢‘ä»¥åŠä¸“ä¸šçº¿ç´¢ï¼ˆå¦‚åŒºåŸŸã€åŠ¨ä½œå’Œç›¸æœºå§¿æ€ï¼‰ï¼Œå¹¶å°†å…¶è½¬åŒ–ä¸ºå¯†é›†çš„ç»“æ„åŒ–å­—å¹•ï¼Œä¸ºè§†é¢‘ç”Ÿæˆå™¨æä¾›æ›´å¥½çš„æŒ‡å¯¼ã€‚æ­¤å¤–ï¼Œè¿˜ä»‹ç»äº†åŒ…å«337Kå®ä¾‹å’Œ407Kæ¡ä»¶çš„Any2CapInså¤§è§„æ¨¡æ•°æ®é›†ï¼Œç”¨äºä»»ä½•æ¡ä»¶åˆ°å­—å¹•çš„æŒ‡ä»¤è°ƒæ•´ã€‚è¯„ä¼°è¡¨æ˜ï¼Œè¯¥ç³»ç»Ÿåœ¨å¯æ§æ€§å’Œè§†é¢‘è´¨é‡æ–¹é¢å‡æœ‰æ˜¾è‘—æé«˜ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Any2Captionæ¡†æ¶è§£å†³äº†å½“å‰è§†é¢‘ç”Ÿæˆç¤¾åŒºä¸­å‡†ç¡®ç”¨æˆ·æ„å›¾è§£è¯»çš„ç“¶é¢ˆã€‚</li>
<li>è¯¥æ¡†æ¶åˆ©ç”¨ç°ä»£çš„å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰æ¥è§£è¯»å¤šæ ·åŒ–çš„è¾“å…¥ï¼ŒåŒ…æ‹¬æ–‡æœ¬ã€å›¾åƒã€è§†é¢‘å’Œä¸“ä¸šçº¿ç´¢ã€‚</li>
<li>Any2Captioné€šè¿‡å°†æ¡ä»¶è§£è¯»æ­¥éª¤ä¸è§†é¢‘åˆæˆæ­¥éª¤è§£è€¦ï¼Œå®ç°å¯æ§è§†é¢‘ç”Ÿæˆã€‚</li>
<li>Any2CapInsæ˜¯ä¸€ä¸ªå¤§è§„æ¨¡æ•°æ®é›†ï¼ŒåŒ…å«ç”¨äºä»»ä½•æ¡ä»¶åˆ°å­—å¹•æŒ‡ä»¤è°ƒæ•´çš„æ•°æ®ã€‚</li>
<li>Any2Captionæ¡†æ¶å°†å„ç§è¾“å…¥è½¬åŒ–ä¸ºç»“æ„åŒ–å­—å¹•ï¼Œä¸ºè§†é¢‘ç”Ÿæˆå™¨æä¾›æ›´å¥½çš„æŒ‡å¯¼ã€‚</li>
<li>ç»¼åˆè¯„ä¼°è¡¨æ˜ï¼ŒAny2Captionç³»ç»Ÿåœ¨å¯æ§æ€§å’Œè§†é¢‘è´¨é‡æ–¹é¢è¾ƒç°æœ‰è§†é¢‘ç”Ÿæˆæ¨¡å‹æœ‰æ˜¾è‘—æé«˜ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.24379">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-1cfb184562ecc513c68e4bde9a549f77.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-bf794d63b289934c5289489a3b5fbbf3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2c7418ade9529bb775315141cf275abb.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f7af8400b22de496bcbf2feabe31f6ed.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-0864348ae0357420605d3fad69de2e40.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="FakeScope-Large-Multimodal-Expert-Model-for-Transparent-AI-Generated-Image-Forensics"><a href="#FakeScope-Large-Multimodal-Expert-Model-for-Transparent-AI-Generated-Image-Forensics" class="headerlink" title="FakeScope: Large Multimodal Expert Model for Transparent AI-Generated   Image Forensics"></a>FakeScope: Large Multimodal Expert Model for Transparent AI-Generated   Image Forensics</h2><p><strong>Authors:Yixuan Li, Yu Tian, Yipo Huang, Wei Lu, Shiqi Wang, Weisi Lin, Anderson Rocha</strong></p>
<p>The rapid and unrestrained advancement of generative artificial intelligence (AI) presents a double-edged sword: while enabling unprecedented creativity, it also facilitates the generation of highly convincing deceptive content, undermining societal trust. As image generation techniques become increasingly sophisticated, detecting synthetic images is no longer just a binary task: it necessitates interpretable, context-aware methodologies that enhance trustworthiness and transparency. However, existing detection models primarily focus on classification, offering limited explanatory insights into image authenticity. In this work, we propose FakeScope, an expert multimodal model (LMM) tailored for AI-generated image forensics, which not only identifies AI-synthetic images with high accuracy but also provides rich, interpretable, and query-driven forensic insights. We first construct FakeChain dataset that contains linguistic authenticity reasoning based on visual trace evidence, developed through a novel human-machine collaborative framework. Building upon it, we further present FakeInstruct, the largest multimodal instruction tuning dataset containing 2 million visual instructions tailored to enhance forensic awareness in LMMs. FakeScope achieves state-of-the-art performance in both closed-ended and open-ended forensic scenarios. It can distinguish synthetic images with high accuracy while offering coherent and insightful explanations, free-form discussions on fine-grained forgery attributes, and actionable enhancement strategies. Notably, despite being trained exclusively on qualitative hard labels, FakeScope demonstrates remarkable zero-shot quantitative capability on detection, enabled by our proposed token-based probability estimation strategy. Furthermore, FakeScope exhibits strong generalization and in-the-wild ability, ensuring its applicability in real-world scenarios. </p>
<blockquote>
<p>äººå·¥æ™ºèƒ½ç”ŸæˆæŠ€æœ¯çš„è¿…çŒ›ä¸”æ— çº¦æŸçš„å‘å±•å‘ˆç°å‡ºä¸€æŠŠåŒåˆƒå‰‘çš„ç‰¹æ€§ï¼šè™½ç„¶å®ƒèƒ½å¤Ÿä¿ƒè¿›å‰æ‰€æœªæœ‰çš„åˆ›é€ åŠ›ï¼Œä½†ä¹Ÿæ–¹ä¾¿ç”Ÿæˆé«˜åº¦æ¬ºéª—æ€§çš„å†…å®¹ï¼Œä»è€Œç ´åç¤¾ä¼šä¿¡ä»»ã€‚éšç€å›¾åƒç”ŸæˆæŠ€æœ¯çš„æ—¥ç›Šæˆç†Ÿï¼Œæ£€æµ‹åˆæˆå›¾åƒä¸å†ä»…ä»…æ˜¯ä¸€ä¸ªç®€å•çš„äºŒé€‰ä¸€ä»»åŠ¡ï¼šå®ƒéœ€è¦å¯è§£é‡Šã€å…·å¤‡è¯­å¢ƒæ„è¯†çš„æ–¹æ³•è®ºæ¥æé«˜å¯ä¿¡åº¦å’Œé€æ˜åº¦ã€‚ç„¶è€Œï¼Œç°æœ‰çš„æ£€æµ‹æ¨¡å‹ä¸»è¦é›†ä¸­åœ¨åˆ†ç±»ä¸Šï¼Œå¯¹äºå›¾åƒçœŸå®æ€§çš„è§£é‡Šæ€§æ´å¯Ÿæœ‰é™ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†FakeScopeï¼Œä¸€ä¸ªä¸“ä¸ºAIç”Ÿæˆå›¾åƒå–è¯å®šåˆ¶çš„ä¸“å®¶å¤šæ¨¡å¼æ¨¡å‹ï¼ˆLMMï¼‰ã€‚å®ƒä¸ä»…èƒ½å¤Ÿé«˜åº¦å‡†ç¡®åœ°è¯†åˆ«AIåˆæˆçš„å›¾åƒï¼Œè€Œä¸”æä¾›ä¸°å¯Œã€å¯è§£é‡Šã€æŸ¥è¯¢é©±åŠ¨çš„å–è¯æ´å¯Ÿã€‚æˆ‘ä»¬é¦–å…ˆæ„å»ºäº†FakeChainæ•°æ®é›†ï¼Œå…¶ä¸­åŒ…å«åŸºäºè§†è§‰ç—•è¿¹è¯æ®çš„è¯­è¨€çœŸå®æ€§æ¨ç†ï¼Œè¿™æ˜¯é€šè¿‡æ–°å‹çš„äººæœºåä½œæ¡†æ¶å¼€å‘è€Œæˆçš„ã€‚åœ¨æ­¤åŸºç¡€ä¸Šï¼Œæˆ‘ä»¬è¿›ä¸€æ­¥æ¨å‡ºäº†FakeInstructï¼Œè¿™æ˜¯æœ€å¤§çš„å¤šæ¨¡å¼æŒ‡ä»¤è°ƒæ•´æ•°æ®é›†ï¼ŒåŒ…å«200ä¸‡æ¡æ—¨åœ¨æé«˜LMMså–è¯æ„è¯†çš„è§†è§‰æŒ‡ä»¤ã€‚FakeScopeåœ¨å°é—­å’Œå¼€æ”¾çš„å–è¯åœºæ™¯ä¸­å‡å®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚å®ƒèƒ½å¤Ÿå‡†ç¡®åœ°åŒºåˆ†åˆæˆå›¾åƒï¼ŒåŒæ—¶æä¾›è¿è´¯è€Œå¯Œæœ‰æ´å¯ŸåŠ›çš„è§£é‡Šã€å…³äºç»†å¾®ä¼ªé€ å±æ€§çš„è‡ªç”±å½¢å¼è®¨è®ºå’Œå¯è¡Œçš„å¢å¼ºç­–ç•¥ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œå°½ç®¡FakeScopeä»…ä½¿ç”¨å®šæ€§ç¡¬æ ‡ç­¾è¿›è¡Œè®­ç»ƒï¼Œä½†å¾—ç›Šäºæˆ‘ä»¬æå‡ºçš„åŸºäºæ ‡è®°çš„æ¦‚ç‡ä¼°è®¡ç­–ç•¥ï¼Œå®ƒåœ¨æ£€æµ‹æ–¹é¢å±•ç°å‡ºäº†æƒŠäººçš„é›¶æ ·æœ¬å®šé‡èƒ½åŠ›ã€‚æ­¤å¤–ï¼ŒFakeScopeè¡¨ç°å‡ºå¼ºå¤§çš„æ³›åŒ–å’Œé‡å¤–èƒ½åŠ›ï¼Œç¡®ä¿å…¶åœ¨ç°å®åœºæ™¯ä¸­çš„é€‚ç”¨æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.24267v1">PDF</a> </p>
<p><strong>Summary</strong><br>     äººå·¥æ™ºèƒ½æŠ€æœ¯çš„å¿«é€Ÿå‘å±•å‘ˆç°å‡ºä¸€æŠŠåŒåˆƒå‰‘çš„ç‰¹æ€§ï¼šåœ¨æ¿€å‘å‰æ‰€æœªæœ‰çš„åˆ›é€ åŠ›çš„åŒæ—¶ï¼Œä¹Ÿæ˜“äºç”Ÿæˆå…·æœ‰é«˜åº¦æ¬ºéª—æ€§çš„å†…å®¹ï¼ŒæŸå®³ç¤¾ä¼šä¿¡ä»»ã€‚é’ˆå¯¹è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€æ¬¾åä¸ºFakeScopeçš„å®šåˆ¶åŒ–å¤šæ¨¡æ€æ¨¡å‹ï¼ˆLMMï¼‰ï¼Œä¸“é—¨ç”¨äºäººå·¥æ™ºèƒ½ç”Ÿæˆçš„å›¾åƒå–è¯ã€‚è¯¥æ¨¡å‹ä¸ä»…èƒ½å‡†ç¡®è¯†åˆ«AIåˆæˆçš„å›¾åƒï¼Œè€Œä¸”æä¾›ä¸°å¯Œã€å¯è§£é‡Šã€æŸ¥è¯¢é©±åŠ¨çš„å–è¯ä¿¡æ¯ã€‚ä¸ºå®ç°æ­¤åŠŸèƒ½ï¼Œæˆ‘ä»¬æ„å»ºäº†FakeChainæ•°æ®é›†å’ŒFakeInstructæŒ‡ä»¤è°ƒä¼˜æ•°æ®é›†ã€‚FakeScopeåœ¨å°é—­å’Œå¼€æ”¾ç¯å¢ƒä¸‹çš„å–è¯åœºæ™¯ä¸­å‡å–å¾—äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œå…·å¤‡å¼ºå¤§çš„é›¶æ ·æœ¬å®šé‡æ£€æµ‹èƒ½åŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç”Ÿæˆå¼äººå·¥æ™ºèƒ½ï¼ˆAIï¼‰çš„å¿«é€Ÿå‘å±•å¸¦æ¥äº†ç¤¾ä¼šä¿¡ä»»çš„é—®é¢˜ï¼Œå› ä¸ºAIå¯ä»¥ç”Ÿæˆé«˜åº¦æ¬ºéª—æ€§çš„å†…å®¹ã€‚</li>
<li>ç°æœ‰çš„æ£€æµ‹æ¨¡å‹ä¸»è¦ä¾§é‡äºåˆ†ç±»ï¼Œå¯¹äºå›¾åƒçœŸå®æ€§çš„è§£é‡Šæ€§æœ‰é™ã€‚</li>
<li>FakeScopeæ˜¯ä¸€æ¬¾ä¸“é—¨ç”¨äºAIç”Ÿæˆçš„å›¾åƒå–è¯çš„å®šåˆ¶åŒ–å¤šæ¨¡æ€æ¨¡å‹ï¼ˆLMMï¼‰ã€‚</li>
<li>FakeScopeä¸ä»…èƒ½å‡†ç¡®è¯†åˆ«AIåˆæˆçš„å›¾åƒï¼Œè€Œä¸”æä¾›ä¸°å¯Œã€å¯è§£é‡Šã€æŸ¥è¯¢é©±åŠ¨çš„å–è¯ä¿¡æ¯ã€‚</li>
<li>ä¸ºè®­ç»ƒFakeScopeï¼Œæ„å»ºäº†FakeChainæ•°æ®é›†å’ŒFakeInstructæŒ‡ä»¤è°ƒä¼˜æ•°æ®é›†ã€‚</li>
<li>FakeScopeåœ¨å°é—­å’Œå¼€æ”¾ç¯å¢ƒä¸‹çš„å–è¯åœºæ™¯ä¸­å‡è¡¨ç°ä¼˜ç§€ï¼Œå…·å¤‡å¼ºå¤§çš„æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.24267">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-694bdba20e34d207a3c0dbc9db54f26e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-8bd6f7f99f737955e1da167a0c876124.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f74842ca58f90816dd3859a5c8c51e26.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-4d7f04a757a2d8760ea837a36c99b828.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5220b28658718646564e588b6fcba51f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-29097bbc089fcafef3b02be6e3349c2a.jpg" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="BeMERC-Behavior-Aware-MLLM-based-Framework-for-Multimodal-Emotion-Recognition-in-Conversation"><a href="#BeMERC-Behavior-Aware-MLLM-based-Framework-for-Multimodal-Emotion-Recognition-in-Conversation" class="headerlink" title="BeMERC: Behavior-Aware MLLM-based Framework for Multimodal Emotion   Recognition in Conversation"></a>BeMERC: Behavior-Aware MLLM-based Framework for Multimodal Emotion   Recognition in Conversation</h2><p><strong>Authors:Yumeng Fu, Junjie Wu, Zhongjie Wang, Meishan Zhang, Yulin Wu, Bingquan Liu</strong></p>
<p>Multimodal emotion recognition in conversation (MERC), the task of identifying the emotion label for each utterance in a conversation, is vital for developing empathetic machines. Current MLLM-based MERC studies focus mainly on capturing the speakerâ€™s textual or vocal characteristics, but ignore the significance of video-derived behavior information. Different from text and audio inputs, learning videos with rich facial expression, body language and posture, provides emotion trigger signals to the models for more accurate emotion predictions. In this paper, we propose a novel behavior-aware MLLM-based framework (BeMERC) to incorporate speakerâ€™s behaviors, including subtle facial micro-expression, body language and posture, into a vanilla MLLM-based MERC model, thereby facilitating the modeling of emotional dynamics during a conversation. Furthermore, BeMERC adopts a two-stage instruction tuning strategy to extend the model to the conversations scenario for end-to-end training of a MERC predictor. Experiments demonstrate that BeMERC achieves superior performance than the state-of-the-art methods on two benchmark datasets, and also provides a detailed discussion on the significance of video-derived behavior information in MERC. </p>
<blockquote>
<p>å¤šæ¨¡æ€æƒ…æ„Ÿè¯†åˆ«æŠ€æœ¯ï¼ˆMERCï¼‰åœ¨å¯¹è¯ä¸­è¯†åˆ«æ¯ä¸ªè¯è¯­çš„æƒ…æ„Ÿæ ‡ç­¾çš„ä»»åŠ¡å¯¹äºå¼€å‘å…±æƒ…æœºå™¨è‡³å…³é‡è¦ã€‚å½“å‰çš„åŸºäºå¤§å‹è¯­è¨€æ¨¡å‹çš„MERCç ”ç©¶ä¸»è¦é›†ä¸­åœ¨æ•æ‰è¯´è¯äººçš„æ–‡æœ¬æˆ–è¯­éŸ³ç‰¹å¾ä¸Šï¼Œè€Œå¿½ç•¥äº†è§†é¢‘è¡ç”Ÿè¡Œä¸ºä¿¡æ¯çš„é‡è¦æ€§ã€‚ä¸åŒäºæ–‡æœ¬å’ŒéŸ³é¢‘è¾“å…¥ï¼Œå­¦ä¹ åŒ…å«ä¸°å¯Œé¢éƒ¨è¡¨æƒ…ã€è‚¢ä½“è¯­è¨€å’Œå§¿åŠ¿çš„è§†é¢‘ä¸ºæ¨¡å‹æä¾›äº†æƒ…æ„Ÿè§¦å‘ä¿¡å·ï¼Œä»¥å®ç°æ›´å‡†ç¡®çš„æƒ…æ„Ÿé¢„æµ‹ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°é¢–çš„è¡Œä¸ºæ„ŸçŸ¥å¤§å‹è¯­è¨€æ¨¡å‹æ¡†æ¶ï¼ˆBeMERCï¼‰ï¼Œå°†è¯´è¯äººçš„è¡Œä¸ºï¼ˆåŒ…æ‹¬å¾®å¦™çš„é¢éƒ¨è¡¨æƒ…ã€è‚¢ä½“è¯­è¨€å’Œå§¿åŠ¿ï¼‰çº³å…¥åŸºæœ¬çš„åŸºäºå¤§å‹è¯­è¨€æ¨¡å‹çš„MERCæ¨¡å‹ä¸­ï¼Œä»è€Œä¿ƒè¿›äº†å¯¹è¯è¿‡ç¨‹ä¸­æƒ…æ„ŸåŠ¨æ€çš„å»ºæ¨¡ã€‚æ­¤å¤–ï¼ŒBeMERCé‡‡ç”¨äº†ä¸¤é˜¶æ®µæŒ‡ä»¤å¾®è°ƒç­–ç•¥ï¼Œå°†æ¨¡å‹æ‰©å±•åˆ°å¯¹è¯åœºæ™¯ï¼Œå¯¹MERCé¢„æµ‹å™¨è¿›è¡Œç«¯åˆ°ç«¯çš„è®­ç»ƒã€‚å®éªŒè¡¨æ˜ï¼ŒBeMERCåœ¨ä¸¤ä¸ªåŸºå‡†æ•°æ®é›†ä¸Šçš„æ€§èƒ½ä¼˜äºæœ€æ–°æ–¹æ³•ï¼Œå¹¶è¯¦ç»†è®¨è®ºäº†è§†é¢‘è¡ç”Ÿè¡Œä¸ºä¿¡æ¯åœ¨MERCä¸­çš„é‡è¦æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.23990v1">PDF</a> </p>
<p><strong>Summary</strong><br>å¤šæ¨¡æ€æƒ…æ„Ÿè¯†åˆ«å¯¹è¯ï¼ˆMERCï¼‰æ˜¯å¼€å‘å…±æƒ…æœºå™¨çš„é‡è¦ä»»åŠ¡ä¹‹ä¸€ï¼Œæ—¨åœ¨è¯†åˆ«å¯¹è¯ä¸­æ¯ä¸ªå¥å­çš„æƒ…æ„Ÿæ ‡ç­¾ã€‚å½“å‰åŸºäºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„MERCç ”ç©¶ä¸»è¦å…³æ³¨æ•æ‰è¯´è¯è€…çš„æ–‡æœ¬æˆ–è¯­éŸ³ç‰¹å¾ï¼Œä½†å¿½ç•¥äº†è§†é¢‘è¡Œä¸ºä¿¡æ¯çš„é‡è¦æ€§ã€‚ä¸æ–‡æœ¬å’ŒéŸ³é¢‘è¾“å…¥ä¸åŒï¼Œè§†é¢‘åŒ…å«ä¸°å¯Œçš„é¢éƒ¨è¡¨æƒ…ã€èº«ä½“è¯­è¨€å’Œå§¿åŠ¿ï¼Œä¸ºæ¨¡å‹æä¾›äº†æƒ…æ„Ÿè§¦å‘ä¿¡å·ï¼Œä»¥å®ç°æ›´å‡†ç¡®çš„æƒ…æ„Ÿé¢„æµ‹ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°å‹çš„è¡Œä¸ºæ„ŸçŸ¥LLMæ¡†æ¶ï¼ˆBeMERCï¼‰ï¼Œè¯¥æ¡†æ¶å°†è¯´è¯è€…çš„è¡Œä¸ºï¼ˆåŒ…æ‹¬å¾®å¦™çš„é¢éƒ¨è¡¨æƒ…ã€èº«ä½“è¯­è¨€å’Œå§¿åŠ¿ï¼‰çº³å…¥åŸºæœ¬çš„LLM-based MERCæ¨¡å‹ä¸­ï¼Œä»è€Œä¿ƒè¿›å¯¹è¯è¿‡ç¨‹ä¸­çš„æƒ…æ„ŸåŠ¨æ€å»ºæ¨¡ã€‚æ­¤å¤–ï¼ŒBeMERCé‡‡ç”¨ä¸¤é˜¶æ®µæŒ‡ä»¤è°ƒæ•´ç­–ç•¥ï¼Œå°†æ¨¡å‹æ‰©å±•åˆ°å¯¹è¯åœºæ™¯ï¼Œä»¥ç«¯åˆ°ç«¯çš„æ–¹å¼è®­ç»ƒMERCé¢„æµ‹å™¨ã€‚å®éªŒè¡¨æ˜ï¼ŒBeMERCåœ¨ä¸¤ä¸ªåŸºå‡†æ•°æ®é›†ä¸Šçš„æ€§èƒ½ä¼˜äºæœ€å…ˆè¿›çš„æ–¹æ³•ï¼Œå¹¶è¯¦ç»†è®¨è®ºäº†è§†é¢‘è¡Œä¸ºä¿¡æ¯åœ¨MERCä¸­çš„é‡è¦æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤šæ¨¡æ€æƒ…æ„Ÿè¯†åˆ«å¯¹è¯ï¼ˆMERCï¼‰å¯¹äºå¼€å‘å…±æƒ…æœºå™¨è‡³å…³é‡è¦ã€‚</li>
<li>å½“å‰LLM-based MERCç ”ç©¶ä¸»è¦å…³æ³¨æ–‡æœ¬å’Œè¯­éŸ³ç‰¹å¾ï¼Œå¿½è§†äº†è§†é¢‘è¡Œä¸ºä¿¡æ¯çš„é‡è¦æ€§ã€‚</li>
<li>è§†é¢‘åŒ…å«ä¸°å¯Œçš„é¢éƒ¨è¡¨æƒ…ã€èº«ä½“è¯­è¨€å’Œå§¿åŠ¿ï¼Œä¸ºæ¨¡å‹æä¾›æƒ…æ„Ÿè§¦å‘ä¿¡å·ï¼Œæé«˜æƒ…æ„Ÿé¢„æµ‹å‡†ç¡®æ€§ã€‚</li>
<li>BeMERCæ¡†æ¶ç»“åˆäº†è¯´è¯è€…çš„è¡Œä¸ºä¿¡æ¯ï¼ˆåŒ…æ‹¬å¾®å¦™çš„é¢éƒ¨è¡¨æƒ…ã€èº«ä½“è¯­è¨€å’Œå§¿åŠ¿ï¼‰ï¼Œä»¥ä¿ƒè¿›å¯¹è¯è¿‡ç¨‹ä¸­çš„æƒ…æ„ŸåŠ¨æ€å»ºæ¨¡ã€‚</li>
<li>BeMERCé‡‡ç”¨ä¸¤é˜¶æ®µæŒ‡ä»¤è°ƒæ•´ç­–ç•¥ï¼Œå°†æ¨¡å‹æ‰©å±•åˆ°å¯¹è¯åœºæ™¯å¹¶è¿›è¡Œç«¯åˆ°ç«¯çš„MERCé¢„æµ‹å™¨è®­ç»ƒã€‚</li>
<li>å®éªŒæ˜¾ç¤ºBeMERCåœ¨åŸºå‡†æ•°æ®é›†ä¸Šçš„æ€§èƒ½ä¼˜äºç°æœ‰æ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.23990">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-0866d551823874ef556899d374f0d081.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-216939f2bff5da162e505b5a9be840c9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5b4dfdd17a5390d7bb50f51566aca1d9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e14bffce7f0ff64d7965aa4d34a1358d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b181770298b00c52f27d026572708383.jpg" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="LLMigrate-Transforming-â€œLazyâ€-Large-Language-Models-into-Efficient-Source-Code-Migrators"><a href="#LLMigrate-Transforming-â€œLazyâ€-Large-Language-Models-into-Efficient-Source-Code-Migrators" class="headerlink" title="LLMigrate: Transforming â€œLazyâ€ Large Language Models into Efficient   Source Code Migrators"></a>LLMigrate: Transforming â€œLazyâ€ Large Language Models into Efficient   Source Code Migrators</h2><p><strong>Authors:Yuchen Liu, Junhao Hu, Yingdi Shan, Ge Li, Yanzhen Zou, Yihong Dong, Tao Xie</strong></p>
<p>Rewriting C code in Rust provides stronger memory safety, yet migrating large codebases such as the 32-million-line Linux kernel remains challenging. While rule-based translators (e.g., C2Rust) provide accurate yet largely unsafe Rust programs, recent Large Language Model (LLM) approaches produce more idiomatic, safe Rust programs but frequently exhibit â€œlazinessâ€, omitting significant portions of the target code. To address the issue, in this paper, we present LLMigrate, an LLM-based C-to-Rust translation tool that splits modules into discrete functions, translating them individually, and then reintegrating them. LLMigrate uses static analysis to retain necessary context, pairs GPT-4o (a state-of-the-art LLM) with compiler-driven translation and program-repair techniques for complex core functions, and leverages call-graph-guided translation to ensure consistent interfaces. Evaluations on three representative Linux kernel modules (math, sort, and ramfs) show that LLMigrate requires modifying less than 15% of the target code, significantly outperforming a pure GPT-4o-based migration. </p>
<blockquote>
<p>é‡å†™Rustä¸­çš„Cä»£ç å¯ä»¥æä¾›æ›´å¼ºçš„å†…å­˜å®‰å…¨æ€§ï¼Œä½†æ˜¯è¿ç§»åƒ3200ä¸‡è¡Œçš„Linuxå†…æ ¸è¿™æ ·çš„å¤§å‹ä»£ç åº“ä»ç„¶å…·æœ‰æŒ‘æˆ˜æ€§ã€‚è™½ç„¶åŸºäºè§„åˆ™çš„ç¿»è¯‘å™¨ï¼ˆä¾‹å¦‚C2Rustï¼‰æä¾›çš„Rustç¨‹åºæ˜¯å‡†ç¡®çš„ï¼Œä½†å¤§å¤šæ˜¯ä¸å®‰å…¨çš„ã€‚æœ€è¿‘çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ–¹æ³•äº§ç”Ÿçš„æ˜¯æ›´åœ°é“çš„ã€å®‰å…¨çš„Rustç¨‹åºï¼Œä½†ç»å¸¸è¡¨ç°å‡ºâ€œæ‡’æƒ°â€ï¼Œçœç•¥äº†å¤§é‡çš„ç›®æ ‡ä»£ç ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæœ¬æ–‡ä»‹ç»äº†LLMigrateï¼Œè¿™æ˜¯ä¸€ç§åŸºäºLLMçš„Cåˆ°Rustç¿»è¯‘å·¥å…·ã€‚å®ƒå°†æ¨¡å—åˆ†å‰²æˆç¦»æ•£å‡½æ•°ï¼Œé€ä¸ªç¿»è¯‘å®ƒä»¬ï¼Œç„¶åå†è¿›è¡Œæ•´åˆã€‚LLMigrateä½¿ç”¨é™æ€åˆ†ææ¥ä¿ç•™å¿…è¦çš„ä¸Šä¸‹æ–‡ï¼Œå°†æœ€å‰æ²¿çš„LLM GPT-4oä¸ç¼–è¯‘å™¨é©±åŠ¨çš„ç¿»è¯‘å’Œç¨‹åºä¿®å¤æŠ€æœ¯ç›¸ç»“åˆç”¨äºå¤„ç†å¤æ‚çš„æ ¸å¿ƒåŠŸèƒ½ï¼Œå¹¶åˆ©ç”¨è°ƒç”¨å›¾æŒ‡å¯¼çš„ç¿»è¯‘æ¥ç¡®ä¿ä¸€è‡´çš„æ¥å£ã€‚å¯¹ä¸‰ä¸ªä»£è¡¨æ€§çš„Linuxå†…æ ¸æ¨¡å—çš„è¯„ä¼°ï¼ˆæ•°å­¦ã€æ’åºå’Œramfsï¼‰æ˜¾ç¤ºï¼ŒLLMigrateåªéœ€è¦ä¿®æ”¹ç›®æ ‡ä»£ç çš„ä¸åˆ°15%ï¼Œæ˜¾è‘—ä¼˜äºçº¯GPT-4oçš„è¿ç§»ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.23791v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>é‡å†™Cä»£ç åˆ°Rustèƒ½æä¾›æ›´å¼ºå¤§çš„å†…å­˜å®‰å…¨æ€§ï¼Œä½†è¿ç§»å¤§å‹ä»£ç åº“å¦‚3200ä¸‡è¡Œçš„Linuxå†…æ ¸ä»å…·æŒ‘æˆ˜ã€‚è§„åˆ™åŸºç¡€ç¿»è¯‘å™¨ï¼ˆå¦‚C2Rustï¼‰èƒ½ç”Ÿæˆå‡†ç¡®ä½†å¤§å¤šä¸å®‰å…¨çš„Rustç¨‹åºï¼Œè€Œæœ€è¿‘çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ–¹æ³•èƒ½ç”Ÿæˆæ›´åœ°é“çš„ã€å®‰å…¨çš„Rustç¨‹åºï¼Œä½†å¸¸æœ‰â€œæ‡’æƒ°â€ç°è±¡ï¼Œçœç•¥ç›®æ ‡ä»£ç çš„é‡è¦éƒ¨åˆ†ã€‚ä¸ºè§£å†³æ­¤é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºLLMigrateï¼Œä¸€ä¸ªåŸºäºLLMçš„Cåˆ°Rustç¿»è¯‘å·¥å…·ï¼Œå®ƒå°†æ¨¡å—åˆ†å‰²æˆç¦»æ•£å‡½æ•°è¿›è¡Œä¸ªåˆ«ç¿»è¯‘ï¼Œå†æ•´åˆã€‚LLMigrateåˆ©ç”¨é™æ€åˆ†æä¿ç•™å¿…è¦çš„ä¸Šä¸‹æ–‡ï¼Œç»“åˆGPT-4oï¼ˆæœ€å…ˆè¿›çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼‰ä¸ç¼–è¯‘å™¨é©±åŠ¨çš„ç¿»è¯‘å’Œç¨‹åºä¿®å¤æŠ€æœ¯å¤„ç†å¤æ‚æ ¸å¿ƒå‡½æ•°ï¼Œå¹¶åˆ©ç”¨è°ƒç”¨å›¾æŒ‡å¯¼ç¿»è¯‘ä»¥ç¡®ä¿ä¸€è‡´çš„æ¥å£ã€‚å¯¹ä¸‰ä¸ªä»£è¡¨æ€§çš„Linuxå†…æ ¸æ¨¡å—ï¼ˆæ•°å­¦ã€æ’åºå’Œramfsï¼‰çš„è¯„ä¼°æ˜¾ç¤ºï¼ŒLLMigrateåªéœ€è¦ä¿®æ”¹ç›®æ ‡ä»£ç çš„ä¸åˆ°15%ï¼Œæ˜¾è‘—ä¼˜äºçº¯GPT-4oçš„è¿ç§»ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLMigrateæ˜¯ä¸€ä¸ªåŸºäºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„Cåˆ°Rustç¿»è¯‘å·¥å…·ã€‚</li>
<li>LLMigrateé€šè¿‡åˆ†å‰²æ¨¡å—å¹¶é€ä¸ªç¿»è¯‘å‡½æ•°æ¥è§£å†³å†…å­˜å®‰å…¨é—®é¢˜å’Œä»£ç è¿ç§»æŒ‘æˆ˜ã€‚</li>
<li>LLMigrateä½¿ç”¨é™æ€åˆ†ææ¥ä¿ç•™å¿…è¦çš„ä¸Šä¸‹æ–‡ä¿¡æ¯ã€‚</li>
<li>LLMigrateç»“åˆäº†GPT-4oä¸ç¼–è¯‘å™¨é©±åŠ¨çš„ç¿»è¯‘å’Œç¨‹åºä¿®å¤æŠ€æœ¯ã€‚</li>
<li>LLMigrateåˆ©ç”¨è°ƒç”¨å›¾æŒ‡å¯¼ç¿»è¯‘ä»¥ç¡®ä¿æ¥å£çš„ä¸€è‡´æ€§ã€‚</li>
<li>åœ¨ä¸‰ä¸ªä»£è¡¨æ€§Linuxå†…æ ¸æ¨¡å—çš„æµ‹è¯•ä¸­ï¼ŒLLMigrateæ˜¾è‘—ä¼˜äºçº¯GPT-4oçš„è¿ç§»æ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.23791">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-355daef3be70915c6faad51a10d09bb0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7aff9d808a0000d3cfef17dd85c7b61d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-60b07c60d1f0acfd939bd84bd263448f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2750813b3eb111ca2ee0407fe9434cad.jpg" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1><h2 id="Building-Instruction-Tuning-Datasets-from-Human-Written-Instructions-with-Open-Weight-Large-Language-Models"><a href="#Building-Instruction-Tuning-Datasets-from-Human-Written-Instructions-with-Open-Weight-Large-Language-Models" class="headerlink" title="Building Instruction-Tuning Datasets from Human-Written Instructions   with Open-Weight Large Language Models"></a>Building Instruction-Tuning Datasets from Human-Written Instructions   with Open-Weight Large Language Models</h2><p><strong>Authors:Youmi Ma, Sakae Mizuki, Kazuki Fujii, Taishi Nakamura, Masanari Ohi, Hinari Shimada, Taihei Shiotani, Koshiro Saito, Koki Maeda, Kakeru Hattori, Takumi Okamoto, Shigeki Ishida, Rio Yokota, Hiroya Takamura, Naoaki Okazaki</strong></p>
<p>Instruction tuning is crucial for enabling Large Language Models (LLMs) to solve real-world tasks. Prior work has shown the effectiveness of instruction-tuning data synthesized solely from LLMs, raising a fundamental question: Do we still need human-originated signals for instruction tuning? This work answers the question affirmatively: we build state-of-the-art instruction-tuning datasets sourced from human-written instructions, by simply pairing them with LLM-generated responses. LLMs fine-tuned on our datasets consistently outperform those fine-tuned on existing ones. Our data construction approach can be easily adapted to other languages; we build datasets for Japanese and confirm that LLMs tuned with our data reach state-of-the-art performance. Analyses suggest that instruction-tuning in a new language allows LLMs to follow instructions, while the tuned models exhibit a notable lack of culture-specific knowledge in that language. The datasets and fine-tuned models will be publicly available. Our datasets, synthesized with open-weight LLMs, are openly distributed under permissive licenses, allowing for diverse use cases. </p>
<blockquote>
<p>æŒ‡ä»¤å¾®è°ƒå¯¹äºä½¿å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è§£å†³ç°å®ä¸–ç•Œä»»åŠ¡è‡³å…³é‡è¦ã€‚å…ˆå‰çš„å·¥ä½œå·²ç»è¯æ˜äº†ä»…é€šè¿‡LLMåˆæˆçš„æŒ‡ä»¤å¾®è°ƒæ•°æ®çš„æœ‰æ•ˆæ€§ï¼Œè¿™å¼•å‘äº†ä¸€ä¸ªæ ¹æœ¬é—®é¢˜ï¼šæˆ‘ä»¬æ˜¯å¦ä»ç„¶éœ€è¦äººç±»äº§ç”Ÿçš„ä¿¡å·æ¥è¿›è¡ŒæŒ‡ä»¤å¾®è°ƒï¼Ÿè¿™é¡¹å·¥ä½œè‚¯å®šåœ°å›ç­”äº†è¿™ä¸ªé—®é¢˜ï¼šæˆ‘ä»¬é€šè¿‡å°†äººç±»ç¼–å†™çš„æŒ‡ä»¤ä¸LLMç”Ÿæˆçš„å“åº”ç®€å•é…å¯¹ï¼Œå»ºç«‹äº†æœ€å…ˆè¿›çš„æŒ‡ä»¤å¾®è°ƒæ•°æ®é›†ã€‚åœ¨æˆ‘ä»¬æ•°æ®é›†ä¸Šå¾®è°ƒçš„LLMå§‹ç»ˆä¼˜äºåœ¨ç°æœ‰æ•°æ®é›†ä¸Šå¾®è°ƒçš„LLMã€‚æˆ‘ä»¬çš„æ•°æ®æ„å»ºæ–¹æ³•å¯ä»¥è½»æ¾åœ°é€‚åº”å…¶ä»–è¯­è¨€ï¼›æˆ‘ä»¬ä¸ºæ—¥è¯­æ„å»ºäº†æ•°æ®é›†ï¼Œå¹¶ç¡®è®¤ä½¿ç”¨æˆ‘ä»¬çš„æ•°æ®è°ƒæ•´çš„LLMè¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚åˆ†æè¡¨æ˜ï¼Œåœ¨æ–°çš„è¯­è¨€ä¸­è¿›è¡ŒæŒ‡ä»¤å¾®è°ƒå¯ä»¥ä½¿LLMéµå¾ªæŒ‡ä»¤ï¼Œè€Œç»è¿‡è°ƒæ•´çš„æ¨¡å‹åœ¨è¯¥è¯­è¨€çš„ç‰¹å®šæ–‡åŒ–çŸ¥è¯†æ–¹é¢è¡¨ç°å‡ºæ˜æ˜¾çš„ç¼ºä¹ã€‚æ•°æ®é›†å’Œç»è¿‡è°ƒæ•´çš„æ¨¡å‹å°†å…¬å¼€å¯ç”¨ã€‚æˆ‘ä»¬çš„æ•°æ®é›†ä½¿ç”¨å…¬å¼€æƒé‡çš„å¤§å‹è¯­è¨€æ¨¡å‹åˆæˆï¼Œåœ¨è®¸å¯çš„è®¸å¯ä¸‹å…¬å¼€åˆ†å‘ï¼Œæ”¯æŒå¤šç§ç”¨ä¾‹ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.23714v1">PDF</a> 15 pages, 5 figures</p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æŒ‡ä»¤å¾®è°ƒå¯¹äºè§£å†³ç°å®ä»»åŠ¡è‡³å…³é‡è¦ã€‚æœ¬ç ”ç©¶é€šè¿‡ç»“åˆäººç±»ç¼–å†™çš„æŒ‡ä»¤ä¸LLMç”Ÿæˆçš„å“åº”ï¼Œæ„å»ºäº†æœ€å…ˆè¿›çš„æŒ‡ä»¤å¾®è°ƒæ•°æ®é›†ã€‚åœ¨è¿™äº›æ•°æ®é›†ä¸Šå¾®è°ƒçš„LLMæ€§èƒ½å§‹ç»ˆä¼˜äºåœ¨ç°æœ‰æ•°æ®é›†ä¸Šå¾®è°ƒçš„æ¨¡å‹ã€‚æœ¬æ–¹æ³•çš„æ•°æ®æ„å»ºæ–¹å¼å¯è½»æ¾é€‚åº”å…¶ä»–è¯­è¨€ï¼Œå¹¶ä¸ºæ—¥è¯­æ„å»ºäº†æ•°æ®é›†ï¼ŒéªŒè¯äº†ä½¿ç”¨æœ¬æ•°æ®è°ƒæ•™çš„LLMå¯è¾¾åˆ°æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚åˆ†ææ˜¾ç¤ºï¼Œåœ¨æ–°è¯­è¨€ä¸­è¿›è¡ŒæŒ‡ä»¤å¾®è°ƒä½¿LLMèƒ½å¤Ÿéµå¾ªæŒ‡ä»¤ï¼Œè€Œè°ƒæ•™çš„æ¨¡å‹åœ¨è¯¥è¯­è¨€çš„æ–‡åŒ–ç‰¹å®šçŸ¥è¯†æ–¹é¢å­˜åœ¨æ˜æ˜¾çš„ç¼ºä¹ã€‚æ•°æ®é›†å’Œè°ƒæ•™æ¨¡å‹å°†å…¬å¼€å¯ç”¨ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æŒ‡ä»¤å¾®è°ƒå¯¹LLMè§£å†³ç°å®ä»»åŠ¡è‡³å…³é‡è¦ã€‚</li>
<li>é€šè¿‡ç»“åˆäººç±»ç¼–å†™çš„æŒ‡ä»¤ä¸LLMç”Ÿæˆçš„å“åº”ï¼Œæ„å»ºäº†æœ€å…ˆè¿›çš„æŒ‡ä»¤å¾®è°ƒæ•°æ®é›†ã€‚</li>
<li>åœ¨æ­¤æ•°æ®é›†ä¸Šå¾®è°ƒçš„LLMæ€§èƒ½ä¼˜äºç°æœ‰æ•°æ®é›†ä¸Šçš„æ€§èƒ½ã€‚</li>
<li>æ•°æ®æ„å»ºæ–¹å¼å¯é€‚åº”å…¶ä»–è¯­è¨€ï¼Œä¾‹å¦‚ä¸ºæ—¥è¯­æ„å»ºäº†æ•°æ®é›†ã€‚</li>
<li>ä½¿ç”¨æœ¬æ•°æ®è°ƒæ•™çš„LLMå¯è¾¾åˆ°æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚</li>
<li>åœ¨æ–°è¯­è¨€ä¸­è¿›è¡ŒæŒ‡ä»¤å¾®è°ƒä½¿LLMèƒ½å¤Ÿéµå¾ªæŒ‡ä»¤ï¼Œä½†å­˜åœ¨æ–‡åŒ–ç‰¹å®šçŸ¥è¯†çš„ç¼ºä¹ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.23714">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-271f876a84e1c148d45235b600e0360d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3dd5cfaedc0b4c1ee61a43019bc9493d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-576243bb6c675f8981be921421718123.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-63960a210eb9ef0d3ad745fa86942a2c.jpg" align="middle">
</details>


<h1 id="-19"><a href="#-19" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-04-03/LLM/">https://kedreamix.github.io/Talk2Paper/Paper/2025-04-03/LLM/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/LLM/">
                                    <span class="chip bg-color">LLM</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-04-03/Agent/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-7550e694fa69b58b370453beee565178.jpg" class="responsive-img" alt="Agent">
                        
                        <span class="card-title">Agent</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Agent æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-04-03  Review, Refine, Repeat Understanding Iterative Decoding of AI Agents   with Dynamic Evaluation and Selection
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-04-03
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Agent/" class="post-category">
                                    Agent
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Agent/">
                        <span class="chip bg-color">Agent</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-04-03/R1_Reasoning/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-d3ea20c763842611ad3c4ee24851c5f8.jpg" class="responsive-img" alt="R1_Reasoning">
                        
                        <span class="card-title">R1_Reasoning</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            R1_Reasoning æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-04-03  OpenCodeReasoning Advancing Data Distillation for Competitive Coding
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-04-03
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/R1-Reasoning/" class="post-category">
                                    R1_Reasoning
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/R1-Reasoning/">
                        <span class="chip bg-color">R1_Reasoning</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">18799.9k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
