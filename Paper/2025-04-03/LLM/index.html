<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="LLM">
    <meta name="description" content="LLM 方向最新论文已更新，请持续关注 Update in 2025-04-03  Towards Unified Referring Expression Segmentation Across Omni-Level   Visual Target Granularities">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>LLM | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loader页面消失采用渐隐的方式*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logo出现动画 */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//使用渐隐的方法淡出loading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//强制显示loading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">嘘~  正在从服务器偷取页面 . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>首页</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>标签</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>分类</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>归档</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="搜索" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="深色/浅色模式" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			首页
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			标签
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			分类
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			归档
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pic1.zhimg.com/v2-e673eac368ee8357e01903864dfaf4db.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">LLM</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- 文章内容详情 -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/LLM/">
                                <span class="chip bg-color">LLM</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/LLM/" class="post-category">
                                LLM
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp;
                    2025-04-03
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>更新日期:&nbsp;&nbsp;
                    2025-05-14
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>文章字数:&nbsp;&nbsp;
                    19.9k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>阅读时长:&nbsp;&nbsp;
                    81 分
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>阅读次数:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>⚠️ 以下所有内容总结都来自于 大语言模型的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p>
</blockquote>
<h1 id="2025-04-03-更新"><a href="#2025-04-03-更新" class="headerlink" title="2025-04-03 更新"></a>2025-04-03 更新</h1><h2 id="Towards-Unified-Referring-Expression-Segmentation-Across-Omni-Level-Visual-Target-Granularities"><a href="#Towards-Unified-Referring-Expression-Segmentation-Across-Omni-Level-Visual-Target-Granularities" class="headerlink" title="Towards Unified Referring Expression Segmentation Across Omni-Level   Visual Target Granularities"></a>Towards Unified Referring Expression Segmentation Across Omni-Level   Visual Target Granularities</h2><p><strong>Authors:Jing Liu, Wenxuan Wang, Yisi Zhang, Yepeng Tang, Xingjian He, Longteng Guo, Tongtian Yue, Xinlong Wang</strong></p>
<p>Referring expression segmentation (RES) aims at segmenting the entities’ masks that match the descriptive language expression. While traditional RES methods primarily address object-level grounding, real-world scenarios demand a more versatile framework that can handle multiple levels of target granularity, such as multi-object, single object or part-level references. This introduces great challenges due to the diverse and nuanced ways users describe targets. However, existing datasets and models mainly focus on designing grounding specialists for object-level target localization, lacking the necessary data resources and unified frameworks for the more practical multi-grained RES. In this paper, we take a step further towards visual granularity unified RES task. To overcome the limitation of data scarcity, we introduce a new multi-granularity referring expression segmentation (MRES) task, alongside the RefCOCOm benchmark, which includes part-level annotations for advancing finer-grained visual understanding. In addition, we create MRES-32M, the largest visual grounding dataset, comprising over 32.2M masks and captions across 1M images, specifically designed for part-level vision-language grounding. To tackle the challenges of multi-granularity RES, we propose UniRES++, a unified multimodal large language model that integrates object-level and part-level RES tasks. UniRES++ incorporates targeted designs for fine-grained visual feature exploration. With the joint model architecture and parameters, UniRES++ achieves state-of-the-art performance across multiple benchmarks, including RefCOCOm for MRES, gRefCOCO for generalized RES, and RefCOCO, RefCOCO+, RefCOCOg for classic RES. To foster future research into multi-grained visual grounding, our RefCOCOm benchmark, MRES-32M dataset and model UniRES++ will be publicly available at <a target="_blank" rel="noopener" href="https://github.com/Rubics-Xuan/MRES">https://github.com/Rubics-Xuan/MRES</a>. </p>
<blockquote>
<p>指代表达式分割（RES）旨在分割与描述性语言表达相匹配的实体掩膜。虽然传统的RES方法主要解决对象级别的定位问题，但现实场景需要更通用的框架，能够处理多种目标粒度级别，例如多目标、单目标或部件级别的引用。这由于用户描述目标的多样性和微妙差异而带来了极大的挑战。然而，现有数据集和模型主要关注为对象级别的目标定位设计定位专家，缺乏更实用的多粒度RES所需的数据资源和统一框架。在本文中，我们朝着统一视觉粒度的RES任务迈进了一步。为了克服数据稀缺的限制，我们引入了新的多粒度指代表达式分割（MRES）任务，以及RefCOCOm基准测试，该基准测试包含部件级别的注释，以促进更精细的视觉理解。此外，我们创建了MRES-32M，这是最大的视觉定位数据集，包含超过3220万个掩膜和标题，涵盖100万张图像，专门用于部件级别的视觉语言定位。为了解决多粒度RES的挑战，我们提出了UniRES++，这是一个统一的多模式大型语言模型，集成了对象级别和部件级别的RES任务。UniRES++针对精细粒度的视觉特征探索进行了有针对性的设计。通过联合模型结构和参数，UniRES++在多个基准测试中实现了最先进的性能，包括用于MRES的RefCOCOm、用于广义RES的gRefCOCO以及用于经典RES的RefCOCO、RefCOCO+和RefCOCOg。为了促进未来对多粒度视觉定位的研究，我们的RefCOCOm基准测试、MRES-32M数据集和UniRES++模型将在<a target="_blank" rel="noopener" href="https://github.com/Rubics-Xuan/MRES">https://github.com/Rubics-Xuan/MRES</a>上公开提供。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.01954v1">PDF</a> </p>
<p><strong>摘要</strong></p>
<p>本文介绍了多粒度引用表达式分割（MRES）任务及其挑战。为解决数据稀缺的问题，作者引入了RefCOCOm基准测试集和MRES-32M数据集，用于推进细粒度视觉理解。为应对多粒度RES的挑战，作者提出了UniRES++统一多模态大型语言模型，该模型整合了对象级别和部分级别的RES任务，并实现了多个基准测试集的最佳性能。</p>
<p><strong>关键见解</strong></p>
<ol>
<li>引用表达式分割（RES）旨在分割与描述性语言表达相匹配的实体掩膜。</li>
<li>传统RES方法主要解决对象级别的定位，但现实场景需要更灵活的处理多种粒度目标。</li>
<li>用户描述目标的多样性和细微差别给RES带来了很大的挑战。</li>
<li>数据稀缺是限制多粒度RES发展的一个主要问题。</li>
<li>作者引入了新的多粒度引用表达式分割（MRES）任务和RefCOCOm基准测试集来解决这一问题。</li>
<li>MRES-32M数据集的创建是为了推进细粒度视觉理解，包含超过32.2M的掩膜和标题。</li>
<li>UniRES++模型是第一个统一的多模态大型语言模型，用于处理对象级别和部分级别的RES任务，并在多个基准测试集上实现了最佳性能。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.01954">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-935f87848d7b498eccf29fb7d7a2c45f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-59f9f116af0f86d86a506ae8512f5f76.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-480c638e94fdc4e1e2f8a69c33591a4e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c999c00260917a8f66034b1ae980ccbb.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-32388cd402944d484cb88f0be43c351d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-57e8546915c8432e910b1c2b853425b5.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-7893fb39ca0d8aa3590b9f672bda352d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-190c45dcf2ccef334328cac3b8a07945.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="OpenCodeReasoning-Advancing-Data-Distillation-for-Competitive-Coding"><a href="#OpenCodeReasoning-Advancing-Data-Distillation-for-Competitive-Coding" class="headerlink" title="OpenCodeReasoning: Advancing Data Distillation for Competitive Coding"></a>OpenCodeReasoning: Advancing Data Distillation for Competitive Coding</h2><p><strong>Authors:Wasi Uddin Ahmad, Sean Narenthiran, Somshubra Majumdar, Aleksander Ficek, Siddhartha Jain, Jocelyn Huang, Vahid Noroozi, Boris Ginsburg</strong></p>
<p>Since the advent of reasoning-based large language models, many have found great success from distilling reasoning capabilities into student models. Such techniques have significantly bridged the gap between reasoning and standard LLMs on coding tasks. Despite this, much of the progress on distilling reasoning models remains locked behind proprietary datasets or lacks details on data curation, filtering and subsequent training. To address this, we construct a superior supervised fine-tuning (SFT) dataset that we use to achieve state-of-the-art coding capability results in models of various sizes. Our distilled models use only SFT to achieve 61.8% on LiveCodeBench and 24.6% on CodeContests, surpassing alternatives trained with reinforcement learning. We then perform analysis on the data sources used to construct our dataset, the impact of code execution filtering, and the importance of instruction&#x2F;solution diversity. We observe that execution filtering negatively affected benchmark accuracy, leading us to prioritize instruction diversity over solution correctness. Finally, we also analyze the token efficiency and reasoning patterns utilized by these models. We will open-source these datasets and distilled models to the community. </p>
<blockquote>
<p>自从基于推理的大型语言模型出现以来，许多人通过将推理能力蒸馏到学生模型中取得了巨大成功。这类技术在编码任务上显著缩小了推理和标准大型语言模型之间的差距。尽管如此，关于蒸馏推理模型的许多进展仍然局限于专有数据集，或者缺乏关于数据收集、过滤和后续训练方面的详细信息。为了解决这个问题，我们构建了一个出色的监督微调（SFT）数据集，我们用它在各种规模的模型中实现了最先进的编码能力结果。我们的蒸馏模型仅使用SFT在LiveCodeBench上达到61.8%，在CodeContests上达到24.6%，超过了使用强化学习训练的替代模型。然后，我们对构建我们数据集所使用的数据来源、代码执行过滤的影响以及指令&#x2F;解决方案多样性的重要性进行了分析。我们发现执行过滤对基准测试准确性产生了负面影响，因此我们优先重视指令的多样性而非解决方案的正确性。最后，我们还分析了这些模型的令牌效率和推理模式。我们将向社区开源这些数据集和蒸馏模型。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.01943v1">PDF</a> Work in progress</p>
<p><strong>Summary</strong></p>
<p>本文介绍了通过构建优质监督微调（SFT）数据集，将推理能力蒸馏到学生模型中的方法，并在各种规模的模型上实现了最先进的编码能力结果。通过使用仅SFT，我们的蒸馏模型在LiveCodeBench上达到61.8%，在CodeContests上达到24.6%，超越了使用强化学习训练的替代模型。本文对构建数据集使用的数据来源、代码执行过滤的影响以及指令&#x2F;解决方案多样性的重要性进行了分析。发现执行过滤会对基准测试准确性产生负面影响，因此指令多样性优先于解决方案的正确性。最后，还分析了这些模型的令牌效率和推理模式。我们将这些数据集和蒸馏模型开源给社区使用。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>通过构建优质监督微调（SFT）数据集，成功将推理能力蒸馏到学生模型中，实现先进编码能力。</li>
<li>使用仅SFT的蒸馏模型在LiveCodeBench和CodeContests上取得较高成绩。</li>
<li>对比强化学习训练的模型，SFT方法表现更优。</li>
<li>分析发现执行过滤对基准测试准确性有负面影响，因此重视指令多样性。</li>
<li>开源数据集和蒸馏模型，便于社区使用。</li>
<li>分析了模型的令牌效率和推理模式。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.01943">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-d4380a2da8480d4b9e1fcb7c28f44a90.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a0bb60cea00b040106932355d54f872f.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-6bd0fdea622909f4419f65eb13767bb7.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-003554efde4e4b026d13dedecf6f056d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8561d6b1998c13873167749f11edc13d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1170c2a7a6ec92d93d72822e29ec3937.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="A-thorough-benchmark-of-automatic-text-classification-From-traditional-approaches-to-large-language-models"><a href="#A-thorough-benchmark-of-automatic-text-classification-From-traditional-approaches-to-large-language-models" class="headerlink" title="A thorough benchmark of automatic text classification: From traditional   approaches to large language models"></a>A thorough benchmark of automatic text classification: From traditional   approaches to large language models</h2><p><strong>Authors:Washington Cunha, Leonardo Rocha, Marcos André Gonçalves</strong></p>
<p>Automatic text classification (ATC) has experienced remarkable advancements in the past decade, best exemplified by recent small and large language models (SLMs and LLMs), leveraged by Transformer architectures. Despite recent effectiveness improvements, a comprehensive cost-benefit analysis investigating whether the effectiveness gains of these recent approaches compensate their much higher costs when compared to more traditional text classification approaches such as SVMs and Logistic Regression is still missing in the literature. In this context, this work’s main contributions are twofold: (i) we provide a scientifically sound comparative analysis of the cost-benefit of twelve traditional and recent ATC solutions including five open LLMs, and (ii) a large benchmark comprising {22 datasets}, including sentiment analysis and topic classification, with their (train-validation-test) partitions based on folded cross-validation procedures, along with documentation, and code. The release of code, data, and documentation enables the community to replicate experiments and advance the field in a more scientifically sound manner. Our comparative experimental results indicate that LLMs outperform traditional approaches (up to 26%-7.1% on average) and SLMs (up to 4.9%-1.9% on average) in terms of effectiveness. However, LLMs incur significantly higher computational costs due to fine-tuning, being, on average 590x and 8.5x slower than traditional methods and SLMs, respectively. Results suggests the following recommendations: (1) LLMs for applications that require the best possible effectiveness and can afford the costs; (2) traditional methods such as Logistic Regression and SVM for resource-limited applications or those that cannot afford the cost of tuning large LLMs; and (3) SLMs like Roberta for near-optimal effectiveness-efficiency trade-off. </p>
<blockquote>
<p>文本分类（ATC）在过去的十年中取得了显著的进步，最近的小型和大型语言模型（SLM和LLM）就是最好的例证，它们利用了Transformer架构。尽管近期的有效性改进得到了很好的验证，但在文献中仍缺乏关于这些新方法的有效性收益是否弥补了其与传统文本分类方法（如SVM和逻辑回归）相比的高昂成本的全面成本效益分析。在此背景下，这项工作的主要贡献有两点：首先，我们对包括五种开源大型语言模型在内的十二种传统和最新的自动文本分类解决方案的成本效益进行了科学的比较分析；其次，我们构建了一个包含情感分析和主题分类的{22个数据集}的大型基准测试集，基于折叠交叉验证程序对其进行了（训练-验证-测试）分区，并附带了文档和代码。代码的发布、数据和文档资料使社区能够复制实验并以更科学的方式推动该领域的发展。我们的比较实验结果指出，大型语言模型在有效性方面优于传统方法（平均高出26%~7.1%）和小型语言模型（平均高出4.9%~1.9%）。然而，由于微调的需要，大型语言模型的计算成本显著更高，平均而言，它们比传统方法和小型语言模型慢590倍和8.5倍。根据研究结果，我们提出以下建议：1）对于要求最佳效果并能承受成本的应用程序，使用大型语言模型；2）对于资源有限或无法承受大型语言模型调整成本的应用程序，使用传统方法，如逻辑回归和SVM；3）对于寻求近乎最优的有效性和效率平衡的应用程序，使用小型语言模型（如Roberta）。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.01930v1">PDF</a> 7 pages, 2 figures, 3 tables</p>
<p><strong>Summary</strong></p>
<p>该文对比分析了传统与最新的自动文本分类（ATC）解决方案的成本效益，包括五种开源的大型语言模型（LLMs）。研究发现，LLMs在效果上优于传统方法和小型语言模型（SLMs），但计算成本更高。因此，根据应用需求和资源限制，可选择不同的方法。对于追求最佳效果且能承受成本的应用，推荐使用LLMs；对于资源有限或无法承担大型语言模型调整成本的应用，推荐使用传统方法，如逻辑回归和支持向量机；对于寻求近似的最优效益效率权衡的，建议使用SLMs，如Roberta。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLMs在自动文本分类中表现出优异的效果，相比传统方法和SLMs有更高的分类准确性。</li>
<li>LLMs的计算成本显著较高，需要进行精细调整。</li>
<li>LLMs、传统方法和SLMs之间存在的效益效率权衡需根据具体应用场景选择。</li>
<li>对于资源受限或成本敏感的应用，推荐使用传统方法如逻辑回归和支持向量机。</li>
<li>对于追求最佳效果的场景，LLMs是理想选择。</li>
<li>SLMs如Roberta提供了近似的最优效益效率权衡。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.01930">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-ba42484bb44a8039dda8e0fdbe5cb485.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-10f73cfd0b46a3130c060a1a0abb54bc.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c82471e7dabb031a45468af7a7aab20d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a93a690b84867ee53fe8054c6435058a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f0f3952e97b50d4d4cc2fd903f21d91e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b51abde8e40dfcd2b42dab547f9072d9.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="STAR-1-Safer-Alignment-of-Reasoning-LLMs-with-1K-Data"><a href="#STAR-1-Safer-Alignment-of-Reasoning-LLMs-with-1K-Data" class="headerlink" title="STAR-1: Safer Alignment of Reasoning LLMs with 1K Data"></a>STAR-1: Safer Alignment of Reasoning LLMs with 1K Data</h2><p><strong>Authors:Zijun Wang, Haoqin Tu, Yuhan Wang, Juncheng Wu, Jieru Mei, Brian R. Bartoldson, Bhavya Kailkhura, Cihang Xie</strong></p>
<p>This paper introduces STAR-1, a high-quality, just-1k-scale safety dataset specifically designed for large reasoning models (LRMs) like DeepSeek-R1. Built on three core principles – diversity, deliberative reasoning, and rigorous filtering – STAR-1 aims to address the critical needs for safety alignment in LRMs. Specifically, we begin by integrating existing open-source safety datasets from diverse sources. Then, we curate safety policies to generate policy-grounded deliberative reasoning samples. Lastly, we apply a GPT-4o-based safety scoring system to select training examples aligned with best practices. Experimental results show that fine-tuning LRMs with STAR-1 leads to an average 40% improvement in safety performance across four benchmarks, while only incurring a marginal decrease (e.g., an average of 1.1%) in reasoning ability measured across five reasoning tasks. Extensive ablation studies further validate the importance of our design principles in constructing STAR-1 and analyze its efficacy across both LRMs and traditional LLMs. Our project page is <a target="_blank" rel="noopener" href="https://ucsc-vlaa.github.io/STAR-1">https://ucsc-vlaa.github.io/STAR-1</a>. </p>
<blockquote>
<p>本文介绍了STAR-1，这是一个专为大型推理模型（如DeepSeek-R1）设计的高质量、仅包含一千个样本的安全数据集。基于多样性、审慎推理和严格筛选三大核心原则，STAR-1旨在解决大型推理模型在安全对齐方面的关键需求。具体来说，我们首先整合来自不同来源的现有开源安全数据集。然后，我们制定安全策略以生成基于策略审慎推理样本。最后，我们应用基于GPT-4的安全评分系统来选择与最佳实践对齐的训练样本。实验结果表明，使用STAR-1对大型推理模型进行微调，在四个基准测试上的安全性能平均提高了4. 提升幅度达 百分之四十，同时仅在五个推理任务中略微降低推理能力（平均下降百分之一点一）。广泛的消融研究进一步验证了我们的设计原则在构建STAR-1中的重要性，并分析了其在大型推理模型和传统大型语言模型中的有效性。我们的项目页面是：<a target="_blank" rel="noopener" href="https://ucsc-vlaa.github.io/STAR-1%E3%80%82">https://ucsc-vlaa.github.io/STAR-1。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.01903v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>STAR-1数据集旨在满足大型推理模型（LRMs）在安全性方面的需求。它通过集成多种开源数据集、制定安全策略，并基于GPT-4o的安全评分系统筛选训练样本，从而确保数据集的多样性和审慎推理能力。实验结果显示，使用STAR-1微调的LRM在四个基准测试中安全性能平均提升40%，同时推理能力仅小幅下降（平均下降率为1.1%）。研究还通过消融研究验证了数据集设计的三个核心原则的重要性。</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>STAR-1是一个专为大型推理模型设计的高质量安全数据集。</li>
<li>数据集构建基于多样性、审慎推理和严格筛选三大原则。</li>
<li>通过集成多种开源数据集来增强数据多样性。</li>
<li>制定安全策略，生成基于策略的审慎推理样本。</li>
<li>使用GPT-4o的安全评分系统筛选训练样本，与最佳实践对齐。</li>
<li>实验结果显示，使用STAR-1微调的LRM安全性能大幅提升，同时推理能力影响较小。</li>
<li>消融研究验证了数据集设计的核心原则的重要性。</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.01903">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-eacf55114141fbf8489fd896f6fca4ae.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a3bce844203c398839f3e89586195374.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-40d99782dafeba56ee8265365c9370be.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-832ebff3e52aed24090d3653c44e6f77.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-85fc740539de2c3066cdb182445fa637.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Ross3D-Reconstructive-Visual-Instruction-Tuning-with-3D-Awareness"><a href="#Ross3D-Reconstructive-Visual-Instruction-Tuning-with-3D-Awareness" class="headerlink" title="Ross3D: Reconstructive Visual Instruction Tuning with 3D-Awareness"></a>Ross3D: Reconstructive Visual Instruction Tuning with 3D-Awareness</h2><p><strong>Authors:Haochen Wang, Yucheng Zhao, Tiancai Wang, Haoqiang Fan, Xiangyu Zhang, Zhaoxiang Zhang</strong></p>
<p>The rapid development of Large Multimodal Models (LMMs) for 2D images and videos has spurred efforts to adapt these models for interpreting 3D scenes. However, the absence of large-scale 3D vision-language datasets has posed a significant obstacle. To address this issue, typical approaches focus on injecting 3D awareness into 2D LMMs by designing 3D input-level scene representations. This work provides a new perspective. We introduce reconstructive visual instruction tuning with 3D-awareness (Ross3D), which integrates 3D-aware visual supervision into the training procedure. Specifically, it incorporates cross-view and global-view reconstruction. The former requires reconstructing masked views by aggregating overlapping information from other views. The latter aims to aggregate information from all available views to recover Bird’s-Eye-View images, contributing to a comprehensive overview of the entire scene. Empirically, Ross3D achieves state-of-the-art performance across various 3D scene understanding benchmarks. More importantly, our semi-supervised experiments demonstrate significant potential in leveraging large amounts of unlabeled 3D vision-only data. </p>
<blockquote>
<p>大型多模态模型（LMM）在2D图像和视频领域的快速发展，激发了将这些模型应用于解释3D场景的努力。然而，缺乏大规模3D视觉语言数据集成为了巨大的障碍。为了解决这个问题，典型的方法是通过设计3D输入级场景表示，将3D意识注入到2D LMM中。这项工作提供了一个新的视角。我们引入了具有3D意识的重建视觉指令调整（Ross3D），它将3D意识视觉监督集成到训练过程中。具体来说，它结合了跨视图和全局视图重建。前者需要通过聚合其他视图的重叠信息来重建被遮挡的视图。后者旨在从所有可用的视图中聚合信息，以恢复鸟瞰图像，为整个场景提供全面的概述。经验上，Ross3D在各种3D场景理解基准测试中达到了最先进的性能。更重要的是，我们的半监督实验表明，利用大量未标记的3D视觉数据具有巨大的潜力。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.01901v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>大型多模态模型（LMMs）在解读二维图像和视频方面发展迅速，但在解读三维场景时仍面临挑战。缺乏大规模的三维视觉语言数据集是主要障碍之一。本文提出一种新型方法Ross3D，通过将三维感知视觉监督融入训练过程，将三维意识注入二维LMMs。它包含跨视图重建和全局视图重建，分别通过聚合其他视图的重叠信息和所有可用视图的信息来重建遮挡视图和获取对整个场景的全面概述。实证结果显示，Ross3D在多种三维场景理解基准测试中达到最佳性能，并且在利用大量未标记的三维视觉数据方面展现出巨大潜力。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>大型多模态模型（LMMs）在解读三维场景时面临挑战。</li>
<li>缺乏大规模三维视觉语言数据集是主要障碍之一。</li>
<li>Ross3D方法通过将三维感知视觉监督融入训练过程来解决这一问题。</li>
<li>Ross3D包含跨视图重建和全局视图重建。</li>
<li>跨视图重建通过聚合其他视图的信息来重建遮挡视图。</li>
<li>全局视图重建旨在获取对整个场景的全面概述。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.01901">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-02cdd63d51c21c4ee60b1aa2fa23109f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-47f832d9af9584b479dced5d24c96c31.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-6653a1d0ced561d38145f3ddb3eb57bb.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-80ca58f098bce993744b0042eee99101.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="TransientTables-Evaluating-LLMs’-Reasoning-on-Temporally-Evolving-Semi-structured-Tables"><a href="#TransientTables-Evaluating-LLMs’-Reasoning-on-Temporally-Evolving-Semi-structured-Tables" class="headerlink" title="TransientTables: Evaluating LLMs’ Reasoning on Temporally Evolving   Semi-structured Tables"></a>TransientTables: Evaluating LLMs’ Reasoning on Temporally Evolving   Semi-structured Tables</h2><p><strong>Authors:Abhilash Shankarampeta, Harsh Mahajan, Tushar Kataria, Dan Roth, Vivek Gupta</strong></p>
<p>Humans continuously make new discoveries, and understanding temporal sequence of events leading to these breakthroughs is essential for advancing science and society. This ability to reason over time allows us to identify future steps and understand the effects of financial and political decisions on our lives. However, large language models (LLMs) are typically trained on static datasets, limiting their ability to perform effective temporal reasoning. To assess the temporal reasoning capabilities of LLMs, we present the TRANSIENTTABLES dataset, which comprises 3,971 questions derived from over 14,000 tables, spanning 1,238 entities across multiple time periods. We introduce a template-based question-generation pipeline that harnesses LLMs to refine both templates and questions. Additionally, we establish baseline results using state-of-the-art LLMs to create a benchmark. We also introduce novel modeling strategies centered around task decomposition, enhancing LLM performance. </p>
<blockquote>
<p>人类不断有新的发现，了解导致这些突破的事件的时间顺序对于推动科学和社会进步至关重要。这种随时间推理的能力使我们能够确定未来的步骤，并了解金融和政治决策对我们的生活的影响。然而，大型语言模型（LLM）通常是在静态数据集上进行训练的，这限制了它们进行有效的时间推理的能力。为了评估LLM的时间推理能力，我们推出了TRANSIENTTABLES数据集，该数据集包含3971个问题，这些问题源于超过14000个表格，涉及多个时间段的1238个实体。我们引入了一个基于模板的问题生成管道，利用LLM来完善模板和问题。此外，我们还使用最先进的LLM建立基线结果，以建立基准测试，并围绕任务分解引入新型建模策略，以提高LLM的性能。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.01879v1">PDF</a> 19 Pages. 21 Tables, 1 figure</p>
<p><strong>Summary</strong>：人类不断有新的发现，了解这些突破事件的时序对于推进科学和社会至关重要。对时间的推理能力有助于我们预测未来并采取影响生活和社会的决策。然而，大型语言模型（LLM）通常在静态数据集上训练，无法有效地进行时间推理。为此，我们引入了TRANSIENTTABLES数据集，其中包含从超过一万四千张表格衍生出的三千九百七十一个问题，涵盖了跨越多个时间段的实体。我们还介绍了一种基于模板的问题生成管道，并利用大型语言模型对模板和问题进行了改进。同时，我们建立了使用最新大型语言模型的基准结果，并提出了以任务分解为中心的新型建模策略来提高大型语言模型的性能。</p>
<p><strong>Key Takeaways</strong>：</p>
<ol>
<li>大型语言模型（LLM）在时序推理方面存在局限性。</li>
<li>TRANSIENTTABLES数据集包含从大量表格衍生的问题，用于评估LLM的时序推理能力。</li>
<li>该数据集涵盖了多个时间段的实体，对LLM提出了挑战。</li>
<li>引入了一种基于模板的问题生成管道，改进了大型语言模型的性能。</li>
<li>使用最新的大型语言模型建立了基准结果。</li>
<li>提出了新型建模策略，如任务分解，以提高LLM的性能。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.01879">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-89af2b462028452ff5332f4254508e05.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-1a5bebf66d486a17704a2234c9c22dbc.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Cross-Lingual-Consistency-A-Novel-Inference-Framework-for-Advancing-Reasoning-in-Large-Language-Models"><a href="#Cross-Lingual-Consistency-A-Novel-Inference-Framework-for-Advancing-Reasoning-in-Large-Language-Models" class="headerlink" title="Cross-Lingual Consistency: A Novel Inference Framework for Advancing   Reasoning in Large Language Models"></a>Cross-Lingual Consistency: A Novel Inference Framework for Advancing   Reasoning in Large Language Models</h2><p><strong>Authors:Zhiwei Yu, Tuo Li, Changhong Wang, Hui Chen, Lang Zhou</strong></p>
<p>Chain-of-thought (CoT) has emerged as a critical mechanism for enhancing reasoning capabilities in large language models (LLMs), with self-consistency demonstrating notable promise in boosting performance. However, inherent linguistic biases in multilingual training corpora frequently cause semantic drift and logical inconsistencies, especially in sub-10B parameter LLMs handling complex inference tasks. To overcome these constraints, we propose the Cross-Lingual Consistency (CLC) framework, an innovative inference paradigm that integrates multilingual reasoning paths through majority voting to elevate LLMs’ reasoning capabilities. Empirical evaluations on the CMATH dataset reveal CLC’s superiority over the conventional self-consistency method, delivering 9.5%, 6.5%, and 6.0% absolute accuracy gains for DeepSeek-Math-7B-Instruct, Qwen2.5-Math-7B-Instruct, and Gemma2-9B-Instruct respectively. Expanding CLC’s linguistic scope to 11 diverse languages implies two synergistic benefits: 1) neutralizing linguistic biases in multilingual training corpora through multilingual ensemble voting, 2) escaping monolingual reasoning traps by exploring the broader multilingual solution space. This dual benefits empirically enables more globally optimal reasoning paths compared to monolingual self-consistency baselines, as evidenced by the 4.1%-18.5% accuracy gains using Gemma2-9B-Instruct on the MGSM dataset. </p>
<blockquote>
<p>思维链（CoT）作为提升大型语言模型（LLM）推理能力的重要机制已经崭露头角，其中自我一致性在提升性能上显示出巨大潜力。然而，多语种训练语料库中的固有语言偏见经常导致语义漂移和逻辑不一致，特别是在处理复杂推理任务的参数小于10B的LLMs中更为明显。为了克服这些限制，我们提出了跨语言一致性（CLC）框架，这是一种创新性的推理范式，它通过多数投票的方式整合多语言推理路径，以提升LLMs的推理能力。在CMATH数据集上的实证评估显示，CLC相较于传统的自我一致性方法具有优越性，分别为DeepSeek-Math-7B-Instruct、Qwen2.5-Math-7B-Instruct和Gemma2-9B-Instruct带来了9.5%、6.5%和6.0%的绝对精度提升。将CLC的语言范围扩展到11种不同的语言，带来了两个协同效益：1）通过多语言集成投票中和多语种训练语料库中的语言偏见；2）通过探索更广泛的多语言解决方案空间，避免单语推理陷阱。这两种效益的结合，使得与单语自我一致性基准相比，能够找到更多全局最优的推理路径。如在MGSM数据集上，使用Gemma2-9B-Instruct的精度提升了4.1%~18.5%。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.01857v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>基于Chain-of-thought（CoT）机制，大型语言模型（LLM）的推理能力得到了增强。然而，在多语言训练语料库中存在的固有语言偏见会导致语义漂移和逻辑不一致。为解决这一问题，提出了Cross-Lingual Consistency（CLC）框架，通过多数投票的方式整合多语言推理路径，提升LLM的推理能力。在CMATH数据集上的实证评估表明，相较于传统的自一致性方法，CLC框架分别实现了DeepSeek-Math-7B-Instruct、Qwen2.5-Math-7B-Instruct和Gemma2-9B-Instruct模型绝对精度分别提升9.5%、6.5%和6.0%。此外，将CLC框架的语言范围扩展到11种语言可以中和多语言训练语料库中的语言偏见，同时通过探索更广泛的多语言解决方案空间，避免了单一语言的推理陷阱。这有助于找到相比单一语言自一致性基准测试更为全局优化的推理路径，如在MGSM数据集上，Gemma2-9B-Instruct模型的精度提高了4.1%~18.5%。</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>Chain-of-thought (CoT)增强大型语言模型（LLM）的推理能力。</li>
<li>多语言训练语料库中的语言偏见会导致语义漂移和逻辑不一致。</li>
<li>Cross-Lingual Consistency (CLC)框架通过多数投票整合多语言推理路径，提升LLM的推理能力。</li>
<li>CLC框架在CMATH数据集上的实证评估表现优越，带来明显的精度提升。</li>
<li>CLC框架扩展到11种语言可以中和语言偏见，避免单一语言的推理陷阱。</li>
<li>多语言解决方案空间探索有助于找到更为全局优化的推理路径。</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.01857">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-ae9bdf4b23091f2c7b4ca0b9c70d22c3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8b817bac65b02e0cf5debe8e6595637d.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="LARGE-Legal-Retrieval-Augmented-Generation-Evaluation-Tool"><a href="#LARGE-Legal-Retrieval-Augmented-Generation-Evaluation-Tool" class="headerlink" title="LARGE: Legal Retrieval Augmented Generation Evaluation Tool"></a>LARGE: Legal Retrieval Augmented Generation Evaluation Tool</h2><p><strong>Authors:Minhu Park, Hongseok Oh, Eunkyung Choi, Wonseok Hwang</strong></p>
<p>Recently, building retrieval-augmented generation (RAG) systems to enhance the capability of large language models (LLMs) has become a common practice. Especially in the legal domain, previous judicial decisions play a significant role under the doctrine of stare decisis which emphasizes the importance of making decisions based on (retrieved) prior documents. However, the overall performance of RAG system depends on many components: (1) retrieval corpora, (2) retrieval algorithms, (3) rerankers, (4) LLM backbones, and (5) evaluation metrics. Here we propose LRAGE, an open-source tool for holistic evaluation of RAG systems focusing on the legal domain. LRAGE provides GUI and CLI interfaces to facilitate seamless experiments and investigate how changes in the aforementioned five components affect the overall accuracy. We validated LRAGE using multilingual legal benches including Korean (KBL), English (LegalBench), and Chinese (LawBench) by demonstrating how the overall accuracy changes when varying the five components mentioned above. The source code is available at <a target="_blank" rel="noopener" href="https://github.com/hoorangyee/LRAGE">https://github.com/hoorangyee/LRAGE</a>. </p>
<blockquote>
<p>最近，为了增强大型语言模型（LLM）的能力，构建增强检索生成（RAG）系统已成为一种常见做法。特别是在法律领域，根据遵循先例原则，先前的司法判决在决策中扮演着重要角色，该原则强调基于（检索到的）先前文档进行决策的重要性。然而，RAG系统的整体性能取决于多个组件：（1）检索语料库、（2）检索算法、（3）重新排序器、（4）LLM主干和（5）评估指标。在这里，我们提出了LRAGE，这是一个专注于法律领域的RAG系统的整体评估的开源工具。LRAGE提供了GUI和CLI接口，以进行无缝实验并研究上述五个组件的变化如何影响总体准确性。我们使用包括韩语（KBL）、英语（LegalBench）和中文（LawBench）在内的多语言法律基准测试验证了LRAGE，通过展示上述五个组件变化时总体准确性的变化来验证。源代码可在<a target="_blank" rel="noopener" href="https://github.com/hoorangyee/LRAGE">https://github.com/hoorangyee/LRAGE</a>获得。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.01840v1">PDF</a> 12 pages</p>
<p><strong>Summary</strong></p>
<p>本文介绍了如何构建增强检索生成（RAG）系统以提升大型语言模型（LLM）的能力，特别是在法律领域中，以往司法决策在遵循先例原则下具有重要意义。文章提出了LRAGE这一开源工具，用于全面评估RAG系统，并关注法律领域。LRAGE提供GUI和CLI接口，方便进行实验并研究五个关键组件的变化对整体准确率的影响。经过包括韩语（KBL）、英语（LegalBench）和中文（LawBench）在内的多语种法律基准测试验证，展示了LRAGE的有效性。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>RAG系统通过增强大型语言模型（LLM）的能力，在法律领域中尤为重要，遵循先例原则使得以往司法决策具有关键参考价值。</li>
<li>LRAGE是一个用于全面评估RAG系统的开源工具，提供GUI和CLI接口以方便实验和性能分析。</li>
<li>LRAGE评估重点包括五个关键组件：检索语料库、检索算法、排序器、LLM主干和评估指标。</li>
<li>通过多语种法律基准测试验证，LRAGE展示了其有效性和实用性。</li>
<li>LRAGE能展示不同组件变化对整体准确率的影响，为优化RAG系统提供有力支持。</li>
<li>该工具源代码已公开，方便进一步开发和改进。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.01840">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-542ac136d895ae0f32b5875f8f282612.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-43a323bd9fe17304d80782c925c25a49.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-173bf64026ff1d75fdfec6f50aecb04b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3747e55b40f40efb0afb6a456be918ba.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ec6d5ef03ca30b015b16e9527f598277.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="YourBench-Easy-Custom-Evaluation-Sets-for-Everyone"><a href="#YourBench-Easy-Custom-Evaluation-Sets-for-Everyone" class="headerlink" title="YourBench: Easy Custom Evaluation Sets for Everyone"></a>YourBench: Easy Custom Evaluation Sets for Everyone</h2><p><strong>Authors:Sumuk Shashidhar, Clémentine Fourrier, Alina Lozovskia, Thomas Wolf, Gokhan Tur, Dilek Hakkani-Tür</strong></p>
<p>Evaluating large language models (LLMs) effectively remains a critical bottleneck, as traditional static benchmarks suffer from saturation and contamination, while human evaluations are costly and slow. This hinders timely or domain-specific assessment, crucial for real-world applications. We introduce YourBench, a novel, open-source framework that addresses these limitations by enabling dynamic, automated generation of reliable, up-to-date, and domain-tailored benchmarks cheaply and without manual annotation, directly from user-provided documents. We demonstrate its efficacy by replicating 7 diverse MMLU subsets using minimal source text, achieving this for under 15 USD in total inference costs while perfectly preserving the relative model performance rankings (Spearman Rho &#x3D; 1) observed on the original benchmark. To ensure that YourBench generates data grounded in provided input instead of relying on posterior parametric knowledge in models, we also introduce Tempora-0325, a novel dataset of over 7K diverse documents, published exclusively after March 2025. Our comprehensive analysis spans 26 SoTA models from 7 major families across varying scales (3-671B parameters) to validate the quality of generated evaluations through rigorous algorithmic checks (e.g., citation grounding) and human assessments. We release the YourBench library, the Tempora-0325 dataset, 150k+ question answer pairs based on Tempora and all evaluation and inference traces to facilitate reproducible research and empower the community to generate bespoke benchmarks on demand, fostering more relevant and trustworthy LLM evaluation. </p>
<blockquote>
<p>评估大型语言模型（LLM）仍然是一个关键的瓶颈，因为传统的静态基准测试饱受饱和和污染之苦，而人工评估则成本高昂且速度慢。这阻碍了针对特定时间或特定领域的评估，对于现实世界的应用至关重要。我们推出了YourBench，这是一个新颖的开源框架，通过动态自动化生成可靠、最新且针对特定领域的基准测试，以低廉的成本而无需手动注释，直接从用户提供的文档中进行评估，从而解决了这些限制。我们通过复制使用最少源文本的7个多样化的MMLU子集来展示其有效性，在总共的推理成本不到15美元的情况下，完美地保持了原始基准测试上观察到的相对模型性能排名（斯皮尔曼等级ρ&#x3D;1）。为了确保YourBench生成的数据基于提供的输入而不是依赖模型中的事后参数知识，我们还推出了Tempora-0325数据集，这是一组超过7,000个多样化的文档的新数据集，仅在2025年3月之后发布。我们的综合分析涵盖了来自7个主要家族的26个最新模型，跨越了不同的规模（3亿至671亿参数），以验证生成评估的质量通过严格的算法检查（例如引文定位）和人工评估。我们发布了YourBench库、Tempora-0325数据集、基于Tempora的超过15万个问题答案对以及所有评估和推理痕迹，以促进可重复的研究并赋予社区按需生成定制基准测试的能力，从而推动更相关和可靠的大型语言模型评估。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.01833v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文介绍了对传统评估大型语言模型（LLM）方法存在的问题，包括饱和和污染等问题。为了解决这些问题，作者提出了一种新型的开源框架YourBench，该框架可以动态地自动生成可靠、最新、针对特定领域的基准测试，并且具有成本低、无需人工标注等优点。作者通过复制多个MMLU子集验证了其有效性，同时引入了一个新型数据集Tempora-0325来确保生成的基准测试具有实际应用价值。此外，作者还对多个最新的大型语言模型进行了全面的评估，并通过严格的算法检查和人工评估验证了生成评估的质量。最后，作者释放了YourBench库、Tempora-0325数据集和其他资源，以促进研究人员的可重复性研究和社区生成定制化的基准测试需求。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>传统的大型语言模型（LLM）评估方法存在饱和和污染问题。</li>
<li>YourBench是一个新型的开源框架，可以动态地自动生成可靠、最新、针对特定领域的基准测试。</li>
<li>YourBench能够降低成本并加速评估过程，同时无需人工标注。</li>
<li>通过复制MMLU子集验证了YourBench的有效性。</li>
<li>Tempora-0325数据集用于确保生成的基准测试具有实际应用价值。</li>
<li>作者对多个最新的大型语言模型进行了全面的评估。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.01833">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-7cfe6cc91d997a1052ed939bc8a54bca.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b4bbea2b75ec3d981ad5ea45b64b8005.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="Leveraging-Embedding-Techniques-in-Multimodal-Machine-Learning-for-Mental-Illness-Assessment"><a href="#Leveraging-Embedding-Techniques-in-Multimodal-Machine-Learning-for-Mental-Illness-Assessment" class="headerlink" title="Leveraging Embedding Techniques in Multimodal Machine Learning for   Mental Illness Assessment"></a>Leveraging Embedding Techniques in Multimodal Machine Learning for   Mental Illness Assessment</h2><p><strong>Authors:Abdelrahaman A. Hassan, Abdelrahman A. Ali, Aya E. Fouda, Radwa J. Hanafy, Mohammed E. Fouda</strong></p>
<p>The increasing global prevalence of mental disorders, such as depression and PTSD, requires objective and scalable diagnostic tools. Traditional clinical assessments often face limitations in accessibility, objectivity, and consistency. This paper investigates the potential of multimodal machine learning to address these challenges, leveraging the complementary information available in text, audio, and video data. Our approach involves a comprehensive analysis of various data preprocessing techniques, including novel chunking and utterance-based formatting strategies. We systematically evaluate a range of state-of-the-art embedding models for each modality and employ Convolutional Neural Networks (CNNs) and Bidirectional LSTM Networks (BiLSTMs) for feature extraction. We explore data-level, feature-level, and decision-level fusion techniques, including a novel integration of Large Language Model (LLM) predictions. We also investigate the impact of replacing Multilayer Perceptron classifiers with Support Vector Machines. We extend our analysis to severity prediction using PHQ-8 and PCL-C scores and multi-class classification (considering co-occurring conditions). Our results demonstrate that utterance-based chunking significantly improves performance, particularly for text and audio modalities. Decision-level fusion, incorporating LLM predictions, achieves the highest accuracy, with a balanced accuracy of 94.8% for depression and 96.2% for PTSD detection. The combination of CNN-BiLSTM architectures with utterance-level chunking, coupled with the integration of external LLM, provides a powerful and nuanced approach to the detection and assessment of mental health conditions. Our findings highlight the potential of MMML for developing more accurate, accessible, and personalized mental healthcare tools. </p>
<blockquote>
<p>随着抑郁症和创伤后应激障碍等精神疾病的全球发病率不断增加，我们需要客观且可规模化的诊断工具。传统的临床评估在可及性、客观性和一致性方面经常面临局限性。本文研究了多模态机器学习在应对这些挑战方面的潜力，利用文本、音频和视频数据中的互补信息。我们的方法涉及对各种数据预处理技术的综合分析，包括新颖的基于分块和发言的格式化策略。我们系统地评估了每种模态的一系列最新嵌入模型，并使用卷积神经网络（CNN）和双向LSTM网络（BiLSTMs）进行特征提取。我们探索了数据层、特征层和决策层的融合技术，包括大型语言模型（LLM）预测的新颖集成。我们还研究了将多层感知器分类器替换为支持向量机的影响。我们将分析扩展到严重性预测，使用PHQ-8和PCL-C分数和多类分类（考虑共病情况）。我们的结果表明，基于发言的分块显著改善性能，特别是对文本和音频模态而言。决策级融合，结合LLM预测，达到了最高精度，抑郁症和创伤后应激障碍检测的平衡精度分别为94.8%和96.2%。结合CNN-BiLSTM架构的发言级分块，以及LLM的集成，为精神健康条件的检测和评估提供了一种强大而细致的方法。我们的研究结果表明多模态机器学习在开发更准确、更可及、更个性化的精神卫生工具方面的潜力。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.01767v1">PDF</a> </p>
<p><strong>Summary</strong><br>    本文探讨了多模态机器学习（MMML）在精神健康诊断中的应用潜力，特别是针对抑郁症和PTSD等精神疾病的诊断。研究通过文本、音频和视频数据的多模态融合，采用多种数据预处理技术、嵌入模型、卷积神经网络（CNN）、双向LSTM网络（BiLSTMs）等，进行系统性的实验评估。决策级融合结合LLM预测，实现了高准确率的精神疾病检测与评估。该研究为开发更准确、可访问和个性化的精神卫生工具提供了有力支持。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>全球精神疾病的普及率增长，需要客观和可规模化的诊断工具。</li>
<li>传统临床评估在可及性、客观性和一致性方面存在局限性。</li>
<li>多模态机器学习（MMML）具有解决这些挑战的巨大潜力。</li>
<li>文本、音频和视频数据的融合有助于提高诊断准确性。</li>
<li>utterance-based chunking技术在文本和音频模态上表现尤为出色。</li>
<li>结合CNN-BiLSTM架构与LLM的决策级融合达到最高准确率。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.01767">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-de1284a8b579cffc56eefa6c95d2ea71.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="TransforMerger-Transformer-based-Voice-Gesture-Fusion-for-Robust-Human-Robot-Communication"><a href="#TransforMerger-Transformer-based-Voice-Gesture-Fusion-for-Robust-Human-Robot-Communication" class="headerlink" title="TransforMerger: Transformer-based Voice-Gesture Fusion for Robust   Human-Robot Communication"></a>TransforMerger: Transformer-based Voice-Gesture Fusion for Robust   Human-Robot Communication</h2><p><strong>Authors:Petr Vanc, Karla Stepanova</strong></p>
<p>As human-robot collaboration advances, natural and flexible communication methods are essential for effective robot control. Traditional methods relying on a single modality or rigid rules struggle with noisy or misaligned data as well as with object descriptions that do not perfectly fit the predefined object names (e.g. ‘Pick that red object’). We introduce TransforMerger, a transformer-based reasoning model that infers a structured action command for robotic manipulation based on fused voice and gesture inputs. Our approach merges multimodal data into a single unified sentence, which is then processed by the language model. We employ probabilistic embeddings to handle uncertainty and we integrate contextual scene understanding to resolve ambiguous references (e.g., gestures pointing to multiple objects or vague verbal cues like “this”). We evaluate TransforMerger in simulated and real-world experiments, demonstrating its robustness to noise, misalignment, and missing information. Our results show that TransforMerger outperforms deterministic baselines, especially in scenarios requiring more contextual knowledge, enabling more robust and flexible human-robot communication. Code and datasets are available at: <a target="_blank" rel="noopener" href="http://imitrob.ciirc.cvut.cz/publications/transformerger">http://imitrob.ciirc.cvut.cz/publications/transformerger</a>. </p>
<blockquote>
<p>随着人机协作的不断发展，自然且灵活的沟通方式对于实现有效的机器人控制至关重要。依赖单一模式或僵化规则的传统方法在处理嘈杂或错位的数据以及不符合预定义对象名称的对象描述（例如“拿起那个红色的物体”）时遇到了困难。我们推出了TransforMerger，这是一款基于变压器的推理模型，它可以根据融合的语音和手势输入推断出用于机器人操作的结构化动作命令。我们的方法将多模式数据合并成单个统一的句子，然后由语言模型进行处理。我们采用概率嵌入来处理不确定性，并整合场景上下文理解来解决模糊引用（例如，指向多个对象的手势或模糊的口头线索，如“这个”）。我们在模拟和真实世界的实验中评估了TransforMerger，证明了它对噪声、错位和缺失信息的稳健性。我们的结果表明，TransforMerger在需要更多上下文知识的场景中表现优于确定性基线，能够实现更稳健和灵活的人机沟通。相关代码和数据集可通过以下网址获取：<a target="_blank" rel="noopener" href="http://imitrob.ciirc.cvut.cz/publications/transformerger">http://imitrob.ciirc.cvut.cz/publications/transformerger</a> 。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.01708v1">PDF</a> 8 pages, 7 figures</p>
<p><strong>Summary</strong><br>在人机协作日益发展的背景下，自然灵活的交流方式对机器人控制至关重要。传统方法在处理噪声或错位数据以及对象描述与预设名称不匹配的问题时显得捉襟见肘。我们推出了基于Transformer的推理模型TransforMerger，通过融合语音和手势输入，来推断用于机器人操控的结构化动作指令。该方法将多模式数据合并成统一语句，并由语言模型处理。我们采用概率嵌入来处理不确定性，并整合场景上下文理解来解决模糊参照问题。在模拟和真实环境中，TransforMerger展现出对噪声、错位和缺失信息的稳健性，并在需要更多上下文知识的场景中表现出超越确定性基准的性能，从而实现更稳健和灵活的人机交流。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>随着人机协作的进步，自然灵活的交流方式对于机器人控制至关重要。</li>
<li>传统方法在处理噪声或错位数据以及对象描述与预设名称不匹配时存在挑战。</li>
<li>TransforMerger是一个基于Transformer的推理模型，能融合语音和手势输入来推断结构化动作指令。</li>
<li>TransforMerger将多模式数据合并成统一语句，并由语言模型处理。</li>
<li>该模型采用概率嵌入处理不确定性，并整合场景上下文理解以解决模糊参照问题。</li>
<li>TransforMerger在模拟和真实环境中展现出对噪声、错位和缺失信息的稳健性。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.01708">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-7d9ba578142da10ab1bae8ed4f7cb36f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-67c87a7977d617801b4a2ad163fae0a6.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-f091bb220138be6d742d3c7d57ebd5e9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b1ea0a2e0fe7ccb62d3a1083b786ed38.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="Q-Adapt-Adapting-LMM-for-Visual-Quality-Assessment-with-Progressive-Instruction-Tuning"><a href="#Q-Adapt-Adapting-LMM-for-Visual-Quality-Assessment-with-Progressive-Instruction-Tuning" class="headerlink" title="Q-Adapt: Adapting LMM for Visual Quality Assessment with Progressive   Instruction Tuning"></a>Q-Adapt: Adapting LMM for Visual Quality Assessment with Progressive   Instruction Tuning</h2><p><strong>Authors:Yiting Lu, Xin Li, Haoning Wu, Bingchen Li, Weisi Lin, Zhibo Chen</strong></p>
<p>The rapid advancement of Large Multi-modal Foundation Models (LMM) has paved the way for the possible Explainable Image Quality Assessment (EIQA) with instruction tuning from two perspectives: overall quality explanation, and attribute-wise perception answering. However, existing works usually overlooked the conflicts between these two types of perception explanations during joint instruction tuning, leading to insufficient perception understanding. To mitigate this, we propose a new paradigm for perception-oriented instruction tuning, i.e., Q-Adapt, which aims to eliminate the conflicts and achieve the synergy between these two EIQA tasks when adapting LMM, resulting in enhanced multi-faceted explanations of IQA. Particularly, we propose a progressive instruction tuning strategy by dividing the adaption process of LMM for EIQA into two stages, where the first stage empowers the LMM with universal perception knowledge tailored for two tasks using an efficient transfer learning strategy, i.e., LoRA, and the second stage introduces the instruction-adaptive visual prompt tuning to dynamically adapt visual features for the different instructions from two tasks. In this way, our proposed Q-Adapt can achieve a lightweight visual quality evaluator, demonstrating comparable performance and, in some instances, superior results across perceptual-related benchmarks and commonly-used IQA databases. The source code is publicly available at <a target="_blank" rel="noopener" href="https://github.com/yeppp27/Q-Adapt">https://github.com/yeppp27/Q-Adapt</a>. </p>
<blockquote>
<p>大型多模态基础模型（LMM）的快速发展为可能的可解释性图像质量评估（EIQA）铺平了道路，通过两个方面的指令调整来实现整体质量解释和属性感知回答。然而，现有工作通常在联合指令调整时忽略了这两种感知解释之间的冲突，导致感知理解不足。为了缓解这一问题，我们提出了一种面向感知的指令调整新范式，即Q-Adapt。其目的是在适应LMM时消除这两种EIQA任务之间的冲突，实现协同作用，从而增强IQA的多方面解释。特别是，我们提出了一种渐进的指令调整策略，将LMM对EIQA的适应过程分为两个阶段。在第一阶段，我们使用高效的迁移学习策略LoRA，赋予LMM针对这两个任务的通用感知知识；在第二阶段，我们引入指令适应性视觉提示调整，以根据不同任务的指令动态适应视觉特征。通过这种方式，我们提出的Q-Adapt可以实现轻量级视觉质量评估器，在感知相关基准测试和常用的IQA数据库上表现出相当的性能，并在某些情况下表现更优秀。源代码可在<a target="_blank" rel="noopener" href="https://github.com/yeppp27/Q-Adapt%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/yeppp27/Q-Adapt找到。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.01655v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>大型多模态基础模型的快速发展，为基于指令调参的可解释图像质量评估（EIQA）提供了可能。现有方法在联合指令调参时忽视了整体质量与属性感知解释之间的冲突，导致感知理解不足。为此，我们提出了面向感知的指令调参新模式——Q-Adapt，旨在消除这两种EIQA任务间的冲突，实现协同适应LMM，增强图像质量评估的多方面解释。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>大型多模态基础模型的进步为EIQA的指令调参提供了机会。</li>
<li>现有方法在联合指令调参时忽视整体质量与属性感知解释之间的冲突。</li>
<li>Q-Adapt旨在消除这种冲突，实现两种EIQA任务的协同适应。</li>
<li>Q-Adapt采用分阶段指令调参策略，第一阶段赋予LMM针对两个任务通用感知知识，第二阶段根据不同指令动态调整视觉特征。</li>
<li>Q-Adapt能实现轻量级视觉质量评估，在感知相关基准测试和常用IQA数据库上表现优异。</li>
<li>Q-Adapt的源代码已公开在<a target="_blank" rel="noopener" href="https://github.com/yeppp27/Q-Adapt%E3%80%82">https://github.com/yeppp27/Q-Adapt。</a></li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.01655">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-e673eac368ee8357e01903864dfaf4db.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0f187acbe4cc287850a959b55d3d6ccf.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-91681af32fe8059583475d81ab36a862.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6e8c91388a63167f835f4420666613dd.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="COST-Contrastive-One-Stage-Transformer-for-Vision-Language-Small-Object-Tracking"><a href="#COST-Contrastive-One-Stage-Transformer-for-Vision-Language-Small-Object-Tracking" class="headerlink" title="COST: Contrastive One-Stage Transformer for Vision-Language Small Object   Tracking"></a>COST: Contrastive One-Stage Transformer for Vision-Language Small Object   Tracking</h2><p><strong>Authors:Chunhui Zhang, Li Liu, Jialin Gao, Xin Sun, Hao Wen, Xi Zhou, Shiming Ge, Yanfeng Wang</strong></p>
<p>Transformer has recently demonstrated great potential in improving vision-language (VL) tracking algorithms. However, most of the existing VL trackers rely on carefully designed mechanisms to perform the multi-stage multi-modal fusion. Additionally, direct multi-modal fusion without alignment ignores distribution discrepancy between modalities in feature space, potentially leading to suboptimal representations. In this work, we propose COST, a contrastive one-stage transformer fusion framework for VL tracking, aiming to learn semantically consistent and unified VL representations. Specifically, we introduce a contrastive alignment strategy that maximizes mutual information (MI) between a video and its corresponding language description. This enables effective cross-modal alignment, yielding semantically consistent features in the representation space. By leveraging a visual-linguistic transformer, we establish an efficient multi-modal fusion and reasoning mechanism, empirically demonstrating that a simple stack of transformer encoders effectively enables unified VL representations. Moreover, we contribute a newly collected VL tracking benchmark dataset for small object tracking, named VL-SOT500, with bounding boxes and language descriptions. Our dataset comprises two challenging subsets, VL-SOT230 and VL-SOT270, dedicated to evaluating generic and high-speed small object tracking, respectively. Small object tracking is notoriously challenging due to weak appearance and limited features, and this dataset is, to the best of our knowledge, the first to explore the usage of language cues to enhance visual representation for small object tracking. Extensive experiments demonstrate that COST achieves state-of-the-art performance on five existing VL tracking datasets, as well as on our proposed VL-SOT500 dataset. Source codes and dataset will be made publicly available. </p>
<blockquote>
<p>Transformer在改进视觉语言（VL）跟踪算法方面表现出了巨大的潜力。然而，大多数现有的VL跟踪器都依赖于精心设计的机制来执行多阶段多模式融合。另外，没有对齐的直接多模式融合忽略了不同模式之间特征空间的分布差异，可能导致表示不佳。在本研究中，我们提出了针对VL跟踪的对比一阶Transformer融合框架COST，旨在学习语义一致且统一的VL表示。具体来说，我们引入了一种对比对齐策略，以最大化视频与其相应的语言描述之间的互信息（MI）。这实现了有效的跨模式对齐，在表示空间中产生语义一致的特征。通过利用视觉语言Transformer，我们建立了高效的多模式融合和推理机制，实证表明，简单的Transformer编码器堆栈可以有效地实现统一的VL表示。此外，我们收集了一个名为VL-SOT500的VL跟踪基准数据集，用于小目标跟踪，其中包含边界框和语言描述。我们的数据集包括两个具有挑战性的子集，即VL-SOT230和VL-SOT270，专门用于评估通用和高速小目标跟踪。由于小目标的外貌较弱且特征有限，小目标跟踪是众所周知的挑战，据我们所知，该数据集是第一个探索使用语言线索来提高小目标跟踪的视觉表示的数据集。大量实验表明，COST在五个现有的VL跟踪数据集以及我们提出的VL-SOT500数据集上达到了最新性能。源代码和数据集将公开发布。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.01321v1">PDF</a> Preprint submitted to Elsevier.   <a target="_blank" rel="noopener" href="https://github.com/983632847/Awesome-Multimodal-Object-Tracking">https://github.com/983632847/Awesome-Multimodal-Object-Tracking</a></p>
<p><strong>摘要</strong></p>
<p>本文提出一种基于对比学习的一站式Transformer融合框架（COST），用于视觉语言（VL）跟踪任务。该框架旨在学习语义一致且统一的VL表示，引入对比对齐策略来最大化视频与其对应的语言描述之间的互信息（MI），从而实现有效的跨模态对齐，在表示空间中产生语义一致的特征。利用视觉语言Transformer，建立了高效的多模态融合和推理机制，实证表明简单的Transformer编码器堆叠可有效实现统一的VL表示。此外，本文贡献了一个新收集的VL跟踪基准数据集VL-SOT500，用于小目标跟踪，包括边界框和语言描述。该数据集包含两个具有挑战性的子集VL-SOT230和VL-SOT270，分别用于评估通用高速小目标跟踪。广泛实验表明，COST在五个现有的VL跟踪数据集以及所提出的VL-SOT500数据集上均达到最新性能。</p>
<p><strong>关键见解</strong></p>
<ol>
<li>提出了基于对比学习的一站式Transformer融合框架（COST）用于视觉语言跟踪任务。</li>
<li>引入对比对齐策略来最大化视频与语言描述之间的互信息，实现跨模态有效对齐。</li>
<li>利用视觉语言Transformer实现多模态融合和推理机制。</li>
<li>提出一个新的VL跟踪基准数据集VL-SOT500，专注于小目标跟踪。</li>
<li>数据集包含VL-SOT230和VL-SOT270两个具有挑战性的子集，分别用于评估通用和高速小目标跟踪。</li>
<li>COST框架在多个数据集上实现了最先进的性能。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.01321">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-2464c33d2475767ad1eb3661aced83c5.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-704574aae0f1bf20472bf105f8ad2a65.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ec10c6ed22bbb1303f949cd58fad5e14.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-db451990559e00474daa9cd61e85e404.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ba2725e8dcff19721c2fbc476b656596.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-e6cf49eed838c4565423b06664e6ffa2.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d6d7f6361951adf776b1afd931d1c9bd.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="Bi-LAT-Bilateral-Control-Based-Imitation-Learning-via-Natural-Language-and-Action-Chunking-with-Transformers"><a href="#Bi-LAT-Bilateral-Control-Based-Imitation-Learning-via-Natural-Language-and-Action-Chunking-with-Transformers" class="headerlink" title="Bi-LAT: Bilateral Control-Based Imitation Learning via Natural Language   and Action Chunking with Transformers"></a>Bi-LAT: Bilateral Control-Based Imitation Learning via Natural Language   and Action Chunking with Transformers</h2><p><strong>Authors:Takumi Kobayashi, Masato Kobayashi, Thanpimon Buamanee, Yuki Uranishi</strong></p>
<p>We present Bi-LAT, a novel imitation learning framework that unifies bilateral control with natural language processing to achieve precise force modulation in robotic manipulation. Bi-LAT leverages joint position, velocity, and torque data from leader-follower teleoperation while also integrating visual and linguistic cues to dynamically adjust applied force. By encoding human instructions such as “softly grasp the cup” or “strongly twist the sponge” through a multimodal Transformer-based model, Bi-LAT learns to distinguish nuanced force requirements in real-world tasks. We demonstrate Bi-LAT’s performance in (1) unimanual cup-stacking scenario where the robot accurately modulates grasp force based on language commands, and (2) bimanual sponge-twisting task that requires coordinated force control. Experimental results show that Bi-LAT effectively reproduces the instructed force levels, particularly when incorporating SigLIP among tested language encoders. Our findings demonstrate the potential of integrating natural language cues into imitation learning, paving the way for more intuitive and adaptive human-robot interaction. For additional material, please visit: <a target="_blank" rel="noopener" href="https://mertcookimg.github.io/bi-lat/">https://mertcookimg.github.io/bi-lat/</a> </p>
<blockquote>
<p>我们提出了Bi-LAT，这是一种新的模仿学习框架，它将双边控制与自然语言处理相结合，实现机器人操作中的精确力度调节。Bi-LAT利用领导者-跟随者遥操作中的关节位置、速度和扭矩数据，同时结合视觉和语言线索来动态调整应用力度。通过多模态Transformer模型编码人类指令，如“轻轻握住杯子”或“用力拧海绵”，Bi-LAT学会了区分现实世界任务中的细微力度要求。我们在（1）单手法堆叠杯子的场景中展示了Bi-LAT的性能，机器人能够根据语言命令准确调节握力，以及在（2）需要协调力度控制的双手拧海绵任务中展示了其性能。实验结果表明，Bi-LAT在测试的语言编码器中融入SigLIP后，能够有效地再现指示的力度水平。我们的研究展示了将自然语言线索融入模仿学习的潜力，为更直观和适应性的人机交互铺平了道路。更多资料请访问：[<a target="_blank" rel="noopener" href="https://mertcookimg.github.io/bi-lat/]">https://mertcookimg.github.io/bi-lat/]</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.01301v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文介绍了Bi-LAT这一新型模仿学习框架，它将双边控制与自然语言处理相结合，实现了机器人操作中的精确力度调节。Bi-LAT利用领导者跟随者的遥操作中的关节位置、速度和扭矩数据，同时整合视觉和语言线索来动态调整应用力度。通过多模态Transformer模型编码人类指令，Bi-LAT能够区分现实任务中的细微力度要求。在单手叠杯和双手扭海绵的实验场景中，Bi-LAT表现出良好的性能，有效复制了指令力度。该研究展示了自然语言线索融入模仿学习的潜力，为更直观和适应性的人机交互铺平了道路。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Bi-LAT是一个结合双边控制与自然语言处理的模仿学习框架，用于机器人操作的精确力度调节。</li>
<li>Bi-LAT利用领导者跟随者的遥操作数据，并整合视觉和语言线索以动态调整力度。</li>
<li>通过多模态Transformer模型编码人类指令，Bi-LAT能区分现实任务中的细微力度要求。</li>
<li>在叠杯和扭海绵的实验场景中，Bi-LAT表现出良好的性能。</li>
<li>Bi-LAT在结合SigLIP语言编码器时，能更有效地复制指令力度。</li>
<li>研究表明，自然语言线索融入模仿学习具有潜力。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.01301">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-b1f77c2725378d59bb268753d972d384.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0abc12fe263921a77ef0c5f5a655d811.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1f98433984e7328ce74b520b785abcde.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-001017459d7afd5367f0a488807bc7ec.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-aee2cd05500d450884f2f16731da17d9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-711a90776358682f1b05a8a5dca5d3e1.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-21e224135314776ef587b2431d61775e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-2e368b57faedfe1f1cc6fbef11b4b8b2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0a2ac4b1bc1b1f3e0ee90bc65e708afc.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="Open-Qwen2VL-Compute-Efficient-Pre-Training-of-Fully-Open-Multimodal-LLMs-on-Academic-Resources"><a href="#Open-Qwen2VL-Compute-Efficient-Pre-Training-of-Fully-Open-Multimodal-LLMs-on-Academic-Resources" class="headerlink" title="Open-Qwen2VL: Compute-Efficient Pre-Training of Fully-Open Multimodal   LLMs on Academic Resources"></a>Open-Qwen2VL: Compute-Efficient Pre-Training of Fully-Open Multimodal   LLMs on Academic Resources</h2><p><strong>Authors:Weizhi Wang, Yu Tian, Linjie Yang, Heng Wang, Xifeng Yan</strong></p>
<p>The reproduction of state-of-the-art multimodal LLM pre-training faces barriers at every stage of the pipeline, including high-quality data filtering, multimodal data mixture strategies, sequence packing techniques, and training frameworks. We introduce Open-Qwen2VL, a fully open-source 2B-parameter Multimodal Large Language Model pre-trained efficiently on 29M image-text pairs using only 220 A100-40G GPU hours. Our approach employs low-to-high dynamic image resolution and multimodal sequence packing to significantly enhance pre-training efficiency. The training dataset was carefully curated using both MLLM-based filtering techniques (e.g., MLM-Filter) and conventional CLIP-based filtering methods, substantially improving data quality and training efficiency. The Open-Qwen2VL pre-training is conducted on academic level 8xA100-40G GPUs at UCSB on 5B packed multimodal tokens, which is 0.36% of 1.4T multimodal pre-training tokens of Qwen2-VL. The final instruction-tuned Open-Qwen2VL outperforms partially-open state-of-the-art MLLM Qwen2-VL-2B on various multimodal benchmarks of MMBench, SEEDBench, MMstar, and MathVista, indicating the remarkable training efficiency of Open-Qwen2VL. We open-source all aspects of our work, including compute-efficient and data-efficient training details, data filtering methods, sequence packing scripts, pre-training data in WebDataset format, FSDP-based training codebase, and both base and instruction-tuned model checkpoints. We redefine “fully open” for multimodal LLMs as the complete release of: 1) the training codebase, 2) detailed data filtering techniques, and 3) all pre-training and supervised fine-tuning data used to develop the model. </p>
<blockquote>
<p>模态多模态大型语言模型预训练的重现面临管道每个阶段的障碍，包括高质量数据过滤、多模态数据混合策略、序列打包技术和训练框架。我们引入Open-Qwen2VL，这是一个完全开源的2B参数多模态大型语言模型，在仅使用29M图像文本对的情况下，使用高效的预训练方法。我们的方法采用从低到高的动态图像分辨率和多模态序列打包，以显著提高预训练效率。训练数据集经过精心筛选，既使用基于MLLM的过滤技术（如MLM-Filter），也使用传统的基于CLIP的过滤方法，大大提高了数据质量和训练效率。Open-Qwen2VL的预训练是在UCSB的学术级8xA100-40G GPU上进行的，处理5B个打包的多模态令牌，这是Qwen2VL的1.4T多模态预训练令牌的0.36%。最终指令调整的Open-Qwen2VL在各种多模态基准测试（如MMBench、SEEDBench、MMstar和MathVista）上优于部分开源的先进MLLM Qwen2-VL-2B，证明了Open-Qwen2VL的显著训练效率。我们开源了工作的各个方面，包括计算高效和数据高效的训练细节、数据过滤方法、序列打包脚本、WebDataset格式的预训练数据、基于FSDP的训练代码库以及基础指令和经过调整的模型检查点。我们重新定义了多模态大型语言模型的“完全开放”，即完全发布：1）训练代码库，2）详细的数据过滤技术，以及3）用于开发模型的所有预训练和监督微调数据。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.00595v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>大规模多模态预训练语言模型（LLM）的复制面临诸多挑战，包括高质量数据过滤、多模态数据混合策略、序列打包技术和训练框架等。我们推出Open-Qwen2VL，这是一个完全开源的2B参数多模态大型语言模型，在只有220个A100-40G GPU小时的情况下，在由精选图像和文本组成的复杂训练集上进行高效训练。通过使用从低到高的动态图像分辨率和多模态序列打包方法，我们显著提高了预训练效率。Open-Qwen2VL在各种多模态基准测试中表现优异，并且我们公开了所有相关的训练细节和数据集等。我们重新定义了多模态LLM的“完全开放”，包括公开训练代码库、详细的数据过滤技术和所有用于开发模型的预训练和监督微调数据。整体来说，这是一个效率出众、开放性高的多模态语言模型。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Open-Qwen2VL是一个开源的多模态大型语言模型，能够在有限的GPU资源下高效训练。</li>
<li>该模型采用了先进的图像分辨率策略和序列打包技术来提升训练效率。</li>
<li>Open-Qwen2VL采用了严格的训练数据集筛选过程，结合了MLLM过滤技术和CLIP过滤方法，提升了数据质量和训练效率。</li>
<li>该模型在各种多模态基准测试中表现优异，优于部分开源的多模态LLM Qwen2-VL-2B。</li>
<li>Open-Qwen2VL实现了真正意义上的开源，包括公开训练代码库、数据过滤技术、预训练数据和训练过程等细节。这为该领域的未来研究提供了重要参考和基础。 </li>
<li>高效的计算和数据利用使其成为研究者和开发者的宝贵资源。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.00595">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-cbc553f14bd356fe5d6ce4548da2cbcd.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3b3e617cdd4eb7e3dfe366eca1cdc5a7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-461103848bc31481129d7aeec36ed8b8.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-ecdb73abe5e9b6b46cffed27e3af2fcc.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-8d0e0a61563d61365b765858d101420b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e72c847c62cfed80798a0b83a7f189fb.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="Any2Caption-Interpreting-Any-Condition-to-Caption-for-Controllable-Video-Generation"><a href="#Any2Caption-Interpreting-Any-Condition-to-Caption-for-Controllable-Video-Generation" class="headerlink" title="Any2Caption:Interpreting Any Condition to Caption for Controllable Video   Generation"></a>Any2Caption:Interpreting Any Condition to Caption for Controllable Video   Generation</h2><p><strong>Authors:Shengqiong Wu, Weicai Ye, Jiahao Wang, Quande Liu, Xintao Wang, Pengfei Wan, Di Zhang, Kun Gai, Shuicheng Yan, Hao Fei, Tat-Seng Chua</strong></p>
<p>To address the bottleneck of accurate user intent interpretation within the current video generation community, we present Any2Caption, a novel framework for controllable video generation under any condition. The key idea is to decouple various condition interpretation steps from the video synthesis step. By leveraging modern multimodal large language models (MLLMs), Any2Caption interprets diverse inputs–text, images, videos, and specialized cues such as region, motion, and camera poses–into dense, structured captions that offer backbone video generators with better guidance. We also introduce Any2CapIns, a large-scale dataset with 337K instances and 407K conditions for any-condition-to-caption instruction tuning. Comprehensive evaluations demonstrate significant improvements of our system in controllability and video quality across various aspects of existing video generation models. Project Page: <a target="_blank" rel="noopener" href="https://sqwu.top/Any2Cap/">https://sqwu.top/Any2Cap/</a> </p>
<blockquote>
<p>针对当前视频生成社区中准确理解用户意图的瓶颈问题，我们提出了Any2Caption，这是一个新型的可控视频生成框架，可在任何条件下进行视频生成。关键思想是将各种条件解读步骤与视频合成步骤解耦。通过利用现代的多模态大型语言模型（MLLMs），Any2Caption能够解释各种输入，包括文本、图像、视频以及专业提示（如区域、运动和相机姿态），并将其转化为密集的结构化字幕，为视频生成器提供更好的指导。我们还介绍了Any2CapIns，这是一个大规模数据集，包含337K个实例和407K个条件，用于任何条件到字幕的指令调整。综合评估表明，我们的系统在可控性和视频质量方面对现有视频生成模型的各种方面都有显著提高。项目页面：<a target="_blank" rel="noopener" href="https://sqwu.top/Any2Cap/">https://sqwu.top/Any2Cap/</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.24379v1">PDF</a> Project Page: <a target="_blank" rel="noopener" href="https://sqwu.top/Any2Cap/">https://sqwu.top/Any2Cap/</a></p>
<p><strong>Summary</strong></p>
<p>Any2Caption是一个针对任何条件下的可控视频生成的新型框架，它通过解耦各种条件解读步骤与视频合成步骤，利用现代的多模态大型语言模型（MLLMs）来解读多样化的输入，如文本、图像、视频以及专业线索（如区域、动作和相机姿态），并将其转化为密集的结构化字幕，为视频生成器提供更好的指导。此外，还介绍了包含337K实例和407K条件的Any2CapIns大规模数据集，用于任何条件到字幕的指令调整。评估表明，该系统在可控性和视频质量方面均有显著提高。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Any2Caption框架解决了当前视频生成社区中准确用户意图解读的瓶颈。</li>
<li>该框架利用现代的多模态大型语言模型（MLLMs）来解读多样化的输入，包括文本、图像、视频和专业线索。</li>
<li>Any2Caption通过将条件解读步骤与视频合成步骤解耦，实现可控视频生成。</li>
<li>Any2CapIns是一个大规模数据集，包含用于任何条件到字幕指令调整的数据。</li>
<li>Any2Caption框架将各种输入转化为结构化字幕，为视频生成器提供更好的指导。</li>
<li>综合评估表明，Any2Caption系统在可控性和视频质量方面较现有视频生成模型有显著提高。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.24379">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-1cfb184562ecc513c68e4bde9a549f77.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-bf794d63b289934c5289489a3b5fbbf3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2c7418ade9529bb775315141cf275abb.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f7af8400b22de496bcbf2feabe31f6ed.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-0864348ae0357420605d3fad69de2e40.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="FakeScope-Large-Multimodal-Expert-Model-for-Transparent-AI-Generated-Image-Forensics"><a href="#FakeScope-Large-Multimodal-Expert-Model-for-Transparent-AI-Generated-Image-Forensics" class="headerlink" title="FakeScope: Large Multimodal Expert Model for Transparent AI-Generated   Image Forensics"></a>FakeScope: Large Multimodal Expert Model for Transparent AI-Generated   Image Forensics</h2><p><strong>Authors:Yixuan Li, Yu Tian, Yipo Huang, Wei Lu, Shiqi Wang, Weisi Lin, Anderson Rocha</strong></p>
<p>The rapid and unrestrained advancement of generative artificial intelligence (AI) presents a double-edged sword: while enabling unprecedented creativity, it also facilitates the generation of highly convincing deceptive content, undermining societal trust. As image generation techniques become increasingly sophisticated, detecting synthetic images is no longer just a binary task: it necessitates interpretable, context-aware methodologies that enhance trustworthiness and transparency. However, existing detection models primarily focus on classification, offering limited explanatory insights into image authenticity. In this work, we propose FakeScope, an expert multimodal model (LMM) tailored for AI-generated image forensics, which not only identifies AI-synthetic images with high accuracy but also provides rich, interpretable, and query-driven forensic insights. We first construct FakeChain dataset that contains linguistic authenticity reasoning based on visual trace evidence, developed through a novel human-machine collaborative framework. Building upon it, we further present FakeInstruct, the largest multimodal instruction tuning dataset containing 2 million visual instructions tailored to enhance forensic awareness in LMMs. FakeScope achieves state-of-the-art performance in both closed-ended and open-ended forensic scenarios. It can distinguish synthetic images with high accuracy while offering coherent and insightful explanations, free-form discussions on fine-grained forgery attributes, and actionable enhancement strategies. Notably, despite being trained exclusively on qualitative hard labels, FakeScope demonstrates remarkable zero-shot quantitative capability on detection, enabled by our proposed token-based probability estimation strategy. Furthermore, FakeScope exhibits strong generalization and in-the-wild ability, ensuring its applicability in real-world scenarios. </p>
<blockquote>
<p>人工智能生成技术的迅猛且无约束的发展呈现出一把双刃剑的特性：虽然它能够促进前所未有的创造力，但也方便生成高度欺骗性的内容，从而破坏社会信任。随着图像生成技术的日益成熟，检测合成图像不再仅仅是一个简单的二选一任务：它需要可解释、具备语境意识的方法论来提高可信度和透明度。然而，现有的检测模型主要集中在分类上，对于图像真实性的解释性洞察有限。在这项工作中，我们提出了FakeScope，一个专为AI生成图像取证定制的专家多模式模型（LMM）。它不仅能够高度准确地识别AI合成的图像，而且提供丰富、可解释、查询驱动的取证洞察。我们首先构建了FakeChain数据集，其中包含基于视觉痕迹证据的语言真实性推理，这是通过新型的人机协作框架开发而成的。在此基础上，我们进一步推出了FakeInstruct，这是最大的多模式指令调整数据集，包含200万条旨在提高LMMs取证意识的视觉指令。FakeScope在封闭和开放的取证场景中均实现了最先进的性能。它能够准确地区分合成图像，同时提供连贯而富有洞察力的解释、关于细微伪造属性的自由形式讨论和可行的增强策略。值得注意的是，尽管FakeScope仅使用定性硬标签进行训练，但得益于我们提出的基于标记的概率估计策略，它在检测方面展现出了惊人的零样本定量能力。此外，FakeScope表现出强大的泛化和野外能力，确保其在现实场景中的适用性。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.24267v1">PDF</a> </p>
<p><strong>Summary</strong><br>     人工智能技术的快速发展呈现出一把双刃剑的特性：在激发前所未有的创造力的同时，也易于生成具有高度欺骗性的内容，损害社会信任。针对这一问题，我们提出了一款名为FakeScope的定制化多模态模型（LMM），专门用于人工智能生成的图像取证。该模型不仅能准确识别AI合成的图像，而且提供丰富、可解释、查询驱动的取证信息。为实现此功能，我们构建了FakeChain数据集和FakeInstruct指令调优数据集。FakeScope在封闭和开放环境下的取证场景中均取得了最先进的性能，具备强大的零样本定量检测能力。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>生成式人工智能（AI）的快速发展带来了社会信任的问题，因为AI可以生成高度欺骗性的内容。</li>
<li>现有的检测模型主要侧重于分类，对于图像真实性的解释性有限。</li>
<li>FakeScope是一款专门用于AI生成的图像取证的定制化多模态模型（LMM）。</li>
<li>FakeScope不仅能准确识别AI合成的图像，而且提供丰富、可解释、查询驱动的取证信息。</li>
<li>为训练FakeScope，构建了FakeChain数据集和FakeInstruct指令调优数据集。</li>
<li>FakeScope在封闭和开放环境下的取证场景中均表现优秀，具备强大的性能。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.24267">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-694bdba20e34d207a3c0dbc9db54f26e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-8bd6f7f99f737955e1da167a0c876124.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f74842ca58f90816dd3859a5c8c51e26.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-4d7f04a757a2d8760ea837a36c99b828.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5220b28658718646564e588b6fcba51f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-29097bbc089fcafef3b02be6e3349c2a.jpg" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="BeMERC-Behavior-Aware-MLLM-based-Framework-for-Multimodal-Emotion-Recognition-in-Conversation"><a href="#BeMERC-Behavior-Aware-MLLM-based-Framework-for-Multimodal-Emotion-Recognition-in-Conversation" class="headerlink" title="BeMERC: Behavior-Aware MLLM-based Framework for Multimodal Emotion   Recognition in Conversation"></a>BeMERC: Behavior-Aware MLLM-based Framework for Multimodal Emotion   Recognition in Conversation</h2><p><strong>Authors:Yumeng Fu, Junjie Wu, Zhongjie Wang, Meishan Zhang, Yulin Wu, Bingquan Liu</strong></p>
<p>Multimodal emotion recognition in conversation (MERC), the task of identifying the emotion label for each utterance in a conversation, is vital for developing empathetic machines. Current MLLM-based MERC studies focus mainly on capturing the speaker’s textual or vocal characteristics, but ignore the significance of video-derived behavior information. Different from text and audio inputs, learning videos with rich facial expression, body language and posture, provides emotion trigger signals to the models for more accurate emotion predictions. In this paper, we propose a novel behavior-aware MLLM-based framework (BeMERC) to incorporate speaker’s behaviors, including subtle facial micro-expression, body language and posture, into a vanilla MLLM-based MERC model, thereby facilitating the modeling of emotional dynamics during a conversation. Furthermore, BeMERC adopts a two-stage instruction tuning strategy to extend the model to the conversations scenario for end-to-end training of a MERC predictor. Experiments demonstrate that BeMERC achieves superior performance than the state-of-the-art methods on two benchmark datasets, and also provides a detailed discussion on the significance of video-derived behavior information in MERC. </p>
<blockquote>
<p>多模态情感识别技术（MERC）在对话中识别每个话语的情感标签的任务对于开发共情机器至关重要。当前的基于大型语言模型的MERC研究主要集中在捕捉说话人的文本或语音特征上，而忽略了视频衍生行为信息的重要性。不同于文本和音频输入，学习包含丰富面部表情、肢体语言和姿势的视频为模型提供了情感触发信号，以实现更准确的情感预测。在本文中，我们提出了一种新颖的行为感知大型语言模型框架（BeMERC），将说话人的行为（包括微妙的面部表情、肢体语言和姿势）纳入基本的基于大型语言模型的MERC模型中，从而促进了对话过程中情感动态的建模。此外，BeMERC采用了两阶段指令微调策略，将模型扩展到对话场景，对MERC预测器进行端到端的训练。实验表明，BeMERC在两个基准数据集上的性能优于最新方法，并详细讨论了视频衍生行为信息在MERC中的重要性。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.23990v1">PDF</a> </p>
<p><strong>Summary</strong><br>多模态情感识别对话（MERC）是开发共情机器的重要任务之一，旨在识别对话中每个句子的情感标签。当前基于大型语言模型（LLM）的MERC研究主要关注捕捉说话者的文本或语音特征，但忽略了视频行为信息的重要性。与文本和音频输入不同，视频包含丰富的面部表情、身体语言和姿势，为模型提供了情感触发信号，以实现更准确的情感预测。本文提出了一种新型的行为感知LLM框架（BeMERC），该框架将说话者的行为（包括微妙的面部表情、身体语言和姿势）纳入基本的LLM-based MERC模型中，从而促进对话过程中的情感动态建模。此外，BeMERC采用两阶段指令调整策略，将模型扩展到对话场景，以端到端的方式训练MERC预测器。实验表明，BeMERC在两个基准数据集上的性能优于最先进的方法，并详细讨论了视频行为信息在MERC中的重要性。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>多模态情感识别对话（MERC）对于开发共情机器至关重要。</li>
<li>当前LLM-based MERC研究主要关注文本和语音特征，忽视了视频行为信息的重要性。</li>
<li>视频包含丰富的面部表情、身体语言和姿势，为模型提供情感触发信号，提高情感预测准确性。</li>
<li>BeMERC框架结合了说话者的行为信息（包括微妙的面部表情、身体语言和姿势），以促进对话过程中的情感动态建模。</li>
<li>BeMERC采用两阶段指令调整策略，将模型扩展到对话场景并进行端到端的MERC预测器训练。</li>
<li>实验显示BeMERC在基准数据集上的性能优于现有方法。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.23990">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-0866d551823874ef556899d374f0d081.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-216939f2bff5da162e505b5a9be840c9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5b4dfdd17a5390d7bb50f51566aca1d9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e14bffce7f0ff64d7965aa4d34a1358d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b181770298b00c52f27d026572708383.jpg" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="LLMigrate-Transforming-“Lazy”-Large-Language-Models-into-Efficient-Source-Code-Migrators"><a href="#LLMigrate-Transforming-“Lazy”-Large-Language-Models-into-Efficient-Source-Code-Migrators" class="headerlink" title="LLMigrate: Transforming “Lazy” Large Language Models into Efficient   Source Code Migrators"></a>LLMigrate: Transforming “Lazy” Large Language Models into Efficient   Source Code Migrators</h2><p><strong>Authors:Yuchen Liu, Junhao Hu, Yingdi Shan, Ge Li, Yanzhen Zou, Yihong Dong, Tao Xie</strong></p>
<p>Rewriting C code in Rust provides stronger memory safety, yet migrating large codebases such as the 32-million-line Linux kernel remains challenging. While rule-based translators (e.g., C2Rust) provide accurate yet largely unsafe Rust programs, recent Large Language Model (LLM) approaches produce more idiomatic, safe Rust programs but frequently exhibit “laziness”, omitting significant portions of the target code. To address the issue, in this paper, we present LLMigrate, an LLM-based C-to-Rust translation tool that splits modules into discrete functions, translating them individually, and then reintegrating them. LLMigrate uses static analysis to retain necessary context, pairs GPT-4o (a state-of-the-art LLM) with compiler-driven translation and program-repair techniques for complex core functions, and leverages call-graph-guided translation to ensure consistent interfaces. Evaluations on three representative Linux kernel modules (math, sort, and ramfs) show that LLMigrate requires modifying less than 15% of the target code, significantly outperforming a pure GPT-4o-based migration. </p>
<blockquote>
<p>重写Rust中的C代码可以提供更强的内存安全性，但是迁移像3200万行的Linux内核这样的大型代码库仍然具有挑战性。虽然基于规则的翻译器（例如C2Rust）提供的Rust程序是准确的，但大多是不安全的。最近的大型语言模型（LLM）方法产生的是更地道的、安全的Rust程序，但经常表现出“懒惰”，省略了大量的目标代码。为了解决这一问题，本文介绍了LLMigrate，这是一种基于LLM的C到Rust翻译工具。它将模块分割成离散函数，逐个翻译它们，然后再进行整合。LLMigrate使用静态分析来保留必要的上下文，将最前沿的LLM GPT-4o与编译器驱动的翻译和程序修复技术相结合用于处理复杂的核心功能，并利用调用图指导的翻译来确保一致的接口。对三个代表性的Linux内核模块的评估（数学、排序和ramfs）显示，LLMigrate只需要修改目标代码的不到15%，显著优于纯GPT-4o的迁移。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.23791v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>重写C代码到Rust能提供更强大的内存安全性，但迁移大型代码库如3200万行的Linux内核仍具挑战。规则基础翻译器（如C2Rust）能生成准确但大多不安全的Rust程序，而最近的大型语言模型（LLM）方法能生成更地道的、安全的Rust程序，但常有“懒惰”现象，省略目标代码的重要部分。为解决此问题，本文提出LLMigrate，一个基于LLM的C到Rust翻译工具，它将模块分割成离散函数进行个别翻译，再整合。LLMigrate利用静态分析保留必要的上下文，结合GPT-4o（最先进的大型语言模型）与编译器驱动的翻译和程序修复技术处理复杂核心函数，并利用调用图指导翻译以确保一致的接口。对三个代表性的Linux内核模块（数学、排序和ramfs）的评估显示，LLMigrate只需要修改目标代码的不到15%，显著优于纯GPT-4o的迁移。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLMigrate是一个基于大型语言模型（LLM）的C到Rust翻译工具。</li>
<li>LLMigrate通过分割模块并逐个翻译函数来解决内存安全问题和代码迁移挑战。</li>
<li>LLMigrate使用静态分析来保留必要的上下文信息。</li>
<li>LLMigrate结合了GPT-4o与编译器驱动的翻译和程序修复技术。</li>
<li>LLMigrate利用调用图指导翻译以确保接口的一致性。</li>
<li>在三个代表性Linux内核模块的测试中，LLMigrate显著优于纯GPT-4o的迁移方法。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.23791">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-355daef3be70915c6faad51a10d09bb0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7aff9d808a0000d3cfef17dd85c7b61d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-60b07c60d1f0acfd939bd84bd263448f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2750813b3eb111ca2ee0407fe9434cad.jpg" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1><h2 id="Building-Instruction-Tuning-Datasets-from-Human-Written-Instructions-with-Open-Weight-Large-Language-Models"><a href="#Building-Instruction-Tuning-Datasets-from-Human-Written-Instructions-with-Open-Weight-Large-Language-Models" class="headerlink" title="Building Instruction-Tuning Datasets from Human-Written Instructions   with Open-Weight Large Language Models"></a>Building Instruction-Tuning Datasets from Human-Written Instructions   with Open-Weight Large Language Models</h2><p><strong>Authors:Youmi Ma, Sakae Mizuki, Kazuki Fujii, Taishi Nakamura, Masanari Ohi, Hinari Shimada, Taihei Shiotani, Koshiro Saito, Koki Maeda, Kakeru Hattori, Takumi Okamoto, Shigeki Ishida, Rio Yokota, Hiroya Takamura, Naoaki Okazaki</strong></p>
<p>Instruction tuning is crucial for enabling Large Language Models (LLMs) to solve real-world tasks. Prior work has shown the effectiveness of instruction-tuning data synthesized solely from LLMs, raising a fundamental question: Do we still need human-originated signals for instruction tuning? This work answers the question affirmatively: we build state-of-the-art instruction-tuning datasets sourced from human-written instructions, by simply pairing them with LLM-generated responses. LLMs fine-tuned on our datasets consistently outperform those fine-tuned on existing ones. Our data construction approach can be easily adapted to other languages; we build datasets for Japanese and confirm that LLMs tuned with our data reach state-of-the-art performance. Analyses suggest that instruction-tuning in a new language allows LLMs to follow instructions, while the tuned models exhibit a notable lack of culture-specific knowledge in that language. The datasets and fine-tuned models will be publicly available. Our datasets, synthesized with open-weight LLMs, are openly distributed under permissive licenses, allowing for diverse use cases. </p>
<blockquote>
<p>指令微调对于使大型语言模型（LLM）解决现实世界任务至关重要。先前的工作已经证明了仅通过LLM合成的指令微调数据的有效性，这引发了一个根本问题：我们是否仍然需要人类产生的信号来进行指令微调？这项工作肯定地回答了这个问题：我们通过将人类编写的指令与LLM生成的响应简单配对，建立了最先进的指令微调数据集。在我们数据集上微调的LLM始终优于在现有数据集上微调的LLM。我们的数据构建方法可以轻松地适应其他语言；我们为日语构建了数据集，并确认使用我们的数据调整的LLM达到了最先进的性能。分析表明，在新的语言中进行指令微调可以使LLM遵循指令，而经过调整的模型在该语言的特定文化知识方面表现出明显的缺乏。数据集和经过调整的模型将公开可用。我们的数据集使用公开权重的大型语言模型合成，在许可的许可下公开分发，支持多种用例。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.23714v1">PDF</a> 15 pages, 5 figures</p>
<p><strong>Summary</strong></p>
<p>大型语言模型（LLM）的指令微调对于解决现实任务至关重要。本研究通过结合人类编写的指令与LLM生成的响应，构建了最先进的指令微调数据集。在这些数据集上微调的LLM性能始终优于在现有数据集上微调的模型。本方法的数据构建方式可轻松适应其他语言，并为日语构建了数据集，验证了使用本数据调教的LLM可达到最先进的性能。分析显示，在新语言中进行指令微调使LLM能够遵循指令，而调教的模型在该语言的文化特定知识方面存在明显的缺乏。数据集和调教模型将公开可用。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>指令微调对LLM解决现实任务至关重要。</li>
<li>通过结合人类编写的指令与LLM生成的响应，构建了最先进的指令微调数据集。</li>
<li>在此数据集上微调的LLM性能优于现有数据集上的性能。</li>
<li>数据构建方式可适应其他语言，例如为日语构建了数据集。</li>
<li>使用本数据调教的LLM可达到最先进的性能。</li>
<li>在新语言中进行指令微调使LLM能够遵循指令，但存在文化特定知识的缺乏。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.23714">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-271f876a84e1c148d45235b600e0360d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3dd5cfaedc0b4c1ee61a43019bc9493d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-576243bb6c675f8981be921421718123.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-63960a210eb9ef0d3ad745fa86942a2c.jpg" align="middle">
</details>


<h1 id="-19"><a href="#-19" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        文章作者:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        文章链接:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-04-03/LLM/">https://kedreamix.github.io/Talk2Paper/Paper/2025-04-03/LLM/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        版权声明:
                    </i>
                </span>
                <span class="reprint-info">
                    本博客所有文章除特別声明外，均采用
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    许可协议。转载请注明来源
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/LLM/">
                                    <span class="chip bg-color">LLM</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;上一篇</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-04-03/Agent/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-7550e694fa69b58b370453beee565178.jpg" class="responsive-img" alt="Agent">
                        
                        <span class="card-title">Agent</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Agent 方向最新论文已更新，请持续关注 Update in 2025-04-03  Review, Refine, Repeat Understanding Iterative Decoding of AI Agents   with Dynamic Evaluation and Selection
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-04-03
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Agent/" class="post-category">
                                    Agent
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Agent/">
                        <span class="chip bg-color">Agent</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                下一篇&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-04-03/R1_Reasoning/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-d3ea20c763842611ad3c4ee24851c5f8.jpg" class="responsive-img" alt="R1_Reasoning">
                        
                        <span class="card-title">R1_Reasoning</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            R1_Reasoning 方向最新论文已更新，请持续关注 Update in 2025-04-03  OpenCodeReasoning Advancing Data Distillation for Competitive Coding
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-04-03
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/R1-Reasoning/" class="post-category">
                                    R1_Reasoning
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/R1-Reasoning/">
                        <span class="chip bg-color">R1_Reasoning</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- 代码块功能依赖 -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- 代码语言 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- 代码块复制 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- 代码块收缩 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC 悬浮按钮. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* 修复文章卡片 div 的宽度. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // 切换TOC目录展开收缩的相关操作.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;站点总字数:&nbsp;<span
                        class="white-color">18799.9k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- 运行天数提醒. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // 区分是否有年份.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = '本站已运行 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                daysTip = '本站已運行 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = '本站已运行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = '本站已運行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="关注我的知乎: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">知</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- 搜索遮罩框 -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span>
            <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- 白天和黑夜主题 -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- 回到顶部按钮 -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // 只在桌面版网页启用特效
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- 雪花特效 -->
    

    <!-- 鼠标星星特效 -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--腾讯兔小巢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- 动态标签 -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Σ(っ °Д °;)っ诶，页面崩溃了嘛？", clearTimeout(st)) : (document.title = "φ(゜▽゜*)♪咦，又好了！", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
