<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Vision Transformer">
    <meta name="description" content="Vision Transformer æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-04-04  Prompting Medical Vision-Language Models to Mitigate Diagnosis Bias by   Generating Realistic Dermoscopic Images">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Vision Transformer | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-5fd0720d589f11ae109f58dcbb423620.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Vision Transformer</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Vision-Transformer/">
                                <span class="chip bg-color">Vision Transformer</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Vision-Transformer/" class="post-category">
                                Vision Transformer
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-04-04
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-04-16
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    20.1k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    83 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-04-04-æ›´æ–°"><a href="#2025-04-04-æ›´æ–°" class="headerlink" title="2025-04-04 æ›´æ–°"></a>2025-04-04 æ›´æ–°</h1><h2 id="Prompting-Medical-Vision-Language-Models-to-Mitigate-Diagnosis-Bias-by-Generating-Realistic-Dermoscopic-Images"><a href="#Prompting-Medical-Vision-Language-Models-to-Mitigate-Diagnosis-Bias-by-Generating-Realistic-Dermoscopic-Images" class="headerlink" title="Prompting Medical Vision-Language Models to Mitigate Diagnosis Bias by   Generating Realistic Dermoscopic Images"></a>Prompting Medical Vision-Language Models to Mitigate Diagnosis Bias by   Generating Realistic Dermoscopic Images</h2><p><strong>Authors:Nusrat Munia, Abdullah-Al-Zubaer Imran</strong></p>
<p>Artificial Intelligence (AI) in skin disease diagnosis has improved significantly, but a major concern is that these models frequently show biased performance across subgroups, especially regarding sensitive attributes such as skin color. To address these issues, we propose a novel generative AI-based framework, namely, Dermatology Diffusion Transformer (DermDiT), which leverages text prompts generated via Vision Language Models and multimodal text-image learning to generate new dermoscopic images. We utilize large vision language models to generate accurate and proper prompts for each dermoscopic image which helps to generate synthetic images to improve the representation of underrepresented groups (patient, disease, etc.) in highly imbalanced datasets for clinical diagnoses. Our extensive experimentation showcases the large vision language models providing much more insightful representations, that enable DermDiT to generate high-quality images. Our code is available at <a target="_blank" rel="noopener" href="https://github.com/Munia03/DermDiT">https://github.com/Munia03/DermDiT</a> </p>
<blockquote>
<p>äººå·¥æ™ºèƒ½ï¼ˆAIï¼‰åœ¨çš®è‚¤ç—…è¯Šæ–­ä¸­çš„åº”ç”¨å·²ç»å–å¾—äº†æ˜¾è‘—è¿›æ­¥ï¼Œä½†ä¸€ä¸ªä¸»è¦é—®é¢˜æ˜¯è¿™äº›æ¨¡å‹åœ¨å­ç¾¤ä½“ä¸­çš„è¡¨ç°ç»å¸¸å­˜åœ¨åè§ï¼Œç‰¹åˆ«æ˜¯åœ¨è‚¤è‰²ç­‰æ•æ„Ÿå±æ€§æ–¹é¢ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°å‹çš„åŸºäºç”Ÿæˆå¼äººå·¥æ™ºèƒ½çš„æ¡†æ¶ï¼Œå³çš®è‚¤ç—…æ‰©æ•£è½¬æ¢å™¨ï¼ˆDermDiTï¼‰ã€‚å®ƒåˆ©ç”¨é€šè¿‡è§†è§‰è¯­è¨€æ¨¡å‹ç”Ÿæˆçš„æ–‡æœ¬æç¤ºå’Œè·¨æ¨¡æ€æ–‡æœ¬å›¾åƒå­¦ä¹ æ¥ç”Ÿæˆæ–°çš„çš®è‚¤é•œå›¾åƒã€‚æˆ‘ä»¬åˆ©ç”¨å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹ä¸ºæ¯å¼ çš®è‚¤é•œå›¾åƒç”Ÿæˆå‡†ç¡®æ°å½“çš„æç¤ºï¼Œè¿™æœ‰åŠ©äºç”Ÿæˆåˆæˆå›¾åƒï¼Œä»¥æ”¹å–„é«˜åº¦ä¸å¹³è¡¡æ•°æ®é›†ä¸­ä»£è¡¨æ€§ä¸è¶³çš„ç¾¤ä½“ï¼ˆæ‚£è€…ã€ç–¾ç—…ç­‰ï¼‰çš„ä¸´åºŠè¯Šæ–­è¡¨ç¤ºã€‚æˆ‘ä»¬çš„å¤§é‡å®éªŒè¡¨æ˜ï¼Œå¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹æä¾›äº†æ›´æœ‰æ´å¯ŸåŠ›çš„è¡¨ç¤ºï¼Œä½¿DermDiTèƒ½å¤Ÿç”Ÿæˆé«˜è´¨é‡å›¾åƒã€‚æˆ‘ä»¬çš„ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/Munia03/DermDiT%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/Munia03/DermDiTæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.01838v1">PDF</a> Paper accepted at International Symposium on Biomedical Imaging (ISBI   2025)</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†é’ˆå¯¹çš®è‚¤ç–¾ç—…è¯Šæ–­ä¸­äººå·¥æ™ºèƒ½æ¨¡å‹å­˜åœ¨çš„åè§é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§åŸºäºç”Ÿæˆå¼äººå·¥æ™ºèƒ½çš„æ–°å‹æ¡†æ¶â€”â€”çš®è‚¤ç—…æ‰©æ•£è½¬æ¢å™¨ï¼ˆDermDiTï¼‰ã€‚è¯¥æ¡†æ¶åˆ©ç”¨è§†è§‰è¯­è¨€æ¨¡å‹ç”Ÿæˆæ–‡æœ¬æç¤ºï¼Œå¹¶é€šè¿‡å¤šæ¨¡æ€æ–‡æœ¬å›¾åƒå­¦ä¹ ç”Ÿæˆæ–°çš„çš®è‚¤é•œå›¾åƒã€‚DermDiTé€šè¿‡å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹ä¸ºæ¯å¼ çš®è‚¤é•œå›¾åƒç”Ÿæˆå‡†ç¡®çš„æç¤ºï¼Œæœ‰åŠ©äºç”Ÿæˆä»£è¡¨æ€§ä¸è¶³çš„ç¾¤ä½“çš„åˆæˆå›¾åƒï¼Œä»è€Œæ”¹è¿›é«˜åº¦ä¸å‡è¡¡æ•°æ®é›†ä¸­çš„ä¸´åºŠè¯Šæ–­ã€‚å®éªŒè¡¨æ˜ï¼Œå¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹æä¾›äº†æ›´æœ‰æ´å¯ŸåŠ›çš„è¡¨ç¤ºï¼Œä½¿DermDiTèƒ½å¤Ÿç”Ÿæˆé«˜è´¨é‡å›¾åƒã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>äººå·¥æ™ºèƒ½åœ¨çš®è‚¤ç–¾ç—…è¯Šæ–­ä¸­çš„åº”ç”¨å·²ç»å–å¾—äº†æ˜¾è‘—è¿›æ­¥ã€‚</li>
<li>ç°æœ‰æ¨¡å‹åœ¨é’ˆå¯¹ä¸åŒå­ç¾¤ä½“çš„æ€§èƒ½ä¸Šç»å¸¸è¡¨ç°å‡ºåè§ï¼Œç‰¹åˆ«æ˜¯åœ¨æ¶‰åŠçš®è‚¤é¢œè‰²ç­‰æ•æ„Ÿå±æ€§æ–¹é¢ã€‚</li>
<li>æå‡ºäº†ä¸€ç§æ–°å‹ç”Ÿæˆå¼AIæ¡†æ¶â€”â€”çš®è‚¤ç—…æ‰©æ•£è½¬æ¢å™¨ï¼ˆDermDiTï¼‰æ¥è§£å†³è¿™ä¸€é—®é¢˜ã€‚</li>
<li>DermDiTåˆ©ç”¨è§†è§‰è¯­è¨€æ¨¡å‹ç”Ÿæˆæ–‡æœ¬æç¤ºï¼Œå¹¶é€šè¿‡å¤šæ¨¡æ€æ–‡æœ¬-å›¾åƒå­¦ä¹ æ¥ç”Ÿæˆæ–°çš„çš®è‚¤é•œå›¾åƒã€‚</li>
<li>è¯¥æ¡†æ¶é€šè¿‡ä¸ºæ¯å¼ çš®è‚¤é•œå›¾åƒç”Ÿæˆå‡†ç¡®çš„æç¤ºï¼Œæœ‰åŠ©äºæ”¹å–„ä»£è¡¨æ€§ä¸è¶³çš„ç¾¤ä½“ï¼ˆå¦‚æ‚£è€…ã€ç–¾ç—…ç­‰ï¼‰åœ¨é«˜åº¦ä¸å‡è¡¡æ•°æ®é›†ä¸­çš„è¡¨ç¤ºã€‚</li>
<li>å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹ä¸ºDermDiTç”Ÿæˆé«˜è´¨é‡å›¾åƒæä¾›äº†æ›´æœ‰æ´å¯ŸåŠ›çš„è¡¨ç¤ºã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.01838">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-f9fbd1bfb0f3694fd918abae2a08ae1b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-935ed7e32dfc9df1ddd73596824aa78f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-142f560989c5e922304e95215fc8cb55.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9860583ebeaf87e005853e7065dc1244.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="STPNet-Scale-aware-Text-Prompt-Network-for-Medical-Image-Segmentation"><a href="#STPNet-Scale-aware-Text-Prompt-Network-for-Medical-Image-Segmentation" class="headerlink" title="STPNet: Scale-aware Text Prompt Network for Medical Image Segmentation"></a>STPNet: Scale-aware Text Prompt Network for Medical Image Segmentation</h2><p><strong>Authors:Dandan Shan, Zihan Li, Yunxiang Li, Qingde Li, Jie Tian, Qingqi Hong</strong></p>
<p>Accurate segmentation of lesions plays a critical role in medical image analysis and diagnosis. Traditional segmentation approaches that rely solely on visual features often struggle with the inherent uncertainty in lesion distribution and size. To address these issues, we propose STPNet, a Scale-aware Text Prompt Network that leverages vision-language modeling to enhance medical image segmentation. Our approach utilizes multi-scale textual descriptions to guide lesion localization and employs retrieval-segmentation joint learning to bridge the semantic gap between visual and linguistic modalities. Crucially, STPNet retrieves relevant textual information from a specialized medical text repository during training, eliminating the need for text input during inference while retaining the benefits of cross-modal learning. We evaluate STPNet on three datasets: COVID-Xray, COVID-CT, and Kvasir-SEG. Experimental results show that our vision-language approach outperforms state-of-the-art segmentation methods, demonstrating the effectiveness of incorporating textual semantic knowledge into medical image analysis. The code has been made publicly on <a target="_blank" rel="noopener" href="https://github.com/HUANGLIZI/STPNet">https://github.com/HUANGLIZI/STPNet</a>. </p>
<blockquote>
<p>åœ¨åŒ»å­¦å›¾åƒåˆ†æå’Œè¯Šæ–­ä¸­ï¼Œç²¾ç¡®åœ°åˆ†å‰²ç—…å˜éƒ¨ä½èµ·ç€è‡³å…³é‡è¦çš„ä½œç”¨ã€‚ä¼ ç»Ÿçš„ä»…ä¾èµ–äºè§†è§‰ç‰¹å¾çš„åˆ†å‰²æ–¹æ³•é€šå¸¸éš¾ä»¥å¤„ç†ç—…å˜åˆ†å¸ƒå’Œå¤§å°ä¸­çš„å›ºæœ‰ä¸ç¡®å®šæ€§ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†STPNetï¼Œè¿™æ˜¯ä¸€ä¸ªåˆ©ç”¨è§†è§‰è¯­è¨€å»ºæ¨¡æ¥æé«˜åŒ»å­¦å›¾åƒåˆ†å‰²çš„å°ºåº¦æ„ŸçŸ¥æ–‡æœ¬æç¤ºç½‘ç»œã€‚æˆ‘ä»¬çš„æ–¹æ³•åˆ©ç”¨å¤šå°ºåº¦æ–‡æœ¬æè¿°æ¥æŒ‡å¯¼ç—…å˜éƒ¨ä½çš„å®šä½ï¼Œå¹¶é‡‡ç”¨æ£€ç´¢åˆ†å‰²è”åˆå­¦ä¹ æ¥å¼¥åˆè§†è§‰å’Œè¯­è¨€æ¨¡æ€ä¹‹é—´çš„è¯­ä¹‰é¸¿æ²Ÿã€‚é‡è¦çš„æ˜¯ï¼ŒSTPNetåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ä»ä¸“ç”¨çš„åŒ»å­¦æ–‡æœ¬å­˜å‚¨åº“ä¸­æ£€ç´¢ç›¸å…³çš„æ–‡æœ¬ä¿¡æ¯ï¼Œä»è€Œåœ¨æ¨ç†è¿‡ç¨‹ä¸­ä¸éœ€è¦æ–‡æœ¬è¾“å…¥ï¼ŒåŒæ—¶ä¿ç•™è·¨æ¨¡æ€å­¦ä¹ çš„ä¼˜ç‚¹ã€‚æˆ‘ä»¬åœ¨ä¸‰ä¸ªæ•°æ®é›†ä¸Šè¯„ä¼°äº†STPNetï¼šCOVID-Xrayã€COVID-CTå’ŒKvasir-SEGã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„è§†è§‰è¯­è¨€æ–¹æ³•ä¼˜äºæœ€å…ˆè¿›çš„åˆ†å‰²æ–¹æ³•ï¼Œè¯æ˜äº†å°†æ–‡æœ¬è¯­ä¹‰çŸ¥è¯†èå…¥åŒ»å­¦å›¾åƒåˆ†æä¸­çš„æœ‰æ•ˆæ€§ã€‚ä»£ç å·²å…¬å¼€åœ¨<a target="_blank" rel="noopener" href="https://github.com/HUANGLIZI/STPNet">https://github.com/HUANGLIZI/STPNet</a>ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.01561v1">PDF</a> </p>
<p><strong>Summary</strong><br>STPNetæ˜¯ä¸€ä¸ªåˆ©ç”¨è§†è§‰è¯­è¨€å»ºæ¨¡å¢å¼ºåŒ»å­¦å›¾åƒåˆ†å‰²çš„è§„æ¨¡æ„ŸçŸ¥æ–‡æœ¬æç¤ºç½‘ç»œã€‚å®ƒåˆ©ç”¨å¤šå°ºåº¦æ–‡æœ¬æè¿°å¼•å¯¼ç—…å˜å®šä½ï¼Œå¹¶é‡‡ç”¨æ£€ç´¢åˆ†å‰²è”åˆå­¦ä¹ æ¥ç¼©å°è§†è§‰å’Œè¯­è¨€æ¨¡æ€ä¹‹é—´çš„è¯­ä¹‰é¸¿æ²Ÿã€‚STPNetåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ä»ä¸“ä¸šåŒ»å­¦æ–‡æœ¬å­˜å‚¨åº“ä¸­æ£€ç´¢ç›¸å…³æ–‡æœ¬ä¿¡æ¯ï¼Œåœ¨æ¨ç†è¿‡ç¨‹ä¸­æ— éœ€æ–‡æœ¬è¾“å…¥ï¼ŒåŒæ—¶ä¿ç•™è·¨æ¨¡æ€å­¦ä¹ çš„ä¼˜åŠ¿ã€‚åœ¨ä¸‰ä¸ªæ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼ŒSTPNetåœ¨åŒ»å­¦å›¾åƒåˆ†æé¢†åŸŸä¼˜äºæœ€å…ˆè¿›çš„åˆ†å‰²æ–¹æ³•ï¼Œèå…¥æ–‡æœ¬è¯­ä¹‰çŸ¥è¯†æ•ˆæœæ˜¾è‘—ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>STPNetæ˜¯ä¸€ä¸ªé’ˆå¯¹åŒ»å­¦å›¾åƒåˆ†å‰²çš„è§„æ¨¡æ„ŸçŸ¥æ–‡æœ¬æç¤ºç½‘ç»œã€‚</li>
<li>è¯¥ç½‘ç»œåˆ©ç”¨å¤šå°ºåº¦æ–‡æœ¬æè¿°æ¥å¼•å¯¼ç—…å˜å®šä½ã€‚</li>
<li>STPNeté‡‡ç”¨æ£€ç´¢åˆ†å‰²è”åˆå­¦ä¹ ï¼Œç¼©å°äº†è§†è§‰å’Œè¯­è¨€æ¨¡æ€ä¹‹é—´çš„è¯­ä¹‰é¸¿æ²Ÿã€‚</li>
<li>STPNetåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ä»ä¸“ä¸šåŒ»å­¦æ–‡æœ¬å­˜å‚¨åº“ä¸­æ£€ç´¢ç›¸å…³æ–‡æœ¬ä¿¡æ¯ã€‚</li>
<li>åœ¨æ¨ç†è¿‡ç¨‹ä¸­ï¼ŒSTPNetä¸éœ€è¦é¢å¤–çš„æ–‡æœ¬è¾“å…¥ã€‚</li>
<li>STPNetåœ¨ä¸‰ä¸ªæ•°æ®é›†ä¸Šçš„å®éªŒè¡¨ç°ä¼˜äºå…¶ä»–æœ€å…ˆè¿›çš„åˆ†å‰²æ–¹æ³•ã€‚</li>
<li>èå…¥æ–‡æœ¬è¯­ä¹‰çŸ¥è¯†åœ¨åŒ»å­¦å›¾åƒåˆ†æä¸­å…·æœ‰æ˜¾è‘—æ•ˆæœã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.01561">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-78c42a000195bd4ef55fdd9c95b9d445.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0f59956388d8ed194813faf60d761217.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f57b1924bc137350c937ad810a82ea0a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-812112309d926c88e206e0c7cf874617.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="BiSeg-SAM-Weakly-Supervised-Post-Processing-Framework-for-Boosting-Binary-Segmentation-in-Segment-Anything-Models"><a href="#BiSeg-SAM-Weakly-Supervised-Post-Processing-Framework-for-Boosting-Binary-Segmentation-in-Segment-Anything-Models" class="headerlink" title="BiSeg-SAM: Weakly-Supervised Post-Processing Framework for Boosting   Binary Segmentation in Segment Anything Models"></a>BiSeg-SAM: Weakly-Supervised Post-Processing Framework for Boosting   Binary Segmentation in Segment Anything Models</h2><p><strong>Authors:Encheng Su, Hu Cao, Alois Knoll</strong></p>
<p>Accurate segmentation of polyps and skin lesions is essential for diagnosing colorectal and skin cancers. While various segmentation methods for polyps and skin lesions using fully supervised deep learning techniques have been developed, the pixel-level annotation of medical images by doctors is both time-consuming and costly. Foundational vision models like the Segment Anything Model (SAM) have demonstrated superior performance; however, directly applying SAM to medical segmentation may not yield satisfactory results due to the lack of domain-specific medical knowledge. In this paper, we propose BiSeg-SAM, a SAM-guided weakly supervised prompting and boundary refinement network for the segmentation of polyps and skin lesions. Specifically, we fine-tune SAM combined with a CNN module to learn local features. We introduce a WeakBox with two functions: automatically generating box prompts for the SAM model and using our proposed Multi-choice Mask-to-Box (MM2B) transformation for rough mask-to-box conversion, addressing the mismatch between coarse labels and precise predictions. Additionally, we apply scale consistency (SC) loss for prediction scale alignment. Our DetailRefine module enhances boundary precision and segmentation accuracy by refining coarse predictions using a limited amount of ground truth labels. This comprehensive approach enables BiSeg-SAM to achieve excellent multi-task segmentation performance. Our method demonstrates significant superiority over state-of-the-art (SOTA) methods when tested on five polyp datasets and one skin cancer dataset. </p>
<blockquote>
<p>å‡†ç¡®åœ°å¯¹æ¯è‚‰å’Œçš®è‚¤ç—…å˜è¿›è¡Œåˆ†å‰²å¯¹äºè¯Šæ–­ç»“è‚ ç™Œå’Œçš®è‚¤ç™Œè‡³å…³é‡è¦ã€‚è™½ç„¶å·²ç»å¼€å‘äº†å„ç§ä½¿ç”¨å®Œå…¨ç›‘ç£æ·±åº¦å­¦ä¹ çš„æ¯è‚‰å’Œçš®è‚¤ç—…å˜åˆ†å‰²æ–¹æ³•ï¼Œä½†åŒ»ç”Ÿå¯¹åŒ»ç–—å›¾åƒè¿›è¡Œåƒç´ çº§æ³¨é‡Šæ—¢è€—æ—¶åˆæˆæœ¬é«˜æ˜‚ã€‚åƒåˆ†æ®µä»»ä½•äº‹æƒ…æ¨¡å‹ï¼ˆSAMï¼‰è¿™æ ·çš„åŸºç¡€è§†è§‰æ¨¡å‹å·²ç»è¡¨ç°å‡ºäº†å“è¶Šçš„æ€§èƒ½ï¼›ç„¶è€Œï¼Œç›´æ¥å°†SAMåº”ç”¨äºåŒ»å­¦åˆ†å‰²å¯èƒ½æ— æ³•äº§ç”Ÿä»¤äººæ»¡æ„çš„ç»“æœï¼Œè¿™æ˜¯ç”±äºç¼ºä¹ç‰¹å®šé¢†åŸŸçš„åŒ»å­¦çŸ¥è¯†ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†BiSeg-SAMï¼Œè¿™æ˜¯ä¸€ä¸ªç”±SAMå¼•å¯¼çš„å¼±ç›‘ç£æç¤ºå’Œè¾¹ç•Œç»†åŒ–ç½‘ç»œï¼Œç”¨äºæ¯è‚‰å’Œçš®è‚¤ç—…å˜çš„åˆ†å‰²ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬ç»“åˆäº†SAMå’ŒCNNæ¨¡å—è¿›è¡Œå¾®è°ƒï¼Œä»¥å­¦ä¹ å±€éƒ¨ç‰¹å¾ã€‚æˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ªWeakBoxï¼Œå®ƒå…·æœ‰ä¸¤ä¸ªåŠŸèƒ½ï¼šè‡ªåŠ¨ä¸ºSAMæ¨¡å‹ç”Ÿæˆæ¡†æç¤ºï¼Œå¹¶ä½¿ç”¨æˆ‘ä»¬æå‡ºçš„Multi-choice Mask-to-Boxï¼ˆMM2Bï¼‰è½¬æ¢è¿›è¡Œç²—ç•¥çš„é®ç½©åˆ°æ¡†è½¬æ¢ï¼Œè§£å†³ç²—æ ‡ç­¾å’Œç²¾ç¡®é¢„æµ‹ä¹‹é—´çš„ä¸åŒ¹é…é—®é¢˜ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜åº”ç”¨äº†å°ºåº¦ä¸€è‡´æ€§ï¼ˆSCï¼‰æŸå¤±æ¥è¿›è¡Œé¢„æµ‹å°ºåº¦å¯¹é½ã€‚æˆ‘ä»¬çš„DetailRefineæ¨¡å—é€šè¿‡åˆ©ç”¨æœ‰é™æ•°é‡çš„çœŸå®æ ‡ç­¾å¯¹ç²—ç•¥é¢„æµ‹è¿›è¡Œç»†åŒ–ï¼Œæé«˜äº†è¾¹ç•Œç²¾åº¦å’Œåˆ†å‰²å‡†ç¡®æ€§ã€‚è¿™ä¸€ç»¼åˆæ–¹æ³•ä½¿BiSeg-SAMåœ¨äº”ä¸ªæ¯è‚‰æ•°æ®é›†å’Œä¸€ä¸ªçš®è‚¤ç™Œæ•°æ®é›†ä¸Šå–å¾—äº†å‡ºè‰²çš„å¤šä»»åŠ¡åˆ†å‰²æ€§èƒ½ã€‚æˆ‘ä»¬çš„æ–¹æ³•åœ¨æµ‹è¯•æ—¶æ˜¾ç¤ºå‡ºå¯¹æœ€å…ˆè¿›æ–¹æ³•çš„æ˜¾è‘—ä¼˜åŠ¿ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.01452v1">PDF</a> 2024 IEEE International Conference on Bioinformatics and Biomedicine   (BIBM)</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºä¸€ç§åŸºäºSegment Anything Modelï¼ˆSAMï¼‰çš„å¼±ç›‘ç£æç¤ºå’Œè¾¹ç•Œç»†åŒ–ç½‘ç»œBiSeg-SAMï¼Œç”¨äºæ¯è‚‰å’Œçš®è‚¤ç—…å˜çš„åˆ†å‰²ã€‚é’ˆå¯¹åŒ»ç–—å›¾åƒåˆ†å‰²ä¸­åƒç´ çº§æ ‡æ³¨è€—æ—¶è€—èµ„çš„é—®é¢˜ï¼ŒBiSeg-Samé€šè¿‡ç»“åˆSAMå’ŒCNNæ¨¡å—å­¦ä¹ å±€éƒ¨ç‰¹å¾ï¼Œå¼•å…¥WeakBoxè‡ªåŠ¨ç”ŸæˆSAMæ¨¡å‹çš„boxæç¤ºï¼Œå¹¶é‡‡ç”¨Multi-choice Mask-to-Boxè½¬æ¢è§£å†³ç²—æ ‡ç­¾ä¸ç²¾ç¡®é¢„æµ‹ä¹‹é—´çš„ä¸åŒ¹é…é—®é¢˜ã€‚æ­¤å¤–ï¼Œé€šè¿‡åº”ç”¨å°ºåº¦ä¸€è‡´æ€§æŸå¤±å®ç°é¢„æµ‹å°ºåº¦å¯¹é½ï¼Œå¹¶å€ŸåŠ©DetailRefineæ¨¡å—ä½¿ç”¨å°‘é‡çœŸå®æ ‡ç­¾æé«˜è¾¹ç•Œç²¾åº¦å’Œåˆ†å‰²å‡†ç¡®æ€§ã€‚åœ¨äº”ä¸ªæ¯è‚‰æ•°æ®é›†å’Œçš®è‚¤ç™Œæ•°æ®é›†ä¸Šçš„æµ‹è¯•ç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•ä¼˜äºç°æœ‰æŠ€æœ¯ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>æå‡ºBiSeg-SAMç½‘ç»œï¼Œç»“åˆSAMå’ŒCNNæ¨¡å—è¿›è¡ŒåŒ»ç–—å›¾åƒåˆ†å‰²ã€‚</li>
<li>WeakBoxè‡ªåŠ¨ä¸ºSAMæ¨¡å‹ç”Ÿæˆboxæç¤ºï¼Œå¹¶è§£å†³äº†ç²—æ ‡ç­¾ä¸ç²¾ç¡®é¢„æµ‹ä¹‹é—´çš„ä¸åŒ¹é…é—®é¢˜ã€‚</li>
<li>é‡‡ç”¨Multi-choice Mask-to-Boxè½¬æ¢è¿›è¡Œç²—ç•¥çš„mask-to-boxè½¬æ¢ã€‚</li>
<li>åº”ç”¨å°ºåº¦ä¸€è‡´æ€§æŸå¤±ä»¥æé«˜é¢„æµ‹å°ºåº¦çš„å‡†ç¡®æ€§ã€‚</li>
<li>DetailRefineæ¨¡å—ä½¿ç”¨å°‘é‡çœŸå®æ ‡ç­¾æé«˜è¾¹ç•Œç²¾åº¦å’Œåˆ†å‰²å‡†ç¡®æ€§ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.01452">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-fd4cf64d2f2c453e1bf510ef148b57b5.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f36b3193fce83b1f6036c7bde1c31c89.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-325db9374f50f98cac9c95084901aebf.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6e8d19e98507976032e732c373e7eb5b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7c968490ebf2b12e12ce708a40e39b2a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a1340c2cc6a4a4907e7591c47fd9877e.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Prompt-Guided-Attention-Head-Selection-for-Focus-Oriented-Image-Retrieval"><a href="#Prompt-Guided-Attention-Head-Selection-for-Focus-Oriented-Image-Retrieval" class="headerlink" title="Prompt-Guided Attention Head Selection for Focus-Oriented Image   Retrieval"></a>Prompt-Guided Attention Head Selection for Focus-Oriented Image   Retrieval</h2><p><strong>Authors:Yuji Nozawa, Yu-Chieh Lin, Kazumoto Nakamura, Youyang Ng</strong></p>
<p>The goal of this paper is to enhance pretrained Vision Transformer (ViT) models for focus-oriented image retrieval with visual prompting. In real-world image retrieval scenarios, both query and database images often exhibit complexity, with multiple objects and intricate backgrounds. Users often want to retrieve images with specific object, which we define as the Focus-Oriented Image Retrieval (FOIR) task. While a standard image encoder can be employed to extract image features for similarity matching, it may not perform optimally in the multi-object-based FOIR task. This is because each image is represented by a single global feature vector. To overcome this, a prompt-based image retrieval solution is required. We propose an approach called Prompt-guided attention Head Selection (PHS) to leverage the head-wise potential of the multi-head attention mechanism in ViT in a promptable manner. PHS selects specific attention heads by matching their attention maps with userâ€™s visual prompts, such as a point, box, or segmentation. This empowers the model to focus on specific object of interest while preserving the surrounding visual context. Notably, PHS does not necessitate model re-training and avoids any image alteration. Experimental results show that PHS substantially improves performance on multiple datasets, offering a practical and training-free solution to enhance model performance in the FOIR task. </p>
<blockquote>
<p>æœ¬æ–‡çš„ç›®æ ‡æ˜¯å¢å¼ºé¢å‘ç„¦ç‚¹çš„å›¾åƒæ£€ç´¢ä»»åŠ¡çš„é¢„è®­ç»ƒVision Transformerï¼ˆViTï¼‰æ¨¡å‹ï¼Œä½¿ç”¨è§†è§‰æç¤ºæ¥å®ç°ã€‚åœ¨ç°å®ä¸–ç•Œçš„å›¾åƒæ£€ç´¢åœºæ™¯ä¸­ï¼ŒæŸ¥è¯¢å›¾åƒå’Œæ•°æ®åº“å›¾åƒé€šå¸¸å…·æœ‰å¤æ‚æ€§ï¼ŒåŒ…å«å¤šä¸ªå¯¹è±¡å’Œå¤æ‚çš„èƒŒæ™¯ã€‚ç”¨æˆ·é€šå¸¸å¸Œæœ›æ£€ç´¢å…·æœ‰ç‰¹å®šå¯¹è±¡çš„å›¾åƒï¼Œæˆ‘ä»¬å°†å…¶å®šä¹‰ä¸ºé¢å‘ç„¦ç‚¹çš„å›¾åƒæ£€ç´¢ï¼ˆFOIRï¼‰ä»»åŠ¡ã€‚è™½ç„¶å¯ä»¥ä½¿ç”¨æ ‡å‡†çš„å›¾åƒç¼–ç å™¨æ¥æå–å›¾åƒç‰¹å¾ä»¥è¿›è¡Œç›¸ä¼¼æ€§åŒ¹é…ï¼Œä½†åœ¨åŸºäºå¤šå¯¹è±¡çš„FOIRä»»åŠ¡ä¸­ï¼Œå…¶æ€§èƒ½å¯èƒ½å¹¶ä¸æœ€ä½³ã€‚è¿™æ˜¯å› ä¸ºæ¯å¼ å›¾åƒéƒ½ç”±å•ä¸ªå…¨å±€ç‰¹å¾å‘é‡è¡¨ç¤ºã€‚ä¸ºäº†å…‹æœè¿™ä¸€ç‚¹ï¼Œéœ€è¦ä¸€ç§åŸºäºæç¤ºçš„å›¾åƒæ£€ç´¢è§£å†³æ–¹æ¡ˆã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§åä¸ºPrompt-guided attention Head Selectionï¼ˆPHSï¼‰çš„æ–¹æ³•ï¼Œä»¥æç¤ºçš„æ–¹å¼åˆ©ç”¨ViTä¸­å¤šå¤´æ³¨æ„åŠ›æœºåˆ¶çš„å¤´éƒ¨æ½œåŠ›ã€‚PHSé€šè¿‡åŒ¹é…ç”¨æˆ·çš„è§†è§‰æç¤ºï¼ˆå¦‚ç‚¹ã€æ¡†æˆ–åˆ†å‰²ï¼‰ä¸ç‰¹å®šçš„æ³¨æ„åŠ›å¤´æ¥é€‰å–æ³¨æ„åŠ›å¤´ï¼Œä½¿ç”¨æˆ·èƒ½å¤Ÿå…³æ³¨ç‰¹å®šçš„æ„Ÿå…´è¶£å¯¹è±¡ï¼ŒåŒæ—¶ä¿ç•™å‘¨å›´çš„è§†è§‰ä¸Šä¸‹æ–‡ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼ŒPHSä¸éœ€è¦æ¨¡å‹é‡æ–°è®­ç»ƒï¼Œå¹¶ä¸”é¿å…äº†ä»»ä½•å›¾åƒä¿®æ”¹ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒPHSåœ¨å¤šä¸ªæ•°æ®é›†ä¸Šçš„æ€§èƒ½å¾—åˆ°äº†æ˜¾è‘—æé«˜ï¼Œä¸ºFOIRä»»åŠ¡ä¸­å¢å¼ºæ¨¡å‹æ€§èƒ½æä¾›äº†å®ç”¨ä¸”æ— éœ€è®­ç»ƒçš„æ–¹æ³•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.01348v1">PDF</a> Accepted to CVPR 2025 PixFoundation Workshop</p>
<p><strong>Summary</strong><br>è¯¥è®ºæ–‡æ—¨åœ¨æ”¹è¿›é¢„è®­ç»ƒçš„Vision Transformerï¼ˆViTï¼‰æ¨¡å‹ï¼Œä»¥è¿›è¡Œé¢å‘ç„¦ç‚¹çš„å›¾åƒæ£€ç´¢ã€‚é’ˆå¯¹ç°å®ä¸–ç•Œä¸­çš„å›¾åƒæ£€ç´¢åœºæ™¯ï¼Œè®ºæ–‡æå‡ºäº†ä¸€ç§åä¸ºPrompt-guided attention Head Selectionï¼ˆPHSï¼‰çš„æ–¹æ³•ï¼Œåˆ©ç”¨ViTçš„å¤šå¤´æ³¨æ„åŠ›æœºåˆ¶çš„å¤´éƒ¨æ½œåŠ›ï¼Œé€šè¿‡åŒ¹é…ç”¨æˆ·çš„è§†è§‰æç¤ºï¼ˆå¦‚ç‚¹ã€æ¡†æˆ–åˆ†å‰²ï¼‰æ¥é€‰æ‹©ç‰¹å®šçš„æ³¨æ„åŠ›å¤´ã€‚æ­¤æ–¹æ³•å¯åœ¨ä¸é‡æ–°è®­ç»ƒæ¨¡å‹æˆ–æ›´æ”¹å›¾åƒçš„æƒ…å†µä¸‹ï¼Œæé«˜æ¨¡å‹åœ¨é¢å‘ç„¦ç‚¹çš„å›¾åƒæ£€ç´¢ä»»åŠ¡ä¸Šçš„æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è®ºæ–‡æ—¨åœ¨æ”¹è¿›é¢„è®­ç»ƒçš„Vision Transformerï¼ˆViTï¼‰æ¨¡å‹ï¼Œç”¨äºé¢å‘ç„¦ç‚¹çš„å›¾åƒæ£€ç´¢ä»»åŠ¡ã€‚</li>
<li>ç°æœ‰å›¾åƒç¼–ç å™¨åœ¨é¢å‘ç„¦ç‚¹çš„å›¾åƒæ£€ç´¢ä»»åŠ¡ä¸­å¯èƒ½è¡¨ç°ä¸ä½³ï¼Œå› ä¸ºæ¯ä¸ªå›¾åƒä»…ç”±ä¸€ä¸ªå…¨å±€ç‰¹å¾å‘é‡è¡¨ç¤ºã€‚</li>
<li>æå‡ºäº†ä¸€ç§åä¸ºPrompt-guided attention Head Selectionï¼ˆPHSï¼‰çš„æ–¹æ³•ï¼Œåˆ©ç”¨ViTçš„å¤šå¤´æ³¨æ„åŠ›æœºåˆ¶ã€‚</li>
<li>PHSé€šè¿‡åŒ¹é…æ³¨æ„åŠ›å›¾ä¸ç”¨æˆ·è§†è§‰æç¤ºï¼ˆå¦‚ç‚¹ã€æ¡†æˆ–åˆ†å‰²ï¼‰æ¥é€‰æ‹©ç‰¹å®šæ³¨æ„åŠ›å¤´ã€‚</li>
<li>PHSæ–¹æ³•ä½¿æ¨¡å‹èƒ½å¤Ÿå…³æ³¨ç‰¹å®šç›®æ ‡å¯¹è±¡ï¼ŒåŒæ—¶ä¿ç•™å‘¨å›´è§†è§‰ä¸Šä¸‹æ–‡ã€‚</li>
<li>PHSä¸éœ€è¦æ¨¡å‹é‡æ–°è®­ç»ƒï¼Œé¿å…äº†å›¾åƒä¿®æ”¹ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.01348">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-bc06053b2cfe5cab0eae0ec9651a5ef5.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ff156bb3affa9b789a278ed03e8a066f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f396ff481fbe044406c429dafde715ca.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-0305bce52138e5bcc0d5a33d5e965e22.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="AI-Judges-in-Design-Statistical-Perspectives-on-Achieving-Human-Expert-Equivalence-With-Vision-Language-Models"><a href="#AI-Judges-in-Design-Statistical-Perspectives-on-Achieving-Human-Expert-Equivalence-With-Vision-Language-Models" class="headerlink" title="AI Judges in Design: Statistical Perspectives on Achieving Human Expert   Equivalence With Vision-Language Models"></a>AI Judges in Design: Statistical Perspectives on Achieving Human Expert   Equivalence With Vision-Language Models</h2><p><strong>Authors:Kristen M. Edwards, Farnaz Tehranchi, Scarlett R. Miller, Faez Ahmed</strong></p>
<p>The subjective evaluation of early stage engineering designs, such as conceptual sketches, traditionally relies on human experts. However, expert evaluations are time-consuming, expensive, and sometimes inconsistent. Recent advances in vision-language models (VLMs) offer the potential to automate design assessments, but it is crucial to ensure that these AI &#96;&#96;judgesâ€™â€™ perform on par with human experts. However, no existing framework assesses expert equivalence. This paper introduces a rigorous statistical framework to determine whether an AI judgeâ€™s ratings match those of human experts. We apply this framework in a case study evaluating four VLM-based judges on key design metrics (uniqueness, creativity, usefulness, and drawing quality). These AI judges employ various in-context learning (ICL) techniques, including uni- vs. multimodal prompts and inference-time reasoning. The same statistical framework is used to assess three trained novices for expert-equivalence. Results show that the top-performing AI judge, using text- and image-based ICL with reasoning, achieves expert-level agreement for uniqueness and drawing quality and outperforms or matches trained novices across all metrics. In 6&#x2F;6 runs for both uniqueness and creativity, and 5&#x2F;6 runs for both drawing quality and usefulness, its agreement with experts meets or exceeds that of the majority of trained novices. These findings suggest that reasoning-supported VLM models can achieve human-expert equivalence in design evaluation. This has implications for scaling design evaluation in education and practice, and provides a general statistical framework for validating AI judges in other domains requiring subjective content evaluation. </p>
<blockquote>
<p>æ—©æœŸé˜¶æ®µå·¥ç¨‹è®¾è®¡çš„ä¸»è§‚è¯„ä¼°ï¼Œå¦‚æ¦‚å¿µè‰å›¾ï¼Œä¼ ç»Ÿä¸Šä¾èµ–äºäººç±»ä¸“å®¶ã€‚ç„¶è€Œï¼Œä¸“å®¶è¯„ä¼°è€—æ—¶ã€æˆæœ¬é«˜ï¼Œä¸”æœ‰æ—¶è¯„ä¼°ç»“æœä¸ä¸€è‡´ã€‚æœ€è¿‘ï¼Œè§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰çš„è¿›æ­¥ä¸ºè‡ªåŠ¨åŒ–è®¾è®¡è¯„ä¼°æä¾›äº†æ½œåŠ›ï¼Œä½†ç¡®ä¿è¿™äº›äººå·¥æ™ºèƒ½â€œåˆ¤å®˜â€çš„è¡¨ç°ä¸ä¸“å®¶ç›¸å½“è‡³å…³é‡è¦ã€‚ç„¶è€Œï¼Œæ²¡æœ‰ç°æœ‰çš„æ¡†æ¶æ¥è¯„ä¼°ä¸“å®¶ç­‰åŒç¨‹åº¦ã€‚æœ¬æ–‡å¼•å…¥äº†ä¸€ä¸ªä¸¥æ ¼çš„ç»Ÿè®¡æ¡†æ¶ï¼Œä»¥ç¡®å®šäººå·¥æ™ºèƒ½åˆ¤å®˜çš„è¯„åˆ†æ˜¯å¦ä¸ä¸“å®¶ç›¸å½“ã€‚æˆ‘ä»¬åœ¨æ­¤æ¡†æ¶ä¸­è¿›è¡Œäº†ä¸€é¡¹æ¡ˆä¾‹ç ”ç©¶ï¼Œè¯„ä¼°äº†å››ä½åŸºäºVLMçš„åˆ¤å®˜åœ¨ä¸»è¦è®¾è®¡æŒ‡æ ‡ï¼ˆç‹¬ç‰¹æ€§ã€åˆ›é€ æ€§ã€å®ç”¨æ€§å’Œç»˜å›¾è´¨é‡ï¼‰ä¸Šçš„è¡¨ç°ã€‚è¿™äº›AIåˆ¤å®˜é‡‡ç”¨äº†å„ç§ä¸Šä¸‹æ–‡å­¦ä¹ ï¼ˆICLï¼‰æŠ€æœ¯ï¼ŒåŒ…æ‹¬å•æ¨¡æ€ä¸å¤šæ¨¡æ€æç¤ºå’Œæ¨ç†æ—¶é—´æ¨ç†ã€‚åŒä¸€ç»Ÿè®¡æ¡†æ¶ä¹Ÿç”¨äºè¯„ä¼°ä¸‰åè®­ç»ƒæœ‰ç´ çš„æ–°æ‰‹æ˜¯å¦è¾¾åˆ°ä¸“å®¶æ°´å¹³ã€‚ç»“æœè¡¨æ˜ï¼Œè¡¨ç°æœ€ä½³çš„äººå·¥æ™ºèƒ½åˆ¤å®˜ï¼Œé‡‡ç”¨åŸºäºæ–‡æœ¬å’Œå›¾åƒçš„ICLä¸æ¨ç†æŠ€æœ¯ï¼Œåœ¨ç‹¬ç‰¹æ€§å’Œç»˜å›¾è´¨é‡æ–¹é¢è¾¾åˆ°äº†ä¸“å®¶çº§åè®®ï¼Œå¹¶åœ¨æ‰€æœ‰æŒ‡æ ‡ä¸Šä¼˜äºæˆ–åŒ¹é…è®­ç»ƒæœ‰ç´ çš„æ–°æ‰‹ã€‚åœ¨ç‹¬ç‰¹æ€§å’Œåˆ›é€ åŠ›çš„6æ¬¡è¿è¡Œä¸­ä»¥åŠåœ¨ç»˜å›¾è´¨é‡å’Œå®ç”¨æ€§çš„5æ¬¡è¿è¡Œä¸­ï¼Œå…¶ä¸ä¸“å®¶çš„åè®®è¾¾åˆ°æˆ–è¶…è¿‡äº†å¤§å¤šæ•°è®­ç»ƒæœ‰ç´ çš„æ–°æ‰‹ã€‚è¿™äº›å‘ç°è¡¨æ˜ï¼Œæ”¯æŒæ¨ç†çš„VLMæ¨¡å‹å¯ä»¥åœ¨è®¾è®¡è¯„ä¼°ä¸­å®ç°ä¸äººç±»ä¸“å®¶çš„ç­‰åŒæ°´å¹³ã€‚è¿™å¯¹æ‰©å¤§æ•™è‚²å’Œå®è·µä¸­çš„è®¾è®¡è¯„ä¼°è§„æ¨¡å…·æœ‰å½±å“ï¼Œå¹¶æä¾›äº†ä¸€ä¸ªé€šç”¨çš„ç»Ÿè®¡æ¡†æ¶ï¼Œå¯ç”¨äºéªŒè¯å…¶ä»–éœ€è¦è¿›è¡Œä¸»è§‚å†…å®¹è¯„ä¼°é¢†åŸŸçš„AIåˆ¤å®˜ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.00938v1">PDF</a> 21 pages, 8 tables, 6 figures, 8 tables in the appendix</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸€ä¸ªè¯„ä¼°äººå·¥æ™ºèƒ½åœ¨è®¾è®¡è¯„ä»·ä¸­æ˜¯å¦è¾¾åˆ°ä¸“å®¶æ°´å¹³çš„ç»Ÿè®¡æ¡†æ¶ã€‚è¯¥æ¡†æ¶åº”ç”¨äºè¯„ä¼°åŸºäºè§†è§‰è¯­è¨€æ¨¡å‹çš„å››ä¸ªAIè¯„å§”åœ¨å…³é”®è®¾è®¡æŒ‡æ ‡ä¸Šçš„è¡¨ç°ï¼Œå¦‚ç‹¬ç‰¹æ€§ã€åˆ›é€ åŠ›ã€å®ç”¨æ€§å’Œç»˜å›¾è´¨é‡ã€‚ç»“æœè¡¨æ˜ï¼Œæœ€ä¼˜ç§€çš„äººå·¥æ™ºèƒ½è¯„å§”ä½¿ç”¨åŸºäºæ–‡æœ¬å’Œå›¾åƒçš„ä¸Šä¸‹æ–‡å­¦ä¹ å¹¶åŠ ä¸Šæ¨ç†æŠ€æœ¯ï¼Œèƒ½å¤Ÿè¾¾åˆ°ä¸“å®¶çº§åˆ«çš„åè®®ï¼Œåœ¨æŸäº›æŒ‡æ ‡ä¸Šç”šè‡³è¶…è¿‡å—è¿‡è®­ç»ƒçš„æ–°æ‰‹ã€‚è¿™ä¸ºè®¾è®¡è¯„ä»·çš„å¯æ‰©å±•æ€§åœ¨æ•™è‚²å’Œå®è·µé¢†åŸŸæä¾›äº†å¯ç¤ºï¼Œä¹Ÿä¸ºéªŒè¯AIè¯„å§”åœ¨å…¶ä»–éœ€è¦è¿›è¡Œä¸»è§‚å†…å®¹è¯„ä»·é¢†åŸŸçš„èƒ½åŠ›æä¾›äº†ä¸€ä¸ªé€šç”¨çš„ç»Ÿè®¡æ¡†æ¶ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ä¼ ç»Ÿçš„å·¥ç¨‹è®¾è®¡è¯„ä¼°ä¾èµ–äºäººç±»ä¸“å®¶ï¼Œä½†å­˜åœ¨è€—æ—¶ã€æˆæœ¬é«˜æ˜‚å’Œä¸ä¸€è‡´çš„é—®é¢˜ã€‚</li>
<li>è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰çš„è¿‘æœŸè¿›å±•ä¸ºè‡ªåŠ¨åŒ–è®¾è®¡è¯„ä¼°æä¾›äº†æ½œåŠ›ã€‚</li>
<li>è®ºæ–‡å¼•å…¥äº†ä¸€ä¸ªç»Ÿè®¡æ¡†æ¶æ¥è¯„ä¼°AIè¯„å§”çš„è¡¨ç°æ˜¯å¦è¾¾åˆ°ä¸“å®¶æ°´å¹³ã€‚</li>
<li>åœ¨å…³é”®è®¾è®¡æŒ‡æ ‡ä¸Šï¼Œæœ€ä¼˜ç§€çš„äººå·¥æ™ºèƒ½è¯„å§”è¾¾åˆ°äº†ä¸ä¸“å®¶ä¸€è‡´çš„æ°´å¹³ï¼Œç”šè‡³åœ¨æŸäº›æŒ‡æ ‡ä¸Šè¶…è¿‡äº†å—è¿‡è®­ç»ƒçš„æ–°æ‰‹ã€‚</li>
<li>äººå·¥æ™ºèƒ½åœ¨è®¾è®¡è¯„ä»·ä¸­çš„è¡¨ç°å¯¹äºæ•™è‚²å’Œå®è·µé¢†åŸŸå…·æœ‰å¯ç¤ºä½œç”¨ã€‚</li>
<li>è¯¥ç ”ç©¶æä¾›äº†ä¸€ä¸ªé€šç”¨çš„ç»Ÿè®¡æ¡†æ¶ï¼Œç”¨äºéªŒè¯AIè¯„å§”åœ¨å…¶ä»–ä¸»è§‚å†…å®¹è¯„ä»·é¢†åŸŸçš„èƒ½åŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.00938">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-aa559beb641e76e4781c7be7c9d1e6d8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-18424d4d8a59bea19977c1abd4e65bdf.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="CellVTA-Enhancing-Vision-Foundation-Models-for-Accurate-Cell-Segmentation-and-Classification"><a href="#CellVTA-Enhancing-Vision-Foundation-Models-for-Accurate-Cell-Segmentation-and-Classification" class="headerlink" title="CellVTA: Enhancing Vision Foundation Models for Accurate Cell   Segmentation and Classification"></a>CellVTA: Enhancing Vision Foundation Models for Accurate Cell   Segmentation and Classification</h2><p><strong>Authors:Yang Yang, Xijie Xu, Yixun Zhou, Jie Zheng</strong></p>
<p>Cell instance segmentation is a fundamental task in digital pathology with broad clinical applications. Recently, vision foundation models, which are predominantly based on Vision Transformers (ViTs), have achieved remarkable success in pathology image analysis. However, their improvements in cell instance segmentation remain limited. A key challenge arises from the tokenization process in ViTs, which substantially reduces the spatial resolution of input images, leading to suboptimal segmentation quality, especially for small and densely packed cells. To address this problem, we propose CellVTA (Cell Vision Transformer with Adapter), a novel method that improves the performance of vision foundation models for cell instance segmentation by incorporating a CNN-based adapter module. This adapter extracts high-resolution spatial information from input images and injects it into the ViT through a cross-attention mechanism. Our method preserves the core architecture of ViT, ensuring seamless integration with pretrained foundation models. Extensive experiments show that CellVTA achieves 0.538 mPQ on the CoNIC dataset and 0.506 mPQ on the PanNuke dataset, which significantly outperforms the state-of-the-art cell segmentation methods. Ablation studies confirm the superiority of our approach over other fine-tuning strategies, including decoder-only fine-tuning and full fine-tuning. Our code and models are publicly available at <a target="_blank" rel="noopener" href="https://github.com/JieZheng-ShanghaiTech/CellVTA">https://github.com/JieZheng-ShanghaiTech/CellVTA</a>. </p>
<blockquote>
<p>ç»†èƒå®ä¾‹åˆ†å‰²æ˜¯æ•°å­—ç—…ç†å­¦ä¸­çš„ä¸€é¡¹åŸºæœ¬ä»»åŠ¡ï¼Œåœ¨ä¸´åºŠåº”ç”¨ä¸­å…·æœ‰å¹¿æ³›çš„åº”ç”¨ã€‚æœ€è¿‘ï¼ŒåŸºäºè§†è§‰è½¬æ¢å™¨ï¼ˆViTï¼‰çš„è§†è§‰åŸºç¡€æ¨¡å‹åœ¨ç—…ç†å­¦å›¾åƒåˆ†ææ–¹é¢å–å¾—äº†æ˜¾è‘—çš„æˆåŠŸã€‚ç„¶è€Œï¼Œå®ƒä»¬åœ¨ç»†èƒå®ä¾‹åˆ†å‰²æ–¹é¢çš„æ”¹è¿›ä»ç„¶æœ‰é™ã€‚ä¸€ä¸ªå…³é”®æŒ‘æˆ˜æ¥è‡ªäºViTä¸­çš„ä»¤ç‰ŒåŒ–è¿‡ç¨‹ï¼Œè¯¥è¿‡ç¨‹å¤§å¤§é™ä½äº†è¾“å…¥å›¾åƒçš„ç©ºé—´åˆ†è¾¨ç‡ï¼Œå¯¼è‡´åˆ†å‰²è´¨é‡ä¸ä½³ï¼Œå°¤å…¶æ˜¯é’ˆå¯¹å°è€Œå¯†é›†åŒ…è£…çš„ç»†èƒã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†CellVTAï¼ˆå¸¦é€‚é…å™¨çš„ç»†èƒè§†è§‰è½¬æ¢å™¨ï¼‰ï¼Œè¿™æ˜¯ä¸€ç§é€šè¿‡ç»“åˆåŸºäºCNNçš„é€‚é…å™¨æ¨¡å—æ¥æé«˜è§†è§‰åŸºç¡€æ¨¡å‹åœ¨ç»†èƒå®ä¾‹åˆ†å‰²æ–¹é¢çš„æ€§èƒ½çš„æ–°æ–¹æ³•ã€‚è¯¥é€‚é…å™¨ä»è¾“å…¥å›¾åƒä¸­æå–é«˜åˆ†è¾¨ç‡çš„ç©ºé—´ä¿¡æ¯ï¼Œå¹¶é€šè¿‡äº¤å‰æ³¨æ„æœºåˆ¶å°†å…¶æ³¨å…¥ViTã€‚æˆ‘ä»¬çš„æ–¹æ³•ä¿ç•™äº†ViTçš„æ ¸å¿ƒæ¶æ„ï¼Œç¡®ä¿ä¸é¢„è®­ç»ƒçš„åŸºç¡€æ¨¡å‹æ— ç¼é›†æˆã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼ŒCellVTAåœ¨CoNICæ•°æ®é›†ä¸Šå®ç°äº†0.538çš„mPQï¼Œåœ¨PanNukeæ•°æ®é›†ä¸Šå®ç°äº†0.506çš„mPQï¼Œæ˜¾è‘—ä¼˜äºæœ€å…ˆè¿›çš„ç»†èƒåˆ†å‰²æ–¹æ³•ã€‚æ¶ˆèç ”ç©¶è¯å®äº†æˆ‘ä»¬çš„æ–¹æ³•ä¼˜äºå…¶ä»–å¾®è°ƒç­–ç•¥ï¼ŒåŒ…æ‹¬ä»…è§£ç å™¨å¾®è°ƒå’Œå…¨å¾®è°ƒã€‚æˆ‘ä»¬çš„ä»£ç å’Œæ¨¡å‹å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/JieZheng-ShanghaiTech/CellVTA%E5%85%AC%E5%BC%80%E8%AE%BF%E9%97%AE%E3%80%82">https://github.com/JieZheng-ShanghaiTech/CellVTAå…¬å¼€è®¿é—®ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.00784v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>åŸºäºVision Transformerçš„æ¨¡å‹åœ¨æ•°å­—ç—…ç†å­¦ä¸­çš„ç»†èƒå®ä¾‹åˆ†å‰²ä»»åŠ¡ä¸­å–å¾—äº†æ˜¾è‘—çš„æˆåŠŸï¼Œä½†ä»æœ‰æå‡ç©ºé—´ã€‚ä¸ºæé«˜åˆ†å‰²è´¨é‡ï¼Œå°¤å…¶æ˜¯é’ˆå¯¹å°ä¸”å¯†é›†æ’åˆ—çš„ç»†èƒï¼Œæå‡ºäº†CellVTAæ–¹æ³•ã€‚è¯¥æ–¹æ³•é€šè¿‡å¼•å…¥CNNé€‚é…å™¨æ¨¡å—ï¼Œæå–é«˜åˆ†è¾¨ç‡ç©ºé—´ä¿¡æ¯å¹¶æ³¨å…¥ViTä¸­ã€‚å®éªŒè¯æ˜ï¼ŒCellVTAåœ¨CoNICå’ŒPanNukeæ•°æ®é›†ä¸Šçš„æ€§èƒ½ä¼˜äºç°æœ‰ç»†èƒåˆ†å‰²æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Vision foundation models based on Vision Transformers (ViTs) have achieved notable success in pathology image analysis.</li>
<li>A key challenge in cell instance segmentation is the tokenization process in ViTs, leading to reduced spatial resolution and suboptimal segmentation quality.</li>
<li>CellVTA addresses this challenge by incorporating a CNN-based adapter module to extract high-resolution spatial information.</li>
<li>CellVTA preserves the core architecture of ViT, ensuring seamless integration with pretrained foundation models.</li>
<li>CellVTA achieves significant performance improvements over state-of-the-art cell segmentation methods, with mPQ scores of 0.538 on the CoNIC dataset and 0.506 on the PanNuke dataset.</li>
<li>Ablation studies confirm the superiority of CellVTA over other fine-tuning strategies, including decoder-only fine-tuning and full fine-tuning.</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.00784">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-4b0dc4d2ce23c9640ad3961fd6a197fa.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e4e8cd733d03a099913175ecfc32372e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e580e79089de9ec6345bdf44f24411a5.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-5c6ae154cfddf1640fbeaaa33ba08a02.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-819a3553211e55e93c7a1f9a5c4a2590.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Deconver-A-Deconvolutional-Network-for-Medical-Image-Segmentation"><a href="#Deconver-A-Deconvolutional-Network-for-Medical-Image-Segmentation" class="headerlink" title="Deconver: A Deconvolutional Network for Medical Image Segmentation"></a>Deconver: A Deconvolutional Network for Medical Image Segmentation</h2><p><strong>Authors:Pooya Ashtari, Shahryar Noei, Fateme Nateghi Haredasht, Jonathan H. Chen, Giuseppe Jurman, Aleksandra Pizurica, Sabine Van Huffel</strong></p>
<p>While convolutional neural networks (CNNs) and vision transformers (ViTs) have advanced medical image segmentation, they face inherent limitations such as local receptive fields in CNNs and high computational complexity in ViTs. This paper introduces Deconver, a novel network that integrates traditional deconvolution techniques from image restoration as a core learnable component within a U-shaped architecture. Deconver replaces computationally expensive attention mechanisms with efficient nonnegative deconvolution (NDC) operations, enabling the restoration of high-frequency details while suppressing artifacts. Key innovations include a backpropagation-friendly NDC layer based on a provably monotonic update rule and a parameter-efficient design. Evaluated across four datasets (ISLESâ€™22, BraTSâ€™23, GlaS, FIVES) covering both 2D and 3D segmentation tasks, Deconver achieves state-of-the-art performance in Dice scores and Hausdorff distance while reducing computational costs (FLOPs) by up to 90% compared to leading baselines. By bridging traditional image restoration with deep learning, this work offers a practical solution for high-precision segmentation in resource-constrained clinical workflows. The project is available at <a target="_blank" rel="noopener" href="https://github.com/pashtari/deconver">https://github.com/pashtari/deconver</a>. </p>
<blockquote>
<p>è™½ç„¶å·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNï¼‰å’Œè§†è§‰è½¬æ¢å™¨ï¼ˆViTï¼‰å·²ç»æ¨åŠ¨äº†åŒ»å­¦å›¾åƒåˆ†å‰²çš„å‘å±•ï¼Œä½†å®ƒä»¬ä¹Ÿé¢ä¸´ç€å›ºæœ‰çš„å±€é™æ€§ï¼Œå¦‚CNNçš„å±€éƒ¨æ„Ÿå—é‡å’ŒViTçš„é«˜è®¡ç®—å¤æ‚åº¦ã€‚æœ¬æ–‡ä»‹ç»äº†Deconverï¼Œè¿™æ˜¯ä¸€ç§æ–°å‹ç½‘ç»œï¼Œå®ƒå°†å›¾åƒæ¢å¤ä¸­çš„ä¼ ç»Ÿåå·ç§¯æŠ€æœ¯é›†æˆåˆ°ä¸€ä¸ªUå½¢æ¶æ„ä¸­çš„æ ¸å¿ƒå¯å­¦ä¹ ç»„ä»¶ã€‚Deconverç”¨é«˜æ•ˆçš„éè´Ÿåå·ç§¯ï¼ˆNDCï¼‰æ“ä½œå–ä»£äº†è®¡ç®—æ˜‚è´µçš„æ³¨æ„åŠ›æœºåˆ¶ï¼Œèƒ½å¤Ÿåœ¨æ¢å¤é«˜é¢‘ç»†èŠ‚çš„åŒæ—¶æŠ‘åˆ¶ä¼ªå½±ã€‚ä¸»è¦åˆ›æ–°åŒ…æ‹¬åŸºäºå¯è¯æ˜å•è°ƒæ›´æ–°è§„åˆ™çš„æœ‰åˆ©äºåå‘ä¼ æ’­çš„NDCå±‚ä»¥åŠå‚æ•°é«˜æ•ˆçš„è®¾è®¡ã€‚åœ¨æ¶µç›–äºŒç»´å’Œä¸‰ç»´åˆ†å‰²ä»»åŠ¡çš„å››ä¸ªæ•°æ®é›†ï¼ˆISLESâ€™22ã€BraTSâ€™23ã€GlaSã€FIVESï¼‰ä¸Šè¿›è¡Œäº†è¯„ä¼°ï¼ŒDeconveråœ¨Diceå¾—åˆ†å’ŒHausdorffè·ç¦»æ–¹é¢è¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œä¸é¢†å…ˆçš„åŸºç¡€æ¨¡å‹ç›¸æ¯”ï¼Œè®¡ç®—æˆæœ¬ï¼ˆFLOPsï¼‰é™ä½äº†é«˜è¾¾90%ã€‚é€šè¿‡æ¡¥æ¥ä¼ ç»Ÿå›¾åƒæ¢å¤å’Œæ·±åº¦å­¦ä¹ ï¼Œè¿™é¡¹å·¥ä½œä¸ºèµ„æºå—é™çš„ä¸´åºŠå·¥ä½œæµç¨‹ä¸­çš„é«˜ç²¾åº¦åˆ†å‰²æä¾›äº†å®ç”¨è§£å†³æ–¹æ¡ˆã€‚è¯¥é¡¹ç›®å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/pashtari/deconver%E8%8E%B7%E5%BE%97%E3%80%82">https://github.com/pashtari/deconverè·å¾—ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.00302v1">PDF</a> 12 pages, 6 figures, 5 tables</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åä¸ºDeconverçš„æ–°å‹ç½‘ç»œï¼Œå®ƒå°†ä¼ ç»Ÿçš„è§£å·ç§¯æŠ€æœ¯ä½œä¸ºæ ¸å¿ƒå­¦ä¹ ç»„ä»¶é›†æˆåˆ°Uå‹æ¶æ„ä¸­ï¼Œä»¥è§£å†³åŒ»ç–—å›¾åƒåˆ†å‰²é¢ä¸´çš„æŒ‘æˆ˜ã€‚é€šè¿‡é‡‡ç”¨åŸºäºè§£å·ç§¯æ“ä½œçš„æ­£é¢ä¼ æ’­ç®—æ³•å’Œé«˜æ•ˆçš„å‚æ•°è®¾è®¡ï¼ŒDeconverèƒ½å¤Ÿåœ¨æ¢å¤é«˜é¢‘ç»†èŠ‚çš„åŒæ—¶æŠ‘åˆ¶ä¼ªå½±ã€‚åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šçš„è¯„ä¼°è¡¨æ˜ï¼Œå®ƒåœ¨Diceå¾—åˆ†å’ŒHausdorffè·ç¦»æ–¹é¢è¾¾åˆ°äº†æœ€æ–°æŠ€æœ¯æ°´å¹³ï¼Œå¹¶é™ä½äº†é«˜è¾¾90%çš„è®¡ç®—æˆæœ¬ã€‚è¯¥ç ”ç©¶ä¸ºèµ„æºå—é™çš„ä¸´åºŠå·¥ä½œæµç¨‹æä¾›äº†é«˜ç²¾åº¦åˆ†å‰²çš„å®ç”¨è§£å†³æ–¹æ¡ˆã€‚</p>
<p><strong>Key Takeaways</strong></p>
<p>ä»¥ä¸‹æ˜¯è¯¥æ–‡çš„å…³é”®è¦ç‚¹æ€»ç»“ï¼š</p>
<ul>
<li>CNNå’ŒViTåœ¨åŒ»ç–—å›¾åƒåˆ†å‰²æ–¹é¢å­˜åœ¨å±€é™æ€§ï¼Œå¦‚CNNçš„å±€éƒ¨æ„Ÿå—é‡å’ŒViTçš„é«˜è®¡ç®—å¤æ‚æ€§ã€‚</li>
<li>Deconverç½‘ç»œç»“åˆäº†ä¼ ç»Ÿè§£å·ç§¯æŠ€æœ¯ï¼Œä½œä¸ºä¸€ç§æ ¸å¿ƒå­¦ä¹ ç»„ä»¶ï¼Œä»¥è§£å†³è¿™äº›é—®é¢˜ã€‚</li>
<li>Deconverä½¿ç”¨é«˜æ•ˆéè´Ÿè§£å·ç§¯ï¼ˆNDCï¼‰æ“ä½œæ›¿ä»£äº†è®¡ç®—æˆæœ¬é«˜æ˜‚çš„æ³¨æ„åŠ›æœºåˆ¶ã€‚</li>
<li>NDCå±‚åŸºäºå¯è¯æ˜çš„å•è°ƒæ›´æ–°è§„åˆ™ï¼Œæœ‰åˆ©äºåå‘ä¼ æ’­ã€‚</li>
<li>Deconverå®ç°äº†å‚æ•°é«˜æ•ˆçš„è®¾è®¡ï¼Œèƒ½å¤Ÿåœ¨æ¢å¤é«˜é¢‘ç»†èŠ‚çš„åŒæ—¶æŠ‘åˆ¶ä¼ªå½±ã€‚</li>
<li>åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šçš„è¯„ä¼°è¡¨æ˜ï¼ŒDeconveråœ¨Diceå¾—åˆ†å’ŒHausdorffè·ç¦»æ–¹é¢è¾¾åˆ°äº†æœ€æ–°æŠ€æœ¯æ°´å¹³ã€‚</li>
<li>Deconveré™ä½äº†é«˜è¾¾90%çš„è®¡ç®—æˆæœ¬ï¼Œä¸ºèµ„æºå—é™çš„ä¸´åºŠå·¥ä½œæµç¨‹æä¾›äº†å®ç”¨è§£å†³æ–¹æ¡ˆã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.00302">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-45333f64721b449e41e5de5130537ed8.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-bf09399a0fad0372088d1325bd1049f8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2a8a7a4086364823041948675c7cb597.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5fd0720d589f11ae109f58dcbb423620.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="SALT-A-Flexible-Semi-Automatic-Labeling-Tool-for-General-LiDAR-Point-Clouds-with-Cross-Scene-Adaptability-and-4D-Consistency"><a href="#SALT-A-Flexible-Semi-Automatic-Labeling-Tool-for-General-LiDAR-Point-Clouds-with-Cross-Scene-Adaptability-and-4D-Consistency" class="headerlink" title="SALT: A Flexible Semi-Automatic Labeling Tool for General LiDAR Point   Clouds with Cross-Scene Adaptability and 4D Consistency"></a>SALT: A Flexible Semi-Automatic Labeling Tool for General LiDAR Point   Clouds with Cross-Scene Adaptability and 4D Consistency</h2><p><strong>Authors:Yanbo Wang, Yongtao Chen, Chuan Cao, Tianchen Deng, Wentao Zhao, Jingchuan Wang, Weidong Chen</strong></p>
<p>We propose a flexible Semi-Automatic Labeling Tool (SALT) for general LiDAR point clouds with cross-scene adaptability and 4D consistency. Unlike recent approaches that rely on camera distillation, SALT operates directly on raw LiDAR data, automatically generating pre-segmentation results. To achieve this, we propose a novel zero-shot learning paradigm, termed data alignment, which transforms LiDAR data into pseudo-images by aligning with the training distribution of vision foundation models. Additionally, we design a 4D-consistent prompting strategy and 4D non-maximum suppression module to enhance SAM2, ensuring high-quality, temporally consistent presegmentation. SALT surpasses the latest zero-shot methods by 18.4% PQ on SemanticKITTI and achieves nearly 40-50% of human annotator performance on our newly collected low-resolution LiDAR data and on combined data from three LiDAR types, significantly boosting annotation efficiency. We anticipate that SALTâ€™s open-sourcing will catalyze substantial expansion of current LiDAR datasets and lay the groundwork for the future development of LiDAR foundation models. Code is available at <a target="_blank" rel="noopener" href="https://github.com/Cavendish518/SALT">https://github.com/Cavendish518/SALT</a>. </p>
<blockquote>
<p>æˆ‘ä»¬æå‡ºäº†ä¸€ç§çµæ´»çš„åŠè‡ªåŠ¨æ ‡æ³¨å·¥å…·ï¼ˆSALTï¼‰ï¼Œé€‚ç”¨äºä¸€èˆ¬çš„æ¿€å…‰é›·è¾¾ç‚¹äº‘ï¼Œå…·æœ‰è·¨åœºæ™¯é€‚åº”æ€§å’Œ4Dä¸€è‡´æ€§ã€‚ä¸æœ€è¿‘ä¾èµ–äºç›¸æœºè’¸é¦çš„æ–¹æ³•ä¸åŒï¼ŒSALTç›´æ¥åœ¨åŸå§‹æ¿€å…‰é›·è¾¾æ•°æ®ä¸Šè¿è¡Œï¼Œè‡ªåŠ¨ç”Ÿæˆé¢„åˆ†å‰²ç»“æœã€‚ä¸ºäº†å®ç°è¿™ä¸€ç‚¹ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„é›¶æ ·æœ¬å­¦ä¹ èŒƒå¼ï¼Œç§°ä¸ºæ•°æ®å¯¹é½ï¼Œé€šè¿‡å°†æ¿€å…‰é›·è¾¾æ•°æ®ä¸è§†è§‰åŸºç¡€æ¨¡å‹çš„è®­ç»ƒåˆ†å¸ƒå¯¹é½ï¼Œå°†æ¿€å…‰é›·è¾¾æ•°æ®è½¬æ¢ä¸ºä¼ªå›¾åƒã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è®¾è®¡äº†4Dä¸€è‡´æç¤ºç­–ç•¥å’Œ4Déæœ€å¤§æŠ‘åˆ¶æ¨¡å—ï¼Œä»¥å¢å¼ºSAM2ï¼Œç¡®ä¿é«˜è´¨é‡ã€æ—¶é—´ä¸€è‡´çš„é¢„åˆ†å‰²ã€‚SALTåœ¨SemanticKITTIä¸Šçš„PQå¾—åˆ†è¶…è¿‡æœ€æ–°é›¶æ ·æœ¬æ–¹æ³•18.4%ï¼Œåœ¨æˆ‘ä»¬æ–°æ”¶é›†çš„ä½åˆ†è¾¨ç‡æ¿€å…‰é›·è¾¾æ•°æ®å’Œä¸‰ç§æ¿€å…‰é›·è¾¾ç±»å‹ç»„åˆçš„æ•°æ®ä¸Šï¼Œè¾¾åˆ°äººç±»æ ‡æ³¨å¸ˆæ€§èƒ½çš„è¿‘40-50%ï¼Œå¤§å¤§æé«˜äº†æ ‡æ³¨æ•ˆç‡ã€‚æˆ‘ä»¬é¢„è®¡SALTçš„å¼€æºå°†æå¤§åœ°æ¨åŠ¨å½“å‰æ¿€å…‰é›·è¾¾æ•°æ®é›†çš„å‘å±•ï¼Œå¹¶ä¸ºæœªæ¥æ¿€å…‰é›·è¾¾åŸºç¡€æ¨¡å‹çš„å¼€å‘å¥ å®šåŸºç¡€ã€‚ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/Cavendish518/SALT%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/Cavendish518/SALTæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.23980v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§çµæ´»çš„åŠè‡ªåŠ¨æ ‡æ³¨å·¥å…·ï¼ˆSALTï¼‰ï¼Œé€‚ç”¨äºä¸€èˆ¬çš„æ¿€å…‰é›·è¾¾ç‚¹äº‘æ•°æ®ï¼Œå…·æœ‰è·¨åœºæ™¯é€‚åº”æ€§å’Œ4Dä¸€è‡´æ€§ã€‚SALTç›´æ¥åœ¨åŸå§‹æ¿€å…‰é›·è¾¾æ•°æ®ä¸Šæ“ä½œï¼Œè‡ªåŠ¨ç”Ÿæˆé¢„åˆ†å‰²ç»“æœï¼Œä¸åŒäºä¾èµ–ç›¸æœºè’¸é¦çš„ç°æœ‰æ–¹æ³•ã€‚ä¸ºå®ç°è¿™ä¸€ç‚¹ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºæ•°æ®å¯¹é½çš„æ–°å‹é›¶æ ·æœ¬å­¦ä¹ èŒƒå¼ï¼Œé€šè¿‡å°†æ¿€å…‰é›·è¾¾æ•°æ®ä¸è§†è§‰åŸºç¡€æ¨¡å‹çš„è®­ç»ƒåˆ†å¸ƒå¯¹é½ï¼Œå°†æ¿€å…‰é›·è¾¾æ•°æ®è½¬æ¢ä¸ºä¼ªå›¾åƒã€‚åŒæ—¶ï¼Œè®¾è®¡äº†4Dä¸€è‡´çš„æç¤ºç­–ç•¥å’Œ4Déæœ€å¤§æŠ‘åˆ¶æ¨¡å—ï¼Œå¢å¼ºSAM2ï¼Œç¡®ä¿é«˜è´¨é‡ã€æ—¶é—´ä¸€è‡´çš„é¢„åˆ†å‰²ã€‚SALTåœ¨SemanticKITTIä¸Šçš„PQå¾—åˆ†æ¯”æœ€æ–°çš„é›¶æ ·æœ¬æ–¹æ³•é«˜å‡º18.4%ï¼Œåœ¨æ–°æ”¶é›†çš„ä½åˆ†è¾¨ç‡æ¿€å…‰é›·è¾¾æ•°æ®å’Œä¸‰ç§æ¿€å…‰é›·è¾¾ç±»å‹çš„ç»„åˆæ•°æ®ä¸Šï¼Œè¾¾åˆ°äº†äººç±»æ ‡æ³¨å™¨æ€§èƒ½çš„è¿‘40-50%ï¼Œæ˜¾è‘—æé«˜äº†æ ‡æ³¨æ•ˆç‡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>SALTæ˜¯ä¸€ç§é€‚ç”¨äºä¸€èˆ¬æ¿€å…‰é›·è¾¾ç‚¹äº‘çš„åŠè‡ªåŠ¨æ ‡æ³¨å·¥å…·ï¼Œå…·æœ‰è·¨åœºæ™¯é€‚åº”æ€§å’Œ4Dä¸€è‡´æ€§ã€‚</li>
<li>SALTç›´æ¥åœ¨åŸå§‹æ¿€å…‰é›·è¾¾æ•°æ®ä¸Šæ“ä½œï¼Œè‡ªåŠ¨ç”Ÿæˆé¢„åˆ†å‰²ç»“æœï¼Œä¸ä¾èµ–ç›¸æœºè’¸é¦ã€‚</li>
<li>æå‡ºäº†ä¸€ç§æ–°å‹é›¶æ ·æœ¬å­¦ä¹ èŒƒå¼â€”â€”æ•°æ®å¯¹é½ï¼Œå°†æ¿€å…‰é›·è¾¾æ•°æ®è½¬æ¢ä¸ºä¼ªå›¾åƒã€‚</li>
<li>è®¾è®¡äº†4Dä¸€è‡´çš„æç¤ºç­–ç•¥å’Œ4Déæœ€å¤§æŠ‘åˆ¶æ¨¡å—ï¼Œä»¥å¢å¼ºSAM2çš„æ€§èƒ½ã€‚</li>
<li>SALTåœ¨SemanticKITTIæ•°æ®é›†ä¸Šçš„æ€§èƒ½ä¼˜äºæœ€æ–°çš„é›¶æ ·æœ¬æ–¹æ³•ï¼ŒPQå¾—åˆ†æé«˜18.4%ã€‚</li>
<li>SALTåœ¨æ–°æ”¶é›†çš„ä½åˆ†è¾¨ç‡æ¿€å…‰é›·è¾¾æ•°æ®å’Œå¤šç§æ¿€å…‰é›·è¾¾ç±»å‹ç»„åˆçš„æ•°æ®ä¸Šè¡¨ç°å‡ºè‰¯å¥½çš„æ€§èƒ½ï¼Œè¾¾åˆ°äº†äººç±»æ ‡æ³¨å™¨æ€§èƒ½çš„è¿‘40-50%ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.23980">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-a0211353e8cbc84281e4c398cdfe1bef.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-097b4375a2145ccfa1b006ffa4366169.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-51af48061339e5e3c544106486e1dc9a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1178b9e3ac6176fde636baab318896f6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b6dd18c4667f82706898e99ae65d9817.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Texture-or-Semantics-Vision-Language-Models-Get-Lost-in-Font-Recognition"><a href="#Texture-or-Semantics-Vision-Language-Models-Get-Lost-in-Font-Recognition" class="headerlink" title="Texture or Semantics? Vision-Language Models Get Lost in Font   Recognition"></a>Texture or Semantics? Vision-Language Models Get Lost in Font   Recognition</h2><p><strong>Authors:Zhecheng Li, Guoxian Song, Yujun Cai, Zhen Xiong, Junsong Yuan, Yiwei Wang</strong></p>
<p>Modern Vision-Language Models (VLMs) exhibit remarkable visual and linguistic capabilities, achieving impressive performance in various tasks such as image recognition and object localization. However, their effectiveness in fine-grained tasks remains an open question. In everyday scenarios, individuals encountering design materials, such as magazines, typography tutorials, research papers, or branding content, may wish to identify aesthetically pleasing fonts used in the text. Given their multimodal capabilities and free accessibility, many VLMs are often considered potential tools for font recognition. This raises a fundamental question: Do VLMs truly possess the capability to recognize fonts? To investigate this, we introduce the Font Recognition Benchmark (FRB), a compact and well-structured dataset comprising 15 commonly used fonts. FRB includes two versions: (i) an easy version, where 10 sentences are rendered in different fonts, and (ii) a hard version, where each text sample consists of the names of the 15 fonts themselves, introducing a stroop effect that challenges model perception. Through extensive evaluation of various VLMs on font recognition tasks, we arrive at the following key findings: (i) Current VLMs exhibit limited font recognition capabilities, with many state-of-the-art models failing to achieve satisfactory performance. (ii) Few-shot learning and Chain-of-Thought (CoT) prompting provide minimal benefits in improving font recognition accuracy across different VLMs. (iii) Attention analysis sheds light on the inherent limitations of VLMs in capturing semantic features. </p>
<blockquote>
<p>ç°ä»£è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰è¡¨ç°å‡ºä»¤äººç©ç›®çš„è§†è§‰å’Œè¯­è¨€èƒ½åŠ›ï¼Œåœ¨å›¾åƒè¯†åˆ«ã€ç›®æ ‡å®šä½ç­‰ä»»åŠ¡ä¸­å–å¾—äº†ä»¤äººå°è±¡æ·±åˆ»çš„è¡¨ç°ã€‚ç„¶è€Œï¼Œå®ƒä»¬åœ¨ç²¾ç»†ä»»åŠ¡ä¸­çš„æœ‰æ•ˆæ€§ä»ç„¶æ˜¯ä¸€ä¸ªæ‚¬è€Œæœªå†³çš„é—®é¢˜ã€‚åœ¨æ—¥å¸¸åœºæ™¯ä¸­ï¼Œä¸ªäººåœ¨é‡åˆ°è®¾è®¡ææ–™ï¼Œå¦‚æ‚å¿—ã€æ’ç‰ˆæ•™ç¨‹ã€ç ”ç©¶è®ºæ–‡æˆ–å“ç‰Œå†…å®¹æ—¶ï¼Œå¯èƒ½å¸Œæœ›è¯†åˆ«æ–‡æœ¬ä¸­ä½¿ç”¨çš„å¸å¼•äººçš„å­—ä½“ã€‚è€ƒè™‘åˆ°å®ƒä»¬çš„å¤šæ¨¡æ€èƒ½åŠ›å’Œè‡ªç”±å¯è®¿é—®æ€§ï¼Œè®¸å¤šVLMsé€šå¸¸è¢«è®¤ä¸ºæ˜¯å­—ä½“è¯†åˆ«çš„æ½œåœ¨å·¥å…·ã€‚è¿™å¼•å‘äº†ä¸€ä¸ªæ ¹æœ¬æ€§çš„é—®é¢˜ï¼šVLMsçœŸçš„å…·æœ‰è¯†åˆ«å­—ä½“çš„èƒ½åŠ›å—ï¼Ÿä¸ºäº†è°ƒæŸ¥è¿™ä¸€ç‚¹ï¼Œæˆ‘ä»¬å¼•å…¥äº†å­—ä½“è¯†åˆ«åŸºå‡†æµ‹è¯•ï¼ˆFRBï¼‰ï¼Œè¿™æ˜¯ä¸€ä¸ªåŒ…å«15ç§å¸¸ç”¨å­—ä½“çš„ç´§å‡‘ä¸”ç»“æ„è‰¯å¥½çš„æ•°æ®é›†ã€‚FRBåŒ…æ‹¬ä¸¤ä¸ªç‰ˆæœ¬ï¼šï¼ˆiï¼‰ä¸€ä¸ªå®¹æ˜“çš„ç‰ˆæœ¬ï¼Œå…¶ä¸­10ä¸ªå¥å­ä»¥ä¸åŒçš„å­—ä½“å‘ˆç°ï¼›ï¼ˆiiï¼‰ä¸€ä¸ªå›°éš¾çš„ç‰ˆæœ¬ï¼Œå…¶ä¸­æ¯ä¸ªæ–‡æœ¬æ ·æœ¬ç”±15ç§å­—ä½“çš„åç§°æœ¬èº«ç»„æˆï¼Œå¼•å…¥ä¸€ç§æ–¯ç‰¹é²æ™®æ•ˆåº”ï¼ŒæŒ‘æˆ˜æ¨¡å‹çš„æ„ŸçŸ¥èƒ½åŠ›ã€‚é€šè¿‡å¯¹å„ç§VLMsåœ¨å­—ä½“è¯†åˆ«ä»»åŠ¡ä¸Šçš„å¹¿æ³›è¯„ä¼°ï¼Œæˆ‘ä»¬å¾—å‡ºä»¥ä¸‹å…³é”®å‘ç°ï¼šï¼ˆiï¼‰å½“å‰çš„VLMsåœ¨å­—ä½“è¯†åˆ«èƒ½åŠ›æ–¹é¢è¡¨ç°æœ‰é™ï¼Œè®¸å¤šæœ€å…ˆè¿›çš„æ¨¡å‹æ— æ³•å–å¾—ä»¤äººæ»¡æ„çš„æ€§èƒ½ã€‚ï¼ˆiiï¼‰å°æ ·æœ¬å­¦ä¹ å’Œæ€ç»´é“¾æç¤ºï¼ˆCoTï¼‰åœ¨æ”¹å–„ä¸åŒVLMsçš„å­—ä½“è¯†åˆ«å‡†ç¡®åº¦æ–¹é¢æä¾›æœ‰é™çš„ç›Šå¤„ã€‚ï¼ˆiiiï¼‰æ³¨æ„åŠ›åˆ†ææ­ç¤ºäº†VLMsåœ¨æ•è·è¯­ä¹‰ç‰¹å¾æ–¹é¢çš„å†…åœ¨å±€é™æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.23768v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ç°ä»£è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰åœ¨å­—ä½“è¯†åˆ«ä»»åŠ¡ä¸Šçš„è¡¨ç°ã€‚ä¸ºè°ƒæŸ¥VLMsçš„çœŸæ­£å­—ä½“è¯†åˆ«èƒ½åŠ›ï¼Œå¼•å…¥äº†å­—ä½“è¯†åˆ«åŸºå‡†æµ‹è¯•ï¼ˆFRBï¼‰ã€‚é€šè¿‡è¯„ä¼°å‘ç°ï¼Œå½“å‰VLMsåœ¨å­—ä½“è¯†åˆ«æ–¹é¢èƒ½åŠ›æœ‰é™ï¼Œé¡¶å°–æ¨¡å‹è¡¨ç°ä¸ä½³ï¼Œä¸”å°‘æ ·æœ¬å­¦ä¹ ä¸Chain-of-Thoughtæç¤ºå¯¹æå‡å­—ä½“è¯†åˆ«å‡†ç¡®ç‡ä½œç”¨æœ‰é™ã€‚æ³¨æ„åŠ›åˆ†ææ­ç¤ºäº†VLMsæ•æ‰è¯­ä¹‰ç‰¹å¾çš„å†…åœ¨å±€é™ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç°ä»£è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰åœ¨å­—ä½“è¯†åˆ«ä»»åŠ¡ä¸Šçš„è¡¨ç°å¼•äººæ³¨ç›®ï¼Œä½†åœ¨ç²¾ç»†ä»»åŠ¡ä¸Šçš„æ•ˆæœä»æœ‰å¾…æé«˜ã€‚</li>
<li>ä¸ºè°ƒæŸ¥VLMsçš„å­—ä½“è¯†åˆ«èƒ½åŠ›ï¼Œå¼•å…¥äº†å­—ä½“è¯†åˆ«åŸºå‡†æµ‹è¯•ï¼ˆFRBï¼‰ï¼ŒåŒ…æ‹¬ç®€æ˜“ç‰ˆå’Œå›°éš¾ç‰ˆï¼Œåˆ†åˆ«ç”¨äºè¯„ä¼°æ¨¡å‹çš„åŸºæœ¬å’Œæé™è¯†åˆ«èƒ½åŠ›ã€‚</li>
<li>å½“å‰VLMsåœ¨å­—ä½“è¯†åˆ«æ–¹é¢å­˜åœ¨å±€é™ï¼Œé¡¶å°–æ¨¡å‹è¡¨ç°å¹¶ä¸ç†æƒ³ã€‚</li>
<li>å°‘æ ·æœ¬å­¦ä¹ ä¸Chain-of-Thoughtæç¤ºåœ¨æå‡VLMsçš„å­—ä½“è¯†åˆ«å‡†ç¡®ç‡æ–¹é¢ä½œç”¨æœ‰é™ã€‚</li>
<li>æ³¨æ„åŠ›åˆ†ææ˜¾ç¤ºï¼ŒVLMsåœ¨æ•æ‰è¯­ä¹‰ç‰¹å¾æ–¹é¢å­˜åœ¨å†…åœ¨å±€é™ã€‚</li>
<li>FRBåŸºå‡†æµ‹è¯•ä¸ºè¯„ä¼°å’Œæ”¹è¿›VLMsçš„å­—ä½“è¯†åˆ«èƒ½åŠ›æä¾›äº†é‡è¦ä¾æ®ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.23768">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-41c3c9729d573df5837faac9360202cd.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-219dbdbeac642dca73de97abbd3c90a1.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-46e6a7140ce22e6df9fbcb3ca3d326bd.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-80fd61d23efe67ac85141d6a96a53b2b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-57444c19a0fd7abae8159d780f65229e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e1ff7acdd72cb2e47e4158428500fac7.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="Improved-Ear-Verification-with-Vision-Transformers-and-Overlapping-Patches"><a href="#Improved-Ear-Verification-with-Vision-Transformers-and-Overlapping-Patches" class="headerlink" title="Improved Ear Verification with Vision Transformers and Overlapping   Patches"></a>Improved Ear Verification with Vision Transformers and Overlapping   Patches</h2><p><strong>Authors:Deeksha Arun, Kagan Ozturk, Kevin W. Bowyer, Patrick Flynn</strong></p>
<p>Ear recognition has emerged as a promising biometric modality due to the relative stability in appearance during adulthood. Although Vision Transformers (ViTs) have been widely used in image recognition tasks, their efficiency in ear recognition has been hampered by a lack of attention to overlapping patches, which is crucial for capturing intricate ear features. In this study, we evaluate ViT-Tiny (ViT-T), ViT-Small (ViT-S), ViT-Base (ViT-B) and ViT-Large (ViT-L) configurations on a diverse set of datasets (OPIB, AWE, WPUT, and EarVN1.0), using an overlapping patch selection strategy. Results demonstrate the critical importance of overlapping patches, yielding superior performance in 44 of 48 experiments in a structured study. Moreover, upon comparing the results of the overlapping patches with the non-overlapping configurations, the increase is significant, reaching up to 10% for the EarVN1.0 dataset. In terms of model performance, the ViT-T model consistently outperformed the ViT-S, ViT-B, and ViT-L models on the AWE, WPUT, and EarVN1.0 datasets. The highest scores were achieved in a configuration with a patch size of 28x28 and a stride of 14 pixels. This patch-stride configuration represents 25% of the normalized image area (112x112 pixels) for the patch size and 12.5% of the row or column size for the stride. This study confirms that transformer architectures with overlapping patch selection can serve as an efficient and high-performing option for ear-based biometric recognition tasks in verification scenarios. </p>
<blockquote>
<p>è€³éƒ¨è¯†åˆ«å› æˆå¹´åå¤–è§‚çš„ç›¸å¯¹ç¨³å®šæ€§è€Œæˆä¸ºä¸€ç§æœ‰å‰æ™¯çš„ç”Ÿç‰©è¯†åˆ«æ–¹å¼ã€‚å°½ç®¡è§†è§‰å˜å‹å™¨ï¼ˆViTï¼‰å·²å¹¿æ³›åº”ç”¨äºå›¾åƒè¯†åˆ«ä»»åŠ¡ï¼Œä½†åœ¨è€³éƒ¨è¯†åˆ«æ–¹é¢ï¼Œç”±äºå¯¹é‡å æ–‘å—ç¼ºä¹å…³æ³¨ï¼Œå…¶æ•ˆç‡å—åˆ°é˜»ç¢ï¼Œè¿™å¯¹äºæ•è·å¤æ‚çš„è€³éƒ¨ç‰¹å¾è‡³å…³é‡è¦ã€‚åœ¨è¿™é¡¹ç ”ç©¶ä¸­ï¼Œæˆ‘ä»¬é‡‡ç”¨é‡å æ–‘å—é€‰æ‹©ç­–ç•¥ï¼Œè¯„ä¼°äº†ViT-Tinyï¼ˆViT-Tï¼‰ã€ViT-Smallï¼ˆViT-Sï¼‰ã€ViT-Baseï¼ˆViT-Bï¼‰å’ŒViT-Largeï¼ˆViT-Lï¼‰é…ç½®åœ¨OPIBã€AWEã€WPUTå’ŒEarVN1.0ç­‰å¤šä¸ªæ•°æ®é›†ä¸Šçš„æ€§èƒ½ã€‚ç»“æœè¡¨æ˜é‡å æ–‘å—è‡³å…³é‡è¦ï¼Œåœ¨ç»“æ„åŒ–çš„48æ¬¡å®éªŒä¸­ï¼Œæœ‰44æ¬¡å®éªŒè¡¨ç°å‡ºä¼˜è¶Šçš„æ€§èƒ½ã€‚æ­¤å¤–ï¼Œå°†é‡å æ–‘å—çš„ç»“æœä¸éé‡å é…ç½®è¿›è¡Œæ¯”è¾ƒï¼Œå…¶å¢é•¿å¹…åº¦æ˜¾è‘—ï¼Œå¯¹äºEarVN1.0æ•°æ®é›†è€Œè¨€ï¼Œå¢é•¿å¹…åº¦é«˜è¾¾10%ã€‚å°±æ¨¡å‹æ€§èƒ½è€Œè¨€ï¼ŒViT-Tæ¨¡å‹åœ¨AWEã€WPUTå’ŒEarVN1.0æ•°æ®é›†ä¸ŠæŒç»­ä¼˜äºViT-Sã€ViT-Bå’ŒViT-Læ¨¡å‹ã€‚åœ¨è¡¥ä¸å¤§å°ä¸º28x28ã€æ­¥å¹…ä¸º14åƒç´ çš„é…ç½®ä¸­å–å¾—äº†æœ€é«˜åˆ†æ•°ã€‚è¯¥è¡¥ä¸æ­¥å¹…é…ç½®ä»£è¡¨è¡¥ä¸å¤§å°çš„å›¾åƒåŒºåŸŸå å½’ä¸€åŒ–å›¾åƒé¢ç§¯çš„25%ï¼ˆå³112x112åƒç´ ï¼‰ï¼Œæ­¥å¹…å è¡Œæˆ–åˆ—å¤§å°çš„12.5%ã€‚è¿™é¡¹ç ”ç©¶è¯å®ï¼Œé‡‡ç”¨é‡å æ–‘å—é€‰æ‹©çš„å˜å‹å™¨æ¶æ„å¯ä½œä¸ºéªŒè¯åœºæ™¯ä¸­åŸºäºè€³æœµçš„ç”Ÿç‰©è¯†åˆ«ä»»åŠ¡çš„é«˜æ•ˆä¸”é«˜æ€§èƒ½é€‰æ‹©ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.23275v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ¢è®¨äº†ä½¿ç”¨Vision Transformersï¼ˆViTï¼‰åœ¨è€³éƒ¨è¯†åˆ«ä¸­çš„æ•ˆç‡é—®é¢˜ã€‚ç ”ç©¶å‘ç°åœ¨ä½¿ç”¨ViTæ¨¡å‹æ—¶ï¼Œé‡‡ç”¨é‡å è¡¥ä¸é€‰æ‹©ç­–ç•¥èƒ½æ˜¾è‘—æé«˜æ€§èƒ½ï¼Œç‰¹åˆ«æ˜¯åœ¨è€³éƒ¨è¯†åˆ«ä»»åŠ¡ä¸­ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œé‡å è¡¥ä¸ç­–ç•¥åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šçš„è¡¨ç°å‡ä¼˜äºéé‡å é…ç½®ï¼Œä¸”æœ€é«˜åˆ†æ•°å‡ºç°åœ¨è¡¥ä¸å¤§å°ä¸º28x28ï¼Œæ­¥é•¿ä¸º14åƒç´ çš„é…ç½®ä¸‹ã€‚ç ”ç©¶ç¡®è®¤äº†é‡‡ç”¨é‡å è¡¥ä¸é€‰æ‹©çš„è½¬æ¢å™¨æ¶æ„å¯ä½œä¸ºéªŒè¯åœºæ™¯ä¸­åŸºäºè€³æœµçš„ç”Ÿç‰©è¯†åˆ«çš„é«˜æ•ˆä¸”é«˜æ€§èƒ½é€‰æ‹©ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Vision Transformers (ViTs) åœ¨è€³éƒ¨è¯†åˆ«ä»»åŠ¡ä¸­çš„æ•ˆç‡å—åˆ°å…³æ³¨ã€‚</li>
<li>é‡å è¡¥ä¸é€‰æ‹©ç­–ç•¥å¯¹äºæ•æ‰è€³æœµçš„ç²¾ç»†ç‰¹å¾è‡³å…³é‡è¦ã€‚</li>
<li>åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šè¿›è¡Œçš„å®éªŒè¡¨æ˜ï¼Œé‡å è¡¥ä¸ç­–ç•¥æ˜¾è‘—æé«˜äº†æ€§èƒ½ã€‚</li>
<li>ViT-Tinyæ¨¡å‹åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šè¡¨ç°æœ€ä½³ã€‚</li>
<li>æœ€ä½³æ€§èƒ½æ˜¯åœ¨è¡¥ä¸å¤§å°ä¸º28x28ï¼Œæ­¥é•¿ä¸º14åƒç´ çš„é…ç½®ä¸‹å®ç°çš„ã€‚</li>
<li>é‡å è¡¥ä¸ç­–ç•¥èƒ½æé«˜æ¨¡å‹æ€§èƒ½é«˜è¾¾10%ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.23275">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-b3ef6e870c638d0b7d6a43f06c83aa60.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-14875ac18d696c62fa1ac8fa00b01392.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-db0e332e44c8df5f235df0b6549ef1c9.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b80318bc7463cd36247e0854accc4e60.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-46de9bd2bcd32c0b980d0112d6584f73.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="4D-LangSplat-4D-Language-Gaussian-Splatting-via-Multimodal-Large-Language-Models"><a href="#4D-LangSplat-4D-Language-Gaussian-Splatting-via-Multimodal-Large-Language-Models" class="headerlink" title="4D LangSplat: 4D Language Gaussian Splatting via Multimodal Large   Language Models"></a>4D LangSplat: 4D Language Gaussian Splatting via Multimodal Large   Language Models</h2><p><strong>Authors:Wanhua Li, Renping Zhou, Jiawei Zhou, Yingwei Song, Johannes Herter, Minghan Qin, Gao Huang, Hanspeter Pfister</strong></p>
<p>Learning 4D language fields to enable time-sensitive, open-ended language queries in dynamic scenes is essential for many real-world applications. While LangSplat successfully grounds CLIP features into 3D Gaussian representations, achieving precision and efficiency in 3D static scenes, it lacks the ability to handle dynamic 4D fields as CLIP, designed for static image-text tasks, cannot capture temporal dynamics in videos. Real-world environments are inherently dynamic, with object semantics evolving over time. Building a precise 4D language field necessitates obtaining pixel-aligned, object-wise video features, which current vision models struggle to achieve. To address these challenges, we propose 4D LangSplat, which learns 4D language fields to handle time-agnostic or time-sensitive open-vocabulary queries in dynamic scenes efficiently. 4D LangSplat bypasses learning the language field from vision features and instead learns directly from text generated from object-wise video captions via Multimodal Large Language Models (MLLMs). Specifically, we propose a multimodal object-wise video prompting method, consisting of visual and text prompts that guide MLLMs to generate detailed, temporally consistent, high-quality captions for objects throughout a video. These captions are encoded using a Large Language Model into high-quality sentence embeddings, which then serve as pixel-aligned, object-specific feature supervision, facilitating open-vocabulary text queries through shared embedding spaces. Recognizing that objects in 4D scenes exhibit smooth transitions across states, we further propose a status deformable network to model these continuous changes over time effectively. Our results across multiple benchmarks demonstrate that 4D LangSplat attains precise and efficient results for both time-sensitive and time-agnostic open-vocabulary queries. </p>
<blockquote>
<p>å­¦ä¹ 4Dè¯­è¨€åœºï¼Œä»¥å®ç°åœ¨åŠ¨æ€åœºæ™¯ä¸­è¿›è¡Œæ—¶é—´æ•æ„Ÿå’Œæ— é™åˆ¶çš„å¼€æ”¾è¯­è¨€æŸ¥è¯¢ï¼Œå¯¹äºè®¸å¤šå®é™…åº”ç”¨è‡³å…³é‡è¦ã€‚è™½ç„¶LangSplatæˆåŠŸåœ°å°†CLIPç‰¹æ€§èå…¥3Dé«˜æ–¯è¡¨ç¤ºä¸­ï¼Œåœ¨3Dé™æ€åœºæ™¯ä¸­å®ç°äº†ç²¾åº¦å’Œæ•ˆç‡ï¼Œä½†å®ƒç¼ºä¹å¤„ç†åŠ¨æ€4Dåœºçš„èƒ½åŠ›ï¼Œå› ä¸ºCLIPæ˜¯ä¸ºé™æ€å›¾åƒæ–‡æœ¬ä»»åŠ¡è®¾è®¡çš„ï¼Œæ— æ³•æ•æ‰è§†é¢‘ä¸­çš„æ—¶é—´åŠ¨æ€ã€‚ç°å®ä¸–ç•Œçš„ç¯å¢ƒæœ¬è´¨ä¸Šæ˜¯åŠ¨æ€çš„ï¼Œç‰©ä½“è¯­ä¹‰ä¼šéšæ—¶é—´æ¼”å˜ã€‚è¦å»ºç«‹ç²¾ç¡®çš„4Dè¯­è¨€åœºï¼Œå¿…é¡»è·å¾—åƒç´ å¯¹é½çš„ã€é¢å‘å¯¹è±¡çš„è§†é¢‘ç‰¹å¾ï¼Œè€Œå½“å‰çš„è§†è§‰æ¨¡å‹å¾ˆéš¾åšåˆ°è¿™ä¸€ç‚¹ã€‚ä¸ºäº†åº”å¯¹è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†4D LangSplatï¼Œå®ƒå­¦ä¹ 4Dè¯­è¨€åœºï¼Œä»¥é«˜æ•ˆåœ°å¤„ç†åŠ¨æ€åœºæ™¯ä¸­çš„æ—¶é—´æ— å…³æˆ–æ—¶é—´æ•æ„Ÿçš„æ— é™åˆ¶è¯æ±‡æŸ¥è¯¢ã€‚4D LangSplatç»•è¿‡ä»è§†è§‰ç‰¹å¾å­¦ä¹ è¯­è¨€åœºçš„æ­¥éª¤ï¼Œè€Œæ˜¯ç›´æ¥ä»é€šè¿‡å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰ç”Ÿæˆçš„é¢å‘å¯¹è±¡çš„è§†é¢‘å­—å¹•ä¸­å­¦ä¹ ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§å¤šæ¨¡æ€é¢å‘å¯¹è±¡è§†é¢‘æç¤ºæ–¹æ³•ï¼ŒåŒ…æ‹¬è§†è§‰å’Œæ–‡æœ¬æç¤ºï¼Œå¼•å¯¼MLLMsä¸ºè§†é¢‘ä¸­çš„å¯¹è±¡ç”Ÿæˆè¯¦ç»†ã€æ—¶é—´ä¸€è‡´ã€é«˜è´¨é‡çš„å­—å¹•ã€‚è¿™äº›å­—å¹•ä½¿ç”¨å¤§å‹è¯­è¨€æ¨¡å‹è¿›è¡Œç¼–ç ï¼Œä»¥ç”Ÿæˆé«˜è´¨é‡çš„å¥å­åµŒå…¥ï¼Œç„¶åä½œä¸ºåƒç´ å¯¹é½çš„ã€é¢å‘å¯¹è±¡çš„ç‰¹å¾ç›‘ç£ï¼Œé€šè¿‡å…±äº«åµŒå…¥ç©ºé—´è¿›è¡Œå¼€æ”¾è¯æ±‡æ–‡æœ¬æŸ¥è¯¢ã€‚æˆ‘ä»¬è®¤è¯†åˆ°4Dåœºæ™¯ä¸­çš„å¯¹è±¡åœ¨çŠ¶æ€ä¹‹é—´å‘ˆç°å‡ºå¹³æ»‘çš„è¿‡æ¸¡ï¼Œå› æ­¤è¿›ä¸€æ­¥æå‡ºäº†ä¸€ä¸ªçŠ¶æ€å¯å˜å½¢ç½‘ç»œï¼Œä»¥æœ‰æ•ˆåœ°å¯¹è¿™äº›éšæ—¶é—´å˜åŒ–çš„è¿ç»­å˜åŒ–è¿›è¡Œå»ºæ¨¡ã€‚æˆ‘ä»¬åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸Šçš„ç»“æœè¡¨æ˜ï¼Œ4D LangSplatå¯¹äºæ—¶é—´æ•æ„Ÿå’Œæ—¶é—´æ— å…³çš„æ— é™åˆ¶è¯æ±‡æŸ¥è¯¢éƒ½è¾¾åˆ°äº†ç²¾ç¡®å’Œé«˜æ•ˆçš„ç»“æœã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.10437v2">PDF</a> CVPR 2025. Project Page: <a target="_blank" rel="noopener" href="https://4d-langsplat.github.io/">https://4d-langsplat.github.io</a></p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ä¸ªæ–°çš„æ¨¡å‹â€”â€”4D LangSplatï¼Œç”¨äºå­¦ä¹ å¤„ç†åŠ¨æ€åœºæ™¯ä¸­çš„æ—¶é—´æ•æ„Ÿæ€§å’Œéæ—¶é—´æ•æ„Ÿæ€§å¼€æ”¾å¼è¯æ±‡æŸ¥è¯¢çš„4Dè¯­è¨€å­—æ®µã€‚é€šè¿‡ç›´æ¥ä»åŸºäºå¯¹è±¡çš„è§†é¢‘å­—å¹•ç”Ÿæˆæ–‡æœ¬ï¼Œä½¿ç”¨å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰å­¦ä¹ è¯­è¨€å­—æ®µï¼Œå¹¶å¼•å…¥æ¨¡æ€å¯¹è±¡è§†é¢‘æç¤ºæ–¹æ³•ï¼Œç”Ÿæˆè¯¦ç»†ã€æ—¶é—´è¿è´¯çš„é«˜è´¨é‡å¯¹è±¡å­—å¹•ã€‚è¿™äº›å­—å¹•è¢«ç¼–ç æˆé«˜è´¨é‡å¥å­åµŒå…¥ï¼Œä½œä¸ºåƒç´ å¯¹é½çš„å¯¹è±¡ç‰¹å®šç‰¹å¾ç›‘ç£ï¼Œæ”¯æŒé€šè¿‡å…±äº«åµŒå…¥ç©ºé—´è¿›è¡Œå¼€æ”¾å¼æ–‡æœ¬æŸ¥è¯¢ã€‚æ­¤å¤–ï¼Œæ¨¡å‹è¿˜å¼•å…¥äº†çŠ¶æ€å¯å˜å½¢ç½‘ç»œæ¥æ¨¡æ‹Ÿå¯¹è±¡åœ¨è¿ç»­æ—¶é—´å†…çš„å˜åŒ–ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œ4D LangSplatå¯¹äºæ—¶é—´æ•æ„Ÿæ€§å’Œéæ—¶é—´æ•æ„Ÿæ€§çš„å¼€æ”¾å¼è¯æ±‡æŸ¥è¯¢éƒ½èƒ½è¾¾åˆ°ç²¾ç¡®å’Œé«˜æ•ˆçš„ç»“æœã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>4D LangSplatæ¨¡å‹å¯ä»¥å­¦ä¹ å¤„ç†åŠ¨æ€åœºæ™¯ä¸­çš„æ—¶é—´æ•æ„Ÿæ€§å’Œéæ—¶é—´æ•æ„Ÿæ€§å¼€æ”¾å¼è¯æ±‡æŸ¥è¯¢çš„4Dè¯­è¨€å­—æ®µã€‚</li>
<li>æ¨¡å‹é€šè¿‡ç›´æ¥ä»åŸºäºå¯¹è±¡çš„è§†é¢‘å­—å¹•ç”Ÿæˆæ–‡æœ¬ï¼Œä½¿ç”¨å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰å­¦ä¹ è¯­è¨€å­—æ®µã€‚</li>
<li>æ¨¡å‹å¼•å…¥äº†æ¨¡æ€å¯¹è±¡è§†é¢‘æç¤ºæ–¹æ³•ï¼Œç”Ÿæˆè¯¦ç»†ã€æ—¶é—´è¿è´¯çš„é«˜è´¨é‡å¯¹è±¡å­—å¹•ã€‚</li>
<li>é«˜è´¨é‡å¥å­åµŒå…¥æ˜¯é€šè¿‡å¤§å‹è¯­è¨€æ¨¡å‹ç¼–ç ç”Ÿæˆçš„ï¼Œè¿™ä¸ºå¼€æ”¾å¼æ–‡æœ¬æŸ¥è¯¢æä¾›äº†æ”¯æŒã€‚</li>
<li>æ¨¡å‹å¼•å…¥äº†çŠ¶æ€å¯å˜å½¢ç½‘ç»œæ¥æ¨¡æ‹Ÿå¯¹è±¡åœ¨è¿ç»­æ—¶é—´å†…çš„å˜åŒ–ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.10437">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-42e8870b6a1f97704a697b4d7463e412.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d3082f1495a058b1d2ccbd09f68a5087.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="InPK-Infusing-Prior-Knowledge-into-Prompt-for-Vision-Language-Models"><a href="#InPK-Infusing-Prior-Knowledge-into-Prompt-for-Vision-Language-Models" class="headerlink" title="InPK: Infusing Prior Knowledge into Prompt for Vision-Language Models"></a>InPK: Infusing Prior Knowledge into Prompt for Vision-Language Models</h2><p><strong>Authors:Shuchang Zhou, Jiwei Wei, Shiyuan He, Yuyang Zhou, Chaoning Zhang, Jie Zou, Ning Xie, Yang Yang</strong></p>
<p>Prompt tuning has become a popular strategy for adapting Vision-Language Models (VLMs) to zero&#x2F;few-shot visual recognition tasks. Some prompting techniques introduce prior knowledge due to its richness, but when learnable tokens are randomly initialized and disconnected from prior knowledge, they tend to overfit on seen classes and struggle with domain shifts for unseen ones. To address this issue, we propose the InPK model, which infuses class-specific prior knowledge into the learnable tokens during initialization, thus enabling the model to explicitly focus on class-relevant information. Furthermore, to mitigate the weakening of class information by multi-layer encoders, we continuously reinforce the interaction between learnable tokens and prior knowledge across multiple feature levels. This progressive interaction allows the learnable tokens to better capture the fine-grained differences and universal visual concepts within prior knowledge, enabling the model to extract more discriminative and generalized text features. Even for unseen classes, the learned interaction allows the model to capture their common representations and infer their appropriate positions within the existing semantic structure. Moreover, we introduce a learnable text-to-vision projection layer to accommodate the text adjustments, ensuring better alignment of visual-text semantics. Extensive experiments on 11 recognition datasets show that InPK significantly outperforms state-of-the-art methods in multiple zero&#x2F;few-shot image classification tasks. </p>
<blockquote>
<p>åœ¨è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰ä¸­ï¼Œæç¤ºè°ƒæ•´å·²æˆä¸ºé€‚åº”é›¶&#x2F;å°‘æ ·æœ¬è§†è§‰è¯†åˆ«ä»»åŠ¡çš„æµè¡Œç­–ç•¥ã€‚ä¸€äº›æç¤ºæŠ€æœ¯ç”±äºå…¶ä¸°å¯Œæ€§è€Œå¼•å…¥äº†å…ˆéªŒçŸ¥è¯†ï¼Œä½†æ˜¯å½“å¯å­¦ä¹ ä»¤ç‰Œéšæœºåˆå§‹åŒ–å¹¶ä¸å…ˆéªŒçŸ¥è¯†æ–­å¼€è¿æ¥æ—¶ï¼Œå®ƒä»¬å¾€å¾€ä¼šå¯¹å·²è§ç±»åˆ«äº§ç”Ÿè¿‡æ‹Ÿåˆï¼Œå¹¶åœ¨æœªè§ç±»åˆ«ä¸Šé‡åˆ°é¢†åŸŸåç§»é—®é¢˜ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†InPKæ¨¡å‹ï¼Œè¯¥æ¨¡å‹åœ¨åˆå§‹åŒ–è¿‡ç¨‹ä¸­å°†ç‰¹å®šç±»åˆ«çš„å…ˆéªŒçŸ¥è¯†æ³¨å…¥å¯å­¦ä¹ ä»¤ç‰Œä¸­ï¼Œä»è€Œä½¿æ¨¡å‹èƒ½å¤Ÿæ˜¾å¼åœ°å…³æ³¨ç±»åˆ«ç›¸å…³çš„ä¿¡æ¯ã€‚æ­¤å¤–ï¼Œä¸ºäº†å‡è½»å¤šå±‚ç¼–ç å™¨å¯¹ç±»åˆ«ä¿¡æ¯çš„å‰Šå¼±ï¼Œæˆ‘ä»¬ä¸æ–­å¼ºåŒ–äº†å¯å­¦ä¹ ä»¤ç‰Œä¸å…ˆéªŒçŸ¥è¯†åœ¨å¤šä¸ªç‰¹å¾å±‚æ¬¡ä¹‹é—´çš„äº¤äº’ã€‚è¿™ç§æ¸è¿›çš„äº¤äº’ä½¿å¯å­¦ä¹ ä»¤ç‰Œèƒ½å¤Ÿæ›´å¥½åœ°æ•æ‰å…ˆéªŒçŸ¥è¯†ä¸­çš„ç»†å¾®å·®åˆ«å’Œé€šç”¨è§†è§‰æ¦‚å¿µï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿæå–æ›´å…·é‰´åˆ«åŠ›å’Œæ™®éæ€§çš„æ–‡æœ¬ç‰¹å¾ã€‚å³ä½¿å¯¹äºæœªè§è¿‡çš„ç±»åˆ«ï¼Œå­¦ä¹ åˆ°çš„äº¤äº’ä¹Ÿå…è®¸æ¨¡å‹æ•è·å®ƒä»¬çš„å…±åŒè¡¨ç¤ºå¹¶æ¨æ–­å®ƒä»¬åœ¨ç°æœ‰è¯­ä¹‰ç»“æ„ä¸­çš„é€‚å½“ä½ç½®ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ªå¯å­¦ä¹ çš„æ–‡æœ¬åˆ°è§†è§‰æŠ•å½±å±‚æ¥é€‚åº”æ–‡æœ¬è°ƒæ•´ï¼Œç¡®ä¿è§†è§‰æ–‡æœ¬è¯­ä¹‰çš„æ›´å¥½å¯¹é½ã€‚åœ¨11ä¸ªè¯†åˆ«æ•°æ®é›†ä¸Šçš„å¤§é‡å®éªŒè¡¨æ˜ï¼ŒInPKåœ¨å¤šä¸ªé›¶&#x2F;å°‘æ ·æœ¬å›¾åƒåˆ†ç±»ä»»åŠ¡ä¸­æ˜¾è‘—ä¼˜äºæœ€å…ˆè¿›çš„æ–¹æ³•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.19777v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†é’ˆå¯¹è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰åœ¨é›¶æ ·æœ¬æˆ–å°‘æ ·æœ¬è§†è§‰è¯†åˆ«ä»»åŠ¡ä¸­çš„é€‚åº”æ€§é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§åä¸ºInPKçš„æ¨¡å‹ã€‚è¯¥æ¨¡å‹é€šè¿‡åˆå§‹åŒ–æ—¶èå…¥ç±»åˆ«ç‰¹å®šçš„å…ˆéªŒçŸ¥è¯†ï¼Œè§£å†³äº†å­¦ä¹ ç¬¦å·éšæœºåˆå§‹åŒ–å¯¼è‡´çš„è¿‡æ‹Ÿåˆé—®é¢˜ã€‚æ­¤å¤–ï¼Œä¸ºäº†ç¼“è§£å¤šå±‚ç¼–ç å™¨å¯¹ç±»åˆ«ä¿¡æ¯çš„å‰Šå¼±ï¼Œæ¨¡å‹åœ¨å¤šçº§ç‰¹å¾ä¸Šä¸æ–­å¼ºåŒ–äº†å­¦ä¹ ç¬¦å·å’Œå…ˆéªŒçŸ¥è¯†é—´çš„äº¤äº’ã€‚å®éªŒè¯æ˜ï¼ŒInPKæ¨¡å‹åœ¨å¤šä¸ªé›¶æ ·æœ¬&#x2F;å°‘æ ·æœ¬å›¾åƒåˆ†ç±»ä»»åŠ¡ä¸­æ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>InPKæ¨¡å‹é€šè¿‡åˆå§‹åŒ–æ—¶èå…¥ç±»åˆ«ç‰¹å®šçš„å…ˆéªŒçŸ¥è¯†ï¼Œè§£å†³å­¦ä¹ ç¬¦å·éšæœºåˆå§‹åŒ–å¯¼è‡´çš„è¿‡æ‹Ÿåˆé—®é¢˜ã€‚</li>
<li>InPKæ¨¡å‹åœ¨å¤šçº§ç‰¹å¾ä¸Šå¼ºåŒ–äº†å­¦ä¹ ç¬¦å·å’Œå…ˆéªŒçŸ¥è¯†é—´çš„äº¤äº’ï¼Œæå‡æ¨¡å‹æ€§èƒ½ã€‚</li>
<li>å¼•å…¥çš„æ–‡æœ¬åˆ°è§†è§‰æŠ•å½±å±‚ç¡®ä¿è§†è§‰æ–‡æœ¬è¯­ä¹‰çš„æ›´å¥½å¯¹é½ã€‚</li>
<li>InPKæ¨¡å‹é€šè¿‡æ˜ç¡®å…³æ³¨ç±»åˆ«ç›¸å…³ä¿¡æ¯ï¼Œèƒ½å¤Ÿæå–æ›´å…·åŒºåˆ†æ€§å’Œé€šç”¨æ€§çš„æ–‡æœ¬ç‰¹å¾ã€‚</li>
<li>å¯¹äºæœªè§è¿‡çš„ç±»åˆ«ï¼ŒInPKæ¨¡å‹èƒ½å¤Ÿæ•æ‰å…¶é€šç”¨è¡¨ç¤ºå¹¶å®šä½å…¶åœ¨ç°æœ‰è¯­ä¹‰ç»“æ„ä¸­çš„ä½ç½®ã€‚</li>
<li>InPKæ¨¡å‹åœ¨å¤šä¸ªé›¶æ ·æœ¬&#x2F;å°‘æ ·æœ¬å›¾åƒåˆ†ç±»ä»»åŠ¡ä¸­æ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.19777">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-47e215cbfff4b988c13241a133c23553.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-9bd7dc2db0c3d1fc515b9af0443d6870.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-5e431123e800fcca0737c395354dfa24.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="DH-Mamba-Exploring-Dual-domain-Hierarchical-State-Space-Models-for-MRI-Reconstruction"><a href="#DH-Mamba-Exploring-Dual-domain-Hierarchical-State-Space-Models-for-MRI-Reconstruction" class="headerlink" title="DH-Mamba: Exploring Dual-domain Hierarchical State Space Models for MRI   Reconstruction"></a>DH-Mamba: Exploring Dual-domain Hierarchical State Space Models for MRI   Reconstruction</h2><p><strong>Authors:Yucong Meng, Zhiwei Yang, Zhijian Song, Yonghong Shi</strong></p>
<p>The accelerated MRI reconstruction poses a challenging ill-posed inverse problem due to the significant undersampling in k-space. Deep neural networks, such as CNNs and ViTs, have shown substantial performance improvements for this task while encountering the dilemma between global receptive fields and efficient computation. To this end, this paper explores selective state space models (Mamba), a new paradigm for long-range dependency modeling with linear complexity, for efficient and effective MRI reconstruction. However, directly applying Mamba to MRI reconstruction faces three significant issues: (1) Mamba typically flattens 2D images into distinct 1D sequences along rows and columns, disrupting k-spaceâ€™s unique spectrum and leaving its potential in k-space learning unexplored. (2) Existing approaches adopt multi-directional lengthy scanning to unfold images at the pixel level, leading to long-range forgetting and high computational burden. (3) Mamba struggles with spatially-varying contents, resulting in limited diversity of local representations. To address these, we propose a dual-domain hierarchical Mamba for MRI reconstruction from the following perspectives: (1) We pioneer vision Mamba in k-space learning. A circular scanning is customized for spectrum unfolding, benefiting the global modeling of k-space. (2) We propose a hierarchical Mamba with an efficient scanning strategy in both image and k-space domains. It mitigates long-range forgetting and achieves a better trade-off between efficiency and performance. (3) We develop a local diversity enhancement module to improve the spatially-varying representation of Mamba. Extensive experiments are conducted on three public datasets for MRI reconstruction under various undersampling patterns. Comprehensive results demonstrate that our method significantly outperforms state-of-the-art methods with lower computational cost. </p>
<blockquote>
<p>åŠ é€ŸMRIé‡å»ºæ˜¯ä¸€ä¸ªå…·æœ‰æŒ‘æˆ˜æ€§çš„ä¸é€‚å®šåé—®é¢˜ï¼Œä¸»è¦æ˜¯ç”±äºkç©ºé—´ä¸­çš„æ˜¾è‘—æ¬ é‡‡æ ·ã€‚æ·±åº¦ç¥ç»ç½‘ç»œï¼Œå¦‚å·ç§¯ç¥ç»ç½‘ç»œå’Œè§†è§‰è½¬æ¢å™¨ï¼Œåœ¨æ­¤ä»»åŠ¡ä¸Šæ˜¾ç¤ºå‡ºå®è´¨æ€§çš„æ€§èƒ½æ”¹è¿›ï¼Œä½†é¢ä¸´ç€å…¨å±€æ„Ÿå—é‡å’Œé«˜æ•ˆè®¡ç®—ä¹‹é—´çš„å›°å¢ƒã€‚ä¸ºæ­¤ï¼Œæœ¬æ–‡æ¢è®¨äº†é€‰æ‹©æ€§çŠ¶æ€ç©ºé—´æ¨¡å‹ï¼ˆMambaï¼‰è¿™ä¸€å…·æœ‰çº¿æ€§å¤æ‚åº¦çš„é•¿è·ç¦»ä¾èµ–å»ºæ¨¡æ–°èŒƒå¼ï¼Œç”¨äºé«˜æ•ˆä¸”æœ‰æ•ˆçš„MRIé‡å»ºã€‚ç„¶è€Œï¼Œç›´æ¥å°†Mambaåº”ç”¨äºMRIé‡å»ºé¢ä¸´ä¸‰ä¸ªé‡å¤§é—®é¢˜ï¼šï¼ˆ1ï¼‰Mambaé€šå¸¸å°†2Då›¾åƒå‹å¹³ä¸ºæ²¿è¡Œå’Œåˆ—çš„ç‹¬ç«‹1Dåºåˆ—ï¼Œç ´åäº†kç©ºé—´çš„ç‹¬ç‰¹é¢‘è°±ï¼Œå¹¶ä¸”æ²¡æœ‰æ¢ç´¢å…¶åœ¨kç©ºé—´å­¦ä¹ çš„æ½œåŠ›ã€‚ï¼ˆ2ï¼‰ç°æœ‰æ–¹æ³•é‡‡ç”¨å¤šæ–¹å‘é•¿æ‰«æä»¥åœ¨åƒç´ çº§åˆ«å±•å¼€å›¾åƒï¼Œè¿™å¯¼è‡´é•¿è·ç¦»é—å¿˜å’Œé«˜è®¡ç®—è´Ÿæ‹…ã€‚ï¼ˆ3ï¼‰Mambaåœ¨å¤„ç†ç©ºé—´å˜åŒ–å†…å®¹æ—¶é‡åˆ°å›°éš¾ï¼Œå¯¼è‡´å±€éƒ¨è¡¨ç¤ºå¤šæ ·æ€§æœ‰é™ã€‚ä¸ºè§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬ä»ä»¥ä¸‹è§’åº¦æå‡ºäº†ç”¨äºMRIé‡å»ºçš„åŒåŸŸåˆ†å±‚Mambaï¼šï¼ˆ1ï¼‰æˆ‘ä»¬åœ¨kç©ºé—´å­¦ä¹ ä¸­å¼€åˆ›äº†è§†è§‰Mambaçš„å…ˆæ²³ã€‚ä¸ºé¢‘è°±å±•å¼€å®šåˆ¶äº†å¾ªç¯æ‰«æï¼Œæœ‰åˆ©äºkç©ºé—´çš„å…¨å±€å»ºæ¨¡ã€‚ï¼ˆ2ï¼‰æˆ‘ä»¬æå‡ºäº†åœ¨å›¾åƒå’Œkç©ºé—´åŸŸä¸­éƒ½é‡‡ç”¨åˆ†å±‚Mambaå’Œæœ‰æ•ˆçš„æ‰«æç­–ç•¥ã€‚å®ƒå‡è½»äº†é•¿è·ç¦»é—å¿˜ï¼Œå¹¶åœ¨æ•ˆç‡å’Œæ€§èƒ½ä¹‹é—´å®ç°äº†æ›´å¥½çš„æƒè¡¡ã€‚ï¼ˆ3ï¼‰æˆ‘ä»¬å¼€å‘äº†ä¸€ä¸ªå±€éƒ¨å¤šæ ·æ€§å¢å¼ºæ¨¡å—ï¼Œä»¥æé«˜Mambaçš„ç©ºé—´å˜åŒ–è¡¨ç¤ºã€‚åœ¨ä¸‰ä¸ªå…¬å…±æ•°æ®é›†ä¸Šè¿›è¡Œäº†å¤§é‡MRIé‡å»ºå®éªŒï¼Œå®éªŒç»“æœè¡¨æ˜ï¼Œåœ¨å„ç§æ¬ é‡‡æ ·æ¨¡å¼ä¸‹ï¼Œæˆ‘ä»¬çš„æ–¹æ³•æ˜¾è‘—ä¼˜äºæœ€å…ˆè¿›çš„æ–¹æ³•ï¼Œä¸”è®¡ç®—æˆæœ¬æ›´ä½ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.08163v3">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ¢è®¨äº†ä¸€ç§æ–°å‹çš„é’ˆå¯¹MRIé‡å»ºçš„é€‰æ‹©æ€§çŠ¶æ€ç©ºé—´æ¨¡å‹ï¼ˆMambaï¼‰ã€‚ç„¶è€Œï¼Œç›´æ¥åº”ç”¨äºMRIé‡å»ºå­˜åœ¨ä¸‰ä¸ªä¸»è¦é—®é¢˜ã€‚ä¸ºåº”å¯¹è¿™äº›é—®é¢˜ï¼Œè¯¥æ–‡ä»ä»¥ä¸‹è§’åº¦æå‡ºäº†åŒåŸŸåˆ†å±‚Mambaæ–¹æ³•ï¼šæ¢ç´¢æ„¿æ™¯Mambaåœ¨kç©ºé—´å­¦ä¹ ä¸­çš„åº”ç”¨ï¼Œå®šåˆ¶å¾ªç¯æ‰«æä»¥ä¿ƒè¿›kç©ºé—´çš„å…¨å±€å»ºæ¨¡ï¼›æå‡ºå…·æœ‰é«˜æ•ˆæ‰«æç­–ç•¥çš„åˆ†å±‚Mambaï¼Œåœ¨å›¾åƒå’Œkç©ºé—´åŸŸä¸Šå‡è½»è¿œç¨‹é—å¿˜ï¼Œå®ç°æ•ˆç‡å’Œæ€§èƒ½çš„æ›´å¥½å¹³è¡¡ï¼›å¼€å‘å±€éƒ¨å¤šæ ·æ€§å¢å¼ºæ¨¡å—ï¼Œæé«˜Mambaçš„ç©ºé—´å˜åŒ–è¡¨ç¤ºèƒ½åŠ›ã€‚åœ¨ä¸‰ä¸ªå…¬å…±æ•°æ®é›†ä¸Šè¿›è¡Œçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨è¾ƒä½çš„è®¡ç®—æˆæœ¬ä¸‹æ˜¾è‘—ä¼˜äºç°æœ‰æŠ€æœ¯ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è®ºæ–‡ä»‹ç»äº†é’ˆå¯¹MRIé‡å»ºçš„é€‰æ‹©æ€§çŠ¶æ€ç©ºé—´æ¨¡å‹ï¼ˆMambaï¼‰çš„æ–°èŒƒå¼ã€‚</li>
<li>Mambaç›´æ¥åº”ç”¨äºMRIé‡å»ºé¢ä¸´ä¸‰ä¸ªä¸»è¦é—®é¢˜ï¼ŒåŒ…æ‹¬ç ´åkç©ºé—´çš„ç‹¬ç‰¹å…‰è°±ã€è¿œç¨‹é—å¿˜å’Œé«˜è®¡ç®—è´Ÿæ‹…ä»¥åŠç©ºé—´å˜åŒ–å†…å®¹çš„å¤„ç†ä¸è¶³ã€‚</li>
<li>ä¸ºè§£å†³è¿™äº›é—®é¢˜ï¼Œè®ºæ–‡æå‡ºäº†åŒåŸŸåˆ†å±‚Mambaæ–¹æ³•ï¼ŒåŒ…æ‹¬åœ¨kç©ºé—´å­¦ä¹ ä¸­çš„æ„¿æ™¯Mambaåº”ç”¨ã€å…·æœ‰é«˜æ•ˆæ‰«æç­–ç•¥çš„åˆ†å±‚Mambaä»¥åŠå±€éƒ¨å¤šæ ·æ€§å¢å¼ºæ¨¡å—çš„å¼€å‘ã€‚</li>
<li>å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å¤šç§æ¬ é‡‡æ ·æ¨¡å¼ä¸‹æ˜¾è‘—ä¼˜äºç°æœ‰æŠ€æœ¯ï¼Œä¸”è®¡ç®—æˆæœ¬è¾ƒä½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.08163">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-b6ee59095917e19ba200ff6484d2b111.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b06768b7591d3587a1725856765741e7.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c48eca9cd60c37cb82ea6d0ca7873aef.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="YOLO11-and-Vision-Transformers-based-3D-Pose-Estimation-of-Immature-Green-Fruits-in-Commercial-Apple-Orchards-for-Robotic-Thinning"><a href="#YOLO11-and-Vision-Transformers-based-3D-Pose-Estimation-of-Immature-Green-Fruits-in-Commercial-Apple-Orchards-for-Robotic-Thinning" class="headerlink" title="YOLO11 and Vision Transformers based 3D Pose Estimation of Immature   Green Fruits in Commercial Apple Orchards for Robotic Thinning"></a>YOLO11 and Vision Transformers based 3D Pose Estimation of Immature   Green Fruits in Commercial Apple Orchards for Robotic Thinning</h2><p><strong>Authors:Ranjan Sapkota, Manoj Karkee</strong></p>
<p>In this study, a robust method for 3D pose estimation of immature green apples (fruitlets) in commercial orchards was developed, utilizing the YOLO11(or YOLOv11) object detection and pose estimation algorithm alongside Vision Transformers (ViT) for depth estimation (Dense Prediction Transformer (DPT) and Depth Anything V2). For object detection and pose estimation, performance comparisons of YOLO11 (YOLO11n, YOLO11s, YOLO11m, YOLO11l and YOLO11x) and YOLOv8 (YOLOv8n, YOLOv8s, YOLOv8m, YOLOv8l and YOLOv8x) were made under identical hyperparameter settings among the all configurations. It was observed that YOLO11n surpassed all configurations of YOLO11 and YOLOv8 in terms of box precision and pose precision, achieving scores of 0.91 and 0.915, respectively. Conversely, YOLOv8n exhibited the highest box and pose recall scores of 0.905 and 0.925, respectively. Regarding the mean average precision at 50% intersection over union (mAP@50), YOLO11s led all configurations with a box mAP@50 score of 0.94, while YOLOv8n achieved the highest pose mAP@50 score of 0.96. In terms of image processing speed, YOLO11n outperformed all configurations with an impressive inference speed of 2.7 ms, significantly faster than the quickest YOLOv8 configuration, YOLOv8n, which processed images in 7.8 ms. Subsequent integration of ViTs for the green fruitâ€™s pose depth estimation revealed that Depth Anything V2 outperformed Dense Prediction Transformer in 3D pose length validation, achieving the lowest Root Mean Square Error (RMSE) of 1.52 and Mean Absolute Error (MAE) of 1.28, demonstrating exceptional precision in estimating immature green fruit lengths. Integration of YOLO11 and Depth Anything Model provides a promising solution to 3D pose estimation of immature green fruits for robotic thinning applications. (YOLOv11 pose detection, YOLOv11 Pose, YOLOv11 Keypoints detection, YOLOv11 pose estimation) </p>
<blockquote>
<p>åœ¨è¿™é¡¹ç ”ç©¶ä¸­ï¼Œå¼€å‘äº†ä¸€ç§åˆ©ç”¨YOLO11ï¼ˆæˆ–YOLOv11ï¼‰ç›®æ ‡æ£€æµ‹ä¸å§¿æ€ä¼°è®¡ç®—æ³•ä»¥åŠç”¨äºæ·±åº¦ä¼°è®¡çš„Vision Transformersï¼ˆViTï¼‰ï¼ˆåŒ…æ‹¬Dense Prediction Transformerï¼ˆDPTï¼‰å’ŒDepth Anything V2ï¼‰å¯¹å•†ä¸šæœå›­ä¸­æœªæˆç†Ÿç»¿è‰²è‹¹æœï¼ˆä¹Ÿç§°ä¸ºå¹¼æœï¼‰è¿›è¡Œ3Då§¿æ€ä¼°è®¡çš„ç¨³å¥æ–¹æ³•ã€‚åœ¨ç›®æ ‡æ£€æµ‹å’Œå§¿æ€ä¼°è®¡æ–¹é¢ï¼Œå¯¹YOLO11ï¼ˆYOLO11nã€YOLO11sã€YOLO11mã€YOLO11lå’ŒYOLO11xï¼‰å’ŒYOLOv8ï¼ˆYOLOv8nã€YOLOv8sã€YOLOv8mã€YOLOv8lå’ŒYOLOv8xï¼‰çš„æ‰€æœ‰é…ç½®è¿›è¡Œäº†ç›¸åŒçš„è¶…å‚æ•°è®¾ç½®ä¸‹çš„æ€§èƒ½æ¯”è¾ƒã€‚è§‚å¯Ÿå‘ç°ï¼ŒYOLO11nåœ¨æ¡†ç²¾åº¦å’Œå§¿æ€ç²¾åº¦æ–¹é¢è¶…è¶Šäº†æ‰€æœ‰YOLO11å’ŒYOLOv8çš„é…ç½®ï¼Œåˆ†åˆ«è¾¾åˆ°äº†0.91å’Œ0.915ã€‚ç›¸åï¼ŒYOLOv8nåœ¨ç›’å­å’Œå§¿æ€çš„å¬å›ç‡æ–¹é¢è¡¨ç°å‡ºæœ€ä½³ï¼Œåˆ†åˆ«è¾¾åˆ°äº†0.905å’Œ0.925ã€‚åœ¨50%äº¤é›†è”åˆçš„å¹³å‡ç²¾åº¦ï¼ˆmAP@50ï¼‰æ–¹é¢ï¼ŒYOLO11såœ¨æ‰€æœ‰é…ç½®ä¸­é¢†å…ˆï¼Œå…¶ç›’å­mAP@50å¾—åˆ†ä¸º0.94ï¼Œè€ŒYOLOv8nçš„å§¿æ€mAP@50å¾—åˆ†æœ€é«˜ï¼Œä¸º0.96ã€‚åœ¨å›¾åƒå¤„ç†é€Ÿåº¦æ–¹é¢ï¼ŒYOLO11nè¡¨ç°æœ€ä½³ï¼Œå…¶æ¨ç†é€Ÿåº¦ä¸º2.7æ¯«ç§’ï¼Œæ˜¾è‘—å¿«äºæœ€å¿«çš„YOLOv8é…ç½®ï¼ˆYOLOv8nï¼‰ï¼Œåè€…å¤„ç†å›¾åƒéœ€è¦7.8æ¯«ç§’ã€‚éšåå¯¹ç»¿è‰²æ°´æœçš„å§¿æ€æ·±åº¦ä¼°è®¡æ•´åˆViTsçš„ç ”ç©¶æ˜¾ç¤ºï¼ŒDepth Anything V2åœ¨3Då§¿æ€é•¿åº¦éªŒè¯æ–¹é¢è¡¨ç°ä¼˜äºDense Prediction Transformerï¼Œè¾¾åˆ°äº†æœ€ä½çš„å‡æ–¹æ ¹è¯¯å·®ï¼ˆRMSEï¼‰ä¸º1.52å’Œå¹³å‡ç»å¯¹è¯¯å·®ï¼ˆMAEï¼‰ä¸º1.28ï¼Œæ˜¾ç¤ºå‡ºåœ¨ä¼°ç®—æœªæˆç†Ÿç»¿è‰²æ°´æœé•¿åº¦æ–¹é¢çš„å‡ºè‰²ç²¾ç¡®åº¦ã€‚æ•´åˆYOLO11å’ŒDepth Anythingæ¨¡å‹ä¸ºè§£å†³æœºå™¨äººç–æœåº”ç”¨ä¸­æœªæˆç†Ÿç»¿è‰²æ°´æœçš„3Då§¿æ€ä¼°è®¡é—®é¢˜æä¾›äº†æœ‰å‰é€”çš„è§£å†³æ–¹æ¡ˆã€‚ï¼ˆYOLOv11å§¿æ€æ£€æµ‹ã€YOLOv11å§¿æ€ã€YOLOv11å…³é”®ç‚¹æ£€æµ‹ã€YOLOv11å§¿æ€ä¼°è®¡ï¼‰</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2410.19846v3">PDF</a> 24 Pages, 13 Figures, 1 Table</p>
<p><strong>Summary</strong></p>
<p>æœ¬ç ”ç©¶åˆ©ç”¨YOLOv11è¿›è¡Œè‹¹æœå¹¼æœçš„ä¸‰ç»´å§¿æ€ä¼°è®¡ï¼Œå¹¶ä¸YOLOv8è¿›è¡Œæ¯”è¾ƒã€‚ç»“æœæ˜¾ç¤ºYOLOv11åœ¨æ¡†ç²¾åº¦å’Œå§¿æ€ç²¾åº¦ä¸Šè¡¨ç°æ›´ä½³ï¼ŒåŒæ—¶å¤„ç†é€Ÿåº¦æ›´å¿«ã€‚æ­¤å¤–ï¼Œæ•´åˆVision Transformersï¼ˆViTï¼‰çš„æ·±åº¦ä¼°è®¡æ¨¡å‹Depth Anything V2å¯¹è‹¹æœå¹¼æœçš„é•¿åº¦ä¼°è®¡å±•ç°å‡ºæé«˜çš„ç²¾ç¡®åº¦ã€‚è¯¥ç ”ç©¶ä¸ºæœºå™¨äººè‡ªåŠ¨ç–æœåº”ç”¨ä¸­çš„è‹¹æœå¹¼æœä¸‰ç»´å§¿æ€ä¼°è®¡æä¾›äº†æœ‰åŠ›æ”¯æŒã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç ”ç©¶åˆ©ç”¨YOLOv11è¿›è¡Œè‹¹æœå¹¼æœçš„ä¸‰ç»´å§¿æ€ä¼°è®¡ï¼Œæ¶‰åŠå¯¹è±¡æ£€æµ‹å’Œå§¿æ€ä¼°è®¡ã€‚</li>
<li>YOLOv11çš„ä¸åŒé…ç½®åœ¨æ¡†ç²¾åº¦å’Œå§¿æ€ç²¾åº¦ä¸Šè¿›è¡Œäº†æ¯”è¾ƒï¼Œå…¶ä¸­YOLO11nè¡¨ç°æœ€ä½³ã€‚</li>
<li>YOLOv8åœ¨å¬å›ç‡æ–¹é¢è¡¨ç°å‡ºè¾ƒé«˜çš„æ€§èƒ½ã€‚</li>
<li>Vision Transformersï¼ˆViTï¼‰è¢«æ•´åˆç”¨äºæ·±åº¦ä¼°è®¡ï¼Œå…¶ä¸­Depth Anything V2åœ¨é•¿åº¦ä¼°è®¡æ–¹é¢è¡¨ç°å‡ºé«˜ç²¾ç¡®åº¦ã€‚</li>
<li>YOLOv11å¤„ç†å›¾åƒçš„é€Ÿåº¦éå¸¸å¿«ï¼Œä¸ºå®é™…åº”ç”¨æä¾›äº†è‰¯å¥½çš„å®æ—¶æ€§èƒ½ã€‚</li>
<li>è¯¥ç ”ç©¶æä¾›äº†ä¸€ä¸ªæœ‰å‰é€”çš„è§£å†³æ–¹æ¡ˆï¼Œå³æ•´åˆYOLOv11å’ŒDepth Anything Modelè¿›è¡Œè‹¹æœå¹¼æœçš„ä¸‰ç»´å§¿æ€ä¼°è®¡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2410.19846">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-0367619303e334055f007dd6c077241b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e8e844f550b4d61b54760aff6aecf768.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d04302418979873c1613027df315461a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ffe6967911603b22b91aaa9bb7abc240.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="Mixture-of-Experts-Made-Personalized-Federated-Prompt-Learning-for-Vision-Language-Models"><a href="#Mixture-of-Experts-Made-Personalized-Federated-Prompt-Learning-for-Vision-Language-Models" class="headerlink" title="Mixture of Experts Made Personalized: Federated Prompt Learning for   Vision-Language Models"></a>Mixture of Experts Made Personalized: Federated Prompt Learning for   Vision-Language Models</h2><p><strong>Authors:Jun Luo, Chen Chen, Shandong Wu</strong></p>
<p>Federated prompt learning benefits federated learning with CLIP-like Vision-Language Modelâ€™s (VLMâ€™s) robust representation learning ability through prompt learning. However, current federated prompt learning methods are habitually restricted to the traditional FL paradigm, where the participating clients are generally only allowed to download a single globally aggregated model from the server. While justifiable for training full-sized models under federated settings, in this work, we argue that this paradigm is ill-suited for lightweight prompts. By facilitating the clients to download multiple pre-aggregated prompts as fixed non-local experts, we propose Personalized Federated Mixture of Adaptive Prompts (pFedMoAP), a novel FL framework that personalizes the prompt learning process through the lens of Mixture of Experts (MoE). pFedMoAP implements a local attention-based gating network that learns to generate enhanced text features for better alignment with local image data, benefiting from both local and downloaded non-local adaptive prompt experts. Extensive experiments on 9 datasets under various federated settings demonstrate the efficacy of the proposed pFedMoAP algorithm. The code is available at <a target="_blank" rel="noopener" href="https://github.com/ljaiverson/pFedMoAP">https://github.com/ljaiverson/pFedMoAP</a>. </p>
<blockquote>
<p>è”é‚¦æç¤ºå­¦ä¹ é€šè¿‡æç¤ºå­¦ä¹ èƒ½åŠ›å¢å¼ºäº†è”é‚¦å­¦ä¹ åœ¨CLIPç±»è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰ä¸­çš„ç¨³å¥è¡¨ç¤ºå­¦ä¹ èƒ½åŠ›ã€‚ç„¶è€Œï¼Œå½“å‰çš„è”é‚¦æç¤ºå­¦ä¹ æ–¹æ³•é€šå¸¸å—é™äºä¼ ç»Ÿçš„è”é‚¦å­¦ä¹ ï¼ˆFLï¼‰èŒƒå¼ï¼Œåœ¨è¿™ç§èŒƒå¼ä¸‹ï¼Œå‚ä¸å®¢æˆ·ç«¯é€šå¸¸åªèƒ½ä»æœåŠ¡å™¨ä¸‹è½½å•ä¸ªå…¨å±€èšåˆæ¨¡å‹ã€‚è™½ç„¶åœ¨è”é‚¦è®¾ç½®ä¸‹è®­ç»ƒå…¨å°ºå¯¸æ¨¡å‹æ—¶è¿™ç§åšæ³•æ˜¯åˆç†çš„ï¼Œä½†åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬è®¤ä¸ºè¿™ç§èŒƒå¼ä¸é€‚åˆè½»é‡çº§æç¤ºã€‚é€šè¿‡ä¿ƒè¿›å®¢æˆ·ç«¯ä¸‹è½½å¤šä¸ªé¢„å…ˆèšåˆçš„æç¤ºä½œä¸ºå›ºå®šçš„éå±€éƒ¨ä¸“å®¶ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸ªæ€§åŒ–è”é‚¦æ··åˆè‡ªé€‚åº”æç¤ºï¼ˆpFedMoAPï¼‰ï¼Œè¿™æ˜¯ä¸€ç§é€šè¿‡ä¸“å®¶æ··åˆï¼ˆMoEï¼‰è§†è§’ä¸ªæ€§åŒ–æç¤ºå­¦ä¹ è¿‡ç¨‹çš„æ–°å‹è”é‚¦å­¦ä¹ æ¡†æ¶ã€‚pFedMoAPå®ç°äº†ä¸€ä¸ªåŸºäºæœ¬åœ°æ³¨æ„åŠ›çš„é—¨æ§ç½‘ç»œï¼Œå­¦ä¹ ç”Ÿæˆå¢å¼ºçš„æ–‡æœ¬ç‰¹å¾ï¼Œä»¥æ›´å¥½åœ°ä¸æœ¬åœ°å›¾åƒæ•°æ®å¯¹é½ï¼Œå—ç›Šäºæœ¬åœ°å’Œä¸‹è½½çš„éå±€éƒ¨è‡ªé€‚åº”æç¤ºä¸“å®¶ã€‚åœ¨å¤šç§è”é‚¦è®¾ç½®ä¸‹çš„9ä¸ªæ•°æ®é›†ä¸Šè¿›è¡Œçš„å¹¿æ³›å®éªŒè¯æ˜äº†æ‰€æå‡ºçš„pFedMoAPç®—æ³•çš„æœ‰æ•ˆæ€§ã€‚ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/ljaiverson/pFedMoAP%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/ljaiverson/pFedMoAPæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2410.10114v4">PDF</a> ICLR 2025</p>
<p><strong>Summary</strong><br>åŸºäºCLIPç±»ä¼¼çš„è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰çš„é²æ£’è¡¨ç¤ºå­¦ä¹ èƒ½åŠ›ï¼Œè”é‚¦æç¤ºå­¦ä¹ ï¼ˆFederated Prompt Learningï¼‰ä¸ºè”é‚¦å­¦ä¹ å¸¦æ¥äº†å¥½å¤„ã€‚ç„¶è€Œï¼Œå½“å‰çš„è”é‚¦æç¤ºå­¦ä¹ æ–¹æ³•é€šå¸¸å—é™äºä¼ ç»Ÿçš„è”é‚¦å­¦ä¹ ï¼ˆFLï¼‰èŒƒå¼ï¼Œå…¶ä¸­å‚ä¸å®¢æˆ·ç«¯é€šå¸¸ä»…å…è®¸ä»æœåŠ¡å™¨ä¸‹è½½å•ä¸ªå…¨å±€èšåˆæ¨¡å‹ã€‚ç„¶è€Œåœ¨è¯¥ç ”ç©¶ä¸­ï¼Œæˆ‘ä»¬è®¤ä¸ºè¯¥èŒƒå¼ä¸é€‚åˆç”¨äºè½»é‡çº§æç¤ºã€‚é€šè¿‡å…è®¸å®¢æˆ·ç«¯ä¸‹è½½å¤šä¸ªé¢„å…ˆèšåˆçš„æç¤ºä½œä¸ºå›ºå®šçš„éå±€éƒ¨ä¸“å®¶ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸ªæ€§åŒ–è”é‚¦æ··åˆè‡ªé€‚åº”æç¤ºï¼ˆpFedMoAPï¼‰è¿™ä¸€æ–°å‹FLæ¡†æ¶ï¼Œå®ƒé€šè¿‡ä¸“å®¶æ··åˆçš„è§†è§’ä¸ªæ€§åŒ–æç¤ºå­¦ä¹ è¿‡ç¨‹ã€‚pFedMoAPå®ç°äº†åŸºäºæœ¬åœ°æ³¨æ„åŠ›æœºåˆ¶çš„ç½‘å…³ç½‘ç»œï¼Œå­¦ä¹ ç”Ÿæˆå¢å¼ºçš„æ–‡æœ¬ç‰¹å¾ä»¥æ›´å¥½åœ°ä¸æœ¬åœ°å›¾åƒæ•°æ®å¯¹é½ï¼Œå—ç›Šäºæœ¬åœ°å’Œä¸‹è½½çš„éå±€éƒ¨è‡ªé€‚åº”æç¤ºä¸“å®¶ã€‚åœ¨å¤šç§è”é‚¦è®¾ç½®ä¸‹çš„ä¹ä¸ªæ•°æ®é›†ä¸Šçš„å¹¿æ³›å®éªŒè¯æ˜äº†pFedMoAPç®—æ³•çš„æœ‰æ•ˆæ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è”é‚¦æç¤ºå­¦ä¹ ç»“åˆäº†è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰çš„é²æ£’è¡¨ç¤ºå­¦ä¹ èƒ½åŠ›ï¼Œä¸ºè”é‚¦å­¦ä¹ å¸¦æ¥äº†å¥½å¤„ã€‚</li>
<li>å½“å‰è”é‚¦æç¤ºå­¦ä¹ æ–¹æ³•å—é™äºä¼ ç»Ÿè”é‚¦å­¦ä¹ èŒƒå¼ï¼Œä»…å…è®¸ä¸‹è½½å•ä¸€å…¨å±€æ¨¡å‹ã€‚</li>
<li>pFedMoAPæ¡†æ¶å…è®¸å®¢æˆ·ç«¯ä¸‹è½½å¤šä¸ªé¢„å…ˆèšåˆçš„æç¤ºï¼Œä½œä¸ºå›ºå®šçš„éå±€éƒ¨ä¸“å®¶ã€‚</li>
<li>pFedMoAPä¸ªæ€§åŒ–æç¤ºå­¦ä¹ è¿‡ç¨‹ï¼Œå€Ÿé‰´äº†æ··åˆä¸“å®¶çš„è§†è§’ã€‚</li>
<li>pFedMoAPé€šè¿‡å®ç°åŸºäºæœ¬åœ°æ³¨æ„åŠ›æœºåˆ¶çš„ç½‘å…³ç½‘ç»œï¼Œç”Ÿæˆå¢å¼ºçš„æ–‡æœ¬ç‰¹å¾ä»¥æ›´å¥½åœ°ä¸æœ¬åœ°å›¾åƒæ•°æ®å¯¹é½ã€‚</li>
<li>pFedMoAPå—ç›Šäºæœ¬åœ°å’Œä¸‹è½½çš„éå±€éƒ¨è‡ªé€‚åº”æç¤ºä¸“å®¶ã€‚</li>
<li>åœ¨ä¹ä¸ªæ•°æ®é›†ä¸Šçš„å®éªŒè¯æ˜äº†pFedMoAPç®—æ³•çš„æœ‰æ•ˆæ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2410.10114">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-fd4dcc334f18d827f441242a2369e1f0.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-0fa4948f80bbbc65df64185622faeb88.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-106d3744acf009344de64512364352e6.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="Can-language-guided-unsupervised-adaptation-improve-medical-image-classification-using-unpaired-images-and-texts"><a href="#Can-language-guided-unsupervised-adaptation-improve-medical-image-classification-using-unpaired-images-and-texts" class="headerlink" title="Can language-guided unsupervised adaptation improve medical image   classification using unpaired images and texts?"></a>Can language-guided unsupervised adaptation improve medical image   classification using unpaired images and texts?</h2><p><strong>Authors:Umaima Rahman, Raza Imam, Mohammad Yaqub, Boulbaba Ben Amor, Dwarikanath Mahapatra</strong></p>
<p>In medical image classification, supervised learning is challenging due to the scarcity of labeled medical images. To address this, we leverage the visual-textual alignment within Vision-Language Models (VLMs) to enable unsupervised learning of a medical image classifier. In this work, we propose \underline{Med}ical \underline{Un}supervised \underline{A}daptation (\texttt{MedUnA}) of VLMs, where the LLM-generated descriptions for each class are encoded into text embeddings and matched with class labels via a cross-modal adapter. This adapter attaches to a visual encoder of \texttt{MedCLIP} and aligns the visual embeddings through unsupervised learning, driven by a contrastive entropy-based loss and prompt tuning. Thereby, improving performance in scenarios where textual information is more abundant than labeled images, particularly in the healthcare domain. Unlike traditional VLMs, \texttt{MedUnA} uses \textbf{unpaired images and text} for learning representations and enhances the potential of VLMs beyond traditional constraints. We evaluate the performance on three chest X-ray datasets and two multi-class datasets (diabetic retinopathy and skin lesions), showing significant accuracy gains over the zero-shot baseline. Our code is available at <a target="_blank" rel="noopener" href="https://github.com/rumaima/meduna">https://github.com/rumaima/meduna</a>. </p>
<blockquote>
<p>åœ¨åŒ»å­¦å›¾åƒåˆ†ç±»ä¸­ï¼Œç”±äºç¼ºä¹æ ‡è®°çš„åŒ»å­¦å›¾åƒï¼Œç›‘ç£å­¦ä¹ é¢ä¸´æŒ‘æˆ˜ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬åˆ©ç”¨è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰ä¸­çš„è§†è§‰æ–‡æœ¬å¯¹é½åŠŸèƒ½ï¼Œå®ç°åŒ»å­¦å›¾åƒåˆ†ç±»å™¨çš„æ— ç›‘ç£å­¦ä¹ ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†åŒ»ç–—æ— ç›‘ç£é€‚é…ï¼ˆMedUnAï¼‰çš„æ–¹æ³•ï¼Œå…¶ä¸­å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¸ºæ¯ä¸€ç±»åˆ«ç”Ÿæˆçš„æè¿°è¢«ç¼–ç ä¸ºæ–‡æœ¬åµŒå…¥ï¼Œå¹¶é€šè¿‡è·¨æ¨¡æ€é€‚é…å™¨ä¸ç±»åˆ«æ ‡ç­¾ç›¸åŒ¹é…ã€‚è¯¥é€‚é…å™¨é™„åŠ åˆ°MedCLIPçš„è§†è§‰ç¼–ç å™¨ä¸Šï¼Œå¹¶é€šè¿‡æ— ç›‘ç£å­¦ä¹ å¯¹é½è§†è§‰åµŒå…¥ï¼Œä»¥å¯¹æ¯”ç†µæŸå¤±å’Œæç¤ºè°ƒæ•´ä¸ºåŸºç¡€è¿›è¡Œé©±åŠ¨ã€‚å› æ­¤ï¼Œåœ¨æ–‡æœ¬ä¿¡æ¯æ¯”æ ‡è®°å›¾åƒæ›´ä¸°å¯Œçš„æƒ…å†µä¸‹ï¼Œç‰¹åˆ«æ˜¯åœ¨åŒ»ç–—é¢†åŸŸï¼Œæé«˜äº†æ€§èƒ½ã€‚ä¸ä¼ ç»Ÿçš„VLMsä¸åŒï¼ŒMedUnAä½¿ç”¨éé…å¯¹å›¾åƒå’Œæ–‡æœ¬æ¥å­¦ä¹ è¡¨ç¤ºï¼Œå¹¶æé«˜äº†VLMsè¶…è¶Šä¼ ç»Ÿçº¦æŸçš„æ½œåŠ›ã€‚æˆ‘ä»¬åœ¨ä¸‰ä¸ªèƒ¸éƒ¨Xå°„çº¿æ•°æ®é›†å’Œä¸¤ä¸ªå¤šç±»æ•°æ®é›†ï¼ˆç³–å°¿ç—…è§†ç½‘è†œç—…å˜å’Œçš®è‚¤ç—…å˜ï¼‰ä¸Šè¯„ä¼°äº†æ€§èƒ½ï¼Œç›¸è¾ƒäºé›¶æ ·æœ¬åŸºçº¿ï¼Œæ˜¾ç¤ºå‡ºæ˜¾è‘—çš„å‡†ç¡®æ€§æå‡ã€‚æˆ‘ä»¬çš„ä»£ç ä½äº<a target="_blank" rel="noopener" href="https://github.com/rumaima/meduna%E3%80%82">https://github.com/rumaima/medunaã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2409.02729v2">PDF</a> Conference paper at International Symposium on Biomedical Imaging   (ISBI) 2025</p>
<p><strong>Summary</strong><br>åŒ»å­¦å›¾åƒåˆ†ç±»ä¸­ç›‘ç£å­¦ä¹ å› ç¼ºä¹æ ‡æ³¨åŒ»å­¦å›¾åƒè€Œå…·æœ‰æŒ‘æˆ˜æ€§ã€‚æœ¬ç ”ç©¶æå‡ºåˆ©ç”¨è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰ä¸­çš„è§†è§‰æ–‡æœ¬å¯¹é½æ¥å®ç°åŒ»å­¦å›¾åƒåˆ†ç±»å™¨çš„æ— ç›‘ç£å­¦ä¹ ã€‚æå‡ºMedical UnAæ–¹æ³•ï¼Œå°†å¤§å‹è¯­è¨€æ¨¡å‹ç”Ÿæˆçš„å„ç±»æè¿°ç¼–ç ä¸ºæ–‡æœ¬åµŒå…¥ï¼Œé€šè¿‡è·¨æ¨¡æ€é€‚é…å™¨ä¸ç±»æ ‡ç­¾åŒ¹é…ã€‚è¯¥é€‚é…å™¨è¿æ¥åˆ°MedCLIPçš„è§†è§‰ç¼–ç å™¨ï¼Œé€šè¿‡å¯¹æ— ç›‘ç£å­¦ä¹ é©±åŠ¨çš„å¯¹é½ç”Ÿæˆå¯¹æ¯”ç†µæŸå¤±å’Œæç¤ºè°ƒæ•´ï¼Œä»è€Œæé«˜åœ¨æ–‡æœ¬ä¿¡æ¯ä¸°å¯Œäºæ ‡æ³¨å›¾åƒçš„åœºæ™¯ä¸­çš„æ€§èƒ½ï¼Œç‰¹åˆ«æ˜¯åœ¨åŒ»ç–—é¢†åŸŸã€‚ç›¸è¾ƒäºä¼ ç»ŸVLMsï¼ŒMedUnAä½¿ç”¨éé…å¯¹å›¾åƒå’Œæ–‡æœ¬å­¦ä¹ è¡¨å¾ï¼Œçªç ´äº†ä¼ ç»Ÿçº¦æŸï¼Œæé«˜äº†VLMsçš„æ½œåŠ›ã€‚åœ¨ä¸‰ä¸ªèƒ¸Xå…‰æ•°æ®é›†å’Œä¸¤ä¸ªå¤šç±»æ•°æ®é›†ï¼ˆç³–å°¿ç—…è§†ç½‘è†œç—…å˜å’Œçš®è‚¤ç—…å˜ï¼‰ä¸Šçš„è¯„ä¼°æ˜¾ç¤ºï¼Œç›¸è¾ƒäºé›¶æ ·æœ¬åŸºçº¿æœ‰æ˜¾è‘—ç²¾åº¦æå‡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>åŒ»å­¦å›¾åƒåˆ†ç±»é¢ä¸´ç›‘ç£å­¦ä¹ æŒ‘æˆ˜ï¼Œå› ç¼ºä¹æ ‡æ³¨åŒ»å­¦å›¾åƒã€‚</li>
<li>åˆ©ç”¨è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰çš„è§†è§‰æ–‡æœ¬å¯¹é½å®ç°æ— ç›‘ç£å­¦ä¹ ã€‚</li>
<li>æå‡ºMedical UnAæ–¹æ³•ï¼Œç»“åˆå¤§å‹è¯­è¨€æ¨¡å‹çš„ç±»æè¿°ä¸è·¨æ¨¡æ€é€‚é…å™¨ã€‚</li>
<li>é€‚é…å™¨ä¸MedCLIPçš„è§†è§‰ç¼–ç å™¨ç»“åˆï¼Œé€šè¿‡å¯¹æ¯”ç†µæŸå¤±å’Œæç¤ºè°ƒæ•´å®ç°è§†è§‰åµŒå…¥å¯¹é½ã€‚</li>
<li>åœ¨æ–‡æœ¬ä¿¡æ¯ä¸°å¯Œäºæ ‡æ³¨å›¾åƒçš„åœºæ™¯ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œç‰¹åˆ«æ˜¯åœ¨åŒ»ç–—é¢†åŸŸã€‚</li>
<li>MedUnAä½¿ç”¨éé…å¯¹å›¾åƒå’Œæ–‡æœ¬å­¦ä¹ è¡¨å¾ï¼Œçªç ´ä¼ ç»ŸVLMsçš„çº¦æŸã€‚</li>
<li>åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šçš„è¯„ä¼°æ˜¾ç¤ºï¼Œç›¸è¾ƒäºé›¶æ ·æœ¬åŸºçº¿æœ‰æ˜¾è‘—ç²¾åº¦æå‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2409.02729">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-73737be3acd911118ec9d376907ef5da.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-50318993b5bc361646f35c05a59bcee4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-545e3673199812d918cb1efbb1fdfbd4.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a1cab6540d78d4b7817187dc0f07e9c4.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="Cropper-Vision-Language-Model-for-Image-Cropping-through-In-Context-Learning"><a href="#Cropper-Vision-Language-Model-for-Image-Cropping-through-In-Context-Learning" class="headerlink" title="Cropper: Vision-Language Model for Image Cropping through In-Context   Learning"></a>Cropper: Vision-Language Model for Image Cropping through In-Context   Learning</h2><p><strong>Authors:Seung Hyun Lee, Jijun Jiang, Yiran Xu, Zhuofang Li, Junjie Ke, Yinxiao Li, Junfeng He, Steven Hickson, Katie Datsenko, Sangpil Kim, Ming-Hsuan Yang, Irfan Essa, Feng Yang</strong></p>
<p>The goal of image cropping is to identify visually appealing crops in an image. Conventional methods are trained on specific datasets and fail to adapt to new requirements. Recent breakthroughs in large vision-language models (VLMs) enable visual in-context learning without explicit training. However, downstream tasks with VLMs remain under explored. In this paper, we propose an effective approach to leverage VLMs for image cropping. First, we propose an efficient prompt retrieval mechanism for image cropping to automate the selection of in-context examples. Second, we introduce an iterative refinement strategy to iteratively enhance the predicted crops. The proposed framework, we refer to as Cropper, is applicable to a wide range of cropping tasks, including free-form cropping, subject-aware cropping, and aspect ratio-aware cropping. Extensive experiments demonstrate that Cropper significantly outperforms state-of-the-art methods across several benchmarks. </p>
<blockquote>
<p>å›¾åƒè£å‰ªçš„ç›®æ ‡æ˜¯è¯†åˆ«å›¾åƒä¸­è§†è§‰å¸å¼•åŠ›å¼ºçš„éƒ¨åˆ†ã€‚ä¼ ç»Ÿæ–¹æ³•æ˜¯åœ¨ç‰¹å®šæ•°æ®é›†ä¸Šè¿›è¡Œè®­ç»ƒï¼Œæ— æ³•é€‚åº”æ–°çš„è¦æ±‚ã€‚æœ€è¿‘å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰çš„çªç ´ä½¿å¾—æ— éœ€æ˜ç¡®è®­ç»ƒå³å¯è¿›è¡Œè§†è§‰ä¸Šä¸‹æ–‡å­¦ä¹ ã€‚ç„¶è€Œï¼Œä½¿ç”¨VLMçš„ä¸‹æ¸¸ä»»åŠ¡ä»ç„¶æœ‰å¾…æ¢ç´¢ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åˆ©ç”¨VLMè¿›è¡Œå›¾åƒè£å‰ªçš„æœ‰æ•ˆæ–¹æ³•ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§é«˜æ•ˆçš„æç¤ºæ£€ç´¢æœºåˆ¶ï¼Œç”¨äºè‡ªåŠ¨é€‰æ‹©ä¸Šä¸‹æ–‡ç¤ºä¾‹æ¥è¿›è¡Œå›¾åƒè£å‰ªã€‚å…¶æ¬¡ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§è¿­ä»£ä¼˜åŒ–ç­–ç•¥ï¼Œä»¥é€æ­¥æ”¹è¿›é¢„æµ‹çš„è£å‰ªéƒ¨åˆ†ã€‚æˆ‘ä»¬ç§°æå‡ºçš„æ¡†æ¶ä¸ºâ€œè£å‰ªå™¨ï¼ˆCropperï¼‰â€ï¼Œé€‚ç”¨äºå„ç§è£å‰ªä»»åŠ¡ï¼ŒåŒ…æ‹¬è‡ªç”±å½¢å¼è£å‰ªã€ä¸»é¢˜æ„ŸçŸ¥è£å‰ªå’Œæ¯”ä¾‹æ„ŸçŸ¥è£å‰ªã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼ŒCropperåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸Šçš„è¡¨ç°å‡ä¼˜äºæœ€æ–°æ–¹æ³•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2408.07790v2">PDF</a> </p>
<p><strong>Summary</strong><br>å›¾åƒè£å‰ªçš„ç›®æ ‡æ˜¯è¯†åˆ«å›¾åƒä¸­çš„è§†è§‰å¸å¼•åŠ›åŒºåŸŸã€‚ä¼ ç»Ÿæ–¹æ³•ä¾èµ–äºç‰¹å®šæ•°æ®é›†çš„è®­ç»ƒï¼Œéš¾ä»¥é€‚åº”æ–°è¦æ±‚ã€‚æœ€è¿‘çš„å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰çš„çªç ´å®ç°äº†æ— éœ€æ˜ç¡®è®­ç»ƒçš„è§†è§‰ä¸Šä¸‹æ–‡å­¦ä¹ ã€‚ç„¶è€Œï¼Œå¯¹äºVLMsçš„ä¸‹æ¸¸ä»»åŠ¡ä»æœ‰å¾…æ¢ç´¢ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§æœ‰æ•ˆåˆ©ç”¨VLMsè¿›è¡Œå›¾åƒè£å‰ªçš„æ–¹æ³•ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬æå‡ºäº†æœ‰æ•ˆçš„æç¤ºæ£€ç´¢æœºåˆ¶ï¼Œä»¥è‡ªåŠ¨åŒ–é€‰æ‹©ä¸Šä¸‹æ–‡ç¤ºä¾‹è¿›è¡Œå›¾åƒè£å‰ªã€‚å…¶æ¬¡ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§è¿­ä»£ä¼˜åŒ–ç­–ç•¥ï¼Œä»¥é€æ­¥æ”¹è¿›é¢„æµ‹çš„è£å‰ªåŒºåŸŸã€‚æˆ‘ä»¬ç§°æ‰€æå‡ºçš„æ¡†æ¶ä¸ºâ€œCropperâ€ï¼Œé€‚ç”¨äºå¤šç§è£å‰ªä»»åŠ¡ï¼ŒåŒ…æ‹¬è‡ªç”±å½¢å¼è£å‰ªã€ä¸»é¢˜æ„ŸçŸ¥è£å‰ªå’Œæ¯”ä¾‹æ„ŸçŸ¥è£å‰ªã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼ŒCropperåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­æ˜¾è‘—ä¼˜äºæœ€æ–°æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å›¾åƒè£å‰ªçš„ç›®æ ‡æ˜¯è¯†åˆ«å›¾åƒä¸­çš„è§†è§‰å¸å¼•åŠ›åŒºåŸŸã€‚</li>
<li>ä¼ ç»Ÿå›¾åƒè£å‰ªæ–¹æ³•ä¾èµ–äºç‰¹å®šæ•°æ®é›†çš„è®­ç»ƒï¼Œéš¾ä»¥é€‚åº”æ–°è¦æ±‚ã€‚</li>
<li>å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰çš„çªç ´å®ç°äº†è§†è§‰ä¸Šä¸‹æ–‡å­¦ä¹ çš„è‡ªåŠ¨åŒ–ã€‚</li>
<li>æœ¬æ–‡æå‡ºäº†æœ‰æ•ˆçš„æç¤ºæ£€ç´¢æœºåˆ¶å’Œè¿­ä»£ä¼˜åŒ–ç­–ç•¥æ¥è¿›è¡Œå›¾åƒè£å‰ªã€‚</li>
<li>Cropperæ¡†æ¶é€‚ç”¨äºå¤šç§å›¾åƒè£å‰ªä»»åŠ¡ï¼ŒåŒ…æ‹¬è‡ªç”±å½¢å¼ã€ä¸»é¢˜æ„ŸçŸ¥å’Œæ¯”ä¾‹æ„ŸçŸ¥è£å‰ªã€‚</li>
<li>å®éªŒç»“æœè¡¨æ˜ï¼ŒCropperåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­æ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ã€‚</li>
<li>è¯¥ç ”ç©¶ä¸ºåˆ©ç”¨VLMsåœ¨å›¾åƒè£å‰ªä»»åŠ¡ä¸­çš„æ½œåŠ›å¼€è¾Ÿäº†æ–°çš„é“è·¯ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2408.07790">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-86df3708d1736df73ab8238f40edf824.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d1b7c0a66857c2d9bcfb3c8ecc27d1c5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4eb9a46417d22214f3a4e6327e11c85f.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-ae5faa138de981cb3fe0df4ce014728d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-25e3e9dd9b472be34911fea32899ffb6.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-200ba9b7783e3db9d9098717b4fd364d.jpg" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="Vision-Transformers-for-End-to-End-Vision-Based-Quadrotor-Obstacle-Avoidance"><a href="#Vision-Transformers-for-End-to-End-Vision-Based-Quadrotor-Obstacle-Avoidance" class="headerlink" title="Vision Transformers for End-to-End Vision-Based Quadrotor Obstacle   Avoidance"></a>Vision Transformers for End-to-End Vision-Based Quadrotor Obstacle   Avoidance</h2><p><strong>Authors:Anish Bhattacharya, Nishanth Rao, Dhruv Parikh, Pratik Kunapuli, Yuwei Wu, Yuezhan Tao, Nikolai Matni, Vijay Kumar</strong></p>
<p>We demonstrate the capabilities of an attention-based end-to-end approach for high-speed vision-based quadrotor obstacle avoidance in dense, cluttered environments, with comparison to various state-of-the-art learning architectures. Quadrotor unmanned aerial vehicles (UAVs) have tremendous maneuverability when flown fast; however, as flight speed increases, traditional model-based approaches to navigation via independent perception, mapping, planning, and control modules breaks down due to increased sensor noise, compounding errors, and increased processing latency. Thus, learning-based, end-to-end vision-to-control networks have shown to have great potential for online control of these fast robots through cluttered environments. We train and compare convolutional, U-Net, and recurrent architectures against vision transformer (ViT) models for depth image-to-control in high-fidelity simulation, observing that ViT models are more effective than others as quadrotor speeds increase and in generalization to unseen environments, while the addition of recurrence further improves performance while reducing quadrotor energy cost across all tested flight speeds. We assess performance at speeds of up to 7m&#x2F;s in simulation and hardware. To the best of our knowledge, this is the first work to utilize vision transformers for end-to-end vision-based quadrotor control. </p>
<blockquote>
<p>æˆ‘ä»¬å±•ç¤ºäº†ä¸€ç§åŸºäºæ³¨æ„åŠ›çš„ç«¯åˆ°ç«¯æ–¹æ³•ï¼Œç”¨äºé«˜é€Ÿè§†è§‰æ— äººæœºåœ¨å¯†é›†æ‚ä¹±ç¯å¢ƒä¸­çš„é¿éšœèƒ½åŠ›ï¼Œå¹¶ä¸å„ç§æœ€æ–°å­¦ä¹ æ¶æ„è¿›è¡Œäº†æ¯”è¾ƒã€‚å››æ—‹ç¿¼æ— äººæœºåœ¨é«˜é€Ÿé£è¡Œæ—¶å…·æœ‰å·¨å¤§çš„æœºåŠ¨æ€§ï¼›ç„¶è€Œï¼Œéšç€é£è¡Œé€Ÿåº¦çš„å¢åŠ ï¼ŒåŸºäºç‹¬ç«‹æ„ŸçŸ¥ã€æ˜ å°„ã€è§„åˆ’å’Œæ§åˆ¶æ¨¡å—çš„åŸºäºä¼ ç»Ÿæ¨¡å‹çš„æ–¹æ³•ç”±äºä¼ æ„Ÿå™¨å™ªå£°å¢åŠ ã€è¯¯å·®ç´¯ç§¯å’Œå¤„ç†å»¶è¿Ÿå¢åŠ è€Œå¤±æ•ˆã€‚å› æ­¤ï¼ŒåŸºäºå­¦ä¹ çš„ç«¯åˆ°ç«¯è§†è§‰æ§åˆ¶ç½‘ç»œå¯¹äºé€šè¿‡è¿™äº›æ‚ä¹±ç¯å¢ƒçš„å¿«é€Ÿæœºå™¨äººçš„åœ¨çº¿æ§åˆ¶æ˜¾ç¤ºå‡ºå·¨å¤§æ½œåŠ›ã€‚æˆ‘ä»¬åœ¨é«˜ä¿çœŸæ¨¡æ‹Ÿä¸­å¯¹å·ç§¯ã€U-Netå’Œé€’å½’æ¶æ„ä¸è§†è§‰Transformerï¼ˆViTï¼‰æ¨¡å‹è¿›è¡Œäº†æ·±åº¦å›¾åƒæ§åˆ¶æ–¹é¢çš„è®­ç»ƒå’Œæ¯”è¾ƒï¼Œè§‚å¯Ÿåˆ°éšç€å››æ—‹ç¿¼æ— äººæœºé€Ÿåº¦çš„å¢åŠ ä»¥åŠåœ¨æœªè§è¿‡ç¯å¢ƒçš„æ³›åŒ–ä¸­ï¼ŒViTæ¨¡å‹çš„æ•ˆæœæ¯”å…¶ä»–æ¨¡å‹æ›´æœ‰æ•ˆï¼Œè€Œé€’å½’çš„åŠ å…¥è¿›ä¸€æ­¥æé«˜äº†æ€§èƒ½ï¼ŒåŒæ—¶é™ä½äº†æ‰€æœ‰æµ‹è¯•é£è¡Œé€Ÿåº¦ä¸‹çš„å››æ—‹ç¿¼èƒ½é‡æˆæœ¬ã€‚æˆ‘ä»¬åœ¨æ¨¡æ‹Ÿå’Œç¡¬ä»¶ä¸­ä»¥æœ€é«˜7ç±³&#x2F;ç§’çš„é€Ÿåº¦è¯„ä¼°æ€§èƒ½ã€‚æ®æˆ‘ä»¬æ‰€çŸ¥ï¼Œè¿™æ˜¯é¦–æ¬¡åˆ©ç”¨è§†è§‰Transformerè¿›è¡Œç«¯åˆ°ç«¯çš„åŸºäºè§†è§‰çš„å››æ—‹ç¿¼æ— äººæœºæ§åˆ¶çš„ç ”ç©¶ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2405.10391v3">PDF</a> 11 pages, 18 figures, 3 tables (with supplementary)</p>
<p><strong>Summary</strong></p>
<p>åŸºäºæ³¨æ„åŠ›çš„ç«¯åˆ°ç«¯æ–¹æ³•ï¼Œå±•ç¤ºäº†åœ¨å¯†é›†æ‚ä¹±ç¯å¢ƒä¸­é«˜é€Ÿæ— äººæœºè§†è§‰é¿éšœçš„èƒ½åŠ›ï¼Œå¹¶ä¸å„ç§æœ€æ–°å­¦ä¹ æ¶æ„è¿›è¡Œäº†æ¯”è¾ƒã€‚éšç€æ— äººæœºé£è¡Œé€Ÿåº¦çš„å¢åŠ ï¼Œä¼ ç»ŸåŸºäºæ¨¡å‹çš„å¯¼èˆªæ–¹æ³•å› ä¼ æ„Ÿå™¨å™ªå£°å¢åŠ ã€è¯¯å·®ç´¯ç§¯å’Œå¤„ç†å»¶è¿Ÿè€Œå‡ºç°ç“¶é¢ˆã€‚å› æ­¤ï¼ŒåŸºäºå­¦ä¹ çš„ç«¯åˆ°ç«¯è§†è§‰æ§åˆ¶ç½‘ç»œåœ¨æ§åˆ¶è¿™äº›å¿«é€Ÿæœºå™¨äººç©¿è¶Šæ‚ä¹±ç¯å¢ƒæ–¹é¢å…·æœ‰å·¨å¤§æ½œåŠ›ã€‚åœ¨ä»¿çœŸç¯å¢ƒä¸­å¯¹å·ç§¯ç¥ç»ç½‘ç»œã€U-Netç½‘ç»œå’Œé€’å½’æ¶æ„è¿›è¡Œäº†å¯¹æ¯”è¯„ä¼°ï¼Œå‘ç°éšç€æ— äººæœºé€Ÿåº¦çš„å¢åŠ ä»¥åŠæ¨å¹¿åˆ°æœªè§çš„ç¯å¢ƒï¼Œä½¿ç”¨ViTæ¨¡å‹çš„æ€§èƒ½æ›´ä¼˜ç§€ã€‚æ‰€ç ”ç©¶é€Ÿåº¦è¾¾åˆ°æœ€é«˜7ç±³æ¯ç§’å·¦å³ï¼Œè¯¥é¡¹å·¥ä½œæ˜¯é¦–æ¬¡å°†è§†è§‰è½¬æ¢å™¨åº”ç”¨äºç«¯åˆ°ç«¯çš„æ— äººæœºè§†è§‰æ§åˆ¶ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<p>ä»¥ä¸‹æ˜¯åŸºäºæ–‡æœ¬çš„é‡è¦è§è§£åˆ—è¡¨ï¼š</p>
<ul>
<li>å±•ç¤ºäº†æ³¨æ„åŠ›æœºåˆ¶åœ¨é«˜é€Ÿæ— äººæœºè§†è§‰é¿éšœä¸­çš„æœ‰æ•ˆæ€§ã€‚</li>
<li>å¯¹æ¯”äº†å¤šç§å­¦ä¹ æ¶æ„ï¼Œå‘ç°ViTæ¨¡å‹åœ¨ä»¿çœŸç¯å¢ƒä¸­æ€§èƒ½ä¼˜è¶Šã€‚</li>
<li>éšç€æ— äººæœºé£è¡Œé€Ÿåº¦çš„å¢åŠ ï¼Œä¼ ç»Ÿå¯¼èˆªæ–¹æ³•å­˜åœ¨å±€é™æ€§ã€‚</li>
<li>åŸºäºå­¦ä¹ çš„ç«¯åˆ°ç«¯è§†è§‰æ§åˆ¶ç½‘ç»œå…·æœ‰å·¨å¤§æ½œåŠ›ã€‚</li>
<li>åœ¨é«˜ä¿çœŸä»¿çœŸå’Œç¡¬ä»¶æµ‹è¯•ä¸­éƒ½è¯„ä¼°äº†æ¨¡å‹æ€§èƒ½ã€‚</li>
<li>Vision Transformeråœ¨é«˜é€Ÿç¯å¢ƒä¸‹çš„æ€§èƒ½ä¼˜å¼‚å¹¶å…·å¤‡è‰¯å¥½çš„æ³›åŒ–èƒ½åŠ›ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2405.10391">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-df4336138f5aefb9d35053a015041028.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d87e71e2ed7e705dd42daf4795a401e7.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-070682aedff4a4d8c77b78c30737ff56.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9880d4a8b6310ac44d1511fdf4a14007.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b42013e629e823102ac9f27031a09a2f.jpg" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="VL-ICL-Bench-The-Devil-in-the-Details-of-Multimodal-In-Context-Learning"><a href="#VL-ICL-Bench-The-Devil-in-the-Details-of-Multimodal-In-Context-Learning" class="headerlink" title="VL-ICL Bench: The Devil in the Details of Multimodal In-Context Learning"></a>VL-ICL Bench: The Devil in the Details of Multimodal In-Context Learning</h2><p><strong>Authors:Yongshuo Zong, Ondrej Bohdal, Timothy Hospedales</strong></p>
<p>Large language models (LLMs) famously exhibit emergent in-context learning (ICL) â€“ the ability to rapidly adapt to new tasks using few-shot examples provided as a prompt, without updating the modelâ€™s weights. Built on top of LLMs, vision large language models (VLLMs) have advanced significantly in areas such as recognition, reasoning, and grounding. However, investigations into \emph{multimodal ICL} have predominantly focused on few-shot visual question answering (VQA), and image captioning, which we will show neither exploit the strengths of ICL, nor test its limitations. The broader capabilities and limitations of multimodal ICL remain under-explored. In this study, we introduce a comprehensive benchmark VL-ICL Bench for multimodal in-context learning, encompassing a broad spectrum of tasks that involve both images and text as inputs and outputs, and different types of challenges, from {perception to reasoning and long context length}. We evaluate the abilities of state-of-the-art VLLMs against this benchmark suite, revealing their diverse strengths and weaknesses, and showing that even the most advanced models, such as GPT-4, find the tasks challenging. By highlighting a range of new ICL tasks, and the associated strengths and limitations of existing models, we hope that our dataset will inspire future work on enhancing the in-context learning capabilities of VLLMs, as well as inspire new applications that leverage VLLM ICL. The code and dataset are available at <a target="_blank" rel="noopener" href="https://github.com/ys-zong/VL-ICL">https://github.com/ys-zong/VL-ICL</a>. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è¡¨ç°å‡ºçªå‡ºçš„ä¸Šä¸‹æ–‡ç¯å¢ƒå­¦ä¹ ï¼ˆICLï¼‰èƒ½åŠ›â€”â€”å³ä½¿ç”¨ä½œä¸ºæç¤ºæä¾›çš„å°‘é‡ç¤ºä¾‹å¿«é€Ÿé€‚åº”æ–°ä»»åŠ¡çš„èƒ½åŠ›ï¼Œè€Œæ— éœ€æ›´æ–°æ¨¡å‹çš„æƒé‡ã€‚å»ºç«‹åœ¨å¤§å‹è¯­è¨€æ¨¡å‹ä¹‹ä¸Šçš„è§†è§‰å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆVLLMï¼‰åœ¨è¯†åˆ«ã€æ¨ç†å’Œæ¥åœ°ç­‰é¢†åŸŸå–å¾—äº†æ˜¾è‘—è¿›å±•ã€‚ç„¶è€Œï¼Œå¯¹å¤šæ¨¡æ€ICLçš„è°ƒæŸ¥ä¸»è¦é›†ä¸­åœ¨å°‘é‡çš„è§†è§‰é—®ç­”å’Œå›¾åƒæè¿°ä¸Šï¼Œæˆ‘ä»¬å°†å±•ç¤ºè¿™ä¸¤è€…éƒ½æ²¡æœ‰åˆ©ç”¨ICLçš„ä¼˜åŠ¿ï¼Œä¹Ÿæ²¡æœ‰æµ‹è¯•å…¶å±€é™æ€§ã€‚å¤šæ¨¡æ€ICLçš„æ›´å¹¿æ³›çš„èƒ½åŠ›å’Œå±€é™æ€§å°šæœªè¢«å……åˆ†æ¢ç´¢ã€‚åœ¨è¿™é¡¹ç ”ç©¶ä¸­ï¼Œæˆ‘ä»¬å¼•å…¥äº†å…¨é¢çš„å¤šæ¨¡æ€ä¸Šä¸‹æ–‡å­¦ä¹ åŸºå‡†æµ‹è¯•VL-ICL Benchï¼Œæ¶µç›–äº†ä¸€ç³»åˆ—æ¶‰åŠå›¾åƒå’Œæ–‡æœ¬ä½œä¸ºè¾“å…¥å’Œè¾“å‡ºçš„ä»»åŠ¡ï¼Œä»¥åŠä»æ„ŸçŸ¥åˆ°æ¨ç†å’Œé•¿ä¸Šä¸‹æ–‡é•¿åº¦çš„ä¸åŒç±»å‹æŒ‘æˆ˜ã€‚æˆ‘ä»¬è¯„ä¼°äº†æœ€å…ˆè¿›VLLMå¯¹æ­¤åŸºå‡†æµ‹è¯•å¥—ä»¶çš„èƒ½åŠ›ï¼Œæ­ç¤ºäº†å®ƒä»¬å„è‡ªçš„ä¼˜åŠ¿å’ŒåŠ£åŠ¿ï¼Œå¹¶è¡¨æ˜å³ä½¿æ˜¯æœ€å…ˆè¿›çš„æ¨¡å‹ï¼Œå¦‚GPT-4ï¼Œä¹Ÿä¼šå‘ç°è¿™äº›ä»»åŠ¡å…·æœ‰æŒ‘æˆ˜æ€§ã€‚é€šè¿‡çªå‡ºä¸€ç³»åˆ—æ–°çš„ICLä»»åŠ¡ä»¥åŠç°æœ‰æ¨¡å‹çš„å…³è”ä¼˜åŠ¿å’Œå±€é™æ€§ï¼Œæˆ‘ä»¬å¸Œæœ›æˆ‘ä»¬çš„æ•°æ®é›†å°†æ¿€åŠ±æœªæ¥å¯¹å¢å¼ºVLLMçš„ä¸Šä¸‹æ–‡å­¦ä¹ èƒ½åŠ›çš„è¿›ä¸€æ­¥ç ”ç©¶ï¼Œå¹¶æ¿€å‘åˆ©ç”¨VLLM ICLçš„æ–°åº”ç”¨ã€‚ä»£ç å’Œæ•°æ®é›†å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/ys-zong/VL-ICL">https://github.com/ys-zong/VL-ICL</a>è·å¾—ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2403.13164v4">PDF</a> ICLR 2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†å¯¹å¤šæ¨¡æ€ä¸Šä¸‹æ–‡å­¦ä¹ ï¼ˆICLï¼‰çš„æ·±å…¥ç ”ç©¶ã€‚é’ˆå¯¹è§†è§‰å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆVLLMï¼‰çš„èƒ½åŠ›ä¸å±€é™ï¼Œå»ºç«‹äº†ä¸€ä¸ªå…¨é¢çš„åŸºå‡†æµ‹è¯•VL-ICL Benchã€‚è¯¥åŸºå‡†æµ‹è¯•æ¶µç›–äº†å¹¿æ³›çš„æ¶‰åŠå›¾åƒå’Œæ–‡æœ¬çš„ä»»åŠ¡ï¼Œä»æ„ŸçŸ¥åˆ°æ¨ç†å’Œé•¿è¯­å¢ƒé•¿åº¦ç­‰ä¸åŒç±»å‹æŒ‘æˆ˜ã€‚é€šè¿‡å¯¹æœ€æ–°VLLMçš„è¯„ä¼°ï¼Œæ­ç¤ºäº†å®ƒä»¬çš„ä¼˜åŠ¿å’Œä¸è¶³ï¼Œå¹¶æŒ‡å‡ºå³ä½¿æ˜¯æœ€å…ˆè¿›çš„æ¨¡å‹å¦‚GPT-4ä¹Ÿé¢ä¸´æŒ‘æˆ˜ã€‚æœ¬æ–‡æ—¨åœ¨é€šè¿‡çªå‡ºæ–°çš„ICLä»»åŠ¡ä»¥åŠç°æœ‰æ¨¡å‹çš„ä¼˜ç¼ºç‚¹æ¥å¯å‘æœªæ¥å·¥ä½œï¼Œä»¥å¢å¼ºVLLMçš„ä¸Šä¸‹æ–‡å­¦ä¹ èƒ½åŠ›å¹¶æ¨åŠ¨æ–°åº”ç”¨çš„å‘å±•ã€‚ä»£ç å’Œæ•°æ®é›†å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/ys-zong/VL-ICL%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/ys-zong/VL-ICLæ‰¾åˆ°ã€‚</a></p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å±•ç°å‡ºä¸Šä¸‹æ–‡å­¦ä¹ èƒ½åŠ›ï¼ˆICLï¼‰ï¼Œèƒ½å¿«é€Ÿé€‚åº”æ–°ä»»åŠ¡ï¼Œä¸”æ— éœ€æ›´æ–°æ¨¡å‹æƒé‡ã€‚</li>
<li>è§†è§‰å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆVLLMsï¼‰åœ¨è¯†åˆ«ã€æ¨ç†å’Œæ¥åœ°ç­‰é¢†åŸŸæœ‰æ˜¾è‘—è¿›å±•ã€‚</li>
<li>å¤šæ¨¡æ€ICLçš„ç ”ç©¶ä¸»è¦é›†ä¸­äºè§†è§‰é—®ç­”å’Œå›¾åƒæè¿°ç”Ÿæˆï¼Œä½†è¿™å¹¶æœªå……åˆ†åˆ©ç”¨ICLçš„ä¼˜åŠ¿ï¼Œä¹Ÿæœªæµ‹è¯•å…¶å±€é™æ€§ã€‚</li>
<li>å¼•å…¥äº†ä¸€ä¸ªå…¨é¢çš„åŸºå‡†æµ‹è¯•VL-ICL Benchï¼Œæ¶µç›–æ¶‰åŠå›¾åƒå’Œæ–‡æœ¬çš„å¤šæ¨¡æ€ICLçš„å¹¿æ³›ä»»åŠ¡ï¼ŒåŒ…æ‹¬æ„ŸçŸ¥ã€æ¨ç†å’Œé•¿è¯­å¢ƒé•¿åº¦ç­‰æŒ‘æˆ˜ã€‚</li>
<li>è¯„ä¼°äº†æœ€æ–°VLLMçš„èƒ½åŠ›ï¼Œæ­ç¤ºäº†å…¶ä¼˜åŠ¿å’Œä¸è¶³ï¼Œå¹¶æŒ‡å‡ºGPT-4ç­‰å…ˆè¿›æ¨¡å‹ä¹Ÿé¢ä¸´æŒ‘æˆ˜ã€‚</li>
<li>å¸Œæœ›æ­¤åŸºå‡†æµ‹è¯•èƒ½å¯å‘æœªæ¥å¯¹å¢å¼ºVLLMçš„ä¸Šä¸‹æ–‡å­¦ä¹ èƒ½åŠ›çš„ç ”ç©¶å’Œæ–°åº”ç”¨çš„å¼€å‘ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2403.13164">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-b999c539bc388e044c297bd9cc8f8c98.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-615437abc2988c94e3bfd80abd3eedc5.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-75bfc5198c7b62621bde4c7858123edc.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-02ef11c2143c9259c5c08e36eac8cbab.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-1e860e2d71c7734fe3e5ee796c28356f.jpg" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1><h2 id="Approximate-Nullspace-Augmented-Finetuning-for-Robust-Vision-Transformers"><a href="#Approximate-Nullspace-Augmented-Finetuning-for-Robust-Vision-Transformers" class="headerlink" title="Approximate Nullspace Augmented Finetuning for Robust Vision   Transformers"></a>Approximate Nullspace Augmented Finetuning for Robust Vision   Transformers</h2><p><strong>Authors:Haoyang Liu, Aditya Singh, Yijiang Li, Haohan Wang</strong></p>
<p>Enhancing the robustness of deep learning models, particularly in the realm of vision transformers (ViTs), is crucial for their real-world deployment. In this work, we provide a finetuning approach to enhance the robustness of vision transformers inspired by the concept of nullspace from linear algebra. Our investigation centers on whether a vision transformer can exhibit resilience to input variations akin to the nullspace property in linear mappings, which would imply that perturbations sampled from this nullspace do not influence the modelâ€™s output when added to the input. We start from the observation that many existing ViTs satisfy this property because their patch embedding layer has a non-trivial nullspace. Then, we extend the notion of nullspace to nonlinear settings and demonstrate that it is possible to synthesize approximate nullspace elements for ViTâ€™s encoder blocks through optimization. Finally, we propose a finetuning strategy for ViTs wherein we augment the training data with synthesized approximate nullspace noise. We find that our finetuning approach significantly improves the modelsâ€™ robustness to both adversarial and natural image perturbations.\footnote{Code is available at: <a target="_blank" rel="noopener" href="https://github.com/Liu-Hy/ns-vit">https://github.com/Liu-Hy/ns-vit</a>. </p>
<blockquote>
<p>å¢å¼ºæ·±åº¦å­¦ä¹ æ¨¡å‹çš„ç¨³å¥æ€§ï¼Œç‰¹åˆ«æ˜¯åœ¨è§†è§‰è½¬æ¢å™¨ï¼ˆViTsï¼‰é¢†åŸŸï¼Œå¯¹äºå®ƒä»¬åœ¨ç°å®ä¸–ç•Œä¸­çš„éƒ¨ç½²è‡³å…³é‡è¦ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æä¾›äº†ä¸€ç§åŸºäºçº¿æ€§ä»£æ•°ä¸­é›¶ç©ºé—´æ¦‚å¿µçš„å¾®è°ƒæ–¹æ³•ï¼Œä»¥å¢å¼ºè§†è§‰è½¬æ¢å™¨çš„ç¨³å¥æ€§ã€‚æˆ‘ä»¬çš„è°ƒæŸ¥é‡ç‚¹æ˜¯è§†è§‰è½¬æ¢å™¨æ˜¯å¦èƒ½è¡¨ç°å‡ºå¯¹è¾“å…¥å˜åŒ–çš„æŠµæŠ—åŠ›ï¼Œç±»ä¼¼äºçº¿æ€§æ˜ å°„ä¸­çš„é›¶ç©ºé—´å±æ€§ï¼Œè¿™æ„å‘³ç€ä»è¯¥é›¶ç©ºé—´é‡‡æ ·çš„æ‰°åŠ¨åœ¨æ·»åŠ åˆ°è¾“å…¥æ—¶ä¸ä¼šå½±å“æ¨¡å‹çš„è¾“å‡ºã€‚æˆ‘ä»¬ä»è§‚å¯Ÿè®¸å¤šç°æœ‰ViTså…·æœ‰æ­¤å±æ€§å¼€å§‹ï¼Œå› ä¸ºå®ƒä»¬çš„è¡¥ä¸åµŒå…¥å±‚å…·æœ‰éå¹³å‡¡çš„é›¶ç©ºé—´ã€‚ç„¶åï¼Œæˆ‘ä»¬å°†é›¶ç©ºé—´çš„æ¦‚å¿µæ‰©å±•åˆ°éçº¿æ€§ç¯å¢ƒï¼Œå¹¶è¯æ˜å¯ä»¥é€šè¿‡ä¼˜åŒ–ä¸ºViTçš„ç¼–ç å™¨å—åˆæˆè¿‘ä¼¼é›¶ç©ºé—´å…ƒç´ ã€‚æœ€åï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§é’ˆå¯¹ViTsçš„å¾®è°ƒç­–ç•¥ï¼Œå…¶ä¸­æˆ‘ä»¬é€šè¿‡åœ¨è®­ç»ƒæ•°æ®ä¸­å¢åŠ åˆæˆçš„è¿‘ä¼¼é›¶ç©ºé—´å™ªå£°æ¥å¢å¼ºæ¨¡å‹ã€‚æˆ‘ä»¬å‘ç°æˆ‘ä»¬çš„å¾®è°ƒæ–¹æ³•æ˜¾è‘—æé«˜äº†æ¨¡å‹å¯¹å¯¹æŠ—æ€§å’Œè‡ªç„¶å›¾åƒæ‰°åŠ¨çš„ç¨³å¥æ€§ã€‚æ³¨é‡Šï¼šä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/Liu-Hy/ns-vit%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/Liu-Hy/ns-vitæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2403.10476v2">PDF</a> CPAL 2025, Oral</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ¢è®¨äº†å¦‚ä½•é€šè¿‡åˆ©ç”¨çº¿æ€§ä»£æ•°ä¸­çš„é›¶ç©ºé—´æ¦‚å¿µï¼Œå¯¹è§†è§‰è½¬æ¢å™¨ï¼ˆViTsï¼‰è¿›è¡Œå¾®è°ƒä»¥æé«˜å…¶ç¨³å¥æ€§ã€‚æ–‡ç« è§‚å¯Ÿåˆ°è®¸å¤šç°æœ‰çš„ViTså…·æœ‰éå¹³å‡¡çš„é›¶ç©ºé—´ï¼Œè¿›è€Œå°†é›¶ç©ºé—´çš„æ¦‚å¿µæ‰©å±•åˆ°éçº¿æ€§ç¯å¢ƒï¼Œå¹¶é€šè¿‡ä¼˜åŒ–ä¸ºViTçš„ç¼–ç å™¨å—åˆæˆè¿‘ä¼¼é›¶ç©ºé—´å…ƒç´ ã€‚æœ€åï¼Œæå‡ºäº†ä¸€ç§é’ˆå¯¹ViTsçš„å¾®è°ƒç­–ç•¥ï¼Œé€šè¿‡å¢åŠ åˆæˆè¿‘ä¼¼é›¶ç©ºé—´å™ªå£°æ¥å¢å¼ºæ¨¡å‹çš„ç¨³å¥æ€§ã€‚å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•æ˜¾è‘—æé«˜äº†æ¨¡å‹å¯¹å¯¹æŠ—æ€§å’Œè‡ªç„¶å›¾åƒæ‰°åŠ¨çš„ç¨³å¥æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>åˆ©ç”¨é›¶ç©ºé—´æ¦‚å¿µæé«˜è§†è§‰è½¬æ¢å™¨ï¼ˆViTsï¼‰çš„ç¨³å¥æ€§ã€‚</li>
<li>è§‚å¯Ÿåˆ°è®¸å¤šç°æœ‰ViTså…·æœ‰éå¹³å‡¡çš„é›¶ç©ºé—´ã€‚</li>
<li>å°†é›¶ç©ºé—´æ¦‚å¿µæ‰©å±•åˆ°éçº¿æ€§ç¯å¢ƒï¼Œå¹¶ä¸ºViTåˆæˆè¿‘ä¼¼é›¶ç©ºé—´å…ƒç´ ã€‚</li>
<li>æå‡ºä¸€ç§é’ˆå¯¹ViTsçš„å¾®è°ƒç­–ç•¥ï¼Œé€šè¿‡å¢åŠ åˆæˆè¿‘ä¼¼é›¶ç©ºé—´å™ªå£°æ¥å¢å¼ºæ¨¡å‹çš„ç¨³å¥æ€§ã€‚</li>
<li>è¯¥æ–¹æ³•æ˜¾è‘—æé«˜äº†æ¨¡å‹å¯¹å¯¹æŠ—æ€§å’Œè‡ªç„¶å›¾åƒæ‰°åŠ¨çš„ç¨³å¥æ€§ã€‚</li>
<li>è¯¥ç ”ç©¶æä¾›äº†ä¸€ç§æ–°çš„è§†è§’æ¥ç†è§£è§†è§‰è½¬æ¢å™¨çš„å†…åœ¨æ€§è´¨ï¼Œå¹¶ä¸ºå…¶ä¼˜åŒ–æä¾›äº†æ–°æ€è·¯ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2403.10476">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-aec83fce05f4c34660a64608eb86ec44.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-250e2f83da278ee5f7da1208b829bf7b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-52e928b2dd24c08927974aef34d86b60.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-7398d993ff3d9705a3a8fce523d79934.jpg" align="middle">
</details>


<h1 id="-19"><a href="#-19" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-04-03/Vision%20Transformer/">https://kedreamix.github.io/Talk2Paper/Paper/2025-04-03/Vision%20Transformer/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Vision-Transformer/">
                                    <span class="chip bg-color">Vision Transformer</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-04-04/LLM/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-c4198d82bea3dd405c405082c9f941d8.jpg" class="responsive-img" alt="LLM">
                        
                        <span class="card-title">LLM</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            LLM æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-04-04  CADFormer Fine-Grained Cross-modal Alignment and Decoding Transformer   for Referring Remote Sensing Image Segmentation
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-04-04
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/LLM/" class="post-category">
                                    LLM
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/LLM/">
                        <span class="chip bg-color">LLM</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-04-04/R1_Reasoning/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-f564bc8a70aca3fa18fce60f6bb5e4f8.jpg" class="responsive-img" alt="R1_Reasoning">
                        
                        <span class="card-title">R1_Reasoning</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            R1_Reasoning æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-04-04  FortisAVQA and MAVEN a Benchmark Dataset and Debiasing Framework for   Robust Multimodal Reasoning
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-04-04
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/R1-Reasoning/" class="post-category">
                                    R1_Reasoning
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/R1-Reasoning/">
                        <span class="chip bg-color">R1_Reasoning</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">15693.2k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
