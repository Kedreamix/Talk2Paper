<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="R1_Reasoning">
    <meta name="description" content="R1_Reasoning 方向最新论文已更新，请持续关注 Update in 2025-04-03  OpenCodeReasoning Advancing Data Distillation for Competitive Coding">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>R1_Reasoning | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loader页面消失采用渐隐的方式*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logo出现动画 */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//使用渐隐的方法淡出loading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//强制显示loading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">嘘~  正在从服务器偷取页面 . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>首页</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>标签</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>分类</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>归档</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="搜索" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="深色/浅色模式" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			首页
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			标签
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			分类
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			归档
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-d3ea20c763842611ad3c4ee24851c5f8.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">R1_Reasoning</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- 文章内容详情 -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/R1-Reasoning/">
                                <span class="chip bg-color">R1_Reasoning</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/R1-Reasoning/" class="post-category">
                                R1_Reasoning
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp;
                    2025-04-03
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>更新日期:&nbsp;&nbsp;
                    2025-04-19
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>文章字数:&nbsp;&nbsp;
                    20.2k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>阅读时长:&nbsp;&nbsp;
                    82 分
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>阅读次数:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>⚠️ 以下所有内容总结都来自于 大语言模型的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p>
</blockquote>
<h1 id="2025-04-03-更新"><a href="#2025-04-03-更新" class="headerlink" title="2025-04-03 更新"></a>2025-04-03 更新</h1><h2 id="OpenCodeReasoning-Advancing-Data-Distillation-for-Competitive-Coding"><a href="#OpenCodeReasoning-Advancing-Data-Distillation-for-Competitive-Coding" class="headerlink" title="OpenCodeReasoning: Advancing Data Distillation for Competitive Coding"></a>OpenCodeReasoning: Advancing Data Distillation for Competitive Coding</h2><p><strong>Authors:Wasi Uddin Ahmad, Sean Narenthiran, Somshubra Majumdar, Aleksander Ficek, Siddhartha Jain, Jocelyn Huang, Vahid Noroozi, Boris Ginsburg</strong></p>
<p>Since the advent of reasoning-based large language models, many have found great success from distilling reasoning capabilities into student models. Such techniques have significantly bridged the gap between reasoning and standard LLMs on coding tasks. Despite this, much of the progress on distilling reasoning models remains locked behind proprietary datasets or lacks details on data curation, filtering and subsequent training. To address this, we construct a superior supervised fine-tuning (SFT) dataset that we use to achieve state-of-the-art coding capability results in models of various sizes. Our distilled models use only SFT to achieve 61.8% on LiveCodeBench and 24.6% on CodeContests, surpassing alternatives trained with reinforcement learning. We then perform analysis on the data sources used to construct our dataset, the impact of code execution filtering, and the importance of instruction&#x2F;solution diversity. We observe that execution filtering negatively affected benchmark accuracy, leading us to prioritize instruction diversity over solution correctness. Finally, we also analyze the token efficiency and reasoning patterns utilized by these models. We will open-source these datasets and distilled models to the community. </p>
<blockquote>
<p>自从基于推理的大型语言模型出现以来，许多人发现通过将推理能力蒸馏到学生模型中取得了巨大成功。这类技术在编码任务上显著缩小了推理和标准大型语言模型之间的差距。尽管如此，大部分关于蒸馏推理模型的进展仍局限于专有数据集，或者缺乏关于数据收集、过滤和后续训练方面的详细信息。为了解决这一问题，我们构建了一个优质的监督微调（SFT）数据集，用于在各种规模的模型中实现最先进的编码能力结果。我们的蒸馏模型仅使用SFT就实现了LiveCodeBench的61.8%和CodeContests的24.6%，超越了使用强化学习训练的替代方案。然后，我们对构建数据集所使用数据源的分析、代码执行过滤的影响以及指令&#x2F;解决方案多样性的重要性。我们发现执行过滤对基准测试准确性产生了负面影响，导致我们优先重视指令的多样性而非解决方案的正确性。最后，我们还分析了这些模型的令牌效率和推理模式。我们将向社区开源这些数据集和蒸馏模型。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.01943v1">PDF</a> Work in progress</p>
<p><strong>Summary</strong></p>
<p>本文介绍了基于推理的大型语言模型的发展情况，特别是模型蒸馏技术在提高模型推理能力方面的应用。文章指出，尽管已有许多成功的尝试，但大部分进展仍然受限于专有数据集，并且缺乏数据收集、筛选和训练方面的详细信息。为解决这一问题，研究者构建了一个高级的监督微调（SFT）数据集，该数据集实现了不同规模的模型在编码任务上的卓越表现。研究者在构建数据集的过程中对数据源、代码执行过滤的影响以及指令&#x2F;解决方案多样性进行了分析，并发现执行过滤会对基准测试准确性产生负面影响，因此强调指令多样性的重要性。最后，研究者还分析了这些模型的令牌效率和推理模式。文章强调将公开这些数据集和蒸馏模型供社区使用。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>推理大型语言模型通过将推理能力蒸馏到学生模型中取得了巨大成功。</li>
<li>目前大部分进展受限于专有数据集，缺乏数据收集、筛选和训练的详细信息。</li>
<li>研究者通过构建一个高级的监督微调（SFT）数据集，实现了在编码任务上的卓越表现。</li>
<li>在构建数据集过程中，发现执行过滤会对基准测试准确性产生负面影响。</li>
<li>研究者强调了指令多样性的重要性，优先于解决方案的正确性。</li>
<li>分析显示，这些模型展示了有效的令牌效率和推理模式。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.01943">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-d4380a2da8480d4b9e1fcb7c28f44a90.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a0bb60cea00b040106932355d54f872f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-6bd0fdea622909f4419f65eb13767bb7.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-003554efde4e4b026d13dedecf6f056d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8561d6b1998c13873167749f11edc13d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-1170c2a7a6ec92d93d72822e29ec3937.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Benchmarking-Synthetic-Tabular-Data-A-Multi-Dimensional-Evaluation-Framework"><a href="#Benchmarking-Synthetic-Tabular-Data-A-Multi-Dimensional-Evaluation-Framework" class="headerlink" title="Benchmarking Synthetic Tabular Data: A Multi-Dimensional Evaluation   Framework"></a>Benchmarking Synthetic Tabular Data: A Multi-Dimensional Evaluation   Framework</h2><p><strong>Authors:Andrey Sidorenko, Michael Platzer, Mario Scriminaci, Paul Tiwald</strong></p>
<p>Evaluating the quality of synthetic data remains a key challenge for ensuring privacy and utility in data-driven research. In this work, we present an evaluation framework that quantifies how well synthetic data replicates original distributional properties while ensuring privacy. The proposed approach employs a holdout-based benchmarking strategy that facilitates quantitative assessment through low- and high-dimensional distribution comparisons, embedding-based similarity measures, and nearest-neighbor distance metrics. The framework supports various data types and structures, including sequential and contextual information, and enables interpretable quality diagnostics through a set of standardized metrics. These contributions aim to support reproducibility and methodological consistency in benchmarking of synthetic data generation techniques. The code of the framework is available at <a target="_blank" rel="noopener" href="https://github.com/mostly-ai/mostlyai-qa">https://github.com/mostly-ai/mostlyai-qa</a>. </p>
<blockquote>
<p>评估合成数据的质量仍然是数据驱动研究中确保隐私和实用性的关键挑战。在这项工作中，我们提出了一个评估框架，该框架量化合成数据复制原始分布属性的程度，同时确保隐私。所提出的方法采用基于留出法的基准测试策略，通过低维和高维分布比较、基于嵌入的相似性度量以及最近邻距离度量，促进定量评估。该框架支持各种数据类型和结构，包括序列和上下文信息，并通过一系列标准化指标实现可解释的质量诊断。这些贡献旨在支持合成数据生成技术的基准测试的再现性和方法一致性。该框架的代码可在<a target="_blank" rel="noopener" href="https://github.com/mostly-ai/mostlyai-qa">https://github.com/mostly-ai/mostlyai-qa</a>找到。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.01908v1">PDF</a> 16 pages, 7 figures, 1 table</p>
<p><strong>Summary</strong></p>
<p>本文介绍了一个评估合成数据质量的框架，该框架能够量化合成数据复制原始分布属性的程度，同时确保隐私。采用基于保留验证的策略进行定量评估，并通过低维和高维分布比较、基于嵌入的相似性度量以及最近邻距离度量等方法实现。该框架支持各种数据类型和结构，包括序列和上下文信息，并通过一系列标准化指标提供可解释的质量诊断。旨在支持合成数据生成技术的基准测试的再现性和方法一致性。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>合成数据质量评估是确保数据驱动研究中隐私和实用性的关键挑战。</li>
<li>提出的框架能够量化合成数据对原始分布属性的复制程度。</li>
<li>框架采用基于保留验证的基准测试策略，支持多种数据类型和结构。</li>
<li>通过低维和高维分布比较、基于嵌入的相似性度量以及最近邻距离度量等方法进行定量评估。</li>
<li>框架旨在平衡合成数据的隐私和实用性，确保数据的可解释性和质量。</li>
<li>框架代码已公开发布，便于研究人员使用和改进。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.01908">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-e47696fec227a79c0184952f2f64ff40.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f0d6ec24117a3524e81aec1d7009cff8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5909381f24e2fd92c9523add1136debb.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-71d1f8026d152d7b10f2173dcd738aa1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-00f33e441b81dad7accd768e763cf71e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-27c74494483f40c2151131bbc78140b2.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="STAR-1-Safer-Alignment-of-Reasoning-LLMs-with-1K-Data"><a href="#STAR-1-Safer-Alignment-of-Reasoning-LLMs-with-1K-Data" class="headerlink" title="STAR-1: Safer Alignment of Reasoning LLMs with 1K Data"></a>STAR-1: Safer Alignment of Reasoning LLMs with 1K Data</h2><p><strong>Authors:Zijun Wang, Haoqin Tu, Yuhan Wang, Juncheng Wu, Jieru Mei, Brian R. Bartoldson, Bhavya Kailkhura, Cihang Xie</strong></p>
<p>This paper introduces STAR-1, a high-quality, just-1k-scale safety dataset specifically designed for large reasoning models (LRMs) like DeepSeek-R1. Built on three core principles – diversity, deliberative reasoning, and rigorous filtering – STAR-1 aims to address the critical needs for safety alignment in LRMs. Specifically, we begin by integrating existing open-source safety datasets from diverse sources. Then, we curate safety policies to generate policy-grounded deliberative reasoning samples. Lastly, we apply a GPT-4o-based safety scoring system to select training examples aligned with best practices. Experimental results show that fine-tuning LRMs with STAR-1 leads to an average 40% improvement in safety performance across four benchmarks, while only incurring a marginal decrease (e.g., an average of 1.1%) in reasoning ability measured across five reasoning tasks. Extensive ablation studies further validate the importance of our design principles in constructing STAR-1 and analyze its efficacy across both LRMs and traditional LLMs. Our project page is <a target="_blank" rel="noopener" href="https://ucsc-vlaa.github.io/STAR-1">https://ucsc-vlaa.github.io/STAR-1</a>. </p>
<blockquote>
<p>本文介绍了STAR-1，这是一个专为DeepSeek-R1等大型推理模型设计的高质量、仅含千分之一规模的安全数据集。基于多样性、审慎推理和严格筛选三大核心原则，STAR-1旨在解决大型推理模型中对安全对齐的迫切需求。具体来说，我们首先整合来自不同来源的现有开源安全数据集。然后，我们制定安全策略来生成基于策略审慎推理样本。最后，我们应用基于GPT-4的安全评分系统来选择符合最佳实践的训练样本。实验结果表明，使用STAR-1对大型推理模型进行微调，在四个基准测试中安全性能平均提高了40%，同时只带来轻微的推理能力下降（例如平均下降1.1%）。广泛的消融研究进一步验证了构建STAR-1的设计原则的重要性，并对其在大型推理模型和传统的大型语言模型上的有效性进行了分析。我们的项目页面是：<a target="_blank" rel="noopener" href="https://ucsc-vlaa.github.io/STAR-1%E3%80%82">https://ucsc-vlaa.github.io/STAR-1。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.01903v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>这是一篇关于STAR-1数据集的文章，该数据集旨在满足大型推理模型的安全需求。数据集遵循多样性、审慎推理和严格筛选三大原则构建，并通过整合现有开源安全数据集、制定安全政策以及应用基于GPT-4o的安全评分系统来生成样本。实验结果显示，使用STAR-1微调的大型推理模型在四项基准测试上的安全性能平均提高了40%，同时仅在五个推理任务中略微降低推理能力。数据集项目页面是：<a target="_blank" rel="noopener" href="https://ucsc-vlaa.github.io/STAR-1">链接地址</a>。</p>
<p><strong>Key Takeaways</strong></p>
<p>以下是本文章的关键要点：</p>
<ol>
<li>文章介绍了STAR-1数据集，这是一个专为大型推理模型设计的高质量安全数据集。</li>
<li>数据集遵循多样性、审慎推理和严格筛选三大原则构建。</li>
<li>数据集通过整合现有开源安全数据集，制定安全政策来生成样本。</li>
<li>应用基于GPT-4o的安全评分系统来筛选符合最佳实践的训练样本。</li>
<li>实验结果显示，使用STAR-1微调的大型推理模型在安全性能上显著提高，同时保持较高的推理能力。</li>
<li>数据集项目页面提供了更多关于STAR-1的详细信息。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.01903">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-eacf55114141fbf8489fd896f6fca4ae.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a3bce844203c398839f3e89586195374.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-40d99782dafeba56ee8265365c9370be.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-832ebff3e52aed24090d3653c44e6f77.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-85fc740539de2c3066cdb182445fa637.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="GMAI-VL-R1-Harnessing-Reinforcement-Learning-for-Multimodal-Medical-Reasoning"><a href="#GMAI-VL-R1-Harnessing-Reinforcement-Learning-for-Multimodal-Medical-Reasoning" class="headerlink" title="GMAI-VL-R1: Harnessing Reinforcement Learning for Multimodal Medical   Reasoning"></a>GMAI-VL-R1: Harnessing Reinforcement Learning for Multimodal Medical   Reasoning</h2><p><strong>Authors:Yanzhou Su, Tianbin Li, Jiyao Liu, Chenglong Ma, Junzhi Ning, Cheng Tang, Sibo Ju, Jin Ye, Pengcheng Chen, Ming Hu, Shixiang Tang, Lihao Liu, Bin Fu, Wenqi Shao, Xiaowei Hu, Xiangwen Liao, Yuanfeng Ji, Junjun He</strong></p>
<p>Recent advances in general medical AI have made significant strides, but existing models often lack the reasoning capabilities needed for complex medical decision-making. This paper presents GMAI-VL-R1, a multimodal medical reasoning model enhanced by reinforcement learning (RL) to improve its reasoning abilities. Through iterative training, GMAI-VL-R1 optimizes decision-making, significantly boosting diagnostic accuracy and clinical support. We also develop a reasoning data synthesis method, generating step-by-step reasoning data via rejection sampling, which further enhances the model’s generalization. Experimental results show that after RL training, GMAI-VL-R1 excels in tasks such as medical image diagnosis and visual question answering. While the model demonstrates basic memorization with supervised fine-tuning, RL is crucial for true generalization. Our work establishes new evaluation benchmarks and paves the way for future advancements in medical reasoning models. Code, data, and model will be released at \href{<a target="_blank" rel="noopener" href="https://github.com/uni-medical/GMAI-VL-R1%7D%7Bthis">https://github.com/uni-medical/GMAI-VL-R1}{this</a> link}. </p>
<blockquote>
<p>近期通用医疗人工智能的进步已经取得了重大突破，但现有模型通常缺乏复杂的医疗决策所需的推理能力。本文提出了一种多模态医疗推理模型GMAI-VL-R1，通过强化学习（RL）增强其推理能力。通过迭代训练，GMAI-VL-R1优化了决策制定，大大提高了诊断准确性和临床支持能力。我们还开发了一种推理数据合成方法，通过拒绝采样生成分步推理数据，这进一步提高了模型的泛化能力。实验结果表明，经过RL训练后，GMAI-VL-R1在医学图像诊断和视觉问答等任务上表现出色。虽然该模型通过监督微调展示了基本的记忆能力，但RL对于真正的泛化至关重要。我们的工作建立了新的评估基准，为医疗推理模型的未来进步铺平了道路。代码、数据和模型将发布在此链接：<a target="_blank" rel="noopener" href="https://github.com/uni-medical/GMAI-VL-R1%E3%80%82">https://github.com/uni-medical/GMAI-VL-R1。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.01886v1">PDF</a> </p>
<p><strong>Summary</strong><br>     近期医学人工智能取得显著进展，但现有模型在复杂医疗决策中缺乏推理能力。本文提出一种通过强化学习增强的多模态医学推理模型GMAI-VL-R1，以提高其推理能力。经过迭代训练，GMAI-VL-R1优化了决策制定，显著提高了诊断准确性和临床支持能力。此外，还开发了一种通过拒绝采样生成逐步推理数据的方法，进一步提高模型的泛化能力。实验结果表明，经过强化学习训练后，GMAI-VL-R1在医学图像诊断和视觉问答等任务上表现出色。虽然模型通过监督微调实现了基本记忆，但强化学习对于真正的泛化至关重要。本研究建立了新的评估基准，为医学推理模型的未来发展铺平了道路。</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>医学人工智能近期取得显著进展，但缺乏复杂医疗决策所需的推理能力。</li>
<li>GMAI-VL-R1是一个多模态医学推理模型，通过强化学习增强。</li>
<li>强化学习训练使GMAI-VL-R1在诊断准确性和临床支持方面显著提高。</li>
<li>开发了一种通过拒绝采样生成逐步推理数据的方法，增强模型的泛化能力。</li>
<li>实验结果表明，GMAI-VL-R1在医学图像诊断和视觉问答等任务上表现优秀。</li>
<li>强化学习对于模型的真正泛化至关重要。</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.01886">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-34ca1460e91fb51bf2e2f1b10815ed39.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-fcd1ce69586f3e7150e001cb2e3f0a8f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-797985ccb31defc3d013562db8b044df.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-898cfafc8b3efe3b48c6c46e6eed35c5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-40f7a50dd758cef8763b7611d1aff15f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-dfa7a2bd349a07fa756d8548f03503e3.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="TransientTables-Evaluating-LLMs’-Reasoning-on-Temporally-Evolving-Semi-structured-Tables"><a href="#TransientTables-Evaluating-LLMs’-Reasoning-on-Temporally-Evolving-Semi-structured-Tables" class="headerlink" title="TransientTables: Evaluating LLMs’ Reasoning on Temporally Evolving   Semi-structured Tables"></a>TransientTables: Evaluating LLMs’ Reasoning on Temporally Evolving   Semi-structured Tables</h2><p><strong>Authors:Abhilash Shankarampeta, Harsh Mahajan, Tushar Kataria, Dan Roth, Vivek Gupta</strong></p>
<p>Humans continuously make new discoveries, and understanding temporal sequence of events leading to these breakthroughs is essential for advancing science and society. This ability to reason over time allows us to identify future steps and understand the effects of financial and political decisions on our lives. However, large language models (LLMs) are typically trained on static datasets, limiting their ability to perform effective temporal reasoning. To assess the temporal reasoning capabilities of LLMs, we present the TRANSIENTTABLES dataset, which comprises 3,971 questions derived from over 14,000 tables, spanning 1,238 entities across multiple time periods. We introduce a template-based question-generation pipeline that harnesses LLMs to refine both templates and questions. Additionally, we establish baseline results using state-of-the-art LLMs to create a benchmark. We also introduce novel modeling strategies centered around task decomposition, enhancing LLM performance. </p>
<blockquote>
<p>人类不断有新的发现，理解导致这些突破的事件的时间序列对于推动科学和社会进步至关重要。这种随时间推理的能力使我们能够确定未来的步骤，并了解金融和政治决策对我们的生活的影响。然而，大型语言模型（LLM）通常是在静态数据集上进行训练的，这限制了它们进行有效的时间推理的能力。为了评估LLM的时间推理能力，我们推出了TRANSIENTTABLES数据集，该数据集包含3971个问题，这些问题由超过14000个表格衍生而来，涵盖了多个时期内的1238个实体。我们引入了一个基于模板的问题生成管道，该管道利用LLM来改进模板和问题。此外，我们还利用最新LLM建立基线结果以创建基准测试，并围绕任务分解引入新型建模策略，以提高LLM的性能。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.01879v1">PDF</a> 19 Pages. 21 Tables, 1 figure</p>
<p><strong>Summary</strong>：</p>
<p>人类对时间的推理能力对于推动科学和社会进步至关重要。然而，大型语言模型（LLMs）通常受限于静态数据集，难以进行有效的时序推理。为此，我们推出了TRANSIENTTABLES数据集，包含3971个问题，源于超过14000个表格，涉及多个时间段的1238个实体。我们采用基于模板的问题生成管道，利用LLMs优化模板和问题。同时，我们建立了使用最新LLMs的基线结果，并引入了以任务分解为中心的新型建模策略，以提高LLMs的性能。</p>
<p><strong>Key Takeaways</strong>：</p>
<ol>
<li>人类理解事件的时间序列对于科学和社会进步至关重要。</li>
<li>大型语言模型（LLMs）在时序推理方面存在局限。</li>
<li>TRANSIENTTABLES数据集包含从超过14000个表格中衍生出的3971个问题，涉及多个时间段的实体。</li>
<li>我们使用基于模板的问题生成管道来提高LLMs的时序推理能力。</li>
<li>基线结果使用最新的LLMs建立，以提供性能标准。</li>
<li>为了增强LLMs的表现，我们引入了新型建模策略，以任务分解为中心。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.01879">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-89af2b462028452ff5332f4254508e05.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1a5bebf66d486a17704a2234c9c22dbc.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Interpreting-Emergent-Planning-in-Model-Free-Reinforcement-Learning"><a href="#Interpreting-Emergent-Planning-in-Model-Free-Reinforcement-Learning" class="headerlink" title="Interpreting Emergent Planning in Model-Free Reinforcement Learning"></a>Interpreting Emergent Planning in Model-Free Reinforcement Learning</h2><p><strong>Authors:Thomas Bush, Stephen Chung, Usman Anwar, Adrià Garriga-Alonso, David Krueger</strong></p>
<p>We present the first mechanistic evidence that model-free reinforcement learning agents can learn to plan. This is achieved by applying a methodology based on concept-based interpretability to a model-free agent in Sokoban – a commonly used benchmark for studying planning. Specifically, we demonstrate that DRC, a generic model-free agent introduced by Guez et al. (2019), uses learned concept representations to internally formulate plans that both predict the long-term effects of actions on the environment and influence action selection. Our methodology involves: (1) probing for planning-relevant concepts, (2) investigating plan formation within the agent’s representations, and (3) verifying that discovered plans (in the agent’s representations) have a causal effect on the agent’s behavior through interventions. We also show that the emergence of these plans coincides with the emergence of a planning-like property: the ability to benefit from additional test-time compute. Finally, we perform a qualitative analysis of the planning algorithm learned by the agent and discover a strong resemblance to parallelized bidirectional search. Our findings advance understanding of the internal mechanisms underlying planning behavior in agents, which is important given the recent trend of emergent planning and reasoning capabilities in LLMs through RL </p>
<blockquote>
<p>我们首次提供了机械证据，证明无模型强化学习代理可以学习规划。这是通过在Sokoban（一个常用于研究规划的基准测试）的无模型代理中应用基于概念的可解释性方法来实现的。具体来说，我们证明了DRC是一个由Guez等人（2019年）提出的一种通用无模型代理，它使用学习的概念表示来内部制定计划，这些计划既预测行动对环境产生的长期影响，又影响行动的选择。我们的方法包括：（1）探测与规划相关的概念；（2）研究代理表示中的计划形成；（3）通过干预验证在代理表示中发现的计划对代理行为具有因果关系。我们还表明，这些计划的出现伴随着一种类似规划的属性出现：利用额外的测试时间计算能力的能力。最后，我们对代理学习的规划算法进行了定性分析，并发现其与并行双向搜索有很强的相似性。我们的研究结果推进了对代理规划行为内在机制的理解，这在最近的趋势中非常重要，即通过强化学习在大型语言模型中出现的规划和推理能力。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.01871v1">PDF</a> ICLR 2025 oral</p>
<p><strong>Summary</strong>：<br>我们首次证明无模型强化学习代理可以通过基于概念可解释性的方法学习规划。在用于研究规划的常用基准Sokoban中，我们对无模型代理应用了此方法。研究结果表明，Guez等人于2019年引入的通用无模型代理DRC使用学习的概念表示来内部制定计划，这些计划可以预测行动对环境的长远影响并影响行动选择。通过探查规划相关概念、研究代理内部计划的形成以及验证代理内部发现的计划对代理行为的影响，我们发现这些计划的产生与一种规划类属性的出现相吻合：即利用额外的测试时间计算的能力。最后，我们对代理所学的规划算法进行了定性分析，发现其与并行双向搜索有很强的相似性。</p>
<p><strong>Key Takeaways</strong>：</p>
<ol>
<li>无模型强化学习代理可以通过概念可解释性的方法学习规划。</li>
<li>研究对象使用了Sokoban基准来展示这一结果。</li>
<li>DRC代理使用学习的概念表示来内部制定计划，预测行动对环境的长远影响并影响行动选择。</li>
<li>通过探查和验证发现了规划相关概念与代理内部计划的形成。</li>
<li>代理利用额外的测试时间计算能力进行规划类活动。</li>
<li>发现的计划与并行双向搜索有相似性。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.01871">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-d100d604b36a27285b4405c828a73ed7.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-661ee843894d3ac95e5dec7b45292563.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-de9cb54a78b6319279d10f3bb4ee2bbf.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-aeb4c6f3d55324f72bf671af581cf661.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Cross-Lingual-Consistency-A-Novel-Inference-Framework-for-Advancing-Reasoning-in-Large-Language-Models"><a href="#Cross-Lingual-Consistency-A-Novel-Inference-Framework-for-Advancing-Reasoning-in-Large-Language-Models" class="headerlink" title="Cross-Lingual Consistency: A Novel Inference Framework for Advancing   Reasoning in Large Language Models"></a>Cross-Lingual Consistency: A Novel Inference Framework for Advancing   Reasoning in Large Language Models</h2><p><strong>Authors:Zhiwei Yu, Tuo Li, Changhong Wang, Hui Chen, Lang Zhou</strong></p>
<p>Chain-of-thought (CoT) has emerged as a critical mechanism for enhancing reasoning capabilities in large language models (LLMs), with self-consistency demonstrating notable promise in boosting performance. However, inherent linguistic biases in multilingual training corpora frequently cause semantic drift and logical inconsistencies, especially in sub-10B parameter LLMs handling complex inference tasks. To overcome these constraints, we propose the Cross-Lingual Consistency (CLC) framework, an innovative inference paradigm that integrates multilingual reasoning paths through majority voting to elevate LLMs’ reasoning capabilities. Empirical evaluations on the CMATH dataset reveal CLC’s superiority over the conventional self-consistency method, delivering 9.5%, 6.5%, and 6.0% absolute accuracy gains for DeepSeek-Math-7B-Instruct, Qwen2.5-Math-7B-Instruct, and Gemma2-9B-Instruct respectively. Expanding CLC’s linguistic scope to 11 diverse languages implies two synergistic benefits: 1) neutralizing linguistic biases in multilingual training corpora through multilingual ensemble voting, 2) escaping monolingual reasoning traps by exploring the broader multilingual solution space. This dual benefits empirically enables more globally optimal reasoning paths compared to monolingual self-consistency baselines, as evidenced by the 4.1%-18.5% accuracy gains using Gemma2-9B-Instruct on the MGSM dataset. </p>
<blockquote>
<p>思维链（CoT）作为提升大型语言模型（LLM）推理能力的重要机制已经崭露头角，自我一致性在提升性能上显示出巨大潜力。然而，在多语言训练语料库中的固有语言偏见经常导致语义漂移和逻辑不一致，特别是在处理复杂推理任务的参数小于10B的LLMs中更为明显。为了克服这些限制，我们提出了跨语言一致性（CLC）框架，这是一种创新性的推理范式，它通过多数投票来整合多语言推理路径，提升LLMs的推理能力。在CMATH数据集上的实证评估表明，CLC框架优于传统的自我一致性方法，为DeepSeek-Math-7B-Instruct、Qwen2.5-Math-7B-Instruct和Gemma2-9B-Instruct分别带来了9.5%、6.5%和6.0%的绝对准确度提升。将CLC的语言范围扩展到11种不同语言，意味着两个协同优势：1）通过多语言集合投票抵消多语言训练语料库中的语言偏见；2）通过探索更广泛的多语言解决方案空间，避免单一语言的推理陷阱。这种双重优势使得与单语自我一致性基线相比，能够找到更多全局最优的推理路径。如在MGSM数据集上，使用Gemma2-9B-Instruct的准确度提升了4.1%-18.5%，这证明了其实效性。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.01857v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>在大型语言模型（LLM）中，链式思维（CoT）已成为提高推理能力的重要机制，而自我一致性在提高性能方面具有显著潜力。然而，在多语种训练语料库中存在的固有语言偏见会导致语义漂移和逻辑不一致，特别是在处理复杂推理任务的参数小于10B的LLM中更为显著。为克服这些局限性，本文提出了跨语言一致性（CLC）框架，这是一种通过多数投票整合多语种推理路径的创新推理范式，旨在提升LLM的推理能力。在CMATH数据集上的实证评估表明，CLC优于传统的自我一致性方法，分别为DeepSeek-Math-7B-Instruct、Qwen2.5-Math-7B-Instruct和Gemma2-9B-Instruct带来了9.5%、6.5%和6.0%的绝对准确率提升。将CLC的语种范围扩大到11种不同的语言，带来了两个协同效益：1）通过多语种集成投票中和多语种训练语料库中的语言偏见；2）通过探索更广泛的多语种解决方案空间，避免单语种推理陷阱。这双重优势使得CLC相比单语种自我一致性基线，能够找到更多全局最优的推理路径，如在MGSM数据集上使用Gemma2-9B-Instruct带来的4.1%~18.5%的准确率提升。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>链式思维（CoT）增强了大型语言模型（LLM）的推理能力。</li>
<li>自我一致性是提升LLM性能的一种有前景的方法。</li>
<li>多语种训练语料库中的语言偏见会导致语义漂移和逻辑不一致。</li>
<li>跨语言一致性（CLC）框架通过多数投票整合多语种推理路径，提升了LLM的推理能力。</li>
<li>CLC框架在CMATH数据集上的实证评估表现优越。</li>
<li>CLC框架扩展至11种语言，具有中和语言偏见和探索多语种解决方案空间的双重优势。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.01857">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-ae9bdf4b23091f2c7b4ca0b9c70d22c3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8b817bac65b02e0cf5debe8e6595637d.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Reasoning-LLMs-for-User-Aware-Multimodal-Conversational-Agents"><a href="#Reasoning-LLMs-for-User-Aware-Multimodal-Conversational-Agents" class="headerlink" title="Reasoning LLMs for User-Aware Multimodal Conversational Agents"></a>Reasoning LLMs for User-Aware Multimodal Conversational Agents</h2><p><strong>Authors:Hamed Rahimi, Jeanne Cattoni, Meriem Beghili, Mouad Abrini, Mahdi Khoramshahi, Maribel Pino, Mohamed Chetouani</strong></p>
<p>Personalization in social robotics is critical for fostering effective human-robot interactions, yet systems often face the cold start problem, where initial user preferences or characteristics are unavailable. This paper proposes a novel framework called USER-LLM R1 for a user-aware conversational agent that addresses this challenge through dynamic user profiling and model initiation. Our approach integrates chain-of-thought (CoT) reasoning models to iteratively infer user preferences and vision-language models (VLMs) to initialize user profiles from multimodal inputs, enabling personalized interactions from the first encounter. Leveraging a Retrieval-Augmented Generation (RAG) architecture, the system dynamically refines user representations within an inherent CoT process, ensuring contextually relevant and adaptive responses. Evaluations on the ElderlyTech-VQA Bench demonstrate significant improvements in ROUGE-1 (+23.2%), ROUGE-2 (+0.6%), and ROUGE-L (+8%) F1 scores over state-of-the-art baselines, with ablation studies underscoring the impact of reasoning model size on performance. Human evaluations further validate the framework’s efficacy, particularly for elderly users, where tailored responses enhance engagement and trust. Ethical considerations, including privacy preservation and bias mitigation, are rigorously discussed and addressed to ensure responsible deployment. </p>
<blockquote>
<p>个性化社会机器人对于促进有效的人机交互至关重要，然而，系统经常面临冷启动问题，即初始用户偏好或特征无法获取。本文针对这一问题，提出了一种名为USER-LLM R1的新型框架，用于构建具有用户意识的对话代理，通过动态用户分析和模型启动来解决这一问题。我们的方法融合了思维链（CoT）推理模型，以迭代方式推断用户偏好，以及视觉语言模型（VLM），从多模态输入初始化用户分析，从而首次交互即可实现个性化交互。通过利用检索增强生成（RAG）架构，系统在一个内在的CoT过程中动态优化用户表示，确保语境相关且自适应的响应。在ElderlyTech-VQA Bench上的评估显示，与最新基线相比，ROUGE-1（+23.2%）、ROUGE-2（+0.6%）和ROUGE-L（+8%）的F1分数有了显著的提高。消融研究强调了推理模型大小对性能的影响。人类评估进一步验证了该框架的有效性，特别是对于老年用户，定制化的回应增强了参与感和信任度。严格讨论了包括隐私保护和偏见缓解在内的道德考量，并予以解决，以确保负责任的部署。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.01700v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>该论文针对社交机器人个性化面临的冷启动问题，提出了一种名为USER-LLM R1的新型框架。该框架通过动态用户分析和模型启动实现用户感知对话代理，集成链式思维推理模型来迭代推断用户偏好，并利用视觉语言模型从多模态输入中初始化用户资料。该框架通过检索增强生成架构动态完善用户内在认知过程，确保回应语境相关并具有适应性。在ElderlyTech-VQA Bench上的评估显示，相较于现有技术基线，该框架在ROUGE-1（+23.2%）、ROUGE-2（+0.6%）和ROUGE-L（+8%）F1分数上取得了显著改善。人类评估进一步验证了该框架对老年用户的功效，其定制的响应增强了参与感和信任感。同时严格讨论并解决了伦理考量，包括隐私保护和偏见缓解，以确保部署责任性。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li><p>社交机器人个性化对于促进有效的人机交互至关重要，但系统面临冷启动问题，即初始用户偏好或特征不可用。</p>
</li>
<li><p>USER-LLM R1框架通过动态用户分析和模型启动来解决这一问题，实现用户感知对话代理。</p>
</li>
<li><p>该框架集成了链式思维推理模型来迭代推断用户偏好，并利用视觉语言模型处理多模态输入以初始化用户资料。</p>
</li>
<li><p>通过检索增强生成架构，系统能在内在认知过程中动态完善用户表达，确保回应具有语境相关性和适应性。</p>
</li>
<li><p>在ElderlyTech-VQA Bench上的评估显示，该框架相较于现有技术基线有显著改进。</p>
</li>
<li><p>人类评估验证了该框架对老年用户的功效，定制的响应增强了参与感和信任感。</p>
</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.01700">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-a7a76a4fb94769482dc0127a81698ea4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-327f448db5fc875a0ddc17827750117b.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-ec285135e1f83c72fb901a0ba345f390.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-cb67508ac6814b3aca0f8d6c7bb449ce.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b1a3356124ad25fa9715da503c3922f7.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-130a296ece7e6c50de7e131b033050fd.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Text-Speaks-Louder-than-Vision-ASCII-Art-Reveals-Textual-Biases-in-Vision-Language-Models"><a href="#Text-Speaks-Louder-than-Vision-ASCII-Art-Reveals-Textual-Biases-in-Vision-Language-Models" class="headerlink" title="Text Speaks Louder than Vision: ASCII Art Reveals Textual Biases in   Vision-Language Models"></a>Text Speaks Louder than Vision: ASCII Art Reveals Textual Biases in   Vision-Language Models</h2><p><strong>Authors:Zhaochen Wang, Yujun Cai, Zi Huang, Bryan Hooi, Yiwei Wang, Ming-Hsuan Yang</strong></p>
<p>Vision-language models (VLMs) have advanced rapidly in processing multimodal information, but their ability to reconcile conflicting signals across modalities remains underexplored. This work investigates how VLMs process ASCII art, a unique medium where textual elements collectively form visual patterns, potentially creating semantic-visual conflicts. We introduce a novel evaluation framework that systematically challenges five state-of-the-art models (including GPT-4o, Claude, and Gemini) using adversarial ASCII art, where character-level semantics deliberately contradict global visual patterns. Our experiments reveal a strong text-priority bias: VLMs consistently prioritize textual information over visual patterns, with visual recognition ability declining dramatically as semantic complexity increases. Various mitigation attempts through visual parameter tuning and prompt engineering yielded only modest improvements, suggesting that this limitation requires architectural-level solutions. These findings uncover fundamental flaws in how current VLMs integrate multimodal information, providing important guidance for future model development while highlighting significant implications for content moderation systems vulnerable to adversarial examples. </p>
<blockquote>
<p>视觉语言模型（VLMs）在处理多模态信息方面取得了快速进展，但其在调和跨模态冲突信号方面的能力仍被忽视。本研究探讨了VLMs如何处理ASCII艺术这一独特媒介，其中的文本元素共同形成视觉模式，可能会产生语义视觉冲突。我们引入了一个新的评估框架，使用对抗性ASCII艺术系统地挑战了五种最先进的模型（包括GPT-4o、Claude和Gemini），其中字符级别的语义故意与全局视觉模式相矛盾。我们的实验揭示了一个强烈的文本优先偏见：VLMs始终优先处理文本信息而非视觉模式，随着语义复杂性的增加，其视觉识别能力急剧下降。通过视觉参数调整和提示工程进行的各种缓解尝试仅产生了适度的改进，这表明这一局限性需要架构级的解决方案。这些发现揭示了当前VLMs如何整合多模态信息的基本缺陷，为未来的模型开发提供了重要指导，同时强调了对于易受对抗性样本影响的内容审核系统所带来的重大影响。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.01589v1">PDF</a> Under review at COLM 2025</p>
<p><strong>Summary</strong></p>
<p>本文探讨了视觉语言模型（VLMs）在处理ASCII艺术时的表现，这是一种文本元素共同形成视觉图案的特殊媒介，可能会产生语义视觉冲突。研究引入了一个新型评估框架，系统地挑战了五种最先进的模型，包括GPT-4o、Claude和Gemini等，使用对抗性的ASCII艺术，其中字符级别的语义故意与全局视觉模式相矛盾。实验表明，VLMs存在强烈的文本优先偏见，在语义复杂性增加时，视觉识别能力急剧下降。通过视觉参数调整和提示工程的各种缓解尝试只产生了适度的改善，表明这一局限性需要架构级的解决方案。这些发现揭示了当前VLMs在集成多模式信息时的根本缺陷，为未来的模型开发提供了重要指导，并突出了对易受对抗性示例影响的内容审核系统的重要意义。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>VLMs在处理ASCII艺术时面临语义视觉冲突的问题。</li>
<li>引入新型评估框架挑战了五种最先进的VLMs模型。</li>
<li>实验发现VLMs存在文本优先的偏见。</li>
<li>在语义复杂性增加时，VLMs的视觉识别能力显著下降。</li>
<li>目前的模型在集成多模式信息方面存在根本缺陷。</li>
<li>尝试通过视觉参数调整和提示工程改善模型表现，但效果有限。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.01589">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-3b943ac9ae73b3d0545f2f9b84548315.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-3ab6db049fabe7e50947297f71cdf839.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d3ea20c763842611ad3c4ee24851c5f8.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e609b94376d4c29bfbbb10626368755a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-82a3cea05d4d4a3b23fd9d3fa054cae7.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="Enabling-Systematic-Generalization-in-Abstract-Spatial-Reasoning-through-Meta-Learning-for-Compositionality"><a href="#Enabling-Systematic-Generalization-in-Abstract-Spatial-Reasoning-through-Meta-Learning-for-Compositionality" class="headerlink" title="Enabling Systematic Generalization in Abstract Spatial Reasoning through   Meta-Learning for Compositionality"></a>Enabling Systematic Generalization in Abstract Spatial Reasoning through   Meta-Learning for Compositionality</h2><p><strong>Authors:Philipp Mondorf, Shijia Zhou, Monica Riedler, Barbara Plank</strong></p>
<p>Systematic generalization refers to the capacity to understand and generate novel combinations from known components. Despite recent progress by large language models (LLMs) across various domains, these models often fail to extend their knowledge to novel compositional scenarios, revealing notable limitations in systematic generalization. There has been an ongoing debate about whether neural networks possess the capacity for systematic generalization, with recent studies suggesting that meta-learning approaches designed for compositionality can significantly enhance this ability. However, these insights have largely been confined to linguistic problems, leaving their applicability to other tasks an open question. In this study, we extend the approach of meta-learning for compositionality to the domain of abstract spatial reasoning. To this end, we introduce $\textit{SYGAR}$-a dataset designed to evaluate the capacity of models to systematically generalize from known geometric transformations (e.g., translation, rotation) of two-dimensional objects to novel combinations of these transformations (e.g., translation+rotation). Our results show that a transformer-based encoder-decoder model, trained via meta-learning for compositionality, can systematically generalize to previously unseen transformation compositions, significantly outperforming state-of-the-art LLMs, including o3-mini, GPT-4o, and Gemini 2.0 Flash, which fail to exhibit similar systematic behavior. Our findings highlight the effectiveness of meta-learning in promoting systematicity beyond linguistic tasks, suggesting a promising direction toward more robust and generalizable models. </p>
<blockquote>
<p>系统化泛化是指理解和生成已知组件的新组合的能力。尽管大型语言模型（LLM）在各个领域都取得了进展，但这些模型往往无法将其知识扩展到新的组合场景，显示出系统化泛化的明显局限性。关于神经网络是否具备系统化泛化能力的问题一直存在争议，近期研究表明，针对组合性设计的元学习方法可以显著增强这种能力。然而，这些见解大多局限于语言问题，对其他任务的适用性仍是未知。在这项研究中，我们将元学习的方法扩展到抽象空间推理领域。为此，我们引入了SYGAR数据集，旨在评估模型从已知的几何变换（如平移、旋转）中系统地推广到这些变换的新组合（如平移+旋转）的能力。我们的结果表明，基于转换器的编码器-解码器模型，通过元学习进行组合性训练，能够系统地推广到之前未见过的变换组合，显著优于最新的大型语言模型，包括o3-mini、GPT-4o和Gemini 2.0 Flash等，这些模型无法展现出类似的系统化行为。我们的研究强调了元学习在促进系统性、超越语言任务方面的有效性，为构建更稳健和可泛化的模型提供了有前景的方向。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.01445v1">PDF</a> 30 pages, 14 figures</p>
<p><strong>Summary</strong></p>
<p>本文探讨了系统泛化的能力，即理解和生成已知组件的新组合的能力。尽管大型语言模型（LLMs）在各个领域都取得了进展，但它们往往无法将知识扩展到新的组合场景，显示出系统泛化的明显局限性。最近的研究表明，为组合性设计的元学习方法可以显著提高神经网络的系统泛化能力，但这些见解主要局限于语言问题，其他任务的适用性仍是一个开放的问题。本研究将元学习方法应用于抽象空间推理领域，并引入了SYGAR数据集来评估模型从已知的几何转换（如平移、旋转）到新的转换组合（如平移+旋转）的系统泛化能力。研究表明，基于变压器的编码器-解码器模型，通过元学习进行训练，可以系统地泛化到之前未见过的转换组合，显著优于包括o3-mini、GPT-4o和Gemini 2.0 Flash在内的最新LLMs，这些模型未能表现出类似的系统性行为。研究结果表明，元学习在促进系统性方面（尤其是在非语言任务中）的有效性，为构建更稳健和通用的模型提供了有希望的方向。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>系统泛化是理解和生成新组合的关键能力，基于已知组件。</li>
<li>大型语言模型（LLMs）在系统泛化方面存在局限性，难以应用于新组合场景。</li>
<li>元学习方法被设计来提高神经网络的系统泛化能力，尤其在语言问题上效果显著。</li>
<li>SYGAR数据集用于评估模型在几何转换上的系统泛化能力。</li>
<li>基于变压器的编码器-解码器模型通过元学习训练，能够系统地泛化到未见过的转换组合。</li>
<li>该模型显著优于现有的LLMs，这些LLMs在类似任务中未能表现出系统性行为。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.01445">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-63d38379aebdc650bd2dee0b0bd3c595.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9f70b19241b3f03f9c3249f6caacebf7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-fe3c7badd18199b05bba50dbb7cbc15f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a409d7cdc13d2b62ed0aeba55fe42a8f.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="Foundations-and-Evaluations-in-NLP"><a href="#Foundations-and-Evaluations-in-NLP" class="headerlink" title="Foundations and Evaluations in NLP"></a>Foundations and Evaluations in NLP</h2><p><strong>Authors:Jungyeul Park</strong></p>
<p>This memoir explores two fundamental aspects of Natural Language Processing (NLP): the creation of linguistic resources and the evaluation of NLP system performance. Over the past decade, my work has focused on developing a morpheme-based annotation scheme for the Korean language that captures linguistic properties from morphology to semantics. This approach has achieved state-of-the-art results in various NLP tasks, including part-of-speech tagging, dependency parsing, and named entity recognition. Additionally, this work provides a comprehensive analysis of segmentation granularity and its critical impact on NLP system performance. In parallel with linguistic resource development, I have proposed a novel evaluation framework, the jp-algorithm, which introduces an alignment-based method to address challenges in preprocessing tasks like tokenization and sentence boundary detection (SBD). Traditional evaluation methods assume identical tokenization and sentence lengths between gold standards and system outputs, limiting their applicability to real-world data. The jp-algorithm overcomes these limitations, enabling robust end-to-end evaluations across a variety of NLP tasks. It enhances accuracy and flexibility by incorporating linear-time alignment while preserving the complexity of traditional evaluation metrics. This memoir provides key insights into the processing of morphologically rich languages, such as Korean, while offering a generalizable framework for evaluating diverse end-to-end NLP systems. My contributions lay the foundation for future developments, with broader implications for multilingual resource development and system evaluation. </p>
<blockquote>
<p>这篇回忆录探讨了自然语言处理（NLP）的两个基本方面：语言资源的创建和NLP系统性能的评估。过去十年，我的工作主要集中在为韩语开发一种基于词素的注释方案，该方案能够捕获从形态到语义的语言特性。这种方法在各种NLP任务中达到了最先进的结果，包括词性标注、依存解析和命名实体识别。此外，这项工作还全面分析了分词粒度及其对NLP系统性能的关键影响。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.01342v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文探讨了自然语言处理（NLP）的两个基本方面：语言资源的创建和NLP系统性能的评估。作者详细介绍了过去十年在韩语语言资源开发方面的工作，特别是基于形态素的标注方案，该方案在多种NLP任务中取得了最先进的成果。同时，作者提出了一个全新的评估框架jp算法，该算法解决了预处理任务中的对齐问题，如分词和句子边界检测。此算法克服了传统评估方法的局限性，实现了多种NLP任务的端到端评估，增强了准确性和灵活性。总的来说，本文为处理形态丰富的语言如韩语提供了关键见解，并为评估多样化的端到端NLP系统提供了可推广的框架。作者的贡献为未来研究和多语言资源开发与系统评估提供了更广泛的启示。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>作者集中于发展基于形态素的韩语标注方案，涵盖了从形态到语义的各个方面。该方案在不同NLP任务中表现卓越。</li>
<li>作者详细分析了分段粒度对NLP系统性能的关键影响。</li>
<li>作者提出了一种新的评估框架jp算法，解决了预处理任务中的对齐问题，如分词和句子边界检测。这一算法克服了传统方法的局限性，增强了评估的准确性和灵活性。</li>
<li>jp算法结合了线性时间对齐技术，既保留了传统评估指标的复杂性，又提高了评估的鲁棒性和灵活性。</li>
<li>本文为处理形态丰富的语言提供了重要见解，并可为其他语言的类似研究提供启示。</li>
<li>作者的贡献为未来的研究和多语言资源开发提供了基础。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.01342">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-108fb837911aab3d37f497065f1e3ce7.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="ThinkPrune-Pruning-Long-Chain-of-Thought-of-LLMs-via-Reinforcement-Learning"><a href="#ThinkPrune-Pruning-Long-Chain-of-Thought-of-LLMs-via-Reinforcement-Learning" class="headerlink" title="ThinkPrune: Pruning Long Chain-of-Thought of LLMs via Reinforcement   Learning"></a>ThinkPrune: Pruning Long Chain-of-Thought of LLMs via Reinforcement   Learning</h2><p><strong>Authors:Bairu Hou, Yang Zhang, Jiabao Ji, Yujian Liu, Kaizhi Qian, Jacob Andreas, Shiyu Chang</strong></p>
<p>We present ThinkPrune, a simple yet effective method for pruning the thinking length for long-thinking LLMs, which has been found to often produce inefficient and redundant thinking processes. Existing preliminary explorations of reducing thinking length primarily focus on forcing the thinking process to early exit, rather than adapting the LLM to optimize and consolidate the thinking process, and therefore the length-performance tradeoff observed so far is sub-optimal. To fill this gap, ThinkPrune offers a simple solution that continuously trains the long-thinking LLMs via reinforcement learning (RL) with an added token limit, beyond which any unfinished thoughts and answers will be discarded, resulting in a zero reward. To further preserve model performance, we introduce an iterative length pruning approach, where multiple rounds of RL are conducted, each with an increasingly more stringent token limit. We observed that ThinkPrune results in a remarkable performance-length tradeoff – on the AIME24 dataset, the reasoning length of DeepSeek-R1-Distill-Qwen-1.5B can be reduced by half with only 2% drop in performance. We also observed that after pruning, the LLMs can bypass unnecessary steps while keeping the core reasoning process complete. Code is available at <a target="_blank" rel="noopener" href="https://github.com/UCSB-NLP-Chang/ThinkPrune">https://github.com/UCSB-NLP-Chang/ThinkPrune</a>. </p>
<blockquote>
<p>我们提出ThinkPrune，这是一种简单但有效的方法，用于缩减长思考LLM的思考长度。已知LLM常常产生低效和冗余的思考过程。目前对减少思考长度的初步探索主要集中在强制思考过程早期退出，而不是让LLM去优化和巩固思考过程，因此目前观察到的长度与性能之间的权衡并不理想。为了填补这一空白，ThinkPrune提供了一个简单的解决方案，即通过强化学习（RL）持续训练长思考LLM，并增加一个令牌限制，超出此限制的思考和答案将被丢弃，从而导致零奖励。为了保持模型性能，我们引入了一种迭代长度修剪方法，进行多轮RL训练，每轮都设置更严格的令牌限制。我们发现ThinkPrune在性能与长度之间取得了显著的权衡——在AIME24数据集上，DeepSeek-R1-Distill-Qwen-1.5B的思考长度可以减少一半，性能仅下降2%。我们还观察到修剪后，LLM可以跳过不必要的步骤，同时保持核心推理过程的完整性。代码可在<a target="_blank" rel="noopener" href="https://github.com/UCSB-NLP-Chang/ThinkPrune%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/UCSB-NLP-Chang/ThinkPrune获取。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.01296v1">PDF</a> 15 pages, 7 figures</p>
<p><strong>Summary</strong></p>
<p>思考修剪（ThinkPrune）是一种简单有效的方法，用于缩减长思考LLM的思考长度。该方法通过强化学习（RL）连续训练LLM，并增加一个令牌限制，超过限制则丢弃未完成的想法和答案，从而实现性能与长度的优化权衡。在AIME24数据集上，DeepSeek-R1-Distill-Qwen-1.5B模型的推理长度可以减少一半，性能仅下降2%。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ThinkPrune是一种针对长思考LLM的思考长度进行修剪的简单有效方法。</li>
<li>现有方法主要强制提前结束思考过程，而非使LLM适应优化和整合思考过程。</li>
<li>ThinkPrune使用强化学习（RL）进行LLM的连续训练，并引入令牌限制，以实现性能与长度的最佳权衡。</li>
<li>ThinkPrune能够显著减少模型的推理长度，同时保持较高的性能。</li>
<li>通过迭代长度修剪方法，每个回合都设置更严格的令牌限制，进一步优化模型性能。</li>
<li>ThinkPrune可以帮助LLM绕过不必要的步骤，同时保持核心推理过程的完整性。</li>
<li>代码的开源可用性为研究者提供了实践和改进的机会。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.01296">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-ddf17329ccce83998d191db9697cd2e5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-311da33074b5e3baffcdd8252a324fb7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-608d56a5de85b5bfc563b8600d7d2330.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="MPCritic-A-plug-and-play-MPC-architecture-for-reinforcement-learning"><a href="#MPCritic-A-plug-and-play-MPC-architecture-for-reinforcement-learning" class="headerlink" title="MPCritic: A plug-and-play MPC architecture for reinforcement learning"></a>MPCritic: A plug-and-play MPC architecture for reinforcement learning</h2><p><strong>Authors:Nathan P. Lawrence, Thomas Banker, Ali Mesbah</strong></p>
<p>The reinforcement learning (RL) and model predictive control (MPC) communities have developed vast ecosystems of theoretical approaches and computational tools for solving optimal control problems. Given their conceptual similarities but differing strengths, there has been increasing interest in synergizing RL and MPC. However, existing approaches tend to be limited for various reasons, including computational cost of MPC in an RL algorithm and software hurdles towards seamless integration of MPC and RL tools. These challenges often result in the use of “simple” MPC schemes or RL algorithms, neglecting the state-of-the-art in both areas. This paper presents MPCritic, a machine learning-friendly architecture that interfaces seamlessly with MPC tools. MPCritic utilizes the loss landscape defined by a parameterized MPC problem, focusing on “soft” optimization over batched training steps; thereby updating the MPC parameters while avoiding costly minimization and parametric sensitivities. Since the MPC structure is preserved during training, an MPC agent can be readily used for online deployment, where robust constraint satisfaction is paramount. We demonstrate the versatility of MPCritic, in terms of MPC architectures and RL algorithms that it can accommodate, on classic control benchmarks. </p>
<blockquote>
<p>强化学习（RL）和模型预测控制（MPC）社区已经为解决最优控制问题建立了丰富的理论方法和计算工具生态系统。鉴于它们概念上的相似性但各有优势，将RL和MPC协同工作的兴趣日益浓厚。然而，现有方法往往因各种原因而受到限制，包括RL算法中的MPC计算成本以及实现MPC和RL工具无缝集成的软件障碍。这些挑战通常导致使用“简单”的MPC方案或RL算法，而忽略了这两个领域的最新技术。本文介绍了MPCritic，一种与MPC工具无缝兼容、适合机器学习的架构。MPCritic利用参数化MPC问题所定义的损失景观，专注于批处理训练步骤上的“软”优化；从而更新MPC参数，避免昂贵的最小化和参数敏感性。由于训练过程中保留了MPC结构，因此可以使用MPC代理进行在线部署，其中鲁棒的约束满足至关重要。我们在经典的控制基准测试上展示了MPCritic在MPC架构和可以适应的RL算法方面的通用性。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.01086v1">PDF</a> Preprint for CDC 2025</p>
<p><strong>Summary</strong></p>
<p>强化学习（RL）与模型预测控制（MPC）是解决最优控制问题的两大重要领域。二者在理论方法与计算工具上各自形成了庞大的生态系统，且存在融合趋势。然而，现有融合方法面临计算成本与软件整合等挑战，导致无法充分利用两个领域的最新技术。本文提出一种名为MPCritic的机器学习友好架构，该架构与MPC工具无缝集成，利用参数化MPC问题的损失景观进行“软”优化，避免昂贵的最小化和参数敏感性计算。在训练过程中保留MPC结构，便于在线部署时使用MPC代理进行稳健约束满足。在经典控制基准测试中验证了MPCritic的通用性。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>强化学习（RL）和模型预测控制（MPC）是求解最优控制问题的两大方法，分别拥有庞大的理论方法和计算工具生态系统。</li>
<li>RL和MPC的融合引起越来越多兴趣，但现有方法因计算成本和软件整合难题而受限。</li>
<li>MPCritic是一个机器学习友好的架构，与MPC工具无缝集成，利用参数化MPC问题的损失景观进行软优化。</li>
<li>MPCritic避免昂贵的最小化和参数敏感性计算，同时保留MPC结构，便于在线部署时的稳健约束满足。</li>
<li>MPCritic具有广泛的适用性，可以适应不同的MPC架构和强化学习算法。</li>
<li>文章通过经典控制基准测试验证了MPCritic的效能和通用性。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.01086">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-19bd941b4d54730011c59cff087f1cc3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-22d6c79986d95be1044744087f8acf38.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-820e6a4a7855a8eb036dd77d11510c02.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6e76756181bb21994c939cb9cfb66e8a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-360142efe12ed4362cd877b981531509.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-9ab404f85cb20034b0310880bea675c6.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="When-To-Solve-When-To-Verify-Compute-Optimal-Problem-Solving-and-Generative-Verification-for-LLM-Reasoning"><a href="#When-To-Solve-When-To-Verify-Compute-Optimal-Problem-Solving-and-Generative-Verification-for-LLM-Reasoning" class="headerlink" title="When To Solve, When To Verify: Compute-Optimal Problem Solving and   Generative Verification for LLM Reasoning"></a>When To Solve, When To Verify: Compute-Optimal Problem Solving and   Generative Verification for LLM Reasoning</h2><p><strong>Authors:Nishad Singhi, Hritik Bansal, Arian Hosseini, Aditya Grover, Kai-Wei Chang, Marcus Rohrbach, Anna Rohrbach</strong></p>
<p>Scaling test-time compute has emerged as a key strategy for enhancing the reasoning capabilities of large language models (LLMs), particularly in tasks like mathematical problem-solving. A traditional approach, Self-Consistency (SC), generates multiple solutions to a problem and selects the most common answer via majority voting. Another common method involves scoring each solution with a reward model (verifier) and choosing the best one. Recent advancements in Generative Reward Models (GenRM) reframe verification as a next-token prediction task, enabling inference-time scaling along a new axis. Specifically, GenRM generates multiple verification chains-of-thought to score each solution. Under a limited inference budget, this introduces a fundamental trade-off: should you spend the budget on scaling solutions via SC or generate fewer solutions and allocate compute to verification via GenRM? To address this, we evaluate GenRM against SC under a fixed inference budget. Interestingly, we find that SC is more compute-efficient than GenRM for most practical inference budgets across diverse models and datasets. For instance, GenRM first matches SC after consuming up to 8x the inference compute and requires significantly more compute to outperform it. Furthermore, we derive inference scaling laws for the GenRM paradigm, revealing that compute-optimal inference favors scaling solution generation more aggressively than scaling the number of verifications. Our work provides practical guidance on optimizing test-time scaling by balancing solution generation and verification. The code is available at <a target="_blank" rel="noopener" href="https://github.com/nishadsinghi/sc-genrm-scaling">https://github.com/nishadsinghi/sc-genrm-scaling</a>. </p>
<blockquote>
<p>在测试时间计算缩放已成为增强大型语言模型（尤其是数学问题解决等任务中的）推理能力的一种关键策略。一种传统的方法是自洽性（SC），它通过多数投票机制生成一个问题的多个解决方案并选择最常见的答案。另一种常见的方法是利用奖励模型（验证器）为每一个解决方案打分，然后选择最佳的解决方案。最近生成的奖励模型（GenRM）的进展将验证重新构建为下一个标记预测任务，沿新的轴实现推理时间的缩放。具体来说，GenRM生成多个验证链来评估每个解决方案的得分。在有限的推理预算下，这引入了一个基本的权衡：你应使用预算通过SC来扩展解决方案，还是生成较少的解决方案并将计算分配给通过GenRM进行验证？为了解决这个问题，我们在固定的推理预算下评估了GenRM与SC之间的表现。有趣的是，我们发现对于各种模型和数据集的实际推理预算而言，SC在计算效率上比GenRM更高。例如，GenRM在消耗高达8倍的推理计算之后才首次达到与SC相匹配的性能，并且为了优于它，需要更多的计算。此外，我们得出了GenRM范式的推理缩放定律，揭示了计算最优推理更倾向于积极扩展解决方案生成而不是扩大验证数量。我们的工作提供了关于通过平衡解决方案生成和验证来优化测试时间缩放的实用指导。代码可在<a target="_blank" rel="noopener" href="https://github.com/nishadsinghi/sc-genrm-scaling">https://github.com/nishadsinghi/sc-genrm-scaling</a>获取。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.01005v1">PDF</a> 29 pages</p>
<p><strong>Summary</strong></p>
<p>文本主要讨论了在大规模语言模型（LLM）的数学问题解决任务中，测试时计算规模的扩大策略。介绍了两种常见的测试时计算规模扩大策略：自我一致性（SC）和生成奖励模型（GenRM）。SC通过产生多个解决方案并选择最常见的答案，而GenRM将验证重新定义为下一个标记预测任务。研究发现，在有限的推理预算下，SC在计算效率上优于GenRM。此外，还推导出了GenRM的推理规模定律，指出在计算最优推理时，更倾向于积极扩大解决方案的生成，而不是增加验证次数。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>测试时计算规模扩大是提升大规模语言模型（LLM）推理能力，特别是在数学问题解决任务中的关键策略。</li>
<li>自我一致性（SC）和生成奖励模型（GenRM）是两种常用的测试时计算规模扩大策略。</li>
<li>在有限的推理预算下，SC的计算效率高于GenRM。</li>
<li>GenRM在消耗高达8倍的推理计算量后才能与SC相匹配，并需要更多的计算才能超越它。</li>
<li>推导出的GenRM推理规模定律表明，在计算最优推理时，应更倾向于扩大解决方案的生成，而不是增加验证次数。</li>
<li>平衡解决方案的生成和验证是优化测试时计算规模扩大的关键。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.01005">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-c86cf6f5c74155934ce7fda0b2c9fdd7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-81fad848d2fb6c0d0cb1dacfee2113dd.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2ef97ef20edb09ba34cd04c6967adc22.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="MedReason-Eliciting-Factual-Medical-Reasoning-Steps-in-LLMs-via-Knowledge-Graphs"><a href="#MedReason-Eliciting-Factual-Medical-Reasoning-Steps-in-LLMs-via-Knowledge-Graphs" class="headerlink" title="MedReason: Eliciting Factual Medical Reasoning Steps in LLMs via   Knowledge Graphs"></a>MedReason: Eliciting Factual Medical Reasoning Steps in LLMs via   Knowledge Graphs</h2><p><strong>Authors:Juncheng Wu, Wenlong Deng, Xingxuan Li, Sheng Liu, Taomian Mi, Yifan Peng, Ziyang Xu, Yi Liu, Hyunjin Cho, Chang-In Choi, Yihan Cao, Hui Ren, Xiang Li, Xiaoxiao Li, Yuyin Zhou</strong></p>
<p>Medical tasks such as diagnosis and treatment planning require precise and complex reasoning, particularly in life-critical domains. Unlike mathematical reasoning, medical reasoning demands meticulous, verifiable thought processes to ensure reliability and accuracy. However, there is a notable lack of datasets that provide transparent, step-by-step reasoning to validate and enhance the medical reasoning ability of AI models. To bridge this gap, we introduce MedReason, a large-scale high-quality medical reasoning dataset designed to enable faithful and explainable medical problem-solving in large language models (LLMs). We utilize a structured medical knowledge graph (KG) to convert clinical QA pairs into logical chains of reasoning, or &#96;&#96;thinking paths’’, which trace connections from question elements to answers via relevant KG entities. Each path is validated for consistency with clinical logic and evidence-based medicine. Our pipeline generates detailed reasoning for various medical questions from 7 medical datasets, resulting in a dataset of 32,682 question-answer pairs, each with detailed, step-by-step explanations. Experiments demonstrate that fine-tuning with our dataset consistently boosts medical problem-solving capabilities, achieving significant gains of up to 7.7% for DeepSeek-Ditill-8B. Our top-performing model, MedReason-8B, outperforms the Huatuo-o1-8B, a state-of-the-art medical reasoning model, by up to 4.2% on the clinical benchmark MedBullets. We also engage medical professionals from diverse specialties to assess our dataset’s quality, ensuring MedReason offers accurate and coherent medical reasoning. Our data, models, and code will be publicly available. </p>
<blockquote>
<p>医学任务如诊断和制定治疗方案需要进行精确且复杂的推理，特别是在关键的生命领域。不同于数学推理，医学推理需要细致且可验证的思考过程，以确保可靠性和准确性。然而，缺乏提供透明、逐步推理的数据集来验证和提升AI模型的医学推理能力。为了弥补这一差距，我们引入了MedReason，这是一个大规模、高质量的医学推理数据集，旨在在大语言模型（LLM）中实现忠实且可解释的医学问题解决。我们利用结构化医学知识图谱（KG）将临床问答对转换为逻辑推理链，或称为“思考路径”，从问题元素追踪到通过相关KG实体得到的答案。每条路径都经过临床逻辑和循证医学的验证。我们的管道从7个医学数据集中生成了各种医学问题的详细推理，形成了一个包含32,682个问答对的数据集，每个问答对都有详细、逐步的解释。实验表明，使用我们的数据集进行微调可以持续提高医学问题解决能力，对DeepSeek-Ditill-8B的改进幅度高达7.7%。我们表现最佳的模型MedReason-8B在临床医学基准测试MedBullets上的表现优于最新的医学推理模型华佗-o1-8B，最高提升了4.2%。我们还邀请了来自不同专业的医疗专家来评估我们数据集的质量，确保MedReason提供准确且连贯的医学推理。我们的数据、模型和代码将公开可用。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.00993v1">PDF</a> </p>
<p><strong>摘要</strong></p>
<p>医疗任务的诊断与治疗规划需要精确且复杂的推理，特别是在关键的生命领域。医疗推理不同于数学推理，需要细致且可验证的思考过程以确保可靠性和准确性。然而，缺乏提供透明、逐步推理的数据集来验证和提升AI模型的医疗推理能力。为弥补这一差距，我们推出MedReason，一个大规模高质量的医疗推理数据集，旨在在大型语言模型中实现忠实且可解释的医疗问题解决。我们利用结构化医疗知识图谱将临床问答对转化为逻辑推理链，即“思考路径”，该路径追踪从问题元素到通过相关KG实体回答的联接。每条路径都经过临床逻辑和循证医学的验证。我们的管道生成了各种医疗问题的详细推理，来自7个医疗数据集，形成包含32682个带有详细逐步解释的问题答案对的数据集。实验表明，使用我们的数据集进行微调，可持续提升医疗问题解决能力，在DeepSeek-Ditill-8B上取得高达7.7%的显著增长。我们的顶尖模型MedReason-8B在医疗推理模型华佗o1-8B上取得显著优势，在临床基准测试MedBullets上高出4.2%。我们还邀请来自不同专业的医疗专家来评估我们数据集的质量，确保MedReason提供准确且连贯的医疗推理。我们的数据、模型和代码将公开提供。</p>
<p><strong>关键见解</strong></p>
<ol>
<li>医疗任务需要精确且复杂的推理，特别是在诊断与治疗规划中。</li>
<li>与数学推理不同，医疗推理需要细致和可验证的思考过程。</li>
<li>目前缺乏用于验证和提升AI模型医疗推理能力的数据集。</li>
<li>MedReason是一个大规模高质量的医疗推理数据集，用于大型语言模型中的忠实且可解释的医疗问题解决。</li>
<li>利用结构化医疗知识图谱将临床问答转化为逻辑推理链。</li>
<li>MedReason数据集包含详细逐步解释的问题答案对，由来自多个医疗数据集的问题生成。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.00993">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-8f22aecfb5ebdd25cc2df9a588690e0b.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-8872d81354e8a2449ca9bf6971a61359.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-255ca0fc871261174452f7da393ae326.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="Grounding-Multimodal-LLMs-to-Embodied-Agents-that-Ask-for-Help-with-Reinforcement-Learning"><a href="#Grounding-Multimodal-LLMs-to-Embodied-Agents-that-Ask-for-Help-with-Reinforcement-Learning" class="headerlink" title="Grounding Multimodal LLMs to Embodied Agents that Ask for Help with   Reinforcement Learning"></a>Grounding Multimodal LLMs to Embodied Agents that Ask for Help with   Reinforcement Learning</h2><p><strong>Authors:Ram Ramrakhya, Matthew Chang, Xavier Puig, Ruta Desai, Zsolt Kira, Roozbeh Mottaghi</strong></p>
<p>Embodied agents operating in real-world environments must interpret ambiguous and under-specified human instructions. A capable household robot should recognize ambiguity and ask relevant clarification questions to infer the user intent accurately, leading to more effective task execution. To study this problem, we introduce the Ask-to-Act task, where an embodied agent must fetch a specific object instance given an ambiguous instruction in a home environment. The agent must strategically ask minimal, yet relevant, clarification questions to resolve ambiguity while navigating under partial observability. To solve this problem, we propose a novel approach that fine-tunes multimodal large language models (MLLMs) as vision-language-action (VLA) policies using online reinforcement learning (RL) with LLM-generated rewards. Our method eliminates the need for large-scale human demonstrations or manually engineered rewards for training such agents. We benchmark against strong zero-shot baselines, including GPT-4o, and supervised fine-tuned MLLMs, on our task. Our results demonstrate that our RL-finetuned MLLM outperforms all baselines by a significant margin ($19.1$-$40.3%$), generalizing well to novel scenes and tasks. To the best of our knowledge, this is the first demonstration of adapting MLLMs as VLA agents that can act and ask for help using LLM-generated rewards with online RL. </p>
<blockquote>
<p>实体代理在现实环境中运作时，必须解读模糊和未明确指定的人类指令。一个能力强大的家用机器人应该能够识别模糊性，并提出相关澄清问题，以准确推断用户意图，从而更有效地执行任务。为了研究这个问题，我们引入了“问再行动”任务，在该任务中，实体代理必须在家庭环境中根据模糊指令获取特定对象实例。代理必须策略性地提出最少但相关的问题来解决模糊性，同时在部分可观察性下进行导航。为了解决这个问题，我们提出了一种新方法，该方法微调多模态大型语言模型（MLLMs），将其作为视觉语言行动（VLA）策略使用在线强化学习（RL）与LLM生成的奖励。我们的方法消除了大型人类演示或手动工程奖励来训练此类代理的需要。我们在任务上与强大的零样本基线进行了基准测试，包括GPT-4o和经过监督训练的MLLMs。我们的结果表明，我们的RL微调MLLM在所有基准测试中表现优越较大幅度（介于19.1％至40.3％），对新场景和任务具有良好的泛化能力。据我们所知，这是首次展示适应MLLM作为VLA代理的能力，该代理可以使用LLM生成的奖励与在线RL来行动和寻求帮助。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.00907v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文探讨了在真实世界环境中运行的实体代理需要解读模糊和未明确指定的人类指令的问题。为了解决这个问题，提出了一种新型方法，即通过在线强化学习微调多模态大型语言模型，将其作为视觉语言行动策略。该方法无需大规模人类演示或手动设计的奖励来训练代理，且相较于零样本基准测试，如GPT-4o和通过监督微调的多模态大型语言模型，表现出显著优势。这是首次将多模态大型语言模型适应为能行动和求助的视觉语言行动代理。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>实体代理需解读模糊和未明确指定的人类指令。</li>
<li>家用机器人应能识别歧义并提出相关澄清问题，以准确推断用户意图，从而提高任务执行效率。</li>
<li>引入了“问询行动”任务，即实体代理需在家庭环境中根据模糊指令获取特定对象实例。</li>
<li>代理需策略性地提出最少但相关的澄清问题以解决歧义，同时在部分可观察性环境下进行导航。</li>
<li>提出一种新型方法，通过在线强化学习微调多模态大型语言模型，作为视觉语言行动策略。</li>
<li>该方法无需大规模人类演示或手动设计的奖励，且表现出显著优于零样本基准测试和监督微调的多模态大型语言模型的性能。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.00907">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-f7c8c62ca23f49913ac819badf630190.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-557dae7264ae9b2d318eff1046e3e062.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ed34d8618db18090f3bf1b277f9f34fa.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="GenPRM-Scaling-Test-Time-Compute-of-Process-Reward-Models-via-Generative-Reasoning"><a href="#GenPRM-Scaling-Test-Time-Compute-of-Process-Reward-Models-via-Generative-Reasoning" class="headerlink" title="GenPRM: Scaling Test-Time Compute of Process Reward Models via   Generative Reasoning"></a>GenPRM: Scaling Test-Time Compute of Process Reward Models via   Generative Reasoning</h2><p><strong>Authors:Jian Zhao, Runze Liu, Kaiyan Zhang, Zhimu Zhou, Junqi Gao, Dong Li, Jiafei Lyu, Zhouyi Qian, Biqing Qi, Xiu Li, Bowen Zhou</strong></p>
<p>Recent advancements in Large Language Models (LLMs) have shown that it is promising to utilize Process Reward Models (PRMs) as verifiers to enhance the performance of LLMs. However, current PRMs face three key challenges: (1) limited process supervision and generalization capabilities, (2) dependence on scalar value prediction without leveraging the generative abilities of LLMs, and (3) inability to scale the test-time compute of PRMs. In this work, we introduce GenPRM, a generative process reward model that performs explicit Chain-of-Thought (CoT) reasoning with code verification before providing judgment for each reasoning step. To obtain high-quality process supervision labels and rationale data, we propose Relative Progress Estimation (RPE) and a rationale synthesis framework that incorporates code verification. Experimental results on ProcessBench and several mathematical reasoning tasks show that GenPRM significantly outperforms prior PRMs with only 23K training data from MATH dataset. Through test-time scaling, a 1.5B GenPRM outperforms GPT-4o, and a 7B GenPRM surpasses Qwen2.5-Math-PRM-72B on ProcessBench. Additionally, GenPRM demonstrates strong abilities to serve as a critic model for policy model refinement. This work establishes a new paradigm for process supervision that bridges the gap between PRMs and critic models in LLMs. Our code, model, and data will be available in <a target="_blank" rel="noopener" href="https://ryanliu112.github.io/GenPRM">https://ryanliu112.github.io/GenPRM</a>. </p>
<blockquote>
<p>近期大型语言模型（LLM）的进展显示出利用过程奖励模型（PRM）作为验证器以提高LLM性能的潜力。然而，当前PRM面临三个关键挑战：（1）过程监督与泛化能力有限，（2）依赖于标量值预测，未能利用LLM的生成能力，（3）无法扩展PRM的测试时间计算。在这项工作中，我们引入了GenPRM，一种生成式过程奖励模型，它在进行代码验证后进行明确的思维链（CoT）推理，然后为每一步推理提供判断。为了获得高质量的过程监督标签和理性数据，我们提出了相对进度估计（RPE）和一个结合代码验证的理性合成框架。在ProcessBench和多个数学推理任务上的实验结果表明，仅使用MATH数据集的23K训练数据，GenPRM就显著优于先前的PRM。通过测试时间缩放，一个1.5B的GenPRM表现优于GPT-4o，而一个7B的GenPRM在ProcessBench上超越了Qwen2.5-Math-PRM-72B。此外，GenPRM表现出强大的能力，可以作为政策模型细化的批评模型。这项工作为过程监督建立了新范式，缩小了PRM和LLM中批评模型之间的差距。我们的代码、模型和数据将在<a target="_blank" rel="noopener" href="https://ryanliu112.github.io/GenPRM%E4%B8%8A%E6%8F%90%E4%BE%9B%E3%80%82">https://ryanliu112.github.io/GenPRM上提供。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.00891v1">PDF</a> </p>
<p><strong>Summary</strong>：<br>最近的大型语言模型（LLM）的进步显示，利用过程奖励模型（PRM）作为验证器来提高LLM的性能具有前景。然而，当前PRM面临三个主要挑战。在这项工作中，我们引入了GenPRM，一个执行显式链式思维（CoT）推理的过程奖励模型，在提供每个推理步骤的判断之前进行代码验证。通过相对进度估计（RPE）和融合代码验证的理性合成框架，GenPRM在ProcessBench和数学推理任务上显著优于先前的PRM，仅使用MATH数据集的23K训练数据。此外，GenPRM还表现出强大的能力，可作为政策模型改进的批评模型。这项工作为PRM和LLM中的批评模型之间的过程监督建立了新的范式。</p>
<p><strong>Key Takeaways</strong>：</p>
<ol>
<li>GenPRM是一个结合显式链式思维（CoT）推理和代码验证的过程奖励模型。</li>
<li>当前PRM面临的主要挑战包括有限的进程监督、缺乏泛化能力以及对标量值预测的依赖等。</li>
<li>GenPRM通过相对进度估计（RPE）和理性合成框架获得高质量的过程监督标签和理性数据。</li>
<li>在数学推理任务上，GenPRM显著优于先前的PRM，仅使用有限的训练数据。</li>
<li>GenPRM在测试时的可扩展性表现优异，较大的模型如7B GenPRM在ProcessBench上超越了Qwen2.5-Math-PRM-72B。</li>
<li>GenPRM不仅作为验证器表现出色，还具备作为批评模型进行政策模型改进的能力。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.00891">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-b2ca0c8ebd3d824a7ce0879f1881e471.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-70fae2e17e281ee21f550d08d308fa92.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-eb9ca27e8126977b2a49c93b31bd2e84.jpg" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="Improved-Visual-Spatial-Reasoning-via-R1-Zero-Like-Training"><a href="#Improved-Visual-Spatial-Reasoning-via-R1-Zero-Like-Training" class="headerlink" title="Improved Visual-Spatial Reasoning via R1-Zero-Like Training"></a>Improved Visual-Spatial Reasoning via R1-Zero-Like Training</h2><p><strong>Authors:Zhenyi Liao, Qingsong Xie, Yanhao Zhang, Zijian Kong, Haonan Lu, Zhenyu Yang, Zhijie Deng</strong></p>
<p>Increasing attention has been placed on improving the reasoning capacities of multi-modal large language models (MLLMs). As the cornerstone for AI agents that function in the physical realm, video-based visual-spatial intelligence (VSI) emerges as one of the most pivotal reasoning capabilities of MLLMs. This work conducts a first, in-depth study on improving the visual-spatial reasoning of MLLMs via R1-Zero-like training. Technically, we first identify that the visual-spatial reasoning capacities of small- to medium-sized Qwen2-VL models cannot be activated via Chain of Thought (CoT) prompts. We then incorporate GRPO training for improved visual-spatial reasoning, using the carefully curated VSI-100k dataset, following DeepSeek-R1-Zero. During the investigation, we identify the necessity to keep the KL penalty (even with a small value) in GRPO. With just 120 GPU hours, our vsGRPO-2B model, fine-tuned from Qwen2-VL-2B, can outperform the base model by 12.1% and surpass GPT-4o. Moreover, our vsGRPO-7B model, fine-tuned from Qwen2-VL-7B, achieves performance comparable to that of the best open-source model LLaVA-NeXT-Video-72B. Additionally, we compare vsGRPO to supervised fine-tuning and direct preference optimization baselines and observe strong performance superiority. The code and dataset will be available soon. </p>
<blockquote>
<p>对多模态大型语言模型（MLLMs）的推理能力越来越重视。作为在物理领域发挥作用的AI代理的基石，基于视频的视觉空间智能（VSI）已成为MLLMs中最关键的推理能力之一。这项工作通过R1-Zero类似的训练，首次深入研究了提高MLLMs的视觉空间推理能力。从技术上讲，我们首先发现中小型Qwen2-VL模型的视觉空间推理能力无法通过思维链（CoT）提示来激活。然后，我们采用GRPO训练来提高视觉空间推理能力，使用精心编制的VSI-100k数据集，遵循DeepSeek-R1-Zero。在调查过程中，我们发现了在GRPO中保持KL惩罚（即使值很小）的必要性。只需120个GPU小时，我们从Qwen2-VL-2B微调得到的vsGRPO-2B模型，就可以比基础模型高出12.1%，并超越GPT-4o。此外，我们从Qwen2-VL-7B微调得到的vsGRPO-7B模型，其性能可与最佳开源模型LLaVA-NeXT-Video-72B相媲美。另外，我们将vsGRPO与监督微调和直接偏好优化基线进行了比较，并观察到其性能卓越。代码和数据集将很快可用。</p>
</blockquote>
<p><strong>简化版翻译</strong></p>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.00883v1">PDF</a> </p>
<p><strong>Summary</strong><br>     随着对多模态大型语言模型（MLLMs）推理能力的关注增加，基于视频的视觉空间智能（VSI）作为MLLMs在物理领域工作的核心，显得尤为重要。本研究通过R1-Zero类似的训练方式，深入探讨了提高MLLMs的视觉空间推理能力。研究发现，中小型Qwen2-VL模型的视觉空间推理能力无法通过Chain of Thought（CoT）提示激活。通过使用GRPO训练和精心制作的VSI-100k数据集，在改进视觉空间推理方面取得了进展。研究还强调了保持KL惩罚的必要性，即使其值很小。研究结果显示，经过GRPO训练的模型性能显著提升。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>多模态大型语言模型（MLLMs）的推理能力日益受到重视。</li>
<li>基于视频的视觉空间智能（VSI）是MLLMs在物理领域工作的关键。</li>
<li>本研究通过类似R1-Zero的训练方法，深入探讨了提高MLLMs的视觉空间推理能力。</li>
<li>中小型Qwen2-VL模型的视觉空间推理能力无法通过Chain of Thought（CoT）提示激活。</li>
<li>使用GRPO训练和VSI-100k数据集有助于提高视觉空间推理能力。</li>
<li>研究强调了保持KL惩罚在GRPO训练中的必要性。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.00883">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-a4f989373a810a4e58e173363e7b030e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3a7e3a317fbbffc23c37ea7f7028b3ff.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1bd62ee9b47b3fb9fea30a8eb7911b76.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-1be129ecd7e013f263b75c042aa150e4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b74d3af0c938099ed721303c2e7f0363.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-82fe3dd7e90155b3f9090f9026ffcb3e.jpg" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="m1-Unleash-the-Potential-of-Test-Time-Scaling-for-Medical-Reasoning-with-Large-Language-Models"><a href="#m1-Unleash-the-Potential-of-Test-Time-Scaling-for-Medical-Reasoning-with-Large-Language-Models" class="headerlink" title="m1: Unleash the Potential of Test-Time Scaling for Medical Reasoning   with Large Language Models"></a>m1: Unleash the Potential of Test-Time Scaling for Medical Reasoning   with Large Language Models</h2><p><strong>Authors:Xiaoke Huang, Juncheng Wu, Hui Liu, Xianfeng Tang, Yuyin Zhou</strong></p>
<p>Test-time scaling has emerged as a powerful technique for enhancing the reasoning capabilities of large language models. However, its effectiveness in medical reasoning remains uncertain, as the medical domain fundamentally differs from mathematical tasks in terms of knowledge representation and decision-making processes. In this paper, we provide the first comprehensive investigation of test-time scaling for medical reasoning and present m1, a simple yet effective approach that increases a model’s medical reasoning capability at inference. Our evaluation across diverse medical tasks demonstrates that test-time scaling consistently enhances medical reasoning, enabling lightweight fine-tuned models under 10B parameters to establish new state-of-the-art performance, while our 32B model rivals previous 70B-scale medical LLMs. However, we identify an optimal reasoning token budget of approximately 4K, beyond which performance may degrade due to overthinking. Budget forcing, which extends test-time computation through iterative prompts, helps models double-check answers but does not necessarily improve the overall medical QA performance and, in some cases, even introduces errors into previously correct responses. Our case-by-case analysis identifies insufficient medical knowledge as a key bottleneck that prevents further performance gains through test-time scaling. We find that increasing data scale, improving data quality, and expanding model capacity consistently enhance medical knowledge grounding, enabling continued performance improvements, particularly on challenging medical benchmarks where smaller models reach saturation. These findings underscore fundamental differences between medical and mathematical reasoning in LLMs, highlighting that enriched medical knowledge, other than increased reasoning depth alone, is essential for realizing the benefits of test-time scaling. </p>
<blockquote>
<p>测试时缩放技术已成为增强大型语言模型推理能力的一种强大技术。然而，其在医疗推理中的有效性尚不确定，因为医疗领域在知识表达和决策过程方面与数学任务存在根本区别。在本文中，我们对医疗推理的测试时缩放技术进行了首次全面调查，并提出了一种简单有效的m1方法，可以在推理过程中提高模型的医疗推理能力。我们在各种医疗任务上的评估表明，测试时缩放技术可以持续增强医疗推理能力，使经过轻量级微调、参数不足10B的模型能够创造新的最先进的性能表现；我们的32B模型的表现则与前些年规模达到70B的医疗大型语言模型（LLM）不相上下。然而，我们确定了大约4K的最佳推理令牌预算，超出此预算后，由于过度思考，性能可能会下降。强制预算通过迭代提示延长测试时的计算时间，有助于模型检查答案，但并不一定会提高整体的医疗问答表现，有时甚至会将之前正确的答案引入错误。我们的逐案分析发现，医疗知识不足是一个关键瓶颈，阻碍了通过测试时缩放进一步提升性能。我们发现，通过增加数据规模、提高数据质量和扩大模型容量，可以持续增强医疗知识的依据性，从而在具有挑战性的医疗基准测试上实现持续的性能改进，尤其是小型模型达到饱和状态的情况下更是如此。这些发现强调了大型语言模型中医疗推理与数学推理之间的根本区别，并指出除了增加推理深度外，丰富的医疗知识对于实现测试时缩放的好处至关重要。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.00869v1">PDF</a> 17 pages; 7 figures; Data, code, and models:   <a target="_blank" rel="noopener" href="https://github.com/UCSC-VLAA/m1">https://github.com/UCSC-VLAA/m1</a></p>
<p><strong>Summary</strong></p>
<p>本文探索了测试时缩放技术在医疗推理中的应用。研究提出了一种简单有效的方法m1，可以提高模型在推理阶段的医疗推理能力。评估表明，测试时缩放技术能持续提高医疗推理能力，使轻量级微调模型达到最新性能水平。然而，研究还发现存在一个约4K的推理令牌预算最优值，超出此范围可能会导致性能下降。预算强制有助于模型进行答案复查，但不一定能提高整体医疗问答性能，甚至可能导致先前正确的答案出现错误。研究认为，医疗知识不足是阻碍性能进一步提高的关键瓶颈。增加数据规模、提高数据质量和扩大模型容量能持续提高医疗知识的定位能力，特别是在挑战性医疗基准测试中，小型模型达到饱和状态后效果更明显。这些发现强调了医疗和数学推理在大型语言模型中的根本区别，并指出除了增加推理深度外，丰富的医疗知识对于实现测试时缩放技术的优势至关重要。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>测试时缩放技术用于提高大型语言模型在医疗推理中的能力。</li>
<li>m1方法能有效提升模型的医疗推理能力。</li>
<li>测试时缩放技术能使轻量级模型达到最新医疗推理性能水平。</li>
<li>存在一个约4K的推理令牌预算最优值，超出此范围可能导致性能下降。</li>
<li>预算强制有助于模型复查答案，但可能不总能提升医疗问答性能。</li>
<li>医疗知识不足是阻碍进一步提高医疗推理性能的关键问题。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.00869">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-cfd5d5f354c61e48ba3907c2d0b033af.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3cc5bb6b764ccd862c0bdc746a0e8f3c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-46882d4b085eb516211e89a4b08ee922.jpg" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1><h2 id="DynMoLE-Boosting-Mixture-of-LoRA-Experts-Fine-Tuning-with-a-Hybrid-Routing-Mechanism"><a href="#DynMoLE-Boosting-Mixture-of-LoRA-Experts-Fine-Tuning-with-a-Hybrid-Routing-Mechanism" class="headerlink" title="DynMoLE: Boosting Mixture of LoRA Experts Fine-Tuning with a Hybrid   Routing Mechanism"></a>DynMoLE: Boosting Mixture of LoRA Experts Fine-Tuning with a Hybrid   Routing Mechanism</h2><p><strong>Authors:Dengchun Li, Naizheng Wang, Zihao Zhang, Haoyang Yin, Lei Duan, Meng Xiao, Mingjie Tang</strong></p>
<p>Instruction-based fine-tuning of large language models (LLMs) has achieved remarkable success in various natural language processing (NLP) tasks. Parameter-efficient fine-tuning (PEFT) methods, such as Mixture of LoRA Experts (MoLE), combine the efficiency of Low-Rank Adaptation (LoRA) with the versatility of Mixture of Experts (MoE) models, demonstrating significant potential for handling multiple downstream tasks. However, the existing routing mechanisms for MoLE often involve a trade-off between computational efficiency and predictive accuracy, and they fail to fully address the diverse expert selection demands across different transformer layers. In this work, we propose DynMoLE, a hybrid routing strategy that dynamically adjusts expert selection based on the Tsallis entropy of the router’s probability distribution. This approach mitigates router uncertainty, enhances stability, and promotes more equitable expert participation, leading to faster convergence and improved model performance. Additionally, we introduce an auxiliary loss based on Tsallis entropy to further guide the model toward convergence with reduced uncertainty, thereby improving training stability and performance. Our extensive experiments on commonsense reasoning benchmarks demonstrate that DynMoLE achieves substantial performance improvements, outperforming LoRA by 9.6% and surpassing the state-of-the-art MoLE method, MoLA, by 2.3%. We also conduct a comprehensive ablation study to evaluate the contributions of DynMoLE’s key components. </p>
<blockquote>
<p>基于指令的大型语言模型（LLM）微调在自然语言处理（NLP）的各种任务中取得了显著的成功。参数高效微调（PEFT）方法，如混合LoRA专家（MoLE），结合了低秩适应（LoRA）的效率与混合专家（MoE）模型的通用性，在处理多个下游任务方面显示出巨大潜力。然而，MoLE的现有路由机制通常在计算效率和预测准确性之间进行权衡，并且未能完全满足不同transformer层之间多样化的专家选择需求。在这项工作中，我们提出了DynMoLE，一种基于路由器概率分布的Tsallis熵动态调整专家选择的混合路由策略。这种方法减轻了路由器的不确定性，增强了稳定性，促进了更公平的专家参与，从而实现了更快的收敛和模型性能的提高。此外，我们引入了一种基于Tsallis熵的辅助损失，进一步引导模型朝着减少不确定性的方向收敛，从而提高了训练稳定性和性能。我们在常识推理基准测试上的大量实验表明，DynMoLE实现了显著的性能改进，比LoRA高出9.6%，并超越了最先进的MoLE方法MoLA，提高了2.3%。我们还进行了全面的消融研究，以评估DynMoLE关键组件的贡献。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.00661v1">PDF</a> 22 pages, 7 figures</p>
<p><strong>Summary</strong></p>
<p>大型语言模型（LLM）的指令微调已经取得显著成功。参数高效微调（PEFT）方法如混合LoRA专家（MoLE）结合了LoRA的效率与MoE模型的灵活性。然而，现有MoLE的路由机制在计算效率和预测精度之间存在权衡，不能满足不同transformer层之间的专家选择需求。本研究提出DynMoLE，一种基于Tsallis熵的路由器概率分布动态调整专家选择的混合路由策略。该方法减轻了路由器的不确定性，增强了稳定性，促进了更公平的专家参与，导致更快的收敛和模型性能的提升。在常识推理基准测试上的实验表明，DynMoLE实现了显著的性能改进。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>大型语言模型（LLM）的指令微调在NLP任务中取得显著成功。</li>
<li>参数高效微调（PEFT）方法结合了LoRA的效率与MoE模型的灵活性。</li>
<li>MoLE的现有路由机制存在计算效率和预测精度之间的权衡。</li>
<li>DynMoLE提出一种基于Tsallis熵的混合路由策略，动态调整专家选择。</li>
<li>DynMoLE减轻了路由器的不确定性，增强了稳定性和模型性能。</li>
<li>DynMoLE在常识推理任务上实现了显著的性能改进，优于LoRA和其他最新方法。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.00661">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-f988dac6d1b1da6d676703bc99001671.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-6bd628a8f16fc35bf72b375c066ff9ca.jpg" align="middle">
</details>


<h1 id="-19"><a href="#-19" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        文章作者:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        文章链接:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-04-03/R1_Reasoning/">https://kedreamix.github.io/Talk2Paper/Paper/2025-04-03/R1_Reasoning/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        版权声明:
                    </i>
                </span>
                <span class="reprint-info">
                    本博客所有文章除特別声明外，均采用
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    许可协议。转载请注明来源
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/R1-Reasoning/">
                                    <span class="chip bg-color">R1_Reasoning</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;上一篇</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-04-03/LLM/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-e673eac368ee8357e01903864dfaf4db.jpg" class="responsive-img" alt="LLM">
                        
                        <span class="card-title">LLM</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            LLM 方向最新论文已更新，请持续关注 Update in 2025-04-03  Towards Unified Referring Expression Segmentation Across Omni-Level   Visual Target Granularities
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-04-03
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/LLM/" class="post-category">
                                    LLM
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/LLM/">
                        <span class="chip bg-color">LLM</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                下一篇&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-03-29/3DGS/">
                    <div class="card-image">
                        
                        <img src="https://pica.zhimg.com/v2-8b3e8b0158a01167bf17e6207de84052.jpg" class="responsive-img" alt="3DGS">
                        
                        <span class="card-title">3DGS</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            3DGS 方向最新论文已更新，请持续关注 Update in 2025-03-29  X$^{2}$-Gaussian 4D Radiative Gaussian Splatting for Continuous-time   Tomographic Reconstruction
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-03-29
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/3DGS/" class="post-category">
                                    3DGS
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/3DGS/">
                        <span class="chip bg-color">3DGS</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- 代码块功能依赖 -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- 代码语言 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- 代码块复制 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- 代码块收缩 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC 悬浮按钮. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* 修复文章卡片 div 的宽度. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // 切换TOC目录展开收缩的相关操作.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;站点总字数:&nbsp;<span
                        class="white-color">16042k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- 运行天数提醒. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // 区分是否有年份.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = '本站已运行 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                daysTip = '本站已運行 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = '本站已运行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = '本站已運行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="关注我的知乎: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">知</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- 搜索遮罩框 -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span>
            <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- 白天和黑夜主题 -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- 回到顶部按钮 -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // 只在桌面版网页启用特效
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- 雪花特效 -->
    

    <!-- 鼠标星星特效 -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--腾讯兔小巢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- 动态标签 -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Σ(っ °Д °;)っ诶，页面崩溃了嘛？", clearTimeout(st)) : (document.title = "φ(゜▽゜*)♪咦，又好了！", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
