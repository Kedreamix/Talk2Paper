<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="R1_Reasoning">
    <meta name="description" content="R1_Reasoning æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-04-03  OpenCodeReasoning Advancing Data Distillation for Competitive Coding">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>R1_Reasoning | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-d3ea20c763842611ad3c4ee24851c5f8.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">R1_Reasoning</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/R1-Reasoning/">
                                <span class="chip bg-color">R1_Reasoning</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/R1-Reasoning/" class="post-category">
                                R1_Reasoning
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-04-03
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-04-19
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    20.2k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    82 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-04-03-æ›´æ–°"><a href="#2025-04-03-æ›´æ–°" class="headerlink" title="2025-04-03 æ›´æ–°"></a>2025-04-03 æ›´æ–°</h1><h2 id="OpenCodeReasoning-Advancing-Data-Distillation-for-Competitive-Coding"><a href="#OpenCodeReasoning-Advancing-Data-Distillation-for-Competitive-Coding" class="headerlink" title="OpenCodeReasoning: Advancing Data Distillation for Competitive Coding"></a>OpenCodeReasoning: Advancing Data Distillation for Competitive Coding</h2><p><strong>Authors:Wasi Uddin Ahmad, Sean Narenthiran, Somshubra Majumdar, Aleksander Ficek, Siddhartha Jain, Jocelyn Huang, Vahid Noroozi, Boris Ginsburg</strong></p>
<p>Since the advent of reasoning-based large language models, many have found great success from distilling reasoning capabilities into student models. Such techniques have significantly bridged the gap between reasoning and standard LLMs on coding tasks. Despite this, much of the progress on distilling reasoning models remains locked behind proprietary datasets or lacks details on data curation, filtering and subsequent training. To address this, we construct a superior supervised fine-tuning (SFT) dataset that we use to achieve state-of-the-art coding capability results in models of various sizes. Our distilled models use only SFT to achieve 61.8% on LiveCodeBench and 24.6% on CodeContests, surpassing alternatives trained with reinforcement learning. We then perform analysis on the data sources used to construct our dataset, the impact of code execution filtering, and the importance of instruction&#x2F;solution diversity. We observe that execution filtering negatively affected benchmark accuracy, leading us to prioritize instruction diversity over solution correctness. Finally, we also analyze the token efficiency and reasoning patterns utilized by these models. We will open-source these datasets and distilled models to the community. </p>
<blockquote>
<p>è‡ªä»åŸºäºæ¨ç†çš„å¤§å‹è¯­è¨€æ¨¡å‹å‡ºç°ä»¥æ¥ï¼Œè®¸å¤šäººå‘ç°é€šè¿‡å°†æ¨ç†èƒ½åŠ›è’¸é¦åˆ°å­¦ç”Ÿæ¨¡å‹ä¸­å–å¾—äº†å·¨å¤§æˆåŠŸã€‚è¿™ç±»æŠ€æœ¯åœ¨ç¼–ç ä»»åŠ¡ä¸Šæ˜¾è‘—ç¼©å°äº†æ¨ç†å’Œæ ‡å‡†å¤§å‹è¯­è¨€æ¨¡å‹ä¹‹é—´çš„å·®è·ã€‚å°½ç®¡å¦‚æ­¤ï¼Œå¤§éƒ¨åˆ†å…³äºè’¸é¦æ¨ç†æ¨¡å‹çš„è¿›å±•ä»å±€é™äºä¸“æœ‰æ•°æ®é›†ï¼Œæˆ–è€…ç¼ºä¹å…³äºæ•°æ®æ”¶é›†ã€è¿‡æ»¤å’Œåç»­è®­ç»ƒæ–¹é¢çš„è¯¦ç»†ä¿¡æ¯ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æ„å»ºäº†ä¸€ä¸ªä¼˜è´¨çš„ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰æ•°æ®é›†ï¼Œç”¨äºåœ¨å„ç§è§„æ¨¡çš„æ¨¡å‹ä¸­å®ç°æœ€å…ˆè¿›çš„ç¼–ç èƒ½åŠ›ç»“æœã€‚æˆ‘ä»¬çš„è’¸é¦æ¨¡å‹ä»…ä½¿ç”¨SFTå°±å®ç°äº†LiveCodeBenchçš„61.8%å’ŒCodeContestsçš„24.6%ï¼Œè¶…è¶Šäº†ä½¿ç”¨å¼ºåŒ–å­¦ä¹ è®­ç»ƒçš„æ›¿ä»£æ–¹æ¡ˆã€‚ç„¶åï¼Œæˆ‘ä»¬å¯¹æ„å»ºæ•°æ®é›†æ‰€ä½¿ç”¨æ•°æ®æºçš„åˆ†æã€ä»£ç æ‰§è¡Œè¿‡æ»¤çš„å½±å“ä»¥åŠæŒ‡ä»¤&#x2F;è§£å†³æ–¹æ¡ˆå¤šæ ·æ€§çš„é‡è¦æ€§ã€‚æˆ‘ä»¬å‘ç°æ‰§è¡Œè¿‡æ»¤å¯¹åŸºå‡†æµ‹è¯•å‡†ç¡®æ€§äº§ç”Ÿäº†è´Ÿé¢å½±å“ï¼Œå¯¼è‡´æˆ‘ä»¬ä¼˜å…ˆé‡è§†æŒ‡ä»¤çš„å¤šæ ·æ€§è€Œéè§£å†³æ–¹æ¡ˆçš„æ­£ç¡®æ€§ã€‚æœ€åï¼Œæˆ‘ä»¬è¿˜åˆ†æäº†è¿™äº›æ¨¡å‹çš„ä»¤ç‰Œæ•ˆç‡å’Œæ¨ç†æ¨¡å¼ã€‚æˆ‘ä»¬å°†å‘ç¤¾åŒºå¼€æºè¿™äº›æ•°æ®é›†å’Œè’¸é¦æ¨¡å‹ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.01943v1">PDF</a> Work in progress</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†åŸºäºæ¨ç†çš„å¤§å‹è¯­è¨€æ¨¡å‹çš„å‘å±•æƒ…å†µï¼Œç‰¹åˆ«æ˜¯æ¨¡å‹è’¸é¦æŠ€æœ¯åœ¨æé«˜æ¨¡å‹æ¨ç†èƒ½åŠ›æ–¹é¢çš„åº”ç”¨ã€‚æ–‡ç« æŒ‡å‡ºï¼Œå°½ç®¡å·²æœ‰è®¸å¤šæˆåŠŸçš„å°è¯•ï¼Œä½†å¤§éƒ¨åˆ†è¿›å±•ä»ç„¶å—é™äºä¸“æœ‰æ•°æ®é›†ï¼Œå¹¶ä¸”ç¼ºä¹æ•°æ®æ”¶é›†ã€ç­›é€‰å’Œè®­ç»ƒæ–¹é¢çš„è¯¦ç»†ä¿¡æ¯ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œç ”ç©¶è€…æ„å»ºäº†ä¸€ä¸ªé«˜çº§çš„ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰æ•°æ®é›†ï¼Œè¯¥æ•°æ®é›†å®ç°äº†ä¸åŒè§„æ¨¡çš„æ¨¡å‹åœ¨ç¼–ç ä»»åŠ¡ä¸Šçš„å“è¶Šè¡¨ç°ã€‚ç ”ç©¶è€…åœ¨æ„å»ºæ•°æ®é›†çš„è¿‡ç¨‹ä¸­å¯¹æ•°æ®æºã€ä»£ç æ‰§è¡Œè¿‡æ»¤çš„å½±å“ä»¥åŠæŒ‡ä»¤&#x2F;è§£å†³æ–¹æ¡ˆå¤šæ ·æ€§è¿›è¡Œäº†åˆ†æï¼Œå¹¶å‘ç°æ‰§è¡Œè¿‡æ»¤ä¼šå¯¹åŸºå‡†æµ‹è¯•å‡†ç¡®æ€§äº§ç”Ÿè´Ÿé¢å½±å“ï¼Œå› æ­¤å¼ºè°ƒæŒ‡ä»¤å¤šæ ·æ€§çš„é‡è¦æ€§ã€‚æœ€åï¼Œç ”ç©¶è€…è¿˜åˆ†æäº†è¿™äº›æ¨¡å‹çš„ä»¤ç‰Œæ•ˆç‡å’Œæ¨ç†æ¨¡å¼ã€‚æ–‡ç« å¼ºè°ƒå°†å…¬å¼€è¿™äº›æ•°æ®é›†å’Œè’¸é¦æ¨¡å‹ä¾›ç¤¾åŒºä½¿ç”¨ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ¨ç†å¤§å‹è¯­è¨€æ¨¡å‹é€šè¿‡å°†æ¨ç†èƒ½åŠ›è’¸é¦åˆ°å­¦ç”Ÿæ¨¡å‹ä¸­å–å¾—äº†å·¨å¤§æˆåŠŸã€‚</li>
<li>ç›®å‰å¤§éƒ¨åˆ†è¿›å±•å—é™äºä¸“æœ‰æ•°æ®é›†ï¼Œç¼ºä¹æ•°æ®æ”¶é›†ã€ç­›é€‰å’Œè®­ç»ƒçš„è¯¦ç»†ä¿¡æ¯ã€‚</li>
<li>ç ”ç©¶è€…é€šè¿‡æ„å»ºä¸€ä¸ªé«˜çº§çš„ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰æ•°æ®é›†ï¼Œå®ç°äº†åœ¨ç¼–ç ä»»åŠ¡ä¸Šçš„å“è¶Šè¡¨ç°ã€‚</li>
<li>åœ¨æ„å»ºæ•°æ®é›†è¿‡ç¨‹ä¸­ï¼Œå‘ç°æ‰§è¡Œè¿‡æ»¤ä¼šå¯¹åŸºå‡†æµ‹è¯•å‡†ç¡®æ€§äº§ç”Ÿè´Ÿé¢å½±å“ã€‚</li>
<li>ç ”ç©¶è€…å¼ºè°ƒäº†æŒ‡ä»¤å¤šæ ·æ€§çš„é‡è¦æ€§ï¼Œä¼˜å…ˆäºè§£å†³æ–¹æ¡ˆçš„æ­£ç¡®æ€§ã€‚</li>
<li>åˆ†ææ˜¾ç¤ºï¼Œè¿™äº›æ¨¡å‹å±•ç¤ºäº†æœ‰æ•ˆçš„ä»¤ç‰Œæ•ˆç‡å’Œæ¨ç†æ¨¡å¼ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.01943">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-d4380a2da8480d4b9e1fcb7c28f44a90.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a0bb60cea00b040106932355d54f872f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-6bd0fdea622909f4419f65eb13767bb7.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-003554efde4e4b026d13dedecf6f056d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8561d6b1998c13873167749f11edc13d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-1170c2a7a6ec92d93d72822e29ec3937.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Benchmarking-Synthetic-Tabular-Data-A-Multi-Dimensional-Evaluation-Framework"><a href="#Benchmarking-Synthetic-Tabular-Data-A-Multi-Dimensional-Evaluation-Framework" class="headerlink" title="Benchmarking Synthetic Tabular Data: A Multi-Dimensional Evaluation   Framework"></a>Benchmarking Synthetic Tabular Data: A Multi-Dimensional Evaluation   Framework</h2><p><strong>Authors:Andrey Sidorenko, Michael Platzer, Mario Scriminaci, Paul Tiwald</strong></p>
<p>Evaluating the quality of synthetic data remains a key challenge for ensuring privacy and utility in data-driven research. In this work, we present an evaluation framework that quantifies how well synthetic data replicates original distributional properties while ensuring privacy. The proposed approach employs a holdout-based benchmarking strategy that facilitates quantitative assessment through low- and high-dimensional distribution comparisons, embedding-based similarity measures, and nearest-neighbor distance metrics. The framework supports various data types and structures, including sequential and contextual information, and enables interpretable quality diagnostics through a set of standardized metrics. These contributions aim to support reproducibility and methodological consistency in benchmarking of synthetic data generation techniques. The code of the framework is available at <a target="_blank" rel="noopener" href="https://github.com/mostly-ai/mostlyai-qa">https://github.com/mostly-ai/mostlyai-qa</a>. </p>
<blockquote>
<p>è¯„ä¼°åˆæˆæ•°æ®çš„è´¨é‡ä»ç„¶æ˜¯æ•°æ®é©±åŠ¨ç ”ç©¶ä¸­ç¡®ä¿éšç§å’Œå®ç”¨æ€§çš„å…³é”®æŒ‘æˆ˜ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªè¯„ä¼°æ¡†æ¶ï¼Œè¯¥æ¡†æ¶é‡åŒ–åˆæˆæ•°æ®å¤åˆ¶åŸå§‹åˆ†å¸ƒå±æ€§çš„ç¨‹åº¦ï¼ŒåŒæ—¶ç¡®ä¿éšç§ã€‚æ‰€æå‡ºçš„æ–¹æ³•é‡‡ç”¨åŸºäºç•™å‡ºæ³•çš„åŸºå‡†æµ‹è¯•ç­–ç•¥ï¼Œé€šè¿‡ä½ç»´å’Œé«˜ç»´åˆ†å¸ƒæ¯”è¾ƒã€åŸºäºåµŒå…¥çš„ç›¸ä¼¼æ€§åº¦é‡ä»¥åŠæœ€è¿‘é‚»è·ç¦»åº¦é‡ï¼Œä¿ƒè¿›å®šé‡è¯„ä¼°ã€‚è¯¥æ¡†æ¶æ”¯æŒå„ç§æ•°æ®ç±»å‹å’Œç»“æ„ï¼ŒåŒ…æ‹¬åºåˆ—å’Œä¸Šä¸‹æ–‡ä¿¡æ¯ï¼Œå¹¶é€šè¿‡ä¸€ç³»åˆ—æ ‡å‡†åŒ–æŒ‡æ ‡å®ç°å¯è§£é‡Šçš„è´¨é‡è¯Šæ–­ã€‚è¿™äº›è´¡çŒ®æ—¨åœ¨æ”¯æŒåˆæˆæ•°æ®ç”ŸæˆæŠ€æœ¯çš„åŸºå‡†æµ‹è¯•çš„å†ç°æ€§å’Œæ–¹æ³•ä¸€è‡´æ€§ã€‚è¯¥æ¡†æ¶çš„ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/mostly-ai/mostlyai-qa">https://github.com/mostly-ai/mostlyai-qa</a>æ‰¾åˆ°ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.01908v1">PDF</a> 16 pages, 7 figures, 1 table</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸€ä¸ªè¯„ä¼°åˆæˆæ•°æ®è´¨é‡çš„æ¡†æ¶ï¼Œè¯¥æ¡†æ¶èƒ½å¤Ÿé‡åŒ–åˆæˆæ•°æ®å¤åˆ¶åŸå§‹åˆ†å¸ƒå±æ€§çš„ç¨‹åº¦ï¼ŒåŒæ—¶ç¡®ä¿éšç§ã€‚é‡‡ç”¨åŸºäºä¿ç•™éªŒè¯çš„ç­–ç•¥è¿›è¡Œå®šé‡è¯„ä¼°ï¼Œå¹¶é€šè¿‡ä½ç»´å’Œé«˜ç»´åˆ†å¸ƒæ¯”è¾ƒã€åŸºäºåµŒå…¥çš„ç›¸ä¼¼æ€§åº¦é‡ä»¥åŠæœ€è¿‘é‚»è·ç¦»åº¦é‡ç­‰æ–¹æ³•å®ç°ã€‚è¯¥æ¡†æ¶æ”¯æŒå„ç§æ•°æ®ç±»å‹å’Œç»“æ„ï¼ŒåŒ…æ‹¬åºåˆ—å’Œä¸Šä¸‹æ–‡ä¿¡æ¯ï¼Œå¹¶é€šè¿‡ä¸€ç³»åˆ—æ ‡å‡†åŒ–æŒ‡æ ‡æä¾›å¯è§£é‡Šçš„è´¨é‡è¯Šæ–­ã€‚æ—¨åœ¨æ”¯æŒåˆæˆæ•°æ®ç”ŸæˆæŠ€æœ¯çš„åŸºå‡†æµ‹è¯•çš„å†ç°æ€§å’Œæ–¹æ³•ä¸€è‡´æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>åˆæˆæ•°æ®è´¨é‡è¯„ä¼°æ˜¯ç¡®ä¿æ•°æ®é©±åŠ¨ç ”ç©¶ä¸­éšç§å’Œå®ç”¨æ€§çš„å…³é”®æŒ‘æˆ˜ã€‚</li>
<li>æå‡ºçš„æ¡†æ¶èƒ½å¤Ÿé‡åŒ–åˆæˆæ•°æ®å¯¹åŸå§‹åˆ†å¸ƒå±æ€§çš„å¤åˆ¶ç¨‹åº¦ã€‚</li>
<li>æ¡†æ¶é‡‡ç”¨åŸºäºä¿ç•™éªŒè¯çš„åŸºå‡†æµ‹è¯•ç­–ç•¥ï¼Œæ”¯æŒå¤šç§æ•°æ®ç±»å‹å’Œç»“æ„ã€‚</li>
<li>é€šè¿‡ä½ç»´å’Œé«˜ç»´åˆ†å¸ƒæ¯”è¾ƒã€åŸºäºåµŒå…¥çš„ç›¸ä¼¼æ€§åº¦é‡ä»¥åŠæœ€è¿‘é‚»è·ç¦»åº¦é‡ç­‰æ–¹æ³•è¿›è¡Œå®šé‡è¯„ä¼°ã€‚</li>
<li>æ¡†æ¶æ—¨åœ¨å¹³è¡¡åˆæˆæ•°æ®çš„éšç§å’Œå®ç”¨æ€§ï¼Œç¡®ä¿æ•°æ®çš„å¯è§£é‡Šæ€§å’Œè´¨é‡ã€‚</li>
<li>æ¡†æ¶ä»£ç å·²å…¬å¼€å‘å¸ƒï¼Œä¾¿äºç ”ç©¶äººå‘˜ä½¿ç”¨å’Œæ”¹è¿›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.01908">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-e47696fec227a79c0184952f2f64ff40.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f0d6ec24117a3524e81aec1d7009cff8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5909381f24e2fd92c9523add1136debb.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-71d1f8026d152d7b10f2173dcd738aa1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-00f33e441b81dad7accd768e763cf71e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-27c74494483f40c2151131bbc78140b2.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="STAR-1-Safer-Alignment-of-Reasoning-LLMs-with-1K-Data"><a href="#STAR-1-Safer-Alignment-of-Reasoning-LLMs-with-1K-Data" class="headerlink" title="STAR-1: Safer Alignment of Reasoning LLMs with 1K Data"></a>STAR-1: Safer Alignment of Reasoning LLMs with 1K Data</h2><p><strong>Authors:Zijun Wang, Haoqin Tu, Yuhan Wang, Juncheng Wu, Jieru Mei, Brian R. Bartoldson, Bhavya Kailkhura, Cihang Xie</strong></p>
<p>This paper introduces STAR-1, a high-quality, just-1k-scale safety dataset specifically designed for large reasoning models (LRMs) like DeepSeek-R1. Built on three core principles â€“ diversity, deliberative reasoning, and rigorous filtering â€“ STAR-1 aims to address the critical needs for safety alignment in LRMs. Specifically, we begin by integrating existing open-source safety datasets from diverse sources. Then, we curate safety policies to generate policy-grounded deliberative reasoning samples. Lastly, we apply a GPT-4o-based safety scoring system to select training examples aligned with best practices. Experimental results show that fine-tuning LRMs with STAR-1 leads to an average 40% improvement in safety performance across four benchmarks, while only incurring a marginal decrease (e.g., an average of 1.1%) in reasoning ability measured across five reasoning tasks. Extensive ablation studies further validate the importance of our design principles in constructing STAR-1 and analyze its efficacy across both LRMs and traditional LLMs. Our project page is <a target="_blank" rel="noopener" href="https://ucsc-vlaa.github.io/STAR-1">https://ucsc-vlaa.github.io/STAR-1</a>. </p>
<blockquote>
<p>æœ¬æ–‡ä»‹ç»äº†STAR-1ï¼Œè¿™æ˜¯ä¸€ä¸ªä¸“ä¸ºDeepSeek-R1ç­‰å¤§å‹æ¨ç†æ¨¡å‹è®¾è®¡çš„é«˜è´¨é‡ã€ä»…å«åƒåˆ†ä¹‹ä¸€è§„æ¨¡çš„å®‰å…¨æ•°æ®é›†ã€‚åŸºäºå¤šæ ·æ€§ã€å®¡æ…æ¨ç†å’Œä¸¥æ ¼ç­›é€‰ä¸‰å¤§æ ¸å¿ƒåŸåˆ™ï¼ŒSTAR-1æ—¨åœ¨è§£å†³å¤§å‹æ¨ç†æ¨¡å‹ä¸­å¯¹å®‰å…¨å¯¹é½çš„è¿«åˆ‡éœ€æ±‚ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬é¦–å…ˆæ•´åˆæ¥è‡ªä¸åŒæ¥æºçš„ç°æœ‰å¼€æºå®‰å…¨æ•°æ®é›†ã€‚ç„¶åï¼Œæˆ‘ä»¬åˆ¶å®šå®‰å…¨ç­–ç•¥æ¥ç”ŸæˆåŸºäºç­–ç•¥å®¡æ…æ¨ç†æ ·æœ¬ã€‚æœ€åï¼Œæˆ‘ä»¬åº”ç”¨åŸºäºGPT-4çš„å®‰å…¨è¯„åˆ†ç³»ç»Ÿæ¥é€‰æ‹©ç¬¦åˆæœ€ä½³å®è·µçš„è®­ç»ƒæ ·æœ¬ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œä½¿ç”¨STAR-1å¯¹å¤§å‹æ¨ç†æ¨¡å‹è¿›è¡Œå¾®è°ƒï¼Œåœ¨å››ä¸ªåŸºå‡†æµ‹è¯•ä¸­å®‰å…¨æ€§èƒ½å¹³å‡æé«˜äº†40%ï¼ŒåŒæ—¶åªå¸¦æ¥è½»å¾®çš„æ¨ç†èƒ½åŠ›ä¸‹é™ï¼ˆä¾‹å¦‚å¹³å‡ä¸‹é™1.1%ï¼‰ã€‚å¹¿æ³›çš„æ¶ˆèç ”ç©¶è¿›ä¸€æ­¥éªŒè¯äº†æ„å»ºSTAR-1çš„è®¾è®¡åŸåˆ™çš„é‡è¦æ€§ï¼Œå¹¶å¯¹å…¶åœ¨å¤§å‹æ¨ç†æ¨¡å‹å’Œä¼ ç»Ÿçš„å¤§å‹è¯­è¨€æ¨¡å‹ä¸Šçš„æœ‰æ•ˆæ€§è¿›è¡Œäº†åˆ†æã€‚æˆ‘ä»¬çš„é¡¹ç›®é¡µé¢æ˜¯ï¼š<a target="_blank" rel="noopener" href="https://ucsc-vlaa.github.io/STAR-1%E3%80%82">https://ucsc-vlaa.github.io/STAR-1ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.01903v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>è¿™æ˜¯ä¸€ç¯‡å…³äºSTAR-1æ•°æ®é›†çš„æ–‡ç« ï¼Œè¯¥æ•°æ®é›†æ—¨åœ¨æ»¡è¶³å¤§å‹æ¨ç†æ¨¡å‹çš„å®‰å…¨éœ€æ±‚ã€‚æ•°æ®é›†éµå¾ªå¤šæ ·æ€§ã€å®¡æ…æ¨ç†å’Œä¸¥æ ¼ç­›é€‰ä¸‰å¤§åŸåˆ™æ„å»ºï¼Œå¹¶é€šè¿‡æ•´åˆç°æœ‰å¼€æºå®‰å…¨æ•°æ®é›†ã€åˆ¶å®šå®‰å…¨æ”¿ç­–ä»¥åŠåº”ç”¨åŸºäºGPT-4oçš„å®‰å…¨è¯„åˆ†ç³»ç»Ÿæ¥ç”Ÿæˆæ ·æœ¬ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œä½¿ç”¨STAR-1å¾®è°ƒçš„å¤§å‹æ¨ç†æ¨¡å‹åœ¨å››é¡¹åŸºå‡†æµ‹è¯•ä¸Šçš„å®‰å…¨æ€§èƒ½å¹³å‡æé«˜äº†40%ï¼ŒåŒæ—¶ä»…åœ¨äº”ä¸ªæ¨ç†ä»»åŠ¡ä¸­ç•¥å¾®é™ä½æ¨ç†èƒ½åŠ›ã€‚æ•°æ®é›†é¡¹ç›®é¡µé¢æ˜¯ï¼š<a target="_blank" rel="noopener" href="https://ucsc-vlaa.github.io/STAR-1">é“¾æ¥åœ°å€</a>ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<p>ä»¥ä¸‹æ˜¯æœ¬æ–‡ç« çš„å…³é”®è¦ç‚¹ï¼š</p>
<ol>
<li>æ–‡ç« ä»‹ç»äº†STAR-1æ•°æ®é›†ï¼Œè¿™æ˜¯ä¸€ä¸ªä¸“ä¸ºå¤§å‹æ¨ç†æ¨¡å‹è®¾è®¡çš„é«˜è´¨é‡å®‰å…¨æ•°æ®é›†ã€‚</li>
<li>æ•°æ®é›†éµå¾ªå¤šæ ·æ€§ã€å®¡æ…æ¨ç†å’Œä¸¥æ ¼ç­›é€‰ä¸‰å¤§åŸåˆ™æ„å»ºã€‚</li>
<li>æ•°æ®é›†é€šè¿‡æ•´åˆç°æœ‰å¼€æºå®‰å…¨æ•°æ®é›†ï¼Œåˆ¶å®šå®‰å…¨æ”¿ç­–æ¥ç”Ÿæˆæ ·æœ¬ã€‚</li>
<li>åº”ç”¨åŸºäºGPT-4oçš„å®‰å…¨è¯„åˆ†ç³»ç»Ÿæ¥ç­›é€‰ç¬¦åˆæœ€ä½³å®è·µçš„è®­ç»ƒæ ·æœ¬ã€‚</li>
<li>å®éªŒç»“æœæ˜¾ç¤ºï¼Œä½¿ç”¨STAR-1å¾®è°ƒçš„å¤§å‹æ¨ç†æ¨¡å‹åœ¨å®‰å…¨æ€§èƒ½ä¸Šæ˜¾è‘—æé«˜ï¼ŒåŒæ—¶ä¿æŒè¾ƒé«˜çš„æ¨ç†èƒ½åŠ›ã€‚</li>
<li>æ•°æ®é›†é¡¹ç›®é¡µé¢æä¾›äº†æ›´å¤šå…³äºSTAR-1çš„è¯¦ç»†ä¿¡æ¯ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.01903">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-eacf55114141fbf8489fd896f6fca4ae.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a3bce844203c398839f3e89586195374.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-40d99782dafeba56ee8265365c9370be.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-832ebff3e52aed24090d3653c44e6f77.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-85fc740539de2c3066cdb182445fa637.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="GMAI-VL-R1-Harnessing-Reinforcement-Learning-for-Multimodal-Medical-Reasoning"><a href="#GMAI-VL-R1-Harnessing-Reinforcement-Learning-for-Multimodal-Medical-Reasoning" class="headerlink" title="GMAI-VL-R1: Harnessing Reinforcement Learning for Multimodal Medical   Reasoning"></a>GMAI-VL-R1: Harnessing Reinforcement Learning for Multimodal Medical   Reasoning</h2><p><strong>Authors:Yanzhou Su, Tianbin Li, Jiyao Liu, Chenglong Ma, Junzhi Ning, Cheng Tang, Sibo Ju, Jin Ye, Pengcheng Chen, Ming Hu, Shixiang Tang, Lihao Liu, Bin Fu, Wenqi Shao, Xiaowei Hu, Xiangwen Liao, Yuanfeng Ji, Junjun He</strong></p>
<p>Recent advances in general medical AI have made significant strides, but existing models often lack the reasoning capabilities needed for complex medical decision-making. This paper presents GMAI-VL-R1, a multimodal medical reasoning model enhanced by reinforcement learning (RL) to improve its reasoning abilities. Through iterative training, GMAI-VL-R1 optimizes decision-making, significantly boosting diagnostic accuracy and clinical support. We also develop a reasoning data synthesis method, generating step-by-step reasoning data via rejection sampling, which further enhances the modelâ€™s generalization. Experimental results show that after RL training, GMAI-VL-R1 excels in tasks such as medical image diagnosis and visual question answering. While the model demonstrates basic memorization with supervised fine-tuning, RL is crucial for true generalization. Our work establishes new evaluation benchmarks and paves the way for future advancements in medical reasoning models. Code, data, and model will be released at \href{<a target="_blank" rel="noopener" href="https://github.com/uni-medical/GMAI-VL-R1%7D%7Bthis">https://github.com/uni-medical/GMAI-VL-R1}{this</a> link}. </p>
<blockquote>
<p>è¿‘æœŸé€šç”¨åŒ»ç–—äººå·¥æ™ºèƒ½çš„è¿›æ­¥å·²ç»å–å¾—äº†é‡å¤§çªç ´ï¼Œä½†ç°æœ‰æ¨¡å‹é€šå¸¸ç¼ºä¹å¤æ‚çš„åŒ»ç–—å†³ç­–æ‰€éœ€çš„æ¨ç†èƒ½åŠ›ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§å¤šæ¨¡æ€åŒ»ç–—æ¨ç†æ¨¡å‹GMAI-VL-R1ï¼Œé€šè¿‡å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰å¢å¼ºå…¶æ¨ç†èƒ½åŠ›ã€‚é€šè¿‡è¿­ä»£è®­ç»ƒï¼ŒGMAI-VL-R1ä¼˜åŒ–äº†å†³ç­–åˆ¶å®šï¼Œå¤§å¤§æé«˜äº†è¯Šæ–­å‡†ç¡®æ€§å’Œä¸´åºŠæ”¯æŒèƒ½åŠ›ã€‚æˆ‘ä»¬è¿˜å¼€å‘äº†ä¸€ç§æ¨ç†æ•°æ®åˆæˆæ–¹æ³•ï¼Œé€šè¿‡æ‹’ç»é‡‡æ ·ç”Ÿæˆåˆ†æ­¥æ¨ç†æ•°æ®ï¼Œè¿™è¿›ä¸€æ­¥æé«˜äº†æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œç»è¿‡RLè®­ç»ƒåï¼ŒGMAI-VL-R1åœ¨åŒ»å­¦å›¾åƒè¯Šæ–­å’Œè§†è§‰é—®ç­”ç­‰ä»»åŠ¡ä¸Šè¡¨ç°å‡ºè‰²ã€‚è™½ç„¶è¯¥æ¨¡å‹é€šè¿‡ç›‘ç£å¾®è°ƒå±•ç¤ºäº†åŸºæœ¬çš„è®°å¿†èƒ½åŠ›ï¼Œä½†RLå¯¹äºçœŸæ­£çš„æ³›åŒ–è‡³å…³é‡è¦ã€‚æˆ‘ä»¬çš„å·¥ä½œå»ºç«‹äº†æ–°çš„è¯„ä¼°åŸºå‡†ï¼Œä¸ºåŒ»ç–—æ¨ç†æ¨¡å‹çš„æœªæ¥è¿›æ­¥é“ºå¹³äº†é“è·¯ã€‚ä»£ç ã€æ•°æ®å’Œæ¨¡å‹å°†å‘å¸ƒåœ¨æ­¤é“¾æ¥ï¼š<a target="_blank" rel="noopener" href="https://github.com/uni-medical/GMAI-VL-R1%E3%80%82">https://github.com/uni-medical/GMAI-VL-R1ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.01886v1">PDF</a> </p>
<p><strong>Summary</strong><br>     è¿‘æœŸåŒ»å­¦äººå·¥æ™ºèƒ½å–å¾—æ˜¾è‘—è¿›å±•ï¼Œä½†ç°æœ‰æ¨¡å‹åœ¨å¤æ‚åŒ»ç–—å†³ç­–ä¸­ç¼ºä¹æ¨ç†èƒ½åŠ›ã€‚æœ¬æ–‡æå‡ºä¸€ç§é€šè¿‡å¼ºåŒ–å­¦ä¹ å¢å¼ºçš„å¤šæ¨¡æ€åŒ»å­¦æ¨ç†æ¨¡å‹GMAI-VL-R1ï¼Œä»¥æé«˜å…¶æ¨ç†èƒ½åŠ›ã€‚ç»è¿‡è¿­ä»£è®­ç»ƒï¼ŒGMAI-VL-R1ä¼˜åŒ–äº†å†³ç­–åˆ¶å®šï¼Œæ˜¾è‘—æé«˜äº†è¯Šæ–­å‡†ç¡®æ€§å’Œä¸´åºŠæ”¯æŒèƒ½åŠ›ã€‚æ­¤å¤–ï¼Œè¿˜å¼€å‘äº†ä¸€ç§é€šè¿‡æ‹’ç»é‡‡æ ·ç”Ÿæˆé€æ­¥æ¨ç†æ•°æ®çš„æ–¹æ³•ï¼Œè¿›ä¸€æ­¥æé«˜æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œç»è¿‡å¼ºåŒ–å­¦ä¹ è®­ç»ƒåï¼ŒGMAI-VL-R1åœ¨åŒ»å­¦å›¾åƒè¯Šæ–­å’Œè§†è§‰é—®ç­”ç­‰ä»»åŠ¡ä¸Šè¡¨ç°å‡ºè‰²ã€‚è™½ç„¶æ¨¡å‹é€šè¿‡ç›‘ç£å¾®è°ƒå®ç°äº†åŸºæœ¬è®°å¿†ï¼Œä½†å¼ºåŒ–å­¦ä¹ å¯¹äºçœŸæ­£çš„æ³›åŒ–è‡³å…³é‡è¦ã€‚æœ¬ç ”ç©¶å»ºç«‹äº†æ–°çš„è¯„ä¼°åŸºå‡†ï¼Œä¸ºåŒ»å­¦æ¨ç†æ¨¡å‹çš„æœªæ¥å‘å±•é“ºå¹³äº†é“è·¯ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>åŒ»å­¦äººå·¥æ™ºèƒ½è¿‘æœŸå–å¾—æ˜¾è‘—è¿›å±•ï¼Œä½†ç¼ºä¹å¤æ‚åŒ»ç–—å†³ç­–æ‰€éœ€çš„æ¨ç†èƒ½åŠ›ã€‚</li>
<li>GMAI-VL-R1æ˜¯ä¸€ä¸ªå¤šæ¨¡æ€åŒ»å­¦æ¨ç†æ¨¡å‹ï¼Œé€šè¿‡å¼ºåŒ–å­¦ä¹ å¢å¼ºã€‚</li>
<li>å¼ºåŒ–å­¦ä¹ è®­ç»ƒä½¿GMAI-VL-R1åœ¨è¯Šæ–­å‡†ç¡®æ€§å’Œä¸´åºŠæ”¯æŒæ–¹é¢æ˜¾è‘—æé«˜ã€‚</li>
<li>å¼€å‘äº†ä¸€ç§é€šè¿‡æ‹’ç»é‡‡æ ·ç”Ÿæˆé€æ­¥æ¨ç†æ•°æ®çš„æ–¹æ³•ï¼Œå¢å¼ºæ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ã€‚</li>
<li>å®éªŒç»“æœè¡¨æ˜ï¼ŒGMAI-VL-R1åœ¨åŒ»å­¦å›¾åƒè¯Šæ–­å’Œè§†è§‰é—®ç­”ç­‰ä»»åŠ¡ä¸Šè¡¨ç°ä¼˜ç§€ã€‚</li>
<li>å¼ºåŒ–å­¦ä¹ å¯¹äºæ¨¡å‹çš„çœŸæ­£æ³›åŒ–è‡³å…³é‡è¦ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.01886">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-34ca1460e91fb51bf2e2f1b10815ed39.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-fcd1ce69586f3e7150e001cb2e3f0a8f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-797985ccb31defc3d013562db8b044df.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-898cfafc8b3efe3b48c6c46e6eed35c5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-40f7a50dd758cef8763b7611d1aff15f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-dfa7a2bd349a07fa756d8548f03503e3.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="TransientTables-Evaluating-LLMsâ€™-Reasoning-on-Temporally-Evolving-Semi-structured-Tables"><a href="#TransientTables-Evaluating-LLMsâ€™-Reasoning-on-Temporally-Evolving-Semi-structured-Tables" class="headerlink" title="TransientTables: Evaluating LLMsâ€™ Reasoning on Temporally Evolving   Semi-structured Tables"></a>TransientTables: Evaluating LLMsâ€™ Reasoning on Temporally Evolving   Semi-structured Tables</h2><p><strong>Authors:Abhilash Shankarampeta, Harsh Mahajan, Tushar Kataria, Dan Roth, Vivek Gupta</strong></p>
<p>Humans continuously make new discoveries, and understanding temporal sequence of events leading to these breakthroughs is essential for advancing science and society. This ability to reason over time allows us to identify future steps and understand the effects of financial and political decisions on our lives. However, large language models (LLMs) are typically trained on static datasets, limiting their ability to perform effective temporal reasoning. To assess the temporal reasoning capabilities of LLMs, we present the TRANSIENTTABLES dataset, which comprises 3,971 questions derived from over 14,000 tables, spanning 1,238 entities across multiple time periods. We introduce a template-based question-generation pipeline that harnesses LLMs to refine both templates and questions. Additionally, we establish baseline results using state-of-the-art LLMs to create a benchmark. We also introduce novel modeling strategies centered around task decomposition, enhancing LLM performance. </p>
<blockquote>
<p>äººç±»ä¸æ–­æœ‰æ–°çš„å‘ç°ï¼Œç†è§£å¯¼è‡´è¿™äº›çªç ´çš„äº‹ä»¶çš„æ—¶é—´åºåˆ—å¯¹äºæ¨åŠ¨ç§‘å­¦å’Œç¤¾ä¼šè¿›æ­¥è‡³å…³é‡è¦ã€‚è¿™ç§éšæ—¶é—´æ¨ç†çš„èƒ½åŠ›ä½¿æˆ‘ä»¬èƒ½å¤Ÿç¡®å®šæœªæ¥çš„æ­¥éª¤ï¼Œå¹¶äº†è§£é‡‘èå’Œæ”¿æ²»å†³ç­–å¯¹æˆ‘ä»¬çš„ç”Ÿæ´»çš„å½±å“ã€‚ç„¶è€Œï¼Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰é€šå¸¸æ˜¯åœ¨é™æ€æ•°æ®é›†ä¸Šè¿›è¡Œè®­ç»ƒçš„ï¼Œè¿™é™åˆ¶äº†å®ƒä»¬è¿›è¡Œæœ‰æ•ˆçš„æ—¶é—´æ¨ç†çš„èƒ½åŠ›ã€‚ä¸ºäº†è¯„ä¼°LLMçš„æ—¶é—´æ¨ç†èƒ½åŠ›ï¼Œæˆ‘ä»¬æ¨å‡ºäº†TRANSIENTTABLESæ•°æ®é›†ï¼Œè¯¥æ•°æ®é›†åŒ…å«3971ä¸ªé—®é¢˜ï¼Œè¿™äº›é—®é¢˜ç”±è¶…è¿‡14000ä¸ªè¡¨æ ¼è¡ç”Ÿè€Œæ¥ï¼Œæ¶µç›–äº†å¤šä¸ªæ—¶æœŸå†…çš„1238ä¸ªå®ä½“ã€‚æˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ªåŸºäºæ¨¡æ¿çš„é—®é¢˜ç”Ÿæˆç®¡é“ï¼Œè¯¥ç®¡é“åˆ©ç”¨LLMæ¥æ”¹è¿›æ¨¡æ¿å’Œé—®é¢˜ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜åˆ©ç”¨æœ€æ–°LLMå»ºç«‹åŸºçº¿ç»“æœä»¥åˆ›å»ºåŸºå‡†æµ‹è¯•ï¼Œå¹¶å›´ç»•ä»»åŠ¡åˆ†è§£å¼•å…¥æ–°å‹å»ºæ¨¡ç­–ç•¥ï¼Œä»¥æé«˜LLMçš„æ€§èƒ½ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.01879v1">PDF</a> 19 Pages. 21 Tables, 1 figure</p>
<p><strong>Summary</strong>ï¼š</p>
<p>äººç±»å¯¹æ—¶é—´çš„æ¨ç†èƒ½åŠ›å¯¹äºæ¨åŠ¨ç§‘å­¦å’Œç¤¾ä¼šè¿›æ­¥è‡³å…³é‡è¦ã€‚ç„¶è€Œï¼Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰é€šå¸¸å—é™äºé™æ€æ•°æ®é›†ï¼Œéš¾ä»¥è¿›è¡Œæœ‰æ•ˆçš„æ—¶åºæ¨ç†ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æ¨å‡ºäº†TRANSIENTTABLESæ•°æ®é›†ï¼ŒåŒ…å«3971ä¸ªé—®é¢˜ï¼Œæºäºè¶…è¿‡14000ä¸ªè¡¨æ ¼ï¼Œæ¶‰åŠå¤šä¸ªæ—¶é—´æ®µçš„1238ä¸ªå®ä½“ã€‚æˆ‘ä»¬é‡‡ç”¨åŸºäºæ¨¡æ¿çš„é—®é¢˜ç”Ÿæˆç®¡é“ï¼Œåˆ©ç”¨LLMsä¼˜åŒ–æ¨¡æ¿å’Œé—®é¢˜ã€‚åŒæ—¶ï¼Œæˆ‘ä»¬å»ºç«‹äº†ä½¿ç”¨æœ€æ–°LLMsçš„åŸºçº¿ç»“æœï¼Œå¹¶å¼•å…¥äº†ä»¥ä»»åŠ¡åˆ†è§£ä¸ºä¸­å¿ƒçš„æ–°å‹å»ºæ¨¡ç­–ç•¥ï¼Œä»¥æé«˜LLMsçš„æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>äººç±»ç†è§£äº‹ä»¶çš„æ—¶é—´åºåˆ—å¯¹äºç§‘å­¦å’Œç¤¾ä¼šè¿›æ­¥è‡³å…³é‡è¦ã€‚</li>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨æ—¶åºæ¨ç†æ–¹é¢å­˜åœ¨å±€é™ã€‚</li>
<li>TRANSIENTTABLESæ•°æ®é›†åŒ…å«ä»è¶…è¿‡14000ä¸ªè¡¨æ ¼ä¸­è¡ç”Ÿå‡ºçš„3971ä¸ªé—®é¢˜ï¼Œæ¶‰åŠå¤šä¸ªæ—¶é—´æ®µçš„å®ä½“ã€‚</li>
<li>æˆ‘ä»¬ä½¿ç”¨åŸºäºæ¨¡æ¿çš„é—®é¢˜ç”Ÿæˆç®¡é“æ¥æé«˜LLMsçš„æ—¶åºæ¨ç†èƒ½åŠ›ã€‚</li>
<li>åŸºçº¿ç»“æœä½¿ç”¨æœ€æ–°çš„LLMså»ºç«‹ï¼Œä»¥æä¾›æ€§èƒ½æ ‡å‡†ã€‚</li>
<li>ä¸ºäº†å¢å¼ºLLMsçš„è¡¨ç°ï¼Œæˆ‘ä»¬å¼•å…¥äº†æ–°å‹å»ºæ¨¡ç­–ç•¥ï¼Œä»¥ä»»åŠ¡åˆ†è§£ä¸ºä¸­å¿ƒã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.01879">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-89af2b462028452ff5332f4254508e05.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1a5bebf66d486a17704a2234c9c22dbc.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Interpreting-Emergent-Planning-in-Model-Free-Reinforcement-Learning"><a href="#Interpreting-Emergent-Planning-in-Model-Free-Reinforcement-Learning" class="headerlink" title="Interpreting Emergent Planning in Model-Free Reinforcement Learning"></a>Interpreting Emergent Planning in Model-Free Reinforcement Learning</h2><p><strong>Authors:Thomas Bush, Stephen Chung, Usman Anwar, AdriÃ  Garriga-Alonso, David Krueger</strong></p>
<p>We present the first mechanistic evidence that model-free reinforcement learning agents can learn to plan. This is achieved by applying a methodology based on concept-based interpretability to a model-free agent in Sokoban â€“ a commonly used benchmark for studying planning. Specifically, we demonstrate that DRC, a generic model-free agent introduced by Guez et al. (2019), uses learned concept representations to internally formulate plans that both predict the long-term effects of actions on the environment and influence action selection. Our methodology involves: (1) probing for planning-relevant concepts, (2) investigating plan formation within the agentâ€™s representations, and (3) verifying that discovered plans (in the agentâ€™s representations) have a causal effect on the agentâ€™s behavior through interventions. We also show that the emergence of these plans coincides with the emergence of a planning-like property: the ability to benefit from additional test-time compute. Finally, we perform a qualitative analysis of the planning algorithm learned by the agent and discover a strong resemblance to parallelized bidirectional search. Our findings advance understanding of the internal mechanisms underlying planning behavior in agents, which is important given the recent trend of emergent planning and reasoning capabilities in LLMs through RL </p>
<blockquote>
<p>æˆ‘ä»¬é¦–æ¬¡æä¾›äº†æœºæ¢°è¯æ®ï¼Œè¯æ˜æ— æ¨¡å‹å¼ºåŒ–å­¦ä¹ ä»£ç†å¯ä»¥å­¦ä¹ è§„åˆ’ã€‚è¿™æ˜¯é€šè¿‡åœ¨Sokobanï¼ˆä¸€ä¸ªå¸¸ç”¨äºç ”ç©¶è§„åˆ’çš„åŸºå‡†æµ‹è¯•ï¼‰çš„æ— æ¨¡å‹ä»£ç†ä¸­åº”ç”¨åŸºäºæ¦‚å¿µçš„å¯è§£é‡Šæ€§æ–¹æ³•æ¥å®ç°çš„ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬è¯æ˜äº†DRCæ˜¯ä¸€ä¸ªç”±Guezç­‰äººï¼ˆ2019å¹´ï¼‰æå‡ºçš„ä¸€ç§é€šç”¨æ— æ¨¡å‹ä»£ç†ï¼Œå®ƒä½¿ç”¨å­¦ä¹ çš„æ¦‚å¿µè¡¨ç¤ºæ¥å†…éƒ¨åˆ¶å®šè®¡åˆ’ï¼Œè¿™äº›è®¡åˆ’æ—¢é¢„æµ‹è¡ŒåŠ¨å¯¹ç¯å¢ƒäº§ç”Ÿçš„é•¿æœŸå½±å“ï¼Œåˆå½±å“è¡ŒåŠ¨çš„é€‰æ‹©ã€‚æˆ‘ä»¬çš„æ–¹æ³•åŒ…æ‹¬ï¼šï¼ˆ1ï¼‰æ¢æµ‹ä¸è§„åˆ’ç›¸å…³çš„æ¦‚å¿µï¼›ï¼ˆ2ï¼‰ç ”ç©¶ä»£ç†è¡¨ç¤ºä¸­çš„è®¡åˆ’å½¢æˆï¼›ï¼ˆ3ï¼‰é€šè¿‡å¹²é¢„éªŒè¯åœ¨ä»£ç†è¡¨ç¤ºä¸­å‘ç°çš„è®¡åˆ’å¯¹ä»£ç†è¡Œä¸ºå…·æœ‰å› æœå…³ç³»ã€‚æˆ‘ä»¬è¿˜è¡¨æ˜ï¼Œè¿™äº›è®¡åˆ’çš„å‡ºç°ä¼´éšç€ä¸€ç§ç±»ä¼¼è§„åˆ’çš„å±æ€§å‡ºç°ï¼šåˆ©ç”¨é¢å¤–çš„æµ‹è¯•æ—¶é—´è®¡ç®—èƒ½åŠ›çš„èƒ½åŠ›ã€‚æœ€åï¼Œæˆ‘ä»¬å¯¹ä»£ç†å­¦ä¹ çš„è§„åˆ’ç®—æ³•è¿›è¡Œäº†å®šæ€§åˆ†æï¼Œå¹¶å‘ç°å…¶ä¸å¹¶è¡ŒåŒå‘æœç´¢æœ‰å¾ˆå¼ºçš„ç›¸ä¼¼æ€§ã€‚æˆ‘ä»¬çš„ç ”ç©¶ç»“æœæ¨è¿›äº†å¯¹ä»£ç†è§„åˆ’è¡Œä¸ºå†…åœ¨æœºåˆ¶çš„ç†è§£ï¼Œè¿™åœ¨æœ€è¿‘çš„è¶‹åŠ¿ä¸­éå¸¸é‡è¦ï¼Œå³é€šè¿‡å¼ºåŒ–å­¦ä¹ åœ¨å¤§å‹è¯­è¨€æ¨¡å‹ä¸­å‡ºç°çš„è§„åˆ’å’Œæ¨ç†èƒ½åŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.01871v1">PDF</a> ICLR 2025 oral</p>
<p><strong>Summary</strong>ï¼š<br>æˆ‘ä»¬é¦–æ¬¡è¯æ˜æ— æ¨¡å‹å¼ºåŒ–å­¦ä¹ ä»£ç†å¯ä»¥é€šè¿‡åŸºäºæ¦‚å¿µå¯è§£é‡Šæ€§çš„æ–¹æ³•å­¦ä¹ è§„åˆ’ã€‚åœ¨ç”¨äºç ”ç©¶è§„åˆ’çš„å¸¸ç”¨åŸºå‡†Sokobanä¸­ï¼Œæˆ‘ä»¬å¯¹æ— æ¨¡å‹ä»£ç†åº”ç”¨äº†æ­¤æ–¹æ³•ã€‚ç ”ç©¶ç»“æœè¡¨æ˜ï¼ŒGuezç­‰äººäº2019å¹´å¼•å…¥çš„é€šç”¨æ— æ¨¡å‹ä»£ç†DRCä½¿ç”¨å­¦ä¹ çš„æ¦‚å¿µè¡¨ç¤ºæ¥å†…éƒ¨åˆ¶å®šè®¡åˆ’ï¼Œè¿™äº›è®¡åˆ’å¯ä»¥é¢„æµ‹è¡ŒåŠ¨å¯¹ç¯å¢ƒçš„é•¿è¿œå½±å“å¹¶å½±å“è¡ŒåŠ¨é€‰æ‹©ã€‚é€šè¿‡æ¢æŸ¥è§„åˆ’ç›¸å…³æ¦‚å¿µã€ç ”ç©¶ä»£ç†å†…éƒ¨è®¡åˆ’çš„å½¢æˆä»¥åŠéªŒè¯ä»£ç†å†…éƒ¨å‘ç°çš„è®¡åˆ’å¯¹ä»£ç†è¡Œä¸ºçš„å½±å“ï¼Œæˆ‘ä»¬å‘ç°è¿™äº›è®¡åˆ’çš„äº§ç”Ÿä¸ä¸€ç§è§„åˆ’ç±»å±æ€§çš„å‡ºç°ç›¸å»åˆï¼šå³åˆ©ç”¨é¢å¤–çš„æµ‹è¯•æ—¶é—´è®¡ç®—çš„èƒ½åŠ›ã€‚æœ€åï¼Œæˆ‘ä»¬å¯¹ä»£ç†æ‰€å­¦çš„è§„åˆ’ç®—æ³•è¿›è¡Œäº†å®šæ€§åˆ†æï¼Œå‘ç°å…¶ä¸å¹¶è¡ŒåŒå‘æœç´¢æœ‰å¾ˆå¼ºçš„ç›¸ä¼¼æ€§ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>æ— æ¨¡å‹å¼ºåŒ–å­¦ä¹ ä»£ç†å¯ä»¥é€šè¿‡æ¦‚å¿µå¯è§£é‡Šæ€§çš„æ–¹æ³•å­¦ä¹ è§„åˆ’ã€‚</li>
<li>ç ”ç©¶å¯¹è±¡ä½¿ç”¨äº†SokobanåŸºå‡†æ¥å±•ç¤ºè¿™ä¸€ç»“æœã€‚</li>
<li>DRCä»£ç†ä½¿ç”¨å­¦ä¹ çš„æ¦‚å¿µè¡¨ç¤ºæ¥å†…éƒ¨åˆ¶å®šè®¡åˆ’ï¼Œé¢„æµ‹è¡ŒåŠ¨å¯¹ç¯å¢ƒçš„é•¿è¿œå½±å“å¹¶å½±å“è¡ŒåŠ¨é€‰æ‹©ã€‚</li>
<li>é€šè¿‡æ¢æŸ¥å’ŒéªŒè¯å‘ç°äº†è§„åˆ’ç›¸å…³æ¦‚å¿µä¸ä»£ç†å†…éƒ¨è®¡åˆ’çš„å½¢æˆã€‚</li>
<li>ä»£ç†åˆ©ç”¨é¢å¤–çš„æµ‹è¯•æ—¶é—´è®¡ç®—èƒ½åŠ›è¿›è¡Œè§„åˆ’ç±»æ´»åŠ¨ã€‚</li>
<li>å‘ç°çš„è®¡åˆ’ä¸å¹¶è¡ŒåŒå‘æœç´¢æœ‰ç›¸ä¼¼æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.01871">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-d100d604b36a27285b4405c828a73ed7.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-661ee843894d3ac95e5dec7b45292563.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-de9cb54a78b6319279d10f3bb4ee2bbf.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-aeb4c6f3d55324f72bf671af581cf661.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Cross-Lingual-Consistency-A-Novel-Inference-Framework-for-Advancing-Reasoning-in-Large-Language-Models"><a href="#Cross-Lingual-Consistency-A-Novel-Inference-Framework-for-Advancing-Reasoning-in-Large-Language-Models" class="headerlink" title="Cross-Lingual Consistency: A Novel Inference Framework for Advancing   Reasoning in Large Language Models"></a>Cross-Lingual Consistency: A Novel Inference Framework for Advancing   Reasoning in Large Language Models</h2><p><strong>Authors:Zhiwei Yu, Tuo Li, Changhong Wang, Hui Chen, Lang Zhou</strong></p>
<p>Chain-of-thought (CoT) has emerged as a critical mechanism for enhancing reasoning capabilities in large language models (LLMs), with self-consistency demonstrating notable promise in boosting performance. However, inherent linguistic biases in multilingual training corpora frequently cause semantic drift and logical inconsistencies, especially in sub-10B parameter LLMs handling complex inference tasks. To overcome these constraints, we propose the Cross-Lingual Consistency (CLC) framework, an innovative inference paradigm that integrates multilingual reasoning paths through majority voting to elevate LLMsâ€™ reasoning capabilities. Empirical evaluations on the CMATH dataset reveal CLCâ€™s superiority over the conventional self-consistency method, delivering 9.5%, 6.5%, and 6.0% absolute accuracy gains for DeepSeek-Math-7B-Instruct, Qwen2.5-Math-7B-Instruct, and Gemma2-9B-Instruct respectively. Expanding CLCâ€™s linguistic scope to 11 diverse languages implies two synergistic benefits: 1) neutralizing linguistic biases in multilingual training corpora through multilingual ensemble voting, 2) escaping monolingual reasoning traps by exploring the broader multilingual solution space. This dual benefits empirically enables more globally optimal reasoning paths compared to monolingual self-consistency baselines, as evidenced by the 4.1%-18.5% accuracy gains using Gemma2-9B-Instruct on the MGSM dataset. </p>
<blockquote>
<p>æ€ç»´é“¾ï¼ˆCoTï¼‰ä½œä¸ºæå‡å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ¨ç†èƒ½åŠ›çš„é‡è¦æœºåˆ¶å·²ç»å´­éœ²å¤´è§’ï¼Œè‡ªæˆ‘ä¸€è‡´æ€§åœ¨æå‡æ€§èƒ½ä¸Šæ˜¾ç¤ºå‡ºå·¨å¤§æ½œåŠ›ã€‚ç„¶è€Œï¼Œåœ¨å¤šè¯­è¨€è®­ç»ƒè¯­æ–™åº“ä¸­çš„å›ºæœ‰è¯­è¨€åè§ç»å¸¸å¯¼è‡´è¯­ä¹‰æ¼‚ç§»å’Œé€»è¾‘ä¸ä¸€è‡´ï¼Œç‰¹åˆ«æ˜¯åœ¨å¤„ç†å¤æ‚æ¨ç†ä»»åŠ¡çš„å‚æ•°å°äº10Bçš„LLMsä¸­æ›´ä¸ºæ˜æ˜¾ã€‚ä¸ºäº†å…‹æœè¿™äº›é™åˆ¶ï¼Œæˆ‘ä»¬æå‡ºäº†è·¨è¯­è¨€ä¸€è‡´æ€§ï¼ˆCLCï¼‰æ¡†æ¶ï¼Œè¿™æ˜¯ä¸€ç§åˆ›æ–°æ€§çš„æ¨ç†èŒƒå¼ï¼Œå®ƒé€šè¿‡å¤šæ•°æŠ•ç¥¨æ¥æ•´åˆå¤šè¯­è¨€æ¨ç†è·¯å¾„ï¼Œæå‡LLMsçš„æ¨ç†èƒ½åŠ›ã€‚åœ¨CMATHæ•°æ®é›†ä¸Šçš„å®è¯è¯„ä¼°è¡¨æ˜ï¼ŒCLCæ¡†æ¶ä¼˜äºä¼ ç»Ÿçš„è‡ªæˆ‘ä¸€è‡´æ€§æ–¹æ³•ï¼Œä¸ºDeepSeek-Math-7B-Instructã€Qwen2.5-Math-7B-Instructå’ŒGemma2-9B-Instructåˆ†åˆ«å¸¦æ¥äº†9.5%ã€6.5%å’Œ6.0%çš„ç»å¯¹å‡†ç¡®åº¦æå‡ã€‚å°†CLCçš„è¯­è¨€èŒƒå›´æ‰©å±•åˆ°11ç§ä¸åŒè¯­è¨€ï¼Œæ„å‘³ç€ä¸¤ä¸ªååŒä¼˜åŠ¿ï¼š1ï¼‰é€šè¿‡å¤šè¯­è¨€é›†åˆæŠ•ç¥¨æŠµæ¶ˆå¤šè¯­è¨€è®­ç»ƒè¯­æ–™åº“ä¸­çš„è¯­è¨€åè§ï¼›2ï¼‰é€šè¿‡æ¢ç´¢æ›´å¹¿æ³›çš„å¤šè¯­è¨€è§£å†³æ–¹æ¡ˆç©ºé—´ï¼Œé¿å…å•ä¸€è¯­è¨€çš„æ¨ç†é™·é˜±ã€‚è¿™ç§åŒé‡ä¼˜åŠ¿ä½¿å¾—ä¸å•è¯­è‡ªæˆ‘ä¸€è‡´æ€§åŸºçº¿ç›¸æ¯”ï¼Œèƒ½å¤Ÿæ‰¾åˆ°æ›´å¤šå…¨å±€æœ€ä¼˜çš„æ¨ç†è·¯å¾„ã€‚å¦‚åœ¨MGSMæ•°æ®é›†ä¸Šï¼Œä½¿ç”¨Gemma2-9B-Instructçš„å‡†ç¡®åº¦æå‡äº†4.1%-18.5%ï¼Œè¿™è¯æ˜äº†å…¶å®æ•ˆæ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.01857v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>åœ¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¸­ï¼Œé“¾å¼æ€ç»´ï¼ˆCoTï¼‰å·²æˆä¸ºæé«˜æ¨ç†èƒ½åŠ›çš„é‡è¦æœºåˆ¶ï¼Œè€Œè‡ªæˆ‘ä¸€è‡´æ€§åœ¨æé«˜æ€§èƒ½æ–¹é¢å…·æœ‰æ˜¾è‘—æ½œåŠ›ã€‚ç„¶è€Œï¼Œåœ¨å¤šè¯­ç§è®­ç»ƒè¯­æ–™åº“ä¸­å­˜åœ¨çš„å›ºæœ‰è¯­è¨€åè§ä¼šå¯¼è‡´è¯­ä¹‰æ¼‚ç§»å’Œé€»è¾‘ä¸ä¸€è‡´ï¼Œç‰¹åˆ«æ˜¯åœ¨å¤„ç†å¤æ‚æ¨ç†ä»»åŠ¡çš„å‚æ•°å°äº10Bçš„LLMä¸­æ›´ä¸ºæ˜¾è‘—ã€‚ä¸ºå…‹æœè¿™äº›å±€é™æ€§ï¼Œæœ¬æ–‡æå‡ºäº†è·¨è¯­è¨€ä¸€è‡´æ€§ï¼ˆCLCï¼‰æ¡†æ¶ï¼Œè¿™æ˜¯ä¸€ç§é€šè¿‡å¤šæ•°æŠ•ç¥¨æ•´åˆå¤šè¯­ç§æ¨ç†è·¯å¾„çš„åˆ›æ–°æ¨ç†èŒƒå¼ï¼Œæ—¨åœ¨æå‡LLMçš„æ¨ç†èƒ½åŠ›ã€‚åœ¨CMATHæ•°æ®é›†ä¸Šçš„å®è¯è¯„ä¼°è¡¨æ˜ï¼ŒCLCä¼˜äºä¼ ç»Ÿçš„è‡ªæˆ‘ä¸€è‡´æ€§æ–¹æ³•ï¼Œåˆ†åˆ«ä¸ºDeepSeek-Math-7B-Instructã€Qwen2.5-Math-7B-Instructå’ŒGemma2-9B-Instructå¸¦æ¥äº†9.5%ã€6.5%å’Œ6.0%çš„ç»å¯¹å‡†ç¡®ç‡æå‡ã€‚å°†CLCçš„è¯­ç§èŒƒå›´æ‰©å¤§åˆ°11ç§ä¸åŒçš„è¯­è¨€ï¼Œå¸¦æ¥äº†ä¸¤ä¸ªååŒæ•ˆç›Šï¼š1ï¼‰é€šè¿‡å¤šè¯­ç§é›†æˆæŠ•ç¥¨ä¸­å’Œå¤šè¯­ç§è®­ç»ƒè¯­æ–™åº“ä¸­çš„è¯­è¨€åè§ï¼›2ï¼‰é€šè¿‡æ¢ç´¢æ›´å¹¿æ³›çš„å¤šè¯­ç§è§£å†³æ–¹æ¡ˆç©ºé—´ï¼Œé¿å…å•è¯­ç§æ¨ç†é™·é˜±ã€‚è¿™åŒé‡ä¼˜åŠ¿ä½¿å¾—CLCç›¸æ¯”å•è¯­ç§è‡ªæˆ‘ä¸€è‡´æ€§åŸºçº¿ï¼Œèƒ½å¤Ÿæ‰¾åˆ°æ›´å¤šå…¨å±€æœ€ä¼˜çš„æ¨ç†è·¯å¾„ï¼Œå¦‚åœ¨MGSMæ•°æ®é›†ä¸Šä½¿ç”¨Gemma2-9B-Instructå¸¦æ¥çš„4.1%~18.5%çš„å‡†ç¡®ç‡æå‡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>é“¾å¼æ€ç»´ï¼ˆCoTï¼‰å¢å¼ºäº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ¨ç†èƒ½åŠ›ã€‚</li>
<li>è‡ªæˆ‘ä¸€è‡´æ€§æ˜¯æå‡LLMæ€§èƒ½çš„ä¸€ç§æœ‰å‰æ™¯çš„æ–¹æ³•ã€‚</li>
<li>å¤šè¯­ç§è®­ç»ƒè¯­æ–™åº“ä¸­çš„è¯­è¨€åè§ä¼šå¯¼è‡´è¯­ä¹‰æ¼‚ç§»å’Œé€»è¾‘ä¸ä¸€è‡´ã€‚</li>
<li>è·¨è¯­è¨€ä¸€è‡´æ€§ï¼ˆCLCï¼‰æ¡†æ¶é€šè¿‡å¤šæ•°æŠ•ç¥¨æ•´åˆå¤šè¯­ç§æ¨ç†è·¯å¾„ï¼Œæå‡äº†LLMçš„æ¨ç†èƒ½åŠ›ã€‚</li>
<li>CLCæ¡†æ¶åœ¨CMATHæ•°æ®é›†ä¸Šçš„å®è¯è¯„ä¼°è¡¨ç°ä¼˜è¶Šã€‚</li>
<li>CLCæ¡†æ¶æ‰©å±•è‡³11ç§è¯­è¨€ï¼Œå…·æœ‰ä¸­å’Œè¯­è¨€åè§å’Œæ¢ç´¢å¤šè¯­ç§è§£å†³æ–¹æ¡ˆç©ºé—´çš„åŒé‡ä¼˜åŠ¿ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.01857">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-ae9bdf4b23091f2c7b4ca0b9c70d22c3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8b817bac65b02e0cf5debe8e6595637d.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Reasoning-LLMs-for-User-Aware-Multimodal-Conversational-Agents"><a href="#Reasoning-LLMs-for-User-Aware-Multimodal-Conversational-Agents" class="headerlink" title="Reasoning LLMs for User-Aware Multimodal Conversational Agents"></a>Reasoning LLMs for User-Aware Multimodal Conversational Agents</h2><p><strong>Authors:Hamed Rahimi, Jeanne Cattoni, Meriem Beghili, Mouad Abrini, Mahdi Khoramshahi, Maribel Pino, Mohamed Chetouani</strong></p>
<p>Personalization in social robotics is critical for fostering effective human-robot interactions, yet systems often face the cold start problem, where initial user preferences or characteristics are unavailable. This paper proposes a novel framework called USER-LLM R1 for a user-aware conversational agent that addresses this challenge through dynamic user profiling and model initiation. Our approach integrates chain-of-thought (CoT) reasoning models to iteratively infer user preferences and vision-language models (VLMs) to initialize user profiles from multimodal inputs, enabling personalized interactions from the first encounter. Leveraging a Retrieval-Augmented Generation (RAG) architecture, the system dynamically refines user representations within an inherent CoT process, ensuring contextually relevant and adaptive responses. Evaluations on the ElderlyTech-VQA Bench demonstrate significant improvements in ROUGE-1 (+23.2%), ROUGE-2 (+0.6%), and ROUGE-L (+8%) F1 scores over state-of-the-art baselines, with ablation studies underscoring the impact of reasoning model size on performance. Human evaluations further validate the frameworkâ€™s efficacy, particularly for elderly users, where tailored responses enhance engagement and trust. Ethical considerations, including privacy preservation and bias mitigation, are rigorously discussed and addressed to ensure responsible deployment. </p>
<blockquote>
<p>ä¸ªæ€§åŒ–ç¤¾ä¼šæœºå™¨äººå¯¹äºä¿ƒè¿›æœ‰æ•ˆçš„äººæœºäº¤äº’è‡³å…³é‡è¦ï¼Œç„¶è€Œï¼Œç³»ç»Ÿç»å¸¸é¢ä¸´å†·å¯åŠ¨é—®é¢˜ï¼Œå³åˆå§‹ç”¨æˆ·åå¥½æˆ–ç‰¹å¾æ— æ³•è·å–ã€‚æœ¬æ–‡é’ˆå¯¹è¿™ä¸€é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§åä¸ºUSER-LLM R1çš„æ–°å‹æ¡†æ¶ï¼Œç”¨äºæ„å»ºå…·æœ‰ç”¨æˆ·æ„è¯†çš„å¯¹è¯ä»£ç†ï¼Œé€šè¿‡åŠ¨æ€ç”¨æˆ·åˆ†æå’Œæ¨¡å‹å¯åŠ¨æ¥è§£å†³è¿™ä¸€é—®é¢˜ã€‚æˆ‘ä»¬çš„æ–¹æ³•èåˆäº†æ€ç»´é“¾ï¼ˆCoTï¼‰æ¨ç†æ¨¡å‹ï¼Œä»¥è¿­ä»£æ–¹å¼æ¨æ–­ç”¨æˆ·åå¥½ï¼Œä»¥åŠè§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰ï¼Œä»å¤šæ¨¡æ€è¾“å…¥åˆå§‹åŒ–ç”¨æˆ·åˆ†æï¼Œä»è€Œé¦–æ¬¡äº¤äº’å³å¯å®ç°ä¸ªæ€§åŒ–äº¤äº’ã€‚é€šè¿‡åˆ©ç”¨æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰æ¶æ„ï¼Œç³»ç»Ÿåœ¨ä¸€ä¸ªå†…åœ¨çš„CoTè¿‡ç¨‹ä¸­åŠ¨æ€ä¼˜åŒ–ç”¨æˆ·è¡¨ç¤ºï¼Œç¡®ä¿è¯­å¢ƒç›¸å…³ä¸”è‡ªé€‚åº”çš„å“åº”ã€‚åœ¨ElderlyTech-VQA Benchä¸Šçš„è¯„ä¼°æ˜¾ç¤ºï¼Œä¸æœ€æ–°åŸºçº¿ç›¸æ¯”ï¼ŒROUGE-1ï¼ˆ+23.2%ï¼‰ã€ROUGE-2ï¼ˆ+0.6%ï¼‰å’ŒROUGE-Lï¼ˆ+8%ï¼‰çš„F1åˆ†æ•°æœ‰äº†æ˜¾è‘—çš„æé«˜ã€‚æ¶ˆèç ”ç©¶å¼ºè°ƒäº†æ¨ç†æ¨¡å‹å¤§å°å¯¹æ€§èƒ½çš„å½±å“ã€‚äººç±»è¯„ä¼°è¿›ä¸€æ­¥éªŒè¯äº†è¯¥æ¡†æ¶çš„æœ‰æ•ˆæ€§ï¼Œç‰¹åˆ«æ˜¯å¯¹äºè€å¹´ç”¨æˆ·ï¼Œå®šåˆ¶åŒ–çš„å›åº”å¢å¼ºäº†å‚ä¸æ„Ÿå’Œä¿¡ä»»åº¦ã€‚ä¸¥æ ¼è®¨è®ºäº†åŒ…æ‹¬éšç§ä¿æŠ¤å’Œåè§ç¼“è§£åœ¨å†…çš„é“å¾·è€ƒé‡ï¼Œå¹¶äºˆä»¥è§£å†³ï¼Œä»¥ç¡®ä¿è´Ÿè´£ä»»çš„éƒ¨ç½²ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.01700v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>è¯¥è®ºæ–‡é’ˆå¯¹ç¤¾äº¤æœºå™¨äººä¸ªæ€§åŒ–é¢ä¸´çš„å†·å¯åŠ¨é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§åä¸ºUSER-LLM R1çš„æ–°å‹æ¡†æ¶ã€‚è¯¥æ¡†æ¶é€šè¿‡åŠ¨æ€ç”¨æˆ·åˆ†æå’Œæ¨¡å‹å¯åŠ¨å®ç°ç”¨æˆ·æ„ŸçŸ¥å¯¹è¯ä»£ç†ï¼Œé›†æˆé“¾å¼æ€ç»´æ¨ç†æ¨¡å‹æ¥è¿­ä»£æ¨æ–­ç”¨æˆ·åå¥½ï¼Œå¹¶åˆ©ç”¨è§†è§‰è¯­è¨€æ¨¡å‹ä»å¤šæ¨¡æ€è¾“å…¥ä¸­åˆå§‹åŒ–ç”¨æˆ·èµ„æ–™ã€‚è¯¥æ¡†æ¶é€šè¿‡æ£€ç´¢å¢å¼ºç”Ÿæˆæ¶æ„åŠ¨æ€å®Œå–„ç”¨æˆ·å†…åœ¨è®¤çŸ¥è¿‡ç¨‹ï¼Œç¡®ä¿å›åº”è¯­å¢ƒç›¸å…³å¹¶å…·æœ‰é€‚åº”æ€§ã€‚åœ¨ElderlyTech-VQA Benchä¸Šçš„è¯„ä¼°æ˜¾ç¤ºï¼Œç›¸è¾ƒäºç°æœ‰æŠ€æœ¯åŸºçº¿ï¼Œè¯¥æ¡†æ¶åœ¨ROUGE-1ï¼ˆ+23.2%ï¼‰ã€ROUGE-2ï¼ˆ+0.6%ï¼‰å’ŒROUGE-Lï¼ˆ+8%ï¼‰F1åˆ†æ•°ä¸Šå–å¾—äº†æ˜¾è‘—æ”¹å–„ã€‚äººç±»è¯„ä¼°è¿›ä¸€æ­¥éªŒè¯äº†è¯¥æ¡†æ¶å¯¹è€å¹´ç”¨æˆ·çš„åŠŸæ•ˆï¼Œå…¶å®šåˆ¶çš„å“åº”å¢å¼ºäº†å‚ä¸æ„Ÿå’Œä¿¡ä»»æ„Ÿã€‚åŒæ—¶ä¸¥æ ¼è®¨è®ºå¹¶è§£å†³äº†ä¼¦ç†è€ƒé‡ï¼ŒåŒ…æ‹¬éšç§ä¿æŠ¤å’Œåè§ç¼“è§£ï¼Œä»¥ç¡®ä¿éƒ¨ç½²è´£ä»»æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li><p>ç¤¾äº¤æœºå™¨äººä¸ªæ€§åŒ–å¯¹äºä¿ƒè¿›æœ‰æ•ˆçš„äººæœºäº¤äº’è‡³å…³é‡è¦ï¼Œä½†ç³»ç»Ÿé¢ä¸´å†·å¯åŠ¨é—®é¢˜ï¼Œå³åˆå§‹ç”¨æˆ·åå¥½æˆ–ç‰¹å¾ä¸å¯ç”¨ã€‚</p>
</li>
<li><p>USER-LLM R1æ¡†æ¶é€šè¿‡åŠ¨æ€ç”¨æˆ·åˆ†æå’Œæ¨¡å‹å¯åŠ¨æ¥è§£å†³è¿™ä¸€é—®é¢˜ï¼Œå®ç°ç”¨æˆ·æ„ŸçŸ¥å¯¹è¯ä»£ç†ã€‚</p>
</li>
<li><p>è¯¥æ¡†æ¶é›†æˆäº†é“¾å¼æ€ç»´æ¨ç†æ¨¡å‹æ¥è¿­ä»£æ¨æ–­ç”¨æˆ·åå¥½ï¼Œå¹¶åˆ©ç”¨è§†è§‰è¯­è¨€æ¨¡å‹å¤„ç†å¤šæ¨¡æ€è¾“å…¥ä»¥åˆå§‹åŒ–ç”¨æˆ·èµ„æ–™ã€‚</p>
</li>
<li><p>é€šè¿‡æ£€ç´¢å¢å¼ºç”Ÿæˆæ¶æ„ï¼Œç³»ç»Ÿèƒ½åœ¨å†…åœ¨è®¤çŸ¥è¿‡ç¨‹ä¸­åŠ¨æ€å®Œå–„ç”¨æˆ·è¡¨è¾¾ï¼Œç¡®ä¿å›åº”å…·æœ‰è¯­å¢ƒç›¸å…³æ€§å’Œé€‚åº”æ€§ã€‚</p>
</li>
<li><p>åœ¨ElderlyTech-VQA Benchä¸Šçš„è¯„ä¼°æ˜¾ç¤ºï¼Œè¯¥æ¡†æ¶ç›¸è¾ƒäºç°æœ‰æŠ€æœ¯åŸºçº¿æœ‰æ˜¾è‘—æ”¹è¿›ã€‚</p>
</li>
<li><p>äººç±»è¯„ä¼°éªŒè¯äº†è¯¥æ¡†æ¶å¯¹è€å¹´ç”¨æˆ·çš„åŠŸæ•ˆï¼Œå®šåˆ¶çš„å“åº”å¢å¼ºäº†å‚ä¸æ„Ÿå’Œä¿¡ä»»æ„Ÿã€‚</p>
</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.01700">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-a7a76a4fb94769482dc0127a81698ea4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-327f448db5fc875a0ddc17827750117b.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-ec285135e1f83c72fb901a0ba345f390.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-cb67508ac6814b3aca0f8d6c7bb449ce.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b1a3356124ad25fa9715da503c3922f7.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-130a296ece7e6c50de7e131b033050fd.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Text-Speaks-Louder-than-Vision-ASCII-Art-Reveals-Textual-Biases-in-Vision-Language-Models"><a href="#Text-Speaks-Louder-than-Vision-ASCII-Art-Reveals-Textual-Biases-in-Vision-Language-Models" class="headerlink" title="Text Speaks Louder than Vision: ASCII Art Reveals Textual Biases in   Vision-Language Models"></a>Text Speaks Louder than Vision: ASCII Art Reveals Textual Biases in   Vision-Language Models</h2><p><strong>Authors:Zhaochen Wang, Yujun Cai, Zi Huang, Bryan Hooi, Yiwei Wang, Ming-Hsuan Yang</strong></p>
<p>Vision-language models (VLMs) have advanced rapidly in processing multimodal information, but their ability to reconcile conflicting signals across modalities remains underexplored. This work investigates how VLMs process ASCII art, a unique medium where textual elements collectively form visual patterns, potentially creating semantic-visual conflicts. We introduce a novel evaluation framework that systematically challenges five state-of-the-art models (including GPT-4o, Claude, and Gemini) using adversarial ASCII art, where character-level semantics deliberately contradict global visual patterns. Our experiments reveal a strong text-priority bias: VLMs consistently prioritize textual information over visual patterns, with visual recognition ability declining dramatically as semantic complexity increases. Various mitigation attempts through visual parameter tuning and prompt engineering yielded only modest improvements, suggesting that this limitation requires architectural-level solutions. These findings uncover fundamental flaws in how current VLMs integrate multimodal information, providing important guidance for future model development while highlighting significant implications for content moderation systems vulnerable to adversarial examples. </p>
<blockquote>
<p>è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰åœ¨å¤„ç†å¤šæ¨¡æ€ä¿¡æ¯æ–¹é¢å–å¾—äº†å¿«é€Ÿè¿›å±•ï¼Œä½†å…¶åœ¨è°ƒå’Œè·¨æ¨¡æ€å†²çªä¿¡å·æ–¹é¢çš„èƒ½åŠ›ä»è¢«å¿½è§†ã€‚æœ¬ç ”ç©¶æ¢è®¨äº†VLMså¦‚ä½•å¤„ç†ASCIIè‰ºæœ¯è¿™ä¸€ç‹¬ç‰¹åª’ä»‹ï¼Œå…¶ä¸­çš„æ–‡æœ¬å…ƒç´ å…±åŒå½¢æˆè§†è§‰æ¨¡å¼ï¼Œå¯èƒ½ä¼šäº§ç”Ÿè¯­ä¹‰è§†è§‰å†²çªã€‚æˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ªæ–°çš„è¯„ä¼°æ¡†æ¶ï¼Œä½¿ç”¨å¯¹æŠ—æ€§ASCIIè‰ºæœ¯ç³»ç»Ÿåœ°æŒ‘æˆ˜äº†äº”ç§æœ€å…ˆè¿›çš„æ¨¡å‹ï¼ˆåŒ…æ‹¬GPT-4oã€Claudeå’ŒGeminiï¼‰ï¼Œå…¶ä¸­å­—ç¬¦çº§åˆ«çš„è¯­ä¹‰æ•…æ„ä¸å…¨å±€è§†è§‰æ¨¡å¼ç›¸çŸ›ç›¾ã€‚æˆ‘ä»¬çš„å®éªŒæ­ç¤ºäº†ä¸€ä¸ªå¼ºçƒˆçš„æ–‡æœ¬ä¼˜å…ˆåè§ï¼šVLMså§‹ç»ˆä¼˜å…ˆå¤„ç†æ–‡æœ¬ä¿¡æ¯è€Œéè§†è§‰æ¨¡å¼ï¼Œéšç€è¯­ä¹‰å¤æ‚æ€§çš„å¢åŠ ï¼Œå…¶è§†è§‰è¯†åˆ«èƒ½åŠ›æ€¥å‰§ä¸‹é™ã€‚é€šè¿‡è§†è§‰å‚æ•°è°ƒæ•´å’Œæç¤ºå·¥ç¨‹è¿›è¡Œçš„å„ç§ç¼“è§£å°è¯•ä»…äº§ç”Ÿäº†é€‚åº¦çš„æ”¹è¿›ï¼Œè¿™è¡¨æ˜è¿™ä¸€å±€é™æ€§éœ€è¦æ¶æ„çº§çš„è§£å†³æ–¹æ¡ˆã€‚è¿™äº›å‘ç°æ­ç¤ºäº†å½“å‰VLMså¦‚ä½•æ•´åˆå¤šæ¨¡æ€ä¿¡æ¯çš„åŸºæœ¬ç¼ºé™·ï¼Œä¸ºæœªæ¥çš„æ¨¡å‹å¼€å‘æä¾›äº†é‡è¦æŒ‡å¯¼ï¼ŒåŒæ—¶å¼ºè°ƒäº†å¯¹äºæ˜“å—å¯¹æŠ—æ€§æ ·æœ¬å½±å“çš„å†…å®¹å®¡æ ¸ç³»ç»Ÿæ‰€å¸¦æ¥çš„é‡å¤§å½±å“ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.01589v1">PDF</a> Under review at COLM 2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ¢è®¨äº†è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰åœ¨å¤„ç†ASCIIè‰ºæœ¯æ—¶çš„è¡¨ç°ï¼Œè¿™æ˜¯ä¸€ç§æ–‡æœ¬å…ƒç´ å…±åŒå½¢æˆè§†è§‰å›¾æ¡ˆçš„ç‰¹æ®Šåª’ä»‹ï¼Œå¯èƒ½ä¼šäº§ç”Ÿè¯­ä¹‰è§†è§‰å†²çªã€‚ç ”ç©¶å¼•å…¥äº†ä¸€ä¸ªæ–°å‹è¯„ä¼°æ¡†æ¶ï¼Œç³»ç»Ÿåœ°æŒ‘æˆ˜äº†äº”ç§æœ€å…ˆè¿›çš„æ¨¡å‹ï¼ŒåŒ…æ‹¬GPT-4oã€Claudeå’ŒGeminiç­‰ï¼Œä½¿ç”¨å¯¹æŠ—æ€§çš„ASCIIè‰ºæœ¯ï¼Œå…¶ä¸­å­—ç¬¦çº§åˆ«çš„è¯­ä¹‰æ•…æ„ä¸å…¨å±€è§†è§‰æ¨¡å¼ç›¸çŸ›ç›¾ã€‚å®éªŒè¡¨æ˜ï¼ŒVLMså­˜åœ¨å¼ºçƒˆçš„æ–‡æœ¬ä¼˜å…ˆåè§ï¼Œåœ¨è¯­ä¹‰å¤æ‚æ€§å¢åŠ æ—¶ï¼Œè§†è§‰è¯†åˆ«èƒ½åŠ›æ€¥å‰§ä¸‹é™ã€‚é€šè¿‡è§†è§‰å‚æ•°è°ƒæ•´å’Œæç¤ºå·¥ç¨‹çš„å„ç§ç¼“è§£å°è¯•åªäº§ç”Ÿäº†é€‚åº¦çš„æ”¹å–„ï¼Œè¡¨æ˜è¿™ä¸€å±€é™æ€§éœ€è¦æ¶æ„çº§çš„è§£å†³æ–¹æ¡ˆã€‚è¿™äº›å‘ç°æ­ç¤ºäº†å½“å‰VLMsåœ¨é›†æˆå¤šæ¨¡å¼ä¿¡æ¯æ—¶çš„æ ¹æœ¬ç¼ºé™·ï¼Œä¸ºæœªæ¥çš„æ¨¡å‹å¼€å‘æä¾›äº†é‡è¦æŒ‡å¯¼ï¼Œå¹¶çªå‡ºäº†å¯¹æ˜“å—å¯¹æŠ—æ€§ç¤ºä¾‹å½±å“çš„å†…å®¹å®¡æ ¸ç³»ç»Ÿçš„é‡è¦æ„ä¹‰ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>VLMsåœ¨å¤„ç†ASCIIè‰ºæœ¯æ—¶é¢ä¸´è¯­ä¹‰è§†è§‰å†²çªçš„é—®é¢˜ã€‚</li>
<li>å¼•å…¥æ–°å‹è¯„ä¼°æ¡†æ¶æŒ‘æˆ˜äº†äº”ç§æœ€å…ˆè¿›çš„VLMsæ¨¡å‹ã€‚</li>
<li>å®éªŒå‘ç°VLMså­˜åœ¨æ–‡æœ¬ä¼˜å…ˆçš„åè§ã€‚</li>
<li>åœ¨è¯­ä¹‰å¤æ‚æ€§å¢åŠ æ—¶ï¼ŒVLMsçš„è§†è§‰è¯†åˆ«èƒ½åŠ›æ˜¾è‘—ä¸‹é™ã€‚</li>
<li>ç›®å‰çš„æ¨¡å‹åœ¨é›†æˆå¤šæ¨¡å¼ä¿¡æ¯æ–¹é¢å­˜åœ¨æ ¹æœ¬ç¼ºé™·ã€‚</li>
<li>å°è¯•é€šè¿‡è§†è§‰å‚æ•°è°ƒæ•´å’Œæç¤ºå·¥ç¨‹æ”¹å–„æ¨¡å‹è¡¨ç°ï¼Œä½†æ•ˆæœæœ‰é™ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.01589">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-3b943ac9ae73b3d0545f2f9b84548315.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-3ab6db049fabe7e50947297f71cdf839.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d3ea20c763842611ad3c4ee24851c5f8.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e609b94376d4c29bfbbb10626368755a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-82a3cea05d4d4a3b23fd9d3fa054cae7.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="Enabling-Systematic-Generalization-in-Abstract-Spatial-Reasoning-through-Meta-Learning-for-Compositionality"><a href="#Enabling-Systematic-Generalization-in-Abstract-Spatial-Reasoning-through-Meta-Learning-for-Compositionality" class="headerlink" title="Enabling Systematic Generalization in Abstract Spatial Reasoning through   Meta-Learning for Compositionality"></a>Enabling Systematic Generalization in Abstract Spatial Reasoning through   Meta-Learning for Compositionality</h2><p><strong>Authors:Philipp Mondorf, Shijia Zhou, Monica Riedler, Barbara Plank</strong></p>
<p>Systematic generalization refers to the capacity to understand and generate novel combinations from known components. Despite recent progress by large language models (LLMs) across various domains, these models often fail to extend their knowledge to novel compositional scenarios, revealing notable limitations in systematic generalization. There has been an ongoing debate about whether neural networks possess the capacity for systematic generalization, with recent studies suggesting that meta-learning approaches designed for compositionality can significantly enhance this ability. However, these insights have largely been confined to linguistic problems, leaving their applicability to other tasks an open question. In this study, we extend the approach of meta-learning for compositionality to the domain of abstract spatial reasoning. To this end, we introduce $\textit{SYGAR}$-a dataset designed to evaluate the capacity of models to systematically generalize from known geometric transformations (e.g., translation, rotation) of two-dimensional objects to novel combinations of these transformations (e.g., translation+rotation). Our results show that a transformer-based encoder-decoder model, trained via meta-learning for compositionality, can systematically generalize to previously unseen transformation compositions, significantly outperforming state-of-the-art LLMs, including o3-mini, GPT-4o, and Gemini 2.0 Flash, which fail to exhibit similar systematic behavior. Our findings highlight the effectiveness of meta-learning in promoting systematicity beyond linguistic tasks, suggesting a promising direction toward more robust and generalizable models. </p>
<blockquote>
<p>ç³»ç»ŸåŒ–æ³›åŒ–æ˜¯æŒ‡ç†è§£å’Œç”Ÿæˆå·²çŸ¥ç»„ä»¶çš„æ–°ç»„åˆçš„èƒ½åŠ›ã€‚å°½ç®¡å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨å„ä¸ªé¢†åŸŸéƒ½å–å¾—äº†è¿›å±•ï¼Œä½†è¿™äº›æ¨¡å‹å¾€å¾€æ— æ³•å°†å…¶çŸ¥è¯†æ‰©å±•åˆ°æ–°çš„ç»„åˆåœºæ™¯ï¼Œæ˜¾ç¤ºå‡ºç³»ç»ŸåŒ–æ³›åŒ–çš„æ˜æ˜¾å±€é™æ€§ã€‚å…³äºç¥ç»ç½‘ç»œæ˜¯å¦å…·å¤‡ç³»ç»ŸåŒ–æ³›åŒ–èƒ½åŠ›çš„é—®é¢˜ä¸€ç›´å­˜åœ¨äº‰è®®ï¼Œè¿‘æœŸç ”ç©¶è¡¨æ˜ï¼Œé’ˆå¯¹ç»„åˆæ€§è®¾è®¡çš„å…ƒå­¦ä¹ æ–¹æ³•å¯ä»¥æ˜¾è‘—å¢å¼ºè¿™ç§èƒ½åŠ›ã€‚ç„¶è€Œï¼Œè¿™äº›è§è§£å¤§å¤šå±€é™äºè¯­è¨€é—®é¢˜ï¼Œå¯¹å…¶ä»–ä»»åŠ¡çš„é€‚ç”¨æ€§ä»æ˜¯æœªçŸ¥ã€‚åœ¨è¿™é¡¹ç ”ç©¶ä¸­ï¼Œæˆ‘ä»¬å°†å…ƒå­¦ä¹ çš„æ–¹æ³•æ‰©å±•åˆ°æŠ½è±¡ç©ºé—´æ¨ç†é¢†åŸŸã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬å¼•å…¥äº†SYGARæ•°æ®é›†ï¼Œæ—¨åœ¨è¯„ä¼°æ¨¡å‹ä»å·²çŸ¥çš„å‡ ä½•å˜æ¢ï¼ˆå¦‚å¹³ç§»ã€æ—‹è½¬ï¼‰ä¸­ç³»ç»Ÿåœ°æ¨å¹¿åˆ°è¿™äº›å˜æ¢çš„æ–°ç»„åˆï¼ˆå¦‚å¹³ç§»+æ—‹è½¬ï¼‰çš„èƒ½åŠ›ã€‚æˆ‘ä»¬çš„ç»“æœè¡¨æ˜ï¼ŒåŸºäºè½¬æ¢å™¨çš„ç¼–ç å™¨-è§£ç å™¨æ¨¡å‹ï¼Œé€šè¿‡å…ƒå­¦ä¹ è¿›è¡Œç»„åˆæ€§è®­ç»ƒï¼Œèƒ½å¤Ÿç³»ç»Ÿåœ°æ¨å¹¿åˆ°ä¹‹å‰æœªè§è¿‡çš„å˜æ¢ç»„åˆï¼Œæ˜¾è‘—ä¼˜äºæœ€æ–°çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ŒåŒ…æ‹¬o3-miniã€GPT-4oå’ŒGemini 2.0 Flashç­‰ï¼Œè¿™äº›æ¨¡å‹æ— æ³•å±•ç°å‡ºç±»ä¼¼çš„ç³»ç»ŸåŒ–è¡Œä¸ºã€‚æˆ‘ä»¬çš„ç ”ç©¶å¼ºè°ƒäº†å…ƒå­¦ä¹ åœ¨ä¿ƒè¿›ç³»ç»Ÿæ€§ã€è¶…è¶Šè¯­è¨€ä»»åŠ¡æ–¹é¢çš„æœ‰æ•ˆæ€§ï¼Œä¸ºæ„å»ºæ›´ç¨³å¥å’Œå¯æ³›åŒ–çš„æ¨¡å‹æä¾›äº†æœ‰å‰æ™¯çš„æ–¹å‘ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.01445v1">PDF</a> 30 pages, 14 figures</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ¢è®¨äº†ç³»ç»Ÿæ³›åŒ–çš„èƒ½åŠ›ï¼Œå³ç†è§£å’Œç”Ÿæˆå·²çŸ¥ç»„ä»¶çš„æ–°ç»„åˆçš„èƒ½åŠ›ã€‚å°½ç®¡å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨å„ä¸ªé¢†åŸŸéƒ½å–å¾—äº†è¿›å±•ï¼Œä½†å®ƒä»¬å¾€å¾€æ— æ³•å°†çŸ¥è¯†æ‰©å±•åˆ°æ–°çš„ç»„åˆåœºæ™¯ï¼Œæ˜¾ç¤ºå‡ºç³»ç»Ÿæ³›åŒ–çš„æ˜æ˜¾å±€é™æ€§ã€‚æœ€è¿‘çš„ç ”ç©¶è¡¨æ˜ï¼Œä¸ºç»„åˆæ€§è®¾è®¡çš„å…ƒå­¦ä¹ æ–¹æ³•å¯ä»¥æ˜¾è‘—æé«˜ç¥ç»ç½‘ç»œçš„ç³»ç»Ÿæ³›åŒ–èƒ½åŠ›ï¼Œä½†è¿™äº›è§è§£ä¸»è¦å±€é™äºè¯­è¨€é—®é¢˜ï¼Œå…¶ä»–ä»»åŠ¡çš„é€‚ç”¨æ€§ä»æ˜¯ä¸€ä¸ªå¼€æ”¾çš„é—®é¢˜ã€‚æœ¬ç ”ç©¶å°†å…ƒå­¦ä¹ æ–¹æ³•åº”ç”¨äºæŠ½è±¡ç©ºé—´æ¨ç†é¢†åŸŸï¼Œå¹¶å¼•å…¥äº†SYGARæ•°æ®é›†æ¥è¯„ä¼°æ¨¡å‹ä»å·²çŸ¥çš„å‡ ä½•è½¬æ¢ï¼ˆå¦‚å¹³ç§»ã€æ—‹è½¬ï¼‰åˆ°æ–°çš„è½¬æ¢ç»„åˆï¼ˆå¦‚å¹³ç§»+æ—‹è½¬ï¼‰çš„ç³»ç»Ÿæ³›åŒ–èƒ½åŠ›ã€‚ç ”ç©¶è¡¨æ˜ï¼ŒåŸºäºå˜å‹å™¨çš„ç¼–ç å™¨-è§£ç å™¨æ¨¡å‹ï¼Œé€šè¿‡å…ƒå­¦ä¹ è¿›è¡Œè®­ç»ƒï¼Œå¯ä»¥ç³»ç»Ÿåœ°æ³›åŒ–åˆ°ä¹‹å‰æœªè§è¿‡çš„è½¬æ¢ç»„åˆï¼Œæ˜¾è‘—ä¼˜äºåŒ…æ‹¬o3-miniã€GPT-4oå’ŒGemini 2.0 Flashåœ¨å†…çš„æœ€æ–°LLMsï¼Œè¿™äº›æ¨¡å‹æœªèƒ½è¡¨ç°å‡ºç±»ä¼¼çš„ç³»ç»Ÿæ€§è¡Œä¸ºã€‚ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œå…ƒå­¦ä¹ åœ¨ä¿ƒè¿›ç³»ç»Ÿæ€§æ–¹é¢ï¼ˆå°¤å…¶æ˜¯åœ¨éè¯­è¨€ä»»åŠ¡ä¸­ï¼‰çš„æœ‰æ•ˆæ€§ï¼Œä¸ºæ„å»ºæ›´ç¨³å¥å’Œé€šç”¨çš„æ¨¡å‹æä¾›äº†æœ‰å¸Œæœ›çš„æ–¹å‘ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç³»ç»Ÿæ³›åŒ–æ˜¯ç†è§£å’Œç”Ÿæˆæ–°ç»„åˆçš„å…³é”®èƒ½åŠ›ï¼ŒåŸºäºå·²çŸ¥ç»„ä»¶ã€‚</li>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨ç³»ç»Ÿæ³›åŒ–æ–¹é¢å­˜åœ¨å±€é™æ€§ï¼Œéš¾ä»¥åº”ç”¨äºæ–°ç»„åˆåœºæ™¯ã€‚</li>
<li>å…ƒå­¦ä¹ æ–¹æ³•è¢«è®¾è®¡æ¥æé«˜ç¥ç»ç½‘ç»œçš„ç³»ç»Ÿæ³›åŒ–èƒ½åŠ›ï¼Œå°¤å…¶åœ¨è¯­è¨€é—®é¢˜ä¸Šæ•ˆæœæ˜¾è‘—ã€‚</li>
<li>SYGARæ•°æ®é›†ç”¨äºè¯„ä¼°æ¨¡å‹åœ¨å‡ ä½•è½¬æ¢ä¸Šçš„ç³»ç»Ÿæ³›åŒ–èƒ½åŠ›ã€‚</li>
<li>åŸºäºå˜å‹å™¨çš„ç¼–ç å™¨-è§£ç å™¨æ¨¡å‹é€šè¿‡å…ƒå­¦ä¹ è®­ç»ƒï¼Œèƒ½å¤Ÿç³»ç»Ÿåœ°æ³›åŒ–åˆ°æœªè§è¿‡çš„è½¬æ¢ç»„åˆã€‚</li>
<li>è¯¥æ¨¡å‹æ˜¾è‘—ä¼˜äºç°æœ‰çš„LLMsï¼Œè¿™äº›LLMsåœ¨ç±»ä¼¼ä»»åŠ¡ä¸­æœªèƒ½è¡¨ç°å‡ºç³»ç»Ÿæ€§è¡Œä¸ºã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.01445">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-63d38379aebdc650bd2dee0b0bd3c595.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9f70b19241b3f03f9c3249f6caacebf7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-fe3c7badd18199b05bba50dbb7cbc15f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a409d7cdc13d2b62ed0aeba55fe42a8f.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="Foundations-and-Evaluations-in-NLP"><a href="#Foundations-and-Evaluations-in-NLP" class="headerlink" title="Foundations and Evaluations in NLP"></a>Foundations and Evaluations in NLP</h2><p><strong>Authors:Jungyeul Park</strong></p>
<p>This memoir explores two fundamental aspects of Natural Language Processing (NLP): the creation of linguistic resources and the evaluation of NLP system performance. Over the past decade, my work has focused on developing a morpheme-based annotation scheme for the Korean language that captures linguistic properties from morphology to semantics. This approach has achieved state-of-the-art results in various NLP tasks, including part-of-speech tagging, dependency parsing, and named entity recognition. Additionally, this work provides a comprehensive analysis of segmentation granularity and its critical impact on NLP system performance. In parallel with linguistic resource development, I have proposed a novel evaluation framework, the jp-algorithm, which introduces an alignment-based method to address challenges in preprocessing tasks like tokenization and sentence boundary detection (SBD). Traditional evaluation methods assume identical tokenization and sentence lengths between gold standards and system outputs, limiting their applicability to real-world data. The jp-algorithm overcomes these limitations, enabling robust end-to-end evaluations across a variety of NLP tasks. It enhances accuracy and flexibility by incorporating linear-time alignment while preserving the complexity of traditional evaluation metrics. This memoir provides key insights into the processing of morphologically rich languages, such as Korean, while offering a generalizable framework for evaluating diverse end-to-end NLP systems. My contributions lay the foundation for future developments, with broader implications for multilingual resource development and system evaluation. </p>
<blockquote>
<p>è¿™ç¯‡å›å¿†å½•æ¢è®¨äº†è‡ªç„¶è¯­è¨€å¤„ç†ï¼ˆNLPï¼‰çš„ä¸¤ä¸ªåŸºæœ¬æ–¹é¢ï¼šè¯­è¨€èµ„æºçš„åˆ›å»ºå’ŒNLPç³»ç»Ÿæ€§èƒ½çš„è¯„ä¼°ã€‚è¿‡å»åå¹´ï¼Œæˆ‘çš„å·¥ä½œä¸»è¦é›†ä¸­åœ¨ä¸ºéŸ©è¯­å¼€å‘ä¸€ç§åŸºäºè¯ç´ çš„æ³¨é‡Šæ–¹æ¡ˆï¼Œè¯¥æ–¹æ¡ˆèƒ½å¤Ÿæ•è·ä»å½¢æ€åˆ°è¯­ä¹‰çš„è¯­è¨€ç‰¹æ€§ã€‚è¿™ç§æ–¹æ³•åœ¨å„ç§NLPä»»åŠ¡ä¸­è¾¾åˆ°äº†æœ€å…ˆè¿›çš„ç»“æœï¼ŒåŒ…æ‹¬è¯æ€§æ ‡æ³¨ã€ä¾å­˜è§£æå’Œå‘½åå®ä½“è¯†åˆ«ã€‚æ­¤å¤–ï¼Œè¿™é¡¹å·¥ä½œè¿˜å…¨é¢åˆ†æäº†åˆ†è¯ç²’åº¦åŠå…¶å¯¹NLPç³»ç»Ÿæ€§èƒ½çš„å…³é”®å½±å“ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.01342v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ¢è®¨äº†è‡ªç„¶è¯­è¨€å¤„ç†ï¼ˆNLPï¼‰çš„ä¸¤ä¸ªåŸºæœ¬æ–¹é¢ï¼šè¯­è¨€èµ„æºçš„åˆ›å»ºå’ŒNLPç³»ç»Ÿæ€§èƒ½çš„è¯„ä¼°ã€‚ä½œè€…è¯¦ç»†ä»‹ç»äº†è¿‡å»åå¹´åœ¨éŸ©è¯­è¯­è¨€èµ„æºå¼€å‘æ–¹é¢çš„å·¥ä½œï¼Œç‰¹åˆ«æ˜¯åŸºäºå½¢æ€ç´ çš„æ ‡æ³¨æ–¹æ¡ˆï¼Œè¯¥æ–¹æ¡ˆåœ¨å¤šç§NLPä»»åŠ¡ä¸­å–å¾—äº†æœ€å…ˆè¿›çš„æˆæœã€‚åŒæ—¶ï¼Œä½œè€…æå‡ºäº†ä¸€ä¸ªå…¨æ–°çš„è¯„ä¼°æ¡†æ¶jpç®—æ³•ï¼Œè¯¥ç®—æ³•è§£å†³äº†é¢„å¤„ç†ä»»åŠ¡ä¸­çš„å¯¹é½é—®é¢˜ï¼Œå¦‚åˆ†è¯å’Œå¥å­è¾¹ç•Œæ£€æµ‹ã€‚æ­¤ç®—æ³•å…‹æœäº†ä¼ ç»Ÿè¯„ä¼°æ–¹æ³•çš„å±€é™æ€§ï¼Œå®ç°äº†å¤šç§NLPä»»åŠ¡çš„ç«¯åˆ°ç«¯è¯„ä¼°ï¼Œå¢å¼ºäº†å‡†ç¡®æ€§å’Œçµæ´»æ€§ã€‚æ€»çš„æ¥è¯´ï¼Œæœ¬æ–‡ä¸ºå¤„ç†å½¢æ€ä¸°å¯Œçš„è¯­è¨€å¦‚éŸ©è¯­æä¾›äº†å…³é”®è§è§£ï¼Œå¹¶ä¸ºè¯„ä¼°å¤šæ ·åŒ–çš„ç«¯åˆ°ç«¯NLPç³»ç»Ÿæä¾›äº†å¯æ¨å¹¿çš„æ¡†æ¶ã€‚ä½œè€…çš„è´¡çŒ®ä¸ºæœªæ¥ç ”ç©¶å’Œå¤šè¯­è¨€èµ„æºå¼€å‘ä¸ç³»ç»Ÿè¯„ä¼°æä¾›äº†æ›´å¹¿æ³›çš„å¯ç¤ºã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ä½œè€…é›†ä¸­äºå‘å±•åŸºäºå½¢æ€ç´ çš„éŸ©è¯­æ ‡æ³¨æ–¹æ¡ˆï¼Œæ¶µç›–äº†ä»å½¢æ€åˆ°è¯­ä¹‰çš„å„ä¸ªæ–¹é¢ã€‚è¯¥æ–¹æ¡ˆåœ¨ä¸åŒNLPä»»åŠ¡ä¸­è¡¨ç°å“è¶Šã€‚</li>
<li>ä½œè€…è¯¦ç»†åˆ†æäº†åˆ†æ®µç²’åº¦å¯¹NLPç³»ç»Ÿæ€§èƒ½çš„å…³é”®å½±å“ã€‚</li>
<li>ä½œè€…æå‡ºäº†ä¸€ç§æ–°çš„è¯„ä¼°æ¡†æ¶jpç®—æ³•ï¼Œè§£å†³äº†é¢„å¤„ç†ä»»åŠ¡ä¸­çš„å¯¹é½é—®é¢˜ï¼Œå¦‚åˆ†è¯å’Œå¥å­è¾¹ç•Œæ£€æµ‹ã€‚è¿™ä¸€ç®—æ³•å…‹æœäº†ä¼ ç»Ÿæ–¹æ³•çš„å±€é™æ€§ï¼Œå¢å¼ºäº†è¯„ä¼°çš„å‡†ç¡®æ€§å’Œçµæ´»æ€§ã€‚</li>
<li>jpç®—æ³•ç»“åˆäº†çº¿æ€§æ—¶é—´å¯¹é½æŠ€æœ¯ï¼Œæ—¢ä¿ç•™äº†ä¼ ç»Ÿè¯„ä¼°æŒ‡æ ‡çš„å¤æ‚æ€§ï¼Œåˆæé«˜äº†è¯„ä¼°çš„é²æ£’æ€§å’Œçµæ´»æ€§ã€‚</li>
<li>æœ¬æ–‡ä¸ºå¤„ç†å½¢æ€ä¸°å¯Œçš„è¯­è¨€æä¾›äº†é‡è¦è§è§£ï¼Œå¹¶å¯ä¸ºå…¶ä»–è¯­è¨€çš„ç±»ä¼¼ç ”ç©¶æä¾›å¯ç¤ºã€‚</li>
<li>ä½œè€…çš„è´¡çŒ®ä¸ºæœªæ¥çš„ç ”ç©¶å’Œå¤šè¯­è¨€èµ„æºå¼€å‘æä¾›äº†åŸºç¡€ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.01342">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-108fb837911aab3d37f497065f1e3ce7.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="ThinkPrune-Pruning-Long-Chain-of-Thought-of-LLMs-via-Reinforcement-Learning"><a href="#ThinkPrune-Pruning-Long-Chain-of-Thought-of-LLMs-via-Reinforcement-Learning" class="headerlink" title="ThinkPrune: Pruning Long Chain-of-Thought of LLMs via Reinforcement   Learning"></a>ThinkPrune: Pruning Long Chain-of-Thought of LLMs via Reinforcement   Learning</h2><p><strong>Authors:Bairu Hou, Yang Zhang, Jiabao Ji, Yujian Liu, Kaizhi Qian, Jacob Andreas, Shiyu Chang</strong></p>
<p>We present ThinkPrune, a simple yet effective method for pruning the thinking length for long-thinking LLMs, which has been found to often produce inefficient and redundant thinking processes. Existing preliminary explorations of reducing thinking length primarily focus on forcing the thinking process to early exit, rather than adapting the LLM to optimize and consolidate the thinking process, and therefore the length-performance tradeoff observed so far is sub-optimal. To fill this gap, ThinkPrune offers a simple solution that continuously trains the long-thinking LLMs via reinforcement learning (RL) with an added token limit, beyond which any unfinished thoughts and answers will be discarded, resulting in a zero reward. To further preserve model performance, we introduce an iterative length pruning approach, where multiple rounds of RL are conducted, each with an increasingly more stringent token limit. We observed that ThinkPrune results in a remarkable performance-length tradeoff â€“ on the AIME24 dataset, the reasoning length of DeepSeek-R1-Distill-Qwen-1.5B can be reduced by half with only 2% drop in performance. We also observed that after pruning, the LLMs can bypass unnecessary steps while keeping the core reasoning process complete. Code is available at <a target="_blank" rel="noopener" href="https://github.com/UCSB-NLP-Chang/ThinkPrune">https://github.com/UCSB-NLP-Chang/ThinkPrune</a>. </p>
<blockquote>
<p>æˆ‘ä»¬æå‡ºThinkPruneï¼Œè¿™æ˜¯ä¸€ç§ç®€å•ä½†æœ‰æ•ˆçš„æ–¹æ³•ï¼Œç”¨äºç¼©å‡é•¿æ€è€ƒLLMçš„æ€è€ƒé•¿åº¦ã€‚å·²çŸ¥LLMå¸¸å¸¸äº§ç”Ÿä½æ•ˆå’Œå†—ä½™çš„æ€è€ƒè¿‡ç¨‹ã€‚ç›®å‰å¯¹å‡å°‘æ€è€ƒé•¿åº¦çš„åˆæ­¥æ¢ç´¢ä¸»è¦é›†ä¸­åœ¨å¼ºåˆ¶æ€è€ƒè¿‡ç¨‹æ—©æœŸé€€å‡ºï¼Œè€Œä¸æ˜¯è®©LLMå»ä¼˜åŒ–å’Œå·©å›ºæ€è€ƒè¿‡ç¨‹ï¼Œå› æ­¤ç›®å‰è§‚å¯Ÿåˆ°çš„é•¿åº¦ä¸æ€§èƒ½ä¹‹é—´çš„æƒè¡¡å¹¶ä¸ç†æƒ³ã€‚ä¸ºäº†å¡«è¡¥è¿™ä¸€ç©ºç™½ï¼ŒThinkPruneæä¾›äº†ä¸€ä¸ªç®€å•çš„è§£å†³æ–¹æ¡ˆï¼Œå³é€šè¿‡å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰æŒç»­è®­ç»ƒé•¿æ€è€ƒLLMï¼Œå¹¶å¢åŠ ä¸€ä¸ªä»¤ç‰Œé™åˆ¶ï¼Œè¶…å‡ºæ­¤é™åˆ¶çš„æ€è€ƒå’Œç­”æ¡ˆå°†è¢«ä¸¢å¼ƒï¼Œä»è€Œå¯¼è‡´é›¶å¥–åŠ±ã€‚ä¸ºäº†ä¿æŒæ¨¡å‹æ€§èƒ½ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§è¿­ä»£é•¿åº¦ä¿®å‰ªæ–¹æ³•ï¼Œè¿›è¡Œå¤šè½®RLè®­ç»ƒï¼Œæ¯è½®éƒ½è®¾ç½®æ›´ä¸¥æ ¼çš„ä»¤ç‰Œé™åˆ¶ã€‚æˆ‘ä»¬å‘ç°ThinkPruneåœ¨æ€§èƒ½ä¸é•¿åº¦ä¹‹é—´å–å¾—äº†æ˜¾è‘—çš„æƒè¡¡â€”â€”åœ¨AIME24æ•°æ®é›†ä¸Šï¼ŒDeepSeek-R1-Distill-Qwen-1.5Bçš„æ€è€ƒé•¿åº¦å¯ä»¥å‡å°‘ä¸€åŠï¼Œæ€§èƒ½ä»…ä¸‹é™2%ã€‚æˆ‘ä»¬è¿˜è§‚å¯Ÿåˆ°ä¿®å‰ªåï¼ŒLLMå¯ä»¥è·³è¿‡ä¸å¿…è¦çš„æ­¥éª¤ï¼ŒåŒæ—¶ä¿æŒæ ¸å¿ƒæ¨ç†è¿‡ç¨‹çš„å®Œæ•´æ€§ã€‚ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/UCSB-NLP-Chang/ThinkPrune%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/UCSB-NLP-Chang/ThinkPruneè·å–ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.01296v1">PDF</a> 15 pages, 7 figures</p>
<p><strong>Summary</strong></p>
<p>æ€è€ƒä¿®å‰ªï¼ˆThinkPruneï¼‰æ˜¯ä¸€ç§ç®€å•æœ‰æ•ˆçš„æ–¹æ³•ï¼Œç”¨äºç¼©å‡é•¿æ€è€ƒLLMçš„æ€è€ƒé•¿åº¦ã€‚è¯¥æ–¹æ³•é€šè¿‡å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰è¿ç»­è®­ç»ƒLLMï¼Œå¹¶å¢åŠ ä¸€ä¸ªä»¤ç‰Œé™åˆ¶ï¼Œè¶…è¿‡é™åˆ¶åˆ™ä¸¢å¼ƒæœªå®Œæˆçš„æƒ³æ³•å’Œç­”æ¡ˆï¼Œä»è€Œå®ç°æ€§èƒ½ä¸é•¿åº¦çš„ä¼˜åŒ–æƒè¡¡ã€‚åœ¨AIME24æ•°æ®é›†ä¸Šï¼ŒDeepSeek-R1-Distill-Qwen-1.5Bæ¨¡å‹çš„æ¨ç†é•¿åº¦å¯ä»¥å‡å°‘ä¸€åŠï¼Œæ€§èƒ½ä»…ä¸‹é™2%ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ThinkPruneæ˜¯ä¸€ç§é’ˆå¯¹é•¿æ€è€ƒLLMçš„æ€è€ƒé•¿åº¦è¿›è¡Œä¿®å‰ªçš„ç®€å•æœ‰æ•ˆæ–¹æ³•ã€‚</li>
<li>ç°æœ‰æ–¹æ³•ä¸»è¦å¼ºåˆ¶æå‰ç»“æŸæ€è€ƒè¿‡ç¨‹ï¼Œè€Œéä½¿LLMé€‚åº”ä¼˜åŒ–å’Œæ•´åˆæ€è€ƒè¿‡ç¨‹ã€‚</li>
<li>ThinkPruneä½¿ç”¨å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰è¿›è¡ŒLLMçš„è¿ç»­è®­ç»ƒï¼Œå¹¶å¼•å…¥ä»¤ç‰Œé™åˆ¶ï¼Œä»¥å®ç°æ€§èƒ½ä¸é•¿åº¦çš„æœ€ä½³æƒè¡¡ã€‚</li>
<li>ThinkPruneèƒ½å¤Ÿæ˜¾è‘—å‡å°‘æ¨¡å‹çš„æ¨ç†é•¿åº¦ï¼ŒåŒæ—¶ä¿æŒè¾ƒé«˜çš„æ€§èƒ½ã€‚</li>
<li>é€šè¿‡è¿­ä»£é•¿åº¦ä¿®å‰ªæ–¹æ³•ï¼Œæ¯ä¸ªå›åˆéƒ½è®¾ç½®æ›´ä¸¥æ ¼çš„ä»¤ç‰Œé™åˆ¶ï¼Œè¿›ä¸€æ­¥ä¼˜åŒ–æ¨¡å‹æ€§èƒ½ã€‚</li>
<li>ThinkPruneå¯ä»¥å¸®åŠ©LLMç»•è¿‡ä¸å¿…è¦çš„æ­¥éª¤ï¼ŒåŒæ—¶ä¿æŒæ ¸å¿ƒæ¨ç†è¿‡ç¨‹çš„å®Œæ•´æ€§ã€‚</li>
<li>ä»£ç çš„å¼€æºå¯ç”¨æ€§ä¸ºç ”ç©¶è€…æä¾›äº†å®è·µå’Œæ”¹è¿›çš„æœºä¼šã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.01296">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-ddf17329ccce83998d191db9697cd2e5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-311da33074b5e3baffcdd8252a324fb7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-608d56a5de85b5bfc563b8600d7d2330.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="MPCritic-A-plug-and-play-MPC-architecture-for-reinforcement-learning"><a href="#MPCritic-A-plug-and-play-MPC-architecture-for-reinforcement-learning" class="headerlink" title="MPCritic: A plug-and-play MPC architecture for reinforcement learning"></a>MPCritic: A plug-and-play MPC architecture for reinforcement learning</h2><p><strong>Authors:Nathan P. Lawrence, Thomas Banker, Ali Mesbah</strong></p>
<p>The reinforcement learning (RL) and model predictive control (MPC) communities have developed vast ecosystems of theoretical approaches and computational tools for solving optimal control problems. Given their conceptual similarities but differing strengths, there has been increasing interest in synergizing RL and MPC. However, existing approaches tend to be limited for various reasons, including computational cost of MPC in an RL algorithm and software hurdles towards seamless integration of MPC and RL tools. These challenges often result in the use of â€œsimpleâ€ MPC schemes or RL algorithms, neglecting the state-of-the-art in both areas. This paper presents MPCritic, a machine learning-friendly architecture that interfaces seamlessly with MPC tools. MPCritic utilizes the loss landscape defined by a parameterized MPC problem, focusing on â€œsoftâ€ optimization over batched training steps; thereby updating the MPC parameters while avoiding costly minimization and parametric sensitivities. Since the MPC structure is preserved during training, an MPC agent can be readily used for online deployment, where robust constraint satisfaction is paramount. We demonstrate the versatility of MPCritic, in terms of MPC architectures and RL algorithms that it can accommodate, on classic control benchmarks. </p>
<blockquote>
<p>å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰å’Œæ¨¡å‹é¢„æµ‹æ§åˆ¶ï¼ˆMPCï¼‰ç¤¾åŒºå·²ç»ä¸ºè§£å†³æœ€ä¼˜æ§åˆ¶é—®é¢˜å»ºç«‹äº†ä¸°å¯Œçš„ç†è®ºæ–¹æ³•å’Œè®¡ç®—å·¥å…·ç”Ÿæ€ç³»ç»Ÿã€‚é‰´äºå®ƒä»¬æ¦‚å¿µä¸Šçš„ç›¸ä¼¼æ€§ä½†å„æœ‰ä¼˜åŠ¿ï¼Œå°†RLå’ŒMPCååŒå·¥ä½œçš„å…´è¶£æ—¥ç›Šæµ“åšã€‚ç„¶è€Œï¼Œç°æœ‰æ–¹æ³•å¾€å¾€å› å„ç§åŸå› è€Œå—åˆ°é™åˆ¶ï¼ŒåŒ…æ‹¬RLç®—æ³•ä¸­çš„MPCè®¡ç®—æˆæœ¬ä»¥åŠå®ç°MPCå’ŒRLå·¥å…·æ— ç¼é›†æˆçš„è½¯ä»¶éšœç¢ã€‚è¿™äº›æŒ‘æˆ˜é€šå¸¸å¯¼è‡´ä½¿ç”¨â€œç®€å•â€çš„MPCæ–¹æ¡ˆæˆ–RLç®—æ³•ï¼Œè€Œå¿½ç•¥äº†è¿™ä¸¤ä¸ªé¢†åŸŸçš„æœ€æ–°æŠ€æœ¯ã€‚æœ¬æ–‡ä»‹ç»äº†MPCriticï¼Œä¸€ç§ä¸MPCå·¥å…·æ— ç¼å…¼å®¹ã€é€‚åˆæœºå™¨å­¦ä¹ çš„æ¶æ„ã€‚MPCriticåˆ©ç”¨å‚æ•°åŒ–MPCé—®é¢˜æ‰€å®šä¹‰çš„æŸå¤±æ™¯è§‚ï¼Œä¸“æ³¨äºæ‰¹å¤„ç†è®­ç»ƒæ­¥éª¤ä¸Šçš„â€œè½¯â€ä¼˜åŒ–ï¼›ä»è€Œæ›´æ–°MPCå‚æ•°ï¼Œé¿å…æ˜‚è´µçš„æœ€å°åŒ–å’Œå‚æ•°æ•æ„Ÿæ€§ã€‚ç”±äºè®­ç»ƒè¿‡ç¨‹ä¸­ä¿ç•™äº†MPCç»“æ„ï¼Œå› æ­¤å¯ä»¥ä½¿ç”¨MPCä»£ç†è¿›è¡Œåœ¨çº¿éƒ¨ç½²ï¼Œå…¶ä¸­é²æ£’çš„çº¦æŸæ»¡è¶³è‡³å…³é‡è¦ã€‚æˆ‘ä»¬åœ¨ç»å…¸çš„æ§åˆ¶åŸºå‡†æµ‹è¯•ä¸Šå±•ç¤ºäº†MPCriticåœ¨MPCæ¶æ„å’Œå¯ä»¥é€‚åº”çš„RLç®—æ³•æ–¹é¢çš„é€šç”¨æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.01086v1">PDF</a> Preprint for CDC 2025</p>
<p><strong>Summary</strong></p>
<p>å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰ä¸æ¨¡å‹é¢„æµ‹æ§åˆ¶ï¼ˆMPCï¼‰æ˜¯è§£å†³æœ€ä¼˜æ§åˆ¶é—®é¢˜çš„ä¸¤å¤§é‡è¦é¢†åŸŸã€‚äºŒè€…åœ¨ç†è®ºæ–¹æ³•ä¸è®¡ç®—å·¥å…·ä¸Šå„è‡ªå½¢æˆäº†åºå¤§çš„ç”Ÿæ€ç³»ç»Ÿï¼Œä¸”å­˜åœ¨èåˆè¶‹åŠ¿ã€‚ç„¶è€Œï¼Œç°æœ‰èåˆæ–¹æ³•é¢ä¸´è®¡ç®—æˆæœ¬ä¸è½¯ä»¶æ•´åˆç­‰æŒ‘æˆ˜ï¼Œå¯¼è‡´æ— æ³•å……åˆ†åˆ©ç”¨ä¸¤ä¸ªé¢†åŸŸçš„æœ€æ–°æŠ€æœ¯ã€‚æœ¬æ–‡æå‡ºä¸€ç§åä¸ºMPCriticçš„æœºå™¨å­¦ä¹ å‹å¥½æ¶æ„ï¼Œè¯¥æ¶æ„ä¸MPCå·¥å…·æ— ç¼é›†æˆï¼Œåˆ©ç”¨å‚æ•°åŒ–MPCé—®é¢˜çš„æŸå¤±æ™¯è§‚è¿›è¡Œâ€œè½¯â€ä¼˜åŒ–ï¼Œé¿å…æ˜‚è´µçš„æœ€å°åŒ–å’Œå‚æ•°æ•æ„Ÿæ€§è®¡ç®—ã€‚åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ä¿ç•™MPCç»“æ„ï¼Œä¾¿äºåœ¨çº¿éƒ¨ç½²æ—¶ä½¿ç”¨MPCä»£ç†è¿›è¡Œç¨³å¥çº¦æŸæ»¡è¶³ã€‚åœ¨ç»å…¸æ§åˆ¶åŸºå‡†æµ‹è¯•ä¸­éªŒè¯äº†MPCriticçš„é€šç”¨æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰å’Œæ¨¡å‹é¢„æµ‹æ§åˆ¶ï¼ˆMPCï¼‰æ˜¯æ±‚è§£æœ€ä¼˜æ§åˆ¶é—®é¢˜çš„ä¸¤å¤§æ–¹æ³•ï¼Œåˆ†åˆ«æ‹¥æœ‰åºå¤§çš„ç†è®ºæ–¹æ³•å’Œè®¡ç®—å·¥å…·ç”Ÿæ€ç³»ç»Ÿã€‚</li>
<li>RLå’ŒMPCçš„èåˆå¼•èµ·è¶Šæ¥è¶Šå¤šå…´è¶£ï¼Œä½†ç°æœ‰æ–¹æ³•å› è®¡ç®—æˆæœ¬å’Œè½¯ä»¶æ•´åˆéš¾é¢˜è€Œå—é™ã€‚</li>
<li>MPCriticæ˜¯ä¸€ä¸ªæœºå™¨å­¦ä¹ å‹å¥½çš„æ¶æ„ï¼Œä¸MPCå·¥å…·æ— ç¼é›†æˆï¼Œåˆ©ç”¨å‚æ•°åŒ–MPCé—®é¢˜çš„æŸå¤±æ™¯è§‚è¿›è¡Œè½¯ä¼˜åŒ–ã€‚</li>
<li>MPCriticé¿å…æ˜‚è´µçš„æœ€å°åŒ–å’Œå‚æ•°æ•æ„Ÿæ€§è®¡ç®—ï¼ŒåŒæ—¶ä¿ç•™MPCç»“æ„ï¼Œä¾¿äºåœ¨çº¿éƒ¨ç½²æ—¶çš„ç¨³å¥çº¦æŸæ»¡è¶³ã€‚</li>
<li>MPCriticå…·æœ‰å¹¿æ³›çš„é€‚ç”¨æ€§ï¼Œå¯ä»¥é€‚åº”ä¸åŒçš„MPCæ¶æ„å’Œå¼ºåŒ–å­¦ä¹ ç®—æ³•ã€‚</li>
<li>æ–‡ç« é€šè¿‡ç»å…¸æ§åˆ¶åŸºå‡†æµ‹è¯•éªŒè¯äº†MPCriticçš„æ•ˆèƒ½å’Œé€šç”¨æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.01086">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-19bd941b4d54730011c59cff087f1cc3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-22d6c79986d95be1044744087f8acf38.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-820e6a4a7855a8eb036dd77d11510c02.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6e76756181bb21994c939cb9cfb66e8a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-360142efe12ed4362cd877b981531509.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-9ab404f85cb20034b0310880bea675c6.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="When-To-Solve-When-To-Verify-Compute-Optimal-Problem-Solving-and-Generative-Verification-for-LLM-Reasoning"><a href="#When-To-Solve-When-To-Verify-Compute-Optimal-Problem-Solving-and-Generative-Verification-for-LLM-Reasoning" class="headerlink" title="When To Solve, When To Verify: Compute-Optimal Problem Solving and   Generative Verification for LLM Reasoning"></a>When To Solve, When To Verify: Compute-Optimal Problem Solving and   Generative Verification for LLM Reasoning</h2><p><strong>Authors:Nishad Singhi, Hritik Bansal, Arian Hosseini, Aditya Grover, Kai-Wei Chang, Marcus Rohrbach, Anna Rohrbach</strong></p>
<p>Scaling test-time compute has emerged as a key strategy for enhancing the reasoning capabilities of large language models (LLMs), particularly in tasks like mathematical problem-solving. A traditional approach, Self-Consistency (SC), generates multiple solutions to a problem and selects the most common answer via majority voting. Another common method involves scoring each solution with a reward model (verifier) and choosing the best one. Recent advancements in Generative Reward Models (GenRM) reframe verification as a next-token prediction task, enabling inference-time scaling along a new axis. Specifically, GenRM generates multiple verification chains-of-thought to score each solution. Under a limited inference budget, this introduces a fundamental trade-off: should you spend the budget on scaling solutions via SC or generate fewer solutions and allocate compute to verification via GenRM? To address this, we evaluate GenRM against SC under a fixed inference budget. Interestingly, we find that SC is more compute-efficient than GenRM for most practical inference budgets across diverse models and datasets. For instance, GenRM first matches SC after consuming up to 8x the inference compute and requires significantly more compute to outperform it. Furthermore, we derive inference scaling laws for the GenRM paradigm, revealing that compute-optimal inference favors scaling solution generation more aggressively than scaling the number of verifications. Our work provides practical guidance on optimizing test-time scaling by balancing solution generation and verification. The code is available at <a target="_blank" rel="noopener" href="https://github.com/nishadsinghi/sc-genrm-scaling">https://github.com/nishadsinghi/sc-genrm-scaling</a>. </p>
<blockquote>
<p>åœ¨æµ‹è¯•æ—¶é—´è®¡ç®—ç¼©æ”¾å·²æˆä¸ºå¢å¼ºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆå°¤å…¶æ˜¯æ•°å­¦é—®é¢˜è§£å†³ç­‰ä»»åŠ¡ä¸­çš„ï¼‰æ¨ç†èƒ½åŠ›çš„ä¸€ç§å…³é”®ç­–ç•¥ã€‚ä¸€ç§ä¼ ç»Ÿçš„æ–¹æ³•æ˜¯è‡ªæ´½æ€§ï¼ˆSCï¼‰ï¼Œå®ƒé€šè¿‡å¤šæ•°æŠ•ç¥¨æœºåˆ¶ç”Ÿæˆä¸€ä¸ªé—®é¢˜çš„å¤šä¸ªè§£å†³æ–¹æ¡ˆå¹¶é€‰æ‹©æœ€å¸¸è§çš„ç­”æ¡ˆã€‚å¦ä¸€ç§å¸¸è§çš„æ–¹æ³•æ˜¯åˆ©ç”¨å¥–åŠ±æ¨¡å‹ï¼ˆéªŒè¯å™¨ï¼‰ä¸ºæ¯ä¸€ä¸ªè§£å†³æ–¹æ¡ˆæ‰“åˆ†ï¼Œç„¶åé€‰æ‹©æœ€ä½³çš„è§£å†³æ–¹æ¡ˆã€‚æœ€è¿‘ç”Ÿæˆçš„å¥–åŠ±æ¨¡å‹ï¼ˆGenRMï¼‰çš„è¿›å±•å°†éªŒè¯é‡æ–°æ„å»ºä¸ºä¸‹ä¸€ä¸ªæ ‡è®°é¢„æµ‹ä»»åŠ¡ï¼Œæ²¿æ–°çš„è½´å®ç°æ¨ç†æ—¶é—´çš„ç¼©æ”¾ã€‚å…·ä½“æ¥è¯´ï¼ŒGenRMç”Ÿæˆå¤šä¸ªéªŒè¯é“¾æ¥è¯„ä¼°æ¯ä¸ªè§£å†³æ–¹æ¡ˆçš„å¾—åˆ†ã€‚åœ¨æœ‰é™çš„æ¨ç†é¢„ç®—ä¸‹ï¼Œè¿™å¼•å…¥äº†ä¸€ä¸ªåŸºæœ¬çš„æƒè¡¡ï¼šä½ åº”ä½¿ç”¨é¢„ç®—é€šè¿‡SCæ¥æ‰©å±•è§£å†³æ–¹æ¡ˆï¼Œè¿˜æ˜¯ç”Ÿæˆè¾ƒå°‘çš„è§£å†³æ–¹æ¡ˆå¹¶å°†è®¡ç®—åˆ†é…ç»™é€šè¿‡GenRMè¿›è¡ŒéªŒè¯ï¼Ÿä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬åœ¨å›ºå®šçš„æ¨ç†é¢„ç®—ä¸‹è¯„ä¼°äº†GenRMä¸SCä¹‹é—´çš„è¡¨ç°ã€‚æœ‰è¶£çš„æ˜¯ï¼Œæˆ‘ä»¬å‘ç°å¯¹äºå„ç§æ¨¡å‹å’Œæ•°æ®é›†çš„å®é™…æ¨ç†é¢„ç®—è€Œè¨€ï¼ŒSCåœ¨è®¡ç®—æ•ˆç‡ä¸Šæ¯”GenRMæ›´é«˜ã€‚ä¾‹å¦‚ï¼ŒGenRMåœ¨æ¶ˆè€—é«˜è¾¾8å€çš„æ¨ç†è®¡ç®—ä¹‹åæ‰é¦–æ¬¡è¾¾åˆ°ä¸SCç›¸åŒ¹é…çš„æ€§èƒ½ï¼Œå¹¶ä¸”ä¸ºäº†ä¼˜äºå®ƒï¼Œéœ€è¦æ›´å¤šçš„è®¡ç®—ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å¾—å‡ºäº†GenRMèŒƒå¼çš„æ¨ç†ç¼©æ”¾å®šå¾‹ï¼Œæ­ç¤ºäº†è®¡ç®—æœ€ä¼˜æ¨ç†æ›´å€¾å‘äºç§¯ææ‰©å±•è§£å†³æ–¹æ¡ˆç”Ÿæˆè€Œä¸æ˜¯æ‰©å¤§éªŒè¯æ•°é‡ã€‚æˆ‘ä»¬çš„å·¥ä½œæä¾›äº†å…³äºé€šè¿‡å¹³è¡¡è§£å†³æ–¹æ¡ˆç”Ÿæˆå’ŒéªŒè¯æ¥ä¼˜åŒ–æµ‹è¯•æ—¶é—´ç¼©æ”¾çš„å®ç”¨æŒ‡å¯¼ã€‚ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/nishadsinghi/sc-genrm-scaling">https://github.com/nishadsinghi/sc-genrm-scaling</a>è·å–ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.01005v1">PDF</a> 29 pages</p>
<p><strong>Summary</strong></p>
<p>æ–‡æœ¬ä¸»è¦è®¨è®ºäº†åœ¨å¤§è§„æ¨¡è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ•°å­¦é—®é¢˜è§£å†³ä»»åŠ¡ä¸­ï¼Œæµ‹è¯•æ—¶è®¡ç®—è§„æ¨¡çš„æ‰©å¤§ç­–ç•¥ã€‚ä»‹ç»äº†ä¸¤ç§å¸¸è§çš„æµ‹è¯•æ—¶è®¡ç®—è§„æ¨¡æ‰©å¤§ç­–ç•¥ï¼šè‡ªæˆ‘ä¸€è‡´æ€§ï¼ˆSCï¼‰å’Œç”Ÿæˆå¥–åŠ±æ¨¡å‹ï¼ˆGenRMï¼‰ã€‚SCé€šè¿‡äº§ç”Ÿå¤šä¸ªè§£å†³æ–¹æ¡ˆå¹¶é€‰æ‹©æœ€å¸¸è§çš„ç­”æ¡ˆï¼Œè€ŒGenRMå°†éªŒè¯é‡æ–°å®šä¹‰ä¸ºä¸‹ä¸€ä¸ªæ ‡è®°é¢„æµ‹ä»»åŠ¡ã€‚ç ”ç©¶å‘ç°ï¼Œåœ¨æœ‰é™çš„æ¨ç†é¢„ç®—ä¸‹ï¼ŒSCåœ¨è®¡ç®—æ•ˆç‡ä¸Šä¼˜äºGenRMã€‚æ­¤å¤–ï¼Œè¿˜æ¨å¯¼å‡ºäº†GenRMçš„æ¨ç†è§„æ¨¡å®šå¾‹ï¼ŒæŒ‡å‡ºåœ¨è®¡ç®—æœ€ä¼˜æ¨ç†æ—¶ï¼Œæ›´å€¾å‘äºç§¯ææ‰©å¤§è§£å†³æ–¹æ¡ˆçš„ç”Ÿæˆï¼Œè€Œä¸æ˜¯å¢åŠ éªŒè¯æ¬¡æ•°ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æµ‹è¯•æ—¶è®¡ç®—è§„æ¨¡æ‰©å¤§æ˜¯æå‡å¤§è§„æ¨¡è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ¨ç†èƒ½åŠ›ï¼Œç‰¹åˆ«æ˜¯åœ¨æ•°å­¦é—®é¢˜è§£å†³ä»»åŠ¡ä¸­çš„å…³é”®ç­–ç•¥ã€‚</li>
<li>è‡ªæˆ‘ä¸€è‡´æ€§ï¼ˆSCï¼‰å’Œç”Ÿæˆå¥–åŠ±æ¨¡å‹ï¼ˆGenRMï¼‰æ˜¯ä¸¤ç§å¸¸ç”¨çš„æµ‹è¯•æ—¶è®¡ç®—è§„æ¨¡æ‰©å¤§ç­–ç•¥ã€‚</li>
<li>åœ¨æœ‰é™çš„æ¨ç†é¢„ç®—ä¸‹ï¼ŒSCçš„è®¡ç®—æ•ˆç‡é«˜äºGenRMã€‚</li>
<li>GenRMåœ¨æ¶ˆè€—é«˜è¾¾8å€çš„æ¨ç†è®¡ç®—é‡åæ‰èƒ½ä¸SCç›¸åŒ¹é…ï¼Œå¹¶éœ€è¦æ›´å¤šçš„è®¡ç®—æ‰èƒ½è¶…è¶Šå®ƒã€‚</li>
<li>æ¨å¯¼å‡ºçš„GenRMæ¨ç†è§„æ¨¡å®šå¾‹è¡¨æ˜ï¼Œåœ¨è®¡ç®—æœ€ä¼˜æ¨ç†æ—¶ï¼Œåº”æ›´å€¾å‘äºæ‰©å¤§è§£å†³æ–¹æ¡ˆçš„ç”Ÿæˆï¼Œè€Œä¸æ˜¯å¢åŠ éªŒè¯æ¬¡æ•°ã€‚</li>
<li>å¹³è¡¡è§£å†³æ–¹æ¡ˆçš„ç”Ÿæˆå’ŒéªŒè¯æ˜¯ä¼˜åŒ–æµ‹è¯•æ—¶è®¡ç®—è§„æ¨¡æ‰©å¤§çš„å…³é”®ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.01005">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-c86cf6f5c74155934ce7fda0b2c9fdd7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-81fad848d2fb6c0d0cb1dacfee2113dd.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2ef97ef20edb09ba34cd04c6967adc22.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="MedReason-Eliciting-Factual-Medical-Reasoning-Steps-in-LLMs-via-Knowledge-Graphs"><a href="#MedReason-Eliciting-Factual-Medical-Reasoning-Steps-in-LLMs-via-Knowledge-Graphs" class="headerlink" title="MedReason: Eliciting Factual Medical Reasoning Steps in LLMs via   Knowledge Graphs"></a>MedReason: Eliciting Factual Medical Reasoning Steps in LLMs via   Knowledge Graphs</h2><p><strong>Authors:Juncheng Wu, Wenlong Deng, Xingxuan Li, Sheng Liu, Taomian Mi, Yifan Peng, Ziyang Xu, Yi Liu, Hyunjin Cho, Chang-In Choi, Yihan Cao, Hui Ren, Xiang Li, Xiaoxiao Li, Yuyin Zhou</strong></p>
<p>Medical tasks such as diagnosis and treatment planning require precise and complex reasoning, particularly in life-critical domains. Unlike mathematical reasoning, medical reasoning demands meticulous, verifiable thought processes to ensure reliability and accuracy. However, there is a notable lack of datasets that provide transparent, step-by-step reasoning to validate and enhance the medical reasoning ability of AI models. To bridge this gap, we introduce MedReason, a large-scale high-quality medical reasoning dataset designed to enable faithful and explainable medical problem-solving in large language models (LLMs). We utilize a structured medical knowledge graph (KG) to convert clinical QA pairs into logical chains of reasoning, or &#96;&#96;thinking pathsâ€™â€™, which trace connections from question elements to answers via relevant KG entities. Each path is validated for consistency with clinical logic and evidence-based medicine. Our pipeline generates detailed reasoning for various medical questions from 7 medical datasets, resulting in a dataset of 32,682 question-answer pairs, each with detailed, step-by-step explanations. Experiments demonstrate that fine-tuning with our dataset consistently boosts medical problem-solving capabilities, achieving significant gains of up to 7.7% for DeepSeek-Ditill-8B. Our top-performing model, MedReason-8B, outperforms the Huatuo-o1-8B, a state-of-the-art medical reasoning model, by up to 4.2% on the clinical benchmark MedBullets. We also engage medical professionals from diverse specialties to assess our datasetâ€™s quality, ensuring MedReason offers accurate and coherent medical reasoning. Our data, models, and code will be publicly available. </p>
<blockquote>
<p>åŒ»å­¦ä»»åŠ¡å¦‚è¯Šæ–­å’Œåˆ¶å®šæ²»ç–—æ–¹æ¡ˆéœ€è¦è¿›è¡Œç²¾ç¡®ä¸”å¤æ‚çš„æ¨ç†ï¼Œç‰¹åˆ«æ˜¯åœ¨å…³é”®çš„ç”Ÿå‘½é¢†åŸŸã€‚ä¸åŒäºæ•°å­¦æ¨ç†ï¼ŒåŒ»å­¦æ¨ç†éœ€è¦ç»†è‡´ä¸”å¯éªŒè¯çš„æ€è€ƒè¿‡ç¨‹ï¼Œä»¥ç¡®ä¿å¯é æ€§å’Œå‡†ç¡®æ€§ã€‚ç„¶è€Œï¼Œç¼ºä¹æä¾›é€æ˜ã€é€æ­¥æ¨ç†çš„æ•°æ®é›†æ¥éªŒè¯å’Œæå‡AIæ¨¡å‹çš„åŒ»å­¦æ¨ç†èƒ½åŠ›ã€‚ä¸ºäº†å¼¥è¡¥è¿™ä¸€å·®è·ï¼Œæˆ‘ä»¬å¼•å…¥äº†MedReasonï¼Œè¿™æ˜¯ä¸€ä¸ªå¤§è§„æ¨¡ã€é«˜è´¨é‡çš„åŒ»å­¦æ¨ç†æ•°æ®é›†ï¼Œæ—¨åœ¨åœ¨å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¸­å®ç°å¿ å®ä¸”å¯è§£é‡Šçš„åŒ»å­¦é—®é¢˜è§£å†³ã€‚æˆ‘ä»¬åˆ©ç”¨ç»“æ„åŒ–åŒ»å­¦çŸ¥è¯†å›¾è°±ï¼ˆKGï¼‰å°†ä¸´åºŠé—®ç­”å¯¹è½¬æ¢ä¸ºé€»è¾‘æ¨ç†é“¾ï¼Œæˆ–ç§°ä¸ºâ€œæ€è€ƒè·¯å¾„â€ï¼Œä»é—®é¢˜å…ƒç´ è¿½è¸ªåˆ°é€šè¿‡ç›¸å…³KGå®ä½“å¾—åˆ°çš„ç­”æ¡ˆã€‚æ¯æ¡è·¯å¾„éƒ½ç»è¿‡ä¸´åºŠé€»è¾‘å’Œå¾ªè¯åŒ»å­¦çš„éªŒè¯ã€‚æˆ‘ä»¬çš„ç®¡é“ä»7ä¸ªåŒ»å­¦æ•°æ®é›†ä¸­ç”Ÿæˆäº†å„ç§åŒ»å­¦é—®é¢˜çš„è¯¦ç»†æ¨ç†ï¼Œå½¢æˆäº†ä¸€ä¸ªåŒ…å«32,682ä¸ªé—®ç­”å¯¹çš„æ•°æ®é›†ï¼Œæ¯ä¸ªé—®ç­”å¯¹éƒ½æœ‰è¯¦ç»†ã€é€æ­¥çš„è§£é‡Šã€‚å®éªŒè¡¨æ˜ï¼Œä½¿ç”¨æˆ‘ä»¬çš„æ•°æ®é›†è¿›è¡Œå¾®è°ƒå¯ä»¥æŒç»­æé«˜åŒ»å­¦é—®é¢˜è§£å†³èƒ½åŠ›ï¼Œå¯¹DeepSeek-Ditill-8Bçš„æ”¹è¿›å¹…åº¦é«˜è¾¾7.7%ã€‚æˆ‘ä»¬è¡¨ç°æœ€ä½³çš„æ¨¡å‹MedReason-8Båœ¨ä¸´åºŠåŒ»å­¦åŸºå‡†æµ‹è¯•MedBulletsä¸Šçš„è¡¨ç°ä¼˜äºæœ€æ–°çš„åŒ»å­¦æ¨ç†æ¨¡å‹åä½—-o1-8Bï¼Œæœ€é«˜æå‡äº†4.2%ã€‚æˆ‘ä»¬è¿˜é‚€è¯·äº†æ¥è‡ªä¸åŒä¸“ä¸šçš„åŒ»ç–—ä¸“å®¶æ¥è¯„ä¼°æˆ‘ä»¬æ•°æ®é›†çš„è´¨é‡ï¼Œç¡®ä¿MedReasonæä¾›å‡†ç¡®ä¸”è¿è´¯çš„åŒ»å­¦æ¨ç†ã€‚æˆ‘ä»¬çš„æ•°æ®ã€æ¨¡å‹å’Œä»£ç å°†å…¬å¼€å¯ç”¨ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.00993v1">PDF</a> </p>
<p><strong>æ‘˜è¦</strong></p>
<p>åŒ»ç–—ä»»åŠ¡çš„è¯Šæ–­ä¸æ²»ç–—è§„åˆ’éœ€è¦ç²¾ç¡®ä¸”å¤æ‚çš„æ¨ç†ï¼Œç‰¹åˆ«æ˜¯åœ¨å…³é”®çš„ç”Ÿå‘½é¢†åŸŸã€‚åŒ»ç–—æ¨ç†ä¸åŒäºæ•°å­¦æ¨ç†ï¼Œéœ€è¦ç»†è‡´ä¸”å¯éªŒè¯çš„æ€è€ƒè¿‡ç¨‹ä»¥ç¡®ä¿å¯é æ€§å’Œå‡†ç¡®æ€§ã€‚ç„¶è€Œï¼Œç¼ºä¹æä¾›é€æ˜ã€é€æ­¥æ¨ç†çš„æ•°æ®é›†æ¥éªŒè¯å’Œæå‡AIæ¨¡å‹çš„åŒ»ç–—æ¨ç†èƒ½åŠ›ã€‚ä¸ºå¼¥è¡¥è¿™ä¸€å·®è·ï¼Œæˆ‘ä»¬æ¨å‡ºMedReasonï¼Œä¸€ä¸ªå¤§è§„æ¨¡é«˜è´¨é‡çš„åŒ»ç–—æ¨ç†æ•°æ®é›†ï¼Œæ—¨åœ¨åœ¨å¤§å‹è¯­è¨€æ¨¡å‹ä¸­å®ç°å¿ å®ä¸”å¯è§£é‡Šçš„åŒ»ç–—é—®é¢˜è§£å†³ã€‚æˆ‘ä»¬åˆ©ç”¨ç»“æ„åŒ–åŒ»ç–—çŸ¥è¯†å›¾è°±å°†ä¸´åºŠé—®ç­”å¯¹è½¬åŒ–ä¸ºé€»è¾‘æ¨ç†é“¾ï¼Œå³â€œæ€è€ƒè·¯å¾„â€ï¼Œè¯¥è·¯å¾„è¿½è¸ªä»é—®é¢˜å…ƒç´ åˆ°é€šè¿‡ç›¸å…³KGå®ä½“å›ç­”çš„è”æ¥ã€‚æ¯æ¡è·¯å¾„éƒ½ç»è¿‡ä¸´åºŠé€»è¾‘å’Œå¾ªè¯åŒ»å­¦çš„éªŒè¯ã€‚æˆ‘ä»¬çš„ç®¡é“ç”Ÿæˆäº†å„ç§åŒ»ç–—é—®é¢˜çš„è¯¦ç»†æ¨ç†ï¼Œæ¥è‡ª7ä¸ªåŒ»ç–—æ•°æ®é›†ï¼Œå½¢æˆåŒ…å«32682ä¸ªå¸¦æœ‰è¯¦ç»†é€æ­¥è§£é‡Šçš„é—®é¢˜ç­”æ¡ˆå¯¹çš„æ•°æ®é›†ã€‚å®éªŒè¡¨æ˜ï¼Œä½¿ç”¨æˆ‘ä»¬çš„æ•°æ®é›†è¿›è¡Œå¾®è°ƒï¼Œå¯æŒç»­æå‡åŒ»ç–—é—®é¢˜è§£å†³èƒ½åŠ›ï¼Œåœ¨DeepSeek-Ditill-8Bä¸Šå–å¾—é«˜è¾¾7.7%çš„æ˜¾è‘—å¢é•¿ã€‚æˆ‘ä»¬çš„é¡¶å°–æ¨¡å‹MedReason-8Båœ¨åŒ»ç–—æ¨ç†æ¨¡å‹åä½—o1-8Bä¸Šå–å¾—æ˜¾è‘—ä¼˜åŠ¿ï¼Œåœ¨ä¸´åºŠåŸºå‡†æµ‹è¯•MedBulletsä¸Šé«˜å‡º4.2%ã€‚æˆ‘ä»¬è¿˜é‚€è¯·æ¥è‡ªä¸åŒä¸“ä¸šçš„åŒ»ç–—ä¸“å®¶æ¥è¯„ä¼°æˆ‘ä»¬æ•°æ®é›†çš„è´¨é‡ï¼Œç¡®ä¿MedReasonæä¾›å‡†ç¡®ä¸”è¿è´¯çš„åŒ»ç–—æ¨ç†ã€‚æˆ‘ä»¬çš„æ•°æ®ã€æ¨¡å‹å’Œä»£ç å°†å…¬å¼€æä¾›ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>åŒ»ç–—ä»»åŠ¡éœ€è¦ç²¾ç¡®ä¸”å¤æ‚çš„æ¨ç†ï¼Œç‰¹åˆ«æ˜¯åœ¨è¯Šæ–­ä¸æ²»ç–—è§„åˆ’ä¸­ã€‚</li>
<li>ä¸æ•°å­¦æ¨ç†ä¸åŒï¼ŒåŒ»ç–—æ¨ç†éœ€è¦ç»†è‡´å’Œå¯éªŒè¯çš„æ€è€ƒè¿‡ç¨‹ã€‚</li>
<li>ç›®å‰ç¼ºä¹ç”¨äºéªŒè¯å’Œæå‡AIæ¨¡å‹åŒ»ç–—æ¨ç†èƒ½åŠ›çš„æ•°æ®é›†ã€‚</li>
<li>MedReasonæ˜¯ä¸€ä¸ªå¤§è§„æ¨¡é«˜è´¨é‡çš„åŒ»ç–—æ¨ç†æ•°æ®é›†ï¼Œç”¨äºå¤§å‹è¯­è¨€æ¨¡å‹ä¸­çš„å¿ å®ä¸”å¯è§£é‡Šçš„åŒ»ç–—é—®é¢˜è§£å†³ã€‚</li>
<li>åˆ©ç”¨ç»“æ„åŒ–åŒ»ç–—çŸ¥è¯†å›¾è°±å°†ä¸´åºŠé—®ç­”è½¬åŒ–ä¸ºé€»è¾‘æ¨ç†é“¾ã€‚</li>
<li>MedReasonæ•°æ®é›†åŒ…å«è¯¦ç»†é€æ­¥è§£é‡Šçš„é—®é¢˜ç­”æ¡ˆå¯¹ï¼Œç”±æ¥è‡ªå¤šä¸ªåŒ»ç–—æ•°æ®é›†çš„é—®é¢˜ç”Ÿæˆã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.00993">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-8f22aecfb5ebdd25cc2df9a588690e0b.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-8872d81354e8a2449ca9bf6971a61359.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-255ca0fc871261174452f7da393ae326.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="Grounding-Multimodal-LLMs-to-Embodied-Agents-that-Ask-for-Help-with-Reinforcement-Learning"><a href="#Grounding-Multimodal-LLMs-to-Embodied-Agents-that-Ask-for-Help-with-Reinforcement-Learning" class="headerlink" title="Grounding Multimodal LLMs to Embodied Agents that Ask for Help with   Reinforcement Learning"></a>Grounding Multimodal LLMs to Embodied Agents that Ask for Help with   Reinforcement Learning</h2><p><strong>Authors:Ram Ramrakhya, Matthew Chang, Xavier Puig, Ruta Desai, Zsolt Kira, Roozbeh Mottaghi</strong></p>
<p>Embodied agents operating in real-world environments must interpret ambiguous and under-specified human instructions. A capable household robot should recognize ambiguity and ask relevant clarification questions to infer the user intent accurately, leading to more effective task execution. To study this problem, we introduce the Ask-to-Act task, where an embodied agent must fetch a specific object instance given an ambiguous instruction in a home environment. The agent must strategically ask minimal, yet relevant, clarification questions to resolve ambiguity while navigating under partial observability. To solve this problem, we propose a novel approach that fine-tunes multimodal large language models (MLLMs) as vision-language-action (VLA) policies using online reinforcement learning (RL) with LLM-generated rewards. Our method eliminates the need for large-scale human demonstrations or manually engineered rewards for training such agents. We benchmark against strong zero-shot baselines, including GPT-4o, and supervised fine-tuned MLLMs, on our task. Our results demonstrate that our RL-finetuned MLLM outperforms all baselines by a significant margin ($19.1$-$40.3%$), generalizing well to novel scenes and tasks. To the best of our knowledge, this is the first demonstration of adapting MLLMs as VLA agents that can act and ask for help using LLM-generated rewards with online RL. </p>
<blockquote>
<p>å®ä½“ä»£ç†åœ¨ç°å®ç¯å¢ƒä¸­è¿ä½œæ—¶ï¼Œå¿…é¡»è§£è¯»æ¨¡ç³Šå’Œæœªæ˜ç¡®æŒ‡å®šçš„äººç±»æŒ‡ä»¤ã€‚ä¸€ä¸ªèƒ½åŠ›å¼ºå¤§çš„å®¶ç”¨æœºå™¨äººåº”è¯¥èƒ½å¤Ÿè¯†åˆ«æ¨¡ç³Šæ€§ï¼Œå¹¶æå‡ºç›¸å…³æ¾„æ¸…é—®é¢˜ï¼Œä»¥å‡†ç¡®æ¨æ–­ç”¨æˆ·æ„å›¾ï¼Œä»è€Œæ›´æœ‰æ•ˆåœ°æ‰§è¡Œä»»åŠ¡ã€‚ä¸ºäº†ç ”ç©¶è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†â€œé—®å†è¡ŒåŠ¨â€ä»»åŠ¡ï¼Œåœ¨è¯¥ä»»åŠ¡ä¸­ï¼Œå®ä½“ä»£ç†å¿…é¡»åœ¨å®¶åº­ç¯å¢ƒä¸­æ ¹æ®æ¨¡ç³ŠæŒ‡ä»¤è·å–ç‰¹å®šå¯¹è±¡å®ä¾‹ã€‚ä»£ç†å¿…é¡»ç­–ç•¥æ€§åœ°æå‡ºæœ€å°‘ä½†ç›¸å…³çš„é—®é¢˜æ¥è§£å†³æ¨¡ç³Šæ€§ï¼ŒåŒæ—¶åœ¨éƒ¨åˆ†å¯è§‚å¯Ÿæ€§ä¸‹è¿›è¡Œå¯¼èˆªã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°æ–¹æ³•ï¼Œè¯¥æ–¹æ³•å¾®è°ƒå¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰ï¼Œå°†å…¶ä½œä¸ºè§†è§‰è¯­è¨€è¡ŒåŠ¨ï¼ˆVLAï¼‰ç­–ç•¥ä½¿ç”¨åœ¨çº¿å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰ä¸LLMç”Ÿæˆçš„å¥–åŠ±ã€‚æˆ‘ä»¬çš„æ–¹æ³•æ¶ˆé™¤äº†å¤§å‹äººç±»æ¼”ç¤ºæˆ–æ‰‹åŠ¨å·¥ç¨‹å¥–åŠ±æ¥è®­ç»ƒæ­¤ç±»ä»£ç†çš„éœ€è¦ã€‚æˆ‘ä»¬åœ¨ä»»åŠ¡ä¸Šä¸å¼ºå¤§çš„é›¶æ ·æœ¬åŸºçº¿è¿›è¡Œäº†åŸºå‡†æµ‹è¯•ï¼ŒåŒ…æ‹¬GPT-4oå’Œç»è¿‡ç›‘ç£è®­ç»ƒçš„MLLMsã€‚æˆ‘ä»¬çš„ç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„RLå¾®è°ƒMLLMåœ¨æ‰€æœ‰åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜è¶Šè¾ƒå¤§å¹…åº¦ï¼ˆä»‹äº19.1ï¼…è‡³40.3ï¼…ï¼‰ï¼Œå¯¹æ–°åœºæ™¯å’Œä»»åŠ¡å…·æœ‰è‰¯å¥½çš„æ³›åŒ–èƒ½åŠ›ã€‚æ®æˆ‘ä»¬æ‰€çŸ¥ï¼Œè¿™æ˜¯é¦–æ¬¡å±•ç¤ºé€‚åº”MLLMä½œä¸ºVLAä»£ç†çš„èƒ½åŠ›ï¼Œè¯¥ä»£ç†å¯ä»¥ä½¿ç”¨LLMç”Ÿæˆçš„å¥–åŠ±ä¸åœ¨çº¿RLæ¥è¡ŒåŠ¨å’Œå¯»æ±‚å¸®åŠ©ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.00907v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ¢è®¨äº†åœ¨çœŸå®ä¸–ç•Œç¯å¢ƒä¸­è¿è¡Œçš„å®ä½“ä»£ç†éœ€è¦è§£è¯»æ¨¡ç³Šå’Œæœªæ˜ç¡®æŒ‡å®šçš„äººç±»æŒ‡ä»¤çš„é—®é¢˜ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§æ–°å‹æ–¹æ³•ï¼Œå³é€šè¿‡åœ¨çº¿å¼ºåŒ–å­¦ä¹ å¾®è°ƒå¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼Œå°†å…¶ä½œä¸ºè§†è§‰è¯­è¨€è¡ŒåŠ¨ç­–ç•¥ã€‚è¯¥æ–¹æ³•æ— éœ€å¤§è§„æ¨¡äººç±»æ¼”ç¤ºæˆ–æ‰‹åŠ¨è®¾è®¡çš„å¥–åŠ±æ¥è®­ç»ƒä»£ç†ï¼Œä¸”ç›¸è¾ƒäºé›¶æ ·æœ¬åŸºå‡†æµ‹è¯•ï¼Œå¦‚GPT-4oå’Œé€šè¿‡ç›‘ç£å¾®è°ƒçš„å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼Œè¡¨ç°å‡ºæ˜¾è‘—ä¼˜åŠ¿ã€‚è¿™æ˜¯é¦–æ¬¡å°†å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹é€‚åº”ä¸ºèƒ½è¡ŒåŠ¨å’Œæ±‚åŠ©çš„è§†è§‰è¯­è¨€è¡ŒåŠ¨ä»£ç†ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å®ä½“ä»£ç†éœ€è§£è¯»æ¨¡ç³Šå’Œæœªæ˜ç¡®æŒ‡å®šçš„äººç±»æŒ‡ä»¤ã€‚</li>
<li>å®¶ç”¨æœºå™¨äººåº”èƒ½è¯†åˆ«æ­§ä¹‰å¹¶æå‡ºç›¸å…³æ¾„æ¸…é—®é¢˜ï¼Œä»¥å‡†ç¡®æ¨æ–­ç”¨æˆ·æ„å›¾ï¼Œä»è€Œæé«˜ä»»åŠ¡æ‰§è¡Œæ•ˆç‡ã€‚</li>
<li>å¼•å…¥äº†â€œé—®è¯¢è¡ŒåŠ¨â€ä»»åŠ¡ï¼Œå³å®ä½“ä»£ç†éœ€åœ¨å®¶åº­ç¯å¢ƒä¸­æ ¹æ®æ¨¡ç³ŠæŒ‡ä»¤è·å–ç‰¹å®šå¯¹è±¡å®ä¾‹ã€‚</li>
<li>ä»£ç†éœ€ç­–ç•¥æ€§åœ°æå‡ºæœ€å°‘ä½†ç›¸å…³çš„æ¾„æ¸…é—®é¢˜ä»¥è§£å†³æ­§ä¹‰ï¼ŒåŒæ—¶åœ¨éƒ¨åˆ†å¯è§‚å¯Ÿæ€§ç¯å¢ƒä¸‹è¿›è¡Œå¯¼èˆªã€‚</li>
<li>æå‡ºä¸€ç§æ–°å‹æ–¹æ³•ï¼Œé€šè¿‡åœ¨çº¿å¼ºåŒ–å­¦ä¹ å¾®è°ƒå¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼Œä½œä¸ºè§†è§‰è¯­è¨€è¡ŒåŠ¨ç­–ç•¥ã€‚</li>
<li>è¯¥æ–¹æ³•æ— éœ€å¤§è§„æ¨¡äººç±»æ¼”ç¤ºæˆ–æ‰‹åŠ¨è®¾è®¡çš„å¥–åŠ±ï¼Œä¸”è¡¨ç°å‡ºæ˜¾è‘—ä¼˜äºé›¶æ ·æœ¬åŸºå‡†æµ‹è¯•å’Œç›‘ç£å¾®è°ƒçš„å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹çš„æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.00907">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-f7c8c62ca23f49913ac819badf630190.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-557dae7264ae9b2d318eff1046e3e062.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ed34d8618db18090f3bf1b277f9f34fa.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="GenPRM-Scaling-Test-Time-Compute-of-Process-Reward-Models-via-Generative-Reasoning"><a href="#GenPRM-Scaling-Test-Time-Compute-of-Process-Reward-Models-via-Generative-Reasoning" class="headerlink" title="GenPRM: Scaling Test-Time Compute of Process Reward Models via   Generative Reasoning"></a>GenPRM: Scaling Test-Time Compute of Process Reward Models via   Generative Reasoning</h2><p><strong>Authors:Jian Zhao, Runze Liu, Kaiyan Zhang, Zhimu Zhou, Junqi Gao, Dong Li, Jiafei Lyu, Zhouyi Qian, Biqing Qi, Xiu Li, Bowen Zhou</strong></p>
<p>Recent advancements in Large Language Models (LLMs) have shown that it is promising to utilize Process Reward Models (PRMs) as verifiers to enhance the performance of LLMs. However, current PRMs face three key challenges: (1) limited process supervision and generalization capabilities, (2) dependence on scalar value prediction without leveraging the generative abilities of LLMs, and (3) inability to scale the test-time compute of PRMs. In this work, we introduce GenPRM, a generative process reward model that performs explicit Chain-of-Thought (CoT) reasoning with code verification before providing judgment for each reasoning step. To obtain high-quality process supervision labels and rationale data, we propose Relative Progress Estimation (RPE) and a rationale synthesis framework that incorporates code verification. Experimental results on ProcessBench and several mathematical reasoning tasks show that GenPRM significantly outperforms prior PRMs with only 23K training data from MATH dataset. Through test-time scaling, a 1.5B GenPRM outperforms GPT-4o, and a 7B GenPRM surpasses Qwen2.5-Math-PRM-72B on ProcessBench. Additionally, GenPRM demonstrates strong abilities to serve as a critic model for policy model refinement. This work establishes a new paradigm for process supervision that bridges the gap between PRMs and critic models in LLMs. Our code, model, and data will be available in <a target="_blank" rel="noopener" href="https://ryanliu112.github.io/GenPRM">https://ryanliu112.github.io/GenPRM</a>. </p>
<blockquote>
<p>è¿‘æœŸå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„è¿›å±•æ˜¾ç¤ºå‡ºåˆ©ç”¨è¿‡ç¨‹å¥–åŠ±æ¨¡å‹ï¼ˆPRMï¼‰ä½œä¸ºéªŒè¯å™¨ä»¥æé«˜LLMæ€§èƒ½çš„æ½œåŠ›ã€‚ç„¶è€Œï¼Œå½“å‰PRMé¢ä¸´ä¸‰ä¸ªå…³é”®æŒ‘æˆ˜ï¼šï¼ˆ1ï¼‰è¿‡ç¨‹ç›‘ç£ä¸æ³›åŒ–èƒ½åŠ›æœ‰é™ï¼Œï¼ˆ2ï¼‰ä¾èµ–äºæ ‡é‡å€¼é¢„æµ‹ï¼Œæœªèƒ½åˆ©ç”¨LLMçš„ç”Ÿæˆèƒ½åŠ›ï¼Œï¼ˆ3ï¼‰æ— æ³•æ‰©å±•PRMçš„æµ‹è¯•æ—¶é—´è®¡ç®—ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬å¼•å…¥äº†GenPRMï¼Œä¸€ç§ç”Ÿæˆå¼è¿‡ç¨‹å¥–åŠ±æ¨¡å‹ï¼Œå®ƒåœ¨è¿›è¡Œä»£ç éªŒè¯åè¿›è¡Œæ˜ç¡®çš„æ€ç»´é“¾ï¼ˆCoTï¼‰æ¨ç†ï¼Œç„¶åä¸ºæ¯ä¸€æ­¥æ¨ç†æä¾›åˆ¤æ–­ã€‚ä¸ºäº†è·å¾—é«˜è´¨é‡çš„è¿‡ç¨‹ç›‘ç£æ ‡ç­¾å’Œç†æ€§æ•°æ®ï¼Œæˆ‘ä»¬æå‡ºäº†ç›¸å¯¹è¿›åº¦ä¼°è®¡ï¼ˆRPEï¼‰å’Œä¸€ä¸ªç»“åˆä»£ç éªŒè¯çš„ç†æ€§åˆæˆæ¡†æ¶ã€‚åœ¨ProcessBenchå’Œå¤šä¸ªæ•°å­¦æ¨ç†ä»»åŠ¡ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼Œä»…ä½¿ç”¨MATHæ•°æ®é›†çš„23Kè®­ç»ƒæ•°æ®ï¼ŒGenPRMå°±æ˜¾è‘—ä¼˜äºå…ˆå‰çš„PRMã€‚é€šè¿‡æµ‹è¯•æ—¶é—´ç¼©æ”¾ï¼Œä¸€ä¸ª1.5Bçš„GenPRMè¡¨ç°ä¼˜äºGPT-4oï¼Œè€Œä¸€ä¸ª7Bçš„GenPRMåœ¨ProcessBenchä¸Šè¶…è¶Šäº†Qwen2.5-Math-PRM-72Bã€‚æ­¤å¤–ï¼ŒGenPRMè¡¨ç°å‡ºå¼ºå¤§çš„èƒ½åŠ›ï¼Œå¯ä»¥ä½œä¸ºæ”¿ç­–æ¨¡å‹ç»†åŒ–çš„æ‰¹è¯„æ¨¡å‹ã€‚è¿™é¡¹å·¥ä½œä¸ºè¿‡ç¨‹ç›‘ç£å»ºç«‹äº†æ–°èŒƒå¼ï¼Œç¼©å°äº†PRMå’ŒLLMä¸­æ‰¹è¯„æ¨¡å‹ä¹‹é—´çš„å·®è·ã€‚æˆ‘ä»¬çš„ä»£ç ã€æ¨¡å‹å’Œæ•°æ®å°†åœ¨<a target="_blank" rel="noopener" href="https://ryanliu112.github.io/GenPRM%E4%B8%8A%E6%8F%90%E4%BE%9B%E3%80%82">https://ryanliu112.github.io/GenPRMä¸Šæä¾›ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.00891v1">PDF</a> </p>
<p><strong>Summary</strong>ï¼š<br>æœ€è¿‘çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„è¿›æ­¥æ˜¾ç¤ºï¼Œåˆ©ç”¨è¿‡ç¨‹å¥–åŠ±æ¨¡å‹ï¼ˆPRMï¼‰ä½œä¸ºéªŒè¯å™¨æ¥æé«˜LLMçš„æ€§èƒ½å…·æœ‰å‰æ™¯ã€‚ç„¶è€Œï¼Œå½“å‰PRMé¢ä¸´ä¸‰ä¸ªä¸»è¦æŒ‘æˆ˜ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬å¼•å…¥äº†GenPRMï¼Œä¸€ä¸ªæ‰§è¡Œæ˜¾å¼é“¾å¼æ€ç»´ï¼ˆCoTï¼‰æ¨ç†çš„è¿‡ç¨‹å¥–åŠ±æ¨¡å‹ï¼Œåœ¨æä¾›æ¯ä¸ªæ¨ç†æ­¥éª¤çš„åˆ¤æ–­ä¹‹å‰è¿›è¡Œä»£ç éªŒè¯ã€‚é€šè¿‡ç›¸å¯¹è¿›åº¦ä¼°è®¡ï¼ˆRPEï¼‰å’Œèåˆä»£ç éªŒè¯çš„ç†æ€§åˆæˆæ¡†æ¶ï¼ŒGenPRMåœ¨ProcessBenchå’Œæ•°å­¦æ¨ç†ä»»åŠ¡ä¸Šæ˜¾è‘—ä¼˜äºå…ˆå‰çš„PRMï¼Œä»…ä½¿ç”¨MATHæ•°æ®é›†çš„23Kè®­ç»ƒæ•°æ®ã€‚æ­¤å¤–ï¼ŒGenPRMè¿˜è¡¨ç°å‡ºå¼ºå¤§çš„èƒ½åŠ›ï¼Œå¯ä½œä¸ºæ”¿ç­–æ¨¡å‹æ”¹è¿›çš„æ‰¹è¯„æ¨¡å‹ã€‚è¿™é¡¹å·¥ä½œä¸ºPRMå’ŒLLMä¸­çš„æ‰¹è¯„æ¨¡å‹ä¹‹é—´çš„è¿‡ç¨‹ç›‘ç£å»ºç«‹äº†æ–°çš„èŒƒå¼ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>GenPRMæ˜¯ä¸€ä¸ªç»“åˆæ˜¾å¼é“¾å¼æ€ç»´ï¼ˆCoTï¼‰æ¨ç†å’Œä»£ç éªŒè¯çš„è¿‡ç¨‹å¥–åŠ±æ¨¡å‹ã€‚</li>
<li>å½“å‰PRMé¢ä¸´çš„ä¸»è¦æŒ‘æˆ˜åŒ…æ‹¬æœ‰é™çš„è¿›ç¨‹ç›‘ç£ã€ç¼ºä¹æ³›åŒ–èƒ½åŠ›ä»¥åŠå¯¹æ ‡é‡å€¼é¢„æµ‹çš„ä¾èµ–ç­‰ã€‚</li>
<li>GenPRMé€šè¿‡ç›¸å¯¹è¿›åº¦ä¼°è®¡ï¼ˆRPEï¼‰å’Œç†æ€§åˆæˆæ¡†æ¶è·å¾—é«˜è´¨é‡çš„è¿‡ç¨‹ç›‘ç£æ ‡ç­¾å’Œç†æ€§æ•°æ®ã€‚</li>
<li>åœ¨æ•°å­¦æ¨ç†ä»»åŠ¡ä¸Šï¼ŒGenPRMæ˜¾è‘—ä¼˜äºå…ˆå‰çš„PRMï¼Œä»…ä½¿ç”¨æœ‰é™çš„è®­ç»ƒæ•°æ®ã€‚</li>
<li>GenPRMåœ¨æµ‹è¯•æ—¶çš„å¯æ‰©å±•æ€§è¡¨ç°ä¼˜å¼‚ï¼Œè¾ƒå¤§çš„æ¨¡å‹å¦‚7B GenPRMåœ¨ProcessBenchä¸Šè¶…è¶Šäº†Qwen2.5-Math-PRM-72Bã€‚</li>
<li>GenPRMä¸ä»…ä½œä¸ºéªŒè¯å™¨è¡¨ç°å‡ºè‰²ï¼Œè¿˜å…·å¤‡ä½œä¸ºæ‰¹è¯„æ¨¡å‹è¿›è¡Œæ”¿ç­–æ¨¡å‹æ”¹è¿›çš„èƒ½åŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.00891">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-b2ca0c8ebd3d824a7ce0879f1881e471.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-70fae2e17e281ee21f550d08d308fa92.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-eb9ca27e8126977b2a49c93b31bd2e84.jpg" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="Improved-Visual-Spatial-Reasoning-via-R1-Zero-Like-Training"><a href="#Improved-Visual-Spatial-Reasoning-via-R1-Zero-Like-Training" class="headerlink" title="Improved Visual-Spatial Reasoning via R1-Zero-Like Training"></a>Improved Visual-Spatial Reasoning via R1-Zero-Like Training</h2><p><strong>Authors:Zhenyi Liao, Qingsong Xie, Yanhao Zhang, Zijian Kong, Haonan Lu, Zhenyu Yang, Zhijie Deng</strong></p>
<p>Increasing attention has been placed on improving the reasoning capacities of multi-modal large language models (MLLMs). As the cornerstone for AI agents that function in the physical realm, video-based visual-spatial intelligence (VSI) emerges as one of the most pivotal reasoning capabilities of MLLMs. This work conducts a first, in-depth study on improving the visual-spatial reasoning of MLLMs via R1-Zero-like training. Technically, we first identify that the visual-spatial reasoning capacities of small- to medium-sized Qwen2-VL models cannot be activated via Chain of Thought (CoT) prompts. We then incorporate GRPO training for improved visual-spatial reasoning, using the carefully curated VSI-100k dataset, following DeepSeek-R1-Zero. During the investigation, we identify the necessity to keep the KL penalty (even with a small value) in GRPO. With just 120 GPU hours, our vsGRPO-2B model, fine-tuned from Qwen2-VL-2B, can outperform the base model by 12.1% and surpass GPT-4o. Moreover, our vsGRPO-7B model, fine-tuned from Qwen2-VL-7B, achieves performance comparable to that of the best open-source model LLaVA-NeXT-Video-72B. Additionally, we compare vsGRPO to supervised fine-tuning and direct preference optimization baselines and observe strong performance superiority. The code and dataset will be available soon. </p>
<blockquote>
<p>å¯¹å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰çš„æ¨ç†èƒ½åŠ›è¶Šæ¥è¶Šé‡è§†ã€‚ä½œä¸ºåœ¨ç‰©ç†é¢†åŸŸå‘æŒ¥ä½œç”¨çš„AIä»£ç†çš„åŸºçŸ³ï¼ŒåŸºäºè§†é¢‘çš„è§†è§‰ç©ºé—´æ™ºèƒ½ï¼ˆVSIï¼‰å·²æˆä¸ºMLLMsä¸­æœ€å…³é”®çš„æ¨ç†èƒ½åŠ›ä¹‹ä¸€ã€‚è¿™é¡¹å·¥ä½œé€šè¿‡R1-Zeroç±»ä¼¼çš„è®­ç»ƒï¼Œé¦–æ¬¡æ·±å…¥ç ”ç©¶äº†æé«˜MLLMsçš„è§†è§‰ç©ºé—´æ¨ç†èƒ½åŠ›ã€‚ä»æŠ€æœ¯ä¸Šè®²ï¼Œæˆ‘ä»¬é¦–å…ˆå‘ç°ä¸­å°å‹Qwen2-VLæ¨¡å‹çš„è§†è§‰ç©ºé—´æ¨ç†èƒ½åŠ›æ— æ³•é€šè¿‡æ€ç»´é“¾ï¼ˆCoTï¼‰æç¤ºæ¥æ¿€æ´»ã€‚ç„¶åï¼Œæˆ‘ä»¬é‡‡ç”¨GRPOè®­ç»ƒæ¥æé«˜è§†è§‰ç©ºé—´æ¨ç†èƒ½åŠ›ï¼Œä½¿ç”¨ç²¾å¿ƒç¼–åˆ¶çš„VSI-100kæ•°æ®é›†ï¼Œéµå¾ªDeepSeek-R1-Zeroã€‚åœ¨è°ƒæŸ¥è¿‡ç¨‹ä¸­ï¼Œæˆ‘ä»¬å‘ç°äº†åœ¨GRPOä¸­ä¿æŒKLæƒ©ç½šï¼ˆå³ä½¿å€¼å¾ˆå°ï¼‰çš„å¿…è¦æ€§ã€‚åªéœ€120ä¸ªGPUå°æ—¶ï¼Œæˆ‘ä»¬ä»Qwen2-VL-2Bå¾®è°ƒå¾—åˆ°çš„vsGRPO-2Bæ¨¡å‹ï¼Œå°±å¯ä»¥æ¯”åŸºç¡€æ¨¡å‹é«˜å‡º12.1%ï¼Œå¹¶è¶…è¶ŠGPT-4oã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬ä»Qwen2-VL-7Bå¾®è°ƒå¾—åˆ°çš„vsGRPO-7Bæ¨¡å‹ï¼Œå…¶æ€§èƒ½å¯ä¸æœ€ä½³å¼€æºæ¨¡å‹LLaVA-NeXT-Video-72Bç›¸åª²ç¾ã€‚å¦å¤–ï¼Œæˆ‘ä»¬å°†vsGRPOä¸ç›‘ç£å¾®è°ƒå’Œç›´æ¥åå¥½ä¼˜åŒ–åŸºçº¿è¿›è¡Œäº†æ¯”è¾ƒï¼Œå¹¶è§‚å¯Ÿåˆ°å…¶æ€§èƒ½å“è¶Šã€‚ä»£ç å’Œæ•°æ®é›†å°†å¾ˆå¿«å¯ç”¨ã€‚</p>
</blockquote>
<p><strong>ç®€åŒ–ç‰ˆç¿»è¯‘</strong></p>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.00883v1">PDF</a> </p>
<p><strong>Summary</strong><br>     éšç€å¯¹å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰æ¨ç†èƒ½åŠ›çš„å…³æ³¨å¢åŠ ï¼ŒåŸºäºè§†é¢‘çš„è§†è§‰ç©ºé—´æ™ºèƒ½ï¼ˆVSIï¼‰ä½œä¸ºMLLMsåœ¨ç‰©ç†é¢†åŸŸå·¥ä½œçš„æ ¸å¿ƒï¼Œæ˜¾å¾—å°¤ä¸ºé‡è¦ã€‚æœ¬ç ”ç©¶é€šè¿‡R1-Zeroç±»ä¼¼çš„è®­ç»ƒæ–¹å¼ï¼Œæ·±å…¥æ¢è®¨äº†æé«˜MLLMsçš„è§†è§‰ç©ºé—´æ¨ç†èƒ½åŠ›ã€‚ç ”ç©¶å‘ç°ï¼Œä¸­å°å‹Qwen2-VLæ¨¡å‹çš„è§†è§‰ç©ºé—´æ¨ç†èƒ½åŠ›æ— æ³•é€šè¿‡Chain of Thoughtï¼ˆCoTï¼‰æç¤ºæ¿€æ´»ã€‚é€šè¿‡ä½¿ç”¨GRPOè®­ç»ƒå’Œç²¾å¿ƒåˆ¶ä½œçš„VSI-100kæ•°æ®é›†ï¼Œåœ¨æ”¹è¿›è§†è§‰ç©ºé—´æ¨ç†æ–¹é¢å–å¾—äº†è¿›å±•ã€‚ç ”ç©¶è¿˜å¼ºè°ƒäº†ä¿æŒKLæƒ©ç½šçš„å¿…è¦æ€§ï¼Œå³ä½¿å…¶å€¼å¾ˆå°ã€‚ç ”ç©¶ç»“æœæ˜¾ç¤ºï¼Œç»è¿‡GRPOè®­ç»ƒçš„æ¨¡å‹æ€§èƒ½æ˜¾è‘—æå‡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰çš„æ¨ç†èƒ½åŠ›æ—¥ç›Šå—åˆ°é‡è§†ã€‚</li>
<li>åŸºäºè§†é¢‘çš„è§†è§‰ç©ºé—´æ™ºèƒ½ï¼ˆVSIï¼‰æ˜¯MLLMsåœ¨ç‰©ç†é¢†åŸŸå·¥ä½œçš„å…³é”®ã€‚</li>
<li>æœ¬ç ”ç©¶é€šè¿‡ç±»ä¼¼R1-Zeroçš„è®­ç»ƒæ–¹æ³•ï¼Œæ·±å…¥æ¢è®¨äº†æé«˜MLLMsçš„è§†è§‰ç©ºé—´æ¨ç†èƒ½åŠ›ã€‚</li>
<li>ä¸­å°å‹Qwen2-VLæ¨¡å‹çš„è§†è§‰ç©ºé—´æ¨ç†èƒ½åŠ›æ— æ³•é€šè¿‡Chain of Thoughtï¼ˆCoTï¼‰æç¤ºæ¿€æ´»ã€‚</li>
<li>ä½¿ç”¨GRPOè®­ç»ƒå’ŒVSI-100kæ•°æ®é›†æœ‰åŠ©äºæé«˜è§†è§‰ç©ºé—´æ¨ç†èƒ½åŠ›ã€‚</li>
<li>ç ”ç©¶å¼ºè°ƒäº†ä¿æŒKLæƒ©ç½šåœ¨GRPOè®­ç»ƒä¸­çš„å¿…è¦æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.00883">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-a4f989373a810a4e58e173363e7b030e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3a7e3a317fbbffc23c37ea7f7028b3ff.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1bd62ee9b47b3fb9fea30a8eb7911b76.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-1be129ecd7e013f263b75c042aa150e4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b74d3af0c938099ed721303c2e7f0363.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-82fe3dd7e90155b3f9090f9026ffcb3e.jpg" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="m1-Unleash-the-Potential-of-Test-Time-Scaling-for-Medical-Reasoning-with-Large-Language-Models"><a href="#m1-Unleash-the-Potential-of-Test-Time-Scaling-for-Medical-Reasoning-with-Large-Language-Models" class="headerlink" title="m1: Unleash the Potential of Test-Time Scaling for Medical Reasoning   with Large Language Models"></a>m1: Unleash the Potential of Test-Time Scaling for Medical Reasoning   with Large Language Models</h2><p><strong>Authors:Xiaoke Huang, Juncheng Wu, Hui Liu, Xianfeng Tang, Yuyin Zhou</strong></p>
<p>Test-time scaling has emerged as a powerful technique for enhancing the reasoning capabilities of large language models. However, its effectiveness in medical reasoning remains uncertain, as the medical domain fundamentally differs from mathematical tasks in terms of knowledge representation and decision-making processes. In this paper, we provide the first comprehensive investigation of test-time scaling for medical reasoning and present m1, a simple yet effective approach that increases a modelâ€™s medical reasoning capability at inference. Our evaluation across diverse medical tasks demonstrates that test-time scaling consistently enhances medical reasoning, enabling lightweight fine-tuned models under 10B parameters to establish new state-of-the-art performance, while our 32B model rivals previous 70B-scale medical LLMs. However, we identify an optimal reasoning token budget of approximately 4K, beyond which performance may degrade due to overthinking. Budget forcing, which extends test-time computation through iterative prompts, helps models double-check answers but does not necessarily improve the overall medical QA performance and, in some cases, even introduces errors into previously correct responses. Our case-by-case analysis identifies insufficient medical knowledge as a key bottleneck that prevents further performance gains through test-time scaling. We find that increasing data scale, improving data quality, and expanding model capacity consistently enhance medical knowledge grounding, enabling continued performance improvements, particularly on challenging medical benchmarks where smaller models reach saturation. These findings underscore fundamental differences between medical and mathematical reasoning in LLMs, highlighting that enriched medical knowledge, other than increased reasoning depth alone, is essential for realizing the benefits of test-time scaling. </p>
<blockquote>
<p>æµ‹è¯•æ—¶ç¼©æ”¾æŠ€æœ¯å·²æˆä¸ºå¢å¼ºå¤§å‹è¯­è¨€æ¨¡å‹æ¨ç†èƒ½åŠ›çš„ä¸€ç§å¼ºå¤§æŠ€æœ¯ã€‚ç„¶è€Œï¼Œå…¶åœ¨åŒ»ç–—æ¨ç†ä¸­çš„æœ‰æ•ˆæ€§å°šä¸ç¡®å®šï¼Œå› ä¸ºåŒ»ç–—é¢†åŸŸåœ¨çŸ¥è¯†è¡¨è¾¾å’Œå†³ç­–è¿‡ç¨‹æ–¹é¢ä¸æ•°å­¦ä»»åŠ¡å­˜åœ¨æ ¹æœ¬åŒºåˆ«ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬å¯¹åŒ»ç–—æ¨ç†çš„æµ‹è¯•æ—¶ç¼©æ”¾æŠ€æœ¯è¿›è¡Œäº†é¦–æ¬¡å…¨é¢è°ƒæŸ¥ï¼Œå¹¶æå‡ºäº†ä¸€ç§ç®€å•æœ‰æ•ˆçš„m1æ–¹æ³•ï¼Œå¯ä»¥åœ¨æ¨ç†è¿‡ç¨‹ä¸­æé«˜æ¨¡å‹çš„åŒ»ç–—æ¨ç†èƒ½åŠ›ã€‚æˆ‘ä»¬åœ¨å„ç§åŒ»ç–—ä»»åŠ¡ä¸Šçš„è¯„ä¼°è¡¨æ˜ï¼Œæµ‹è¯•æ—¶ç¼©æ”¾æŠ€æœ¯å¯ä»¥æŒç»­å¢å¼ºåŒ»ç–—æ¨ç†èƒ½åŠ›ï¼Œä½¿ç»è¿‡è½»é‡çº§å¾®è°ƒã€å‚æ•°ä¸è¶³10Bçš„æ¨¡å‹èƒ½å¤Ÿåˆ›é€ æ–°çš„æœ€å…ˆè¿›çš„æ€§èƒ½è¡¨ç°ï¼›æˆ‘ä»¬çš„32Bæ¨¡å‹çš„è¡¨ç°åˆ™ä¸å‰äº›å¹´è§„æ¨¡è¾¾åˆ°70Bçš„åŒ»ç–—å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¸ç›¸ä¸Šä¸‹ã€‚ç„¶è€Œï¼Œæˆ‘ä»¬ç¡®å®šäº†å¤§çº¦4Kçš„æœ€ä½³æ¨ç†ä»¤ç‰Œé¢„ç®—ï¼Œè¶…å‡ºæ­¤é¢„ç®—åï¼Œç”±äºè¿‡åº¦æ€è€ƒï¼Œæ€§èƒ½å¯èƒ½ä¼šä¸‹é™ã€‚å¼ºåˆ¶é¢„ç®—é€šè¿‡è¿­ä»£æç¤ºå»¶é•¿æµ‹è¯•æ—¶çš„è®¡ç®—æ—¶é—´ï¼Œæœ‰åŠ©äºæ¨¡å‹æ£€æŸ¥ç­”æ¡ˆï¼Œä½†å¹¶ä¸ä¸€å®šä¼šæé«˜æ•´ä½“çš„åŒ»ç–—é—®ç­”è¡¨ç°ï¼Œæœ‰æ—¶ç”šè‡³ä¼šå°†ä¹‹å‰æ­£ç¡®çš„ç­”æ¡ˆå¼•å…¥é”™è¯¯ã€‚æˆ‘ä»¬çš„é€æ¡ˆåˆ†æå‘ç°ï¼ŒåŒ»ç–—çŸ¥è¯†ä¸è¶³æ˜¯ä¸€ä¸ªå…³é”®ç“¶é¢ˆï¼Œé˜»ç¢äº†é€šè¿‡æµ‹è¯•æ—¶ç¼©æ”¾è¿›ä¸€æ­¥æå‡æ€§èƒ½ã€‚æˆ‘ä»¬å‘ç°ï¼Œé€šè¿‡å¢åŠ æ•°æ®è§„æ¨¡ã€æé«˜æ•°æ®è´¨é‡å’Œæ‰©å¤§æ¨¡å‹å®¹é‡ï¼Œå¯ä»¥æŒç»­å¢å¼ºåŒ»ç–—çŸ¥è¯†çš„ä¾æ®æ€§ï¼Œä»è€Œåœ¨å…·æœ‰æŒ‘æˆ˜æ€§çš„åŒ»ç–—åŸºå‡†æµ‹è¯•ä¸Šå®ç°æŒç»­çš„æ€§èƒ½æ”¹è¿›ï¼Œå°¤å…¶æ˜¯å°å‹æ¨¡å‹è¾¾åˆ°é¥±å’ŒçŠ¶æ€çš„æƒ…å†µä¸‹æ›´æ˜¯å¦‚æ­¤ã€‚è¿™äº›å‘ç°å¼ºè°ƒäº†å¤§å‹è¯­è¨€æ¨¡å‹ä¸­åŒ»ç–—æ¨ç†ä¸æ•°å­¦æ¨ç†ä¹‹é—´çš„æ ¹æœ¬åŒºåˆ«ï¼Œå¹¶æŒ‡å‡ºé™¤äº†å¢åŠ æ¨ç†æ·±åº¦å¤–ï¼Œä¸°å¯Œçš„åŒ»ç–—çŸ¥è¯†å¯¹äºå®ç°æµ‹è¯•æ—¶ç¼©æ”¾çš„å¥½å¤„è‡³å…³é‡è¦ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.00869v1">PDF</a> 17 pages; 7 figures; Data, code, and models:   <a target="_blank" rel="noopener" href="https://github.com/UCSC-VLAA/m1">https://github.com/UCSC-VLAA/m1</a></p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ¢ç´¢äº†æµ‹è¯•æ—¶ç¼©æ”¾æŠ€æœ¯åœ¨åŒ»ç–—æ¨ç†ä¸­çš„åº”ç”¨ã€‚ç ”ç©¶æå‡ºäº†ä¸€ç§ç®€å•æœ‰æ•ˆçš„æ–¹æ³•m1ï¼Œå¯ä»¥æé«˜æ¨¡å‹åœ¨æ¨ç†é˜¶æ®µçš„åŒ»ç–—æ¨ç†èƒ½åŠ›ã€‚è¯„ä¼°è¡¨æ˜ï¼Œæµ‹è¯•æ—¶ç¼©æ”¾æŠ€æœ¯èƒ½æŒç»­æé«˜åŒ»ç–—æ¨ç†èƒ½åŠ›ï¼Œä½¿è½»é‡çº§å¾®è°ƒæ¨¡å‹è¾¾åˆ°æœ€æ–°æ€§èƒ½æ°´å¹³ã€‚ç„¶è€Œï¼Œç ”ç©¶è¿˜å‘ç°å­˜åœ¨ä¸€ä¸ªçº¦4Kçš„æ¨ç†ä»¤ç‰Œé¢„ç®—æœ€ä¼˜å€¼ï¼Œè¶…å‡ºæ­¤èŒƒå›´å¯èƒ½ä¼šå¯¼è‡´æ€§èƒ½ä¸‹é™ã€‚é¢„ç®—å¼ºåˆ¶æœ‰åŠ©äºæ¨¡å‹è¿›è¡Œç­”æ¡ˆå¤æŸ¥ï¼Œä½†ä¸ä¸€å®šèƒ½æé«˜æ•´ä½“åŒ»ç–—é—®ç­”æ€§èƒ½ï¼Œç”šè‡³å¯èƒ½å¯¼è‡´å…ˆå‰æ­£ç¡®çš„ç­”æ¡ˆå‡ºç°é”™è¯¯ã€‚ç ”ç©¶è®¤ä¸ºï¼ŒåŒ»ç–—çŸ¥è¯†ä¸è¶³æ˜¯é˜»ç¢æ€§èƒ½è¿›ä¸€æ­¥æé«˜çš„å…³é”®ç“¶é¢ˆã€‚å¢åŠ æ•°æ®è§„æ¨¡ã€æé«˜æ•°æ®è´¨é‡å’Œæ‰©å¤§æ¨¡å‹å®¹é‡èƒ½æŒç»­æé«˜åŒ»ç–—çŸ¥è¯†çš„å®šä½èƒ½åŠ›ï¼Œç‰¹åˆ«æ˜¯åœ¨æŒ‘æˆ˜æ€§åŒ»ç–—åŸºå‡†æµ‹è¯•ä¸­ï¼Œå°å‹æ¨¡å‹è¾¾åˆ°é¥±å’ŒçŠ¶æ€åæ•ˆæœæ›´æ˜æ˜¾ã€‚è¿™äº›å‘ç°å¼ºè°ƒäº†åŒ»ç–—å’Œæ•°å­¦æ¨ç†åœ¨å¤§å‹è¯­è¨€æ¨¡å‹ä¸­çš„æ ¹æœ¬åŒºåˆ«ï¼Œå¹¶æŒ‡å‡ºé™¤äº†å¢åŠ æ¨ç†æ·±åº¦å¤–ï¼Œä¸°å¯Œçš„åŒ»ç–—çŸ¥è¯†å¯¹äºå®ç°æµ‹è¯•æ—¶ç¼©æ”¾æŠ€æœ¯çš„ä¼˜åŠ¿è‡³å…³é‡è¦ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æµ‹è¯•æ—¶ç¼©æ”¾æŠ€æœ¯ç”¨äºæé«˜å¤§å‹è¯­è¨€æ¨¡å‹åœ¨åŒ»ç–—æ¨ç†ä¸­çš„èƒ½åŠ›ã€‚</li>
<li>m1æ–¹æ³•èƒ½æœ‰æ•ˆæå‡æ¨¡å‹çš„åŒ»ç–—æ¨ç†èƒ½åŠ›ã€‚</li>
<li>æµ‹è¯•æ—¶ç¼©æ”¾æŠ€æœ¯èƒ½ä½¿è½»é‡çº§æ¨¡å‹è¾¾åˆ°æœ€æ–°åŒ»ç–—æ¨ç†æ€§èƒ½æ°´å¹³ã€‚</li>
<li>å­˜åœ¨ä¸€ä¸ªçº¦4Kçš„æ¨ç†ä»¤ç‰Œé¢„ç®—æœ€ä¼˜å€¼ï¼Œè¶…å‡ºæ­¤èŒƒå›´å¯èƒ½å¯¼è‡´æ€§èƒ½ä¸‹é™ã€‚</li>
<li>é¢„ç®—å¼ºåˆ¶æœ‰åŠ©äºæ¨¡å‹å¤æŸ¥ç­”æ¡ˆï¼Œä½†å¯èƒ½ä¸æ€»èƒ½æå‡åŒ»ç–—é—®ç­”æ€§èƒ½ã€‚</li>
<li>åŒ»ç–—çŸ¥è¯†ä¸è¶³æ˜¯é˜»ç¢è¿›ä¸€æ­¥æé«˜åŒ»ç–—æ¨ç†æ€§èƒ½çš„å…³é”®é—®é¢˜ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.00869">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-cfd5d5f354c61e48ba3907c2d0b033af.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3cc5bb6b764ccd862c0bdc746a0e8f3c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-46882d4b085eb516211e89a4b08ee922.jpg" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1><h2 id="DynMoLE-Boosting-Mixture-of-LoRA-Experts-Fine-Tuning-with-a-Hybrid-Routing-Mechanism"><a href="#DynMoLE-Boosting-Mixture-of-LoRA-Experts-Fine-Tuning-with-a-Hybrid-Routing-Mechanism" class="headerlink" title="DynMoLE: Boosting Mixture of LoRA Experts Fine-Tuning with a Hybrid   Routing Mechanism"></a>DynMoLE: Boosting Mixture of LoRA Experts Fine-Tuning with a Hybrid   Routing Mechanism</h2><p><strong>Authors:Dengchun Li, Naizheng Wang, Zihao Zhang, Haoyang Yin, Lei Duan, Meng Xiao, Mingjie Tang</strong></p>
<p>Instruction-based fine-tuning of large language models (LLMs) has achieved remarkable success in various natural language processing (NLP) tasks. Parameter-efficient fine-tuning (PEFT) methods, such as Mixture of LoRA Experts (MoLE), combine the efficiency of Low-Rank Adaptation (LoRA) with the versatility of Mixture of Experts (MoE) models, demonstrating significant potential for handling multiple downstream tasks. However, the existing routing mechanisms for MoLE often involve a trade-off between computational efficiency and predictive accuracy, and they fail to fully address the diverse expert selection demands across different transformer layers. In this work, we propose DynMoLE, a hybrid routing strategy that dynamically adjusts expert selection based on the Tsallis entropy of the routerâ€™s probability distribution. This approach mitigates router uncertainty, enhances stability, and promotes more equitable expert participation, leading to faster convergence and improved model performance. Additionally, we introduce an auxiliary loss based on Tsallis entropy to further guide the model toward convergence with reduced uncertainty, thereby improving training stability and performance. Our extensive experiments on commonsense reasoning benchmarks demonstrate that DynMoLE achieves substantial performance improvements, outperforming LoRA by 9.6% and surpassing the state-of-the-art MoLE method, MoLA, by 2.3%. We also conduct a comprehensive ablation study to evaluate the contributions of DynMoLEâ€™s key components. </p>
<blockquote>
<p>åŸºäºæŒ‡ä»¤çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å¾®è°ƒåœ¨è‡ªç„¶è¯­è¨€å¤„ç†ï¼ˆNLPï¼‰çš„å„ç§ä»»åŠ¡ä¸­å–å¾—äº†æ˜¾è‘—çš„æˆåŠŸã€‚å‚æ•°é«˜æ•ˆå¾®è°ƒï¼ˆPEFTï¼‰æ–¹æ³•ï¼Œå¦‚æ··åˆLoRAä¸“å®¶ï¼ˆMoLEï¼‰ï¼Œç»“åˆäº†ä½ç§©é€‚åº”ï¼ˆLoRAï¼‰çš„æ•ˆç‡ä¸æ··åˆä¸“å®¶ï¼ˆMoEï¼‰æ¨¡å‹çš„é€šç”¨æ€§ï¼Œåœ¨å¤„ç†å¤šä¸ªä¸‹æ¸¸ä»»åŠ¡æ–¹é¢æ˜¾ç¤ºå‡ºå·¨å¤§æ½œåŠ›ã€‚ç„¶è€Œï¼ŒMoLEçš„ç°æœ‰è·¯ç”±æœºåˆ¶é€šå¸¸åœ¨è®¡ç®—æ•ˆç‡å’Œé¢„æµ‹å‡†ç¡®æ€§ä¹‹é—´è¿›è¡Œæƒè¡¡ï¼Œå¹¶ä¸”æœªèƒ½å®Œå…¨æ»¡è¶³ä¸åŒtransformerå±‚ä¹‹é—´å¤šæ ·åŒ–çš„ä¸“å®¶é€‰æ‹©éœ€æ±‚ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†DynMoLEï¼Œä¸€ç§åŸºäºè·¯ç”±å™¨æ¦‚ç‡åˆ†å¸ƒçš„Tsallisç†µåŠ¨æ€è°ƒæ•´ä¸“å®¶é€‰æ‹©çš„æ··åˆè·¯ç”±ç­–ç•¥ã€‚è¿™ç§æ–¹æ³•å‡è½»äº†è·¯ç”±å™¨çš„ä¸ç¡®å®šæ€§ï¼Œå¢å¼ºäº†ç¨³å®šæ€§ï¼Œä¿ƒè¿›äº†æ›´å…¬å¹³çš„ä¸“å®¶å‚ä¸ï¼Œä»è€Œå®ç°äº†æ›´å¿«çš„æ”¶æ•›å’Œæ¨¡å‹æ€§èƒ½çš„æé«˜ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§åŸºäºTsallisç†µçš„è¾…åŠ©æŸå¤±ï¼Œè¿›ä¸€æ­¥å¼•å¯¼æ¨¡å‹æœç€å‡å°‘ä¸ç¡®å®šæ€§çš„æ–¹å‘æ”¶æ•›ï¼Œä»è€Œæé«˜äº†è®­ç»ƒç¨³å®šæ€§å’Œæ€§èƒ½ã€‚æˆ‘ä»¬åœ¨å¸¸è¯†æ¨ç†åŸºå‡†æµ‹è¯•ä¸Šçš„å¤§é‡å®éªŒè¡¨æ˜ï¼ŒDynMoLEå®ç°äº†æ˜¾è‘—çš„æ€§èƒ½æ”¹è¿›ï¼Œæ¯”LoRAé«˜å‡º9.6%ï¼Œå¹¶è¶…è¶Šäº†æœ€å…ˆè¿›çš„MoLEæ–¹æ³•MoLAï¼Œæé«˜äº†2.3%ã€‚æˆ‘ä»¬è¿˜è¿›è¡Œäº†å…¨é¢çš„æ¶ˆèç ”ç©¶ï¼Œä»¥è¯„ä¼°DynMoLEå…³é”®ç»„ä»¶çš„è´¡çŒ®ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.00661v1">PDF</a> 22 pages, 7 figures</p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æŒ‡ä»¤å¾®è°ƒå·²ç»å–å¾—æ˜¾è‘—æˆåŠŸã€‚å‚æ•°é«˜æ•ˆå¾®è°ƒï¼ˆPEFTï¼‰æ–¹æ³•å¦‚æ··åˆLoRAä¸“å®¶ï¼ˆMoLEï¼‰ç»“åˆäº†LoRAçš„æ•ˆç‡ä¸MoEæ¨¡å‹çš„çµæ´»æ€§ã€‚ç„¶è€Œï¼Œç°æœ‰MoLEçš„è·¯ç”±æœºåˆ¶åœ¨è®¡ç®—æ•ˆç‡å’Œé¢„æµ‹ç²¾åº¦ä¹‹é—´å­˜åœ¨æƒè¡¡ï¼Œä¸èƒ½æ»¡è¶³ä¸åŒtransformerå±‚ä¹‹é—´çš„ä¸“å®¶é€‰æ‹©éœ€æ±‚ã€‚æœ¬ç ”ç©¶æå‡ºDynMoLEï¼Œä¸€ç§åŸºäºTsallisç†µçš„è·¯ç”±å™¨æ¦‚ç‡åˆ†å¸ƒåŠ¨æ€è°ƒæ•´ä¸“å®¶é€‰æ‹©çš„æ··åˆè·¯ç”±ç­–ç•¥ã€‚è¯¥æ–¹æ³•å‡è½»äº†è·¯ç”±å™¨çš„ä¸ç¡®å®šæ€§ï¼Œå¢å¼ºäº†ç¨³å®šæ€§ï¼Œä¿ƒè¿›äº†æ›´å…¬å¹³çš„ä¸“å®¶å‚ä¸ï¼Œå¯¼è‡´æ›´å¿«çš„æ”¶æ•›å’Œæ¨¡å‹æ€§èƒ½çš„æå‡ã€‚åœ¨å¸¸è¯†æ¨ç†åŸºå‡†æµ‹è¯•ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒDynMoLEå®ç°äº†æ˜¾è‘—çš„æ€§èƒ½æ”¹è¿›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æŒ‡ä»¤å¾®è°ƒåœ¨NLPä»»åŠ¡ä¸­å–å¾—æ˜¾è‘—æˆåŠŸã€‚</li>
<li>å‚æ•°é«˜æ•ˆå¾®è°ƒï¼ˆPEFTï¼‰æ–¹æ³•ç»“åˆäº†LoRAçš„æ•ˆç‡ä¸MoEæ¨¡å‹çš„çµæ´»æ€§ã€‚</li>
<li>MoLEçš„ç°æœ‰è·¯ç”±æœºåˆ¶å­˜åœ¨è®¡ç®—æ•ˆç‡å’Œé¢„æµ‹ç²¾åº¦ä¹‹é—´çš„æƒè¡¡ã€‚</li>
<li>DynMoLEæå‡ºä¸€ç§åŸºäºTsallisç†µçš„æ··åˆè·¯ç”±ç­–ç•¥ï¼ŒåŠ¨æ€è°ƒæ•´ä¸“å®¶é€‰æ‹©ã€‚</li>
<li>DynMoLEå‡è½»äº†è·¯ç”±å™¨çš„ä¸ç¡®å®šæ€§ï¼Œå¢å¼ºäº†ç¨³å®šæ€§å’Œæ¨¡å‹æ€§èƒ½ã€‚</li>
<li>DynMoLEåœ¨å¸¸è¯†æ¨ç†ä»»åŠ¡ä¸Šå®ç°äº†æ˜¾è‘—çš„æ€§èƒ½æ”¹è¿›ï¼Œä¼˜äºLoRAå’Œå…¶ä»–æœ€æ–°æ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.00661">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-f988dac6d1b1da6d676703bc99001671.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-6bd628a8f16fc35bf72b375c066ff9ca.jpg" align="middle">
</details>


<h1 id="-19"><a href="#-19" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-04-03/R1_Reasoning/">https://kedreamix.github.io/Talk2Paper/Paper/2025-04-03/R1_Reasoning/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/R1-Reasoning/">
                                    <span class="chip bg-color">R1_Reasoning</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-04-03/LLM/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-e673eac368ee8357e01903864dfaf4db.jpg" class="responsive-img" alt="LLM">
                        
                        <span class="card-title">LLM</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            LLM æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-04-03  Towards Unified Referring Expression Segmentation Across Omni-Level   Visual Target Granularities
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-04-03
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/LLM/" class="post-category">
                                    LLM
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/LLM/">
                        <span class="chip bg-color">LLM</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-03-29/3DGS/">
                    <div class="card-image">
                        
                        <img src="https://pica.zhimg.com/v2-8b3e8b0158a01167bf17e6207de84052.jpg" class="responsive-img" alt="3DGS">
                        
                        <span class="card-title">3DGS</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            3DGS æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-03-29  X$^{2}$-Gaussian 4D Radiative Gaussian Splatting for Continuous-time   Tomographic Reconstruction
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-03-29
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/3DGS/" class="post-category">
                                    3DGS
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/3DGS/">
                        <span class="chip bg-color">3DGS</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">16042k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
