<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="åŒ»å­¦å›¾åƒ">
    <meta name="description" content="åŒ»å­¦å›¾åƒ æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-04-04  BioAtt Anatomical Prior Driven Low-Dose CT Denoising">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>åŒ»å­¦å›¾åƒ | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pica.zhimg.com/v2-cb3b6f3635d1eecc5dacd6980ba7b48c.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">åŒ»å­¦å›¾åƒ</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                                <span class="chip bg-color">åŒ»å­¦å›¾åƒ</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/" class="post-category">
                                åŒ»å­¦å›¾åƒ
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-04-04
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-05-14
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    19.5k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    80 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-04-04-æ›´æ–°"><a href="#2025-04-04-æ›´æ–°" class="headerlink" title="2025-04-04 æ›´æ–°"></a>2025-04-04 æ›´æ–°</h1><h2 id="BioAtt-Anatomical-Prior-Driven-Low-Dose-CT-Denoising"><a href="#BioAtt-Anatomical-Prior-Driven-Low-Dose-CT-Denoising" class="headerlink" title="BioAtt: Anatomical Prior Driven Low-Dose CT Denoising"></a>BioAtt: Anatomical Prior Driven Low-Dose CT Denoising</h2><p><strong>Authors:Namhun Kim, UiHyun Cho</strong></p>
<p>Deep-learning-based denoising methods have significantly improved Low-Dose CT (LDCT) image quality. However, existing models often over-smooth important anatomical details due to their purely data-driven attention mechanisms. To address this challenge, we propose a novel LDCT denoising framework, BioAtt. The key innovation lies in attending anatomical prior distributions extracted from the pretrained vision-language model BiomedCLIP. These priors guide the denoising model to focus on anatomically relevant regions to suppress noise while preserving clinically relevant structures. We highlight three main contributions: BioAtt outperforms baseline and attention-based models in SSIM, PSNR, and RMSE across multiple anatomical regions. The framework introduces a new architectural paradigm by embedding anatomic priors directly into spatial attention. Finally, BioAtt attention maps provide visual confirmation that the improvements stem from anatomical guidance rather than increased model complexity. </p>
<blockquote>
<p>åŸºäºæ·±åº¦å­¦ä¹ çš„é™å™ªæ–¹æ³•å·²ç»æ˜¾è‘—æé«˜äº†ä½å‰‚é‡CTï¼ˆLDCTï¼‰å›¾åƒçš„è´¨é‡ã€‚ç„¶è€Œï¼Œç°æœ‰æ¨¡å‹ç”±äºå…¶çº¯ç²¹çš„æ•°æ®é©±åŠ¨æ³¨æ„åŠ›æœºåˆ¶ï¼Œå¾€å¾€ä¼šè¿‡åº¦å¹³æ»‘é‡è¦çš„è§£å‰–ç»†èŠ‚ã€‚ä¸ºäº†åº”å¯¹è¿™ä¸€æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°å‹çš„LDCTé™å™ªæ¡†æ¶BioAttã€‚å…³é”®åˆ›æ–°ä¹‹å¤„åœ¨äºå…³æ³¨ç”±é¢„è®­ç»ƒçš„è§†è§‰è¯­è¨€æ¨¡å‹BiomedCLIPæå–çš„è§£å‰–å…ˆéªŒåˆ†å¸ƒã€‚è¿™äº›å…ˆéªŒçŸ¥è¯†å¼•å¯¼é™å™ªæ¨¡å‹å…³æ³¨è§£å‰–ç›¸å…³çš„åŒºåŸŸï¼Œä»¥æŠ‘åˆ¶å™ªå£°çš„åŒæ—¶ä¿ç•™ä¸´åºŠä¸Šç›¸å…³çš„ç»“æ„ã€‚æˆ‘ä»¬å¼ºè°ƒä¸‰ä¸ªä¸»è¦è´¡çŒ®ï¼šBioAttåœ¨å¤šä¸ªè§£å‰–åŒºåŸŸçš„SSIMã€PSNRå’ŒRMSEæ–¹é¢ä¼˜äºåŸºå‡†æ¨¡å‹å’ŒåŸºäºæ³¨æ„åŠ›çš„æ¨¡å‹ã€‚è¯¥æ¡†æ¶é€šè¿‡ç›´æ¥å°†è§£å‰–å…ˆéªŒåµŒå…¥ç©ºé—´æ³¨æ„åŠ›ï¼Œå¼•å…¥äº†ä¸€ç§æ–°çš„æ¶æ„èŒƒå¼ã€‚æœ€åï¼ŒBioAttçš„æ³¨æ„åŠ›å›¾è§†è§‰ç¡®è®¤æ”¹è¿›æºäºè§£å‰–æŒ‡å¯¼è€Œéæ¨¡å‹å¤æ‚åº¦çš„å¢åŠ ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.01662v1">PDF</a> 14 pages</p>
<p><strong>Summary</strong><br>æ·±åº¦å­¦ä¹ åœ¨åŒ»å­¦å›¾åƒå»å™ªæ–¹é¢å–å¾—æ˜¾è‘—è¿›å±•ï¼Œç‰¹åˆ«æ˜¯åœ¨ä½å‰‚é‡CTï¼ˆLDCTï¼‰å›¾åƒå»å™ªä¸­ã€‚ç„¶è€Œï¼Œç°æœ‰æ¨¡å‹å› è¿‡äºä¾èµ–æ•°æ®é©±åŠ¨è€Œå¿½è§†äº†è§£å‰–å­¦ç»†èŠ‚ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„LDCTå»å™ªæ¡†æ¶BioAttï¼Œé€šè¿‡å¼•å…¥è§£å‰–å­¦å…ˆéªŒä¿¡æ¯æé«˜å»å™ªæ•ˆæœï¼Œå¹¶ä¼˜åŒ–ç»“æ„ç›¸ä¼¼æ€§æŒ‡æ ‡ï¼ˆSSIMï¼‰ã€å³°å€¼ä¿¡å™ªæ¯”ï¼ˆPSNRï¼‰å’Œå‡æ–¹æ ¹è¯¯å·®ï¼ˆRMSEï¼‰ã€‚æ–°æ¶æ„çš„èŒƒå¼åŒ–ä»¥åŠé€šè¿‡è§†è§‰ç¡®è®¤çš„è§£å‰–å­¦æŒ‡å¯¼æ”¹è¿›æå‡äº†æ¨¡å‹æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>æ·±åº¦å­¦ä¹ åœ¨å»å™ªé¢†åŸŸå–å¾—æ˜¾è‘—è¿›å±•ï¼Œç‰¹åˆ«æ˜¯åœ¨LDCTå›¾åƒå»å™ªä¸­ã€‚</li>
<li>ç°æœ‰æ¨¡å‹è¿‡åº¦ä¾èµ–æ•°æ®é©±åŠ¨ï¼Œå®¹æ˜“å¿½è§†è§£å‰–å­¦ç»†èŠ‚ï¼Œå¯¼è‡´è¿‡åº¦å¹³æ»‘ç°è±¡ã€‚</li>
<li>BioAttæ¡†æ¶å¼•å…¥è§£å‰–å­¦å…ˆéªŒä¿¡æ¯ä½œä¸ºæ ¸å¿ƒåˆ›æ–°ç‚¹ï¼Œé€šè¿‡ç»“åˆé¢„è®­ç»ƒçš„è§†è§‰è¯­è¨€æ¨¡å‹BiomedCLIPæå–å…ˆéªŒåˆ†å¸ƒä¿¡æ¯ã€‚</li>
<li>BioAttæ¡†æ¶åœ¨å¤šä¸ªè§£å‰–å­¦åŒºåŸŸçš„è¡¨ç°ä¼˜äºåŸºå‡†æ¨¡å‹å’ŒåŸºäºæ³¨æ„åŠ›çš„æ¨¡å‹ï¼Œæé«˜äº†SSIMã€PSNRå’ŒRMSEç­‰æŒ‡æ ‡ã€‚</li>
<li>BioAttå¼•å…¥äº†ä¸€ç§æ–°æ¶æ„èŒƒå¼ï¼Œé€šè¿‡ç›´æ¥å°†è§£å‰–å­¦å…ˆéªŒåµŒå…¥ç©ºé—´æ³¨æ„åŠ›æœºåˆ¶ä¸­ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.01662">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-8a7cc805b5049b86ac65265602e45fd3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-aa46b1b6ab43ebfed1df90218c0f21c2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b3d4b04863d1ff4c813b46c1e221b61f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-50d71440fa6257a1787ffafda16f81d4.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-f813b686e7ca6927c1fbe967666d0ca9.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-764c107b1109e223c09e5c1c2e20bd2f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5a26333241195175946fc1d0dd291450.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="KTaO3-001-Preparation-Methods-in-Vacuum-Effects-on-Surface-Stoichiometry-Crystallography-and-in-gap-States"><a href="#KTaO3-001-Preparation-Methods-in-Vacuum-Effects-on-Surface-Stoichiometry-Crystallography-and-in-gap-States" class="headerlink" title="KTaO3(001) Preparation Methods in Vacuum: Effects on Surface   Stoichiometry, Crystallography, and in-gap States"></a>KTaO3(001) Preparation Methods in Vacuum: Effects on Surface   Stoichiometry, Crystallography, and in-gap States</h2><p><strong>Authors:Andrea M. Lucero Manzano, Esteban D. Cantero, Emanuel A. MartÃ­nez, F. Y. Bruno, Esteban A. SÃ¡nchez, Oscar Grizzi</strong></p>
<p>KTaO3 single crystals with different orientations are used as substrates for the epitaxial growth of thin films and&#x2F;or as hosts for two-dimensional electron gases. Due to the polar nature of the KTaO3(001) surface, one can expect difficulties and challenges to arise in its preparation. Maintaining good insulating characteristics without adding undesirable in-gap electronic states, obtaining good crystalline order up to the top surface layer, a sufficiently flat surface, and complete cleanliness of the surface (without water, C or OH contaminants), are in general difficult conditions to accomplish simultaneously. Cleaving in vacuum is likely the best option for obtaining a clean surface. However, since KTaO3 is cubic and lacks a well-defined cleavage plane, this method is notsuitable for sample growth or reproducible device fabrication. Here, we systematically evaluate the effect of typical preparation methods applied on the surfaces of KTaO3(001) single crystals. In particular, we used annealing in vacuum at different temperatures, light sputtering with Ar+ ions at low energy (500 eV) followed by annealing, heavy Ar+ ion bombardment and annealing, and grazing Ar+ ion bombardment under continuous azimuthal rotation combined with both annealing in vacuum and in O2 atmosphere. Possible side effects after each treatment are evaluated by a combination of techniques, including low-energy ion scattering at forward angles, Auger electron spectroscopy, low-energy electron energy loss, X-ray photoelectron spectroscopy, low-energy electron diffraction, and time of flightsecondary ion mass spectrometry. Advantages and shortcomings of each preparation method are discussed in detail. </p>
<blockquote>
<p>å…·æœ‰ä¸åŒå–å‘çš„KTaO3å•æ™¶è¢«ç”¨ä½œå¤–å»¶ç”Ÿé•¿è–„è†œçš„åŸºç‰‡æˆ–äºŒç»´ç”µå­æ°”ä½“çš„å®¿ä¸»ã€‚ç”±äºKTaO3(001)è¡¨é¢çš„ææ€§ç‰¹æ€§ï¼Œå…¶åˆ¶å¤‡è¿‡ç¨‹ä¸­å¯èƒ½ä¼šé‡åˆ°å›°éš¾å’ŒæŒ‘æˆ˜ã€‚åœ¨ä¿æŒä¼˜è‰¯çš„ç»ç¼˜æ€§èƒ½è€Œä¸å¢åŠ ä¸å¿…è¦çš„å¸¦å†…ç”µå­æ€çš„åŒæ—¶ï¼Œè¿˜éœ€è¦ä¿æŒè‰¯å¥½çš„æ™¶ä½“æœ‰åºæ€§è‡³é¡¶å±‚è¡¨é¢ã€è¡¨é¢å¹³å¦æ€§å’Œæ¸…æ´åº¦ï¼ˆæ— æ°´åˆ†ã€ç¢³æˆ–ç¾ŸåŸºæ±¡æŸ“ç‰©ï¼‰ã€‚ä¸€èˆ¬æ¥è¯´ï¼ŒåŒæ—¶å®ç°è¿™äº›æ¡ä»¶æ˜¯éå¸¸å›°éš¾çš„ã€‚çœŸç©ºä¸‹çš„åŠˆè£‚å¯èƒ½æ˜¯è·å¾—æ¸…æ´è¡¨é¢çš„æœ€ä½³æ–¹æ³•ã€‚ç„¶è€Œï¼Œç”±äºKTaO3æ˜¯ç«‹æ–¹çš„å¹¶ä¸”ç¼ºä¹æ˜ç¡®çš„åŠˆè£‚å¹³é¢ï¼Œè¿™ç§æ–¹æ³•ä¸é€‚ç”¨äºæ ·å“ç”Ÿé•¿æˆ–å¯é‡å¤çš„å™¨ä»¶åˆ¶é€ ã€‚åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬ç³»ç»Ÿåœ°è¯„ä¼°äº†å…¸å‹çš„åˆ¶å¤‡æ–¹æ³•å¯¹KTaO3(001)å•æ™¶è¡¨é¢çš„å½±å“ã€‚ç‰¹åˆ«æ˜¯æˆ‘ä»¬é€šè¿‡çœŸç©ºé€€ç«å¤„ç†ä¸åŒæ¸©åº¦ä¸‹çš„æ ·å“ã€ä½èƒ½é‡ï¼ˆ500 eVï¼‰çš„æ°©ç¦»å­æº…å°„åç»“åˆé€€ç«å¤„ç†ã€é‡æ°©ç¦»å­è½°å‡»å’Œé€€ç«å¤„ç†ä»¥åŠè¿ç»­æ–¹ä½æ—‹è½¬ä¸‹çš„å€¾æ–œæ°©ç¦»å­è½°å‡»ç»“åˆçœŸç©ºå’Œæ°§æ°”ç¯å¢ƒä¸‹çš„é€€ç«å¤„ç†ç­‰æ–¹æ³•ã€‚é€šè¿‡ä½èƒ½ç¦»å­å‰å‘æ•£å°„ã€ä¿„æ­‡ç”µå­å…‰è°±ã€ä½èƒ½ç”µå­èƒ½é‡æŸå¤±ã€Xå°„çº¿å…‰ç”µå­å…‰è°±ã€ä½èƒ½ç”µå­è¡å°„ä»¥åŠé£è¡Œæ—¶é—´äºŒæ¬¡ç¦»å­è´¨è°±ç­‰æŠ€æœ¯ç»„åˆæ¥è¯„ä¼°æ¯ç§å¤„ç†åçš„å¯èƒ½å‰¯ä½œç”¨ã€‚æ¯ç§åˆ¶å¤‡æ–¹æ³•çš„ä¼˜ç¼ºç‚¹éƒ½è¿›è¡Œäº†è¯¦ç»†è®¨è®ºã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.01590v1">PDF</a> 28 pages, 8 figures</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ç ”ç©¶äº†ä¸åŒé¢„å¤„ç†æ–¹æ³•å¯¹KTaO3(001)å•æ™¶è¡¨é¢æ€§èƒ½çš„å½±å“ï¼Œæ¢è®¨äº†å„ç§é¢„å¤„ç†æ–¹æ³•çš„ä¼˜ç¼ºç‚¹ï¼ŒåŒ…æ‹¬çœŸç©ºé€€ç«ã€ä½èƒ½ç¦»å­è½°å‡»ç­‰å¤„ç†æ–¹æ³•ã€‚æ–‡ç« é€šè¿‡ä¸€ç³»åˆ—æŠ€æœ¯æ‰‹æ®µå¯¹å¤„ç†åçš„è¡¨é¢è¿›è¡Œè¯„ä¼°ï¼Œè®¨è®ºäº†å„ç§æ–¹æ³•çš„ä¼˜åŠ¿å’Œå±€é™æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>KTaO3å•æ™¶å› å…¶ææ€§è¡¨é¢ç‰¹æ€§ï¼Œåœ¨åˆ¶å¤‡è¿‡ç¨‹ä¸­é¢ä¸´è¯¸å¤šæŒ‘æˆ˜ã€‚</li>
<li>æ–‡ä¸­è¯„ä¼°äº†å¤šç§è¡¨é¢å¤„ç†æ–¹æ³•å¯¹KTaO3(001)å•æ™¶çš„å½±å“ã€‚</li>
<li>çœŸç©ºé€€ç«æ˜¯è·å¾—å¹²å‡€è¡¨é¢çš„å¯èƒ½æ–¹æ³•ï¼Œä½†ç”±äºKTaO3çš„ç«‹æ–¹ç»“æ„ï¼Œè¯¥æ–¹æ³•ä¸é€‚ç”¨äºæ ·å“ç”Ÿé•¿å’Œå¯é‡å¤çš„å™¨ä»¶åˆ¶é€ ã€‚</li>
<li>ä½èƒ½ç¦»å­è½°å‡»å’Œé€€ç«ç­‰å¤„ç†æ–¹æ³•è¢«ç”¨äºæ”¹å–„KTaO3çš„è¡¨é¢æ€§èƒ½ã€‚</li>
<li>å„ç§å¤„ç†æŠ€æœ¯å¯¹è¡¨é¢çš„å¯èƒ½å½±å“é€šè¿‡ä¸€ç³»åˆ—æŠ€æœ¯æ‰‹æ®µè¿›è¡Œè¯„ä¼°ã€‚</li>
<li>æ–‡ç« è¯¦ç»†è®¨è®ºäº†å„ç§é¢„å¤„ç†æ–¹æ³•çš„ä¼˜åŠ¿å’Œå±€é™æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.01590">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-3c244ecdc7b14f8d49f85e2f938574c8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e5fc7da10581e940c2c1cc1677f4a78e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c06c957fefeaedfb046a5ad47fd713e9.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="STPNet-Scale-aware-Text-Prompt-Network-for-Medical-Image-Segmentation"><a href="#STPNet-Scale-aware-Text-Prompt-Network-for-Medical-Image-Segmentation" class="headerlink" title="STPNet: Scale-aware Text Prompt Network for Medical Image Segmentation"></a>STPNet: Scale-aware Text Prompt Network for Medical Image Segmentation</h2><p><strong>Authors:Dandan Shan, Zihan Li, Yunxiang Li, Qingde Li, Jie Tian, Qingqi Hong</strong></p>
<p>Accurate segmentation of lesions plays a critical role in medical image analysis and diagnosis. Traditional segmentation approaches that rely solely on visual features often struggle with the inherent uncertainty in lesion distribution and size. To address these issues, we propose STPNet, a Scale-aware Text Prompt Network that leverages vision-language modeling to enhance medical image segmentation. Our approach utilizes multi-scale textual descriptions to guide lesion localization and employs retrieval-segmentation joint learning to bridge the semantic gap between visual and linguistic modalities. Crucially, STPNet retrieves relevant textual information from a specialized medical text repository during training, eliminating the need for text input during inference while retaining the benefits of cross-modal learning. We evaluate STPNet on three datasets: COVID-Xray, COVID-CT, and Kvasir-SEG. Experimental results show that our vision-language approach outperforms state-of-the-art segmentation methods, demonstrating the effectiveness of incorporating textual semantic knowledge into medical image analysis. The code has been made publicly on <a target="_blank" rel="noopener" href="https://github.com/HUANGLIZI/STPNet">https://github.com/HUANGLIZI/STPNet</a>. </p>
<blockquote>
<p>åœ¨åŒ»å­¦å›¾åƒåˆ†æå’Œè¯Šæ–­ä¸­ï¼Œç²¾ç¡®åœ°åˆ†å‰²ç—…ç¶èµ·ç€è‡³å…³é‡è¦çš„ä½œç”¨ã€‚ä¼ ç»Ÿçš„ä»…ä¾èµ–äºè§†è§‰ç‰¹å¾çš„åˆ†å‰²æ–¹æ³•å¾€å¾€éš¾ä»¥åº”å¯¹ç—…ç¶åˆ†å¸ƒå’Œå¤§å°çš„ä¸ç¡®å®šæ€§ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†STPNetï¼Œè¿™æ˜¯ä¸€ä¸ªåˆ©ç”¨è§†è§‰è¯­è¨€å»ºæ¨¡å¢å¼ºåŒ»å­¦å›¾åƒåˆ†å‰²çš„è§„æ¨¡æ„ŸçŸ¥æ–‡æœ¬æç¤ºç½‘ç»œã€‚æˆ‘ä»¬çš„æ–¹æ³•åˆ©ç”¨å¤šå°ºåº¦æ–‡æœ¬æè¿°æ¥å¼•å¯¼ç—…ç¶å®šä½ï¼Œå¹¶é‡‡ç”¨æ£€ç´¢åˆ†å‰²è”åˆå­¦ä¹ æ¥å¼¥åˆè§†è§‰å’Œè¯­è¨€æ¨¡æ€ä¹‹é—´çš„è¯­ä¹‰é¸¿æ²Ÿã€‚é‡è¦çš„æ˜¯ï¼ŒSTPNetåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ä»ä¸“é—¨çš„åŒ»å­¦æ–‡æœ¬å­˜å‚¨åº“ä¸­æ£€ç´¢ç›¸å…³çš„æ–‡æœ¬ä¿¡æ¯ï¼Œä»è€Œåœ¨æ¨ç†è¿‡ç¨‹ä¸­ä¸éœ€è¦æ–‡æœ¬è¾“å…¥ï¼ŒåŒæ—¶ä¿ç•™è·¨æ¨¡æ€å­¦ä¹ çš„ä¼˜åŠ¿ã€‚æˆ‘ä»¬åœ¨ä¸‰ä¸ªæ•°æ®é›†ä¸Šè¯„ä¼°äº†STPNetï¼šCOVID-Xrayã€COVID-CTå’ŒKvasir-SEGã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„è§†è§‰è¯­è¨€æ–¹æ³•ä¼˜äºæœ€å…ˆè¿›çš„åˆ†å‰²æ–¹æ³•ï¼Œè¯æ˜äº†å°†æ–‡æœ¬è¯­ä¹‰çŸ¥è¯†èå…¥åŒ»å­¦å›¾åƒåˆ†æä¸­çš„æœ‰æ•ˆæ€§ã€‚ä»£ç å·²å…¬å¼€åœ¨<a target="_blank" rel="noopener" href="https://github.com/HUANGLIZI/STPNet">https://github.com/HUANGLIZI/STPNet</a>ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.01561v1">PDF</a> </p>
<p><strong>Summary</strong><br>åŒ»å­¦å›¾åƒåˆ†æä¸­ï¼Œç—…ç¶å‡†ç¡®åˆ†å‰²å¯¹è¯Šæ–­è‡³å…³é‡è¦ã€‚ä¼ ç»Ÿæ–¹æ³•å¸¸å› ç—…ç¶åˆ†å¸ƒå’Œå¤§å°çš„ä¸ç¡®å®šæ€§è€Œé¢ä¸´æŒ‘æˆ˜ã€‚STPNetæ˜¯ä¸€ä¸ªå°ºåº¦æ„ŸçŸ¥æ–‡æœ¬æç¤ºç½‘ç»œï¼Œé‡‡ç”¨è·¨è§†è§‰è¯­è¨€å»ºæ¨¡å¢å¼ºåŒ»å­¦å›¾åƒåˆ†å‰²ã€‚å®ƒåˆ©ç”¨å¤šå°ºåº¦æ–‡æœ¬æè¿°å¼•å¯¼ç—…ç¶å®šä½ï¼Œå¹¶é€šè¿‡æ£€ç´¢åˆ†å‰²è”åˆå­¦ä¹ æ¥å¼¥åˆè§†è§‰å’Œè¯­è¨€æ¨¡æ€ä¹‹é—´çš„è¯­ä¹‰é¸¿æ²Ÿã€‚STPNetåœ¨è®­ç»ƒæ—¶ä»ä¸“ä¸šåŒ»å­¦æ–‡æœ¬åº“ä¸­æ£€ç´¢ç›¸å…³æ–‡æœ¬ä¿¡æ¯ï¼Œæ¨ç†æ—¶æ— éœ€æ–‡æœ¬è¾“å…¥ï¼Œä»èƒ½ä¿ç•™è·¨æ¨¡æ€å­¦ä¹ çš„ä¼˜åŠ¿ã€‚åœ¨ä¸‰ä¸ªæ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒSTPNetçš„è§†è¯­è¨€æ–¹æ³•ä¼˜äºæœ€å…ˆè¿›çš„åˆ†å‰²æ–¹æ³•ï¼Œè¯æ˜äº†å°†æ–‡æœ¬è¯­ä¹‰çŸ¥è¯†èå…¥åŒ»å­¦å›¾åƒåˆ†æçš„æœ‰æ•ˆæ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>åŒ»å­¦å›¾åƒåˆ†æä¸­ï¼ŒSTPNetç½‘ç»œæ¨¡å‹åˆ©ç”¨è§†è§‰è¯­è¨€å»ºæ¨¡æé«˜åŒ»å­¦å›¾åƒåˆ†å‰²çš„å‡†ç¡®æ€§ã€‚</li>
<li>STPNeté‡‡ç”¨å¤šå°ºåº¦æ–‡æœ¬æè¿°ï¼Œä»¥å¢å¼ºæ¨¡å‹å¯¹ç—…ç¶çš„å®šä½èƒ½åŠ›ã€‚</li>
<li>é€šè¿‡æ£€ç´¢åˆ†å‰²è”åˆå­¦ä¹ ï¼ŒSTPNetèƒ½å¤Ÿå¼¥åˆè§†è§‰å’Œè¯­è¨€æ¨¡æ€ä¹‹é—´çš„è¯­ä¹‰é¸¿æ²Ÿã€‚</li>
<li>STPNetä»ä¸“ä¸šåŒ»å­¦æ–‡æœ¬åº“ä¸­æ£€ç´¢ç›¸å…³æ–‡æœ¬ä¿¡æ¯ï¼Œä»¥æå‡æ¨¡å‹çš„æ€§èƒ½ã€‚</li>
<li>åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šçš„å®éªŒè¯æ˜STPNetçš„è§†è¯­è¨€æ–¹æ³•ä¼˜äºä¼ ç»Ÿæ–¹æ³•ã€‚</li>
<li>STPNetæé«˜äº†åŒ»å­¦å›¾åƒåˆ†æçš„æ•ˆç‡å’Œå‡†ç¡®æ€§ï¼Œå…·æœ‰å®é™…åº”ç”¨ä»·å€¼ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.01561">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-78c42a000195bd4ef55fdd9c95b9d445.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0f59956388d8ed194813faf60d761217.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f57b1924bc137350c937ad810a82ea0a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-812112309d926c88e206e0c7cf874617.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Semi-Supervised-Biomedical-Image-Segmentation-via-Diffusion-Models-and-Teacher-Student-Co-Training"><a href="#Semi-Supervised-Biomedical-Image-Segmentation-via-Diffusion-Models-and-Teacher-Student-Co-Training" class="headerlink" title="Semi-Supervised Biomedical Image Segmentation via Diffusion Models and   Teacher-Student Co-Training"></a>Semi-Supervised Biomedical Image Segmentation via Diffusion Models and   Teacher-Student Co-Training</h2><p><strong>Authors:Luca Ciampi, Gabriele Lagani, Giuseppe Amato, Fabrizio Falchi</strong></p>
<p>Supervised deep learning for semantic segmentation has achieved excellent results in accurately identifying anatomical and pathological structures in medical images. However, it often requires large annotated training datasets, which limits its scalability in clinical settings. To address this challenge, semi-supervised learning is a well-established approach that leverages both labeled and unlabeled data. In this paper, we introduce a novel semi-supervised teacher-student framework for biomedical image segmentation, inspired by the recent success of generative models. Our approach leverages denoising diffusion probabilistic models (DDPMs) to generate segmentation masks by progressively refining noisy inputs conditioned on the corresponding images. The teacher model is first trained in an unsupervised manner using a cycle-consistency constraint based on noise-corrupted image reconstruction, enabling it to generate informative semantic masks. Subsequently, the teacher is integrated into a co-training process with a twin-student network. The student learns from ground-truth labels when available and from teacher-generated pseudo-labels otherwise, while the teacher continuously improves its pseudo-labeling capabilities. Finally, to further enhance performance, we introduce a multi-round pseudo-label generation strategy that iteratively improves the pseudo-labeling process. We evaluate our approach on multiple biomedical imaging benchmarks, spanning multiple imaging modalities and segmentation tasks. Experimental results show that our method consistently outperforms state-of-the-art semi-supervised techniques, highlighting its effectiveness in scenarios with limited annotated data. The code to replicate our experiments can be found at <a target="_blank" rel="noopener" href="https://github.com/ciampluca/diffusion_semi_supervised_biomedical_image_segmentation">https://github.com/ciampluca/diffusion_semi_supervised_biomedical_image_segmentation</a> </p>
<blockquote>
<p>åŸºäºæ·±åº¦å­¦ä¹ çš„è¯­ä¹‰åˆ†å‰²åœ¨å‡†ç¡®è¯†åˆ«åŒ»å­¦å›¾åƒä¸­çš„è§£å‰–å’Œç—…ç†ç»“æ„æ–¹é¢å–å¾—äº†å“è¶Šæˆæœã€‚ç„¶è€Œï¼Œè¿™é€šå¸¸éœ€è¦å¤§é‡å·²æ ‡æ³¨çš„è®­ç»ƒæ•°æ®é›†ï¼Œè¿™åœ¨ä¸´åºŠç¯å¢ƒä¸­é™åˆ¶äº†å…¶å¯æ‰©å±•æ€§ã€‚ä¸ºäº†åº”å¯¹è¿™ä¸€æŒ‘æˆ˜ï¼ŒåŠç›‘ç£å­¦ä¹ æ˜¯ä¸€ç§æ—¢åˆ©ç”¨æ ‡è®°æ•°æ®åˆåˆ©ç”¨æœªæ ‡è®°æ•°æ®çš„æˆç†Ÿæ–¹æ³•ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†ä¸€ç§å—ç”Ÿæˆæ¨¡å‹æœ€æ–°æˆåŠŸå¯å‘çš„ç”Ÿç‰©åŒ»å­¦å›¾åƒåˆ†å‰²æ–°å‹åŠç›‘ç£å¸ˆå¾’æ¡†æ¶ã€‚æˆ‘ä»¬çš„æ–¹æ³•åˆ©ç”¨å»å™ªæ‰©æ•£æ¦‚ç‡æ¨¡å‹ï¼ˆDDPMï¼‰é€æ­¥ç»†åŒ–åŸºäºç›¸åº”å›¾åƒçš„å™ªå£°è¾“å…¥æ¥ç”Ÿæˆåˆ†å‰²æ©è†œã€‚é¦–å…ˆï¼Œä½¿ç”¨åŸºäºå™ªå£°æŸåå›¾åƒé‡å»ºçš„å¾ªç¯ä¸€è‡´æ€§çº¦æŸä»¥æ— ç›‘ç£æ–¹å¼è®­ç»ƒæ•™å¸ˆæ¨¡å‹ï¼Œä½¿å…¶èƒ½å¤Ÿç”Ÿæˆä¿¡æ¯ä¸°å¯Œçš„è¯­ä¹‰æ©è†œã€‚éšåï¼Œæ•™å¸ˆè¢«æ•´åˆåˆ°ä¸€ä¸ªä¸å­¦ç”Ÿç½‘ç»œååŒè®­ç»ƒçš„è¿‡ç¨‹ä¸­ã€‚å½“å­¦ç”Ÿå¯ä»å¯ç”¨çš„çœŸå®æ ‡ç­¾ä¸­å­¦ä¹ æ—¶ï¼Œåˆ™ä»å…¶ç”Ÿæˆä¼ªæ ‡ç­¾ä¸­å­¦ä¹ ï¼ŒåŒæ—¶æ•™å¸ˆä¸æ–­æé«˜å…¶ä¼ªæ ‡ç­¾ç”Ÿæˆèƒ½åŠ›ã€‚æœ€åï¼Œä¸ºäº†è¿›ä¸€æ­¥æé«˜æ€§èƒ½ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§å¤šè½®ä¼ªæ ‡ç­¾ç”Ÿæˆç­–ç•¥ï¼Œè¯¥ç­–ç•¥é€šè¿‡è¿­ä»£æ”¹è¿›ä¼ªæ ‡ç­¾ç”Ÿæˆè¿‡ç¨‹ã€‚æˆ‘ä»¬åœ¨å¤šä¸ªç”Ÿç‰©åŒ»å­¦æˆåƒåŸºå‡†æµ‹è¯•ä¸Šå¯¹æå‡ºçš„æ–¹æ³•è¿›è¡Œäº†è¯„ä¼°ï¼Œæ¶‰åŠå¤šç§æˆåƒæ–¹å¼å’Œåˆ†å‰²ä»»åŠ¡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•å§‹ç»ˆä¼˜äºæœ€æ–°çš„åŠç›‘ç£æŠ€æœ¯ï¼Œåœ¨æ ‡æ³¨æ•°æ®æœ‰é™çš„æƒ…å†µä¸‹è¡¨ç°å‡ºäº†æœ‰æ•ˆæ€§ã€‚æˆ‘ä»¬å®éªŒçš„ä»£ç å¯åœ¨ <a target="_blank" rel="noopener" href="https://github.com/ciampluca/diffusion_semi_supervised_biomedical_image_segmentation">https://github.com/ciampluca/diffusion_semi_supervised_biomedical_image_segmentation</a> æ‰¾åˆ°ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.01547v1">PDF</a> </p>
<p><strong>Summary</strong><br>æ·±åº¦å­¦ä¹ ç›‘ç£æ³•å¯¹äºåŒ»å­¦å›¾åƒä¸­çš„è§£å‰–ç»“æ„å’Œç—…ç†ç»“æ„çš„è¯†åˆ«å–å¾—äº†ä¼˜å¼‚æˆæœã€‚ç„¶è€Œï¼Œå®ƒé€šå¸¸éœ€è¦å¤§é‡çš„æ ‡æ³¨è®­ç»ƒæ•°æ®é›†ï¼Œè¿™åœ¨ä¸´åºŠç¯å¢ƒä¸­é™åˆ¶äº†å…¶å¯æ‰©å±•æ€§ã€‚ä¸ºè§£å†³æ­¤æŒ‘æˆ˜ï¼ŒåŠç›‘ç£å­¦ä¹ æ³•åˆ©ç”¨æ ‡æ³¨å’Œéæ ‡æ³¨æ•°æ®ï¼Œæˆä¸ºä¸€ç§æœ‰æ•ˆçš„æ–¹æ³•ã€‚æœ¬æ–‡ä»‹ç»äº†ä¸€ç§å—ç”Ÿæˆæ¨¡å‹å¯å‘çš„æ–°å‹åŠç›‘ç£æ•™å¸ˆ-å­¦ç”Ÿæ¡†æ¶ï¼Œç”¨äºç”Ÿç‰©åŒ»å­¦å›¾åƒåˆ†å‰²ã€‚è¯¥æ¡†æ¶åˆ©ç”¨å»å™ªæ‰©æ•£æ¦‚ç‡æ¨¡å‹ï¼ˆDDPMsï¼‰é€æ­¥ä¼˜åŒ–å™ªå£°è¾“å…¥ï¼Œç”ŸæˆåŸºäºå¯¹åº”å›¾åƒçš„åˆ†å‰²æ©è†œã€‚é¦–å…ˆï¼Œä»¥å¾ªç¯ä¸€è‡´æ€§çº¦æŸè®­ç»ƒæ•™å¸ˆæ¨¡å‹ï¼Œé€šè¿‡å™ªå£°å¹²æ‰°å›¾åƒé‡å»ºç”Ÿæˆä¿¡æ¯ä¸°å¯Œçš„è¯­ä¹‰æ©è†œã€‚éšåï¼Œæ•™å¸ˆæ¨¡å‹ä¸åŒå­¦ç”Ÿç½‘ç»œå…±åŒè®­ç»ƒã€‚å­¦ç”Ÿæ¨¡å‹åœ¨æœ‰å¯ç”¨çœŸå®æ ‡ç­¾æ—¶å­¦ä¹ ï¼Œå¦åˆ™ä»æ•™å¸ˆç”Ÿæˆçš„ä¼ªæ ‡ç­¾ä¸­å­¦ä¹ ï¼ŒåŒæ—¶æ•™å¸ˆä¸æ–­æé«˜å…¶ä¼ªæ ‡ç­¾ç”Ÿæˆèƒ½åŠ›ã€‚æœ€åï¼Œå¼•å…¥å¤šè½®ä¼ªæ ‡ç­¾ç”Ÿæˆç­–ç•¥ï¼Œä»¥è¿­ä»£æ”¹è¿›ä¼ªæ ‡ç­¾è¿‡ç¨‹ã€‚åœ¨å¤šä¸ªç”Ÿç‰©åŒ»å­¦æˆåƒåŸºå‡†æµ‹è¯•ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•ä¼˜äºæœ€æ–°çš„åŠç›‘ç£æŠ€æœ¯ï¼Œåœ¨æ ‡æ³¨æ•°æ®æœ‰é™çš„æƒ…å†µä¸‹å°¤ä¸ºæœ‰æ•ˆã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ·±åº¦å­¦ä¹ åœ¨åŒ»å­¦å›¾åƒåˆ†å‰²ä¸­è¡¨ç°ä¼˜ç§€ï¼Œä½†éœ€æ±‚å¤§é‡æ ‡æ³¨æ•°æ®ã€‚</li>
<li>åŠç›‘ç£å­¦ä¹ æ³•åˆ©ç”¨æ ‡æ³¨å’Œéæ ‡æ³¨æ•°æ®ï¼Œè§£å†³æ ‡æ³¨æ•°æ®éœ€æ±‚é—®é¢˜ã€‚</li>
<li>å¼•å…¥æ–°å‹æ•™å¸ˆ-å­¦ç”Ÿæ¡†æ¶ï¼Œåˆ©ç”¨å»å™ªæ‰©æ•£æ¦‚ç‡æ¨¡å‹ï¼ˆDDPMsï¼‰ç”Ÿæˆåˆ†å‰²æ©è†œã€‚</li>
<li>æ•™å¸ˆæ¨¡å‹é€šè¿‡å¾ªç¯ä¸€è‡´æ€§çº¦æŸè¿›è¡Œè®­ç»ƒï¼Œç”Ÿæˆè¯­ä¹‰æ©è†œã€‚</li>
<li>æ•™å¸ˆå’Œå­¦ç”Ÿæ¨¡å‹å…±åŒè®­ç»ƒï¼Œå­¦ç”Ÿæ¨¡å‹å¯ä»æ•™å¸ˆç”Ÿæˆçš„ä¼ªæ ‡ç­¾ä¸­å­¦ä¹ ã€‚</li>
<li>å¼•å…¥å¤šè½®ä¼ªæ ‡ç­¾ç”Ÿæˆç­–ç•¥æé«˜æ€§èƒ½ã€‚</li>
<li>åœ¨å¤šä¸ªç”Ÿç‰©åŒ»å­¦æˆåƒåŸºå‡†æµ‹è¯•ä¸Šï¼Œè¯¥æ–¹æ³•è¡¨ç°ä¼˜è¶Šã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.01547">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-bfef968e055c4696384af9ee212be722.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ab3af817e43e9dfef4768cc6b6bbae29.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-02c8a00fea2265b06b5a97651c2c8d3c.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="FUSION-Frequency-guided-Underwater-Spatial-Image-recOnstructioN"><a href="#FUSION-Frequency-guided-Underwater-Spatial-Image-recOnstructioN" class="headerlink" title="FUSION: Frequency-guided Underwater Spatial Image recOnstructioN"></a>FUSION: Frequency-guided Underwater Spatial Image recOnstructioN</h2><p><strong>Authors:Jaskaran Singh Walia, Shravan Venkatraman, Pavithra LK</strong></p>
<p>Underwater images suffer from severe degradations, including color distortions, reduced visibility, and loss of structural details due to wavelength-dependent attenuation and scattering. Existing enhancement methods primarily focus on spatial-domain processing, neglecting the frequency domainâ€™s potential to capture global color distributions and long-range dependencies. To address these limitations, we propose FUSION, a dual-domain deep learning framework that jointly leverages spatial and frequency domain information. FUSION independently processes each RGB channel through multi-scale convolutional kernels and adaptive attention mechanisms in the spatial domain, while simultaneously extracting global structural information via FFT-based frequency attention. A Frequency Guided Fusion module integrates complementary features from both domains, followed by inter-channel fusion and adaptive channel recalibration to ensure balanced color distributions. Extensive experiments on benchmark datasets (UIEB, EUVP, SUIM-E) demonstrate that FUSION achieves state-of-the-art performance, consistently outperforming existing methods in reconstruction fidelity (highest PSNR of 23.717 dB and SSIM of 0.883 on UIEB), perceptual quality (lowest LPIPS of 0.112 on UIEB), and visual enhancement metrics (best UIQM of 3.414 on UIEB), while requiring significantly fewer parameters (0.28M) and lower computational complexity, demonstrating its suitability for real-time underwater imaging applications. </p>
<blockquote>
<p>æ°´ä¸‹å›¾åƒå—åˆ°ä¸¥é‡çš„é€€åŒ–å½±å“ï¼ŒåŒ…æ‹¬è‰²å½©å¤±çœŸã€èƒ½è§åº¦é™ä½ä»¥åŠç”±äºæ³¢é•¿ç›¸å…³è¡°å‡å’Œæ•£å°„å¯¼è‡´çš„ç»“æ„ç»†èŠ‚æŸå¤±ã€‚ç°æœ‰çš„å¢å¼ºæ–¹æ³•ä¸»è¦å…³æ³¨ç©ºé—´åŸŸå¤„ç†ï¼Œå¿½è§†äº†é¢‘ç‡åŸŸåœ¨æ•æ‰å…¨å±€é¢œè‰²åˆ†å¸ƒå’Œé•¿è·ç¦»ä¾èµ–æ–¹é¢çš„æ½œåŠ›ã€‚ä¸ºäº†è§£å†³è¿™äº›å±€é™æ€§ï¼Œæˆ‘ä»¬æå‡ºäº†FUSIONï¼Œè¿™æ˜¯ä¸€ä¸ªåŒåŸŸæ·±åº¦å­¦ä¹ æ¡†æ¶ï¼Œèƒ½å¤Ÿè”åˆåˆ©ç”¨ç©ºé—´å’Œé¢‘ç‡åŸŸä¿¡æ¯ã€‚FUSIONé€šè¿‡ç©ºé—´åŸŸä¸­çš„å¤šå°ºåº¦å·ç§¯æ ¸å’Œè‡ªé€‚åº”æ³¨æ„åŠ›æœºåˆ¶ç‹¬ç«‹å¤„ç†æ¯ä¸ªRGBé€šé“ï¼ŒåŒæ—¶åˆ©ç”¨åŸºäºFFTçš„é¢‘ç‡æ³¨æ„åŠ›æå–å…¨å±€ç»“æ„ä¿¡æ¯ã€‚é¢‘ç‡å¼•å¯¼èåˆæ¨¡å—æ•´åˆäº†æ¥è‡ªä¸¤ä¸ªåŸŸçš„ç‰¹å¾ï¼Œç„¶åè¿›è¡Œè·¨é€šé“èåˆå’Œè‡ªé€‚åº”é€šé“å†æ ¡å‡†ï¼Œä»¥ç¡®ä¿é¢œè‰²åˆ†å¸ƒå¹³è¡¡ã€‚åœ¨åŸºå‡†æ•°æ®é›†ï¼ˆUIEBã€EUVPã€SUIM-Eï¼‰ä¸Šçš„å¤§é‡å®éªŒè¡¨æ˜ï¼ŒFUSIONè¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œåœ¨é‡å»ºä¿çœŸåº¦ã€æ„ŸçŸ¥è´¨é‡å’Œè§†è§‰å¢å¼ºæŒ‡æ ‡æ–¹é¢å‡ä¼˜äºç°æœ‰æ–¹æ³•ï¼ˆåœ¨UIEBä¸Šï¼ŒPSNRæœ€é«˜è¾¾23.717 dBï¼ŒSSIMä¸º0.883ï¼›åœ¨æ„ŸçŸ¥è´¨é‡æ–¹é¢ï¼ŒUIEBä¸Šçš„LPIPSæœ€ä½ï¼Œä¸º0.112ï¼›åœ¨UIEBä¸Šè·å¾—æœ€ä½³UIQMä¸º3.414ï¼‰ï¼ŒåŒæ—¶æ‰€éœ€å‚æ•°å¤§å¤§å‡å°‘ï¼ˆä»…0.28Mï¼‰ä¸”è®¡ç®—å¤æ‚åº¦è¾ƒä½ï¼Œæ˜¾ç¤ºå‡ºå…¶é€‚ç”¨äºå®æ—¶æ°´ä¸‹æˆåƒåº”ç”¨ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.01243v1">PDF</a> </p>
<p><strong>Summary</strong><br>    æœ¬æ–‡æå‡ºä¸€ç§åä¸ºFUSIONçš„æ·±åº¦å­¦ä¹ æ¡†æ¶ï¼Œè¯¥æ¡†æ¶ç»“åˆäº†ç©ºé—´åŸŸå’Œé¢‘åŸŸä¿¡æ¯ï¼Œæ—¨åœ¨è§£å†³æ°´ä¸‹å›¾åƒè‰²å½©å¤±çœŸã€èƒ½è§åº¦é™ä½å’Œç»“æ„ç»†èŠ‚ä¸¢å¤±çš„é—®é¢˜ã€‚FUSIONé€šè¿‡å¤šå°ºåº¦å·ç§¯æ ¸å’Œè‡ªé€‚åº”æ³¨æ„åŠ›æœºåˆ¶ç‹¬ç«‹å¤„ç†æ¯ä¸ªRGBé€šé“çš„ç©ºé—´åŸŸä¿¡æ¯ï¼ŒåŒæ—¶åˆ©ç”¨FFTåŸºäºé¢‘åŸŸçš„æ³¨æ„åŠ›æœºåˆ¶æå–å…¨å±€ç»“æ„ä¿¡æ¯ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒFUSIONåœ¨åŸºå‡†æ•°æ®é›†ä¸Šçš„æ€§èƒ½è¾¾åˆ°é¢†å…ˆæ°´å¹³ï¼Œåœ¨é‡å»ºä¿çœŸåº¦ã€æ„ŸçŸ¥è´¨é‡å’Œè§†è§‰å¢å¼ºæŒ‡æ ‡æ–¹é¢å‡ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œä¸”å‚æ•°è¾ƒå°‘ã€è®¡ç®—å¤æ‚åº¦è¾ƒä½ï¼Œé€‚åˆç”¨äºå®æ—¶æ°´ä¸‹æˆåƒåº”ç”¨ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ°´ä¸‹å›¾åƒå­˜åœ¨ä¸¥é‡çš„å¤±çœŸé—®é¢˜ï¼ŒåŒ…æ‹¬è‰²å½©å¤±çœŸã€èƒ½è§åº¦é™ä½å’Œç»“æ„ç»†èŠ‚ä¸¢å¤±ã€‚</li>
<li>ç°æœ‰å¢å¼ºæ–¹æ³•ä¸»è¦å…³æ³¨ç©ºé—´åŸŸå¤„ç†ï¼Œå¿½ç•¥äº†é¢‘åŸŸä¿¡æ¯åœ¨æ•æ‰å…¨å±€é¢œè‰²åˆ†å¸ƒå’Œè¿œç¨‹ä¾èµ–æ–¹é¢çš„æ½œåŠ›ã€‚</li>
<li>FUSIONæ¡†æ¶ç»“åˆäº†ç©ºé—´åŸŸå’Œé¢‘åŸŸä¿¡æ¯ï¼Œé€šè¿‡å¤šå°ºåº¦å·ç§¯æ ¸å’Œè‡ªé€‚åº”æ³¨æ„åŠ›æœºåˆ¶å¤„ç†RGBé€šé“çš„ç©ºé—´åŸŸä¿¡æ¯ã€‚</li>
<li>FUSIONåˆ©ç”¨FFT-basedé¢‘ç‡æ³¨æ„åŠ›æœºåˆ¶æå–å…¨å±€ç»“æ„ä¿¡æ¯ã€‚</li>
<li>FUSIONé€šè¿‡é¢‘ç‡å¼•å¯¼èåˆæ¨¡å—æ•´åˆä¸¤ä¸ªåŸŸçš„ç‰¹å¾ï¼Œç„¶åè¿›è¡Œè·¨é€šé“èåˆå’Œè‡ªé€‚åº”é€šé“æ ¡å‡†ä»¥ç¡®ä¿é¢œè‰²åˆ†å¸ƒçš„å¹³è¡¡ã€‚</li>
<li>å®éªŒç»“æœè¡¨æ˜ï¼ŒFUSIONåœ¨å¤šä¸ªåŸºå‡†æ•°æ®é›†ä¸Šè¾¾åˆ°æœ€ä½³æ€§èƒ½ï¼Œä¼˜äºå…¶ä»–æ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.01243">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-b29178d911aabe126872cb60d16889a0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-34708165bd811d1d2ddd396ca43f8d5d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ed55b0331072203cf11580ef281b3a37.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-4a33ce3a37b57e6bffccca3b1ec85510.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="An-Integrated-AI-Enabled-System-Using-One-Class-Twin-Cross-Learning-OCT-X-for-Early-Gastric-Cancer-Detection"><a href="#An-Integrated-AI-Enabled-System-Using-One-Class-Twin-Cross-Learning-OCT-X-for-Early-Gastric-Cancer-Detection" class="headerlink" title="An Integrated AI-Enabled System Using One Class Twin Cross Learning   (OCT-X) for Early Gastric Cancer Detection"></a>An Integrated AI-Enabled System Using One Class Twin Cross Learning   (OCT-X) for Early Gastric Cancer Detection</h2><p><strong>Authors:Xian-Xian Liu, Yuanyuan Wei, Mingkun Xu, Yongze Guo, Hongwei Zhang, Huicong Dong, Qun Song, Qi Zhao, Wei Luo, Feng Tien, Juntao Gao, Simon Fong</strong></p>
<p>Early detection of gastric cancer, a leading cause of cancer-related mortality worldwide, remains hampered by the limitations of current diagnostic technologies, leading to high rates of misdiagnosis and missed diagnoses. To address these challenges, we propose an integrated system that synergizes advanced hardware and software technologies to balance speed-accuracy. Our study introduces the One Class Twin Cross Learning (OCT-X) algorithm. Leveraging a novel fast double-threshold grid search strategy (FDT-GS) and a patch-based deep fully convolutional network, OCT-X maximizes diagnostic accuracy through real-time data processing and seamless lesion surveillance. The hardware component includes an all-in-one point-of-care testing (POCT) device with high-resolution imaging sensors, real-time data processing, and wireless connectivity, facilitated by the NI CompactDAQ and LabVIEW software. Our integrated system achieved an unprecedented diagnostic accuracy of 99.70%, significantly outperforming existing models by up to 4.47%, and demonstrated a 10% improvement in multirate adaptability. These findings underscore the potential of OCT-X as well as the integrated system in clinical diagnostics, offering a path toward more accurate, efficient, and less invasive early gastric cancer detection. Future research will explore broader applications, further advancing oncological diagnostics. Code is available at <a target="_blank" rel="noopener" href="https://github.com/liu37972/Multirate-Location-on-OCT-X-Learning.git">https://github.com/liu37972/Multirate-Location-on-OCT-X-Learning.git</a>. </p>
<blockquote>
<p>èƒƒç™Œæ˜¯å…¨çƒç™Œç—‡ç›¸å…³æ­»äº¡çš„ä¸»è¦åŸå› ä¹‹ä¸€ï¼Œå…¶æ—©æœŸæ£€æµ‹ä»å—åˆ°å½“å‰è¯Šæ–­æŠ€æœ¯å±€é™çš„é˜»ç¢ï¼Œå¯¼è‡´è¯¯è¯Šå’Œæ¼è¯Šç‡è¾ƒé«˜ã€‚ä¸ºäº†åº”å¯¹è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ååŒå…ˆè¿›ç¡¬ä»¶å’Œè½¯ä»¶æŠ€æœ¯çš„é›†æˆç³»ç»Ÿï¼Œä»¥å¹³è¡¡é€Ÿåº¦å’Œå‡†ç¡®æ€§ã€‚æœ¬ç ”ç©¶ä»‹ç»äº†å•ç±»åŒäº¤å‰å­¦ä¹ ï¼ˆOCT-Xï¼‰ç®—æ³•ã€‚è¯¥ç®—æ³•åˆ©ç”¨æ–°å‹å¿«é€ŸåŒé˜ˆå€¼ç½‘æ ¼æœç´¢ç­–ç•¥ï¼ˆFDT-GSï¼‰å’ŒåŸºäºè¡¥ä¸çš„æ·±åº¦å…¨å·ç§¯ç½‘ç»œï¼Œé€šè¿‡å®æ—¶æ•°æ®å¤„ç†å’Œæ— ç¼ç—…å˜ç›‘æµ‹ï¼Œæœ€å¤§é™åº¦åœ°æé«˜è¯Šæ–­å‡†ç¡®æ€§ã€‚ç¡¬ä»¶ç»„ä»¶åŒ…æ‹¬å…·æœ‰é«˜æ¸…æˆåƒä¼ æ„Ÿå™¨ã€å®æ—¶æ•°æ®å¤„ç†å’Œæ— çº¿è¿æ¥çš„ä¸€ä½“åŒ–å³æ—¶æ£€æµ‹ï¼ˆPOCTï¼‰è®¾å¤‡ï¼Œç”±NI CompactDAQå’ŒLabVIEWè½¯ä»¶æä¾›æ”¯æŒã€‚æˆ‘ä»¬çš„é›†æˆç³»ç»Ÿè¾¾åˆ°äº†å‰æ‰€æœªæœ‰çš„99.70%çš„è¯Šæ–­å‡†ç¡®ç‡ï¼Œæ¯”ç°æœ‰æ¨¡å‹é«˜å‡º4.47%ï¼Œå¹¶ä¸”åœ¨å¤šé€Ÿç‡é€‚åº”æ€§æ–¹é¢æé«˜äº†10%ã€‚è¿™äº›å‘ç°å¼ºè°ƒäº†OCT-XåŠå…¶åœ¨ä¸´åºŠè¯Šæ–­ä¸­çš„é›†æˆç³»ç»Ÿçš„æ½œåŠ›ï¼Œä¸ºæ›´å‡†ç¡®ã€é«˜æ•ˆå’Œéä¾µå…¥æ€§çš„æ—©æœŸèƒƒç™Œæ£€æµ‹æä¾›äº†é€”å¾„ã€‚æœªæ¥çš„ç ”ç©¶å°†æ¢ç´¢æ›´å¹¿æ³›çš„åº”ç”¨ï¼Œå¹¶è¿›ä¸€æ­¥æ¨åŠ¨è‚¿ç˜¤è¯Šæ–­çš„å‘å±•ã€‚ä»£ç å¯è®¿é—®<a target="_blank" rel="noopener" href="https://github.com/liu37972/Multirate-Location-on-OCT-X-Learning.git%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/liu37972/Multirate-Location-on-OCT-X-Learning.gitè·å–ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.01038v1">PDF</a> 26 pages, 4 figures, 6 tables</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸€ç§é’ˆå¯¹èƒƒç™Œæ—©æœŸè¯Šæ–­çš„é›†æˆç³»ç»Ÿï¼Œè¯¥ç³»ç»Ÿç»“åˆäº†å…ˆè¿›çš„è½¯ç¡¬ä»¶æŠ€æœ¯ï¼Œå®ç°äº†å¿«é€Ÿå‡†ç¡®çš„æ•°æ®å¤„ç†å’Œæ— ç¼ç—…ç¶ç›‘æµ‹ã€‚ç ”ç©¶ä¸­æå‡ºçš„One Class Twin Cross Learningï¼ˆOCT-Xï¼‰ç®—æ³•ï¼Œå€ŸåŠ©æ–°å‹å¿«é€ŸåŒé˜ˆå€¼ç½‘æ ¼æœç´¢ç­–ç•¥ï¼ˆFDT-GSï¼‰å’ŒåŸºäºè¡¥ä¸çš„æ·±åº¦å…¨å·ç§¯ç½‘ç»œï¼Œæœ€å¤§åŒ–äº†è¯Šæ–­å‡†ç¡®æ€§ã€‚æ­¤å¤–ï¼Œç³»ç»Ÿçš„ç¡¬ä»¶éƒ¨åˆ†åŒ…æ‹¬ä¸€ä¸ªé›†é«˜åˆ†è¾¨ç‡æˆåƒä¼ æ„Ÿå™¨ã€å®æ—¶æ•°æ®å¤„ç†å’Œæ— çº¿è¿æ¥äºä¸€ä½“çš„ä¾¿æºå¼ç°åœºæ£€æµ‹ï¼ˆPOCTï¼‰è®¾å¤‡ï¼Œè½¯ä»¶éƒ¨åˆ†é‡‡ç”¨NI CompactDAQå’ŒLabVIEWã€‚è¯¥é›†æˆç³»ç»Ÿè¯Šæ–­å‡†ç¡®ç‡é«˜è¾¾99.70%ï¼Œè¾ƒç°æœ‰æ¨¡å‹æé«˜äº†4.47%ï¼Œå¹¶åœ¨å¤šé€Ÿç‡é€‚åº”æ€§æ–¹é¢æé«˜äº†10%ã€‚è¿™æ˜¾ç¤ºäº†OCT-XåŠå…¶åœ¨èƒƒç™Œä¸´åºŠè¯Šæ–­ä¸­åº”ç”¨çš„æ½œåŠ›ï¼Œä¸ºæ›´å‡†ç¡®ã€é«˜æ•ˆå’Œéä¾µå…¥æ€§çš„æ—©æœŸèƒƒç™Œæ£€æµ‹æä¾›äº†æ–°çš„é€”å¾„ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>å½“å‰èƒƒç™Œè¯Šæ–­æŠ€æœ¯å­˜åœ¨æŒ‘æˆ˜ï¼Œå¯¼è‡´é«˜è¯¯è¯Šç‡ã€‚</li>
<li>æå‡ºäº†ä¸€ä¸ªé›†æˆç³»ç»Ÿï¼Œç»“åˆå…ˆè¿›è½¯ç¡¬ä»¶æŠ€æœ¯ï¼Œæé«˜è¯Šæ–­é€Ÿåº¦å’Œå‡†ç¡®æ€§ã€‚</li>
<li>å¼•å…¥OCT-Xç®—æ³•ï¼Œé€šè¿‡FDT-GSå’Œæ·±åº¦å…¨å·ç§¯ç½‘ç»œæœ€å¤§åŒ–è¯Šæ–­å‡†ç¡®æ€§ã€‚</li>
<li>ç¡¬ä»¶åŒ…æ‹¬é«˜åˆ†è¾¨ç‡æˆåƒä¼ æ„Ÿå™¨ã€å®æ—¶æ•°æ®å¤„ç†å’Œæ— çº¿è¿æ¥çš„POCTè®¾å¤‡ã€‚</li>
<li>è¯Šæ–­å‡†ç¡®ç‡é«˜è¾¾99.7%ï¼Œè¾ƒç°æœ‰æ¨¡å‹æ˜¾è‘—æé«˜ï¼Œå¹¶å¢å¼ºäº†å¤šé€Ÿç‡é€‚åº”æ€§ã€‚</li>
<li>OCT-Xç³»ç»Ÿåœ¨ä¸´åºŠè¯Šæ–­å’Œæ²»ç–—ä¸­æœ‰å¹¿æ³›åº”ç”¨æ½œåŠ›ã€‚</li>
<li>æœªæ¥çš„ç ”ç©¶å°†æ¢ç´¢æ›´å¹¿æ³›çš„åº”ç”¨ï¼Œå¹¶è¿›ä¸€æ­¥æ¨åŠ¨è‚¿ç˜¤å­¦è¯Šæ–­çš„è¿›æ­¥ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.01038">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-de054125acb7f4c86e1930e7982cd5aa.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7218d9e652e9f145e1511f3dbb5d3426.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="GKAN-Explainable-Diagnosis-of-Alzheimerâ€™s-Disease-Using-Graph-Neural-Network-with-Kolmogorov-Arnold-Networks"><a href="#GKAN-Explainable-Diagnosis-of-Alzheimerâ€™s-Disease-Using-Graph-Neural-Network-with-Kolmogorov-Arnold-Networks" class="headerlink" title="GKAN: Explainable Diagnosis of Alzheimerâ€™s Disease Using Graph Neural   Network with Kolmogorov-Arnold Networks"></a>GKAN: Explainable Diagnosis of Alzheimerâ€™s Disease Using Graph Neural   Network with Kolmogorov-Arnold Networks</h2><p><strong>Authors:Tianqi Ding, Dawei Xiang, Keith E Schubert, Liang Dong</strong></p>
<p>Alzheimerâ€™s Disease (AD) is a progressive neurodegenerative disorder that poses significant diagnostic challenges due to its complex etiology. Graph Convolutional Networks (GCNs) have shown promise in modeling brain connectivity for AD diagnosis, yet their reliance on linear transformations limits their ability to capture intricate nonlinear patterns in neuroimaging data. To address this, we propose GCN-KAN, a novel single-modal framework that integrates Kolmogorov-Arnold Networks (KAN) into GCNs to enhance both diagnostic accuracy and interpretability. Leveraging structural MRI data, our model employs learnable spline-based transformations to better represent brain region interactions. Evaluated on the Alzheimerâ€™s Disease Neuroimaging Initiative (ADNI) dataset, GCN-KAN outperforms traditional GCNs by 4-8% in classification accuracy while providing interpretable insights into key brain regions associated with AD. This approach offers a robust and explainable tool for early AD diagnosis. </p>
<blockquote>
<p>é˜¿å°”èŒ¨æµ·é»˜ç—…ï¼ˆADï¼‰æ˜¯ä¸€ç§è¿›å±•æ€§çš„ç¥ç»é€€è¡Œæ€§ç–¾ç—…ï¼Œç”±äºå…¶å¤æ‚çš„ç—…å› ï¼Œç»™è¯Šæ–­å¸¦æ¥äº†é‡å¤§æŒ‘æˆ˜ã€‚å›¾å·ç§¯ç½‘ç»œï¼ˆGCNsï¼‰åœ¨ADè¯Šæ–­çš„è„‘è¿æ¥å»ºæ¨¡ä¸­æ˜¾ç¤ºå‡ºæ½œåŠ›ï¼Œä½†å®ƒä»¬å¯¹çº¿æ€§è½¬æ¢çš„ä¾èµ–é™åˆ¶äº†æ•æ‰ç¥ç»æˆåƒæ•°æ®ä¸­å¤æ‚éçº¿æ€§æ¨¡å¼çš„èƒ½åŠ›ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†GCN-KANï¼Œè¿™æ˜¯ä¸€ç§æ–°å‹çš„å•æ¨¡æ€æ¡†æ¶ï¼Œå®ƒå°†Kolmogorov-Arnoldç½‘ç»œï¼ˆKANï¼‰é›†æˆåˆ°GCNsä¸­ï¼Œä»¥æé«˜è¯Šæ–­å‡†ç¡®æ€§å’Œå¯è§£é‡Šæ€§ã€‚æˆ‘ä»¬çš„æ¨¡å‹åˆ©ç”¨ç»“æ„MRIæ•°æ®ï¼Œé‡‡ç”¨å¯å­¦ä¹ çš„åŸºäºæ ·æ¡çš„è½¬æ¢æ¥æ›´å¥½åœ°è¡¨ç¤ºè„‘åŒºç›¸äº’ä½œç”¨ã€‚åœ¨é˜¿å°”èŒ¨æµ·é»˜ç—…ç¥ç»å½±åƒå­¦å€¡è®®ï¼ˆADNIï¼‰æ•°æ®é›†ä¸Šè¯„ä¼°ï¼ŒGCN-KANçš„åˆ†ç±»ç²¾åº¦æ¯”ä¼ ç»ŸGCNsé«˜å‡º4-8%ï¼ŒåŒæ—¶æä¾›äº†å…³äºä¸ADç›¸å…³çš„å…³é”®è„‘åŒºçš„å¯è§£é‡Šè§è§£ã€‚è¿™ç§æ–¹æ³•ä¸ºæ—©æœŸADè¯Šæ–­æä¾›äº†ç¨³å¥å’Œå¯è§£é‡Šçš„å·¥å…·ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.00946v1">PDF</a> 12 pages, 4 figures, under review of The Southwest Data Science   Conference (SDSC 2025)</p>
<p><strong>Summary</strong><br>     é’ˆå¯¹é˜¿å°”èŒ¨æµ·é»˜ç—…ï¼ˆADï¼‰è¯Šæ–­çš„æŒ‘æˆ˜ï¼Œæå‡ºä¸€ç§ç»“åˆå›¾å·ç§¯ç½‘ç»œï¼ˆGCNï¼‰å’ŒæŸ¯å°”è«å“¥æ´›å¤«-é˜¿è¯ºå°”å¾·ç½‘ç»œï¼ˆKANï¼‰çš„æ–°å‹å•æ¨¡æ€æ¡†æ¶GCN-KANã€‚è¯¥æ¡†æ¶åˆ©ç”¨ç»“æ„ç£å…±æŒ¯æˆåƒæ•°æ®ï¼Œé€šè¿‡é‡‡ç”¨å¯å­¦ä¹ çš„åŸºäºæ ·æ¡çš„è½¬æ¢ï¼Œæ›´å¥½åœ°è¡¨ç¤ºè„‘åŒºäº¤äº’ä½œç”¨ï¼Œæé«˜è¯Šæ–­å‡†ç¡®æ€§å’Œè§£é‡Šæ€§ã€‚åœ¨é˜¿å°”èŒ¨æµ·é»˜ç—…ç¥ç»å½±åƒå­¦å€¡è®®ï¼ˆADNIï¼‰æ•°æ®é›†ä¸Šè¯„ä¼°ï¼ŒGCN-KANåœ¨åˆ†ç±»å‡†ç¡®æ€§ä¸Šæ¯”ä¼ ç»ŸGCNé«˜å‡º4-8%ï¼ŒåŒæ—¶æä¾›å¯¹ADç›¸å…³å…³é”®è„‘åŒºçš„è§£é‡Šæ€§è§è§£ã€‚æ­¤æ¡†æ¶ä¸ºæ—©æœŸADè¯Šæ–­æä¾›äº†ç¨³å¥å’Œå¯è§£é‡Šçš„å·¥å…·ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Alzheimerâ€™s Disease (AD) æ˜¯ä¸€ç§å…·æœ‰å¤æ‚ç—…å› çš„æ¸è¿›æ€§ç¥ç»é€€è¡Œæ€§ç–¾ç—…ï¼Œå­˜åœ¨é‡å¤§è¯Šæ–­æŒ‘æˆ˜ã€‚</li>
<li>å›¾å·ç§¯ç½‘ç»œï¼ˆGCNï¼‰åœ¨ADè¯Šæ–­çš„è„‘è¿æ¥å»ºæ¨¡ä¸­æ˜¾ç¤ºå‡ºæ½œåŠ›ï¼Œä½†å…¶å¯¹çº¿æ€§è½¬æ¢çš„ä¾èµ–é™åˆ¶äº†æ•æ‰ç¥ç»æˆåƒæ•°æ®ä¸­å¤æ‚éçº¿æ€§æ¨¡å¼çš„èƒ½åŠ›ã€‚</li>
<li>æå‡ºä¸€ç§æ–°å‹å•æ¨¡æ€æ¡†æ¶GCN-KANï¼Œé›†æˆäº†Kolmogorov-Arnold Networksï¼ˆKANï¼‰ä»¥å¢å¼ºGCNsçš„è¯Šæ–­å‡†ç¡®æ€§å’Œè§£é‡Šæ€§ã€‚</li>
<li>GCN-KANåˆ©ç”¨ç»“æ„ç£å…±æŒ¯æˆåƒæ•°æ®ï¼Œé‡‡ç”¨å¯å­¦ä¹ çš„åŸºäºæ ·æ¡çš„è½¬æ¢ï¼Œæ›´å¥½åœ°è¡¨ç¤ºè„‘åŒºäº¤äº’ã€‚</li>
<li>åœ¨ADNIæ•°æ®é›†ä¸Šè¯„ä¼°ï¼ŒGCN-KANåœ¨åˆ†ç±»å‡†ç¡®æ€§ä¸Šä¼˜äºä¼ ç»ŸGCNæ–¹æ³•ï¼Œæé«˜äº†4-8%ã€‚</li>
<li>GCN-KANæä¾›äº†å¯¹ä¸ADç›¸å…³çš„å…³é”®è„‘åŒºçš„è§£é‡Šæ€§è§è§£ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.00946">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-c6c645d178a76839dae05925266aaf96.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3cd77d98c68725c7e99878b4410385b0.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Graph-Classification-and-Radiomics-Signature-for-Identification-of-Tuberculous-Meningitis"><a href="#Graph-Classification-and-Radiomics-Signature-for-Identification-of-Tuberculous-Meningitis" class="headerlink" title="Graph Classification and Radiomics Signature for Identification of   Tuberculous Meningitis"></a>Graph Classification and Radiomics Signature for Identification of   Tuberculous Meningitis</h2><p><strong>Authors:Snigdha Agarwal, Ganaraja V H, Neelam Sinha, Abhilasha Indoria, Netravathi M, Jitender Saini</strong></p>
<p>Introduction: Tuberculous meningitis (TBM) is a serious brain infection caused by Mycobacterium tuberculosis, characterized by inflammation of the meninges covering the brain and spinal cord. Diagnosis often requires invasive lumbar puncture (LP) and cerebrospinal fluid (CSF) analysis. Objectives: This study aims to classify TBM patients using T1-weighted (T1w) non-contrast Magnetic Resonance Imaging (MRI) scans. We hypothesize that specific brain regions, such as the interpeduncular cisterns, bone, and corpus callosum, contain visual markers that can non-invasively distinguish TBM patients from healthy controls. We propose a novel Pixel-array Graphs Classifier (PAG-Classifier) that leverages spatial relationships between neighbouring 3D pixels in a graph-based framework to extract significant features through eigen decomposition. These features are then used to train machine learning classifiers for effective patient classification. We validate our approach using a radiomics-based methodology, classifying TBM patients based on relevant radiomics features. Results: We utilized an internal dataset consisting of 52 scans, 32 from confirmed TBM patients based on mycobacteria detection in CSF, and 20 from healthy individuals. We achieved a 5-fold cross-validated average F1 score of 85.71% for cistern regions with our PAG-Classifier and 92.85% with the radiomics features classifier, surpassing current state-of-the-art benchmarks by 15% and 22%, respectively. However, bone and corpus callosum regions showed poor classification effectiveness, with average F1 scores below 50%. Conclusion: Our study suggests that algorithms like the PAG-Classifier serve as effective tools for non-invasive TBM analysis, particularly by targeting the interpeduncular cistern. Findings indicate that the bone and corpus callosum regions lack distinctive patterns for differentiation. </p>
<blockquote>
<p>å¼•è¨€ï¼šç»“æ ¸æ€§è„‘è†œç‚ï¼ˆTBMï¼‰æ˜¯ç”±ç»“æ ¸åˆ†ææ†èŒå¼•èµ·çš„ä¸€ç§ä¸¥é‡çš„å¤§è„‘æ„ŸæŸ“ï¼Œè¡¨ç°ä¸ºå¤§è„‘å’Œè„Šé«“çš„è„‘è†œå‘ç‚ã€‚è¯Šæ–­é€šå¸¸éœ€è¦ä¾µå…¥æ€§çš„è…°æ¤ç©¿åˆºï¼ˆLPï¼‰å’Œè„‘è„Šæ¶²ï¼ˆCSFï¼‰åˆ†æã€‚ç›®æ ‡ï¼šæœ¬ç ”ç©¶æ—¨åœ¨åˆ©ç”¨T1åŠ æƒï¼ˆT1wï¼‰éå¯¹æ¯”ç£å…±æŒ¯æˆåƒï¼ˆMRIï¼‰æ‰«æå¯¹TBMæ‚£è€…è¿›è¡Œåˆ†ç±»ã€‚æˆ‘ä»¬å‡è®¾ç‰¹å®šçš„è„‘åŒºï¼Œå¦‚è„šé—´æ± ã€éª¨éª¼å’Œèƒ¼èƒä½“ï¼Œå­˜åœ¨è§†è§‰æ ‡è®°ï¼Œå¯ä»¥æ— åˆ›åœ°åŒºåˆ†TBMæ‚£è€…å’Œå¥åº·å¯¹ç…§ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°å‹çš„åƒç´ é˜µåˆ—å›¾åˆ†ç±»å™¨ï¼ˆPAG-Classifierï¼‰ï¼Œå®ƒåˆ©ç”¨åŸºäºå›¾çš„æ¡†æ¶ä¸­ç›¸é‚»3Dåƒç´ ä¹‹é—´çš„ç©ºé—´å…³ç³»ï¼Œé€šè¿‡ç‰¹å¾åˆ†è§£æå–é‡è¦ç‰¹å¾ã€‚ç„¶åï¼Œè¿™äº›ç‰¹å¾è¢«ç”¨æ¥è®­ç»ƒæœºå™¨å­¦ä¹ åˆ†ç±»å™¨ï¼Œä»¥å®ç°æœ‰æ•ˆçš„æ‚£è€…åˆ†ç±»ã€‚æˆ‘ä»¬ä½¿ç”¨åŸºäºæ”¾å°„ç»„å­¦çš„æ–¹æ³•éªŒè¯äº†æˆ‘ä»¬çš„æ–¹æ³•ï¼Œæ ¹æ®ç›¸å…³çš„æ”¾å°„å­¦ç‰¹å¾å¯¹TBMæ‚£è€…è¿›è¡Œåˆ†ç±»ã€‚ç»“æœï¼šæˆ‘ä»¬ä½¿ç”¨äº†å†…éƒ¨æ•°æ®é›†ï¼ŒåŒ…æ‹¬52æ¬¡æ‰«æï¼Œå…¶ä¸­32æ¬¡æ¥è‡ªç»è„‘è„Šæ¶²ä¸­åˆ†ææ†èŒæ£€æµ‹ç¡®è®¤çš„TBMæ‚£è€…ï¼Œ20æ¬¡æ¥è‡ªå¥åº·ä¸ªä½“ã€‚ä½¿ç”¨æˆ‘ä»¬çš„PAG-Classifierï¼Œåœ¨æ± åŒºå®ç°äº†å¹³å‡F1åˆ†æ•°ä¸º85.71%çš„5å€äº¤å‰éªŒè¯ï¼Œä½¿ç”¨æ”¾å°„å­¦ç‰¹å¾åˆ†ç±»å™¨è¾¾åˆ°äº†92.85%ï¼Œåˆ†åˆ«æ¯”å½“å‰æœ€æ–°æŠ€æœ¯é«˜å‡º15%å’Œ22%ã€‚ç„¶è€Œï¼Œéª¨éª¼å’Œèƒ¼èƒä½“åŒºåŸŸçš„åˆ†ç±»æ•ˆæœè¾ƒå·®ï¼Œå¹³å‡F1åˆ†æ•°ä½äº50%ã€‚ç»“è®ºï¼šæˆ‘ä»¬çš„ç ”ç©¶è¡¨æ˜ï¼ŒåƒPAG-Classifierè¿™æ ·çš„ç®—æ³•æ˜¯ç»“æ ¸æ€§è„‘è†œç‚æ— åˆ›åˆ†æçš„æœ‰æ•ˆå·¥å…·ï¼Œå°¤å…¶æ˜¯é€šè¿‡è„šé—´æ± è¿›è¡Œçš„åˆ†æã€‚ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œéª¨éª¼å’Œèƒ¼èƒä½“åŒºåŸŸç¼ºä¹æ˜æ˜¾çš„æ¨¡å¼æ¥è¿›è¡ŒåŒºåˆ†ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.00943v1">PDF</a> 19 pages, 6 figures, 3 tables</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ—¨åœ¨åˆ©ç”¨T1åŠ æƒéå¯¹æ¯”ç£å…±æŒ¯æˆåƒï¼ˆMRIï¼‰æ‰«æå¯¹ç»“æ ¸æ€§è„‘è†œç‚ï¼ˆTBMï¼‰æ‚£è€…è¿›è¡Œåˆ†ç±»ã€‚ç ”ç©¶æå‡ºäº†ä¸€ç§æ–°çš„Pixel-array Graphs Classifierï¼ˆPAG-Classifierï¼‰ï¼Œé€šè¿‡å›¾æ¡†æ¶ä¸­çš„é‚»æ¥ä¸‰ç»´åƒç´ çš„ç©ºé—´å…³ç³»æå–ç‰¹å¾ï¼Œå¹¶é€šè¿‡ç‰¹å¾åˆ†è§£è¿›è¡Œè®­ç»ƒæœºå™¨å­¦ä¹ åˆ†ç±»å™¨ï¼Œä»¥æœ‰æ•ˆåœ°å¯¹æ‚£è€…è¿›è¡Œåˆ†ç±»ã€‚ä½¿ç”¨åŸºäºæ”¾å°„å­¦çš„æ–¹æ³•å¯¹TBMæ‚£è€…è¿›è¡Œåˆ†ç±»éªŒè¯æ—¶ï¼Œç»“æœæ˜¾ç¤ºåœ¨æŸäº›åŒºåŸŸï¼Œå¦‚è„‘æ¡¥å°è„‘èš“éƒ¨åŒºåŸŸçš„å¹³å‡F1åˆ†æ•°è¾¾åˆ°85.71%ï¼Œè€Œæ”¾å°„å­¦ç‰¹å¾åˆ†ç±»å™¨çš„å¹³å‡F1åˆ†æ•°ä¸º92.85%ã€‚ä½†éª¨å’Œèƒ¼èƒä½“åŒºåŸŸåˆ†ç±»æ•ˆæœä¸ä½³ï¼Œå¹³å‡F1å¾—åˆ†ä½äº50%ã€‚æ€»ä½“è€Œè¨€ï¼Œè¯¥ç ”ç©¶å»ºè®®ï¼ŒåƒPAG-Classifierè¿™æ ·çš„ç®—æ³•å¯ä»¥ä½œä¸ºéä¾µå…¥æ€§TBMåˆ†æçš„æœ‰æ•ˆå·¥å…·ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>TBMæ˜¯ä¸€ç§ç”±ç»“æ ¸åˆ†ææ†èŒå¼•èµ·çš„ä¸¥é‡è„‘éƒ¨æ„ŸæŸ“ï¼Œé€šå¸¸è¡¨ç°ä¸ºè„‘è†œå’Œè„Šé«“çš„ç‚ç—‡ã€‚</li>
<li>ç ”ç©¶ä½¿ç”¨T1åŠ æƒMRIæ‰«æå¯¹TBMæ‚£è€…è¿›è¡Œåˆ†ç±»ï¼Œæå‡ºäº†ä¸€ç§æ–°çš„Pixel-array Graphs Classifierï¼ˆPAG-Classifierï¼‰ã€‚</li>
<li>PAG-Classifieré€šè¿‡åˆ©ç”¨é‚»æ¥ä¸‰ç»´åƒç´ çš„ç©ºé—´å…³ç³»æå–ç‰¹å¾ï¼Œå¹¶ç”¨äºè®­ç»ƒæœºå™¨å­¦ä¹ åˆ†ç±»å™¨ã€‚</li>
<li>åœ¨è„‘æ¡¥å°è„‘èš“éƒ¨åŒºåŸŸï¼ŒPAG-Classifierçš„å¹³å‡F1åˆ†æ•°è¾¾åˆ°85.71%ï¼Œæ”¾å°„å­¦ç‰¹å¾åˆ†ç±»å™¨çš„å¹³å‡F1åˆ†æ•°ä¸º92.85%ï¼Œè¶…è¿‡äº†å½“å‰çš„æœ€ä½³æ°´å¹³ã€‚</li>
<li>éª¨å’Œèƒ¼èƒä½“åŒºåŸŸåœ¨åˆ†ç±»ä¸­çš„è¡¨ç°è¾ƒå·®ï¼Œå¹³å‡F1å¾—åˆ†ä½äº50%ã€‚</li>
<li>è¯¥ç ”ç©¶è¯æ˜äº†åƒPAG-Classifierè¿™æ ·çš„ç®—æ³•å¯ä»¥ä½œä¸ºéä¾µå…¥æ€§TBMåˆ†æçš„æœ‰æ•ˆå·¥å…·ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.00943">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-18256e5b6ac07021aedfa7263e285a88.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-3afda935c17c1c7e5b4dc12063fa30c4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0500e63fe4a33b7ef3422c7c6676b993.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Balancing-Multi-Target-Semi-Supervised-Medical-Image-Segmentation-with-Collaborative-Generalist-and-Specialists"><a href="#Balancing-Multi-Target-Semi-Supervised-Medical-Image-Segmentation-with-Collaborative-Generalist-and-Specialists" class="headerlink" title="Balancing Multi-Target Semi-Supervised Medical Image Segmentation with   Collaborative Generalist and Specialists"></a>Balancing Multi-Target Semi-Supervised Medical Image Segmentation with   Collaborative Generalist and Specialists</h2><p><strong>Authors:You Wang, Zekun Li, Lei Qi, Qian Yu, Yinghuan Shi, Yang Gao</strong></p>
<p>Despite the promising performance achieved by current semi-supervised models in segmenting individual medical targets, many of these models suffer a notable decrease in performance when tasked with the simultaneous segmentation of multiple targets. A vital factor could be attributed to the imbalanced scales among different targets: during simultaneously segmenting multiple targets, large targets dominate the loss, leading to small targets being misclassified as larger ones. To this end, we propose a novel method, which consists of a Collaborative Generalist and several Specialists, termed CGS. It is centered around the idea of employing a specialist for each target class, thus avoiding the dominance of larger targets. The generalist performs conventional multi-target segmentation, while each specialist is dedicated to distinguishing a specific target class from the remaining target classes and the background. Based on a theoretical insight, we demonstrate that CGS can achieve a more balanced training. Moreover, we develop cross-consistency losses to foster collaborative learning between the generalist and the specialists. Lastly, regarding their intrinsic relation that the target class of any specialized head should belong to the remaining classes of the other heads, we introduce an inter-head error detection module to further enhance the quality of pseudo-labels. Experimental results on three popular benchmarks showcase its superior performance compared to state-of-the-art methods. </p>
<blockquote>
<p>å°½ç®¡å½“å‰åŠç›‘ç£æ¨¡å‹åœ¨åˆ†å‰²å•ä¸ªåŒ»å­¦ç›®æ ‡æ–¹é¢å–å¾—äº†ä»¤äººé¼“èˆçš„è¡¨ç°ï¼Œä½†å®ƒä»¬åœ¨åŒæ—¶åˆ†å‰²å¤šä¸ªç›®æ ‡æ—¶ï¼Œæ€§èƒ½æ˜¾è‘—ä¸‹é™ã€‚ä¸€ä¸ªé‡è¦çš„å› ç´ å¯èƒ½æ˜¯ç”±äºä¸åŒç›®æ ‡ä¹‹é—´çš„å°ºåº¦ä¸å¹³è¡¡ï¼šåœ¨åŒæ—¶åˆ†å‰²å¤šä¸ªç›®æ ‡æ—¶ï¼Œå¤§ç›®æ ‡ä¼šä¸»å¯¼æŸå¤±ï¼Œå¯¼è‡´å°ç›®æ ‡è¢«è¯¯åˆ†ç±»ä¸ºå¤§ç›®æ ‡ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°æ–¹æ³•ï¼Œå®ƒç”±é€šç”¨ä¸“å®¶ï¼ˆGeneralistï¼‰å’Œå¤šä¸ªä¸“ä¸šä¸“å®¶ï¼ˆSpecialistsï¼‰ç»„æˆï¼Œç§°ä¸ºCGSã€‚å®ƒå›´ç»•ä¸ºæ¯ä¸ªç›®æ ‡ç±»åˆ«é›‡ä½£ä¸€ä¸ªä¸“ä¸šä¸“å®¶çš„ç†å¿µï¼Œä»è€Œé¿å…å¤§ç›®æ ‡çš„ä¸»å¯¼ã€‚é€šç”¨ä¸“å®¶æ‰§è¡Œå¸¸è§„çš„å¤šç›®æ ‡åˆ†å‰²ï¼Œè€Œæ¯ä¸ªä¸“ä¸šä¸“å®¶è‡´åŠ›äºå°†å…¶ç‰¹å®šçš„ç›®æ ‡ç±»åˆ«ä¸å…¶ä½™ç›®æ ‡ç±»åˆ«å’ŒèƒŒæ™¯åŒºåˆ†å¼€ã€‚åŸºäºç†è®ºæ´å¯Ÿï¼Œæˆ‘ä»¬è¯æ˜CGSå¯ä»¥å®ç°æ›´å¹³è¡¡çš„è®­ç»ƒã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å¼€å‘äº†äº¤å‰ä¸€è‡´æ€§æŸå¤±ï¼Œä»¥ä¿ƒè¿›é€šç”¨ä¸“å®¶å’Œä¸“ä¸šä¸“å®¶ä¹‹é—´çš„åä½œå­¦ä¹ ã€‚æœ€åï¼Œé‰´äºä¸“ä¸šå¤´éƒ¨ç›®æ ‡ç±»åˆ«ä¸å…¶ä»–å¤´éƒ¨çš„å‰©ä½™ç±»åˆ«ä¹‹é—´å­˜åœ¨å†…åœ¨è”ç³»ï¼Œæˆ‘ä»¬å¼•å…¥äº†è·¨å¤´éƒ¨è¯¯å·®æ£€æµ‹æ¨¡å—ï¼Œä»¥è¿›ä¸€æ­¥æé«˜ä¼ªæ ‡ç­¾çš„è´¨é‡ã€‚åœ¨ä¸‰ä¸ªæµè¡ŒåŸºå‡†æµ‹è¯•ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼Œä¸æœ€å…ˆè¿›çš„æ–¹æ³•ç›¸æ¯”ï¼Œå…¶æ€§èƒ½å“è¶Šã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.00862v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºä¸€ç§åä¸ºCGSçš„æ–°å‹åŠç›‘ç£åŒ»å­¦å›¾åƒåˆ†å‰²æ–¹æ³•ï¼Œé€šè¿‡å¼•å…¥é€šç”¨ä¸“å®¶ï¼ˆgeneralistï¼‰å’Œå¤šä¸ªé’ˆå¯¹ç‰¹å®šç›®æ ‡çš„ä¸“å®¶ï¼ˆspecialistï¼‰æ¥è§£å†³åŒæ—¶åˆ†å‰²å¤šä¸ªç›®æ ‡æ—¶æ€§èƒ½ä¸‹é™çš„é—®é¢˜ã€‚è¯¥æ–¹æ³•é€šè¿‡ä¸ºæ¯ä¸ªç›®æ ‡ç±»åˆ«åˆ†é…ä¸€ä¸ªä¸“å®¶ï¼Œé¿å…äº†å¤§ç›®æ ‡çš„ä¸»å¯¼ä½œç”¨ï¼Œå®ç°äº†æ›´å¹³è¡¡çš„è®­ç»ƒï¼Œå¹¶åœ¨ä¸‰ä¸ªæµè¡ŒåŸºå‡†æµ‹è¯•ä¸Šè¡¨ç°å‡ºå“è¶Šæ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å½“å‰åŠç›‘ç£æ¨¡å‹åœ¨åŒæ—¶åˆ†å‰²å¤šä¸ªåŒ»å­¦ç›®æ ‡æ—¶æ€§èƒ½ä¸‹é™çš„é—®é¢˜ã€‚</li>
<li>é—®é¢˜åŸå› æ˜¯ä¸åŒç›®æ ‡ä¹‹é—´çš„å°ºåº¦å¤±è¡¡ï¼Œå¤§ç›®æ ‡åœ¨åŒæ—¶åˆ†å‰²å¤šä¸ªç›®æ ‡æ—¶å æ®ä¸»å¯¼ï¼Œå¯¼è‡´å°ç›®æ ‡è¢«è¯¯åˆ†ç±»ã€‚</li>
<li>æå‡ºçš„CGSæ–¹æ³•åŒ…æ‹¬ä¸€ä¸ªé€šç”¨ä¸“å®¶ï¼ˆè¿›è¡Œå¸¸è§„å¤šç›®æ ‡åˆ†å‰²ï¼‰å’Œå¤šä¸ªé’ˆå¯¹ç‰¹å®šç›®æ ‡çš„ä¸“å®¶ï¼ˆä¸“æ³¨äºå°†ç‰¹å®šç›®æ ‡ç±»åˆ«ä¸å…¶ä½™ç±»åˆ«å’ŒèƒŒæ™¯åŒºåˆ†å¼€ï¼‰ã€‚</li>
<li>CGSæ–¹æ³•é€šè¿‡å¼•å…¥äº¤å‰ä¸€è‡´æ€§æŸå¤±ï¼ˆcross-consistency lossesï¼‰ä¿ƒè¿›äº†é€šç”¨ä¸“å®¶ä¸ä¸“å®¶ä¹‹é—´çš„åä½œå­¦ä¹ ã€‚</li>
<li>CGSæ–¹æ³•å¼•å…¥äº†è·¨å¤´è¯¯å·®æ£€æµ‹æ¨¡å—ï¼ˆinter-head error detection moduleï¼‰ï¼Œä»¥æé«˜ä¼ªæ ‡ç­¾çš„è´¨é‡ã€‚</li>
<li>å®éªŒç»“æœè¡¨æ˜ï¼ŒCGSæ–¹æ³•åœ¨ä¸‰ä¸ªæµè¡ŒåŸºå‡†æµ‹è¯•ä¸Šçš„æ€§èƒ½ä¼˜äºç°æœ‰æ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.00862">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-f6177683d156bb704fc96e2af819c268.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-b350b7bd50f535d2b0d4eb0acbd9d818.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-8e46739987110efe6f739d1dc99f55e3.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d3a2b52b76122031619bee1d25bcd70a.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="SCFANet-Style-Distribution-Constraint-Feature-Alignment-Network-For-Pathological-Staining-Translation"><a href="#SCFANet-Style-Distribution-Constraint-Feature-Alignment-Network-For-Pathological-Staining-Translation" class="headerlink" title="SCFANet: Style Distribution Constraint Feature Alignment Network For   Pathological Staining Translation"></a>SCFANet: Style Distribution Constraint Feature Alignment Network For   Pathological Staining Translation</h2><p><strong>Authors:Zetong Chen, Yuzhuo Chen, Hai Zhong, Xu Qiao</strong></p>
<p>Immunohistochemical (IHC) staining serves as a valuable technique for detecting specific antigens or proteins through antibody-mediated visualization. However, the IHC staining process is both time-consuming and costly. To address these limitations, the application of deep learning models for direct translation of cost-effective Hematoxylin and Eosin (H&amp;E) stained images into IHC stained images has emerged as an efficient solution. Nevertheless, the conversion from H&amp;E to IHC images presents significant challenges, primarily due to alignment discrepancies between image pairs and the inherent diversity in IHC staining style patterns. To overcome these challenges, we propose the Style Distribution Constraint Feature Alignment Network (SCFANet), which incorporates two innovative modules: the Style Distribution Constrainer (SDC) and Feature Alignment Learning (FAL). The SDC ensures consistency between the generated and target imagesâ€™ style distributions while integrating cycle consistency loss to maintain structural consistency. To mitigate the complexity of direct image-to-image translation, the FAL module decomposes the end-to-end translation task into two subtasks: image reconstruction and feature alignment. Furthermore, we ensure pathological consistency between generated and target images by maintaining pathological pattern consistency and Optical Density (OD) uniformity. Extensive experiments conducted on the Breast Cancer Immunohistochemical (BCI) dataset demonstrate that our SCFANet model outperforms existing methods, achieving precise transformation of H&amp;E-stained images into their IHC-stained counterparts. The proposed approach not only addresses the technical challenges in H&amp;E to IHC image translation but also provides a robust framework for accurate and efficient stain conversion in pathological analysis. </p>
<blockquote>
<p>å…ç–«ç»„ç»‡åŒ–å­¦ï¼ˆIHCï¼‰æŸ“è‰²ä½œä¸ºä¸€ç§é€šè¿‡æŠ—ä½“ä»‹å¯¼çš„å¯è§†åŒ–æ£€æµ‹ç‰¹å®šæŠ—åŸæˆ–è›‹ç™½è´¨çš„æœ‰ä»·å€¼çš„æŠ€æœ¯ã€‚ç„¶è€Œï¼ŒIHCæŸ“è‰²è¿‡ç¨‹æ—¢è€—æ—¶åˆæˆæœ¬é«˜æ˜‚ã€‚ä¸ºäº†è§£å†³è¿™äº›å±€é™æ€§ï¼Œåº”ç”¨æ·±åº¦å­¦ä¹ æ¨¡å‹å°†ç»æµå®æƒ çš„è‹æœ¨ç²¾-ä¼Šçº¢ï¼ˆH&amp;Eï¼‰æŸ“è‰²å›¾åƒç›´æ¥ç¿»è¯‘ä¸ºIHCæŸ“è‰²å›¾åƒå·²æˆä¸ºä¸€ç§é«˜æ•ˆçš„è§£å†³æ–¹æ¡ˆã€‚ç„¶è€Œï¼Œä»H&amp;Eåˆ°IHCå›¾åƒçš„è½¬æ¢å­˜åœ¨é‡å¤§æŒ‘æˆ˜ï¼Œä¸»è¦æ˜¯å› ä¸ºå›¾åƒå¯¹ä¹‹é—´çš„å¯¹é½å·®å¼‚å’ŒIHCæŸ“è‰²é£æ ¼æ¨¡å¼çš„å›ºæœ‰å¤šæ ·æ€§ã€‚ä¸ºäº†å…‹æœè¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†é£æ ¼åˆ†å¸ƒçº¦æŸç‰¹å¾å¯¹é½ç½‘ç»œï¼ˆSCFANetï¼‰ï¼Œå®ƒåŒ…å«ä¸¤ä¸ªåˆ›æ–°æ¨¡å—ï¼šé£æ ¼åˆ†å¸ƒçº¦æŸå™¨ï¼ˆSDCï¼‰å’Œç‰¹å¾å¯¹é½å­¦ä¹ ï¼ˆFALï¼‰ã€‚SDCç¡®ä¿ç”Ÿæˆå›¾åƒå’Œç›®æ ‡å›¾åƒä¹‹é—´é£æ ¼åˆ†å¸ƒçš„ä¸€è‡´æ€§ï¼ŒåŒæ—¶ç»“åˆå¾ªç¯ä¸€è‡´æ€§æŸå¤±ä»¥ä¿æŒç»“æ„ä¸€è‡´æ€§ã€‚ä¸ºäº†å‡è½»ç›´æ¥å›¾åƒåˆ°å›¾åƒç¿»è¯‘çš„å¤æ‚æ€§ï¼ŒFALæ¨¡å—å°†ç«¯åˆ°ç«¯çš„ç¿»è¯‘ä»»åŠ¡åˆ†è§£ä¸ºä¸¤ä¸ªå­ä»»åŠ¡ï¼šå›¾åƒé‡å»ºå’Œç‰¹å¾å¯¹é½ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬é€šè¿‡ä¿æŒç—…ç†æ¨¡å¼çš„ä¸€è‡´æ€§å’Œå…‰å­¦å¯†åº¦ï¼ˆODï¼‰çš„å‡åŒ€æ€§ï¼Œç¡®ä¿ç”Ÿæˆå›¾åƒå’Œç›®æ ‡å›¾åƒä¹‹é—´çš„ç—…ç†ä¸€è‡´æ€§ã€‚åœ¨ä¹³è…ºç™Œå…ç–«ç»„ç»‡åŒ–å­¦ï¼ˆBCIï¼‰æ•°æ®é›†ä¸Šè¿›è¡Œçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„SCFANetæ¨¡å‹ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œå®ç°äº†H&amp;EæŸ“è‰²å›¾åƒå‘IHCæŸ“è‰²å›¾åƒçš„ç²¾ç¡®è½¬æ¢ã€‚æ‰€æå‡ºçš„æ–¹æ³•ä¸ä»…è§£å†³äº†H&amp;Eåˆ°IHCå›¾åƒè½¬æ¢ä¸­çš„æŠ€æœ¯æŒ‘æˆ˜ï¼Œè€Œä¸”ä¸ºç—…ç†åˆ†æä¸­çš„å‡†ç¡®å’Œé«˜æ•ˆæŸ“è‰²è½¬æ¢æä¾›äº†ç¨³å¥çš„æ¡†æ¶ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.00490v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>åŸºäºå…ç–«ç»„åŒ–æŸ“è‰²åœ¨ç—…ç†å­¦åˆ†æä¸­çš„é‡è¦æ€§ï¼Œä½†å…¶è€—æ—¶é•¿ã€æˆæœ¬é«˜çš„ç¼ºç‚¹ï¼Œç ”ç©¶äººå‘˜é€šè¿‡æ·±åº¦å­¦ä¹ æ¨¡å‹å®ç°åŸºäºH&amp;EæŸ“æ–™çš„å›¾åƒè½¬æ¢ä¸ºIHCæŸ“è‰²å›¾åƒçš„ç›´æ¥è½¬æ¢ã€‚æå‡ºäº†SCFANetæ¨¡å‹åŒ…å«SDCå’ŒFALä¸¤ä¸ªåˆ›æ–°æ¨¡å—ï¼Œèƒ½å¤Ÿåœ¨ç¡®ä¿é£æ ¼åˆ†å¸ƒå’Œç»“æ„ä¸€è‡´æ€§çš„åŸºç¡€ä¸Šï¼Œå®Œæˆä»H&amp;Eåˆ°IHCçš„å›¾åƒè½¬æ¢ï¼Œä¸”åœ¨ä¹³è…ºç™Œå…ç–«ç»„åŒ–æ•°æ®é›†ä¸Šçš„è¡¨ç°ä¼˜äºç°æœ‰æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>å…ç–«ç»„åŒ–æŸ“è‰²æ˜¯æ£€æµ‹ç‰¹å®šæŠ—åŸæˆ–è›‹ç™½è´¨çš„é‡è¦æŠ€æœ¯ï¼Œä½†å…¶è¿‡ç¨‹è€—æ—¶ä¸”æˆæœ¬è¾ƒé«˜ã€‚</li>
<li>æ·±åº¦å­¦ä¹ æ¨¡å‹è¢«åº”ç”¨äºå°†H&amp;EæŸ“è‰²å›¾åƒç›´æ¥è½¬æ¢ä¸ºIHCæŸ“è‰²å›¾åƒï¼Œä»¥è§£å†³è¿™ä¸€é—®é¢˜ã€‚</li>
<li>ä»H&amp;Eåˆ°IHCçš„å›¾åƒè½¬æ¢é¢ä¸´æŒ‘æˆ˜ï¼Œå¦‚å›¾åƒå¯¹é½å’ŒIHCæŸ“è‰²é£æ ¼çš„å¤šæ ·æ€§ã€‚</li>
<li>SCFANetæ¨¡å‹é€šè¿‡SDCå’ŒFALä¸¤ä¸ªæ¨¡å—å…‹æœè¿™äº›æŒ‘æˆ˜ï¼Œå®ç°é£æ ¼åˆ†å¸ƒå’Œç»“æ„çš„ä¸€è‡´æ€§ã€‚</li>
<li>SCFANetåœ¨ä¹³è…ºç™Œå…ç–«ç»„åŒ–æ•°æ®é›†ä¸Šçš„è¡¨ç°ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œå®ç°äº†ç²¾ç¡®çš„H&amp;EæŸ“è‰²å›¾åƒå‘IHCæŸ“è‰²å›¾åƒçš„è½¬æ¢ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.00490">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-a6c77991d1297560af1521e98891059e.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="Hybrid-Global-Local-Representation-with-Augmented-Spatial-Guidance-for-Zero-Shot-Referring-Image-Segmentation"><a href="#Hybrid-Global-Local-Representation-with-Augmented-Spatial-Guidance-for-Zero-Shot-Referring-Image-Segmentation" class="headerlink" title="Hybrid Global-Local Representation with Augmented Spatial Guidance for   Zero-Shot Referring Image Segmentation"></a>Hybrid Global-Local Representation with Augmented Spatial Guidance for   Zero-Shot Referring Image Segmentation</h2><p><strong>Authors:Ting Liu, Siyuan Li</strong></p>
<p>Recent advances in zero-shot referring image segmentation (RIS), driven by models such as the Segment Anything Model (SAM) and CLIP, have made substantial progress in aligning visual and textual information. Despite these successes, the extraction of precise and high-quality mask region representations remains a critical challenge, limiting the full potential of RIS tasks. In this paper, we introduce a training-free, hybrid global-local feature extraction approach that integrates detailed mask-specific features with contextual information from the surrounding area, enhancing mask region representation. To further strengthen alignment between mask regions and referring expressions, we propose a spatial guidance augmentation strategy that improves spatial coherence, which is essential for accurately localizing described areas. By incorporating multiple spatial cues, this approach facilitates more robust and precise referring segmentation. Extensive experiments on standard RIS benchmarks demonstrate that our method significantly outperforms existing zero-shot RIS models, achieving substantial performance gains. We believe our approach advances RIS tasks and establishes a versatile framework for region-text alignment, offering broader implications for cross-modal understanding and interaction. Code is available at <a target="_blank" rel="noopener" href="https://github.com/fhgyuanshen/HybridGL">https://github.com/fhgyuanshen/HybridGL</a> . </p>
<blockquote>
<p>è¿‘æœŸï¼Œé›¶æ ·æœ¬å›¾åƒåˆ†å‰²ï¼ˆZero-Shot Referring Image Segmentationï¼Œç®€ç§°RISï¼‰é¢†åŸŸåœ¨Segment Anything Modelï¼ˆSAMï¼‰å’ŒCLIPç­‰æ¨¡å‹çš„æ¨åŠ¨ä¸‹å–å¾—äº†é‡å¤§è¿›å±•ï¼Œå…¶åœ¨è§†è§‰å’Œæ–‡æœ¬ä¿¡æ¯çš„å¯¹é½æ–¹é¢å–å¾—äº†å®è´¨æ€§è¿›æ­¥ã€‚ç„¶è€Œï¼Œæå–ç²¾ç¡®é«˜è´¨é‡çš„æ©è†œåŒºåŸŸè¡¨ç¤ºä»ç„¶æ˜¯ä¸€ä¸ªå…³é”®æŒ‘æˆ˜ï¼Œé™åˆ¶äº†RISä»»åŠ¡çš„å…¨æ½œåŠ›ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ— è®­ç»ƒã€æ··åˆå…¨å±€-å±€éƒ¨ç‰¹å¾æå–æ–¹æ³•ï¼Œè¯¥æ–¹æ³•å°†è¯¦ç»†çš„æ©è†œç‰¹å®šç‰¹å¾ä¸å‘¨å›´åŒºåŸŸçš„ä¸Šæ–‡ä¿¡æ¯ç›¸ç»“åˆï¼Œå¢å¼ºäº†æ©è†œåŒºåŸŸè¡¨ç¤ºã€‚ä¸ºäº†è¿›ä¸€æ­¥åŠ å¼ºå¯¹æ©è†œåŒºåŸŸå’Œå‚ç…§è¡¨è¾¾å¼ä¹‹é—´çš„å¯¹é½ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ç©ºé—´å¼•å¯¼å¢å¼ºç­–ç•¥ï¼Œè¯¥ç­–ç•¥æé«˜äº†ç©ºé—´è¿è´¯æ€§ï¼Œå¯¹äºå‡†ç¡®å®šä½æè¿°åŒºåŸŸè‡³å…³é‡è¦ã€‚é€šè¿‡èå…¥å¤šç§ç©ºé—´çº¿ç´¢ï¼Œè¯¥æ–¹æ³•å®ç°äº†æ›´ç¨³å¥å’Œç²¾ç¡®çš„å‚ç…§åˆ†å‰²ã€‚åœ¨æ ‡å‡†çš„RISåŸºå‡†æµ‹è¯•ä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•æ˜¾è‘—ä¼˜äºç°æœ‰çš„é›¶æ ·æœ¬RISæ¨¡å‹ï¼Œå®ç°äº†å®è´¨æ€§çš„æ€§èƒ½æå‡ã€‚æˆ‘ä»¬ç›¸ä¿¡ï¼Œæˆ‘ä»¬çš„æ–¹æ³•æ¨åŠ¨äº†RISä»»åŠ¡çš„å‘å±•ï¼Œå¹¶å»ºç«‹äº†ç”¨äºåŒºåŸŸæ–‡æœ¬å¯¹é½çš„é€šç”¨æ¡†æ¶ï¼Œä¸ºè·¨æ¨¡æ€ç†è§£å’Œäº¤äº’æä¾›äº†æ›´å¹¿æ³›çš„å¯ç¤ºã€‚ä»£ç å·²å‘å¸ƒåœ¨<a target="_blank" rel="noopener" href="https://github.com/fhgyuanshen/HybridGL%E4%B8%8A%E3%80%82">https://github.com/fhgyuanshen/HybridGLä¸Šã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.00356v1">PDF</a> accepted to CVPR2025</p>
<p><strong>Summary</strong></p>
<p>è¿‘æœŸï¼Œé›¶æ ·æœ¬æŒ‡ä»£å›¾åƒåˆ†å‰²ï¼ˆRISï¼‰é¢†åŸŸåœ¨Segment Anything Modelï¼ˆSAMï¼‰å’ŒCLIPç­‰æ¨¡å‹çš„æ¨åŠ¨ä¸‹å–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œä½†åœ¨ç”Ÿæˆç²¾ç¡®é«˜è´¨é‡æ©è†œåŒºåŸŸè¡¨ç¤ºæ–¹é¢ä»å­˜åœ¨æŒ‘æˆ˜ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§æ— éœ€è®­ç»ƒçš„æ··åˆå…¨å±€å±€éƒ¨ç‰¹å¾æå–æ–¹æ³•ï¼Œè¯¥æ–¹æ³•ç»“åˆäº†æ©è†œç‰¹å®šç‰¹å¾ä¸å‘¨å›´åŒºåŸŸçš„ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼Œæ”¹è¿›äº†æ©è†œåŒºåŸŸè¡¨ç¤ºã€‚ä¸ºè¿›ä¸€æ­¥å¼ºåŒ–æ©è†œåŒºåŸŸå’ŒæŒ‡ä»£è¡¨è¾¾å¼ä¹‹é—´çš„å¯¹é½ï¼Œæœ¬æ–‡æå‡ºäº†ç©ºé—´å¼•å¯¼å¢å¼ºç­–ç•¥ï¼Œæé«˜äº†ç©ºé—´è¿è´¯æ€§ï¼Œæœ‰åŠ©äºå‡†ç¡®å®šä½æè¿°åŒºåŸŸã€‚é€šè¿‡èå…¥å¤šç§ç©ºé—´çº¿ç´¢ï¼Œè¯¥æ–¹æ³•å®ç°äº†æ›´ç¨³å¥ç²¾ç¡®çš„æŒ‡ä»£åˆ†å‰²ã€‚åœ¨æ ‡å‡†RISåŸºå‡†æµ‹è¯•ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•æ˜¾è‘—ä¼˜äºç°æœ‰é›¶æ ·æœ¬RISæ¨¡å‹ï¼Œå®ç°äº†æ€§èƒ½çš„å¤§å¹…æå‡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Segment Anything Model (SAM) å’Œ CLIP ç­‰æ¨¡å‹æ¨åŠ¨äº†é›¶æ ·æœ¬æŒ‡ä»£å›¾åƒåˆ†å‰²ï¼ˆRISï¼‰çš„è¿›æ­¥ã€‚</li>
<li>ç²¾ç¡®å’Œé«˜è´¨é‡çš„æ©è†œåŒºåŸŸè¡¨ç¤ºä»æ˜¯ RIS ä»»åŠ¡çš„å…³é”®æŒ‘æˆ˜ã€‚</li>
<li>æå‡ºäº†ä¸€ç§æ— éœ€è®­ç»ƒçš„æ··åˆå…¨å±€å±€éƒ¨ç‰¹å¾æå–æ–¹æ³•ï¼Œç»“åˆäº†æ©è†œç‰¹å®šç‰¹å¾å’Œä¸Šä¸‹æ–‡ä¿¡æ¯ã€‚</li>
<li>æå‡ºäº†ç©ºé—´å¼•å¯¼å¢å¼ºç­–ç•¥ï¼Œä»¥å¼ºåŒ–æ©è†œåŒºåŸŸå’ŒæŒ‡ä»£è¡¨è¾¾å¼ä¹‹é—´çš„å¯¹é½ï¼Œæé«˜ç©ºé—´è¿è´¯æ€§ã€‚</li>
<li>é€šè¿‡èå…¥å¤šç§ç©ºé—´çº¿ç´¢ï¼Œå®ç°äº†æ›´ç¨³å¥å’Œç²¾ç¡®çš„æŒ‡ä»£åˆ†å‰²ã€‚</li>
<li>åœ¨æ ‡å‡†RISåŸºå‡†æµ‹è¯•ä¸Šï¼Œè¯¥æ–¹æ³•æ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.00356">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-1e440c4c98584f6f46f31f5e2071c9f7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b2de8d8a1137b0424513bc056d8558fc.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c4f6e0fc09ef5f37fa33da133f858f72.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-48c8676920b79610dfadfb6d856e3c60.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="Deconver-A-Deconvolutional-Network-for-Medical-Image-Segmentation"><a href="#Deconver-A-Deconvolutional-Network-for-Medical-Image-Segmentation" class="headerlink" title="Deconver: A Deconvolutional Network for Medical Image Segmentation"></a>Deconver: A Deconvolutional Network for Medical Image Segmentation</h2><p><strong>Authors:Pooya Ashtari, Shahryar Noei, Fateme Nateghi Haredasht, Jonathan H. Chen, Giuseppe Jurman, Aleksandra Pizurica, Sabine Van Huffel</strong></p>
<p>While convolutional neural networks (CNNs) and vision transformers (ViTs) have advanced medical image segmentation, they face inherent limitations such as local receptive fields in CNNs and high computational complexity in ViTs. This paper introduces Deconver, a novel network that integrates traditional deconvolution techniques from image restoration as a core learnable component within a U-shaped architecture. Deconver replaces computationally expensive attention mechanisms with efficient nonnegative deconvolution (NDC) operations, enabling the restoration of high-frequency details while suppressing artifacts. Key innovations include a backpropagation-friendly NDC layer based on a provably monotonic update rule and a parameter-efficient design. Evaluated across four datasets (ISLESâ€™22, BraTSâ€™23, GlaS, FIVES) covering both 2D and 3D segmentation tasks, Deconver achieves state-of-the-art performance in Dice scores and Hausdorff distance while reducing computational costs (FLOPs) by up to 90% compared to leading baselines. By bridging traditional image restoration with deep learning, this work offers a practical solution for high-precision segmentation in resource-constrained clinical workflows. The project is available at <a target="_blank" rel="noopener" href="https://github.com/pashtari/deconver">https://github.com/pashtari/deconver</a>. </p>
<blockquote>
<p>å°½ç®¡å·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNsï¼‰å’Œè§†è§‰å˜å‹å™¨ï¼ˆViTsï¼‰å·²ç»æ¨åŠ¨äº†åŒ»å­¦å›¾åƒåˆ†å‰²çš„å‘å±•ï¼Œä½†å®ƒä»¬ä»é¢ä¸´å›ºæœ‰çš„å±€é™æ€§ï¼Œä¾‹å¦‚CNNçš„å±€éƒ¨æ„Ÿå—é‡å’ŒViTçš„é«˜è®¡ç®—å¤æ‚åº¦ã€‚æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åä¸ºDeconverçš„æ–°å‹ç½‘ç»œï¼Œå®ƒå°†å›¾åƒæ¢å¤ä¸­çš„ä¼ ç»Ÿåå·ç§¯æŠ€æœ¯é›†æˆåˆ°ä¸€ä¸ªUå½¢æ¶æ„ä¸­çš„æ ¸å¿ƒå¯å­¦ä¹ ç»„ä»¶ã€‚Deconverç”¨é«˜æ•ˆçš„éè´Ÿåå·ç§¯ï¼ˆNDCï¼‰æ“ä½œå–ä»£äº†è®¡ç®—æ˜‚è´µçš„æ³¨æ„åŠ›æœºåˆ¶ï¼Œèƒ½å¤Ÿåœ¨æ¢å¤é«˜é¢‘ç»†èŠ‚çš„åŒæ—¶æŠ‘åˆ¶ä¼ªå½±ã€‚ä¸»è¦åˆ›æ–°åŒ…æ‹¬åŸºäºå¯è¯æ˜çš„å•è°ƒæ›´æ–°è§„åˆ™çš„åå‘ä¼ æ’­å‹å¥½çš„NDCå±‚ä»¥åŠå‚æ•°é«˜æ•ˆçš„è®¾è®¡ã€‚åœ¨æ¶µç›–2Då’Œ3Dåˆ†å‰²ä»»åŠ¡çš„å››ä¸ªæ•°æ®é›†ï¼ˆISLESâ€™22ã€BraTSâ€™23ã€GlaSã€FIVESï¼‰ä¸Šè¿›è¡Œäº†è¯„ä¼°ï¼ŒDeconveråœ¨Diceå¾—åˆ†å’ŒHausdorffè·ç¦»æ–¹é¢è¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œä¸é¢†å…ˆçš„åŸºçº¿ç›¸æ¯”ï¼Œè®¡ç®—æˆæœ¬ï¼ˆFLOPsï¼‰é™ä½äº†é«˜è¾¾90%ã€‚é€šè¿‡æ¡¥æ¥ä¼ ç»Ÿå›¾åƒæ¢å¤å’Œæ·±åº¦å­¦ä¹ ï¼Œè¿™é¡¹å·¥ä½œä¸ºèµ„æºå—é™çš„ä¸´åºŠå·¥ä½œæµç¨‹ä¸­çš„é«˜ç²¾åº¦åˆ†å‰²æä¾›äº†å®ç”¨è§£å†³æ–¹æ¡ˆã€‚è¯¥é¡¹ç›®å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/pashtari/deconver%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/pashtari/deconverè·å–ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.00302v1">PDF</a> 12 pages, 6 figures, 5 tables</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºDeconverçš„æ–°å‹ç½‘ç»œï¼Œè¯¥ç½‘ç»œå°†ä¼ ç»Ÿå›¾åƒæ¢å¤ä¸­çš„åå·ç§¯æŠ€æœ¯ä½œä¸ºæ ¸å¿ƒå­¦ä¹ ç»„ä»¶ï¼Œé›†æˆåˆ°Uå½¢æ¶æ„ä¸­ã€‚å®ƒä½¿ç”¨é«˜æ•ˆçš„éè´Ÿåå·ç§¯ï¼ˆNDCï¼‰æ“ä½œæ›¿ä»£äº†è®¡ç®—æˆæœ¬é«˜æ˜‚çš„æ³¨æ„åŠ›æœºåˆ¶ï¼Œèƒ½å¤Ÿåœ¨æ¢å¤é«˜é¢‘ç»†èŠ‚çš„åŒæ—¶æŠ‘åˆ¶ä¼ªå½±ã€‚åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šè¯„ä¼°ï¼ŒDeconveråœ¨Diceå¾—åˆ†å’ŒHausdorffè·ç¦»æ–¹é¢è¾¾åˆ°äº†æœ€æ–°æŠ€æœ¯æ°´å¹³ï¼Œå¹¶é™ä½äº†é«˜è¾¾90%çš„è®¡ç®—æˆæœ¬ã€‚è¯¥ç ”ç©¶ä¸ºèµ„æºå—é™çš„ä¸´åºŠå·¥ä½œæµç¨‹æä¾›äº†é«˜ç²¾åº¦åˆ†å‰²çš„å®ç”¨è§£å†³æ–¹æ¡ˆã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Deconverç½‘ç»œç»“åˆäº†ä¼ ç»Ÿå›¾åƒæ¢å¤ä¸­çš„åå·ç§¯æŠ€æœ¯ï¼Œè§£å†³äº†CNNå’ŒViTåœ¨åŒ»å­¦å›¾åƒåˆ†å‰²ä¸­çš„å±€é™æ€§ã€‚</li>
<li>Deconverä½¿ç”¨éè´Ÿåå·ç§¯ï¼ˆNDCï¼‰æ“ä½œæ›¿ä»£äº†è®¡ç®—æˆæœ¬é«˜æ˜‚çš„æ³¨æ„åŠ›æœºåˆ¶ã€‚</li>
<li>NDCæ“ä½œåŸºäºå¯è¯æ˜çš„å•è°ƒæ›´æ–°è§„åˆ™ï¼Œä¾¿äºåå‘ä¼ æ’­ã€‚</li>
<li>Deconverå®ç°äº†é«˜æ•ˆçš„å‚æ•°è®¾è®¡ã€‚</li>
<li>åœ¨å››ä¸ªæ•°æ®é›†ä¸Šè¯„ä¼°ï¼ŒDeconveråœ¨Diceå¾—åˆ†å’ŒHausdorffè·ç¦»æ–¹é¢è¾¾åˆ°äº†æœ€æ–°æŠ€æœ¯æ°´å¹³ã€‚</li>
<li>Deconveré™ä½äº†é«˜è¾¾90%çš„è®¡ç®—æˆæœ¬ï¼Œé€‚ç”¨äºèµ„æºå—é™çš„ç¯å¢ƒã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.00302">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-45333f64721b449e41e5de5130537ed8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-bf09399a0fad0372088d1325bd1049f8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2a8a7a4086364823041948675c7cb597.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5fd0720d589f11ae109f58dcbb423620.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="DiffDenoise-Self-Supervised-Medical-Image-Denoising-with-Conditional-Diffusion-Models"><a href="#DiffDenoise-Self-Supervised-Medical-Image-Denoising-with-Conditional-Diffusion-Models" class="headerlink" title="DiffDenoise: Self-Supervised Medical Image Denoising with Conditional   Diffusion Models"></a>DiffDenoise: Self-Supervised Medical Image Denoising with Conditional   Diffusion Models</h2><p><strong>Authors:Basar Demir, Yikang Liu, Xiao Chen, Eric Z. Chen, Lin Zhao, Boris Mailhe, Terrence Chen, Shanhui Sun</strong></p>
<p>Many self-supervised denoising approaches have been proposed in recent years. However, these methods tend to overly smooth images, resulting in the loss of fine structures that are essential for medical applications. In this paper, we propose DiffDenoise, a powerful self-supervised denoising approach tailored for medical images, designed to preserve high-frequency details. Our approach comprises three stages. First, we train a diffusion model on noisy images, using the outputs of a pretrained Blind-Spot Network as conditioning inputs. Next, we introduce a novel stabilized reverse sampling technique, which generates clean images by averaging diffusion sampling outputs initialized with a pair of symmetric noises. Finally, we train a supervised denoising network using noisy images paired with the denoised outputs generated by the diffusion model. Our results demonstrate that DiffDenoise outperforms existing state-of-the-art methods in both synthetic and real-world medical image denoising tasks. We provide both a theoretical foundation and practical insights, demonstrating the methodâ€™s effectiveness across various medical imaging modalities and anatomical structures. </p>
<blockquote>
<p>è¿‘å¹´æ¥å·²ç»æå‡ºäº†è®¸å¤šè‡ªç›‘ç£é™å™ªæ–¹æ³•ã€‚ç„¶è€Œï¼Œè¿™äº›æ–¹æ³•å¾€å¾€è¿‡åº¦å¹³æ»‘å›¾åƒï¼Œå¯¼è‡´ä¸¢å¤±å¯¹åŒ»ç–—åº”ç”¨è‡³å…³é‡è¦çš„ç²¾ç»†ç»“æ„ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†DiffDenoiseï¼Œè¿™æ˜¯ä¸€ç§é’ˆå¯¹åŒ»å­¦å›¾åƒè®¾è®¡çš„å¼ºå¤§è‡ªç›‘ç£é™å™ªæ–¹æ³•ï¼Œæ—¨åœ¨ä¿ç•™é«˜é¢‘ç»†èŠ‚ã€‚æˆ‘ä»¬çš„æ–¹æ³•åˆ†ä¸ºä¸‰ä¸ªæ­¥éª¤ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬åœ¨å™ªå£°å›¾åƒä¸Šè®­ç»ƒæ‰©æ•£æ¨¡å‹ï¼Œä½¿ç”¨é¢„è®­ç»ƒçš„ç›²ç‚¹ç½‘ç»œçš„è¾“å‡ºæ¥ä½œä¸ºæ¡ä»¶è¾“å…¥ã€‚æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§æ–°é¢–çš„ç¨³å®šåå‘é‡‡æ ·æŠ€æœ¯ï¼Œè¯¥æŠ€æœ¯é€šè¿‡å¹³å‡ä½¿ç”¨ä¸€å¯¹å¯¹ç§°å™ªå£°åˆå§‹åŒ–çš„æ‰©æ•£é‡‡æ ·è¾“å‡ºæ¥ç”Ÿæˆå¹²å‡€å›¾åƒã€‚æœ€åï¼Œæˆ‘ä»¬ä½¿ç”¨ä¸æ‰©æ•£æ¨¡å‹ç”Ÿæˆçš„é™å™ªè¾“å‡ºé…å¯¹çš„å™ªå£°å›¾åƒæ¥è®­ç»ƒä¸€ä¸ªç›‘ç£é™å™ªç½‘ç»œã€‚æˆ‘ä»¬çš„ç»“æœè¡¨æ˜ï¼Œæ— è®ºæ˜¯åœ¨åˆæˆè¿˜æ˜¯åœ¨çœŸå®ä¸–ç•Œçš„åŒ»å­¦å›¾åƒé™å™ªä»»åŠ¡ä¸­ï¼ŒDiffDenoiseéƒ½ä¼˜äºç°æœ‰çš„æœ€å…ˆè¿›æ–¹æ³•ã€‚æˆ‘ä»¬æ—¢æä¾›äº†ç†è®ºåŸºç¡€ï¼Œä¹Ÿæä¾›äº†å®è·µè§è§£ï¼Œè¯æ˜äº†è¯¥æ–¹æ³•åœ¨å„ç§åŒ»å­¦æˆåƒæ¨¡å¼å’Œè§£å‰–ç»“æ„ä¸­çš„æœ‰æ•ˆæ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.00264v1">PDF</a> </p>
<p><strong>æ‘˜è¦</strong></p>
<p>æœ¬æ–‡æå‡ºä¸€ç§é’ˆå¯¹åŒ»å­¦å›¾åƒçš„å¼ºå¤§è‡ªç›‘ç£å»å™ªæ–¹æ³•â€”â€”DiffDenoiseï¼Œæ—¨åœ¨ä¿ç•™é«˜é¢‘ç»†èŠ‚ã€‚è¯¥æ–¹æ³•åŒ…æ‹¬ä¸‰ä¸ªé˜¶æ®µï¼šé¦–å…ˆï¼Œä½¿ç”¨é¢„è®­ç»ƒçš„ç›²ç‚¹ç½‘ç»œè¾“å‡ºä½œä¸ºæ¡ä»¶è¾“å…¥ï¼Œå¯¹å™ªå£°å›¾åƒè¿›è¡Œæ‰©æ•£æ¨¡å‹è®­ç»ƒï¼›æ¥ç€ï¼Œå¼•å…¥ä¸€ç§æ–°é¢–çš„ç¨³å®šé€†å‘é‡‡æ ·æŠ€æœ¯ï¼Œé€šè¿‡å¹³å‡æ‰©æ•£é‡‡æ ·è¾“å‡ºå¹¶åˆå§‹åŒ–ä¸ºä¸€å¯¹å¯¹ç§°å™ªå£°æ¥ç”Ÿæˆå¹²å‡€å›¾åƒï¼›æœ€åï¼Œä½¿ç”¨ä¸æ‰©æ•£æ¨¡å‹ç”Ÿæˆçš„å»å™ªè¾“å‡ºé…å¯¹çš„å™ªå£°å›¾åƒï¼Œè®­ç»ƒä¸€ä¸ªç›‘ç£å»å™ªç½‘ç»œã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒDiffDenoiseåœ¨åˆæˆå’ŒçœŸå®ä¸–ç•ŒåŒ»å­¦å›¾åƒå»å™ªä»»åŠ¡ä¸­å‡ä¼˜äºç°æœ‰æœ€å…ˆè¿›çš„æ–¹æ³•ï¼Œå¹¶æä¾›äº†ç†è®ºæ¡†æ¶å’Œå®è·µè§è§£ï¼Œè¯æ˜è¯¥æ–¹æ³•åœ¨ä¸åŒåŒ»å­¦æˆåƒæ¨¡æ€å’Œè§£å‰–ç»“æ„ä¸­çš„æœ‰æ•ˆæ€§ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>DiffDenoiseæ˜¯ä¸€ç§é’ˆå¯¹åŒ»å­¦å›¾åƒè®¾è®¡çš„è‡ªç›‘ç£å»å™ªæ–¹æ³•ï¼Œæ—¨åœ¨ä¿ç•™é«˜é¢‘ç»†èŠ‚ï¼Œè§£å†³ç°æœ‰æ–¹æ³•è¿‡åº¦å¹³æ»‘å›¾åƒçš„é—®é¢˜ã€‚</li>
<li>æ–¹æ³•åŒ…æ‹¬ä¸‰ä¸ªé˜¶æ®µï¼šæ‰©æ•£æ¨¡å‹è®­ç»ƒã€ç¨³å®šé€†å‘é‡‡æ ·æŠ€æœ¯ã€ç›‘ç£å»å™ªç½‘ç»œè®­ç»ƒã€‚</li>
<li>ä½¿ç”¨é¢„è®­ç»ƒçš„ç›²ç‚¹ç½‘ç»œè¾“å‡ºä½œä¸ºæ¡ä»¶è¾“å…¥ï¼Œå¯¹å™ªå£°å›¾åƒè¿›è¡Œæ‰©æ•£æ¨¡å‹è®­ç»ƒã€‚</li>
<li>å¼•å…¥æ–°é¢–çš„ç¨³å®šé€†å‘é‡‡æ ·æŠ€æœ¯ï¼Œé€šè¿‡å¹³å‡æ‰©æ•£é‡‡æ ·è¾“å‡ºå¹¶åˆå§‹åŒ–ä¸ºå¯¹ç§°å™ªå£°å¯¹æ¥ç”Ÿæˆå¹²å‡€å›¾åƒã€‚</li>
<li>DiffDenoiseåœ¨åˆæˆå’ŒçœŸå®ä¸–ç•ŒåŒ»å­¦å›¾åƒå»å™ªä»»åŠ¡ä¸­è¡¨ç°ä¼˜å¼‚ã€‚</li>
<li>æä¾›ç†è®ºæ¡†æ¶å’Œå®è·µè§è§£ï¼Œè¯æ˜è¯¥æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚</li>
<li>DiffDenoiseæ–¹æ³•é€‚ç”¨äºå„ç§åŒ»å­¦æˆåƒæ¨¡æ€å’Œè§£å‰–ç»“æ„ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.00264">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-d060fa7e0cf9a99d8116b579a68c0b11.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-cb3b6f3635d1eecc5dacd6980ba7b48c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6e7076a4f8670e51111dc0215d03e1a0.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-1d1a7835f99e1d47a61426cf2eeb58a6.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="Adapting-Vision-Foundation-Models-for-Real-time-Ultrasound-Image-Segmentation"><a href="#Adapting-Vision-Foundation-Models-for-Real-time-Ultrasound-Image-Segmentation" class="headerlink" title="Adapting Vision Foundation Models for Real-time Ultrasound Image   Segmentation"></a>Adapting Vision Foundation Models for Real-time Ultrasound Image   Segmentation</h2><p><strong>Authors:Xiaoran Zhang, Eric Z. Chen, Lin Zhao, Xiao Chen, Yikang Liu, Boris Maihe, James S. Duncan, Terrence Chen, Shanhui Sun</strong></p>
<p>We propose a novel approach that adapts hierarchical vision foundation models for real-time ultrasound image segmentation. Existing ultrasound segmentation methods often struggle with adaptability to new tasks, relying on costly manual annotations, while real-time approaches generally fail to match state-of-the-art performance. To overcome these limitations, we introduce an adaptive framework that leverages the vision foundation model Hiera to extract multi-scale features, interleaved with DINOv2 representations to enhance visual expressiveness. These enriched features are then decoded to produce precise and robust segmentation. We conduct extensive evaluations on six public datasets and one in-house dataset, covering both cardiac and thyroid ultrasound segmentation. Experiments show that our approach outperforms state-of-the-art methods across multiple datasets and excels with limited supervision, surpassing nnUNet by over 20% on average in the 1% and 10% data settings. Our method achieves $\sim$77 FPS inference speed with TensorRT on a single GPU, enabling real-time clinical applications. </p>
<blockquote>
<p>æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°å‹æ–¹æ³•ï¼Œè¯¥æ–¹æ³•å°†å±‚æ¬¡åŒ–çš„è§†è§‰åŸºç¡€æ¨¡å‹åº”ç”¨äºå®æ—¶è¶…å£°å›¾åƒåˆ†å‰²ã€‚ç°æœ‰çš„è¶…å£°åˆ†å‰²æ–¹æ³•å¾€å¾€éš¾ä»¥é€‚åº”æ–°ä»»åŠ¡ï¼Œå¹¶ä¾èµ–äºæ˜‚è´µçš„æ‰‹åŠ¨æ ‡æ³¨ï¼Œè€Œå®æ—¶æ–¹æ³•é€šå¸¸æ— æ³•è¾¾åˆ°æœ€æ–°æŠ€æœ¯çš„æ€§èƒ½æ°´å¹³ã€‚ä¸ºäº†å…‹æœè¿™äº›å±€é™æ€§ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ªè‡ªé€‚åº”æ¡†æ¶ï¼Œè¯¥æ¡†æ¶åˆ©ç”¨è§†è§‰åŸºç¡€æ¨¡å‹Hieraæå–å¤šå°ºåº¦ç‰¹å¾ï¼Œå¹¶ä¸DINOv2è¡¨ç¤ºäº¤æ›¿ä½¿ç”¨ï¼Œä»¥å¢å¼ºè§†è§‰è¡¨ç°åŠ›ã€‚è¿™äº›ä¸°å¯Œçš„ç‰¹å¾éšåè¢«è§£ç ä»¥äº§ç”Ÿç²¾ç¡®ä¸”ç¨³å®šçš„åˆ†å‰²ã€‚æˆ‘ä»¬åœ¨å…­ä¸ªå…¬å¼€æ•°æ®é›†å’Œä¸€ä¸ªå†…éƒ¨æ•°æ®é›†ä¸Šè¿›è¡Œäº†å¹¿æ³›è¯„ä¼°ï¼Œæ¶µç›–äº†å¿ƒè„å’Œç”²çŠ¶è…ºè¶…å£°åˆ†å‰²ã€‚å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨å¤šæ•°æ®é›†ä¸Šä¼˜äºæœ€æ–°æŠ€æœ¯çš„æ–¹æ³•ï¼Œå¹¶åœ¨æœ‰é™ç›‘ç£ä¸‹è¡¨ç°å‡ºè‰²ï¼Œåœ¨1%å’Œ10%çš„æ•°æ®è®¾ç½®ä¸‹å¹³å‡è¶…å‡ºnnUNetè¶…è¿‡20%ã€‚æˆ‘ä»¬çš„æ–¹æ³•åœ¨å•ä¸ªGPUä¸Šä½¿ç”¨TensorRTè¾¾åˆ°äº†çº¦77 FPSçš„æ¨ç†é€Ÿåº¦ï¼Œå¯å®ç°å®æ—¶ä¸´åºŠåº”ç”¨ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.24368v1">PDF</a> </p>
<p><strong>Summary</strong><br>     æœ¬æ–‡æå‡ºä¸€ç§æ–°å‹æ–¹æ³•ï¼Œé‡‡ç”¨åˆ†å±‚è§†è§‰åŸºç¡€æ¨¡å‹è¿›è¡Œå®æ—¶è¶…å£°å›¾åƒåˆ†å‰²ã€‚è¯¥æ–¹æ³•å…‹æœäº†ç°æœ‰è¶…å£°åˆ†å‰²æ–¹æ³•å¯¹æ–°ä»»åŠ¡é€‚åº”æ€§å·®ã€ä¾èµ–æ˜‚è´µæ‰‹åŠ¨æ ‡æ³¨çš„å±€é™æ€§ï¼Œä»¥åŠå®æ—¶æ–¹æ³•éš¾ä»¥è¾¾åˆ°æœ€æ–°æ€§èƒ½çš„ä¸è¶³ã€‚é€šè¿‡å¼•å…¥è‡ªé€‚åº”æ¡†æ¶ï¼Œåˆ©ç”¨è§†è§‰åŸºç¡€æ¨¡å‹Hieraæå–å¤šå°ºåº¦ç‰¹å¾ï¼Œå¹¶ä¸DINOv2è¡¨ç¤ºè¿›è¡Œäº¤æ›¿ï¼Œæé«˜è§†è§‰è¡¨ç°åŠ›ã€‚ç»è¿‡å¹¿æ³›è¯„ä¼°ï¼Œè¯¥æ–¹æ³•åœ¨å¤šæ•°æ®é›†ä¸Šè¡¨ç°ä¼˜äºæœ€æ–°æ–¹æ³•ï¼Œå¹¶åœ¨æœ‰é™ç›‘ç£ä¸‹è¡¨ç°å°¤ä¸ºå‡ºè‰²ï¼Œåœ¨1%å’Œ10%æ•°æ®é›†è®¾ç½®ä¸Šå¹³å‡è¶…è¿‡nnUNetè¶…è¿‡20%ã€‚è¯¥æ–¹æ³•åœ¨å•ä¸ªGPUä¸Šä½¿ç”¨TensorRTè¾¾åˆ°çº¦77 FPSçš„æ¨ç†é€Ÿåº¦ï¼Œé€‚ç”¨äºå®æ—¶ä¸´åºŠåº”ç”¨ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æå‡ºä¸€ç§æ–°å‹è¶…å£°å›¾åƒåˆ†å‰²æ–¹æ³•ï¼Œé‡‡ç”¨åˆ†å±‚è§†è§‰åŸºç¡€æ¨¡å‹ã€‚</li>
<li>å¼•å…¥è‡ªé€‚åº”æ¡†æ¶ï¼Œç»“åˆHieraå’ŒDINOv2æŠ€æœ¯ï¼Œæé«˜å›¾åƒåˆ†å‰²ç²¾åº¦å’Œé²æ£’æ€§ã€‚</li>
<li>æ–¹æ³•åœ¨å¤šæ•°æ®é›†ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œç‰¹åˆ«æ˜¯åœ¨æœ‰é™ç›‘ç£ä¸‹ã€‚</li>
<li>ä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼Œæ–°æ–¹æ³•åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šçš„æ€§èƒ½å¹³å‡æé«˜è¶…è¿‡20%ã€‚</li>
<li>æ–¹æ³•å®ç°å®æ—¶æ¨ç†ï¼Œé€‚ç”¨äºä¸´åºŠåº”ç”¨çš„å®æ—¶éœ€æ±‚ã€‚</li>
<li>ä½¿ç”¨TensorRTåœ¨å•ä¸ªGPUä¸Šå®ç°çº¦77 FPSçš„æ¨ç†é€Ÿåº¦ã€‚</li>
<li>è¯¥æ–¹æ³•èƒ½å¤Ÿå¤„ç†å¿ƒè„å’Œç”²çŠ¶è…ºçš„è¶…å£°å›¾åƒåˆ†å‰²ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.24368">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-cc8d5a6e23391b23de0bcde6f704da76.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-789b7c735e00060bca57b31e10303173.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ffb50aef96b1351acfdfe3561a2d3a3d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-550f702d500e758b5ca7de1a33003cd4.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="IMPACT-A-Generic-Semantic-Loss-for-Multimodal-Medical-Image-Registration"><a href="#IMPACT-A-Generic-Semantic-Loss-for-Multimodal-Medical-Image-Registration" class="headerlink" title="IMPACT: A Generic Semantic Loss for Multimodal Medical Image   Registration"></a>IMPACT: A Generic Semantic Loss for Multimodal Medical Image   Registration</h2><p><strong>Authors:Valentin Boussot, CÃ©dric HÃ©mon, Jean-Claude Nunes, Jason Downling, Simon RouzÃ©, Caroline Lafond, AnaÃ¯s Barateau, Jean-Louis Dillenseger</strong></p>
<p>Image registration is fundamental in medical imaging, enabling precise alignment of anatomical structures for diagnosis, treatment planning, image-guided treatment or longitudinal monitoring. This work introduces IMPACT (Image Metric with Pretrained model-Agnostic Comparison for Transmodality registration), a generic semantic similarity metric designed for seamless integration into diverse image registration frameworks (such as Elastix and Voxelmorph). It compares deep learning-based features extracted from medical images without requiring task-specific training, ensuring broad applicability across various modalities. By leveraging the features of the large-scale pretrained TotalSegmentator models and the ability to integrate Segment Anything Model (SAM) and other large-scale segmentation networks, this approach offers significant advantages. It provides robust, scalable, and efficient solutions for multimodal image registration. The IMPACT loss was evaluated on five challenging registration tasks involving thoracic CT&#x2F;CBCT, and pelvic MR&#x2F;CT datasets. Quantitative metrics, such as Target Registration Error and Dice Similarity Coefficient, demonstrated significant improvements in anatomical alignment compared to baseline methods. Qualitative analyses further confirmed the increased robustness of the proposed metric in the face of noise, artifacts, and modality variations. IMPACTâ€™s versatility and efficiency make it a valuable tool for advancing registration performance in clinical and research applications, addressing critical challenges in multimodal medical imaging. </p>
<blockquote>
<p>åŒ»å­¦å½±åƒä¸­çš„å›¾åƒé…å‡†æ˜¯åŸºæœ¬ä¸”è‡³å…³é‡è¦çš„æŠ€æœ¯ï¼Œå®ƒèƒ½å¤Ÿå®ç°è§£å‰–ç»“æ„çš„ç²¾ç¡®å¯¹é½ï¼Œä¸ºè¯Šæ–­ã€æ²»ç–—è®¡åˆ’ã€å›¾åƒå¼•å¯¼æ²»ç–—æˆ–çºµå‘ç›‘æµ‹æä¾›æ”¯æŒã€‚æœ¬æ–‡ä»‹ç»äº†IMPACTï¼ˆç”¨äºè·¨æ¨¡æ€é…å‡†çš„é¢„è®­ç»ƒæ¨¡å‹æ— å…³æ¯”è¾ƒçš„å›¾åƒåº¦é‡æŒ‡æ ‡ï¼‰ï¼Œè¿™æ˜¯ä¸€ç§é€šç”¨è¯­ä¹‰ç›¸ä¼¼æ€§åº¦é‡æŒ‡æ ‡ï¼Œæ—¨åœ¨æ— ç¼é›†æˆåˆ°å¤šç§å›¾åƒé…å‡†æ¡†æ¶ï¼ˆå¦‚Elastixå’ŒVoxelmorphï¼‰ä¸­ã€‚å®ƒæ¯”è¾ƒåŒ»å­¦å›¾åƒä¸­æå–çš„æ·±åº¦å­¦ä¹ ç‰¹å¾ï¼Œæ— éœ€ç‰¹å®šä»»åŠ¡çš„è®­ç»ƒï¼Œç¡®ä¿åœ¨ä¸åŒæ¨¡æ€ä¹‹é—´å…·æœ‰å¹¿æ³›çš„åº”ç”¨æ€§ã€‚é€šè¿‡åˆ©ç”¨å¤§è§„æ¨¡é¢„è®­ç»ƒTotalSegmentatoræ¨¡å‹çš„ç‰¹å¾ä»¥åŠé›†æˆSegment Anything Modelï¼ˆSAMï¼‰å’Œå…¶ä»–å¤§è§„æ¨¡åˆ†å‰²ç½‘ç»œçš„èƒ½åŠ›ï¼Œæ­¤æ–¹æ³•å…·æœ‰æ˜¾è‘—ä¼˜åŠ¿ã€‚å®ƒä¸ºå¤šæ¨¡æ€å›¾åƒé…å‡†æä¾›äº†ç¨³å¥ã€å¯æ‰©å±•å’Œé«˜æ•ˆçš„è§£å†³æ–¹æ¡ˆã€‚IMPACTæŸå¤±åœ¨äº”ä¸ªå…·æœ‰æŒ‘æˆ˜æ€§çš„é…å‡†ä»»åŠ¡ä¸Šè¿›è¡Œäº†è¯„ä¼°ï¼Œæ¶‰åŠèƒ¸éƒ¨CT&#x2F;CBCTå’Œç›†è…”MR&#x2F;CTæ•°æ®é›†ã€‚ç›®æ ‡æ³¨å†Œè¯¯å·®å’ŒDiceç›¸ä¼¼ç³»æ•°ç­‰å®šé‡æŒ‡æ ‡è¯æ˜ï¼Œä¸åŸºå‡†æ–¹æ³•ç›¸æ¯”ï¼Œå…¶åœ¨è§£å‰–ç»“æ„å¯¹é½æ–¹é¢å®ç°äº†æ˜¾è‘—æ”¹å–„ã€‚å®šæ€§åˆ†æè¿›ä¸€æ­¥è¯å®äº†æ‰€æå‡ºçš„åº¦é‡æŒ‡æ ‡åœ¨åº”å¯¹å™ªå£°ã€ä¼ªå½±å’Œæ¨¡æ€å˜åŒ–æ—¶çš„å¢å¼ºç¨³å¥æ€§ã€‚IMPACTçš„é€šç”¨æ€§å’Œæ•ˆç‡ä½¿å…¶æˆä¸ºæ¨åŠ¨ä¸´åºŠå’Œç ”ç©¶åº”ç”¨ä¸­é…å‡†æ€§èƒ½æé«˜çš„å®è´µå·¥å…·ï¼Œè§£å†³äº†å¤šæ¨¡æ€åŒ»å­¦å½±åƒä¸­çš„å…³é”®æŒ‘æˆ˜ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.24121v1">PDF</a> Submitted to IEEE Transactions on Pattern Analysis and Machine   Intelligence (TPAMI). This is a preprint version and has not been   peer-reviewed</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ‘˜è¦æå‡ºäº†ä¸€ç§æ–°å‹çš„å›¾åƒé…å‡†æ–¹æ³•IMPACTï¼Œå®ƒé€šè¿‡åˆ©ç”¨å¤§è§„æ¨¡é¢„è®­ç»ƒæ¨¡å‹çš„ç‰¹æ€§ï¼Œæ— éœ€ç‰¹å®šä»»åŠ¡è®­ç»ƒå³å¯ä»åŒ»å­¦å›¾åƒä¸­æå–æ·±åº¦ç‰¹å¾ï¼Œä¸ºå¤šç§å›¾åƒé…å‡†æ¡†æ¶æä¾›äº†æ— ç¼é›†æˆã€‚IMPACTåœ¨è·¨æ¨¡æ€å›¾åƒé…å‡†ä¸­è¡¨ç°å‡ºç¨³å¥ã€å¯æ‰©å±•å’Œé«˜æ•ˆçš„è§£å†³æ–¹æ¡ˆï¼Œå¹¶åœ¨äº”ä¸ªå…·æœ‰æŒ‘æˆ˜æ€§çš„æ³¨å†Œä»»åŠ¡ä¸­è¿›è¡Œäº†è¯„ä¼°ï¼ŒåŒ…æ‹¬èƒ¸éƒ¨CT&#x2F;CBCTå’Œç›†è…”MR&#x2F;CTæ•°æ®é›†ã€‚ä¸åŸºçº¿æ–¹æ³•ç›¸æ¯”ï¼Œå®šé‡æŒ‡æ ‡ï¼ˆå¦‚ç›®æ ‡æ³¨å†Œè¯¯å·®å’ŒDiceç›¸ä¼¼ç³»æ•°ï¼‰è¯æ˜äº†å…¶åœ¨è§£å‰–å­¦å¯¹é½æ–¹é¢çš„æ˜¾è‘—æ”¹å–„ã€‚æ­¤å¤–ï¼Œå®šæ€§åˆ†æè¿›ä¸€æ­¥è¯å®äº†è¯¥åº¦é‡åœ¨é¢å¯¹å™ªå£°ã€ä¼ªå½±å’Œæ¨¡æ€å˜åŒ–æ—¶çš„ç¨³å¥æ€§ã€‚å®ƒä¸ºä¸´åºŠå’Œç ”ç©¶åº”ç”¨ä¸­æ³¨å†Œæ€§èƒ½çš„æå‡æä¾›äº†æœ‰ä»·å€¼çš„å·¥å…·ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>IMPACTæ˜¯ä¸€ç§ä¸ºæ— ç¼é›†æˆåˆ°å¤šç§å›¾åƒé…å‡†æ¡†æ¶è€Œè®¾è®¡çš„é€šç”¨è¯­ä¹‰ç›¸ä¼¼æ€§åº¦é‡æ–¹æ³•ã€‚</li>
<li>å®ƒåˆ©ç”¨å¤§è§„æ¨¡é¢„è®­ç»ƒæ¨¡å‹çš„ç‰¹æ€§ï¼Œå¯ä»åŒ»å­¦å›¾åƒä¸­æå–æ·±åº¦ç‰¹å¾ï¼Œæ— éœ€ç‰¹å®šä»»åŠ¡è®­ç»ƒã€‚</li>
<li>IMPACTå…·æœ‰ç¨³å¥ã€å¯æ‰©å±•å’Œé«˜æ•ˆçš„è§£å†³æ–¹æ¡ˆï¼Œé€‚ç”¨äºè·¨æ¨¡æ€å›¾åƒé…å‡†ã€‚</li>
<li>åœ¨äº”ä¸ªæŒ‘æˆ˜æ€§çš„æ³¨å†Œä»»åŠ¡ä¸­è¯„ä¼°äº†IMPACTï¼ŒåŒ…æ‹¬ä½¿ç”¨èƒ¸éƒ¨CT&#x2F;CBCTå’Œç›†è…”MR&#x2F;CTæ•°æ®é›†çš„ä»»åŠ¡ã€‚</li>
<li>ä¸åŸºçº¿æ–¹æ³•ç›¸æ¯”ï¼ŒIMPACTåœ¨è§£å‰–å­¦å¯¹é½æ–¹é¢è¡¨ç°å‡ºæ˜¾è‘—æ”¹å–„ï¼Œå®šé‡æŒ‡æ ‡è¯æ˜äº†è¿™ä¸€ç‚¹ã€‚</li>
<li>å®šæ€§åˆ†æè¯å®äº†IMPACTåœ¨é¢å¯¹å™ªå£°ã€ä¼ªå½±å’Œæ¨¡æ€å˜åŒ–æ—¶çš„ç¨³å¥æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.24121">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-5c01e1eed6bc894a585081e936f5b092.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8bccee1892a78586def518746399d25e.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="Learned-Image-Compression-and-Restoration-for-Digital-Pathology"><a href="#Learned-Image-Compression-and-Restoration-for-Digital-Pathology" class="headerlink" title="Learned Image Compression and Restoration for Digital Pathology"></a>Learned Image Compression and Restoration for Digital Pathology</h2><p><strong>Authors:SeonYeong Lee, EonSeung Seong, DongEon Lee, SiYeoul Lee, Yubin Cho, Chunsu Park, Seonho Kim, MinKyung Seo, YoungSin Ko, MinWoo Kim</strong></p>
<p>Digital pathology images play a crucial role in medical diagnostics, but their ultra-high resolution and large file sizes pose significant challenges for storage, transmission, and real-time visualization. To address these issues, we propose CLERIC, a novel deep learning-based image compression framework designed specifically for whole slide images (WSIs). CLERIC integrates a learnable lifting scheme and advanced convolutional techniques to enhance compression efficiency while preserving critical pathological details. Our framework employs a lifting-scheme transform in the analysis stage to decompose images into low- and high-frequency components, enabling more structured latent representations. These components are processed through parallel encoders incorporating Deformable Residual Blocks (DRB) and Recurrent Residual Blocks (R2B) to improve feature extraction and spatial adaptability. The synthesis stage applies an inverse lifting transform for effective image reconstruction, ensuring high-fidelity restoration of fine-grained tissue structures. We evaluate CLERIC on a digital pathology image dataset and compare its performance against state-of-the-art learned image compression (LIC) models. Experimental results demonstrate that CLERIC achieves superior rate-distortion (RD) performance, significantly reducing storage requirements while maintaining high diagnostic image quality. Our study highlights the potential of deep learning-based compression in digital pathology, facilitating efficient data management and long-term storage while ensuring seamless integration into clinical workflows and AI-assisted diagnostic systems. Code and models are available at: <a target="_blank" rel="noopener" href="https://github.com/pnu-amilab/CLERIC">https://github.com/pnu-amilab/CLERIC</a>. </p>
<blockquote>
<p>æ•°å­—ç—…ç†å›¾åƒåœ¨åŒ»å­¦è¯Šæ–­ä¸­èµ·ç€è‡³å…³é‡è¦çš„ä½œç”¨ï¼Œä½†å…¶è¶…é«˜çš„åˆ†è¾¨ç‡å’Œåºå¤§çš„æ–‡ä»¶å¤§å°ç»™å­˜å‚¨ã€ä¼ è¾“å’Œå®æ—¶å¯è§†åŒ–å¸¦æ¥äº†é‡å¤§æŒ‘æˆ˜ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†CLERICï¼Œè¿™æ˜¯ä¸€ä¸ªåŸºäºæ·±åº¦å­¦ä¹ ä¸“ä¸ºå…¨å¹»ç¯ç‰‡å›¾åƒï¼ˆWSIï¼‰è®¾è®¡çš„å›¾åƒå‹ç¼©æ¡†æ¶ã€‚CLERICé›†æˆäº†ä¸€ç§å¯å­¦ä¹ çš„æå‡æ–¹æ¡ˆå’Œå…ˆè¿›çš„å·ç§¯æŠ€æœ¯ï¼Œä»¥æé«˜å‹ç¼©æ•ˆç‡çš„åŒæ—¶ä¿ç•™å…³é”®çš„ç—…ç†ç»†èŠ‚ã€‚æˆ‘ä»¬çš„æ¡†æ¶åœ¨åˆ†æé˜¶æ®µé‡‡ç”¨æå‡æ–¹æ¡ˆå˜æ¢ï¼Œå°†å›¾åƒåˆ†è§£ä¸ºä½é¢‘å’Œé«˜é¢‘æˆåˆ†ï¼Œä»¥å®ç°æ›´ç»“æ„åŒ–çš„æ½œåœ¨è¡¨ç¤ºã€‚è¿™äº›æˆåˆ†é€šè¿‡åŒ…å«å¯å˜å½¢æ®‹å·®å—ï¼ˆDRBï¼‰å’Œå¾ªç¯æ®‹å·®å—ï¼ˆR2Bï¼‰çš„å¹¶è¡Œç¼–ç å™¨è¿›è¡Œå¤„ç†ï¼Œä»¥æ”¹å–„ç‰¹å¾æå–å’Œç©ºé—´é€‚åº”æ€§ã€‚åˆæˆé˜¶æ®µåº”ç”¨é€†å‘æå‡å˜æ¢è¿›è¡Œæœ‰æ•ˆçš„å›¾åƒé‡å»ºï¼Œç¡®ä¿ç²¾ç»†çº¹ç†ç»„ç»‡ç»“æ„çš„é«˜ä¿çœŸæ¢å¤ã€‚æˆ‘ä»¬åœ¨æ•°å­—ç—…ç†å›¾åƒæ•°æ®é›†ä¸Šè¯„ä¼°äº†CLERICçš„æ€§èƒ½ï¼Œå¹¶å°†å…¶ä¸æœ€å…ˆè¿›çš„å­¦ä¹ å›¾åƒå‹ç¼©ï¼ˆLICï¼‰æ¨¡å‹è¿›è¡Œäº†æ¯”è¾ƒã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒCLERICåœ¨é€Ÿç‡å¤±çœŸï¼ˆRDï¼‰æ€§èƒ½ä¸Šè¡¨ç°ä¼˜è¶Šï¼Œåœ¨ä¿æŒé«˜è¯Šæ–­å›¾åƒè´¨é‡çš„åŒæ—¶æ˜¾è‘—å‡å°‘äº†å­˜å‚¨éœ€æ±‚ã€‚æˆ‘ä»¬çš„ç ”ç©¶çªå‡ºäº†åŸºäºæ·±åº¦å­¦ä¹ çš„å‹ç¼©åœ¨æ•°å­—ç—…ç†å­¦ä¸­çš„æ½œåŠ›ï¼Œæœ‰åŠ©äºé«˜æ•ˆçš„æ•°æ®ç®¡ç†å’Œé•¿æœŸå­˜å‚¨ï¼ŒåŒæ—¶ç¡®ä¿æ— ç¼é›†æˆåˆ°ä¸´åºŠå·¥ä½œæµç¨‹å’Œäººå·¥æ™ºèƒ½è¾…åŠ©è¯Šæ–­ç³»ç»Ÿä¸­ã€‚ä»£ç å’Œæ¨¡å‹å¯åœ¨ï¼š<a target="_blank" rel="noopener" href="https://github.com/pnu-amilab/CLERIC%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/pnu-amilab/CLERICè·å–ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.23862v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>åŸºäºæ·±åº¦å­¦ä¹ çš„å›¾åƒå‹ç¼©æ¡†æ¶CLERICï¼Œä¸“ä¸ºå…¨å¹»ç¯ç‰‡å›¾åƒï¼ˆWSIsï¼‰è®¾è®¡ï¼Œæ—¨åœ¨è§£å†³æ•°å­—ç—…ç†å›¾åƒåœ¨åŒ»ç–—è¯Šæ–­ä¸­çš„è¶…é«˜åˆ†è¾¨ç‡å’Œå¤§æ–‡ä»¶å°ºå¯¸å¸¦æ¥çš„å­˜å‚¨ã€ä¼ è¾“å’Œå®æ—¶å¯è§†åŒ–æŒ‘æˆ˜ã€‚CLERICé‡‡ç”¨å¯å­¦ä¹ çš„æå‡æ–¹æ¡ˆå’Œå…ˆè¿›çš„å·ç§¯æŠ€æœ¯ï¼Œæé«˜å‹ç¼©æ•ˆç‡çš„åŒæ—¶ä¿ç•™å…³é”®çš„ç—…ç†ç»†èŠ‚ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ•°å­—ç—…ç†å›¾åƒåœ¨åŒ»ç–—è¯Šæ–­ä¸­è‡³å…³é‡è¦ï¼Œä½†å…¶è¶…é«˜åˆ†è¾¨ç‡å’Œå¤§æ–‡ä»¶å°ºå¯¸å¸¦æ¥å­˜å‚¨ã€ä¼ è¾“å’Œå®æ—¶å¯è§†åŒ–æŒ‘æˆ˜ã€‚</li>
<li>CLERICæ˜¯ä¸€ä¸ªåŸºäºæ·±åº¦å­¦ä¹ çš„å›¾åƒå‹ç¼©æ¡†æ¶ï¼Œä¸“ä¸ºå…¨å¹»ç¯ç‰‡å›¾åƒï¼ˆWSIsï¼‰è®¾è®¡ã€‚</li>
<li>CLERICé‡‡ç”¨å¯å­¦ä¹ çš„æå‡æ–¹æ¡ˆï¼Œå°†å›¾åƒåˆ†è§£ä¸ºä½é¢‘å’Œé«˜é¢‘æˆåˆ†ï¼Œå®ç°æ›´ç»“æ„åŒ–çš„æ½œåœ¨è¡¨ç¤ºã€‚</li>
<li>æ¡†æ¶ä¸­çš„å¹¶è¡Œç¼–ç å™¨é‡‡ç”¨å¯å˜æ®‹ä½™å—å’Œå¾ªç¯æ®‹ä½™å—ï¼Œæ”¹è¿›ç‰¹å¾æå–å’Œç©ºé—´é€‚åº”æ€§ã€‚</li>
<li>CLERICåœ¨åˆæˆé˜¶æ®µåº”ç”¨é€†å‘æå‡å˜æ¢ï¼Œå®ç°æœ‰æ•ˆçš„å›¾åƒé‡å»ºï¼Œç¡®ä¿ç»†ç²’åº¦ç»„ç»‡ç»“æ„çš„é«˜ä¿çœŸæ¢å¤ã€‚</li>
<li>åœ¨æ•°å­—ç—…ç†å›¾åƒæ•°æ®é›†ä¸Šè¯„ä¼°CLERICï¼Œä¸æœ€å…ˆè¿›çš„å›¾åƒå‹ç¼©æ¨¡å‹ç›¸æ¯”ï¼Œå®ç°äº†ä¼˜è¶Šçš„é€Ÿç‡å¤±çœŸæ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.23862">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-e96dbd59f3c9400e34c67b8822ecc0e5.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b45ecbc464b87949ffe6c36b6c7b702f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-76add58e3450369d674cdb7cf8ab0c5e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-20ebe0243333704b0ba25369308a1326.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ca6543ea76a1adec48c82b932e8cb8fe.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="WaveFormer-A-3D-Transformer-with-Wavelet-Driven-Feature-Representation-for-Efficient-Medical-Image-Segmentation"><a href="#WaveFormer-A-3D-Transformer-with-Wavelet-Driven-Feature-Representation-for-Efficient-Medical-Image-Segmentation" class="headerlink" title="WaveFormer: A 3D Transformer with Wavelet-Driven Feature Representation   for Efficient Medical Image Segmentation"></a>WaveFormer: A 3D Transformer with Wavelet-Driven Feature Representation   for Efficient Medical Image Segmentation</h2><p><strong>Authors:Md Mahfuz Al Hasan, Mahdi Zaman, Abdul Jawad, Alberto Santamaria-Pang, Ho Hin Lee, Ivan Tarapov, Kyle See, Md Shah Imran, Antika Roy, Yaser Pourmohammadi Fallah, Navid Asadizanjani, Reza Forghani</strong></p>
<p>Transformer-based architectures have advanced medical image analysis by effectively modeling long-range dependencies, yet they often struggle in 3D settings due to substantial memory overhead and insufficient capture of fine-grained local features. We address these limitations with WaveFormer, a novel 3D-transformer that: i) leverages the fundamental frequency-domain properties of features for contextual representation, and ii) is inspired by the top-down mechanism of the human visual recognition system, making it a biologically motivated architecture. By employing discrete wavelet transformations (DWT) at multiple scales, WaveFormer preserves both global context and high-frequency details while replacing heavy upsampling layers with efficient wavelet-based summarization and reconstruction. This significantly reduces the number of parameters, which is critical for real-world deployment where computational resources and training times are constrained. Furthermore, the model is generic and easily adaptable to diverse applications. Evaluations on BraTS2023, FLARE2021, and KiTS2023 demonstrate performance on par with state-of-the-art methods while offering substantially lower computational complexity. </p>
<blockquote>
<p>åŸºäºTransformerçš„æ¶æ„é€šè¿‡æœ‰æ•ˆåœ°å»ºæ¨¡é•¿ç¨‹ä¾èµ–å…³ç³»ï¼Œå·²ç»æ¨åŠ¨äº†åŒ»å­¦å›¾åƒåˆ†æçš„å‘å±•ã€‚ç„¶è€Œï¼Œå®ƒä»¬åœ¨3Dç¯å¢ƒä¸­å¾€å¾€é¢ä¸´å·¨å¤§çš„å†…å­˜å¼€é”€å’Œç²¾ç»†å±€éƒ¨ç‰¹å¾æ•æ‰ä¸è¶³çš„é—®é¢˜ã€‚ä¸ºäº†è§£å†³è¿™äº›å±€é™æ€§ï¼Œæˆ‘ä»¬æå‡ºäº†WaveFormerï¼Œè¿™æ˜¯ä¸€ç§æ–°å‹çš„3D transformerï¼Œå®ƒï¼š(i)åˆ©ç”¨ç‰¹å¾çš„åŸºé¢‘åŸŸå±æ€§è¿›è¡Œä¸Šä¸‹æ–‡è¡¨ç¤ºï¼›(ii)å—åˆ°äººç±»è§†è§‰è¯†åˆ«ç³»ç»Ÿçš„è‡ªä¸Šè€Œä¸‹æœºåˆ¶çš„å¯å‘ï¼Œæ˜¯ä¸€ç§å…·æœ‰ç”Ÿç‰©åŠ¨æœºçš„æ¶æ„ã€‚é€šè¿‡é‡‡ç”¨å¤šå°ºåº¦çš„ç¦»æ•£å°æ³¢å˜æ¢ï¼ˆDWTï¼‰ï¼ŒWaveFormeråœ¨ä¿ç•™å…¨å±€ä¸Šä¸‹æ–‡å’Œé«˜é¢‘ç»†èŠ‚çš„åŒæ—¶ï¼Œç”¨é«˜æ•ˆçš„å°æ³¢æ€»ç»“ä¸é‡å»ºæ›¿ä»£å¤æ‚çš„ä¸Šé‡‡æ ·å±‚ã€‚è¿™æ˜¾è‘—å‡å°‘äº†å‚æ•°æ•°é‡ï¼Œå¯¹äºè®¡ç®—èµ„æºå’Œè®­ç»ƒæ—¶é—´å—é™çš„ç°å®ä¸–ç•Œéƒ¨ç½²ç¯å¢ƒè€Œè¨€è‡³å…³é‡è¦ã€‚æ­¤å¤–ï¼Œè¯¥æ¨¡å‹å…·æœ‰é€šç”¨æ€§ï¼Œå¯è½»æ¾é€‚åº”å¤šç§åº”ç”¨ã€‚åœ¨BraTS2023ã€FLARE2021å’ŒKiTS2023ä¸Šçš„è¯„ä¼°ç»“æœè¡¨æ˜ï¼Œå…¶æ€§èƒ½ä¸æœ€æ–°æ–¹æ³•ç›¸å½“ï¼ŒåŒæ—¶è®¡ç®—å¤æ‚åº¦å¤§å¤§é™ä½ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.23764v2">PDF</a> </p>
<p><strong>Summary</strong><br>     æ³¢å‰ç½‘ç»œï¼ˆWaveFormerï¼‰æ˜¯ä¸€ç§æ–°å‹ä¸‰ç»´å˜å‹å™¨ï¼ˆ3D-transformerï¼‰ï¼Œé€šè¿‡åˆ©ç”¨ç‰¹å¾çš„é¢‘åŸŸå±æ€§è¿›è¡Œä¸Šä¸‹æ–‡è¡¨ç¤ºï¼Œå¹¶ç»“åˆäººç±»è§†è§‰è¯†åˆ«ç³»ç»Ÿçš„è‡ªä¸Šè€Œä¸‹æœºåˆ¶ï¼Œè§£å†³äº†å½“å‰ä¸‰ç»´è®¾ç½®ä¸­å˜å‹å™¨æ¶æ„é¢ä¸´çš„ä¸€äº›å±€é™æ€§ã€‚WaveFormeré‡‡ç”¨å¤šå°ºåº¦ç¦»æ•£å°æ³¢å˜æ¢ï¼ˆDWTï¼‰ï¼Œä¿ç•™äº†å…¨å±€ä¸Šä¸‹æ–‡å’Œé«˜é¢‘ç»†èŠ‚ï¼Œç”¨é«˜æ•ˆçš„åŸºäºå°æ³¢æ€»ç»“ä¸é‡å»ºæ›¿æ¢äº†å¤§é‡çš„ä¸Šé‡‡æ ·å±‚ï¼Œæ˜¾è‘—å‡å°‘äº†å‚æ•°æ•°é‡ã€‚è¯¥æ¨¡å‹å…·æœ‰é€šç”¨æ€§ï¼Œæ˜“äºé€‚åº”å¤šç§åº”ç”¨ï¼Œå¹¶åœ¨BraTS2023ã€FLARE2021å’ŒKiTS2023ä¸Šçš„è¯„ä¼°æ˜¾ç¤ºï¼Œå…¶æ€§èƒ½ä¸æœ€å…ˆè¿›çš„æ–¹æ³•ç›¸å½“ï¼Œä½†è®¡ç®—å¤æ‚åº¦å¤§å¹…é™ä½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>WaveFormeræ˜¯ä¸€ç§é’ˆå¯¹åŒ»å­¦å›¾åƒåˆ†æçš„ä¸‰ç»´å˜å‹å™¨æ¶æ„ã€‚</li>
<li>å®ƒåˆ©ç”¨ç‰¹å¾çš„é¢‘åŸŸå±æ€§è¿›è¡Œä¸Šä¸‹æ–‡è¡¨ç¤ºï¼Œè¿™æ˜¯å…¶ç‹¬ç‰¹ä¹‹å¤„ã€‚</li>
<li>è¯¥æ¨¡å‹å—åˆ°äººç±»è§†è§‰è¯†åˆ«ç³»ç»Ÿè‡ªä¸Šè€Œä¸‹æœºåˆ¶çš„å¯å‘ã€‚</li>
<li>é€šè¿‡ä½¿ç”¨ç¦»æ•£å°æ³¢å˜æ¢ï¼ˆDWTï¼‰ï¼ŒWaveFormerèƒ½å¤ŸåŒæ—¶ä¿ç•™å…¨å±€ä¸Šä¸‹æ–‡å’Œé«˜é¢‘ç»†èŠ‚ã€‚</li>
<li>WaveFormeré€šè¿‡é«˜æ•ˆçš„åŸºäºå°æ³¢çš„æ–¹æ³•å‡å°‘è®¡ç®—å¤æ‚æ€§ã€‚</li>
<li>è¯¥æ¨¡å‹å…·æœ‰é€šç”¨æ€§ï¼Œå¯ä»¥å¾ˆå®¹æ˜“åœ°é€‚åº”ä¸åŒçš„åŒ»å­¦å›¾åƒåˆ†æåº”ç”¨ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.23764">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-fbab1237edc99526af53e2fcc9d4de2c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a82b43d6d6e0aa8972910d2f003ff1ad.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-81ac25e1e16dfdfb706cf6375c6addf8.jpg" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="BiPVL-Seg-Bidirectional-Progressive-Vision-Language-Fusion-with-Global-Local-Alignment-for-Medical-Image-Segmentation"><a href="#BiPVL-Seg-Bidirectional-Progressive-Vision-Language-Fusion-with-Global-Local-Alignment-for-Medical-Image-Segmentation" class="headerlink" title="BiPVL-Seg: Bidirectional Progressive Vision-Language Fusion with   Global-Local Alignment for Medical Image Segmentation"></a>BiPVL-Seg: Bidirectional Progressive Vision-Language Fusion with   Global-Local Alignment for Medical Image Segmentation</h2><p><strong>Authors:Rafi Ibn Sultan, Hui Zhu, Chengyin Li, Dongxiao Zhu</strong></p>
<p>Medical image segmentation typically relies solely on visual data, overlooking the rich textual information clinicians use for diagnosis. Vision-language models attempt to bridge this gap, but existing approaches often process visual and textual features independently, resulting in weak cross-modal alignment. Simple fusion techniques fail due to the inherent differences between spatial visual features and sequential text embeddings. Additionally, medical terminology deviates from general language, limiting the effectiveness of off-the-shelf text encoders and further hindering vision-language alignment. We propose BiPVL-Seg, an end-to-end framework that integrates vision-language fusion and embedding alignment through architectural and training innovations, where both components reinforce each other to enhance medical image segmentation. BiPVL-Seg introduces bidirectional progressive fusion in the architecture, which facilitates stage-wise information exchange between vision and text encoders. Additionally, it incorporates global-local contrastive alignment, a training objective that enhances the text encoderâ€™s comprehension by aligning text and vision embeddings at both class and concept levels. Extensive experiments on diverse medical imaging benchmarks across CT and MR modalities demonstrate BiPVL-Segâ€™s superior performance when compared with state-of-the-art methods in complex multi-class segmentation. Source code is available in this GitHub repository. </p>
<blockquote>
<p>åŒ»å­¦å›¾åƒåˆ†å‰²é€šå¸¸ä»…ä¾èµ–äºè§†è§‰æ•°æ®ï¼Œå¿½ç•¥äº†ä¸´åºŠåŒ»ç”Ÿç”¨äºè¯Šæ–­çš„ä¸°å¯Œæ–‡æœ¬ä¿¡æ¯ã€‚è§†å¬è¯­è¨€æ¨¡å‹è¯•å›¾å¼¥åˆè¿™ä¸€é¸¿æ²Ÿï¼Œä½†ç°æœ‰æ–¹æ³•é€šå¸¸ç‹¬ç«‹å¤„ç†è§†è§‰å’Œæ–‡æœ¬ç‰¹å¾ï¼Œå¯¼è‡´è·¨æ¨¡æ€å¯¹é½æ•ˆæœä¸ä½³ã€‚ç”±äºç©ºé—´è§†è§‰ç‰¹å¾å’Œé¡ºåºæ–‡æœ¬åµŒå…¥ä¹‹é—´çš„å›ºæœ‰å·®å¼‚ï¼Œç®€å•çš„èåˆæŠ€æœ¯ä¼šå¤±è´¥ã€‚æ­¤å¤–ï¼ŒåŒ»å­¦æœ¯è¯­ä¸é€šç”¨è¯­è¨€æœ‰åå·®ï¼Œé™åˆ¶äº†ç°æˆçš„æ–‡æœ¬ç¼–ç å™¨çš„æœ‰æ•ˆæ€§ï¼Œå¹¶è¿›ä¸€æ­¥é˜»ç¢äº†è§†å¬è¯­è¨€å¯¹é½ã€‚æˆ‘ä»¬æå‡ºäº†BiPVL-Segï¼Œè¿™æ˜¯ä¸€ä¸ªç«¯åˆ°ç«¯çš„æ¡†æ¶ï¼Œé€šè¿‡æ¶æ„å’Œè®­ç»ƒåˆ›æ–°ï¼Œå°†è§†å¬è¯­è¨€èåˆå’ŒåµŒå…¥å¯¹é½ç›¸ç»“åˆï¼Œä¸¤ä¸ªç»„ä»¶ç›¸äº’å¢å¼ºï¼Œä»¥æé«˜åŒ»å­¦å›¾åƒåˆ†å‰²çš„æ•ˆæœã€‚BiPVL-Segåœ¨æ¶æ„ä¸­å¼•å…¥äº†åŒå‘æ¸è¿›èåˆï¼Œä¾¿äºè§†å¬ç¼–ç å™¨ä¹‹é—´åˆ†é˜¶æ®µçš„ä¿¡æ¯äº¤æ¢ã€‚æ­¤å¤–ï¼Œå®ƒç»“åˆäº†å…¨å±€å±€éƒ¨å¯¹æ¯”å¯¹é½ï¼Œè¿™æ˜¯ä¸€ä¸ªè®­ç»ƒç›®æ ‡ï¼Œé€šè¿‡å¯¹é½æ–‡æœ¬å’Œè§†è§‰åµŒå…¥çš„ç±»åˆ«å’Œæ¦‚å¿µçº§åˆ«ï¼Œæé«˜æ–‡æœ¬ç¼–ç å™¨çš„ç†è§£èƒ½åŠ›ã€‚åœ¨CTå’ŒMRæ¨¡æ€çš„å¤šç§åŒ»å­¦æˆåƒåŸºå‡†æµ‹è¯•ä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼Œä¸æœ€æ–°æ–¹æ³•ç›¸æ¯”ï¼ŒBiPVL-Segåœ¨å¤æ‚çš„å¤šç±»åˆ†å‰²ä¸­å…·æœ‰å“è¶Šçš„æ€§èƒ½ã€‚æºä»£ç å¯åœ¨GitHubä»“åº“ä¸­æ‰¾åˆ°ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.23534v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>åŒ»å­¦å›¾åƒåˆ†å‰²å¸¸ä»…ä¾èµ–è§†è§‰æ•°æ®ï¼Œå¿½ç•¥äº†ä¸´åºŠåŒ»ç”Ÿç”¨äºè¯Šæ–­çš„ä¸°å¯Œæ–‡æœ¬ä¿¡æ¯ã€‚æœ¬ç ”ç©¶æå‡ºBiPVL-Segæ¡†æ¶ï¼Œé€šè¿‡æ¶æ„å’Œè®­ç»ƒåˆ›æ–°å®ç°è§†è§‰è¯­è¨€èåˆå’ŒåµŒå…¥å¯¹é½ï¼Œæå‡åŒ»å­¦å›¾åƒåˆ†å‰²æ•ˆæœã€‚è¯¥æ¡†æ¶å¼•å…¥åŒå‘æ¸è¿›èåˆï¼Œä¿ƒè¿›è§†è§‰å’Œæ–‡æœ¬ç¼–ç å™¨ä¹‹é—´çš„é˜¶æ®µæ€§ä¿¡æ¯äº¤æµã€‚åŒæ—¶é‡‡ç”¨å…¨å±€å±€éƒ¨å¯¹æ¯”å¯¹é½è®­ç»ƒç›®æ ‡ï¼Œæé«˜æ–‡æœ¬ç¼–ç å™¨å¯¹åŒ»å­¦æœ¯è¯­çš„ç†è§£ï¼Œå®ç°æ–‡æœ¬ä¸è§†è§‰åµŒå…¥çš„ç±»åˆ«å’Œæ¦‚å¿µçº§åˆ«å¯¹é½ã€‚åœ¨å¤šç§åŒ»å­¦å½±åƒåŸºå‡†æµ‹è¯•ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒBiPVL-Segåœ¨å¤æ‚å¤šç±»åˆ†å‰²ä¸­çš„æ€§èƒ½ä¼˜äºç°æœ‰æŠ€æœ¯ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>åŒ»å­¦å›¾åƒåˆ†å‰²å¸¸ä¾èµ–è§†è§‰æ•°æ®è€Œå¿½è§†ä¸´åºŠæ–‡æœ¬ä¿¡æ¯ã€‚</li>
<li>BiPVL-Segæ¡†æ¶é€šè¿‡è§†è§‰ä¸è¯­è¨€çš„èåˆä¸åµŒå…¥å¯¹é½æå‡åŒ»å­¦å›¾åƒåˆ†å‰²æ•ˆæœã€‚</li>
<li>BiPVL-Segå¼•å…¥åŒå‘æ¸è¿›èåˆä¿ƒè¿›è§†è§‰å’Œæ–‡æœ¬ç¼–ç å™¨çš„ä¿¡æ¯äº¤æµã€‚</li>
<li>å…¨å±€å±€éƒ¨å¯¹æ¯”å¯¹é½è®­ç»ƒç›®æ ‡ç”¨äºæé«˜æ–‡æœ¬ç¼–ç å™¨å¯¹åŒ»å­¦æœ¯è¯­çš„ç†è§£ã€‚</li>
<li>BiPVL-Segåœ¨å¤šæ ·åŒ»å­¦å½±åƒåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚ã€‚</li>
<li>BiPVL-Segå¯å®ç°æ–‡æœ¬ä¸è§†è§‰åµŒå…¥çš„ç±»åˆ«å’Œæ¦‚å¿µçº§åˆ«å¯¹é½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.23534">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-09d7b3d29945578c4c40696d8bcc68c5.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-b5452e4664425316da8a062b233c82b4.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-97c0968cdb7e63669d5fe8671e0b9659.jpg" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="Federated-Self-Supervised-Learning-for-One-Shot-Cross-Modal-and-Cross-Imaging-Technique-Segmentation"><a href="#Federated-Self-Supervised-Learning-for-One-Shot-Cross-Modal-and-Cross-Imaging-Technique-Segmentation" class="headerlink" title="Federated Self-Supervised Learning for One-Shot Cross-Modal and   Cross-Imaging Technique Segmentation"></a>Federated Self-Supervised Learning for One-Shot Cross-Modal and   Cross-Imaging Technique Segmentation</h2><p><strong>Authors:Siladittya Manna, Suresh Das, Sayantari Ghosh, Saumik Bhattacharya</strong></p>
<p>Decentralized federated learning enables learning of data representations from multiple sources without compromising the privacy of the clients. In applications like medical image segmentation, where obtaining a large annotated dataset from a single source is a distressing problem, federated self-supervised learning can provide some solace. In this work, we push the limits further by exploring a federated self-supervised one-shot segmentation task representing a more data-scarce scenario. We adopt a pre-existing self-supervised few-shot segmentation framework CoWPro and adapt it to the federated learning scenario. To the best of our knowledge, this work is the first to attempt a self-supervised few-shot segmentation task in the federated learning domain. Moreover, we consider the clients to be constituted of data from different modalities and imaging techniques like MR or CT, which makes the problem even harder. Additionally, we reinforce and improve the baseline CoWPro method using a fused dice loss which shows considerable improvement in performance over the baseline CoWPro. Finally, we evaluate this novel framework on a completely unseen held-out part of the local client dataset. We observe that the proposed framework can achieve performance at par or better than the FedAvg version of the CoWPro framework on the held-out validation dataset. </p>
<blockquote>
<p>åˆ†å¸ƒå¼è”é‚¦å­¦ä¹ èƒ½å¤Ÿä»å¤šä¸ªæºå­¦ä¹ æ•°æ®è¡¨ç¤ºï¼Œè€Œä¸æŸå®³å®¢æˆ·ç«¯çš„éšç§ã€‚åœ¨åŒ»å­¦å›¾åƒåˆ†å‰²ç­‰åº”ç”¨ä¸­ï¼Œä»å•ä¸€æ¥æºè·å–å¤§é‡æ ‡æ³¨æ•°æ®é›†æ˜¯ä¸€ä¸ªä»¤äººå¤´ç–¼çš„é—®é¢˜ï¼Œè”é‚¦è‡ªç›‘ç£å­¦ä¹ å¯ä»¥æä¾›ä¸€äº›æ…°è—‰ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬é€šè¿‡æ¢ç´¢è”é‚¦è‡ªç›‘ç£ä¸€æ¬¡åˆ†å‰²ä»»åŠ¡ï¼Œè¿›ä¸€æ­¥æ‹“å±•äº†ç•Œé™ï¼Œè¿™ä»£è¡¨äº†ä¸€ä¸ªæ•°æ®æ›´ç¨€ç¼ºçš„åœºæ™¯ã€‚æˆ‘ä»¬é‡‡ç”¨é¢„å…ˆå­˜åœ¨çš„è‡ªç›‘ç£å°‘é•œå¤´åˆ†å‰²æ¡†æ¶CoWProï¼Œå¹¶å°†å…¶é€‚åº”äºè”é‚¦å­¦ä¹ åœºæ™¯ã€‚æ®æˆ‘ä»¬æ‰€çŸ¥ï¼Œè¿™æ˜¯é¦–æ¬¡å°è¯•åœ¨è”é‚¦å­¦ä¹ é¢†åŸŸè¿›è¡Œè‡ªç›‘ç£å°‘é•œå¤´åˆ†å‰²ä»»åŠ¡ã€‚è€Œä¸”ï¼Œæˆ‘ä»¬è®¤ä¸ºå®¢æˆ·ç«¯ç”±ä¸åŒæ¨¡æ€å’Œæ•°æ®æˆåƒæŠ€æœ¯ï¼ˆå¦‚MRæˆ–CTï¼‰æ„æˆçš„æ•°æ®ç»„æˆï¼Œè¿™ä½¿å¾—é—®é¢˜å˜å¾—æ›´åŠ å›°éš¾ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬ä½¿ç”¨èåˆDiceæŸå¤±æ¥åŠ å¼ºå’Œæ”¹è¿›åŸºçº¿CoWProæ–¹æ³•ï¼Œä¸åŸºçº¿CoWProç›¸æ¯”ï¼Œå®ƒåœ¨æ€§èƒ½ä¸Šæ˜¾ç¤ºå‡ºæ˜¾è‘—æ”¹è¿›ã€‚æœ€åï¼Œæˆ‘ä»¬åœ¨æœ¬åœ°å®¢æˆ·ç«¯æ•°æ®é›†çš„å®Œå…¨æœªè§è¿‡çš„ä¿ç•™éƒ¨åˆ†ä¸Šè¯„ä¼°äº†è¿™ç§æ–°å‹æ¡†æ¶ã€‚æˆ‘ä»¬å‘ç°ï¼Œåœ¨ä¿ç•™çš„éªŒè¯æ•°æ®é›†ä¸Šï¼Œæ‰€æå‡ºçš„æ¡†æ¶çš„æ€§èƒ½å¯ä»¥è¾¾åˆ°æˆ–è¶…è¿‡CoWProæ¡†æ¶çš„FedAvgç‰ˆæœ¬ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.23507v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>è”é‚¦åŒ–è‡ªæˆ‘ç›‘ç£çš„ä¸€ç«™å¼åˆ†å‰²ä»»åŠ¡åœ¨åŒ»å­¦å›¾åƒåˆ†å‰²ä¸­çš„åº”ç”¨å®ç°äº†æ•°æ®ç¨€ç¼ºåœºæ™¯ä¸‹çš„å­¦ä¹ ã€‚è¯¥ç ”ç©¶é‡‡ç”¨ç°æœ‰çš„è‡ªæˆ‘ç›‘ç£å°‘æ ·æœ¬åˆ†å‰²æ¡†æ¶CoWProï¼Œå¹¶é€‚åº”è”é‚¦å­¦ä¹ åœºæ™¯ã€‚è¿™æ˜¯é¦–æ¬¡å°è¯•åœ¨è”é‚¦å­¦ä¹ é¢†åŸŸè¿›è¡Œè‡ªç›‘ç£çš„å°‘é‡æ ·æœ¬åˆ†å‰²ä»»åŠ¡ï¼Œè§£å†³äº†ä»å¤šä¸ªæ¥æºè·å–å¤§é‡æ ‡æ³¨æ•°æ®çš„é—®é¢˜ã€‚è¯¥ç ”ç©¶è¿˜èåˆäº†ä¸åŒæ¨¡æ€å’ŒæˆåƒæŠ€æœ¯çš„æ•°æ®ï¼Œå¦‚MRæˆ–CTï¼Œæé«˜äº†åŸºçº¿CoWProæ–¹æ³•ä½¿ç”¨èåˆDiceæŸå¤±çš„æ”¹è¿›ç‰ˆæ€§èƒ½ï¼Œå¹¶åœ¨æœ¬åœ°å®¢æˆ·ç«¯æ•°æ®é›†çš„æœªè§éƒ¨åˆ†è¿›è¡Œäº†è¯„ä¼°ã€‚è¯„ä¼°ç»“æœæ˜¾ç¤ºï¼Œè¯¥æ¡†æ¶çš„æ€§èƒ½å¯ä¸CoWProçš„FedAvgç‰ˆæœ¬ç›¸å½“æˆ–æ›´å¥½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è”é‚¦åŒ–è‡ªæˆ‘ç›‘ç£å­¦ä¹ ç”¨äºåŒ»å­¦å›¾åƒåˆ†å‰²ï¼Œèƒ½åœ¨æ•°æ®ç¨€ç¼ºåœºæ™¯ä¸‹å­¦ä¹ æ•°æ®è¡¨ç¤ºã€‚</li>
<li>é‡‡ç”¨å¹¶æ”¹è¿›äº†ç°æœ‰çš„è‡ªæˆ‘ç›‘ç£å°‘æ ·æœ¬åˆ†å‰²æ¡†æ¶CoWProï¼Œé€‚åº”è”é‚¦å­¦ä¹ åœºæ™¯ã€‚</li>
<li>æ­¤ç ”ç©¶æ˜¯é¦–æ¬¡åœ¨è”é‚¦å­¦ä¹ é¢†åŸŸå°è¯•è‡ªç›‘ç£çš„å°‘é‡æ ·æœ¬åˆ†å‰²ä»»åŠ¡ã€‚</li>
<li>ç ”ç©¶è€ƒè™‘äº†ä¸åŒæ¨¡æ€å’ŒæˆåƒæŠ€æœ¯çš„æ•°æ®ï¼Œå¦‚MRæˆ–CTã€‚</li>
<li>ä½¿ç”¨äº†èåˆDiceæŸå¤±çš„æ”¹è¿›ç‰ˆCoWProæ–¹æ³•ï¼Œæé«˜äº†æ€§èƒ½ã€‚</li>
<li>åœ¨æœ¬åœ°å®¢æˆ·ç«¯æ•°æ®é›†çš„æœªè§éƒ¨åˆ†è¿›è¡Œäº†è¯„ä¼°ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.23507">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-0eb86a4a62166dbdac0149eb272f1aa9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ff41ab7a511ef72df0c3a43f70ebb3b5.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-9a4fa682e2275a903a3673bdaf4029c2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4bad396c5e49f86a79e3cf259d3bd438.jpg" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1><h2 id="Evolutionary-Prompt-Optimization-Discovers-Emergent-Multimodal-Reasoning-Strategies-in-Vision-Language-Models"><a href="#Evolutionary-Prompt-Optimization-Discovers-Emergent-Multimodal-Reasoning-Strategies-in-Vision-Language-Models" class="headerlink" title="Evolutionary Prompt Optimization Discovers Emergent Multimodal Reasoning   Strategies in Vision-Language Models"></a>Evolutionary Prompt Optimization Discovers Emergent Multimodal Reasoning   Strategies in Vision-Language Models</h2><p><strong>Authors:Sid Bharthulwar, John Rho, Katrina Brown</strong></p>
<p>We present a framework for optimizing prompts in vision-language models to elicit multimodal reasoning without model retraining. Using an evolutionary algorithm to guide prompt updates downstream of visual tasks, our approach improves upon baseline prompt-updating algorithms, which lack evolution-style â€œsurvival of the fittestâ€ iteration. Crucially, we find this approach enables the language model to independently discover progressive problem-solving techniques across several evolution generations. For example, the model reasons that to â€œbreak downâ€ visually complex spatial tasks, making a tool call to a Python interpreter to perform tasks (such as cropping, image segmentation, or saturation changes) would improve performance significantly. Our experimentation shows that explicitly evoking this â€œtool callingâ€ call, via system-level XML $â€¦\texttt{<tool>} â€¦ \texttt{</tool>}â€¦$ tags, can effectively flag Python interpreter access for the same language model to generate relevant programs, generating advanced multimodal functionality. This functionality can be crystallized into a system-level prompt that induces improved performance at inference time, and our experimentation suggests up to $\approx 50%$ relative improvement across select visual tasks. Downstream performance is trained and evaluated across subtasks from MathVista, M3CoT, and GeoBench-VLM datasets. Importantly, our approach shows that evolutionary prompt optimization guides language models towards self-reasoning discoveries, which result in improved zero-shot generalization across tasks. </p>
<blockquote>
<p>æˆ‘ä»¬æå‡ºäº†ä¸€ç§ä¼˜åŒ–è§†è§‰è¯­è¨€æ¨¡å‹æç¤ºçš„æ¡†æ¶ï¼Œä»¥æ¿€å‘å¤šæ¨¡æ€æ¨ç†è€Œæ— éœ€é‡æ–°è®­ç»ƒæ¨¡å‹ã€‚æˆ‘ä»¬ä½¿ç”¨è¿›åŒ–ç®—æ³•æ¥æŒ‡å¯¼è§†è§‰ä»»åŠ¡ä¸‹æ¸¸çš„æç¤ºæ›´æ–°ï¼Œæˆ‘ä»¬çš„æ–¹æ³•æ”¹è¿›äº†åŸºçº¿æç¤ºæ›´æ–°ç®—æ³•ï¼Œåè€…ç¼ºä¹è¿›åŒ–å¼çš„â€œé€‚è€…ç”Ÿå­˜â€è¿­ä»£ã€‚å…³é”®çš„æ˜¯ï¼Œæˆ‘ä»¬å‘ç°è¿™ç§æ–¹æ³•ä½¿è¯­è¨€æ¨¡å‹èƒ½å¤Ÿåœ¨å¤šä¸ªè¿›åŒ–ä¸–ä»£ä¸­ç‹¬ç«‹åœ°å‘ç°æ¸è¿›çš„é—®é¢˜è§£å†³æŠ€æœ¯ã€‚ä¾‹å¦‚ï¼Œæ¨¡å‹æ¨ç†å‡ºä¸ºäº†â€œåˆ†è§£â€è§†è§‰ä¸Šå¤æ‚çš„ç©ºé—´ä»»åŠ¡ï¼Œè°ƒç”¨Pythonè§£é‡Šå™¨æ‰§è¡Œä¸€äº›ä»»åŠ¡ï¼ˆå¦‚è£å‰ªã€å›¾åƒåˆ†å‰²æˆ–é¥±å’Œåº¦æ›´æ”¹ï¼‰å°†æ˜¾è‘—æé«˜æ€§èƒ½ã€‚æˆ‘ä»¬çš„å®éªŒè¡¨æ˜ï¼Œé€šè¿‡ç³»ç»Ÿçº§åˆ«çš„XML &#96;$â€¦\texttt{<tool>} â€¦ \texttt{</tool>}â€¦$ æ ‡ç­¾æ˜¾å¼è°ƒç”¨è¿™ç§â€œå·¥å…·è°ƒç”¨â€ï¼Œå¯ä»¥æœ‰æ•ˆåœ°æ ‡è®°åŒä¸€è¯­è¨€æ¨¡å‹çš„Pythonè§£é‡Šå™¨è®¿é—®ï¼Œä»¥ç”Ÿæˆç›¸å…³ç¨‹åºï¼Œä»è€Œäº§ç”Ÿé«˜çº§å¤šæ¨¡æ€åŠŸèƒ½ã€‚æ­¤åŠŸèƒ½å¯ä»¥è½¬åŒ–ä¸ºç³»ç»Ÿçº§åˆ«çš„æç¤ºï¼Œåœ¨æ¨ç†æ—¶é—´æ—¶æé«˜æ€§èƒ½ï¼Œæˆ‘ä»¬çš„å®éªŒè¡¨æ˜ï¼Œåœ¨æŸäº›è§†è§‰ä»»åŠ¡ä¸Šç›¸å¯¹æ”¹è¿›äº†é«˜è¾¾çº¦50ï¼…ã€‚ä¸‹æ¸¸æ€§èƒ½æ˜¯åœ¨MathVistaã€M3CoTå’ŒGeoBench-VLMæ•°æ®é›†çš„ä»»åŠ¡ä¸­è¿›è¡Œè®­ç»ƒå’Œè¯„ä¼°çš„ã€‚é‡è¦çš„æ˜¯ï¼Œæˆ‘ä»¬çš„æ–¹æ³•è¡¨æ˜ï¼Œè¿›åŒ–æç¤ºä¼˜åŒ–æŒ‡å¯¼è¯­è¨€æ¨¡å‹è¿›è¡Œè‡ªæˆ‘æ¨ç†å‘ç°ï¼Œä»è€Œå¯¼è‡´ä»»åŠ¡ä¹‹é—´çš„é›¶æ ·æœ¬æ³›åŒ–èƒ½åŠ›æé«˜ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.23503v1">PDF</a> Published at ICLR 2025 Workshop on Reasoning and Planning for LLMs</p>
<p><strong>Summary</strong><br>ä¼˜åŒ–è§†è§‰è¯­è¨€æ¨¡å‹æç¤ºæ¡†æ¶ï¼Œä¿ƒè¿›å¤šæ¨¡æ€æ¨ç†ï¼Œæ— éœ€æ¨¡å‹é‡è®­ã€‚åˆ©ç”¨è¿›åŒ–ç®—æ³•å¼•å¯¼è§†è§‰ä»»åŠ¡ä¸‹æ¸¸çš„æç¤ºæ›´æ–°ï¼Œæ”¹è¿›åŸºç¡€æç¤ºæ›´æ–°ç®—æ³•ã€‚æ¨¡å‹ç‹¬ç«‹å‘ç°è·¨å¤šä»£è¿›åŒ–çš„æ¸è¿›é—®é¢˜è§£å†³æŠ€æœ¯ã€‚ä¾‹å¦‚ï¼Œæ¨¡å‹æ¨ç†å‡ºåˆ©ç”¨Pythonè§£é‡Šå™¨æ‰§è¡Œè£å‰ªã€å›¾åƒåˆ†å‰²æˆ–é¥±å’Œåº¦æ›´æ”¹ç­‰ä»»åŠ¡èƒ½æ˜¾è‘—æé«˜æ€§èƒ½ã€‚é€šè¿‡ç³»ç»Ÿçº§åˆ«çš„XMLæ ‡ç­¾æ˜ç¡®è°ƒç”¨Pythonè§£é‡Šå™¨è®¿é—®åŠŸèƒ½ï¼Œå°†å…¶èå…¥ç³»ç»Ÿçº§åˆ«çš„æç¤ºä¸­ï¼Œå¯åœ¨æ¨ç†æ—¶é—´æé«˜æ€§èƒ½è¡¨ç°ï¼Œå®éªŒç»“æœæ˜¾è¯¥åŠŸèƒ½åœ¨æŸäº›è§†è§‰ä»»åŠ¡ä¸­æé«˜ç›¸å¯¹æ€§èƒ½è¾¾çº¦50%ã€‚è®­ç»ƒå’Œè¯„ä¼°èŒƒå›´åŒ…æ‹¬MathVistaã€M3CoTå’ŒGeoBench-VLMæ•°æ®é›†çš„ä»»åŠ¡ã€‚æ–¹æ³•æ˜¾ç¤ºè¿›åŒ–æç¤ºä¼˜åŒ–å¯å¼•å¯¼è¯­è¨€æ¨¡å‹è¿›è¡Œè‡ªæˆ‘æ¨ç†å‘ç°ï¼Œæé«˜è·¨ä»»åŠ¡çš„é›¶æ ·æœ¬æ³›åŒ–èƒ½åŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>æ¡†æ¶ä½¿ç”¨è¿›åŒ–ç®—æ³•ä¼˜åŒ–è§†è§‰è¯­è¨€æ¨¡å‹çš„æç¤ºï¼Œä»¥ä¿ƒè¿›å¤šæ¨¡æ€æ¨ç†ã€‚</li>
<li>ä¸åŸºç¡€æç¤ºæ›´æ–°ç®—æ³•ç›¸æ¯”ï¼Œè¿›åŒ–ç®—æ³•èƒ½æ›´å¥½åœ°æ”¹è¿›æ€§èƒ½ã€‚</li>
<li>æ¨¡å‹èƒ½å¤Ÿç‹¬ç«‹å‘ç°æ¸è¿›é—®é¢˜è§£å†³æŠ€æœ¯ï¼Œé€šè¿‡å¤šä»£è¿›åŒ–æ”¹å–„æ€§èƒ½ã€‚</li>
<li>é€šè¿‡ç³»ç»Ÿçº§XMLæ ‡ç­¾æ˜ç¡®è°ƒç”¨Pythonç­‰è§£é‡Šå™¨æ‰§è¡Œç‰¹å®šä»»åŠ¡å¯ä»¥æé«˜æ€§èƒ½ã€‚</li>
<li>å°†è¿™äº›åŠŸèƒ½èå…¥ç³»ç»Ÿçº§æç¤ºä¸­ï¼Œåœ¨æ¨ç†æ—¶é—´æé«˜æ€§èƒ½è¡¨ç°ã€‚</li>
<li>å®éªŒç»“æœæ˜¾ç¤ºåœ¨æŸäº›è§†è§‰ä»»åŠ¡ä¸­æ€§èƒ½æé«˜è¾¾çº¦50%ã€‚</li>
<li>è®­ç»ƒå’Œè¯„ä¼°æ¶µç›–MathVistaã€M3CoTå’ŒGeoBench-VLMæ•°æ®é›†çš„ä»»åŠ¡ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.23503">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-641ab55735816cb108bc2ff403fc3d89.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0cc27723580f3d6a7e8a4e6926a4a508.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-8562c795f743889194892fc77fa2bcde.jpg" align="middle">
</details>


<h1 id="-19"><a href="#-19" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-04-03/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">https://kedreamix.github.io/Talk2Paper/Paper/2025-04-03/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                                    <span class="chip bg-color">åŒ»å­¦å›¾åƒ</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-04-04/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-6e7076a4f8670e51111dc0215d03e1a0.jpg" class="responsive-img" alt="åŒ»å­¦å›¾åƒ">
                        
                        <span class="card-title">åŒ»å­¦å›¾åƒ</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            åŒ»å­¦å›¾åƒ æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-04-04  BioAtt Anatomical Prior Driven Low-Dose CT Denoising
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-04-04
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/" class="post-category">
                                    åŒ»å­¦å›¾åƒ
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                        <span class="chip bg-color">åŒ»å­¦å›¾åƒ</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-04-04/Diffusion%20Models/">
                    <div class="card-image">
                        
                        <img src="https://pica.zhimg.com/v2-e3a59a108dd9892170ffcc61518b1904.jpg" class="responsive-img" alt="Diffusion Models">
                        
                        <span class="card-title">Diffusion Models</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Diffusion Models æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-04-04  ILLUME+ Illuminating Unified MLLM with Dual Visual Tokenization and   Diffusion Refinement
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-04-04
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Diffusion-Models/" class="post-category">
                                    Diffusion Models
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Diffusion-Models/">
                        <span class="chip bg-color">Diffusion Models</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">32140.8k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
