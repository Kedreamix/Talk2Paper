<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Few-Shot">
    <meta name="description" content="Few-Shot æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-04-03  Is Temporal Prompting All We Need For Limited Labeled Action   Recognition?">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Few-Shot | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-f811b76b1be7649f806abbab165d8628.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Few-Shot</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Few-Shot/">
                                <span class="chip bg-color">Few-Shot</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Few-Shot/" class="post-category">
                                Few-Shot
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-04-03
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-04-04
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    20.6k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    84 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-04-03-æ›´æ–°"><a href="#2025-04-03-æ›´æ–°" class="headerlink" title="2025-04-03 æ›´æ–°"></a>2025-04-03 æ›´æ–°</h1><h2 id="Is-Temporal-Prompting-All-We-Need-For-Limited-Labeled-Action-Recognition"><a href="#Is-Temporal-Prompting-All-We-Need-For-Limited-Labeled-Action-Recognition" class="headerlink" title="Is Temporal Prompting All We Need For Limited Labeled Action   Recognition?"></a>Is Temporal Prompting All We Need For Limited Labeled Action   Recognition?</h2><p><strong>Authors:Shreyank N Gowda, Boyan Gao, Xiao Gu, Xiaobo Jin</strong></p>
<p>Video understanding has shown remarkable improvements in recent years, largely dependent on the availability of large scaled labeled datasets. Recent advancements in visual-language models, especially based on contrastive pretraining, have shown remarkable generalization in zero-shot tasks, helping to overcome this dependence on labeled datasets. Adaptations of such models for videos, typically involve modifying the architecture of vision-language models to cater to video data. However, this is not trivial, since such adaptations are mostly computationally intensive and struggle with temporal modeling. We present TP-CLIP, an adaptation of CLIP that leverages temporal visual prompting for temporal adaptation without modifying the core CLIP architecture. This preserves its generalization abilities. TP-CLIP efficiently integrates into the CLIP architecture, leveraging its pre-trained capabilities for video data. Extensive experiments across various datasets demonstrate its efficacy in zero-shot and few-shot learning, outperforming existing approaches with fewer parameters and computational efficiency. In particular, we use just 1&#x2F;3 the GFLOPs and 1&#x2F;28 the number of tuneable parameters in comparison to recent state-of-the-art and still outperform it by up to 15.8% depending on the task and dataset. </p>
<blockquote>
<p>è§†é¢‘ç†è§£åœ¨æœ€è¿‘å‡ å¹´å–å¾—äº†æ˜¾è‘—çš„è¿›æ­¥ï¼Œè¿™å¾ˆå¤§ç¨‹åº¦ä¸Šä¾èµ–äºå¤§è§„æ¨¡æ ‡æ³¨æ•°æ®é›†çš„å¯ç”¨æ€§ã€‚åŸºäºå¯¹æ¯”é¢„è®­ç»ƒçš„è§†è§‰è¯­è¨€æ¨¡å‹çš„æœ€æ–°è¿›å±•åœ¨é›¶æ ·æœ¬ä»»åŠ¡ä¸­è¡¨ç°å‡ºäº†æ˜¾è‘—çš„æ³›åŒ–èƒ½åŠ›ï¼Œæœ‰åŠ©äºå…‹æœå¯¹æ ‡æ³¨æ•°æ®é›†çš„ä¾èµ–ã€‚æ­¤ç±»æ¨¡å‹å¯¹è§†é¢‘çš„é€‚åº”é€šå¸¸æ¶‰åŠè°ƒæ•´è§†è§‰è¯­è¨€æ¨¡å‹çš„æ¶æ„ä»¥é€‚åº”è§†é¢‘æ•°æ®ã€‚ç„¶è€Œï¼Œè¿™å¹¶ä¸æ˜¯ä¸€ä»¶ç®€å•çš„äº‹ï¼Œå› ä¸ºè¿™ç§é€‚åº”é€šå¸¸éœ€è¦å¤§é‡çš„è®¡ç®—å¹¶ä¸”é¢ä¸´æ—¶é—´å»ºæ¨¡çš„æŒ‘æˆ˜ã€‚æˆ‘ä»¬æå‡ºäº†TP-CLIPï¼Œè¿™æ˜¯CLIPçš„ä¸€ç§é€‚åº”ç‰ˆæœ¬ï¼Œå®ƒåˆ©ç”¨æ—¶é—´è§†è§‰æç¤ºè¿›è¡Œæ—¶é—´é€‚åº”ï¼Œè€Œæ— éœ€ä¿®æ”¹CLIPæ¶æ„çš„æ ¸å¿ƒéƒ¨åˆ†ï¼Œä»è€Œä¿ç•™äº†å…¶æ³›åŒ–èƒ½åŠ›ã€‚TP-CLIPæœ‰æ•ˆåœ°é›†æˆåˆ°CLIPæ¶æ„ä¸­ï¼Œåˆ©ç”¨å…¶å¯¹è§†é¢‘æ•°æ®çš„é¢„è®­ç»ƒèƒ½åŠ›ã€‚åœ¨ä¸åŒæ•°æ®é›†ä¸Šçš„å¤§é‡å®éªŒè¡¨æ˜ï¼Œå®ƒåœ¨é›¶æ ·æœ¬å’Œå°‘æ ·æœ¬å­¦ä¹ ä¸­çš„æœ‰æ•ˆæ€§ï¼Œä½¿ç”¨è¾ƒå°‘çš„å‚æ•°å’Œè®¡ç®—æ•ˆç‡å°±èƒ½è¶…è¶Šç°æœ‰æ–¹æ³•ã€‚ç‰¹åˆ«æ˜¯ï¼Œæˆ‘ä»¬ä½¿ç”¨çš„GFLOPsåªæœ‰æœ€è¿‘çš„å…ˆè¿›æ°´å¹³çš„1&#x2F;3ï¼Œå¯è°ƒå‚æ•°çš„æ•°é‡æ˜¯1&#x2F;28ï¼Œæ ¹æ®ä»»åŠ¡å’Œæ•°æ®é›†çš„ä¸åŒï¼Œä»ç„¶èƒ½è¶…å‡ºå…¶æ€§èƒ½é«˜è¾¾15.8%ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.01890v1">PDF</a> Accepted in CVPR-W 2025</p>
<p><strong>Summary</strong></p>
<p>è§†è§‰ç†è§£å’Œè§†è§‰è¯­è¨€æ¨¡å‹åœ¨è¿‘å¹´çš„ç ”ç©¶ä¸­å–å¾—æ˜¾è‘—è¿›æ­¥ï¼Œå°¤å…¶ä¾èµ–å¤§è§„æ¨¡æ ‡æ³¨æ•°æ®é›†ã€‚åŸºäºå¯¹æ¯”é¢„è®­ç»ƒçš„æ¨¡å‹åœ¨é›¶æ ·æœ¬ä»»åŠ¡ä¸­å±•ç°å‡ºå¼ºå¤§çš„æ³›åŒ–èƒ½åŠ›ã€‚æœ¬æ–‡ä»‹ç»TP-CLIPæ¨¡å‹ï¼Œå®ƒåœ¨CLIPæ¨¡å‹åŸºç¡€ä¸Šè¿›è¡Œæ”¹è¿›ï¼Œé€šè¿‡åˆ©ç”¨æ—¶ç©ºè§†è§‰æç¤ºå®ç°æ—¶ç©ºé€‚åº”ï¼Œæ— éœ€ä¿®æ”¹æ ¸å¿ƒCLIPæ¶æ„ï¼Œä¿ç•™äº†å…¶æ³›åŒ–èƒ½åŠ›ã€‚TP-CLIPé«˜æ•ˆé›†æˆäºCLIPæ¶æ„ä¸­ï¼Œåˆ©ç”¨å…¶å¯¹è§†é¢‘æ•°æ®çš„é¢„è®­ç»ƒèƒ½åŠ›ã€‚å®éªŒè¯æ˜å…¶åœ¨é›¶æ ·æœ¬å’Œå°‘æ ·æœ¬å­¦ä¹ ä¸­çš„æœ‰æ•ˆæ€§ï¼Œç›¸è¾ƒäºå…¶ä»–æœ€æ–°æŠ€æœ¯ä½¿ç”¨æ›´å°‘çš„å‚æ•°å’Œè®¡ç®—èµ„æºï¼Œè¡¨ç°å‡ºæ›´é«˜çš„æ€§èƒ½ã€‚ç‰¹åˆ«æ˜¯åœ¨ä¸æœ€æ–°æŠ€æœ¯ç›¸æ¯”ï¼Œä½¿ç”¨è¾ƒå°‘çš„GFLOPså’Œå‚æ•°æ•°é‡çš„æƒ…å†µä¸‹ï¼Œä¾ç„¶èƒ½åœ¨ä¸åŒä»»åŠ¡å’Œæ•°æ®é›†ä¸Šé«˜å‡ºæœ€é«˜è¾¾15.8%çš„æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è§†é¢‘ç†è§£å’Œè§†è§‰è¯­è¨€æ¨¡å‹ä¾èµ–å¤§è§„æ¨¡æ ‡æ³¨æ•°æ®é›†å–å¾—æ˜¾è‘—è¿›æ­¥ã€‚</li>
<li>åŸºäºå¯¹æ¯”é¢„è®­ç»ƒçš„æ¨¡å‹å±•ç°å‡ºå¼ºå¤§çš„é›¶æ ·æœ¬ä»»åŠ¡æ³›åŒ–èƒ½åŠ›ã€‚</li>
<li>TP-CLIPæ¨¡å‹æ”¹è¿›äº†CLIPæ¶æ„ï¼Œåˆ©ç”¨æ—¶ç©ºè§†è§‰æç¤ºå®ç°æ—¶ç©ºé€‚åº”ã€‚</li>
<li>TP-CLIPä¿ç•™äº†CLIPçš„æ³›åŒ–èƒ½åŠ›ä¸”æ— éœ€ä¿®æ”¹æ ¸å¿ƒæ¶æ„ã€‚</li>
<li>TP-CLIPé›†æˆäº†é«˜æ•ˆçš„è§†é¢‘æ•°æ®å¤„ç†èƒ½åŠ›ï¼Œåˆ©ç”¨é¢„è®­ç»ƒèƒ½åŠ›åº”å¯¹è§†é¢‘æ•°æ®ã€‚</li>
<li>å®éªŒè¯æ˜TP-CLIPåœ¨é›¶æ ·æœ¬å’Œå°‘æ ·æœ¬å­¦ä¹ ä¸­çš„æœ‰æ•ˆæ€§ä¼˜äºå…¶ä»–æŠ€æœ¯ã€‚</li>
<li>TP-CLIPåœ¨å‚æ•°å’Œè®¡ç®—èµ„æºä½¿ç”¨ä¸Šæ›´ä¸ºé«˜æ•ˆï¼Œæ€§èƒ½è¡¨ç°ä¼˜å¼‚ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.01890">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-17a2c1c235389617f72e8273e78f7d71.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9b6c2aa3ae99becc4e7c0927328b4b70.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-417ef1ffb0067a4e2cf829aae9072368.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-1e6bb86d792584235b9d1b94a78b3988.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-f73eb86a510e50a1c0627a01fcf66c37.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="CoRAG-Collaborative-Retrieval-Augmented-Generation"><a href="#CoRAG-Collaborative-Retrieval-Augmented-Generation" class="headerlink" title="CoRAG: Collaborative Retrieval-Augmented Generation"></a>CoRAG: Collaborative Retrieval-Augmented Generation</h2><p><strong>Authors:Aashiq Muhamed, Mona Diab, Virginia Smith</strong></p>
<p>Retrieval-Augmented Generation (RAG) models excel in knowledge-intensive tasks, especially under few-shot learning constraints. We introduce CoRAG, a framework extending RAG to collaborative settings, where clients jointly train a shared model using a collaborative passage store. To evaluate CoRAG, we introduce CRAB, a benchmark for collaborative homogeneous open-domain question answering. Our experiments demonstrate that CoRAG consistently outperforms both parametric collaborative learning methods and locally trained RAG models in low-resource scenarios. Further analysis reveals the critical importance of relevant passages within the shared store, the surprising benefits of incorporating irrelevant passages, and the potential for hard negatives to negatively impact performance. This introduces a novel consideration in collaborative RAG: the trade-off between leveraging a collectively enriched knowledge base and the potential risk of incorporating detrimental passages from other clients. Our findings underscore the viability of CoRAG, while also highlighting key design challenges and promising avenues for future research. </p>
<blockquote>
<p>æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰æ¨¡å‹åœ¨çŸ¥è¯†å¯†é›†å‹ä»»åŠ¡ä¸Šè¡¨ç°å‡ºè‰²ï¼Œå°¤å…¶æ˜¯åœ¨å°æ ·æœ¬å­¦ä¹ çº¦æŸä¸‹ã€‚æˆ‘ä»¬ä»‹ç»äº†CoRAGï¼Œè¿™æ˜¯ä¸€ä¸ªå°†RAGæ‰©å±•åˆ°åä½œè®¾ç½®ä¸­çš„æ¡†æ¶ï¼Œå®¢æˆ·ç«¯ä½¿ç”¨åä½œæ®µè½å­˜å‚¨æ¥å…±åŒè®­ç»ƒå…±äº«æ¨¡å‹ã€‚ä¸ºäº†è¯„ä¼°CoRAGï¼Œæˆ‘ä»¬æ¨å‡ºäº†CRABï¼Œè¿™æ˜¯ä¸€ä¸ªç”¨äºåä½œåŒæ„å¼€æ”¾åŸŸé—®ç­”çš„åŸºå‡†æµ‹è¯•ã€‚æˆ‘ä»¬çš„å®éªŒè¡¨æ˜ï¼Œåœ¨èµ„æºåŒ®ä¹çš„åœºæ™¯ä¸‹ï¼ŒCoRAGå§‹ç»ˆä¼˜äºå‚æ•°åŒ–åä½œå­¦ä¹ æ–¹æ³•ä»¥åŠæœ¬åœ°è®­ç»ƒçš„RAGæ¨¡å‹ã€‚è¿›ä¸€æ­¥çš„åˆ†ææ­ç¤ºäº†å…±äº«å­˜å‚¨ä¸­çš„ç›¸å…³æ®µè½çš„é‡è¦æ€§ï¼Œä»¥åŠèå…¥ä¸ç›¸å…³æ®µè½çš„æ„å¤–å¥½å¤„ï¼Œä»¥åŠç¡¬è´Ÿæ ·æœ¬å¯èƒ½å¯¹æ€§èƒ½äº§ç”Ÿçš„è´Ÿé¢å½±å“ã€‚è¿™ä¸ºåä½œRAGå¼•å…¥äº†ä¸€ç§æ–°çš„è€ƒè™‘å› ç´ ï¼šåœ¨åˆ©ç”¨é›†ä½“ä¸°å¯Œçš„çŸ¥è¯†åº“å’Œå¯èƒ½çš„é£é™©ä¹‹é—´æƒè¡¡ï¼Œåè€…æ¥è‡ªèå…¥æ¥è‡ªå…¶ä»–å®¢æˆ·ç«¯çš„ä¸åˆ©æ®µè½ã€‚æˆ‘ä»¬çš„ç ”ç©¶ç»“æœå¼ºè°ƒäº†CoRAGçš„å¯è¡Œæ€§ï¼ŒåŒæ—¶çªå‡ºäº†å…³é”®çš„è®¾è®¡æŒ‘æˆ˜ä»¥åŠæœªæ¥ç ”ç©¶çš„å¸Œæœ›æ–¹å‘ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.01883v1">PDF</a> NAACL 2024</p>
<p><strong>Summary</strong></p>
<p>CoRAGæ¡†æ¶æ‰©å±•äº†RAGæ¨¡å‹ï¼Œä½¿å…¶é€‚åº”åä½œç¯å¢ƒã€‚åœ¨çŸ¥è¯†å¯†é›†å‹ä»»åŠ¡ä¸­ï¼Œç‰¹åˆ«æ˜¯åœ¨å°æ ·æœ¬å­¦ä¹ åœºæ™¯ä¸‹ï¼ŒCoRAGé€šè¿‡å…±åŒè®­ç»ƒå…±äº«æ¨¡å‹å’Œåä½œæ®µè½å­˜å‚¨æ¥è¡¨ç°å‡ºè‰²ã€‚é€šè¿‡å¼•å…¥CRABåŸºå‡†æµ‹è¯•æ¥è¯„ä¼°CoRAGçš„æ•ˆæœï¼Œæ˜¾ç¤ºå…¶åœ¨ä½èµ„æºåœºæ™¯ä¸­å§‹ç»ˆä¼˜äºå‚æ•°åŒ–åä½œå­¦ä¹ æ–¹æ³•å’Œæœ¬åœ°è®­ç»ƒçš„RAGæ¨¡å‹ã€‚ç ”ç©¶è¿›ä¸€æ­¥åˆ†æäº†å…±äº«å­˜å‚¨ä¸­çš„å…³é”®æ®µè½çš„é‡è¦æ€§ï¼Œå¹¶æ­ç¤ºäº†ä¸ç›¸å…³æ®µè½å’Œç¡¬è´Ÿæ ·æœ¬å¯¹æ€§èƒ½çš„å½±å“ï¼Œä¸ºåä½œRAGæå‡ºäº†æƒè¡¡è€ƒè™‘ï¼šåœ¨åˆ©ç”¨é›†ä½“ä¸°å¯Œçš„çŸ¥è¯†åº“å’Œå¯èƒ½å¼•å…¥æœ‰å®³æ®µè½ä¹‹é—´å–å¾—å¹³è¡¡ã€‚ç ”ç©¶å¼ºè°ƒäº†CoRAGçš„å¯è¡Œæ€§ï¼Œå¹¶æŒ‡å‡ºäº†å…³é”®çš„è®¾è®¡æŒ‘æˆ˜å’Œæœªæ¥çš„ç ”ç©¶æ–¹å‘ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>CoRAGæ¡†æ¶æ‰©å±•äº†RAGæ¨¡å‹ï¼Œä½¿å…¶èƒ½åœ¨åä½œç¯å¢ƒä¸­è¿è¡Œã€‚</li>
<li>CoRAGåœ¨çŸ¥è¯†å¯†é›†å‹ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ï¼Œå°¤å…¶æ˜¯åœ¨å°æ ·æœ¬å­¦ä¹ åœºæ™¯ä¸­ã€‚</li>
<li>CoRAGé€šè¿‡å…±åŒè®­ç»ƒå…±äº«æ¨¡å‹å’Œåä½œæ®µè½å­˜å‚¨æ¥æå‡æ€§èƒ½ã€‚</li>
<li>CRABåŸºå‡†æµ‹è¯•ç”¨äºè¯„ä¼°CoRAGçš„æ•ˆæœã€‚</li>
<li>CoRAGåœ¨ä½èµ„æºåœºæ™¯ä¸­ä¼˜äºå…¶ä»–æ–¹æ³•ã€‚</li>
<li>åˆ†æäº†å…±äº«å­˜å‚¨ä¸­çš„å…³é”®æ®µè½çš„é‡è¦æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.01883">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-d9c6e1f0a61755a3f9f712e4b7b91f32.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5d4fb806a88c4f81d2ddf8091020e952.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-fb365c59619afbe8e28f258f62551101.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-bb375c1730df66db17a9fd26ff90ad60.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-21398b1840e6290eb4a54e9a3279718e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-63b94d895aa30c588b0d2430bd2f15fd.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="UniFault-A-Fault-Diagnosis-Foundation-Model-from-Bearing-Data"><a href="#UniFault-A-Fault-Diagnosis-Foundation-Model-from-Bearing-Data" class="headerlink" title="UniFault: A Fault Diagnosis Foundation Model from Bearing Data"></a>UniFault: A Fault Diagnosis Foundation Model from Bearing Data</h2><p><strong>Authors:Emadeldeen Eldele, Mohamed Ragab, Xu Qing,  Edward, Zhenghua Chen, Min Wu, Xiaoli Li, Jay Lee</strong></p>
<p>Machine fault diagnosis (FD) is a critical task for predictive maintenance, enabling early fault detection and preventing unexpected failures. Despite its importance, existing FD models are operation-specific with limited generalization across diverse datasets. Foundation models (FM) have demonstrated remarkable potential in both visual and language domains, achieving impressive generalization capabilities even with minimal data through few-shot or zero-shot learning. However, translating these advances to FD presents unique hurdles. Unlike the large-scale, cohesive datasets available for images and text, FD datasets are typically smaller and more heterogeneous, with significant variations in sampling frequencies and the number of channels across different systems and applications. This heterogeneity complicates the design of a universal architecture capable of effectively processing such diverse data while maintaining robust feature extraction and learning capabilities. In this paper, we introduce UniFault, a foundation model for fault diagnosis that systematically addresses these issues. Specifically, the model incorporates a comprehensive data harmonization pipeline featuring two key innovations. First, a unification scheme transforms multivariate inputs into standardized univariate sequences while retaining local inter-channel relationships. Second, a novel cross-domain temporal fusion strategy mitigates distribution shifts and enriches sample diversity and count, improving the model generalization across varying conditions. UniFault is pretrained on over 9 billion data points spanning diverse FD datasets, enabling superior few-shot performance. Extensive experiments on real-world FD datasets demonstrate that UniFault achieves SoTA performance, setting a new benchmark for fault diagnosis models and paving the way for more scalable and robust predictive maintenance solutions. </p>
<blockquote>
<p>æœºå™¨æ•…éšœè¯Šæ–­ï¼ˆFDï¼‰æ˜¯é¢„æµ‹æ€§ç»´æŠ¤ä¸­çš„ä¸€é¡¹å…³é”®ä»»åŠ¡ï¼Œèƒ½å¤Ÿå®ç°æ—©æœŸæ•…éšœæ£€æµ‹å¹¶é˜²æ­¢æ„å¤–æ•…éšœã€‚å°½ç®¡å…¶é‡è¦æ€§å¾ˆé«˜ï¼Œä½†ç°æœ‰çš„FDæ¨¡å‹éƒ½æ˜¯é’ˆå¯¹ç‰¹å®šæ“ä½œçš„ï¼Œåœ¨å¤šç§æ•°æ®é›†ä¸Šçš„æ³›åŒ–èƒ½åŠ›æœ‰é™ã€‚åŸºç¡€æ¨¡å‹ï¼ˆFMï¼‰åœ¨è§†è§‰å’Œè¯­è¨€é¢†åŸŸéƒ½å±•ç°å‡ºäº†å¼•äººæ³¨ç›®çš„æ½œåŠ›ï¼Œå³ä½¿åœ¨é€šè¿‡å°‘é‡æ•°æ®çš„æƒ…å†µä¸‹ï¼Œä¹Ÿèƒ½å®ç°ä»¤äººå°è±¡æ·±åˆ»çš„æ³›åŒ–èƒ½åŠ›ã€‚ç„¶è€Œï¼Œå°†è¿™äº›è¿›å±•åº”ç”¨äºFDå´å­˜åœ¨ç‹¬ç‰¹çš„éšœç¢ã€‚ä¸å¯ç”¨äºå›¾åƒå’Œæ–‡æœ¬çš„å¤§è§„æ¨¡ã€è¿è´¯æ•°æ®é›†ä¸åŒï¼ŒFDæ•°æ®é›†é€šå¸¸è¾ƒå°ä¸”æ›´åŠ å¤šæ ·åŒ–ï¼Œä¸åŒç³»ç»Ÿå’Œåº”ç”¨ä¹‹é—´çš„é‡‡æ ·é¢‘ç‡å’Œæ•°æ®é€šé“æ•°é‡å­˜åœ¨é‡å¤§å·®å¼‚ã€‚è¿™ç§å¼‚è´¨æ€§ä½¿å¾—è®¾è®¡ä¸€ä¸ªèƒ½å¤Ÿå¤„ç†è¿™ç§å¤šæ ·åŒ–æ•°æ®çš„é€šç”¨æ¶æ„å˜å¾—æ›´åŠ å¤æ‚ï¼ŒåŒæ—¶è¿˜éœ€è¦ä¿æŒå¼ºå¤§çš„ç‰¹å¾æå–å’Œå­¦ä¹ èƒ½åŠ›ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†UniFaultï¼Œè¿™æ˜¯ä¸€ä¸ªç”¨äºæ•…éšœè¯Šæ–­çš„åŸºç¡€æ¨¡å‹ï¼Œç³»ç»Ÿåœ°è§£å†³äº†è¿™äº›é—®é¢˜ã€‚å…·ä½“æ¥è¯´ï¼Œè¯¥æ¨¡å‹é‡‡ç”¨å…¨é¢çš„æ•°æ®è°ƒå’Œç®¡é“ï¼ŒåŒ…å«ä¸¤ä¸ªå…³é”®åˆ›æ–°ç‚¹ã€‚é¦–å…ˆï¼Œç»Ÿä¸€æ–¹æ¡ˆå°†å¤šå…ƒè¾“å…¥è½¬æ¢ä¸ºæ ‡å‡†åŒ–çš„å•å˜é‡åºåˆ—ï¼ŒåŒæ—¶ä¿ç•™å±€éƒ¨é€šé“é—´å…³ç³»ã€‚å…¶æ¬¡ï¼Œä¸€ç§æ–°çš„è·¨åŸŸæ—¶é—´èåˆç­–ç•¥ç¼“è§£äº†åˆ†å¸ƒåç§»é—®é¢˜å¹¶ä¸°å¯Œäº†æ ·æœ¬çš„å¤šæ ·æ€§å’Œæ•°é‡ï¼Œæé«˜äº†æ¨¡å‹åœ¨ä¸åŒæ¡ä»¶ä¸‹çš„æ³›åŒ–èƒ½åŠ›ã€‚UniFaultåœ¨è·¨è¶Šå¤šç§FDæ•°æ®é›†è¶…è¿‡9äº¿ä¸ªæ•°æ®ç‚¹ä¸Šè¿›è¡Œé¢„è®­ç»ƒï¼Œå®ç°äº†å‡ºè‰²çš„å°‘é‡æ ·æœ¬æ€§èƒ½ã€‚åœ¨çœŸå®ä¸–ç•Œçš„FDæ•°æ®é›†ä¸Šè¿›è¡Œçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼ŒUniFaultè¾¾åˆ°äº†æœ€æ–°æŠ€æœ¯æ€§èƒ½ï¼Œä¸ºæ•…éšœè¯Šæ–­æ¨¡å‹è®¾å®šäº†æ–°çš„åŸºå‡†ï¼Œå¹¶ä¸ºæ›´å¯æ‰©å±•å’Œç¨³å¥çš„é¢„æµ‹æ€§ç»´æŠ¤è§£å†³æ–¹æ¡ˆé“ºå¹³äº†é“è·¯ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.01373v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†UniFaultè¿™ä¸€æ•…éšœé¢„æµ‹æ¨¡å‹ï¼Œè¯¥æ¨¡å‹è§£å†³äº†ç°æœ‰æ•…éšœé¢„æµ‹æ¨¡å‹åœ¨å¤šæ ·æ•°æ®é›†ä¸Šæ³›åŒ–èƒ½åŠ›å—é™çš„é—®é¢˜ã€‚æ–‡ç« é€šè¿‡å¼•å…¥æ•°æ®è°ƒå’Œç®¡é“å’Œè·¨åŸŸæ—¶é—´èåˆç­–ç•¥ï¼Œå®ç°äº†å¯¹å¤šå…ƒè¾“å…¥çš„æ ‡å‡†åŒ–ä¸€å…ƒåºåˆ—è½¬æ¢ï¼Œå¹¶ä¸°å¯Œäº†æ ·æœ¬å¤šæ ·æ€§å’Œæ•°é‡ï¼Œæé«˜äº†æ¨¡å‹åœ¨ä¸åŒæ¡ä»¶ä¸‹çš„æ³›åŒ–èƒ½åŠ›ã€‚UniFaultåœ¨çœŸå®æ•…éšœé¢„æµ‹æ•°æ®é›†ä¸Šçš„è¡¨ç°è¾¾åˆ°äº†æœ€æ–°æ°´å¹³ï¼Œä¸ºæ•…éšœé¢„æµ‹æ¨¡å‹æ ‘ç«‹äº†æ–°çš„æ ‡æ†ï¼Œä¸ºæ›´å¯æ‰©å±•å’Œç¨³å¥çš„é¢„æµ‹ç»´æŠ¤è§£å†³æ–¹æ¡ˆé“ºå¹³äº†é“è·¯ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<p>ä»¥ä¸‹æ˜¯åŸºäºæ–‡æœ¬å†…å®¹çš„ä¸ƒä¸ªå…³é”®è§è§£ï¼š</p>
<ol>
<li>æ•…éšœè¯Šæ–­ï¼ˆFDï¼‰æ˜¯é¢„æµ‹æ€§ç»´æŠ¤ä¸­çš„å…³é”®ä»»åŠ¡ï¼Œæœ‰åŠ©äºæ—©æœŸæ•…éšœæ£€æµ‹å¹¶é¿å…æ„å¤–æ•…éšœã€‚</li>
<li>å°½ç®¡é‡è¦ï¼Œä½†ç°æœ‰çš„FDæ¨¡å‹åœ¨æ“ä½œä¸Šå…·æœ‰ç‰¹å®šæ€§ï¼Œåœ¨å¤šæ ·æ•°æ®é›†ä¸Šçš„æ³›åŒ–èƒ½åŠ›å—é™ã€‚</li>
<li>Foundationæ¨¡å‹ï¼ˆFMï¼‰åœ¨è§†è§‰å’Œè¯­è¨€é¢†åŸŸå±•ç°äº†å·¨å¤§çš„æ½œåŠ›ï¼Œå³ä½¿åœ¨å°‘é‡æ•°æ®ä¸‹ä¹Ÿèƒ½å®ç°ä»¤äººå°è±¡æ·±åˆ»çš„æ³›åŒ–èƒ½åŠ›ã€‚</li>
<li>å°†FMçš„è¿›å±•åº”ç”¨äºFDé¢ä¸´ç‹¬ç‰¹æŒ‘æˆ˜ï¼Œå› ä¸ºFDæ•°æ®é›†é€šå¸¸è¾ƒå°ã€æ›´å¼‚è´¨ã€‚</li>
<li>UniFaultæ˜¯ä¸€ä¸ªé’ˆå¯¹æ•…éšœè¯Šæ–­çš„foundationæ¨¡å‹ï¼Œé€šè¿‡æ•°æ®è°ƒå’Œç®¡é“è§£å†³è¿™äº›é—®é¢˜ã€‚</li>
<li>UniFaultæ¨¡å‹åŒ…å«ä¸¤ä¸ªå…³é”®åˆ›æ–°ï¼šç»Ÿä¸€æ–¹æ¡ˆå°†å¤šå…ƒè¾“å…¥è½¬æ¢ä¸ºæ ‡å‡†åŒ–ä¸€å…ƒåºåˆ—ï¼ŒåŒæ—¶ä¿ç•™å±€éƒ¨é€šé“é—´å…³ç³»ï¼›è·¨åŸŸæ—¶é—´èåˆç­–ç•¥ç¼“è§£åˆ†å¸ƒåç§»å¹¶ä¸°å¯Œæ ·æœ¬å¤šæ ·æ€§å’Œæ•°é‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.01373">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-ebf09123c7ce6dcde9c49d0f45ea7984.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-cb2f867e8850b4fa95c5225151462878.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-da8a894b53c9c78b2a1e2f39065388cd.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Detecting-PTSD-in-Clinical-Interviews-A-Comparative-Analysis-of-NLP-Methods-and-Large-Language-Models"><a href="#Detecting-PTSD-in-Clinical-Interviews-A-Comparative-Analysis-of-NLP-Methods-and-Large-Language-Models" class="headerlink" title="Detecting PTSD in Clinical Interviews: A Comparative Analysis of NLP   Methods and Large Language Models"></a>Detecting PTSD in Clinical Interviews: A Comparative Analysis of NLP   Methods and Large Language Models</h2><p><strong>Authors:Feng Chen, Dror Ben-Zeev, Gillian Sparks, Arya Kadakia, Trevor Cohen</strong></p>
<p>Post-Traumatic Stress Disorder (PTSD) remains underdiagnosed in clinical settings, presenting opportunities for automated detection to identify patients. This study evaluates natural language processing approaches for detecting PTSD from clinical interview transcripts. We compared general and mental health-specific transformer models (BERT&#x2F;RoBERTa), embedding-based methods (SentenceBERT&#x2F;LLaMA), and large language model prompting strategies (zero-shot&#x2F;few-shot&#x2F;chain-of-thought) using the DAIC-WOZ dataset. Domain-specific models significantly outperformed general models (Mental-RoBERTa F1&#x3D;0.643 vs. RoBERTa-base 0.485). LLaMA embeddings with neural networks achieved the highest performance (F1&#x3D;0.700). Zero-shot prompting using DSM-5 criteria yielded competitive results without training data (F1&#x3D;0.657). Performance varied significantly across symptom severity and comorbidity status, with higher accuracy for severe PTSD cases and patients with comorbid depression. Our findings highlight the potential of domain-adapted embeddings and LLMs for scalable screening while underscoring the need for improved detection of nuanced presentations and offering insights for developing clinically viable AI tools for PTSD assessment. </p>
<blockquote>
<p>åˆ›ä¼¤ååº”æ¿€éšœç¢ï¼ˆPTSDï¼‰åœ¨ä¸´åºŠç¯å¢ƒä¸­çš„è¯Šæ–­ç‡ä»ç„¶è¾ƒä½ï¼Œä¸ºè‡ªåŠ¨æ£€æµ‹è¯†åˆ«æ‚£è€…æä¾›äº†æœºä¼šã€‚æœ¬ç ”ç©¶è¯„ä¼°äº†ä»ä¸´åºŠè®¿è°ˆè®°å½•ä¸­æ£€æµ‹PTSDçš„è‡ªç„¶è¯­è¨€å¤„ç†æ–¹æ³•ã€‚æˆ‘ä»¬æ¯”è¾ƒäº†é€šç”¨å’Œå¿ƒç†å¥åº·ä¸“ç”¨å˜å‹å™¨æ¨¡å‹ï¼ˆBERT&#x2F;RoBERTaï¼‰ã€åŸºäºåµŒå…¥çš„æ–¹æ³•ï¼ˆSentenceBERT&#x2F;LLaMAï¼‰ï¼Œä»¥åŠå¤§å‹è¯­è¨€æ¨¡å‹çš„æç¤ºç­–ç•¥ï¼ˆé›¶æ ·æœ¬&#x2F;å°‘æ ·æœ¬&#x2F;æ€ç»´é“¾ï¼‰ä½¿ç”¨DAIC-WOZæ•°æ®é›†ã€‚ç‰¹å®šé¢†åŸŸçš„æ¨¡å‹æ˜¾è‘—ä¼˜äºé€šç”¨æ¨¡å‹ï¼ˆMental-RoBERTaçš„F1åˆ†æ•°ä¸º0.643ï¼Œè€ŒRoBERTa-baseçš„F1åˆ†æ•°ä¸º0.485ï¼‰ã€‚LLaMAåµŒå…¥ä¸ç¥ç»ç½‘ç»œç›¸ç»“åˆå–å¾—äº†æœ€ä½³æ€§èƒ½ï¼ˆF1åˆ†æ•°ä¸º0.700ï¼‰ã€‚ä½¿ç”¨DSM-5æ ‡å‡†çš„é›¶æ ·æœ¬æç¤ºåœ¨æ²¡æœ‰è®­ç»ƒæ•°æ®çš„æƒ…å†µä¸‹å–å¾—äº†å…·æœ‰ç«äº‰åŠ›çš„ç»“æœï¼ˆF1åˆ†æ•°ä¸º0.657ï¼‰ã€‚æ€§èƒ½åœ¨ç—‡çŠ¶ä¸¥é‡ç¨‹åº¦å’Œåˆå¹¶ç—‡çŠ¶æ€ä¹‹é—´å·®å¼‚æ˜¾è‘—ï¼Œä¸¥é‡PTSDæ‚£è€…å’Œä¼´æœ‰æŠ‘éƒç—‡æ‚£è€…çš„å‡†ç¡®ç‡æ›´é«˜ã€‚æˆ‘ä»¬çš„ç ”ç©¶ç»“æœçªå‡ºäº†é¢†åŸŸé€‚åº”åµŒå…¥å’Œå¤§å‹è¯­è¨€æ¨¡å‹åœ¨å¯æ‰©å±•ç­›æŸ¥ä¸­çš„æ½œåŠ›ï¼ŒåŒæ—¶å¼ºè°ƒäº†éœ€è¦æ”¹è¿›å¾®å¦™è¡¨ç°çš„æ£€æµ‹ï¼Œå¹¶ä¸ºå¼€å‘ç”¨äºPTSDè¯„ä¼°çš„ä¸´åºŠå¯è¡Œçš„AIå·¥å…·æä¾›äº†è§è§£ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.01216v1">PDF</a> 10 pages, 4 tables, 1 figure</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ç ”ç©¶ä½¿ç”¨è‡ªç„¶è¯­è¨€å¤„ç†æŠ€æœ¯å¯¹åˆ›ä¼¤ååº”æ¿€éšœç¢ï¼ˆPTSDï¼‰è¿›è¡Œè‡ªåŠ¨æ£€æµ‹ã€‚é€šè¿‡å¯¹æ¯”å¤šç§æ¨¡å‹æ–¹æ³•ï¼Œå‘ç°ç‰¹å®šé¢†åŸŸçš„æ¨¡å‹åœ¨æ£€æµ‹PTSDæ—¶è¡¨ç°ä¼˜äºé€šç”¨æ¨¡å‹ï¼ŒLLaMAåµŒå…¥ç¥ç»ç½‘ç»œçš„æ–¹æ³•æ€§èƒ½æœ€ä½³ã€‚æ­¤å¤–ï¼Œä½¿ç”¨DSM-5æ ‡å‡†çš„é›¶æ ·æœ¬æç¤ºæ³•åœ¨æ— è®­ç»ƒæ•°æ®çš„æƒ…å†µä¸‹ä¹Ÿè¡¨ç°å‡ºç«äº‰åŠ›ã€‚æ€§èƒ½ä¼šå› ç—‡çŠ¶ä¸¥é‡ç¨‹åº¦å’Œå…±ç—…çŠ¶å†µè€Œæœ‰æ‰€ä¸åŒï¼Œå¯¹äºä¸¥é‡PTSDå’Œå…±ç—…æŠ‘éƒç—‡æ‚£è€…çš„æ£€æµ‹å‡†ç¡®æ€§æ›´é«˜ã€‚ç ”ç©¶ä¸ºå¼€å‘ä¸´åºŠå¯è¡Œçš„AIå·¥å…·è¿›è¡ŒPTSDè¯„ä¼°æä¾›äº†é‡è¦è§è§£ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è‡ªç„¶è¯­è¨€å¤„ç†æŠ€æœ¯å¯ç”¨äºè‡ªåŠ¨æ£€æµ‹åˆ›ä¼¤ååº”æ¿€éšœç¢ï¼ˆPTSDï¼‰ã€‚</li>
<li>ç‰¹å®šé¢†åŸŸçš„æ¨¡å‹åœ¨æ£€æµ‹PTSDæ—¶è¡¨ç°ä¼˜äºé€šç”¨æ¨¡å‹ã€‚</li>
<li>LLaMAåµŒå…¥ç¥ç»ç½‘ç»œçš„æ–¹æ³•åœ¨æ€§èƒ½ä¸Šè¡¨ç°æœ€ä½³ã€‚</li>
<li>é›¶æ ·æœ¬æç¤ºæ³•åœ¨æ— è®­ç»ƒæ•°æ®çš„æƒ…å†µä¸‹å…·æœ‰ç«äº‰åŠ›ã€‚</li>
<li>æ€§èƒ½ä¼šå› ç—‡çŠ¶ä¸¥é‡ç¨‹åº¦å’Œå…±ç—…çŠ¶å†µè€Œæœ‰æ‰€ä¸åŒã€‚</li>
<li>ä¸¥é‡PTSDå’Œå…±ç—…æŠ‘éƒç—‡æ‚£è€…çš„æ£€æµ‹å‡†ç¡®æ€§æ›´é«˜ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.01216">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-138972990b225ac1a1787728c6c25958.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-457e35af97e6fa1cb5d74c4642eeb930.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-122fbb6b425c218e5ce22e1d8d107b5f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-eae3507695173f1c23deb3072ab23faf.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Aplicacao-de-Large-Language-Models-na-Analise-e-Sintese-de-Documentos-Juridicos-Uma-Revisao-de-Literatura"><a href="#Aplicacao-de-Large-Language-Models-na-Analise-e-Sintese-de-Documentos-Juridicos-Uma-Revisao-de-Literatura" class="headerlink" title="AplicaÃ§Ã£o de Large Language Models na AnÃ¡lise e SÃ­ntese de   Documentos JurÃ­dicos: Uma RevisÃ£o de Literatura"></a>AplicaÃ§Ã£o de Large Language Models na AnÃ¡lise e SÃ­ntese de   Documentos JurÃ­dicos: Uma RevisÃ£o de Literatura</h2><p><strong>Authors:Matheus Belarmino, Rackel Coelho, Roberto Lotudo, Jayr Pereira</strong></p>
<p>Large Language Models (LLMs) have been increasingly used to optimize the analysis and synthesis of legal documents, enabling the automation of tasks such as summarization, classification, and retrieval of legal information. This study aims to conduct a systematic literature review to identify the state of the art in prompt engineering applied to LLMs in the legal context. The results indicate that models such as GPT-4, BERT, Llama 2, and Legal-Pegasus are widely employed in the legal field, and techniques such as Few-shot Learning, Zero-shot Learning, and Chain-of-Thought prompting have proven effective in improving the interpretation of legal texts. However, challenges such as biases in models and hallucinations still hinder their large-scale implementation. It is concluded that, despite the great potential of LLMs for the legal field, there is a need to improve prompt engineering strategies to ensure greater accuracy and reliability in the generated results. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨æ³•å¾‹æ–‡æ¡£çš„åˆ†æå’Œåˆæˆä¸­å¾—åˆ°äº†è¶Šæ¥è¶Šå¤šçš„åº”ç”¨ï¼Œå®ç°äº†æ‘˜è¦ã€åˆ†ç±»å’Œæ£€ç´¢ç­‰æ³•å¾‹ä¿¡æ¯çš„è‡ªåŠ¨åŒ–ã€‚æœ¬ç ”ç©¶æ—¨åœ¨è¿›è¡Œæ–‡çŒ®çš„ç³»ç»Ÿå›é¡¾ï¼Œä»¥ç¡®å®šåœ¨æ³•å¾‹èƒŒæ™¯ä¸‹åº”ç”¨äºLLMçš„æç¤ºå·¥ç¨‹çš„æœ€å…ˆè¿›çŠ¶æ€ã€‚ç»“æœè¡¨æ˜ï¼ŒGPT-4ã€BERTã€Llama 2å’ŒLegal-Pegasusç­‰æ¨¡å‹åœ¨æ³•å¾‹é¢†åŸŸå¾—åˆ°äº†å¹¿æ³›åº”ç”¨ï¼Œè€ŒFew-shot Learningã€Zero-shot Learningå’ŒChain-of-Thoughtæç¤ºç­‰æŠ€æœ¯å·²è¢«è¯æ˜åœ¨æé«˜æ³•å¾‹æ–‡æœ¬è§£é‡Šæ–¹é¢éå¸¸æœ‰æ•ˆã€‚ç„¶è€Œï¼Œæ¨¡å‹ä¸­çš„åè§å’Œå¹»è§‰ç­‰æŒ‘æˆ˜ä»ç„¶é˜»ç¢å…¶å¤§è§„æ¨¡å®æ–½ã€‚å› æ­¤ï¼Œå°½ç®¡LLMå¯¹æ³•å¾‹é¢†åŸŸå…·æœ‰å·¨å¤§æ½œåŠ›ï¼Œä½†ä»éœ€è¦æ”¹è¿›æç¤ºå·¥ç¨‹ç­–ç•¥ï¼Œä»¥ç¡®ä¿ç”Ÿæˆç»“æœå…·æœ‰æ›´é«˜çš„å‡†ç¡®æ€§å’Œå¯é æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.00725v1">PDF</a> in Portuguese language</p>
<p><strong>Summary</strong></p>
<p>LLMså·²å¹¿æ³›åº”ç”¨äºæ³•å¾‹æ–‡æ¡£çš„åˆ†æä¸åˆæˆä¼˜åŒ–ï¼Œå®ç°äº†å¦‚æ‘˜è¦ã€åˆ†ç±»å’Œæ£€ç´¢ç­‰ä»»åŠ¡çš„è‡ªåŠ¨åŒ–ã€‚æœ¬ç ”ç©¶æ—¨åœ¨é€šè¿‡ç³»ç»Ÿæ–‡çŒ®ç»¼è¿°ï¼Œè¯†åˆ«åœ¨LLMsä¸­åº”ç”¨äºæ³•å¾‹è¯­å¢ƒçš„æç¤ºå·¥ç¨‹ç°çŠ¶ã€‚ç ”ç©¶è¡¨æ˜ï¼ŒGPT-4ã€BERTã€Llama 2å’ŒLegal-Pegasusç­‰æ¨¡å‹åœ¨æ³•å¾‹é¢†åŸŸå¹¿æ³›åº”ç”¨ï¼Œè€Œå°‘æ ·æœ¬å­¦ä¹ ã€é›¶æ ·æœ¬å­¦ä¹ å’Œé“¾å¼æ€ç»´æç¤ºç­‰æŠ€æœ¯èƒ½æœ‰æ•ˆæé«˜æ³•å¾‹æ–‡æœ¬çš„è§£é‡Šèƒ½åŠ›ã€‚ç„¶è€Œï¼Œæ¨¡å‹çš„åè§å’Œå¹»è§‰ç­‰æŒ‘æˆ˜ä»é˜»ç¢å…¶å¤§è§„æ¨¡å®æ–½ã€‚å°½ç®¡LLMsåœ¨æ³•å¾‹é¢†åŸŸå…·æœ‰å·¨å¤§æ½œåŠ›ï¼Œä½†ä»éœ€æ”¹è¿›æç¤ºå·¥ç¨‹ç­–ç•¥ä»¥ç¡®ä¿æ›´é«˜çš„å‡†ç¡®æ€§å’Œå¯é æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLMså·²å¹¿æ³›åº”ç”¨äºæ³•å¾‹æ–‡æ¡£å¤„ç†ï¼Œå®ç°äº†è‡ªåŠ¨åŒ–å¤„ç†å¦‚æ‘˜è¦ã€åˆ†ç±»å’Œæ£€ç´¢ç­‰ä»»åŠ¡ã€‚</li>
<li>GPT-4ã€BERTã€Llama 2å’ŒLegal-Pegasusç­‰æ¨¡å‹åœ¨æ³•å¾‹é¢†åŸŸå¹¿æ³›åº”ç”¨ã€‚</li>
<li>å°‘æ ·æœ¬å­¦ä¹ ã€é›¶æ ·æœ¬å­¦ä¹ å’Œé“¾å¼æ€ç»´æç¤ºç­‰æŠ€æœ¯èƒ½æ˜¾è‘—æé«˜æ³•å¾‹æ–‡æœ¬çš„è§£é‡Šèƒ½åŠ›ã€‚</li>
<li>LLMsåœ¨å¤„ç†æ³•å¾‹æ–‡æœ¬æ—¶é¢ä¸´æ¨¡å‹åè§å’Œå¹»è§‰ç­‰æŒ‘æˆ˜ã€‚</li>
<li>éœ€è¦æ”¹è¿›æç¤ºå·¥ç¨‹ç­–ç•¥ä»¥æé«˜LLMsåœ¨å¤„ç†æ³•å¾‹æ–‡æ¡£æ—¶çš„å‡†ç¡®æ€§å’Œå¯é æ€§ã€‚</li>
<li>å½“å‰ç ”ç©¶ä¸»è¦å…³æ³¨LLMsåœ¨æ³•å¾‹é¢†åŸŸçš„ç³»ç»Ÿæ–‡çŒ®ç»¼è¿°å’Œæç¤ºå·¥ç¨‹ç°çŠ¶ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.00725">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-1f52c0797686a952783c3bab17930fc2.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-7139eadfdb094377bafe2b240711c4bc.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e2ee17fbce343939008813fd66be6c4a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5d64a13d31dcea5924063e9e56caf7dc.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-04-03\./crop_Few-Shot/2504.00725v1/page_4_0.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="GLiNER-biomed-A-Suite-of-Efficient-Models-for-Open-Biomedical-Named-Entity-Recognition"><a href="#GLiNER-biomed-A-Suite-of-Efficient-Models-for-Open-Biomedical-Named-Entity-Recognition" class="headerlink" title="GLiNER-biomed: A Suite of Efficient Models for Open Biomedical Named   Entity Recognition"></a>GLiNER-biomed: A Suite of Efficient Models for Open Biomedical Named   Entity Recognition</h2><p><strong>Authors:Anthony Yazdani, Ihor Stepanov, Douglas Teodoro</strong></p>
<p>Biomedical named entity recognition (NER) presents unique challenges due to specialized vocabularies, the sheer volume of entities, and the continuous emergence of novel entities. Traditional NER models, constrained by fixed taxonomies and human annotations, struggle to generalize beyond predefined entity types or efficiently adapt to emerging concepts. To address these issues, we introduce GLiNER-biomed, a domain-adapted suite of Generalist and Lightweight Model for NER (GLiNER) models specifically tailored for biomedical NER. In contrast to conventional approaches, GLiNER uses natural language descriptions to infer arbitrary entity types, enabling zero-shot recognition. Our approach first distills the annotation capabilities of large language models (LLMs) into a smaller, more efficient model, enabling the generation of high-coverage synthetic biomedical NER data. We subsequently train two GLiNER architectures, uni- and bi-encoder, at multiple scales to balance computational efficiency and recognition performance. Evaluations on several biomedical datasets demonstrate that GLiNER-biomed outperforms state-of-the-art GLiNER models in both zero- and few-shot scenarios, achieving 5.96% improvement in F1-score over the strongest baseline. Ablation studies highlight the effectiveness of our synthetic data generation strategy and emphasize the complementary benefits of synthetic biomedical pre-training combined with fine-tuning on high-quality general-domain annotations. All datasets, models, and training pipelines are publicly available at <a target="_blank" rel="noopener" href="https://github.com/ds4dh/GLiNER-biomed">https://github.com/ds4dh/GLiNER-biomed</a>. </p>
<blockquote>
<p>ç”Ÿç‰©åŒ»å­¦å‘½åå®ä½“è¯†åˆ«ï¼ˆNERï¼‰ç”±äºä¸“ä¸šè¯æ±‡ã€å®ä½“æ•°é‡åºå¤§ä»¥åŠæ–°å®ä½“ä¸æ–­æ¶Œç°ï¼Œé¢ä¸´ç€ç‹¬ç‰¹çš„æŒ‘æˆ˜ã€‚ä¼ ç»ŸNERæ¨¡å‹å—é™äºå›ºå®šçš„åˆ†ç±»ä½“ç³»å’Œäººå·¥æ ‡æ³¨ï¼Œéš¾ä»¥æ¨å¹¿åˆ°é¢„å®šä¹‰å®ä½“ç±»å‹ä¹‹å¤–æˆ–æœ‰æ•ˆé€‚åº”æ–°å…´æ¦‚å¿µã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æ¨å‡ºäº†GLiNER-biomedï¼Œè¿™æ˜¯ä¸€ä¸ªä¸“é—¨ä¸ºç”Ÿç‰©åŒ»å­¦NERå®šåˆ¶çš„é¢†åŸŸé€‚åº”æ€§å¼ºçš„GLiNERæ¨¡å‹å¥—ä»¶ã€‚ä¸ä¼ ç»Ÿçš„NERæ–¹æ³•ä¸åŒï¼ŒGLiNERä½¿ç”¨è‡ªç„¶è¯­è¨€æè¿°æ¥æ¨æ–­ä»»æ„å®ä½“ç±»å‹ï¼Œä»è€Œå®ç°é›¶æ ·æœ¬è¯†åˆ«ã€‚æˆ‘ä»¬çš„æ–¹æ³•é¦–å…ˆæç‚¼äº†å¤§è¯­è¨€æ¨¡å‹çš„æ ‡æ³¨èƒ½åŠ›ï¼Œå¹¶å°†å…¶è½¬åŒ–ä¸ºæ›´å°ã€æ›´é«˜æ•ˆçš„æ¨¡å‹ï¼Œä»è€Œç”Ÿæˆé«˜è¦†ç›–ç‡çš„åˆæˆç”Ÿç‰©åŒ»å­¦NERæ•°æ®ã€‚éšåï¼Œæˆ‘ä»¬è®­ç»ƒäº†ä¸¤ä¸ªGLiNERæ¶æ„ï¼Œå•ç¼–ç å™¨å’ŒåŒç¼–ç å™¨ï¼Œä»¥å¹³è¡¡è®¡ç®—æ•ˆç‡å’Œè¯†åˆ«æ€§èƒ½ã€‚åœ¨å¤šä¸ªç”Ÿç‰©åŒ»å­¦æ•°æ®é›†ä¸Šçš„è¯„ä¼°è¡¨æ˜ï¼ŒGLiNER-biomedåœ¨é›¶æ ·æœ¬å’Œå°‘æ ·æœ¬åœºæ™¯ä¸­å‡ä¼˜äºæœ€æ–°çš„GLiNERæ¨¡å‹ï¼Œç›¸è¾ƒäºæœ€å¼ºåŸºçº¿æ¨¡å‹åœ¨F1åˆ†æ•°ä¸Šæé«˜äº†5.96%ã€‚æ¶ˆèç ”ç©¶çªæ˜¾äº†æˆ‘ä»¬åˆæˆæ•°æ®ç”Ÿæˆç­–ç•¥çš„æœ‰æ•ˆæ€§ï¼Œå¹¶å¼ºè°ƒäº†åˆæˆç”Ÿç‰©åŒ»å­¦é¢„è®­ç»ƒä¸é«˜è´¨é‡é€šç”¨åŸŸæ ‡æ³¨å¾®è°ƒä¹‹é—´çš„äº’è¡¥ä¼˜åŠ¿ã€‚æ‰€æœ‰æ•°æ®é›†ã€æ¨¡å‹å’Œè®­ç»ƒç®¡é“å‡å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/ds4dh/GLiNER-biomed">https://github.com/ds4dh/GLiNER-biomed</a>å…¬å¼€è®¿é—®ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.00676v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>GLiNER-biomedæ˜¯ä¸€ä¸ªé’ˆå¯¹ç”Ÿç‰©åŒ»å­¦å‘½åå®ä½“è¯†åˆ«ï¼ˆNERï¼‰ä»»åŠ¡çš„é€šç”¨å’Œè½»é‡åŒ–æ¨¡å‹å¥—ä»¶ã€‚å®ƒé€šè¿‡è‡ªç„¶è¯­è¨€æè¿°æ¥æ¨æ–­ä»»æ„å®ä½“ç±»å‹ï¼Œå®ç°äº†é›¶æ ·æœ¬è¯†åˆ«ã€‚è¯¥ç ”ç©¶é¦–å…ˆåˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ³¨é‡Šèƒ½åŠ›ï¼Œè®­ç»ƒå‡ºæ›´å°ã€æ›´é«˜æ•ˆçš„æ¨¡å‹ï¼Œç”Ÿæˆé«˜è¦†ç›–åº¦çš„åˆæˆç”Ÿç‰©åŒ»å­¦NERæ•°æ®ã€‚éšåï¼Œç ”ç©¶å›¢é˜Ÿè®­ç»ƒäº†ä¸¤ç§GLiNERæ¶æ„ï¼Œå•ç¼–ç å™¨å’ŒåŒç¼–ç å™¨ï¼Œä»¥åœ¨è®¡ç®—æ•ˆç‡å’Œè¯†åˆ«æ€§èƒ½ä¹‹é—´å–å¾—å¹³è¡¡ã€‚åœ¨å¤šä¸ªç”Ÿç‰©åŒ»å­¦æ•°æ®é›†ä¸Šçš„è¯„ä¼°ç»“æœè¡¨æ˜ï¼ŒGLiNER-biomedåœ¨é›¶æ ·æœ¬å’Œå°‘æ ·æœ¬åœºæ™¯ä¸­å‡ä¼˜äºæœ€æ–°çš„GLiNERæ¨¡å‹ï¼ŒF1åˆ†æ•°æé«˜äº†5.96%ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ä¼ ç»Ÿçš„ç”Ÿç‰©åŒ»å­¦å‘½åå®ä½“è¯†åˆ«æ¨¡å‹å—é™äºå›ºå®šçš„åˆ†ç±»å’Œäººå·¥æ ‡æ³¨ï¼Œéš¾ä»¥åº”å¯¹æ–°å®ä½“çš„å¿«é€Ÿæ¶Œç°å’Œå¤§é‡çš„æ•°æ®å®ä½“é—®é¢˜ã€‚</li>
<li>æå‡ºäº†ä¸€ç§æ–°å‹çš„è§£å†³æ–¹æ¡ˆâ€”â€”GLiNER-biomedæ¨¡å‹å¥—ä»¶ï¼Œæ—¨åœ¨é’ˆå¯¹ç”Ÿç‰©åŒ»å­¦å‘½åå®ä½“è¯†åˆ«é—®é¢˜åšå‡ºæœ‰æ•ˆæ”¹è¿›ã€‚å®ƒå¯ä»¥é€šè¿‡è‡ªç„¶è¯­è¨€æè¿°æ¨æ–­ä»»æ„å®ä½“ç±»å‹å®ç°é›¶æ ·æœ¬è¯†åˆ«ã€‚</li>
<li>GLiNER-biomedæ¨¡å‹ä½¿ç”¨å¤§å‹è¯­è¨€æ¨¡å‹çš„æ³¨é‡Šèƒ½åŠ›ç”Ÿæˆåˆæˆæ•°æ®ï¼Œè¯¥æ•°æ®å…·æœ‰é«˜è¦†ç›–åº¦ï¼Œå¹¶æœ‰åŠ©äºæé«˜æ¨¡å‹çš„æ•ˆç‡ä¸æ€§èƒ½ã€‚é‡‡ç”¨äº†å•ç¼–ç å™¨å’ŒåŒç¼–ç å™¨ä¸¤ç§æ¶æ„æ¥å®ç°æ€§èƒ½çš„å¹³è¡¡å’Œä¼˜åŒ–ã€‚æ¨¡å‹æ³¨é‡å®ä½“è¯†åˆ«åŠŸèƒ½ä¸”å…·æœ‰è¾ƒé«˜çš„é€šç”¨æ€§ï¼Œå…·æœ‰å¹¿é˜”çš„å®ç”¨ä»·å€¼å’Œåº”ç”¨å‰æ™¯ã€‚æ¨¡å‹åœ¨ä¸¤ä¸ªå®éªŒè®¾ç½®ä¸­éƒ½è¡¨ç°è‰¯å¥½ã€‚ç›¸å¯¹æœ€ä½³åŸºçº¿è€Œè¨€ï¼Œå®ƒåœ¨F1å¾—åˆ†æ–¹é¢è¡¨ç°å‡ºæ›´é«˜çš„ä¼˜åŠ¿ï¼Œæé«˜äº†5.96%ã€‚åŒæ—¶å…¬å¼€äº†æ‰€æœ‰æ•°æ®é›†ã€æ¨¡å‹å’Œè®­ç»ƒç®¡é“ä¾›å…¬ä¼—ä½¿ç”¨ã€‚è¿™ä¸€åˆ›æ–°æ€§çš„è§£å†³æ–¹æ¡ˆå¯¹ç”Ÿç‰©åŒ»å­¦æ–‡æœ¬æŒ–æ˜å’Œæ–‡æœ¬åˆ†æç­‰é¢†åŸŸå…·æœ‰é‡è¦æ„ä¹‰ã€‚å®ƒä¸ä»…å¯ä»¥æ”¹å–„ç›¸å…³é¢†åŸŸçš„æ•ˆç‡å’Œè´¨é‡ï¼Œä¹Ÿå¯ä»¥æ¨åŠ¨è‡ªç„¶è¯­è¨€å¤„ç†æŠ€æœ¯çš„è¿›ä¸€æ­¥å‘å±•å’Œåº”ç”¨ã€‚è¿™ä¸ºæ„å»ºæ›´å…·æ™ºèƒ½åŒ–ã€é«˜æ•ˆåŒ–çš„ç”Ÿç‰©åŒ»å­¦ä¿¡æ¯æå–ç³»ç»Ÿæä¾›äº†æ–°çš„æ€è·¯å’Œæ–¹å‘ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.00676">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-d07a8c4ab2da0d65b667677340afe3f0.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-313cebb6784e03be4a644250428da11a.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-04-03\./crop_Few-Shot/2504.00676v1/page_5_0.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="FSSUWNet-Mitigating-the-Fragility-of-Pre-trained-Models-with-Feature-Enhancement-for-Few-Shot-Semantic-Segmentation-in-Underwater-Images"><a href="#FSSUWNet-Mitigating-the-Fragility-of-Pre-trained-Models-with-Feature-Enhancement-for-Few-Shot-Semantic-Segmentation-in-Underwater-Images" class="headerlink" title="FSSUWNet: Mitigating the Fragility of Pre-trained Models with Feature   Enhancement for Few-Shot Semantic Segmentation in Underwater Images"></a>FSSUWNet: Mitigating the Fragility of Pre-trained Models with Feature   Enhancement for Few-Shot Semantic Segmentation in Underwater Images</h2><p><strong>Authors:Zhuohao Li, Zhicheng Huang, Wenchao Liu, Zhuxing Zhang, Jianming Miao</strong></p>
<p>Few-Shot Semantic Segmentation (FSS), which focuses on segmenting new classes in images using only a limited number of annotated examples, has recently progressed in data-scarce domains. However, in this work, we show that the existing FSS methods often struggle to generalize to underwater environments. Specifically, the prior features extracted by pre-trained models used as feature extractors are fragile due to the unique challenges of underwater images. To address this, we propose FSSUWNet, a tailored FSS framework for underwater images with feature enhancement. FSSUWNet exploits the integration of complementary features, emphasizing both low-level and high-level image characteristics. In addition to employing a pre-trained model as the primary encoder, we propose an auxiliary encoder called Feature Enhanced Encoder which extracts complementary features to better adapt to underwater scene characteristics. Furthermore, a simple and effective Feature Alignment Module aims to provide global prior knowledge and align low-level features with high-level features in dimensions. Given the scarcity of underwater images, we introduce a cross-validation dataset version based on the Segmentation of Underwater Imagery dataset. Extensive experiments on public underwater segmentation datasets demonstrate that our approach achieves state-of-the-art performance. For example, our method outperforms the previous best method by 2.8% and 2.6% in terms of the mean Intersection over Union metric for 1-shot and 5-shot scenarios in the datasets, respectively. Our implementation is available at <a target="_blank" rel="noopener" href="https://github.com/lizhh268/FSSUWNet">https://github.com/lizhh268/FSSUWNet</a>. </p>
<blockquote>
<p>å°‘æ•°æ ·æœ¬è¯­ä¹‰åˆ†å‰²ï¼ˆFSSï¼‰æ—¨åœ¨åˆ©ç”¨æœ‰é™çš„æ ‡æ³¨æ ·æœ¬å¯¹å›¾åƒä¸­çš„æ–°ç±»åˆ«è¿›è¡Œåˆ†å‰²ï¼Œæœ€è¿‘åœ¨æ•°æ®ç¨€ç¼ºé¢†åŸŸå–å¾—äº†è¿›å±•ã€‚ç„¶è€Œï¼Œåœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬å±•ç¤ºäº†ç°æœ‰çš„FSSæ–¹æ³•åœ¨æ°´ä¸‹ç¯å¢ƒä¸­å¾€å¾€éš¾ä»¥æ¨å¹¿ã€‚å…·ä½“æ¥è¯´ï¼Œç”±äºæ°´ä¸‹å›¾åƒçš„ç‹¬ç‰¹æŒ‘æˆ˜ï¼Œé¢„è®­ç»ƒæ¨¡å‹æå–çš„å…ˆéªŒç‰¹å¾æ˜¯è„†å¼±çš„ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†FSSUWNetï¼Œè¿™æ˜¯ä¸€ä¸ªé’ˆå¯¹æ°´ä¸‹å›¾åƒçš„FSSæ¡†æ¶ï¼Œå…·æœ‰ç‰¹å¾å¢å¼ºåŠŸèƒ½ã€‚FSSUWNetåˆ©ç”¨äº’è¡¥ç‰¹å¾çš„é›†æˆï¼Œå¼ºè°ƒå›¾åƒçš„ä½çº§å’Œé«˜çº§ç‰¹å¾ã€‚é™¤äº†ä½¿ç”¨é¢„è®­ç»ƒæ¨¡å‹ä½œä¸ºä¸»è¦ç¼–ç å™¨å¤–ï¼Œæˆ‘ä»¬è¿˜æå‡ºäº†ä¸€ä¸ªåä¸ºç‰¹å¾å¢å¼ºç¼–ç å™¨çš„è¾…åŠ©ç¼–ç å™¨ï¼Œç”¨äºæå–äº’è¡¥ç‰¹å¾ï¼Œä»¥æ›´å¥½åœ°é€‚åº”æ°´ä¸‹åœºæ™¯ç‰¹å¾ã€‚æ­¤å¤–ï¼Œç®€å•æœ‰æ•ˆçš„ç‰¹å¾å¯¹é½æ¨¡å—æ—¨åœ¨æä¾›å…¨å±€å…ˆéªŒçŸ¥è¯†ï¼Œå¹¶åœ¨ç»´åº¦ä¸Šå¯¹é½ä½çº§ç‰¹å¾å’Œé«˜çº§ç‰¹å¾ã€‚é‰´äºæ°´ä¸‹å›¾åƒçš„ç¨€ç¼ºæ€§ï¼Œæˆ‘ä»¬åŸºäºæ°´ä¸‹å›¾åƒåˆ†å‰²æ•°æ®é›†å¼•å…¥äº†ä¸€ä¸ªäº¤å‰éªŒè¯æ•°æ®é›†ç‰ˆæœ¬ã€‚åœ¨å…¬å…±æ°´ä¸‹åˆ†å‰²æ•°æ®é›†ä¸Šçš„å¤§é‡å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•è¾¾åˆ°äº†æœ€æ–°æ€§èƒ½ã€‚ä¾‹å¦‚ï¼Œåœ¨æˆ‘ä»¬çš„æ–¹æ³•ä¸­ï¼Œä¸ä¹‹å‰æœ€ä½³æ–¹æ³•ç›¸æ¯”ï¼Œåœ¨æ•°æ®é›†çš„ä¸€æ¬¡å°„å‡»å’Œäº”æ¬¡å°„å‡»åœºæ™¯ä¸­ï¼Œå¹³å‡äº¤å¹¶æ¯”æŒ‡æ ‡åˆ†åˆ«æé«˜äº†2.8%å’Œ2.6%ã€‚æˆ‘ä»¬çš„å®ç°å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/lizhh268/FSSUWNet%E4%B8%8A%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/lizhh268/FSSUWNetä¸Šæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.00478v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†é’ˆå¯¹æ°´ä¸‹å›¾åƒåˆ†å‰²çš„æ–°æ–¹æ³•â€”â€”FSSUWNetã€‚é’ˆå¯¹æ°´ä¸‹ç¯å¢ƒç‹¬ç‰¹çš„æŒ‘æˆ˜ï¼Œè¯¥ç½‘ç»œæ¡†æ¶é‡‡ç”¨è¾…åŠ©ç¼–ç å™¨ï¼Œå¼ºåŒ–äº†æ°´ä¸‹åœºæ™¯ç‰¹å¾çš„æå–ï¼Œå¹¶åˆ©ç”¨ç‰¹å¾å¯¹é½æ¨¡å—å°†ä½çº§ä¸é«˜çº§ç‰¹å¾ç›¸ç»“åˆã€‚æ­¤å¤–ï¼Œä¸ºäº†æé«˜æ€§èƒ½ï¼Œå»ºç«‹äº†ä¸€ä¸ªåŸºäºæ°´ä¸‹å½±åƒåˆ†å‰²æ•°æ®é›†ç‰ˆæœ¬è¿›è¡Œäº¤å‰éªŒè¯çš„ç‰ˆæœ¬ã€‚å®éªŒç»“æœåœ¨å…¬å¼€çš„æ°´ä¸‹åˆ†å‰²æ•°æ®é›†ä¸Šæ˜¾ç¤ºå‡ºè¯¥æ–¹æ³•çš„å“è¶Šæ€§èƒ½ï¼Œç›¸æ¯”ä»¥å‰çš„æœ€ä¼˜æ–¹æ³•å¹³å‡æé«˜äº†çº¦ç™¾åˆ†ä¹‹äºŒç‚¹å‡ çš„äº¤å¹¶æ¯”ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.00478">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-04-03\./crop_Few-Shot/2504.00478v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-04-03\./crop_Few-Shot/2504.00478v1/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-04-03\./crop_Few-Shot/2504.00478v1/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-04-03\./crop_Few-Shot/2504.00478v1/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-04-03\./crop_Few-Shot/2504.00478v1/page_5_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-04-03\./crop_Few-Shot/2504.00478v1/page_5_1.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Self-Evolving-Visual-Concept-Library-using-Vision-Language-Critics"><a href="#Self-Evolving-Visual-Concept-Library-using-Vision-Language-Critics" class="headerlink" title="Self-Evolving Visual Concept Library using Vision-Language Critics"></a>Self-Evolving Visual Concept Library using Vision-Language Critics</h2><p><strong>Authors:Atharva Sehgal, Patrick Yuan, Ziniu Hu, Yisong Yue, Jennifer J. Sun, Swarat Chaudhuri</strong></p>
<p>We study the problem of building a visual concept library for visual recognition. Building effective visual concept libraries is challenging, as manual definition is labor-intensive, while relying solely on LLMs for concept generation can result in concepts that lack discriminative power or fail to account for the complex interactions between them. Our approach, ESCHER, takes a library learning perspective to iteratively discover and improve visual concepts. ESCHER uses a vision-language model (VLM) as a critic to iteratively refine the concept library, including accounting for interactions between concepts and how they affect downstream classifiers. By leveraging the in-context learning abilities of LLMs and the history of performance using various concepts, ESCHER dynamically improves its concept generation strategy based on the VLM criticâ€™s feedback. Finally, ESCHER does not require any human annotations, and is thus an automated plug-and-play framework. We empirically demonstrate the ability of ESCHER to learn a concept library for zero-shot, few-shot, and fine-tuning visual classification tasks. This work represents, to our knowledge, the first application of concept library learning to real-world visual tasks. </p>
<blockquote>
<p>æˆ‘ä»¬ç ”ç©¶æ„å»ºç”¨äºè§†è§‰è¯†åˆ«çš„è§†è§‰æ¦‚å¿µåº“çš„é—®é¢˜ã€‚æ„å»ºæœ‰æ•ˆçš„è§†è§‰æ¦‚å¿µåº“å…·æœ‰æŒ‘æˆ˜æ€§ï¼Œå› ä¸ºæ‰‹åŠ¨å®šä¹‰éœ€è¦å¤§é‡åŠ³åŠ¨åŠ›ï¼Œè€Œä»…ä¾èµ–å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰è¿›è¡Œæ¦‚å¿µç”Ÿæˆå¯èƒ½ä¼šå¯¼è‡´ç¼ºä¹è¾¨åˆ«åŠ›çš„æ¦‚å¿µï¼Œæˆ–è€…æ— æ³•è§£é‡Šæ¦‚å¿µä¹‹é—´çš„å¤æ‚äº¤äº’ã€‚æˆ‘ä»¬çš„æ–¹æ³•ESCHERé‡‡ç”¨å›¾ä¹¦é¦†å­¦ä¹ è§†è§’æ¥è¿­ä»£åœ°å‘ç°å’Œæ”¹è¿›è§†è§‰æ¦‚å¿µã€‚ESCHERä½¿ç”¨è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰ä½œä¸ºæ‰¹åˆ¤è€…ï¼Œè¿­ä»£åœ°å®Œå–„æ¦‚å¿µåº“ï¼ŒåŒ…æ‹¬è§£é‡Šæ¦‚å¿µä¹‹é—´çš„äº¤äº’ä»¥åŠå®ƒä»¬å¦‚ä½•å½±å“ä¸‹æ¸¸åˆ†ç±»å™¨ã€‚é€šè¿‡åˆ©ç”¨LLMsçš„ä¸Šä¸‹æ–‡å­¦ä¹ èƒ½åŠ›ä»¥åŠå„ç§æ¦‚å¿µçš„æ€§èƒ½å†å²è®°å½•ï¼ŒESCHERæ ¹æ®VLMæ‰¹è¯„è€…çš„åé¦ˆåŠ¨æ€æ”¹è¿›å…¶æ¦‚å¿µç”Ÿæˆç­–ç•¥ã€‚æœ€åï¼ŒESCHERä¸éœ€è¦ä»»ä½•äººå·¥æ³¨é‡Šï¼Œå› æ­¤æ˜¯ä¸€ä¸ªè‡ªåŠ¨åŒ–çš„å³æ’å³ç”¨æ¡†æ¶ã€‚æˆ‘ä»¬é€šè¿‡å®è¯è¯æ˜äº†ESCHERåœ¨é›¶æ ·æœ¬ã€å°‘æ ·æœ¬å’Œå¾®è°ƒè§†è§‰åˆ†ç±»ä»»åŠ¡ä¸­å­¦ä¹ æ¦‚å¿µåº“çš„èƒ½åŠ›ã€‚æ®æˆ‘ä»¬æ‰€çŸ¥ï¼Œè¿™é¡¹å·¥ä½œä»£è¡¨äº†æ¦‚å¿µåº“å­¦ä¹ åœ¨ç°å®ä¸–ç•Œè§†è§‰ä»»åŠ¡ä¸­çš„é¦–æ¬¡åº”ç”¨ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.00185v1">PDF</a> CVPR camera ready</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ç ”ç©¶äº†å»ºç«‹ç”¨äºè§†è§‰è¯†åˆ«çš„è§†è§‰æ¦‚å¿µåº“çš„æŒ‘æˆ˜æ€§é—®é¢˜ã€‚é’ˆå¯¹æ‰‹åŠ¨å®šä¹‰å·¥ä½œé‡å¤§ã€å•çº¯ä¾èµ–LLMsè¿›è¡Œæ¦‚å¿µç”Ÿæˆå¯èƒ½å¯¼è‡´æ¦‚å¿µé‰´åˆ«åŠ›ä¸è¶³æˆ–å¿½è§†æ¦‚å¿µé—´å¤æ‚äº¤äº’çš„é—®é¢˜ï¼Œæå‡ºäº†ESCHERæ–¹æ³•ã€‚è¯¥æ–¹æ³•é‡‡ç”¨å›¾ä¹¦é¦†å­¦ä¹ è§†è§’ï¼Œè¿­ä»£å‘ç°å’Œæ”¹è¿›è§†è§‰æ¦‚å¿µã€‚ESCHERä½¿ç”¨è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰ä½œä¸ºæ‰¹è¯„è€…ï¼Œè¿­ä»£ä¼˜åŒ–æ¦‚å¿µåº“ï¼ŒåŒ…æ‹¬è€ƒè™‘æ¦‚å¿µé—´çš„äº¤äº’ä»¥åŠå®ƒä»¬å¯¹ä¸‹æ¸¸åˆ†ç±»å™¨çš„å½±å“ã€‚åˆ©ç”¨LLMsçš„ä¸Šä¸‹æ–‡å­¦ä¹ èƒ½åŠ›åŠä¸åŒæ¦‚å¿µçš„æ€§èƒ½å†å²è®°å½•ï¼ŒESCHERæ ¹æ®VLMæ‰¹è¯„è€…çš„åé¦ˆåŠ¨æ€æ”¹è¿›å…¶æ¦‚å¿µç”Ÿæˆç­–ç•¥ã€‚æœ€ç»ˆï¼ŒESCHERæ— éœ€äººå·¥æ³¨é‡Šï¼Œæ˜¯ä¸€ä¸ªè‡ªåŠ¨åŒ–çš„å³æ’å³ç”¨æ¡†æ¶ã€‚å®è¯è¡¨æ˜ï¼ŒESCHERåœ¨é›¶æ ·æœ¬ã€å°‘æ ·æœ¬å’Œå¾®è°ƒè§†è§‰åˆ†ç±»ä»»åŠ¡ä¸­å­¦ä¹ æ¦‚å¿µåº“çš„èƒ½åŠ›ã€‚æœ¬æ–‡æ®æˆ‘ä»¬æ‰€çŸ¥ï¼Œæ˜¯æ¦‚å¿µåº“å­¦ä¹ åœ¨çœŸå®è§†è§‰ä»»åŠ¡ä¸­çš„é¦–æ¬¡åº”ç”¨ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ESCHERæ–¹æ³•æ—¨åœ¨è§£å†³å»ºç«‹è§†è§‰æ¦‚å¿µåº“çš„æŒ‘æˆ˜ï¼ŒåŒ…æ‹¬æ‰‹åŠ¨å®šä¹‰çš„åŠ³åŠ¨å¯†é›†åº¦å’Œå•çº¯ä¾èµ–LLMså¯¼è‡´çš„é—®é¢˜ã€‚</li>
<li>ESCHERé‡‡ç”¨å›¾ä¹¦é¦†å­¦ä¹ è§†è§’ï¼Œé€šè¿‡è¿­ä»£å‘ç°å’Œæ”¹è¿›è§†è§‰æ¦‚å¿µã€‚</li>
<li>ä½¿ç”¨è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰ä½œä¸ºæ‰¹è¯„è€…ï¼ŒESCHERèƒ½å¤Ÿè¿­ä»£ä¼˜åŒ–æ¦‚å¿µåº“ï¼ŒåŒ…æ‹¬è€ƒè™‘æ¦‚å¿µé—´çš„äº¤äº’åŠå…¶å¯¹ä¸‹æ¸¸åˆ†ç±»å™¨çš„å½±å“ã€‚</li>
<li>ESCHERåˆ©ç”¨LLMsçš„ä¸Šä¸‹æ–‡å­¦ä¹ èƒ½åŠ›åŠä¸åŒæ¦‚å¿µçš„æ€§èƒ½å†å²åé¦ˆæ¥åŠ¨æ€æ”¹è¿›å…¶æ¦‚å¿µç”Ÿæˆç­–ç•¥ã€‚</li>
<li>ESCHERæ¡†æ¶æ— éœ€äººå·¥æ³¨é‡Šï¼Œå…·å¤‡è‡ªåŠ¨åŒ–ç‰¹æ€§ã€‚</li>
<li>å®è¯è¡¨æ˜ï¼ŒESCHERåœ¨å¤šç§è§†è§‰ä»»åŠ¡ä¸­å­¦ä¹ æ¦‚å¿µåº“çš„èƒ½åŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.00185">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-04-03\./crop_Few-Shot/2504.00185v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-04-03\./crop_Few-Shot/2504.00185v1/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-04-03\./crop_Few-Shot/2504.00185v1/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-04-03\./crop_Few-Shot/2504.00185v1/page_5_0.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="AI-Assisted-Colonoscopy-Polyp-Detection-and-Segmentation-using-Foundation-Models"><a href="#AI-Assisted-Colonoscopy-Polyp-Detection-and-Segmentation-using-Foundation-Models" class="headerlink" title="AI-Assisted Colonoscopy: Polyp Detection and Segmentation using   Foundation Models"></a>AI-Assisted Colonoscopy: Polyp Detection and Segmentation using   Foundation Models</h2><p><strong>Authors:Uxue Delaquintana-Aramendi, Leire Benito-del-Valle, Aitor Alvarez-Gila, Javier Pascau, Luisa F SÃ¡nchez-Peralta, Artzai PicÃ³n, J Blas Pagador, Cristina L Saratxaga</strong></p>
<p>In colonoscopy, 80% of the missed polyps could be detected with the help of Deep Learning models. In the search for algorithms capable of addressing this challenge, foundation models emerge as promising candidates. Their zero-shot or few-shot learning capabilities, facilitate generalization to new data or tasks without extensive fine-tuning. A concept that is particularly advantageous in the medical imaging domain, where large annotated datasets for traditional training are scarce. In this context, a comprehensive evaluation of foundation models for polyp segmentation was conducted, assessing both detection and delimitation. For the study, three different colonoscopy datasets have been employed to compare the performance of five different foundation models, DINOv2, YOLO-World, GroundingDINO, SAM and MedSAM, against two benchmark networks, YOLOv8 and Mask R-CNN. Results show that the success of foundation models in polyp characterization is highly dependent on domain specialization. For optimal performance in medical applications, domain-specific models are essential, and generic models require fine-tuning to achieve effective results. Through this specialization, foundation models demonstrated superior performance compared to state-of-the-art detection and segmentation models, with some models even excelling in zero-shot evaluation; outperforming fine-tuned models on unseen data. </p>
<blockquote>
<p>åœ¨ç»“è‚ é•œæ£€æŸ¥ä¸­ï¼Œå€ŸåŠ©æ·±åº¦å­¦ä¹ æ¨¡å‹å¯ä»¥æ£€æµ‹åˆ°80%é—æ¼çš„æ¯è‚‰ã€‚åœ¨å¯»æ‰¾èƒ½å¤Ÿåº”å¯¹è¿™ä¸€æŒ‘æˆ˜çš„ç®—æ³•æ—¶ï¼ŒåŸºç¡€æ¨¡å‹è¡¨ç°å‡ºå·¨å¤§çš„æ½œåŠ›ã€‚å®ƒä»¬å…·å¤‡é›¶æ ·æœ¬æˆ–å°‘æ ·æœ¬å­¦ä¹ èƒ½åŠ›ï¼Œèƒ½å¤Ÿè½»æ¾æ¨å¹¿åˆ°æ–°çš„æ•°æ®æˆ–ä»»åŠ¡ï¼Œè€Œæ— éœ€è¿›è¡Œå¤§é‡çš„å¾®è°ƒã€‚è¿™ä¸€æ¦‚å¿µåœ¨åŒ»å­¦æˆåƒé¢†åŸŸç‰¹åˆ«æœ‰åˆ©ï¼Œå› ä¸ºè¯¥é¢†åŸŸç¼ºä¹ç”¨äºä¼ ç»Ÿè®­ç»ƒçš„å¤§å‹æ³¨é‡Šæ•°æ®é›†ã€‚åœ¨æ­¤èƒŒæ™¯ä¸‹ï¼Œå¯¹åŸºç¡€æ¨¡å‹è¿›è¡Œäº†å…¨é¢çš„æ¯è‚‰åˆ†å‰²è¯„ä¼°ï¼ŒåŒ…æ‹¬æ£€æµ‹å’Œç•Œå®šä¸¤ä¸ªæ–¹é¢ã€‚ä¸ºäº†è¿›è¡Œç ”ç©¶ï¼Œé‡‡ç”¨äº†ä¸‰ä¸ªä¸åŒçš„ç»“è‚ é•œæ£€æŸ¥æ•°æ®é›†ï¼Œæ¯”è¾ƒäº†äº”ç§ä¸åŒåŸºç¡€æ¨¡å‹ï¼ˆDINOv2ã€YOLO-Worldã€GroundingDINOã€SAMå’ŒMedSAMï¼‰ä¸ä¸¤ä¸ªåŸºå‡†ç½‘ç»œï¼ˆYOLOv8å’ŒMask R-CNNï¼‰çš„æ€§èƒ½ã€‚ç»“æœè¡¨æ˜ï¼ŒåŸºç¡€æ¨¡å‹åœ¨æ¯è‚‰è¡¨å¾æ–¹é¢çš„æˆåŠŸé«˜åº¦ä¾èµ–äºé¢†åŸŸä¸“ä¸šåŒ–ã€‚ä¸ºäº†åœ¨åŒ»ç–—åº”ç”¨ä¸­è·å¾—æœ€ä½³æ€§èƒ½ï¼Œé¢†åŸŸç‰¹å®šæ¨¡å‹æ˜¯ä¸å¯æˆ–ç¼ºçš„ï¼Œè€Œé€šç”¨æ¨¡å‹éœ€è¦å¾®è°ƒæ‰èƒ½å®ç°æœ‰æ•ˆç»“æœã€‚é€šè¿‡ä¸“ä¸šåŒ–ï¼ŒåŸºç¡€æ¨¡å‹è¡¨ç°å‡ºä¼˜äºæœ€æ–°æ£€æµ‹å’Œåˆ†å‰²æ¨¡å‹çš„æ€§èƒ½ï¼Œéƒ¨åˆ†æ¨¡å‹ç”šè‡³åœ¨é›¶æ ·æœ¬è¯„ä¼°ä¸­è¡¨ç°å‡ºè‰²ï¼Œåœ¨æœªè§è¿‡çš„æ•°æ®ä¸Šè¶…è¶Šäº†å¾®è°ƒæ¨¡å‹ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.24138v1">PDF</a> This work has been submitted to the IEEE TMI for possible publication</p>
<p><strong>Summary</strong></p>
<p>æ·±åº¦å­¦ä¹ æ¨¡å‹å¯æ£€æµ‹å‡ºç»“è‚ é•œæ£€æŸ¥ä¸­æœªå‘ç°çš„çº¦å…«æˆçš„æ¯è‚‰ã€‚åŸºç¡€æ¨¡å‹ä»¥å…¶é›¶æ ·æœ¬æˆ–å°‘æ ·æœ¬å­¦ä¹ èƒ½åŠ›ï¼Œå±•ç°å‡ºå¯¹æ–°æ•°æ®æˆ–ä»»åŠ¡çš„å¿«é€Ÿé€‚åº”æ½œåŠ›ï¼Œå°¤å…¶åœ¨ç¼ºå°‘å¤§é‡æ ‡æ³¨æ•°æ®çš„åŒ»å­¦å½±åƒé¢†åŸŸå…·æœ‰æ˜¾è‘—ä¼˜åŠ¿ã€‚é’ˆå¯¹åŸºç¡€æ¨¡å‹åœ¨æ¯è‚‰åˆ†å‰²ä¸Šçš„åº”ç”¨è¿›è¡Œäº†å…¨é¢è¯„ä¼°ï¼Œæ¯”è¾ƒäº†äº”ç§åŸºç¡€æ¨¡å‹ï¼ˆDINOv2ã€YOLO-Worldã€GroundingDINOã€SAMå’ŒMedSAMï¼‰ä¸ä¸¤ä¸ªåŸºå‡†ç½‘ç»œYOLOv8å’ŒMask R-CNNåœ¨ä¸‰ä¸ªç»“è‚ é•œæ£€æŸ¥æ•°æ®é›†ä¸Šçš„è¡¨ç°ã€‚ç»“æœå¼ºè°ƒäº†åœ¨åŒ»ç–—åº”ç”¨ä¸­ï¼ŒåŸºç¡€æ¨¡å‹çš„é¢†åŸŸä¸“ä¸šåŒ–å¯¹æ€§èƒ½è‡³å…³é‡è¦ã€‚å¯¹äºæœ€ä½³è¡¨ç°ï¼Œéœ€è¦é¢†åŸŸç‰¹å®šæ¨¡å‹ï¼Œé€šç”¨æ¨¡å‹éœ€è¦å¾®è°ƒä»¥å®ç°æœ‰æ•ˆç»“æœã€‚åŸºç¡€æ¨¡å‹æ˜¾ç¤ºå‡ºä¼˜äºæœ€æ–°æ£€æµ‹å’Œåˆ†å‰²æ¨¡å‹çš„è¡¨ç°ï¼Œéƒ¨åˆ†æ¨¡å‹åœ¨é›¶æ ·æœ¬è¯„ä¼°ä¸­è¡¨ç°å°¤ä¸ºå‡ºè‰²ï¼Œåœ¨æœªæ ‡æ³¨æ•°æ®ä¸Šè¶…è¶Šäº†å¾®è°ƒæ¨¡å‹ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ·±åº¦å­¦ä¹ æ¨¡å‹èƒ½æœ‰æ•ˆæ£€æµ‹ç»“è‚ é•œæ£€æŸ¥ä¸­æœªå‘ç°çš„å¤šæ•°æ¯è‚‰ã€‚</li>
<li>åŸºç¡€æ¨¡å‹å…·å¤‡é›¶æ ·æœ¬å’Œå°‘æ ·æœ¬å­¦ä¹ èƒ½åŠ›ï¼Œèƒ½å¿«é€Ÿé€‚åº”æ–°æ•°æ®æˆ–ä»»åŠ¡ã€‚</li>
<li>åœ¨åŒ»å­¦å½±åƒé¢†åŸŸï¼ŒåŸºç¡€æ¨¡å‹å› ç¼ºå°‘å¤§é‡æ ‡æ³¨æ•°æ®è€Œæ˜¾å¾—æ ¼å¤–é‡è¦ã€‚</li>
<li>åŸºç¡€æ¨¡å‹åœ¨æ¯è‚‰åˆ†å‰²æ–¹é¢çš„å…¨é¢è¯„ä¼°æ˜¾ç¤ºï¼Œé¢†åŸŸä¸“ä¸šåŒ–å¯¹åŒ»ç–—åº”ç”¨æ€§èƒ½è‡³å…³é‡è¦ã€‚</li>
<li>é¢†åŸŸç‰¹å®šæ¨¡å‹å¯¹äºè·å¾—æœ€ä½³è¡¨ç°æ˜¯å¿…è¦çš„ï¼Œè€Œé€šç”¨æ¨¡å‹éœ€è¦å¾®è°ƒã€‚</li>
<li>åŸºç¡€æ¨¡å‹åœ¨åŒ»ç–—å›¾åƒåˆ†ææ–¹é¢æ˜¾ç¤ºå‡ºæ˜¾è‘—ä¼˜åŠ¿ï¼Œä¼˜äºè®¸å¤šæœ€æ–°çš„æ£€æµ‹å’Œåˆ†å‰²æ¨¡å‹ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.24138">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-04-03\./crop_Few-Shot/2503.24138v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-04-03\./crop_Few-Shot/2503.24138v1/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-04-03\./crop_Few-Shot/2503.24138v1/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-04-03\./crop_Few-Shot/2503.24138v1/page_5_0.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="Texture-or-Semantics-Vision-Language-Models-Get-Lost-in-Font-Recognition"><a href="#Texture-or-Semantics-Vision-Language-Models-Get-Lost-in-Font-Recognition" class="headerlink" title="Texture or Semantics? Vision-Language Models Get Lost in Font   Recognition"></a>Texture or Semantics? Vision-Language Models Get Lost in Font   Recognition</h2><p><strong>Authors:Zhecheng Li, Guoxian Song, Yujun Cai, Zhen Xiong, Junsong Yuan, Yiwei Wang</strong></p>
<p>Modern Vision-Language Models (VLMs) exhibit remarkable visual and linguistic capabilities, achieving impressive performance in various tasks such as image recognition and object localization. However, their effectiveness in fine-grained tasks remains an open question. In everyday scenarios, individuals encountering design materials, such as magazines, typography tutorials, research papers, or branding content, may wish to identify aesthetically pleasing fonts used in the text. Given their multimodal capabilities and free accessibility, many VLMs are often considered potential tools for font recognition. This raises a fundamental question: Do VLMs truly possess the capability to recognize fonts? To investigate this, we introduce the Font Recognition Benchmark (FRB), a compact and well-structured dataset comprising 15 commonly used fonts. FRB includes two versions: (i) an easy version, where 10 sentences are rendered in different fonts, and (ii) a hard version, where each text sample consists of the names of the 15 fonts themselves, introducing a stroop effect that challenges model perception. Through extensive evaluation of various VLMs on font recognition tasks, we arrive at the following key findings: (i) Current VLMs exhibit limited font recognition capabilities, with many state-of-the-art models failing to achieve satisfactory performance. (ii) Few-shot learning and Chain-of-Thought (CoT) prompting provide minimal benefits in improving font recognition accuracy across different VLMs. (iii) Attention analysis sheds light on the inherent limitations of VLMs in capturing semantic features. </p>
<blockquote>
<p>ç°ä»£è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰å±•ç°å‡ºä»¤äººç©ç›®çš„è§†è§‰å’Œè¯­è¨€èƒ½åŠ›ï¼Œåœ¨å„ç§ä»»åŠ¡ä¸­å–å¾—äº†ä»¤äººå°è±¡æ·±åˆ»çš„è¡¨ç°ï¼Œå¦‚å›¾åƒè¯†åˆ«å’Œç‰©ä½“å®šä½ã€‚ç„¶è€Œï¼Œå®ƒä»¬åœ¨ç²¾ç»†ä»»åŠ¡ä¸­çš„æœ‰æ•ˆæ€§ä»ç„¶æ˜¯ä¸€ä¸ªæ‚¬è€Œæœªå†³çš„é—®é¢˜ã€‚åœ¨æ—¥å¸¸åœºæ™¯ä¸­ï¼Œä¸ªäººé‡åˆ°è®¾è®¡ææ–™ï¼Œå¦‚æ‚å¿—ã€æ’ç‰ˆæ•™ç¨‹ã€ç ”ç©¶è®ºæ–‡æˆ–å“ç‰Œå†…å®¹ç­‰ï¼Œå¯èƒ½ä¼šæƒ³è¦è¯†åˆ«æ–‡æœ¬ä¸­ä½¿ç”¨çš„å¸å¼•äººçš„å­—ä½“ã€‚è€ƒè™‘åˆ°å®ƒä»¬çš„å¤šæ¨¡å¼èƒ½åŠ›å’Œè‡ªç”±å¯è®¿é—®æ€§ï¼Œè®¸å¤šVLMsé€šå¸¸è¢«è®¤ä¸ºæ˜¯å­—ä½“è¯†åˆ«çš„æ½œåœ¨å·¥å…·ã€‚è¿™å°±å¼•å‘äº†ä¸€ä¸ªæ ¹æœ¬æ€§çš„é—®é¢˜ï¼šVLMsæ˜¯å¦çœŸçš„å…·å¤‡è¯†åˆ«å­—ä½“çš„èƒ½åŠ›ï¼Ÿä¸ºäº†è°ƒæŸ¥è¿™ä¸€ç‚¹ï¼Œæˆ‘ä»¬å¼•å…¥äº†å­—ä½“è¯†åˆ«åŸºå‡†æµ‹è¯•ï¼ˆFRBï¼‰ï¼Œè¿™æ˜¯ä¸€ä¸ªç”±15ç§å¸¸ç”¨å­—ä½“ç»„æˆçš„ç´§å‡‘ä¸”ç»“æ„è‰¯å¥½çš„æ•°æ®é›†ã€‚FRBåŒ…æ‹¬ä¸¤ä¸ªç‰ˆæœ¬ï¼šï¼ˆiï¼‰ç®€å•ç‰ˆæœ¬ï¼Œå…¶ä¸­10ä¸ªå¥å­ä»¥ä¸åŒçš„å­—ä½“å‘ˆç°ï¼›ï¼ˆiiï¼‰å›°éš¾ç‰ˆæœ¬ï¼Œå…¶ä¸­æ¯ä¸ªæ–‡æœ¬æ ·æœ¬ç”±è¿™15ç§å­—ä½“çš„åç§°ç»„æˆï¼Œå¼•å…¥äº†ä¸€ç§æ–¯ç‰¹é²æ™®æ•ˆåº”ï¼ŒæŒ‘æˆ˜äº†æ¨¡å‹çš„æ„ŸçŸ¥èƒ½åŠ›ã€‚é€šè¿‡å¯¹å„ç§VLMåœ¨å­—ä½“è¯†åˆ«ä»»åŠ¡ä¸Šçš„å¹¿æ³›è¯„ä¼°ï¼Œæˆ‘ä»¬å¾—å‡ºä»¥ä¸‹å…³é”®å‘ç°ï¼šï¼ˆiï¼‰å½“å‰VLMçš„å­—ä½“è¯†åˆ«èƒ½åŠ›æœ‰é™ï¼Œè®¸å¤šæœ€å…ˆè¿›çš„æ¨¡å‹æ— æ³•å–å¾—ä»¤äººæ»¡æ„çš„æ€§èƒ½ã€‚ï¼ˆiiï¼‰å°æ ·æœ¬å­¦ä¹ å’Œæ€ç»´é“¾æç¤ºå¯¹æ”¹å–„ä¸åŒVLMçš„å­—ä½“è¯†åˆ«å‡†ç¡®æ€§æä¾›æœ‰é™çš„å¥½å¤„ã€‚ï¼ˆiiiï¼‰æ³¨æ„åŠ›åˆ†ææ­ç¤ºäº†VLMåœ¨æ•è·è¯­ä¹‰ç‰¹å¾æ–¹é¢çš„å†…åœ¨å±€é™æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.23768v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ç°ä»£è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰åœ¨å­—ä½“è¯†åˆ«æ–¹é¢çš„è¡¨ç°ã€‚è™½ç„¶è¿™äº›æ¨¡å‹åœ¨å›¾åƒè¯†åˆ«å’Œç‰©ä½“å®šä½ç­‰ä»»åŠ¡ä¸Šè¡¨ç°å‡ºè‰²ï¼Œä½†åœ¨ç²¾ç»†ä»»åŠ¡å¦‚å­—ä½“è¯†åˆ«ä¸Šä»å­˜åœ¨å±€é™æ€§ã€‚ä¸ºæ¢ç©¶VLMsåœ¨å­—ä½“è¯†åˆ«æ–¹é¢çš„çœŸæ­£èƒ½åŠ›ï¼Œå¼•å…¥äº†å­—ä½“è¯†åˆ«åŸºå‡†æµ‹è¯•ï¼ˆFRBï¼‰ï¼ŒåŒ…æ‹¬ä¸¤ä¸ªç‰ˆæœ¬ï¼šç®€å•ç‰ˆä¸å›°éš¾ç‰ˆã€‚è¯„ä¼°å‘ç°ï¼Œå½“å‰VLMsçš„å­—ä½“è¯†åˆ«èƒ½åŠ›æœ‰é™ï¼Œå°‘æ ·æœ¬å­¦ä¹ ä¸Chain-of-Thoughtï¼ˆCoTï¼‰æç¤ºå¯¹æå‡æ¨¡å‹æ€§èƒ½æ•ˆæœç”šå¾®ï¼Œæ³¨æ„åŠ›åˆ†ææ­ç¤ºäº†VLMsæ•æ‰è¯­ä¹‰ç‰¹å¾çš„å†…åœ¨å±€é™ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç°ä»£è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰åœ¨å­—ä½“è¯†åˆ«ä»»åŠ¡ä¸Šçš„è¡¨ç°å¼•äººå…³æ³¨ã€‚</li>
<li>å¼•å…¥çš„å­—ä½“è¯†åˆ«åŸºå‡†æµ‹è¯•ï¼ˆFRBï¼‰åŒ…å«ç®€å•ç‰ˆå’Œå›°éš¾ç‰ˆï¼Œç”¨äºè¯„ä¼°VLMsçš„å­—ä½“è¯†åˆ«èƒ½åŠ›ã€‚</li>
<li>å½“å‰VLMsåœ¨å­—ä½“è¯†åˆ«æ–¹é¢å­˜åœ¨å±€é™æ€§ï¼Œè¡¨ç°å¹¶ä¸ä»¤äººæ»¡æ„ã€‚</li>
<li>å°‘æ ·æœ¬å­¦ä¹ ä¸Chain-of-Thoughtï¼ˆCoTï¼‰æç¤ºå¯¹æå‡VLMsåœ¨å­—ä½“è¯†åˆ«ä»»åŠ¡ä¸Šçš„æ€§èƒ½æ•ˆæœç”šå¾®ã€‚</li>
<li>æ³¨æ„åŠ›åˆ†æè¡¨æ˜ï¼ŒVLMsåœ¨æ•æ‰è¯­ä¹‰ç‰¹å¾æ–¹é¢å­˜åœ¨å†…åœ¨å±€é™ã€‚</li>
<li>FRBä¸ºç ”ç©¶å’Œæ”¹è¿›VLMsåœ¨å­—ä½“è¯†åˆ«ä»»åŠ¡ä¸Šçš„æ€§èƒ½æä¾›äº†æœ‰ä»·å€¼çš„åŸºå‡†ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.23768">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-04-03\./crop_Few-Shot/2503.23768v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-04-03\./crop_Few-Shot/2503.23768v1/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-04-03\./crop_Few-Shot/2503.23768v1/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-04-03\./crop_Few-Shot/2503.23768v1/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-04-03\./crop_Few-Shot/2503.23768v1/page_4_1.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-04-03\./crop_Few-Shot/2503.23768v1/page_5_0.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="Diffusion-Meets-Few-shot-Class-Incremental-Learning"><a href="#Diffusion-Meets-Few-shot-Class-Incremental-Learning" class="headerlink" title="Diffusion Meets Few-shot Class Incremental Learning"></a>Diffusion Meets Few-shot Class Incremental Learning</h2><p><strong>Authors:Junsu Kim, Yunhoe Ku, Dongyoon Han, Seungryul Baek</strong></p>
<p>Few-shot class-incremental learning (FSCIL) is challenging due to extremely limited training data; while aiming to reduce catastrophic forgetting and learn new information. We propose Diffusion-FSCIL, a novel approach that employs a text-to-image diffusion model as a frozen backbone. Our conjecture is that FSCIL can be tackled using a large generative modelâ€™s capabilities benefiting from 1) generation ability via large-scale pre-training; 2) multi-scale representation; 3) representational flexibility through the text encoder. To maximize the representation capability, we propose to extract multiple complementary diffusion features to play roles as latent replay with slight support from feature distillation for preventing generative biases. Our framework realizes efficiency through 1) using a frozen backbone; 2) minimal trainable components; 3) batch processing of multiple feature extractions. Extensive experiments on CUB-200, miniImageNet, and CIFAR-100 show that Diffusion-FSCIL surpasses state-of-the-art methods, preserving performance on previously learned classes and adapting effectively to new ones. </p>
<blockquote>
<p>å°‘æ•°ç±»å¢é‡å­¦ä¹ ï¼ˆFSCILï¼‰å› ä¸ºè®­ç»ƒæ•°æ®çš„æåº¦æœ‰é™è€Œå…·æœ‰æŒ‘æˆ˜æ€§ï¼ŒåŒæ—¶è¿˜éœ€è¦å‡å°‘ç¾éš¾æ€§é—å¿˜å¹¶å­¦ä¹ æ–°ä¿¡æ¯ã€‚æˆ‘ä»¬æå‡ºäº†Diffusion-FSCILï¼Œè¿™æ˜¯ä¸€ç§é‡‡ç”¨æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹ä½œä¸ºå†»ç»“ä¸»å¹²çš„æ–°å‹æ–¹æ³•ã€‚æˆ‘ä»¬çš„å‡è®¾æ˜¯ï¼Œåˆ©ç”¨å¤§å‹ç”Ÿæˆæ¨¡å‹çš„èƒ½åŠ›å¯ä»¥è§£å†³FSCILé—®é¢˜ï¼Œå—ç›Šäº1ï¼‰é€šè¿‡å¤§è§„æ¨¡é¢„è®­ç»ƒäº§ç”Ÿçš„ç”Ÿæˆèƒ½åŠ›ï¼›2ï¼‰å¤šå°ºåº¦è¡¨ç¤ºï¼›3ï¼‰é€šè¿‡æ–‡æœ¬ç¼–ç å™¨å®ç°çš„è¡¨ç¤ºçµæ´»æ€§ã€‚ä¸ºäº†æœ€å¤§åŒ–è¡¨ç¤ºèƒ½åŠ›ï¼Œæˆ‘ä»¬æå‡ºæå–å¤šä¸ªäº’è¡¥çš„æ‰©æ•£ç‰¹å¾ï¼Œä½œä¸ºæ½œåœ¨å›æ”¾çš„è§’è‰²ï¼Œç•¥å¾®æ”¯æŒç‰¹å¾è’¸é¦ï¼Œä»¥é˜²æ­¢ç”Ÿæˆåè§ã€‚æˆ‘ä»¬çš„æ¡†æ¶é€šè¿‡ä»¥ä¸‹æ–¹å¼å®ç°æ•ˆç‡ï¼š1ï¼‰ä½¿ç”¨å†»ç»“çš„ä¸»å¹²ï¼›2ï¼‰æœ€å°‘çš„å¯è®­ç»ƒç»„ä»¶ï¼›3ï¼‰å¤šä¸ªç‰¹å¾æå–çš„æ‰¹å¤„ç†ã€‚åœ¨CUB-200ã€miniImageNetå’ŒCIFAR-100ä¸Šçš„å¤§é‡å®éªŒè¡¨æ˜ï¼ŒDiffusion-FSCILè¶…è¶Šäº†æœ€å…ˆè¿›çš„æ–¹æ³•ï¼Œæ—¢ä¿ç•™äº†ä»¥å‰å­¦ä¹ çš„ç±»çš„æ€§èƒ½ï¼Œåˆèƒ½æœ‰æ•ˆåœ°é€‚åº”æ–°ç±»ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.23402v1">PDF</a> pre-print</p>
<p><strong>Summary</strong></p>
<p>åŸºäºæœ‰é™è®­ç»ƒæ•°æ®çš„æŒ‘æˆ˜ï¼Œæå‡ºä¸€ç§æ–°é¢–çš„è§£å†³å°‘é‡æ ·æœ¬ç±»å¢é‡å­¦ä¹ ï¼ˆFSCILï¼‰çš„æ–¹æ³•â€”â€”Diffusion-FSCILã€‚è¯¥æ–¹æ³•é‡‡ç”¨æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹ä½œä¸ºå†»ç»“ä¸»å¹²ï¼Œå€ŸåŠ©å¤§å‹ç”Ÿæˆæ¨¡å‹çš„å¤šç§èƒ½åŠ›æ¥è§£å†³FSCILé—®é¢˜ã€‚é€šè¿‡æå–å¤šç§äº’è¡¥æ‰©æ•£ç‰¹å¾æ¥å®ç°é«˜æ•ˆè¡¨ç¤ºï¼Œå¹¶åˆ©ç”¨è½»å¾®çš„ç‰¹å¾è’¸é¦æ”¯æŒæ¥é˜²æ­¢ç”Ÿæˆåè§ã€‚å®éªŒè¯æ˜ï¼ŒDiffusion-FSCILåœ¨CUB-200ã€miniImageNetå’ŒCIFAR-100ä¸Šè¶…è¿‡äº†ç°æœ‰æ–¹æ³•ï¼Œèƒ½å¤Ÿåœ¨ä¿æŒå¯¹å…ˆå‰å­¦ä¹ ç±»çš„æ€§èƒ½çš„åŒæ—¶ï¼Œæœ‰æ•ˆåœ°é€‚åº”æ–°ç±»ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Diffusion-FSCILé‡‡ç”¨æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹ä½œä¸ºå†»ç»“ä¸»å¹²ï¼Œä»¥è§£å†³å°‘é‡æ ·æœ¬ç±»å¢é‡å­¦ä¹ ï¼ˆFSCILï¼‰çš„æŒ‘æˆ˜ã€‚</li>
<li>å¤§å‹ç”Ÿæˆæ¨¡å‹çš„å¤šé‡èƒ½åŠ›ï¼ŒåŒ…æ‹¬ç”Ÿæˆèƒ½åŠ›ã€å¤šå°ºåº¦è¡¨ç¤ºå’Œæ–‡æœ¬ç¼–ç å™¨çš„è¡¨å¾çµæ´»æ€§ï¼Œè¢«ç”¨äºè§£å†³FSCILé—®é¢˜ã€‚</li>
<li>æå–å¤šç§äº’è¡¥æ‰©æ•£ç‰¹å¾ä»¥å®ç°å¼ºå¤§çš„è¡¨ç¤ºèƒ½åŠ›ï¼ŒåŒæ—¶åˆ©ç”¨è½»å¾®çš„ç‰¹å¾è’¸é¦æ¥é˜²æ­¢ç”Ÿæˆåè§ã€‚</li>
<li>Diffusion-FSCILæ¡†æ¶å…·æœ‰æ•ˆç‡ä¼˜åŠ¿ï¼Œä½“ç°åœ¨ä½¿ç”¨å†»ç»“ä¸»å¹²ã€å°‘é‡å¯è®­ç»ƒç»„ä»¶ä»¥åŠå¤šç‰¹å¾æå–çš„æ‰¹é‡å¤„ç†ä¸Šã€‚</li>
<li>è¯¥æ–¹æ³•åœ¨CUB-200ã€miniImageNetå’ŒCIFAR-100ä¸Šçš„å®éªŒè¡¨ç°è¶…è¿‡ç°æœ‰æ–¹æ³•ã€‚</li>
<li>Diffusion-FSCILèƒ½å¤Ÿåœ¨ä¿æŒå¯¹å…ˆå‰å­¦ä¹ ç±»çš„æ€§èƒ½çš„åŒæ—¶ï¼Œæœ‰æ•ˆåœ°é€‚åº”æ–°ç±»ã€‚</li>
<li>Diffusion-FSCILçš„æˆåŠŸåœ¨äºç»“åˆç”Ÿæˆæ¨¡å‹çš„å¼ºå¤§èƒ½åŠ›ä¸å¢é‡å­¦ä¹ çš„éœ€æ±‚ï¼Œå±•ç¤ºäº†åœ¨æœ‰é™æ•°æ®ä¸‹å®ç°é«˜æ•ˆå­¦ä¹ çš„å¯èƒ½æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.23402">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-04-03\./crop_Few-Shot/2503.23402v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-04-03\./crop_Few-Shot/2503.23402v1/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-04-03\./crop_Few-Shot/2503.23402v1/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-04-03\./crop_Few-Shot/2503.23402v1/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-04-03\./crop_Few-Shot/2503.23402v1/page_5_0.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="Ethereum-Price-Prediction-Employing-Large-Language-Models-for-Short-term-and-Few-shot-Forecasting"><a href="#Ethereum-Price-Prediction-Employing-Large-Language-Models-for-Short-term-and-Few-shot-Forecasting" class="headerlink" title="Ethereum Price Prediction Employing Large Language Models for Short-term   and Few-shot Forecasting"></a>Ethereum Price Prediction Employing Large Language Models for Short-term   and Few-shot Forecasting</h2><p><strong>Authors:Eftychia Makri, Georgios Palaiokrassas, Sarah Bouraga, Antigoni Polychroniadou, Leandros Tassiulas</strong></p>
<p>Cryptocurrencies have transformed financial markets with their innovative blockchain technology and volatile price movements, presenting both challenges and opportunities for predictive analytics. Ethereum, being one of the leading cryptocurrencies, has experienced significant market fluctuations, making its price prediction an attractive yet complex problem. This paper presents a comprehensive study on the effectiveness of Large Language Models (LLMs) in predicting Ethereum prices for short-term and few-shot forecasting scenarios. The main challenge in training models for time series analysis is the lack of data. We address this by leveraging a novel approach that adapts existing pre-trained LLMs on natural language or images from billions of tokens to the unique characteristics of Ethereum price time series data. Through thorough experimentation and comparison with traditional and contemporary models, our results demonstrate that selectively freezing certain layers of pre-trained LLMs achieves state-of-the-art performance in this domain. This approach consistently surpasses benchmarks across multiple metrics, including Mean Squared Error (MSE), Mean Absolute Error (MAE), and Root Mean Squared Error (RMSE), demonstrating its effectiveness and robustness. Our research not only contributes to the existing body of knowledge on LLMs but also provides practical insights in the cryptocurrency prediction domain. The adaptability of pre-trained LLMs to handle the nature of Ethereum prices suggests a promising direction for future research, potentially including the integration of sentiment analysis to further refine forecasting accuracy. </p>
<blockquote>
<p>åŠ å¯†è´§å¸å‡­å€Ÿå…¶åˆ›æ–°çš„åŒºå—é“¾æŠ€æœ¯å’Œæ³¢åŠ¨çš„ä»·æ ¼èµ°åŠ¿ï¼Œä¸ºé‡‘èå¸‚åœºå¸¦æ¥äº†æŒ‘æˆ˜å’Œæœºé‡ï¼Œä»è€Œå®ç°äº†é‡‘èå¸‚åœºçš„è½¬å‹ã€‚ä»¥å¤ªåŠä½œä¸ºé¢†å…ˆçš„åŠ å¯†è´§å¸ä¹‹ä¸€ï¼Œå¸‚åœºæ³¢åŠ¨æ˜¾è‘—ï¼Œä½¿å…¶ä»·æ ¼é¢„æµ‹æˆä¸ºä¸€ä¸ªå¸å¼•äººä½†åˆå¤æ‚çš„é—®é¢˜ã€‚æœ¬æ–‡å…¨é¢ç ”ç©¶äº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨çŸ­æœŸå’Œå°‘é‡æ ·æœ¬é¢„æµ‹åœºæ™¯ä¸­å¯¹ä»¥å¤ªåŠä»·æ ¼é¢„æµ‹çš„æ•ˆåŠ›ã€‚æ—¶é—´åºåˆ—åˆ†ææ¨¡å‹è®­ç»ƒçš„ä¸»è¦æŒ‘æˆ˜æ˜¯æ•°æ®ç¼ºä¹ã€‚æˆ‘ä»¬é€šè¿‡é‡‡ç”¨ä¸€ç§æ–°æ–¹æ³•æ¥è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œè¯¥æ–¹æ³•é€‚åº”äºåœ¨æ•°åäº¿ä¸ªä»¤ç‰Œä¸Šé¢„å…ˆè®­ç»ƒçš„ç°æœ‰LLMsçš„è‡ªç„¶è¯­è¨€æˆ–å›¾åƒï¼Œå¹¶å°†å…¶åº”ç”¨äºä»¥å¤ªåŠä»·æ ¼æ—¶é—´åºåˆ—æ•°æ®çš„ç‹¬ç‰¹ç‰¹å¾ã€‚é€šè¿‡ä¸ä¼ ç»Ÿå’Œå½“ä»£æ¨¡å‹çš„å®éªŒæ¯”è¾ƒï¼Œæˆ‘ä»¬çš„ç»“æœè¡¨æ˜ï¼Œé€‰æ‹©æ€§å†»ç»“é¢„è®­ç»ƒLLMsçš„æŸäº›å±‚å¯ä»¥åœ¨è¯¥é¢†åŸŸå®ç°æœ€æ–°æ€§èƒ½ã€‚è¯¥æ–¹æ³•åœ¨å¤šä¸ªæŒ‡æ ‡ä¸Šå‡è¶…è¶ŠåŸºå‡†æµ‹è¯•ï¼ŒåŒ…æ‹¬å‡æ–¹è¯¯å·®ï¼ˆMSEï¼‰ã€å¹³å‡ç»å¯¹è¯¯å·®ï¼ˆMAEï¼‰å’Œå‡æ–¹æ ¹è¯¯å·®ï¼ˆRMSEï¼‰ï¼Œè¯æ˜äº†å…¶æœ‰æ•ˆæ€§å’Œç¨³å¥æ€§ã€‚æˆ‘ä»¬çš„ç ”ç©¶ä¸ä»…ä¸ºLLMsçš„ç°æœ‰çŸ¥è¯†ä½“ç³»åšå‡ºäº†è´¡çŒ®ï¼Œè€Œä¸”è¿˜ä¸ºåŠ å¯†è´§å¸é¢„æµ‹é¢†åŸŸæä¾›äº†å®é™…è§è§£ã€‚é¢„è®­ç»ƒLLMsé€‚åº”ä»¥å¤ªåŠä»·æ ¼ç‰¹æ€§çš„èƒ½åŠ›ä¸ºæœªæ¥çš„ç ”ç©¶æä¾›äº†ä¸€ä¸ªæœ‰å‰æ™¯çš„æ–¹å‘ï¼Œæœªæ¥å¯èƒ½åŒ…æ‹¬æƒ…æ„Ÿåˆ†æçš„æ•´åˆï¼Œä»¥è¿›ä¸€æ­¥æé«˜é¢„æµ‹ç²¾åº¦ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.23190v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<pre><code>æœ¬æ–‡ç ”ç©¶äº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨é¢„æµ‹ä»¥å¤ªåŠä»·æ ¼çŸ­æœŸå’Œå°‘é‡é¢„æµ‹åœºæ™¯ä¸­çš„æœ‰æ•ˆæ€§ã€‚ç ”ç©¶é¢ä¸´çš„ä¸»è¦æŒ‘æˆ˜æ˜¯ç¼ºä¹æ—¶é—´åºåˆ—åˆ†æçš„æ•°æ®ã€‚é€šè¿‡é€‚åº”ç°æœ‰çš„é¢„è®­ç»ƒLLMsï¼Œå°†å…¶ä»è‡ªç„¶è¯­è¨€æˆ–å›¾åƒæ•°åäº¿ä»¤ç‰Œçš„ç‰¹å¾è½¬æ¢ä¸ºä»¥å¤ªåŠä»·æ ¼æ—¶é—´åºåˆ—æ•°æ®çš„ç‹¬ç‰¹ç‰¹å¾æ¥è§£å†³æ­¤æŒ‘æˆ˜ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæœ‰é€‰æ‹©åœ°å†»ç»“é¢„è®­ç»ƒLLMsçš„æŸäº›å±‚å¯å®ç°è¯¥é¢†åŸŸçš„æœ€æ–°æ€§èƒ½ã€‚æ­¤æ–¹æ³•åœ¨å¤šä¸ªæŒ‡æ ‡ä¸Šå‡è¶…è¿‡åŸºå‡†æµ‹è¯•ï¼ŒåŒ…æ‹¬å‡æ–¹è¯¯å·®ï¼ˆMSEï¼‰ã€å¹³å‡ç»å¯¹è¯¯å·®ï¼ˆMAEï¼‰å’Œå‡æ–¹æ ¹è¯¯å·®ï¼ˆRMSEï¼‰ï¼Œè¯æ˜äº†å…¶æœ‰æ•ˆæ€§å’Œç¨³å¥æ€§ã€‚
</code></pre>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>åŒºå—é“¾æŠ€æœ¯ä¸ä»¥å¤ªåŠä»·æ ¼çš„å¤§å¹…æ³¢åŠ¨å¯¹é‡‘èå¸‚åœºé¢„æµ‹å¸¦æ¥äº†æŒ‘æˆ˜ä¸æœºé‡ã€‚</li>
<li>åˆ©ç”¨é¢„è®­ç»ƒçš„å¤§å‹è¯­è¨€æ¨¡å‹å¯¹ä»¥å¤ªåŠä»·æ ¼è¿›è¡ŒçŸ­æœŸå’Œå°‘é‡é¢„æµ‹æ˜¯æœ‰æ•ˆçš„ã€‚</li>
<li>è®­ç»ƒæ¨¡å‹é¢ä¸´çš„ä¸»è¦æŒ‘æˆ˜æ˜¯ç¼ºä¹æ—¶é—´åºåˆ—åˆ†ææ•°æ®ã€‚ä¸ºæ­¤ï¼Œç ”ç©¶äººå‘˜åˆ©ç”¨ç°æœ‰é¢„è®­ç»ƒçš„å¤§å‹è¯­è¨€æ¨¡å‹çš„è‡ªç„¶è¯­è¨€æˆ–å›¾åƒç‰¹å¾æ¥é€‚åº”ä»¥å¤ªåŠä»·æ ¼æ•°æ®çš„ç‹¬ç‰¹æ€§ã€‚</li>
<li>é€šè¿‡é€‰æ‹©æ€§å†»ç»“é¢„è®­ç»ƒæ¨¡å‹çš„æŸäº›å±‚ä»¥è·å¾—æœ€ä½³æ€§èƒ½ï¼Œå®éªŒè¯æ˜è¿™ä¸€æ–¹æ³•åœ¨å¤šä¸ªé¢„æµ‹æŒ‡æ ‡ä¸Šéƒ½è¾¾åˆ°äº†é¢†åŸŸæœ€ä½³æ°´å¹³ã€‚è¯¥æ–¹æ³•å±•ç¤ºäº†æé«˜é¢„æµ‹æ€§èƒ½çš„å‰æ²¿æŠ€æœ¯å’Œæ½œåŠ›ã€‚</li>
<li>ç ”ç©¶ä¸ä»…ä¸°å¯Œäº†å¤§å‹è¯­è¨€æ¨¡å‹çš„çŸ¥è¯†ä½“ç³»ï¼Œä¹Ÿä¸ºåŠ å¯†è´§å¸é¢„æµ‹é¢†åŸŸæä¾›äº†å®é™…è§è§£ã€‚è¿™è¡¨æ˜é¢„è®­ç»ƒçš„å¤§å‹è¯­è¨€æ¨¡å‹èƒ½å¤Ÿé€‚åº”ä»¥å¤ªåŠä»·æ ¼ç‰¹æ€§ï¼Œä¸ºæœªæ¥ç ”ç©¶æä¾›äº†æ–¹å‘ã€‚</li>
<li>æœªæ¥ç ”ç©¶å¯æ•´åˆæƒ…æ„Ÿåˆ†ææ¥è¿›ä¸€æ­¥æé«˜é¢„æµ‹å‡†ç¡®æ€§ã€‚ä»¥å¤ªåŠä»·æ ¼çš„é¢„æµ‹å¯èƒ½å—åˆ°å¸‚åœºæƒ…ç»ªçš„å½±å“ï¼Œå› æ­¤æ•´åˆæƒ…æ„Ÿåˆ†æå¯èƒ½æœ‰åŠ©äºæ›´å‡†ç¡®åœ°æ•æ‰å¸‚åœºåŠ¨æ€å¹¶æ”¹è¿›é¢„æµ‹æ¨¡å‹ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.23190">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-04-03\./crop_Few-Shot/2503.23190v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-04-03\./crop_Few-Shot/2503.23190v1/page_3_0.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="Unbiased-Max-Min-Embedding-Classification-for-Transductive-Few-Shot-Learning-Clustering-and-Classification-Are-All-You-Need"><a href="#Unbiased-Max-Min-Embedding-Classification-for-Transductive-Few-Shot-Learning-Clustering-and-Classification-Are-All-You-Need" class="headerlink" title="Unbiased Max-Min Embedding Classification for Transductive Few-Shot   Learning: Clustering and Classification Are All You Need"></a>Unbiased Max-Min Embedding Classification for Transductive Few-Shot   Learning: Clustering and Classification Are All You Need</h2><p><strong>Authors:Yang Liu, Feixiang Liu, Jiale Du, Xinbo Gao, Jungong Han</strong></p>
<p>Convolutional neural networks and supervised learning have achieved remarkable success in various fields but are limited by the need for large annotated datasets. Few-shot learning (FSL) addresses this limitation by enabling models to generalize from only a few labeled examples. Transductive few-shot learning (TFSL) enhances FSL by leveraging both labeled and unlabeled data, though it faces challenges like the hubness problem. To overcome these limitations, we propose the Unbiased Max-Min Embedding Classification (UMMEC) Method, which addresses the key challenges in few-shot learning through three innovative contributions. First, we introduce a decentralized covariance matrix to mitigate the hubness problem, ensuring a more uniform distribution of embeddings. Second, our method combines local alignment and global uniformity through adaptive weighting and nonlinear transformation, balancing intra-class clustering with inter-class separation. Third, we employ a Variational Sinkhorn Few-Shot Classifier to optimize the distances between samples and class prototypes, enhancing classification accuracy and robustness. These combined innovations allow the UMMEC method to achieve superior performance with minimal labeled data. Our UMMEC method significantly improves classification performance with minimal labeled data, advancing the state-of-the-art in TFSL. </p>
<blockquote>
<p>å·ç§¯ç¥ç»ç½‘ç»œå’Œæ·±åº¦å­¦ä¹ çš„ç›‘ç£å­¦ä¹ å·²ç»åœ¨å¤šä¸ªé¢†åŸŸå–å¾—äº†æ˜¾è‘—çš„æˆåŠŸï¼Œä½†å®ƒä»¬å—é™äºéœ€è¦å¤§é‡æ ‡æ³¨æ•°æ®é›†çš„é—®é¢˜ã€‚å°æ ·æœ¬å­¦ä¹ ï¼ˆFSLï¼‰é€šè¿‡è®©æ¨¡å‹ä»…ä»å°‘æ•°æ ‡æ³¨æ ·æœ¬ä¸­è¿›è¡Œæ¨å¹¿æ¥å…‹æœè¿™ä¸€é™åˆ¶ã€‚è·¨ä¼ å¯¼çš„å°æ ·æœ¬å­¦ä¹ ï¼ˆTFSLï¼‰åˆ™åˆ©ç”¨æ ‡æ³¨çš„å’Œæœªæ ‡æ³¨çš„æ•°æ®æ¥å¢å¼ºFSLï¼Œå°½ç®¡å®ƒé¢ä¸´ç€è¯¸å¦‚ä¸­å¿ƒé—®é¢˜ï¼ˆhubness problemï¼‰ç­‰æŒ‘æˆ˜ã€‚ä¸ºäº†å…‹æœè¿™äº›å±€é™æ€§ï¼Œæˆ‘ä»¬æå‡ºäº†æ— åæœ€å¤§æœ€å°åµŒå…¥åˆ†ç±»ï¼ˆUMMECï¼‰æ–¹æ³•ï¼Œå®ƒé€šè¿‡ä¸‰ä¸ªåˆ›æ–°ç‚¹è§£å†³äº†å°æ ·æœ¬å­¦ä¹ çš„å…³é”®æŒ‘æˆ˜ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬å¼•å…¥åˆ†æ•£çš„åæ–¹å·®çŸ©é˜µæ¥ç¼“è§£ä¸­å¿ƒé—®é¢˜ï¼Œç¡®ä¿åµŒå…¥çš„æ›´å‡åŒ€åˆ†å¸ƒã€‚å…¶æ¬¡ï¼Œæˆ‘ä»¬çš„æ–¹æ³•é€šè¿‡è‡ªé€‚åº”æƒé‡å’Œéçº¿æ€§å˜æ¢ç»“åˆå±€éƒ¨å¯¹é½å’Œå…¨å±€å‡åŒ€æ€§ï¼Œå¹³è¡¡äº†ç±»å†…èšç±»å’Œç±»é—´åˆ†ç¦»ã€‚ç¬¬ä¸‰ï¼Œæˆ‘ä»¬é‡‡ç”¨å˜åˆ†è¾›å…‹éœæ©å°æ ·æœ¬åˆ†ç±»å™¨æ¥ä¼˜åŒ–æ ·æœ¬ä¸ç±»åŸå‹ä¹‹é—´çš„è·ç¦»ï¼Œæé«˜åˆ†ç±»ç²¾åº¦å’Œç¨³å¥æ€§ã€‚è¿™äº›ç»¼åˆåˆ›æ–°ä½¿å¾—UMMECæ–¹æ³•åœ¨æå°‘é‡æ ‡æ³¨æ•°æ®çš„æƒ…å†µä¸‹å®ç°å“è¶Šæ€§èƒ½ã€‚æˆ‘ä»¬çš„UMMECæ–¹æ³•èƒ½åœ¨æå°‘æ ‡æ³¨æ•°æ®çš„æƒ…å†µä¸‹æ˜¾è‘—æé«˜åˆ†ç±»æ€§èƒ½ï¼Œæ¨åŠ¨äº†è·¨ä¼ å¯¼çš„å°æ ·æœ¬å­¦ä¹ ï¼ˆTFSLï¼‰çš„æœ€æ–°è¿›å±•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.22193v1">PDF</a> </p>
<p><strong>Summary</strong><br>å·ç§¯ç¥ç»ç½‘ç»œå’Œç›‘ç£å­¦ä¹ åœ¨å¤šä¸ªé¢†åŸŸå–å¾—äº†æ˜¾è‘—æˆåŠŸï¼Œä½†å—é™äºéœ€è¦å¤§é‡æ ‡æ³¨æ•°æ®é›†ã€‚å°‘æ ·æœ¬å­¦ä¹ ï¼ˆFSLï¼‰é€šè¿‡æ¨¡å‹ä»å°‘é‡æ ‡æ³¨æ ·æœ¬ä¸­æ³›åŒ–æ¥è§£å†³æ­¤é—®é¢˜ã€‚ä¼ é€’å¼å°‘æ ·æœ¬å­¦ä¹ ï¼ˆTFSLï¼‰åˆ©ç”¨æ ‡æ³¨å’Œæ— æ ‡ç­¾æ•°æ®ï¼Œä½†é¢ä¸´å¦‚ä¸­å¿ƒé—®é¢˜ã€‚ä¸ºè§£å†³è¿™äº›é™åˆ¶ï¼Œæˆ‘ä»¬æå‡ºäº†æ— åæœ€å¤§æœ€å°åµŒå…¥åˆ†ç±»ï¼ˆUMMECï¼‰æ–¹æ³•ï¼Œé€šè¿‡ä¸‰ä¸ªåˆ›æ–°è´¡çŒ®è§£å†³å°‘æ ·æœ¬å­¦ä¹ çš„å…³é”®æŒ‘æˆ˜ã€‚é¦–å…ˆï¼Œå¼•å…¥åˆ†æ•£åæ–¹å·®çŸ©é˜µç¼“è§£ä¸­å¿ƒé—®é¢˜ï¼Œç¡®ä¿æ›´å‡åŒ€çš„åµŒå…¥åˆ†å¸ƒã€‚å…¶æ¬¡ï¼Œç»“åˆå±€éƒ¨å¯¹é½å’Œå…¨å±€å‡åŒ€æ€§ï¼Œé€šè¿‡è‡ªé€‚åº”æƒé‡å’Œéçº¿æ€§å˜æ¢å¹³è¡¡ç±»å†…èšç±»å’Œç±»é—´åˆ†ç¦»ã€‚æœ€åï¼Œé‡‡ç”¨å˜åˆ†è¾›å…‹éœæ©å°‘æ ·æœ¬åˆ†ç±»å™¨ä¼˜åŒ–æ ·æœ¬ä¸ç±»åˆ«åŸå‹çš„è·ç¦»ï¼Œæé«˜åˆ†ç±»å‡†ç¡®æ€§å’Œç¨³å¥æ€§ã€‚è¿™äº›æ–¹æ³•åˆ›æ–°ä½¿UMMECåœ¨å°‘é‡æ ‡æ³¨æ•°æ®çš„æƒ…å†µä¸‹å®ç°å“è¶Šæ€§èƒ½ï¼Œåœ¨TFSLé¢†åŸŸå–å¾—äº†æœ€æ–°è¿›å±•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>å·ç§¯ç¥ç»ç½‘ç»œå’Œç›‘ç£å­¦ä¹ è™½å–å¾—æ˜¾è‘—æˆåŠŸï¼Œä½†éœ€å¤§é‡æ ‡æ³¨æ•°æ®é›†ã€‚</li>
<li>å°‘æ ·æœ¬å­¦ä¹ ï¼ˆFSLï¼‰é€šè¿‡æ¨¡å‹æ³›åŒ–è§£å†³æ­¤é—®é¢˜ï¼Œä¼ é€’å¼å°‘æ ·æœ¬å­¦ä¹ ï¼ˆTFSLï¼‰è¿›ä¸€æ­¥åˆ©ç”¨æ— æ ‡ç­¾æ•°æ®ã€‚</li>
<li>TFSLé¢ä¸´æŒ‘æˆ˜å¦‚ä¸­å¿ƒé—®é¢˜ï¼Œéœ€é€šè¿‡æœ‰æ•ˆç­–ç•¥è§£å†³ã€‚</li>
<li>UMMECæ–¹æ³•é€šè¿‡å¼•å…¥åˆ†æ•£åæ–¹å·®çŸ©é˜µç¼“è§£ä¸­å¿ƒé—®é¢˜ï¼Œç¡®ä¿æ›´å‡åŒ€çš„åµŒå…¥åˆ†å¸ƒã€‚</li>
<li>UMMECç»“åˆå±€éƒ¨å¯¹é½å’Œå…¨å±€å‡åŒ€æ€§ä»¥å¹³è¡¡ç±»å†…èšç±»å’Œç±»é—´åˆ†ç¦»ã€‚</li>
<li>é‡‡ç”¨å˜åˆ†è¾›å…‹éœæ©å°‘æ ·æœ¬åˆ†ç±»å™¨ä¼˜åŒ–æ ·æœ¬ä¸ç±»åˆ«åŸå‹é—´çš„è·ç¦»ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.22193">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-04-03\./crop_Few-Shot/2503.22193v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-04-03\./crop_Few-Shot/2503.22193v1/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-04-03\./crop_Few-Shot/2503.22193v1/page_5_0.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="Few-Shot-Graph-Out-of-Distribution-Detection-with-LLMs"><a href="#Few-Shot-Graph-Out-of-Distribution-Detection-with-LLMs" class="headerlink" title="Few-Shot Graph Out-of-Distribution Detection with LLMs"></a>Few-Shot Graph Out-of-Distribution Detection with LLMs</h2><p><strong>Authors:Haoyan Xu, Zhengtao Yao, Yushun Dong, Ziyi Wang, Ryan A. Rossi, Mengyuan Li, Yue Zhao</strong></p>
<p>Existing methods for graph out-of-distribution (OOD) detection typically depend on training graph neural network (GNN) classifiers using a substantial amount of labeled in-distribution (ID) data. However, acquiring high-quality labeled nodes in text-attributed graphs (TAGs) is challenging and costly due to their complex textual and structural characteristics. Large language models (LLMs), known for their powerful zero-shot capabilities in textual tasks, show promise but struggle to naturally capture the critical structural information inherent to TAGs, limiting their direct effectiveness.   To address these challenges, we propose LLM-GOOD, a general framework that effectively combines the strengths of LLMs and GNNs to enhance data efficiency in graph OOD detection. Specifically, we first leverage LLMsâ€™ strong zero-shot capabilities to filter out likely OOD nodes, significantly reducing the human annotation burden. To minimize the usage and cost of the LLM, we employ it only to annotate a small subset of unlabeled nodes. We then train a lightweight GNN filter using these noisy labels, enabling efficient predictions of ID status for all other unlabeled nodes by leveraging both textual and structural information. After obtaining node embeddings from the GNN filter, we can apply informativeness-based methods to select the most valuable nodes for precise human annotation. Finally, we train the target ID classifier using these accurately annotated ID nodes. Extensive experiments on four real-world TAG datasets demonstrate that LLM-GOOD significantly reduces human annotation costs and outperforms state-of-the-art baselines in terms of both ID classification accuracy and OOD detection performance. </p>
<blockquote>
<p>ç°æœ‰å›¾å¤–åˆ†å¸ƒï¼ˆOODï¼‰æ£€æµ‹çš„æ–¹æ³•é€šå¸¸ä¾èµ–äºä½¿ç”¨å¤§é‡çš„å·²æ ‡æ³¨å†…åˆ†å¸ƒï¼ˆIDï¼‰æ•°æ®æ¥è®­ç»ƒå›¾ç¥ç»ç½‘ç»œï¼ˆGNNï¼‰åˆ†ç±»å™¨ã€‚ç„¶è€Œï¼Œç”±äºæ–‡æœ¬å±æ€§å›¾ï¼ˆTAGsï¼‰çš„å¤æ‚æ–‡æœ¬å’Œç»“æ„ç‰¹æ€§ï¼Œè·å–é«˜è´¨é‡æ ‡æ³¨èŠ‚ç‚¹å…·æœ‰æŒ‘æˆ˜æ€§å’Œæˆæœ¬é«˜æ˜‚ã€‚å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨æ–‡æœ¬ä»»åŠ¡ä¸­è¡¨ç°å‡ºå¼ºå¤§çš„é›¶æ ·æœ¬èƒ½åŠ›ï¼Œæ˜¾ç¤ºå‡ºæ½œåŠ›ï¼Œä½†éš¾ä»¥è‡ªç„¶åœ°æ•è·TAGsæ‰€å›ºæœ‰çš„å…³é”®ç»“æ„ä¿¡æ¯ï¼Œä»è€Œé™åˆ¶äº†å…¶ç›´æ¥æ•ˆæœã€‚</p>
</blockquote>
<p>ä¸ºäº†è§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†LLM-GOODï¼Œè¿™æ˜¯ä¸€ä¸ªæœ‰æ•ˆç»“åˆLLMså’ŒGNNsä¼˜åŠ¿çš„é€šç”¨æ¡†æ¶ï¼Œä»¥æé«˜å›¾OODæ£€æµ‹ä¸­çš„æ•°æ®æ•ˆç‡ã€‚å…·ä½“è€Œè¨€ï¼Œæˆ‘ä»¬é¦–å…ˆåˆ©ç”¨LLMsçš„å¼ºå¤§é›¶æ ·æœ¬èƒ½åŠ›æ¥ç­›é€‰å‡ºå¯èƒ½çš„OODèŠ‚ç‚¹ï¼Œä»è€Œå¤§å¤§å‡å°‘äº†äººå·¥æ ‡æ³¨çš„è´Ÿæ‹…ã€‚ä¸ºäº†æœ€å°åŒ–LLMçš„ä½¿ç”¨å’Œæˆæœ¬ï¼Œæˆ‘ä»¬åªä½¿ç”¨å®ƒæ¥æ ‡æ³¨ä¸€å°éƒ¨åˆ†æœªæ ‡è®°çš„èŠ‚ç‚¹ã€‚ç„¶åï¼Œæˆ‘ä»¬ä½¿ç”¨è¿™äº›å¸¦å™ªå£°çš„æ ‡ç­¾è®­ç»ƒä¸€ä¸ªè½»é‡çº§çš„GNNè¿‡æ»¤å™¨ï¼Œè¯¥è¿‡æ»¤å™¨èƒ½å¤Ÿåˆ©ç”¨æ–‡æœ¬å’Œç»“æ„ä¿¡æ¯å¯¹æ‰€æœ‰å…¶ä»–æœªæ ‡è®°èŠ‚ç‚¹è¿›è¡ŒIDçŠ¶æ€çš„é«˜æ•ˆé¢„æµ‹ã€‚ä»GNNè¿‡æ»¤å™¨è·å¾—èŠ‚ç‚¹åµŒå…¥åï¼Œæˆ‘ä»¬å¯ä»¥åº”ç”¨åŸºäºä¿¡æ¯çš„æ–¹æ³•æ¥é€‰æ‹©æœ€æœ‰ä»·å€¼çš„èŠ‚ç‚¹è¿›è¡Œç²¾ç¡®çš„äººå·¥æ ‡æ³¨ã€‚æœ€åï¼Œæˆ‘ä»¬ä½¿ç”¨è¿™äº›å‡†ç¡®æ ‡æ³¨çš„IDèŠ‚ç‚¹è®­ç»ƒç›®æ ‡IDåˆ†ç±»å™¨ã€‚åœ¨å››ä¸ªçœŸå®ä¸–ç•Œçš„TAGæ•°æ®é›†ä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼ŒLLM-GOODæ˜¾è‘—é™ä½äº†äººå·¥æ ‡æ³¨æˆæœ¬ï¼Œå¹¶ä¸”åœ¨IDåˆ†ç±»å‡†ç¡®æ€§å’ŒOODæ£€æµ‹æ€§èƒ½æ–¹é¢å‡ä¼˜äºæœ€æ–°åŸºçº¿ã€‚</p>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.22097v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºä¸€ç§ç»“åˆå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å’Œå›¾ç¥ç»ç½‘ç»œï¼ˆGNNsï¼‰çš„é€šç”¨æ¡†æ¶LLM-GOODï¼Œä»¥æé«˜æ–‡æœ¬å±æ€§å›¾ï¼ˆTAGsï¼‰ä¸­å›¾å¼‚å¸¸å€¼æ£€æµ‹çš„æ•°æ®æ•ˆç‡ã€‚è¯¥æ¡†æ¶åˆ©ç”¨LLMsçš„å¼ºå¤§é›¶æ ·æœ¬èƒ½åŠ›è¿‡æ»¤å‡ºå¯èƒ½çš„å¼‚å¸¸èŠ‚ç‚¹ï¼Œå‡å°‘äººå·¥æ ‡æ³¨è´Ÿæ‹…ã€‚ç„¶åè®­ç»ƒè½»é‡çº§GNNæ»¤æ³¢å™¨ä½¿ç”¨è¿™äº›å™ªå£°æ ‡ç­¾è¿›è¡Œé¢„æµ‹ï¼Œåˆ©ç”¨æ–‡æœ¬å’Œç»“æ„ä¿¡æ¯æé«˜é¢„æµ‹æ•ˆç‡ã€‚æœ€åï¼Œä½¿ç”¨ç²¾é€‰çš„å‡†ç¡®æ ‡æ³¨çš„èŠ‚ç‚¹è®­ç»ƒç›®æ ‡åˆ†ç±»å™¨ã€‚è¯¥æ¡†æ¶åœ¨å››ä¸ªçœŸå®ä¸–ç•ŒTAGæ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œå®ƒæ˜¾è‘—é™ä½äº†äººå·¥æ ‡æ³¨æˆæœ¬ï¼Œå¹¶åœ¨èº«ä»½åˆ†ç±»å‡†ç¡®æ€§å’Œå¼‚å¸¸æ£€æµ‹æ€§èƒ½ä¸Šä¼˜äºæœ€æ–°åŸºçº¿ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æå‡ºLLM-GOODæ¡†æ¶ï¼Œç»“åˆLLMså’ŒGNNsä»¥æé«˜æ–‡æœ¬å±æ€§å›¾ï¼ˆTAGsï¼‰ä¸­çš„å›¾å¼‚å¸¸å€¼æ£€æµ‹æ•°æ®æ•ˆç‡ã€‚</li>
<li>åˆ©ç”¨LLMsçš„å¼ºå¤§é›¶æ ·æœ¬èƒ½åŠ›è¿‡æ»¤å¯èƒ½çš„å¼‚å¸¸èŠ‚ç‚¹ï¼Œé™ä½äººå·¥æ ‡æ³¨è´Ÿæ‹…ã€‚</li>
<li>ä½¿ç”¨è½»é‡çº§GNNæ»¤æ³¢å™¨è¿›è¡Œé¢„æµ‹ï¼Œåˆ©ç”¨æ–‡æœ¬å’Œç»“æ„ä¿¡æ¯æé«˜é¢„æµ‹æ•ˆç‡ã€‚</li>
<li>é€šè¿‡ä¿¡æ¯æ€§æ–¹æ³•é€‰æ‹©æœ€æœ‰ä»·å€¼çš„èŠ‚ç‚¹è¿›è¡Œç²¾ç¡®äººå·¥æ ‡æ³¨ã€‚</li>
<li>è®­ç»ƒç›®æ ‡åˆ†ç±»å™¨ä½¿ç”¨å‡†ç¡®æ ‡æ³¨çš„èŠ‚ç‚¹ã€‚</li>
<li>åœ¨å››ä¸ªçœŸå®ä¸–ç•ŒTAGæ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜LLM-GOODçš„æœ‰æ•ˆæ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.22097">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-04-03\./crop_Few-Shot/2503.22097v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-04-03\./crop_Few-Shot/2503.22097v1/page_2_0.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="Think-or-Not-Think-A-Study-of-Explicit-Thinking-inRule-Based-Visual-Reinforcement-Fine-Tuning"><a href="#Think-or-Not-Think-A-Study-of-Explicit-Thinking-inRule-Based-Visual-Reinforcement-Fine-Tuning" class="headerlink" title="Think or Not Think: A Study of Explicit Thinking inRule-Based Visual   Reinforcement Fine-Tuning"></a>Think or Not Think: A Study of Explicit Thinking inRule-Based Visual   Reinforcement Fine-Tuning</h2><p><strong>Authors:Ming Li, Jike Zhong, Shitian Zhao, Yuxiang Lai, Kaipeng Zhang</strong></p>
<p>This paper investigates rule-based reinforcement learning (RL) fine-tuning for visual classification using multi-modal large language models (MLLMs) and the role of the thinking process. We begin by exploring \textit{CLS-RL}, a method that leverages verifiable signals as rewards to encourage MLLMs to â€˜thinkâ€™ before classifying. Our experiments across \textbf{eleven} datasets demonstrate that CLS-RL achieves significant improvements over supervised fine-tuning (SFT) in both base-to-new generalization and few-shot learning scenarios. Notably, we observe a â€˜free-lunchâ€™ phenomenon where fine-tuning on one dataset unexpectedly enhances performance on others, suggesting that RL effectively teaches fundamental classification skills. However, we question whether the explicit thinking, a critical aspect of rule-based RL, is always beneficial or indispensable. Challenging the conventional assumption that complex reasoning enhances performance, we introduce \textit{No-Thinking-RL}, a novel approach that minimizes the modelâ€™s thinking during fine-tuning by utilizing an equality accuracy reward. Our experiments reveal that No-Thinking-RL achieves superior in-domain performance and generalization capabilities compared to CLS-RL, while requiring significantly less fine-tuning time. This underscores that, contrary to prevailing assumptions, reducing the thinking process can lead to more efficient and effective MLLM fine-tuning for some visual tasks. Furthermore, No-Thinking-RL demonstrates enhanced performance on other visual benchmarks, such as a 6.4% improvement on CVBench. We hope our findings provides insights into the impact of thinking in RL-based fine-tuning. </p>
<blockquote>
<p>æœ¬æ–‡æ¢è®¨äº†åŸºäºè§„åˆ™çš„å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰åœ¨å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰çš„è§†è§‰åˆ†ç±»ä»»åŠ¡ä¸­çš„å¾®è°ƒæ–¹æ³•ï¼Œä»¥åŠæ€è€ƒè¿‡ç¨‹çš„ä½œç”¨ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬æ¢è®¨äº†CLS-RLæ–¹æ³•ï¼Œè¯¥æ–¹æ³•åˆ©ç”¨å¯éªŒè¯çš„ä¿¡å·ä½œä¸ºå¥–åŠ±æ¥é¼“åŠ±MLLMåœ¨åˆ†ç±»ä¹‹å‰è¿›è¡Œâ€œæ€è€ƒâ€ã€‚æˆ‘ä»¬çš„å®éªŒè·¨è¶Šäº†åä¸€ä¸ªæ•°æ®é›†ï¼Œè¯æ˜äº†CLS-RLåœ¨åŸºç¡€åˆ°æ–°çš„æ³›åŒ–å’Œå°‘æ ·æœ¬å­¦ä¹ åœºæ™¯ä¸­å‡æ˜¾è‘—ä¼˜äºç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œæˆ‘ä»¬è§‚å¯Ÿåˆ°äº†ä¸€ç§â€œå…è´¹åˆé¤â€ç°è±¡ï¼Œå³åœ¨ä¸€ä¸ªæ•°æ®é›†ä¸Šçš„å¾®è°ƒæ„å¤–åœ°æé«˜äº†å…¶ä»–æ•°æ®é›†çš„æ€§èƒ½ï¼Œè¿™è¡¨æ˜RLæœ‰æ•ˆåœ°ä¼ æˆäº†åŸºæœ¬çš„åˆ†ç±»æŠ€èƒ½ã€‚ç„¶è€Œï¼Œæˆ‘ä»¬è´¨ç–‘åŸºäºè§„åˆ™çš„RLä¸­çš„æ˜ç¡®æ€è€ƒæ˜¯å¦æ€»æ˜¯æœ‰ç›Šæˆ–ä¸å¯æˆ–ç¼ºã€‚æˆ‘ä»¬æŒ‘æˆ˜äº†å¤æ‚æ¨ç†èƒ½æé«˜æ€§èƒ½çš„å¸¸è§„å‡è®¾ï¼Œå¹¶å¼•å…¥äº†No-Thinking-RLè¿™ä¸€æ–°æ–¹æ³•ï¼Œå®ƒé€šè¿‡é‡‡ç”¨å¹³ç­‰å‡†ç¡®æ€§å¥–åŠ±æ¥æœ€å°åŒ–æ¨¡å‹åœ¨å¾®è°ƒè¿‡ç¨‹ä¸­çš„æ€è€ƒã€‚æˆ‘ä»¬çš„å®éªŒè¡¨æ˜ï¼Œä¸CLS-RLç›¸æ¯”ï¼ŒNo-Thinking-RLåœ¨åŸŸå†…æ€§èƒ½å’Œæ³›åŒ–èƒ½åŠ›æ–¹é¢æ›´èƒœä¸€ç­¹ï¼ŒåŒæ—¶æ˜¾è‘—å‡å°‘äº†å¾®è°ƒæ—¶é—´ã€‚è¿™å¼ºè°ƒäº†ä¸€ä¸ªäº‹å®ï¼šä¸æ™®éå‡è®¾ç›¸åï¼Œå‡å°‘æ€è€ƒè¿‡ç¨‹å¯ä»¥ä¸ºæŸäº›è§†è§‰ä»»åŠ¡æä¾›æ›´é«˜æ•ˆã€æ›´æœ‰æ•ˆçš„MLLMå¾®è°ƒã€‚æ­¤å¤–ï¼ŒNo-Thinking-RLåœ¨å…¶ä»–è§†è§‰åŸºå‡†æµ‹è¯•ä¸­ä¹Ÿè¡¨ç°å‡ºå“è¶Šçš„æ€§èƒ½ï¼Œå¦‚åœ¨CVBenchä¸Šæé«˜äº†6.4%ã€‚æˆ‘ä»¬å¸Œæœ›è¿™äº›å‘ç°èƒ½ä¸ºåŸºäºRLçš„å¾®è°ƒä¸­çš„æ€è€ƒå½±å“æä¾›è§è§£ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.16188v2">PDF</a> Preprint, work in progress. Add results on CVBench</p>
<p><strong>Summary</strong><br>     æœ¬æ–‡æ¢è®¨äº†åŸºäºè§„åˆ™çš„å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰åœ¨è§†è§‰åˆ†ç±»æ–¹é¢çš„å¾®è°ƒï¼Œä½¿ç”¨äº†å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰ã€‚é€šè¿‡CLS-RLæ–¹æ³•åˆ©ç”¨å¯éªŒè¯ä¿¡å·ä½œä¸ºå¥–åŠ±æ¥é¼“åŠ±MLLMåœ¨åˆ†ç±»ä¹‹å‰è¿›è¡Œæ€è€ƒã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒCLS-RLåœ¨åŸºç¡€åˆ°æ–°çš„æ³›åŒ–èƒ½åŠ›å’Œå°‘æ ·æœ¬å­¦ä¹ åœºæ™¯ä¸‹å‡ä¼˜äºç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰ã€‚è¿˜è§‚å¯Ÿåˆ°äº†ä¸€ä¸ªç°è±¡ï¼šåœ¨ä¸€ä¸ªæ•°æ®é›†ä¸Šè¿›è¡Œå¾®è°ƒä¼šæ„å¤–åœ°æé«˜åœ¨å…¶ä»–æ•°æ®é›†ä¸Šçš„æ€§èƒ½ï¼Œè¿™è¡¨æ˜RLå®é™…ä¸Šæ•™ä¼šäº†åŸºæœ¬çš„åˆ†ç±»æŠ€èƒ½ã€‚æ­¤å¤–ï¼Œæœ¬æ–‡è´¨ç–‘äº†æ˜¾å¼æ€è€ƒæ˜¯å¦æ€»æ˜¯æœ‰ç›Šæˆ–ä¸å¯æˆ–ç¼ºã€‚æå‡ºäº†No-Thinking-RLæ–¹æ³•ï¼Œé€šè¿‡å¹³ç­‰å‡†ç¡®æ€§å¥–åŠ±æ¥æœ€å°åŒ–æ¨¡å‹çš„æ€è€ƒè¿‡ç¨‹ã€‚å®éªŒè¡¨æ˜ï¼ŒNo-Thinking-RLåœ¨åŸŸå†…æ€§èƒ½å’Œæ³›åŒ–èƒ½åŠ›ä¸Šä¼˜äºCLS-RLï¼Œä¸”å¾®è°ƒæ—¶é—´å¤§å¤§å‡å°‘ã€‚è¿™æ‰“ç ´äº†å‡å°‘æ€è€ƒè¿‡ç¨‹ä¼šå¯¼è‡´æ€§èƒ½ä¸‹é™çš„å¸¸è§„å‡è®¾ï¼Œå¹¶ä¸ºæŸäº›è§†è§‰ä»»åŠ¡çš„MLLMå¾®è°ƒæä¾›äº†æ›´æœ‰æ•ˆçš„é€”å¾„ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>CLS-RLæ–¹æ³•åˆ©ç”¨å¯éªŒè¯ä¿¡å·ä½œä¸ºå¥–åŠ±é¼“åŠ±MLLMåœ¨åˆ†ç±»å‰æ€è€ƒï¼Œåœ¨å¤šä¸ªæ•°æ®é›†ä¸Šè¡¨ç°ä¼˜äºç›‘ç£å¾®è°ƒã€‚</li>
<li>RLåœ¨è§†è§‰åˆ†ç±»ä»»åŠ¡ä¸­èƒ½æœ‰æ•ˆæ•™ä¼šæ¨¡å‹åŸºæœ¬åˆ†ç±»æŠ€èƒ½ã€‚</li>
<li>å‡ºç°äº†â€œå…è´¹åˆé¤â€ç°è±¡ï¼šåœ¨ä¸€ä¸ªæ•°æ®é›†ä¸Šçš„å¾®è°ƒèƒ½æé«˜åœ¨å…¶ä»–æ•°æ®é›†ä¸Šçš„æ€§èƒ½ã€‚</li>
<li>è´¨ç–‘äº†æ˜¾å¼æ€è€ƒåœ¨RLå¾®è°ƒä¸­çš„å¿…è¦æ€§ã€‚</li>
<li>No-Thinking-RLæ–¹æ³•é€šè¿‡æœ€å°åŒ–æ¨¡å‹æ€è€ƒè¿‡ç¨‹ï¼Œå®ç°äº†ä¼˜å¼‚çš„åŸŸå†…æ€§èƒ½å’Œæ³›åŒ–èƒ½åŠ›ã€‚</li>
<li>No-Thinking-RLç›¸è¾ƒäºCLS-RLæ˜¾è‘—å‡å°‘äº†å¾®è°ƒæ—¶é—´ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.16188">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-04-03\./crop_Few-Shot/2503.16188v2/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-04-03\./crop_Few-Shot/2503.16188v2/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-04-03\./crop_Few-Shot/2503.16188v2/page_5_0.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="InPK-Infusing-Prior-Knowledge-into-Prompt-for-Vision-Language-Models"><a href="#InPK-Infusing-Prior-Knowledge-into-Prompt-for-Vision-Language-Models" class="headerlink" title="InPK: Infusing Prior Knowledge into Prompt for Vision-Language Models"></a>InPK: Infusing Prior Knowledge into Prompt for Vision-Language Models</h2><p><strong>Authors:Shuchang Zhou, Jiwei Wei, Shiyuan He, Yuyang Zhou, Chaoning Zhang, Jie Zou, Ning Xie, Yang Yang</strong></p>
<p>Prompt tuning has become a popular strategy for adapting Vision-Language Models (VLMs) to zero&#x2F;few-shot visual recognition tasks. Some prompting techniques introduce prior knowledge due to its richness, but when learnable tokens are randomly initialized and disconnected from prior knowledge, they tend to overfit on seen classes and struggle with domain shifts for unseen ones. To address this issue, we propose the InPK model, which infuses class-specific prior knowledge into the learnable tokens during initialization, thus enabling the model to explicitly focus on class-relevant information. Furthermore, to mitigate the weakening of class information by multi-layer encoders, we continuously reinforce the interaction between learnable tokens and prior knowledge across multiple feature levels. This progressive interaction allows the learnable tokens to better capture the fine-grained differences and universal visual concepts within prior knowledge, enabling the model to extract more discriminative and generalized text features. Even for unseen classes, the learned interaction allows the model to capture their common representations and infer their appropriate positions within the existing semantic structure. Moreover, we introduce a learnable text-to-vision projection layer to accommodate the text adjustments, ensuring better alignment of visual-text semantics. Extensive experiments on 11 recognition datasets show that InPK significantly outperforms state-of-the-art methods in multiple zero&#x2F;few-shot image classification tasks. </p>
<blockquote>
<p>æç¤ºè°ƒæ•´å·²æˆä¸ºé€‚åº”è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰åˆ°é›¶&#x2F;å°‘æ ·æœ¬è§†è§‰è¯†åˆ«ä»»åŠ¡çš„æµè¡Œç­–ç•¥ã€‚ä¸€äº›æç¤ºæŠ€æœ¯ç”±äºå…¶ä¸°å¯Œæ€§è€Œå¼•å…¥äº†å…ˆéªŒçŸ¥è¯†ï¼Œä½†æ˜¯å½“å¯å­¦ä¹ ä»¤ç‰Œéšæœºåˆå§‹åŒ–å¹¶ä¸å…ˆéªŒçŸ¥è¯†æ–­å¼€è¿æ¥æ—¶ï¼Œå®ƒä»¬å¾€å¾€ä¼šåœ¨å·²è§ç±»åˆ«ä¸Šè¿‡åº¦æ‹Ÿåˆï¼Œå¹¶ä¸”åœ¨æœªè§ç±»åˆ«ä¸Šå› é¢†åŸŸåç§»è€ŒæŒ£æ‰ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†InPKæ¨¡å‹ï¼Œè¯¥æ¨¡å‹åœ¨åˆå§‹åŒ–æœŸé—´å°†ç‰¹å®šç±»åˆ«çš„å…ˆéªŒçŸ¥è¯†æ³¨å…¥å¯å­¦ä¹ ä»¤ç‰Œä¸­ï¼Œä»è€Œä½¿æ¨¡å‹èƒ½å¤Ÿæ˜¾å¼å…³æ³¨ä¸ç±»åˆ«ç›¸å…³çš„ä¿¡æ¯ã€‚æ­¤å¤–ï¼Œä¸ºäº†å‡è½»å¤šå±‚ç¼–ç å™¨å¯¹ç±»åˆ«ä¿¡æ¯çš„å‰Šå¼±ï¼Œæˆ‘ä»¬è·¨å¤šä¸ªç‰¹å¾å±‚é¢æŒç»­åŠ å¼ºå¯å­¦ä¹ ä»¤ç‰Œå’Œå…ˆéªŒçŸ¥è¯†ä¹‹é—´çš„äº¤äº’ã€‚è¿™ç§æ¸è¿›çš„äº¤äº’å…è®¸å¯å­¦ä¹ ä»¤ç‰Œæ›´å¥½åœ°æ•æ‰å…ˆéªŒçŸ¥è¯†ä¸­çš„ç»†å¾®å·®åˆ«å’Œé€šç”¨è§†è§‰æ¦‚å¿µï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿæå–æ›´å…·è¾¨åˆ«åŠ›å’Œé€šç”¨åŒ–çš„æ–‡æœ¬ç‰¹å¾ã€‚å³ä½¿å¯¹äºæœªè§è¿‡çš„ç±»åˆ«ï¼Œå­¦ä¹ åˆ°çš„äº¤äº’ä¹Ÿå…è®¸æ¨¡å‹æ•è·å®ƒä»¬çš„é€šç”¨è¡¨ç¤ºå¹¶æ¨æ–­å®ƒä»¬åœ¨ç°æœ‰è¯­ä¹‰ç»“æ„ä¸­çš„é€‚å½“ä½ç½®ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ªå¯å­¦ä¹ çš„æ–‡æœ¬åˆ°è§†è§‰æŠ•å½±å±‚ä»¥é€‚åº”æ–‡æœ¬è°ƒæ•´ï¼Œç¡®ä¿è§†è§‰æ–‡æœ¬è¯­ä¹‰çš„æ›´å¥½å¯¹é½ã€‚åœ¨11ä¸ªè¯†åˆ«æ•°æ®é›†ä¸Šçš„å¤§é‡å®éªŒè¡¨æ˜ï¼ŒInPKåœ¨å¤šä¸ªé›¶&#x2F;å°‘æ ·æœ¬å›¾åƒåˆ†ç±»ä»»åŠ¡ä¸­æ˜¾è‘—ä¼˜äºæœ€æ–°æŠ€æœ¯çš„æ–¹æ³•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.19777v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†é’ˆå¯¹è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰åœ¨é›¶&#x2F;å°‘æ ·æœ¬è§†è§‰è¯†åˆ«ä»»åŠ¡ä¸­çš„é€‚åº”ç­–ç•¥â€”â€”æç¤ºè°ƒæ•´æŠ€æœ¯ã€‚é’ˆå¯¹ç°æœ‰æç¤ºæŠ€æœ¯ä¸­å¯å­¦ä¹ æ ‡è®°éšæœºåˆå§‹åŒ–å¹¶è„±ç¦»å…ˆéªŒçŸ¥è¯†å¯¼è‡´çš„è¿‡åº¦æ‹Ÿåˆå·²è§ç±»åˆ«å’Œé¢å¯¹æœªè§ç±»åˆ«æ—¶åŸŸåç§»çš„é—®é¢˜ï¼Œæå‡ºäº†InPKæ¨¡å‹ã€‚è¯¥æ¨¡å‹åœ¨åˆå§‹åŒ–æ—¶èå…¥ç±»åˆ«ç‰¹å®šçš„å…ˆéªŒçŸ¥è¯†åˆ°å¯å­¦ä¹ æ ‡è®°ä¸­ï¼Œä½¿æ¨¡å‹èƒ½æ˜¾å¼å…³æ³¨ç±»åˆ«ç›¸å…³ä¿¡æ¯ã€‚åŒæ—¶ï¼Œä¸ºäº†å‡è½»å¤šå±‚ç¼–ç å™¨å¯¹ç±»åˆ«ä¿¡æ¯çš„å‰Šå¼±ï¼ŒInPKæ¨¡å‹åœ¨å¤šç‰¹å¾çº§åˆ«ä¸ŠæŒç»­å¼ºåŒ–å¯å­¦ä¹ æ ‡è®°å’Œå…ˆéªŒçŸ¥è¯†ä¹‹é—´çš„äº¤äº’ã€‚è¿™ç§æ¸è¿›çš„äº¤äº’ä½¿å¯å­¦ä¹ æ ‡è®°èƒ½å¤Ÿæ›´å¥½åœ°æ•æ‰å…ˆéªŒçŸ¥è¯†ä¸­çš„ç»†å¾®å·®åˆ«å’Œé€šç”¨è§†è§‰æ¦‚å¿µï¼Œä»è€Œæå–æ›´å…·é‰´åˆ«åŠ›å’Œæ³›åŒ–çš„æ–‡æœ¬ç‰¹å¾ã€‚å¯¹äºæœªè§ç±»åˆ«ï¼Œè¿™ç§å­¦ä¹ äº¤äº’ä½¿æ¨¡å‹èƒ½å¤Ÿæ•æ‰å…¶å…±åŒè¡¨ç¤ºå¹¶æ¨æ–­å…¶åœ¨ç°æœ‰è¯­ä¹‰ç»“æ„ä¸­çš„é€‚å½“ä½ç½®ã€‚æ­¤å¤–ï¼Œè¿˜å¼•å…¥äº†ä¸€ä¸ªå¯å­¦ä¹ çš„æ–‡æœ¬åˆ°è§†è§‰æŠ•å½±å±‚ä»¥é€‚åº”æ–‡æœ¬è°ƒæ•´ï¼Œç¡®ä¿è§†è§‰æ–‡æœ¬è¯­ä¹‰çš„æ›´å¥½å¯¹é½ã€‚åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒInPKåœ¨å¤šä¸ªé›¶&#x2F;å°‘æ ·æœ¬å›¾åƒåˆ†ç±»ä»»åŠ¡ä¸­æ˜¾è‘—ä¼˜äºç°æœ‰æŠ€æœ¯ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¼•å‡ºé—®é¢˜ï¼šç°æœ‰çš„æç¤ºè°ƒæ•´æŠ€æœ¯åœ¨è§†è§‰è¯­è¨€æ¨¡å‹ä¸­éšæœºåˆå§‹åŒ–å¯å­¦ä¹ æ ‡è®°å¯¼è‡´è¿‡åº¦æ‹Ÿåˆå·²è§ç±»åˆ«å¹¶éš¾ä»¥é€‚åº”æœªè§ç±»åˆ«çš„åŸŸåç§»é—®é¢˜ã€‚</li>
<li>æå‡ºè§£å†³æ–¹æ¡ˆï¼šInPKæ¨¡å‹é€šè¿‡åˆå§‹åŒ–æ—¶èå…¥ç±»åˆ«ç‰¹å®šå…ˆéªŒçŸ¥è¯†åˆ°å¯å­¦ä¹ æ ‡è®°ä¸­è§£å†³ä¸Šè¿°é—®é¢˜ã€‚</li>
<li>æ¨¡å‹ç‰¹æ€§ï¼šInPKæ¨¡å‹åœ¨å¤šç‰¹å¾çº§åˆ«ä¸Šå¼ºåŒ–å¯å­¦ä¹ æ ‡è®°å’Œå…ˆéªŒçŸ¥è¯†ä¹‹é—´çš„äº¤äº’ï¼Œä»¥æå–æ›´å…·é‰´åˆ«åŠ›å’Œæ³›åŒ–çš„æ–‡æœ¬ç‰¹å¾ã€‚</li>
<li>é€‚åº”æœªè§ç±»åˆ«ï¼šæ¨¡å‹èƒ½å¤Ÿé€šè¿‡å­¦ä¹ äº¤äº’æ•æ‰æœªè§ç±»åˆ«çš„å…±åŒè¡¨ç¤ºå¹¶æ¨æ–­å…¶åœ¨ç°æœ‰è¯­ä¹‰ç»“æ„ä¸­çš„ä½ç½®ã€‚</li>
<li>åˆ›æ–°ç‚¹ï¼šå¼•å…¥å¯å­¦ä¹ çš„æ–‡æœ¬åˆ°è§†è§‰æŠ•å½±å±‚ä»¥é€‚åº”æ–‡æœ¬è°ƒæ•´ï¼Œç¡®ä¿è§†è§‰æ–‡æœ¬è¯­ä¹‰çš„æ›´å¥½å¯¹é½ã€‚</li>
<li>å®éªŒéªŒè¯ï¼šåœ¨å¤šä¸ªæ•°æ®é›†ä¸Šçš„å®éªŒè¯æ˜InPKæ¨¡å‹åœ¨é›¶&#x2F;å°‘æ ·æœ¬å›¾åƒåˆ†ç±»ä»»åŠ¡ä¸­æ˜¾è‘—ä¼˜äºç°æœ‰æŠ€æœ¯ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.19777">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-04-03\./crop_Few-Shot/2502.19777v2/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-04-03\./crop_Few-Shot/2502.19777v2/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-04-03\./crop_Few-Shot/2502.19777v2/page_3_0.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="Synthetic-Prior-for-Few-Shot-Drivable-Head-Avatar-Inversion"><a href="#Synthetic-Prior-for-Few-Shot-Drivable-Head-Avatar-Inversion" class="headerlink" title="Synthetic Prior for Few-Shot Drivable Head Avatar Inversion"></a>Synthetic Prior for Few-Shot Drivable Head Avatar Inversion</h2><p><strong>Authors:Wojciech Zielonka, Stephan J. Garbin, Alexandros Lattas, George Kopanas, Paulo Gotardo, Thabo Beeler, Justus Thies, Timo Bolkart</strong></p>
<p>We present SynShot, a novel method for the few-shot inversion of a drivable head avatar based on a synthetic prior. We tackle three major challenges. First, training a controllable 3D generative network requires a large number of diverse sequences, for which pairs of images and high-quality tracked meshes are not always available. Second, the use of real data is strictly regulated (e.g., under the General Data Protection Regulation, which mandates frequent deletion of models and data to accommodate a situation when a participantâ€™s consent is withdrawn). Synthetic data, free from these constraints, is an appealing alternative. Third, state-of-the-art monocular avatar models struggle to generalize to new views and expressions, lacking a strong prior and often overfitting to a specific viewpoint distribution. Inspired by machine learning models trained solely on synthetic data, we propose a method that learns a prior model from a large dataset of synthetic heads with diverse identities, expressions, and viewpoints. With few input images, SynShot fine-tunes the pretrained synthetic prior to bridge the domain gap, modeling a photorealistic head avatar that generalizes to novel expressions and viewpoints. We model the head avatar using 3D Gaussian splatting and a convolutional encoder-decoder that outputs Gaussian parameters in UV texture space. To account for the different modeling complexities over parts of the head (e.g., skin vs hair), we embed the prior with explicit control for upsampling the number of per-part primitives. Compared to SOTA monocular and GAN-based methods, SynShot significantly improves novel view and expression synthesis. </p>
<blockquote>
<p>æˆ‘ä»¬æå‡ºäº†SynShotï¼Œè¿™æ˜¯ä¸€ç§åŸºäºåˆæˆå…ˆéªŒçš„æ–°é¢–çš„å°‘æ ·æœ¬é©±åŠ¨å¤´éƒ¨åŒ–èº«åè½¬æ–¹æ³•ã€‚æˆ‘ä»¬è§£å†³äº†ä¸‰ä¸ªä¸»è¦æŒ‘æˆ˜ã€‚é¦–å…ˆï¼Œè®­ç»ƒå¯æ§çš„3Dç”Ÿæˆç½‘ç»œéœ€è¦å¤§é‡çš„ä¸åŒåºåˆ—ï¼Œè€Œå›¾åƒå’Œé«˜è´¨é‡è·Ÿè¸ªç½‘æ ¼çš„é…å¯¹å¹¶ä¸æ€»æ˜¯å¯ç”¨çš„ã€‚å…¶æ¬¡ï¼ŒçœŸå®æ•°æ®çš„ä½¿ç”¨å—åˆ°ä¸¥æ ¼ç›‘ç®¡ï¼ˆä¾‹å¦‚ï¼Œæ ¹æ®ã€Šé€šç”¨æ•°æ®ä¿æŠ¤æ¡ä¾‹ã€‹ï¼Œå½“å‚ä¸è€…åŒæ„æ’¤å›æ—¶ï¼Œéœ€è¦é¢‘ç¹åˆ é™¤æ¨¡å‹å’Œæ•°æ®è¿›è¡Œé€‚åº”ï¼‰ã€‚ä¸å—è¿™äº›çº¦æŸçš„åˆæˆæ•°æ®æ˜¯ä¸€ä¸ªå¸å¼•äººçš„é€‰æ‹©ã€‚ç¬¬ä¸‰ï¼Œæœ€å…ˆè¿›çš„å•çœ¼åŒ–èº«æ¨¡å‹åœ¨åº”å¯¹æ–°è§†è§’å’Œè¡¨æƒ…æ—¶é¢ä¸´éš¾ä»¥æ¨å¹¿çš„é—®é¢˜ï¼Œç¼ºä¹å¼ºå¤§çš„å…ˆéªŒçŸ¥è¯†å¹¶ä¸”ç»å¸¸è¿‡åº¦é€‚åº”ç‰¹å®šçš„è§†è§’åˆ†å¸ƒã€‚å—ä»…å—åˆæˆæ•°æ®è®­ç»ƒçš„æœºå™¨å­¦ä¹ æ¨¡å‹çš„å¯å‘ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–¹æ³•ï¼Œè¯¥æ–¹æ³•ä»åˆæˆå¤´éƒ¨çš„å¤§æ•°æ®é›†ä¸­å­¦ä¹ å…ˆéªŒæ¨¡å‹ï¼Œè¿™äº›åˆæˆå¤´éƒ¨å…·æœ‰ä¸åŒçš„èº«ä»½ã€è¡¨æƒ…å’Œè§†è§’ã€‚å‡­å€Ÿå°‘é‡çš„è¾“å…¥å›¾åƒï¼ŒSynShotå¾®è°ƒäº†é¢„è®­ç»ƒçš„åˆæˆå…ˆéªŒä»¥å¼¥åˆé¢†åŸŸå·®è·ï¼Œä»è€Œå»ºç«‹ä¸€ä¸ªé€¼çœŸçš„å¤´éƒ¨åŒ–èº«æ¨¡å‹ï¼Œè¯¥æ¨¡å‹èƒ½å¤Ÿæ¨å¹¿åˆ°æ–°çš„è¡¨æƒ…å’Œè§†è§’ã€‚æˆ‘ä»¬ä½¿ç”¨ä¸‰ç»´é«˜æ–¯æ¶‚ç‰‡å’Œå·ç§¯ç¼–ç å™¨è§£ç å™¨å»ºç«‹å¤´éƒ¨åŒ–èº«æ¨¡å‹ï¼Œè¯¥ç¼–ç å™¨è§£ç å™¨ä»¥UVçº¹ç†ç©ºé—´è¾“å‡ºé«˜æ–¯å‚æ•°ã€‚è€ƒè™‘åˆ°å¤´éƒ¨å„éƒ¨ä½å»ºæ¨¡å¤æ‚æ€§çš„å·®å¼‚ï¼ˆä¾‹å¦‚çš®è‚¤å’Œå¤´å‘ï¼‰ï¼Œæˆ‘ä»¬å°†å…ˆéªŒä¸ç”¨äºå¢åŠ æ¯ä¸ªéƒ¨ä½åŸå§‹æ•°é‡çš„ä¸Šé‡‡æ ·æ˜¾å¼æ§åˆ¶ç›¸ç»“åˆã€‚ä¸æœ€å…ˆè¿›çš„å•çœ¼å’ŒåŸºäºGANçš„æ–¹æ³•ç›¸æ¯”ï¼ŒSynShotåœ¨æ–°å‹è§†è§’å’Œè¡¨æƒ…åˆæˆæ–¹é¢æ˜¾è‘—æé«˜äº†æ€§èƒ½ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.06903v3">PDF</a> Accepted to CVPR25 Website: <a target="_blank" rel="noopener" href="https://zielon.github.io/synshot/">https://zielon.github.io/synshot/</a></p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†åŸºäºåˆæˆå…ˆéªŒçš„SynShotæ–¹æ³•ï¼Œç”¨äºåŸºäºå°‘æ•°å›¾åƒè¿›è¡Œå¯é©¾é©¶å¤´éƒ¨è§’è‰²çš„åå‘æ“ä½œã€‚è§£å†³äº†ä¸‰ä¸ªä¸»è¦é—®é¢˜ï¼šè®­ç»ƒå¯æ§çš„3Dç”Ÿæˆç½‘ç»œéœ€è¦å¤§é‡å¤šæ ·åºåˆ—çš„æ•°æ®ï¼Œä½†çœŸå®æ•°æ®å—é™ä¸”å—åˆ°ä¸¥æ ¼ç›‘ç®¡ï¼›å½“å‰ä¸»æµçš„å•çœ¼å¤´åƒæ¨¡å‹éš¾ä»¥æ³›åŒ–åˆ°æ–°çš„è§†è§’å’Œè¡¨æƒ…ï¼›ä»¥åŠé€šè¿‡åˆ©ç”¨åˆæˆæ•°æ®çš„å…ˆéªŒæ¨¡å‹è¿›è¡Œå¡«å……å…‹æœä»¥ä¸ŠæŒ‘æˆ˜ï¼Œå°†å¤´éƒ¨è§’è‰²è¿›è¡Œå»ºæ¨¡å¹¶å®ç°é«˜è´¨é‡çš„é¢éƒ¨å’Œçº¹ç†è¡¨ç°ã€‚åœ¨åªæœ‰å°‘æ•°å›¾åƒçš„æƒ…å†µä¸‹ï¼ŒSynShotæ–¹æ³•å¯å¯¹é¢„è®­ç»ƒçš„åˆæˆå…ˆéªŒè¿›è¡Œå¾®è°ƒä»¥å¼¥è¡¥é¢†åŸŸé—´çš„å·®è·ï¼ŒåŒæ—¶èƒ½é€‚åº”ä¸åŒè§†ç‚¹è¡¨è¾¾å’Œå§¿æ€åŠ¨ä½œåœºæ™¯ç”Ÿæˆè™šæ‹Ÿå¤´éƒ¨æ¨¡å‹çš„æ–°è§†å›¾ä¸è¡¨è¾¾åˆæˆã€‚è¿™ç§æ–¹æ³•åˆ©ç”¨UVçº¹ç†ç©ºé—´ä¸­çš„å·ç§¯ç¼–ç å™¨è§£ç å™¨æ¥ç”Ÿæˆæ¨¡å‹ï¼Œå¹¶åˆ©ç”¨åˆæˆæ•°æ®çš„ä¼˜åŠ¿å¯¹å¤´éƒ¨è¿›è¡Œå»ºæ¨¡ä¸å±€éƒ¨å»ºæ¨¡è°ƒæ•´æ¥å®ç°ç‰¹æ•ˆå¤´éƒ¨å½¢è±¡çš„å‡†ç¡®å†ç°å’Œæ›´åŠ å¹¿æ³›çš„ç°å®è¡¨ç°åŠ›ä¸è‡ªç”±å‘æŒ¥æƒ³è±¡èƒ½åŠ›çš„é¢†åŸŸåº”ç”¨ã€‚ç›¸è¾ƒäºå…¶ä»–å•çœ¼å’ŒåŸºäºGANçš„æ–¹æ³•ï¼ŒSynShotæ˜¾è‘—æé«˜äº†æ–°è§†è§’å’Œè¡¨æƒ…åˆæˆçš„è´¨é‡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>SynShotæ˜¯ä¸€ç§ç”¨äºé©±åŠ¨å¤´éƒ¨è§’è‰²çš„æ–°é¢–æ–¹æ³•ï¼Œå®ƒè§£å†³äº†å½“å‰é¢ä¸´çš„æ•°æ®ç¨€ç¼ºé—®é¢˜ä»¥åŠæ¨¡å‹çš„æ³›åŒ–é—®é¢˜ã€‚</li>
<li>è¯¥æ–¹æ³•åŸºäºåˆæˆå…ˆéªŒï¼Œè§£å†³äº†çœŸå®æ•°æ®è·å–å›°éš¾çš„é—®é¢˜ï¼Œå¹¶é¿å…äº†çœŸå®æ•°æ®ä½¿ç”¨ä¸­çš„æ³•è§„é™åˆ¶ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.06903">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-04-03\./crop_Few-Shot/2501.06903v3/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-04-03\./crop_Few-Shot/2501.06903v3/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-04-03\./crop_Few-Shot/2501.06903v3/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-04-03\./crop_Few-Shot/2501.06903v3/page_4_1.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-04-03\./crop_Few-Shot/2501.06903v3/page_5_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-04-03\./crop_Few-Shot/2501.06903v3/page_5_1.jpg" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="COBRA-COmBinatorial-Retrieval-Augmentation-for-Few-Shot-Adaptation"><a href="#COBRA-COmBinatorial-Retrieval-Augmentation-for-Few-Shot-Adaptation" class="headerlink" title="COBRA: COmBinatorial Retrieval Augmentation for Few-Shot Adaptation"></a>COBRA: COmBinatorial Retrieval Augmentation for Few-Shot Adaptation</h2><p><strong>Authors:Arnav M. Das, Gantavya Bhatt, Lilly Kumari, Sahil Verma, Jeff Bilmes</strong></p>
<p>Retrieval augmentation, the practice of retrieving additional data from large auxiliary pools, has emerged as an effective technique for enhancing model performance in the low-data regime. Prior approaches have employed only nearest-neighbor based strategies for data selection, which retrieve auxiliary samples with high similarity to instances in the target task. However, these approaches are prone to selecting highly redundant samples, since they fail to incorporate any notion of diversity. In our work, we first demonstrate that data selection strategies used in prior retrieval-augmented few-shot adaptation settings can be generalized using a class of functions known as Combinatorial Mutual Information (CMI) measures. We then propose COBRA (COmBinatorial Retrieval Augmentation), which employs an alternative CMI measure that considers both diversity and similarity to a target dataset. COBRA consistently outperforms previous retrieval approaches across image classification tasks and few-shot learning techniques when used to retrieve samples from LAION-2B. COBRA introduces negligible computational overhead to the cost of retrieval while providing significant gains in downstream model performance. </p>
<blockquote>
<p>æ£€ç´¢å¢å¼ºæ˜¯é€šè¿‡ä»å¤§å‹è¾…åŠ©æ± ä¸­æ£€ç´¢é¢å¤–æ•°æ®çš„ä¸€ç§å®è·µï¼Œå·²æˆä¸ºåœ¨ä½æ•°æ®æƒ…å†µä¸‹æé«˜æ¨¡å‹æ€§èƒ½çš„æœ‰æ•ˆæŠ€æœ¯ã€‚ä¹‹å‰çš„æ–¹æ³•ä»…é‡‡ç”¨åŸºäºæœ€è¿‘é‚»çš„ç­–ç•¥è¿›è¡Œæ•°æ®é€‰æ‹©ï¼Œæ£€ç´¢ä¸ç›®æ ‡ä»»åŠ¡å®ä¾‹é«˜åº¦ç›¸ä¼¼çš„è¾…åŠ©æ ·æœ¬ã€‚ç„¶è€Œï¼Œè¿™äº›æ–¹æ³•å®¹æ˜“é€‰æ‹©é«˜åº¦å†—ä½™çš„æ ·æœ¬ï¼Œå› ä¸ºå®ƒä»¬æ²¡æœ‰èå…¥ä»»ä½•å¤šæ ·æ€§çš„æ¦‚å¿µã€‚åœ¨æˆ‘ä»¬çš„å·¥ä½œä¸­ï¼Œæˆ‘ä»¬é¦–å…ˆè¯æ˜ï¼Œå…ˆå‰ç”¨äºå¢å¼ºå°‘æ•°é•œå¤´é€‚åº”çš„æ£€ç´¢æ•°æ®é€‰æ‹©ç­–ç•¥å¯ä»¥ä½¿ç”¨ç§°ä¸ºç»„åˆäº’ä¿¡æ¯ï¼ˆCMIï¼‰åº¦é‡çš„ä¸€ç±»å‡½æ•°è¿›è¡Œæ¦‚æ‹¬ã€‚ç„¶åï¼Œæˆ‘ä»¬æå‡ºäº†COBRAï¼ˆç»„åˆæ£€ç´¢å¢å¼ºï¼‰ï¼Œå®ƒé‡‡ç”¨ä¸€ç§æ›¿ä»£çš„CMIåº¦é‡ï¼ŒåŒæ—¶è€ƒè™‘å¤šæ ·æ€§å’Œå¯¹ç›®æ ‡æ•°æ®é›†çš„ç›¸ä¼¼æ€§ã€‚åœ¨LAION-2Bæ£€ç´¢æ ·æœ¬è¿›è¡Œå›¾åƒåˆ†ç±»ä»»åŠ¡å’Œå°‘é‡å­¦ä¹ ä»»åŠ¡æ—¶ï¼ŒCOBRAå§‹ç»ˆä¼˜äºä¹‹å‰çš„æ£€ç´¢æ–¹æ³•ã€‚COBRAåœ¨æ£€ç´¢è¿‡ç¨‹ä¸­å¢åŠ äº†å¯å¿½ç•¥çš„è®¡ç®—å¼€é”€ï¼ŒåŒæ—¶ä¸ºä¸‹æ¸¸æ¨¡å‹æ€§èƒ½æä¾›äº†æ˜¾è‘—çš„æ”¹è¿›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.17684v2">PDF</a> Accepted at CVPR 2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†æ•°æ®æ£€ç´¢å¢å¼ºæŠ€æœ¯åœ¨ä½æ•°æ®ç¯å¢ƒä¸‹çš„åº”ç”¨ï¼Œé€šè¿‡ä»å¤§å‹è¾…åŠ©æ± ä¸­æ£€ç´¢é¢å¤–æ•°æ®æ¥æé«˜æ¨¡å‹æ€§èƒ½ã€‚å…ˆå‰çš„æ–¹æ³•ä»…ä½¿ç”¨åŸºäºæœ€è¿‘é‚»çš„ç­–ç•¥è¿›è¡Œæ•°æ®é€‰æ‹©ï¼Œå®¹æ˜“é€‰å–å†—ä½™æ ·æœ¬ï¼Œæœªè€ƒè™‘å¤šæ ·æ€§ã€‚æœ¬æ–‡æå‡ºäº†ä½¿ç”¨ç»„åˆäº’ä¿¡æ¯ï¼ˆCMIï¼‰åº¦é‡çš„æ•°æ®é€‰æ‹©ç­–ç•¥ï¼Œå¹¶è®¾è®¡äº†COBRAæ–¹æ³•ï¼Œç»“åˆå¤šæ ·æ€§å’Œç›¸ä¼¼æ€§è¿›è¡Œç›®æ ‡æ•°æ®é›†æ£€ç´¢ã€‚åœ¨å›¾åƒåˆ†ç±»ä»»åŠ¡å’Œå°‘é‡å­¦ä¹ ä»»åŠ¡ä¸­ï¼ŒCOBRAä»LAION-2Bä¸­æ£€ç´¢æ ·æœ¬çš„æ€§èƒ½ä¼˜äºä»¥å‰çš„æ–¹æ³•ï¼ŒåŒæ—¶å¯¹æ£€ç´¢çš„è®¡ç®—å¼€é”€å½±å“è¾ƒå°ï¼Œå¤§å¤§æé«˜äº†ä¸‹æ¸¸æ¨¡å‹çš„æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ£€ç´¢å¢å¼ºæŠ€æœ¯é€šè¿‡ä»å¤§å‹è¾…åŠ©æ± ä¸­æ£€ç´¢é¢å¤–æ•°æ®åœ¨ä½æ•°æ®ç¯å¢ƒä¸‹æé«˜æ¨¡å‹æ€§èƒ½ã€‚</li>
<li>ä»¥å¾€æ–¹æ³•ä½¿ç”¨åŸºäºæœ€è¿‘é‚»çš„ç­–ç•¥è¿›è¡Œæ•°æ®é€‰æ‹©ï¼Œå­˜åœ¨é€‰å–å†—ä½™æ ·æœ¬çš„é—®é¢˜ã€‚</li>
<li>æœ¬æ–‡ä½¿ç”¨ç»„åˆäº’ä¿¡æ¯ï¼ˆCMIï¼‰åº¦é‡æ¥æ”¹è¿›æ•°æ®é€‰æ‹©ç­–ç•¥ã€‚</li>
<li>COBRAæ–¹æ³•ç»“åˆäº†å¤šæ ·æ€§å’Œç›¸ä¼¼æ€§è¿›è¡Œç›®æ ‡æ•°æ®é›†æ£€ç´¢ã€‚</li>
<li>COBRAåœ¨å›¾åƒåˆ†ç±»ä»»åŠ¡å’Œå°‘é‡å­¦ä¹ ä»»åŠ¡ä¸­çš„æ€§èƒ½ä¼˜äºå…ˆå‰çš„æ–¹æ³•ã€‚</li>
<li>COBRAå¯¹æ£€ç´¢çš„è®¡ç®—å¼€é”€å½±å“è¾ƒå°ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.17684">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-04-03\./crop_Few-Shot/2412.17684v2/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-04-03\./crop_Few-Shot/2412.17684v2/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-04-03\./crop_Few-Shot/2412.17684v2/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-04-03\./crop_Few-Shot/2412.17684v2/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-04-03\./crop_Few-Shot/2412.17684v2/page_5_0.jpg" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="MVREC-A-General-Few-shot-Defect-Classification-Model-Using-Multi-View-Region-Context"><a href="#MVREC-A-General-Few-shot-Defect-Classification-Model-Using-Multi-View-Region-Context" class="headerlink" title="MVREC: A General Few-shot Defect Classification Model Using Multi-View   Region-Context"></a>MVREC: A General Few-shot Defect Classification Model Using Multi-View   Region-Context</h2><p><strong>Authors:Shuai Lyu, Rongchen Zhang, Zeqi Ma, Fangjian Liao, Dongmei Mo, Waikeung Wong</strong></p>
<p>Few-shot defect multi-classification (FSDMC) is an emerging trend in quality control within industrial manufacturing. However, current FSDMC research often lacks generalizability due to its focus on specific datasets. Additionally, defect classification heavily relies on contextual information within images, and existing methods fall short of effectively extracting this information. To address these challenges, we propose a general FSDMC framework called MVREC, which offers two primary advantages: (1) MVREC extracts general features for defect instances by incorporating the pre-trained AlphaCLIP model. (2) It utilizes a region-context framework to enhance defect features by leveraging mask region input and multi-view context augmentation. Furthermore, Few-shot Zip-Adapter(-F) classifiers within the model are introduced to cache the visual features of the support set and perform few-shot classification. We also introduce MVTec-FS, a new FSDMC benchmark based on MVTec AD, which includes 1228 defect images with instance-level mask annotations and 46 defect types. Extensive experiments conducted on MVTec-FS and four additional datasets demonstrate its effectiveness in general defect classification and its ability to incorporate contextual information to improve classification performance. Code: <a target="_blank" rel="noopener" href="https://github.com/ShuaiLYU/MVREC">https://github.com/ShuaiLYU/MVREC</a> </p>
<blockquote>
<p>å°‘æ ·æœ¬ç¼ºé™·å¤šåˆ†ç±»ï¼ˆFSDMCï¼‰æ˜¯å·¥ä¸šåˆ¶é€ ä¸­è´¨é‡æ§åˆ¶çš„æ–°å…´è¶‹åŠ¿ã€‚ç„¶è€Œï¼Œå½“å‰çš„FSDMCç ”ç©¶å¾€å¾€ç”±äºç¼ºä¹æ™®éæ€§è€Œä¸“æ³¨äºç‰¹å®šæ•°æ®é›†ã€‚æ­¤å¤–ï¼Œç¼ºé™·åˆ†ç±»ä¸¥é‡ä¾èµ–äºå›¾åƒä¸­çš„ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼Œè€Œç°æœ‰æ–¹æ³•ä¸è¶³ä»¥æœ‰æ•ˆåœ°æå–è¿™äº›ä¿¡æ¯ã€‚ä¸ºäº†è§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§é€šç”¨çš„FSDMCæ¡†æ¶ï¼Œåä¸ºMVRECï¼Œå®ƒæœ‰ä¸¤ä¸ªä¸»è¦ä¼˜ç‚¹ï¼šï¼ˆ1ï¼‰MVRECé€šè¿‡èå…¥é¢„è®­ç»ƒçš„AlphaCLIPæ¨¡å‹ï¼Œæå–ç¼ºé™·å®ä¾‹çš„é€šç”¨ç‰¹å¾ã€‚ï¼ˆ2ï¼‰å®ƒåˆ©ç”¨åŒºåŸŸä¸Šä¸‹æ–‡æ¡†æ¶ï¼Œé€šè¿‡åˆ©ç”¨æ©è†œåŒºåŸŸè¾“å…¥å’Œå¤šè§†å›¾ä¸Šä¸‹æ–‡å¢å¼ºæ¥å¢å¼ºç¼ºé™·ç‰¹å¾ã€‚æ­¤å¤–ï¼Œæ¨¡å‹ä¸­å¼•å…¥äº†å°‘æ ·æœ¬Zip-Adapterï¼ˆ-Fï¼‰åˆ†ç±»å™¨ï¼Œä»¥ç¼“å­˜æ”¯æŒé›†çš„è§†è§‰ç‰¹å¾å¹¶æ‰§è¡Œå°‘æ ·æœ¬åˆ†ç±»ã€‚æˆ‘ä»¬è¿˜åŸºäºMVTec ADå¼•å…¥äº†æ–°çš„FSDMCåŸºå‡†æµ‹è¯•MVTec-FSï¼Œå…¶ä¸­åŒ…æ‹¬å…·æœ‰å®ä¾‹çº§æ©è†œæ³¨é‡Šçš„1228ä¸ªç¼ºé™·å›¾åƒå’Œ46ç§ç¼ºé™·ç±»å‹ã€‚åœ¨MVTec-FSå’Œå¦å¤–å››ä¸ªæ•°æ®é›†ä¸Šè¿›è¡Œçš„å¹¿æ³›å®éªŒè¯æ˜äº†å…¶åœ¨ä¸€èˆ¬ç¼ºé™·åˆ†ç±»ä¸­çš„æœ‰æ•ˆæ€§ä»¥åŠç»“åˆä¸Šä¸‹æ–‡ä¿¡æ¯æé«˜åˆ†ç±»æ€§èƒ½çš„èƒ½åŠ›ã€‚ä»£ç ï¼š<a target="_blank" rel="noopener" href="https://github.com/ShuaiLYU/MVREC">https://github.com/ShuaiLYU/MVREC</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.16897v2">PDF</a> Accepted by AAAI 2025</p>
<p><strong>Summary</strong></p>
<p>è¯¥æ‘˜è¦é’ˆå¯¹å·¥ä¸šåˆ¶é€ ä¸­çš„è´¨é‡æ§åˆ¶é¢†åŸŸçš„æ–°å…´è¶‹åŠ¿â€”â€”å°‘æ ·æœ¬ç¼ºé™·å¤šåˆ†ç±»é—®é¢˜ï¼ˆFSDMCï¼‰å±•å¼€ã€‚å½“å‰ç ”ç©¶å› è¿‡äºä¾èµ–ç‰¹å®šæ•°æ®é›†è€Œç¼ºä¹æ³›åŒ–èƒ½åŠ›ã€‚ä¸ºè§£å†³æ­¤é—®é¢˜ï¼Œæå‡ºä¸€ç§åä¸ºMVRECçš„é€šç”¨FSDMCæ¡†æ¶ï¼Œå…¶åˆ©ç”¨é¢„è®­ç»ƒçš„AlphaCLIPæ¨¡å‹æå–ç¼ºé™·å®ä¾‹çš„ä¸€èˆ¬ç‰¹å¾ï¼Œå¹¶é‡‡ç”¨åŒºåŸŸä¸Šä¸‹æ–‡æ¡†æ¶å¢å¼ºç¼ºé™·ç‰¹å¾ï¼Œé€šè¿‡åˆ©ç”¨æ©æ¨¡åŒºåŸŸè¾“å…¥å’Œå¤šè§†å›¾ä¸Šä¸‹æ–‡å¢å¼ºå®ç°ã€‚æ­¤å¤–ï¼Œå¼•å…¥å°‘æ ·æœ¬Zip-Adapterï¼ˆ-Fï¼‰åˆ†ç±»å™¨ä»¥ç¼“å­˜æ”¯æŒé›†çš„è§†è§‰ç‰¹å¾å¹¶æ‰§è¡Œå°‘æ ·æœ¬åˆ†ç±»ã€‚åŒæ—¶ä»‹ç»åŸºäºMVTec ADçš„æ–°FSDMCåŸºå‡†æ•°æ®é›†MVTec-FSï¼ŒåŒ…å«å¸¦æœ‰å®ä¾‹çº§æ©æ¨¡æ³¨é‡Šçš„1228ä¸ªç¼ºé™·å›¾åƒå’Œ46ç§ç¼ºé™·ç±»å‹ã€‚åœ¨MVTec-FSå’Œå…¶ä»–å››ä¸ªæ•°æ®é›†ä¸Šçš„å¤§é‡å®éªŒè¯æ˜äº†å…¶åœ¨é€šç”¨ç¼ºé™·åˆ†ç±»ä¸­çš„æœ‰æ•ˆæ€§ä»¥åŠåˆ©ç”¨ä¸Šä¸‹æ–‡ä¿¡æ¯æé«˜åˆ†ç±»æ€§èƒ½çš„èƒ½åŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>FSDMCæ˜¯å½“å‰å·¥ä¸šåˆ¶é€ ä¸­è´¨é‡æ§åˆ¶çš„é‡è¦ç ”ç©¶é¢†åŸŸï¼Œä½†ç°æœ‰æ–¹æ³•å› æ•°æ®é›†å±€é™æ€§å¯¼è‡´æ³›åŒ–èƒ½åŠ›å¼±ã€‚</li>
<li>MVRECæ¡†æ¶åˆ©ç”¨é¢„è®­ç»ƒçš„AlphaCLIPæ¨¡å‹æå–ç¼ºé™·å®ä¾‹çš„ä¸€èˆ¬ç‰¹å¾ï¼Œå¢å¼ºäº†æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ã€‚</li>
<li>MVRECé‡‡ç”¨åŒºåŸŸä¸Šä¸‹æ–‡æ¡†æ¶ï¼Œé€šè¿‡æ©æ¨¡åŒºåŸŸè¾“å…¥å’Œå¤šè§†å›¾ä¸Šä¸‹æ–‡å¢å¼ºï¼Œæœ‰æ•ˆå¢å¼ºç¼ºé™·ç‰¹å¾ã€‚</li>
<li>å¼•å…¥å°‘æ ·æœ¬Zip-Adapterï¼ˆ-Fï¼‰åˆ†ç±»å™¨ä»¥ç¼“å­˜æ”¯æŒé›†çš„è§†è§‰ç‰¹å¾å¹¶æ‰§è¡Œå°‘æ ·æœ¬åˆ†ç±»ä»»åŠ¡ã€‚</li>
<li>MVTec-FSæ˜¯ä¸€ä¸ªåŸºäºMVTec ADçš„æ–°FSDMCåŸºå‡†æ•°æ®é›†ï¼ŒåŒ…å«å¤§é‡å¸¦æœ‰å®ä¾‹çº§æ©æ¨¡æ³¨é‡Šçš„ç¼ºé™·å›¾åƒå’Œå¤šç§ç¼ºé™·ç±»å‹ã€‚</li>
<li>åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šçš„å®éªŒè¯æ˜äº†MVRECæ¡†æ¶åœ¨é€šç”¨ç¼ºé™·åˆ†ç±»ä¸­çš„æœ‰æ•ˆæ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.16897">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-04-03\./crop_Few-Shot/2412.16897v2/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-04-03\./crop_Few-Shot/2412.16897v2/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-04-03\./crop_Few-Shot/2412.16897v2/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-04-03\./crop_Few-Shot/2412.16897v2/page_5_0.jpg" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1><h2 id="PICLe-Pseudo-Annotations-for-In-Context-Learning-in-Low-Resource-Named-Entity-Detection"><a href="#PICLe-Pseudo-Annotations-for-In-Context-Learning-in-Low-Resource-Named-Entity-Detection" class="headerlink" title="PICLe: Pseudo-Annotations for In-Context Learning in Low-Resource Named   Entity Detection"></a>PICLe: Pseudo-Annotations for In-Context Learning in Low-Resource Named   Entity Detection</h2><p><strong>Authors:Sepideh Mamooler, Syrielle Montariol, Alexander Mathis, Antoine Bosselut</strong></p>
<p>In-context learning (ICL) enables Large Language Models (LLMs) to perform tasks using few demonstrations, facilitating task adaptation when labeled examples are hard to obtain. However, ICL is sensitive to the choice of demonstrations, and it remains unclear which demonstration attributes enable in-context generalization. In this work, we conduct a perturbation study of in-context demonstrations for low-resource Named Entity Detection (NED). Our surprising finding is that in-context demonstrations with partially correct annotated entity mentions can be as effective for task transfer as fully correct demonstrations. Based off our findings, we propose Pseudo-annotated In-Context Learning (PICLe), a framework for in-context learning with noisy, pseudo-annotated demonstrations. PICLe leverages LLMs to annotate many demonstrations in a zero-shot first pass. We then cluster these synthetic demonstrations, sample specific sets of in-context demonstrations from each cluster, and predict entity mentions using each set independently. Finally, we use self-verification to select the final set of entity mentions. We evaluate PICLe on five biomedical NED datasets and show that, with zero human annotation, PICLe outperforms ICL in low-resource settings where limited gold examples can be used as in-context demonstrations. </p>
<blockquote>
<p>ä¸Šä¸‹æ–‡å­¦ä¹ ï¼ˆICLï¼‰ä½¿å¾—å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰èƒ½å¤Ÿåœ¨å°‘é‡æ¼”ç¤ºçš„æƒ…å†µä¸‹å®Œæˆä»»åŠ¡ï¼Œåœ¨éš¾ä»¥è·å–æ ‡æ³¨æ ·æœ¬æ—¶ä¿ƒè¿›ä»»åŠ¡é€‚åº”ã€‚ç„¶è€Œï¼ŒICLå¯¹æ¼”ç¤ºçš„é€‰æ‹©å¾ˆæ•æ„Ÿï¼Œå°šä¸æ¸…æ¥šå“ªäº›æ¼”ç¤ºå±æ€§èƒ½å¤Ÿå®ç°ä¸Šä¸‹æ–‡æ³›åŒ–ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬å¯¹ä½èµ„æºå‘½åå®ä½“æ£€æµ‹ï¼ˆNEDï¼‰çš„ä¸Šä¸‹æ–‡æ¼”ç¤ºè¿›è¡Œæ‰°åŠ¨ç ”ç©¶ã€‚æˆ‘ä»¬æ„å¤–åœ°å‘ç°ï¼Œå¸¦æœ‰éƒ¨åˆ†æ­£ç¡®æ³¨é‡Šçš„å®ä½“æåŠçš„ä¸Šä¸‹æ–‡æ¼”ç¤ºä¸å®Œå…¨æ­£ç¡®çš„æ¼”ç¤ºä¸€æ ·æœ‰æ•ˆï¼Œéƒ½èƒ½å®ç°ä»»åŠ¡è¿ç§»ã€‚åŸºäºæˆ‘ä»¬çš„å‘ç°ï¼Œæˆ‘ä»¬æå‡ºäº†ä¼ªæ³¨é‡Šä¸Šä¸‹æ–‡å­¦ä¹ ï¼ˆPICLeï¼‰æ¡†æ¶ï¼Œè¯¥æ¡†æ¶ä½¿ç”¨å¸¦æœ‰å™ªå£°çš„ä¼ªæ³¨é‡Šæ¼”ç¤ºè¿›è¡Œä¸Šä¸‹æ–‡å­¦ä¹ ã€‚PICLeé¦–å…ˆåˆ©ç”¨LLMåœ¨é›¶æ ·æœ¬é¦–æ¬¡é€šè¿‡ä¸­æ³¨é‡Šè®¸å¤šæ¼”ç¤ºã€‚ç„¶åæˆ‘ä»¬å¯¹è¿™äº›åˆæˆæ¼”ç¤ºè¿›è¡Œèšç±»ï¼Œä»æ¯ä¸ªé›†ç¾¤ä¸­æŠ½å–ç‰¹å®šçš„ä¸Šä¸‹æ–‡æ¼”ç¤ºé›†ï¼Œå¹¶ç‹¬ç«‹ä½¿ç”¨æ¯ä¸ªé›†åˆé¢„æµ‹å®ä½“æåŠã€‚æœ€åï¼Œæˆ‘ä»¬ä½¿ç”¨è‡ªæˆ‘éªŒè¯é€‰æ‹©æœ€ç»ˆçš„å®ä½“æåŠé›†åˆã€‚æˆ‘ä»¬åœ¨äº”ä¸ªç”Ÿç‰©åŒ»å­¦NEDæ•°æ®é›†ä¸Šè¯„ä¼°PICLeï¼Œç»“æœè¡¨æ˜ï¼Œåœ¨æ— éœ€äººå·¥æ ‡æ³¨çš„æƒ…å†µä¸‹ï¼ŒPICLeåœ¨ä½èµ„æºç¯å¢ƒä¸­ä¼˜äºICLï¼Œåœ¨é‚£é‡Œå¯ä»¥ä½¿ç”¨æœ‰é™æ•°é‡çš„é»„é‡‘æ ·æœ¬ä½œä¸ºä¸Šä¸‹æ–‡æ¼”ç¤ºã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.11923v2">PDF</a> In Proceedings of NAACL2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬è®ºæ–‡ç ”ç©¶äº†åŸºäºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„ä¸Šä¸‹æ–‡å­¦ä¹ ï¼ˆICLï¼‰åœ¨å‘½åå®ä½“æ£€æµ‹ï¼ˆNEDï¼‰ä»»åŠ¡ä¸­çš„è¡¨ç°ã€‚é€šè¿‡æ‰°åŠ¨åˆ†æå‘ç°ï¼Œéƒ¨åˆ†æ­£ç¡®æ ‡æ³¨çš„å®ä½“æåŠçš„ä¸Šä¸‹æ–‡æ¼”ç¤ºä¸å®Œå…¨æ­£ç¡®çš„æ¼”ç¤ºåœ¨ä»»åŠ¡è¿ç§»ä¸­å…·æœ‰ç›¸åŒçš„æ•ˆæœã€‚åŸºäºæ­¤å‘ç°ï¼Œè®ºæ–‡æå‡ºäº†ä¼ªæ ‡æ³¨ä¸Šä¸‹æ–‡å­¦ä¹ ï¼ˆPICLeï¼‰æ¡†æ¶ï¼Œè¯¥æ¡†æ¶åˆ©ç”¨LLMè¿›è¡Œé›¶æ ·æœ¬é¦–æ¬¡æ ‡æ³¨å¤§é‡æ¼”ç¤ºå†…å®¹ï¼Œå¹¶å°†å…¶èšç±»ã€‚ä»æ¯ä¸ªé›†ç¾¤ä¸­é‡‡æ ·ç‰¹å®šçš„ä¸Šä¸‹æ–‡æ¼”ç¤ºé›†ï¼Œç‹¬ç«‹é¢„æµ‹å®ä½“æåŠï¼Œå¹¶ä½¿ç”¨è‡ªæˆ‘éªŒè¯é€‰æ‹©æœ€ç»ˆçš„å®ä½“æåŠé›†ã€‚åœ¨äº”ä¸ªç”Ÿç‰©åŒ»å­¦NEDæ•°æ®é›†ä¸Šçš„è¯„ä¼°ç»“æœè¡¨æ˜ï¼Œåœ¨æ— éœ€äººå·¥æ ‡æ³¨çš„æƒ…å†µä¸‹ï¼ŒPICLeåœ¨ä½èµ„æºè®¾ç½®ä¸­çš„è¡¨ç°ä¼˜äºICLã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ICLï¼ˆä¸Šä¸‹æ–‡å­¦ä¹ ï¼‰åœ¨å‘½åå®ä½“æ£€æµ‹ä»»åŠ¡ä¸­ï¼Œå¯ä»¥åˆ©ç”¨å°‘é‡æ¼”ç¤ºå®Œæˆä»»åŠ¡é€‚åº”ï¼Œå½“éš¾ä»¥è·å–æ ‡è®°ç¤ºä¾‹æ—¶å°¤å…¶æœ‰ç”¨ã€‚</li>
<li>ICLå¯¹æ¼”ç¤ºçš„é€‰æ‹©å¾ˆæ•æ„Ÿï¼Œå°šä¸æ¸…æ¥šå“ªäº›æ¼”ç¤ºå±æ€§æœ‰åŠ©äºä¸Šä¸‹æ–‡æ³›åŒ–ã€‚</li>
<li>ç ”ç©¶å‘ç°ï¼Œéƒ¨åˆ†æ­£ç¡®æ ‡æ³¨çš„å®ä½“æåŠçš„æ¼”ç¤ºä¸å®Œå…¨æ­£ç¡®çš„æ¼”ç¤ºåœ¨ä»»åŠ¡è¿ç§»ä¸­å…·æœ‰ç›¸ä¼¼æ•ˆæœã€‚</li>
<li>åŸºäºè¿™ä¸€å‘ç°ï¼Œæå‡ºäº†PICLeï¼ˆä¼ªæ ‡æ³¨ä¸Šä¸‹æ–‡å­¦ä¹ ï¼‰æ¡†æ¶ï¼Œè¯¥æ¡†æ¶åˆ©ç”¨LLMè¿›è¡Œé›¶æ ·æœ¬é¦–æ¬¡æ ‡æ³¨å¤§é‡æ¼”ç¤ºå†…å®¹ã€‚</li>
<li>PICLeé€šè¿‡å°†åˆæˆæ¼”ç¤ºå†…å®¹èšç±»ï¼Œä»æ¯ä¸ªé›†ç¾¤ä¸­é‡‡æ ·ç‰¹å®šçš„ä¸Šä¸‹æ–‡æ¼”ç¤ºé›†æ¥é¢„æµ‹å®ä½“æåŠã€‚</li>
<li>ä½¿ç”¨è‡ªæˆ‘éªŒè¯é€‰æ‹©æœ€ç»ˆçš„å®ä½“æåŠé›†ï¼Œæé«˜äº†å‡†ç¡®æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.11923">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-04-03\./crop_Few-Shot/2412.11923v2/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-04-03\./crop_Few-Shot/2412.11923v2/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-04-03\./crop_Few-Shot/2412.11923v2/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-04-03\./crop_Few-Shot/2412.11923v2/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-04-03\./crop_Few-Shot/2412.11923v2/page_5_0.jpg" align="middle">
</details>


<h1 id="-19"><a href="#-19" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-04-03/Few-Shot/">https://kedreamix.github.io/Talk2Paper/Paper/2025-04-03/Few-Shot/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Few-Shot/">
                                    <span class="chip bg-color">Few-Shot</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-04-03/I2I%20Translation/">
                    <div class="card-image">
                        
                        <img src="https://pica.zhimg.com/v2-56ba19ee851fd0b044661cc31ed9a0b2.jpg" class="responsive-img" alt="I2I Translation">
                        
                        <span class="card-title">I2I Translation</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            I2I Translation æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-04-04  Leveraging Generalizability of Image-to-Image Translation for Enhanced   Adversarial Defense
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-04-04
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/I2I-Translation/" class="post-category">
                                    I2I Translation
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/I2I-Translation/">
                        <span class="chip bg-color">I2I Translation</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-04-03/MMT/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-4eb529541b04553defce20fdfac56c89.jpg" class="responsive-img" alt="MMT">
                        
                        <span class="card-title">MMT</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            MMT æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-04-03  Style transfer between Microscopy and Magnetic Resonance Imaging via   Generative Adversarial Network in small sample size settings
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-04-03
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/MMT/" class="post-category">
                                    MMT
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/MMT/">
                        <span class="chip bg-color">MMT</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">14643.7k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
