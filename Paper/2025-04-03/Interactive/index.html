<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Interactive">
    <meta name="description" content="Interactive 方向最新论文已更新，请持续关注 Update in 2025-04-04  Are you really listening? Boosting Perceptual Awareness in Music-QA   Benchmarks">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Interactive | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loader页面消失采用渐隐的方式*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logo出现动画 */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//使用渐隐的方法淡出loading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//强制显示loading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">嘘~  正在从服务器偷取页面 . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>首页</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>标签</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>分类</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>归档</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="搜索" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="深色/浅色模式" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			首页
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			标签
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			分类
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			归档
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-f576e97911c89bcec3f541befb9f41f6.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Interactive</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- 文章内容详情 -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Interactive/">
                                <span class="chip bg-color">Interactive</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Interactive/" class="post-category">
                                Interactive
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp;
                    2025-04-04
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>更新日期:&nbsp;&nbsp;
                    2025-05-14
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>文章字数:&nbsp;&nbsp;
                    7.5k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>阅读时长:&nbsp;&nbsp;
                    30 分
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>阅读次数:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>⚠️ 以下所有内容总结都来自于 大语言模型的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p>
</blockquote>
<h1 id="2025-04-04-更新"><a href="#2025-04-04-更新" class="headerlink" title="2025-04-04 更新"></a>2025-04-04 更新</h1><h2 id="Are-you-really-listening-Boosting-Perceptual-Awareness-in-Music-QA-Benchmarks"><a href="#Are-you-really-listening-Boosting-Perceptual-Awareness-in-Music-QA-Benchmarks" class="headerlink" title="Are you really listening? Boosting Perceptual Awareness in Music-QA   Benchmarks"></a>Are you really listening? Boosting Perceptual Awareness in Music-QA   Benchmarks</h2><p><strong>Authors:Yongyi Zang, Sean O’Brien, Taylor Berg-Kirkpatrick, Julian McAuley, Zachary Novack</strong></p>
<p>Large Audio Language Models (LALMs), where pretrained text LLMs are finetuned with audio input, have made remarkable progress in music understanding. However, current evaluation methodologies exhibit critical limitations: on the leading Music Question Answering benchmark, MuchoMusic, text-only LLMs without audio perception capabilities achieve surprisingly high accuracy of up to 56.4%, on par or above most LALMs. Furthermore, when presented with random Gaussian noise instead of actual audio, LALMs still perform significantly above chance. These findings suggest existing benchmarks predominantly assess reasoning abilities rather than audio perception. To overcome this challenge, we present RUListening: Robust Understanding through Listening, a framework that enhances perceptual evaluation in Music-QA benchmarks. We introduce the Perceptual Index (PI), a quantitative metric that measures a question’s reliance on audio perception by analyzing log probability distributions from text-only language models. Using this metric, we generate synthetic, challenging distractors to create QA pairs that necessitate genuine audio perception. When applied to MuchoMusic, our filtered dataset successfully forces models to rely on perceptual information-text-only LLMs perform at chance levels, while LALMs similarly deteriorate when audio inputs are replaced with noise. These results validate our framework’s effectiveness in creating benchmarks that more accurately evaluate audio perception capabilities. </p>
<blockquote>
<p>大规模音频语言模型（LALMs）通过在音频输入上微调预训练的文本大型语言模型（LLMs），在音乐理解方面取得了显著的进步。然而，当前的评估方法存在明显的局限性：在领先的Music Question Answering基准测试MuchoMusic上，没有音频感知能力的纯文本LLMs出人意料地达到了高达56.4%的高准确率，与大多数LALM的表现持平或更高。此外，当面对随机高斯噪声而非实际音频时，LALM的表现仍然显著高于随机水平。这些发现表明，现有的基准测试主要评估的是推理能力，而不是音频感知能力。为了克服这一挑战，我们提出了RUListening：通过倾听实现稳健理解，这是一个增强音乐问答基准测试中感知评估的框架。我们引入了感知指数（PI）这一量化指标，通过分析纯文本语言模型的日志概率分布来衡量问题对音频感知的依赖程度。使用该指标，我们生成了合成且具有挑战性的干扰项，以创建必须依赖真实音频感知的QA对。在MuchoMusic上的应用显示，我们的过滤数据集成功迫使模型依赖感知信息——纯文本LLMs的表现达到了随机水平，而当将音频输入替换为噪声时，LALM的表现也类似地下降。这些结果验证了我们的框架在创建更能准确评估音频感知能力的基准测试方面的有效性。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.00369v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>预训练文本LLM通过音频输入微调后形成的大型音频语言模型（LALMs）在音乐理解方面取得了显著进展。然而，现有的评估方法存在重大缺陷，因为即便是在领先的音乐问答基准测试MuchoMusic上，不具备音频感知能力的文本LLM也能达到令人惊讶的高准确率（高达56.4%），与大多数LALM的表现不相上下。甚至当面对随机高斯噪声而非实际音频时，LALM的表现也显著优于随机水平。这些发现表明，当前基准测试主要评估的是推理能力而非音频感知能力。为此，我们提出RUListening：通过倾听实现稳健理解，一个增强音乐问答基准测试中感知评估的框架。我们引入了感知指数（PI）这一量化指标，通过分析文本模型的日志概率分布来衡量问题对音频感知的依赖程度。利用这一指标，我们生成了具有挑战性的合成干扰项来创建需要真实音频感知的问答对。在MuchoMusic上的应用显示，我们的筛选数据集成功迫使模型依赖感知信息——文本LLM的表现接近随机水平，而LALM在音频输入被噪声替代时表现同样不佳。这些结果验证了我们的框架在创建更准确地评估音频感知能力的基准测试方面的有效性。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LALMs在音乐理解方面取得显著进步。</li>
<li>当前评估方法主要评估推理能力而非音频感知能力。</li>
<li>文本LLM在基准测试中表现出高准确率，即使不具备音频感知能力。</li>
<li>随机噪声对LALM的影响显示其并非真正依赖于音频感知。</li>
<li>提出RUListening框架以增强音乐问答基准测试中的感知评估。</li>
<li>引入感知指数（PI）来衡量问题对音频感知的依赖程度。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.00369">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-d3f244078c20c8addb5f35f5894c1170.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-894bc19d39dfb80370cdbe57f7aa0100.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-4887062fdf95aebc8fd2536c4d5d3d9f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4c98d6f92555a3768667bad5ebcd8682.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7122aa8bcee9d851402f42e29c1ddd58.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-ce319e27432554d5fd748b69aeadb690.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="DeepDubber-V1-Towards-High-Quality-and-Dialogue-Narration-Monologue-Adaptive-Movie-Dubbing-Via-Multi-Modal-Chain-of-Thoughts-Reasoning-Guidance"><a href="#DeepDubber-V1-Towards-High-Quality-and-Dialogue-Narration-Monologue-Adaptive-Movie-Dubbing-Via-Multi-Modal-Chain-of-Thoughts-Reasoning-Guidance" class="headerlink" title="DeepDubber-V1: Towards High Quality and Dialogue, Narration, Monologue   Adaptive Movie Dubbing Via Multi-Modal Chain-of-Thoughts Reasoning Guidance"></a>DeepDubber-V1: Towards High Quality and Dialogue, Narration, Monologue   Adaptive Movie Dubbing Via Multi-Modal Chain-of-Thoughts Reasoning Guidance</h2><p><strong>Authors:Junjie Zheng, Zihao Chen, Chaofan Ding, Xinhan Di</strong></p>
<p>Current movie dubbing technology can generate the desired voice from a given speech prompt, ensuring good synchronization between speech and visuals while accurately conveying the intended emotions. However, in movie dubbing, key aspects such as adapting to different dubbing styles, handling dialogue, narration, and monologue effectively, and understanding subtle details like the age and gender of speakers, have not been well studied. To address this challenge, we propose a framework of multi-modal large language model. First, it utilizes multimodal Chain-of-Thought (CoT) reasoning methods on visual inputs to understand dubbing styles and fine-grained attributes. Second, it generates high-quality dubbing through large speech generation models, guided by multimodal conditions. Additionally, we have developed a movie dubbing dataset with CoT annotations. The evaluation results demonstrate a performance improvement over state-of-the-art methods across multiple datasets. In particular, for the evaluation metrics, the SPK-SIM and EMO-SIM increases from 82.48% to 89.74%, 66.24% to 78.88% for dubbing setting 2.0 on V2C Animation dataset, LSE-D and MCD-SL decreases from 14.79 to 14.63, 5.24 to 4.74 for dubbing setting 2.0 on Grid dataset, SPK-SIM increases from 64.03 to 83.42 and WER decreases from 52.69% to 23.20% for initial reasoning setting on proposed CoT-Movie-Dubbing dataset in the comparison with the state-of-the art models. </p>
<blockquote>
<p>当前的电影配音技术可以根据给定的语音提示生成所需的声音，确保语音和视觉之间的良好同步，同时准确传达预期的情绪。然而，在电影配音中，如何适应不同的配音风格、有效处理对话、旁白和独白，以及理解如说话人的年龄和性别等细微细节尚未得到很好的研究。为了应对这一挑战，我们提出了一个多模态大语言模型框架。首先，它利用视觉输入的链式思维（CoT）推理方法，理解配音风格和精细属性。其次，通过大型语音生成模型生成高质量的配音，在多模态条件下进行引导。此外，我们还开发了一个带有CoT注释的电影配音数据集。评估结果表明，与最先进的方法相比，我们在多个数据集上的性能有所提高。特别是，在V2C动画数据集的配音设置2.0上，SPK-SIM和EMO-SIM分别从82.48%提高到89.74%，从66.24%提高到78.88%；在Grid数据集的配音设置2.0上，LSE-D和MCD-SL分别从14.79下降到14.63，从5.24下降到4.74；在与最新模型的比较中，我们在提出的CoT-Movie-Dubbing数据集上的初步推理设置上，SPK-SIM从64.03%提高到83.42%，WER从52.69%降低到23.20%。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.23660v1">PDF</a> 11 pages, 5 figures</p>
<p><strong>摘要</strong></p>
<p>当前电影配音技术可根据给定的语音提示生成所需的配音，确保语音与视觉的良好同步，同时准确传达情感。然而，在电影配音中，如何适应不同的配音风格、有效处理对话、旁白和独白，以及理解如说话人的年龄和性别等细微之处尚未得到深入研究。为应对这些挑战，我们提出了多模态大型语言模型框架。首先，它利用视觉输入的多模态Chain-of-Thought（CoT）推理方法，理解配音风格和精细属性。其次，通过大型语音生成模型生成高质量的配音，由多模态条件指导。此外，我们还开发了带有CoT注释的电影配音数据集。评估结果表明，与最新方法相比，我们在多个数据集上的表现有所提高。特别是在评估指标上，V2C动画数据集的说话人相似性（SPK-SIM）和情感相似性（EMO-SIM）分别从82.48%提高到89.74%和从66.24%提高到78.88%，Grid数据集的说话人位置估计偏差（LSE-D）和音乐与说话人感知距离（MCD-SL）分别从14.79降至到14.63和从5.24降至到4.74。与我们提出的CoT-Movie-Dubbing数据集上的初步推理设置相比，SPK-SIM从64.03%提高到83.42%，单词错误率（WER）从52.69%降至到到23.20%。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>当前电影配音技术可以同步语音和视觉，但还需要研究如何适应不同的配音风格和处理对话、旁白和独白等。</li>
<li>提出了多模态大型语言模型框架来解决这些问题，结合视觉输入的多模态Chain-of-Thought（CoT）推理和大型语音生成模型。</li>
<li>开发了带有CoT注释的电影配音数据集以进行评估。</li>
<li>与现有方法相比，该框架在多个数据集上的表现有所提升。</li>
<li>在V2C动画数据集上，SPK-SIM和EMO-SIM指标有明显提高。</li>
<li>在Grid数据集上，LSE-D和MCD-SL指标有所改善。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.23660">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-fc5ee49ef0b6f667d475d787613844d9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-42464c87df81b2e865fcb29544188de6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f89e56211abb82d4a8eb3e75f076e215.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-862807fe09cf0fc2741ea16e70ca8a2f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-820ea6cba6d2d3300f25867b3bd41411.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Integrating-Large-Language-Models-For-Monte-Carlo-Simulation-of-Chemical-Reaction-Networks"><a href="#Integrating-Large-Language-Models-For-Monte-Carlo-Simulation-of-Chemical-Reaction-Networks" class="headerlink" title="Integrating Large Language Models For Monte Carlo Simulation of Chemical   Reaction Networks"></a>Integrating Large Language Models For Monte Carlo Simulation of Chemical   Reaction Networks</h2><p><strong>Authors:Sadikshya Gyawali, Ashwini Mandal, Manish Dahal, Manish Awale, Sanjay Rijal, Shital Adhikari, Vaghawan Ojha</strong></p>
<p>Chemical reaction network is an important method for modeling and exploring complex biological processes, bio-chemical interactions and the behavior of different dynamics in system biology. But, formulating such reaction kinetics takes considerable time. In this paper, we leverage the efficiency of modern large language models to automate the stochastic monte carlo simulation of chemical reaction networks and enable the simulation through the reaction description provided in the form of natural languages. We also integrate this process into widely used simulation tool Copasi to further give the edge and ease to the modelers and researchers. In this work, we show the efficacy and limitations of the modern large language models to parse and create reaction kinetics for modelling complex chemical reaction processes. </p>
<blockquote>
<p>化学反应网络是系统生物学中建模和探索复杂生物过程、生物化学交互以及不同动态行为的重要方法。但是，制定这样的反应动力学需要相当长的时间。在本文中，我们利用现代大型语言模型的效率，自动进行化学反应网络的随机蒙特卡洛模拟，并通过自然语言形式提供的反应描述启用模拟。我们还将此过程整合到广泛使用的模拟工具Copasi中，以进一步为建模人员和研究人员提供优势和便利。在这项工作中，我们展示了现代大型语言模型在解析和创建反应动力学以模拟复杂的化学反应过程中的有效性和局限性。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.21178v1">PDF</a> Accepted on MadeAI 2025 Conference</p>
<p><strong>Summary</strong><br>化学反应网络是系统生物学中模拟和探索复杂生物过程、生物化学相互作用以及不同动力学行为的重要方法，但构建反应动力学需要消耗大量时间。本文利用现代大型语言模型的效率，实现化学反应网络的随机蒙特卡洛模拟自动化，并通过自然语言形式提供的反应描述进行模拟。我们还将此过程集成到广泛使用的模拟工具Copasi中，为建模人员和研究人员提供优势和便利。本文展示了现代大型语言模型在解析和创建反应动力学以模拟复杂化学反应过程中的效果和局限性。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>化学反应网络是模拟复杂生物过程、生物化学相互作用和系统生物学中不同动力学行为的重要方法。</li>
<li>构建化学反应网络的反应动力学需要消耗大量时间。</li>
<li>现代大型语言模型可以有效地自动化化学反应网络的随机蒙特卡洛模拟。</li>
<li>自然语言的反应描述形式可以用于模拟过程。</li>
<li>将此过程集成到Copasi等广泛使用的模拟工具中，为建模人员和研究人员提供了便利。</li>
<li>现代大型语言模型在解析和创建反应动力学方面表现出良好的效果。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.21178">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-0a22983696be9477538a365a12ed6596.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f985b98e1c0252e1ae46f0ba31203740.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f2b7f9c702565be49365b292216ec788.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4a947db43d28fe761180c92c9a156df5.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-208b93fcbfdac29f2292073dc75ed1ea.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f576e97911c89bcec3f541befb9f41f6.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="StreamMind-Unlocking-Full-Frame-Rate-Streaming-Video-Dialogue-through-Event-Gated-Cognition"><a href="#StreamMind-Unlocking-Full-Frame-Rate-Streaming-Video-Dialogue-through-Event-Gated-Cognition" class="headerlink" title="StreamMind: Unlocking Full Frame Rate Streaming Video Dialogue through   Event-Gated Cognition"></a>StreamMind: Unlocking Full Frame Rate Streaming Video Dialogue through   Event-Gated Cognition</h2><p><strong>Authors:Xin Ding, Hao Wu, Yifan Yang, Shiqi Jiang, Donglin Bai, Zhibo Chen, Ting Cao</strong></p>
<p>With the rise of real-world human-AI interaction applications, such as AI assistants, the need for Streaming Video Dialogue is critical. To address this need, we introduce StreamMind, a video LLM framework that achieves ultra-FPS streaming video processing (100 fps on a single A100) and enables proactive, always-on responses in real time, without explicit user intervention.   To solve the key challenge of the contradiction between linear video streaming speed and quadratic transformer computation cost, we propose a novel perception-cognition interleaving paradigm named ‘’event-gated LLM invocation’’, in contrast to the existing per-time-step LLM invocation. By introducing a Cognition Gate network between the video encoder and the LLM, LLM is only invoked when relevant events occur. To realize the event feature extraction with constant cost, we propose Event-Preserving Feature Extractor (EPFE) based on state-space method, generating a single perception token for spatiotemporal features. These techniques enable the video LLM with full-FPS perception and real-time cognition response.   Experiments on Ego4D and SoccerNet streaming tasks, as well as standard offline benchmarks, demonstrate state-of-the-art performance in both model capability and real-time efficiency, paving the way for ultra-high-FPS applications, such as Game AI and interactive media. The code and data is available at <a target="_blank" rel="noopener" href="https://aka.ms/StreamMind">https://aka.ms/StreamMind</a>. </p>
<blockquote>
<p>随着人工智能助手等现实世界中人机交互应用的兴起，对流式视频对话的需求变得至关重要。为了解决这一需求，我们推出了StreamMind，这是一款视频LLM框架，可实现超FPS流式视频处理（在单个A100上达到100fps），并可在不需要用户明确干预的情况下，实现实时主动响应。为了解决线性视频流速度与二次方转换器计算成本之间的主要矛盾，我们提出了一种名为“事件门控LLM调用”的新型感知认知交替范式，这与现有的按时间步长LLM调用形成对比。通过在视频编码器和LLM之间引入认知网关网络，只有在相关事件发生时才调用LLM。为了以恒定成本实现事件特征提取，我们提出了基于状态空间方法的Event-Preserving Feature Extractor（EPFE），为时空特征生成单个感知令牌。这些技术使视频LLM具备全FPS感知和实时认知响应能力。在Ego4D和SoccerNet流媒体任务以及标准离线基准测试上的实验表明，其在模型能力和实时效率方面都达到了最先进的性能，为超高FPS应用（如游戏AI和交互式媒体）铺平了道路。代码和数据可通过<a target="_blank" rel="noopener" href="https://aka.ms/StreamMind%E8%8E%B7%E5%8F%96%E3%80%82">https://aka.ms/StreamMind获取。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.06220v2">PDF</a> </p>
<p><strong>Summary</strong><br>     随着人工智能助手等现实世界中人机交互应用的兴起，流式视频对话的需求变得至关重要。为应对这一需求，我们推出了StreamMind，这是一款视频LLM框架，可实现超帧速率流式视频处理（单A100上达100fps），并能在无需用户明确干预的情况下实时进行前瞻性、全天候响应。为解决线性视频流速度与二次方转换器计算成本之间的主要矛盾，我们提出了一种名为“事件门控LLM调用”的新型感知认知交替范式，与现有的按时间步长调用LLM的方法形成对比。通过视频编码器和LLM之间增加一个认知门网络，只有在相关事件发生时才会调用LLM。我们提出了基于状态空间方法的Event-Preserving Feature Extractor（EPFE），以恒定成本实现事件特征提取，为时空特征生成单个感知令牌。这些技术使视频LLM具备全帧感知和实时认知响应能力。在Ego4D、SoccerNet流任务以及标准离线基准测试上的实验证明了其在模型能力和实时效率方面的卓越性能，为超高帧率应用（如游戏AI和交互式媒体）铺平了道路。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>StreamMind框架满足流式视频对话需求，支持实时、前瞻性、全天候响应。</li>
<li>通过引入事件门控LLM调用解决线性视频流速度与计算成本矛盾。</li>
<li>认知门网络仅在相关事件发生时调用LLM，提高效率。</li>
<li>Event-Preserving Feature Extractor实现恒定成本的事件特征提取。</li>
<li>技术使视频LLM具备全帧感知和实时认知响应能力。</li>
<li>在多个基准测试上表现出卓越性能。</li>
<li>代码和数据可供公众访问。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.06220">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-31f16313e09db10aa620456dc0e7a7ea.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-256aef52eb786d6f7eba53fc8197aab6.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-eeb115a5203f28e0dbc4b903ecc95cf9.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d4365d3f35c533643161154d734be9aa.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-3fcbb9d2d7eeb9a2ea5b44ccce00a126.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b6a102c40fc07abe5e8f07d5023053f8.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-51b4063d3a52b916e7cbc63094b6274e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4c1fd060dfe8d17a63331c806ea89c54.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="CosForce-A-Force-Based-General-Pedestrian-Model-with-Anticipation-and-Reaction-Mechanisms"><a href="#CosForce-A-Force-Based-General-Pedestrian-Model-with-Anticipation-and-Reaction-Mechanisms" class="headerlink" title="CosForce: A Force-Based General Pedestrian Model with Anticipation and   Reaction Mechanisms"></a>CosForce: A Force-Based General Pedestrian Model with Anticipation and   Reaction Mechanisms</h2><p><strong>Authors:Jinghui Wang, Wei Lv, Shuchao Cao, Chenglin Guo</strong></p>
<p>In this study, we developed a force-based general pedestrian model named CosForce. To the best of our knowledge, this may represent the simplest version of the force-based method. The model employs cosine functions to characterize asymmetric interactions, implicitly incorporating anticipation and reaction mechanisms. By focusing on binary interactions, the CosForce model provides new insights into pedestrian modeling while achieving linear computational complexity. Two specific scenarios in crowd dynamics were analyzed: self-organization (entropy decrease) and crowd collapse (entropy increase). The average normalized speed and order parameter were introduced to quantitatively describe the processes of crowd dynamics. Quantitative evaluations demonstrate that phase separation in crowds is effectively reproduced by the model, including lane formation, stripe formation, and cross-channel formation. Next, in the simulation of mass gathering, within a density-accumulating scenario, processes of critical phase transition in high-density crowds are clearly revealed through time series observations of the order parameter. These findings provide valuable insights into crowd dynamics. </p>
<blockquote>
<p>在这项研究中，我们开发了一种基于力的一般行人模型，名为CosForce。据我们所知，这可能是基于力方法的最简单版本。该模型采用余弦函数来表征不对称相互作用，隐含地融入了预期和反应机制。通过关注二元交互，CosForce模型在行人建模方面提供了新的见解，同时实现了线性计算复杂度。分析了人群动力学中的两个特定场景：自组织（熵减少）和人群崩溃（熵增加）。引入平均归一化速度和顺序参数来定量描述人群动力学的过程。定量评估表明，该模型有效地再现了人群中的相分离，包括车道形成、条纹形成和跨通道形成。接下来，在模拟聚集时，在密度累积的场景下，通过秩序参数的时间序列观察，清楚地揭示了高密度人群中的临界相变过程。这些发现对人群动力学提供了有价值的见解。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2410.10746v3">PDF</a> 28 pages, 25 figures</p>
<p><strong>Summary</strong></p>
<p>本研究开发了一种基于力量的通用行人模型——CosForce。据我们所知，这可能是基于力量的方法中最简单的版本。该模型采用余弦函数来表征不对称的相互作用，隐含地融入了预期和反应机制。通过关注二元交互，CosForce模型为行人建模提供了新的见解，同时实现了线性的计算复杂度。分析了人群动力学的两个特定场景：自组织（熵减少）和人群崩溃（熵增加）。引入平均归一化速度和秩序参数来定量描述人群动力学过程。定量评估表明，该模型有效地再现了人群中的相位分离，包括车道形成、条纹形成和跨通道形成。在模拟密度累积场景的大规模聚集时，通过秩序参数的时间序列观察，清晰地揭示了高密度人群中的临界相变过程。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>开发了一种名为CosForce的基于力量的通用行人模型，可能是迄今为止最简化的力量模型版本。</li>
<li>该模型利用余弦函数描述不对称的相互作用，包含预期和反应机制。</li>
<li>CosForce模型专注于二元交互，为行人建模提供新视角，具有线性计算复杂度。</li>
<li>分析了人群动力学的两个重要场景：自组织和人群崩溃（熵的增加和减少）。</li>
<li>通过平均归一化速度和秩序参数定量描述人群动力学过程。</li>
<li>评估表明，模型能有效模拟人群中的相位分离现象，如车道、条纹和跨通道的形成。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2410.10746">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-040aec2eb602781b544a3273a7437a6a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-cdb9b644dff6bef3595f2bfa7ec0d7d0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d0d3ec3aeec1f6c5e34245db144fd58e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e022a304cd0ae847b30d8606bff0aaf1.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-880b1f42119d94834bc3610ce646bce6.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="An-End-to-End-Model-for-Photo-Sharing-Multi-modal-Dialogue-Generation"><a href="#An-End-to-End-Model-for-Photo-Sharing-Multi-modal-Dialogue-Generation" class="headerlink" title="An End-to-End Model for Photo-Sharing Multi-modal Dialogue Generation"></a>An End-to-End Model for Photo-Sharing Multi-modal Dialogue Generation</h2><p><strong>Authors:Peiming Guo, Sinuo Liu, Yanzhao Zhang, Dingkun Long, Pengjun Xie, Meishan Zhang, Min Zhang</strong></p>
<p>Photo-Sharing Multi-modal dialogue generation requires a dialogue agent not only to generate text responses but also to share photos at the proper moment. Using image text caption as the bridge, a pipeline model integrates an image caption model, a text generation model, and an image generation model to handle this complex multi-modal task. However, representing the images with text captions may loss important visual details and information and cause error propagation in the complex dialogue system. Besides, the pipeline model isolates the three models separately because discrete image text captions hinder end-to-end gradient propagation. We propose the first end-to-end model for photo-sharing multi-modal dialogue generation, which integrates an image perceptron and an image generator with a large language model. The large language model employs the Q-Former to perceive visual images in the input end. For image generation in the output end, we propose a dynamic vocabulary transformation matrix and use straight-through and gumbel-softmax techniques to align the large language model and stable diffusion model and achieve end-to-end gradient propagation. We perform experiments on PhotoChat and DialogCC datasets to evaluate our end-to-end model. Compared with pipeline models, the end-to-end model gains state-of-the-art performances on various metrics of text and image generation. More analysis experiments also verify the effectiveness of the end-to-end model for photo-sharing multi-modal dialogue generation. </p>
<blockquote>
<p>图片共享多模态对话生成要求对话代理不仅生成文本响应，还必须在适当的时刻共享照片。以图像文本标题作为桥梁，管道模型通过整合图像标题模型、文本生成模型和图像生成模型来处理这项复杂的多模态任务。然而，用文本标题表示图像可能会丢失重要的视觉细节和信息，并在复杂对话系统中引起误差传播。此外，管道模型将这三个模型分开处理，因为离散的图像文本标题会阻碍端到端的梯度传播。我们提出了第一个针对图片共享多模态对话生成端到端模型，该模型将图像感知器和图像生成器与大型语言模型集成在一起。大型语言模型采用Q-Former感知输入端的视觉图像。在输出端进行图像生成时，我们提出了动态词汇转换矩阵，并使用直通和软轴混合技术将大型语言模型和稳定扩散模型对齐，以实现端到端的梯度传播。我们在PhotoChat和DialogCC数据集上进行了实验，以评估我们的端到端模型。与管道模型相比，端到端模型在文本和图像生成的各项指标上均达到了最先进的性能。更多的分析实验也验证了端到端模型在图片共享多模态对话生成中的有效性。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2408.08650v2">PDF</a> Accepted by ICME2025</p>
<p><strong>Summary</strong></p>
<p>多模态对话生成在照片共享场景下要求对话智能体不仅生成文本回应，还需在适当时候分享照片。当前使用图像文本描述作为桥梁的管道模型整合了图像描述模型、文本生成模型和图像生成模型来完成这项复杂的多模态任务。然而，用文本描述来表示图像可能会丢失重要的视觉细节和信息，并在复杂的对话系统中造成误差传播。此外，管道模型将三个模型孤立分开，因为离散的图像文本描述阻碍了端到端的梯度传播。为此，我们提出了首个端到端模型用于照片共享多模态对话生成，该模型整合了图像感知器和图像生成器与大型语言模型。大型语言模型在输入端采用Q-Former感知视觉图像，在输出端则通过动态词汇转换矩阵、直通和Gumbel-softmax技术实现与稳定扩散模型的对接，并达成端到端的梯度传播。我们在PhotoChat和DialogCC数据集上对我们的端到端模型进行了实验评估。相较于管道模型，端到端模型在文本和图像生成的各项指标上取得了最先进的性能表现。进一步的分析实验也验证了该端到端模型在照片共享多模态对话生成中的有效性。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>多模态对话生成在照片共享场景下要求对话智能体具备生成文本回应和分享照片的能力。</li>
<li>当前使用的管道模型通过图像文本描述来连接各模型，但存在丢失视觉细节和信息的风险，并可能导致误差传播。</li>
<li>提出的端到端模型首次整合了图像感知器、图像生成器和大型语言模型，以提高多模态对话的效率和准确性。</li>
<li>大型语言模型采用Q-Former技术感知视觉图像，并实现了与稳定扩散模型的对接。</li>
<li>通过动态词汇转换矩阵、直通和Gumbel-softmax技术，实现了端到端的梯度传播，提高了模型的性能。</li>
<li>在PhotoChat和DialogCC数据集上的实验表明，端到端模型在文本和图像生成方面表现优异。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2408.08650">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-dbb46f7a081ec2ce10fb2cbc2f1b2c39.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-edfcf7a3dfba741f02a6fa40ecc615d3.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-018de950e3915df3e363e45a3d79498c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f31eb855558ef8f2d8df6944c307e237.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-5279ee584426b2976515283ab8f0428f.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Safety-Aware-Human-Lead-Vehicle-Platooning-by-Proactively-Reacting-to-Uncertain-Human-Behaving"><a href="#Safety-Aware-Human-Lead-Vehicle-Platooning-by-Proactively-Reacting-to-Uncertain-Human-Behaving" class="headerlink" title="Safety-Aware Human-Lead Vehicle Platooning by Proactively Reacting to   Uncertain Human Behaving"></a>Safety-Aware Human-Lead Vehicle Platooning by Proactively Reacting to   Uncertain Human Behaving</h2><p><strong>Authors:Jia Hu, Shuhan Wang, Yiming Zhang, Haoran Wang, Zhilong Liu, Guangzhi Cao</strong></p>
<p>Human-Lead Cooperative Adaptive Cruise Control (HL-CACC) is regarded as a promising vehicle platooning technology in real-world implementation. By utilizing a Human-driven Vehicle (HV) as the platoon leader, HL-CACC reduces the cost and enhances the reliability of perception and decision-making. However, state-of-the-art HL-CACC technology still has a great limitation on driving safety due to the lack of considering the leading human driver’s uncertain behavior. In this study, a HL-CACC controller is designed based on Stochastic Model Predictive Control (SMPC). It is enabled to predict the driving intention of the leading Connected Human-Driven Vehicle (CHV). The proposed controller has the following features: i) enhanced perceived safety in oscillating traffic; ii) guaranteed safety against hard brakes; iii) computational efficiency for real-time implementation. The proposed controller is evaluated on a PreScan&amp;Simulink simulation platform. Real vehicle trajectory data is collected for the calibration of the simulation. Results reveal that the proposed controller: i) improves perceived safety by 19.17% in oscillating traffic; ii) enhances actual safety by 7.76% against hard brakes; iii) is confirmed with string stability. The computation time is approximately 3.2 milliseconds when running on a laptop equipped with an Intel i5-13500H CPU. This indicates the proposed controller is ready for real-time implementation. </p>
<blockquote>
<p>人类主导的协同自适应巡航控制（HL-CACC）被认为是一种有前景的车辆编队技术，在实际应用中具有广阔的前景。通过利用人类驾驶车辆（HV）作为车队领导者，HL-CACC降低了感知和决策的成本，提高了其可靠性。然而，最先进的HL-CACC技术在驾驶安全方面仍有很大局限，因为该技术未能充分考虑领头的人类驾驶员的不确定性行为。本研究设计了一种基于随机模型预测控制（SMPC）的HL-CACC控制器。它能够预测领先的人车互联（CHV）的驾驶意图。该控制器具有以下特点：一、在振荡交通中增强了感知安全性；二、对急刹车情况保证安全；三、具有实时实现的计算效率。该控制器在PreScan&amp;Simulink仿真平台上进行了评估。收集了真实车辆轨迹数据对仿真进行校准。结果表明，该控制器：一、在振荡交通中提高了19.17%的感知安全性；二、在急刹车情况下提高了7.76%的实际安全性；三、具有队列稳定性。在配备Intel i5-13500H CPU的笔记本电脑上运行时，计算时间约为3.2毫秒，这表明该控制器已准备好进行实时实现。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2405.07556v3">PDF</a> </p>
<p><strong>Summary</strong><br>     基于随机模型预测控制（SMPC）设计的人类引导合作自适应巡航控制（HL-CACC）控制器，能预测领先连接人类驾驶车辆（CHV）的驾驶意图，具有增强感知安全、保障硬制动安全和计算效率高等特点。模拟评估显示，该控制器在提高感知安全和实际安全性方面有明显效果，并具备实时实施条件。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>HL-CACC技术利用人类驾驶车辆作为车队领导者，降低成本并增强感知和决策可靠性。</li>
<li>当前HL-CACC技术在驾驶安全方面存在局限，未能充分考虑领先人类驾驶员的不确定性行为。</li>
<li>基于SMPC设计的HL-CACC控制器能够预测CHV的驾驶意图。</li>
<li>控制器在振荡交通中增强了感知安全，并对硬刹车情况提供了安全保障。</li>
<li>控制器在模拟评估中表现出良好的性能，包括提高感知安全和实际安全性，以及具备字符串稳定性。</li>
<li>控制器计算时间短，具备实时实施条件。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2405.07556">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-03c119da6e840c14f7bb21bcec6cf260.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-291a7737b02746de5de67cca80548f8b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a3ba8317b8be8f31bdfc0ea8618b95f7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-32b85910a96109a378086c60e7752b9b.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        文章作者:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        文章链接:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-04-03/Interactive/">https://kedreamix.github.io/Talk2Paper/Paper/2025-04-03/Interactive/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        版权声明:
                    </i>
                </span>
                <span class="reprint-info">
                    本博客所有文章除特別声明外，均采用
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    许可协议。转载请注明来源
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Interactive/">
                                    <span class="chip bg-color">Interactive</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;上一篇</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-04-04/Interactive/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-018de950e3915df3e363e45a3d79498c.jpg" class="responsive-img" alt="Interactive">
                        
                        <span class="card-title">Interactive</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Interactive 方向最新论文已更新，请持续关注 Update in 2025-04-04  Are you really listening? Boosting Perceptual Awareness in Music-QA   Benchmarks
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-04-04
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Interactive/" class="post-category">
                                    Interactive
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Interactive/">
                        <span class="chip bg-color">Interactive</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                下一篇&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-04-04/TTS/">
                    <div class="card-image">
                        
                        <img src="https://pica.zhimg.com/v2-ef56415eb65cfb8300e0a1913078f61c.jpg" class="responsive-img" alt="TTS">
                        
                        <span class="card-title">TTS</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            TTS 方向最新论文已更新，请持续关注 Update in 2025-04-04  LEP3 A High-Luminosity e+e- Higgs and ElectroweakFactory in the LHC   Tunnel
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-04-04
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/TTS/" class="post-category">
                                    TTS
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/TTS/">
                        <span class="chip bg-color">TTS</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- 代码块功能依赖 -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- 代码语言 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- 代码块复制 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- 代码块收缩 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC 悬浮按钮. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* 修复文章卡片 div 的宽度. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // 切换TOC目录展开收缩的相关操作.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;站点总字数:&nbsp;<span
                        class="white-color">27544.6k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- 运行天数提醒. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // 区分是否有年份.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = '本站已运行 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                daysTip = '本站已運行 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = '本站已运行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = '本站已運行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="关注我的知乎: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">知</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- 搜索遮罩框 -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span>
            <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- 白天和黑夜主题 -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- 回到顶部按钮 -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // 只在桌面版网页启用特效
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- 雪花特效 -->
    

    <!-- 鼠标星星特效 -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--腾讯兔小巢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- 动态标签 -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Σ(っ °Д °;)っ诶，页面崩溃了嘛？", clearTimeout(st)) : (document.title = "φ(゜▽゜*)♪咦，又好了！", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
