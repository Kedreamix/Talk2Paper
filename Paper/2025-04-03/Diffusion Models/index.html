<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Diffusion Models">
    <meta name="description" content="Diffusion Models æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-04-04  ILLUME+ Illuminating Unified MLLM with Dual Visual Tokenization and   Diffusion Refinement">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Diffusion Models | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pic1.zhimg.com/v2-c7b0c6b8320afd10d25f4c857613fcf9.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Diffusion Models</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Diffusion-Models/">
                                <span class="chip bg-color">Diffusion Models</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Diffusion-Models/" class="post-category">
                                Diffusion Models
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-04-04
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-04-06
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    20.7k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    84 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-04-04-æ›´æ–°"><a href="#2025-04-04-æ›´æ–°" class="headerlink" title="2025-04-04 æ›´æ–°"></a>2025-04-04 æ›´æ–°</h1><h2 id="ILLUME-Illuminating-Unified-MLLM-with-Dual-Visual-Tokenization-and-Diffusion-Refinement"><a href="#ILLUME-Illuminating-Unified-MLLM-with-Dual-Visual-Tokenization-and-Diffusion-Refinement" class="headerlink" title="ILLUME+: Illuminating Unified MLLM with Dual Visual Tokenization and   Diffusion Refinement"></a>ILLUME+: Illuminating Unified MLLM with Dual Visual Tokenization and   Diffusion Refinement</h2><p><strong>Authors:Runhui Huang, Chunwei Wang, Junwei Yang, Guansong Lu, Yunlong Yuan, Jianhua Han, Lu Hou, Wei Zhang, Lanqing Hong, Hengshuang Zhao, Hang Xu</strong></p>
<p>We present ILLUME+ that leverages dual visual tokenization and a diffusion decoder to improve both deep semantic understanding and high-fidelity image generation. Existing unified models have struggled to simultaneously handle the three fundamental capabilities in a unified model: understanding, generation, and editing. Models like Chameleon and EMU3 utilize VQGAN for image discretization, due to the lack of deep semantic interaction, they lag behind specialist models like LLaVA in visual understanding tasks. To mitigate this, LaViT and ILLUME employ semantic encoders for tokenization, but they struggle with image editing due to poor texture preservation. Meanwhile, Janus series decouples the input and output image representation, limiting their abilities to seamlessly handle interleaved image-text understanding and generation. In contrast, ILLUME+ introduces a unified dual visual tokenizer, DualViTok, which preserves both fine-grained textures and text-aligned semantics while enabling a coarse-to-fine image representation strategy for multimodal understanding and generation. Additionally, we employ a diffusion model as the image detokenizer for enhanced generation quality and efficient super-resolution. ILLUME+ follows a continuous-input, discrete-output scheme within the unified MLLM and adopts a progressive training procedure that supports dynamic resolution across the vision tokenizer, MLLM, and diffusion decoder. This design allows for flexible and efficient context-aware image editing and generation across diverse tasks. ILLUME+ (3B) exhibits competitive performance against existing unified MLLMs and specialized models across multimodal understanding, generation, and editing benchmarks. With its strong performance, ILLUME+ provides a scalable and versatile foundation for future multimodal applications. Project Page: <a target="_blank" rel="noopener" href="https://illume-unified-mllm.github.io/">https://illume-unified-mllm.github.io/</a>. </p>
<blockquote>
<p>æˆ‘ä»¬æå‡ºäº†ILLUME+ï¼Œå®ƒåˆ©ç”¨åŒè§†è§‰ç¬¦å·åŒ–å’Œæ‰©æ•£è§£ç å™¨ï¼Œæé«˜äº†æ·±åº¦è¯­ä¹‰ç†è§£å’Œé«˜ä¿çœŸå›¾åƒç”Ÿæˆçš„æ€§èƒ½ã€‚ç°æœ‰çš„ç»Ÿä¸€æ¨¡å‹åœ¨ç»Ÿä¸€æ¨¡å‹ä¸­åŒæ—¶å¤„ç†ä¸‰ç§åŸºæœ¬èƒ½åŠ›æ–¹é¢é‡åˆ°äº†å›°éš¾ï¼Œå³ç†è§£ã€ç”Ÿæˆå’Œç¼–è¾‘ã€‚åƒå˜è‰²é¾™å’ŒEMU3è¿™æ ·çš„æ¨¡å‹ä½¿ç”¨VQGANè¿›è¡Œå›¾åƒç¦»æ•£åŒ–ï¼Œç”±äºç¼ºä¹æ·±åº¦è¯­ä¹‰äº¤äº’ï¼Œå®ƒä»¬åœ¨è§†è§‰ç†è§£ä»»åŠ¡æ–¹é¢è½åäºLLaVAç­‰ä¸“ä¸šæ¨¡å‹ã€‚ä¸ºäº†ç¼“è§£è¿™ä¸€é—®é¢˜ï¼ŒLaViTå’ŒILLUMEé‡‡ç”¨è¯­ä¹‰ç¼–ç å™¨è¿›è¡Œç¬¦å·åŒ–ï¼Œä½†åœ¨å›¾åƒç¼–è¾‘æ–¹é¢ç”±äºçº¹ç†ä¿å­˜ä¸ä½³è€Œé‡åˆ°å›°éš¾ã€‚ä¸æ­¤åŒæ—¶ï¼ŒJanusç³»åˆ—å°†è¾“å…¥å’Œè¾“å‡ºå›¾åƒè¡¨ç¤ºè§£è€¦ï¼Œé™åˆ¶äº†å®ƒä»¬æ— ç¼å¤„ç†äº¤ç»‡çš„å›¾åƒæ–‡æœ¬ç†è§£å’Œç”Ÿæˆçš„èƒ½åŠ›ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼ŒILLUME+å¼•å…¥äº†ä¸€ä¸ªç»Ÿä¸€çš„åŒè§†è§‰æ ‡è®°å™¨DualViTokï¼Œå®ƒæ—¢ä¿ç•™äº†ç²¾ç»†çº¹ç†å’Œæ–‡æœ¬å¯¹é½è¯­ä¹‰ï¼Œåˆå®ç°äº†ç²—ç»†ç»“åˆçš„å›¾åƒè¡¨ç¤ºç­–ç•¥ï¼Œç”¨äºå¤šæ¨¡æ€ç†è§£å’Œç”Ÿæˆã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬é‡‡ç”¨æ‰©æ•£æ¨¡å‹ä½œä¸ºå›¾åƒè§£æ ‡è®°å™¨ï¼Œä»¥æé«˜ç”Ÿæˆè´¨é‡å’Œè¶…åˆ†è¾¨ç‡æ•ˆç‡ã€‚ILLUME+åœ¨ç»Ÿä¸€MLLMå†…é‡‡ç”¨è¿ç»­è¾“å…¥ã€ç¦»æ•£è¾“å‡ºçš„æ–¹æ¡ˆï¼Œå¹¶é‡‡ç”¨æ¸è¿›å¼è®­ç»ƒç¨‹åºï¼Œæ”¯æŒè§†è§‰æ ‡è®°å™¨ã€MLLMå’Œæ‰©æ•£è§£ç å™¨ä¹‹é—´çš„åŠ¨æ€åˆ†è¾¨ç‡ã€‚è¿™ç§è®¾è®¡ä½¿è·¨ä¸åŒä»»åŠ¡çš„çµæ´»å’Œé«˜æ•ˆçš„ä¸Šä¸‹æ–‡æ„ŸçŸ¥å›¾åƒç¼–è¾‘å’Œç”Ÿæˆæˆä¸ºå¯èƒ½ã€‚ILLUME+ï¼ˆ3Bï¼‰åœ¨å¤šæ¨¡æ€ç†è§£ã€ç”Ÿæˆå’Œç¼–è¾‘åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºä¸ç°æœ‰ç»Ÿä¸€MLLMå’Œä¸“ç”¨æ¨¡å‹ç›¸ç«äº‰çš„æ€§èƒ½ã€‚å‡­å€Ÿå…¶å“è¶Šæ€§èƒ½ï¼ŒILLUME+ä¸ºæœªæ¥å¤šæ¨¡æ€åº”ç”¨æä¾›äº†å¯æ‰©å±•å’Œé€šç”¨çš„åŸºç¡€ã€‚é¡¹ç›®é¡µé¢ï¼š<a target="_blank" rel="noopener" href="https://illume-unified-mllm.github.io/">https://illume-unified-mllm.github.io/</a>.</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.01934v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>åŸºäºåŒè§†è§‰æ ‡è®°æŠ€æœ¯å’Œæ‰©æ•£è§£ç å™¨çš„ILLUME+æ¨¡å‹ï¼Œæé«˜äº†æ·±åº¦è¯­ä¹‰ç†è§£å’Œé«˜ä¿çœŸå›¾åƒç”Ÿæˆçš„èƒ½åŠ›ã€‚è¯¥æ¨¡å‹åœ¨ç»Ÿä¸€æ¨¡å‹ä¸­åŒæ—¶å®ç°äº†ç†è§£ã€ç”Ÿæˆå’Œç¼–è¾‘ä¸‰ä¸ªåŸºæœ¬åŠŸèƒ½ï¼Œå…·æœ‰å‡ºè‰²çš„æ€§èƒ½è¡¨ç°ã€‚å®ƒå¼•å…¥äº†åŒè§†è§‰æ ‡è®°å™¨DualViTokï¼Œæ—¢ä¿ç•™äº†ç²¾ç»†çº¹ç†åˆä½¿æ–‡æœ¬è¯­ä¹‰å¯¹é½ï¼Œå®ç°äº†ä»ç²—åˆ°ç»†çš„å¤šæ¨¡å¼ç†è§£å’Œç”Ÿæˆç­–ç•¥ã€‚åŒæ—¶ï¼Œé‡‡ç”¨æ‰©æ•£æ¨¡å‹ä½œä¸ºå›¾åƒåˆ†è§£å™¨ï¼Œæé«˜äº†ç”Ÿæˆå›¾åƒçš„è´¨é‡å’Œè¶…åˆ†è¾¨ç‡æ•ˆç‡ã€‚ILLUME+çš„è®¾è®¡çµæ´»é«˜æ•ˆï¼Œæ”¯æŒåœ¨è§†è§‰æ ‡è®°å™¨ã€MLLMå’Œæ‰©æ•£è§£ç å™¨ä¹‹é—´è¿›è¡ŒåŠ¨æ€åˆ†è¾¨ç‡è°ƒæ•´ï¼Œå¯åœ¨å„ç§ä»»åŠ¡ä¸­å®ç°ä¸Šä¸‹æ–‡æ„ŸçŸ¥çš„å›¾åƒç¼–è¾‘å’Œç”Ÿæˆã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ILLUME+æ¨¡å‹ç»“åˆäº†åŒè§†è§‰æ ‡è®°æŠ€æœ¯å’Œæ‰©æ•£è§£ç å™¨ï¼Œæå‡äº†è¯­ä¹‰ç†è§£å’Œå›¾åƒç”Ÿæˆçš„æ•ˆæœã€‚</li>
<li>ç°æœ‰ç»Ÿä¸€æ¨¡å‹åœ¨å¤„ç†ç†è§£ã€ç”Ÿæˆå’Œç¼–è¾‘ä¸‰ä¸ªåŸºæœ¬åŠŸèƒ½æ—¶å­˜åœ¨å›°éš¾ï¼Œè€ŒILLUME+åœ¨ç»Ÿä¸€æ¨¡å‹ä¸­å®ç°äº†è¿™äº›åŠŸèƒ½ã€‚</li>
<li>DualViTokæ ‡è®°å™¨å®ç°äº†çº¹ç†å’Œè¯­ä¹‰çš„ä¿ç•™ä¸å¯¹é½ï¼Œæ”¯æŒç²—åˆ°ç»†çš„å¤šæ¨¡å¼ç†è§£å’Œç”Ÿæˆç­–ç•¥ã€‚</li>
<li>æ‰©æ•£æ¨¡å‹çš„é‡‡ç”¨æé«˜äº†å›¾åƒç”Ÿæˆçš„è´¨é‡å’Œè¶…åˆ†è¾¨ç‡æ•ˆç‡ã€‚</li>
<li>ILLUME+è®¾è®¡çµæ´»ï¼Œæ”¯æŒåŠ¨æ€è°ƒæ•´è§†è§‰æ ‡è®°å™¨ã€MLLMå’Œæ‰©æ•£è§£ç å™¨ä¹‹é—´çš„åˆ†è¾¨ç‡ã€‚</li>
<li>ILLUME+åœ¨å„ç§ä»»åŠ¡ä¸­å®ç°äº†ä¸Šä¸‹æ–‡æ„ŸçŸ¥çš„å›¾åƒç¼–è¾‘å’Œç”Ÿæˆã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.01934">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-f52434e3dacf1616e9d13b50bd68274d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-fc92edbce4f88d4f781481b545d1475d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e6b0e79eb6b7dd27324f19e04b9430c4.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Implicit-Bias-Injection-Attacks-against-Text-to-Image-Diffusion-Models"><a href="#Implicit-Bias-Injection-Attacks-against-Text-to-Image-Diffusion-Models" class="headerlink" title="Implicit Bias Injection Attacks against Text-to-Image Diffusion Models"></a>Implicit Bias Injection Attacks against Text-to-Image Diffusion Models</h2><p><strong>Authors:Huayang Huang, Xiangye Jin, Jiaxu Miao, Yu Wu</strong></p>
<p>The proliferation of text-to-image diffusion models (T2I DMs) has led to an increased presence of AI-generated images in daily life. However, biased T2I models can generate content with specific tendencies, potentially influencing peopleâ€™s perceptions. Intentional exploitation of these biases risks conveying misleading information to the public. Current research on bias primarily addresses explicit biases with recognizable visual patterns, such as skin color and gender. This paper introduces a novel form of implicit bias that lacks explicit visual features but can manifest in diverse ways across various semantic contexts. This subtle and versatile nature makes this bias challenging to detect, easy to propagate, and adaptable to a wide range of scenarios. We further propose an implicit bias injection attack framework (IBI-Attacks) against T2I diffusion models by precomputing a general bias direction in the prompt embedding space and adaptively adjusting it based on different inputs. Our attack module can be seamlessly integrated into pre-trained diffusion models in a plug-and-play manner without direct manipulation of user input or model retraining. Extensive experiments validate the effectiveness of our scheme in introducing bias through subtle and diverse modifications while preserving the original semantics. The strong concealment and transferability of our attack across various scenarios further underscore the significance of our approach. Code is available at <a target="_blank" rel="noopener" href="https://github.com/Hannah1102/IBI-attacks">https://github.com/Hannah1102/IBI-attacks</a>. </p>
<blockquote>
<p>æ–‡æœ¬åˆ°å›¾åƒçš„æ‰©æ•£æ¨¡å‹ï¼ˆT2I DMï¼‰çš„æ¿€å¢å¯¼è‡´äº†æ—¥å¸¸ç”Ÿæ´»ä¸­AIç”Ÿæˆçš„å›¾åƒçš„å‡ºç°é¢‘ç‡å¢åŠ ã€‚ç„¶è€Œï¼Œæœ‰åè§çš„T2Iæ¨¡å‹å¯èƒ½ä¼šç”Ÿæˆå…·æœ‰ç‰¹å®šå€¾å‘çš„å†…å®¹ï¼Œä»è€Œå¯èƒ½å½±å“äººä»¬çš„æ„ŸçŸ¥ã€‚æ•…æ„åˆ©ç”¨è¿™äº›åè§å¯èƒ½å‘å…¬ä¼—ä¼ é€’è¯¯å¯¼ä¿¡æ¯ã€‚ç›®å‰å…³äºåè§çš„ç ”ç©¶ä¸»è¦é›†ä¸­åœ¨å…·æœ‰å¯è¯†åˆ«è§†è§‰æ¨¡å¼çš„æ˜æ˜¾åè§ä¸Šï¼Œä¾‹å¦‚è‚¤è‰²å’Œæ€§åˆ«ã€‚æœ¬æ–‡ä»‹ç»äº†ä¸€ç§æ–°å‹éšå¼åè§ï¼Œè¿™ç§åè§æ²¡æœ‰æ˜ç¡®çš„è§†è§‰ç‰¹å¾ï¼Œä½†å¯ä»¥åœ¨å„ç§è¯­ä¹‰ä¸Šä¸‹æ–‡ä¸­ä»¥å¤šç§æ–¹å¼è¡¨ç°ã€‚è¿™ç§ç»†å¾®ä¸”å¤šåŠŸèƒ½çš„ç‰¹æ€§ä½¿è¿™ç§åè§éš¾ä»¥æ£€æµ‹ã€æ˜“äºä¼ æ’­å¹¶ä¸”é€‚åº”å¤šç§åœºæ™¯ã€‚æˆ‘ä»¬è¿›ä¸€æ­¥æå‡ºäº†é’ˆå¯¹T2Iæ‰©æ•£æ¨¡å‹çš„éšå¼åè§æ³¨å…¥æ”»å‡»æ¡†æ¶ï¼ˆIBI-Attacksï¼‰ï¼Œé€šè¿‡é¢„å…ˆè®¡ç®—æç¤ºåµŒå…¥ç©ºé—´ä¸­çš„ä¸€èˆ¬åè§æ–¹å‘å¹¶æ ¹æ®ä¸åŒçš„è¾“å…¥è¿›è¡Œè‡ªé€‚åº”è°ƒæ•´ã€‚æˆ‘ä»¬çš„æ”»å‡»æ¨¡å—å¯ä»¥æ— ç¼é›†æˆåˆ°é¢„è®­ç»ƒçš„æ‰©æ•£æ¨¡å‹ä¸­ï¼Œä»¥å³æ’å³ç”¨æ–¹å¼è¿è¡Œï¼Œæ— éœ€ç›´æ¥æ“ä½œç”¨æˆ·è¾“å…¥æˆ–é‡æ–°è®­ç»ƒæ¨¡å‹ã€‚å¤§é‡å®éªŒéªŒè¯äº†æˆ‘ä»¬çš„æ–¹æ¡ˆé€šè¿‡ç»†å¾®å’Œå¤šæ ·çš„ä¿®æ”¹å¼•å…¥åè§çš„æœ‰æ•ˆæ€§ï¼ŒåŒæ—¶ä¿ç•™åŸå§‹è¯­ä¹‰ã€‚æˆ‘ä»¬æ”»å‡»çš„å¼ºå¤§éšè”½æ€§å’Œè·¨ä¸åŒåœºæ™¯çš„è¿ç§»æ€§è¿›ä¸€æ­¥çªå‡ºäº†æˆ‘ä»¬çš„æ–¹æ³•çš„é‡è¦æ€§ã€‚ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/Hannah1102/IBI-attacks%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/Hannah1102/IBI-attacksæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.01819v1">PDF</a> Accept to CVPR 2025</p>
<p><strong>Summary</strong></p>
<p>æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹ï¼ˆT2I DMsï¼‰çš„æ™®åŠå¯¼è‡´AIç”Ÿæˆçš„å›¾åƒåœ¨æ—¥å¸¸ç”Ÿæ´»ä¸­è¶Šæ¥è¶Šå¸¸è§ã€‚ç„¶è€Œï¼Œå­˜åœ¨åè§çš„T2Iæ¨¡å‹å¯èƒ½ç”Ÿæˆå…·æœ‰ç‰¹å®šå€¾å‘çš„å†…å®¹ï¼Œä»è€Œå½±å“äººä»¬çš„æ„ŸçŸ¥ã€‚æ•…æ„åˆ©ç”¨è¿™äº›åè§å¯èƒ½å‘å…¬ä¼—ä¼ é€’è¯¯å¯¼ä¿¡æ¯ã€‚å½“å‰å…³äºåè§çš„ç ”ç©¶ä¸»è¦å…³æ³¨å…·æœ‰æ˜æ˜¾è§†è§‰å›¾æ¡ˆçš„æ˜¾æ€§åè§ï¼Œå¦‚è‚¤è‰²å’Œæ€§åˆ«ã€‚æœ¬æ–‡ä»‹ç»äº†ä¸€ç§æ–°å‹çš„éšæ€§åè§ï¼Œå®ƒç¼ºä¹æ˜ç¡®çš„è§†è§‰ç‰¹å¾ï¼Œä½†å¯ä»¥åœ¨å„ç§è¯­ä¹‰ä¸Šä¸‹æ–‡ä¸­ä»¥å¤šç§æ–¹å¼è¡¨ç°ã€‚è¿™ç§ç»†å¾®ä¸”å¤šæ‰å¤šè‰ºçš„ç‰¹æ€§ä½¿è¿™ç§åè§éš¾ä»¥æ£€æµ‹ã€æ˜“äºä¼ æ’­å¹¶ä¸”é€‚åº”èŒƒå›´å¹¿ã€‚æˆ‘ä»¬è¿›ä¸€æ­¥æå‡ºäº†é’ˆå¯¹æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹çš„éšæ€§åè§æ³¨å…¥æ”»å‡»æ¡†æ¶ï¼ˆIBI-Attacksï¼‰ï¼Œé€šè¿‡é¢„å…ˆè®¡ç®—æç¤ºåµŒå…¥ç©ºé—´ä¸­çš„ä¸€èˆ¬åè§æ–¹å‘å¹¶é’ˆå¯¹ä¸åŒè¾“å…¥è¿›è¡Œé€‚åº”æ€§è°ƒæ•´ã€‚æˆ‘ä»¬çš„æ”»å‡»æ¨¡å—å¯ä»¥æ— ç¼é›†æˆåˆ°é¢„è®­ç»ƒçš„æ‰©æ•£æ¨¡å‹ä¸­ï¼Œä»¥å³æ’å³ç”¨æ–¹å¼è¿è¡Œï¼Œæ— éœ€ç›´æ¥æ“ä½œç”¨æˆ·è¾“å…¥æˆ–æ¨¡å‹é‡æ–°è®­ç»ƒã€‚å¤§é‡å®éªŒéªŒè¯äº†æˆ‘ä»¬æ–¹æ¡ˆçš„æœ‰æ•ˆæ€§ï¼Œé€šè¿‡ç»†å¾®å’Œå¤šæ ·çš„ä¿®æ”¹å¼•å…¥åè§ï¼ŒåŒæ—¶ä¿ç•™åŸå§‹è¯­ä¹‰ã€‚æˆ‘ä»¬æ”»å‡»çš„å¼ºéšè”½æ€§å’Œè·¨åœºæ™¯çš„å¯è½¬ç§»æ€§è¿›ä¸€æ­¥çªå‡ºäº†æˆ‘ä»¬æ–¹æ³•çš„é‡è¦æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹ï¼ˆT2I DMsï¼‰åœ¨æ—¥å¸¸ç”Ÿæ´»ä¸­è¶Šæ¥è¶Šæ™®éï¼Œç”Ÿæˆçš„å†…å®¹å¯èƒ½å­˜åœ¨åè§ã€‚</li>
<li>å½“å‰ç ”ç©¶ä¸»è¦å…³æ³¨æ˜¾æ€§åè§ï¼Œå¦‚è‚¤è‰²å’Œæ€§åˆ«ï¼Œä½†å­˜åœ¨æ–°å‹éšæ€§åè§ï¼Œéš¾ä»¥æ£€æµ‹ä¸”æ˜“äºä¼ æ’­ã€‚</li>
<li>éšæ€§åè§å¯ä»¥åœ¨å„ç§è¯­ä¹‰ä¸Šä¸‹æ–‡ä¸­ä»¥å¤šç§æ–¹å¼è¡¨ç°ï¼Œé€‚åº”èŒƒå›´å¹¿ã€‚</li>
<li>è®ºæ–‡æå‡ºäº†éšæ€§åè§æ³¨å…¥æ”»å‡»æ¡†æ¶ï¼ˆIBI-Attacksï¼‰ï¼Œå¯é›†æˆåˆ°é¢„è®­ç»ƒçš„æ‰©æ•£æ¨¡å‹ä¸­ï¼Œæ— éœ€ç›´æ¥æ“ä½œç”¨æˆ·è¾“å…¥æˆ–æ¨¡å‹é‡æ–°è®­ç»ƒã€‚</li>
<li>è¯¥æ¡†æ¶é€šè¿‡ç»†å¾®å’Œå¤šæ ·çš„ä¿®æ”¹å¼•å…¥éšæ€§åè§ï¼ŒåŒæ—¶ä¿ç•™åŸå§‹è¯­ä¹‰ã€‚</li>
<li>æ”»å‡»å…·æœ‰å¼ºéšè”½æ€§å’Œè·¨åœºæ™¯çš„å¯è½¬ç§»æ€§ã€‚</li>
<li>ä»£ç å·²å‘å¸ƒåœ¨GitHubä¸Šã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.01819">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-6ee07af5670699b2de3e093ac19e9fec.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-56e8eae2229d4c3dd8044ddd7d86008c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-68726db3cd26d8a3d557e4b7b529fa26.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b91862d6fcbe708c3d40e9b0c526f699.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Hyperbolic-Diffusion-Recommender-Model"><a href="#Hyperbolic-Diffusion-Recommender-Model" class="headerlink" title="Hyperbolic Diffusion Recommender Model"></a>Hyperbolic Diffusion Recommender Model</h2><p><strong>Authors:Meng Yuan, Yutian Xiao, Wei Chen, Chu Zhao, Deqing Wang, Fuzhen Zhuang</strong></p>
<p>Diffusion models (DMs) have emerged as the new state-of-the-art family of deep generative models. To gain deeper insights into the limitations of diffusion models in recommender systems, we investigate the fundamental structural disparities between images and items. Consequently, items often exhibit distinct anisotropic and directional structures that are less prevalent in images. However, the traditional forward diffusion process continuously adds isotropic Gaussian noise, causing anisotropic signals to degrade into noise, which impairs the semantically meaningful representations in recommender systems.   Inspired by the advancements in hyperbolic spaces, we propose a novel \textit{\textbf{H}yperbolic} \textit{\textbf{D}iffusion} \textit{\textbf{R}ecommender} \textit{\textbf{M}odel} (named HDRM). Unlike existing directional diffusion methods based on Euclidean space, the intrinsic non-Euclidean structure of hyperbolic space makes it particularly well-adapted for handling anisotropic diffusion processes. In particular, we begin by formulating concepts to characterize latent directed diffusion processes within a geometrically grounded hyperbolic space. Subsequently, we propose a novel hyperbolic latent diffusion process specifically tailored for users and items. Drawing upon the natural geometric attributes of hyperbolic spaces, we impose structural restrictions on the space to enhance hyperbolic diffusion propagation, thereby ensuring the preservation of the intrinsic topology of user-item graphs. Extensive experiments on three benchmark datasets demonstrate the effectiveness of HDRM. </p>
<blockquote>
<p>æ‰©æ•£æ¨¡å‹ï¼ˆDMsï¼‰å·²ç»æˆä¸ºæœ€æ–°çš„æ·±åº¦ç”Ÿæˆæ¨¡å‹å®¶æ—ã€‚ä¸ºäº†æ·±å…¥äº†è§£æ‰©æ•£æ¨¡å‹åœ¨æ¨èç³»ç»Ÿä¸­çš„å±€é™æ€§ï¼Œæˆ‘ä»¬ç ”ç©¶äº†å›¾åƒå’Œé¡¹ç›®ä¹‹é—´æ ¹æœ¬çš„ç»“æ„å·®å¼‚ã€‚å› æ­¤ï¼Œé¡¹ç›®é€šå¸¸è¡¨ç°å‡ºç‹¬ç‰¹çš„å„å‘å¼‚æ€§å’Œæ–¹å‘æ€§ç»“æ„ï¼Œè¿™äº›åœ¨å›¾åƒä¸­è¾ƒå°‘è§ã€‚ç„¶è€Œï¼Œä¼ ç»Ÿçš„æ­£å‘æ‰©æ•£è¿‡ç¨‹ä¸æ–­æ·»åŠ å„å‘åŒæ€§çš„é«˜æ–¯å™ªå£°ï¼Œå¯¼è‡´å„å‘å¼‚æ€§çš„ä¿¡å·é™çº§ä¸ºå™ªå£°ï¼Œä»è€ŒæŸå®³äº†æ¨èç³»ç»Ÿä¸­çš„è¯­ä¹‰è¡¨ç¤ºã€‚å—åˆ°åŒæ›²ç©ºé—´å‘å±•çš„å¯å‘ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°å‹çš„\textbf{H}yperbolic \textbf{D}iffusion \textbf{R}ecommender \textbf{M}odelï¼ˆç®€ç§°HDRMï¼‰ã€‚ä¸åŸºäºæ¬§å‡ é‡Œå¾—ç©ºé—´çš„ç°æœ‰æ–¹å‘æ‰©æ•£æ–¹æ³•ä¸åŒï¼ŒåŒæ›²ç©ºé—´çš„å†…åœ¨éæ¬§å‡ é‡Œå¾—ç»“æ„ç‰¹åˆ«é€‚åˆäºå¤„ç†å„å‘å¼‚æ€§æ‰©æ•£è¿‡ç¨‹ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬é¦–å…ˆåˆ¶å®šæ¦‚å¿µï¼Œä»¥åœ¨å‡ ä½•åŸºç¡€çš„åŒæ›²ç©ºé—´ä¸­åˆ»ç”»æ½œåœ¨çš„å®šå‘æ‰©æ•£è¿‡ç¨‹ã€‚éšåï¼Œæˆ‘ä»¬é’ˆå¯¹ç”¨æˆ·å’Œé¡¹ç›®æå‡ºäº†ä¸€ç§æ–°å‹çš„åŒæ›²æ½œåœ¨æ‰©æ•£è¿‡ç¨‹ã€‚åˆ©ç”¨åŒæ›²ç©ºé—´çš„è‡ªç„¶å‡ ä½•å±æ€§ï¼Œæˆ‘ä»¬å¯¹ç©ºé—´æ–½åŠ ç»“æ„çº¦æŸï¼Œä»¥å¢å¼ºåŒæ›²æ‰©æ•£ä¼ æ’­ï¼Œä»è€Œç¡®ä¿ç”¨æˆ·-é¡¹ç›®å›¾çš„å†…åœ¨æ‹“æ‰‘çš„å®Œæ•´æ€§ã€‚åœ¨ä¸‰ä¸ªåŸºå‡†æ•°æ®é›†ä¸Šçš„å¤§é‡å®éªŒè¯æ˜äº†HDRMçš„æœ‰æ•ˆæ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.01541v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ¢è®¨äº†æ‰©æ•£æ¨¡å‹åœ¨æ¨èç³»ç»Ÿä¸­çš„å±€é™æ€§ï¼ŒæŒ‡å‡ºä¼ ç»Ÿæ‰©æ•£æ¨¡å‹åœ¨å¤„ç†å›¾åƒå’Œé¡¹ç›®æ—¶å­˜åœ¨çš„ç»“æ„å·®å¼‚é—®é¢˜ã€‚é¡¹ç›®é€šå¸¸å…·æœ‰ç‹¬ç‰¹çš„å®šå‘ç»“æ„ï¼Œä½†ä¼ ç»Ÿçš„å‰å‘æ‰©æ•£è¿‡ç¨‹æ·»åŠ çš„æ˜¯å„å‘åŒæ€§çš„é«˜æ–¯å™ªå£°ï¼Œè¿™ä¼šå¯¼è‡´å®šå‘ä¿¡å·ä¸§å¤±ã€‚ä¸ºè§£å†³æ­¤é—®é¢˜ï¼Œæœ¬ç ”ç©¶å€Ÿé‰´äº†åŒæ›²ç©ºé—´çš„ä¼˜åŠ¿ï¼Œæå‡ºäº†æ–°å‹çš„åŸºäºåŒæ›²ç©ºé—´çš„æ‰©æ•£æ¨èæ¨¡å‹ï¼ˆHDRMï¼‰ã€‚è¯¥æ¨¡å‹åˆ©ç”¨åŒæ›²ç©ºé—´çš„éæ¬§å‡ é‡Œå¾—ç»“æ„ç‰¹æ€§ï¼Œç‰¹åˆ«é€‚åˆå¤„ç†å®šå‘æ‰©æ•£è¿‡ç¨‹ï¼Œç¡®ä¿ç”¨æˆ·-é¡¹ç›®å›¾çš„å†…åœ¨æ‹“æ‰‘ç»“æ„å¾—ä»¥ä¿ç•™ã€‚å®éªŒè¯æ˜ï¼ŒHDRMåœ¨ä¸‰ä¸ªåŸºå‡†æ•°æ®é›†ä¸Šçš„è¡¨ç°å‡ä¼˜äºä¼ ç»Ÿæ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ‰©æ•£æ¨¡å‹åœ¨æ¨èç³»ç»Ÿä¸­é¢ä¸´å±€é™æ€§ï¼Œå°¤å…¶æ˜¯å¤„ç†å…·æœ‰å®šå‘ç»“æ„çš„é¡¹ç›®æ—¶ã€‚</li>
<li>ä¼ ç»Ÿçš„å‰å‘æ‰©æ•£è¿‡ç¨‹æ·»åŠ çš„æ˜¯å„å‘åŒæ€§çš„é«˜æ–¯å™ªå£°ï¼Œä¸é€‚ç”¨äºå¤„ç†å®šå‘ä¿¡å·ã€‚</li>
<li>ä¸ºè§£å†³æ­¤é—®é¢˜ï¼Œç ”ç©¶å€Ÿé‰´åŒæ›²ç©ºé—´çš„ä¼˜åŠ¿ï¼Œæå‡ºäº†æ–°å‹çš„åŸºäºåŒæ›²ç©ºé—´çš„æ‰©æ•£æ¨èæ¨¡å‹ï¼ˆHDRMï¼‰ã€‚</li>
<li>HDRMåˆ©ç”¨åŒæ›²ç©ºé—´çš„éæ¬§å‡ é‡Œå¾—ç»“æ„ç‰¹æ€§ï¼Œç‰¹åˆ«é€‚åˆå¤„ç†å®šå‘æ‰©æ•£è¿‡ç¨‹ã€‚</li>
<li>HDRMé€šè¿‡åˆ¶å®šé’ˆå¯¹ç”¨æˆ·å’Œé¡¹ç›®çš„åŒæ›²æ½œåœ¨æ‰©æ•£è¿‡ç¨‹ï¼Œç¡®ä¿äº†ç”¨æˆ·-é¡¹ç›®å›¾çš„å†…åœ¨æ‹“æ‰‘ç»“æ„å¾—ä»¥ä¿ç•™ã€‚</li>
<li>å¹¿æ³›çš„å®éªŒè¯æ˜ï¼ŒHDRMåœ¨æ¨èç³»ç»Ÿæ€§èƒ½ä¸Šä¼˜äºä¼ ç»Ÿæ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.01541">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-2c71f33ecd3ab19d66d21464bcd7a427.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-00fad0193c1bfe8f80e6c54132fb36c2.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-94a3bc9dff25e7af83478322492d6318.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-03dd31fcf13d799605c183f2c939250e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0f8d4b96b7b3ba34f5a427df451deebf.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-cb5b6507ee46ecb428a45582d9a6bb9c.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="From-Easy-to-Hard-Building-a-Shortcut-for-Differentially-Private-Image-Synthesis"><a href="#From-Easy-to-Hard-Building-a-Shortcut-for-Differentially-Private-Image-Synthesis" class="headerlink" title="From Easy to Hard: Building a Shortcut for Differentially Private Image   Synthesis"></a>From Easy to Hard: Building a Shortcut for Differentially Private Image   Synthesis</h2><p><strong>Authors:Kecen Li, Chen Gong, Xiaochen Li, Yuzhong Zhao, Xinwen Hou, Tianhao Wang</strong></p>
<p>Differentially private (DP) image synthesis aims to generate synthetic images from a sensitive dataset, alleviating the privacy leakage concerns of organizations sharing and utilizing synthetic images. Although previous methods have significantly progressed, especially in training diffusion models on sensitive images with DP Stochastic Gradient Descent (DP-SGD), they still suffer from unsatisfactory performance. In this work, inspired by curriculum learning, we propose a two-stage DP image synthesis framework, where diffusion models learn to generate DP synthetic images from easy to hard. Unlike existing methods that directly use DP-SGD to train diffusion models, we propose an easy stage in the beginning, where diffusion models learn simple features of the sensitive images. To facilitate this easy stage, we propose to use &#96;central imagesâ€™, simply aggregations of random samples of the sensitive dataset. Intuitively, although those central images do not show details, they demonstrate useful characteristics of all images and only incur minimal privacy costs, thus helping early-phase model training. We conduct experiments to present that on the average of four investigated image datasets, the fidelity and utility metrics of our synthetic images are 33.1% and 2.1% better than the state-of-the-art method. </p>
<blockquote>
<p>å·®åˆ†éšç§ï¼ˆDPï¼‰å›¾åƒåˆæˆæ—¨åœ¨ä»æ•æ„Ÿæ•°æ®é›†ä¸­ç”Ÿæˆåˆæˆå›¾åƒï¼Œä»è€Œå‡è½»ç»„ç»‡å’Œæœºæ„å…±äº«å’Œåˆ©ç”¨åˆæˆå›¾åƒæ—¶çš„éšç§æ³„éœ²æ‹…å¿§ã€‚å°½ç®¡ä¹‹å‰çš„æ–¹æ³•å·²ç»æœ‰äº†æ˜¾è‘—è¿›å±•ï¼Œç‰¹åˆ«æ˜¯åœ¨ä½¿ç”¨å·®åˆ†éšç§éšæœºæ¢¯åº¦ä¸‹é™æ³•ï¼ˆDP-SGDï¼‰å¯¹æ•æ„Ÿå›¾åƒè¿›è¡Œæ‰©æ•£æ¨¡å‹è®­ç»ƒæ–¹é¢ï¼Œä½†å®ƒä»¬ä»ç„¶å­˜åœ¨ç€æ€§èƒ½ä¸ä½³çš„é—®é¢˜ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬å—åˆ°è¯¾ç¨‹å­¦ä¹ çš„å¯å‘ï¼Œæå‡ºäº†ä¸€ä¸ªä¸¤é˜¶æ®µçš„å·®åˆ†éšç§å›¾åƒåˆæˆæ¡†æ¶ï¼Œå…¶ä¸­æ‰©æ•£æ¨¡å‹ä»ç®€å•åˆ°å¤æ‚å­¦ä¹ ç”Ÿæˆå·®åˆ†éšç§åˆæˆå›¾åƒã€‚ä¸ç°æœ‰æ–¹æ³•ç›´æ¥ä½¿ç”¨DP-SGDè®­ç»ƒæ‰©æ•£æ¨¡å‹ä¸åŒï¼Œæˆ‘ä»¬åœ¨å¼€å§‹æ—¶æå‡ºäº†ä¸€ä¸ªç®€å•é˜¶æ®µï¼Œå…¶ä¸­æ‰©æ•£æ¨¡å‹å­¦ä¹ æ•æ„Ÿå›¾åƒçš„ç®€å•ç‰¹å¾ã€‚ä¸ºäº†ä¿ƒè¿›è¿™ä¸€ç®€å•é˜¶æ®µï¼Œæˆ‘ä»¬å»ºè®®ä½¿ç”¨â€œä¸­å¿ƒå›¾åƒâ€ï¼Œå³æ•æ„Ÿæ•°æ®é›†éšæœºæ ·æœ¬çš„ç®€å•é›†åˆã€‚ç›´è§‚åœ°çœ‹ï¼Œè™½ç„¶è¿™äº›ä¸­å¿ƒå›¾åƒä¸æ˜¾ç¤ºç»†èŠ‚ï¼Œä½†å®ƒä»¬å±•ç¤ºäº†æ‰€æœ‰å›¾åƒçš„æœ‰ç”¨ç‰¹å¾å¹¶ä¸”åªäº§ç”Ÿäº†æœ€å°çš„éšç§æˆæœ¬ï¼Œä»è€Œæœ‰åŠ©äºæ—©æœŸæ¨¡å‹è®­ç»ƒã€‚æˆ‘ä»¬è¿›è¡Œå®éªŒè¡¨æ˜ï¼Œåœ¨è°ƒæŸ¥çš„å››ä¸ªå›¾åƒæ•°æ®é›†ä¸Šï¼Œæˆ‘ä»¬åˆæˆå›¾åƒçš„ä¿çœŸåº¦å’Œæ•ˆç”¨æŒ‡æ ‡å¹³å‡æ¯”ç°æœ‰æœ€å…ˆè¿›çš„æ–¹æ³•é«˜å‡º33.1%å’Œ2.1%ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.01395v1">PDF</a> Accepted at IEEE S&amp;P (Oakland) 2025; code available at   <a target="_blank" rel="noopener" href="https://github.com/SunnierLee/DP-FETA">https://github.com/SunnierLee/DP-FETA</a></p>
<p><strong>Summary</strong><br>å·®åˆ†éšç§ï¼ˆDPï¼‰å›¾åƒåˆæˆæ—¨åœ¨ä»æ•æ„Ÿæ•°æ®é›†ä¸­ç”Ÿæˆåˆæˆå›¾åƒï¼Œä»¥è§£å†³ç»„ç»‡åœ¨å…±äº«å’Œåˆ©ç”¨åˆæˆå›¾åƒæ—¶çš„éšç§æ³„éœ²é—®é¢˜ã€‚å°½ç®¡ä»¥å‰çš„æ–¹æ³•ï¼ˆç‰¹åˆ«æ˜¯åœ¨ä½¿ç”¨å·®åˆ†éšç§éšæœºæ¢¯åº¦ä¸‹é™ï¼ˆDP-SGDï¼‰è®­ç»ƒæ‰©æ•£æ¨¡å‹æ–¹é¢ï¼‰å·²ç»å–å¾—äº†é‡å¤§è¿›å±•ï¼Œä½†å®ƒä»¬ä»ç„¶æ€§èƒ½ä¸ä½³ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬å—åˆ°è¯¾ç¨‹å­¦ä¹ çš„å¯å‘ï¼Œæå‡ºäº†ä¸€ä¸ªä¸¤é˜¶æ®µçš„DPå›¾åƒåˆæˆæ¡†æ¶ï¼Œå…¶ä¸­æ‰©æ•£æ¨¡å‹ä»ç®€å•åˆ°å¤æ‚åœ°å­¦ä¹ ç”ŸæˆDPåˆæˆå›¾åƒã€‚ä¸ç›´æ¥ä½¿ç”¨DP-SGDè®­ç»ƒæ‰©æ•£æ¨¡å‹çš„æ–¹æ³•ä¸åŒï¼Œæˆ‘ä»¬åœ¨å¼€å§‹æ—¶æå‡ºäº†ä¸€ä¸ªç®€å•é˜¶æ®µï¼Œåœ¨è¿™ä¸ªé˜¶æ®µä¸­ï¼Œæ‰©æ•£æ¨¡å‹å­¦ä¹ æ•æ„Ÿå›¾åƒçš„ç®€å•ç‰¹å¾ã€‚ä¸ºäº†ä¿ƒè¿›è¿™ä¸€ç®€å•é˜¶æ®µï¼Œæˆ‘ä»¬å»ºè®®ä½¿ç”¨â€œä¸­å¿ƒå›¾åƒâ€ï¼Œå³æ•æ„Ÿæ•°æ®é›†éšæœºæ ·æœ¬çš„ç®€å•èšåˆã€‚è™½ç„¶è¿™äº›ä¸­å¿ƒå›¾åƒä¸ä¼šæ˜¾ç¤ºç»†èŠ‚ï¼Œä½†å®ƒä»¬å±•ç¤ºäº†æ‰€æœ‰å›¾åƒçš„æœ‰ç”¨ç‰¹å¾ï¼Œå¹¶ä¸”åªäº§ç”Ÿäº†æœ€å°çš„éšç§æˆæœ¬ï¼Œä»è€Œæœ‰åŠ©äºæ—©æœŸæ¨¡å‹è®­ç»ƒã€‚å®éªŒè¡¨æ˜ï¼Œåœ¨å››ä¸ªè°ƒæŸ¥çš„å›¾åƒæ•°æ®é›†ä¸Šï¼Œæˆ‘ä»¬ç”Ÿæˆçš„åˆæˆå›¾åƒçš„ä¿çœŸåº¦å’Œæ•ˆç”¨æŒ‡æ ‡æ¯”ä¹‹å‰çš„æœ€å…ˆè¿›æ–¹æ³•å¹³å‡æé«˜äº†33.1%å’Œ2.1%ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å·®åˆ†éšç§å›¾åƒåˆæˆæ—¨åœ¨è§£å†³å…±äº«å’Œåˆ©ç”¨åˆæˆå›¾åƒæ—¶çš„éšç§æ³„éœ²é—®é¢˜ã€‚</li>
<li>ç°æœ‰æ–¹æ³•åœ¨DPå›¾åƒåˆæˆä¸Šæ€§èƒ½ä¸ä½³ï¼Œå°¤å…¶æ˜¯åœ¨ä½¿ç”¨DP-SGDè®­ç»ƒæ‰©æ•£æ¨¡å‹æ—¶ã€‚</li>
<li>æå‡ºäº†ä¸€ä¸ªä¸¤é˜¶æ®µçš„DPå›¾åƒåˆæˆæ¡†æ¶ï¼Œä»ç®€å•åˆ°å¤æ‚åœ°å­¦ä¹ ç”ŸæˆDPåˆæˆå›¾åƒã€‚</li>
<li>åœ¨åˆå§‹é˜¶æ®µï¼Œä½¿ç”¨ä¸­å¿ƒå›¾åƒï¼ˆæ•æ„Ÿæ•°æ®é›†çš„éšæœºæ ·æœ¬èšåˆï¼‰æ¥ä¿ƒè¿›æ¨¡å‹å­¦ä¹ ç®€å•ç‰¹å¾ã€‚</li>
<li>ä¸­å¿ƒå›¾åƒæœ‰åŠ©äºæ—©æœŸæ¨¡å‹è®­ç»ƒï¼Œå› ä¸ºå®ƒä»¬å±•ç¤ºäº†æ‰€æœ‰å›¾åƒçš„æœ‰ç”¨ç‰¹å¾ä¸”åªäº§ç”Ÿæœ€å°éšç§æˆæœ¬ã€‚</li>
<li>å®éªŒç»“æœæ˜¾ç¤ºï¼Œåœ¨å››ä¸ªè°ƒæŸ¥çš„å›¾åƒæ•°æ®é›†ä¸Šï¼Œæ–°æ–¹æ³•çš„åˆæˆå›¾åƒåœ¨ä¿çœŸåº¦å’Œæ•ˆç”¨æŒ‡æ ‡ä¸Šæœ‰æ‰€æå‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.01395">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-58b9f71beef6f32493d3cc50f11c9131.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e3a59a108dd9892170ffcc61518b1904.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5950abc0d33401bbdd76dfc65d2f93a9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-78e83401a11da22e25f7853e9ad2999f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c8128766f7acb6dd9a39a37893db4562.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-e7218dc417e1af5c4e584533c7c216eb.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Safeguarding-Vision-Language-Models-Mitigating-Vulnerabilities-to-Gaussian-Noise-in-Perturbation-based-Attacks"><a href="#Safeguarding-Vision-Language-Models-Mitigating-Vulnerabilities-to-Gaussian-Noise-in-Perturbation-based-Attacks" class="headerlink" title="Safeguarding Vision-Language Models: Mitigating Vulnerabilities to   Gaussian Noise in Perturbation-based Attacks"></a>Safeguarding Vision-Language Models: Mitigating Vulnerabilities to   Gaussian Noise in Perturbation-based Attacks</h2><p><strong>Authors:Jiawei Wang, Yushen Zuo, Yuanjun Chai, Zhendong Liu, Yichen Fu, Yichun Feng, Kin-man Lam</strong></p>
<p>Vision-Language Models (VLMs) extend the capabilities of Large Language Models (LLMs) by incorporating visual information, yet they remain vulnerable to jailbreak attacks, especially when processing noisy or corrupted images. Although existing VLMs adopt security measures during training to mitigate such attacks, vulnerabilities associated with noise-augmented visual inputs are overlooked. In this work, we identify that missing noise-augmented training causes critical security gaps: many VLMs are susceptible to even simple perturbations such as Gaussian noise. To address this challenge, we propose Robust-VLGuard, a multimodal safety dataset with aligned &#x2F; misaligned image-text pairs, combined with noise-augmented fine-tuning that reduces attack success rates while preserving functionality of VLM. For stronger optimization-based visual perturbation attacks, we propose DiffPure-VLM, leveraging diffusion models to convert adversarial perturbations into Gaussian-like noise, which can be defended by VLMs with noise-augmented safety fine-tuning. Experimental results demonstrate that the distribution-shifting property of diffusion model aligns well with our fine-tuned VLMs, significantly mitigating adversarial perturbations across varying intensities. The dataset and code are available at <a target="_blank" rel="noopener" href="https://github.com/JarvisUSTC/DiffPure-RobustVLM">https://github.com/JarvisUSTC/DiffPure-RobustVLM</a>. </p>
<blockquote>
<p>è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰é€šè¿‡èå…¥è§†è§‰ä¿¡æ¯æ‰©å±•äº†å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„åŠŸèƒ½ï¼Œä½†å®ƒä»¬åœ¨å¤„ç†å™ªå£°æˆ–æŸåçš„å›¾åƒæ—¶ä»å®¹æ˜“é­å—è¶Šç‹±æ”»å‡»ã€‚å°½ç®¡ç°æœ‰çš„VLMsåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­é‡‡å–äº†å®‰å…¨æªæ–½æ¥å‡è½»æ­¤ç±»æ”»å‡»ï¼Œä½†ä¸å™ªå£°å¢å¼ºè§†è§‰è¾“å…¥ç›¸å…³çš„æ¼æ´å´è¢«å¿½è§†äº†ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬å‘ç°ç¼ºå°‘å™ªå£°å¢å¼ºè®­ç»ƒä¼šå¯¼è‡´å…³é”®çš„å®‰å…¨æ¼æ´ï¼šè®¸å¤šVLMsç”šè‡³å®¹æ˜“å—åˆ°é«˜æ–¯å™ªå£°ç­‰ç®€å•å¹²æ‰°çš„å½±å“ã€‚ä¸ºäº†åº”å¯¹è¿™ä¸€æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†Robust-VLGuardï¼Œè¿™æ˜¯ä¸€ä¸ªå¤šæ¨¡å¼å®‰å…¨æ•°æ®é›†ï¼ŒåŒ…å«å¯¹é½çš„&#x2F;æœªå¯¹é½çš„å›¾åƒæ–‡æœ¬å¯¹ï¼Œç»“åˆå™ªå£°å¢å¼ºå¾®è°ƒï¼Œä»¥é™ä½æ”»å‡»æˆåŠŸç‡ï¼ŒåŒæ—¶ä¿ç•™VLMçš„åŠŸèƒ½ã€‚å¯¹äºæ›´ä¼˜åŒ–çš„åŸºäºè§†è§‰æ‰°åŠ¨æ”»å‡»ï¼Œæˆ‘ä»¬æå‡ºäº†DiffPure-VLMï¼Œåˆ©ç”¨æ‰©æ•£æ¨¡å‹å°†å¯¹æŠ—æ€§æ‰°åŠ¨è½¬åŒ–ä¸ºé«˜æ–¯å‹å™ªå£°ï¼Œé€šè¿‡å…·æœ‰å™ªå£°å¢å¼ºå®‰å…¨å¾®è°ƒçš„VLMè¿›è¡Œé˜²å¾¡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæ‰©æ•£æ¨¡å‹çš„åˆ†å¸ƒè½¬ç§»å±æ€§ä¸æˆ‘ä»¬çš„å¾®è°ƒVLMséå¸¸å¥‘åˆï¼Œèƒ½åœ¨ä¸åŒå¼ºåº¦ä¸‹æ˜¾è‘—å‡è½»å¯¹æŠ—æ€§æ‰°åŠ¨ã€‚æ•°æ®é›†å’Œä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/JarvisUSTC/DiffPure-RobustVLM%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/JarvisUSTC/DiffPure-RobustVLMè·å–ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.01308v1">PDF</a> </p>
<p><strong>Summary</strong><br>     è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰é€šè¿‡èå…¥è§†è§‰ä¿¡æ¯æ‰©å±•äº†å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„åŠŸèƒ½ï¼Œä½†å®ƒä»¬ä»æ˜“å—æ”»å‡»ï¼Œç‰¹åˆ«æ˜¯åœ¨å¤„ç†å™ªå£°æˆ–æŸåçš„å›¾åƒæ—¶ã€‚å°½ç®¡ç°æœ‰VLMåœ¨è®­ç»ƒæœŸé—´é‡‡å–å®‰å…¨æªæ–½æ¥å‡è½»è¿™äº›æ”»å‡»ï¼Œä½†å™ªå£°å¢å¼ºè§†è§‰è¾“å…¥ç›¸å…³çš„æ¼æ´å´è¢«å¿½è§†ã€‚æœ¬ç ”ç©¶å‘ç°ç¼ºå¤±å™ªå£°å¢å¼ºè®­ç»ƒä¼šé€ æˆå…³é”®çš„å®‰å…¨æ¼æ´ï¼šè®¸å¤šVLMå®¹æ˜“å—åˆ°ç”šè‡³ç®€å•çš„æ‰°åŠ¨ï¼Œå¦‚é«˜æ–¯å™ªå£°çš„å½±å“ã€‚ä¸ºè§£å†³è¿™ä¸€æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†Robust-VLGuardï¼Œè¿™æ˜¯ä¸€ä¸ªå¤šæ¨¡å¼å®‰å…¨æ•°æ®é›†ï¼ŒåŒ…å«å¯¹é½&#x2F;æœªå¯¹é½çš„å›¾åƒæ–‡æœ¬å¯¹ï¼Œç»“åˆå™ªå£°å¢å¼ºå¾®è°ƒï¼Œä»¥é™ä½æ”»å‡»æˆåŠŸç‡ï¼ŒåŒæ—¶ä¿æŒVLMçš„åŠŸèƒ½æ€§ã€‚é’ˆå¯¹æ›´ä¼˜åŒ–çš„è§†è§‰æ‰°åŠ¨æ”»å‡»ï¼Œæˆ‘ä»¬æå‡ºäº†DiffPure-VLMï¼Œåˆ©ç”¨æ‰©æ•£æ¨¡å‹å°†å¯¹æŠ—æ€§æ‰°åŠ¨è½¬åŒ–ä¸ºé«˜æ–¯å‹å™ªå£°ï¼Œå¯ä»¥é€šè¿‡å™ªå£°å¢å¼ºçš„å®‰å…¨å¾®è°ƒç”±VLMè¿›è¡Œé˜²å¾¡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæ‰©æ•£æ¨¡å‹çš„åˆ†å¸ƒè½¬ç§»å±æ€§ä¸æˆ‘ä»¬çš„å¾®è°ƒVLMséå¸¸å¥‘åˆï¼Œæ˜¾è‘—å‡è½»äº†ä¸åŒå¼ºåº¦çš„å¯¹æŠ—æ€§æ‰°åŠ¨ã€‚æ•°æ®é›†å’Œä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/JarvisUSTC/DiffPure-RobustVLM%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/JarvisUSTC/DiffPure-RobustVLMè·å–ã€‚</a></p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>VLMså°½ç®¡åœ¨èå…¥è§†è§‰ä¿¡æ¯ååŠŸèƒ½å¢å¼ºï¼Œä½†ä»å­˜åœ¨å¯¹å™ªå£°æˆ–æŸåå›¾åƒçš„æ”»å‡»è„†å¼±æ€§ã€‚</li>
<li>ç°æœ‰VLMçš„å®‰å…¨æªæ–½ä¸»è¦å…³æ³¨è®­ç»ƒè¿‡ç¨‹ä¸­çš„å¯¹æŠ—æ”»å‡»é˜²æŠ¤ï¼Œä½†å¿½è§†äº†å™ªå£°å¢å¼ºè§†è§‰è¾“å…¥çš„æ¼æ´ã€‚</li>
<li>ç¼ºå¤±å™ªå£°å¢å¼ºè®­ç»ƒå¯¼è‡´VLMæ˜“å—ç®€å•æ‰°åŠ¨ï¼Œå¦‚é«˜æ–¯å™ªå£°çš„å½±å“ã€‚</li>
<li>æå‡ºRobust-VLGuardæ•°æ®é›†ä¸å™ªå£°å¢å¼ºå¾®è°ƒæ–¹æ³•ï¼Œé™ä½æ”»å‡»æˆåŠŸç‡åŒæ—¶ä¿æŒVLMåŠŸèƒ½ã€‚</li>
<li>é’ˆå¯¹ä¼˜åŒ–è§†è§‰æ‰°åŠ¨æ”»å‡»ï¼Œåˆ©ç”¨æ‰©æ•£æ¨¡å‹è½¬åŒ–å¯¹æŠ—æ€§æ‰°åŠ¨ä¸ºé«˜æ–¯å‹å™ªå£°ã€‚</li>
<li>DiffPure-VLMæ–¹æ³•é€šè¿‡å™ªå£°å¢å¼ºçš„å®‰å…¨å¾®è°ƒèƒ½æœ‰æ•ˆé˜²å¾¡è½¬åŒ–åçš„å™ªå£°ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.01308">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-ce86214a4d6408b2e819cb382fe3b769.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a559cc1bf6190fd25d58077a4bb91799.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9417fd9d3eff057332ab3a5c50f7ee49.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b3078b631421816dfb957834f0811276.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-49923dfda67b907568219914f9e733cb.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Prompting-Forgetting-Unlearning-in-GANs-via-Textual-Guidance"><a href="#Prompting-Forgetting-Unlearning-in-GANs-via-Textual-Guidance" class="headerlink" title="Prompting Forgetting: Unlearning in GANs via Textual Guidance"></a>Prompting Forgetting: Unlearning in GANs via Textual Guidance</h2><p><strong>Authors:Piyush Nagasubramaniam, Neeraj Karamchandani, Chen Wu, Sencun Zhu</strong></p>
<p>State-of-the-art generative models exhibit powerful image-generation capabilities, introducing various ethical and legal challenges to service providers hosting these models. Consequently, Content Removal Techniques (CRTs) have emerged as a growing area of research to control outputs without full-scale retraining. Recent work has explored the use of Machine Unlearning in generative models to address content removal. However, the focus of such research has been on diffusion models, and unlearning in Generative Adversarial Networks (GANs) has remained largely unexplored. We address this gap by proposing Text-to-Unlearn, a novel framework that selectively unlearns concepts from pre-trained GANs using only text prompts, enabling feature unlearning, identity unlearning, and fine-grained tasks like expression and multi-attribute removal in models trained on human faces. Leveraging natural language descriptions, our approach guides the unlearning process without requiring additional datasets or supervised fine-tuning, offering a scalable and efficient solution. To evaluate its effectiveness, we introduce an automatic unlearning assessment method adapted from state-of-the-art image-text alignment metrics, providing a comprehensive analysis of the unlearning methodology. To our knowledge, Text-to-Unlearn is the first cross-modal unlearning framework for GANs, representing a flexible and efficient advancement in managing generative model behavior. </p>
<blockquote>
<p>å½“å‰å…ˆè¿›çš„ç”Ÿæˆæ¨¡å‹å±•ç°å‡ºå¼ºå¤§çš„å›¾åƒç”Ÿæˆèƒ½åŠ›ï¼Œç»™æ‰˜ç®¡è¿™äº›æ¨¡å‹çš„æœåŠ¡æä¾›å•†å¸¦æ¥äº†å„ç§ä¼¦ç†å’Œæ³•å¾‹æŒ‘æˆ˜ã€‚å› æ­¤ï¼Œå†…å®¹åˆ é™¤æŠ€æœ¯ï¼ˆCRTsï¼‰ä½œä¸ºä¸€ä¸ªä¸æ–­å¢é•¿çš„ç ”ç©¶é¢†åŸŸå‡ºç°ï¼Œå¯ä»¥åœ¨ä¸è¿›è¡Œå…¨é¢å†è®­ç»ƒçš„æƒ…å†µä¸‹æ§åˆ¶è¾“å‡ºã€‚è¿‘æœŸçš„ç ”ç©¶æ¢ç´¢äº†ç”Ÿæˆæ¨¡å‹ä¸­æœºå™¨é—å¿˜çš„ä½¿ç”¨ä»¥å®ç°å†…å®¹åˆ é™¤ã€‚ç„¶è€Œï¼Œè¿™ç§ç ”ç©¶çš„é‡ç‚¹ä¸»è¦æ”¾åœ¨æ‰©æ•£æ¨¡å‹ä¸Šï¼Œç”Ÿæˆå¯¹æŠ—ç½‘ç»œï¼ˆGANsï¼‰ä¸­çš„é—å¿˜ç ”ç©¶ä»è¢«å¤§å¤§å¿½è§†ã€‚æˆ‘ä»¬é€šè¿‡æå‡ºText-to-Unlearnæ¥è§£å†³è¿™ä¸€é—®é¢˜ï¼Œè¿™æ˜¯ä¸€ä¸ªæ–°å‹æ¡†æ¶ï¼Œå¯ä»é¢„è®­ç»ƒçš„GANsä¸­é€‰æ‹©æ€§é—å¿˜æ¦‚å¿µï¼Œä»…ä½¿ç”¨æ–‡æœ¬æç¤ºå°±èƒ½å®ç°åŠŸèƒ½é—å¿˜ã€èº«ä»½é—å¿˜å’Œåœ¨äººè„¸æ¨¡å‹ä¸Šæ‰§è¡Œç²¾ç»†ä»»åŠ¡ï¼Œå¦‚è¡¨æƒ…å’Œå¤šå±æ€§åˆ é™¤ã€‚æˆ‘ä»¬çš„æ–¹æ³•åˆ©ç”¨è‡ªç„¶è¯­è¨€æè¿°æ¥å¼•å¯¼é—å¿˜è¿‡ç¨‹ï¼Œæ— éœ€é¢å¤–æ•°æ®é›†æˆ–ç›‘ç£å¾®è°ƒï¼Œæä¾›å¯ä¼¸ç¼©å’Œé«˜æ•ˆçš„è§£å†³æ–¹æ¡ˆã€‚ä¸ºäº†è¯„ä¼°å…¶æœ‰æ•ˆæ€§ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§è‡ªåŠ¨é—å¿˜è¯„ä¼°æ–¹æ³•ï¼Œè¯¥æ–¹æ³•æ”¹ç¼–è‡ªæœ€å…ˆè¿›çš„å›¾åƒæ–‡æœ¬å¯¹é½æŒ‡æ ‡ï¼Œå¯¹é—å¿˜æ–¹æ³•è¿›è¡Œå…¨é¢åˆ†æã€‚æ®æˆ‘ä»¬æ‰€çŸ¥ï¼ŒText-to-Unlearnæ˜¯é¦–ä¸ªç”¨äºGANsçš„è·¨æ¨¡æ€é—å¿˜æ¡†æ¶ï¼Œä»£è¡¨ç€åœ¨ç®¡ç†ç”Ÿæˆæ¨¡å‹è¡Œä¸ºæ–¹é¢çµæ´»é«˜æ•ˆçš„è¿›æ­¥ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.01218v1">PDF</a> </p>
<p><strong>Summary</strong><br>     æœ¬æ–‡ä»‹ç»äº†ç”Ÿæˆæ¨¡å‹çš„æ–°æŒ‘æˆ˜å’Œå†…å®¹ç§»é™¤æŠ€æœ¯ï¼ˆCRTsï¼‰åœ¨æ§åˆ¶è¾“å‡ºæ–¹é¢çš„é‡è¦æ€§ï¼Œé‡ç‚¹ç ”ç©¶ä½¿ç”¨æœºå™¨é—å¿˜åœ¨ç”Ÿæˆæ¨¡å‹ä¸­çš„å¯èƒ½æ€§ã€‚é’ˆå¯¹æ‰©æ•£æ¨¡å‹æå‡ºäº†Text-to-Unlearnæ¡†æ¶ï¼Œè¯¥æ¡†æ¶å¯åˆ©ç”¨æ–‡æœ¬æç¤ºé€‰æ‹©æ€§é—å¿˜é¢„è®­ç»ƒGANsä¸­çš„æ¦‚å¿µï¼Œä»è€Œå®ç°ç‰¹å¾é—å¿˜ã€èº«ä»½é—å¿˜å’Œç²¾ç»†ä»»åŠ¡ï¼Œå¦‚è¡¨æƒ…å’Œå¤šå±æ€§ç§»é™¤ç­‰ã€‚æ­¤æ–¹æ³•é€šè¿‡è‡ªç„¶è¯­è¨€æè¿°å¼•å¯¼é—å¿˜è¿‡ç¨‹ï¼Œæ— éœ€é¢å¤–æ•°æ®é›†æˆ–ç›‘ç£å¾®è°ƒï¼Œæ˜¯ä¸€ç§å¯æ‰©å±•ä¸”é«˜æ•ˆçš„è§£å†³æ–¹æ¡ˆã€‚è¯„ä¼°æ–¹æ³•é‡‡ç”¨æœ€æ–°çš„å›¾åƒæ–‡æœ¬å¯¹é½åº¦é‡æŒ‡æ ‡ã€‚Text-to-Unlearnæ˜¯é¦–ä¸ªè·¨æ¨¡æ€çš„GANé—å¿˜æ¡†æ¶ï¼Œä»£è¡¨äº†åœ¨ç®¡ç†ç”Ÿæˆæ¨¡å‹è¡Œä¸ºæ–¹é¢çš„çµæ´»å’Œé«˜æ•ˆè¿›å±•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç”Ÿæˆæ¨¡å‹å¸¦æ¥ä¼¦ç†å’Œæ³•å¾‹æŒ‘æˆ˜ï¼Œéœ€è¦å†…å®¹ç§»é™¤æŠ€æœ¯ï¼ˆCRTsï¼‰æ¥æ§åˆ¶è¾“å‡ºã€‚</li>
<li>ç›®å‰ç ”ç©¶é›†ä¸­åœ¨æ‰©æ•£æ¨¡å‹çš„æœºå™¨é—å¿˜ï¼Œè€ŒGANsä¸­çš„æœºå™¨é—å¿˜å°šæœªå¾—åˆ°å……åˆ†æ¢ç´¢ã€‚</li>
<li>Text-to-Unlearnæ¡†æ¶å¯ä»¥é€‰æ‹©æ€§é—å¿˜é¢„è®­ç»ƒGANsä¸­çš„æ¦‚å¿µï¼Œé€šè¿‡æ–‡æœ¬æç¤ºå®ç°ä¸åŒå±‚æ¬¡çš„é—å¿˜ï¼Œå¦‚ç‰¹å¾ã€èº«ä»½å’Œç²¾ç»†ä»»åŠ¡ã€‚</li>
<li>è¯¥æ–¹æ³•åˆ©ç”¨è‡ªç„¶è¯­è¨€æè¿°æ¥å¼•å¯¼é—å¿˜è¿‡ç¨‹ï¼Œæ— éœ€é¢å¤–æ•°æ®é›†æˆ–ç›‘ç£å¾®è°ƒã€‚</li>
<li>Text-to-Unlearnæ¡†æ¶é‡‡ç”¨å›¾åƒæ–‡æœ¬å¯¹é½åº¦é‡æŒ‡æ ‡è¿›è¡Œè‡ªåŠ¨è¯„ä¼°ã€‚</li>
<li>æ­¤æ¡†æ¶æ˜¯é¦–ä¸ªé’ˆå¯¹GANsçš„è·¨æ¨¡æ€é—å¿˜æ¡†æ¶ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.01218">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-ce7efed644f29823794ac569b2343bc0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-314f1d89fb12216a96e360c7bfb5fa7e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-5fe9c1683527b12a7c4c12aeb2691c2b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7a3ef7b00893ade8c040e80318e82c73.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c9292e701a6915ddc2aca931bd67329a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-422d801be0aac01f8bb39eb64f2e6abf.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-28943ea8a45a22e30fa7a6639aa783f0.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="AnimeGamer-Infinite-Anime-Life-Simulation-with-Next-Game-State-Prediction"><a href="#AnimeGamer-Infinite-Anime-Life-Simulation-with-Next-Game-State-Prediction" class="headerlink" title="AnimeGamer: Infinite Anime Life Simulation with Next Game State   Prediction"></a>AnimeGamer: Infinite Anime Life Simulation with Next Game State   Prediction</h2><p><strong>Authors:Junhao Cheng, Yuying Ge, Yixiao Ge, Jing Liao, Ying Shan</strong></p>
<p>Recent advancements in image and video synthesis have opened up new promise in generative games. One particularly intriguing application is transforming characters from anime films into interactive, playable entities. This allows players to immerse themselves in the dynamic anime world as their favorite characters for life simulation through language instructions. Such games are defined as infinite game since they eliminate predetermined boundaries and fixed gameplay rules, where players can interact with the game world through open-ended language and experience ever-evolving storylines and environments. Recently, a pioneering approach for infinite anime life simulation employs large language models (LLMs) to translate multi-turn text dialogues into language instructions for image generation. However, it neglects historical visual context, leading to inconsistent gameplay. Furthermore, it only generates static images, failing to incorporate the dynamics necessary for an engaging gaming experience. In this work, we propose AnimeGamer, which is built upon Multimodal Large Language Models (MLLMs) to generate each game state, including dynamic animation shots that depict character movements and updates to character states, as illustrated in Figure 1. We introduce novel action-aware multimodal representations to represent animation shots, which can be decoded into high-quality video clips using a video diffusion model. By taking historical animation shot representations as context and predicting subsequent representations, AnimeGamer can generate games with contextual consistency and satisfactory dynamics. Extensive evaluations using both automated metrics and human evaluations demonstrate that AnimeGamer outperforms existing methods in various aspects of the gaming experience. Codes and checkpoints are available at <a target="_blank" rel="noopener" href="https://github.com/TencentARC/AnimeGamer">https://github.com/TencentARC/AnimeGamer</a>. </p>
<blockquote>
<p>å›¾åƒå’Œè§†é¢‘åˆæˆçš„æœ€æ–°è¿›å±•ä¸ºç”Ÿæˆæ¸¸æˆé¢†åŸŸå¸¦æ¥äº†æ–°å¸Œæœ›ã€‚ä¸€ä¸ªç‰¹åˆ«æœ‰è¶£çš„åº”ç”¨æ˜¯å°†åŠ¨æ¼«ç”µå½±ä¸­çš„è§’è‰²è½¬å˜ä¸ºå¯äº’åŠ¨çš„æ¸¸æˆå®ä½“ã€‚è¿™å…è®¸ç©å®¶é€šè¿‡è¯­è¨€æŒ‡ä»¤æ²‰æµ¸åœ¨åŠ¨æ€çš„åŠ¨æ¼«ä¸–ç•Œä¸­ï¼Œæ‰®æ¼”ä»–ä»¬æœ€å–œæ¬¢çš„è§’è‰²è¿›è¡Œç”Ÿæ´»æ¨¡æ‹Ÿã€‚è¿™ç±»æ¸¸æˆè¢«å®šä¹‰ä¸ºæ— é™æ¸¸æˆï¼Œå› ä¸ºå®ƒä»¬æ¶ˆé™¤äº†é¢„å…ˆè®¾å®šçš„ç•Œé™å’Œå›ºå®šçš„æ¸¸æˆè§„åˆ™ï¼Œç©å®¶å¯ä»¥é€šè¿‡å¼€æ”¾å¼çš„è¯­è¨€ä¸æ¸¸æˆä¸–ç•Œäº’åŠ¨ï¼Œä½“éªŒä¸æ–­æ¼”å˜çš„æ•…äº‹æƒ…èŠ‚å’Œç¯å¢ƒã€‚æœ€è¿‘ï¼Œä¸€ç§å¼€åˆ›æ€§çš„æ— é™åŠ¨æ¼«ç”Ÿæ´»æ¨¡æ‹Ÿæ–¹æ³•é‡‡ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å°†å¤šè½®æ–‡æœ¬å¯¹è¯ç¿»è¯‘æˆå›¾åƒç”Ÿæˆçš„è¯­è¨€æŒ‡ä»¤ã€‚ç„¶è€Œï¼Œå®ƒå¿½ç•¥äº†å†å²è§†è§‰ä¸Šä¸‹æ–‡ï¼Œå¯¼è‡´æ¸¸æˆè¿‡ç¨‹ä¸ä¸€è‡´ã€‚æ­¤å¤–ï¼Œå®ƒåªèƒ½ç”Ÿæˆé™æ€å›¾åƒï¼Œæ— æ³•èå…¥åŠ¨æ€å…ƒç´ ï¼Œæ— æ³•ä¸ºç©å®¶æä¾›å¼•äººå…¥èƒœçš„æ¸¸æˆä½“éªŒã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†AnimeGamerï¼Œå®ƒå»ºç«‹åœ¨å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰ä¹‹ä¸Šï¼Œç”¨äºç”Ÿæˆæ¯ä¸ªæ¸¸æˆçŠ¶æ€ï¼ŒåŒ…æ‹¬æç»˜è§’è‰²åŠ¨ä½œå’ŒçŠ¶æ€æ›´æ–°çš„åŠ¨æ€åŠ¨ç”»é•œå¤´ï¼Œå¦‚å›¾1æ‰€ç¤ºã€‚æˆ‘ä»¬å¼•å…¥äº†æ–°å‹çš„åŠ¨ä½œæ„ŸçŸ¥å¤šæ¨¡æ€è¡¨ç¤ºæ³•æ¥è¡¨ç¤ºåŠ¨ç”»é•œå¤´ï¼Œå¯ä»¥ä½¿ç”¨è§†é¢‘æ‰©æ•£æ¨¡å‹å°†å…¶è§£ç æˆé«˜è´¨é‡çš„è§†é¢‘ç‰‡æ®µã€‚é€šè¿‡ä»¥å†å²åŠ¨ç”»é•œå¤´è¡¨ç¤ºä¸ºä¸Šä¸‹æ–‡å¹¶é¢„æµ‹åç»­è¡¨ç¤ºï¼ŒAnimeGamerå¯ä»¥ç”Ÿæˆå…·æœ‰ä¸Šä¸‹æ–‡ä¸€è‡´æ€§å’Œæ»¡æ„åŠ¨æ€æ€§çš„æ¸¸æˆã€‚é€šè¿‡è‡ªåŠ¨åŒ–æŒ‡æ ‡å’Œäººå·¥è¯„ä¼°çš„å¹¿æ³›è¯„ä¼°è¡¨æ˜ï¼ŒAnimeGameråœ¨æ¸¸æˆä½“éªŒçš„å„ä¸ªæ–¹é¢çš„è¡¨ç°å‡ä¼˜äºç°æœ‰æ–¹æ³•ã€‚ç›¸å…³ä»£ç å’Œæ£€æŸ¥ç‚¹å·²å‘å¸ƒåœ¨<a target="_blank" rel="noopener" href="https://github.com/TencentARC/AnimeGamer%E4%B8%8A%E3%80%82">https://github.com/TencentARC/AnimeGamerä¸Šã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.01014v1">PDF</a> Project released at: <a target="_blank" rel="noopener" href="https://howe125.github.io/AnimeGamer.github.io/">https://howe125.github.io/AnimeGamer.github.io/</a></p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å’Œå¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨åŠ¨æ¼«æ¸¸æˆä¸­çš„åº”ç”¨ã€‚é€šè¿‡ç»“åˆå›¾åƒç”Ÿæˆå’Œæ–‡æœ¬å¯¹è¯ï¼Œå®ç°äº†åŠ¨æ¼«è§’è‰²çš„æ— é™ç”Ÿå‘½æ¨¡æ‹Ÿæ¸¸æˆã€‚è¯¥æ¸¸æˆæ¶ˆé™¤äº†é¢„è®¾è¾¹ç•Œå’Œå›ºå®šè§„åˆ™ï¼Œå…è®¸ç©å®¶é€šè¿‡å¼€æ”¾å¼è¯­è¨€ä¸æ¸¸æˆä¸–ç•Œäº’åŠ¨ï¼Œä½“éªŒä¸æ–­å˜åŒ–çš„æ•…äº‹æƒ…èŠ‚å’Œç¯å¢ƒã€‚ç„¶è€Œï¼Œç°æœ‰æ–¹æ³•å¿½ç•¥äº†å†å²è§†è§‰ä¸Šä¸‹æ–‡ï¼Œå¯¼è‡´æ¸¸æˆæ€§ä¸ä¸€è‡´ï¼Œå¹¶ä¸”åªèƒ½ç”Ÿæˆé™æ€å›¾åƒã€‚å› æ­¤ï¼Œæœ¬æ–‡æå‡ºäº†AnimeGamerï¼Œåˆ©ç”¨è§†é¢‘æ‰©æ•£æ¨¡å‹ç”Ÿæˆé«˜è´¨é‡çš„è§†é¢‘å‰ªè¾‘ï¼Œå¹¶å¼•å…¥æ–°å‹çš„åŠ¨ä½œæ„ŸçŸ¥å¤šæ¨¡æ€è¡¨ç¤ºæ¥å±•ç¤ºåŠ¨ç”»é•œå¤´ï¼Œä»è€Œç”Ÿæˆå…·æœ‰ä¸Šä¸‹æ–‡ä¸€è‡´æ€§å’Œæ»¡æ„åŠ¨æ€æ€§çš„æ¸¸æˆã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹å’Œå¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹è¢«åº”ç”¨äºåŠ¨æ¼«æ¸¸æˆçš„å¼€å‘ã€‚</li>
<li>åŠ¨æ¼«è§’è‰²çš„æ— é™ç”Ÿå‘½æ¨¡æ‹Ÿæ¸¸æˆå…è®¸ç©å®¶é€šè¿‡å¼€æ”¾å¼è¯­è¨€ä¸æ¸¸æˆä¸–ç•Œäº’åŠ¨ã€‚</li>
<li>ç°æœ‰æ–¹æ³•å¿½ç•¥äº†å†å²è§†è§‰ä¸Šä¸‹æ–‡ï¼Œå¯¼è‡´æ¸¸æˆæ€§ä¸ä¸€è‡´ã€‚</li>
<li>AnimeGameråˆ©ç”¨è§†é¢‘æ‰©æ•£æ¨¡å‹ç”Ÿæˆé«˜è´¨é‡çš„è§†é¢‘å‰ªè¾‘ã€‚</li>
<li>AnimeGamerå¼•å…¥æ–°å‹çš„åŠ¨ä½œæ„ŸçŸ¥å¤šæ¨¡æ€è¡¨ç¤ºæ¥å±•ç¤ºåŠ¨ç”»é•œå¤´ã€‚</li>
<li>AnimeGamerèƒ½ç”Ÿæˆå…·æœ‰ä¸Šä¸‹æ–‡ä¸€è‡´æ€§å’Œæ»¡æ„åŠ¨æ€æ€§çš„æ¸¸æˆã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.01014">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-cd0ebf6fe3158be837dc673128def6b7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8fd29e104e47e0ff54d6807a15db67d9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-696e28d03c6550c4c5c82861b2ad2d4e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-27966b67be617eeb22bb1d6a7c5065e3.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-3f3546a6de895403e043b8e44e47447f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-2cef586b07bbc87c8a676a1da28206c4.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="TurboFill-Adapting-Few-step-Text-to-image-Model-for-Fast-Image-Inpainting"><a href="#TurboFill-Adapting-Few-step-Text-to-image-Model-for-Fast-Image-Inpainting" class="headerlink" title="TurboFill: Adapting Few-step Text-to-image Model for Fast Image   Inpainting"></a>TurboFill: Adapting Few-step Text-to-image Model for Fast Image   Inpainting</h2><p><strong>Authors:Liangbin Xie, Daniil Pakhomov, Zhonghao Wang, Zongze Wu, Ziyan Chen, Yuqian Zhou, Haitian Zheng, Zhifei Zhang, Zhe Lin, Jiantao Zhou, Chao Dong</strong></p>
<p>This paper introduces TurboFill, a fast image inpainting model that enhances a few-step text-to-image diffusion model with an inpainting adapter for high-quality and efficient inpainting. While standard diffusion models generate high-quality results, they incur high computational costs. We overcome this by training an inpainting adapter on a few-step distilled text-to-image model, DMD2, using a novel 3-step adversarial training scheme to ensure realistic, structurally consistent, and visually harmonious inpainted regions. To evaluate TurboFill, we propose two benchmarks: DilationBench, which tests performance across mask sizes, and HumanBench, based on human feedback for complex prompts. Experiments show that TurboFill outperforms both multi-step BrushNet and few-step inpainting methods, setting a new benchmark for high-performance inpainting tasks. Our project page: <a target="_blank" rel="noopener" href="https://liangbinxie.github.io/projects/TurboFill/">https://liangbinxie.github.io/projects/TurboFill/</a> </p>
<blockquote>
<p>æœ¬æ–‡ä»‹ç»äº†TurboFillï¼Œè¿™æ˜¯ä¸€ç§å¿«é€Ÿå›¾åƒä¿®å¤æ¨¡å‹ï¼Œå®ƒä¸ºå°‘æ•°å‡ æ­¥æ–‡æœ¬åˆ°å›¾åƒçš„æ‰©æ•£æ¨¡å‹æä¾›äº†ä¸€ä¸ªä¿®å¤é€‚é…å™¨ï¼Œä»¥å®ç°é«˜è´¨é‡å’Œé«˜æ•ˆçš„ä¿®å¤ã€‚è™½ç„¶æ ‡å‡†æ‰©æ•£æ¨¡å‹å¯ä»¥ç”Ÿæˆé«˜è´¨é‡çš„ç»“æœï¼Œä½†å®ƒä»¬è®¡ç®—æˆæœ¬è¾ƒé«˜ã€‚æˆ‘ä»¬é€šè¿‡ä½¿ç”¨ä¸€ç§æ–°çš„ä¸‰æ­¥å¯¹æŠ—è®­ç»ƒæ–¹æ¡ˆï¼Œåœ¨å‡ æ­¥æç‚¼çš„æ–‡æœ¬åˆ°å›¾åƒæ¨¡å‹DMD2ä¸Šè®­ç»ƒä¿®å¤é€‚é…å™¨ï¼Œä»¥äº§ç”Ÿé€¼çœŸçš„ã€ç»“æ„ä¸€è‡´ä¸”è§†è§‰ä¸Šå’Œè°çš„ä¿®å¤åŒºåŸŸæ¥å…‹æœè¿™ä¸€éš¾é¢˜ã€‚ä¸ºäº†è¯„ä¼°TurboFillï¼Œæˆ‘ä»¬æå‡ºäº†ä¸¤ä¸ªåŸºå‡†æµ‹è¯•ï¼šDilationBenchï¼Œç”¨äºæµ‹è¯•ä¸åŒæ©è†œå¤§å°çš„æ€§èƒ½ï¼›HumanBenchï¼ŒåŸºäºäººç±»åé¦ˆæ¥åº”å¯¹å¤æ‚çš„æç¤ºã€‚å®éªŒè¡¨æ˜ï¼ŒTurboFillåœ¨é«˜æ€§èƒ½ä¿®å¤ä»»åŠ¡æ–¹é¢è¡¨ç°å‡ºä¼˜äºå¤šæ­¥BrushNetå’Œå°‘æ•°å‡ æ­¥ä¿®å¤æ–¹æ³•çš„æ•ˆæœï¼Œä¸ºé«˜è´¨é‡ä¿®å¤ä»»åŠ¡æ ‘ç«‹äº†æ–°çš„åŸºå‡†ã€‚æˆ‘ä»¬çš„é¡¹ç›®é¡µé¢ï¼š<a target="_blank" rel="noopener" href="https://liangbinxie.github.io/projects/TurboFill/">https://liangbinxie.github.io/projects/TurboFill/</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.00996v1">PDF</a> Project webpage available at   <a target="_blank" rel="noopener" href="https://liangbinxie.github.io/projects/TurboFill/">https://liangbinxie.github.io/projects/TurboFill/</a></p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†TurboFillï¼Œè¿™æ˜¯ä¸€ç§å¿«é€Ÿå›¾åƒä¿®å¤æ¨¡å‹ã€‚å®ƒé€šè¿‡åœ¨ä¸€ä¸ªç®€åŒ–çš„æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹ä¸Šè®­ç»ƒä¿®å¤é€‚é…å™¨ï¼Œå®ç°äº†é«˜è´¨é‡å’Œé«˜æ•ˆçš„å›¾åƒä¿®å¤ã€‚è¯¥æ¨¡å‹é‡‡ç”¨ä¸€ç§æ–°çš„ä¸‰æ­¥å¯¹æŠ—è®­ç»ƒæ–¹æ¡ˆï¼Œç¡®ä¿ä¿®å¤åŒºåŸŸçœŸå®ã€ç»“æ„ä¸€è‡´ä¸”è§†è§‰å’Œè°ã€‚å®éªŒè¡¨æ˜ï¼ŒTurboFillåœ¨è†¨èƒ€åŸºå‡†æµ‹è¯•å’Œäººç±»åé¦ˆåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œä¸ºé«˜æ€§èƒ½å›¾åƒä¿®å¤ä»»åŠ¡è®¾å®šäº†æ–°çš„åŸºå‡†ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>TurboFillæ˜¯ä¸€ä¸ªå¿«é€Ÿå›¾åƒä¿®å¤æ¨¡å‹ï¼ŒåŸºäºç®€åŒ–çš„æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹ã€‚</li>
<li>TurboFillé€šè¿‡ä½¿ç”¨ä¿®å¤é€‚é…å™¨æé«˜æ•ˆç‡å’Œä¿®å¤è´¨é‡ã€‚</li>
<li>æ–°çš„ä¸‰æ­¥å¯¹æŠ—è®­ç»ƒæ–¹æ¡ˆç¡®ä¿ä¿®å¤åŒºåŸŸçš„çœŸå®æ€§å’Œè§†è§‰å’Œè°æ€§ã€‚</li>
<li>TurboFillåœ¨è†¨èƒ€åŸºå‡†æµ‹è¯•å’Œäººç±»åé¦ˆåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜äºå…¶ä»–æ–¹æ³•ã€‚</li>
<li>TurboFillé¡¹ç›®æä¾›äº†ä¸€ä¸ªé«˜æ•ˆã€é«˜è´¨é‡å›¾åƒä¿®å¤çš„è§£å†³æ–¹æ¡ˆã€‚</li>
<li>è¯¥æ¨¡å‹é€‚ç”¨äºå¤šç§å›¾åƒä¿®å¤ä»»åŠ¡ï¼ŒåŒ…æ‹¬å¤„ç†å¤æ‚æç¤ºçš„ä¿®å¤ä»»åŠ¡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.00996">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-ce0cdd27817814cb3f016ebc1cb6387b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-669db6627a7a3bb90a76ffffa8365123.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-ce755e2f32dbb9e8c127c5f621014800.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-ff7c45a61830a4ed0e684da442590787.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Data-free-Knowledge-Distillation-with-Diffusion-Models"><a href="#Data-free-Knowledge-Distillation-with-Diffusion-Models" class="headerlink" title="Data-free Knowledge Distillation with Diffusion Models"></a>Data-free Knowledge Distillation with Diffusion Models</h2><p><strong>Authors:Xiaohua Qi, Renda Li, Long Peng, Qiang Ling, Jun Yu, Ziyi Chen, Peng Chang, Mei Han, Jing Xiao</strong></p>
<p>Recently Data-Free Knowledge Distillation (DFKD) has garnered attention and can transfer knowledge from a teacher neural network to a student neural network without requiring any access to training data. Although diffusion models are adept at synthesizing high-fidelity photorealistic images across various domains, existing methods cannot be easiliy implemented to DFKD. To bridge that gap, this paper proposes a novel approach based on diffusion models, DiffDFKD. Specifically, DiffDFKD involves targeted optimizations in two key areas. Firstly, DiffDFKD utilizes valuable information from teacher models to guide the pre-trained diffusion modelsâ€™ data synthesis, generating datasets that mirror the training data distribution and effectively bridge domain gaps. Secondly, to reduce computational burdens, DiffDFKD introduces Latent CutMix Augmentation, an efficient technique, to enhance the diversity of diffusion model-generated images for DFKD while preserving key attributes for effective knowledge transfer. Extensive experiments validate the efficacy of DiffDFKD, yielding state-of-the-art results exceeding existing DFKD approaches. We release our code at <a target="_blank" rel="noopener" href="https://github.com/xhqi0109/DiffDFKD">https://github.com/xhqi0109/DiffDFKD</a>. </p>
<blockquote>
<p>æœ€è¿‘ï¼Œæ— æ•°æ®çŸ¥è¯†è’¸é¦ï¼ˆDFKDï¼‰å¼•èµ·äº†äººä»¬çš„å…³æ³¨ï¼Œå®ƒå¯ä»¥ä»æ•™å¸ˆç¥ç»ç½‘ç»œå‘å­¦ç”Ÿç¥ç»ç½‘ç»œè½¬ç§»çŸ¥è¯†ï¼Œè€Œæ— éœ€è®¿é—®ä»»ä½•è®­ç»ƒæ•°æ®ã€‚å°½ç®¡æ‰©æ•£æ¨¡å‹æ“…é•¿åœ¨å„ç§é¢†åŸŸåˆæˆé«˜ä¿çœŸç…§ç‰‡çº§å›¾åƒï¼Œä½†ç°æœ‰æ–¹æ³•æ— æ³•è½»æ˜“åº”ç”¨äºDFKDã€‚ä¸ºäº†å¼¥è¡¥è¿™ä¸€å·®è·ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºæ‰©æ•£æ¨¡å‹çš„æ–°æ–¹æ³•ï¼Œç§°ä¸ºDiffDFKDã€‚å…·ä½“æ¥è¯´ï¼ŒDiffDFKDæ¶‰åŠä¸¤ä¸ªå…³é”®é¢†åŸŸçš„ç›®æ ‡ä¼˜åŒ–ã€‚é¦–å…ˆï¼ŒDiffDFKDåˆ©ç”¨æ•™å¸ˆæ¨¡å‹ä¸­çš„æœ‰ä»·å€¼ä¿¡æ¯æ¥æŒ‡å¯¼é¢„è®­ç»ƒçš„æ‰©æ•£æ¨¡å‹çš„æ•°æ®åˆæˆï¼Œç”Ÿæˆåæ˜ è®­ç»ƒæ•°æ®åˆ†å¸ƒçš„æ•°æ®é›†ï¼Œå¹¶æœ‰æ•ˆåœ°å¼¥åˆé¢†åŸŸå·®è·ã€‚å…¶æ¬¡ï¼Œä¸ºäº†å‡å°‘è®¡ç®—è´Ÿæ‹…ï¼ŒDiffDFKDå¼•å…¥äº†æ½œCutMixå¢å¼ºæŠ€æœ¯ï¼Œè¿™æ˜¯ä¸€ç§é«˜æ•ˆçš„æŠ€æœ¯ï¼Œå¯ä»¥å¢å¼ºæ‰©æ•£æ¨¡å‹ç”Ÿæˆå›¾åƒçš„å¤šæ ·æ€§ï¼Œç”¨äºDFKDï¼ŒåŒæ—¶ä¿ç•™å…³é”®å±æ€§ä»¥å®ç°æœ‰æ•ˆçš„çŸ¥è¯†è½¬ç§»ã€‚å¤§é‡å®éªŒéªŒè¯äº†DiffDFKDçš„æœ‰æ•ˆæ€§ï¼Œå…¶æ€§èƒ½è¾¾åˆ°äº†è¶…è¶Šç°æœ‰DFKDæ–¹æ³•çš„æœ€å…ˆè¿›æ°´å¹³ã€‚æˆ‘ä»¬åœ¨<a target="_blank" rel="noopener" href="https://github.com/xhqi0109/DiffDFKD">https://github.com/xhqi0109/DiffDFKD</a>å‘å¸ƒäº†æˆ‘ä»¬çš„ä»£ç ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.00870v1">PDF</a> Accepted by ICME2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†åŸºäºæ‰©æ•£æ¨¡å‹çš„Data-Free Knowledge Distillationï¼ˆDFKDï¼‰æ–°æ–¹æ³•â€”â€”DiffDFKDã€‚è¯¥æ–¹æ³•é€šè¿‡åˆ©ç”¨æ•™å¸ˆæ¨¡å‹ä¸­çš„æœ‰ä»·å€¼ä¿¡æ¯ï¼ŒæŒ‡å¯¼é¢„è®­ç»ƒçš„æ‰©æ•£æ¨¡å‹è¿›è¡Œæ•°æ®åˆæˆï¼Œç”Ÿæˆåæ˜ è®­ç»ƒæ•°æ®åˆ†å¸ƒçš„æ•°æ®é›†ï¼Œæœ‰æ•ˆå¼¥åŸŸå·®è·ã€‚åŒæ—¶ï¼Œä¸ºæé«˜æ•ˆç‡ï¼ŒDiffDFKDå¼•å…¥Latent CutMix AugmentationæŠ€æœ¯ï¼Œå¢å¼ºæ‰©æ•£æ¨¡å‹ç”Ÿæˆå›¾åƒçš„å¤šæ ·æ€§ï¼ŒåŒæ—¶ä¿ç•™å…³é”®å±æ€§ä»¥å®ç°æœ‰æ•ˆçš„çŸ¥è¯†ä¼ é€’ã€‚è¯¥æ–¹æ³•åœ¨å®éªŒä¸­è¡¨ç°ä¼˜å¼‚ï¼Œè¶…è¶Šç°æœ‰DFKDæ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Data-Free Knowledge Distillation (DFKD)èƒ½å¤Ÿåœ¨æ— éœ€è®¿é—®è®­ç»ƒæ•°æ®çš„æƒ…å†µä¸‹ï¼Œä»æ•™å¸ˆç¥ç»ç½‘ç»œå‘å­¦ç”Ÿç¥ç»ç½‘ç»œä¼ é€’çŸ¥è¯†ã€‚</li>
<li>æ‰©æ•£æ¨¡å‹èƒ½å¤Ÿåˆæˆé«˜ä¿çœŸåº¦çš„è·¨åŸŸå›¾åƒã€‚</li>
<li>DiffDFKDæ˜¯ä¸€ç§åŸºäºæ‰©æ•£æ¨¡å‹çš„å…¨æ–°DFKDæ–¹æ³•ã€‚</li>
<li>DiffDFKDåˆ©ç”¨æ•™å¸ˆæ¨¡å‹ä¸­çš„ä¿¡æ¯æ¥æŒ‡å¯¼é¢„è®­ç»ƒçš„æ‰©æ•£æ¨¡å‹è¿›è¡Œæ•°æ®åˆæˆï¼Œç”Ÿæˆåæ˜ è®­ç»ƒæ•°æ®åˆ†å¸ƒçš„æ•°æ®é›†ã€‚</li>
<li>DiffDFKDé€šè¿‡ç”Ÿæˆé•œåƒè®­ç»ƒæ•°æ®åˆ†å¸ƒçš„æ•°æ®é›†ï¼Œæœ‰æ•ˆå¼¥åŸŸå·®è·ã€‚</li>
<li>DiffDFKDå¼•å…¥Latent CutMix AugmentationæŠ€æœ¯ï¼Œä»¥æé«˜æ‰©æ•£æ¨¡å‹ç”Ÿæˆå›¾åƒçš„å¤šæ ·æ€§ï¼ŒåŒæ—¶ä¿æŒå…³é”®å±æ€§çš„ä¼ é€’ã€‚</li>
<li>å®éªŒè¯æ˜ï¼ŒDiffDFKDæ–¹æ³•è¡¨ç°ä¼˜å¼‚ï¼Œè¶…è¶Šç°æœ‰DFKDæ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.00870">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-9274a7091b7e8d49a5827ae6855f3a71.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-144391497f54f507fc4384ee7d8f3115.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-116e9cfcc0ed080256dba2e47b8c39c2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9363f4de8a069f97fc3c363681d513b8.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-7bee94dc557e6a5547b53523e7ecf8f1.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="Diffusion-Model-Based-Size-Variable-Virtual-Try-On-Technology-and-Evaluation-Method"><a href="#Diffusion-Model-Based-Size-Variable-Virtual-Try-On-Technology-and-Evaluation-Method" class="headerlink" title="Diffusion Model-Based Size Variable Virtual Try-On Technology and   Evaluation Method"></a>Diffusion Model-Based Size Variable Virtual Try-On Technology and   Evaluation Method</h2><p><strong>Authors:Shufang Zhang, Hang Qian, Minxue Ni, Yaxuan Li, Wenxin Ding, Jun Liu</strong></p>
<p>With the rapid development of e-commerce, virtual try-on technology has become an essential tool to satisfy consumersâ€™ personalized clothing preferences. Diffusion-based virtual try-on systems aim to naturally align garments with target individuals, generating realistic and detailed try-on images. However, existing methods overlook the importance of garment size variations in meeting personalized consumer needs. To address this, we propose a novel virtual try-on method named SV-VTON, which introduces garment sizing concepts into virtual try-on tasks. The SV-VTON method first generates refined masks for multiple garment sizes, then integrates these masks with garment images at varying proportions, enabling virtual try-on simulations across different sizes. In addition, we developed a specialized size evaluation module to quantitatively assess the accuracy of size variations. This module calculates differences between generated size increments and international sizing standards, providing objective measurements of size accuracy. To further validate SV-VTONâ€™s generalization capability across different models, we conducted experiments on multiple SOTA Diffusion models. The results demonstrate that SV-VTON consistently achieves precise multi-size virtual try-on across various SOTA models, and validates the effectiveness and rationality of the proposed method, significantly fulfilling usersâ€™ personalized multi-size virtual try-on requirements. </p>
<blockquote>
<p>éšç€ç”µå­å•†åŠ¡çš„å¿«é€Ÿå‘å±•ï¼Œè™šæ‹Ÿè¯•ç©¿æŠ€æœ¯å·²æˆä¸ºæ»¡è¶³æ¶ˆè´¹è€…ä¸ªæ€§åŒ–æœè£…éœ€æ±‚çš„é‡è¦å·¥å…·ã€‚åŸºäºæ‰©æ•£çš„è™šæ‹Ÿè¯•ç©¿ç³»ç»Ÿæ—¨åœ¨è‡ªç„¶åœ°ä½¿æœè£…ä¸ç›®æ ‡ä¸ªä½“å¯¹é½ï¼Œç”Ÿæˆé€¼çœŸä¸”è¯¦ç»†çš„è¯•ç©¿å›¾åƒã€‚ç„¶è€Œï¼Œç°æœ‰æ–¹æ³•å¿½è§†äº†æœè£…å°ºå¯¸å˜åŒ–åœ¨æ»¡è¶³ä¸ªæ€§åŒ–æ¶ˆè´¹è€…éœ€æ±‚ä¸­çš„é‡è¦æ€§ã€‚ä¸ºè§£å†³æ­¤é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°å‹çš„è™šæ‹Ÿè¯•ç©¿æ–¹æ³•ï¼Œåä¸ºSV-VTONï¼Œå®ƒå°†æœè£…å°ºå¯¸æ¦‚å¿µå¼•å…¥è™šæ‹Ÿè¯•ç©¿ä»»åŠ¡ä¸­ã€‚SV-VTONæ–¹æ³•é¦–å…ˆä¸ºå¤šç§æœè£…å°ºå¯¸ç”Ÿæˆç²¾ç»†é®ç½©ï¼Œç„¶åå°†è¿™äº›é®ç½©ä¸ä¸åŒæ¯”ä¾‹çš„æœè£…å›¾åƒé›†æˆï¼Œä»è€Œå®ç°ä¸åŒå°ºå¯¸çš„è™šæ‹Ÿè¯•ç©¿æ¨¡æ‹Ÿã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜å¼€å‘äº†ä¸€ä¸ªä¸“é—¨çš„å°ºå¯¸è¯„ä¼°æ¨¡å—ï¼Œä»¥å®šé‡è¯„ä¼°å°ºå¯¸å˜åŒ–çš„å‡†ç¡®æ€§ã€‚è¯¥æ¨¡å—è®¡ç®—ç”Ÿæˆå°ºå¯¸å¢é‡ä¸å›½é™…å°ºå¯¸æ ‡å‡†ä¹‹é—´çš„å·®å¼‚ï¼Œä¸ºå°ºå¯¸ç²¾åº¦æä¾›å®¢è§‚æµ‹é‡ã€‚ä¸ºäº†è¿›ä¸€æ­¥éªŒè¯SV-VTONåœ¨ä¸åŒæ¨¡å‹ä¸Šçš„æ³›åŒ–èƒ½åŠ›ï¼Œæˆ‘ä»¬åœ¨å¤šä¸ªæœ€æ–°æ‰©æ•£æ¨¡å‹ä¸Šè¿›è¡Œäº†å®éªŒã€‚ç»“æœè¡¨æ˜ï¼ŒSV-VTONåœ¨å„ç§æœ€æ–°æ¨¡å‹ä¸Šå‡å®ç°äº†ç²¾ç¡®çš„å¤šå°ºå¯¸è™šæ‹Ÿè¯•ç©¿ï¼ŒéªŒè¯äº†æ‰€ææ–¹æ³•çš„æœ‰æ•ˆæ€§å’Œåˆç†æ€§ï¼Œå……åˆ†æ»¡è¶³äº†ç”¨æˆ·çš„ä¸ªæ€§åŒ–å¤šå°ºå¯¸è™šæ‹Ÿè¯•ç©¿éœ€æ±‚ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.00562v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>åŸºäºæ‰©æ•£æ¨¡å‹çš„è™šæ‹Ÿè¯•ç©¿æŠ€æœ¯ï¼Œä¸ºæ»¡è¶³æ¶ˆè´¹è€…çš„ä¸ªæ€§åŒ–æœè£…éœ€æ±‚æä¾›äº†é‡è¦å·¥å…·ã€‚ç°æœ‰æ–¹æ³•å¿½ç•¥äº†æœè£…å°ºå¯¸å˜åŒ–çš„é‡è¦æ€§ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§åä¸ºSV-VTONçš„æ–°å‹è™šæ‹Ÿè¯•ç©¿æ–¹æ³•ï¼Œå¼•å…¥æœè£…å°ºå¯¸æ¦‚å¿µï¼Œç”Ÿæˆä¸åŒå°ºå¯¸çš„ç²¾ç»†æ©è†œï¼Œå¹¶å°†å…¶ä¸æœè£…å›¾åƒæŒ‰ä¸åŒæ¯”ä¾‹èåˆï¼Œå®ç°ä¸åŒå°ºå¯¸çš„è™šæ‹Ÿè¯•ç©¿æ¨¡æ‹Ÿã€‚æˆ‘ä»¬è¿˜å¼€å‘äº†ä¸€ä¸ªä¸“é—¨çš„å°ºå¯¸è¯„ä¼°æ¨¡å—ï¼Œå®šé‡è¯„ä¼°å°ºå¯¸å˜åŒ–çš„å‡†ç¡®æ€§ã€‚å®éªŒè¡¨æ˜ï¼ŒSV-VTONåœ¨ä¸åŒSOTAæ‰©æ•£æ¨¡å‹ä¸Šå®ç°äº†ç²¾ç¡®çš„å¤šå°ºå¯¸è™šæ‹Ÿè¯•ç©¿ï¼ŒéªŒè¯äº†æ–¹æ³•çš„æœ‰æ•ˆæ€§å’Œåˆç†æ€§ï¼Œå……åˆ†æ»¡è¶³äº†ç”¨æˆ·çš„ä¸ªæ€§åŒ–å¤šå°ºå¯¸è™šæ‹Ÿè¯•ç©¿éœ€æ±‚ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è™šæ‹Ÿè¯•ç©¿æŠ€æœ¯å·²æˆä¸ºæ»¡è¶³æ¶ˆè´¹è€…ä¸ªæ€§åŒ–æœè£…éœ€æ±‚çš„é‡è¦å·¥å…·ã€‚</li>
<li>ç°æœ‰è™šæ‹Ÿè¯•ç©¿æ–¹æ³•å¿½ç•¥äº†æœè£…å°ºå¯¸å˜åŒ–çš„é‡è¦æ€§ã€‚</li>
<li>SV-VTONæ–¹æ³•å¼•å…¥æœè£…å°ºå¯¸æ¦‚å¿µï¼Œç”Ÿæˆä¸åŒå°ºå¯¸çš„ç²¾ç»†æ©è†œã€‚</li>
<li>SV-VTONæ–¹æ³•å°†ä¸åŒå°ºå¯¸çš„æ©è†œä¸æœè£…å›¾åƒèåˆï¼Œå®ç°å¤šå°ºå¯¸è™šæ‹Ÿè¯•ç©¿ã€‚</li>
<li>å¼€å‘äº†ä¸€ä¸ªä¸“é—¨çš„å°ºå¯¸è¯„ä¼°æ¨¡å—ï¼Œä»¥å®šé‡è¯„ä¼°å°ºå¯¸å˜åŒ–çš„å‡†ç¡®æ€§ã€‚</li>
<li>SV-VTONåœ¨ä¸åŒSOTAæ‰©æ•£æ¨¡å‹ä¸Šå®ç°äº†ç²¾ç¡®çš„å¤šå°ºå¯¸è™šæ‹Ÿè¯•ç©¿ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.00562">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-7643b6c42935833d796a9e09cdb558ff.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-fb266029db41a4fc2d60f300a95d9faf.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9df426ff5fefca75b735693cc4b09dc3.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-8b59e0daced40d8092599213159a7ab6.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="Distilling-Multi-view-Diffusion-Models-into-3D-Generators"><a href="#Distilling-Multi-view-Diffusion-Models-into-3D-Generators" class="headerlink" title="Distilling Multi-view Diffusion Models into 3D Generators"></a>Distilling Multi-view Diffusion Models into 3D Generators</h2><p><strong>Authors:Hao Qin, Luyuan Chen, Ming Kong, Mengxu Lu, Qiang Zhu</strong></p>
<p>We introduce DD3G, a formulation that Distills a multi-view Diffusion model (MV-DM) into a 3D Generator using gaussian splatting. DD3G compresses and integrates extensive visual and spatial geometric knowledge from the MV-DM by simulating its ordinary differential equation (ODE) trajectory, ensuring the distilled generator generalizes better than those trained solely on 3D data. Unlike previous amortized optimization approaches, we align the MV-DM and 3D generator representation spaces to transfer the teacherâ€™s probabilistic flow to the student, thus avoiding inconsistencies in optimization objectives caused by probabilistic sampling. The introduction of probabilistic flow and the coupling of various attributes in 3D Gaussians introduce challenges in the generation process. To tackle this, we propose PEPD, a generator consisting of Pattern Extraction and Progressive Decoding phases, which enables efficient fusion of probabilistic flow and converts a single image into 3D Gaussians within 0.06 seconds. Furthermore, to reduce knowledge loss and overcome sparse-view supervision, we design a joint optimization objective that ensures the quality of generated samples through explicit supervision and implicit verification. Leveraging existing 2D generation models, we compile 120k high-quality RGBA images for distillation. Experiments on synthetic and public datasets demonstrate the effectiveness of our method. Our project is available at: <a target="_blank" rel="noopener" href="https://qinbaigao.github.io/DD3G_project/">https://qinbaigao.github.io/DD3G_project/</a> </p>
<blockquote>
<p>æˆ‘ä»¬ä»‹ç»äº†DD3Gï¼Œè¿™æ˜¯ä¸€ç§é€šè¿‡é«˜æ–¯æ‹¼è´´æŠ€æœ¯å°†å¤šè§†å›¾æ‰©æ•£æ¨¡å‹ï¼ˆMV-DMï¼‰è’¸é¦åˆ°3Dç”Ÿæˆå™¨çš„æ–¹æ³•ã€‚DD3Gé€šè¿‡æ¨¡æ‹ŸMV-DMçš„å¸¸å¾®åˆ†æ–¹ç¨‹ï¼ˆODEï¼‰è½¨è¿¹ï¼Œå‹ç¼©å¹¶æ•´åˆäº†å¤§é‡çš„è§†è§‰å’Œç©ºé—´å‡ ä½•çŸ¥è¯†ï¼Œç¡®ä¿è’¸é¦å‡ºçš„ç”Ÿæˆå™¨æ¯”ä»…åŸºäº3Dæ•°æ®è®­ç»ƒçš„ç”Ÿæˆå™¨å…·æœ‰æ›´å¥½çš„æ³›åŒ–æ€§èƒ½ã€‚ä¸å‰æœŸçš„æ‘Šé”€ä¼˜åŒ–æ–¹æ³•ä¸åŒï¼Œæˆ‘ä»¬å¯¹MV-DMå’Œ3Dç”Ÿæˆå™¨çš„è¡¨ç¤ºç©ºé—´è¿›è¡Œå¯¹é½ï¼Œå°†æ•™å¸ˆçš„æ¦‚ç‡æµä¼ è¾“ç»™å­¦ç”Ÿï¼Œä»è€Œé¿å…äº†ç”±æ¦‚ç‡é‡‡æ ·å¼•èµ·çš„ä¼˜åŒ–ç›®æ ‡ä¸ä¸€è‡´çš„é—®é¢˜ã€‚æ¦‚ç‡æµçš„å¼•å…¥å’Œ3Dé«˜æ–¯ä¸­å„ç§å±æ€§çš„è€¦åˆç»™ç”Ÿæˆè¿‡ç¨‹å¸¦æ¥äº†æŒ‘æˆ˜ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†PEPDç”Ÿæˆå™¨ï¼Œå®ƒç”±æ¨¡å¼æå–å’Œæ¸è¿›è§£ç ä¸¤ä¸ªé˜¶æ®µç»„æˆï¼Œèƒ½å¤Ÿé«˜æ•ˆåœ°èåˆæ¦‚ç‡æµï¼Œå¹¶åœ¨0.06ç§’å†…å°†å•å¹…å›¾åƒè½¬æ¢ä¸º3Dé«˜æ–¯ã€‚æ­¤å¤–ï¼Œä¸ºäº†å‡å°‘çŸ¥è¯†æŸå¤±å¹¶å…‹æœç¨€ç–è§†å›¾ç›‘ç£ï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸€ä¸ªè”åˆä¼˜åŒ–ç›®æ ‡ï¼Œé€šè¿‡æ˜ç¡®ç›‘ç£å’Œéšæ€§éªŒè¯ç¡®ä¿ç”Ÿæˆæ ·æœ¬çš„è´¨é‡ã€‚æˆ‘ä»¬åˆ©ç”¨ç°æœ‰çš„2Dç”Ÿæˆæ¨¡å‹ï¼Œç¼–è¯‘äº†12ä¸‡å¼ é«˜è´¨é‡RGBAå›¾åƒè¿›è¡Œè’¸é¦ã€‚åœ¨åˆæˆæ•°æ®é›†å’Œå…¬å¼€æ•°æ®é›†ä¸Šçš„å®éªŒè¯æ˜äº†æˆ‘ä»¬çš„æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚æˆ‘ä»¬çš„é¡¹ç›®åœ¨[<a target="_blank" rel="noopener" href="https://qinbaigao.github.io/DD3G_project/]%E4%B8%8A%E5%8F%AF%E7%94%A8%E3%80%82">https://qinbaigao.github.io/DD3G_project/]ä¸Šå¯ç”¨ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.00457v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†DD3Gæ–¹æ³•ï¼Œå®ƒå°†å¤šè§†å›¾æ‰©æ•£æ¨¡å‹ï¼ˆMV-DMï¼‰è’¸é¦åˆ°3Dç”Ÿæˆå™¨ä¸­ã€‚é€šè¿‡é«˜æ–¯æ‹¼è´´æŠ€æœ¯ï¼ŒDD3Gèƒ½å¤Ÿä»MV-DMä¸­æå–å’Œæ•´åˆä¸°å¯Œçš„è§†è§‰å’Œç©ºé—´å‡ ä½•çŸ¥è¯†ï¼Œæ¨¡æ‹Ÿå…¶å¸¸å¾®åˆ†æ–¹ç¨‹ï¼ˆODEï¼‰è½¨è¿¹ï¼Œç¡®ä¿è’¸é¦åçš„ç”Ÿæˆå™¨æ¯”ä»…ä½¿ç”¨3Dæ•°æ®è®­ç»ƒçš„ç”Ÿæˆå™¨å…·æœ‰æ›´å¥½çš„æ³›åŒ–èƒ½åŠ›ã€‚è¯¥æ–¹æ³•é€šè¿‡å¯¹é½MV-DMå’Œ3Dç”Ÿæˆå™¨çš„è¡¨ç¤ºç©ºé—´ï¼Œé¿å…äº†ç”±äºæ¦‚ç‡é‡‡æ ·å¯¼è‡´çš„ä¼˜åŒ–ç›®æ ‡ä¸ä¸€è‡´ã€‚ä¸ºåº”å¯¹3Dé«˜æ–¯ä¸­çš„æ¦‚ç‡æµå’Œå„å±æ€§è€¦åˆå¸¦æ¥çš„ç”ŸæˆæŒ‘æˆ˜ï¼Œæå‡ºäº†PEPDç”Ÿæˆå™¨ã€‚å®éªŒè¯æ˜ï¼Œè¯¥æ–¹æ³•åœ¨åˆæˆå’Œå…¬å¼€æ•°æ®é›†ä¸Šå‡æœ‰æ•ˆã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>DD3Gæ˜¯ä¸€ç§å°†å¤šè§†å›¾æ‰©æ•£æ¨¡å‹ï¼ˆMV-DMï¼‰è’¸é¦åˆ°3Dç”Ÿæˆå™¨çš„æ–¹æ³•ã€‚</li>
<li>é€šè¿‡é«˜æ–¯æ‹¼è´´æŠ€æœ¯ï¼ŒDD3Gèƒ½å¤Ÿä»MV-DMä¸­æå–è§†è§‰å’Œç©ºé—´å‡ ä½•çŸ¥è¯†ã€‚</li>
<li>DD3Gæ¨¡æ‹ŸMV-DMçš„ODEè½¨è¿¹ï¼Œæé«˜ç”Ÿæˆå™¨çš„æ³›åŒ–èƒ½åŠ›ã€‚</li>
<li>å¯¹é½MV-DMå’Œ3Dç”Ÿæˆå™¨çš„è¡¨ç¤ºç©ºé—´ï¼Œé¿å…ä¼˜åŒ–ç›®æ ‡çš„ä¸ä¸€è‡´æ€§ã€‚</li>
<li>é¢å¯¹3Dé«˜æ–¯ä¸­çš„æ¦‚ç‡æµå’Œå„å±æ€§è€¦åˆçš„æŒ‘æˆ˜ï¼Œæå‡ºPEPDç”Ÿæˆå™¨ã€‚</li>
<li>DD3Gåˆ©ç”¨ç°æœ‰2Dç”Ÿæˆæ¨¡å‹ç¼–è¯‘é«˜è´¨é‡RGBAå›¾åƒç”¨äºè’¸é¦ã€‚</li>
<li>å®éªŒè¯æ˜DD3Gåœ¨åˆæˆå’Œå…¬å¼€æ•°æ®é›†ä¸Šå‡æœ‰æ•ˆã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.00457">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-b1072ca818acce967788bf801b1c2be1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-dd5ced50961b209c9ff6ef2cd5c0663e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f7b487ff167a29735997ac5d402d1d2d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-6eb6f5bc327dbaba705f3566712c6fc2.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-6b572b2194b81b3883cafe56abae3366.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="Towards-Physically-Plausible-Video-Generation-via-VLM-Planning"><a href="#Towards-Physically-Plausible-Video-Generation-via-VLM-Planning" class="headerlink" title="Towards Physically Plausible Video Generation via VLM Planning"></a>Towards Physically Plausible Video Generation via VLM Planning</h2><p><strong>Authors:Xindi Yang, Baolu Li, Yiming Zhang, Zhenfei Yin, Lei Bai, Liqian Ma, Zhiyong Wang, Jianfei Cai, Tien-Tsin Wong, Huchuan Lu, Xu Jia</strong></p>
<p>Video diffusion models (VDMs) have advanced significantly in recent years, enabling the generation of highly realistic videos and drawing the attention of the community in their potential as world simulators. However, despite their capabilities, VDMs often fail to produce physically plausible videos due to an inherent lack of understanding of physics, resulting in incorrect dynamics and event sequences. To address this limitation, we propose a novel two-stage image-to-video generation framework that explicitly incorporates physics. In the first stage, we employ a Vision Language Model (VLM) as a coarse-grained motion planner, integrating chain-of-thought and physics-aware reasoning to predict a rough motion trajectories&#x2F;changes that approximate real-world physical dynamics while ensuring the inter-frame consistency. In the second stage, we use the predicted motion trajectories&#x2F;changes to guide the video generation of a VDM. As the predicted motion trajectories&#x2F;changes are rough, noise is added during inference to provide freedom to the VDM in generating motion with more fine details. Extensive experimental results demonstrate that our framework can produce physically plausible motion, and comparative evaluations highlight the notable superiority of our approach over existing methods. More video results are available on our Project Page: <a target="_blank" rel="noopener" href="https://madaoer.github.io/projects/physically_plausible_video_generation">https://madaoer.github.io/projects/physically_plausible_video_generation</a>. </p>
<blockquote>
<p>è§†é¢‘æ‰©æ•£æ¨¡å‹ï¼ˆVDMsï¼‰è¿‘å¹´æ¥å–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œèƒ½å¤Ÿç”Ÿæˆé«˜åº¦é€¼çœŸçš„è§†é¢‘ï¼Œå¹¶ä½œä¸ºä¸–ç•Œæ¨¡æ‹Ÿå™¨å¼•èµ·äº†ç¤¾åŒºçš„å¹¿æ³›å…³æ³¨ã€‚ç„¶è€Œï¼Œå°½ç®¡VDMså…·æœ‰å¼ºå¤§çš„èƒ½åŠ›ï¼Œä½†ç”±äºå¯¹ç‰©ç†çš„å›ºæœ‰ç†è§£ä¸è¶³ï¼Œå®ƒä»¬å¾€å¾€æ— æ³•ç”Ÿæˆç‰©ç†ä¸Šåˆç†çš„è§†é¢‘ï¼Œå¯¼è‡´åŠ¨æ€å’Œäº‹ä»¶åºåˆ—ä¸æ­£ç¡®ã€‚ä¸ºäº†è§£å†³è¿™ä¸€å±€é™æ€§ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„ä¸¤é˜¶æ®µå›¾åƒåˆ°è§†é¢‘ç”Ÿæˆæ¡†æ¶ï¼Œè¯¥æ¡†æ¶æ˜¾å¼åœ°ç»“åˆäº†ç‰©ç†å­¦ã€‚åœ¨ç¬¬ä¸€é˜¶æ®µï¼Œæˆ‘ä»¬é‡‡ç”¨è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰ä½œä¸ºç²—ç²’åº¦è¿åŠ¨è§„åˆ’å™¨ï¼Œé€šè¿‡é›†æˆæ€è€ƒå’Œç‰©ç†æ„ŸçŸ¥æ¨ç†æ¥é¢„æµ‹ç²—ç•¥çš„è¿åŠ¨è½¨è¿¹&#x2F;å˜åŒ–ï¼Œè¿™äº›é¢„æµ‹èƒ½å¤Ÿè¿‘ä¼¼ç°å®ä¸–ç•Œä¸­çš„ç‰©ç†åŠ¨æ€åŒæ—¶ç¡®ä¿å¸§é—´ä¸€è‡´æ€§ã€‚åœ¨ç¬¬äºŒé˜¶æ®µï¼Œæˆ‘ä»¬ä½¿ç”¨é¢„æµ‹çš„è¿åŠ¨è½¨è¿¹&#x2F;å˜åŒ–æ¥æŒ‡å¯¼VDMçš„è§†é¢‘ç”Ÿæˆã€‚ç”±äºé¢„æµ‹çš„è¿åŠ¨è½¨è¿¹&#x2F;å˜åŒ–æ˜¯ç²—ç•¥çš„ï¼Œå› æ­¤åœ¨æ¨ç†è¿‡ç¨‹ä¸­åŠ å…¥äº†å™ªå£°ï¼Œä¸ºVDMç”Ÿæˆå…·æœ‰æ›´å¤šç»†èŠ‚çš„è¿åŠ¨æä¾›äº†è‡ªç”±åº¦ã€‚å¤§é‡çš„å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ¡†æ¶å¯ä»¥äº§ç”Ÿç‰©ç†ä¸Šåˆç†çš„è¿åŠ¨ï¼Œå¹¶ä¸”å¯¹æ¯”è¯„ä¼°å‡¸æ˜¾äº†æˆ‘ä»¬æ–¹æ³•åœ¨ç°æœ‰æŠ€æœ¯ä¸Šçš„æ˜¾è‘—ä¼˜åŠ¿ã€‚æ›´å¤šè§†é¢‘ç»“æœè¯·è®¿é—®æˆ‘ä»¬çš„é¡¹ç›®é¡µé¢ï¼š<a target="_blank" rel="noopener" href="https://madaoer.github.io/projects/physically_plausible_video_generation">https://madaoer.github.io/projects/physically_plausible_video_generation</a>ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.23368v2">PDF</a> 18 pages, 11 figures</p>
<p><strong>Summary</strong></p>
<p>è¿‘æœŸè§†é¢‘æ‰©æ•£æ¨¡å‹ï¼ˆVDMsï¼‰åœ¨ç”Ÿæˆé«˜åº¦é€¼çœŸçš„è§†é¢‘æ–¹é¢å–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œå¼•èµ·äº†ç¤¾åŒºå¯¹å…¶ä½œä¸ºä¸–ç•Œæ¨¡æ‹Ÿå™¨æ½œåŠ›çš„å…³æ³¨ã€‚ç„¶è€Œï¼Œç”±äºç¼ºä¹å¯¹ç‰©ç†çš„ç†è§£ï¼ŒVDMså¾€å¾€æ— æ³•ç”Ÿæˆç‰©ç†ä¸Šåˆç†çš„è§†é¢‘ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§æ–°é¢–çš„ä¸¤é˜¶æ®µå›¾åƒåˆ°è§†é¢‘ç”Ÿæˆæ¡†æ¶ï¼Œæ˜ç¡®èå…¥äº†ç‰©ç†ã€‚ç¬¬ä¸€é˜¶æ®µä½¿ç”¨è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰ä½œä¸ºç²—ç•¥è¿åŠ¨è§„åˆ’å™¨ï¼Œç»“åˆæ€ç»´é“¾å’Œç‰©ç†æ„ŸçŸ¥æ¨ç†ï¼Œé¢„æµ‹è¿‘ä¼¼çœŸå®ä¸–ç•Œç‰©ç†åŠ¨æ€çš„ç²—ç³™è¿åŠ¨è½¨è¿¹&#x2F;å˜åŒ–ï¼Œç¡®ä¿å¸§é—´ä¸€è‡´æ€§ã€‚ç¬¬äºŒé˜¶æ®µåˆ©ç”¨é¢„æµ‹çš„è¿åŠ¨è½¨è¿¹&#x2F;å˜åŒ–å¼•å¯¼VDMçš„è§†é¢‘ç”Ÿæˆã€‚ç”±äºé¢„æµ‹çš„è¿åŠ¨è½¨è¿¹&#x2F;å˜åŒ–æ˜¯ç²—ç•¥çš„ï¼Œå› æ­¤åœ¨æ¨ç†è¿‡ç¨‹ä¸­åŠ å…¥äº†å™ªå£°ï¼Œä¸ºVDMç”Ÿæˆå…·æœ‰æ›´å¤šç»†èŠ‚çš„è¿åŠ¨æä¾›äº†è‡ªç”±ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ¡†æ¶èƒ½ç”Ÿæˆç‰©ç†ä¸Šåˆç†çš„è¿åŠ¨ï¼Œä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”å…·æœ‰æ˜¾è‘—ä¼˜åŠ¿ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è§†é¢‘æ‰©æ•£æ¨¡å‹ï¼ˆVDMsï¼‰åœ¨ç”Ÿæˆé€¼çœŸè§†é¢‘æ–¹é¢å–å¾—æ˜¾è‘—è¿›å±•ï¼Œä½†ä½œä¸ºä¸–ç•Œæ¨¡æ‹Ÿå™¨ä»å­˜åœ¨ç‰©ç†åˆç†æ€§é—®é¢˜ã€‚</li>
<li>æå‡ºçš„ä¸¤é˜¶æ®µå›¾åƒåˆ°è§†é¢‘ç”Ÿæˆæ¡†æ¶èå…¥ç‰©ç†ï¼Œè§£å†³VDMsçš„å›ºæœ‰ç¼ºé™·ã€‚</li>
<li>ç¬¬ä¸€é˜¶æ®µä½¿ç”¨è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰è¿›è¡Œç²—ç•¥è¿åŠ¨è§„åˆ’ï¼Œç»“åˆæ€ç»´é“¾å’Œç‰©ç†æ„ŸçŸ¥æ¨ç†é¢„æµ‹è¿åŠ¨è½¨è¿¹ã€‚</li>
<li>ç¬¬äºŒé˜¶æ®µåˆ©ç”¨é¢„æµ‹çš„è¿åŠ¨è½¨è¿¹å¼•å¯¼VDMç”Ÿæˆè§†é¢‘ã€‚</li>
<li>é¢„æµ‹çš„è¿åŠ¨è½¨è¿¹æ˜¯ç²—ç•¥çš„ï¼Œå› æ­¤åœ¨æ¨ç†è¿‡ç¨‹ä¸­åŠ å…¥å™ªå£°ï¼Œå¢åŠ VDMç”Ÿæˆè¿åŠ¨çš„ç»†èŠ‚è‡ªç”±åº¦ã€‚</li>
<li>å®éªŒè¯æ˜è¯¥æ¡†æ¶èƒ½ç”Ÿæˆç‰©ç†ä¸Šåˆç†çš„è¿åŠ¨ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.23368">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-9a706d651493dcdc95cba43025ee37fd.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0b04ae2d0e383733eb4d82dfa222dfdd.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-5f7af36f51e83f4b35c713450e1da9c9.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-6a8879d6375211516ac02353e31b767b.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="Target-Aware-Video-Diffusion-Models"><a href="#Target-Aware-Video-Diffusion-Models" class="headerlink" title="Target-Aware Video Diffusion Models"></a>Target-Aware Video Diffusion Models</h2><p><strong>Authors:Taeksoo Kim, Hanbyul Joo</strong></p>
<p>We present a target-aware video diffusion model that generates videos from an input image in which an actor interacts with a specified target while performing a desired action. The target is defined by a segmentation mask and the desired action is described via a text prompt. Unlike existing controllable image-to-video diffusion models that often rely on dense structural or motion cues to guide the actorâ€™s movements toward the target, our target-aware model requires only a simple mask to indicate the target, leveraging the generalization capabilities of pretrained models to produce plausible actions. This makes our method particularly effective for human-object interaction (HOI) scenarios, where providing precise action guidance is challenging, and further enables the use of video diffusion models for high-level action planning in applications such as robotics. We build our target-aware model by extending a baseline model to incorporate the target mask as an additional input. To enforce target awareness, we introduce a special token that encodes the targetâ€™s spatial information within the text prompt. We then fine-tune the model with our curated dataset using a novel cross-attention loss that aligns the cross-attention maps associated with this token with the input target mask. To further improve performance, we selectively apply this loss to the most semantically relevant transformer blocks and attention regions. Experimental results show that our target-aware model outperforms existing solutions in generating videos where actors interact accurately with the specified targets. We further demonstrate its efficacy in two downstream applications: video content creation and zero-shot 3D HOI motion synthesis. </p>
<blockquote>
<p>æˆ‘ä»¬æå‡ºäº†ä¸€ç§ç›®æ ‡æ„ŸçŸ¥è§†é¢‘æ‰©æ•£æ¨¡å‹ï¼Œè¯¥æ¨¡å‹ä»è¾“å…¥å›¾åƒç”Ÿæˆè§†é¢‘ï¼Œå…¶ä¸­æ¼”å‘˜åœ¨ä¸æŒ‡å®šç›®æ ‡äº’åŠ¨æ—¶æ‰§è¡ŒæŒ‡å®šåŠ¨ä½œã€‚ç›®æ ‡ç”±åˆ†å‰²æ©è†œå®šä¹‰ï¼Œæ‰€éœ€åŠ¨ä½œåˆ™é€šè¿‡æ–‡æœ¬æç¤ºæè¿°ã€‚ä¸ç°æœ‰çš„å¯æ§å›¾åƒåˆ°è§†é¢‘æ‰©æ•£æ¨¡å‹ä¸åŒï¼Œè¿™äº›æ¨¡å‹é€šå¸¸ä¾èµ–äºå¯†é›†çš„ç»“æ„æˆ–è¿åŠ¨çº¿ç´¢æ¥å¼•å¯¼æ¼”å‘˜çš„åŠ¨ä½œæœå‘ç›®æ ‡ï¼Œæˆ‘ä»¬çš„ç›®æ ‡æ„ŸçŸ¥æ¨¡å‹ä»…éœ€è¦ç®€å•çš„æ©è†œæ¥æŒ‡ç¤ºç›®æ ‡ï¼Œå¹¶åˆ©ç”¨é¢„è®­ç»ƒæ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›æ¥äº§ç”Ÿåˆç†çš„åŠ¨ä½œã€‚è¿™ä½¿å¾—æˆ‘ä»¬çš„æ–¹æ³•åœ¨äººæœºäº¤äº’ï¼ˆHOIï¼‰åœºæ™¯ä¸­ç‰¹åˆ«æœ‰æ•ˆï¼Œåœ¨è¿™äº›åœºæ™¯ä¸­ï¼Œæä¾›ç²¾ç¡®çš„åŠ¨ä½œæŒ‡å¯¼æ˜¯å…·æœ‰æŒ‘æˆ˜æ€§çš„ï¼Œå¹¶ä¸”è¿›ä¸€æ­¥å¯ç”¨äº†è§†é¢‘æ‰©æ•£æ¨¡å‹åœ¨è¯¸å¦‚æœºå™¨äººæŠ€æœ¯çš„é«˜çº§åŠ¨ä½œè§„åˆ’ä¸­çš„åº”ç”¨ã€‚æˆ‘ä»¬é€šè¿‡å°†ç›®æ ‡æ©è†œä½œä¸ºé™„åŠ è¾“å…¥æ¥æ‰©å±•åŸºçº¿æ¨¡å‹ï¼Œæ„å»ºäº†æˆ‘ä»¬çš„ç›®æ ‡æ„ŸçŸ¥æ¨¡å‹ã€‚ä¸ºäº†å¼ºåˆ¶å®æ–½ç›®æ ‡æ„ŸçŸ¥ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ªç‰¹æ®Šä»¤ç‰Œï¼Œè¯¥ä»¤ç‰Œåœ¨æ–‡æœ¬æç¤ºä¸­ç¼–ç ç›®æ ‡çš„ç©ºé—´ä¿¡æ¯ã€‚ç„¶åï¼Œæˆ‘ä»¬ä½¿ç”¨æ–°çš„è·¨æ³¨æ„æŸå¤±æ¥å¾®è°ƒæˆ‘ä»¬çš„æ•°æ®é›†ä¸Šçš„æ¨¡å‹ï¼Œè¯¥æŸå¤±å°†ä¸æ­¤ä»¤ç‰Œç›¸å…³çš„è·¨æ³¨æ„å›¾ä¸è¾“å…¥ç›®æ ‡æ©è†œå¯¹é½ã€‚ä¸ºäº†è¿›ä¸€æ­¥æé«˜æ€§èƒ½ï¼Œæˆ‘ä»¬é€‰æ‹©æ€§åœ°å°†æ­¤æŸå¤±åº”ç”¨äºæœ€è¯­ä¹‰ç›¸å…³çš„å˜å‹å™¨å—å’Œæ³¨æ„åŒºåŸŸã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„ç›®æ ‡æ„ŸçŸ¥æ¨¡å‹åœ¨ç”Ÿæˆæ¼”å‘˜ä¸æŒ‡å®šç›®æ ‡å‡†ç¡®äº’åŠ¨çš„è§†é¢‘æ–¹é¢ä¼˜äºç°æœ‰è§£å†³æ–¹æ¡ˆã€‚æˆ‘ä»¬è¿›ä¸€æ­¥é€šè¿‡ä¸¤ä¸ªä¸‹æ¸¸åº”ç”¨è¯æ˜äº†å…¶æœ‰æ•ˆæ€§ï¼šè§†é¢‘å†…å®¹åˆ›å»ºå’Œé›¶å°„å‡»3Däººæœºäº¤äº’è¿åŠ¨åˆæˆã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.18950v2">PDF</a> The project page is available at <a target="_blank" rel="noopener" href="https://taeksuu.github.io/tavid/">https://taeksuu.github.io/tavid/</a></p>
<p><strong>æ‘˜è¦</strong></p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§ç›®æ ‡æ„ŸçŸ¥è§†é¢‘æ‰©æ•£æ¨¡å‹ï¼Œè¯¥æ¨¡å‹èƒ½å¤Ÿä»è¾“å…¥å›¾åƒä¸­ç”Ÿæˆè§†é¢‘ï¼Œå…¶ä¸­æ¼”å‘˜ä¸æŒ‡å®šç›®æ ‡è¿›è¡Œäº¤äº’å¹¶æ‰§è¡ŒæŒ‡å®šåŠ¨ä½œã€‚ç›®æ ‡é€šè¿‡åˆ†å‰²æ©è†œå®šä¹‰ï¼Œæ‰€éœ€åŠ¨ä½œåˆ™é€šè¿‡æ–‡æœ¬æç¤ºæè¿°ã€‚ä¸ç°æœ‰çš„å¯æ§å›¾åƒåˆ°è§†é¢‘æ‰©æ•£æ¨¡å‹ä¸åŒï¼Œè¿™äº›æ¨¡å‹é€šå¸¸ä¾èµ–äºå¯†é›†çš„ç»“æ„æˆ–è¿åŠ¨çº¿ç´¢æ¥å¼•å¯¼æ¼”å‘˜å‘ç›®æ ‡ç§»åŠ¨ï¼Œæˆ‘ä»¬çš„ç›®æ ‡æ„ŸçŸ¥æ¨¡å‹ä»…éœ€è¦ç®€å•çš„æ©è†œæ¥æŒ‡ç¤ºç›®æ ‡ï¼Œå¹¶åˆ©ç”¨é¢„è®­ç»ƒæ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›æ¥äº§ç”Ÿå¯è¡ŒåŠ¨ä½œã€‚è¿™ä½¿å¾—æˆ‘ä»¬çš„æ–¹æ³•åœ¨äººæœºäº¤äº’åœºæ™¯ä¸­ç‰¹åˆ«æœ‰æ•ˆï¼Œå…¶ä¸­æä¾›ç²¾ç¡®çš„åŠ¨ä½œæŒ‡å¯¼å…·æœ‰æŒ‘æˆ˜æ€§ï¼Œå¹¶ä¸”è¿›ä¸€æ­¥ä½¿è§†é¢‘æ‰©æ•£æ¨¡å‹åœ¨è¯¸å¦‚æœºå™¨äººæŠ€æœ¯çš„é«˜çº§åŠ¨ä½œè§„åˆ’åº”ç”¨ä¸­ä½¿ç”¨æˆä¸ºå¯èƒ½ã€‚æˆ‘ä»¬é€šè¿‡å°†ç›®æ ‡æ©è†œä½œä¸ºé™„åŠ è¾“å…¥æ¥æ‰©å±•åŸºçº¿æ¨¡å‹æ„å»ºæˆ‘ä»¬çš„ç›®æ ‡æ„ŸçŸ¥æ¨¡å‹ã€‚ä¸ºäº†å¼ºåˆ¶æ‰§è¡Œç›®æ ‡æ„ŸçŸ¥ï¼Œæˆ‘ä»¬å¼•å…¥ä¸€ä¸ªç‰¹æ®Šä»¤ç‰Œï¼Œè¯¥ä»¤ç‰Œåœ¨æ–‡æœ¬æç¤ºä¸­ç¼–ç ç›®æ ‡çš„ç©ºé—´ä¿¡æ¯ã€‚ç„¶åï¼Œæˆ‘ä»¬ä½¿ç”¨æ–°çš„è·¨æ³¨æ„åŠ›æŸå¤±æ¥å¾®è°ƒæ¨¡å‹ï¼Œè¯¥æŸå¤±ä½¿æˆ‘ä»¬çš„è·¨æ³¨æ„åŠ›å›¾ä¸è¾“å…¥ç›®æ ‡æ©è†œç›¸å…³è”ã€‚ä¸ºäº†è¿›ä¸€æ­¥æé«˜æ€§èƒ½ï¼Œæˆ‘ä»¬é€‰æ‹©æ€§åœ°å°†æ­¤æŸå¤±åº”ç”¨äºæœ€è¯­ä¹‰ç›¸å…³çš„å˜å‹å™¨å—å’Œæ³¨æ„åŠ›åŒºåŸŸã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„ç›®æ ‡æ„ŸçŸ¥æ¨¡å‹åœ¨ç”Ÿæˆæ¼”å‘˜ä¸æŒ‡å®šç›®æ ‡å‡†ç¡®äº¤äº’çš„è§†é¢‘æ–¹é¢ä¼˜äºç°æœ‰è§£å†³æ–¹æ¡ˆã€‚æˆ‘ä»¬è¿˜é€šè¿‡ä¸¤ä¸ªä¸‹æ¸¸åº”ç”¨è¿›ä¸€æ­¥è¯æ˜äº†å…¶æœ‰æ•ˆæ€§ï¼šè§†é¢‘å†…å®¹åˆ›å»ºå’Œé›¶é•œå¤´3Däººæœºäº¤äº’è¿åŠ¨åˆæˆã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>æå‡ºäº†ä¸€ç§ç›®æ ‡æ„ŸçŸ¥è§†é¢‘æ‰©æ•£æ¨¡å‹ï¼Œèƒ½å¤ŸåŸºäºè¾“å…¥å›¾åƒç”Ÿæˆè§†é¢‘ï¼Œå…¶ä¸­æ¼”å‘˜ä¸æŒ‡å®šç›®æ ‡è¿›è¡Œäº¤äº’ã€‚</li>
<li>ä»…é€šè¿‡ç®€å•æ©è†œå®šä¹‰ç›®æ ‡ï¼Œæ— éœ€å¯†é›†çš„ç»“æ„æˆ–è¿åŠ¨çº¿ç´¢ã€‚</li>
<li>åˆ©ç”¨é¢„è®­ç»ƒæ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›äº§ç”Ÿå¯è¡ŒåŠ¨ä½œï¼Œç‰¹åˆ«é€‚ç”¨äºäººæœºäº¤äº’åœºæ™¯ã€‚</li>
<li>é€šè¿‡å¼•å…¥ç‰¹æ®Šä»¤ç‰Œå’Œè·¨æ³¨æ„åŠ›æŸå¤±æœºåˆ¶æ¥å¼ºåŒ–æ¨¡å‹å¯¹ç›®æ ‡çš„è®¤è¯†ã€‚</li>
<li>é€‰æ‹©æ€§åœ°åº”ç”¨æŸå¤±ä»¥æé«˜æœ€è¯­ä¹‰ç›¸å…³éƒ¨åˆ†çš„æ€§èƒ½ã€‚</li>
<li>åœ¨ç”Ÿæˆæ¼”å‘˜ä¸æŒ‡å®šç›®æ ‡å‡†ç¡®äº¤äº’çš„è§†é¢‘æ–¹é¢ï¼Œä¼˜äºç°æœ‰è§£å†³æ–¹æ¡ˆã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.18950">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-8d055e0abf2be79da543eaf2b207e9fa.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e0689334fcc83e4a844b9dd7f2c78cad.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-984c49e727dcf862943098a2b0b01e11.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c6a06fe99785103110350b8067363c31.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ce374f79c976ed47c260611aa08f479b.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="Adapting-Video-Diffusion-Models-for-Time-Lapse-Microscopy"><a href="#Adapting-Video-Diffusion-Models-for-Time-Lapse-Microscopy" class="headerlink" title="Adapting Video Diffusion Models for Time-Lapse Microscopy"></a>Adapting Video Diffusion Models for Time-Lapse Microscopy</h2><p><strong>Authors:Alexander Holmberg, Nils Mechtel, Wei Ouyang</strong></p>
<p>We present a domain adaptation of video diffusion models to generate highly realistic time-lapse microscopy videos of cell division in HeLa cells. Although state-of-the-art generative video models have advanced significantly for natural videos, they remain underexplored in microscopy domains. To address this gap, we fine-tune a pretrained video diffusion model on microscopy-specific sequences, exploring three conditioning strategies: (1) text prompts derived from numeric phenotypic measurements (e.g., proliferation rates, migration speeds, cell-death frequencies), (2) direct numeric embeddings of phenotype scores, and (3) image-conditioned generation, where an initial microscopy frame is extended into a complete video sequence. Evaluation using biologically meaningful morphological, proliferation, and migration metrics demonstrates that fine-tuning substantially improves realism and accurately captures critical cellular behaviors such as mitosis and migration. Notably, the fine-tuned model also generalizes beyond the training horizon, generating coherent cell dynamics even in extended sequences. However, precisely controlling specific phenotypic characteristics remains challenging, highlighting opportunities for future work to enhance conditioning methods. Our results demonstrate the potential for domain-specific fine-tuning of generative video models to produce biologically plausible synthetic microscopy data, supporting applications such as in-silico hypothesis testing and data augmentation. </p>
<blockquote>
<p>æˆ‘ä»¬å‘ˆç°äº†ä¸€ä¸ªè§†é¢‘æ‰©æ•£æ¨¡å‹çš„é¢†åŸŸé€‚åº”æ€§åº”ç”¨ï¼Œç”¨äºç”Ÿæˆé«˜åº¦é€¼çœŸçš„ç»†èƒåˆ†è£‚æ—¶é—´æ¨ç§»æ˜¾å¾®é•œè§†é¢‘ã€‚å°½ç®¡æœ€å…ˆè¿›çš„ç”Ÿæˆå¼è§†é¢‘æ¨¡å‹åœ¨è‡ªç„¶è§†é¢‘æ–¹é¢å–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œä½†åœ¨æ˜¾å¾®é•œé¢†åŸŸä»ç„¶ç ”ç©¶ä¸è¶³ã€‚ä¸ºäº†å¡«è¡¥è¿™ä¸€ç©ºç™½ï¼Œæˆ‘ä»¬å¯¹é¢„è®­ç»ƒçš„è§†é¢‘æ‰©æ•£æ¨¡å‹è¿›è¡Œå¾®è°ƒï¼Œä»¥é€‚åº”æ˜¾å¾®é•œç‰¹å®šåºåˆ—ï¼Œæ¢ç´¢äº†ä¸‰ç§æ¡ä»¶ç­–ç•¥ï¼šï¼ˆ1ï¼‰æ ¹æ®æ•°å€¼è¡¨å‹æµ‹é‡å¾—å‡ºçš„æ–‡æœ¬æç¤ºï¼ˆä¾‹å¦‚å¢æ®–ç‡ã€è¿ç§»é€Ÿåº¦ã€ç»†èƒæ­»äº¡é¢‘ç‡ï¼‰ï¼›ï¼ˆ2ï¼‰ç›´æ¥å¯¹è¡¨å‹åˆ†æ•°è¿›è¡Œæ•°å€¼åµŒå…¥ï¼›ï¼ˆ3ï¼‰å›¾åƒæ¡ä»¶ä¸‹çš„ç”Ÿæˆï¼Œå…¶ä¸­å°†åˆå§‹æ˜¾å¾®é•œå›¾åƒæ‰©å±•ä¸ºä¸€ä¸ªå®Œæ•´çš„è§†é¢‘åºåˆ—ã€‚ä½¿ç”¨å…·æœ‰ç”Ÿç‰©å­¦æ„ä¹‰çš„å½¢æ€å­¦ã€å¢æ®–å’Œè¿ç§»æŒ‡æ ‡çš„è¯„ä¼°è¡¨æ˜ï¼Œå¾®è°ƒå¤§å¤§æé«˜äº†çœŸå®æ€§ï¼Œå¹¶å‡†ç¡®æ•æ‰äº†å…³é”®çš„ç»†èƒè¡Œä¸ºï¼Œå¦‚åˆ†è£‚å’Œè¿ç§»ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œç»è¿‡å¾®è°ƒåçš„æ¨¡å‹åœ¨è®­ç»ƒèŒƒå›´ä¹‹å¤–ä¹Ÿå…·æœ‰é€šç”¨æ€§ï¼Œå³ä½¿åœ¨æ‰©å±•åºåˆ—ä¸­ä¹Ÿèƒ½äº§ç”Ÿè¿è´¯çš„ç»†èƒåŠ¨æ€ã€‚ç„¶è€Œï¼Œç²¾ç¡®æ§åˆ¶ç‰¹å®šçš„è¡¨å‹ç‰¹å¾ä»ç„¶å…·æœ‰æŒ‘æˆ˜æ€§ï¼Œè¿™çªæ˜¾äº†æœªæ¥å¢å¼ºæ¡ä»¶æ–¹æ³•çš„æœºé‡ã€‚æˆ‘ä»¬çš„ç»“æœå±•ç¤ºäº†é¢†åŸŸç‰¹å®šå¾®è°ƒç”Ÿæˆè§†é¢‘æ¨¡å‹çš„æ½œåŠ›ï¼Œå¯ä»¥ç”Ÿæˆç”Ÿç‰©å­¦ä¸Šåˆç†çš„åˆæˆæ˜¾å¾®é•œæ•°æ®ï¼Œæ”¯æŒè¯¸å¦‚ä½“å¤–å‡è®¾æµ‹è¯•å’Œæ•°æ®å¢å¼ºç­‰åº”ç”¨ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.18583v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†å¯¹è§†é¢‘æ‰©æ•£æ¨¡å‹çš„é¢†åŸŸé€‚åº”ï¼Œä»¥ç”Ÿæˆé«˜åº¦é€¼çœŸçš„æ—¶é—´æ¨ç§»æ˜¾å¾®é•œè§†é¢‘ï¼Œå±•ç¤ºç»†èƒåˆ†è£‚è¿‡ç¨‹ã€‚æ–‡ç« è§£å†³äº†å½“å‰ä¸»æµç”Ÿæˆè§†é¢‘æ¨¡å‹åœ¨æ˜¾å¾®é•œé¢†åŸŸæ¢ç´¢ä¸è¶³çš„é—®é¢˜ï¼Œé€šè¿‡å¯¹é¢„è®­ç»ƒçš„è§†é¢‘æ‰©æ•£æ¨¡å‹è¿›è¡Œå¾®è°ƒï¼Œæ¢ç´¢äº†ä¸‰ç§æ¡ä»¶ç­–ç•¥ï¼š1ï¼‰æ¥è‡ªæ•°å€¼è¡¨å‹æµ‹é‡çš„æ–‡æœ¬æç¤ºï¼›2ï¼‰ç°è±¡å‹è¯„åˆ†çš„ç›´æ¥æ•°å€¼åµŒå…¥ï¼›3ï¼‰å›¾åƒæ¡ä»¶ç”Ÿæˆã€‚è¯„ä¼°æ˜¾ç¤ºï¼Œå¾®è°ƒåæ¨¡å‹æ›´èƒ½çœŸå®åæ˜ ç»†èƒè¡Œä¸ºï¼Œå¦‚åˆ†è£‚å’Œè¿ç§»ç­‰ã€‚è™½ç„¶æ¨¡å‹èƒ½å¤Ÿç”Ÿæˆè¿è´¯çš„ç»†èƒåŠ¨æ€ï¼Œä½†åœ¨ç²¾ç¡®æ§åˆ¶ç‰¹å®šè¡¨å‹ç‰¹å¾æ–¹é¢ä»å­˜åœ¨æŒ‘æˆ˜ã€‚æœ¬æ–‡å±•ç¤ºäº†é¢†åŸŸç‰¹å®šå¾®è°ƒç”Ÿæˆè§†é¢‘æ¨¡å‹çš„æ½œåŠ›ï¼Œå¯äº§ç”Ÿç”Ÿç‰©å­¦ä¸Šåˆç†çš„æ˜¾å¾®é•œæ•°æ®ï¼Œæ”¯æŒå‡è®¾æµ‹è¯•å’Œæ•°æ®å¢å¼ºç­‰åº”ç”¨ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æœ¬æ–‡å®ç°äº†è§†é¢‘æ‰©æ•£æ¨¡å‹çš„é¢†åŸŸé€‚åº”ï¼Œç”Ÿæˆé€¼çœŸçš„æ—¶é—´æ¨ç§»æ˜¾å¾®é•œè§†é¢‘ï¼Œå±•ç¤ºç»†èƒåˆ†è£‚è¿‡ç¨‹ã€‚</li>
<li>é€šè¿‡å¾®è°ƒé¢„è®­ç»ƒçš„è§†é¢‘æ‰©æ•£æ¨¡å‹ï¼Œæ¢ç´¢äº†ä¸‰ç§æ¡ä»¶ç­–ç•¥æ¥é€‚åº”æ˜¾å¾®é•œæ•°æ®ç‰¹æ€§ã€‚</li>
<li>è¯„ä¼°è¡¨æ˜å¾®è°ƒæ¨¡å‹åœ¨æ•æ‰ç»†èƒè¡Œä¸ºæ–¹é¢æ›´ä¸ºçœŸå®ï¼Œå¦‚åˆ†è£‚å’Œè¿ç§»ç­‰ã€‚</li>
<li>æ¨¡å‹çš„é€šç”¨æ€§è‰¯å¥½ï¼Œèƒ½å¤Ÿåœ¨è®­ç»ƒèŒƒå›´ä¹‹å¤–ç”Ÿæˆè¿è´¯çš„ç»†èƒåŠ¨æ€åºåˆ—ã€‚</li>
<li>å°½ç®¡æ¨¡å‹è¡¨ç°ä¼˜å¼‚ï¼Œä½†åœ¨ç²¾ç¡®æ§åˆ¶ç‰¹å®šè¡¨å‹ç‰¹å¾æ–¹é¢ä»å­˜åœ¨æŒ‘æˆ˜ï¼Œéœ€è¦æœªæ¥æ”¹è¿›ã€‚</li>
<li>é¢†åŸŸç‰¹å®šå¾®è°ƒç”Ÿæˆè§†é¢‘æ¨¡å‹å…·æœ‰æ½œåŠ›ï¼Œèƒ½å¤Ÿäº§ç”Ÿç”Ÿç‰©å­¦ä¸Šåˆç†çš„åˆæˆæ˜¾å¾®é•œæ•°æ®ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.18583">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-f2f42c15cf0168979ec5b2219da92326.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6a22349d600f03d4cf94348fc400cee5.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c7b0c6b8320afd10d25f4c857613fcf9.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="Att-Adapter-A-Robust-and-Precise-Domain-Specific-Multi-Attributes-T2I-Diffusion-Adapter-via-Conditional-Variational-Autoencoder"><a href="#Att-Adapter-A-Robust-and-Precise-Domain-Specific-Multi-Attributes-T2I-Diffusion-Adapter-via-Conditional-Variational-Autoencoder" class="headerlink" title="Att-Adapter: A Robust and Precise Domain-Specific Multi-Attributes T2I   Diffusion Adapter via Conditional Variational Autoencoder"></a>Att-Adapter: A Robust and Precise Domain-Specific Multi-Attributes T2I   Diffusion Adapter via Conditional Variational Autoencoder</h2><p><strong>Authors:Wonwoong Cho, Yan-Ying Chen, Matthew Klenk, David I. Inouye, Yanxia Zhang</strong></p>
<p>Text-to-Image (T2I) Diffusion Models have achieved remarkable performance in generating high quality images. However, enabling precise control of continuous attributes, especially multiple attributes simultaneously, in a new domain (e.g., numeric values like eye openness or car width) with text-only guidance remains a significant challenge. To address this, we introduce the Attribute (Att) Adapter, a novel plug-and-play module designed to enable fine-grained, multi-attributes control in pretrained diffusion models. Our approach learns a single control adapter from a set of sample images that can be unpaired and contain multiple visual attributes. The Att-Adapter leverages the decoupled cross attention module to naturally harmonize the multiple domain attributes with text conditioning. We further introduce Conditional Variational Autoencoder (CVAE) to the Att-Adapter to mitigate overfitting, matching the diverse nature of the visual world. Evaluations on two public datasets show that Att-Adapter outperforms all LoRA-based baselines in controlling continuous attributes. Additionally, our method enables a broader control range and also improves disentanglement across multiple attributes, surpassing StyleGAN-based techniques. Notably, Att-Adapter is flexible, requiring no paired synthetic data for training, and is easily scalable to multiple attributes within a single model. </p>
<blockquote>
<p>æ–‡æœ¬åˆ°å›¾åƒï¼ˆT2Iï¼‰æ‰©æ•£æ¨¡å‹åœ¨ç”Ÿæˆé«˜è´¨é‡å›¾åƒæ–¹é¢å–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½ã€‚ç„¶è€Œï¼Œå¦‚ä½•åœ¨æ–°çš„é¢†åŸŸï¼ˆå¦‚çœ¼å¼€åº¦æˆ–æ±½è½¦å®½åº¦ç­‰æ•°å€¼ï¼‰å®ç°è¿ç»­å±æ€§çš„ç²¾ç¡®æ§åˆ¶ï¼Œç‰¹åˆ«æ˜¯åŒæ—¶æ§åˆ¶å¤šä¸ªå±æ€§ï¼Œä»…é€šè¿‡æ–‡æœ¬æŒ‡å¯¼ä»æ˜¯ä¸€ä¸ªå·¨å¤§æŒ‘æˆ˜ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†å±æ€§ï¼ˆAttï¼‰é€‚é…å™¨ï¼Œè¿™æ˜¯ä¸€ç§æ–°å‹å³æ’å³ç”¨æ¨¡å—ï¼Œæ—¨åœ¨åœ¨é¢„è®­ç»ƒçš„æ‰©æ•£æ¨¡å‹ä¸­å®ç°ç²¾ç»†çš„å¤šå±æ€§æ§åˆ¶ã€‚æˆ‘ä»¬çš„æ–¹æ³•ä»ä¸€ç»„æ ·æœ¬å›¾åƒä¸­å­¦ä¹ å•ä¸ªæ§åˆ¶é€‚é…å™¨ï¼Œè¿™äº›å›¾åƒå¯ä»¥æ˜¯æœªé…å¯¹çš„ï¼Œå¹¶åŒ…å«å¤šä¸ªè§†è§‰å±æ€§ã€‚Att-Adapteråˆ©ç”¨è§£è€¦äº¤å‰æ³¨æ„åŠ›æ¨¡å—ï¼Œè‡ªç„¶åœ°åè°ƒå¤šä¸ªåŸŸå±æ€§ä¸æ–‡æœ¬æ¡ä»¶ã€‚æˆ‘ä»¬è¿›ä¸€æ­¥å°†æ¡ä»¶å˜åˆ†è‡ªåŠ¨ç¼–ç å™¨ï¼ˆCVAEï¼‰å¼•å…¥åˆ°Att-Adapterä¸­ï¼Œä»¥å‡è½»è¿‡æ‹Ÿåˆé—®é¢˜ï¼Œé€‚åº”è§†è§‰ä¸–ç•Œçš„å¤šæ ·æ€§ã€‚åœ¨ä¸¤ä¸ªå…¬å…±æ•°æ®é›†ä¸Šçš„è¯„ä¼°è¡¨æ˜ï¼ŒAtt-Adapteråœ¨æ§åˆ¶è¿ç»­å±æ€§æ–¹é¢ä¼˜äºæ‰€æœ‰åŸºäºLoRAçš„åŸºçº¿ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬çš„æ–¹æ³•æ‰©å¤§äº†æ§åˆ¶èŒƒå›´ï¼Œå¹¶æ”¹è¿›äº†å¤šä¸ªå±æ€§ä¹‹é—´çš„è§£çº ç¼ ï¼Œè¶…è¶Šäº†StyleGANæŠ€æœ¯ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼ŒAtt-Adapteréå¸¸çµæ´»ï¼Œæ— éœ€é…å¯¹åˆæˆæ•°æ®è¿›è¡Œè®­ç»ƒï¼Œå¹¶ä¸”å¾ˆå®¹æ˜“åœ¨å•ä¸ªæ¨¡å‹ä¸­æ‰©å±•åˆ°å¤šä¸ªå±æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.11937v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æ–‡æœ¬åˆ°å›¾åƒï¼ˆT2Iï¼‰æ‰©æ•£æ¨¡å‹åœ¨ç”Ÿæˆé«˜è´¨é‡å›¾åƒæ–¹é¢å–å¾—äº†æ˜¾è‘—æˆæ•ˆï¼Œä½†åœ¨æ–°é¢†åŸŸå®ç°è¿ç»­å±æ€§ï¼ˆå¦‚çœ¼ç›å¼€åˆæˆ–æ±½è½¦å®½åº¦ç­‰æ•°å€¼ï¼‰çš„ç²¾ç¡®æ§åˆ¶ï¼Œå°¤å…¶æ˜¯åŒæ—¶æ§åˆ¶å¤šä¸ªå±æ€§æ—¶ï¼Œä»é¢ä¸´æŒ‘æˆ˜ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬å¼•å…¥äº†å±æ€§é€‚é…å™¨ï¼ˆAtt-Adapterï¼‰ï¼Œè¿™æ˜¯ä¸€ç§æ–°å‹å³æ’å³ç”¨æ¨¡å—ï¼Œæ—¨åœ¨åœ¨é¢„è®­ç»ƒçš„æ‰©æ•£æ¨¡å‹ä¸­å®ç°ç²¾ç»†çš„å¤šå±æ€§æ§åˆ¶ã€‚è¯¥æ–¹æ³•ä»ä¸€ç»„æœªé…å¯¹çš„åŒ…å«å¤šä¸ªè§†è§‰å±æ€§çš„æ ·æœ¬å›¾åƒä¸­å­¦ä¹ å•ä¸ªæ§åˆ¶é€‚é…å™¨ã€‚Att-Adapteråˆ©ç”¨è§£è€¦äº¤å‰æ³¨æ„åŠ›æ¨¡å—ï¼Œè‡ªç„¶åœ°åè°ƒå¤šä¸ªåŸŸå±æ€§ä¸æ–‡æœ¬æ¡ä»¶ã€‚ä¸ºè¿›ä¸€æ­¥ç¼“è§£è¿‡æ‹Ÿåˆé—®é¢˜å¹¶é€‚åº”è§†è§‰ä¸–ç•Œçš„å¤šæ ·æ€§ï¼Œæˆ‘ä»¬è¿˜å°†æ¡ä»¶å˜åˆ†è‡ªç¼–ç å™¨ï¼ˆCVAEï¼‰å¼•å…¥Att-Adapterã€‚åœ¨å…¬å…±æ•°æ®é›†ä¸Šçš„è¯„ä¼°è¡¨æ˜ï¼ŒAtt-Adapteråœ¨æ§åˆ¶è¿ç»­å±æ€§æ–¹é¢ä¼˜äºæ‰€æœ‰åŸºäºLoRAçš„æ–¹æ³•ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬çš„æ–¹æ³•å…·æœ‰æ›´å¹¿æ³›çš„æ§åˆ¶èŒƒå›´å’Œæ”¹è¿›çš„å¤šå±æ€§è§£çº ç¼ æ€§èƒ½ï¼Œè¶…è¶Šäº†StyleGANæŠ€æœ¯ã€‚å€¼å¾—ä¸€æçš„æ˜¯ï¼ŒAtt-Adapteréå¸¸çµæ´»ï¼Œæ— éœ€é…å¯¹åˆæˆæ•°æ®è¿›è¡Œè®­ç»ƒï¼Œä¸”å¯ä»¥è½»æ¾æ‰©å±•åˆ°å•ä¸ªæ¨¡å‹å†…çš„å¤šä¸ªå±æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>T2I Diffusion Modelsåœ¨ç”Ÿæˆé«˜è´¨é‡å›¾åƒæ–¹é¢è¡¨ç°å‡ºè‰²ã€‚</li>
<li>å®ç°è¿ç»­å±æ€§çš„ç²¾ç¡®æ§åˆ¶ï¼Œç‰¹åˆ«æ˜¯åœ¨æ–°é¢†åŸŸä¸­ï¼Œä»ç„¶æ˜¯ä¸€ä¸ªæŒ‘æˆ˜ã€‚</li>
<li>å¼•å…¥äº†Attribute (Att) Adapterï¼Œä¸€ç§æ–°å‹å³æ’å³ç”¨æ¨¡å—ï¼Œç”¨äºé¢„è®­ç»ƒçš„æ‰©æ•£æ¨¡å‹ä¸­çš„ç²¾ç»†å¤šå±æ€§æ§åˆ¶ã€‚</li>
<li>Att-Adapterèƒ½ä»æœªé…å¯¹çš„æ ·æœ¬å›¾åƒä¸­å­¦ä¹ ï¼Œå¹¶å¤„ç†å¤šä¸ªè§†è§‰å±æ€§ã€‚</li>
<li>åˆ©ç”¨è§£è€¦äº¤å‰æ³¨æ„åŠ›æ¨¡å—å’Œæ¡ä»¶å˜åˆ†è‡ªç¼–ç å™¨ï¼ˆCVAEï¼‰å¢å¼ºæ€§èƒ½ã€‚</li>
<li>åœ¨å…¬å…±æ•°æ®é›†ä¸Šçš„è¯„ä¼°æ˜¾ç¤ºï¼ŒAtt-Adapteråœ¨æ§åˆ¶è¿ç»­å±æ€§æ–¹é¢ä¼˜äºå…¶ä»–æ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.11937">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-50629aed4fae0a17a1c42ccc011f8cc4.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d986435c039237a7e970cd0226135aa7.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-875e25fedbf0e4b14a9f58e93517bcda.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5bf7c2ca605970f534731b4f292f64f6.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="StarGen-A-Spatiotemporal-Autoregression-Framework-with-Video-Diffusion-Model-for-Scalable-and-Controllable-Scene-Generation"><a href="#StarGen-A-Spatiotemporal-Autoregression-Framework-with-Video-Diffusion-Model-for-Scalable-and-Controllable-Scene-Generation" class="headerlink" title="StarGen: A Spatiotemporal Autoregression Framework with Video Diffusion   Model for Scalable and Controllable Scene Generation"></a>StarGen: A Spatiotemporal Autoregression Framework with Video Diffusion   Model for Scalable and Controllable Scene Generation</h2><p><strong>Authors:Shangjin Zhai, Zhichao Ye, Jialin Liu, Weijian Xie, Jiaqi Hu, Zhen Peng, Hua Xue, Danpeng Chen, Xiaomeng Wang, Lei Yang, Nan Wang, Haomin Liu, Guofeng Zhang</strong></p>
<p>Recent advances in large reconstruction and generative models have significantly improved scene reconstruction and novel view generation. However, due to compute limitations, each inference with these large models is confined to a small area, making long-range consistent scene generation challenging. To address this, we propose StarGen, a novel framework that employs a pre-trained video diffusion model in an autoregressive manner for long-range scene generation. The generation of each video clip is conditioned on the 3D warping of spatially adjacent images and the temporally overlapping image from previously generated clips, improving spatiotemporal consistency in long-range scene generation with precise pose control. The spatiotemporal condition is compatible with various input conditions, facilitating diverse tasks, including sparse view interpolation, perpetual view generation, and layout-conditioned city generation. Quantitative and qualitative evaluations demonstrate StarGenâ€™s superior scalability, fidelity, and pose accuracy compared to state-of-the-art methods. Project page: <a target="_blank" rel="noopener" href="https://zju3dv.github.io/StarGen">https://zju3dv.github.io/StarGen</a>. </p>
<blockquote>
<p>è¿‘æœŸé‡å»ºå’Œç”Ÿæˆæ¨¡å‹çš„è¿›å±•æ˜¾è‘—æé«˜äº†åœºæ™¯é‡å»ºå’Œæ–°é¢–è§†è§’ç”Ÿæˆçš„èƒ½åŠ›ã€‚ç„¶è€Œï¼Œç”±äºè®¡ç®—é™åˆ¶ï¼Œè¿™äº›å¤§å‹æ¨¡å‹çš„æ¯æ¬¡æ¨ç†éƒ½å±€é™äºä¸€ä¸ªå°åŒºåŸŸï¼Œä½¿å¾—å¤§èŒƒå›´ä¸€è‡´çš„åœºæ™¯ç”Ÿæˆé¢ä¸´æŒ‘æˆ˜ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†StarGenï¼Œè¿™æ˜¯ä¸€ä¸ªé‡‡ç”¨é¢„è®­ç»ƒè§†é¢‘æ‰©æ•£æ¨¡å‹çš„æ–°å‹æ¡†æ¶ï¼Œä»¥è‡ªå›å½’çš„æ–¹å¼ç”¨äºå¤§èŒƒå›´åœºæ™¯ç”Ÿæˆã€‚æ¯ä¸ªè§†é¢‘å‰ªè¾‘çš„ç”Ÿæˆéƒ½æ˜¯ä»¥ç©ºé—´ç›¸é‚»å›¾åƒçš„3Dæ‰­æ›²å’Œå…ˆå‰ç”Ÿæˆçš„å‰ªè¾‘ä¸­æ—¶é—´ä¸Šé‡å çš„å›¾åƒä¸ºæ¡ä»¶ï¼Œæé«˜äº†å¤§èŒƒå›´åœºæ™¯ç”Ÿæˆä¸­çš„æ—¶ç©ºä¸€è‡´æ€§ï¼Œå¹¶å®ç°äº†ç²¾ç¡®çš„å§¿åŠ¿æ§åˆ¶ã€‚æ—¶ç©ºæ¡ä»¶ä¸å„ç§è¾“å…¥æ¡ä»¶å…¼å®¹ï¼Œä¿ƒè¿›äº†åŒ…æ‹¬ç¨€ç–è§†å›¾æ’å€¼ã€æ°¸ä¹…è§†å›¾ç”Ÿæˆå’Œå¸ƒå±€æ§åˆ¶çš„åŸå¸‚ç”Ÿæˆåœ¨å†…çš„å„ç§ä»»åŠ¡ã€‚å®šé‡å’Œå®šæ€§è¯„ä¼°è¡¨æ˜ï¼ŒStarGenåœ¨å¯æ‰©å±•æ€§ã€ä¿çœŸåº¦å’Œå§¿æ€å‡†ç¡®æ€§æ–¹é¢ä¼˜äºæœ€å…ˆè¿›çš„æ–¹æ³•ã€‚é¡¹ç›®é¡µé¢ï¼š<a target="_blank" rel="noopener" href="https://zju3dv.github.io/StarGen%E3%80%82">https://zju3dv.github.io/StarGenã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.05763v3">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤§å‹é‡å»ºå’Œç”Ÿæˆæ¨¡å‹çš„æœ€æ–°è¿›å±•æå¤§åœ°æé«˜äº†åœºæ™¯é‡å»ºå’Œæ–°é¢–è§†è§’ç”Ÿæˆçš„èƒ½åŠ›ã€‚ç„¶è€Œï¼Œç”±äºè®¡ç®—é™åˆ¶ï¼Œè¿™äº›å¤§å‹æ¨¡å‹çš„æ¯æ¬¡æ¨ç†éƒ½å±€é™äºå°èŒƒå›´ï¼Œä½¿å¾—é•¿è·ç¦»ä¸€è‡´åœºæ™¯ç”Ÿæˆé¢ä¸´æŒ‘æˆ˜ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºStarGenæ¡†æ¶ï¼Œé‡‡ç”¨é¢„è®­ç»ƒè§†é¢‘æ‰©æ•£æ¨¡å‹è¿›è¡Œé•¿è·ç¦»åœºæ™¯ç”Ÿæˆçš„è‡ªå›å½’æ–¹å¼ã€‚æ¯ä¸ªè§†é¢‘ç‰‡æ®µçš„ç”Ÿæˆéƒ½æ˜¯ä»¥ç©ºé—´ç›¸é‚»å›¾åƒçš„3Då˜å½¢å’Œå…ˆå‰ç”Ÿæˆç‰‡æ®µçš„æ—¶é—´é‡å å›¾åƒä¸ºæ¡ä»¶ï¼Œæé«˜äº†é•¿è·ç¦»åœºæ™¯ç”Ÿæˆä¸­çš„æ—¶ç©ºä¸€è‡´æ€§ï¼Œå¹¶å®ç°äº†ç²¾ç¡®çš„å§¿æ€æ§åˆ¶ã€‚StarGençš„æ—¶ç©ºæ¡ä»¶ä¸å„ç§è¾“å…¥æ¡ä»¶å…¼å®¹ï¼Œå¯ç”¨äºç¨€ç–è§†å›¾æ’å€¼ã€æ°¸ä¹…è§†å›¾ç”Ÿæˆå’Œå¸ƒå±€æ§åˆ¶åŸå¸‚ç”Ÿæˆç­‰ä»»åŠ¡ã€‚è¯„ä¼°å’Œæµ‹è¯•æ˜¾ç¤ºï¼ŒStarGenåœ¨å¯æ‰©å±•æ€§ã€ä¿çœŸåº¦å’Œå§¿æ€å‡†ç¡®æ€§æ–¹é¢ä¼˜äºç°æœ‰æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹é‡å»ºå’Œç”Ÿæˆæ¨¡å‹çš„æœ€æ–°è¿›å±•ä¿ƒè¿›äº†åœºæ™¯é‡å»ºå’Œæ–°é¢–è§†è§’ç”Ÿæˆçš„å‘å±•ã€‚</li>
<li>ç”±äºè®¡ç®—é™åˆ¶ï¼Œç°æœ‰æ¨¡å‹åœ¨é•¿è·ç¦»ä¸€è‡´åœºæ™¯ç”Ÿæˆæ–¹é¢é¢ä¸´æŒ‘æˆ˜ã€‚</li>
<li>StarGenæ¡†æ¶é‡‡ç”¨é¢„è®­ç»ƒè§†é¢‘æ‰©æ•£æ¨¡å‹è¿›è¡Œè‡ªå›å½’æ–¹å¼ï¼Œè§£å†³äº†è¿™ä¸€é—®é¢˜ã€‚</li>
<li>StarGenåˆ©ç”¨æ—¶ç©ºæ¡ä»¶ï¼Œç»“åˆç©ºé—´ç›¸é‚»å›¾åƒçš„3Då˜å½¢å’Œæ—¶é—´é‡å å›¾åƒï¼Œæé«˜é•¿è·ç¦»åœºæ™¯ç”Ÿæˆçš„æ—¶ç©ºä¸€è‡´æ€§ã€‚</li>
<li>StarGenå…·å¤‡ç²¾ç¡®çš„å§¿æ€æ§åˆ¶èƒ½åŠ›ã€‚</li>
<li>StarGençš„æ—¶ç©ºæ¡ä»¶å…¼å®¹å¤šç§è¾“å…¥æ¡ä»¶ï¼Œæ”¯æŒå¤šç§ä»»åŠ¡ï¼Œå¦‚ç¨€ç–è§†å›¾æ’å€¼ã€æ°¸ä¹…è§†å›¾ç”Ÿæˆå’Œå¸ƒå±€æ§åˆ¶åŸå¸‚ç”Ÿæˆã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.05763">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-aeb6cbc6173a013f5744e3bf4cd04a26.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-7cf0e8dcc9abb373f6f850fcd56b32a4.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-aa1a7dec6813a97f986d23fee4a718fa.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a973b1603e00e9a3843ad9f288cc4aa7.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="Controllable-Human-Image-Generation-with-Personalized-Multi-Garments"><a href="#Controllable-Human-Image-Generation-with-Personalized-Multi-Garments" class="headerlink" title="Controllable Human Image Generation with Personalized Multi-Garments"></a>Controllable Human Image Generation with Personalized Multi-Garments</h2><p><strong>Authors:Yisol Choi, Sangkyung Kwak, Sihyun Yu, Hyungwon Choi, Jinwoo Shin</strong></p>
<p>We present BootComp, a novel framework based on text-to-image diffusion models for controllable human image generation with multiple reference garments. Here, the main bottleneck is data acquisition for training: collecting a large-scale dataset of high-quality reference garment images per human subject is quite challenging, i.e., ideally, one needs to manually gather every single garment photograph worn by each human. To address this, we propose a data generation pipeline to construct a large synthetic dataset, consisting of human and multiple-garment pairs, by introducing a model to extract any reference garment images from each human image. To ensure data quality, we also propose a filtering strategy to remove undesirable generated data based on measuring perceptual similarities between the garment presented in human image and extracted garment. Finally, by utilizing the constructed synthetic dataset, we train a diffusion model having two parallel denoising paths that use multiple garment images as conditions to generate human images while preserving their fine-grained details. We further show the wide-applicability of our framework by adapting it to different types of reference-based generation in the fashion domain, including virtual try-on, and controllable human image generation with other conditions, e.g., pose, face, etc. </p>
<blockquote>
<p>æˆ‘ä»¬æå‡ºäº†BootCompï¼Œè¿™æ˜¯ä¸€ä¸ªåŸºäºæ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹çš„æ–°å‹æ¡†æ¶ï¼Œç”¨äºå¯æ§çš„äººä½“å›¾åƒç”Ÿæˆï¼Œå…·æœ‰å¤šé‡å‚è€ƒæœè£…ã€‚åœ¨è¿™é‡Œï¼Œè®­ç»ƒä¸­çš„ä¸»è¦ç“¶é¢ˆæ˜¯æ•°æ®é‡‡é›†ï¼šé’ˆå¯¹æ¯ä¸ªå—è¯•è€…æ”¶é›†å¤§è§„æ¨¡çš„é«˜è´¨é‡å‚è€ƒæœè£…å›¾åƒæ•°æ®é›†æ˜¯ä¸€é¡¹æŒ‘æˆ˜ï¼Œä¹Ÿå°±æ˜¯è¯´ï¼Œç†æƒ³æƒ…å†µä¸‹ï¼Œéœ€è¦æ‰‹åŠ¨æ”¶é›†æ¯ä¸ªäººæ‰€ç©¿çš„æ¯ä¸€ä»¶æœè£…ç…§ç‰‡ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ•°æ®ç”Ÿæˆç®¡é“ï¼Œé€šè¿‡å¼•å…¥ä¸€ä¸ªæ¨¡å‹æ¥ä»æ¯ä¸ªäººä½“å›¾åƒä¸­æå–ä»»ä½•å‚è€ƒæœè£…å›¾åƒï¼Œä»è€Œæ„å»ºä¸€ä¸ªäººä½“å’Œå¤šé‡æœè£…é…å¯¹çš„å¤§å‹åˆæˆæ•°æ®é›†ã€‚ä¸ºäº†ä¿è¯æ•°æ®è´¨é‡ï¼Œæˆ‘ä»¬è¿˜æå‡ºäº†ä¸€ç§è¿‡æ»¤ç­–ç•¥ï¼Œé€šè¿‡æµ‹é‡äººä½“å›¾åƒä¸­å±•ç¤ºçš„æœè£…å’Œæå–çš„æœè£…ä¹‹é—´çš„æ„ŸçŸ¥ç›¸ä¼¼æ€§æ¥å»é™¤ä¸è‰¯ç”Ÿæˆæ•°æ®ã€‚æœ€åï¼Œé€šè¿‡åˆ©ç”¨æ„å»ºçš„åˆæˆæ•°æ®é›†ï¼Œæˆ‘ä»¬è®­ç»ƒäº†ä¸€ä¸ªæ‰©æ•£æ¨¡å‹ï¼Œè¯¥æ¨¡å‹å…·æœ‰ä¸¤ä¸ªå¹¶è¡Œå»å™ªè·¯å¾„ï¼Œä½¿ç”¨å¤šå¼ æœè£…å›¾åƒä½œä¸ºæ¡ä»¶æ¥ç”Ÿæˆäººä½“å›¾åƒï¼ŒåŒæ—¶ä¿ç•™å…¶ç»†èŠ‚ã€‚æˆ‘ä»¬è¿˜é€šè¿‡å°†è¯¥æ¡†æ¶é€‚åº”æ—¶å°šé¢†åŸŸçš„ä¸åŒå‚è€ƒåŸºç”Ÿæˆä»»åŠ¡æ¥å±•ç¤ºå…¶å¹¿æ³›çš„åº”ç”¨æ€§ï¼ŒåŒ…æ‹¬è™šæ‹Ÿè¯•ç©¿å’Œå…¶ä»–å¯æ§äººä½“å›¾åƒç”Ÿæˆæ¡ä»¶ï¼Œå¦‚å§¿åŠ¿ã€é¢éƒ¨ç­‰ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2411.16801v3">PDF</a> CVPR 2025. Project page: <a target="_blank" rel="noopener" href="https://omnious.github.io/BootComp">https://omnious.github.io/BootComp</a></p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†BootCompæ¡†æ¶ï¼ŒåŸºäºæ–‡æœ¬åˆ°å›¾åƒçš„æ‰©æ•£æ¨¡å‹ï¼Œå®ç°äº†å¯æ§çš„äººç‰©å›¾åƒç”Ÿæˆï¼Œå¹¶æ”¯æŒå¤šå‚è€ƒæœè£…ã€‚ä¸ºè§£å†³è®­ç»ƒæ•°æ®è·å–ç“¶é¢ˆï¼Œæå‡ºæ„å»ºå¤§å‹åˆæˆæ•°æ®é›†çš„æ–¹æ³•ï¼Œé€šè¿‡å¼•å…¥æ¨¡å‹ä»æ¯ä¸ªäººç‰©å›¾åƒä¸­æå–ä»»ä½•å‚è€ƒæœè£…å›¾åƒã€‚ä¸ºç¡®ä¿æ•°æ®è´¨é‡ï¼Œè¿˜æå‡ºäº†åŸºäºæ„ŸçŸ¥ç›¸ä¼¼åº¦çš„è¿‡æ»¤ç­–ç•¥ï¼Œå»é™¤ä¸è‰¯ç”Ÿæˆæ•°æ®ã€‚æœ€ç»ˆï¼Œåˆ©ç”¨æ„å»ºå¥½çš„åˆæˆæ•°æ®é›†è®­ç»ƒæ‰©æ•£æ¨¡å‹ï¼Œæ‹¥æœ‰ä¸¤æ¡å¹¶è¡Œå»å™ªè·¯å¾„ï¼Œä»¥å¤šå¼ æœè£…å›¾åƒä¸ºæ¡ä»¶ç”Ÿæˆäººç‰©å›¾åƒï¼ŒåŒæ—¶ä¿ç•™ç²¾ç»†ç»†èŠ‚ã€‚æ¡†æ¶è¿˜é€‚ç”¨äºæ—¶å°šé¢†åŸŸçš„ä¸åŒå‚è€ƒåŸºå‡†ç”Ÿæˆï¼ŒåŒ…æ‹¬è™šæ‹Ÿè¯•ç©¿ã€ä»¥åŠå…¶ä»–æ¡ä»¶å¯æ§çš„äººç‰©å›¾åƒç”Ÿæˆï¼Œå¦‚å§¿åŠ¿ã€é¢éƒ¨ç­‰ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>BootCompæ¡†æ¶åŸºäºæ–‡æœ¬åˆ°å›¾åƒçš„æ‰©æ•£æ¨¡å‹ï¼Œæ”¯æŒå¯æ§çš„äººç‰©å›¾åƒç”Ÿæˆï¼Œå¹¶é‡‡ç”¨å¤šå‚è€ƒæœè£…ã€‚</li>
<li>æ•°æ®è·å–æ˜¯è®­ç»ƒçš„ä¸»è¦ç“¶é¢ˆï¼Œå› æ­¤æå‡ºæ„å»ºå¤§å‹åˆæˆæ•°æ®é›†çš„æ–¹æ³•ï¼Œé€šè¿‡æ¨¡å‹ä»äººç‰©å›¾åƒä¸­æå–å‚è€ƒæœè£…å›¾åƒã€‚</li>
<li>ä¸ºç¡®ä¿æ•°æ®è´¨é‡ï¼Œé‡‡ç”¨åŸºäºæ„ŸçŸ¥ç›¸ä¼¼åº¦çš„è¿‡æ»¤ç­–ç•¥ã€‚</li>
<li>æ¡†æ¶åˆ©ç”¨åˆæˆæ•°æ®é›†è®­ç»ƒæ‰©æ•£æ¨¡å‹ï¼Œæ‹¥æœ‰ä¸¤æ¡å¹¶è¡Œå»å™ªè·¯å¾„ï¼Œèƒ½ç”Ÿæˆç²¾ç»†ç»†èŠ‚çš„äººç‰©å›¾åƒã€‚</li>
<li>æ¡†æ¶é€‚ç”¨äºå¤šç§ç±»å‹çš„å‚è€ƒåŸºå‡†ç”Ÿæˆï¼ŒåŒ…æ‹¬è™šæ‹Ÿè¯•ç©¿ã€‚</li>
<li>æ¡†æ¶æ”¯æŒå…¶ä»–æ¡ä»¶å¯æ§çš„äººç‰©å›¾åƒç”Ÿæˆï¼Œå¦‚å§¿åŠ¿ã€é¢éƒ¨ç­‰ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2411.16801">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-dd98c487a23772ec4410193f2a94a72f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4d7c476f5915e2ac8f62fd124f5a6009.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-7c3a202bbb2a39030553983d628d790a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-128ca770e8247efd150d306a3e46d22c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-67ae76ce4d023263f329c7f8592b9f37.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-deacd110450f01464eda12994b1e7aa9.jpg" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="ControlSR-Taming-Diffusion-Models-for-Consistent-Real-World-Image-Super-Resolution"><a href="#ControlSR-Taming-Diffusion-Models-for-Consistent-Real-World-Image-Super-Resolution" class="headerlink" title="ControlSR: Taming Diffusion Models for Consistent Real-World Image Super   Resolution"></a>ControlSR: Taming Diffusion Models for Consistent Real-World Image Super   Resolution</h2><p><strong>Authors:Yuhao Wan, Peng-Tao Jiang, Qibin Hou, Hao Zhang, Jinwei Chen, Ming-Ming Cheng, Bo Li</strong></p>
<p>We present ControlSR, a new method that can tame Diffusion Models for consistent real-world image super-resolution (Real-ISR). Previous Real-ISR models mostly focus on how to activate more generative priors of text-to-image diffusion models to make the output high-resolution (HR) images look better. However, since these methods rely too much on the generative priors, the content of the output images is often inconsistent with the input LR ones. To mitigate the above issue, in this work, we tame Diffusion Models by effectively utilizing LR information to impose stronger constraints on the control signals from ControlNet in the latent space. We show that our method can produce higher-quality control signals, which enables the super-resolution results to be more consistent with the LR image and leads to clearer visual results. In addition, we also propose an inference strategy that imposes constraints in the latent space using LR information, allowing for the simultaneous improvement of fidelity and generative ability. Experiments demonstrate that our model can achieve better performance across multiple metrics on several test sets and generate more consistent SR results with LR images than existing methods. Our code is available at <a target="_blank" rel="noopener" href="https://github.com/HVision-NKU/ControlSR">https://github.com/HVision-NKU/ControlSR</a>. </p>
<blockquote>
<p>æˆ‘ä»¬æå‡ºäº†ControlSRï¼Œè¿™æ˜¯ä¸€ç§æ–°çš„æ–¹æ³•ï¼Œå¯ä»¥é©¯åŒ–æ‰©æ•£æ¨¡å‹ï¼Œä»¥å®ç°ä¸€è‡´çš„ç°å®ä¸–ç•Œå›¾åƒè¶…åˆ†è¾¨ç‡ï¼ˆReal-ISRï¼‰ã€‚ä¹‹å‰çš„Real-ISRæ¨¡å‹å¤§å¤šå…³æ³¨å¦‚ä½•æ¿€æ´»æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹çš„ç”Ÿäº§æ€§å…ˆéªŒçŸ¥è¯†ï¼Œä»¥ä½¿è¾“å‡ºçš„é«˜åˆ†è¾¨ç‡ï¼ˆHRï¼‰å›¾åƒçœ‹èµ·æ¥æ›´å¥½ã€‚ç„¶è€Œï¼Œç”±äºè¿™äº›æ–¹æ³•è¿‡äºä¾èµ–ç”Ÿæˆå…ˆéªŒçŸ¥è¯†ï¼Œè¾“å‡ºå›¾åƒçš„å†…å®¹å¾€å¾€ä¸è¾“å…¥çš„LRå›¾åƒä¸ä¸€è‡´ã€‚ä¸ºäº†ç¼“è§£ä¸Šè¿°é—®é¢˜ï¼Œåœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬é€šè¿‡æœ‰æ•ˆåˆ©ç”¨LRä¿¡æ¯ï¼Œå¯¹ControlNetä¸­çš„æ§åˆ¶ä¿¡å·æ–½åŠ æ›´å¼ºçš„çº¦æŸï¼Œä»è€Œé©¯åŒ–æ‰©æ•£æ¨¡å‹ã€‚æˆ‘ä»¬è¯æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•å¯ä»¥äº§ç”Ÿæ›´é«˜è´¨é‡çš„æ§åˆ¶ä¿¡å·ï¼Œè¿™ä½¿å¾—è¶…åˆ†è¾¨ç‡ç»“æœä¸LRå›¾åƒæ›´åŠ ä¸€è‡´ï¼Œå¹¶å¸¦æ¥æ›´æ¸…æ™°çš„è§†è§‰æ•ˆæœã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜æå‡ºäº†ä¸€ç§åˆ©ç”¨LRä¿¡æ¯åœ¨æ½œåœ¨ç©ºé—´æ–½åŠ çº¦æŸçš„æ¨ç†ç­–ç•¥ï¼Œå¯ä»¥åŒæ—¶æé«˜ä¿çœŸåº¦å’Œç”Ÿæˆèƒ½åŠ›ã€‚å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ¨¡å‹åœ¨å¤šä¸ªæµ‹è¯•é›†ä¸Šçš„å¤šä¸ªæŒ‡æ ‡ä¸Šéƒ½èƒ½å®ç°æ›´å¥½çš„æ€§èƒ½ï¼Œä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼Œç”Ÿæˆçš„SRç»“æœä¸LRå›¾åƒæ›´åŠ ä¸€è‡´ã€‚æˆ‘ä»¬çš„ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/HVision-NKU/ControlSR">https://github.com/HVision-NKU/ControlSR</a>æ‰¾åˆ°ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2410.14279v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æ§åˆ¶SRæ–¹æ³•èƒ½å¤Ÿé©¯æœæ‰©æ•£æ¨¡å‹ï¼Œå®ç°çœŸå®ä¸–ç•Œå›¾åƒè¶…åˆ†è¾¨ç‡ï¼ˆReal-ISRï¼‰çš„ä¸€è‡´æ€§ã€‚è¯¥æ–¹æ³•ä¾§é‡äºåˆ©ç”¨ä½åˆ†è¾¨ç‡ï¼ˆLRï¼‰ä¿¡æ¯ï¼Œåœ¨æ½œåœ¨ç©ºé—´å†…å¯¹æ§åˆ¶ä¿¡å·æ–½åŠ æ›´å¼ºçº¦æŸï¼Œè§£å†³ä»¥å¾€æ–¹æ³•ç”Ÿæˆå›¾åƒå†…å®¹ä¸è¾“å…¥LRå›¾åƒä¸ä¸€è‡´çš„é—®é¢˜ã€‚å®éªŒè¯æ˜ï¼Œè¯¥æ¨¡å‹åœ¨å¤šä¸ªæµ‹è¯•é›†ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œç”Ÿæˆçš„é«˜åˆ†è¾¨ç‡ï¼ˆHRï¼‰å›¾åƒä¸LRå›¾åƒæ›´ä¸€è‡´ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ControlSRæ˜¯ä¸€ç§æ–°çš„æ–¹æ³•ï¼Œèƒ½å¤Ÿé©¯æœæ‰©æ•£æ¨¡å‹ï¼Œå®ç°çœŸå®ä¸–ç•Œå›¾åƒè¶…åˆ†è¾¨ç‡çš„ä¸€è‡´æ€§ã€‚</li>
<li>ä»¥å¾€çš„æ–¹æ³•è¿‡äºä¾èµ–ç”Ÿæˆå…ˆéªŒï¼Œå¯¼è‡´è¾“å‡ºå›¾åƒå†…å®¹ä¸è¾“å…¥ä½åˆ†è¾¨ç‡å›¾åƒä¸ä¸€è‡´ã€‚</li>
<li>ControlSRé€šè¿‡æœ‰æ•ˆåˆ©ç”¨ä½åˆ†è¾¨ç‡ä¿¡æ¯ï¼Œåœ¨æ½œåœ¨ç©ºé—´å†…å¯¹æ§åˆ¶ä¿¡å·æ–½åŠ æ›´å¼ºçº¦æŸï¼Œè§£å†³ä¸Šè¿°é—®é¢˜ã€‚</li>
<li>ControlSRèƒ½äº§ç”Ÿæ›´é«˜è´¨é‡çš„æ§åˆ¶ä¿¡å·ï¼Œä½¿è¶…åˆ†è¾¨ç‡ç»“æœä¸ä½åˆ†è¾¨ç‡å›¾åƒæ›´ä¸€è‡´ï¼Œè§†è§‰æ•ˆæœæ›´æ¸…æ™°ã€‚</li>
<li>ControlSRè¿˜æå‡ºäº†ä¸€ç§æ¨ç†ç­–ç•¥ï¼Œåœ¨æ½œåœ¨ç©ºé—´å†…ä½¿ç”¨ä½åˆ†è¾¨ç‡ä¿¡æ¯æ–½åŠ çº¦æŸï¼ŒåŒæ—¶æé«˜ä¿çœŸåº¦å’Œç”Ÿæˆèƒ½åŠ›ã€‚</li>
<li>å®éªŒè¯æ˜ï¼ŒControlSRæ¨¡å‹åœ¨å¤šä¸ªæµ‹è¯•é›†ä¸Šè¡¨ç°ä¼˜å¼‚ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2410.14279">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-b0b46cddf2e22051e57d5e293d8088e6.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-ac42168fbf311c7bb7c8abf9ebbc82f6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e19c6e6264667d03ff87b49b1e1fe308.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ea065ba4652c5ab44d264f2cf1639546.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4bf479f31577052bffabda3cd3b18b92.jpg" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="Automated-Filtering-of-Human-Feedback-Data-for-Aligning-Text-to-Image-Diffusion-Models"><a href="#Automated-Filtering-of-Human-Feedback-Data-for-Aligning-Text-to-Image-Diffusion-Models" class="headerlink" title="Automated Filtering of Human Feedback Data for Aligning Text-to-Image   Diffusion Models"></a>Automated Filtering of Human Feedback Data for Aligning Text-to-Image   Diffusion Models</h2><p><strong>Authors:Yongjin Yang, Sihyeon Kim, Hojung Jung, Sangmin Bae, SangMook Kim, Se-Young Yun, Kimin Lee</strong></p>
<p>Fine-tuning text-to-image diffusion models with human feedback is an effective method for aligning model behavior with human intentions. However, this alignment process often suffers from slow convergence due to the large size and noise present in human feedback datasets. In this work, we propose FiFA, a novel automated data filtering algorithm designed to enhance the fine-tuning of diffusion models using human feedback datasets with direct preference optimization (DPO). Specifically, our approach selects data by solving an optimization problem to maximize three components: preference margin, text quality, and text diversity. The concept of preference margin is used to identify samples that are highly informative in addressing the noisy nature of feedback dataset, which is calculated using a proxy reward model. Additionally, we incorporate text quality, assessed by large language models to prevent harmful contents, and consider text diversity through a k-nearest neighbor entropy estimator to improve generalization. Finally, we integrate all these components into an optimization process, with approximating the solution by assigning importance score to each data pair and selecting the most important ones. As a result, our method efficiently filters data automatically, without the need for manual intervention, and can be applied to any large-scale dataset. Experimental results show that FiFA significantly enhances training stability and achieves better performance, being preferred by humans 17% more, while using less than 0.5% of the full data and thus 1% of the GPU hours compared to utilizing full human feedback datasets. </p>
<blockquote>
<p>ä½¿ç”¨äººç±»åé¦ˆå¯¹æ–‡æœ¬åˆ°å›¾åƒçš„æ‰©æ•£æ¨¡å‹è¿›è¡Œå¾®è°ƒæ˜¯ä¸€ç§ä½¿æ¨¡å‹è¡Œä¸ºä¸äººç±»æ„å›¾å¯¹é½çš„æœ‰æ•ˆæ–¹æ³•ã€‚ç„¶è€Œï¼Œç”±äºäººç±»åé¦ˆæ•°æ®é›†è§„æ¨¡å¤§ä¸”å­˜åœ¨å™ªå£°ï¼Œè¿™ç§å¯¹é½è¿‡ç¨‹å¾€å¾€å­˜åœ¨æ”¶æ•›ç¼“æ…¢çš„é—®é¢˜ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†FiFAï¼Œè¿™æ˜¯ä¸€ç§æ–°å‹è‡ªåŠ¨åŒ–æ•°æ®è¿‡æ»¤ç®—æ³•ï¼Œæ—¨åœ¨åˆ©ç”¨äººç±»åé¦ˆæ•°æ®é›†å¯¹æ‰©æ•£æ¨¡å‹è¿›è¡Œå¾®è°ƒï¼Œå¹¶ç»“åˆç›´æ¥åå¥½ä¼˜åŒ–ï¼ˆDPOï¼‰ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬çš„æ–¹æ³•é€šè¿‡è§£å†³ä¸€ä¸ªä¼˜åŒ–é—®é¢˜æ¥é€‰æ‹©æ•°æ®ï¼Œä»¥æœ€å¤§åŒ–ä¸‰ä¸ªç»„æˆéƒ¨åˆ†ï¼šåå¥½å·®è·ã€æ–‡æœ¬è´¨é‡å’Œæ–‡æœ¬å¤šæ ·æ€§ã€‚åå¥½å·®è·çš„æ¦‚å¿µç”¨äºè¯†åˆ«é«˜åº¦ä¿¡æ¯æ ·æœ¬ä»¥è§£å†³åé¦ˆæ•°æ®é›†çš„å™ªå£°æœ¬è´¨ï¼Œå®ƒæ˜¯é€šè¿‡ä½¿ç”¨ä»£ç†å¥–åŠ±æ¨¡å‹æ¥è®¡ç®—çš„ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬ç»“åˆå¤§å‹è¯­è¨€æ¨¡å‹å¯¹æ–‡æœ¬è´¨é‡è¿›è¡Œè¯„ä¼°ä»¥é˜²æ­¢æœ‰å®³å†…å®¹ï¼Œå¹¶é€šè¿‡kè¿‘é‚»ç†µä¼°è®¡å™¨è€ƒè™‘æ–‡æœ¬å¤šæ ·æ€§ä»¥æé«˜æ³›åŒ–èƒ½åŠ›ã€‚æœ€åï¼Œæˆ‘ä»¬å°†æ‰€æœ‰è¿™äº›ç»„ä»¶æ•´åˆåˆ°ä¼˜åŒ–è¿‡ç¨‹ä¸­ï¼Œé€šè¿‡ä¸ºæ¯ä¸ªæ•°æ®å¯¹åˆ†é…é‡è¦æ€§å¾—åˆ†å¹¶é€‰æ‹©æœ€é‡è¦çš„æ•°æ®æ¥è¿‘ä¼¼è§£å†³æ–¹æ¡ˆã€‚å› æ­¤ï¼Œæˆ‘ä»¬çš„æ–¹æ³•å¯ä»¥è‡ªåŠ¨è¿‡æ»¤æ•°æ®ï¼Œæ— éœ€äººå·¥å¹²é¢„ï¼Œå¹¶å¯åº”ç”¨äºä»»ä½•å¤§è§„æ¨¡æ•°æ®é›†ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒFiFAæ˜¾è‘—æé«˜äº†è®­ç»ƒç¨³å®šæ€§ï¼Œå¹¶å–å¾—äº†æ›´å¥½çš„æ€§èƒ½ï¼Œäººç±»æ›´å–œæ¬¢ä½¿ç”¨FiFAçš„æ¨¡å‹è¾¾17%ï¼ŒåŒæ—¶ä½¿ç”¨ä¸åˆ°0.5%çš„å…¨æ•°æ®å’Œä»…ä½¿ç”¨å…¨äººç±»åé¦ˆæ•°æ®é›†1%çš„GPUå°æ—¶æ•°ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2410.10166v2">PDF</a> ICLR 2025; Project Page available at :   <a target="_blank" rel="noopener" href="https://sprain02.github.io/FiFA/">https://sprain02.github.io/FiFA/</a></p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºFiFAçš„è‡ªåŠ¨åŒ–æ•°æ®è¿‡æ»¤ç®—æ³•ï¼Œç”¨äºä¼˜åŒ–ä½¿ç”¨äººç±»åé¦ˆæ•°æ®é›†å¯¹æ‰©æ•£æ¨¡å‹è¿›è¡Œå¾®è°ƒçš„è¿‡ç¨‹ã€‚è¯¥ç®—æ³•é€šè¿‡è§£å†³ä¼˜åŒ–é—®é¢˜æ¥é€‰æ‹©æ•°æ®ï¼Œä»¥æœ€å¤§åŒ–åå¥½å·®è·ã€æ–‡æœ¬è´¨é‡å’Œæ–‡æœ¬å¤šæ ·æ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒFiFAèƒ½è‡ªåŠ¨è¿‡æ»¤æ•°æ®ï¼Œæ— éœ€äººå·¥å¹²é¢„ï¼Œå¯åº”ç”¨äºå¤§è§„æ¨¡æ•°æ®é›†ï¼Œæ˜¾è‘—æé«˜è®­ç»ƒç¨³å®šæ€§ï¼Œæ€§èƒ½æ›´ä½³ï¼Œä¸”ä½¿ç”¨çš„äººç±»åé¦ˆæ•°æ®é›†æ›´å°‘ï¼ŒèŠ‚çœäº†GPUå°æ—¶æ•°ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>FiFAæ˜¯ä¸€ç§ç”¨äºä¼˜åŒ–æ‰©æ•£æ¨¡å‹çš„æ–°å‹è‡ªåŠ¨åŒ–æ•°æ®è¿‡æ»¤ç®—æ³•ã€‚</li>
<li>è¯¥ç®—æ³•é€šè¿‡è§£å†³ä¼˜åŒ–é—®é¢˜é€‰æ‹©æ•°æ®ï¼Œä»¥æœ€å¤§åŒ–åå¥½å·®è·ã€æ–‡æœ¬è´¨é‡å’Œæ–‡æœ¬å¤šæ ·æ€§ã€‚</li>
<li>åå¥½å·®è·çš„æ¦‚å¿µç”¨äºè¯†åˆ«é«˜åº¦ä¿¡æ¯æ€§çš„æ ·æœ¬ï¼Œä»¥è§£å†³åé¦ˆæ•°æ®é›†çš„å™ªå£°é—®é¢˜ã€‚</li>
<li>ä½¿ç”¨ä»£ç†å¥–åŠ±æ¨¡å‹è®¡ç®—åå¥½å·®è·ã€‚</li>
<li>é€šè¿‡å¤§å‹è¯­è¨€æ¨¡å‹è¯„ä¼°æ–‡æœ¬è´¨é‡ä»¥é˜²æ­¢æœ‰å®³å†…å®¹ã€‚</li>
<li>é€šè¿‡kè¿‘é‚»ç†µä¼°è®¡å™¨æé«˜æ–‡æœ¬å¤šæ ·æ€§ï¼Œæ”¹å–„æ³›åŒ–èƒ½åŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2410.10166">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-6138b9ab0ed1ae7de171b283dd1a14c6.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c28d81c94d7bbbba83f52e9ce3aee047.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0f6c12a6424198dbad9a5750ce417c7d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-48496c8453686246038c8a3c55b38801.jpg" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1><h2 id="Diffusion-Models-in-3D-Vision-A-Survey"><a href="#Diffusion-Models-in-3D-Vision-A-Survey" class="headerlink" title="Diffusion Models in 3D Vision: A Survey"></a>Diffusion Models in 3D Vision: A Survey</h2><p><strong>Authors:Zhen Wang, Dongyuan Li, Yaozu Wu, Tianyu He, Jiang Bian, Renhe Jiang</strong></p>
<p>In recent years, 3D vision has become a crucial field within computer vision, powering a wide range of applications such as autonomous driving, robotics, augmented reality, and medical imaging. This field relies on accurate perception, understanding, and reconstruction of 3D scenes from 2D images or text data sources. Diffusion models, originally designed for 2D generative tasks, offer the potential for more flexible, probabilistic methods that can better capture the variability and uncertainty present in real-world 3D data. In this paper, we review the state-of-the-art methods that use diffusion models for 3D visual tasks, including but not limited to 3D object generation, shape completion, point-cloud reconstruction, and scene construction. We provide an in-depth discussion of the underlying mathematical principles of diffusion models, outlining their forward and reverse processes, as well as the various architectural advancements that enable these models to work with 3D datasets. We also discuss the key challenges in applying diffusion models to 3D vision, such as handling occlusions and varying point densities, and the computational demands of high-dimensional data. Finally, we discuss potential solutions, including improving computational efficiency, enhancing multimodal fusion, and exploring the use of large-scale pretraining for better generalization across 3D tasks. This paper serves as a foundation for future exploration and development in this rapidly evolving field. </p>
<blockquote>
<p>è¿‘å¹´æ¥ï¼Œ3Dè§†è§‰å·²æˆä¸ºè®¡ç®—æœºè§†è§‰é¢†åŸŸä¸­çš„ä¸€ä¸ªå…³é”®åˆ†æ”¯ï¼Œä¸ºè‡ªåŠ¨é©¾é©¶ã€æœºå™¨äººæŠ€æœ¯ã€å¢å¼ºç°å®å’ŒåŒ»å­¦å½±åƒç­‰å¹¿æ³›åº”ç”¨æä¾›äº†æ”¯æŒã€‚è¯¥é¢†åŸŸä¾èµ–äºä»äºŒç»´å›¾åƒæˆ–æ–‡æœ¬æ•°æ®æºå¯¹ä¸‰ç»´åœºæ™¯çš„å‡†ç¡®æ„ŸçŸ¥ã€ç†è§£å’Œé‡å»ºã€‚æ‰©æ•£æ¨¡å‹æœ€åˆæ˜¯ä¸ºäºŒç»´ç”Ÿæˆä»»åŠ¡è€Œè®¾è®¡çš„ï¼Œå®ƒæä¾›äº†æ›´çµæ´»çš„æ¦‚ç‡æ–¹æ³•ï¼Œå¯ä»¥æ›´å¥½åœ°æ•æ‰çœŸå®ä¸–ç•Œä¸‰ç»´æ•°æ®ä¸­çš„å˜æ€§å’Œä¸ç¡®å®šæ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2410.04738v3">PDF</a> </p>
<p><strong>Summary</strong><br>     æœ¬æ–‡ç»¼è¿°äº†åˆ©ç”¨æ‰©æ•£æ¨¡å‹è¿›è¡Œ3Dè§†è§‰ä»»åŠ¡çš„æœ€æ–°æ–¹æ³•ï¼ŒåŒ…æ‹¬3Då¯¹è±¡ç”Ÿæˆã€å½¢çŠ¶è¡¥å…¨ã€ç‚¹äº‘é‡å»ºå’Œåœºæ™¯æ„å»ºç­‰ã€‚æ–‡ç« æ·±å…¥è®¨è®ºäº†æ‰©æ•£æ¨¡å‹çš„å‰å‘å’Œåå‘è¿‡ç¨‹ã€å„ç§æ¶æ„è¿›å±•ï¼Œä»¥åŠåœ¨å¤„ç†3Dæ•°æ®é›†æ—¶é¢ä¸´çš„æŒ‘æˆ˜ï¼Œå¦‚é®æŒ¡å’Œç‚¹å¯†åº¦ä¸ä¸€ç­‰ã€‚æ­¤å¤–ï¼Œæœ¬æ–‡è¿˜è®¨è®ºäº†æ”¹è¿›è®¡ç®—æ•ˆç‡ã€å¢å¼ºå¤šæ¨¡æ€èåˆå’Œæ¢ç´¢å¤§è§„æ¨¡é¢„è®­ç»ƒåœ¨è·¨3Dä»»åŠ¡ä¸­çš„æ¨å¹¿ç­‰æ½œåœ¨è§£å†³æ–¹æ¡ˆã€‚æœ¬æ–‡ä¸ºè¿™ä¸€å¿«é€Ÿå‘å±•é¢†åŸŸæœªæ¥çš„æ¢ç´¢å’Œå‘å±•å¥ å®šäº†åŸºç¡€ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ‰©æ•£æ¨¡å‹ä¸ºå¤„ç†ä¸‰ç»´è§†è§‰ä»»åŠ¡æä¾›äº†çµæ´»çš„æ¦‚ç‡æ–¹æ³•ã€‚</li>
<li>è¿™äº›æ¨¡å‹å¯ä»äºŒç»´å›¾åƒæˆ–æ–‡æœ¬æ•°æ®æºä¸­æ„ŸçŸ¥ã€ç†è§£å’Œé‡å»ºä¸‰ç»´åœºæ™¯ã€‚</li>
<li>å½“å‰ä½¿ç”¨æ‰©æ•£æ¨¡å‹è¿›è¡Œä¸‰ç»´è§†è§‰ä»»åŠ¡çš„æ–¹æ³•åŒ…æ‹¬3Då¯¹è±¡ç”Ÿæˆã€å½¢çŠ¶è¡¥å…¨ç­‰ã€‚</li>
<li>æ‰©æ•£æ¨¡å‹çš„å‰å‘å’Œåå‘è¿‡ç¨‹åŠå…¶æ¶æ„è¿›å±•æ˜¯å¤„ç†ä¸‰ç»´æ•°æ®é›†çš„å…³é”®ã€‚</li>
<li>åº”ç”¨æ‰©æ•£æ¨¡å‹å¤„ç†ä¸‰ç»´è§†è§‰é¢ä¸´çš„ä¸»è¦æŒ‘æˆ˜åŒ…æ‹¬é®æŒ¡å¤„ç†å’Œç‚¹å¯†åº¦ä¸ä¸€ç­‰ã€‚</li>
<li>æé«˜è®¡ç®—æ•ˆç‡ã€å¢å¼ºå¤šæ¨¡æ€èåˆä»¥åŠå¤§è§„æ¨¡é¢„è®­ç»ƒæ˜¯æ½œåœ¨çš„è§£å†³æ–¹æ¡ˆã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2410.04738">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-6c5510fc9c411527f6b03faaf820fbdd.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-f58afa7aa8030111915ce88f88bfb9b9.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-42352d2558e2eddcf7b8d17a5181cf60.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-c9a31b55b506dfdfbe27ff08d70f494a.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-ec0c694e293a2e888b477d0cfe72590b.jpg" align="middle">
</details>


<h1 id="-19"><a href="#-19" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-04-03/Diffusion%20Models/">https://kedreamix.github.io/Talk2Paper/Paper/2025-04-03/Diffusion%20Models/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Diffusion-Models/">
                                    <span class="chip bg-color">Diffusion Models</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-04-04/Diffusion%20Models/">
                    <div class="card-image">
                        
                        <img src="https://pica.zhimg.com/v2-e3a59a108dd9892170ffcc61518b1904.jpg" class="responsive-img" alt="Diffusion Models">
                        
                        <span class="card-title">Diffusion Models</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Diffusion Models æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-04-04  ILLUME+ Illuminating Unified MLLM with Dual Visual Tokenization and   Diffusion Refinement
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-04-04
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Diffusion-Models/" class="post-category">
                                    Diffusion Models
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Diffusion-Models/">
                        <span class="chip bg-color">Diffusion Models</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-04-04/NeRF/">
                    <div class="card-image">
                        
                        <img src="https://pica.zhimg.com/v2-592a6485d72b21b72232c5a2ce2bb71f.jpg" class="responsive-img" alt="NeRF">
                        
                        <span class="card-title">NeRF</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            NeRF æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-04-04  Diffusion-Guided Gaussian Splatting for Large-Scale Unconstrained 3D   Reconstruction and Novel View Synthesis
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-04-04
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/NeRF/" class="post-category">
                                    NeRF
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/NeRF/">
                        <span class="chip bg-color">NeRF</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">14807.1k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
