<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="åŒ»å­¦å›¾åƒ">
    <meta name="description" content="åŒ»å­¦å›¾åƒ æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-03-28  Explaining the UV to X-ray correlation in AGN within the framework of   X-ray illumination of accretion discs">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>åŒ»å­¦å›¾åƒ | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pic1.zhimg.com/v2-0184ea86406ab9a6846aa56a6f59dc02.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">åŒ»å­¦å›¾åƒ</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                                <span class="chip bg-color">åŒ»å­¦å›¾åƒ</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/" class="post-category">
                                åŒ»å­¦å›¾åƒ
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-03-28
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-04-21
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    20.7k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    85 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-03-28-æ›´æ–°"><a href="#2025-03-28-æ›´æ–°" class="headerlink" title="2025-03-28 æ›´æ–°"></a>2025-03-28 æ›´æ–°</h1><h2 id="Explaining-the-UV-to-X-ray-correlation-in-AGN-within-the-framework-of-X-ray-illumination-of-accretion-discs"><a href="#Explaining-the-UV-to-X-ray-correlation-in-AGN-within-the-framework-of-X-ray-illumination-of-accretion-discs" class="headerlink" title="Explaining the UV to X-ray correlation in AGN within the framework of   X-ray illumination of accretion discs"></a>Explaining the UV to X-ray correlation in AGN within the framework of   X-ray illumination of accretion discs</h2><p><strong>Authors:E. Kammoun, I. E. Papadakis, M. DovÄiak, E. Lusso, E. Nardini, G. Risaliti</strong></p>
<p>It is established that the ultraviolet (UV) and X-ray emissions in active galactic nuclei (AGN) are tightly correlated. This correlation is observed both in low- and high-redshift sources. In particular, observations of large samples of quasars revealed the presence of a non-linear correlation between UV and X-rays. The physical origin of this correlation is poorly understood. In this work, we explore this observed correlation in the framework of the X-ray illumination of the accretion disc by a central source. We have shown in previous works that this model successfully explains the continuum UV&#x2F;optical time delays, variability, and the broadband spectral energy distribution in AGN. We use this model to produce $150,000$ model SEDs assuming a uniform distribution of model parameters. We compute the corresponding UV ($ 2500~\r{A} $) and X-ray (2 keV) monochromatic luminosities and select only the model data points that agree with the observed UV-to-X-ray correlation. Our results show that the X-ray illumination of accretion disc model can reproduce the observed correlation for a subset of model configurations with a non-uniform distribution of black hole mass ($M_{\rm BH}$), accretion rate ($\dot{m}&#x2F;\dot{m}<em>{\rm Edd}$), and power transferred from the accretion disc to the corona ($L</em>{\rm transf}&#x2F;L_{\rm disc}$). In addition, our results reveal the presence of a correlation between $M_{\rm BH}$ and $\dot{m}&#x2F;\dot{m}<em>{\rm Edd}$, and between $\dot{m}&#x2F;\dot{m}</em>{\rm Edd}$ and $L_{\rm transf}&#x2F;L_{\rm disc}$, to explain the observed X-ray-UV correlation. We also present evidence based on observed luminosities supporting our findings. We finally discuss the implications of our results. </p>
<blockquote>
<p>å·²ç»ç¡®å®šæ´»åŠ¨æ˜Ÿç³»æ ¸ï¼ˆAGNï¼‰ä¸­çš„ç´«å¤–çº¿ï¼ˆUVï¼‰å’ŒXå°„çº¿å‘å°„ä¹‹é—´å­˜åœ¨ç´§å¯†å…³è”ã€‚è¿™ç§å…³è”åœ¨ä½çº¢ç§»å’Œé«˜çº¢ç§»æºä¸­éƒ½è§‚å¯Ÿåˆ°ã€‚ç‰¹åˆ«æ˜¯ï¼Œå¯¹å¤§é‡ç±»æ˜Ÿä½“çš„è§‚æµ‹æ˜¾ç¤ºUVå’ŒXå°„çº¿ä¹‹é—´å­˜åœ¨éçº¿æ€§å…³è”ã€‚è¿™ç§å…³è”çš„ç‰©ç†èµ·æºå°šä¸æ¸…æ¥šã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬åœ¨ä¸­å¤®å…‰æºå¯¹å¸ç§¯ç›˜çš„Xå°„çº¿ç…§æ˜çš„æ¡†æ¶ä¸‹æ¢ç´¢äº†è¿™ä¸€è§‚æµ‹åˆ°çš„å…³è”ã€‚æˆ‘ä»¬ä¹‹å‰çš„å·¥ä½œå·²ç»æ˜¾ç¤ºï¼Œè¯¥æ¨¡å‹æˆåŠŸåœ°è§£é‡Šäº†AGNçš„è¿ç»­ç´«å¤–çº¿&#x2F;å…‰å­¦æ—¶å»¶ã€å˜åŒ–ä»¥åŠå®½å¸¦è°±èƒ½é‡åˆ†å¸ƒã€‚æˆ‘ä»¬å‡è®¾æ¨¡å‹å‚æ•°å‡åŒ€åˆ†å¸ƒï¼Œä½¿ç”¨è¯¥æ¨¡å‹ç”Ÿæˆäº†$ 150,000 $ä¸ªæ¨¡å‹SEDã€‚æˆ‘ä»¬è®¡ç®—äº†ç›¸åº”çš„ç´«å¤–çº¿ï¼ˆ$ 2500 \text{A} $ï¼‰å’ŒXå°„çº¿ï¼ˆ2 keVï¼‰å•è‰²å…‰åº¦ï¼Œå¹¶ä»…é€‰æ‹©ç¬¦åˆè§‚å¯Ÿåˆ°çš„ç´«å¤–çº¿åˆ°Xå°„çº¿å…³è”çš„æ¨¡å‹æ•°æ®ç‚¹ã€‚æˆ‘ä»¬çš„ç»“æœè¡¨æ˜ï¼Œå¸ç§¯ç›˜çš„Xå°„çº¿ç…§æ˜æ¨¡å‹å¯ä»¥å†ç°è§‚æµ‹åˆ°çš„å…³è”ï¼Œä½†è¿™ä»…é™äºå…·æœ‰é»‘æ´è´¨é‡ï¼ˆ$ M_{\rm BH} $ï¼‰ã€å¸ç§¯ç‡ï¼ˆ$\dot{m}&#x2F;\dot{m}<em>{\rm Edd}$ï¼‰ä»¥åŠä»å¸ç§¯ç›˜è½¬ç§»åˆ°æ—¥å†•çš„èƒ½é‡ï¼ˆ$ L</em>{\rm transf}&#x2F;L_{\rm disc}$ï¼‰ä¸å‡åŒ€åˆ†å¸ƒçš„æ¨¡å‹é…ç½®å­é›†ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬çš„ç»“æœè¿˜æ­ç¤ºäº†é»‘æ´è´¨é‡ï¼ˆ$ M_{\rm BH} $ï¼‰ä¸å¸ç§¯ç‡ï¼ˆ$\dot{m}&#x2F;\dot{m}_{\rm Edd}$ï¼‰ä¹‹é—´ä»¥åŠå¸ç§¯ç‡ä¸è½¬ç§»çš„èƒ½é‡ä¹‹é—´å­˜åœ¨å…³è”ï¼Œä»¥è§£é‡Šè§‚å¯Ÿåˆ°çš„Xå°„çº¿-ç´«å¤–çº¿å…³è”ã€‚æˆ‘ä»¬è¿˜æä¾›äº†åŸºäºè§‚æµ‹åˆ°çš„å…‰åº¦æ”¯æŒæˆ‘ä»¬çš„å‘ç°è¯æ®ã€‚æœ€åï¼Œæˆ‘ä»¬è®¨è®ºäº†æˆ‘ä»¬çš„ç»“æœçš„å«ä¹‰ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.20770v1">PDF</a> Accepted for publication in A&amp;A</p>
<p><strong>æ‘˜è¦</strong></p>
<p>ç´«å¤–çº¿å’ŒXå°„çº¿åœ¨æ´»è·ƒæ˜Ÿç³»æ ¸ï¼ˆAGNï¼‰ä¸­çš„å‘å°„ç´§å¯†ç›¸å…³ï¼Œè¿™ä¸€å…³è”åœ¨ä½çº¢ç§»å’Œé«˜çº¢ç§»æºä¸­éƒ½å­˜åœ¨ã€‚å¯¹å¤§é‡ç±»æ˜Ÿä½“çš„è§‚æµ‹æ˜¾ç¤ºç´«å¤–çº¿å’ŒXå°„çº¿ä¹‹é—´å­˜åœ¨éçº¿æ€§å…³è”ã€‚è¿™ç§å…³è”çš„ç‰©ç†èµ·æºå°šä¸æ¸…æ¥šã€‚æœ¬ç ”ç©¶åœ¨Xå°„çº¿ç…§å°„å¸ç§¯ç›˜çš„æ¡†æ¶å†…æ¢è®¨äº†è¿™ä¸€è§‚æµ‹åˆ°çš„å…³è”ã€‚æˆ‘ä»¬ä»¥å‰çš„å·¥ä½œè¡¨æ˜ï¼Œè¯¥æ¨¡å‹æˆåŠŸåœ°è§£é‡Šäº†AGNçš„è¿ç»­ç´«å¤–çº¿&#x2F;å…‰å­¦æ—¶å»¶ã€å˜æ€§å’Œå®½å¸¦è°±èƒ½é‡åˆ†å¸ƒã€‚æˆ‘ä»¬ä½¿ç”¨æ­¤æ¨¡å‹ç”Ÿæˆäº†å‡è®¾æ¨¡å‹å‚æ•°å‡åŒ€åˆ†å¸ƒçš„15ä¸‡ä¸ªæ¨¡å‹SEDã€‚æˆ‘ä»¬è®¡ç®—äº†ç›¸åº”çš„ç´«å¤–ï¼ˆ$ 2500 $ Ã…ï¼‰å’ŒXå°„çº¿ï¼ˆ2åƒç”µå­ä¼ï¼‰å•è‰²å…‰åº¦ï¼Œä»…é€‰æ‹©ç¬¦åˆè§‚å¯Ÿåˆ°çš„ç´«å¤–çº¿åˆ°Xå°„çº¿å…³è”æ¨¡å‹æ•°æ®ç‚¹ã€‚ç»“æœè¡¨æ˜ï¼ŒXå°„çº¿ç…§å°„å¸ç§¯ç›˜æ¨¡å‹å¯ä»¥å†ç°è§‚å¯Ÿåˆ°çš„å…³è”ï¼Œä½†è¿™ä»…é™äºæ¨¡å‹é…ç½®çš„å­é›†ï¼Œå…¶é»‘æ´è´¨é‡ï¼ˆ$ M_{\rm BH} $ï¼‰ã€å¸ç§¯ç‡ï¼ˆ$\dot{m}&#x2F;\dot{m}<em>{\rm Edd}$ï¼‰ä»¥åŠä»å¸ç§¯ç›˜è½¬ç§»åˆ°æ—¥å†•çš„èƒ½é‡ï¼ˆ$ L</em>{\rm transf}&#x2F;L_{\rm disc}$ï¼‰çš„åˆ†å¸ƒä¸å‡åŒ€ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬çš„ç»“æœè¿˜æ­ç¤ºäº†é»‘æ´è´¨é‡ï¼ˆ$ M_{\rm BH} $ï¼‰ä¸å¸ç§¯ç‡ï¼ˆ$\dot{m}&#x2F;\dot{m}_{\rm Edd}$ï¼‰ä¹‹é—´çš„å…³è”ä»¥åŠå¸ç§¯ç‡ä¸è½¬ç§»èƒ½é‡ä¹‹é—´çš„å…³è”ï¼Œä»¥è§£é‡Šè§‚å¯Ÿåˆ°çš„Xå°„çº¿ç´«å¤–å…³è”ã€‚æˆ‘ä»¬è¿˜åŸºäºè§‚å¯Ÿåˆ°çš„å…‰åº¦æä¾›äº†æ”¯æŒæˆ‘ä»¬å‘ç°çš„è¯æ®ã€‚æœ€åï¼Œæˆ‘ä»¬è®¨è®ºäº†ç ”ç©¶ç»“æœçš„å½±å“ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>æ´»è·ƒæ˜Ÿç³»æ ¸ï¼ˆAGNï¼‰çš„ç´«å¤–å’ŒXå°„çº¿å‘å°„å­˜åœ¨ç´§å¯†å…³è”ï¼Œè¿™ä¸€å…³è”åœ¨ä½çº¢ç§»å’Œé«˜çº¢ç§»æºä¸­å‡è¢«è§‚å¯Ÿåˆ°ã€‚</li>
<li>åœ¨ç±»æ˜Ÿä½“æ ·æœ¬ä¸­è§‚å¯Ÿåˆ°ç´«å¤–å’ŒXå°„çº¿ä¹‹é—´çš„éçº¿æ€§å…³è”ã€‚</li>
<li>Xå°„çº¿ç…§å°„å¸ç§¯ç›˜æ¨¡å‹æˆåŠŸåœ°è§£é‡Šäº†è§‚æµ‹åˆ°çš„ç´«å¤–å’ŒXå°„çº¿ä¹‹é—´çš„å…³è”ã€‚</li>
<li>æ¨¡å‹ç»“æœæ˜¾ç¤ºï¼Œé»‘æ´è´¨é‡ã€å¸ç§¯ç‡å’Œä»å¸ç§¯ç›˜è½¬ç§»åˆ°æ—¥å†•çš„èƒ½é‡ä¹‹é—´å­˜åœ¨å…³è”ã€‚</li>
<li>è¿™ç§å…³è”å¯ä»¥é€šè¿‡ç‰¹å®šæ¨¡å‹é…ç½®çš„å­é›†æ¥å†ç°ï¼Œè¿™äº›æ¨¡å‹é…ç½®åœ¨å‚æ•°ç©ºé—´ä¸­å…·æœ‰éå‡åŒ€åˆ†å¸ƒã€‚</li>
<li>è§‚å¯Ÿåˆ°çš„ä¸€äº›å…‰åº¦æ•°æ®æ”¯æŒæ¨¡å‹ç»“æœã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.20770">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-72a5d975f469f1d38a7f29ff80ebef70.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0fa3b6c7825c0d19fdb1653d9326967a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-50a06985620f6ff80c67a29eccb054cd.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-616e86618cd0fe5057d6e0f3006432dd.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e37f235542a64b8ca395a90eb17114b7.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f4a51c20288e5d3c4425ac0783ac293d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-256bc07ef4552fb785225d5b0e63e491.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="MindfulLIME-A-Stable-Solution-for-Explanations-of-Machine-Learning-Models-with-Enhanced-Localization-Precision-â€“-A-Medical-Image-Case-Study"><a href="#MindfulLIME-A-Stable-Solution-for-Explanations-of-Machine-Learning-Models-with-Enhanced-Localization-Precision-â€“-A-Medical-Image-Case-Study" class="headerlink" title="MindfulLIME: A Stable Solution for Explanations of Machine Learning   Models with Enhanced Localization Precision â€“ A Medical Image Case Study"></a>MindfulLIME: A Stable Solution for Explanations of Machine Learning   Models with Enhanced Localization Precision â€“ A Medical Image Case Study</h2><p><strong>Authors:Shakiba Rahimiaghdam, Hande Alemdar</strong></p>
<p>Ensuring transparency in machine learning decisions is critically important, especially in sensitive sectors such as healthcare, finance, and justice. Despite this, some popular explainable algorithms, such as Local Interpretable Model-agnostic Explanations (LIME), often produce unstable explanations due to the random generation of perturbed samples. Random perturbation introduces small changes or noise to modified instances of the original data, leading to inconsistent explanations. Even slight variations in the generated samples significantly affect the explanations provided by such models, undermining trust and hindering the adoption of interpretable models. To address this challenge, we propose MindfulLIME, a novel algorithm that intelligently generates purposive samples using a graph-based pruning algorithm and uncertainty sampling. MindfulLIME substantially improves the consistency of visual explanations compared to random sampling approaches. Our experimental evaluation, conducted on a widely recognized chest X-ray dataset, confirms MindfulLIMEâ€™s stability with a 100% success rate in delivering reliable explanations under identical conditions. Additionally, MindfulLIME improves the localization precision of visual explanations by reducing the distance between the generated explanations and the actual local annotations compared to LIME. We also performed comprehensive experiments considering various segmentation algorithms and sample numbers, focusing on stability, quality, and efficiency. The results demonstrate the outstanding performance of MindfulLIME across different segmentation settings, generating fewer high-quality samples within a reasonable processing time. By addressing the stability limitations of LIME in image data, MindfulLIME enhances the trustworthiness and interpretability of machine learning models in specific medical imaging applications, a critical domain. </p>
<blockquote>
<p>ç¡®ä¿æœºå™¨å­¦ä¹ å†³ç­–ä¸­çš„é€æ˜åº¦åœ¨åŒ»ç–—ã€é‡‘èå’Œå¸æ³•ç­‰æ•æ„Ÿé¢†åŸŸå°¤ä¸ºå…³é”®ã€‚ç„¶è€Œï¼Œå°½ç®¡ä¸€äº›æµè¡Œçš„å¯è§£é‡Šç®—æ³•ï¼ˆå¦‚æœ¬åœ°å¯è§£é‡Šæ¨¡å‹æ— å…³è§£é‡Šï¼ˆLIMEï¼‰ï¼‰ç”±äºå…¶éšæœºç”Ÿæˆçš„æ‰°åŠ¨æ ·æœ¬ï¼Œå¾€å¾€ä¼šäº§ç”Ÿä¸ç¨³å®šçš„è§£é‡Šç»“æœã€‚éšæœºæ‰°åŠ¨æ˜¯å¯¹åŸå§‹æ•°æ®çš„ä¿®æ”¹å®ä¾‹è¿›è¡Œå°å˜åŒ–æˆ–å¼•å…¥å™ªå£°ï¼Œå¯¼è‡´è§£é‡Šç»“æœä¸ä¸€è‡´ã€‚å³ä½¿åœ¨ç”Ÿæˆçš„æ ·æœ¬ä¸­å‡ºç°äº†å¾®å°çš„å˜åŒ–ï¼Œä¹Ÿä¼šæ˜¾è‘—å½±å“æ­¤ç±»æ¨¡å‹æ‰€æä¾›çš„è§£é‡Šï¼Œä»è€Œç ´åä¿¡ä»»å¹¶é˜»ç¢å¯è§£é‡Šæ¨¡å‹çš„é‡‡ç”¨ã€‚ä¸ºäº†åº”å¯¹è¿™ä¸€æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†MindfulLIMEï¼Œè¿™æ˜¯ä¸€ç§ä½¿ç”¨åŸºäºå›¾çš„ä¿®å‰ªç®—æ³•å’Œä¸ç¡®å®šæ€§é‡‡æ ·æ™ºèƒ½ç”Ÿæˆæœ‰ç›®çš„æ€§æ ·æœ¬çš„æ–°ç®—æ³•ã€‚ä¸éšæœºé‡‡æ ·æ–¹æ³•ç›¸æ¯”ï¼ŒMindfulLIMEæå¤§åœ°æé«˜äº†è§†è§‰è§£é‡Šçš„ç¨³å®šæ€§ä¸€è‡´æ€§ã€‚æˆ‘ä»¬åœ¨å…¬è®¤çš„èƒ¸éƒ¨Xå°„çº¿æ•°æ®é›†ä¸Šè¿›è¡Œçš„å®éªŒè¯„ä¼°è¯å®ï¼Œåœ¨ç›¸åŒæ¡ä»¶ä¸‹ï¼ŒMindfulLIMEåœ¨æä¾›å¯é è§£é‡Šæ–¹é¢è¾¾åˆ°äº†100%çš„æˆåŠŸç‡ã€‚æ­¤å¤–ï¼Œä¸LIMEç›¸æ¯”ï¼ŒMindfulLIMEé€šè¿‡å‡å°‘ç”Ÿæˆè§£é‡Šä¸å®é™…å±€éƒ¨æ³¨é‡Šä¹‹é—´çš„è·ç¦»ï¼Œæé«˜äº†è§†è§‰è§£é‡Šçš„å®šä½ç²¾åº¦ã€‚æˆ‘ä»¬è¿˜è¿›è¡Œäº†å…¨é¢çš„å®éªŒï¼Œè€ƒè™‘äº†ä¸åŒçš„åˆ†å‰²ç®—æ³•å’Œæ ·æœ¬æ•°é‡ï¼Œé‡ç‚¹æµ‹è¯•ç¨³å®šæ€§ã€è´¨é‡å’Œæ•ˆç‡ã€‚ç»“æœè¡¨æ˜ï¼ŒMindfulLIMEåœ¨ä¸åŒåˆ†å‰²è®¾ç½®ä¸‹è¡¨ç°å‡ºå“è¶Šçš„æ€§èƒ½ï¼Œèƒ½åœ¨åˆç†çš„æ—¶é—´å†…ç”Ÿæˆè¾ƒå°‘çš„é«˜è´¨é‡æ ·æœ¬ã€‚é€šè¿‡è§£å†³LIMEåœ¨å›¾åƒæ•°æ®ä¸­çš„ç¨³å®šæ€§å±€é™æ€§ï¼ŒMindfulLIMEæé«˜äº†æœºå™¨å­¦ä¹ æ¨¡å‹åœ¨ç‰¹å®šåŒ»å­¦å½±åƒåº”ç”¨ä¸­çš„å¯ä¿¡åº¦å’Œå¯è§£é‡Šæ€§ï¼Œè¿™æ˜¯è‡³å…³é‡è¦çš„é¢†åŸŸã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.20758v1">PDF</a> </p>
<p><strong>Summary</strong><br>     é’ˆå¯¹æœºå™¨å­¦ä¹ å†³ç­–é€æ˜åº¦çš„å…³é”®é‡è¦æ€§ï¼Œç‰¹åˆ«æ˜¯åœ¨åŒ»ç–—ã€é‡‘èå’Œå¸æ³•ç­‰æ•æ„Ÿé¢†åŸŸï¼Œæå‡ºäº†MindfulLIMEç®—æ³•ã€‚è¯¥ç®—æ³•é€šè¿‡åŸºäºå›¾çš„ä¿®å‰ªç®—æ³•å’Œä¸ç¡®å®šæ€§é‡‡æ ·æ™ºèƒ½ç”Ÿæˆç›®æ ‡æ ·æœ¬ï¼Œè§£å†³äº†ç°æœ‰è§£é‡Šæ€§ç®—æ³•å¦‚LIMEå› éšæœºæ‰°åŠ¨äº§ç”Ÿçš„è§£é‡Šä¸ç¨³å®šé—®é¢˜ã€‚MindfulLIMEåœ¨èƒ¸éƒ¨Xå…‰æ•°æ®é›†ä¸Šçš„å®éªŒéªŒè¯äº†å…¶ç¨³å®šæ€§ï¼Œå¹¶æé«˜äº†å±€éƒ¨è§£é‡Šçš„å®šä½ç²¾åº¦ã€‚è¯¥ç®—æ³•åœ¨ä¸åŒåˆ†å‰²è®¾ç½®ä¸‹è¡¨ç°å‡ºå“è¶Šæ€§èƒ½ï¼Œèƒ½å¤Ÿåœ¨åˆç†çš„æ—¶é—´å†…ç”Ÿæˆé«˜è´¨é‡æ ·æœ¬ã€‚MindfulLIMEæé«˜äº†æœºå™¨å­¦ä¹ æ¨¡å‹åœ¨åŒ»å­¦æˆåƒç­‰ç‰¹å®šé¢†åŸŸçš„å¯ä¿¡åº¦å’Œå¯è§£é‡Šæ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æœºå™¨å­¦ä¹ å†³ç­–é€æ˜åº¦çš„å…³é”®é‡è¦æ€§åœ¨æ•æ„Ÿé¢†åŸŸå¦‚åŒ»ç–—ã€é‡‘èå’Œå¸æ³•ä¸­å¾—ä»¥ä½“ç°ã€‚</li>
<li>ç°æœ‰è§£é‡Šæ€§ç®—æ³•å¦‚LIMEå­˜åœ¨å› éšæœºæ‰°åŠ¨å¯¼è‡´çš„è§£é‡Šä¸ç¨³å®šé—®é¢˜ã€‚</li>
<li>MindfulLIMEç®—æ³•é€šè¿‡æ™ºèƒ½ç”Ÿæˆç›®æ ‡æ ·æœ¬è§£å†³äº†è§£é‡Šä¸ç¨³å®šé—®é¢˜ã€‚</li>
<li>MindfulLIMEåœ¨èƒ¸éƒ¨Xå…‰æ•°æ®é›†ä¸Šçš„å®éªŒéªŒè¯äº†å…¶ç¨³å®šæ€§ï¼Œå¹¶æé«˜äº†å±€éƒ¨è§£é‡Šçš„å®šä½ç²¾åº¦ã€‚</li>
<li>MindfulLIMEå®éªŒè€ƒè™‘äº†ä¸åŒçš„åˆ†å‰²ç®—æ³•å’Œæ ·æœ¬æ•°é‡ï¼Œå¹¶å…³æ³¨ç¨³å®šæ€§ã€è´¨é‡å’Œæ•ˆç‡ã€‚</li>
<li>MindfulLIMEèƒ½å¤Ÿç”Ÿæˆé«˜è´¨é‡æ ·æœ¬ï¼Œæé«˜äº†æœºå™¨å­¦ä¹ æ¨¡å‹çš„å¯ä¿¡åº¦å’Œå¯è§£é‡Šæ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.20758">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-b075b5a1390cf2a097b3c8b61e7e0cbe.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-0184ea86406ab9a6846aa56a6f59dc02.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0f674e67f382c2094d3abaf0ae359865.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Flip-Learning-Weakly-Supervised-Erase-to-Segment-Nodules-in-Breast-Ultrasound"><a href="#Flip-Learning-Weakly-Supervised-Erase-to-Segment-Nodules-in-Breast-Ultrasound" class="headerlink" title="Flip Learning: Weakly Supervised Erase to Segment Nodules in Breast   Ultrasound"></a>Flip Learning: Weakly Supervised Erase to Segment Nodules in Breast   Ultrasound</h2><p><strong>Authors:Yuhao Huang, Ao Chang, Haoran Dou, Xing Tao, Xinrui Zhou, Yan Cao, Ruobing Huang, Alejandro F Frangi, Lingyun Bao, Xin Yang, Dong Ni</strong></p>
<p>Accurate segmentation of nodules in both 2D breast ultrasound (BUS) and 3D automated breast ultrasound (ABUS) is crucial for clinical diagnosis and treatment planning. Therefore, developing an automated system for nodule segmentation can enhance user independence and expedite clinical analysis. Unlike fully-supervised learning, weakly-supervised segmentation (WSS) can streamline the laborious and intricate annotation process. However, current WSS methods face challenges in achieving precise nodule segmentation, as many of them depend on inaccurate activation maps or inefficient pseudo-mask generation algorithms. In this study, we introduce a novel multi-agent reinforcement learning-based WSS framework called Flip Learning, which relies solely on 2D&#x2F;3D boxes for accurate segmentation. Specifically, multiple agents are employed to erase the target from the box to facilitate classification tag flipping, with the erased region serving as the predicted segmentation mask. The key contributions of this research are as follows: (1) Adoption of a superpixel&#x2F;supervoxel-based approach to encode the standardized environment, capturing boundary priors and expediting the learning process. (2) Introduction of three meticulously designed rewards, comprising a classification score reward and two intensity distribution rewards, to steer the agentsâ€™ erasing process precisely, thereby avoiding both under- and over-segmentation. (3) Implementation of a progressive curriculum learning strategy to enable agents to interact with the environment in a progressively challenging manner, thereby enhancing learning efficiency. Extensively validated on the large in-house BUS and ABUS datasets, our Flip Learning method outperforms state-of-the-art WSS methods and foundation models, and achieves comparable performance as fully-supervised learning algorithms. </p>
<blockquote>
<p>äºŒç»´å’Œä¸‰ç»´è‡ªåŠ¨ä¹³è…ºè¶…å£°ï¼ˆABUSï¼‰ä¸­ç»“èŠ‚çš„ç²¾ç¡®åˆ†å‰²å¯¹äºä¸´åºŠè¯Šæ–­å’Œæ²»ç–—è®¡åˆ’è‡³å…³é‡è¦ã€‚å› æ­¤ï¼Œå¼€å‘ç”¨äºç»“èŠ‚åˆ†å‰²çš„è‡ªåŠ¨åŒ–ç³»ç»Ÿå¯ä»¥æé«˜ç”¨æˆ·ç‹¬ç«‹æ€§å¹¶åŠ å¿«ä¸´åºŠåˆ†æé€Ÿåº¦ã€‚ä¸ä¼ ç»Ÿçš„å®Œå…¨ç›‘ç£å­¦ä¹ ä¸åŒï¼Œå¼±ç›‘ç£åˆ†å‰²ï¼ˆWSSï¼‰å¯ä»¥ç®€åŒ–ç¹çè€Œå¤æ‚çš„æ³¨é‡Šè¿‡ç¨‹ã€‚ç„¶è€Œï¼Œå½“å‰çš„WSSæ–¹æ³•åœ¨å®ç°ç²¾ç¡®ç»“èŠ‚åˆ†å‰²æ–¹é¢é¢ä¸´æŒ‘æˆ˜ï¼Œå› ä¸ºå®ƒä»¬ä¸­çš„è®¸å¤šæ–¹æ³•ä¾èµ–äºä¸å‡†ç¡®çš„æ¿€æ´»å›¾æˆ–ä½æ•ˆçš„ä¼ªæ©è†œç”Ÿæˆç®—æ³•ã€‚åœ¨æœ¬ç ”ç©¶ä¸­ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§åŸºäºå¤šæ™ºèƒ½ä½“å¼ºåŒ–å­¦ä¹ çš„æ–°å‹WSSæ¡†æ¶ï¼Œç§°ä¸ºFlip Learningï¼Œå®ƒä»…ä¾èµ–äºäºŒç»´&#x2F;ä¸‰ç»´æ¡†è¿›è¡Œç²¾ç¡®åˆ†å‰²ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬é‡‡ç”¨å¤šä¸ªæ™ºèƒ½ä½“æ¥ä»æ¡†ä¸­æ¶ˆé™¤ç›®æ ‡ï¼Œä»¥ä¿ƒè¿›åˆ†ç±»æ ‡ç­¾ç¿»è½¬ï¼Œæ¶ˆé™¤çš„åŒºåŸŸä½œä¸ºé¢„æµ‹çš„åˆ†å‰²æ©è†œã€‚æœ¬ç ”ç©¶çš„å…³é”®è´¡çŒ®å¦‚ä¸‹ï¼šï¼ˆ1ï¼‰é‡‡ç”¨åŸºäºè¶…åƒç´ &#x2F;è¶…ä½“ç´ çš„æ–¹æ³•å¯¹æ ‡å‡†åŒ–ç¯å¢ƒè¿›è¡Œç¼–ç ï¼Œæ•æ‰è¾¹ç•Œå…ˆéªŒå¹¶åŠ é€Ÿå­¦ä¹ è¿‡ç¨‹ã€‚ï¼ˆ2ï¼‰å¼•å…¥äº†ä¸‰ç§ç²¾å¿ƒè®¾è®¡çš„å¥–åŠ±æªæ–½ï¼ŒåŒ…æ‹¬åˆ†ç±»åˆ†æ•°å¥–åŠ±å’Œä¸¤ä¸ªå¼ºåº¦åˆ†å¸ƒå¥–åŠ±ï¼Œä»¥ç²¾ç¡®å¼•å¯¼æ™ºèƒ½ä½“çš„æ“¦é™¤è¿‡ç¨‹ï¼Œä»è€Œé¿å…æ¬ åˆ†å‰²å’Œè¿‡åˆ†å‰²ã€‚ï¼ˆ3ï¼‰å®æ–½æ¸è¿›çš„è¯¾ç¨‹å­¦ä¹ ç­–ç•¥ï¼Œä½¿æ™ºèƒ½ä½“èƒ½ä»¥æ¸è¿›æŒ‘æˆ˜çš„æ–¹å¼ä¸ç¯å¢ƒè¿›è¡Œäº¤äº’ï¼Œä»è€Œæé«˜å­¦ä¹ æ•ˆç‡ã€‚ç»è¿‡å¤§è§„æ¨¡çš„å†…éƒ¨BUSå’ŒABUSæ•°æ®é›†éªŒè¯ï¼Œæˆ‘ä»¬çš„Flip Learningæ–¹æ³•ä¼˜äºæœ€æ–°çš„WSSæ–¹æ³•å’ŒåŸºç¡€æ¨¡å‹ï¼Œå¹¶å®ç°äº†ä¸å®Œå…¨ç›‘ç£å­¦ä¹ ç®—æ³•ç›¸å½“çš„æ€§èƒ½ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.20685v1">PDF</a> Accepted by Medical Image Analysis. 24 pages, 13 figures, 18 tabels</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åŸºäºå¤šæ™ºèƒ½ä½“å¼ºåŒ–å­¦ä¹ çš„æ–°å‹å¼±ç›‘ç£åˆ†å‰²æ¡†æ¶Flip Learningï¼Œç”¨äºäºŒç»´å’Œä¸‰ç»´è¶…å£°å›¾åƒçš„ä¹³è…ºç»“èŠ‚åˆ†å‰²ã€‚Flip Learningåˆ©ç”¨äºŒç»´æˆ–ä¸‰ç»´æ¡†è¿›è¡Œå‡†ç¡®åˆ†å‰²ï¼Œé€šè¿‡å¤šä¸ªæ™ºèƒ½ä½“æ“¦é™¤ç›®æ ‡ä»¥ååŠ©åˆ†ç±»æ ‡ç­¾ç¿»è½¬ï¼Œå¹¶åˆ©ç”¨ç²¾å¿ƒè®¾è®¡çš„å¥–åŠ±å‡½æ•°å’Œæ¸è¿›çš„è¯¾ç¨‹å­¦ä¹ ç­–ç•¥æ¥æé«˜æ€§èƒ½ã€‚è¯¥æ–¹æ³•åœ¨å¤§é‡å†…éƒ¨è¶…å£°æ•°æ®é›†ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œå®ç°äº†å¯¹å…ˆè¿›å¼±ç›‘ç£åˆ†å‰²æ–¹æ³•å’ŒåŸºç¡€æ¨¡å‹çš„è¶…è¶Šï¼Œå¹¶ä¸å…¨ç›‘ç£å­¦ä¹ ç®—æ³•è¡¨ç°ç›¸å½“ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>Flip Learningæ˜¯ä¸€ç§åŸºäºå¤šæ™ºèƒ½ä½“å¼ºåŒ–å­¦ä¹ çš„å¼±ç›‘ç£åˆ†å‰²æ¡†æ¶ï¼Œç”¨äºäºŒç»´å’Œä¸‰ç»´è¶…å£°ä¹³è…ºç»“èŠ‚åˆ†å‰²ã€‚</li>
<li>Flip Learningåˆ©ç”¨äºŒç»´æˆ–ä¸‰ç»´æ¡†è¿›è¡Œå‡†ç¡®åˆ†å‰²ï¼Œé€šè¿‡æ™ºèƒ½ä½“æ“¦é™¤ç›®æ ‡åŒºåŸŸæ¥é¢„æµ‹åˆ†å‰²æ©è†œã€‚</li>
<li>é‡‡ç”¨è¶…åƒç´ &#x2F;è¶…ä½“ç´ æ–¹æ³•ç¼–ç æ ‡å‡†åŒ–ç¯å¢ƒï¼ŒåŠ å¿«å­¦ä¹ è¿‡ç¨‹å¹¶æ•æ‰è¾¹ç•Œå…ˆéªŒã€‚</li>
<li>å¼•å…¥ä¸‰ç§ç²¾å¿ƒè®¾è®¡çš„å¥–åŠ±å‡½æ•°ï¼ŒåŒ…æ‹¬åˆ†ç±»å¾—åˆ†å¥–åŠ±å’Œä¸¤ä¸ªå¼ºåº¦åˆ†å¸ƒå¥–åŠ±ï¼Œä»¥ç²¾ç¡®å¼•å¯¼æ™ºèƒ½ä½“çš„æ“¦é™¤è¿‡ç¨‹ï¼Œé¿å…æ¬ åˆ†å‰²å’Œè¿‡åˆ†å‰²ã€‚</li>
<li>å®æ–½æ¸è¿›çš„è¯¾ç¨‹å­¦ä¹ ç­–ç•¥ï¼Œä½¿æ™ºèƒ½ä½“èƒ½ä»¥é€æ¸æŒ‘æˆ˜çš„æ–¹å¼ä¸ç¯å¢ƒäº’åŠ¨ï¼Œæé«˜å­¦ä¹ æ•ˆç‡ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.20685">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-295975c54246833755a9f563f0ae29f3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d0f90bbd3658233a51e726c7ddddd87b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-fb1ffc368df48c5be1483058f98def21.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-fdb6621b18132cb076accecdb36e7ba3.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="UWarp-A-Whole-Slide-Image-Registration-Pipeline-to-Characterize-Scanner-Induced-Local-Domain-Shift"><a href="#UWarp-A-Whole-Slide-Image-Registration-Pipeline-to-Characterize-Scanner-Induced-Local-Domain-Shift" class="headerlink" title="UWarp: A Whole Slide Image Registration Pipeline to Characterize   Scanner-Induced Local Domain Shift"></a>UWarp: A Whole Slide Image Registration Pipeline to Characterize   Scanner-Induced Local Domain Shift</h2><p><strong>Authors:Antoine Schieb, Bilal Hadjadji, Daniel Tshokola Mweze, Natalia Fernanda Valderrama, Valentin DerangÃ¨re, Laurent Arnould, Sylvain Ladoire, Alain Lalande, Louis-Oscar Morel, Nathan VinÃ§on</strong></p>
<p>Histopathology slide digitization introduces scanner-induced domain shift that can significantly impact computational pathology models based on deep learning methods. In the state-of-the-art, this shift is often characterized at a broad scale (slide-level or dataset-level) but not patch-level, which limits our comprehension of the impact of localized tissue characteristics on the accuracy of the deep learning models. To address this challenge, we present a domain shift analysis framework based on UWarp, a novel registration tool designed to accurately align histological slides scanned under varying conditions. UWarp employs a hierarchical registration approach, combining global affine transformations with fine-grained local corrections to achieve robust tissue patch alignment. We evaluate UWarp using two private datasets, CypathLung and BosomShieldBreast, containing whole slide images scanned by multiple devices. Our experiments demonstrate that UWarp outperforms existing open-source registration methods, achieving a median target registration error (TRE) of less than 4 pixels (&lt;1 micrometer at 40x magnification) while significantly reducing computational time. Additionally, we apply UWarp to characterize scanner-induced local domain shift in the predictions of Breast-NEOprAIdict, a deep learning model for breast cancer pathological response prediction. We find that prediction variability is strongly correlated with tissue density on a given patch. Our findings highlight the importance of localized domain shift analysis and suggest that UWarp can serve as a valuable tool for improving model robustness and domain adaptation strategies in computational pathology. </p>
<blockquote>
<p>ç—…ç†åˆ‡ç‰‡æ•°å­—åŒ–å¼•å…¥äº†ç”±æ‰«æä»ªå¼•èµ·çš„é¢†åŸŸæ¼‚ç§»ï¼ˆdomain shiftï¼‰ï¼Œè¿™å¯èƒ½ä¸¥é‡å½±å“åŸºäºæ·±åº¦å­¦ä¹ çš„è®¡ç®—ç—…ç†å­¦æ¨¡å‹ã€‚ç›®å‰ï¼Œè¿™ç§æ¼‚ç§»é€šå¸¸åœ¨å¤§è§„æ¨¡ï¼ˆå¹»ç¯ç‰‡çº§åˆ«æˆ–æ•°æ®é›†çº§åˆ«ï¼‰ä¸Šè¿›è¡Œè¡¨å¾ï¼Œè€Œä¸æ˜¯åœ¨è¡¥ä¸çº§åˆ«ï¼ˆpatch-levelï¼‰ä¸Šè¿›è¡Œï¼Œè¿™é™åˆ¶äº†æˆ‘ä»¬å¯¹å±€éƒ¨ç»„ç»‡ç‰¹æ€§å¯¹æ·±åº¦å­¦ä¹ æ¨¡å‹å‡†ç¡®åº¦å½±å“çš„ç†è§£ã€‚ä¸ºäº†è§£å†³è¿™ä¸€æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªåŸºäºUWarpçš„é¢†åŸŸæ¼‚ç§»åˆ†ææ¡†æ¶ã€‚UWarpæ˜¯ä¸€ç§æ–°å‹æ³¨å†Œå·¥å…·ï¼Œæ—¨åœ¨å‡†ç¡®å¯¹é½åœ¨ä¸åŒæ¡ä»¶ä¸‹æ‰«æçš„ç»„ç»‡åˆ‡ç‰‡ã€‚UWarpé‡‡ç”¨åˆ†å±‚æ³¨å†Œæ–¹æ³•ï¼Œç»“åˆå…¨å±€ä»¿å°„å˜æ¢å’Œç²¾ç»†çš„å±€éƒ¨æ ¡æ­£ï¼Œå®ç°ç¨³å¥çš„ç»„ç»‡æ–‘å—å¯¹é½ã€‚æˆ‘ä»¬ä½¿ç”¨ä¸¤ä¸ªç§æœ‰æ•°æ®é›†CypathLungå’ŒBosomShieldBreastï¼ˆåŒ…å«ç”±å¤šå°è®¾å¤‡æ‰«æçš„æ•´ä¸ªå¹»ç¯ç‰‡å›¾åƒï¼‰å¯¹UWarpè¿›è¡Œäº†è¯„ä¼°ã€‚å®éªŒè¡¨æ˜ï¼ŒUWarpåœ¨ç›®æ ‡æ³¨å†Œè¯¯å·®ï¼ˆTREï¼‰æ–¹é¢çš„è¡¨ç°ä¼˜äºç°æœ‰çš„å¼€æºæ³¨å†Œæ–¹æ³•ï¼Œè¯¯å·®ä¸­ä½æ•°ä½äº4ä¸ªåƒç´ ï¼ˆåœ¨40å€æ”¾å¤§ç‡ä¸‹å°äº1å¾®ç±³ï¼‰ï¼ŒåŒæ—¶æ˜¾è‘—å‡å°‘äº†è®¡ç®—æ—¶é—´ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å°†UWarpåº”ç”¨äºè¡¨å¾æ‰«æä»ªå¼•èµ·çš„Breast-NEOprAIdicté¢„æµ‹ä¸­çš„å±€éƒ¨é¢†åŸŸæ¼‚ç§»ã€‚Breast-NEOprAIdictæ˜¯ä¸€ä¸ªç”¨äºé¢„æµ‹ä¹³è…ºç™Œç—…ç†ååº”çš„æ·±åº¦å­¦ä¹ æ¨¡å‹ã€‚æˆ‘ä»¬å‘ç°é¢„æµ‹å˜åŒ–ä¸ç»™å®šæ–‘å—ä¸Šçš„ç»„ç»‡å¯†åº¦å¯†åˆ‡ç›¸å…³ã€‚æˆ‘ä»¬çš„ç ”ç©¶ç»“æœçªå‡ºäº†å±€éƒ¨é¢†åŸŸæ¼‚ç§»åˆ†æçš„é‡è¦æ€§ï¼Œå¹¶è¡¨æ˜UWarpå¯ä»¥ä½œä¸ºæé«˜è®¡ç®—ç—…ç†å­¦ä¸­çš„æ¨¡å‹ç¨³å¥æ€§å’Œé¢†åŸŸé€‚åº”ç­–ç•¥çš„æœ‰ä»·å€¼çš„å·¥å…·ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.20653v1">PDF</a> </p>
<p><strong>Summary</strong><br>     ç—…ç†åˆ‡ç‰‡æ•°å­—åŒ–å¼•å…¥æ‰«æå™¨å¼•èµ·çš„é¢†åŸŸåç§»ï¼Œå½±å“æ·±åº¦å­¦ä¹ æ¨¡å‹åœ¨ç—…ç†å­¦ä¸Šçš„åº”ç”¨ã€‚ä¸ºè§£å†³æŒ‘æˆ˜ï¼Œæå‡ºåŸºäºUWarpçš„åŸŸåç§»åˆ†ææ¡†æ¶ï¼Œé‡‡ç”¨å±‚æ¬¡æ³¨å†Œæ–¹æ³•ï¼Œå®ç°ç¨³å¥çš„ç»„ç»‡æ–‘å—å¯¹é½ã€‚å®éªŒè¯æ˜UWarpä¼˜äºç°æœ‰å¼€æºæ³¨å†Œæ–¹æ³•ï¼Œå¯æ˜¾è‘—é™ä½é¢„æµ‹ä¸­çš„å±€éƒ¨é¢†åŸŸåç§»ï¼Œæé«˜æ¨¡å‹ç¨³å¥æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ•°å­—åŒ–ç—…ç†åˆ‡ç‰‡æ—¶ï¼Œæ‰«æå™¨å¼•èµ·çš„é¢†åŸŸåç§»å¯¹æ·±åº¦å­¦ä¹ æ¨¡å‹äº§ç”Ÿå½±å“ã€‚</li>
<li>ç°æœ‰çš„é¢†åŸŸåç§»åˆ†æå¤šåœ¨è¾ƒå¤§è§„æ¨¡ï¼ˆå¦‚åˆ‡ç‰‡æˆ–æ•°æ®é›†çº§åˆ«ï¼‰è¿›è¡Œï¼Œè€Œè¾ƒå°‘å…³æ³¨æ–‘å—çº§åˆ«çš„åˆ†æã€‚</li>
<li>UWarpæ˜¯ä¸€ç§æ–°å‹çš„æ³¨å†Œå·¥å…·ï¼Œé‡‡ç”¨å±‚æ¬¡æ³¨å†Œæ–¹æ³•ï¼Œç»“åˆå…¨å±€ä»¿å°„å˜æ¢å’Œå±€éƒ¨ç²¾ç»†æ ¡æ­£ï¼Œå®ç°ç»„ç»‡æ–‘å—çš„ç²¾ç¡®å¯¹é½ã€‚</li>
<li>å®éªŒè¯æ˜UWarpåœ¨æ€§èƒ½ä¸Šä¼˜äºå…¶ä»–å¼€æºæ³¨å†Œæ–¹æ³•ï¼Œç›®æ ‡æ³¨å†Œè¯¯å·®ä¸­ä½æ•°ä½äº4åƒç´ ã€‚</li>
<li>UWarpèƒ½æ˜¾è‘—é™ä½è®¡ç®—æ—¶é—´ã€‚</li>
<li>åœ¨ä¹³è…ºç™Œç—…ç†ååº”é¢„æµ‹çš„æ·±åº¦å­¦ä¹ ä¸­ï¼Œæ‰«æå™¨å¼•èµ·çš„å±€éƒ¨é¢†åŸŸåç§»ä¼šå½±å“é¢„æµ‹ç»“æœã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.20653">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-46eb8d4e916de2280d64381a9a2c3c5e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ecdb244a9c63785919002c1a0b4f34da.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-569c1c1595a04d658676a87f70fccd3e.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Attention-Xception-UNet-AXUNet-A-Novel-Combination-of-CNN-and-Self-Attention-for-Brain-Tumor-Segmentation"><a href="#Attention-Xception-UNet-AXUNet-A-Novel-Combination-of-CNN-and-Self-Attention-for-Brain-Tumor-Segmentation" class="headerlink" title="Attention Xception UNet (AXUNet): A Novel Combination of CNN and   Self-Attention for Brain Tumor Segmentation"></a>Attention Xception UNet (AXUNet): A Novel Combination of CNN and   Self-Attention for Brain Tumor Segmentation</h2><p><strong>Authors:Farzan Moodi, Fereshteh Khodadadi Shoushtari, Gelareh Valizadeh, Dornaz Mazinani, Hanieh Mobarak Salari, Hamidreza Saligheh Rad</strong></p>
<p>Accurate segmentation of glioma brain tumors is crucial for diagnosis and treatment planning. Deep learning techniques offer promising solutions, but optimal model architectures remain under investigation. We used the BraTS 2021 dataset, selecting T1 with contrast enhancement (T1CE), T2, and Fluid-Attenuated Inversion Recovery (FLAIR) sequences for model development. The proposed Attention Xception UNet (AXUNet) architecture integrates an Xception backbone with dot-product self-attention modules, inspired by state-of-the-art (SOTA) large language models such as Google Bard and OpenAI ChatGPT, within a UNet-shaped model. We compared AXUNet with SOTA models. Comparative evaluation on the test set demonstrated improved results over baseline models. Inception-UNet and Xception-UNet achieved mean Dice scores of 90.88 and 93.24, respectively. Attention ResUNet (AResUNet) attained a mean Dice score of 92.80, with the highest score of 84.92 for enhancing tumor (ET) among all models. Attention Gate UNet (AGUNet) yielded a mean Dice score of 90.38. AXUNet outperformed all models with a mean Dice score of 93.73. It demonstrated superior Dice scores across whole tumor (WT) and tumor core (TC) regions, achieving 92.59 for WT, 86.81 for TC, and 84.89 for ET. The integration of the Xception backbone and dot-product self-attention mechanisms in AXUNet showcases enhanced performance in capturing spatial and contextual information. The findings underscore the potential utility of AXUNet in facilitating precise tumor delineation. </p>
<blockquote>
<p>èƒ¶è´¨ç˜¤è„‘è‚¿ç˜¤ï¼ˆåˆç§°è„‘èƒ¶è´¨ç˜¤ï¼‰çš„å‡†ç¡®åˆ†å‰²å¯¹äºè¯Šæ–­å’Œæ²»ç–—è®¡åˆ’è‡³å…³é‡è¦ã€‚æ·±åº¦å­¦ä¹ æŠ€æœ¯æä¾›äº†æœ‰å‰æ™¯çš„è§£å†³æ–¹æ¡ˆï¼Œä½†æœ€ä½³æ¨¡å‹æ¶æ„ä»åœ¨ç ”ç©¶ä¸­ã€‚æˆ‘ä»¬ä½¿ç”¨BraTS 2021æ•°æ®é›†ï¼Œé€‰ç”¨T1å¯¹æ¯”å¢å¼ºï¼ˆT1CEï¼‰ã€T2å’Œæ¶²ä½“è¡°å‡åè½¬æ¢å¤ï¼ˆFLAIRï¼‰åºåˆ—è¿›è¡Œæ¨¡å‹å¼€å‘ã€‚æå‡ºçš„Attention Xception UNetï¼ˆAXUNetï¼‰æ¶æ„ç»“åˆäº†Xceptionéª¨å¹²ç½‘ç»œå’Œç‚¹ç§¯è‡ªæ³¨æ„åŠ›æ¨¡å—ï¼Œè¯¥æ¶æ„å—åˆ°æœ€å‰æ²¿çš„å¤§å‹è¯­è¨€æ¨¡å‹å¦‚Googlebardå’ŒOpenAI ChatGPTçš„å¯å‘ï¼Œåœ¨UNetå½¢çŠ¶çš„æ¨¡å‹ä¸­å¾—ä»¥å®ç°ã€‚æˆ‘ä»¬å°†AXUNetä¸æœ€å…ˆè¿›çš„æ¨¡å‹è¿›è¡Œäº†æ¯”è¾ƒã€‚åœ¨æµ‹è¯•é›†ä¸Šçš„æ¯”è¾ƒè¯„ä¼°è¡¨æ˜ï¼Œç›¸è¾ƒäºåŸºçº¿æ¨¡å‹ï¼Œå…¶è¡¨ç°æœ‰æ‰€æå‡ã€‚Inception-UNetå’ŒXception-UNetçš„å¹³å‡Diceç³»æ•°åˆ†åˆ«ä¸º90.88å’Œ93.24ã€‚Attention ResUNetï¼ˆAResUNetï¼‰çš„å¹³å‡Diceç³»æ•°ä¸º92.80ï¼Œåœ¨å¢å¼ºè‚¿ç˜¤ï¼ˆETï¼‰æ–¹é¢åœ¨æ‰€æœ‰æ¨¡å‹ä¸­å¾—åˆ†æœ€é«˜ï¼Œä¸º84.92ã€‚Attention Gate UNetï¼ˆAGUNetï¼‰çš„å¹³å‡Diceç³»æ•°ä¸º90.38ã€‚AXUNetåœ¨æ‰€æœ‰æ¨¡å‹ä¸­è¡¨ç°æœ€ä½³ï¼Œå¹³å‡Diceç³»æ•°ä¸º93.73ã€‚å®ƒåœ¨æ•´ä¸ªè‚¿ç˜¤ï¼ˆWTï¼‰å’Œè‚¿ç˜¤æ ¸å¿ƒï¼ˆTCï¼‰åŒºåŸŸè¡¨ç°å‡ºè¾ƒé«˜çš„Diceç³»æ•°ï¼Œå…¶ä¸­WTä¸º92.59ï¼ŒTCä¸º86.81ï¼ŒETä¸º84.89ã€‚AXUNetä¸­çš„Xceptionéª¨å¹²ç½‘ç»œå’Œç‚¹ç§¯è‡ªæ³¨æ„åŠ›æœºåˆ¶çš„é›†æˆå±•ç¤ºäº†å…¶åœ¨æ•è·ç©ºé—´å’Œä¸Šä¸‹æ–‡ä¿¡æ¯æ–¹é¢çš„å¢å¼ºæ€§èƒ½ã€‚è¿™äº›å‘ç°çªæ˜¾äº†AXUNetåœ¨ä¿ƒè¿›ç²¾ç¡®è‚¿ç˜¤è½®å»“æç»˜æ–¹é¢çš„æ½œåŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.20446v1">PDF</a> </p>
<p><strong>æ‘˜è¦</strong></p>
<p>æœ¬æ–‡æ¢è®¨äº†åˆ©ç”¨æ·±åº¦å­¦ä¹ æŠ€æœ¯å‡†ç¡®åˆ†å‰²èƒ¶è´¨è„‘è‚¿ç˜¤çš„é‡è¦æ€§ã€‚ç ”ç©¶ä½¿ç”¨äº†BraTS 2021æ•°æ®é›†ï¼Œå¹¶ç»“åˆT1å¯¹æ¯”åº¦å¢å¼ºï¼ˆT1CEï¼‰ã€T2å’ŒFLAIRåºåˆ—å¼€å‘æ¨¡å‹ã€‚æå‡ºçš„Attention Xception UNetï¼ˆAXUNetï¼‰æ¶æ„ç»“åˆäº†Xceptionä¸»å¹²ç½‘ç»œå’Œç‚¹ç§¯è‡ªæ³¨æ„åŠ›æ¨¡å—ï¼Œåœ¨UNetå½¢çŠ¶çš„æ¨¡å‹ä¸­å€Ÿé‰´äº†å¦‚Google Bardå’ŒOpenAI ChatGPTç­‰æœ€æ–°å¤§å‹è¯­è¨€æ¨¡å‹çš„çµæ„Ÿã€‚ä¸ç°æœ‰å…ˆè¿›æŠ€æœ¯æ¨¡å‹ç›¸æ¯”ï¼ŒAXUNetåœ¨æµ‹è¯•é›†ä¸Šçš„è¡¨ç°æœ‰æ‰€æé«˜ã€‚Inception-UNetå’ŒXception-UNetçš„å¹³å‡Diceå¾—åˆ†åˆ†åˆ«ä¸º90.88å’Œ93.24ã€‚Attention ResUNetï¼ˆAResUNetï¼‰åœ¨å¢å¼ºè‚¿ç˜¤ï¼ˆETï¼‰æ–¹é¢çš„å¹³å‡Diceå¾—åˆ†ä¸º92.80ï¼Œå±…æ‰€æœ‰æ¨¡å‹ä¹‹é¦–ã€‚Attention Gate UNetï¼ˆAGUNetï¼‰çš„å¹³å‡Diceå¾—åˆ†ä¸º90.38ã€‚AXUNetè¡¨ç°æœ€ä½³ï¼Œå¹³å‡Diceå¾—åˆ†ä¸º93.73ï¼Œåœ¨æ•´ä½“è‚¿ç˜¤ï¼ˆWTï¼‰å’Œè‚¿ç˜¤æ ¸å¿ƒï¼ˆTCï¼‰åŒºåŸŸè¡¨ç°å‡ºæ›´é«˜çš„Diceå¾—åˆ†ï¼Œåˆ†åˆ«ä¸º92.59ã€86.81å’Œ84.89ã€‚AXUNetä¸­Xceptionä¸»å¹²å’Œç‚¹ç§¯è‡ªæ³¨æ„åŠ›æœºåˆ¶çš„ç»“åˆï¼Œæé«˜äº†æ•æ‰ç©ºé—´å’Œä¸Šä¸‹æ–‡ä¿¡æ¯çš„èƒ½åŠ›ï¼Œæœ‰æœ›ä¸ºç²¾ç¡®è‚¿ç˜¤è½®å»“æç»˜æä¾›å¸®åŠ©ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>èƒ¶è´¨è„‘è‚¿ç˜¤çš„å‡†ç¡®åˆ†å‰²å¯¹è¯Šæ–­å’Œæ²»ç–—è®¡åˆ’è‡³å…³é‡è¦ã€‚</li>
<li>ç ”ç©¶ä½¿ç”¨BraTS 2021æ•°æ®é›†å’Œå¤šç§åºåˆ—ï¼ˆT1CEã€T2ã€FLAIRï¼‰è¿›è¡Œæ¨¡å‹å¼€å‘ã€‚</li>
<li>æå‡ºçš„AXUNetæ¶æ„ç»“åˆäº†Xceptionä¸»å¹²å’Œè‡ªæ³¨æ„åŠ›æ¨¡å—ï¼Œå€Ÿé‰´äº†å¤§å‹è¯­è¨€æ¨¡å‹çš„çµæ„Ÿã€‚</li>
<li>AXUNetåœ¨æµ‹è¯•é›†ä¸Šçš„è¡¨ç°ä¼˜äºå…¶ä»–å…ˆè¿›æŠ€æœ¯æ¨¡å‹ï¼Œå¹³å‡Diceå¾—åˆ†æœ€é«˜ã€‚</li>
<li>AXUNetåœ¨WTã€TCå’ŒETåŒºåŸŸçš„Diceå¾—åˆ†è¡¨ç°ä¼˜å¼‚ã€‚</li>
<li>ç»“åˆXceptionä¸»å¹²å’Œç‚¹ç§¯è‡ªæ³¨æ„åŠ›æœºåˆ¶ï¼ŒAXUNetæé«˜äº†æ•æ‰ç©ºé—´å’Œä¸Šä¸‹æ–‡ä¿¡æ¯çš„èƒ½åŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.20446">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-f6254147152d3e2ab33c23a4e6e1ee38.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7a844367a4aee8ce26e0956f9670766d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a901f25ce1c9d3cbe3d6c1e886f7c301.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Modality-Independent-Brain-Lesion-Segmentation-with-Privacy-aware-Continual-Learning"><a href="#Modality-Independent-Brain-Lesion-Segmentation-with-Privacy-aware-Continual-Learning" class="headerlink" title="Modality-Independent Brain Lesion Segmentation with Privacy-aware   Continual Learning"></a>Modality-Independent Brain Lesion Segmentation with Privacy-aware   Continual Learning</h2><p><strong>Authors:Yousef Sadegheih, Pratibha Kumari, Dorit Merhof</strong></p>
<p>Traditional brain lesion segmentation models for multi-modal MRI are typically tailored to specific pathologies, relying on datasets with predefined modalities. Adapting to new MRI modalities or pathologies often requires training separate models, which contrasts with how medical professionals incrementally expand their expertise by learning from diverse datasets over time. Inspired by this human learning process, we propose a unified segmentation model capable of sequentially learning from multiple datasets with varying modalities and pathologies. Our approach leverages a privacy-aware continual learning framework that integrates a mixture-of-experts mechanism and dual knowledge distillation to mitigate catastrophic forgetting while not compromising performance on newly encountered datasets. Extensive experiments across five diverse brain MRI datasets and four dataset sequences demonstrate the effectiveness of our framework in maintaining a single adaptable model, capable of handling varying hospital protocols, imaging modalities, and disease types. Compared to widely used privacy-aware continual learning methods such as LwF, SI, EWC, and MiB, our method achieves an average Dice score improvement of approximately 11%. Our framework represents a significant step toward more versatile and practical brain lesion segmentation models, with implementation available at \href{<a target="_blank" rel="noopener" href="https://github.com/xmindflow/BrainCL%7D%7BGitHub%7D">https://github.com/xmindflow/BrainCL}{GitHub}</a>. </p>
<blockquote>
<p>ä¼ ç»Ÿçš„å¤šæ¨¡æ€MRIè„‘ç—…å˜åˆ†å‰²æ¨¡å‹é€šå¸¸é’ˆå¯¹ç‰¹å®šçš„ç—…ç†æƒ…å†µè¿›è¡Œå®šåˆ¶ï¼Œä¾èµ–äºé¢„å®šä¹‰çš„æ¨¡æ€æ•°æ®é›†ã€‚é€‚åº”æ–°çš„MRIæ¨¡æ€æˆ–ç—…ç†æƒ…å†µé€šå¸¸éœ€è¦è®­ç»ƒå•ç‹¬çš„æ¨¡å‹ï¼Œè¿™ä¸åŒ»å­¦ä¸“å®¶é€šè¿‡é•¿æ—¶é—´å­¦ä¹ ä¸åŒæ•°æ®é›†æ¥é€æ­¥æ‰©å±•ä»–ä»¬çš„ä¸“ä¸šçŸ¥è¯†çš„æ–¹å¼å½¢æˆé²œæ˜å¯¹æ¯”ã€‚å—è¿™ä¸€äººç±»å­¦ä¹ è¿‡ç¨‹å¯å‘ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ç»Ÿä¸€çš„åˆ†å‰²æ¨¡å‹ï¼Œèƒ½å¤ŸæŒ‰é¡ºåºä»å…·æœ‰ä¸åŒæ¨¡æ€å’Œç—…ç†æƒ…å†µçš„å¤šä¸ªæ•°æ®é›†ä¸­å­¦ä¹ ã€‚æˆ‘ä»¬çš„æ–¹æ³•åˆ©ç”¨äº†ä¸€ç§éšç§æ„ŸçŸ¥çš„æŒç»­å­¦ä¹ æ¡†æ¶ï¼Œè¯¥æ¡†æ¶é›†æˆäº†æ··åˆä¸“å®¶æœºåˆ¶å’ŒåŒå‘çŸ¥è¯†è’¸é¦ï¼Œä»¥å‡è½»ç¾éš¾æ€§é—å¿˜ï¼ŒåŒæ—¶ä¸æŸå®³å¯¹æ–°é‡åˆ°æ•°æ®é›†çš„æ€§èƒ½ã€‚åœ¨äº”ä¸ªä¸åŒçš„è„‘MRIæ•°æ®é›†å’Œå››ä¸ªæ•°æ®é›†åºåˆ—ä¸Šçš„å¹¿æ³›å®éªŒè¯æ˜äº†æˆ‘ä»¬æ¡†æ¶çš„æœ‰æ•ˆæ€§ï¼Œè¯¥æ¡†æ¶èƒ½å¤Ÿç»´æŒä¸€ä¸ªå•ä¸€çš„é€‚åº”æ¨¡å‹ï¼Œå¤„ç†ä¸åŒçš„åŒ»é™¢åè®®ã€æˆåƒæ¨¡æ€å’Œç–¾ç—…ç±»å‹ã€‚ä¸å¹¿æ³›ä½¿ç”¨çš„éšç§æ„ŸçŸ¥æŒç»­å­¦ä¹ æ–¹æ³•ï¼ˆå¦‚LwFã€SIã€EWCå’ŒMiBï¼‰ç›¸æ¯”ï¼Œæˆ‘ä»¬çš„æ–¹æ³•å®ç°äº†å¹³å‡Diceå¾—åˆ†çº¦æé«˜11%ã€‚æˆ‘ä»¬çš„æ¡†æ¶æœç€æ›´é€šç”¨å’Œå®ç”¨çš„è„‘ç—…å˜åˆ†å‰²æ¨¡å‹è¿ˆå‡ºäº†é‡è¦çš„ä¸€æ­¥ï¼Œå…·ä½“å®ç°å¯åœ¨GitHubä¸Šæ‰¾åˆ°ï¼š<a target="_blank" rel="noopener" href="https://github.com/xmindflow/BrainCL">https://github.com/xmindflow/BrainCL</a>ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.20326v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>åŸºäºä¼ ç»Ÿçš„å¤§è„‘ç—…å˜åˆ†å‰²æ¨¡å‹åœ¨å¤šæ¨¡æ€MRIä¸Šçš„å±€é™æ€§ï¼Œç ”ç©¶æå‡ºäº†ä¸€ç§ç»Ÿä¸€çš„åˆ†å‰²æ¨¡å‹ï¼Œè¯¥æ¨¡å‹å¯é€‚åº”å¤šç§æ•°æ®é›†çš„æ¨¡æ€å’Œç—…ç†å˜åŒ–ã€‚è¯¥æ¨¡å‹å€Ÿé‰´åŒ»å­¦ä¸“å®¶é€šè¿‡å¤šæ ·åŒ–æ•°æ®é›†é€æ¸ç§¯ç´¯çŸ¥è¯†çš„è¿‡ç¨‹ï¼Œé€šè¿‡éšç§æ„ŸçŸ¥çš„æŒç»­å­¦ä¹ æ¡†æ¶å®ç°é€æ­¥å­¦ä¹ ã€‚åˆ©ç”¨ä¸“å®¶ç»„åˆæœºåˆ¶å’ŒåŒé‡çŸ¥è¯†è’¸é¦ç­–ç•¥å‡è½»ç¾éš¾æ€§é—å¿˜é—®é¢˜ï¼Œç¡®ä¿æ–°æ•°æ®é›†çš„æ€§èƒ½ä¸å—å½±å“ã€‚åœ¨äº”ä¸ªä¸åŒçš„è„‘MRIæ•°æ®é›†å’Œå››ç»„æ•°æ®é›†åºåˆ—ä¸Šè¿›è¡Œçš„å®éªŒè¡¨æ˜ï¼Œä¸å¸¸ç”¨çš„éšç§æ„ŸçŸ¥æŒç»­å­¦ä¹ æ–¹æ³•ç›¸æ¯”ï¼Œè¯¥æ¡†æ¶åœ¨ä¿æŒå•ä¸€å¯é€‚åº”æ¨¡å‹çš„åŒæ—¶ï¼Œèƒ½å¤Ÿå¤„ç†ä¸åŒçš„åŒ»é™¢åè®®ã€æˆåƒæ¨¡æ€å’Œç–¾ç—…ç±»å‹ï¼Œå¹³å‡Diceå¾—åˆ†æé«˜äº†çº¦11%ã€‚è¯¥æ¡†æ¶å‘å¼€å‘æ›´åŠ å®ç”¨å’Œå¤šåŠŸèƒ½çš„è„‘ç—…å˜åˆ†å‰²æ¨¡å‹è¿ˆå‡ºäº†é‡è¦çš„ä¸€æ­¥ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>æå‡ºä¸€ç§ç»Ÿä¸€çš„åˆ†å‰²æ¨¡å‹ï¼Œèƒ½å¤Ÿé€‚åº”å¤šç§æ•°æ®é›†çš„æ¨¡æ€å’Œç—…ç†å˜åŒ–ã€‚</li>
<li>æ¨¡å‹å€Ÿé‰´åŒ»å­¦ä¸“å®¶çš„å­¦ä¹ æ¨¡å¼ï¼Œå®ç°é€æ­¥å­¦ä¹ ã€‚</li>
<li>åˆ©ç”¨éšç§æ„ŸçŸ¥çš„æŒç»­å­¦ä¹ æ¡†æ¶æ¥åº”å¯¹æ–°MRIæ¨¡æ€æˆ–ç—…ç†çš„æŒ‘æˆ˜ã€‚</li>
<li>ç»“åˆä¸“å®¶ç»„åˆæœºåˆ¶å’ŒåŒé‡çŸ¥è¯†è’¸é¦ç­–ç•¥æ¥å‡è½»ç¾éš¾æ€§é—å¿˜é—®é¢˜ã€‚</li>
<li>åœ¨å¤šä¸ªè„‘MRIæ•°æ®é›†ä¸Šçš„å®éªŒéªŒè¯äº†æ¨¡å‹çš„æœ‰æ•ˆæ€§ã€‚</li>
<li>ä¸å…¶ä»–éšç§æ„ŸçŸ¥æŒç»­å­¦ä¹ æ–¹æ³•ç›¸æ¯”ï¼Œè¯¥æ¨¡å‹çš„æ€§èƒ½æœ‰æ‰€æå‡ã€‚</li>
<li>è¯¥æ¡†æ¶åœ¨åº”å¯¹ä¸åŒåŒ»é™¢åè®®ã€æˆåƒæ¨¡æ€å’Œç–¾ç—…ç±»å‹æ–¹é¢å…·æœ‰å¹¿æ³›çš„åº”ç”¨æ½œåŠ›ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.20326">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-f7c5b3f14a3c93b39bdc35338f838a3d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-64b5d86e6010cc796a4c4bb2ec4f1449.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f5016f85474ea4a43c75c3e5f4a4a78c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-592e94ce071048447d450a51daf7525e.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Mamba-3D-as-Masked-Autoencoders-for-Accurate-and-Data-Efficient-Analysis-of-Medical-Ultrasound-Videos"><a href="#Mamba-3D-as-Masked-Autoencoders-for-Accurate-and-Data-Efficient-Analysis-of-Medical-Ultrasound-Videos" class="headerlink" title="Mamba-3D as Masked Autoencoders for Accurate and Data-Efficient Analysis   of Medical Ultrasound Videos"></a>Mamba-3D as Masked Autoencoders for Accurate and Data-Efficient Analysis   of Medical Ultrasound Videos</h2><p><strong>Authors:Jiaheng Zhou, Yanfeng Zhou, Wei Fang, Yuxing Tang, Le Lu, Ge Yang</strong></p>
<p>Ultrasound videos are an important form of clinical imaging data, and deep learning-based automated analysis can improve diagnostic accuracy and clinical efficiency. However, the scarcity of labeled data and the inherent challenges of video analysis have impeded the advancement of related methods. In this work, we introduce E-ViM$^3$, a data-efficient Vision Mamba network that preserves the 3D structure of video data, enhancing long-range dependencies and inductive biases to better model space-time correlations. With our design of Enclosure Global Tokens (EGT), the model captures and aggregates global features more effectively than competing methods. To further improve data efficiency, we employ masked video modeling for self-supervised pre-training, with the proposed Spatial-Temporal Chained (STC) masking strategy designed to adapt to various video scenarios. Experiments demonstrate that E-ViM$^3$ performs as the state-of-the-art in two high-level semantic analysis tasks across four datasets of varying sizes: EchoNet-Dynamic, CAMUS, MICCAI-BUV, and WHBUS. Furthermore, our model achieves competitive performance with limited labels, highlighting its potential impact on real-world clinical applications. </p>
<blockquote>
<p>è¶…å£°è§†é¢‘æ˜¯ä¸´åºŠå½±åƒæ•°æ®çš„é‡è¦å½¢å¼ï¼ŒåŸºäºæ·±åº¦å­¦ä¹ çš„è‡ªåŠ¨åˆ†æå¯ä»¥æé«˜è¯Šæ–­å‡†ç¡®æ€§å’Œä¸´åºŠæ•ˆç‡ã€‚ç„¶è€Œï¼Œæ ‡è®°æ•°æ®çš„ç¨€ç¼ºä»¥åŠè§†é¢‘åˆ†æå›ºæœ‰çš„æŒ‘æˆ˜é˜»ç¢äº†ç›¸å…³æ–¹æ³•çš„è¿›æ­¥ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬å¼•å…¥äº†E-ViM$^3$ï¼Œè¿™æ˜¯ä¸€ä¸ªæ•°æ®é«˜æ•ˆçš„è§†è§‰Mambaç½‘ç»œï¼Œå®ƒä¿ç•™äº†è§†é¢‘æ•°æ®çš„3Dç»“æ„ï¼Œå¢å¼ºäº†è¿œç¨‹ä¾èµ–å…³ç³»å’Œå½’çº³åè§ï¼Œä»¥æ›´å¥½åœ°å»ºæ¨¡æ—¶ç©ºç›¸å…³æ€§ã€‚é€šè¿‡æˆ‘ä»¬å¯¹å°é—­å…¨å±€ä»¤ç‰Œï¼ˆEGTï¼‰çš„è®¾è®¡ï¼Œè¯¥æ¨¡å‹æ¯”ç«äº‰æ–¹æ³•æ›´æœ‰æ•ˆåœ°æ•è·å’Œèšåˆå…¨å±€ç‰¹å¾ã€‚ä¸ºäº†è¿›ä¸€æ­¥æé«˜æ•°æ®æ•ˆç‡ï¼Œæˆ‘ä»¬é‡‡ç”¨æ©ç è§†é¢‘å»ºæ¨¡è¿›è¡Œè‡ªç›‘ç£é¢„è®­ç»ƒï¼Œæå‡ºæ—¶ç©ºé“¾ï¼ˆSTCï¼‰æ©ç ç­–ç•¥ï¼Œä»¥é€‚åº”å„ç§è§†é¢‘åœºæ™¯ã€‚å®éªŒè¡¨æ˜ï¼ŒE-ViM$^3$åœ¨ä¸¤ä¸ªé«˜çº§è¯­ä¹‰åˆ†æä»»åŠ¡ä¸­çš„è¡¨ç°å‡ä¸ºæœ€ä¼˜ï¼Œæ¶‰åŠå››ä¸ªä¸åŒè§„æ¨¡çš„æ•°æ®é›†ï¼šEchoNet-Dynamicã€CAMUSã€MICCAI-BUVå’ŒWHBUSã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬çš„æ¨¡å‹åœ¨æœ‰é™æ ‡ç­¾çš„æƒ…å†µä¸‹ä¹Ÿå®ç°äº†æœ‰ç«äº‰åŠ›çš„æ€§èƒ½ï¼Œè¿™çªæ˜¾äº†å…¶åœ¨ç°å®ä¸–ç•Œä¸´åºŠåº”ç”¨ä¸­çš„æ½œåœ¨å½±å“ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.20258v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>è¶…å£°è§†é¢‘æ˜¯é‡è¦çš„ä¸´åºŠå½±åƒæ•°æ®å½¢å¼ï¼Œæ·±åº¦å­¦ä¹ åŸºç¡€ä¸Šçš„è‡ªåŠ¨åŒ–åˆ†æå¯ä»¥æé«˜è¯Šæ–­å‡†ç¡®æ€§å’Œä¸´åºŠæ•ˆç‡ã€‚ç„¶è€Œï¼Œæ ‡è®°æ•°æ®çš„ç¨€ç¼ºæ€§å’Œè§†é¢‘åˆ†æå›ºæœ‰çš„æŒ‘æˆ˜é˜»ç¢äº†ç›¸å…³æ–¹æ³•çš„è¿›æ­¥ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬å¼•å…¥äº†E-ViM$^3$ï¼Œä¸€ä¸ªæ•°æ®é«˜æ•ˆçš„è§†è§‰æ¨¡å‹ç½‘ç»œï¼Œå®ƒèƒ½å¤Ÿä¿æŒè§†é¢‘æ•°æ®çš„ä¸‰ç»´ç»“æ„ï¼Œå¼ºåŒ–è¿œç¨‹ä¾èµ–å…³ç³»å’Œå½’çº³åè§ï¼Œä»¥æ›´å¥½åœ°æ¨¡æ‹Ÿæ—¶ç©ºç›¸å…³æ€§ã€‚é€šè¿‡è®¾è®¡å°é—­å…¨å±€ä»¤ç‰Œï¼ˆEGTï¼‰ï¼Œè¯¥æ¨¡å‹æ›´æœ‰æ•ˆåœ°æ•è·å’Œèšåˆå…¨å±€ç‰¹å¾ï¼Œç›¸è¾ƒäºå…¶ä»–æ–¹æ³•æ›´å…·ä¼˜åŠ¿ã€‚ä¸ºäº†è¿›ä¸€æ­¥æé«˜æ•°æ®æ•ˆç‡ï¼Œæˆ‘ä»¬é‡‡ç”¨æ©ç è§†é¢‘å»ºæ¨¡è¿›è¡Œè‡ªç›‘ç£é¢„è®­ç»ƒï¼Œå¹¶è®¾è®¡äº†é’ˆå¯¹å„ç§è§†é¢‘åœºæ™¯çš„æ—¶ç©ºé“¾å¼ï¼ˆSTCï¼‰æ©è”½ç­–ç•¥ã€‚å®éªŒè¡¨æ˜ï¼ŒE-ViM$^3$åœ¨ä¸¤ä¸ªé«˜çº§è¯­ä¹‰åˆ†æä»»åŠ¡ä¸­çš„è¡¨ç°å‡è¾¾åˆ°äº†é¡¶å°–æ°´å¹³ï¼Œè·¨è¶Šäº†å››ä¸ªä¸åŒå¤§å°çš„æ•°æ®é›†ï¼šEchoNet-Dynamicã€CAMUSã€MICCAI-BUVå’ŒWHBUSã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬çš„æ¨¡å‹åœ¨æœ‰é™æ ‡ç­¾çš„æƒ…å†µä¸‹å®ç°äº†æœ‰ç«äº‰åŠ›çš„æ€§èƒ½è¡¨ç°ï¼Œçªæ˜¾å…¶åœ¨ç°å®ä¸–ç•Œä¸´åºŠåº”ç”¨ä¸­çš„æ½œåœ¨å½±å“åŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è¶…å£°è§†é¢‘æ˜¯ä¸´åºŠå½±åƒæ•°æ®çš„é‡è¦å½¢å¼ä¹‹ä¸€ã€‚</li>
<li>æ·±åº¦å­¦ä¹ å¯ç”¨äºè‡ªåŠ¨åŒ–åˆ†æè¶…å£°è§†é¢‘ä»¥æé«˜è¯Šæ–­å’Œä¸´åºŠæ•ˆç‡ã€‚</li>
<li>ç¼ºä¹æ ‡è®°æ•°æ®å’Œè§†é¢‘åˆ†ææŒ‘æˆ˜é™åˆ¶äº†ç›¸å…³æ–¹æ³•çš„å‘å±•ã€‚</li>
<li>å¼•å…¥E-ViM$^3$æ¨¡å‹ï¼Œèƒ½æœ‰æ•ˆå¤„ç†è§†é¢‘æ•°æ®çš„ä¸‰ç»´ç»“æ„å¹¶æ¨¡æ‹Ÿæ—¶ç©ºç›¸å…³æ€§ã€‚</li>
<li>E-ViM$^3$é€šè¿‡å°é—­å…¨å±€ä»¤ç‰Œï¼ˆEGTï¼‰è®¾è®¡æé«˜äº†å…¨å±€ç‰¹å¾çš„æ•è·å’Œèšåˆèƒ½åŠ›ã€‚</li>
<li>é‡‡ç”¨æ©ç è§†é¢‘å»ºæ¨¡å’Œæ—¶ç©ºé“¾å¼ï¼ˆSTCï¼‰æ©è”½ç­–ç•¥æé«˜æ•°æ®æ•ˆç‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.20258">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-b0b6bf24f90f0327c502e3fb3dc136b6.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-056ecd4743d2ca891e4f753f32754a70.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-325936b6a6a54989a9000d1d209671a4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f941079ae7123d88ee5d7626fdf27c23.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-9d747dcf4a9c6a8f34c1c029ea0d1c7b.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Med3DVLM-An-Efficient-Vision-Language-Model-for-3D-Medical-Image-Analysis"><a href="#Med3DVLM-An-Efficient-Vision-Language-Model-for-3D-Medical-Image-Analysis" class="headerlink" title="Med3DVLM: An Efficient Vision-Language Model for 3D Medical Image   Analysis"></a>Med3DVLM: An Efficient Vision-Language Model for 3D Medical Image   Analysis</h2><p><strong>Authors:Yu Xin, Gorkem Can Ates, Kuang Gong, Wei Shao</strong></p>
<p>Vision-language models (VLMs) have shown promise in 2D medical image analysis, but extending them to 3D remains challenging due to the high computational demands of volumetric data and the difficulty of aligning 3D spatial features with clinical text. We present Med3DVLM, a 3D VLM designed to address these challenges through three key innovations: (1) DCFormer, an efficient encoder that uses decomposed 3D convolutions to capture fine-grained spatial features at scale; (2) SigLIP, a contrastive learning strategy with pairwise sigmoid loss that improves image-text alignment without relying on large negative batches; and (3) a dual-stream MLP-Mixer projector that fuses low- and high-level image features with text embeddings for richer multi-modal representations. We evaluate our model on the M3D dataset, which includes radiology reports and VQA data for 120,084 3D medical images. Results show that Med3DVLM achieves superior performance across multiple benchmarks. For image-text retrieval, it reaches 61.00% R@1 on 2,000 samples, significantly outperforming the current state-of-the-art M3D model (19.10%). For report generation, it achieves a METEOR score of 36.42% (vs. 14.38%). In open-ended visual question answering (VQA), it scores 36.76% METEOR (vs. 33.58%), and in closed-ended VQA, it achieves 79.95% accuracy (vs. 75.78%). These results highlight Med3DVLMâ€™s ability to bridge the gap between 3D imaging and language, enabling scalable, multi-task reasoning across clinical applications. Our code is publicly available at <a target="_blank" rel="noopener" href="https://github.com/mirthAI/Med3DVLM">https://github.com/mirthAI/Med3DVLM</a>. </p>
<blockquote>
<p>è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰åœ¨2DåŒ»å­¦å›¾åƒåˆ†ææ–¹é¢æ˜¾ç¤ºå‡ºæ½œåŠ›ï¼Œä½†å°†å…¶æ‰©å±•åˆ°3Dä»ç„¶å…·æœ‰æŒ‘æˆ˜æ€§ï¼Œä¸»è¦æ˜¯ç”±äºä½“ç§¯æ•°æ®çš„é«˜è®¡ç®—éœ€æ±‚å’Œå°†3Dç©ºé—´ç‰¹å¾ä¸ä¸´åºŠæ–‡æœ¬å¯¹é½çš„å›°éš¾ã€‚æˆ‘ä»¬æå‡ºäº†Med3DVLMï¼Œè¿™æ˜¯ä¸€ç§3D VLMï¼Œé€šè¿‡ä¸‰ä¸ªå…³é”®åˆ›æ–°æ¥è§£å†³è¿™äº›æŒ‘æˆ˜ï¼šï¼ˆ1ï¼‰DCFormerï¼Œä¸€ç§é«˜æ•ˆçš„ç¼–ç å™¨ï¼Œä½¿ç”¨åˆ†è§£çš„3Då·ç§¯æ¥æ•è·å¤§è§„æ¨¡çš„ç²¾ç»†ç©ºé—´ç‰¹å¾ï¼›ï¼ˆ2ï¼‰SigLIPï¼Œä¸€ç§å¯¹æ¯”å­¦ä¹ ç­–ç•¥ï¼Œä½¿ç”¨é…å¯¹SigmoidæŸå¤±æ¥æ”¹å–„å›¾åƒæ–‡æœ¬å¯¹é½ï¼Œè€Œæ— éœ€ä¾èµ–å¤§é‡è´Ÿé¢æ‰¹æ¬¡ï¼›ï¼ˆ3ï¼‰åŒæµMLPæ··åˆå™¨æŠ•å½±ä»ªï¼Œèåˆä½çº§å’Œé«˜çº§å›¾åƒç‰¹å¾ä¸æ–‡æœ¬åµŒå…¥ï¼Œä»¥äº§ç”Ÿæ›´ä¸°å¯Œçš„å¤šæ¨¡å¼è¡¨ç¤ºã€‚æˆ‘ä»¬åœ¨M3Dæ•°æ®é›†ä¸Šè¯„ä¼°äº†æˆ‘ä»¬çš„æ¨¡å‹ï¼Œè¯¥æ•°æ®é›†åŒ…å«ç”¨äº120,084ä¸ª3DåŒ»å­¦å›¾åƒçš„æ”¾å°„å­¦æŠ¥å‘Šå’ŒVQAæ•°æ®ã€‚ç»“æœè¡¨æ˜ï¼ŒMed3DVLMåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­å®ç°äº†å“è¶Šæ€§èƒ½ã€‚åœ¨å›¾åƒæ–‡æœ¬æ£€ç´¢æ–¹é¢ï¼Œå®ƒåœ¨2,000ä¸ªæ ·æœ¬ä¸Šè¾¾åˆ°äº†61.00ï¼…çš„R@1å‡†ç¡®ç‡ï¼Œæ˜¾è‘—ä¼˜äºå½“å‰æœ€å…ˆè¿›çš„M3Dæ¨¡å‹ï¼ˆ19.10ï¼…ï¼‰ã€‚åœ¨æŠ¥å‘Šç”Ÿæˆæ–¹é¢ï¼Œå®ƒçš„METEORå¾—åˆ†ä¸º36.42ï¼…ï¼ˆå¯¹æ¯”14.38ï¼…ï¼‰ã€‚åœ¨å¼€æ”¾å¼çš„è§†è§‰é—®ç­”ï¼ˆVQAï¼‰ä¸­ï¼Œå®ƒçš„METEORå¾—åˆ†ä¸º36.76ï¼…ï¼ˆå¯¹æ¯”33.58ï¼…ï¼‰ï¼Œè€Œåœ¨å°é—­å¼çš„VQAä¸­ï¼Œå®ƒè¾¾åˆ°äº†79.95ï¼…çš„å‡†ç¡®ç‡ï¼ˆå¯¹æ¯”75.78ï¼…ï¼‰ã€‚è¿™äº›ç»“æœçªå‡ºäº†Med3DVLMåœ¨è¿æ¥3Dæˆåƒå’Œè¯­è¨€æ–¹é¢çš„èƒ½åŠ›ï¼Œå®ç°äº†ä¸´åºŠåº”ç”¨ä¸­å¯ä¼¸ç¼©çš„å¤šä»»åŠ¡æ¨ç†ã€‚æˆ‘ä»¬çš„ä»£ç å…¬å¼€åœ¨<a target="_blank" rel="noopener" href="https://github.com/mirthAI/Med3DVLM%E3%80%82">https://github.com/mirthAI/Med3DVLMã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.20047v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†é’ˆå¯¹3DåŒ»å­¦å½±åƒåˆ†æçš„Med3DVLMæ¨¡å‹ã€‚è¯¥æ¨¡å‹é€šè¿‡ä¸‰ä¸ªå…³é”®åˆ›æ–°ç‚¹è§£å†³äº†å°†è§†è§‰è¯­è¨€æ¨¡å‹æ‰©å±•åˆ°3Dæ‰€é¢ä¸´çš„æŒ‘æˆ˜ï¼šé‡‡ç”¨DCFormeré«˜æ•ˆç¼–ç å™¨è¿›è¡Œç²¾ç»†ç©ºé—´ç‰¹å¾æ•æ‰ã€ä½¿ç”¨SigLIPå¯¹æ¯”å­¦ä¹ ç­–ç•¥æ”¹å–„å›¾åƒæ–‡æœ¬å¯¹é½ã€ä»¥åŠé€šè¿‡åŒæµMLPæ··åˆå™¨èåˆé«˜ä½çº§åˆ«å›¾åƒç‰¹å¾ä¸æ–‡æœ¬åµŒå…¥ä»¥ç”Ÿæˆæ›´ä¸°å¯Œå¤šæ¨¡æ€è¡¨ç¤ºã€‚åœ¨M3Dæ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒMed3DVLMåœ¨å¤šé¡¹åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œå›¾åƒæ–‡æœ¬æ£€ç´¢R@1è¾¾åˆ°61.00%ï¼ŒæŠ¥å‘Šç”ŸæˆMETEORå¾—åˆ†36.42%ï¼Œå¼€æ”¾ä¸å°é—­ç«¯è§†è§‰é—®ç­”ä¹Ÿæœ‰æ˜¾è‘—æˆç»©ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Med3DVLMæ˜¯ä¸€ä¸ªç”¨äºè§£å†³åŒ»å­¦å½±åƒä¸­æŒ‘æˆ˜çš„æ–°æ¨¡å‹ï¼Œä¸“æ³¨äºåœ¨å¤æ‚ä¸”é«˜éœ€æ±‚çš„3DåŒ»å­¦å½±åƒåˆ†æé¢†åŸŸåº”ç”¨è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰ã€‚</li>
<li>Med3DVLMæå‡ºä¸‰ä¸ªå…³é”®åˆ›æ–°ç‚¹ï¼ŒåŒ…æ‹¬é«˜æ•ˆç¼–ç å™¨DCFormerï¼Œæ”¹è¿›çš„å¯¹æ¯”å­¦ä¹ ç­–ç•¥SigLIPå’ŒåŒæµMLPæ··åˆå™¨æŠ•å½±æŠ€æœ¯ã€‚è¿™äº›æŠ€æœ¯ä¸ºç²¾ç»†ç©ºé—´ç‰¹å¾çš„æ•æ‰ã€å›¾åƒæ–‡æœ¬çš„ç²¾ç¡®å¯¹é½ä»¥åŠä¸°å¯Œå¤šæ¨¡æ€çš„ç”Ÿæˆæä¾›äº†å¼ºå¤§æ”¯æŒã€‚</li>
<li>æ¨¡å‹åœ¨M3Dæ•°æ®é›†ä¸Šè¿›è¡Œäº†å¹¿æ³›è¯„ä¼°ï¼ŒåŒ…æ‹¬å›¾åƒæ–‡æœ¬æ£€ç´¢ã€æŠ¥å‘Šç”Ÿæˆå’Œè§†è§‰é—®ç­”ä»»åŠ¡ã€‚ç»“æœæ˜¾ç¤ºMed3DVLMåœ¨å¤šä»»åŠ¡æ¨ç†æ–¹é¢è¡¨ç°å‡ºå¼ºå¤§çš„èƒ½åŠ›ã€‚å…¶åœ¨ä¸åŒä»»åŠ¡ä¸Šçš„æ€§èƒ½æ˜¾è‘—è¶…è¶Šç°æœ‰æ¨¡å‹ï¼Œå±•ç°å‡ºå…¶å¯¹åŒ»å­¦å›¾åƒå’Œè¯­è¨€çš„å‡ºè‰²ç†è§£å’Œèåˆèƒ½åŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.20047">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-5cf5d9f32a5722a2fa9b40e8fd59e3b9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c9d4edb63892626f0e51af91393acb91.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-3cf47ab0b550daa1b62d28becfded024.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Structure-Aware-Correspondence-Learning-for-Relative-Pose-Estimation"><a href="#Structure-Aware-Correspondence-Learning-for-Relative-Pose-Estimation" class="headerlink" title="Structure-Aware Correspondence Learning for Relative Pose Estimation"></a>Structure-Aware Correspondence Learning for Relative Pose Estimation</h2><p><strong>Authors:Yihan Chen, Wenfei Yang, Huan Ren, Shifeng Zhang, Tianzhu Zhang, Feng Wu</strong></p>
<p>Relative pose estimation provides a promising way for achieving object-agnostic pose estimation. Despite the success of existing 3D correspondence-based methods, the reliance on explicit feature matching suffers from small overlaps in visible regions and unreliable feature estimation for invisible regions. Inspired by humansâ€™ ability to assemble two object parts that have small or no overlapping regions by considering object structure, we propose a novel Structure-Aware Correspondence Learning method for Relative Pose Estimation, which consists of two key modules. First, a structure-aware keypoint extraction module is designed to locate a set of kepoints that can represent the structure of objects with different shapes and appearance, under the guidance of a keypoint based image reconstruction loss. Second, a structure-aware correspondence estimation module is designed to model the intra-image and inter-image relationships between keypoints to extract structure-aware features for correspondence estimation. By jointly leveraging these two modules, the proposed method can naturally estimate 3D-3D correspondences for unseen objects without explicit feature matching for precise relative pose estimation. Experimental results on the CO3D, Objaverse and LineMOD datasets demonstrate that the proposed method significantly outperforms prior methods, i.e., with 5.7{\deg}reduction in mean angular error on the CO3D dataset. </p>
<blockquote>
<p>ç›¸å¯¹å§¿æ€ä¼°è®¡ä¸ºç‹¬ç«‹äºå¯¹è±¡çš„å§¿æ€ä¼°è®¡æä¾›äº†ä¸€ä¸ªæœ‰å‰é€”çš„æ–¹æ³•ã€‚å°½ç®¡ç°æœ‰çš„åŸºäºä¸‰ç»´å¯¹åº”çš„è¿™äº›æ–¹æ³•å–å¾—äº†æˆåŠŸï¼Œä½†å®ƒä»¬ä¾èµ–äºæ˜¾å¼ç‰¹å¾åŒ¹é…ï¼Œè¿™å¯¼è‡´åœ¨å¯è§åŒºåŸŸçš„é‡å éƒ¨åˆ†è¾ƒå°ï¼Œå¹¶ä¸”å¯¹äºä¸å¯è§åŒºåŸŸè¿›è¡Œç‰¹å¾ä¼°è®¡æ—¶å­˜åœ¨ä¸å¯é æ€§ã€‚å—åˆ°äººç±»è€ƒè™‘å¯¹è±¡ç»“æ„æ¥ç»„åˆä¸¤ä¸ªå…·æœ‰å°æˆ–æ²¡æœ‰é‡å åŒºåŸŸçš„ç‰©ä½“éƒ¨åˆ†çš„èƒ½åŠ›çš„å¯å‘ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ç”¨äºç›¸å¯¹å§¿æ€ä¼°è®¡çš„æ–°å‹ç»“æ„æ„ŸçŸ¥å¯¹åº”å­¦ä¹ æ–¹æ³•ï¼Œè¯¥æ–¹æ³•ç”±ä¸¤ä¸ªå…³é”®æ¨¡å—ç»„æˆã€‚é¦–å…ˆï¼Œè®¾è®¡äº†ä¸€ä¸ªç»“æ„æ„ŸçŸ¥å…³é”®ç‚¹æå–æ¨¡å—ï¼Œç”¨äºå®šä½ä¸€ç»„å…³é”®ç‚¹ï¼Œè¿™äº›å…³é”®ç‚¹å¯ä»¥ä»£è¡¨ä¸åŒå½¢çŠ¶å’Œå¤–è§‚çš„å¯¹è±¡çš„ç»“æ„ï¼Œå¹¶åœ¨åŸºäºå…³é”®ç‚¹çš„å›¾åƒé‡å»ºæŸå¤±çš„æŒ‡å¯¼ä¸‹ã€‚å…¶æ¬¡ï¼Œè®¾è®¡äº†ä¸€ä¸ªç»“æ„æ„ŸçŸ¥å¯¹åº”ä¼°è®¡æ¨¡å—ï¼Œä»¥æ¨¡æ‹Ÿå…³é”®ç‚¹ä¹‹é—´çš„å›¾åƒå†…å’Œå›¾åƒé—´çš„å…³ç³»ï¼Œä»¥æå–ç”¨äºå¯¹åº”ä¼°è®¡çš„ç»“æ„æ„ŸçŸ¥ç‰¹å¾ã€‚é€šè¿‡è”åˆä½¿ç”¨è¿™ä¸¤ä¸ªæ¨¡å—ï¼Œè¯¥æ–¹æ³•å¯ä»¥è‡ªç„¶åœ°ä¼°è®¡æœªè§å¯¹è±¡çš„3D-3Då¯¹åº”å…³ç³»ï¼Œæ— éœ€è¿›è¡Œæ˜¾å¼ç‰¹å¾åŒ¹é…å³å¯å®ç°ç²¾ç¡®ç›¸å¯¹å§¿æ€ä¼°è®¡ã€‚åœ¨CO3Dã€Objaverseå’ŒLineMODæ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨CO3Dæ•°æ®é›†ä¸Šçš„å¹³å‡è§’åº¦è¯¯å·®é™ä½äº†5.7åº¦ï¼Œæ˜¾è‘—ä¼˜äºä»¥å‰çš„æ–¹æ³•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.18671v1">PDF</a> CVPR2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºä¸€ç§åŸºäºç»“æ„æ„ŸçŸ¥çš„å¯¹åº”å­¦ä¹ ï¼ˆStructure-Aware Correspondence Learningï¼‰æ–¹æ³•ç”¨äºç›¸å¯¹å§¿æ€ä¼°è®¡ï¼Œæ—¨åœ¨è§£å†³ç°æœ‰æ–¹æ³•åœ¨å°é‡å åŒºåŸŸå’Œä¸å¯è§åŒºåŸŸç‰¹å¾ä¼°è®¡ä¸å¯é çš„é—®é¢˜ã€‚é€šè¿‡è®¾è®¡ç»“æ„æ„ŸçŸ¥å…³é”®ç‚¹æå–æ¨¡å—å’Œç»“æ„æ„ŸçŸ¥å¯¹åº”ä¼°è®¡æ¨¡å—ï¼Œè¯¥æ–¹æ³•å¯è‡ªç„¶åœ°ä¼°è®¡æœªè§ç‰©ä½“çš„3D-3Då¯¹åº”å…³ç³»ï¼Œæ— éœ€æ˜¾å¼ç‰¹å¾åŒ¹é…å³å¯å®ç°ç²¾ç¡®ç›¸å¯¹å§¿æ€ä¼°è®¡ã€‚å®éªŒç»“æœåœ¨CO3Dã€Objaverseå’ŒLineMODæ•°æ®é›†ä¸Šè¡¨ç°å‡ºä¼˜è¶Šæ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç›¸å¯¹å§¿æ€ä¼°è®¡å¯¹äºå®ç°å¯¹è±¡æ— å…³çš„å§¿æ€ä¼°è®¡æä¾›äº†ä¸€ç§æœ‰å‰é€”çš„æ–¹æ³•ã€‚</li>
<li>å½“å‰3Då¯¹åº”æ–¹æ³•ä¾èµ–æ˜¾å¼ç‰¹å¾åŒ¹é…ï¼Œä½†å­˜åœ¨å°é‡å åŒºåŸŸå’Œä¸å¯è§åŒºåŸŸçš„é—®é¢˜ã€‚</li>
<li>æå‡ºçš„ç»“æ„æ„ŸçŸ¥å¯¹åº”å­¦ä¹ æ–¹æ³•åŒ…å«ä¸¤ä¸ªå…³é”®æ¨¡å—ï¼šç»“æ„æ„ŸçŸ¥å…³é”®ç‚¹æå–å’Œç»“æ„æ„ŸçŸ¥å¯¹åº”ä¼°è®¡ã€‚</li>
<li>ç»“æ„æ„ŸçŸ¥å…³é”®ç‚¹æå–æ¨¡å—å¯ä»¥å®šä½ä»£è¡¨ä¸åŒå½¢çŠ¶å’Œå¤–è§‚å¯¹è±¡ç»“æ„çš„å…³é”®ç‚¹ã€‚</li>
<li>ç»“æ„æ„ŸçŸ¥å¯¹åº”ä¼°è®¡æ¨¡å—å¯ä»¥å»ºæ¨¡å…³é”®ç‚¹ä¹‹é—´çš„å›¾åƒå†…å’Œå›¾åƒé—´å…³ç³»ï¼Œä»¥æå–ç”¨äºå¯¹åº”ä¼°è®¡çš„ç»“æ„æ„ŸçŸ¥ç‰¹å¾ã€‚</li>
<li>è¯¥æ–¹æ³•å¯è‡ªç„¶åœ°ä¼°è®¡æœªè§ç‰©ä½“çš„3D-3Då¯¹åº”å…³ç³»ï¼Œæ— éœ€æ˜¾å¼ç‰¹å¾åŒ¹é…ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.18671">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-a1ecf3f1c470e64dcfe7220112d58605.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-8eff55857d15e667ed72697e7773c3a4.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="RLCAD-Reinforcement-Learning-Training-Gym-for-Revolution-Involved-CAD-Command-Sequence-Generation"><a href="#RLCAD-Reinforcement-Learning-Training-Gym-for-Revolution-Involved-CAD-Command-Sequence-Generation" class="headerlink" title="RLCAD: Reinforcement Learning Training Gym for Revolution Involved CAD   Command Sequence Generation"></a>RLCAD: Reinforcement Learning Training Gym for Revolution Involved CAD   Command Sequence Generation</h2><p><strong>Authors:Xiaolong Yin, Xingyu Lu, Jiahang Shen, Jingzhe Ni, Hailong Li, Ruofeng Tong, Min Tang, Peng Du</strong></p>
<p>A CAD command sequence is a typical parametric design paradigm in 3D CAD systems where a model is constructed by overlaying 2D sketches with operations such as extrusion, revolution, and Boolean operations. Although there is growing academic interest in the automatic generation of command sequences, existing methods and datasets only support operations such as 2D sketching, extrusion,and Boolean operations. This limitation makes it challenging to represent more complex geometries. In this paper, we present a reinforcement learning (RL) training environment (gym) built on a CAD geometric engine. Given an input boundary representation (B-Rep) geometry, the policy network in the RL algorithm generates an action. This action, along with previously generated actions, is processed within the gym to produce the corresponding CAD geometry, which is then fed back into the policy network. The rewards, determined by the difference between the generated and target geometries within the gym, are used to update the RL network. Our method supports operations beyond sketches, Boolean, and extrusion, including revolution operations. With this training gym, we achieve state-of-the-art (SOTA) quality in generating command sequences from B-Rep geometries. In addition, our method can significantly improve the efficiency of command sequence generation by a factor of 39X compared with the previous training gym. </p>
<blockquote>
<p>è®¡ç®—æœºè¾…åŠ©è®¾è®¡ï¼ˆCADï¼‰å‘½ä»¤åºåˆ—æ˜¯ä¸‰ç»´CADç³»ç»Ÿä¸­å…¸å‹çš„å‚æ•°åŒ–è®¾è®¡èŒƒä¾‹ï¼Œå…¶ä¸­æ¨¡å‹æ˜¯é€šè¿‡å åŠ äºŒç»´è‰å›¾å¹¶ä½¿ç”¨æŒ¤å‹ã€æ—‹è½¬å’Œå¸ƒå°”è¿ç®—ç­‰æ“ä½œæ¥æ„å»ºçš„ã€‚å°½ç®¡å­¦æœ¯ç•Œå¯¹å‘½ä»¤åºåˆ—çš„è‡ªåŠ¨ç”Ÿæˆè¶Šæ¥è¶Šæ„Ÿå…´è¶£ï¼Œä½†ç°æœ‰æ–¹æ³•å’Œæ•°æ®é›†ä»…æ”¯æŒäºŒç»´è‰å›¾ã€æŒ¤å‹å’Œå¸ƒå°”è¿ç®—ç­‰æ“ä½œã€‚è¿™ä¸€å±€é™æ€§ä½¿å¾—è¡¨ç¤ºæ›´å¤æ‚çš„å‡ ä½•å½¢çŠ¶å…·æœ‰æŒ‘æˆ˜æ€§ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬å±•ç¤ºäº†ä¸€ä¸ªåŸºäºCADå‡ ä½•å¼•æ“çš„å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰è®­ç»ƒç¯å¢ƒï¼ˆgymï¼‰ã€‚ç»™å®šè¾“å…¥è¾¹ç•Œè¡¨ç¤ºï¼ˆB-Repï¼‰å‡ ä½•ä½“ï¼ŒRLç®—æ³•ä¸­çš„ç­–ç•¥ç½‘ç»œä¼šç”Ÿæˆä¸€ä¸ªåŠ¨ä½œã€‚è¿™ä¸ªåŠ¨ä½œä¸å…ˆå‰ç”Ÿæˆçš„åŠ¨ä½œä¸€èµ·åœ¨gymä¸­è¿›è¡Œå¤„ç†ï¼Œä»¥äº§ç”Ÿç›¸åº”çš„CADå‡ ä½•ä½“ï¼Œç„¶ååé¦ˆåˆ°ç­–ç•¥ç½‘ç»œã€‚å¥–åŠ±ç”±gymä¸­ç”Ÿæˆç›®æ ‡å‡ ä½•ä½“ä¹‹é—´çš„å·®å¼‚æ¥ç¡®å®šï¼Œç”¨äºæ›´æ–°RLç½‘ç»œã€‚æˆ‘ä»¬çš„æ–¹æ³•æ”¯æŒè‰å›¾ã€å¸ƒå°”å’ŒæŒ¤å‹æ“ä½œä»¥å¤–çš„æ“ä½œï¼ŒåŒ…æ‹¬æ—‹è½¬æ“ä½œã€‚ä½¿ç”¨è¿™ä¸ªè®­ç»ƒç¯å¢ƒï¼Œæˆ‘ä»¬åœ¨ä»B-Repå‡ ä½•ä½“ç”Ÿæˆå‘½ä»¤åºåˆ—æ–¹é¢è¾¾åˆ°äº†æœ€å…ˆè¿›çš„è´¨é‡ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨å‘½ä»¤åºåˆ—ç”Ÿæˆæ•ˆç‡ä¸Šæ¯”ä¹‹å‰çš„è®­ç»ƒç¯å¢ƒæé«˜äº†39å€ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.18549v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>è¯¥æ–‡ä»‹ç»äº†ä¸€ç§åŸºäºå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰çš„CADå‘½ä»¤åºåˆ—ç”Ÿæˆæ–¹æ³•ï¼Œè¯¥æ–¹æ³•èƒ½å¤Ÿåœ¨CADå‡ ä½•å¼•æ“ä¸Šæ„å»ºè®­ç»ƒç¯å¢ƒï¼ˆgymï¼‰ã€‚ç»™å®šè¾“å…¥è¾¹ç•Œè¡¨ç¤ºï¼ˆB-Repï¼‰å‡ ä½•ä½“ï¼Œç­–ç•¥ç½‘ç»œç”Ÿæˆç›¸åº”çš„æ“ä½œåŠ¨ä½œï¼Œé€šè¿‡gymå¤„ç†è¿™äº›åŠ¨ä½œç”Ÿæˆå¯¹åº”çš„CADå‡ ä½•ä½“ï¼Œå¹¶æ ¹æ®ç”Ÿæˆä¸ç›®æ ‡å‡ ä½•ä½“ä¹‹é—´çš„å·®å¼‚æ¥æ›´æ–°RLç½‘ç»œã€‚è¯¥æ–¹æ³•æ”¯æŒé™¤è‰å›¾ã€å¸ƒå°”è¿ç®—å’ŒæŒ¤å‹å¤–çš„æ—‹è½¬æ“ä½œï¼Œèƒ½æ˜¾è‘—æé«˜ä»B-Repå‡ ä½•ä½“ç”Ÿæˆå‘½ä»¤åºåˆ—çš„æ•ˆç‡å’Œè´¨é‡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è¯¥æ–‡æå‡ºäº†ä¸€ç§åŸºäºå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰çš„CADè®­ç»ƒç¯å¢ƒï¼ˆgymï¼‰ï¼Œç”¨äºç”ŸæˆCADå‘½ä»¤åºåˆ—ã€‚</li>
<li>ç»™å®šB-Repå‡ ä½•ä½“ä½œä¸ºè¾“å…¥ï¼Œç­–ç•¥ç½‘ç»œåœ¨RLç®—æ³•ä¸­ç”Ÿæˆæ“ä½œåŠ¨ä½œã€‚</li>
<li>åŠ¨ä½œåœ¨gymä¸­è¢«å¤„ç†ä»¥äº§ç”Ÿç›¸åº”çš„CADå‡ ä½•ä½“ï¼Œç„¶ååé¦ˆåˆ°ç­–ç•¥ç½‘ç»œä¸­ã€‚</li>
<li>å¥–åŠ±åŸºäºç”Ÿæˆå’Œç›®æ ‡å‡ ä½•ä½“ä¹‹é—´çš„å·®å¼‚æ¥ç¡®å®šï¼Œç”¨äºæ›´æ–°RLç½‘ç»œã€‚</li>
<li>è¯¥æ–¹æ³•æ”¯æŒæ›´å¤æ‚çš„æ“ä½œï¼ŒåŒ…æ‹¬æ—‹è½¬æ“ä½œï¼Œè¶…è¶Šäº†ç°æœ‰çš„è‰å›¾ã€å¸ƒå°”è¿ç®—å’ŒæŒ¤å‹æ“ä½œã€‚</li>
<li>ä¸ä¹‹å‰çš„è®­ç»ƒç¯å¢ƒç›¸æ¯”ï¼Œè¯¥æ–¹æ³•åœ¨ç”Ÿæˆå‘½ä»¤åºåˆ—çš„æ•ˆç‡ä¸Šæé«˜äº†39å€ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.18549">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-33e522606c0c91be3ca0a19c8ccbc72e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-bf0e934e1a11b517327c971ae57f58dd.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-27b2ed805ad2e00eef71e29363fe4959.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-fcb6cc913d611e99dd51a0327f7d94dc.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1de8791594e50afb4485e067714ef655.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-438502589776f10649ff4539beb07226.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="PG-SAM-Prior-Guided-SAM-with-Medical-for-Multi-organ-Segmentation"><a href="#PG-SAM-Prior-Guided-SAM-with-Medical-for-Multi-organ-Segmentation" class="headerlink" title="PG-SAM: Prior-Guided SAM with Medical for Multi-organ Segmentation"></a>PG-SAM: Prior-Guided SAM with Medical for Multi-organ Segmentation</h2><p><strong>Authors:Yiheng Zhong, Zihong Luo, Chengzhi Liu, Feilong Tang, Zelin Peng, Ming Hu, Yingzhen Hu, Jionglong Su, Zongyuan Ge, Imran Razzak</strong></p>
<p>Segment Anything Model (SAM) demonstrates powerful zero-shot capabilities; however, its accuracy and robustness significantly decrease when applied to medical image segmentation. Existing methods address this issue through modality fusion, integrating textual and image information to provide more detailed priors. In this study, we argue that the granularity of text and the domain gap affect the accuracy of the priors. Furthermore, the discrepancy between high-level abstract semantics and pixel-level boundary details in images can introduce noise into the fusion process. To address this, we propose Prior-Guided SAM (PG-SAM), which employs a fine-grained modality prior aligner to leverage specialized medical knowledge for better modality alignment. The core of our method lies in efficiently addressing the domain gap with fine-grained text from a medical LLM. Meanwhile, it also enhances the priorsâ€™ quality after modality alignment, ensuring more accurate segmentation. In addition, our decoder enhances the modelâ€™s expressive capabilities through multi-level feature fusion and iterative mask optimizer operations, supporting unprompted learning. We also propose a unified pipeline that effectively supplies high-quality semantic information to SAM. Extensive experiments on the Synapse dataset demonstrate that the proposed PG-SAM achieves state-of-the-art performance. Our code is released at <a target="_blank" rel="noopener" href="https://github.com/logan-0623/PG-SAM">https://github.com/logan-0623/PG-SAM</a>. </p>
<blockquote>
<p>Segment Anything Modelï¼ˆSAMï¼‰å±•ç°äº†å¼ºå¤§çš„é›¶æ ·æœ¬èƒ½åŠ›ï¼Œç„¶è€Œï¼Œå½“åº”ç”¨äºåŒ»å­¦å›¾åƒåˆ†å‰²æ—¶ï¼Œå…¶ç²¾åº¦å’Œç¨³å¥æ€§ä¼šæ˜¾è‘—é™ä½ã€‚ç°æœ‰æ–¹æ³•é€šè¿‡æ¨¡æ€èåˆæ¥è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæ•´åˆæ–‡æœ¬å’Œå›¾åƒä¿¡æ¯ä»¥æä¾›æ›´è¯¦ç»†çš„å…ˆéªŒçŸ¥è¯†ã€‚åœ¨è¿™é¡¹ç ”ç©¶ä¸­ï¼Œæˆ‘ä»¬è®¤ä¸ºæ–‡æœ¬çš„ç²’åº¦å’Œé¢†åŸŸå·®è·ä¼šå½±å“å…ˆéªŒçŸ¥è¯†çš„å‡†ç¡®æ€§ã€‚æ­¤å¤–ï¼Œé«˜çº§æŠ½è±¡è¯­ä¹‰å’Œå›¾åƒä¸­åƒç´ çº§è¾¹ç•Œç»†èŠ‚ä¹‹é—´çš„å·®å¼‚å¯èƒ½ä¼šç»™èåˆè¿‡ç¨‹å¸¦æ¥å™ªå£°ã€‚</p>
</blockquote>
<p>ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†Prior-Guided SAMï¼ˆPG-SAMï¼‰ï¼Œå®ƒé‡‡ç”¨ç²¾ç»†ç²’åº¦çš„æ¨¡æ€å…ˆéªŒå¯¹é½å™¨ï¼Œåˆ©ç”¨ä¸“ä¸šåŒ»å­¦çŸ¥è¯†å®ç°æ›´å¥½çš„æ¨¡æ€å¯¹é½ã€‚æˆ‘ä»¬çš„æ–¹æ³•çš„æ ¸å¿ƒåœ¨äºåˆ©ç”¨æ¥è‡ªåŒ»ç–—å¤§å‹è¯­è¨€æ¨¡å‹çš„ç²¾ç»†ç²’åº¦æ–‡æœ¬æœ‰æ•ˆåœ°è§£å†³é¢†åŸŸå·®è·é—®é¢˜ã€‚åŒæ—¶ï¼Œå®ƒè¿˜åœ¨æ¨¡æ€å¯¹é½åæé«˜äº†å…ˆéªŒçŸ¥è¯†çš„è´¨é‡ï¼Œç¡®ä¿æ›´ç²¾ç¡®çš„åˆ†å‰²ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬çš„è§£ç å™¨é€šè¿‡å¤šçº§ç‰¹å¾èåˆå’Œè¿­ä»£æ©è†œä¼˜åŒ–æ“ä½œå¢å¼ºæ¨¡å‹çš„è¡¨ç°èƒ½åŠ›ï¼Œæ”¯æŒæ— æç¤ºå­¦ä¹ ã€‚æˆ‘ä»¬è¿˜æå‡ºäº†ä¸€ä¸ªç»Ÿä¸€çš„ç®¡é“ï¼Œæœ‰æ•ˆåœ°ä¸ºSAMæä¾›é«˜è´¨é‡è¯­ä¹‰ä¿¡æ¯ã€‚åœ¨Synapseæ•°æ®é›†ä¸Šçš„å¤§é‡å®éªŒè¡¨æ˜ï¼Œæ‰€æå‡ºçš„PG-SAMè¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚æˆ‘ä»¬çš„ä»£ç å·²å‘å¸ƒåœ¨<a target="_blank" rel="noopener" href="https://github.com/logan-0623/PG-SAM%E3%80%82">https://github.com/logan-0623/PG-SAMã€‚</a></p>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.18227v3">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>SAMæ¨¡å‹åœ¨åŒ»å­¦å›¾åƒåˆ†å‰²ä¸­è¡¨ç°å‡ºå¼ºå¤§çš„é›¶æ ·æœ¬èƒ½åŠ›ï¼Œä½†å…¶å‡†ç¡®æ€§å’Œé²æ£’æ€§æœ‰æ‰€ä¸‹é™ã€‚ç°æœ‰æ–¹æ³•é€šè¿‡æ¨¡æ€èåˆå’Œæ–‡æœ¬å›¾åƒä¿¡æ¯é›†æˆæä¾›è¯¦ç»†å…ˆéªŒæ¥è§£å†³è¿™ä¸€é—®é¢˜ã€‚æœ¬ç ”ç©¶è®¤ä¸ºæ–‡æœ¬ç²’åº¦å’Œé¢†åŸŸå·®è·å½±å“å…ˆéªŒçš„å‡†ç¡®æ€§ã€‚ä¸ºè§£å†³æ­¤é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºPrior-Guided SAMï¼ˆPG-SAMï¼‰ï¼Œåˆ©ç”¨ç²¾ç»†ç²’åº¦æ¨¡æ€å…ˆéªŒå¯¹é½å™¨ï¼Œå€ŸåŠ©ä¸“ä¸šåŒ»å­¦çŸ¥è¯†å®ç°æ›´å¥½çš„æ¨¡æ€å¯¹é½ã€‚å…¶æ ¸å¿ƒåœ¨äºåˆ©ç”¨ç²¾ç»†ç²’åº¦æ–‡æœ¬å’ŒåŒ»ç–—å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰é«˜æ•ˆè§£å†³é¢†åŸŸå·®è·é—®é¢˜ï¼ŒåŒæ—¶æé«˜æ¨¡æ€å¯¹é½åçš„å…ˆéªŒè´¨é‡ï¼Œç¡®ä¿æ›´å‡†ç¡®çš„åˆ†å‰²ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬çš„è§£ç å™¨é€šè¿‡å¤šçº§åˆ«ç‰¹å¾èåˆå’Œè¿­ä»£æ©è†œä¼˜åŒ–æ“ä½œæé«˜äº†æ¨¡å‹çš„è¡¨è¾¾èƒ½åŠ›ã€‚åœ¨Synapseæ•°æ®é›†ä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼Œæå‡ºçš„PG-SAMè¾¾åˆ°äº†æœ€æ–°æŠ€æœ¯æ°´å¹³ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>SAMæ¨¡å‹åœ¨åŒ»å­¦å›¾åƒåˆ†å‰²ä¸­é¢ä¸´å‡†ç¡®æ€§å’Œé²æ£’æ€§é—®é¢˜ã€‚</li>
<li>ç°æœ‰æ–¹æ³•é€šè¿‡æ¨¡æ€èåˆé›†æˆæ–‡æœ¬å’Œå›¾åƒä¿¡æ¯ä»¥æ”¹å–„å…ˆéªŒçŸ¥è¯†ã€‚</li>
<li>æ–‡æœ¬ç²’åº¦å’Œé¢†åŸŸå·®è·å½±å“å…ˆéªŒçš„å‡†ç¡®æ€§ã€‚</li>
<li>æå‡ºPrior-Guided SAMï¼ˆPG-SAMï¼‰æ¨¡å‹ï¼Œåˆ©ç”¨ç²¾ç»†ç²’åº¦æ¨¡æ€å…ˆéªŒå¯¹é½å™¨è§£å†³ä¸Šè¿°é—®é¢˜ã€‚</li>
<li>PG-SAMåˆ©ç”¨ä¸“ä¸šåŒ»å­¦çŸ¥è¯†å’Œç²¾ç»†ç²’åº¦æ–‡æœ¬è§£å†³é¢†åŸŸå·®è·é—®é¢˜ï¼Œæé«˜æ¨¡æ€å¯¹é½åçš„å…ˆéªŒè´¨é‡ã€‚</li>
<li>è§£ç å™¨é€šè¿‡å¤šçº§åˆ«ç‰¹å¾èåˆå’Œè¿­ä»£æ©è†œä¼˜åŒ–æ“ä½œæé«˜äº†æ¨¡å‹çš„è¡¨è¾¾èƒ½åŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.18227">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-a6366180887c56e420227b49f60b7b07.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-668fe91cbd652ca4f1e6e3c175ba7b83.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-eb480b9d1de51ada663954d42cd67614.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9c89976041a5b62bad5335f55f7b7de0.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="Self-Attention-Diffusion-Models-for-Zero-Shot-Biomedical-Image-Segmentation-Unlocking-New-Frontiers-in-Medical-Imaging"><a href="#Self-Attention-Diffusion-Models-for-Zero-Shot-Biomedical-Image-Segmentation-Unlocking-New-Frontiers-in-Medical-Imaging" class="headerlink" title="Self-Attention Diffusion Models for Zero-Shot Biomedical Image   Segmentation: Unlocking New Frontiers in Medical Imaging"></a>Self-Attention Diffusion Models for Zero-Shot Biomedical Image   Segmentation: Unlocking New Frontiers in Medical Imaging</h2><p><strong>Authors:Abderrachid Hamrani, Anuradha Godavarty</strong></p>
<p>Producing high-quality segmentation masks for medical images is a fundamental challenge in biomedical image analysis. Recent research has explored large-scale supervised training to enable segmentation across various medical imaging modalities and unsupervised training to facilitate segmentation without dense annotations. However, constructing a model capable of segmenting diverse medical images in a zero-shot manner without any annotations remains a significant hurdle. This paper introduces the Attention Diffusion Zero-shot Unsupervised System (ADZUS), a novel approach that leverages self-attention diffusion models for zero-shot biomedical image segmentation. ADZUS harnesses the intrinsic capabilities of pre-trained diffusion models, utilizing their generative and discriminative potentials to segment medical images without requiring annotated training data or prior domain-specific knowledge. The ADZUS architecture is detailed, with its integration of self-attention mechanisms that facilitate context-aware and detail-sensitive segmentations being highlighted. Experimental results across various medical imaging datasets, including skin lesion segmentation, chest X-ray infection segmentation, and white blood cell segmentation, reveal that ADZUS achieves state-of-the-art performance. Notably, ADZUS reached Dice scores ranging from 88.7% to 92.9% and IoU scores from 66.3% to 93.3% across different segmentation tasks, demonstrating significant improvements in handling novel, unseen medical imagery. It is noteworthy that while ADZUS demonstrates high effectiveness, it demands substantial computational resources and extended processing times. The modelâ€™s efficacy in zero-shot settings underscores its potential to reduce reliance on costly annotations and seamlessly adapt to new medical imaging tasks, thereby expanding the diagnostic capabilities of AI-driven medical imaging technologies. </p>
<blockquote>
<p>åŒ»å­¦å›¾åƒé«˜è´¨é‡åˆ†å‰²æ©è†œçš„åˆ¶ä½œæ˜¯ç”Ÿç‰©åŒ»å­¦å›¾åƒåˆ†æä¸­çš„ä¸€é¡¹åŸºæœ¬æŒ‘æˆ˜ã€‚æœ€è¿‘çš„ç ”ç©¶æ¢ç´¢äº†å¤§è§„æ¨¡æœ‰ç›‘ç£è®­ç»ƒï¼Œä»¥å®ç°è·¨å„ç§åŒ»å­¦æˆåƒæ¨¡å¼çš„åˆ†å‰²ï¼Œä»¥åŠæ— ç›‘ç£è®­ç»ƒï¼Œä»¥ä¿ƒè¿›æ— éœ€å¯†é›†æ³¨é‡Šçš„åˆ†å‰²ã€‚ç„¶è€Œï¼Œæ„å»ºä¸€ç§èƒ½å¤Ÿåœ¨é›¶æ ·æœ¬æ–¹å¼ä¸‹å¯¹å¤šç§åŒ»å­¦å›¾åƒè¿›è¡Œåˆ†å‰²çš„æ¨¡å‹ä»ç„¶å­˜åœ¨é‡å¤§éšœç¢ã€‚æœ¬æ–‡ä»‹ç»äº†æ³¨æ„åŠ›æ‰©æ•£é›¶æ ·æœ¬æ— ç›‘ç£ç³»ç»Ÿï¼ˆADZUSï¼‰ï¼Œè¿™æ˜¯ä¸€ç§åˆ©ç”¨è‡ªæ³¨æ„åŠ›æ‰©æ•£æ¨¡å‹è¿›è¡Œé›¶æ ·æœ¬ç”Ÿç‰©åŒ»å­¦å›¾åƒåˆ†å‰²çš„æ–°æ–¹æ³•ã€‚ADZUSåˆ©ç”¨é¢„è®­ç»ƒæ‰©æ•£æ¨¡å‹çš„å†…åœ¨èƒ½åŠ›ï¼Œåˆ©ç”¨å…¶ç”Ÿæˆå’Œåˆ¤åˆ«æ½œåŠ›ï¼Œæ— éœ€æ³¨é‡Šçš„è®­ç»ƒæ•°æ®æˆ–ç‰¹å®šçš„é¢†åŸŸçŸ¥è¯†ï¼Œå¯¹åŒ»ç–—å›¾åƒè¿›è¡Œåˆ†å‰²ã€‚è¯¦ç»†ä»‹ç»äº†ADZUSæ¶æ„ï¼Œé‡ç‚¹ä»‹ç»äº†å…¶æ•´åˆè‡ªæ³¨æ„åŠ›æœºåˆ¶ï¼Œä¿ƒè¿›ä¸Šä¸‹æ–‡æ„ŸçŸ¥å’Œç»†èŠ‚æ•æ„Ÿçš„åˆ†å‰²ã€‚åœ¨å„ç§åŒ»å­¦å½±åƒæ•°æ®é›†ä¸Šçš„å®éªŒç»“æœï¼ŒåŒ…æ‹¬çš®è‚¤ç—…å˜åˆ†å‰²ã€èƒ¸éƒ¨Xå°„çº¿æ„ŸæŸ“åˆ†å‰²å’Œç™½ç»†èƒåˆ†å‰²ï¼Œè¡¨æ˜ADZUSè¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼ŒADZUSåœ¨ä¸åŒåˆ†å‰²ä»»åŠ¡ä¸Šçš„Diceå¾—åˆ†ç‡åœ¨88.7%åˆ°92.9%ä¹‹é—´ï¼ŒIoUå¾—åˆ†ç‡åœ¨66.3%åˆ°93.3%ä¹‹é—´ï¼Œåœ¨å¤„ç†æ–°å‹æœªè§åŒ»å­¦å›¾åƒæ–¹é¢å–å¾—äº†æ˜¾è‘—æ”¹è¿›ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œè™½ç„¶ADZUSè¡¨ç°å‡ºå¾ˆé«˜çš„æœ‰æ•ˆæ€§ï¼Œä½†å®ƒéœ€è¦å¤§é‡çš„è®¡ç®—èµ„æºå’Œè¾ƒé•¿çš„å¤„ç†æ—¶é—´ã€‚è¯¥æ¨¡å‹åœ¨é›¶æ ·æœ¬è®¾ç½®ä¸­çš„æœ‰æ•ˆæ€§çªå‡ºäº†å…¶å‡å°‘å¯¹æ˜‚è´µæ³¨é‡Šçš„ä¾èµ–ï¼Œå¹¶æ— ç¼é€‚åº”æ–°åŒ»å­¦æˆåƒä»»åŠ¡çš„èƒ½åŠ›ï¼Œä»è€Œæ‰©å¤§äº†äººå·¥æ™ºèƒ½é©±åŠ¨åŒ»å­¦æˆåƒæŠ€æœ¯çš„è¯Šæ–­èƒ½åŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.18170v1">PDF</a> 15 pages, 5 figures</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°å‹é›¶æ ·æœ¬æ— ç›‘ç£åŒ»å­¦å›¾åƒåˆ†å‰²æ–¹æ³•â€”â€”æ³¨æ„åŠ›æ‰©æ•£é›¶æ ·æœ¬ç³»ç»Ÿï¼ˆADZUSï¼‰ã€‚è¯¥æ–¹æ³•åˆ©ç”¨é¢„è®­ç»ƒçš„æ‰©æ•£æ¨¡å‹çš„ç”Ÿæˆå’Œåˆ¤åˆ«æ½œåŠ›ï¼Œå®ç°äº†åŒ»å­¦å›¾åƒçš„é›¶æ ·æœ¬åˆ†å‰²ï¼Œæ— éœ€æ ‡æ³¨çš„è®­ç»ƒæ•°æ®æˆ–ç‰¹å®šçš„é¢†åŸŸçŸ¥è¯†ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒADZUSåœ¨å¤šç§åŒ»å­¦å›¾åƒæ•°æ®é›†ä¸Šå–å¾—äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼ŒåŒ…æ‹¬çš®è‚¤ç—…å˜åˆ†å‰²ã€èƒ¸éƒ¨Xå°„çº¿æ„ŸæŸ“åˆ†å‰²å’Œç™½ç»†èƒåˆ†å‰²ç­‰ã€‚ç„¶è€Œï¼ŒADZUSéœ€è¦å¤§é‡çš„è®¡ç®—èµ„æºå’Œè¾ƒé•¿çš„å¤„ç†æ—¶é—´ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>åŒ»å­¦å›¾åƒåˆ†å‰²ä¸­é¢ä¸´çš„ä¸»è¦æŒ‘æˆ˜æ˜¯ç”Ÿæˆé«˜è´¨é‡åˆ†å‰²æ©è†œã€‚</li>
<li>æœ€è¿‘çš„ç ”ç©¶æ¢ç´¢äº†å¤§è§„æ¨¡ç›‘ç£è®­ç»ƒå’ŒåŠç›‘ç£è®­ç»ƒæ–¹æ³•æ¥è§£å†³è¯¥é—®é¢˜ã€‚</li>
<li>æ— æ ‡æ³¨æ•°æ®çš„é›¶æ ·æœ¬åŒ»å­¦å›¾åƒåˆ†å‰²ä»ç„¶æ˜¯ä¸€ä¸ªé‡å¤§æŒ‘æˆ˜ã€‚</li>
<li>æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°å‹æ–¹æ³•ADZUSï¼Œåˆ©ç”¨è‡ªæ³¨æ„åŠ›æ‰©æ•£æ¨¡å‹å®ç°é›¶æ ·æœ¬ç”Ÿç‰©åŒ»å­¦å›¾åƒåˆ†å‰²ã€‚</li>
<li>ADZUSåˆ©ç”¨é¢„è®­ç»ƒçš„æ‰©æ•£æ¨¡å‹çš„ç”Ÿæˆå’Œåˆ¤åˆ«èƒ½åŠ›è¿›è¡ŒåŒ»å­¦å›¾åƒåˆ†å‰²ã€‚</li>
<li>å®éªŒç»“æœæ˜¾ç¤ºADZUSåœ¨å¤šç§åŒ»å­¦å›¾åƒæ•°æ®é›†ä¸Šå–å¾—äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚</li>
<li>ADZUSè™½ç„¶æœ‰æ•ˆï¼Œä½†éœ€è¦å¤§é‡çš„è®¡ç®—èµ„æºå’Œå¤„ç†æ—¶é—´ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.18170">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-eef01e99272d3a86ac6e27e270ec3601.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-501451b069b9ada84695b549d776e260.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c01755a2c0f396d0e8692c0d44432d95.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="Histomorphology-driven-multi-instance-learning-for-breast-cancer-WSI-classification"><a href="#Histomorphology-driven-multi-instance-learning-for-breast-cancer-WSI-classification" class="headerlink" title="Histomorphology-driven multi-instance learning for breast cancer WSI   classification"></a>Histomorphology-driven multi-instance learning for breast cancer WSI   classification</h2><p><strong>Authors:Baizhi Wang, Rui Yan, Wenxin Ma, Xu Zhang, Yuhao Wang, Xiaolong Li, Yunjie Gu, Zihang Jiang, S. Kevin Zhou</strong></p>
<p>Histomorphology is crucial in breast cancer diagnosis. However, existing whole slide image (WSI) classification methods struggle to effectively incorporate histomorphology information, limiting their ability to capture key and fine-grained pathological features. To address this limitation, we propose a novel framework that explicitly incorporates histomorphology (tumor cellularity, cellular morphology, and tissue architecture) into WSI classification. Specifically, our approach consists of three key components: (1) estimating the importance of tumor-related histomorphology information at the patch level based on medical prior knowledge; (2) generating representative cluster-level features through histomorphology-driven cluster pooling; and (3) enabling WSI-level classification through histomorphology-driven multi-instance aggregation. With the incorporation of histomorphological information, our framework strengthens the modelâ€™s ability to capture key and fine-grained pathological patterns, thereby enhancing WSI classification performance. Experimental results demonstrate its effectiveness, achieving high diagnostic accuracy for molecular subtyping and cancer subtyping. The code will be made available at <a target="_blank" rel="noopener" href="https://github.com/Badgewho/HMDMIL">https://github.com/Badgewho/HMDMIL</a>. </p>
<blockquote>
<p>ç»„ç»‡å½¢æ€å­¦åœ¨ä¹³è…ºç™Œè¯Šæ–­ä¸­è‡³å…³é‡è¦ã€‚ç„¶è€Œï¼Œç°æœ‰çš„å…¨åˆ‡ç‰‡å›¾åƒï¼ˆWSIï¼‰åˆ†ç±»æ–¹æ³•åœ¨èå…¥ç»„ç»‡å½¢æ€å­¦ä¿¡æ¯æ–¹é¢å­˜åœ¨å›°éš¾ï¼Œè¿™é™åˆ¶äº†å®ƒä»¬æ•æ‰å…³é”®å’Œç²¾ç»†ç—…ç†ç‰¹å¾çš„èƒ½åŠ›ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°å‹æ¡†æ¶ï¼Œè¯¥æ¡†æ¶æ˜ç¡®åœ°å°†ç»„ç»‡å½¢æ€å­¦ï¼ˆè‚¿ç˜¤ç»†èƒæ€§ã€ç»†èƒå½¢æ€å’Œç»„ç»‡ç»“æ„ï¼‰èå…¥WSIåˆ†ç±»ä¸­ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åŒ…æ‹¬ä¸‰ä¸ªå…³é”®éƒ¨åˆ†ï¼šï¼ˆ1ï¼‰åŸºäºåŒ»å­¦å…ˆéªŒçŸ¥è¯†åœ¨æ–‘å—å±‚é¢è¯„ä¼°è‚¿ç˜¤ç›¸å…³ç»„ç»‡å½¢æ€å­¦ä¿¡æ¯çš„é‡è¦æ€§ï¼›ï¼ˆ2ï¼‰é€šè¿‡ç»„ç»‡å½¢æ€å­¦é©±åŠ¨çš„èšç±»æ± åŒ–ç”Ÿæˆå…·æœ‰ä»£è¡¨æ€§çš„èšç±»çº§åˆ«ç‰¹å¾ï¼›ï¼ˆ3ï¼‰é€šè¿‡ç»„ç»‡å½¢æ€å­¦é©±åŠ¨çš„å¤šå®ä¾‹èšåˆå®ç°WSIçº§åˆ«çš„åˆ†ç±»ã€‚é€šè¿‡èå…¥ç»„ç»‡å½¢æ€å­¦ä¿¡æ¯ï¼Œæˆ‘ä»¬çš„æ¡†æ¶å¢å¼ºäº†æ¨¡å‹æ•æ‰å…³é”®å’Œç²¾ç»†ç—…ç†æ¨¡å¼çš„èƒ½åŠ›ï¼Œä»è€Œæé«˜äº†WSIåˆ†ç±»æ€§èƒ½ã€‚å®éªŒç»“æœè¯æ˜äº†å…¶æœ‰æ•ˆæ€§ï¼Œå®ç°äº†åˆ†å­äºšå‹å’Œç™Œç—‡äºšå‹çš„è¯Šæ–­é«˜å‡†ç¡®æ€§ã€‚ä»£ç å°†åœ¨<a target="_blank" rel="noopener" href="https://github.com/Badgewho/HMDMIL">https://github.com/Badgewho/HMDMIL</a>ä¸Šæä¾›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.17983v1">PDF</a> 10 pages,5 figures</p>
<p><strong>Summary</strong><br>åŒ»å­¦å›¾åƒä¸­çš„ä¹³è…ºç™Œè¯Šæ–­éœ€è¦å…³æ³¨ç»†èƒå½¢æ€å’Œç»„ç»‡ç»“æ„ç­‰æ˜¾å¾®ç»“æ„ä¿¡æ¯ã€‚ç°æœ‰çš„å…¨å¹»ç¯ç‰‡å›¾åƒåˆ†ç±»æ–¹æ³•éš¾ä»¥æœ‰æ•ˆæ•´åˆè¿™äº›ä¿¡æ¯ï¼Œå› æ­¤æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„æ¡†æ¶ï¼ŒåŒ…æ‹¬åŸºäºåŒ»å­¦å…ˆéªŒçŸ¥è¯†çš„æ˜¾å¾®ç»“æ„ä¿¡æ¯é‡è¦æ€§è¯„ä¼°ã€åŸºäºæ˜¾å¾®ç»“æ„é©±åŠ¨çš„èšç±»æ± åŒ–å’Œå¤šå®ä¾‹èšåˆç­‰æ–¹æ³•ï¼Œä»¥æé«˜å…¨å¹»ç¯ç‰‡å›¾åƒåˆ†ç±»çš„æ€§èƒ½ï¼Œå®ç°åˆ†å­äºšå‹å’Œç™Œç—‡äºšå‹çš„å‡†ç¡®è¯Šæ–­ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ä¹³è…ºç™Œè¯Šæ–­ä¸­ï¼Œæ˜¾å¾®ç»“æ„ä¿¡æ¯ï¼ˆå¦‚è‚¿ç˜¤ç»†èƒæ€§ã€ç»†èƒå½¢æ€å’Œç»„ç»‡ç»“æ„ï¼‰è‡³å…³é‡è¦ã€‚</li>
<li>å½“å‰çš„å…¨å¹»ç¯ç‰‡å›¾åƒåˆ†ç±»æ–¹æ³•éš¾ä»¥æœ‰æ•ˆæ•´åˆæ˜¾å¾®ç»“æ„ä¿¡æ¯ã€‚</li>
<li>æ–°æ¡†æ¶æå‡ºä¸€ç§æ•´åˆæ˜¾å¾®ç»“æ„ä¿¡æ¯çš„æ–¹æ³•ï¼Œä»¥æé«˜å…¨å¹»ç¯ç‰‡å›¾åƒåˆ†ç±»çš„æ€§èƒ½ã€‚</li>
<li>è¯¥æ¡†æ¶åŒ…æ‹¬ä¸‰ä¸ªå…³é”®ç»„ä»¶ï¼šåŸºäºåŒ»å­¦å…ˆéªŒçŸ¥è¯†çš„æ˜¾å¾®ç»“æ„ä¿¡æ¯é‡è¦æ€§è¯„ä¼°ã€åŸºäºæ˜¾å¾®ç»“æ„é©±åŠ¨çš„èšç±»æ± åŒ–å’Œå¤šå®ä¾‹èšåˆã€‚</li>
<li>é€šè¿‡æ•´åˆæ˜¾å¾®ç»“æ„ä¿¡æ¯ï¼Œè¯¥æ¡†æ¶èƒ½å¤Ÿæ•æ‰å…³é”®å’Œç²¾ç»†çš„ç—…ç†æ¨¡å¼ã€‚</li>
<li>å®éªŒç»“æœè¡¨æ˜è¯¥æ¡†æ¶åœ¨åˆ†å­äºšå‹å’Œç™Œç—‡äºšå‹è¯Šæ–­æ–¹é¢çš„é«˜å‡†ç¡®æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.17983">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-7fd54859c32ae3c7eb0c962c0add6fcd.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-20ebfbdc0aa5e2dbb1ccc257b0b1ceab.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-3a8b108843f3f7fc9dbc7b5cac66cfea.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="PathoHR-Breast-Cancer-Survival-Prediction-on-High-Resolution-Pathological-Images"><a href="#PathoHR-Breast-Cancer-Survival-Prediction-on-High-Resolution-Pathological-Images" class="headerlink" title="PathoHR: Breast Cancer Survival Prediction on High-Resolution   Pathological Images"></a>PathoHR: Breast Cancer Survival Prediction on High-Resolution   Pathological Images</h2><p><strong>Authors:Yang Luo, Shiru Wang, Jun Liu, Jiaxuan Xiao, Rundong Xue, Zeyu Zhang, Hao Zhang, Yu Lu, Yang Zhao, Yutong Xie</strong></p>
<p>Breast cancer survival prediction in computational pathology presents a remarkable challenge due to tumor heterogeneity. For instance, different regions of the same tumor in the pathology image can show distinct morphological and molecular characteristics. This makes it difficult to extract representative features from whole slide images (WSIs) that truly reflect the tumorâ€™s aggressive potential and likely survival outcomes. In this paper, we present PathoHR, a novel pipeline for accurate breast cancer survival prediction that enhances any size of pathological images to enable more effective feature learning. Our approach entails (1) the incorporation of a plug-and-play high-resolution Vision Transformer (ViT) to enhance patch-wise WSI representation, enabling more detailed and comprehensive feature extraction, (2) the systematic evaluation of multiple advanced similarity metrics for comparing WSI-extracted features, optimizing the representation learning process to better capture tumor characteristics, (3) the demonstration that smaller image patches enhanced follow the proposed pipeline can achieve equivalent or superior prediction accuracy compared to raw larger patches, while significantly reducing computational overhead. Experimental findings valid that PathoHR provides the potential way of integrating enhanced image resolution with optimized feature learning to advance computational pathology, offering a promising direction for more accurate and efficient breast cancer survival prediction. Code will be available at <a target="_blank" rel="noopener" href="https://github.com/AIGeeksGroup/PathoHR">https://github.com/AIGeeksGroup/PathoHR</a>. </p>
<blockquote>
<p>ä¹³è…ºç™Œåœ¨ç—…ç†è®¡ç®—ä¸­çš„ç”Ÿå­˜é¢„æµ‹æ˜¯ä¸€ä¸ªå·¨å¤§çš„æŒ‘æˆ˜ï¼Œå› ä¸ºè‚¿ç˜¤çš„å¼‚è´¨æ€§ã€‚ä¾‹å¦‚ï¼Œç—…ç†å›¾åƒä¸­çš„åŒä¸€è‚¿ç˜¤çš„ä¸åŒåŒºåŸŸå¯èƒ½ä¼šæ˜¾ç¤ºå‡ºä¸åŒçš„å½¢æ€å’Œåˆ†å­ç‰¹å¾ã€‚è¿™ä½¿å¾—ä»å…¨ç‰‡å›¾åƒï¼ˆWSIsï¼‰ä¸­æå–çœŸæ­£åæ˜ è‚¿ç˜¤ä¾µè¢­æ½œåŠ›å’Œå¯èƒ½çš„ç”Ÿå­˜ç»“æœçš„ä»£è¡¨æ€§ç‰¹å¾å˜å¾—å›°éš¾ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†PathoHRï¼Œè¿™æ˜¯ä¸€ç§ç”¨äºå‡†ç¡®é¢„æµ‹ä¹³è…ºç™Œç”Ÿå­˜çš„æ–°å‹ç®¡é“ï¼Œå¯ä»¥å¯¹ä»»ä½•å¤§å°çš„ç—…ç†å›¾åƒè¿›è¡Œå¢å¼ºï¼Œä»¥å®ç°æ›´æœ‰æ•ˆçš„ç‰¹å¾å­¦ä¹ ã€‚æˆ‘ä»¬çš„æ–¹æ³•åŒ…æ‹¬ï¼ˆ1ï¼‰é‡‡ç”¨å³æ’å³ç”¨çš„é«˜åˆ†è¾¨ç‡è§†è§‰è½¬æ¢å™¨ï¼ˆViTï¼‰æ¥å¢å¼ºè¡¥ä¸å¼WSIè¡¨ç¤ºï¼Œä»è€Œå®ç°æ›´è¯¦ç»†å’Œå…¨é¢çš„ç‰¹å¾æå–ï¼›ï¼ˆ2ï¼‰ç³»ç»Ÿè¯„ä¼°å¤šç§å…ˆè¿›çš„ç›¸ä¼¼æ€§åº¦é‡æŒ‡æ ‡ï¼Œä»¥æ¯”è¾ƒWSIæå–çš„ç‰¹å¾ï¼Œä¼˜åŒ–è¡¨ç¤ºå­¦ä¹ è¿‡ç¨‹ä»¥æ›´å¥½åœ°æ•è·è‚¿ç˜¤ç‰¹å¾ï¼›ï¼ˆ3ï¼‰æ¼”ç¤ºéµå¾ªæ‰€æå‡ºç®¡é“çš„å°å‹å›¾åƒå—å¢å¼ºåå¯ä»¥è¾¾åˆ°ä¸åŸå§‹è¾ƒå¤§å—ç›¸å½“çš„é¢„æµ‹ç²¾åº¦ï¼Œç”šè‡³æ›´é«˜ï¼ŒåŒæ—¶å¤§å¤§é™ä½è®¡ç®—å¼€é”€ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒPathoHRé€šè¿‡å°†å¢å¼ºçš„å›¾åƒåˆ†è¾¨ç‡ä¸ä¼˜åŒ–åçš„ç‰¹å¾å­¦ä¹ ç›¸ç»“åˆï¼Œä¸ºè®¡ç®—ç—…ç†å­¦æä¾›äº†æ½œåœ¨çš„å‘å±•é€”å¾„ï¼Œä¸ºæ›´å‡†ç¡®ã€æ›´æœ‰æ•ˆçš„ä¹³è…ºç™Œç”Ÿå­˜é¢„æµ‹æä¾›äº†æœ‰å‰æ™¯çš„æ–¹å‘ã€‚ä»£ç å°†åœ¨<a target="_blank" rel="noopener" href="https://github.com/AIGeeksGroup/PathoHR%E4%B8%8A%E6%8F%90%E4%BE%9B%E3%80%82">https://github.com/AIGeeksGroup/PathoHRä¸Šæä¾›ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.17970v1">PDF</a> </p>
<p><strong>Summary</strong><br>     ä¹³è…ºç™Œç”Ÿå­˜é¢„æµ‹åœ¨ç—…ç†è®¡ç®—ä¸­æ˜¯ä¸€é¡¹é‡å¤§æŒ‘æˆ˜ï¼Œå› è‚¿ç˜¤å¼‚è´¨æ€§å¯¼è‡´ã€‚æœ¬æ–‡æå‡ºPathoHRæ¨¡å‹ï¼Œè¯¥æ¨¡å‹ç»“åˆé«˜åˆ†è¾¨ç‡Vision Transformerï¼Œåº”ç”¨å…ˆè¿›ç›¸ä¼¼æ€§åº¦é‡æŒ‡æ ‡è¿›è¡Œä¼˜åŒ–è¯„ä¼°ï¼Œå¹¶å¯¹è¾ƒå°çš„å›¾åƒå—è¿›è¡Œå¢å¼ºå¤„ç†ï¼Œæé«˜ä¹³è…ºç™Œç”Ÿå­˜é¢„æµ‹çš„å‡†ç¡®æ€§å¹¶é™ä½è®¡ç®—å¼€é”€ã€‚ä»£ç å°†å…¬å¼€äºGitHubä¸Šã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ä¹³è…ºç™Œç”Ÿå­˜é¢„æµ‹åœ¨ç—…ç†è®¡ç®—ä¸­æ˜¯ä¸€é¡¹æŒ‘æˆ˜ï¼Œä¸»è¦ç”±äºè‚¿ç˜¤å¼‚è´¨æ€§é€ æˆçš„å›°éš¾ã€‚</li>
<li>PathoHRæ¨¡å‹é€šè¿‡ç»“åˆé«˜åˆ†è¾¨ç‡Vision Transformerï¼Œå¢å¼ºå›¾åƒè¡¨ç¤ºèƒ½åŠ›ï¼Œæé«˜ç‰¹å¾æå–çš„è¯¦ç»†æ€§å’Œå®Œæ•´æ€§ã€‚</li>
<li>é‡‡ç”¨å…ˆè¿›çš„ç›¸ä¼¼æ€§åº¦é‡æŒ‡æ ‡ï¼Œä¼˜åŒ–ç‰¹å¾è¡¨ç¤ºå­¦ä¹ è¿‡ç¨‹ï¼Œæ›´å¥½åœ°æ•æ‰è‚¿ç˜¤ç‰¹æ€§ã€‚</li>
<li>é€šè¿‡å¢å¼ºè¾ƒå°çš„å›¾åƒå—å¹¶éµå¾ªPathoHRç®¡é“ï¼Œå¯ä»¥è¾¾åˆ°ä¸åŸå§‹è¾ƒå¤§å—ç›¸å½“æˆ–æ›´é«˜çš„é¢„æµ‹ç²¾åº¦ã€‚</li>
<li>PathoHRæ¨¡å‹æ˜¾è‘—æé«˜è®¡ç®—æ•ˆç‡ï¼Œé™ä½è®¡ç®—å¼€é”€ã€‚</li>
<li>PathoHRæ¨¡å‹çš„ä»£ç å°†å…¬å¼€äºGitHubä¸Šï¼Œä¾¿äºå…±äº«å’Œè¿›ä¸€æ­¥å¼€å‘ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.17970">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-f4aef8202ff9ee47de44638c74173e2d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9dba6759144e17df99c3c7a6c83a695e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0e77e063677b3a553fd52e86ca30210a.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="GOAL-Global-local-Object-Alignment-Learning"><a href="#GOAL-Global-local-Object-Alignment-Learning" class="headerlink" title="GOAL: Global-local Object Alignment Learning"></a>GOAL: Global-local Object Alignment Learning</h2><p><strong>Authors:Hyungyu Choi, Young Kyun Jang, Chanho Eom</strong></p>
<p>Vision-language models like CLIP have shown impressive capabilities in aligning images and text, but they often struggle with lengthy and detailed text descriptions because of their training focus on short and concise captions. We present GOAL (Global-local Object Alignment Learning), a novel fine-tuning method that enhances CLIPâ€™s ability to handle lengthy text by leveraging both global and local semantic alignments between image and lengthy text. Our approach consists of two key components: Local Image-Sentence Matching (LISM), which identifies corresponding pairs between image segments and descriptive sentences, and Token Similarity-based Learning (TSL), which efficiently propagates local element attention through these matched pairs. Evaluating GOAL on three new benchmarks for image-lengthy text retrieval, we demonstrate significant improvements over baseline CLIP fine-tuning, establishing a simple yet effective approach for adapting CLIP to detailed textual descriptions. Through extensive experiments, we show that our methodâ€™s focus on local semantic alignment alongside global context leads to more nuanced and representative embeddings, particularly beneficial for tasks requiring fine-grained understanding of lengthy text descriptions. </p>
<blockquote>
<p>CLIPç­‰è§†è§‰è¯­è¨€æ¨¡å‹åœ¨å›¾åƒå’Œæ–‡æœ¬çš„åŒ¹é…ä¸Šå±•ç°äº†ä»¤äººå°è±¡æ·±åˆ»çš„èƒ½åŠ›ï¼Œä½†ç”±äºå…¶è®­ç»ƒä¸»è¦é›†ä¸­åœ¨ç®€çŸ­ç²¾æ‚çš„æ ‡é¢˜ä¸Šï¼Œå› æ­¤åœ¨å¤„ç†å†—é•¿ä¸”è¯¦ç»†çš„æ–‡æœ¬æè¿°æ—¶å¸¸å¸¸ä¼šé‡åˆ°å›°éš¾ã€‚æˆ‘ä»¬æå‡ºäº†GOALï¼ˆå…¨å±€-å±€éƒ¨å¯¹è±¡å¯¹é½å­¦ä¹ ï¼‰æ–¹æ³•ï¼Œè¿™æ˜¯ä¸€ç§æ–°å‹çš„å¾®è°ƒæŠ€æœ¯ï¼Œå®ƒé€šè¿‡åˆ©ç”¨å›¾åƒå’Œå†—é•¿æ–‡æœ¬ä¹‹é—´çš„å…¨å±€å’Œå±€éƒ¨è¯­ä¹‰å¯¹é½ï¼Œæå‡CLIPå¤„ç†å†—é•¿æ–‡æœ¬çš„èƒ½åŠ›ã€‚æˆ‘ä»¬çš„æ–¹æ³•åŒ…å«ä¸¤ä¸ªå…³é”®ç»„æˆéƒ¨åˆ†ï¼šå±€éƒ¨å›¾åƒ-å¥å­åŒ¹é…ï¼ˆLISMï¼‰ï¼Œå®ƒè´Ÿè´£è¯†åˆ«å›¾åƒç‰‡æ®µå’Œæè¿°æ€§å¥å­ä¹‹é—´çš„å¯¹åº”å¯¹ï¼›ä»¥åŠåŸºäºæ ‡è®°ç›¸ä¼¼æ€§çš„å­¦ä¹ ï¼ˆTSLï¼‰ï¼Œå®ƒé€šè¿‡åŒ¹é…çš„å±€éƒ¨å…ƒç´ å¯¹æœ‰æ•ˆåœ°ä¼ æ’­å±€éƒ¨å…ƒç´ æ³¨æ„åŠ›ã€‚æˆ‘ä»¬åœ¨ä¸‰ä¸ªæ–°çš„å›¾åƒå†—é•¿æ–‡æœ¬æ£€ç´¢åŸºå‡†æµ‹è¯•ä¸Šå¯¹GOALè¿›è¡Œè¯„ä¼°ï¼Œè¯æ˜äº†å…¶åœ¨åŸºçº¿CLIPå¾®è°ƒä¸Šçš„æ˜¾è‘—æ”¹è¿›ï¼Œä»è€Œç¡®ç«‹äº†ä¸€ç§ç®€å•æœ‰æ•ˆçš„é€‚åº”è¯¦ç»†æ–‡æœ¬æè¿°çš„æ–¹æ³•ã€‚é€šè¿‡å¤§é‡å®éªŒï¼Œæˆ‘ä»¬è¯æ˜äº†æˆ‘ä»¬çš„æ–¹æ³•å…³æ³¨å±€éƒ¨è¯­ä¹‰å¯¹é½å’Œå…¨å±€ä¸Šä¸‹æ–‡ï¼Œäº§ç”Ÿäº†æ›´ç²¾ç»†å’Œæ›´å…·ä»£è¡¨æ€§çš„åµŒå…¥ï¼Œå°¤å…¶å¯¹äºéœ€è¦æ·±å…¥ç†è§£å†—é•¿æ–‡æœ¬æè¿°çš„ä»»åŠ¡éå¸¸æœ‰ç›Šã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.17782v2">PDF</a> 16 pages, 5 figures</p>
<p><strong>Summary</strong></p>
<p>åŸºäºCLIPæ¨¡å‹çš„è§†è§‰è¯­è¨€æ¨¡å‹åœ¨å¤„ç†å›¾åƒä¸æ–‡æœ¬å¯¹é½æ–¹é¢è¡¨ç°å‡ºå¼ºå¤§çš„èƒ½åŠ›ï¼Œä½†å¯¹äºé•¿ç¯‡è¯¦ç»†æ–‡æœ¬æè¿°å­˜åœ¨å¤„ç†å›°éš¾çš„é—®é¢˜ã€‚æœ¬ç ”ç©¶æå‡ºäº†GOALï¼ˆå…¨å±€å±€éƒ¨å¯¹è±¡å¯¹é½å­¦ä¹ ï¼‰æ–¹æ³•ï¼Œé€šè¿‡åˆ©ç”¨å›¾åƒä¸é•¿ç¯‡æ–‡æœ¬ä¹‹é—´çš„å…¨å±€å’Œå±€éƒ¨è¯­ä¹‰å¯¹é½æ¥å¢å¼ºCLIPæ¨¡å‹å¤„ç†é•¿ç¯‡æ–‡æœ¬çš„èƒ½åŠ›ã€‚é€šè¿‡æœ¬åœ°å›¾åƒå¥å­åŒ¹é…ï¼ˆLISMï¼‰å’ŒåŸºäºç¬¦å·ç›¸ä¼¼æ€§çš„å­¦ä¹ ï¼ˆTSLï¼‰ä¸¤ä¸ªå…³é”®ç»„ä»¶ï¼Œå®ç°äº†å¯¹å›¾åƒåˆ†æ®µä¸æè¿°æ€§å¥å­ä¹‹é—´çš„å¯¹åº”å¯¹è¯†åˆ«ä»¥åŠå±€éƒ¨å…ƒç´ æ³¨æ„åŠ›çš„æœ‰æ•ˆä¼ æ’­ã€‚åœ¨æ–°çš„å›¾åƒä¸é•¿ç¯‡æ–‡æœ¬æ£€ç´¢çš„ä¸‰ä¸ªåŸºå‡†æµ‹è¯•ä¸­éªŒè¯äº†GOALæ–¹æ³•çš„æœ‰æ•ˆæ€§ï¼Œç›¸æ¯”åŸºå‡†CLIPå¾®è°ƒå–å¾—äº†æ˜¾è‘—çš„æ”¹è¿›ï¼Œè¯æ˜äº†ç®€å•æœ‰æ•ˆçš„é€‚åº”é•¿ç¯‡æ–‡æœ¬æè¿°çš„é€‚åº”ç­–ç•¥ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>CLIPç­‰è§†è§‰è¯­è¨€æ¨¡å‹åœ¨å¤„ç†é•¿ç¯‡è¯¦ç»†æ–‡æœ¬æè¿°æ—¶é¢ä¸´æŒ‘æˆ˜ã€‚</li>
<li>GOALæ˜¯ä¸€ç§æ–°å‹çš„å¾®è°ƒæ–¹æ³•ï¼Œæ—¨åœ¨å¢å¼ºæ¨¡å‹å¤„ç†é•¿ç¯‡æ–‡æœ¬çš„èƒ½åŠ›ã€‚</li>
<li>GOALåŒ…æ‹¬ä¸¤ä¸ªå…³é”®ç»„ä»¶ï¼šæœ¬åœ°å›¾åƒå¥å­åŒ¹é…ï¼ˆLISMï¼‰å’ŒåŸºäºç¬¦å·ç›¸ä¼¼æ€§çš„å­¦ä¹ ï¼ˆTSLï¼‰ã€‚</li>
<li>LISMèƒ½å¤Ÿè¯†åˆ«å›¾åƒåˆ†æ®µä¸æè¿°æ€§å¥å­ä¹‹é—´çš„å¯¹åº”å¯¹ã€‚</li>
<li>TSLèƒ½å¤Ÿé«˜æ•ˆåœ°ä¼ æ’­å±€éƒ¨å…ƒç´ æ³¨æ„åŠ›ï¼Œæé«˜æ¨¡å‹å¯¹æ–‡æœ¬ç»†èŠ‚çš„ç†è§£èƒ½åŠ›ã€‚</li>
<li>åœ¨æ–°çš„å›¾åƒä¸é•¿ç¯‡æ–‡æœ¬æ£€ç´¢åŸºå‡†æµ‹è¯•ä¸­ï¼ŒGOALæ–¹æ³•æ˜¾è‘—ä¼˜äºåŸºå‡†CLIPå¾®è°ƒæ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.17782">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-5f2fdefc0204a49d74a2fbef99d9dbed.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a8bd2a3d889295d0966f6662f27c9fb8.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-6dee8bd0a587516135d4ab3658d84748.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-74c3eb285a3ad9e694a0de5bfb28763c.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="MM-UNet-Meta-Mamba-UNet-for-Medical-Image-Segmentation"><a href="#MM-UNet-Meta-Mamba-UNet-for-Medical-Image-Segmentation" class="headerlink" title="MM-UNet: Meta Mamba UNet for Medical Image Segmentation"></a>MM-UNet: Meta Mamba UNet for Medical Image Segmentation</h2><p><strong>Authors:Bin Xie, Yan Yan, Gady Agam</strong></p>
<p>State Space Models (SSMs) have recently demonstrated outstanding performance in long-sequence modeling, particularly in natural language processing. However, their direct application to medical image segmentation poses several challenges. SSMs, originally designed for 1D sequences, struggle with 3D spatial structures in medical images due to discontinuities introduced by flattening. Additionally, SSMs have difficulty fitting high-variance data, which is common in medical imaging.   In this paper, we analyze the intrinsic limitations of SSMs in medical image segmentation and propose a unified U-shaped encoder-decoder architecture, Meta Mamba UNet (MM-UNet), designed to leverage the advantages of SSMs while mitigating their drawbacks. MM-UNet incorporates hybrid modules that integrate SSMs within residual connections, reducing variance and improving performance. Furthermore, we introduce a novel bi-directional scan order strategy to alleviate discontinuities when processing medical images.   Extensive experiments on the AMOS2022 and Synapse datasets demonstrate the superiority of MM-UNet over state-of-the-art methods. MM-UNet achieves a Dice score of 91.0% on AMOS2022, surpassing nnUNet by 3.2%, and a Dice score of 87.1% on Synapse. These results confirm the effectiveness of integrating SSMs in medical image segmentation through architectural design optimizations. </p>
<blockquote>
<p>çŠ¶æ€ç©ºé—´æ¨¡å‹ï¼ˆSSMsï¼‰åœ¨åºåˆ—å»ºæ¨¡ä¸Šè¿‘æœŸå±•ç°å‡ºäº†å‡ºè‰²çš„æ€§èƒ½ï¼Œç‰¹åˆ«æ˜¯åœ¨è‡ªç„¶è¯­è¨€å¤„ç†é¢†åŸŸã€‚ç„¶è€Œï¼Œå®ƒä»¬åœ¨ç›´æ¥åº”ç”¨äºåŒ»å­¦å›¾åƒåˆ†å‰²æ—¶é¢ä¸´è¯¸å¤šæŒ‘æˆ˜ã€‚æœ€åˆè®¾è®¡ç”¨äºä¸€ç»´åºåˆ—çš„SSMsåœ¨å¤„ç†åŒ»å­¦å›¾åƒä¸­çš„ä¸‰ç»´ç©ºé—´ç»“æ„æ—¶é‡åˆ°äº†å›°éš¾ï¼Œå› ä¸ºå¹³é“ºå¼•å…¥çš„ä¸è¿ç»­æ€§é€ æˆäº†å›°æ‰°ã€‚æ­¤å¤–ï¼ŒSSMséš¾ä»¥é€‚åº”é«˜æ–¹å·®æ•°æ®ï¼Œè¿™åœ¨åŒ»å­¦æˆåƒä¸­æ˜¯å¸¸è§çš„ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬åˆ†æäº†SSMsåœ¨åŒ»å­¦å›¾åƒåˆ†å‰²ä¸­çš„å†…åœ¨å±€é™æ€§ï¼Œå¹¶æå‡ºäº†ä¸€ç§ç»Ÿä¸€çš„Uå‹ç¼–ç å™¨-è§£ç å™¨æ¶æ„ï¼Œå³Meta Mamba UNetï¼ˆMM-UNetï¼‰ï¼Œæ—¨åœ¨åˆ©ç”¨SSMsçš„ä¼˜åŠ¿å¹¶ç¼“è§£å…¶ç¼ºç‚¹ã€‚MM-UNetç»“åˆäº†æ··åˆæ¨¡å—ï¼Œè¿™äº›æ¨¡å—å°†SSMsçº³å…¥æ®‹å·®è¿æ¥ä¸­ï¼Œä»¥é™ä½æ–¹å·®å¹¶æ”¹å–„æ€§èƒ½ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§æ–°çš„åŒå‘æ‰«æé¡ºåºç­–ç•¥ï¼Œä»¥å‡è½»åœ¨å¤„ç†åŒ»å­¦å›¾åƒæ—¶çš„ä¸è¿ç»­æ€§ã€‚åœ¨AMOS2022å’ŒSynapseæ•°æ®é›†ä¸Šçš„å¤§é‡å®éªŒè¡¨æ˜ï¼ŒMM-UNetä¼˜äºæœ€æ–°æ–¹æ³•ã€‚MM-UNetåœ¨AMOS2022ä¸Šçš„Diceç³»æ•°ä¸º91.0%ï¼Œæ¯”nnUNeté«˜å‡º3.2%ï¼Œè€Œåœ¨Synapseä¸Šçš„Diceç³»æ•°ä¸º87.1%ã€‚è¿™äº›ç»“æœè¯å®äº†é€šè¿‡æ¶æ„ä¼˜åŒ–è®¾è®¡å°†SSMsæ•´åˆåˆ°åŒ»å­¦å›¾åƒåˆ†å‰²ä¸­çš„æœ‰æ•ˆæ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.17540v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä¸»è¦æ¢è®¨äº†çŠ¶æ€ç©ºé—´æ¨¡å‹ï¼ˆSSMsï¼‰åœ¨åŒ»å­¦å›¾åƒåˆ†å‰²ä¸­çš„å±€é™æ€§ï¼Œå¹¶æå‡ºäº†ä¸€ç§ç»Ÿä¸€çš„Uå‹ç¼–ç è§£ç å™¨æ¶æ„Meta Mamba UNetï¼ˆMM-UNetï¼‰ã€‚è¯¥æ¶æ„æ—¨åœ¨åˆ©ç”¨SSMsçš„ä¼˜ç‚¹å¹¶å…‹æœå…¶ç¼ºç‚¹ï¼Œé€šè¿‡æ··åˆæ¨¡å—åœ¨æ®‹å·®è¿æ¥ä¸­é›†æˆSSMsï¼Œé™ä½æ–¹å·®å¹¶æå‡æ€§èƒ½ã€‚æ­¤å¤–ï¼Œè¿˜å¼•å…¥äº†ä¸€ç§æ–°å‹åŒå‘æ‰«æé¡ºåºç­–ç•¥ï¼Œä»¥å‡è½»å¤„ç†åŒ»å­¦å›¾åƒæ—¶çš„é—´æ–­æ€§ã€‚åœ¨AMOS2022å’ŒSynapseæ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒMM-UNetä¼˜äºç°æœ‰æ–¹æ³•ï¼Œåœ¨AMOS2022ä¸Šå®ç°Diceå¾—åˆ†91.0%ï¼Œæ¯”nnUNeté«˜å‡º3.2%ï¼Œåœ¨Synapseä¸Šå®ç°Diceå¾—åˆ†87.1%ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>çŠ¶æ€ç©ºé—´æ¨¡å‹ï¼ˆSSMsï¼‰åœ¨åŒ»å­¦å›¾åƒåˆ†å‰²ä¸­é¢ä¸´æŒ‘æˆ˜ï¼Œå› ä¸ºå®ƒä»¬åœ¨å¤„ç†ä¸‰ç»´ç©ºé—´ç»“æ„æ—¶é‡åˆ°ç”±äºæ‰å¹³åŒ–é€ æˆçš„é—´æ–­æ€§ã€‚</li>
<li>SSMséš¾ä»¥é€‚åº”åŒ»å­¦æˆåƒä¸­å¸¸è§çš„é«˜æ–¹å·®æ•°æ®ã€‚</li>
<li>MM-UNetæ˜¯ä¸€ç§ç»“åˆSSMsä¼˜åŠ¿çš„Uå‹ç¼–ç è§£ç å™¨æ¶æ„ï¼Œæ—¨åœ¨å…‹æœSSMsçš„å±€é™æ€§ã€‚</li>
<li>MM-UNeté€šè¿‡æ··åˆæ¨¡å—åœ¨æ®‹å·®è¿æ¥ä¸­é›†æˆSSMsï¼Œé™ä½æ–¹å·®ï¼Œæå‡æ€§èƒ½ã€‚</li>
<li>å¼•å…¥äº†ä¸€ç§æ–°å‹åŒå‘æ‰«æé¡ºåºç­–ç•¥ï¼Œä»¥å‡è½»å¤„ç†åŒ»å­¦å›¾åƒæ—¶çš„é—´æ–­æ€§ã€‚</li>
<li>åœ¨AMOS2022å’ŒSynapseæ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒMM-UNetæ€§èƒ½ä¼˜è¶Šï¼Œè¶…è¿‡ç°æœ‰æ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.17540">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-71f0efbb87ca2f8ed8d60aad7a81b130.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ef0b8acbcae6b14a6eb8015e7079b0e6.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-4d51db52dbeb78a6963d25ae01424c73.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-1fa10c329bf6921a5ff934e3b9ac971b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-040703ba1b10587e77c60bcbc283e1be.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e026045a023b1874798b7df002668420.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7669b24dd2eab292ad79bf89827a0f01.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="Cross-Modal-Interactive-Perception-Network-with-Mamba-for-Lung-Tumor-Segmentation-in-PET-CT-Images"><a href="#Cross-Modal-Interactive-Perception-Network-with-Mamba-for-Lung-Tumor-Segmentation-in-PET-CT-Images" class="headerlink" title="Cross-Modal Interactive Perception Network with Mamba for Lung Tumor   Segmentation in PET-CT Images"></a>Cross-Modal Interactive Perception Network with Mamba for Lung Tumor   Segmentation in PET-CT Images</h2><p><strong>Authors:Jie Mei, Chenyu Lin, Yu Qiu, Yaonan Wang, Hui Zhang, Ziyang Wang, Dong Dai</strong></p>
<p>Lung cancer is a leading cause of cancer-related deaths globally. PET-CT is crucial for imaging lung tumors, providing essential metabolic and anatomical information, while it faces challenges such as poor image quality, motion artifacts, and complex tumor morphology. Deep learning-based models are expected to address these problems, however, existing small-scale and private datasets limit significant performance improvements for these methods. Hence, we introduce a large-scale PET-CT lung tumor segmentation dataset, termed PCLT20K, which comprises 21,930 pairs of PET-CT images from 605 patients. Furthermore, we propose a cross-modal interactive perception network with Mamba (CIPA) for lung tumor segmentation in PET-CT images. Specifically, we design a channel-wise rectification module (CRM) that implements a channel state space block across multi-modal features to learn correlated representations and helps filter out modality-specific noise. A dynamic cross-modality interaction module (DCIM) is designed to effectively integrate position and context information, which employs PET images to learn regional position information and serves as a bridge to assist in modeling the relationships between local features of CT images. Extensive experiments on a comprehensive benchmark demonstrate the effectiveness of our CIPA compared to the current state-of-the-art segmentation methods. We hope our research can provide more exploration opportunities for medical image segmentation. The dataset and code are available at <a target="_blank" rel="noopener" href="https://github.com/mj129/CIPA">https://github.com/mj129/CIPA</a>. </p>
<blockquote>
<p>è‚ºç™Œæ˜¯å…¨çƒç™Œç—‡ç›¸å…³æ­»äº¡çš„ä¸»è¦åŸå› ä¹‹ä¸€ã€‚PET-CTåœ¨è‚ºéƒ¨è‚¿ç˜¤æˆåƒä¸­è‡³å…³é‡è¦ï¼Œèƒ½å¤Ÿæä¾›å…³é”®çš„ä»£è°¢å’Œè§£å‰–ä¿¡æ¯ï¼Œä½†åŒæ—¶ä¹Ÿé¢ä¸´ç€å›¾åƒè´¨é‡å·®ã€è¿åŠ¨ä¼ªå½±å’Œå¤æ‚çš„è‚¿ç˜¤å½¢æ€ç­‰æŒ‘æˆ˜ã€‚åŸºäºæ·±åº¦å­¦ä¹ çš„æ–¹æ³•æœ‰æœ›è§£å†³è¿™äº›é—®é¢˜ï¼Œç„¶è€Œï¼Œç°æœ‰çš„å°è§„æ¨¡ç§æœ‰æ•°æ®é›†é™åˆ¶äº†è¿™äº›æ–¹æ³•æ€§èƒ½çš„æ˜¾è‘—æé«˜ã€‚å› æ­¤ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ªå¤§è§„æ¨¡çš„PET-CTè‚ºéƒ¨è‚¿ç˜¤åˆ†å‰²æ•°æ®é›†PCLT20Kï¼ŒåŒ…å«æ¥è‡ª605åæ‚£è€…çš„21ï¼Œ930å¯¹PET-CTå›¾åƒã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åä¸ºCIPAçš„è·¨æ¨¡æ€äº¤äº’æ„ŸçŸ¥ç½‘ç»œè¿›è¡ŒPET-CTå›¾åƒä¸­çš„è‚ºéƒ¨è‚¿ç˜¤åˆ†å‰²ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸€ä¸ªé€šé“æ ¡æ­£æ¨¡å—ï¼ˆCRMï¼‰ï¼Œè¯¥æ¨¡å—åœ¨å¤šæ¨¡æ€ç‰¹å¾ä¸Šå®ç°é€šé“çŠ¶æ€ç©ºé—´å—æ¥å­¦ä¹ ç›¸å…³è¡¨ç¤ºï¼Œå¹¶æœ‰åŠ©äºè¿‡æ»¤æ‰æ¨¡æ€ç‰¹å®šçš„å™ªå£°ã€‚æˆ‘ä»¬è¿˜è®¾è®¡äº†ä¸€ä¸ªåŠ¨æ€è·¨æ¨¡æ€äº¤äº’æ¨¡å—ï¼ˆDCIMï¼‰ï¼Œä»¥æœ‰æ•ˆåœ°é›†æˆä½ç½®å’Œä¸Šä¸‹æ–‡ä¿¡æ¯ï¼Œè¯¥æ¨¡å—åˆ©ç”¨PETå›¾åƒæ¥å­¦ä¹ åŒºåŸŸä½ç½®ä¿¡æ¯ï¼Œå¹¶ä½œä¸ºæ¡¥æ¢æ¥ååŠ©å»ºæ¨¡CTå›¾åƒå±€éƒ¨ç‰¹å¾ä¹‹é—´çš„å…³ç³»ã€‚åœ¨å…¨é¢åŸºå‡†æµ‹è¯•ä¸Šçš„å¤§é‡å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„CIPAæ–¹æ³•ç›¸æ¯”å½“å‰æœ€å…ˆè¿›çš„åˆ†å‰²æ–¹æ³•æ›´å…·ä¼˜åŠ¿ã€‚æˆ‘ä»¬å¸Œæœ›æœ¬ç ”ç©¶èƒ½ä¸ºåŒ»å­¦å›¾åƒåˆ†å‰²æä¾›æ›´å¤šçš„æ¢ç´¢æœºä¼šã€‚æ•°æ®é›†å’Œä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/mj129/CIPA">https://github.com/mj129/CIPA</a>ä¸Šè·å–ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.17261v1">PDF</a> Accepted to CVPR 2025</p>
<p><strong>æ‘˜è¦</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸€ç§é’ˆå¯¹PET-CTè‚ºéƒ¨è‚¿ç˜¤å›¾åƒåˆ†å‰²çš„å¤§å‹æ•°æ®é›†PCLT20Kï¼ŒåŒ…å«äº†æ¥è‡ª605åæ‚£è€…çš„21,930å¯¹PET-CTå›¾åƒã€‚æå‡ºäº†åŸºäºæ·±åº¦å­¦ä¹ çš„è·¨æ¨¡æ€äº¤äº’æ„ŸçŸ¥ç½‘ç»œCIPAï¼ˆå«é€šé“æ ¡æ­£æ¨¡å—CRMå’ŒåŠ¨æ€è·¨æ¨¡æ€äº¤äº’æ¨¡å—DCIMï¼‰ï¼Œèƒ½æœ‰æ•ˆè§£å†³PET-CTè‚ºéƒ¨è‚¿ç˜¤å›¾åƒåˆ†å‰²ä¸­å­˜åœ¨çš„é—®é¢˜ã€‚å®éªŒè¡¨æ˜ï¼ŒCIPAç›¸è¾ƒäºå½“å‰æœ€å…ˆè¿›çš„åˆ†å‰²æ–¹æ³•æ›´ä¸ºæœ‰æ•ˆã€‚ç ”ç©¶æˆæœä¸ºåŒ»å­¦å›¾åƒåˆ†å‰²é¢†åŸŸæä¾›äº†æ›´å¤šæ¢ç´¢æœºä¼šã€‚æ•°æ®é›†å’Œä»£ç å·²å…¬å¼€äº<a target="_blank" rel="noopener" href="https://github.com/mj129/CIPA%E3%80%82">https://github.com/mj129/CIPAã€‚</a></p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>è‚ºç™Œæ˜¯å…¨çƒç™Œç—‡ç›¸å…³æ­»äº¡çš„ä¸»è¦åŸå› ä¹‹ä¸€ï¼ŒPET-CTæˆåƒåœ¨è‚ºéƒ¨è‚¿ç˜¤è¯Šæ–­å’Œæ²»ç–—ä¸­èµ·ç€é‡è¦ä½œç”¨ã€‚</li>
<li>PET-CTæˆåƒé¢ä¸´å›¾åƒè´¨é‡ã€è¿åŠ¨ä¼ªå½±å’Œå¤æ‚è‚¿ç˜¤å½¢æ€ç­‰æŒ‘æˆ˜ã€‚</li>
<li>æ·±åº¦å­¦ä¹ åœ¨è§£å†³è¿™äº›é—®é¢˜æ–¹é¢æ˜¾ç¤ºå‡ºæ½œåŠ›ï¼Œä½†ç°æœ‰æ•°æ®é›†è§„æ¨¡è¾ƒå°ä¸”ç§æœ‰ï¼Œé™åˆ¶äº†æ€§èƒ½æå‡ã€‚</li>
<li>å¼•å…¥äº†ä¸€ä¸ªå¤§å‹PET-CTè‚ºéƒ¨è‚¿ç˜¤åˆ†å‰²æ•°æ®é›†PCLT20Kï¼ŒåŒ…å«æ¥è‡ª605åæ‚£è€…çš„21,930å¯¹å›¾åƒã€‚</li>
<li>æå‡ºäº†è·¨æ¨¡æ€äº¤äº’æ„ŸçŸ¥ç½‘ç»œCIPAï¼ŒåŒ…æ‹¬é€šé“æ ¡æ­£æ¨¡å—CRMå’ŒåŠ¨æ€è·¨æ¨¡æ€äº¤äº’æ¨¡å—DCIMã€‚</li>
<li>CIPAé€šè¿‡å­¦ä¹ å’Œæ•´åˆå¤šæ¨¡æ€ç‰¹å¾ï¼Œèƒ½æœ‰æ•ˆè¿‡æ»¤æ¨¡æ€ç‰¹å®šå™ªå£°ï¼Œå¹¶æ•´åˆä½ç½®å’Œä¸Šä¸‹æ–‡ä¿¡æ¯ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.17261">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-00b9295247980743283c8d488457a44e.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-daf4fc1e1e8bd49b26d1c4b8da100a38.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-257d9a0aaeb79c6d6c5d1c22c549f6cb.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-8a97bc3a5f46f87befd8f122ff97618c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-5b92bc72041d8b7de2d60d85806ffca8.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-bfe0d5666aa191f7c0afb366df6bc96a.jpg" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="Steady-Progress-Beats-Stagnation-Mutual-Aid-of-Foundation-and-Conventional-Models-in-Mixed-Domain-Semi-Supervised-Medical-Image-Segmentation"><a href="#Steady-Progress-Beats-Stagnation-Mutual-Aid-of-Foundation-and-Conventional-Models-in-Mixed-Domain-Semi-Supervised-Medical-Image-Segmentation" class="headerlink" title="Steady Progress Beats Stagnation: Mutual Aid of Foundation and   Conventional Models in Mixed Domain Semi-Supervised Medical Image   Segmentation"></a>Steady Progress Beats Stagnation: Mutual Aid of Foundation and   Conventional Models in Mixed Domain Semi-Supervised Medical Image   Segmentation</h2><p><strong>Authors:Qinghe Ma, Jian Zhang, Zekun Li, Lei Qi, Qian Yu, Yinghuan Shi</strong></p>
<p>Large pretrained visual foundation models exhibit impressive general capabilities. However, the extensive prior knowledge inherent in these models can sometimes be a double-edged sword when adapting them to downstream tasks in specific domains. In the context of semi-supervised medical image segmentation with domain shift, foundation models like MedSAM tend to make overconfident predictions, some of which are incorrect. The error accumulation hinders the effective utilization of unlabeled data and limits further improvements. In this paper, we introduce a Synergistic training framework for Foundation and Conventional models (SynFoC) to address the issue. We observe that a conventional model trained from scratch has the ability to correct the high-confidence mispredictions of the foundation model, while the foundation model can supervise it with high-quality pseudo-labels in the early training stages. Furthermore, to enhance the collaborative training effectiveness of both models and promote reliable convergence towards optimization, the consensus-divergence consistency regularization is proposed. We demonstrate the superiority of our method across four public multi-domain datasets. In particular, our method improves the Dice score by 10.31% on the Prostate dataset. Our code is available at <a target="_blank" rel="noopener" href="https://github.com/MQinghe/SynFoC">https://github.com/MQinghe/SynFoC</a> . </p>
<blockquote>
<p>å¤§å‹é¢„è®­ç»ƒè§†è§‰åŸºç¡€æ¨¡å‹å±•ç°å‡ºä»¤äººå°è±¡æ·±åˆ»çš„é€šç”¨èƒ½åŠ›ã€‚ç„¶è€Œï¼Œå½“å°†è¿™äº›æ¨¡å‹é€‚åº”åˆ°ç‰¹å®šé¢†åŸŸçš„ä¸‹æ¸¸ä»»åŠ¡æ—¶ï¼Œå…¶ä¸­è•´å«çš„å¤§é‡å…ˆéªŒçŸ¥è¯†æœ‰æ—¶ä¼šæˆä¸ºä¸€æŠŠåŒåˆƒå‰‘ã€‚åœ¨åŠç›‘ç£åŒ»å­¦å›¾åƒåˆ†å‰²ä¸”å­˜åœ¨é¢†åŸŸåç§»çš„æƒ…å¢ƒä¸­ï¼ŒåŸºç¡€æ¨¡å‹ï¼ˆå¦‚MedSAMï¼‰å¾€å¾€ä¼šäº§ç”Ÿè¿‡äºè‡ªä¿¡çš„é¢„æµ‹ï¼Œå…¶ä¸­ä¸€äº›é¢„æµ‹æ˜¯é”™è¯¯çš„ã€‚é”™è¯¯ç´¯ç§¯é˜»ç¢äº†æœªæ ‡è®°æ•°æ®çš„æœ‰æ•ˆåˆ©ç”¨ï¼Œå¹¶é™åˆ¶äº†è¿›ä¸€æ­¥çš„æ”¹è¿›ã€‚é’ˆå¯¹è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬åœ¨æœ¬æ–‡ä¸­å¼•å…¥äº†é’ˆå¯¹åŸºç¡€æ¨¡å‹å’Œä¼ ç»Ÿæ¨¡å‹çš„ååŒè®­ç»ƒæ¡†æ¶ï¼ˆSynFoCï¼‰ã€‚æˆ‘ä»¬å‘ç°ï¼Œä»å¤´å¼€å§‹è®­ç»ƒçš„ä¼ ç»Ÿæ¨¡å‹æœ‰èƒ½åŠ›çº æ­£åŸºç¡€æ¨¡å‹çš„é«˜ç½®ä¿¡åº¦é”™è¯¯é¢„æµ‹ï¼Œè€ŒåŸºç¡€æ¨¡å‹å¯ä»¥åœ¨æ—©æœŸè®­ç»ƒé˜¶æ®µä½¿ç”¨é«˜è´¨é‡çš„ä¼ªæ ‡ç­¾å¯¹å…¶è¿›è¡Œç›‘ç£ã€‚æ­¤å¤–ï¼Œä¸ºäº†æé«˜ä¸¤ç§æ¨¡å‹çš„ååŒè®­ç»ƒæ•ˆæœï¼Œå¹¶ä¿ƒè¿›å¯é çš„æ”¶æ•›ä»¥è¾¾åˆ°ä¼˜åŒ–ç›®çš„ï¼Œæˆ‘ä»¬æå‡ºäº†å…±è¯†åˆ†æ­§ä¸€è‡´æ€§æ­£åˆ™åŒ–æ–¹æ³•ã€‚æˆ‘ä»¬åœ¨å››ä¸ªå…¬å…±å¤šé¢†åŸŸæ•°æ®é›†ä¸Šå±•ç¤ºäº†æˆ‘ä»¬çš„æ–¹æ³•ä¼˜è¶Šæ€§ã€‚å°¤å…¶å€¼å¾—ä¸€æçš„æ˜¯ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨å‰åˆ—è…ºæ•°æ®é›†ä¸Šæé«˜äº†Diceå¾—åˆ†10.31%ã€‚æˆ‘ä»¬çš„ä»£ç ä½äº<a target="_blank" rel="noopener" href="https://github.com/MQinghe/SynFoC%E3%80%82">https://github.com/MQinghe/SynFoCã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.16997v1">PDF</a> Accepted by CVPR 2025</p>
<p><strong>Summary</strong></p>
<p>å¤§å‹é¢„è®­ç»ƒè§†è§‰åŸºç¡€æ¨¡å‹å…·æœ‰ä»¤äººå°è±¡æ·±åˆ»çš„é€šç”¨èƒ½åŠ›ï¼Œä½†åœ¨é€‚åº”å…·æœ‰é¢†åŸŸåç§»çš„ä¸‹æ¸¸ä»»åŠ¡æ—¶ï¼Œå…¶å›ºæœ‰çš„å¤§é‡å…ˆéªŒçŸ¥è¯†å¯èƒ½æ˜¯ä¸€æŠŠåŒåˆƒå‰‘ã€‚åœ¨åŒ»å­¦å›¾åƒåˆ†å‰²çš„åŠç›‘ç£å­¦ä¹ ä¸­ï¼ŒåŸºç¡€æ¨¡å‹å¦‚MedSAMä¼šäº§ç”Ÿè¿‡åº¦è‡ªä¿¡çš„é¢„æµ‹ï¼Œå…¶ä¸­ä¸€äº›æ˜¯ä¸æ­£ç¡®çš„ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ä¸ªååŒè®­ç»ƒæ¡†æ¶SynFoCï¼Œè¯¥æ¡†æ¶ç»“åˆäº†åŸºç¡€æ¨¡å‹å’Œä¼ ç»Ÿæ¨¡å‹çš„ä¼˜åŠ¿ï¼Œä»¥è§£å†³è¿™ä¸€é—®é¢˜ã€‚ä¼ ç»Ÿæ¨¡å‹èƒ½å¤Ÿçº æ­£åŸºç¡€æ¨¡å‹çš„é«˜ä¿¡å¿ƒè¯¯åˆ¤ï¼Œè€ŒåŸºç¡€æ¨¡å‹åˆ™å¯ä»¥åœ¨æ—©æœŸè®­ç»ƒé˜¶æ®µä¸ºå…¶æä¾›é«˜è´¨é‡ä¼ªæ ‡ç­¾ã€‚æ­¤å¤–ï¼Œä¸ºäº†å¢å¼ºä¸¤ç§æ¨¡å‹çš„ååŒè®­ç»ƒæ•ˆæœå¹¶ä¿ƒè¿›å¯é çš„ä¼˜åŒ–æ”¶æ•›ï¼Œæœ¬æ–‡è¿˜æå‡ºäº†å…±è¯†åˆ†æ­§ä¸€è‡´æ€§æ­£åˆ™åŒ–æ–¹æ³•ã€‚åœ¨å››ä¸ªå…¬å¼€çš„å¤šåŸŸæ•°æ®é›†ä¸ŠéªŒè¯äº†æ‰€ææ–¹æ³•çš„æœ‰æ•ˆæ€§ï¼Œç‰¹åˆ«æ˜¯åœ¨å‰åˆ—è…ºæ•°æ®é›†ä¸Šï¼ŒDiceå¾—åˆ†æé«˜äº†10.31%ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹é¢„è®­ç»ƒè§†è§‰åŸºç¡€æ¨¡å‹åœ¨åŒ»å­¦å›¾åƒåˆ†å‰²ä»»åŠ¡ä¸­å¯èƒ½å­˜åœ¨è¿‡åº¦è‡ªä¿¡é¢„æµ‹çš„é—®é¢˜ã€‚</li>
<li>åŸºç¡€æ¨¡å‹çš„è¿‡åº¦è‡ªä¿¡é¢„æµ‹åœ¨é€‚åº”ç‰¹å®šé¢†åŸŸçš„ä¸‹æ¸¸ä»»åŠ¡æ—¶å¯èƒ½æˆä¸ºæŒ‘æˆ˜ã€‚</li>
<li>SynFoCæ¡†æ¶ç»“åˆäº†åŸºç¡€æ¨¡å‹å’Œä¼ ç»Ÿæ¨¡å‹çš„ä¼˜ç‚¹ï¼Œä»¥æé«˜æ¨¡å‹åœ¨åŒ»å­¦å›¾åƒåˆ†å‰²ä»»åŠ¡ä¸­çš„æ€§èƒ½ã€‚</li>
<li>ä¼ ç»Ÿæ¨¡å‹èƒ½å¤Ÿçº æ­£åŸºç¡€æ¨¡å‹çš„é«˜ä¿¡å¿ƒè¯¯åˆ¤ã€‚</li>
<li>åŸºç¡€æ¨¡å‹å¯åœ¨æ—©æœŸè®­ç»ƒé˜¶æ®µä¸ºä¼ ç»Ÿæ¨¡å‹æä¾›é«˜è´¨é‡ä¼ªæ ‡ç­¾ã€‚</li>
<li>å…±è¯†åˆ†æ­§ä¸€è‡´æ€§æ­£åˆ™åŒ–æ–¹æ³•è¢«æå‡ºï¼Œä»¥å¢å¼ºæ¨¡å‹çš„ååŒè®­ç»ƒæ•ˆæœå¹¶ä¿ƒè¿›ä¼˜åŒ–æ”¶æ•›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.16997">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-d4043faccf3c1347b4efa33bffebe259.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e0f86c0d591e08cd7f71f80279c78f3f.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-0d68823541baaac24b7ab332a5071395.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-80d3492b74bfc8cfbf762c50fa28a958.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-68f961089bf03f5479b3f6906d235709.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c816a22275b645df3afb701049887cce.jpg" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="Leveraging-Two-Phase-Data-for-Improved-Prediction-of-Survival-Outcomes-with-Application-to-Nasopharyngeal-Cancer"><a href="#Leveraging-Two-Phase-Data-for-Improved-Prediction-of-Survival-Outcomes-with-Application-to-Nasopharyngeal-Cancer" class="headerlink" title="Leveraging Two-Phase Data for Improved Prediction of Survival Outcomes   with Application to Nasopharyngeal Cancer"></a>Leveraging Two-Phase Data for Improved Prediction of Survival Outcomes   with Application to Nasopharyngeal Cancer</h2><p><strong>Authors:Eun Jeong Oh, Seungjun Ahn, Tristan Tham, Min Qian</strong></p>
<p>Accurate survival predicting models are essential for improving targeted cancer therapies and clinical care among cancer patients. In this article, we investigate and develop a method to improve predictions of survival in cancer by leveraging two-phase data with expert knowledge and prognostic index. Our work is motivated by two-phase data in nasopharyngeal cancer (NPC), where traditional covariates are readily available for all subjects, but the primary viral factor, Human Papillomavirus (HPV), is substantially missing. To address this challenge, we propose an expert guided method that incorporates prognostic index based on the observed covariates and clinical importance of key factors. The proposed method makes efficient use of available data, not simply discarding patients with unknown HPV status. We apply the proposed method and evaluate it against other existing approaches through a series of simulation studies and real data example of NPC patients. Under various settings, the proposed method consistently outperforms competing methods in terms of c-index, calibration slope, and integrated Brier score. By efficiently leveraging two-phase data, the model provides a more accurate and reliable predictive ability of survival models. </p>
<blockquote>
<p>ç²¾ç¡®ç”Ÿå­˜é¢„æµ‹æ¨¡å‹å¯¹äºæ”¹è¿›ç™Œç—‡æ‚£è€…çš„é¶å‘ç–—æ³•å’Œä¸´åºŠæŠ¤ç†è‡³å…³é‡è¦ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬è°ƒæŸ¥å¹¶å¼€å‘äº†ä¸€ç§æ–¹æ³•ï¼Œé€šè¿‡å€ŸåŠ©ä¸¤é˜¶æ®µæ•°æ®ã€ä¸“ä¸šçŸ¥è¯†å’Œé¢„åæŒ‡æ•°æ¥æé«˜ç™Œç—‡ç”Ÿå­˜é¢„æµ‹çš„å‡†ç¡®æ€§ã€‚æˆ‘ä»¬çš„å·¥ä½œå—åˆ°é¼»å’½ç™Œï¼ˆNPCï¼‰ä¸¤é˜¶æ®µæ•°æ®çš„æ¨åŠ¨ï¼Œåœ¨ä¼ ç»Ÿçš„åå˜é‡å¯è½»æ˜“è·å–æ‰€æœ‰ä¸»ä½“çš„ä¿¡æ¯çš„åŒæ—¶ï¼Œä¸»è¦çš„ç—…æ¯’å› ç´ äººä¹³å¤´ç˜¤ç—…æ¯’ï¼ˆHPVï¼‰å¤§é‡ç¼ºå¤±ã€‚ä¸ºäº†åº”å¯¹è¿™ä¸€æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ä»¥ä¸“å®¶å¼•å¯¼çš„æ–¹æ³•ï¼Œå®ƒç»“åˆäº†åŸºäºè§‚å¯Ÿåˆ°çš„åå˜é‡å’Œå…³é”®å› ç´ çš„ä¸´åºŠé‡è¦æ€§çš„é¢„åæŒ‡æ•°ã€‚æ‰€æå‡ºçš„æ–¹æ³•æœ‰æ•ˆåˆ©ç”¨ç°æœ‰æ•°æ®ï¼Œå¹¶éç®€å•èˆå¼ƒHPVçŠ¶æ€æœªçŸ¥çš„æ‚£è€…ã€‚æˆ‘ä»¬å°†æ‰€æå‡ºçš„æ–¹æ³•åº”ç”¨äºå®é™…æ•°æ®ï¼Œå¹¶é€šè¿‡ä¸€ç³»åˆ—æ¨¡æ‹Ÿç ”ç©¶å’Œé¼»å’½ç™Œæ‚£è€…çš„å®é™…æ•°æ®ç¤ºä¾‹ä¸å…¶ä»–ç°æœ‰æ–¹æ³•è¿›è¡Œæ¯”è¾ƒè¯„ä¼°ã€‚åœ¨å„ç§è®¾ç½®ä¸‹ï¼Œæ‰€æå‡ºçš„æ–¹æ³•åœ¨cæŒ‡æ•°ã€æ ¡å‡†æ–œç‡å’Œç»¼åˆå¸ƒç‘å°”å¾—åˆ†æ–¹é¢å§‹ç»ˆä¼˜äºç«äº‰æ–¹æ³•ã€‚é€šè¿‡æœ‰æ•ˆåˆ©ç”¨ä¸¤é˜¶æ®µæ•°æ®ï¼Œè¯¥æ¨¡å‹æä¾›äº†æ›´å‡†ç¡®å¯é çš„ç”Ÿå­˜é¢„æµ‹èƒ½åŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.16732v2">PDF</a> </p>
<p><strong>Summary</strong><br>     æœ¬æ–‡æ¢è®¨äº†åˆ©ç”¨ä¸¤é˜¶æ®µæ•°æ®ã€ä¸“å®¶çŸ¥è¯†å’Œé¢„åæŒ‡æ•°æé«˜ç™Œç—‡ç”Ÿå­˜é¢„æµ‹å‡†ç¡®æ€§çš„æ–¹æ³•ã€‚é’ˆå¯¹é¼»å’½ç™Œï¼ˆNPCï¼‰çš„ä¸¤é˜¶æ®µæ•°æ®ï¼Œæå‡ºä¸€ç§ä¸“å®¶å¼•å¯¼çš„æ–¹æ³•ï¼Œè¯¥æ–¹æ³•åŸºäºè§‚å¯Ÿåˆ°çš„åå˜é‡å’Œå…³é”®å› ç´ çš„ä¸´åºŠé‡è¦æ€§æ„å»ºé¢„åæŒ‡æ•°ã€‚æ¨¡æ‹Ÿç ”ç©¶å’ŒçœŸå®æ•°æ®ç¤ºä¾‹è¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å„ç§è®¾ç½®ä¸‹åœ¨cæŒ‡æ•°ã€æ ¡å‡†æ–œç‡å’Œç»¼åˆå¸ƒç‘å°”å¾—åˆ†æ–¹é¢å‡ä¼˜äºå…¶ä»–æ–¹æ³•ã€‚è¯¥æ¨¡å‹èƒ½å¤Ÿæ›´æœ‰æ•ˆåœ°åˆ©ç”¨ä¸¤é˜¶æ®µæ•°æ®ï¼Œæä¾›æ›´å‡†ç¡®å¯é çš„ç”Ÿå­˜é¢„æµ‹èƒ½åŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å‡†ç¡®çš„ç”Ÿå­˜é¢„æµ‹æ¨¡å‹å¯¹äºæ”¹è¿›é¶å‘ç™Œç—‡ç–—æ³•å’Œç™Œç—‡æ‚£è€…çš„ä¸´åºŠæŠ¤ç†è‡³å…³é‡è¦ã€‚</li>
<li>æ–‡ç« é’ˆå¯¹é¼»å’½ç™Œï¼ˆNPCï¼‰çš„ä¸¤é˜¶æ®µæ•°æ®ï¼Œæå‡ºäº†ä¸€ä¸ªç»“åˆä¸“å®¶çŸ¥è¯†å’Œé¢„åæŒ‡æ•°çš„é¢„æµ‹æ–¹æ³•ã€‚</li>
<li>æå‡ºçš„æ–¹æ³•å……åˆ†åˆ©ç”¨äº†å¯ç”¨æ•°æ®ï¼Œå¹¶æœªç®€å•ä¸¢å¼ƒHPVçŠ¶æ€æœªçŸ¥çš„æ‚£è€…ã€‚</li>
<li>æ¨¡æ‹Ÿç ”ç©¶å’ŒçœŸå®æ•°æ®ç¤ºä¾‹éªŒè¯äº†è¯¥æ–¹æ³•åœ¨ç”Ÿå­˜é¢„æµ‹æ–¹é¢çš„ä¼˜è¶Šæ€§ã€‚</li>
<li>è¯¥æ–¹æ³•åœ¨cæŒ‡æ•°ã€æ ¡å‡†æ–œç‡å’Œç»¼åˆå¸ƒç‘å°”å¾—åˆ†ç­‰æ–¹é¢å‡ä¼˜äºå…¶ä»–æ–¹æ³•ã€‚</li>
<li>è¯¥æ¨¡å‹èƒ½å¤Ÿæ›´æœ‰æ•ˆåœ°åˆ©ç”¨ä¸¤é˜¶æ®µæ•°æ®ï¼Œæé«˜ç”Ÿå­˜é¢„æµ‹çš„å‡†ç¡®æ€§å’Œå¯é æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.16732">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-afefb3a33515bb366e99ba3511665354.jpg" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1><h2 id="Supernova-production-of-axion-like-particles-coupling-to-electrons-reloaded"><a href="#Supernova-production-of-axion-like-particles-coupling-to-electrons-reloaded" class="headerlink" title="Supernova production of axion-like particles coupling to electrons,   reloaded"></a>Supernova production of axion-like particles coupling to electrons,   reloaded</h2><p><strong>Authors:Damiano F. G. Fiorillo, Tetyana Pitik, Edoardo Vitagliano</strong></p>
<p>We revisit the production of axion-like particles (ALPs) coupled to electrons at tree-level in a relativistic plasma. We explicitly demonstrate the equivalence between pseudoscalar and derivative couplings, incorporate previously neglected processes for the first time-namely, semi-Compton production ($\gamma e^-\rightarrow a e^-$) and pair annihilation ($e^+e^-\rightarrow a\gamma$)-and derive analytical expressions for the bremsstrahlung ($e^- N\to e^- N a$) production rate, enabling a more computationally efficient evaluation of the ALP flux. Additionally, we assess uncertainties in the production rate arising from electron thermal mass corrections, electron-electron Coulomb interactions, and the Landau-Pomeranchuk-Migdal effect. The ALP emissivity is made available in a public repository as a function of the ALP mass, the temperature, and the electron chemical potential of the plasma. Finally, we examine the impact of ALP production and subsequent decays on astrophysical observables, deriving the leading bounds on ALPs coupling to electrons. At small couplings, the dominant constraints come from the previously neglected decay $a\to e^+ e^-\gamma$, except for a region of fireball formation where SN 1987A X-ray observations offer the best probe. At large couplings, bounds are dominated by the energy deposition argument, with a recently developed new prescription for the trapping regime. </p>
<blockquote>
<p>æˆ‘ä»¬é‡æ–°ç ”ç©¶äº†åœ¨ç›¸å¯¹è®ºæ€§ç­‰ç¦»å­ä½“ä¸­ä»¥æ ‘çŠ¶å›¾ä¸ç”µå­è€¦åˆçš„è½´å­æ ·ç²’å­ï¼ˆALPsï¼‰çš„äº§ç”Ÿã€‚æˆ‘ä»¬æ˜ç¡®å±•ç¤ºäº†æ ‡é‡å’Œå¯¼æ•°è€¦åˆä¹‹é—´çš„ç­‰ä»·æ€§ï¼Œé¦–æ¬¡è€ƒè™‘äº†ä¹‹å‰è¢«å¿½ç•¥çš„è¿‡ç¨‹ï¼Œå³åŠåº·æ™®é¡¿äº§ç”Ÿï¼ˆÎ³e^-â†’ae^-ï¼‰å’Œé…å¯¹æ¹®ç­ï¼ˆe^+e^-â†’aÎ³ï¼‰ï¼Œå¹¶æ¨å¯¼å‡ºå…³äºåˆ¶åŠ¨è¾å°„ï¼ˆe^- Nâ†’e^- Naï¼‰äº§ç”Ÿç‡çš„è§£æè¡¨è¾¾å¼ï¼Œä»¥ä¾¿æ›´é«˜æ•ˆåœ°è®¡ç®—ALPé€šé‡ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜è¯„ä¼°äº†ç”±äºç”µå­çƒ­è´¨é‡æ ¡æ­£ã€ç”µå­-ç”µå­åº“ä»‘ç›¸äº’ä½œç”¨ä»¥åŠLandau-Pomeranchuk-Migdalæ•ˆåº”è€Œäº§ç”Ÿçš„ç”Ÿäº§ç‡ä¸ç¡®å®šæ€§ã€‚ALPå‘å°„ç‡ä½œä¸ºALPè´¨é‡ã€æ¸©åº¦å’Œç­‰ç¦»å­ä½“ç”µå­åŒ–å­¦åŠ¿çš„å‡½æ•°å­˜å‚¨åœ¨å…¬å…±å­˜å‚¨åº“ä¸­ã€‚æœ€åï¼Œæˆ‘ä»¬ç ”ç©¶äº†ALPäº§ç”ŸåŠå…¶éšåçš„è¡°å˜å¯¹å¤©æ–‡è§‚æµ‹çš„å½±å“ï¼Œæ¨å¯¼å‡ºä¸ç”µå­è€¦åˆçš„ALPsçš„ä¸»è¦ç•Œé™ã€‚åœ¨å°è€¦åˆæƒ…å†µä¸‹ï¼Œä¸»è¦çš„çº¦æŸæ¥è‡ªäºä¹‹å‰è¢«å¿½ç•¥çš„è¡°å˜aâ†’e^+ e^-Î³ï¼Œé™¤äº†ç«çƒå½¢æˆåŒºåŸŸï¼Œå…¶ä¸­SN 1987A Xå°„çº¿è§‚æµ‹æä¾›äº†æœ€ä½³çš„æ¢æµ‹æ‰‹æ®µã€‚åœ¨å¤§è€¦åˆæƒ…å†µä¸‹ï¼Œç•Œé™ä¸»è¦ç”±èƒ½é‡æ²‰ç§¯è®ºè¯å†³å®šï¼Œæœ‰ä¸€ä¸ªæ–°è¿‘å¼€å‘çš„æ–°å…¬å¼é€‚ç”¨äºæ•è·æœºåˆ¶ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.15630v2">PDF</a> Removed typo in Appendix C and fixed constraints in Fig.8</p>
<p><strong>Summary</strong><br>     æœ¬æ–‡é‡æ–°æ¢è®¨äº†è½´å­ç²’å­ä¸ç”µå­åœ¨ç›¸å¯¹è®ºæ€§ç­‰ç¦»å­ä½“ä¸­çš„è€¦åˆäº§ç”Ÿé—®é¢˜ã€‚æ–‡ç« å±•ç¤ºäº†æ ‡é‡åœºä¸å¯¼æ•°è€¦åˆçš„ç­‰ä»·æ€§ï¼Œé¦–æ¬¡è€ƒè™‘äº†ä¹‹å‰è¢«å¿½ç•¥çš„è¿‡ç¨‹ï¼Œå¦‚åŠåº·æ™®é¡¿äº§ç”Ÿå’Œæ­£è´Ÿç”µå­å¯¹æ¹®ç­ï¼Œå¹¶æ¨å¯¼äº†è¾å°„åˆ¶åŠ¨äº§ç”Ÿç‡çš„è§£æè¡¨è¾¾å¼ï¼Œæé«˜äº†è½´å­ç²’å­æµé‡è¯„ä¼°çš„è®¡ç®—æ•ˆç‡ã€‚æ­¤å¤–ï¼Œæ–‡ç« è¿˜è¯„ä¼°äº†æ¥è‡ªç”µå­çƒ­è´¨é‡ä¿®æ­£ã€ç”µå­-ç”µå­åº“ä»‘ç›¸äº’ä½œç”¨ä»¥åŠæœ—é“-æ³¢æ¢…å°”å…°å…‹æ•ˆåº”çš„ç”Ÿäº§ç‡ä¸ç¡®å®šæ€§ã€‚è½´å­ç²’å­å‘å°„ç‡ä½œä¸ºç²’å­è´¨é‡ã€æ¸©åº¦å’Œç­‰ç¦»å­ä½“ç”µå­åŒ–å­¦åŠ¿çš„å‡½æ•°å·²åœ¨å…¬å…±å­˜å‚¨åº“ä¸­æä¾›ã€‚æœ€åï¼Œæ–‡ç« æ¢è®¨äº†è½´å­ç²’å­çš„äº§ç”Ÿå’Œéšåçš„è¡°å˜å¯¹å¤©æ–‡è§‚æµ‹çš„å½±å“ï¼Œæ¨å¯¼äº†è½´å­ç²’å­ä¸ç”µå­è€¦åˆçš„ä¸»è¦ç•Œé™ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å±•ç¤ºäº†è½´å­ç²’å­ä¸ç”µå­åœ¨ç›¸å¯¹è®ºæ€§ç­‰ç¦»å­ä½“ä¸­çš„è€¦åˆäº§ç”Ÿé—®é¢˜çš„é‡æ–°ç ”ç©¶ã€‚</li>
<li>è¯æ˜äº†æ ‡é‡åœºä¸å¯¼æ•°è€¦åˆçš„ç­‰ä»·æ€§ã€‚</li>
<li>è€ƒè™‘äº†ä¹‹å‰è¢«å¿½ç•¥çš„è¿‡ç¨‹ï¼Œå¦‚åŠåº·æ™®é¡¿äº§ç”Ÿå’Œæ­£è´Ÿç”µå­å¯¹æ¹®ç­ã€‚</li>
<li>æ¨å¯¼äº†è¾å°„åˆ¶åŠ¨äº§ç”Ÿç‡çš„è§£æè¡¨è¾¾å¼ï¼Œæé«˜äº†è®¡ç®—æ•ˆç‡ã€‚</li>
<li>è¯„ä¼°äº†æ¥è‡ªå¤šç§æ•ˆåº”çš„ç”Ÿäº§ç‡ä¸ç¡®å®šæ€§ã€‚</li>
<li>è½´å­ç²’å­å‘å°„ç‡å·²ä½œä¸ºå‡½æ•°åœ¨å…¬å…±å­˜å‚¨åº“ä¸­æä¾›ã€‚</li>
<li>æ¢è®¨äº†è½´å­ç²’å­çš„äº§ç”Ÿå’Œéšåçš„è¡°å˜å¯¹å¤©æ–‡è§‚æµ‹çš„å½±å“ï¼Œæä¾›äº†å…³äºè½´å­ç²’å­ä¸ç”µå­è€¦åˆçš„ä¸»è¦ç•Œé™çš„æ–°è§è§£ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.15630">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-57124e97b5c6f2f77ad2ebb8052d77dc.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-93d5fa12daa19fe1bd1e96c186f60ac4.jpg" align="middle">
</details>


<h1 id="-19"><a href="#-19" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-03-28/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">https://kedreamix.github.io/Talk2Paper/Paper/2025-03-28/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                                    <span class="chip bg-color">åŒ»å­¦å›¾åƒ</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-03-28/Interactive/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-8ffbe0487dfd5c42088b2a229c40eef2.jpg" class="responsive-img" alt="Interactive">
                        
                        <span class="card-title">Interactive</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Interactive æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-03-28  ARFlow Human Action-Reaction Flow Matching with Physical Guidance
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-03-28
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Interactive/" class="post-category">
                                    Interactive
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Interactive/">
                        <span class="chip bg-color">Interactive</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-03-28/Diffusion%20Models/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-baa866e41c9ab04e01cd17d20f2995b4.jpg" class="responsive-img" alt="Diffusion Models">
                        
                        <span class="card-title">Diffusion Models</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Diffusion Models æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-03-28  RecTable Fast Modeling Tabular Data with Rectified Flow
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-03-28
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Diffusion-Models/" class="post-category">
                                    Diffusion Models
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Diffusion-Models/">
                        <span class="chip bg-color">Diffusion Models</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">17196.4k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
