<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Few-Shot">
    <meta name="description" content="Few-Shot 方向最新论文已更新，请持续关注 Update in 2025-03-28  Reason-RFT Reinforcement Fine-Tuning for Visual Reasoning">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Few-Shot | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loader页面消失采用渐隐的方式*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logo出现动画 */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//使用渐隐的方法淡出loading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//强制显示loading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">嘘~  正在从服务器偷取页面 . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>首页</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>标签</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>分类</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>归档</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="搜索" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="深色/浅色模式" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			首页
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			标签
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			分类
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			归档
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pic1.zhimg.com/v2-b676d91c12009bfda247722bff30df3e.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Few-Shot</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- 文章内容详情 -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Few-Shot/">
                                <span class="chip bg-color">Few-Shot</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Few-Shot/" class="post-category">
                                Few-Shot
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp;
                    2025-03-28
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>更新日期:&nbsp;&nbsp;
                    2025-05-14
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>文章字数:&nbsp;&nbsp;
                    12k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>阅读时长:&nbsp;&nbsp;
                    48 分
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>阅读次数:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>⚠️ 以下所有内容总结都来自于 大语言模型的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p>
</blockquote>
<h1 id="2025-03-28-更新"><a href="#2025-03-28-更新" class="headerlink" title="2025-03-28 更新"></a>2025-03-28 更新</h1><h2 id="Reason-RFT-Reinforcement-Fine-Tuning-for-Visual-Reasoning"><a href="#Reason-RFT-Reinforcement-Fine-Tuning-for-Visual-Reasoning" class="headerlink" title="Reason-RFT: Reinforcement Fine-Tuning for Visual Reasoning"></a>Reason-RFT: Reinforcement Fine-Tuning for Visual Reasoning</h2><p><strong>Authors:Huajie Tan, Yuheng Ji, Xiaoshuai Hao, Minglan Lin, Pengwei Wang, Zhongyuan Wang, Shanghang Zhang</strong></p>
<p>Visual reasoning abilities play a crucial role in understanding complex multimodal data, advancing both domain-specific applications and artificial general intelligence (AGI). Existing methods improve VLM reasoning via Chain-of-Thought (CoT) supervised fine-tuning, using meticulously annotated training data to enhance visual reasoning capabilities. However, this training paradigm may lead to overfitting and cognitive rigidity, restricting the model’s ability to transfer visual reasoning skills across domains and limiting its real-world applicability. To address these limitations, we propose Reason-RFT, a novel reinforcement fine-tuning framework that significantly enhances generalization capabilities in visual reasoning tasks. Reason-RFT introduces a two-phase training framework for visual reasoning: (1) Supervised Fine-Tuning (SFT) with curated Chain-of-Thought (CoT) data activates the reasoning potential of Vision-Language Models (VLMs), followed by (2) Group Relative Policy Optimization (GRPO)-based reinforcement learning that generates multiple reasoning-response pairs, significantly enhancing generalization in visual reasoning tasks. To evaluate Reason-RFT’s visual reasoning capabilities, we reconstructed a comprehensive dataset spanning visual counting, structure perception, and spatial transformation.cExperimental results demonstrate Reasoning-RFT’s three key advantages: (1) Performance Enhancement: achieving state-of-the-art results across multiple tasks, outperforming most mainstream open-source and proprietary models; (2) Generalization Superiority: consistently maintaining robust performance across diverse tasks and domains, outperforming alternative training paradigms; (3) Data Efficiency: excelling in few-shot learning scenarios while surpassing full-dataset SFT baselines. </p>
<blockquote>
<p>视觉推理能力在理解复杂的多模式数据、推动领域特定应用和人工通用智能（AGI）方面发挥着至关重要的作用。现有方法通过基于思维链（CoT）的监督微调（SFT）改进VLM推理，使用精心注释的训练数据以增强视觉推理能力。然而，这种训练范式可能导致过拟合和认知僵化，限制模型在不同领域转移视觉推理技能的能力，并限制其在现实世界中的应用。为了解决这个问题，我们提出了Reason-RFT，这是一个新的强化微调框架，显著提高了视觉推理任务的泛化能力。Reason-RFT为视觉推理引入了一个两阶段训练框架：首先使用精选的思维链（CoT）数据进行监督微调（SFT），激活视觉语言模型（VLMs）的推理潜力；其次是基于群体相对策略优化（GRPO）的强化学习生成多个推理响应对，显著提高视觉推理任务的泛化能力。为了评估Reason-RFT的视觉推理能力，我们重建了一个全面的数据集，涵盖视觉计数、结构感知和空间变换等领域。实验结果表明，Reasoning-RFT具有三大关键优势：（1）性能提升：在多个任务上实现卓越的结果，优于大多数主流开源和专有模型；（2）泛化优势：在各种任务和领域上始终保持良好的性能，优于其他训练范式；（3）数据效率：在少量学习场景中表现优异，超越了全数据集SFT基准测试。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.20752v1">PDF</a> 35 pages, 22 figures</p>
<p><strong>Summary</strong></p>
<p>视觉推理能力在多模态数据理解中扮演着重要角色，推动着领域特定应用和通用人工智能的发展。现有方法通过链式思维监督微调来提升视觉推理能力，但这可能导致过拟合和认知僵化。为了解决这个问题，我们提出了Reason-RFT，一个增强视觉推理任务泛化能力的新型强化微调框架。该框架包括两个阶段：第一阶段是激活视觉语言模型的推理潜力；第二阶段是基于群体相对策略优化的强化学习，生成多个推理响应对，从而显著增强视觉推理任务的泛化能力。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>视觉推理在理解复杂多模态数据和推动领域特定应用及通用人工智能发展方面起关键作用。</li>
<li>现有方法通过链式思维监督微调提升视觉推理能力，但可能导致过拟合和认知僵化。</li>
<li>Reason-RFT框架旨在增强视觉推理任务的泛化能力，包括两个阶段：激活视觉语言模型的推理潜力和基于强化学习的优化。</li>
<li>该框架通过生成多个推理响应对来显著提高视觉推理任务的性能。</li>
<li>实验结果表明，Reason-RFT在多个任务上取得最先进的成果，并表现出卓越的性能、泛化能力和数据效率。</li>
<li>Reason-RFT框架在多种任务和数据集上的稳健性能验证了其优越性。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.20752">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-e0bfea3ced615b6f5e7a485d4761ee43.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-6ff52be7618643b781f0b047988a2a27.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-06b95518386236ce42fcfb65735cb601.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Synthetic-Data-Augmentation-for-Cross-domain-Implicit-Discourse-Relation-Recognition"><a href="#Synthetic-Data-Augmentation-for-Cross-domain-Implicit-Discourse-Relation-Recognition" class="headerlink" title="Synthetic Data Augmentation for Cross-domain Implicit Discourse Relation   Recognition"></a>Synthetic Data Augmentation for Cross-domain Implicit Discourse Relation   Recognition</h2><p><strong>Authors:Frances Yung, Varsha Suresh, Zaynab Reza, Mansoor Ahmad, Vera Demberg</strong></p>
<p>Implicit discourse relation recognition (IDRR) – the task of identifying the implicit coherence relation between two text spans – requires deep semantic understanding. Recent studies have shown that zero- or few-shot approaches significantly lag behind supervised models, but LLMs may be useful for synthetic data augmentation, where LLMs generate a second argument following a specified coherence relation. We applied this approach in a cross-domain setting, generating discourse continuations using unlabelled target-domain data to adapt a base model which was trained on source-domain labelled data. Evaluations conducted on a large-scale test set revealed that different variations of the approach did not result in any significant improvements. We conclude that LLMs often fail to generate useful samples for IDRR, and emphasize the importance of considering both statistical significance and comparability when evaluating IDRR models. </p>
<blockquote>
<p>隐式文本关系识别（IDRR）——识别两个文本片段之间的隐式连贯关系——需要深入语义理解。最近的研究表明，零样本或少样本方法明显落后于监督模型，但大型语言模型（LLMs）可用于合成数据增强，其中LLMs根据指定的连贯关系生成第二个论点。我们在跨域环境中应用了这种方法，使用未标记的目标域数据生成文本延续，以适应在源域标记数据上训练的基准模型。在大规模测试集上进行的评估表明，该方法的不同变体并未带来任何显著改进。我们得出结论，大型语言模型在为IDRR生成有用样本时经常失败，并强调在评估IDRR模型时，既要考虑统计意义，也要考虑可比性。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.20588v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文探讨了隐式话语关系识别（IDRR）任务，该任务需要深度语义理解。虽然零样本或少样本方法在这方面表现不佳，但大型语言模型（LLMs）可用于合成数据增强。研究尝试在跨域环境中使用LLMs生成与特定连贯关系相符的第二论点和文本延续，以适配基于源域标注数据的基准模型。然而，大规模测试集上的评估显示，不同方法的应用并未带来显著改进。因此，研究认为LLMs在生成IDRR样本方面并不总是有效，并强调在评估IDRR模型时需要考虑统计显著性和可比性。</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>隐式话语关系识别（IDRR）需要深度语义理解。</li>
<li>零样本或少样本方法在此任务上的表现不及监督模型。</li>
<li>大型语言模型（LLMs）可用于合成数据增强，通过生成与特定连贯关系相符的文本延续。</li>
<li>在跨域环境中使用LLMs生成数据以适配基准模型是一个尝试。</li>
<li>不同方法的应用在大型测试集上并未带来显著改进。</li>
<li>LLMs在生成IDRR样本方面并不总是有效。</li>
<li>在评估IDRR模型时，需要考虑统计显著性和可比性。</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.20588">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-676ada67e54fb9a0ebc0ae06f8d3686b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-22718eac3fff926d1b44cd1f699bbb72.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-79111d5c2638ea1f6c6db11718cf0ae1.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Attribute-formed-Class-specific-Concept-Space-Endowing-Language-Bottleneck-Model-with-Better-Interpretability-and-Scalability"><a href="#Attribute-formed-Class-specific-Concept-Space-Endowing-Language-Bottleneck-Model-with-Better-Interpretability-and-Scalability" class="headerlink" title="Attribute-formed Class-specific Concept Space: Endowing Language   Bottleneck Model with Better Interpretability and Scalability"></a>Attribute-formed Class-specific Concept Space: Endowing Language   Bottleneck Model with Better Interpretability and Scalability</h2><p><strong>Authors:Jianyang Zhang, Qianli Luo, Guowu Yang, Wenjing Yang, Weide Liu, Guosheng Lin, Fengmao Lv</strong></p>
<p>Language Bottleneck Models (LBMs) are proposed to achieve interpretable image recognition by classifying images based on textual concept bottlenecks. However, current LBMs simply list all concepts together as the bottleneck layer, leading to the spurious cue inference problem and cannot generalized to unseen classes. To address these limitations, we propose the Attribute-formed Language Bottleneck Model (ALBM). ALBM organizes concepts in the attribute-formed class-specific space, where concepts are descriptions of specific attributes for specific classes. In this way, ALBM can avoid the spurious cue inference problem by classifying solely based on the essential concepts of each class. In addition, the cross-class unified attribute set also ensures that the concept spaces of different classes have strong correlations, as a result, the learned concept classifier can be easily generalized to unseen classes. Moreover, to further improve interpretability, we propose Visual Attribute Prompt Learning (VAPL) to extract visual features on fine-grained attributes. Furthermore, to avoid labor-intensive concept annotation, we propose the Description, Summary, and Supplement (DSS) strategy to automatically generate high-quality concept sets with a complete and precise attribute. Extensive experiments on 9 widely used few-shot benchmarks demonstrate the interpretability, transferability, and performance of our approach. The code and collected concept sets are available at <a target="_blank" rel="noopener" href="https://github.com/tiggers23/ALBM">https://github.com/tiggers23/ALBM</a>. </p>
<blockquote>
<p>语言瓶颈模型（LBMs）旨在通过基于文本概念瓶颈的图像分类来实现可解释的图像识别。然而，目前的LBMs只是将所有概念一起列为瓶颈层，这导致了虚假线索推断问题，并且无法推广到未见过的类别。为了解决这些局限性，我们提出了属性形成语言瓶颈模型（ALBM）。ALBM在属性形成的类别特定空间中组织概念，其中概念是特定类别的特定属性的描述。通过这种方式，ALBM可以仅基于每个类别的关键概念进行分类，从而避免虚假线索推断问题。此外，跨类别统一属性集也确保了不同类别的概念空间具有强相关性，因此，所学的概念分类器可以很容易地推广到未见过的类别。而且，为了进一步提高可解释性，我们提出了视觉属性提示学习（VAPL）来提取细粒度属性上的视觉特征。此外，为了避免繁琐的概念标注，我们提出了描述、总结和补充（DSS）策略，以自动生成具有完整和精确属性的高质量概念集。在9个广泛使用的少样本基准测试上的大量实验证明了我们方法的可解释性、可迁移性和性能。代码和收集的概念集可在<a target="_blank" rel="noopener" href="https://github.com/tiggers23/ALBM%E4%B8%AD%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/tiggers23/ALBM中找到。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.20301v1">PDF</a> This paper has been accepted to CVPR 2025</p>
<p><strong>Summary</strong></p>
<p>本文提出了一种基于属性分类的语言瓶颈模型（ALBM），解决了现有图像识别模型的局限性问题。ALBM通过构建属性形式的类特定空间来组织概念，避免了基于非关键概念的误判问题，并能将概念分类器推广到未见类别。同时，通过视觉属性提示学习（VAPL）提高模型的解释性，并采用描述、摘要和补充（DSS）策略自动生成高质量的概念集。实验证明，该方法在少数样本场景下具有良好的解释性、迁移性和性能表现。</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>ALBM解决了现有图像识别模型因概念瓶颈层设计不当导致的误判问题。</li>
<li>ALBM构建属性形式的类特定空间来组织概念，能够基于每个类的关键概念进行分类，避免了非关键因素的干扰。</li>
<li>VAPL技术用于提取精细粒度上的视觉特征，提高了模型的解释性。</li>
<li>DSS策略能自动生成高质量的概念集，实现概念自动标注，降低了劳动强度。</li>
<li>实验结果表明，ALBM在少数样本场景下具有良好的解释性、迁移性和性能表现。</li>
<li>ALBM通过构建跨类别统一属性集，确保了不同类别概念空间的强关联性。</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.20301">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-dc0371183d1f1b6f3e067ea6dcca2e39.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6c3c764f8b644e299095f2de326ebcfd.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ef9440f4defe3a6ccfa7be1d35700880.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-8a0a9b09bab40508eca025ecce2bf25c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-52356e1b98e4affcb6248ee8e9ba7ebc.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="LogicQA-Logical-Anomaly-Detection-with-Vision-Language-Model-Generated-Questions"><a href="#LogicQA-Logical-Anomaly-Detection-with-Vision-Language-Model-Generated-Questions" class="headerlink" title="LogicQA: Logical Anomaly Detection with Vision Language Model Generated   Questions"></a>LogicQA: Logical Anomaly Detection with Vision Language Model Generated   Questions</h2><p><strong>Authors:Yejin Kwon, Daeun Moon, Youngje Oh, Hyunsoo Yoon</strong></p>
<p>Anomaly Detection (AD) focuses on detecting samples that differ from the standard pattern, making it a vital tool in process control. Logical anomalies may appear visually normal yet violate predefined constraints on object presence, arrangement, or quantity, depending on reasoning and explainability. We introduce LogicQA, a framework that enhances AD by providing industrial operators with explanations for logical anomalies. LogicQA compiles automatically generated questions into a checklist and collects responses to identify violations of logical constraints. LogicQA is training-free, annotation-free, and operates in a few-shot setting. We achieve state-of-the-art (SOTA) Logical AD performance on public benchmarks, MVTec LOCO AD, with an AUROC of 87.6 percent and an F1-max of 87.0 percent along with the explanations of anomalies. Also, our approach has shown outstanding performance on semiconductor SEM corporate data, further validating its effectiveness in industrial applications. </p>
<blockquote>
<p>异常检测（AD）专注于检测与标准模式不同的样本，使其成为过程控制中的关键工具。逻辑异常在视觉上可能看似正常，但会违反对象存在、排列或数量等方面的预先定义约束，这些约束取决于推理和可解释性。我们引入了LogicQA框架，它通过为工业操作员提供逻辑异常的解释来增强AD的功能。LogicQA将自动生成的问题编译成清单，并收集回应来识别逻辑约束的违反情况。LogicQA无需训练，无需标注，并且在小样本设置下运行。我们在公共基准测试MVTec LOCO AD上实现了最先进的逻辑AD性能，具有87.6％的AUROC和87.0％的F1-max，并提供了异常的解释。此外，我们的方法在半导体SEM企业数据上也表现出了出色的性能，进一步验证了其在工业应用中的有效性。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.20252v1">PDF</a> </p>
<p><strong>Summary</strong><br>逻辑异常检测（AD）对于发现与标准模式不同的样本至关重要，是过程控制中的关键工具。我们引入了LogicQA框架，它通过提供对逻辑异常的解释来增强AD。LogicQA自动生成问题清单并收集答案，以识别逻辑约束的违反情况。该框架无需训练和标注，适用于小样本环境。在公共基准测试MVTec LOCO AD上，我们实现了最先进的逻辑AD性能，其中AUROC为87.6%，F1-max为87.0%，并提供了异常解释。此外，我们的方法在半导体的SEM企业数据上表现出卓越的性能，进一步证明了它在工业应用中的有效性。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>逻辑异常检测（AD）在过程控制中具有关键作用，能够识别与标准模式不同的样本。</li>
<li>LogicQA框架通过提供对逻辑异常的解释来增强AD。</li>
<li>LogicQA自动生成问题清单并收集答案以识别逻辑约束的违反情况。</li>
<li>LogicQA框架适用于小样本环境，无需训练和标注。</li>
<li>在公共基准测试MVTec LOCO AD上，LogicQA实现了最先进的逻辑AD性能。</li>
<li>LogicQA的AUROC和F1-max指标分别达到了87.6%和87.0%。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.20252">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-0337956edf84f0b02a3fe97e60f2350f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5a84f1eadb30a499a5e68d4dae8a043b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b35b3e797d30a70be5b06e152c338dcf.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b9069c06875e78034da4f7c6c37b67de.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-09ddcb731fe555b7ecf8c0e12a1accf9.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-abeaab0cecbcc26b32aeea9ae6b8e577.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="DINeMo-Learning-Neural-Mesh-Models-with-no-3D-Annotations"><a href="#DINeMo-Learning-Neural-Mesh-Models-with-no-3D-Annotations" class="headerlink" title="DINeMo: Learning Neural Mesh Models with no 3D Annotations"></a>DINeMo: Learning Neural Mesh Models with no 3D Annotations</h2><p><strong>Authors:Weijie Guo, Guofeng Zhang, Wufei Ma, Alan Yuille</strong></p>
<p>Category-level 3D&#x2F;6D pose estimation is a crucial step towards comprehensive 3D scene understanding, which would enable a broad range of applications in robotics and embodied AI. Recent works explored neural mesh models that approach a range of 2D and 3D tasks from an analysis-by-synthesis perspective. Despite the largely enhanced robustness to partial occlusion and domain shifts, these methods depended heavily on 3D annotations for part-contrastive learning, which confines them to a narrow set of categories and hinders efficient scaling. In this work, we present DINeMo, a novel neural mesh model that is trained with no 3D annotations by leveraging pseudo-correspondence obtained from large visual foundation models. We adopt a bidirectional pseudo-correspondence generation method, which produce pseudo correspondence utilize both local appearance features and global context information. Experimental results on car datasets demonstrate that our DINeMo outperforms previous zero- and few-shot 3D pose estimation by a wide margin, narrowing the gap with fully-supervised methods by 67.3%. Our DINeMo also scales effectively and efficiently when incorporating more unlabeled images during training, which demonstrate the advantages over supervised learning methods that rely on 3D annotations. Our project page is available at <a target="_blank" rel="noopener" href="https://analysis-by-synthesis.github.io/DINeMo/">https://analysis-by-synthesis.github.io/DINeMo/</a>. </p>
<blockquote>
<p>类别级别的3D&#x2F;6D姿态估计是实现全面3D场景理解的关键步骤，这将为机器人技术和嵌入式人工智能的广泛应用提供可能。近期的研究探索了神经网格模型，从综合分析的角度来解决一系列2D和3D任务。尽管这些方法在很大程度上增强了处理部分遮挡和领域偏移的稳健性，但它们严重依赖于3D注释进行部分对比学习，这限制了它们的应用范围并阻碍了有效扩展。在这项工作中，我们提出了DINeMo，这是一种新型神经网格模型，它通过从大型视觉基础模型中获得伪对应物来进行训练，无需任何3D注释。我们采用了一种双向伪对应生成方法，该方法利用局部外观特征和全局上下文信息来生成伪对应物。在汽车数据集上的实验结果表明，我们的DINeMo大幅超越了之前的零样本和少样本3D姿态估计，与完全监督的方法的差距缩小了67.3%。此外，当在训练过程中加入更多无标签图像时，我们的DINeMo能够有效地进行扩展，这显示了与依赖3D注释的监督学习方法的优势。我们的项目页面可在<a target="_blank" rel="noopener" href="https://analysis-by-synthesis.github.io/DINeMo/">https://analysis-by-synthesis.github.io/DINeMo/</a>访问。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.20220v1">PDF</a> Technical report</p>
<p><strong>Summary</strong></p>
<p>基于伪对应的大型视觉基础模型的神经网络模型DINeMo，无需3D注释即可进行训练，实现了跨类别级别的3D&#x2F;6D姿态估计。通过利用局部外观特征和全局上下文信息的双向伪对应生成方法，DINeMo在汽车数据集上的实验结果显示出了卓越的零样本和少样本学习能力，相较于完全监督的方法缩小了差距。此外，DINeMo在训练过程中能够有效地利用更多的无标签图像，显示出其相较于依赖3D注释的监督学习方法的优势。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>DINeMo是一种新型的神经网络模型，用于实现无需3D注释的类别级别的3D&#x2F;6D姿态估计。</li>
<li>DINeMo通过利用伪对应，借助大型视觉基础模型进行训练。</li>
<li>该模型采用双向伪对应生成方法，结合局部外观特征和全局上下文信息。</li>
<li>在汽车数据集上的实验结果显示，DINeMo在零样本和少样本学习方面表现出卓越性能。</li>
<li>DINeMo缩小了与完全监督方法之间的差距，达到67.3%。</li>
<li>DINeMo在训练过程中能够有效地利用更多的无标签图像。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.20220">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-3883d8644f3e299967e982abcce9a986.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-4a7c3e6ac49cfc6d366dcb7311d6e048.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-cd8995511b17b2a28c0b9161a9f7400d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-79e93ebc9e2e254fd36162a698b906ea.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-803007072af6b64901d21078ca72630f.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="MMRL-Multi-Modal-Representation-Learning-for-Vision-Language-Models"><a href="#MMRL-Multi-Modal-Representation-Learning-for-Vision-Language-Models" class="headerlink" title="MMRL: Multi-Modal Representation Learning for Vision-Language Models"></a>MMRL: Multi-Modal Representation Learning for Vision-Language Models</h2><p><strong>Authors:Yuncheng Guo, Xiaodong Gu</strong></p>
<p>Large-scale pre-trained Vision-Language Models (VLMs) have become essential for transfer learning across diverse tasks. However, adapting these models with limited few-shot data often leads to overfitting, diminishing their performance on new tasks. To tackle this issue, we propose a novel Multi-Modal Representation Learning (MMRL) framework that introduces a shared, learnable, and modality-agnostic representation space. MMRL projects the space tokens to text and image representation tokens, facilitating more effective multi-modal interactions. Unlike previous approaches that solely optimize class token features, MMRL integrates representation tokens at higher layers of the encoders–where dataset-specific features are more prominent–while preserving generalized knowledge in the lower layers. During training, both representation and class features are optimized, with trainable projection layer applied to the representation tokens, whereas the class token projection layer remains frozen to retain pre-trained knowledge. Furthermore, a regularization term is introduced to align the class features and text features with the zero-shot features from the frozen VLM, thereby safeguarding the model’s generalization capacity. For inference, a decoupling strategy is employed, wherein both representation and class features are utilized for base classes, while only the class features, which retain more generalized knowledge, are used for new tasks. Extensive experiments across 15 datasets demonstrate that MMRL outperforms state-of-the-art methods, achieving a balanced trade-off between task-specific adaptation and generalization. Code is available at <a target="_blank" rel="noopener" href="https://github.com/yunncheng/MMRL">https://github.com/yunncheng/MMRL</a>. </p>
<blockquote>
<p>大规模预训练的视觉语言模型（VLMs）对于跨不同任务的迁移学习已经变得至关重要。然而，使用有限的少量数据适应这些模型往往会导致过拟合，从而降低它们在新任务上的性能。为了解决这个问题，我们提出了一种新的多模态表示学习（MMRL）框架，它引入了一个共享、可学习和与模态无关的表示空间。MMRL将空间标记投射到文本和图像表示标记上，促进更有效的多模态交互。不同于之前仅优化类别标记特征的方法，MMRL在编码器的更高层集成表示标记，在这里数据集特定的特征更加突出，同时在下层保持通用知识。在训练过程中，表示特征和类别特征都得到优化，可训练的投影层应用于表示标记，而类别标记投影层保持冻结以保留预训练知识。此外，引入了一个正则化项，以使类别特征和文本特征与冻结VLM的零射特征对齐，从而保护模型的泛化能力。对于推断，采用了解耦策略，其中表示特征和类别特征都用于基本类别，而仅使用保留更多通用知识的类别特征用于新任务。在15个数据集上的大量实验表明，MMRL优于现有方法，实现了任务特定适应性和泛化之间的平衡。代码可用在 <a target="_blank" rel="noopener" href="https://github.com/yunncheng/MMRL%E3%80%82">https://github.com/yunncheng/MMRL。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.08497v2">PDF</a> Accepted by CVPR 2025</p>
<p><strong>Summary</strong></p>
<p>大规模预训练视听觉语言模型（VLMs）对于跨多种任务的迁移学习至关重要。然而，使用有限的少量数据进行模型适应往往会导致过拟合，影响模型在新任务上的表现。为解决这一问题，我们提出了全新的多模态表示学习（MMRL）框架，引入了一个共享、可学习和模态无关的表示空间。MMRL将空间标记投影到文本和图像表示标记上，促进更有效的多模态交互。与其他方法不同，MMRL在编码器的更高层集成表示标记，同时保留较低层次的通用知识。在训练过程中，同时优化表示和类别特征，对表示标记应用可训练的投影层，而保持类别标记投影层冻结以保留预训练知识。此外，引入正则化项以对齐类别特征和文本特征，以及与冻结VLM的零样本特征，从而保护模型的泛化能力。对于推断，采用了解耦策略，其中表示和类别特征用于基础类别，而仅使用保留更多通用知识的类别特征用于新任务。广泛的实验表明，MMRL在15个数据集上的表现均优于现有方法，实现了任务特定适应和泛化之间的平衡。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>大规模预训练视听觉语言模型（VLMs）在迁移学习中很重要。</li>
<li>使用有限的少量数据进行模型适应可能导致过拟合。</li>
<li>提出新的多模态表示学习（MMRL）框架来解决这一问题。</li>
<li>MMRL引入共享、可学习和模态无关的表示空间。</li>
<li>MMRL通过投影空间标记到文本和图像表示标记来促进多模态交互。</li>
<li>MMRL在编码器的更高层集成表示标记，同时保留通用知识。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.08497">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-f4efa7739b65eff08b2d6ef8ae748ec2.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-dae975ba432b158293f3f1a0ab1b109b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-cb187fc42024f5b87086306375cfba0a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1f7612300d9f368546758d7630a0a27b.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Ensemble-Debiasing-Across-Class-and-Sample-Levels-for-Fairer-Prompting-Accuracy"><a href="#Ensemble-Debiasing-Across-Class-and-Sample-Levels-for-Fairer-Prompting-Accuracy" class="headerlink" title="Ensemble Debiasing Across Class and Sample Levels for Fairer Prompting   Accuracy"></a>Ensemble Debiasing Across Class and Sample Levels for Fairer Prompting   Accuracy</h2><p><strong>Authors:Ruixi Lin, Ziqiao Wang, Yang You</strong></p>
<p>Language models are strong few-shot learners and achieve good overall accuracy in text classification tasks, masking the fact that their results suffer from great class accuracy imbalance. We believe that the pursuit of overall accuracy should not come from enriching the strong classes, but from raising up the weak ones. To address the imbalance, we propose a Heaviside step function based ensemble debiasing method, which enables flexible rectifications of in-context learned class probabilities at both class and sample levels. Evaluations with Llama-2-13B on seven text classification benchmarks show that our approach achieves state-of-the-art overall accuracy gains with balanced class accuracies. More importantly, we perform analyses on the resulted probability correction scheme, showing that sample-level corrections are necessary to elevate weak classes. Due to effectively correcting weak classes, our method also brings significant performance gains to a larger model variant, Llama-2-70B, especially on a biomedical domain task, further demonstrating the necessity of ensemble debiasing at both levels. </p>
<blockquote>
<p>语言模型是强大的小样本学习者，在文本分类任务中达到了良好的总体精度，掩盖了其结果受到严重类别精度不平衡的影响。我们认为追求总体精度不应通过丰富强类来实现，而应通过提高弱类来实现。为了解决不平衡问题，我们提出了一种基于海维赛德阶跃函数的集成去偏方法，该方法能够在类和样本级别灵活地修正上下文学习的类概率。使用Llama-2-13B在七个文本分类基准测试上的评估表明，我们的方法在实现最先进的总体精度增益的同时，也实现了平衡的类别精度。更重要的是，我们对所得概率修正方案进行了分析，表明样本级修正对于提高弱类是必要的。由于有效地纠正了弱类，我们的方法也为更大的模型变体Llama-2-70B带来了显著的性能提升，特别是在生物医学领域任务中更是如此，这进一步证明了在两级进行集成去偏的必要性。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.05157v3">PDF</a> </p>
<p><strong>Summary</strong><br>语言模型在少数样本学习方面表现出色，并在文本分类任务中达到较高的整体准确率。然而，存在类别准确率失衡的问题。为提高弱类别的准确率，提出了一种基于海维赛德阶跃函数的集成去偏方法，可在类和样本级别灵活地修正上下文学习到的类别概率。在多个文本分类基准测试上的评估表明，该方法实现了最先进的总体精度增益和平衡的类别精度。对概率修正方案的分析表明，样本级修正对于提高弱类别是必要的。该方法在大型模型变体（尤其是生物医学领域任务）上也带来了显著的性能提升，进一步证明了在两级进行集成去偏的必要性。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>语言模型是强大的少数样本学习者，在文本分类任务中表现出高整体准确率。</li>
<li>存在类别准确率失衡的问题，即某些类别的预测结果比其他类别更不可靠。</li>
<li>提出了基于海维赛德阶跃函数的集成去偏方法，用于修正类别概率。</li>
<li>该方法在多个文本分类任务上实现了先进的总体精度增益和平衡的类别精度。</li>
<li>样本级修正对于提高弱类别（原先预测较差的类别）的准确率是必要的。</li>
<li>该方法适用于不同规模的语言模型，特别是在处理复杂领域（如生物医学）时表现出优势。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.05157">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-455bf876f4c51f487c10f76839fb8ade.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-49e7abf837ae065b0f6b125e263f6ea4.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-206aed54cacde9744d1d5f74312a649e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7b10245ff8527ca89793162cfdbbc0b9.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="FrugalNeRF-Fast-Convergence-for-Few-shot-Novel-View-Synthesis-without-Learned-Priors"><a href="#FrugalNeRF-Fast-Convergence-for-Few-shot-Novel-View-Synthesis-without-Learned-Priors" class="headerlink" title="FrugalNeRF: Fast Convergence for Few-shot Novel View Synthesis without   Learned Priors"></a>FrugalNeRF: Fast Convergence for Few-shot Novel View Synthesis without   Learned Priors</h2><p><strong>Authors:Chin-Yang Lin, Chung-Ho Wu, Chang-Han Yeh, Shih-Han Yen, Cheng Sun, Yu-Lun Liu</strong></p>
<p>Neural Radiance Fields (NeRF) face significant challenges in extreme few-shot scenarios, primarily due to overfitting and long training times. Existing methods, such as FreeNeRF and SparseNeRF, use frequency regularization or pre-trained priors but struggle with complex scheduling and bias. We introduce FrugalNeRF, a novel few-shot NeRF framework that leverages weight-sharing voxels across multiple scales to efficiently represent scene details. Our key contribution is a cross-scale geometric adaptation scheme that selects pseudo ground truth depth based on reprojection errors across scales. This guides training without relying on externally learned priors, enabling full utilization of the training data. It can also integrate pre-trained priors, enhancing quality without slowing convergence. Experiments on LLFF, DTU, and RealEstate-10K show that FrugalNeRF outperforms other few-shot NeRF methods while significantly reducing training time, making it a practical solution for efficient and accurate 3D scene reconstruction. </p>
<blockquote>
<p>神经辐射场（NeRF）在极端小样本场景中面临重大挑战，主要是由于过拟合和训练时间过长。现有方法，如FreeNeRF和SparseNeRF，使用频率正则化或预训练先验，但面临复杂的调度和偏见问题。我们引入了FrugalNeRF，这是一种新的少样本NeRF框架，它通过跨多个尺度共享权重体积来有效地表示场景细节。我们的主要贡献是一种跨尺度的几何自适应方案，该方案根据跨尺度的重投影误差选择伪真实深度。这可以在不依赖外部学习先验的情况下指导训练，实现训练数据的充分利用。它还可以集成预训练的先验，提高质量而不会减慢收敛速度。在LLFF、DTU和RealEstate-10K上的实验表明，FrugalNeRF在其他少样本NeRF方法中表现突出，同时显著减少了训练时间，成为高效且准确的3D场景重建的实际解决方案。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2410.16271v2">PDF</a> Paper accepted to CVPR 2025. Project page:   <a target="_blank" rel="noopener" href="https://linjohnss.github.io/frugalnerf/">https://linjohnss.github.io/frugalnerf/</a></p>
<p><strong>Summary</strong></p>
<p>神经网络辐射场（NeRF）在极端小样本场景中面临重大挑战，主要表现为过度拟合和训练时间长。现有方法如FreeNeRF和SparseNeRF采用频率正则化或预训练先验，但面临复杂调度和偏差问题。我们推出FrugalNeRF，一种全新的少样本NeRF框架，通过跨多个尺度的权重共享体素有效地表示场景细节。其关键贡献在于提出了一种跨尺度几何自适应方案，根据跨尺度的重投影误差选择伪真实深度，从而引导训练，无需依赖外部学习先验，充分利用训练数据。同时，它也能集成预训练的先验知识，提高质量而不会减慢收敛速度。实验表明，在LLFF、DTU和RealEstate-10K数据集上，FrugalNeRF在少样本NeRF方法中表现优异，训练时间也大大缩短，成为高效准确3D场景重建的实际解决方案。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>FrugalNeRF解决了神经网络辐射场（NeRF）在极端小样本场景中的挑战。</li>
<li>它通过跨尺度的权重共享体素有效地表示场景细节。</li>
<li>FrugalNeRF采用跨尺度几何自适应方案，根据重投影误差选择伪真实深度。</li>
<li>该方法无需依赖外部学习先验，能充分利用训练数据。</li>
<li>FrugalNeRF能集成预训练的先验知识，提高质量而不会减慢收敛速度。</li>
<li>实验表明，FrugalNeRF在多个数据集上表现优异，训练时间缩短。</li>
<li>FrugalNeRF成为高效准确的3D场景重建的实际解决方案。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2410.16271">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-50699341b52a4a7b2529ace90b706f17.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-43245106be4869fc1597a67e5e0b5280.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4fe536df3ec9788b35f091f9817c2d15.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-616ab28c0df68c6bcf579767ac650c1b.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Few-Shot-Joint-Multimodal-Entity-Relation-Extraction-via-Knowledge-Enhanced-Cross-modal-Prompt-Model"><a href="#Few-Shot-Joint-Multimodal-Entity-Relation-Extraction-via-Knowledge-Enhanced-Cross-modal-Prompt-Model" class="headerlink" title="Few-Shot Joint Multimodal Entity-Relation Extraction via   Knowledge-Enhanced Cross-modal Prompt Model"></a>Few-Shot Joint Multimodal Entity-Relation Extraction via   Knowledge-Enhanced Cross-modal Prompt Model</h2><p><strong>Authors:Li Yuan, Yi Cai, Junsheng Huang</strong></p>
<p>Joint Multimodal Entity-Relation Extraction (JMERE) is a challenging task that aims to extract entities and their relations from text-image pairs in social media posts. Existing methods for JMERE require large amounts of labeled data. However, gathering and annotating fine-grained multimodal data for JMERE poses significant challenges. Initially, we construct diverse and comprehensive multimodal few-shot datasets fitted to the original data distribution. To address the insufficient information in the few-shot setting, we introduce the \textbf{K}nowledge-\textbf{E}nhanced \textbf{C}ross-modal \textbf{P}rompt \textbf{M}odel (KECPM) for JMERE. This method can effectively address the problem of insufficient information in the few-shot setting by guiding a large language model to generate supplementary background knowledge. Our proposed method comprises two stages: (1) a knowledge ingestion stage that dynamically formulates prompts based on semantic similarity guide ChatGPT generating relevant knowledge and employs self-reflection to refine the knowledge; (2) a knowledge-enhanced language model stage that merges the auxiliary knowledge with the original input and utilizes a transformer-based model to align with JMERE’s required output format. We extensively evaluate our approach on a few-shot dataset derived from the JMERE dataset, demonstrating its superiority over strong baselines in terms of both micro and macro F$_1$ scores. Additionally, we present qualitative analyses and case studies to elucidate the effectiveness of our model. </p>
<blockquote>
<p>联合多模态实体关系提取（JMERE）是一项具有挑战性的任务，旨在从社交媒体帖子中的文本-图像对中提取实体及其关系。现有的JMERE方法需要大量的标记数据。然而，为JMERE收集并标注细粒度的多模态数据面临着巨大的挑战。首先，我们构建了多样且全面的多模态小样本数据集，以适应原始数据分布。为了解决小样本设置中信息不足的问题，我们引入了针对JMERE的<strong>知识增强跨模态提示模型（KECPM）</strong>。该方法可以通过引导大型语言模型生成补充背景知识，有效解决小样本设置中的信息不足问题。我们提出的方法包括两个阶段：（1）知识摄取阶段，该阶段根据语义相似性动态制定提示，指导ChatGPT生成相关知识，并利用自我反思来完善知识；（2）知识增强语言模型阶段，该阶段将辅助知识与原始输入合并，并使用基于转换器的方法来对齐JMERE所需的输出格式。我们在从JMERE数据集中派生的小样本数据集上广泛评估了我们的方法，结果显示它在微观和宏观F1分数方面都优于强大的基准模型。此外，我们还进行了定性分析和案例研究，以阐明我们模型的有效性。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2410.14225v2">PDF</a> accepted by ACM MM 2024</p>
<p><strong>Summary</strong></p>
<p>本文介绍了联合多模态实体关系提取（JMERE）的挑战性任务，旨在从社交媒体帖子中的文本-图像对中提取实体和它们的关系。现有方法需要大量标记数据，但为JMERE收集并标注细粒度的多模态数据具有很大挑战。为此，本文构建了适应原始数据分布的多样且全面的多模态小样本数据集，并引入知识增强跨模态提示模型（KECPM）解决信息不足的问题。该方法通过指导大型语言模型生成补充背景知识，有效地解决了小样本设置中的信息不足问题。该方法包括两个阶段：知识摄取阶段和知识增强语言模型阶段。在少量数据集上的评估结果表明，该方法在微观和宏观F1分数方面优于强基线。此外，本文还提供了定性分析和案例研究来阐明模型的有效性。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>JMERE是一个从社交媒体帖子中的文本-图像对提取实体和关系的挑战性任务。</li>
<li>现有方法需要大量标记数据，但收集并标注细粒度的多模态数据具有挑战。</li>
<li>引入知识增强跨模态提示模型（KECPM）解决小样本设置中的信息不足问题。</li>
<li>KECPM方法包括知识摄取阶段和知识增强语言模型阶段。</li>
<li>在少量数据集上的评估表明，该方法在F1分数方面优于强基线。</li>
<li>定性分析和案例研究证明了模型的有效性。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2410.14225">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-b676d91c12009bfda247722bff30df3e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9c03ea5dc6de9dd2af1785ae7ebcadd9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b7612697005cd40805f54213debd3811.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c01cffd6bb6c7ee950aa547ef32d37ad.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-8d5b6e6a5b0b454fb461a4760d41e2cd.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4deffd817b50db31ab282f576b42e3f1.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="RobustEMD-Domain-Robust-Matching-for-Cross-domain-Few-shot-Medical-Image-Segmentation"><a href="#RobustEMD-Domain-Robust-Matching-for-Cross-domain-Few-shot-Medical-Image-Segmentation" class="headerlink" title="RobustEMD: Domain Robust Matching for Cross-domain Few-shot Medical   Image Segmentation"></a>RobustEMD: Domain Robust Matching for Cross-domain Few-shot Medical   Image Segmentation</h2><p><strong>Authors:Yazhou Zhu, Minxian Li, Qiaolin Ye, Shidong Wang, Tong Xin, Haofeng Zhang</strong></p>
<p>Few-shot medical image segmentation (FSMIS) aims to perform the limited annotated data learning in the medical image analysis scope. Despite the progress has been achieved, current FSMIS models are all trained and deployed on the same data domain, as is not consistent with the clinical reality that medical imaging data is always across different data domains (e.g. imaging modalities, institutions and equipment sequences). How to enhance the FSMIS models to generalize well across the different specific medical imaging domains? In this paper, we focus on the matching mechanism of the few-shot semantic segmentation models and introduce an Earth Mover’s Distance (EMD) calculation based domain robust matching mechanism for the cross-domain scenario. Specifically, we formulate the EMD transportation process between the foreground support-query features, the texture structure aware weights generation method, which proposes to perform the sobel based image gradient calculation over the nodes, is introduced in the EMD matching flow to restrain the domain relevant nodes. Besides, the point set level distance measurement metric is introduced to calculated the cost for the transportation from support set nodes to query set nodes. To evaluate the performance of our model, we conduct experiments on three scenarios (i.e., cross-modal, cross-sequence and cross-institution), which includes eight medical datasets and involves three body regions, and the results demonstrate that our model achieves the SoTA performance against the compared models. </p>
<blockquote>
<p>少量标注数据的医疗图像分割（FSMIS）旨在实现医疗图像分析范围内的有限标注数据学习。尽管已经取得了一些进展，但目前的FSMIS模型都在同一数据域上进行训练和应用，这与医学成像数据在临床上的实际情况不符，医学成像数据总是跨越不同的数据域（例如成像模式、机构和设备序列）。如何增强FSMIS模型在不同特定医学成像域之间的泛化能力？在本文中，我们关注少量语义分割模型的匹配机制，并引入一种基于地球移动距离（EMD）计算的域鲁棒匹配机制，用于跨域场景。具体来说，我们制定了前景支持查询特征之间的EMD传输过程，并引入了纹理结构感知权重生成方法，该方法建议在节点上执行基于Sobel的图像梯度计算。在EMD匹配流中引入这种方法来约束与域相关的节点。此外，还引入了点集级距离测量指标，用于计算从支持集节点到查询集节点的传输成本。为了评估我们模型的性能，我们在三种场景（即跨模态、跨序列和跨机构）进行了实验，包括八个医学数据集和三个身体区域，结果表明我们的模型与对比模型相比达到了最新性能水平。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2410.01110v4">PDF</a> More details should be included, and more experiments</p>
<p><strong>Summary</strong></p>
<p>本文介绍了针对医学图像分析中有限标注数据学习问题的Few-shot医学图像分割（FSMIS）技术。现有FSMIS模型局限在同一数据域内训练部署，不符合医学成像数据跨不同域（如成像模式、机构和设备序列等）的临床现实。本文关注少样本语义分割模型的匹配机制，并提出基于地球移动距离（EMD）计算的域鲁棒匹配机制，适用于跨域场景。实验表明，该模型在跨模态、跨序列和跨机构等三种场景下表现优异，达到当前最佳技术水平。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Few-shot医学图像分割（FSMIS）关注有限标注数据学习在医学图像分析中的应用。</li>
<li>当前FSMIS模型局限在同一数据域内，不符合医学成像数据的实际跨域情况。</li>
<li>本文提出基于地球移动距离（EMD）的域鲁棒匹配机制，适用于跨域医学图像分割。</li>
<li>匹配机制中引入EMD计算，制定前景支持特征之间的运输过程。</li>
<li>为约束域相关节点，提出基于Sobel的图像梯度计算法，并引入点集级别距离测量标准。</li>
<li>实验表明，该模型在跨模态、跨序列和跨机构等场景下表现优越。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2410.01110">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-6ead464cd01cc60a5a1bc36b7226bc26.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-9694b36b6f2c838aa4e62edcc696614d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9ca9d0e32e4b4033b35a3a671a4b8c4c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-2d79e93e1b52bcd63795303434c3a06e.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="Few-Shot-Recognition-via-Stage-Wise-Retrieval-Augmented-Finetuning"><a href="#Few-Shot-Recognition-via-Stage-Wise-Retrieval-Augmented-Finetuning" class="headerlink" title="Few-Shot Recognition via Stage-Wise Retrieval-Augmented Finetuning"></a>Few-Shot Recognition via Stage-Wise Retrieval-Augmented Finetuning</h2><p><strong>Authors:Tian Liu, Huixin Zhang, Shubham Parashar, Shu Kong</strong></p>
<p>Few-shot recognition (FSR) aims to train a classification model with only a few labeled examples of each concept concerned by a downstream task, where data annotation cost can be prohibitively high. We develop methods to solve FSR by leveraging a pretrained Vision-Language Model (VLM). We particularly explore retrieval-augmented learning (RAL), which retrieves open data, e.g., the VLM’s pretraining dataset, to learn models for better serving downstream tasks. RAL has been studied in zero-shot recognition but remains under-explored in FSR. Although applying RAL to FSR may seem straightforward, we observe interesting and novel challenges and opportunities. First, somewhat surprisingly, finetuning a VLM on a large amount of retrieved data underperforms state-of-the-art zero-shot methods. This is due to the imbalanced distribution of retrieved data and its domain gaps with the few-shot examples in the downstream task. Second, more surprisingly, we find that simply finetuning a VLM solely on few-shot examples significantly outperforms previous FSR methods, and finetuning on the mix of retrieved and few-shot data yields even better results. Third, to mitigate the imbalanced distribution and domain gap issues, we propose Stage-Wise retrieval-Augmented fineTuning (SWAT), which involves end-to-end finetuning on mixed data in the first stage and retraining the classifier on the few-shot data in the second stage. Extensive experiments on nine popular benchmarks demonstrate that SWAT significantly outperforms previous methods by &gt;6% accuracy. </p>
<blockquote>
<p>少量样本识别（FSR）旨在仅使用下游任务所涉及概念的一些标记样本训练分类模型，其中数据标注成本可能非常高。我们开发了一种利用预训练视觉语言模型（VLM）来解决FSR的方法。我们特别探讨了检索增强学习（RAL），它检索开放数据，例如VLM的预训练数据集，以学习更好地为下游任务服务的模型。虽然RAL在零样本识别中已被研究，但在FSR中仍然被低估。尽管将RAL应用于FSR可能看起来很简单，但我们观察到有趣的新挑战和机会。首先，令人惊讶的是，在大量检索数据上微调VLM的表现不及最先进的零样本方法。这是由于检索数据的分布不平衡以及与下游任务中少量样本的域差距。第二，更令人惊讶的是，我们发现在少量样本上仅微调VLM显著优于之前的FSR方法，而在检索数据和少量样本数据混合上微调则能产生更好的结果。第三，为了缓解分布不平衡和域差距问题，我们提出了分阶段检索增强微调（SWAT），第一阶段对混合数据进行端到端微调，第二阶段在少量样本数据上重新训练分类器。在九个流行基准测试上的大量实验表明，SWAT在准确率上显著优于之前的方法，提高了超过6%。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2406.11148v3">PDF</a> Accepted to CVPR 2025. Website and code:   <a target="_blank" rel="noopener" href="https://tian1327.github.io/SWAT/">https://tian1327.github.io/SWAT/</a></p>
<p><strong>Summary</strong></p>
<p>本文探讨了基于预训练视觉语言模型的少样本识别（FSR）方法，特别是探索了检索增强学习（RAL）在FSR中的应用。研究发现，尽管RAL在零样本识别中已有研究，但在FSR中仍面临新的挑战和机会。通过对检索数据的平衡分布和领域差距的分析，提出了分阶段检索增强微调（SWAT）的策略，该策略在九个流行基准测试上的表现显著优于以前的方法，提高了超过6%的准确率。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>少样本识别（FSR）旨在使用下游任务相关概念的少量标注样本训练分类模型，降低数据标注成本。</li>
<li>借助预训练的视觉语言模型（VLM），通过检索增强学习（RAL）解决FSR问题。</li>
<li>观察到将RAL应用于FSR时存在新的挑战和机会。</li>
<li>单纯在大量检索数据上微调VLM的性能不如零样本方法，原因在于检索数据的分布不平衡及其与下游任务中的少样本示例之间存在领域差异。</li>
<li>仅在少样本示例上微调VLM的性能优于以往的FSR方法，而在混合检索数据和少样本数据上进行微调则效果更佳。</li>
<li>提出分阶段检索增强微调（SWAT）策略，通过两个阶段的端对端微调与分类器重训练来解决数据分布和领域差异问题。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2406.11148">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-51b85e0d40a4eb07d901814cc027615d.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-44279977f6e3b79bce408ded4862ed2f.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-1d5d995207a0d13c53199e9184036d87.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-79d47593499b9af560ffbe5cf73d11d9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-088a583a37a4d4c48cdb1b6a1f6df802.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="Socratic-Planner-Self-QA-Based-Zero-Shot-Planning-for-Embodied-Instruction-Following"><a href="#Socratic-Planner-Self-QA-Based-Zero-Shot-Planning-for-Embodied-Instruction-Following" class="headerlink" title="Socratic Planner: Self-QA-Based Zero-Shot Planning for Embodied   Instruction Following"></a>Socratic Planner: Self-QA-Based Zero-Shot Planning for Embodied   Instruction Following</h2><p><strong>Authors:Suyeon Shin, Sujin jeon, Junghyun Kim, Gi-Cheon Kang, Byoung-Tak Zhang</strong></p>
<p>Embodied Instruction Following (EIF) is the task of executing natural language instructions by navigating and interacting with objects in interactive environments. A key challenge in EIF is compositional task planning, typically addressed through supervised learning or few-shot in-context learning with labeled data. To this end, we introduce the Socratic Planner, a self-QA-based zero-shot planning method that infers an appropriate plan without any further training. The Socratic Planner first facilitates self-questioning and answering by the Large Language Model (LLM), which in turn helps generate a sequence of subgoals. While executing the subgoals, an embodied agent may encounter unexpected situations, such as unforeseen obstacles. The Socratic Planner then adjusts plans based on dense visual feedback through a visually-grounded re-planning mechanism. Experiments demonstrate the effectiveness of the Socratic Planner, outperforming current state-of-the-art planning models on the ALFRED benchmark across all metrics, particularly excelling in long-horizon tasks that demand complex inference. We further demonstrate its real-world applicability through deployment on a physical robot for long-horizon tasks. </p>
<blockquote>
<p>以具身为导向（Embodied Instruction Following，EIF）的任务是通过在交互环境中导航和与对象交互来执行自然语言指令。EIF的关键挑战在于组合任务规划，通常通过监督学习或利用标注数据的少量上下文学习来解决。为此，我们引入了基于自我问答的零样本规划方法苏格拉底规划器（Socratic Planner），它可以在无需进一步训练的情况下推断出适当的计划。苏格拉底规划器首先通过大型语言模型（LLM）促进自我提问和回答，进而帮助生成一系列子目标。在执行子目标时，实体代理可能会遇到意外情况，例如未预见的障碍。然后，苏格拉底规划器会通过视觉基础的重新规划机制，根据密集的视觉反馈调整计划。实验表明，苏格拉底规划器非常有效，在ALFRED基准测试中优于当前最先进的规划模型，在所有指标上都表现出色，特别是在需要复杂推理的长周期任务中表现尤为出色。我们进一步通过在实际机器人上进行部署来展示其在现实世界中的适用性，以处理长周期任务。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2404.15190v2">PDF</a> 8 pages, 6 figures, published to ICRA 2025</p>
<p><strong>Summary</strong><br>在自然语言指令执行的环境中，通过问答式规划机制来规划行动。提出了一种零训练的方式制定行动计划并执行导航等任务，完成人机交互场景下的指令执行任务。利用大型语言模型进行自问自答来生成一系列子目标序列，根据视觉反馈进行计划调整。实验表明该规划机制有效超越了当前最优的规划模型。其在机器人领域的复杂推理长周期任务具有潜在应用价值。</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>EIF的任务是执行自然语言指令并操作环境中的物体。它主要面对的问题之一是构成式任务规划，通常用标签数据来完成的任务是通过监督学习或小规模的实例学习完成的。该文章则采用了不同的策略解决这一问题。 </li>
<li>文章中引入了名为“苏格拉底式规划者”的自我问答式的规划方式来实现零样本的训练计划的制定。其首先由大型语言模型（LLM）自行产生问题和答案，生成一系列子目标序列。 </li>
<li>在执行任务过程中，遇到意外情况（如障碍物），苏格拉底式规划者会根据视觉反馈来调整其规划的策略和流程，从而达到自我纠正和改进的效果。这一特点是它在规划方面的优点之一。 </li>
<li>苏格拉底式规划者的优势在于其在ALFRED基准测试上的表现优于当前最优的规划模型，特别是在需要复杂推理的长周期任务上表现尤为出色。 </li>
<li>最后，文章展示了苏格拉底式规划者在现实世界的机器人任务中的适用性，证明了其在实际应用中的价值。 </li>
<li>苏格拉底式规划者具有处理未知障碍的能力，使得其在执行任务时更加灵活和适应环境变化的能力更强。</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2404.15190">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-aed09dceacbeba30e005102791cf2e51.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-3ad71a38d0043d65cf320efb127bfbc4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f161ec60c2a98b126a65f3573f8a637e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ab1d4affde29392a1d0d63f38202879e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a255a2a490420620abc60def3dc874ff.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0499f2a3e8a2f5b5f50187b6c214573d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-80b16fab23e6e66590a8c741b45a9f31.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-09b194be49d089e748eebdac3679a125.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0e5aaf11a669b863ebf91ec5e26e5b41.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        文章作者:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        文章链接:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-03-28/Few-Shot/">https://kedreamix.github.io/Talk2Paper/Paper/2025-03-28/Few-Shot/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        版权声明:
                    </i>
                </span>
                <span class="reprint-info">
                    本博客所有文章除特別声明外，均采用
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    许可协议。转载请注明来源
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Few-Shot/">
                                    <span class="chip bg-color">Few-Shot</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;上一篇</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-03-28/I2I%20Translation/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-2567334efa197ea9ec8efd0c31a1c361.jpg" class="responsive-img" alt="I2I Translation">
                        
                        <span class="card-title">I2I Translation</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            I2I Translation 方向最新论文已更新，请持续关注 Update in 2025-03-28  Scale-Equivariant Imaging Self-Supervised Learning for Image   Super-Resolution and Deblurring
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-03-28
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/I2I-Translation/" class="post-category">
                                    I2I Translation
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/I2I-Translation/">
                        <span class="chip bg-color">I2I Translation</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                下一篇&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-03-28/Agent/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-ab30b0eba029d65e0ca1119bd6e746ce.jpg" class="responsive-img" alt="Agent">
                        
                        <span class="card-title">Agent</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Agent 方向最新论文已更新，请持续关注 Update in 2025-03-28  Feature4X Bridging Any Monocular Video to 4D Agentic AI with Versatile   Gaussian Feature Fields
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-03-28
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Agent/" class="post-category">
                                    Agent
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Agent/">
                        <span class="chip bg-color">Agent</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- 代码块功能依赖 -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- 代码语言 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- 代码块复制 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- 代码块收缩 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC 悬浮按钮. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* 修复文章卡片 div 的宽度. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // 切换TOC目录展开收缩的相关操作.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;站点总字数:&nbsp;<span
                        class="white-color">23251k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- 运行天数提醒. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // 区分是否有年份.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = '本站已运行 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                daysTip = '本站已運行 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = '本站已运行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = '本站已運行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="关注我的知乎: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">知</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- 搜索遮罩框 -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span>
            <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- 白天和黑夜主题 -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- 回到顶部按钮 -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // 只在桌面版网页启用特效
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- 雪花特效 -->
    

    <!-- 鼠标星星特效 -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--腾讯兔小巢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- 动态标签 -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Σ(っ °Д °;)っ诶，页面崩溃了嘛？", clearTimeout(st)) : (document.title = "φ(゜▽゜*)♪咦，又好了！", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
