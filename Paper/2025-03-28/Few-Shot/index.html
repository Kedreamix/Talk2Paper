<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Few-Shot">
    <meta name="description" content="Few-Shot æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-03-28  Reason-RFT Reinforcement Fine-Tuning for Visual Reasoning">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Few-Shot | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pic1.zhimg.com/v2-b676d91c12009bfda247722bff30df3e.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Few-Shot</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Few-Shot/">
                                <span class="chip bg-color">Few-Shot</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Few-Shot/" class="post-category">
                                Few-Shot
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-03-28
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-05-14
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    12k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    48 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-03-28-æ›´æ–°"><a href="#2025-03-28-æ›´æ–°" class="headerlink" title="2025-03-28 æ›´æ–°"></a>2025-03-28 æ›´æ–°</h1><h2 id="Reason-RFT-Reinforcement-Fine-Tuning-for-Visual-Reasoning"><a href="#Reason-RFT-Reinforcement-Fine-Tuning-for-Visual-Reasoning" class="headerlink" title="Reason-RFT: Reinforcement Fine-Tuning for Visual Reasoning"></a>Reason-RFT: Reinforcement Fine-Tuning for Visual Reasoning</h2><p><strong>Authors:Huajie Tan, Yuheng Ji, Xiaoshuai Hao, Minglan Lin, Pengwei Wang, Zhongyuan Wang, Shanghang Zhang</strong></p>
<p>Visual reasoning abilities play a crucial role in understanding complex multimodal data, advancing both domain-specific applications and artificial general intelligence (AGI). Existing methods improve VLM reasoning via Chain-of-Thought (CoT) supervised fine-tuning, using meticulously annotated training data to enhance visual reasoning capabilities. However, this training paradigm may lead to overfitting and cognitive rigidity, restricting the modelâ€™s ability to transfer visual reasoning skills across domains and limiting its real-world applicability. To address these limitations, we propose Reason-RFT, a novel reinforcement fine-tuning framework that significantly enhances generalization capabilities in visual reasoning tasks. Reason-RFT introduces a two-phase training framework for visual reasoning: (1) Supervised Fine-Tuning (SFT) with curated Chain-of-Thought (CoT) data activates the reasoning potential of Vision-Language Models (VLMs), followed by (2) Group Relative Policy Optimization (GRPO)-based reinforcement learning that generates multiple reasoning-response pairs, significantly enhancing generalization in visual reasoning tasks. To evaluate Reason-RFTâ€™s visual reasoning capabilities, we reconstructed a comprehensive dataset spanning visual counting, structure perception, and spatial transformation.cExperimental results demonstrate Reasoning-RFTâ€™s three key advantages: (1) Performance Enhancement: achieving state-of-the-art results across multiple tasks, outperforming most mainstream open-source and proprietary models; (2) Generalization Superiority: consistently maintaining robust performance across diverse tasks and domains, outperforming alternative training paradigms; (3) Data Efficiency: excelling in few-shot learning scenarios while surpassing full-dataset SFT baselines. </p>
<blockquote>
<p>è§†è§‰æ¨ç†èƒ½åŠ›åœ¨ç†è§£å¤æ‚çš„å¤šæ¨¡å¼æ•°æ®ã€æ¨åŠ¨é¢†åŸŸç‰¹å®šåº”ç”¨å’Œäººå·¥é€šç”¨æ™ºèƒ½ï¼ˆAGIï¼‰æ–¹é¢å‘æŒ¥ç€è‡³å…³é‡è¦çš„ä½œç”¨ã€‚ç°æœ‰æ–¹æ³•é€šè¿‡åŸºäºæ€ç»´é“¾ï¼ˆCoTï¼‰çš„ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰æ”¹è¿›VLMæ¨ç†ï¼Œä½¿ç”¨ç²¾å¿ƒæ³¨é‡Šçš„è®­ç»ƒæ•°æ®ä»¥å¢å¼ºè§†è§‰æ¨ç†èƒ½åŠ›ã€‚ç„¶è€Œï¼Œè¿™ç§è®­ç»ƒèŒƒå¼å¯èƒ½å¯¼è‡´è¿‡æ‹Ÿåˆå’Œè®¤çŸ¥åƒµåŒ–ï¼Œé™åˆ¶æ¨¡å‹åœ¨ä¸åŒé¢†åŸŸè½¬ç§»è§†è§‰æ¨ç†æŠ€èƒ½çš„èƒ½åŠ›ï¼Œå¹¶é™åˆ¶å…¶åœ¨ç°å®ä¸–ç•Œä¸­çš„åº”ç”¨ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†Reason-RFTï¼Œè¿™æ˜¯ä¸€ä¸ªæ–°çš„å¼ºåŒ–å¾®è°ƒæ¡†æ¶ï¼Œæ˜¾è‘—æé«˜äº†è§†è§‰æ¨ç†ä»»åŠ¡çš„æ³›åŒ–èƒ½åŠ›ã€‚Reason-RFTä¸ºè§†è§‰æ¨ç†å¼•å…¥äº†ä¸€ä¸ªä¸¤é˜¶æ®µè®­ç»ƒæ¡†æ¶ï¼šé¦–å…ˆä½¿ç”¨ç²¾é€‰çš„æ€ç»´é“¾ï¼ˆCoTï¼‰æ•°æ®è¿›è¡Œç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰ï¼Œæ¿€æ´»è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰çš„æ¨ç†æ½œåŠ›ï¼›å…¶æ¬¡æ˜¯åŸºäºç¾¤ä½“ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆGRPOï¼‰çš„å¼ºåŒ–å­¦ä¹ ç”Ÿæˆå¤šä¸ªæ¨ç†å“åº”å¯¹ï¼Œæ˜¾è‘—æé«˜è§†è§‰æ¨ç†ä»»åŠ¡çš„æ³›åŒ–èƒ½åŠ›ã€‚ä¸ºäº†è¯„ä¼°Reason-RFTçš„è§†è§‰æ¨ç†èƒ½åŠ›ï¼Œæˆ‘ä»¬é‡å»ºäº†ä¸€ä¸ªå…¨é¢çš„æ•°æ®é›†ï¼Œæ¶µç›–è§†è§‰è®¡æ•°ã€ç»“æ„æ„ŸçŸ¥å’Œç©ºé—´å˜æ¢ç­‰é¢†åŸŸã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒReasoning-RFTå…·æœ‰ä¸‰å¤§å…³é”®ä¼˜åŠ¿ï¼šï¼ˆ1ï¼‰æ€§èƒ½æå‡ï¼šåœ¨å¤šä¸ªä»»åŠ¡ä¸Šå®ç°å“è¶Šçš„ç»“æœï¼Œä¼˜äºå¤§å¤šæ•°ä¸»æµå¼€æºå’Œä¸“æœ‰æ¨¡å‹ï¼›ï¼ˆ2ï¼‰æ³›åŒ–ä¼˜åŠ¿ï¼šåœ¨å„ç§ä»»åŠ¡å’Œé¢†åŸŸä¸Šå§‹ç»ˆä¿æŒè‰¯å¥½çš„æ€§èƒ½ï¼Œä¼˜äºå…¶ä»–è®­ç»ƒèŒƒå¼ï¼›ï¼ˆ3ï¼‰æ•°æ®æ•ˆç‡ï¼šåœ¨å°‘é‡å­¦ä¹ åœºæ™¯ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œè¶…è¶Šäº†å…¨æ•°æ®é›†SFTåŸºå‡†æµ‹è¯•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.20752v1">PDF</a> 35 pages, 22 figures</p>
<p><strong>Summary</strong></p>
<p>è§†è§‰æ¨ç†èƒ½åŠ›åœ¨å¤šæ¨¡æ€æ•°æ®ç†è§£ä¸­æ‰®æ¼”ç€é‡è¦è§’è‰²ï¼Œæ¨åŠ¨ç€é¢†åŸŸç‰¹å®šåº”ç”¨å’Œé€šç”¨äººå·¥æ™ºèƒ½çš„å‘å±•ã€‚ç°æœ‰æ–¹æ³•é€šè¿‡é“¾å¼æ€ç»´ç›‘ç£å¾®è°ƒæ¥æå‡è§†è§‰æ¨ç†èƒ½åŠ›ï¼Œä½†è¿™å¯èƒ½å¯¼è‡´è¿‡æ‹Ÿåˆå’Œè®¤çŸ¥åƒµåŒ–ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†Reason-RFTï¼Œä¸€ä¸ªå¢å¼ºè§†è§‰æ¨ç†ä»»åŠ¡æ³›åŒ–èƒ½åŠ›çš„æ–°å‹å¼ºåŒ–å¾®è°ƒæ¡†æ¶ã€‚è¯¥æ¡†æ¶åŒ…æ‹¬ä¸¤ä¸ªé˜¶æ®µï¼šç¬¬ä¸€é˜¶æ®µæ˜¯æ¿€æ´»è§†è§‰è¯­è¨€æ¨¡å‹çš„æ¨ç†æ½œåŠ›ï¼›ç¬¬äºŒé˜¶æ®µæ˜¯åŸºäºç¾¤ä½“ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–çš„å¼ºåŒ–å­¦ä¹ ï¼Œç”Ÿæˆå¤šä¸ªæ¨ç†å“åº”å¯¹ï¼Œä»è€Œæ˜¾è‘—å¢å¼ºè§†è§‰æ¨ç†ä»»åŠ¡çš„æ³›åŒ–èƒ½åŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è§†è§‰æ¨ç†åœ¨ç†è§£å¤æ‚å¤šæ¨¡æ€æ•°æ®å’Œæ¨åŠ¨é¢†åŸŸç‰¹å®šåº”ç”¨åŠé€šç”¨äººå·¥æ™ºèƒ½å‘å±•æ–¹é¢èµ·å…³é”®ä½œç”¨ã€‚</li>
<li>ç°æœ‰æ–¹æ³•é€šè¿‡é“¾å¼æ€ç»´ç›‘ç£å¾®è°ƒæå‡è§†è§‰æ¨ç†èƒ½åŠ›ï¼Œä½†å¯èƒ½å¯¼è‡´è¿‡æ‹Ÿåˆå’Œè®¤çŸ¥åƒµåŒ–ã€‚</li>
<li>Reason-RFTæ¡†æ¶æ—¨åœ¨å¢å¼ºè§†è§‰æ¨ç†ä»»åŠ¡çš„æ³›åŒ–èƒ½åŠ›ï¼ŒåŒ…æ‹¬ä¸¤ä¸ªé˜¶æ®µï¼šæ¿€æ´»è§†è§‰è¯­è¨€æ¨¡å‹çš„æ¨ç†æ½œåŠ›å’ŒåŸºäºå¼ºåŒ–å­¦ä¹ çš„ä¼˜åŒ–ã€‚</li>
<li>è¯¥æ¡†æ¶é€šè¿‡ç”Ÿæˆå¤šä¸ªæ¨ç†å“åº”å¯¹æ¥æ˜¾è‘—æé«˜è§†è§‰æ¨ç†ä»»åŠ¡çš„æ€§èƒ½ã€‚</li>
<li>å®éªŒç»“æœè¡¨æ˜ï¼ŒReason-RFTåœ¨å¤šä¸ªä»»åŠ¡ä¸Šå–å¾—æœ€å…ˆè¿›çš„æˆæœï¼Œå¹¶è¡¨ç°å‡ºå“è¶Šçš„æ€§èƒ½ã€æ³›åŒ–èƒ½åŠ›å’Œæ•°æ®æ•ˆç‡ã€‚</li>
<li>Reason-RFTæ¡†æ¶åœ¨å¤šç§ä»»åŠ¡å’Œæ•°æ®é›†ä¸Šçš„ç¨³å¥æ€§èƒ½éªŒè¯äº†å…¶ä¼˜è¶Šæ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.20752">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-e0bfea3ced615b6f5e7a485d4761ee43.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-6ff52be7618643b781f0b047988a2a27.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-06b95518386236ce42fcfb65735cb601.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Synthetic-Data-Augmentation-for-Cross-domain-Implicit-Discourse-Relation-Recognition"><a href="#Synthetic-Data-Augmentation-for-Cross-domain-Implicit-Discourse-Relation-Recognition" class="headerlink" title="Synthetic Data Augmentation for Cross-domain Implicit Discourse Relation   Recognition"></a>Synthetic Data Augmentation for Cross-domain Implicit Discourse Relation   Recognition</h2><p><strong>Authors:Frances Yung, Varsha Suresh, Zaynab Reza, Mansoor Ahmad, Vera Demberg</strong></p>
<p>Implicit discourse relation recognition (IDRR) â€“ the task of identifying the implicit coherence relation between two text spans â€“ requires deep semantic understanding. Recent studies have shown that zero- or few-shot approaches significantly lag behind supervised models, but LLMs may be useful for synthetic data augmentation, where LLMs generate a second argument following a specified coherence relation. We applied this approach in a cross-domain setting, generating discourse continuations using unlabelled target-domain data to adapt a base model which was trained on source-domain labelled data. Evaluations conducted on a large-scale test set revealed that different variations of the approach did not result in any significant improvements. We conclude that LLMs often fail to generate useful samples for IDRR, and emphasize the importance of considering both statistical significance and comparability when evaluating IDRR models. </p>
<blockquote>
<p>éšå¼æ–‡æœ¬å…³ç³»è¯†åˆ«ï¼ˆIDRRï¼‰â€”â€”è¯†åˆ«ä¸¤ä¸ªæ–‡æœ¬ç‰‡æ®µä¹‹é—´çš„éšå¼è¿è´¯å…³ç³»â€”â€”éœ€è¦æ·±å…¥è¯­ä¹‰ç†è§£ã€‚æœ€è¿‘çš„ç ”ç©¶è¡¨æ˜ï¼Œé›¶æ ·æœ¬æˆ–å°‘æ ·æœ¬æ–¹æ³•æ˜æ˜¾è½åäºç›‘ç£æ¨¡å‹ï¼Œä½†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å¯ç”¨äºåˆæˆæ•°æ®å¢å¼ºï¼Œå…¶ä¸­LLMsæ ¹æ®æŒ‡å®šçš„è¿è´¯å…³ç³»ç”Ÿæˆç¬¬äºŒä¸ªè®ºç‚¹ã€‚æˆ‘ä»¬åœ¨è·¨åŸŸç¯å¢ƒä¸­åº”ç”¨äº†è¿™ç§æ–¹æ³•ï¼Œä½¿ç”¨æœªæ ‡è®°çš„ç›®æ ‡åŸŸæ•°æ®ç”Ÿæˆæ–‡æœ¬å»¶ç»­ï¼Œä»¥é€‚åº”åœ¨æºåŸŸæ ‡è®°æ•°æ®ä¸Šè®­ç»ƒçš„åŸºå‡†æ¨¡å‹ã€‚åœ¨å¤§è§„æ¨¡æµ‹è¯•é›†ä¸Šè¿›è¡Œçš„è¯„ä¼°è¡¨æ˜ï¼Œè¯¥æ–¹æ³•çš„ä¸åŒå˜ä½“å¹¶æœªå¸¦æ¥ä»»ä½•æ˜¾è‘—æ”¹è¿›ã€‚æˆ‘ä»¬å¾—å‡ºç»“è®ºï¼Œå¤§å‹è¯­è¨€æ¨¡å‹åœ¨ä¸ºIDRRç”Ÿæˆæœ‰ç”¨æ ·æœ¬æ—¶ç»å¸¸å¤±è´¥ï¼Œå¹¶å¼ºè°ƒåœ¨è¯„ä¼°IDRRæ¨¡å‹æ—¶ï¼Œæ—¢è¦è€ƒè™‘ç»Ÿè®¡æ„ä¹‰ï¼Œä¹Ÿè¦è€ƒè™‘å¯æ¯”æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.20588v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ¢è®¨äº†éšå¼è¯è¯­å…³ç³»è¯†åˆ«ï¼ˆIDRRï¼‰ä»»åŠ¡ï¼Œè¯¥ä»»åŠ¡éœ€è¦æ·±åº¦è¯­ä¹‰ç†è§£ã€‚è™½ç„¶é›¶æ ·æœ¬æˆ–å°‘æ ·æœ¬æ–¹æ³•åœ¨è¿™æ–¹é¢è¡¨ç°ä¸ä½³ï¼Œä½†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å¯ç”¨äºåˆæˆæ•°æ®å¢å¼ºã€‚ç ”ç©¶å°è¯•åœ¨è·¨åŸŸç¯å¢ƒä¸­ä½¿ç”¨LLMsç”Ÿæˆä¸ç‰¹å®šè¿è´¯å…³ç³»ç›¸ç¬¦çš„ç¬¬äºŒè®ºç‚¹å’Œæ–‡æœ¬å»¶ç»­ï¼Œä»¥é€‚é…åŸºäºæºåŸŸæ ‡æ³¨æ•°æ®çš„åŸºå‡†æ¨¡å‹ã€‚ç„¶è€Œï¼Œå¤§è§„æ¨¡æµ‹è¯•é›†ä¸Šçš„è¯„ä¼°æ˜¾ç¤ºï¼Œä¸åŒæ–¹æ³•çš„åº”ç”¨å¹¶æœªå¸¦æ¥æ˜¾è‘—æ”¹è¿›ã€‚å› æ­¤ï¼Œç ”ç©¶è®¤ä¸ºLLMsåœ¨ç”ŸæˆIDRRæ ·æœ¬æ–¹é¢å¹¶ä¸æ€»æ˜¯æœ‰æ•ˆï¼Œå¹¶å¼ºè°ƒåœ¨è¯„ä¼°IDRRæ¨¡å‹æ—¶éœ€è¦è€ƒè™‘ç»Ÿè®¡æ˜¾è‘—æ€§å’Œå¯æ¯”æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>éšå¼è¯è¯­å…³ç³»è¯†åˆ«ï¼ˆIDRRï¼‰éœ€è¦æ·±åº¦è¯­ä¹‰ç†è§£ã€‚</li>
<li>é›¶æ ·æœ¬æˆ–å°‘æ ·æœ¬æ–¹æ³•åœ¨æ­¤ä»»åŠ¡ä¸Šçš„è¡¨ç°ä¸åŠç›‘ç£æ¨¡å‹ã€‚</li>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å¯ç”¨äºåˆæˆæ•°æ®å¢å¼ºï¼Œé€šè¿‡ç”Ÿæˆä¸ç‰¹å®šè¿è´¯å…³ç³»ç›¸ç¬¦çš„æ–‡æœ¬å»¶ç»­ã€‚</li>
<li>åœ¨è·¨åŸŸç¯å¢ƒä¸­ä½¿ç”¨LLMsç”Ÿæˆæ•°æ®ä»¥é€‚é…åŸºå‡†æ¨¡å‹æ˜¯ä¸€ä¸ªå°è¯•ã€‚</li>
<li>ä¸åŒæ–¹æ³•çš„åº”ç”¨åœ¨å¤§å‹æµ‹è¯•é›†ä¸Šå¹¶æœªå¸¦æ¥æ˜¾è‘—æ”¹è¿›ã€‚</li>
<li>LLMsåœ¨ç”ŸæˆIDRRæ ·æœ¬æ–¹é¢å¹¶ä¸æ€»æ˜¯æœ‰æ•ˆã€‚</li>
<li>åœ¨è¯„ä¼°IDRRæ¨¡å‹æ—¶ï¼Œéœ€è¦è€ƒè™‘ç»Ÿè®¡æ˜¾è‘—æ€§å’Œå¯æ¯”æ€§ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.20588">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-676ada67e54fb9a0ebc0ae06f8d3686b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-22718eac3fff926d1b44cd1f699bbb72.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-79111d5c2638ea1f6c6db11718cf0ae1.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Attribute-formed-Class-specific-Concept-Space-Endowing-Language-Bottleneck-Model-with-Better-Interpretability-and-Scalability"><a href="#Attribute-formed-Class-specific-Concept-Space-Endowing-Language-Bottleneck-Model-with-Better-Interpretability-and-Scalability" class="headerlink" title="Attribute-formed Class-specific Concept Space: Endowing Language   Bottleneck Model with Better Interpretability and Scalability"></a>Attribute-formed Class-specific Concept Space: Endowing Language   Bottleneck Model with Better Interpretability and Scalability</h2><p><strong>Authors:Jianyang Zhang, Qianli Luo, Guowu Yang, Wenjing Yang, Weide Liu, Guosheng Lin, Fengmao Lv</strong></p>
<p>Language Bottleneck Models (LBMs) are proposed to achieve interpretable image recognition by classifying images based on textual concept bottlenecks. However, current LBMs simply list all concepts together as the bottleneck layer, leading to the spurious cue inference problem and cannot generalized to unseen classes. To address these limitations, we propose the Attribute-formed Language Bottleneck Model (ALBM). ALBM organizes concepts in the attribute-formed class-specific space, where concepts are descriptions of specific attributes for specific classes. In this way, ALBM can avoid the spurious cue inference problem by classifying solely based on the essential concepts of each class. In addition, the cross-class unified attribute set also ensures that the concept spaces of different classes have strong correlations, as a result, the learned concept classifier can be easily generalized to unseen classes. Moreover, to further improve interpretability, we propose Visual Attribute Prompt Learning (VAPL) to extract visual features on fine-grained attributes. Furthermore, to avoid labor-intensive concept annotation, we propose the Description, Summary, and Supplement (DSS) strategy to automatically generate high-quality concept sets with a complete and precise attribute. Extensive experiments on 9 widely used few-shot benchmarks demonstrate the interpretability, transferability, and performance of our approach. The code and collected concept sets are available at <a target="_blank" rel="noopener" href="https://github.com/tiggers23/ALBM">https://github.com/tiggers23/ALBM</a>. </p>
<blockquote>
<p>è¯­è¨€ç“¶é¢ˆæ¨¡å‹ï¼ˆLBMsï¼‰æ—¨åœ¨é€šè¿‡åŸºäºæ–‡æœ¬æ¦‚å¿µç“¶é¢ˆçš„å›¾åƒåˆ†ç±»æ¥å®ç°å¯è§£é‡Šçš„å›¾åƒè¯†åˆ«ã€‚ç„¶è€Œï¼Œç›®å‰çš„LBMsåªæ˜¯å°†æ‰€æœ‰æ¦‚å¿µä¸€èµ·åˆ—ä¸ºç“¶é¢ˆå±‚ï¼Œè¿™å¯¼è‡´äº†è™šå‡çº¿ç´¢æ¨æ–­é—®é¢˜ï¼Œå¹¶ä¸”æ— æ³•æ¨å¹¿åˆ°æœªè§è¿‡çš„ç±»åˆ«ã€‚ä¸ºäº†è§£å†³è¿™äº›å±€é™æ€§ï¼Œæˆ‘ä»¬æå‡ºäº†å±æ€§å½¢æˆè¯­è¨€ç“¶é¢ˆæ¨¡å‹ï¼ˆALBMï¼‰ã€‚ALBMåœ¨å±æ€§å½¢æˆçš„ç±»åˆ«ç‰¹å®šç©ºé—´ä¸­ç»„ç»‡æ¦‚å¿µï¼Œå…¶ä¸­æ¦‚å¿µæ˜¯ç‰¹å®šç±»åˆ«çš„ç‰¹å®šå±æ€§çš„æè¿°ã€‚é€šè¿‡è¿™ç§æ–¹å¼ï¼ŒALBMå¯ä»¥ä»…åŸºäºæ¯ä¸ªç±»åˆ«çš„å…³é”®æ¦‚å¿µè¿›è¡Œåˆ†ç±»ï¼Œä»è€Œé¿å…è™šå‡çº¿ç´¢æ¨æ–­é—®é¢˜ã€‚æ­¤å¤–ï¼Œè·¨ç±»åˆ«ç»Ÿä¸€å±æ€§é›†ä¹Ÿç¡®ä¿äº†ä¸åŒç±»åˆ«çš„æ¦‚å¿µç©ºé—´å…·æœ‰å¼ºç›¸å…³æ€§ï¼Œå› æ­¤ï¼Œæ‰€å­¦çš„æ¦‚å¿µåˆ†ç±»å™¨å¯ä»¥å¾ˆå®¹æ˜“åœ°æ¨å¹¿åˆ°æœªè§è¿‡çš„ç±»åˆ«ã€‚è€Œä¸”ï¼Œä¸ºäº†è¿›ä¸€æ­¥æé«˜å¯è§£é‡Šæ€§ï¼Œæˆ‘ä»¬æå‡ºäº†è§†è§‰å±æ€§æç¤ºå­¦ä¹ ï¼ˆVAPLï¼‰æ¥æå–ç»†ç²’åº¦å±æ€§ä¸Šçš„è§†è§‰ç‰¹å¾ã€‚æ­¤å¤–ï¼Œä¸ºäº†é¿å…ç¹ççš„æ¦‚å¿µæ ‡æ³¨ï¼Œæˆ‘ä»¬æå‡ºäº†æè¿°ã€æ€»ç»“å’Œè¡¥å……ï¼ˆDSSï¼‰ç­–ç•¥ï¼Œä»¥è‡ªåŠ¨ç”Ÿæˆå…·æœ‰å®Œæ•´å’Œç²¾ç¡®å±æ€§çš„é«˜è´¨é‡æ¦‚å¿µé›†ã€‚åœ¨9ä¸ªå¹¿æ³›ä½¿ç”¨çš„å°‘æ ·æœ¬åŸºå‡†æµ‹è¯•ä¸Šçš„å¤§é‡å®éªŒè¯æ˜äº†æˆ‘ä»¬æ–¹æ³•çš„å¯è§£é‡Šæ€§ã€å¯è¿ç§»æ€§å’Œæ€§èƒ½ã€‚ä»£ç å’Œæ”¶é›†çš„æ¦‚å¿µé›†å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/tiggers23/ALBM%E4%B8%AD%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/tiggers23/ALBMä¸­æ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.20301v1">PDF</a> This paper has been accepted to CVPR 2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºå±æ€§åˆ†ç±»çš„è¯­è¨€ç“¶é¢ˆæ¨¡å‹ï¼ˆALBMï¼‰ï¼Œè§£å†³äº†ç°æœ‰å›¾åƒè¯†åˆ«æ¨¡å‹çš„å±€é™æ€§é—®é¢˜ã€‚ALBMé€šè¿‡æ„å»ºå±æ€§å½¢å¼çš„ç±»ç‰¹å®šç©ºé—´æ¥ç»„ç»‡æ¦‚å¿µï¼Œé¿å…äº†åŸºäºéå…³é”®æ¦‚å¿µçš„è¯¯åˆ¤é—®é¢˜ï¼Œå¹¶èƒ½å°†æ¦‚å¿µåˆ†ç±»å™¨æ¨å¹¿åˆ°æœªè§ç±»åˆ«ã€‚åŒæ—¶ï¼Œé€šè¿‡è§†è§‰å±æ€§æç¤ºå­¦ä¹ ï¼ˆVAPLï¼‰æé«˜æ¨¡å‹çš„è§£é‡Šæ€§ï¼Œå¹¶é‡‡ç”¨æè¿°ã€æ‘˜è¦å’Œè¡¥å……ï¼ˆDSSï¼‰ç­–ç•¥è‡ªåŠ¨ç”Ÿæˆé«˜è´¨é‡çš„æ¦‚å¿µé›†ã€‚å®éªŒè¯æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å°‘æ•°æ ·æœ¬åœºæ™¯ä¸‹å…·æœ‰è‰¯å¥½çš„è§£é‡Šæ€§ã€è¿ç§»æ€§å’Œæ€§èƒ½è¡¨ç°ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>ALBMè§£å†³äº†ç°æœ‰å›¾åƒè¯†åˆ«æ¨¡å‹å› æ¦‚å¿µç“¶é¢ˆå±‚è®¾è®¡ä¸å½“å¯¼è‡´çš„è¯¯åˆ¤é—®é¢˜ã€‚</li>
<li>ALBMæ„å»ºå±æ€§å½¢å¼çš„ç±»ç‰¹å®šç©ºé—´æ¥ç»„ç»‡æ¦‚å¿µï¼Œèƒ½å¤ŸåŸºäºæ¯ä¸ªç±»çš„å…³é”®æ¦‚å¿µè¿›è¡Œåˆ†ç±»ï¼Œé¿å…äº†éå…³é”®å› ç´ çš„å¹²æ‰°ã€‚</li>
<li>VAPLæŠ€æœ¯ç”¨äºæå–ç²¾ç»†ç²’åº¦ä¸Šçš„è§†è§‰ç‰¹å¾ï¼Œæé«˜äº†æ¨¡å‹çš„è§£é‡Šæ€§ã€‚</li>
<li>DSSç­–ç•¥èƒ½è‡ªåŠ¨ç”Ÿæˆé«˜è´¨é‡çš„æ¦‚å¿µé›†ï¼Œå®ç°æ¦‚å¿µè‡ªåŠ¨æ ‡æ³¨ï¼Œé™ä½äº†åŠ³åŠ¨å¼ºåº¦ã€‚</li>
<li>å®éªŒç»“æœè¡¨æ˜ï¼ŒALBMåœ¨å°‘æ•°æ ·æœ¬åœºæ™¯ä¸‹å…·æœ‰è‰¯å¥½çš„è§£é‡Šæ€§ã€è¿ç§»æ€§å’Œæ€§èƒ½è¡¨ç°ã€‚</li>
<li>ALBMé€šè¿‡æ„å»ºè·¨ç±»åˆ«ç»Ÿä¸€å±æ€§é›†ï¼Œç¡®ä¿äº†ä¸åŒç±»åˆ«æ¦‚å¿µç©ºé—´çš„å¼ºå…³è”æ€§ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.20301">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-dc0371183d1f1b6f3e067ea6dcca2e39.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6c3c764f8b644e299095f2de326ebcfd.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ef9440f4defe3a6ccfa7be1d35700880.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-8a0a9b09bab40508eca025ecce2bf25c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-52356e1b98e4affcb6248ee8e9ba7ebc.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="LogicQA-Logical-Anomaly-Detection-with-Vision-Language-Model-Generated-Questions"><a href="#LogicQA-Logical-Anomaly-Detection-with-Vision-Language-Model-Generated-Questions" class="headerlink" title="LogicQA: Logical Anomaly Detection with Vision Language Model Generated   Questions"></a>LogicQA: Logical Anomaly Detection with Vision Language Model Generated   Questions</h2><p><strong>Authors:Yejin Kwon, Daeun Moon, Youngje Oh, Hyunsoo Yoon</strong></p>
<p>Anomaly Detection (AD) focuses on detecting samples that differ from the standard pattern, making it a vital tool in process control. Logical anomalies may appear visually normal yet violate predefined constraints on object presence, arrangement, or quantity, depending on reasoning and explainability. We introduce LogicQA, a framework that enhances AD by providing industrial operators with explanations for logical anomalies. LogicQA compiles automatically generated questions into a checklist and collects responses to identify violations of logical constraints. LogicQA is training-free, annotation-free, and operates in a few-shot setting. We achieve state-of-the-art (SOTA) Logical AD performance on public benchmarks, MVTec LOCO AD, with an AUROC of 87.6 percent and an F1-max of 87.0 percent along with the explanations of anomalies. Also, our approach has shown outstanding performance on semiconductor SEM corporate data, further validating its effectiveness in industrial applications. </p>
<blockquote>
<p>å¼‚å¸¸æ£€æµ‹ï¼ˆADï¼‰ä¸“æ³¨äºæ£€æµ‹ä¸æ ‡å‡†æ¨¡å¼ä¸åŒçš„æ ·æœ¬ï¼Œä½¿å…¶æˆä¸ºè¿‡ç¨‹æ§åˆ¶ä¸­çš„å…³é”®å·¥å…·ã€‚é€»è¾‘å¼‚å¸¸åœ¨è§†è§‰ä¸Šå¯èƒ½çœ‹ä¼¼æ­£å¸¸ï¼Œä½†ä¼šè¿åå¯¹è±¡å­˜åœ¨ã€æ’åˆ—æˆ–æ•°é‡ç­‰æ–¹é¢çš„é¢„å…ˆå®šä¹‰çº¦æŸï¼Œè¿™äº›çº¦æŸå–å†³äºæ¨ç†å’Œå¯è§£é‡Šæ€§ã€‚æˆ‘ä»¬å¼•å…¥äº†LogicQAæ¡†æ¶ï¼Œå®ƒé€šè¿‡ä¸ºå·¥ä¸šæ“ä½œå‘˜æä¾›é€»è¾‘å¼‚å¸¸çš„è§£é‡Šæ¥å¢å¼ºADçš„åŠŸèƒ½ã€‚LogicQAå°†è‡ªåŠ¨ç”Ÿæˆçš„é—®é¢˜ç¼–è¯‘æˆæ¸…å•ï¼Œå¹¶æ”¶é›†å›åº”æ¥è¯†åˆ«é€»è¾‘çº¦æŸçš„è¿åæƒ…å†µã€‚LogicQAæ— éœ€è®­ç»ƒï¼Œæ— éœ€æ ‡æ³¨ï¼Œå¹¶ä¸”åœ¨å°æ ·æœ¬è®¾ç½®ä¸‹è¿è¡Œã€‚æˆ‘ä»¬åœ¨å…¬å…±åŸºå‡†æµ‹è¯•MVTec LOCO ADä¸Šå®ç°äº†æœ€å…ˆè¿›çš„é€»è¾‘ADæ€§èƒ½ï¼Œå…·æœ‰87.6ï¼…çš„AUROCå’Œ87.0ï¼…çš„F1-maxï¼Œå¹¶æä¾›äº†å¼‚å¸¸çš„è§£é‡Šã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨åŠå¯¼ä½“SEMä¼ä¸šæ•°æ®ä¸Šä¹Ÿè¡¨ç°å‡ºäº†å‡ºè‰²çš„æ€§èƒ½ï¼Œè¿›ä¸€æ­¥éªŒè¯äº†å…¶åœ¨å·¥ä¸šåº”ç”¨ä¸­çš„æœ‰æ•ˆæ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.20252v1">PDF</a> </p>
<p><strong>Summary</strong><br>é€»è¾‘å¼‚å¸¸æ£€æµ‹ï¼ˆADï¼‰å¯¹äºå‘ç°ä¸æ ‡å‡†æ¨¡å¼ä¸åŒçš„æ ·æœ¬è‡³å…³é‡è¦ï¼Œæ˜¯è¿‡ç¨‹æ§åˆ¶ä¸­çš„å…³é”®å·¥å…·ã€‚æˆ‘ä»¬å¼•å…¥äº†LogicQAæ¡†æ¶ï¼Œå®ƒé€šè¿‡æä¾›å¯¹é€»è¾‘å¼‚å¸¸çš„è§£é‡Šæ¥å¢å¼ºADã€‚LogicQAè‡ªåŠ¨ç”Ÿæˆé—®é¢˜æ¸…å•å¹¶æ”¶é›†ç­”æ¡ˆï¼Œä»¥è¯†åˆ«é€»è¾‘çº¦æŸçš„è¿åæƒ…å†µã€‚è¯¥æ¡†æ¶æ— éœ€è®­ç»ƒå’Œæ ‡æ³¨ï¼Œé€‚ç”¨äºå°æ ·æœ¬ç¯å¢ƒã€‚åœ¨å…¬å…±åŸºå‡†æµ‹è¯•MVTec LOCO ADä¸Šï¼Œæˆ‘ä»¬å®ç°äº†æœ€å…ˆè¿›çš„é€»è¾‘ADæ€§èƒ½ï¼Œå…¶ä¸­AUROCä¸º87.6%ï¼ŒF1-maxä¸º87.0%ï¼Œå¹¶æä¾›äº†å¼‚å¸¸è§£é‡Šã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨åŠå¯¼ä½“çš„SEMä¼ä¸šæ•°æ®ä¸Šè¡¨ç°å‡ºå“è¶Šçš„æ€§èƒ½ï¼Œè¿›ä¸€æ­¥è¯æ˜äº†å®ƒåœ¨å·¥ä¸šåº”ç”¨ä¸­çš„æœ‰æ•ˆæ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>é€»è¾‘å¼‚å¸¸æ£€æµ‹ï¼ˆADï¼‰åœ¨è¿‡ç¨‹æ§åˆ¶ä¸­å…·æœ‰å…³é”®ä½œç”¨ï¼Œèƒ½å¤Ÿè¯†åˆ«ä¸æ ‡å‡†æ¨¡å¼ä¸åŒçš„æ ·æœ¬ã€‚</li>
<li>LogicQAæ¡†æ¶é€šè¿‡æä¾›å¯¹é€»è¾‘å¼‚å¸¸çš„è§£é‡Šæ¥å¢å¼ºADã€‚</li>
<li>LogicQAè‡ªåŠ¨ç”Ÿæˆé—®é¢˜æ¸…å•å¹¶æ”¶é›†ç­”æ¡ˆä»¥è¯†åˆ«é€»è¾‘çº¦æŸçš„è¿åæƒ…å†µã€‚</li>
<li>LogicQAæ¡†æ¶é€‚ç”¨äºå°æ ·æœ¬ç¯å¢ƒï¼Œæ— éœ€è®­ç»ƒå’Œæ ‡æ³¨ã€‚</li>
<li>åœ¨å…¬å…±åŸºå‡†æµ‹è¯•MVTec LOCO ADä¸Šï¼ŒLogicQAå®ç°äº†æœ€å…ˆè¿›çš„é€»è¾‘ADæ€§èƒ½ã€‚</li>
<li>LogicQAçš„AUROCå’ŒF1-maxæŒ‡æ ‡åˆ†åˆ«è¾¾åˆ°äº†87.6%å’Œ87.0%ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.20252">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-0337956edf84f0b02a3fe97e60f2350f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5a84f1eadb30a499a5e68d4dae8a043b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b35b3e797d30a70be5b06e152c338dcf.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b9069c06875e78034da4f7c6c37b67de.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-09ddcb731fe555b7ecf8c0e12a1accf9.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-abeaab0cecbcc26b32aeea9ae6b8e577.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="DINeMo-Learning-Neural-Mesh-Models-with-no-3D-Annotations"><a href="#DINeMo-Learning-Neural-Mesh-Models-with-no-3D-Annotations" class="headerlink" title="DINeMo: Learning Neural Mesh Models with no 3D Annotations"></a>DINeMo: Learning Neural Mesh Models with no 3D Annotations</h2><p><strong>Authors:Weijie Guo, Guofeng Zhang, Wufei Ma, Alan Yuille</strong></p>
<p>Category-level 3D&#x2F;6D pose estimation is a crucial step towards comprehensive 3D scene understanding, which would enable a broad range of applications in robotics and embodied AI. Recent works explored neural mesh models that approach a range of 2D and 3D tasks from an analysis-by-synthesis perspective. Despite the largely enhanced robustness to partial occlusion and domain shifts, these methods depended heavily on 3D annotations for part-contrastive learning, which confines them to a narrow set of categories and hinders efficient scaling. In this work, we present DINeMo, a novel neural mesh model that is trained with no 3D annotations by leveraging pseudo-correspondence obtained from large visual foundation models. We adopt a bidirectional pseudo-correspondence generation method, which produce pseudo correspondence utilize both local appearance features and global context information. Experimental results on car datasets demonstrate that our DINeMo outperforms previous zero- and few-shot 3D pose estimation by a wide margin, narrowing the gap with fully-supervised methods by 67.3%. Our DINeMo also scales effectively and efficiently when incorporating more unlabeled images during training, which demonstrate the advantages over supervised learning methods that rely on 3D annotations. Our project page is available at <a target="_blank" rel="noopener" href="https://analysis-by-synthesis.github.io/DINeMo/">https://analysis-by-synthesis.github.io/DINeMo/</a>. </p>
<blockquote>
<p>ç±»åˆ«çº§åˆ«çš„3D&#x2F;6Då§¿æ€ä¼°è®¡æ˜¯å®ç°å…¨é¢3Dåœºæ™¯ç†è§£çš„å…³é”®æ­¥éª¤ï¼Œè¿™å°†ä¸ºæœºå™¨äººæŠ€æœ¯å’ŒåµŒå…¥å¼äººå·¥æ™ºèƒ½çš„å¹¿æ³›åº”ç”¨æä¾›å¯èƒ½ã€‚è¿‘æœŸçš„ç ”ç©¶æ¢ç´¢äº†ç¥ç»ç½‘æ ¼æ¨¡å‹ï¼Œä»ç»¼åˆåˆ†æçš„è§’åº¦æ¥è§£å†³ä¸€ç³»åˆ—2Då’Œ3Dä»»åŠ¡ã€‚å°½ç®¡è¿™äº›æ–¹æ³•åœ¨å¾ˆå¤§ç¨‹åº¦ä¸Šå¢å¼ºäº†å¤„ç†éƒ¨åˆ†é®æŒ¡å’Œé¢†åŸŸåç§»çš„ç¨³å¥æ€§ï¼Œä½†å®ƒä»¬ä¸¥é‡ä¾èµ–äº3Dæ³¨é‡Šè¿›è¡Œéƒ¨åˆ†å¯¹æ¯”å­¦ä¹ ï¼Œè¿™é™åˆ¶äº†å®ƒä»¬çš„åº”ç”¨èŒƒå›´å¹¶é˜»ç¢äº†æœ‰æ•ˆæ‰©å±•ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†DINeMoï¼Œè¿™æ˜¯ä¸€ç§æ–°å‹ç¥ç»ç½‘æ ¼æ¨¡å‹ï¼Œå®ƒé€šè¿‡ä»å¤§å‹è§†è§‰åŸºç¡€æ¨¡å‹ä¸­è·å¾—ä¼ªå¯¹åº”ç‰©æ¥è¿›è¡Œè®­ç»ƒï¼Œæ— éœ€ä»»ä½•3Dæ³¨é‡Šã€‚æˆ‘ä»¬é‡‡ç”¨äº†ä¸€ç§åŒå‘ä¼ªå¯¹åº”ç”Ÿæˆæ–¹æ³•ï¼Œè¯¥æ–¹æ³•åˆ©ç”¨å±€éƒ¨å¤–è§‚ç‰¹å¾å’Œå…¨å±€ä¸Šä¸‹æ–‡ä¿¡æ¯æ¥ç”Ÿæˆä¼ªå¯¹åº”ç‰©ã€‚åœ¨æ±½è½¦æ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„DINeMoå¤§å¹…è¶…è¶Šäº†ä¹‹å‰çš„é›¶æ ·æœ¬å’Œå°‘æ ·æœ¬3Då§¿æ€ä¼°è®¡ï¼Œä¸å®Œå…¨ç›‘ç£çš„æ–¹æ³•çš„å·®è·ç¼©å°äº†67.3%ã€‚æ­¤å¤–ï¼Œå½“åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­åŠ å…¥æ›´å¤šæ— æ ‡ç­¾å›¾åƒæ—¶ï¼Œæˆ‘ä»¬çš„DINeMoèƒ½å¤Ÿæœ‰æ•ˆåœ°è¿›è¡Œæ‰©å±•ï¼Œè¿™æ˜¾ç¤ºäº†ä¸ä¾èµ–3Dæ³¨é‡Šçš„ç›‘ç£å­¦ä¹ æ–¹æ³•çš„ä¼˜åŠ¿ã€‚æˆ‘ä»¬çš„é¡¹ç›®é¡µé¢å¯åœ¨<a target="_blank" rel="noopener" href="https://analysis-by-synthesis.github.io/DINeMo/">https://analysis-by-synthesis.github.io/DINeMo/</a>è®¿é—®ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.20220v1">PDF</a> Technical report</p>
<p><strong>Summary</strong></p>
<p>åŸºäºä¼ªå¯¹åº”çš„å¤§å‹è§†è§‰åŸºç¡€æ¨¡å‹çš„ç¥ç»ç½‘ç»œæ¨¡å‹DINeMoï¼Œæ— éœ€3Dæ³¨é‡Šå³å¯è¿›è¡Œè®­ç»ƒï¼Œå®ç°äº†è·¨ç±»åˆ«çº§åˆ«çš„3D&#x2F;6Då§¿æ€ä¼°è®¡ã€‚é€šè¿‡åˆ©ç”¨å±€éƒ¨å¤–è§‚ç‰¹å¾å’Œå…¨å±€ä¸Šä¸‹æ–‡ä¿¡æ¯çš„åŒå‘ä¼ªå¯¹åº”ç”Ÿæˆæ–¹æ³•ï¼ŒDINeMoåœ¨æ±½è½¦æ•°æ®é›†ä¸Šçš„å®éªŒç»“æœæ˜¾ç¤ºå‡ºäº†å“è¶Šçš„é›¶æ ·æœ¬å’Œå°‘æ ·æœ¬å­¦ä¹ èƒ½åŠ›ï¼Œç›¸è¾ƒäºå®Œå…¨ç›‘ç£çš„æ–¹æ³•ç¼©å°äº†å·®è·ã€‚æ­¤å¤–ï¼ŒDINeMoåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­èƒ½å¤Ÿæœ‰æ•ˆåœ°åˆ©ç”¨æ›´å¤šçš„æ— æ ‡ç­¾å›¾åƒï¼Œæ˜¾ç¤ºå‡ºå…¶ç›¸è¾ƒäºä¾èµ–3Dæ³¨é‡Šçš„ç›‘ç£å­¦ä¹ æ–¹æ³•çš„ä¼˜åŠ¿ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>DINeMoæ˜¯ä¸€ç§æ–°å‹çš„ç¥ç»ç½‘ç»œæ¨¡å‹ï¼Œç”¨äºå®ç°æ— éœ€3Dæ³¨é‡Šçš„ç±»åˆ«çº§åˆ«çš„3D&#x2F;6Då§¿æ€ä¼°è®¡ã€‚</li>
<li>DINeMoé€šè¿‡åˆ©ç”¨ä¼ªå¯¹åº”ï¼Œå€ŸåŠ©å¤§å‹è§†è§‰åŸºç¡€æ¨¡å‹è¿›è¡Œè®­ç»ƒã€‚</li>
<li>è¯¥æ¨¡å‹é‡‡ç”¨åŒå‘ä¼ªå¯¹åº”ç”Ÿæˆæ–¹æ³•ï¼Œç»“åˆå±€éƒ¨å¤–è§‚ç‰¹å¾å’Œå…¨å±€ä¸Šä¸‹æ–‡ä¿¡æ¯ã€‚</li>
<li>åœ¨æ±½è½¦æ•°æ®é›†ä¸Šçš„å®éªŒç»“æœæ˜¾ç¤ºï¼ŒDINeMoåœ¨é›¶æ ·æœ¬å’Œå°‘æ ·æœ¬å­¦ä¹ æ–¹é¢è¡¨ç°å‡ºå“è¶Šæ€§èƒ½ã€‚</li>
<li>DINeMoç¼©å°äº†ä¸å®Œå…¨ç›‘ç£æ–¹æ³•ä¹‹é—´çš„å·®è·ï¼Œè¾¾åˆ°67.3%ã€‚</li>
<li>DINeMoåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­èƒ½å¤Ÿæœ‰æ•ˆåœ°åˆ©ç”¨æ›´å¤šçš„æ— æ ‡ç­¾å›¾åƒã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.20220">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-3883d8644f3e299967e982abcce9a986.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-4a7c3e6ac49cfc6d366dcb7311d6e048.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-cd8995511b17b2a28c0b9161a9f7400d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-79e93ebc9e2e254fd36162a698b906ea.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-803007072af6b64901d21078ca72630f.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="MMRL-Multi-Modal-Representation-Learning-for-Vision-Language-Models"><a href="#MMRL-Multi-Modal-Representation-Learning-for-Vision-Language-Models" class="headerlink" title="MMRL: Multi-Modal Representation Learning for Vision-Language Models"></a>MMRL: Multi-Modal Representation Learning for Vision-Language Models</h2><p><strong>Authors:Yuncheng Guo, Xiaodong Gu</strong></p>
<p>Large-scale pre-trained Vision-Language Models (VLMs) have become essential for transfer learning across diverse tasks. However, adapting these models with limited few-shot data often leads to overfitting, diminishing their performance on new tasks. To tackle this issue, we propose a novel Multi-Modal Representation Learning (MMRL) framework that introduces a shared, learnable, and modality-agnostic representation space. MMRL projects the space tokens to text and image representation tokens, facilitating more effective multi-modal interactions. Unlike previous approaches that solely optimize class token features, MMRL integrates representation tokens at higher layers of the encodersâ€“where dataset-specific features are more prominentâ€“while preserving generalized knowledge in the lower layers. During training, both representation and class features are optimized, with trainable projection layer applied to the representation tokens, whereas the class token projection layer remains frozen to retain pre-trained knowledge. Furthermore, a regularization term is introduced to align the class features and text features with the zero-shot features from the frozen VLM, thereby safeguarding the modelâ€™s generalization capacity. For inference, a decoupling strategy is employed, wherein both representation and class features are utilized for base classes, while only the class features, which retain more generalized knowledge, are used for new tasks. Extensive experiments across 15 datasets demonstrate that MMRL outperforms state-of-the-art methods, achieving a balanced trade-off between task-specific adaptation and generalization. Code is available at <a target="_blank" rel="noopener" href="https://github.com/yunncheng/MMRL">https://github.com/yunncheng/MMRL</a>. </p>
<blockquote>
<p>å¤§è§„æ¨¡é¢„è®­ç»ƒçš„è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰å¯¹äºè·¨ä¸åŒä»»åŠ¡çš„è¿ç§»å­¦ä¹ å·²ç»å˜å¾—è‡³å…³é‡è¦ã€‚ç„¶è€Œï¼Œä½¿ç”¨æœ‰é™çš„å°‘é‡æ•°æ®é€‚åº”è¿™äº›æ¨¡å‹å¾€å¾€ä¼šå¯¼è‡´è¿‡æ‹Ÿåˆï¼Œä»è€Œé™ä½å®ƒä»¬åœ¨æ–°ä»»åŠ¡ä¸Šçš„æ€§èƒ½ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„å¤šæ¨¡æ€è¡¨ç¤ºå­¦ä¹ ï¼ˆMMRLï¼‰æ¡†æ¶ï¼Œå®ƒå¼•å…¥äº†ä¸€ä¸ªå…±äº«ã€å¯å­¦ä¹ å’Œä¸æ¨¡æ€æ— å…³çš„è¡¨ç¤ºç©ºé—´ã€‚MMRLå°†ç©ºé—´æ ‡è®°æŠ•å°„åˆ°æ–‡æœ¬å’Œå›¾åƒè¡¨ç¤ºæ ‡è®°ä¸Šï¼Œä¿ƒè¿›æ›´æœ‰æ•ˆçš„å¤šæ¨¡æ€äº¤äº’ã€‚ä¸åŒäºä¹‹å‰ä»…ä¼˜åŒ–ç±»åˆ«æ ‡è®°ç‰¹å¾çš„æ–¹æ³•ï¼ŒMMRLåœ¨ç¼–ç å™¨çš„æ›´é«˜å±‚é›†æˆè¡¨ç¤ºæ ‡è®°ï¼Œåœ¨è¿™é‡Œæ•°æ®é›†ç‰¹å®šçš„ç‰¹å¾æ›´åŠ çªå‡ºï¼ŒåŒæ—¶åœ¨ä¸‹å±‚ä¿æŒé€šç”¨çŸ¥è¯†ã€‚åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œè¡¨ç¤ºç‰¹å¾å’Œç±»åˆ«ç‰¹å¾éƒ½å¾—åˆ°ä¼˜åŒ–ï¼Œå¯è®­ç»ƒçš„æŠ•å½±å±‚åº”ç”¨äºè¡¨ç¤ºæ ‡è®°ï¼Œè€Œç±»åˆ«æ ‡è®°æŠ•å½±å±‚ä¿æŒå†»ç»“ä»¥ä¿ç•™é¢„è®­ç»ƒçŸ¥è¯†ã€‚æ­¤å¤–ï¼Œå¼•å…¥äº†ä¸€ä¸ªæ­£åˆ™åŒ–é¡¹ï¼Œä»¥ä½¿ç±»åˆ«ç‰¹å¾å’Œæ–‡æœ¬ç‰¹å¾ä¸å†»ç»“VLMçš„é›¶å°„ç‰¹å¾å¯¹é½ï¼Œä»è€Œä¿æŠ¤æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ã€‚å¯¹äºæ¨æ–­ï¼Œé‡‡ç”¨äº†è§£è€¦ç­–ç•¥ï¼Œå…¶ä¸­è¡¨ç¤ºç‰¹å¾å’Œç±»åˆ«ç‰¹å¾éƒ½ç”¨äºåŸºæœ¬ç±»åˆ«ï¼Œè€Œä»…ä½¿ç”¨ä¿ç•™æ›´å¤šé€šç”¨çŸ¥è¯†çš„ç±»åˆ«ç‰¹å¾ç”¨äºæ–°ä»»åŠ¡ã€‚åœ¨15ä¸ªæ•°æ®é›†ä¸Šçš„å¤§é‡å®éªŒè¡¨æ˜ï¼ŒMMRLä¼˜äºç°æœ‰æ–¹æ³•ï¼Œå®ç°äº†ä»»åŠ¡ç‰¹å®šé€‚åº”æ€§å’Œæ³›åŒ–ä¹‹é—´çš„å¹³è¡¡ã€‚ä»£ç å¯ç”¨åœ¨ <a target="_blank" rel="noopener" href="https://github.com/yunncheng/MMRL%E3%80%82">https://github.com/yunncheng/MMRLã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.08497v2">PDF</a> Accepted by CVPR 2025</p>
<p><strong>Summary</strong></p>
<p>å¤§è§„æ¨¡é¢„è®­ç»ƒè§†å¬è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰å¯¹äºè·¨å¤šç§ä»»åŠ¡çš„è¿ç§»å­¦ä¹ è‡³å…³é‡è¦ã€‚ç„¶è€Œï¼Œä½¿ç”¨æœ‰é™çš„å°‘é‡æ•°æ®è¿›è¡Œæ¨¡å‹é€‚åº”å¾€å¾€ä¼šå¯¼è‡´è¿‡æ‹Ÿåˆï¼Œå½±å“æ¨¡å‹åœ¨æ–°ä»»åŠ¡ä¸Šçš„è¡¨ç°ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†å…¨æ–°çš„å¤šæ¨¡æ€è¡¨ç¤ºå­¦ä¹ ï¼ˆMMRLï¼‰æ¡†æ¶ï¼Œå¼•å…¥äº†ä¸€ä¸ªå…±äº«ã€å¯å­¦ä¹ å’Œæ¨¡æ€æ— å…³çš„è¡¨ç¤ºç©ºé—´ã€‚MMRLå°†ç©ºé—´æ ‡è®°æŠ•å½±åˆ°æ–‡æœ¬å’Œå›¾åƒè¡¨ç¤ºæ ‡è®°ä¸Šï¼Œä¿ƒè¿›æ›´æœ‰æ•ˆçš„å¤šæ¨¡æ€äº¤äº’ã€‚ä¸å…¶ä»–æ–¹æ³•ä¸åŒï¼ŒMMRLåœ¨ç¼–ç å™¨çš„æ›´é«˜å±‚é›†æˆè¡¨ç¤ºæ ‡è®°ï¼ŒåŒæ—¶ä¿ç•™è¾ƒä½å±‚æ¬¡çš„é€šç”¨çŸ¥è¯†ã€‚åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ï¼ŒåŒæ—¶ä¼˜åŒ–è¡¨ç¤ºå’Œç±»åˆ«ç‰¹å¾ï¼Œå¯¹è¡¨ç¤ºæ ‡è®°åº”ç”¨å¯è®­ç»ƒçš„æŠ•å½±å±‚ï¼Œè€Œä¿æŒç±»åˆ«æ ‡è®°æŠ•å½±å±‚å†»ç»“ä»¥ä¿ç•™é¢„è®­ç»ƒçŸ¥è¯†ã€‚æ­¤å¤–ï¼Œå¼•å…¥æ­£åˆ™åŒ–é¡¹ä»¥å¯¹é½ç±»åˆ«ç‰¹å¾å’Œæ–‡æœ¬ç‰¹å¾ï¼Œä»¥åŠä¸å†»ç»“VLMçš„é›¶æ ·æœ¬ç‰¹å¾ï¼Œä»è€Œä¿æŠ¤æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ã€‚å¯¹äºæ¨æ–­ï¼Œé‡‡ç”¨äº†è§£è€¦ç­–ç•¥ï¼Œå…¶ä¸­è¡¨ç¤ºå’Œç±»åˆ«ç‰¹å¾ç”¨äºåŸºç¡€ç±»åˆ«ï¼Œè€Œä»…ä½¿ç”¨ä¿ç•™æ›´å¤šé€šç”¨çŸ¥è¯†çš„ç±»åˆ«ç‰¹å¾ç”¨äºæ–°ä»»åŠ¡ã€‚å¹¿æ³›çš„å®éªŒè¡¨æ˜ï¼ŒMMRLåœ¨15ä¸ªæ•°æ®é›†ä¸Šçš„è¡¨ç°å‡ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œå®ç°äº†ä»»åŠ¡ç‰¹å®šé€‚åº”å’Œæ³›åŒ–ä¹‹é—´çš„å¹³è¡¡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§è§„æ¨¡é¢„è®­ç»ƒè§†å¬è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰åœ¨è¿ç§»å­¦ä¹ ä¸­å¾ˆé‡è¦ã€‚</li>
<li>ä½¿ç”¨æœ‰é™çš„å°‘é‡æ•°æ®è¿›è¡Œæ¨¡å‹é€‚åº”å¯èƒ½å¯¼è‡´è¿‡æ‹Ÿåˆã€‚</li>
<li>æå‡ºæ–°çš„å¤šæ¨¡æ€è¡¨ç¤ºå­¦ä¹ ï¼ˆMMRLï¼‰æ¡†æ¶æ¥è§£å†³è¿™ä¸€é—®é¢˜ã€‚</li>
<li>MMRLå¼•å…¥å…±äº«ã€å¯å­¦ä¹ å’Œæ¨¡æ€æ— å…³çš„è¡¨ç¤ºç©ºé—´ã€‚</li>
<li>MMRLé€šè¿‡æŠ•å½±ç©ºé—´æ ‡è®°åˆ°æ–‡æœ¬å’Œå›¾åƒè¡¨ç¤ºæ ‡è®°æ¥ä¿ƒè¿›å¤šæ¨¡æ€äº¤äº’ã€‚</li>
<li>MMRLåœ¨ç¼–ç å™¨çš„æ›´é«˜å±‚é›†æˆè¡¨ç¤ºæ ‡è®°ï¼ŒåŒæ—¶ä¿ç•™é€šç”¨çŸ¥è¯†ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.08497">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-f4efa7739b65eff08b2d6ef8ae748ec2.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-dae975ba432b158293f3f1a0ab1b109b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-cb187fc42024f5b87086306375cfba0a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1f7612300d9f368546758d7630a0a27b.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Ensemble-Debiasing-Across-Class-and-Sample-Levels-for-Fairer-Prompting-Accuracy"><a href="#Ensemble-Debiasing-Across-Class-and-Sample-Levels-for-Fairer-Prompting-Accuracy" class="headerlink" title="Ensemble Debiasing Across Class and Sample Levels for Fairer Prompting   Accuracy"></a>Ensemble Debiasing Across Class and Sample Levels for Fairer Prompting   Accuracy</h2><p><strong>Authors:Ruixi Lin, Ziqiao Wang, Yang You</strong></p>
<p>Language models are strong few-shot learners and achieve good overall accuracy in text classification tasks, masking the fact that their results suffer from great class accuracy imbalance. We believe that the pursuit of overall accuracy should not come from enriching the strong classes, but from raising up the weak ones. To address the imbalance, we propose a Heaviside step function based ensemble debiasing method, which enables flexible rectifications of in-context learned class probabilities at both class and sample levels. Evaluations with Llama-2-13B on seven text classification benchmarks show that our approach achieves state-of-the-art overall accuracy gains with balanced class accuracies. More importantly, we perform analyses on the resulted probability correction scheme, showing that sample-level corrections are necessary to elevate weak classes. Due to effectively correcting weak classes, our method also brings significant performance gains to a larger model variant, Llama-2-70B, especially on a biomedical domain task, further demonstrating the necessity of ensemble debiasing at both levels. </p>
<blockquote>
<p>è¯­è¨€æ¨¡å‹æ˜¯å¼ºå¤§çš„å°æ ·æœ¬å­¦ä¹ è€…ï¼Œåœ¨æ–‡æœ¬åˆ†ç±»ä»»åŠ¡ä¸­è¾¾åˆ°äº†è‰¯å¥½çš„æ€»ä½“ç²¾åº¦ï¼Œæ©ç›–äº†å…¶ç»“æœå—åˆ°ä¸¥é‡ç±»åˆ«ç²¾åº¦ä¸å¹³è¡¡çš„å½±å“ã€‚æˆ‘ä»¬è®¤ä¸ºè¿½æ±‚æ€»ä½“ç²¾åº¦ä¸åº”é€šè¿‡ä¸°å¯Œå¼ºç±»æ¥å®ç°ï¼Œè€Œåº”é€šè¿‡æé«˜å¼±ç±»æ¥å®ç°ã€‚ä¸ºäº†è§£å†³ä¸å¹³è¡¡é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºæµ·ç»´èµ›å¾·é˜¶è·ƒå‡½æ•°çš„é›†æˆå»åæ–¹æ³•ï¼Œè¯¥æ–¹æ³•èƒ½å¤Ÿåœ¨ç±»å’Œæ ·æœ¬çº§åˆ«çµæ´»åœ°ä¿®æ­£ä¸Šä¸‹æ–‡å­¦ä¹ çš„ç±»æ¦‚ç‡ã€‚ä½¿ç”¨Llama-2-13Båœ¨ä¸ƒä¸ªæ–‡æœ¬åˆ†ç±»åŸºå‡†æµ‹è¯•ä¸Šçš„è¯„ä¼°è¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨å®ç°æœ€å…ˆè¿›çš„æ€»ä½“ç²¾åº¦å¢ç›Šçš„åŒæ—¶ï¼Œä¹Ÿå®ç°äº†å¹³è¡¡çš„ç±»åˆ«ç²¾åº¦ã€‚æ›´é‡è¦çš„æ˜¯ï¼Œæˆ‘ä»¬å¯¹æ‰€å¾—æ¦‚ç‡ä¿®æ­£æ–¹æ¡ˆè¿›è¡Œäº†åˆ†æï¼Œè¡¨æ˜æ ·æœ¬çº§ä¿®æ­£å¯¹äºæé«˜å¼±ç±»æ˜¯å¿…è¦çš„ã€‚ç”±äºæœ‰æ•ˆåœ°çº æ­£äº†å¼±ç±»ï¼Œæˆ‘ä»¬çš„æ–¹æ³•ä¹Ÿä¸ºæ›´å¤§çš„æ¨¡å‹å˜ä½“Llama-2-70Bå¸¦æ¥äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ï¼Œç‰¹åˆ«æ˜¯åœ¨ç”Ÿç‰©åŒ»å­¦é¢†åŸŸä»»åŠ¡ä¸­æ›´æ˜¯å¦‚æ­¤ï¼Œè¿™è¿›ä¸€æ­¥è¯æ˜äº†åœ¨ä¸¤çº§è¿›è¡Œé›†æˆå»åçš„å¿…è¦æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.05157v3">PDF</a> </p>
<p><strong>Summary</strong><br>è¯­è¨€æ¨¡å‹åœ¨å°‘æ•°æ ·æœ¬å­¦ä¹ æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œå¹¶åœ¨æ–‡æœ¬åˆ†ç±»ä»»åŠ¡ä¸­è¾¾åˆ°è¾ƒé«˜çš„æ•´ä½“å‡†ç¡®ç‡ã€‚ç„¶è€Œï¼Œå­˜åœ¨ç±»åˆ«å‡†ç¡®ç‡å¤±è¡¡çš„é—®é¢˜ã€‚ä¸ºæé«˜å¼±ç±»åˆ«çš„å‡†ç¡®ç‡ï¼Œæå‡ºäº†ä¸€ç§åŸºäºæµ·ç»´èµ›å¾·é˜¶è·ƒå‡½æ•°çš„é›†æˆå»åæ–¹æ³•ï¼Œå¯åœ¨ç±»å’Œæ ·æœ¬çº§åˆ«çµæ´»åœ°ä¿®æ­£ä¸Šä¸‹æ–‡å­¦ä¹ åˆ°çš„ç±»åˆ«æ¦‚ç‡ã€‚åœ¨å¤šä¸ªæ–‡æœ¬åˆ†ç±»åŸºå‡†æµ‹è¯•ä¸Šçš„è¯„ä¼°è¡¨æ˜ï¼Œè¯¥æ–¹æ³•å®ç°äº†æœ€å…ˆè¿›çš„æ€»ä½“ç²¾åº¦å¢ç›Šå’Œå¹³è¡¡çš„ç±»åˆ«ç²¾åº¦ã€‚å¯¹æ¦‚ç‡ä¿®æ­£æ–¹æ¡ˆçš„åˆ†æè¡¨æ˜ï¼Œæ ·æœ¬çº§ä¿®æ­£å¯¹äºæé«˜å¼±ç±»åˆ«æ˜¯å¿…è¦çš„ã€‚è¯¥æ–¹æ³•åœ¨å¤§å‹æ¨¡å‹å˜ä½“ï¼ˆå°¤å…¶æ˜¯ç”Ÿç‰©åŒ»å­¦é¢†åŸŸä»»åŠ¡ï¼‰ä¸Šä¹Ÿå¸¦æ¥äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ï¼Œè¿›ä¸€æ­¥è¯æ˜äº†åœ¨ä¸¤çº§è¿›è¡Œé›†æˆå»åçš„å¿…è¦æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è¯­è¨€æ¨¡å‹æ˜¯å¼ºå¤§çš„å°‘æ•°æ ·æœ¬å­¦ä¹ è€…ï¼Œåœ¨æ–‡æœ¬åˆ†ç±»ä»»åŠ¡ä¸­è¡¨ç°å‡ºé«˜æ•´ä½“å‡†ç¡®ç‡ã€‚</li>
<li>å­˜åœ¨ç±»åˆ«å‡†ç¡®ç‡å¤±è¡¡çš„é—®é¢˜ï¼Œå³æŸäº›ç±»åˆ«çš„é¢„æµ‹ç»“æœæ¯”å…¶ä»–ç±»åˆ«æ›´ä¸å¯é ã€‚</li>
<li>æå‡ºäº†åŸºäºæµ·ç»´èµ›å¾·é˜¶è·ƒå‡½æ•°çš„é›†æˆå»åæ–¹æ³•ï¼Œç”¨äºä¿®æ­£ç±»åˆ«æ¦‚ç‡ã€‚</li>
<li>è¯¥æ–¹æ³•åœ¨å¤šä¸ªæ–‡æœ¬åˆ†ç±»ä»»åŠ¡ä¸Šå®ç°äº†å…ˆè¿›çš„æ€»ä½“ç²¾åº¦å¢ç›Šå’Œå¹³è¡¡çš„ç±»åˆ«ç²¾åº¦ã€‚</li>
<li>æ ·æœ¬çº§ä¿®æ­£å¯¹äºæé«˜å¼±ç±»åˆ«ï¼ˆåŸå…ˆé¢„æµ‹è¾ƒå·®çš„ç±»åˆ«ï¼‰çš„å‡†ç¡®ç‡æ˜¯å¿…è¦çš„ã€‚</li>
<li>è¯¥æ–¹æ³•é€‚ç”¨äºä¸åŒè§„æ¨¡çš„è¯­è¨€æ¨¡å‹ï¼Œç‰¹åˆ«æ˜¯åœ¨å¤„ç†å¤æ‚é¢†åŸŸï¼ˆå¦‚ç”Ÿç‰©åŒ»å­¦ï¼‰æ—¶è¡¨ç°å‡ºä¼˜åŠ¿ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.05157">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-455bf876f4c51f487c10f76839fb8ade.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-49e7abf837ae065b0f6b125e263f6ea4.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-206aed54cacde9744d1d5f74312a649e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7b10245ff8527ca89793162cfdbbc0b9.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="FrugalNeRF-Fast-Convergence-for-Few-shot-Novel-View-Synthesis-without-Learned-Priors"><a href="#FrugalNeRF-Fast-Convergence-for-Few-shot-Novel-View-Synthesis-without-Learned-Priors" class="headerlink" title="FrugalNeRF: Fast Convergence for Few-shot Novel View Synthesis without   Learned Priors"></a>FrugalNeRF: Fast Convergence for Few-shot Novel View Synthesis without   Learned Priors</h2><p><strong>Authors:Chin-Yang Lin, Chung-Ho Wu, Chang-Han Yeh, Shih-Han Yen, Cheng Sun, Yu-Lun Liu</strong></p>
<p>Neural Radiance Fields (NeRF) face significant challenges in extreme few-shot scenarios, primarily due to overfitting and long training times. Existing methods, such as FreeNeRF and SparseNeRF, use frequency regularization or pre-trained priors but struggle with complex scheduling and bias. We introduce FrugalNeRF, a novel few-shot NeRF framework that leverages weight-sharing voxels across multiple scales to efficiently represent scene details. Our key contribution is a cross-scale geometric adaptation scheme that selects pseudo ground truth depth based on reprojection errors across scales. This guides training without relying on externally learned priors, enabling full utilization of the training data. It can also integrate pre-trained priors, enhancing quality without slowing convergence. Experiments on LLFF, DTU, and RealEstate-10K show that FrugalNeRF outperforms other few-shot NeRF methods while significantly reducing training time, making it a practical solution for efficient and accurate 3D scene reconstruction. </p>
<blockquote>
<p>ç¥ç»è¾å°„åœºï¼ˆNeRFï¼‰åœ¨æç«¯å°æ ·æœ¬åœºæ™¯ä¸­é¢ä¸´é‡å¤§æŒ‘æˆ˜ï¼Œä¸»è¦æ˜¯ç”±äºè¿‡æ‹Ÿåˆå’Œè®­ç»ƒæ—¶é—´è¿‡é•¿ã€‚ç°æœ‰æ–¹æ³•ï¼Œå¦‚FreeNeRFå’ŒSparseNeRFï¼Œä½¿ç”¨é¢‘ç‡æ­£åˆ™åŒ–æˆ–é¢„è®­ç»ƒå…ˆéªŒï¼Œä½†é¢ä¸´å¤æ‚çš„è°ƒåº¦å’Œåè§é—®é¢˜ã€‚æˆ‘ä»¬å¼•å…¥äº†FrugalNeRFï¼Œè¿™æ˜¯ä¸€ç§æ–°çš„å°‘æ ·æœ¬NeRFæ¡†æ¶ï¼Œå®ƒé€šè¿‡è·¨å¤šä¸ªå°ºåº¦å…±äº«æƒé‡ä½“ç§¯æ¥æœ‰æ•ˆåœ°è¡¨ç¤ºåœºæ™¯ç»†èŠ‚ã€‚æˆ‘ä»¬çš„ä¸»è¦è´¡çŒ®æ˜¯ä¸€ç§è·¨å°ºåº¦çš„å‡ ä½•è‡ªé€‚åº”æ–¹æ¡ˆï¼Œè¯¥æ–¹æ¡ˆæ ¹æ®è·¨å°ºåº¦çš„é‡æŠ•å½±è¯¯å·®é€‰æ‹©ä¼ªçœŸå®æ·±åº¦ã€‚è¿™å¯ä»¥åœ¨ä¸ä¾èµ–å¤–éƒ¨å­¦ä¹ å…ˆéªŒçš„æƒ…å†µä¸‹æŒ‡å¯¼è®­ç»ƒï¼Œå®ç°è®­ç»ƒæ•°æ®çš„å……åˆ†åˆ©ç”¨ã€‚å®ƒè¿˜å¯ä»¥é›†æˆé¢„è®­ç»ƒçš„å…ˆéªŒï¼Œæé«˜è´¨é‡è€Œä¸ä¼šå‡æ…¢æ”¶æ•›é€Ÿåº¦ã€‚åœ¨LLFFã€DTUå’ŒRealEstate-10Kä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒFrugalNeRFåœ¨å…¶ä»–å°‘æ ·æœ¬NeRFæ–¹æ³•ä¸­è¡¨ç°çªå‡ºï¼ŒåŒæ—¶æ˜¾è‘—å‡å°‘äº†è®­ç»ƒæ—¶é—´ï¼Œæˆä¸ºé«˜æ•ˆä¸”å‡†ç¡®çš„3Dåœºæ™¯é‡å»ºçš„å®é™…è§£å†³æ–¹æ¡ˆã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2410.16271v2">PDF</a> Paper accepted to CVPR 2025. Project page:   <a target="_blank" rel="noopener" href="https://linjohnss.github.io/frugalnerf/">https://linjohnss.github.io/frugalnerf/</a></p>
<p><strong>Summary</strong></p>
<p>ç¥ç»ç½‘ç»œè¾å°„åœºï¼ˆNeRFï¼‰åœ¨æç«¯å°æ ·æœ¬åœºæ™¯ä¸­é¢ä¸´é‡å¤§æŒ‘æˆ˜ï¼Œä¸»è¦è¡¨ç°ä¸ºè¿‡åº¦æ‹Ÿåˆå’Œè®­ç»ƒæ—¶é—´é•¿ã€‚ç°æœ‰æ–¹æ³•å¦‚FreeNeRFå’ŒSparseNeRFé‡‡ç”¨é¢‘ç‡æ­£åˆ™åŒ–æˆ–é¢„è®­ç»ƒå…ˆéªŒï¼Œä½†é¢ä¸´å¤æ‚è°ƒåº¦å’Œåå·®é—®é¢˜ã€‚æˆ‘ä»¬æ¨å‡ºFrugalNeRFï¼Œä¸€ç§å…¨æ–°çš„å°‘æ ·æœ¬NeRFæ¡†æ¶ï¼Œé€šè¿‡è·¨å¤šä¸ªå°ºåº¦çš„æƒé‡å…±äº«ä½“ç´ æœ‰æ•ˆåœ°è¡¨ç¤ºåœºæ™¯ç»†èŠ‚ã€‚å…¶å…³é”®è´¡çŒ®åœ¨äºæå‡ºäº†ä¸€ç§è·¨å°ºåº¦å‡ ä½•è‡ªé€‚åº”æ–¹æ¡ˆï¼Œæ ¹æ®è·¨å°ºåº¦çš„é‡æŠ•å½±è¯¯å·®é€‰æ‹©ä¼ªçœŸå®æ·±åº¦ï¼Œä»è€Œå¼•å¯¼è®­ç»ƒï¼Œæ— éœ€ä¾èµ–å¤–éƒ¨å­¦ä¹ å…ˆéªŒï¼Œå……åˆ†åˆ©ç”¨è®­ç»ƒæ•°æ®ã€‚åŒæ—¶ï¼Œå®ƒä¹Ÿèƒ½é›†æˆé¢„è®­ç»ƒçš„å…ˆéªŒçŸ¥è¯†ï¼Œæé«˜è´¨é‡è€Œä¸ä¼šå‡æ…¢æ”¶æ•›é€Ÿåº¦ã€‚å®éªŒè¡¨æ˜ï¼Œåœ¨LLFFã€DTUå’ŒRealEstate-10Kæ•°æ®é›†ä¸Šï¼ŒFrugalNeRFåœ¨å°‘æ ·æœ¬NeRFæ–¹æ³•ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œè®­ç»ƒæ—¶é—´ä¹Ÿå¤§å¤§ç¼©çŸ­ï¼Œæˆä¸ºé«˜æ•ˆå‡†ç¡®3Dåœºæ™¯é‡å»ºçš„å®é™…è§£å†³æ–¹æ¡ˆã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>FrugalNeRFè§£å†³äº†ç¥ç»ç½‘ç»œè¾å°„åœºï¼ˆNeRFï¼‰åœ¨æç«¯å°æ ·æœ¬åœºæ™¯ä¸­çš„æŒ‘æˆ˜ã€‚</li>
<li>å®ƒé€šè¿‡è·¨å°ºåº¦çš„æƒé‡å…±äº«ä½“ç´ æœ‰æ•ˆåœ°è¡¨ç¤ºåœºæ™¯ç»†èŠ‚ã€‚</li>
<li>FrugalNeRFé‡‡ç”¨è·¨å°ºåº¦å‡ ä½•è‡ªé€‚åº”æ–¹æ¡ˆï¼Œæ ¹æ®é‡æŠ•å½±è¯¯å·®é€‰æ‹©ä¼ªçœŸå®æ·±åº¦ã€‚</li>
<li>è¯¥æ–¹æ³•æ— éœ€ä¾èµ–å¤–éƒ¨å­¦ä¹ å…ˆéªŒï¼Œèƒ½å……åˆ†åˆ©ç”¨è®­ç»ƒæ•°æ®ã€‚</li>
<li>FrugalNeRFèƒ½é›†æˆé¢„è®­ç»ƒçš„å…ˆéªŒçŸ¥è¯†ï¼Œæé«˜è´¨é‡è€Œä¸ä¼šå‡æ…¢æ”¶æ•›é€Ÿåº¦ã€‚</li>
<li>å®éªŒè¡¨æ˜ï¼ŒFrugalNeRFåœ¨å¤šä¸ªæ•°æ®é›†ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œè®­ç»ƒæ—¶é—´ç¼©çŸ­ã€‚</li>
<li>FrugalNeRFæˆä¸ºé«˜æ•ˆå‡†ç¡®çš„3Dåœºæ™¯é‡å»ºçš„å®é™…è§£å†³æ–¹æ¡ˆã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2410.16271">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-50699341b52a4a7b2529ace90b706f17.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-43245106be4869fc1597a67e5e0b5280.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4fe536df3ec9788b35f091f9817c2d15.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-616ab28c0df68c6bcf579767ac650c1b.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Few-Shot-Joint-Multimodal-Entity-Relation-Extraction-via-Knowledge-Enhanced-Cross-modal-Prompt-Model"><a href="#Few-Shot-Joint-Multimodal-Entity-Relation-Extraction-via-Knowledge-Enhanced-Cross-modal-Prompt-Model" class="headerlink" title="Few-Shot Joint Multimodal Entity-Relation Extraction via   Knowledge-Enhanced Cross-modal Prompt Model"></a>Few-Shot Joint Multimodal Entity-Relation Extraction via   Knowledge-Enhanced Cross-modal Prompt Model</h2><p><strong>Authors:Li Yuan, Yi Cai, Junsheng Huang</strong></p>
<p>Joint Multimodal Entity-Relation Extraction (JMERE) is a challenging task that aims to extract entities and their relations from text-image pairs in social media posts. Existing methods for JMERE require large amounts of labeled data. However, gathering and annotating fine-grained multimodal data for JMERE poses significant challenges. Initially, we construct diverse and comprehensive multimodal few-shot datasets fitted to the original data distribution. To address the insufficient information in the few-shot setting, we introduce the \textbf{K}nowledge-\textbf{E}nhanced \textbf{C}ross-modal \textbf{P}rompt \textbf{M}odel (KECPM) for JMERE. This method can effectively address the problem of insufficient information in the few-shot setting by guiding a large language model to generate supplementary background knowledge. Our proposed method comprises two stages: (1) a knowledge ingestion stage that dynamically formulates prompts based on semantic similarity guide ChatGPT generating relevant knowledge and employs self-reflection to refine the knowledge; (2) a knowledge-enhanced language model stage that merges the auxiliary knowledge with the original input and utilizes a transformer-based model to align with JMEREâ€™s required output format. We extensively evaluate our approach on a few-shot dataset derived from the JMERE dataset, demonstrating its superiority over strong baselines in terms of both micro and macro F$_1$ scores. Additionally, we present qualitative analyses and case studies to elucidate the effectiveness of our model. </p>
<blockquote>
<p>è”åˆå¤šæ¨¡æ€å®ä½“å…³ç³»æå–ï¼ˆJMEREï¼‰æ˜¯ä¸€é¡¹å…·æœ‰æŒ‘æˆ˜æ€§çš„ä»»åŠ¡ï¼Œæ—¨åœ¨ä»ç¤¾äº¤åª’ä½“å¸–å­ä¸­çš„æ–‡æœ¬-å›¾åƒå¯¹ä¸­æå–å®ä½“åŠå…¶å…³ç³»ã€‚ç°æœ‰çš„JMEREæ–¹æ³•éœ€è¦å¤§é‡çš„æ ‡è®°æ•°æ®ã€‚ç„¶è€Œï¼Œä¸ºJMEREæ”¶é›†å¹¶æ ‡æ³¨ç»†ç²’åº¦çš„å¤šæ¨¡æ€æ•°æ®é¢ä¸´ç€å·¨å¤§çš„æŒ‘æˆ˜ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬æ„å»ºäº†å¤šæ ·ä¸”å…¨é¢çš„å¤šæ¨¡æ€å°æ ·æœ¬æ•°æ®é›†ï¼Œä»¥é€‚åº”åŸå§‹æ•°æ®åˆ†å¸ƒã€‚ä¸ºäº†è§£å†³å°æ ·æœ¬è®¾ç½®ä¸­ä¿¡æ¯ä¸è¶³çš„é—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†é’ˆå¯¹JMEREçš„<strong>çŸ¥è¯†å¢å¼ºè·¨æ¨¡æ€æç¤ºæ¨¡å‹ï¼ˆKECPMï¼‰</strong>ã€‚è¯¥æ–¹æ³•å¯ä»¥é€šè¿‡å¼•å¯¼å¤§å‹è¯­è¨€æ¨¡å‹ç”Ÿæˆè¡¥å……èƒŒæ™¯çŸ¥è¯†ï¼Œæœ‰æ•ˆè§£å†³å°æ ·æœ¬è®¾ç½®ä¸­çš„ä¿¡æ¯ä¸è¶³é—®é¢˜ã€‚æˆ‘ä»¬æå‡ºçš„æ–¹æ³•åŒ…æ‹¬ä¸¤ä¸ªé˜¶æ®µï¼šï¼ˆ1ï¼‰çŸ¥è¯†æ‘„å–é˜¶æ®µï¼Œè¯¥é˜¶æ®µæ ¹æ®è¯­ä¹‰ç›¸ä¼¼æ€§åŠ¨æ€åˆ¶å®šæç¤ºï¼ŒæŒ‡å¯¼ChatGPTç”Ÿæˆç›¸å…³çŸ¥è¯†ï¼Œå¹¶åˆ©ç”¨è‡ªæˆ‘åæ€æ¥å®Œå–„çŸ¥è¯†ï¼›ï¼ˆ2ï¼‰çŸ¥è¯†å¢å¼ºè¯­è¨€æ¨¡å‹é˜¶æ®µï¼Œè¯¥é˜¶æ®µå°†è¾…åŠ©çŸ¥è¯†ä¸åŸå§‹è¾“å…¥åˆå¹¶ï¼Œå¹¶ä½¿ç”¨åŸºäºè½¬æ¢å™¨çš„æ–¹æ³•æ¥å¯¹é½JMEREæ‰€éœ€çš„è¾“å‡ºæ ¼å¼ã€‚æˆ‘ä»¬åœ¨ä»JMEREæ•°æ®é›†ä¸­æ´¾ç”Ÿçš„å°æ ·æœ¬æ•°æ®é›†ä¸Šå¹¿æ³›è¯„ä¼°äº†æˆ‘ä»¬çš„æ–¹æ³•ï¼Œç»“æœæ˜¾ç¤ºå®ƒåœ¨å¾®è§‚å’Œå®è§‚F1åˆ†æ•°æ–¹é¢éƒ½ä¼˜äºå¼ºå¤§çš„åŸºå‡†æ¨¡å‹ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜è¿›è¡Œäº†å®šæ€§åˆ†æå’Œæ¡ˆä¾‹ç ”ç©¶ï¼Œä»¥é˜æ˜æˆ‘ä»¬æ¨¡å‹çš„æœ‰æ•ˆæ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2410.14225v2">PDF</a> accepted by ACM MM 2024</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†è”åˆå¤šæ¨¡æ€å®ä½“å…³ç³»æå–ï¼ˆJMEREï¼‰çš„æŒ‘æˆ˜æ€§ä»»åŠ¡ï¼Œæ—¨åœ¨ä»ç¤¾äº¤åª’ä½“å¸–å­ä¸­çš„æ–‡æœ¬-å›¾åƒå¯¹ä¸­æå–å®ä½“å’Œå®ƒä»¬çš„å…³ç³»ã€‚ç°æœ‰æ–¹æ³•éœ€è¦å¤§é‡æ ‡è®°æ•°æ®ï¼Œä½†ä¸ºJMEREæ”¶é›†å¹¶æ ‡æ³¨ç»†ç²’åº¦çš„å¤šæ¨¡æ€æ•°æ®å…·æœ‰å¾ˆå¤§æŒ‘æˆ˜ã€‚ä¸ºæ­¤ï¼Œæœ¬æ–‡æ„å»ºäº†é€‚åº”åŸå§‹æ•°æ®åˆ†å¸ƒçš„å¤šæ ·ä¸”å…¨é¢çš„å¤šæ¨¡æ€å°æ ·æœ¬æ•°æ®é›†ï¼Œå¹¶å¼•å…¥çŸ¥è¯†å¢å¼ºè·¨æ¨¡æ€æç¤ºæ¨¡å‹ï¼ˆKECPMï¼‰è§£å†³ä¿¡æ¯ä¸è¶³çš„é—®é¢˜ã€‚è¯¥æ–¹æ³•é€šè¿‡æŒ‡å¯¼å¤§å‹è¯­è¨€æ¨¡å‹ç”Ÿæˆè¡¥å……èƒŒæ™¯çŸ¥è¯†ï¼Œæœ‰æ•ˆåœ°è§£å†³äº†å°æ ·æœ¬è®¾ç½®ä¸­çš„ä¿¡æ¯ä¸è¶³é—®é¢˜ã€‚è¯¥æ–¹æ³•åŒ…æ‹¬ä¸¤ä¸ªé˜¶æ®µï¼šçŸ¥è¯†æ‘„å–é˜¶æ®µå’ŒçŸ¥è¯†å¢å¼ºè¯­è¨€æ¨¡å‹é˜¶æ®µã€‚åœ¨å°‘é‡æ•°æ®é›†ä¸Šçš„è¯„ä¼°ç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å¾®è§‚å’Œå®è§‚F1åˆ†æ•°æ–¹é¢ä¼˜äºå¼ºåŸºçº¿ã€‚æ­¤å¤–ï¼Œæœ¬æ–‡è¿˜æä¾›äº†å®šæ€§åˆ†æå’Œæ¡ˆä¾‹ç ”ç©¶æ¥é˜æ˜æ¨¡å‹çš„æœ‰æ•ˆæ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>JMEREæ˜¯ä¸€ä¸ªä»ç¤¾äº¤åª’ä½“å¸–å­ä¸­çš„æ–‡æœ¬-å›¾åƒå¯¹æå–å®ä½“å’Œå…³ç³»çš„æŒ‘æˆ˜æ€§ä»»åŠ¡ã€‚</li>
<li>ç°æœ‰æ–¹æ³•éœ€è¦å¤§é‡æ ‡è®°æ•°æ®ï¼Œä½†æ”¶é›†å¹¶æ ‡æ³¨ç»†ç²’åº¦çš„å¤šæ¨¡æ€æ•°æ®å…·æœ‰æŒ‘æˆ˜ã€‚</li>
<li>å¼•å…¥çŸ¥è¯†å¢å¼ºè·¨æ¨¡æ€æç¤ºæ¨¡å‹ï¼ˆKECPMï¼‰è§£å†³å°æ ·æœ¬è®¾ç½®ä¸­çš„ä¿¡æ¯ä¸è¶³é—®é¢˜ã€‚</li>
<li>KECPMæ–¹æ³•åŒ…æ‹¬çŸ¥è¯†æ‘„å–é˜¶æ®µå’ŒçŸ¥è¯†å¢å¼ºè¯­è¨€æ¨¡å‹é˜¶æ®µã€‚</li>
<li>åœ¨å°‘é‡æ•°æ®é›†ä¸Šçš„è¯„ä¼°è¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨F1åˆ†æ•°æ–¹é¢ä¼˜äºå¼ºåŸºçº¿ã€‚</li>
<li>å®šæ€§åˆ†æå’Œæ¡ˆä¾‹ç ”ç©¶è¯æ˜äº†æ¨¡å‹çš„æœ‰æ•ˆæ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2410.14225">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-b676d91c12009bfda247722bff30df3e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9c03ea5dc6de9dd2af1785ae7ebcadd9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b7612697005cd40805f54213debd3811.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c01cffd6bb6c7ee950aa547ef32d37ad.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-8d5b6e6a5b0b454fb461a4760d41e2cd.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4deffd817b50db31ab282f576b42e3f1.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="RobustEMD-Domain-Robust-Matching-for-Cross-domain-Few-shot-Medical-Image-Segmentation"><a href="#RobustEMD-Domain-Robust-Matching-for-Cross-domain-Few-shot-Medical-Image-Segmentation" class="headerlink" title="RobustEMD: Domain Robust Matching for Cross-domain Few-shot Medical   Image Segmentation"></a>RobustEMD: Domain Robust Matching for Cross-domain Few-shot Medical   Image Segmentation</h2><p><strong>Authors:Yazhou Zhu, Minxian Li, Qiaolin Ye, Shidong Wang, Tong Xin, Haofeng Zhang</strong></p>
<p>Few-shot medical image segmentation (FSMIS) aims to perform the limited annotated data learning in the medical image analysis scope. Despite the progress has been achieved, current FSMIS models are all trained and deployed on the same data domain, as is not consistent with the clinical reality that medical imaging data is always across different data domains (e.g. imaging modalities, institutions and equipment sequences). How to enhance the FSMIS models to generalize well across the different specific medical imaging domains? In this paper, we focus on the matching mechanism of the few-shot semantic segmentation models and introduce an Earth Moverâ€™s Distance (EMD) calculation based domain robust matching mechanism for the cross-domain scenario. Specifically, we formulate the EMD transportation process between the foreground support-query features, the texture structure aware weights generation method, which proposes to perform the sobel based image gradient calculation over the nodes, is introduced in the EMD matching flow to restrain the domain relevant nodes. Besides, the point set level distance measurement metric is introduced to calculated the cost for the transportation from support set nodes to query set nodes. To evaluate the performance of our model, we conduct experiments on three scenarios (i.e., cross-modal, cross-sequence and cross-institution), which includes eight medical datasets and involves three body regions, and the results demonstrate that our model achieves the SoTA performance against the compared models. </p>
<blockquote>
<p>å°‘é‡æ ‡æ³¨æ•°æ®çš„åŒ»ç–—å›¾åƒåˆ†å‰²ï¼ˆFSMISï¼‰æ—¨åœ¨å®ç°åŒ»ç–—å›¾åƒåˆ†æèŒƒå›´å†…çš„æœ‰é™æ ‡æ³¨æ•°æ®å­¦ä¹ ã€‚å°½ç®¡å·²ç»å–å¾—äº†ä¸€äº›è¿›å±•ï¼Œä½†ç›®å‰çš„FSMISæ¨¡å‹éƒ½åœ¨åŒä¸€æ•°æ®åŸŸä¸Šè¿›è¡Œè®­ç»ƒå’Œåº”ç”¨ï¼Œè¿™ä¸åŒ»å­¦æˆåƒæ•°æ®åœ¨ä¸´åºŠä¸Šçš„å®é™…æƒ…å†µä¸ç¬¦ï¼ŒåŒ»å­¦æˆåƒæ•°æ®æ€»æ˜¯è·¨è¶Šä¸åŒçš„æ•°æ®åŸŸï¼ˆä¾‹å¦‚æˆåƒæ¨¡å¼ã€æœºæ„å’Œè®¾å¤‡åºåˆ—ï¼‰ã€‚å¦‚ä½•å¢å¼ºFSMISæ¨¡å‹åœ¨ä¸åŒç‰¹å®šåŒ»å­¦æˆåƒåŸŸä¹‹é—´çš„æ³›åŒ–èƒ½åŠ›ï¼Ÿåœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬å…³æ³¨å°‘é‡è¯­ä¹‰åˆ†å‰²æ¨¡å‹çš„åŒ¹é…æœºåˆ¶ï¼Œå¹¶å¼•å…¥ä¸€ç§åŸºäºåœ°çƒç§»åŠ¨è·ç¦»ï¼ˆEMDï¼‰è®¡ç®—çš„åŸŸé²æ£’åŒ¹é…æœºåˆ¶ï¼Œç”¨äºè·¨åŸŸåœºæ™¯ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬åˆ¶å®šäº†å‰æ™¯æ”¯æŒæŸ¥è¯¢ç‰¹å¾ä¹‹é—´çš„EMDä¼ è¾“è¿‡ç¨‹ï¼Œå¹¶å¼•å…¥äº†çº¹ç†ç»“æ„æ„ŸçŸ¥æƒé‡ç”Ÿæˆæ–¹æ³•ï¼Œè¯¥æ–¹æ³•å»ºè®®åœ¨èŠ‚ç‚¹ä¸Šæ‰§è¡ŒåŸºäºSobelçš„å›¾åƒæ¢¯åº¦è®¡ç®—ã€‚åœ¨EMDåŒ¹é…æµä¸­å¼•å…¥è¿™ç§æ–¹æ³•æ¥çº¦æŸä¸åŸŸç›¸å…³çš„èŠ‚ç‚¹ã€‚æ­¤å¤–ï¼Œè¿˜å¼•å…¥äº†ç‚¹é›†çº§è·ç¦»æµ‹é‡æŒ‡æ ‡ï¼Œç”¨äºè®¡ç®—ä»æ”¯æŒé›†èŠ‚ç‚¹åˆ°æŸ¥è¯¢é›†èŠ‚ç‚¹çš„ä¼ è¾“æˆæœ¬ã€‚ä¸ºäº†è¯„ä¼°æˆ‘ä»¬æ¨¡å‹çš„æ€§èƒ½ï¼Œæˆ‘ä»¬åœ¨ä¸‰ç§åœºæ™¯ï¼ˆå³è·¨æ¨¡æ€ã€è·¨åºåˆ—å’Œè·¨æœºæ„ï¼‰è¿›è¡Œäº†å®éªŒï¼ŒåŒ…æ‹¬å…«ä¸ªåŒ»å­¦æ•°æ®é›†å’Œä¸‰ä¸ªèº«ä½“åŒºåŸŸï¼Œç»“æœè¡¨æ˜æˆ‘ä»¬çš„æ¨¡å‹ä¸å¯¹æ¯”æ¨¡å‹ç›¸æ¯”è¾¾åˆ°äº†æœ€æ–°æ€§èƒ½æ°´å¹³ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2410.01110v4">PDF</a> More details should be included, and more experiments</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†é’ˆå¯¹åŒ»å­¦å›¾åƒåˆ†æä¸­æœ‰é™æ ‡æ³¨æ•°æ®å­¦ä¹ é—®é¢˜çš„Few-shotåŒ»å­¦å›¾åƒåˆ†å‰²ï¼ˆFSMISï¼‰æŠ€æœ¯ã€‚ç°æœ‰FSMISæ¨¡å‹å±€é™åœ¨åŒä¸€æ•°æ®åŸŸå†…è®­ç»ƒéƒ¨ç½²ï¼Œä¸ç¬¦åˆåŒ»å­¦æˆåƒæ•°æ®è·¨ä¸åŒåŸŸï¼ˆå¦‚æˆåƒæ¨¡å¼ã€æœºæ„å’Œè®¾å¤‡åºåˆ—ç­‰ï¼‰çš„ä¸´åºŠç°å®ã€‚æœ¬æ–‡å…³æ³¨å°‘æ ·æœ¬è¯­ä¹‰åˆ†å‰²æ¨¡å‹çš„åŒ¹é…æœºåˆ¶ï¼Œå¹¶æå‡ºåŸºäºåœ°çƒç§»åŠ¨è·ç¦»ï¼ˆEMDï¼‰è®¡ç®—çš„åŸŸé²æ£’åŒ¹é…æœºåˆ¶ï¼Œé€‚ç”¨äºè·¨åŸŸåœºæ™¯ã€‚å®éªŒè¡¨æ˜ï¼Œè¯¥æ¨¡å‹åœ¨è·¨æ¨¡æ€ã€è·¨åºåˆ—å’Œè·¨æœºæ„ç­‰ä¸‰ç§åœºæ™¯ä¸‹è¡¨ç°ä¼˜å¼‚ï¼Œè¾¾åˆ°å½“å‰æœ€ä½³æŠ€æœ¯æ°´å¹³ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Few-shotåŒ»å­¦å›¾åƒåˆ†å‰²ï¼ˆFSMISï¼‰å…³æ³¨æœ‰é™æ ‡æ³¨æ•°æ®å­¦ä¹ åœ¨åŒ»å­¦å›¾åƒåˆ†æä¸­çš„åº”ç”¨ã€‚</li>
<li>å½“å‰FSMISæ¨¡å‹å±€é™åœ¨åŒä¸€æ•°æ®åŸŸå†…ï¼Œä¸ç¬¦åˆåŒ»å­¦æˆåƒæ•°æ®çš„å®é™…è·¨åŸŸæƒ…å†µã€‚</li>
<li>æœ¬æ–‡æå‡ºåŸºäºåœ°çƒç§»åŠ¨è·ç¦»ï¼ˆEMDï¼‰çš„åŸŸé²æ£’åŒ¹é…æœºåˆ¶ï¼Œé€‚ç”¨äºè·¨åŸŸåŒ»å­¦å›¾åƒåˆ†å‰²ã€‚</li>
<li>åŒ¹é…æœºåˆ¶ä¸­å¼•å…¥EMDè®¡ç®—ï¼Œåˆ¶å®šå‰æ™¯æ”¯æŒç‰¹å¾ä¹‹é—´çš„è¿è¾“è¿‡ç¨‹ã€‚</li>
<li>ä¸ºçº¦æŸåŸŸç›¸å…³èŠ‚ç‚¹ï¼Œæå‡ºåŸºäºSobelçš„å›¾åƒæ¢¯åº¦è®¡ç®—æ³•ï¼Œå¹¶å¼•å…¥ç‚¹é›†çº§åˆ«è·ç¦»æµ‹é‡æ ‡å‡†ã€‚</li>
<li>å®éªŒè¡¨æ˜ï¼Œè¯¥æ¨¡å‹åœ¨è·¨æ¨¡æ€ã€è·¨åºåˆ—å’Œè·¨æœºæ„ç­‰åœºæ™¯ä¸‹è¡¨ç°ä¼˜è¶Šã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2410.01110">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-6ead464cd01cc60a5a1bc36b7226bc26.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-9694b36b6f2c838aa4e62edcc696614d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9ca9d0e32e4b4033b35a3a671a4b8c4c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-2d79e93e1b52bcd63795303434c3a06e.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="Few-Shot-Recognition-via-Stage-Wise-Retrieval-Augmented-Finetuning"><a href="#Few-Shot-Recognition-via-Stage-Wise-Retrieval-Augmented-Finetuning" class="headerlink" title="Few-Shot Recognition via Stage-Wise Retrieval-Augmented Finetuning"></a>Few-Shot Recognition via Stage-Wise Retrieval-Augmented Finetuning</h2><p><strong>Authors:Tian Liu, Huixin Zhang, Shubham Parashar, Shu Kong</strong></p>
<p>Few-shot recognition (FSR) aims to train a classification model with only a few labeled examples of each concept concerned by a downstream task, where data annotation cost can be prohibitively high. We develop methods to solve FSR by leveraging a pretrained Vision-Language Model (VLM). We particularly explore retrieval-augmented learning (RAL), which retrieves open data, e.g., the VLMâ€™s pretraining dataset, to learn models for better serving downstream tasks. RAL has been studied in zero-shot recognition but remains under-explored in FSR. Although applying RAL to FSR may seem straightforward, we observe interesting and novel challenges and opportunities. First, somewhat surprisingly, finetuning a VLM on a large amount of retrieved data underperforms state-of-the-art zero-shot methods. This is due to the imbalanced distribution of retrieved data and its domain gaps with the few-shot examples in the downstream task. Second, more surprisingly, we find that simply finetuning a VLM solely on few-shot examples significantly outperforms previous FSR methods, and finetuning on the mix of retrieved and few-shot data yields even better results. Third, to mitigate the imbalanced distribution and domain gap issues, we propose Stage-Wise retrieval-Augmented fineTuning (SWAT), which involves end-to-end finetuning on mixed data in the first stage and retraining the classifier on the few-shot data in the second stage. Extensive experiments on nine popular benchmarks demonstrate that SWAT significantly outperforms previous methods by &gt;6% accuracy. </p>
<blockquote>
<p>å°‘é‡æ ·æœ¬è¯†åˆ«ï¼ˆFSRï¼‰æ—¨åœ¨ä»…ä½¿ç”¨ä¸‹æ¸¸ä»»åŠ¡æ‰€æ¶‰åŠæ¦‚å¿µçš„ä¸€äº›æ ‡è®°æ ·æœ¬è®­ç»ƒåˆ†ç±»æ¨¡å‹ï¼Œå…¶ä¸­æ•°æ®æ ‡æ³¨æˆæœ¬å¯èƒ½éå¸¸é«˜ã€‚æˆ‘ä»¬å¼€å‘äº†ä¸€ç§åˆ©ç”¨é¢„è®­ç»ƒè§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰æ¥è§£å†³FSRçš„æ–¹æ³•ã€‚æˆ‘ä»¬ç‰¹åˆ«æ¢è®¨äº†æ£€ç´¢å¢å¼ºå­¦ä¹ ï¼ˆRALï¼‰ï¼Œå®ƒæ£€ç´¢å¼€æ”¾æ•°æ®ï¼Œä¾‹å¦‚VLMçš„é¢„è®­ç»ƒæ•°æ®é›†ï¼Œä»¥å­¦ä¹ æ›´å¥½åœ°ä¸ºä¸‹æ¸¸ä»»åŠ¡æœåŠ¡çš„æ¨¡å‹ã€‚è™½ç„¶RALåœ¨é›¶æ ·æœ¬è¯†åˆ«ä¸­å·²è¢«ç ”ç©¶ï¼Œä½†åœ¨FSRä¸­ä»ç„¶è¢«ä½ä¼°ã€‚å°½ç®¡å°†RALåº”ç”¨äºFSRå¯èƒ½çœ‹èµ·æ¥å¾ˆç®€å•ï¼Œä½†æˆ‘ä»¬è§‚å¯Ÿåˆ°æœ‰è¶£çš„æ–°æŒ‘æˆ˜å’Œæœºä¼šã€‚é¦–å…ˆï¼Œä»¤äººæƒŠè®¶çš„æ˜¯ï¼Œåœ¨å¤§é‡æ£€ç´¢æ•°æ®ä¸Šå¾®è°ƒVLMçš„è¡¨ç°ä¸åŠæœ€å…ˆè¿›çš„é›¶æ ·æœ¬æ–¹æ³•ã€‚è¿™æ˜¯ç”±äºæ£€ç´¢æ•°æ®çš„åˆ†å¸ƒä¸å¹³è¡¡ä»¥åŠä¸ä¸‹æ¸¸ä»»åŠ¡ä¸­å°‘é‡æ ·æœ¬çš„åŸŸå·®è·ã€‚ç¬¬äºŒï¼Œæ›´ä»¤äººæƒŠè®¶çš„æ˜¯ï¼Œæˆ‘ä»¬å‘ç°åœ¨å°‘é‡æ ·æœ¬ä¸Šä»…å¾®è°ƒVLMæ˜¾è‘—ä¼˜äºä¹‹å‰çš„FSRæ–¹æ³•ï¼Œè€Œåœ¨æ£€ç´¢æ•°æ®å’Œå°‘é‡æ ·æœ¬æ•°æ®æ··åˆä¸Šå¾®è°ƒåˆ™èƒ½äº§ç”Ÿæ›´å¥½çš„ç»“æœã€‚ç¬¬ä¸‰ï¼Œä¸ºäº†ç¼“è§£åˆ†å¸ƒä¸å¹³è¡¡å’ŒåŸŸå·®è·é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†åˆ†é˜¶æ®µæ£€ç´¢å¢å¼ºå¾®è°ƒï¼ˆSWATï¼‰ï¼Œç¬¬ä¸€é˜¶æ®µå¯¹æ··åˆæ•°æ®è¿›è¡Œç«¯åˆ°ç«¯å¾®è°ƒï¼Œç¬¬äºŒé˜¶æ®µåœ¨å°‘é‡æ ·æœ¬æ•°æ®ä¸Šé‡æ–°è®­ç»ƒåˆ†ç±»å™¨ã€‚åœ¨ä¹ä¸ªæµè¡ŒåŸºå‡†æµ‹è¯•ä¸Šçš„å¤§é‡å®éªŒè¡¨æ˜ï¼ŒSWATåœ¨å‡†ç¡®ç‡ä¸Šæ˜¾è‘—ä¼˜äºä¹‹å‰çš„æ–¹æ³•ï¼Œæé«˜äº†è¶…è¿‡6%ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2406.11148v3">PDF</a> Accepted to CVPR 2025. Website and code:   <a target="_blank" rel="noopener" href="https://tian1327.github.io/SWAT/">https://tian1327.github.io/SWAT/</a></p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ¢è®¨äº†åŸºäºé¢„è®­ç»ƒè§†è§‰è¯­è¨€æ¨¡å‹çš„å°‘æ ·æœ¬è¯†åˆ«ï¼ˆFSRï¼‰æ–¹æ³•ï¼Œç‰¹åˆ«æ˜¯æ¢ç´¢äº†æ£€ç´¢å¢å¼ºå­¦ä¹ ï¼ˆRALï¼‰åœ¨FSRä¸­çš„åº”ç”¨ã€‚ç ”ç©¶å‘ç°ï¼Œå°½ç®¡RALåœ¨é›¶æ ·æœ¬è¯†åˆ«ä¸­å·²æœ‰ç ”ç©¶ï¼Œä½†åœ¨FSRä¸­ä»é¢ä¸´æ–°çš„æŒ‘æˆ˜å’Œæœºä¼šã€‚é€šè¿‡å¯¹æ£€ç´¢æ•°æ®çš„å¹³è¡¡åˆ†å¸ƒå’Œé¢†åŸŸå·®è·çš„åˆ†æï¼Œæå‡ºäº†åˆ†é˜¶æ®µæ£€ç´¢å¢å¼ºå¾®è°ƒï¼ˆSWATï¼‰çš„ç­–ç•¥ï¼Œè¯¥ç­–ç•¥åœ¨ä¹ä¸ªæµè¡ŒåŸºå‡†æµ‹è¯•ä¸Šçš„è¡¨ç°æ˜¾è‘—ä¼˜äºä»¥å‰çš„æ–¹æ³•ï¼Œæé«˜äº†è¶…è¿‡6%çš„å‡†ç¡®ç‡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å°‘æ ·æœ¬è¯†åˆ«ï¼ˆFSRï¼‰æ—¨åœ¨ä½¿ç”¨ä¸‹æ¸¸ä»»åŠ¡ç›¸å…³æ¦‚å¿µçš„å°‘é‡æ ‡æ³¨æ ·æœ¬è®­ç»ƒåˆ†ç±»æ¨¡å‹ï¼Œé™ä½æ•°æ®æ ‡æ³¨æˆæœ¬ã€‚</li>
<li>å€ŸåŠ©é¢„è®­ç»ƒçš„è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰ï¼Œé€šè¿‡æ£€ç´¢å¢å¼ºå­¦ä¹ ï¼ˆRALï¼‰è§£å†³FSRé—®é¢˜ã€‚</li>
<li>è§‚å¯Ÿåˆ°å°†RALåº”ç”¨äºFSRæ—¶å­˜åœ¨æ–°çš„æŒ‘æˆ˜å’Œæœºä¼šã€‚</li>
<li>å•çº¯åœ¨å¤§é‡æ£€ç´¢æ•°æ®ä¸Šå¾®è°ƒVLMçš„æ€§èƒ½ä¸å¦‚é›¶æ ·æœ¬æ–¹æ³•ï¼ŒåŸå› åœ¨äºæ£€ç´¢æ•°æ®çš„åˆ†å¸ƒä¸å¹³è¡¡åŠå…¶ä¸ä¸‹æ¸¸ä»»åŠ¡ä¸­çš„å°‘æ ·æœ¬ç¤ºä¾‹ä¹‹é—´å­˜åœ¨é¢†åŸŸå·®å¼‚ã€‚</li>
<li>ä»…åœ¨å°‘æ ·æœ¬ç¤ºä¾‹ä¸Šå¾®è°ƒVLMçš„æ€§èƒ½ä¼˜äºä»¥å¾€çš„FSRæ–¹æ³•ï¼Œè€Œåœ¨æ··åˆæ£€ç´¢æ•°æ®å’Œå°‘æ ·æœ¬æ•°æ®ä¸Šè¿›è¡Œå¾®è°ƒåˆ™æ•ˆæœæ›´ä½³ã€‚</li>
<li>æå‡ºåˆ†é˜¶æ®µæ£€ç´¢å¢å¼ºå¾®è°ƒï¼ˆSWATï¼‰ç­–ç•¥ï¼Œé€šè¿‡ä¸¤ä¸ªé˜¶æ®µçš„ç«¯å¯¹ç«¯å¾®è°ƒä¸åˆ†ç±»å™¨é‡è®­ç»ƒæ¥è§£å†³æ•°æ®åˆ†å¸ƒå’Œé¢†åŸŸå·®å¼‚é—®é¢˜ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2406.11148">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-51b85e0d40a4eb07d901814cc027615d.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-44279977f6e3b79bce408ded4862ed2f.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-1d5d995207a0d13c53199e9184036d87.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-79d47593499b9af560ffbe5cf73d11d9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-088a583a37a4d4c48cdb1b6a1f6df802.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="Socratic-Planner-Self-QA-Based-Zero-Shot-Planning-for-Embodied-Instruction-Following"><a href="#Socratic-Planner-Self-QA-Based-Zero-Shot-Planning-for-Embodied-Instruction-Following" class="headerlink" title="Socratic Planner: Self-QA-Based Zero-Shot Planning for Embodied   Instruction Following"></a>Socratic Planner: Self-QA-Based Zero-Shot Planning for Embodied   Instruction Following</h2><p><strong>Authors:Suyeon Shin, Sujin jeon, Junghyun Kim, Gi-Cheon Kang, Byoung-Tak Zhang</strong></p>
<p>Embodied Instruction Following (EIF) is the task of executing natural language instructions by navigating and interacting with objects in interactive environments. A key challenge in EIF is compositional task planning, typically addressed through supervised learning or few-shot in-context learning with labeled data. To this end, we introduce the Socratic Planner, a self-QA-based zero-shot planning method that infers an appropriate plan without any further training. The Socratic Planner first facilitates self-questioning and answering by the Large Language Model (LLM), which in turn helps generate a sequence of subgoals. While executing the subgoals, an embodied agent may encounter unexpected situations, such as unforeseen obstacles. The Socratic Planner then adjusts plans based on dense visual feedback through a visually-grounded re-planning mechanism. Experiments demonstrate the effectiveness of the Socratic Planner, outperforming current state-of-the-art planning models on the ALFRED benchmark across all metrics, particularly excelling in long-horizon tasks that demand complex inference. We further demonstrate its real-world applicability through deployment on a physical robot for long-horizon tasks. </p>
<blockquote>
<p>ä»¥å…·èº«ä¸ºå¯¼å‘ï¼ˆEmbodied Instruction Followingï¼ŒEIFï¼‰çš„ä»»åŠ¡æ˜¯é€šè¿‡åœ¨äº¤äº’ç¯å¢ƒä¸­å¯¼èˆªå’Œä¸å¯¹è±¡äº¤äº’æ¥æ‰§è¡Œè‡ªç„¶è¯­è¨€æŒ‡ä»¤ã€‚EIFçš„å…³é”®æŒ‘æˆ˜åœ¨äºç»„åˆä»»åŠ¡è§„åˆ’ï¼Œé€šå¸¸é€šè¿‡ç›‘ç£å­¦ä¹ æˆ–åˆ©ç”¨æ ‡æ³¨æ•°æ®çš„å°‘é‡ä¸Šä¸‹æ–‡å­¦ä¹ æ¥è§£å†³ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬å¼•å…¥äº†åŸºäºè‡ªæˆ‘é—®ç­”çš„é›¶æ ·æœ¬è§„åˆ’æ–¹æ³•è‹æ ¼æ‹‰åº•è§„åˆ’å™¨ï¼ˆSocratic Plannerï¼‰ï¼Œå®ƒå¯ä»¥åœ¨æ— éœ€è¿›ä¸€æ­¥è®­ç»ƒçš„æƒ…å†µä¸‹æ¨æ–­å‡ºé€‚å½“çš„è®¡åˆ’ã€‚è‹æ ¼æ‹‰åº•è§„åˆ’å™¨é¦–å…ˆé€šè¿‡å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¿ƒè¿›è‡ªæˆ‘æé—®å’Œå›ç­”ï¼Œè¿›è€Œå¸®åŠ©ç”Ÿæˆä¸€ç³»åˆ—å­ç›®æ ‡ã€‚åœ¨æ‰§è¡Œå­ç›®æ ‡æ—¶ï¼Œå®ä½“ä»£ç†å¯èƒ½ä¼šé‡åˆ°æ„å¤–æƒ…å†µï¼Œä¾‹å¦‚æœªé¢„è§çš„éšœç¢ã€‚ç„¶åï¼Œè‹æ ¼æ‹‰åº•è§„åˆ’å™¨ä¼šé€šè¿‡è§†è§‰åŸºç¡€çš„é‡æ–°è§„åˆ’æœºåˆ¶ï¼Œæ ¹æ®å¯†é›†çš„è§†è§‰åé¦ˆè°ƒæ•´è®¡åˆ’ã€‚å®éªŒè¡¨æ˜ï¼Œè‹æ ¼æ‹‰åº•è§„åˆ’å™¨éå¸¸æœ‰æ•ˆï¼Œåœ¨ALFREDåŸºå‡†æµ‹è¯•ä¸­ä¼˜äºå½“å‰æœ€å…ˆè¿›çš„è§„åˆ’æ¨¡å‹ï¼Œåœ¨æ‰€æœ‰æŒ‡æ ‡ä¸Šéƒ½è¡¨ç°å‡ºè‰²ï¼Œç‰¹åˆ«æ˜¯åœ¨éœ€è¦å¤æ‚æ¨ç†çš„é•¿å‘¨æœŸä»»åŠ¡ä¸­è¡¨ç°å°¤ä¸ºå‡ºè‰²ã€‚æˆ‘ä»¬è¿›ä¸€æ­¥é€šè¿‡åœ¨å®é™…æœºå™¨äººä¸Šè¿›è¡Œéƒ¨ç½²æ¥å±•ç¤ºå…¶åœ¨ç°å®ä¸–ç•Œä¸­çš„é€‚ç”¨æ€§ï¼Œä»¥å¤„ç†é•¿å‘¨æœŸä»»åŠ¡ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2404.15190v2">PDF</a> 8 pages, 6 figures, published to ICRA 2025</p>
<p><strong>Summary</strong><br>åœ¨è‡ªç„¶è¯­è¨€æŒ‡ä»¤æ‰§è¡Œçš„ç¯å¢ƒä¸­ï¼Œé€šè¿‡é—®ç­”å¼è§„åˆ’æœºåˆ¶æ¥è§„åˆ’è¡ŒåŠ¨ã€‚æå‡ºäº†ä¸€ç§é›¶è®­ç»ƒçš„æ–¹å¼åˆ¶å®šè¡ŒåŠ¨è®¡åˆ’å¹¶æ‰§è¡Œå¯¼èˆªç­‰ä»»åŠ¡ï¼Œå®Œæˆäººæœºäº¤äº’åœºæ™¯ä¸‹çš„æŒ‡ä»¤æ‰§è¡Œä»»åŠ¡ã€‚åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹è¿›è¡Œè‡ªé—®è‡ªç­”æ¥ç”Ÿæˆä¸€ç³»åˆ—å­ç›®æ ‡åºåˆ—ï¼Œæ ¹æ®è§†è§‰åé¦ˆè¿›è¡Œè®¡åˆ’è°ƒæ•´ã€‚å®éªŒè¡¨æ˜è¯¥è§„åˆ’æœºåˆ¶æœ‰æ•ˆè¶…è¶Šäº†å½“å‰æœ€ä¼˜çš„è§„åˆ’æ¨¡å‹ã€‚å…¶åœ¨æœºå™¨äººé¢†åŸŸçš„å¤æ‚æ¨ç†é•¿å‘¨æœŸä»»åŠ¡å…·æœ‰æ½œåœ¨åº”ç”¨ä»·å€¼ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>EIFçš„ä»»åŠ¡æ˜¯æ‰§è¡Œè‡ªç„¶è¯­è¨€æŒ‡ä»¤å¹¶æ“ä½œç¯å¢ƒä¸­çš„ç‰©ä½“ã€‚å®ƒä¸»è¦é¢å¯¹çš„é—®é¢˜ä¹‹ä¸€æ˜¯æ„æˆå¼ä»»åŠ¡è§„åˆ’ï¼Œé€šå¸¸ç”¨æ ‡ç­¾æ•°æ®æ¥å®Œæˆçš„ä»»åŠ¡æ˜¯é€šè¿‡ç›‘ç£å­¦ä¹ æˆ–å°è§„æ¨¡çš„å®ä¾‹å­¦ä¹ å®Œæˆçš„ã€‚è¯¥æ–‡ç« åˆ™é‡‡ç”¨äº†ä¸åŒçš„ç­–ç•¥è§£å†³è¿™ä¸€é—®é¢˜ã€‚ </li>
<li>æ–‡ç« ä¸­å¼•å…¥äº†åä¸ºâ€œè‹æ ¼æ‹‰åº•å¼è§„åˆ’è€…â€çš„è‡ªæˆ‘é—®ç­”å¼çš„è§„åˆ’æ–¹å¼æ¥å®ç°é›¶æ ·æœ¬çš„è®­ç»ƒè®¡åˆ’çš„åˆ¶å®šã€‚å…¶é¦–å…ˆç”±å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è‡ªè¡Œäº§ç”Ÿé—®é¢˜å’Œç­”æ¡ˆï¼Œç”Ÿæˆä¸€ç³»åˆ—å­ç›®æ ‡åºåˆ—ã€‚ </li>
<li>åœ¨æ‰§è¡Œä»»åŠ¡è¿‡ç¨‹ä¸­ï¼Œé‡åˆ°æ„å¤–æƒ…å†µï¼ˆå¦‚éšœç¢ç‰©ï¼‰ï¼Œè‹æ ¼æ‹‰åº•å¼è§„åˆ’è€…ä¼šæ ¹æ®è§†è§‰åé¦ˆæ¥è°ƒæ•´å…¶è§„åˆ’çš„ç­–ç•¥å’Œæµç¨‹ï¼Œä»è€Œè¾¾åˆ°è‡ªæˆ‘çº æ­£å’Œæ”¹è¿›çš„æ•ˆæœã€‚è¿™ä¸€ç‰¹ç‚¹æ˜¯å®ƒåœ¨è§„åˆ’æ–¹é¢çš„ä¼˜ç‚¹ä¹‹ä¸€ã€‚ </li>
<li>è‹æ ¼æ‹‰åº•å¼è§„åˆ’è€…çš„ä¼˜åŠ¿åœ¨äºå…¶åœ¨ALFREDåŸºå‡†æµ‹è¯•ä¸Šçš„è¡¨ç°ä¼˜äºå½“å‰æœ€ä¼˜çš„è§„åˆ’æ¨¡å‹ï¼Œç‰¹åˆ«æ˜¯åœ¨éœ€è¦å¤æ‚æ¨ç†çš„é•¿å‘¨æœŸä»»åŠ¡ä¸Šè¡¨ç°å°¤ä¸ºå‡ºè‰²ã€‚ </li>
<li>æœ€åï¼Œæ–‡ç« å±•ç¤ºäº†è‹æ ¼æ‹‰åº•å¼è§„åˆ’è€…åœ¨ç°å®ä¸–ç•Œçš„æœºå™¨äººä»»åŠ¡ä¸­çš„é€‚ç”¨æ€§ï¼Œè¯æ˜äº†å…¶åœ¨å®é™…åº”ç”¨ä¸­çš„ä»·å€¼ã€‚ </li>
<li>è‹æ ¼æ‹‰åº•å¼è§„åˆ’è€…å…·æœ‰å¤„ç†æœªçŸ¥éšœç¢çš„èƒ½åŠ›ï¼Œä½¿å¾—å…¶åœ¨æ‰§è¡Œä»»åŠ¡æ—¶æ›´åŠ çµæ´»å’Œé€‚åº”ç¯å¢ƒå˜åŒ–çš„èƒ½åŠ›æ›´å¼ºã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2404.15190">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-aed09dceacbeba30e005102791cf2e51.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-3ad71a38d0043d65cf320efb127bfbc4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f161ec60c2a98b126a65f3573f8a637e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ab1d4affde29392a1d0d63f38202879e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a255a2a490420620abc60def3dc874ff.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0499f2a3e8a2f5b5f50187b6c214573d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-80b16fab23e6e66590a8c741b45a9f31.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-09b194be49d089e748eebdac3679a125.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0e5aaf11a669b863ebf91ec5e26e5b41.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-03-28/Few-Shot/">https://kedreamix.github.io/Talk2Paper/Paper/2025-03-28/Few-Shot/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Few-Shot/">
                                    <span class="chip bg-color">Few-Shot</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-03-28/I2I%20Translation/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-2567334efa197ea9ec8efd0c31a1c361.jpg" class="responsive-img" alt="I2I Translation">
                        
                        <span class="card-title">I2I Translation</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            I2I Translation æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-03-28  Scale-Equivariant Imaging Self-Supervised Learning for Image   Super-Resolution and Deblurring
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-03-28
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/I2I-Translation/" class="post-category">
                                    I2I Translation
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/I2I-Translation/">
                        <span class="chip bg-color">I2I Translation</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-03-28/Agent/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-ab30b0eba029d65e0ca1119bd6e746ce.jpg" class="responsive-img" alt="Agent">
                        
                        <span class="card-title">Agent</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Agent æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-03-28  Feature4X Bridging Any Monocular Video to 4D Agentic AI with Versatile   Gaussian Feature Fields
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-03-28
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Agent/" class="post-category">
                                    Agent
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Agent/">
                        <span class="chip bg-color">Agent</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">23251k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
