<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Diffusion Models">
    <meta name="description" content="Diffusion Models æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-03-28  RecTable Fast Modeling Tabular Data with Rectified Flow">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Diffusion Models | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pic1.zhimg.com/v2-baa866e41c9ab04e01cd17d20f2995b4.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Diffusion Models</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Diffusion-Models/">
                                <span class="chip bg-color">Diffusion Models</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Diffusion-Models/" class="post-category">
                                Diffusion Models
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-03-28
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-04-21
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    18.1k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    73 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-03-28-æ›´æ–°"><a href="#2025-03-28-æ›´æ–°" class="headerlink" title="2025-03-28 æ›´æ–°"></a>2025-03-28 æ›´æ–°</h1><h2 id="RecTable-Fast-Modeling-Tabular-Data-with-Rectified-Flow"><a href="#RecTable-Fast-Modeling-Tabular-Data-with-Rectified-Flow" class="headerlink" title="RecTable: Fast Modeling Tabular Data with Rectified Flow"></a>RecTable: Fast Modeling Tabular Data with Rectified Flow</h2><p><strong>Authors:Masane Fuchi, Tomohiro Takagi</strong></p>
<p>Score-based or diffusion models generate high-quality tabular data, surpassing GAN-based and VAE-based models. However, these methods require substantial training time. In this paper, we introduce RecTable, which uses the rectified flow modeling, applied in such as text-to-image generation and text-to-video generation. RecTable features a simple architecture consisting of a few stacked gated linear unit blocks. Additionally, our training strategies are also simple, incorporating a mixed-type noise distribution and a logit-normal timestep distribution. Our experiments demonstrate that RecTable achieves competitive performance compared to the several state-of-the-art diffusion and score-based models while reducing the required training time. Our code is available at <a target="_blank" rel="noopener" href="https://github.com/fmp453/rectable">https://github.com/fmp453/rectable</a>. </p>
<blockquote>
<p>åŸºäºåˆ†æ•°çš„æ¨¡å‹æˆ–æ‰©æ•£æ¨¡å‹ç”Ÿæˆäº†é«˜è´¨é‡è¡¨æ ¼æ•°æ®ï¼Œè¶…è¶Šäº†åŸºäºGANå’ŒVAEçš„æ¨¡å‹ã€‚ç„¶è€Œï¼Œè¿™äº›æ–¹æ³•éœ€è¦å¤§é‡çš„è®­ç»ƒæ—¶é—´ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬å¼•å…¥äº†RecTableï¼Œå®ƒä½¿ç”¨æ ¡æ­£æµå»ºæ¨¡ï¼Œåº”ç”¨äºæ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆå’Œæ–‡æœ¬åˆ°è§†é¢‘ç”Ÿæˆç­‰é¢†åŸŸã€‚RecTableå…·æœ‰ç®€å•çš„æ¶æ„ï¼Œç”±å‡ ä¸ªå †å çš„é—¨æ§çº¿æ€§å•å…ƒå—ç»„æˆã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬çš„è®­ç»ƒç­–ç•¥ä¹Ÿå¾ˆç®€å•ï¼Œèå…¥äº†æ··åˆç±»å‹çš„å™ªå£°åˆ†å¸ƒå’Œé€»è¾‘æ—¶é—´æ­¥é•¿åˆ†å¸ƒã€‚æˆ‘ä»¬çš„å®éªŒè¡¨æ˜ï¼ŒRecTableåœ¨å‡å°‘æ‰€éœ€è®­ç»ƒæ—¶é—´çš„åŒæ—¶ï¼Œä¸å‡ ç§å…ˆè¿›çš„æ‰©æ•£æ¨¡å‹å’ŒåŸºäºåˆ†æ•°çš„æ¨¡å‹ç›¸æ¯”å…·æœ‰ç«äº‰åŠ›ã€‚æˆ‘ä»¬çš„ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/kmp453/rectable">https://github.com/kmp453/rectable</a>æ‰¾åˆ°ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.20731v1">PDF</a> 19 pages, 7 figures, 10 tables</p>
<p><strong>Summary</strong></p>
<p>æ‰©æ•£æ¨¡å‹åœ¨ç”Ÿæˆé«˜è´¨é‡è¡¨æ ¼æ•°æ®æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œè¶…è¶ŠGANå’ŒVAEæ¨¡å‹ã€‚ç„¶è€Œï¼Œè¿™äº›æ–¹æ³•éœ€è¦å¤§é‡è®­ç»ƒæ—¶é—´ã€‚æœ¬æ–‡æå‡ºä½¿ç”¨ä¿®æ­£æµå»ºæ¨¡çš„RecTableæ–¹æ³•ï¼Œç”¨äºæ–‡æœ¬åˆ°å›¾åƒå’Œæ–‡æœ¬åˆ°è§†é¢‘ç”Ÿæˆã€‚RecTableå…·æœ‰ç®€å•çš„æ¶æ„å’Œè®­ç»ƒç­–ç•¥ï¼ŒåŒ…æ‹¬å‡ ä¸ªå †å çš„é—¨æ§çº¿æ€§å•å…ƒå—ï¼Œé‡‡ç”¨æ··åˆç±»å‹çš„å™ªå£°åˆ†å¸ƒå’Œå¯¹æ•°æ­£æ€åˆ†å¸ƒçš„æ—¶é—´æ­¥é•¿åˆ†å¸ƒã€‚å®éªŒè¯æ˜ï¼ŒRecTableåœ¨å‡å°‘è®­ç»ƒæ—¶é—´çš„åŒæ—¶ï¼Œå®ç°äº†ä¸å¤šä¸ªæœ€å…ˆè¿›çš„æ‰©æ•£å’Œè¯„åˆ†æ¨¡å‹ç›¸æ¯”å…·æœ‰ç«äº‰åŠ›çš„æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ‰©æ•£æ¨¡å‹åœ¨ç”Ÿæˆé«˜è´¨é‡è¡¨æ ¼æ•°æ®ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œè¶…è¶ŠGANå’ŒVAEæ¨¡å‹ã€‚</li>
<li>æœ¬æ–‡æå‡ºRecTableæ–¹æ³•ï¼Œä½¿ç”¨ä¿®æ­£æµå»ºæ¨¡ï¼Œé€‚ç”¨äºæ–‡æœ¬åˆ°å›¾åƒå’Œæ–‡æœ¬åˆ°è§†é¢‘ç”Ÿæˆã€‚</li>
<li>RecTableå…·æœ‰ç®€å•çš„æ¶æ„ï¼Œç”±å‡ ä¸ªå †å çš„é—¨æ§çº¿æ€§å•å…ƒå—ç»„æˆã€‚</li>
<li>é‡‡ç”¨æ··åˆç±»å‹çš„å™ªå£°åˆ†å¸ƒå’Œå¯¹æ•°æ­£æ€åˆ†å¸ƒçš„æ—¶é—´æ­¥é•¿åˆ†å¸ƒä½œä¸ºè®­ç»ƒç­–ç•¥ã€‚</li>
<li>å®éªŒè¯æ˜RecTableåœ¨å‡å°‘è®­ç»ƒæ—¶é—´çš„åŒæ—¶å®ç°äº†ä¸å…ˆè¿›æ¨¡å‹ç›¸å½“çš„ç«äº‰åŠ›ã€‚</li>
<li>RecTableçš„ä»£ç å·²å…¬å¼€åœ¨GitHubä¸Šï¼Œæ–¹ä¾¿å…¶ä»–ç ”ç©¶è€…ä½¿ç”¨å’Œæ”¹è¿›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.20731">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-6ad482922d6b17fc07d4d076e9d3cddf.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c6d0987104919edf790662e9166b03eb.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-7efbcbebc18bc59bd7df336a8f2e39b6.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-cc9773d7c9e30770810c842596671ad7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7a435d300d74200965ee7435ed732244.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="MMGen-Unified-Multi-modal-Image-Generation-and-Understanding-in-One-Go"><a href="#MMGen-Unified-Multi-modal-Image-Generation-and-Understanding-in-One-Go" class="headerlink" title="MMGen: Unified Multi-modal Image Generation and Understanding in One Go"></a>MMGen: Unified Multi-modal Image Generation and Understanding in One Go</h2><p><strong>Authors:Jiepeng Wang, Zhaoqing Wang, Hao Pan, Yuan Liu, Dongdong Yu, Changhu Wang, Wenping Wang</strong></p>
<p>A unified diffusion framework for multi-modal generation and understanding has the transformative potential to achieve seamless and controllable image diffusion and other cross-modal tasks. In this paper, we introduce MMGen, a unified framework that integrates multiple generative tasks into a single diffusion model. This includes: (1) multi-modal category-conditioned generation, where multi-modal outputs are generated simultaneously through a single inference process, given category information; (2) multi-modal visual understanding, which accurately predicts depth, surface normals, and segmentation maps from RGB images; and (3) multi-modal conditioned generation, which produces corresponding RGB images based on specific modality conditions and other aligned modalities. Our approach develops a novel diffusion transformer that flexibly supports multi-modal output, along with a simple modality-decoupling strategy to unify various tasks. Extensive experiments and applications demonstrate the effectiveness and superiority of MMGen across diverse tasks and conditions, highlighting its potential for applications that require simultaneous generation and understanding. </p>
<blockquote>
<p>ä¸€ä¸ªç»Ÿä¸€çš„æ‰©æ•£æ¡†æ¶ï¼Œç”¨äºå¤šæ¨¡å¼ç”Ÿæˆå’Œç†è§£ï¼Œå…·æœ‰å®ç°æ— ç¼å’Œå¯æ§çš„å›¾åƒæ‰©æ•£å’Œå…¶ä»–è·¨æ¨¡å¼ä»»åŠ¡çš„å˜é©æ½œåŠ›ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†MMGenï¼Œè¿™æ˜¯ä¸€ä¸ªç»Ÿä¸€çš„æ¡†æ¶ï¼Œå®ƒå°†å¤šä¸ªç”Ÿæˆä»»åŠ¡é›†æˆåˆ°ä¸€ä¸ªå•ä¸€çš„æ‰©æ•£æ¨¡å‹ä¸­ã€‚è¿™åŒ…æ‹¬ï¼š(1)å¤šæ¨¡å¼ç±»åˆ«æ¡ä»¶ç”Ÿæˆï¼Œé€šè¿‡å•æ¨ç†è¿‡ç¨‹ç»™å®šç±»åˆ«ä¿¡æ¯ï¼ŒåŒæ—¶ç”Ÿæˆå¤šæ¨¡å¼è¾“å‡ºï¼›(2)å¤šæ¨¡å¼è§†è§‰ç†è§£ï¼Œä»RGBå›¾åƒå‡†ç¡®é¢„æµ‹æ·±åº¦ã€è¡¨é¢æ³•çº¿å’Œåˆ†å‰²å›¾ï¼›(3)å¤šæ¨¡å¼æ¡ä»¶ç”Ÿæˆï¼Œæ ¹æ®ç‰¹å®šæ¨¡å¼æ¡ä»¶å’Œå…¶ä»–å¯¹é½æ¨¡å¼ç”Ÿæˆç›¸åº”çš„RGBå›¾åƒã€‚æˆ‘ä»¬çš„æ–¹æ³•å¼€å‘äº†ä¸€ç§çµæ´»æ”¯æŒå¤šæ¨¡å¼è¾“å‡ºçš„æ–°å‹æ‰©æ•£å˜å‹å™¨ï¼Œä»¥åŠä¸€ç§ç®€å•çš„æ¨¡æ€è§£è€¦ç­–ç•¥æ¥ç»Ÿä¸€å„ç§ä»»åŠ¡ã€‚å¹¿æ³›çš„å®éªŒå’Œåº”ç”¨è¡¨æ˜ï¼ŒMMGenåœ¨ä¸åŒä»»åŠ¡å’Œæ¡ä»¶ä¸‹æ•ˆæœæ˜¾è‘—ã€ä¼˜è¶Šæ€§æ˜æ˜¾ï¼Œçªå‡ºäº†å…¶åœ¨éœ€è¦åŒæ—¶ç”Ÿæˆå’Œç†è§£çš„åº”ç”¨ä¸­çš„æ½œåŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.20644v1">PDF</a> Our project page: <a target="_blank" rel="noopener" href="https://jiepengwang.github.io/MMGen/">https://jiepengwang.github.io/MMGen/</a></p>
<p><strong>Summary</strong></p>
<p>å¤šæ¨¡æ€æ‰©æ•£æ¡†æ¶MMGenç»Ÿä¸€äº†å¤šæ¨¡æ€ç”Ÿæˆä¸ç†è§£ä»»åŠ¡ï¼Œå®ç°æ— ç¼å¯æ§çš„å›¾åƒæ‰©æ•£å’Œå…¶ä»–è·¨æ¨¡æ€ä»»åŠ¡ã€‚è¯¥æ¡†æ¶é€šè¿‡å•ä¸€æ¨ç†è¿‡ç¨‹ç”Ÿæˆå¤šæ¨¡æ€è¾“å‡ºï¼Œæ”¯æŒå¤šæ¨¡æ€è§†è§‰ç†è§£å¹¶äº§ç”ŸåŸºäºç‰¹å®šæ¨¡æ€æ¡ä»¶å’Œå…¶ä»–å¯¹é½æ¨¡æ€çš„å¯¹åº”RGBå›¾åƒã€‚MMGenæ¡†æ¶é€šè¿‡å¼€å‘æ”¯æŒå¤šæ¨¡æ€è¾“å‡ºçš„æ–°å‹æ‰©æ•£å˜å‹å™¨å’Œç®€å•çš„æ¨¡æ€è§£è€¦ç­–ç•¥ï¼Œå®ç°äº†å„ç§ä»»åŠ¡çš„ç»Ÿä¸€ã€‚å®éªŒå’Œåº”ç”¨ç¨‹åºè¯æ˜äº†MMGenåœ¨ä¸åŒä»»åŠ¡å’Œæ¡ä»¶ä¸‹çš„æœ‰æ•ˆæ€§å’Œä¼˜è¶Šæ€§ï¼Œå±•ç°å‡ºå…¶åœ¨éœ€è¦åŒæ—¶ç”Ÿæˆå’Œç†è§£çš„åº”ç”¨ä¸­çš„æ½œåŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>MMGenæ˜¯ä¸€ä¸ªç»Ÿä¸€çš„å¤šæ¨¡æ€æ‰©æ•£æ¡†æ¶ï¼Œèƒ½å¤Ÿæ•´åˆå¤šç§ç”Ÿæˆä»»åŠ¡ã€‚</li>
<li>è¯¥æ¡†æ¶æ”¯æŒå¤šæ¨¡æ€è¾“å‡ºï¼Œé€šè¿‡å•ä¸€æ¨ç†è¿‡ç¨‹ç”Ÿæˆå¤šæ¨¡æ€ç»“æœã€‚</li>
<li>MMGenå®ç°äº†å¤šæ¨¡æ€è§†è§‰ç†è§£ï¼Œèƒ½å¤Ÿä»RGBå›¾åƒä¸­å‡†ç¡®é¢„æµ‹æ·±åº¦ã€è¡¨é¢æ³•çº¿å’Œåˆ†å‰²å›¾ã€‚</li>
<li>è¯¥æ¡†æ¶æ”¯æŒåŸºäºç‰¹å®šæ¨¡æ€æ¡ä»¶å’Œå…¶ä»–å¯¹é½æ¨¡æ€çš„ç”Ÿæˆã€‚</li>
<li>MMGené€šè¿‡å¼€å‘æ–°å‹æ‰©æ•£å˜å‹å™¨å’Œæ¨¡æ€è§£è€¦ç­–ç•¥ï¼Œå®ç°äº†ä¸åŒä»»åŠ¡çš„ç»Ÿä¸€ã€‚</li>
<li>å®éªŒå’Œåº”ç”¨ç¨‹åºè¯æ˜äº†MMGençš„æœ‰æ•ˆæ€§å’Œä¼˜è¶Šæ€§ï¼Œå±•ç°å‡ºå…¶åœ¨å®é™…åº”ç”¨ä¸­çš„æ½œåŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.20644">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-b7c47789f0035c811b50f75e9fb4c29a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-893493cbbde8b9321e327d1df7db53b0.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-73379c893cffba66843233ac4e5eef26.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="TD-BFR-Truncated-Diffusion-Model-for-Efficient-Blind-Face-Restoration"><a href="#TD-BFR-Truncated-Diffusion-Model-for-Efficient-Blind-Face-Restoration" class="headerlink" title="TD-BFR: Truncated Diffusion Model for Efficient Blind Face Restoration"></a>TD-BFR: Truncated Diffusion Model for Efficient Blind Face Restoration</h2><p><strong>Authors:Ziying Zhang, Xiang Gao, Zhixin Wang, Qiang hu, Xiaoyun Zhang</strong></p>
<p>Diffusion-based methodologies have shown significant potential in blind face restoration (BFR), leveraging their robust generative capabilities. However, they are often criticized for two significant problems: 1) slow training and inference speed, and 2) inadequate recovery of fine-grained facial details. To address these problems, we propose a novel Truncated Diffusion model for efficient Blind Face Restoration (TD-BFR), a three-stage paradigm tailored for the progressive resolution of degraded images. Specifically, TD-BFR utilizes an innovative truncated sampling method, starting from low-quality (LQ) images at low resolution to enhance sampling speed, and then introduces an adaptive degradation removal module to handle unknown degradations and connect the generation processes across different resolutions. Additionally, we further adapt the priors of pre-trained diffusion models to recover rich facial details. Our method efficiently restores high-quality images in a coarse-to-fine manner and experimental results demonstrate that TD-BFR is, on average, \textbf{4.75$\times$} faster than current state-of-the-art diffusion-based BFR methods while maintaining competitive quality. </p>
<blockquote>
<p>åŸºäºæ‰©æ•£çš„æ–¹æ³•åœ¨ç›²è„¸ä¿®å¤ï¼ˆBFRï¼‰ä¸­æ˜¾ç¤ºå‡ºå·¨å¤§çš„æ½œåŠ›ï¼Œåˆ©ç”¨å…¶å¼ºå¤§çš„ç”Ÿæˆèƒ½åŠ›ã€‚ç„¶è€Œï¼Œå®ƒä»¬ç»å¸¸å› ä¸¤ä¸ªé—®é¢˜è€Œå—åˆ°æ‰¹è¯„ï¼š1ï¼‰è®­ç»ƒå’Œæ¨ç†é€Ÿåº¦æ…¢ï¼›2ï¼‰å¯¹ç»†å¾®çš„é¢éƒ¨ç»†èŠ‚æ¢å¤ä¸è¶³ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ç”¨äºé«˜æ•ˆç›²è„¸ä¿®å¤ï¼ˆTD-BFRï¼‰çš„æ–°å‹æˆªæ–­æ‰©æ•£æ¨¡å‹ï¼Œè¿™æ˜¯ä¸€ä¸ªé’ˆå¯¹é€€åŒ–å›¾åƒé€æ­¥æ¢å¤çš„é‡èº«å®šåˆ¶çš„ä¸‰é˜¶æ®µèŒƒå¼ã€‚å…·ä½“æ¥è¯´ï¼ŒTD-BFRåˆ©ç”¨ä¸€ç§åˆ›æ–°æ€§çš„æˆªæ–­é‡‡æ ·æ–¹æ³•ï¼Œä»ä½åˆ†è¾¨ç‡çš„ä½è´¨é‡ï¼ˆLQï¼‰å›¾åƒå¼€å§‹æé«˜é‡‡æ ·é€Ÿåº¦ï¼Œç„¶åå¼•å…¥è‡ªé€‚åº”é€€åŒ–æ¶ˆé™¤æ¨¡å—æ¥å¤„ç†æœªçŸ¥çš„é€€åŒ–å¹¶è¿æ¥ä¸åŒåˆ†è¾¨ç‡çš„ç”Ÿæˆè¿‡ç¨‹ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿›ä¸€æ­¥è°ƒæ•´é¢„è®­ç»ƒæ‰©æ•£æ¨¡å‹çš„å…ˆéªŒçŸ¥è¯†ä»¥æ¢å¤ä¸°å¯Œçš„é¢éƒ¨ç»†èŠ‚ã€‚æˆ‘ä»¬çš„æ–¹æ³•ä»¥ç”±ç²—åˆ°ç»†çš„æ–¹å¼é«˜æ•ˆåœ°æ¢å¤é«˜è´¨é‡å›¾åƒï¼Œå®éªŒç»“æœè¡¨æ˜ï¼ŒTD-BFRå¹³å‡æ¯”å½“å‰æœ€å…ˆè¿›çš„åŸºäºæ‰©æ•£çš„BFRæ–¹æ³•å¿«4.75å€ï¼ŒåŒæ—¶ä¿æŒç«äº‰æ€§çš„è´¨é‡ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.20537v1">PDF</a> Accepted by ICME 2025</p>
<p><strong>Summary</strong><br>     æˆªæ–­æ‰©æ•£æ¨¡å‹åœ¨ç›²è„¸ä¿®å¤ä¸­æœ‰æ˜¾è‘—ä¼˜åŠ¿ï¼Œå…¶åˆ©ç”¨ç”Ÿæˆèƒ½åŠ›è§£å†³ä¼ ç»Ÿæ‰©æ•£æ¨¡å‹çš„è®­ç»ƒä¸æ¨ç†é€Ÿåº¦æ…¢åŠé¢éƒ¨ç»†èŠ‚æ¢å¤ä¸è¶³çš„é—®é¢˜ã€‚æ–°æ–¹æ³•é‡‡ç”¨ä¸‰é˜¶æ®µæ¨¡å¼ï¼Œä»ä½è´¨é‡å›¾åƒå¼€å§‹ï¼Œæå‡é‡‡æ ·é€Ÿåº¦ï¼Œå¼•å…¥è‡ªé€‚åº”å»å™ªæ¨¡å—å¤„ç†æœªçŸ¥é€€åŒ–é—®é¢˜ï¼Œå¹¶é€‚åº”é¢„è®­ç»ƒæ‰©æ•£æ¨¡å‹çš„å…ˆéªŒä¿¡æ¯ä»¥æ¢å¤ä¸°å¯Œçš„é¢éƒ¨ç»†èŠ‚ã€‚æ­¤æ–¹æ³•èƒ½ä»¥ç²—åˆ°ç»†çš„æ–¹å¼é«˜æ•ˆæ¢å¤é«˜è´¨é‡å›¾åƒï¼Œä¸”å¹³å‡é€Ÿåº¦æ˜¯å½“å‰å…ˆè¿›æ‰©æ•£æ¨¡å‹çš„4.75å€ï¼ŒåŒæ—¶ä¿æŒç«äº‰åŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æˆªæ–­æ‰©æ•£æ¨¡å‹åœ¨ç›²è„¸ä¿®å¤ä¸­æœ‰æ˜¾è‘—æ½œåŠ›ã€‚</li>
<li>è¯¥æ¨¡å‹è§£å†³äº†ä¼ ç»Ÿæ‰©æ•£æ¨¡å‹çš„è®­ç»ƒä¸æ¨ç†é€Ÿåº¦æ…¢çš„é—®é¢˜ã€‚</li>
<li>æ¨¡å‹èƒ½å¤Ÿåˆ©ç”¨ç”Ÿæˆèƒ½åŠ›å¤„ç†æœªçŸ¥é€€åŒ–é—®é¢˜ã€‚</li>
<li>é€šè¿‡å¼•å…¥è‡ªé€‚åº”å»å™ªæ¨¡å—ï¼Œæ¨¡å‹åœ¨å»é™¤å›¾åƒé€€åŒ–æ–¹é¢è¡¨ç°å‡ºè‰²ã€‚</li>
<li>æ–°æ–¹æ³•é‡‡ç”¨ä¸‰é˜¶æ®µæ¨¡å¼ï¼Œå®ç°ä»ä½åˆ†è¾¨ç‡åˆ°é«˜åˆ†è¾¨ç‡çš„æ¸è¿›å¼å›¾åƒæ¢å¤ã€‚</li>
<li>æ¨¡å‹é€‚åº”é¢„è®­ç»ƒæ‰©æ•£æ¨¡å‹çš„å…ˆéªŒä¿¡æ¯ä»¥æ¢å¤ä¸°å¯Œçš„é¢éƒ¨ç»†èŠ‚ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.20537">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-d6a5161a4d0e5eb29fbb9c6cf1ef48c6.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a9c626be97e23f7f037ecd54e94ab4d8.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-39e420b6036bf155bd06252b27139145.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-011d35e565bfb67180f93f884581c253.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e3c1d3f28d41625194fcfe88b20537af.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-076cbcf153fad7044be74ade13fba9c8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2d86db3aba0c539d1bdf09e68c978326.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a8d672350d0d55c2e3733ef7ab52951b.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Dissecting-and-Mitigating-Diffusion-Bias-via-Mechanistic-Interpretability"><a href="#Dissecting-and-Mitigating-Diffusion-Bias-via-Mechanistic-Interpretability" class="headerlink" title="Dissecting and Mitigating Diffusion Bias via Mechanistic   Interpretability"></a>Dissecting and Mitigating Diffusion Bias via Mechanistic   Interpretability</h2><p><strong>Authors:Yingdong Shi, Changming Li, Yifan Wang, Yongxiang Zhao, Anqi Pang, Sibei Yang, Jingyi Yu, Kan Ren</strong></p>
<p>Diffusion models have demonstrated impressive capabilities in synthesizing diverse content. However, despite their high-quality outputs, these models often perpetuate social biases, including those related to gender and race. These biases can potentially contribute to harmful real-world consequences, reinforcing stereotypes and exacerbating inequalities in various social contexts. While existing research on diffusion bias mitigation has predominantly focused on guiding content generation, it often neglects the intrinsic mechanisms within diffusion models that causally drive biased outputs. In this paper, we investigate the internal processes of diffusion models, identifying specific decision-making mechanisms, termed bias features, embedded within the model architecture. By directly manipulating these features, our method precisely isolates and adjusts the elements responsible for bias generation, permitting granular control over the bias levels in the generated content. Through experiments on both unconditional and conditional diffusion models across various social bias attributes, we demonstrate our methodâ€™s efficacy in managing generation distribution while preserving image quality. We also dissect the discovered model mechanism, revealing different intrinsic features controlling fine-grained aspects of generation, boosting further research on mechanistic interpretability of diffusion models. </p>
<blockquote>
<p>æ‰©æ•£æ¨¡å‹åœ¨åˆæˆå¤šæ ·åŒ–å†…å®¹æ–¹é¢å±•ç¤ºäº†ä»¤äººå°è±¡æ·±åˆ»çš„èƒ½åŠ›ã€‚ç„¶è€Œï¼Œå°½ç®¡å®ƒä»¬çš„è¾“å‡ºè´¨é‡å¾ˆé«˜ï¼Œä½†è¿™äº›æ¨¡å‹å¾€å¾€ä¼šå»¶ç»­ç¤¾ä¼šåè§ï¼ŒåŒ…æ‹¬ä¸æ€§åˆ«å’Œç§æ—æœ‰å…³çš„åè§ã€‚è¿™äº›åè§å¯èƒ½å¯¼è‡´æœ‰å®³çš„ç°å®ä¸–ç•Œåæœï¼Œå¼ºåŒ–åˆ»æ¿å°è±¡å¹¶åŠ å‰§å„ç§ç¤¾ä¼šèƒŒæ™¯ä¸‹çš„ä¸å¹³ç­‰ã€‚å°½ç®¡ç°æœ‰çš„å…³äºæ‰©æ•£åè§ç¼“è§£çš„ç ”ç©¶ä¸»è¦é›†ä¸­åœ¨å¼•å¯¼å†…å®¹ç”Ÿæˆä¸Šï¼Œä½†å®ƒå¾€å¾€å¿½è§†äº†æ‰©æ•£æ¨¡å‹å†…éƒ¨çš„å›ºæœ‰æœºåˆ¶ï¼Œè¿™äº›æœºåˆ¶æ˜¯é©±åŠ¨åè§è¾“å‡ºçš„æ ¹æœ¬åŸå› ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬ç ”ç©¶äº†æ‰©æ•£æ¨¡å‹çš„å†…éƒ¨è¿‡ç¨‹ï¼Œè¯†åˆ«å‡ºåµŒå…¥åœ¨æ¨¡å‹æ¶æ„ä¸­çš„ç‰¹å®šå†³ç­–æœºåˆ¶ï¼Œç§°ä¸ºåè§ç‰¹å¾ã€‚é€šè¿‡ç›´æ¥æ“ä½œè¿™äº›ç‰¹å¾ï¼Œæˆ‘ä»¬çš„æ–¹æ³•ç²¾ç¡®åœ°éš”ç¦»å¹¶è°ƒæ•´äº†è´Ÿè´£äº§ç”Ÿåè§çš„å…ƒç´ ï¼Œå®ç°å¯¹ç”Ÿæˆå†…å®¹ä¸­åè§æ°´å¹³çš„ç²¾ç»†æ§åˆ¶ã€‚æˆ‘ä»¬åœ¨æ— æ¡ä»¶æ‰©æ•£æ¨¡å‹å’Œæ¡ä»¶æ‰©æ•£æ¨¡å‹ä¸Šè¿›è¡Œäº†å„ç§ç¤¾ä¼šåè§å±æ€§çš„å®éªŒï¼Œè¯æ˜äº†æˆ‘ä»¬çš„æ–¹æ³•åœ¨æ§åˆ¶ç”Ÿæˆåˆ†å¸ƒçš„åŒæ—¶ä¿æŒå›¾åƒè´¨é‡çš„æœ‰æ•ˆæ€§ã€‚æˆ‘ä»¬è¿˜å‰–æäº†å‘ç°çš„æ¨¡å‹æœºåˆ¶ï¼Œæ­ç¤ºäº†æ§åˆ¶ç”Ÿæˆç²¾ç»†æ–¹é¢çš„ä¸åŒå†…åœ¨ç‰¹å¾ï¼Œæ¨åŠ¨äº†æ‰©æ•£æ¨¡å‹æœºåˆ¶æ€§è§£é‡Šçš„ç ”ç©¶ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.20483v1">PDF</a> CVPR 2025; Project Page:   <a target="_blank" rel="noopener" href="https://foundation-model-research.github.io/difflens">https://foundation-model-research.github.io/difflens</a></p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ¢è®¨äº†æ‰©æ•£æ¨¡å‹åœ¨åˆæˆå†…å®¹æ—¶çš„å¼ºå¤§èƒ½åŠ›ï¼Œä½†åŒæ—¶ä¹ŸæŒ‡å‡ºäº†è¿™äº›æ¨¡å‹åœ¨è¾“å‡ºä¸­ç»å¸¸å­˜åœ¨çš„ç¤¾ä¼šåè§é—®é¢˜ï¼ŒåŒ…æ‹¬æ€§åˆ«å’Œç§æ—åè§ã€‚æ–‡ç« æ·±å…¥ç ”ç©¶äº†æ‰©æ•£æ¨¡å‹çš„å†…éƒ¨è¿‡ç¨‹ï¼Œè¯†åˆ«å‡ºæ¨¡å‹æ¶æ„ä¸­çš„åè§ç‰¹å¾ï¼Œå¹¶æå‡ºé€šè¿‡ç›´æ¥æ“ä½œè¿™äº›ç‰¹å¾æ¥ç²¾ç¡®éš”ç¦»å’Œè°ƒæ•´åè§ç”Ÿæˆå…ƒç´ çš„æ–¹æ³•ã€‚å®éªŒè¯æ˜è¯¥æ–¹æ³•åœ¨æ§åˆ¶ç”Ÿæˆå†…å®¹çš„åè§æ°´å¹³çš„åŒæ—¶ï¼Œèƒ½å¤Ÿä¿æŒå›¾åƒè´¨é‡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ‰©æ•£æ¨¡å‹è™½èƒ½åˆæˆå¤šæ ·å†…å®¹ï¼Œä½†å¸¸å­˜åœ¨ç¤¾ä¼šåè§ï¼ŒåŒ…æ‹¬æ€§åˆ«å’Œç§æ—åè§ã€‚</li>
<li>ç°æœ‰ç ”ç©¶å¤šå…³æ³¨æŒ‡å¯¼å†…å®¹ç”Ÿæˆæ¥å‡è½»æ‰©æ•£åè§ï¼Œä½†å¿½ç•¥äº†æ¨¡å‹å†…éƒ¨æœºåˆ¶ã€‚</li>
<li>æœ¬æ–‡è¯†åˆ«å‡ºæ‰©æ•£æ¨¡å‹æ¶æ„ä¸­çš„åè§ç‰¹å¾ï¼Œå³å†³ç­–åˆ¶å®šæœºåˆ¶ã€‚</li>
<li>é€šè¿‡ç›´æ¥æ“ä½œè¿™äº›åè§ç‰¹å¾ï¼Œèƒ½ç²¾ç¡®éš”ç¦»å’Œè°ƒæ•´ç”Ÿæˆå†…å®¹ä¸­çš„åè§å…ƒç´ ã€‚</li>
<li>å®éªŒè¯æ˜è¯¥æ–¹æ³•åœ¨æ§åˆ¶ç”Ÿæˆå†…å®¹çš„åè§æ°´å¹³çš„åŒæ—¶ï¼Œèƒ½å¤Ÿä¿æŒå›¾åƒè´¨é‡ã€‚</li>
<li>æ–‡ç« å‰–æäº†å‘ç°çš„æ¨¡å‹æœºåˆ¶ï¼Œæ­ç¤ºäº†æ§åˆ¶ç”Ÿæˆç»†èŠ‚çš„å†…åœ¨ç‰¹å¾ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.20483">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-c13c894bb942404606f1e5682f086387.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7bfd20c40a1406776529eaa7ba32ecc4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9085f0f7650d32a0f3165d0258167605.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Latent-Beam-Diffusion-Models-for-Decoding-Image-Sequences"><a href="#Latent-Beam-Diffusion-Models-for-Decoding-Image-Sequences" class="headerlink" title="Latent Beam Diffusion Models for Decoding Image Sequences"></a>Latent Beam Diffusion Models for Decoding Image Sequences</h2><p><strong>Authors:Guilherme Fernandes, Vasco Ramos, Regev Cohen, Idan Szpektor, JoÃ£o MagalhÃ£es</strong></p>
<p>While diffusion models excel at generating high-quality images from text prompts, they struggle with visual consistency in image sequences. Existing methods generate each image independently, leading to disjointed narratives - a challenge further exacerbated in non-linear storytelling, where scenes must connect beyond adjacent frames. We introduce a novel beam search strategy for latent space exploration, enabling conditional generation of full image sequences with beam search decoding. Unlike prior approaches that use fixed latent priors, our method dynamically searches for an optimal sequence of latent representations, ensuring coherent visual transitions. To address beam searchâ€™s quadratic complexity, we integrate a cross-attention mechanism that efficiently scores search paths and enables pruning, prioritizing alignment with both textual prompts and visual context. Human evaluations confirm that our approach outperforms baseline methods, producing full sequences with superior coherence, visual continuity, and textual alignment. By bridging advances in search optimization and latent space refinement, this work sets a new standard for structured image sequence generation. </p>
<blockquote>
<p>æ‰©æ•£æ¨¡å‹è™½ç„¶åœ¨ä»æ–‡æœ¬æç¤ºç”Ÿæˆé«˜è´¨é‡å›¾åƒæ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œä½†åœ¨å›¾åƒåºåˆ—çš„è§†è§‰ä¸€è‡´æ€§æ–¹é¢å­˜åœ¨å›°éš¾ã€‚ç°æœ‰æ–¹æ³•ç‹¬ç«‹ç”Ÿæˆæ¯å¼ å›¾åƒï¼Œå¯¼è‡´å™äº‹ä¸è¿è´¯â€”â€”åœ¨éçº¿æ€§å™äº‹ä¸­ï¼Œåœºæ™¯å¿…é¡»åœ¨ç›¸é‚»å¸§ä¹‹å¤–å»ºç«‹è”ç³»ï¼Œè¿™ä¸€æŒ‘æˆ˜è¿›ä¸€æ­¥åŠ å‰§ã€‚æˆ‘ä»¬å¼•å…¥äº†ä¸€ç§æ–°å‹çš„æ½œåœ¨ç©ºé—´æ¢ç´¢å…‰æŸæœç´¢ç­–ç•¥ï¼Œé€šè¿‡å…‰æŸæœç´¢è§£ç å®ç°æœ‰æ¡ä»¶ç”Ÿæˆå®Œæ•´çš„å›¾åƒåºåˆ—ã€‚ä¸ä»¥å‰ä½¿ç”¨å›ºå®šæ½œåœ¨å…ˆéªŒçš„æ–¹æ³•ä¸åŒï¼Œæˆ‘ä»¬çš„æ–¹æ³•åŠ¨æ€æœç´¢æœ€ä¼˜æ½œåœ¨è¡¨ç¤ºåºåˆ—ï¼Œç¡®ä¿è§†è§‰è¿‡æ¸¡è¿è´¯ã€‚ä¸ºäº†è§£å†³å…‰æŸæœç´¢çš„äºŒæ¬¡å¤æ‚æ€§ï¼Œæˆ‘ä»¬æ•´åˆäº†äº¤å‰æ³¨æ„åŠ›æœºåˆ¶ï¼Œè¯¥æœºåˆ¶å¯ä»¥æœ‰æ•ˆåœ°è¯„åˆ†æœç´¢è·¯å¾„å¹¶å®ç°ä¿®å‰ªï¼Œä¼˜å…ˆä¸æ–‡æœ¬æç¤ºå’Œè§†è§‰ä¸Šä¸‹æ–‡å¯¹é½ã€‚äººç±»è¯„ä¼°è¯å®ï¼Œæˆ‘ä»¬çš„æ–¹æ³•ä¼˜äºåŸºçº¿æ–¹æ³•ï¼Œäº§ç”Ÿå…·æœ‰å“è¶Šè¿è´¯æ€§ã€è§†è§‰è¿è´¯æ€§å’Œæ–‡æœ¬å¯¹é½æ€§çš„å®Œæ•´åºåˆ—ã€‚é€šè¿‡ç»“åˆæœç´¢ä¼˜åŒ–å’Œæ½œåœ¨ç©ºé—´ç²¾åŒ–çš„è¿›æ­¥ï¼Œè¿™é¡¹å·¥ä½œä¸ºç»“æ„åŒ–å›¾åƒåºåˆ—ç”Ÿæˆè®¾å®šäº†æ–°çš„æ ‡å‡†ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.20429v1">PDF</a> </p>
<p><strong>Summary</strong><br>æ‰©æ•£æ¨¡å‹åœ¨æ ¹æ®æ–‡æœ¬æç¤ºç”Ÿæˆé«˜è´¨é‡å›¾åƒæ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œä½†åœ¨å›¾åƒåºåˆ—çš„è§†è§‰ä¸€è‡´æ€§æ–¹é¢å­˜åœ¨æŒ‘æˆ˜ã€‚ç°æœ‰æ–¹æ³•ç‹¬ç«‹ç”Ÿæˆæ¯å¼ å›¾åƒï¼Œå¯¼è‡´å™äº‹ä¸è¿è´¯ï¼Œç‰¹åˆ«æ˜¯åœ¨éçº¿æ€§å™äº‹ä¸­ï¼Œåœºæ™¯éœ€è¦åœ¨è¶…å‡ºç›¸é‚»å¸§ä¹‹å¤–è¿›è¡Œè¿æ¥ã€‚æœ¬ç ”ç©¶å¼•å…¥äº†ä¸€ç§æ–°å‹beamæœç´¢ç­–ç•¥ï¼Œç”¨äºæ½œåœ¨ç©ºé—´æ¢ç´¢ï¼Œé€šè¿‡beamæœç´¢è§£ç å®ç°æœ‰æ¡ä»¶ç”Ÿæˆå®Œæ•´çš„å›¾åƒåºåˆ—ã€‚ä¸ä»¥å¾€ä½¿ç”¨å›ºå®šæ½œåœ¨å…ˆéªŒçš„æ–¹æ³•ä¸åŒï¼Œæˆ‘ä»¬çš„æ–¹æ³•åŠ¨æ€æœç´¢æœ€ä¼˜æ½œåœ¨è¡¨ç¤ºåºåˆ—ï¼Œç¡®ä¿è§†è§‰è¿‡æ¸¡è¿è´¯ã€‚ä¸ºè§£å†³beamæœç´¢çš„äºŒæ¬¡å¤æ‚æ€§ï¼Œæˆ‘ä»¬ç»“åˆäº†è·¨æ³¨æ„åŠ›æœºåˆ¶ï¼Œæœ‰æ•ˆè¯„åˆ†æœç´¢è·¯å¾„å¹¶å®ç°ä¿®å‰ªï¼Œä¼˜å…ˆä¸æ–‡æœ¬æç¤ºå’Œè§†è§‰ä¸Šä¸‹æ–‡å¯¹é½ã€‚äººç±»è¯„ä¼°è¯å®ï¼Œæˆ‘ä»¬çš„æ–¹æ³•ä¼˜äºåŸºçº¿æ–¹æ³•ï¼Œäº§ç”Ÿå…·æœ‰æ›´é«˜è¿è´¯æ€§ã€è§†è§‰è¿ç»­æ€§å’Œæ–‡æœ¬å¯¹é½çš„å®Œæ•´åºåˆ—ã€‚æœ¬ç ”ç©¶ç»“åˆäº†æœç´¢ä¼˜åŒ–å’Œæ½œåœ¨ç©ºé—´ç²¾åŒ–çš„è¿›æ­¥ï¼Œä¸ºç»“æ„åŒ–å›¾åƒåºåˆ—ç”Ÿæˆè®¾å®šäº†æ–°çš„æ ‡å‡†ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ‰©æ•£æ¨¡å‹åœ¨æ–‡æœ¬æç¤ºç”Ÿæˆå›¾åƒæ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œä½†åœ¨å›¾åƒåºåˆ—çš„è§†è§‰ä¸€è‡´æ€§ä¸Šå­˜åœ¨æŒ‘æˆ˜ã€‚</li>
<li>ç°æœ‰æ–¹æ³•ç‹¬ç«‹ç”Ÿæˆå›¾åƒï¼Œå¯¼è‡´å™äº‹ä¸è¿è´¯ï¼Œç‰¹åˆ«æ˜¯åœ¨éçº¿æ€§å™äº‹ä¸­ã€‚</li>
<li>å¼•å…¥æ–°å‹beamæœç´¢ç­–ç•¥ï¼Œç”¨äºæ½œåœ¨ç©ºé—´æ¢ç´¢ï¼Œå®ç°æœ‰æ¡ä»¶ç”Ÿæˆå®Œæ•´çš„å›¾åƒåºåˆ—ã€‚</li>
<li>åŠ¨æ€æœç´¢æœ€ä¼˜æ½œåœ¨è¡¨ç¤ºåºåˆ—ï¼Œç¡®ä¿è§†è§‰è¿‡æ¸¡è¿è´¯ã€‚</li>
<li>ç»“åˆè·¨æ³¨æ„åŠ›æœºåˆ¶ï¼Œè§£å†³beamæœç´¢çš„äºŒæ¬¡å¤æ‚æ€§ï¼Œä¼˜å…ˆä¸æ–‡æœ¬å’Œè§†è§‰ä¸Šä¸‹æ–‡å¯¹é½ã€‚</li>
<li>äººç±»è¯„ä¼°è¯å®æ–°æ–¹æ³•ä¼˜äºåŸºçº¿æ–¹æ³•ï¼Œäº§ç”Ÿæ›´è¿è´¯ã€è§†è§‰è¿ç»­å’Œæ–‡æœ¬å¯¹é½çš„åºåˆ—ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.20429">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-9f57784d4ba6830dae06d92aedbad3ea.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ec636aa88f9c4a39f6a49c88cf3ef099.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-92338271cb51a5da384ebd122a67559f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7f61e535fdf80555c07ba2eeb3084a7f.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="ITA-MDT-Image-Timestep-Adaptive-Masked-Diffusion-Transformer-Framework-for-Image-Based-Virtual-Try-On"><a href="#ITA-MDT-Image-Timestep-Adaptive-Masked-Diffusion-Transformer-Framework-for-Image-Based-Virtual-Try-On" class="headerlink" title="ITA-MDT: Image-Timestep-Adaptive Masked Diffusion Transformer Framework   for Image-Based Virtual Try-On"></a>ITA-MDT: Image-Timestep-Adaptive Masked Diffusion Transformer Framework   for Image-Based Virtual Try-On</h2><p><strong>Authors:Ji Woo Hong, Tri Ton, Trung X. Pham, Gwanhyeong Koo, Sunjae Yoon, Chang D. Yoo</strong></p>
<p>This paper introduces ITA-MDT, the Image-Timestep-Adaptive Masked Diffusion Transformer Framework for Image-Based Virtual Try-On (IVTON), designed to overcome the limitations of previous approaches by leveraging the Masked Diffusion Transformer (MDT) for improved handling of both global garment context and fine-grained details. The IVTON task involves seamlessly superimposing a garment from one image onto a person in another, creating a realistic depiction of the person wearing the specified garment. Unlike conventional diffusion-based virtual try-on models that depend on large pre-trained U-Net architectures, ITA-MDT leverages a lightweight, scalable transformer-based denoising diffusion model with a mask latent modeling scheme, achieving competitive results while reducing computational overhead. A key component of ITA-MDT is the Image-Timestep Adaptive Feature Aggregator (ITAFA), a dynamic feature aggregator that combines all of the features from the image encoder into a unified feature of the same size, guided by diffusion timestep and garment image complexity. This enables adaptive weighting of features, allowing the model to emphasize either global information or fine-grained details based on the requirements of the denoising stage. Additionally, the Salient Region Extractor (SRE) module is presented to identify complex region of the garment to provide high-resolution local information to the denoising model as an additional condition alongside the global information of the full garment image. This targeted conditioning strategy enhances detail preservation of fine details in highly salient garment regions, optimizing computational resources by avoiding unnecessarily processing entire garment image. Comparative evaluations confirms that ITA-MDT improves efficiency while maintaining strong performance, reaching state-of-the-art results in several metrics. </p>
<blockquote>
<p>æœ¬æ–‡ä»‹ç»äº†é’ˆå¯¹åŸºäºå›¾åƒçš„è™šæ‹Ÿè¯•ç©¿ï¼ˆIVTONï¼‰è®¾è®¡çš„å›¾åƒæ—¶åºè‡ªé€‚åº”æ©è†œæ‰©æ•£è½¬æ¢æ¡†æ¶ITA-MDTã€‚è¯¥æ¡†æ¶æ—¨åœ¨å…‹æœä»¥å¾€æ–¹æ³•çš„å±€é™æ€§ï¼Œåˆ©ç”¨æ©è†œæ‰©æ•£è½¬æ¢å™¨ï¼ˆMDTï¼‰æ”¹è¿›å…¨å±€æœè£…ä¸Šä¸‹æ–‡å’Œç²¾ç»†ç»†èŠ‚çš„å¤„ç†ã€‚IVTONä»»åŠ¡æ¶‰åŠå°†ä¸€å¼ å›¾åƒä¸­çš„æœè£…æ— ç¼åœ°å åŠ åˆ°å¦ä¸€å¼ å›¾åƒä¸­çš„äººç‰©èº«ä¸Šï¼Œåˆ›å»ºäººç‰©ç©¿ç€æŒ‡å®šæœè£…çš„ç°å®ä¸»ä¹‰æç»˜ã€‚ä¸åŒäºä¾èµ–å¤§å‹é¢„è®­ç»ƒU-Netæ¶æ„çš„ä¼ ç»ŸåŸºäºæ‰©æ•£çš„è™šæ‹Ÿè¯•ç©¿æ¨¡å‹ï¼ŒITA-MDTé‡‡ç”¨äº†ä¸€ç§åŸºäºè½»é‡çº§ã€å¯æ‰©å±•çš„å˜å‹å™¨å»å™ªæ‰©æ•£æ¨¡å‹ï¼Œå¹¶å¸¦æœ‰æ©è†œæ½œåœ¨å»ºæ¨¡æ–¹æ¡ˆï¼Œåœ¨å‡å°‘è®¡ç®—å¼€é”€çš„åŒæ—¶å®ç°äº†å…·æœ‰ç«äº‰åŠ›çš„ç»“æœã€‚ITA-MDTçš„å…³é”®ç»„ä»¶æ˜¯å›¾åƒæ—¶åºè‡ªé€‚åº”ç‰¹å¾èšåˆå™¨ï¼ˆITAFAï¼‰ï¼Œè¿™æ˜¯ä¸€ä¸ªåŠ¨æ€ç‰¹å¾èšåˆå™¨ï¼Œå®ƒå°†æ¥è‡ªå›¾åƒç¼–ç å™¨çš„æ‰€æœ‰ç‰¹å¾åˆå¹¶æˆå…·æœ‰ç›¸åŒå¤§å°çš„ç»Ÿä¸€ç‰¹å¾ï¼Œç”±æ‰©æ•£æ­¥éª¤å’Œæœè£…å›¾åƒå¤æ‚æ€§å¼•å¯¼ã€‚è¿™å®ç°äº†ç‰¹å¾çš„è‡ªé€‚åº”åŠ æƒï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿæ ¹æ®å»å™ªé˜¶æ®µçš„è¦æ±‚å¼ºè°ƒå…¨å±€ä¿¡æ¯æˆ–ç²¾ç»†ç»†èŠ‚ã€‚æ­¤å¤–ï¼Œè¿˜ä»‹ç»äº†æ˜¾è‘—åŒºåŸŸæå–å™¨ï¼ˆSREï¼‰æ¨¡å—ï¼Œç”¨äºè¯†åˆ«æœè£…çš„å¤æ‚åŒºåŸŸï¼Œä¸ºå»å™ªæ¨¡å‹æä¾›é«˜åˆ†è¾¨ç‡çš„å±€éƒ¨ä¿¡æ¯ï¼Œä½œä¸ºé™¤å…¨æœè£…å›¾åƒçš„å…¨å±€ä¿¡æ¯å¤–çš„é™„åŠ æ¡ä»¶ã€‚è¿™ç§æœ‰é’ˆå¯¹æ€§çš„æ¡ä»¶ç­–ç•¥æé«˜äº†ä¿ç•™ç»†å¾®ç»†èŠ‚å¤„çš„ç»†èŠ‚çš„èƒ½åŠ›ï¼Œå¹¶é€šè¿‡é¿å…ä¸å¿…è¦åœ°å¤„ç†æ•´ä¸ªæœè£…å›¾åƒæ¥ä¼˜åŒ–è®¡ç®—èµ„æºã€‚æ¯”è¾ƒè¯„ä¼°è¯å®ï¼ŒITA-MDTåœ¨æé«˜æ•ˆç‡çš„åŒæ—¶ä¿æŒäº†å¼ºå¤§çš„æ€§èƒ½ï¼Œåœ¨å¤šä¸ªæŒ‡æ ‡ä¸Šè¾¾åˆ°äº†æœ€æ–°æ°´å¹³ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.20418v1">PDF</a> CVPR 2025, Project Page: <a target="_blank" rel="noopener" href="https://jiwoohong93.github.io/ita-mdt/">https://jiwoohong93.github.io/ita-mdt/</a></p>
<p><strong>Summary</strong></p>
<p>è¯¥è®ºæ–‡æå‡ºäº†åŸºäºå›¾åƒæ—¶åºè‡ªé€‚åº”æ©è†œæ‰©æ•£è½¬æ¢å™¨æ¡†æ¶ï¼ˆITA-MDTï¼‰çš„å›¾åƒè™šæ‹Ÿè¯•ç©¿ï¼ˆIVTONï¼‰æ–°æ–¹æ³•ã€‚å®ƒå…‹æœäº†å…ˆå‰æ–¹æ³•çš„å±€é™æ€§ï¼Œåˆ©ç”¨æ©è†œæ‰©æ•£è½¬æ¢å™¨ï¼ˆMDTï¼‰åŒæ—¶å¤„ç†å…¨å±€æœè£…ä¸Šä¸‹æ–‡å’Œç²¾ç»†ç»†èŠ‚ã€‚ITA-MDTé‡‡ç”¨è½»é‡çº§ã€å¯æ‰©å±•çš„åŸºäºè½¬æ¢å™¨çš„å»å™ªæ‰©æ•£æ¨¡å‹ï¼Œç»“åˆæ©è†œæ½œåœ¨å»ºæ¨¡æ–¹æ¡ˆï¼Œåœ¨å‡å°‘è®¡ç®—å¼€é”€çš„åŒæ—¶å®ç°ç«äº‰ç»“æœã€‚è®ºæ–‡çš„å…³é”®ç»„ä»¶åŒ…æ‹¬å›¾åƒæ—¶åºè‡ªé€‚åº”ç‰¹å¾èšåˆå™¨ï¼ˆITAFAï¼‰å’Œæ˜¾è‘—åŒºåŸŸæå–å™¨ï¼ˆSREï¼‰æ¨¡å—ï¼Œå®ƒä»¬åˆ†åˆ«ç”¨äºè‡ªé€‚åº”åŠ æƒç‰¹å¾å’Œè¯†åˆ«æœè£…çš„å¤æ‚åŒºåŸŸï¼Œä»¥æé«˜æ¨¡å‹çš„æ€§èƒ½ã€‚ç»¼åˆè¯„ä»·è¡¨æ˜ï¼ŒITA-MDTåœ¨æé«˜æ•ˆç‡çš„åŒæ—¶ä¿æŒäº†å¼ºå¤§çš„æ€§èƒ½ï¼Œåœ¨å¤šä¸ªæŒ‡æ ‡ä¸Šè¾¾åˆ°äº†æœ€æ–°æ°´å¹³ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ITA-MDTæ¡†æ¶ç»“åˆäº†å›¾åƒæ—¶åºè‡ªé€‚åº”å’Œæ©è†œæ‰©æ•£è½¬æ¢å™¨ï¼ˆMDTï¼‰ï¼Œæé«˜äº†å¤„ç†å…¨å±€æœè£…ä¸Šä¸‹æ–‡å’Œç²¾ç»†ç»†èŠ‚çš„èƒ½åŠ›ã€‚</li>
<li>ä¸ä¾èµ–å¤§å‹é¢„è®­ç»ƒU-Netæ¶æ„çš„ä¼ ç»Ÿæ‰©æ•£è™šæ‹Ÿè¯•ç©¿æ¨¡å‹ä¸åŒï¼ŒITA-MDTé‡‡ç”¨è½»é‡çº§ã€å¯æ‰©å±•çš„åŸºäºè½¬æ¢å™¨çš„å»å™ªæ‰©æ•£æ¨¡å‹ã€‚</li>
<li>ITA-MDTä¸­çš„å›¾åƒæ—¶åºè‡ªé€‚åº”ç‰¹å¾èšåˆå™¨ï¼ˆITAFAï¼‰èƒ½è‡ªé€‚åº”åœ°åŠ æƒç‰¹å¾ï¼Œæ ¹æ®å»å™ªé˜¶æ®µçš„éœ€æ±‚å¼ºè°ƒå…¨å±€ä¿¡æ¯æˆ–ç²¾ç»†ç»†èŠ‚ã€‚</li>
<li>æ˜¾è‘—åŒºåŸŸæå–å™¨ï¼ˆSREï¼‰æ¨¡å—ç”¨äºè¯†åˆ«æœè£…çš„å¤æ‚åŒºåŸŸï¼Œä¸ºå»å™ªæ¨¡å‹æä¾›é«˜åˆ†è¾¨ç‡çš„å±€éƒ¨ä¿¡æ¯ï¼ŒåŒæ—¶è€ƒè™‘å…¨å±€çš„æœè£…å›¾åƒä¿¡æ¯ã€‚</li>
<li>ITA-MDTé‡‡ç”¨æœ‰é’ˆå¯¹æ€§çš„æ¡ä»¶ç­–ç•¥ï¼Œä¼˜åŒ–äº†å¯¹ç²¾ç»†æœè£…åŒºåŸŸçš„ç»†èŠ‚ä¿ç•™ï¼Œé¿å…äº†ä¸å¿…è¦çš„å¯¹æ•´ä¸ªæœè£…å›¾åƒçš„å¤„ç†ï¼Œä»è€Œä¼˜åŒ–äº†è®¡ç®—èµ„æºã€‚</li>
<li>è¯„ä¼°ç»“æœè¡¨æ˜ï¼ŒITA-MDTåœ¨æé«˜æ•ˆç‡çš„åŒæ—¶ä¿æŒäº†å¼ºå¤§çš„æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.20418">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-3552b442d0d86dcaae0c4db856cb5c18.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1f41303c7cb96a3d12e37f031355772c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-5af2bd4d89f98cd9b02f4158912a02c4.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-9ee6e9b6b4e07b1ea068e21675411291.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Video-Motion-Graphs"><a href="#Video-Motion-Graphs" class="headerlink" title="Video Motion Graphs"></a>Video Motion Graphs</h2><p><strong>Authors:Haiyang Liu, Zhan Xu, Fa-Ting Hong, Hsin-Ping Huang, Yi Zhou, Yang Zhou</strong></p>
<p>We present Video Motion Graphs, a system designed to generate realistic human motion videos. Using a reference video and conditional signals such as music or motion tags, the system synthesizes new videos by first retrieving video clips with gestures matching the conditions and then generating interpolation frames to seamlessly connect clip boundaries. The core of our approach is HMInterp, a robust Video Frame Interpolation (VFI) model that enables seamless interpolation of discontinuous frames, even for complex motion scenarios like dancing. HMInterp i) employs a dual-branch interpolation approach, combining a Motion Diffusion Model for human skeleton motion interpolation with a diffusion-based video frame interpolation model for final frame generation. ii) adopts condition progressive training to effectively leverage identity strong and weak conditions, such as images and pose. These designs ensure both high video texture quality and accurate motion trajectory. Results show that our Video Motion Graphs outperforms existing generative- and retrieval-based methods for multi-modal conditioned human motion video generation. Project page can be found at <a target="_blank" rel="noopener" href="https://h-liu1997.github.io/Video-Motion-Graphs/">https://h-liu1997.github.io/Video-Motion-Graphs/</a> </p>
<blockquote>
<p>æˆ‘ä»¬æå‡ºäº†è§†é¢‘è¿åŠ¨å›¾ï¼ˆVideo Motion Graphsï¼‰ç³»ç»Ÿï¼Œæ—¨åœ¨ç”Ÿæˆé€¼çœŸçš„äººç±»è¿åŠ¨è§†é¢‘ã€‚è¯¥ç³»ç»Ÿä½¿ç”¨å‚è€ƒè§†é¢‘å’Œæ¡ä»¶ä¿¡å·ï¼ˆå¦‚éŸ³ä¹æˆ–è¿åŠ¨æ ‡ç­¾ï¼‰æ¥åˆæˆæ–°è§†é¢‘ã€‚å®ƒé¦–å…ˆæ£€ç´¢ä¸æ¡ä»¶åŒ¹é…çš„è§†é¢‘ç‰‡æ®µï¼Œç„¶åç”Ÿæˆæ’å¸§ä»¥æ— ç¼è¿æ¥ç‰‡æ®µè¾¹ç•Œã€‚æˆ‘ä»¬çš„æ–¹æ³•çš„æ ¸å¿ƒæ˜¯HMInterpï¼Œè¿™æ˜¯ä¸€ä¸ªå¼ºå¤§çš„è§†é¢‘å¸§æ’å€¼ï¼ˆVFIï¼‰æ¨¡å‹ï¼Œå³ä½¿å¯¹äºè·³èˆç­‰å¤æ‚è¿åŠ¨åœºæ™¯ï¼Œä¹Ÿèƒ½å®ç°ä¸è¿ç»­å¸§çš„æ— ç¼æ’å€¼ã€‚HMInterpé‡‡ç”¨åŒåˆ†æ”¯æ’å€¼æ–¹æ³•ï¼Œç»“åˆç”¨äºäººç±»éª¨æ¶è¿åŠ¨æ’å€¼çš„è¿åŠ¨æ‰©æ•£æ¨¡å‹ä¸åŸºäºæ‰©æ•£çš„è§†é¢‘å¸§æ’å€¼æ¨¡å‹è¿›è¡Œæœ€ç»ˆå¸§ç”Ÿæˆã€‚å®ƒé‡‡ç”¨æ¡ä»¶æ¸è¿›è®­ç»ƒï¼Œæœ‰æ•ˆåˆ©ç”¨å¼ºæ¡ä»¶å’Œå¼±æ¡ä»¶ï¼ˆå¦‚å›¾åƒå’Œå§¿æ€ï¼‰ã€‚è¿™äº›è®¾è®¡ç¡®ä¿äº†è§†é¢‘çº¹ç†çš„é«˜è´¨é‡ä»¥åŠè¿åŠ¨è½¨è¿¹çš„å‡†ç¡®æ€§ã€‚ç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„è§†é¢‘è¿åŠ¨å›¾åœ¨åŸºäºå¤šæ¨¡æ€æ¡ä»¶çš„äººç±»è¿åŠ¨è§†é¢‘ç”Ÿæˆæ–¹é¢ä¼˜äºç°æœ‰çš„ç”Ÿæˆå’Œæ£€ç´¢æ–¹æ³•ã€‚é¡¹ç›®é¡µé¢å¯è®¿é—®ï¼š[<a target="_blank" rel="noopener" href="https://h-liu1997.github.io/Video-Motion-Graphs/]">https://h-liu1997.github.io/Video-Motion-Graphs/]</a> ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.20218v1">PDF</a> 14 pages,10 figures</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†Video Motion Graphsç³»ç»Ÿï¼Œè¯¥ç³»ç»Ÿèƒ½å¤Ÿæ ¹æ®å‚è€ƒè§†é¢‘ã€éŸ³ä¹æˆ–åŠ¨ä½œæ ‡ç­¾ç­‰æ¡ä»¶ä¿¡å·ç”Ÿæˆé€¼çœŸçš„äººç±»è¿åŠ¨è§†é¢‘ã€‚ç³»ç»Ÿé€šè¿‡æ£€ç´¢åŒ¹é…æ¡ä»¶çš„è§†é¢‘ç‰‡æ®µï¼Œå¹¶åˆ©ç”¨HMInterpæ ¸å¿ƒæ–¹æ³•ç”Ÿæˆæ’å¸§ï¼Œå®ç°è§†é¢‘ç‰‡æ®µä¹‹é—´çš„æ— ç¼è¿æ¥ã€‚HMInterpé‡‡ç”¨åŒåˆ†æ”¯æ’å€¼æ–¹æ³•ï¼Œç»“åˆMotion Diffusion Modelè¿›è¡Œäººä½“éª¨æ¶è¿åŠ¨æ’å€¼ï¼Œä»¥åŠåŸºäºæ‰©æ•£çš„è§†é¢‘å¸§æ’å€¼æ¨¡å‹è¿›è¡Œæœ€ç»ˆå¸§ç”Ÿæˆã€‚è¯¥é¡¹ç›®ç¡®ä¿äº†é«˜è§†é¢‘çº¹ç†è´¨é‡å’Œå‡†ç¡®çš„è¿åŠ¨è½¨è¿¹ï¼Œå¹¶è¶…è¶Šäº†ç°æœ‰çš„ç”Ÿæˆå¼å’Œæ£€ç´¢å¼äººç±»è¿åŠ¨è§†é¢‘ç”Ÿæˆæ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Video Motion Graphsç³»ç»Ÿèƒ½å¤Ÿç”Ÿæˆé€¼çœŸçš„äººç±»è¿åŠ¨è§†é¢‘ã€‚</li>
<li>ç³»ç»Ÿæ ¹æ®å‚è€ƒè§†é¢‘å’Œæ¡ä»¶ä¿¡å·ï¼ˆå¦‚éŸ³ä¹æˆ–åŠ¨ä½œæ ‡ç­¾ï¼‰ç”Ÿæˆæ–°è§†é¢‘ã€‚</li>
<li>é€šè¿‡æ£€ç´¢åŒ¹é…æ¡ä»¶çš„è§†é¢‘ç‰‡æ®µï¼Œå¹¶å®ç°æ’å¸§æŠ€æœ¯ï¼Œä½¿è§†é¢‘ç‰‡æ®µæ— ç¼è¿æ¥ã€‚</li>
<li>HMInterpæ˜¯ç³»ç»Ÿçš„æ ¸å¿ƒæ–¹æ³•ï¼Œé‡‡ç”¨åŒåˆ†æ”¯æ’å€¼æ–¹æ³•ï¼Œç»“åˆMotion Diffusion Modelè¿›è¡Œéª¨æ¶æ’å€¼å’Œæœ€ç»ˆå¸§ç”Ÿæˆã€‚</li>
<li>HMInterpé‡‡ç”¨æ¡ä»¶æ¸è¿›è®­ç»ƒï¼Œæœ‰æ•ˆåˆ©ç”¨å¼ºå¼±æ¡ä»¶ï¼Œå¦‚å›¾åƒå’Œå§¿æ€ã€‚</li>
<li>è¯¥ç³»ç»Ÿç¡®ä¿äº†é«˜è§†é¢‘çº¹ç†è´¨é‡å’Œå‡†ç¡®çš„è¿åŠ¨è½¨è¿¹ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.20218">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-669092500bc0a506aed5764727eb4fb9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-89667fcf5061de9ded7521b5261f3aaf.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-a485419ba519d8307051c54ddbed60aa.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-60ce912a43c969c5622d828a83479878.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-cf409feb4ddc735e4f8f428ca04cef9a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-8dc2f233a82a7d984235a783014a1a17.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Beyond-Words-Advancing-Long-Text-Image-Generation-via-Multimodal-Autoregressive-Models"><a href="#Beyond-Words-Advancing-Long-Text-Image-Generation-via-Multimodal-Autoregressive-Models" class="headerlink" title="Beyond Words: Advancing Long-Text Image Generation via Multimodal   Autoregressive Models"></a>Beyond Words: Advancing Long-Text Image Generation via Multimodal   Autoregressive Models</h2><p><strong>Authors:Alex Jinpeng Wang, Linjie Li, Zhengyuan Yang, Lijuan Wang, Min Li</strong></p>
<p>Recent advancements in autoregressive and diffusion models have led to strong performance in image generation with short scene text words. However, generating coherent, long-form text in images, such as paragraphs in slides or documents, remains a major challenge for current generative models. We present the first work specifically focused on long text image generation, addressing a critical gap in existing text-to-image systems that typically handle only brief phrases or single sentences. Through comprehensive analysis of state-of-the-art autoregressive generation models, we identify the image tokenizer as a critical bottleneck in text generating quality. To address this, we introduce a novel text-focused, binary tokenizer optimized for capturing detailed scene text features. Leveraging our tokenizer, we develop \ModelName, a multimodal autoregressive model that excels in generating high-quality long-text images with unprecedented fidelity. Our model offers robust controllability, enabling customization of text properties such as font style, size, color, and alignment. Extensive experiments demonstrate that \ModelName<del>significantly outperforms SD3.5 Large</del>\cite{sd3} and GPT4o<del>\cite{gpt4o} with DALL-E 3</del>\cite{dalle3} in generating long text accurately, consistently, and flexibly. Beyond its technical achievements, \ModelName~opens up exciting opportunities for innovative applications like interleaved document and PowerPoint generation, establishing a new frontier in long-text image generating. </p>
<blockquote>
<p>è¿‘æœŸè‡ªå›å½’æ¨¡å‹å’Œæ‰©æ•£æ¨¡å‹çš„è¿›å±•åœ¨çŸ­åœºæ™¯æ–‡æœ¬å­—çš„å›¾åƒç”Ÿæˆæ–¹é¢å–å¾—äº†å‡ºè‰²çš„è¡¨ç°ã€‚ç„¶è€Œï¼Œåœ¨å›¾åƒä¸­ç”Ÿæˆè¿è´¯çš„é•¿æ–‡æœ¬ï¼Œå¦‚åœ¨å¹»ç¯ç‰‡æˆ–æ–‡æ¡£ä¸­çš„æ®µè½ï¼Œä»ç„¶æ˜¯å½“å‰ç”Ÿæˆæ¨¡å‹çš„ä¸€å¤§æŒ‘æˆ˜ã€‚æˆ‘ä»¬é¦–æ¬¡ä¸“æ³¨äºé•¿æ–‡æœ¬å›¾åƒç”Ÿæˆçš„ç ”ç©¶ï¼Œå¡«è¡¥äº†ç°æœ‰æ–‡æœ¬åˆ°å›¾åƒç³»ç»Ÿçš„ä¸€ä¸ªå…³é”®ç©ºç™½ï¼Œè¿™äº›ç³»ç»Ÿé€šå¸¸åªèƒ½å¤„ç†ç®€çŸ­çš„çŸ­è¯­æˆ–å•ä¸ªå¥å­ã€‚é€šè¿‡å¯¹æœ€æ–°è‡ªå›å½’ç”Ÿæˆæ¨¡å‹çš„ç»¼åˆåˆ†æï¼Œæˆ‘ä»¬å‘ç°å›¾åƒåˆ†è¯å™¨æ˜¯æ–‡æœ¬ç”Ÿæˆè´¨é‡çš„å…³é”®ç“¶é¢ˆã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§æ–°çš„ä¸“æ³¨äºæ–‡æœ¬çš„äºŒå…ƒåˆ†è¯å™¨ï¼Œå®ƒç»è¿‡ä¼˜åŒ–å¯æ•è·è¯¦ç»†çš„åœºæ™¯æ–‡æœ¬ç‰¹å¾ã€‚åˆ©ç”¨æˆ‘ä»¬çš„åˆ†è¯å™¨ï¼Œæˆ‘ä»¬å¼€å‘äº†ModelNameï¼Œè¿™æ˜¯ä¸€ç§å¤šæ¨¡å¼è‡ªå›å½’æ¨¡å‹ï¼Œæ“…é•¿ç”Ÿæˆé«˜è´¨é‡çš„é•¿æ–‡æœ¬å›¾åƒï¼Œå…·æœ‰å‰æ‰€æœªæœ‰çš„ä¿çœŸåº¦ã€‚æˆ‘ä»¬çš„æ¨¡å‹æä¾›å¼ºå¤§çš„å¯æ§æ€§ï¼Œèƒ½å¤Ÿå®ç°æ–‡æœ¬å±æ€§çš„å®šåˆ¶ï¼Œå¦‚å­—ä½“æ ·å¼ã€å¤§å°ã€é¢œè‰²å’Œå¯¹é½æ–¹å¼ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼ŒModelNameåœ¨å‡†ç¡®ã€ä¸€è‡´å’Œçµæ´»åœ°ç”Ÿæˆé•¿æ–‡æœ¬æ–¹é¢æ˜¾è‘—ä¼˜äºSD3.5 Large[sd3]ã€GPT4o[gpt4o]å’ŒDALL-E 3[dalle3]ã€‚é™¤äº†å…¶æŠ€æœ¯æˆå°±ä¹‹å¤–ï¼ŒModelNameä¸ºäº¤äº’å¼æ–‡æ¡£å’ŒPowerPointç”Ÿæˆç­‰åˆ›æ–°åº”ç”¨æä¾›äº†æ¿€åŠ¨äººå¿ƒçš„æœºä¼šï¼Œä¸ºé•¿æ–‡æœ¬å›¾åƒç”Ÿæˆæ ‘ç«‹äº†æ–°çš„å‰æ²¿ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.20198v1">PDF</a> 16 pages</p>
<p><strong>Summary</strong></p>
<p>è¿‘æœŸè‡ªå›å½’æ¨¡å‹å’Œæ‰©æ•£æ¨¡å‹çš„å‘å±•åœ¨çŸ­æ–‡æœ¬å›¾åƒç”Ÿæˆæ–¹é¢è¡¨ç°å‡ºå¼ºå¤§çš„æ€§èƒ½ã€‚ç„¶è€Œï¼Œç”Ÿæˆè¿è´¯çš„é•¿æ–‡æœ¬å›¾åƒï¼Œå¦‚å¹»ç¯ç‰‡ä¸­çš„æ®µè½æˆ–æ–‡æ¡£ï¼Œä»æ˜¯å½“å‰ç”Ÿæˆæ¨¡å‹çš„ä¸€å¤§æŒ‘æˆ˜ã€‚æœ¬æ–‡é¦–æ¬¡ä¸“æ³¨äºé•¿æ–‡æœ¬å›¾åƒç”Ÿæˆï¼Œè§£å†³äº†ç°æœ‰æ–‡æœ¬åˆ°å›¾åƒç³»ç»Ÿåªèƒ½å¤„ç†ç®€çŸ­çŸ­è¯­æˆ–å•ä¸ªå¥å­çš„é—®é¢˜ã€‚é€šè¿‡å¯¹æœ€å…ˆè¿›çš„è‡ªå›å½’ç”Ÿæˆæ¨¡å‹çš„ç»¼åˆåˆ†æï¼Œæœ¬æ–‡ç¡®å®šäº†å›¾åƒåˆ†è¯å™¨æ˜¯æ–‡æœ¬ç”Ÿæˆè´¨é‡çš„å…³é”®ç“¶é¢ˆã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œæœ¬æ–‡å¼•å…¥äº†ä¸€ç§æ–°å‹çš„æ–‡æœ¬èšç„¦äºŒè¿›åˆ¶åˆ†è¯å™¨ï¼Œæ—¨åœ¨æ•æ‰è¯¦ç»†çš„åœºæ™¯æ–‡æœ¬ç‰¹å¾ã€‚åˆ©ç”¨è¯¥åˆ†è¯å™¨ï¼Œæœ¬æ–‡å¼€å‘äº†ä¸€ä¸ªå¤šæ¨¡å¼è‡ªå›å½’æ¨¡å‹â€”â€”ModelNameï¼Œè¯¥æ¨¡å‹æ“…é•¿ç”Ÿæˆå…·æœ‰å‰æ‰€æœªæœ‰çš„é€¼çœŸåº¦çš„é«˜è´¨é‡é•¿æ–‡æœ¬å›¾åƒï¼Œå¹¶æä¾›å¼ºå¤§çš„å¯æ§æ€§ï¼Œèƒ½å¤Ÿè‡ªå®šä¹‰æ–‡æœ¬å±æ€§ï¼Œå¦‚å­—ä½“æ ·å¼ã€å¤§å°ã€é¢œè‰²å’Œå¯¹é½æ–¹å¼ã€‚å®éªŒè¡¨æ˜ï¼ŒModelNameåœ¨å‡†ç¡®ã€ä¸€è‡´å’Œçµæ´»åœ°ç”Ÿæˆé•¿æ–‡æœ¬æ–¹é¢æ˜¾è‘—ä¼˜äºSD3.5 Largeã€GPT4oå’ŒDALL-E 3ã€‚é™¤äº†æŠ€æœ¯æˆå°±ä¹‹å¤–ï¼ŒModelNameä¸ºäº¤äº’å¼æ–‡æ¡£å’ŒPowerPointç”Ÿæˆç­‰åˆ›æ–°åº”ç”¨å¼€è¾Ÿäº†æ¿€åŠ¨äººå¿ƒçš„æœºä¼šï¼Œä¸ºé•¿æ–‡æœ¬å›¾åƒç”Ÿæˆæ ‘ç«‹äº†æ–°é‡Œç¨‹ç¢‘ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è¿‘æœŸè‡ªå›å½’æ¨¡å‹å’Œæ‰©æ•£æ¨¡å‹åœ¨çŸ­æ–‡æœ¬å›¾åƒç”Ÿæˆæ–¹é¢å–å¾—äº†æ˜¾è‘—è¿›å±•ã€‚</li>
<li>é•¿æ–‡æœ¬å›¾åƒç”Ÿæˆä»ç„¶æ˜¯ä¸€ä¸ªä¸»è¦æŒ‘æˆ˜ï¼Œç‰¹åˆ«æ˜¯åœ¨ç”Ÿæˆè¿è´¯çš„æ®µè½æˆ–æ–‡æ¡£æ—¶ã€‚</li>
<li>ç°æœ‰æ–‡æœ¬åˆ°å›¾åƒç³»ç»Ÿé€šå¸¸åªèƒ½å¤„ç†ç®€çŸ­çŸ­è¯­æˆ–å•ä¸ªå¥å­ã€‚</li>
<li>é€šè¿‡åˆ†æè‡ªå›å½’ç”Ÿæˆæ¨¡å‹ï¼Œç¡®å®šäº†å›¾åƒåˆ†è¯å™¨æ˜¯æ–‡æœ¬ç”Ÿæˆè´¨é‡çš„å…³é”®ç“¶é¢ˆã€‚</li>
<li>å¼•å…¥äº†ä¸€ç§æ–°å‹çš„æ–‡æœ¬èšç„¦äºŒè¿›åˆ¶åˆ†è¯å™¨ï¼Œä»¥æ•æ‰è¯¦ç»†çš„åœºæ™¯æ–‡æœ¬ç‰¹å¾ã€‚</li>
<li>å¼€å‘äº†ä¸€ä¸ªå¤šæ¨¡å¼è‡ªå›å½’æ¨¡å‹â€”â€”ModelNameï¼Œæ“…é•¿ç”Ÿæˆé«˜è´¨é‡çš„é•¿æ–‡æœ¬å›¾åƒï¼Œå…·æœ‰å¼ºå¤§çš„å¯æ§æ€§å’Œå‰æ‰€æœªæœ‰çš„é€¼çœŸåº¦ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.20198">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-14e719af6548766e28e1da18521b1636.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-0d5e555ebd3e064e0fd92eb7b037511b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0461ac137daf86e14eb257e173ee2963.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3f0ceebe6a5bbfd2284aaf8359fbebcd.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a9550a28bac3b56474b0abe5dc9a589e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-36ec2c2de7fe754224f61567bee47382.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="AvatarArtist-Open-Domain-4D-Avatarization"><a href="#AvatarArtist-Open-Domain-4D-Avatarization" class="headerlink" title="AvatarArtist: Open-Domain 4D Avatarization"></a>AvatarArtist: Open-Domain 4D Avatarization</h2><p><strong>Authors:Hongyu Liu, Xuan Wang, Ziyu Wan, Yue Ma, Jingye Chen, Yanbo Fan, Yujun Shen, Yibing Song, Qifeng Chen</strong></p>
<p>This work focuses on open-domain 4D avatarization, with the purpose of creating a 4D avatar from a portrait image in an arbitrary style. We select parametric triplanes as the intermediate 4D representation and propose a practical training paradigm that takes advantage of both generative adversarial networks (GANs) and diffusion models. Our design stems from the observation that 4D GANs excel at bridging images and triplanes without supervision yet usually face challenges in handling diverse data distributions. A robust 2D diffusion prior emerges as the solution, assisting the GAN in transferring its expertise across various domains. The synergy between these experts permits the construction of a multi-domain image-triplane dataset, which drives the development of a general 4D avatar creator. Extensive experiments suggest that our model, AvatarArtist, is capable of producing high-quality 4D avatars with strong robustness to various source image domains. The code, the data, and the models will be made publicly available to facilitate future studies. </p>
<blockquote>
<p>æœ¬æ–‡é‡ç‚¹å…³æ³¨å¼€æ”¾åŸŸ4Dä¸ªæ€§åŒ–å»ºæ¨¡ï¼Œæ—¨åœ¨ä»ä»»æ„é£æ ¼çš„è‚–åƒå›¾åƒåˆ›å»º4Dä¸ªæ€§åŒ–è§’è‰²ã€‚æˆ‘ä»¬é€‰æ‹©å‚æ•°åŒ–triplanesä½œä¸ºä¸­é—´4Dè¡¨ç¤ºå½¢å¼ï¼Œå¹¶æå‡ºäº†ä¸€ç§å®ç”¨çš„è®­ç»ƒèŒƒå¼ï¼Œè¯¥èŒƒå¼ç»“åˆäº†ç”Ÿæˆå¯¹æŠ—ç½‘ç»œï¼ˆGANsï¼‰å’Œæ‰©æ•£æ¨¡å‹ã€‚æˆ‘ä»¬çš„è®¾è®¡æºäºä¸€é¡¹è§‚å¯Ÿï¼Œå³4D GANsåœ¨æ¡¥æ¥å›¾åƒå’Œtriplanesæ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œæ— éœ€ç›‘ç£ï¼Œä½†åœ¨å¤„ç†å¤šæ ·æ•°æ®åˆ†å¸ƒæ—¶é€šå¸¸é¢ä¸´æŒ‘æˆ˜ã€‚å¼ºå¤§çš„2Dæ‰©æ•£å…ˆéªŒä½œä¸ºè§£å†³æ–¹æ¡ˆå‡ºç°ï¼Œå¸®åŠ©GANåœ¨ä¸åŒé¢†åŸŸè½¬ç§»å…¶ä¸“ä¸šçŸ¥è¯†ã€‚è¿™äº›ä¸“å®¶ä¹‹é—´çš„ååŒä½œç”¨ä½¿å¾—èƒ½å¤Ÿæ„å»ºå¤šåŸŸå›¾åƒ-triplaneæ•°æ®é›†ï¼Œè¿™æ¨åŠ¨äº†é€šç”¨4Dä¸ªæ€§åŒ–è§’è‰²åˆ›å»ºå™¨çš„å‘å±•ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„AvatarArtistæ¨¡å‹èƒ½å¤Ÿç”Ÿæˆé«˜è´¨é‡çš„4Dä¸ªæ€§åŒ–è§’è‰²ï¼Œå¯¹å„ç§æºå›¾åƒåŸŸå…·æœ‰å¼ºå¤§çš„ç¨³å¥æ€§ã€‚ä»£ç ã€æ•°æ®å’Œæ¨¡å‹å°†å…¬å¼€æä¾›ï¼Œä»¥ä¾¿è¿›è¡Œæœªæ¥ç ”ç©¶ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.19906v2">PDF</a> Accepted to CVPR 2025. Project page:   <a target="_blank" rel="noopener" href="https://kumapowerliu.github.io/AvatarArtist">https://kumapowerliu.github.io/AvatarArtist</a></p>
<p><strong>Summary</strong><br>     æœ¬æ–‡å…³æ³¨å¼€æ”¾åŸŸ4D avatarizationï¼Œæ—¨åœ¨ä»ä»»æ„é£æ ¼çš„è‚–åƒå›¾åƒåˆ›å»º4D avatarã€‚ç ”ç©¶å›¢é˜Ÿé€‰ç”¨å‚æ•°åŒ–triplanesä½œä¸ºä¸­é—´4Dè¡¨ç¤ºï¼Œå¹¶æå‡ºç»“åˆç”Ÿæˆå¯¹æŠ—ç½‘ç»œï¼ˆGANsï¼‰å’Œæ‰©æ•£æ¨¡å‹çš„å®ç”¨è®­ç»ƒèŒƒå¼ã€‚ç ”ç©¶è®¾è®¡æºäºè§‚å¯Ÿå‘ç°ï¼Œå°½ç®¡GANsåœ¨æ— ç›‘ç£æƒ…å†µä¸‹èƒ½å¤Ÿå¾ˆå¥½åœ°å®ç°å›¾åƒå’Œtriplanesä¹‹é—´çš„æ¡¥æ¢ä½œç”¨ï¼Œä½†åœ¨å¤„ç†å¤šæ ·åŒ–æ•°æ®åˆ†å¸ƒæ—¶é¢ä¸´æŒ‘æˆ˜ã€‚ä¸ºè§£å†³æ­¤é—®é¢˜ï¼Œç ”ç©¶å›¢é˜Ÿå¼•å…¥ç¨³å¥çš„äºŒç»´æ‰©æ•£å…ˆéªŒçŸ¥è¯†ï¼ŒååŠ©GANåœ¨ä¸åŒé¢†åŸŸé—´è½¬ç§»çŸ¥è¯†ã€‚è¿™äº›ä¸“å®¶ä¹‹é—´çš„ååŒä½œç”¨ä½¿å¾—æ„å»ºå¤šé¢†åŸŸå›¾åƒ-triplanesæ•°æ®é›†æˆä¸ºå¯èƒ½ï¼Œè¿›è€Œæ¨åŠ¨é€šç”¨4D avataråˆ›ä½œè€…çš„å‘å±•ã€‚å®éªŒè¯æ˜ï¼Œæ‰€æ„å»ºçš„AvatarArtistæ¨¡å‹èƒ½å¤Ÿä»å„ç§æºå›¾åƒåŸŸä¸­äº§ç”Ÿé«˜è´¨é‡çš„4D avatarsï¼Œè¡¨ç°å‡ºå¼ºå¤§çš„ç¨³å¥æ€§ã€‚ä»£ç ã€æ•°æ®å’Œæ¨¡å‹å°†å…¬å¼€ä¾›æœªæ¥ç ”ç©¶ä½¿ç”¨ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç ”ç©¶èšç„¦äºå¼€æ”¾åŸŸ4D avatarizationæŠ€æœ¯ï¼Œæ—¨åœ¨é€šè¿‡è‚–åƒå›¾åƒåˆ›å»º4D avatarã€‚</li>
<li>å‚æ•°åŒ–triplanesè¢«é€‰ä¸ºä¸­é—´4Dè¡¨ç¤ºå½¢å¼ã€‚</li>
<li>æå‡ºäº†ç»“åˆGANså’Œæ‰©æ•£æ¨¡å‹çš„å®ç”¨è®­ç»ƒèŒƒå¼ã€‚</li>
<li>GANsåœ¨å¤„ç†å¤šæ ·åŒ–æ•°æ®åˆ†å¸ƒæ—¶å­˜åœ¨æŒ‘æˆ˜ã€‚</li>
<li>å¼•å…¥äºŒç»´æ‰©æ•£å…ˆéªŒçŸ¥è¯†ä»¥å¢å¼ºGANçš„æ€§èƒ½ï¼Œå¹¶å¸®åŠ©å…¶è·¨ä¸åŒé¢†åŸŸè½¬ç§»çŸ¥è¯†ã€‚</li>
<li>ä¸“å®¶ä¹‹é—´çš„ååŒä½œç”¨ä½¿å¾—æ„å»ºå¤šé¢†åŸŸå›¾åƒ-triplanesæ•°æ®é›†æˆä¸ºå¯èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.19906">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-378bba218e71cadb1f65c8aa8913385f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ea16d6a700d3d7ab1b01cea39a78f1f4.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-f846e42e4d2905e2771588ee66b27a5b.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-e29711d534a981fd7ac40ac11f5578c9.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-4b3014ffdb5e751ad6bd3b845ec67cf6.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="In-the-Blink-of-an-Eye-Instant-Game-Map-Editing-using-a-Generative-AI-Smart-Brush"><a href="#In-the-Blink-of-an-Eye-Instant-Game-Map-Editing-using-a-Generative-AI-Smart-Brush" class="headerlink" title="In the Blink of an Eye: Instant Game Map Editing using a Generative-AI   Smart Brush"></a>In the Blink of an Eye: Instant Game Map Editing using a Generative-AI   Smart Brush</h2><p><strong>Authors:Vitaly Gnatyuk, Valeriia Koriukina, Ilya Levoshevich, Pavel Nurminskiy, Guenter Wallner</strong></p>
<p>With video games steadily increasing in complexity, automated generation of game content has found widespread interest. However, the task of 3D gaming map art creation remains underexplored to date due to its unique complexity and domain-specific challenges. While recent works have addressed related topics such as retro-style level generation and procedural terrain creation, these works primarily focus on simpler data distributions. To the best of our knowledge, we are the first to demonstrate the application of modern AI techniques for high-resolution texture manipulation in complex, highly detailed AAA 3D game environments. We introduce a novel Smart Brush for map editing, designed to assist artists in seamlessly modifying selected areas of a game map with minimal effort. By leveraging generative adversarial networks and diffusion models we propose two variants of the brush that enable efficient and context-aware generation. Our hybrid workflow aims to enhance both artistic flexibility and production efficiency, enabling the refinement of environments without manually reworking every detail, thus helping to bridge the gap between automation and creative control in game development. A comparative evaluation of our two methods with adapted versions of several state-of-the art models shows that our GAN-based brush produces the sharpest and most detailed outputs while preserving image context while the evaluated state-of-the-art models tend towards blurrier results and exhibit difficulties in maintaining contextual consistency. </p>
<blockquote>
<p>éšç€è§†é¢‘æ¸¸æˆçš„å¤æ‚åº¦ä¸æ–­æé«˜ï¼Œæ¸¸æˆå†…å®¹çš„è‡ªåŠ¨ç”Ÿæˆå·²ç»å¼•èµ·äº†å¹¿æ³›çš„å…´è¶£ã€‚ç„¶è€Œï¼Œç”±äº3Dæ¸¸æˆåœ°å›¾è‰ºæœ¯åˆ›ä½œçš„ç‹¬ç‰¹å¤æ‚æ€§å’Œç‰¹å®šé¢†åŸŸçš„æŒ‘æˆ˜ï¼Œè‡³ä»Šè¯¥ä»»åŠ¡ä»è¢«æ¢ç´¢å¾—ä¸å¤Ÿæ·±å…¥ã€‚è™½ç„¶è¿‘æœŸçš„ç ”ç©¶å·²ç»æ¶‰åŠäº†ç›¸å…³ä¸»é¢˜ï¼Œå¦‚å¤å¤é£æ ¼çš„å…³å¡ç”Ÿæˆå’Œç¨‹åºåŒ–åœ°å½¢ç”Ÿæˆï¼Œä½†è¿™äº›ç ”ç©¶ä¸»è¦å…³æ³¨äºæ›´ç®€å•çš„æ•°æ®åˆ†å¸ƒã€‚æ®æˆ‘ä»¬æ‰€çŸ¥ï¼Œæˆ‘ä»¬æ˜¯é¦–æ¬¡å±•ç¤ºåœ¨ç°ä»£AIæŠ€æœ¯åº”ç”¨äºå¤æ‚ã€é«˜åº¦è¯¦ç»†çš„AAAçº§3Dæ¸¸æˆç¯å¢ƒä¸­çš„é«˜åˆ†è¾¨ç‡çº¹ç†æ“ä½œçš„åº”ç”¨ã€‚æˆ‘ä»¬å¼•å…¥äº†ä¸€ç§æ–°å‹çš„æ™ºèƒ½ç”»ç¬”ç”¨äºåœ°å›¾ç¼–è¾‘ï¼Œæ—¨åœ¨å¸®åŠ©è‰ºæœ¯å®¶è½»æ¾ä¿®æ”¹æ¸¸æˆåœ°å›¾çš„é€‰å®šåŒºåŸŸã€‚é€šè¿‡åˆ©ç”¨ç”Ÿæˆå¯¹æŠ—ç½‘ç»œå’Œæ‰©æ•£æ¨¡å‹ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸¤ç§ç”»ç¬”å˜ä½“ï¼Œèƒ½å¤Ÿå®ç°é«˜æ•ˆä¸”è¯­å¢ƒæ„ŸçŸ¥çš„ç”Ÿæˆã€‚æˆ‘ä»¬çš„æ··åˆå·¥ä½œæµç¨‹æ—¨åœ¨æé«˜è‰ºæœ¯çµæ´»æ€§å’Œç”Ÿäº§æ•ˆç‡ï¼Œèƒ½å¤Ÿåœ¨ä¸é‡æ–°æ‰‹åŠ¨å¤„ç†æ¯ä¸ªç»†èŠ‚çš„æƒ…å†µä¸‹ä¼˜åŒ–ç¯å¢ƒï¼Œä»è€Œæœ‰åŠ©äºå¼¥åˆæ¸¸æˆå¼€å‘ä¸­è‡ªåŠ¨åŒ–å’Œåˆ›æ„æ§åˆ¶ä¹‹é—´çš„é¸¿æ²Ÿã€‚æˆ‘ä»¬æ–¹æ³•çš„ä¸¤ç§å˜ä½“ä¸ç°ä»£æœ€å‰æ²¿æ¨¡å‹çš„æ”¹ç¼–ç‰ˆæœ¬çš„æ¯”è¾ƒè¯„ä¼°è¡¨æ˜ï¼Œæˆ‘ä»¬çš„åŸºäºGANçš„ç”»ç¬”äº§ç”Ÿçš„è¾“å‡ºæœ€æ¸…æ™°ã€æœ€è¯¦ç»†ï¼ŒåŒæ—¶ä¿ç•™äº†å›¾åƒä¸Šä¸‹æ–‡ï¼Œè€Œè¢«è¯„ä¼°çš„ç°ä»£å‰æ²¿æ¨¡å‹å¾€å¾€äº§ç”Ÿæ¨¡ç³Šçš„ç»“æœï¼Œå¹¶ä¸”åœ¨ä¿æŒä¸Šä¸‹æ–‡ä¸€è‡´æ€§æ–¹é¢å­˜åœ¨å›°éš¾ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.19793v2">PDF</a> </p>
<p><strong>Summary</strong><br>     éšç€è§†é¢‘æ¸¸æˆçš„å¤æ‚åº¦ä¸æ–­æå‡ï¼Œæ¸¸æˆå†…å®¹è‡ªåŠ¨ç”ŸæˆæŠ€æœ¯å¤‡å—å…³æ³¨ã€‚ä½†3Dæ¸¸æˆåœ°å›¾è‰ºæœ¯åˆ›ä½œä»»åŠ¡å› ç‹¬ç‰¹å¤æ‚æ€§å’Œé¢†åŸŸç‰¹å®šæŒ‘æˆ˜è€Œè¢«å¿½è§†ã€‚ç°æœ‰ç ”ç©¶ä¸»è¦å…³æ³¨ç®€å•æ•°æ®åˆ†å¸ƒçš„ç”Ÿæˆï¼Œå¦‚å¤å¤é£æ ¼åœ°å›¾ç”Ÿæˆå’Œç¨‹åºåŒ–åœ°å½¢ç”Ÿæˆã€‚æœ¬æ–‡é¦–æ¬¡å±•ç¤ºäº†ç°ä»£AIæŠ€æœ¯åœ¨é«˜åˆ†è¾¨ç‡çº¹ç†æ“ä½œä¸­çš„åº”ç”¨ï¼Œé’ˆå¯¹å¤æ‚ã€é«˜åº¦è¯¦ç»†çš„AAA 3Dæ¸¸æˆç¯å¢ƒã€‚å¼•å…¥äº†ä¸€ç§æ–°å‹æ™ºèƒ½ç”»ç¬”ç”¨äºåœ°å›¾ç¼–è¾‘ï¼Œæ—¨åœ¨å¸®åŠ©è‰ºæœ¯å®¶è½»æ¾ä¿®æ”¹åœ°å›¾çš„é€‰å®šåŒºåŸŸã€‚å€ŸåŠ©ç”Ÿæˆå¯¹æŠ—ç½‘ç»œå’Œæ‰©æ•£æ¨¡å‹ï¼Œæå‡ºä¸¤ç§ç”»ç¬”å˜ä½“ï¼Œå®ç°é«˜æ•ˆã€æƒ…å¢ƒæ„ŸçŸ¥çš„ç”Ÿæˆã€‚è¯¥æ··åˆå·¥ä½œæµç¨‹æ—¨åœ¨æé«˜è‰ºæœ¯çµæ´»æ€§å’Œç”Ÿäº§æ•ˆç‡ï¼Œå¯åœ¨ä¸æ‰‹åŠ¨é‡æ–°å¤„ç†æ¯ä¸ªç»†èŠ‚çš„æƒ…å†µä¸‹ä¼˜åŒ–ç¯å¢ƒï¼Œæœ‰åŠ©äºå¼¥åˆæ¸¸æˆå¼€å‘ä¸­è‡ªåŠ¨åŒ–ä¸åˆ›æ„æ§åˆ¶ä¹‹é—´çš„å·®è·ã€‚è¯„ä¼°æ˜¾ç¤ºï¼ŒåŸºäºGANçš„ç”»ç¬”èƒ½ç”Ÿæˆæœ€æ¸…æ™°ã€æœ€è¯¦ç»†çš„è¾“å‡ºå¹¶ä¿æŒå›¾åƒä¸Šä¸‹æ–‡ï¼Œè€Œå…¶ä»–ç°æœ‰æ¨¡å‹å¯èƒ½å€¾å‘äºç”Ÿæˆæ¨¡ç³Šçš„ç»“æœå¹¶ä¸”åœ¨ä¿æŒä¸Šä¸‹æ–‡ä¸€è‡´æ€§æ–¹é¢å­˜åœ¨å›°éš¾ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è§†é¢‘æ¸¸æˆå†…å®¹è‡ªåŠ¨ç”ŸæˆæŠ€æœ¯å—åˆ°å…³æ³¨ï¼Œä½†3Dæ¸¸æˆåœ°å›¾è‰ºæœ¯åˆ›ä½œä»ç„¶å…·æœ‰æŒ‘æˆ˜æ€§ã€‚</li>
<li>ç°æœ‰ç ”ç©¶ä¸»è¦å…³æ³¨ç®€å•æ•°æ®åˆ†å¸ƒçš„ç”Ÿæˆï¼Œç¼ºä¹å¯¹å¤æ‚ã€é«˜ç»†èŠ‚æ¸¸æˆç¯å¢ƒçš„ç ”ç©¶ã€‚</li>
<li>æœ¬æ–‡é¦–æ¬¡å±•ç¤ºäº†ç°ä»£AIæŠ€æœ¯åœ¨é«˜åˆ†è¾¨ç‡çº¹ç†æ“ä½œä¸­çš„åº”ç”¨ã€‚</li>
<li>å¼•å…¥äº†ä¸€ç§æ–°å‹æ™ºèƒ½ç”»ç¬”ç”¨äºåœ°å›¾ç¼–è¾‘ï¼Œæ—¨åœ¨å¸®åŠ©è‰ºæœ¯å®¶è½»æ¾ä¿®æ”¹æ¸¸æˆåœ°å›¾ã€‚</li>
<li>åˆ©ç”¨ç”Ÿæˆå¯¹æŠ—ç½‘ç»œå’Œæ‰©æ•£æ¨¡å‹ï¼Œå®ç°äº†é«˜æ•ˆã€æƒ…å¢ƒæ„ŸçŸ¥çš„ç”Ÿæˆã€‚</li>
<li>æ··åˆå·¥ä½œæµç¨‹æ—¨åœ¨æé«˜è‰ºæœ¯çµæ´»æ€§å’Œç”Ÿäº§æ•ˆç‡ï¼Œä¼˜åŒ–ç¯å¢ƒç»†èŠ‚ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.19793">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-57e44300201f62ef4b118207f85fec2b.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-e15d179d1580f2c0dd020d404c96858e.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-96909c2845a90845202c28a6c501a4e3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-23b3ef9229cf940119438fd7eeca86d3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0d8d75da2de1068e7f1c407f3e6ea88a.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="Inference-Time-Scaling-for-Flow-Models-via-Stochastic-Generation-and-Rollover-Budget-Forcing"><a href="#Inference-Time-Scaling-for-Flow-Models-via-Stochastic-Generation-and-Rollover-Budget-Forcing" class="headerlink" title="Inference-Time Scaling for Flow Models via Stochastic Generation and   Rollover Budget Forcing"></a>Inference-Time Scaling for Flow Models via Stochastic Generation and   Rollover Budget Forcing</h2><p><strong>Authors:Jaihoon Kim, Taehoon Yoon, Jisung Hwang, Minhyuk Sung</strong></p>
<p>We propose an inference-time scaling approach for pretrained flow models. Recently, inference-time scaling has gained significant attention in LLMs and diffusion models, improving sample quality or better aligning outputs with user preferences by leveraging additional computation. For diffusion models, particle sampling has allowed more efficient scaling due to the stochasticity at intermediate denoising steps. On the contrary, while flow models have gained popularity as an alternative to diffusion modelsâ€“offering faster generation and high-quality outputs in state-of-the-art image and video generative modelsâ€“efficient inference-time scaling methods used for diffusion models cannot be directly applied due to their deterministic generative process. To enable efficient inference-time scaling for flow models, we propose three key ideas: 1) SDE-based generation, enabling particle sampling in flow models, 2) Interpolant conversion, broadening the search space and enhancing sample diversity, and 3) Rollover Budget Forcing (RBF), an adaptive allocation of computational resources across timesteps to maximize budget utilization. Our experiments show that SDE-based generation, particularly variance-preserving (VP) interpolant-based generation, improves the performance of particle sampling methods for inference-time scaling in flow models. Additionally, we demonstrate that RBF with VP-SDE achieves the best performance, outperforming all previous inference-time scaling approaches. </p>
<blockquote>
<p>æˆ‘ä»¬ä¸ºé¢„è®­ç»ƒçš„æµæ¨¡å‹æå‡ºäº†ä¸€ç§æ¨ç†æ—¶é—´å°ºåº¦æ–¹æ³•ã€‚æœ€è¿‘ï¼Œæ¨ç†æ—¶é—´å°ºåº¦åœ¨å¤§å‹è¯­è¨€æ¨¡å‹å’Œæ‰©æ•£æ¨¡å‹ä¸­å¼•èµ·äº†å¹¿æ³›å…³æ³¨ï¼Œé€šè¿‡åˆ©ç”¨é¢å¤–çš„è®¡ç®—æ¥æé«˜æ ·æœ¬è´¨é‡æˆ–æ›´å¥½åœ°ä½¿è¾“å‡ºä¸ç”¨æˆ·åå¥½å¯¹é½ã€‚å¯¹äºæ‰©æ•£æ¨¡å‹ï¼Œç”±äºä¸­é—´å»å™ªæ­¥éª¤çš„éšæœºæ€§ï¼Œç²’å­é‡‡æ ·å…è®¸æ›´æœ‰æ•ˆçš„å°ºåº¦æ‰©å±•ã€‚ç›¸åï¼Œè™½ç„¶æµæ¨¡å‹ä½œä¸ºæ‰©æ•£æ¨¡å‹çš„æ›¿ä»£å“è€Œå¹¿å—æ¬¢è¿ï¼Œä¸ºæœ€å…ˆè¿›çš„å›¾åƒå’Œè§†é¢‘ç”Ÿæˆæ¨¡å‹æä¾›äº†æ›´å¿«çš„ç”Ÿæˆé€Ÿåº¦å’Œé«˜è´¨é‡è¾“å‡ºï¼Œä½†ç”¨äºæ‰©æ•£æ¨¡å‹çš„é«˜æ•ˆæ¨ç†æ—¶é—´å°ºåº¦æ–¹æ³•æ— æ³•ç›´æ¥åº”ç”¨äºæµæ¨¡å‹ï¼Œå› ä¸ºå®ƒä»¬å…·æœ‰ç¡®å®šçš„ç”Ÿæˆè¿‡ç¨‹ã€‚ä¸ºäº†å®ç°æµæ¨¡å‹çš„æ¨ç†æ—¶é—´å°ºåº¦çš„é«˜æ•ˆæ‰©å±•ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸‰ä¸ªå…³é”®æƒ³æ³•ï¼š1ï¼‰åŸºäºSDEçš„ç”Ÿæˆï¼Œä½¿æµæ¨¡å‹èƒ½å¤Ÿå®ç°ç²’å­é‡‡æ ·ï¼›2ï¼‰æ’å€¼è½¬æ¢ï¼Œæ‰©å¤§æœç´¢ç©ºé—´å¹¶æé«˜æ ·æœ¬å¤šæ ·æ€§ï¼›3ï¼‰æ»šåŠ¨é¢„ç®—å¼ºåˆ¶ï¼ˆRBFï¼‰ï¼Œè‡ªé€‚åº”åœ°åœ¨æ—¶é—´æ­¥é•¿ä¹‹é—´åˆ†é…è®¡ç®—èµ„æºï¼Œä»¥æœ€å¤§åŒ–é¢„ç®—åˆ©ç”¨ã€‚æˆ‘ä»¬çš„å®éªŒè¡¨æ˜ï¼ŒåŸºäºSDEçš„ç”Ÿæˆï¼Œç‰¹åˆ«æ˜¯åŸºäºæ–¹å·®ä¿æŒï¼ˆVPï¼‰æ’å€¼çš„ç”Ÿæˆï¼Œæé«˜äº†æµæ¨¡å‹ä¸­ç”¨äºæ¨ç†æ—¶é—´å°ºåº¦çš„ç²’å­é‡‡æ ·çš„æ€§èƒ½ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜è¯æ˜äº†ä½¿ç”¨VP-SDEçš„RBFå–å¾—äº†æœ€ä½³æ€§èƒ½ï¼Œä¼˜äºä»¥å‰æ‰€æœ‰çš„æ¨ç†æ—¶é—´å°ºåº¦æ–¹æ³•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.19385v2">PDF</a> Project page: <a target="_blank" rel="noopener" href="https://flow-inference-time-scaling.github.io/">https://flow-inference-time-scaling.github.io/</a></p>
<p><strong>Summary</strong><br>     æˆ‘ä»¬ä¸ºé¢„è®­ç»ƒçš„æµç¨‹æ¨¡å‹æå‡ºäº†ä¸€ç§æ¨ç†æ—¶é—´ç¼©æ”¾æ–¹æ³•ã€‚é€šè¿‡SDEï¼ˆéšæœºå¾®åˆ†æ–¹ç¨‹ï¼‰ç”Ÿæˆæ–¹æ³•ï¼Œç²’å­é‡‡æ ·æ–¹æ³•èƒ½å¤Ÿåœ¨æµç¨‹æ¨¡å‹ä¸­è¿›è¡Œé«˜æ•ˆçš„æ¨ç†æ—¶é—´ç¼©æ”¾ï¼Œæå‡æ ·æœ¬è´¨é‡å¹¶ä¼˜åŒ–è®¡ç®—èµ„æºåˆ†é…ã€‚æˆ‘ä»¬æå‡ºäº†ä¸‰ç§å…³é”®æ€è·¯ï¼šåŸºäºSDEçš„ç”Ÿæˆã€æ’å€¼è½¬æ¢å’Œæ»šå­˜é¢„ç®—å¼ºåˆ¶ï¼ˆRBFï¼‰ã€‚å®éªŒè¯æ˜ï¼ŒåŸºäºæ–¹å·®ä¿æŒï¼ˆVPï¼‰æ’å€¼çš„SDEç”Ÿæˆæ–¹æ³•æ”¹å–„äº†æµç¨‹æ¨¡å‹çš„æ¨ç†æ—¶é—´ç¼©æ”¾ä¸­çš„ç²’å­é‡‡æ ·æ–¹æ³•çš„æ€§èƒ½ï¼Œç»“åˆRBFä¸VP-SDEè¡¨ç°æœ€ä½³ï¼Œè¶…è¶Šäº†å…ˆå‰çš„æ¨ç†æ—¶é—´ç¼©æ”¾æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æå‡ºäº†é’ˆå¯¹é¢„è®­ç»ƒæµç¨‹æ¨¡å‹çš„æ¨ç†æ—¶é—´ç¼©æ”¾æ–¹æ³•ã€‚</li>
<li>åˆ©ç”¨SDEï¼ˆéšæœºå¾®åˆ†æ–¹ç¨‹ï¼‰ç”Ÿæˆæ–¹æ³•å®ç°æµç¨‹æ¨¡å‹ä¸­çš„ç²’å­é‡‡æ ·ã€‚</li>
<li>æ’å€¼è½¬æ¢ç”¨äºæ‰©å¤§æœç´¢ç©ºé—´å¹¶å¢å¼ºæ ·æœ¬å¤šæ ·æ€§ã€‚</li>
<li>å¼•å…¥æ»šå­˜é¢„ç®—å¼ºåˆ¶ï¼ˆRBFï¼‰ï¼Œä»¥åœ¨æ—¶é—´ä¸Šè‡ªé€‚åº”åˆ†é…è®¡ç®—èµ„æºï¼Œå®ç°é¢„ç®—æœ€å¤§åŒ–åˆ©ç”¨ã€‚</li>
<li>åŸºäºæ–¹å·®ä¿æŒï¼ˆVPï¼‰æ’å€¼çš„SDEç”Ÿæˆæ–¹æ³•æ”¹å–„äº†æ¨ç†æ—¶é—´ç¼©æ”¾çš„æ€§èƒ½ã€‚</li>
<li>å®éªŒç»“æœè¡¨æ˜ç»“åˆRBFä¸VP-SDEè¡¨ç°æœ€ä½³ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.19385">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-283c64b2149f90ed9aaaf93b37d70711.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ae294f9952228030d7ceab9a0a80ec7f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8e9b5a2bffa2ebbd34523d62d0b2891a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-46e07c647c6a57550b8dd2f53dab3df3.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="PhysAnimator-Physics-Guided-Generative-Cartoon-Animation"><a href="#PhysAnimator-Physics-Guided-Generative-Cartoon-Animation" class="headerlink" title="PhysAnimator: Physics-Guided Generative Cartoon Animation"></a>PhysAnimator: Physics-Guided Generative Cartoon Animation</h2><p><strong>Authors:Tianyi Xie, Yiwei Zhao, Ying Jiang, Chenfanfu Jiang</strong></p>
<p>Creating hand-drawn animation sequences is labor-intensive and demands professional expertise. We introduce PhysAnimator, a novel approach for generating physically plausible meanwhile anime-stylized animation from static anime illustrations. Our method seamlessly integrates physics-based simulations with data-driven generative models to produce dynamic and visually compelling animations. To capture the fluidity and exaggeration characteristic of anime, we perform image-space deformable body simulations on extracted mesh geometries. We enhance artistic control by introducing customizable energy strokes and incorporating rigging point support, enabling the creation of tailored animation effects such as wind interactions. Finally, we extract and warp sketches from the simulation sequence, generating a texture-agnostic representation, and employ a sketch-guided video diffusion model to synthesize high-quality animation frames. The resulting animations exhibit temporal consistency and visual plausibility, demonstrating the effectiveness of our method in creating dynamic anime-style animations. See our project page for more demos: <a target="_blank" rel="noopener" href="https://xpandora.github.io/PhysAnimator/">https://xpandora.github.io/PhysAnimator/</a> </p>
<blockquote>
<p>åˆ›å»ºæ‰‹ç»˜åŠ¨ç”»åºåˆ—æ˜¯ä¸€é¡¹åŠ³åŠ¨å¯†é›†å‹å·¥ä½œï¼Œéœ€è¦ä¸“ä¸šçŸ¥è¯†å’ŒæŠ€èƒ½ã€‚æˆ‘ä»¬å¼•å…¥äº†PhysAnimatorï¼Œè¿™æ˜¯ä¸€ç§ä»é™æ€åŠ¨æ¼«æ’å›¾ç”Ÿæˆç‰©ç†ä¸Šåˆç†ä¸”åŠ¨æ¼«é£æ ¼åŒ–çš„åŠ¨ç”»çš„æ–°æ–¹æ³•ã€‚æˆ‘ä»¬çš„æ–¹æ³•æ— ç¼é›†æˆäº†åŸºäºç‰©ç†çš„æ¨¡æ‹Ÿå’Œæ•°æ®é©±åŠ¨çš„ç”Ÿæˆæ¨¡å‹ï¼Œä»¥äº§ç”ŸåŠ¨æ€ä¸”è§†è§‰ä¸Šå¼•äººæ³¨ç›®çš„åŠ¨ç”»ã€‚ä¸ºäº†æ•æ‰åŠ¨æ¼«çš„æµç•…æ€§å’Œå¤¸å¼ ç‰¹ç‚¹ï¼Œæˆ‘ä»¬åœ¨æå–çš„ç½‘æ ¼å‡ ä½•ç»“æ„ä¸Šæ‰§è¡Œå›¾åƒç©ºé—´çš„å¯å˜å½¢èº«ä½“æ¨¡æ‹Ÿã€‚é€šè¿‡å¼•å…¥å¯å®šåˆ¶çš„èƒ½é‡ç¬”è§¦å’Œèå…¥éª¨éª¼ç‚¹æ”¯æŒï¼Œæˆ‘ä»¬å¢å¼ºäº†è‰ºæœ¯æ§åˆ¶åŠ›ï¼Œèƒ½å¤Ÿåˆ›å»ºå®šåˆ¶çš„åŠ¨ç”»æ•ˆæœï¼Œå¦‚é£åŠ›äº¤äº’ã€‚æœ€åï¼Œæˆ‘ä»¬ä»æ¨¡æ‹Ÿåºåˆ—ä¸­æå–å¹¶æ‰­æ›²è‰å›¾ï¼Œç”Ÿæˆä¸€ç§ä¸çº¹ç†æ— å…³çš„è¡¨ç¤ºå½¢å¼ï¼Œå¹¶é‡‡ç”¨è‰å›¾å¼•å¯¼çš„è§†é¢‘æ‰©æ•£æ¨¡å‹æ¥åˆæˆé«˜è´¨é‡çš„åŠ¨ç”»å¸§ã€‚ç»“æœåŠ¨ç”»è¡¨ç°å‡ºæ—¶é—´è¿è´¯æ€§å’Œè§†è§‰åˆç†æ€§ï¼Œè¯æ˜äº†æˆ‘ä»¬æ–¹æ³•åœ¨åˆ›å»ºåŠ¨æ€åŠ¨æ¼«é£æ ¼åŠ¨ç”»æ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚æ›´å¤šæ¼”ç¤ºè¯·å‚è§æˆ‘ä»¬çš„é¡¹ç›®é¡µé¢ï¼š<a target="_blank" rel="noopener" href="https://xpandora.github.io/PhysAnimator/">https://xpandora.github.io/PhysAnimator/</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.16550v2">PDF</a> Accepted by CVPR 2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åä¸ºPhysAnimatorçš„æ–°å‹åŠ¨ç”»ç”Ÿæˆæ–¹æ³•ï¼Œè¯¥æ–¹æ³•ç»“åˆäº†ç‰©ç†æ¨¡æ‹Ÿå’Œæ•°æ®é©±åŠ¨æ¨¡å‹ï¼Œå¯ä»é™æ€çš„åŠ¨æ¼«æ’ç”»ç”Ÿæˆç¬¦åˆç‰©ç†è§„å¾‹çš„åŠ¨ç”»ã€‚é€šè¿‡å›¾åƒç©ºé—´çš„å˜å½¢ä½“ä»¿çœŸå’Œè‡ªå®šä¹‰èƒ½é‡ç¬”è§¦ï¼ŒPhysAnimatorèƒ½å¤Ÿæ•æ‰åŠ¨æ¼«çš„æµç•…æ€§å’Œå¤¸å¼ ç‰¹ç‚¹ï¼Œå¹¶æ”¯æŒéª¨éª¼ç‚¹ä»¥å®ç°å®šåˆ¶åŠ¨ç”»æ•ˆæœã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ³•è¿˜é€šè¿‡æå–å’Œæ‰­æ›²ä»¿çœŸåºåˆ—ä¸­çš„è‰å›¾ï¼Œå¹¶é‡‡ç”¨è‰å›¾å¼•å¯¼çš„è§†é¢‘æ‰©æ•£æ¨¡å‹ï¼Œç”Ÿæˆé«˜è´¨é‡åŠ¨ç”»å¸§ï¼Œå±•ç°å‡ºæ—¶é—´è¿è´¯æ€§å’Œè§†è§‰å¯ä¿¡åº¦ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>PhysAnimatoræ˜¯ä¸€ç§èƒ½å¤Ÿä»é™æ€åŠ¨æ¼«æ’ç”»ç”ŸæˆåŠ¨æ€åŠ¨æ¼«é£æ ¼åŠ¨ç”»çš„æ–°æ–¹æ³•ã€‚</li>
<li>è¯¥æ–¹æ³•ç»“åˆäº†ç‰©ç†æ¨¡æ‹Ÿå’Œæ•°æ®é©±åŠ¨æ¨¡å‹ï¼Œç”ŸæˆåŠ¨ç”»ç¬¦åˆç‰©ç†è§„å¾‹ã€‚</li>
<li>é€šè¿‡å›¾åƒç©ºé—´çš„å˜å½¢ä½“ä»¿çœŸï¼ŒPhysAnimatorèƒ½å¤Ÿæ•æ‰åŠ¨æ¼«çš„æµç•…æ€§å’Œå¤¸å¼ ç‰¹ç‚¹ã€‚</li>
<li>è‡ªå®šä¹‰èƒ½é‡ç¬”è§¦å’Œè‰ºæœ¯æ§åˆ¶åŠŸèƒ½çš„åŠ å…¥ï¼Œä½¿PhysAnimatorèƒ½å¤Ÿåˆ›å»ºå®šåˆ¶åŠ¨ç”»æ•ˆæœã€‚</li>
<li>æ”¯æŒéª¨éª¼ç‚¹ï¼Œä½¿åŠ¨ç”»æ›´åŠ ç”ŸåŠ¨è‡ªç„¶ã€‚</li>
<li>é€šè¿‡æå–ä»¿çœŸåºåˆ—ä¸­çš„è‰å›¾å¹¶é‡‡ç”¨è‰å›¾å¼•å¯¼çš„è§†é¢‘æ‰©æ•£æ¨¡å‹ï¼Œç”Ÿæˆé«˜è´¨é‡åŠ¨ç”»å¸§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.16550">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-39178611c211d70e851e33e8aeae8f6a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b0fef1cf075cb81915620f2713542286.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="Black-Box-Forgery-Attacks-on-Semantic-Watermarks-for-Diffusion-Models"><a href="#Black-Box-Forgery-Attacks-on-Semantic-Watermarks-for-Diffusion-Models" class="headerlink" title="Black-Box Forgery Attacks on Semantic Watermarks for Diffusion Models"></a>Black-Box Forgery Attacks on Semantic Watermarks for Diffusion Models</h2><p><strong>Authors:Andreas MÃ¼ller, Denis Lukovnikov, Jonas Thietke, Asja Fischer, Erwin Quiring</strong></p>
<p>Integrating watermarking into the generation process of latent diffusion models (LDMs) simplifies detection and attribution of generated content. Semantic watermarks, such as Tree-Rings and Gaussian Shading, represent a novel class of watermarking techniques that are easy to implement and highly robust against various perturbations. However, our work demonstrates a fundamental security vulnerability of semantic watermarks. We show that attackers can leverage unrelated models, even with different latent spaces and architectures (UNet vs DiT), to perform powerful and realistic forgery attacks. Specifically, we design two watermark forgery attacks. The first imprints a targeted watermark into real images by manipulating the latent representation of an arbitrary image in an unrelated LDM to get closer to the latent representation of a watermarked image. We also show that this technique can be used for watermark removal. The second attack generates new images with the target watermark by inverting a watermarked image and re-generating it with an arbitrary prompt. Both attacks just need a single reference image with the target watermark. Overall, our findings question the applicability of semantic watermarks by revealing that attackers can easily forge or remove these watermarks under realistic conditions. </p>
<blockquote>
<p>å°†æ°´å°é›†æˆåˆ°æ½œåœ¨æ‰©æ•£æ¨¡å‹ï¼ˆLDMï¼‰çš„ç”Ÿæˆè¿‡ç¨‹ä¸­ï¼Œå¯ä»¥ç®€åŒ–ç”Ÿæˆå†…å®¹çš„æ£€æµ‹å’Œå½’å±ã€‚è¯­ä¹‰æ°´å°ï¼Œå¦‚æ ‘ç¯å’Œé«˜æ–¯é˜´å½±ï¼Œä»£è¡¨äº†ä¸€ç§æ˜“äºå®ç°ä¸”å¯¹å„ç§æ‰°åŠ¨å…·æœ‰æé«˜é²æ£’æ€§çš„æ–°å‹æ°´å°æŠ€æœ¯ã€‚ç„¶è€Œï¼Œæˆ‘ä»¬çš„å·¥ä½œå±•ç¤ºäº†è¯­ä¹‰æ°´å°çš„åŸºæœ¬å®‰å…¨æ¼æ´ã€‚æˆ‘ä»¬è¡¨æ˜ï¼Œæ”»å‡»è€…å¯ä»¥åˆ©ç”¨æ— å…³æ¨¡å‹ï¼Œå³ä½¿å…·æœ‰ä¸åŒçš„æ½œåœ¨ç©ºé—´å’Œæ¶æ„ï¼ˆUNetä¸DiTï¼‰ï¼Œæ‰§è¡Œå¼ºå¤§ä¸”ç°å®çš„ä¼ªé€ æ”»å‡»ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸¤ç§æ°´å°ä¼ªé€ æ”»å‡»ã€‚ç¬¬ä¸€ç§æ˜¯é€šè¿‡æ“ä½œæ— å…³LDMä¸­ä»»æ„å›¾åƒçš„æ½œåœ¨è¡¨ç¤ºï¼Œä½¿å…¶æ›´æ¥è¿‘å¸¦æ°´å°å›¾åƒçš„æ½œåœ¨è¡¨ç¤ºï¼Œä»è€Œå°†ç›®æ ‡æ°´å°å°å…¥çœŸå®å›¾åƒä¸­ã€‚æˆ‘ä»¬è¿˜è¡¨æ˜ï¼Œè¯¥æŠ€æœ¯ä¹Ÿå¯ç”¨äºæ°´å°å»é™¤ã€‚ç¬¬äºŒç§æ”»å‡»æ˜¯é€šè¿‡åè½¬å¸¦æ°´å°çš„å›¾åƒå¹¶ä½¿ç”¨ä»»æ„æç¤ºé‡æ–°ç”Ÿæˆå¸¦æœ‰ç›®æ ‡æ°´å°çš„æ–°å›¾åƒã€‚è¿™ä¸¤ç§æ”»å‡»éƒ½åªéœ€è¦ä¸€å¼ å¸¦æœ‰ç›®æ ‡æ°´å°çš„å‚è€ƒå›¾åƒå³å¯ã€‚æ€»çš„æ¥è¯´ï¼Œæˆ‘ä»¬çš„å‘ç°å¯¹è¯­ä¹‰æ°´å°çš„é€‚ç”¨æ€§æå‡ºäº†è´¨ç–‘ï¼Œæ­ç¤ºäº†æ”»å‡»è€…åœ¨ç°å®æ¡ä»¶ä¸‹å¯ä»¥è½»æ˜“ä¼ªé€ æˆ–å»é™¤è¿™äº›æ°´å°ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.03283v2">PDF</a> 28 pages, 22 figures, 8 tables, to be published in The IEEE&#x2F;CVF   Conference on Computer Vision and Pattern Recognition 2025 (CVPR)</p>
<p><strong>Summary</strong></p>
<p>é›†æˆæ°´å°åˆ°æ½œåœ¨æ‰©æ•£æ¨¡å‹ï¼ˆLDMï¼‰çš„ç”Ÿæˆè¿‡ç¨‹ä¸­ï¼Œç®€åŒ–äº†ç”Ÿæˆå†…å®¹çš„æ£€æµ‹å’Œå½’å±ã€‚è¯­ä¹‰æ°´å°å¦‚Tree-Ringså’ŒGaussian Shadingç­‰ï¼Œæ˜¯ä¸€ç±»æ˜“äºå®ç°ä¸”å¯¹å„ç§æ‰°åŠ¨é«˜åº¦ç¨³å¥çš„æ–°å‹æ°´å°æŠ€æœ¯ã€‚ç„¶è€Œï¼Œæˆ‘ä»¬çš„å·¥ä½œå±•ç¤ºäº†è¯­ä¹‰æ°´å°çš„åŸºæœ¬å®‰å…¨æ¼æ´ã€‚æˆ‘ä»¬è¯æ˜æ”»å‡»è€…å¯ä»¥åˆ©ç”¨æ— å…³æ¨¡å‹ï¼Œå³ä½¿å…¶æ½œåœ¨ç©ºé—´å’Œæ¶æ„ä¸åŒï¼ˆå¦‚UNetä¸DiTï¼‰ï¼Œæ‰§è¡Œå¼ºå¤§ä¸”é€¼çœŸçš„ä¼ªé€ æ”»å‡»ã€‚æˆ‘ä»¬è®¾è®¡äº†ä¸¤ç§æ°´å°ä¼ªé€ æ”»å‡»ï¼Œç¬¬ä¸€ç§é€šè¿‡å°†ä»»æ„å›¾åƒçš„æ½œåœ¨è¡¨ç¤ºæ“çºµå¾—æ›´æ¥è¿‘å¸¦æ°´å°å›¾åƒçš„æ½œåœ¨è¡¨ç¤ºï¼Œå°†ç›®æ ‡æ°´å°å°å…¥çœŸå®å›¾åƒä¸­ã€‚æˆ‘ä»¬è¿˜å±•ç¤ºäº†è¯¥æŠ€æœ¯ä¹Ÿå¯ç”¨äºæ°´å°ç§»é™¤ã€‚ç¬¬äºŒç§æ”»å‡»æ˜¯é€šè¿‡åè½¬å¸¦æ°´å°çš„å›¾åƒå¹¶ä»¥ä»»æ„æç¤ºé‡æ–°ç”Ÿæˆï¼Œæ¥ç”Ÿæˆå¸¦æœ‰ç›®æ ‡æ°´å°çš„æ–°å›¾åƒã€‚ä¸¤ç§æ”»å‡»éƒ½åªéœ€è¦ä¸€å¼ å¸¦æœ‰ç›®æ ‡æ°´å°çš„å‚è€ƒå›¾åƒã€‚æ€»ä½“è€Œè¨€ï¼Œæˆ‘ä»¬çš„å‘ç°å¯¹è¯­ä¹‰æ°´å°çš„é€‚ç”¨æ€§æå‡ºäº†è´¨ç–‘ï¼Œå› ä¸ºæ”»å‡»è€…å¯ä»¥åœ¨ç°å®æ¡ä»¶ä¸‹è½»æ¾ä¼ªé€ æˆ–ç§»é™¤è¿™äº›æ°´å°ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è¯­ä¹‰æ°´å°æŠ€æœ¯å¦‚Tree-Ringså’ŒGaussian Shadingåœ¨æ½œåœ¨æ‰©æ•£æ¨¡å‹ï¼ˆLDMï¼‰ä¸­æ˜“äºå®æ–½ä¸”å¯¹å¤šç§æ‰°åŠ¨å…·æœ‰ç¨³å¥æ€§ã€‚</li>
<li>ç ”ç©¶è¡¨æ˜è¯­ä¹‰æ°´å°å­˜åœ¨åŸºæœ¬çš„å®‰å…¨æ¼æ´ã€‚</li>
<li>æ”»å‡»è€…èƒ½å¤Ÿåˆ©ç”¨ä¸åŒæ½œåœ¨ç©ºé—´å’Œæ¶æ„çš„æ— å…³æ¨¡å‹æ‰§è¡Œå¼ºå¤§çš„ä¼ªé€ æ”»å‡»ã€‚</li>
<li>ç¬¬ä¸€ç§æ”»å‡»æ–¹æ³•èƒ½å°†ç›®æ ‡æ°´å°å°å…¥çœŸå®å›¾åƒä¸­ï¼Œä¹Ÿèƒ½ç”¨äºç§»é™¤æ°´å°ã€‚</li>
<li>ç¬¬äºŒç§æ”»å‡»é€šè¿‡åè½¬å’Œé‡æ–°ç”Ÿæˆå¸¦æ°´å°çš„å›¾åƒæ¥åˆ›å»ºæ–°çš„å¸¦ç›®æ ‡æ°´å°çš„å›¾åƒã€‚</li>
<li>ä¸¤ç§æ”»å‡»æ–¹æ³•éƒ½åªéœ€è¦ä¸€å¼ å¸¦æœ‰ç›®æ ‡æ°´å°çš„å‚è€ƒå›¾åƒå³å¯æ“ä½œã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.03283">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-039024992ec36d340f57f00a192c5174.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-98060b07ee639a3167e08631d259786a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2c48f988928d9f106e3713b1043a56ce.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-5ad6b21964b5ea05b06c6f00795a65d3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ece7524d1d5b33e8e3f7e1703286f49d.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="Image-Generation-with-Supervised-Selection-Based-on-Multimodal-Features-for-Semantic-Communications"><a href="#Image-Generation-with-Supervised-Selection-Based-on-Multimodal-Features-for-Semantic-Communications" class="headerlink" title="Image Generation with Supervised Selection Based on Multimodal Features   for Semantic Communications"></a>Image Generation with Supervised Selection Based on Multimodal Features   for Semantic Communications</h2><p><strong>Authors:Chengyang Liang, Dong Li</strong></p>
<p>Semantic communication (SemCom) has emerged as a promising technique for the next-generation communication systems, in which the generation at the receiver side is allowed with semantic featuresâ€™ recovery. However, the majority of existing research predominantly utilizes a singular type of semantic information, such as text, images, or speech, to supervise and choose the generated source signals, which may not sufficiently encapsulate the comprehensive and accurate semantic information, and thus creating a performance bottleneck. In order to bridge this gap, in this paper, we propose and investigate a SemCom framework using multimodal information to supervise the generated image. To be specific, in this framework, we first extract semantic features at both the image and text levels utilizing the Convolutional Neural Network (CNN) architecture and the Contrastive Language-Image Pre-Training (CLIP) model before transmission. Then, we employ a generative diffusion model at the receiver to generate multiple images. In order to ensure the accurate extraction and facilitate high-fidelity image reconstruction, we select the â€œbestâ€ image with the minimum reconstruction errors by taking both the aided image and text semantic features into account. We further extend multimodal semantic communication (MMSemCom) system to the multiuser scenario for orthogonal transmission. Experimental results demonstrate that the proposed framework can not only achieve the enhanced fidelity and robustness in image transmission compared with existing communication systems but also sustain a high performance in the low signal-to-noise ratio (SNR) conditions. </p>
<blockquote>
<p>è¯­ä¹‰é€šä¿¡ï¼ˆSemComï¼‰ä½œä¸ºä¸‹ä¸€ä»£é€šä¿¡ç³»ç»Ÿçš„æœ‰å‰é€”çš„æŠ€æœ¯å‡ºç°ã€‚åœ¨æ­¤æŠ€æœ¯ä¸­ï¼Œæ¥æ”¶ç«¯çš„ç”Ÿæˆç‰©è¢«å…è®¸æ¢å¤è¯­ä¹‰ç‰¹å¾ã€‚ç„¶è€Œï¼Œç°æœ‰çš„å¤§å¤šæ•°ç ”ç©¶ä¸»è¦ä½¿ç”¨ä¸€ç§ç±»å‹çš„è¯­ä¹‰ä¿¡æ¯ï¼ˆå¦‚æ–‡æœ¬ã€å›¾åƒæˆ–è¯­éŸ³ï¼‰æ¥ç›‘ç£å’Œé€‰æ‹©ç”Ÿæˆçš„æºä¿¡å·ï¼Œè¿™å¯èƒ½æ— æ³•å……åˆ†åŒ…å«å…¨é¢å’Œå‡†ç¡®çš„è¯­ä¹‰ä¿¡æ¯ï¼Œå› æ­¤é€ æˆæ€§èƒ½ç“¶é¢ˆã€‚ä¸ºäº†å¼¥è¡¥è¿™ä¸€å·®è·ï¼Œæœ¬æ–‡æå‡ºå¹¶ç ”ç©¶äº†ä¸€ç§ä½¿ç”¨å¤šæ¨¡æ€ä¿¡æ¯ç›‘ç£ç”Ÿæˆå›¾åƒçš„SemComæ¡†æ¶ã€‚å…·ä½“æ¥è¯´ï¼Œåœ¨æ­¤æ¡†æ¶ä¸­ï¼Œæˆ‘ä»¬é¦–å…ˆåœ¨ä¼ è¾“ä¹‹å‰ï¼Œåˆ©ç”¨å·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNï¼‰æ¶æ„å’Œå¯¹æ¯”è¯­è¨€-å›¾åƒé¢„è®­ç»ƒï¼ˆCLIPï¼‰æ¨¡å‹ï¼Œåœ¨å›¾åƒå’Œæ–‡æœ¬å±‚é¢æå–è¯­ä¹‰ç‰¹å¾ã€‚ç„¶åï¼Œæˆ‘ä»¬åœ¨æ¥æ”¶ç«¯é‡‡ç”¨ç”Ÿæˆæ‰©æ•£æ¨¡å‹æ¥ç”Ÿæˆå¤šä¸ªå›¾åƒã€‚ä¸ºäº†ç¡®ä¿å‡†ç¡®çš„æå–å’Œä¿ƒè¿›é«˜ä¿çœŸå›¾åƒé‡å»ºï¼Œæˆ‘ä»¬é€šè¿‡è€ƒè™‘è¾…åŠ©å›¾åƒå’Œæ–‡æœ¬è¯­ä¹‰ç‰¹å¾æ¥é€‰æ‹©â€œæœ€ä½³â€å›¾åƒï¼Œä»¥æœ€å°çš„é‡å»ºè¯¯å·®ä¸ºæ ‡å‡†ã€‚æˆ‘ä»¬å°†å¤šæ¨¡æ€è¯­ä¹‰é€šä¿¡ï¼ˆMMSemComï¼‰ç³»ç»Ÿè¿›ä¸€æ­¥æ‰©å±•åˆ°å¤šç”¨æˆ·åœºæ™¯ï¼Œä»¥å®ç°æ­£äº¤ä¼ è¾“ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œä¸ä¼ ç»Ÿé€šä¿¡ç³»ç»Ÿç›¸æ¯”ï¼Œæ‰€ææ¡†æ¶åœ¨å›¾åƒä¼ è¾“ä¸­ä¸ä»…æé«˜äº†ä¿çœŸåº¦å’Œç¨³å¥æ€§ï¼Œè€Œä¸”åœ¨ä½ä¿¡å™ªæ¯”ï¼ˆSNRï¼‰æ¡ä»¶ä¸‹ä¹Ÿä¿æŒäº†é«˜æ€§èƒ½ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2411.17428v2">PDF</a> </p>
<p><strong>Summary</strong><br>     è¯­ä¹‰é€šä¿¡ï¼ˆSemComï¼‰ä½œä¸ºä¸‹ä¸€ä»£é€šä¿¡ç³»ç»Ÿçš„æœ‰å‰é€”çš„æŠ€æœ¯ï¼Œå…è®¸åœ¨æ¥æ”¶ç«¯è¿›è¡Œè¯­ä¹‰ç‰¹å¾çš„æ¢å¤ã€‚ç„¶è€Œï¼Œå¤§å¤šæ•°ç°æœ‰ç ”ç©¶ä¸»è¦ä½¿ç”¨ä¸€ç§ç±»å‹çš„è¯­ä¹‰ä¿¡æ¯ï¼ˆå¦‚æ–‡æœ¬ã€å›¾åƒæˆ–è¯­éŸ³ï¼‰æ¥ç›‘ç£å’Œé€‰æ‹©ç”Ÿæˆçš„æºä¿¡å·ï¼Œè¿™å¯èƒ½ä¸è¶³ä»¥å…¨é¢å’Œå‡†ç¡®åœ°å°è£…è¯­ä¹‰ä¿¡æ¯ï¼Œä»è€Œæˆä¸ºæ€§èƒ½ç“¶é¢ˆã€‚ä¸ºäº†å¼¥è¡¥è¿™ä¸€å·®è·ï¼Œæœ¬æ–‡æå‡ºå¹¶ç ”ç©¶äº†ä¸€ç§ä½¿ç”¨å¤šæ¨¡æ€ä¿¡æ¯ç›‘ç£ç”Ÿæˆå›¾åƒçš„SemComæ¡†æ¶ã€‚è¯¥æ¡†æ¶åœ¨ä¼ è¾“å‰ï¼Œåˆ©ç”¨å·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNï¼‰æ¶æ„å’Œå¯¹æ¯”è¯­è¨€-å›¾åƒé¢„è®­ç»ƒï¼ˆCLIPï¼‰æ¨¡å‹ï¼Œåœ¨å›¾åƒå’Œæ–‡æœ¬å±‚é¢æå–è¯­ä¹‰ç‰¹å¾ã€‚æ¥æ”¶ç«¯é‡‡ç”¨ç”Ÿæˆæ€§æ‰©æ•£æ¨¡å‹ç”Ÿæˆå¤šå¼ å›¾åƒï¼Œé€šè¿‡è€ƒè™‘è¾…åŠ©å›¾åƒå’Œæ–‡æœ¬è¯­ä¹‰ç‰¹å¾ï¼Œé€‰æ‹©é‡å»ºè¯¯å·®æœ€å°çš„â€œæœ€ä½³â€å›¾åƒã€‚æ­¤å¤–ï¼Œå°†å¤šæ¨¡æ€è¯­ä¹‰é€šä¿¡ï¼ˆMMSemComï¼‰ç³»ç»Ÿæ‰©å±•åˆ°å¤šç”¨æˆ·åœºæ™¯ï¼Œå®ç°æ­£äº¤ä¼ è¾“ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ¡†æ¶ä¸ç°æœ‰é€šä¿¡ç³»ç»Ÿç›¸æ¯”ï¼Œåœ¨æé«˜å›¾åƒä¼ è¾“çš„ä¿çœŸåº¦å’Œç¨³å¥æ€§çš„åŒæ—¶ï¼Œåœ¨ä½ä¿¡å™ªæ¯”ï¼ˆSNRï¼‰æ¡ä»¶ä¸‹ä¹Ÿèƒ½ä¿æŒé«˜æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è¯­ä¹‰é€šä¿¡ï¼ˆSemComï¼‰ä½œä¸ºä¸‹ä¸€ä»£é€šä¿¡ç³»ç»Ÿçš„æœ‰å‰é€”æŠ€æœ¯ï¼Œå…è®¸åœ¨æ¥æ”¶ç«¯è¿›è¡Œè¯­ä¹‰ç‰¹å¾çš„æ¢å¤ã€‚</li>
<li>ç°æœ‰ç ”ç©¶ä¸»è¦ä½¿ç”¨å•ä¸€ç±»å‹è¯­ä¹‰ä¿¡æ¯ï¼Œå¯èƒ½ä¸è¶³ä»¥å…¨é¢å‡†ç¡®åœ°å°è£…è¯­ä¹‰ä¿¡æ¯ï¼Œå­˜åœ¨æ€§èƒ½ç“¶é¢ˆã€‚</li>
<li>æå‡ºä½¿ç”¨å¤šæ¨¡æ€ä¿¡æ¯çš„SemComæ¡†æ¶ï¼Œè¯¥æ¡†æ¶åœ¨å›¾åƒå’Œæ–‡æœ¬å±‚é¢æå–è¯­ä¹‰ç‰¹å¾ï¼Œä»¥æé«˜å‡†ç¡®æ€§å’Œå…¨é¢æ€§ã€‚</li>
<li>æ¥æ”¶ç«¯é‡‡ç”¨ç”Ÿæˆæ€§æ‰©æ•£æ¨¡å‹ç”Ÿæˆå¤šå¼ å›¾åƒï¼Œå¹¶é€‰æ‹©â€œæœ€ä½³â€å›¾åƒã€‚</li>
<li>å°†å¤šæ¨¡æ€è¯­ä¹‰é€šä¿¡ï¼ˆMMSemComï¼‰ç³»ç»Ÿæ‰©å±•åˆ°å¤šç”¨æˆ·åœºæ™¯ï¼Œå®ç°æ­£äº¤ä¼ è¾“ã€‚</li>
<li>å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ¡†æ¶åœ¨å›¾åƒä¼ è¾“çš„ä¿çœŸåº¦å’Œç¨³å¥æ€§æ–¹é¢æœ‰æ‰€æå‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2411.17428">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-cf1dcc7a7dab5a0060ae3a674f4d4a6b.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-8c77fade9c58b376bbe805db0bf2d3f8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-24c8539db340fb473d97b427933759d7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-476b72f301f72fa77302ac8417c64a21.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="Data-Augmentation-in-Earth-Observation-A-Diffusion-Model-Approach"><a href="#Data-Augmentation-in-Earth-Observation-A-Diffusion-Model-Approach" class="headerlink" title="Data Augmentation in Earth Observation: A Diffusion Model Approach"></a>Data Augmentation in Earth Observation: A Diffusion Model Approach</h2><p><strong>Authors:Tiago Sousa, BenoÃ®t Ries, Nicolas Guelfi</strong></p>
<p>High-quality Earth Observation (EO) imagery is essential for accurate analysis and informed decision making across sectors. However, data scarcity caused by atmospheric conditions, seasonal variations, and limited geographical coverage hinders the effective application of Artificial Intelligence (AI) in EO. Traditional data augmentation techniques, which rely on basic parameterized image transformations, often fail to introduce sufficient diversity across key semantic axes. These axes include natural changes such as snow and floods, human impacts like urbanization and roads, and disasters such as wildfires and storms, which limits the accuracy of AI models in EO applications. To address this, we propose a four-stage data augmentation approach that integrates diffusion models to enhance semantic diversity. Our method employs meta-prompts for instruction generation, vision-language models for rich captioning, EO-specific diffusion model fine-tuning, and iterative data augmentation. Extensive experiments using four augmentation techniques demonstrate that our approach consistently outperforms established methods, generating semantically diverse EO images and improving AI model performance. </p>
<blockquote>
<p>é«˜è´¨é‡çš„åœ°çƒè§‚æµ‹ï¼ˆEOï¼‰å½±åƒå¯¹äºå„é¢†åŸŸçš„ç²¾ç¡®åˆ†æå’Œå†³ç­–åˆ¶å®šè‡³å…³é‡è¦ã€‚ç„¶è€Œï¼Œå¤§æ°”æ¡ä»¶ã€å­£èŠ‚å˜åŒ–å’Œåœ°ç†è¦†ç›–èŒƒå›´æœ‰é™æ‰€å¯¼è‡´çš„æ•°æ®ç¨€ç¼ºï¼Œé˜»ç¢äº†äººå·¥æ™ºèƒ½ï¼ˆAIï¼‰åœ¨åœ°çƒè§‚æµ‹ï¼ˆEOï¼‰ä¸­çš„æœ‰æ•ˆåº”ç”¨ã€‚ä¼ ç»Ÿçš„æ•°æ®å¢å¼ºæŠ€æœ¯ä¾èµ–äºåŸºæœ¬çš„å‚æ•°åŒ–å›¾åƒè½¬æ¢ï¼Œå¾€å¾€æ— æ³•åœ¨å…³é”®è¯­ä¹‰è½´ä¸Šå¼•å…¥è¶³å¤Ÿçš„å¤šæ ·æ€§ã€‚è¿™äº›è½´åŒ…æ‹¬è‡ªç„¶å˜åŒ–ï¼ˆå¦‚å†°é›ªå’Œæ´ªæ°´ï¼‰ã€äººä¸ºå½±å“ï¼ˆå¦‚åŸé•‡åŒ–å’Œé“è·¯å»ºè®¾ï¼‰ä»¥åŠç¾å®³ï¼ˆå¦‚é‡ç«å’Œé£æš´ï¼‰ã€‚è¿™é™åˆ¶äº†äººå·¥æ™ºèƒ½æ¨¡å‹åœ¨åœ°çƒè§‚æµ‹åº”ç”¨ä¸­çš„å‡†ç¡®æ€§ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åˆ†é˜¶æ®µæ•°æ®å¢å¼ºæ–¹æ³•ï¼Œé€šè¿‡é›†æˆæ‰©æ•£æ¨¡å‹æ¥æé«˜è¯­ä¹‰å¤šæ ·æ€§ã€‚æˆ‘ä»¬çš„æ–¹æ³•é‡‡ç”¨å…ƒæç¤ºè¿›è¡ŒæŒ‡ä»¤ç”Ÿæˆã€è§†è§‰è¯­è¨€æ¨¡å‹è¿›è¡Œä¸°å¯Œçš„å›¾åƒæè¿°ã€é’ˆå¯¹åœ°çƒè§‚æµ‹çš„æ‰©æ•£æ¨¡å‹çš„å¾®è°ƒä»¥åŠè¿­ä»£æ•°æ®å¢å¼ºã€‚ä½¿ç”¨å››ç§å¢å¼ºæŠ€æœ¯çš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•å§‹ç»ˆä¼˜äºç°æœ‰æ–¹æ³•ï¼Œèƒ½å¤Ÿç”Ÿæˆè¯­ä¹‰å¤šæ ·çš„åœ°çƒè§‚æµ‹å›¾åƒï¼Œæé«˜äººå·¥æ™ºèƒ½æ¨¡å‹çš„æ€§èƒ½ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2406.06218v2">PDF</a> 25 pages, 12 figures</p>
<p><strong>Summary</strong></p>
<p>é«˜è´¨é‡çš„åœ°è§‚æµ‹åƒå¯¹äºå‡†ç¡®çš„éƒ¨é—¨åˆ†æä¸å†³ç­–è‡³å…³é‡è¦ã€‚ä½†ç”±äºå¤§æ°”æ¡ä»¶ã€å­£èŠ‚å˜åŒ–å’Œåœ°ç†è¦†ç›–èŒƒå›´æœ‰é™ç­‰å› ç´ å¯¼è‡´çš„è§‚æµ‹æ•°æ®ç¨€ç¼ºï¼Œé˜»ç¢äº†äººå·¥æ™ºèƒ½åœ¨åœ°è§‚æµ‹ä¸­çš„æœ‰æ•ˆåº”ç”¨ã€‚ä¼ ç»Ÿä¾èµ–åŸºæœ¬å‚æ•°åŒ–å›¾åƒè½¬æ¢çš„æ•°æ®å¢å¼ºæŠ€æœ¯å¾€å¾€æ— æ³•åœ¨å…³é”®è¯­ä¹‰è½´ä¸Šå¼•å…¥è¶³å¤Ÿçš„å¤šæ ·æ€§ã€‚è¿™äº›è¯­ä¹‰è½´åŒ…æ‹¬è‡ªç„¶å˜åŒ–ã€äººä¸ºå½±å“ä»¥åŠç¾å®³ç­‰ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ç»“åˆæ‰©æ•£æ¨¡å‹è¿›è¡Œè¯­ä¹‰å¢å¼ºçš„å››é˜¶æ®µæ•°æ®å¢å¼ºæ–¹æ³•ã€‚é€šè¿‡é‡‡ç”¨å…ƒæç¤ºæŒ‡ä»¤ç”Ÿæˆã€è§†è§‰è¯­è¨€æ¨¡å‹ä¸°å¯Œæ ‡æ³¨ã€é’ˆå¯¹åœ°è§‚æµ‹çš„æ‰©æ•£æ¨¡å‹å¾®è°ƒä»¥åŠè¿­ä»£æ•°æ®å¢å¼ºç­‰æ–¹æ³•ï¼Œæˆ‘ä»¬çš„æ–¹æ³•èƒ½å¤Ÿç”Ÿæˆè¯­ä¹‰ä¸°å¯Œçš„åœ°è§‚æµ‹å›¾åƒï¼Œå¹¶æå‡äººå·¥æ™ºèƒ½æ¨¡å‹çš„æ€§èƒ½ã€‚å®éªŒè¯æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨ä¸€ç³»åˆ—å¢å¼ºæŠ€æœ¯ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œè¶…è¶Šç°æœ‰æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>é«˜è´¨é‡çš„åœ°è§‚æµ‹å›¾åƒå¯¹äºå„éƒ¨é—¨åˆ†æå’Œå†³ç­–è‡³å…³é‡è¦ã€‚</li>
<li>æ•°æ®ç¨€ç¼ºæ˜¯äººå·¥æ™ºèƒ½åœ¨åœ°è§‚æµ‹ä¸­åº”ç”¨çš„éšœç¢ä¹‹ä¸€ã€‚</li>
<li>ä¼ ç»Ÿæ•°æ®å¢å¼ºæŠ€æœ¯åœ¨å¼•å…¥è¯­ä¹‰å¤šæ ·æ€§æ–¹é¢å­˜åœ¨ä¸è¶³ã€‚</li>
<li>æå‡ºçš„å››é˜¶æ®µæ•°æ®å¢å¼ºæ–¹æ³•æ•´åˆæ‰©æ•£æ¨¡å‹ä»¥å¢å¼ºè¯­ä¹‰å¤šæ ·æ€§ã€‚</li>
<li>è¯¥æ–¹æ³•åŒ…æ‹¬å…ƒæç¤ºæŒ‡ä»¤ç”Ÿæˆã€è§†è§‰è¯­è¨€æ¨¡å‹ä¸°å¯Œæ ‡æ³¨ã€é’ˆå¯¹åœ°è§‚æµ‹çš„æ‰©æ•£æ¨¡å‹å¾®è°ƒä»¥åŠè¿­ä»£æ•°æ®å¢å¼ºã€‚</li>
<li>å®éªŒè¯æ˜ï¼Œè¯¥æ–¹æ³•åœ¨ç”Ÿæˆè¯­ä¹‰ä¸°å¯Œçš„åœ°è§‚æµ‹å›¾åƒå’Œæå‡AIæ¨¡å‹æ€§èƒ½ä¸Šè¡¨ç°ä¼˜å¼‚ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2406.06218">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-7378c5300bc8e4a52d35c2038b4a49ca.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-6dd4820450d7ef4ea3e9b9a464170926.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c982dbf189008d1a166d11c8783a0d78.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="Aesthetic-Post-Training-Diffusion-Models-from-Generic-Preferences-with-Step-by-step-Preference-Optimization"><a href="#Aesthetic-Post-Training-Diffusion-Models-from-Generic-Preferences-with-Step-by-step-Preference-Optimization" class="headerlink" title="Aesthetic Post-Training Diffusion Models from Generic Preferences with   Step-by-step Preference Optimization"></a>Aesthetic Post-Training Diffusion Models from Generic Preferences with   Step-by-step Preference Optimization</h2><p><strong>Authors:Zhanhao Liang, Yuhui Yuan, Shuyang Gu, Bohan Chen, Tiankai Hang, Mingxi Cheng, Ji Li, Liang Zheng</strong></p>
<p>Generating visually appealing images is fundamental to modern text-to-image generation models. A potential solution to better aesthetics is direct preference optimization (DPO), which has been applied to diffusion models to improve general image quality including prompt alignment and aesthetics. Popular DPO methods propagate preference labels from clean image pairs to all the intermediate steps along the two generation trajectories. However, preference labels provided in existing datasets are blended with layout and aesthetic opinions, which would disagree with aesthetic preference. Even if aesthetic labels were provided (at substantial cost), it would be hard for the two-trajectory methods to capture nuanced visual differences at different steps. To improve aesthetics economically, this paper uses existing generic preference data and introduces step-by-step preference optimization (SPO) that discards the propagation strategy and allows fine-grained image details to be assessed. Specifically, at each denoising step, we 1) sample a pool of candidates by denoising from a shared noise latent, 2) use a step-aware preference model to find a suitable win-lose pair to supervise the diffusion model, and 3) randomly select one from the pool to initialize the next denoising step. This strategy ensures that diffusion models focus on the subtle, fine-grained visual differences instead of layout aspect. We find that aesthetics can be significantly enhanced by accumulating these improved minor differences. When fine-tuning Stable Diffusion v1.5 and SDXL, SPO yields significant improvements in aesthetics compared with existing DPO methods while not sacrificing image-text alignment compared with vanilla models. Moreover, SPO converges much faster than DPO methods due to the use of more correct preference labels provided by the step-aware preference model. </p>
<blockquote>
<p>ç”Ÿæˆè§†è§‰å¸å¼•åŠ›å¼ºçš„å›¾åƒæ˜¯ç°ä»£æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆæ¨¡å‹çš„æ ¸å¿ƒè¦ç´ ã€‚è§£å†³æ›´å¥½ç¾å­¦é—®é¢˜çš„æ½œåœ¨æ–¹æ³•æ˜¯ç›´æ¥åå¥½ä¼˜åŒ–ï¼ˆDPOï¼‰ï¼Œå®ƒå·²åº”ç”¨äºæ‰©æ•£æ¨¡å‹ï¼Œä»¥æé«˜å›¾åƒè´¨é‡ï¼ŒåŒ…æ‹¬æç¤ºå¯¹é½å’Œç¾å­¦ã€‚æµè¡Œçš„DPOæ–¹æ³•ä»æ¸…æ™°çš„å›¾åƒå¯¹å‘ä¸¤æ¡ç”Ÿæˆè½¨è¿¹çš„æ‰€æœ‰ä¸­é—´æ­¥éª¤ä¼ æ’­åå¥½æ ‡ç­¾ã€‚ç„¶è€Œï¼Œç°æœ‰æ•°æ®é›†ä¸­æä¾›çš„åå¥½æ ‡ç­¾ä¸å¸ƒå±€å’Œç¾å­¦è§‚ç‚¹æ··åˆåœ¨ä¸€èµ·ï¼Œè¿™ä¸ç¾å­¦åå¥½ç›¸çŸ›ç›¾ã€‚å³ä½¿æä¾›äº†ç¾å­¦æ ‡ç­¾ï¼ˆæˆæœ¬ç›¸å½“é«˜ï¼‰ï¼ŒåŒè½¨è¿¹æ–¹æ³•ä¹Ÿå¾ˆéš¾åœ¨ä¸åŒæ­¥éª¤ä¸­æ•æ‰å¾®å¦™çš„è§†è§‰å·®å¼‚ã€‚ä¸ºäº†ä»¥ç»æµçš„æ–¹å¼æé«˜ç¾å­¦æ•ˆæœï¼Œæœ¬æ–‡åˆ©ç”¨ç°æœ‰çš„é€šç”¨åå¥½æ•°æ®ï¼Œå¹¶å¼•å…¥äº†é€æ­¥åå¥½ä¼˜åŒ–ï¼ˆSPOï¼‰ï¼Œå®ƒæ”¾å¼ƒäº†ä¼ æ’­ç­–ç•¥ï¼Œå¹¶å…è®¸å¯¹å›¾åƒç»†èŠ‚è¿›è¡Œç²¾ç»†è¯„ä¼°ã€‚å…·ä½“æ¥è¯´ï¼Œåœ¨æ¯ä¸€æ­¥å»å™ªè¿‡ç¨‹ä¸­ï¼Œæˆ‘ä»¬1ï¼‰é€šè¿‡ä»å…±äº«å™ªå£°æ½œåœ¨ä¸­è¿›è¡Œå»å™ªæ¥é‡‡æ ·ä¸€ç»„å€™é€‰å¯¹è±¡ï¼Œ2ï¼‰ä½¿ç”¨é€æ­¥åå¥½æ¨¡å‹æ¥æ‰¾åˆ°é€‚åˆç›‘ç£æ‰©æ•£æ¨¡å‹çš„èƒœè´Ÿå¯¹ï¼Œ3ï¼‰ä»æ± ä¸­éšæœºé€‰æ‹©ä¸€ä¸ªæ¥åˆå§‹åŒ–ä¸‹ä¸€ä¸ªå»å™ªæ­¥éª¤ã€‚è¿™ç§ç­–ç•¥ç¡®ä¿æ‰©æ•£æ¨¡å‹ä¸“æ³¨äºç»†å¾®ã€ç²¾ç»†çš„è§†è§‰å·®å¼‚ï¼Œè€Œä¸æ˜¯å¸ƒå±€æ–¹é¢ã€‚æˆ‘ä»¬å‘ç°é€šè¿‡è¿™äº›æ”¹è¿›çš„å°å·®å¼‚ç§¯ç´¯ï¼Œå¯ä»¥æ˜¾è‘—æé«˜ç¾å­¦æ•ˆæœã€‚å½“å¯¹Stable Diffusion v1.5å’ŒSDXLè¿›è¡Œå¾®è°ƒæ—¶ï¼ŒSPOåœ¨ç¾å­¦æ–¹é¢ä¸ç°æœ‰DPOæ–¹æ³•ç›¸æ¯”å–å¾—äº†æ˜¾è‘—æ”¹è¿›ï¼ŒåŒæ—¶åœ¨å›¾åƒæ–‡æœ¬å¯¹é½æ–¹é¢ä¸åŸå§‹æ¨¡å‹ç›¸æ¯”æ²¡æœ‰ç‰ºç‰²ã€‚æ­¤å¤–ï¼Œç”±äºé€æ­¥åå¥½æ¨¡å‹æä¾›äº†æ›´æ­£ç¡®çš„åå¥½æ ‡ç­¾ï¼ŒSPOçš„æ”¶æ•›é€Ÿåº¦ä¹Ÿæ¯”DPOæ–¹æ³•å¿«å¾—å¤šã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2406.04314v3">PDF</a> CVPR 2025. Project Page: <a target="_blank" rel="noopener" href="https://rockeycoss.github.io/spo.github.io/">https://rockeycoss.github.io/spo.github.io/</a></p>
<p><strong>æ‘˜è¦</strong></p>
<p>æœ¬æ–‡æ¢è®¨äº†ç°ä»£æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆæ¨¡å‹ä¸­å›¾åƒç¾è§‚æ€§çš„é‡è¦æ€§ã€‚æå‡ºäº†ä¸€ç§åä¸ºé€æ­¥åå¥½ä¼˜åŒ–ï¼ˆSPOï¼‰çš„æ–¹æ³•æ¥è§£å†³è¿™ä¸€é—®é¢˜ï¼Œè¯¥æ–¹æ³•é‡‡ç”¨é€šç”¨åå¥½æ•°æ®ï¼Œæ‘’å¼ƒäº†ä¼ æ’­ç­–ç•¥ï¼Œèƒ½å¤Ÿè¯„ä¼°å›¾åƒçš„ç»†èŠ‚ã€‚åœ¨æ¯ä¸€æ­¥å»å™ªè¿‡ç¨‹ä¸­ï¼Œé€šè¿‡é‡‡æ ·å€™é€‰æ± ã€ä½¿ç”¨æ­¥éª¤æ„ŸçŸ¥åå¥½æ¨¡å‹æ‰¾åˆ°é€‚åˆçš„èƒœè´Ÿå¯¹æ¥ç›‘ç£æ‰©æ•£æ¨¡å‹ï¼Œå¹¶éšæœºé€‰æ‹©å€™é€‰æ± ä¸­çš„ä¸€ä¸ªæ¥åˆå§‹åŒ–ä¸‹ä¸€æ­¥çš„å»å™ªã€‚è¿™ç¡®ä¿äº†æ‰©æ•£æ¨¡å‹å…³æ³¨ç»†å¾®çš„è§†è§‰å·®å¼‚è€Œä¸æ˜¯å¸ƒå±€æ–¹é¢ã€‚å®éªŒè¡¨æ˜ï¼Œé€šè¿‡ç´¯ç§¯è¿™äº›æ”¹è¿›çš„å·®å¼‚ï¼Œå¯ä»¥æ˜¾è‘—æé«˜ç¾å­¦æ•ˆæœã€‚å¯¹Stable Diffusion v1.5å’ŒSDXLè¿›è¡Œå¾®è°ƒæ—¶ï¼ŒSPOåœ¨ç¾å­¦æ–¹é¢çš„æ”¹è¿›æ˜¾è‘—ä¼˜äºç°æœ‰çš„DPOæ–¹æ³•ï¼ŒåŒæ—¶ä¸ä¼šç‰ºç‰²å›¾åƒä¸æ–‡æœ¬çš„åŒ¹é…åº¦ï¼Œå¹¶ä¸”ç”±äºæ­¥éª¤æ„ŸçŸ¥åå¥½æ¨¡å‹æä¾›äº†æ›´æ­£ç¡®çš„åå¥½æ ‡ç­¾ï¼ŒSPOçš„æ”¶æ•›é€Ÿåº¦ä¹Ÿæ¯”DPOæ–¹æ³•å¿«å¾—å¤šã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆæ¨¡å‹ä¸­å›¾åƒç¾è§‚æ€§è‡³å…³é‡è¦ã€‚</li>
<li>ç›´æ¥åå¥½ä¼˜åŒ–ï¼ˆDPOï¼‰æ˜¯æé«˜å›¾åƒè´¨é‡çš„ä¸€ç§æ½œåœ¨è§£å†³æ–¹æ¡ˆï¼ŒåŒ…æ‹¬æç¤ºå¯¹é½å’Œç¾å­¦ã€‚</li>
<li>ç°æœ‰æ•°æ®é›†ä¸­çš„åå¥½æ ‡ç­¾ä¸å¸ƒå±€å’Œç¾å­¦æ„è§æ··åˆï¼Œå¯¼è‡´ä¸çœŸå®ç¾å­¦åå¥½å­˜åœ¨åˆ†æ­§ã€‚</li>
<li>ä¸¤è½¨è¿¹æ–¹æ³•éš¾ä»¥æ•æ‰ä¸åŒæ­¥éª¤ä¸­çš„ç»†å¾®è§†è§‰å·®å¼‚ã€‚</li>
<li>æœ¬æ–‡æå‡ºé€æ­¥åå¥½ä¼˜åŒ–ï¼ˆSPOï¼‰ï¼Œåˆ©ç”¨é€šç”¨åå¥½æ•°æ®ï¼Œå¹¶å…è®¸è¯„ä¼°å›¾åƒçš„ç»†èŠ‚ã€‚</li>
<li>SPOåœ¨æ¯ä¸€æ­¥å»å™ªè¿‡ç¨‹ä¸­é‡‡æ ·å€™é€‰æ± ï¼Œå¹¶ä½¿ç”¨æ­¥éª¤æ„ŸçŸ¥åå¥½æ¨¡å‹æ¥ç›‘ç£æ‰©æ•£æ¨¡å‹ã€‚</li>
<li>å®éªŒè¡¨æ˜ï¼ŒSPOåœ¨ç¾å­¦æ–¹é¢æœ‰æ˜¾è‘—æ”¹è¿›ï¼Œä¸”ä¸ä¼šç‰ºç‰²å›¾åƒä¸æ–‡æœ¬çš„åŒ¹é…åº¦ï¼Œæ”¶æ•›é€Ÿåº¦ä¹Ÿæ¯”DPOå¿«ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2406.04314">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-ad9181ffcb5fc318f76f3bd5d8386eea.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-eb6fe4239b095b0ca443fd68ce895f23.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-73e35948b0ad4f0c247473126fc8fe6b.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-2842cc2ae309a436b207fafb138ec820.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-baa866e41c9ab04e01cd17d20f2995b4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6bbe3ac49013638cf5c1223107bafa47.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="Reanimating-Images-using-Neural-Representations-of-Dynamic-Stimuli"><a href="#Reanimating-Images-using-Neural-Representations-of-Dynamic-Stimuli" class="headerlink" title="Reanimating Images using Neural Representations of Dynamic Stimuli"></a>Reanimating Images using Neural Representations of Dynamic Stimuli</h2><p><strong>Authors:Jacob Yeung, Andrew F. Luo, Gabriel Sarch, Margaret M. Henderson, Deva Ramanan, Michael J. Tarr</strong></p>
<p>While computer vision models have made incredible strides in static image recognition, they still do not match human performance in tasks that require the understanding of complex, dynamic motion. This is notably true for real-world scenarios where embodied agents face complex and motion-rich environments. Our approach, BrainNRDS (Brain-Neural Representations of Dynamic Stimuli), leverages state-of-the-art video diffusion models to decouple static image representation from motion generation, enabling us to utilize fMRI brain activity for a deeper understanding of human responses to dynamic visual stimuli. Conversely, we also demonstrate that information about the brainâ€™s representation of motion can enhance the prediction of optical flow in artificial systems. Our novel approach leads to four main findings: (1) Visual motion, represented as fine-grained, object-level resolution optical flow, can be decoded from brain activity generated by participants viewing video stimuli; (2) Video encoders outperform image-based models in predicting video-driven brain activity; (3) Brain-decoded motion signals enable realistic video reanimation based only on the initial frame of the video; and (4) We extend prior work to achieve full video decoding from video-driven brain activity. BrainNRDS advances our understanding of how the brain represents spatial and temporal information in dynamic visual scenes. Our findings demonstrate the potential of combining brain imaging with video diffusion models for developing more robust and biologically-inspired computer vision systems. We show additional decoding and encoding examples on this site: <a target="_blank" rel="noopener" href="https://brain-nrds.github.io/">https://brain-nrds.github.io/</a>. </p>
<blockquote>
<p>è®¡ç®—æœºè§†è§‰æ¨¡å‹åœ¨é™æ€å›¾åƒè¯†åˆ«æ–¹é¢å–å¾—äº†ä»¤äººéš¾ä»¥ç½®ä¿¡çš„è¿›å±•ï¼Œä½†åœ¨éœ€è¦ç†è§£å¤æ‚åŠ¨æ€è¿åŠ¨çš„ä»»åŠ¡ä¸­ï¼Œå®ƒä»¬ä»ç„¶æ— æ³•ä¸äººç±»çš„è¡¨ç°ç›¸åŒ¹é…ã€‚è¿™åœ¨é¢å¯¹å¤æ‚ä¸”åŠ¨ä½œä¸°å¯Œçš„ç¯å¢ƒçš„ç°å®åœºæ™¯ä¸­æ˜¯å°¤ä¸ºæ˜æ˜¾çš„ã€‚æˆ‘ä»¬çš„æ–¹æ³•BrainNRDSï¼ˆåŠ¨æ€åˆºæ¿€çš„å¤§è„‘ç¥ç»è¡¨å¾ï¼‰åˆ©ç”¨æœ€å…ˆè¿›çš„è§†é¢‘æ‰©æ•£æ¨¡å‹ï¼Œå°†é™æ€å›¾åƒè¡¨å¾ä¸è¿åŠ¨ç”Ÿæˆè§£è€¦ï¼Œä½¿æˆ‘ä»¬èƒ½å¤Ÿåˆ©ç”¨åŠŸèƒ½æ€§ç£å…±æŒ¯æˆåƒï¼ˆfMRIï¼‰çš„å¤§è„‘æ´»åŠ¨æ¥æ›´æ·±å…¥åœ°ç†è§£äººç±»å¯¹åŠ¨æ€è§†è§‰åˆºæ¿€çš„ååº”ã€‚ç›¸åï¼Œæˆ‘ä»¬è¿˜è¯æ˜äº†å…³äºå¤§è„‘å¯¹è¿åŠ¨çš„è¡¨å¾çš„ä¿¡æ¯å¯ä»¥æé«˜äººå·¥ç³»ç»Ÿçš„å…‰æµé¢„æµ‹èƒ½åŠ›ã€‚æˆ‘ä»¬çš„æ–°æ–¹æ³•å¸¦æ¥äº†å››ä¸ªä¸»è¦å‘ç°ï¼šï¼ˆ1ï¼‰è§†è§‰è¿åŠ¨ï¼Œè¡¨ç°ä¸ºç²¾ç»†çš„ã€ä»¥ç‰©ä½“ä¸ºçº§åˆ«çš„åˆ†è¾¨ç‡çš„å…‰æµï¼Œå¯ä»¥ä»å‚ä¸è€…è§‚çœ‹è§†é¢‘åˆºæ¿€æ‰€äº§ç”Ÿçš„å¤§è„‘æ´»åŠ¨ä¸­è§£ç å‡ºæ¥ï¼›ï¼ˆ2ï¼‰è§†é¢‘ç¼–ç å™¨åœ¨é¢„æµ‹è§†é¢‘é©±åŠ¨çš„å¤§è„‘æ´»åŠ¨æ–¹é¢ä¼˜äºåŸºäºå›¾åƒæ¨¡å‹ï¼›ï¼ˆ3ï¼‰é€šè¿‡å¤§è„‘è§£ç çš„è¿åŠ¨ä¿¡å·ä»…åŸºäºè§†é¢‘çš„åˆå§‹å¸§å®ç°ç°å®è§†é¢‘çš„é‡ç°ï¼›ï¼ˆ4ï¼‰æˆ‘ä»¬åœ¨å…ˆå‰å·¥ä½œçš„åŸºç¡€ä¸Šå®ç°äº†ä»è§†é¢‘é©±åŠ¨çš„å¤§è„‘æ´»åŠ¨ä¸­è¿›è¡Œå®Œæ•´è§†é¢‘è§£ç ã€‚BrainNRDSæ¨è¿›äº†æˆ‘ä»¬å¯¹å¤§è„‘å¦‚ä½•åœ¨åŠ¨æ€è§†è§‰åœºæ™¯ä¸­è¡¨å¾ç©ºé—´å’Œæ—¶é—´ä¿¡æ¯çš„ç†è§£ã€‚æˆ‘ä»¬çš„ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œç»“åˆè„‘æˆåƒå’Œè§†é¢‘æ‰©æ•£æ¨¡å‹åœ¨å¼€å‘æ›´ç¨³å¥ä¸”å—ç”Ÿç‰©å­¦å¯å‘çš„è®¡ç®—æœºè§†è§‰ç³»ç»Ÿä¸­å…·æœ‰æ½œåŠ›ã€‚æˆ‘ä»¬åœ¨ä»¥ä¸‹ç½‘ç«™å±•ç¤ºäº†æ›´å¤šçš„è§£ç å’Œç¼–ç ç¤ºä¾‹ï¼š<a target="_blank" rel="noopener" href="https://brain-nrds.github.io/">https://brain-nrds.github.io/</a>.</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2406.02659v3">PDF</a> Project Page: <a target="_blank" rel="noopener" href="https://brain-nrds.github.io/">https://brain-nrds.github.io</a></p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†BrainNRDSï¼ˆåŠ¨æ€åˆºæ¿€çš„å¤§è„‘ç¥ç»è¡¨ç¤ºï¼‰æ–¹æ³•ï¼Œè¯¥æ–¹æ³•åˆ©ç”¨å…ˆè¿›çš„è§†é¢‘æ‰©æ•£æ¨¡å‹å°†é™æ€å›¾åƒè¡¨ç¤ºä¸è¿åŠ¨ç”Ÿæˆç›¸åˆ†ç¦»ï¼Œä»è€Œåˆ©ç”¨fMRIå¤§è„‘æ´»åŠ¨æ›´æ·±å…¥åœ°äº†è§£äººç±»å¯¹åŠ¨æ€è§†è§‰åˆºæ¿€çš„ååº”ã€‚æ­¤å¤–ï¼Œè¿˜å±•ç¤ºäº†å¤§è„‘å¯¹è¿åŠ¨çš„è¡¨ç¤ºä¿¡æ¯å¯ä»¥æé«˜äººå·¥ç³»ç»Ÿçš„å…‰å­¦æµé¢„æµ‹èƒ½åŠ›ã€‚BrainNRDSæ–¹æ³•çš„ä¸»è¦è´¡çŒ®åŒ…æ‹¬ä»å¤§è„‘æ´»åŠ¨ä¸­è§£ç ç”±è§‚çœ‹è§†é¢‘åˆºæ¿€äº§ç”Ÿçš„è§†è§‰è¿åŠ¨ä¿¡æ¯ã€è§†é¢‘ç¼–ç å™¨åœ¨é¢„æµ‹è§†é¢‘é©±åŠ¨çš„å¤§è„‘æ´»åŠ¨æ–¹é¢ä¼˜äºå›¾åƒæ¨¡å‹ã€ä»…åŸºäºè§†é¢‘çš„åˆå§‹å¸§è¿›è¡Œé€¼çœŸçš„è§†é¢‘å†åŠ¨ç”»ä»¥åŠä»è§†é¢‘é©±åŠ¨çš„å¤§è„‘æ´»åŠ¨ä¸­å®ç°å…¨è§†é¢‘è§£ç ã€‚è¯¥ç ”ç©¶ä¸ºç»“åˆè„‘æˆåƒå’Œè§†é¢‘æ‰©æ•£æ¨¡å‹å¼€å‘æ›´å¼ºå¤§å’Œæ›´å—ç”Ÿç‰©å­¦å¯å‘çš„è®¡ç®—æœºè§†è§‰ç³»ç»Ÿæä¾›äº†æ½œåŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>BrainNRDSæ–¹æ³•åˆ©ç”¨è§†é¢‘æ‰©æ•£æ¨¡å‹å°†é™æ€å›¾åƒè¡¨ç¤ºä¸è¿åŠ¨ç”Ÿæˆåˆ†ç¦»ã€‚</li>
<li>åˆ©ç”¨fMRIå¤§è„‘æ´»åŠ¨è§£ç äººç±»å¯¹åŠ¨æ€è§†è§‰åˆºæ¿€çš„ååº”ã€‚</li>
<li>è§†é¢‘ç¼–ç å™¨åœ¨é¢„æµ‹è§†é¢‘é©±åŠ¨çš„å¤§è„‘æ´»åŠ¨æ–¹é¢ä¼˜äºå›¾åƒæ¨¡å‹ã€‚</li>
<li>ä»å¤§è„‘æ´»åŠ¨ä¸­è§£ç å‡ºçš„è¿åŠ¨ä¿¡å·å¯å®ç°åŸºäºåˆå§‹å¸§çš„è§†é¢‘å†åŠ¨ç”»ã€‚</li>
<li>BrainNRDSå®ç°äº†ä»è§†é¢‘é©±åŠ¨çš„å¤§è„‘æ´»åŠ¨ä¸­è¿›è¡Œå…¨è§†é¢‘è§£ç ã€‚</li>
<li>BrainNRDSä¸ºç†è§£å¤§è„‘åœ¨åŠ¨æ€è§†è§‰åœºæ™¯ä¸­å¦‚ä½•è¡¨ç¤ºç©ºé—´å’Œæ—¶é—´ä¿¡æ¯æä¾›äº†è¿›å±•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2406.02659">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-cfa6913afaac183c68838e1d7b481f9b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-092239dc9ae851338f35b245bb4f8d78.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-7b7b234fa1b121d30ad9e2f4371c87c6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6cf7c1d5c1adf8fb59cfa7f696d74f7a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3bd24f6de653b81789d8a730e7a10081.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-bfa333f03e7ff87583617c89114fac69.jpg" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="ManiCM-Real-time-3D-Diffusion-Policy-via-Consistency-Model-for-Robotic-Manipulation"><a href="#ManiCM-Real-time-3D-Diffusion-Policy-via-Consistency-Model-for-Robotic-Manipulation" class="headerlink" title="ManiCM: Real-time 3D Diffusion Policy via Consistency Model for Robotic   Manipulation"></a>ManiCM: Real-time 3D Diffusion Policy via Consistency Model for Robotic   Manipulation</h2><p><strong>Authors:Guanxing Lu, Zifeng Gao, Tianxing Chen, Wenxun Dai, Ziwei Wang, Wenbo Ding, Yansong Tang</strong></p>
<p>Diffusion models have been verified to be effective in generating complex distributions from natural images to motion trajectories. Recent diffusion-based methods show impressive performance in 3D robotic manipulation tasks, whereas they suffer from severe runtime inefficiency due to multiple denoising steps, especially with high-dimensional observations. To this end, we propose a real-time robotic manipulation model named ManiCM that imposes the consistency constraint to the diffusion process, so that the model can generate robot actions in only one-step inference. Specifically, we formulate a consistent diffusion process in the robot action space conditioned on the point cloud input, where the original action is required to be directly denoised from any point along the ODE trajectory. To model this process, we design a consistency distillation technique to predict the action sample directly instead of predicting the noise within the vision community for fast convergence in the low-dimensional action manifold. We evaluate ManiCM on 31 robotic manipulation tasks from Adroit and Metaworld, and the results demonstrate that our approach accelerates the state-of-the-art method by 10 times in average inference speed while maintaining competitive average success rate. </p>
<blockquote>
<p>æ‰©æ•£æ¨¡å‹å·²è¢«éªŒè¯åœ¨ç”Ÿæˆä»è‡ªç„¶å›¾åƒåˆ°è¿åŠ¨è½¨è¿¹çš„å¤æ‚åˆ†å¸ƒæ–¹é¢éå¸¸æœ‰æ•ˆã€‚æœ€è¿‘çš„åŸºäºæ‰©æ•£çš„æ–¹æ³•åœ¨3Dæœºå™¨äººæ“ä½œä»»åŠ¡ä¸­è¡¨ç°å‡ºä»¤äººå°è±¡æ·±åˆ»çš„æ•ˆæœï¼Œä½†ç”±äºå¤šä¸ªé™å™ªæ­¥éª¤ï¼Œå°¤å…¶æ˜¯åœ¨é«˜ç»´è§‚å¯Ÿæƒ…å†µä¸‹ï¼Œå®ƒä»¬é­å—ä¸¥é‡çš„è¿è¡Œæ—¶æ•ˆç‡ä½ä¸‹çš„é—®é¢˜ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§å®æ—¶æœºå™¨äººæ“ä½œæ¨¡å‹ï¼Œåä¸ºManiCMï¼Œè¯¥æ¨¡å‹å¯¹æ‰©æ•£è¿‡ç¨‹æ–½åŠ ä¸€è‡´æ€§çº¦æŸï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿåœ¨ä¸€æ¬¡æ­¥éª¤æ¨æ–­ä¸­ç”Ÿæˆæœºå™¨äººåŠ¨ä½œã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬åœ¨æœºå™¨äººåŠ¨ä½œç©ºé—´ä¸­åˆ¶å®šäº†åŸºäºç‚¹äº‘è¾“å…¥çš„è¿ç»­æ‰©æ•£è¿‡ç¨‹ï¼Œå…¶ä¸­åŸå§‹åŠ¨ä½œéœ€è¦ç›´æ¥ä»ODEè½¨è¿¹çš„ä»»ä½•ä¸€ç‚¹è¿›è¡Œé™å™ªã€‚ä¸ºäº†æ¨¡æ‹Ÿè¿™ä¸€è¿‡ç¨‹ï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸€ç§ä¸€è‡´æ€§è’¸é¦æŠ€æœ¯ï¼Œç›´æ¥é¢„æµ‹åŠ¨ä½œæ ·æœ¬ï¼Œè€Œä¸æ˜¯åœ¨è§†è§‰ç¤¾åŒºå†…é¢„æµ‹å™ªå£°ï¼Œä»¥åœ¨ä½ç»´åŠ¨ä½œæµå½¢ä¸­å®ç°å¿«é€Ÿæ”¶æ•›ã€‚æˆ‘ä»¬åœ¨Adroitå’ŒMetaworldçš„31ä¸ªæœºå™¨äººæ“ä½œä»»åŠ¡ä¸Šè¯„ä¼°äº†ManiCMï¼Œç»“æœè¡¨æ˜æˆ‘ä»¬çš„æ–¹æ³•å¹³å‡æ¨æ–­é€Ÿåº¦åŠ å¿«äº†10å€ï¼ŒåŒæ—¶ä¿æŒäº†å…·æœ‰ç«äº‰åŠ›çš„å¹³å‡æˆåŠŸç‡ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2406.01586v2">PDF</a> <a target="_blank" rel="noopener" href="https://manicm-fast.github.io/">https://manicm-fast.github.io/</a></p>
<p><strong>Summary</strong><br>æ‰©æ•£æ¨¡å‹åœ¨è‡ªç„¶å›¾åƒåˆ°è¿åŠ¨è½¨è¿¹çš„å¤æ‚åˆ†å¸ƒç”Ÿæˆä¸­è¡¨ç°å‡ºæ•ˆæœã€‚é’ˆå¯¹ç°æœ‰æ‰©æ•£æ–¹æ³•åœ¨é«˜ç»´è§‚æµ‹ä¸­çš„è¿è¡Œæ—¶æ•ˆç‡ä½ä¸‹é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§å®æ—¶æœºå™¨äººæ“ä½œæ¨¡å‹ManiCMã€‚è¯¥æ¨¡å‹å¯¹æ‰©æ•£è¿‡ç¨‹æ–½åŠ ä¸€è‡´æ€§çº¦æŸï¼Œå®ç°äº†ä¸€æ­¥æ¨ç†ç”Ÿæˆæœºå™¨äººåŠ¨ä½œã€‚é€šè¿‡è®¾è®¡ä¸€è‡´æ€§è’¸é¦æŠ€æœ¯ï¼Œç›´æ¥åœ¨æœºå™¨äººåŠ¨ä½œç©ºé—´å†…å¯¹ç‚¹äº‘è¾“å…¥è¿›è¡Œæ¡ä»¶åŒ–ï¼Œç›´æ¥é¢„æµ‹åŠ¨ä½œæ ·æœ¬ï¼Œè€Œéé¢„æµ‹å™ªå£°ï¼Œå®ç°äº†å¿«é€Ÿæ”¶æ•›ã€‚åœ¨Adroitå’ŒMetaworldçš„31ä¸ªæœºå™¨äººæ“ä½œä»»åŠ¡ä¸Šçš„è¯„ä¼°ç»“æœè¡¨æ˜ï¼ŒManiCMå¹³å‡æ¨ç†é€Ÿåº¦æé«˜äº†10å€ï¼ŒåŒæ—¶ä¿æŒäº†ç«äº‰åŠ›å¹³å‡æˆåŠŸç‡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ‰©æ•£æ¨¡å‹åœ¨è‡ªç„¶å›¾åƒåˆ°è¿åŠ¨è½¨è¿¹çš„ç”Ÿæˆä¸­æ•ˆæœæ˜¾è‘—ã€‚</li>
<li>ç°æœ‰æ‰©æ•£æ–¹æ³•åœ¨é«˜ç»´è§‚æµ‹ä¸­å­˜åœ¨è¿è¡Œæ—¶æ•ˆç‡ä½ä¸‹çš„é—®é¢˜ã€‚</li>
<li>æå‡ºäº†å®æ—¶æœºå™¨äººæ“ä½œæ¨¡å‹ManiCMï¼Œé€šè¿‡ä¸€è‡´æ€§çº¦æŸå®ç°äº†ä¸€æ­¥æ¨ç†ç”Ÿæˆæœºå™¨äººåŠ¨ä½œã€‚</li>
<li>ManiCMåœ¨æœºå™¨äººåŠ¨ä½œç©ºé—´å†…ç›´æ¥å¯¹ç‚¹äº‘è¾“å…¥è¿›è¡Œæ¡ä»¶åŒ–ï¼Œæé«˜äº†æ•ˆç‡ã€‚</li>
<li>è®¾è®¡äº†ä¸€è‡´æ€§è’¸é¦æŠ€æœ¯ï¼Œç›´æ¥é¢„æµ‹åŠ¨ä½œæ ·æœ¬ï¼Œè€Œéé¢„æµ‹å™ªå£°ã€‚</li>
<li>åœ¨å¤šä¸ªæœºå™¨äººæ“ä½œä»»åŠ¡ä¸Šçš„è¯„ä¼°è¡¨æ˜ï¼ŒManiCMæ˜¾è‘—æé«˜äº†æ¨ç†é€Ÿåº¦å¹¶ä¿æŒç«äº‰åŠ›æˆåŠŸç‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2406.01586">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-d485c2113c1ee635a4bf0064142b5f95.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-3b214f0644c76a00bb208e038b35ca78.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c45d9e3ff3f28b23ad776265a771d974.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d4b3db196d604850af82352f5d71725a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-02b516812233761858cc51ecf744163a.jpg" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-03-28/Diffusion%20Models/">https://kedreamix.github.io/Talk2Paper/Paper/2025-03-28/Diffusion%20Models/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Diffusion-Models/">
                                    <span class="chip bg-color">Diffusion Models</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-03-28/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-0184ea86406ab9a6846aa56a6f59dc02.jpg" class="responsive-img" alt="åŒ»å­¦å›¾åƒ">
                        
                        <span class="card-title">åŒ»å­¦å›¾åƒ</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            åŒ»å­¦å›¾åƒ æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-03-28  Explaining the UV to X-ray correlation in AGN within the framework of   X-ray illumination of accretion discs
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-03-28
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/" class="post-category">
                                    åŒ»å­¦å›¾åƒ
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                        <span class="chip bg-color">åŒ»å­¦å›¾åƒ</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-03-28/NeRF/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-81672d48d555559aeac30660193b639e.jpg" class="responsive-img" alt="NeRF">
                        
                        <span class="card-title">NeRF</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            NeRF æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-03-28  EVolSplat Efficient Volume-based Gaussian Splatting for Urban View   Synthesis
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-03-28
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/NeRF/" class="post-category">
                                    NeRF
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/NeRF/">
                        <span class="chip bg-color">NeRF</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">16190.2k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
