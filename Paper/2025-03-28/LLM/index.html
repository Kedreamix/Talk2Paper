<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="LLM">
    <meta name="description" content="LLM æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-03-28  Mobile-MMLU A Mobile Intelligence Language Understanding Benchmark">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>LLM | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pica.zhimg.com/v2-dd109bd7323dd466451e40f008ab6c81.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">LLM</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/LLM/">
                                <span class="chip bg-color">LLM</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/LLM/" class="post-category">
                                LLM
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-03-28
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-05-14
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    20.2k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    82 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-03-28-æ›´æ–°"><a href="#2025-03-28-æ›´æ–°" class="headerlink" title="2025-03-28 æ›´æ–°"></a>2025-03-28 æ›´æ–°</h1><h2 id="Mobile-MMLU-A-Mobile-Intelligence-Language-Understanding-Benchmark"><a href="#Mobile-MMLU-A-Mobile-Intelligence-Language-Understanding-Benchmark" class="headerlink" title="Mobile-MMLU: A Mobile Intelligence Language Understanding Benchmark"></a>Mobile-MMLU: A Mobile Intelligence Language Understanding Benchmark</h2><p><strong>Authors:Sondos Mahmoud Bsharat, Mukul Ranjan, Aidar Myrzakhan, Jiacheng Liu, Bowei Guo, Shengkun Tang, Zhuang Liu, Yuanzhi Li, Zhiqiang Shen</strong></p>
<p>Rapid advancements in large language models (LLMs) have increased interest in deploying them on mobile devices for on-device AI applications. Mobile users interact differently with LLMs compared to desktop users, creating unique expectations and data biases. Current benchmark datasets primarily target at server and desktop environments, and there is a notable lack of extensive datasets specifically designed for mobile contexts. Additionally, mobile devices face strict limitations in storage and computing resources, constraining model size and capabilities, thus requiring optimized efficiency and prioritized knowledge. To address these challenges, we introduce Mobile-MMLU, a large-scale benchmark dataset tailored for mobile intelligence. It consists of 16,186 questions across 80 mobile-related fields, designed to evaluate LLM performance in realistic mobile scenarios. A challenging subset, Mobile-MMLU-Pro, provides advanced evaluation similar in size to MMLU-Pro but significantly more difficult than our standard full set. Both benchmarks use multiple-choice, order-invariant questions focused on practical mobile interactions, such as recipe suggestions, travel planning, and essential daily tasks. The dataset emphasizes critical mobile-specific metrics like inference latency, energy consumption, memory usage, and response quality, offering comprehensive insights into model performance under mobile constraints. Moreover, it prioritizes privacy and adaptability, assessing modelsâ€™ ability to perform on-device processing, maintain user privacy, and adapt to personalized usage patterns. Mobile-MMLU family offers a standardized framework for developing and comparing mobile-optimized LLMs, enabling advancements in productivity and decision-making within mobile computing environments. Our code and data are available at: <a target="_blank" rel="noopener" href="https://github.com/VILA-Lab/Mobile-MMLU">https://github.com/VILA-Lab/Mobile-MMLU</a>. </p>
<blockquote>
<p>éšç€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„å¿«é€Ÿå‘å±•ï¼Œäººä»¬å¯¹å…¶åœ¨ç§»åŠ¨è®¾å¤‡ä¸Šçš„éƒ¨ç½²å…´è¶£æ—¥ç›Šæµ“åšï¼Œä»¥æ”¯æŒç§»åŠ¨è®¾å¤‡ä¸Šçš„AIåº”ç”¨ç¨‹åºã€‚ä¸æ¡Œé¢ç”¨æˆ·ç›¸æ¯”ï¼Œç§»åŠ¨ç”¨æˆ·ä¸LLMçš„äº¤äº’æ–¹å¼ä¸åŒï¼Œä»è€Œäº§ç”Ÿç‹¬ç‰¹çš„æœŸæœ›å’Œæ•°æ®åè§ã€‚å½“å‰çš„åŸºå‡†æ•°æ®é›†ä¸»è¦é¢å‘æœåŠ¡å™¨å’Œæ¡Œé¢ç¯å¢ƒï¼Œé’ˆå¯¹ç§»åŠ¨ç¯å¢ƒçš„ä¸“é—¨è®¾è®¡çš„ç»¼åˆæ•°æ®é›†æ˜æ˜¾ç¼ºä¹ã€‚æ­¤å¤–ï¼Œç§»åŠ¨è®¾å¤‡åœ¨å­˜å‚¨å’Œè®¡ç®—èµ„æºæ–¹é¢é¢ä¸´ä¸¥æ ¼é™åˆ¶ï¼Œé™åˆ¶äº†æ¨¡å‹çš„å¤§å°å’ŒåŠŸèƒ½ï¼Œå› æ­¤éœ€è¦ä¼˜åŒ–æ•ˆç‡å¹¶ä¼˜å…ˆè€ƒè™‘çŸ¥è¯†ã€‚ä¸ºäº†åº”å¯¹è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æ¨å‡ºäº†Mobile-MMLUï¼Œè¿™æ˜¯ä¸€ä¸ªä¸“ä¸ºç§»åŠ¨æ™ºèƒ½å®šåˆ¶çš„å¤§è§„æ¨¡åŸºå‡†æ•°æ®é›†ã€‚å®ƒåŒ…å«80ä¸ªä¸ç§»åŠ¨ç›¸å…³çš„é¢†åŸŸçš„16,186ä¸ªé—®é¢˜ï¼Œæ—¨åœ¨è¯„ä¼°LLMåœ¨çœŸå®ç§»åŠ¨åœºæ™¯ä¸­çš„æ€§èƒ½ã€‚å…·æœ‰æŒ‘æˆ˜æ€§çš„å­é›†Mobile-MMLU-Proæä¾›äº†ä¸MMLU-Proç±»ä¼¼è§„æ¨¡çš„å…ˆè¿›è¯„ä¼°ï¼Œä½†æ¯”æˆ‘ä»¬çš„æ ‡å‡†å…¨é›†éš¾åº¦æ›´å¤§ã€‚è¿™ä¸¤ä¸ªåŸºå‡†æµ‹è¯•éƒ½ä½¿ç”¨å¤šé€‰ã€é¡ºåºä¸å˜çš„é—®é¢˜ï¼Œä¾§é‡äºå®é™…çš„ç§»åŠ¨äº¤äº’ï¼Œå¦‚æä¾›é£Ÿè°±å»ºè®®ã€æ—…è¡Œè®¡åˆ’å’Œæ—¥å¸¸å¿…è¦ä»»åŠ¡ç­‰ã€‚è¯¥æ•°æ®é›†å¼ºè°ƒå…³é”®çš„ç§»åŠ¨ç‰¹å®šæŒ‡æ ‡ï¼Œå¦‚æ¨ç†å»¶è¿Ÿã€èƒ½è€—ã€å†…å­˜ä½¿ç”¨å’Œå“åº”è´¨é‡ç­‰ï¼Œæä¾›äº†åœ¨ç§»åŠ¨çº¦æŸä¸‹æ¨¡å‹æ€§èƒ½çš„å…¨é¢è§è§£ã€‚æ­¤å¤–ï¼Œå®ƒä¼˜å…ˆè€ƒè™‘éšç§å’Œé€‚åº”æ€§ï¼Œè¯„ä¼°æ¨¡å‹åœ¨è®¾å¤‡ä¸Šå¤„ç†ã€ç»´æŠ¤ç”¨æˆ·éšç§å’Œé€‚åº”ä¸ªæ€§åŒ–ä½¿ç”¨æ¨¡å¼çš„èƒ½åŠ›ã€‚Mobile-MMLUç³»åˆ—ä¸ºå¼€å‘æ¯”è¾ƒç§»åŠ¨è®¾å¤‡ä¼˜åŒ–çš„LLMæä¾›äº†æ ‡å‡†åŒ–æ¡†æ¶ï¼Œæœ‰åŠ©äºåœ¨ç§»åŠ¨è®¡ç®—ç¯å¢ƒä¸­æé«˜ç”Ÿäº§åŠ›å’Œå†³ç­–èƒ½åŠ›ã€‚æˆ‘ä»¬çš„ä»£ç å’Œæ•°æ®é›†å¯åœ¨ä»¥ä¸‹ç½‘å€æ‰¾åˆ°ï¼š<a target="_blank" rel="noopener" href="https://github.com/VILA-Lab/Mobile-MMLU%E3%80%82">https://github.com/VILA-Lab/Mobile-MMLUã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.20786v1">PDF</a> An order-invariant and mobile-centric benchmark. Code and data are   available at: <a target="_blank" rel="noopener" href="https://github.com/VILA-Lab/Mobile-MMLU">https://github.com/VILA-Lab/Mobile-MMLU</a></p>
<p><strong>Summary</strong></p>
<p>éšç€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„å¿«é€Ÿå‘å±•ï¼Œç§»åŠ¨è®¾å¤‡ä¸Šå¯¹LLMçš„éƒ¨ç½²æ—¥ç›Šå—åˆ°å…³æ³¨ã€‚ä¸ºæ»¡è¶³ç§»åŠ¨ç”¨æˆ·å¯¹LLMçš„ç‰¹å®šéœ€æ±‚å’Œé¢„æœŸï¼Œè¿«åˆ‡éœ€è¦é’ˆå¯¹ç§»åŠ¨è®¾å¤‡çš„å¤§å‹åŸºå‡†æ•°æ®é›†ã€‚æˆ‘ä»¬æ¨å‡ºäº†Mobile-MMLUï¼Œè¿™æ˜¯ä¸€ä¸ªä¸“é—¨ä¸ºç§»åŠ¨æ™ºèƒ½è®¾è®¡çš„å¤§è§„æ¨¡åŸºå‡†æ•°æ®é›†ï¼ŒåŒ…å«é’ˆå¯¹ç§»åŠ¨ç›¸å…³é¢†åŸŸçš„å®é™…é—®é¢˜ã€‚æ­¤æ•°æ®é›†å¼ºè°ƒç§»åŠ¨ç‰¹å®šæŒ‡æ ‡ï¼Œå¹¶æä¾›éšç§å’Œé€‚åº”æ€§çš„è¯„ä¼°ã€‚Mobile-MMLUç³»åˆ—ä¸ºå¼€å‘å¹¶æ¯”è¾ƒç§»åŠ¨ä¼˜åŒ–çš„LLMæä¾›äº†æ ‡å‡†åŒ–æ¡†æ¶ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨ç§»åŠ¨è®¾å¤‡ä¸Šçš„éƒ¨ç½²å’Œåº”ç”¨éœ€æ±‚ä¸æ–­å¢é•¿ã€‚</li>
<li>ç§»åŠ¨ç”¨æˆ·å¯¹LLMçš„éœ€æ±‚å’Œé¢„æœŸä¸æ¡Œé¢ç”¨æˆ·ä¸åŒã€‚</li>
<li>å½“å‰åŸºå‡†æ•°æ®é›†ä¸»è¦é¢å‘æœåŠ¡å™¨å’Œæ¡Œé¢ç¯å¢ƒï¼Œç¼ºä¹é’ˆå¯¹ç§»åŠ¨ç¯å¢ƒçš„ä¸“é—¨æ•°æ®é›†ã€‚</li>
<li>Mobile-MMLUæ˜¯ä¸€ä¸ªé’ˆå¯¹ç§»åŠ¨æ™ºèƒ½çš„å¤§è§„æ¨¡åŸºå‡†æ•°æ®é›†ï¼ŒåŒ…å«é’ˆå¯¹ç§»åŠ¨ç›¸å…³é¢†åŸŸçš„å®é™…é—®é¢˜ã€‚</li>
<li>Mobile-MMLUå¼ºè°ƒç§»åŠ¨ç‰¹å®šæŒ‡æ ‡ï¼Œå¦‚æ¨ç†å»¶è¿Ÿã€èƒ½è€—ã€å†…å­˜ä½¿ç”¨å’Œå“åº”è´¨é‡ã€‚</li>
<li>Mobile-MMLUæ³¨é‡éšç§å’Œé€‚åº”æ€§è¯„ä¼°ï¼Œè¯„ä¼°æ¨¡å‹åœ¨è®¾å¤‡ä¸Šçš„å¤„ç†èƒ½åŠ›ã€ç”¨æˆ·éšç§ä¿æŠ¤å’Œé€‚åº”ä¸ªæ€§åŒ–ä½¿ç”¨æ¨¡å¼çš„èƒ½åŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.20786">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-efb4e71d65af5e96180e96b2117a3cf8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-857866a3b8659f4695b8fcbca168a4c2.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-5f4268afc5f991d8d2ad68ca53925ba9.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-9a5fd959712c540880053e2ab4134e80.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Beyond-Believability-Accurate-Human-Behavior-Simulation-with-Fine-Tuned-LLMs"><a href="#Beyond-Believability-Accurate-Human-Behavior-Simulation-with-Fine-Tuned-LLMs" class="headerlink" title="Beyond Believability: Accurate Human Behavior Simulation with Fine-Tuned   LLMs"></a>Beyond Believability: Accurate Human Behavior Simulation with Fine-Tuned   LLMs</h2><p><strong>Authors:Yuxuan Lu, Jing Huang, Yan Han, Bennet Bei, Yaochen Xie, Dakuo Wang, Jessie Wang, Qi He</strong></p>
<p>Recent research shows that LLMs can simulate <code>believable&#39;&#39; human behaviors to power LLM agents via prompt-only methods. In this work, we focus on evaluating and improving LLM&#39;s objective </code>accuracyâ€™â€™ rather than the subjective &#96;&#96;believabilityâ€™â€™ in the web action generation task, leveraging a large-scale, real-world dataset collected from online shopping human actions. We present the first comprehensive quantitative evaluation of state-of-the-art LLMs (e.g., DeepSeek-R1, Llama, and Claude) on the task of web action generation. Our results show that fine-tuning LLMs on real-world behavioral data substantially improves their ability to generate actions compared to prompt-only methods. Furthermore, incorporating synthesized reasoning traces into model training leads to additional performance gains, demonstrating the value of explicit rationale in behavior modeling. This work establishes a new benchmark for evaluating LLMs in behavior simulation and offers actionable insights into how real-world action data and reasoning augmentation can enhance the fidelity of LLM agents. </p>
<blockquote>
<p>æœ€æ–°ç ”ç©¶è¡¨æ˜ï¼Œå¤§å‹è¯­è¨€æ¨¡å‹å¯ä»¥é€šè¿‡ä»…æç¤ºçš„æ–¹æ³•æ¨¡æ‹Ÿâ€œå¯ä¿¡â€çš„äººç±»è¡Œä¸ºæ¥ä¸ºå¤§å‹è¯­è¨€æ¨¡å‹ä»£ç†æä¾›åŠ¨åŠ›ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬ä¸“æ³¨äºè¯„ä¼°å’Œæ”¹è¿›å¤§å‹è¯­è¨€æ¨¡å‹åœ¨ç½‘é¡µåŠ¨ä½œç”Ÿæˆä»»åŠ¡ä¸­çš„å®¢è§‚â€œå‡†ç¡®æ€§â€ï¼Œè€Œä¸æ˜¯ä¸»è§‚çš„â€œå¯ä¿¡åº¦â€ï¼Œæˆ‘ä»¬åˆ©ç”¨ä»åœ¨çº¿è´­ç‰©äººç±»åŠ¨ä½œæ”¶é›†çš„å¤§è§„æ¨¡ç°å®ä¸–ç•Œæ•°æ®é›†ã€‚æˆ‘ä»¬å¯¹æœ€æ–°çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆå¦‚DeepSeek-R1ã€Llamaå’ŒClaudeï¼‰åœ¨ç½‘é¡µåŠ¨ä½œç”Ÿæˆä»»åŠ¡ä¸Šè¿›è¡Œäº†é¦–æ¬¡å…¨é¢çš„å®šé‡è¯„ä¼°ã€‚ç»“æœè¡¨æ˜ï¼Œä¸ä»…æç¤ºçš„æ–¹æ³•ç›¸æ¯”ï¼Œåœ¨ç°å®ä¸–ç•Œçš„è¡Œä¸ºæ•°æ®ä¸Šå¯¹å¤§å‹è¯­è¨€æ¨¡å‹è¿›è¡Œå¾®è°ƒï¼Œå¯ä»¥æ˜¾è‘—æé«˜å…¶åœ¨åŠ¨ä½œç”Ÿæˆæ–¹é¢çš„èƒ½åŠ›ã€‚æ­¤å¤–ï¼Œåœ¨æ¨¡å‹è®­ç»ƒä¸­èå…¥åˆæˆæ¨ç†è½¨è¿¹ï¼Œä¼šå¸¦æ¥é¢å¤–çš„æ€§èƒ½æå‡ï¼Œè¯æ˜äº†æ˜¾å¼æ¨ç†åœ¨è¡Œä¸ºå»ºæ¨¡ä¸­çš„ä»·å€¼ã€‚è¿™é¡¹å·¥ä½œä¸ºè¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹åœ¨è¡Œä¸ºæ¨¡æ‹Ÿæ–¹é¢çš„è¡¨ç°å»ºç«‹äº†æ–°çš„åŸºå‡†ï¼Œå¹¶æä¾›äº†å…³äºå¦‚ä½•å€ŸåŠ©ç°å®ä¸–ç•Œè¡ŒåŠ¨æ•°æ®å’Œæ¨ç†å¢å¼ºæ¥æé«˜å¤§å‹è¯­è¨€æ¨¡å‹ä»£ç†ä¿çœŸåº¦çš„å¯æ“ä½œè§è§£ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.20749v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ€æ–°ç ”ç©¶è¡¨æ˜ï¼Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å¯ä»¥é€šè¿‡ä»…ä½¿ç”¨æç¤ºçš„æ–¹æ³•æ¨¡æ‹Ÿâ€œå¯ä¿¡â€çš„äººç±»è¡Œä¸ºæ¥ä¸ºLLMä»£ç†æä¾›åŠ¨åŠ›ã€‚æœ¬æ–‡çš„é‡ç‚¹æ˜¯è¯„ä¼°å’Œæé«˜LLMåœ¨ç½‘é¡µåŠ¨ä½œç”Ÿæˆä»»åŠ¡ä¸­çš„å®¢è§‚â€œå‡†ç¡®æ€§â€ï¼Œè€Œä¸æ˜¯ä¸»è§‚çš„â€œå¯ä¿¡åº¦â€ï¼Œå¹¶ä½¿ç”¨äº†ä»åœ¨çº¿è´­ç‰©äººç±»åŠ¨ä½œæ”¶é›†çš„å¤§è§„æ¨¡ç°å®ä¸–ç•Œæ•°æ®é›†ã€‚æˆ‘ä»¬å¯¹æœ€å…ˆè¿›çš„LLMsï¼ˆå¦‚DeepSeek-R1ã€Llamaå’ŒClaudeï¼‰åœ¨ç½‘é¡µåŠ¨ä½œç”Ÿæˆä»»åŠ¡ä¸Šè¿›è¡Œäº†é¦–æ¬¡å…¨é¢çš„å®šé‡è¯„ä¼°ã€‚ç»“æœè¡¨æ˜ï¼Œä¸ä»…ä½¿ç”¨æç¤ºçš„æ–¹æ³•ç›¸æ¯”ï¼Œåœ¨ç°å®ä¸–ç•Œè¡Œä¸ºæ•°æ®ä¸Šå¾®è°ƒLLMså¯ä»¥æ˜¾è‘—æé«˜å…¶ç”ŸæˆåŠ¨ä½œçš„èƒ½åŠ›ã€‚æ­¤å¤–ï¼Œå°†åˆæˆæ¨ç†è½¨è¿¹çº³å…¥æ¨¡å‹è®­ç»ƒä¼šå¸¦æ¥é¢å¤–çš„æ€§èƒ½æå‡ï¼Œè¿™è¯æ˜äº†æ˜¾å¼æ¨ç†åœ¨è¡Œä¸ºå»ºæ¨¡ä¸­çš„ä»·å€¼ã€‚æœ¬æ–‡ä¸ºè¯„ä¼°LLMsåœ¨è¡Œä¸ºæ¨¡æ‹Ÿæ–¹é¢çš„è¡¨ç°å»ºç«‹äº†æ–°çš„åŸºå‡†ï¼Œå¹¶æä¾›äº†å…³äºå¦‚ä½•å€ŸåŠ©ç°å®ä¸–ç•Œè¡ŒåŠ¨æ•°æ®å’Œæ¨ç†å¢å¼ºæ¥æé«˜LLMä»£ç†ä¿çœŸåº¦çš„å¯è¡Œè§è§£ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLMså¯ä»¥é€šè¿‡ä»…ä½¿ç”¨æç¤ºçš„æ–¹æ³•æ¨¡æ‹Ÿâ€œå¯ä¿¡â€çš„äººç±»è¡Œä¸ºã€‚</li>
<li>æœ¬æ–‡å…³æ³¨LLMsåœ¨ç½‘é¡µåŠ¨ä½œç”Ÿæˆä»»åŠ¡ä¸­çš„â€œå‡†ç¡®æ€§â€è¯„ä¼°ã€‚</li>
<li>ä½¿ç”¨å¤§è§„æ¨¡ã€ç°å®ä¸–ç•Œçš„åœ¨çº¿è´­ç‰©äººç±»åŠ¨ä½œæ•°æ®é›†è¿›è¡Œè¯„ä¼°ã€‚</li>
<li>å¾®è°ƒLLMsåœ¨ç°å®ä¸–ç•Œè¡Œä¸ºæ•°æ®ä¸Šèƒ½æ˜¾è‘—æé«˜å…¶ç”ŸæˆåŠ¨ä½œçš„èƒ½åŠ›ã€‚</li>
<li>å°†åˆæˆæ¨ç†è½¨è¿¹çº³å…¥LLMè®­ç»ƒèƒ½å¸¦æ¥é¢å¤–çš„æ€§èƒ½æå‡ã€‚</li>
<li>æ˜¾å¼æ¨ç†åœ¨è¡Œä¸ºå»ºæ¨¡ä¸­å…·æœ‰ä»·å€¼ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.20749">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-c3fd888e2f0d6ccf2cce77f9a113e84c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-bf6c889e656768c4567e2b647c9b51b2.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Dynamic-Motion-Blending-for-Versatile-Motion-Editing"><a href="#Dynamic-Motion-Blending-for-Versatile-Motion-Editing" class="headerlink" title="Dynamic Motion Blending for Versatile Motion Editing"></a>Dynamic Motion Blending for Versatile Motion Editing</h2><p><strong>Authors:Nan Jiang, Hongjie Li, Ziye Yuan, Zimo He, Yixin Chen, Tengyu Liu, Yixin Zhu, Siyuan Huang</strong></p>
<p>Text-guided motion editing enables high-level semantic control and iterative modifications beyond traditional keyframe animation. Existing methods rely on limited pre-collected training triplets, which severely hinders their versatility in diverse editing scenarios. We introduce MotionCutMix, an online data augmentation technique that dynamically generates training triplets by blending body part motions based on input text. While MotionCutMix effectively expands the training distribution, the compositional nature introduces increased randomness and potential body part incoordination. To model such a rich distribution, we present MotionReFit, an auto-regressive diffusion model with a motion coordinator. The auto-regressive architecture facilitates learning by decomposing long sequences, while the motion coordinator mitigates the artifacts of motion composition. Our method handles both spatial and temporal motion edits directly from high-level human instructions, without relying on additional specifications or Large Language Models. Through extensive experiments, we show that MotionReFit achieves state-of-the-art performance in text-guided motion editing. </p>
<blockquote>
<p>æ–‡æœ¬å¼•å¯¼çš„è¿åŠ¨ç¼–è¾‘å®ç°äº†é«˜çº§è¯­ä¹‰æ§åˆ¶å’Œä¼ ç»Ÿå…³é”®å¸§åŠ¨ç”»ä¹‹å¤–çš„è¿­ä»£ä¿®æ”¹ã€‚ç°æœ‰æ–¹æ³•ä¾èµ–äºæœ‰é™é¢„æ”¶é›†çš„è®­ç»ƒä¸‰å…ƒç»„ï¼Œè¿™ä¸¥é‡é™åˆ¶äº†å®ƒä»¬åœ¨å¤šæ ·åŒ–ç¼–è¾‘åœºæ™¯ä¸­çš„é€šç”¨æ€§ã€‚æˆ‘ä»¬å¼•å…¥äº†MotionCutMixï¼Œè¿™æ˜¯ä¸€ç§åœ¨çº¿æ•°æ®å¢å¼ºæŠ€æœ¯ï¼Œé€šè¿‡åŸºäºè¾“å…¥æ–‡æœ¬èåˆèº«ä½“éƒ¨ä½çš„è¿åŠ¨æ¥åŠ¨æ€ç”Ÿæˆè®­ç»ƒä¸‰å…ƒç»„ã€‚è™½ç„¶MotionCutMixæœ‰æ•ˆåœ°æ‰©å±•äº†è®­ç»ƒåˆ†å¸ƒï¼Œä½†å…¶ç»„åˆæ€§è´¨å¼•å…¥äº†å¢åŠ çš„éšæœºæ€§å’Œæ½œåœ¨çš„èº«ä½“éƒ¨ä½ä¸åè°ƒã€‚ä¸ºäº†å¯¹è¿™ç§ä¸°å¯Œçš„åˆ†å¸ƒè¿›è¡Œå»ºæ¨¡ï¼Œæˆ‘ä»¬æå‡ºäº†MotionReFitï¼Œè¿™æ˜¯ä¸€ç§å¸¦æœ‰è¿åŠ¨åè°ƒå™¨çš„è‡ªå›å½’æ‰©æ•£æ¨¡å‹ã€‚è‡ªå›å½’æ¶æ„é€šè¿‡åˆ†è§£é•¿åºåˆ—æ¥ä¿ƒè¿›å­¦ä¹ ï¼Œè€Œè¿åŠ¨åè°ƒå™¨å‡è½»äº†è¿åŠ¨ç»„åˆçš„ä¼ªå½±ã€‚æˆ‘ä»¬çš„æ–¹æ³•å¯ä»¥ç›´æ¥ä»é«˜çº§äººç±»æŒ‡ä»¤è¿›è¡Œç©ºé—´å’Œä¸´æ—¶è¿åŠ¨ç¼–è¾‘ï¼Œæ— éœ€ä¾èµ–å…¶ä»–ç‰¹å®šè§„æ ¼æˆ–å¤§å‹è¯­è¨€æ¨¡å‹ã€‚é€šè¿‡å¤§é‡å®éªŒï¼Œæˆ‘ä»¬è¯æ˜äº†MotionReFitåœ¨æ–‡æœ¬å¼•å¯¼çš„è¿åŠ¨ç¼–è¾‘æ–¹é¢è¾¾åˆ°äº†æœ€æ–°æŠ€æœ¯æ°´å¹³ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.20724v1">PDF</a> </p>
<p><strong>Summary</strong><br>æ–‡æœ¬å¼•å¯¼çš„è¿åŠ¨ç¼–è¾‘å¯å®ç°é«˜çº§è¯­ä¹‰æ§åˆ¶å’Œè¿­ä»£ä¿®æ”¹ï¼Œè¶…è¶Šä¼ ç»Ÿå…³é”®å¸§åŠ¨ç”»ã€‚MotionCutMixåœ¨çº¿æ•°æ®å¢å¼ºæŠ€æœ¯é€šè¿‡æ··åˆèº«ä½“éƒ¨ä½çš„è¿åŠ¨æ¥åŠ¨æ€ç”Ÿæˆè®­ç»ƒæ ·æœ¬ï¼Œæé«˜äº†æ–‡æœ¬æŒ‡å¯¼ä¸‹çš„è¿åŠ¨ç¼–è¾‘èƒ½åŠ›ã€‚ç„¶è€Œï¼Œè¿™ç§æ–¹æ³•çš„ç»„åˆæ€§è´¨å¼•å…¥äº†å¢åŠ çš„éšæœºæ€§å’Œæ½œåœ¨çš„èº«ä½“éƒ¨ä½ä¸åè°ƒé—®é¢˜ã€‚ä¸ºäº†æ¨¡æ‹Ÿè¿™ç§ä¸°å¯Œçš„åˆ†å¸ƒï¼Œæˆ‘ä»¬æå‡ºäº†MotionReFitï¼Œä¸€ç§å…·æœ‰è¿åŠ¨åè°ƒå™¨çš„è‡ªå›å½’æ‰©æ•£æ¨¡å‹ã€‚è¯¥æ¨¡å‹é€šè¿‡åˆ†è§£é•¿åºåˆ—è¿›è¡Œè‡ªå›å½’å­¦ä¹ ï¼Œå¹¶é€šè¿‡è¿åŠ¨åè°ƒå™¨å‡è½»è¿åŠ¨ç»„åˆçš„ä¼ªå½±ã€‚æˆ‘ä»¬çš„æ–¹æ³•å¯ä»¥ç›´æ¥ä»é«˜çº§äººç±»æŒ‡ä»¤è¿›è¡Œç©ºé—´å’Œæ—¶é—´è¿åŠ¨ç¼–è¾‘ï¼Œæ— éœ€ä¾èµ–é¢å¤–çš„è§„èŒƒæˆ–å¤§è¯­è¨€æ¨¡å‹ã€‚å®éªŒè¡¨æ˜ï¼ŒMotionReFitåœ¨æ–‡æœ¬å¼•å¯¼çš„è¿åŠ¨ç¼–è¾‘æ–¹é¢è¾¾åˆ°äº†æœ€ä½³æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è¿åŠ¨ç¼–è¾‘è¶…è¶Šä¼ ç»Ÿå…³é”®å¸§åŠ¨ç”»ï¼Œå®ç°é«˜çº§è¯­ä¹‰æ§åˆ¶å’Œè¿­ä»£ä¿®æ”¹ã€‚</li>
<li>MotionCutMixåœ¨çº¿æ•°æ®å¢å¼ºæŠ€æœ¯é€šè¿‡æ··åˆèº«ä½“éƒ¨ä½è¿åŠ¨åŠ¨æ€ç”Ÿæˆè®­ç»ƒæ ·æœ¬ã€‚</li>
<li>ç»„åˆæ€§è´¨å¸¦æ¥éšæœºæ€§å’Œæ½œåœ¨çš„èº«ä½“éƒ¨ä½ä¸åè°ƒé—®é¢˜ã€‚</li>
<li>MotionReFitæ¨¡å‹æ˜¯ä¸€ä¸ªè‡ªå›å½’æ‰©æ•£æ¨¡å‹ï¼Œå…·æœ‰è¿åŠ¨åè°ƒå™¨æ¥è§£å†³è¿™ä¸€é—®é¢˜ã€‚</li>
<li>MotionReFitèƒ½å¤Ÿç›´æ¥ä»é«˜çº§äººç±»æŒ‡ä»¤è¿›è¡Œç©ºé—´å’Œæ—¶é—´è¿åŠ¨ç¼–è¾‘ã€‚</li>
<li>è¯¥æ–¹æ³•æ— éœ€ä¾èµ–é¢å¤–çš„è§„èŒƒæˆ–å¤§è¯­è¨€æ¨¡å‹ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.20724">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-0f782e4e55a5d3beb1359636ad47b8e1.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-9ebbce1c886d49a8eafc64dfa6bb96f3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d8b18eb326c7bdf9d15fc29b46ba585a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-82e38a8b6ddc59684d5472cd32653327.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-d7677cee95daf620aee3f23cd71f6a7e.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Mitigating-Low-Level-Visual-Hallucinations-Requires-Self-Awareness-Database-Model-and-Training-Strategy"><a href="#Mitigating-Low-Level-Visual-Hallucinations-Requires-Self-Awareness-Database-Model-and-Training-Strategy" class="headerlink" title="Mitigating Low-Level Visual Hallucinations Requires Self-Awareness:   Database, Model and Training Strategy"></a>Mitigating Low-Level Visual Hallucinations Requires Self-Awareness:   Database, Model and Training Strategy</h2><p><strong>Authors:Yinan Sun, Xiongkuo Min, Zicheng Zhang, Yixuan Gao, Yuqin Cao, Guangtao Zhai</strong></p>
<p>The rapid development of multimodal large language models has resulted in remarkable advancements in visual perception and understanding, consolidating several tasks into a single visual question-answering framework. However, these models are prone to hallucinations, which limit their reliability as artificial intelligence systems. While this issue is extensively researched in natural language processing and image captioning, there remains a lack of investigation of hallucinations in Low-level Visual Perception and Understanding (HLPU), especially in the context of image quality assessment tasks. We consider that these hallucinations arise from an absence of clear self-awareness within the models. To address this issue, we first introduce the HLPU instruction database, the first instruction database specifically focused on hallucinations in low-level vision tasks. This database contains approximately 200K question-answer pairs and comprises four subsets, each covering different types of instructions. Subsequently, we propose the Self-Awareness Failure Elimination (SAFEQA) model, which utilizes image features, salient region features and quality features to improve the perception and comprehension abilities of the model in low-level vision tasks. Furthermore, we propose the Enhancing Self-Awareness Preference Optimization (ESA-PO) framework to increase the modelâ€™s awareness of knowledge boundaries, thereby mitigating the incidence of hallucination. Finally, we conduct comprehensive experiments on low-level vision tasks, with the results demonstrating that our proposed method significantly enhances self-awareness of the model in these tasks and reduces hallucinations. Notably, our proposed method improves both accuracy and self-awareness of the proposed model and outperforms close-source models in terms of various evaluation metrics. </p>
<blockquote>
<p>å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹çš„å¿«é€Ÿå‘å±•åœ¨è§†è§‰æ„ŸçŸ¥å’Œç†è§£æ–¹é¢å–å¾—äº†æ˜¾è‘—çš„è¿›æ­¥ï¼Œå®ƒå°†å¤šä¸ªä»»åŠ¡æ•´åˆåˆ°ä¸€ä¸ªå•ä¸€çš„è§†è§‰é—®ç­”æ¡†æ¶ä¸­ã€‚ç„¶è€Œï¼Œè¿™äº›æ¨¡å‹å®¹æ˜“äº§ç”Ÿå¹»è§‰ï¼Œé™åˆ¶äº†å®ƒä»¬ä½œä¸ºäººå·¥æ™ºèƒ½ç³»ç»Ÿçš„å¯é æ€§ã€‚è™½ç„¶è¿™ä¸ªé—®é¢˜åœ¨è‡ªç„¶è¯­è¨€å¤„ç†å’Œå›¾åƒæè¿°ä¸­å¾—åˆ°äº†å¹¿æ³›çš„ç ”ç©¶ï¼Œä½†åœ¨ä½å±‚æ¬¡è§†è§‰æ„ŸçŸ¥å’Œç†è§£ï¼ˆHLPUï¼‰ä¸­çš„å¹»è§‰ï¼Œå°¤å…¶æ˜¯åœ¨å›¾åƒè´¨é‡è¯„ä¼°ä»»åŠ¡ä¸­ï¼Œä»ç„¶ç¼ºä¹ç ”ç©¶ã€‚æˆ‘ä»¬è®¤ä¸ºè¿™äº›å¹»è§‰æ˜¯ç”±äºæ¨¡å‹ç¼ºä¹æ¸…æ™°çš„è‡ªæˆ‘æ„è¯†è€Œäº§ç”Ÿçš„ã€‚</p>
</blockquote>
<p>ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬é¦–å…ˆå¼•å…¥äº†HLPUæŒ‡ä»¤æ•°æ®åº“ï¼Œè¿™æ˜¯ç¬¬ä¸€ä¸ªä¸“æ³¨äºä½å±‚æ¬¡è§†è§‰ä»»åŠ¡ä¸­å¹»è§‰çš„æŒ‡ä»¤æ•°æ®åº“ã€‚è¯¥æ•°æ®åº“åŒ…å«å¤§çº¦20ä¸‡ä¸ªé—®ç­”å¯¹ï¼Œåˆ†ä¸ºå››ä¸ªå­é›†ï¼Œæ¯ä¸ªå­é›†æ¶µç›–ä¸åŒç±»å‹çš„æŒ‡ä»¤ã€‚éšåï¼Œæˆ‘ä»¬æå‡ºäº†è‡ªæˆ‘æ„è¯†å¤±è´¥æ¶ˆé™¤ï¼ˆSAFEQAï¼‰æ¨¡å‹ï¼Œè¯¥æ¨¡å‹åˆ©ç”¨å›¾åƒç‰¹å¾ã€æ˜¾è‘—åŒºåŸŸç‰¹å¾å’Œè´¨é‡ç‰¹å¾ï¼Œæé«˜æ¨¡å‹åœ¨ä½å±‚æ¬¡è§†è§‰ä»»åŠ¡ä¸­çš„æ„ŸçŸ¥å’Œç†è§£èƒ½åŠ›ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜æå‡ºäº†å¢å¼ºè‡ªæˆ‘æ„è¯†åå¥½ä¼˜åŒ–ï¼ˆESA-POï¼‰æ¡†æ¶ï¼Œä»¥æé«˜æ¨¡å‹å¯¹çŸ¥è¯†è¾¹ç•Œçš„æ„è¯†ï¼Œä»è€Œå‡è½»å¹»è§‰çš„å‘ç”Ÿã€‚æœ€åï¼Œæˆ‘ä»¬å¯¹ä½å±‚æ¬¡è§†è§‰ä»»åŠ¡è¿›è¡Œäº†å…¨é¢çš„å®éªŒï¼Œç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬æå‡ºçš„æ–¹æ³•æ˜¾è‘—æé«˜äº†æ¨¡å‹åœ¨è¿™äº›ä»»åŠ¡ä¸­çš„è‡ªæˆ‘æ„è¯†ï¼Œå‡å°‘äº†å¹»è§‰ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œæˆ‘ä»¬æå‡ºçš„æ–¹æ³•æé«˜äº†æ¨¡å‹çš„å‡†ç¡®æ€§å’Œè‡ªæˆ‘æ„è¯†ï¼Œå¹¶åœ¨å„ç§è¯„ä¼°æŒ‡æ ‡ä¸Šä¼˜äºå°é—­æºæ¨¡å‹ã€‚</p>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.20673v1">PDF</a> </p>
<p><strong>Summary</strong><br>     å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹çš„å¿«é€Ÿå‘å±•ä¸ºè§†è§‰æ„ŸçŸ¥å’Œç†è§£å¸¦æ¥äº†æ˜¾è‘—è¿›æ­¥ï¼Œæ¨åŠ¨äº†å¤šé¡¹ä»»åŠ¡æ•´åˆè‡³å•ä¸€è§†è§‰é—®ç­”æ¡†æ¶ã€‚ç„¶è€Œï¼Œè¿™äº›æ¨¡å‹å®¹æ˜“å‡ºç°å¹»è§‰ï¼Œé™åˆ¶äº†å…¶ä½œä¸ºäººå·¥æ™ºèƒ½ç³»ç»Ÿçš„å¯é æ€§ã€‚é’ˆå¯¹ä½çº§åˆ«è§†è§‰æ„ŸçŸ¥å’Œç†è§£ï¼ˆHLPUï¼‰ä¸­çš„å¹»è§‰é—®é¢˜ï¼Œå¼•å…¥HLPUæŒ‡ä»¤æ•°æ®åº“å’ŒSAFEQAæ¨¡å‹æé«˜æ¨¡å‹æ„ŸçŸ¥å’Œè®¤çŸ¥ï¼Œæå‡ºESA-POæ¡†æ¶æå‡æ¨¡å‹çŸ¥è¯†è¾¹ç•Œæ„è¯†ï¼Œå‡å°‘å¹»è§‰å‘ç”Ÿã€‚å®éªŒè¯æ˜ï¼Œè¯¥æ–¹æ³•æ˜¾è‘—æé«˜æ¨¡å‹è‡ªæˆ‘æ„è¯†å’Œå‡†ç¡®æ€§ï¼Œä¼˜äºå…¶ä»–æ¨¡å‹ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹çš„è¿›æ­¥æ¨åŠ¨äº†è§†è§‰é—®ç­”æ¡†æ¶çš„å‘å±•ã€‚</li>
<li>è¿™äº›æ¨¡å‹å­˜åœ¨å¹»è§‰é—®é¢˜ï¼Œå½±å“å¯é æ€§ã€‚</li>
<li>HLPUæŒ‡ä»¤æ•°æ®åº“é¦–æ¬¡é’ˆå¯¹ä½çº§åˆ«è§†è§‰ä»»åŠ¡ä¸­çš„å¹»è§‰é—®é¢˜è®¾ç«‹ã€‚</li>
<li>SAFEQAæ¨¡å‹åˆ©ç”¨å›¾åƒç‰¹å¾ã€æ˜¾è‘—åŒºåŸŸç‰¹å¾å’Œè´¨é‡ç‰¹å¾æ”¹å–„æ¨¡å‹åœ¨ä½çº§åˆ«è§†è§‰ä»»åŠ¡ä¸­çš„æ„ŸçŸ¥å’Œç†è§£èƒ½åŠ›ã€‚</li>
<li>ESA-POæ¡†æ¶æå‡æ¨¡å‹çš„çŸ¥è¯†è¾¹ç•Œæ„è¯†ï¼Œå‡å°‘å¹»è§‰ã€‚</li>
<li>å®éªŒè¯æ˜ï¼Œè¯¥æ–¹æ³•æ˜¾è‘—æé«˜æ¨¡å‹çš„è‡ªæˆ‘æ„è¯†å’Œå‡†ç¡®æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.20673">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-8913da0498616c8027d9eef1def2b4f9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d79b2c586ba148ffe555f333a0be81e8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d7eefcac8944973d1097b2e7a3e9d701.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-1980b38d1d80c5f16507bf65f4693fd4.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="TAMA-A-Human-AI-Collaborative-Thematic-Analysis-Framework-Using-Multi-Agent-LLMs-for-Clinical-Interviews"><a href="#TAMA-A-Human-AI-Collaborative-Thematic-Analysis-Framework-Using-Multi-Agent-LLMs-for-Clinical-Interviews" class="headerlink" title="TAMA: A Human-AI Collaborative Thematic Analysis Framework Using   Multi-Agent LLMs for Clinical Interviews"></a>TAMA: A Human-AI Collaborative Thematic Analysis Framework Using   Multi-Agent LLMs for Clinical Interviews</h2><p><strong>Authors:Huimin Xu, Seungjun Yi, Terence Lim, Jiawei Xu, Andrew Well, Carlos Mery, Aidong Zhang, Yuji Zhang, Heng Ji, Keshav Pingali, Yan Leng, Ying Ding</strong></p>
<p>Thematic analysis (TA) is a widely used qualitative approach for uncovering latent meanings in unstructured text data. TA provides valuable insights in healthcare but is resource-intensive. Large Language Models (LLMs) have been introduced to perform TA, yet their applications in healthcare remain unexplored. Here, we propose TAMA: A Human-AI Collaborative Thematic Analysis framework using Multi-Agent LLMs for clinical interviews. We leverage the scalability and coherence of multi-agent systems through structured conversations between agents and coordinate the expertise of cardiac experts in TA. Using interview transcripts from parents of children with Anomalous Aortic Origin of a Coronary Artery (AAOCA), a rare congenital heart disease, we demonstrate that TAMA outperforms existing LLM-assisted TA approaches, achieving higher thematic hit rate, coverage, and distinctiveness. TAMA demonstrates strong potential for automated TA in clinical settings by leveraging multi-agent LLM systems with human-in-the-loop integration by enhancing quality while significantly reducing manual workload. </p>
<blockquote>
<p>ä¸»é¢˜åˆ†æï¼ˆTAï¼‰æ˜¯ä¸€ç§å¹¿æ³›åº”ç”¨äºæ­ç¤ºæ— ç»“æ„æ–‡æœ¬æ•°æ®ä¸­æ½œåœ¨å«ä¹‰çš„å®šæ€§æ–¹æ³•ã€‚TAåœ¨åŒ»ç–—é¢†åŸŸæä¾›äº†å®è´µçš„è§è§£ï¼Œä½†èµ„æºæ¶ˆè€—å¤§ã€‚å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å·²è¢«å¼•å…¥ç”¨äºTAï¼Œä½†å®ƒä»¬åœ¨åŒ»ç–—é¢†åŸŸçš„åº”ç”¨ä»æœªè¢«æ¢ç´¢ã€‚åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬æå‡ºäº†ä½¿ç”¨å¤šæ™ºèƒ½ä½“LLMè¿›è¡Œä¸´åºŠè®¿è°ˆçš„TAMAï¼šä¸€ç§äººæœºååŒä¸»é¢˜åˆ†ææ¡†æ¶ã€‚æˆ‘ä»¬åˆ©ç”¨å¤šæ™ºèƒ½ä½“ç³»ç»Ÿé€šè¿‡æ™ºèƒ½ä½“ä¹‹é—´çš„ç»“æ„åŒ–å¯¹è¯çš„å¯æ‰©å±•æ€§å’Œè¿è´¯æ€§ï¼Œå¹¶åè°ƒå¿ƒè„ä¸“å®¶åœ¨TAæ–¹é¢çš„ä¸“ä¸šçŸ¥è¯†ã€‚æˆ‘ä»¬ä½¿ç”¨æ‚£æœ‰ç½•è§å…ˆå¤©æ€§å¿ƒè„ç–¾ç—…å¼‚å¸¸å† çŠ¶åŠ¨è„‰èµ·æºï¼ˆAAOCAï¼‰çš„å„¿ç«¥çš„çˆ¶æ¯è®¿è°ˆè®°å½•ï¼Œè¯æ˜TAMAä¼˜äºç°æœ‰çš„LLMè¾…åŠ©TAæ–¹æ³•ï¼Œå®ç°äº†æ›´é«˜çš„ä¸»é¢˜å‘½ä¸­ç‡å’Œè¦†ç›–ç‡ä»¥åŠç‹¬ç‰¹æ€§ã€‚TAMAé€šè¿‡åˆ©ç”¨å¤šæ™ºèƒ½ä½“LLMç³»ç»Ÿå¹¶èå…¥äººå·¥å¹²é¢„ï¼Œå±•ç¤ºäº†åœ¨ä¸´åºŠç¯å¢ƒä¸­è‡ªåŠ¨åŒ–è¿›è¡ŒTAçš„å¼ºå¤§æ½œåŠ›ï¼Œæ—¢æé«˜äº†è´¨é‡åˆæ˜¾è‘—å‡å°‘äº†æ‰‹åŠ¨å·¥ä½œé‡ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.20666v1">PDF</a> Submitted to the American Medical Informatics Association (AMIA) 2025   Annual Symposium, 10 pages</p>
<p><strong>æ‘˜è¦</strong></p>
<p>ä¸»é¢˜åˆ†æï¼ˆTAï¼‰æ˜¯ä¸€ç§å¹¿æ³›ç”¨äºå‘ç°éç»“æ„åŒ–æ–‡æœ¬æ•°æ®ä¸­æ½œåœ¨å«ä¹‰çš„å®šæ€§æ–¹æ³•ã€‚TAåœ¨åŒ»ç–—é¢†åŸŸæä¾›æœ‰ä»·å€¼çš„è§è§£ï¼Œä½†èµ„æºå¯†é›†ã€‚å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å·²è¢«ç”¨äºæ‰§è¡ŒTAï¼Œä½†å…¶åœ¨åŒ»ç–—é¢†åŸŸçš„åº”ç”¨ä»å¾…æ¢ç´¢ã€‚æœ¬æ–‡æå‡ºTAMAï¼šä¸€ç§åˆ©ç”¨å¤šæ™ºèƒ½ä½“LLMè¿›è¡Œä¸´åºŠè®¿è°ˆçš„äººæœºååŒä¸»é¢˜åˆ†ææ¡†æ¶ã€‚æˆ‘ä»¬åˆ©ç”¨å¤šæ™ºèƒ½ç³»ç»Ÿçš„å¯æ‰©å±•æ€§å’Œè¿è´¯æ€§ï¼Œé€šè¿‡æ™ºèƒ½ä½“ä¹‹é—´çš„ç»“æ„åŒ–å¯¹è¯è¿›è¡Œåè°ƒï¼Œå¹¶æ•´åˆå¿ƒè„ä¸“å®¶çš„ä¸“ä¸šçŸ¥è¯†è¿›è¡ŒTAã€‚æˆ‘ä»¬ä½¿ç”¨æ‚£æœ‰å¼‚å¸¸å† çŠ¶åŠ¨è„‰èµ·æºï¼ˆAAOCAï¼‰çš„å„¿ç«¥çš„çˆ¶æ¯è®¿è°ˆè®°å½•æ¥æ¼”ç¤ºï¼ŒTAMAåœ¨ç°æœ‰LLMè¾…åŠ©çš„TAæ–¹æ³•ä¸­å…·æœ‰ä¼˜åŠ¿ï¼Œåœ¨ä¸»é¢˜å‘½ä¸­ç‡ã€è¦†ç›–ç‡å’Œç‹¬ç‰¹æ€§æ–¹é¢è¡¨ç°æ›´é«˜ã€‚TAMAå±•ç¤ºäº†é€šè¿‡åˆ©ç”¨å¤šæ™ºèƒ½ä½“LLMç³»ç»Ÿå¹¶ç»“åˆäººå·¥å¹²é¢„ï¼Œåœ¨é™ä½æ‰‹åŠ¨å·¥ä½œé‡çš„åŒæ—¶æé«˜è´¨é‡çš„æƒ…å†µä¸‹ï¼Œåœ¨åŒ»ç–—ç¯å¢ƒä¸­è‡ªåŠ¨åŒ–TAçš„å¼ºå¤§æ½œåŠ›ã€‚</p>
<p><strong>è¦ç‚¹æç‚¼</strong></p>
<ol>
<li>ä¸»é¢˜åˆ†æï¼ˆTAï¼‰æ˜¯å‘ç°éç»“æ„åŒ–æ–‡æœ¬æ•°æ®ä¸­æ½œåœ¨å«ä¹‰çš„ä¸€ç§å®šæ€§æ–¹æ³•ï¼Œå¯¹åŒ»ç–—é¢†åŸŸå…·æœ‰åº”ç”¨ä»·å€¼ã€‚</li>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å·²åº”ç”¨äºä¸»é¢˜åˆ†æï¼Œä½†åœ¨åŒ»ç–—é¢†åŸŸçš„åº”ç”¨å°šæœªå¾—åˆ°å……åˆ†æ¢ç´¢ã€‚</li>
<li>æœ¬æ–‡æå‡ºäº†TAMAæ¡†æ¶ï¼Œè¿™æ˜¯ä¸€ç§äººæœºååŒä¸»é¢˜åˆ†ææ¡†æ¶ï¼Œåˆ©ç”¨å¤šæ™ºèƒ½ä½“LLMè¿›è¡Œä¸´åºŠè®¿è°ˆåˆ†æã€‚</li>
<li>TAMAé€šè¿‡åˆ©ç”¨å¤šæ™ºèƒ½ç³»ç»Ÿçš„ä¼˜åŠ¿ï¼Œç»“åˆå¿ƒè„ä¸“å®¶çš„ä¸“ä¸šçŸ¥è¯†è¿›è¡Œä¸»é¢˜åˆ†æã€‚</li>
<li>é€šè¿‡æ‚£æœ‰å¼‚å¸¸å† çŠ¶åŠ¨è„‰èµ·æºçš„å„¿ç«¥çš„çˆ¶æ¯è®¿è°ˆè®°å½•è¿›è¡Œæ¼”ç¤ºï¼ŒTAMAåœ¨ä¸»é¢˜å‘½ä¸­ç‡ã€è¦†ç›–ç‡å’Œç‹¬ç‰¹æ€§æ–¹é¢è¡¨ç°å‡ºæ›´é«˜çš„æ€§èƒ½ã€‚</li>
<li>TAMAå±•ç¤ºäº†åœ¨åŒ»ç–—ç¯å¢ƒä¸­è‡ªåŠ¨åŒ–ä¸»é¢˜åˆ†æçš„å·¨å¤§æ½œåŠ›ï¼Œé€šè¿‡ç»“åˆäººå·¥å¹²é¢„å’Œå¤šæ™ºèƒ½ä½“LLMç³»ç»Ÿï¼Œå¯ä»¥åœ¨æé«˜è´¨é‡çš„åŒæ—¶æ˜¾è‘—é™ä½æ‰‹åŠ¨å·¥ä½œé‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.20666">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-0814ff304d27c6f1078e672d98ff3a77.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2cd7db83cef869a9e9dbb7a653df0175.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-55ce015910d46b535989b93ea600450d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-aec749ce90f742210189342b1b61952f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-45df6e64deb1d85b3f5759c3d8ceee22.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Unlocking-Efficient-Long-to-Short-LLM-Reasoning-with-Model-Merging"><a href="#Unlocking-Efficient-Long-to-Short-LLM-Reasoning-with-Model-Merging" class="headerlink" title="Unlocking Efficient Long-to-Short LLM Reasoning with Model Merging"></a>Unlocking Efficient Long-to-Short LLM Reasoning with Model Merging</h2><p><strong>Authors:Han Wu, Yuxuan Yao, Shuqi Liu, Zehua Liu, Xiaojin Fu, Xiongwei Han, Xing Li, Hui-Ling Zhen, Tao Zhong, Mingxuan Yuan</strong></p>
<p>The transition from System 1 to System 2 reasoning in large language models (LLMs) has marked significant advancements in handling complex tasks through deliberate, iterative thinking. However, this progress often comes at the cost of efficiency, as models tend to overthink, generating redundant reasoning steps without proportional improvements in output quality. Long-to-Short (L2S) reasoning has emerged as a promising solution to this challenge, aiming to balance reasoning depth with practical efficiency. While existing approaches, such as supervised fine-tuning (SFT), reinforcement learning (RL), and prompt engineering, have shown potential, they are either computationally expensive or unstable. Model merging, on the other hand, offers a cost-effective and robust alternative by integrating the quick-thinking capabilities of System 1 models with the methodical reasoning of System 2 models. In this work, we present a comprehensive empirical study on model merging for L2S reasoning, exploring diverse methodologies, including task-vector-based, SVD-based, and activation-informed merging. Our experiments reveal that model merging can reduce average response length by up to 55% while preserving or even improving baseline performance. We also identify a strong correlation between model scale and merging efficacy with extensive evaluations on 1.5B&#x2F;7B&#x2F;14B&#x2F;32B models. Furthermore, we investigate the merged modelâ€™s ability to self-critique and self-correct, as well as its adaptive response length based on task complexity. Our findings highlight model merging as a highly efficient and effective paradigm for L2S reasoning, offering a practical solution to the overthinking problem while maintaining the robustness of System 2 reasoning. This work can be found on Github <a target="_blank" rel="noopener" href="https://github.com/hahahawu/Long-to-Short-via-Model-Merging">https://github.com/hahahawu/Long-to-Short-via-Model-Merging</a>. </p>
<blockquote>
<p>ä»System 1åˆ°System 2æ¨ç†çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„è½¬å˜æ ‡å¿—ç€é€šè¿‡æ·±æ€ç†Ÿè™‘ã€åå¤æ€è€ƒæ¥å¤„ç†å¤æ‚ä»»åŠ¡çš„èƒ½åŠ›æœ‰äº†é‡å¤§è¿›æ­¥ã€‚ç„¶è€Œï¼Œè¿™ç§è¿›å±•å¾€å¾€ä»¥æ•ˆç‡ä¸ºä»£ä»·ï¼Œå› ä¸ºæ¨¡å‹å¾€å¾€ä¼šè¿‡åº¦æ€è€ƒï¼Œäº§ç”Ÿå†—ä½™çš„æ¨ç†æ­¥éª¤ï¼Œè€Œæ²¡æœ‰åœ¨è¾“å‡ºè´¨é‡ä¸Šå¾—åˆ°ç›¸åº”çš„æ”¹è¿›ã€‚é•¿çŸ­æ¨ç†ï¼ˆL2Sï¼‰ä½œä¸ºä¸€ç§æœ‰å‰æ™¯çš„è§£å†³æ–¹æ¡ˆåº”è¿è€Œç”Ÿï¼Œæ—¨åœ¨å¹³è¡¡æ¨ç†æ·±åº¦ä¸å®ç”¨æ•ˆç‡ã€‚è™½ç„¶ç°æœ‰çš„æ–¹æ³•ï¼Œå¦‚ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰ã€å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰å’Œæç¤ºå·¥ç¨‹ï¼Œå·²ç»æ˜¾ç¤ºå‡ºæ½œåŠ›ï¼Œä½†å®ƒä»¬è¦ä¹ˆè®¡ç®—æˆæœ¬é«˜ï¼Œè¦ä¹ˆä¸ç¨³å®šã€‚å¦ä¸€æ–¹é¢ï¼Œæ¨¡å‹èåˆé€šè¿‡èåˆSystem 1æ¨¡å‹çš„å¿«é€Ÿæ€ç»´èƒ½åŠ›å’ŒSystem 2æ¨¡å‹çš„æ–¹æ³•æ€§æ¨ç†èƒ½åŠ›ï¼Œæä¾›äº†ä¸€ä¸ªç»æµå®æƒ ä¸”ç¨³å¥çš„æ›¿ä»£æ–¹æ¡ˆã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬å¯¹æ¨¡å‹èåˆè¿›è¡Œäº†å…¨é¢çš„å®è¯ç ”ç©¶ï¼Œä»¥ç”¨äºé•¿çŸ­æ¨ç†ï¼ˆL2Sï¼‰ï¼Œæ¢ç´¢äº†åŒ…æ‹¬åŸºäºä»»åŠ¡å‘é‡ã€åŸºäºSVDå’ŒåŸºäºæ¿€æ´»çš„èåˆæ–¹æ³•ç­‰å¤šæ ·åŒ–æ–¹æ³•ã€‚æˆ‘ä»¬çš„å®éªŒè¡¨æ˜ï¼Œæ¨¡å‹èåˆå¯ä»¥åœ¨ä¿æŒæˆ–ç”šè‡³æé«˜åŸºçº¿æ€§èƒ½çš„åŒæ—¶ï¼Œå°†å¹³å‡å“åº”é•¿åº¦ç¼©çŸ­é«˜è¾¾55%ã€‚æˆ‘ä»¬è¿˜é€šè¿‡åœ¨å¯¹è§„æ¨¡ä¸º1.5B&#x2F;7B&#x2F;14B&#x2F;32Bçš„æ¨¡å‹è¿›è¡Œå…¨é¢è¯„ä¼°ä¸­ï¼Œå‘ç°äº†æ¨¡å‹è§„æ¨¡å’Œèåˆæ•ˆæœä¹‹é—´çš„å¼ºçƒˆç›¸å…³æ€§ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜æ¢è®¨äº†èåˆæ¨¡å‹çš„è‡ªæˆ‘æ‰¹åˆ¤å’Œè‡ªæˆ‘çº æ­£èƒ½åŠ›ï¼Œä»¥åŠæ ¹æ®ä»»åŠ¡å¤æ‚æ€§è°ƒæ•´å“åº”é•¿åº¦çš„èƒ½åŠ›ã€‚æˆ‘ä»¬çš„ç ”ç©¶ç»“æœå¼ºè°ƒäº†æ¨¡å‹èåˆä½œä¸ºä¸€ç§é«˜æ•ˆä¸”æœ‰æ•ˆçš„é•¿çŸ­æ¨ç†èŒƒå¼ï¼Œä¸ºè§£å†³è¿‡åº¦æ€è€ƒé—®é¢˜æä¾›äº†å®ç”¨è§£å†³æ–¹æ¡ˆï¼ŒåŒæ—¶ä¿æŒäº†System 2æ¨ç†çš„ç¨³å¥æ€§ã€‚è¿™é¡¹å·¥ä½œå¯ä»¥åœ¨Githubä¸Šæ‰¾åˆ°ï¼š<a target="_blank" rel="noopener" href="https://github.com/hahahawu/Long-to-Short-via-Model-Merging%E3%80%82">https://github.com/hahahawu/Long-to-Short-via-Model-Mergingã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.20641v1">PDF</a> Work in progress; technical report</p>
<p><strong>Summary</strong></p>
<p>ç³»ç»Ÿ1åˆ°ç³»ç»Ÿ2çš„è¿‡æ¸¡æ¨ç†åœ¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¸­æ ‡å¿—ç€é€šè¿‡æ·±æ€ç†Ÿè™‘ã€è¿­ä»£æ€è€ƒæ¥å¤„ç†å¤æ‚ä»»åŠ¡çš„æ˜¾è‘—è¿›å±•ã€‚ç„¶è€Œï¼Œè¿™ç§è¿›å±•å¾€å¾€ä»¥æ•ˆç‡ä¸ºä»£ä»·ï¼Œæ¨¡å‹å€¾å‘äºè¿‡åº¦æ€è€ƒï¼Œäº§ç”Ÿå†—ä½™çš„æ¨ç†æ­¥éª¤ï¼Œè€Œè¾“å‡ºè´¨é‡çš„æ”¹è¿›å¹¶ä¸æˆæ¯”ä¾‹ã€‚é•¿çŸ­æ¨ç†ï¼ˆL2Sï¼‰æ—¨åœ¨å¹³è¡¡æ¨ç†æ·±åº¦ä¸å®ç”¨æ•ˆç‡ã€‚å°½ç®¡ç°æœ‰æ–¹æ³•å¦‚ç›‘ç£å¾®è°ƒã€å¼ºåŒ–å­¦ä¹ å’Œæç¤ºå·¥ç¨‹ç­‰æ˜¾ç¤ºå‡ºæ½œåŠ›ï¼Œä½†å®ƒä»¬è¦ä¹ˆè®¡ç®—æˆæœ¬é«˜ï¼Œè¦ä¹ˆä¸ç¨³å®šã€‚æ¨¡å‹åˆå¹¶ä½œä¸ºä¸€ç§æˆæœ¬æ•ˆç›Šé«˜ä¸”ç¨³å¥çš„æ›¿ä»£æ–¹æ¡ˆï¼Œé€šè¿‡æ•´åˆç³»ç»Ÿ1æ¨¡å‹çš„å¿«é€Ÿæ€è€ƒèƒ½åŠ›å’Œç³»ç»Ÿ2æ¨¡å‹çš„æ–¹æ³•æ€§æ¨ç†ã€‚æœ¬æ–‡å…¨é¢å®è¯ç ”ç©¶äº†æ¨¡å‹åˆå¹¶ç”¨äºL2Sæ¨ç†çš„æ–¹æ³•ï¼ŒåŒ…æ‹¬åŸºäºä»»åŠ¡å‘é‡ã€SVDå’Œæ¿€æ´»ä¿¡æ¯åˆå¹¶çš„æ–¹æ³•ã€‚å®éªŒè¡¨æ˜ï¼Œæ¨¡å‹åˆå¹¶å¯åœ¨ä¿æŒæˆ–æé«˜åŸºçº¿æ€§èƒ½çš„åŒæ—¶ï¼Œå°†å¹³å‡å“åº”é•¿åº¦å‡å°‘é«˜è¾¾55%ã€‚æˆ‘ä»¬è¿˜å‘ç°æ¨¡å‹è§„æ¨¡ä¸åˆå¹¶æ•ˆæœä¹‹é—´å­˜åœ¨å¼ºçƒˆç›¸å…³æ€§ï¼Œå¹¶åœ¨1.5B&#x2F;7B&#x2F;14B&#x2F;32Bæ¨¡å‹ä¸Šè¿›è¡Œäº†å¹¿æ³›è¯„ä¼°ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬ç ”ç©¶äº†åˆå¹¶æ¨¡å‹è‡ªæˆ‘æ‰¹åˆ¤ã€è‡ªæˆ‘ä¿®æ­£çš„èƒ½åŠ›ï¼Œä»¥åŠå…¶åŸºäºä»»åŠ¡å¤æ‚æ€§çš„è‡ªé€‚åº”å“åº”é•¿åº¦ã€‚ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œæ¨¡å‹åˆå¹¶æ˜¯ä¸€ç§é«˜æ•ˆä¸”å®ç”¨çš„L2Sæ¨ç†èŒƒå¼ï¼Œä¸ºè§£å†³è¿‡åº¦æ€è€ƒé—®é¢˜åŒæ—¶ä¿æŒç³»ç»Ÿ2æ¨ç†çš„ç¨³å¥æ€§æä¾›äº†å®é™…è§£å†³æ–¹æ¡ˆã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLMsä»System 1åˆ°System 2çš„è¿‡æ¸¡æ¨ç†åœ¨å¤„ç†å¤æ‚ä»»åŠ¡æ—¶å–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œä½†å­˜åœ¨æ•ˆç‡é—®é¢˜ã€‚</li>
<li>é•¿çŸ­æ¨ç†ï¼ˆL2Sï¼‰æ—¨åœ¨å¹³è¡¡æ¨ç†æ·±åº¦ä¸å®ç”¨æ•ˆç‡ã€‚</li>
<li>ç°æœ‰æ–¹æ³•å¦‚ç›‘ç£å¾®è°ƒã€å¼ºåŒ–å­¦ä¹ å’Œæç¤ºå·¥ç¨‹åœ¨è§£å†³L2Sæ¨ç†é—®é¢˜æ—¶å­˜åœ¨è®¡ç®—æˆæœ¬é«˜æˆ–ä¸ç¨³å®šçš„ç¼ºé™·ã€‚</li>
<li>æ¨¡å‹åˆå¹¶æ–¹æ³•æä¾›äº†æˆæœ¬æ•ˆç›Šé«˜ä¸”ç¨³å¥çš„æ›¿ä»£æ–¹æ¡ˆï¼Œç»“åˆäº†System 1å’ŒSystem 2æ¨¡å‹çš„ä¼˜ç‚¹ã€‚</li>
<li>æ¨¡å‹åˆå¹¶å®éªŒè¡¨æ˜å¯å¤§å¹…å‡å°‘å¹³å‡å“åº”é•¿åº¦ï¼ˆè¾¾55%ï¼‰ï¼ŒåŒæ—¶ä¿æŒæˆ–æé«˜åŸºçº¿æ€§èƒ½ã€‚</li>
<li>æ¨¡å‹è§„æ¨¡ä¸åˆå¹¶æ•ˆæœä¹‹é—´å­˜åœ¨å¼ºçƒˆç›¸å…³æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.20641">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-1532536a13b11fe7ecf32f9203a0126d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c25b579b3e003f6c8600f2fd497cac91.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="What-to-Retrieve-for-Effective-Retrieval-Augmented-Code-Generation-An-Empirical-Study-and-Beyond"><a href="#What-to-Retrieve-for-Effective-Retrieval-Augmented-Code-Generation-An-Empirical-Study-and-Beyond" class="headerlink" title="What to Retrieve for Effective Retrieval-Augmented Code Generation? An   Empirical Study and Beyond"></a>What to Retrieve for Effective Retrieval-Augmented Code Generation? An   Empirical Study and Beyond</h2><p><strong>Authors:Wenchao Gu, Juntao Chen, Yanlin Wang, Tianyue Jiang, Xingzhe Li, Mingwei Liu, Xilin Liu, Yuchi Ma, Zibin Zheng</strong></p>
<p>Repository-level code generation remains challenging due to complex code dependencies and the limitations of large language models (LLMs) in processing long contexts. While retrieval-augmented generation (RAG) frameworks are widely adopted, the effectiveness of different retrieved information sources-contextual code, APIs, and similar snippets-has not been rigorously analyzed. Through an empirical study on two benchmarks, we demonstrate that in-context code and potential API information significantly enhance LLM performance, whereas retrieved similar code often introduces noise, degrading results by up to 15%. Based on the preliminary results, we propose AllianceCoder, a novel context-integrated method that employs chain-of-thought prompting to decompose user queries into implementation steps and retrieves APIs via semantic description matching. Through extensive experiments on CoderEval and RepoExec, AllianceCoder achieves state-of-the-art performance, improving Pass@1 by up to 20% over existing approaches. </p>
<blockquote>
<p>ç”±äºå¤æ‚çš„ä»£ç ä¾èµ–æ€§å’Œå¤§å‹è¯­è¨€æ¨¡å‹åœ¨å¤„ç†é•¿æ–‡æœ¬æ—¶çš„å±€é™æ€§ï¼Œä»“åº“çº§åˆ«çš„ä»£ç ç”Ÿæˆä»ç„¶å…·æœ‰æŒ‘æˆ˜æ€§ã€‚è™½ç„¶æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰æ¡†æ¶å·²è¢«å¹¿æ³›åº”ç”¨ï¼Œä½†ä¸åŒæ£€ç´¢ä¿¡æ¯æºï¼ˆä¸Šä¸‹æ–‡ä»£ç ã€APIå’Œç±»ä¼¼ä»£ç ç‰‡æ®µï¼‰çš„æœ‰æ•ˆæ€§å°šæœªè¿›è¡Œä¸¥æ ¼åˆ†æã€‚é€šè¿‡å¯¹ä¸¤ä¸ªåŸºå‡†æµ‹è¯•é›†çš„å®è¯ç ”ç©¶ï¼Œæˆ‘ä»¬è¯æ˜ä¸Šä¸‹æ–‡ä»£ç å’Œæ½œåœ¨APIä¿¡æ¯å¯ä»¥æ˜¾è‘—æé«˜å¤§å‹è¯­è¨€æ¨¡å‹çš„æ€§èƒ½ï¼Œè€Œæ£€ç´¢åˆ°çš„ç±»ä¼¼ä»£ç å¾€å¾€ä¼šå¼•å…¥å™ªå£°ï¼Œå¯¼è‡´ç»“æœæœ€å¤šé™ä½15%ã€‚åŸºäºåˆæ­¥ç»“æœï¼Œæˆ‘ä»¬æå‡ºäº†AllianceCoderï¼Œè¿™æ˜¯ä¸€ç§æ–°å‹ä¸Šä¸‹æ–‡é›†æˆæ–¹æ³•ï¼Œé‡‡ç”¨é“¾å¼æ€ç»´æç¤ºå°†ç”¨æˆ·æŸ¥è¯¢åˆ†è§£ä¸ºå®ç°æ­¥éª¤ï¼Œå¹¶é€šè¿‡è¯­ä¹‰æè¿°åŒ¹é…æ£€ç´¢APIã€‚åœ¨CoderEvalå’ŒRepoExecä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼ŒAllianceCoderè¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼ŒPass@1æé«˜äº†é«˜è¾¾20%ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.20589v1">PDF</a> </p>
<p><strong>Summary</strong><br>åœ¨å¤æ‚ä»£ç ä¾èµ–çš„æŒ‘æˆ˜ä¸‹ï¼Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨å¤„ç†é•¿ä¸Šä¸‹æ–‡æ–¹é¢å­˜åœ¨å±€é™æ€§ï¼Œå½±å“ä»£ç ç”Ÿæˆçš„æ•ˆæœã€‚é€šè¿‡å®è¯ç ”ç©¶å‘ç°ï¼Œé‡‡ç”¨æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰æ¡†æ¶æ—¶ï¼Œä¸Šä¸‹æ–‡ä»£ç å’Œæ½œåœ¨APIä¿¡æ¯èƒ½æœ‰æ•ˆæå‡LLMæ€§èƒ½ï¼Œè€Œæ£€ç´¢åˆ°çš„ç›¸ä¼¼ä»£ç å¾€å¾€ä¼šå¼•å…¥å™ªå£°ï¼Œé™ä½ç»“æœæ•ˆæœã€‚åŸºäºè¿™äº›åˆæ­¥ç»“æœï¼Œæå‡ºä¸€ç§æ–°å‹ä¸Šä¸‹æ–‡é›†æˆæ–¹æ³•â€”â€”AllianceCoderï¼Œé‡‡ç”¨é“¾å¼æ€ç»´æç¤ºå°†ç”¨æˆ·æŸ¥è¯¢åˆ†è§£ä¸ºå®ç°æ­¥éª¤ï¼Œå¹¶é€šè¿‡è¯­ä¹‰æè¿°åŒ¹é…æ£€ç´¢APIã€‚åœ¨CoderEvalå’ŒRepoExecä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒAllianceCoderå®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½è¡¨ç°ï¼Œåœ¨ç°æœ‰æ–¹æ³•çš„åŸºç¡€ä¸Šå°†Pass@1æå‡äº†æœ€å¤šè¾¾ç™¾åˆ†ä¹‹äºŒåã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨å¤„ç†å¤æ‚ä»£ç ç”Ÿæˆæ—¶é¢ä¸´å¤„ç†é•¿ä¸Šä¸‹æ–‡çš„æŒ‘æˆ˜ã€‚</li>
<li>æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰æ¡†æ¶å¹¿æ³›é‡‡ç”¨ï¼Œä½†ä¸åŒæ£€ç´¢ä¿¡æ¯æºçš„æ•ˆæœå°šæœªè¿›è¡Œä¸¥è°¨åˆ†æã€‚</li>
<li>ä¸Šä¸‹æ–‡ä»£ç å’Œæ½œåœ¨APIä¿¡æ¯èƒ½æœ‰æ•ˆæå‡LLMæ€§èƒ½ã€‚</li>
<li>æ£€ç´¢åˆ°çš„ç›¸ä¼¼ä»£ç å¯èƒ½å¼•å…¥å™ªå£°ï¼Œé™ä½ç»“æœæ•ˆæœã€‚</li>
<li>åˆæ­¥ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œå¼•å…¥é“¾å¼æ€ç»´æç¤ºå’Œè¯­ä¹‰æè¿°åŒ¹é…çš„AllianceCoderæ–¹æ³•èƒ½æœ‰æ•ˆæ”¹å–„ä»£ç ç”Ÿæˆæ•ˆæœã€‚</li>
<li>AllianceCoderåœ¨CoderEvalå’ŒRepoExecä¸Šçš„å®éªŒè¡¨ç°è¾¾åˆ°æˆ–è¶…è¶Šäº†ç°æœ‰æ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.20589">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-ce24dd40d241db1187d8a8f28d302607.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2cc8e09fec340305e0b9f18d506eca0b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-63a7eff23e381b5a1ca9d995cf9081d1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0a0a84d19284c884f9da2a5cb68a3dae.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c6b7b5ed3e24dbf8ccc41f3724249912.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Optimizing-Case-Based-Reasoning-System-for-Functional-Test-Script-Generation-with-Large-Language-Models"><a href="#Optimizing-Case-Based-Reasoning-System-for-Functional-Test-Script-Generation-with-Large-Language-Models" class="headerlink" title="Optimizing Case-Based Reasoning System for Functional Test Script   Generation with Large Language Models"></a>Optimizing Case-Based Reasoning System for Functional Test Script   Generation with Large Language Models</h2><p><strong>Authors:Siyuan Guo, Huiwu Liu, Xiaolong Chen, Yuming Xie, Liang Zhang, Tao Han, Hechang Chen, Yi Chang, Jun Wang</strong></p>
<p>In this work, we explore the potential of large language models (LLMs) for generating functional test scripts, which necessitates understanding the dynamically evolving code structure of the target software. To achieve this, we propose a case-based reasoning (CBR) system utilizing a 4R cycle (i.e., retrieve, reuse, revise, and retain), which maintains and leverages a case bank of test intent descriptions and corresponding test scripts to facilitate LLMs for test script generation. To improve user experience further, we introduce Re4, an optimization method for the CBR system, comprising reranking-based retrieval finetuning and reinforced reuse finetuning. Specifically, we first identify positive examples with high semantic and script similarity, providing reliable pseudo-labels for finetuning the retriever model without costly labeling. Then, we apply supervised finetuning, followed by a reinforcement learning finetuning stage, to align LLMs with our production scenarios, ensuring the faithful reuse of retrieved cases. Extensive experimental results on two product development units from Huawei Datacom demonstrate the superiority of the proposed CBR+Re4. Notably, we also show that the proposed Re4 method can help alleviate the repetitive generation issues with LLMs. </p>
<blockquote>
<p>åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æ¢ç´¢äº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨ç”ŸæˆåŠŸèƒ½æµ‹è¯•è„šæœ¬æ–¹é¢çš„æ½œåŠ›ï¼Œè¿™éœ€è¦ç†è§£ç›®æ ‡è½¯ä»¶çš„åŠ¨æ€æ¼”åŒ–ä»£ç ç»“æ„ã€‚ä¸ºäº†å®ç°è¿™ä¸€ç‚¹ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªåŸºäºæ¡ˆä¾‹æ¨ç†ï¼ˆCBRï¼‰çš„ç³»ç»Ÿï¼Œé‡‡ç”¨4Rå¾ªç¯ï¼ˆå³æ£€ç´¢ã€é‡ç”¨ã€ä¿®è®¢å’Œä¿ç•™ï¼‰ï¼Œè¯¥ç³»ç»Ÿç»´æŠ¤å’Œåˆ©ç”¨æµ‹è¯•æ„å›¾æè¿°å’Œç›¸åº”æµ‹è¯•è„šæœ¬çš„æ¡ˆä¾‹åº“ï¼Œä»¥ä¿ƒè¿›LLMè¿›è¡Œæµ‹è¯•è„šæœ¬ç”Ÿæˆã€‚ä¸ºäº†è¿›ä¸€æ­¥æå‡ç”¨æˆ·ä½“éªŒï¼Œæˆ‘ä»¬å¼•å…¥äº†Re4ï¼Œè¿™æ˜¯CBRç³»ç»Ÿçš„ä¼˜åŒ–æ–¹æ³•ï¼ŒåŒ…æ‹¬åŸºäºæ’åºé‡æ’çš„æ£€ç´¢å¾®è°ƒå¼ºåŒ–é‡ç”¨å¾®è°ƒã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬é¦–å…ˆè¯†åˆ«å‡ºé«˜è¯­ä¹‰å’Œè„šæœ¬ç›¸ä¼¼æ€§çš„æ­£é¢ä¾‹å­ï¼Œä¸ºæ— æˆæœ¬æ ‡ç­¾çš„æ£€ç´¢æ¨¡å‹å¾®è°ƒæä¾›å¯é çš„ä¼ªæ ‡ç­¾ã€‚ç„¶åï¼Œæˆ‘ä»¬åº”ç”¨ç›‘ç£å¾®è°ƒï¼Œéšåæ˜¯å¼ºåŒ–å­¦ä¹ å¾®è°ƒé˜¶æ®µï¼Œä½¿LLMç¬¦åˆæˆ‘ä»¬çš„ç”Ÿäº§åœºæ™¯ï¼Œç¡®ä¿æ£€ç´¢åˆ°çš„æ¡ˆä¾‹çš„å¿ å®é‡ç”¨ã€‚åœ¨åä¸ºæ•°æ®é€šä¿¡çš„ä¸¤ä¸ªäº§å“å¼€å‘å•å…ƒçš„å¹¿æ³›å®éªŒç»“æœè¡¨æ˜ï¼Œæå‡ºçš„CBR+Re4å…·æœ‰ä¼˜è¶Šæ€§ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œæˆ‘ä»¬è¿˜è¡¨æ˜ï¼Œæ‰€æå‡ºçš„Re4æ–¹æ³•æœ‰åŠ©äºç¼“è§£LLMçš„é‡å¤ç”Ÿæˆé—®é¢˜ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.20576v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨ç”ŸæˆåŠŸèƒ½æµ‹è¯•è„šæœ¬æ–¹é¢çš„æ½œåŠ›å¾—åˆ°äº†æ¢ç´¢ã€‚ä¸ºæ­¤ï¼Œæå‡ºäº†åŸºäºæ¡ˆä¾‹æ¨ç†ï¼ˆCBRï¼‰çš„ç³»ç»Ÿï¼Œåˆ©ç”¨4Rå¾ªç¯ï¼ˆå³æ£€ç´¢ã€é‡ç”¨ã€ä¿®è®¢å’Œä¿ç•™ï¼‰æ¥ç»´æŠ¤å¹¶åˆ©ç”¨æµ‹è¯•æ„å›¾æè¿°å’Œç›¸åº”æµ‹è¯•è„šæœ¬çš„æ¡ˆä¾‹åº“ï¼Œä»¥ä¿ƒè¿›LLMçš„æµ‹è¯•è„šæœ¬ç”Ÿæˆã€‚ä¸ºè¿›ä¸€æ­¥ä¼˜åŒ–ç”¨æˆ·ä½“éªŒï¼Œå¼•å…¥äº†Re4æ–¹æ³•ï¼ŒåŒ…æ‹¬åŸºäºæ’åºçš„æ£€ç´¢å¾®è°ƒå¼ºåŒ–é‡ç”¨å¾®è°ƒã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒCBR+Re4åœ¨åä¸ºæ•°æ®é€šä¿¡çš„ä¸¤ä¸ªäº§å“å¼€å‘å•å…ƒä¸­çš„è¡¨ç°ä¼˜äºä¼ ç»Ÿæ–¹æ³•ï¼ŒRe4æ–¹æ³•è¿˜å¯ä»¥å¸®åŠ©ç¼“è§£LLMçš„é‡å¤ç”Ÿæˆé—®é¢˜ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ¢ç´¢äº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨ç”ŸæˆåŠŸèƒ½æµ‹è¯•è„šæœ¬æ–¹é¢çš„æ½œåŠ›ã€‚</li>
<li>æå‡ºäº†åŸºäºæ¡ˆä¾‹æ¨ç†ï¼ˆCBRï¼‰çš„ç³»ç»Ÿï¼Œåˆ©ç”¨4Rå¾ªç¯æ¥ä¿ƒè¿›LLMçš„æµ‹è¯•è„šæœ¬ç”Ÿæˆã€‚</li>
<li>å¼•å…¥äº†Re4æ–¹æ³•ï¼ŒåŒ…æ‹¬åŸºäºæ’åºçš„æ£€ç´¢å¾®è°ƒåŠå¼ºåŒ–é‡ç”¨å¾®è°ƒï¼Œä»¥ä¼˜åŒ–ç”¨æˆ·ä½“éªŒã€‚</li>
<li>å®éªŒç»“æœè¡¨æ˜ï¼ŒCBR+Re4åœ¨çœŸå®åœºæ™¯ä¸­çš„è¡¨ç°ä¼˜äºä¼ ç»Ÿæ–¹æ³•ã€‚</li>
<li>CBR+Re4èƒ½å¸®åŠ©è§£å†³LLMåœ¨æµ‹è¯•è„šæœ¬ç”Ÿæˆä¸­çš„é‡å¤ç”Ÿæˆé—®é¢˜ã€‚</li>
<li>åˆ©ç”¨äº†ä¼ªæ ‡ç­¾æŠ€æœ¯æ¥æé«˜æ¨¡å‹çš„æ€§èƒ½ï¼Œå‡å°‘äº†æ˜‚è´µçš„æ ‡ç­¾æˆæœ¬ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.20576">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-ebed85b76ecc601a9d30102b3eddb40b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-bef69a94ffbc5a443d0dc7275728e04c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3012b9e035028772e621a27537432aff.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-dd109bd7323dd466451e40f008ab6c81.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-00d58fc5b08cccba5d72b4246e5ef585.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Low-resource-Information-Extraction-with-the-European-Clinical-Case-Corpus"><a href="#Low-resource-Information-Extraction-with-the-European-Clinical-Case-Corpus" class="headerlink" title="Low-resource Information Extraction with the European Clinical Case   Corpus"></a>Low-resource Information Extraction with the European Clinical Case   Corpus</h2><p><strong>Authors:Soumitra Ghosh, Begona Altuna, Saeed Farzi, Pietro Ferrazzi, Alberto Lavelli, Giulia Mezzanotte, Manuela Speranza, Bernardo Magnini</strong></p>
<p>We present E3C-3.0, a multilingual dataset in the medical domain, comprising clinical cases annotated with diseases and test-result relations. The dataset includes both native texts in five languages (English, French, Italian, Spanish and Basque) and texts translated and projected from the English source into five target languages (Greek, Italian, Polish, Slovak, and Slovenian). A semi-automatic approach has been implemented, including automatic annotation projection based on Large Language Models (LLMs) and human revision. We present several experiments showing that current state-of-the-art LLMs can benefit from being fine-tuned on the E3C-3.0 dataset. We also show that transfer learning in different languages is very effective, mitigating the scarcity of data. Finally, we compare performance both on native data and on projected data. We release the data at <a target="_blank" rel="noopener" href="https://huggingface.co/collections/NLP-FBK/e3c-projected-676a7d6221608d60e4e9fd89">https://huggingface.co/collections/NLP-FBK/e3c-projected-676a7d6221608d60e4e9fd89</a> . </p>
<blockquote>
<p>æˆ‘ä»¬æ¨å‡ºäº†E3C-3.0ï¼Œè¿™æ˜¯ä¸€ä¸ªåŒ»å­¦é¢†åŸŸçš„å¤šè¯­è¨€æ•°æ®é›†ï¼Œå…¶ä¸­åŒ…å«å¸¦æœ‰ç–¾ç—…å’Œæµ‹è¯•ç»“æœå…³ç³»çš„ä¸´åºŠç—…ä¾‹ã€‚è¯¥æ•°æ®é›†åŒ…æ‹¬äº”ç§è¯­è¨€ï¼ˆè‹±è¯­ã€æ³•è¯­ã€æ„å¤§åˆ©è¯­ã€è¥¿ç­ç‰™è¯­å’Œå·´æ–¯å…‹è¯­ï¼‰çš„åŸæ–‡ï¼Œä»¥åŠä»è‹±è¯­æºè¯­è¨€ç¿»è¯‘å¹¶æŠ•å°„åˆ°äº”ç§ç›®æ ‡è¯­è¨€ï¼ˆå¸Œè…Šè¯­ã€æ„å¤§åˆ©è¯­ã€æ³¢å…°è¯­ã€æ–¯æ´›ä¼å…‹è¯­å’Œæ–¯æ´›æ–‡å°¼äºšè¯­ï¼‰çš„æ–‡æœ¬ã€‚æˆ‘ä»¬å®æ–½äº†ä¸€ç§åŠè‡ªåŠ¨æ–¹æ³•ï¼ŒåŒ…æ‹¬åŸºäºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„è‡ªåŠ¨æ³¨é‡ŠæŠ•å½±å’Œäººå·¥ä¿®è®¢ã€‚æˆ‘ä»¬è¿›è¡Œäº†å‡ é¡¹å®éªŒï¼Œè¡¨æ˜å½“å‰å…ˆè¿›çš„LLMå¯ä»¥åœ¨E3C-3.0æ•°æ®é›†ä¸Šè¿›è¡Œå¾®è°ƒè€Œå—ç›Šã€‚æˆ‘ä»¬è¿˜è¡¨æ˜ï¼Œä¸åŒè¯­è¨€çš„è¿ç§»å­¦ä¹ éå¸¸æœ‰æ•ˆï¼Œå¯ä»¥ç¼“è§£æ•°æ®ç¨€ç¼ºçš„é—®é¢˜ã€‚æœ€åï¼Œæˆ‘ä»¬æ¯”è¾ƒäº†åŸç”Ÿæ•°æ®å’ŒæŠ•å°„æ•°æ®ä¸Šçš„æ€§èƒ½ã€‚æˆ‘ä»¬åœ¨<a target="_blank" rel="noopener" href="https://huggingface.co/collections/NLP-FBK/e3c-projected-676a7d6221608d60e4e9fd8f">https://huggingface.co/collections/NLP-FBK/e3c-projected-676a7d6221608d60e4e9fd89ä¸Šå‘å¸ƒæ•°æ®ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.20568v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>E3C-3.0æ•°æ®é›†å‘å¸ƒï¼ŒåŒ…å«åŒ»å­¦é¢†åŸŸå¤šè¯­è¨€ä¸´åºŠç—…ä¾‹æ•°æ®ï¼Œæ ‡æ³¨äº†ç–¾ç—…å’Œæµ‹è¯•ç»“æœå…³ç³»ã€‚æ•°æ®é›†åŒ…å«äº”ç§åŸç”Ÿè¯­è¨€å’Œä»è‹±è¯­ç¿»è¯‘è‡³äº”ç§ç›®æ ‡è¯­è¨€çš„æ–‡æœ¬ã€‚é‡‡ç”¨åŸºäºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„åŠè‡ªåŠ¨æ ‡æ³¨æ–¹æ³•ï¼Œå¹¶æœ‰äººå·¥ä¿®æ­£ã€‚å®éªŒè¡¨æ˜ï¼ŒE3C-3.0æ•°æ®é›†èƒ½æå‡å½“å‰æœ€å‰æ²¿çš„LLMæ€§èƒ½ï¼Œä¸åŒè¯­è¨€çš„è¿ç§»å­¦ä¹ éå¸¸æœ‰æ•ˆï¼Œèƒ½ç¼“è§£æ•°æ®ç¨€ç¼ºé—®é¢˜ã€‚æ­¤å¤–ï¼Œè¿˜å¯¹æ¯”äº†åŸç”Ÿæ•°æ®å’Œç¿»è¯‘æ•°æ®çš„æ€§èƒ½è¡¨ç°ã€‚æ•°æ®é›†å·²å‘å¸ƒåœ¨huggingface.coä¸Šã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>E3C-3.0æ˜¯ä¸€ä¸ªåŒ»å­¦é¢†åŸŸçš„å¤šè¯­è¨€æ•°æ®é›†ï¼ŒåŒ…å«æ ‡æ³¨çš„ç–¾ç—…å’Œæµ‹è¯•ç»“æœå…³ç³»çš„ä¸´åºŠç—…ä¾‹æ•°æ®ã€‚</li>
<li>æ•°æ®é›†æ¶µç›–äº”ç§åŸç”Ÿè¯­è¨€å’Œä»è‹±è¯­ç¿»è¯‘è‡³äº”ç§ç›®æ ‡è¯­è¨€çš„æ–‡æœ¬ã€‚</li>
<li>ä½¿ç”¨äº†åŸºäºå¤§å‹è¯­è¨€æ¨¡å‹çš„åŠè‡ªåŠ¨æ ‡æ³¨æ–¹æ³•ï¼Œå¹¶è¿›è¡Œäº†äººå·¥ä¿®æ­£ã€‚</li>
<li>E3C-3.0æ•°æ®é›†èƒ½æå‡å½“å‰æœ€å‰æ²¿çš„è¯­è¨€æ¨¡å‹æ€§èƒ½ã€‚</li>
<li>ä¸åŒè¯­è¨€çš„è¿ç§»å­¦ä¹ åœ¨E3C-3.0æ•°æ®é›†ä¸­éå¸¸æœ‰æ•ˆã€‚</li>
<li>è¿ç§»å­¦ä¹ èƒ½ç¼“è§£æ•°æ®ç¨€ç¼ºé—®é¢˜ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.20568">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-fd3ce87456fd760bc681e0035dd64fe0.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e635acd5303db06af60538db3e97f049.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c44a61bfabf94eb0cf59d4f9d3525726.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-86f4d50856555dc2cc8a4f4f4edf2516.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="Injecting-Adrenaline-into-LLM-Serving-Boosting-Resource-Utilization-and-Throughput-via-Attention-Disaggregation"><a href="#Injecting-Adrenaline-into-LLM-Serving-Boosting-Resource-Utilization-and-Throughput-via-Attention-Disaggregation" class="headerlink" title="Injecting Adrenaline into LLM Serving: Boosting Resource Utilization and   Throughput via Attention Disaggregation"></a>Injecting Adrenaline into LLM Serving: Boosting Resource Utilization and   Throughput via Attention Disaggregation</h2><p><strong>Authors:Yunkai Liang, Zhangyu Chen, Pengfei Zuo, Zhi Zhou, Xu Chen, Zhou Yu</strong></p>
<p>In large language model (LLM) serving systems, executing each request consists of two phases: the compute-intensive prefill phase and the memory-intensive decoding phase. To prevent performance interference between the two phases, current LLM serving systems typically adopt prefill-decoding disaggregation, where the two phases are split across separate machines. However, we observe this approach leads to significant resource underutilization. Specifically, prefill instances that are compute-intensive suffer from low memory utilization, while decoding instances that are memory-intensive experience low compute utilization. To address this problem, this paper proposes Adrenaline, an attention disaggregation and offloading mechanism designed to enhance resource utilization and performance in LLM serving systems. Adrenalineâ€™s key innovation lies in disaggregating part of the attention computation in the decoding phase and offloading them to prefill instances. The memory-bound nature of decoding-phase attention computation inherently enables an effective offloading strategy, yielding two complementary advantages: 1) improved memory capacity and bandwidth utilization in prefill instances, and 2) increased decoding batch sizes that enhance compute utilization in decoding instances, collectively boosting overall system performance. Adrenaline achieves these gains through three key techniques: low-latency decoding synchronization, resource-efficient prefill colocation, and load-aware offloading scheduling. Experimental results show that Adrenaline achieves 2.28x higher memory capacity and 2.07x better memory bandwidth utilization in prefill instances, up to 1.67x improvements in compute utilization for decoding instances, and 1.68x higher overall inference throughput compared to state-of-the-art systems. </p>
<blockquote>
<p>åœ¨å¤§è§„æ¨¡è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æœåŠ¡ç³»ç»Ÿä¸­ï¼Œæ‰§è¡Œæ¯ä¸ªè¯·æ±‚åŒ…å«ä¸¤ä¸ªé˜¶æ®µï¼šè®¡ç®—å¯†é›†å‹çš„é¢„å¡«å……é˜¶æ®µå’Œå†…å­˜å¯†é›†å‹çš„è§£ç é˜¶æ®µã€‚ä¸ºäº†é¿å…ä¸¤ä¸ªé˜¶æ®µä¹‹é—´çš„æ€§èƒ½å¹²æ‰°ï¼Œå½“å‰çš„LLMæœåŠ¡ç³»ç»Ÿé€šå¸¸é‡‡ç”¨é¢„å¡«å……-è§£ç åˆ†ç¦»çš„æ–¹æ³•ï¼Œå³å°†ä¸¤ä¸ªé˜¶æ®µåˆ†å¸ƒåœ¨ä¸åŒçš„æœºå™¨ä¸Šã€‚ç„¶è€Œï¼Œæˆ‘ä»¬å‘ç°è¿™ç§æ–¹æ³•ä¼šå¯¼è‡´èµ„æºåˆ©ç”¨ç‡ä½ä¸‹ã€‚å…·ä½“æ¥è¯´ï¼Œè®¡ç®—å¯†é›†å‹çš„é¢„å¡«å……å®ä¾‹å†…å­˜åˆ©ç”¨ç‡è¾ƒä½ï¼Œè€Œå†…å­˜å¯†é›†å‹çš„è§£ç å®ä¾‹è®¡ç®—åˆ©ç”¨ç‡è¾ƒä½ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†Adrenalineï¼Œä¸€ç§æ³¨æ„åŠ›åˆ†æ•£å’Œå¸è½½æœºåˆ¶ï¼Œæ—¨åœ¨æé«˜LLMæœåŠ¡ç³»ç»Ÿçš„èµ„æºåˆ©ç”¨ç‡å’Œæ€§èƒ½ã€‚Adrenalineçš„å…³é”®åˆ›æ–°åœ¨äºè§£ç é˜¶æ®µæ³¨æ„åŠ›è®¡ç®—çš„åˆ†æ•£å’Œå¸è½½åˆ°é¢„å¡«å……å®ä¾‹ã€‚è§£ç é˜¶æ®µæ³¨æ„åŠ›è®¡ç®—çš„å†…å­˜ç»‘å®šç‰¹æ€§å¤©ç„¶åœ°ä½¿æœ‰æ•ˆçš„å¸è½½ç­–ç•¥æˆä¸ºå¯èƒ½ï¼Œä»è€Œäº§ç”Ÿä¸¤ä¸ªäº’è¡¥çš„ä¼˜åŠ¿ï¼š1ï¼‰æé«˜é¢„å¡«å……å®ä¾‹çš„å†…å­˜å®¹é‡å’Œå¸¦å®½åˆ©ç”¨ç‡ï¼›2ï¼‰å¢åŠ è§£ç æ‰¹æ¬¡å¤§å°ï¼Œæé«˜è§£ç å®ä¾‹çš„è®¡ç®—åˆ©ç”¨ç‡ï¼Œå…±åŒæé«˜æ•´ä½“ç³»ç»Ÿæ€§èƒ½ã€‚Adrenalineé€šè¿‡ä¸‰é¡¹å…³é”®æŠ€æœ¯å®ç°è¿™äº›æ”¶ç›Šï¼šä½å»¶è¿Ÿè§£ç åŒæ­¥ã€èµ„æºé«˜æ•ˆçš„é¢„å¡«å……å…±ç½®å’Œè´Ÿè½½æ„ŸçŸ¥çš„å¸è½½è°ƒåº¦ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œä¸æœ€æ–°ç³»ç»Ÿç›¸æ¯”ï¼ŒAdrenalineåœ¨é¢„å¡«å……å®ä¾‹ä¸­å®ç°äº†2.28å€çš„é«˜å†…å­˜å®¹é‡å’Œ2.07å€æ›´å¥½çš„å†…å­˜å¸¦å®½åˆ©ç”¨ç‡ï¼Œè§£ç å®ä¾‹çš„è®¡ç®—åˆ©ç”¨ç‡æé«˜äº†1.67å€ï¼Œæ€»ä½“æ¨ç†ååé‡æé«˜äº†1.68å€ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.20552v1">PDF</a> 14 pages, 18 figures</p>
<p><strong>Summary</strong></p>
<p>è¯¥æ–‡ä»‹ç»äº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æœåŠ¡ç³»ç»Ÿä¸­çš„æ€§èƒ½ä¼˜åŒ–æ–¹æ³•ã€‚ä¼ ç»Ÿçš„LLMæœåŠ¡ç³»ç»Ÿå°†è®¡ç®—å¯†é›†å‹çš„é¢„å¡«å……é˜¶æ®µå’Œå†…å­˜å¯†é›†å‹çš„è§£ç é˜¶æ®µåˆ†å¼€å¤„ç†ï¼Œå¯¼è‡´èµ„æºåˆ©ç”¨ç‡ä½ä¸‹ã€‚ä¸ºæ­¤ï¼Œæœ¬æ–‡æå‡ºäº†Adrenalineæœºåˆ¶ï¼Œé€šè¿‡æ³¨æ„åŠ›è®¡ç®—çš„åˆ†æ•£å’Œå¸è½½ï¼Œå°†è§£ç é˜¶æ®µçš„æ³¨æ„åŠ›è®¡ç®—éƒ¨åˆ†è½¬ç§»åˆ°é¢„å¡«å……å®ä¾‹ä¸­ï¼Œä»è€Œæé«˜èµ„æºåˆ©ç”¨ç‡å’Œç³»ç»Ÿæ€§èƒ½ã€‚Adrenalineé€šè¿‡ä¸‰ä¸ªå…³é”®æŠ€æœ¯å®ç°è¿™äº›ä¼˜åŠ¿ï¼šä½å»¶è¿Ÿè§£ç åŒæ­¥ã€èµ„æºé«˜æ•ˆçš„é¢„å¡«å……å…±ç½®å’Œè´Ÿè½½æ„ŸçŸ¥çš„å¸è½½è°ƒåº¦ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒAdrenalineç›¸è¾ƒäºç°æœ‰ç³»ç»Ÿï¼Œåœ¨å†…å­˜å®¹é‡ã€å†…å­˜å¸¦å®½åˆ©ç”¨ç‡ã€è®¡ç®—åˆ©ç”¨ç‡å’Œæ•´ä½“æ¨ç†ååé‡ç­‰æ–¹é¢å‡æœ‰æ˜¾è‘—æé«˜ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLMæœåŠ¡ç³»ç»Ÿä¸­çš„æ€§èƒ½å¹²æ‰°é—®é¢˜ï¼šç”±äºé¢„å¡«å……å’Œè§£ç ä¸¤ä¸ªé˜¶æ®µçš„ç‰¹ç‚¹ä¸åŒï¼Œä¼ ç»Ÿç³»ç»Ÿå°†å®ƒä»¬åˆ†å¸ƒåœ¨ä¸åŒçš„æœºå™¨ä¸Šå¤„ç†ï¼Œå¯¼è‡´èµ„æºåˆ©ç”¨ç‡ä½ä¸‹ã€‚</li>
<li>Adrenalineæœºåˆ¶ï¼šé€šè¿‡æ³¨æ„åŠ›è®¡ç®—çš„åˆ†æ•£å’Œå¸è½½ï¼Œå°†è§£ç é˜¶æ®µçš„æ³¨æ„åŠ›è®¡ç®—éƒ¨åˆ†è½¬ç§»åˆ°é¢„å¡«å……å®ä¾‹ä¸­ï¼Œä»è€Œæé«˜èµ„æºåˆ©ç”¨ç‡å’Œç³»ç»Ÿæ€§èƒ½ã€‚</li>
<li>Adrenalineçš„ä¸‰ä¸ªå…³é”®æŠ€æœ¯ï¼šä½å»¶è¿Ÿè§£ç åŒæ­¥ã€èµ„æºé«˜æ•ˆçš„é¢„å¡«å……å…±ç½®å’Œè´Ÿè½½æ„ŸçŸ¥çš„å¸è½½è°ƒåº¦ã€‚</li>
<li>å®éªŒç»“æœè¡¨æ˜ï¼ŒAdrenalineåœ¨å†…å­˜å®¹é‡ã€å†…å­˜å¸¦å®½åˆ©ç”¨ç‡ã€è®¡ç®—åˆ©ç”¨ç‡å’Œæ•´ä½“æ¨ç†ååé‡ç­‰æ–¹é¢æœ‰æ˜¾è‘—æé«˜ï¼Œç›¸è¾ƒäºç°æœ‰ç³»ç»Ÿè¾¾åˆ°2.28å€ã€2.07å€ã€1.67å€å’Œ1.68å€çš„ä¼˜åŒ–æ•ˆæœã€‚</li>
<li>Adrenalineæœºåˆ¶æé«˜äº†å†…å­˜å®¹é‡å’Œå¸¦å®½åˆ©ç”¨ç‡åœ¨é¢„å¡«å……å®ä¾‹ä¸­ã€‚</li>
<li>Adrenalineé€šè¿‡å¢åŠ è§£ç æ‰¹æ¬¡å¤§å°æé«˜äº†è§£ç å®ä¾‹çš„è®¡ç®—åˆ©ç”¨ç‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.20552">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-5ec27739485c734a27a9b549e1a6ac52.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-517c2c1b704fe4166e5ce2906cf741e7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-dda77f8fcf969819896dee84f1453d90.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-fc83aef3c81eb122706e1322a2f3a425.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-56636d34ac63220fc191fa7e6e7d5336.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-922fe76cf3d9c4a330cf17e774206a4e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d5575f214d55d24f28b2c786b2b9dd2f.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="StableToolBench-MirrorAPI-Modeling-Tool-Environments-as-Mirrors-of-7-000-Real-World-APIs"><a href="#StableToolBench-MirrorAPI-Modeling-Tool-Environments-as-Mirrors-of-7-000-Real-World-APIs" class="headerlink" title="StableToolBench-MirrorAPI: Modeling Tool Environments as Mirrors of   7,000+ Real-World APIs"></a>StableToolBench-MirrorAPI: Modeling Tool Environments as Mirrors of   7,000+ Real-World APIs</h2><p><strong>Authors:Zhicheng Guo, Sijie Cheng, Yuchen Niu, Hao Wang, Sicheng Zhou, Wenbing Huang, Yang Liu</strong></p>
<p>The rapid advancement of large language models (LLMs) has spurred significant interest in tool learning, where LLMs are augmented with external tools to tackle complex tasks. However, existing tool environments face challenges in balancing stability, scalability, and realness, particularly for benchmarking purposes. To address this problem, we propose MirrorAPI, a novel framework that trains specialized LLMs to accurately simulate real API responses, effectively acting as â€œmirrorsâ€ to tool environments. Using a comprehensive dataset of request-response pairs from 7,000+ APIs, we employ supervised fine-tuning and chain-of-thought reasoning to enhance simulation fidelity. MirrorAPI achieves superior accuracy and stability compared to state-of-the-art methods, as demonstrated by its performance on the newly constructed MirrorAPI-Bench and its integration into StableToolBench. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„å¿«é€Ÿå‘å±•æ¿€å‘äº†äººä»¬å¯¹å·¥å…·å­¦ä¹ çš„æµ“åšå…´è¶£ï¼Œå·¥å…·å­¦ä¹ æ˜¯é€šè¿‡å¤–éƒ¨å·¥å…·æ¥å¢å¼ºLLMçš„èƒ½åŠ›ï¼Œä»¥åº”å¯¹å¤æ‚çš„ä»»åŠ¡ã€‚ç„¶è€Œï¼Œç°æœ‰çš„å·¥å…·ç¯å¢ƒåœ¨å¹³è¡¡ç¨³å®šæ€§ã€å¯æ‰©å±•æ€§å’ŒçœŸå®æ€§æ–¹é¢é¢ä¸´æŒ‘æˆ˜ï¼Œç‰¹åˆ«æ˜¯åœ¨åŸºå‡†æµ‹è¯•æ–¹é¢ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†MirrorAPIï¼Œè¿™æ˜¯ä¸€ä¸ªæ–°å‹æ¡†æ¶ï¼Œé€šè¿‡è®­ç»ƒä¸“é—¨çš„LLMæ¥å‡†ç¡®æ¨¡æ‹ŸçœŸå®çš„APIå“åº”ï¼Œæœ‰æ•ˆåœ°å……å½“å·¥å…·ç¯å¢ƒçš„â€œé•œåƒâ€ã€‚æˆ‘ä»¬ä½¿ç”¨æ¥è‡ª7000å¤šä¸ªAPIçš„è¯·æ±‚-å“åº”å¯¹ç»„æˆçš„ç»¼åˆæ•°æ®é›†ï¼Œé‡‡ç”¨æœ‰ç›‘ç£çš„å¾®è°ƒå’Œé“¾å¼æ€ç»´æ¨ç†æŠ€æœ¯ï¼Œæé«˜æ¨¡æ‹Ÿçš„ä¿çœŸåº¦ã€‚MirrorAPIç›¸è¾ƒäºæœ€æ–°æ–¹æ³•å®ç°äº†æ›´é«˜çš„å‡†ç¡®æ€§å’Œç¨³å®šæ€§ï¼Œè¿™ä½“ç°åœ¨å…¶åœ¨æ–°æ„å»ºçš„MirrorAPI-Benchä¸Šçš„è¡¨ç°ä»¥åŠå…¶é›†æˆåˆ°StableToolBenchä¸­ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.20527v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„å¿«é€Ÿå‘å±•æ¿€å‘äº†å·¥å…·å­¦ä¹ çš„å…³æ³¨ã€‚ä¸ºåº”å¯¹ç°æœ‰å·¥å…·ç¯å¢ƒåœ¨ç¨³å®šæ€§ã€å¯æ‰©å±•æ€§å’ŒçœŸå®æ€§æ–¹é¢çš„æŒ‘æˆ˜ï¼Œç‰¹åˆ«æ˜¯ä¸ºäº†åŸºå‡†æµ‹è¯•ç›®çš„ï¼Œæˆ‘ä»¬æå‡ºäº†MirrorAPIè¿™ä¸€æ–°å‹æ¡†æ¶ã€‚å®ƒé€šè¿‡è®­ç»ƒä¸“é—¨çš„LLMæ¥ç²¾ç¡®æ¨¡æ‹ŸçœŸå®çš„APIå“åº”ï¼Œæœ‰æ•ˆæ‰®æ¼”äº†å·¥å…·ç¯å¢ƒçš„â€œé•œåƒâ€ã€‚ä½¿ç”¨åŒ…å«7000å¤šä¸ªAPIçš„è¯·æ±‚-å“åº”å¯¹çš„ç»¼åˆæ•°æ®é›†ï¼Œé€šè¿‡ç›‘ç£å¾®è°ƒä¸é“¾å¼æ€ç»´æ¨ç†å¢å¼ºæ¨¡æ‹Ÿä¿çœŸåº¦ã€‚MirrorAPIç›¸è¾ƒäºæœ€æ–°æ–¹æ³•å…·æœ‰æ›´é«˜çš„å‡†ç¡®æ€§ä¸ç¨³å®šæ€§ï¼Œé€šè¿‡æ–°æ„å»ºçš„MirrorAPI-Benchçš„è¡¨ç°åŠå…¶ä¸StableToolBenchçš„é›†æˆå¾—åˆ°äº†éªŒè¯ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„å¿«é€Ÿå‘å±•æ¨åŠ¨äº†å·¥å…·å­¦ä¹ çš„å…³æ³¨ã€‚</li>
<li>ç°æœ‰å·¥å…·ç¯å¢ƒåœ¨ç¨³å®šæ€§ã€å¯æ‰©å±•æ€§å’ŒçœŸå®æ€§æ–¹é¢å­˜åœ¨æŒ‘æˆ˜ã€‚</li>
<li>MirrorAPIæ˜¯ä¸€ç§æ–°å‹æ¡†æ¶ï¼Œé€šè¿‡è®­ç»ƒLLMæ¥æ¨¡æ‹ŸçœŸå®çš„APIå“åº”ã€‚</li>
<li>MirrorAPIä½¿ç”¨ç»¼åˆæ•°æ®é›†è¿›è¡Œç›‘ç£å¾®è°ƒï¼Œå¢å¼ºæ¨¡æ‹Ÿçš„çœŸå®æ€§ã€‚</li>
<li>MirrorAPIå®ç°äº†é«˜å‡†ç¡®æ€§å’Œç¨³å®šæ€§ï¼Œé€šè¿‡æ–°æ„å»ºçš„MirrorAPI-Benchè¿›è¡Œäº†éªŒè¯ã€‚</li>
<li>MirrorAPIå·²é›†æˆåˆ°StableToolBenchä¸­ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.20527">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-3df9694c2ec09a064d0a77f4bf1c10ca.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-61a8fbd7fe80fe57abde31e44d3dabb5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b51e82315a66f2d6770cd6f01c3378df.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5bf5f2fa3e78a469d0e9a44113c856f0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9d0f3c064c3a03a0c8d4b502910a5520.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-47ca442dd902a93859af410cf43aa72e.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="Vision-Amplified-Semantic-Entropy-for-Hallucination-Detection-in-Medical-Visual-Question-Answering"><a href="#Vision-Amplified-Semantic-Entropy-for-Hallucination-Detection-in-Medical-Visual-Question-Answering" class="headerlink" title="Vision-Amplified Semantic Entropy for Hallucination Detection in Medical   Visual Question Answering"></a>Vision-Amplified Semantic Entropy for Hallucination Detection in Medical   Visual Question Answering</h2><p><strong>Authors:Zehui Liao, Shishuai Hu, Ke Zou, Huazhu Fu, Liangli Zhen, Yong Xia</strong></p>
<p>Multimodal large language models (MLLMs) have demonstrated significant potential in medical Visual Question Answering (VQA). Yet, they remain prone to hallucinations-incorrect responses that contradict input images, posing substantial risks in clinical decision-making. Detecting these hallucinations is essential for establishing trust in MLLMs among clinicians and patients, thereby enabling their real-world adoption. Current hallucination detection methods, especially semantic entropy (SE), have demonstrated promising hallucination detection capacity for LLMs. However, adapting SE to medical MLLMs by incorporating visual perturbations presents a dilemma. Weak perturbations preserve image content and ensure clinical validity, but may be overlooked by medical MLLMs, which tend to over rely on language priors. In contrast, strong perturbations can distort essential diagnostic features, compromising clinical interpretation. To address this issue, we propose Vision Amplified Semantic Entropy (VASE), which incorporates weak image transformations and amplifies the impact of visual input, to improve hallucination detection in medical VQA. We first estimate the semantic predictive distribution under weak visual transformations to preserve clinical validity, and then amplify visual influence by contrasting this distribution with that derived from a distorted image. The entropy of the resulting distribution is estimated as VASE. Experiments on two medical open-ended VQA datasets demonstrate that VASE consistently outperforms existing hallucination detection methods. </p>
<blockquote>
<p>å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨åŒ»ç–—è§†è§‰é—®ç­”ï¼ˆVQAï¼‰ä¸­æ˜¾ç¤ºå‡ºå·¨å¤§æ½œåŠ›ã€‚ç„¶è€Œï¼Œå®ƒä»¬ä»æ˜“å‡ºç°å¹»è§‰â€”â€”å³ç»™å‡ºä¸è¾“å…¥å›¾åƒç›¸çŸ›ç›¾çš„é”™è¯¯ç­”æ¡ˆï¼Œè¿™åœ¨ä¸´åºŠå†³ç­–ä¸­æ„æˆé‡å¤§é£é™©ã€‚å› æ­¤ï¼Œæ£€æµ‹è¿™äº›å¹»è§‰å¯¹äºåœ¨åŒ»ç”Ÿå’Œæ‚£è€…ä¹‹é—´å»ºç«‹å¯¹MLLMsçš„ä¿¡ä»»è‡³å…³é‡è¦ï¼Œä»è€Œæ¨åŠ¨å…¶åœ¨ç°å®ä¸–ç•Œä¸­çš„é‡‡ç”¨ã€‚å½“å‰ï¼Œå°¤å…¶æ˜¯åŸºäºè¯­ä¹‰ç†µï¼ˆSEï¼‰çš„å¹»è§‰æ£€æµ‹æ–¹æ³•å·²ç»æ˜¾ç¤ºå‡ºå¯¹å¤§å‹è¯­è¨€æ¨¡å‹çš„å¹»è§‰æ£€æµ‹æ½œåŠ›ã€‚ç„¶è€Œï¼Œé€šè¿‡å°†è§†è§‰æ‰°åŠ¨çº³å…¥å…¶ä¸­æ¥é€‚åº”åŒ»å­¦MLLMsæ—¶å´é¢ä¸´å›°å¢ƒã€‚å¼±æ‰°åŠ¨ä¿ç•™äº†å›¾åƒå†…å®¹å¹¶ç¡®ä¿ä¸´åºŠæœ‰æ•ˆæ€§ï¼Œä½†å¯èƒ½è¢«åŒ»å­¦MLLMsæ‰€å¿½è§†ï¼Œå› ä¸ºå®ƒä»¬å€¾å‘äºè¿‡åº¦ä¾èµ–è¯­è¨€å…ˆéªŒçŸ¥è¯†ã€‚ç›¸åï¼Œå¼ºçƒˆçš„æ‰°åŠ¨ä¼šç ´åé‡è¦çš„è¯Šæ–­ç‰¹å¾ï¼Œä»è€Œå½±å“ä¸´åºŠè§£é‡Šã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†è§†è§‰æ”¾å¤§è¯­ä¹‰ç†µï¼ˆVASEï¼‰ï¼Œå®ƒç»“åˆäº†å¼±å›¾åƒè½¬æ¢å¹¶æ”¾å¤§äº†è§†è§‰è¾“å…¥çš„å½±å“ï¼Œä»¥æé«˜åŒ»å­¦VQAä¸­çš„å¹»è§‰æ£€æµ‹èƒ½åŠ›ã€‚æˆ‘ä»¬é¦–å…ˆä¼°è®¡å¼±è§†è§‰è½¬æ¢ä¸‹çš„è¯­ä¹‰é¢„æµ‹åˆ†å¸ƒä»¥ä¿ç•™ä¸´åºŠæœ‰æ•ˆæ€§ï¼Œç„¶åé€šè¿‡å¯¹æ¯”æ­¤åˆ†å¸ƒä¸æ¥è‡ªå¤±çœŸå›¾åƒçš„åˆ†å¸ƒæ¥æ”¾å¤§è§†è§‰å½±å“ã€‚æ‰€å¾—åˆ†å¸ƒçš„ç†µä¼°è®¡ä¸ºVASEã€‚åœ¨ä¸¤ä¸ªåŒ»ç–—å¼€æ”¾å¼VQAæ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒVASEå§‹ç»ˆä¼˜äºç°æœ‰çš„å¹»è§‰æ£€æµ‹æ–¹æ³•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.20504v1">PDF</a> 11 pages, 2 figures</p>
<p><strong>Summary</strong></p>
<p>å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨åŒ»ç–—è§†è§‰é—®ç­”ï¼ˆVQAï¼‰ä¸­å±•ç°å‡ºå·¨å¤§æ½œåŠ›ï¼Œä½†ä»å­˜åœ¨ç”Ÿæˆä¸è¾“å…¥å›¾åƒç›¸çŸ›ç›¾çš„é”™è¯¯å›ç­”ï¼ˆå³å¹»è±¡ï¼‰çš„é£é™©ï¼Œå¯¹ä¸´åºŠå†³ç­–äº§ç”Ÿå®è´¨å½±å“ã€‚å¹»è±¡çš„æ£€æµ‹å¯¹äºåœ¨åŒ»ç”Ÿå’Œæ‚£è€…ä¹‹é—´å»ºç«‹å¯¹MLLMsçš„ä¿¡ä»»è‡³å…³é‡è¦ï¼Œæ˜¯å®ç°å…¶åœ¨ç°å®ä¸–ç•Œä¸­çš„åº”ç”¨çš„å…³é”®ã€‚å½“å‰ï¼Œè¯­ä¹‰ç†µï¼ˆSEï¼‰ç­‰å¹»è±¡æ£€æµ‹æ–¹æ³•åœ¨LLMsçš„å¹»è±¡æ£€æµ‹ä¸­æ˜¾ç¤ºå‡ºè‰¯å¥½æ½œåŠ›ã€‚ç„¶è€Œï¼Œå°†SEé€‚åº”äºåŒ»ç–—MLLMså¹¶ç»“åˆè§†è§‰æ‰°åŠ¨å´å­˜åœ¨éš¾é¢˜ã€‚å¼±æ‰°åŠ¨èƒ½ä¿ç•™å›¾åƒå†…å®¹å¹¶ç¡®ä¿ä¸´åºŠæœ‰æ•ˆæ€§ï¼Œä½†å¯èƒ½è¢«åŒ»ç–—MLLMså¿½è§†ï¼Œå› ä¸ºå®ƒä»¬å€¾å‘äºè¿‡åº¦ä¾èµ–è¯­è¨€å…ˆéªŒçŸ¥è¯†ã€‚ç›¸åï¼Œå¼ºæ‰°åŠ¨å¯èƒ½ä¼šç ´åé‡è¦çš„è¯Šæ–­ç‰¹å¾ï¼Œå½±å“ä¸´åºŠè§£è¯»ã€‚ä¸ºè§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºè§†è§‰æ”¾å¤§è¯­ä¹‰ç†µï¼ˆVASEï¼‰ï¼Œç»“åˆå¼±å›¾åƒè½¬æ¢å¹¶æ”¾å¤§è§†è§‰è¾“å…¥çš„å½±å“ï¼Œä»¥æ”¹å–„åŒ»ç–—VQAä¸­çš„å¹»è±¡æ£€æµ‹ã€‚æˆ‘ä»¬é¦–å…ˆåœ¨å¼±è§†è§‰è½¬æ¢ä¸‹ä¼°è®¡è¯­ä¹‰é¢„æµ‹åˆ†å¸ƒä»¥ä¿ç•™ä¸´åºŠæœ‰æ•ˆæ€§ï¼Œç„¶åé€šè¿‡å¯¹æ¯”è¿™ä¸ªåˆ†å¸ƒä¸æ¥è‡ªå¤±çœŸå›¾åƒçš„åˆ†å¸ƒæ¥æ”¾å¤§è§†è§‰å½±å“ã€‚æ‰€å¾—åˆ†å¸ƒçš„ç†µä¼°è®¡ä¸ºVASEã€‚åœ¨ä¸¤ä¸ªåŒ»ç–—å¼€æ”¾å¼VQAæ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒVASEæŒç»­ä¼˜äºç°æœ‰å¹»è±¡æ£€æµ‹æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨åŒ»ç–—è§†è§‰é—®ç­”ï¼ˆVQAï¼‰ä¸­æœ‰æ˜¾è‘—æ½œåŠ›ï¼Œä½†å­˜åœ¨ç”Ÿæˆå¹»è±¡çš„é£é™©ã€‚</li>
<li>å¹»è±¡æ£€æµ‹å¯¹äºåœ¨åŒ»ç”Ÿå’Œæ‚£è€…ä¹‹é—´å»ºç«‹å¯¹MLLMsçš„ä¿¡ä»»è‡³å…³é‡è¦ã€‚</li>
<li>å½“å‰å¹»è±¡æ£€æµ‹æ–¹æ³•åœ¨é€‚åº”åŒ»ç–—MLLMsæ—¶é¢ä¸´éš¾é¢˜ï¼Œéœ€è¦åœ¨å¼±æ‰°åŠ¨å’Œå¼ºæ‰°åŠ¨ä¹‹é—´å–å¾—å¹³è¡¡ã€‚</li>
<li>å¼±æ‰°åŠ¨èƒ½ä¿ç•™ä¸´åºŠæœ‰æ•ˆæ€§ä½†å¯èƒ½è¢«åŒ»ç–—MLLMså¿½è§†ï¼Œè€Œå¼ºæ‰°åŠ¨å¯èƒ½å½±å“ä¸´åºŠè§£è¯»ã€‚</li>
<li>æå‡ºçš„Vision Amplified Semantic Entropyï¼ˆVASEï¼‰æ–¹æ³•é€šè¿‡ç»“åˆå¼±å›¾åƒè½¬æ¢å¹¶æ”¾å¤§è§†è§‰è¾“å…¥æ¥æ”¹å–„å¹»è±¡æ£€æµ‹ã€‚</li>
<li>VASEé€šè¿‡ä¼°è®¡è¯­ä¹‰é¢„æµ‹åˆ†å¸ƒå¹¶å¯¹æ¯”ä¸åŒå›¾åƒæ¡ä»¶ä¸‹çš„åˆ†å¸ƒæ¥å·¥ä½œã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.20504">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-f153004042b44709356f01a24490fe3f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-69a83824c6c6c29b1b6267e94d67c30c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5fe6bc279a937b654cc51647420ddbf9.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="Pedagogy-of-Teaching-Pointers-in-the-C-Programming-Language-using-Graph-Transformations"><a href="#Pedagogy-of-Teaching-Pointers-in-the-C-Programming-Language-using-Graph-Transformations" class="headerlink" title="Pedagogy of Teaching Pointers in the C Programming Language using Graph   Transformations"></a>Pedagogy of Teaching Pointers in the C Programming Language using Graph   Transformations</h2><p><strong>Authors:Adwoa Donyina, Reiko Heckel</strong></p>
<p>Visual learners think in pictures rather than words and learn best when they utilize representations based on graphs, tables, charts, maps, colors and diagrams. We propose a new pedagogy for teaching pointers in the C programming language using graph transformation systems to visually simulate pointer manipulation. In an Introduction to C course, the topic of pointers is often the most difficult one for students to understand; therefore, we experiment with graph-based representations of dynamic pointer structures to reinforce the learning. Groove, a graph transformation tool, is used to illustrate the behaviour of pointers through modelling and simulation. A study is presented to evaluate the effectiveness of the approach. This paper will also provide a comparison to other teaching methods in this area. </p>
<blockquote>
<p>è§†è§‰å­¦ä¹ è€…å€¾å‘äºä»¥å›¾ç‰‡è€Œéæ–‡å­—æ€è€ƒï¼Œå½“ä»–ä»¬åˆ©ç”¨åŸºäºå›¾è¡¨ã€è¡¨æ ¼ã€å›¾å½¢ã€åœ°å›¾ã€é¢œè‰²å’Œå›¾è§£çš„è¡¨ç¤ºæ—¶ï¼Œå­¦ä¹ æ•ˆæœæœ€ä½³ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„æ•™å­¦æ–¹æ³•ï¼Œä½¿ç”¨å›¾å½¢è½¬æ¢ç³»ç»Ÿæ¥æ¨¡æ‹ŸæŒ‡é’ˆæ“ä½œï¼Œä»¥è§†è§‰æ–¹å¼æ•™æˆCè¯­è¨€ä¸­çš„æŒ‡é’ˆã€‚åœ¨Cè¯­è¨€å…¥é—¨è¯¾ç¨‹ä¸­ï¼ŒæŒ‡é’ˆé€šå¸¸æ˜¯å­¦ç”Ÿæœ€éš¾ç†è§£çš„ä¸»é¢˜ä¹‹ä¸€ï¼›å› æ­¤ï¼Œæˆ‘ä»¬é€šè¿‡åŸºäºå›¾å½¢çš„åŠ¨æ€æŒ‡é’ˆç»“æ„è¡¨ç¤ºæ¥è¿›è¡Œå®éªŒï¼Œä»¥åŠ å¼ºå­¦ä¹ ã€‚æœ¬ç ”ç©¶ä½¿ç”¨Grooveè¿™ä¸€å›¾å½¢è½¬æ¢å·¥å…·ï¼Œé€šè¿‡å»ºæ¨¡å’Œæ¨¡æ‹Ÿæ¥å±•ç¤ºæŒ‡é’ˆçš„è¡Œä¸ºã€‚æœ¬ç ”ç©¶è¿˜æä¾›äº†ä¸€é¡¹è¯„ä¼°è¯¥æ–¹æ³•æœ‰æ•ˆæ€§çš„ç ”ç©¶ï¼Œå¹¶å°†ä¸å…¶ä»–æ•™å­¦æ–¹æ³•è¿›è¡Œæ¯”è¾ƒã€‚æœ¬æ–‡è¿˜å°†æ¯”è¾ƒè¿™ä¸€é¢†åŸŸçš„å…¶ä»–æ•™å­¦æ–¹æ³•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.20469v1">PDF</a> In Proceedings GCM 2023 and 2024, arXiv:2503.19632</p>
<p><strong>Summary</strong></p>
<p>æœ¬è®ºæ–‡æå‡ºäº†ä¸€ç§æ–°çš„æ•™å­¦æ–¹æ³•ï¼Œåˆ©ç”¨å›¾å½¢è½¬æ¢ç³»ç»Ÿæ¥æ¨¡æ‹ŸæŒ‡é’ˆæ“ä½œï¼Œä»¥å›¾å½¢ã€è¡¨æ ¼ã€å›¾è¡¨ç­‰æ–¹å¼è¾…åŠ©æ•™å­¦ï¼Œå¸®åŠ©è§†è§‰å­¦ä¹ è€…æ›´å¥½åœ°ç†è§£å’ŒæŒæ¡Cè¯­è¨€ä¸­æŒ‡é’ˆçš„æ¦‚å¿µã€‚é’ˆå¯¹Cè¯­è¨€å…¥é—¨è¯¾ç¨‹ä¸­çš„éš¾ç‚¹â€”â€”æŒ‡é’ˆæ¦‚å¿µï¼Œå®éªŒæ€§åœ°é‡‡ç”¨åŸºäºå›¾å½¢çš„è¡¨ç¤ºæ–¹æ³•æ¥å¼ºåŒ–å­¦ä¹ ã€‚è¯¥ç ”ç©¶ä½¿ç”¨Grooveå›¾å½¢è½¬æ¢å·¥å…·æ¥æ¨¡æ‹ŸæŒ‡é’ˆè¡Œä¸ºã€‚è®ºæ–‡æœ€åé€šè¿‡å¯¹æ¯”å…¶ä»–æ•™å­¦æ–¹æ³•æ¥è¯„ä¼°æ­¤æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è§†è§‰å­¦ä¹ è€…å€¾å‘äºé€šè¿‡å›¾ç‰‡è€Œéæ–‡å­—è¿›è¡Œå­¦ä¹ ï¼Œå› æ­¤åˆ©ç”¨å›¾å½¢è½¬æ¢ç³»ç»Ÿè¾…åŠ©æ•™å­¦æœ‰åŠ©äºæå‡å­¦ä¹ æ•ˆæœã€‚</li>
<li>æŒ‡é’ˆæ˜¯Cè¯­è¨€å­¦ä¹ ä¸­éš¾åº¦è¾ƒå¤§çš„çŸ¥è¯†ç‚¹ï¼Œæ–°çš„æ•™å­¦æ–¹æ³•é€šè¿‡å›¾å½¢åŒ–è¡¨ç¤ºå’Œæ¨¡æ‹Ÿèƒ½å¤Ÿå¸®åŠ©ç†è§£æŒ‡é’ˆè¡Œä¸ºã€‚</li>
<li>Grooveå›¾å½¢è½¬æ¢å·¥å…·è¢«ç”¨äºå±•ç¤ºæŒ‡é’ˆè¡Œä¸ºï¼Œä»¥å¢å¼ºæ•™å­¦æ•ˆæœã€‚</li>
<li>æ­¤æ–¹æ³•ç›¸å¯¹äºä¼ ç»Ÿçš„æ•™å­¦æ–¹æ³•æ›´ä¸ºæœ‰æ•ˆï¼Œèƒ½æ›´å¥½åœ°å¸®åŠ©å­¦ç”ŸæŒæ¡æŒ‡é’ˆçš„æ¦‚å¿µã€‚</li>
<li>è¯¥æ–¹æ³•ä¸ä»…å…³æ³¨ç†è®ºæ•™å­¦ï¼Œè¿˜é€šè¿‡æ¨¡æ‹Ÿå’Œå»ºæ¨¡æ¥å¼ºåŒ–å­¦ä¹ ï¼Œæå‡äº†å­¦ç”Ÿçš„å®è·µèƒ½åŠ›ã€‚</li>
<li>è¯¥æ–¹æ³•æ³¨é‡åŠ¨æ€æŒ‡é’ˆç»“æ„çš„å›¾å½¢è¡¨ç¤ºï¼Œæœ‰åŠ©äºå­¦ç”Ÿåœ¨åŠ¨æ€ç¯å¢ƒä¸­ç†è§£æŒ‡é’ˆæ“ä½œã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.20469">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-319f666f1206640cd1c3c7a434ecaec5.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-f69d8e1bbbb030a4906365856f8e774a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2245bf75c7c0157c630138fb2cd4001d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7667c1e145014797b6805d38ce1afc8d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-25b8c164116c858523672fba275b0a08.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-bdf95909cc91d7be1956cd965281c6e0.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="Siformer-Feature-isolated-Transformer-for-Efficient-Skeleton-based-Sign-Language-Recognition"><a href="#Siformer-Feature-isolated-Transformer-for-Efficient-Skeleton-based-Sign-Language-Recognition" class="headerlink" title="Siformer: Feature-isolated Transformer for Efficient Skeleton-based Sign   Language Recognition"></a>Siformer: Feature-isolated Transformer for Efficient Skeleton-based Sign   Language Recognition</h2><p><strong>Authors:Muxin Pu, Mei Kuan Lim, Chun Yong Chong</strong></p>
<p>Sign language recognition (SLR) refers to interpreting sign language glosses from given videos automatically. This research area presents a complex challenge in computer vision because of the rapid and intricate movements inherent in sign languages, which encompass hand gestures, body postures, and even facial expressions. Recently, skeleton-based action recognition has attracted increasing attention due to its ability to handle variations in subjects and backgrounds independently. However, current skeleton-based SLR methods exhibit three limitations: 1) they often neglect the importance of realistic hand poses, where most studies train SLR models on non-realistic skeletal representations; 2) they tend to assume complete data availability in both training or inference phases, and capture intricate relationships among different body parts collectively; 3) these methods treat all sign glosses uniformly, failing to account for differences in complexity levels regarding skeletal representations. To enhance the realism of hand skeletal representations, we present a kinematic hand pose rectification method for enforcing constraints. Mitigating the impact of missing data, we propose a feature-isolated mechanism to focus on capturing local spatial-temporal context. This method captures the context concurrently and independently from individual features, thus enhancing the robustness of the SLR model. Additionally, to adapt to varying complexity levels of sign glosses, we develop an input-adaptive inference approach to optimise computational efficiency and accuracy. Experimental results demonstrate the effectiveness of our approach, as evidenced by achieving a new state-of-the-art (SOTA) performance on WLASL100 and LSA64. For WLASL100, we achieve a top-1 accuracy of 86.50%, marking a relative improvement of 2.39% over the previous SOTA. For LSA64, we achieve a top-1 accuracy of 99.84%. </p>
<blockquote>
<p>æ‰‹åŠ¿è¯­è¨€è¯†åˆ«ï¼ˆSLRï¼‰æŒ‡çš„æ˜¯è‡ªåŠ¨è§£è¯»è§†é¢‘ä¸­çš„æ‰‹åŠ¿è¯­è¨€è¯‘è¯ã€‚è¿™ä¸€ç ”ç©¶é¢†åŸŸåœ¨è®¡ç®—æœºè§†è§‰ä¸­æ„æˆäº†ä¸€ä¸ªå¤æ‚çš„æŒ‘æˆ˜ï¼Œå› ä¸ºæ‰‹åŠ¿è¯­è¨€åŒ…å«å¿«é€Ÿè€Œå¤æ‚çš„æ‰‹åŠ¿ã€èº«ä½“å§¿åŠ¿ï¼Œç”šè‡³é¢éƒ¨è¡¨æƒ…ã€‚æœ€è¿‘ï¼ŒåŸºäºéª¨æ¶çš„åŠ¨ä½œè¯†åˆ«å› å…¶èƒ½å¤Ÿç‹¬ç«‹å¤„ç†ä¸»ä½“å’ŒèƒŒæ™¯çš„å˜åŒ–è€Œå¤‡å—å…³æ³¨ã€‚ç„¶è€Œï¼Œå½“å‰çš„åŸºäºéª¨æ¶çš„SLRæ–¹æ³•å­˜åœ¨ä¸‰ä¸ªå±€é™æ€§ï¼š1ï¼‰å®ƒä»¬å¾€å¾€å¿½è§†äº†ç°å®æ‰‹åŠ¿å§¿åŠ¿çš„é‡è¦æ€§ï¼Œå¤§å¤šæ•°ç ”ç©¶éƒ½åœ¨éç°å®éª¨æ¶è¡¨ç¤ºä¸Šè®­ç»ƒSLRæ¨¡å‹ï¼›2ï¼‰å®ƒä»¬å¾€å¾€å‡è®¾åœ¨è®­ç»ƒæˆ–æ¨ç†é˜¶æ®µæ•°æ®å®Œæ•´ï¼Œå¹¶æ•æ‰ä¸åŒèº«ä½“éƒ¨ä½ä¹‹é—´çš„å¤æ‚å…³ç³»ï¼›3ï¼‰è¿™äº›æ–¹æ³•å¯¹æ‰€æœ‰æ‰‹åŠ¿è¯‘è¯ä¸€è§†åŒä»ï¼Œæœªèƒ½è€ƒè™‘å…³äºéª¨æ¶è¡¨ç¤ºçš„å¤æ‚ç¨‹åº¦å·®å¼‚ã€‚ä¸ºäº†æé«˜æ‰‹éƒ¨éª¨æ¶è¡¨ç¤ºçš„é€¼çœŸæ€§ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºåŠ¨åŠ›å­¦çš„æ‰‹åŠ¿å§¿åŠ¿çŸ«æ­£æ–¹æ³•æ¥æ–½åŠ çº¦æŸã€‚ä¸ºäº†å‡è½»ç¼ºå¤±æ•°æ®çš„å½±å“ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ç‰¹å¾éš”ç¦»æœºåˆ¶ï¼Œä¸“æ³¨äºæ•æ‰å±€éƒ¨æ—¶ç©ºä¸Šä¸‹æ–‡ã€‚è¿™ç§æ–¹æ³•å¯ä»¥åŒæ—¶ç‹¬ç«‹åœ°ä»å„ä¸ªç‰¹å¾ä¸­æ•è·ä¸Šä¸‹æ–‡ï¼Œä»è€Œæé«˜äº†SLRæ¨¡å‹çš„ç¨³å¥æ€§ã€‚æ­¤å¤–ï¼Œä¸ºäº†é€‚åº”ä¸åŒå¤æ‚ç¨‹åº¦çš„æ‰‹åŠ¿è¯‘è¯ï¼Œæˆ‘ä»¬å¼€å‘äº†ä¸€ç§è¾“å…¥è‡ªé€‚åº”æ¨ç†æ–¹æ³•ï¼Œä»¥ä¼˜åŒ–è®¡ç®—æ•ˆç‡å’Œå‡†ç¡®æ€§ã€‚å®éªŒç»“æœè¯æ˜äº†æˆ‘ä»¬çš„æ–¹æ³•çš„æœ‰æ•ˆæ€§ï¼Œæˆ‘ä»¬åœ¨WLASL100å’ŒLSA64ä¸Šå®ç°äº†æœ€æ–°çš„æœ€ä½³æ€§èƒ½ã€‚å¯¹äºWLASL100ï¼Œæˆ‘ä»¬è¾¾åˆ°äº†86.50%çš„top-1å‡†ç¡®ç‡ï¼Œç›¸å¯¹äºä¹‹å‰çš„æœ€ä½³æ€§èƒ½æœ‰2.39%çš„ç›¸å¯¹æ”¹è¿›ã€‚å¯¹äºLSA64ï¼Œæˆ‘ä»¬è¾¾åˆ°äº†99.84%çš„top-1å‡†ç¡®ç‡ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.20436v1">PDF</a> 10 pages, ACM Multimedia</p>
<p><strong>Summary</strong></p>
<p>è¯¥æ–‡æœ¬ç ”ç©¶äº†åŸºäºéª¨æ¶åŠ¨ä½œè¯†åˆ«çš„æ‰‹åŠ¿è¯­è¨€è¯†åˆ«ï¼ˆSLRï¼‰ã€‚æ–‡ä¸­æåˆ°å½“å‰SLRæ–¹æ³•å­˜åœ¨ä¸‰ä¸ªå±€é™æ€§ï¼Œå¹¶é’ˆå¯¹è¿™äº›é—®é¢˜æå‡ºäº†æ”¹è¿›æ–¹æ¡ˆã€‚é¦–å…ˆï¼Œé€šè¿‡å¼•å…¥è¿åŠ¨å­¦æ‰‹åŠ¿å§¿æ€æ ¡æ­£æ–¹æ³•å¢å¼ºæ‰‹éƒ¨éª¨æ¶è¡¨ç¤ºçš„ç°å®æ„Ÿï¼›å…¶æ¬¡ï¼Œæå‡ºä¸€ç§ç‰¹å¾éš”ç¦»æœºåˆ¶ä»¥åº”å¯¹ç¼ºå¤±æ•°æ®é—®é¢˜ï¼Œå¹¶ä¸“æ³¨äºæ•æ‰å±€éƒ¨æ—¶ç©ºä¸Šä¸‹æ–‡ï¼›æœ€åï¼Œä¸ºäº†é€‚åº”ä¸åŒå¤æ‚ç¨‹åº¦çš„æ ‡å¿—è¯­ï¼Œå¼€å‘äº†ä¸€ç§è¾“å…¥è‡ªé€‚åº”æ¨ç†æ–¹æ³•ä»¥æé«˜è®¡ç®—æ•ˆç‡å’Œå‡†ç¡®æ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨WLASL100å’ŒLSA64æ•°æ®é›†ä¸Šå–å¾—äº†æœ€æ–°æŠ€æœ¯æ°´å¹³çš„æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>SLRæ˜¯æŒ‡è‡ªåŠ¨è§£é‡Šæ ‡å¿—è¯­è¨€æ‰‹åŠ¿çš„ç³»ç»Ÿã€‚åœ¨è®¡ç®—æœºè§†è§‰é¢†åŸŸï¼Œè¿™æ˜¯ä¸€ä¸ªå¤æ‚çš„æŒ‘æˆ˜ï¼Œå› ä¸ºæ ‡å¿—è¯­è¨€åŒ…å«å¿«é€Ÿå’Œå¤æ‚çš„åŠ¨ä½œï¼Œå¦‚æ‰‹åŠ¿ã€èº«ä½“å§¿åŠ¿å’Œé¢éƒ¨è¡¨æƒ…ã€‚</li>
<li>å½“å‰åŸºäºéª¨æ¶çš„åŠ¨ä½œè¯†åˆ«å·²å¼•èµ·å…³æ³¨ï¼Œå› ä¸ºå®ƒå¯ä»¥ç‹¬ç«‹å¤„ç†ä¸»ä½“å’ŒèƒŒæ™¯çš„å˜åŒ–ã€‚ä½†ç°æœ‰çš„éª¨æ¶SLRæ–¹æ³•å­˜åœ¨ä¸‰ä¸ªä¸»è¦å±€é™æ€§ã€‚</li>
<li>ä¸ºäº†å¢å¼ºæ‰‹éƒ¨éª¨æ¶è¡¨ç¤ºçš„ç°å®æ„Ÿï¼Œå¼•å…¥äº†è¿åŠ¨å­¦æ‰‹åŠ¿å§¿æ€æ ¡æ­£æ–¹æ³•ã€‚</li>
<li>ä¸ºäº†å¤„ç†ç¼ºå¤±æ•°æ®é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§ç‰¹å¾éš”ç¦»æœºåˆ¶ï¼Œä¸“æ³¨äºæ•æ‰å±€éƒ¨æ—¶ç©ºä¸Šä¸‹æ–‡ï¼Œä»è€Œæé«˜SLRæ¨¡å‹çš„ç¨³å¥æ€§ã€‚</li>
<li>ä¸ºäº†é€‚åº”ä¸åŒå¤æ‚ç¨‹åº¦çš„æ ‡å¿—è¯­ï¼Œå¼€å‘äº†è¾“å…¥è‡ªé€‚åº”æ¨ç†æ–¹æ³•ä»¥æé«˜è®¡ç®—æ•ˆç‡å’Œå‡†ç¡®æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.20436">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-32c5b5d5b02618c8fb15c1a5d7f4f85a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-781df4837947b2569f0845e3903ef5ba.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-727bef83ed3bad66fefc1347de332042.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c90faab45b8b3eb5e7f04f4fbc8447d9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-531fab495b514e916bb52fbeb09a41e2.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ea7850c0cf98736e9f7a729244f838c5.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="Advancements-in-Natural-Language-Processing-Exploring-Transformer-Based-Architectures-for-Text-Understanding"><a href="#Advancements-in-Natural-Language-Processing-Exploring-Transformer-Based-Architectures-for-Text-Understanding" class="headerlink" title="Advancements in Natural Language Processing: Exploring Transformer-Based   Architectures for Text Understanding"></a>Advancements in Natural Language Processing: Exploring Transformer-Based   Architectures for Text Understanding</h2><p><strong>Authors:Tianhao Wu, Yu Wang, Ngoc Quach</strong></p>
<p>Natural Language Processing (NLP) has witnessed a transformative leap with the advent of transformer-based architectures, which have significantly enhanced the ability of machines to understand and generate human-like text. This paper explores the advancements in transformer models, such as BERT and GPT, focusing on their superior performance in text understanding tasks compared to traditional methods like recurrent neural networks (RNNs). By analyzing statistical properties through visual representations-including probability density functions of text length distributions and feature space classifications-the study highlights the modelsâ€™ proficiency in handling long-range dependencies, adapting to conditional shifts, and extracting features for classification, even with overlapping classes. Drawing on recent 2024 research, including enhancements in multi-hop knowledge graph reasoning and context-aware chat interactions, the paper outlines a methodology involving data preparation, model selection, pretraining, fine-tuning, and evaluation. The results demonstrate state-of-the-art performance on benchmarks like GLUE and SQuAD, with F1 scores exceeding 90%, though challenges such as high computational costs persist. This work underscores the pivotal role of transformers in modern NLP and suggests future directions, including efficiency optimization and multimodal integration, to further advance language-based AI systems. </p>
<blockquote>
<p>è‡ªç„¶è¯­è¨€å¤„ç†ï¼ˆNLPï¼‰éšç€åŸºäºTransformerçš„æ¶æ„çš„å‡ºç°è€Œå‘ç”Ÿäº†å·¨å¤§çš„é£è·ƒï¼Œæå¤§åœ°å¢å¼ºäº†æœºå™¨ç†è§£å’Œç”Ÿæˆç±»ä¼¼äººç±»æ–‡æœ¬çš„èƒ½åŠ›ã€‚æœ¬æ–‡æ¢è®¨äº†Transformeræ¨¡å‹çš„æœ€æ–°è¿›å±•ï¼Œå¦‚BERTå’ŒGPTï¼Œé‡ç‚¹ä»‹ç»äº†å®ƒä»¬åœ¨æ–‡æœ¬ç†è§£ä»»åŠ¡ä¸­ç›¸è¾ƒäºä¼ ç»Ÿæ–¹æ³•ï¼ˆå¦‚å¾ªç¯ç¥ç»ç½‘ç»œï¼ˆRNNï¼‰ï¼‰çš„å“è¶Šæ€§èƒ½ã€‚æœ¬æ–‡é€šè¿‡è§†è§‰è¡¨ç¤ºåˆ†æç»Ÿè®¡å±æ€§ï¼ŒåŒ…æ‹¬æ–‡æœ¬é•¿åº¦åˆ†å¸ƒçš„æ¦‚ç‡å¯†åº¦å‡½æ•°å’Œç‰¹å¾ç©ºé—´åˆ†ç±»ï¼Œç ”ç©¶äº†è¿™äº›æ¨¡å‹åœ¨å¤„ç†é•¿è·ç¦»ä¾èµ–ã€é€‚åº”æ¡ä»¶å˜åŒ–å’Œåˆ†ç±»ç‰¹å¾æå–æ–¹é¢çš„èƒ½åŠ›ï¼Œå³ä½¿å­˜åœ¨é‡å çš„ç±»åˆ«ä¹Ÿæ˜¯å¦‚æ­¤ã€‚æœ¬æ–‡å€Ÿé‰´äº†æœ€è¿‘çš„2024å¹´ç ”ç©¶ï¼ŒåŒ…æ‹¬å¤šè·³çŸ¥è¯†å›¾è°±æ¨ç†å’Œä¸Šä¸‹æ–‡æ„ŸçŸ¥èŠå¤©äº¤äº’çš„æ”¹è¿›ï¼Œæ¦‚è¿°äº†ä¸€ç§æ¶‰åŠæ•°æ®å‡†å¤‡ã€æ¨¡å‹é€‰æ‹©ã€é¢„è®­ç»ƒã€å¾®è°ƒè¯„ä¼°çš„æ–¹æ³•è®ºã€‚ç»“æœè¡¨æ˜ï¼Œåœ¨GLUEå’ŒSQuADç­‰åŸºå‡†æµ‹è¯•ä¸Šçš„æ€§èƒ½è¾¾åˆ°äº†æœ€æ–°æ°´å¹³ï¼ŒF1åˆ†æ•°è¶…è¿‡90%ï¼Œå°½ç®¡ä»å­˜åœ¨é«˜è®¡ç®—æˆæœ¬ç­‰æŒ‘æˆ˜ã€‚è¿™é¡¹å·¥ä½œå¼ºè°ƒäº†Transformeråœ¨ç°ä»£NLPä¸­çš„æ ¸å¿ƒä½œç”¨ï¼Œå¹¶æŒ‡å‡ºäº†æœªæ¥çš„å‘å±•æ–¹å‘ï¼ŒåŒ…æ‹¬ä¼˜åŒ–æ•ˆç‡å’Œå¤šæ¨¡å¼é›†æˆï¼Œä»¥è¿›ä¸€æ­¥æ¨åŠ¨åŸºäºè¯­è¨€çš„AIç³»ç»Ÿçš„è¿›æ­¥ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.20227v1">PDF</a> This paper has been accepted by the 5th International Conference on   Artificial Intelligence and Industrial Technology Applications (AIITA 2025)</p>
<p><strong>Summary</strong></p>
<p>éšç€åŸºäºè½¬æ¢å™¨çš„æ¶æ„çš„å‡ºç°ï¼Œè‡ªç„¶è¯­è¨€å¤„ç†ï¼ˆNLPï¼‰é¢†åŸŸå‘ç”Ÿäº†å·¨å¤§çš„é£è·ƒï¼Œæ˜¾è‘—å¢å¼ºäº†æœºå™¨ç†è§£å’Œç”Ÿæˆç±»ä¼¼äººç±»æ–‡æœ¬çš„èƒ½åŠ›ã€‚æœ¬æ–‡æ¢è®¨äº†BERTå’ŒGPTç­‰è½¬æ¢å™¨æ¨¡å‹çš„è¿›æ­¥ï¼Œç‰¹åˆ«æ˜¯åœ¨æ–‡æœ¬ç†è§£ä»»åŠ¡ä¸­çš„å“è¶Šæ€§èƒ½ã€‚é€šè¿‡å¯è§†åŒ–è¡¨ç°å½¢å¼åˆ†æç»Ÿè®¡å±æ€§ï¼Œæœ¬ç ”ç©¶å¼ºè°ƒäº†æ¨¡å‹å¤„ç†é•¿è·ç¦»ä¾èµ–å…³ç³»çš„èƒ½åŠ›ï¼Œé€‚åº”æ¡ä»¶å˜åŒ–ï¼Œå¹¶ä¸ºåˆ†ç±»æå–ç‰¹å¾çš„èƒ½åŠ›ã€‚æœ¬æ–‡ä»‹ç»äº†åŒ…æ‹¬å¤šè·³çŸ¥è¯†å›¾è°±æ¨ç†å’Œä¸Šä¸‹æ–‡æ„ŸçŸ¥èŠå¤©äº¤äº’åœ¨å†…çš„æœ€æ–°ç ”ç©¶æˆæœã€‚æ­¤æ–¹æ³•åŒ…æ‹¬æ•°æ®å‡†å¤‡ã€æ¨¡å‹é€‰æ‹©ã€é¢„è®­ç»ƒã€å¾®è°ƒåŠè¯„ä¼°ç­‰ç¯èŠ‚ã€‚ç»“æœå±•ç¤ºäº†åœ¨GLUEå’ŒSQuADç­‰åŸºå‡†æµ‹è¯•ä¸Šçš„å“è¶Šæ€§èƒ½ï¼ŒF1åˆ†æ•°è¶…è¿‡90%ï¼Œä½†ä»é¢ä¸´é«˜è®¡ç®—æˆæœ¬ç­‰æŒ‘æˆ˜ã€‚æœ¬æ–‡å¼ºè°ƒäº†è½¬æ¢å™¨åœ¨ç°ä»£NLPä¸­çš„å…³é”®ä½œç”¨ï¼Œå¹¶å»ºè®®æœªæ¥çš„å‘å±•æ–¹å‘åŒ…æ‹¬ä¼˜åŒ–æ•ˆç‡å’Œå¤šåª’ä½“èåˆç­‰ã€‚æ€»ä¹‹ï¼ŒNLPé¢†åŸŸçš„æœ€æ–°å‘å±•ä½¿å¾—æœºå™¨çš„è¯­è¨€å¤„ç†èƒ½åŠ›æ˜¾è‘—æé«˜ã€‚è™½ç„¶è¿˜å­˜åœ¨æŒ‘æˆ˜ï¼Œä½†è¯¥é¢†åŸŸå…·æœ‰å¹¿é˜”çš„åº”ç”¨å‰æ™¯å’Œå‘å±•æ½œåŠ›ã€‚æ€»çš„æ¥è¯´NLPçš„ç ”ç©¶æ­£èµ°åœ¨æŒç»­ä¼˜åŒ–çš„é“è·¯ä¸Šï¼Œè®©æœºå™¨æ›´å¥½åœ°ç†è§£äººç±»è¯­è¨€æˆä¸ºäº†ä¸€ä¸ªé‡è¦çš„ç ”ç©¶æ–¹å‘ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è½¬æ¢å™¨æ¶æ„çš„å‡ºç°ä½¿è‡ªç„¶è¯­è¨€å¤„ç†é¢†åŸŸå‘ç”Ÿäº†å·¨å¤§é£è·ƒï¼Œæé«˜äº†æœºå™¨ç†è§£å’Œç”Ÿæˆç±»ä¼¼äººç±»æ–‡æœ¬çš„èƒ½åŠ›ã€‚</li>
<li>BERTå’ŒGPTç­‰è½¬æ¢å™¨æ¨¡å‹åœ¨æ–‡æœ¬ç†è§£ä»»åŠ¡ä¸­è¡¨ç°å‡ºå“è¶Šæ€§èƒ½ã€‚</li>
<li>é€šè¿‡å¯è§†åŒ–è¡¨ç°å½¢å¼åˆ†æç»Ÿè®¡å±æ€§ï¼Œæ­ç¤ºäº†è½¬æ¢å™¨æ¨¡å‹å¤„ç†é•¿è·ç¦»ä¾èµ–å…³ç³»ã€é€‚åº”æ¡ä»¶å˜åŒ–å’Œæå–ç‰¹å¾çš„èƒ½åŠ›ã€‚</li>
<li>æœ€æ–°ç ”ç©¶æˆæœåŒ…æ‹¬å¤šè·³çŸ¥è¯†å›¾è°±æ¨ç†å’Œä¸Šä¸‹æ–‡æ„ŸçŸ¥èŠå¤©äº¤äº’ç­‰ã€‚</li>
<li>NLPé¢†åŸŸçš„æœ€æ–°æŠ€æœ¯å·²ç»å–å¾—äº†åœ¨GLUEå’ŒSQuADç­‰åŸºå‡†æµ‹è¯•ä¸Šçš„å“è¶Šæ€§èƒ½ï¼ŒF1åˆ†æ•°è¶…è¿‡90%ã€‚</li>
<li>NLPé¢†åŸŸä»é¢ä¸´é«˜è®¡ç®—æˆæœ¬ç­‰æŒ‘æˆ˜ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.20227">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-3db5142929fa0fdac41589db81f570ef.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-52867f3e316745a90e07f71eaf39b62b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-aa8fdbe251f607734f047ceab57b82c4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ad200e447b1e7d7c721a6c8d7fddb34e.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="LangBridge-Interpreting-Image-as-a-Combination-of-Language-Embeddings"><a href="#LangBridge-Interpreting-Image-as-a-Combination-of-Language-Embeddings" class="headerlink" title="LangBridge: Interpreting Image as a Combination of Language Embeddings"></a>LangBridge: Interpreting Image as a Combination of Language Embeddings</h2><p><strong>Authors:Jiaqi Liao, Yuwei Niu, Fanqing Meng, Hao Li, Changyao Tian, Yinuo Du, Yuwen Xiong, Dianqi Li, Xizhou Zhu, Li Yuan, Jifeng Dai, Yu Cheng</strong></p>
<p>Recent years have witnessed remarkable advances in Large Vision-Language Models (LVLMs), which have achieved human-level performance across various complex vision-language tasks. Following LLaVAâ€™s paradigm, mainstream LVLMs typically employ a shallow MLP for visual-language alignment through a two-stage training process: pretraining for cross-modal alignment followed by instruction tuning. While this approach has proven effective, the underlying mechanisms of how MLPs bridge the modality gap remain poorly understood. Although some research has explored how LLMs process transformed visual tokens, few studies have investigated the fundamental alignment mechanism. Furthermore, the MLP adapter requires retraining whenever switching LLM backbones. To address these limitations, we first investigate the working principles of MLP adapters and discover that they learn to project visual embeddings into subspaces spanned by corresponding text embeddings progressively. Based on this insight, we propose LangBridge, a novel adapter that explicitly maps visual tokens to linear combinations of LLM vocabulary embeddings. This innovative design enables pretraining-free adapter transfer across different LLMs while maintaining performance. Our experimental results demonstrate that a LangBridge adapter pre-trained on Qwen2-0.5B can be directly applied to larger models such as LLaMA3-8B or Qwen2.5-14B while maintaining competitive performance. Overall, LangBridge enables interpretable vision-language alignment by grounding visual representations in LLM vocab embedding, while its plug-and-play design ensures efficient reuse across multiple LLMs with nearly no performance degradation. See our project page at <a target="_blank" rel="noopener" href="https://jiaqiliao77.github.io/LangBridge.github.io/">https://jiaqiliao77.github.io/LangBridge.github.io/</a> </p>
<blockquote>
<p>è¿‘å¹´æ¥ï¼Œå¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆLVLMsï¼‰å–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œåœ¨å„ç§å¤æ‚çš„è§†è§‰è¯­è¨€ä»»åŠ¡ä¸­è¾¾åˆ°äº†äººç±»æ°´å¹³çš„æ€§èƒ½ã€‚éµå¾ªLLaVAçš„æ¨¡å¼ï¼Œä¸»æµçš„LVLMé€šå¸¸é‡‡ç”¨æµ…å±‚çš„MLPè¿›è¡Œè§†è§‰è¯­è¨€å¯¹é½ï¼Œé€šè¿‡ä¸¤é˜¶æ®µè®­ç»ƒè¿‡ç¨‹ï¼šé¦–å…ˆæ˜¯è·¨æ¨¡æ€å¯¹é½çš„é¢„è®­ç»ƒï¼Œç„¶åæ˜¯æŒ‡ä»¤å¾®è°ƒã€‚å°½ç®¡è¿™ç§æ–¹æ³•å·²è¢«è¯æ˜æ˜¯æœ‰æ•ˆçš„ï¼Œä½†MLPå¦‚ä½•å¼¥åˆæ¨¡æ€é¸¿æ²Ÿçš„å†…åœ¨æœºåˆ¶ä»ç„¶çŸ¥ä¹‹ç”šå°‘ã€‚è™½ç„¶æœ‰ä¸€äº›ç ”ç©¶æ¢è®¨äº†LLMå¦‚ä½•å¤„ç†è½¬æ¢åçš„è§†è§‰ä»¤ç‰Œï¼Œä½†å¾ˆå°‘æœ‰ç ”ç©¶æ¢è®¨åŸºæœ¬çš„å¯¹é½æœºåˆ¶ã€‚æ­¤å¤–ï¼ŒMLPé€‚é…å™¨åœ¨åˆ‡æ¢LLMä¸»å¹²æ—¶éœ€è¦é‡æ–°è®­ç»ƒã€‚ä¸ºäº†è§£å†³è¿™äº›å±€é™æ€§ï¼Œæˆ‘ä»¬é¦–å…ˆè°ƒæŸ¥äº†MLPé€‚é…å™¨çš„å·¥ä½œåŸç†ï¼Œå¹¶å‘ç°å®ƒä»¬å­¦ä¹ å°†è§†è§‰åµŒå…¥é€æ­¥æŠ•å½±åˆ°ç”±ç›¸åº”çš„æ–‡æœ¬åµŒå…¥æ‰€è·¨è¶Šçš„å­ç©ºé—´ã€‚åŸºäºè¿™ä¸€è§è§£ï¼Œæˆ‘ä»¬æå‡ºäº†LangBridgeï¼Œè¿™æ˜¯ä¸€ç§æ–°å‹é€‚é…å™¨ï¼Œå®ƒèƒ½å¤Ÿå°†è§†è§‰ä»¤ç‰Œæ˜¾å¼æ˜ å°„åˆ°LLMè¯æ±‡åµŒå…¥çš„çº¿æ€§ç»„åˆã€‚è¿™ç§åˆ›æ–°çš„è®¾è®¡å®ç°äº†ä¸åŒLLMä¹‹é—´çš„é¢„è®­ç»ƒå…è´¹é€‚é…å™¨è½¬ç§»ï¼ŒåŒæ—¶ä¿æŒæ€§èƒ½ã€‚æˆ‘ä»¬çš„å®éªŒç»“æœè¡¨æ˜ï¼Œåœ¨Qwen2-0.5Bä¸Šé¢„è®­ç»ƒçš„LangBridgeé€‚é…å™¨å¯ä»¥ç›´æ¥åº”ç”¨äºè¾ƒå¤§çš„æ¨¡å‹ï¼Œå¦‚LLaMA3-8Bæˆ–Qwen2.5-14Bï¼ŒåŒæ—¶ä¿æŒç«äº‰åŠ›ã€‚æ€»çš„æ¥è¯´ï¼ŒLangBridgeé€šè¿‡ä»¥LLMè¯æ±‡åµŒå…¥ä¸ºåŸºç¡€å®ç°è§†è§‰è¯­è¨€å¯¹é½ï¼Œä»è€Œå®ç°äº†è§£é‡Šæ€§ï¼›è€Œå…¶å³æ’å³ç”¨è®¾è®¡ç¡®ä¿äº†åœ¨å¤šä¸ªLLMä¹‹é—´çš„é«˜æ•ˆé‡ç”¨ï¼Œå‡ ä¹æ²¡æœ‰ä»»ä½•æ€§èƒ½æŸå¤±ã€‚è¯·è®¿é—®æˆ‘ä»¬çš„é¡¹ç›®é¡µé¢<a target="_blank" rel="noopener" href="https://jiaqiliao77.github.io/LangBridge.github.io/%E4%BA%86%E8%A7%A3%E8%AF%A6%E6%83%85%E3%80%82">https://jiaqiliao77.github.io/LangBridge.github.io/äº†è§£è¯¦æƒ…ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.19404v2">PDF</a> The code and weights will be open-sourced. Project page:   <a target="_blank" rel="noopener" href="https://jiaqiliao77.github.io/LangBridge.github.io/">https://jiaqiliao77.github.io/LangBridge.github.io/</a></p>
<p><strong>Summary</strong></p>
<p>è¿‘å¹´æ¥ï¼Œå¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆLVLMsï¼‰åœ¨å¤šç§å¤æ‚çš„è§†è§‰è¯­è¨€ä»»åŠ¡ä¸Šå–å¾—äº†ä¸äººç±»ç›¸å½“çš„æ€§èƒ½ã€‚ä¸»æµæ¨¡å‹å¦‚LLaVAé€šå¸¸é‡‡ç”¨æµ…å±‚MLPè¿›è¡Œè§†è§‰è¯­è¨€å¯¹é½çš„ä¸¤é˜¶æ®µè®­ç»ƒï¼šå…ˆè¿›è¡Œè·¨æ¨¡æ€å¯¹é½çš„é¢„è®­ç»ƒï¼Œå†è¿›è¡ŒæŒ‡ä»¤è°ƒæ•´ã€‚å°½ç®¡è¿™ç§æ–¹æ³•æœ‰æ•ˆï¼Œä½†MLPå¦‚ä½•æ¡¥æ¥æ¨¡æ€å·®è·çš„åŸºç¡€æœºåˆ¶ä»çŸ¥ä¹‹ç”šå°‘ã€‚æœ¬æ–‡æ·±å…¥ç ”ç©¶äº†MLPé€‚é…å™¨çš„å·¥ä½œåŸç†ï¼Œå¹¶æå‡ºäº†LangBridgeï¼Œä¸€ç§æ˜¾å¼åœ°å°†è§†è§‰ä»¤ç‰Œæ˜ å°„åˆ°LLMè¯æ±‡åµŒå…¥çš„çº¿æ€§ç»„åˆçš„æ–°é€‚é…å™¨ã€‚è¿™ç§è®¾è®¡å®ç°äº†æ— éœ€é¢„è®­ç»ƒçš„é€‚é…å™¨åœ¨ä¸åŒLLMsä¹‹é—´çš„è¿ç§»ï¼ŒåŒæ—¶ä¿æŒäº†æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆLVLMsï¼‰åœ¨å¤šç§è§†è§‰è¯­è¨€ä»»åŠ¡ä¸Šè¾¾åˆ°äººç±»æ°´å¹³æ€§èƒ½ã€‚</li>
<li>ä¸»æµLVLMsé‡‡ç”¨æµ…å±‚MLPè¿›è¡Œè§†è§‰è¯­è¨€å¯¹é½ã€‚</li>
<li>MLPå¦‚ä½•æ¡¥æ¥æ¨¡æ€å·®è·çš„åŸºç¡€æœºåˆ¶å°šä¸æ¸…æ¥šã€‚</li>
<li>LangBridgeæ˜¯ä¸€ç§æ–°å‹çš„è§†è§‰è¯­è¨€é€‚é…å™¨ï¼Œå¯ä»¥æ˜¾å¼åœ°å°†è§†è§‰ä»¤ç‰Œæ˜ å°„åˆ°LLMè¯æ±‡åµŒå…¥ã€‚</li>
<li>LangBridgeå®ç°äº†ä¸åŒLLMsä¹‹é—´çš„æ— éœ€é¢„è®­ç»ƒçš„é€‚é…å™¨è¿ç§»ã€‚</li>
<li>LangBridgeé€šè¿‡å°†è§†è§‰è¡¨ç¤ºæ ¹æ¤äºLLMè¯æ±‡åµŒå…¥æ¥å®ç°å¯è§£é‡Šçš„è§†è§‰è¯­è¨€å¯¹é½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.19404">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-31dad24a42698b50b3352e181a59f59b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ed8aecdd83d44938e2cb894253d8d521.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-982fecb535e850c29bfcd74086ba0f71.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e1e73028dce61d7fcaa7b6d6462807a9.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="ST-VLM-Kinematic-Instruction-Tuning-for-Spatio-Temporal-Reasoning-in-Vision-Language-Models"><a href="#ST-VLM-Kinematic-Instruction-Tuning-for-Spatio-Temporal-Reasoning-in-Vision-Language-Models" class="headerlink" title="ST-VLM: Kinematic Instruction Tuning for Spatio-Temporal Reasoning in   Vision-Language Models"></a>ST-VLM: Kinematic Instruction Tuning for Spatio-Temporal Reasoning in   Vision-Language Models</h2><p><strong>Authors:Dohwan Ko, Sihyeon Kim, Yumin Suh, Vijay Kumar B. G, Minseo Yoon, Manmohan Chandraker, Hyunwoo J. Kim</strong></p>
<p>Spatio-temporal reasoning is essential in understanding real-world environments in various fields, eg, autonomous driving and sports analytics. Recent advances have improved the spatial reasoning ability of Vision-Language Models (VLMs) by introducing large-scale data, but these models still struggle to analyze kinematic elements like traveled distance and speed of moving objects. To bridge this gap, we construct a spatio-temporal reasoning dataset and benchmark involving kinematic instruction tuning, referred to as STKit and STKit-Bench. They consist of real-world videos with 3D annotations, detailing object motion dynamics: traveled distance, speed, movement direction, inter-object distance comparisons, and relative movement direction. To further scale such data construction to videos without 3D labels, we propose an automatic pipeline to generate pseudo-labels using 4D reconstruction in real-world scale. With our kinematic instruction tuning data for spatio-temporal reasoning, we present ST-VLM, a VLM enhanced for spatio-temporal reasoning, which exhibits outstanding performance on STKit-Bench. Furthermore, we show that ST-VLM generalizes robustly across diverse domains and tasks, outperforming baselines on other spatio-temporal benchmarks (eg, ActivityNet, TVQA+). Finally, by integrating learned spatio-temporal reasoning with existing abilities, ST-VLM enables complex multi-step reasoning. Project page: <a target="_blank" rel="noopener" href="https://ikodoh.github.io/ST-VLM">https://ikodoh.github.io/ST-VLM</a>. </p>
<blockquote>
<p>æ—¶ç©ºæ¨ç†åœ¨ç†è§£å„ä¸ªé¢†åŸŸçš„çœŸå®ä¸–ç•Œç¯å¢ƒï¼ˆå¦‚è‡ªåŠ¨é©¾é©¶å’Œè¿åŠ¨åˆ†æï¼‰ä¸­è‡³å…³é‡è¦ã€‚æœ€è¿‘çš„å‘å±•é€šè¿‡å¼•å…¥å¤§è§„æ¨¡æ•°æ®æé«˜äº†è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰çš„ç©ºé—´æ¨ç†èƒ½åŠ›ï¼Œä½†è¿™äº›æ¨¡å‹ä»ç„¶éš¾ä»¥åˆ†æè¿åŠ¨ç‰©ä½“çš„è·ç¦»å’Œé€Ÿåº¦ç­‰è¿åŠ¨å­¦å…ƒç´ ã€‚ä¸ºäº†å¼¥è¡¥è¿™ä¸€å·®è·ï¼Œæˆ‘ä»¬æ„å»ºäº†ä¸€ä¸ªæ¶‰åŠè¿åŠ¨å­¦æŒ‡ä»¤è°ƒæ•´çš„æ—¶ç©ºæ¨ç†æ•°æ®é›†å’ŒåŸºå‡†æµ‹è¯•ï¼Œç§°ä¸ºSTKitå’ŒSTKit-Benchã€‚å®ƒä»¬åŒ…å«å¸¦æœ‰3Dæ³¨é‡Šçš„çœŸå®ä¸–ç•Œè§†é¢‘ï¼Œè¯¦ç»†æè¿°äº†ç‰©ä½“è¿åŠ¨çš„åŠ¨åŠ›å­¦ï¼šè¡Œè¿›è·ç¦»ã€é€Ÿåº¦ã€è¿åŠ¨æ–¹å‘ã€ç‰©ä½“é—´è·ç¦»æ¯”è¾ƒå’Œç›¸å¯¹è¿åŠ¨æ–¹å‘ã€‚ä¸ºäº†å°†æ­¤ç±»æ•°æ®æ„å»ºè¿›ä¸€æ­¥æ‰©å±•åˆ°æ²¡æœ‰3Dæ ‡ç­¾çš„è§†é¢‘ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªä½¿ç”¨çœŸå®ä¸–ç•Œå°ºåº¦ä¸‹çš„å››ç»´é‡å»ºç”Ÿæˆä¼ªæ ‡ç­¾çš„è‡ªåŠ¨ç®¡é“ã€‚é€šè¿‡ä½¿ç”¨æˆ‘ä»¬çš„æ—¶ç©ºæ¨ç†è¿åŠ¨å­¦æŒ‡ä»¤è°ƒæ•´æ•°æ®ï¼Œæˆ‘ä»¬æ¨å‡ºäº†ST-VLMï¼Œè¿™æ˜¯ä¸€ä¸ªå¢å¼ºæ—¶ç©ºæ¨ç†çš„VLMï¼Œåœ¨STKit-Benchä¸Šè¡¨ç°å‡ºå“è¶Šçš„æ€§èƒ½ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å±•ç¤ºäº†ST-VLMåœ¨ä¸åŒé¢†åŸŸå’Œä»»åŠ¡ä¸­çš„ç¨³å¥æ³›åŒ–èƒ½åŠ›ï¼Œåœ¨å…¶ä»–æ—¶ç©ºåŸºå‡†æµ‹è¯•ï¼ˆä¾‹å¦‚ActivityNetã€TVQA+ï¼‰ä¸Šä¼˜äºåŸºçº¿ã€‚æœ€åï¼Œé€šè¿‡æ•´åˆä¹ å¾—çš„æ—¶ç©ºæ¨ç†ä¸ç°æœ‰èƒ½åŠ›ï¼ŒST-VLMèƒ½å¤Ÿå®ç°å¤æ‚çš„å¤šæ­¥éª¤æ¨ç†ã€‚é¡¹ç›®é¡µé¢ï¼š<a target="_blank" rel="noopener" href="https://ikodoh.github.io/ST-VLM%E3%80%82">https://ikodoh.github.io/ST-VLMã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.19355v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡å¼ºè°ƒäº†æ—¶ç©ºæ¨ç†åœ¨ç†è§£ç°å®ä¸–ç•Œç¯å¢ƒçš„é‡è¦æ€§ï¼Œç‰¹åˆ«æ˜¯åœ¨è‡ªåŠ¨é©¾é©¶å’Œä½“è‚²åˆ†æç­‰é¢†åŸŸã€‚ä¸ºæ”¹å–„è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰çš„ç©ºé—´æ¨ç†èƒ½åŠ›ï¼Œç ”ç©¶è€…å¼•å…¥äº†å¤§è§„æ¨¡æ•°æ®ã€‚ç„¶è€Œï¼Œè¿™äº›æ¨¡å‹åœ¨åˆ†æè¿åŠ¨ç‰©ä½“çš„è¿åŠ¨è·ç¦»å’Œé€Ÿåº¦ç­‰åŠ¨åŠ›å­¦å…ƒç´ æ—¶ä»æœ‰å›°éš¾ã€‚ä¸ºå¼¥è¡¥è¿™ä¸€å·®è·ï¼Œç ”ç©¶è€…æ„å»ºäº†åŒ…å«åŠ¨åŠ›å­¦æŒ‡ä»¤è°ƒæ•´çš„æ—¶ç©ºæ¨ç†æ•°æ®é›†å’ŒåŸºå‡†æµ‹è¯•ï¼Œå³STKitå’ŒSTKit-Benchã€‚æ­¤å¤–ï¼Œä»–ä»¬è¿˜æå‡ºäº†ä¸€ç§ä½¿ç”¨çœŸå®ä¸–ç•Œè§„æ¨¡çš„å››ç»´é‡å»ºè‡ªåŠ¨ç”Ÿæˆä¼ªæ ‡ç­¾çš„è‡ªåŠ¨ç®¡é“ã€‚é€šè¿‡å¼•å…¥æ—¶ç©ºæ¨ç†çš„åŠ¨åŠ›å­¦æŒ‡ä»¤è°ƒæ•´æ•°æ®ï¼Œç ”ç©¶è€…å±•ç¤ºäº†å¢å¼ºçš„è§†è§‰è¯­è¨€æ¨¡å‹ST-VLMçš„ä¼˜ç§€æ€§èƒ½ã€‚è¯¥æ¨¡å‹ä¸ä»…åœ¨STKit-Benchä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œè€Œä¸”åœ¨å…¶ä»–æ—¶ç©ºåŸºå‡†æµ‹è¯•ï¼ˆå¦‚ActivityNetã€TVQA+ï¼‰ä¸Šä¹Ÿèƒ½å¾ˆå¥½åœ°æ³›åŒ–ã€‚æœ€åï¼Œé€šè¿‡å°†å­¦ä¹ åˆ°çš„æ—¶ç©ºæ¨ç†ä¸ç°æœ‰èƒ½åŠ›ç›¸ç»“åˆï¼ŒST-VLMå®ç°äº†å¤æ‚çš„å¤šæ­¥éª¤æ¨ç†ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ—¶ç©ºæ¨ç†å¯¹äºç†è§£ç°å®ä¸–ç•Œçš„ç¯å¢ƒåœ¨å„ç§é¢†åŸŸéƒ½éå¸¸é‡è¦ï¼Œå¦‚è‡ªåŠ¨é©¾é©¶å’Œä½“è‚²åˆ†æã€‚</li>
<li>è™½ç„¶å¼•å…¥äº†å¤§è§„æ¨¡æ•°æ®ï¼Œä½†ç°æœ‰çš„è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰åœ¨åˆ†æè¿åŠ¨ç‰©ä½“çš„åŠ¨åŠ›å­¦ç‰¹å¾ï¼ˆå¦‚è·ç¦»å’Œé€Ÿåº¦ï¼‰æ—¶ä»æœ‰å›°éš¾ã€‚</li>
<li>ä¸ºæé«˜VLMsçš„æ—¶ç©ºæ¨ç†èƒ½åŠ›ï¼Œç ”ç©¶è€…æ„å»ºäº†STKitå’ŒSTKit-Benchæ•°æ®é›†å’ŒåŸºå‡†æµ‹è¯•ï¼Œå…¶ä¸­åŒ…å«å¸¦æœ‰3Dæ³¨é‡Šçš„çœŸå®ä¸–ç•Œè§†é¢‘ï¼Œè¯¦ç»†æè¿°äº†ç‰©ä½“çš„è¿åŠ¨åŠ¨æ€ã€‚</li>
<li>æå‡ºäº†ä¸€ç§ä½¿ç”¨çœŸå®ä¸–ç•Œè§„æ¨¡çš„å››ç»´é‡å»ºè‡ªåŠ¨ç”Ÿæˆä¼ªæ ‡ç­¾çš„è‡ªåŠ¨ç®¡é“æ¥æ‰©å±•æ­¤ç±»æ•°æ®çš„æ„å»ºã€‚</li>
<li>ST-VLMæ¨¡å‹å±•ç¤ºäº†å‡ºè‰²çš„æ—¶ç©ºæ¨ç†èƒ½åŠ›ï¼Œä¸ä»…åœ¨STKit-Benchä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œè€Œä¸”åœ¨å…¶ä»–æ—¶ç©ºåŸºå‡†æµ‹è¯•ä¸Šä¹Ÿæœ‰è‰¯å¥½çš„æ³›åŒ–æ€§èƒ½ã€‚</li>
<li>ST-VLMæ¨¡å‹ç»“åˆäº†å­¦ä¹ çš„æ—¶ç©ºæ¨ç†ä¸ç°æœ‰èƒ½åŠ›ï¼Œå®ç°äº†å¤æ‚çš„å¤šæ­¥éª¤æ¨ç†ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.19355">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-90e915a2c37858642805b4dd628ee749.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d8e0fc995603c14e9dbb59bec545fd16.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c084e9998c5bd21ee62e3b8ab498e4c2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3a83fee94008c4eb63eb5913ba6b6fdb.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8bf26d67f83df6c8c93a5c2dc429dcce.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-32a68729a67a41a12a06f038493170e3.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-2baa39b34006ef12edd50ab6a02d4a0d.jpg" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="On-the-Perception-Bottleneck-of-VLMs-for-Chart-Understanding"><a href="#On-the-Perception-Bottleneck-of-VLMs-for-Chart-Understanding" class="headerlink" title="On the Perception Bottleneck of VLMs for Chart Understanding"></a>On the Perception Bottleneck of VLMs for Chart Understanding</h2><p><strong>Authors:Junteng Liu, Weihao Zeng, Xiwen Zhang, Yijun Wang, Zifei Shan, Junxian He</strong></p>
<p>Chart understanding requires models to effectively analyze and reason about numerical data, textual elements, and complex visual components. Our observations reveal that the perception capabilities of existing large vision-language models (LVLMs) constitute a critical bottleneck in this process. In this study, we delve into this perception bottleneck by decomposing it into two components: the vision encoder bottleneck, where the visual representation may fail to encapsulate the correct information, and the extraction bottleneck, where the language model struggles to extract the necessary information from the provided visual representations. Through comprehensive experiments, we find that (1) the information embedded within visual representations is substantially richer than what is typically captured by linear extractors, such as the widely used retrieval accuracy metric; (2) While instruction tuning effectively enhances the extraction capability of LVLMs, the vision encoder remains a critical bottleneck, demanding focused attention and improvement. Therefore, we further enhance the visual encoder to mitigate the vision encoder bottleneck under a contrastive learning framework. Empirical results demonstrate that our approach significantly mitigates the perception bottleneck and improves the ability of LVLMs to comprehend charts. Code is publicly available at <a target="_blank" rel="noopener" href="https://github.com/hkust-nlp/Vision4Chart">https://github.com/hkust-nlp/Vision4Chart</a>. </p>
<blockquote>
<p>å›¾è¡¨ç†è§£éœ€è¦æ¨¡å‹å¯¹æ•°å€¼æ•°æ®ã€æ–‡æœ¬å…ƒç´ å’Œå¤æ‚è§†è§‰æˆåˆ†è¿›è¡Œæœ‰æ•ˆçš„åˆ†æå’Œæ¨ç†ã€‚æˆ‘ä»¬çš„è§‚å¯Ÿå‘ç°ï¼Œç°æœ‰å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆLVLMsï¼‰çš„æ„ŸçŸ¥èƒ½åŠ›æ„æˆäº†è¿™ä¸€è¿‡ç¨‹ä¸­çš„å…³é”®ç“¶é¢ˆã€‚åœ¨è¿™é¡¹ç ”ç©¶ä¸­ï¼Œæˆ‘ä»¬é€šè¿‡å°†å…¶åˆ†è§£ä¸ºä¸¤ä¸ªç»„æˆéƒ¨åˆ†æ¥æ·±å…¥ç ”ç©¶è¿™ä¸€æ„ŸçŸ¥ç“¶é¢ˆï¼šè§†è§‰ç¼–ç å™¨ç“¶é¢ˆï¼Œå…¶ä¸­è§†è§‰è¡¨ç¤ºå¯èƒ½æ— æ³•å°è£…æ­£ç¡®çš„ä¿¡æ¯ï¼›ä»¥åŠæå–ç“¶é¢ˆï¼Œå…¶ä¸­è¯­è¨€æ¨¡å‹éš¾ä»¥ä»æä¾›çš„è§†è§‰è¡¨ç¤ºä¸­æå–å¿…è¦çš„ä¿¡æ¯ã€‚é€šè¿‡ç»¼åˆå®éªŒï¼Œæˆ‘ä»¬å‘ç°ï¼ˆ1ï¼‰è§†è§‰è¡¨ç¤ºä¸­æ‰€åµŒå…¥çš„ä¿¡æ¯è¿œæ¯”çº¿æ€§æå–å™¨ï¼ˆå¦‚å¹¿æ³›ä½¿ç”¨çš„æ£€ç´¢å‡†ç¡®ç‡æŒ‡æ ‡ï¼‰æ‰€æ•è·çš„è¦ä¸°å¯Œï¼›ï¼ˆ2ï¼‰è™½ç„¶æŒ‡ä»¤è°ƒæ•´æœ‰æ•ˆåœ°å¢å¼ºäº†LVLMsçš„æå–èƒ½åŠ›ï¼Œä½†è§†è§‰ç¼–ç å™¨ä»ç„¶æ˜¯ä¸€ä¸ªå…³é”®çš„ç“¶é¢ˆï¼Œéœ€è¦é‡ç‚¹å…³æ³¨å’Œæ”¹è¿›ã€‚å› æ­¤ï¼Œæˆ‘ä»¬è¿›ä¸€æ­¥å¢å¼ºäº†è§†è§‰ç¼–ç å™¨ï¼Œä»¥åœ¨å¯¹æ¯”å­¦ä¹ æ¡†æ¶ä¸‹ç¼“è§£è§†è§‰ç¼–ç å™¨ç“¶é¢ˆã€‚ç»éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•æ˜¾è‘—ç¼“è§£äº†æ„ŸçŸ¥ç“¶é¢ˆï¼Œæé«˜äº†LVLMsç†è§£å›¾è¡¨çš„èƒ½åŠ›ã€‚ä»£ç å·²å…¬å¼€åœ¨<a target="_blank" rel="noopener" href="https://github.com/hkust-nlp/Vision4Chart%E3%80%82">https://github.com/hkust-nlp/Vision4Chartã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.18435v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ¢è®¨äº†å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆLVLMsï¼‰åœ¨å¤„ç†å›¾è¡¨ç†è§£æ—¶çš„æ„ŸçŸ¥ç“¶é¢ˆé—®é¢˜ã€‚ç ”ç©¶å°†æ„ŸçŸ¥ç“¶é¢ˆåˆ†è§£ä¸ºè§†è§‰ç¼–ç å™¨ç“¶é¢ˆå’Œä¿¡æ¯æå–ç“¶é¢ˆä¸¤éƒ¨åˆ†ã€‚ç ”ç©¶å‘ç°ï¼Œè§†è§‰è¡¨ç¤ºä¸­åŒ…å«çš„ä¿¡æ¯æ¯”çº¿æ€§æå–å™¨ï¼ˆå¦‚å¸¸ç”¨çš„æ£€ç´¢å‡†ç¡®ç‡æŒ‡æ ‡ï¼‰æ‰€æ•è·çš„è¦ä¸°å¯Œå¾—å¤šã€‚è™½ç„¶æŒ‡ä»¤å¾®è°ƒæé«˜äº†LVLMsçš„æå–èƒ½åŠ›ï¼Œä½†è§†è§‰ç¼–ç å™¨ä»æ˜¯å…³é”®ç“¶é¢ˆã€‚ä¸ºäº†ç¼“è§£è§†è§‰ç¼–ç å™¨ç“¶é¢ˆï¼Œç ”ç©¶é‡‡ç”¨å¯¹æ¯”å­¦ä¹ æ¡†æ¶å¢å¼ºäº†è§†è§‰ç¼–ç å™¨ã€‚å®è¯ç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•æ˜¾è‘—ç¼“è§£äº†æ„ŸçŸ¥ç“¶é¢ˆï¼Œæé«˜äº†LVLMsçš„å›¾è¡¨ç†è§£èƒ½åŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆLVLMsï¼‰åœ¨å›¾è¡¨ç†è§£ä¸Šå­˜åœ¨æ„ŸçŸ¥ç“¶é¢ˆã€‚</li>
<li>æ„ŸçŸ¥ç“¶é¢ˆå¯åˆ†è§£ä¸ºè§†è§‰ç¼–ç å™¨ç“¶é¢ˆå’Œä¿¡æ¯æå–ç“¶é¢ˆã€‚</li>
<li>è§†è§‰è¡¨ç¤ºçš„ä¿¡æ¯ä¸°å¯Œç¨‹åº¦è¶…è¿‡çº¿æ€§æå–å™¨çš„æ•è·èƒ½åŠ›ã€‚</li>
<li>æŒ‡ä»¤å¾®è°ƒèƒ½æé«˜LVLMsçš„æå–èƒ½åŠ›ï¼Œä½†è§†è§‰ç¼–ç å™¨ä»æ˜¯å…³é”®ç“¶é¢ˆã€‚</li>
<li>é‡‡ç”¨å¯¹æ¯”å­¦ä¹ æ¡†æ¶å¢å¼ºè§†è§‰ç¼–ç å™¨ï¼Œä»¥ç¼“è§£è§†è§‰ç¼–ç å™¨ç“¶é¢ˆã€‚</li>
<li>æ–¹æ³•æ˜¾è‘—ç¼“è§£äº†æ„ŸçŸ¥ç“¶é¢ˆã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.18435">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-067245d4a7665c70304e9a0c7c15c8da.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-44af51b11fa63a1eb20ce5ea5246515c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-63a7596c12fd7308cb9c1c5e95bc870e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-2bc82eda912ecb1ca48d328289b07f47.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1c4d962c16e96c7d3cab5fe875f9970c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-709fcb5d7cf493b5393161df9d10f8e3.jpg" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="Bridging-Writing-Manner-Gap-in-Visual-Instruction-Tuning-by-Creating-LLM-aligned-Instructions"><a href="#Bridging-Writing-Manner-Gap-in-Visual-Instruction-Tuning-by-Creating-LLM-aligned-Instructions" class="headerlink" title="Bridging Writing Manner Gap in Visual Instruction Tuning by Creating   LLM-aligned Instructions"></a>Bridging Writing Manner Gap in Visual Instruction Tuning by Creating   LLM-aligned Instructions</h2><p><strong>Authors:Dong Jing, Nanyi Fei, Zhiwu Lu</strong></p>
<p>In the realm of Large Multi-modal Models (LMMs), the instruction quality during the visual instruction tuning stage significantly influences the performance of modality alignment. In this paper, we assess the instruction quality from a unique perspective termed \textbf{Writing Manner}, which encompasses the selection of vocabulary, grammar and sentence structure to convey specific semantics. We argue that there exists a substantial writing manner gap between the visual instructions and the base Large Language Models (LLMs) within LMMs. This gap forces the pre-trained base LLMs to deviate from their original writing styles, leading to capability degradation of both base LLMs and LMMs. To bridge the writing manner gap while preserving the original semantics, we propose directly leveraging the base LLM to align the writing manner of soft-format visual instructions with that of the base LLM itself, resulting in novel LLM-aligned instructions. The manual writing manner evaluation results demonstrate that our approach successfully minimizes the writing manner gap. By utilizing LLM-aligned instructions, the baseline models LLaVA-7B and QwenVL demonstrate enhanced resistance to hallucinations and non-trivial comprehensive improvements across all $15$ visual and language benchmarks. </p>
<blockquote>
<p>åœ¨å¤§å‹å¤šæ¨¡æ€æ¨¡å‹ï¼ˆLMMsï¼‰é¢†åŸŸï¼Œè§†è§‰æŒ‡ä»¤è°ƒæ•´é˜¶æ®µçš„æŒ‡ä»¤è´¨é‡å¯¹æ¨¡æ€å¯¹é½æ€§èƒ½å…·æœ‰æ˜¾è‘—å½±å“ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬ä»ç‹¬ç‰¹çš„â€œå†™ä½œæ–¹å¼â€è§’åº¦è¯„ä¼°æŒ‡ä»¤è´¨é‡ï¼Œè¿™ç§æ–¹å¼æ¶µç›–äº†è¯æ±‡é€‰æ‹©ã€è¯­æ³•å’Œå¥å­ç»“æ„ï¼Œä»¥ä¼ è¾¾ç‰¹å®šçš„è¯­ä¹‰ã€‚æˆ‘ä»¬è®¤ä¸ºï¼Œåœ¨å¤§å‹å¤šæ¨¡æ€æ¨¡å‹ä¸­çš„è§†è§‰æŒ‡ä»¤å’ŒåŸºäºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ä¹‹é—´å­˜åœ¨æ˜¾è‘—çš„å†™ä½œæ–¹å¼å·®è·ã€‚è¿™ä¸€å·®è·è¿«ä½¿é¢„è®­ç»ƒçš„åŸºæ¨¡å‹åç¦»å…¶åŸå§‹å†™ä½œé£æ ¼ï¼Œå¯¼è‡´åŸºæ¨¡å‹å’Œå¤§å‹å¤šæ¨¡æ€æ¨¡å‹çš„èƒ½åŠ›ä¸‹é™ã€‚ä¸ºäº†ç¼©å°å†™ä½œæ–¹å¼çš„å·®è·åŒæ—¶ä¿ç•™åŸå§‹è¯­ä¹‰ï¼Œæˆ‘ä»¬æå‡ºç›´æ¥ä½¿ç”¨åŸºæ¨¡å‹å¯¹é½è½¯æ ¼å¼è§†è§‰æŒ‡ä»¤çš„å†™ä½œæ–¹å¼ï¼Œä½¿å…¶ä¸åŸºæ¨¡å‹æœ¬èº«çš„å†™ä½œæ–¹å¼ç›¸ç¬¦ï¼Œä»è€Œç”Ÿæˆæ–°é¢–çš„å¤§å‹è¯­è¨€æ¨¡å‹å¯¹é½æŒ‡ä»¤ã€‚æ‰‹åŠ¨å†™ä½œæ–¹å¼è¯„ä¼°ç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•æˆåŠŸç¼©å°äº†å†™ä½œæ–¹å¼å·®è·ã€‚é€šè¿‡ä½¿ç”¨å¤§å‹è¯­è¨€æ¨¡å‹å¯¹é½æŒ‡ä»¤ï¼ŒåŸºå‡†æ¨¡å‹LLaVA-7Bå’ŒQwenVLè¡¨ç°å‡ºæ›´å¼ºçš„æŠ—å¹»è§‰èƒ½åŠ›ï¼Œåœ¨æ‰€æœ‰15ä¸ªè§†è§‰å’Œè¯­è¨€åŸºå‡†æµ‹è¯•ä¸­éƒ½æœ‰æ˜¾è‘—çš„æ”¹è¿›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.18320v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ¢è®¨äº†å¤§å‹å¤šæ¨¡æ€æ¨¡å‹ï¼ˆLMMsï¼‰ä¸­è§†è§‰æŒ‡ä»¤è°ƒæ•´é˜¶æ®µæŒ‡ä»¤è´¨é‡å¯¹æ¨¡æ€å¯¹é½æ€§èƒ½çš„å½±å“ã€‚æ–‡ç« ä»å†™ä½œæ–¹å¼è¿™ä¸€ç‹¬ç‰¹è§†è§’è¯„ä¼°æŒ‡ä»¤è´¨é‡ï¼ŒåŒ…æ‹¬è¯æ±‡é€‰æ‹©ã€è¯­æ³•å’Œå¥å­ç»“æ„ï¼Œä»¥ä¼ è¾¾ç‰¹å®šè¯­ä¹‰ã€‚ç ”ç©¶å‘ç°ï¼Œè§†è§‰æŒ‡ä»¤ä¸å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ä¹‹é—´å­˜åœ¨æ˜æ˜¾çš„å†™ä½œæ–¹å¼å·®è·ï¼Œå¯¼è‡´é¢„è®­ç»ƒLLMsåç¦»å…¶åŸå§‹å†™ä½œé£æ ¼ï¼Œè¿›è€Œå½±å“LMMsçš„æ€§èƒ½ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºåˆ©ç”¨åŸºç¡€LLMå¯¹é½è½¯æ ¼å¼è§†è§‰æŒ‡ä»¤çš„å†™ä½œæ–¹å¼ï¼Œå½¢æˆä¸åŸºç¡€LLMç›¸ç¬¦çš„LLMå¯¹é½æŒ‡ä»¤ã€‚è¯„ä¼°ç»“æœæ˜¾ç¤ºï¼Œè¯¥æ–¹æ³•æˆåŠŸç¼©å°äº†å†™ä½œæ–¹å¼å·®è·ï¼Œä½¿ç”¨LLMå¯¹é½æŒ‡ä»¤çš„åŸºå‡†æ¨¡å‹LLaVA-7Bå’ŒQwenVLè¡¨ç°å‡ºæ›´å¼ºçš„æŠ—è™šæ„èƒ½åŠ›ï¼Œå¹¶åœ¨æ‰€æœ‰15ä¸ªè§†è§‰å’Œè¯­è¨€åŸºå‡†æµ‹è¯•ä¸­å–å¾—äº†æ˜¾è‘—çš„æ”¹è¿›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è§†è§‰æŒ‡ä»¤è°ƒæ•´é˜¶æ®µçš„æŒ‡ä»¤è´¨é‡å¯¹å¤šæ¨¡æ€æ¨¡å‹ï¼ˆLMMsï¼‰çš„æ¨¡æ€å¯¹é½æ€§èƒ½æœ‰é‡è¦å½±å“ã€‚</li>
<li>æ–‡ç« é¦–æ¬¡ä»å†™ä½œæ–¹å¼çš„è§’åº¦è¯„ä¼°æŒ‡ä»¤è´¨é‡ï¼ŒåŒ…æ‹¬è¯æ±‡é€‰æ‹©ã€è¯­æ³•å’Œå¥å­ç»“æ„ã€‚</li>
<li>è§†è§‰æŒ‡ä»¤ä¸å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ä¹‹é—´å­˜åœ¨æ˜æ˜¾çš„å†™ä½œæ–¹å¼å·®è·ï¼Œå½±å“æ¨¡å‹æ€§èƒ½ã€‚</li>
<li>æå‡ºåˆ©ç”¨åŸºç¡€LLMå¯¹é½è§†è§‰æŒ‡ä»¤çš„å†™ä½œæ–¹å¼ï¼Œå½¢æˆLLMå¯¹é½æŒ‡ä»¤ã€‚</li>
<li>LLMå¯¹é½æŒ‡ä»¤èƒ½å¤ŸæˆåŠŸç¼©å°å†™ä½œæ–¹å¼å·®è·ã€‚</li>
<li>ä½¿ç”¨LLMå¯¹é½æŒ‡ä»¤çš„æ¨¡å‹è¡¨ç°å‡ºæ›´å¼ºçš„æŠ—è™šæ„èƒ½åŠ›å’Œç»¼åˆæ”¹è¿›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.18320">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-2139afb2de8357071bf084c30e179302.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-55c5129994bff280c1b7bd1898833983.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-34e615a8fb63a9b54a38f5a6e9226e5a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2f7ab5ed39eab851b009c0c65babb14b.jpg" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1><h2 id="Decoupling-Angles-and-Strength-in-Low-rank-Adaptation"><a href="#Decoupling-Angles-and-Strength-in-Low-rank-Adaptation" class="headerlink" title="Decoupling Angles and Strength in Low-rank Adaptation"></a>Decoupling Angles and Strength in Low-rank Adaptation</h2><p><strong>Authors:Massimo Bini, Leander Girrbach, Zeynep Akata</strong></p>
<p>Parameter-Efficient FineTuning (PEFT) methods have recently gained significant popularity thanks to the widespread availability of large-scale pretrained models. These methods allow for quick adaptation to downstream tasks with minimal computational cost. However, popular finetuning methods such as LoRA exhibit limited robustness when it comes to hyperparameter choices or extended training regimes, preventing optimal out-of-the-box performance. In contrast, bounded approaches, such as ETHER, provide greater robustness but are limited to extremely low-rank adaptations and fixed-strength transformations, reducing their adaptation expressive power. In this work, we propose Decoupled Low-rank Adaptation (DeLoRA), a novel finetuning method that normalizes and scales learnable low-rank matrices. By bounding the distance of the transformation, DeLoRA effectively decouples the angular learning from the adaptation strength, enhancing robustness without compromising performance. Through evaluations on subject-driven image generation, natural language understanding, and instruction tuning, we show that DeLoRA matches or surpasses performance of competing PEFT methods, while exhibiting stronger robustness. Code is available at <a target="_blank" rel="noopener" href="https://github.com/ExplainableML/DeLoRA">https://github.com/ExplainableML/DeLoRA</a>. </p>
<blockquote>
<p>å‚æ•°é«˜æ•ˆå¾®è°ƒï¼ˆPEFTï¼‰æ–¹æ³•ç”±äºå¤§è§„æ¨¡é¢„è®­ç»ƒæ¨¡å‹çš„å¹¿æ³›åº”ç”¨è€Œè¿‘æœŸå¤‡å—ç©ç›®ã€‚è¿™äº›æ–¹æ³•èƒ½å¤Ÿä»¥æä½çš„è®¡ç®—æˆæœ¬å¿«é€Ÿé€‚åº”ä¸‹æ¸¸ä»»åŠ¡ã€‚ç„¶è€Œï¼Œæµè¡Œçš„å¾®è°ƒæ–¹æ³•ï¼Œå¦‚LoRAï¼Œåœ¨è¶…å‚æ•°é€‰æ‹©æˆ–æ‰©å±•è®­ç»ƒæ–¹æ¡ˆæ–¹é¢è¡¨ç°å‡ºæœ‰é™çš„ç¨³å¥æ€§ï¼Œæ— æ³•å®ç°æœ€ä¼˜çš„å³æ’å³ç”¨æ€§èƒ½ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼Œæœ‰ç•Œæ–¹æ³•ï¼ˆå¦‚ETHERï¼‰æä¾›äº†æ›´å¤§çš„ç¨³å¥æ€§ï¼Œä½†ä»…é™äºæä½é˜¶é€‚åº”å’Œå›ºå®šå¼ºåº¦è½¬æ¢ï¼Œä»è€Œé™ä½äº†å…¶é€‚åº”è¡¨è¾¾èƒ½åŠ›ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†â€œè§£è€¦ä½é˜¶é€‚åº”â€ï¼ˆDeLoRAï¼‰æ–¹æ³•ï¼Œè¿™æ˜¯ä¸€ç§æ–°å‹å¾®è°ƒæ–¹æ³•ï¼Œèƒ½å¤Ÿå½’ä¸€åŒ–å’Œç¼©æ”¾å¯å­¦ä¹ çš„ä½é˜¶çŸ©é˜µã€‚é€šè¿‡æ§åˆ¶è½¬æ¢çš„è·ç¦»ï¼ŒDeLoRAæœ‰æ•ˆåœ°å°†è§’åº¦å­¦ä¹ ä¸é€‚åº”å¼ºåº¦è§£è€¦ï¼Œåœ¨ä¸æŸå®³æ€§èƒ½çš„æƒ…å†µä¸‹æé«˜äº†ç¨³å¥æ€§ã€‚é€šè¿‡å¯¹ä¸»é¢˜é©±åŠ¨å›¾åƒç”Ÿæˆã€è‡ªç„¶è¯­è¨€ç†è§£å’ŒæŒ‡ä»¤è°ƒæ•´è¿›è¡Œè¯„ä¼°ï¼Œæˆ‘ä»¬è¯æ˜äº†DeLoRAåœ¨åŒ¹é…æˆ–è¶…è¶Šå…¶ä»–PEFTæ–¹æ³•æ€§èƒ½çš„åŒæ—¶ï¼Œè¡¨ç°å‡ºæ›´å¼ºçš„ç¨³å¥æ€§ã€‚ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/ExplainableML/DeLoRA%E4%B8%AD%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/ExplainableML/DeLoRAä¸­æ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.18225v1">PDF</a> ICLR 2025</p>
<p><strong>Summary</strong></p>
<p>PEFTæ–¹æ³•å› å…¶èƒ½è¿…é€Ÿé€‚åº”ä¸‹æ¸¸ä»»åŠ¡ä¸”è®¡ç®—æˆæœ¬ä½è€Œå—åˆ°å¹¿æ³›å…³æ³¨ã€‚ç„¶è€Œï¼Œç°æœ‰æ–¹æ³•å¦‚LoRAåœ¨è¶…å‚æ•°é€‰æ‹©ä¸é•¿æœŸè®­ç»ƒæ–¹é¢çš„ç¨³å¥æ€§æœ‰é™ã€‚ç›¸åï¼Œå¦‚ETHERçš„ç•Œé™æ–¹æ³•è™½ç„¶ç¨³å¥ï¼Œä½†é€‚åº”è¡¨è¾¾èƒ½åŠ›å—é™ã€‚æœ¬æ–‡æå‡ºä¸€ç§æ–°çš„å¾®è°ƒæ–¹æ³•â€”â€”è§£è€¦ä½ç§©é€‚åº”ï¼ˆDeLoRAï¼‰ï¼Œå®ƒé€šè¿‡è§„èŒƒå¹¶ç¼©æ”¾å¯å­¦ä¹ çš„ä½ç§©çŸ©é˜µæ¥å¢å¼ºç¨³å¥æ€§ï¼ŒåŒæ—¶ä¸æŸå¤±æ€§èƒ½ã€‚åœ¨ä¸»é¢˜é©±åŠ¨å›¾åƒç”Ÿæˆã€è‡ªç„¶è¯­è¨€ç†è§£å’ŒæŒ‡ä»¤è°ƒæ•´ä¸Šçš„è¯„ä¼°è¡¨æ˜ï¼ŒDeLoRAçš„ç¨³å¥æ€§ä¸æ€§èƒ½ä¼˜äºç«äº‰å¯¹æ‰‹çš„PEFTæ–¹æ³•ã€‚ä»£ç å¯åœ¨ExplainableML&#x2F;DeLoRAè·å–ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>PEFTæ–¹æ³•å› å…¶å¿«é€Ÿé€‚åº”æ€§å’Œä½è®¡ç®—æˆæœ¬è€Œå—åˆ°å…³æ³¨ã€‚</li>
<li>ç°æœ‰æ–¹æ³•å¦‚LoRAåœ¨è¶…å‚æ•°é€‰æ‹©å’Œé•¿æœŸè®­ç»ƒæ–¹é¢çš„ç¨³å¥æ€§æœ‰é™ã€‚</li>
<li>ETHERç­‰ç•Œé™æ–¹æ³•è™½ç„¶ç¨³å¥ä½†é€‚åº”è¡¨è¾¾èƒ½åŠ›å—é™ã€‚</li>
<li>DeLoRAæ˜¯ä¸€ç§æ–°çš„å¾®è°ƒæ–¹æ³•ï¼Œé€šè¿‡è§„èŒƒå¹¶ç¼©æ”¾ä½ç§©çŸ©é˜µæ¥å¢å¼ºç¨³å¥æ€§ã€‚</li>
<li>DeLoRAé€šè¿‡è§£è€¦è§’åº¦å­¦ä¹ ä¸é€‚åº”å¼ºåº¦æ¥æé«˜æ€§èƒ½ã€‚</li>
<li>DeLoRAåœ¨ä¸»é¢˜é©±åŠ¨å›¾åƒç”Ÿæˆã€è‡ªç„¶è¯­è¨€ç†è§£å’ŒæŒ‡ä»¤è°ƒæ•´ä¸Šçš„æ€§èƒ½ä¼˜äºå…¶ä»–PEFTæ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.18225">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-ca4b5d9a186d498416b14077dd8b1fa2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2e23bb1b749313ecfbed9ea9ec5009be.jpg" align="middle">
</details>


<h1 id="-19"><a href="#-19" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-03-28/LLM/">https://kedreamix.github.io/Talk2Paper/Paper/2025-03-28/LLM/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/LLM/">
                                    <span class="chip bg-color">LLM</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-03-28/Agent/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-ab30b0eba029d65e0ca1119bd6e746ce.jpg" class="responsive-img" alt="Agent">
                        
                        <span class="card-title">Agent</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Agent æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-03-28  Feature4X Bridging Any Monocular Video to 4D Agentic AI with Versatile   Gaussian Feature Fields
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-03-28
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Agent/" class="post-category">
                                    Agent
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Agent/">
                        <span class="chip bg-color">Agent</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-03-28/R1_Reasoning/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-f5e22e528aee49d30687ed5bbf57d32d.jpg" class="responsive-img" alt="R1_Reasoning">
                        
                        <span class="card-title">R1_Reasoning</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            R1_Reasoning æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-03-28  Understanding R1-Zero-Like Training A Critical Perspective
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-03-28
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/R1-Reasoning/" class="post-category">
                                    R1_Reasoning
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/R1-Reasoning/">
                        <span class="chip bg-color">R1_Reasoning</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">18723.3k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
