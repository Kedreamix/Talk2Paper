<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="R1_Reasoning">
    <meta name="description" content="R1_Reasoning æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-03-28  Understanding R1-Zero-Like Training A Critical Perspective">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>R1_Reasoning | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pic1.zhimg.com/v2-f5e22e528aee49d30687ed5bbf57d32d.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">R1_Reasoning</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/R1-Reasoning/">
                                <span class="chip bg-color">R1_Reasoning</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/R1-Reasoning/" class="post-category">
                                R1_Reasoning
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-03-28
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-05-14
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    20.1k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    82 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-03-28-æ›´æ–°"><a href="#2025-03-28-æ›´æ–°" class="headerlink" title="2025-03-28 æ›´æ–°"></a>2025-03-28 æ›´æ–°</h1><h2 id="Understanding-R1-Zero-Like-Training-A-Critical-Perspective"><a href="#Understanding-R1-Zero-Like-Training-A-Critical-Perspective" class="headerlink" title="Understanding R1-Zero-Like Training: A Critical Perspective"></a>Understanding R1-Zero-Like Training: A Critical Perspective</h2><p><strong>Authors:Zichen Liu, Changyu Chen, Wenjun Li, Penghui Qi, Tianyu Pang, Chao Du, Wee Sun Lee, Min Lin</strong></p>
<p>DeepSeek-R1-Zero has shown that reinforcement learning (RL) at scale can directly enhance the reasoning capabilities of LLMs without supervised fine-tuning. In this work, we critically examine R1-Zero-like training by analyzing its two core components: base models and RL. We investigate a wide range of base models, including DeepSeek-V3-Base, to understand how pretraining characteristics influence RL performance. Our analysis reveals that DeepSeek-V3-Base already exhibit â€˜â€™Aha momentâ€™â€™, while Qwen2.5 base models demonstrate strong reasoning capabilities even without prompt templates, suggesting potential pretraining biases. Additionally, we identify an optimization bias in Group Relative Policy Optimization (GRPO), which artificially increases response length (especially for incorrect outputs) during training. To address this, we introduce Dr. GRPO, an unbiased optimization method that improves token efficiency while maintaining reasoning performance. Leveraging these insights, we present a minimalist R1-Zero recipe that achieves 43.3% accuracy on AIME 2024 with a 7B base model, establishing a new state-of-the-art. Our code is available at <a target="_blank" rel="noopener" href="https://github.com/sail-sg/understand-r1-zero">https://github.com/sail-sg/understand-r1-zero</a>. </p>
<blockquote>
<p>DeepSeek-R1-Zeroçš„ç ”ç©¶è¡¨æ˜ï¼Œå¤§è§„æ¨¡å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰å¯ä»¥ç›´æ¥æå‡å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ¨ç†èƒ½åŠ›ï¼Œè€Œæ— éœ€ç›‘ç£å¾®è°ƒã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬é€šè¿‡å¯¹R1-Zeroç±»ä¼¼çš„è®­ç»ƒæ–¹å¼è¿›è¡Œåˆ†æï¼Œé‡ç‚¹ç ”ç©¶å…¶ä¸¤ä¸ªæ ¸å¿ƒç»„ä»¶ï¼šåŸºç¡€æ¨¡å‹å’ŒRLã€‚æˆ‘ä»¬è°ƒæŸ¥äº†ä¸€ç³»åˆ—åŸºç¡€æ¨¡å‹ï¼ŒåŒ…æ‹¬DeepSeek-V3-Baseï¼Œä»¥äº†è§£é¢„è®­ç»ƒç‰¹æ€§å¦‚ä½•å½±å“RLæ€§èƒ½ã€‚æˆ‘ä»¬çš„åˆ†æå‘ç°ï¼ŒDeepSeek-V3-Baseå·²ç»å±•ç°å‡ºâ€œé¡¿æ‚Ÿæ—¶åˆ»â€ï¼Œè€ŒQwen2.5åŸºç¡€æ¨¡å‹å³ä½¿åœ¨æ²¡æœ‰ä»»ä½•æç¤ºæ¨¡æ¿çš„æƒ…å†µä¸‹ä¹Ÿè¡¨ç°å‡ºå¼ºå¤§çš„æ¨ç†èƒ½åŠ›ï¼Œè¿™è¡¨æ˜å¯èƒ½å­˜åœ¨é¢„è®­ç»ƒåè§ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å‘ç°ç¾¤ä½“ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆGRPOï¼‰ä¸­å­˜åœ¨ä¼˜åŒ–åè§ï¼Œè¿™åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ä¼šäººä¸ºå¢åŠ å“åº”é•¿åº¦ï¼ˆå°¤å…¶æ˜¯é”™è¯¯è¾“å‡ºï¼‰ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†Dr. GRPOè¿™ä¸€æ— åä¼˜åŒ–æ–¹æ³•ï¼Œå®ƒåœ¨æé«˜ä»¤ç‰Œæ•ˆç‡çš„åŒæ—¶ä¿æŒæ¨ç†æ€§èƒ½ã€‚åŸºäºè¿™äº›è§è§£ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªæç®€çš„R1-Zeroé…æ–¹ï¼Œåœ¨AIME 2024ä¸Šè¾¾åˆ°äº†43.3%çš„å‡†ç¡®ç‡ï¼Œä½¿ç”¨7BåŸºç¡€æ¨¡å‹ï¼Œåˆ›é€ äº†æ–°çš„ä¸–ç•Œçºªå½•ã€‚æˆ‘ä»¬çš„ä»£ç ä½äº<a target="_blank" rel="noopener" href="https://github.com/sail-sg/understand-r1-zero%E3%80%82">https://github.com/sail-sg/understand-r1-zeroã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.20783v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰åœ¨å¤§è§„æ¨¡åº”ç”¨æ—¶ï¼Œèƒ½å¤Ÿç›´æ¥æå‡å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ¨ç†èƒ½åŠ›ï¼Œè€Œæ— éœ€ç›‘ç£å¾®è°ƒã€‚æœ¬ç ”ç©¶æ·±å…¥åˆ†æäº†R1-Zeroç±»ä¼¼çš„è®­ç»ƒæ–¹å¼ï¼Œé‡ç‚¹ç ”ç©¶å…¶åŸºç¡€æ¨¡å‹å’ŒRLä¸¤ä¸ªæ ¸å¿ƒç»„ä»¶ã€‚ç ”ç©¶å‘ç°ï¼Œä¸åŒçš„é¢„è®­ç»ƒæ¨¡å‹ç‰¹æ€§å¯¹RLæ€§èƒ½äº§ç”Ÿå½±å“ã€‚å…¶ä¸­ï¼ŒDeepSeek-V3-Baseå·²è¡¨ç°å‡ºâ€œå•Šå“ˆæ—¶åˆ»â€ï¼Œè€ŒQwen2.5åŸºç¡€æ¨¡å‹å³ä½¿åœ¨æ— éœ€æç¤ºæ¨¡æ¿çš„æƒ…å†µä¸‹ä¹Ÿå±•ç°å‡ºå¼ºå¤§çš„æ¨ç†èƒ½åŠ›ï¼Œè¿™å¯èƒ½ä¸å…¶é¢„è®­ç»ƒåè§æœ‰å…³ã€‚åŒæ—¶ï¼Œç ”ç©¶å‘ç°Group Relative Policy Optimizationï¼ˆGRPOï¼‰å­˜åœ¨ä¼˜åŒ–åè§ï¼Œä¼šäººä¸ºå¢åŠ å“åº”é•¿åº¦ï¼ˆå°¤å…¶æ˜¯é”™è¯¯è¾“å‡ºï¼‰ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œç ”ç©¶å›¢é˜Ÿæå‡ºäº†Dr. GRPOè¿™ä¸€æ— åè§ä¼˜åŒ–æ–¹æ³•ï¼Œæ—¢æé«˜äº†æ ‡è®°æ•ˆç‡åˆä¿æŒäº†æ¨ç†æ€§èƒ½ã€‚åŸºäºè¿™äº›è§è§£ï¼Œç ”ç©¶å›¢é˜Ÿä½¿ç”¨ç®€çº¦çš„R1-Zeroé…æ–¹åœ¨AIME 2024ä¸Šå®ç°äº†43.3%çš„å‡†ç¡®ç‡ï¼Œä½¿ç”¨7BåŸºç¡€æ¨¡å‹åˆ›é€ äº†æ–°çš„æŠ€æœ¯çºªå½•ã€‚ç›¸å…³ä»£ç å¯é€šè¿‡é“¾æ¥<a target="_blank" rel="noopener" href="https://github.com/sail-sg/understand-r1-zero%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/sail-sg/understand-r1-zeroè·å–ã€‚</a></p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¼ºåŒ–å­¦ä¹ åœ¨å¤§è§„æ¨¡åº”ç”¨æ—¶å¯ä»¥ç›´æ¥æå‡å¤§å‹è¯­è¨€æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ã€‚</li>
<li>R1-Zeroè®­ç»ƒæ–¹å¼çš„æ·±å…¥åˆ†ææ˜¾ç¤ºå…¶å…³æ³¨åŸºç¡€æ¨¡å‹å’ŒRLä¸¤ä¸ªæ ¸å¿ƒç»„ä»¶ã€‚</li>
<li>ä¸åŒé¢„è®­ç»ƒæ¨¡å‹ç‰¹æ€§å¯¹RLæ€§èƒ½äº§ç”Ÿå½±å“ï¼Œä¾‹å¦‚DeepSeek-V3-Baseè¡¨ç°å‡ºâ€œå•Šå“ˆæ—¶åˆ»â€ã€‚</li>
<li>Qwen2.5åŸºç¡€æ¨¡å‹åœ¨æ— éœ€æç¤ºæ¨¡æ¿çš„æƒ…å†µä¸‹å±•ç°å‡ºå¼ºå¤§çš„æ¨ç†èƒ½åŠ›ï¼Œå¯èƒ½ä¸é¢„è®­ç»ƒåè§æœ‰å…³ã€‚</li>
<li>Group Relative Policy Optimizationå­˜åœ¨ä¼˜åŒ–åè§ï¼Œå¯èƒ½äººä¸ºå¢åŠ å“åº”é•¿åº¦ã€‚</li>
<li>æå‡ºäº†Dr. GRPOè¿™ä¸€æ— åè§ä¼˜åŒ–æ–¹æ³•ï¼Œä»¥æé«˜æ ‡è®°æ•ˆç‡å’Œä¿æŒæ¨ç†æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.20783">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-247f45cc737b5b22dd85ecd131a87e76.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-afabb46b794067c9e2f0c9afac623e9c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-577ebbbfd490f1372f0e5aec0bfa41cd.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-0d35f7fe5ddcda5326c5541fb4fc22fe.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Reason-RFT-Reinforcement-Fine-Tuning-for-Visual-Reasoning"><a href="#Reason-RFT-Reinforcement-Fine-Tuning-for-Visual-Reasoning" class="headerlink" title="Reason-RFT: Reinforcement Fine-Tuning for Visual Reasoning"></a>Reason-RFT: Reinforcement Fine-Tuning for Visual Reasoning</h2><p><strong>Authors:Huajie Tan, Yuheng Ji, Xiaoshuai Hao, Minglan Lin, Pengwei Wang, Zhongyuan Wang, Shanghang Zhang</strong></p>
<p>Visual reasoning abilities play a crucial role in understanding complex multimodal data, advancing both domain-specific applications and artificial general intelligence (AGI). Existing methods improve VLM reasoning via Chain-of-Thought (CoT) supervised fine-tuning, using meticulously annotated training data to enhance visual reasoning capabilities. However, this training paradigm may lead to overfitting and cognitive rigidity, restricting the modelâ€™s ability to transfer visual reasoning skills across domains and limiting its real-world applicability. To address these limitations, we propose Reason-RFT, a novel reinforcement fine-tuning framework that significantly enhances generalization capabilities in visual reasoning tasks. Reason-RFT introduces a two-phase training framework for visual reasoning: (1) Supervised Fine-Tuning (SFT) with curated Chain-of-Thought (CoT) data activates the reasoning potential of Vision-Language Models (VLMs), followed by (2) Group Relative Policy Optimization (GRPO)-based reinforcement learning that generates multiple reasoning-response pairs, significantly enhancing generalization in visual reasoning tasks. To evaluate Reason-RFTâ€™s visual reasoning capabilities, we reconstructed a comprehensive dataset spanning visual counting, structure perception, and spatial transformation.cExperimental results demonstrate Reasoning-RFTâ€™s three key advantages: (1) Performance Enhancement: achieving state-of-the-art results across multiple tasks, outperforming most mainstream open-source and proprietary models; (2) Generalization Superiority: consistently maintaining robust performance across diverse tasks and domains, outperforming alternative training paradigms; (3) Data Efficiency: excelling in few-shot learning scenarios while surpassing full-dataset SFT baselines. </p>
<blockquote>
<p>è§†è§‰æ¨ç†èƒ½åŠ›åœ¨ç†è§£å¤æ‚çš„å¤šæ¨¡å¼æ•°æ®ã€æ¨åŠ¨ç‰¹å®šé¢†åŸŸåº”ç”¨å’Œäººå·¥é€šç”¨æ™ºèƒ½ï¼ˆAGIï¼‰å‘å±•æ–¹é¢å‘æŒ¥ç€è‡³å…³é‡è¦çš„ä½œç”¨ã€‚ç°æœ‰æ–¹æ³•é€šè¿‡æ€ç»´é“¾ï¼ˆCoTï¼‰ç›‘ç£å¾®è°ƒï¼ˆSupervised Fine-Tuning, SFTï¼‰æ”¹è¿›VLMæ¨ç†ï¼Œä½¿ç”¨ç²¾å¿ƒæ ‡æ³¨çš„è®­ç»ƒæ•°æ®ä»¥å¢å¼ºè§†è§‰æ¨ç†èƒ½åŠ›ã€‚ç„¶è€Œï¼Œè¿™ç§è®­ç»ƒæ¨¡å¼å¯èƒ½ä¼šå¯¼è‡´è¿‡æ‹Ÿåˆå’Œè®¤çŸ¥åƒµåŒ–ï¼Œé™åˆ¶æ¨¡å‹åœ¨ä¸åŒé¢†åŸŸè½¬ç§»è§†è§‰æ¨ç†æŠ€èƒ½çš„èƒ½åŠ›ï¼Œå¹¶é™åˆ¶å…¶åœ¨ç°å®ä¸–ç•Œä¸­çš„åº”ç”¨ã€‚ä¸ºäº†è§£å†³è¿™äº›å±€é™æ€§ï¼Œæˆ‘ä»¬æå‡ºäº†Reason-RFTï¼Œè¿™æ˜¯ä¸€ç§æ–°å‹å¼ºåŒ–å¾®è°ƒæ¡†æ¶ï¼Œèƒ½æ˜¾è‘—å¢å¼ºè§†è§‰æ¨ç†ä»»åŠ¡çš„æ³›åŒ–èƒ½åŠ›ã€‚Reason-RFTå¼•å…¥äº†è§†è§‰æ¨ç†çš„ä¸¤é˜¶æ®µè®­ç»ƒæ¡†æ¶ï¼šé¦–å…ˆä½¿ç”¨ç²¾é€‰çš„æ€ç»´é“¾ï¼ˆCoTï¼‰æ•°æ®è¿›è¡Œç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰ï¼Œæ¿€æ´»è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰çš„æ¨ç†æ½œåŠ›ï¼Œç„¶ååŸºäºç¾¤ä½“ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆGRPOï¼‰çš„å¼ºåŒ–å­¦ä¹ ç”Ÿæˆå¤šä¸ªæ¨ç†åº”ç­”å¯¹ï¼Œæ˜¾è‘—å¢å¼ºè§†è§‰æ¨ç†ä»»åŠ¡çš„æ³›åŒ–èƒ½åŠ›ã€‚ä¸ºäº†è¯„ä¼°Reason-RFTçš„è§†è§‰æ¨ç†èƒ½åŠ›ï¼Œæˆ‘ä»¬é‡æ–°æ„å»ºäº†ä¸€ä¸ªç»¼åˆæ•°æ®é›†ï¼Œæ¶µç›–è§†è§‰è®¡æ•°ã€ç»“æ„æ„ŸçŸ¥å’Œç©ºé—´è½¬æ¢ã€‚å®éªŒç»“æœè¯æ˜äº†Reasoning-RFTçš„ä¸‰ä¸ªå…³é”®ä¼˜åŠ¿ï¼šï¼ˆ1ï¼‰æ€§èƒ½æå‡ï¼šåœ¨å¤šä¸ªä»»åŠ¡ä¸Šå®ç°æœ€æ–°ç»“æœï¼Œä¼˜äºå¤§å¤šæ•°ä¸»æµå¼€æºå’Œä¸“æœ‰æ¨¡å‹ï¼›ï¼ˆ2ï¼‰æ³›åŒ–ä¼˜åŠ¿ï¼šåœ¨ä¸åŒä»»åŠ¡å’Œé¢†åŸŸä¸Šå§‹ç»ˆä¿æŒè‰¯å¥½çš„æ€§èƒ½ï¼Œä¼˜äºå…¶ä»–è®­ç»ƒæ¨¡å¼ï¼›ï¼ˆ3ï¼‰æ•°æ®æ•ˆç‡ï¼šåœ¨å°‘æ ·æœ¬å­¦ä¹ åœºæ™¯ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œè¶…è¶Šå…¨æ•°æ®é›†çš„SFTåŸºå‡†ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.20752v1">PDF</a> 35 pages, 22 figures</p>
<p><strong>Summary</strong></p>
<p>è§†è§‰æ¨ç†èƒ½åŠ›å¯¹äºç†è§£å¤æ‚çš„å¤šæ¨¡æ€æ•°æ®ã€æ¨åŠ¨é¢†åŸŸç‰¹å®šåº”ç”¨å’Œäººå·¥æ™ºèƒ½é€šç”¨åŒ–ï¼ˆAGIï¼‰çš„å‘å±•è‡³å…³é‡è¦ã€‚ç°æœ‰æ–¹æ³•é€šè¿‡é“¾å¼æ€ç»´ï¼ˆCoTï¼‰ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰æå‡è§†è§‰æ¨ç†èƒ½åŠ›ï¼Œä½†å¯èƒ½å¯¼è‡´è¿‡åº¦æ‹Ÿåˆå’Œè®¤çŸ¥åƒµåŒ–ã€‚ä¸ºè§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºReason-RFTå¼ºåŒ–å¾®è°ƒæ¡†æ¶ï¼Œæ˜¾è‘—æé«˜äº†è§†è§‰æ¨ç†ä»»åŠ¡çš„æ³›åŒ–èƒ½åŠ›ã€‚è¯¥æ¡†æ¶åŒ…æ‹¬ä¸¤ä¸ªé˜¶æ®µï¼šç¬¬ä¸€é˜¶æ®µæ˜¯æ¿€æ´»è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰çš„æ¨ç†æ½œèƒ½çš„ç›‘ç£å¾®è°ƒï¼›ç¬¬äºŒé˜¶æ®µæ˜¯åŸºäºç¾¤ä½“ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆGRPOï¼‰çš„å¼ºåŒ–å­¦ä¹ ï¼Œç”Ÿæˆå¤šä¸ªæ¨ç†åº”ç­”å¯¹ï¼Œå¢å¼ºè§†è§‰æ¨ç†ä»»åŠ¡çš„æ³›åŒ–èƒ½åŠ›ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒReasoning-RFTå…·æœ‰ä¸‰å¤§ä¼˜åŠ¿ï¼šæ€§èƒ½æå‡ã€æ³›åŒ–ä¼˜è¶Šå’Œæ•°æ®é«˜æ•ˆã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è§†è§‰æ¨ç†èƒ½åŠ›å¯¹äºç†è§£å’Œåº”ç”¨å¤æ‚å¤šæ¨¡æ€æ•°æ®è‡³å…³é‡è¦ã€‚</li>
<li>å½“å‰è§†è§‰æ¨ç†æ–¹æ³•é€šè¿‡é“¾å¼æ€ç»´ï¼ˆCoTï¼‰ç›‘ç£å¾®è°ƒæå‡èƒ½åŠ›ï¼Œä½†å­˜åœ¨è¿‡åº¦æ‹Ÿåˆå’Œè®¤çŸ¥åƒµåŒ–é—®é¢˜ã€‚</li>
<li>Reason-RFTæ¡†æ¶æ—¨åœ¨è§£å†³è¿™äº›é—®é¢˜ï¼Œé€šè¿‡ä¸¤ä¸ªé˜¶æ®µæé«˜è§†è§‰æ¨ç†çš„æ³›åŒ–èƒ½åŠ›ã€‚</li>
<li>ç¬¬ä¸€é˜¶æ®µä¸ºç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰ï¼Œæ¿€æ´»è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰çš„æ¨ç†æ½œèƒ½ã€‚</li>
<li>ç¬¬äºŒé˜¶æ®µé‡‡ç”¨åŸºäºç¾¤ä½“ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆGRPOï¼‰çš„å¼ºåŒ–å­¦ä¹ ï¼Œå¢å¼ºæ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ã€‚</li>
<li>å®éªŒè¡¨æ˜Reasoning-RFTå…·æœ‰æ€§èƒ½æå‡ã€æ³›åŒ–ä¼˜è¶Šå’Œæ•°æ®é«˜æ•ˆä¸‰å¤§ä¼˜åŠ¿ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.20752">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-e0bfea3ced615b6f5e7a485d4761ee43.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6ff52be7618643b781f0b047988a2a27.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-06b95518386236ce42fcfb65735cb601.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="GLRD-Global-Local-Collaborative-Reason-and-Debate-with-PSL-for-3D-Open-Vocabulary-Detection"><a href="#GLRD-Global-Local-Collaborative-Reason-and-Debate-with-PSL-for-3D-Open-Vocabulary-Detection" class="headerlink" title="GLRD: Global-Local Collaborative Reason and Debate with PSL for 3D   Open-Vocabulary Detection"></a>GLRD: Global-Local Collaborative Reason and Debate with PSL for 3D   Open-Vocabulary Detection</h2><p><strong>Authors:Xingyu Peng, Si Liu, Chen Gao, Yan Bai, Beipeng Mu, Xiaofei Wang, Huaxia Xia</strong></p>
<p>The task of LiDAR-based 3D Open-Vocabulary Detection (3D OVD) requires the detector to learn to detect novel objects from point clouds without off-the-shelf training labels. Previous methods focus on the learning of object-level representations and ignore the scene-level information, thus it is hard to distinguish objects with similar classes. In this work, we propose a Global-Local Collaborative Reason and Debate with PSL (GLRD) framework for the 3D OVD task, considering both local object-level information and global scene-level information. Specifically, LLM is utilized to perform common sense reasoning based on object-level and scene-level information, where the detection result is refined accordingly. To further boost the LLMâ€™s ability of precise decisions, we also design a probabilistic soft logic solver (OV-PSL) to search for the optimal solution, and a debate scheme to confirm the class of confusable objects. In addition, to alleviate the uneven distribution of classes, a static balance scheme (SBC) and a dynamic balance scheme (DBC) are designed. In addition, to reduce the influence of noise in data and training, we further propose Reflected Pseudo Labels Generation (RPLG) and Background-Aware Object Localization (BAOL). Extensive experiments conducted on ScanNet and SUN RGB-D demonstrate the superiority of GLRD, where absolute improvements in mean average precision are $+2.82%$ on SUN RGB-D and $+3.72%$ on ScanNet in the partial open-vocabulary setting. In the full open-vocabulary setting, the absolute improvements in mean average precision are $+4.03%$ on ScanNet and $+14.11%$ on SUN RGB-D. </p>
<blockquote>
<p>åŸºäºæ¿€å…‰é›·è¾¾çš„3Då¼€æ”¾è¯æ±‡æ£€æµ‹ï¼ˆ3D OVDï¼‰ä»»åŠ¡è¦æ±‚æ£€æµ‹å™¨ä»ç‚¹äº‘ä¸­å­¦ä¹ æ£€æµ‹æ–°å‹å¯¹è±¡ï¼Œè€Œæ— éœ€ç°æˆçš„è®­ç»ƒæ ‡ç­¾ã€‚ä¹‹å‰çš„æ–¹æ³•ä¾§é‡äºå­¦ä¹ å¯¹è±¡çº§åˆ«çš„è¡¨ç¤ºï¼Œè€Œå¿½ç•¥äº†åœºæ™¯çº§åˆ«çš„ä¿¡æ¯ï¼Œå› æ­¤å¾ˆéš¾åŒºåˆ†å…·æœ‰ç›¸ä¼¼ç±»åˆ«çš„å¯¹è±¡ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬é’ˆå¯¹3D OVDä»»åŠ¡æå‡ºäº†ä¸€ä¸ªå…¨å±€-å±€éƒ¨ååŒæ¨ç†ä¸è¾©è®ºçš„PSLï¼ˆGLRDï¼‰æ¡†æ¶ï¼Œè¯¥æ¡†æ¶æ—¢è€ƒè™‘å±€éƒ¨å¯¹è±¡çº§åˆ«çš„ä¿¡æ¯ï¼Œåˆè€ƒè™‘å…¨å±€åœºæ™¯çº§åˆ«çš„ä¿¡æ¯ã€‚å…·ä½“æ¥è¯´ï¼Œåˆ©ç”¨LLMï¼ˆå¤§å‹è¯­è¨€æ¨¡å‹ï¼‰åŸºäºå¯¹è±¡çº§åˆ«å’Œåœºæ™¯çº§åˆ«çš„ä¿¡æ¯è¿›è¡Œå¸¸è¯†æ¨ç†ï¼Œå¹¶æ®æ­¤ä¼˜åŒ–æ£€æµ‹ç»“æœã€‚ä¸ºäº†è¿›ä¸€æ­¥æå‡LLMçš„ç²¾ç¡®å†³ç­–èƒ½åŠ›ï¼Œæˆ‘ä»¬è¿˜è®¾è®¡äº†ä¸€ä¸ªæ¦‚ç‡è½¯é€»è¾‘æ±‚è§£å™¨ï¼ˆOV-PSLï¼‰æ¥å¯»æ‰¾æœ€ä¼˜è§£ï¼Œä»¥åŠä¸€ä¸ªè¾©è®ºæ–¹æ¡ˆæ¥ç¡®è®¤æ··æ·†å¯¹è±¡çš„ç±»åˆ«ã€‚æ­¤å¤–ï¼Œä¸ºäº†ç¼“è§£ç±»åˆ«åˆ†å¸ƒä¸å‡çš„é—®é¢˜ï¼Œæˆ‘ä»¬è®¾è®¡äº†é™æ€å¹³è¡¡æ–¹æ¡ˆï¼ˆSBCï¼‰å’ŒåŠ¨æ€å¹³è¡¡æ–¹æ¡ˆï¼ˆDBCï¼‰ã€‚æ­¤å¤–ï¼Œä¸ºäº†å‡å°‘æ•°æ®å’Œè®­ç»ƒä¸­çš„å™ªå£°å½±å“ï¼Œæˆ‘ä»¬è¿›ä¸€æ­¥æå‡ºäº†åå°„ä¼ªæ ‡ç­¾ç”Ÿæˆï¼ˆRPLGï¼‰å’ŒèƒŒæ™¯æ„ŸçŸ¥å¯¹è±¡å®šä½ï¼ˆBAOLï¼‰ã€‚åœ¨ScanNetå’ŒSUN RGB-Dä¸Šè¿›è¡Œçš„å¹¿æ³›å®éªŒè¯æ˜äº†GLRDçš„ä¼˜è¶Šæ€§ï¼Œåœ¨éƒ¨åˆ†å¼€æ”¾è¯æ±‡è®¾ç½®ä¸­ï¼ŒSUN RGB-Dä¸Šçš„å¹³å‡ç²¾åº¦æé«˜äº†+2.82ï¼…ï¼ŒScanNetä¸Šçš„å¹³å‡ç²¾åº¦æé«˜äº†+3.72ï¼…ã€‚åœ¨å…¨å¼€æ”¾è¯æ±‡è®¾ç½®ä¸‹ï¼ŒScanNetä¸Šçš„å¹³å‡ç²¾åº¦æé«˜äº†+4.03ï¼…ï¼ŒSUN RGB-Dä¸Šæé«˜äº†+14.11ï¼…ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.20682v1">PDF</a> 15 pages</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†é’ˆå¯¹LiDAR-based 3D Open-Vocabulary Detectionï¼ˆ3D OVDï¼‰ä»»åŠ¡çš„GLRDæ¡†æ¶ã€‚è¯¥æ¡†æ¶ç»“åˆå±€éƒ¨å¯¹è±¡çº§ä¿¡æ¯å’Œå…¨å±€åœºæ™¯çº§ä¿¡æ¯ï¼Œé€šè¿‡LLMè¿›è¡Œå¸¸è¯†æ¨ç†ï¼Œå¹¶ä½¿ç”¨PSLä¼˜åŒ–å†³ç­–ã€‚åŒæ—¶ï¼Œè®¾è®¡é™æ€å’ŒåŠ¨æ€å¹³è¡¡æ–¹æ¡ˆè§£å†³ç±»åˆ«åˆ†å¸ƒä¸å‡é—®é¢˜ï¼Œå¹¶å¼•å…¥åå°„ä¼ªæ ‡ç­¾ç”Ÿæˆå’ŒèƒŒæ™¯æ„ŸçŸ¥å¯¹è±¡å®šä½ç­–ç•¥ä»¥é™ä½å™ªå£°å’Œæ•°æ®è®­ç»ƒçš„å½±å“ã€‚å®éªŒç»“æœè¡¨æ˜GLRDæ¡†æ¶åœ¨ScanNetå’ŒSUN RGB-Dæ•°æ®é›†ä¸Šæ€§èƒ½ä¼˜è¶Šã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>LiDAR-based 3D Open-Vocabulary Detectionï¼ˆ3D OVDï¼‰ä»»åŠ¡è¦æ±‚æ£€æµ‹å™¨ä»ç‚¹äº‘ä¸­å­¦ä¹ æ£€æµ‹æ–°å‹å¯¹è±¡ï¼Œæ— éœ€ç°æˆçš„è®­ç»ƒæ ‡ç­¾ã€‚</li>
<li>ç°æœ‰æ–¹æ³•ä¸»è¦å…³æ³¨å¯¹è±¡çº§è¡¨ç¤ºçš„å­¦ä¹ ï¼Œå¿½ç•¥äº†åœºæ™¯çº§ä¿¡æ¯ï¼Œéš¾ä»¥åŒºåˆ†ç›¸ä¼¼ç±»åˆ«çš„å¯¹è±¡ã€‚</li>
<li>GLRDæ¡†æ¶ç»“åˆå±€éƒ¨å¯¹è±¡çº§å’Œå…¨å±€åœºæ™¯çº§ä¿¡æ¯ï¼Œä½¿ç”¨LLMè¿›è¡Œå¸¸è¯†æ¨ç†å¹¶ä¼˜åŒ–æ£€æµ‹ç»“æœçš„ç²¾åº¦ã€‚</li>
<li>è®¾è®¡äº†æ¦‚ç‡è½¯é€»è¾‘æ±‚è§£å™¨å’Œè¾©è®ºæ–¹æ¡ˆï¼Œä»¥æé«˜LLMçš„ç²¾ç¡®å†³ç­–èƒ½åŠ›ã€‚</li>
<li>å¼•å…¥é™æ€å’ŒåŠ¨æ€å¹³è¡¡æ–¹æ¡ˆè§£å†³ç±»åˆ«åˆ†å¸ƒä¸å‡é—®é¢˜ã€‚</li>
<li>æå‡ºåå°„ä¼ªæ ‡ç­¾ç”Ÿæˆå’ŒèƒŒæ™¯æ„ŸçŸ¥å¯¹è±¡å®šä½ç­–ç•¥ï¼Œé™ä½å™ªå£°å’Œæ•°æ®è®­ç»ƒçš„å½±å“ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.20682">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-f2e0d740a565c0f27c029c78d547276a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-db7558eb00352572dbd02a6fc82e1199.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9ab192184623ee352d950c3cc163145c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-de433d79bee9ef977f91688ce00e7a2b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-acbfb6d6de46025aa59d4f49e4cdbdc4.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Inductive-Link-Prediction-on-N-ary-Relational-Facts-via-Semantic-Hypergraph-Reasoning"><a href="#Inductive-Link-Prediction-on-N-ary-Relational-Facts-via-Semantic-Hypergraph-Reasoning" class="headerlink" title="Inductive Link Prediction on N-ary Relational Facts via Semantic   Hypergraph Reasoning"></a>Inductive Link Prediction on N-ary Relational Facts via Semantic   Hypergraph Reasoning</h2><p><strong>Authors:Gongzhu Yin, Hongli Zhang, Yuchen Yang, Yi Luo</strong></p>
<p>N-ary relational facts represent semantic correlations among more than two entities. While recent studies have developed link prediction (LP) methods to infer missing relations for knowledge graphs (KGs) containing n-ary relational facts, they are generally limited to transductive settings. Fully inductive settings, where predictions are made on previously unseen entities, remain a significant challenge. As existing methods are mainly entity embedding-based, they struggle to capture entity-independent logical rules. To fill in this gap, we propose an n-ary subgraph reasoning framework for fully inductive link prediction (ILP) on n-ary relational facts. This framework reasons over local subgraphs and has a strong inductive inference ability to capture n-ary patterns. Specifically, we introduce a novel graph structure, the n-ary semantic hypergraph, to facilitate subgraph extraction. Moreover, we develop a subgraph aggregating network, NS-HART, to effectively mine complex semantic correlations within subgraphs. Theoretically, we provide a thorough analysis from the score function optimization perspective to shed light on NS-HARTâ€™s effectiveness for n-ary ILP tasks. Empirically, we conduct extensive experiments on a series of inductive benchmarks, including transfer reasoning (with and without entity features) and pairwise subgraph reasoning. The results highlight the superiority of the n-ary subgraph reasoning framework and the exceptional inductive ability of NS-HART. The source code of this paper has been made publicly available at <a target="_blank" rel="noopener" href="https://github.com/yin-gz/Nary-Inductive-SubGraph">https://github.com/yin-gz/Nary-Inductive-SubGraph</a>. </p>
<blockquote>
<p>Nå…ƒå…³ç³»äº‹å®ä»£è¡¨ä¸¤ä¸ªä»¥ä¸Šå®ä½“ä¹‹é—´çš„è¯­ä¹‰å…³è”ã€‚å°½ç®¡æœ€è¿‘çš„ç ”ç©¶å·²ç»å¼€å‘äº†é“¾æ¥é¢„æµ‹ï¼ˆLPï¼‰æ–¹æ³•ï¼Œä»¥æ¨æ–­åŒ…å«nå…ƒå…³ç³»äº‹å®çš„çŸ¥è¯†å›¾ï¼ˆKGsï¼‰ä¸­ç¼ºå¤±çš„å…³ç³»ï¼Œä½†å®ƒä»¬é€šå¸¸å±€é™äºè½¬æ¢è®¾ç½®ã€‚åœ¨ä»¥å‰æœªè§è¿‡çš„å®ä½“ä¸Šè¿›è¡Œé¢„æµ‹çš„å…¨å½’çº³è®¾ç½®ä»ç„¶æ˜¯ä¸€ä¸ªå·¨å¤§çš„æŒ‘æˆ˜ã€‚ç”±äºç°æœ‰æ–¹æ³•ä¸»è¦æ˜¯åŸºäºå®ä½“åµŒå…¥çš„ï¼Œå®ƒä»¬å¾ˆéš¾æ•è·ä¸å®ä½“æ— å…³çš„é€»è¾‘è§„åˆ™ã€‚ä¸ºäº†å¡«è¡¥è¿™ä¸€ç©ºç™½ï¼Œæˆ‘ä»¬é’ˆå¯¹åŒ…å«nå…ƒå…³ç³»äº‹å®çš„å…¨å½’çº³é“¾æ¥é¢„æµ‹ï¼ˆILPï¼‰æå‡ºäº†ä¸€ä¸ªnå…ƒå­å›¾æ¨ç†æ¡†æ¶ã€‚è¯¥æ¡†æ¶å¯¹å±€éƒ¨å­å›¾è¿›è¡Œæ¨ç†ï¼Œå¹¶å…·æœ‰å¾ˆå¼ºçš„å½’çº³æ¨ç†èƒ½åŠ›ï¼Œå¯ä»¥æ•è·nå…ƒæ¨¡å¼ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§æ–°çš„å›¾ç»“æ„ï¼Œå³nå…ƒè¯­ä¹‰è¶…å›¾ï¼Œä»¥ä¿ƒè¿›å­å›¾æå–ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å¼€å‘äº†ä¸€ä¸ªå­å›¾èšåˆç½‘ç»œNS-HARTï¼Œä»¥æœ‰æ•ˆåœ°æŒ–æ˜å­å›¾å†…çš„å¤æ‚è¯­ä¹‰å…³è”ã€‚ä»ç†è®ºä¸Šè®²ï¼Œæˆ‘ä»¬ä»è¯„åˆ†å‡½æ•°ä¼˜åŒ–çš„è§’åº¦è¿›è¡Œäº†æ·±å…¥åˆ†æï¼Œä»¥é˜æ˜NS-HARTåœ¨nå…ƒILPä»»åŠ¡ä¸­çš„æœ‰æ•ˆæ€§ã€‚åœ¨å®è¯æ–¹é¢ï¼Œæˆ‘ä»¬åœ¨ä¸€ç³»åˆ—å½’çº³åŸºå‡†æµ‹è¯•ä¸Šè¿›è¡Œäº†å¹¿æ³›å®éªŒï¼ŒåŒ…æ‹¬è½¬ç§»æ¨ç†ï¼ˆå¸¦å’Œä¸å¸¦å®ä½“ç‰¹å¾ï¼‰å’Œé…å¯¹å­å›¾æ¨ç†ã€‚ç»“æœçªå‡ºäº†nå…ƒå­å›¾æ¨ç†æ¡†æ¶çš„ä¼˜è¶Šæ€§ä»¥åŠNS-HARTçš„å‡ºè‰²å½’çº³èƒ½åŠ›ã€‚æœ¬æ–‡çš„æºä»£ç å·²å…¬å¼€å‘å¸ƒåœ¨<a target="_blank" rel="noopener" href="https://github.com/yin-gz/Nary-Inductive-SubGraph%E3%80%82">https://github.com/yin-gz/Nary-Inductive-SubGraphã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.20676v1">PDF</a> To be published in Proceedings of the 31st ACM SIGKDD Conference on   Knowledge Discovery and Data Mining V.1 (KDDâ€™25)</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ä¸ªé’ˆå¯¹nå…ƒå…³ç³»äº‹å®çš„å®Œå…¨å½’çº³é“¾æ¥é¢„æµ‹ï¼ˆILPï¼‰çš„nå…ƒå­å›¾æ¨ç†æ¡†æ¶ã€‚è¯¥æ¡†æ¶é€šè¿‡å±€éƒ¨å­å›¾æ¨ç†ï¼Œå…·æœ‰å¼ºå¤§çš„å½’çº³æ¨ç†èƒ½åŠ›ï¼Œèƒ½æ•æ‰nå…ƒæ¨¡å¼ã€‚æ–‡ç« ä»‹ç»äº†nå…ƒè¯­ä¹‰è¶…å›¾è¿™ä¸€æ–°å‹å›¾ç»“æ„æ¥ä¿ƒè¿›å­å›¾æå–ï¼Œå¹¶å¼€å‘äº†å­å›¾èšåˆç½‘ç»œNS-HARTæ¥æŒ–æ˜å­å›¾å†…çš„å¤æ‚è¯­ä¹‰å…³è”ã€‚æ–‡ç« ä»å¾—åˆ†å‡½æ•°ä¼˜åŒ–è§’åº¦è¿›è¡Œäº†ç†è®ºåˆ†æï¼Œå¹¶é€šè¿‡ä¸€ç³»åˆ—å½’çº³åŸºå‡†æµ‹è¯•éªŒè¯äº†nå…ƒå­å›¾æ¨ç†æ¡†æ¶å’ŒNS-HARTçš„å‡ºè‰²å½’çº³èƒ½åŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ–‡ç« æå‡ºäº†ä¸€ä¸ªé’ˆå¯¹nå…ƒå…³ç³»äº‹å®çš„å®Œå…¨å½’çº³é“¾æ¥é¢„æµ‹ï¼ˆILPï¼‰çš„nå…ƒå­å›¾æ¨ç†æ¡†æ¶ã€‚</li>
<li>æ¡†æ¶å…·æœ‰å¼ºå¤§çš„å½’çº³æ¨ç†èƒ½åŠ›ï¼Œé€šè¿‡å±€éƒ¨å­å›¾æ¨ç†æ•æ‰nå…ƒæ¨¡å¼ã€‚</li>
<li>å¼•å…¥nå…ƒè¯­ä¹‰è¶…å›¾è¿™ä¸€æ–°å‹å›¾ç»“æ„ï¼Œä¿ƒè¿›å­å›¾æå–ã€‚</li>
<li>å¼€å‘å­å›¾èšåˆç½‘ç»œNS-HARTï¼Œæœ‰æ•ˆæŒ–æ˜å­å›¾å†…çš„å¤æ‚è¯­ä¹‰å…³è”ã€‚</li>
<li>æ–‡ç« ä»å¾—åˆ†å‡½æ•°ä¼˜åŒ–è§’åº¦è¿›è¡Œäº†ç†è®ºåˆ†æï¼ŒéªŒè¯äº†æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚</li>
<li>é€šè¿‡ä¸€ç³»åˆ—å½’çº³åŸºå‡†æµ‹è¯•ï¼Œå±•ç°äº†nå…ƒå­å›¾æ¨ç†æ¡†æ¶å’ŒNS-HARTçš„ä¼˜è¶Šæ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.20676">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-b6977e103c2af007358991b380d1ca6e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9948f2d87cb6148852f002d210ba7ccf.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-75aa1b7c311feb9640b47751e4971f4a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e144bc080743ca270cf753c890e23d60.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-64cdb6593a28107c839b46ef5dc63b90.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Unlocking-Efficient-Long-to-Short-LLM-Reasoning-with-Model-Merging"><a href="#Unlocking-Efficient-Long-to-Short-LLM-Reasoning-with-Model-Merging" class="headerlink" title="Unlocking Efficient Long-to-Short LLM Reasoning with Model Merging"></a>Unlocking Efficient Long-to-Short LLM Reasoning with Model Merging</h2><p><strong>Authors:Han Wu, Yuxuan Yao, Shuqi Liu, Zehua Liu, Xiaojin Fu, Xiongwei Han, Xing Li, Hui-Ling Zhen, Tao Zhong, Mingxuan Yuan</strong></p>
<p>The transition from System 1 to System 2 reasoning in large language models (LLMs) has marked significant advancements in handling complex tasks through deliberate, iterative thinking. However, this progress often comes at the cost of efficiency, as models tend to overthink, generating redundant reasoning steps without proportional improvements in output quality. Long-to-Short (L2S) reasoning has emerged as a promising solution to this challenge, aiming to balance reasoning depth with practical efficiency. While existing approaches, such as supervised fine-tuning (SFT), reinforcement learning (RL), and prompt engineering, have shown potential, they are either computationally expensive or unstable. Model merging, on the other hand, offers a cost-effective and robust alternative by integrating the quick-thinking capabilities of System 1 models with the methodical reasoning of System 2 models. In this work, we present a comprehensive empirical study on model merging for L2S reasoning, exploring diverse methodologies, including task-vector-based, SVD-based, and activation-informed merging. Our experiments reveal that model merging can reduce average response length by up to 55% while preserving or even improving baseline performance. We also identify a strong correlation between model scale and merging efficacy with extensive evaluations on 1.5B&#x2F;7B&#x2F;14B&#x2F;32B models. Furthermore, we investigate the merged modelâ€™s ability to self-critique and self-correct, as well as its adaptive response length based on task complexity. Our findings highlight model merging as a highly efficient and effective paradigm for L2S reasoning, offering a practical solution to the overthinking problem while maintaining the robustness of System 2 reasoning. This work can be found on Github <a target="_blank" rel="noopener" href="https://github.com/hahahawu/Long-to-Short-via-Model-Merging">https://github.com/hahahawu/Long-to-Short-via-Model-Merging</a>. </p>
<blockquote>
<p>ä»System 1åˆ°System 2æ¨ç†åœ¨å¤§è§„æ¨¡è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¸­çš„è¿‡æ¸¡æ ‡å¿—ç€é€šè¿‡æ·±æ€ç†Ÿè™‘ã€è¿­ä»£æ€è€ƒæ¥å¤„ç†å¤æ‚ä»»åŠ¡çš„èƒ½åŠ›å–å¾—äº†é‡å¤§è¿›å±•ã€‚ç„¶è€Œï¼Œè¿™ç§è¿›å±•å¾€å¾€ä»¥æ•ˆç‡ä¸ºä»£ä»·ï¼Œå› ä¸ºæ¨¡å‹å¾€å¾€è¿‡åº¦æ€è€ƒï¼Œäº§ç”Ÿå†—ä½™çš„æ¨ç†æ­¥éª¤ï¼Œè€Œæ²¡æœ‰åœ¨è¾“å‡ºè´¨é‡ä¸Šå¾—åˆ°ç›¸åº”çš„æ”¹è¿›ã€‚é•¿çŸ­æ¨ç†ï¼ˆL2Sï¼‰å·²æˆä¸ºåº”å¯¹è¿™ä¸€æŒ‘æˆ˜çš„æœ‰å‰é€”çš„è§£å†³æ–¹æ¡ˆï¼Œæ—¨åœ¨å¹³è¡¡æ¨ç†æ·±åº¦ä¸å®é™…æ•ˆç‡ã€‚è™½ç„¶ç°æœ‰çš„æ–¹æ³•ï¼Œå¦‚ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰ã€å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰å’Œæç¤ºå·¥ç¨‹ï¼Œå·²ç»æ˜¾ç¤ºå‡ºæ½œåŠ›ï¼Œä½†å®ƒä»¬è¦ä¹ˆè®¡ç®—æˆæœ¬é«˜æ˜‚ï¼Œè¦ä¹ˆä¸ç¨³å®šã€‚å¦ä¸€æ–¹é¢ï¼Œæ¨¡å‹èåˆé€šè¿‡èåˆSystem 1æ¨¡å‹çš„å¿«é€Ÿæ€è€ƒèƒ½åŠ›å’ŒSystem 2æ¨¡å‹çš„æ–¹æ³•æ€§æ¨ç†ï¼Œæä¾›äº†ä¸€ç§ç»æµé«˜æ•ˆä¸”ç¨³å¥çš„æ›¿ä»£æ–¹æ¡ˆã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬å¯¹æ¨¡å‹èåˆè¿›è¡Œäº†ä¸€é¡¹å…¨é¢çš„å®è¯ç ”ç©¶ï¼Œæ¢ç´¢äº†åŒ…æ‹¬åŸºäºä»»åŠ¡å‘é‡ã€åŸºäºSVDå’ŒåŸºäºæ¿€æ´»ä¿¡æ¯èåˆç­‰å¤šç§æ–¹æ³•ã€‚æˆ‘ä»¬çš„å®éªŒè¡¨æ˜ï¼Œæ¨¡å‹èåˆå¯ä»¥åœ¨ä¿æŒæˆ–ç”šè‡³æé«˜åŸºçº¿æ€§èƒ½çš„åŒæ—¶ï¼Œå°†å¹³å‡å“åº”é•¿åº¦å‡å°‘é«˜è¾¾55%ã€‚æˆ‘ä»¬è¿˜å¯¹æ¨¡å‹è§„æ¨¡ä¸èåˆæ•ˆæœä¹‹é—´çš„å¼ºç›¸å…³æ€§è¿›è¡Œäº†å¹¿æ³›è¯„ä¼°ï¼Œæ¶‰åŠ1.5B&#x2F;7B&#x2F;14B&#x2F;32Bæ¨¡å‹ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜ç ”ç©¶äº†èåˆæ¨¡å‹çš„è‡ªæˆ‘æ‰¹åˆ¤å’Œè‡ªæˆ‘çº æ­£èƒ½åŠ›ï¼Œä»¥åŠå…¶æ ¹æ®ä»»åŠ¡å¤æ‚æ€§çš„è‡ªé€‚åº”å“åº”é•¿åº¦ã€‚æˆ‘ä»¬çš„ç ”ç©¶ç»“æœå¼ºè°ƒäº†æ¨¡å‹èåˆä½œä¸ºä¸€ç§é«˜æ•ˆä¸”æœ‰æ•ˆçš„é•¿çŸ­æ¨ç†èŒƒå¼ï¼Œä¸ºè§£å†³è¿‡åº¦æ€è€ƒé—®é¢˜æä¾›äº†å®ç”¨è§£å†³æ–¹æ¡ˆï¼ŒåŒæ—¶ä¿æŒäº†System 2æ¨ç†çš„ç¨³å¥æ€§ã€‚è¿™é¡¹å·¥ä½œå¯ä»¥åœ¨Githubä¸Šæ‰¾åˆ°ï¼š[<a target="_blank" rel="noopener" href="https://github.com/hahahawu/Long-to-Short-via-Model-Merging%E3%80%82]">https://github.com/hahahawu/Long-to-Short-via-Model-Mergingã€‚]</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.20641v1">PDF</a> Work in progress; technical report</p>
<p><strong>Summary</strong></p>
<p>è¯¥æ–‡æ¢è®¨äº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä»System 1åˆ°System 2æ¨ç†çš„è½¬å˜å¸¦æ¥çš„æŒ‘æˆ˜ã€‚å°½ç®¡è¿™ç§è½¬å˜æé«˜äº†å¤„ç†å¤æ‚ä»»åŠ¡çš„èƒ½åŠ›ï¼Œä½†å¾€å¾€ç‰ºç‰²äº†æ•ˆç‡ï¼Œå¯¼è‡´æ¨¡å‹è¿‡åº¦æ€è€ƒï¼Œäº§ç”Ÿå†—ä½™çš„æ¨ç†æ­¥éª¤ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œå‡ºç°äº†Long-to-Shortï¼ˆL2Sï¼‰æ¨ç†æ–¹æ³•ï¼Œæ—¨åœ¨å¹³è¡¡æ¨ç†æ·±åº¦ä¸å®é™…åº”ç”¨æ•ˆç‡ã€‚æ–‡ç« ä»‹ç»äº†æ¨¡å‹åˆå¹¶ä½œä¸ºä¸€ç§é«˜æ•ˆä¸”ç¨³å¥çš„L2Sæ¨ç†æ–¹æ³•ï¼Œé€šè¿‡å°†System 1æ¨¡å‹çš„å¿«é€Ÿæ€è€ƒä¸System 2æ¨¡å‹çš„æ–¹æ³•æ€§æ¨ç†ç›¸ç»“åˆã€‚é€šè¿‡å®è¯ç ”ç©¶ï¼Œå‘ç°æ¨¡å‹åˆå¹¶å¯ä»¥åœ¨ä¿æŒæˆ–æé«˜åŸºçº¿æ€§èƒ½çš„åŒæ—¶ï¼Œå°†å¹³å‡å“åº”é•¿åº¦ç¼©çŸ­55%ã€‚åŒæ—¶ï¼Œæ–‡ç« è¿˜æ¢è®¨äº†åˆå¹¶æ¨¡å‹è‡ªæˆ‘æ‰¹åˆ¤ä¸è‡ªæˆ‘çº æ­£çš„èƒ½åŠ›ï¼Œä»¥åŠå…¶æ ¹æ®ä»»åŠ¡å¤æ‚æ€§è‡ªé€‚åº”è°ƒæ•´å“åº”é•¿åº¦çš„ç‰¹æ€§ã€‚æ¨¡å‹åˆå¹¶çš„é«˜æ•ˆæ€§å’Œå®ç”¨æ€§ä¸ºè§£å†³è¿‡åº¦æ€è€ƒé—®é¢˜æä¾›äº†æ–°çš„è§†è§’ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹çš„System 1åˆ°System 2æ¨ç†è½¬å˜å¸¦æ¥äº†æ•ˆç‡ä¸æ€§èƒ½ä¹‹é—´çš„æŒ‘æˆ˜ã€‚</li>
<li>Long-to-Shortï¼ˆL2Sï¼‰æ¨ç†æ—¨åœ¨å¹³è¡¡æ¨ç†æ·±åº¦ä¸å®é™…åº”ç”¨æ•ˆç‡ã€‚</li>
<li>æ¨¡å‹åˆå¹¶æ˜¯ä¸€ç§æ–°å…´æ–¹æ³•ï¼Œé€šè¿‡æ•´åˆSystem 1å’ŒSystem 2æ¨¡å‹çš„ä¼˜ç‚¹æ¥ä¼˜åŒ–L2Sæ¨ç†ã€‚</li>
<li>å®è¯ç ”ç©¶è¯æ˜ï¼Œæ¨¡å‹åˆå¹¶èƒ½æœ‰æ•ˆç¼©çŸ­å“åº”é•¿åº¦å¹¶ç»´æŒæˆ–æé«˜æ€§èƒ½ã€‚</li>
<li>æ¨¡å‹åˆå¹¶çš„æ•ˆæœä¸æ¨¡å‹è§„æ¨¡å¯†åˆ‡ç›¸å…³ã€‚</li>
<li>åˆå¹¶æ¨¡å‹å…·å¤‡è‡ªæˆ‘æ‰¹åˆ¤å’Œè‡ªæˆ‘çº æ­£çš„èƒ½åŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.20641">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-1532536a13b11fe7ecf32f9203a0126d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c25b579b3e003f6c8600f2fd497cac91.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Optimizing-Case-Based-Reasoning-System-for-Functional-Test-Script-Generation-with-Large-Language-Models"><a href="#Optimizing-Case-Based-Reasoning-System-for-Functional-Test-Script-Generation-with-Large-Language-Models" class="headerlink" title="Optimizing Case-Based Reasoning System for Functional Test Script   Generation with Large Language Models"></a>Optimizing Case-Based Reasoning System for Functional Test Script   Generation with Large Language Models</h2><p><strong>Authors:Siyuan Guo, Huiwu Liu, Xiaolong Chen, Yuming Xie, Liang Zhang, Tao Han, Hechang Chen, Yi Chang, Jun Wang</strong></p>
<p>In this work, we explore the potential of large language models (LLMs) for generating functional test scripts, which necessitates understanding the dynamically evolving code structure of the target software. To achieve this, we propose a case-based reasoning (CBR) system utilizing a 4R cycle (i.e., retrieve, reuse, revise, and retain), which maintains and leverages a case bank of test intent descriptions and corresponding test scripts to facilitate LLMs for test script generation. To improve user experience further, we introduce Re4, an optimization method for the CBR system, comprising reranking-based retrieval finetuning and reinforced reuse finetuning. Specifically, we first identify positive examples with high semantic and script similarity, providing reliable pseudo-labels for finetuning the retriever model without costly labeling. Then, we apply supervised finetuning, followed by a reinforcement learning finetuning stage, to align LLMs with our production scenarios, ensuring the faithful reuse of retrieved cases. Extensive experimental results on two product development units from Huawei Datacom demonstrate the superiority of the proposed CBR+Re4. Notably, we also show that the proposed Re4 method can help alleviate the repetitive generation issues with LLMs. </p>
<blockquote>
<p>åœ¨æœ¬æ¬¡å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æ¢ç´¢äº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨ç”ŸæˆåŠŸèƒ½æ€§æµ‹è¯•è„šæœ¬æ–¹é¢çš„æ½œåŠ›ï¼Œè¿™éœ€è¦ç†è§£ç›®æ ‡è½¯ä»¶çš„åŠ¨æ€æ¼”åŒ–ä»£ç ç»“æ„ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åˆ©ç”¨4Rå¾ªç¯ï¼ˆå³æ£€ç´¢ã€é‡ç”¨ã€ä¿®è®¢å’Œä¿ç•™ï¼‰çš„æ¡ˆä¾‹æ¨ç†ï¼ˆCBRï¼‰ç³»ç»Ÿï¼Œè¯¥ç³»ç»Ÿç»´æŠ¤å’Œåˆ©ç”¨æµ‹è¯•æ„å›¾æè¿°å’Œç›¸åº”æµ‹è¯•è„šæœ¬çš„æ¡ˆä¾‹åº“ï¼Œä»¥ä¿ƒè¿›LLMè¿›è¡Œæµ‹è¯•è„šæœ¬ç”Ÿæˆã€‚ä¸ºäº†è¿›ä¸€æ­¥æå‡ç”¨æˆ·ä½“éªŒï¼Œæˆ‘ä»¬å¼•å…¥äº†Re4ï¼Œè¿™æ˜¯CBRç³»ç»Ÿçš„ä¼˜åŒ–æ–¹æ³•ï¼ŒåŒ…æ‹¬åŸºäºæ’åºé‡æ’çš„æ£€ç´¢å¾®è°ƒå¼ºåŒ–é‡ç”¨å¾®è°ƒã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬é¦–å…ˆé€šè¿‡é«˜è¯­ä¹‰å’Œè„šæœ¬ç›¸ä¼¼åº¦è¯†åˆ«æ­£é¢ä¾‹å­ï¼Œä¸ºæ— éœ€æ˜‚è´µæ ‡ç­¾çš„æ£€ç´¢æ¨¡å‹å¾®è°ƒæä¾›å¯é çš„ä¼ªæ ‡ç­¾ã€‚ç„¶åï¼Œæˆ‘ä»¬åº”ç”¨ç›‘ç£å¾®è°ƒï¼Œæ¥ç€æ˜¯å¼ºåŒ–å­¦ä¹ å¾®è°ƒé˜¶æ®µï¼Œä½¿LLMç¬¦åˆæˆ‘ä»¬çš„ç”Ÿäº§åœºæ™¯ï¼Œç¡®ä¿æ£€ç´¢åˆ°çš„æ¡ˆä¾‹çš„å¿ å®é‡ç”¨ã€‚åœ¨åä¸ºæ•°æ®é€šä¿¡çš„ä¸¤ä¸ªäº§å“å¼€å‘å•å…ƒçš„å¹¿æ³›å®éªŒç»“æœè¡¨æ˜ï¼Œæ‰€æå‡ºçš„CBR+Re4å…·æœ‰ä¼˜è¶Šæ€§ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œæˆ‘ä»¬è¿˜è¡¨æ˜æ‰€æå‡ºçš„Re4æ–¹æ³•æœ‰åŠ©äºç¼“è§£LLMçš„é‡å¤ç”Ÿæˆé—®é¢˜ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.20576v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ¢è®¨äº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨ç”ŸæˆåŠŸèƒ½æ€§æµ‹è¯•è„šæœ¬æ–¹é¢çš„æ½œåŠ›ã€‚ä¸ºå®ç°è¿™ä¸€ç›®æ ‡ï¼Œæå‡ºäº†ä¸€ç§åŸºäºæ¡ˆä¾‹æ¨ç†ï¼ˆCBRï¼‰çš„ç³»ç»Ÿï¼Œåˆ©ç”¨4Rå¾ªç¯ï¼ˆå³æ£€ç´¢ã€é‡ç”¨ã€ä¿®è®¢å’Œä¿ç•™ï¼‰æ¥ç»´æŠ¤å¹¶åˆ©ç”¨ä¸€ä¸ªæ¡ˆä¾‹åº“ï¼Œå…¶ä¸­åŒ…å«æµ‹è¯•æ„å›¾æè¿°å’Œç›¸åº”çš„æµ‹è¯•è„šæœ¬ï¼Œä»¥ä¿ƒè¿›LLMsçš„æµ‹è¯•è„šæœ¬ç”Ÿæˆã€‚ä¸ºè¿›ä¸€æ­¥ä¼˜åŒ–ç”¨æˆ·ä½“éªŒï¼Œå¼•å…¥äº†Re4ï¼Œè¿™æ˜¯ä¸€ç§é’ˆå¯¹CBRç³»ç»Ÿçš„ä¼˜åŒ–æ–¹æ³•ï¼ŒåŒ…æ‹¬åŸºäºé‡æ–°æ’åºçš„æ£€ç´¢å¾®è°ƒä»¥åŠå¼ºåŒ–é‡ç”¨å¾®è°ƒã€‚è¯¥æ–¹æ³•é€šè¿‡è¯†åˆ«é«˜è¯­ä¹‰å’Œè„šæœ¬ç›¸ä¼¼æ€§çš„æ­£é¢ä¾‹å­ï¼Œä¸ºå¾®è°ƒæ£€ç´¢æ¨¡å‹æä¾›å¯é çš„ä¼ªæ ‡ç­¾ï¼Œç„¶ååº”ç”¨ç›‘ç£å¾®è°ƒï¼Œæ¥ç€æ˜¯å¼ºåŒ–å­¦ä¹ å¾®è°ƒé˜¶æ®µï¼Œä½¿LLMsä¸æˆ‘ä»¬çš„ç”Ÿäº§åœºæ™¯å¯¹é½ï¼Œç¡®ä¿æ£€ç´¢åˆ°çš„æ¡ˆä¾‹çš„å¿ å®é‡ç”¨ã€‚åœ¨åä¸ºæ•°æ®é€šä¿¡çš„ä¸¤ä¸ªäº§å“å¼€å‘å•å…ƒä¸Šçš„å¹¿æ³›å®éªŒç»“æœè¯æ˜äº†æ‰€æå‡ºçš„CBR+Re4çš„ä¼˜è¶Šæ€§ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œæ‰€æå‡ºçš„Re4æ–¹æ³•è¿˜å¯ä»¥å¸®åŠ©ç¼“è§£LLMsçš„é‡å¤æ€§ç”Ÿæˆé—®é¢˜ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å¯ç”¨äºç”ŸæˆåŠŸèƒ½æ€§æµ‹è¯•è„šæœ¬ï¼Œéœ€ç†è§£ç›®æ ‡è½¯ä»¶çš„åŠ¨æ€æ¼”åŒ–ä»£ç ç»“æ„ã€‚</li>
<li>å¼•å…¥åŸºäºæ¡ˆä¾‹æ¨ç†ï¼ˆCBRï¼‰çš„ç³»ç»Ÿï¼Œåˆ©ç”¨4Rå¾ªç¯ï¼ˆæ£€ç´¢ã€é‡ç”¨ã€ä¿®è®¢å’Œä¿ç•™ï¼‰æ¥ç»´æŠ¤æ¡ˆä¾‹åº“ï¼Œä¿ƒè¿›LLMsç”Ÿæˆæµ‹è¯•è„šæœ¬ã€‚</li>
<li>Re4æ–¹æ³•ç”¨äºä¼˜åŒ–CBRç³»ç»Ÿï¼ŒåŒ…æ‹¬åŸºäºé‡æ–°æ’åºçš„æ£€ç´¢å¾®è°ƒåŠå¼ºåŒ–é‡ç”¨å¾®è°ƒã€‚</li>
<li>Re4é€šè¿‡è¯†åˆ«æ­£é¢ä¾‹å­ä¸ºæ£€ç´¢æ¨¡å‹æä¾›ä¼ªæ ‡ç­¾ï¼Œåº”ç”¨ç›‘ç£å¾®è°ƒå’Œå¼ºåŒ–å­¦ä¹ å¾®è°ƒï¼Œä½¿LLMsä¸çœŸå®åœºæ™¯å¯¹é½ã€‚</li>
<li>åä¸ºæ•°æ®é€šä¿¡çš„å®éªŒç»“æœè¯æ˜CBR+Re4æ–¹æ³•çš„ä¼˜è¶Šæ€§ã€‚</li>
<li>Re4æ–¹æ³•æœ‰åŠ©äºç¼“è§£LLMsçš„é‡å¤æ€§ç”Ÿæˆé—®é¢˜ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.20576">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-ebed85b76ecc601a9d30102b3eddb40b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-bef69a94ffbc5a443d0dc7275728e04c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-3012b9e035028772e621a27537432aff.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-dd109bd7323dd466451e40f008ab6c81.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-00d58fc5b08cccba5d72b4246e5ef585.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Accelerate-Parallelizable-Reasoning-via-Parallel-Decoding-within-One-Sequence"><a href="#Accelerate-Parallelizable-Reasoning-via-Parallel-Decoding-within-One-Sequence" class="headerlink" title="Accelerate Parallelizable Reasoning via Parallel Decoding within One   Sequence"></a>Accelerate Parallelizable Reasoning via Parallel Decoding within One   Sequence</h2><p><strong>Authors:Yijiong Yu</strong></p>
<p>Recent advances in reasoning models have demonstrated significant improvements in accuracy, particularly for complex tasks such as mathematical reasoning, by employing detailed and comprehensive reasoning processes. However, generating these lengthy reasoning sequences is computationally expensive and time-consuming. To address this inefficiency, we leverage the inherent parallelizability of certain tasks to accelerate the reasoning process. Specifically, when multiple parallel reasoning branches exist, we decode multiple tokens per step using a specialized attention mask, processing them within a single sequence. Experimental results show that our method achieves over 100% speedup in decoding time while basically maintaining accuracy. </p>
<blockquote>
<p>è¿‘æœŸæ¨ç†æ¨¡å‹çš„è¿›æ­¥å·²ç»æ˜¾ç¤ºå‡ºåœ¨å‡†ç¡®æ€§ä¸Šçš„å·¨å¤§æå‡ï¼Œç‰¹åˆ«æ˜¯åœ¨æ•°å­¦æ¨ç†ç­‰å¤æ‚ä»»åŠ¡ä¸­ï¼Œé€šè¿‡é‡‡ç”¨è¯¦ç»†ä¸”å…¨é¢çš„æ¨ç†è¿‡ç¨‹ã€‚ç„¶è€Œï¼Œç”Ÿæˆè¿™äº›å†—é•¿çš„æ¨ç†åºåˆ—åœ¨è®¡ç®—ä¸Šæˆæœ¬é«˜æ˜‚ä¸”è€—æ—¶ã€‚ä¸ºäº†è§£å†³è¿™ç§ä½æ•ˆé—®é¢˜ï¼Œæˆ‘ä»¬åˆ©ç”¨æŸäº›ä»»åŠ¡çš„å›ºæœ‰å¹¶è¡Œæ€§æ¥åŠ é€Ÿæ¨ç†è¿‡ç¨‹ã€‚å…·ä½“æ¥è¯´ï¼Œå½“å­˜åœ¨å¤šä¸ªå¹¶è¡Œæ¨ç†åˆ†æ”¯æ—¶ï¼Œæˆ‘ä»¬ä½¿ç”¨ä¸“é—¨çš„æ³¨æ„åŠ›æ©ç ä¸€æ¬¡è§£ç å¤šä¸ªæ ‡è®°ï¼Œå¹¶åœ¨å•ä¸ªåºåˆ—å†…è¿›è¡Œå¤„ç†ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨è§£ç æ—¶é—´ä¸Šå®ç°äº†è¶…è¿‡100%çš„åŠ é€Ÿï¼ŒåŒæ—¶åŸºæœ¬ä¿æŒäº†å‡†ç¡®æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.20533v1">PDF</a> Our code is available in   <a target="_blank" rel="noopener" href="https://github.com/yuyijiong/parallel-decoding-in-one-sequence">https://github.com/yuyijiong/parallel-decoding-in-one-sequence</a></p>
<p><strong>Summary</strong>ï¼š<br>æœ€æ–°è¿›å±•çš„æ¨ç†æ¨¡å‹é€šè¿‡é‡‡ç”¨è¯¦ç»†è€Œå…¨é¢çš„æ¨ç†è¿‡ç¨‹ï¼Œåœ¨å‡†ç¡®æ€§ä¸Šå–å¾—äº†æ˜¾è‘—æ”¹è¿›ï¼Œç‰¹åˆ«æ˜¯åœ¨æ•°å­¦æ¨ç†ç­‰å¤æ‚ä»»åŠ¡ä¸­ã€‚ç„¶è€Œï¼Œç”Ÿæˆè¿™äº›å†—é•¿çš„æ¨ç†åºåˆ—åœ¨è®¡ç®—ä¸Šå¾ˆæ˜‚è´µä¸”è€—æ—¶ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬åˆ©ç”¨æŸäº›ä»»åŠ¡çš„å›ºæœ‰å¹¶è¡Œæ€§æ¥åŠ é€Ÿæ¨ç†è¿‡ç¨‹ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨è§£ç æ—¶é—´ä¸Šå®ç°äº†è¶…è¿‡100%çš„åŠ é€Ÿï¼ŒåŒæ—¶åŸºæœ¬ä¿æŒäº†å‡†ç¡®æ€§ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>æ¨ç†æ¨¡å‹åœ¨å‡†ç¡®æ€§ä¸Šå–å¾—æ˜¾è‘—æ”¹è¿›ï¼Œç‰¹åˆ«æ˜¯å¤æ‚ä»»åŠ¡å¦‚æ•°å­¦æ¨ç†ã€‚</li>
<li>ç”Ÿæˆå†—é•¿çš„æ¨ç†åºåˆ—å­˜åœ¨è®¡ç®—æ˜‚è´µå’Œè€—æ—¶çš„é—®é¢˜ã€‚</li>
<li>é€šè¿‡åˆ©ç”¨ä»»åŠ¡çš„å›ºæœ‰å¹¶è¡Œæ€§æ¥åŠ é€Ÿæ¨ç†è¿‡ç¨‹ã€‚</li>
<li>ä½¿ç”¨ä¸“é—¨è®¾è®¡çš„æ³¨æ„åŠ›æ©ç æ¥è§£ç å¤šä¸ªå¹¶è¡Œæ¨ç†åˆ†æ”¯ä¸­çš„å¤šä¸ªä»¤ç‰Œã€‚</li>
<li>å®éªŒç»“æœæ˜¾ç¤ºè§£ç æ—¶é—´è¶…è¿‡100%çš„åŠ é€Ÿã€‚</li>
<li>æ‰€æå‡ºçš„æ–¹æ³•åœ¨åŠ é€Ÿæ¨ç†è¿‡ç¨‹çš„åŒæ—¶åŸºæœ¬ä¿æŒäº†å‡†ç¡®æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.20533">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-4367468eaa2e530cfc96918dab437b73.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-747978868058e15980471db074a5387c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-cea6e36ca17e7ac57f61fe2613dce28c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e90df4227e52085b569cca5d5ab862f2.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="QualiSpeech-A-Speech-Quality-Assessment-Dataset-with-Natural-Language-Reasoning-and-Descriptions"><a href="#QualiSpeech-A-Speech-Quality-Assessment-Dataset-with-Natural-Language-Reasoning-and-Descriptions" class="headerlink" title="QualiSpeech: A Speech Quality Assessment Dataset with Natural Language   Reasoning and Descriptions"></a>QualiSpeech: A Speech Quality Assessment Dataset with Natural Language   Reasoning and Descriptions</h2><p><strong>Authors:Siyin Wang, Wenyi Yu, Xianzhao Chen, Xiaohai Tian, Jun Zhang, Yu Tsao, Junichi Yamagishi, Yuxuan Wang, Chao Zhang</strong></p>
<p>This paper explores a novel perspective to speech quality assessment by leveraging natural language descriptions, offering richer, more nuanced insights than traditional numerical scoring methods. Natural language feedback provides instructive recommendations and detailed evaluations, yet existing datasets lack the comprehensive annotations needed for this approach. To bridge this gap, we introduce QualiSpeech, a comprehensive low-level speech quality assessment dataset encompassing 11 key aspects and detailed natural language comments that include reasoning and contextual insights. Additionally, we propose the QualiSpeech Benchmark to evaluate the low-level speech understanding capabilities of auditory large language models (LLMs). Experimental results demonstrate that finetuned auditory LLMs can reliably generate detailed descriptions of noise and distortion, effectively identifying their types and temporal characteristics. The results further highlight the potential for incorporating reasoning to enhance the accuracy and reliability of quality assessments. The dataset will be released at <a target="_blank" rel="noopener" href="https://huggingface.co/datasets/tsinghua-ee/QualiSpeech">https://huggingface.co/datasets/tsinghua-ee/QualiSpeech</a>. </p>
<blockquote>
<p>æœ¬æ–‡æ¢ç´¢äº†ä¸€ç§åˆ©ç”¨è‡ªç„¶è¯­è¨€æè¿°è¿›è¡Œè¯­éŸ³è´¨é‡è¯„ä¼°çš„æ–°è§†è§’ï¼Œæä¾›æ¯”ä¼ ç»Ÿæ•°å­—è¯„åˆ†æ–¹æ³•æ›´ä¸°å¯Œã€æ›´ç»†å¾®çš„è§è§£ã€‚è‡ªç„¶è¯­è¨€åé¦ˆæä¾›äº†æŒ‡å¯¼æ€§çš„å»ºè®®å’Œè¯¦ç»†çš„è¯„ä»·ï¼Œä½†ç°æœ‰æ•°æ®é›†ç¼ºä¹è¿™ç§æ–¹æ³•æ‰€éœ€çš„å…¨é¢æ³¨é‡Šã€‚ä¸ºäº†å¼¥è¡¥è¿™ä¸€å·®è·ï¼Œæˆ‘ä»¬æ¨å‡ºäº†QualiSpeechæ•°æ®é›†ï¼Œè¿™æ˜¯ä¸€ä¸ªå…¨é¢çš„ä½å±‚æ¬¡è¯­éŸ³è´¨é‡è¯„ä¼°æ•°æ®é›†ï¼Œæ¶µç›–11ä¸ªå…³é”®æ–¹é¢å’ŒåŒ…å«æ¨ç†å’Œä¸Šä¸‹æ–‡æ´å¯Ÿçš„è‡ªç„¶è¯­è¨€è¯¦ç»†è¯„è®ºã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æå‡ºäº†QualiSpeechåŸºå‡†æµ‹è¯•ï¼Œä»¥è¯„ä¼°å¬è§‰å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„ä½å±‚æ¬¡è¯­éŸ³ç†è§£èƒ½åŠ›ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œç»è¿‡å¾®è°ƒåçš„å¬è§‰LLMèƒ½å¤Ÿå¯é åœ°æè¿°å™ªå£°å’Œå¤±çœŸçš„ç»†èŠ‚ï¼Œæœ‰æ•ˆåœ°è¯†åˆ«å®ƒä»¬çš„ç±»å‹å’Œæ—¶é—´ç‰¹å¾ã€‚ç»“æœè¿›ä¸€æ­¥çªæ˜¾äº†èå…¥æ¨ç†æ¥æå‡è´¨é‡è¯„ä¼°å‡†ç¡®æ€§å’Œå¯é æ€§çš„æ½œåŠ›ã€‚æ•°æ®é›†å°†åœ¨<a target="_blank" rel="noopener" href="https://huggingface.co/datasets/tsinghua-ee/QualiSpeech%E5%8F%91%E5%B8%83%E3%80%82">https://huggingface.co/datasets/tsinghua-ee/QualiSpeechå‘å¸ƒã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.20290v1">PDF</a> 23 pages, 16 figures</p>
<p><strong>Summary</strong>ï¼šæœ¬æ–‡å¼•å…¥äº†ä¸€ç§åˆ©ç”¨è‡ªç„¶è¯­è¨€æè¿°æ¥è¿›è¡Œè¯­éŸ³è´¨é‡è¯„ä¼°çš„æ–°è§†è§’ï¼Œç›¸è¾ƒäºä¼ ç»Ÿçš„æ•°å­—è¯„åˆ†æ–¹æ³•ï¼Œè¿™ç§æ–¹æ³•èƒ½æä¾›æ›´åŠ ä¸°å¯Œå’Œå¾®å¦™çš„è§è§£ã€‚ä¸ºäº†è§£å†³ç°æœ‰æ•°æ®é›†ç¼ºä¹å…¨é¢æ³¨é‡Šçš„é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†QualiSpeechæ•°æ®é›†ï¼Œè¯¥æ•°æ®é›†æ¶µç›–äº†ä½çº§åˆ«çš„è¯­éŸ³è´¨é‡è¯„ä¼°çš„11ä¸ªå…³é”®æ–¹é¢ï¼Œå¹¶åŒ…å«è¯¦ç»†çš„è‡ªç„¶è¯­è¨€æ³¨é‡Šï¼ŒåŒ…æ‹¬æ¨ç†å’Œä¸Šä¸‹æ–‡æ´å¯Ÿã€‚åŒæ—¶ï¼Œæœ¬æ–‡æå‡ºäº†QualiSpeech Benchmarkæ¥è¯„ä¼°å¬è§‰å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„ä½çº§åˆ«è¯­éŸ³ç†è§£èƒ½åŠ›ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œç»è¿‡å¾®è°ƒçš„å¤§å‹è¯­è¨€æ¨¡å‹èƒ½å¤Ÿå¯é åœ°æè¿°å™ªå£°å’Œå¤±çœŸï¼Œæœ‰æ•ˆåœ°è¯†åˆ«å®ƒä»¬çš„ç±»å‹å’Œæ—¶åºç‰¹å¾ã€‚æœ¬æ–‡å±•ç¤ºäº†ç»“åˆæ¨ç†å¢å¼ºè¯­éŸ³è´¨é‡è¯„ä¼°å‡†ç¡®æ€§å’Œå¯é æ€§çš„æ½œåŠ›ã€‚æ•°æ®é›†å°†åœ¨<a target="_blank" rel="noopener" href="https://huggingface.co/datasets/tsinghua-ee/QualiSpeech%E5%8F%91%E5%B8%83%E3%80%82">https://huggingface.co/datasets/tsinghua-ee/QualiSpeechå‘å¸ƒã€‚</a></p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>è¯¥è®ºæ–‡æ¢ç´¢äº†ä¸€ç§åˆ©ç”¨è‡ªç„¶è¯­è¨€æè¿°è¿›è¡Œè¯­éŸ³è´¨é‡è¯„ä¼°çš„æ–°æ–¹æ³•ã€‚</li>
<li>æ­¤æ–¹æ³•èƒ½æä¾›æ›´åŠ ä¸°å¯Œå’Œå¾®å¦™çš„è§è§£ï¼Œè¶…è¶Šä¼ ç»Ÿçš„æ•°å­—è¯„åˆ†æ–¹æ³•ã€‚</li>
<li>QualiSpeechæ•°æ®é›†è§£å†³äº†ç°æœ‰æ•°æ®é›†ç¼ºä¹å…¨é¢æ³¨é‡Šçš„é—®é¢˜ï¼Œæ¶µç›–ä½çº§åˆ«è¯­éŸ³è´¨é‡è¯„ä¼°çš„å¤šä¸ªå…³é”®æ–¹é¢ã€‚</li>
<li>QualiSpeech Benchmarkç”¨äºè¯„ä¼°å¬è§‰å¤§å‹è¯­è¨€æ¨¡å‹çš„ä½çº§åˆ«è¯­éŸ³ç†è§£èƒ½åŠ›ã€‚</li>
<li>å®éªŒç»“æœè¡¨æ˜ï¼Œç»è¿‡å¾®è°ƒçš„å¤§å‹è¯­è¨€æ¨¡å‹èƒ½å¤Ÿè¯¦ç»†åœ°æè¿°å™ªå£°å’Œå¤±çœŸã€‚</li>
<li>ç»“åˆæ¨ç†å¯å¢å¼ºè¯­éŸ³è´¨é‡è¯„ä¼°çš„å‡†ç¡®æ€§å’Œå¯é æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.20290">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-815bfe49661757c156007494bda44c2d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-91978749794794dfc280d966b6381780.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-954ad441dd32fc16510f9c15b46f45e5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-92f61a13cedc03a3f0f000df2f33e779.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-9b4cb303644bc2e13b7ba7798e2e41c6.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Reasoning-and-Learning-a-Perceptual-Metric-for-Self-Training-of-Reflective-Objects-in-Bin-Picking-with-a-Low-cost-Camera"><a href="#Reasoning-and-Learning-a-Perceptual-Metric-for-Self-Training-of-Reflective-Objects-in-Bin-Picking-with-a-Low-cost-Camera" class="headerlink" title="Reasoning and Learning a Perceptual Metric for Self-Training of   Reflective Objects in Bin-Picking with a Low-cost Camera"></a>Reasoning and Learning a Perceptual Metric for Self-Training of   Reflective Objects in Bin-Picking with a Low-cost Camera</h2><p><strong>Authors:Peiyuan Ni, Chee Meng Chew, Marcelo H. Ang Jr., Gregory S. Chirikjian</strong></p>
<p>Bin-picking of metal objects using low-cost RGB-D cameras often suffers from sparse depth information and reflective surface textures, leading to errors and the need for manual labeling. To reduce human intervention, we propose a two-stage framework consisting of a metric learning stage and a self-training stage. Specifically, to automatically process data captured by a low-cost camera (LC), we introduce a Multi-object Pose Reasoning (MoPR) algorithm that optimizes pose hypotheses under depth, collision, and boundary constraints. To further refine pose candidates, we adopt a Symmetry-aware Lie-group based Bayesian Gaussian Mixture Model (SaL-BGMM), integrated with the Expectation-Maximization (EM) algorithm, for symmetry-aware filtering. Additionally, we propose a Weighted Ranking Information Noise Contrastive Estimation (WR-InfoNCE) loss to enable the LC to learn a perceptual metric from reconstructed data, supporting self-training on untrained or even unseen objects. Experimental results show that our approach outperforms several state-of-the-art methods on both the ROBI dataset and our newly introduced Self-ROBI dataset. </p>
<blockquote>
<p>ä½¿ç”¨ä½æˆæœ¬RGB-Dç›¸æœºè¿›è¡Œé‡‘å±ç‰©ä½“çš„æŠ“å–ç»å¸¸å‡ºç°æ·±åº¦ä¿¡æ¯ç¨€ç–å’Œè¡¨é¢çº¹ç†åå°„çš„é—®é¢˜ï¼Œä»è€Œå¯¼è‡´è¯¯å·®å¹¶éœ€è¦æ‰‹åŠ¨æ ‡æ³¨ã€‚ä¸ºäº†å‡å°‘äººå·¥å¹²é¢„ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ç”±åº¦é‡å­¦ä¹ é˜¶æ®µå’Œè‡ªè®­ç»ƒé˜¶æ®µç»„æˆçš„ä¸¤é˜¶æ®µæ¡†æ¶ã€‚å…·ä½“æ¥è¯´ï¼Œä¸ºäº†è‡ªåŠ¨å¤„ç†ä½æˆæœ¬ç›¸æœºï¼ˆLCï¼‰æ•è·çš„æ•°æ®ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§å¤šç›®æ ‡å§¿æ€æ¨ç†ï¼ˆMoPRï¼‰ç®—æ³•ï¼Œè¯¥ç®—æ³•åœ¨æ·±åº¦ã€ç¢°æ’å’Œè¾¹ç•Œçº¦æŸä¸‹ä¼˜åŒ–å§¿æ€å‡è®¾ã€‚ä¸ºäº†è¿›ä¸€æ­¥ç»†åŒ–å§¿æ€å€™é€‰ï¼Œæˆ‘ä»¬é‡‡ç”¨äº†ä¸€ç§ç»“åˆæœŸæœ›æœ€å¤§åŒ–ï¼ˆEMï¼‰ç®—æ³•çš„å¯¹ç§°æ„ŸçŸ¥æç¾¤åŸºè´å¶æ–¯é«˜æ–¯æ··åˆæ¨¡å‹ï¼ˆSaL-BGMMï¼‰ï¼Œç”¨äºå¯¹ç§°æ„ŸçŸ¥æ»¤æ³¢ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜æå‡ºäº†åŠ æƒæ’åä¿¡æ¯å™ªå£°å¯¹æ¯”ä¼°è®¡ï¼ˆWR-InfoNCEï¼‰æŸå¤±ï¼Œä½¿ä½æˆæœ¬ç›¸æœºèƒ½å¤Ÿä»é‡å»ºæ•°æ®ä¸­å­¦ä¹ æ„ŸçŸ¥åº¦é‡ï¼Œæ”¯æŒåœ¨æœªè®­ç»ƒç”šè‡³æœªè§è¿‡çš„å¯¹è±¡ä¸Šè¿›è¡Œè‡ªè®­ç»ƒã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨ROBIæ•°æ®é›†å’Œæˆ‘ä»¬æ–°å¼•å…¥çš„Self-ROBIæ•°æ®é›†ä¸Šéƒ½ä¼˜äºå‡ ç§æœ€æ–°æ–¹æ³•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.20207v1">PDF</a> 9 pages, 10 figures</p>
<p><strong>Summary</strong></p>
<p>åŸºäºä½æˆæœ¬RGB-Dç›¸æœºè¿›è¡Œé‡‘å±ç‰©ä½“çš„åˆ†æ‹£å¸¸å¸¸å—åˆ°æ·±åº¦ä¿¡æ¯ç¨€ç–å’Œè¡¨é¢çº¹ç†åå°„çš„å½±å“ï¼Œå¯¼è‡´è¯¯å·®å¹¶éœ€è¦äººå·¥æ ‡æ³¨ã€‚ä¸ºå‡å°‘äººå·¥å¹²é¢„ï¼Œæˆ‘ä»¬æå‡ºä¸€ä¸ªä¸¤é˜¶æ®µçš„æ¡†æ¶ï¼ŒåŒ…æ‹¬åº¦é‡å­¦ä¹ é˜¶æ®µå’Œè‡ªæˆ‘è®­ç»ƒé˜¶æ®µã€‚æˆ‘ä»¬å¼•å…¥å¤šç›®æ ‡å§¿æ€æ¨ç†ç®—æ³•ï¼Œä¼˜åŒ–åœ¨æ·±åº¦ã€ç¢°æ’å’Œè¾¹ç•Œçº¦æŸä¸‹çš„å§¿æ€å‡è®¾ã€‚ä¸ºè¿›ä¸€æ­¥ä¼˜åŒ–å§¿æ€å€™é€‰ï¼Œæˆ‘ä»¬é‡‡ç”¨ç»“åˆæœŸæœ›æœ€å¤§åŒ–ç®—æ³•çš„å¯¹ç§°æ„ŸçŸ¥æç¾¤åŸºé«˜æ–¯æ··åˆæ¨¡å‹ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æå‡ºåŠ æƒæ’åä¿¡æ¯å™ªå£°å¯¹æ¯”ä¼°è®¡æŸå¤±ï¼Œä½¿ä½æˆæœ¬ç›¸æœºèƒ½ä»é‡å»ºæ•°æ®ä¸­å­¦ä¹ æ„ŸçŸ¥åº¦é‡ï¼Œæ”¯æŒåœ¨æœªè®­ç»ƒç”šè‡³æœªè§è¿‡çš„ç‰©ä½“ä¸Šè¿›è¡Œè‡ªæˆ‘è®­ç»ƒã€‚å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•ä¼˜äºå¤šä¸ªå…ˆè¿›æ–¹æ³•åœ¨ROBIå’Œè‡ªæˆ‘æ–°å¼•å…¥çš„Self-ROBIæ•°æ®é›†ä¸Šçš„è¡¨ç°ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ä½æˆæœ¬RGB-Dç›¸æœºåœ¨é‡‘å±ç‰©ä½“åˆ†æ‹£ä¸­é¢ä¸´æ·±åº¦ä¿¡æ¯ç¨€ç–å’Œè¡¨é¢çº¹ç†åå°„çš„æŒ‘æˆ˜ã€‚</li>
<li>æå‡ºä¸€ä¸ªä¸¤é˜¶æ®µæ¡†æ¶ï¼ŒåŒ…æ‹¬åº¦é‡å­¦ä¹ é˜¶æ®µå’Œè‡ªæˆ‘è®­ç»ƒé˜¶æ®µä»¥å‡å°‘äººå·¥å¹²é¢„ã€‚</li>
<li>å¼•å…¥å¤šç›®æ ‡å§¿æ€æ¨ç†ç®—æ³•ï¼Œä¼˜åŒ–åœ¨å¤šç§çº¦æŸä¸‹çš„å§¿æ€å‡è®¾ã€‚</li>
<li>é‡‡ç”¨å¯¹ç§°æ„ŸçŸ¥æç¾¤åŸºé«˜æ–¯æ··åˆæ¨¡å‹è¿›ä¸€æ­¥ä¼˜åŒ–å§¿æ€å€™é€‰ã€‚</li>
<li>æå‡ºåŠ æƒæ’åä¿¡æ¯å™ªå£°å¯¹æ¯”ä¼°è®¡æŸå¤±ï¼Œä½¿ä½æˆæœ¬ç›¸æœºèƒ½ä»é‡å»ºæ•°æ®ä¸­å­¦ä¹ æ„ŸçŸ¥åº¦é‡ã€‚</li>
<li>æ–¹æ³•åœ¨ROBIæ•°æ®é›†å’Œè‡ªæˆ‘å¼•å…¥çš„Self-ROBIæ•°æ®é›†ä¸Šè¡¨ç°ä¼˜è¶Šã€‚</li>
<li>è¯¥æ–¹æ³•èƒ½å¤Ÿå¤„ç†æœªè®­ç»ƒç”šè‡³æœªè§è¿‡çš„ç‰©ä½“ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.20207">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-709ced2d1beed00bd8f6841e363b89d2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6d91166e5457d491b11cdf0e97c00141.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-bcf06642bb7dad461540d1ac3ea69bed.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-32680568ee79b1647cc48a26d36d07c0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b90dacbf45eea5d1e1f8681278a9e833.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ee7049c37ab8d2e3a80a579d324c13ee.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="Open-Deep-Search-Democratizing-Search-with-Open-source-Reasoning-Agents"><a href="#Open-Deep-Search-Democratizing-Search-with-Open-source-Reasoning-Agents" class="headerlink" title="Open Deep Search: Democratizing Search with Open-source Reasoning Agents"></a>Open Deep Search: Democratizing Search with Open-source Reasoning Agents</h2><p><strong>Authors:Salaheddin Alzubi, Creston Brooks, Purva Chiniya, Edoardo Contente, Chiara von Gerlach, Lucas Irwin, Yihan Jiang, Arda Kaz, Windsor Nguyen, Sewoong Oh, Himanshu Tyagi, Pramod Viswanath</strong></p>
<p>We introduce Open Deep Search (ODS) to close the increasing gap between the proprietary search AI solutions, such as Perplexityâ€™s Sonar Reasoning Pro and OpenAIâ€™s GPT-4o Search Preview, and their open-source counterparts. The main innovation introduced in ODS is to augment the reasoning capabilities of the latest open-source LLMs with reasoning agents that can judiciously use web search tools to answer queries. Concretely, ODS consists of two components that work with a base LLM chosen by the user: Open Search Tool and Open Reasoning Agent. Open Reasoning Agent interprets the given task and completes it by orchestrating a sequence of actions that includes calling tools, one of which is the Open Search Tool. Open Search Tool is a novel web search tool that outperforms proprietary counterparts. Together with powerful open-source reasoning LLMs, such as DeepSeek-R1, ODS nearly matches and sometimes surpasses the existing state-of-the-art baselines on two benchmarks: SimpleQA and FRAMES. For example, on the FRAMES evaluation benchmark, ODS improves the best existing baseline of the recently released GPT-4o Search Preview by 9.7% in accuracy. ODS is a general framework for seamlessly augmenting any LLMs â€“ for example, DeepSeek-R1 that achieves 82.4% on SimpleQA and 30.1% on FRAMES â€“ with search and reasoning capabilities to achieve state-of-the-art performance: 88.3% on SimpleQA and 75.3% on FRAMES. </p>
<blockquote>
<p>æˆ‘ä»¬æ¨å‡ºOpen Deep Searchï¼ˆODSï¼‰ï¼Œä»¥ç¼©å°ä¸“æœ‰æœç´¢äººå·¥æ™ºèƒ½è§£å†³æ–¹æ¡ˆï¼ˆå¦‚Perplexityçš„Sonar Reasoning Proå’ŒOpenAIçš„GPT-4o Search Previewï¼‰å’Œå¼€æºè§£å†³æ–¹æ¡ˆä¹‹é—´çš„æ—¥ç›Šæ‰©å¤§çš„å·®è·ã€‚ODSçš„ä¸»è¦åˆ›æ–°ä¹‹å¤„åœ¨äºï¼Œå®ƒä½¿ç”¨æ¨ç†ä»£ç†å¢å¼ºäº†æœ€æ–°å¼€æºå¤§å‹è¯­è¨€æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ï¼Œè¿™äº›ä»£ç†èƒ½å¤Ÿå®¡æ…åœ°ä½¿ç”¨ç½‘ç»œæœç´¢å·¥å…·æ¥å›ç­”é—®é¢˜ã€‚å…·ä½“æ¥è¯´ï¼ŒODSåŒ…å«ä¸¤ä¸ªä¸ç”¨æˆ·é€‰æ‹©çš„åŸºç¡€å¤§å‹è¯­è¨€æ¨¡å‹ä¸€èµ·å·¥ä½œçš„ç»„ä»¶ï¼šOpen Search Toolå’ŒOpen Reasoning Agentã€‚Open Reasoning Agentè§£é‡Šç»™å®šä»»åŠ¡å¹¶å®Œæˆä»»åŠ¡ï¼Œé€šè¿‡åè°ƒä¸€ç³»åˆ—åŠ¨ä½œæ¥å®Œæˆä»»åŠ¡ï¼ŒåŒ…æ‹¬è°ƒç”¨å·¥å…·ï¼Œå…¶ä¸­ä¹‹ä¸€æ˜¯Open Search Toolã€‚Open Search Toolæ˜¯ä¸€æ¬¾æ–°å‹çš„ç½‘é¡µæœç´¢å·¥å…·ï¼Œå…¶æ€§èƒ½è¶…è¿‡äº†ä¸“æœ‰å·¥å…·ã€‚ç»“åˆå¼ºå¤§çš„å¼€æºæ¨ç†å¤§å‹è¯­è¨€æ¨¡å‹ï¼Œå¦‚DeepSeek-R1ï¼ŒODSåœ¨SimpleQAå’ŒFRAMESä¸¤ä¸ªåŸºå‡†æµ‹è¯•ä¸Šçš„è¡¨ç°å‡ ä¹è¾¾åˆ°äº†ç°æœ‰æœ€å…ˆè¿›çš„æ°´å¹³ï¼Œæœ‰æ—¶ç”šè‡³è¶…è¿‡äº†å®ƒä»¬ã€‚ä¾‹å¦‚ï¼Œåœ¨FRAMESè¯„ä¼°åŸºå‡†æµ‹è¯•ä¸­ï¼ŒODSæé«˜äº†æœ€è¿‘å‘å¸ƒçš„GPT-4o Search Previewçš„æœ€ä½³ç°æœ‰åŸºå‡†æµ‹è¯•çš„å‡†ç¡®æ€§ï¼Œæé«˜äº†9.7%ã€‚ODSæ˜¯ä¸€ä¸ªé€šç”¨æ¡†æ¶ï¼Œå¯ä»¥æ— ç¼åœ°å¢å¼ºä»»ä½•å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆä¾‹å¦‚DeepSeek-R1ï¼‰ï¼Œåœ¨SimpleQAä¸Šå®ç°82.4%çš„å‡†ç¡®ç‡ï¼Œåœ¨FRAMESä¸Šå®ç°30.1%çš„å‡†ç¡®ç‡ï¼‰ï¼Œé€šè¿‡æœç´¢å’Œæ¨ç†èƒ½åŠ›è¾¾åˆ°æœ€æ–°æ€§èƒ½ï¼šåœ¨SimpleQAä¸Šè¾¾åˆ°88.3%çš„å‡†ç¡®ç‡ï¼Œåœ¨FRAMESä¸Šè¾¾åˆ°75.3%çš„å‡†ç¡®ç‡ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.20201v1">PDF</a> 27 pages, 8 figures, 4 tables</p>
<p><strong>æ‘˜è¦</strong></p>
<p>Open Deep Searchï¼ˆODSï¼‰æ—¨åœ¨ç¼©å°ä¸“æœ‰æœç´¢AIè§£å†³æ–¹æ¡ˆä¸å…¶å¼€æºå¯¹åº”ç‰©ä¹‹é—´çš„æ—¥ç›Šå¢é•¿çš„å·®è·ã€‚ODSçš„ä¸»è¦åˆ›æ–°ä¹‹å¤„åœ¨äºé€šè¿‡æ¨ç†ä»£ç†å¢å¼ºæœ€æ–°å¼€æºå¤§å‹è¯­è¨€æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ï¼Œè¿™äº›ä»£ç†èƒ½å¤Ÿå®¡æ…åœ°ä½¿ç”¨ç½‘ç»œæœç´¢å·¥å…·æ¥å›ç­”é—®é¢˜ã€‚ODSåŒ…å«ä¸¤ä¸ªç»„ä»¶ï¼šç”¨æˆ·é€‰æ‹©çš„åŸºäºå¤§å‹è¯­è¨€æ¨¡å‹çš„Open Search Toolå’ŒOpen Reasoning Agentã€‚Open Reasoning Agentè§£é‡Šç»™å®šä»»åŠ¡å¹¶é€šè¿‡åè°ƒä¸€ç³»åˆ—åŠ¨ä½œæ¥å®Œæˆå®ƒï¼Œå…¶ä¸­åŒ…æ‹¬è°ƒç”¨å·¥å…·ï¼Œå…¶ä¸­ä¹‹ä¸€æ˜¯Open Search Toolã€‚Open Search Toolæ˜¯ä¸€ç§æ–°å‹çš„ç½‘é¡µæœç´¢å·¥å…·ï¼Œå…¶æ€§èƒ½è¶…è¿‡äº†ä¸“æœ‰å·¥å…·ã€‚é€šè¿‡ä¸å¼ºå¤§çš„å¼€æºæ¨ç†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆå¦‚DeepSeek-R1ï¼‰çš„ç»“åˆï¼ŒODSåœ¨SimpleQAå’ŒFRAMESä¸¤ä¸ªåŸºå‡†æµ‹è¯•ä¸Šçš„è¡¨ç°è¿‘ä¹è¾¾åˆ°äº†æœ€æ–°çš„æŠ€æœ¯æ°´å¹³ï¼Œæœ‰æ—¶ç”šè‡³è¶…è¶Šäº†å®ƒä»¬ã€‚ä¾‹å¦‚ï¼Œåœ¨FRAMESè¯„ä¼°åŸºå‡†æµ‹è¯•ä¸­ï¼ŒODSæé«˜äº†æœ€è¿‘å‘å¸ƒçš„GPT-4o Search Previewæœ€ä½³ç°æœ‰åŸºå‡†çš„å‡†ç¡®åº¦9.7%ã€‚ODSæ˜¯ä¸€ä¸ªé€šç”¨æ¡†æ¶ï¼Œå¯ä»¥æ— ç¼åœ°å¢å¼ºä»»ä½•å¤§å‹è¯­è¨€æ¨¡å‹çš„æœç´¢å’Œæ¨ç†èƒ½åŠ›ï¼Œä»¥å®ç°æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>Open Deep Search (ODS) æ—¨åœ¨ç¼©å°ä¸“æœ‰æœç´¢AIè§£å†³æ–¹æ¡ˆä¸å¼€æºè§£å†³æ–¹æ¡ˆä¹‹é—´çš„å·®è·ã€‚</li>
<li>ODSé€šè¿‡å¼•å…¥æ¨ç†ä»£ç†ï¼Œå¢å¼ºäº†å¼€æºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„æ¨ç†èƒ½åŠ›ã€‚</li>
<li>ODSåŒ…å«ä¸¤ä¸ªç»„ä»¶ï¼šOpen Search Toolå’ŒOpen Reasoning Agentï¼Œå®ƒä»¬ä¸ç”¨æˆ·é€‰æ‹©çš„åŸºç¡€LLMä¸€èµ·å·¥ä½œã€‚</li>
<li>Open Search Toolæ˜¯ä¸€ç§æ–°å‹çš„ç½‘é¡µæœç´¢å·¥å…·ï¼Œå…¶æ€§èƒ½è¶…è¿‡äº†ä¸“æœ‰æœç´¢å·¥å…·ã€‚</li>
<li>ODSèƒ½ä¸å¼ºå¤§çš„å¼€æºæ¨ç†LLMsï¼ˆå¦‚DeepSeek-R1ï¼‰ç»“åˆï¼Œæé«˜æœç´¢å’Œæ¨ç†èƒ½åŠ›ã€‚</li>
<li>åœ¨SimpleQAå’ŒFRAMESä¸¤ä¸ªåŸºå‡†æµ‹è¯•ä¸­ï¼ŒODSçš„è¡¨ç°è¿‘ä¹è¾¾åˆ°äº†æœ€æ–°çš„æŠ€æœ¯æ°´å¹³ï¼Œæœ‰æ—¶ç”šè‡³è¶…è¶Šå®ƒä»¬ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.20201">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-9061116c74cb80a4bad85414dafeb7b4.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-87f80bd75d02ee70f8ab80688e2d8493.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0ddf9d855f51b54d4bfe29201f11866b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-98887c303b6fbdc9355ef1f80e3a4341.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-ab30b0eba029d65e0ca1119bd6e746ce.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="Synthesizing-world-models-for-bilevel-planning"><a href="#Synthesizing-world-models-for-bilevel-planning" class="headerlink" title="Synthesizing world models for bilevel planning"></a>Synthesizing world models for bilevel planning</h2><p><strong>Authors:Zergham Ahmed, Joshua B. Tenenbaum, Christopher J. Bates, Samuel J. Gershman</strong></p>
<p>Modern reinforcement learning (RL) systems have demonstrated remarkable capabilities in complex environments, such as video games. However, they still fall short of achieving human-like sample efficiency and adaptability when learning new domains. Theory-based reinforcement learning (TBRL) is an algorithmic framework specifically designed to address this gap. Modeled on cognitive theories, TBRL leverages structured, causal world models - â€œtheoriesâ€ - as forward simulators for use in planning, generalization and exploration. Although current TBRL systems provide compelling explanations of how humans learn to play video games, they face several technical limitations: their theory languages are restrictive, and their planning algorithms are not scalable. To address these challenges, we introduce TheoryCoder, an instantiation of TBRL that exploits hierarchical representations of theories and efficient program synthesis methods for more powerful learning and planning. TheoryCoder equips agents with general-purpose abstractions (e.g., â€œmove toâ€), which are then grounded in a particular environment by learning a low-level transition model (a Python program synthesized from observations by a large language model). A bilevel planning algorithm can exploit this hierarchical structure to solve large domains. We demonstrate that this approach can be successfully applied to diverse and challenging grid-world games, where approaches based on directly synthesizing a policy perform poorly. Ablation studies demonstrate the benefits of using hierarchical abstractions. </p>
<blockquote>
<p>ç°ä»£å¼ºåŒ–å­¦ä¹ ç³»ç»Ÿå·²åœ¨è§†é¢‘æ¸¸æˆç­‰å¤æ‚ç¯å¢ƒä¸­å±•ç°å‡ºå“è¶Šçš„èƒ½åŠ›ã€‚ç„¶è€Œï¼Œåœ¨å­¦ä¹ æ–°é¢†åŸŸæ—¶ï¼Œå®ƒä»¬ä»æœªèƒ½å®ç°ä¸äººç±»æ ·æœ¬æ•ˆç‡å’Œé€‚åº”æ€§çš„åŒ¹é…ã€‚åŸºäºç†è®ºçš„å¼ºåŒ–å­¦ä¹ ï¼ˆTBRLï¼‰æ˜¯ä¸€ç§ä¸“é—¨è®¾è®¡ç”¨äºè§£å†³è¿™ä¸€å·®è·çš„ç®—æ³•æ¡†æ¶ã€‚TBRLä»¥è®¤çŸ¥ç†è®ºä¸ºæ¨¡å‹ï¼Œåˆ©ç”¨ç»“æ„åŒ–çš„å› æœä¸–ç•Œæ¨¡å‹ï¼ˆâ€œç†è®ºâ€ï¼‰ä½œä¸ºç”¨äºè§„åˆ’ã€æ³›åŒ–å’Œæ¢ç´¢çš„å‰å‘æ¨¡æ‹Ÿå™¨ã€‚å°½ç®¡å½“å‰çš„TBRLç³»ç»Ÿä¸ºäººç±»å¦‚ä½•å­¦ä¹ ç©æ¸¸æˆæä¾›äº†ä»¤äººä¿¡æœçš„è§£é‡Šï¼Œä½†å®ƒä»¬ä»é¢ä¸´ä¸€äº›æŠ€æœ¯ä¸Šçš„å±€é™æ€§ï¼šå®ƒä»¬çš„ç†è®ºè¯­è¨€å…·æœ‰é™åˆ¶æ€§ï¼Œå¹¶ä¸”å®ƒä»¬çš„è§„åˆ’ç®—æ³•å¹¶ä¸å¯æ‰©å±•ã€‚ä¸ºäº†åº”å¯¹è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†TheoryCoderï¼Œè¿™æ˜¯TBRLçš„ä¸€ä¸ªå®ä¾‹ï¼Œå®ƒåˆ©ç”¨ç†è®ºå±‚æ¬¡è¡¨ç¤ºå’Œé«˜æ•ˆçš„ç¨‹åºç»¼åˆæ–¹æ³•æ¥å®ç°æ›´å¼ºå¤§çš„å­¦ä¹ å’Œè§„åˆ’ã€‚TheoryCoderä¸ºä»£ç†æä¾›é€šç”¨æŠ½è±¡ï¼ˆä¾‹å¦‚ï¼Œâ€œç§»åŠ¨â€ï¼‰ï¼Œç„¶åé€šè¿‡å­¦ä¹ ç‰¹å®šç¯å¢ƒçš„åº•å±‚è½¬æ¢æ¨¡å‹ï¼ˆç”±å¤§å‹è¯­è¨€æ¨¡å‹ä»è§‚å¯Ÿä¸­åˆæˆçš„Pythonç¨‹åºï¼‰æ¥å°†è¿™äº›æŠ½è±¡ä¸ç‰¹å®šç¯å¢ƒç›¸è”ç³»ã€‚ä¸¤çº§è§„åˆ’ç®—æ³•å¯ä»¥åˆ©ç”¨è¿™ç§å±‚æ¬¡ç»“æ„æ¥è§£å†³å¤§è§„æ¨¡é¢†åŸŸé—®é¢˜ã€‚æˆ‘ä»¬è¯æ˜ï¼Œè¯¥æ–¹æ³•å¯ä»¥æˆåŠŸåº”ç”¨äºå¤šæ ·ä¸”å…·æœ‰æŒ‘æˆ˜æ€§çš„ç½‘æ ¼ä¸–ç•Œæ¸¸æˆï¼Œè€ŒåŸºäºç›´æ¥åˆæˆç­–ç•¥çš„æ–¹æ³•åœ¨è¿™é‡Œè¡¨ç°ä¸ä½³ã€‚æ¶ˆèç ”ç©¶è¯æ˜äº†ä½¿ç”¨å±‚æ¬¡æŠ½è±¡çš„ä¼˜åŠ¿ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.20124v1">PDF</a> 25 pages</p>
<p><strong>Summary</strong>ï¼šç°ä»£å¼ºåŒ–å­¦ä¹ ç³»ç»Ÿåœ¨æ¸¸æˆç­‰å¤æ‚ç¯å¢ƒä¸­è¡¨ç°å‡ºå“è¶Šçš„èƒ½åŠ›ï¼Œä½†åœ¨å­¦ä¹ æ–°é¢†åŸŸæ—¶ä»ç¼ºä¹äººç±»æ ·æœ¬æ•ˆç‡å’Œé€‚åº”æ€§ã€‚ç†è®ºå¼ºåŒ–å­¦ä¹ ï¼ˆTBRLï¼‰æ˜¯ä¸€ç§ä¸“é—¨é’ˆå¯¹è¿™ä¸€å·®è·è®¾è®¡çš„ç®—æ³•æ¡†æ¶ã€‚TBRLå€Ÿé‰´è®¤çŸ¥ç†è®ºï¼Œåˆ©ç”¨ç»“æ„åŒ–çš„å› æœä¸–ç•Œæ¨¡å‹ä½œä¸ºå‰å‘æ¨¡æ‹Ÿå™¨ï¼Œç”¨äºè§„åˆ’ã€æ³›åŒ–å’Œæ¢ç´¢ã€‚å°½ç®¡å½“å‰TBRLç³»ç»Ÿå¯¹äººç±»å¦‚ä½•å­¦ä¹ æ¸¸æˆæä¾›äº†æœ‰åŠ›çš„è§£é‡Šï¼Œä½†å®ƒä»¬é¢ä¸´æŠ€æœ¯ä¸Šçš„é™åˆ¶ï¼Œå¦‚ç†è®ºè¯­è¨€å—é™å’Œè§„åˆ’ç®—æ³•ä¸å¯æ‰©å±•æ€§ã€‚ä¸ºè§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†TheoryCoderï¼Œå®ƒæ˜¯TBRLçš„ä¸€ä¸ªå®ä¾‹ï¼Œåˆ©ç”¨ç†è®ºå±‚æ¬¡è¡¨ç¤ºå’Œé«˜æ•ˆç¨‹åºåˆæˆæ–¹æ³•ï¼Œå®ç°æ›´å¼ºå¤§çš„å­¦ä¹ å’Œè§„åˆ’ã€‚TheoryCoderä¸ºä»£ç†æä¾›é€šç”¨æŠ½è±¡ï¼ˆä¾‹å¦‚ï¼Œâ€œç§»åŠ¨â€ï¼‰ï¼Œç„¶åé€šè¿‡å­¦ä¹ ç‰¹å®šç¯å¢ƒçš„ä½çº§è½¬æ¢æ¨¡å‹ï¼ˆç”±å¤§å‹è¯­è¨€æ¨¡å‹ä»è§‚å¯Ÿä¸­åˆæˆçš„Pythonç¨‹åºï¼‰å°†å…¶å…·ä½“åŒ–ã€‚ä¸¤çº§è§„åˆ’ç®—æ³•å¯ä»¥åˆ©ç”¨è¿™ç§å±‚æ¬¡ç»“æ„æ¥è§£å†³å¤§è§„æ¨¡é¢†åŸŸçš„é—®é¢˜ã€‚æˆ‘ä»¬åœ¨å¤šæ ·åŒ–çš„ã€æœ‰æŒ‘æˆ˜æ€§çš„ç½‘æ ¼ä¸–ç•Œæ¸¸æˆä¸­å±•ç¤ºäº†è¯¥æ–¹æ³•çš„æˆåŠŸåº”ç”¨ï¼Œåœ¨è¿™äº›æ¸¸æˆä¸­ç›´æ¥åˆæˆç­–ç•¥çš„æ–¹æ³•è¡¨ç°ä¸ä½³ã€‚é€šè¿‡æ¶ˆå»ç ”ç©¶è¯æ˜ä½¿ç”¨å±‚æ¬¡æŠ½è±¡çš„å¥½å¤„ã€‚</p>
<p><strong>Key Takeaways</strong>:</p>
<ol>
<li>ç°ä»£å¼ºåŒ–å­¦ä¹ ç³»ç»Ÿåœ¨å¤æ‚ç¯å¢ƒä¸­è¡¨ç°å‡ºå“è¶Šæ€§èƒ½ï¼Œä½†ä»å­˜åœ¨æ ·æœ¬æ•ˆç‡å’Œé€‚åº”æ–°é¢†åŸŸçš„æŒ‘æˆ˜ã€‚</li>
<li>ç†è®ºå¼ºåŒ–å­¦ä¹ ï¼ˆTBRLï¼‰æ—¨åœ¨ç¼©å°è¿™ä¸€å·®è·ï¼Œå€Ÿé‰´è®¤çŸ¥ç†è®ºï¼Œåˆ©ç”¨ç»“æ„åŒ–å› æœä¸–ç•Œæ¨¡å‹è¿›è¡Œè§„åˆ’ã€æ³›åŒ–å’Œæ¢ç´¢ã€‚</li>
<li>å½“å‰TBRLç³»ç»Ÿé¢ä¸´æŠ€æœ¯é™åˆ¶ï¼Œå¦‚ç†è®ºè¯­è¨€çš„å±€é™æ€§å’Œè§„åˆ’ç®—æ³•çš„éæ‰©å±•æ€§ã€‚</li>
<li>TheoryCoderæ˜¯TBRLçš„ä¸€ä¸ªå®ä¾‹ï¼Œåˆ©ç”¨ç†è®ºå±‚æ¬¡è¡¨ç¤ºå’Œé«˜æ•ˆç¨‹åºåˆæˆæ–¹æ³•ï¼Œå®ç°æ›´å¼ºå¤§çš„å­¦ä¹ å’Œè§„åˆ’ã€‚</li>
<li>TheoryCoderä¸ºä»£ç†æä¾›é€šç”¨æŠ½è±¡ï¼Œå¹¶å°†å…¶å…·ä½“åŒ–äºç‰¹å®šç¯å¢ƒï¼Œé€šè¿‡ä¸¤çº§è§„åˆ’ç®—æ³•è§£å†³å¤§è§„æ¨¡é¢†åŸŸé—®é¢˜ã€‚</li>
<li>åœ¨å¤šæ ·åŒ–çš„ç½‘æ ¼ä¸–ç•Œæ¸¸æˆä¸­ï¼ŒTheoryCoderæˆåŠŸåº”ç”¨ï¼Œç›´æ¥åˆæˆç­–ç•¥çš„æ–¹æ³•åœ¨è¿™äº›æ¸¸æˆä¸­è¡¨ç°ä¸ä½³ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.20124">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-711eafcdf725162323c5ef58cf0e14af.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-307377f54b5b7d76075d00c8667597f1.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="Can-Multi-modal-reasoning-LLMs-work-as-deepfake-detectors"><a href="#Can-Multi-modal-reasoning-LLMs-work-as-deepfake-detectors" class="headerlink" title="Can Multi-modal (reasoning) LLMs work as deepfake detectors?"></a>Can Multi-modal (reasoning) LLMs work as deepfake detectors?</h2><p><strong>Authors:Simiao Ren, Yao Yao, Kidus Zewde, Zisheng Liang,  Tsang,  Ng, Ning-Yau Cheng, Xiaoou Zhan, Qinzhe Liu, Yifei Chen, Hengwei Xu</strong></p>
<p>Deepfake detection remains a critical challenge in the era of advanced generative models, particularly as synthetic media becomes more sophisticated. In this study, we explore the potential of state of the art multi-modal (reasoning) large language models (LLMs) for deepfake image detection such as (OpenAI O1&#x2F;4o, Gemini thinking Flash 2, Deepseek Janus, Grok 3, llama 3.2, Qwen 2&#x2F;2.5 VL, Mistral Pixtral, Claude 3.5&#x2F;3.7 sonnet) . We benchmark 12 latest multi-modal LLMs against traditional deepfake detection methods across multiple datasets, including recently published real-world deepfake imagery. To enhance performance, we employ prompt tuning and conduct an in-depth analysis of the modelsâ€™ reasoning pathways to identify key contributing factors in their decision-making process. Our findings indicate that best multi-modal LLMs achieve competitive performance with promising generalization ability with zero shot, even surpass traditional deepfake detection pipelines in out-of-distribution datasets while the rest of the LLM families performs extremely disappointing with some worse than random guess. Furthermore, we found newer model version and reasoning capabilities does not contribute to performance in such niche tasks of deepfake detection while model size do help in some cases. This study highlights the potential of integrating multi-modal reasoning in future deepfake detection frameworks and provides insights into model interpretability for robustness in real-world scenarios. </p>
<blockquote>
<p>æ·±åº¦ä¼ªé€ æ£€æµ‹åœ¨å…ˆè¿›çš„ç”Ÿæˆæ¨¡å‹æ—¶ä»£ä»ç„¶æ˜¯ä¸€ä¸ªå·¨å¤§çš„æŒ‘æˆ˜ï¼Œå°¤å…¶æ˜¯éšç€åˆæˆåª’ä½“å˜å¾—è¶Šæ¥è¶Šç²¾ç»†ã€‚åœ¨è¿™é¡¹ç ”ç©¶ä¸­ï¼Œæˆ‘ä»¬æ¢ç´¢äº†æœ€æ–°å…ˆè¿›çš„å¤šæ¨¡æ€ï¼ˆæ¨ç†ï¼‰å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨æ·±åº¦ä¼ªé€ å›¾åƒæ£€æµ‹æ–¹é¢çš„æ½œåŠ›ï¼Œå¦‚ï¼ˆOpenAI O1&#x2F;4oã€Gemini thinking Flash 2ã€Deepseek Janusã€Grok 3ã€llama 3.2ã€Qwen 2&#x2F;2.5 VLã€Mistral Pixtralã€Claude 3.5&#x2F;3.7 sonnetç­‰ï¼‰ã€‚æˆ‘ä»¬åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šå¯¹æ¯”äº†æœ€æ–°çš„12ä¸ªå¤šæ¨¡æ€LLMä¸ä¼ ç»Ÿæ·±åº¦ä¼ªé€ æ£€æµ‹æ–¹æ³•çš„æ€§èƒ½ï¼ŒåŒ…æ‹¬æœ€è¿‘å‘å¸ƒçš„ç°å®æ·±åº¦ä¼ªé€ å›¾åƒã€‚ä¸ºäº†æé«˜æ€§èƒ½ï¼Œæˆ‘ä»¬é‡‡ç”¨äº†æç¤ºè°ƒæ•´ï¼Œå¹¶å¯¹æ¨¡å‹çš„æ¨ç†è·¯å¾„è¿›è¡Œäº†æ·±å…¥åˆ†æï¼Œä»¥è¯†åˆ«å…¶å†³ç­–è¿‡ç¨‹ä¸­çš„å…³é”®å½±å“å› ç´ ã€‚æˆ‘ä»¬çš„ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œæœ€ä½³å¤šæ¨¡æ€LLMå…·æœ‰å¼ºå¤§çš„ç«äº‰åŠ›ï¼Œå…¶æ³›åŒ–èƒ½åŠ›å¼ºå¤§ä¸”èƒ½åœ¨é›¶æ ·æœ¬æƒ…å†µä¸‹å®ç°è‰¯å¥½æ€§èƒ½ï¼Œç”šè‡³åœ¨ç¦»ç¾¤æ•°æ®é›†ä¸Šè¶…è¶Šäº†ä¼ ç»Ÿæ·±åº¦ä¼ªé€ æ£€æµ‹ç®¡é“ï¼›è€Œå…¶ä»–LLMå®¶æ—çš„è¡¨ç°åˆ™ä»¤äººå¤±æœ›ï¼Œéƒ¨åˆ†æ¨¡å‹çš„è¡¨ç°ç”šè‡³ä¸åŠéšæœºçŒœæµ‹ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å‘ç°æ–°ç‰ˆæœ¬æ¨¡å‹å’Œæ¨ç†èƒ½åŠ›å¹¶ä¸æœ‰åŠ©äºæ·±åº¦ä¼ªé€ æ£€æµ‹è¿™ç±»ä¸“ä¸šä»»åŠ¡çš„æ€§èƒ½æå‡ï¼Œä½†åœ¨æŸäº›æƒ…å†µä¸‹æ¨¡å‹è§„æ¨¡ç¡®å®æœ‰å¸®åŠ©ã€‚æœ¬ç ”ç©¶å¼ºè°ƒäº†å°†å¤šæ¨¡æ€æ¨ç†é›†æˆåˆ°æœªæ¥æ·±åº¦ä¼ªé€ æ£€æµ‹æ¡†æ¶ä¸­çš„æ½œåŠ›ï¼Œå¹¶ä¸ºç°å®åœºæ™¯ä¸­çš„æ¨¡å‹ç¨³å¥æ€§æä¾›äº†è§è§£ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.20084v1">PDF</a> </p>
<p><strong>Summary</strong><br>åœ¨å…ˆè¿›ç”Ÿæˆæ¨¡å‹çš„æ—¶ä»£ï¼Œæ·±åº¦ä¼ªé€ æ£€æµ‹ä»ç„¶æ˜¯ä¸€ä¸ªé‡å¤§æŒ‘æˆ˜ï¼Œå°¤å…¶æ˜¯éšç€åˆæˆåª’ä½“è¶Šæ¥è¶Šç²¾ç»†ã€‚æœ¬ç ”ç©¶æ¢è®¨äº†æœ€æ–°å¤šæ¨¡æ€æ¨ç†å¤§å‹è¯­è¨€æ¨¡å‹åœ¨æ·±åº¦ä¼ªé€ å›¾åƒæ£€æµ‹æ–¹é¢çš„æ½œåŠ›ï¼Œå¹¶å¯¹å¤šæ¨¡æ€è¯­è¨€æ¨¡å‹ä¸ä¼ ç»Ÿçš„æ·±åº¦ä¼ªé€ æ£€æµ‹æ–¹æ³•è¿›è¡Œäº†å¯¹æ¯”åŸºå‡†æµ‹è¯•ã€‚é€šè¿‡æç¤ºè°ƒæ•´å’Œæ·±å…¥åˆ†ææ¨¡å‹çš„æ¨ç†è·¯å¾„ï¼Œæˆ‘ä»¬å‘ç°æœ€ä½³çš„å¤šæ¨¡æ€è¯­è¨€æ¨¡å‹å…·æœ‰å‡ºè‰²çš„æ³›åŒ–èƒ½åŠ›å’Œé›¶æ ·æœ¬è¡¨ç°ï¼Œç”šè‡³åœ¨æŸäº›æ–¹é¢è¶…è¶Šäº†ä¼ ç»Ÿçš„æ·±åº¦ä¼ªé€ æ£€æµ‹æµç¨‹ã€‚ç„¶è€Œï¼Œå¹¶éæ‰€æœ‰è¯­è¨€æ¨¡å‹ç‰ˆæœ¬åœ¨æ–°ä»»åŠ¡ä¸­éƒ½è¡¨ç°å‡ºæœ€ä½³æ€§èƒ½ã€‚æœ¬ç ”ç©¶çš„é‡ç‚¹æ˜¯æœªæ¥æ·±åº¦ä¼ªé€ æ£€æµ‹æ¡†æ¶ä¸­æ•´åˆå¤šæ¨¡æ€æ¨ç†çš„æ½œåŠ›ï¼Œå¹¶ä¸ºçœŸå®åœºæ™¯ä¸­æ¨¡å‹çš„ç¨³å¥æ€§æä¾›äº†è§è§£ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>åœ¨å…ˆè¿›çš„ç”Ÿæˆæ¨¡å‹æ—¶ä»£ï¼Œæ·±åº¦ä¼ªé€ æ£€æµ‹ä¾ç„¶æ˜¯ä¸€é¡¹é‡è¦æŒ‘æˆ˜ã€‚</li>
<li>ç ”ç©¶è¯„ä¼°äº†å¤šç§å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹åœ¨æ·±åº¦ä¼ªé€ æ£€æµ‹æ–¹é¢çš„æ½œåŠ›ã€‚</li>
<li>å¯¹æ¯”åŸºå‡†æµ‹è¯•è¡¨æ˜ï¼Œå¤šæ¨¡æ€è¯­è¨€æ¨¡å‹åœ¨æŸäº›æ–¹é¢è¶…è¶Šäº†ä¼ ç»Ÿæ–¹æ³•ã€‚</li>
<li>æœ€ä½³çš„å¤šæ¨¡æ€è¯­è¨€æ¨¡å‹å±•ç°å‡ºå‡ºè‰²çš„æ³›åŒ–èƒ½åŠ›å’Œé›¶æ ·æœ¬è¡¨ç°ã€‚</li>
<li>æ¨¡å‹å¤§å°åœ¨æŸäº›æƒ…å†µä¸‹æœ‰åŠ©äºæé«˜æ€§èƒ½ï¼Œä½†æ–°ç‰ˆæœ¬å’Œæ¨ç†èƒ½åŠ›ä¸ä¸€å®šæœ‰åŠ©äºæ·±åº¦ä¼ªé€ æ£€æµ‹ä»»åŠ¡ã€‚</li>
<li>ç ”ç©¶å¼ºè°ƒäº†æ•´åˆå¤šæ¨¡æ€æ¨ç†åœ¨æœªæ¥æ·±åº¦ä¼ªé€ æ£€æµ‹æ¡†æ¶ä¸­çš„æ½œåŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.20084">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-29a6af56185beae960b5d1210d3c89b6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2258fe28f22e6f5e4da123bb22d566bb.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3e3702b75ded1b6a1313ffba08a7d501.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b09c643a669dd2d7d54e844b85f8a9b7.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-2a09d65e30f4e6e291b2efa89e6c53df.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="LEGO-Puzzles-How-Good-Are-MLLMs-at-Multi-Step-Spatial-Reasoning"><a href="#LEGO-Puzzles-How-Good-Are-MLLMs-at-Multi-Step-Spatial-Reasoning" class="headerlink" title="LEGO-Puzzles: How Good Are MLLMs at Multi-Step Spatial Reasoning?"></a>LEGO-Puzzles: How Good Are MLLMs at Multi-Step Spatial Reasoning?</h2><p><strong>Authors:Kexian Tang, Junyao Gao, Yanhong Zeng, Haodong Duan, Yanan Sun, Zhening Xing, Wenran Liu, Kaifeng Lyu, Kai Chen</strong></p>
<p>Multi-step spatial reasoning entails understanding and reasoning about spatial relationships across multiple sequential steps, which is crucial for tackling complex real-world applications, such as robotic manipulation, autonomous navigation, and automated assembly. To assess how well current Multimodal Large Language Models (MLLMs) have acquired this fundamental capability, we introduce \textbf{LEGO-Puzzles}, a scalable benchmark designed to evaluate both \textbf{spatial understanding} and \textbf{sequential reasoning} in MLLMs through LEGO-based tasks. LEGO-Puzzles consists of 1,100 carefully curated visual question-answering (VQA) samples spanning 11 distinct tasks, ranging from basic spatial understanding to complex multi-step reasoning. Based on LEGO-Puzzles, we conduct a comprehensive evaluation of state-of-the-art MLLMs and uncover significant limitations in their spatial reasoning capabilities: even the most powerful MLLMs can answer only about half of the test cases, whereas human participants achieve over 90% accuracy. In addition to VQA tasks, we evaluate MLLMsâ€™ abilities to generate LEGO images following assembly illustrations. Our experiments show that only Gemini-2.0-Flash and GPT-4o exhibit a limited ability to follow these instructions, while other MLLMs either replicate the input image or generate completely irrelevant outputs. Overall, LEGO-Puzzles exposes critical deficiencies in existing MLLMsâ€™ spatial understanding and sequential reasoning capabilities, and underscores the need for further advancements in multimodal spatial reasoning. </p>
<blockquote>
<p>å¤šæ­¥ç©ºé—´æ¨ç†æ¶‰åŠç†è§£å’Œæ¨ç†è·¨å¤šä¸ªè¿ç»­æ­¥éª¤çš„ç©ºé—´å…³ç³»ï¼Œè¿™å¯¹äºåº”å¯¹ç°å®ä¸–ç•Œä¸­çš„å¤æ‚åº”ç”¨è‡³å…³é‡è¦ï¼Œä¾‹å¦‚æœºå™¨äººæ“ä½œã€è‡ªä¸»å¯¼èˆªå’Œè‡ªåŠ¨åŒ–ç»„è£…ã€‚ä¸ºäº†è¯„ä¼°å½“å‰çš„å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨è·å–è¿™ç§åŸºæœ¬èƒ½åŠ›æ–¹é¢çš„è¡¨ç°ï¼Œæˆ‘ä»¬å¼•å…¥äº†<strong>ä¹é«˜æ‹¼å›¾</strong>ï¼Œè¿™æ˜¯ä¸€ä¸ªå¯æ‰©å±•çš„åŸºå‡†æµ‹è¯•ï¼Œæ—¨åœ¨é€šè¿‡åŸºäºä¹é«˜çš„ä»»åŠ¡æ¥è¯„ä¼°MLLMsä¸­çš„<strong>ç©ºé—´ç†è§£</strong>å’Œ<strong>åºåˆ—æ¨ç†</strong>ã€‚ä¹é«˜æ‹¼å›¾ç”±ç²¾å¿ƒæŒ‘é€‰çš„1100ä¸ªè§†è§‰é—®ç­”ï¼ˆVQAï¼‰æ ·æœ¬ç»„æˆï¼Œæ¶µç›–11ä¸ªä¸åŒçš„ä»»åŠ¡ï¼Œä»åŸºæœ¬ç©ºé—´ç†è§£åˆ°å¤æ‚çš„å¤šæ­¥æ¨ç†ã€‚åŸºäºä¹é«˜æ‹¼å›¾ï¼Œæˆ‘ä»¬å¯¹æœ€æ–°çš„MLLMsè¿›è¡Œäº†å…¨é¢çš„è¯„ä¼°ï¼Œå¹¶å‘ç°äº†å®ƒä»¬åœ¨ç©ºé—´æ¨ç†èƒ½åŠ›ä¸Šçš„é‡å¤§å±€é™æ€§ï¼šå³ä½¿æ˜¯æœ€å¼ºå¤§çš„MLLMä¹Ÿåªèƒ½å›ç­”å¤§çº¦ä¸€åŠçš„æµ‹è¯•æ¡ˆä¾‹ï¼Œè€Œäººç±»å‚ä¸è€…çš„å‡†ç¡®ç‡è¶…è¿‡90%ã€‚é™¤äº†VQAä»»åŠ¡å¤–ï¼Œæˆ‘ä»¬è¿˜è¯„ä¼°äº†MLLMsæŒ‰ç…§ç»„è£…è¯´æ˜ç”Ÿæˆä¹é«˜å›¾åƒçš„èƒ½åŠ›ã€‚æˆ‘ä»¬çš„å®éªŒè¡¨æ˜ï¼Œåªæœ‰Gemini-2.0-Flashå’ŒGPT-4oè¡¨ç°å‡ºä¸€å®šçš„éµå¾ªæŒ‡ä»¤çš„èƒ½åŠ›ï¼Œè€Œå…¶ä»–MLLMsè¦ä¹ˆå¤åˆ¶è¾“å…¥å›¾åƒï¼Œè¦ä¹ˆç”Ÿæˆå®Œå…¨ä¸ç›¸å…³çš„è¾“å‡ºã€‚æ€»ä½“è€Œè¨€ï¼Œä¹é«˜æ‹¼å›¾æš´éœ²äº†ç°æœ‰MLLMsåœ¨ç©ºé—´ç†è§£å’Œåºåˆ—æ¨ç†èƒ½åŠ›æ–¹é¢çš„å…³é”®ç¼ºé™·ï¼Œå¹¶å¼ºè°ƒäº†å¯¹å¤šæ¨¡æ€ç©ºé—´æ¨ç†è¿›ä¸€æ­¥å‘å±•çš„éœ€æ±‚ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.19990v1">PDF</a> 12 pages, 7 figures</p>
<p><strong>Summary</strong>ï¼š<br>è¯¥ç ”ç©¶å¼•å…¥äº†LEGO-Puzzlesè¿™ä¸€å¯è¯„ä¼°å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰ç©ºé—´ç†è§£å’Œåºè´¯æ¨ç†èƒ½åŠ›çš„åŸºå‡†æµ‹è¯•ã€‚è¯¥æµ‹è¯•åŒ…å«11ä¸ªä¸åŒçš„ä»»åŠ¡ï¼Œæ¶µç›–ä»åŸºæœ¬ç©ºé—´ç†è§£åˆ°å¤æ‚å¤šæ­¥æ¨ç†ã€‚ç ”ç©¶è¯„ä¼°äº†æœ€å…ˆè¿›çš„MLLMsï¼Œå‘ç°å®ƒä»¬åœ¨ç©ºé—´æ¨ç†èƒ½åŠ›ä¸Šå­˜åœ¨æ˜¾è‘—å±€é™ï¼Œåªæœ‰éƒ¨åˆ†æ¨¡å‹èƒ½å¤Ÿå›ç­”æµ‹è¯•æ¡ˆä¾‹ä¸­çš„ä¸€åŠé—®é¢˜ï¼Œè€Œäººç±»å‚ä¸è€…çš„å‡†ç¡®ç‡è¶…è¿‡90%ã€‚æ­¤å¤–ï¼Œå®éªŒè¿˜è¯„ä¼°äº†MLLMsæ ¹æ®ç»„è£…è¯´æ˜ç”Ÿæˆä¹é«˜å›¾åƒçš„èƒ½åŠ›ï¼Œå‘ç°ä»…æœ‰éƒ¨åˆ†æ¨¡å‹å…·å¤‡æœ‰é™çš„èƒ½åŠ›éµå¾ªæŒ‡ä»¤ã€‚æ€»ä½“è€Œè¨€ï¼ŒLEGO-Puzzlesæ­ç¤ºäº†ç°æœ‰MLLMsåœ¨ç©ºé—´ç†è§£å’Œåºè´¯æ¨ç†èƒ½åŠ›ä¸Šçš„ä¸è¶³ï¼Œå¹¶å¼ºè°ƒäº†å¤šæ¨¡æ€ç©ºé—´æ¨ç†éœ€è¦è¿›ä¸€æ­¥å‘å±•çš„å¿…è¦æ€§ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>å¤šæ­¥ç©ºé—´æ¨ç†å¯¹äºè§£å†³çœŸå®ä¸–ç•Œåº”ç”¨å¦‚æœºå™¨äººæ“ä½œã€è‡ªä¸»å¯¼èˆªå’Œè‡ªåŠ¨åŒ–ç»„è£…è‡³å…³é‡è¦ã€‚</li>
<li>LEGO-Puzzlesæ˜¯ä¸€ä¸ªç”¨äºè¯„ä¼°å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰ç©ºé—´ç†è§£å’Œåºè´¯æ¨ç†èƒ½åŠ›çš„åŸºå‡†æµ‹è¯•ã€‚</li>
<li>MLLMsåœ¨ç©ºé—´æ¨ç†èƒ½åŠ›ä¸Šå­˜åœ¨å±€é™ï¼Œåªèƒ½å›ç­”ä¸€åŠå·¦å³çš„æµ‹è¯•æ¡ˆä¾‹ï¼Œè€Œäººç±»å‚ä¸è€…å‡†ç¡®ç‡è¶…è¿‡90%ã€‚</li>
<li>åœ¨æ ¹æ®ç»„è£…è¯´æ˜ç”Ÿæˆä¹é«˜å›¾åƒçš„ä»»åŠ¡ä¸­ï¼Œä»…æœ‰éƒ¨åˆ†MLLMså…·å¤‡æœ‰é™çš„èƒ½åŠ›éµå¾ªæŒ‡ä»¤ã€‚</li>
<li>LEGO-Puzzlesæ­ç¤ºäº†ç°æœ‰MLLMsåœ¨ç©ºé—´ç†è§£å’Œåºè´¯æ¨ç†èƒ½åŠ›ä¸Šçš„ä¸è¶³ã€‚</li>
<li>å¤šæ¨¡æ€ç©ºé—´æ¨ç†èƒ½åŠ›çš„å‘å±•æ˜¯å¿…è¦çš„ï¼Œä»¥æé«˜MLLMsçš„æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.19990">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-1956755a8f68485f05d9a86231af749f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-471a29ee537ddceb0039c982604742a2.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-919ae21a925c2fe7156ab8bc55ef0435.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-0bf2667e296c87c8ecd51a2c2d8633e3.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-ac643f2c00d654ff728148b95bfde648.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="ExCoT-Optimizing-Reasoning-for-Text-to-SQL-with-Execution-Feedback"><a href="#ExCoT-Optimizing-Reasoning-for-Text-to-SQL-with-Execution-Feedback" class="headerlink" title="ExCoT: Optimizing Reasoning for Text-to-SQL with Execution Feedback"></a>ExCoT: Optimizing Reasoning for Text-to-SQL with Execution Feedback</h2><p><strong>Authors:Bohan Zhai, Canwen Xu, Yuxiong He, Zhewei Yao</strong></p>
<p>Text-to-SQL demands precise reasoning to convert natural language questions into structured queries. While large language models (LLMs) excel in many reasoning tasks, their ability to leverage Chain-of-Thought (CoT) reasoning for text-to-SQL remains underexplored. We identify critical limitations: zero-shot CoT offers minimal gains, and Direct Preference Optimization (DPO) applied without CoT yields marginal improvements. We propose ExCoT, a novel framework that iteratively optimizes open-source LLMs by combining CoT reasoning with off-policy and on-policy DPO, relying solely on execution accuracy as feedback. This approach eliminates the need for reward models or human-annotated preferences.   Our experimental results demonstrate significant performance gains: ExCoT improves execution accuracy on BIRD dev set from 57.37% to 68.51% and on Spider test set from 78.81% to 86.59% for LLaMA-3 70B, with Qwen-2.5-Coder demonstrating similar improvements. Our best model achieves state-of-the-art performance in the single-model setting on both BIRD and Spider datasets, notably achieving 68.53% on the BIRD test set. </p>
<blockquote>
<p>æ–‡æœ¬åˆ°SQLè¦æ±‚ç²¾ç¡®æ¨ç†ï¼Œå°†è‡ªç„¶è¯­è¨€é—®é¢˜è½¬åŒ–ä¸ºç»“æ„åŒ–æŸ¥è¯¢ã€‚è™½ç„¶å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨è®¸å¤šæ¨ç†ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ï¼Œä½†å®ƒä»¬åˆ©ç”¨æ€ç»´é“¾ï¼ˆCoTï¼‰æ¨ç†è¿›è¡Œæ–‡æœ¬åˆ°SQLçš„èƒ½åŠ›ä»æœªè¢«å……åˆ†æ¢ç´¢ã€‚æˆ‘ä»¬å‘ç°äº†å…³é”®é™åˆ¶ï¼šé›¶é•œå¤´CoTæä¾›çš„å¢ç›Šæœ‰é™ï¼Œè€Œç›´æ¥åå¥½ä¼˜åŒ–ï¼ˆDPOï¼‰åœ¨ä¸ä½¿ç”¨CoTçš„æƒ…å†µä¸‹åº”ç”¨åªå¸¦æ¥å¾®å°çš„æ”¹è¿›ã€‚æˆ‘ä»¬æå‡ºäº†ExCoTï¼Œè¿™æ˜¯ä¸€ä¸ªæ–°å‹æ¡†æ¶ï¼Œå®ƒé€šè¿‡ç»“åˆCoTæ¨ç†ã€ç¦»ç­–ç•¥å’Œç°ç­–ç•¥çš„DPOæ¥è¿­ä»£ä¼˜åŒ–å¼€æºLLMï¼Œä»…ä¾èµ–æ‰§è¡Œå‡†ç¡®æ€§ä½œä¸ºåé¦ˆã€‚è¿™ç§æ–¹æ³•æ¶ˆé™¤äº†å¯¹å¥–åŠ±æ¨¡å‹æˆ–äººå·¥æ ‡æ³¨åå¥½çš„éœ€æ±‚ã€‚æˆ‘ä»¬çš„å®éªŒç»“æœè¡¨æ˜ï¼ŒExCoTå¸¦æ¥äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ï¼šåœ¨BIRDå¼€å‘é›†ä¸Šï¼ŒLLaMA-3 70Bçš„æ‰§è¡Œå‡†ç¡®æ€§ä»57.37%æé«˜åˆ°68.51%ï¼Œåœ¨Spideræµ‹è¯•é›†ä¸Šä»78.81%æé«˜åˆ°86.59%ï¼Œè€ŒQwen-2.5-Coderä¹Ÿæ˜¾ç¤ºå‡ºç±»ä¼¼çš„æ”¹è¿›ã€‚æˆ‘ä»¬æœ€å¥½çš„æ¨¡å‹åœ¨BIRDå’ŒSpideræ•°æ®é›†ä¸Šéƒ½è¾¾åˆ°äº†å•æ¨¡å‹è®¾ç½®çš„æœ€æ–°æ€§èƒ½æ°´å¹³ï¼Œç‰¹åˆ«æ˜¯åœ¨BIRDæµ‹è¯•é›†ä¸Šè¾¾åˆ°äº†68.53%ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.19988v1">PDF</a> </p>
<p><strong>Summary</strong>ï¼šæœ¬æ–‡æ¢è®¨äº†å°†è‡ªç„¶è¯­è¨€é—®é¢˜è½¬åŒ–ä¸ºç»“æ„åŒ–æŸ¥è¯¢çš„Text-to-SQLä»»åŠ¡ä¸­ï¼Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å¦‚ä½•åˆ©ç”¨Chain-of-Thoughtï¼ˆCoTï¼‰æ¨ç†æ¥æé«˜æ€§èƒ½çš„é—®é¢˜ã€‚æ–‡ç« æå‡ºExCoTæ¡†æ¶ï¼Œç»“åˆCoTæ¨ç†å’ŒDirect Preference Optimizationï¼ˆDPOï¼‰æŠ€æœ¯ï¼Œä»¥æ‰§è¡Œå‡†ç¡®æ€§ä¸ºåé¦ˆæ¥ä¼˜åŒ–LLMsã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒExCoTæ˜¾è‘—æé«˜äº†æ‰§è¡Œå‡†ç¡®æ€§ï¼Œè¾¾åˆ°äº†å…ˆè¿›æ€§èƒ½æ°´å¹³ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>Text-to-SQLè½¬æ¢éœ€è¦ç²¾ç¡®æ¨ç†ï¼Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨æ­¤ä»»åŠ¡ä¸­å…·æœ‰æ½œåŠ›ã€‚</li>
<li>Chain-of-Thoughtï¼ˆCoTï¼‰æ¨ç†åœ¨Text-to-SQLä»»åŠ¡ä¸­çš„åº”ç”¨å—åˆ°å…³æ³¨ï¼Œä½†ç›¸å…³ç ”ç©¶ä¸è¶³ã€‚</li>
<li>ExCoTæ¡†æ¶ç»“åˆCoTæ¨ç†å’ŒDirect Preference Optimizationï¼ˆDPOï¼‰æŠ€æœ¯ï¼Œä»¥æ‰§è¡Œå‡†ç¡®æ€§ä¸ºåé¦ˆä¼˜åŒ–LLMsã€‚</li>
<li>ExCoTæ¡†æ¶æ— éœ€å¥–åŠ±æ¨¡å‹æˆ–äººå·¥æ ‡æ³¨åå¥½ï¼Œé™ä½äº†å®æ–½éš¾åº¦ã€‚</li>
<li>å®éªŒç»“æœæ˜¾ç¤ºï¼ŒExCoTæé«˜äº†æ‰§è¡Œå‡†ç¡®æ€§ï¼Œåœ¨BIRDå’ŒSpideræ•°æ®é›†ä¸Šå–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚</li>
<li>ExCoTæ¡†æ¶è¾¾åˆ°äº†BIRDå’ŒSpideræ•°æ®é›†ä¸Šçš„å…ˆè¿›æ€§èƒ½æ°´å¹³ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.19988">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-0d071312b9a1d999a6341f6163826c8b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3109f5ee05fd5c4f48662184d5869af7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7248a70466a5235e74b4aa654510fae9.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="ST-VLM-Kinematic-Instruction-Tuning-for-Spatio-Temporal-Reasoning-in-Vision-Language-Models"><a href="#ST-VLM-Kinematic-Instruction-Tuning-for-Spatio-Temporal-Reasoning-in-Vision-Language-Models" class="headerlink" title="ST-VLM: Kinematic Instruction Tuning for Spatio-Temporal Reasoning in   Vision-Language Models"></a>ST-VLM: Kinematic Instruction Tuning for Spatio-Temporal Reasoning in   Vision-Language Models</h2><p><strong>Authors:Dohwan Ko, Sihyeon Kim, Yumin Suh, Vijay Kumar B. G, Minseo Yoon, Manmohan Chandraker, Hyunwoo J. Kim</strong></p>
<p>Spatio-temporal reasoning is essential in understanding real-world environments in various fields, eg, autonomous driving and sports analytics. Recent advances have improved the spatial reasoning ability of Vision-Language Models (VLMs) by introducing large-scale data, but these models still struggle to analyze kinematic elements like traveled distance and speed of moving objects. To bridge this gap, we construct a spatio-temporal reasoning dataset and benchmark involving kinematic instruction tuning, referred to as STKit and STKit-Bench. They consist of real-world videos with 3D annotations, detailing object motion dynamics: traveled distance, speed, movement direction, inter-object distance comparisons, and relative movement direction. To further scale such data construction to videos without 3D labels, we propose an automatic pipeline to generate pseudo-labels using 4D reconstruction in real-world scale. With our kinematic instruction tuning data for spatio-temporal reasoning, we present ST-VLM, a VLM enhanced for spatio-temporal reasoning, which exhibits outstanding performance on STKit-Bench. Furthermore, we show that ST-VLM generalizes robustly across diverse domains and tasks, outperforming baselines on other spatio-temporal benchmarks (eg, ActivityNet, TVQA+). Finally, by integrating learned spatio-temporal reasoning with existing abilities, ST-VLM enables complex multi-step reasoning. Project page: <a target="_blank" rel="noopener" href="https://ikodoh.github.io/ST-VLM">https://ikodoh.github.io/ST-VLM</a>. </p>
<blockquote>
<p>æ—¶ç©ºæ¨ç†åœ¨ç†è§£è‡ªä¸»é©¾é©¶ã€ä½“è‚²åˆ†æç­‰é¢†åŸŸä¸­çš„çœŸå®ä¸–ç•Œç¯å¢ƒè‡³å…³é‡è¦ã€‚æœ€è¿‘é€šè¿‡å¼•å…¥å¤§è§„æ¨¡æ•°æ®æé«˜äº†è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰çš„ç©ºé—´æ¨ç†èƒ½åŠ›ï¼Œä½†è¿™äº›æ¨¡å‹åœ¨åˆ†æè¿åŠ¨ç‰©ä½“çš„è¿åŠ¨è·ç¦»å’Œé€Ÿåº¦ç­‰è¿åŠ¨å­¦å…ƒç´ æ—¶ä»å­˜åœ¨å›°éš¾ã€‚ä¸ºäº†å¼¥è¡¥è¿™ä¸€å·®è·ï¼Œæˆ‘ä»¬æ„å»ºäº†ä¸€ä¸ªæ¶‰åŠè¿åŠ¨å­¦æŒ‡ä»¤è°ƒæ•´çš„æ—¶ç©ºæ¨ç†æ•°æ®é›†å’ŒåŸºå‡†æµ‹è¯•ï¼Œç§°ä¸ºSTKitå’ŒSTKit-Benchã€‚å®ƒä»¬åŒ…å«å¸¦æœ‰ä¸‰ç»´æ³¨é‡Šçš„çœŸå®ä¸–ç•Œè§†é¢‘ï¼Œè¯¦ç»†æè¿°äº†ç‰©ä½“è¿åŠ¨çš„åŠ¨åŠ›å­¦ï¼šè¡Œè¿›è·ç¦»ã€é€Ÿåº¦ã€è¿åŠ¨æ–¹å‘ã€ç‰©ä½“é—´çš„è·ç¦»æ¯”è¾ƒä»¥åŠç›¸å¯¹è¿åŠ¨æ–¹å‘ã€‚ä¸ºäº†å°†è¿™ç§æ•°æ®æ„å»ºè¿›ä¸€æ­¥æ‰©å±•åˆ°æ²¡æœ‰ä¸‰ç»´æ ‡ç­¾çš„è§†é¢‘ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ä½¿ç”¨çœŸå®ä¸–ç•Œè§„æ¨¡çš„å››ç»´é‡å»ºç”Ÿæˆä¼ªæ ‡ç­¾çš„è‡ªåŠ¨ç®¡é“ã€‚é€šè¿‡æˆ‘ä»¬çš„æ—¶ç©ºæ¨ç†è¿åŠ¨å­¦æŒ‡ä»¤è°ƒæ•´æ•°æ®ï¼Œæˆ‘ä»¬æ¨å‡ºäº†ST-VLMï¼Œè¿™æ˜¯ä¸€ä¸ªå¢å¼ºæ—¶ç©ºæ¨ç†çš„VLMæ¨¡å‹ï¼Œåœ¨STKit-Benchä¸Šè¡¨ç°å‡ºå“è¶Šçš„æ€§èƒ½ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¯æ˜äº†ST-VLMåœ¨ä¸åŒé¢†åŸŸå’Œä»»åŠ¡ä¸­çš„ç¨³å¥æ³›åŒ–èƒ½åŠ›ï¼Œåœ¨å…¶ä»–æ—¶ç©ºåŸºå‡†æµ‹è¯•ï¼ˆä¾‹å¦‚ActivityNetã€TVQA+ï¼‰ä¸Šçš„è¡¨ç°ä¼˜äºåŸºçº¿ã€‚æœ€åï¼Œé€šè¿‡æ•´åˆå­¦åˆ°çš„æ—¶ç©ºæ¨ç†ä¸ç°æœ‰èƒ½åŠ›ï¼ŒST-VLMå¯å®ç°å¤æ‚çš„å¤šæ­¥éª¤æ¨ç†ã€‚é¡¹ç›®é¡µé¢ï¼š<a target="_blank" rel="noopener" href="https://ikodoh.github.io/ST-VLM%E3%80%82">https://ikodoh.github.io/ST-VLMã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.19355v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>è¯¥æ–‡ä»‹ç»äº†æ—¶ç©ºæ¨ç†åœ¨ç°å®ç¯å¢ƒç†è§£ä¸­çš„é‡è¦æ€§ï¼Œç‰¹åˆ«æ˜¯åœ¨è‡ªåŠ¨é©¾é©¶å’Œä½“è‚²åˆ†æç­‰é¢†åŸŸã€‚å°½ç®¡å¼•å…¥äº†å¤§è§„æ¨¡æ•°æ®åï¼Œè§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰çš„æ¨ç†èƒ½åŠ›æœ‰æ‰€æå‡ï¼Œä½†åœ¨åˆ†æè¿åŠ¨ç‰©ä½“çš„è·ç¦»å’Œé€Ÿåº¦ç­‰åŠ¨åŠ›å­¦å…ƒç´ æ—¶ä»å­˜åœ¨å›°éš¾ã€‚ä¸ºè§£å†³æ­¤é—®é¢˜ï¼Œç ”ç©¶å›¢é˜Ÿæ„å»ºäº†åŒ…å«è¿åŠ¨ç‰©ä½“ä¸‰ç»´æ³¨é‡Šçš„æ—¶ç©ºæ¨ç†æ•°æ®é›†å’ŒåŸºå‡†æµ‹è¯•å¹³å°STKitä¸STKit-Benchã€‚æ­¤å¤–ï¼Œä¸ºæ‰©å±•æ­¤ç±»æ•°æ®æ„å»ºè‡³æ— ä¸‰ç»´æ ‡ç­¾çš„è§†é¢‘ï¼Œç ”ç©¶å›¢é˜Ÿæå‡ºäº†åˆ©ç”¨å››ç»´é‡å»ºç”Ÿæˆä¼ªæ ‡ç­¾çš„è‡ªåŠ¨ç®¡é“ã€‚ç ”ç©¶å›¢é˜Ÿè¿˜å±•ç¤ºäº†é’ˆå¯¹æ—¶ç©ºæ¨ç†çš„å¢å¼ºå‹VLMâ€”â€”ST-VLMï¼Œåœ¨STKit-Benchä¸Šè¡¨ç°å‡ºå“è¶Šæ€§èƒ½ï¼Œå¹¶èƒ½ç¨³å¥åœ°åº”ç”¨äºä¸åŒé¢†åŸŸå’Œä»»åŠ¡ã€‚æœ€åï¼Œé€šè¿‡å°†å­¦åˆ°çš„æ—¶ç©ºæ¨ç†èƒ½åŠ›ä¸ç°æœ‰èƒ½åŠ›ç›¸ç»“åˆï¼ŒST-VLMå¯å®ç°å¤æ‚çš„å¤šæ­¥æ¨ç†ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ—¶ç©ºæ¨ç†åœ¨ç°å®ç¯å¢ƒç†è§£ä¸­è‡³å…³é‡è¦ï¼Œå°¤å…¶åœ¨è‡ªåŠ¨é©¾é©¶å’Œä½“è‚²åˆ†æé¢†åŸŸã€‚</li>
<li>è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰è™½å·²å¼•å…¥å¤§è§„æ¨¡æ•°æ®æå‡å…¶æ¨ç†èƒ½åŠ›ï¼Œä½†åœ¨åˆ†æè¿åŠ¨ç‰©ä½“çš„åŠ¨åŠ›å­¦ç‰¹å¾æ–¹é¢ä»æœ‰æŒ‘æˆ˜ã€‚</li>
<li>ç ”ç©¶å›¢é˜Ÿæ„å»ºäº†æ—¶ç©ºæ¨ç†æ•°æ®é›†å’ŒåŸºå‡†æµ‹è¯•å¹³å°STKitä¸STKit-Benchï¼ŒåŒ…å«åŒ…å«ç°å®ä¸–ç•Œä¸­ç‰©ä½“è¿åŠ¨çš„ä¸‰ç»´æ³¨é‡Šã€‚</li>
<li>ä¸ºæ‰©å±•è§†é¢‘æ•°æ®æ„å»ºï¼Œç ”ç©¶å›¢é˜Ÿæå‡ºäº†åˆ©ç”¨å››ç»´é‡å»ºç”Ÿæˆä¼ªæ ‡ç­¾çš„è‡ªåŠ¨ç®¡é“ã€‚</li>
<li>ST-VLMæ˜¯ä¸“ä¸ºæ—¶ç©ºæ¨ç†å¢å¼ºçš„è§†è§‰è¯­è¨€æ¨¡å‹ï¼Œåœ¨STKit-Benchä¸Šè¡¨ç°ä¼˜è¶Šã€‚</li>
<li>ST-VLMèƒ½å¤Ÿç¨³å¥åœ°åº”ç”¨äºå¤šä¸ªé¢†åŸŸå’Œä»»åŠ¡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.19355">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-90e915a2c37858642805b4dd628ee749.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d8e0fc995603c14e9dbb59bec545fd16.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-c084e9998c5bd21ee62e3b8ab498e4c2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3a83fee94008c4eb63eb5913ba6b6fdb.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-8bf26d67f83df6c8c93a5c2dc429dcce.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-32a68729a67a41a12a06f038493170e3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2baa39b34006ef12edd50ab6a02d4a0d.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="SRMIR-Shadow-Reward-Models-Based-on-Introspective-Reasoning-for-LLM-Alignment"><a href="#SRMIR-Shadow-Reward-Models-Based-on-Introspective-Reasoning-for-LLM-Alignment" class="headerlink" title="SRMIR: Shadow Reward Models Based on Introspective Reasoning for LLM   Alignment"></a>SRMIR: Shadow Reward Models Based on Introspective Reasoning for LLM   Alignment</h2><p><strong>Authors:Ruoxi Cheng, Shuirong Cao</strong></p>
<p>Aligning large language models (LLMs) with human preferences and values is vital for application. However, current alignment methods face three main limitations: (1) reliance on costly human annotation; (2) alignment tax; (3) shallow alignment vulnerable to jailbreak attacks. Additionally, current alignment datasets often suffer from uneven distributions, leading to overrepresentation of some topics and neglect of others. To address these issues, we propose SRMIR (Shadow Reward Models Based on Introspective Reasoning), inspired by shadow models in membership inference attacks. We first construct a balanced safety Chain of Draft (CoD) dataset across $7$ harmful types with structured prompt leveraging the introspective reasoning capabilities of LLMs, then train a set of specialized reward models to guide policy optimization through Group Relative Policy Optimization (GRPO). We apply two strategies, linear combination and categorized approach, to integrate shadow reward models for policy optimization. By comparison, we find that the latter achieves superior alignment despite higher computational costs. Experiments across several LLMs demonstrate SRMIR significantly outperforms existing methods. </p>
<blockquote>
<p>å°†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¸äººç±»åå¥½å’Œä»·å€¼è§‚å¯¹é½å¯¹äºå…¶åº”ç”¨è‡³å…³é‡è¦ã€‚ç„¶è€Œï¼Œå½“å‰çš„å¯¹é½æ–¹æ³•é¢ä¸´ä¸‰ä¸ªä¸»è¦å±€é™æ€§ï¼šï¼ˆ1ï¼‰ä¾èµ–äºæ˜‚è´µçš„äººå·¥æ ‡æ³¨ï¼›ï¼ˆ2ï¼‰å¯¹é½ç¨ï¼›ï¼ˆ3ï¼‰æµ…å¯¹é½å®¹æ˜“å—åˆ°è¶Šç‹±æ”»å‡»ã€‚æ­¤å¤–ï¼Œå½“å‰çš„å¯¹é½æ•°æ®é›†å¾€å¾€åˆ†å¸ƒä¸å‡ï¼Œå¯¼è‡´æŸäº›è¯é¢˜è¿‡åº¦ä»£è¡¨è€Œå¿½è§†å…¶ä»–è¯é¢˜ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬å—åˆ°æˆå‘˜æ¨ç†æ”»å‡»ä¸­çš„å½±å­æ¨¡å‹çš„å¯å‘ï¼Œæå‡ºäº†åŸºäºå†…çœæ¨ç†çš„å½±å­å¥–åŠ±æ¨¡å‹SRMIRã€‚æˆ‘ä»¬é¦–å…ˆæ„å»ºä¸€ä¸ªå¹³è¡¡çš„å®‰å…¨è‰æ¡ˆé“¾ï¼ˆCoDï¼‰æ•°æ®é›†ï¼Œè¯¥æ•°æ®é›†è·¨è¶Š7ç§æœ‰å®³ç±»å‹ï¼Œåˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹çš„å†…çœæ¨ç†èƒ½åŠ›è¿›è¡Œç»“æ„åŒ–æç¤ºã€‚ç„¶åï¼Œæˆ‘ä»¬è®­ç»ƒä¸€ç»„ä¸“é—¨çš„å¥–åŠ±æ¨¡å‹ï¼Œé€šè¿‡ç¾¤ä½“ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆGRPOï¼‰æŒ‡å¯¼ç­–ç•¥ä¼˜åŒ–ã€‚æˆ‘ä»¬åº”ç”¨çº¿æ€§ç»„åˆå’Œåˆ†ç±»æ–¹æ³•ä¸¤ç§ç­–ç•¥æ¥æ•´åˆå½±å­å¥–åŠ±æ¨¡å‹ä»¥è¿›è¡Œç­–ç•¥ä¼˜åŒ–ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼Œå°½ç®¡è®¡ç®—æˆæœ¬è¾ƒé«˜ï¼Œä½†æˆ‘ä»¬å‘ç°åè€…å®ç°äº†æ›´å¥½çš„å¯¹é½æ•ˆæœã€‚åœ¨å¤šä¸ªå¤§å‹è¯­è¨€æ¨¡å‹ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒSRMIRæ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.18991v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>è¯¥æ–‡æ¢è®¨äº†å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ä¸äººç±»åå¥½å’Œä»·å€¼è§‚å¯¹é½çš„é‡è¦æ€§ï¼ŒæŒ‡å‡ºå½“å‰æ–¹æ³•çš„ä¸‰å¤§å±€é™ï¼Œå¹¶é’ˆå¯¹è¿™äº›é—®é¢˜æå‡ºäº†åŸºäºå†…çœæ¨ç†çš„å½±å­å¥–åŠ±æ¨¡å‹ï¼ˆSRMIRï¼‰ã€‚ä¸ºè§£å†³æ•°æ®åˆ†å¸ƒä¸å‡çš„é—®é¢˜ï¼Œæ„å»ºäº†å¹³è¡¡çš„å®‰å…¨è‰æ¡ˆé“¾æ•°æ®é›†ã€‚é‡‡ç”¨çº¿æ€§ç»„åˆå’Œåˆ†ç±»é›†æˆæ–¹æ³•æ•´åˆå½±å­å¥–åŠ±æ¨¡å‹è¿›è¡Œç­–ç•¥ä¼˜åŒ–ï¼Œåè€…è™½ç„¶è®¡ç®—æˆæœ¬è¾ƒé«˜ä½†æ•ˆæœæ›´ä½³ã€‚å®éªŒè¯æ˜SRMIRæ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§è¯­è¨€æ¨¡å‹ä¸äººç±»åå¥½å’Œä»·å€¼è§‚å¯¹é½å¯¹äºåº”ç”¨è‡³å…³é‡è¦ã€‚</li>
<li>å½“å‰å¯¹é½æ–¹æ³•å­˜åœ¨ä¸‰å¤§å±€é™æ€§ï¼šä¾èµ–æ˜‚è´µçš„äººåŠ›æ ‡æ³¨ã€å¯¹é½ç¨å’Œæµ…å¯¹é½æ˜“å—è¶Šç‹±æ”»å‡»ã€‚</li>
<li>æ•°æ®é›†åˆ†å¸ƒä¸å‡çš„é—®é¢˜éœ€è¦é€šè¿‡æ„å»ºå¹³è¡¡çš„å®‰å…¨è‰æ¡ˆé“¾æ•°æ®é›†æ¥è§£å†³ã€‚</li>
<li>SRMIRåŸºäºå†…çœæ¨ç†çš„å½±å­æ¨¡å‹æ¥è§£å†³ä¸Šè¿°é—®é¢˜ï¼Œå—åˆ°æˆå‘˜èº«ä»½æ¨æ–­æ”»å‡»ä¸­çš„å½±å­æ¨¡å‹çš„å¯å‘ã€‚</li>
<li>é€šè¿‡ç»“æ„åŒ–æç¤ºåˆ©ç”¨LLMçš„å†…çœæ¨ç†èƒ½åŠ›ï¼Œæ„å»ºäº†è·¨è¶Š7ç§æœ‰å®³ç±»å‹çš„å¹³è¡¡æ•°æ®é›†ã€‚</li>
<li>é‡‡ç”¨å¤šç§ç­–ç•¥è¿›è¡Œç­–ç•¥ä¼˜åŒ–ï¼ŒåŒ…æ‹¬çº¿æ€§ç»„åˆå’Œåˆ†ç±»é›†æˆæ–¹æ³•æ•´åˆå½±å­å¥–åŠ±æ¨¡å‹ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.18991">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-2aedc7b8d5bf62db595a44b0f8f7a25f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-fc6c154c0383412239d4d6fa0e8c971f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2a7e7fe82ab1c681b63371c1fa01cd8c.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-6b6a32d8bafd4e873bdf29f591f5d1e3.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="Teaching-LLMs-for-Step-Level-Automatic-Math-Correction-via-Reinforcement-Learning"><a href="#Teaching-LLMs-for-Step-Level-Automatic-Math-Correction-via-Reinforcement-Learning" class="headerlink" title="Teaching LLMs for Step-Level Automatic Math Correction via Reinforcement   Learning"></a>Teaching LLMs for Step-Level Automatic Math Correction via Reinforcement   Learning</h2><p><strong>Authors:Junsong Li, Jie Zhou, Yutao Yang, Bihao Zhan, Qianjun Pan, Yuyang Ding, Qin Chen, Jiang Bo, Xin Lin, Liang He</strong></p>
<p>Automatic math correction aims to check studentsâ€™ solutions to mathematical problems via artificial intelligence technologies. Most existing studies focus on judging the final answer at the problem level, while they ignore detailed feedback on each step in a math problem-solving process, which requires abilities of semantic understanding and reasoning. In this paper, we propose a reinforcement learning (RL)-based method to boost large language model (LLM) for step-level automatic math correction, named StepAMC. Particularly, we convert the step-level automatic math correction within the text classification task into an RL problem to enhance the reasoning capabilities of LLMs. Then, we design a space-constrained policy network to improve the stability of RL. Then, we introduce a fine-grained reward network to convert the binary human feedback into a continuous value. We conduct extensive experiments over two benchmark datasets and the results show that our model outperforms the eleven strong baselines. </p>
<blockquote>
<p>è‡ªåŠ¨æ•°å­¦çº é”™æ—¨åœ¨é€šè¿‡äººå·¥æ™ºèƒ½æŠ€æœ¯å¯¹å­¦ç”Ÿçš„æ•°å­¦è§£é¢˜ç­”æ¡ˆè¿›è¡Œæ£€æŸ¥ã€‚ç°æœ‰çš„å¤§å¤šæ•°ç ”ç©¶ä¸»è¦é›†ä¸­åœ¨ä»é—®é¢˜å±‚é¢åˆ¤æ–­æœ€ç»ˆç­”æ¡ˆï¼Œè€Œå¿½ç•¥æ•°å­¦è§£é¢˜è¿‡ç¨‹ä¸­æ¯ä¸€æ­¥çš„è¯¦ç»†åé¦ˆï¼Œè¿™éœ€è¦è¯­ä¹‰ç†è§£å’Œæ¨ç†çš„èƒ½åŠ›ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰çš„æ–¹æ³•ï¼Œç”¨äºå¢å¼ºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è¿›è¡Œæ­¥éª¤çº§åˆ«çš„è‡ªåŠ¨æ•°å­¦çº é”™ï¼Œåä¸ºStepAMCã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬å°†æ–‡æœ¬åˆ†ç±»ä»»åŠ¡ä¸­çš„æ­¥éª¤çº§è‡ªåŠ¨æ•°å­¦çº é”™è½¬åŒ–ä¸ºRLé—®é¢˜ï¼Œä»¥æé«˜LLMçš„æ¨ç†èƒ½åŠ›ã€‚ç„¶åï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸€ä¸ªç©ºé—´çº¦æŸç­–ç•¥ç½‘ç»œæ¥æé«˜RLçš„ç¨³å®šæ€§ã€‚æ¥ç€ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ªç²¾ç»†çš„å¥–åŠ±ç½‘ç»œï¼Œå°†äºŒè¿›åˆ¶çš„äººç±»åé¦ˆè½¬åŒ–ä¸ºè¿ç»­å€¼ã€‚æˆ‘ä»¬åœ¨ä¸¤ä¸ªåŸºå‡†æ•°æ®é›†ä¸Šè¿›è¡Œäº†å¤§é‡å®éªŒï¼Œç»“æœè¡¨æ˜æˆ‘ä»¬çš„æ¨¡å‹ä¼˜äºå…¶ä»–åä¸€ä¸ªå¼ºå¤§çš„åŸºå‡†æ¨¡å‹ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.18432v1">PDF</a> </p>
<p><strong>Summary</strong>ï¼šæœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰çš„è‡ªåŠ¨æ•°å­¦çº é”™æ–¹æ³•ï¼Œæ—¨åœ¨æé«˜å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å¯¹æ•°å­¦é—®é¢˜è§£é¢˜æ­¥éª¤çš„è‡ªåŠ¨çº é”™èƒ½åŠ›ã€‚è¯¥æ–¹æ³•å°†æ•°å­¦æ­¥éª¤çº§åˆ«çš„è‡ªåŠ¨çº é”™è½¬åŒ–ä¸ºæ–‡æœ¬åˆ†ç±»ä»»åŠ¡ä¸­çš„RLé—®é¢˜ï¼Œä»¥å¢å¼ºLLMçš„æ¨ç†èƒ½åŠ›ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¯¥æ¨¡å‹åœ¨åŸºå‡†æ•°æ®é›†ä¸Šè¡¨ç°å‡ºè‰²ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>ç°æœ‰æ•°å­¦çº é”™ç ”ç©¶å¤šé›†ä¸­åœ¨é—®é¢˜çº§åˆ«ç­”æ¡ˆçš„åˆ¤æ–­ï¼Œå¿½ç•¥äº†é—®é¢˜è§£å†³è¿‡ç¨‹ä¸­æ¯ä¸€æ­¥çš„è¯¦ç»†åé¦ˆã€‚</li>
<li>è¯­ä¹‰ç†è§£å’Œæ¨ç†èƒ½åŠ›æ˜¯æ•°å­¦é—®é¢˜è§£å†³è¿‡ç¨‹ä¸­çš„å…³é”®èƒ½åŠ›ã€‚</li>
<li>æå‡ºäº†ä¸€ç§åŸºäºå¼ºåŒ–å­¦ä¹ çš„å¤§å‹è¯­è¨€æ¨¡å‹è‡ªåŠ¨æ•°å­¦çº é”™æ–¹æ³•ï¼ˆStepAMCï¼‰ã€‚</li>
<li>StepAMCå°†æ•°å­¦æ­¥éª¤çº§åˆ«çš„è‡ªåŠ¨çº é”™è½¬åŒ–ä¸ºæ–‡æœ¬åˆ†ç±»ä»»åŠ¡ä¸­çš„å¼ºåŒ–å­¦ä¹ é—®é¢˜ã€‚</li>
<li>StepAMCè®¾è®¡äº†ä¸€ç§ç©ºé—´çº¦æŸç­–ç•¥ç½‘ç»œä»¥æé«˜å¼ºåŒ–å­¦ä¹ çš„ç¨³å®šæ€§ã€‚</li>
<li>StepAMCå¼•å…¥ç²¾ç»†å¥–åŠ±ç½‘ç»œï¼Œå°†äºŒå…ƒäººç±»åé¦ˆè½¬åŒ–ä¸ºè¿ç»­å€¼ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.18432">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-c4b20bc2d8b5c9de20ef19f92f86b5dc.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-1ca385f7d56d23daa7b0fb223604b4ae.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-27f25afc6eacd735514413278609eba5.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-8023b44713b357378b1bc9204d9a8a38.jpg" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="Mitigating-Reward-Over-Optimization-in-RLHF-via-Behavior-Supported-Regularization"><a href="#Mitigating-Reward-Over-Optimization-in-RLHF-via-Behavior-Supported-Regularization" class="headerlink" title="Mitigating Reward Over-Optimization in RLHF via Behavior-Supported   Regularization"></a>Mitigating Reward Over-Optimization in RLHF via Behavior-Supported   Regularization</h2><p><strong>Authors:Juntao Dai, Taiye Chen, Yaodong Yang, Qian Zheng, Gang Pan</strong></p>
<p>Reinforcement learning from human feedback (RLHF) is an effective method for aligning large language models (LLMs) with human values. However, reward over-optimization remains an open challenge leading to discrepancies between the performance of LLMs under the reward model and the true human objectives. A primary contributor to reward over-optimization is the extrapolation error that arises when the reward model evaluates out-of-distribution (OOD) responses. However, current methods still fail to prevent the increasing frequency of OOD response generation during the reinforcement learning (RL) process and are not effective at handling extrapolation errors from OOD responses. In this work, we propose the Behavior-Supported Policy Optimization (BSPO) method to mitigate the reward over-optimization issue. Specifically, we define behavior policy as the next token distribution of the reward training dataset to model the in-distribution (ID) region of the reward model. Building on this, we introduce the behavior-supported Bellman operator to regularize the value function, penalizing all OOD values without impacting the ID ones. Consequently, BSPO reduces the generation of OOD responses during the RL process, thereby avoiding overestimation caused by the reward modelâ€™s extrapolation errors. Theoretically, we prove that BSPO guarantees a monotonic improvement of the supported policy until convergence to the optimal behavior-supported policy. Empirical results from extensive experiments show that BSPO outperforms baselines in preventing reward over-optimization due to OOD evaluation and finding the optimal ID policy. </p>
<blockquote>
<p>å¼ºåŒ–å­¦ä¹ ä»äººç±»åé¦ˆï¼ˆRLHFï¼‰æ˜¯ä¸€ç§å°†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¸äººç±»ä»·å€¼å¯¹é½çš„æœ‰æ•ˆæ–¹æ³•ã€‚ç„¶è€Œï¼Œå¥–åŠ±è¿‡ä¼˜åŒ–ä»ç„¶æ˜¯ä¸€ä¸ªå¼€æ”¾æ€§çš„æŒ‘æˆ˜ï¼Œå¯¼è‡´è¯­è¨€æ¨¡å‹åœ¨å¥–åŠ±æ¨¡å‹ä¸‹çš„æ€§èƒ½ä¸çœŸå®äººç±»ç›®æ ‡ä¹‹é—´å­˜åœ¨å·®å¼‚ã€‚å¯¼è‡´å¥–åŠ±è¿‡åº¦ä¼˜åŒ–çš„ä¸»è¦åŸå› æ˜¯å½“å¥–åŠ±æ¨¡å‹è¯„ä¼°è¶…å‡ºåˆ†å¸ƒï¼ˆOODï¼‰çš„å“åº”æ—¶å‡ºç°çš„æ¨æ–­è¯¯å·®ã€‚ç„¶è€Œï¼Œå½“å‰çš„æ–¹æ³•ä»ç„¶æ— æ³•é˜²æ­¢åœ¨å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰è¿‡ç¨‹ä¸­OODå“åº”ç”Ÿæˆçš„é¢‘ç‡å¢åŠ ï¼Œå¹¶ä¸”ä¸æ“…é•¿å¤„ç†æ¥è‡ªOODå“åº”çš„æ¨æ–­è¯¯å·®ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†è¡Œä¸ºæ”¯æŒç­–ç•¥ä¼˜åŒ–ï¼ˆBSPOï¼‰æ–¹æ³•æ¥ç¼“è§£å¥–åŠ±è¿‡åº¦ä¼˜åŒ–é—®é¢˜ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬å°†è¡Œä¸ºç­–ç•¥å®šä¹‰ä¸ºå¥–åŠ±è®­ç»ƒæ•°æ®é›†çš„ä¸‹ä¸€ä¸ªä»¤ç‰Œåˆ†å¸ƒï¼Œä»¥æ¨¡æ‹Ÿå¥–åŠ±æ¨¡å‹çš„å†…éƒ¨åˆ†å¸ƒï¼ˆIDï¼‰åŒºåŸŸã€‚åœ¨æ­¤åŸºç¡€ä¸Šï¼Œæˆ‘ä»¬å¼•å…¥äº†è¡Œä¸ºæ”¯æŒBellmanç®—å­æ¥è§„èŒƒå€¼å‡½æ•°ï¼Œæƒ©ç½šæ‰€æœ‰OODå€¼è€Œä¸å½±å“IDå€¼ã€‚å› æ­¤ï¼ŒBSPOå‡å°‘äº†RLè¿‡ç¨‹ä¸­OODå“åº”çš„ç”Ÿæˆï¼Œä»è€Œé¿å…äº†ç”±å¥–åŠ±æ¨¡å‹çš„æ¨æ–­è¯¯å·®å¼•èµ·çš„è¿‡é«˜ä¼°è®¡ã€‚ä»ç†è®ºä¸Šè®²ï¼Œæˆ‘ä»¬è¯æ˜äº†BSPOä¿è¯äº†æ”¯æŒç­–ç•¥çš„å•è°ƒæ”¹è¿›ï¼Œç›´è‡³æ”¶æ•›åˆ°æœ€ä½³è¡Œä¸ºæ”¯æŒç­–ç•¥ã€‚å¹¿æ³›çš„å®éªŒç»éªŒç»“æœè¡¨æ˜ï¼Œç”±äºOODè¯„ä¼°å’Œæ‰¾åˆ°æœ€ä½³IDç­–ç•¥ï¼ŒBSPOåœ¨é˜²æ­¢å¥–åŠ±è¿‡åº¦ä¼˜åŒ–æ–¹é¢ä¼˜äºåŸºçº¿æ–¹æ³•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.18130v1">PDF</a> Published as a conference paper at ICLR 2025</p>
<p><strong>Summary</strong><br>å¼ºåŒ–å­¦ä¹ ä»äººç±»åé¦ˆï¼ˆRLHFï¼‰æ˜¯ä¸€ç§æœ‰æ•ˆçš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¸äººç±»ä»·å€¼è§‚å¯¹é½çš„æ–¹æ³•ã€‚ç„¶è€Œï¼Œå¥–åŠ±è¿‡åº¦ä¼˜åŒ–ä»ç„¶æ˜¯ä¸€ä¸ªæŒ‘æˆ˜ï¼Œå¯¼è‡´LLMåœ¨å¥–åŠ±æ¨¡å‹ä¸‹çš„è¡¨ç°ä¸çœŸå®äººç±»ç›®æ ‡ä¹‹é—´å­˜åœ¨å·®å¼‚ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§è¡Œä¸ºæ”¯æŒç­–ç•¥ä¼˜åŒ–ï¼ˆBSPOï¼‰æ–¹æ³•æ¥è§£å†³å¥–åŠ±è¿‡åº¦ä¼˜åŒ–é—®é¢˜ã€‚è¯¥æ–¹æ³•é€šè¿‡å®šä¹‰è¡Œä¸ºç­–ç•¥æ¥å»ºæ¨¡å¥–åŠ±æ¨¡å‹çš„å†…éƒ¨åŒºåŸŸï¼Œå¹¶å¼•å…¥è¡Œä¸ºæ”¯æŒBellmanç®—å­æ¥è§„èŒƒåŒ–ä»·å€¼å‡½æ•°ï¼Œæƒ©ç½šæ‰€æœ‰è¶…å‡ºåˆ†å¸ƒçš„å€¼ï¼Œè€Œä¸å½±å“å†…éƒ¨å€¼ã€‚å› æ­¤ï¼ŒBSPOå‡å°‘äº†åœ¨å¼ºåŒ–å­¦ä¹ è¿‡ç¨‹æœŸé—´çš„ä¸ç¬¦åˆé¢„æœŸå›åº”çš„äº§ç”Ÿï¼Œé¿å…äº†å¥–åŠ±æ¨¡å‹è¿‡åº¦æ¨æµ‹çš„é”™è¯¯å¯¼è‡´çš„è¿‡åº¦ä¼˜åŒ–é—®é¢˜ã€‚å®è¯ç»“æœè¡¨æ˜ï¼ŒBSPOåœ¨é˜²æ­¢å› è¶…å‡ºåˆ†å¸ƒè¯„ä¼°å¯¼è‡´çš„å¥–åŠ±è¿‡åº¦ä¼˜åŒ–æ–¹é¢ä¼˜äºåŸºçº¿æ–¹æ³•ï¼Œå¹¶èƒ½æ‰¾åˆ°æœ€ä½³å†…éƒ¨ç­–ç•¥ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>å¼ºåŒ–å­¦ä¹ ä»äººç±»åé¦ˆï¼ˆRLHFï¼‰æ˜¯ä¸€ç§ä½¿å¤§å‹è¯­è¨€æ¨¡å‹ä¸äººç±»ä»·å€¼è§‚å¯¹é½çš„æœ‰æ•ˆæ–¹æ³•ã€‚</li>
<li>å¥–åŠ±è¿‡åº¦ä¼˜åŒ–æ˜¯RLHFä¸­çš„ä¸€ä¸ªæŒ‘æˆ˜ï¼Œä¼šå¯¼è‡´æ¨¡å‹åœ¨å®é™…åº”ç”¨ä¸­çš„è¡¨ç°ä¸äººçš„æœŸæœ›å­˜åœ¨åå·®ã€‚</li>
<li>è¡Œä¸ºæ”¯æŒç­–ç•¥ä¼˜åŒ–ï¼ˆBSPOï¼‰æ–¹æ³•è¢«æå‡ºæ¥è§£å†³å¥–åŠ±è¿‡åº¦ä¼˜åŒ–é—®é¢˜ã€‚</li>
<li>BSPOé€šè¿‡å®šä¹‰è¡Œä¸ºç­–ç•¥æ¥å»ºæ¨¡å¥–åŠ±æ¨¡å‹çš„å†…éƒ¨åŒºåŸŸï¼Œå¹¶å¼•å…¥è¡Œä¸ºæ”¯æŒBellmanç®—å­æ¥è§„èŒƒåŒ–ä»·å€¼å‡½æ•°ã€‚</li>
<li>BSPOèƒ½å¤Ÿå‡å°‘åœ¨å¼ºåŒ–å­¦ä¹ è¿‡ç¨‹æœŸé—´çš„ä¸ç¬¦åˆé¢„æœŸå›åº”çš„äº§ç”Ÿï¼Œå¹¶é¿å…å¥–åŠ±æ¨¡å‹çš„è¿‡åº¦æ¨æµ‹é”™è¯¯ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.18130">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-8ffb8c30c368e262e4b16a62060b5580.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ef3e0c09cee9618be5328bd525bc4fba.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f5e22e528aee49d30687ed5bbf57d32d.jpg" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="GeoBenchX-Benchmarking-LLMs-for-Multistep-Geospatial-Tasks"><a href="#GeoBenchX-Benchmarking-LLMs-for-Multistep-Geospatial-Tasks" class="headerlink" title="GeoBenchX: Benchmarking LLMs for Multistep Geospatial Tasks"></a>GeoBenchX: Benchmarking LLMs for Multistep Geospatial Tasks</h2><p><strong>Authors:Varvara Krechetova, Denis Kochedykov</strong></p>
<p>In this paper, we establish a benchmark for evaluating large language models (LLMs) on multi-step geospatial tasks relevant to commercial GIS practitioners. We assess seven leading commercial LLMs (Sonnet 3.5 and 3.7, Haiku 3.5, Gemini 2.0, GPT-4o, GPT-4o mini, and o3-mini) using a simple tool-calling agent equipped with 23 geospatial functions. Our benchmark comprises tasks across four categories of increasing complexity, with both solvable and intentionally unsolvable tasks to test hallucination rejection. We develop an LLM-as-Judge evaluation framework to compare agent solutions against reference implementations. Results show Sonnet 3.5 and GPT-4o achieve the best overall performance, with Claude models excelling on solvable tasks while OpenAI models better identify unsolvable scenarios. We observe significant differences in token usage, with Anthropic models consuming substantially more tokens than competitors. Common errors include misunderstanding geometrical relationships, relying on outdated knowledge, and inefficient data manipulation. The resulting benchmark set, evaluation framework, and data generation pipeline are released as open-source resources, providing one more standardized method for ongoing evaluation of LLMs for GeoAI. </p>
<blockquote>
<p>åœ¨è¿™ç¯‡è®ºæ–‡ä¸­ï¼Œæˆ‘ä»¬ä¸ºè¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨å•†ä¸šGISä»ä¸šè€…ç›¸å…³çš„å¤šæ­¥éª¤åœ°ç†ç©ºé—´ä»»åŠ¡ä¸Šçš„è¡¨ç°å»ºç«‹äº†åŸºå‡†æµ‹è¯•ã€‚æˆ‘ä»¬ä½¿ç”¨é…å¤‡23ä¸ªåœ°ç†ç©ºé—´åŠŸèƒ½çš„ç®€å•å·¥å…·è°ƒç”¨ä»£ç†ï¼Œè¯„ä¼°äº†ä¸ƒä¸ªé¢†å…ˆçš„å•†ä¸šLLMï¼ˆSonnet 3.5å’Œ3.7ã€Haiku 3.5ã€Gemini 2.0ã€GPT-4oã€GPT-4o miniå’Œo3-miniï¼‰ã€‚æˆ‘ä»¬çš„åŸºå‡†æµ‹è¯•åŒ…æ‹¬å››ä¸ªç±»åˆ«ã€éš¾åº¦é€’å¢çš„ä»»åŠ¡ï¼Œå…¶ä¸­åŒ…å«å¯è§£å†³å’Œæ•…æ„æ— æ³•è§£å†³çš„æµ‹è¯•ä»»åŠ¡ï¼Œä»¥æ£€éªŒæ’æ–¥å¹»è§‰çš„èƒ½åŠ›ã€‚æˆ‘ä»¬å¼€å‘äº†ä¸€ä¸ªLLM-as-Judgeè¯„ä¼°æ¡†æ¶ï¼Œç”¨äºå°†ä»£ç†è§£å†³æ–¹æ¡ˆä¸å‚è€ƒå®ç°è¿›è¡Œæ¯”è¾ƒã€‚ç»“æœè¡¨æ˜ï¼ŒSonnet 3.5å’ŒGPT-4oæ€»ä½“æ€§èƒ½æœ€ä½³ï¼ŒClaudeæ¨¡å‹åœ¨å¯è§£å†³çš„ä»»åŠ¡ä¸Šè¡¨ç°å‡ºè‰²ï¼Œè€ŒOpenAIæ¨¡å‹åœ¨æ— æ³•è§£å†³çš„åœºæ™¯ä¸‹è¡¨ç°æ›´å¥½ã€‚æˆ‘ä»¬è§‚å¯Ÿåˆ°æ˜¾è‘—çš„ä»¤ç‰Œä½¿ç”¨é‡å·®å¼‚ï¼ŒAnthropicæ¨¡å‹æ¶ˆè€—çš„ä»¤ç‰Œæ˜æ˜¾å¤šäºç«äº‰å¯¹æ‰‹ã€‚å¸¸è§çš„é”™è¯¯åŒ…æ‹¬è¯¯è§£å‡ ä½•å…³ç³»ã€ä¾èµ–è¿‡æ—¶çŸ¥è¯†å’Œä½æ•ˆçš„æ•°æ®æ“ä½œã€‚æ‰€å¾—åŸºå‡†æµ‹è¯•é›†ã€è¯„ä¼°æ¡†æ¶å’Œæ•°æ®ç”Ÿæˆç®¡é“ä½œä¸ºå¼€æºèµ„æºå‘å¸ƒï¼Œä¸ºGeoAIä¸­LLMçš„æŒç»­è¯„ä¼°æä¾›äº†å¦ä¸€ç§æ ‡å‡†åŒ–æ–¹æ³•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.18129v1">PDF</a> Github with code and benchmark set:   <a target="_blank" rel="noopener" href="https://github.com/Solirinai/GeoBenchX">https://github.com/Solirinai/GeoBenchX</a></p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡å»ºç«‹äº†ä¸€ä¸ªè¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨å¤šæ­¥åœ°ç†ç©ºé—´ä»»åŠ¡ä¸Šçš„æ€§èƒ½çš„åŸºå‡†æµ‹è¯•ï¼Œæ¶‰åŠå•†ä¸šGISå®è·µè€…ã€‚é€šè¿‡å¯¹ä¸ƒæ¬¾é¢†å…ˆå•†ä¸šLLMsçš„è¯„ä¼°ï¼Œå‘ç°Sonnet 3.5å’ŒGPT-4oè¡¨ç°æœ€ä½³ã€‚è¯„ä»·æ¡†æ¶åŒ…å«ä¸€ä¸ªç”¨äºæ¯”è¾ƒæ™ºèƒ½ä½“è§£å†³æ–¹æ¡ˆä¸å‚è€ƒå®ç°çš„ç³»ç»Ÿã€‚å‘ç°å­˜åœ¨æ˜¾è‘—çš„å·®å¼‚ï¼Œå¦‚æŸäº›æ¨¡å‹å­˜åœ¨ç†è§£å‡ ä½•å…³ç³»é”™è¯¯ã€ä¾èµ–è¿‡æ—¶çŸ¥è¯†ä»¥åŠæ•°æ®å¤„ç†æ•ˆç‡ä½ä¸‹ç­‰é—®é¢˜ã€‚è¿™äº›èµ„æºä½œä¸ºå¼€æºèµ„æºå‘å¸ƒï¼Œä¸ºåœ°ç†AIçš„LLMsè¯„ä¼°æä¾›äº†æ ‡å‡†åŒ–æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å»ºç«‹äº†è¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨å¤šæ­¥åœ°ç†ç©ºé—´ä»»åŠ¡ä¸Šçš„æ€§èƒ½åŸºå‡†æµ‹è¯•ã€‚</li>
<li>æ¶‰åŠå•†ä¸šGISå®è·µè€…çš„å®é™…åœºæ™¯å’Œä»»åŠ¡ã€‚</li>
<li>è¯„ä¼°äº†ä¸ƒæ¬¾å•†ä¸šLLMsï¼Œå‘ç°Sonnet 3.5å’ŒGPT-4oè¡¨ç°æœ€ä½³ã€‚</li>
<li>è¯„ä»·æ¡†æ¶åŒ…å«ä¸€ä¸ªç”¨äºæ¯”è¾ƒæ™ºèƒ½ä½“è§£å†³æ–¹æ¡ˆä¸å‚è€ƒå®ç°çš„ç³»ç»Ÿã€‚</li>
<li>ä¸åŒæ¨¡å‹ä¹‹é—´å­˜åœ¨æ˜¾è‘—æ€§èƒ½å·®å¼‚ï¼Œå¦‚å¤„ç†åœ°ç†ç©ºé—´ä»»åŠ¡çš„èƒ½åŠ›å’Œç†è§£å‡ ä½•å…³ç³»çš„èƒ½åŠ›ç­‰ã€‚</li>
<li>å‘ç°äº†ä¸€äº›å¸¸è§é”™è¯¯ï¼Œå¦‚ä¾èµ–è¿‡æ—¶çŸ¥è¯†ã€æ•°æ®å¤„ç†æ•ˆç‡ä½ä¸‹ç­‰ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.18129">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-c3d483779eb46d528db82e59e1d78f5f.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-52232d50615c91e120e63c75f71a85bc.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-85f88d7284a3869c0967b757d6545ac0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f4dafb70908885c6c6a930f16fd887e4.jpg" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1><h2 id="Reasoning-with-LLMs-for-Zero-Shot-Vulnerability-Detection"><a href="#Reasoning-with-LLMs-for-Zero-Shot-Vulnerability-Detection" class="headerlink" title="Reasoning with LLMs for Zero-Shot Vulnerability Detection"></a>Reasoning with LLMs for Zero-Shot Vulnerability Detection</h2><p><strong>Authors:Arastoo Zibaeirad, Marco Vieira</strong></p>
<p>Automating software vulnerability detection (SVD) remains a critical challenge in an era of increasingly complex and interdependent software systems. Despite significant advances in Large Language Models (LLMs) for code analysis, prevailing evaluation methodologies often lack the \textbf{context-aware robustness} necessary to capture real-world intricacies and cross-component interactions. To address these limitations, we present \textbf{VulnSage}, a comprehensive evaluation framework and a dataset curated from diverse, large-scale open-source system software projects developed in C&#x2F;C++. Unlike prior datasets, it leverages a heuristic noise pre-filtering approach combined with LLM-based reasoning to ensure a representative and minimally noisy spectrum of vulnerabilities. The framework supports multi-granular analysis across function, file, and inter-function levels and employs four diverse zero-shot prompt strategies: Baseline, Chain-of-Thought, Think, and Think &amp; Verify. Through this evaluation, we uncover that structured reasoning prompts substantially improve LLM performance, with Think &amp; Verify reducing ambiguous responses from 20.3% to 9.1% while increasing accuracy. We further demonstrate that code-specialized models consistently outperform general-purpose alternatives, with performance varying significantly across vulnerability types, revealing that no single approach universally excels across all security contexts. Link to dataset and codes: <a target="_blank" rel="noopener" href="https://github.com/Erroristotle/VulnSage.git">https://github.com/Erroristotle/VulnSage.git</a> </p>
<blockquote>
<p>è‡ªåŠ¨åŒ–è½¯ä»¶æ¼æ´æ£€æµ‹ï¼ˆSVDï¼‰åœ¨å¤æ‚ä¸”ç›¸äº’ä¾å­˜è½¯ä»¶ç³»ç»Ÿæ—¥ç›Šå¢å¤šçš„æ—¶ä»£ä»ç„¶æ˜¯ä¸€ä¸ªå·¨å¤§çš„æŒ‘æˆ˜ã€‚å°½ç®¡ç”¨äºä»£ç åˆ†æçš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å–å¾—äº†é‡å¤§è¿›å±•ï¼Œä½†ç°æœ‰çš„è¯„ä¼°æ–¹æ³•å¾€å¾€ç¼ºä¹æ•æ‰ç°å®ä¸–ç•Œä¸­å¤æ‚æƒ…å†µå’Œè·¨ç»„ä»¶äº¤äº’æ‰€å¿…éœ€çš„â€œä¸Šä¸‹æ–‡æ„ŸçŸ¥ç¨³å¥æ€§â€ã€‚ä¸ºäº†è§£å†³è¿™äº›å±€é™æ€§ï¼Œæˆ‘ä»¬æå‡ºäº†â€œVulnSageâ€ï¼Œè¿™æ˜¯ä¸€ä¸ªå…¨é¢çš„è¯„ä¼°æ¡†æ¶ï¼Œä»¥åŠä»C&#x2F;C++å¼€å‘çš„å¤§å‹å¼€æºç³»ç»Ÿè½¯ä»¶é¡¹ç›®ä¸­ç²¾å¿ƒæŒ‘é€‰çš„æ•°æ®é›†ã€‚ä¸ä»¥å‰çš„æ•°æ®é›†ä¸åŒï¼Œå®ƒé‡‡ç”¨å¯å‘å¼å™ªå£°é¢„è¿‡æ»¤æ–¹æ³•ä¸LLMåŸºäºæ¨ç†ç›¸ç»“åˆçš„æ–¹å¼ï¼Œç¡®ä¿äº†ä¸€ä¸ªä»£è¡¨æ€§ä¸”å™ªå£°è¾ƒå°çš„æ¼æ´é¢‘è°±ã€‚è¯¥æ¡†æ¶æ”¯æŒå‡½æ•°ã€æ–‡ä»¶å’Œè·¨åŠŸèƒ½çº§åˆ«çš„å¤šç²’åº¦åˆ†æï¼Œå¹¶é‡‡ç”¨å››ç§ä¸åŒçš„é›¶æ ·æœ¬æç¤ºç­–ç•¥ï¼šåŸºçº¿ã€æ€ç»´é“¾ã€æ€è€ƒä»¥åŠæ€è€ƒå¹¶éªŒè¯ã€‚é€šè¿‡æ­¤æ¬¡è¯„ä¼°ï¼Œæˆ‘ä»¬å‘ç°ç»“æ„åŒ–æ¨ç†æç¤ºå¯ä»¥å¤§å¤§æé«˜LLMçš„æ€§èƒ½ï¼Œå…¶ä¸­æ€è€ƒå¹¶éªŒè¯å°†æ¨¡ç³Šå“åº”ä»20.3%å‡å°‘åˆ°9.1%ï¼ŒåŒæ—¶æé«˜äº†å‡†ç¡®æ€§ã€‚æˆ‘ä»¬è¿˜è¿›ä¸€æ­¥è¯æ˜ï¼Œé’ˆå¯¹ä»£ç çš„ä¸“ä¸šæ¨¡å‹å§‹ç»ˆä¼˜äºé€šç”¨æ¨¡å‹ï¼Œè€Œä¸”æ€§èƒ½å› æ¼æ´ç±»å‹è€Œå¼‚ï¼Œè¿™è¡¨æ˜æ²¡æœ‰ä¸€ç§æ–¹æ³•èƒ½åœ¨æ‰€æœ‰å®‰å…¨ç¯å¢ƒä¸­æ™®éè¡¨ç°ä¼˜å¼‚ã€‚æ•°æ®é›†å’Œä»£ç çš„é“¾æ¥ï¼š<a target="_blank" rel="noopener" href="https://github.com/Erroristotle/VulnSage.git%E3%80%82">https://github.com/Erroristotle/VulnSage.gitã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.17885v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†è‡ªåŠ¨åŒ–è½¯ä»¶æ¼æ´æ£€æµ‹çš„æŒ‘æˆ˜ï¼Œå¹¶æŒ‡å‡ºäº†ç°æœ‰è¯„ä¼°æ–¹æ³•ç¼ºä¹ä¸Šä¸‹æ–‡æ„ŸçŸ¥ç¨³å¥æ€§çš„é—®é¢˜ã€‚ä¸ºè§£å†³è¿™äº›é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§å…¨é¢çš„è¯„ä¼°æ¡†æ¶å’Œæ•°æ®é›†VulnSageï¼Œè¯¥æ•°æ®é›†åˆ©ç”¨å¯å‘å¼å™ªå£°é¢„è¿‡æ»¤æ–¹æ³•å’ŒåŸºäºLLMçš„æ¨ç†æŠ€æœ¯ï¼Œç¡®ä¿æ¼æ´çš„ä»£è¡¨æ€§ä¸”å™ªå£°æœ€å°ã€‚è¯„ä¼°æ¡†æ¶æ”¯æŒå‡½æ•°ã€æ–‡ä»¶å’Œè·¨åŠŸèƒ½çº§åˆ«çš„å¤šç²’åº¦åˆ†æï¼Œå¹¶é‡‡ç”¨å››ç§ä¸åŒçš„é›¶æ ·æœ¬æç¤ºç­–ç•¥ã€‚ç ”ç©¶å‘ç°ç»“æ„åŒ–æ¨ç†æç¤ºå¯æ˜¾è‘—æé«˜LLMæ€§èƒ½ï¼Œä»£ç ä¸“ä¸šåŒ–æ¨¡å‹æ€§èƒ½ä¼˜äºé€šç”¨æ¨¡å‹ï¼Œä¸”æ€§èƒ½åœ¨ä¸åŒæ¼æ´ç±»å‹ä¹‹é—´å­˜åœ¨æ˜¾è‘—å·®å¼‚ã€‚æ•°æ®é›†å’Œä»£ç é“¾æ¥ï¼š<a target="_blank" rel="noopener" href="https://github.com/Erroristotle/VulnSage.git%E3%80%82">https://github.com/Erroristotle/VulnSage.gitã€‚</a></p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å½“å‰è‡ªåŠ¨åŒ–è½¯ä»¶æ¼æ´æ£€æµ‹é¢ä¸´æŒ‘æˆ˜ï¼Œç‰¹åˆ«æ˜¯åœ¨å¤æ‚çš„ç›¸äº’ä¾èµ–çš„è½¯ä»¶ç³»ç»Ÿä¸­ã€‚</li>
<li>ç°æœ‰çš„è¯„ä¼°æ–¹æ³•ç¼ºä¹ä¸Šä¸‹æ–‡æ„ŸçŸ¥ç¨³å¥æ€§ï¼Œéš¾ä»¥æ•æ‰ç°å®ä¸–ç•Œä¸­çš„å¤æ‚æ€§å’Œè·¨ç»„ä»¶äº¤äº’ã€‚</li>
<li>VulnSageæ¡†æ¶æ—¨åœ¨è§£å†³è¿™äº›é—®é¢˜ï¼Œå®ƒæ˜¯ä¸€ä¸ªå…¨é¢çš„è¯„ä¼°æ¡†æ¶å’Œæ•°æ®é›†ã€‚æ•°æ®é›†æºè‡ªå¤§è§„æ¨¡å¼€æºç³»ç»Ÿè½¯ä»¶é¡¹ç›®ï¼ˆä½¿ç”¨C&#x2F;C++ç¼–å†™ï¼‰ã€‚</li>
<li>VulnSageé‡‡ç”¨å¯å‘å¼å™ªå£°é¢„è¿‡æ»¤æ–¹æ³•å’ŒåŸºäºLLMçš„æ¨ç†æŠ€æœ¯æ¥ç¡®ä¿æ•°æ®é›†ä¸­æ¼æ´çš„ä»£è¡¨æ€§ä¸”å™ªå£°æœ€å°ã€‚</li>
<li>è¯„ä¼°æ¡†æ¶æ”¯æŒå¤šç²’åº¦åˆ†æï¼ŒåŒ…æ‹¬å‡½æ•°ã€æ–‡ä»¶å’Œè·¨åŠŸèƒ½çº§åˆ«ã€‚</li>
<li>é‡‡ç”¨å››ç§ä¸åŒçš„é›¶æ ·æœ¬æç¤ºç­–ç•¥è¿›è¡Œç»“æ„åŒ–æ¨ç†æç¤ºï¼Œä»¥æé«˜LLMæ€§èƒ½ã€‚å…¶ä¸­Think &amp; Verifyç­–ç•¥æ˜¾è‘—å‡å°‘äº†æ¨¡ç³Šå“åº”å¹¶æé«˜äº†å‡†ç¡®æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.17885">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-7e6d5531a38c6542d3c1d8b2a72a2fe7.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-bb0cb0023265b2ba940305abd4aca6a7.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-1730b49c55d38b21e94df3ba1382fe3c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a611808111a8f4b44ae40019077ee432.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-bda5edd9245e2bde06e684ebaa449e3e.jpg" align="middle">
</details>


<h1 id="-19"><a href="#-19" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-03-28/R1_Reasoning/">https://kedreamix.github.io/Talk2Paper/Paper/2025-03-28/R1_Reasoning/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/R1-Reasoning/">
                                    <span class="chip bg-color">R1_Reasoning</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-03-28/LLM/">
                    <div class="card-image">
                        
                        <img src="https://pica.zhimg.com/v2-dd109bd7323dd466451e40f008ab6c81.jpg" class="responsive-img" alt="LLM">
                        
                        <span class="card-title">LLM</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            LLM æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-03-28  Mobile-MMLU A Mobile Intelligence Language Understanding Benchmark
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-03-28
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/LLM/" class="post-category">
                                    LLM
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/LLM/">
                        <span class="chip bg-color">LLM</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-03-27/Text-to-Motion/">
                    <div class="card-image">
                        
                        <img src="https://pica.zhimg.com/v2-ded493e8d8cf71fa8db80d83aaf16444.jpg" class="responsive-img" alt="Text-to-Motion">
                        
                        <span class="card-title">Text-to-Motion</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Text-to-Motion æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-03-27  Dance Like a Chicken Low-Rank Stylization for Human Motion Diffusion
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-03-27
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Text-to-Motion/" class="post-category">
                                    Text-to-Motion
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Text-to-Motion/">
                        <span class="chip bg-color">Text-to-Motion</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">30806.7k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
