<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Speech">
    <meta name="description" content="Speech æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-10-10  How much speech data is necessary for ASR in African languages? An   evaluation of data scaling in Kinyarwanda and Kikuyu">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Speech | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pic-private.zhihu.com/v2-4941265dac737ba27e75f33086bdd192~resize:0:q75.jpg?source=1f5c5e47&expiration=1760043589&auth_key=1760043589-0-0-790797aab8a427a09316c948b97f05fb&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Speech</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Speech/">
                                <span class="chip bg-color">Speech</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Speech/" class="post-category">
                                Speech
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-10-10
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-10-11
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    18.4k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    74 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-10-10-æ›´æ–°"><a href="#2025-10-10-æ›´æ–°" class="headerlink" title="2025-10-10 æ›´æ–°"></a>2025-10-10 æ›´æ–°</h1><h2 id="How-much-speech-data-is-necessary-for-ASR-in-African-languages-An-evaluation-of-data-scaling-in-Kinyarwanda-and-Kikuyu"><a href="#How-much-speech-data-is-necessary-for-ASR-in-African-languages-An-evaluation-of-data-scaling-in-Kinyarwanda-and-Kikuyu" class="headerlink" title="How much speech data is necessary for ASR in African languages? An   evaluation of data scaling in Kinyarwanda and Kikuyu"></a>How much speech data is necessary for ASR in African languages? An   evaluation of data scaling in Kinyarwanda and Kikuyu</h2><p><strong>Authors:Benjamin Akera, Evelyn Nafula, Patrick Walukagga, Gilbert Yiga, John Quinn, Ernest Mwebaze</strong></p>
<p>The development of Automatic Speech Recognition (ASR) systems for low-resource African languages remains challenging due to limited transcribed speech data. While recent advances in large multilingual models like OpenAIâ€™s Whisper offer promising pathways for low-resource ASR development, critical questions persist regarding practical deployment requirements. This paper addresses two fundamental concerns for practitioners: determining the minimum data volumes needed for viable performance and characterizing the primary failure modes that emerge in production systems. We evaluate Whisperâ€™s performance through comprehensive experiments on two Bantu languages: systematic data scaling analysis on Kinyarwanda using training sets from 1 to 1,400 hours, and detailed error characterization on Kikuyu using 270 hours of training data. Our scaling experiments demonstrate that practical ASR performance (WER &lt; 13%) becomes achievable with as little as 50 hours of training data, with substantial improvements continuing through 200 hours (WER &lt; 10%). Complementing these volume-focused findings, our error analysis reveals that data quality issues, particularly noisy ground truth transcriptions, account for 38.6% of high-error cases, indicating that careful data curation is as critical as data volume for robust system performance. These results provide actionable benchmarks and deployment guidance for teams developing ASR systems across similar low-resource language contexts. We release accompanying and models see <a target="_blank" rel="noopener" href="https://github.com/SunbirdAI/kinyarwanda-whisper-eval">https://github.com/SunbirdAI/kinyarwanda-whisper-eval</a> </p>
<blockquote>
<p>å¯¹äºéæ´²çš„ä½èµ„æºè¯­è¨€æ¥è¯´ï¼Œç”±äºè¯­éŸ³è½¬å½•æ•°æ®çš„æœ‰é™æ€§ï¼Œè‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰ç³»ç»Ÿçš„å¼€å‘ä»ç„¶å…·æœ‰æŒ‘æˆ˜æ€§ã€‚è™½ç„¶æœ€è¿‘çš„å¤§å‹å¤šè¯­è¨€æ¨¡å‹ï¼ˆå¦‚OpenAIçš„Whisperï¼‰ä¸ºä½èµ„æºASRå¼€å‘æä¾›äº†æœ‰å‰æ™¯çš„è·¯å¾„ï¼Œä½†å…³äºå®é™…éƒ¨ç½²è¦æ±‚çš„å…³é”®é—®é¢˜ä»ç„¶å­˜åœ¨ã€‚è¿™ç¯‡è®ºæ–‡è§£å†³äº†ä»ä¸šè€…é¢ä¸´çš„ä¸¤ä¸ªåŸºæœ¬é—®é¢˜ï¼šç¡®å®šå®ç°å¯è¡Œæ€§èƒ½æ‰€éœ€çš„æœ€å°æ•°æ®é‡ï¼Œä»¥åŠåˆ»ç”»ç”Ÿäº§ç³»ç»Ÿä¸­å‡ºç°çš„ä¸»è¦æ•…éšœæ¨¡å¼ã€‚æˆ‘ä»¬é€šè¿‡ä¸¤ç§ç­å›¾è¯­ï¼ˆKinyarwandaå’ŒKikuyuï¼‰çš„å…¨é¢å®éªŒæ¥è¯„ä¼°Whisperçš„æ€§èƒ½ï¼šé’ˆå¯¹Kinyarwandaè¿›è¡Œç³»ç»Ÿçš„æ•°æ®è§„æ¨¡åˆ†æï¼Œä½¿ç”¨ä»1å°æ—¶åˆ°1400å°æ—¶çš„åŸ¹è®­æ•°æ®é›†ï¼›é’ˆå¯¹Kikuyuè¿›è¡Œè¯¦ç»†çš„è¯¯å·®ç‰¹å¾åˆ†æï¼Œä½¿ç”¨270å°æ—¶çš„åŸ¹è®­æ•°æ®ã€‚æˆ‘ä»¬çš„è§„æ¨¡å®éªŒè¡¨æ˜ï¼Œä½¿ç”¨ä»…50å°æ—¶çš„è®­ç»ƒæ•°æ®å³å¯å®ç°å®ç”¨çš„ASRæ€§èƒ½ï¼ˆWER &lt; 13%ï¼‰ï¼Œå¹¶ä¸”åœ¨å‰200å°æ—¶å†…æŒç»­è·å¾—å®è´¨æ€§æ”¹è¿›ï¼ˆWER &lt; 10%ï¼‰ã€‚è¿™äº›ä½“ç§¯é›†ä¸­å‘ç°çš„è¡¥å……ï¼Œæˆ‘ä»¬çš„è¯¯å·®åˆ†æè¡¨æ˜ï¼Œæ•°æ®è´¨é‡é—®é¢˜ï¼Œå°¤å…¶æ˜¯çœŸå®åœ°é¢æ–‡æœ¬çš„å˜ˆæ‚æ€§ï¼Œå é«˜è¯¯å·®æ¡ˆä¾‹çš„38.6%ï¼Œè¿™è¡¨æ˜ä»”ç»†çš„æ•°æ®æ•´ç†ä¸æ•°æ®é‡å¯¹äºç³»ç»Ÿçš„ç¨³å¥æ€§èƒ½åŒæ ·å…³é”®ã€‚è¿™äº›ç»“æœæä¾›äº†å¯æ“ä½œçš„åŸºå‡†å’Œéƒ¨ç½²æŒ‡å¯¼ï¼Œå¯ä¸ºå¼€å‘ç±»ä¼¼ä½èµ„æºè¯­è¨€ç¯å¢ƒä¸­çš„ASRç³»ç»Ÿçš„å›¢é˜Ÿæä¾›å¸®åŠ©ã€‚æˆ‘ä»¬å‘å¸ƒç›¸å…³æ¨¡å‹å’Œè¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚è§ <a target="_blank" rel="noopener" href="https://github.com/SunbirdAI/kinyarwanda-whisper-eval">https://github.com/SunbirdAI/kinyarwanda-whisper-eval</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.07221v1">PDF</a> </p>
<p><strong>æ‘˜è¦</strong><br>åœ¨éæ´²ä½èµ„æºè¯­è¨€ç¯å¢ƒä¸‹ï¼Œè‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰ç³»ç»Ÿçš„å¼€å‘ä»ç„¶é¢ä¸´æŒ‘æˆ˜ï¼Œä¸»è¦é—®é¢˜åœ¨äºç¼ºä¹è½¬å½•è¯­éŸ³æ•°æ®ã€‚è™½ç„¶å¤§å‹å¤šè¯­è¨€æ¨¡å‹ï¼ˆå¦‚OpenAIçš„Whisperï¼‰ä¸ºè¯¥é—®é¢˜æä¾›äº†æ½œåœ¨è§£å†³æ–¹æ¡ˆï¼Œä½†åœ¨å®é™…åº”ç”¨éƒ¨ç½²æ–¹é¢ä»å­˜åœ¨å…³é”®ç–‘é—®ã€‚æœ¬æ–‡è‡´åŠ›äºè§£å†³ä»ä¸šè€…çš„ä¸¤å¤§æ ¸å¿ƒå…³åˆ‡ï¼šç¡®å®šå®ç°å¯è¡Œæ€§èƒ½æ‰€éœ€çš„æœ€å°æ•°æ®é‡ä»¥åŠè¯†åˆ«ç”Ÿäº§ç³»ç»Ÿä¸­å‡ºç°çš„ä¸»è¦æ•…éšœæ¨¡å¼ã€‚æˆ‘ä»¬é€šè¿‡å¯¹ä¸¤ç§ç­å›¾è¯­ç³»è¯­è¨€çš„å®éªŒè¯„ä¼°äº†Whisperçš„æ€§èƒ½ï¼šé’ˆå¯¹åŸºå°¼äºšå¢æ—ºè¾¾è¯­è¿›è¡Œäº†ç³»ç»Ÿçš„æ•°æ®è§„æ¨¡åˆ†æï¼Œä½¿ç”¨äº†ä»1å°æ—¶åˆ°1400å°æ—¶ä¸ç­‰çš„è®­ç»ƒæ•°æ®é›†ï¼›é’ˆå¯¹åŸºåº“å°¤è¯­ï¼Œåˆ™ä½¿ç”¨270å°æ—¶çš„åŸ¹è®­æ•°æ®è¿›è¡Œäº†è¯¦ç»†çš„è¯¯å·®ç‰¹å¾åˆ†æã€‚æˆ‘ä»¬çš„è§„æ¨¡å®éªŒè¡¨æ˜ï¼Œåªéœ€50å°æ—¶çš„åŸ¹è®­æ•°æ®å³å¯å®ç°å®ç”¨çš„ASRæ€§èƒ½ï¼ˆWER &lt; 13%ï¼‰ï¼Œå¹¶ä¸”åœ¨200å°æ—¶å†…æŒç»­å–å¾—å®è´¨æ€§æ”¹è¿›ï¼ˆWER &lt; 10%ï¼‰ã€‚é™¤äº†å¯¹æ•°æ®é‡è¿›è¡Œå…³æ³¨å¤–ï¼Œæˆ‘ä»¬çš„è¯¯å·®åˆ†æè¿˜æ­ç¤ºï¼Œæ•°æ®è´¨é‡é—®é¢˜ï¼Œç‰¹åˆ«æ˜¯çœŸå®è½¬å½•ä¸­çš„å™ªå£°é—®é¢˜å åˆ°äº†é«˜è¯¯å·®æ¡ˆä¾‹çš„38.6%ï¼Œè¿™è¡¨æ˜å¯¹äºç¨³å¥çš„ç³»ç»Ÿæ€§èƒ½è€Œè¨€ï¼Œæ•°æ®çš„ä»”ç»†ç­›é€‰ä¸æ•°æ®é‡åŒæ ·é‡è¦ã€‚è¿™äº›ç»“æœæä¾›äº†å¯æ“ä½œçš„åŸºå‡†æ ‡å‡†å’Œéƒ¨ç½²æŒ‡å—ï¼Œå¯¹ç±»ä¼¼ä½èµ„æºè¯­è¨€èƒŒæ™¯ä¸‹å¼€å‘ASRç³»ç»Ÿçš„å›¢é˜Ÿå…·æœ‰é‡è¦æ„ä¹‰ã€‚ç›¸å…³ç ”ç©¶æ¨¡å‹å·²åœ¨SunbirdAIå‘å¸ƒçš„kinyarwanda-whisper-evalä¸­è·å¾—å…±äº«ä¸ä½“ç°ã€‚ </p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>ä½èµ„æºéæ´²è¯­è¨€çš„ASRç³»ç»Ÿå¼€å‘é¢ä¸´æŒ‘æˆ˜ï¼Œä¸»è¦æ˜¯ç”±äºç¼ºä¹è½¬å½•è¯­éŸ³æ•°æ®ã€‚</li>
<li>OpenAIçš„Whisperç­‰å¤§å‹å¤šè¯­è¨€æ¨¡å‹ä¸ºä½èµ„æºASRå¼€å‘æä¾›äº†å¸Œæœ›ã€‚</li>
<li>ç¡®å®šå®ç°å¯è¡Œæ€§èƒ½æ‰€éœ€çš„æœ€å°æ•°æ®é‡æ˜¯å…³é”®é—®é¢˜ä¹‹ä¸€ã€‚</li>
<li>ç ”ç©¶å‘ç°ä»…éœ€å°‘é‡è®­ç»ƒæ•°æ®ï¼ˆå¦‚50å°æ—¶ï¼‰å³å¯å®ç°è‰¯å¥½çš„ASRæ€§èƒ½ã€‚</li>
<li>æ•°æ®è´¨é‡é—®é¢˜å¯¹ASRæ€§èƒ½çš„å½±å“ä¸å®¹å¿½è§†ï¼Œç‰¹åˆ«æ˜¯åœ¨çœŸå®è½¬å½•ä¸­çš„å™ªå£°é—®é¢˜ã€‚</li>
<li>ä»”ç»†ç­›é€‰æ•°æ®å¯¹äºå®ç°ç¨³å¥çš„ç³»ç»Ÿæ€§èƒ½è‡³å…³é‡è¦ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.07221">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-985a3eb044be097b7622cd8c7cfc59e6~resize:0:q75.jpg?source=1f5c5e47&expiration=1760043352&auth_key=1760043352-0-0-0ae1fedf731c051d9cb4292696a2961d&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-08101d51ad83a79b369dbaedac779d99~resize:0:q75.jpg?source=1f5c5e47&expiration=1760043360&auth_key=1760043360-0-0-7031e58175a4c3ecdeaeec05f6cea5d1&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-e4a0a6c1fa5ec07fbcfc0134cf462276~resize:0:q75.jpg?source=1f5c5e47&expiration=1760043366&auth_key=1760043366-0-0-d7bfd3710a91b28d71526f2ed55f73fa&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-8c9caa274d138bd0795d0aaaf8dbe51b~resize:0:q75.jpg?source=1f5c5e47&expiration=1760043372&auth_key=1760043372-0-0-255436406e3ed27857b02d8b78c42bed&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-e2c8ae008d782562d1c33a251b7fd5c0~resize:0:q75.jpg?source=1f5c5e47&expiration=1760043379&auth_key=1760043379-0-0-29dc4781f62ca630e225db420852cc79&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Making-Machines-Sound-Sarcastic-LLM-Enhanced-and-Retrieval-Guided-Sarcastic-Speech-Synthesis"><a href="#Making-Machines-Sound-Sarcastic-LLM-Enhanced-and-Retrieval-Guided-Sarcastic-Speech-Synthesis" class="headerlink" title="Making Machines Sound Sarcastic: LLM-Enhanced and Retrieval-Guided   Sarcastic Speech Synthesis"></a>Making Machines Sound Sarcastic: LLM-Enhanced and Retrieval-Guided   Sarcastic Speech Synthesis</h2><p><strong>Authors:Zhu Li, Yuqing Zhang, Xiyuan Gao, Shekhar Nayak, Matt Coler</strong></p>
<p>Sarcasm is a subtle form of non-literal language that poses significant challenges for speech synthesis due to its reliance on nuanced semantic, contextual, and prosodic cues. While existing speech synthesis research has focused primarily on broad emotional categories, sarcasm remains largely unexplored. In this paper, we propose a Large Language Model (LLM)-enhanced Retrieval-Augmented framework for sarcasm-aware speech synthesis. Our approach combines (1) semantic embeddings from a LoRA-fine-tuned LLaMA 3, which capture pragmatic incongruity and discourse-level cues of sarcasm, and (2) prosodic exemplars retrieved via a Retrieval Augmented Generation (RAG) module, which provide expressive reference patterns of sarcastic delivery. Integrated within a VITS backbone, this dual conditioning enables more natural and contextually appropriate sarcastic speech. Experiments demonstrate that our method outperforms baselines in both objective measures and subjective evaluations, yielding improvements in speech naturalness, sarcastic expressivity, and downstream sarcasm detection. </p>
<blockquote>
<p>è®½åˆºæ˜¯ä¸€ç§å¾®å¦™çš„éå­—é¢è¯­è¨€å½¢å¼ï¼Œç”±äºå…¶ä¾èµ–äºå¾®å¦™çš„è¯­ä¹‰ã€ä¸Šä¸‹æ–‡å’ŒéŸµå¾‹çº¿ç´¢ï¼Œå¯¹è¯­éŸ³åˆæˆæ„æˆäº†é‡å¤§æŒ‘æˆ˜ã€‚è™½ç„¶ç°æœ‰çš„è¯­éŸ³åˆæˆç ”ç©¶ä¸»è¦é›†ä¸­åœ¨å¹¿æ³›çš„æƒ…æ„Ÿç±»åˆ«ä¸Šï¼Œä½†è®½åˆºåœ¨å¾ˆå¤§ç¨‹åº¦ä¸Šä»æœªè¢«æ¢ç´¢ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ç»“åˆå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ£€ç´¢å¢å¼ºæ¡†æ¶ï¼Œç”¨äºè®½åˆºæ€§è¯­éŸ³åˆæˆã€‚æˆ‘ä»¬çš„æ–¹æ³•ç»“åˆäº†ï¼ˆ1ï¼‰æ¥è‡ªLoRAå¾®è°ƒLLaMA 3çš„è¯­ä¹‰åµŒå…¥ï¼Œæ•æ‰å®ç”¨ä¸Šçš„ä¸ä¸€è‡´å’Œè¯è¯­å±‚é¢çš„è®½åˆºçº¿ç´¢ï¼›ï¼ˆ2ï¼‰é€šè¿‡æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰æ¨¡å—æ£€ç´¢åˆ°çš„éŸµå¾‹èŒƒä¾‹ï¼Œä¸ºè®½åˆºæ€§è¡¨è¾¾æä¾›äº†è¡¨è¾¾æ€§å‚è€ƒæ¨¡å¼ã€‚åœ¨VITSä¸»å¹²ä¸­é›†æˆï¼Œè¿™ç§åŒé‡æ¡ä»¶ä½¿å¾—æ›´è‡ªç„¶ã€ä¸Šä¸‹æ–‡æ›´æ°å½“çš„è®½åˆºæ€§è¯­éŸ³æˆä¸ºå¯èƒ½ã€‚å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨å®¢è§‚æŒ‡æ ‡å’Œä¸»è§‚è¯„ä¼°ä¸Šå‡ä¼˜äºåŸºçº¿ï¼Œåœ¨è¯­éŸ³è‡ªç„¶åº¦ã€è®½åˆºè¡¨ç°åŠ›å’Œä¸‹æ¸¸è®½åˆºæ£€æµ‹æ–¹é¢å–å¾—äº†æ”¹è¿›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.07096v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ¢è®¨äº†è®½åˆºä½œä¸ºä¸€ç§éå­—é¢è¯­è¨€åœ¨è¯­éŸ³åˆæˆä¸­çš„æŒ‘æˆ˜ï¼Œå¹¶æå‡ºäº†ä¸€ç§ç»“åˆå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ£€ç´¢å¢å¼ºæ¡†æ¶æ¥è§£å†³è¿™ä¸€é—®é¢˜ã€‚è¯¥ç ”ç©¶é€šè¿‡è¯­ä¹‰åµŒå…¥å’Œè¯­éŸ³éŸµå¾‹èŒƒä¾‹çš„æ•´åˆï¼Œæé«˜äº†è®½åˆºè¯­éŸ³çš„è‡ªç„¶åº¦å’Œè¯­å¢ƒé€‚å®œæ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è®½åˆºæ˜¯ä¸€ç§éå­—é¢è¯­è¨€ï¼Œå¯¹è¯­éŸ³åˆæˆæ„æˆæŒ‘æˆ˜ï¼Œéœ€æ•æ‰è¯­ä¹‰ã€è¯­å¢ƒå’ŒéŸµå¾‹çº¿ç´¢ã€‚</li>
<li>ç°æœ‰è¯­éŸ³åˆæˆç ”ç©¶ä¸»è¦å…³æ³¨å¹¿æ³›æƒ…ç»ªç±»åˆ«ï¼Œè€Œè®½åˆºå°šæœªå¾—åˆ°å……åˆ†æ¢ç´¢ã€‚</li>
<li>æå‡ºç»“åˆå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ£€ç´¢å¢å¼ºæ¡†æ¶ï¼Œç”¨äºè®½åˆºæ„è¯†è¯­éŸ³åˆæˆã€‚</li>
<li>ç ”ç©¶é€šè¿‡LoRAå¾®è°ƒLLaMA 3è·å–è¯­ä¹‰åµŒå…¥ï¼Œæ•æ‰è®½åˆºçš„è¯­ç”¨ä¸ä¸€è‡´å’Œç¯‡ç« çº§çº¿ç´¢ã€‚</li>
<li>å€ŸåŠ©æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰æ¨¡å—æ£€ç´¢éŸµå¾‹èŒƒä¾‹ï¼Œæä¾›è®½åˆºè¡¨è¾¾çš„å‚è€ƒæ¨¡å¼ã€‚</li>
<li>åœ¨VITSèƒŒæ™¯ä¸‹é›†æˆä¸Šè¿°åŒé‡æ¡ä»¶ï¼Œä½¿è®½åˆºè¯­éŸ³æ›´è‡ªç„¶ã€è¯­å¢ƒæ›´é€‚å®œã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.07096">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-3531789f38443b66779af1b0c5513ba8~resize:0:q75.jpg?source=1f5c5e47&expiration=1760043387&auth_key=1760043387-0-0-79ba3c7a34a8ae5e32706f7f60b5250d&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-c981097f21d942ca619f24ad33241a35~resize:0:q75.jpg?source=1f5c5e47&expiration=1760043394&auth_key=1760043394-0-0-9c5baa82b8be70f4ebb96a40df60f2ef&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-da4d41c2cba87c4027e47f419abf70da~resize:0:q75.jpg?source=1f5c5e47&expiration=1760043401&auth_key=1760043401-0-0-5fcf45cf7579fef50ec6089ef6b32204&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-90e0da9f7b33347996d2e928fd276e9d~resize:0:q75.jpg?source=1f5c5e47&expiration=1760043407&auth_key=1760043407-0-0-6c054c7c97173b6e21c167c91c710ac0&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Enhancing-Speech-Emotion-Recognition-via-Fine-Tuning-Pre-Trained-Models-and-Hyper-Parameter-Optimisation"><a href="#Enhancing-Speech-Emotion-Recognition-via-Fine-Tuning-Pre-Trained-Models-and-Hyper-Parameter-Optimisation" class="headerlink" title="Enhancing Speech Emotion Recognition via Fine-Tuning Pre-Trained Models   and Hyper-Parameter Optimisation"></a>Enhancing Speech Emotion Recognition via Fine-Tuning Pre-Trained Models   and Hyper-Parameter Optimisation</h2><p><strong>Authors:Aryan Golbaghi, Shuo Zhou</strong></p>
<p>We propose a workflow for speech emotion recognition (SER) that combines pre-trained representations with automated hyperparameter optimisation (HPO). Using SpeechBrain wav2vec2-base model fine-tuned on IEMOCAP as the encoder, we compare two HPO strategies, Gaussian Process Bayesian Optimisation (GP-BO) and Tree-structured Parzen Estimators (TPE), under an identical four-dimensional search space and 15-trial budget, with balanced class accuracy (BCA) on the German EmoDB corpus as the objective. All experiments run on 8 CPU cores with 32 GB RAM. GP-BO achieves 0.96 BCA in 11 minutes, and TPE (Hyperopt implementation) attains 0.97 in 15 minutes. In contrast, grid search requires 143 trials and 1,680 minutes to exceed 0.9 BCA, and the best AutoSpeech 2020 baseline reports only 0.85 in 30 minutes on GPU. For cross-lingual generalisation, an EmoDB-trained HPO-tuned model improves zero-shot accuracy by 0.25 on CREMA-D and 0.26 on RAVDESS. Results show that efficient HPO with pre-trained encoders delivers competitive SER on commodity CPUs. Source code to this work is available at: <a target="_blank" rel="noopener" href="https://github.com/youngaryan/speechbrain-emotion-hpo">https://github.com/youngaryan/speechbrain-emotion-hpo</a>. </p>
<blockquote>
<p>æˆ‘ä»¬æå‡ºäº†ä¸€ç§ç»“åˆé¢„è®­ç»ƒè¡¨ç¤ºå’Œè‡ªåŠ¨åŒ–è¶…å‚æ•°ä¼˜åŒ–ï¼ˆHPOï¼‰çš„è¯­éŸ³æƒ…æ„Ÿè¯†åˆ«ï¼ˆSERï¼‰å·¥ä½œæµç¨‹ã€‚ä½¿ç”¨åœ¨IEMOCAPä¸Šå¾®è°ƒè¿‡çš„SpeechBrain wav2vec2-baseæ¨¡å‹ä½œä¸ºç¼–ç å™¨ï¼Œæˆ‘ä»¬åœ¨ç›¸åŒçš„å››ç»´æœç´¢ç©ºé—´å’Œ15æ¬¡è¯•éªŒé¢„ç®—ä¸‹ï¼Œæ¯”è¾ƒäº†é«˜æ–¯è¿‡ç¨‹è´å¶æ–¯ä¼˜åŒ–ï¼ˆGP-BOï¼‰å’Œæ ‘å½¢å¸•æ›¾ä¼°è®¡å™¨ï¼ˆTPEï¼‰è¿™ä¸¤ç§HPOç­–ç•¥ï¼Œä»¥å¾·å›½EmoDBè¯­æ–™åº“ä¸Šçš„å¹³è¡¡ç±»å‡†ç¡®ç‡ï¼ˆBCAï¼‰ä½œä¸ºç›®æ ‡ã€‚æ‰€æœ‰å®éªŒå‡åœ¨8ä¸ªCPUæ ¸å¿ƒå’Œ32GB RAMä¸Šè¿›è¡Œã€‚GP-BOåœ¨11åˆ†é’Ÿå†…è¾¾åˆ°0.96çš„BCAï¼Œè€ŒTPEï¼ˆHyperoptå®ç°ï¼‰åœ¨15åˆ†é’Ÿå†…è¾¾åˆ°0.97ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼Œç½‘æ ¼æœç´¢éœ€è¦143æ¬¡è¯•éªŒå’Œ1680åˆ†é’Ÿæ‰èƒ½è¾¾åˆ°è¶…è¿‡0.9çš„BCAï¼Œè€Œæœ€ä½³çš„AutoSpeech 2020åŸºå‡†æŠ¥å‘Šä»…åœ¨GPUä¸Šè¿è¡Œ30åˆ†é’Ÿï¼Œå‡†ç¡®ç‡ä¸º0.85ã€‚å¯¹äºè·¨è¯­è¨€æ³›åŒ–ï¼Œä½¿ç”¨EmoDBè®­ç»ƒçš„HPOè°ƒæ•´æ¨¡å‹åœ¨CREMA-Då’ŒRAVDESSä¸Šçš„é›¶æ ·æœ¬å‡†ç¡®ç‡åˆ†åˆ«æé«˜äº†0.25å’Œ0.26ã€‚ç»“æœè¡¨æ˜ï¼Œä½¿ç”¨é¢„è®­ç»ƒç¼–ç å™¨çš„æœ‰æ•ˆHPOåœ¨å•†å“CPUä¸Šæä¾›äº†å…·æœ‰ç«äº‰åŠ›çš„SERæ€§èƒ½ã€‚è¯¥å·¥ä½œçš„æºä»£ç å¯åœ¨ä»¥ä¸‹ç½‘å€æ‰¾åˆ°ï¼š<a target="_blank" rel="noopener" href="https://github.com/youngaryan/speechbrain-emotion-hpo%E3%80%82">https://github.com/youngaryan/speechbrain-emotion-hpoã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.07052v1">PDF</a> </p>
<p><strong>æ‘˜è¦</strong><br>    æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§ç»“åˆé¢„è®­ç»ƒè¡¨ç¤ºå’Œè‡ªåŠ¨åŒ–è¶…å‚æ•°ä¼˜åŒ–ï¼ˆHPOï¼‰çš„è¯­éŸ³æƒ…æ„Ÿè¯†åˆ«ï¼ˆSERï¼‰å·¥ä½œæµç¨‹ã€‚ç ”ç©¶ä½¿ç”¨SpeechBrainçš„wav2vec2-baseæ¨¡å‹ä½œä¸ºç¼–ç å™¨ï¼Œåœ¨IEMOCAPä¸Šè¿›è¡Œå¾®è°ƒã€‚å¯¹æ¯”äº†Gaussian Process Bayesian Optimisationï¼ˆGP-BOï¼‰å’ŒTree-structured Parzen Estimatorsï¼ˆTPEï¼‰ä¸¤ç§HPOç­–ç•¥ï¼Œåœ¨ç›¸åŒçš„å››ç»´æœç´¢ç©ºé—´å’Œ15æ¬¡è¯•éªŒé¢„ç®—ä¸‹ï¼Œä»¥å¾·å›½EmoDBè¯­æ–™åº“ä¸Šçš„å¹³è¡¡ç±»å‡†ç¡®ç‡ï¼ˆBCAï¼‰ä¸ºè¯„ä»·æŒ‡æ ‡è¿›è¡Œå®éªŒã€‚æ‰€æœ‰å®éªŒå‡åœ¨8ä¸ªCPUæ ¸å¿ƒå’Œ32GB RAMä¸Šè¿›è¡Œã€‚GP-BOåœ¨11åˆ†é’Ÿå†…è¾¾åˆ°0.96çš„BCAï¼Œè€ŒTPEï¼ˆHyperoptå®ç°ï¼‰åœ¨15åˆ†é’Ÿå†…è¾¾åˆ°0.97ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼Œç½‘æ ¼æœç´¢éœ€è¦143æ¬¡è¯•éªŒå’Œ1,680åˆ†é’Ÿæ‰èƒ½è¾¾åˆ°è¶…è¿‡0.9çš„BCAï¼Œè€Œæœ€ä½³çš„AutoSpeech 2020åŸºçº¿ä»…åœ¨30åˆ†é’Ÿå†…è¾¾åˆ°0.85ï¼ˆåœ¨GPUä¸Šï¼‰ã€‚å¯¹äºè·¨è¯­è¨€æ³›åŒ–ï¼Œä½¿ç”¨EmoDBè®­ç»ƒçš„HPOè°ƒä¼˜æ¨¡å‹åœ¨é›¶æ ·æœ¬æƒ…å†µä¸‹ï¼ŒCREMA-Då’ŒRAVDESSä¸Šçš„å‡†ç¡®ç‡åˆ†åˆ«æé«˜äº†0.25å’Œ0.26ã€‚ç»“æœè¡¨æ˜ï¼Œä½¿ç”¨é¢„è®­ç»ƒç¼–ç å™¨çš„é«˜æ•ˆHPOåœ¨å•†å“CPUä¸Šå³å¯å®ç°æœ‰ç«äº‰åŠ›çš„SERæ€§èƒ½ã€‚ç›¸å…³æºä»£ç å·²å…¬å¼€äºï¼š<a target="_blank" rel="noopener" href="https://github.com/youngaryan/speechbrain-emotion-hpo%E3%80%82">https://github.com/youngaryan/speechbrain-emotion-hpoã€‚</a> </p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>ç ”ç©¶æå‡ºäº†ä¸€ç§ç»“åˆé¢„è®­ç»ƒæ¨¡å‹å’Œè‡ªåŠ¨åŒ–è¶…å‚æ•°ä¼˜åŒ–ï¼ˆHPOï¼‰çš„è¯­éŸ³æƒ…æ„Ÿè¯†åˆ«ï¼ˆSERï¼‰å·¥ä½œæµç¨‹ã€‚</li>
<li>å¯¹æ¯”äº†GP-BOå’ŒTPEä¸¤ç§HPOç­–ç•¥ï¼Œæ˜¾ç¤ºå®ƒä»¬ç›¸è¾ƒäºç½‘æ ¼æœç´¢åœ¨æ—¶é—´å’Œæ€§èƒ½ä¸Šå…·æœ‰ä¼˜åŠ¿ã€‚</li>
<li>åœ¨å¾·å›½EmoDBè¯­æ–™åº“ä¸Šï¼ŒHPOç­–ç•¥é…åˆé¢„è®­ç»ƒæ¨¡å‹è¾¾åˆ°äº†è¾ƒé«˜çš„å¹³è¡¡ç±»å‡†ç¡®ç‡ï¼ˆBCAï¼‰ã€‚</li>
<li>GP-BOåœ¨11åˆ†é’Ÿå†…è¾¾åˆ°0.96çš„BCAï¼ŒTPEåœ¨ç¨é•¿çš„æ—¶é—´å†…è¾¾åˆ°0.97çš„BCAã€‚</li>
<li>ç›¸è¾ƒäºæœ€ä½³AutoSpeech 2020åŸºçº¿ï¼Œæå‡ºçš„ç­–ç•¥åœ¨æ€§èƒ½ä¸Šæœ‰æ‰€æå‡ã€‚</li>
<li>ç ”ç©¶çš„æ¨¡å‹åœ¨é›¶æ ·æœ¬æƒ…å†µä¸‹ï¼Œå¯¹CREMA-Då’ŒRAVDESSæ•°æ®é›†çš„æ³›åŒ–èƒ½åŠ›è¾ƒå¼ºã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.07052">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-280123de68487b234e77f0e488200740~resize:0:q75.jpg?source=1f5c5e47&expiration=1760043415&auth_key=1760043415-0-0-29f9dd7877dd6cfac523bcc91ed3e275&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-a6c0bf66a96546bdfcfd71a737c325ca~resize:0:q75.jpg?source=1f5c5e47&expiration=1760043423&auth_key=1760043423-0-0-f72ae457132a77994c731a11f2cd4497&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-b141bda1dfb3a0ca6357505b74b2c401~resize:0:q75.jpg?source=1f5c5e47&expiration=1760043430&auth_key=1760043430-0-0-d5c0a77c4f5cafbd3a00b47d68708ec8&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-67b206753f931b781d3acf763b9e3a9b~resize:0:q75.jpg?source=1f5c5e47&expiration=1760043436&auth_key=1760043436-0-0-1f223e97f58fcc84e2a7fd2d8513b77a&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="SHANKS-Simultaneous-Hearing-and-Thinking-for-Spoken-Language-Models"><a href="#SHANKS-Simultaneous-Hearing-and-Thinking-for-Spoken-Language-Models" class="headerlink" title="SHANKS: Simultaneous Hearing and Thinking for Spoken Language Models"></a>SHANKS: Simultaneous Hearing and Thinking for Spoken Language Models</h2><p><strong>Authors:Cheng-Han Chiang, Xiaofei Wang, Linjie Li, Chung-Ching Lin, Kevin Lin, Shujie Liu, Zhendong Wang, Zhengyuan Yang, Hung-yi Lee, Lijuan Wang</strong></p>
<p>Current large language models (LLMs) and spoken language models (SLMs) begin thinking and taking actions only after the user has finished their turn. This prevents the model from interacting during the userâ€™s turn and can lead to high response latency while it waits to think. Consequently, thinking after receiving the full input is not suitable for speech-to-speech interaction, where real-time, low-latency exchange is important. We address this by noting that humans naturally â€œthink while listening.â€ In this paper, we propose SHANKS, a general inference framework that enables SLMs to generate unspoken chain-of-thought reasoning while listening to the user input. SHANKS streams the input speech in fixed-duration chunks and, as soon as a chunk is received, generates unspoken reasoning based on all previous speech and reasoning, while the user continues speaking. SHANKS uses this unspoken reasoning to decide whether to interrupt the user and to make tool calls to complete the task. We demonstrate that SHANKS enhances real-time user-SLM interaction in two scenarios: (1) when the user is presenting a step-by-step solution to a math problem, SHANKS can listen, reason, and interrupt when the user makes a mistake, achieving 37.1% higher interruption accuracy than a baseline that interrupts without thinking; and (2) in a tool-augmented dialogue, SHANKS can complete 56.9% of the tool calls before the user finishes their turn. Overall, SHANKS moves toward models that keep thinking throughout the conversation, not only after a turn ends. Animated illustrations of Shanks can be found at <a target="_blank" rel="noopener" href="https://d223302.github.io/SHANKS/">https://d223302.github.io/SHANKS/</a> </p>
<blockquote>
<p>å½“å‰çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å’Œå£è¯­æ¨¡å‹ï¼ˆSLMï¼‰éƒ½æ˜¯åœ¨ç”¨æˆ·å®Œæˆå‘è¨€åæ‰å¼€å§‹æ€è€ƒå’Œè¡ŒåŠ¨ã€‚è¿™é˜»æ­¢äº†æ¨¡å‹åœ¨ç”¨æˆ·å‘è¨€è¿‡ç¨‹ä¸­çš„äº¤äº’ï¼Œå¹¶å¯èƒ½å¯¼è‡´å“åº”å»¶è¿Ÿã€‚å› æ­¤ï¼Œåœ¨æ¥æ”¶å®Œæ•´è¾“å…¥åå†æ€è€ƒä¸é€‚åˆè¯­éŸ³åˆ°è¯­éŸ³çš„äº¤äº’ï¼Œå…¶ä¸­å®æ—¶ã€ä½å»¶è¿Ÿçš„äº¤äº’éå¸¸é‡è¦ã€‚æˆ‘ä»¬é€šè¿‡æ³¨æ„åˆ°äººç±»è‡ªç„¶åœ°â€œè¾¹å¬è¾¹æ€è€ƒâ€æ¥è§£å†³è¿™ä¸ªé—®é¢˜ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†SHANKSï¼Œè¿™æ˜¯ä¸€ä¸ªé€šç”¨æ¨ç†æ¡†æ¶ï¼Œå®ƒä½¿SLMèƒ½å¤Ÿåœ¨å¬å–ç”¨æˆ·è¾“å…¥æ—¶ç”Ÿæˆæœªè¯´å‡ºçš„æ€ç»´é“¾æ¨ç†ã€‚SHANKSä»¥å›ºå®šæŒç»­æ—¶é—´çš„å—æµè¾“å…¥è¯­éŸ³ï¼Œåœ¨æ”¶åˆ°ä¸€ä¸ªå—åï¼Œç«‹å³åŸºäºä¹‹å‰çš„è¯­éŸ³å’Œæ¨ç†ç”Ÿæˆæœªè¯´å‡ºçš„æ€è€ƒï¼ŒåŒæ—¶ç”¨æˆ·ç»§ç»­è¯´è¯ã€‚SHANKSä½¿ç”¨è¿™ç§æœªè¯´å‡ºçš„æ¨ç†æ¥å†³å®šæ˜¯å¦ä¸­æ–­ç”¨æˆ·å¹¶è¿›è¡Œå·¥å…·è°ƒç”¨ä»¥å®Œæˆä»»åŠ¡ã€‚æˆ‘ä»¬è¯æ˜äº†SHANKSåœ¨ä¸¤ç§æƒ…å†µä¸‹å¢å¼ºäº†å®æ—¶ç”¨æˆ·ä¸SLMçš„äº¤äº’ï¼šï¼ˆ1ï¼‰å½“ç”¨æˆ·é€æ­¥è§£å†³æ•°å­¦é—®é¢˜æ—¶ï¼ŒSHANKSå¯ä»¥å€¾å¬ã€æ¨ç†ï¼Œå¹¶åœ¨ç”¨æˆ·çŠ¯é”™æ—¶ä¸­æ–­ï¼Œå…¶ä¸­æ–­å‡†ç¡®æ€§æ¯”åœ¨æ²¡æœ‰æ€è€ƒçš„æƒ…å†µä¸‹ä¸­æ–­çš„åŸºçº¿é«˜å‡º37.1%ï¼›ï¼ˆ2ï¼‰åœ¨å·¥å…·å¢å¼ºçš„å¯¹è¯ä¸­ï¼ŒSHANKSå¯ä»¥åœ¨ç”¨æˆ·å®Œæˆå‘è¨€ä¹‹å‰å®Œæˆ56.9%çš„å·¥å…·è°ƒç”¨ã€‚æ€»çš„æ¥è¯´ï¼ŒSHANKSæœç€è®©æ¨¡å‹åœ¨æ•´ä¸ªå¯¹è¯è¿‡ç¨‹ä¸­ä¿æŒæ€è€ƒçš„æ–¹å‘å‘å±•ï¼Œè€Œä¸ä»…ä»…æ˜¯åœ¨ä¸€è½®å¯¹è¯ç»“æŸåã€‚æœ‰å…³Shanksçš„åŠ¨ç”»æ’å›¾ï¼Œè¯·è®¿é—®ï¼š[<a target="_blank" rel="noopener" href="https://d223302.github.io/SHANKS/]">https://d223302.github.io/SHANKS/]</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.06917v1">PDF</a> Work in progress</p>
<p><strong>Summary</strong>ï¼š</p>
<p>å½“å‰å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å’Œå£è¯­æ¨¡å‹ï¼ˆSLMsï¼‰åœ¨ç”¨æˆ·å®Œæˆå‘è¨€åæ‰è¿›è¡Œæ€è€ƒå¹¶è¡ŒåŠ¨ï¼Œè¿™å¯¼è‡´æ¨¡å‹æ— æ³•åœ¨ç”¨æˆ·å‘è¨€è¿‡ç¨‹ä¸­è¿›è¡Œäº¤äº’ï¼Œä»è€Œäº§ç”Ÿè¾ƒé«˜çš„å“åº”å»¶è¿Ÿã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºSHANKSçš„é€šç”¨æ¨ç†æ¡†æ¶ï¼Œå®ƒä½¿SLMsèƒ½å¤Ÿåœ¨å¬å–ç”¨æˆ·è¾“å…¥çš„åŒæ—¶è¿›è¡Œæœªè¨€æ˜çš„æ€ç»´æ¨ç†ã€‚SHANKSå°†è¾“å…¥è¯­éŸ³æµåˆ’åˆ†ä¸ºå›ºå®šæ—¶é•¿çš„ç‰‡æ®µï¼Œå¹¶åœ¨æ¥æ”¶åˆ°ç‰‡æ®µåç«‹å³åŸºäºä¹‹å‰çš„è¯­éŸ³å’Œæ¨ç†ç”Ÿæˆæœªè¨€æ˜çš„æ€è€ƒï¼ŒåŒæ—¶ç”¨æˆ·å¯ä»¥ç»§ç»­å‘è¨€ã€‚è¯¥æ¡†æ¶é€šè¿‡æœªè¨€æ˜çš„æ€è€ƒæ¥å†³å®šæ˜¯å¦æ‰“æ–­ç”¨æˆ·å¹¶å®Œæˆä»»åŠ¡ã€‚åœ¨ç”¨æˆ·çš„æ•°å­¦é—®é¢˜è§£å†³æ­¥éª¤å±•ç¤ºå’Œå·¥å…·å¢å¼ºå¯¹è¯åœºæ™¯ä¸­ï¼ŒSHANKSè¡¨ç°å‡ºä¼˜ç§€çš„å®æ—¶ç”¨æˆ·-SLMäº¤äº’æ€§èƒ½ï¼ŒåŒ…æ‹¬åœ¨ç”¨æˆ·çŠ¯é”™æ—¶èƒ½å¤Ÿå‡†ç¡®æ‰“æ–­å¹¶æå‰å®Œæˆå·¥å…·è°ƒç”¨ã€‚æ€»ä¹‹ï¼ŒSHANKSæ¨åŠ¨äº†æ¨¡å‹åœ¨å¯¹è¯è¿‡ç¨‹ä¸­çš„æŒç»­æ€è€ƒï¼Œè€Œä¸ä»…ä»…æ˜¯åœ¨ä¸€ä¸ªå›åˆç»“æŸåã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>å½“å‰å¤§å‹è¯­è¨€æ¨¡å‹å’Œå£è¯­æ¨¡å‹åœ¨ç”¨æˆ·å®Œæˆå‘è¨€åæ‰è¿›è¡Œæ€è€ƒï¼Œå¯¼è‡´äº¤äº’å»¶è¿Ÿã€‚</li>
<li>SHANKSæ¡†æ¶èƒ½ä½¿SLMsåœ¨ç”¨æˆ·å‘è¨€è¿‡ç¨‹ä¸­è¿›è¡Œæœªè¨€æ˜çš„æ€ç»´æ¨ç†ã€‚</li>
<li>SHANKSé€šè¿‡å›ºå®šæ—¶é•¿è¯­éŸ³ç‰‡æ®µçš„æµå¼å¤„ç†æ¥å®ç°å®æ—¶æ¨ç†ã€‚</li>
<li>SHANKSèƒ½å¤Ÿåœ¨ç”¨æˆ·çŠ¯é”™æ—¶å‡†ç¡®æ‰“æ–­å¹¶æå‰å®Œæˆå·¥å…·è°ƒç”¨ã€‚</li>
<li>SHANKSæé«˜äº†å®æ—¶ç”¨æˆ·-SLMäº¤äº’çš„æ•ˆæœã€‚</li>
<li>SHANKSé€šè¿‡æœªè¨€æ˜çš„æ€è€ƒæ¨åŠ¨æ¨¡å‹åœ¨å¯¹è¯è¿‡ç¨‹ä¸­çš„æŒç»­æ€è€ƒã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.06917">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-3f139f75350402e7dbcef59962db2850~resize:0:q75.jpg?source=1f5c5e47&expiration=1760043444&auth_key=1760043444-0-0-813a66abd5e3f8f0bc663e9a5eadd321&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-803df79cd6bb2806b37f96c795cee3ad~resize:0:q75.jpg?source=1f5c5e47&expiration=1760043451&auth_key=1760043451-0-0-f5c9cee8e13a74d8ccd865cd239b670f&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-77208d58dd00b35ca19159fc9e22ce30~resize:0:q75.jpg?source=1f5c5e47&expiration=1760043457&auth_key=1760043457-0-0-0e5ead7fd1b198e815067ef8868a17d4&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="EmoHRNet-High-Resolution-Neural-Network-Based-Speech-Emotion-Recognition"><a href="#EmoHRNet-High-Resolution-Neural-Network-Based-Speech-Emotion-Recognition" class="headerlink" title="EmoHRNet: High-Resolution Neural Network Based Speech Emotion   Recognition"></a>EmoHRNet: High-Resolution Neural Network Based Speech Emotion   Recognition</h2><p><strong>Authors:Akshay Muppidi, Martin Radfar</strong></p>
<p>Speech emotion recognition (SER) is pivotal for enhancing human-machine interactions. This paper introduces â€œEmoHRNetâ€, a novel adaptation of High-Resolution Networks (HRNet) tailored for SER. The HRNet structure is designed to maintain high-resolution representations from the initial to the final layers. By transforming audio samples into spectrograms, EmoHRNet leverages the HRNet architecture to extract high-level features. EmoHRNetâ€™s unique architecture maintains high-resolution representations throughout, capturing both granular and overarching emotional cues from speech signals. The model outperforms leading models, achieving accuracies of 92.45% on RAVDESS, 80.06% on IEMOCAP, and 92.77% on EMOVO. Thus, we show that EmoHRNet sets a new benchmark in the SER domain. </p>
<blockquote>
<p>è¯­éŸ³æƒ…æ„Ÿè¯†åˆ«ï¼ˆSERï¼‰å¯¹äºå¢å¼ºäººæœºäº¤äº’è‡³å…³é‡è¦ã€‚æœ¬æ–‡ä»‹ç»äº†â€EmoHRNetâ€ï¼Œè¿™æ˜¯é’ˆå¯¹SERå®šåˆ¶çš„High-Resolution Networksï¼ˆHRNetï¼‰çš„ä¸€ç§æ–°å‹é€‚åº”ã€‚HRNetç»“æ„æ—¨åœ¨ä»åˆå§‹å±‚åˆ°æœ€ç»ˆå±‚ä¿æŒé«˜åˆ†è¾¨ç‡è¡¨ç¤ºã€‚é€šè¿‡å°†éŸ³é¢‘æ ·æœ¬è½¬æ¢ä¸ºé¢‘è°±å›¾ï¼ŒEmoHRNetåˆ©ç”¨HRNetæ¶æ„æå–é«˜çº§ç‰¹å¾ã€‚EmoHRNetçš„ç‹¬ç‰¹æ¶æ„åœ¨æ•´ä¸ªè¿‡ç¨‹ä¸­ä¿æŒäº†é«˜åˆ†è¾¨ç‡çš„è¡¨ç¤ºï¼Œä»è¯­éŸ³ä¿¡å·ä¸­æ•è·äº†ç²’åº¦å’Œæ€»ä½“çš„æƒ…æ„Ÿçº¿ç´¢ã€‚è¯¥æ¨¡å‹åœ¨RAVDESSä¸Šè¾¾åˆ°äº†92.45%çš„å‡†ç¡®ç‡ï¼Œåœ¨IEMOCAPä¸Šè¾¾åˆ°äº†8.06%ï¼Œåœ¨EMOVOä¸Šè¾¾åˆ°äº†92.77%ï¼Œå› æ­¤ï¼Œæˆ‘ä»¬è¯æ˜äº†EmoHRNetåœ¨SERé¢†åŸŸæ ‘ç«‹äº†æ–°çš„åŸºå‡†ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.06072v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†é’ˆå¯¹è¯­éŸ³æƒ…æ„Ÿè¯†åˆ«ï¼ˆSERï¼‰çš„æ–°å‹ç½‘ç»œæ¨¡å‹EmoHRNetã€‚è¯¥æ¨¡å‹åŸºäºé«˜åˆ†è¾¨ç‡ç½‘ç»œï¼ˆHRNetï¼‰æ¶æ„è¿›è¡Œè®¾è®¡ï¼Œå¯å…¨ç¨‹ç»´æŒé«˜åˆ†è¾¨ç‡è¡¨è¾¾ï¼Œæ•æ‰éŸ³é¢‘ä¿¡å·çš„ç»†å¾®ä¸æ•´ä½“æƒ…æ„Ÿçº¿ç´¢ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒEmoHRNetåœ¨RAVDESSã€IEMOCAPå’ŒEMOVOä¸‰ä¸ªæ•°æ®é›†ä¸Šçš„å‡†ç¡®ç‡åˆ†åˆ«è¾¾åˆ°äº†92.45%ã€80.06%å’Œ92.77%ï¼Œä¸ºSERé¢†åŸŸæ ‘ç«‹äº†æ–°çš„åŸºå‡†ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>EmoHRNetæ˜¯ä¸€ä¸ªé’ˆå¯¹è¯­éŸ³æƒ…æ„Ÿè¯†åˆ«ï¼ˆSERï¼‰ä»»åŠ¡çš„é«˜åˆ†è¾¨ç‡ç½‘ç»œæ¨¡å‹ã€‚</li>
<li>EmoHRNeté€šè¿‡ç»´æŒé«˜åˆ†è¾¨ç‡è¡¨è¾¾ï¼Œèƒ½å¤Ÿæ•æ‰éŸ³é¢‘ä¿¡å·ä¸­çš„ç»†å¾®ä¸æ•´ä½“æƒ…æ„Ÿçº¿ç´¢ã€‚</li>
<li>EmoHRNetå°†éŸ³é¢‘æ ·æœ¬è½¬æ¢ä¸ºè°±å›¾ï¼Œåˆ©ç”¨HRNetæ¶æ„æå–é«˜çº§ç‰¹å¾ã€‚</li>
<li>æ¨¡å‹åœ¨RAVDESSã€IEMOCAPå’ŒEMOVOæ•°æ®é›†ä¸Šçš„è¡¨ç°å‡è¶…è¿‡äº†ç°æœ‰æ¨¡å‹ã€‚</li>
<li>EmoHRNetåœ¨RAVDESSæ•°æ®é›†ä¸Šçš„å‡†ç¡®ç‡è¾¾åˆ°äº†92.45%ã€‚</li>
<li>EmoHRNetåœ¨IEMOCAPæ•°æ®é›†ä¸Šçš„å‡†ç¡®ç‡ä¸º80.06%ã€‚</li>
<li>EmoHRNetåœ¨EMOVOæ•°æ®é›†ä¸Šçš„å‡†ç¡®ç‡è¾¾åˆ°äº†92.77%ï¼Œæ˜¾ç¤ºå‡ºå…¶åœ¨æ–°é¢†åŸŸä¸­çš„å¼ºå¤§æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.06072">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-cc2b126653ec8ea51e4bdd658fcafd5c~resize:0:q75.jpg?source=1f5c5e47&expiration=1760043465&auth_key=1760043465-0-0-22cb94014451dbea8ee60f826788559f&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-f545cc5e0a70383d9e6c48e1d7e06ff4~resize:0:q75.jpg?source=1f5c5e47&expiration=1760043473&auth_key=1760043473-0-0-e1f500c3227bac3c1119e768e0b1e93a&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-45a7d9ad4be539ad8acecc3c08f38bd9~resize:0:q75.jpg?source=1f5c5e47&expiration=1760043479&auth_key=1760043479-0-0-3afaddc33d9e8e0001ffcbe60d4c2f29&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-9c3b3c62bd6185c59f6fdeeae9ce7c6a~resize:0:q75.jpg?source=1f5c5e47&expiration=1760043486&auth_key=1760043486-0-0-775a2b97379ddd8d5202bcbd6c48fe4d&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="ECTSpeech-Enhancing-Efficient-Speech-Synthesis-via-Easy-Consistency-Tuning"><a href="#ECTSpeech-Enhancing-Efficient-Speech-Synthesis-via-Easy-Consistency-Tuning" class="headerlink" title="ECTSpeech: Enhancing Efficient Speech Synthesis via Easy Consistency   Tuning"></a>ECTSpeech: Enhancing Efficient Speech Synthesis via Easy Consistency   Tuning</h2><p><strong>Authors:Tao Zhu, Yinfeng Yu, Liejun Wang, Fuchun Sun, Wendong Zheng</strong></p>
<p>Diffusion models have demonstrated remarkable performance in speech synthesis, but typically require multi-step sampling, resulting in low inference efficiency. Recent studies address this issue by distilling diffusion models into consistency models, enabling efficient one-step generation. However, these approaches introduce additional training costs and rely heavily on the performance of pre-trained teacher models. In this paper, we propose ECTSpeech, a simple and effective one-step speech synthesis framework that, for the first time, incorporates the Easy Consistency Tuning (ECT) strategy into speech synthesis. By progressively tightening consistency constraints on a pre-trained diffusion model, ECTSpeech achieves high-quality one-step generation while significantly reducing training complexity. In addition, we design a multi-scale gate module (MSGate) to enhance the denoiserâ€™s ability to fuse features at different scales. Experimental results on the LJSpeech dataset demonstrate that ECTSpeech achieves audio quality comparable to state-of-the-art methods under single-step sampling, while substantially reducing the modelâ€™s training cost and complexity. </p>
<blockquote>
<p>æ‰©æ•£æ¨¡å‹åœ¨è¯­éŸ³åˆæˆä¸­è¡¨ç°å‡ºäº†å‡ºè‰²çš„æ€§èƒ½ï¼Œä½†é€šå¸¸éœ€è¦å¤šæ­¥é‡‡æ ·ï¼Œå¯¼è‡´æ¨ç†æ•ˆç‡ä½ä¸‹ã€‚æœ€è¿‘çš„ç ”ç©¶é€šè¿‡å°†æ‰©æ•£æ¨¡å‹æç‚¼æˆä¸€è‡´æ€§æ¨¡å‹æ¥è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œå®ç°äº†ä¸€æ¬¡æ€§ç”Ÿæˆã€‚ç„¶è€Œï¼Œè¿™äº›æ–¹æ³•å¢åŠ äº†é¢å¤–çš„è®­ç»ƒæˆæœ¬ï¼Œå¹¶ä¸¥é‡ä¾èµ–äºé¢„è®­ç»ƒæ•™å¸ˆæ¨¡å‹çš„æ€§èƒ½ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ECTSpeechï¼Œè¿™æ˜¯ä¸€ä¸ªç®€å•æœ‰æ•ˆçš„ä¸€æ¬¡æ€§è¯­éŸ³åˆæˆæ¡†æ¶ï¼Œé¦–æ¬¡å°†Easy Consistency Tuningï¼ˆECTï¼‰ç­–ç•¥èå…¥è¯­éŸ³åˆæˆã€‚é€šè¿‡å¯¹é¢„è®­ç»ƒæ‰©æ•£æ¨¡å‹ä¸Šçš„ä¸€è‡´æ€§çº¦æŸè¿›è¡Œé€æ­¥åŠ å¼ºï¼ŒECTSpeechå®ç°äº†ä¸€æ¬¡æ€§é«˜è´¨é‡ç”Ÿæˆï¼ŒåŒæ—¶æ˜¾è‘—é™ä½äº†è®­ç»ƒå¤æ‚åº¦ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸€ä¸ªå¤šå°ºåº¦é—¨æ¨¡å—ï¼ˆMSGateï¼‰ä»¥å¢å¼ºå»å™ªå™¨åœ¨ä¸åŒå°ºåº¦ä¸Šèåˆç‰¹å¾çš„èƒ½åŠ›ã€‚åœ¨LJSpeechæ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼ŒECTSpeechåœ¨ä¸€æ¬¡æ€§é‡‡æ ·ä¸‹ç”Ÿæˆçš„éŸ³é¢‘è´¨é‡ä¸æœ€å…ˆè¿›çš„æ–¹æ³•ç›¸å½“ï¼ŒåŒæ—¶å¤§å¤§é™ä½äº†æ¨¡å‹çš„è®­ç»ƒæˆæœ¬å’Œå¤æ‚åº¦ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.05984v1">PDF</a> Accepted for publication by Proceedings of the 2025 ACM Multimedia   Asia Conference(MMAsia â€˜25)</p>
<p><strong>æ€»ç»“</strong></p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§ç®€å•æœ‰æ•ˆçš„å•æ­¥è¯­éŸ³åˆæˆæ¡†æ¶ECTSpeechï¼Œé¦–æ¬¡å°†Easy Consistency Tuningï¼ˆECTï¼‰ç­–ç•¥åº”ç”¨äºè¯­éŸ³åˆæˆã€‚é€šè¿‡å¯¹é¢„è®­ç»ƒçš„æ‰©æ•£æ¨¡å‹é€æ­¥åŠ å¼ºä¸€è‡´æ€§çº¦æŸï¼ŒECTSpeechå®ç°äº†é«˜è´¨é‡çš„å•æ­¥ç”Ÿæˆï¼Œå¹¶æ˜¾è‘—é™ä½äº†è®­ç»ƒå¤æ‚åº¦ã€‚æ­¤å¤–ï¼Œè¿˜è®¾è®¡äº†ä¸€ä¸ªå¤šå°ºåº¦é—¨æ¨¡å—ï¼ˆMSGateï¼‰ä»¥å¢å¼ºå»å™ªå™¨åœ¨ä¸åŒå°ºåº¦ä¸Šèåˆç‰¹å¾çš„èƒ½åŠ›ã€‚åœ¨LJSpeechæ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼ŒECTSpeechåœ¨å•æ­¥é‡‡æ ·ä¸‹ç”Ÿæˆçš„éŸ³é¢‘è´¨é‡ä¸æœ€å…ˆè¿›çš„æ–¹æ³•ç›¸å½“ï¼ŒåŒæ—¶å¤§å¤§é™ä½äº†æ¨¡å‹çš„è®­ç»ƒæˆæœ¬å’Œå¤æ‚åº¦ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>æ‰©æ•£æ¨¡å‹åœ¨è¯­éŸ³åˆæˆä¸­è¡¨ç°å‡ºå“è¶Šçš„æ€§èƒ½ï¼Œä½†é€šå¸¸éœ€è¦å¤šæ­¥é‡‡æ ·ï¼Œå¯¼è‡´æ¨ç†æ•ˆç‡ä½ä¸‹ã€‚</li>
<li>ç°æœ‰ç ”ç©¶é€šè¿‡è’¸é¦æ‰©æ•£æ¨¡å‹åˆ°ä¸€è‡´æ€§æ¨¡å‹æ¥è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œå®ç°äº†ä¸€æ¬¡æ€§ç”Ÿæˆã€‚</li>
<li>æœ¬æ–‡é¦–æ¬¡å°†Easy Consistency Tuningï¼ˆECTï¼‰ç­–ç•¥å¼•å…¥è¯­éŸ³åˆæˆï¼Œé€šè¿‡é€æ­¥åŠ å¼ºé¢„è®­ç»ƒæ‰©æ•£æ¨¡å‹çš„ä¸€è‡´æ€§çº¦æŸï¼Œå®ç°é«˜è´¨é‡çš„å•æ­¥ç”Ÿæˆã€‚</li>
<li>è®¾è®¡äº†å¤šå°ºåº¦é—¨æ¨¡å—ï¼ˆMSGateï¼‰ä»¥å¢å¼ºå»å™ªå™¨åœ¨ä¸åŒå°ºåº¦ä¸Šèåˆç‰¹å¾çš„èƒ½åŠ›ï¼Œè¿›ä¸€æ­¥æé«˜è¯­éŸ³åˆæˆè´¨é‡ã€‚</li>
<li>å®éªŒç»“æœè¡¨æ˜ï¼ŒECTSpeechåœ¨å•æ­¥é‡‡æ ·ä¸‹ç”Ÿæˆçš„éŸ³é¢‘è´¨é‡ä¸ç°æœ‰æœ€å…ˆè¿›çš„æ–¹æ³•ç›¸å½“ã€‚</li>
<li>ECTSpeechæ˜¾è‘—é™ä½äº†æ¨¡å‹çš„è®­ç»ƒæˆæœ¬å’Œå¤æ‚åº¦ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.05984">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-1be76a4e9441b6dc9626d002faab9b13~resize:0:q75.jpg?source=1f5c5e47&expiration=1760043493&auth_key=1760043493-0-0-0666dc4dbb4a73992b2539808d5ff8f1&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-476ad2384c1d6e63744614a7cd1667ec~resize:0:q75.jpg?source=1f5c5e47&expiration=1760043500&auth_key=1760043500-0-0-23f5eadad40b85dbd7c0f554c7d48688&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-971a33e808f4113ae4707484fa8e6179~resize:0:q75.jpg?source=1f5c5e47&expiration=1760043507&auth_key=1760043507-0-0-d896618cec539c7d3c3a0f8121fb8c18&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-297b28ebeb0760d991de0b1d18e95134~resize:0:q75.jpg?source=1f5c5e47&expiration=1760043513&auth_key=1760043513-0-0-9c88d555c7edcdeaa6313c5d14bad5a2&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-5dd44fc65286ce285d5ab94cad6fd289~resize:0:q75.jpg?source=1f5c5e47&expiration=1760043520&auth_key=1760043520-0-0-9a9072721192d764af4a6e6e1b43525b&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-819a79c1f423b4cd27f10980cb00d372~resize:0:q75.jpg?source=1f5c5e47&expiration=1760043526&auth_key=1760043526-0-0-b77fcdd5af95d076047352cb613bf568&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="MSF-SER-Enriching-Acoustic-Modeling-with-Multi-Granularity-Semantics-for-Speech-Emotion-Recognition"><a href="#MSF-SER-Enriching-Acoustic-Modeling-with-Multi-Granularity-Semantics-for-Speech-Emotion-Recognition" class="headerlink" title="MSF-SER: Enriching Acoustic Modeling with Multi-Granularity Semantics   for Speech Emotion Recognition"></a>MSF-SER: Enriching Acoustic Modeling with Multi-Granularity Semantics   for Speech Emotion Recognition</h2><p><strong>Authors:Haoxun Li, Yuqing Sun, Hanlei Shi, Yu Liu, Leyuan Qu, Taihao Li</strong></p>
<p>Continuous dimensional speech emotion recognition captures affective variation along valence, arousal, and dominance, providing finer-grained representations than categorical approaches. Yet most multimodal methods rely solely on global transcripts, leading to two limitations: (1) all words are treated equally, overlooking that emphasis on different parts of a sentence can shift emotional meaning; (2) only surface lexical content is represented, lacking higher-level interpretive cues. To overcome these issues, we propose MSF-SER (Multi-granularity Semantic Fusion for Speech Emotion Recognition), which augments acoustic features with three complementary levels of textual semanticsâ€“Local Emphasized Semantics (LES), Global Semantics (GS), and Extended Semantics (ES). These are integrated via an intra-modal gated fusion and a cross-modal FiLM-modulated lightweight Mixture-of-Experts (FM-MOE). Experiments on MSP-Podcast and IEMOCAP show that MSF-SER consistently improves dimensional prediction, demonstrating the effectiveness of enriched semantic fusion for SER. </p>
<blockquote>
<p>è¿ç»­ç»´åº¦è¯­éŸ³æƒ…æ„Ÿè¯†åˆ«èƒ½å¤Ÿæ•æ‰æ²¿ä»·å€¼ã€å…´å¥‹å’Œæ”¯é…åœ°ä½çš„æƒ…æ„Ÿå˜åŒ–ï¼Œæä¾›æ¯”åˆ†ç±»æ–¹æ³•æ›´ç²¾ç»†çš„è¡¨ç¤ºã€‚ç„¶è€Œï¼Œå¤§å¤šæ•°å¤šæ¨¡å¼æ–¹æ³•ä»…ä¾èµ–äºå…¨å±€æ–‡å­—ç¨¿ï¼Œå¯¼è‡´ä¸¤ä¸ªå±€é™æ€§ï¼šï¼ˆ1ï¼‰æ‰€æœ‰å•è¯éƒ½è¢«å¹³ç­‰å¯¹å¾…ï¼Œå¿½ç•¥äº†å¥å­ä¸­ä¸åŒéƒ¨åˆ†çš„é‡ç‚¹ä¼šæ”¹å˜æƒ…æ„Ÿæ„ä¹‰ï¼›ï¼ˆ2ï¼‰åªè¡¨ç¤ºè¡¨é¢è¯æ±‡å†…å®¹ï¼Œç¼ºä¹é«˜çº§è§£é‡Šæ€§çº¿ç´¢ã€‚ä¸ºäº†å…‹æœè¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†MSF-SERï¼ˆç”¨äºè¯­éŸ³æƒ…æ„Ÿè¯†åˆ«çš„å¤šç²’åº¦è¯­ä¹‰èåˆï¼‰ï¼Œå®ƒé€šè¿‡ä¸‰ä¸ªäº’è¡¥çº§åˆ«çš„æ–‡æœ¬è¯­ä¹‰å¢å¼ºå£°å­¦ç‰¹å¾ï¼ŒåŒ…æ‹¬å±€éƒ¨å¼ºè°ƒè¯­ä¹‰ï¼ˆLESï¼‰ã€å…¨å±€è¯­ä¹‰ï¼ˆGSï¼‰å’Œæ‰©å±•è¯­ä¹‰ï¼ˆESï¼‰ã€‚è¿™äº›é€šè¿‡æ¨¡æ€å†…é—¨æ§èåˆå’Œè·¨æ¨¡æ€FiLMè°ƒåˆ¶çš„è½»é‡çº§æ··åˆä¸“å®¶ï¼ˆFM-MOEï¼‰è¿›è¡Œé›†æˆã€‚åœ¨MSP-Podcastå’ŒIEMOCAPä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒMSF-SERåœ¨ç»´åº¦é¢„æµ‹æ–¹é¢å§‹ç»ˆè¡¨ç°å‡ºæ”¹è¿›ï¼Œè¯æ˜äº†ä¸°å¯Œè¯­ä¹‰èåˆå¯¹äºSERçš„æœ‰æ•ˆæ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.05749v1">PDF</a> Under review for ICASSP 2026</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†è¿ç»­ç»´åº¦è¯­éŸ³æƒ…æ„Ÿè¯†åˆ«ï¼Œé€šè¿‡æ•æ‰æƒ…æ„Ÿå˜åŒ–çš„ç»´åº¦ï¼Œå¦‚æ•ˆä»·ã€å…´å¥‹å’Œæ”¯é…ï¼Œæä¾›æ¯”åˆ†ç±»æ–¹æ³•æ›´ç²¾ç»†çš„è¡¨å¾ã€‚ç„¶è€Œï¼Œå¤§å¤šæ•°å¤šæ¨¡å¼æ–¹æ³•ä»…ä¾èµ–å…¨å±€æ–‡æœ¬ï¼Œå­˜åœ¨ä¸¤ä¸ªå±€é™æ€§ï¼šä¸€æ˜¯æ‰€æœ‰è¯è¯­å¹³ç­‰å¯¹å¾…ï¼Œå¿½ç•¥äº†å¥å­ä¸­ä¸åŒéƒ¨åˆ†çš„å¼ºè°ƒä¼šæ”¹å˜æƒ…æ„Ÿæ„ä¹‰ï¼›äºŒæ˜¯ä»…ä»£è¡¨è¡¨é¢è¯æ±‡å†…å®¹ï¼Œç¼ºä¹é«˜çº§è§£é‡Šæ€§çº¿ç´¢ã€‚ä¸ºå…‹æœè¿™äº›é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†MSF-SERï¼ˆç”¨äºè¯­éŸ³æƒ…æ„Ÿè¯†åˆ«çš„å¤šç²’åº¦è¯­ä¹‰èåˆï¼‰ï¼Œå®ƒé€šè¿‡ä¸‰ç§äº’è¡¥çš„æ–‡æœ¬è¯­ä¹‰çº§åˆ«â€”â€”å±€éƒ¨å¼ºè°ƒè¯­ä¹‰ï¼ˆLESï¼‰ã€å…¨å±€è¯­ä¹‰ï¼ˆGSï¼‰å’Œæ‰©å±•è¯­ä¹‰ï¼ˆESï¼‰æ¥å¢å¼ºå£°å­¦ç‰¹å¾ã€‚è¿™äº›é€šè¿‡æ¨¡æ€å†…é—¨æ§èåˆå’Œè·¨æ¨¡æ€FiLMè°ƒåˆ¶çš„è½»é‡çº§æ··åˆä¸“å®¶ï¼ˆFM-MOEï¼‰è¿›è¡Œé›†æˆã€‚åœ¨MSP-Podcastå’ŒIEMOCAPä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒMSF-SERåœ¨ç»´åº¦é¢„æµ‹ä¸Šè¡¨ç°ä¸€è‡´ï¼Œè¯æ˜äº†ä¸°å¯Œè¯­ä¹‰èåˆåœ¨SERä¸­çš„æœ‰æ•ˆæ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è¯­éŸ³æƒ…æ„Ÿè¯†åˆ«ä¸ä»…ä»…æ˜¯ç®€å•çš„åˆ†ç±»é—®é¢˜ï¼Œè¿˜è¦æ¶‰åŠå¯¹æ•ˆä»·ã€å…´å¥‹å’Œæ”¯é…ç­‰ç»´åº¦çš„æ•æ‰ã€‚</li>
<li>å½“å‰çš„å¤šæ¨¡å¼æ–¹æ³•ä¸»è¦ä¾èµ–å…¨å±€æ–‡æœ¬ï¼Œå­˜åœ¨å¿½è§†å¥å­ä¸­ä¸åŒéƒ¨åˆ†å¼ºè°ƒå’Œæƒ…æ„Ÿæ„ä¹‰çš„é—®é¢˜ã€‚</li>
<li>æå‡ºäº†ä¸€ç§æ–°çš„æ–¹æ³•MSF-SERï¼Œé€šè¿‡ä¸‰ç§äº’è¡¥çš„æ–‡æœ¬è¯­ä¹‰çº§åˆ«å¢å¼ºå£°å­¦ç‰¹å¾è¯†åˆ«è¯­éŸ³æƒ…æ„Ÿã€‚</li>
<li>MSF-SERé€šè¿‡æ¨¡æ€å†…é—¨æ§èåˆå’Œè·¨æ¨¡æ€é›†æˆæ–¹æ³•å°†ä¸åŒçº§åˆ«çš„è¯­ä¹‰ä¿¡æ¯ç»“åˆã€‚</li>
<li>å®éªŒè¯æ˜ï¼ŒMSF-SERåœ¨ç»´åº¦é¢„æµ‹ä¸Šè¡¨ç°ä¼˜å¼‚ï¼ŒéªŒè¯äº†å…¶åœ¨è¯­éŸ³æƒ…æ„Ÿè¯†åˆ«ä¸­çš„æœ‰æ•ˆæ€§ã€‚</li>
<li>å¼•å…¥çš„ä¸‰ç§æ–‡æœ¬è¯­ä¹‰çº§åˆ«åŒ…æ‹¬å±€éƒ¨å¼ºè°ƒè¯­ä¹‰ï¼ˆLESï¼‰ã€å…¨å±€è¯­ä¹‰ï¼ˆGSï¼‰å’Œæ‰©å±•è¯­ä¹‰ï¼ˆESï¼‰ï¼Œæœ‰åŠ©äºæ›´å‡†ç¡®åœ°æ•æ‰æƒ…æ„Ÿè¡¨è¾¾ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.05749">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-58e9bc9dd9d126c2d7bba9564e3c758f~resize:0:q75.jpg?source=1f5c5e47&expiration=1760043534&auth_key=1760043534-0-0-8ad9d370aa4a6c2668780c3cf0ed6c32&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-06c9c2fa9319f378bc4092bb03c9b615~resize:0:q75.jpg?source=1f5c5e47&expiration=1760043541&auth_key=1760043541-0-0-bfa39b0f3d902e32779682ac524379c8&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-ebade4d43208e5a6fd8c626b9bdfec1b~resize:0:q75.jpg?source=1f5c5e47&expiration=1760043548&auth_key=1760043548-0-0-a152222f03bc755329b0696e98efb706&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-9c33806b6880626d8173bd698e97694c~resize:0:q75.jpg?source=1f5c5e47&expiration=1760043554&auth_key=1760043554-0-0-84f49f4a680912270c4eff54b3489472&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-d386601ca20d041bb7f1024e69328997~resize:0:q75.jpg?source=1f5c5e47&expiration=1760043561&auth_key=1760043561-0-0-23f847e7c131efe07cb6c665321fbae7&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-0e8a0a3cc3c4c6c73e08e263995b45b4~resize:0:q75.jpg?source=1f5c5e47&expiration=1760043568&auth_key=1760043568-0-0-e7a6f3823cd4356d315c34464423bc77&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="WaveSP-Net-Learnable-Wavelet-Domain-Sparse-Prompt-Tuning-for-Speech-Deepfake-Detection"><a href="#WaveSP-Net-Learnable-Wavelet-Domain-Sparse-Prompt-Tuning-for-Speech-Deepfake-Detection" class="headerlink" title="WaveSP-Net: Learnable Wavelet-Domain Sparse Prompt Tuning for Speech   Deepfake Detection"></a>WaveSP-Net: Learnable Wavelet-Domain Sparse Prompt Tuning for Speech   Deepfake Detection</h2><p><strong>Authors:Xi Xuan, Xuechen Liu, Wenxin Zhang, Yi-Cheng Lin, Xiaojian Lin, Tomi Kinnunen</strong></p>
<p>Modern front-end design for speech deepfake detection relies on full fine-tuning of large pre-trained models like XLSR. However, this approach is not parameter-efficient and may lead to suboptimal generalization to realistic, in-the-wild data types. To address these limitations, we introduce a new family of parameter-efficient front-ends that fuse prompt-tuning with classical signal processing transforms. These include FourierPT-XLSR, which uses the Fourier Transform, and two variants based on the Wavelet Transform: WSPT-XLSR and Partial-WSPT-XLSR. We further propose WaveSP-Net, a novel architecture combining a Partial-WSPT-XLSR front-end and a bidirectional Mamba-based back-end. This design injects multi-resolution features into the prompt embeddings, which enhances the localization of subtle synthetic artifacts without altering the frozen XLSR parameters. Experimental results demonstrate that WaveSP-Net outperforms several state-of-the-art models on two new and challenging benchmarks, Deepfake-Eval-2024 and SpoofCeleb, with low trainable parameters and notable performance gains. The code and models are available at <a target="_blank" rel="noopener" href="https://github.com/xxuan-acoustics/WaveSP-Net">https://github.com/xxuan-acoustics/WaveSP-Net</a>. </p>
<blockquote>
<p>ç°ä»£è¯­éŸ³æ·±åº¦ä¼ªé€ æ£€æµ‹å‰ç«¯è®¾è®¡ä¾èµ–äºå¤§å‹é¢„è®­ç»ƒæ¨¡å‹ï¼ˆå¦‚XLSRï¼‰çš„å®Œå…¨å¾®è°ƒã€‚ç„¶è€Œï¼Œè¿™ç§æ–¹æ³•åœ¨å‚æ•°æ•ˆç‡æ–¹é¢å¹¶ä¸ç†æƒ³ï¼Œå¹¶ä¸”å¯èƒ½å¯¼è‡´å¯¹ç°å®ã€é‡ç”Ÿæ•°æ®ç±»å‹çš„ä¸€èˆ¬åŒ–è¡¨ç°ä¸ä½³ã€‚ä¸ºäº†è§£å†³è¿™äº›å±€é™æ€§ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç³»åˆ—å‚æ•°é«˜æ•ˆå‰ç«¯ï¼Œå°†æç¤ºå¾®è°ƒä¸ç»å…¸ä¿¡å·å¤„ç†å˜æ¢ç›¸ç»“åˆã€‚è¿™äº›åŒ…æ‹¬ä½¿ç”¨å‚…ç«‹å¶å˜æ¢çš„FourierPT-XLSRï¼Œä»¥åŠåŸºäºå°æ³¢å˜æ¢çš„ä¸¤ä¸ªå˜ä½“ï¼šWSPT-XLSRå’Œéƒ¨åˆ†WSPT-XLSRã€‚æˆ‘ä»¬è¿›ä¸€æ­¥æå‡ºäº†WaveSP-Netï¼Œè¿™æ˜¯ä¸€ç§æ–°å‹æ¶æ„ï¼Œç»“åˆäº†Partial-WSPT-XLSRå‰ç«¯å’ŒåŸºäºåŒå‘Mambaçš„åç«¯ã€‚è¿™ç§è®¾è®¡å°†å¤šåˆ†è¾¨ç‡ç‰¹å¾æ³¨å…¥æç¤ºåµŒå…¥ä¸­ï¼Œæé«˜äº†ç»†å¾®åˆæˆä¼ªå½±çš„å®šä½èƒ½åŠ›ï¼ŒåŒæ—¶ä¸æ”¹å˜å†»ç»“çš„XLSRå‚æ•°ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒWaveSP-Netåœ¨ä¸¤ä¸ªæ–°çš„å…·æœ‰æŒ‘æˆ˜æ€§çš„åŸºå‡†æµ‹è¯•ï¼ˆDeepfake-Eval-2024å’ŒSpoofCelebï¼‰ä¸Šçš„è¡¨ç°ä¼˜äºè®¸å¤šæœ€æ–°æ¨¡å‹ï¼Œå…·æœ‰è¾ƒå°‘çš„å¯è®­ç»ƒå‚æ•°å’Œæ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚ç›¸å…³ä»£ç å’Œæ¨¡å‹å¯åœ¨xxuanå£°å­¦WaveSP-Netç½‘ç«™ä¸Šæ‰¾åˆ°ï¼ˆ<a target="_blank" rel="noopener" href="https://github.com/xxuan-acoustics/WaveSP-Net%EF%BC%89%E3%80%82">https://github.com/xxuan-acoustics/WaveSP-Netï¼‰ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.05305v1">PDF</a> Submitted to ICASSP 2026</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ‘˜è¦é’ˆå¯¹è¯­éŸ³æ·±åº¦ä¼ªé€ æ£€æµ‹çš„å‰ç«¯è®¾è®¡æå‡ºäº†æ”¹è¿›æ–¹æ¡ˆã€‚ä¸ºäº†è§£å†³ç°æœ‰åŸºäºå¤§å‹é¢„è®­ç»ƒæ¨¡å‹å¦‚XLSRçš„å…¨é‡å¾®è°ƒæ–¹æ³•å­˜åœ¨çš„å‚æ•°æ•ˆç‡ä½ä¸‹å’Œå¯¹çœŸå®åœºæ™¯æ•°æ®æ³›åŒ–æ€§èƒ½ä¸ä½³çš„é—®é¢˜ï¼Œç ”ç©¶å›¢é˜Ÿå¼•å…¥äº†å‚æ•°é«˜æ•ˆçš„å‰ç«¯è®¾è®¡ï¼Œèåˆäº†æç¤ºè°ƒè°å’Œç»å…¸ä¿¡å·å¤„ç†å˜æ¢æŠ€æœ¯ã€‚åŒæ—¶ï¼Œä»–ä»¬æå‡ºäº†ç»“åˆå°æ³¢å˜æ¢çš„ä¸‰ç§æ–°å‹å‰ç«¯æŠ€æœ¯ï¼ŒåŒ…æ‹¬WaveSP-Netç½‘ç»œç»“æ„ã€‚WaveSP-Neté€šè¿‡åœ¨æç¤ºåµŒå…¥ä¸­æ³¨å…¥å¤šåˆ†è¾¨ç‡ç‰¹å¾ï¼Œæå‡äº†æ£€æµ‹å¾®å¼±åˆæˆä¼ªé€ çš„æœ¬åœ°åŒ–èƒ½åŠ›ï¼Œä¸”ä¸ä¼šæ”¹å˜XLSRçš„å‚æ•°ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒWaveSP-Netåœ¨ä¸¤ä¸ªæ–°çš„å…·æœ‰æŒ‘æˆ˜æ€§çš„åŸºå‡†æµ‹è¯•ä¸Šè¶…è¶Šäº†æœ€æ–°çš„æŠ€æœ¯æ°´å‡†ï¼Œæ€§èƒ½ä¼˜è¶Šä¸”å…·æœ‰è¾ƒä½çš„å‚æ•°å¯è®­ç»ƒæ€§ã€‚å…·ä½“æ¨¡å‹å’Œä»£ç å¯é€šè¿‡xxuanå£°å­¦ç ”ç©¶æ‰€çš„å…¬å¼€ä»“åº“è·å–ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç°ä»£å‰ç«¯è®¾è®¡åœ¨è¯­éŸ³æ·±åº¦ä¼ªé€ æ£€æµ‹ä¸­ä¾èµ–å¤§å‹é¢„è®­ç»ƒæ¨¡å‹çš„å…¨é¢å¾®è°ƒï¼Œä½†è¿™ç§æ–¹æ³•å‚æ•°æ•ˆç‡ä½ä¸‹ä¸”å¯èƒ½å¯¹çœŸå®ä¸–ç•Œæ•°æ®çš„æ³›åŒ–æ€§èƒ½ä¸ä½³ã€‚</li>
<li>é’ˆå¯¹ä¸Šè¿°é—®é¢˜ï¼Œå¼•å…¥äº†ä¸€ç§æ–°å‹å‚æ•°é«˜æ•ˆå‰ç«¯è®¾è®¡ï¼Œèåˆäº†æç¤ºè°ƒè°å’Œç»å…¸ä¿¡å·å¤„ç†å˜æ¢æŠ€æœ¯ã€‚</li>
<li>æå‡ºäº†ä¸‰ç§åŸºäºå°æ³¢å˜æ¢çš„æ–°å‹å‰ç«¯æŠ€æœ¯ï¼ŒåŒ…æ‹¬WaveSP-Netç½‘ç»œç»“æ„ã€‚è¯¥ç»“æ„é€šè¿‡å°†å¤šåˆ†è¾¨ç‡ç‰¹å¾æ³¨å…¥æç¤ºåµŒå…¥ä¸­ï¼Œæå‡äº†å®šä½å¾®å¦™åˆæˆä¼ªé€ çš„å‡†ç¡®æ€§ã€‚</li>
<li>WaveSP-Netåœ¨åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºå“è¶Šæ€§èƒ½ï¼Œè¶…è¶Šäº†ç°æœ‰æŠ€æœ¯æ°´å‡†ï¼Œä¸”å…·å¤‡è¾ƒä½çš„å‚æ•°å¯è®­ç»ƒæ€§ã€‚</li>
<li>è¯¥ç ”ç©¶æä¾›çš„æ¨¡å‹å’Œä»£ç å¯é€šè¿‡å…¬å¼€ä»“åº“è·å–ï¼Œä¾¿äºå…¶ä»–ç ”ç©¶è€…ä½¿ç”¨å’Œè¿›ä¸€æ­¥å¼€å‘ã€‚</li>
<li>é€šè¿‡ç»“åˆä¿¡å·å¤„ç†æŠ€æœ¯ä¸æ·±åº¦å­¦ä¹ æ¨¡å‹ï¼Œè¯¥ç ”ç©¶ä¸ºè¯­éŸ³æ·±åº¦ä¼ªé€ æ£€æµ‹é¢†åŸŸæä¾›äº†æ–°çš„æ–¹å‘ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.05305">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-65fe0b353d5c5ca1acc380522a44a558~resize:0:q75.jpg?source=1f5c5e47&expiration=1760043575&auth_key=1760043575-0-0-a207238113c587a04ef308aa4ab4b081&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-6ade1af72f3776b9ca860d43578439a9~resize:0:q75.jpg?source=1f5c5e47&expiration=1760043583&auth_key=1760043583-0-0-6ffad740ea25bb4f9f7647eeff61d7ca&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-4941265dac737ba27e75f33086bdd192~resize:0:q75.jpg?source=1f5c5e47&expiration=1760043589&auth_key=1760043589-0-0-790797aab8a427a09316c948b97f05fb&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-9532f5e3ca9532e119e2725c85ecdce9~resize:0:q75.jpg?source=1f5c5e47&expiration=1760043596&auth_key=1760043596-0-0-f3213f41a6495d97a4d73d3e63ba5b3d&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="AUREXA-SE-Audio-Visual-Unified-Representation-Exchange-Architecture-with-Cross-Attention-and-Squeezeformer-for-Speech-Enhancement"><a href="#AUREXA-SE-Audio-Visual-Unified-Representation-Exchange-Architecture-with-Cross-Attention-and-Squeezeformer-for-Speech-Enhancement" class="headerlink" title="AUREXA-SE: Audio-Visual Unified Representation Exchange Architecture   with Cross-Attention and Squeezeformer for Speech Enhancement"></a>AUREXA-SE: Audio-Visual Unified Representation Exchange Architecture   with Cross-Attention and Squeezeformer for Speech Enhancement</h2><p><strong>Authors:M. Sajid, Deepanshu Gupta, Yash Modi, Sanskriti Jain, Harshith Jai Surya Ganji, A. Rahaman, Harshvardhan Choudhary, Nasir Saleem, Amir Hussain, M. Tanveer</strong></p>
<p>In this paper, we propose AUREXA-SE (Audio-Visual Unified Representation Exchange Architecture with Cross-Attention and Squeezeformer for Speech Enhancement), a progressive bimodal framework tailored for audio-visual speech enhancement (AVSE). AUREXA-SE jointly leverages raw audio waveforms and visual cues by employing a U-Net-based 1D convolutional encoder for audio and a Swin Transformer V2 for efficient and expressive visual feature extraction. Central to the architecture is a novel bidirectional cross-attention mechanism, which facilitates deep contextual fusion between modalities, enabling rich and complementary representation learning. To capture temporal dependencies within the fused embeddings, a stack of lightweight Squeezeformer blocks combining convolutional and attention modules is introduced. The enhanced embeddings are then decoded via a U-Net-style decoder for direct waveform reconstruction, ensuring perceptually consistent and intelligible speech output. Experimental evaluations demonstrate the effectiveness of AUREXA-SE, achieving significant performance improvements over noisy baselines, with STOI of 0.516, PESQ of 1.323, and SI-SDR of -4.322 dB. The source code of AUREXA-SE is available at <a target="_blank" rel="noopener" href="https://github.com/mtanveer1/AVSEC-4-Challenge-2025">https://github.com/mtanveer1/AVSEC-4-Challenge-2025</a>. </p>
<blockquote>
<p>æœ¬æ–‡æå‡ºäº†AUREXA-SEï¼ˆä¸€ç§ç”¨äºè¯­éŸ³å¢å¼ºçš„è§†å¬ç»Ÿä¸€è¡¨ç¤ºäº¤æ¢æ¶æ„ï¼Œå…·æœ‰è·¨æ³¨æ„åŠ›å’ŒSqueezeformeråŠŸèƒ½ï¼‰ï¼Œè¿™æ˜¯ä¸€ä¸ªä¸ºéŸ³é¢‘è§†è§‰è¯­éŸ³å¢å¼ºï¼ˆAVSEï¼‰é‡èº«å®šåˆ¶çš„å…ˆè¿›åŒæ¨¡æ€æ¡†æ¶ã€‚AUREXA-SEé€šè¿‡é‡‡ç”¨åŸºäºU-Netçš„1Då·ç§¯ç¼–ç å™¨è¿›è¡ŒéŸ³é¢‘å¤„ç†å’ŒSwin Transformer V2è¿›è¡Œé«˜æ•ˆä¸”å¯Œæœ‰è¡¨ç°åŠ›çš„è§†è§‰ç‰¹å¾æå–ï¼Œè”åˆåˆ©ç”¨åŸå§‹éŸ³é¢‘æ³¢å½¢å’Œè§†è§‰çº¿ç´¢ã€‚è¯¥æ¶æ„çš„æ ¸å¿ƒæ˜¯ä¸€ç§æ–°å‹åŒå‘è·¨æ³¨æ„æœºåˆ¶ï¼Œæœ‰åŠ©äºä¿ƒè¿›ä¸åŒæ¨¡æ€ä¹‹é—´çš„æ·±åº¦ä¸Šä¸‹æ–‡èåˆï¼Œä»è€Œå®ç°ä¸°å¯Œä¸”äº’è¡¥çš„è¡¨ç¤ºå­¦ä¹ ã€‚ä¸ºäº†æ•è·èåˆåµŒå…¥ä¸­çš„æ—¶é—´ä¾èµ–æ€§ï¼Œå¼•å…¥äº†ä¸€ç³»åˆ—ç»“åˆäº†å·ç§¯å’Œæ³¨æ„åŠ›æ¨¡å—çš„è½»é‡åŒ–Squeezeformerå—ã€‚ç„¶åï¼Œé€šè¿‡U-Neté£æ ¼çš„è§£ç å™¨å¯¹å¢å¼ºçš„åµŒå…¥è¿›è¡Œè§£ç ï¼Œä»¥å®ç°ç›´æ¥æ³¢å½¢é‡å»ºï¼Œç¡®ä¿æ„ŸçŸ¥ä¸Šä¸€è‡´ä¸”å¯ç†è§£çš„è¯­éŸ³è¾“å‡ºã€‚å®éªŒè¯„ä¼°è¡¨æ˜ï¼ŒAUREXA-SEçš„æœ‰æ•ˆæ€§åœ¨å™ªå£°åŸºå‡†æµ‹è¯•ä¸Šå–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½æ”¹è¿›ï¼Œå®ç°äº†STOIä¸º0.516ã€PESQä¸º1.323å’ŒSI-SDRä¸º-4.322 dBçš„ç»“æœã€‚AUREXA-SEçš„æºä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/mtanveer1/AVSEC-4-Challenge-2025%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/mtanveer1/AVSEC-4-Challenge-2025æ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.05295v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>AUREXA-SEæ˜¯ä¸€ä¸ªä¸ºè§†å¬è¯­éŸ³å¢å¼ºï¼ˆAVSEï¼‰è®¾è®¡çš„å…ˆè¿›åŒæ¨¡æ€æ¡†æ¶ã€‚å®ƒåˆ©ç”¨åŸå§‹éŸ³é¢‘æ³¢å½¢å’Œè§†è§‰çº¿ç´¢ï¼Œé€šè¿‡U-NetåŸºäº1Då·ç§¯ç¼–ç å™¨ä¸Swin Transformer V2é«˜æ•ˆè§†è§‰ç‰¹å¾æå–æŠ€æœ¯ï¼Œå®ç°è·¨æ¨¡æ€æ·±åº¦ä¸Šä¸‹æ–‡èåˆã€‚æ¡†æ¶å¼•å…¥äº†è½»é‡çº§çš„Squeezeformerå—æ•æ‰èåˆåµŒå…¥ä¸­çš„æ—¶é—´ä¾èµ–æ€§ï¼Œå¹¶é€šè¿‡U-Neté£æ ¼çš„è§£ç å™¨è§£ç å¢å¼ºåµŒå…¥ï¼Œå®ç°ç›´æ¥æ³¢å½¢é‡å»ºã€‚å®éªŒè¯„ä¼°æ˜¾ç¤ºï¼ŒAUREXA-SEåœ¨å™ªå£°åŸºå‡†çº¿ä¸Šå–å¾—äº†æ˜¾è‘—æ€§èƒ½æå‡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>AUREXA-SEæ˜¯ä¸€ä¸ªé’ˆå¯¹éŸ³é¢‘-è§†è§‰è¯­éŸ³å¢å¼ºï¼ˆAVSEï¼‰è®¾è®¡çš„åŒæ¨¡æ€æ¡†æ¶ã€‚</li>
<li>è¯¥æ¡†æ¶ç»“åˆåŸå§‹éŸ³é¢‘æ³¢å½¢å’Œè§†è§‰çº¿ç´¢ï¼Œä½¿ç”¨U-Netå’ŒSwin Transformer V2æŠ€æœ¯ã€‚</li>
<li>æ¡†æ¶ä¸­çš„æ ¸å¿ƒæ˜¯ä¸€ä¸ªåŒå‘è·¨æ³¨æ„æœºåˆ¶ï¼Œå®ç°äº†æ¨¡æ€é—´çš„æ·±åº¦ä¸Šä¸‹æ–‡èåˆã€‚</li>
<li>å¼•å…¥Squeezeformerå—æ•æ‰èåˆåµŒå…¥ä¸­çš„æ—¶é—´ä¾èµ–æ€§ã€‚</li>
<li>é€šè¿‡U-Neté£æ ¼çš„è§£ç å™¨è¿›è¡Œå¢å¼ºåµŒå…¥çš„è§£ç ï¼Œå®ç°ç›´æ¥æ³¢å½¢é‡å»ºã€‚</li>
<li>å®éªŒè¯„ä¼°æ˜¾ç¤ºAUREXA-SEåœ¨å™ªå£°ç¯å¢ƒä¸‹çš„æ€§èƒ½æ˜¾è‘—æå‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.05295">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-6f7a44a0505cfe24612189dd25e07d9f~resize:0:q75.jpg?source=1f5c5e47&expiration=1760043604&auth_key=1760043604-0-0-119c4fd9732c750f19b7dd8a0f262cbe&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-9ba0e2646084e43f6d2ed7928cc7f79f~resize:0:q75.jpg?source=1f5c5e47&expiration=1760043611&auth_key=1760043611-0-0-439e57c74eb26fafadb4c82aeacb005b&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-895b48520877107eaac72ab5caa6c75d~resize:0:q75.jpg?source=1f5c5e47&expiration=1760043617&auth_key=1760043617-0-0-400676bd208b1a9ee5771a48c6a16803&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-efc16d56f5546b2e73034d8441d35259~resize:0:q75.jpg?source=1f5c5e47&expiration=1760043623&auth_key=1760043623-0-0-ebf2509d004e7cd4306325798146f76c&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-6e43c38af263ccf86f7933472a377ac9~resize:0:q75.jpg?source=1f5c5e47&expiration=1760043630&auth_key=1760043630-0-0-26b2fb08f992d18edc19b001e53bf72d&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-1d7aa6e4513be0653e132d4f9626e2db~resize:0:q75.jpg?source=1f5c5e47&expiration=1760043636&auth_key=1760043636-0-0-c9e887db6f96277c6fba2bc751280188&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-77a18bc140131e4e94e05c45b552fd85~resize:0:q75.jpg?source=1f5c5e47&expiration=1760043643&auth_key=1760043643-0-0-4e0aab7a67885c660ea0393c37304152&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="Machine-Unlearning-in-Speech-Emotion-Recognition-via-Forget-Set-Alone"><a href="#Machine-Unlearning-in-Speech-Emotion-Recognition-via-Forget-Set-Alone" class="headerlink" title="Machine Unlearning in Speech Emotion Recognition via Forget Set Alone"></a>Machine Unlearning in Speech Emotion Recognition via Forget Set Alone</h2><p><strong>Authors:Zhao Ren, Rathi Adarshi Rammohan, Kevin Scheck, Tanja Schultz</strong></p>
<p>Speech emotion recognition aims to identify emotional states from speech signals and has been widely applied in human-computer interaction, education, healthcare, and many other fields. However, since speech data contain rich sensitive information, partial data can be required to be deleted by speakers due to privacy concerns. Current machine unlearning approaches largely depend on data beyond the samples to be forgotten. However, this reliance poses challenges when data redistribution is restricted and demands substantial computational resources in the context of big data. We propose a novel adversarial-attack-based approach that fine-tunes a pre-trained speech emotion recognition model using only the data to be forgotten. The experimental results demonstrate that the proposed approach can effectively remove the knowledge of the data to be forgotten from the model, while preserving high model performance on the test set for emotion recognition. </p>
<blockquote>
<p>è¯­éŸ³æƒ…æ„Ÿè¯†åˆ«æ—¨åœ¨ä»è¯­éŸ³ä¿¡å·ä¸­è¯†åˆ«æƒ…æ„ŸçŠ¶æ€ï¼Œå·²å¹¿æ³›åº”ç”¨äºäººæœºäº¤äº’ã€æ•™è‚²ã€åŒ»ç–—å’Œè®¸å¤šå…¶ä»–é¢†åŸŸã€‚ç„¶è€Œï¼Œç”±äºè¯­éŸ³æ•°æ®åŒ…å«ä¸°å¯Œçš„æ•æ„Ÿä¿¡æ¯ï¼Œå‡ºäºéšç§è€ƒè™‘ï¼Œéƒ¨åˆ†æ•°æ®å¯èƒ½éœ€è¦è¯´è¯è€…åˆ é™¤ã€‚å½“å‰æœºå™¨åˆ é™¤æ—§æ•°æ®çš„æ–¹æ³•åœ¨å¾ˆå¤§ç¨‹åº¦ä¸Šä¾èµ–äºè¦å¿˜è®°çš„æ ·æœ¬ä¹‹å¤–çš„æ•°æ®ã€‚ç„¶è€Œï¼Œè¿™ç§ä¾èµ–åœ¨æ•°æ®é‡æ–°åˆ†é…å—é™å’Œå¤§æ•°æ®èƒŒæ™¯ä¸‹éœ€è¦å¤§é‡è®¡ç®—èµ„æºæ—¶æ„æˆäº†æŒ‘æˆ˜ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºå¯¹æŠ—æ”»å‡»çš„æ–°æ–¹æ³•ï¼Œè¯¥æ–¹æ³•ä»…ä½¿ç”¨è¦å¿˜è®°çš„æ•°æ®å¯¹é¢„è®­ç»ƒçš„è¯­éŸ³æƒ…æ„Ÿè¯†åˆ«æ¨¡å‹è¿›è¡Œå¾®è°ƒã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•å¯ä»¥æœ‰æ•ˆåœ°ä»æ¨¡å‹ä¸­åˆ é™¤è¦å¿˜è®°çš„æ•°æ®çš„çŸ¥è¯†ï¼ŒåŒæ—¶ä¿æŒæ¨¡å‹åœ¨æµ‹è¯•é›†ä¸Šçš„æƒ…æ„Ÿè¯†åˆ«æ€§èƒ½ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.04251v1">PDF</a> Submitted to ICASSP 2026</p>
<p><strong>æ€»ç»“</strong></p>
<p>è¯­éŸ³æƒ…æ„Ÿè¯†åˆ«æ—¨åœ¨ä»è¯­éŸ³ä¿¡å·ä¸­è¯†åˆ«æƒ…æ„ŸçŠ¶æ€ï¼Œå·²å¹¿æ³›åº”ç”¨äºäººæœºäº¤äº’ã€æ•™è‚²ã€åŒ»ç–—ç­‰å¤šä¸ªé¢†åŸŸã€‚ç„¶è€Œï¼Œç”±äºè¯­éŸ³æ•°æ®åŒ…å«ä¸°å¯Œçš„æ•æ„Ÿä¿¡æ¯ï¼Œå‡ºäºéšç§è€ƒè™‘ï¼Œéƒ¨åˆ†æ•°æ®å¯èƒ½è¦æ±‚è¢«åˆ é™¤ã€‚å½“å‰æœºå™¨æ“¦é™¤æ–¹æ³•å¾ˆå¤§ç¨‹åº¦ä¸Šä¾èµ–äºè¶…å‡ºè¦å¿˜è®°æ ·æœ¬çš„æ•°æ®ã€‚ç„¶è€Œï¼Œè¿™ç§ä¾èµ–åœ¨æ•°æ®é‡æ–°åˆ†é…å—é™å’Œå¤§æ•°æ®èƒŒæ™¯ä¸‹éœ€è¦å¤§é‡è®¡ç®—èµ„æºæ—¶å¸¦æ¥äº†æŒ‘æˆ˜ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºå¯¹æŠ—æ”»å‡»çš„æ–¹æ³•ï¼Œé€šè¿‡å¾®è°ƒé¢„è®­ç»ƒçš„è¯­éŸ³æƒ…æ„Ÿè¯†åˆ«æ¨¡å‹ï¼Œåªä½¿ç”¨è¦å¿˜è®°çš„æ•°æ®ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•å¯ä»¥æœ‰æ•ˆåœ°ä»æ¨¡å‹ä¸­ç§»é™¤è¦å¿˜è®°çš„æ•°æ®çš„çŸ¥è¯†ï¼ŒåŒæ—¶ä¿æŒæ¨¡å‹åœ¨æµ‹è¯•é›†ä¸Šçš„æƒ…æ„Ÿè¯†åˆ«æ€§èƒ½ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>è¯­éŸ³æƒ…æ„Ÿè¯†åˆ«æ˜¯é€šè¿‡è¯†åˆ«è¯­éŸ³ä¿¡å·ä¸­çš„æƒ…æ„ŸçŠ¶æ€æ¥å®ç°ï¼Œå¹¿æ³›åº”ç”¨äºå¤šä¸ªé¢†åŸŸã€‚</li>
<li>ç”±äºéšç§è€ƒè™‘ï¼Œéƒ¨åˆ†è¯­éŸ³æ•°æ®å¯èƒ½éœ€è¦è¢«åˆ é™¤ã€‚</li>
<li>å½“å‰æœºå™¨æ“¦é™¤æ–¹æ³•é€šå¸¸ä¾èµ–äºè¶…å‡ºè¦åˆ é™¤æ ·æœ¬çš„æ•°æ®ã€‚</li>
<li>æ•°æ®é‡æ–°åˆ†é…å—é™å’Œå¤§æ•°æ®èƒŒæ™¯ä¸‹ï¼Œå½“å‰æ–¹æ³•é¢ä¸´æŒ‘æˆ˜ã€‚</li>
<li>æå‡ºäº†ä¸€ç§åŸºäºå¯¹æŠ—æ”»å‡»çš„æ–°å‹æœºå™¨æ“¦é™¤æ–¹æ³•ï¼Œåªä½¿ç”¨éœ€è¦è¢«å¿˜è®°çš„æ•°æ®å¯¹é¢„è®­ç»ƒæ¨¡å‹è¿›è¡Œå¾®è°ƒã€‚</li>
<li>å®éªŒè¯æ˜è¯¥æ–¹æ³•èƒ½æœ‰æ•ˆç§»é™¤æ¨¡å‹ä¸­éœ€è¦å¿˜è®°çš„æ•°æ®çŸ¥è¯†ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.04251">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-0341ae5eec1ff2c55fcc35afb7d1d83c~resize:0:q75.jpg?source=1f5c5e47&expiration=1760043651&auth_key=1760043651-0-0-a10e90de3171ace61eb4406b1b2fe48b&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-91bd30f7419610de2a2a3c25c9491028~resize:0:q75.jpg?source=1f5c5e47&expiration=1760043658&auth_key=1760043658-0-0-1fc31f41c9056821df9616b70e7479c0&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-ecdb8a46b923274e2517a43ea89adcc0~resize:0:q75.jpg?source=1f5c5e47&expiration=1760043664&auth_key=1760043664-0-0-86721c666e04a111b6f194fde17e9bef&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-8c731da8d0ed8fa4df368e4e0df59b18~resize:0:q75.jpg?source=1f5c5e47&expiration=1760043671&auth_key=1760043671-0-0-b76dc4a5d4d01f4376dfb1c4ac18d969&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="Probing-Whisper-for-Dysarthric-Speech-in-Detection-and-Assessment"><a href="#Probing-Whisper-for-Dysarthric-Speech-in-Detection-and-Assessment" class="headerlink" title="Probing Whisper for Dysarthric Speech in Detection and Assessment"></a>Probing Whisper for Dysarthric Speech in Detection and Assessment</h2><p><strong>Authors:Zhengjun Yue, Devendra Kayande, Zoran Cvetkovic, Erfan Loweimi</strong></p>
<p>Large-scale end-to-end models such as Whisper have shown strong performance on diverse speech tasks, but their internal behavior on pathological speech remains poorly understood. Understanding how dysarthric speech is represented across layers is critical for building reliable and explainable clinical assessment tools. This study probes the Whisper-Medium model encoder for dysarthric speech for detection and assessment (i.e., severity classification). We evaluate layer-wise embeddings with a linear classifier under both single-task and multi-task settings, and complement these results with Silhouette scores and mutual information to provide perspectives on layer informativeness. To examine adaptability, we repeat the analysis after fine-tuning Whisper on a dysarthric speech recognition task. Across metrics, the mid-level encoder layers (13-15) emerge as most informative, while fine-tuning induces only modest changes. The findings improve the interpretability of Whisperâ€™s embeddings and highlight the potential of probing analyses to guide the use of large-scale pretrained models for pathological speech. </p>
<blockquote>
<p>å¤§è§„æ¨¡ç«¯åˆ°ç«¯æ¨¡å‹ï¼Œå¦‚Whisperï¼Œåœ¨å¤šç§è¯­éŸ³ä»»åŠ¡ä¸­è¡¨ç°å‡ºå¼ºå¤§çš„æ€§èƒ½ï¼Œä½†å®ƒä»¬åœ¨ç—…ç†æ€§è¯­éŸ³ä¸Šçš„å†…éƒ¨è¡Œä¸ºä»çŸ¥ä¹‹ç”šå°‘ã€‚äº†è§£å¦‚ä½•è·¨å±‚è¡¨ç¤ºæ„éŸ³éšœç¢è¯­éŸ³å¯¹äºæ„å»ºå¯é ä¸”å¯è§£é‡Šçš„ä¸´åºŠè¯„ä¼°å·¥å…·è‡³å…³é‡è¦ã€‚æœ¬ç ”ç©¶ä½¿ç”¨Whisper-Mediumæ¨¡å‹ç¼–ç å™¨å¯¹æ„éŸ³éšœç¢è¯­éŸ³è¿›è¡Œæ£€æµ‹å’Œè¯„ä¼°ï¼ˆå³ä¸¥é‡ç¨‹åº¦åˆ†ç±»ï¼‰ã€‚æˆ‘ä»¬åœ¨å•ä»»åŠ¡å’Œå¤šä»»åŠ¡è®¾ç½®ä¸‹ä½¿ç”¨çº¿æ€§åˆ†ç±»å™¨è¯„ä¼°é€å±‚åµŒå…¥ï¼Œå¹¶ç”¨è½®å»“åˆ†æ•°å’Œäº’ä¿¡æ¯æ¥è¡¥å……è¿™äº›ç»“æœï¼Œä»¥æä¾›å…³äºå±‚ä¿¡æ¯å«é‡çš„è§†è§’ã€‚ä¸ºäº†æ£€éªŒé€‚åº”æ€§ï¼Œæˆ‘ä»¬åœ¨å¯¹æ„éŸ³éšœç¢è¯­éŸ³è¯†åˆ«ä»»åŠ¡å¾®è°ƒWhisperåé‡å¤åˆ†æã€‚åœ¨å„é¡¹æŒ‡æ ‡ä¸­ï¼Œä¸­å±‚ç¼–ç å™¨å±‚ï¼ˆ13-15å±‚ï¼‰è¡¨ç°å‡ºæœ€ä¸°å¯Œçš„ä¿¡æ¯é‡ï¼Œè€Œå¾®è°ƒå¼•èµ·çš„å˜åŒ–è¾ƒå°ã€‚è¿™äº›å‘ç°æé«˜äº†å¯¹WhisperåµŒå…¥çš„å¯è§£é‡Šæ€§ï¼Œå¹¶çªå‡ºäº†æ¢æµ‹åˆ†æåœ¨æŒ‡å¯¼å¤§è§„æ¨¡é¢„è®­ç»ƒæ¨¡å‹ç”¨äºç—…ç†æ€§è¯­éŸ³æ–¹é¢çš„æ½œåŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.04219v1">PDF</a> Submitted to ICASSP 2026</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ç ”ç©¶äº†å¤§å‹ç«¯åˆ°ç«¯æ¨¡å‹å¦‚Whisperåœ¨ç—…ç†è¯­éŸ³ä¸Šçš„è¡¨ç°ï¼Œå°¤å…¶æ˜¯é€šè¿‡æ¢ç´¢æ¨¡å‹å¯¹å‘éŸ³éšœç¢æ€§è¯­éŸ³ï¼ˆdysarthric speechï¼‰åœ¨ä¸åŒå±‚çº§çš„è¡¨è¾¾æ–¹å¼ã€‚ç ”ç©¶è¡¨æ˜ä¸­å±‚ç¼–ç å™¨ï¼ˆä¸­å±‚å¦‚ç¬¬13-15å±‚ï¼‰å¯¹äºå¤„ç†å‘éŸ³éšœç¢æ€§è¯­éŸ³ä»»åŠ¡å°¤ä¸ºå…³é”®ã€‚ç ”ç©¶é€šè¿‡çº¿æ€§åˆ†ç±»å™¨åœ¨å•ä»»åŠ¡å’Œå¤šä»»åŠ¡ç¯å¢ƒä¸‹è¯„ä¼°äº†å±‚åµŒå…¥çš„è¡¨ç°ï¼Œå¹¶ç»“åˆè½®å»“ç³»æ•°å’Œäº’ä¿¡æ¯æ¥è¯„ä¼°å„å±‚çš„ä¿¡æ¯é‡è¦æ€§ã€‚å¯¹Whisperæ¨¡å‹è¿›è¡Œç‰¹å®šè¯­éŸ³éšœç¢è¯†åˆ«ä»»åŠ¡çš„å¾®è°ƒåè¿›è¡Œçš„è¯„ä¼°åˆ†æè¡¨æ˜å¾®è°ƒå¼•èµ·çš„å˜åŒ–æœ‰é™ã€‚ç ”ç©¶ç»“æœæœ‰åŠ©äºå¢å¼ºå¯¹WhisperåµŒå…¥æ¨¡å‹çš„è§£é‡Šæ€§ï¼ŒåŒæ—¶æ˜¾ç¤ºå‡ºé’ˆå¯¹ç—…ç†æ€§è¯­éŸ³çš„å¤§å‹é¢„è®­ç»ƒæ¨¡å‹æ¢æŸ¥åˆ†æåœ¨æŒ‡å¯¼ä½¿ç”¨æ–¹é¢çš„æ½œåŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>å¤§å‹ç«¯åˆ°ç«¯æ¨¡å‹å¦‚Whisperåœ¨ç—…ç†è¯­éŸ³å¤„ç†æ–¹é¢è¡¨ç°å‡ºè‰¯å¥½çš„æ€§èƒ½ï¼Œä½†å…¶å†…éƒ¨å¤„ç†æœºåˆ¶å°šå¾…æ·±å…¥ç†è§£ã€‚</li>
<li>å¯¹äºå»ºç«‹å¯é çš„ç—…ç†è¯­éŸ³ä¸´åºŠè¯„ä¼°å·¥å…·è€Œè¨€ï¼Œäº†è§£å¦‚ä½•ä»£è¡¨ç—…ç†æ€§è¯­éŸ³çš„å…³é”®ä¿¡æ¯æä¸ºé‡è¦ã€‚</li>
<li>å¯¹å‘éŸ³éšœç¢æ€§è¯­éŸ³ï¼Œä¸­å±‚ç¼–ç å™¨å±‚çº§è¡¨ç°å°¤ä¸ºå‡ºè‰²ï¼ˆç¬¬13-15å±‚ï¼‰ã€‚è¿™ä¸€å‘ç°è¡¨æ˜ï¼Œåœ¨å¯¹ç—…ç†æ€§è¯­éŸ³è¿›è¡Œè¯Šæ–­è¯„ä¼°æ—¶ï¼Œæ¨¡å‹çš„è¿™äº›å±‚æ¬¡å¯èƒ½ä¼šæˆä¸ºé‡è¦çš„å†³ç­–ç‚¹ã€‚è¿™ä¸€ä¿¡æ¯å¯èƒ½å¯¹å¼€å‘æ›´åŠ ç²¾ç¡®å’Œé«˜æ•ˆçš„è¯­éŸ³éšœç¢è¯†åˆ«ç³»ç»Ÿæœ‰é‡è¦ä»·å€¼ã€‚æ­¤å¤–ï¼Œè¿™ä¹Ÿæä¾›äº†ä¸€ä¸ªç†è§£å¤§å‹ç¥ç»ç½‘ç»œå¦‚ä½•å¤„ç†å¤æ‚è¯­éŸ³ä¿¡æ¯çš„è§†è§’ã€‚åœ¨å¤æ‚çš„è¯­éŸ³ä»»åŠ¡ä¸­ï¼Œæ¨¡å‹çš„è¿™äº›å±‚æ¬¡å¯èƒ½æ‰®æ¼”ç€å…³é”®è§’è‰²ï¼Œæœ‰åŠ©äºç†è§£è¿™äº›æ¨¡å‹çš„å·¥ä½œæœºåˆ¶ã€‚è¿™å¯¹äºå¼€å‘æ›´å‡†ç¡®çš„è¯­éŸ³è¯†åˆ«ç³»ç»Ÿè‡³å…³é‡è¦ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.04219">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-4876b6136c41070c573a7663aacb3d07~resize:0:q75.jpg?source=1f5c5e47&expiration=1760043679&auth_key=1760043679-0-0-223081c2d37edc3c903950789a234204&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-943c2955a0b3019bb065f1e12651845b~resize:0:q75.jpg?source=1f5c5e47&expiration=1760043686&auth_key=1760043686-0-0-62f35921e2dfc7db4cbfec5f3e1d3e0c&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-0c73dba57ed3c1e9fc408e52b6fe3cf5~resize:0:q75.jpg?source=1f5c5e47&expiration=1760043693&auth_key=1760043693-0-0-604bf3dd436bd514b14c6a81d5a079fa&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-151bc29dc14afc834c7c7bd5cb517535~resize:0:q75.jpg?source=1f5c5e47&expiration=1760043700&auth_key=1760043700-0-0-9ea3d5c2a3ece0e16adda352a7019b1a&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="Drax-Speech-Recognition-with-Discrete-Flow-Matching"><a href="#Drax-Speech-Recognition-with-Discrete-Flow-Matching" class="headerlink" title="Drax: Speech Recognition with Discrete Flow Matching"></a>Drax: Speech Recognition with Discrete Flow Matching</h2><p><strong>Authors:Aviv Navon, Aviv Shamsian, Neta Glazer, Yael Segal-Feldman, Gill Hetz, Joseph Keshet, Ethan Fetaya</strong></p>
<p>Diffusion and flow-based non-autoregressive (NAR) models have shown strong promise in large language modeling, however, their potential for automatic speech recognition (ASR) remains largely unexplored. We propose Drax, a discrete flow matching framework for ASR that enables efficient parallel decoding. To better align training with inference, we construct an audio-conditioned probability path that guides the model through trajectories resembling likely intermediate inference errors, rather than direct random noise to target transitions. Our theoretical analysis links the generalization gap to divergences between training and inference occupancies, controlled by cumulative velocity errors, thereby motivating our design choice. Empirical evaluation demonstrates that our approach attains recognition accuracy on par with state-of-the-art speech models while offering improved accuracy-efficiency trade-offs, highlighting discrete flow matching as a promising direction for advancing NAR ASR. </p>
<blockquote>
<p>æ‰©æ•£å’ŒåŸºäºæµçš„éè‡ªå›å½’ï¼ˆNARï¼‰æ¨¡å‹åœ¨å¤§è§„æ¨¡è¯­è¨€å»ºæ¨¡æ–¹é¢æ˜¾ç¤ºå‡ºå¼ºå¤§çš„æ½œåŠ›ï¼Œç„¶è€Œå®ƒä»¬åœ¨è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰æ–¹é¢çš„æ½œåŠ›å°šæœªå¾—åˆ°å¹¿æ³›æ¢ç´¢ã€‚æˆ‘ä»¬æå‡ºäº†Draxï¼Œè¿™æ˜¯ä¸€ä¸ªç”¨äºASRçš„ç¦»æ•£æµåŒ¹é…æ¡†æ¶ï¼Œå¯ä»¥å®ç°é«˜æ•ˆçš„å¹¶è¡Œè§£ç ã€‚ä¸ºäº†æ›´å¥½åœ°å°†è®­ç»ƒä¸æ¨ç†å¯¹é½ï¼Œæˆ‘ä»¬æ„å»ºäº†ä¸€ä¸ªå—éŸ³é¢‘å½±å“çš„æ¦‚ç‡è·¯å¾„ï¼Œè¯¥è·¯å¾„é€šè¿‡ç±»ä¼¼äºå¯èƒ½çš„ä¸­é—´æ¨ç†é”™è¯¯çš„è½¨è¿¹æ¥å¼•å¯¼æ¨¡å‹ï¼Œè€Œä¸æ˜¯ç›´æ¥å‘ç›®æ ‡è½¬æ¢æ·»åŠ éšæœºå™ªå£°ã€‚æˆ‘ä»¬çš„ç†è®ºåˆ†æå°†æ³›åŒ–å·®è·ä¸è®­ç»ƒå’Œæ¨ç†å ç”¨ä¹‹é—´çš„åˆ†æ­§è”ç³»èµ·æ¥ï¼Œè¿™ç”±ç´¯ç§¯é€Ÿåº¦è¯¯å·®æ§åˆ¶ï¼Œä»è€Œæ¿€å‘äº†æˆ‘ä»¬çš„è®¾è®¡é€‰æ‹©ã€‚ç»éªŒè¯„ä¼°è¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨ä¿è¯è¯†åˆ«å‡†ç¡®ç‡çš„åŒæ—¶ï¼Œæä¾›äº†æ”¹è¿›çš„å‡†ç¡®æ€§-æ•ˆç‡æƒè¡¡ï¼Œå‡¸æ˜¾å‡ºç¦»æ•£æµåŒ¹é…åœ¨æ¨è¿›NAR ASRæ–¹é¢æ˜¯ä¸€ä¸ªæœ‰å‰æ™¯çš„æ–¹å‘ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.04162v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æ‰©æ•£ä¸æµåŸºéè‡ªå›å½’ï¼ˆNARï¼‰æ¨¡å‹åœ¨å¤§å‹è¯­è¨€å»ºæ¨¡ä¸­å±•ç°å‡ºå¼ºå¤§çš„æ½œåŠ›ï¼Œä½†åœ¨è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰é¢†åŸŸçš„åº”ç”¨ä»å¾…æ¢ç´¢ã€‚æœ¬æ–‡æå‡ºä¸€ç§åä¸ºDraxçš„ç¦»æ•£æµåŒ¹é…æ¡†æ¶ï¼Œç”¨äºASRï¼Œå¯å®ç°é«˜æ•ˆå¹¶è¡Œè§£ç ã€‚ä¸ºæ›´å¥½åœ°å°†è®­ç»ƒä¸æ¨ç†å¯¹é½ï¼Œæœ¬æ–‡æ„å»ºäº†ä¸€ä¸ªå—éŸ³é¢‘å½±å“çš„æ¦‚ç‡è·¯å¾„ï¼Œé€šè¿‡å¼•å¯¼æ¨¡å‹é€šè¿‡ç±»ä¼¼å¯èƒ½ä¸­é—´æ¨ç†é”™è¯¯çš„è½¨è¿¹ï¼Œè€Œéç›´æ¥éšæœºå™ªå£°åˆ°ç›®æ ‡è½¬æ¢ã€‚æœ¬æ–‡çš„ç†è®ºåˆ†æå°†æ³›åŒ–å·®è·ä¸è®­ç»ƒå’Œæ¨ç†å ç”¨ä¹‹é—´çš„åˆ†æ­§è”ç³»èµ·æ¥ï¼Œå—ç´¯ç§¯é€Ÿåº¦è¯¯å·®æ§åˆ¶ï¼Œä»è€ŒéªŒè¯äº†è®¾è®¡é€‰æ‹©ã€‚å®è¯è¯„ä¼°è¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨è¯†åˆ«å‡†ç¡®ç‡æ–¹é¢è¾¾åˆ°äº†æœ€å…ˆè¿›çš„è¯­éŸ³æ¨¡å‹æ°´å¹³ï¼ŒåŒæ—¶æä¾›äº†æ›´å¥½çš„å‡†ç¡®æ€§-æ•ˆç‡æƒè¡¡ï¼Œçªæ˜¾å‡ºç¦»æ•£æµåŒ¹é…åœ¨æ¨è¿›NAR ASRæ–¹é¢çš„å‰æ™¯ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ‰©æ•£ä¸æµåŸºéè‡ªå›å½’æ¨¡å‹åœ¨å¤§å‹è¯­è¨€å»ºæ¨¡ä¸­å…·æ½œåŠ›ï¼Œä½†åœ¨ASRé¢†åŸŸåº”ç”¨å°šå¾…æ¢ç´¢ã€‚</li>
<li>æå‡ºäº†åä¸ºDraxçš„ç¦»æ•£æµåŒ¹é…æ¡†æ¶ï¼Œç”¨äºASRï¼Œå®ç°é«˜æ•ˆå¹¶è¡Œè§£ç ã€‚</li>
<li>æ„å»ºéŸ³é¢‘å½±å“æ¦‚ç‡è·¯å¾„ï¼Œä½¿æ¨¡å‹è®­ç»ƒæ›´è´´è¿‘æ¨ç†è¿‡ç¨‹ã€‚</li>
<li>ç†è®ºåˆ†ææ˜¾ç¤ºæ³›åŒ–å·®è·ä¸è®­ç»ƒã€æ¨ç†å ç”¨åˆ†æ­§æœ‰å…³ï¼Œå—ç´¯ç§¯é€Ÿåº¦è¯¯å·®æ§åˆ¶ã€‚</li>
<li>ç¦»æ•£æµåŒ¹é…æ¡†æ¶åœ¨æå‡ASRæ€§èƒ½æ–¹é¢å…·æœ‰æ½œåŠ›ã€‚</li>
<li>å®è¯è¯„ä¼°è¡¨æ˜ï¼Œè¯¥æ–¹æ³•è¯†åˆ«å‡†ç¡®ç‡ä¸æœ€å…ˆè¿›çš„è¯­éŸ³æ¨¡å‹ç›¸å½“ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.04162">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-a9308536b3be96919ca53496665fbc19~resize:0:q75.jpg?source=1f5c5e47&expiration=1760043707&auth_key=1760043707-0-0-ad869f9f3771f78c2e59d30456633137&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-d9d2099b8290edd8035925717cd583ea~resize:0:q75.jpg?source=1f5c5e47&expiration=1760043714&auth_key=1760043714-0-0-d0ed0923f6ef7058282ec8ab6f2165cf&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-dc92bebdd786aa16bcc1b03d173e9856~resize:0:q75.jpg?source=1f5c5e47&expiration=1760043721&auth_key=1760043721-0-0-d9e3a782faeab5245082b84233291918&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="GDiffuSE-Diffusion-based-speech-enhancement-with-noise-model-guidance"><a href="#GDiffuSE-Diffusion-based-speech-enhancement-with-noise-model-guidance" class="headerlink" title="GDiffuSE: Diffusion-based speech enhancement with noise model guidance"></a>GDiffuSE: Diffusion-based speech enhancement with noise model guidance</h2><p><strong>Authors:Efrayim Yanir, David Burshtein, Sharon Gannot</strong></p>
<p>This paper introduces a novel speech enhancement (SE) approach based on a denoising diffusion probabilistic model (DDPM), termed Guided diffusion for speech enhancement (GDiffuSE). In contrast to conventional methods that directly map noisy speech to clean speech, our method employs a lightweight helper model to estimate the noise distribution, which is then incorporated into the diffusion denoising process via a guidance mechanism. This design improves robustness by enabling seamless adaptation to unseen noise types and by leveraging large-scale DDPMs originally trained for speech generation in the context of SE. We evaluate our approach on noisy signals obtained by adding noise samples from the BBC sound effects database to LibriSpeech utterances, showing consistent improvements over state-of-the-art baselines under mismatched noise conditions. Examples are available at our project webpage. </p>
<blockquote>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åŸºäºé™å™ªæ‰©æ•£æ¦‚ç‡æ¨¡å‹ï¼ˆDDPMï¼‰çš„æ–°å‹è¯­éŸ³å¢å¼ºï¼ˆSEï¼‰æ–¹æ³•ï¼Œç§°ä¸ºç”¨äºè¯­éŸ³å¢å¼ºçš„å¼•å¯¼æ‰©æ•£ï¼ˆGDiffuSEï¼‰ã€‚ä¸ä¼ ç»Ÿçš„ç›´æ¥æ˜ å°„å¸¦å™ªè¯­éŸ³åˆ°æ¸…æ´è¯­éŸ³çš„æ–¹æ³•ä¸åŒï¼Œæˆ‘ä»¬çš„æ–¹æ³•é‡‡ç”¨è½»é‡çº§è¾…åŠ©æ¨¡å‹æ¥ä¼°è®¡å™ªå£°åˆ†å¸ƒï¼Œç„¶åé€šè¿‡å¼•å¯¼æœºåˆ¶å°†å…¶çº³å…¥æ‰©æ•£å»å™ªè¿‡ç¨‹ã€‚è¿™ç§è®¾è®¡é€šè¿‡æ— ç¼é€‚åº”æœªè§è¿‡çš„å™ªå£°ç±»å‹ä»¥åŠåˆ©ç”¨æœ€åˆä¸ºè¯­éŸ³å¢å¼ºèƒŒæ™¯ä¸‹çš„è¯­éŸ³ç”Ÿæˆè€Œè®­ç»ƒçš„å¤§è§„æ¨¡DDPMï¼Œæé«˜äº†ç¨³å¥æ€§ã€‚æˆ‘ä»¬åœ¨é€šè¿‡å°†ä»BBCéŸ³æ•ˆæ•°æ®åº“è·å¾—çš„å™ªå£°æ ·æœ¬æ·»åŠ åˆ°LibriSpeechè¯è¯­ä¸­è€Œè·å¾—çš„å¸¦å™ªä¿¡å·ä¸Šè¯„ä¼°äº†æˆ‘ä»¬çš„æ–¹æ³•ï¼Œåœ¨å™ªå£°æ¡ä»¶ä¸åŒ¹é…çš„æƒ…å¢ƒä¸‹ï¼Œç›¸å¯¹äºæœ€æ–°åŸºçº¿æŠ€æœ¯ï¼Œæ˜¾ç¤ºå‡ºäº†ä¸€è‡´æ€§çš„æ”¹è¿›ã€‚ç¤ºä¾‹å¯åœ¨æˆ‘ä»¬çš„é¡¹ç›®ç½‘é¡µä¸Šæ‰¾åˆ°ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.04157v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>åŸºäºå»å™ªæ‰©æ•£æ¦‚ç‡æ¨¡å‹çš„è¯­éŸ³å¢å¼ºæ–°æ–¹æ³•ä»‹ç»ã€‚è¯¥æ–¹æ³•é‡‡ç”¨è½»é‡çº§è¾…åŠ©æ¨¡å‹ä¼°è®¡å™ªå£°åˆ†å¸ƒï¼Œå¹¶å°†å…¶é€šè¿‡æŒ‡å¯¼æœºåˆ¶èå…¥æ‰©æ•£å»å™ªè¿‡ç¨‹ï¼Œæé«˜äº†å¯¹ä¸åŒå™ªå£°ç±»å‹çš„é€‚åº”æ€§å’Œç¨³å¥æ€§ã€‚åœ¨LibriSpeechè¯­éŸ³æ•°æ®ä¸Šæ·»åŠ BBCéŸ³æ•ˆåº“å™ªå£°æ ·æœ¬è¿›è¡Œè¯„ä¼°ï¼Œæ˜¾ç¤ºäº†åœ¨ä¸åŒ¹é…å™ªå£°æ¡ä»¶ä¸‹çš„æ”¹è¿›æ•ˆæœã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ä»‹ç»äº†åŸºäºå»å™ªæ‰©æ•£æ¦‚ç‡æ¨¡å‹ï¼ˆDDPMï¼‰çš„è¯­éŸ³å¢å¼ºæ–°æ–¹æ³•GDiffuSEã€‚</li>
<li>GDiffuSEé‡‡ç”¨è¾…åŠ©æ¨¡å‹ä¼°è®¡å™ªå£°åˆ†å¸ƒï¼Œå¹¶èå…¥æ‰©æ•£å»å™ªè¿‡ç¨‹ã€‚</li>
<li>GDiffuSEé€šè¿‡æŒ‡å¯¼æœºåˆ¶æé«˜äº†å¯¹ä¸åŒå™ªå£°ç±»å‹çš„é€‚åº”æ€§ã€‚</li>
<li>åœ¨LibriSpeechæ•°æ®ä¸Šæ·»åŠ BBCéŸ³æ•ˆåº“å™ªå£°æ ·æœ¬è¿›è¡Œè¯„ä¼°ã€‚</li>
<li>GDiffuSEåœ¨ä¸åŒ¹é…å™ªå£°æ¡ä»¶ä¸‹è¡¨ç°ä¼˜äºç°æœ‰å…ˆè¿›æŠ€æœ¯ã€‚</li>
<li>GDiffuSEæ–¹æ³•é€šè¿‡èåˆå™ªå£°ä¼°è®¡å’Œæ‰©æ•£å»å™ªï¼Œæé«˜äº†è¯­éŸ³å¢å¼ºçš„æ•ˆæœã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.04157">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-d94b6683fb55719a1383392020347b1a~resize:0:q75.jpg?source=1f5c5e47&expiration=1760043729&auth_key=1760043729-0-0-91b01bf45f943b91bd19c25306e2f201&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-9a77f1fa0b0d71fb7c2d794c6885175e~resize:0:q75.jpg?source=1f5c5e47&expiration=1760043736&auth_key=1760043736-0-0-a2c9ca734b09eaab64aa1e410a582c64&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-e6af7a2f943289c879fa574123fe6ccb~resize:0:q75.jpg?source=1f5c5e47&expiration=1760043742&auth_key=1760043742-0-0-f19fdf9c44f1e29ce0855dabad107656&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-525c1435bc62eadb0c71ccf25e1debcd~resize:0:q75.jpg?source=1f5c5e47&expiration=1760043748&auth_key=1760043748-0-0-62acdec0c8fdb9a60e6ad69a5c935d7c&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-4acb0188d51b2fc2de11da4892aa900d~resize:0:q75.jpg?source=1f5c5e47&expiration=1760043755&auth_key=1760043755-0-0-45eb140170327da311961781203f8f76&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="MoME-Mixture-of-Matryoshka-Experts-for-Audio-Visual-Speech-Recognition"><a href="#MoME-Mixture-of-Matryoshka-Experts-for-Audio-Visual-Speech-Recognition" class="headerlink" title="MoME: Mixture of Matryoshka Experts for Audio-Visual Speech Recognition"></a>MoME: Mixture of Matryoshka Experts for Audio-Visual Speech Recognition</h2><p><strong>Authors:Umberto Cappellazzo, Minsu Kim, Pingchuan Ma, Honglie Chen, Xubo Liu, Stavros Petridis, Maja Pantic</strong></p>
<p>Large language models (LLMs) have recently shown strong potential in audio-visual speech recognition (AVSR), but their high computational demands and sensitivity to token granularity limit their practicality in resource-constrained settings. Token compression methods can reduce inference cost, but they require fixing a compression rate in advance and produce a single fixed-length output, offering no flexibility to balance information density and efficiency at inference time. Matryoshka representation learning (MRL) addresses this by enabling a single model to operate across multiple token granularities, allowing compression rates to be adjusted dynamically. However, current MRL-based methods treat each scale independently during training, limiting cross-scale generalization, robustness at high compression, and interpretability. To overcome these limitations, we propose MoME (Mixture of Matryoshka Experts), a novel framework that integrates sparse Mixture-of-Experts (MoE) into MRL-based LLMs for AVSR. MoME augments a frozen LLM with top-k routed and shared experts, allowing dynamic capacity allocation across scales and modalities. A shared router promotes consistent expert activation across granularities, enabling compressed sequences to benefit from representations learned at lower compression. Experiments on LRS2 and LRS3 demonstrate that MoME achieves state-of-the-art performance across AVSR, ASR, and VSR tasks, while requiring significantly fewer parameters and maintaining robustness under noise. MoME unifies the adaptability of MRL with the efficiency of MoE, offering a scalable and interpretable solution for resource-aware speech recognition. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨éŸ³é¢‘è§†è§‰è¯­éŸ³è¯†åˆ«ï¼ˆAVSRï¼‰æ–¹é¢æœ€è¿‘æ˜¾ç¤ºå‡ºå¼ºå¤§çš„æ½œåŠ›ï¼Œä½†å…¶é«˜è®¡ç®—éœ€æ±‚å’Œé’ˆå¯¹ä»¤ç‰Œç²’åº¦çš„æ•æ„Ÿæ€§é™åˆ¶äº†å…¶åœ¨èµ„æºå—é™ç¯å¢ƒä¸­çš„å®ç”¨æ€§ã€‚ä»¤ç‰Œå‹ç¼©æ–¹æ³•å¯ä»¥é™ä½æ¨ç†æˆæœ¬ï¼Œä½†å®ƒä»¬éœ€è¦æå‰ç¡®å®šå‹ç¼©ç‡ï¼Œå¹¶äº§ç”Ÿå•ä¸€å›ºå®šé•¿åº¦çš„è¾“å‡ºï¼Œæ— æ³•åœ¨æ¨ç†æ—¶å¹³è¡¡ä¿¡æ¯å¯†åº¦å’Œæ•ˆç‡ã€‚Matryoshkaè¡¨ç¤ºå­¦ä¹ ï¼ˆMRLï¼‰é€šè¿‡ä½¿å•ä¸ªæ¨¡å‹èƒ½å¤Ÿåœ¨å¤šä¸ªä»¤ç‰Œç²’åº¦ä¸Šè¿è¡Œï¼Œä»è€Œèƒ½å¤ŸåŠ¨æ€è°ƒæ•´å‹ç¼©ç‡æ¥è§£å†³è¿™ä¸€é—®é¢˜ã€‚ç„¶è€Œï¼Œå½“å‰çš„MRLæ–¹æ³•åœ¨å¤„ç†è®­ç»ƒæ—¶ç‹¬ç«‹å¤„ç†æ¯ä¸ªè§„æ¨¡ï¼Œè¿™é™åˆ¶äº†è·¨è§„æ¨¡æ³›åŒ–ã€é«˜å‹ç¼©æ—¶çš„ç¨³å¥æ€§å’Œå¯è§£é‡Šæ€§ã€‚ä¸ºäº†å…‹æœè¿™äº›å±€é™æ€§ï¼Œæˆ‘ä»¬æå‡ºäº†MoMEï¼ˆMatryoshkaä¸“å®¶æ··åˆç‰©ï¼‰ï¼Œè¿™æ˜¯ä¸€ç§å°†ç¨€ç–çš„ä¸“å®¶æ··åˆç‰©ï¼ˆMoEï¼‰é›†æˆåˆ°åŸºäºMRLçš„LLMä¸­çš„æ–°å‹æ¡†æ¶ï¼Œç”¨äºAVSRã€‚MoMEé€šè¿‡é¡¶çº§kè·¯ç”±å’Œå…±äº«ä¸“å®¶å¢å¼ºå†»ç»“çš„LLMï¼Œå…è®¸è·¨è§„æ¨¡å’Œè·¨æ¨¡æ€çš„åŠ¨æ€å®¹é‡åˆ†é…ã€‚å…±äº«è·¯ç”±å™¨ä¿ƒè¿›äº†è·¨ç²’åº¦çš„ä¸“å®¶æ¿€æ´»çš„ä¸€è‡´æ€§ï¼Œä½¿å¾—å‹ç¼©åºåˆ—èƒ½å¤Ÿå—ç›Šäºè¾ƒä½å‹ç¼©ä¸‹å­¦ä¹ çš„è¡¨ç¤ºã€‚åœ¨LRS2å’ŒLRS3ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒMoMEåœ¨AVSRã€ASRå’ŒVSRä»»åŠ¡ä¸Šè¾¾åˆ°äº†æœ€æ–°æ€§èƒ½æ°´å¹³ï¼ŒåŒæ—¶æ˜¾è‘—å‡å°‘äº†å‚æ•°éœ€æ±‚ï¼Œå¹¶åœ¨å™ªå£°ä¸‹ä¿æŒäº†ç¨³å¥æ€§ã€‚MoMEå°†MRLçš„é€‚åº”æ€§ä¸MoEçš„æ•ˆç‡ç»Ÿä¸€èµ·æ¥ï¼Œä¸ºèµ„æºæ„ŸçŸ¥è¯­éŸ³è¯†åˆ«æä¾›äº†å¯æ‰©å±•å’Œå¯è§£é‡Šçš„è§£å†³æ–¹æ¡ˆã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.04136v1">PDF</a> NeurIPS 2025</p>
<p><strong>æ‘˜è¦</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹åœ¨éŸ³è§†é¢‘è¯­éŸ³è¯†åˆ«é¢†åŸŸå±•ç°å‡ºå·¨å¤§æ½œåŠ›ï¼Œä½†å…¶é«˜è®¡ç®—éœ€æ±‚å’Œä»¤ç‰Œç²’åº¦æ•æ„Ÿæ€§é™åˆ¶äº†å…¶åœ¨èµ„æºå—é™ç¯å¢ƒä¸­çš„åº”ç”¨ã€‚MoMEï¼ˆæ··åˆé©¬æ‰˜ä»€å¡ä¸“å®¶ï¼‰ä½œä¸ºä¸€ç§æ–°é¢–æ¡†æ¶ï¼Œèåˆäº†ç¨€ç–æ··åˆä¸“å®¶ä¸åŸºäºé©¬æ‰˜ä»€å¡çš„é€’å½’å­¦ä¹ æ¨¡å‹ï¼Œå®ç°åŠ¨æ€å®¹é‡è·¨å°ºåº¦å’Œæ¨¡æ€åˆ†é…ï¼Œèƒ½å¤Ÿåœ¨èµ„æºå—é™çš„æƒ…å†µä¸‹æœ‰æ•ˆæå‡éŸ³é¢‘è¯†åˆ«çš„æ•ˆæœä¸æ•ˆç‡ã€‚å®éªŒç»“æœåœ¨LRS2å’ŒLRS3æ•°æ®é›†ä¸Šè¯æ˜äº†MoMEåœ¨éŸ³è§†é¢‘è¯­éŸ³è¯†åˆ«ã€è¯­éŸ³è¯†åˆ«å’Œè§†é¢‘è¯†åˆ«ä»»åŠ¡ä¸Šçš„è¡¨ç°å‡è¾¾åˆ°ä¸šç•Œæœ€ä½³æ°´å¹³ï¼ŒåŒæ—¶æ˜¾è‘—å‡å°‘äº†å‚æ•°éœ€æ±‚å¹¶ä¿æŒäº†æŠ—å™ªæ€§ã€‚MoMEç»Ÿä¸€äº†é©¬æ‰˜ä»€å¡é€’å½’å­¦ä¹ çš„é€‚åº”æ€§å’Œæ··åˆä¸“å®¶çš„æ•ˆç‡ï¼Œä¸ºèµ„æºæ„ŸçŸ¥è¯­éŸ³è¯†åˆ«æä¾›äº†å¯æ‰©å±•å’Œå¯è§£é‡Šçš„è§£å†³æ–¹æ¡ˆã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹åœ¨éŸ³è§†é¢‘è¯­éŸ³è¯†åˆ«ä¸­è¡¨ç°å‡ºå¼ºå¤§çš„æ½œåŠ›ï¼Œä½†èµ„æºå—é™ç¯å¢ƒä¸‹å­˜åœ¨é«˜è®¡ç®—éœ€æ±‚å’Œä»¤ç‰Œç²’åº¦æ•æ„Ÿæ€§æŒ‘æˆ˜ã€‚</li>
<li>ä¼ ç»Ÿä»¤ç‰Œå‹ç¼©æ–¹æ³•æ— æ³•å¹³è¡¡ä¿¡æ¯å¯†åº¦å’Œæ¨ç†æ•ˆç‡ï¼Œè€Œé©¬æ‰˜ä»€å¡è¡¨ç¤ºå­¦ä¹ èƒ½è°ƒæ•´å‹ç¼©ç‡ä½†ç¼ºä¹è·¨å°ºåº¦æ³›åŒ–å’Œé²æ£’æ€§ã€‚</li>
<li>MoMEæ¡†æ¶èåˆäº†ç¨€ç–æ··åˆä¸“å®¶ä¸åŸºäºé©¬æ‰˜ä»€å¡çš„é€’å½’å­¦ä¹ æ¨¡å‹ï¼Œå…è®¸åŠ¨æ€å®¹é‡åˆ†é…å¹¶ä¿ƒè¿›è·¨å°ºåº¦å’Œæ¨¡æ€çš„ä¸€è‡´æ€§ä¸“å®¶æ¿€æ´»ã€‚</li>
<li>MoMEé€šè¿‡å¢å¼ºå†»ç»“çš„å¤§å‹è¯­è¨€æ¨¡å‹å¹¶å…±äº«ä¸“å®¶è·¯ç”±ï¼Œä½¿å‹ç¼©åºåˆ—èƒ½å¤Ÿå—ç›Šäºè¾ƒä½å‹ç¼©æ—¶çš„è¡¨ç¤ºå­¦ä¹ ã€‚</li>
<li>åœ¨LRS2å’ŒLRS3æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜MoMEåœ¨éŸ³è§†é¢‘è¯­éŸ³è¯†åˆ«ã€è¯­éŸ³è¯†åˆ«å’Œè§†é¢‘è¯†åˆ«ä»»åŠ¡ä¸Šå®ç°äº†ä¸šç•Œæœ€ä½³æ€§èƒ½ã€‚</li>
<li>MoMEç›¸è¾ƒäºä¼ ç»Ÿæ–¹æ³•æ˜¾è‘—å‡å°‘äº†å‚æ•°éœ€æ±‚å¹¶ä¿æŒäº†è‰¯å¥½çš„æŠ—å™ªæ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.04136">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-30ded001a72afca7e8d7d504e48c7304~resize:0:q75.jpg?source=1f5c5e47&expiration=1760043762&auth_key=1760043762-0-0-072e09927537bb3b5280e5e9b970976a&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-d1d8e9a48e8c6df2bdf6e5e93b1a478b~resize:0:q75.jpg?source=1f5c5e47&expiration=1760043769&auth_key=1760043769-0-0-640df219631b4b07f8a98e297d0d8dff&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-671778d3c626e52b64a30376f0777ce2~resize:0:q75.jpg?source=1f5c5e47&expiration=1760043776&auth_key=1760043776-0-0-43831fd0a8bdb77f793b9b6fae0e20d4&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="Cross-Lingual-Multi-Granularity-Framework-for-Interpretable-Parkinsonâ€™s-Disease-Diagnosis-from-Speech"><a href="#Cross-Lingual-Multi-Granularity-Framework-for-Interpretable-Parkinsonâ€™s-Disease-Diagnosis-from-Speech" class="headerlink" title="Cross-Lingual Multi-Granularity Framework for Interpretable Parkinsonâ€™s   Disease Diagnosis from Speech"></a>Cross-Lingual Multi-Granularity Framework for Interpretable Parkinsonâ€™s   Disease Diagnosis from Speech</h2><p><strong>Authors:Ilias Tougui, Mehdi Zakroum, Mounir Ghogho</strong></p>
<p>Parkinsonâ€™s Disease (PD) affects over 10 million people worldwide, with speech impairments in up to 89% of patients. Current speech-based detection systems analyze entire utterances, potentially overlooking the diagnostic value of specific phonetic elements. We developed a granularity-aware approach for multilingual PD detection using an automated pipeline that extracts time-aligned phonemes, syllables, and words from recordings. Using Italian, Spanish, and English datasets, we implemented a bidirectional LSTM with multi-head attention to compare diagnostic performance across the different granularity levels. Phoneme-level analysis achieved superior performance with AUROC of 93.78% +- 2.34% and accuracy of 92.17% +- 2.43%. This demonstrates enhanced diagnostic capability for cross-linguistic PD detection. Importantly, attention analysis revealed that the most informative speech features align with those used in established clinical protocols: sustained vowels (&#x2F;a&#x2F;, &#x2F;e&#x2F;, &#x2F;o&#x2F;, &#x2F;i&#x2F;) at phoneme level, diadochokinetic syllables (&#x2F;ta&#x2F;, &#x2F;pa&#x2F;, &#x2F;la&#x2F;, &#x2F;ka&#x2F;) at syllable level, and &#x2F;pataka&#x2F; sequences at word level. Source code will be available at <a target="_blank" rel="noopener" href="https://github.com/jetliqs/clearpd">https://github.com/jetliqs/clearpd</a>. </p>
<blockquote>
<p>å¸•é‡‘æ£®ç—…ï¼ˆPDï¼‰å½±å“å…¨çƒè¶…è¿‡1000ä¸‡äººï¼Œå…¶ä¸­é«˜è¾¾89%çš„æ‚£è€…å­˜åœ¨è¯­è¨€éšœç¢ã€‚å½“å‰çš„åŸºäºè¯­éŸ³çš„æ£€æµ‹ç³»ç»Ÿåˆ†ææ•´ä¸ªè¯è¯­ï¼Œå¯èƒ½ä¼šå¿½ç•¥ç‰¹å®šè¯­éŸ³å…ƒç´ åœ¨è¯Šæ–­ä¸­çš„ä»·å€¼ã€‚æˆ‘ä»¬å¼€å‘äº†ä¸€ç§ç”¨äºå¤šè¯­è¨€PDæ£€æµ‹çš„ç²’åº¦æ„ŸçŸ¥æ–¹æ³•ï¼Œä½¿ç”¨è‡ªåŠ¨åŒ–ç®¡é“ä»å½•éŸ³ä¸­æå–æ—¶é—´å¯¹é½çš„éŸ³ç´ ã€éŸ³èŠ‚å’Œå•è¯ã€‚æˆ‘ä»¬ä½¿ç”¨æ„å¤§åˆ©è¯­ã€è¥¿ç­ç‰™è¯­å’Œè‹±è¯­æ•°æ®é›†ï¼Œé€šè¿‡åŒå‘LSTMå’Œå¤šå¤´æ³¨æ„åŠ›æœºåˆ¶å®ç°äº†å¯¹ä¸åŒç²’åº¦çº§åˆ«çš„è¯Šæ–­æ€§èƒ½çš„æ¯”è¾ƒã€‚éŸ³ç´ çº§åˆ†æå–å¾—äº†å“è¶Šçš„æ€§èƒ½ï¼Œæ›²çº¿ä¸‹é¢ç§¯ï¼ˆAUROCï¼‰ä¸º93.78%Â±2.34%ï¼Œå‡†ç¡®åº¦ä¸º92.17%Â±2.43%ã€‚è¿™è¯æ˜äº†è·¨è¯­è¨€PDæ£€æµ‹çš„å¢å¼ºè¯Šæ–­èƒ½åŠ›ã€‚é‡è¦çš„æ˜¯ï¼Œæ³¨æ„åŠ›åˆ†æè¡¨æ˜ï¼Œæœ€å…·ä¿¡æ¯é‡çš„è¯­éŸ³ç‰¹å¾ç¬¦åˆå·²å»ºç«‹çš„ä¸´åºŠåè®®ä¸­ä½¿ç”¨çš„ç‰¹å¾ï¼šéŸ³ç´ çº§çš„æŒç»­å…ƒéŸ³ï¼ˆ&#x2F;a&#x2F;ã€&#x2F;e&#x2F;ã€&#x2F;o&#x2F;ã€&#x2F;i&#x2F;ï¼‰ï¼ŒéŸ³èŠ‚çº§çš„è¿ç»­åŠ¨è§‰éŸ³èŠ‚ï¼ˆ&#x2F;ta&#x2F;ã€&#x2F;pa&#x2F;ã€&#x2F;la&#x2F;ã€&#x2F;ka&#x2F;ï¼‰ï¼Œä»¥åŠå•è¯çº§åˆ«çš„&#x2F;pataka&#x2F;åºåˆ—ã€‚æºä»£ç å°†å‘å¸ƒåœ¨<a target="_blank" rel="noopener" href="https://github.com/jetliqs/clearpd%E3%80%82">https://github.com/jetliqs/clearpdã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.03758v1">PDF</a> </p>
<p><strong>Summary</strong>ï¼š<br>å¸•é‡‘æ£®ç—…å½±å“å…¨çƒè¶…è¿‡åƒä¸‡äººç¾¤ï¼Œæ‚£è€…ä¸­æœ‰é«˜è¾¾89%å­˜åœ¨è¯­éŸ³éšœç¢ã€‚ä¼ ç»Ÿè¯­éŸ³æ£€æµ‹ä½“ç³»åˆ†ææ•´ä¸ªè¯è¯­ï¼Œå¯èƒ½å¿½ç•¥ç‰¹å®šéŸ³ç´ å¯¹è¯Šæ–­çš„ä»·å€¼ã€‚ç ”ç©¶è€…å¼€å‘äº†ä¸€ç§ç²’åº¦æ„ŸçŸ¥çš„å¤šè¯­è¨€å¸•é‡‘æ£®ç—…æ£€æµ‹æ³•ï¼Œé€šè¿‡è‡ªåŠ¨åŒ–æµç¨‹æå–æ—¶é—´å¯¹é½çš„éŸ³ç´ ã€éŸ³èŠ‚å’Œå•è¯ã€‚ç ”ç©¶ä½¿ç”¨æ„å¤§åˆ©è¯­ã€è¥¿ç­ç‰™è¯­å’Œè‹±è¯­æ•°æ®é›†ï¼Œé‡‡ç”¨åŒå‘LSTMå’Œå¤šå¤´æ³¨æ„åŠ›æœºåˆ¶ï¼Œå¯¹æ¯”ä¸åŒç²’åº¦çº§åˆ«çš„è¯Šæ–­æ€§èƒ½ã€‚éŸ³ç´ çº§åˆ«çš„åˆ†æè¡¨ç°æœ€ä½³ï¼ŒAUCè¾¾åˆ°93.78%Â±2.34%ï¼Œå‡†ç¡®ç‡92.17%Â±2.43%ï¼Œæ˜¾ç¤ºè·¨è¯­è¨€å¸•é‡‘æ£®ç—…è¯Šæ–­çš„å¢å¼ºèƒ½åŠ›ã€‚æ³¨æ„åŠ›åˆ†ææ­ç¤ºï¼Œæœ€å…·æœ‰è¯Šæ–­æ„ä¹‰çš„è¯­éŸ³ç‰¹å¾ä¸ä¸´åºŠåè®®ä¸­çš„ä¸€è‡´ï¼šéŸ³ç´ çº§åˆ«çš„æŒç»­å…ƒéŸ³ï¼ˆå¦‚&#x2F;a&#x2F;ã€&#x2F;e&#x2F;ã€&#x2F;o&#x2F;ã€&#x2F;i&#x2F;ï¼‰ï¼ŒéŸ³èŠ‚çº§åˆ«çš„è¿ç»­åŠ¨è§‰éŸ³èŠ‚ï¼ˆå¦‚&#x2F;ta&#x2F;ã€&#x2F;pa&#x2F;ã€&#x2F;la&#x2F;ã€&#x2F;ka&#x2F;ï¼‰ï¼Œå•è¯çº§åˆ«çš„&#x2F;patakaåºåˆ—ç­‰ã€‚ç›¸å…³æºä»£ç å°†åœ¨<a target="_blank" rel="noopener" href="https://github.com/jetliqs/clearpd">é“¾æ¥</a>å…¬å¼€ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>å¸•é‡‘æ£®ç—…å½±å“å¹¿æ³›ï¼Œå¤šæ•°æ‚£è€…å­˜åœ¨è¯­éŸ³éšœç¢ã€‚</li>
<li>ä¼ ç»Ÿè¯­éŸ³æ£€æµ‹ç³»ç»Ÿå¯èƒ½å¿½ç•¥ç‰¹å®šéŸ³ç´ åœ¨è¯Šæ–­ä¸­çš„é‡è¦æ€§ã€‚</li>
<li>ç ”ç©¶è€…é‡‡ç”¨ç²’åº¦æ„ŸçŸ¥çš„å¤šè¯­è¨€æ£€æµ‹æ³•ï¼Œä»¥è‡ªåŠ¨åŒ–æµç¨‹åˆ†æéŸ³ç´ ã€éŸ³èŠ‚å’Œå•è¯ã€‚</li>
<li>éŸ³ç´ çº§åˆ«åˆ†æè¡¨ç°æœ€ä½³ï¼Œæ˜¾ç¤ºå‡ºè·¨è¯­è¨€è¯Šæ–­çš„å¢å¼ºèƒ½åŠ›ã€‚</li>
<li>è¯Šæ–­æ€§è¯­éŸ³ç‰¹å¾åŒ…æ‹¬éŸ³ç´ çº§åˆ«çš„æŒç»­å…ƒéŸ³ã€éŸ³èŠ‚çº§åˆ«çš„è¿ç»­åŠ¨è§‰éŸ³èŠ‚åŠç‰¹å®šå•è¯åºåˆ—ã€‚</li>
<li>è¯¥ç ”ç©¶æä¾›äº†å¼€æºä»£ç ä»¥ä¾›å…¬ä¼—å‚è€ƒå’Œä½¿ç”¨ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.03758">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-994b22bfaf75ba86bde9398c53d1f0cf~resize:0:q75.jpg?source=1f5c5e47&expiration=1760043783&auth_key=1760043783-0-0-6eb075f0df41d5d719d4bf9290d5e52b&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-a0557f0bb804313e8d414e5577c437f6~resize:0:q75.jpg?source=1f5c5e47&expiration=1760043791&auth_key=1760043791-0-0-9cf875623591837f363c8e967964e6bb&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-62f6aa37d94e36105729840453c3fd77~resize:0:q75.jpg?source=1f5c5e47&expiration=1760043797&auth_key=1760043797-0-0-051232939facfd4a9d5698e56da0fb1d&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-8cddd92640982572724b18dcc5849009~resize:0:q75.jpg?source=1f5c5e47&expiration=1760043804&auth_key=1760043804-0-0-14106232bb0c32738fdfb83ec5cadf0d&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-df3bf8ce196f136a5352cc9eabff453c~resize:0:q75.jpg?source=1f5c5e47&expiration=1760043810&auth_key=1760043810-0-0-ce91b9d46be27a0fffaaf12be2d25ac2&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="Adapting-Diarization-Conditioned-Whisper-for-End-to-End-Multi-Talker-Speech-Recognition"><a href="#Adapting-Diarization-Conditioned-Whisper-for-End-to-End-Multi-Talker-Speech-Recognition" class="headerlink" title="Adapting Diarization-Conditioned Whisper for End-to-End Multi-Talker   Speech Recognition"></a>Adapting Diarization-Conditioned Whisper for End-to-End Multi-Talker   Speech Recognition</h2><p><strong>Authors:Martin Kocour, Martin Karafiat, Alexander Polok, Dominik Klement, LukÃ¡Å¡ Burget, Jan ÄŒernockÃ½</strong></p>
<p>We propose a speaker-attributed (SA) Whisper-based model for multi-talker speech recognition that combines target-speaker modeling with serialized output training (SOT). Our approach leverages a Diarization-Conditioned Whisper (DiCoW) encoder to extract target-speaker embeddings, which are concatenated into a single representation and passed to a shared decoder. This enables the model to transcribe overlapping speech as a serialized output stream with speaker tags and timestamps. In contrast to target-speaker ASR systems such as DiCoW, which decode each speaker separately, our approach performs joint decoding, allowing the decoder to condition on the context of all speakers simultaneously. Experiments show that the model outperforms existing SOT-based approaches and surpasses DiCoW on multi-talker mixtures (e.g., LibriMix). </p>
<blockquote>
<p>æˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºè¯´è¯äººå±æ€§ï¼ˆSAï¼‰çš„åŸºäºè€³è¯­çš„å¤šè¯´è¯äººè¯­éŸ³è¯†åˆ«æ¨¡å‹ï¼Œè¯¥æ¨¡å‹ç»“åˆäº†ç›®æ ‡è¯´è¯äººå»ºæ¨¡å’Œåºåˆ—åŒ–è¾“å‡ºè®­ç»ƒï¼ˆSOTï¼‰ã€‚æˆ‘ä»¬çš„æ–¹æ³•åˆ©ç”¨ä»¥è¯†åˆ«ä¸ºä¸­å¿ƒçš„è€³è¯­ï¼ˆDiCoWï¼‰ç¼–ç å™¨æå–ç›®æ ‡è¯´è¯äººçš„åµŒå…¥ï¼Œå¹¶å°†å…¶ä¸²è”æˆä¸€ä¸ªå•ä¸€è¡¨ç¤ºï¼Œç„¶åä¼ é€’ç»™å…±äº«è§£ç å™¨ã€‚è¿™ä½¿å¾—æ¨¡å‹èƒ½å¤Ÿå°†é‡å çš„è¯­éŸ³è½¬å½•ä¸ºå¸¦æœ‰è¯´è¯äººæ ‡ç­¾å’Œæ—¶é—´æˆ³çš„åºåˆ—åŒ–è¾“å‡ºæµã€‚ä¸è¯¸å¦‚DiCoWä¹‹ç±»çš„ç›®æ ‡è¯´è¯äººASRç³»ç»Ÿä¸åŒï¼Œè¿™äº›ç³»ç»Ÿåˆ†åˆ«è§£ç æ¯ä¸ªè¯´è¯äººï¼Œè€Œæˆ‘ä»¬çš„æ–¹æ³•æ‰§è¡Œè”åˆè§£ç ï¼Œå…è®¸è§£ç å™¨åŒæ—¶è€ƒè™‘æ‰€æœ‰è¯´è¯äººçš„ä¸Šä¸‹æ–‡ã€‚å®éªŒè¡¨æ˜ï¼Œè¯¥æ¨¡å‹ä¼˜äºç°æœ‰çš„åŸºäºSOTçš„æ–¹æ³•ï¼Œå¹¶ä¸”åœ¨å¤šè¯´è¯äººæ··åˆï¼ˆä¾‹å¦‚LibriMixï¼‰æ–¹é¢è¶…è¿‡äº†DiCoWã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.03723v1">PDF</a> </p>
<p><strong>Summary</strong>ï¼šé’ˆå¯¹å¤šè¯´è¯äººè¯­éŸ³è¯†åˆ«é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºè¯´è¯äººå±æ€§ï¼ˆSAï¼‰çš„Whisperæ¨¡å‹ï¼Œè¯¥æ¨¡å‹ç»“åˆäº†ç›®æ ‡è¯´è¯äººå»ºæ¨¡å’Œåºåˆ—åŒ–è¾“å‡ºè®­ç»ƒï¼ˆSOTï¼‰ã€‚é€šè¿‡ä½¿ç”¨Diarization-Conditioned Whisperï¼ˆDiCoWï¼‰ç¼–ç å™¨æå–ç›®æ ‡è¯´è¯äººçš„åµŒå…¥ä¿¡æ¯ï¼Œç„¶åå°†å®ƒä»¬ç»„åˆæˆä¸€ä¸ªå•ä¸€è¡¨ç¤ºå¹¶ä¼ é€’ç»™å…±äº«è§£ç å™¨ã€‚è¯¥æ¨¡å‹å¯å°†é‡å è¯­éŸ³è½¬å½•ä¸ºå¸¦æœ‰è¯´è¯äººæ ‡ç­¾å’Œæ—¶é—´æˆ³çš„åºåˆ—åŒ–è¾“å‡ºæµã€‚ä¸å•ç‹¬è§£ç æ¯ä¸ªè¯´è¯äººçš„ç›®æ ‡è¯´è¯äººASRç³»ç»Ÿï¼ˆå¦‚DiCoWï¼‰ç›¸æ¯”ï¼Œæˆ‘ä»¬çš„æ–¹æ³•é‡‡ç”¨è”åˆè§£ç ï¼Œå…è®¸è§£ç å™¨åŒæ—¶è€ƒè™‘æ‰€æœ‰è¯´è¯äººçš„ä¸Šä¸‹æ–‡ã€‚å®éªŒè¡¨æ˜ï¼Œè¯¥æ¨¡å‹ä¼˜äºç°æœ‰çš„åŸºäºSOTçš„æ–¹æ³•ï¼Œå¹¶åœ¨å¤šè¯´è¯äººæ··åˆï¼ˆä¾‹å¦‚LibriMixï¼‰æ–¹é¢è¶…è¿‡äº†DiCoWã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>æå‡ºäº†åŸºäºè¯´è¯äººå±æ€§ï¼ˆSAï¼‰çš„Whisperæ¨¡å‹ï¼Œç”¨äºå¤šè¯´è¯äººè¯­éŸ³è¯†åˆ«ã€‚</li>
<li>ç»“åˆäº†ç›®æ ‡è¯´è¯äººå»ºæ¨¡å’Œåºåˆ—åŒ–è¾“å‡ºè®­ç»ƒï¼ˆSOTï¼‰ã€‚</li>
<li>ä½¿ç”¨DiCoWç¼–ç å™¨æå–ç›®æ ‡è¯´è¯äººçš„åµŒå…¥ä¿¡æ¯ã€‚</li>
<li>æ¨¡å‹å°†é‡å è¯­éŸ³è½¬å½•ä¸ºå¸¦æœ‰è¯´è¯äººæ ‡ç­¾å’Œæ—¶é—´æˆ³çš„åºåˆ—åŒ–è¾“å‡ºã€‚</li>
<li>ç›¸æ¯”ç›®æ ‡è¯´è¯äººASRç³»ç»Ÿçš„å•ç‹¬è§£ç ï¼Œé‡‡ç”¨äº†è”åˆè§£ç æ–¹æ³•ã€‚</li>
<li>æ¨¡å‹åœ¨å¤šè¯´è¯äººæ··åˆæ–¹é¢çš„æ€§èƒ½è¶…è¿‡äº†ç°æœ‰æ–¹æ³•å’ŒDiCoWã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.03723">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-2199531bb49ee5c2e601f8376a2c74a1~resize:0:q75.jpg?source=1f5c5e47&expiration=1760043818&auth_key=1760043818-0-0-e559647ac046b8c1c1b0214e110d8d38&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-d03b000d9cdd13c177a2c92f7db2992a~resize:0:q75.jpg?source=1f5c5e47&expiration=1760043825&auth_key=1760043825-0-0-1e33e3a01469d9e15ccec868e1dfcc11&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-f320f479a50f5fc82bfcfeeed4f22b23~resize:0:q75.jpg?source=1f5c5e47&expiration=1760043831&auth_key=1760043831-0-0-11dc5c349fa74bb8001b24170abcf17c&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-ed0dc42ce470d6c0fc337c9457935add~resize:0:q75.jpg?source=1f5c5e47&expiration=1760043839&auth_key=1760043839-0-0-c51cb952e52e46553aa1089dc2e7668b&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-5e819bff4abae987ebaacafd6461feb1~resize:0:q75.jpg?source=1f5c5e47&expiration=1760043845&auth_key=1760043845-0-0-ad39b4b8c3b3e28f8f84b7cba1ec5b9b&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-ab04f2c62d48483683f27a0d7356c6aa~resize:0:q75.jpg?source=1f5c5e47&expiration=1760043852&auth_key=1760043852-0-0-9f16b3f4e3824915352465ba801c1122&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-edf65a29187af959b94b2554e0a6ef5b~resize:0:q75.jpg?source=1f5c5e47&expiration=1760043858&auth_key=1760043858-0-0-ee197c5aadf7f8e5e1537bd1b55e9cca&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="HiKE-Hierarchical-Evaluation-Framework-for-Korean-English-Code-Switching-Speech-Recognition"><a href="#HiKE-Hierarchical-Evaluation-Framework-for-Korean-English-Code-Switching-Speech-Recognition" class="headerlink" title="HiKE: Hierarchical Evaluation Framework for Korean-English   Code-Switching Speech Recognition"></a>HiKE: Hierarchical Evaluation Framework for Korean-English   Code-Switching Speech Recognition</h2><p><strong>Authors:Gio Paik, Yongbeom Kim, Soungmin Lee, Sangmin Ahn, Chanwoo Kim</strong></p>
<p>Despite advances in multilingual automatic speech recognition (ASR), code-switching (CS), the mixing of languages within an utterance common in daily speech, remains a severely underexplored challenge. In this paper, we introduce HiKE: the Hierarchical Korean-English code-switching benchmark, the first globally accessible evaluation framework for Korean-English CS, aiming to provide a means for the precise evaluation of multilingual ASR models and to foster research in the field. The proposed framework not only consists of high-quality, natural CS data across various topics, but also provides meticulous loanword labels and a hierarchical CS-level labeling scheme (word, phrase, and sentence) that together enable a systematic evaluation of a modelâ€™s ability to handle each distinct level of code-switching. Through evaluations of diverse multilingual ASR models and fine-tuning experiments, this paper demonstrates that although most multilingual ASR models initially exhibit inadequate CS-ASR performance, this capability can be enabled through fine-tuning with synthetic CS data. HiKE is available at <a target="_blank" rel="noopener" href="https://github.com/ThetaOne-AI/HiKE">https://github.com/ThetaOne-AI/HiKE</a> </p>
<blockquote>
<p>å°½ç®¡å¤šè¯­ç§è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰æœ‰æ‰€å‘å±•ï¼Œä½†è¯­è¨€åˆ‡æ¢ï¼ˆCSï¼‰ä»ç„¶æ˜¯ä¸€ä¸ªè¢«ä¸¥é‡å¿½è§†çš„æŒ‘æˆ˜ï¼Œåœ¨æ—¥å¸¸å¯¹è¯ä¸­ç»å¸¸å‡ºç°å°†ä¸åŒè¯­è¨€æ··åˆåœ¨ä¸€èµ·çš„ç°è±¡ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†HiKEï¼šåˆ†å±‚éŸ©è‹±åŒè¯­åˆ‡æ¢åŸºå‡†æµ‹è¯•ï¼Œè¿™æ˜¯é¦–ä¸ªå…¨çƒå¯è®¿é—®çš„éŸ©è‹±åŒè¯­åˆ‡æ¢è¯„ä¼°æ¡†æ¶ï¼Œæ—¨åœ¨æä¾›ä¸€ç§ç²¾ç¡®è¯„ä¼°å¤šè¯­ç§ASRæ¨¡å‹çš„æ‰‹æ®µï¼Œå¹¶æ¨åŠ¨è¯¥é¢†åŸŸçš„ç ”ç©¶ã€‚æ‰€æå‡ºçš„æ¡†æ¶ä¸ä»…åŒ…æ‹¬è·¨å„ç§ä¸»é¢˜çš„é«˜è´¨é‡ã€è‡ªç„¶åŒè¯­åˆ‡æ¢æ•°æ®ï¼Œè¿˜åŒ…æ‹¬ç»†è‡´çš„å¤–æ¥è¯æ ‡ç­¾å’Œåˆ†å±‚åŒè¯­åˆ‡æ¢çº§åˆ«æ ‡ç­¾æ–¹æ¡ˆï¼ˆå•è¯ã€çŸ­è¯­å’Œå¥å­ï¼‰ï¼Œè¿™äº›å…±åŒä½¿æ¨¡å‹èƒ½å¤Ÿç³»ç»Ÿåœ°å¤„ç†æ¯ä¸ªç‹¬ç‰¹çš„åŒè¯­åˆ‡æ¢çº§åˆ«èƒ½åŠ›è¿›è¡Œè¯„ä»·ã€‚é€šè¿‡å¯¹å„ç§å¤šè¯­ç§ASRæ¨¡å‹çš„è¯„ä¼°å’Œå¾®è°ƒå®éªŒï¼Œæœ¬æ–‡è¡¨æ˜ï¼Œå°½ç®¡å¤§å¤šæ•°å¤šè¯­ç§ASRæ¨¡å‹æœ€åˆè¡¨ç°å‡ºä¸è¶³çš„åŒè¯­åˆ‡æ¢è‡ªåŠ¨è¯­éŸ³è¯†åˆ«æ€§èƒ½ï¼Œä½†é€šè¿‡åˆ©ç”¨åˆæˆåŒè¯­åˆ‡æ¢æ•°æ®è¿›è¡Œå¾®è°ƒå¯ä»¥å¢å¼ºè¿™ä¸€åŠŸèƒ½ã€‚HiKEå¯è®¿é—®åœ°å€æ˜¯<a target="_blank" rel="noopener" href="https://github.com/ThetaOne-AI/HiKE%E3%80%82">https://github.com/ThetaOne-AI/HiKEã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.24613v2">PDF</a> Updated table 2 and 3 due to bug fix, Under Review</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†HiKEï¼šä¸€ä¸ªé¢å‘éŸ©è¯­-è‹±è¯­ä»£ç åˆ‡æ¢çš„å±‚æ¬¡æ€§è¯„ä»·æ¡†æ¶ã€‚è¯¥æ¡†æ¶æ—¨åœ¨ä¸ºç²¾ç¡®è¯„ä¼°å¤šè¯­è¨€è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰æ¨¡å‹æä¾›æ‰‹æ®µï¼Œå¹¶æ¨åŠ¨ç›¸å…³é¢†åŸŸçš„ç ”ç©¶ã€‚HiKEä¸ä»…åŒ…å«é«˜è´¨é‡çš„è‡ªç„¶ä»£ç åˆ‡æ¢æ•°æ®ï¼Œè¿˜æä¾›äº†è¯¦ç»†çš„å€Ÿè¯æ ‡ç­¾å’Œå±‚æ¬¡æ€§çš„ä»£ç åˆ‡æ¢çº§åˆ«æ ‡ç­¾æ–¹æ¡ˆï¼ˆå•è¯ã€çŸ­è¯­å’Œå¥å­ï¼‰ã€‚é€šè¿‡å¯¹å¤šç§å¤šè¯­è¨€ASRæ¨¡å‹çš„è¯„ä¼°ä»¥åŠå¾®è°ƒå®éªŒï¼Œè¯æ˜äº†è™½ç„¶å¤§å¤šæ•°å¤šè¯­è¨€ASRæ¨¡å‹åœ¨åˆå§‹é˜¶æ®µå¯¹ä»£ç åˆ‡æ¢çš„è¯†åˆ«æ€§èƒ½ä¸è¶³ï¼Œä½†é€šè¿‡åˆæˆä»£ç åˆ‡æ¢æ•°æ®çš„å¾®è°ƒå¯ä»¥æå‡å…¶æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ä»£ç åˆ‡æ¢ï¼ˆCSï¼‰åœ¨æ—¥å¸¸è¯­è¨€ä¸­æ™®éå­˜åœ¨ï¼Œä½†åœ¨å¤šè¯­è¨€è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰ä¸­ä»æ˜¯ä¸€ä¸ªè¢«ä¸¥é‡å¿½è§†çš„æŒ‘æˆ˜ã€‚</li>
<li>HiKEæ˜¯é¦–ä¸ªé¢å‘éŸ©è¯­-è‹±è¯­ä»£ç åˆ‡æ¢çš„è¯„ä»·æ¡†æ¶ï¼Œæ—¨åœ¨æä¾›ç²¾ç¡®è¯„ä¼°å¤šè¯­è¨€ASRæ¨¡å‹çš„æ‰‹æ®µã€‚</li>
<li>HiKEæ¡†æ¶åŒ…å«é«˜è´¨é‡çš„è‡ªç„¶ä»£ç åˆ‡æ¢æ•°æ®ï¼Œè¦†ç›–å„ç§ä¸»é¢˜ã€‚</li>
<li>HiKEæä¾›äº†è¯¦ç»†çš„å€Ÿè¯æ ‡ç­¾å’Œå±‚æ¬¡æ€§çš„ä»£ç åˆ‡æ¢çº§åˆ«æ ‡ç­¾æ–¹æ¡ˆï¼ŒåŒ…æ‹¬å•è¯ã€çŸ­è¯­å’Œå¥å­ã€‚</li>
<li>é€šè¿‡å¯¹å¤šç§å¤šè¯­è¨€ASRæ¨¡å‹çš„è¯„ä¼°ï¼Œå‘ç°å¤§å¤šæ•°æ¨¡å‹åœ¨åˆå§‹é˜¶æ®µå¯¹ä»£ç åˆ‡æ¢çš„è¯†åˆ«æ€§èƒ½ä¸è¶³ã€‚</li>
<li>é€šè¿‡ä½¿ç”¨åˆæˆä»£ç åˆ‡æ¢æ•°æ®å¯¹æ¨¡å‹è¿›è¡Œå¾®è°ƒï¼Œå¯ä»¥æé«˜æ¨¡å‹å¯¹ä»£ç åˆ‡æ¢çš„è¯†åˆ«æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.24613">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-76f0dc5497f64a2f87c7dd3b0480433b~resize:0:q75.jpg?source=1f5c5e47&expiration=1760043865&auth_key=1760043865-0-0-907bfd7d28a665659ef30e2f7fdd6024&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-a0abd65a3769c5d038480baa850fcbc4~resize:0:q75.jpg?source=1f5c5e47&expiration=1760043873&auth_key=1760043873-0-0-538240f7136ce640fb1de301d78b02c3&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-418afd003e08225c309f48fcdb719946~resize:0:q75.jpg?source=1f5c5e47&expiration=1760043879&auth_key=1760043879-0-0-9ffd6f170e9101046b1a3f87863e00bc&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-1cd3afc4f35a0609d308b1b6cd06e784~resize:0:q75.jpg?source=1f5c5e47&expiration=1760043886&auth_key=1760043886-0-0-c5fcde73d4c35d87f37f16d9400554d4&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-58399dc9ae28a4c33b915b6fc56ef742~resize:0:q75.jpg?source=1f5c5e47&expiration=1760043892&auth_key=1760043892-0-0-3af7bbd62fe52cb118f52bc1ddf76bda&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="Fun-ASR-Technical-Report"><a href="#Fun-ASR-Technical-Report" class="headerlink" title="Fun-ASR Technical Report"></a>Fun-ASR Technical Report</h2><p><strong>Authors:Keyu An, Yanni Chen, Chong Deng, Changfeng Gao, Zhifu Gao, Bo Gong, Xiangang Li, Yabin Li, Xiang Lv, Yunjie Ji, Yiheng Jiang, Bin Ma, Haoneng Luo, Chongjia Ni, Zexu Pan, Yiping Peng, Zhendong Peng, Peiyao Wang, Hao Wang, Wen Wang, Wupeng Wang, Biao Tian, Zhentao Tan, Nan Yang, Bin Yuan, Jieping Ye, Jixing Yu, Qinglin Zhang, Kun Zou, Han Zhao, Shengkui Zhao, Jingren Zhou</strong></p>
<p>In recent years, automatic speech recognition (ASR) has witnessed transformative advancements driven by three complementary paradigms: data scaling, model size scaling, and deep integration with large language models (LLMs). However, LLMs are prone to hallucination, which can significantly degrade user experience in real-world ASR applications. In this paper, we present Fun-ASR, a large-scale, LLM-based ASR system that synergistically combines massive data, large model capacity, LLM integration, and reinforcement learning to achieve state-of-the-art performance across diverse and complex speech recognition scenarios. Moreover, Fun-ASR is specifically optimized for practical deployment, with enhancements in streaming capability, noise robustness, code-switching, hotword customization, and satisfying other real-world application requirements. Experimental results show that while most LLM-based ASR systems achieve strong performance on open-source benchmarks, they often underperform on real industry evaluation sets. Thanks to production-oriented optimizations, Fun-ASR achieves state-of-the-art performance on real application datasets, demonstrating its effectiveness and robustness in practical settings. </p>
<blockquote>
<p>è¿‘å¹´æ¥ï¼Œè‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰åœ¨æ•°æ®è§„æ¨¡åŒ–ã€æ¨¡å‹è§„æ¨¡æ‰©å±•ä»¥åŠä¸å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ·±åº¦æ•´åˆä¸‰å¤§äº’è¡¥èŒƒå¼æ¨åŠ¨ä¸‹å–å¾—äº†é©å‘½æ€§çš„è¿›å±•ã€‚ç„¶è€Œï¼Œå¤§å‹è¯­è¨€æ¨¡å‹å®¹æ˜“å‡ºç°å¹»è§‰ï¼Œè¿™åœ¨ç°å®ä¸–ç•Œçš„ASRåº”ç”¨ä¸­å¯èƒ½ä¼šæ˜¾è‘—åœ°é™ä½ç”¨æˆ·ä½“éªŒã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†Fun-ASRï¼Œè¿™æ˜¯ä¸€ä¸ªåŸºäºå¤§å‹è¯­è¨€æ¨¡å‹çš„ASRç³»ç»Ÿï¼Œå®ƒååŒç»“åˆäº†å¤§è§„æ¨¡æ•°æ®ã€å¤§å‹æ¨¡å‹å®¹é‡ã€è¯­è¨€æ¨¡å‹é›†æˆå’Œå¼ºåŒ–å­¦ä¹ ï¼Œåœ¨å¤šæ ·ä¸”å¤æ‚çš„è¯­éŸ³è¯†åˆ«åœºæ™¯ä¸­å®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚æ­¤å¤–ï¼ŒFun-ASRé’ˆå¯¹å®é™…éƒ¨ç½²è¿›è¡Œäº†ä¸“é—¨ä¼˜åŒ–ï¼Œå¢å¼ºäº†æµå¼å¤„ç†åŠŸèƒ½ã€å™ªå£°é²æ£’æ€§ã€ä»£ç åˆ‡æ¢ã€çƒ­è¯è‡ªå®šä¹‰ä»¥åŠå…¶ä»–ç°å®ä¸–ç•Œåº”ç”¨çš„è¦æ±‚ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè™½ç„¶å¤§å¤šæ•°åŸºäºå¤§å‹è¯­è¨€æ¨¡å‹çš„ASRç³»ç»Ÿåœ¨å¼€æºåŸºå‡†æµ‹è¯•ä¸Šè¡¨ç°å¼ºåŠ²ï¼Œä½†å®ƒä»¬åœ¨çœŸå®çš„è¡Œä¸šè¯„ä¼°é›†ä¸Šå¾€å¾€è¡¨ç°ä¸ä½³ã€‚å¾—ç›Šäºé¢å‘ç”Ÿäº§çš„ä¼˜åŒ–ï¼ŒFun-ASRåœ¨çœŸå®åº”ç”¨æ•°æ®é›†ä¸Šå®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œè¯æ˜äº†å…¶åœ¨ç°å®ç¯å¢ƒä¸­çš„æœ‰æ•ˆæ€§å’Œé²æ£’æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.12508v3">PDF</a> Authors are listed in alphabetical order</p>
<p><strong>Summary</strong></p>
<p>è¿‘å¹´æ¥ï¼Œéšç€æ•°æ®è§„æ¨¡çš„æ‰©å¤§ã€æ¨¡å‹å®¹é‡çš„å¢é•¿å’Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ·±åº¦é›†æˆï¼Œè‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰å–å¾—äº†çªç ´æ€§è¿›å±•ã€‚ç„¶è€Œï¼ŒLLMæ˜“äº§ç”Ÿå¹»è§‰ï¼Œè¿™å¯èƒ½ä¼šæ˜¾è‘—å½±å“çœŸå®ä¸–ç•ŒASRåº”ç”¨çš„ç”¨æˆ·ä½“éªŒã€‚æœ¬æ–‡æå‡ºäº†Fun-ASRç³»ç»Ÿï¼Œè¿™æ˜¯ä¸€ä¸ªåŸºäºå¤§è§„æ¨¡LLMçš„ASRç³»ç»Ÿï¼Œé€šè¿‡ç»“åˆå¤§è§„æ¨¡æ•°æ®ã€æ¨¡å‹å®¹é‡ã€LLMé›†æˆå’Œå¼ºåŒ–å­¦ä¹ ï¼Œå®ç°äº†ä¸åŒå¤æ‚è¯­éŸ³è¯†åˆ«åœºæ™¯çš„å“è¶Šæ€§èƒ½ã€‚æ­¤å¤–ï¼ŒFun-ASRé’ˆå¯¹å®é™…éƒ¨ç½²è¿›è¡Œäº†ä¼˜åŒ–ï¼Œå¢å¼ºäº†æµå¤„ç†èƒ½åŠ›ã€å™ªå£°é²æ£’æ€§ã€ä»£ç åˆ‡æ¢ã€çƒ­è¯å®šåˆ¶ç­‰èƒ½åŠ›ï¼Œæ»¡è¶³å…¶ä»–çœŸå®ä¸–ç•Œåº”ç”¨çš„éœ€æ±‚ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè™½ç„¶å¤§å¤šæ•°åŸºäºLLMçš„ASRç³»ç»Ÿåœ¨å¼€æºåŸºå‡†æµ‹è¯•ä¸Šè¡¨ç°å¼ºåŠ²ï¼Œä½†åœ¨çœŸå®çš„è¡Œä¸šè¯„ä¼°é›†ä¸Šå¾€å¾€è¡¨ç°ä¸ä½³ã€‚å¾—ç›Šäºé¢å‘ç”Ÿäº§çš„ä¼˜åŒ–ï¼ŒFun-ASRåœ¨çœŸå®åº”ç”¨æ•°æ®é›†ä¸Šè¾¾åˆ°äº†æœ€æ–°æŠ€æœ¯æ°´å¹³ï¼Œè¯æ˜äº†å…¶åœ¨å®è·µç¯å¢ƒä¸­çš„æœ‰æ•ˆæ€§å’Œç¨³å¥æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰çš„è¿›å±•å¾—ç›Šäºæ•°æ®è§„æ¨¡ã€æ¨¡å‹å®¹é‡å’Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„é›†æˆã€‚</li>
<li>LLMæ˜“äº§ç”Ÿå¹»è§‰ï¼Œå½±å“çœŸå®ä¸–ç•ŒASRåº”ç”¨çš„ç”¨æˆ·ä½“éªŒã€‚</li>
<li>Fun-ASRç³»ç»Ÿæ˜¯ä¸€ä¸ªåŸºäºLLMçš„ASRç³»ç»Ÿï¼Œç»“åˆäº†å¤§è§„æ¨¡æ•°æ®ã€æ¨¡å‹å®¹é‡å’Œå¼ºåŒ–å­¦ä¹ ï¼Œå®ç°å“è¶Šæ€§èƒ½ã€‚</li>
<li>Fun-ASRé’ˆå¯¹å®é™…éƒ¨ç½²è¿›è¡Œäº†ä¼˜åŒ–ï¼ŒåŒ…æ‹¬æµå¤„ç†èƒ½åŠ›ã€å™ªå£°é²æ£’æ€§ã€ä»£ç åˆ‡æ¢ã€çƒ­è¯å®šåˆ¶ç­‰èƒ½åŠ›çš„æå‡ã€‚</li>
<li>å¤§å¤šæ•°åŸºäºLLMçš„ASRç³»ç»Ÿåœ¨å¼€æºåŸºå‡†æµ‹è¯•ä¸Šè¡¨ç°è‰¯å¥½ï¼Œä½†åœ¨çœŸå®è¡Œä¸šè¯„ä¼°é›†ä¸Šè¡¨ç°ä¸ä½³ã€‚</li>
<li>Fun-ASRåœ¨çœŸå®åº”ç”¨æ•°æ®é›†ä¸Šè¾¾åˆ°äº†æœ€æ–°æŠ€æœ¯æ°´å¹³ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.12508">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-c73e96c90d61ea9a43e002a6552e160b~resize:0:q75.jpg?source=1f5c5e47&expiration=1760043899&auth_key=1760043899-0-0-9c31032689572aff11a102900f4ba3d6&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-73d5efc03acccd51c062a631d55e3c88~resize:0:q75.jpg?source=1f5c5e47&expiration=1760043906&auth_key=1760043906-0-0-8c2b56529c1857f52a771978905bb681&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-38ded6572a631e2c2b1217d09dbacd7d~resize:0:q75.jpg?source=1f5c5e47&expiration=1760043912&auth_key=1760043912-0-0-0fdf5da4380e15f4dd7df3320824fb69&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-6964cceb713f79415a39cd4a8b279316~resize:0:q75.jpg?source=1f5c5e47&expiration=1760043919&auth_key=1760043919-0-0-3d89c508e2c439898dd2385c747ee357&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="RooseBERT-A-New-Deal-For-Political-Language-Modelling"><a href="#RooseBERT-A-New-Deal-For-Political-Language-Modelling" class="headerlink" title="RooseBERT: A New Deal For Political Language Modelling"></a>RooseBERT: A New Deal For Political Language Modelling</h2><p><strong>Authors:Deborah Dore, Elena Cabrio, Serena Villata</strong></p>
<p>The increasing amount of political debates and politics-related discussions calls for the definition of novel computational methods to automatically analyse such content with the final goal of lightening up political deliberation to citizens. However, the specificity of the political language and the argumentative form of these debates (employing hidden communication strategies and leveraging implicit arguments) make this task very challenging, even for current general-purpose pre-trained Language Models. To address this issue, we introduce a novel pre-trained Language Model for political discourse language called RooseBERT. Pre-training a language model on a specialised domain presents different technical and linguistic challenges, requiring extensive computational resources and large-scale data. RooseBERT has been trained on large political debate and speech corpora (8K debates, each composed of several sub-debates on different topics) in English. To evaluate its performances, we fine-tuned it on four downstream tasks related to political debate analysis, i.e., stance detection, sentiment analysis, argument component detection and classification, and argument relation prediction and classification. Our results demonstrate significant improvements over general-purpose Language Models on these four tasks, highlighting how domain-specific pre-training enhances performance in political debate analysis. We release RooseBERT for the research community. </p>
<blockquote>
<p>éšç€æ”¿æ²»è¾©è®ºå’Œæ”¿æ²»ç›¸å…³è®¨è®ºçš„æ—¥ç›Šå¢å¤šï¼Œéœ€è¦å®šä¹‰æ–°çš„è®¡ç®—æ–¹æ³•æ¥è‡ªåŠ¨åˆ†æè¿™äº›å†…å®¹ï¼Œæœ€ç»ˆç›®æ ‡æ˜¯ä¸ºå…¬æ°‘æä¾›æ”¿æ²»è®¨è®ºçš„æœºä¼šã€‚ç„¶è€Œï¼Œæ”¿æ²»è¯­è¨€çš„ç‰¹æ®Šæ€§å’Œè¾©è®ºçš„è®ºè¯å½¢å¼ï¼ˆé‡‡ç”¨éšè”½çš„æ²Ÿé€šç­–ç•¥å’Œåˆ©ç”¨éšå«çš„è®ºæ®ï¼‰ä½¿å¾—è¿™ä¸€ä»»åŠ¡éå¸¸å…·æœ‰æŒ‘æˆ˜æ€§ï¼Œå³ä½¿å¯¹äºå½“å‰çš„é€šç”¨é¢„è®­ç»ƒè¯­è¨€æ¨¡å‹ä¹Ÿæ˜¯å¦‚æ­¤ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§åä¸ºRooseBERTçš„æ”¿æ²»è¯è¯­è¯­è¨€é¢„è®­ç»ƒæ¨¡å‹ã€‚åœ¨ç‰¹å®šé¢†åŸŸä¸Šé¢„è®­ç»ƒè¯­è¨€æ¨¡å‹é¢ä¸´ç€ä¸åŒçš„æŠ€æœ¯å’Œè¯­è¨€æŒ‘æˆ˜ï¼Œéœ€è¦å·¨å¤§çš„è®¡ç®—èµ„æºå’Œå¤§è§„æ¨¡çš„æ•°æ®ã€‚RooseBERTæ˜¯åœ¨è‹±æ–‡çš„å¤§è§„æ¨¡æ”¿æ²»è¾©è®ºå’Œæ¼”è®²è¯­æ–™åº“ï¼ˆåŒ…å«8Kåœºè¾©è®ºï¼Œæ¯åœºè¾©è®ºç”±ä¸åŒä¸»é¢˜çš„å¤šä¸ªå­è¾©è®ºç»„æˆï¼‰ä¸Šè®­ç»ƒçš„ã€‚ä¸ºäº†è¯„ä¼°å…¶æ€§èƒ½ï¼Œæˆ‘ä»¬åœ¨ä¸æ”¿æ²»è¾©è®ºåˆ†æç›¸å…³çš„å››ä¸ªä¸‹æ¸¸ä»»åŠ¡ä¸Šè¿›è¡Œäº†å¾®è°ƒï¼Œå³ç«‹åœºæ£€æµ‹ã€æƒ…æ„Ÿåˆ†æã€è®ºè¯æˆåˆ†æ£€æµ‹å’Œåˆ†ç±»ä»¥åŠè®ºè¯å…³ç³»é¢„æµ‹å’Œåˆ†ç±»ã€‚æˆ‘ä»¬çš„ç»“æœåœ¨è¿™å››ä¸ªä»»åŠ¡ä¸Šæ˜¾è‘—ä¼˜äºé€šç”¨è¯­è¨€æ¨¡å‹ï¼Œè¯æ˜äº†é¢†åŸŸç‰¹å®šé¢„è®­ç»ƒåœ¨æ”¿æ²»è¾©è®ºåˆ†æä¸­çš„æ€§èƒ½æå‡ã€‚æˆ‘ä»¬å‘å¸ƒRooseBERTä»¥ä¾›ç ”ç©¶ç•Œä½¿ç”¨ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.03250v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>ä¸€ç§æ–°çš„é’ˆå¯¹æ”¿æ²»è¯è¯­çš„é¢„è®­ç»ƒè¯­è¨€æ¨¡å‹â€”â€”RooseBERTè¢«æå‡ºå¹¶ä»‹ç»ã€‚è¯¥æ¨¡å‹åœ¨å¤§é‡æ”¿æ²»è¾©è®ºå’Œæ¼”è®²è¯­æ–™åº“ä¸Šè¿›è¡Œè®­ç»ƒï¼Œé€‚ç”¨äºæ”¿æ²»å†…å®¹è‡ªåŠ¨åˆ†æï¼Œæœ‰åŠ©äºå‡è½»å…¬æ°‘çš„æ”¿æ²»è®¨è®ºè´Ÿæ‹…ã€‚RooseBERTåœ¨å››ä¸ªä¸æ”¿æ²»è¾©è®ºåˆ†æç›¸å…³çš„ä¸‹æ¸¸ä»»åŠ¡ä¸Šçš„è¡¨ç°å¾—åˆ°äº†æ˜¾è‘—çš„æå‡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ”¿æ²»è¾©è®ºå’Œè®¨è®ºå†…å®¹çš„è‡ªåŠ¨åˆ†æéœ€è¦æ–°çš„è®¡ç®—æ–¹æ³•ã€‚</li>
<li>RooseBERTæ˜¯ä¸€ç§é’ˆå¯¹æ”¿æ²»è¯è¯­çš„é¢„è®­ç»ƒè¯­è¨€æ¨¡å‹ã€‚</li>
<li>RooseBERTåœ¨å¤§é‡æ”¿æ²»è¾©è®ºå’Œæ¼”è®²è¯­æ–™åº“ä¸Šè¿›è¡Œè®­ç»ƒã€‚</li>
<li>RooseBERTåœ¨å››ä¸ªä¸‹æ¸¸ä»»åŠ¡ä¸Šçš„è¡¨ç°ä¼˜äºé€šç”¨è¯­è¨€æ¨¡å‹ã€‚</li>
<li>RooseBERTåŒ…æ‹¬ç«‹åœºæ£€æµ‹ã€æƒ…æ„Ÿåˆ†æã€è®ºè¯æˆåˆ†æ£€æµ‹å’Œåˆ†ç±»ä»¥åŠè®ºè¯å…³ç³»é¢„æµ‹å’Œåˆ†ç±»ç­‰ä»»åŠ¡ã€‚</li>
<li>æ”¿æ²»è¯­è¨€çš„ç‰¹å¼‚æ€§å’Œè¾©è®ºä¸­çš„éšæ€§æ²Ÿé€šç­–ç•¥ä½¿è¯¥ä»»åŠ¡å…·æœ‰æŒ‘æˆ˜æ€§ã€‚</li>
<li>RooseBERTçš„å‘å¸ƒæ—¨åœ¨ä¸ºç ”ç©¶é¢†åŸŸæä¾›åŠ©åŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.03250">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-18a91354279ca7374c007c880f2e905a~resize:0:q75.jpg?source=1f5c5e47&expiration=1760043926&auth_key=1760043926-0-0-8a657311a26d6f84d6abedbd8e0c6d75&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-65660007887b0773df7b731aa1b47416~resize:0:q75.jpg?source=1f5c5e47&expiration=1760043933&auth_key=1760043933-0-0-6ae594cbb96f8e425c5d1a0013a7c6c1&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-a9fb4d621fbd8b5127a6ebfe6b19f01b~resize:0:q75.jpg?source=1f5c5e47&expiration=1760043939&auth_key=1760043939-0-0-ee77eb8e590856d3d03a6747fbb4cef5&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-ea618e9a1526fd22ac87db7a67423b4e~resize:0:q75.jpg?source=1f5c5e47&expiration=1760043946&auth_key=1760043946-0-0-0349156094bcca61b34a34928ab6afc1&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1><h2 id="Context-Biasing-for-Pronunciations-Orthography-Mismatch-in-Automatic-Speech-Recognition"><a href="#Context-Biasing-for-Pronunciations-Orthography-Mismatch-in-Automatic-Speech-Recognition" class="headerlink" title="Context Biasing for Pronunciations-Orthography Mismatch in Automatic   Speech Recognition"></a>Context Biasing for Pronunciations-Orthography Mismatch in Automatic   Speech Recognition</h2><p><strong>Authors:Christian Huber, Alexander Waibel</strong></p>
<p>Neural sequence-to-sequence systems deliver state-of-the-art performance for automatic speech recognition. When using appropriate modeling units, e.g., byte-pair encoded characters, these systems are in principal open vocabulary systems. In practice, however, they often fail to recognize words not seen during training, e.g., named entities, acronyms, or domain-specific special words. To address this problem, many context biasing methods have been proposed; however, for words with a pronunciation-orthography mismatch, these methods may still struggle. We propose a method which allows corrections of substitution errors to improve the recognition accuracy of such challenging words. Users can add corrections on the fly during inference. We show that with this method we get a relative improvement in biased word error rate of up to 8%, while maintaining a competitive overall word error rate. </p>
<blockquote>
<p>ç¥ç»ç½‘ç»œåºåˆ—åˆ°åºåˆ—ç³»ç»Ÿåœ¨è‡ªåŠ¨è¯­éŸ³è¯†åˆ«æ–¹é¢è¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚åœ¨ä½¿ç”¨é€‚å½“çš„å»ºæ¨¡å•å…ƒï¼ˆä¾‹å¦‚ï¼Œå­—èŠ‚å¯¹ç¼–ç å­—ç¬¦ï¼‰æ—¶ï¼Œè¿™äº›ç³»ç»Ÿåœ¨åŸåˆ™ä¸Šéƒ½æ˜¯å¼€æ”¾è¯æ±‡ç³»ç»Ÿã€‚ç„¶è€Œï¼Œåœ¨å®è·µä¸­ï¼Œå®ƒä»¬å¾€å¾€æ— æ³•è¯†åˆ«åœ¨è®­ç»ƒæœŸé—´æœªè§è¿‡çš„å•è¯ï¼Œä¾‹å¦‚ï¼Œå‘½åå®ä½“ã€ç¼©å†™æˆ–ç‰¹å®šé¢†åŸŸçš„ç‰¹æ®Šè¯æ±‡ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œå·²ç»æå‡ºäº†è®¸å¤šä¸Šä¸‹æ–‡åå‘æ–¹æ³•ï¼›ä½†å¯¹äºå‘éŸ³å’Œä¹¦å†™ä¸åŒ¹é…çš„å•å­—ï¼Œè¿™äº›æ–¹æ³•å¯èƒ½ä»ç„¶é¢ä¸´æŒ‘æˆ˜ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–¹æ³•ï¼Œå¯ä»¥çº æ­£æ›¿æ¢é”™è¯¯ï¼Œä»¥æé«˜æ­¤ç±»å…·æœ‰æŒ‘æˆ˜æ€§å•è¯çš„è¯†åˆ«å‡†ç¡®æ€§ã€‚ç”¨æˆ·å¯ä»¥åœ¨æ¨ç†è¿‡ç¨‹ä¸­å³æ—¶æ·»åŠ æ›´æ­£ã€‚æˆ‘ä»¬è¯æ˜ï¼Œä½¿ç”¨è¿™ç§æ–¹æ³•ï¼Œå¯ä»¥åœ¨ä¿æŒæ€»ä½“å•è¯é”™è¯¯ç‡ç«äº‰åŠ›çš„åŒæ—¶ï¼Œå°†åå‘å•è¯é”™è¯¯ç‡æé«˜é«˜è¾¾8%ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.18703v2">PDF</a> </p>
<p><strong>Summary</strong>ï¼šç¥ç»åºåˆ—åˆ°åºåˆ—ç³»ç»Ÿåœ¨è‡ªåŠ¨è¯­éŸ³è¯†åˆ«é¢†åŸŸå…·æœ‰å“è¶Šæ€§èƒ½ã€‚é€šè¿‡ä½¿ç”¨é€‚å½“çš„å»ºæ¨¡å•å…ƒï¼Œå¦‚å­—èŠ‚å¯¹ç¼–ç å­—ç¬¦ï¼Œè¿™äº›ç³»ç»ŸåŸåˆ™ä¸Šå…·æœ‰å¼€æ”¾è¯æ±‡ã€‚ä½†åœ¨å®è·µä¸­ï¼Œå®ƒä»¬å¾€å¾€æ— æ³•è¯†åˆ«è®­ç»ƒæœŸé—´æœªè§è¿‡çš„è¯æ±‡ï¼Œå¦‚å®ä½“åç§°ã€ç¼©å†™æˆ–ç‰¹å®šé¢†åŸŸçš„ç‰¹æ®Šè¯æ±‡ã€‚é’ˆå¯¹è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§å…è®¸çº æ­£æ›¿æ¢é”™è¯¯çš„æ–¹æ³•ï¼Œä»¥æé«˜è¿™äº›å…·æœ‰æŒ‘æˆ˜æ€§çš„å•è¯çš„è¯†åˆ«å‡†ç¡®ç‡ã€‚ç”¨æˆ·å¯ä»¥åœ¨æ¨ç†è¿‡ç¨‹ä¸­å³æ—¶æ·»åŠ æ›´æ­£ã€‚æˆ‘ä»¬æ˜¾ç¤ºä½¿ç”¨æ­¤æ–¹æ³•å¯ä½¿å…·æœ‰åå·®çš„å•è¯é”™è¯¯ç‡ç›¸å¯¹æé«˜è¾¾8%ï¼ŒåŒæ—¶ä¿æŒæ•´ä½“å•è¯é”™è¯¯ç‡çš„ç«äº‰åŠ›ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>ç¥ç»åºåˆ—åˆ°åºåˆ—ç³»ç»Ÿåœ¨è‡ªåŠ¨è¯­éŸ³è¯†åˆ«é¢†åŸŸè¡¨ç°å“è¶Šï¼Œä½¿ç”¨é€‚å½“çš„å»ºæ¨¡å•å…ƒå…·æœ‰å¼€æ”¾è¯æ±‡ç‰¹ç‚¹ã€‚</li>
<li>è¿™äº›ç³»ç»Ÿåœ¨è¯†åˆ«è®­ç»ƒæœŸé—´æœªè§è¯æ±‡æ–¹é¢å­˜åœ¨æŒ‘æˆ˜ï¼Œå¦‚å‘½åå®ä½“ã€ç¼©å†™å’Œç‰¹å®šé¢†åŸŸçš„ç‰¹æ®Šè¯æ±‡ã€‚</li>
<li>é’ˆå¯¹å…·æœ‰æŒ‘æˆ˜æ€§çš„å•è¯è¯†åˆ«é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§å…è®¸ç”¨æˆ·å³æ—¶æ·»åŠ æ›´æ­£çš„æ–¹æ³•æ¥è§£å†³æ›¿æ¢é”™è¯¯ã€‚</li>
<li>è¯¥æ–¹æ³•å¯åœ¨ä¿æŒæ•´ä½“å•è¯é”™è¯¯ç‡ç«äº‰åŠ›çš„åŒæ—¶ï¼Œæé«˜å…·æœ‰åå·®çš„å•è¯é”™è¯¯ç‡è¾¾8%ã€‚</li>
<li>è¯¥æ–¹æ³•å¢å¼ºäº†ç¥ç»åºåˆ—åˆ°åºåˆ—ç³»ç»Ÿçš„çµæ´»æ€§å’Œé€‚åº”æ€§ï¼Œä½¿å…¶æ›´å¥½åœ°å¤„ç†å¤æ‚çš„è¯­éŸ³è¯†åˆ«ä»»åŠ¡ã€‚</li>
<li>ç”¨æˆ·å‚ä¸æ›´æ­£è¿‡ç¨‹æœ‰åŠ©äºæé«˜ç³»ç»Ÿçš„å®ç”¨æ€§å’Œç”¨æˆ·æ»¡æ„åº¦ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.18703">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-71f9eb8cff3fecdcaafee16fcd5eac60~resize:0:q75.jpg?source=1f5c5e47&expiration=1760043953&auth_key=1760043953-0-0-b258067d4cd2dad418b60104ae009e9a&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-ef234b7d871a61911752f606692b807b~resize:0:q75.jpg?source=1f5c5e47&expiration=1760043961&auth_key=1760043961-0-0-39593560a4828d2f5effebd6e9391c62&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-19"><a href="#-19" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-10-10/Speech/">https://kedreamix.github.io/Talk2Paper/Paper/2025-10-10/Speech/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Speech/">
                                    <span class="chip bg-color">Speech</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-10-10/Face%20Swapping/">
                    <div class="card-image">
                        
                        <img src="https://pic-private.zhihu.com/v2-45810d09557cc88586579a49aefb5ef9~resize:0:q75.jpg?source=1f5c5e47&expiration=1760044523&auth_key=1760044523-0-0-b97931a7993fac155a158394a0cd4e4a&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" class="responsive-img" alt="Face Swapping">
                        
                        <span class="card-title">Face Swapping</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Face Swapping æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-10-10  Unmasking Puppeteers Leveraging Biometric Leakage to Disarm   Impersonation in AI-based Videoconferencing
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-10-10
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Face-Swapping/" class="post-category">
                                    Face Swapping
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Face-Swapping/">
                        <span class="chip bg-color">Face Swapping</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-10-10/%E6%97%A0%E7%9B%91%E7%9D%A3_%E5%8D%8A%E7%9B%91%E7%9D%A3_%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0/">
                    <div class="card-image">
                        
                        <img src="https://pic-private.zhihu.com/v2-9fe1b96dd5125dfe8097a60a3d3efe4e~resize:0:q75.jpg?source=1f5c5e47&expiration=1760041980&auth_key=1760041980-0-0-d8862cd40932120bbe80eeeee05baeef&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" class="responsive-img" alt="æ— ç›‘ç£/åŠç›‘ç£/å¯¹æ¯”å­¦ä¹ ">
                        
                        <span class="card-title">æ— ç›‘ç£/åŠç›‘ç£/å¯¹æ¯”å­¦ä¹ </span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            æ— ç›‘ç£/åŠç›‘ç£/å¯¹æ¯”å­¦ä¹  æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-10-10  The best performance in the CARE 2025 -- Liver Task (LiSeg-Contrast)   Contrast-Aware Semi-Supervised Segmentation with Domain Generalization and   Test-Time Adaptation
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-10-10
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E6%97%A0%E7%9B%91%E7%9D%A3-%E5%8D%8A%E7%9B%91%E7%9D%A3-%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0/" class="post-category">
                                    æ— ç›‘ç£/åŠç›‘ç£/å¯¹æ¯”å­¦ä¹ 
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E6%97%A0%E7%9B%91%E7%9D%A3-%E5%8D%8A%E7%9B%91%E7%9D%A3-%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0/">
                        <span class="chip bg-color">æ— ç›‘ç£/åŠç›‘ç£/å¯¹æ¯”å­¦ä¹ </span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">30341.4k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
