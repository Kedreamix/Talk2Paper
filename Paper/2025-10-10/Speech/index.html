<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Speech">
    <meta name="description" content="Speech 方向最新论文已更新，请持续关注 Update in 2025-10-10  How much speech data is necessary for ASR in African languages? An   evaluation of data scaling in Kinyarwanda and Kikuyu">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Speech | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loader页面消失采用渐隐的方式*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logo出现动画 */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//使用渐隐的方法淡出loading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//强制显示loading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">嘘~  正在从服务器偷取页面 . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>首页</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>标签</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>分类</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>归档</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="搜索" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="深色/浅色模式" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			首页
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			标签
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			分类
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			归档
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pic-private.zhihu.com/v2-4941265dac737ba27e75f33086bdd192~resize:0:q75.jpg?source=1f5c5e47&expiration=1760043589&auth_key=1760043589-0-0-790797aab8a427a09316c948b97f05fb&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Speech</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- 文章内容详情 -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Speech/">
                                <span class="chip bg-color">Speech</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Speech/" class="post-category">
                                Speech
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp;
                    2025-10-10
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>更新日期:&nbsp;&nbsp;
                    2025-10-11
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>文章字数:&nbsp;&nbsp;
                    18.4k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>阅读时长:&nbsp;&nbsp;
                    74 分
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>阅读次数:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>⚠️ 以下所有内容总结都来自于 大语言模型的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p>
</blockquote>
<h1 id="2025-10-10-更新"><a href="#2025-10-10-更新" class="headerlink" title="2025-10-10 更新"></a>2025-10-10 更新</h1><h2 id="How-much-speech-data-is-necessary-for-ASR-in-African-languages-An-evaluation-of-data-scaling-in-Kinyarwanda-and-Kikuyu"><a href="#How-much-speech-data-is-necessary-for-ASR-in-African-languages-An-evaluation-of-data-scaling-in-Kinyarwanda-and-Kikuyu" class="headerlink" title="How much speech data is necessary for ASR in African languages? An   evaluation of data scaling in Kinyarwanda and Kikuyu"></a>How much speech data is necessary for ASR in African languages? An   evaluation of data scaling in Kinyarwanda and Kikuyu</h2><p><strong>Authors:Benjamin Akera, Evelyn Nafula, Patrick Walukagga, Gilbert Yiga, John Quinn, Ernest Mwebaze</strong></p>
<p>The development of Automatic Speech Recognition (ASR) systems for low-resource African languages remains challenging due to limited transcribed speech data. While recent advances in large multilingual models like OpenAI’s Whisper offer promising pathways for low-resource ASR development, critical questions persist regarding practical deployment requirements. This paper addresses two fundamental concerns for practitioners: determining the minimum data volumes needed for viable performance and characterizing the primary failure modes that emerge in production systems. We evaluate Whisper’s performance through comprehensive experiments on two Bantu languages: systematic data scaling analysis on Kinyarwanda using training sets from 1 to 1,400 hours, and detailed error characterization on Kikuyu using 270 hours of training data. Our scaling experiments demonstrate that practical ASR performance (WER &lt; 13%) becomes achievable with as little as 50 hours of training data, with substantial improvements continuing through 200 hours (WER &lt; 10%). Complementing these volume-focused findings, our error analysis reveals that data quality issues, particularly noisy ground truth transcriptions, account for 38.6% of high-error cases, indicating that careful data curation is as critical as data volume for robust system performance. These results provide actionable benchmarks and deployment guidance for teams developing ASR systems across similar low-resource language contexts. We release accompanying and models see <a target="_blank" rel="noopener" href="https://github.com/SunbirdAI/kinyarwanda-whisper-eval">https://github.com/SunbirdAI/kinyarwanda-whisper-eval</a> </p>
<blockquote>
<p>对于非洲的低资源语言来说，由于语音转录数据的有限性，自动语音识别（ASR）系统的开发仍然具有挑战性。虽然最近的大型多语言模型（如OpenAI的Whisper）为低资源ASR开发提供了有前景的路径，但关于实际部署要求的关键问题仍然存在。这篇论文解决了从业者面临的两个基本问题：确定实现可行性能所需的最小数据量，以及刻画生产系统中出现的主要故障模式。我们通过两种班图语（Kinyarwanda和Kikuyu）的全面实验来评估Whisper的性能：针对Kinyarwanda进行系统的数据规模分析，使用从1小时到1400小时的培训数据集；针对Kikuyu进行详细的误差特征分析，使用270小时的培训数据。我们的规模实验表明，使用仅50小时的训练数据即可实现实用的ASR性能（WER &lt; 13%），并且在前200小时内持续获得实质性改进（WER &lt; 10%）。这些体积集中发现的补充，我们的误差分析表明，数据质量问题，尤其是真实地面文本的嘈杂性，占高误差案例的38.6%，这表明仔细的数据整理与数据量对于系统的稳健性能同样关键。这些结果提供了可操作的基准和部署指导，可为开发类似低资源语言环境中的ASR系统的团队提供帮助。我们发布相关模型和详细信息，请参见 <a target="_blank" rel="noopener" href="https://github.com/SunbirdAI/kinyarwanda-whisper-eval">https://github.com/SunbirdAI/kinyarwanda-whisper-eval</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.07221v1">PDF</a> </p>
<p><strong>摘要</strong><br>在非洲低资源语言环境下，自动语音识别（ASR）系统的开发仍然面临挑战，主要问题在于缺乏转录语音数据。虽然大型多语言模型（如OpenAI的Whisper）为该问题提供了潜在解决方案，但在实际应用部署方面仍存在关键疑问。本文致力于解决从业者的两大核心关切：确定实现可行性能所需的最小数据量以及识别生产系统中出现的主要故障模式。我们通过对两种班图语系语言的实验评估了Whisper的性能：针对基尼亚卢旺达语进行了系统的数据规模分析，使用了从1小时到1400小时不等的训练数据集；针对基库尤语，则使用270小时的培训数据进行了详细的误差特征分析。我们的规模实验表明，只需50小时的培训数据即可实现实用的ASR性能（WER &lt; 13%），并且在200小时内持续取得实质性改进（WER &lt; 10%）。除了对数据量进行关注外，我们的误差分析还揭示，数据质量问题，特别是真实转录中的噪声问题占到了高误差案例的38.6%，这表明对于稳健的系统性能而言，数据的仔细筛选与数据量同样重要。这些结果提供了可操作的基准标准和部署指南，对类似低资源语言背景下开发ASR系统的团队具有重要意义。相关研究模型已在SunbirdAI发布的kinyarwanda-whisper-eval中获得共享与体现。 </p>
<p><strong>关键见解</strong></p>
<ol>
<li>低资源非洲语言的ASR系统开发面临挑战，主要是由于缺乏转录语音数据。</li>
<li>OpenAI的Whisper等大型多语言模型为低资源ASR开发提供了希望。</li>
<li>确定实现可行性能所需的最小数据量是关键问题之一。</li>
<li>研究发现仅需少量训练数据（如50小时）即可实现良好的ASR性能。</li>
<li>数据质量问题对ASR性能的影响不容忽视，特别是在真实转录中的噪声问题。</li>
<li>仔细筛选数据对于实现稳健的系统性能至关重要。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.07221">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic-private.zhihu.com/v2-985a3eb044be097b7622cd8c7cfc59e6~resize:0:q75.jpg?source=1f5c5e47&expiration=1760043352&auth_key=1760043352-0-0-0ae1fedf731c051d9cb4292696a2961d&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-08101d51ad83a79b369dbaedac779d99~resize:0:q75.jpg?source=1f5c5e47&expiration=1760043360&auth_key=1760043360-0-0-7031e58175a4c3ecdeaeec05f6cea5d1&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-e4a0a6c1fa5ec07fbcfc0134cf462276~resize:0:q75.jpg?source=1f5c5e47&expiration=1760043366&auth_key=1760043366-0-0-d7bfd3710a91b28d71526f2ed55f73fa&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-8c9caa274d138bd0795d0aaaf8dbe51b~resize:0:q75.jpg?source=1f5c5e47&expiration=1760043372&auth_key=1760043372-0-0-255436406e3ed27857b02d8b78c42bed&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-e2c8ae008d782562d1c33a251b7fd5c0~resize:0:q75.jpg?source=1f5c5e47&expiration=1760043379&auth_key=1760043379-0-0-29dc4781f62ca630e225db420852cc79&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Making-Machines-Sound-Sarcastic-LLM-Enhanced-and-Retrieval-Guided-Sarcastic-Speech-Synthesis"><a href="#Making-Machines-Sound-Sarcastic-LLM-Enhanced-and-Retrieval-Guided-Sarcastic-Speech-Synthesis" class="headerlink" title="Making Machines Sound Sarcastic: LLM-Enhanced and Retrieval-Guided   Sarcastic Speech Synthesis"></a>Making Machines Sound Sarcastic: LLM-Enhanced and Retrieval-Guided   Sarcastic Speech Synthesis</h2><p><strong>Authors:Zhu Li, Yuqing Zhang, Xiyuan Gao, Shekhar Nayak, Matt Coler</strong></p>
<p>Sarcasm is a subtle form of non-literal language that poses significant challenges for speech synthesis due to its reliance on nuanced semantic, contextual, and prosodic cues. While existing speech synthesis research has focused primarily on broad emotional categories, sarcasm remains largely unexplored. In this paper, we propose a Large Language Model (LLM)-enhanced Retrieval-Augmented framework for sarcasm-aware speech synthesis. Our approach combines (1) semantic embeddings from a LoRA-fine-tuned LLaMA 3, which capture pragmatic incongruity and discourse-level cues of sarcasm, and (2) prosodic exemplars retrieved via a Retrieval Augmented Generation (RAG) module, which provide expressive reference patterns of sarcastic delivery. Integrated within a VITS backbone, this dual conditioning enables more natural and contextually appropriate sarcastic speech. Experiments demonstrate that our method outperforms baselines in both objective measures and subjective evaluations, yielding improvements in speech naturalness, sarcastic expressivity, and downstream sarcasm detection. </p>
<blockquote>
<p>讽刺是一种微妙的非字面语言形式，由于其依赖于微妙的语义、上下文和韵律线索，对语音合成构成了重大挑战。虽然现有的语音合成研究主要集中在广泛的情感类别上，但讽刺在很大程度上仍未被探索。在本文中，我们提出了一种结合大型语言模型（LLM）的检索增强框架，用于讽刺性语音合成。我们的方法结合了（1）来自LoRA微调LLaMA 3的语义嵌入，捕捉实用上的不一致和话语层面的讽刺线索；（2）通过检索增强生成（RAG）模块检索到的韵律范例，为讽刺性表达提供了表达性参考模式。在VITS主干中集成，这种双重条件使得更自然、上下文更恰当的讽刺性语音成为可能。实验表明，我们的方法在客观指标和主观评估上均优于基线，在语音自然度、讽刺表现力和下游讽刺检测方面取得了改进。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.07096v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文探讨了讽刺作为一种非字面语言在语音合成中的挑战，并提出了一种结合大型语言模型（LLM）的检索增强框架来解决这一问题。该研究通过语义嵌入和语音韵律范例的整合，提高了讽刺语音的自然度和语境适宜性。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>讽刺是一种非字面语言，对语音合成构成挑战，需捕捉语义、语境和韵律线索。</li>
<li>现有语音合成研究主要关注广泛情绪类别，而讽刺尚未得到充分探索。</li>
<li>提出结合大型语言模型（LLM）的检索增强框架，用于讽刺意识语音合成。</li>
<li>研究通过LoRA微调LLaMA 3获取语义嵌入，捕捉讽刺的语用不一致和篇章级线索。</li>
<li>借助检索增强生成（RAG）模块检索韵律范例，提供讽刺表达的参考模式。</li>
<li>在VITS背景下集成上述双重条件，使讽刺语音更自然、语境更适宜。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.07096">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic-private.zhihu.com/v2-3531789f38443b66779af1b0c5513ba8~resize:0:q75.jpg?source=1f5c5e47&expiration=1760043387&auth_key=1760043387-0-0-79ba3c7a34a8ae5e32706f7f60b5250d&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-c981097f21d942ca619f24ad33241a35~resize:0:q75.jpg?source=1f5c5e47&expiration=1760043394&auth_key=1760043394-0-0-9c5baa82b8be70f4ebb96a40df60f2ef&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-da4d41c2cba87c4027e47f419abf70da~resize:0:q75.jpg?source=1f5c5e47&expiration=1760043401&auth_key=1760043401-0-0-5fcf45cf7579fef50ec6089ef6b32204&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-90e0da9f7b33347996d2e928fd276e9d~resize:0:q75.jpg?source=1f5c5e47&expiration=1760043407&auth_key=1760043407-0-0-6c054c7c97173b6e21c167c91c710ac0&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Enhancing-Speech-Emotion-Recognition-via-Fine-Tuning-Pre-Trained-Models-and-Hyper-Parameter-Optimisation"><a href="#Enhancing-Speech-Emotion-Recognition-via-Fine-Tuning-Pre-Trained-Models-and-Hyper-Parameter-Optimisation" class="headerlink" title="Enhancing Speech Emotion Recognition via Fine-Tuning Pre-Trained Models   and Hyper-Parameter Optimisation"></a>Enhancing Speech Emotion Recognition via Fine-Tuning Pre-Trained Models   and Hyper-Parameter Optimisation</h2><p><strong>Authors:Aryan Golbaghi, Shuo Zhou</strong></p>
<p>We propose a workflow for speech emotion recognition (SER) that combines pre-trained representations with automated hyperparameter optimisation (HPO). Using SpeechBrain wav2vec2-base model fine-tuned on IEMOCAP as the encoder, we compare two HPO strategies, Gaussian Process Bayesian Optimisation (GP-BO) and Tree-structured Parzen Estimators (TPE), under an identical four-dimensional search space and 15-trial budget, with balanced class accuracy (BCA) on the German EmoDB corpus as the objective. All experiments run on 8 CPU cores with 32 GB RAM. GP-BO achieves 0.96 BCA in 11 minutes, and TPE (Hyperopt implementation) attains 0.97 in 15 minutes. In contrast, grid search requires 143 trials and 1,680 minutes to exceed 0.9 BCA, and the best AutoSpeech 2020 baseline reports only 0.85 in 30 minutes on GPU. For cross-lingual generalisation, an EmoDB-trained HPO-tuned model improves zero-shot accuracy by 0.25 on CREMA-D and 0.26 on RAVDESS. Results show that efficient HPO with pre-trained encoders delivers competitive SER on commodity CPUs. Source code to this work is available at: <a target="_blank" rel="noopener" href="https://github.com/youngaryan/speechbrain-emotion-hpo">https://github.com/youngaryan/speechbrain-emotion-hpo</a>. </p>
<blockquote>
<p>我们提出了一种结合预训练表示和自动化超参数优化（HPO）的语音情感识别（SER）工作流程。使用在IEMOCAP上微调过的SpeechBrain wav2vec2-base模型作为编码器，我们在相同的四维搜索空间和15次试验预算下，比较了高斯过程贝叶斯优化（GP-BO）和树形帕曾估计器（TPE）这两种HPO策略，以德国EmoDB语料库上的平衡类准确率（BCA）作为目标。所有实验均在8个CPU核心和32GB RAM上进行。GP-BO在11分钟内达到0.96的BCA，而TPE（Hyperopt实现）在15分钟内达到0.97。相比之下，网格搜索需要143次试验和1680分钟才能达到超过0.9的BCA，而最佳的AutoSpeech 2020基准报告仅在GPU上运行30分钟，准确率为0.85。对于跨语言泛化，使用EmoDB训练的HPO调整模型在CREMA-D和RAVDESS上的零样本准确率分别提高了0.25和0.26。结果表明，使用预训练编码器的有效HPO在商品CPU上提供了具有竞争力的SER性能。该工作的源代码可在以下网址找到：<a target="_blank" rel="noopener" href="https://github.com/youngaryan/speechbrain-emotion-hpo%E3%80%82">https://github.com/youngaryan/speechbrain-emotion-hpo。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.07052v1">PDF</a> </p>
<p><strong>摘要</strong><br>    本研究提出了一种结合预训练表示和自动化超参数优化（HPO）的语音情感识别（SER）工作流程。研究使用SpeechBrain的wav2vec2-base模型作为编码器，在IEMOCAP上进行微调。对比了Gaussian Process Bayesian Optimisation（GP-BO）和Tree-structured Parzen Estimators（TPE）两种HPO策略，在相同的四维搜索空间和15次试验预算下，以德国EmoDB语料库上的平衡类准确率（BCA）为评价指标进行实验。所有实验均在8个CPU核心和32GB RAM上进行。GP-BO在11分钟内达到0.96的BCA，而TPE（Hyperopt实现）在15分钟内达到0.97。相比之下，网格搜索需要143次试验和1,680分钟才能达到超过0.9的BCA，而最佳的AutoSpeech 2020基线仅在30分钟内达到0.85（在GPU上）。对于跨语言泛化，使用EmoDB训练的HPO调优模型在零样本情况下，CREMA-D和RAVDESS上的准确率分别提高了0.25和0.26。结果表明，使用预训练编码器的高效HPO在商品CPU上即可实现有竞争力的SER性能。相关源代码已公开于：<a target="_blank" rel="noopener" href="https://github.com/youngaryan/speechbrain-emotion-hpo%E3%80%82">https://github.com/youngaryan/speechbrain-emotion-hpo。</a> </p>
<p><strong>关键见解</strong></p>
<ol>
<li>研究提出了一种结合预训练模型和自动化超参数优化（HPO）的语音情感识别（SER）工作流程。</li>
<li>对比了GP-BO和TPE两种HPO策略，显示它们相较于网格搜索在时间和性能上具有优势。</li>
<li>在德国EmoDB语料库上，HPO策略配合预训练模型达到了较高的平衡类准确率（BCA）。</li>
<li>GP-BO在11分钟内达到0.96的BCA，TPE在稍长的时间内达到0.97的BCA。</li>
<li>相较于最佳AutoSpeech 2020基线，提出的策略在性能上有所提升。</li>
<li>研究的模型在零样本情况下，对CREMA-D和RAVDESS数据集的泛化能力较强。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.07052">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic-private.zhihu.com/v2-280123de68487b234e77f0e488200740~resize:0:q75.jpg?source=1f5c5e47&expiration=1760043415&auth_key=1760043415-0-0-29f9dd7877dd6cfac523bcc91ed3e275&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-a6c0bf66a96546bdfcfd71a737c325ca~resize:0:q75.jpg?source=1f5c5e47&expiration=1760043423&auth_key=1760043423-0-0-f72ae457132a77994c731a11f2cd4497&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-b141bda1dfb3a0ca6357505b74b2c401~resize:0:q75.jpg?source=1f5c5e47&expiration=1760043430&auth_key=1760043430-0-0-d5c0a77c4f5cafbd3a00b47d68708ec8&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-67b206753f931b781d3acf763b9e3a9b~resize:0:q75.jpg?source=1f5c5e47&expiration=1760043436&auth_key=1760043436-0-0-1f223e97f58fcc84e2a7fd2d8513b77a&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="SHANKS-Simultaneous-Hearing-and-Thinking-for-Spoken-Language-Models"><a href="#SHANKS-Simultaneous-Hearing-and-Thinking-for-Spoken-Language-Models" class="headerlink" title="SHANKS: Simultaneous Hearing and Thinking for Spoken Language Models"></a>SHANKS: Simultaneous Hearing and Thinking for Spoken Language Models</h2><p><strong>Authors:Cheng-Han Chiang, Xiaofei Wang, Linjie Li, Chung-Ching Lin, Kevin Lin, Shujie Liu, Zhendong Wang, Zhengyuan Yang, Hung-yi Lee, Lijuan Wang</strong></p>
<p>Current large language models (LLMs) and spoken language models (SLMs) begin thinking and taking actions only after the user has finished their turn. This prevents the model from interacting during the user’s turn and can lead to high response latency while it waits to think. Consequently, thinking after receiving the full input is not suitable for speech-to-speech interaction, where real-time, low-latency exchange is important. We address this by noting that humans naturally “think while listening.” In this paper, we propose SHANKS, a general inference framework that enables SLMs to generate unspoken chain-of-thought reasoning while listening to the user input. SHANKS streams the input speech in fixed-duration chunks and, as soon as a chunk is received, generates unspoken reasoning based on all previous speech and reasoning, while the user continues speaking. SHANKS uses this unspoken reasoning to decide whether to interrupt the user and to make tool calls to complete the task. We demonstrate that SHANKS enhances real-time user-SLM interaction in two scenarios: (1) when the user is presenting a step-by-step solution to a math problem, SHANKS can listen, reason, and interrupt when the user makes a mistake, achieving 37.1% higher interruption accuracy than a baseline that interrupts without thinking; and (2) in a tool-augmented dialogue, SHANKS can complete 56.9% of the tool calls before the user finishes their turn. Overall, SHANKS moves toward models that keep thinking throughout the conversation, not only after a turn ends. Animated illustrations of Shanks can be found at <a target="_blank" rel="noopener" href="https://d223302.github.io/SHANKS/">https://d223302.github.io/SHANKS/</a> </p>
<blockquote>
<p>当前的大型语言模型（LLM）和口语模型（SLM）都是在用户完成发言后才开始思考和行动。这阻止了模型在用户发言过程中的交互，并可能导致响应延迟。因此，在接收完整输入后再思考不适合语音到语音的交互，其中实时、低延迟的交互非常重要。我们通过注意到人类自然地“边听边思考”来解决这个问题。在本文中，我们提出了SHANKS，这是一个通用推理框架，它使SLM能够在听取用户输入时生成未说出的思维链推理。SHANKS以固定持续时间的块流输入语音，在收到一个块后，立即基于之前的语音和推理生成未说出的思考，同时用户继续说话。SHANKS使用这种未说出的推理来决定是否中断用户并进行工具调用以完成任务。我们证明了SHANKS在两种情况下增强了实时用户与SLM的交互：（1）当用户逐步解决数学问题时，SHANKS可以倾听、推理，并在用户犯错时中断，其中断准确性比在没有思考的情况下中断的基线高出37.1%；（2）在工具增强的对话中，SHANKS可以在用户完成发言之前完成56.9%的工具调用。总的来说，SHANKS朝着让模型在整个对话过程中保持思考的方向发展，而不仅仅是在一轮对话结束后。有关Shanks的动画插图，请访问：[<a target="_blank" rel="noopener" href="https://d223302.github.io/SHANKS/]">https://d223302.github.io/SHANKS/]</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.06917v1">PDF</a> Work in progress</p>
<p><strong>Summary</strong>：</p>
<p>当前大型语言模型（LLMs）和口语模型（SLMs）在用户完成发言后才进行思考并行动，这导致模型无法在用户发言过程中进行交互，从而产生较高的响应延迟。本文提出了一种名为SHANKS的通用推理框架，它使SLMs能够在听取用户输入的同时进行未言明的思维推理。SHANKS将输入语音流划分为固定时长的片段，并在接收到片段后立即基于之前的语音和推理生成未言明的思考，同时用户可以继续发言。该框架通过未言明的思考来决定是否打断用户并完成任务。在用户的数学问题解决步骤展示和工具增强对话场景中，SHANKS表现出优秀的实时用户-SLM交互性能，包括在用户犯错时能够准确打断并提前完成工具调用。总之，SHANKS推动了模型在对话过程中的持续思考，而不仅仅是在一个回合结束后。</p>
<p><strong>Key Takeaways</strong>：</p>
<ol>
<li>当前大型语言模型和口语模型在用户完成发言后才进行思考，导致交互延迟。</li>
<li>SHANKS框架能使SLMs在用户发言过程中进行未言明的思维推理。</li>
<li>SHANKS通过固定时长语音片段的流式处理来实现实时推理。</li>
<li>SHANKS能够在用户犯错时准确打断并提前完成工具调用。</li>
<li>SHANKS提高了实时用户-SLM交互的效果。</li>
<li>SHANKS通过未言明的思考推动模型在对话过程中的持续思考。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.06917">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic-private.zhihu.com/v2-3f139f75350402e7dbcef59962db2850~resize:0:q75.jpg?source=1f5c5e47&expiration=1760043444&auth_key=1760043444-0-0-813a66abd5e3f8f0bc663e9a5eadd321&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-803df79cd6bb2806b37f96c795cee3ad~resize:0:q75.jpg?source=1f5c5e47&expiration=1760043451&auth_key=1760043451-0-0-f5c9cee8e13a74d8ccd865cd239b670f&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-77208d58dd00b35ca19159fc9e22ce30~resize:0:q75.jpg?source=1f5c5e47&expiration=1760043457&auth_key=1760043457-0-0-0e5ead7fd1b198e815067ef8868a17d4&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="EmoHRNet-High-Resolution-Neural-Network-Based-Speech-Emotion-Recognition"><a href="#EmoHRNet-High-Resolution-Neural-Network-Based-Speech-Emotion-Recognition" class="headerlink" title="EmoHRNet: High-Resolution Neural Network Based Speech Emotion   Recognition"></a>EmoHRNet: High-Resolution Neural Network Based Speech Emotion   Recognition</h2><p><strong>Authors:Akshay Muppidi, Martin Radfar</strong></p>
<p>Speech emotion recognition (SER) is pivotal for enhancing human-machine interactions. This paper introduces “EmoHRNet”, a novel adaptation of High-Resolution Networks (HRNet) tailored for SER. The HRNet structure is designed to maintain high-resolution representations from the initial to the final layers. By transforming audio samples into spectrograms, EmoHRNet leverages the HRNet architecture to extract high-level features. EmoHRNet’s unique architecture maintains high-resolution representations throughout, capturing both granular and overarching emotional cues from speech signals. The model outperforms leading models, achieving accuracies of 92.45% on RAVDESS, 80.06% on IEMOCAP, and 92.77% on EMOVO. Thus, we show that EmoHRNet sets a new benchmark in the SER domain. </p>
<blockquote>
<p>语音情感识别（SER）对于增强人机交互至关重要。本文介绍了”EmoHRNet”，这是针对SER定制的High-Resolution Networks（HRNet）的一种新型适应。HRNet结构旨在从初始层到最终层保持高分辨率表示。通过将音频样本转换为频谱图，EmoHRNet利用HRNet架构提取高级特征。EmoHRNet的独特架构在整个过程中保持了高分辨率的表示，从语音信号中捕获了粒度和总体的情感线索。该模型在RAVDESS上达到了92.45%的准确率，在IEMOCAP上达到了8.06%，在EMOVO上达到了92.77%，因此，我们证明了EmoHRNet在SER领域树立了新的基准。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.06072v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文介绍了针对语音情感识别（SER）的新型网络模型EmoHRNet。该模型基于高分辨率网络（HRNet）架构进行设计，可全程维持高分辨率表达，捕捉音频信号的细微与整体情感线索。实验结果显示，EmoHRNet在RAVDESS、IEMOCAP和EMOVO三个数据集上的准确率分别达到了92.45%、80.06%和92.77%，为SER领域树立了新的基准。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>EmoHRNet是一个针对语音情感识别（SER）任务的高分辨率网络模型。</li>
<li>EmoHRNet通过维持高分辨率表达，能够捕捉音频信号中的细微与整体情感线索。</li>
<li>EmoHRNet将音频样本转换为谱图，利用HRNet架构提取高级特征。</li>
<li>模型在RAVDESS、IEMOCAP和EMOVO数据集上的表现均超过了现有模型。</li>
<li>EmoHRNet在RAVDESS数据集上的准确率达到了92.45%。</li>
<li>EmoHRNet在IEMOCAP数据集上的准确率为80.06%。</li>
<li>EmoHRNet在EMOVO数据集上的准确率达到了92.77%，显示出其在新领域中的强大性能。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.06072">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic-private.zhihu.com/v2-cc2b126653ec8ea51e4bdd658fcafd5c~resize:0:q75.jpg?source=1f5c5e47&expiration=1760043465&auth_key=1760043465-0-0-22cb94014451dbea8ee60f826788559f&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-f545cc5e0a70383d9e6c48e1d7e06ff4~resize:0:q75.jpg?source=1f5c5e47&expiration=1760043473&auth_key=1760043473-0-0-e1f500c3227bac3c1119e768e0b1e93a&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-45a7d9ad4be539ad8acecc3c08f38bd9~resize:0:q75.jpg?source=1f5c5e47&expiration=1760043479&auth_key=1760043479-0-0-3afaddc33d9e8e0001ffcbe60d4c2f29&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-9c3b3c62bd6185c59f6fdeeae9ce7c6a~resize:0:q75.jpg?source=1f5c5e47&expiration=1760043486&auth_key=1760043486-0-0-775a2b97379ddd8d5202bcbd6c48fe4d&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="ECTSpeech-Enhancing-Efficient-Speech-Synthesis-via-Easy-Consistency-Tuning"><a href="#ECTSpeech-Enhancing-Efficient-Speech-Synthesis-via-Easy-Consistency-Tuning" class="headerlink" title="ECTSpeech: Enhancing Efficient Speech Synthesis via Easy Consistency   Tuning"></a>ECTSpeech: Enhancing Efficient Speech Synthesis via Easy Consistency   Tuning</h2><p><strong>Authors:Tao Zhu, Yinfeng Yu, Liejun Wang, Fuchun Sun, Wendong Zheng</strong></p>
<p>Diffusion models have demonstrated remarkable performance in speech synthesis, but typically require multi-step sampling, resulting in low inference efficiency. Recent studies address this issue by distilling diffusion models into consistency models, enabling efficient one-step generation. However, these approaches introduce additional training costs and rely heavily on the performance of pre-trained teacher models. In this paper, we propose ECTSpeech, a simple and effective one-step speech synthesis framework that, for the first time, incorporates the Easy Consistency Tuning (ECT) strategy into speech synthesis. By progressively tightening consistency constraints on a pre-trained diffusion model, ECTSpeech achieves high-quality one-step generation while significantly reducing training complexity. In addition, we design a multi-scale gate module (MSGate) to enhance the denoiser’s ability to fuse features at different scales. Experimental results on the LJSpeech dataset demonstrate that ECTSpeech achieves audio quality comparable to state-of-the-art methods under single-step sampling, while substantially reducing the model’s training cost and complexity. </p>
<blockquote>
<p>扩散模型在语音合成中表现出了出色的性能，但通常需要多步采样，导致推理效率低下。最近的研究通过将扩散模型提炼成一致性模型来解决这个问题，实现了一次性生成。然而，这些方法增加了额外的训练成本，并严重依赖于预训练教师模型的性能。在本文中，我们提出了ECTSpeech，这是一个简单有效的一次性语音合成框架，首次将Easy Consistency Tuning（ECT）策略融入语音合成。通过对预训练扩散模型上的一致性约束进行逐步加强，ECTSpeech实现了一次性高质量生成，同时显著降低了训练复杂度。此外，我们设计了一个多尺度门模块（MSGate）以增强去噪器在不同尺度上融合特征的能力。在LJSpeech数据集上的实验结果表明，ECTSpeech在一次性采样下生成的音频质量与最先进的方法相当，同时大大降低了模型的训练成本和复杂度。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.05984v1">PDF</a> Accepted for publication by Proceedings of the 2025 ACM Multimedia   Asia Conference(MMAsia ‘25)</p>
<p><strong>总结</strong></p>
<p>本文提出了一种简单有效的单步语音合成框架ECTSpeech，首次将Easy Consistency Tuning（ECT）策略应用于语音合成。通过对预训练的扩散模型逐步加强一致性约束，ECTSpeech实现了高质量的单步生成，并显著降低了训练复杂度。此外，还设计了一个多尺度门模块（MSGate）以增强去噪器在不同尺度上融合特征的能力。在LJSpeech数据集上的实验结果表明，ECTSpeech在单步采样下生成的音频质量与最先进的方法相当，同时大大降低了模型的训练成本和复杂度。</p>
<p><strong>关键见解</strong></p>
<ol>
<li>扩散模型在语音合成中表现出卓越的性能，但通常需要多步采样，导致推理效率低下。</li>
<li>现有研究通过蒸馏扩散模型到一致性模型来解决这个问题，实现了一次性生成。</li>
<li>本文首次将Easy Consistency Tuning（ECT）策略引入语音合成，通过逐步加强预训练扩散模型的一致性约束，实现高质量的单步生成。</li>
<li>设计了多尺度门模块（MSGate）以增强去噪器在不同尺度上融合特征的能力，进一步提高语音合成质量。</li>
<li>实验结果表明，ECTSpeech在单步采样下生成的音频质量与现有最先进的方法相当。</li>
<li>ECTSpeech显著降低了模型的训练成本和复杂度。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.05984">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic-private.zhihu.com/v2-1be76a4e9441b6dc9626d002faab9b13~resize:0:q75.jpg?source=1f5c5e47&expiration=1760043493&auth_key=1760043493-0-0-0666dc4dbb4a73992b2539808d5ff8f1&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-476ad2384c1d6e63744614a7cd1667ec~resize:0:q75.jpg?source=1f5c5e47&expiration=1760043500&auth_key=1760043500-0-0-23f5eadad40b85dbd7c0f554c7d48688&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-971a33e808f4113ae4707484fa8e6179~resize:0:q75.jpg?source=1f5c5e47&expiration=1760043507&auth_key=1760043507-0-0-d896618cec539c7d3c3a0f8121fb8c18&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-297b28ebeb0760d991de0b1d18e95134~resize:0:q75.jpg?source=1f5c5e47&expiration=1760043513&auth_key=1760043513-0-0-9c88d555c7edcdeaa6313c5d14bad5a2&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-5dd44fc65286ce285d5ab94cad6fd289~resize:0:q75.jpg?source=1f5c5e47&expiration=1760043520&auth_key=1760043520-0-0-9a9072721192d764af4a6e6e1b43525b&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-819a79c1f423b4cd27f10980cb00d372~resize:0:q75.jpg?source=1f5c5e47&expiration=1760043526&auth_key=1760043526-0-0-b77fcdd5af95d076047352cb613bf568&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="MSF-SER-Enriching-Acoustic-Modeling-with-Multi-Granularity-Semantics-for-Speech-Emotion-Recognition"><a href="#MSF-SER-Enriching-Acoustic-Modeling-with-Multi-Granularity-Semantics-for-Speech-Emotion-Recognition" class="headerlink" title="MSF-SER: Enriching Acoustic Modeling with Multi-Granularity Semantics   for Speech Emotion Recognition"></a>MSF-SER: Enriching Acoustic Modeling with Multi-Granularity Semantics   for Speech Emotion Recognition</h2><p><strong>Authors:Haoxun Li, Yuqing Sun, Hanlei Shi, Yu Liu, Leyuan Qu, Taihao Li</strong></p>
<p>Continuous dimensional speech emotion recognition captures affective variation along valence, arousal, and dominance, providing finer-grained representations than categorical approaches. Yet most multimodal methods rely solely on global transcripts, leading to two limitations: (1) all words are treated equally, overlooking that emphasis on different parts of a sentence can shift emotional meaning; (2) only surface lexical content is represented, lacking higher-level interpretive cues. To overcome these issues, we propose MSF-SER (Multi-granularity Semantic Fusion for Speech Emotion Recognition), which augments acoustic features with three complementary levels of textual semantics–Local Emphasized Semantics (LES), Global Semantics (GS), and Extended Semantics (ES). These are integrated via an intra-modal gated fusion and a cross-modal FiLM-modulated lightweight Mixture-of-Experts (FM-MOE). Experiments on MSP-Podcast and IEMOCAP show that MSF-SER consistently improves dimensional prediction, demonstrating the effectiveness of enriched semantic fusion for SER. </p>
<blockquote>
<p>连续维度语音情感识别能够捕捉沿价值、兴奋和支配地位的情感变化，提供比分类方法更精细的表示。然而，大多数多模式方法仅依赖于全局文字稿，导致两个局限性：（1）所有单词都被平等对待，忽略了句子中不同部分的重点会改变情感意义；（2）只表示表面词汇内容，缺乏高级解释性线索。为了克服这些问题，我们提出了MSF-SER（用于语音情感识别的多粒度语义融合），它通过三个互补级别的文本语义增强声学特征，包括局部强调语义（LES）、全局语义（GS）和扩展语义（ES）。这些通过模态内门控融合和跨模态FiLM调制的轻量级混合专家（FM-MOE）进行集成。在MSP-Podcast和IEMOCAP上的实验表明，MSF-SER在维度预测方面始终表现出改进，证明了丰富语义融合对于SER的有效性。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.05749v1">PDF</a> Under review for ICASSP 2026</p>
<p><strong>Summary</strong></p>
<p>本文介绍了连续维度语音情感识别，通过捕捉情感变化的维度，如效价、兴奋和支配，提供比分类方法更精细的表征。然而，大多数多模式方法仅依赖全局文本，存在两个局限性：一是所有词语平等对待，忽略了句子中不同部分的强调会改变情感意义；二是仅代表表面词汇内容，缺乏高级解释性线索。为克服这些问题，本文提出了MSF-SER（用于语音情感识别的多粒度语义融合），它通过三种互补的文本语义级别——局部强调语义（LES）、全局语义（GS）和扩展语义（ES）来增强声学特征。这些通过模态内门控融合和跨模态FiLM调制的轻量级混合专家（FM-MOE）进行集成。在MSP-Podcast和IEMOCAP上的实验表明，MSF-SER在维度预测上表现一致，证明了丰富语义融合在SER中的有效性。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>语音情感识别不仅仅是简单的分类问题，还要涉及对效价、兴奋和支配等维度的捕捉。</li>
<li>当前的多模式方法主要依赖全局文本，存在忽视句子中不同部分强调和情感意义的问题。</li>
<li>提出了一种新的方法MSF-SER，通过三种互补的文本语义级别增强声学特征识别语音情感。</li>
<li>MSF-SER通过模态内门控融合和跨模态集成方法将不同级别的语义信息结合。</li>
<li>实验证明，MSF-SER在维度预测上表现优异，验证了其在语音情感识别中的有效性。</li>
<li>引入的三种文本语义级别包括局部强调语义（LES）、全局语义（GS）和扩展语义（ES），有助于更准确地捕捉情感表达。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.05749">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic-private.zhihu.com/v2-58e9bc9dd9d126c2d7bba9564e3c758f~resize:0:q75.jpg?source=1f5c5e47&expiration=1760043534&auth_key=1760043534-0-0-8ad9d370aa4a6c2668780c3cf0ed6c32&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-06c9c2fa9319f378bc4092bb03c9b615~resize:0:q75.jpg?source=1f5c5e47&expiration=1760043541&auth_key=1760043541-0-0-bfa39b0f3d902e32779682ac524379c8&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-ebade4d43208e5a6fd8c626b9bdfec1b~resize:0:q75.jpg?source=1f5c5e47&expiration=1760043548&auth_key=1760043548-0-0-a152222f03bc755329b0696e98efb706&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-9c33806b6880626d8173bd698e97694c~resize:0:q75.jpg?source=1f5c5e47&expiration=1760043554&auth_key=1760043554-0-0-84f49f4a680912270c4eff54b3489472&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-d386601ca20d041bb7f1024e69328997~resize:0:q75.jpg?source=1f5c5e47&expiration=1760043561&auth_key=1760043561-0-0-23f847e7c131efe07cb6c665321fbae7&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-0e8a0a3cc3c4c6c73e08e263995b45b4~resize:0:q75.jpg?source=1f5c5e47&expiration=1760043568&auth_key=1760043568-0-0-e7a6f3823cd4356d315c34464423bc77&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="WaveSP-Net-Learnable-Wavelet-Domain-Sparse-Prompt-Tuning-for-Speech-Deepfake-Detection"><a href="#WaveSP-Net-Learnable-Wavelet-Domain-Sparse-Prompt-Tuning-for-Speech-Deepfake-Detection" class="headerlink" title="WaveSP-Net: Learnable Wavelet-Domain Sparse Prompt Tuning for Speech   Deepfake Detection"></a>WaveSP-Net: Learnable Wavelet-Domain Sparse Prompt Tuning for Speech   Deepfake Detection</h2><p><strong>Authors:Xi Xuan, Xuechen Liu, Wenxin Zhang, Yi-Cheng Lin, Xiaojian Lin, Tomi Kinnunen</strong></p>
<p>Modern front-end design for speech deepfake detection relies on full fine-tuning of large pre-trained models like XLSR. However, this approach is not parameter-efficient and may lead to suboptimal generalization to realistic, in-the-wild data types. To address these limitations, we introduce a new family of parameter-efficient front-ends that fuse prompt-tuning with classical signal processing transforms. These include FourierPT-XLSR, which uses the Fourier Transform, and two variants based on the Wavelet Transform: WSPT-XLSR and Partial-WSPT-XLSR. We further propose WaveSP-Net, a novel architecture combining a Partial-WSPT-XLSR front-end and a bidirectional Mamba-based back-end. This design injects multi-resolution features into the prompt embeddings, which enhances the localization of subtle synthetic artifacts without altering the frozen XLSR parameters. Experimental results demonstrate that WaveSP-Net outperforms several state-of-the-art models on two new and challenging benchmarks, Deepfake-Eval-2024 and SpoofCeleb, with low trainable parameters and notable performance gains. The code and models are available at <a target="_blank" rel="noopener" href="https://github.com/xxuan-acoustics/WaveSP-Net">https://github.com/xxuan-acoustics/WaveSP-Net</a>. </p>
<blockquote>
<p>现代语音深度伪造检测前端设计依赖于大型预训练模型（如XLSR）的完全微调。然而，这种方法在参数效率方面并不理想，并且可能导致对现实、野生数据类型的一般化表现不佳。为了解决这些局限性，我们引入了一系列参数高效前端，将提示微调与经典信号处理变换相结合。这些包括使用傅立叶变换的FourierPT-XLSR，以及基于小波变换的两个变体：WSPT-XLSR和部分WSPT-XLSR。我们进一步提出了WaveSP-Net，这是一种新型架构，结合了Partial-WSPT-XLSR前端和基于双向Mamba的后端。这种设计将多分辨率特征注入提示嵌入中，提高了细微合成伪影的定位能力，同时不改变冻结的XLSR参数。实验结果表明，WaveSP-Net在两个新的具有挑战性的基准测试（Deepfake-Eval-2024和SpoofCeleb）上的表现优于许多最新模型，具有较少的可训练参数和显著的性能提升。相关代码和模型可在xxuan声学WaveSP-Net网站上找到（<a target="_blank" rel="noopener" href="https://github.com/xxuan-acoustics/WaveSP-Net%EF%BC%89%E3%80%82">https://github.com/xxuan-acoustics/WaveSP-Net）。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.05305v1">PDF</a> Submitted to ICASSP 2026</p>
<p><strong>Summary</strong></p>
<p>本摘要针对语音深度伪造检测的前端设计提出了改进方案。为了解决现有基于大型预训练模型如XLSR的全量微调方法存在的参数效率低下和对真实场景数据泛化性能不佳的问题，研究团队引入了参数高效的前端设计，融合了提示调谐和经典信号处理变换技术。同时，他们提出了结合小波变换的三种新型前端技术，包括WaveSP-Net网络结构。WaveSP-Net通过在提示嵌入中注入多分辨率特征，提升了检测微弱合成伪造的本地化能力，且不会改变XLSR的参数。实验结果表明，WaveSP-Net在两个新的具有挑战性的基准测试上超越了最新的技术水准，性能优越且具有较低的参数可训练性。具体模型和代码可通过xxuan声学研究所的公开仓库获取。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>现代前端设计在语音深度伪造检测中依赖大型预训练模型的全面微调，但这种方法参数效率低下且可能对真实世界数据的泛化性能不佳。</li>
<li>针对上述问题，引入了一种新型参数高效前端设计，融合了提示调谐和经典信号处理变换技术。</li>
<li>提出了三种基于小波变换的新型前端技术，包括WaveSP-Net网络结构。该结构通过将多分辨率特征注入提示嵌入中，提升了定位微妙合成伪造的准确性。</li>
<li>WaveSP-Net在基准测试中表现出卓越性能，超越了现有技术水准，且具备较低的参数可训练性。</li>
<li>该研究提供的模型和代码可通过公开仓库获取，便于其他研究者使用和进一步开发。</li>
<li>通过结合信号处理技术与深度学习模型，该研究为语音深度伪造检测领域提供了新的方向。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.05305">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic-private.zhihu.com/v2-65fe0b353d5c5ca1acc380522a44a558~resize:0:q75.jpg?source=1f5c5e47&expiration=1760043575&auth_key=1760043575-0-0-a207238113c587a04ef308aa4ab4b081&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-6ade1af72f3776b9ca860d43578439a9~resize:0:q75.jpg?source=1f5c5e47&expiration=1760043583&auth_key=1760043583-0-0-6ffad740ea25bb4f9f7647eeff61d7ca&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-4941265dac737ba27e75f33086bdd192~resize:0:q75.jpg?source=1f5c5e47&expiration=1760043589&auth_key=1760043589-0-0-790797aab8a427a09316c948b97f05fb&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-9532f5e3ca9532e119e2725c85ecdce9~resize:0:q75.jpg?source=1f5c5e47&expiration=1760043596&auth_key=1760043596-0-0-f3213f41a6495d97a4d73d3e63ba5b3d&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="AUREXA-SE-Audio-Visual-Unified-Representation-Exchange-Architecture-with-Cross-Attention-and-Squeezeformer-for-Speech-Enhancement"><a href="#AUREXA-SE-Audio-Visual-Unified-Representation-Exchange-Architecture-with-Cross-Attention-and-Squeezeformer-for-Speech-Enhancement" class="headerlink" title="AUREXA-SE: Audio-Visual Unified Representation Exchange Architecture   with Cross-Attention and Squeezeformer for Speech Enhancement"></a>AUREXA-SE: Audio-Visual Unified Representation Exchange Architecture   with Cross-Attention and Squeezeformer for Speech Enhancement</h2><p><strong>Authors:M. Sajid, Deepanshu Gupta, Yash Modi, Sanskriti Jain, Harshith Jai Surya Ganji, A. Rahaman, Harshvardhan Choudhary, Nasir Saleem, Amir Hussain, M. Tanveer</strong></p>
<p>In this paper, we propose AUREXA-SE (Audio-Visual Unified Representation Exchange Architecture with Cross-Attention and Squeezeformer for Speech Enhancement), a progressive bimodal framework tailored for audio-visual speech enhancement (AVSE). AUREXA-SE jointly leverages raw audio waveforms and visual cues by employing a U-Net-based 1D convolutional encoder for audio and a Swin Transformer V2 for efficient and expressive visual feature extraction. Central to the architecture is a novel bidirectional cross-attention mechanism, which facilitates deep contextual fusion between modalities, enabling rich and complementary representation learning. To capture temporal dependencies within the fused embeddings, a stack of lightweight Squeezeformer blocks combining convolutional and attention modules is introduced. The enhanced embeddings are then decoded via a U-Net-style decoder for direct waveform reconstruction, ensuring perceptually consistent and intelligible speech output. Experimental evaluations demonstrate the effectiveness of AUREXA-SE, achieving significant performance improvements over noisy baselines, with STOI of 0.516, PESQ of 1.323, and SI-SDR of -4.322 dB. The source code of AUREXA-SE is available at <a target="_blank" rel="noopener" href="https://github.com/mtanveer1/AVSEC-4-Challenge-2025">https://github.com/mtanveer1/AVSEC-4-Challenge-2025</a>. </p>
<blockquote>
<p>本文提出了AUREXA-SE（一种用于语音增强的视听统一表示交换架构，具有跨注意力和Squeezeformer功能），这是一个为音频视觉语音增强（AVSE）量身定制的先进双模态框架。AUREXA-SE通过采用基于U-Net的1D卷积编码器进行音频处理和Swin Transformer V2进行高效且富有表现力的视觉特征提取，联合利用原始音频波形和视觉线索。该架构的核心是一种新型双向跨注意机制，有助于促进不同模态之间的深度上下文融合，从而实现丰富且互补的表示学习。为了捕获融合嵌入中的时间依赖性，引入了一系列结合了卷积和注意力模块的轻量化Squeezeformer块。然后，通过U-Net风格的解码器对增强的嵌入进行解码，以实现直接波形重建，确保感知上一致且可理解的语音输出。实验评估表明，AUREXA-SE的有效性在噪声基准测试上取得了显著的性能改进，实现了STOI为0.516、PESQ为1.323和SI-SDR为-4.322 dB的结果。AUREXA-SE的源代码可在<a target="_blank" rel="noopener" href="https://github.com/mtanveer1/AVSEC-4-Challenge-2025%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/mtanveer1/AVSEC-4-Challenge-2025找到。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.05295v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>AUREXA-SE是一个为视听语音增强（AVSE）设计的先进双模态框架。它利用原始音频波形和视觉线索，通过U-Net基于1D卷积编码器与Swin Transformer V2高效视觉特征提取技术，实现跨模态深度上下文融合。框架引入了轻量级的Squeezeformer块捕捉融合嵌入中的时间依赖性，并通过U-Net风格的解码器解码增强嵌入，实现直接波形重建。实验评估显示，AUREXA-SE在噪声基准线上取得了显著性能提升。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>AUREXA-SE是一个针对音频-视觉语音增强（AVSE）设计的双模态框架。</li>
<li>该框架结合原始音频波形和视觉线索，使用U-Net和Swin Transformer V2技术。</li>
<li>框架中的核心是一个双向跨注意机制，实现了模态间的深度上下文融合。</li>
<li>引入Squeezeformer块捕捉融合嵌入中的时间依赖性。</li>
<li>通过U-Net风格的解码器进行增强嵌入的解码，实现直接波形重建。</li>
<li>实验评估显示AUREXA-SE在噪声环境下的性能显著提升。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.05295">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic-private.zhihu.com/v2-6f7a44a0505cfe24612189dd25e07d9f~resize:0:q75.jpg?source=1f5c5e47&expiration=1760043604&auth_key=1760043604-0-0-119c4fd9732c750f19b7dd8a0f262cbe&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-9ba0e2646084e43f6d2ed7928cc7f79f~resize:0:q75.jpg?source=1f5c5e47&expiration=1760043611&auth_key=1760043611-0-0-439e57c74eb26fafadb4c82aeacb005b&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-895b48520877107eaac72ab5caa6c75d~resize:0:q75.jpg?source=1f5c5e47&expiration=1760043617&auth_key=1760043617-0-0-400676bd208b1a9ee5771a48c6a16803&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-efc16d56f5546b2e73034d8441d35259~resize:0:q75.jpg?source=1f5c5e47&expiration=1760043623&auth_key=1760043623-0-0-ebf2509d004e7cd4306325798146f76c&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-6e43c38af263ccf86f7933472a377ac9~resize:0:q75.jpg?source=1f5c5e47&expiration=1760043630&auth_key=1760043630-0-0-26b2fb08f992d18edc19b001e53bf72d&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-1d7aa6e4513be0653e132d4f9626e2db~resize:0:q75.jpg?source=1f5c5e47&expiration=1760043636&auth_key=1760043636-0-0-c9e887db6f96277c6fba2bc751280188&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-77a18bc140131e4e94e05c45b552fd85~resize:0:q75.jpg?source=1f5c5e47&expiration=1760043643&auth_key=1760043643-0-0-4e0aab7a67885c660ea0393c37304152&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="Machine-Unlearning-in-Speech-Emotion-Recognition-via-Forget-Set-Alone"><a href="#Machine-Unlearning-in-Speech-Emotion-Recognition-via-Forget-Set-Alone" class="headerlink" title="Machine Unlearning in Speech Emotion Recognition via Forget Set Alone"></a>Machine Unlearning in Speech Emotion Recognition via Forget Set Alone</h2><p><strong>Authors:Zhao Ren, Rathi Adarshi Rammohan, Kevin Scheck, Tanja Schultz</strong></p>
<p>Speech emotion recognition aims to identify emotional states from speech signals and has been widely applied in human-computer interaction, education, healthcare, and many other fields. However, since speech data contain rich sensitive information, partial data can be required to be deleted by speakers due to privacy concerns. Current machine unlearning approaches largely depend on data beyond the samples to be forgotten. However, this reliance poses challenges when data redistribution is restricted and demands substantial computational resources in the context of big data. We propose a novel adversarial-attack-based approach that fine-tunes a pre-trained speech emotion recognition model using only the data to be forgotten. The experimental results demonstrate that the proposed approach can effectively remove the knowledge of the data to be forgotten from the model, while preserving high model performance on the test set for emotion recognition. </p>
<blockquote>
<p>语音情感识别旨在从语音信号中识别情感状态，已广泛应用于人机交互、教育、医疗和许多其他领域。然而，由于语音数据包含丰富的敏感信息，出于隐私考虑，部分数据可能需要说话者删除。当前机器删除旧数据的方法在很大程度上依赖于要忘记的样本之外的数据。然而，这种依赖在数据重新分配受限和大数据背景下需要大量计算资源时构成了挑战。我们提出了一种基于对抗攻击的新方法，该方法仅使用要忘记的数据对预训练的语音情感识别模型进行微调。实验结果表明，该方法可以有效地从模型中删除要忘记的数据的知识，同时保持模型在测试集上的情感识别性能。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.04251v1">PDF</a> Submitted to ICASSP 2026</p>
<p><strong>总结</strong></p>
<p>语音情感识别旨在从语音信号中识别情感状态，已广泛应用于人机交互、教育、医疗等多个领域。然而，由于语音数据包含丰富的敏感信息，出于隐私考虑，部分数据可能要求被删除。当前机器擦除方法很大程度上依赖于超出要忘记样本的数据。然而，这种依赖在数据重新分配受限和大数据背景下需要大量计算资源时带来了挑战。本文提出了一种基于对抗攻击的方法，通过微调预训练的语音情感识别模型，只使用要忘记的数据。实验结果表明，该方法可以有效地从模型中移除要忘记的数据的知识，同时保持模型在测试集上的情感识别性能。</p>
<p><strong>关键见解</strong></p>
<ol>
<li>语音情感识别是通过识别语音信号中的情感状态来实现，广泛应用于多个领域。</li>
<li>由于隐私考虑，部分语音数据可能需要被删除。</li>
<li>当前机器擦除方法通常依赖于超出要删除样本的数据。</li>
<li>数据重新分配受限和大数据背景下，当前方法面临挑战。</li>
<li>提出了一种基于对抗攻击的新型机器擦除方法，只使用需要被忘记的数据对预训练模型进行微调。</li>
<li>实验证明该方法能有效移除模型中需要忘记的数据知识。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.04251">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic-private.zhihu.com/v2-0341ae5eec1ff2c55fcc35afb7d1d83c~resize:0:q75.jpg?source=1f5c5e47&expiration=1760043651&auth_key=1760043651-0-0-a10e90de3171ace61eb4406b1b2fe48b&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-91bd30f7419610de2a2a3c25c9491028~resize:0:q75.jpg?source=1f5c5e47&expiration=1760043658&auth_key=1760043658-0-0-1fc31f41c9056821df9616b70e7479c0&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-ecdb8a46b923274e2517a43ea89adcc0~resize:0:q75.jpg?source=1f5c5e47&expiration=1760043664&auth_key=1760043664-0-0-86721c666e04a111b6f194fde17e9bef&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-8c731da8d0ed8fa4df368e4e0df59b18~resize:0:q75.jpg?source=1f5c5e47&expiration=1760043671&auth_key=1760043671-0-0-b76dc4a5d4d01f4376dfb1c4ac18d969&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="Probing-Whisper-for-Dysarthric-Speech-in-Detection-and-Assessment"><a href="#Probing-Whisper-for-Dysarthric-Speech-in-Detection-and-Assessment" class="headerlink" title="Probing Whisper for Dysarthric Speech in Detection and Assessment"></a>Probing Whisper for Dysarthric Speech in Detection and Assessment</h2><p><strong>Authors:Zhengjun Yue, Devendra Kayande, Zoran Cvetkovic, Erfan Loweimi</strong></p>
<p>Large-scale end-to-end models such as Whisper have shown strong performance on diverse speech tasks, but their internal behavior on pathological speech remains poorly understood. Understanding how dysarthric speech is represented across layers is critical for building reliable and explainable clinical assessment tools. This study probes the Whisper-Medium model encoder for dysarthric speech for detection and assessment (i.e., severity classification). We evaluate layer-wise embeddings with a linear classifier under both single-task and multi-task settings, and complement these results with Silhouette scores and mutual information to provide perspectives on layer informativeness. To examine adaptability, we repeat the analysis after fine-tuning Whisper on a dysarthric speech recognition task. Across metrics, the mid-level encoder layers (13-15) emerge as most informative, while fine-tuning induces only modest changes. The findings improve the interpretability of Whisper’s embeddings and highlight the potential of probing analyses to guide the use of large-scale pretrained models for pathological speech. </p>
<blockquote>
<p>大规模端到端模型，如Whisper，在多种语音任务中表现出强大的性能，但它们在病理性语音上的内部行为仍知之甚少。了解如何跨层表示构音障碍语音对于构建可靠且可解释的临床评估工具至关重要。本研究使用Whisper-Medium模型编码器对构音障碍语音进行检测和评估（即严重程度分类）。我们在单任务和多任务设置下使用线性分类器评估逐层嵌入，并用轮廓分数和互信息来补充这些结果，以提供关于层信息含量的视角。为了检验适应性，我们在对构音障碍语音识别任务微调Whisper后重复分析。在各项指标中，中层编码器层（13-15层）表现出最丰富的信息量，而微调引起的变化较小。这些发现提高了对Whisper嵌入的可解释性，并突出了探测分析在指导大规模预训练模型用于病理性语音方面的潜力。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.04219v1">PDF</a> Submitted to ICASSP 2026</p>
<p><strong>Summary</strong></p>
<p>本文研究了大型端到端模型如Whisper在病理语音上的表现，尤其是通过探索模型对发音障碍性语音（dysarthric speech）在不同层级的表达方式。研究表明中层编码器（中层如第13-15层）对于处理发音障碍性语音任务尤为关键。研究通过线性分类器在单任务和多任务环境下评估了层嵌入的表现，并结合轮廓系数和互信息来评估各层的信息重要性。对Whisper模型进行特定语音障碍识别任务的微调后进行的评估分析表明微调引起的变化有限。研究结果有助于增强对Whisper嵌入模型的解释性，同时显示出针对病理性语音的大型预训练模型探查分析在指导使用方面的潜力。</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>大型端到端模型如Whisper在病理语音处理方面表现出良好的性能，但其内部处理机制尚待深入理解。</li>
<li>对于建立可靠的病理语音临床评估工具而言，了解如何代表病理性语音的关键信息极为重要。</li>
<li>对发音障碍性语音，中层编码器层级表现尤为出色（第13-15层）。这一发现表明，在对病理性语音进行诊断评估时，模型的这些层次可能会成为重要的决策点。这一信息可能对开发更加精确和高效的语音障碍识别系统有重要价值。此外，这也提供了一个理解大型神经网络如何处理复杂语音信息的视角。在复杂的语音任务中，模型的这些层次可能扮演着关键角色，有助于理解这些模型的工作机制。这对于开发更准确的语音识别系统至关重要。</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.04219">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic-private.zhihu.com/v2-4876b6136c41070c573a7663aacb3d07~resize:0:q75.jpg?source=1f5c5e47&expiration=1760043679&auth_key=1760043679-0-0-223081c2d37edc3c903950789a234204&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-943c2955a0b3019bb065f1e12651845b~resize:0:q75.jpg?source=1f5c5e47&expiration=1760043686&auth_key=1760043686-0-0-62f35921e2dfc7db4cbfec5f3e1d3e0c&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-0c73dba57ed3c1e9fc408e52b6fe3cf5~resize:0:q75.jpg?source=1f5c5e47&expiration=1760043693&auth_key=1760043693-0-0-604bf3dd436bd514b14c6a81d5a079fa&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-151bc29dc14afc834c7c7bd5cb517535~resize:0:q75.jpg?source=1f5c5e47&expiration=1760043700&auth_key=1760043700-0-0-9ea3d5c2a3ece0e16adda352a7019b1a&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="Drax-Speech-Recognition-with-Discrete-Flow-Matching"><a href="#Drax-Speech-Recognition-with-Discrete-Flow-Matching" class="headerlink" title="Drax: Speech Recognition with Discrete Flow Matching"></a>Drax: Speech Recognition with Discrete Flow Matching</h2><p><strong>Authors:Aviv Navon, Aviv Shamsian, Neta Glazer, Yael Segal-Feldman, Gill Hetz, Joseph Keshet, Ethan Fetaya</strong></p>
<p>Diffusion and flow-based non-autoregressive (NAR) models have shown strong promise in large language modeling, however, their potential for automatic speech recognition (ASR) remains largely unexplored. We propose Drax, a discrete flow matching framework for ASR that enables efficient parallel decoding. To better align training with inference, we construct an audio-conditioned probability path that guides the model through trajectories resembling likely intermediate inference errors, rather than direct random noise to target transitions. Our theoretical analysis links the generalization gap to divergences between training and inference occupancies, controlled by cumulative velocity errors, thereby motivating our design choice. Empirical evaluation demonstrates that our approach attains recognition accuracy on par with state-of-the-art speech models while offering improved accuracy-efficiency trade-offs, highlighting discrete flow matching as a promising direction for advancing NAR ASR. </p>
<blockquote>
<p>扩散和基于流的非自回归（NAR）模型在大规模语言建模方面显示出强大的潜力，然而它们在自动语音识别（ASR）方面的潜力尚未得到广泛探索。我们提出了Drax，这是一个用于ASR的离散流匹配框架，可以实现高效的并行解码。为了更好地将训练与推理对齐，我们构建了一个受音频影响的概率路径，该路径通过类似于可能的中间推理错误的轨迹来引导模型，而不是直接向目标转换添加随机噪声。我们的理论分析将泛化差距与训练和推理占用之间的分歧联系起来，这由累积速度误差控制，从而激发了我们的设计选择。经验评估表明，我们的方法在保证识别准确率的同时，提供了改进的准确性-效率权衡，凸显出离散流匹配在推进NAR ASR方面是一个有前景的方向。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.04162v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>扩散与流基非自回归（NAR）模型在大型语言建模中展现出强大的潜力，但在自动语音识别（ASR）领域的应用仍待探索。本文提出一种名为Drax的离散流匹配框架，用于ASR，可实现高效并行解码。为更好地将训练与推理对齐，本文构建了一个受音频影响的概率路径，通过引导模型通过类似可能中间推理错误的轨迹，而非直接随机噪声到目标转换。本文的理论分析将泛化差距与训练和推理占用之间的分歧联系起来，受累积速度误差控制，从而验证了设计选择。实证评估表明，该方法在识别准确率方面达到了最先进的语音模型水平，同时提供了更好的准确性-效率权衡，突显出离散流匹配在推进NAR ASR方面的前景。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>扩散与流基非自回归模型在大型语言建模中具潜力，但在ASR领域应用尚待探索。</li>
<li>提出了名为Drax的离散流匹配框架，用于ASR，实现高效并行解码。</li>
<li>构建音频影响概率路径，使模型训练更贴近推理过程。</li>
<li>理论分析显示泛化差距与训练、推理占用分歧有关，受累积速度误差控制。</li>
<li>离散流匹配框架在提升ASR性能方面具有潜力。</li>
<li>实证评估表明，该方法识别准确率与最先进的语音模型相当。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.04162">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic-private.zhihu.com/v2-a9308536b3be96919ca53496665fbc19~resize:0:q75.jpg?source=1f5c5e47&expiration=1760043707&auth_key=1760043707-0-0-ad869f9f3771f78c2e59d30456633137&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-d9d2099b8290edd8035925717cd583ea~resize:0:q75.jpg?source=1f5c5e47&expiration=1760043714&auth_key=1760043714-0-0-d0ed0923f6ef7058282ec8ab6f2165cf&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-dc92bebdd786aa16bcc1b03d173e9856~resize:0:q75.jpg?source=1f5c5e47&expiration=1760043721&auth_key=1760043721-0-0-d9e3a782faeab5245082b84233291918&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="GDiffuSE-Diffusion-based-speech-enhancement-with-noise-model-guidance"><a href="#GDiffuSE-Diffusion-based-speech-enhancement-with-noise-model-guidance" class="headerlink" title="GDiffuSE: Diffusion-based speech enhancement with noise model guidance"></a>GDiffuSE: Diffusion-based speech enhancement with noise model guidance</h2><p><strong>Authors:Efrayim Yanir, David Burshtein, Sharon Gannot</strong></p>
<p>This paper introduces a novel speech enhancement (SE) approach based on a denoising diffusion probabilistic model (DDPM), termed Guided diffusion for speech enhancement (GDiffuSE). In contrast to conventional methods that directly map noisy speech to clean speech, our method employs a lightweight helper model to estimate the noise distribution, which is then incorporated into the diffusion denoising process via a guidance mechanism. This design improves robustness by enabling seamless adaptation to unseen noise types and by leveraging large-scale DDPMs originally trained for speech generation in the context of SE. We evaluate our approach on noisy signals obtained by adding noise samples from the BBC sound effects database to LibriSpeech utterances, showing consistent improvements over state-of-the-art baselines under mismatched noise conditions. Examples are available at our project webpage. </p>
<blockquote>
<p>本文介绍了一种基于降噪扩散概率模型（DDPM）的新型语音增强（SE）方法，称为用于语音增强的引导扩散（GDiffuSE）。与传统的直接映射带噪语音到清洁语音的方法不同，我们的方法采用轻量级辅助模型来估计噪声分布，然后通过引导机制将其纳入扩散去噪过程。这种设计通过无缝适应未见过的噪声类型以及利用最初为语音增强背景下的语音生成而训练的大规模DDPM，提高了稳健性。我们在通过将从BBC音效数据库获得的噪声样本添加到LibriSpeech话语中而获得的带噪信号上评估了我们的方法，在噪声条件不匹配的情境下，相对于最新基线技术，显示出了一致性的改进。示例可在我们的项目网页上找到。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.04157v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>基于去噪扩散概率模型的语音增强新方法介绍。该方法采用轻量级辅助模型估计噪声分布，并将其通过指导机制融入扩散去噪过程，提高了对不同噪声类型的适应性和稳健性。在LibriSpeech语音数据上添加BBC音效库噪声样本进行评估，显示了在不匹配噪声条件下的改进效果。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>介绍了基于去噪扩散概率模型（DDPM）的语音增强新方法GDiffuSE。</li>
<li>GDiffuSE采用辅助模型估计噪声分布，并融入扩散去噪过程。</li>
<li>GDiffuSE通过指导机制提高了对不同噪声类型的适应性。</li>
<li>在LibriSpeech数据上添加BBC音效库噪声样本进行评估。</li>
<li>GDiffuSE在不匹配噪声条件下表现优于现有先进技术。</li>
<li>GDiffuSE方法通过融合噪声估计和扩散去噪，提高了语音增强的效果。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.04157">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic-private.zhihu.com/v2-d94b6683fb55719a1383392020347b1a~resize:0:q75.jpg?source=1f5c5e47&expiration=1760043729&auth_key=1760043729-0-0-91b01bf45f943b91bd19c25306e2f201&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-9a77f1fa0b0d71fb7c2d794c6885175e~resize:0:q75.jpg?source=1f5c5e47&expiration=1760043736&auth_key=1760043736-0-0-a2c9ca734b09eaab64aa1e410a582c64&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-e6af7a2f943289c879fa574123fe6ccb~resize:0:q75.jpg?source=1f5c5e47&expiration=1760043742&auth_key=1760043742-0-0-f19fdf9c44f1e29ce0855dabad107656&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-525c1435bc62eadb0c71ccf25e1debcd~resize:0:q75.jpg?source=1f5c5e47&expiration=1760043748&auth_key=1760043748-0-0-62acdec0c8fdb9a60e6ad69a5c935d7c&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-4acb0188d51b2fc2de11da4892aa900d~resize:0:q75.jpg?source=1f5c5e47&expiration=1760043755&auth_key=1760043755-0-0-45eb140170327da311961781203f8f76&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="MoME-Mixture-of-Matryoshka-Experts-for-Audio-Visual-Speech-Recognition"><a href="#MoME-Mixture-of-Matryoshka-Experts-for-Audio-Visual-Speech-Recognition" class="headerlink" title="MoME: Mixture of Matryoshka Experts for Audio-Visual Speech Recognition"></a>MoME: Mixture of Matryoshka Experts for Audio-Visual Speech Recognition</h2><p><strong>Authors:Umberto Cappellazzo, Minsu Kim, Pingchuan Ma, Honglie Chen, Xubo Liu, Stavros Petridis, Maja Pantic</strong></p>
<p>Large language models (LLMs) have recently shown strong potential in audio-visual speech recognition (AVSR), but their high computational demands and sensitivity to token granularity limit their practicality in resource-constrained settings. Token compression methods can reduce inference cost, but they require fixing a compression rate in advance and produce a single fixed-length output, offering no flexibility to balance information density and efficiency at inference time. Matryoshka representation learning (MRL) addresses this by enabling a single model to operate across multiple token granularities, allowing compression rates to be adjusted dynamically. However, current MRL-based methods treat each scale independently during training, limiting cross-scale generalization, robustness at high compression, and interpretability. To overcome these limitations, we propose MoME (Mixture of Matryoshka Experts), a novel framework that integrates sparse Mixture-of-Experts (MoE) into MRL-based LLMs for AVSR. MoME augments a frozen LLM with top-k routed and shared experts, allowing dynamic capacity allocation across scales and modalities. A shared router promotes consistent expert activation across granularities, enabling compressed sequences to benefit from representations learned at lower compression. Experiments on LRS2 and LRS3 demonstrate that MoME achieves state-of-the-art performance across AVSR, ASR, and VSR tasks, while requiring significantly fewer parameters and maintaining robustness under noise. MoME unifies the adaptability of MRL with the efficiency of MoE, offering a scalable and interpretable solution for resource-aware speech recognition. </p>
<blockquote>
<p>大型语言模型（LLM）在音频视觉语音识别（AVSR）方面最近显示出强大的潜力，但其高计算需求和针对令牌粒度的敏感性限制了其在资源受限环境中的实用性。令牌压缩方法可以降低推理成本，但它们需要提前确定压缩率，并产生单一固定长度的输出，无法在推理时平衡信息密度和效率。Matryoshka表示学习（MRL）通过使单个模型能够在多个令牌粒度上运行，从而能够动态调整压缩率来解决这一问题。然而，当前的MRL方法在处理训练时独立处理每个规模，这限制了跨规模泛化、高压缩时的稳健性和可解释性。为了克服这些局限性，我们提出了MoME（Matryoshka专家混合物），这是一种将稀疏的专家混合物（MoE）集成到基于MRL的LLM中的新型框架，用于AVSR。MoME通过顶级k路由和共享专家增强冻结的LLM，允许跨规模和跨模态的动态容量分配。共享路由器促进了跨粒度的专家激活的一致性，使得压缩序列能够受益于较低压缩下学习的表示。在LRS2和LRS3上的实验表明，MoME在AVSR、ASR和VSR任务上达到了最新性能水平，同时显著减少了参数需求，并在噪声下保持了稳健性。MoME将MRL的适应性与MoE的效率统一起来，为资源感知语音识别提供了可扩展和可解释的解决方案。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.04136v1">PDF</a> NeurIPS 2025</p>
<p><strong>摘要</strong></p>
<p>大型语言模型在音视频语音识别领域展现出巨大潜力，但其高计算需求和令牌粒度敏感性限制了其在资源受限环境中的应用。MoME（混合马托什卡专家）作为一种新颖框架，融合了稀疏混合专家与基于马托什卡的递归学习模型，实现动态容量跨尺度和模态分配，能够在资源受限的情况下有效提升音频识别的效果与效率。实验结果在LRS2和LRS3数据集上证明了MoME在音视频语音识别、语音识别和视频识别任务上的表现均达到业界最佳水平，同时显著减少了参数需求并保持了抗噪性。MoME统一了马托什卡递归学习的适应性和混合专家的效率，为资源感知语音识别提供了可扩展和可解释的解决方案。</p>
<p><strong>关键见解</strong></p>
<ol>
<li>大型语言模型在音视频语音识别中表现出强大的潜力，但资源受限环境下存在高计算需求和令牌粒度敏感性挑战。</li>
<li>传统令牌压缩方法无法平衡信息密度和推理效率，而马托什卡表示学习能调整压缩率但缺乏跨尺度泛化和鲁棒性。</li>
<li>MoME框架融合了稀疏混合专家与基于马托什卡的递归学习模型，允许动态容量分配并促进跨尺度和模态的一致性专家激活。</li>
<li>MoME通过增强冻结的大型语言模型并共享专家路由，使压缩序列能够受益于较低压缩时的表示学习。</li>
<li>在LRS2和LRS3数据集上的实验表明MoME在音视频语音识别、语音识别和视频识别任务上实现了业界最佳性能。</li>
<li>MoME相较于传统方法显著减少了参数需求并保持了良好的抗噪性。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.04136">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic-private.zhihu.com/v2-30ded001a72afca7e8d7d504e48c7304~resize:0:q75.jpg?source=1f5c5e47&expiration=1760043762&auth_key=1760043762-0-0-072e09927537bb3b5280e5e9b970976a&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-d1d8e9a48e8c6df2bdf6e5e93b1a478b~resize:0:q75.jpg?source=1f5c5e47&expiration=1760043769&auth_key=1760043769-0-0-640df219631b4b07f8a98e297d0d8dff&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-671778d3c626e52b64a30376f0777ce2~resize:0:q75.jpg?source=1f5c5e47&expiration=1760043776&auth_key=1760043776-0-0-43831fd0a8bdb77f793b9b6fae0e20d4&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="Cross-Lingual-Multi-Granularity-Framework-for-Interpretable-Parkinson’s-Disease-Diagnosis-from-Speech"><a href="#Cross-Lingual-Multi-Granularity-Framework-for-Interpretable-Parkinson’s-Disease-Diagnosis-from-Speech" class="headerlink" title="Cross-Lingual Multi-Granularity Framework for Interpretable Parkinson’s   Disease Diagnosis from Speech"></a>Cross-Lingual Multi-Granularity Framework for Interpretable Parkinson’s   Disease Diagnosis from Speech</h2><p><strong>Authors:Ilias Tougui, Mehdi Zakroum, Mounir Ghogho</strong></p>
<p>Parkinson’s Disease (PD) affects over 10 million people worldwide, with speech impairments in up to 89% of patients. Current speech-based detection systems analyze entire utterances, potentially overlooking the diagnostic value of specific phonetic elements. We developed a granularity-aware approach for multilingual PD detection using an automated pipeline that extracts time-aligned phonemes, syllables, and words from recordings. Using Italian, Spanish, and English datasets, we implemented a bidirectional LSTM with multi-head attention to compare diagnostic performance across the different granularity levels. Phoneme-level analysis achieved superior performance with AUROC of 93.78% +- 2.34% and accuracy of 92.17% +- 2.43%. This demonstrates enhanced diagnostic capability for cross-linguistic PD detection. Importantly, attention analysis revealed that the most informative speech features align with those used in established clinical protocols: sustained vowels (&#x2F;a&#x2F;, &#x2F;e&#x2F;, &#x2F;o&#x2F;, &#x2F;i&#x2F;) at phoneme level, diadochokinetic syllables (&#x2F;ta&#x2F;, &#x2F;pa&#x2F;, &#x2F;la&#x2F;, &#x2F;ka&#x2F;) at syllable level, and &#x2F;pataka&#x2F; sequences at word level. Source code will be available at <a target="_blank" rel="noopener" href="https://github.com/jetliqs/clearpd">https://github.com/jetliqs/clearpd</a>. </p>
<blockquote>
<p>帕金森病（PD）影响全球超过1000万人，其中高达89%的患者存在语言障碍。当前的基于语音的检测系统分析整个话语，可能会忽略特定语音元素在诊断中的价值。我们开发了一种用于多语言PD检测的粒度感知方法，使用自动化管道从录音中提取时间对齐的音素、音节和单词。我们使用意大利语、西班牙语和英语数据集，通过双向LSTM和多头注意力机制实现了对不同粒度级别的诊断性能的比较。音素级分析取得了卓越的性能，曲线下面积（AUROC）为93.78%±2.34%，准确度为92.17%±2.43%。这证明了跨语言PD检测的增强诊断能力。重要的是，注意力分析表明，最具信息量的语音特征符合已建立的临床协议中使用的特征：音素级的持续元音（&#x2F;a&#x2F;、&#x2F;e&#x2F;、&#x2F;o&#x2F;、&#x2F;i&#x2F;），音节级的连续动觉音节（&#x2F;ta&#x2F;、&#x2F;pa&#x2F;、&#x2F;la&#x2F;、&#x2F;ka&#x2F;），以及单词级别的&#x2F;pataka&#x2F;序列。源代码将发布在<a target="_blank" rel="noopener" href="https://github.com/jetliqs/clearpd%E3%80%82">https://github.com/jetliqs/clearpd。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.03758v1">PDF</a> </p>
<p><strong>Summary</strong>：<br>帕金森病影响全球超过千万人群，患者中有高达89%存在语音障碍。传统语音检测体系分析整个话语，可能忽略特定音素对诊断的价值。研究者开发了一种粒度感知的多语言帕金森病检测法，通过自动化流程提取时间对齐的音素、音节和单词。研究使用意大利语、西班牙语和英语数据集，采用双向LSTM和多头注意力机制，对比不同粒度级别的诊断性能。音素级别的分析表现最佳，AUC达到93.78%±2.34%，准确率92.17%±2.43%，显示跨语言帕金森病诊断的增强能力。注意力分析揭示，最具有诊断意义的语音特征与临床协议中的一致：音素级别的持续元音（如&#x2F;a&#x2F;、&#x2F;e&#x2F;、&#x2F;o&#x2F;、&#x2F;i&#x2F;），音节级别的连续动觉音节（如&#x2F;ta&#x2F;、&#x2F;pa&#x2F;、&#x2F;la&#x2F;、&#x2F;ka&#x2F;），单词级别的&#x2F;pataka序列等。相关源代码将在<a target="_blank" rel="noopener" href="https://github.com/jetliqs/clearpd">链接</a>公开。</p>
<p><strong>Key Takeaways</strong>：</p>
<ol>
<li>帕金森病影响广泛，多数患者存在语音障碍。</li>
<li>传统语音检测系统可能忽略特定音素在诊断中的重要性。</li>
<li>研究者采用粒度感知的多语言检测法，以自动化流程分析音素、音节和单词。</li>
<li>音素级别分析表现最佳，显示出跨语言诊断的增强能力。</li>
<li>诊断性语音特征包括音素级别的持续元音、音节级别的连续动觉音节及特定单词序列。</li>
<li>该研究提供了开源代码以供公众参考和使用。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.03758">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic-private.zhihu.com/v2-994b22bfaf75ba86bde9398c53d1f0cf~resize:0:q75.jpg?source=1f5c5e47&expiration=1760043783&auth_key=1760043783-0-0-6eb075f0df41d5d719d4bf9290d5e52b&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-a0557f0bb804313e8d414e5577c437f6~resize:0:q75.jpg?source=1f5c5e47&expiration=1760043791&auth_key=1760043791-0-0-9cf875623591837f363c8e967964e6bb&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-62f6aa37d94e36105729840453c3fd77~resize:0:q75.jpg?source=1f5c5e47&expiration=1760043797&auth_key=1760043797-0-0-051232939facfd4a9d5698e56da0fb1d&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-8cddd92640982572724b18dcc5849009~resize:0:q75.jpg?source=1f5c5e47&expiration=1760043804&auth_key=1760043804-0-0-14106232bb0c32738fdfb83ec5cadf0d&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-df3bf8ce196f136a5352cc9eabff453c~resize:0:q75.jpg?source=1f5c5e47&expiration=1760043810&auth_key=1760043810-0-0-ce91b9d46be27a0fffaaf12be2d25ac2&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="Adapting-Diarization-Conditioned-Whisper-for-End-to-End-Multi-Talker-Speech-Recognition"><a href="#Adapting-Diarization-Conditioned-Whisper-for-End-to-End-Multi-Talker-Speech-Recognition" class="headerlink" title="Adapting Diarization-Conditioned Whisper for End-to-End Multi-Talker   Speech Recognition"></a>Adapting Diarization-Conditioned Whisper for End-to-End Multi-Talker   Speech Recognition</h2><p><strong>Authors:Martin Kocour, Martin Karafiat, Alexander Polok, Dominik Klement, Lukáš Burget, Jan Černocký</strong></p>
<p>We propose a speaker-attributed (SA) Whisper-based model for multi-talker speech recognition that combines target-speaker modeling with serialized output training (SOT). Our approach leverages a Diarization-Conditioned Whisper (DiCoW) encoder to extract target-speaker embeddings, which are concatenated into a single representation and passed to a shared decoder. This enables the model to transcribe overlapping speech as a serialized output stream with speaker tags and timestamps. In contrast to target-speaker ASR systems such as DiCoW, which decode each speaker separately, our approach performs joint decoding, allowing the decoder to condition on the context of all speakers simultaneously. Experiments show that the model outperforms existing SOT-based approaches and surpasses DiCoW on multi-talker mixtures (e.g., LibriMix). </p>
<blockquote>
<p>我们提出了一种基于说话人属性（SA）的基于耳语的多说话人语音识别模型，该模型结合了目标说话人建模和序列化输出训练（SOT）。我们的方法利用以识别为中心的耳语（DiCoW）编码器提取目标说话人的嵌入，并将其串联成一个单一表示，然后传递给共享解码器。这使得模型能够将重叠的语音转录为带有说话人标签和时间戳的序列化输出流。与诸如DiCoW之类的目标说话人ASR系统不同，这些系统分别解码每个说话人，而我们的方法执行联合解码，允许解码器同时考虑所有说话人的上下文。实验表明，该模型优于现有的基于SOT的方法，并且在多说话人混合（例如LibriMix）方面超过了DiCoW。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.03723v1">PDF</a> </p>
<p><strong>Summary</strong>：针对多说话人语音识别问题，我们提出了一种基于说话人属性（SA）的Whisper模型，该模型结合了目标说话人建模和序列化输出训练（SOT）。通过使用Diarization-Conditioned Whisper（DiCoW）编码器提取目标说话人的嵌入信息，然后将它们组合成一个单一表示并传递给共享解码器。该模型可将重叠语音转录为带有说话人标签和时间戳的序列化输出流。与单独解码每个说话人的目标说话人ASR系统（如DiCoW）相比，我们的方法采用联合解码，允许解码器同时考虑所有说话人的上下文。实验表明，该模型优于现有的基于SOT的方法，并在多说话人混合（例如LibriMix）方面超过了DiCoW。</p>
<p><strong>Key Takeaways</strong>：</p>
<ol>
<li>提出了基于说话人属性（SA）的Whisper模型，用于多说话人语音识别。</li>
<li>结合了目标说话人建模和序列化输出训练（SOT）。</li>
<li>使用DiCoW编码器提取目标说话人的嵌入信息。</li>
<li>模型将重叠语音转录为带有说话人标签和时间戳的序列化输出。</li>
<li>相比目标说话人ASR系统的单独解码，采用了联合解码方法。</li>
<li>模型在多说话人混合方面的性能超过了现有方法和DiCoW。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.03723">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic-private.zhihu.com/v2-2199531bb49ee5c2e601f8376a2c74a1~resize:0:q75.jpg?source=1f5c5e47&expiration=1760043818&auth_key=1760043818-0-0-e559647ac046b8c1c1b0214e110d8d38&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-d03b000d9cdd13c177a2c92f7db2992a~resize:0:q75.jpg?source=1f5c5e47&expiration=1760043825&auth_key=1760043825-0-0-1e33e3a01469d9e15ccec868e1dfcc11&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-f320f479a50f5fc82bfcfeeed4f22b23~resize:0:q75.jpg?source=1f5c5e47&expiration=1760043831&auth_key=1760043831-0-0-11dc5c349fa74bb8001b24170abcf17c&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-ed0dc42ce470d6c0fc337c9457935add~resize:0:q75.jpg?source=1f5c5e47&expiration=1760043839&auth_key=1760043839-0-0-c51cb952e52e46553aa1089dc2e7668b&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-5e819bff4abae987ebaacafd6461feb1~resize:0:q75.jpg?source=1f5c5e47&expiration=1760043845&auth_key=1760043845-0-0-ad39b4b8c3b3e28f8f84b7cba1ec5b9b&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-ab04f2c62d48483683f27a0d7356c6aa~resize:0:q75.jpg?source=1f5c5e47&expiration=1760043852&auth_key=1760043852-0-0-9f16b3f4e3824915352465ba801c1122&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-edf65a29187af959b94b2554e0a6ef5b~resize:0:q75.jpg?source=1f5c5e47&expiration=1760043858&auth_key=1760043858-0-0-ee197c5aadf7f8e5e1537bd1b55e9cca&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="HiKE-Hierarchical-Evaluation-Framework-for-Korean-English-Code-Switching-Speech-Recognition"><a href="#HiKE-Hierarchical-Evaluation-Framework-for-Korean-English-Code-Switching-Speech-Recognition" class="headerlink" title="HiKE: Hierarchical Evaluation Framework for Korean-English   Code-Switching Speech Recognition"></a>HiKE: Hierarchical Evaluation Framework for Korean-English   Code-Switching Speech Recognition</h2><p><strong>Authors:Gio Paik, Yongbeom Kim, Soungmin Lee, Sangmin Ahn, Chanwoo Kim</strong></p>
<p>Despite advances in multilingual automatic speech recognition (ASR), code-switching (CS), the mixing of languages within an utterance common in daily speech, remains a severely underexplored challenge. In this paper, we introduce HiKE: the Hierarchical Korean-English code-switching benchmark, the first globally accessible evaluation framework for Korean-English CS, aiming to provide a means for the precise evaluation of multilingual ASR models and to foster research in the field. The proposed framework not only consists of high-quality, natural CS data across various topics, but also provides meticulous loanword labels and a hierarchical CS-level labeling scheme (word, phrase, and sentence) that together enable a systematic evaluation of a model’s ability to handle each distinct level of code-switching. Through evaluations of diverse multilingual ASR models and fine-tuning experiments, this paper demonstrates that although most multilingual ASR models initially exhibit inadequate CS-ASR performance, this capability can be enabled through fine-tuning with synthetic CS data. HiKE is available at <a target="_blank" rel="noopener" href="https://github.com/ThetaOne-AI/HiKE">https://github.com/ThetaOne-AI/HiKE</a> </p>
<blockquote>
<p>尽管多语种自动语音识别（ASR）有所发展，但语言切换（CS）仍然是一个被严重忽视的挑战，在日常对话中经常出现将不同语言混合在一起的现象。在本文中，我们介绍了HiKE：分层韩英双语切换基准测试，这是首个全球可访问的韩英双语切换评估框架，旨在提供一种精确评估多语种ASR模型的手段，并推动该领域的研究。所提出的框架不仅包括跨各种主题的高质量、自然双语切换数据，还包括细致的外来词标签和分层双语切换级别标签方案（单词、短语和句子），这些共同使模型能够系统地处理每个独特的双语切换级别能力进行评价。通过对各种多语种ASR模型的评估和微调实验，本文表明，尽管大多数多语种ASR模型最初表现出不足的双语切换自动语音识别性能，但通过利用合成双语切换数据进行微调可以增强这一功能。HiKE可访问地址是<a target="_blank" rel="noopener" href="https://github.com/ThetaOne-AI/HiKE%E3%80%82">https://github.com/ThetaOne-AI/HiKE。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.24613v2">PDF</a> Updated table 2 and 3 due to bug fix, Under Review</p>
<p><strong>Summary</strong></p>
<p>本文介绍了HiKE：一个面向韩语-英语代码切换的层次性评价框架。该框架旨在为精确评估多语言自动语音识别（ASR）模型提供手段，并推动相关领域的研究。HiKE不仅包含高质量的自然代码切换数据，还提供了详细的借词标签和层次性的代码切换级别标签方案（单词、短语和句子）。通过对多种多语言ASR模型的评估以及微调实验，证明了虽然大多数多语言ASR模型在初始阶段对代码切换的识别性能不足，但通过合成代码切换数据的微调可以提升其性能。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>代码切换（CS）在日常语言中普遍存在，但在多语言自动语音识别（ASR）中仍是一个被严重忽视的挑战。</li>
<li>HiKE是首个面向韩语-英语代码切换的评价框架，旨在提供精确评估多语言ASR模型的手段。</li>
<li>HiKE框架包含高质量的自然代码切换数据，覆盖各种主题。</li>
<li>HiKE提供了详细的借词标签和层次性的代码切换级别标签方案，包括单词、短语和句子。</li>
<li>通过对多种多语言ASR模型的评估，发现大多数模型在初始阶段对代码切换的识别性能不足。</li>
<li>通过使用合成代码切换数据对模型进行微调，可以提高模型对代码切换的识别性能。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.24613">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic-private.zhihu.com/v2-76f0dc5497f64a2f87c7dd3b0480433b~resize:0:q75.jpg?source=1f5c5e47&expiration=1760043865&auth_key=1760043865-0-0-907bfd7d28a665659ef30e2f7fdd6024&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-a0abd65a3769c5d038480baa850fcbc4~resize:0:q75.jpg?source=1f5c5e47&expiration=1760043873&auth_key=1760043873-0-0-538240f7136ce640fb1de301d78b02c3&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-418afd003e08225c309f48fcdb719946~resize:0:q75.jpg?source=1f5c5e47&expiration=1760043879&auth_key=1760043879-0-0-9ffd6f170e9101046b1a3f87863e00bc&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-1cd3afc4f35a0609d308b1b6cd06e784~resize:0:q75.jpg?source=1f5c5e47&expiration=1760043886&auth_key=1760043886-0-0-c5fcde73d4c35d87f37f16d9400554d4&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-58399dc9ae28a4c33b915b6fc56ef742~resize:0:q75.jpg?source=1f5c5e47&expiration=1760043892&auth_key=1760043892-0-0-3af7bbd62fe52cb118f52bc1ddf76bda&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="Fun-ASR-Technical-Report"><a href="#Fun-ASR-Technical-Report" class="headerlink" title="Fun-ASR Technical Report"></a>Fun-ASR Technical Report</h2><p><strong>Authors:Keyu An, Yanni Chen, Chong Deng, Changfeng Gao, Zhifu Gao, Bo Gong, Xiangang Li, Yabin Li, Xiang Lv, Yunjie Ji, Yiheng Jiang, Bin Ma, Haoneng Luo, Chongjia Ni, Zexu Pan, Yiping Peng, Zhendong Peng, Peiyao Wang, Hao Wang, Wen Wang, Wupeng Wang, Biao Tian, Zhentao Tan, Nan Yang, Bin Yuan, Jieping Ye, Jixing Yu, Qinglin Zhang, Kun Zou, Han Zhao, Shengkui Zhao, Jingren Zhou</strong></p>
<p>In recent years, automatic speech recognition (ASR) has witnessed transformative advancements driven by three complementary paradigms: data scaling, model size scaling, and deep integration with large language models (LLMs). However, LLMs are prone to hallucination, which can significantly degrade user experience in real-world ASR applications. In this paper, we present Fun-ASR, a large-scale, LLM-based ASR system that synergistically combines massive data, large model capacity, LLM integration, and reinforcement learning to achieve state-of-the-art performance across diverse and complex speech recognition scenarios. Moreover, Fun-ASR is specifically optimized for practical deployment, with enhancements in streaming capability, noise robustness, code-switching, hotword customization, and satisfying other real-world application requirements. Experimental results show that while most LLM-based ASR systems achieve strong performance on open-source benchmarks, they often underperform on real industry evaluation sets. Thanks to production-oriented optimizations, Fun-ASR achieves state-of-the-art performance on real application datasets, demonstrating its effectiveness and robustness in practical settings. </p>
<blockquote>
<p>近年来，自动语音识别（ASR）在数据规模化、模型规模扩展以及与大型语言模型（LLM）的深度整合三大互补范式推动下取得了革命性的进展。然而，大型语言模型容易出现幻觉，这在现实世界的ASR应用中可能会显著地降低用户体验。在本文中，我们提出了Fun-ASR，这是一个基于大型语言模型的ASR系统，它协同结合了大规模数据、大型模型容量、语言模型集成和强化学习，在多样且复杂的语音识别场景中实现了最先进的性能。此外，Fun-ASR针对实际部署进行了专门优化，增强了流式处理功能、噪声鲁棒性、代码切换、热词自定义以及其他现实世界应用的要求。实验结果表明，虽然大多数基于大型语言模型的ASR系统在开源基准测试上表现强劲，但它们在真实的行业评估集上往往表现不佳。得益于面向生产的优化，Fun-ASR在真实应用数据集上实现了最先进的性能，证明了其在现实环境中的有效性和鲁棒性。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.12508v3">PDF</a> Authors are listed in alphabetical order</p>
<p><strong>Summary</strong></p>
<p>近年来，随着数据规模的扩大、模型容量的增长和大型语言模型（LLM）的深度集成，自动语音识别（ASR）取得了突破性进展。然而，LLM易产生幻觉，这可能会显著影响真实世界ASR应用的用户体验。本文提出了Fun-ASR系统，这是一个基于大规模LLM的ASR系统，通过结合大规模数据、模型容量、LLM集成和强化学习，实现了不同复杂语音识别场景的卓越性能。此外，Fun-ASR针对实际部署进行了优化，增强了流处理能力、噪声鲁棒性、代码切换、热词定制等能力，满足其他真实世界应用的需求。实验结果表明，虽然大多数基于LLM的ASR系统在开源基准测试上表现强劲，但在真实的行业评估集上往往表现不佳。得益于面向生产的优化，Fun-ASR在真实应用数据集上达到了最新技术水平，证明了其在实践环境中的有效性和稳健性。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>自动语音识别（ASR）的进展得益于数据规模、模型容量和大型语言模型（LLM）的集成。</li>
<li>LLM易产生幻觉，影响真实世界ASR应用的用户体验。</li>
<li>Fun-ASR系统是一个基于LLM的ASR系统，结合了大规模数据、模型容量和强化学习，实现卓越性能。</li>
<li>Fun-ASR针对实际部署进行了优化，包括流处理能力、噪声鲁棒性、代码切换、热词定制等能力的提升。</li>
<li>大多数基于LLM的ASR系统在开源基准测试上表现良好，但在真实行业评估集上表现不佳。</li>
<li>Fun-ASR在真实应用数据集上达到了最新技术水平。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.12508">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic-private.zhihu.com/v2-c73e96c90d61ea9a43e002a6552e160b~resize:0:q75.jpg?source=1f5c5e47&expiration=1760043899&auth_key=1760043899-0-0-9c31032689572aff11a102900f4ba3d6&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-73d5efc03acccd51c062a631d55e3c88~resize:0:q75.jpg?source=1f5c5e47&expiration=1760043906&auth_key=1760043906-0-0-8c2b56529c1857f52a771978905bb681&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-38ded6572a631e2c2b1217d09dbacd7d~resize:0:q75.jpg?source=1f5c5e47&expiration=1760043912&auth_key=1760043912-0-0-0fdf5da4380e15f4dd7df3320824fb69&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-6964cceb713f79415a39cd4a8b279316~resize:0:q75.jpg?source=1f5c5e47&expiration=1760043919&auth_key=1760043919-0-0-3d89c508e2c439898dd2385c747ee357&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="RooseBERT-A-New-Deal-For-Political-Language-Modelling"><a href="#RooseBERT-A-New-Deal-For-Political-Language-Modelling" class="headerlink" title="RooseBERT: A New Deal For Political Language Modelling"></a>RooseBERT: A New Deal For Political Language Modelling</h2><p><strong>Authors:Deborah Dore, Elena Cabrio, Serena Villata</strong></p>
<p>The increasing amount of political debates and politics-related discussions calls for the definition of novel computational methods to automatically analyse such content with the final goal of lightening up political deliberation to citizens. However, the specificity of the political language and the argumentative form of these debates (employing hidden communication strategies and leveraging implicit arguments) make this task very challenging, even for current general-purpose pre-trained Language Models. To address this issue, we introduce a novel pre-trained Language Model for political discourse language called RooseBERT. Pre-training a language model on a specialised domain presents different technical and linguistic challenges, requiring extensive computational resources and large-scale data. RooseBERT has been trained on large political debate and speech corpora (8K debates, each composed of several sub-debates on different topics) in English. To evaluate its performances, we fine-tuned it on four downstream tasks related to political debate analysis, i.e., stance detection, sentiment analysis, argument component detection and classification, and argument relation prediction and classification. Our results demonstrate significant improvements over general-purpose Language Models on these four tasks, highlighting how domain-specific pre-training enhances performance in political debate analysis. We release RooseBERT for the research community. </p>
<blockquote>
<p>随着政治辩论和政治相关讨论的日益增多，需要定义新的计算方法来自动分析这些内容，最终目标是为公民提供政治讨论的机会。然而，政治语言的特殊性和辩论的论证形式（采用隐蔽的沟通策略和利用隐含的论据）使得这一任务非常具有挑战性，即使对于当前的通用预训练语言模型也是如此。为了解决这一问题，我们引入了一种名为RooseBERT的政治话语语言预训练模型。在特定领域上预训练语言模型面临着不同的技术和语言挑战，需要巨大的计算资源和大规模的数据。RooseBERT是在英文的大规模政治辩论和演讲语料库（包含8K场辩论，每场辩论由不同主题的多个子辩论组成）上训练的。为了评估其性能，我们在与政治辩论分析相关的四个下游任务上进行了微调，即立场检测、情感分析、论证成分检测和分类以及论证关系预测和分类。我们的结果在这四个任务上显著优于通用语言模型，证明了领域特定预训练在政治辩论分析中的性能提升。我们发布RooseBERT以供研究界使用。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.03250v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>一种新的针对政治话语的预训练语言模型——RooseBERT被提出并介绍。该模型在大量政治辩论和演讲语料库上进行训练，适用于政治内容自动分析，有助于减轻公民的政治讨论负担。RooseBERT在四个与政治辩论分析相关的下游任务上的表现得到了显著的提升。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>政治辩论和讨论内容的自动分析需要新的计算方法。</li>
<li>RooseBERT是一种针对政治话语的预训练语言模型。</li>
<li>RooseBERT在大量政治辩论和演讲语料库上进行训练。</li>
<li>RooseBERT在四个下游任务上的表现优于通用语言模型。</li>
<li>RooseBERT包括立场检测、情感分析、论证成分检测和分类以及论证关系预测和分类等任务。</li>
<li>政治语言的特异性和辩论中的隐性沟通策略使该任务具有挑战性。</li>
<li>RooseBERT的发布旨在为研究领域提供助力。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.03250">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic-private.zhihu.com/v2-18a91354279ca7374c007c880f2e905a~resize:0:q75.jpg?source=1f5c5e47&expiration=1760043926&auth_key=1760043926-0-0-8a657311a26d6f84d6abedbd8e0c6d75&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-65660007887b0773df7b731aa1b47416~resize:0:q75.jpg?source=1f5c5e47&expiration=1760043933&auth_key=1760043933-0-0-6ae594cbb96f8e425c5d1a0013a7c6c1&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-a9fb4d621fbd8b5127a6ebfe6b19f01b~resize:0:q75.jpg?source=1f5c5e47&expiration=1760043939&auth_key=1760043939-0-0-ee77eb8e590856d3d03a6747fbb4cef5&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-ea618e9a1526fd22ac87db7a67423b4e~resize:0:q75.jpg?source=1f5c5e47&expiration=1760043946&auth_key=1760043946-0-0-0349156094bcca61b34a34928ab6afc1&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1><h2 id="Context-Biasing-for-Pronunciations-Orthography-Mismatch-in-Automatic-Speech-Recognition"><a href="#Context-Biasing-for-Pronunciations-Orthography-Mismatch-in-Automatic-Speech-Recognition" class="headerlink" title="Context Biasing for Pronunciations-Orthography Mismatch in Automatic   Speech Recognition"></a>Context Biasing for Pronunciations-Orthography Mismatch in Automatic   Speech Recognition</h2><p><strong>Authors:Christian Huber, Alexander Waibel</strong></p>
<p>Neural sequence-to-sequence systems deliver state-of-the-art performance for automatic speech recognition. When using appropriate modeling units, e.g., byte-pair encoded characters, these systems are in principal open vocabulary systems. In practice, however, they often fail to recognize words not seen during training, e.g., named entities, acronyms, or domain-specific special words. To address this problem, many context biasing methods have been proposed; however, for words with a pronunciation-orthography mismatch, these methods may still struggle. We propose a method which allows corrections of substitution errors to improve the recognition accuracy of such challenging words. Users can add corrections on the fly during inference. We show that with this method we get a relative improvement in biased word error rate of up to 8%, while maintaining a competitive overall word error rate. </p>
<blockquote>
<p>神经网络序列到序列系统在自动语音识别方面达到了最先进的性能。在使用适当的建模单元（例如，字节对编码字符）时，这些系统在原则上都是开放词汇系统。然而，在实践中，它们往往无法识别在训练期间未见过的单词，例如，命名实体、缩写或特定领域的特殊词汇。为了解决这一问题，已经提出了许多上下文偏向方法；但对于发音和书写不匹配的单字，这些方法可能仍然面临挑战。我们提出了一种方法，可以纠正替换错误，以提高此类具有挑战性单词的识别准确性。用户可以在推理过程中即时添加更正。我们证明，使用这种方法，可以在保持总体单词错误率竞争力的同时，将偏向单词错误率提高高达8%。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.18703v2">PDF</a> </p>
<p><strong>Summary</strong>：神经序列到序列系统在自动语音识别领域具有卓越性能。通过使用适当的建模单元，如字节对编码字符，这些系统原则上具有开放词汇。但在实践中，它们往往无法识别训练期间未见过的词汇，如实体名称、缩写或特定领域的特殊词汇。针对这一问题，我们提出了一种允许纠正替换错误的方法，以提高这些具有挑战性的单词的识别准确率。用户可以在推理过程中即时添加更正。我们显示使用此方法可使具有偏差的单词错误率相对提高达8%，同时保持整体单词错误率的竞争力。</p>
<p><strong>Key Takeaways</strong>：</p>
<ol>
<li>神经序列到序列系统在自动语音识别领域表现卓越，使用适当的建模单元具有开放词汇特点。</li>
<li>这些系统在识别训练期间未见词汇方面存在挑战，如命名实体、缩写和特定领域的特殊词汇。</li>
<li>针对具有挑战性的单词识别问题，提出了一种允许用户即时添加更正的方法来解决替换错误。</li>
<li>该方法可在保持整体单词错误率竞争力的同时，提高具有偏差的单词错误率达8%。</li>
<li>该方法增强了神经序列到序列系统的灵活性和适应性，使其更好地处理复杂的语音识别任务。</li>
<li>用户参与更正过程有助于提高系统的实用性和用户满意度。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.18703">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic-private.zhihu.com/v2-71f9eb8cff3fecdcaafee16fcd5eac60~resize:0:q75.jpg?source=1f5c5e47&expiration=1760043953&auth_key=1760043953-0-0-b258067d4cd2dad418b60104ae009e9a&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-ef234b7d871a61911752f606692b807b~resize:0:q75.jpg?source=1f5c5e47&expiration=1760043961&auth_key=1760043961-0-0-39593560a4828d2f5effebd6e9391c62&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-19"><a href="#-19" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        文章作者:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        文章链接:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-10-10/Speech/">https://kedreamix.github.io/Talk2Paper/Paper/2025-10-10/Speech/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        版权声明:
                    </i>
                </span>
                <span class="reprint-info">
                    本博客所有文章除特別声明外，均采用
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    许可协议。转载请注明来源
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Speech/">
                                    <span class="chip bg-color">Speech</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;上一篇</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-10-10/Face%20Swapping/">
                    <div class="card-image">
                        
                        <img src="https://pic-private.zhihu.com/v2-45810d09557cc88586579a49aefb5ef9~resize:0:q75.jpg?source=1f5c5e47&expiration=1760044523&auth_key=1760044523-0-0-b97931a7993fac155a158394a0cd4e4a&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" class="responsive-img" alt="Face Swapping">
                        
                        <span class="card-title">Face Swapping</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Face Swapping 方向最新论文已更新，请持续关注 Update in 2025-10-10  Unmasking Puppeteers Leveraging Biometric Leakage to Disarm   Impersonation in AI-based Videoconferencing
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-10-10
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Face-Swapping/" class="post-category">
                                    Face Swapping
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Face-Swapping/">
                        <span class="chip bg-color">Face Swapping</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                下一篇&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-10-10/%E6%97%A0%E7%9B%91%E7%9D%A3_%E5%8D%8A%E7%9B%91%E7%9D%A3_%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0/">
                    <div class="card-image">
                        
                        <img src="https://pic-private.zhihu.com/v2-9fe1b96dd5125dfe8097a60a3d3efe4e~resize:0:q75.jpg?source=1f5c5e47&expiration=1760041980&auth_key=1760041980-0-0-d8862cd40932120bbe80eeeee05baeef&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" class="responsive-img" alt="无监督/半监督/对比学习">
                        
                        <span class="card-title">无监督/半监督/对比学习</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            无监督/半监督/对比学习 方向最新论文已更新，请持续关注 Update in 2025-10-10  The best performance in the CARE 2025 -- Liver Task (LiSeg-Contrast)   Contrast-Aware Semi-Supervised Segmentation with Domain Generalization and   Test-Time Adaptation
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-10-10
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E6%97%A0%E7%9B%91%E7%9D%A3-%E5%8D%8A%E7%9B%91%E7%9D%A3-%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0/" class="post-category">
                                    无监督/半监督/对比学习
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E6%97%A0%E7%9B%91%E7%9D%A3-%E5%8D%8A%E7%9B%91%E7%9D%A3-%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0/">
                        <span class="chip bg-color">无监督/半监督/对比学习</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- 代码块功能依赖 -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- 代码语言 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- 代码块复制 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- 代码块收缩 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC 悬浮按钮. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* 修复文章卡片 div 的宽度. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // 切换TOC目录展开收缩的相关操作.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;站点总字数:&nbsp;<span
                        class="white-color">30341.4k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- 运行天数提醒. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // 区分是否有年份.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = '本站已运行 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                daysTip = '本站已運行 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = '本站已运行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = '本站已運行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="关注我的知乎: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">知</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- 搜索遮罩框 -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span>
            <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- 白天和黑夜主题 -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- 回到顶部按钮 -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // 只在桌面版网页启用特效
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- 雪花特效 -->
    

    <!-- 鼠标星星特效 -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--腾讯兔小巢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- 动态标签 -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Σ(っ °Д °;)っ诶，页面崩溃了嘛？", clearTimeout(st)) : (document.title = "φ(゜▽゜*)♪咦，又好了！", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
