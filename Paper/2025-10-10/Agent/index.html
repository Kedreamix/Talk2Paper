<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Agent">
    <meta name="description" content="Agent 方向最新论文已更新，请持续关注 Update in 2025-10-10  Multi-Objective Multi-Agent Path Finding with Lexicographic Cost   Preferences">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Agent | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loader页面消失采用渐隐的方式*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logo出现动画 */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//使用渐隐的方法淡出loading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//强制显示loading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">嘘~  正在从服务器偷取页面 . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>首页</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>标签</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>分类</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>归档</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="搜索" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="深色/浅色模式" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			首页
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			标签
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			分类
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			归档
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pic-private.zhihu.com/v2-1cb8a4b5ab6999a2190e744d64b8aadd~resize:0:q75.jpg?source=1f5c5e47&expiration=1760083139&auth_key=1760083139-0-0-9d2ec30427e235b33afe1d1e5f64497b&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Agent</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- 文章内容详情 -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Agent/">
                                <span class="chip bg-color">Agent</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Agent/" class="post-category">
                                Agent
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp;
                    2025-10-10
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>更新日期:&nbsp;&nbsp;
                    2025-10-10
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>文章字数:&nbsp;&nbsp;
                    13.5k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>阅读时长:&nbsp;&nbsp;
                    54 分
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>阅读次数:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>⚠️ 以下所有内容总结都来自于 大语言模型的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p>
</blockquote>
<h1 id="2025-10-10-更新"><a href="#2025-10-10-更新" class="headerlink" title="2025-10-10 更新"></a>2025-10-10 更新</h1><h2 id="Multi-Objective-Multi-Agent-Path-Finding-with-Lexicographic-Cost-Preferences"><a href="#Multi-Objective-Multi-Agent-Path-Finding-with-Lexicographic-Cost-Preferences" class="headerlink" title="Multi-Objective Multi-Agent Path Finding with Lexicographic Cost   Preferences"></a>Multi-Objective Multi-Agent Path Finding with Lexicographic Cost   Preferences</h2><p><strong>Authors:Pulkit Rustagi, Kyle Hollins Wray, Sandhya Saisubramanian</strong></p>
<p>Many real-world scenarios require multiple agents to coordinate in shared environments, while balancing trade-offs between multiple, potentially competing objectives. Current multi-objective multi-agent path finding (MO-MAPF) algorithms typically produce conflict-free plans by computing Pareto frontiers. They do not explicitly optimize for user-defined preferences, even when the preferences are available, and scale poorly with the number of objectives. We propose a lexicographic framework for modeling MO-MAPF, along with an algorithm \textit{Lexicographic Conflict-Based Search} (LCBS) that directly computes a single solution aligned with a lexicographic preference over objectives. LCBS integrates a priority-aware low-level $A^*$ search with conflict-based search, avoiding Pareto frontier construction and enabling efficient planning guided by preference over objectives. We provide insights into optimality and scalability, and empirically demonstrate that LCBS computes optimal solutions while scaling to instances with up to ten objectives – far beyond the limits of existing MO-MAPF methods. Evaluations on standard and randomized MAPF benchmarks show consistently higher success rates against state-of-the-art baselines, especially with increasing number of objectives. </p>
<blockquote>
<p>在现实世界中的许多场景中，需要在共享环境中协调多个代理，同时平衡多个可能相互竞争目标之间的权衡。现有的多目标多智能体路径搜索算法通常通过计算帕累托前沿来生成无冲突的规划。它们即使在有用户偏好时也不显式优化用户定义的偏好，并且随着目标数量的增加，其扩展性较差。我们提出了一种用于建模多目标多智能体路径搜索的词典编纂框架，以及一种直接计算与词典优先目标对齐的单解算法的字典优先冲突搜索算法（LCBS）。LCBS将优先级感知的低级A搜索与基于冲突搜索相结合，避免了帕累托前沿的构建，并通过目标偏好引导高效规划。我们深入探讨了最优性和可扩展性，并实证表明，LCBS在计算最优解的同时，能够扩展到多达十个目标的情况——远超现有MO-MAPF方法的极限。在标准化和随机化的MAPF基准测试上的评估显示，相较于最先进的基线技术，其成功率更高，尤其是在目标数量增加时更是如此。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.07276v1">PDF</a> 8 pages, 7 figures</p>
<p><strong>Summary</strong><br>多智能体在现实场景中协同工作时需要在共享环境中平衡多个潜在冲突目标之间的权衡。当前的多目标多智能体路径查找算法（MO-MAPF）主要通过计算帕累托前沿实现无冲突计划，但对于用户定义偏好的优化不明显，随着目标数量的增多其性能会变差。为此我们提出采用词法编码框架对MO-MAPF进行建模，并开发了一种算法，称为词法编码冲突搜索算法（LCBS），能直接计算符合词法偏好的单一解决方案。LCBS整合优先级感知的低级A^搜索与基于冲突搜索，避免了帕累托前沿的构建，使计划能够在目标偏好下高效执行。通过实证分析，我们证明了LCBS在计算最优解方面表现优秀，同时能够扩展到十个目标的情况——远超现有MO-MAPF方法的极限。在标准化和随机化的MAPF基准测试上，相较于最先进的基线测试，其成功率更高，尤其是在目标数量增加时。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>当前的多目标多智能体路径查找算法在应对现实场景中的多智能体协调时存在局限性，无法很好地平衡多个潜在冲突目标。</li>
<li>词法编码框架用于建模MO-MAPF能有效解决这一问题。</li>
<li>词法编码冲突搜索算法（LCBS）结合了优先级感知的低级A^搜索和基于冲突搜索的技术，避免计算帕累托前沿。</li>
<li>LCBS能直接计算符合用户定义的词法偏好的单一解决方案。</li>
<li>LCBS具有优秀的计算最优解的能力，并能扩展到处理大量目标的情况。</li>
<li>LCBS在标准化和随机化的MAPF基准测试上表现出较高的成功率。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.07276">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic-private.zhihu.com/v2-3fe13aac502f0ab5291904abeb49a780~resize:0:q75.jpg?source=1f5c5e47&expiration=1760083146&auth_key=1760083146-0-0-c5a866b717af06ab58fd35d7a247f1dd&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-ed1c9b3515cc8469d378c448d4450a89~resize:0:q75.jpg?source=1f5c5e47&expiration=1760083154&auth_key=1760083154-0-0-1cfc69f6cb02cbfea5a58b3f703813f1&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-1a823dc38a164f93e97fff8c3f58e49e~resize:0:q75.jpg?source=1f5c5e47&expiration=1760083161&auth_key=1760083161-0-0-bea996c04ca3b7643882738224258977&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-aa8d8154068c29e74ce0c0b8a52f943b~resize:0:q75.jpg?source=1f5c5e47&expiration=1760083167&auth_key=1760083167-0-0-245efef710d1d57093db9d229681298e&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-10-10\./crop_Agent/2510.07276v1/page_3_0.jpg" align="middle">
<img src="https://pic-private.zhihu.com/v2-526bddfaa3f91052ee6f11a11115aa0c~resize:0:q75.jpg?source=1f5c5e47&expiration=1760083239&auth_key=1760083239-0-0-6f4b28be8891da836db15478be32df76&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-b10a4503412aa344a978939c9b5c9a71~resize:0:q75.jpg?source=1f5c5e47&expiration=1760083246&auth_key=1760083246-0-0-6c92b88b3294ed6a3fa41e6214ee203f&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Customer-R1-Personalized-Simulation-of-Human-Behaviors-via-RL-based-LLM-Agent-in-Online-Shopping"><a href="#Customer-R1-Personalized-Simulation-of-Human-Behaviors-via-RL-based-LLM-Agent-in-Online-Shopping" class="headerlink" title="Customer-R1: Personalized Simulation of Human Behaviors via RL-based LLM   Agent in Online Shopping"></a>Customer-R1: Personalized Simulation of Human Behaviors via RL-based LLM   Agent in Online Shopping</h2><p><strong>Authors:Ziyi Wang, Yuxuan Lu, Yimeng Zhang, Jing Huang, Dakuo Wang</strong></p>
<p>Simulating step-wise human behavior with Large Language Models (LLMs) has become an emerging research direction, enabling applications in various practical domains. While prior methods, including prompting, supervised fine-tuning (SFT), and reinforcement learning (RL), have shown promise in modeling step-wise behavior, they primarily learn a population-level policy without conditioning on a user’s persona, yielding generic rather than personalized simulations. In this work, we pose a critical question: how can LLM agents better simulate personalized user behavior? We introduce Customer-R1, an RL-based method for personalized, step-wise user behavior simulation in online shopping environments. Our policy is conditioned on an explicit persona, and we optimize next-step rationale and action generation via action correctness reward signals. Experiments on the OPeRA dataset emonstrate that Customer-R1 not only significantly outperforms prompting and SFT-based baselines in next-action prediction tasks, but also better matches users’ action distribution, indicating higher fidelity in personalized behavior simulation. </p>
<blockquote>
<p>使用大型语言模型（LLM）模拟分步骤的人类行为已成为新兴的研究方向，并广泛应用于各种实际领域。虽然先前的方法，包括提示、监督微调（SFT）和强化学习（RL），在模拟分步骤行为方面显示出希望，但它们主要学习群体层面的策略，而不考虑用户的个性，从而产生通用而非个性化的模拟。在这项工作中，我们提出了一个重要问题：如何使LLM代理更好地模拟个性化用户行为？我们介绍了Customer-R1，这是一种基于RL的个性化分步骤用户行为模拟方法，适用于在线购物环境。我们的策略依赖于明确的个性特征，并通过行动正确性奖励信号优化下一步的合理性行动生成。在OPeRA数据集上的实验表明，Customer-R1不仅在预测下一步行动的任务中显著优于基于提示和SFT的基线，而且更好地匹配了用户的行动分布，这显示出在个性化行为模拟中更高的保真度。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.07230v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>大语言模型（LLM）在模拟人类行为方面展现出巨大的潜力，但在模拟个性化用户行为方面仍存在挑战。本文提出一种基于强化学习（RL）的方法Customer-R1，用于在线购物环境中个性化、逐步的用户行为模拟。该方法通过明确的个性特征制定策略，并通过行动正确性奖励信号优化下一步的合理性行动生成。实验表明，Customer-R1在行动预测任务中显著优于基于提示和精细调整的方法，更好地匹配用户行动分布，在个性化行为模拟中具有更高的保真度。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>大语言模型（LLM）正在成为模拟人类行为的热门研究方向，具有广泛的应用前景。</li>
<li>现有方法主要学习群体水平的策略，无法根据用户个性进行模拟，导致模拟结果缺乏个性化。</li>
<li>本文提出一种基于强化学习（RL）的方法Customer-R1，旨在解决个性化用户行为模拟的问题。</li>
<li>Customer-R1通过明确的个性特征制定策略，并优化下一步的合理性行动生成。</li>
<li>实验表明，Customer-R1在行动预测任务中显著优于其他方法。</li>
<li>Customer-R1能够更好地匹配用户行动分布，显示出在个性化行为模拟中的高保真度。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.07230">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic-private.zhihu.com/v2-a6dc9b828df8cf0922fdc346aef126c4~resize:0:q75.jpg?source=1f5c5e47&expiration=1760083254&auth_key=1760083254-0-0-1ad248e5e947567fe384b980e547d107&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-6b0e010d86f1439067e4c6c8cb197152~resize:0:q75.jpg?source=1f5c5e47&expiration=1760083261&auth_key=1760083261-0-0-4940aa853cbe62a52b100c273ccd0a4e&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-4c1a32a6d9a1cb2221554b2013aa0dcf~resize:0:q75.jpg?source=1f5c5e47&expiration=1760083268&auth_key=1760083268-0-0-e7acd3eb78ba18a2f6c5f1608e70dc78&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-9e4924c9548ae064168e8fd778932fa4~resize:0:q75.jpg?source=1f5c5e47&expiration=1760083274&auth_key=1760083274-0-0-60f7a1e123a586e14c02cb4e34053b29&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-f23c896ca8627d4133366e99772dc64c~resize:0:q75.jpg?source=1f5c5e47&expiration=1760083281&auth_key=1760083281-0-0-9e45ededb264b535ec576dbc692c24e2&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="GenPilot-A-Multi-Agent-System-for-Test-Time-Prompt-Optimization-in-Image-Generation"><a href="#GenPilot-A-Multi-Agent-System-for-Test-Time-Prompt-Optimization-in-Image-Generation" class="headerlink" title="GenPilot: A Multi-Agent System for Test-Time Prompt Optimization in   Image Generation"></a>GenPilot: A Multi-Agent System for Test-Time Prompt Optimization in   Image Generation</h2><p><strong>Authors:Wen Ye, Zhaocheng Liu, Yuwei Gui, Tingyu Yuan, Yunyue Su, Bowen Fang, Chaoyang Zhao, Qiang Liu, Liang Wang</strong></p>
<p>Text-to-image synthesis has made remarkable progress, yet accurately interpreting complex and lengthy prompts remains challenging, often resulting in semantic inconsistencies and missing details. Existing solutions, such as fine-tuning, are model-specific and require training, while prior automatic prompt optimization (APO) approaches typically lack systematic error analysis and refinement strategies, resulting in limited reliability and effectiveness. Meanwhile, test-time scaling methods operate on fixed prompts and on noise or sample numbers, limiting their interpretability and adaptability. To solve these, we introduce a flexible and efficient test-time prompt optimization strategy that operates directly on the input text. We propose a plug-and-play multi-agent system called GenPilot, integrating error analysis, clustering-based adaptive exploration, fine-grained verification, and a memory module for iterative optimization. Our approach is model-agnostic, interpretable, and well-suited for handling long and complex prompts. Simultaneously, we summarize the common patterns of errors and the refinement strategy, offering more experience and encouraging further exploration. Experiments on DPG-bench and Geneval with improvements of up to 16.9% and 5.7% demonstrate the strong capability of our methods in enhancing the text and image consistency and structural coherence of generated images, revealing the effectiveness of our test-time prompt optimization strategy. The code is available at <a target="_blank" rel="noopener" href="https://github.com/27yw/GenPilot">https://github.com/27yw/GenPilot</a>. </p>
<blockquote>
<p>文本到图像合成已经取得了显著的进步，然而，准确解释复杂且冗长的提示仍然是一个挑战，这常常会导致语义不一致和细节缺失。现有的解决方案，如微调，是模型特定的并且需要进行训练，而先前的自动提示优化（APO）方法通常缺乏系统的误差分析和改进策略，导致可靠性和有效性有限。同时，测试时缩放方法固定在提示、噪声或样本数量上，限制了其解释性和适应性。为了解决这些问题，我们引入了一种灵活高效的测试时提示优化策略，该策略直接在输入文本上操作。我们提出了一个即插即用的多智能体系统，称为GenPilot，它集成了误差分析、基于聚类的自适应探索、精细验证和一个用于迭代优化的记忆模块。我们的方法是模型无关的、可解释的，并且非常适合处理长和复杂的提示。同时，我们总结了常见的错误模式和改进策略，提供更多的经验并鼓励进一步探索。在DPG-bench和Geneval上的实验，改进率高达16.9%和5.7%，证明了我们方法在增强文本和图像的一致性以及生成图像的结构连贯性方面的强大能力，显示了我们的测试时提示优化策略的有效性。代码可在<a target="_blank" rel="noopener" href="https://github.com/27yw/GenPilot">https://github.com/27yw/GenPilot</a>获取。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.07217v1">PDF</a> 30 pages, 21 figures, accepted to EMNLP 2025 findings</p>
<p><strong>Summary</strong><br>文本至图像合成技术取得显著进展，但仍面临复杂和冗长提示语的准确解读挑战，导致语义不一致和细节缺失。现有解决方案如微调存在模型特定性和训练需求，而自动提示优化方法缺乏系统错误分析和改进策略，可靠性有限。为解决这些问题，我们提出一种灵活高效的测试时间提示优化策略，直接在输入文本上操作。我们提出了一个即插即用的多智能体系统GenPilot，集成错误分析、基于聚类的自适应探索、精细验证和记忆模块进行迭代优化。该方法具有模型无关性、可解释性，适合处理长而复杂的提示语。在DPG-bench和Geneval上的实验表明，我们的方法提高了文本与图像的一致性和图像的结构连贯性，显示出测试时间提示优化策略的有效性。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>文本至图像合成技术面临准确解读复杂和冗长提示语的挑战。</li>
<li>现有解决方案如微调具有模型特定性和训练需求。</li>
<li>自动提示优化方法缺乏系统错误分析和改进策略，影响可靠性和有效性。</li>
<li>提出了一个灵活高效的测试时间提示优化策略，直接在输入文本上操作。</li>
<li>插即用的多智能体系统GenPilot集成了错误分析、自适应探索、精细验证和记忆模块。</li>
<li>GenPilot具有模型无关性、可解释性，适用于处理长而复杂的提示语。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.07217">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic-private.zhihu.com/v2-3fac7ca15344ac4333eae93c7f78b8b0~resize:0:q75.jpg?source=1f5c5e47&expiration=1760083290&auth_key=1760083290-0-0-2341b74552718c8ffd1a7ddd155b098e&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-323c681c400d8b4380c84810f4e33400~resize:0:q75.jpg?source=1f5c5e47&expiration=1760083297&auth_key=1760083297-0-0-17c5b50a42e714c2c8c209c055dcf34a&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-c95cc928041425445ac32b55d3a75c69~resize:0:q75.jpg?source=1f5c5e47&expiration=1760083305&auth_key=1760083305-0-0-d6f56dc2a2f54369459ce4f99e31dc65&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-3c5e85bd43d61f5ddea49ca351d806c8~resize:0:q75.jpg?source=1f5c5e47&expiration=1760083313&auth_key=1760083313-0-0-03af777d865f9081532191d6cbbe337b&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="DecompGAIL-Learning-Realistic-Traffic-Behaviors-with-Decomposed-Multi-Agent-Generative-Adversarial-Imitation-Learning"><a href="#DecompGAIL-Learning-Realistic-Traffic-Behaviors-with-Decomposed-Multi-Agent-Generative-Adversarial-Imitation-Learning" class="headerlink" title="DecompGAIL: Learning Realistic Traffic Behaviors with Decomposed   Multi-Agent Generative Adversarial Imitation Learning"></a>DecompGAIL: Learning Realistic Traffic Behaviors with Decomposed   Multi-Agent Generative Adversarial Imitation Learning</h2><p><strong>Authors:Ke Guo, Haochen Liu, Xiaojun Wu, Chen Lv</strong></p>
<p>Realistic traffic simulation is critical for the development of autonomous driving systems and urban mobility planning, yet existing imitation learning approaches often fail to model realistic traffic behaviors. Behavior cloning suffers from covariate shift, while Generative Adversarial Imitation Learning (GAIL) is notoriously unstable in multi-agent settings. We identify a key source of this instability: irrelevant interaction misguidance, where a discriminator penalizes an ego vehicle’s realistic behavior due to unrealistic interactions among its neighbors. To address this, we propose Decomposed Multi-agent GAIL (DecompGAIL), which explicitly decomposes realism into ego-map and ego-neighbor components, filtering out misleading neighbor: neighbor and neighbor: map interactions. We further introduce a social PPO objective that augments ego rewards with distance-weighted neighborhood rewards, encouraging overall realism across agents. Integrated into a lightweight SMART-based backbone, DecompGAIL achieves state-of-the-art performance on the WOMD Sim Agents 2025 benchmark. </p>
<blockquote>
<p>真实交通仿真对于自动驾驶系统的发展和城市流动规划至关重要，然而现有的模仿学习方法往往无法模拟真实的交通行为。行为克隆受到协变量偏移的影响，而生成对抗模仿学习（GAIL）在多智能体环境中极为不稳定。我们确定了这种不稳定性的一个关键来源：无关的交互误导，其中鉴别器会由于周围车辆的非现实交互而对自我车辆的现实行为施加惩罚。为了解决这一问题，我们提出了分解多智能体GAIL（DecompGAIL），它明确地将真实性分解为自我与地图、自我与周围车辆之间的关系成分，并过滤掉了误导向的邻居与目标之间以及与地图之间的交互。我们进一步引入了一个社交PPO目标，通过距离加权邻居奖励来增强自我奖励，鼓励整体的真实跨智能体性能。被整合到一个轻量级的SMART背后支撑系统之中，DecompGAIL达到了针对未来十年城市交通的仿真代理标杆—2025城市仿真测试平台的先进水平。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.06913v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>现实交通模拟对于自动驾驶系统和城市流动规划的发展至关重要，但现有的模仿学习法往往无法模拟真实的交通行为。行为克隆面临协变量偏移问题，而生成对抗性模仿学习（GAIL）在多智能体环境中极为不稳定。为解决这一问题，我们提出分解多智能体GAIL（DecompGAIL），它将真实主义分解为自我地图和自我邻居的组成部分，过滤掉误导性的邻居与邻居和邻居与地图的互动。此外，我们还引入了社会PPO目标，通过距离加权邻居奖励来增强自我奖励，鼓励整体的真实主义跨智能体。融入轻量级SMART基础架构，DecompGAIL在WOMD Sim Agents 2025基准测试中取得最佳性能。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>现实交通模拟对自动驾驶和城市流动规划至关重要。</li>
<li>现有模仿学习法存在模拟真实交通行为的局限性。</li>
<li>行为克隆面临协变量偏移问题。</li>
<li>生成对抗性模仿学习（GAIL）在多智能体环境中不稳定。</li>
<li>DecompGAIL通过分解真实主义解决GAIL的不稳定问题，并过滤误导性互动。</li>
<li>DecompGAIL引入了社会PPO目标，通过增强自我奖励和邻居奖励来鼓励整体真实主义跨智能体。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.06913">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic-private.zhihu.com/v2-e3a67a77760f7826c3c8622ebbdf0276~resize:0:q75.jpg?source=1f5c5e47&expiration=1760083320&auth_key=1760083320-0-0-d3b3174d1083e30aee9f81184c55f779&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-6749a871804625cca00d3237b68c1762~resize:0:q75.jpg?source=1f5c5e47&expiration=1760083327&auth_key=1760083327-0-0-46aea72e17c2e0d20208a4da8920359e&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-1700582aab2e7fd8916a53f2195b6bb5~resize:0:q75.jpg?source=1f5c5e47&expiration=1760083334&auth_key=1760083334-0-0-088c24fbcc1e5e65706d7dd51a0693d2&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="FURINA-A-Fully-Customizable-Role-Playing-Benchmark-via-Scalable-Multi-Agent-Collaboration-Pipeline"><a href="#FURINA-A-Fully-Customizable-Role-Playing-Benchmark-via-Scalable-Multi-Agent-Collaboration-Pipeline" class="headerlink" title="FURINA: A Fully Customizable Role-Playing Benchmark via Scalable   Multi-Agent Collaboration Pipeline"></a>FURINA: A Fully Customizable Role-Playing Benchmark via Scalable   Multi-Agent Collaboration Pipeline</h2><p><strong>Authors:Haotian Wu, Shufan Jiang, Chios Chen, Yiyang Feng, Hehai Lin, Heqing Zou, Yao Shu, Yanran Li, Chengwei Qin</strong></p>
<p>As large language models (LLMs) advance in role-playing (RP) tasks, existing benchmarks quickly become obsolete due to their narrow scope, outdated interaction paradigms, and limited adaptability across diverse application scenarios. To address this gap, we introduce FURINA-Builder, a novel multi-agent collaboration pipeline that automatically constructs fully customizable RP benchmarks at any scale. It enables evaluation of arbitrary characters across diverse scenarios and prompt formats, as the first benchmark builder in RP area for adaptable assessment. FURINA-Builder simulates dialogues between a test character and other characters drawn from a well-constructed character-scene pool, while an LLM judge selects fine-grained evaluation dimensions and adjusts the test character’s responses into final test utterances. Using this pipeline, we build FURINA-Bench, a new comprehensive role-playing benchmark featuring both established and synthesized test characters, each assessed with dimension-specific evaluation criteria. Human evaluation and preliminary separability analysis justify our pipeline and benchmark design. We conduct extensive evaluations of cutting-edge LLMs and find that o3 and DeepSeek-R1 achieve the best performance on English and Chinese RP tasks, respectively. Across all models, established characters consistently outperform synthesized ones, with reasoning capabilities further amplifying this disparity. Interestingly, we observe that model scale does not monotonically reduce hallucinations. More critically, for reasoning LLMs, we uncover a novel trade-off: reasoning improves RP performance but simultaneously increases RP hallucinations. This trade-off extends to a broader Pareto frontier between RP performance and reliability for all LLMs. These findings demonstrate the effectiveness of FURINA-Builder and the challenge posed by FURINA-Bench. </p>
<blockquote>
<p>随着大型语言模型（LLM）在角色扮演（RP）任务中的进步，现有基准测试由于其范围狭窄、交互模式过时以及在不同应用场景中的适应性有限，很快变得过时。为了解决这一差距，我们引入了FURINA-Builder，这是一种新型多智能体协作管道，能够自动构建任何规模的完全可定制的角色扮演基准测试。它能够在多样化的场景和提示格式中评估任意角色，作为角色扮演领域第一个可适应的评估基准测试构建器。FURINA-Builder模拟测试角色与其他角色之间的对话，这些其他角色来自构建良好的角色场景池，同时由LLM裁判选择精细的评价维度，并调整测试角色的响应以形成最终的测试话语。使用此管道，我们构建了FURINA-Bench，这是一个新的全面的角色扮演基准测试，其中包括既定的和合成的测试角色，每个角色都根据特定的评价维度进行评估。人类评估和初步的可分离性分析证明了我们管道和基准测试设计的合理性。我们对最前沿的LLM进行了广泛评估，发现o3和DeepSeek-R1在英语和中文角色扮演任务上表现最佳。在所有模型中，既定角色的表现始终优于合成角色，推理能力进一步加大了这种差距。有趣的是，我们观察到模型规模并不单调减少幻觉。更重要的是，对于具有推理能力的LLM，我们发现了一个新的权衡：推理能力提高了角色扮演的性能，但同时增加了角色扮演的幻觉。这种权衡扩展到了所有LLM的RP性能和可靠性之间的帕累托前沿。这些发现证明了FURINA-Builder的有效性以及FURINA-Bench所构成的挑战。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.06800v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>该文本介绍了随着大型语言模型（LLM）在角色扮演（RP）任务中的发展，现有基准测试由于其狭窄范围、过时交互模式和有限适应不同应用场景的能力而迅速变得过时。为解决这一问题，引入了FURINA-Builder这一新型多智能体协作管道，能够自动构建任意规模的定制化RP基准测试。该管道能够评估不同角色在不同场景和提示格式下的表现，成为RP领域可适应评估的首个基准测试构建器。通过模拟测试角色与从精心构建的角色场景中抽取的其他角色之间的对话，并结合LLM法官选择的精细评价维度来调整测试角色的回应，从而构建FURINA-Bench这一新的综合性角色扮演基准测试。人类评估和初步分离性分析验证了管道和基准测试设计的合理性。对最前沿的LLM进行了广泛评估，发现o3和DeepSeek-R1在英语和中文RP任务上表现最佳。在所有模型中，已建立的角色始终优于合成角色，推理能力进一步加剧了这种差异。有趣的是，观察到模型规模并不总是减少幻视现象。更关键的是，对于具有推理能力的LLM，发现了一个新权衡：推理能力提高RP性能但同时增加RP幻视现象。这一权衡扩展到所有LLM的RP性能和可靠性之间的帕累托前沿。这些发现证明了FURINA-Builder的有效性以及FURINA-Bench所带来的挑战。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>大型语言模型（LLM）在角色扮演（RP）任务中的进步导致现有基准测试的迅速过时。</li>
<li>FURINA-Builder作为一种新型多智能体协作管道被引入，能够自动构建任意规模的定制化RP基准测试。</li>
<li>FURINA-Builder模拟对话，结合LLM评估角色表现，构建FURINA-Bench基准测试。</li>
<li>人类评估和初步分析验证了管道和基准测试设计的合理性。</li>
<li>在RP任务评估中，前沿LLM如o3和DeepSeek-R1表现最佳。</li>
<li>已建立的角色通常优于合成角色，推理能力对表现有重要影响。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.06800">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic-private.zhihu.com/v2-81fb965ba4d60125fa08203787d0d18d~resize:0:q75.jpg?source=1f5c5e47&expiration=1760083341&auth_key=1760083341-0-0-38cf48aacd5028f1e430976432aec211&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-8bbf9f92f8d4b30f6439201e56780160~resize:0:q75.jpg?source=1f5c5e47&expiration=1760083349&auth_key=1760083349-0-0-c3b642a91fc8af76a2ee0d2839c3ea81&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-2dbece0635c51c8d7c3cbec8e2710d8e~resize:0:q75.jpg?source=1f5c5e47&expiration=1760083355&auth_key=1760083355-0-0-59d9c3899bdd5a4c3b62b7e94947209c&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-1cb8a4b5ab6999a2190e744d64b8aadd~resize:0:q75.jpg?source=1f5c5e47&expiration=1760083362&auth_key=1760083362-0-0-0196f64306d26cb04cedf0dc3b8e9410&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-10-10\./crop_Agent/2510.06800v1/page_5_0.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Evolving-and-Executing-Research-Plans-via-Double-Loop-Multi-Agent-Collaboration"><a href="#Evolving-and-Executing-Research-Plans-via-Double-Loop-Multi-Agent-Collaboration" class="headerlink" title="Evolving and Executing Research Plans via Double-Loop Multi-Agent   Collaboration"></a>Evolving and Executing Research Plans via Double-Loop Multi-Agent   Collaboration</h2><p><strong>Authors:Zhi Zhang, Yan Liu, Zhejing Hu, Gong Chen, Sheng-hua Zhong, Jiannong Cao</strong></p>
<p>Automating the end-to-end scientific research process poses a fundamental challenge: it requires both evolving high-level plans that are novel and sound, and executing these plans correctly amidst dynamic and uncertain conditions. To address this bilevel challenge, we propose a novel Double-Loop Multi-Agent (DLMA) framework to solve the given research problem automatically. The leader loop, composed of professor agents, is responsible for evolving research plans. It employs an evolutionary algorithm through involvement, improvement, and integration meetings to iteratively generate and refine a pool of research proposals, exploring the solution space effectively. The follower loop, composed of doctoral student agents, is responsible for executing the best-evolved plan. It dynamically adjusts the plan during implementation via pre-hoc and post-hoc meetings, ensuring each step (e.g., drafting, coding) is well-supported by contextual and external observations. Extensive experiments on benchmarks like ACLAward and Laboratory show that DLMA generates research papers that achieve state-of-the-art scores in automated evaluation, significantly outperforming strong baselines. Ablation studies confirm the critical roles of both loops, with evolution driving novelty and execution ensuring soundness. </p>
<blockquote>
<p>自动化端到端的科学研究过程带来了一个根本性的挑战：它需要不断发展和完善新颖且合理的计划，并在动态和不确定的条件下正确执行这些计划。为了应对这一双重挑战，我们提出了一种新型的双循环多智能体（DLMA）框架来自动解决给定的研究问题。由教授智能体组成的领导者循环，负责发展研究计划。它通过参与、改进和整合会议来不断迭代生成和优化研究方案池，有效地探索解决方案空间。由博士生智能体组成的追随者循环，负责执行最佳发展方案。它通过预设会议和后续会议来动态调整实施过程中的计划，确保每一步（如起草、编码等）都得到上下文和外部观察的有力支持。在ACLAward和实验室等基准测试上的大量实验表明，DLMA生成的研究论文在自动评估中取得了最先进的分数，显著优于强基线。消融研究证实了这两个循环的关键作用，其中进化驱动新颖性，执行确保合理性。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.06761v1">PDF</a> </p>
<p><strong>Summary</strong><br>自动化端到端的科学研究过程面临双重挑战：需要制定新颖且健全的高级计划，并在动态和不确定的条件下正确执行这些计划。为解决这一双重挑战，我们提出了一种解决给定研究问题的新型双循环多智能体（DLMA）框架。由教授智能体组成的领导者循环负责研究计划的制定，通过参与、改进和整合会议，采用进化算法有效探索解决方案空间并迭代生成和细化研究提案池。由博士生智能体组成的追随者循环负责执行最佳进化计划，并在实施过程中通过预先和事后会议动态调整计划，确保每一步（如起草、编码等）都得到上下文和外部观察的良好支持。在ACLAward和Laboratory等基准测试上的大量实验表明，DLMA生成的研究论文在自动评估中达到最新水平，显著优于强大的基线。消融研究证实了这两个循环的关键作用，进化驱动了新颖性，执行确保了健全性。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>双循环多智能体（DLMA）框架旨在自动化端到端的科学研究过程。</li>
<li>领导者循环由教授智能体组成，负责通过进化算法制定新颖且健全的研究计划。</li>
<li>追随者循环由博士生智能体组成，负责执行最佳进化计划并在实施中动态调整。</li>
<li>DLMA框架能有效探索解决方案空间并生成研究提案池。</li>
<li>上下文和外部观察在每一步的执行中都得到充分利用。</li>
<li>在基准测试上的实验表明，DLMA生成的研究论文达到最新水平，显著优于基线。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.06761">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-10-10\./crop_Agent/2510.06761v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-10-10\./crop_Agent/2510.06761v1/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-10-10\./crop_Agent/2510.06761v1/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-10-10\./crop_Agent/2510.06761v1/page_5_0.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Memory-R1-Enhancing-Large-Language-Model-Agents-to-Manage-and-Utilize-Memories-via-Reinforcement-Learning"><a href="#Memory-R1-Enhancing-Large-Language-Model-Agents-to-Manage-and-Utilize-Memories-via-Reinforcement-Learning" class="headerlink" title="Memory-R1: Enhancing Large Language Model Agents to Manage and Utilize   Memories via Reinforcement Learning"></a>Memory-R1: Enhancing Large Language Model Agents to Manage and Utilize   Memories via Reinforcement Learning</h2><p><strong>Authors:Sikuan Yan, Xiufeng Yang, Zuchao Huang, Ercong Nie, Zifeng Ding, Zonggen Li, Xiaowen Ma, Kristian Kersting, Jeff Z. Pan, Hinrich Schütze, Volker Tresp, Yunpu Ma</strong></p>
<p>Large Language Models (LLMs) have demonstrated impressive capabilities across a wide range of NLP tasks, but they remain fundamentally stateless, constrained by limited context windows that hinder long-horizon reasoning. Recent efforts to address this limitation often augment LLMs with an external memory bank, yet most existing pipelines are static and heuristic-driven, lacking a learned mechanism for deciding what to store, update, or retrieve. We present Memory-R1, a reinforcement learning (RL) framework that equips LLMs with the ability to actively manage and utilize external memory through two specialized agents: a Memory Manager that learns structured operations, including ADD, UPDATE, DELETE, and NOOP; and an Answer Agent that pre-selects and reasons over relevant entries. Both agents are fine-tuned with outcome-driven RL (PPO and GRPO), enabling adaptive memory management with minimal supervision. With only 152 training QA pairs, Memory-R1 outperforms strong baselines and generalizes across diverse question types, three benchmarks (LoCoMo, MSC, LongMemEval), and multiple model scales (3B-14B). </p>
<blockquote>
<p>大型语言模型（LLMs）已在广泛的NLP任务中展现出令人印象深刻的能力，但它们仍然是根本性的无状态模型，受限于有限的上下文窗口，这阻碍了长期视野的推理。最近为解决这一限制的尝试通常是通过外部存储器增强LLMs，但大多数现有流程是静态的和启发式驱动的，缺乏一种学习机制来决定存储、更新或检索什么内容。我们提出了Memory-R1，这是一个强化学习（RL）框架，它为LLMs配备了主动管理和利用外部内存的能力，通过两个特殊代理实现：一个是学习结构化操作的内存管理器，包括添加、更新、删除和无操作；另一个是答案代理，它预先选择并对相关条目进行推理。这两个代理都是通过以结果驱动的强化学习（PPO和GRPO）进行微调，从而实现自适应内存管理，监督最少。仅使用152个训练问答对，Memory-R1就超越了强大的基线，并在各种类型的问题、三个基准测试（LoCoMo、MSC、LongMemEval）和多个模型规模（3B-14B）上实现了泛化。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.19828v4">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>大型语言模型（LLMs）在多个NLP任务中展现出强大的能力，但其本质上是无状态的，受限于有限的上下文窗口，影响长期推理。最近的研究尝试通过外部记忆库来增强LLMs，但现有管道多为静态、启发式驱动，缺乏学习机制来决定存储、更新或检索的内容。本研究提出Memory-R1，一种强化学习（RL）框架，通过两个专门代理——内存管理器（学习结构操作，如添加、更新、删除和空操作）和答案代理（预先选择和推理相关条目），使LLMs能够主动管理和利用外部记忆。两个代理均采用结果驱动的RL（PPO和GRPO）进行微调，实现自适应内存管理，监督需求最小。仅使用152对训练问答，Memory-R1表现优于强大基线，并在多个问题类型、三个基准（LoCoMo、MSC、LongMemEval）和多种模型规模（3B-14B）上实现泛化。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>大型语言模型（LLMs）在NLP任务中表现出强大的能力，但存在无状态性和长期推理的局限性。</li>
<li>现有研究通过外部记忆库增强LLMs，但现有管道缺乏学习机制来决定存储、更新或检索的内容。</li>
<li>Memory-R1框架采用强化学习，通过两个专门代理——内存管理器和答案代理，使LLMs能够主动管理和利用外部记忆。</li>
<li>内存管理器学习结构操作，如添加、更新、删除和空操作。</li>
<li>答案代理能够预先选择和推理相关条目。</li>
<li>两个代理采用结果驱动的RL进行微调，实现自适应内存管理，且监督需求最小。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.19828">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-10-10\./crop_Agent/2508.19828v4/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-10-10\./crop_Agent/2508.19828v4/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-10-10\./crop_Agent/2508.19828v4/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-10-10\./crop_Agent/2508.19828v4/page_5_0.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Multi-Agent-Taskforce-Collaboration-Self-Correction-of-Compounding-Errors-in-Long-Form-Literature-Review-Generation"><a href="#Multi-Agent-Taskforce-Collaboration-Self-Correction-of-Compounding-Errors-in-Long-Form-Literature-Review-Generation" class="headerlink" title="Multi-Agent Taskforce Collaboration: Self-Correction of Compounding   Errors in Long-Form Literature Review Generation"></a>Multi-Agent Taskforce Collaboration: Self-Correction of Compounding   Errors in Long-Form Literature Review Generation</h2><p><strong>Authors:Zhi Zhang, Yan Liu, Zhejing Hu, Gong Chen, Sheng-hua Zhong, Jiannong Cao</strong></p>
<p>Compounding error is critical in long-form literature review generation, where minor inaccuracies cascade and amplify across subsequent steps, severely compromising the faithfulness of the final output. To address this challenge, we propose the Multi-Agent Taskforce Collaboration (MATC) framework, which proactively mitigates errors by orchestrating LLM-based agents into three specialized taskforces: (1) an exploration taskforce that interleaves retrieval and outlining using a tree-based strategy to establish a grounded structure; (2) an exploitation taskforce that iteratively cycles between fact location and draft refinement to ensure evidential support; and (3) a feedback taskforce that leverages historical experience for self-correction before errors propagate. Experimental results show that MATC achieves state-of-the-art performance on existing benchmarks (AutoSurvey and SurveyEval), significantly outperforming strong baselines in both citation quality (e.g., +15.7% recall) and content quality. We further contribute TopSurvey, a new large-scale benchmark of 195 peer-reviewed survey topics, on which MATC maintains robust performance, demonstrating its generalizability. </p>
<blockquote>
<p>在长篇文献综述生成中，累积误差至关重要。轻微的不准确会在后续步骤中累积并放大，严重损害最终输出的忠实度。为了应对这一挑战，我们提出了多智能体协作（MATC）框架，它通过协调基于大型语言模型的智能体来主动减少错误，分为以下三个专业小组：(1)一个探索小组，采用基于树状结构的策略进行检索和概要介绍，以建立基础结构；(2)一个开发小组，在事实定位和初稿修订之间循环迭代，以确保证据支持；(3)一个反馈小组，利用历史经验进行自我校正，以防误差传播。实验结果表明，MATC在现有基准测试（AutoSurvey和SurveyEval）上取得了最新技术水平的高性能表现，并且在引文质量（如+ 15.7％召回率）和内容质量上均大大优于强大的基线模型。我们还推出了TopSurvey，这是一个包含195个同行评审调查主题的大规模新基准测试集，MATC在该数据集上同样表现出稳健的性能，证明了其通用性。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.04306v2">PDF</a> </p>
<p><strong>Summary</strong><br>文本中强调了长文本文献综述生成中累积误差的重要性，并提出了多代理任务协作（MATC）框架来解决这一问题。该框架通过协调LLM代理来主动减少错误，分为三个专业小组：探索小组采用树形策略进行检索和提纲编制；开发小组负责事实定位和初稿修改以确保证据支持；反馈小组利用历史经验进行自我修正以避免错误传播。实验结果表明，MATC在现有基准测试上取得了最佳性能，显著提高了引用质量（如召回率提高15.7%）和内容质量。此外，还推出了新的大规模基准测试TopSurvey，MATC在此测试中同样表现稳健，证明了其通用性。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>长文本文献综述生成存在累积误差问题，对最终输出的忠实度造成严重影响。</li>
<li>提出Multi-Agent Taskforce Collaboration (MATC)框架，通过协调LLM代理主动减少错误。</li>
<li>MATC框架包括三个专业小组：探索小组、开发小组和反馈小组，分别负责不同的任务。</li>
<li>实验结果表明，MATC在现有基准测试中表现最佳，显著提高引用质量和内容质量。</li>
<li>推出新的大规模基准测试TopSurvey，用于评估文献综述生成模型的性能。</li>
<li>MATC在TopSurvey测试上表现稳健，证明了其通用性。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.04306">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-10-10\./crop_Agent/2508.04306v2/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-10-10\./crop_Agent/2508.04306v2/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-10-10\./crop_Agent/2508.04306v2/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-10-10\./crop_Agent/2508.04306v2/page_5_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-10-10\./crop_Agent/2508.04306v2/page_5_1.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Understanding-Software-Engineering-Agents-A-Study-of-Thought-Action-Result-Trajectories"><a href="#Understanding-Software-Engineering-Agents-A-Study-of-Thought-Action-Result-Trajectories" class="headerlink" title="Understanding Software Engineering Agents: A Study of   Thought-Action-Result Trajectories"></a>Understanding Software Engineering Agents: A Study of   Thought-Action-Result Trajectories</h2><p><strong>Authors:Islem Bouzenia, Michael Pradel</strong></p>
<p>Large Language Model (LLM)-based agents are increasingly employed to automate complex software engineering tasks, such as program repair and issue resolution. These agents operate by autonomously generating natural language thoughts, invoking external tools, and iteratively refining their solutions. Despite their widespread adoption, the internal decision-making processes of these agents remain largely unexplored, limiting our understanding of their operational dynamics and failure modes. In this paper, we present a large-scale empirical study of the thought-action-result trajectories of three state-of-the-art LLM-based agents: RepairAgent, AutoCodeRover, and OpenHands. We unify their interaction logs into a common format, capturing 120 trajectories and 2,822 LLM interactions focused on program repair and issue resolution. Our study combines quantitative analyses of structural properties, action patterns, and token usage with qualitative assessments of reasoning coherence and feedback integration. We identify key trajectory characteristics, such as iteration counts and token consumption, recurring action sequences, and the semantic coherence of thoughts, actions, and their results. Our findings reveal behavioral motifs and anti-patterns that distinguish successful from failed executions, providing actionable insights for improving agent design, including prompting strategies, failure diagnosis, and anti-pattern detection. We release our dataset and annotation framework to support further research on transparent and robust autonomous software engineering agents. </p>
<blockquote>
<p>基于大型语言模型（LLM）的代理正被越来越多地用于自动化复杂的软件工程任务，如程序修复和问题解决方案。这些代理通过自主生成自然语言思想、调用外部工具并迭代优化其解决方案来运行。尽管这些代理得到了广泛应用，但其内部决策过程在很大程度上仍未被探索，这限制了我们对它们运行动态和故障模式的了解。在本文中，我们对三个最新基于LLM的代理（RepairAgent、AutoCodeRover和OpenHands）的思想行动结果轨迹进行了大规模实证研究。我们将它们的交互日志统一到一种通用格式中，捕获了120条轨迹和2822次以程序修复和问题解决为中心的LLM交互。我们的研究结合了结构属性、行动模式和令牌使用量的定量分析，以及与推理连贯性和反馈整合的定性评估。我们确定了关键轨迹特征，如迭代次数和令牌消耗、重复的行动序列以及思想、行动和结果之间的语义连贯性。我们的研究结果揭示了区分成功执行和失败执行的行为模式和反模式，为改进代理设计提供了可操作性的见解，包括提示策略、故障诊断和反模式检测。我们发布我们的数据集和注释框架，以支持对透明和稳健的自主软件工程代理的进一步研究。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.18824v2">PDF</a> ACCEPTED FOR ASE 2025</p>
<p><strong>Summary</strong></p>
<p>LLM基于的代理被广泛应用于自动化复杂的软件工程任务，如程序修复和问题解析。这些代理通过自主生成自然语言思维、调用外部工具并迭代优化解决方案来运行。本文进行了一项大规模实证研究，探讨了三种最新LLM代理（RepairAgent、AutoCodeRover和OpenHands）的思维行动结果轨迹。通过统一互动记录格式，聚焦于程序修复和问题解析的120条轨迹和2822次LLM互动进行分析。研究结合了结构性、行动模式与符号使用的定量分析，以及推理连贯性和反馈整合的定性评估。研究发现成功与失败执行的关键轨迹特征，提供改进代理设计的实用见解，包括提示策略、故障诊断和反模式检测。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLM基于的代理广泛应用于自动化复杂的软件工程任务，如程序修复和问题解析。</li>
<li>这些代理通过自主生成自然语言思维来运行，涉及调用外部工具和迭代优化解决方案。</li>
<li>本文对三种LLM代理进行了大规模实证研究，探讨了其思维行动结果轨迹。</li>
<li>通过统一互动记录格式，聚焦于程序修复和问题解析的互动进行分析。</li>
<li>研究结合了定量和定性分析，包括结构性、行动模式、符号使用、推理连贯性和反馈整合。</li>
<li>研究发现成功与失败执行的关键轨迹特征，包括迭代次数、符号消耗、重复行动序列和思维结果的语义连贯性。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.18824">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-10-10\./crop_Agent/2506.18824v2/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-10-10\./crop_Agent/2506.18824v2/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-10-10\./crop_Agent/2506.18824v2/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-10-10\./crop_Agent/2506.18824v2/page_4_1.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="AutoMind-Adaptive-Knowledgeable-Agent-for-Automated-Data-Science"><a href="#AutoMind-Adaptive-Knowledgeable-Agent-for-Automated-Data-Science" class="headerlink" title="AutoMind: Adaptive Knowledgeable Agent for Automated Data Science"></a>AutoMind: Adaptive Knowledgeable Agent for Automated Data Science</h2><p><strong>Authors:Yixin Ou, Yujie Luo, Jingsheng Zheng, Lanning Wei, Zhuoyun Yu, Shuofei Qiao, Jintian Zhang, Da Zheng, Yuren Mao, Yunjun Gao, Huajun Chen, Ningyu Zhang</strong></p>
<p>Large Language Model (LLM) agents have shown great potential in addressing real-world data science problems. LLM-driven data science agents promise to automate the entire machine learning pipeline, yet their real-world effectiveness remains limited. Existing frameworks depend on rigid, pre-defined workflows and inflexible coding strategies; consequently, they excel only on relatively simple, classical problems and fail to capture the empirical expertise that human practitioners bring to complex, innovative tasks. In this work, we introduce AutoMind, an adaptive, knowledgeable LLM-agent framework that overcomes these deficiencies through three key advances: (1) a curated expert knowledge base that grounds the agent in domain expert knowledge, (2) an agentic knowledgeable tree search algorithm that strategically explores possible solutions, and (3) a self-adaptive coding strategy that dynamically tailors code generation to task complexity. Evaluations on two automated data science benchmarks demonstrate that AutoMind delivers superior performance versus state-of-the-art baselines. Additional analyses confirm favorable effectiveness, efficiency, and qualitative solution quality, highlighting AutoMind as an efficient and robust step toward fully automated data science. Code is at <a target="_blank" rel="noopener" href="https://github.com/innovatingAI/AutoMind">https://github.com/innovatingAI/AutoMind</a>. </p>
<blockquote>
<p>大型语言模型（LLM）代理在解决现实世界的数据科学问题方面显示出巨大潜力。LLM驱动的数据科学代理有望自动化整个机器学习管道，但它们在现实世界中的实际效果仍然有限。现有框架依赖于僵化、预先定义的工作流程和不可灵活调整的编码策略；因此，它们仅在相对简单、经典的问题上表现出色，而无法获取人类实践者在复杂、创新任务中所带来的经验知识。在这项工作中，我们介绍了AutoMind，这是一个自适应、知识型LLM代理框架，通过三个关键进展克服了这些不足：（1）一个精选的专家知识库，将代理与领域专家知识相结合；（2）一种智能知识树搜索算法，战略性探索可能的解决方案；（3）一种自适应编码策略，根据任务复杂性动态调整代码生成。在两个自动化数据科学基准测试上的评估表明，AutoMind相较于最新基线技术具有优越性能。额外的分析证实了其高效性、优越的有效性和良好的解决方案质量，突出了AutoMind作为实现完全自动化数据科学的稳健而高效的步骤。代码位于<a target="_blank" rel="noopener" href="https://github.com/innovatingAI/AutoMind%E3%80%82">https://github.com/innovatingAI/AutoMind。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.10974v3">PDF</a> Ongoing work</p>
<p><strong>Summary</strong></p>
<p>大型语言模型（LLM）在解决真实世界数据科学问题方面展现出巨大潜力，但以LLM驱动的数据科学代理在现实世界中的有效性仍然有限。现有框架依赖于僵化、预先定义的工作流程和灵活的编码策略，只能在相对简单、经典的问题上表现出色，无法捕捉人类实践者在复杂、创新任务中的经验知识。本研究介绍了一种自适应知识型LLM代理框架AutoMind，它通过三个关键进展克服了这些缺点：一个精选的专家知识库使代理具备领域专家知识，一种策略性的知识树搜索算法动态探索可能的解决方案，以及一种自适应编码策略根据任务复杂性动态调整代码生成。评估结果表明，AutoMind在自动化数据科学基准测试中表现出卓越性能。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>大型语言模型（LLM）在数据科学中具有巨大潜力。</li>
<li>现有数据科学框架在复杂任务上表现有限，缺乏灵活性。</li>
<li>AutoMind框架通过三个关键进展克服这些缺点：专家知识库、知识树搜索算法和自适应编码策略。</li>
<li>AutoMind在自动化数据科学基准测试中表现出卓越性能。</li>
<li>AutoMind具备领域专家知识，能动态探索解决方案，并根据任务复杂性调整代码生成。</li>
<li>AutoMind在有效性、效率和解决方案质量方面表现出优势。</li>
<li>AutoMind朝着全自动数据科学的稳健方向迈出了一步。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.10974">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-10-10\./crop_Agent/2506.10974v3/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-10-10\./crop_Agent/2506.10974v3/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-10-10\./crop_Agent/2506.10974v3/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-10-10\./crop_Agent/2506.10974v3/page_5_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-10-10\./crop_Agent/2506.10974v3/page_5_1.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="Dyna-Think-Synergizing-Reasoning-Acting-and-World-Model-Simulation-in-AI-Agents"><a href="#Dyna-Think-Synergizing-Reasoning-Acting-and-World-Model-Simulation-in-AI-Agents" class="headerlink" title="Dyna-Think: Synergizing Reasoning, Acting, and World Model Simulation in   AI Agents"></a>Dyna-Think: Synergizing Reasoning, Acting, and World Model Simulation in   AI Agents</h2><p><strong>Authors:Xiao Yu, Baolin Peng, Ruize Xu, Michel Galley, Hao Cheng, Suman Nath, Jianfeng Gao, Zhou Yu</strong></p>
<p>Recent progress in reasoning with large language models (LLMs), such as DeepSeek-R1, demonstrates impressive capabilities in domains like mathematics and coding, by exhibiting complex cognitive behaviors such as verification, goal decomposition, and self-reflection. However, it is unclear what behavior is effective and what behavior is missing for long-horizon AI agents tasks. In this work, we propose Dyna-Think, a thinking framework that integrates planning with an internal world model with reasoning and acting to enhance AI agent performance. To enable Dyna-Think, we propose Dyna-Think Imitation Learning (DIT) and Dyna-Think Dyna Training (DDT). To initialize a policy with Dyna-Think, DIT reconstructs the thinking process of R1 to focus on performing world model simulation relevant to the proposed (and planned) action, and trains the policy using this reconstructed data. To enhance Dyna-Think, DDT uses a two-stage training process to first improve the agent’s world modeling ability via objectives such as state prediction or critique generation, and then improve the agent’s action via policy training. We evaluate our methods on OSWorld and WindowsAgentArena, and demonstrate that Dyna-Think improves the agent’s in-domain and out-of-domain performance, achieving similar best-of-n performance compared to R1 while generating 2x less tokens on average. Our extensive empirical studies reveal that 1) using critique generation for world model training is effective to improve policy performance; and 2) AI agents with better performance correlate with better world modeling abilities. We believe our results suggest a promising research direction to integrate world model simulation into AI agents to enhance their reasoning, planning, and acting capabilities. </p>
<blockquote>
<p>最近大型语言模型（LLMs）如DeepSeek-R1在推理方面的进展展示出了令人印象深刻的数学和编程领域能力，展现了如验证、目标分解和自我反思等复杂的认知行为。然而，对于长期的人工智能代理任务，尚不清楚哪些行为是有效的，哪些行为是缺失的。在这项工作中，我们提出了Dyna-Think，这是一个结合了规划、内部世界模型、推理和行动的思考框架，旨在提高人工智能代理的性能。为了实现Dyna-Think，我们提出了Dyna-Think模仿学习（DIT）和Dyna-Think动态训练（DDT）。为了用Dyna-Think初始化策略，DIT重构了R1的思考过程，专注于执行与所提议（和计划）的行动相关的世界模型模拟。然后使用这种重构的数据来训练策略。为了增强Dyna-Think，DDT采用两阶段训练过程，首先通过状态预测或评论生成等目标来提高代理的世界建模能力，然后通过策略训练来提高代理的行动能力。我们在OSWorld和WindowsAgentArena上评估了我们的方法，并证明Dyna-Think提高了代理的域内和域外性能，与R1相比实现了类似的最佳n次性能，但平均生成的令牌数量减少了2倍。我们广泛的实证研究证实：1）使用评论生成进行世界模型训练对于提高策略性能是有效的；2）表现更好的人工智能代理与更好的世界建模能力相关。我们相信我们的研究结果为实现将世界模型模拟融入人工智能代理以提高其推理、规划和行动能力这一研究方向提供了希望。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.00320v2">PDF</a> </p>
<p><strong>摘要</strong></p>
<p>近期大型语言模型（LLM）如DeepSeek-R1在数学和编程等领域的推理能力展现令人印象深刻，表现出验证、目标分解和自我反思等复杂认知行为。然而，对于长期AI代理任务，尚不清楚哪些行为有效，哪些行为缺失。本研究提出Dyna-Think框架，整合规划与世界模型内部推理与行动，以提高AI代理性能。为此，我们提出了Dyna-Think模仿学习（DIT）和Dyna-Think训练（DDT）。通过DIT重建R1的思考过程，专注于执行与提议（和计划）的行动相关的世界模型模拟，并使用这些数据训练策略。DDT则采用两阶段训练过程，首先通过状态预测或批判生成等目标提高代理的世界建模能力，然后通过政策训练提高代理的行动能力。我们在OSWorld和WindowsAgentArena上评估了这些方法，证明了Dyna-Think提高了代理的域内和域外性能，实现了与R1类似的最佳性能，平均生成令牌数量减少了一半。广泛的实证研究证明，利用批判生成进行世界模型训练对提高政策性能是有效的；AI代理的性能与其世界建模能力正相关。我们相信，将世界模型模拟整合到AI代理中以提高其推理、规划和行动能力的方向具有前景。</p>
<p><strong>关键见解</strong></p>
<ol>
<li>大型语言模型如DeepSeek-R1在数学和编程等领域展现出强大的推理能力，包括验证、目标分解和自我反思。</li>
<li>对于长期AI代理任务，尚不清楚哪些行为有效，哪些行为缺失。提出Dyna-Think框架以提高AI代理性能。</li>
<li>Dyna-Think通过整合规划与世界模型内部推理与行动来实现性能提升。</li>
<li>Dyna-Think Imitation Learning (DIT) 重建思考过程以训练策略关注与提议行动相关的世界模型模拟。</li>
<li>Dyna-Think Dyna Training (DDT) 采用两阶段训练过程来提高世界建模能力和行动能力。</li>
<li>在OSWorld和WindowsAgentArena上的评估显示，Dyna-Think提高了代理的域内和域外性能。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.00320">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-10-10\./crop_Agent/2506.00320v2/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-10-10\./crop_Agent/2506.00320v2/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-10-10\./crop_Agent/2506.00320v2/page_3_0.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="AgentDroid-A-Multi-Agent-Framework-for-Detecting-Fraudulent-Android-Applications"><a href="#AgentDroid-A-Multi-Agent-Framework-for-Detecting-Fraudulent-Android-Applications" class="headerlink" title="AgentDroid: A Multi-Agent Framework for Detecting Fraudulent Android   Applications"></a>AgentDroid: A Multi-Agent Framework for Detecting Fraudulent Android   Applications</h2><p><strong>Authors:Ruwei Pan, Hongyu Zhang, Zhonghao Jiang, Ran Hou</strong></p>
<p>With the increasing prevalence of fraudulent Android applications such as fake and malicious applications, it is crucial to detect them with high accuracy and adaptability. We present AgentDroid, a novel tool for Android fraudulent application detection based on multi-modal analysis and multi-agent systems. AgentDroid overcomes the limitations of traditional detection methods such as the inability to handle multimodal data and high false alarm rates. It processes Android applications and extracts a series of multi-modal data for analysis. Multiple LLM-based agents with specialized roles analyze the relevant data and collaborate to detect complex fraud effectively. We curated a dataset containing various categories of fraudulent applications and legitimate applications and validated our tool on this dataset. Experimental results indicate that our multi-agent tool based on GPT-4o achieves an accuracy of 91.7% and an F1-Score of 91.68%, outperforming the baseline methods. A video of AgentDroid is available at <a target="_blank" rel="noopener" href="https://youtu.be/YOM9Ex-nBts">https://youtu.be/YOM9Ex-nBts</a>. </p>
<blockquote>
<p>随着虚假和恶意等欺诈性Android应用程序的普及，以高准确度和适应性检测它们变得至关重要。我们推出了AgentDroid，这是一种基于多模态分析和多智能体系统的新型Android欺诈应用程序检测工具。AgentDroid克服了传统检测方法的局限性，如无法处理多模态数据和误报率高的问题。它处理Android应用程序并提取一系列多模态数据进行分析。多个具有专业角色的基于大型语言模型的智能体分析相关数据并进行协作，有效检测复杂的欺诈行为。我们整理了一个包含各类欺诈应用程序和合法应用程序的数据集，并在该数据集上验证了我们工具的性能。实验结果表明，我们基于GPT-4o的多智能体工具达到了91.7%的准确率和91.68%的F1分数，优于基准方法。有关AgentDroid的视频可在<a target="_blank" rel="noopener" href="https://youtu.be/YOM9Ex-nBts%E8%A7%82%E7%9C%8B%E3%80%82">https://youtu.be/YOM9Ex-nBts观看。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.12163v2">PDF</a> </p>
<p><strong>Summary</strong><br>全新工具AgentDroid用于检测Android欺诈应用。基于多模态分析和多智能体系统，克服传统检测方法的局限性，实现高准确率和适应性的欺诈应用检测。实验结果显示，其准确率高达91.7%，F1分数为91.68%，优于基线方法。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>AgentDroid是一种用于检测Android欺诈应用的新型工具。</li>
<li>它基于多模态分析和多智能体系统设计，旨在克服传统检测方法的不足。</li>
<li>AgentDroid能够处理多种模式的数据，包括从Android应用中提取的数据。</li>
<li>利用多个基于大型语言模型的智能体进行专门分析，有效检测复杂欺诈行为。</li>
<li>实验结果显示，AgentDroid在检测欺诈应用方面表现出高准确率和良好的性能。</li>
<li>AgentDroid的准确率高达91.7%，F1分数为91.68%，优于基线方法。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.12163">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-10-10\./crop_Agent/2503.12163v2/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-10-10\./crop_Agent/2503.12163v2/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-10-10\./crop_Agent/2503.12163v2/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-10-10\./crop_Agent/2503.12163v2/page_2_1.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-10-10\./crop_Agent/2503.12163v2/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-10-10\./crop_Agent/2503.12163v2/page_3_1.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="A-Dual-Agent-Adversarial-Framework-for-Robust-Generalization-in-Deep-Reinforcement-Learning"><a href="#A-Dual-Agent-Adversarial-Framework-for-Robust-Generalization-in-Deep-Reinforcement-Learning" class="headerlink" title="A Dual-Agent Adversarial Framework for Robust Generalization in Deep   Reinforcement Learning"></a>A Dual-Agent Adversarial Framework for Robust Generalization in Deep   Reinforcement Learning</h2><p><strong>Authors:Zhengpeng Xie, Yulong Zhang</strong></p>
<p>Recently, empowered with the powerful capabilities of neural networks, reinforcement learning (RL) has successfully tackled numerous challenging tasks. However, while these models demonstrate enhanced decision-making abilities, they are increasingly prone to overfitting. For instance, a trained RL model often fails to generalize to even minor variations of the same task, such as a change in background color or other minor semantic differences. To address this issue, we propose a dual-agent adversarial policy learning framework, which allows agents to spontaneously learn the underlying semantics without introducing any human prior knowledge. Specifically, our framework involves a game process between two agents: each agent seeks to maximize the impact of perturbing on the opponent’s policy by producing representation differences for the same state, while maintaining its own stability against such perturbations. This interaction encourages agents to learn generalizable policies, capable of handling irrelevant features from the high-dimensional observations. Extensive experimental results on the Procgen benchmark demonstrate that the adversarial process significantly improves the generalization performance of both agents, while also being applied to various RL algorithms, e.g., Proximal Policy Optimization (PPO). With the adversarial framework, the RL agent outperforms the baseline methods by a significant margin, especially in hard-level tasks, marking a significant step forward in the generalization capabilities of deep reinforcement learning. </p>
<blockquote>
<p>近期，强化学习（RL）在神经网络强大的能力赋能下，已成功解决了许多具有挑战性的任务。然而，尽管这些模型展现出增强的决策能力，但它们越来越容易过拟合。例如，经过训练的RL模型通常无法概括甚至同一任务的微小变化，如背景颜色的变化或其他微小的语义差异。为了解决这一问题，我们提出了一种双智能体对抗性策略学习框架，该框架允许智能体自发地学习潜在语义，而无需引入任何人为的先验知识。具体来说，我们的框架涉及两个智能体之间的游戏过程：每个智能体都试图通过对对手的策略产生扰动影响来产生相同状态的表现差异，同时保持其自身对此类扰动的稳定性。这种互动鼓励智能体学习可概括的策略，能够处理来自高维观察结果的不相关特征。在Procgen基准测试上的大量实验结果表明，对抗过程显著提高了两个智能体的概括性能，并可应用于各种RL算法，例如近端策略优化（PPO）。通过对抗性框架，RL智能体的表现远远超过基线方法，特别是在困难级别的任务中，标志着深度强化学习概括能力的一个重大进步。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.17384v2">PDF</a> </p>
<p><strong>Summary</strong><br>强化学习在应对诸多挑战性任务中表现出决策能力的增强，但其也存在过拟合的问题。为应对这一问题，提出一种双代理对抗策略学习框架，让代理自发学习潜在语义，无需引入任何人为先验知识。该框架通过两个代理间的游戏过程实现，鼓励代理学习可泛化的策略，处理高维观察中的无关特征。在Procgen基准测试上的实验结果表明，对抗过程显著提高了代理的泛化性能，并适用于各种强化学习算法。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>强化学习在解决许多挑战性任务中表现出色，但存在过拟合问题。</li>
<li>双代理对抗策略学习框架旨在解决过拟合问题，让代理自发学习潜在语义。</li>
<li>框架通过两个代理之间的游戏过程实现，每个代理都试图最大化对对方策略的干扰影响。</li>
<li>代理在游戏中通过产生相同状态的表示差异来对抗，同时保持自身稳定性。</li>
<li>这种对抗互动鼓励代理学习可泛化的策略，处理高维观察中的无关特征。</li>
<li>在Procgen基准测试上的实验表明，对抗过程显著提高了代理的泛化性能。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.17384">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-10-10\./crop_Agent/2501.17384v2/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-10-10\./crop_Agent/2501.17384v2/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-10-10\./crop_Agent/2501.17384v2/page_5_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-10-10\./crop_Agent/2501.17384v2/page_5_1.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        文章作者:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        文章链接:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-10-10/Agent/">https://kedreamix.github.io/Talk2Paper/Paper/2025-10-10/Agent/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        版权声明:
                    </i>
                </span>
                <span class="reprint-info">
                    本博客所有文章除特別声明外，均采用
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    许可协议。转载请注明来源
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Agent/">
                                    <span class="chip bg-color">Agent</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;上一篇</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-10-10/MMT/">
                    <div class="card-image">
                        
                        <img src="https://pic-private.zhihu.com/v2-c847d5b1dd97a51c096bd79df71ca723~resize:0:q75.jpg?source=1f5c5e47&expiration=1760083434&auth_key=1760083434-0-0-2e6e9a636f09ac8790bc7887027bfbff&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" class="responsive-img" alt="MMT">
                        
                        <span class="card-title">MMT</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            MMT 方向最新论文已更新，请持续关注 Update in 2025-10-10  GIIFT Graph-guided Inductive Image-free Multimodal Machine Translation
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-10-10
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/MMT/" class="post-category">
                                    MMT
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/MMT/">
                        <span class="chip bg-color">MMT</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                下一篇&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-10-10/LLM/">
                    <div class="card-image">
                        
                        <img src="https://pic-private.zhihu.com/v2-5d30dc1b2c174d4bb19a9f51df938ba9~resize:0:q75.jpg?source=1f5c5e47&expiration=1760082923&auth_key=1760082923-0-0-161031898c3a921cfcc49d31605667a8&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" class="responsive-img" alt="LLM">
                        
                        <span class="card-title">LLM</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            LLM 方向最新论文已更新，请持续关注 Update in 2025-10-10  Vibe Checker Aligning Code Evaluation with Human Preference
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-10-10
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/LLM/" class="post-category">
                                    LLM
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/LLM/">
                        <span class="chip bg-color">LLM</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- 代码块功能依赖 -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- 代码语言 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- 代码块复制 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- 代码块收缩 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC 悬浮按钮. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* 修复文章卡片 div 的宽度. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // 切换TOC目录展开收缩的相关操作.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;站点总字数:&nbsp;<span
                        class="white-color">30191.9k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- 运行天数提醒. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // 区分是否有年份.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = '本站已运行 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                daysTip = '本站已運行 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = '本站已运行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = '本站已運行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="关注我的知乎: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">知</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- 搜索遮罩框 -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span>
            <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- 白天和黑夜主题 -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- 回到顶部按钮 -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // 只在桌面版网页启用特效
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- 雪花特效 -->
    

    <!-- 鼠标星星特效 -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--腾讯兔小巢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- 动态标签 -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Σ(っ °Д °;)っ诶，页面崩溃了嘛？", clearTimeout(st)) : (document.title = "φ(゜▽゜*)♪咦，又好了！", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
